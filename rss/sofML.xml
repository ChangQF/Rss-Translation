<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 31 May 2024 03:17:03 GMT</lastBuildDate>
    <item>
      <title>训练 YOLOv8 进行裁判员姿势估计</title>
      <link>https://stackoverflow.com/questions/78557557/training-yolov8-for-pose-estimation-of-referees</link>
      <description><![CDATA[我目前正在使用预训练的 YOLOv8 模型训练姿势估计模型。我的目标是检测图像中的体育裁判并确定他们的姿势。我想知道我是否可以简单地标记图像中的裁判，让 YOLO 模型将他们识别为人，然后确定关键点，而不是手动注释数据集图像中每个裁判的关键点。
这种方法可行吗？如果可行，最好的实现方法是什么？任何指导或建议都将不胜感激。
到目前为止，我已经注释了裁判的标签并使用了物体检测模型。但是，首先使用物体检测，然后对裁判标签应用姿势估计会变得非常耗费计算资源。]]></description>
      <guid>https://stackoverflow.com/questions/78557557/training-yolov8-for-pose-estimation-of-referees</guid>
      <pubDate>Thu, 30 May 2024 23:15:49 GMT</pubDate>
    </item>
    <item>
      <title>XGBtree 模型准确率报告 1</title>
      <link>https://stackoverflow.com/questions/78557288/xgbtree-model-reporting-1-in-accuracy-metric</link>
      <description><![CDATA[我正在尝试使用此代码和 {caret} 包训练一个 XGB 模型，用于对正面或负面评论进行分类。
这似乎是正确的，但我得到的准确率全是 1？
这个模型是否过度拟合？
#install.packages(&quot;xgboost&quot;)
library(xgboost)
library(readxl)
library(caret)
library(e1071)

# 加载带有标签的原始 DataFrame
df_1 &lt;- read_excel(&quot;path/to/excel&quot;)

# 将标签转换为因子
df_1$label &lt;- as.factor(df_1$label)

train_control &lt;- trainControl(method = &quot;repeatedcv&quot;, 
number = 5, 
repeats = 10,
summaryFunction = defaultSummary,
classProb = TRUE,
savePredictions = &quot;final&quot;)

scale_pos_weight &lt;- c(&quot;X0&quot; = 1.53, &quot;X1&quot; = 1) 

xgb_model &lt;- train(x = as.matrix(sapply(df_1, as.numeric), weights = scale_pos_weight), 
y = df_1$label,
method = &quot;xgbTree&quot;,
trControl = train_control)

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78557288/xgbtree-model-reporting-1-in-accuracy-metric</guid>
      <pubDate>Thu, 30 May 2024 21:19:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 K 均值方法解决时间序列的维数灾难</title>
      <link>https://stackoverflow.com/questions/78557067/curse-of-dimensionality-in-time-series-with-k-means</link>
      <description><![CDATA[我一直在查看以下笔记本：时间序列聚类
其中作者说数据集受到“维度诅咒”的影响，因此应用tslearn提供的 TimeSeriesKMeans 是不正确的。相反，PCA 应用于列“值”时间序列的 2 个成分，然后使用普通的 k 均值函数划分聚类。
我在网上查了一下，似乎只有当数据集具有许多特征和少量数据时，才会发生“维数灾难”。文章中的数据集只有 2 个特征（时间戳、值）和许多数据，所以在我看来，不存在“维数灾难”，因此对所有“值”进行 PCA列是无用的。
我是否遗漏了重要的东西？
另一方面，如果文章是正确的，每次我想研究时间序列时，我可能会对所有值应用 PCA，然后使用经典的 k-means，而不是 tslearn 提供的 TimeSeriesKMeans 函数]]></description>
      <guid>https://stackoverflow.com/questions/78557067/curse-of-dimensionality-in-time-series-with-k-means</guid>
      <pubDate>Thu, 30 May 2024 20:05:20 GMT</pubDate>
    </item>
    <item>
      <title>给定一个包含症状和器官的文本，有没有办法检测出哪个器官导致了该症状？</title>
      <link>https://stackoverflow.com/questions/78556953/given-a-text-which-consist-of-symptoms-and-organs-are-there-ways-to-detect-whic</link>
      <description><![CDATA[假设我有以下文本：
胸部计算机断层扫描 (CT) 检测到一名患者患有劳力性呼吸困难和侧方双侧胸腔积液。

此处，由于胸部出现一些问题，检测到了呼吸困难症状。有没有办法实现一个模型来帮助我实现这一点，即根据给定文本中的症状获取受影响的身体部位？
并且，是否有任何预训练模型可以完成此任务？]]></description>
      <guid>https://stackoverflow.com/questions/78556953/given-a-text-which-consist-of-symptoms-and-organs-are-there-ways-to-detect-whic</guid>
      <pubDate>Thu, 30 May 2024 19:35:13 GMT</pubDate>
    </item>
    <item>
      <title>如何提高 452 图上二元分类的 GCN 模型性能？</title>
      <link>https://stackoverflow.com/questions/78556908/how-to-improve-gcn-model-performance-for-binary-classification-on-452-graphs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78556908/how-to-improve-gcn-model-performance-for-binary-classification-on-452-graphs</guid>
      <pubDate>Thu, 30 May 2024 19:23:59 GMT</pubDate>
    </item>
    <item>
      <title>我是否必须重置进度条？</title>
      <link>https://stackoverflow.com/questions/78556829/do-i-have-to-reset-the-progressbar</link>
      <description><![CDATA[我有一个 fit 函数，其中包含以下 for 循环：
for epoch in self.progressbar(range(n_epochs)):
如果我使用 Jupyter 笔记本，它可以正常工作。但是，如果我重新运行该单元，它只会抛出一个错误：
File ~/.local/share/virtualenvs/ML_Zero_To_Hero-tWttz9GG/lib/python3.11/site-packages/progressbar/progressbar.py:154, in ProgressBar.__next__(self)
152 self.start()
153 else:
--&gt; 154 self.update(self.currval + 1)
155 返回值
156 除 StopIteration 外：

文件 ~/.local/share/virtualenvs/ML_Zero_To_Hero-tWttz9GG/lib/python3.11/site-packages/progressbar/progressbar.py:250，在 ProgressBar.update(self, value) 中
246 如果值不为 None 且值不是 widgets.UnknownLength：
247 如果 (self.maxval 不是 widgets.UnknownLength
248 且不为 0 &lt;= value &lt;= self.maxval)：
--&gt; 250 raise ValueError(&#39;值超出范围&#39;)
252 self.currval = value
255 if not self._need_update(): return

ValueError: 值超出范围

我是否必须以某种方式重置进度条？
我将进度条实例化为：progressbar.ProgressBar(widgets=bar_widgets)
其中 bar_widgets = [ &#39;Training: &#39;, progressbar.Percentage(), &#39; &#39;, progressbar.Bar(marker=&#39;-&#39;, left=&#39;[&#39;, right=&#39;]&#39;), &#39; &#39;, progressbar.ETA() ]
我正在使用 Progressbar2 库。]]></description>
      <guid>https://stackoverflow.com/questions/78556829/do-i-have-to-reset-the-progressbar</guid>
      <pubDate>Thu, 30 May 2024 19:05:30 GMT</pubDate>
    </item>
    <item>
      <title>fastapi 机器学习脚本中的结果为空</title>
      <link>https://stackoverflow.com/questions/78556222/null-results-in-a-fastapi-machine-learning-script</link>
      <description><![CDATA[我正在开发一个 fastAPI 简单机器学习脚本。我已经成功训练了管道，它按预期工作。但现在我正尝试使用 fastAPI 和 uvicorn 来部署它。我不知道为什么它总是返回一个空值作为结果。这是一个二元分类。一些数据输入，1 或 0 应该输出。现在我这样做了：
从 pydantic 导入 BaseModel
从 fastapi 导入 FastAPI
导入 pandas 作为 pd
导入 joblib

导入 uvicorn

# 实例
app = FastAPI()

# 数据模型
class DataModel(BaseModel):
id: str
radius_mean: float
...
# 除必须预测的目标变量之外的所有其他特征

class ItemOut(BaseModel):
id: str
诊断：int

def load_weights():
model = joblib.load(&quot;models/RF/RF_weights_v1.0.pkl&quot;)
返回模型

def predict(data):
model = load_weights()
preprocessing = model.named_steps[&quot;preprocessing&quot;]
perceived_data = preprocessing.transform(data)
classifier = model.named_steps[&quot;classifier&quot;]
predict_values = classifier.predict(transformed_data)
return predict_values

@app.post(&quot;/predict&quot;, response_model=ItemOut)
async def predict(dataModel: DataModel):
data = pd.DataFrame([dataModel.dict()])
predictions = predict(data)
return {&quot;id&quot;: &quot;id&quot;, &quot;diagnosis&quot;: predictions}

if __name__ == &quot;__main__&quot;:
uvicorn.run(app, host=&quot;0.0.0.1&quot;, port=8000)

现在，我不知道模型的权重是否正确加载。最重要的是，预测的结果应该是 1 维数组（在这种情况下只有一个值），根据分类结果为 0 或 1。
但返回的却是空值。
我使用的模型是“SVC”（我用随机森林做了第二个模型，没什么特别的）。
我做错了什么？
同样，我不知道这是否是最好的方法，或者例如，最佳实践应该是加载包含这些值的 json 或加载包含多个值的 json 文件。]]></description>
      <guid>https://stackoverflow.com/questions/78556222/null-results-in-a-fastapi-machine-learning-script</guid>
      <pubDate>Thu, 30 May 2024 16:27:16 GMT</pubDate>
    </item>
    <item>
      <title>具有不平衡类别的 U-Net 分割的图像块提取</title>
      <link>https://stackoverflow.com/questions/78555784/image-patch-extraction-for-u-net-segmentation-with-imbalanced-classes</link>
      <description><![CDATA[我正在使用 U-Net 进行多类图像分割项目。
我的数据集的类别分布不平衡。有些类别几乎出现在每幅图像中，而其他类别则很少见。我不确定图像修补的最佳方法：
在每个补丁内做出相等的类别表示？在这种情况下，对于某些类别，我将不得不使用数据增强技术。
还是保持补丁内原始的不平衡比例？]]></description>
      <guid>https://stackoverflow.com/questions/78555784/image-patch-extraction-for-u-net-segmentation-with-imbalanced-classes</guid>
      <pubDate>Thu, 30 May 2024 14:57:49 GMT</pubDate>
    </item>
    <item>
      <title>平衡的 KNN 分类器，用于不平衡的二元响应</title>
      <link>https://stackoverflow.com/questions/78555660/balanced-knn-classifier-for-imbalance-binary-response</link>
      <description><![CDATA[我想建立平衡 k-NN 分类器关于平衡风险的一致性。
设 ( \hat{I}(x) ) 为一组指标 ( i )，使得 ( X_i \in B(x; \hat{\xi}_x) )。回归函数 ( \hat{\eta}(x) ) 的估计定义为：
\hat{\eta}(x) = \frac{1}{k} \sum_{i \in \hat{I}(x)} \mathbf{1}_{Y_i = 1
在标准最近邻分类中，使用 ( \hat{\eta}(x) ) 后的多数投票进行预测。具体来说，只要 ( \hat{\eta}(x) \geq \frac{1}{2} )，就会预测为类别 1。
我正在寻找有关如何理解和可能通过替换 ( \hat{\eta}(x) \geq \hat{p}) 来实现平衡 k-NN 方法的见解。其中 \hat{p} 是训练集中少数类的概率。如有任何指导或说明，我们将不胜感激。
library(class)
library(dplyr)
set.seed(123)
n &lt;- 1000
p &lt;- 0.05 # 正例比例
X &lt;- matrix(rnorm(n * 2), ncol = 2)
Y &lt;- ifelse(runif(n) &lt; p, 1, 0)

# 将数据拆分为训练集和测试集
set.seed(123)
train_index &lt;- sample(1:n, 0.7 * n)
train_data &lt;- data.frame(X = X[train_index, ], Y = Y[train_index])
test_data &lt;- data.frame(X = X[-train_index, ], Y = Y[-train_index])
# 计算训练集中正例的比例数据
#p_hat &lt;- mean(train_data$Y)

train_X &lt;- train_data %&gt;% select(-Y) %&gt;% as.matrix()
train_Y &lt;- train_data$Y
test_X &lt;- test_data %&gt;% select(-Y) %&gt;% as.matrix()
train_X &lt;- train_data %&gt;% select(-Y) %&gt;% as.matrix()

# 使用 k-NN 分类器
knn_result &lt;- knn(train = train_X, test = test_X, cl = train_Y, k = k, prob = TRUE)


如何使用上述方法或以下第 3.2 节“不平衡分类的尖锐误差界限：少数类中有多少个示例？”将标准 kNN 更改为平衡 kNN哪篇论文专门讨论了这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78555660/balanced-knn-classifier-for-imbalance-binary-response</guid>
      <pubDate>Thu, 30 May 2024 14:34:16 GMT</pubDate>
    </item>
    <item>
      <title>如何正确计算图像和文本嵌入之间的余弦相似度以检索结果</title>
      <link>https://stackoverflow.com/questions/78554528/how-to-correctly-compute-cosine-similarity-between-image-and-text-embeddings-for</link>
      <description><![CDATA[我在图像-文本对上训练了一个对比学习模型，现在我想根据图像检索最相似的文本。为此，我使用预训练的图像和文本编码器生成测试图像（嵌入）和测试文本（嵌入）。
然后我想测量每个文本和所有图像嵌入之间的余弦相似度，如下所示
images = (i1, i2, i3) text = (t1,t2, t3)

余弦相似度 = [(t1,i1),(t1, i2), (t1, i3)], [(t2, i1), (t2, i2), (t2, i3)], [(t3, i1), (t3, i2), (t3, i3)]

使用此代码
 image_embeddings = [loaded_global_vision_encoder(image) for image in test_images]

# 生成查找嵌入
finding_embeddings = [loaded_finding_encoder(tf.convert_to_tensor([finding])) for finding in test_findings]

# 计算图像和查找嵌入之间的余弦相似度

cosine_similarities = []
for finding_emb in finding_embeddings:
similarities = [tf.keras.losses.cosine_similarity(finding_emb, image_emb, axis=-1) for image_emb in image_embeddings]
cosine_similarities.append(similarities)


然后我找到具有高相似度得分的前 1 对，如下所示
image_embeddings = [
[0.1, 0.2, 0.3],
[0.4, 0.5, 0.6],
]

finding_embeddings = [
[0.3, 0.4, 0.5],
[0.6, 0.7, 0.8]
]

对于第一个发现的嵌入 [0.3, 0.4, 0.5]:
余弦相似度:

对于 [0.1, 0.2, 0.3]: 0.9746

对于 [0.4, 0.5, 0.6]: 0.9873


对于第二个发现的嵌入 [0.6, 0.7, 0.8]:
余弦相似度:

对于 [0.1, 0.2, 0.3]: 0.8847

对于 [0.4, 0.5, 0.6]: 0.9603


步骤 2：找到前 k 个最相似的图像索引
假设 k = 2
对于第一个发现嵌入 [0.3, 0.4, 0.5]：

前 2 个最相似的图像索引：[3, 1]

对于第二个发现嵌入 [0.6, 0.7, 0.8]：

前 2 个最相似的图像索引：[2, 4]
results = [
[3, 1],
[2, 4],
]


使用此代码
 k = 1
results = [tf.math.top_k(similarities, k).indices.numpy() for余弦相似性]

所以现在我完全不知道这个检索部分，我非常困惑。我这样做代码
predicted_reports = [[test_findings[int(idx)] for idx in indices] for indices in results]

我认为它是这样工作的
test_findings = [
&quot;无急性心肺过程。&quot;,
&quot;心脏大小正常。无局灶性气腔疾病或积液。&quot;,
&quot;肺容量低。无急性发现。&quot;,
&quot;肺部清晰。无胸腔积液或气胸。&quot;,
&quot;胸椎退行性变化。&quot;
]

results = [
[3, 1],
[2, 4],
[0, 3]
]

predicted_reports = [
[
&quot;肺部清晰。无胸腔积液或气胸。&quot;,
&quot;心脏大小正常。无局灶性气腔疾病或积液。&quot;
],
[
&quot;肺容量低。无急性发现。&quot;,
&quot;胸椎退行性变化。&quot;
],
[
&quot;无急性心肺过程。&quot;,
&quot;肺部清晰。无胸腔积液或气胸。&quot;
]
]

如果它按照我解释的方式工作，那么我会很高兴，因为我对使用这种检索方式的结果并不满意，但是我很想知道我对这个代码的概念是否正确。]]></description>
      <guid>https://stackoverflow.com/questions/78554528/how-to-correctly-compute-cosine-similarity-between-image-and-text-embeddings-for</guid>
      <pubDate>Thu, 30 May 2024 10:57:43 GMT</pubDate>
    </item>
    <item>
      <title>如何在推荐系统中处理货到付款的邮政特征？[关闭]</title>
      <link>https://stackoverflow.com/questions/78529077/how-to-deal-with-cod-postal-feature-in-recommendation-systems</link>
      <description><![CDATA[我有一个包含邮政编码列的数据集。它们具有一定意义，我想将其用作特征。我处于预处理阶段，仍不确定要使用的算法。
我需要有关使用邮政编码列作为特征的最佳方法的建议。
我发现您可以使用独热编码，但就我而言，我要处理大量唯一的邮政编码（例如 3513 个不同的邮政编码），独热编码（或虚拟编码）由于其引入的高维数而不切实际。]]></description>
      <guid>https://stackoverflow.com/questions/78529077/how-to-deal-with-cod-postal-feature-in-recommendation-systems</guid>
      <pubDate>Fri, 24 May 2024 14:06:08 GMT</pubDate>
    </item>
    <item>
      <title>相册强度增强会破坏图像</title>
      <link>https://stackoverflow.com/questions/78477344/albumentations-intensity-augmentations-disrupt-the-image</link>
      <description><![CDATA[我使用预处理的 z 分数标准化列表作为数据集的来源。
这是 Albumentations 增强的图像拼贴图：
在此处输入图像描述
这是我的 Compose：
augmentation = A.Compose([
A.Horizo​​ntalFlip(),
A.RandomBrightnessContrast(brightness_limit=(-0.0001, 0.0001),contrast_limit=(-0.01, 0.01)),
A.CoarseDropout(8, 0.1, 0.1),
A.Rotate(limit=15),
A.Affine(shear=(-2, 2), scale=(0.95, 1.05)),
&gt;! ToTensorV2()
])

在 50% 的图像上，即使使用非常小的参数，应用 RandBrightnessContrast 后，图像的整个分布也会压缩到 [0, 1]（从 z 分数标准化图像的预期值 ~-2,~2）。
有什么办法吗？
也许我应该在这些之后执行 z 分数标准化，但我的初衷是将所有确定性步骤（调整大小、标准化等）与增强步骤分开以提高效率。]]></description>
      <guid>https://stackoverflow.com/questions/78477344/albumentations-intensity-augmentations-disrupt-the-image</guid>
      <pubDate>Tue, 14 May 2024 10:11:54 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv8：如何在测试集上计算映射</title>
      <link>https://stackoverflow.com/questions/78073911/yolov8-how-to-calculate-map-on-test-set</link>
      <description><![CDATA[假设我有一个名为“test”的文件夹，里面有“images”和“labels”文件夹。我还有一个经过训练的 YOLOv8 模型，名为“best.pt”。我的标签是多边形（yolo-obb .txt 文件）。
我想找到我的 YOLOv8 模型在此测试集上的平均精度 (MAP)。
我已经阅读了预测和基准测试的文档，但是，我很难找到从一些测试图像计算地图的示例。
https://docs.ultralytics.com/modes/predict/
https://docs.ultralytics.com/modes/benchmark/
from ultralytics import YOLO

# 加载预训练YOLOv8n 模型
model = YOLO(&#39;best.pt&#39;)

# 对图像运行推理
results = model([&#39;test/images/bus.jpg&#39;, &#39;test/images/zidane.jpg&#39;]) # 2 个 Results 对象的列表

我想我必须将图像列表放在上面，然后编写代码来计算测试文件夹中所有内容的映射并取平均值。有没有已经完成此操作的软件包？
实现此任务的代码是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78073911/yolov8-how-to-calculate-map-on-test-set</guid>
      <pubDate>Wed, 28 Feb 2024 11:05:52 GMT</pubDate>
    </item>
    <item>
      <title>用于多标签分类的 CLIP</title>
      <link>https://stackoverflow.com/questions/74927358/clip-for-multi-label-classification</link>
      <description><![CDATA[我正在使用 CLIP 来确定单词和图像之间的相似性。
目前我正在使用此 repo 和以下代码，对于分类，它给出了很好的结果。我需要它来进行多标签分类，其中我需要使用 sigmoid 而不是 softmax。
import torch
from PIL import Image
import open_clip

model, _, preprocess = open_clip.create_model_and_transforms(&#39;ViT-B-32-quickgelu&#39;, pretrained=&#39;laion400m_e32&#39;)
tokenizer = open_clip.get_tokenizer(&#39;ViT-B-32-quickgelu&#39;)

image = preprocess(Image.open(&quot;CLIP.png&quot;)).unsqueeze(0)
text = tokenizer([&quot;a diagram&quot;, &quot;a dog&quot;, &quot;a cat&quot;])

with torch.no_grad(), torch.cuda.amp.autocast():
image_features = model.encode_image(image)
text_features = model.encode_text(text)
image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)

text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)

print(&quot;Label probs:&quot;, text_probs) # prints: [[1., 0., 0.]]

现在我想将它用于多类。例如，如果我们在图像上有狗和猫，我希望两者都有较高的概率，所以我需要用 sigmoid 来运行它。但是这给我的结果都在 0.55 左右，正确的类别是 0.56，错误的是 0.54，所以结果是这样的 [0.54, 0.555, 0.56]。使用 S 形函数后，我希望得到 [0.01, 0.98, 0.99] 这样的值。
我做错了什么？我怎样才能得到我想要的结果？]]></description>
      <guid>https://stackoverflow.com/questions/74927358/clip-for-multi-label-classification</guid>
      <pubDate>Tue, 27 Dec 2022 08:56:22 GMT</pubDate>
    </item>
    <item>
      <title>如何将文档分成训练集和测试集？</title>
      <link>https://stackoverflow.com/questions/42471570/how-can-i-split-documents-into-training-set-and-test-set</link>
      <description><![CDATA[我正在尝试构建一个分类模型。本地文件夹中有 1000 个文本文档。我想将它们分成训练集和测试集，分割比例为 70:30（70 -&gt; 训练和 30 -&gt; 测试）。有什么更好的方法吗？我正在使用 Python。

我想要一种以编程方式分割训练集和测试集的方法。首先读取本地目录中的文件。其次，建立这些文件的列表并对其进行随机排序。第三，将它们分成训练集和测试集。
我尝试了几种使用内置 Python 关键字和函数的方法，但都失败了。最后，我想到了接近它的想法。此外，交叉验证也是构建一般分类模型的一个不错的选择。]]></description>
      <guid>https://stackoverflow.com/questions/42471570/how-can-i-split-documents-into-training-set-and-test-set</guid>
      <pubDate>Sun, 26 Feb 2017 17:08:29 GMT</pubDate>
    </item>
    </channel>
</rss>