<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 30 Oct 2024 06:36:04 GMT</lastBuildDate>
    <item>
      <title>Kong AI 代理插件：在 Kong AI 网关上配置自托管 LLM 的正确参数</title>
      <link>https://stackoverflow.com/questions/79139619/kong-ai-proxy-plugin-correct-parameters-for-configuring-a-self-hosted-llm-on-ko</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79139619/kong-ai-proxy-plugin-correct-parameters-for-configuring-a-self-hosted-llm-on-ko</guid>
      <pubDate>Wed, 30 Oct 2024 03:54:46 GMT</pubDate>
    </item>
    <item>
      <title>验证数据的输入形状无效</title>
      <link>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</link>
      <description><![CDATA[我正在使用 Tensorflow 在 Python 中开发一个简单的 ML 模型。代码如下：
import tensorflow as tf
import pandas as pd

# 加载 CSV 数据
def load_data(filename):
data = pd.read_csv(filename)
X = data[[&#39;X0&#39;,&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;]]
Y = data[[&#39;Y0&#39;,&#39;Y1&#39;]]
return tf.data.Dataset.from_tensor_slices((X.values, Y.values))

training_data = load_data(&quot;binarydatatraining.csv&quot;)
print(training_data)

# 构建一个简单的神经网络模型
model = tf.keras.models.Sequential([
tf.keras.layers.Dense(4,activation=&#39;relu&#39;),
tf.keras.layers.Dense(2)
])
# 编译模型
model.compile(optimizer=&#39;adam&#39;,
loss=&#39;mean_squared_error&#39;)

# 加载验证数据
validation_data = load_data(&quot;binarydatavalidation.csv&quot;)
print(validation_data)

# 训练模型
model.summary()
model.fit(training_data.batch(9), epochs=5)
model.summary()
model.fit(training_data.batch(9), epochs=1, validation_data = validation_data, validation_steps = 2)

一切都运行正常，直到我开始包含验证数据，该数据具有与训练数据相同数量的参数。然后我收到错误
ValueError：调用 Sequential.call() 时遇到异常。

[1m输入 Tensor(&quot;sequence_1/Cast:0&quot;, shape=(4,), dtype=float32) 的输入形状无效。预期形状 (None, 4)，但输入具有不兼容的形状 (4,)[0m

Sequential.call() 收到的参数：
• 输入=tf.Tensor(shape=(4,), dtype=int64)
• 训练=False
• 掩码=None

打印验证和训练数据集显示它们具有相同的维度，并且运行 print(training_data) 和 print(validation_data) 都给出
&lt;_TensorSliceDataset element_spec=(TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(2,), dtype=tf.int64, name=None))&gt;

如何正确设置验证数据以与 model.fit 内联运行？]]></description>
      <guid>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</guid>
      <pubDate>Tue, 29 Oct 2024 21:59:29 GMT</pubDate>
    </item>
    <item>
      <title>我应该怎么做才能将 SHAP 与 CLIP 模型一起使用？[关闭]</title>
      <link>https://stackoverflow.com/questions/79138283/what-should-i-do-to-use-the-shap-together-with-a-clip-model</link>
      <description><![CDATA[我想了解为什么图像和文本具有较高的余弦相似度度量，所以我计划使用 SHAP 来了解文本和图像中每个特征的重要性。我没有找到任何为 CLIP 实现 SHAP 的材料，有人知道怎么做吗？
我没有找到任何资源，我一直在尝试调整任何处理文本的代码。]]></description>
      <guid>https://stackoverflow.com/questions/79138283/what-should-i-do-to-use-the-shap-together-with-a-clip-model</guid>
      <pubDate>Tue, 29 Oct 2024 17:11:29 GMT</pubDate>
    </item>
    <item>
      <title>CNN 字符识别问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79137980/problem-with-a-cnn-for-character-recognition</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79137980/problem-with-a-cnn-for-character-recognition</guid>
      <pubDate>Tue, 29 Oct 2024 15:48:11 GMT</pubDate>
    </item>
    <item>
      <title>问题：ValueError：分类指标无法处理多类多输出和多标签指标目标的混合[关闭]</title>
      <link>https://stackoverflow.com/questions/79137738/problem-valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-mul</link>
      <description><![CDATA[ValueError：分类指标无法处理多类多输出和多标签指标目标的混合

我已检查预测和 y 的 dtype 和维度是否相同，但错误仍然存​​在。
这是我的代码：
def train(tr_set, dv_set, model, config, device):
&#39;&#39;&#39; 多标签分类的训练函数 &#39;&#39;&#39;

n_epochs = config[&#39;n_epochs&#39;]
optimizer = getattr(torch.optim, config[&#39;optimizer&#39;])(model.parameters(), **config[&#39;optim_hparas&#39;])
best_acc = 0.0
loss_record = {&#39;train&#39;: [], &#39;dev&#39;: []}
early_stop_cnt = 0
epoch = 0
criterion = nn.BCEWithLogitsLoss()

当 epoch &lt; n_epochs:
model.train()
total_correct = 0
total_samples = 0
total_train_loss = 0.0
all_labels = []
all_preds = []

for x, y in tr_set:
optimizer.zero_grad()
x = x.to(device)
y = y.float().to(device)
pred = model(x)
loss = criterion(pred, y)
loss.backward()
total_train_loss += loss.detach().cpu().item()
optimizer.step()

# 记录损失和准确度指标
loss_record[&#39;train&#39;].append(loss.detach().cpu().item())
predict = (torch.sigmoid(pred) &gt; 0.5).float()
all_labels.append(y.cpu())
all_preds.append(predicted.cpu())

correct_preds = (predicted == y).float().mean().item()
total_correct += correct_preds
total_samples += 1
.......

print(f&#39;在 {epoch} 个 epoch 后完成训练&#39;)
return best_acc, loss_record

def dev(dv_set, model, device, criterion):
&#39;&#39;&#39; 在验证集上评估 &#39;&#39;&#39;
model.eval()
total_loss = 0.0
all_labels = []
all_preds = []

with torch.no_grad():
for x, y in dv_set:
x = x.to(device)
y = y.float().to(device)
pred = model(x)
loss = criterion(pred, y)
total_loss += loss.item()

# 附加标签和预测以进行指标计算
predicted = (torch.sigmoid(pred) &gt; 0.5).float()
all_labels.append(y.cpu())
all_preds.append(predicted.cpu())

all_labels = torch.cat(all_labels).numpy()
all_preds = torch.cat(all_preds).numpy()

# 计算指标
accuracy = accuracy_score(all_labels, all_preds)
precision = precision_score(all_labels, all_preds, average=&#39;micro&#39;, zero_division=1)
recall = recall_score(all_labels, all_preds, average=&#39;micro&#39;, zero_division=1)
f1 = f1_score(all_labels, all_preds, average=&#39;micro&#39;, zero_division=1)

dev_acc = accuracy

如何解决问题？]]></description>
      <guid>https://stackoverflow.com/questions/79137738/problem-valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-mul</guid>
      <pubDate>Tue, 29 Oct 2024 14:49:22 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow 进行多轮训练时如何控制内存增长？</title>
      <link>https://stackoverflow.com/questions/79137676/how-to-control-memory-growth-when-using-tensorflow-in-multi-round-training</link>
      <description><![CDATA[在使用 TensorFlow 进行多轮训练时，我遇到了与内存增长相关的问题。具体来说，我有一个模型训练循环，其中我在每个循环中生成训练和评估数据，并且我的内存使用量似乎不断增长，最终导致内存不足错误。我正在尝试了解如何在这些迭代过程中有效地管理或释放内存。
在此处输入图片描述
这是我的代码的简化版本
# 定义用于训练数据的 TensorFlow 变量
for num_round in range(1, 1 + total_num_round):

train_data = generate_all_batch_s_path_samples(s_0_, net_list_c, batch_size, epochs_t + 1)
eval_data = generate_all_batch_s_path_samples(s_0_, net_list_c, batch_size, eval_num_batch)

# 训练和评估过程

# 删除使用的数据
del train_data, eval_data
gc.collect()

我遇到的问题面临：

每轮生成的 train_data 和 eval_data 占用大量内存，我似乎无法有效地释放这些内存，导致内存不断增长。
函数 generate_all_batch_s_path_samples 没有使用 tf.function 修饰，因为它使用线程进行并行计算，这使其与 tf.function 不兼容。

问题：

除了使用 tf.keras.backend.clear_session() 之外，还有没有更有效的方法来在迭代之间释放内存？
是否有推荐的方法来管理多轮训练场景中的内存增长，例如这个？

上下文：

我使用的是 TensorFlow 2.16.0。

数据生成过程 (generate_all_batch_s_path_samples) 会在每一轮中创建新的张量用于训练和评估。

我尝试了几种方法来控制内存使用情况：

使用 assign() 而不是重复定义 train_data 和 eval_data。
使用 gc.collect() 和 del train_data, eval_data 释放内存，但这些方法不起作用。


]]></description>
      <guid>https://stackoverflow.com/questions/79137676/how-to-control-memory-growth-when-using-tensorflow-in-multi-round-training</guid>
      <pubDate>Tue, 29 Oct 2024 14:36:13 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：'KerasHistory'对象没有属性'layer'</title>
      <link>https://stackoverflow.com/questions/79135894/attributeerror-kerashistory-object-has-no-attribute-layer</link>
      <description><![CDATA[我在使用 Keras 模型时遇到错误“AttributeError：&#39;KerasHistory&#39; 对象没有属性 &#39;layer&#39;”。
我尝试访问层信息，但似乎我引用了错误的对象。我尝试将名称层更改为操作，但没有成功。
我使用的是 TensorFlow v2.17.0。这是代码：
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.layers import Input, ZeroPadding2D, Conv2D, MaxPooling2D, BatchNormalization, Activation, Add, AveragePooling2D, Flatten, Dense, Dropout

input_shape = (96, 96, 1)

# 输入张量形状
X_input = Input(input_shape)

# 零填充
X = ZeroPadding2D((3,3))(X_input)

# 1 - 阶段
X = Conv2D(64, (7,7), strides= (2,2), name = &#39;conv1&#39;, kernel_initializer= glorot_uniform(seed = 0))(X)
X = BatchNormalization(axis =3, name = &#39;bn_conv1&#39;)(X)
X = Activation(&#39;relu&#39;)(X)
X = MaxPooling2D((3,3), strides= (2,2))(X)

# 2 - 阶段
X = res_block(X, filter= [64,64,256], stage= 2)

# 3 - 阶段
X = res_block(X, filter= [128,128,512], stage= 3)

# 平均池化
X = AveragePooling2D((2,2), name = &#39;Averagea_Pooling&#39;)(X)

# 最终层
X = Flatten()(X)
X = Dense(4096, 激活 = &#39;relu&#39;)(X)
X = Dropout(0.2)(X)
X = Dense(2048, 激活 = &#39;relu&#39;)(X)
X = Dropout(0.1)(X)
X = Dense(30, 激活 = &#39;relu&#39;)(X)

model_1_facialKeyPoints = Model(inputs= X_input, 输出 = X)
model_1_facialKeyPoints.summary()

这是回溯：
AttributeError Traceback (最近一次调用最后一次)
&lt;ipython-input-366-fd266d53d661&gt; 在 &lt;cell line: 34&gt;()
32 
33 
---&gt; 34 model_1_facialKeyPoints = Model( 输入= X_input，输出 = X)
35 model_1_facialKeyPoints.summary()

4 帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/ functional.py in _validate_graph_inputs_and_outputs(self)
692 # 检查 x 是否为输入张量。
693 # pylint：disable=protected-access
--&gt; 694 
695 layer = x._keras_history.layer
696 if len(layer._inbound_nodes) &gt; 1 or (

AttributeError: &#39;KerasHistory&#39; 对象没有属性 &#39;layer&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/79135894/attributeerror-kerashistory-object-has-no-attribute-layer</guid>
      <pubDate>Tue, 29 Oct 2024 05:11:46 GMT</pubDate>
    </item>
    <item>
      <title>评估模型预测性能时，mase() 错误无法索引数据以外的行[关闭]</title>
      <link>https://stackoverflow.com/questions/79134999/error-with-mase-cant-indexes-rows-beyond-data-when-evaluating-model-forecasti</link>
      <description><![CDATA[我正在预测时间序列结果 Y，其预测因子滞后 t-12 天，以产生 12 天前的预测，样本外验证框架为 10 个不重叠的折叠。我想使用平均缩放误差作为性能指标。根据我对这篇文章的理解，我将朴素预测的时间步长设置为 12，以便朴素预测和我的模型预测在同一时间间隔。我理解这会将我的模型预测（使用滞后值预测）与 12 天前的 y 值进行比较。由于某种原因，mase() 函数似乎无法以 12 天的步长循环遍历数据，因为它会返回一条错误消息，指出它必须索引“负值行”，这是不可能的。我使用 yardstick 包 mase 函数 时也遇到了同样的情况。有谁知道如何修复该问题，或者可以指出我做错了什么吗？
查看带有模拟数据的示例
#load libraries#

library(tidiverse)
library(Metrics)
library(caret)

#create simulation data

set.seed(123)
y&lt;-sample(1:150,662,replace = T)
X1_L12&lt;-runif(662,-1,1)#假设 X1 滞后 12 天
X2_L12&lt;-runif(662,-1:1)#假设 X2 滞后 12 天
date&lt;-sample(1:662)

dat&lt;-data.frame(date,y,X1_L12,X2_L12)

#create the function to compute mase setting the t-12 时的简单预测

mase_lag12 &lt;- function(data, lev = NULL, model = NULL) {
data$pred &lt;- as.numeric(data$pred)
data$obs &lt;- as.numeric(data$obs)
masefunction = mase(data$obs,data$pred,12)
names(masefunction) &lt;- c(&#39;MASE&#39;)
masefunction
}

#OOS 验证框架
#创建时间片并定义性能指标
myTimeControlmase &lt;- trainControl(method = &quot;timeslice&quot;,
initialWindow = 53,
horizo​​n = 13,
skip=65,
fixedWindow = TRUE,
summaryFunction = mase_lag12)

#model
glmnetmod_lag12 = train(y~X1_L12+X2_L12,
method = &quot;glmnet&quot;,
family=&quot;poisson&quot;,
trControl = myTimeControlmase,maximize=FALSE,
preProc = c(&quot;range&quot;),
data=dat)

在代码的最后一部分拟合模型时，它会返回以下错误消息：

actual[1:naive_end] 中的错误：只有 0 可以与负下标混合
]]></description>
      <guid>https://stackoverflow.com/questions/79134999/error-with-mase-cant-indexes-rows-beyond-data-when-evaluating-model-forecasti</guid>
      <pubDate>Mon, 28 Oct 2024 20:20:47 GMT</pubDate>
    </item>
    <item>
      <title>如何对多个特征应用多个估计量来选择具有最高 f1 分数的组合？</title>
      <link>https://stackoverflow.com/questions/79134937/how-to-apply-multiple-estimator-on-multiple-number-of-features-to-select-the-com</link>
      <description><![CDATA[我想对多个特征使用多个估计器算法运行递归特征消除，并在测试数据上保留最高的 f1 分数组合。
如何创建一个代码，可以在测试数据上生成并显示具有最佳算法的最佳特征数量（最高 f1，其次是最高 ROC），而不是逐一查看结果？我的数据集不平衡。
我尝试了下面的代码，它可以为不同数量的特征生成不同估计器的结果。但我仍然需要逐一查看结果。如何做到这一点？
estimators = [(&#39;Logistic Regression&#39;, LogisticRegression()),
(&#39;Random Forest&#39;, RandomForestClassifier())]
num_features_to_select = [4,5,7,9,11,15]

for estimator_name, estimator in estimators:
for n_features in num_features_to_select:
# 创建 RFE 对象
rfe = RFE(estimator=estimator, n_features_to_select=n_features)
# 将 RFE 与数据拟合
rfe.fit(X_resampled,Y_resampled)
# 获取选定的特征
selector = X_resampled.columns[rfe.support_]
X_train_selected = X_resampled[selector]
X_test_selected = X_test[selector]
log_reg_model = sm.Logit(Y_resampled, X_train_selected).fit()
pred_test = log_reg_model.predict(X_test_selected)
pred_test_1 = np.where(pred_test&gt;0.5,1,0)
logit_roc_auc = roc_auc_score(Y_test, pred_test)
fpr, tpr, Thresholds = roc_curve(Y_test, pred_test)
precision, recall, Thresholds = precision_recall_curve(Y_test, pred_test)
# 使用交叉验证评估模型性能
scores = cross_val_score(estimator, X_resampled[selected_features], Y_resampled, cv=5)
# 打印结果
print(f&#39;Estimator: {estimator_name}, 特征数量: {n_features}, 平均 CV 分数: {scores.mean()}&#39;)
print(f&#39;估计器：{estimator_name}，特征数量：{n_features}，ROC：{logit_roc_auc}&#39;)
print(f&#39;估计器：{estimator_name}，特征数量：{n_features}，f1 分数：{f1_score(Y_test, pred_test_1)}&#39;)
print(f&#39;估计器：{estimator_name}，特征数量：{n_features}，PRC AUC：{auc(recall,precision)}&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79134937/how-to-apply-multiple-estimator-on-multiple-number-of-features-to-select-the-com</guid>
      <pubDate>Mon, 28 Oct 2024 19:57:20 GMT</pubDate>
    </item>
    <item>
      <title>在 lightgbm 中，为什么 train 和 cv API 会接受 categorical_feature 参数，而它已经存在于数据集构造中</title>
      <link>https://stackoverflow.com/questions/78383840/in-lightgbm-why-do-the-train-and-the-cv-apis-accept-categorical-feature-argument</link>
      <description><![CDATA[以下是 lightgbm 的 .cv API

lightgbm.cv(params, train_set, num_boost_round=100, folds=None, nfold=5, stratified=True, shuffle=True, metrics=None, feval=None, init_model=None, feature_name=&#39;auto&#39;, categorical_feature=&#39;auto&#39;, fpreproc=None, seed=0, callbacks=None, eval_train_metric=False, return_cvbooster=False)

有一个参数cateogrical_feature

分类特征。如果是 int 列表，则解释为索引。如果是 str 列表，则解释为特征名称（也需要指定 feature_name）。

现在 .train API

lightgbm.train(params, train_set, num_boost_round=100, valid_sets=None, valid_names=None, feval=None, init_model=None, feature_name=&#39;auto&#39;, categorical_feature=&#39;auto&#39;, keep_training_booster=False, callbacks=None)

这里还有一个 categorical_feature 参数。文档与上述相同
现在，正如您所注意到的，这两个 API 都使用 lightgbm 数据集，而该数据集本身采用 categorical_feature 参数。文档完全相同
问题：

如果两者都指定，哪一个优先？
建议在哪个位置指定 categorical_feature？
这两个选择在内部与 lightgbm 管道的工作方式有何不同？
]]></description>
      <guid>https://stackoverflow.com/questions/78383840/in-lightgbm-why-do-the-train-and-the-cv-apis-accept-categorical-feature-argument</guid>
      <pubDate>Thu, 25 Apr 2024 10:03:27 GMT</pubDate>
    </item>
    <item>
      <title>警告：tensorflow：顺序模型中的层只能有一个输入张量</title>
      <link>https://stackoverflow.com/questions/73181243/warningtensorflowlayers-in-a-sequential-model-should-only-have-a-single-input</link>
      <description><![CDATA[我从 tensorflow 网站对自动编码器的第一个示例的介绍 复制了过去的代码，以下代码适用于 mnist 时尚数据集，但不适用于我的数据集。这给了我一个很长的警告。请告诉我我的数据集出了什么问题
警告
屏幕上缺少相同的错误
这里 x_train 是我的数据集：
tf.shape(x_train)

输出 &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([169,** **28, 28])&gt;

这里 x_train 是 mnist 数据集：
tf.shape(x_train)

输出&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([60000, 28, 28])&gt;

我制作数据集的整个代码：
dir_path=&#39;auto/ttt/&#39;
data=[]
x_train=[]
for i in os.listdir(dir_path):
img=image.load_img(dir_path+&#39;//&#39;+i,color_mode=&#39;grayscale&#39;,target_size=(28,28)) 
data=np.array(img)
data=data/255.0
x_train.append(data)

这是警告：
警告：tensorflow：顺序模型中的层应该只有一个输入张量。已接收：输入=(&lt;tf.Tensor&#39;IteratorGetNext:0&#39;shape=(None, 28)dtype=float32&gt;,&lt;tf.Tensor&#39;IteratorGetNext:1&#39;shape=(None, 28)dtype=float32&gt;,&lt;tf.Tensor&#39;IteratorGetNext:2&#39;shape=(None, 28)
dtype=float32&gt;,&lt;tf.Tensor&#39;IteratorGetNext:3&#39;shape=(None, 28)
dtype=float32&gt;,&lt;tf.Tensor&#39;IteratorGetNext:4&#39;shape=(None, 28)dtype=float32&gt;,&lt;tf.Tensor&#39;IteratorGetNext:5&#39;shape=(None, 28)dtype=float32&gt;,&lt;tf.Tensor &#39;IteratorGetNext:6&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:7&#39; shape=(None, 28) dtype=flo...

还有这个值错误（相同的警告）：
ValueError：调用层“sequential_4”（类型为 Sequential）时遇到异常。

层“flatten_2”需要 1 个输入，但它收到了 169 个输入张量。收到的输入：[&lt;tf.Tensor &#39;IteratorGetNext:0&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:1&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:2&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:3&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:4&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:5&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:6&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:7&#39; shape=(None, 28) dtype=float32&gt;, &lt;tf.Tensor &#39;IteratorGetNext:8&#39; shape=(None, 28) dtype=float3...
]]></description>
      <guid>https://stackoverflow.com/questions/73181243/warningtensorflowlayers-in-a-sequential-model-should-only-have-a-single-input</guid>
      <pubDate>Sun, 31 Jul 2022 06:54:28 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：层“顺序”的输入 0 与层不兼容</title>
      <link>https://stackoverflow.com/questions/71370360/valueerror-input-0-of-layer-sequential-is-incompatible-with-the-layer</link>
      <description><![CDATA[我有一个聊天机器人模型，我用数据集对其进行了训练，以提供“标准”对话，例如你好，你好吗等。现在我想用一个数据集来“扩展”现有模型，该数据集可以提供与运输、库存等相关的问题的答案。
这是我的工作/已经训练过的模型：
# 创建顺序模型
model = Sequential()

# 添加第一层，其输入形状取决于输入的大小和“relu”激活函数
model.add(Dense(256, input_shape=(len(training_data_x[0]),),activation=activations.relu))

# 添加 Dropout 以防止过度拟合
model.add(Dropout(0.6))
# 具有 64 个神经元的附加层
model.add(Dense(128,activation=activations.relu))
model.add(Dropout(0.2))
# 具有类别神经元数量的附加密集层 &amp; softmax 激活函数
# -&gt; 将输出层中的结果添加到“1”得到 %
model.add(Dense(len(training_data_y[0]),activation=activations.softmax))
# print(len(training_data_y[0])) = 71
sgd = SGD(learning_rate=0.01, decay=1e-6, motivation=0.9, nesterov=True)
# 编译模型
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=sgd, metrics=[&#39;accuracy&#39;])

output = model.fit(np.array(training_data_x), np.array(training_data_y), epochs=200, batch_size=5, verbose=1)
plot_model_output(output)
model.summary()
model.save(&#39;./MyModel_tf&#39;, save_format=&#39;tf&#39;)

训练数据准备如下一个单独的类，并将某个 json 文件作为输入。
现在我只需将 JSON 文件替换为包含与我想添加到模型中的内容相关的数据的文件，并尝试像这样拟合它：
json_data = json.loads(open(&#39;data.json&#39;).read())

model = load_model(&#39;MyModel_tf&#39;)

model.fit(np.array(training_data_x), np.array(training_data_y), epochs=200, batch_size=5, verbose=1)

但是当我运行它时，我收到此错误：
ValueError: 输入 0 of layer &quot;sequence&quot;与层不兼容：预期形状=（无，652），发现形状=（无，71）

我假设数据是问题所在……但它的结构完全相同，只是更短。
我的问题：

我尝试实现它的方式有意义吗？
我应该尝试以不同的方式添加其他数据吗？
第二个数据集的长度必须与第一个数据集的长度相同吗？
]]></description>
      <guid>https://stackoverflow.com/questions/71370360/valueerror-input-0-of-layer-sequential-is-incompatible-with-the-layer</guid>
      <pubDate>Sun, 06 Mar 2022 12:41:31 GMT</pubDate>
    </item>
    <item>
      <title>SVM 模型将概率得分大于 0.1（默认阈值 0.5）的实例预测为正例</title>
      <link>https://stackoverflow.com/questions/68475534/svm-model-predicts-instances-with-probability-scores-greater-than-0-1default-th</link>
      <description><![CDATA[我正在研究一个二分类问题。我遇到的情况是，我使用了从 sklearn 导入的逻辑回归和支持向量机模型。这两个模型适合相同的、不平衡的训练数据，并调整了类别权重。它们取得了相当的表现。当我使用这两个预训练模型来预测一个新的数据集时。LR 模型和 SVM 模型预测的正例数量相似。并且预测的实例有很大的重叠。
然而，当我查看被归类为正例的概率分数时，LR 的分布从 0.5 到 1，而 SVM 从 0.1 左右开始。我调用函数 model.predict(prediction_data) 来找出预测为各个类别的实例，并调用函数
model.predict_proba(prediction_data) 给出被分类为 0（neg）和 1（pos）的概率分数，并假设它们都具有默认阈值 0.5。
我的代码没有错误，我不知道为什么 SVM 也将概率分数 &lt;0.5 的实例预测为正值。对如何解释这种情况有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/68475534/svm-model-predicts-instances-with-probability-scores-greater-than-0-1default-th</guid>
      <pubDate>Wed, 21 Jul 2021 19:36:41 GMT</pubDate>
    </item>
    <item>
      <title>维度问题：检查输入时出错：预期 conv2d_1_input 有 4 个维度，但得到的数组形状为 (26, 26, 1)</title>
      <link>https://stackoverflow.com/questions/60802354/dimension-problems-error-when-checking-input-expected-conv2d-1-input-to-have-4</link>
      <description><![CDATA[我有一个 CNN，它以以下通过 Canny 边缘检测转换为二值图像的图像作为输入。
​​并输出三个类别之一。
img = cv2.imread(path)
img = cv2.Canny(img, 33, 76)
img = np.resize(img, (26, 26, 1))
imgs.append(img)

据我所知，我必须将其转换为 3 维 (26,26,1) 图像，以便网络可以使用它。这是我的网络：
IMG_HEIGHT = 26
IMG_WIDTH = 26
no_Of_Filters=60
size_of_Filter=(5,5)
size_of_pool=(2,2)
no_Of_Nodes = 500
model_new = Sequential([
Conv2D(no_Of_Filters, size_of_Filter, padding=&#39;same&#39;,activation=&#39;relu&#39;, input_shape=(IMG_HEIGHT, IMG_WIDTH , 1)),
MaxPooling2D(pool_size=size_of_pool),
Conv2D(no_Of_Filters, size_of_Filter, padding=&#39;same&#39;,activation=&#39;relu&#39;),
MaxPooling2D(pool_size=size_of_pool),
Conv2D(64, size_of_Filter, padding=&#39;same&#39;,激活=&#39;relu&#39;),
MaxPooling2D(pool_size=size_of_pool),
Flatten(),
Dense(512, 激活=&#39;relu&#39;),
Dense(3, 激活=&#39;softmax&#39;)
])

训练效果良好。在我训练并创建模型后，我想针对该网络测试图像
test_image = cv2.Canny(test_image ,33,76)
test_image = np.resize(test_image, (26, 26, 1))
test_image = test_image [np.newaxis, ...]
prediction = model.predict(test_image)
print(prediction)

现在我收到错误：
ValueError：检查输入时出错：预期 conv2d_1_input 有 4 个维度，但得到的数组形状为 (26, 26, 1)

为什么训练后的模型现在需要 4 维输入？]]></description>
      <guid>https://stackoverflow.com/questions/60802354/dimension-problems-error-when-checking-input-expected-conv2d-1-input-to-have-4</guid>
      <pubDate>Sun, 22 Mar 2020 17:06:46 GMT</pubDate>
    </item>
    <item>
      <title>如何将输入图像与 CNN 中第一个卷积层的神经元进行映射？[关闭]</title>
      <link>https://stackoverflow.com/questions/60690923/how-to-map-input-image-with-neurons-in-first-conv-layer-in-cnn</link>
      <description><![CDATA[我很难将输入图像与第一个 CNN 转换层中的神经元进行映射，但我对输入特征如何映射到 ANN 中的第一个隐藏层有基本的了解。
理解输入图像与第一个转换层中的神经元之间的映射的最佳方法是什么？
我如何澄清对以下代码示例的疑问？代码取自 Coursera 的 DL 课程。
def initialize_parameters():
&quot;&quot;&quot;
初始化权重参数以使用 tensorflow 构建神经网络。形状为：
W1：[4, 4, 3, 8]
W2：[2, 2, 8, 16]
返回：
参数——包含 W1、W2 的张量字典
&quot;&quot;&quot;

tf.set_random_seed(1) # 以便您的&quot;random&quot;数字与我们的数字相匹配

### 此处开始代码 ###（大约 2 行代码）
W1 = tf.get_variable(&quot;W1&quot;,[4,4,3,8],initializer = tf.contrib.layers.xavier_initializer(seed = 0))
W2 = tf.get_variable(&quot;W2&quot;,[2,2,8,16],initializer = tf.contrib.layers.xavier_initializer(seed = 0))
### 此处结束代码 ###

parameters = {&quot;W1&quot;: W1,
&quot;W2&quot;: W2}

返回参数

def forward_propagation(X,parameters):
&quot;&quot;&quot;
为模型实现前向传播：
CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED

参数：
X -- 输入数据集占位符，形状为 (输入大小、示例数量)
参数 -- 包含参数“W1”、“W2”的 Python 字典
形状在 Initialize_parameters 中给出

返回：
Z3 -- 最后一个 LINEAR 单元的输出
“”

# 从字典“parameters”中检索参数
W1 = 参数[&#39;W1&#39;]
W2 = 参数[&#39;W2&#39;]

### 此处开始代码 ###
# CONV2D：步幅为 1，填充“相同”
Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1], padding = &#39;相同&#39;)
# RELU
A1 = tf.nn.relu(Z1)
# MAXPOOL：窗口 8x8，步幅 8，填充“相同”
P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = &#39;相同&#39;)
# CONV2D：过滤器 W2，步幅 1，填充“相同”
Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = &#39;SAME&#39;)
# RELU
A2 = tf.nn.relu(Z2)
# MAXPOOL: 窗口 4x4, 步幅 4, padding &#39;SAME&#39;
P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = &#39;SAME&#39;)
# FLATTEN
P2 = tf.contrib.layers.flatten(P2)
# FULLY-CONNECTED，无非线性激活函数（不调用 softmax）。
# 输出层中有 6 个神经元。提示：其中一个参数应该是“activation_fn=None”
Z3 = tf.contrib.layers.fully_connected(P2, 6,activation_fn=None)
### END CODE HERE ###

return Z3

with tf.Session() as sess:
np.random.seed(1)
X, Y = create_placeholders(64, 64, 3, 6)
parameters = initialize_parameters()
Z3 = forward_propagation(X, parameters)
init = tf.global_variables_initializer()
sess.run(init)
a = sess.run(Z3, {X: np.random.randn(1,64,64,3), Y: np.random.randn(1,6)})
print(&quot;Z3 = &quot; + str(a))

这个输入图像的大小是多少64643 由 8 个大小为 443 的过滤器处理？
stride = 1、padding = same 和 batch_size = 1。
到目前为止，我所理解的是，第一个卷积层中的每个神经元将有 8 个过滤器，每个过滤器的大小为 443。第一个卷积层中的每个神经元将获取与过滤器大小相同的输入图像部分（这里是 443），并应用卷积运算并产生八个 64*64 特征映射。
如果我的理解正确，那么：

为什么我们需要跨步操作，因为每个神经元处理的内核大小和输入图像部分是相同的，如果我们应用步幅 = 1（或 2），则输入图像部分的边界是交叉的，这是我们不需要的，对吗？

我们如何知道输入图像的哪一部分（与内核大小相同）映射到第一个卷积层的哪个神经元？


如果不是，那么：

输入图像如何在第一个卷积层的神经元上传递，是完整的输入图像传递给每个神经元（就像在完全连接的 ANN 中一样，其中所有输入特征被映射到第一个隐藏层中的每个神经元）？

或输入图像的一部分？我们如何知道输入图像的哪一部分映射到第一个卷积层的哪个神经元？

上述示例（W1= [4, 4, 3, 8]）指定的内核数量是每个神经元还是第一个卷积层中的内核总数？

我们如何知道上述示例在第一个卷积层中使用了多少个神经元？

神经元数量和第一个卷积层的内核数量之间有什么关系吗？

]]></description>
      <guid>https://stackoverflow.com/questions/60690923/how-to-map-input-image-with-neurons-in-first-conv-layer-in-cnn</guid>
      <pubDate>Sun, 15 Mar 2020 08:16:45 GMT</pubDate>
    </item>
    </channel>
</rss>