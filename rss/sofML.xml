<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 03 Sep 2024 09:17:24 GMT</lastBuildDate>
    <item>
      <title>如何实现自适应负荷和光伏预测模型的强化学习？</title>
      <link>https://stackoverflow.com/questions/78942987/how-to-implement-reinforcement-learning-for-adaptive-load-and-pv-forecasting-mod</link>
      <description><![CDATA[我正在开展一个项目，使用时间序列数据构建负载和光伏 (PV) 预测模型。目前，我已经使用随机森林和 LSTM 实现了模型，虽然它们表现相当不错，但由于趋势多变，我遇到了挑战。
我面临的问题是，负载和 PV 输出由于外部因素而高度可变。例如，负载因不同的消费模式而变化，PV 输出因气候变化而波动。我需要一个可以动态适应这些变化的模型。
我正在考虑使用强化学习 (RL) 来开发一个可以实时适应这些环境变化的模型。我的目标是实施一种在线学习方法，让模型根据新数据不断学习和更新自身。但是，我发现重新训练随机森林和 LSTM 等模型在计算上非常昂贵。
我的问题是：
如何有效地实施强化学习以实现这种自适应预测？
是否有特定的 RL 算法或技术非常适合动态变化环境中的时间序列预测？
在在线学习场景中，我可以使用哪些策略来平衡模型性能和计算成本之间的权衡？
任何指导或资源都将不胜感激！
我已经实施了随机森林和 LSTM 模型来进行负载和 PV 预测。这些模型对于初始预测效果很好，但很难适应数据随时间变化的趋势。我探索了在线学习方法，希望它们能提供实时适应的解决方案。但是，我发现不断重新训练这些模型在计算上非常昂贵，并且不适合我的用例。
我希望开发一种可以实时适应负载和 PV 趋势变化的模型，而无需不断重新训练。我的目标是找到一种可以动态高效地处理数据变化的方法。
到目前为止，我尝试过的模型要么需要频繁重新训练，这很昂贵，要么它们无法足够快地适应数据的变化。这导致预测准确性随着时间的推移而下降。]]></description>
      <guid>https://stackoverflow.com/questions/78942987/how-to-implement-reinforcement-learning-for-adaptive-load-and-pv-forecasting-mod</guid>
      <pubDate>Tue, 03 Sep 2024 06:46:11 GMT</pubDate>
    </item>
    <item>
      <title>SAM 模型中无法检测图像 - TypeError：无法处理此数据类型</title>
      <link>https://stackoverflow.com/questions/78942834/image-not-detecting-in-sam-model-typeerror-cannot-handle-this-data-type</link>
      <description><![CDATA[以下是我的代码。每当我上传任何类型的图像时，都会出现相同的错误。它是否只处理高质量图像（或任何特定类型的图像）或我的代码中存在任何错误？
我尝试了不同的图像，上面的代码是 chatgpt 经过一些修改后给出的。仍然没有运气。
错误
TypeError：无法处理此数据类型：（1, 1, 640, 3），|u1
code
import torch
import numpy as np
from PIL import Image
fromsegment_anything import sam_model_registry, SamPredictor
from google.colab import files

# 加载 SAM 模型

sam = sam_model_registry[&quot;vit_b&quot;](checkpoint=&quot;/content/sam_vit_b_01ec64.pth&quot;)
predictor = SamPredictor(sam)

uploaded = files.upload()
image_name = list(uploaded.keys())[0]

image = Image.open(image_name).convert(&quot;RGB&quot;)
image_np = np.array(image)

# 检查图像形状

print(f&quot;原始图像形状：{image_np.shape}&quot;)

# 如果存在额外维度，则删除它们

if len(image_np.shape) == 4 and image_np.shape[0] == 1:
image_np = image_np.squeeze(0) # 如果第一个维度的大小为 1，则删除它

# 确保图像的格式和类型正确

image_np = image_np.astype(np.uint8)

print(f&quot;处理后的图像形状：{image_np.shape}&quot;)

# 将图像设置为 SAM 预测器

predictor.set_image(image_np)

# 预测图像的蒙版

masks = predictor.predict()

# 确保您有蒙版并使用它们

如果蒙版不为 None 且 len(masks) &gt; 0:
mask = mask[0] # 假设第一个蒙版是您需要的

# 将蒙版应用于图像
masked_image = np.where(mask[..., None], image_np, 0) # 将蒙版应用于图像

# 将蒙版图像转换回 PIL 图像
masked_image = Image.fromarray(masked_image)

# 显示蒙版图像
masked_image.show()

否则：
print(&quot;未找到给定图像的蒙版。&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78942834/image-not-detecting-in-sam-model-typeerror-cannot-handle-this-data-type</guid>
      <pubDate>Tue, 03 Sep 2024 05:49:49 GMT</pubDate>
    </item>
    <item>
      <title>RL：梯度赌博机代理</title>
      <link>https://stackoverflow.com/questions/78942595/rl-gradient-bandit-agent</link>
      <description><![CDATA[我正在阅读 Sutton&amp;Barto 的《强化学习：导论》。尝试测试梯度强盗代理（第 2.7 章）。但性能极低。我试过：

使用基线 = 平均奖励，不使用基线；
alpha = 0.1、0.2、0.3、0.4；
初始偏好 H = 0、1、10、100。

没有任何帮助。
这是我的 Python 代码，用于代理的 生命步骤 = 动作选择 + 参数更新 (self = agent)：
# 用于概率计算：
pref_exps = np.exp(self.params[&quot;preferences&quot;])
pref_exps_sum = sum(pref_exps)

# 选择强盗：
choice_dice = np.random.uniform() * pref_exps_sum
accum_pref_exp = 0
for i, pref_exp in enumerate(pref_exps):
accum_pref_exp += pref_exp
if accum_pref_exp &gt;= choice_dice:
self.chosen_bandit_i = i
break

# self.reward 在此处填充：
self.perform_bandit(self.chosen_bandit_i)

# 更新基线：
self.params[&quot;lifetime&quot;] += 1
self.params[&quot;average_reward&quot;] += 1 / self.params[&quot;lifetime&quot;] * (self.reward - self.params[&quot;average_reward&quot;])

# 更新偏好：
for i, pref_exp in enumerate(pref_exps):
probability = pref_exp / pref_exps_sum
if i == self.chosen_bandit_i:
self.params[&quot;preferences&quot;][i] += self.params[&quot;alpha&quot;] * (self.reward - self.params[&quot;average_reward&quot;]) * (1 - probability)
else:
self.params[&quot;preferences&quot;][i] -= self.params[&quot;alpha&quot;] * (self.reward - self.params[&quot;average_reward&quot;]) * probability

此代码导致性能极差（100 个代理，每个代理访问自己的 10 个 1-armed-bandits，测试超过 2000 步），我们可以从下图中看到：

我看过这篇帖子，修复错误后，它的代码似乎与我的代码相同，这也是我写这篇帖子的原因。但与我的代码不同，那篇帖子的代码在纠正后可以正常工作！
我不知道自己在哪里犯了错误。你们能帮我正确使用Gradient-bandit agent的全部功能吗？]]></description>
      <guid>https://stackoverflow.com/questions/78942595/rl-gradient-bandit-agent</guid>
      <pubDate>Tue, 03 Sep 2024 03:40:50 GMT</pubDate>
    </item>
    <item>
      <title>我应该做什么 Web 开发或 Ai/ML [关闭]</title>
      <link>https://stackoverflow.com/questions/78942412/what-should-i-do-web-development-or-ai-ml</link>
      <description><![CDATA[我是计算机科学专业的学生。我的第三年才刚刚开始，我学过 html、CSS 和 JavaScript。但我不确定我应该做 Ai/ML 还是 Web 开发。
我尝试过 Web 开发，但我不确定是否应该继续。]]></description>
      <guid>https://stackoverflow.com/questions/78942412/what-should-i-do-web-development-or-ai-ml</guid>
      <pubDate>Tue, 03 Sep 2024 01:40:05 GMT</pubDate>
    </item>
    <item>
      <title>是否可以将 PyTorch CVAE 模型转换为 Java 程序？[关闭]</title>
      <link>https://stackoverflow.com/questions/78942272/is-it-possible-to-convert-a-pytorch-cvae-model-into-a-java-program</link>
      <description><![CDATA[我编写了一个 Python 程序，可以使用 CVAE 模型架构生成图像。它使用 torch、torchvision、pillow、BertTokenizer 和 numpy 等库。它从 .pth 文件加载权重。我的代码如下所示：
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
from transformers import BertTokenizer
import numpy as np

LATENT_DIM = 128
HIDDEN_DIM = 256
model_path = &quot;./model.pth&quot;
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

class TextEncoder(nn.Module):
def __init__(self, input_size, output_size):
super(TextEncoder, self).__init__()
self.fc = nn.Linear(input_size, output_size)

def forward(self, x):
return self.fc(x)

class CVAE(nn.Module):
def __init__(self, input_encoder):
super(CVAE, self).__init__()
self.input_encoder = input_encoder

self.encoder = nn.Sequential(
nn.Conv2d(4, 32, 3, stride=1, padding=1),
nn.ReLU(),
nn.Conv2d(32, 64, 3, stride=2, padding=1),
nn.ReLU(),
nn.Conv2d(64, 128, 3, stride=2, padding=1),
nn.ReLU(),
nn.Flatten(),
nn.Linear(128 * 4 * 4, HIDDEN_DIM),
)

self.fc_mu = nn.Linear(HIDDEN_DIM + HIDDEN_DIM, LATENT_DIM)
self.fc_logvar = nn.Linear(HIDDEN_DIM + HIDDEN_DIM, LATENT_DIM)

self.decoder_input = nn.Linear(LATENT_DIM + HIDDEN_DIM, 128 * 4 * 4)
self.decoder = nn.Sequential(
nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),
nn.ReLU(),
nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),
nn.ReLU(),
nn.Conv2d(32, 4, 3, stride=1, padding=1),
nn.Tanh(),
)

def encode(self, x, c):
x = self.encoder(x)
x = torch.cat([x, c], dim=1)
mu = self.fc_mu(x)
logvar = self.fc_logvar(x)
return mu, logvar

def decrypt(self, z, c):
z = torch.cat([z, c], dim=1)
x = self.decoder_input(z)
x = x.view(-1, 128, 4, 4)
return self.decoder(x)

def reparameterize(self, mu, logvar):
std = torch.exp(0.5 * logvar)
eps = torch.randn_like(std)
return mu + eps * std

def forward(self, x, c):
mu, logvar = self.encode(x, c)
z = self.reparameterize(mu, logvar)
return self.decode(z, c), mu, logvar

tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)

def generate_image(model, text_prompt, device):
coded_input = tokenizer(
text_prompt, padding=True, truncation=True, return_tensors=&quot;pt&quot;
)
input_ids =coded_input[&quot;input_ids&quot;].to(device)
tention_mask =coded_input[&quot;attention_mask&quot;].to(device)

使用 torch.no_grad():
text_encoding = model.text_encoder(input_ids,tention_mask)

z = torch.randn(1, LATENT_DIM).to(device)

使用 torch.no_grad():
generated_image = model.decode(z, text_encoding)

generated_image = generated_image.squeeze(0).cpu()
generated_image = (generated_image + 1) / 2
generated_image = generated_image.clamp(0, 1)
generated_image = transforms.ToPILImage()(generated_image)

return generated_image

def clean_image(image, Threshold=0.75):
np_image = np.array(image)
alpha_channel = np_image[:, :, 3]
alpha_channel[alpha_channel &lt;= int(threshold * 255)] = 0
alpha_channel[alpha_channel &gt; int(threshold * 255)] = 255
return Image.fromarray(np_image)

def gen(prompt):
text_encoder = TextEncoder(hidden_​​size=HIDDEN_DIM, output_size=HIDDEN_DIM)
model = CVAE(text_encoder).to(device)
model.load_state_dict(torch.load(model_path, map_location=device))
model.eval()
return clean_image(generate_image(model, prompt, device)).resize(
(16, 16), resample=Image.NEAREST
)

我需要将此生成器和模型嵌入到 jar 文件中，因此必须将脚本转换为 Java 代码。
我听说过 DL4J 之类的库，但我不知道如何使用它们，或者它们是否支持我想要实现的目标。对此事的任何见解都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78942272/is-it-possible-to-convert-a-pytorch-cvae-model-into-a-java-program</guid>
      <pubDate>Mon, 02 Sep 2024 23:50:39 GMT</pubDate>
    </item>
    <item>
      <title>在 Google colab 中克隆 GitHub 代码并解决库和依赖项版本不匹配错误</title>
      <link>https://stackoverflow.com/questions/78941693/cloning-github-code-in-google-colab-and-solving-libraries-and-dependency-version</link>
      <description><![CDATA[由于本地机器资源有限，我想使用 GitHub 中的代码在 Google colab 中使用 ML 进行面部表情识别。一个问题是 Google colab 的环境与其他环境不同，后者的代码结构很容易理解，比如 vs code。另一个问题是由于代码版本太旧而导致的库版本和依赖项错误。
以下是 GitHub 链接：
https://github.com/Talented-Q/POSTER_V2
我尝试在我的 Google colab 中从 GitHub 克隆代码，但执行文件非常复杂，因为文件太多，Google colab 环境与我们在本地（如 vs code）中使用的环境完全不同。而且我还收到了很多与库版本和依赖项相关的错误，解决起来非常具有挑战性。]]></description>
      <guid>https://stackoverflow.com/questions/78941693/cloning-github-code-in-google-colab-and-solving-libraries-and-dependency-version</guid>
      <pubDate>Mon, 02 Sep 2024 18:29:56 GMT</pubDate>
    </item>
    <item>
      <title>RandomForest 模型未按预期工作</title>
      <link>https://stackoverflow.com/questions/78941615/randomforest-model-not-working-as-expected</link>
      <description><![CDATA[我想尝试预测股票价格。我知道股市基本上是一个随机过程，因此我不期望任何正回报。我编写了一个小脚本，使用随机森林回归器，该回归器已根据过去 20 年左右的 AAPL 股票数据（过去 100 天除外）进行训练。我使用过去 100 天作为验证。
根据开盘价/收盘价/最高价/最低价和交易量，我在数据框中创建了另外两列：收盘价的涨跌百分比和 days_since_start_column，因为如果我的判断正确，模型无法根据日期时间进行学习。
无论如何，这是代码的其余部分：
df = pd.read_csv(&#39;stock_data.csv&#39;)
df = df[::-1].reset_index()
df[&#39;timestamp&#39;] = pd.to_datetime(df[&#39;timestamp&#39;])
df[&#39;% Difference&#39;] = df[&#39;close&#39;].pct_change()

splits = [
{&#39;date&#39;: &#39;2020-08-31&#39;, &#39;ratio&#39;: 4},
{&#39;date&#39;: &#39;2014-06-09&#39;, &#39;ratio&#39;: 7},
{&#39;date&#39;: &#39;2005-02-28&#39;, &#39;ratio&#39;: 2},
{&#39;date&#39;: &#39;2000-06-21&#39;, &#39;ratio&#39;: 2}
]

用于 split 中的 split:
split[&#39;date&#39;] = pd.to_datetime(split[&#39;date&#39;])
split_date = split[&#39;date&#39;]
ratio = split[&#39;ratio&#39;]
df.loc[df[&#39;timestamp&#39;] &lt; split_date, &#39;close&#39;] /= 比率

df[&#39;days_since_start&#39;] = (df[&#39;timestamp&#39;] - df[&#39;timestamp&#39;].min()).dt.days
#data = r.json()
target = df.close
features = [&#39;days_since_start&#39;,&#39;open&#39;,&#39;high&#39;,&#39;low&#39;,&#39;volume&#39;]

X_train = (df[features][:-100])
X_validation = df[features][-100:]

y_train = df[&#39;close&#39;][:-100]
y_validation = df[&#39;close&#39;][-100:]

#X_train,X_validation,y_train,y_validation = train_test_split(df[features][:-100],target[:-100],random_state=0)

model = RandomForestRegressor()
model.fit(X_train,y_train)
predictions = model.predict(X_validation)

predictions_df = pd.DataFrame(columns=[&#39;days_since_start&#39;,&#39;close&#39;])
predictions_df[&#39;close&#39;] = predictions
predictions_df[&#39;days_since_start&#39;] = df[&#39;timestamp&#39;][-100:].values
plt.xlabel(&#39;日期&#39;)
#plt.scatter(df.loc[X_validation.index, &#39;timestamp&#39;], predictions, color=&#39;red&#39;, label=&#39;预测收盘价&#39;, alpha=0.6)
plt.plot(df.timestamp[:-100],df.close[:-100],color=&#39;black&#39;)
plt.plot(df.timestamp[-100:],df.close[-100:],color=&#39;green&#39;)
plt.plot(predictions_df.days_since_start,predictions_df.close,color=&#39;red&#39;)
plt.show()

我用黑色绘制了过去几年截至最近 100 天的收盘价，用绿色绘制了最近 100 天的收盘价，用红色绘制了最近 100 天的预测收盘价。这是结果（过去 100 天）：

为什么价格大幅上涨后模型保持平稳？我在训练过程中做错了什么吗？我的验证数据集太小了吗？还是这只是超参数调整的问题？]]></description>
      <guid>https://stackoverflow.com/questions/78941615/randomforest-model-not-working-as-expected</guid>
      <pubDate>Mon, 02 Sep 2024 17:53:54 GMT</pubDate>
    </item>
    <item>
      <title>huggingface_hub/file_download.py 收到 urllib3 ConnectTimeoutError</title>
      <link>https://stackoverflow.com/questions/78941230/huggingface-hub-file-download-py-receives-urllib3-connecttimeouterror</link>
      <description><![CDATA[尝试复制代码：https://github.com/ximinng/DiffSketcher.Do 所有环境配置操作均成功完成，但在尝试运行代码启动模型时出现错误：在此处输入图像描述、在此处输入图像描述、在此处输入图像描述。如何处理？请帮忙！！！！！！我尝试用 huggingface 上发布的其他相同模型替换模型（runwayml/stable-diffusion-v1-5-ov）
替代模型：在此处输入图片说明。
但我只能更改代码中的这一行，无法解决。我不知道如何直接替换它：在此处输入图片说明]]></description>
      <guid>https://stackoverflow.com/questions/78941230/huggingface-hub-file-download-py-receives-urllib3-connecttimeouterror</guid>
      <pubDate>Mon, 02 Sep 2024 15:43:01 GMT</pubDate>
    </item>
    <item>
      <title>异常检测：如何找出数据集最后一天的800个混合样本？</title>
      <link>https://stackoverflow.com/questions/78940634/anomaly-detection-how-to-find-out-the-800-mixed-sample-on-the-last-day-of-the-d</link>
      <description><![CDATA[有 8000 名患者。 4 年内共进行了 22666 组测试。
每位患者至少进行了两组在不同日期进行的测试。
每组测试有 11 个不同的实验室
（实验室：白蛋白、ALP、ALT、AST、BUN、肌酐、GGT、葡萄糖、LDH、总胆红素、总蛋白）
每 11 个测试彼此独立，参考范围各不相同。
医院 A 的患者临时涌入导致最后一天数据集中的标本溢出（4000 个测试集）
计算机错误导致 4000 个测试集中的 800 多个测试集的结果在患者高峰当天被逆转。
如何找到被逆转的 800 个标本？
我尝试使用非混合日进行训练（不包括最后一天的数据）使用 xgboost，预测最后一天测试的 11 个测试结果。我试图找出实际值和预测值之间的差异，以找出具有多个异常值的样本。但 F1 分数很差。我还尝试使用 min-max 或 z-score 进行规范化，但仍然没有改善。我想我必须研究不同的方法。有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78940634/anomaly-detection-how-to-find-out-the-800-mixed-sample-on-the-last-day-of-the-d</guid>
      <pubDate>Mon, 02 Sep 2024 13:10:44 GMT</pubDate>
    </item>
    <item>
      <title>并行化原生单批次 PyTorch 模型</title>
      <link>https://stackoverflow.com/questions/78940523/paralellizing-a-natively-single-batch-pytorch-model</link>
      <description><![CDATA[是否可以并行化（原生）单批模型？
通常，并行化是通过 torch.bmm（批处理矩阵乘法）而不是 torch.matmul 来完成的，并且专门为批处理固定一个维度。但是，例如对于 torch.tensordot 函数，这不可用。
因此，如果有这样的模型，是否可以并行计算批处理的每个梯度？理想情况下，并行化应该适用于训练和推理。
代码示例：
import torch
import torch.nn as nn

class LinearMultidimModel(nn.Module):
def __init__(self, input_dim, output_dim):
super(LinearMultidimModel, self).__init__()
self.weight = nn.Parameter(torch.randn(input_dim, hidden_​​dim, output_dim))
self.bias = nn.Parameter(torch.randn(output_dim))

def forward(self, x):
# 使用 torch.tensordot 执行线性变换
out = torch.tensordot(x, self.weight, dims=[[0,1],[0,1]]) + self.bias
return out

# 示例用法
input_dim = 3
hidden_​​dim=2
output_dim = 1
model = LinearMultidimModel(input_dim, output_dim)

# 虚拟输入
x = torch.randn(input_dim, hidden_​​dim)# 但是如果我想放入一个批次，torch.randn(batch_size, input_dim, hidden_​​dim) 怎么办？
output = model(x)
print(output)

请记住，如果没有 hidden_​​dim，它会原生地进行并行化，可以完全删除 hidden_​​dim 并使用 获得结果
x = torch.randn(5, input_dim)。
我尝试过使用 Einsum，但它适用于固定数量的隐藏维度...]]></description>
      <guid>https://stackoverflow.com/questions/78940523/paralellizing-a-natively-single-batch-pytorch-model</guid>
      <pubDate>Mon, 02 Sep 2024 12:42:44 GMT</pubDate>
    </item>
    <item>
      <title>如果模型在训练集上表现正常但在验证集上表现异常，这意味着什么[关闭]</title>
      <link>https://stackoverflow.com/questions/78938961/what-does-it-mean-if-a-model-acts-normal-on-a-training-set-but-is-abnormal-on-va</link>
      <description><![CDATA[我尝试将堆叠在一起的 25x25 像素图像分类为 50x25 像素图像是相同 (1) 还是不同 (0)。我使用 keras 创建 NN 层。 Keras 顺序层如下所示：
layers.Input((2*imsize,imsize,3)), # 具有 3 个通道的输入形状
layers.Reshape((2,imsize,imsize,3)), # 将输入转换为两个 25x25 图像
layers.LayerNormalization(axis=[-1,-2,-3]), # 规范化图像
layers.Flatten(), # 展平数组
layers.Dense(16,activation=&#39;relu&#39;), # 16 个输出隐藏层
layers.Dense(2,activation=&#39;softmax&#39;) 

然后我使用 adam 优化器编译了这些层，损失和准确率如下：
ml.compile(optimizer=&#39;adam&#39;,
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
metrics=[&#39;accuracy&#39;])

之后，我使用 epoch=20 和 batch_size=100 训练模型。我根据 epoch 绘制了这些结果。
结果


当前评估：
我目前的观察是

模型过度拟合，因为它仅在训练集上表现正常？
模型是学习到了错误的东西，因为损失在验证集上不减反增

我的问题是：我对模型的评估正确吗？我应该如何理解这个结果以便改进它？
更新：
相同（1）的示例数据集：

不同（0）的示例数据集：
]]></description>
      <guid>https://stackoverflow.com/questions/78938961/what-does-it-mean-if-a-model-acts-normal-on-a-training-set-but-is-abnormal-on-va</guid>
      <pubDate>Mon, 02 Sep 2024 04:51:20 GMT</pubDate>
    </item>
    <item>
      <title>Yolov9 C++ 推理输出的 x、y、宽度和高度不符合预期</title>
      <link>https://stackoverflow.com/questions/78923447/yolov9-c-inference-outputs-for-x-y-width-and-height-not-as-expected</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78923447/yolov9-c-inference-outputs-for-x-y-width-and-height-not-as-expected</guid>
      <pubDate>Wed, 28 Aug 2024 12:54:32 GMT</pubDate>
    </item>
    <item>
      <title>无需深度学习或 Tesseract 的文本图像二元分类器</title>
      <link>https://stackoverflow.com/questions/78842184/text-image-binary-classifier-without-deep-learning-or-tesseract</link>
      <description><![CDATA[我有 20k 张小标签图像，每张图像都有单词“Back”或“Front”。
图像分辨率为全部 (200px, 25px)

我可以使用 tesseract_OCR 对这些图像进行 100% 准确率的分类。
 txt = pytesseract.image_to_string(img, lang=&#39;eng&#39;)
if &quot;Front&quot; in txt:
return &quot;Front&quot;
if &quot;Back&quot; in txt:
return &quot;Back&quot;

问题是，它太慢了（20k 张图像需要 1 小时）并且需要安装 OCR 包。
我知道即使是 3 层的简单 CNN 也能很好地完成它，但我认为这个问题似乎可以用简单的算法解决，而不需要复杂的技术。
你能给我推荐一种新方法吗？
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78842184/text-image-binary-classifier-without-deep-learning-or-tesseract</guid>
      <pubDate>Wed, 07 Aug 2024 06:46:36 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Core ML 中的 MLMultiArray 中设置正确的步幅？步幅的值是什么意思？</title>
      <link>https://stackoverflow.com/questions/72166348/how-to-set-right-strides-in-mlmultiarray-in-core-ml-whats-the-strides-values</link>
      <description><![CDATA[如何在 Core ML 中的 MLMultiArray 中设置正确的 strides？每个 strides 的值的含义是什么？
例如，假设 Core ML 模型输入 shape 为 (1, 3, 1280, 720)，那么在使用 initWithDataPointer:shape:dataType:strides:deallocator:error: 创建 MLMultiArray 对象时如何设置 strides？
我从一些网站将 strides 设置为 (720 * 1280, 720 * 1280, 720, 1)，但我不知道为什么这样设置，或者是否正确。 shape 和 strides 之间有什么联系。Apple 开发者网站上没有更多相关文档。]]></description>
      <guid>https://stackoverflow.com/questions/72166348/how-to-set-right-strides-in-mlmultiarray-in-core-ml-whats-the-strides-values</guid>
      <pubDate>Mon, 09 May 2022 02:07:53 GMT</pubDate>
    </item>
    <item>
      <title>如何将两个不同的训练过的 ML 模型合二为一？</title>
      <link>https://stackoverflow.com/questions/64801479/how-to-combine-two-different-trained-ml-models-as-one</link>
      <description><![CDATA[我根据两个不同的数据集训练了两个 ml 模型。然后我将它们保存为 model1.pkl 和 model2.pkl 。有两个用户输入（不是模型的输入数据），如 x=0 和 x=1，如果 x=0，我必须使用 model1.pkl 进行预测，否则我必须使用 model2.pkl 进行预测。我可以使用 if 条件来执行它们，但我的问题是我必须知道是否有可能将其保存回 model.pkl 包括此条件语句。如果我将它们组合并保存为模型，它将很容易在其他 IDE 中加载。]]></description>
      <guid>https://stackoverflow.com/questions/64801479/how-to-combine-two-different-trained-ml-models-as-one</guid>
      <pubDate>Thu, 12 Nov 2020 09:44:27 GMT</pubDate>
    </item>
    </channel>
</rss>