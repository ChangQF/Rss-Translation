<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 13 Jun 2024 06:21:16 GMT</lastBuildDate>
    <item>
      <title>Torch.unique() 的替代方案不会破坏梯度流吗？</title>
      <link>https://stackoverflow.com/questions/78615860/torch-unique-alternatives-that-do-not-break-gradient-flow</link>
      <description><![CDATA[在 Pytorch 梯度下降算法中，函数
def TShentropy(wf):
unique_elements, counts = wf.unique(return_counts=True)
entrsum = 0
for x in counts:
p = x/len_a #计算 x 的概率
entrsum-= p*torch.log2(p) #Shannon 熵公式 
return entrsum

使用方法 torch.unique()，该方法会破坏梯度流。每当我将其切换到连续概率计算（例如 torch.softmax()）时，程序就会运行。但是，该公式需要使用离散概率质量分布，而这不适用于 softmax。
我尝试使用 torch.nn. functional.one_hot 和 torch.bincount，两者都给出了相同的错误：
RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn

这注定会失败吗？我应该尝试以某种方式插入概率函数吗？]]></description>
      <guid>https://stackoverflow.com/questions/78615860/torch-unique-alternatives-that-do-not-break-gradient-flow</guid>
      <pubDate>Thu, 13 Jun 2024 04:48:39 GMT</pubDate>
    </item>
    <item>
      <title>Keras 库在 Google Colab 中不起作用</title>
      <link>https://stackoverflow.com/questions/78614179/keras-library-not-working-in-google-colab</link>
      <description><![CDATA[正在研究基于图像识别的 ML 模型。
以下代码行显示错误。
“from keras.utils import np_utils, to_categorical”
在 google colab 上工作
显示错误-
“无法从“keras.utils”（/usr/local/lib/python3.10/dist-packages/keras/utils/init.py）导入名称“np_utils”

注意：如果由于缺少包而导致导入失败，您可以
使用 !pip 或 !apt 手动安装依赖项。&quot;
虽然我已经使用了以下内容-
!pip install keras
导入最新版本
导入包并设置 numpy 随机种子
导入随机
将 numpy 导入为 np
从 keras.utils 导入 np_utils，to_categorical
从 keras_preprocessing.image 导入 load_img，img_to_array
从 os 导入listdir
from os.path import isdir, join
加载预洗牌的训练和测试数据集
(x_train, y_train), (x_test, y_test) = sign_language.load_data()
这些是前几行代码，错误出现在第三行。]]></description>
      <guid>https://stackoverflow.com/questions/78614179/keras-library-not-working-in-google-colab</guid>
      <pubDate>Wed, 12 Jun 2024 17:28:53 GMT</pubDate>
    </item>
    <item>
      <title>PySpark：具有 min_frequency 的 StringIndexer，类似 scikit-learn 的 OrdinalEncoder</title>
      <link>https://stackoverflow.com/questions/78613394/pyspark-stringindexer-with-min-frequency-like-in-scikit-learns-ordinalencoder</link>
      <description><![CDATA[我正在 PySpark 中构建一个机器学习管道，其中 StringIndexer 是其中一个阶段。问题是有些类别非常小，所以我希望将它们映射到同一个标签。使用 scikit-learn 中的 OrdinalEncoder 可以实现这一点。
我想我正在寻找一种方法来扩展 StringIndexer 类，但我无论如何也想不出如何做到这一点。我想我必须覆盖 fit（或 _fit）方法，但我甚至无法在源代码中找到它。欢迎提出其他建议。
这里有一个小例子：
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer

spark = SparkSession.builder.getOrCreate()

data = [
(&quot;a&quot;,),
(&quot;b&quot;,),
(&quot;a&quot;,),
(&quot;b&quot;,),
(&quot;c&quot;,),
(&quot;b&quot;,),
(&quot;b&quot;,),
(&quot;d&quot;,),
(&quot;d&quot;,),
(&quot;d&quot;,),
(&quot;e&quot;,)
]

data = spark.createDataFrame(data, [&quot;category&quot;])

indexer = StringIndexer(inputCol=&#39;category&#39;, outputCol=&#39;label&#39;)
categories_and_labels = indexer.fit(data).transform(data)


以上代码给出以下结果：
+--------+-----+
|category|label|
+--------+-----+
| a| 2.0|
| b| 0.0|
| a| 2.0|
| b| 0.0|
| c| 3.0|
| b| 0.0|
| b| 0.0|
| d| 1.0|
| d| 1.0|
| d| 1.0|
| e| 4.0|
+--------+-----+

我想要一个带有参数 minFrequency 的自定义类，我可以像下面这样使用：
indexer = CustomStringIndexer(inputCol=&#39;category&#39;, outputCol=&#39;label&#39;, minFrequency=3)
categories_and_labels = indexer.fit(data).transform(data)

预期结果将是
+--------+-----+
|category|label|
+--------+-----+
| a| 2.0|
| b| 0.0|
| a| 2.0|
| b| 0.0|
| c| 2.0|
| b| 0.0|
| b| 0.0|
| d| 1.0|
| d| 1.0|
| d| 1.0|
| e| 2.0|
+--------+-----+

我使用的是 Spark 版本 3.5]]></description>
      <guid>https://stackoverflow.com/questions/78613394/pyspark-stringindexer-with-min-frequency-like-in-scikit-learns-ordinalencoder</guid>
      <pubDate>Wed, 12 Jun 2024 14:43:04 GMT</pubDate>
    </item>
    <item>
      <title>预测性维护 [关闭]</title>
      <link>https://stackoverflow.com/questions/78612921/predictive-maintenance</link>
      <description><![CDATA[我从事预测性维护工作，其中有每小时的传感器数据，其中包含振动、压力等列。要求是预测机器何时会出现故障。
以下是数据示例：



ID
压力
振动
故障状态
时间




1
75.53
450.65
0
2023-03-01 00:30:00


2
143.54
543.40
1
2023-03-01 01:30:00



有了这些数据，我们需要预测接下来 10 个时间步骤的故障状态。
我不确定是否要使用时间序列分类模型或常规分类模型来解决这个问题。您能建议最好的方法吗？
我尝试使用 DES-SVM 方法双指数平滑和 SVM 模型来处理数据，但预测非常差。之后我做了一些特征工程，并浏览了一些 kaggle 笔记本以进一步了解，但解决方案并不令人满意。因为我需要预测接下来的 n 步，并告诉用户该机器可能在接下来的 12 小时内出现故障。但这种模型只能在时间序列中使用，但我不知道如何在时间序列中使用基于预测的分类模型]]></description>
      <guid>https://stackoverflow.com/questions/78612921/predictive-maintenance</guid>
      <pubDate>Wed, 12 Jun 2024 13:17:49 GMT</pubDate>
    </item>
    <item>
      <title>推荐系统 - 奇异值分解 (SVD) 提供随机结果</title>
      <link>https://stackoverflow.com/questions/78612024/recommender-system-singular-value-decomposition-svd-is-providing-random-resu</link>
      <description><![CDATA[有时，输出的质量仅通过目测来评估。查看下面提供的示例，如果我们使用简单的直觉，很明显预期的用户评分是 2 和 3（请参考空数据单元格）。
示例已简化 - 我使用了更大的数据集，但仍然获得了相同的结果。
表 1

用户 电影 1 电影 2 电影 3

1 - 3 4

2 2 3 4

3 2 3 4

表 2

用户 电影 1 电影 2 电影 3

1 - 3 3

2 3 3 3

3 3 3 3

ALS 模型提供了这些评分，但 SVD 模型却惨遭失败。有人知道这是为什么吗？为什么使用 SVD 进行用户预测结果时，我们会得到 2 和 3 以外的结果？]]></description>
      <guid>https://stackoverflow.com/questions/78612024/recommender-system-singular-value-decomposition-svd-is-providing-random-resu</guid>
      <pubDate>Wed, 12 Jun 2024 10:21:28 GMT</pubDate>
    </item>
    <item>
      <title>在实践代码中，它说 keras 层无效</title>
      <link>https://stackoverflow.com/questions/78611171/in-practice-code-it-is-saying-the-keras-layer-is-not-valid</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78611171/in-practice-code-it-is-saying-the-keras-layer-is-not-valid</guid>
      <pubDate>Wed, 12 Jun 2024 07:26:52 GMT</pubDate>
    </item>
    <item>
      <title>机器学习模型的输入</title>
      <link>https://stackoverflow.com/questions/78610947/input-to-machine-learning-model</link>
      <description><![CDATA[我训练了一个基于 bert 的模型，我已经研究了很长时间了。训练后，我在模型目录中得到了几个文件 - pytorch_model.bin、training_args.bin、merges.txt、vocab.json。现在我想通过向模型提供输入并检查其输出来测试模型。但我不知道该怎么做。
我试着在网上查找，有人建议我使用 Gradio。]]></description>
      <guid>https://stackoverflow.com/questions/78610947/input-to-machine-learning-model</guid>
      <pubDate>Wed, 12 Jun 2024 06:27:31 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 对象检测-数据集格式[关闭]</title>
      <link>https://stackoverflow.com/questions/78610630/tensorflow-object-detection-dataset-format</link>
      <description><![CDATA[我正在使用深度学习进行实时对象检测任务。哪种类型的 TensorFlow 数据集格式是强烈推荐的？我看到 TfRecord 格式和 tf.data.Dataset 格式使用最多。哪一个最好用？我的注释采用 PASCAL VOC（xml）格式。]]></description>
      <guid>https://stackoverflow.com/questions/78610630/tensorflow-object-detection-dataset-format</guid>
      <pubDate>Wed, 12 Jun 2024 04:26:15 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中对离散概率函数进行梯度下降编码？</title>
      <link>https://stackoverflow.com/questions/78610447/how-do-i-code-gradient-descent-over-a-discrete-probability-function-in-pytorch</link>
      <description><![CDATA[我正在尝试编写梯度下降算法，以最小化一维数组 X 和较小的一维数组 A 之间的卷积的香农熵，其中要优化的参数是 A 的条目。但是，要计算熵，我需要先计算分布的离散概率。但是，我相信这会破坏 PyTorch 内部的梯度计算。
这是我的损失函数：
def loss_function(A):
return Shentropy(F.conv1d(padded_input, A.unsqueeze(0).unsqueeze(0), padding=0))

def Shentropy(wf):
unique_elements, counts = wf.unique(return_counts=True)
entrsum = 0
for x in counts:
p = x/len(wf) #计算 x 的概率
entrsum-= p*torch.log2(p) #Shannon 熵公式 
return entrsum

但是，这给了我以下错误：
RuntimeError：张量的元素 0 不需要梯度，也没有grad_fn
我尝试将 wf.unique(return_counts=True) 与 wf.softmax(dim=0) 交换，代码确实以这种方式运行。但是，softmax 不适用于熵公式（给出错误的结果）。
有没有其他方法可以使其可微分，从而不破坏梯度或损害公式？或者我应该使用某种“离散梯度”？]]></description>
      <guid>https://stackoverflow.com/questions/78610447/how-do-i-code-gradient-descent-over-a-discrete-probability-function-in-pytorch</guid>
      <pubDate>Wed, 12 Jun 2024 02:58:28 GMT</pubDate>
    </item>
    <item>
      <title>实现神经网络的岭回归方程</title>
      <link>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</link>
      <description><![CDATA[我试图在 MATLAB 中复制以下方程，以使用岭回归训练找到神经网络的最佳输出权重矩阵。
使用岭回归训练后的神经网络输出权重矩阵：

此方程来自 Mantas Lukosevicius 提供的回声状态网络指南，可在以下位置找到：https://www.researchgate.net/publication/319770153_A_practical_guide_to_applying_echo_state_networks（见第 11 页）
我的尝试如下。我认为外括号（红色）使其成为非传统的双重求和，这意味着 Voss 提出的方法（见 https://www.mathworks.com/matlabcentral/answers/1694960-nested-loops-for-double-summation）无法遵循。请注意，y_i 是一个 T x 1 向量，而 y_i_target 也是一个 T x 1 向量。Wout_i 是一个 N x 1 向量，其中 N 是神经网络中的节点数。我为每个 i^th 目标训练信号生成三个 Ny x 1 向量 Wout_i,y_i,y_i_target，其中 Ny 是训练信号的数量。Wout 的最终输出是一个 N x 1 向量，其中向量中的每个元素都是网络中每个节点的最佳权重。
N = 100; % 神经网络节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度 
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
outer_sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 针对每个第 i 个目标训练信号收集的每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
inner_sum = sum(((y_i&#39;-y_i_target).^2)+reg*norm(Wout_i)^2);
outer_sum(i) = inner_sum;
end
outer_sum = outer_sum.*(1/Ny);
[minval, minidx] = min(outer_sum);
Wout = cell2mat(Wouts(minidx));

我对 Wout 的最终答案是 N 乘以 1，正如它应该的那样，但我对我的答案不确定。我特别不确定我是否正确地完成了关于 Wout 操作的双重求和和 arg min。有什么方法可以验证我的答案吗？
替代方法：
我尝试了另一种方法/尝试，如下所示：
N = 100; % 神经网络中的节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
Sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 为每个第 i 个目标训练信号收集每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
Sum(i) = (1/T)*sum((y_i&#39;-y_i_target).^2);
end
[minval, minidx] = min(Sum);
Wout = cell2mat(Wouts(minidx));

我相信这次尝试比第一次更好，但我不确定它是否仍然正确。]]></description>
      <guid>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</guid>
      <pubDate>Sat, 08 Jun 2024 22:31:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Flutter 中使用 flutter_tesseract_ocr 和 google_mlkit_text_recognizer 以行格式提取文本？</title>
      <link>https://stackoverflow.com/questions/78591825/how-to-extract-text-in-row-wise-format-using-flutter-tesseract-ocr-and-google-ml</link>
      <description><![CDATA[我正在开发一个 Flutter 项目，需要对图像/PDF 执行文本识别。我想比较使用 flutter_tesseract_ocr 和 google_mlkit_text_recognizer 的结果。我在 Flutter 应用中集成这两个库时遇到了麻烦，需要一些指导。我想按行获取数据。我已附加图像以供识别。
这是 flutter_tesseract_ocr 输出：-
在此处输入图像说明
这是 google mlkit 文本识别器输出：- 在此处输入图像说明
我尝试过的方法
我查看了这两个库的文档，但找不到有关按行格式化输出文本的具体信息。
我也在网上搜索过例子，但没有找到适合我的情况的解决方案。输入图像在此处输入图像描述在此处输入描述
我的期望
我期望以行格式获取识别的文本，这意味着图像中的每一行文本都应单独打印或存储，以便于进一步处理。
实际发生了什么
google_mlkit_text_recognizer：文本被识别，但我不确定如何按行格式化。输出是每个块内逐行的，但我需要确保它在整个图像中是逐行的。
flutter_tesseract_ocr：文本被识别为单个字符串，很难区分行。
具体问题
如何使用 google_mlkit_text_recognizer 按行格式化识别的文本？
有没有办法使用 flutter_tesseract_ocr 按行格式化文本输出？
是否有任何最佳实践或其他步骤可确保以行方式正确识别和格式化文本？
其他信息
Flutter 版本：3.19.1
Dart SDK 版本：3.3.0
google_mlkit_text_recognizer 版本：最新
flutter_tesseract_ocr 版本：最新
平台：Android/iOS]]></description>
      <guid>https://stackoverflow.com/questions/78591825/how-to-extract-text-in-row-wise-format-using-flutter-tesseract-ocr-and-google-ml</guid>
      <pubDate>Fri, 07 Jun 2024 12:06:38 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 Elixir Nx/Schorar 中进行 ELISA 分析？</title>
      <link>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</link>
      <description><![CDATA[我已阅读 Medium 上的文章 ELISA Analysis in Python。
上述文章使用 SciPy 的 curve_fit 函数根据 4 参数逻辑回归 (4PL) 模型找到近似曲线，如下所示：
from scipy.optimize import curve_fit

x = [1.95, 3.91, 7.381, 15.63, 31.25, 62.5, 125,250, 500, 1000]
y = [0.274, 0.347, 0.392, 0.420, 0.586, 1.115, 1.637, 2.227, 2.335, 2.372]

def log4pl(x, A, B, C, D):
return(((A - D) / (1.0 + ((x / C) ** B))) + D)

params, _ = curve_fit(log4pl, x, y)
A, B, C, D = params[0], params[1], params[2], params[3]

我想使用 Nx/Scholar 库。
可能吗？如果您能给我任何提示，我将不胜感激。

[更新]
快速浏览一下 Python scipy.optimize 源代码，似乎 curve_fit 在内部使用了 Fortran 的 MINPACK 库。
据我所知，没有简单的方法可以从 Elixir 使用 MINPACK。
因此，我得出结论，目前在 Elixir 中进行 ELISA 分析很困难。
欢迎提供任何其他信息。]]></description>
      <guid>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</guid>
      <pubDate>Sun, 02 Jun 2024 04:29:18 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv8n PyTorch 与转换后的 CoreML 模型之间的对象检测性能不一致</title>
      <link>https://stackoverflow.com/questions/77541935/inconsistent-object-detection-performance-between-yolov8n-pytorch-and-converted</link>
      <description><![CDATA[我发现，YOLOv8n 对象检测模型在其原始 PyTorch 格式 (.pt) 和将其转换为 CoreML 格式以在 iOS 设备上部署后之间存在显著的性能差异。原始模型在自定义数据集上训练，成功检测了给定图像中的对象。但是，转换后的 CoreML 模型无法在同一图像中检测到任何对象。
我在其他一些图像中进行了测试。虽然它可以在 iOS 和 Mac 设备中检测到对象，但其性能与原始 .pt 检测不同。
详细信息：
原始模型：YOLOv8n，使用 Ultralytics 的实现在自定义数据集上进行训练。在 CoLap 中使用 A100。
转换工具：
!pip install coremltools

from ultralytics import YOLO
model_path=f&quot;{HOME}/runs/detect/train/weights/best.pt&quot;
model=YOLO(model_path)
model.export(format=&#39;coreml&#39;, nms=True)

问题：

YOLOv8n 中是否存在已知与 CoreML 存在兼容性问题的特定层或操作？

调试原始模型和转换后的模型之间对象检测性能差异的推荐步骤是什么？


对进一步故障排除有任何见解或建议吗？

]]></description>
      <guid>https://stackoverflow.com/questions/77541935/inconsistent-object-detection-performance-between-yolov8n-pytorch-and-converted</guid>
      <pubDate>Fri, 24 Nov 2023 09:09:33 GMT</pubDate>
    </item>
    <item>
      <title>神经网络预测变成直线</title>
      <link>https://stackoverflow.com/questions/72775077/neural-network-prediction-becomes-a-straight-line</link>
      <description><![CDATA[我实现了一个两层神经网络（根据 Kolmogorov-Arnold 定理，这足以表示 n 个变量的任何非线性函数）来预测时间序列。然而，在神经网络的末端，收到的预测的波动性下降到几乎为零，并变成一条直线（我附上了预测屏幕和神经网络的源代码）。我增加了隐藏层中的神经元数量、时期数、训练样本的大小、学习率，改变了训练样本数据的规范化范围，改变了初始权重的范围。但都无济于事。训练样本的大小为 336 个示例，训练方法是误差的反向传播，规范化方法是极小极大。此外，当使用双曲正切作为激活函数时，情况有所改善，但图形看起来也很奇怪。ReLU 输出“直接预测”。有人对这个问题有什么想法吗？
import random
import sys
import numpy
import math

eta=0.0001 #学习率
n=200 #训练周期数。还有 500、1000、5000
inp=30 #输入层大小
m=60 #隐藏层大小
y=0 #输出信号
t=0 #目标信号
e=0 #错误
d_y=0 #最后一个神经元的局部梯度
err=0 #计算输出神经元的网络误差
err_av=0 #平均网络误差
path=&#39;dataTrain.txt&#39; #训练样本
path2=&#39;dataLaunch.txt&#39; #启动预测
day = 365 #预测天数
...

其余内容在网站上：https://ideone.com/vV2QW6
屏幕截图（激活函数 - sigmoid）：https://ibb.co/GHrTGLr
屏幕截图（激活函数 - 双曲正切）：https://ibb.co/WHFX3Sc
感谢关注。]]></description>
      <guid>https://stackoverflow.com/questions/72775077/neural-network-prediction-becomes-a-straight-line</guid>
      <pubDate>Mon, 27 Jun 2022 15:59:46 GMT</pubDate>
    </item>
    <item>
      <title>如何将生成的数据转换为 Pandas 数据框</title>
      <link>https://stackoverflow.com/questions/67266153/how-to-convert-generated-data-into-pandas-dataframe</link>
      <description><![CDATA[from sklearn.datasets import make_classification
df = make_classification(n_samples=10000, n_features=9, n_classes=1, random_state = 18,
class_sep=2, n_informative=4)

创建数据后。它是元组，将元组转换为 pandas 数据框后
 df = pd.DataFrame(data, columns=[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;,&quot;7&quot;,&quot;8&quot;,&quot;9&quot;])

所以我得到了 9 个特征（列），但是当我尝试插入 9 个列时，它显示。

ValueError：传递值的形状为 (2, 1)，索引暗示 (2, 9)

基本上我想生成数据并将其转换为 pandas 数据框，但无法获取它。
错误是：]]></description>
      <guid>https://stackoverflow.com/questions/67266153/how-to-convert-generated-data-into-pandas-dataframe</guid>
      <pubDate>Mon, 26 Apr 2021 11:54:24 GMT</pubDate>
    </item>
    </channel>
</rss>