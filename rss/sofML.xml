<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 03 Feb 2024 00:57:48 GMT</lastBuildDate>
    <item>
      <title>如何修复一层的输出，使其与另一层兼容以构建我的张量流模型？</title>
      <link>https://stackoverflow.com/questions/77930107/how-do-i-fix-the-output-of-one-layer-so-it-is-compatible-to-another-layer-to-bui</link>
      <description><![CDATA[我的输入是由 21 个氨基酸标签编码的序列。它是一个包含 21 个元素 (1x21) 的数组。我想通过嵌入层然后通过卷积层（等等）将其提供给它。但它不允许我添加卷积层。我明白为什么（输出和输入的尺寸不匹配），但我不知道如何修复它。
这是我的代码：
&lt;前&gt;&lt;代码&gt;embeded_vector_size = 9
最大长度 = 21
vocab_size = len(标签)
模型=顺序（）
model.add（嵌入（vocab_size，embeded_vector_size，input_length = max_length，name =“嵌入”））
model.add(Conv2D(filters=64,activation=&#39;relu&#39;, kernel_size=(10, 3), input_shape=(21, 9, 1)))

我收到此错误：层“conv2d_17”的输入 0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、21、9）
ValueError Traceback（最近一次调用最后一次）
[177] 中的单元格，第 7 行
      5 model.add(Embedding(vocab_size,embeded_vector_size,input_length=max_length, name=“embedding”))
      6 打印(模型.summary())
----&gt; 7 model.add(Conv2D(filters=3,activation=&#39;relu&#39;, kernel_size=(10, 3), input_shape=(21, 9, 1)))
      8 # 打印(模型.summary())
      9 # 模型.add(MaxPooling2D())

文件 /opt/conda/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204，在 no_automatic_dependency_tracking.._method_wrapper(self, *args, **kwargs)
    第202章
    203 尝试：
--&gt; [第 204 章]
    205 最后：
    206 self._self_setattr_tracking = previous_value # pylint：禁用=受保护访问

文件 /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70，位于filter_traceback..error_handler(*args, **kwargs)
     67、filtered_tb = _process_traceback_frames（e.__traceback__）
     68 # 要获取完整的堆栈跟踪，请调用：
     69 # `tf.debugging.disable_traceback_filtering()`
---&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
     71 最后：
     72 删除filtered_tb

文件/opt/conda/lib/python3.10/site-packages/keras/src/engine/input_spec.py:253，在assert_input_compatibility(input_spec、inputs、layer_name)中
    第251章
    [252] 第252话规格.min_ndim:
--&gt;第253章
    254 f&#39;层“{layer_name}”的输入{input_index} &#39;
    255、“与层不兼容：”
    [256] 第256话
    257 f”发现 ndim={ndim}。 ”
    258 f“接收到完整形状：{tuple(shape)}”
    第259章）
    260 # 检查数据类型。
    第261章

ValueError：层“conv2d_17”的输入 0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、21、9）

我尝试过在线学习等，但我很困惑，不知道如何继续。]]></description>
      <guid>https://stackoverflow.com/questions/77930107/how-do-i-fix-the-output-of-one-layer-so-it-is-compatible-to-another-layer-to-bui</guid>
      <pubDate>Fri, 02 Feb 2024 22:14:15 GMT</pubDate>
    </item>
    <item>
      <title>如何调整我的Python代码来安装最新的tensorflow对象检测包</title>
      <link>https://stackoverflow.com/questions/77929860/how-can-i-adapt-my-python-code-to-install-the-latest-tensorflow-object-detection</link>
      <description><![CDATA[我一直在关注这个并且已经解决了到视频的训练和检测部分。但是，我在安装各种不同的库（包括tensorflow和matplotlib）时遇到了一些问题。
主要错误始终相同：https://github 找到讨论.com/pypa/pip/issues/12330
我尝试了它引导我访问的 github 链接，但该链接没有提到我的问题的任何解决方案。我还尝试浏览不同的论坛并向更有经验的人寻求帮助，但他们也不知道。需要注意的是，只有当我在虚拟环境中时才会出现此错误。当我不在 venv 内时，我能够完美地卸载并重新安装tensorflow。
在线研究后，我发现我在代码中使用的张量流研究模型已被弃用，我应该使用类似于 这个。
我的原始代码如下所示：
# 安装 Tensorflow 对象检测
从 setuptools 导入 find_packages
从 setuptools 导入设置

如果 os.name==&#39;posix&#39;:
    !apt-get 安装 protobuf 编译器
    !cd Tensorflow/models/research &amp;&amp;协议 object_detection/protos/*.proto --python_out=. &amp;&amp; cp object_detection/packages/tf2/setup.py 。 &amp;&amp; python -m pip install .
    
如果 os.name==&#39;nt&#39;:
    url =“https://github.com/protocolbuffers/protobuf/releases/download/v3.15.6/protoc-3.15.6-win64.zip”
    wget.download(url)
    !move protoc-3.15.6-win64.zip {paths[&#39;PROTOC_PATH&#39;]}
    !cd {paths[&#39;PROTOC_PATH&#39;]} &amp;&amp; tar -xf protoc-3.15.6-win64.zip
    os.environ[&#39;PATH&#39;] += os.pathsep + os.path.abspath(os.path.join(paths[&#39;PROTOC_PATH&#39;], &#39;bin&#39;))
    !cd Tensorflow/models/blob &amp;&amp;协议 object_detection/protos/*.proto --python_out=. &amp;&amp;复制 object_detection\\packages\\tf2\\setup.py setup.py &amp;&amp; python setup.py build &amp;&amp; python setup.py 安装



    REQUIRED_PACKAGES = [
        # 带有 PY3 的 apache-beam 需要
        &#39;avro-python3&#39;,
        &#39;阿帕奇光束&#39;,
        &#39;枕头&#39;，
        &#39;lxml&#39;,
        &#39;matplotlib&#39;,
        &#39;赛通&#39;,
        &#39;contextlib2&#39;,
        &#39;tf-苗条&#39;,
        &#39;六&#39;，
        &#39;pycocotools&#39;,
        &#39;利维斯&#39;,
        &#39;scipy&#39;,
        &#39;熊猫&#39;,
        &#39;tf-models-official&gt;=2.5.1&#39;,
        &#39;张量流_io&#39;,
        &#39;喀拉斯&#39;,
        &#39;pyparsing==2.4.7&#39;, # TODO(b/204103388)
        &#39;sacrebleu&lt;=2.2.0&#39; # https://github.com/mjpost/sacrebleu/issues/209
    ]

    设置（
        name=&#39;object_detection&#39;,
        版本=&#39;0.1&#39;,
        install_requires=REQUIRED_PACKAGES,
        include_package_data=真，
        包=（
            [p for p in find_packages() if p.startswith(&#39;object_detection&#39;)] +
            find_packages(where=os.path.join(&#39;.&#39;, &#39;slim&#39;))),
        包目录={
            &#39;数据集&#39;: os.path.join(&#39;slim&#39;, &#39;数据集&#39;),
            &#39;网&#39;: os.path.join(&#39;slim&#39;, &#39;网&#39;),
            &#39;预处理&#39;: os.path.join(&#39;slim&#39;, &#39;预处理&#39;),
            &#39;部署&#39;: os.path.join(&#39;slim&#39;, &#39;部署&#39;),
            &#39;脚本&#39;: os.path.join(&#39;slim&#39;, &#39;脚本&#39;),
        },
        description=&#39;Tensorflow 对象检测库&#39;,
        python_requires=&#39;&gt;3.6&#39;,
    ）
    
    !cd Tensorflow/models/research/slim &amp;&amp; pip install -e 。

wget 等模块和其他先决条件能够成功包含。
如何调整此代码以更好地匹配最新的张量流更新。如果这个问题晦涩难懂或者已经在堆栈溢出深处的某个地方得到了解答，请链接我。
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/77929860/how-can-i-adapt-my-python-code-to-install-the-latest-tensorflow-object-detection</guid>
      <pubDate>Fri, 02 Feb 2024 21:06:44 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试使用 Gemini-Pro 制作 LLM 聊天机器人，但遇到类型错误，不知道为什么？</title>
      <link>https://stackoverflow.com/questions/77929619/im-trying-to-make-a-llm-chatbot-using-gemini-pro-and-im-getting-a-type-error-a</link>
      <description><![CDATA[这是代码
导入操作系统

将streamlit导入为st
从 dotenv 导入 load_dotenv
将 google.generativeai 导入为 gen_ai

加载_dotenv()

st.set_page_config(
    page_title=&quot;与 Gemini Pro 聊天&quot;,
    page_icon=&quot;:大脑:&quot;,
    布局=“居中”
）

GOOGLE_API_KEY = os.getenv(“GOOGLE_API_KEY”)
gen_ai.configure(api_key=GOOGLE_API_KEY)
模型 = gen_ai.GenerativeModel(“gemini-pro”)

deftranslate_role_for_streamlit(user_role):
  如果 user_role == “模型”：
    返回“助手”
  别的：
    返回用户角色

如果“聊天会话”是不在 st.session_state 中：
  st.session_state.chat_session = model.start_chat(history=[])
st.title(“CAIE 机器人”)

对于 st.session_state.chat_session.history 中的消息：
  与 st.chat_message(translate_role_for_streamlit(message.role))：
    st.markdown(message.parts[0].text)

user_prompt = st.chat_input(“询问 CAIE 机器人...”)
如果用户提示：
  st.chat_message(“用户”).markdown(user_prompt)
  gemini_response = st.session_state.chat_session.send_message(user_prompt)
  与 st.chat_message(“助理”)：
    st.markdown(gemini_response.text)

!streamlit 运行 main.py

我在线路中遇到错误
如果“chat_session”是不在 st.session_state 中：

我想检查用户是否与机器人进行了活跃的聊天，如果是，那么机器人将保存之前的对话历史记录以供下一次对话使用。会话结束后，历史记录将重置]]></description>
      <guid>https://stackoverflow.com/questions/77929619/im-trying-to-make-a-llm-chatbot-using-gemini-pro-and-im-getting-a-type-error-a</guid>
      <pubDate>Fri, 02 Feb 2024 20:09:24 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch：在一个时期内的一批中出现梯度爆炸和 NaN 损失</title>
      <link>https://stackoverflow.com/questions/77929468/pytorch-exploding-gradient-and-nan-loss-at-exactly-one-batch-in-the-epoch</link>
      <description><![CDATA[摘要：
我正在尝试训练一个机器学习模型，在训练过程中，我在该纪元中的一个批次上不断收到 NaN（无限）损失，而所有其他批次都会产生合理的损失（通过跳过参数更新）该特定批次）。
&lt;前&gt;&lt;代码&gt;3027
losstensor(2.9372, device=&#39;cuda:0&#39;, grad_fn=)
3028
losstensor(2.9796, device=&#39;cuda:0&#39;, grad_fn=)
3029
losstensor(2.9189, device=&#39;cuda:0&#39;, grad_fn=)
3030
losstensor(2.9621, device=&#39;cuda:0&#39;, grad_fn=)
3031
losstensor(3.3539, device=&#39;cuda:0&#39;, grad_fn=)
3032
losstensor(2.8540, device=&#39;cuda:0&#39;, grad_fn=)
3033
losstensor(inf, device=&#39;cuda:0&#39;, grad_fn=) &lt;---------------------------------------- ----
3034
losstensor(3.0785, device=&#39;cuda:0&#39;, grad_fn=)
3035
losstensor(2.9671, device=&#39;cuda:0&#39;, grad_fn=)
3036
losstensor(2.9451，device=&#39;cuda:0&#39;，grad_fn=)
3037
losstensor(3.0042, device=&#39;cuda:0&#39;, grad_fn=)
3038


通过调整不同的学习率，产生 NaN 损失的批次索引保持不变

通过调整批次大小，仍然总会有一个批次产生 NaN 损失，尽管现在它是包含不同数据样本的不同批次索引


一些特定于我的模型的信息：

使用 AdamW 优化器和 CTC 损失训练的变压器

我的数据集样本在分组之前会被打乱


导致此问题的原因可能是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77929468/pytorch-exploding-gradient-and-nan-loss-at-exactly-one-batch-in-the-epoch</guid>
      <pubDate>Fri, 02 Feb 2024 19:36:21 GMT</pubDate>
    </item>
    <item>
      <title>专家混合与专家选择</title>
      <link>https://stackoverflow.com/questions/77929272/mixture-of-experts-with-choice-of-expert</link>
      <description><![CDATA[我有一个 keras 神经网络分类器。
该模型将形状为 (batch_size, 20, 10) 的张量作为输入，并执行二元分类。
我想提高分类精度，并且我认为专家混合 (MoE) 机制可以帮助我实现更好的精度。
所以我尝试创建多个模型（专家），然后使用 MoE 机制选择最好的 K 个专家来处理批次的每个 (20, 10) 输入。
我的想法是实现一个行为类似于路由器的 keras 层。
类似这样的事情：
from keras.layers import Layer

MixtureOfExperts 类（层）：
    def __init__(self, 专家: list[Layer], **kwargs):
        super().__init__(**kwargs)
        self.experts = 专家
        ...
    ...


该层的行为应类似于路由器。当对一批输入调用该层时，每个输入应由 K 个最佳专家处理。
如何完成此类的代码以实现所需的行为？假设每个专家都有相同的输出形状]]></description>
      <guid>https://stackoverflow.com/questions/77929272/mixture-of-experts-with-choice-of-expert</guid>
      <pubDate>Fri, 02 Feb 2024 18:57:45 GMT</pubDate>
    </item>
    <item>
      <title>准确性缺失：R 中的随机森林</title>
      <link>https://stackoverflow.com/questions/77929219/accuracy-missing-random-forest-in-r</link>
      <description><![CDATA[我想执行重复（100 次）5 倍交叉验证过程。之后，从结果中获得平均分类精度和OOB误差。
我已经尝试了下面的代码，看起来很简单：
# 安装并加载必要的包
库（插入符号）
库（随机森林）

# 加载数据集
数据 &lt;- turismo_rf

# 设置种子以实现可重复性
设置.种子(222)

# 定义交叉验证的控制参数
ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, # 重复交叉验证
                     number = 5, # 折叠次数
                     Repeats = 100) # 重复次数

# 使用插入符的训练函数训练随机森林模型
rf_model &lt;- train(Membership. Degree ~ P.165 + P.105 + Museum.heritage.密度 +
                    农场.企业 + 历史.绿色.密度 + 城市.绿色.分散 + 文化.企业.工人 +
                    耕地面积+城市面积，
                  数据=火车，
                  method = &quot;rf&quot;, # 随机森林方法
                  trControl = ctrl) # 交叉验证控制

# 打印训练好的随机森林模型
打印（rf_model）

# 从模型中提取混淆矩阵
conf &lt;- rf_model$finalModel$confusion

# 将 OOB 误差计算为 1 - 准确度
oob_error &lt;- 1 - (sum(diag(conf)) / sum(conf))

# 打印OOB错误
打印（oob_错误）

但是输出是：
出了点问题；缺少所有准确度指标值：
    准确度卡帕值
 分钟。 ：NA 最小值。 : 不适用
 第一季度：不适用 第一季度：不适用
 中位数 : NA 中位数 : NA
 平均值：NaN 平均值：NaN
 第三季度：不适用 第三季度：不适用
 最大限度。 ：不适用 最大。 : 不适用
 不适用 :3 不适用 :3

我被困在这里很长一段时间，我不明白我应该集中在哪里来解决这个问题。
我应该完全重构代码吗？或者我应该添加一些缺失或不正确的部分？
我甚至尝试过这个更简单的代码：
库（随机森林）
库（数据集）
库（插入符号）

数据 &lt;- turismo_rf

设置.种子(222)
ind &lt;- 样本(2, nrow(数据), 替换 = TRUE, prob = c(0.7, 0.3))
火车 &lt;- 数据[ind==1,]
测试 &lt;- 数据[ind==2,]

rf &lt;- randomForest(会员.度 ~ P.165 + P.105 + 博物馆.遗产.密度 +
                     农场.企业 + 历史.绿色.密度 + 城市.绿色.分散 + 文化.企业.工人 +
                     耕地.区域 + 城市.区域，数据=火车，邻近度=TRUE）
打印（射频）
]]></description>
      <guid>https://stackoverflow.com/questions/77929219/accuracy-missing-random-forest-in-r</guid>
      <pubDate>Fri, 02 Feb 2024 18:43:54 GMT</pubDate>
    </item>
    <item>
      <title>即使两个句子相同，图像字幕模型的 CIDEr 评估指标也会输出 0.0</title>
      <link>https://stackoverflow.com/questions/77927272/cider-evaluation-metric-for-image-captioning-model-outputs-0-0-even-if-both-sent</link>
      <description><![CDATA[从 pycocoevalcap.cider.cider 导入 Cider
导入 json

Reference_file = &#39;/content/ref.json&#39;
候选文件 = &#39;/content/preds.json&#39;

将 open(reference_file, &#39;r&#39;) 作为 f：
    引用 = json.load(f)

以 open(candidate_file, &#39;r&#39;) 作为 f：
    候选人 = json.load(f)
打印（候选人）
# 创建 CIDEr 记分器
cider_scorer = 苹果酒()

# 计算 CIDEr 分数
cider_score, cider_scores = cider_scorer.compute_score(参考文献, 候选人)

# 打印 CIDEr 分数
print(&quot;CIDEr 分数：&quot;, cider_score)`

它给出输出 0.0。]]></description>
      <guid>https://stackoverflow.com/questions/77927272/cider-evaluation-metric-for-image-captioning-model-outputs-0-0-even-if-both-sent</guid>
      <pubDate>Fri, 02 Feb 2024 13:18:02 GMT</pubDate>
    </item>
    <item>
      <title>Rust 中的神经网络针对 ReLu 和 SoftMax 收敛到零</title>
      <link>https://stackoverflow.com/questions/77927151/neural-network-in-rust-converging-to-zeros-for-relu-and-softmax</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77927151/neural-network-in-rust-converging-to-zeros-for-relu-and-softmax</guid>
      <pubDate>Fri, 02 Feb 2024 12:58:20 GMT</pubDate>
    </item>
    <item>
      <title>KerasRL：没有名为“keras.utils.generic_utils”的模块</title>
      <link>https://stackoverflow.com/questions/77925862/kerasrl-no-module-named-keras-utils-generic-utils</link>
      <description><![CDATA[在 Python 3.10.10 中，我尝试遵循 https: //github.com/keras-rl/keras-rl，但是当我这样做时，我收到错误：
ModuleNotFoundError：没有名为“keras.utils.generic_utils”的模块

通过运行“dqn_cartpole.py”中的源代码一行一行，我发现导入了
从 rl.agents.dqn 导入 DQNAgent

触发此错误。
我尝试用 KerasRL2 替换 KerasRL，但这样做会产生另一个错误：
导入错误：无法从“tensorflow.keras”导入名称“__version__”

我不知道为什么会发生这个错误，但我认为 KerasRL 的安装有问题，尽管我遵循了官方文档。有什么帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/77925862/kerasrl-no-module-named-keras-utils-generic-utils</guid>
      <pubDate>Fri, 02 Feb 2024 09:22:02 GMT</pubDate>
    </item>
    <item>
      <title>我自己的 MNIST 数据集神经网络的成本函数没有减少</title>
      <link>https://stackoverflow.com/questions/77925060/cost-function-not-decreasing-for-my-own-neural-network-for-mnist-dataset</link>
      <description><![CDATA[类 MLP():
    
    def __init__(自身、输入节点、隐藏节点、输出节点):
        self.input_nodes = input_nodes
        self.hidden_​​nodes = 隐藏节点
        self.output_nodes = 输出节点
        
        
        self.input_weights = np.random.randn(self.hidden_​​nodes, self.input_nodes) / np.sqrt(self.input_nodes)
        self.hidden_​​weights = np.random.rand(self.output_nodes, self.hidden_​​nodes) / np.sqrt(self.hidden_​​nodes)
        #self.hidden_​​bias = np.random.rand(self.output_nodes, 1) - 0.5
        #self.input_bias = np.random.rand(self.hidden_​​nodes, 1) - 0.5
        self.hidden_​​bias = np.zeros((self.output_nodes, 1))
        self.input_bias = np.zeros((self.hidden_​​nodes, 1))
         
        
    def feed_forward(自身,X):
        
        self.Z1 = np.dot(self.input_weights, X) + self.input_bias
        self.A1 = self.sigmoid(self.Z1)
        self.Z2 = np.dot(self.hidden_​​weights, self.A1) + self.hidden_​​bias
        输出 = softmax(self.Z2)
        
        返回
        
        
        
    def sigmoid(自身, x):
        返回 np.where(x &gt;= 0,
                    1 / (1 + np.exp(-x)),
                    np.exp(x) / (1 + np.exp(x)))
        
    def cross_entropy(self, y_true, y_pred):
        m = y_true.shape[1]
        log_likelihood = -np.sum(np.log(y_pred) * y_true) / m
        返回对数似然
        
        
    def backprop（自身，inputs_list，targets_list）：
        
        m = 目标列表大小
        输出 = self.feed_forward(inputs_list)
        输出错误 = 输出 - 目标列表
        打印（self.cross_entropy（targets_list，outs））
        soft_error = 出局数 * (1-出局数)
        dz2 = 输出错误 * 软错误
        dw2 = 1/m * np.dot(dz2, self.A1.T)
        db2 = 1/m * dz2
        da1 = np.dot(self.hidden_​​weights.T, dz2)
        dz1 = da1 * 自身.A1*(1-自身.A1)
        dw1 = 1/m * np.dot(dz1, input_list.T)
        db1 = 1/m * dz1
        
        
        
        返回 dw1、dw2、db1、db2
    
    
    def get_predictions(self, X2):
        返回 np.argmax(X2, 0)
    
    
    def 火车（自我，inputs_list，targets_list，epochs = 100，lr = 0.001）：
        
        对于范围内的 i（纪元）：
            #print(“迭代：” + str(i))
            
            
            input_weights，hidden_​​weights，input_bias，hidden_​​bias = self.backprop（inputs_list，targets_list）
            self.input_weights = self.input_weights - (lr*input_weights)
            self.hidden_​​weights = self.hidden_​​weights - (lr*hidden_​​weights)
            self.input_bias = self.input_bias - (lr*input_bias)
            self.hidden_​​bias = self.hidden_​​bias - (lr*hidden_​​bias)
            
        输出 = self.feed_forward(inputs_list)
            
        返回 self.get_predictions(输出)
        
        
        
        
        
        
        

        
    
        
        
        
    



这就是我传递来训练神经网络的内容。为了测试我的神经网络，我训练了1000 个示例。 X_training 只是从 mnist 数据集中读取 train.csv。
X_data = np.array(X_training).T
X_train = X_data[1:785]
X_train.shape



X_train_1 = X_train[:, 0:1000]
X_train_1.shape


Y_train = X_data[0]
Y_targs = np.eye(10)[Y_train]

Y_targs = Y_targs.T
Y_targs.shape


Y_targs_1 = Y_targs[:, 0:1000]
Y_targs_1.shape


c = MLP(784, 100, 10) # 创建具有 784 个输入节点、100 个隐藏层和 10 个输出节点的 MLP 对象。

b = c.train(X_train_1, Y_targs_1, 500, 0.01) # 这给出了成本函数的输出，其永远不会减少



正如您所看到的，在训练 mnist 数据集时，我的成本函数没有减少。它适用于成本函数起作用的一个示例，但是，当存在一个或多个示例时，成本永远不会下降。我想知道我的反向传播方法是否有问题，但看起来没问题？我使用 scipy 库中的 softmax。我将不胜感激任何建议！我基本上对每个输出都进行了一次热编码，因此我的尺寸看起来也正确。
&lt;前&gt;&lt;代码&gt;
9.247459184921587
9.247459131086748
9.24745907725312
9.247459023420703
9.247458969589495
9.247458915759498
9.247458861930712
9.247458808103138
9.247458754276773
9.24745870045162
9.247458646627678
9.247458592804948
9.247458538983429
9.247458485163119
9.247458431344022
9.247458377526135
9.24745832370946
9.247458269893995
9.247458216079744


]]></description>
      <guid>https://stackoverflow.com/questions/77925060/cost-function-not-decreasing-for-my-own-neural-network-for-mnist-dataset</guid>
      <pubDate>Fri, 02 Feb 2024 06:29:54 GMT</pubDate>
    </item>
    <item>
      <title>OpenPCDet 与 Windows 兼容还是仅与 Linux 兼容？</title>
      <link>https://stackoverflow.com/questions/77924600/is-openpcdet-compatible-with-windows-or-only-linux</link>
      <description><![CDATA[我正在使用 OpenPCDet 开发一个项目，在 Windows 环境中设置它时遇到一些问题。官方文档主要参考Linux环境。我想知道 OpenPCDet 是否本质上与 Windows 不兼容，或者是否有特定步骤使其在 Windows 上工作。
是否有人在 Windows 上成功运行 OpenPCDet，如果是，您能否提供一些设置指导或资源？或者，如果 OpenPCDet 仅限于 Linux 环境，是否有针对 Windows 用户的推荐解决方法？
我尝试设置环境，但是当我运行 python setup.pydevelopment 时遇到错误。]]></description>
      <guid>https://stackoverflow.com/questions/77924600/is-openpcdet-compatible-with-windows-or-only-linux</guid>
      <pubDate>Fri, 02 Feb 2024 03:56:47 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：调用层“time_distributed_4”时遇到异常（类型 TimeDistributed）</title>
      <link>https://stackoverflow.com/questions/77909132/valueerror-exception-encountered-when-calling-layer-time-distributed-4-type</link>
      <description><![CDATA[我尝试使用 Kvasir 数据集制作 CNN-LSTM 模型。我使用 image_dataset_from_directory 分割数据集，如下所示：
dataset_path = “/kaggle/working/dataset”
图像大小 = 224, 224
批量大小 = 64

train_ds = image_dataset_from_directory(
  数据集_路径，
  验证分割=0.2，
  子集=“训练”，
  label_mode =“分类”，
  种子=23，
  图像大小=图像大小，
  color_mode =“rgb”，
  批量大小=批量大小）


val_ds = image_dataset_from_directory(
  数据集_路径，
  验证分割=0.2，
  子集=“验证”，
  label_mode =“分类”，
  种子=23，
  图像大小=图像大小，
  color_mode=“RGB”，
  批量大小=批量大小）

这个函数给了我一个 BatchDataset。然后我将基数设置如下：
val_batches = tf.data.experimental.cardinality(val_ds)
test_ds = val_ds.take(val_batches // 2)
val_ds = val_ds.skip(val_batches // 2)

然后
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)

这段代码还给了我一个预取数据集。当我运行 print(train_ds) 时它给出：
&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 8), dtype=tf .float32，名称=无））&gt;

然后我添加了我的模型，
 model = tf.keras.models.Sequential([
    # 具有批量归一化和最大池化的卷积层
    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, (3, 3), 激活=无,input_shape=(224, 224,3))),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3), 激活=无)),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),



    # 压平输出并添加密集层
    tf.keras.layers.TimeDistributed(Flatten()),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.LSTM(256,激活=&#39;tanh&#39;),
    
    
    
    tf.keras.layers.Dense(256),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),

    # 具有 8 个节点的输出层用于分类
    tf.keras.layers.Dense(8, 激活=&#39;softmax&#39;)
]）

# 编译模型

model.compile(优化器=AdamW(learning_rate=0.001,weight_decay=0.004,beta_1=0.9,beta_2=0.999,epsilon=1e-8),
          损失=分类交叉熵(),
          指标=[&#39;准确性&#39;])

当我适合这个模型时它没有运行并且出现错误：
ValueError：调用层“time_distributed_4”（类型 TimeDistributed）时遇到异常。
    
    层“conv2d_2”的输入0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、224、3）
    
    调用层“time_distributed_4”接收的参数（类型 TimeDistributed）：
      输入= tf.Tensor（形状=（无，224，224，3），dtype = float32）
      • 训练=真
      • 掩码=无

我不知道如何解决这个问题，你能帮助我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77909132/valueerror-exception-encountered-when-calling-layer-time-distributed-4-type</guid>
      <pubDate>Tue, 30 Jan 2024 20:05:50 GMT</pubDate>
    </item>
    <item>
      <title>Tesseract 的训练自定义数据集以及版本之间的差异</title>
      <link>https://stackoverflow.com/questions/77894617/training-custom-dataset-for-tesseract-and-difference-between-versions</link>
      <description><![CDATA[我正在尝试构建自定义数据集，然后对其进行训练，以改进tesseract 的 OCR。
然而，我很难理解确切的步骤或正确的方法。请注意，我在机器学习方面的经验很少，尤其是在神经网络方面。
在开始我的问题之前，我想说，我认为 LSTM 是从 Tesseract 4 开始添加的，并且对于较低版本，默认使用自适应分类器。
我在这里查看了文档：https://github.com/tesseract-ocr/tessdoc  但老实说，我很困惑，因为我的经验为零，并且通过查看一些教程，我发现了很多差异。
我的问题是：

在为 tesseract 4 及更高版本创建数据集时，我只需要图像、box 文件和真实数据文件？
对于 4 以下的超正方体，我只需要图像和盒子文件，而不需要真实数据文件？

我尝试识别的图像上的文本如下：








如果您还可以提供有关我的案例场景的确切步骤的详细说明，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77894617/training-custom-dataset-for-tesseract-and-difference-between-versions</guid>
      <pubDate>Sun, 28 Jan 2024 11:33:15 GMT</pubDate>
    </item>
    <item>
      <title>将经过训练的机器学习模型与 React Native 应用程序集成</title>
      <link>https://stackoverflow.com/questions/74961856/to-integrate-a-trained-machine-learning-model-with-react-native-app</link>
      <description><![CDATA[我有一个 FYP 项目（Instagram 等社交媒体应用程序），需要我创建一个简单的推荐系统。我已经使用 Python 对余弦相似度数据集进行了训练，但我不知道下一步该做什么。如何将经过训练的机器学习模型集成到 React Native 中，或者是否有更好、更简单的方法来制作推荐系统？
我尝试阅读文档和观看视频。但我似乎仍然无法掌握一些困难的概念。如果您能在训练我的模型后为我提供有关学习内容的说明或步骤，我将不胜感激。或者，如果我必须使用一些库或软件包等。[不确定这是否是适合此查询的论坛]]]></description>
      <guid>https://stackoverflow.com/questions/74961856/to-integrate-a-trained-machine-learning-model-with-react-native-app</guid>
      <pubDate>Fri, 30 Dec 2022 13:02:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Minimax 算法和 Alpha Beta 剪枝解决 Tic Tac Toe 4x4 游戏</title>
      <link>https://stackoverflow.com/questions/51427156/how-to-solve-tic-tac-toe-4x4-game-using-minimax-algorithm-and-alpha-beta-pruning</link>
      <description><![CDATA[我使用 Minimax 和 Alpha Beta 剪枝制作了一款 Tic Tac Toe 游戏。我想为 Tic Tac Toe (10x10) 游戏制作一个计算机 AI，但它的游戏树尺寸大得离谱。 
我的代码是这样的，我只需要更改两个变量来更改棋盘大小+连续获胜所需的单元格。
示例：
boardSize = 3 // 这是用于 3x3 井字游戏
boardSize = 4 // 这是用于 4x4 井字游戏
boardSize = 10 // 这是用于 10x10 井字游戏
和
winStreak = 3 // 需要连续出现 3 个方格才能获胜
winStreak = 4 // 需要连续出现 4 个方格才能获胜
希望你能明白。
所以，我改变了制作井字棋的计划，从 10x10 改为 3x3，效果很好。 
然后我更改 boardSize = 4 和 winStreak = 3 使其成为 (4x4) 井字棋游戏。 
现在，我认为 Minimax 结合 Alpha Beta 剪枝足以解决 4x4 问题，但令人惊讶的是，事实并非如此。 
当我（人类）迈出第一步时，极小极大算法会搜索 5-10 分钟，然后浏览器选项卡就会崩溃。它甚至无法迈出第一步。 
如何提高其速度？人们甚至能够使用 Minimax + Alpha Beta 剪枝来解决国际象棋问题，但是我的代码有什么问题吗？ 
我的完整代码大约有 200-300 行，所以我只写伪代码。 
 humanMakeMove();

极小极大(); // Bot 调用 Minimax 函数来采取行动

函数 Minimax(棋盘，玩家) {
if (checkResult() != 0) // 0 = 游戏开始
   返回检查结果（）； // 1 = 赢，0.5 = 平局，-1 = 输

   可用移动 = getAvailableMoves();

   for(i = 0; i &lt; availableMoves.length;i++)
   {
        移动=可用移动[i]；
        从可用移动数组中删除移动（移动）；
        if (玩家==“X”)
            分数 = Minimax(板, &quot;O&quot;);
        别的
            分数 = Minimax(板, &quot;X&quot;);
        板[i] =“-”； // “-”表示空白


        if (树的深度在第一层 &amp;&amp; 分数 == 1)
                返回最大分数； //这里应用了Alpha Beta Pruning，如果我们得到1分，那么我们就不需要再搜索了。


   }
}

我还可以应用哪些优化来使代码运行得更快？]]></description>
      <guid>https://stackoverflow.com/questions/51427156/how-to-solve-tic-tac-toe-4x4-game-using-minimax-algorithm-and-alpha-beta-pruning</guid>
      <pubDate>Thu, 19 Jul 2018 15:57:26 GMT</pubDate>
    </item>
    </channel>
</rss>