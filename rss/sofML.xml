<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 21 Dec 2024 03:18:58 GMT</lastBuildDate>
    <item>
      <title>如何使用 ARI 比较具有不同数量数据点的集群？</title>
      <link>https://stackoverflow.com/questions/79298516/how-can-i-compare-clusters-with-different-amounts-of-datapoints-using-ari</link>
      <description><![CDATA[我有两个 KModes 集群，结构为 1D 列表，其中每个索引代表一个数据点，该索引处的值代表数据点所属的集群。
我的问题是我想比较这两个集群，我正在使用 ARI 来做到这一点。ARI 需要 2 个大小相同的输入。有没有办法使用 ARI 比较这两个集群？我尝试向较小的列表添加虚拟值，但似乎不起作用。我正在使用 SkLearn“adjusted_rand_score”。]]></description>
      <guid>https://stackoverflow.com/questions/79298516/how-can-i-compare-clusters-with-different-amounts-of-datapoints-using-ari</guid>
      <pubDate>Fri, 20 Dec 2024 22:21:33 GMT</pubDate>
    </item>
    <item>
      <title>如何使用安装在我的 Windows 机器上的 cuda 和 cudnn 在 wsl 中通过 jupiter notebook 训练 ml 模型</title>
      <link>https://stackoverflow.com/questions/79297793/how-to-use-cuda-and-cudnn-which-is-installed-on-my-windows-machine-side-in-wsl-f</link>
      <description><![CDATA[如何在 wsl 中使用安装在我的 Windows 机器上的 cuda 和 cudnn。我需要通过 jupiter 笔记本训练 ml 模型，我创建了一个虚拟环境并安装了 tensorflow 和 pytorch，然后安装了所有与 jupyter 相关的依赖项，但似乎存在一些小问题，即 cudnnn 未被识别，即使它已正确配置
警告：调用 absl::InitializeLog() 之前的所有日志消息都写入为平台 CUDA 初始化的 STDERR（这不能保证将使用 XLA）。设备：StreamExecutor 设备 (0)：NVIDIA GeForce RTX 4060 笔记本电脑 GPU，计算能力 8.9：I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] 禁用 MLIR 崩溃重现器，设置 env var MLIR_CRASH_REPRODUCER_DIRECTORY 以启用。675 cuda_dnn.cc:522] 加载的运行时 CuDNN 库：9.1.0 但源代码是使用 9.3.0 编译的。CuDNN 库需要具有匹配的主版本和相同或更高的次版本。如果使用二进制安装，请升级您的 CuDNN 库。如果从源代码构建，请确保运行时加载的库与编译配置期间指定的版本兼容。 675 cuda_dnn.cc:522] 加载的运行时 CuDNN 库：9.1.0 但源代码是使用 9.3.0 编译的。CuDNN 库需要具有匹配的主版本和相同或更高的次版本。如果使用二进制安装，请升级您的 CuDNN 库。如果从源代码构建，请确保运行时加载的库与编译配置期间指定的版本兼容。W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES 在 xla_ops.cc:577 失败：FAILED_PRECONDITION：DNN 库初始化失败。查看上述错误了解更多详细信息。：I tensorflow/core/framework/local_rendezvous.cc:405] 本地会合正在中止，状态为：FAILED_PRECONDITION：DNN 库初始化失败。查看上述错误了解更多详细信息。 [[{{node StatefulPartitionedCall}}]]
当我执行 nvcc-version 时，为什么它显示 cuda 12.7，即使我的 Windows 上安装了 cuda 12.2。它的配置是正确的，因为我能够用它运行另一个不同的计算机视觉项目
我尝试了所有不同类型的 pytorch、torch vision 和 cuda 兼容版本，甚至还有多种配置，但似乎没有任何效果，我甚至尝试使用 docker。我认为问题是我没有连接或配置正确的 wsl 连接与我的 Windows 驱动程序，据我所知，或者我可能会再次安装 wsl，似乎存在一些冲突，我无法弄清楚]]></description>
      <guid>https://stackoverflow.com/questions/79297793/how-to-use-cuda-and-cudnn-which-is-installed-on-my-windows-machine-side-in-wsl-f</guid>
      <pubDate>Fri, 20 Dec 2024 16:09:17 GMT</pubDate>
    </item>
    <item>
      <title>如何显示自动编码器生成的图像？</title>
      <link>https://stackoverflow.com/questions/79297188/how-do-i-display-the-images-generated-by-an-autoencoder</link>
      <description><![CDATA[我使用 python 创建了一个自动编码器，没有任何错误。但是，我不知道如何显示自动编码器生成的图像的代码。自动编码器的代码如下所示：
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras import layer, models, datasets, callbacks
import tensorflow.keras.backend as K

#由于某些奇怪的原因，文本中没有包含
from keras.models import Model

#将数据导入训练集和测试集
from tensorflow.keras import datasets
(x_train,y_train), (x_test,y_test) = datasets.fashion_mnist.load_data()

#缩放图像
def preprocess(imgs):
imgs = imgs.astype(&quot;float32&quot;) / 255.0
imgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)
imgs = np.expand_dims(imgs, -1)
return imgs
x_train = preprocess(x_train)
x_test = preprocess(x_test)

#编码器
encoder_input = layer.Input(
shape=(32, 32, 1), name = &quot;encoder_input&quot;
)
x = layer.Conv2D(32, (3, 3), strides = 2,activation = &#39;relu&#39;, padding=&quot;same&quot;)(
encoder_input
)
x = layer.Conv2D(64, (3, 3), strides = 2,activation = &#39;relu&#39;, padding=&quot;same&quot;)(x)
x = layer.Conv2D(128, (3, 3), strides = 2,activation = &#39;relu&#39;, padding=&quot;same&quot;)(x)
shape_before_flattening = K.int_shape(x)[1:]
x = layer.Flatten()(x)
encoder_output = layer.Dense(2, name=&quot;encoder_output&quot;)(x)
encoder = models.Model(encoder_input,coder_output)

#解码器
decoder_input = layer.Input(shape=(2,), name=&quot;decoder_input&quot;)
x = layer.Dense(np.prod(shape_before_flattening))(decoder_input)
x = layer.Reshape(shape_before_flattening)(x)
x = layer.Conv2DTranspose(
128, (3, 3), strides=2, 激活 = &#39;relu&#39;, padding=&quot;same&quot;
)(x)
x = layer.Conv2DTranspose(
64, (3, 3), strides=2,activation = &#39;relu&#39;, padding=&quot;same&quot;
)(x)
x = layer.Conv2DTranspose(
32, (3, 3), strides=2,activation = &#39;relu&#39;, padding=&quot;same&quot;
)(x)
decoder_output = layer.Conv2D(
1,
(3, 3),
strides = 1,
activation=&quot;sigmoid&quot;,
padding=&quot;same&quot;,
name=&quot;decoder_output&quot;
)(x)
decoder = models.Model(decoder_input,coder_output)

# 将编码器与解码器连接起来
autoencoder = Model(encoder_input,coder(encoder_output))

# 编译自动编码器
autoencoder.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;)
# 通过将输入图像作为输入和输出传入来训练自动编码器
# 使用一个 epoch

autoencoder.fit(
x_train,
x_train,
epochs=1,
batch_size=100,
shuffle=True,
validation_data=(x_test, x_test),
)

# 重建图像
example_images = x_test[:50]
predictions = autoencoder.predict(example_images)

我尝试使用 plt.imshow，如下所示。我期望看到自动编码器生成的 10 张图像。但是它不起作用。我真的不知道如何使用它：
for i in range(10):
plt.figure(figsize=(20,3))
plt.imshow(predictions[i].astype(&quot;float32&quot;), cmap=&quot;gray_r&quot;)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79297188/how-do-i-display-the-images-generated-by-an-autoencoder</guid>
      <pubDate>Fri, 20 Dec 2024 12:10:50 GMT</pubDate>
    </item>
    <item>
      <title>ST-GCN 过时了吗？</title>
      <link>https://stackoverflow.com/questions/79297114/is-st-gcn-outdated</link>
      <description><![CDATA[我正在尝试构建一个管道来跟踪视频监控录像中的异常情况。
为了对检测到的人的行为进行分类，我想使用 ST-GCN，但是我能找到的唯一文档是 5 年前更新的，但阅读最新研究 ST-GCN 仍在使用中。
有谁知道更新的文档或对我如何找到有关如何将其实现到我的管道中的信息有任何提示吗？
感谢您的任何提示&lt;3]]></description>
      <guid>https://stackoverflow.com/questions/79297114/is-st-gcn-outdated</guid>
      <pubDate>Fri, 20 Dec 2024 11:42:14 GMT</pubDate>
    </item>
    <item>
      <title>在 FastAPI 中提供多种机器学习模型的最佳实践：Docker 与 Celery 与 Redis 或其他方法 [关闭]</title>
      <link>https://stackoverflow.com/questions/79296580/best-practice-for-serving-multiple-machine-learning-models-in-fastapi-docker-vs</link>
      <description><![CDATA[我正在开发一个 FastAPI 应用程序，该应用程序处理来自 Web 应用程序的请求，以使用多个机器学习模型（例如，糖尿病预测模型、面部分析模型等）进行预测。我正在尝试确定部署和管理这些模型的最佳架构。
以下是我正在考虑的方法：
选项 1：Docker + Flask 服务器
将每个模型打包在其自己的 Docker 映像中。
在各自的 Docker 容器内为每个模型运行一个 Flask 服务器。
FastAPI 通过维护每个模型的连接字符串与这些服务器通信（例如，一个用于糖尿病模型，另一个用于面部分析模型等）。

但是，这种方法意味着 FastAPI 必须管理多个连接字符串，随着模型数量的增加，这可能会变得混乱。
选项 2：Celery + Redis
使用 Celery 工作程序处理预测，每个工作程序负责一个特定模型。
使用 Redis 作为任务队列。
FastAPI 只会在 Redis 队列中注册任务，而无需知道各个工作程序的连接详细信息。

这种方法集中了任务管理，并消除了从 FastAPI 管理多个连接字符串的负担。
选项 3：其他方法
在这样的设置中，是否有用于管理和提供多个机器学习模型的替代架构或最佳实践？
我希望系统具有可扩展性、可维护性和高效性。如果您曾经使用过类似的设置，我将不胜感激您的见解！]]></description>
      <guid>https://stackoverflow.com/questions/79296580/best-practice-for-serving-multiple-machine-learning-models-in-fastapi-docker-vs</guid>
      <pubDate>Fri, 20 Dec 2024 08:19:43 GMT</pubDate>
    </item>
    <item>
      <title>iOS Swift 根据用户数据进行动态机器学习</title>
      <link>https://stackoverflow.com/questions/79295972/ios-swift-dynamic-machine-learning-from-user-data</link>
      <description><![CDATA[是否可以使用 Apple ML 框架动态学习应用中的用户行为？我已经使用 Create ML 应用程序训练了一个模型，然后我可以从 iOS 设备更新并重新训练吗？这就是我目前使用该模型的方式。
public func calculateMuscleRecoveryTime(_ workout: Workout) {
do {

let config = MLModelConfiguration()
let model = try MuscleRecoveryModel(configuration: config)

let allMuscleGroups = workout.exercises
.compactMap { $0.muscles } // 展平每个锻炼的肌肉数组
.reduce(Set&lt;MuscleGroup&gt;()) { $0.union($1) } // 联合以删除重复项

let uniqueMuscleGroups = Array(allMuscleGroups)

for muscleGroup in uniqueMuscleGroups {
let trainingIntensity = Int64(workout.intensity.intValue)
let lastTrainedTimestamp = workout.date
let timeAgo = timeAgoInSeconds(from: lastTrainedTimestamp)
let muscleName = muscleGroup.rawValue.lowercased()

let prediction = try model.prediction(muscle: muscleName, intense: trainingIntensity, lastTrained: timeAgo)
}
} catch let error {
print(&quot;Error: &quot;, error)
}
}
]]></description>
      <guid>https://stackoverflow.com/questions/79295972/ios-swift-dynamic-machine-learning-from-user-data</guid>
      <pubDate>Fri, 20 Dec 2024 01:10:59 GMT</pubDate>
    </item>
    <item>
      <title>无论如何，PyTorch DeiT 模型都会持续预测一个类别</title>
      <link>https://stackoverflow.com/questions/79293139/pytorch-deit-model-keeps-predicting-one-class-no-matter-what</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79293139/pytorch-deit-model-keeps-predicting-one-class-no-matter-what</guid>
      <pubDate>Thu, 19 Dec 2024 04:52:07 GMT</pubDate>
    </item>
    <item>
      <title>在 Google Cloud Functions 中部署 Keras 模型进行预测</title>
      <link>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</link>
      <description><![CDATA[我一直在尝试将一个非常简单的 Keras 玩具模型部署到 Cloud Functions，该模型可以预测图像的类别，但由于未知原因，当执行到 predict 方法时，它会卡住，不会抛出任何错误，最终会超时。
import functions_framework
import io
import numpy as np
import tensorflow as tf

from tensorflow.keras.models import load_model
from PIL import Image

model = load_model(&quot;gs://&lt;my-bucket&gt;/cifar10_model.keras&quot;)

class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]

def preprocess_image(image_file):
img = Image.open(io.BytesIO(image_file.read()))
img = img.resize((32, 32))
img = np.array(img)
img = img / 255.0
img = img.reshape(1, 32, 32, 3)
return img

@functions_framework.http
def predict(request):
image = preprocess_image(request.files[&#39;image_file&#39;])
print(image.shape) # 这会打印 OK
prediction = model.predict(image)
print(prediction) # 永远不会打印
predict_class = class_names[np.argmax(prediction)]
return f&quot;Predicted class: {predicted_class}&quot;

本地调试运行良好，预测速度如预期一样快（模型权重文件为 2MB）。我还在此过程中添加了几个打印（从上面的代码片段中删除），执行工作正常，直到 predict 方法。
即使最小计算配置应该可以工作，我还是尝试保留更多内存和 CPU，但没有任何效果。该模型托管在存储中，我尝试先下载它，但也没有用。我也尝试在 tf.device(&#39;/cpu:0&#39;) 上下文中进行预测，传递 step=1 参数并首先将图像数组转换为 Keras 数据集，如 ChatGPT 所建议的那样，结果相同。实际上，调用 predict 根本没有打印任何内容。调用 call 而不是 predict 没有任何效果。
我错过了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</guid>
      <pubDate>Tue, 17 Dec 2024 13:51:16 GMT</pubDate>
    </item>
    <item>
      <title>如何加载使用（version ='latest'）框架训练的 Sagemaker XGBoost 模型？</title>
      <link>https://stackoverflow.com/questions/79269787/how-to-load-sagemaker-xgboost-model-which-was-trained-using-version-latest</link>
      <description><![CDATA[管道中有一个使用此容器创建的现有 xgboost 模型
sagemaker.image_uris.retrieve(&#39;xgboost&#39;, sagemaker.Session().boto_region_name, version=&#39;latest&#39;)

输出：
&#39;{accountid}.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest&#39;

我从模型工件中提取了 model.tar.gz 并加载了 xgboost-model 文件
但它给出了这个错误
XGBoostError: basic_string::resize

我运行了一个 shell 脚本，使用所有可用的 XGBoost 版本加载模型，但没有任何效果。
我只想使用 model.get_score 检查特征重要性。]]></description>
      <guid>https://stackoverflow.com/questions/79269787/how-to-load-sagemaker-xgboost-model-which-was-trained-using-version-latest</guid>
      <pubDate>Tue, 10 Dec 2024 20:57:42 GMT</pubDate>
    </item>
    <item>
      <title>SAM 2.1 是什么导致 hydra.errors.MissingConfigException：未找到主配置模块“sam2”？</title>
      <link>https://stackoverflow.com/questions/79199682/sam-2-1-what-is-causing-hydra-errors-missingconfigexception-primary-config-modu</link>
      <description><![CDATA[我正在尝试使用此处给出的 roboflow 指南微调新的 SAM 2.1 分割模型：Sam 2.1 roboflow 指南
使用 google collab 时，此代码运行正常，没有遇到任何错误。当我在本地机器上运行完全相同的代码时，运行训练代码命令时会出现以下错误：
!python training/train.py -c &#39;configs/train.yaml&#39; --use-cluster 0 --num-gpus 1
在 Windows 10 上使用 vscode 运行时出现以下错误：
hydra.errors.MissingConfigException：未找到主配置模块“sam2”。
检查它是否正确并包含 __init__.py 文件

我的工作目录：
C:\..\SAM_2_1\sam2

]]></description>
      <guid>https://stackoverflow.com/questions/79199682/sam-2-1-what-is-causing-hydra-errors-missingconfigexception-primary-config-modu</guid>
      <pubDate>Mon, 18 Nov 2024 11:00:31 GMT</pubDate>
    </item>
    <item>
      <title>总参数：0，执行 model.summary() keras</title>
      <link>https://stackoverflow.com/questions/78462277/total-params-0-on-doing-model-summary-keras</link>
      <description><![CDATA[model = Sequential()
model.add(Embedding(283, 100, input_length=56))
model.add(LSTM(150))
model.add(LSTM(150))
model.add(Dense(283,activation=&#39;softmax&#39;))

model.compile(loss=&#39;categorical_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;])

model.summary()

Tensorflow 版本：2.16.1，
Keras 版本：3.3.3，
设备 - M3 pro macbook
我尝试使用虚拟数据集（有 282 个唯一单词，使用 tokenizer 检查）构建用于文本生成的 LSTM 模型，预期参数为非零，但输出每个层都有 0 个参数。]]></description>
      <guid>https://stackoverflow.com/questions/78462277/total-params-0-on-doing-model-summary-keras</guid>
      <pubDate>Fri, 10 May 2024 19:49:53 GMT</pubDate>
    </item>
    <item>
      <title>Optuna Hyperband 算法不遵循预期的模型训练方案</title>
      <link>https://stackoverflow.com/questions/78251318/optuna-hyperband-algorithm-not-following-expected-model-training-scheme</link>
      <description><![CDATA[我在 Optuna 中使用 Hyperband 算法时发现了一个问题。根据 Hyperband 算法，当 min_resources = 5、ma​​x_resources = 20 和 reduction_factor = 2 时，搜索应从 1 组别的 4 个模型的初始空间开始，每个模型在第一轮中接收 5 个 epoch。随后，模型数量在每一轮中减少 2 倍，搜索空间也应在下一组的 2 倍减少，即组别 2 的初始搜索空间为 2 个模型，其余模型的 epoch 数量在随后的每一轮中加倍。因此预计总模型数应为 11，但需要训练大量模型。
文章链接：- https://arxiv.org/pdf/1603.06560.pdf
import optuna
import numpy as np
import pandas as pd 
from tensorflow.keras.layers import Dense,Flatten,Dropout
import tensorflow as tf
from tensorflow.keras.models import Sequential

# 玩具数据集生成
def generate_toy_dataset():
np.random.seed(0)
X_train = np.random.rand(100, 10)
y_train = np.random.randint(0, 2, size=(100,))
X_val = np.random.rand(20, 10)
y_val = np.random.randint(0, 2, size=(20,))
return X_train, y_train, X_val, y_val

X_train, y_train, X_val, y_val = generate_toy_dataset()

# 模型构建函数
def build_model(trial):
model = Sequential()
model.add(Dense(units=trial.suggest_int(&#39;unit_input&#39;, 20, 30),
activation=&#39;selu&#39;,
input_shape=(X_train.shape[1],)))

num_layers = trial.suggest_int(&#39;num_layers&#39;, 2, 3)
for i in range(num_layers):
units = trial.suggest_int(f&#39;num_layer_{i}&#39;, 20, 30)
activation = trial.suggest_categorical(f&#39;activation_layer_{i}&#39;, [&#39;relu&#39;, &#39;selu&#39;, &#39;tanh&#39;])
model.add(Dense(units=units,activation=activation))
如果 trial.suggest_categorical(f&#39;dropout_layer_{i}&#39;, [True, False]):
model.add(Dropout(rate=0.5))

model.add(Dense(1,activation=&#39;sigmoid&#39;))

optimizer_name = trial.suggest_categorical(&#39;optimizer&#39;, [&#39;adam&#39;, &#39;rmsprop&#39;])
如果 optimizer_name == &#39;adam&#39;:
optimizer = tf.keras.optimizers.Adam()
否则:
optimizer = tf.keras.optimizers.RMSprop()

model.compile(optimizer=optimizer, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;, tf.keras.metrics.AUC(name=&#39;val_auc&#39;)])

return model

def objective(trial):
model = build_model(trial)
# 假设您已准备好数据
# 修改拟合方法以包含 AUC 指标
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), verbose=1)

# 检查是否记录了“val_auc”
auc_key = None
for key in history.history.keys():
if key.startswith(&#39;val_auc&#39;):
auc_key = key
print(f&quot;auc_key is {auc_key}&quot;)
break

if auc_key is None:
raise ValueError(&quot;历史记录中未找到AUC指标。确保在训练期间记录它。&quot;)

# 报告每个模型的验证 AUC

if auc_key ==&quot;val_auc&quot;:
step=0
else:
step = int(auc_key.split(&#39;_&#39;)[-1])

auc_value=history.history[auc_key][0]
trial.report(auc_value, step=step)
print(f&quot;prune or not:-{trial.should_prune()}&quot;)
if trial.should_prune():
raise optuna.TrialPruned()

return history.history[auc_key]

# Optuna 研究创建
study = optuna.create_study(
direction=&#39;maximize&#39;,
pruner=optuna.pruners.HyperbandPruner(
min_resource=5,
max_resource=20,
reduction_factor=2
)
)

# 开始优化
study.optimize(objective)

]]></description>
      <guid>https://stackoverflow.com/questions/78251318/optuna-hyperband-algorithm-not-following-expected-model-training-scheme</guid>
      <pubDate>Sun, 31 Mar 2024 12:38:07 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：使用“bitsandbytes”8 位量化需要加速：“pip install accelerate”</title>
      <link>https://stackoverflow.com/questions/78040978/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</link>
      <description><![CDATA[我正在尝试使用开源数据集微调 llama2-13b-chat-hf。
我一直使用此模板，但现在出现此错误：
ImportError：使用 bitsandbytes 8 位量化需要 Accelerate：pip install accelerate 和最新版本的 bitsandbytes：pip install -i https://pypi.org/simple/ bitsandbytes
我安装了所有必需的软件包，这些是版本：
 accelerate @ git+https://github.com/huggingface/accelerate.git@97d2168e5953fe7373a06c69c02c5a00a84d5344
bitsandbytes==0.42.0
datasets==2.17.1
huggingface-hub==0.20.3
peft==0.8.2
tokenizers==0.13.3
torch==2.1.0+cu118
torchaudio==2.1.0+cu118
torchvision==0.16.0+cu118
transformers==4.30.0
trl==0.7.11

有人知道这是不是版本问题吗？
你是怎么解决的？
我尝试安装其他版本，但没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/78040978/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</guid>
      <pubDate>Thu, 22 Feb 2024 12:37:11 GMT</pubDate>
    </item>
    <item>
      <title>如何处理缺失值超过 80% 的特征</title>
      <link>https://stackoverflow.com/questions/72611870/how-to-deal-with-features-with-more-than-80-missingness</link>
      <description><![CDATA[我正在处理一个非常糟糕的临床数据集，它有 300 个样本、400 个特征，将用于机器学习。我的导师告诉我这个数据集中有一些具有生物学意义的特征，并要求我保留它们，但其中许多特征缺失了 50% 以上，甚至 80% 以上。我该怎么办？使用模式填充是否会影响它们的性能。]]></description>
      <guid>https://stackoverflow.com/questions/72611870/how-to-deal-with-features-with-more-than-80-missingness</guid>
      <pubDate>Tue, 14 Jun 2022 05:23:12 GMT</pubDate>
    </item>
    <item>
      <title>Google Cloud Vision API 和 Mobile Vision 有什么区别？</title>
      <link>https://stackoverflow.com/questions/44091577/what-is-the-difference-between-google-cloud-vision-api-and-mobile-vision</link>
      <description><![CDATA[我一直在使用 cloud vision API。我做了一些标签和面部检测。在这次 Google I/O 期间，有一个会议讨论了 mobile vision。我知道这两个 API 都与 Google Cloud 中的机器学习有关。
有人能解释（用例）何时使用其中一个而不是另一个吗？
我们可以同时使用这两个 API 来构建什么样的应用程序？]]></description>
      <guid>https://stackoverflow.com/questions/44091577/what-is-the-difference-between-google-cloud-vision-api-and-mobile-vision</guid>
      <pubDate>Sat, 20 May 2017 22:59:56 GMT</pubDate>
    </item>
    </channel>
</rss>