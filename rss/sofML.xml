<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 04 May 2024 12:23:16 GMT</lastBuildDate>
    <item>
      <title>错误：HuggingFaceInstructEmbeddings 初始化</title>
      <link>https://stackoverflow.com/questions/78428830/error-huggingfaceinstructembeddings-initalization</link>
      <description><![CDATA[`python
from langchain.embeddings import HuggingFaceInstructEmbeddings
instructor_embeddings = HuggingFaceInstructEmbeddings()

`
我被这个错误困住了，似乎无法解决这个问题：
&#39;&#39;&#39;
TypeError：INSTRUCTOR._load_sbert_model() 获得了一个意外的关键字参数“token”
&#39;&#39;&#39;
请帮忙！我该如何解决这个问题？
我想使用 HuggingFaceInstructEmbeddings 来矢量化我的数据集，但它不起作用]]></description>
      <guid>https://stackoverflow.com/questions/78428830/error-huggingfaceinstructembeddings-initalization</guid>
      <pubDate>Sat, 04 May 2024 11:32:32 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch：不同的 DataLoader 批量大小会产生截然不同的损失</title>
      <link>https://stackoverflow.com/questions/78428584/pytorch-different-dataloader-batch-sizes-yield-very-different-losses</link>
      <description><![CDATA[我正在使用 此近似示例将正弦波作为学习近似的基础。我是 PyTorch 新手……为什么不同的 BATCH_SIZE 值会显着改变结果？
批量大小 512：

批量大小 10000（所有数据点）：

我发布了下面的代码，但首先我将解释我对原始代码所做的更改：

所有随机种子都是静态的，禁用洗牌：

学习率更改为 1e-4，X 大小更改为 10**4，MAX_EPOCH 更改为 20。

添加了一个绘图来绘制 X 范围内所有值的近似值，以显示差异


完整代码如下...
谢谢！
导入火炬
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

从火炬导入 nn，优化
从 torch.utils.data 导入 TensorDataset、DataLoader
从 sklearn.model_selection 导入 train_test_split

device = torch.device(“cuda”) if torch.cuda.is_available() else torch.device(“cpu”)
LR = 1e-4
最大纪元 = 20
BATCH_SIZE = 10**4

类 SineApproximator(nn.Module):
    def __init__(自身):
        超级（SineApproximator，自我）.__init__()
        self.regressor = nn.Sequential(nn.Linear(1, 1024),
                                       nn.ReLU(inplace=True),
                                       nn.线性(1024, 1024),
                                       nn.ReLU(inplace=True),
                                       nn.线性(1024, 1))
    def 前向（自身，x）：
        输出 = self.regressor(x)
        返回输出


火炬.manual_seed(41)
如果 torch.cuda.is_available():
    torch.cuda.manual_seed_all(41)
np.随机.种子(41)

X = np.random.rand(10**4) * 2 * np.pi
y = np.sin(X)

X_train, X_val, y_train, y_val = map(torch.tensor, train_test_split(X, y, test_size=0.2, shuffle=False, random_state=41))
train_dataloader = DataLoader(TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1)), batch_size=BATCH_SIZE,
                              pin_memory=True, shuffle=True)
val_dataloader = DataLoader(TensorDataset(X_val.unsqueeze(1), y_val.unsqueeze(1)), batch_size=BATCH_SIZE,
                            pin_memory=True, shuffle=True)

模型 = SineApproximator().to(设备)
优化器 = optim.Adam(model.parameters(), lr=LR)
标准 = nn.MSELoss(reduction=&quot;mean&quot;)

train_loss_list = 列表()
val_loss_list = 列表()
对于范围内的纪元（MAX_EPOCH）：
    print(&quot;纪元 %d / %d&quot; % (纪元 + 1, MAX_EPOCH))
    模型.train()
    # 训练循环
    temp_loss_list = 列表()
    对于train_dataloader中的X_train、y_train：
        X_train = X_train.type(torch.float32).to(设备)
        y_train = y_train.type(torch.float32).to(设备)

        优化器.zero_grad()

        分数 = 模型(X_train)
        损失=标准（输入=分数，目标=y_train）
        loss.backward()

        优化器.step()

        temp_loss_list.append(loss.detach().cpu().numpy())

    temp_loss_list = 列表()
    对于train_dataloader中的X_train、y_train：
        X_train = X_train.type(torch.float32).to(设备)
        y_train = y_train.type(torch.float32).to(设备)

        分数 = 模型(X_train)
        损失=标准（输入=分数，目标=y_train）

        temp_loss_list.append(loss.detach().cpu().numpy())

    avg_loss = np.average（temp_loss_list）
    train_loss_list.append(avg_loss)
    print(&quot;\ttrain 损失: %.5f&quot; % train_loss_list[-1])

# 构建一个 np 数组，其中所有 X 值都在最小值和最大值之间，间隔为 0.01，每个值都在自己的数组中：
模型.eval()
X_all = torch.tensor(np.arange(X.min(), X.max(), 0.01)).type(torch.float32).unsqueeze(1).to(device)
y_预测 = 模型(X_all)
y_prediction = y_prediction.detach().cpu().numpy().flatten()
原始 = plt.scatter(X, y, s=1)
预测 = plt.scatter(X_all.detach().cpu().numpy(), y_prediction, s=1)
plt.legend((预测，原始), (“函数”, “样本”))
plt.waitforbuttonpress()
]]></description>
      <guid>https://stackoverflow.com/questions/78428584/pytorch-different-dataloader-batch-sizes-yield-very-different-losses</guid>
      <pubDate>Sat, 04 May 2024 10:10:58 GMT</pubDate>
    </item>
    <item>
      <title>如何从未分割的（正常）图像中提取放射学特征？</title>
      <link>https://stackoverflow.com/questions/78428542/how-to-extract-radiomic-features-from-an-unsegmented-normal-image</link>
      <description><![CDATA[我正在通过从医学图像中提取放射组学特征来利用机器学习进行肿瘤检测。我的问题是：
由于健康数据上没有肿瘤，因此图像中没有分割部分。如何从健康图像中提取放射组学特征？
“值错误：在此掩码中找不到标签（即没有任何内容被分段）！”]]></description>
      <guid>https://stackoverflow.com/questions/78428542/how-to-extract-radiomic-features-from-an-unsegmented-normal-image</guid>
      <pubDate>Sat, 04 May 2024 09:57:03 GMT</pubDate>
    </item>
    <item>
      <title>饮食推荐系统的python代码[关闭]</title>
      <link>https://stackoverflow.com/questions/78428234/python-code-for-diet-recommendation-system</link>
      <description><![CDATA[我一直在尝试在 GitHub 上找到的这段代码：https://github .com/zakaria-narjis/Diet-Recommendation-System 用于饮食和食物建议。
它的效果很好，但食物建议（食谱中的总卡路里）通常比我每天必须摄入的饮食计划和卡路里（“计划”卡路里）更高，尤其是当它是五顿饭并且它提供的时候用正餐换零食，这没有意义，数据包含真正的零食。
我对完整代码做了一些调整，因为它一开始不起作用，但是这里是我认为有问题的函数：
defgenerate_recommendations(self,):
    总卡路里=self.weight_loss*self.calories_calculator()
    建议=[]
    self.meals_calories_perc 中的膳食：
        膳食卡路里=self.meals_calories_perc[膳食]*total_calories
        如果餐==&#39;早餐&#39;：
            推荐营养 = [膳食卡路里,rnd(10,30),rnd(0,4),rnd(0,30),rnd(0,400),rnd(40,75),rnd(4,10),rnd(0,10 ),rnd(30,100)]
        elif 餐==&#39;发射&#39;：
            推荐营养 = [膳食卡路里,rnd(20,40),rnd(0,4),rnd(0,30),rnd(0,400),rnd(40,75),rnd(4,20),rnd(0,10 ),rnd(50,175)]
        elif 餐==&#39;晚餐&#39;：
            推荐营养 = [膳食卡路里,rnd(20,40),rnd(0,4),rnd(0,30),rnd(0,400),rnd(40,75),rnd(4,20),rnd(0,10 ),rnd(50,175)]
        别的：
            推荐营养 = [膳食卡路里,rnd(10,30),rnd(0,4),rnd(0,30),rnd(0,400),rnd(40,75),rnd(4,10),rnd(0,10 ),rnd(30,100)]
        生成器=生成器（推荐_营养）
        Recommended_recipes=generator.generate().json()[&#39;输出&#39;]
        推荐.追加（推荐菜谱）
    对于推荐中的推荐：
        推荐食谱：
            食谱[&#39;image_link&#39;]=find_image(食谱[&#39;名称&#39;])
    返回建议
]]></description>
      <guid>https://stackoverflow.com/questions/78428234/python-code-for-diet-recommendation-system</guid>
      <pubDate>Sat, 04 May 2024 07:57:23 GMT</pubDate>
    </item>
    <item>
      <title>在ReactJs中将ai模型集成到chart.js中</title>
      <link>https://stackoverflow.com/questions/78428075/integration-ai-model-in-chart-js-in-reactjs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78428075/integration-ai-model-in-chart-js-in-reactjs</guid>
      <pubDate>Sat, 04 May 2024 06:56:42 GMT</pubDate>
    </item>
    <item>
      <title>这些随机森林决策边界表明什么？</title>
      <link>https://stackoverflow.com/questions/78427857/what-are-these-random-forest-decision-boundary-indicate</link>
      <description><![CDATA[从这些图像中可以推断出关于观察到的计算行为的哪些见解或含义？
在特定的二元分类问题中，我利用包含 34 个输入特征和一个目标列（包含 7428 条记录）的完整特征数据集为随机森林模型生成了决策边界图。随后，我应用了“信息增益”特征选择技术，导致包含 25 个输入特征和一个目标列的缩减数据集，但保持相同数量的记录 (7428)。通过 GridSearchCV 将具有一致超参数值的相同随机森林模型应用于两个数据集后，我发现在处理缩减后的数据集时，计算时间意外增加，尽管其特征较少。
为了清晰起见，我决定将两个数据集的决策边界可视化，并且所附图像代表这些图（只有几列）。


其余图像没有复杂的边界。
绘制示例代码为：
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns
将 pandas 导入为 pd
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.model_selection 导入 train_test_split

# 加载数据集
df = pd.read_excel(“curated_data.xlsx”)

# 将数据拆分为特征 (X) 和目标变量 (y)
X = df.drop(columns=[&#39;病原体检测结果&#39;])
y = df[&#39;病原体检测结果&#39;]

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 验证X_train和X_test中的特征数量
print(&quot;X_train 中的特征数量:&quot;, X_train.shape[1])
print(&quot;X_test 中的特征数量:&quot;, X_test.shape[1])

# 训练随机森林分类器
clf = RandomForestClassifier(n_estimators=200, max_depth=None, max_features = &#39;log2&#39;, min_samples_leaf = 1, min_samples_split = 2, random_state=42)
clf.fit(X_train, y_train)

# 绘制决策边界
plt.figure(figsize=(8, 6))

# 定义网格
x_min, x_max = X_train.iloc[:, 0].min() - 1, X_train.iloc[:, 0].max() + 1
y_min, y_max = X_train.iloc[:, 1].min() - 1, X_train.iloc[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))

# 创建一个包含所有特征的网格
mesh_data = np.column_stack((xx.ravel(), yy.ravel())) # 假设仅使用前两个特征进行绘图

# 用零扩展网格以匹配分类器期望的特征数量
对于范围 (2, 34) 内的 i：
    mesh_data = np.column_stack((mesh_data, np.zeros_like(xx.ravel())))

# 在网格上进行预测
Z = clf.predict(mesh_data)
Z = Z.reshape(xx.shape)

# 绘制决策边界
plt.contourf(xx, yy, Z, alpha=0.8)

对于 df1.columns 中的 i：
    # 绘制数据点
    sns.scatterplot(x=df1[i], y=df1[&#39;病原体检测结果&#39;],hue=y_train,palette=&#39;Set1&#39;,edgecolor=&#39;k&#39;, alpha=0.7)
    plt.title(&#39;全特征集上随机森林分类器的决策边界&#39;)
    plt.xlabel(i)
    plt.ylabel(&#39;病原体检测结果&#39;)
    plt.legend(loc=&#39;最佳&#39;)
    plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78427857/what-are-these-random-forest-decision-boundary-indicate</guid>
      <pubDate>Sat, 04 May 2024 05:16:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 tesseract OCR 检测数字</title>
      <link>https://stackoverflow.com/questions/78427754/detect-digital-numbers-using-tesseract-ocr</link>
      <description><![CDATA[我正在制作一个用于在数字测量仪器上读取数字的模型，其中结果必须是十进制数。但是，我的问题是我编写的模型无法识别十进制数字中的标点符号。有谁可以帮助我，以便我可以更好地读取 OCR 结果吗？
谢谢您
&lt;前&gt;&lt;代码&gt;导入cv2
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np
导入 pytesseract

# 预处理图像
def 预处理图像（图像路径）：
   图像 = cv2.imread(image_path)
   # 灰度
   Gray_image = cv2.cvtColor(图像, cv2.COLOR_BGR2GRAY)
   # 阈值
   _, thresh_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
   # 膨胀
   扩张图像 = 扩张（thresh_image）
   # 腐蚀
   侵蚀图像 = 侵蚀（扩张图像）
   # 开运算（先腐蚀后膨胀）
   打开的图像=打开（侵蚀的图像）
   # 噪声去除
   去噪图像 = 去除噪声（打开图像）
   返回去噪图像

# 获取灰度图像
def get_grayscale(图像):
   返回 cv2.cvtColor(图像, cv2.COLOR_BGR2GRAY)

#阈值
定义阈值（图像）：
   返回 cv2.threshold(图像, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

＃扩张
def 膨胀（图像）：
   内核 = np.ones((5,5),np.uint8)
   返回 cv2.dilate(图像、内核、迭代 = 1)
   
＃侵蚀
def 侵蚀（图像）：
   内核 = np.ones((5,5),np.uint8)
   返回 cv2.erode(图像、内核、迭代 = 1)

#opening - 腐蚀后膨胀
默认开场（图片）：
   内核 = np.ones((5,5),np.uint8)
   返回 cv2.morphologyEx(图像, cv2.MORPH_OPEN, 内核)

# 噪声去除
def remove_noise(图像):
   返回 cv2.medianBlur(图像,5)

# 图片路径
input_image_path = &#39;图像/train7.jpg&#39;

# 预处理图像
预处理图像 = 预处理图像（输入图像路径）

# 显示预处理结果
plt.imshow(preprocessed_image, cmap=&#39;灰色&#39;)
plt.title(&#39;预处理后的图像&#39;)
plt.axis(&#39;关闭&#39;)
plt.show()
pytesseract.pytesseract.tesseract_cmd = r“C:/Program Files/Tesseract-OCR/tesseract.exe”

custom_config = r&#39;--oem 3 --psm 7 -l ssd -c tessedit_char_whitelist=0123456789。&#39;
打印（&#39; -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - -&#39;）
打印（&#39;TESSERACT输出&#39;）
打印（&#39; -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - -&#39;）
打印（pytesseract.image_to_string（预处理图像，配置=自定义配置））



输出：
&lt;前&gt;&lt;代码&gt;----------------------------------------
正方体输出
----------------------------------------------------
第1485章
]]></description>
      <guid>https://stackoverflow.com/questions/78427754/detect-digital-numbers-using-tesseract-ocr</guid>
      <pubDate>Sat, 04 May 2024 04:19:09 GMT</pubDate>
    </item>
    <item>
      <title>COCO分段json格式的合并和减去注释</title>
      <link>https://stackoverflow.com/questions/78427741/merge-and-subtract-annotations-in-coco-segmentation-json-format</link>
      <description><![CDATA[我是 Python 和机器学习新手，遇到以下问题：我以 COCO .json 格式注释了数据。在这种情况下，水下照片上的珊瑚表面区域是活的，而珊瑚的部分区域是死的。有时，蒙版会重叠。
我想减去我注释为“死区”的区域来自我注释为“活着”的区域。在每张照片上，只有一个珊瑚被注释，有时我会分多个部分进行注释，因此我也想在减法之前合并每个类别的蒙版。注释类“死”了。仅出现在图像的子集中。
有人能指出我如何做到这一点的正确方向吗？
我添加了一张示例照片：黄色表示“活着”类别，绿色表示“死亡”类别。

非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/78427741/merge-and-subtract-annotations-in-coco-segmentation-json-format</guid>
      <pubDate>Sat, 04 May 2024 04:11:41 GMT</pubDate>
    </item>
    <item>
      <title>如何导入 Gensim？Pip Install 成功了，但是我无法运行 Import Gensim，否则会出现 ImportError: 无法导入名称“triu”错误</title>
      <link>https://stackoverflow.com/questions/78427675/how-do-i-import-gensim-pip-install-worked-but-i-cant-run-import-gensim-withou</link>
      <description><![CDATA[我尝试执行此 Word2Vec 代码并收到 NameError：名称“gensim”未定义：
w2v_model = gensim.models.Word2Vec(docgen, min_count=5, sg=1, seed=22122, worker=1)
当我导入 gensim 时，我收到此错误：ImportError：无法从“scipy.linalg”导入名称“triu”（/opt/conda/lib/python3.9/site-packages/scipy/linalg/init.py）
我该如何修复代码才能调用 Gensim？我需要 Word2Vec 才能运行，之前没有遇到过这个问题；我今天早上运行这个模型时一切正常，但现在突然出现错误。
我看到你必须升级 Scipy，它似乎有效。这是完整代码：
!pip install scipy==1.10.1
!pip install gensim 
from numpy import triu
import gensim
from gensim import corpora, models

w2v_model = gensim.models.Word2Vec(docgen, min_count=3, sg=1, seed=22122, worker=1)


谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78427675/how-do-i-import-gensim-pip-install-worked-but-i-cant-run-import-gensim-withou</guid>
      <pubDate>Sat, 04 May 2024 03:23:37 GMT</pubDate>
    </item>
    <item>
      <title>如何解释基于情感分析数据训练的朴素贝叶斯模型？</title>
      <link>https://stackoverflow.com/questions/78426520/how-to-explain-a-naive-bayes-model-trained-on-data-for-sentiment-analysis</link>
      <description><![CDATA[我正在编写此代码来训练朴素贝叶斯模型：
# 加载必要的库
库（readxl）
库（插入符号）
图书馆(e1071)

# 加载预处理后的TF-IDF矩阵
tfidf_df &lt;- read_excel(&#39;~/Downloads/tfidf_r.xlsx&#39;)

# 加载带有标签的原始DataFrame
df &lt;- read_excel(&#39;~/Downloads/all-review_label.xlsx&#39;)

# 将 TF-IDF 矩阵与标签 DataFrame 合并
merged_df &lt;- 合并(tfidf_df, df, by=&#39;review_id&#39;)

# 将数据分为训练集和测试集
  设置.种子(42)
  train_indices &lt;- createDataPartition(merged_df$review_id, p = 0.8, list = FALSE)
  train_data &lt;- merged_df[train_indices, ]
  test_data &lt;- merged_df[-train_indices, ]
  
  # 初始化并训练朴素贝叶斯分类器
  naive_bayes_model &lt;- naiveBayes(标签 ~ ., data = train_data)
  
  # 对测试集进行预测
  y_pred &lt;- 预测（naive_bayes_model，newdata = test_data）
  打印（y_pred）
  
  # 将预测值和实际值转换为相同水平的因子
  级别 &lt;- 唯一（c（级别（y_pred），级别（test_data$review）））
  y_pred &lt;- 因子(y_pred, 级别 = 级别)
  test_data$label&lt;- 因子(test_data$label, 级别 = 级别)

我想解释使用 SHAP 从模型中获得的结果，我使用了以下代码：
库（kernelshap）
图书馆（shapviz）

xvars &lt;- setdiff(colnames(merged_df), &quot;label...2&quot;)

# 如果 length(xvars) 大于 10，则使用 kernelshap()。对 bg_X 进行子采样至 100-500 行
shap_values &lt;- kernelshap(naive_bayes_model,
                          X = 合并_df,
                          bg_X = 合并_df,
                          功能名称 = xvars)

shap_values &lt;- shapviz(shap_values)
sv_importance(shap_values, kind = “bar”)

但是 R 显示“停止”图标几个小时，但没有给出任何结果。
如何修复它？]]></description>
      <guid>https://stackoverflow.com/questions/78426520/how-to-explain-a-naive-bayes-model-trained-on-data-for-sentiment-analysis</guid>
      <pubDate>Fri, 03 May 2024 18:57:59 GMT</pubDate>
    </item>
    <item>
      <title>为什么最终模型中的树木数量不是我指定的数量？[关闭]</title>
      <link>https://stackoverflow.com/questions/78426497/why-is-the-number-of-trees-in-the-final-model-not-the-number-that-i-specified</link>
      <description><![CDATA[我使用 quantregForest 进行分位数回归森林模型，代码如下：
opt_mdl &lt;- quantregForest(x = train[, features],
                          y = 训练[，目标]，
                          节点大小 = 5,
                          尝试= 14，
                          n树= 500，
                          nthreads = 并行::DetectCores() - 1)

其中 Train 是一个包含 1909 个数据实例和特征列的数据框，features 是要使用的 24 个特征名称的列表。这里我将ntree指定为500。但是在opt_mdl中，值ntree (opt_mdl$ntree)是 34。为什么会发生这种情况以及如何解决它？]]></description>
      <guid>https://stackoverflow.com/questions/78426497/why-is-the-number-of-trees-in-the-final-model-not-the-number-that-i-specified</guid>
      <pubDate>Fri, 03 May 2024 18:51:19 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的多线程无法在 Raspberry Pi 上正常工作</title>
      <link>https://stackoverflow.com/questions/78424618/multithreading-in-python-not-working-correctly-with-raspberry-pi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78424618/multithreading-in-python-not-working-correctly-with-raspberry-pi</guid>
      <pubDate>Fri, 03 May 2024 12:10:47 GMT</pubDate>
    </item>
    <item>
      <title>是否有可能数据集不适合构建准确的模型？</title>
      <link>https://stackoverflow.com/questions/78423313/is-there-a-possibility-dataset-is-just-not-suitable-for-building-accurate-model</link>
      <description><![CDATA[我正在尝试通过使用糖尿病健康指标数据集（https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset）。我使用 Azure 机器学习工作室（经典），在那里我尝试了“训练模型”和“训练模型”。和“调整模型超参数” （随机扫描）方法，但我无法达到 cca 72% 以上的准确性和/或其他指标。数据集是平衡的（0 为 35346 个实例，1 为 35346 个实例），其大多数特征只是 0 或 1。我删除了其中一些特征，它们对糖尿病预测并不重要（CholCheck、AnyHealthCare、NoDocbcCost、教育、收入）。&lt; /p&gt;
https://i.ibb.co/Wn5cSY9/Bez-naslova.png 
该数据集是否适合准确预测，或者我必须改变解决问题的方法？]]></description>
      <guid>https://stackoverflow.com/questions/78423313/is-there-a-possibility-dataset-is-just-not-suitable-for-building-accurate-model</guid>
      <pubDate>Fri, 03 May 2024 07:42:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在短时间内建立准确的数据集？</title>
      <link>https://stackoverflow.com/questions/78418098/how-can-i-build-an-accurate-dataset-in-a-short-span-of-time</link>
      <description><![CDATA[我们正在开发一款 iOS 应用，让用户可以发送可定制的数字卡片。用户可以从各种卡片模板中进行选择，输入自己的文本，并根据自己的喜好对卡片进行编辑。我们还有一项功能，用户可以提供短信，例如“妈妈生日快乐”，并收到文本的扩展版本，例如“祝我特别的母亲生日快乐！”我爱你，希望你度过愉快的一天。”
我正在研究如何实现这一目标，并计划使用自然语言处理 (NLP) 和 CoreML 创建一个模型。但是，我在为该特定任务寻找合适的数据集时遇到了问题。因此，我有兴趣构建专门为此目的而定制的准确数据集。但是，我不确定从哪里可以获得必要的数据，或者是否有其他数据源可供快速使用。
如果您有任何见解或替代方法来实现此功能，请分享。]]></description>
      <guid>https://stackoverflow.com/questions/78418098/how-can-i-build-an-accurate-dataset-in-a-short-span-of-time</guid>
      <pubDate>Thu, 02 May 2024 09:18:54 GMT</pubDate>
    </item>
    <item>
      <title>小数据集上 R 平方得分变化极大</title>
      <link>https://stackoverflow.com/questions/50630012/extremely-varying-r-squared-score-on-small-dataset</link>
      <description><![CDATA[我目前正在进行回归任务。我们收到了一个非常小的数据集，由 47 个数据点组成，具有 2 个特征和 1 个目标值。它看起来像这样：
N级，品种，植株重量(g)
L，布朗尼，0.3008
L，布朗尼，0.3288
M，布朗尼，0.3304
M，布朗尼，0.388
M，布朗尼，0.406
H,布朗尼,0.3955
H,布朗尼,0.3797
H,布朗尼,0.2962

每个植物有 3L、3M 和 3H（因此每个植物有 9 个）。任务是获得最佳 r 平方分数，但有 6 个数据点被保留（从我得到的数据集中删除了 6 个数据点，这意味着对于每朵花（有 6 个），L、M 或 H 中的一个数据点被删除。正如您在示例 abvoe 中看到的，“brownii”中的一个 L 被删除了，我尝试了几种回归算法，尝试了 KFolds、LeaveOneOut 并手动分割数据集，但似乎数据集太小，以至于取决于测试。数据，结果变化很大。在某些测试数据上我可以得到 0.95 的分数，但在某些测试数据上我可能只能得到 0.2。

有什么方法可以实现一致性吗？ ]]></description>
      <guid>https://stackoverflow.com/questions/50630012/extremely-varying-r-squared-score-on-small-dataset</guid>
      <pubDate>Thu, 31 May 2018 17:46:51 GMT</pubDate>
    </item>
    </channel>
</rss>