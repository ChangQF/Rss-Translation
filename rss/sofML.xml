<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 14 May 2024 09:15:33 GMT</lastBuildDate>
    <item>
      <title>如何修改 Pytorch ImageFolder 分配标签的方式</title>
      <link>https://stackoverflow.com/questions/78476619/how-to-modify-how-pytorch-imagefolder-assigns-labels</link>
      <description><![CDATA[我有一个数据集存储在这样的目录中：

我希望属于同一图像目录中同一特征目录的所有图像都获得相同的标签。分配给图像的标签本质上应该基于“图像#”和“图像#”的组合。和“feature_#”目录。如果不同图像目录中的图像共享相同的功能目录名称，我不希望为它们分配相同的标签。有没有一种简单的方法来修改 ImageFolder 分配标签的方式？]]></description>
      <guid>https://stackoverflow.com/questions/78476619/how-to-modify-how-pytorch-imagefolder-assigns-labels</guid>
      <pubDate>Tue, 14 May 2024 08:04:53 GMT</pubDate>
    </item>
    <item>
      <title>通过系数和截距进行线性回归---</title>
      <link>https://stackoverflow.com/questions/78476584/linear-regression-by-coefficieents-and-intercept</link>
      <description><![CDATA[我想通过系数和截距绘制线性回归。我在 Jupiter 笔记本上看到了它的代码。但我无法理解其中的特殊部分。这是代码：
plt.scatter(train.ENGINESIZE, train.CO2EMISSION, color=&#39;blue&#39;)
plt.plot (train_x, regr.coef_[0][0]*train_x +regr.intercept_[0], &#39;-r&#39;)

为什么它不像 regr.coef_*train_x +regr.intercept_]]></description>
      <guid>https://stackoverflow.com/questions/78476584/linear-regression-by-coefficieents-and-intercept</guid>
      <pubDate>Tue, 14 May 2024 07:57:40 GMT</pubDate>
    </item>
    <item>
      <title>我想从雅虎源获取数据，但遇到一些错误。这是我的代码</title>
      <link>https://stackoverflow.com/questions/78476065/i-wanted-to-get-the-data-from-yahoo-source-but-i-am-encountering-some-error-her</link>
      <description><![CDATA[导入数学
将 numpy 导入为 np
将 pandas 导入为 pd
将 yfinance 导入为 yf
导入 pandas_datareader 作为数据
将日期时间导入为 dt
从 keras.models 导入顺序
从 keras.layers 导入密集，LSTM

开始日期 = dt.datetime(2006, 1, 1)
end_date = dt.datetime(2016, 1, 1)

df = data.DataReader(&#39;FPT&#39;, data_source = &#39;yahoo&#39;, start = start_date, end = end_date)
df

这是错误
AttributeError Traceback（最近一次调用最后一次）
&lt;ipython-input-13-0e1792ee66db&gt;在&lt;细胞系：4&gt;()
      2 结束日期 = dt.datetime(2016, 1, 1)
      3
----&gt; 4 df = data.DataReader(&#39;FPT&#39;, data_source = &#39;yahoo&#39;, start = start_date, end = end_date)
      5 df

3帧
/usr/local/lib/python3.10/dist-packages/pandas_datareader/yahoo/daily.py 在 _read_one_data(self, url, params)
    第150章 150
    151 尝试：
--&gt; 152 j = json.loads(re.search(ptrn, resp.text, re.DOTALL).group(1))
    [第 153 回]
    154 除了KeyError：

AttributeError：“NoneType”对象没有属性“group”

我想使用来自雅虎的数据。我也知道我可以使用“yfinance”但我想尝试“DataReader”。我非常欣赏]]></description>
      <guid>https://stackoverflow.com/questions/78476065/i-wanted-to-get-the-data-from-yahoo-source-but-i-am-encountering-some-error-her</guid>
      <pubDate>Tue, 14 May 2024 06:14:56 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：CUDA 错误：设备端断言已触发</title>
      <link>https://stackoverflow.com/questions/78475975/runtimeerror-cuda-error-device-side-assert-triggered</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78475975/runtimeerror-cuda-error-device-side-assert-triggered</guid>
      <pubDate>Tue, 14 May 2024 05:51:57 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：用户偏好向量和餐厅特征向量必须具有相同的维数</title>
      <link>https://stackoverflow.com/questions/78475573/valueerror-user-preference-vector-and-restaurant-feature-vectors-must-have-the</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78475573/valueerror-user-preference-vector-and-restaurant-feature-vectors-must-have-the</guid>
      <pubDate>Tue, 14 May 2024 03:27:58 GMT</pubDate>
    </item>
    <item>
      <title>我该如何解决这个类型错误？</title>
      <link>https://stackoverflow.com/questions/78475048/how-can-i-solve-this-typeerror</link>
      <description><![CDATA[我正在研究我的第一个完整机器学习，我现在正在尝试处理原始数据，将其转换为监督学习。
在其中一个步骤中，为了重新编码数据：我使用了重新编码，然后使用了 apply 方法并通过采样字典指定不同列的聚合函数。
如下：
导入 pandas 作为 pd
从全局导入全局

#------------------------------------------------- -------------
# 转成函数
#------------------------------------------------- -------------

文件 = glob(“../../data/raw/MetaMotion/MetaMotion/*.csv”)
data_path =“../../data/raw/MetaMotion/MetaMotion”

def read_data_from_files(文件):
    
    acc_df = pd.DataFrame()
    gyr_df = pd.DataFrame()
    
    acc_set = 1
    陀螺仪设置 = 1
    
    对于文件中的 f：
        参与者 = (f.split(&quot;-&quot;)[0].replace(data_path, &quot;&quot;))[-1]
        标签 = f.split(&quot;-&quot;)[1]
        类别 = f.split(“-”)[2].rstrip(“123”).rstrip(“_MetaWear_2019”)

        df = pd.read_csv(f)
        
        df[“参与者”] = 参与者
        df[“标签”] = 标签
        df[“类别”] = 类别

        如果“加速度计”是在 f 中：
            df[“设置”] = acc_set
            acc_set =+ 1
            acc_df = pd.concat([acc_df, df])
            
        如果“陀螺仪”在 f 中：
            df[“设置”] = gyr_set
            gyr_set =+ 1
            gyr_df = pd.concat([gyr_df, df])

        
    acc_df.index = pd.to_datetime(acc_df[“纪元 (毫秒)”], 单位=“毫秒”)
    gyr_df.index = pd.to_datetime(gyr_df[“纪元 (毫秒)”], 单位=“毫秒”)
    
    del acc_df[“纪元（毫秒）”]
    del acc_df[“时间 (01:00)”]
    del acc_df[“经过时间”]

    del gyr_df[“纪元（毫秒）”]
    del gyr_df[“时间 (01:00)”]
    del gyr_df[“经过时间”]

    返回 acc_df、gyr_df


acc_df, gyr_df = read_data_from_files(文件)

#------------------------------------------------- -------------
# 合并数据集
#------------------------------------------------- -------------


data_merged = pd.concat([acc_df.iloc[:,:3], gyr_df], axis=1)
data_merged.dropna()
data_merged.info()


#重命名列
data_merged.columns = {
    “acc_x”，
    “acc_y”，
    “acc_z”，
    “gyr_x”，
    “gyr_y”，
    “gyr_z”，
    “标签”，
    “类别”，
    “参与者”，
    “设定”，
}


#------------------------------------------------- -------------
# 重新采样数据（变频）
#------------------------------------------------- -------------

# 加速度计：12.500HZ
# 陀螺仪：25.000Hz
 
采样={
    “acc_x”：“平均值”，
    “acc_y”：“平均值”，
    “acc_z”：“平均值”，
    “gyr_x”：“平均值”，
    “gyr_y”：“平均值”，
    “gyr_z”：“平均值”，
    “标签”：“最后”，
    “类别”：“最后”，
    “参与者”：“最后”，
    “设置”：“最后”，
}

(data_merged[:1000].resample(rule=“200ms”)).apply(采样)
resampled_data = data_merged[:1000].resample(rule=&quot;200ms&quot;).agg(采样, numeric_only=False)

]]></description>
      <guid>https://stackoverflow.com/questions/78475048/how-can-i-solve-this-typeerror</guid>
      <pubDate>Mon, 13 May 2024 22:37:37 GMT</pubDate>
    </item>
    <item>
      <title>OSError：[model] 似乎没有名为 config.json 的文件</title>
      <link>https://stackoverflow.com/questions/78474448/oserror-model-does-not-appear-to-have-a-file-named-config-json</link>
      <description><![CDATA[我想加载一个拥抱模型。 我要加载的模型大约有 15 万次下载所以我不认为模型本身有什么问题。
使用下面的两个加载代码我得到相同的错误：
从变压器导入 AutoModel
AutoModel.from_pretrained(“laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup”)

还有
从 Transformers 导入 CLIPProcessor、CLIPModel
model_id =“laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup”
处理器 = CLIPProcessor.from_pretrained(model_id)
模型 = CLIPModel.from_pretrained(model_id)

两者我都得到：
OSError：laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup 似乎没有名为 preprocessor_config.json 的文件。查看“https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup/main”以获取可用文件。

任何加载模型的帮助将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78474448/oserror-model-does-not-appear-to-have-a-file-named-config-json</guid>
      <pubDate>Mon, 13 May 2024 19:38:47 GMT</pubDate>
    </item>
    <item>
      <title>如何创建我自己的自定义输入器以在 pyspark.ml 管道中无缝输入常量值</title>
      <link>https://stackoverflow.com/questions/78472581/how-to-create-my-own-custom-imputter-to-input-constant-values-seamlessly-in-pysp</link>
      <description><![CDATA[我想通过 CV 搜索来优化数据集上缺失值的插补。这在我熟悉的 sklearn 中是微不足道的——但是，我是第一次使用集群分布式 Spark 数据帧，并且必须使用 pyspark.ml 模块。
据我所知， pyspark.ml.feature.Imputer 类无法估算（选择）常量值，这是我想测试的一件事。
您建议我如何执行此操作？我研究了编写一个自定义转换器，这在 sklearn API 中也很容易，但我还没有在 pyspark.ml 中找到明确的方法来做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/78472581/how-to-create-my-own-custom-imputter-to-input-constant-values-seamlessly-in-pysp</guid>
      <pubDate>Mon, 13 May 2024 13:26:04 GMT</pubDate>
    </item>
    <item>
      <title>使用随机森林模型的多目标预测</title>
      <link>https://stackoverflow.com/questions/78471569/multi-objective-prediction-using-random-forest-model</link>
      <description><![CDATA[以下是实际流程

原始混凝土配合比的实验数据为1000块，采用的算法模型为随机森林回归模型。

以下代码用于创建模型、训练模型、预测目标值和优化 Optuna。


通过多重预测多个输出项时如何优化参数，使RMSE拟合值接近0输入项目？ 
RMSE的拟合指数为27.781625571862275。]]></description>
      <guid>https://stackoverflow.com/questions/78471569/multi-objective-prediction-using-random-forest-model</guid>
      <pubDate>Mon, 13 May 2024 10:17:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv5 标记对象的正确方法</title>
      <link>https://stackoverflow.com/questions/78471070/correct-way-to-tag-objects-with-yolov5</link>
      <description><![CDATA[我需要标记一系列图像以用于织物上的缝纫检测。我使用 YOLOv5 算法。
我遇到的问题是，我不清楚标记这些缝纫的最佳方式应该是什么。
下图显示了织物中的缝线。

正如您在图片中看到的，缝线总是会占据布料的整个宽度。最初，我曾想过对缝纫的几个部分/部分进行标记（检测到的缝纫数量并不重要，对我来说真正重要的是它检测到至少有一个缝纫）。下图展示了这个想法：

但是，我不清楚这是否是正确的方法（而是最佳方法），或者应该创建一个完全包围缝纫的单个标签。
另一方面，根据文档中给出的标签提示，标签应该恰好包围要检测的对象，在对象和标签的边界框之间留出尽可能小的空间。
&lt;块引用&gt;
标签准确性。标签必须紧密包围每个对象。没有空间
应该存在于对象与其边界框之间。没有物体
应该缺少标签。

获得最佳训练结果的提示
在这种特殊情况下，考虑到缝线总是以非常相似的方式出现（它们总是具有水平方向），这些标签将非常薄（高度很小），因此不清楚我认为该算法将能够检测到它们。以我的拙见，我认为稍微增加标签的高度将使算法更有效地检测缝纫，因为通过这些缝纫连接的织物可能具有相同的颜色。 （第二张图片显示了我正在谈论的想法）。
如果您能帮助我并告诉我进行此标记的最佳方法，我将不胜感激。
提前非常感谢您。]]></description>
      <guid>https://stackoverflow.com/questions/78471070/correct-way-to-tag-objects-with-yolov5</guid>
      <pubDate>Mon, 13 May 2024 08:48:35 GMT</pubDate>
    </item>
    <item>
      <title>如何将 standardscaler() 用于具有多列的单行的 Predict() 函数？</title>
      <link>https://stackoverflow.com/questions/78469729/how-to-use-standardscaler-for-the-predict-function-for-a-single-row-having-m</link>
      <description><![CDATA[我正在尝试建立一个房价预测系统。数据具有异常值并且是非高斯的，并且对于目标特征y，使用对数变换。在进行 one-hot 编码之前，我已经使用 StandardScaler() 来适应我的模型。代码如下所示：
numerical_features = df4[[&#39;bhk&#39;, &#39;area&#39;, &#39;price_lakhs&#39;, &#39;price_per_sqft&#39;]]
categorical_features = df4.select_dtypes(include=[&#39;object&#39;])

定标器=标准定标器()
Standardized_features = scaler.fit_transform(numerical_features)

std_df4 = pd.DataFrame(standardized_features, columns=numerical_features.columns)
std_df4.head()

现在为了预测新值，我使用了 predict_price() 函数。我很难理解如何像上面的代码块一样做到这一点。我将数值和分类值分开。我不能在下面做同样的事情。这段代码工作错误，我认为列 x[3:] 中的一个热编码值也可能已被缩放，这不是上层代码的工作方式。我使用的任何回归模型（下面代码中的 clf.predict()）对于下面 predict() 输入的不同值都会给出相同的答案。
def Predict_price（bhk，面积，price_per_sqft，类型，区域）：
    
    house_type_loc_index = np.where(X.columns == &#39;type_&#39; + type)[0][0]
    打印（房屋类型位置索引）
    
    Region_loc_index = np.where(X.columns == &#39;region_&#39; + 区域)[0][0]
    打印（region_loc_index）

    x = np.zeros(len(X.columns))
    x[0] = bhk
    x[1] = 面积
    x[2] = 每平方英尺价格
    
    如果 house_type_loc_index &gt;= 0：
        x[房屋类型位置索引] = 1
        
    如果region_loc_index &gt;= 0：
        x[区域位置索引] = 1
    
    列 = X.列
    x = x.reshape(1, -len(列))

    定标器=标准定标器()
    标准化特征 = 缩放器.fit_transform(x)
    数据 = pd.DataFrame(standardized_features, columns = columns)
    
    打印（数据）
    
    ans = clf.predict(数据)[0]
    返回exp(ans)

我期望模型能够根据我给预测函数的输入来预测值。预测函数的调用如下。
predict_price(bhk = 2，面积 = 2000，price_per_sqft = 35，类型 = &#39;公寓&#39;，区域 = &#39;Airoli&#39;)

我得到的答案是：42.610385322858455
predict_price(bhk = 3，面积 = 600，price_per_sqft = 70，类型 = &#39;别墅&#39;，区域 = &#39;Vashi&#39;)

我得到的答案是：42.610385322858455
我进一步检查了上面的 predict_price() 行，它收到的每个值都是 0。这就是为什么我强烈认为我在 predict() 中错误地使用了 standardScaler()。
bhk面积价格_每尺户型_公寓户型_独立屋\
0 0.0 0.0 0.0 0.0 0.0

   类型_顶层公寓类型_单间公寓类型_别墅区_阿格里帕达\
0 0.0 0.0 0.0 0.0

   地区_艾罗利 ... 地区_瓦赛 地区_瓦希 地区_维赫罗利 \
0 0.0 ... 0.0 0.0 0.0

   地区_Ville Parle East 地区_Ville Parle West 地区_Virar \
0 0.0 0.0 0.0

   地区_Virar West 地区_Wadala 地区_Worli 地区_other
0 0.0 0.0 0.0 0.0
]]></description>
      <guid>https://stackoverflow.com/questions/78469729/how-to-use-standardscaler-for-the-predict-function-for-a-single-row-having-m</guid>
      <pubDate>Mon, 13 May 2024 01:25:36 GMT</pubDate>
    </item>
    <item>
      <title>Kohonen 神经网络用于确定点平面的象限</title>
      <link>https://stackoverflow.com/questions/78456978/kohonen-neural-network-for-determining-the-quadrant-of-a-point-plane</link>
      <description><![CDATA[我需要实现 Kohonen 神经网络（没有老师）来确定点的平面（从 1 到 8）的象限。坐标 [x, y, z] 的向量被馈送到神经网络的输入。在输出中，我们得到一个向量，通过该向量可以确定该点属于哪个象限。
这是我训练神经网络的代码：
将 numpy 导入为 np
将 numpy.typing 导入为 npt
将 matplotlib.pyplot 导入为 plt


def 生成数据集（
        大小：整数，
        高：浮动= 10.0
）-&gt;列表[元组[npt.NDArray[npt.NDArray[浮点]], npt.NDArray[npt.NDArray[浮点]]]]:
    数据集 = []

    象限乘数 = {
        1: (1, 1, 1),
        2: (-1, 1, 1),
        3: (-1, -1, 1),
        4: (1, -1, 1),
        5: (1, 1, -1),
        6: (-1, 1, -1),
        7：（-1，-1，-1），
        8：（1，-1，-1）
    }

    对于范围 (1, 8 + 1) 中的象限：
        数据集 += [
            （
                np.array([[x, y, z]]) *quadrants_multipliers[象限],
                np.eye(8)[象限 - 1].reshape(1, 8)
            ) 对于 zip 中的 x、y、z(
                np.random.uniform（低= 0.0，高=高，大小=大小），
                np.random.uniform（低= 0.0，高=高，大小=大小），
                np.random.uniform(低=0.0，高=高，大小=大小)
            ）
        ]

    返回数据集

定义火车（
        数据集：list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]],
        学习率：浮动，
        纪元：int
）-&gt; npt.NDArray[npt.NDArray[浮点]]：

    W = np.random.randn(3, 8)

    对于范围（1，纪元 + 1）中的纪元：
        random_indexes = np.random.choice(len(数据集)，size=len(数据集)，replace=False)
        对于 random_indexes 中的索引：
            x = 数据集[索引][0] # [[x, y, z]]
            y = 数据集[索引][1]

            x_normalized = x / np.sqrt(np.sum(x ** 2))

            z = np.dot(x_归一化，W)

            W[0][z.argmax()] += 学习率 * (x[0][0] - W[0][z.argmax()])
            W[1][z.argmax()] += 学习率 * (x[0][1] - W[1][z.argmax()])
            W[2][z.argmax()] += 学习率 * (x[0][2] - W[2][z.argmax()])

        学习率 = 学习率 / 时期

    返回W

def calc_accuracy(
        数据集：list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]],
        W: npt.NDArray[npt.NDArray[float]]) -&gt;漂浮：
    正确 = 0
    对于数据集中的 x、y：
        z = np.dot(x, W)
        如果 z.argmax() == y.argmax():
            正确+=1
    返回（正确/len（数据集））* 100


def draw_accuracy_epoch_plots(
        数据集：list[list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]]],
        epoch_number：int
）-&gt;没有任何：
    对于索引，枚举中的数据集（数据集）：
        准确度列表 = []
        准确度列表_2 = []
        对于范围（1，epoch_number + 1）中的纪元：
            W = 训练（数据集，learning_rate=0.7，epochs=epochs）
            precision_list.append(calc_accuracy(数据集，W))
            precision_list_2.append(calc_accuracy(generate_dataset(15), W))

        plt.subplot(3, 2, (2 * 索引) + 1)
        plt.plot(accuracy_list)

        plt.subplot(3, 2, (2 * 索引) + 2)
        plt.plot(accuracy_list_2)


def main() -&gt;;没有任何：
    draw_accuracy_epoch_plots([generate_dataset(250),generate_dataset(500),generate_dataset(1000)],200)

    plt.tight_layout()
    plt.show()


如果 __name__ == “__main__”：
    主要的（）


在代码中，我计算了不同时期神经网络预测的准确性。结果，我得到的准确度为 0 到 45%，这不适合我。同时，神经网络经常产生 0% 的匹配。
如果更改学习率和历元没有帮助，我该如何解决这个问题？我可以添加图层吗（我还没有找到这个问题的答案）？或者也许我错误地构建了学习算法？
注意：
我昨天创建了准确度图表 - 花了大约半小时，也许更多。 Y 轴上可以看到精度，X 轴上可以看到纪元。左边是不同训练选择的图表（此类样本中的点数在图表上方签名），在右边的图表中，我们使用从左边图表中的样本获得的权重进行新的测试样本200点。在我看来，情况不应该如此，日程安排本质上应该增加，但事实并非如此。
]]></description>
      <guid>https://stackoverflow.com/questions/78456978/kohonen-neural-network-for-determining-the-quadrant-of-a-point-plane</guid>
      <pubDate>Thu, 09 May 2024 21:05:03 GMT</pubDate>
    </item>
    <item>
      <title>MLPerf Docker 容器</title>
      <link>https://stackoverflow.com/questions/78419544/mlperf-docker-container</link>
      <description><![CDATA[我正在尝试复制 MLPerf 任务，但无法弄清楚他们想要哪个 Docker 容器。我对 Docker 的经验很少；我错过了什么？
情况是这样的：MLPerf 是一组机器学习基准。我对图像分类训练任务感兴趣。他们所有训练任务的参考实现可在 https://github.com/mlcommons/training/ 获取tree/master，特别是 install_cuda_docker。 sh 脚本。
但是，我找不到任何对我应该获取的 docker 容器的引用。 MLCommons（生产 MLPerf 的组织）拥有一系列 docker 容器，但没有一个描述似乎合适。 （最接近的是图像分割 docker 图像，但是（1）这是一个不同的问题，（2）它是为 PyTorch 构建的，但我知道参考图像分类代码使用 TensorFlow。我还偶然发现了对 inference&lt; 的引用/em&gt; 容器，但想必这些也不适合训练。）
顺便说一句，其他一些训练任务目录（例如rnn_speech_recognition或graph_neural_network）包含Dockerfile，但图像分类目录中没有Dockerfile。]]></description>
      <guid>https://stackoverflow.com/questions/78419544/mlperf-docker-container</guid>
      <pubDate>Thu, 02 May 2024 13:37:09 GMT</pubDate>
    </item>
    <item>
      <title>学习曲线是否表明过度拟合或模型性能处于可接受的水平？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78411015/does-the-learning-curve-suggest-overfitting-or-an-acceptable-level-of-model-perf</link>
      <description><![CDATA[
学习曲线是否表明模型性能过度拟合或处于可接受的水平？结果基于xgboost。我需要重新调整超参数吗？如果是的话，如何调整超参数？目前，我使用 scikit-optimize 中的 BayesSearchCV 来自动调整超参数。我的搜索空间是
from skopt.space import Real, Integer
search_spaces = {&#39;learning_rate&#39;: Real(0.0001, 0.04, &#39;统一&#39;),
                 &#39;最大深度&#39;: 整数(2, 20),
                 &#39;子样本&#39;: Real(0.1, 1.0, &#39;均匀&#39;),
                 &#39;colsample_bytree&#39;: Real(0.1, 1.0, &#39;uniform&#39;), # 按树的列子样本比例
                 &#39;reg_lambda&#39;: Real(1e-9, 100., &#39;uniform&#39;), # L2 正则化, 默认 = 0
                 &#39;reg_alpha&#39;: Real(1e-9, 100., &#39;uniform&#39;), # L1 正则化, 默认 = 0
                 &#39;n_estimators&#39;: Integer(100, 3000), # boosting 轮数或决策树数
                 &#39;min_child_weight&#39;: Real(2, 8, &#39;统一&#39;),
                 &#39;伽玛&#39;：真实（0.1，0.9，&#39;均匀&#39;）
}
]]></description>
      <guid>https://stackoverflow.com/questions/78411015/does-the-learning-curve-suggest-overfitting-or-an-acceptable-level-of-model-perf</guid>
      <pubDate>Tue, 30 Apr 2024 21:03:33 GMT</pubDate>
    </item>
    <item>
      <title>深度学习Nan丢失原因</title>
      <link>https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons</link>
      <description><![CDATA[什么会导致卷积神经网络发散？
具体：
我正在使用 Tensorflow 的 iris_training 模型和我自己的一些数据，并不断得到
&lt;块引用&gt;
错误：tensorflow：模型发散，损失 = NaN。
回溯...
tensorflow.contrib.learn.python.learn.monitors.NanLossDuringTrainingError：训练期间的 NaN 损失。

回溯源自行：
 tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                        隐藏单位=[300, 300, 300],
                                        #optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.001, l1_regularization_strength=0.00001),
                                        n_classes=11,
                                        model_dir=&quot;/tmp/iris_model&quot;)

我尝试过调整优化器，使用零学习率，并且不使用优化器。]]></description>
      <guid>https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons</guid>
      <pubDate>Fri, 14 Oct 2016 19:07:18 GMT</pubDate>
    </item>
    </channel>
</rss>