<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 13 Jan 2025 06:25:56 GMT</lastBuildDate>
    <item>
      <title>使用 FLOPS 和 FLOP 计算 GPU 执行时间</title>
      <link>https://stackoverflow.com/questions/79351271/calculate-gpu-excution-time-of-with-flops-and-flops</link>
      <description><![CDATA[我正在对深度学习模型进行性能分析，并想验证以下计算执行时间的公式：
公式：时间（秒）= 模型 FLOP/GPU FLOP 每秒
上下文：
我有一个总共 24,029,362,176 FLOP 的模型。
我正在两个不同的 GPU 上对其进行测试：

NVIDIA A100 (80 GB)，峰值性能为 312 TFLOP（FP16）。
NVIDIA Tesla T4（Colab GPU），峰值性能为 8.1 TFLOP（FP32）。

使用该公式，我计算了这些 GPU 的执行时间。但是，我想确认一下这个公式是否适用于估算比较 2 个 GPU 的执行时间。
这个公式对于估算执行时间模型是否准确？
]]></description>
      <guid>https://stackoverflow.com/questions/79351271/calculate-gpu-excution-time-of-with-flops-and-flops</guid>
      <pubDate>Mon, 13 Jan 2025 05:53:50 GMT</pubDate>
    </item>
    <item>
      <title>非互斥事件/结果的学习模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/79349814/learning-models-for-non-mutually-exclusive-events-outcomes</link>
      <description><![CDATA[我有以下数据框（长格式），记录了IQ、Hours（学习小时数）和Score（不同班级（Class_ID）学生 1、2、3、4 的过去考试成绩），我想使用这些特征来预测谁将以班级前两名的成绩毕业（Top）
Class_ID IQ 小时 分数 前两名
1 101 10 98 1
1 99 19 80 0
1 130 3 95 1
1 100 12 88 0
2 93 5 50 0
2 103 9 88 0
2 112 12 99 1
2 200 10 100 1
3 100 12 84 0
3 102 13 88 1
3 101 17 89 0
3 102 9 98 1

因此，输入是像上面这样的数据框，输出（对于任何给定的类）是概率向量 [p_1,p_2,p_3,p_4]，其中 p_1+p_2+p_3+p_4=2（因为总会有 2 名学生进入前 2 名）。
我想问一下，对于这些非互斥的多标签学习，我们应该使用什么样的学习模型架构？以下是我尝试过的方法以及每种方法的不足之处：

最明显的选择是多标签分类。我们首先将数据框从长格式转换为宽格式：

Class_ID IQ_1 IQ_2 IQ_3 IQ_4 Hours_1 Hours_2 Hours_3 Hours_4 Score_1 Score_2 Score_3 Score_4 Top_1 Top_2 Top_3 Top_4
1 101 99 130 100 10 19 3 12 98 80 95 88 1 0 1 0 
2 93 103 112 200 5 9 12 10 50 88 99 100 0 0 1 1 
3 100 102 101 102 12 13 17 9 84 88 89 98 0 1 0 1 

然后我们实现sklearn 中的 MultiOutputClassifier 
但是，通常在多标签分类中，任何给定训练示例/行中的概率总和不等于任何固定数字。在我的特定用例中，概率 P(Top_1)+P(Top_2)+P(Top_3)+P(Top_4) 应始终等于 2。我想我们可以“标准化”概率使它们的总和为 2，但这似乎不太自然，因为例如，如果我们的结果为 [0.01, 0.00333, 0.00333, 0.00333]（即模型对任何学生进入前 2 名都不是很有信心），它将被规范化为 [1, 0.333, 0.333, 0.333]，即学生 1 进入前 2 名的概率为 100%。

另一个想法是使用排名模型对学生进行排名并选出得分最高的前 2 名学生。但是，据我所知，这些排名模型都没有给出概率。相反，他们只给出一个“分数”，并且这些分数没有自然的概率解释。 stackoverflow 的一位成员建议通过像上面那样给出训练分数 1 和 0 来学习排名，这样结果分数将是 0 到 1 之间的数字，我尝试使用 xgboost 中的排名器，但大多数分数都变成了负数，所以它似乎也不起作用。

我的最终想法是对长格式数据框中的每一行进行二元分类（首先开发新特征，以便每一行都考虑学生的智商相对于班级平均智商）：


Class_ID IQ_rel Hours_rel Score_rel Top
1 0.2349 0.2273 0.2715 1
1 0.2302 0.4318 0.2216 0
1 0.3023 0.0682 0.2632 1
1 0.2326 0.2727 0.2438 0
2 0.1831 0.1389 0.1484 0
2 0.2028 0.2500 0.2611 0
2 0.2205 0.3333 0.2938 1
2 0.3937 0.2778 0.2967 1
3 0.2469 0.1967 0.2340 0
3 0.2519 0.2131 0.2451 1
3 0.2494 0.2787 0.2479 0
3 0.2519 0.3115 0.2730 1

我们将每个学生视为一个训练样本（而不是将一个班级视为一个训练样本）然后我们将概率标准化，使它们的总和为 2。但这与多标记方法存在同样的问题，因为例如如果 p = exp(ax)/(1+exp(ax)) 和 p&#39; = exp(ax&#39;)/(1+exp(ax&#39;))，那么“标准化概率”等于 p/(p+p&#39;) = (exp(ax) + exp(a(x+x&#39;)))/(exp(ax) + exp(ax&#39;) + 2exp(a(x+x&#39;)))，这似乎没有任何自然的概率解释，而且似乎在概率中引入了偏差。
是否有专门针对上述学习任务的技巧/模型？]]></description>
      <guid>https://stackoverflow.com/questions/79349814/learning-models-for-non-mutually-exclusive-events-outcomes</guid>
      <pubDate>Sun, 12 Jan 2025 11:30:23 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 CLI 在 spaCy 训练管道中包含自定义组件？</title>
      <link>https://stackoverflow.com/questions/79348814/how-do-i-include-a-custom-component-in-a-spacy-training-pipeline-using-the-cli</link>
      <description><![CDATA[我正在尝试在我的 spaCy 训练管道中实现一个简单的自定义组件。我正在使用 spaCy CLI 进行训练，这意味着我正在通过 config.cfg 文件指导管道配置，尽管我确实有用于生成和注释训练和评估数据的脚本。我创建的自定义组件是无状态的，从各方面来看，这意味着它不需要工厂，只需使用 @Language.component() 装饰器即可。该组件只需获取 doc 对象并根据给定的一组命名实体中任何实体的出现次数为其添加分类。如果任何给定的实体类型有多个，则分类分数为 1.0；如果不是，则为 0.0。
以下是该函数的代码：
# custom_classifier.py

from spacy.language import Language

@Language.component(&quot;custom_classifier&quot;)
def custom_classifier(doc):
entity_types = [&quot;ENTITY1&quot;, &quot;ENTITY2&quot;, &quot;ENTITY3&quot;]
entity_counts = [sum(1 for ent in doc.ents if ent.label_ == entity_type) for entity_type in entity_types]

if any(count &gt; 1 for count in entity_counts):
doc.cats[&quot;MULTIPLE&quot;] = 1.0
else:
doc.cats[&quot;MULTIPLE&quot;] = 0.0

return doc

然后我尝试在我的训练管道中使用这个自定义组件，方法是将其添加到我的 config.cfg 定义中，如下所示：
[nlp]
lang = &quot;en&quot;
pipeline = [&quot;tok2vec&quot;,&quot;ner&quot;,&quot;custom_classifier&quot;]
batch_size = 1000
...

[components]
...

[components.taper_classifier]
source = &quot;custom_classifier.custom_classifier&quot;
after = &quot;ner&quot;

无论我做什么，当我尝试运行 spacy train config.cfg 时，我最终都会收到此错误消息的某个版本：
OSError：[E050] 找不到模型“custom_classifier.custom_classifier”。它似乎不是 Python 包或数据目录的有效路径。

到目前为止，我尝试了各种解决方案，但都没有奏效。这些包括：

尝试我能想到的模块路径的所有可能的排列。
将组件函数转换为工厂并相应地修改装饰器和配置。
使用 @spacy.registry 装饰器注册函数。

我一直在梳理文档，我就是想不出我做错了什么，尽管我承认文档似乎假设使用自定义组件的训练是在代码中而不是使用 CLI 完成的，并且没有详细说明如何使用 config.cfg 文件声明自定义组件。我觉得要么是我忽略了一些简单的东西，要么是我从根本上误解了这些自定义组件的工作原理。]]></description>
      <guid>https://stackoverflow.com/questions/79348814/how-do-i-include-a-custom-component-in-a-spacy-training-pipeline-using-the-cli</guid>
      <pubDate>Sat, 11 Jan 2025 20:14:52 GMT</pubDate>
    </item>
    <item>
      <title>Xavier 初始化在经过第一个隐藏层后不保持方差</title>
      <link>https://stackoverflow.com/questions/79348329/xavier-initialization-doesnt-maintain-variance-after-pass-through-first-hidden</link>
      <description><![CDATA[一直在尝试初始化不同的权重。目前使用 MNIST 数据集，其中有 1 个隐藏层，每个层有 256 个神经元，并使用 tanh 激活。输入为零均值和单位方差。
因此，我假设在第一层之后的第一次传递之后，方差将大致相同或乘以我们提供的增益。
因此，如果输入方差为 1，则 fc1 之后的方差应为 1*5/3 或接近 1。但我得到的方差是 489。有人能更好地启发我吗？我感觉我直觉上缺少了一些东西。我尝试了
init.xavier_normal_(self.fc1.weight,gain=nn.init.calculate_gain(&#39;tanh&#39;)) 和
init.xavier_normal_(self.fc1.weight)
import torch
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.optim as optim
import torch.nn. functional as F
import random
import torch.nn.init as init

random.seed(42)
torch.manual_seed(42)

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
# 下载并加载训练和测试集
trainset = datasets.MNIST(root=&#39;./data&#39;, train=True, download=True, transform=transform)
testset = datasets.MNIST(root=&#39;./data&#39;, train=False, download=True, transform=transform)

# 创建 DataLoader 来分批处理数据
trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)
testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)
class SimpleNN(nn.Module):
def __init__(self):
super(SimpleNN, self).__init__()
self.flatten = nn.Flatten()
self.fc1 = nn.Linear(28 * 28, 256)
self.fc2 = nn.Tanh() 
self.fc3 = nn.Linear(256, 10)
init.xavier_normal_(self.fc1.weight,gain=nn.init.calculate_gain(&#39;tanh&#39;))
init.xavier_normal_(self.fc3.weight)

def forward(self, x):
self.activations = {} 

x = self.flatten(x)
self.activations[&#39;input&#39;] = x

x = self.fc1(x)
self.activations[&#39;fc1&#39;] = x

x = self.fc2(x)
self.activations[&#39;fc2&#39;] = x

x = self.fc3(x)
self.activations[&#39;fc3&#39;] = x

return x
def analyze_layer_statistics(model):

stats = {}
for layer_name,activations in model.activations.items():
stats[layer_name] = {
&#39;mean&#39;: torch.mean(activations).item(),
&#39;var&#39;: torch.var(activations).item(),
&#39;min&#39;: torch.min(activations).item(),
&#39;max&#39;: torch.max(activations).item()
}

返回统计信息
model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(),lr=0.01)
epochs = 20
losses = []
stats = []
for epoch in range(epochs):
model.train()
total = 0
correct = 0
running_loss = 0.0
for images, labels in trainloader:
optimizer.zero_grad() 
output = model(images)
loss = criterion(output, labels)
loss.backward()
optimizer.step()
running_loss += loss
total += images.shape[0]
correct += (torch.argmax(output,dim=1)==labels).sum()
print(f&#39;Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader):.4f}&#39;)
print(f&#39;Accuracy = {(correct/total)*100}&#39;)
loss.append(running_loss/len(trainloader)) 
stats.append(analyze_layer_statistics(model))
break

输出：
stats 输出为

[{&#39;input&#39;: {&#39;mean&#39;: 0.023540694266557693,
&#39;var&#39;: 1.056724190711975,
&#39;min&#39;: -0.4242129623889923,
&#39;max&#39;: 2.821486711502075},
&#39;fc1&#39;: {&#39;平均值&#39;: 0.15851731598377228,
&#39;变量&#39;: 489.0372009277344,
&#39;最小值&#39;: -79.83320617675781,
&#39;最大值&#39;: 87.52055358886719},
&#39;fc2&#39;: {&#39;平均值&#39;: -0.0067255799658596516,
&#39;变量&#39;: 0.9763329029083252,
&#39;最小值&#39;: -1.0,
&#39;最大值&#39;: 1.0},
&#39;fc3&#39;: {&#39;平均值&#39;: -0.31016167998313904,
&#39;var&#39;: 17.94700050354004,
&#39;min&#39;: -11.344281196594238,
&#39;max&#39;: 13.964859962463379}}]

当我一次性传递整个数据集时，即一次性将整个 60000 个条目传递到第一层 - 第一次传递后的方差为 4
[{&#39;input&#39;: {&#39;mean&#39;: -0.00012828917533624917,
&#39;var&#39;: 1.0000507831573486,
&#39;min&#39;: -0.4242129623889923,
   “最大”：2.821486711502075}，
  &#39;fc1&#39;：{&#39;平均值&#39;：-0.04259666055440903，
   “变量”：3.9302892684936523，
   “分钟”：-13.193024635314941，
   “最大”：11.443111419677734}，
  &#39;fc2&#39;：{&#39;平均值&#39;：-0.012426979839801788，
   “变量”：0.6235496401786804，
   “分钟”：-1.0，
   “最大”：1.0}，
&#39;fc3&#39;: {&#39;mean&#39;: -0.197908416390419,
&#39;var&#39;: 1.3438835144042969,
&#39;min&#39;: -4.865184783935547,
&#39;max&#39;: 5.251534938812256}}]

我觉得我在初始化权重时缺少方差逻辑背后的一些直觉。]]></description>
      <guid>https://stackoverflow.com/questions/79348329/xavier-initialization-doesnt-maintain-variance-after-pass-through-first-hidden</guid>
      <pubDate>Sat, 11 Jan 2025 14:56:26 GMT</pubDate>
    </item>
    <item>
      <title>验证准确率非常低但训练准确率很高</title>
      <link>https://stackoverflow.com/questions/73410090/very-low-validation-accuracy-but-high-training-accuracy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/73410090/very-low-validation-accuracy-but-high-training-accuracy</guid>
      <pubDate>Thu, 18 Aug 2022 22:32:25 GMT</pubDate>
    </item>
    <item>
      <title>Azure 数据集 .to_pandas_dataframe() 错误</title>
      <link>https://stackoverflow.com/questions/71939604/azure-dataset-to-pandas-dataframe-error</link>
      <description><![CDATA[我正在 udemy 上学习 azure ml 课程，但无法解决以下错误：
Dataset(id=&#39;id&#39;, name=&#39;Loan Applications Using SDK&#39;, version=1, error_code=None, exception_type=PandasImportError) 的操作“to_pandas_dataframe”执行失败
以下是提交脚本的代码：
from azureml.core import Workspace, Experiment, ScriptRunConfig, 
Environment

ws = Workspace.from_config(path=&quot;./config&quot;)

new_experiment = Experiment(workspace=ws,
name=&quot;Loan_Script&quot;)

script_config = ScriptRunConfig(source_directory=&quot;.&quot;,
script=&quot;180 - Script to Run.py”）

script_config.framework = “python”
script_config.environment = Environment(&quot;conda_env&quot;)

new_run = new_experiment.submit(config=script_config)

这是正在运行的脚本：
from azureml.core import Workspace, Datastore, Dataset, 
Experiment

from azureml.core import Run

ws = Workspace.from_config(path=&quot;./config&quot;)
az_store = Datastore.get(ws, &quot;bencouser_sdk_blob01&quot;)
az_dataset = Dataset.get_by_name(ws, name=&#39;Loan Applications Using SDK&#39;)
az_default_store = ws.get_default_datastore()

#%%----------------------------------------------------
# 获取运行的上下文
#------------------------------------------------------

new_run = Run.get_context()

#%%----------------------------------------------------
# 将记录的内容
#------------------------------------------------------

df = az_dataset.to_pandas_dataframe()

total_observations = len(df)

nulldf = df.isnull().sum()

#%%----------------------------------------------------
# 完成实验
#---------------------------------------------------

new_run.log(&quot;Total Observations:&quot;, total_observations)

for columns in df.columns:
new_run.log(columns, nulldf[columns])

new_run.complete()

我在实验之外运行了 .to_pandas_dataframe() 部分，并且没有出现错误。我还尝试了以下操作（驱动程序日志中推荐的操作）：
InnerException 无法导入 pandas。通过运行以下命令确保安装了兼容版本：pip install azureml-dataprep[pandas]
我之前见过有人遇到过这种情况，但我找不到解决方案，任何帮助都值得感激。]]></description>
      <guid>https://stackoverflow.com/questions/71939604/azure-dataset-to-pandas-dataframe-error</guid>
      <pubDate>Wed, 20 Apr 2022 12:24:54 GMT</pubDate>
    </item>
    <item>
      <title>使用支持向量回归进行预测</title>
      <link>https://stackoverflow.com/questions/51350585/predictions-using-support-vector-regression</link>
      <description><![CDATA[我的问题中有四个特征 (X)；a、b、c、d 和两个从属项 (Y)；e、f。我手头有一个数据集，其中包含所有这些变量的一组值。当给出新的 a,b,c,d 值时，如何使用 scikit-learn 通过支持向量回归预测 e,f 变量的值？
我发现很难遵循 scikit-learn 文档中的 SVR。
这是我在 sklearn 文档中的示例的帮助下到目前为止所做的。
train = pd.read_csv(&#39;/Desktop/test.csv&#39;)
X = train.iloc[:, 4]
y = train.iloc[:, 4:5]

svr_rbf = SVR(kernel=&#39;rbf&#39;, C=1e3, gamma=0.1)
y_rbf = svr_rbf.fit(X, y).predict(X)
lw = 2
plt.scatter(X, y, color=&#39;darkorange&#39;, label=&#39;data&#39;)
plt.plot(X, y_rbf, color=&#39;navy&#39;, lw=lw, label=&#39;RBF model&#39;)
plt.xlabel(&#39;data&#39;)
plt.ylabel(&#39;target&#39;)
plt.title(&#39;支持向量回归&#39;)
plt.legend()
plt.show()

这会出现错误，
ValueError: 预期为 2D 数组，但得到的却是 1D 数组：
:
如果您的数据只有一个特征，则使用 array.reshape(-1, 1) 重塑数据，如果数据包含单个样本，则使用 array.reshape(1, -1)。
 ]]></description>
      <guid>https://stackoverflow.com/questions/51350585/predictions-using-support-vector-regression</guid>
      <pubDate>Sun, 15 Jul 2018 17:31:02 GMT</pubDate>
    </item>
    <item>
      <title>在 sklearn 中预测训练数据</title>
      <link>https://stackoverflow.com/questions/43210970/predict-training-data-in-sklearn</link>
      <description><![CDATA[我这样使用 scikit-learn 的 SVM：
clf = svm.SVC()
clf.fit(td_X, td_y) 

当我使用分类器预测训练集成员的类别时，即使在 scikit-learn 实现中，分类器也会出错吗（例如 clf.predict(td_X[a])==td_Y[a]）？]]></description>
      <guid>https://stackoverflow.com/questions/43210970/predict-training-data-in-sklearn</guid>
      <pubDate>Tue, 04 Apr 2017 15:04:36 GMT</pubDate>
    </item>
    <item>
      <title>weka 对旋转森林方法中的分类属性做了什么？</title>
      <link>https://stackoverflow.com/questions/29838606/what-does-weka-do-for-categorical-attributes-in-rotation-forest-method</link>
      <description><![CDATA[我有一个包含数值和分类属性的数据集。我正在 Weka 中通过旋转森林进行分类。我知道旋转森林只适用于数值属性，因为它计算 PCA 和其他东西。
我的期望是 Weka 忽略分类属性，但当我使用整个数据集进行分类时和从数据集中删除分类属性时，性能结果不同。
Weka 在旋转森林方法中对分类属性做了什么？]]></description>
      <guid>https://stackoverflow.com/questions/29838606/what-does-weka-do-for-categorical-attributes-in-rotation-forest-method</guid>
      <pubDate>Fri, 24 Apr 2015 04:16:31 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Weka 中应用特征减少方法？</title>
      <link>https://stackoverflow.com/questions/20880365/how-can-i-apply-feature-reduction-methods-in-weka</link>
      <description><![CDATA[
如何在 weka 中应用 LSI 等特征缩减方法进行文本分类？

LSI 等特征缩减方法能否提高分类的准确性？

]]></description>
      <guid>https://stackoverflow.com/questions/20880365/how-can-i-apply-feature-reduction-methods-in-weka</guid>
      <pubDate>Thu, 02 Jan 2014 10:00:30 GMT</pubDate>
    </item>
    <item>
      <title>随机森林分类 weka</title>
      <link>https://stackoverflow.com/questions/18854599/randomforest-classification-weka</link>
      <description><![CDATA[csv 文件中的属性被保存在 11 列中。如果列的顺序发生变化，Randomforest 和 RandomTree 每次给出的准确率是否不同？]]></description>
      <guid>https://stackoverflow.com/questions/18854599/randomforest-classification-weka</guid>
      <pubDate>Tue, 17 Sep 2013 16:03:37 GMT</pubDate>
    </item>
    <item>
      <title>分类器 weka 的组合</title>
      <link>https://stackoverflow.com/questions/18854042/combination-of-classifiers-weka</link>
      <description><![CDATA[我已基于每个阶段的 107 个实例、11 个特征和 2 个类别构建了三个分类器。Weka 用作机器学习工具。

第一个分类器预测类别 0 和类别 1-2-3。（所有 107 个实例均用于交叉验证方法中的训练和测试）
第二个分类器预测类别 1 和类别 2-3。（移除类别 0 的实例以进行训练和测试）
第三个分类器预测类别 2 和类别 3。（移除类别 1 的实例以进行训练和测试）

每个分类器均应用了 Randoforest。有人知道我该如何组合这三个分类器吗？]]></description>
      <guid>https://stackoverflow.com/questions/18854042/combination-of-classifiers-weka</guid>
      <pubDate>Tue, 17 Sep 2013 15:34:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 weka 进行文本分类</title>
      <link>https://stackoverflow.com/questions/9639707/text-classification-with-weka</link>
      <description><![CDATA[我正在使用 Weka 库在 Java 中构建一个文本分类器。
首先我删除停用词，然后使用词干提取器（例如将 cars 转换为 car）。
现在我有 6 个预定义类别。我针对每个类别在
5 个文档上训练分类器。文档的长度相似。
当要分类的文本很短时，结果还不错。但是当文本长度超过 100 个字时，结果会变得越来越奇怪。
我返回每个类别的概率如下：
概率：
[0.0015560238056109177, 0.1808919321002592, 0.6657404531908249, 0.004793498469427115, 
0.13253647895234325, 0.014481613481534815] 

这是一个相当可靠的分类。
但是当我使用长度超过 100 个字的文本时，我得到的结果如下：
概率： [1.2863123678314889E-5, 4.3728547754744305E-5, 0.9964710903856974, 
5.539960514402068E-5, 0.002993481218084141, 4.234371196414616E-4]

哪个好。
现在我使用朴素贝叶斯多项式对文档进行分类。我读过
关于它的内容，发现我在处理较长的文本时会表现得很奇怪。这可能是我现在的问题吗？
为什么会这样？]]></description>
      <guid>https://stackoverflow.com/questions/9639707/text-classification-with-weka</guid>
      <pubDate>Fri, 09 Mar 2012 19:16:21 GMT</pubDate>
    </item>
    <item>
      <title>如何在weka中表示文本以进行分类？</title>
      <link>https://stackoverflow.com/questions/8313426/how-to-represent-text-for-classification-in-weka</link>
      <description><![CDATA[如何在 weka 中表示文本分类的属性或类别？使用什么属性可以进行分类，词频还是仅单词？ARFF 格式的可能结构是什么？您能给我几行该结构的示例吗？]]></description>
      <guid>https://stackoverflow.com/questions/8313426/how-to-represent-text-for-classification-in-weka</guid>
      <pubDate>Tue, 29 Nov 2011 15:32:02 GMT</pubDate>
    </item>
    <item>
      <title>weka java api stringtovector 异常</title>
      <link>https://stackoverflow.com/questions/6644191/weka-java-api-stringtovector-exception</link>
      <description><![CDATA[所以我有这个使用 Weka 的 Java API 的代码：
 String html = &quot;blaaah&quot;;
Attribute input = new Attribute(&quot;html&quot;,(FastVector) null);

FastVector inputVec = new FastVector();
inputVec.addElement(input);

Instances htmlInst = new Instances(&quot;html&quot;,inputVec,1);
htmlInst.add(new Instance(1)); 
htmlInst.instance(0).setValue(0, html);

System.out.println(htmlInst);

StringToWordVector filter = new StringToWordVector();
filter.setInputFormat(htmlInst);
Instances dataFiltered = Filter.useFilter(htmlInst, filter);

但是在 filter.setInputFormat(htmlInst) 行上，Java 抱怨该函数抛出了未处理的异常...
我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/6644191/weka-java-api-stringtovector-exception</guid>
      <pubDate>Sun, 10 Jul 2011 22:35:28 GMT</pubDate>
    </item>
    </channel>
</rss>