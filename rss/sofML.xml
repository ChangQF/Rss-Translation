<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 29 Nov 2023 12:26:20 GMT</lastBuildDate>
    <item>
      <title>Llama-2、Q4-量化模型在不同CPU上的响应时间</title>
      <link>https://stackoverflow.com/questions/77570944/llama-2-q4-quantized-models-response-time-on-different-cpus</link>
      <description><![CDATA[我正在此处运行量化的 llama-2 模型。我使用两台不同的机器。

第 11 代英特尔(R) 酷睿(TM) i7-1165G7 @ 2.80GHz 2.80 GHz
16.0 GB（15.8 GB 可用）

这台机器上的推理时间非常好。我在 3-4 分钟内得到了我想要的答复

Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz 2.20 GHz（2 个处理器）
224GB

这台机器上的推理时间很长。大约需要半个小时才能给出不满意的答复。它甚至还有 Nvidia 2080-Ti GPU。 （但不使用它来加载模型的权重。
为什么会出现这种行为？ CPU如何影响性能？
我正在使用 llama_cpp python 包来加载模型。]]></description>
      <guid>https://stackoverflow.com/questions/77570944/llama-2-q4-quantized-models-response-time-on-different-cpus</guid>
      <pubDate>Wed, 29 Nov 2023 11:56:01 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中用 Transformer 替代 LSTM</title>
      <link>https://stackoverflow.com/questions/77570734/replace-lstm-with-transformer-in-neural-network</link>
      <description><![CDATA[我对 Tensorflow 的经验很少，但我正在尝试开发一个现有项目，该项目在汽车行车记录仪视频中实现了事故预测模型（预期事故）。我的目标是用 Transformer 替换原始项目中使用的 LSTM 并比较结果。使用 Tensorflow 2 可行吗？我做了一些研究，但我不确定这种变化会对代码结构和模型逻辑产生多大影响。如果有人有任何建议，我们将非常感谢任何帮助。到目前为止，我刚刚做了一些小的调整，使代码可以与 Tensorflow 2 和 Python 3 一起使用，因为原始代码已经过时了（你可以找到我的分叉存储库 此处）。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77570734/replace-lstm-with-transformer-in-neural-network</guid>
      <pubDate>Wed, 29 Nov 2023 11:26:12 GMT</pubDate>
    </item>
    <item>
      <title>如何根据特征集子集值的不可用性选择性地训练深度模型</title>
      <link>https://stackoverflow.com/questions/77570428/how-to-selectively-train-a-deep-model-based-on-the-unavailability-of-values-for</link>
      <description><![CDATA[我正在创建一个深度学习二元分类模型。数据集中的每个样本都包含两个互斥的特征集X和Y。
特征集 X 存在于所有样本中；然而，总样本中约有 45% 的特征集 Y 的值不可用。 Y 中的特征值本质上是二进制的。
我希望我的模型在推理过程中如果测试样本不包含特征集 Y 的值，则应仅对特征集 X 进行推理。如果 Y 的值可用，则推理应基于 X 和 Y。
我正在使用 PyTorch Lightning 框架进行模型架构设计和开发。
根据我的理解，我可以遵循的一种方法是“填写” Y 中的特征的一些默认值（以防它们不存在）并训练模型。但是，要填充什么值呢？应该是 0 或 1 还是任何其他类似 -1 等
另一种方法可能是创建一个名为 isYPresent 的二进制功能（例如）。如果 Y 不存在则为 0，如果存在则为 1。是否存在一种技术可以根据此新功能的值有条件地在模型训练期间（以及稍后的推理期间）处理这种情况？]]></description>
      <guid>https://stackoverflow.com/questions/77570428/how-to-selectively-train-a-deep-model-based-on-the-unavailability-of-values-for</guid>
      <pubDate>Wed, 29 Nov 2023 10:41:41 GMT</pubDate>
    </item>
    <item>
      <title>xgboost 中的形状值通过 1000 个样本的值误差</title>
      <link>https://stackoverflow.com/questions/77570420/shap-values-in-xgboost-thorughing-value-error-with-1000-samples</link>
      <description><![CDATA[您好，我正在使用下面的代码来生成形状图
defplot_shap(
    n_sample：int =无，
    n_features：int =无，
    model_output：字典=无，
    标题：str = 无
）：
    如果 n_sample 不是 None：
        X_sampled = model_output[“X_test”].sample(n_sample,random_state=10)
        打印（X_sampled.shape）
    别的：
        X_sampled = model_output[“X_test”]
        print(&#39;无:&#39;, X_sampled.shape)
    解释器 = shap.TreeExplainer(model_output[“模型”])
    打印（解释器）
    shap_values =explainer.shap_values(X_sampled, check_additivity=False)
    打印（形状值）
    plt.标题（标题）
    shap.summary_plot(
        形状值，
        X_采样，
        最大显示=20
    ）
    返回 shap_values

使用下面的代码调用此函数时，它的值错误
shap_values =plot_shap(
    n_样本=1000，
    模型输出=模型输出，
    标题=模型输出[&#39;模型名称&#39;]
）

ValueError：此重塑错误通常是由于将错误的数据矩阵传递给 SHAP 引起的。请参阅https://github.com/shap/shap/issues/580。&lt; /p&gt;
打印时在病房上打印（shap_values）失败
(1000, 360) for 语句 print(X_sampled.shape)
 for语句打印（解释器）
原始X_test包含13827行和360列。
情况紧急，请帮忙]]></description>
      <guid>https://stackoverflow.com/questions/77570420/shap-values-in-xgboost-thorughing-value-error-with-1000-samples</guid>
      <pubDate>Wed, 29 Nov 2023 10:41:00 GMT</pubDate>
    </item>
    <item>
      <title>在 kfold 交叉验证中选择适当的分割</title>
      <link>https://stackoverflow.com/questions/77570349/selecting-appropriate-splits-in-kfold-cross-validation</link>
      <description><![CDATA[我正在调整基于自行车租赁数据训练的 DecisionTreeRegressor。我的目的是学习超参数调整。
我的问题是为什么 kfold 中不同数量的分割会产生如此不同的结果，哪个结果更好以及如何决定适当的分割/折叠数量。
这是我创建的函数：
defune(param_grid, reg=DecisionTreeRegressor(random_state=2), cv=5):
    网格 = skm.GridSearchCV(reg, cv=cv, 评分=“neg_mean_squared_error”, n_jobs=-1, param_grid=param_grid)
    grid.fit(X_train, y_train)
    print(&quot;最佳参数：&quot;, grid.best_params_)

    print(&quot;CV RMSE:&quot;, np.sqrt(-grid.best_score_))

    y_pred = grid.predict(X_train)
    print(&quot;训练 RMSE:&quot;, np.sqrt(skmt.mean_squared_error(y_train, y_pred)))

    y_pred = grid.predict(X_test)
    print(&quot;测试 RMSE:&quot;, np.sqrt(skmt.mean_squared_error(y_test, y_pred)))

使用 cv=5：
tune({“min_samples_leaf”: [1,2,3,4,6,8,10,20],
      “最大深度”：[2,3,4,6,8,10,20,无]}，cv=5)

输出：
最佳参数：{&#39;max_depth&#39;: 6, &#39;min_samples_leaf&#39;: 2}
CV RMSE：870.3962060281716
训练均方根误差：537.6546032752227
测试均方根误差：912.9995795416623

使用 cv=10：
tune({“min_samples_leaf”: [1,2,3,4,6,8,10,20],
      “最大深度”：[2,3,4,6,8,10,20,无]}，cv=10)

输出：
最佳参数：{&#39;max_depth&#39;: 20, &#39;min_samples_leaf&#39;: 4}
CV RMSE：838.0643420317033
训练均方根误差：449.2062167577294
测试均方根误差：881.9551697123571
]]></description>
      <guid>https://stackoverflow.com/questions/77570349/selecting-appropriate-splits-in-kfold-cross-validation</guid>
      <pubDate>Wed, 29 Nov 2023 10:29:55 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的线性回归实现不起作用？</title>
      <link>https://stackoverflow.com/questions/77569740/why-is-my-implementation-of-linear-regression-not-working</link>
      <description><![CDATA[我正在尝试在 python 中从头开始实现线性回归。
作为参考，以下是我使用过的数学公式：方程
这是我尝试过的：
类线性回归：
    
    def __init__(
    自己，
    特征：np.ndarray[np.float64]，
    目标：np.ndarray[np.float64]，
    ）-&gt;没有任何：
        self.features = np.concatenate((np.ones((features.shape[0], 1)), features), axis=1)
        self.targets = 目标
        self.params = np.random.randn(features.shape[1] + 1)
        self.num_samples = features.shape[0]
        self.num_feats = features.shape[1]
        自我成本 = []
    
    def假设（自我）-&gt; np.ndarray[np.float64]：
        返回 np.dot(self.features, self.params)
    
    def cost_function(self) -&gt;; def cost_function(self) -&gt; np.float64：
        pred_vals = self.hypothesis()
        return (1 / (2 * self.num_samples)) * np.dot((pred_vals - self.targets).T, pred_vals - self.targets)
    
    def update(self, alpha: np.float64) -&gt;;没有任何：
        self.params = self.params - (alpha / self.num_samples) * (self.features.T @ (self.hypothesis() - self.targets))
    
    defgradientDescent(self, alpha: np.float64, 阈值: np.float64, max_iter: int) -&gt;没有任何：
        收敛=假
        计数器 = 0
        未收敛时：
            计数器 += 1
            curr_cost = self.cost_function()
            self.costs.append(curr_cost)
            自我更新（阿尔法）
            new_cost = self.cost_function()
            如果abs(new_cost - curr_cost) &lt;临界点：
                收敛=真
            如果计数器&gt;最大迭代次数：
                收敛=真

我使用了这样的类：
regr = LinearRegression(features=np.linspace(0, 1000, 200, dtype=np.float64).reshape((20, 10)), 目标= np.linspace(0, 200, 20, dtype=np.float64))
regr.gradientDescent(0.1, 1e-3, 1e+3)
regr.cost_function()

但是，我收到以下错误：
RuntimeWarning：标量幂中遇到溢出
  return (1 / (2 * self.num_samples)) * (la.norm(self.hypothesis() - self.targets) ** 4)

RuntimeWarning：标量减法中遇到无效值
  如果abs(new_cost - curr_cost) &lt;临界点：

RuntimeWarning：matmul 中遇到溢出
  self.params = self.params - (alpha / self.num_samples) * (self.features.T @ (self.hypothesis() - self.targets))

任何人都可以帮助我了解到底出了什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77569740/why-is-my-implementation-of-linear-regression-not-working</guid>
      <pubDate>Wed, 29 Nov 2023 08:54:58 GMT</pubDate>
    </item>
    <item>
      <title>tf2onnx.convert 没有 from_saved_model 属性</title>
      <link>https://stackoverflow.com/questions/77569503/tf2onnx-convert-has-no-attribute-from-saved-model</link>
      <description><![CDATA[我正在尝试将我的自定义对象检测模型 .pb 转换为 .onnx 模型
这是代码片段
&lt;前&gt;&lt;代码&gt;导入 tf2onnx

# 替换为你的路径
已保存模型路径 = &#39;/path/to/saved_model&#39;
onnx_export_path = &#39;/path/to/model.onnx&#39;

# 将 SavedModel 转换为 ONNX
tf2onnx.convert.from_saved_model（saved_model_path，output_path = onnx_export_path）

系统信息
操作系统平台和发行版：WINDOW
TensorFlow版本：2.15.1
Python版本：3.9.12
ONNX 版本（如果适用，例如 1.1.）：1.15.1
ONNXRuntime 版本（如果适用，例如 1.11）：1.16.2
重现
我读了库 tf2onnx.convert 文件，没有名为 from_saved_model 以及 from_tensorflow 的模块/函数
其他上下文
我现有的模型是使用 SSD EfficientNet-D2 和 BiFPN 进行训练
我试图将带有 BiFPN .pb 模型的自定义对象检测 EfficientNet-D2 转换为 .onnx 模型]]></description>
      <guid>https://stackoverflow.com/questions/77569503/tf2onnx-convert-has-no-attribute-from-saved-model</guid>
      <pubDate>Wed, 29 Nov 2023 08:17:46 GMT</pubDate>
    </item>
    <item>
      <title>巨大图像集的水印去除</title>
      <link>https://stackoverflow.com/questions/77568725/watermark-removal-for-a-huge-image-set</link>
      <description><![CDATA[我需要从大约 100,000 张图像中删除水印。
我最初的想法是利用卷积神经网络（CNN）来完成这项任务，但我在这方面的知识非常基础。
我正在寻求有关有效实现这一目标的最佳途径或方法的建议。]]></description>
      <guid>https://stackoverflow.com/questions/77568725/watermark-removal-for-a-huge-image-set</guid>
      <pubDate>Wed, 29 Nov 2023 05:13:16 GMT</pubDate>
    </item>
    <item>
      <title>使用 VGG16 MNIST 数字进行迁移学习</title>
      <link>https://stackoverflow.com/questions/77568420/transfer-learning-using-vgg16-mnist-digits</link>
      <description><![CDATA[我正在尝试对 MNIST 数字执行迁移学习。我有兴趣获取 logits 并将其用于基于梯度的攻击。但由于某种原因，即使我的计算机是启用了 GPU 的 Apple m2max 计算机，内核仍然会死机。我也尝试使用 GPU 进行 colab，但遇到同样的问题。该数据集不太好学，我正在重用 imagenet 权重。我该如何解决这个问题？
类 VGG16TransferLearning(tf.keras.Model)：
  def __init__(自我，基本模型，模型)：
    超级（VGG16TransferLearning，自我）.__init__（）
    #基础模型
    self.base_model = 基本模型

   # 其他层
   self.flatten = tf.keras.layers.Flatten()
   self.dense1 = tf.keras.layers.Dense(512, 激活=&#39;relu&#39;)
   self.dense2 = tf.keras.layers.Dense(512, 激活=&#39;relu&#39;)
   self.dense3 = tf.keras.layers.Dense(10)
   self.layers_list = [self.flatten, self.dense1, self.dense2, self.dense3]
  
  #用其他层实例化基础模型
  self.model = models.Sequential(
    [self.base_model, *self.layers_list]
   ）

def 调用(self, *args, **kwargs):
  激活列表 = []
  输出=参数[0]
  
  对于 self.model.layers 中的图层：
    输出 = 层（输出）
    激活列表.append(out)
  如果 kwargs[&#39;训练&#39;]:
   返回
  别的：
   概率 = tf.nn.softmax(输出)
   返回，问题

这是上面类的实例化：
base_model = VGG16(weights=“imagenet”, include_top=False, input_shape=x_train[0].shape)

base_model.trainable = False
我的输入形状是(75,75,3)
这是编译和拟合方法
从tensorflow.keras导入层、模型

模型 = VGG16TransferLearning(base_model, 模型)
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
          优化器=tf.keras.optimizers.legacy.Adam(),
          指标=[&#39;准确性&#39;])

model.fit(x_train, y_train, epochs=10,validation_data=(x_test, y_test))

这是我每次调用 fit 方法时遇到的错误：
内核重启
Untitled.ipynb 的内核似乎已经死亡。它将自动重新启动
]]></description>
      <guid>https://stackoverflow.com/questions/77568420/transfer-learning-using-vgg16-mnist-digits</guid>
      <pubDate>Wed, 29 Nov 2023 03:29:36 GMT</pubDate>
    </item>
    <item>
      <title>使用 SpaCy 的 GPE 识别非地理位置的位置</title>
      <link>https://stackoverflow.com/questions/77568059/gpe-using-spacy-identifies-locations-which-are-not-geographical-locations</link>
      <description><![CDATA[我在 NLP 代码中使用 spacy 库来识别文本中提到的重要地理位置。
但是 Spacy 正在识别不是地理位置的单词以及地理位置，例如：“BA297”、“Legal”、“Nutrigain”、“Arham”、“Stephen”还有更多。
有没有解决方案或更好的库？
这是我用来识别地理位置的代码：
# 加载spaCy英文模型
nlp = spacy.load(“en_core_web_sm”)

# 使用 spaCy 执行命名实体识别 (NER)
实体=[]
对于 df[&#39;words&#39;] 中的单词：
    文档 = nlp(单词)
    对于 doc.ents 中的 ent：
        实体.append((ent.text, ent.label_))

# 提取GPE实体
gpe_entities = [实体中的实体的实体[0]，如果实体[1] == &#39;GPE&#39;]
]]></description>
      <guid>https://stackoverflow.com/questions/77568059/gpe-using-spacy-identifies-locations-which-are-not-geographical-locations</guid>
      <pubDate>Wed, 29 Nov 2023 01:20:01 GMT</pubDate>
    </item>
    <item>
      <title>在 Python 中使用 Gensim 加载 word2vec-google-news-300 模型时出现 JSONDecodeError 和 FileNotFoundError</title>
      <link>https://stackoverflow.com/questions/77567868/jsondecodeerror-and-filenotfounderror-when-loading-word2vec-google-news-300-mode</link>
      <description><![CDATA[我尝试在 Python 中使用 Gensim 加载 word2vec-google-news-300 模型，但遇到 FileNotFoundError 和 JSONDecodeError。当 Gensim 尝试使用其下载器获取模型时，会发生错误。这是我的代码的相关部分：
导入 gensim.downloader 作为 api

模型 = api.load(“word2vec-google-news-300”)

第一条错误消息：
FileNotFoundError：[Errno 2]没有这样的文件或目录：&#39;/Users/myusername/gensim-data/information.json&#39;
第二条错误消息：
json.decoder.JSONDecodeError：期望值：第 1 行第 1 列（字符 0）
故障排除步骤

确认“/Users/myusername/gensim-data”目录确实存在，并确保它具有正确的读/写/执行权限 (rwxrwx-r)
在 gensim-data 文件夹中创建了一个空的“information.json”文件（以防 Gensim 识别它）
通过删除 gensim-data 目录清除 gensim 缓存，然后再次运行脚本，以便 Gensim 重新创建该目录
使用 pip uninstall gensim 重新安装 Gensim，然后使用 pip install gensim

尽管如此，问题仍然存在:(...我在 VSCode 中运行 Python 3.11，在 Mac M1 上运行 Gensim 4.3.2。
试图完成我的人工智能课程的作业，但这个问题占用了很多时间，所以我非常感谢任何关于如何解决这个问题的帮助或见解，谢谢！！
编辑
我还从 GoogleNews-vectors- 手动下载模型负300.bin.gz

并更改模型用法以使用下载的版本，使用 KeyedVectors 而不是 gensim.downloader 作为 api：
from gensim.models import KeyedVectors

model_path = &#39;/Users/myusername/Downloads/GoogleNews-vectors-negative300.bin.gz&#39;
模型= KeyedVectors.load_word2vec_format（model_path，二进制= True）

这似乎有效，但作业要求我“首先，使用 gensim.downloader.load 加载 word2vec-google-news-300 预训练嵌入模型”，因此即使此解决方法有效，我仍然需要弄清楚了解如何改为使用 load 方法]]></description>
      <guid>https://stackoverflow.com/questions/77567868/jsondecodeerror-and-filenotfounderror-when-loading-word2vec-google-news-300-mode</guid>
      <pubDate>Wed, 29 Nov 2023 00:09:21 GMT</pubDate>
    </item>
    <item>
      <title>ML 脚本无法正确学习 Sec2sec</title>
      <link>https://stackoverflow.com/questions/77567303/ml-script-not-learning-correctly-sec2sec</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77567303/ml-script-not-learning-correctly-sec2sec</guid>
      <pubDate>Tue, 28 Nov 2023 21:41:07 GMT</pubDate>
    </item>
    <item>
      <title>形状错误：TreeExplainer 中的可加性检查失败</title>
      <link>https://stackoverflow.com/questions/77566228/shap-error-additivity-check-failed-in-treeexplainer</link>
      <description><![CDATA[我尝试使用 shap 作为可解释的人工智能并绘制摘要，但它不断给出错误：
ExplainerError：TreeExplainer 中的可加性检查失败！请确保您传递给解释器的数据矩阵与模型训练时的形状相同。如果您的数据形状正确，请在 GitHub 上报告此情况。此检查失败，因为其中一个样本的 SHAP 值总和为 0.732971，而模型输出为 0.706032。如果这种差异可以接受，您可以设置 check_additivity=False 来禁用此检查。

这是我使用的代码：
model = ModelPipeline.named_steps[&#39;clf&#39;]
预处理 = ModelPipeline.named_steps[&#39;预处理&#39;]

x_test_pre = 预处理.transform(x_test)

model.fit(x_test_pre, y_test)

shap_values = shap.TreeExplainer(
    模型，
    数据=x_test_pre，
    model_output=&#39;原始&#39;,
    feature_perturbation=&#39;干预&#39;
).shap_values(x_test_pre)

shap.summary_plot(shap_values, x_test_pre, feature_names=preprocess.get_feature_names_out())

我在此代码中使用的模型是随机森林分类器，它具有不平衡目标数据，其管道如下所示。
ModelPipeline = imPipeline([
            (&#39;预处理&#39;, finTransformer),
            (&#39;采样&#39;,RandomUnderSampler(random_state=48)),
            (&#39;clf&#39;, finModel)
        ]）

我错过了什么步骤吗？]]></description>
      <guid>https://stackoverflow.com/questions/77566228/shap-error-additivity-check-failed-in-treeexplainer</guid>
      <pubDate>Tue, 28 Nov 2023 18:12:30 GMT</pubDate>
    </item>
    <item>
      <title>如何预测2050年家庭用水量</title>
      <link>https://stackoverflow.com/questions/77534533/how-to-predict-the-household-water-consumption-in-2050</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77534533/how-to-predict-the-household-water-consumption-in-2050</guid>
      <pubDate>Thu, 23 Nov 2023 05:31:32 GMT</pubDate>
    </item>
    <item>
      <title>将 MONAI 变换统一应用于图像序列</title>
      <link>https://stackoverflow.com/questions/77505170/applying-monai-transforms-to-sequences-of-images-uniformly</link>
      <description><![CDATA[我正在处理一个医学数据集，其中每个数据点都是 10 个图像的序列。我想将 MONAI 变换（Rand2DElastic、RandRotate、RandZoom、RandGaussianNoise）应用于这些序列以进行增强。这些变换应随机应用于每个序列，但出于一致性目的，给定序列中的每个图像应具有完全相同的变换。是否已有功能可以做到这一点？如果没有的话有什么好的方法吗？
数据存储为形状为 (n, 10, 3, 128, 128) 的 np 数组
谢谢！
images = [Image.open(os.path.join(self.root_dir, image_path)) for image_path inequence_path]
images = [np.transpose(np.array(image), (2,0,1)) 图像中的图像]

        如果自我增强：
            增强变换=撰写（[
                Rand2DElastic(概率=0.6, 间距=(30, 30), 幅度_范围=(0.1, 0.3)),
                RandRotate(range_x=np.pi / 60, prob=0.2, keep_size=True),
                RandZoom(min_zoom=0.8, max_zoom=1.5, prob=0.6),
                RandGaussianNoise（概率=0.5，平均值=0，标准=0.01）
            ]）
            图像 = Augment_transforms({“图像”: np.array(图像)})[“图像”]
            打印（图像）

这是我到目前为止所尝试的，但没有给出我需要的结果]]></description>
      <guid>https://stackoverflow.com/questions/77505170/applying-monai-transforms-to-sequences-of-images-uniformly</guid>
      <pubDate>Fri, 17 Nov 2023 23:59:40 GMT</pubDate>
    </item>
    </channel>
</rss>