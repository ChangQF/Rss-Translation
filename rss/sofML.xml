<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 12 Dec 2023 18:17:13 GMT</lastBuildDate>
    <item>
      <title>组合 2 个或更多具有不同响应时间的 ML 模型</title>
      <link>https://stackoverflow.com/questions/77648139/combine-out-of-2-or-more-ml-models-with-different-response-times</link>
      <description><![CDATA[我有几个模型，它们对提示的响应时间不同。我想将它们的输出和供应结合到不同的机器学习模型中。这些模型具有不同的运行时，因此更多的是异步方式。
我怎样才能实现这个目标？]]></description>
      <guid>https://stackoverflow.com/questions/77648139/combine-out-of-2-or-more-ml-models-with-different-response-times</guid>
      <pubDate>Tue, 12 Dec 2023 17:53:10 GMT</pubDate>
    </item>
    <item>
      <title>MNIST 的 CLIP 零样本准确率为 30%？</title>
      <link>https://stackoverflow.com/questions/77647994/zero-shot-accuracy-of-clip-for-mnist-is-30</link>
      <description><![CDATA[《Learning Transferable Visual Models From Natural Language Supervision》论文指出，CLIP 在 MNIST 上的准确率约为 88%。我从这里从 CLIP 下载了他们的实现： https://github.com/openai/CLIP
我在 MNIST 上对其进行了测试，仅获得 30% 左右的准确率。有谁知道我做错了什么？
导入火炬
导入剪辑
从 PIL 导入图像
导入火炬视觉
导入 torchvision.transforms 作为变换
将 numpy 导入为 np
设备=“cuda”； if torch.cuda.is_available() else “cpu”
模型，预处理=clip.load(“ViT-B/32”，device=device)

Training_set = torchvision.datasets.MNIST(&#39;./data&#39;,train=True,transform=preprocess,download=True)
validation_set = torchvision.datasets.MNIST(&#39;./data&#39;,train=False,transform=preprocess,download=True)

trainloader = torch.utils.data.DataLoader(training_set,batch_size=32,
                                          洗牌=真，num_workers=0）
testloader = torch.utils.data.DataLoader(validation_set,batch_size=32,
                                         洗牌=假，num_workers=0）
类 = [&#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;,
    &#39;5&#39;、&#39;6&#39;、&#39;7&#39;、&#39;8&#39;、&#39;9&#39;]

正确 = 0
总计 = 0
对于测试加载器中的 i、j：
    文本 = Clip.tokenize(classes).to(device)
    标签 = [j 中 x 的类 [x]]
    使用 torch.no_grad()：
        logits_per_image, logits_per_text = 模型(i, 文本)
        probs = logits_per_image.softmax(dim=-1).cpu().numpy()
    结果 = [np.argmax(probs, axis=1) 中 x 的类[x]]
    对于 zip 中的 i、j（结果、标签）：
        如果我==j：
            正确+=1
打印（正确/len（testloader.dataset））
]]></description>
      <guid>https://stackoverflow.com/questions/77647994/zero-shot-accuracy-of-clip-for-mnist-is-30</guid>
      <pubDate>Tue, 12 Dec 2023 17:28:36 GMT</pubDate>
    </item>
    <item>
      <title>使用部分地面实况信息的图像恢复[关闭]</title>
      <link>https://stackoverflow.com/questions/77647934/image-restoration-using-partial-ground-truth-information</link>
      <description><![CDATA[我正在开展一个图像恢复项目，其中有四张图像，每张图像都揭示了真实情况的不同部分，而其余部分则被遮挡。我还有真实图像来计算损失。
我熟悉 GAN 等较旧的架构，但尚未探索 Vision Transformers 或扩散模型。我渴望了解类似的方法或论文来解决像我这样的挑战。您能推荐一些相关的想法、技术或论文吗？
我记得读过有关 GAN 模型的文章，这些模型可以解决与此类似的任务，但我想尝试一些现代方法。]]></description>
      <guid>https://stackoverflow.com/questions/77647934/image-restoration-using-partial-ground-truth-information</guid>
      <pubDate>Tue, 12 Dec 2023 17:16:50 GMT</pubDate>
    </item>
    <item>
      <title>Python：fillna()函数输出单词“None”[重复]</title>
      <link>https://stackoverflow.com/questions/77647622/python-fillna-function-is-outputting-the-word-none</link>
      <description><![CDATA[我正在尝试为一堂课进行练习，其中我必须使用 fillna() 函数将列的缺失值替换为数据的中位数。在被称为“housing”的数据框中，有一列标题为“Mas Vnr Area”。其中有几行具有“NA”。价值观。为了填写和验证这些更改，在阅读必要的 .csv 文件后，我一直使用以下代码行（使用 PyCharm IDE）：
median_val = housing[&quot;Mas Vnr Area&quot;].median()

print(housing[&quot;Mas Vnr Area&quot;].fillna(median_val,inplace = True))

但是，每次我运行该程序时，这部分代码都只输出单词“None”。我已经多次核实“住房”是真的。里面有必要的信息，但我每次都会得到这个输出。有谁知道可能是什么原因造成的？]]></description>
      <guid>https://stackoverflow.com/questions/77647622/python-fillna-function-is-outputting-the-word-none</guid>
      <pubDate>Tue, 12 Dec 2023 16:25:12 GMT</pubDate>
    </item>
    <item>
      <title>如何使我的模型从经过训练的数据集中预测数据？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77646376/how-to-make-my-model-predicted-a-data-from-trained-dataset</link>
      <description><![CDATA[&lt;前&gt;&lt;代码&gt;输入大小 = 8
输出大小 = 1
隐藏层大小 = 50
模型 = tf.keras.Sequential([
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense(output_size, 激活=&#39;tanh&#39;)
  ]）
model.compile(optimizer=&#39;adam&#39;,loss=&#39;mean_squared_error&#39;,metrics=[&#39;accuracy&#39;])
批量大小 = 150
最大纪元 = 100
Early_stopping = tf.keras.callbacks.EarlyStopping(耐心=10)
model.fit(train_inputs_with_longtitude,
            train_targets_with_longtitude，
            批量大小=批量大小，
            纪元 = max_epochs，
            回调=[early_stopping],
            validation_data=(validation_inputs_with_longtitude ,validation_targets_with_longtitude ), # 验证数据
            详细 = 2
            ）
model.predict(test_targets_with_longtitude)

我尝试了一些方法model.predict()但它不起作用
我应该添加什么代码才能预测数据？]]></description>
      <guid>https://stackoverflow.com/questions/77646376/how-to-make-my-model-predicted-a-data-from-trained-dataset</guid>
      <pubDate>Tue, 12 Dec 2023 13:12:28 GMT</pubDate>
    </item>
    <item>
      <title>我的训练功能仅几步之后就意外结束</title>
      <link>https://stackoverflow.com/questions/77646034/my-training-function-ends-unexpectedly-after-only-a-few-steps</link>
      <description><![CDATA[我正在尝试运行 Pix2Pix，但是我的训练功能在前 1k 步中突然停止，没有错误。我使用 PyTorch 来创建鉴别器和生成器。下面是包含 2 个负责训练的函数的代码，一个用于训练每个步骤，一个用于拟合模型。
训练步骤函数：
def train_step(input_image, 目标, 步骤):
    生成器.train()
    判别器.train()

    # 前向传递
    gen_output = 生成器（输入图像）

    Disc_real_output = 鉴别器（输入图像，目标）
    Disc_ generated_output = 鉴别器（input_image，gen_output）

    # 计算损失
    gen_total_loss, gen_gan_loss, gen_l1_loss = 生成器_loss(disc_generate_output,
        生成输出，目标）
    光盘损失 = 判别器损失（光盘真实输出，光盘生成输出）

    # 向后传递
    Generator_optimizer.zero_grad()
    discriminator_optimizer.zero_grad()

    gen_total_loss.backward(retain_graph=True)
    discriminator_optimizer.zero_grad() # 清除生成器梯度
        判别器后向传递
    Disc_loss.backward()

    # 更新权重
    生成器优化器.step()
    discriminator_optimizer.step()

    # 日志记录
    使用 torch.no_grad()：
        writer.add_scalar(&#39;gen_total_loss&#39;, gen_total_loss.item(), global_step=step // 1000)
        writer.add_scalar(&#39;gen_gan_loss&#39;, gen_gan_loss.item(), global_step=step // 1000)
        writer.add_scalar(&#39;gen_l1_loss&#39;, gen_l1_loss.item(), global_step=step // 1000)
        writer.add_scalar(&#39;disc_loss&#39;,disc_loss.item(),global_step=step // 1000)

拟合函数：
def fit(train_loader, test_loader, 步骤):
   example_target, example_input = next(iter(test_loader))
   开始 = 时间.time()

   对于枚举（train_loader）中的步骤（目标，input_image）：
    如果（步骤）% 1000 == 0：
        显示.clear_output(等待=True)

        如果步骤！= 0：
            print(f&#39;1000 步所用时间: {time.time()-start:.2f} 秒\n&#39;)

        开始 = 时间.time()

        生成图像（生成器，示例_输入，示例_目标）
        print(f&quot;步长: {step//1000}k&quot;)

    train_step（输入图像，目标，步骤）

    # 训练步骤
    如果（步长+1）% 10 == 0：
        打印（&#39;。&#39;，结束=&#39;&#39;，刷新= True）

    # 每 5k 步保存（检查点）模型
    如果（步长 + 1）% 5000 == 0：
        火炬.保存（{
            &#39;generator_state_dict&#39;：generator.state_dict(),
            &#39;discriminator_state_dict&#39;: discriminator.state_dict(),
            &#39;generator_optimizer_state_dict&#39;：generator_optimizer.state_dict(),
            &#39;discriminator_optimizer_state_dict&#39;: discriminator_optimizer.state_dict(),
        }, f&#39;检查点_{step + 1}.pt&#39;)

我是使用 GAN 的新手，我不确定这里的问题是什么。我尝试检查训练循环期间是否发生任何异常并打印它，但没有打印任何内容。]]></description>
      <guid>https://stackoverflow.com/questions/77646034/my-training-function-ends-unexpectedly-after-only-a-few-steps</guid>
      <pubDate>Tue, 12 Dec 2023 12:15:15 GMT</pubDate>
    </item>
    <item>
      <title>寻找用于构建人工智能领域推荐系统的数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/77645042/seeking-datasets-for-building-a-ai-domain-recommendation-system</link>
      <description><![CDATA[我正在创建一个用于域名推荐的人工智能系统，并正在寻找数据集来训练我的模型。我需要的关键数据包括：
有关域名注册和可用性的历史数据。
有关域服务提供商的信息，包括他们的产品和服务。
与域选择相关的用户偏好和行为（如果有）。
我的主要挑战和疑问是：
获取此类数据集的最佳来源是什么？是否有已知包含此类信息的公共存储库或数据库？
在这种情况下，在选择用于训练人工智能模型的数据集时，我应该考虑哪些因素？例如，在这个特定用例中，数据量、多样性和准确性等因素有多重要？
如果现成的数据集很稀缺，我可以采用什么策略来以合乎道德和合法的方式编译和管理这些数据？
在此处发帖之前，我进行了初步研究，但尚未找到满足这些特定要求的数据集。如果有人能向我指出可能提供此类信息的公共数据集或存储库，我将不胜感激。另外，有关如何收集此类数据的建议也会非常有帮助。
请注意，我并不是在寻找专有或机密信息，而是在寻找公开数据或有关如何以道德和合法方式汇总这些数据的建议。]]></description>
      <guid>https://stackoverflow.com/questions/77645042/seeking-datasets-for-building-a-ai-domain-recommendation-system</guid>
      <pubDate>Tue, 12 Dec 2023 09:42:31 GMT</pubDate>
    </item>
    <item>
      <title>如何解决运行时错误尝试在pytorch中的backward()函数中再次向后浏览图形</title>
      <link>https://stackoverflow.com/questions/77644164/how-to-solve-runtimeerror-trying-to-backward-through-the-graph-a-second-time-in</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77644164/how-to-solve-runtimeerror-trying-to-backward-through-the-graph-a-second-time-in</guid>
      <pubDate>Tue, 12 Dec 2023 06:52:02 GMT</pubDate>
    </item>
    <item>
      <title>如何利用 GPU 减少 xgboost 的处理时间？</title>
      <link>https://stackoverflow.com/questions/77643788/how-can-i-reduce-processing-time-with-xgboost-by-utilizing-my-gpu</link>
      <description><![CDATA[我正在关注数据营的本教程，他们有一件事提到的是利用 GPU 来加快处理时间。他们甚至说它“速度极快”。
然而，我看到了相反的结果。对于下面的代码块，在 10k 提升的情况下，我看到在我的 params 中传递 “hist” 大约需要 30 秒，而在 ” 中传递则只需一分多钟。 gpu_hist&quot; 与我的 params 一起传递。
使用 “gpu_hist” 时，我的 GPU 的使用率上限为 40%，使用 “hist” 时，所有 24 个逻辑核心的使用率上限为 100%
params = {“objective”: “reg:squarederror”, “tree_method”: “gpu_hist”, “subsample”: 0.8,
    “colsample_bytree”：0.8}

evals = [(dtrain_reg, “训练”),(dtest_reg, “验证”)]

n = 10000


模型 = xgb.train(
   参数=参数，
   dtrain=dtrain_reg,
   num_boost_round=n,
   评估=评估，
   详细评估=50，
）

我正在尝试在 jupyter 笔记本的 VSCode 中运行它。

我已安装 CUDA 工具包和 cuDNN
我已检查它们是否已添加到路径中
我已确保安装了正确版本的 xgboost 来使用 GPU。
数据集有 53k 行 10 列，所以我不认为数据集太小
我已确认兼容性（使用 RTX 2060）

我问过 chatGPT，在网上搜索过，甚至问过我正在学习的课程中的导师，但无法诊断为什么“gpu_hist”花费了这么长时间。 vs 只是“历史”。
4 个月前，Stack Overflow 上还有另一个类似问题其响应为零。]]></description>
      <guid>https://stackoverflow.com/questions/77643788/how-can-i-reduce-processing-time-with-xgboost-by-utilizing-my-gpu</guid>
      <pubDate>Tue, 12 Dec 2023 05:02:17 GMT</pubDate>
    </item>
    <item>
      <title>不使用 OpenAI Gym 环境的近端策略优化代码 [关闭]</title>
      <link>https://stackoverflow.com/questions/77641484/proximal-policy-optimization-code-without-using-openai-gym-environments</link>
      <description><![CDATA[对于一个项目，我必须在 Python 中对使用在线物理系统收集的数据实施近端策略优化。我见过的所有示例都使用 OpenAI 的 Gym 环境。我将如何修改/设置使用来自我的收集系统的数据而不是健身房环境数据的实现？]]></description>
      <guid>https://stackoverflow.com/questions/77641484/proximal-policy-optimization-code-without-using-openai-gym-environments</guid>
      <pubDate>Mon, 11 Dec 2023 17:58:03 GMT</pubDate>
    </item>
    <item>
      <title>重新排列 LGBMClassifier Predict_proba 输出列</title>
      <link>https://stackoverflow.com/questions/77639975/rearranging-lgbmclassifier-predict-proba-outputs-columns</link>
      <description><![CDATA[我正在训练一个 LGBMClassifier，以便使用其 predict_proba 方法。目标有 3 个类别：a、b 和 c。我想确保模型 predict_proba 按 b、a、c 的顺序输出列的概率。
有没有办法确保 LGBMClassifier predict_proba 的输出具有上述顺序？
导入 pandas 作为 pd
从 lightgbm 导入 LGBMClassifier
将 numpy 导入为 np

＃数据
特征 = [&#39;feat_1&#39;]
目标=&#39;目标&#39;
df = pd.DataFrame({
    &#39;feat_1&#39;：np.random.uniform（大小= 100），
    &#39;目标&#39;:np.random.choice(a=[&#39;b&#39;,&#39;c&#39;,&#39;a&#39;], size=100)
})

＃训练
模型 = LGBMClassifier()
model.fit(df[特征], df[目标])
打印（模型.classes_）

&lt;块引用&gt;
[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]

我尝试过的事情

只需重新排列 .classes_ 属性即可。
model.classes_ = [&#39;b&#39;,&#39;a&#39;,&#39;c&#39;]

&lt;块引用&gt;
AttributeError：无法设置属性“classes_”


根据 .classes_ 属性手动重新排列列。

desired_order = [&#39;b&#39;,&#39;a&#39;,&#39;c&#39;]
Correct_idx = [list(model._classes).index(val) for val indesired_order]
model.predict_proba(测试[特征])[:, Correct_idx]

#2 有效，但我不必在每次 predict_proba 调用时重新排列列顺序。]]></description>
      <guid>https://stackoverflow.com/questions/77639975/rearranging-lgbmclassifier-predict-proba-outputs-columns</guid>
      <pubDate>Mon, 11 Dec 2023 13:35:28 GMT</pubDate>
    </item>
    <item>
      <title>如何修复我的感知器来识别数字？</title>
      <link>https://stackoverflow.com/questions/77594625/how-can-i-fix-my-perceptron-to-recognize-numbers</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77594625/how-can-i-fix-my-perceptron-to-recognize-numbers</guid>
      <pubDate>Sun, 03 Dec 2023 14:03:49 GMT</pubDate>
    </item>
    <item>
      <title>CNN 架构的问题</title>
      <link>https://stackoverflow.com/questions/75060717/issue-with-cnn-architecture</link>
      <description><![CDATA[我正在尝试实现 CNN 架构，但是输出的形状存在问题。集合的形状如下：
x_train.shape、y_train.shape、x_test.shape、y_test.shape

&lt;前&gt;&lt;代码&gt;((1203, 162, 1), (1203, 7), (402, 162, 1), (402, 7))

架构设置如下：
input_x = tf.keras.layers.Input(shape = (x_train.shape[1],1))
conv_1 = tf.keras.layers.Conv1D(filters=16,kernel_size=3,padding=“相同”,activation=“relu”)(input_x)
pool_1 = tf.keras.layers.MaxPooling1D(2)(conv_1)
conv_2 = tf.keras.layers.Conv1D(filters=32,kernel_size=3,padding=“相同”,activation=“relu”)(pool_1)
pool_2 = tf.keras.layers.MaxPooling1D(2)(conv_2)

展平 = tf.keras.layers.Flatten()(pool_2)
密集= tf.keras.layers.Dense（512，激活=“relu”）（展平）
fb = tf.keras.layers.Dropout(0.4)（密集）
fb = tf.keras.layers.Dense(512，激活=“relu”)(fb)
fb = tf.keras.layers.Dropout(0.4)(fb)

输出= tf.keras.layers.Dense（8，激活=“softmax”）（fb）
model_branching_summed = tf.keras.models.Model(输入=input_x，输出=输出)
model_branching_summed.summary()
model_branching_summed.compile(optimizer=SGD(learning_rate=0.01,momentum=0.8),loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])


历史= model_branching_summed.fit（x_train，y_train，batch_size = 128，epochs = 100，validation_data =（x_test，y_test），callbacks = [rlrp]）

但是当我运行模型时，它给出以下错误：
ValueError Traceback（最近一次调用最后一次）
[192] 中的单元格，第 5 行
      1 rlrp =ReduceLROnPlateau(监视器=&#39;损失&#39;,因子=0.4,详细=0,耐心=2,min_lr=0.0001)
      2 #(min_lr=0.000001)
----&gt; 5 历史=model_branching_summed.fit（x_train，y_train，batch_size = 128，epochs = 100，validation_data =（x_test，y_test），callbacks = [rlrp]）

ValueError：形状（无，7）和（无，8）不兼容

哪里出错了？]]></description>
      <guid>https://stackoverflow.com/questions/75060717/issue-with-cnn-architecture</guid>
      <pubDate>Mon, 09 Jan 2023 17:09:09 GMT</pubDate>
    </item>
    <item>
      <title>将循环转换为双循环</title>
      <link>https://stackoverflow.com/questions/72054967/converting-recurrent-to-bi-recurrent</link>
      <description><![CDATA[我想将下面的 RNN 转换为双向 RNN，我该怎么做？
&lt;前&gt;&lt;代码&gt;模型 = RNN()
模型.summary()
model.compile(loss=&#39;categorical_crossentropy&#39;,optimizer=RMSprop(),metrics=[&#39;accuracy&#39;])
model.fit(X_train,Y_train,batch_size=10,epochs=20,
          验证分割=0.1）
]]></description>
      <guid>https://stackoverflow.com/questions/72054967/converting-recurrent-to-bi-recurrent</guid>
      <pubDate>Fri, 29 Apr 2022 08:27:10 GMT</pubDate>
    </item>
    <item>
      <title>感知器权重更新规则的直觉</title>
      <link>https://stackoverflow.com/questions/34477827/intuition-for-perceptron-weight-update-rule</link>
      <description><![CDATA[我无法理解感知器的权重更新规则：
w(t + 1) = w(t) + y(t)x(t)。
假设我们有一个线性可分离的数据集。

w 是一组权重 [w0, w1, w2, ...]，其中 w0 是偏差。
x 是一组输入参数 [x0, x1, x2, ...]，其中 x0 固定为 1 以适应偏差。

在迭代 t 时，其中 t = 0, 1, 2, ...,

w(t) 是迭代 t 时的权重集。
x(t) 是一个错误分类的训练示例。
y(t) 是 x(t) 的目标输出（-1 或 1）。


&lt;小时/&gt;

为什么这个更新规则会将边界向正确的方向移动？]]></description>
      <guid>https://stackoverflow.com/questions/34477827/intuition-for-perceptron-weight-update-rule</guid>
      <pubDate>Sun, 27 Dec 2015 05:20:30 GMT</pubDate>
    </item>
    </channel>
</rss>