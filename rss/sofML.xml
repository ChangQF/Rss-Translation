<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 13 Sep 2024 15:17:43 GMT</lastBuildDate>
    <item>
      <title>ImportError：无法从“image_processing”导入名称“func”</title>
      <link>https://stackoverflow.com/questions/78982569/importerror-cannot-import-name-func-from-image-processing</link>
      <description><![CDATA[我正在使用 Python 开发一个手部识别系统，并使用其他人的 github 存储库进行开发，但在执行代码时出现此错误。我还尝试安装 image_processing 和 scikit-learn，但仍然显示错误。我是否缺少其他库？
from image_processing import func

pip install -U scikit-learn
pip install image_processing
]]></description>
      <guid>https://stackoverflow.com/questions/78982569/importerror-cannot-import-name-func-from-image-processing</guid>
      <pubDate>Fri, 13 Sep 2024 14:24:58 GMT</pubDate>
    </item>
    <item>
      <title>哪些机器学习技术可以结合文本和数字/分类数据？[关闭]</title>
      <link>https://stackoverflow.com/questions/78981863/what-machine-learning-techniques-can-combine-text-and-numerical-categorical-data</link>
      <description><![CDATA[问题描述：我正在开展一个项目，需要结合文本数据（例如，客户评论）和数字/分类数据（如年龄、产品类别）来构建预测模型。虽然这不是专门针对客户评论的，但我仍处于研究的早期阶段。我的数据集中有大约 150 万行，但只有大约 1200 行具有真实的 Y 值。
我正在寻找机器学习技术来处理这个问题，但我现在想避免深度学习，特别是因为我不确定我的数据集大小是否足以支持深度学习模型。
我的问题：
我可以使用哪些机器学习技术将文本数据与数字和分类数据相结合，而无需使用深度学习？我已经研究过堆叠，但我想知道还有哪些其他选项可以有效地组合这些类型的数据。
数据集详细信息：
大约 150 万行，其中只有 1200 行具有真实 Y 值。
文本特征：类似于客户评论（仅作为示例）。
数字/分类特征：年龄、产品类别等。
目标：
我正在寻找有关技术或工作流程的建议，以帮助我有效地组合这些数据类型而无需深度学习。堆叠是我最好的选择，还是我应该考虑其他方法？]]></description>
      <guid>https://stackoverflow.com/questions/78981863/what-machine-learning-techniques-can-combine-text-and-numerical-categorical-data</guid>
      <pubDate>Fri, 13 Sep 2024 11:00:29 GMT</pubDate>
    </item>
    <item>
      <title>带有随机森林的分类链：为什么即使 Base Estimator 可以处理 np.nan 却不支持它？</title>
      <link>https://stackoverflow.com/questions/78981288/classifierchain-with-random-forest-why-is-np-nan-not-supported-even-though-base</link>
      <description><![CDATA[我正在研究一个多标签分类问题，使用ClassifierChain方法，以RandomForestClassifier作为基础估计器。我遇到了一个问题，我的输入矩阵X包含np.nan值。当单独使用RandomForestClassifier时，它可以毫无问题地处理NaN值，因为它通过其内部树分割机制原生支持缺失值。
这让我很困惑，因为基础估计器（RandomForestClassifier）确实可以正确处理NaN值。我不明白为什么 ClassifierChain（它只是一个包装器）会在底层分类器没有 NaN 问题的情况下引发此错误。
当我训练一个简单的 RandomClassifier 时，它确实会处理 np.nan：
from sklearn.ensemble import RandomForestClassifier
import numpy as np

X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)
y_single_label = [0, 0, 1, 1]

tree = RandomForestClassifier(random_state=0)
tree.fit(X, y_single_label)
X_test = np.array([np.nan]).reshape(-1, 1)
tree.predict(X_test)

即使我使用 MultiOutputClassifier 而不是ClassifierChain（不模拟标签之间的依赖关系），训练进行时没有任何错误，即使输入中有 NaN - 正如预期的那样。
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import ClassifierChain , MultiOutputClassifier

X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)

# 用于多标签分类的两个标签列
y = np.array([[0, 1], [0, 0], [1, 0], [1, 1]])

# 基础分类器
base_clf = RandomForestClassifier()

# MultiOutputClassifier（二元相关性）与基础分类器
clf_BR = MultiOutputClassifier(base_clf)

# 拟合模型
clf_BR.fit(X, y)

但是，当我切换到 ClassifierChain 方法时：
# 带有基础分类器的分类器链
clf_chain = ClassifierChain(base_clf)

# 拟合模型
clf_chain.fit(X, y)

我在超参数调整期间收到以下错误：

试验 0 失败，参数为：{&#39;n_estimators&#39;: 30, &#39;max_depth&#39;: 16, &#39;max_samples&#39;: 0.4497444900238575, &#39;max_features&#39;: 550, &#39;order_type&#39;: &#39;random&#39;}，错误原因如下：ValueError(&#39;输入 X 包含 NaN.\nClassifierChain 不接受编码为 NaN 的缺失值原生。对于监督学习，您可能需要考虑 sklearn.ensemble.HistGradientBoostingClassifier 和 Regressor，它们原生接受编码为 NaN 的缺失值。或者，可以预处理数据，例如通过在管道中使用 imputer 转换器或删除具有缺失值的样本。请参阅https://scikit-learn.org/stable/modules/impute.html 您可以在以下页面找到处理 NaN 值的所有估算器的列表：https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values&#39;)

由于保持缺失值原样而不是对其进行插补或删除非常重要，我想知道是否有办法让 ClassifierChain 处理缺失值。是否有任何解决方法或我遗漏了什么？
以下是我的环境详细信息：

Python 版本：3.12.5（由 conda-forge 打包）
scikit-learn 版本：1.5.1
]]></description>
      <guid>https://stackoverflow.com/questions/78981288/classifierchain-with-random-forest-why-is-np-nan-not-supported-even-though-base</guid>
      <pubDate>Fri, 13 Sep 2024 08:25:24 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：'Dense' 对象在 TensorFlow/Keras 的 ann_visualizer 中没有属性 'output_shape'</title>
      <link>https://stackoverflow.com/questions/78980852/attributeerror-dense-object-has-no-attribute-output-shape-in-ann-visualizer</link>
      <description><![CDATA[我遇到了这个错误：
Epoch 1/2
1/1 ━━━━━━━━━━━━━━━━━━━━━━ 0s 391ms/step - 损失：6272135168.0000
Epoch 2/2
1/1 ━━━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - 损失：6272133632.0000
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step
均方误差：7431671762.639743
回溯（最近一次调用）：

文件 ~\anaconda3\Lib\site-packages\spyder_kernels\py3compat.py:356 in compat_exec
exec(code, globals, locals)

文件 c:\users\mouli\.spyder-py3\temp.py:27
ann_viz(model,title=&#39;线性回归&#39;)

文件 ~\anaconda3\Lib\site-packages\ann_visualizer\visualize.py:42 in ann_viz
input_layer = int(str(layer.input_shape).split(&quot;,&quot;)[1][1:-1]);

AttributeError: &#39;Dense&#39; 对象没有属性 &#39;input_shape

为此：
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import pandas as pd`
`# 导入数据集
dataset = pd.read_csv(r&quot;Salary_Data (1).csv&quot;)
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = keras.Sequential([
keras.layers.Input(shape=(1,)),
keras.layers.Dense`(1)
])`
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)
model.fit(`你的文本`X_train, y_train, epochs=2, batch_size=32, verbose=1)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f&quot;均方误差：{mse}&quot;)`

# 可视化数据和回归线

# 可视化神经网络
from ann_visualizer.visualize import ann_viz
from graphviz `你的文本`import Source
ann_viz(model,title=&#39;线性回归&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78980852/attributeerror-dense-object-has-no-attribute-output-shape-in-ann-visualizer</guid>
      <pubDate>Fri, 13 Sep 2024 06:14:36 GMT</pubDate>
    </item>
    <item>
      <title>我如何创建一个模型来根据过去的选择预测会选择哪个选项？</title>
      <link>https://stackoverflow.com/questions/78980646/how-do-i-create-a-model-to-predict-which-option-will-be-chosen-based-on-past-cho</link>
      <description><![CDATA[我有一个数据集，其中包含一系列涉及用户选择的购买。每个数据集包括：

1 个或多个可供购买的商品（商品 + 价格 + 每个商品的详细信息）
用户从可用商品中选择了哪个商品

通常，只有一件商品可供购买，但有时会有多个。对于有多个商品的情况，我希望建立一个可以预测将购买哪些商品的模型。我对预测型算法（和机器学习）还不熟悉，因此欢迎提出建议。]]></description>
      <guid>https://stackoverflow.com/questions/78980646/how-do-i-create-a-model-to-predict-which-option-will-be-chosen-based-on-past-cho</guid>
      <pubDate>Fri, 13 Sep 2024 04:40:07 GMT</pubDate>
    </item>
    <item>
      <title>线性模型中的规范化[关闭]</title>
      <link>https://stackoverflow.com/questions/78979243/normalization-in-linear-models</link>
      <description><![CDATA[我知道对于线性模型来说，有必要对所有数据进行归一化。但是如果我们得到一个预测，它也将被归一化。我怎样才能在原始尺度上得到这个预测？
例如，如果我进行标准化（减去平均值并除以方差），我认为当我得到模型预测（假设为 X）时，我可以执行 X_original = X * 方差 + 平均值。但是有没有内置的方法可以做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78979243/normalization-in-linear-models</guid>
      <pubDate>Thu, 12 Sep 2024 17:03:00 GMT</pubDate>
    </item>
    <item>
      <title>尽管分类报告很好，但模型无法正确预测</title>
      <link>https://stackoverflow.com/questions/78976316/model-cant-predict-correctly-even-though-has-a-good-classification-report</link>
      <description><![CDATA[我尝试从链接运行此模型：
https://www.kaggle.com/code/alexfordna/garbage-classification-mobilenetv2-92-accuracy/notebook
当我在 colab 上使用类似数据集（但较小，2100 张图片到 6 个类）执行此操作时，效果很好。但是当我添加此代码来预测输入图像时：
from google.colab import files
from PIL import Image

def process_uploaded_image(image_path, target_size=(224, 224)):
img = Image.open(image_path)
img = img.resize(target_size) 
img_array = np.array(img) 

if img_array.shape[-1] == 4: 
img_array = img_array[..., :3]

img_array = img_array / 255.0 
img_array = np.expand_dims(img_array, axis=0) 
img_array = mobilenetv2.preprocess_input(img_array) 

return img_array

uploaded = files.upload()

for fn in uploaded.keys(): 
processed_image = process_uploaded_image(fn, target_size=IMAGE_SIZE) 
preds = model.predict(processed_image)
pred_class = np.argmax(preds, axis=1)

plt.imshow(Image.open(fn)) # 显示上传的图片
plt.title(f&#39;预测的类别：{categories[pred_class[0]]}&#39;)
plt.axis(&#39;off&#39;)
plt.show()
print(f&#39;文件 {fn} 被预测为：{categories[pred_class[0]]}&#39;)

结果是错误的预测。例如，模型总是将我的输入预测为“垃圾”类。当我停止运行时，它会更改为另一个类，但它仍然处于错误的预测中。
我还添加了此代码来检查预测概率：
preds = model.predict(processed_image)
pred_probs = preds[0] # 获取第一个（也是唯一一个）批次的预测概率
print(&quot;Prediction probabilities:&quot;, pred_probs)
pred_class = np.argmax(pred_probs)
print(&quot;Predicted class:&quot;, categories[pred_class])

输出：
**1/1** ━━━━━━━━━━━━━━━━━━━━━━ **0s** 24ms/步 预测概率：\[0.31027108 0.12315894 0.47848797 0.00863316 0.07789086 0.00155797\] 
预测类别：金属 

为什么会发生这种情况，我的模型如何正确预测结果？]]></description>
      <guid>https://stackoverflow.com/questions/78976316/model-cant-predict-correctly-even-though-has-a-good-classification-report</guid>
      <pubDate>Thu, 12 Sep 2024 02:58:40 GMT</pubDate>
    </item>
    <item>
      <title>从测量位置数据集中分离系统误差和随机性</title>
      <link>https://stackoverflow.com/questions/78975875/separating-systematic-errors-and-randomness-from-a-measured-position-dataset</link>
      <description><![CDATA[我正在尝试找出一种方法，将系统误差从一组数据集中分离出来，这些数据集表示机械平台的位置和位置误差。数据在 pandas 数据框中。
背景：平台在 2D 平面中移动。测量报告平台位置的 X 和 Y 坐标以及 X 和 Y 方向的位置误差。2D 平面上有一个相距 1nm 的标记网格，平台平稳移动到这些标记，并且仅在这些标记处进行测量。通过算法减去这些标记的真实位置和测量的平台位置来计算误差。
数据格式：这里的倾斜表示误差。



StageCoords_X
StageCoords_Y
Skew_X
Skew_Y




118760606
112836409
-29
-45


118760622
112836426
-18
5



**数据预处理：**我正在使用 nm 尺度进行工作，每次我扫描相同的“标记”位置时，测量的舞台位置都会在这些“标记”位置周围略有不同。这就是为什么我想在 X 和 Y 方向上对数据进行分类，以便每个矩形箱（结合 X 和 Y 轴箱宽度）将覆盖用于特定标记的数据点。我可以计算每个箱的平均和峰峰值误差并绘制它们。
# 根据箱宽度定义箱边界
bin_width = 1000000
x_bins = np.arange(df[&#39;StageCoordsNM.X&#39;].min(), df[&#39;StageCoordsNM.X&#39;].max() + bin_width, bin_width)
y_bins = np.arange(df[&#39;StageCoordsNM.Y&#39;].min(), df[&#39;StageCoordsNM.Y&#39;].max() + bin_width, bin_width)

df[&#39;X_bin&#39;] = pd.cut(df[&#39;StageCoordsNM.X&#39;], bins=x_bins, labels=False)
df[&#39;Y_bin&#39;] = pd.cut(df[&#39;StageCoordsNM.Y&#39;], bins=y_bins, labels=False)

我想要做的：现在假设，我已经将舞台扫过同一区域 25 次。在 x 和 y 方向上进行分箱后，每个箱将有 25 个数据点，意味着每个箱有 25 个测量舞台位置误差。现在对于每个箱，我想提取误差的系统或可重复部分。每个箱 25 个数据点可能不足以得出任何可靠的结论。因此使用机器学习很困难。我想找出一种更具统计性的方法来做到这一点。
我到目前为止所做的：计算了“归一化加权调整重复性指数”。这应该表明我对特定分箱误差是否可重复的信心。忽略 &#39;_before&#39; 下标。
#*__Pk-to-Pk X 和 Y__
df[&#39;Error_pk2pk_X&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.X&#39;].transform(lambda x: x.max() - x.min())
df[&#39;Error_pk2pk_Y&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.Y&#39;].transform(lambda x: x.max() - x.min())

df[&#39;Mean_SkewNM_X_before&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.X&#39;].transform(lambda x: x.mean())
df[&#39;Mean_SkewNM_Y_before&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.Y&#39;].transform(lambda x: x.mean())

def compute_confidence_X_before(group):
group = group.reset_index(drop=True)
group[&#39;WARI_x_before&#39;] = (w_a * np.abs(group[&#39;Mean_SkewNM_X_before&#39;].mean()) ) / ( w_p * (np.abs(group[&#39;Error_pk2pk_X&#39;].mean()) + epsilon) )
group[&#39;NWARI_x_before&#39;] = (group[&#39;WARI_x_before&#39;]) / (1+group[&#39;WARI_x_before&#39;])

group[&#39;Confidence_X_before&#39;] = ( np.abs(group[&#39;Mean_SkewNM_X_before&#39;].mean()) - np.abs(group[&#39;Error_pk2pk_X&#39;].mean()) ) / np.abs(group[&#39;SkewNM.X&#39;].mean())
group[&#39;Confidence_X_before&#39;] = group[&#39;Confidence_X_before&#39;].apply(lambda x: max(x, -1))
return group

def compute_confidence_Y_before(group):
group = group.reset_index(drop=True)
group[&#39;WARI_y_before&#39;] = (w_a * np.abs(group[&#39;Mean_SkewNM_Y_before&#39;].mean()) ) / ( w_p * (np.abs(group[&#39;Error_pk2pk_Y&#39;].mean()) + epsilon) )
group[&#39;NWARI_y_before&#39;] = (group[&#39;WARI_y_before&#39;]) / (1+group[&#39;WARI_y_before&#39;])

group[&#39;Confidence_Y_before&#39;] = ( np.abs(group[&#39;Mean_SkewNM_Y_before&#39;].mean()) - np.abs(group[&#39;Error_pk2pk_Y&#39;].mean()) ) / np.abs(group[&#39;SkewNM.Y&#39;].mean())
group[&#39;Confidence_Y_before&#39;] = group[&#39;Confidence_Y_before&#39;].apply(lambda x: max(x, -1))
返回组

df = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;], group_keys=False).apply(compute_confidence_X_before)
df = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;], group_keys=False).apply(compute_confidence_Y_before)
df[&#39;NWARI_X_before&#39;] = df[&#39;NWARI_x_before&#39;]
df[&#39;NWARI_Y_before&#39;] = df[&#39;NWARI_y_before&#39;]

但我不确定这是否是正确的方法。或者是否有其他方法可以验证这一点。
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78975875/separating-systematic-errors-and-randomness-from-a-measured-position-dataset</guid>
      <pubDate>Wed, 11 Sep 2024 22:23:55 GMT</pubDate>
    </item>
    <item>
      <title>FFN 模型在预测总和方面实现了 100% 的准确率</title>
      <link>https://stackoverflow.com/questions/78975293/ffn-model-achieving-100-accuracy-in-predicting-sums</link>
      <description><![CDATA[我有一个模型，可以对 -10 到正 10 之间的数字进行加法运算，但使用神经网络通过两个数字相加的数据集来预测结果。然而，在获得训练准确度时，它只是打印出很多 100% 的准确度。我不确定模型是否只是快速训练，或者是否存在问题并且没有正确学习。有人能提供一些见解吗？
这是我的代码
import torch
import torch.nn as nn
import torch.nn. functional as F
from torch.utils.data import DataLoader,TensorDataset
from sklearn.model_selection import train_test_split

import numpy as np

import matplotlib.pyplot as plt
import matplotlib_inline.backend_inline
matplotlib_inline.backend_inline.set_matplotlib_formats(&#39;svg&#39;)

data = []
labels = []

datasetAmount = 2000

for i in range(datasetAmount):
x = np.random.randint(-10, 10)
y = np.random.randint(-10,10)
bothNumber = [x,y]
data.append(bothNumber)
labels.append(x+y)

data_np = np.array(data)
labels_np = np.array(labels).reshape(-1,1)

train_data, test_data, train_labels, test_labels = train_test_split(data_np, labels_np, train_size =.9)

train_data = TensorDataset(torch.tensor(train_data),torch.tensor(train_labels))
test_data = TensorDataset(torch.tensor(test_data),torch.tensor(test_labels))

batchsize = 20

train_loader = DataLoader(train_data, batch_size = batchsize, shuffle = True, drop_last = True)
test_loader = DataLoader(test_data, batch_size = test_data.tensors[0].shape[0])

def createModel():
class myModel(nn.Module):
def __init__(self):
super().__init__()

self.input = nn.Linear(2,8)
self.fc1 = nn.Linear(8,8)
self.output = nn.Linear(8,1)

def forward(self,x):
x = F.relu( self.input(x) )
x = F.relu( self.fc1(x) )
return self.output(x)

net = myModel()
lossfun = nn.MSELoss()
optimizer = torch.optim.SGD(net.parameters(),lr=.001)

return net,lossfun,optimizer

def trainModel():

numepochs = 100
net,lossfun,optimizer = createModel()
loss = torch.zeros(numepochs)
trainacc = []
testacc = []

for epochi in range(numepochs):
batchLoss = []

for X,y in train_loader:
X = X.float()
y = y.float()
yHat = net(X)

loss = lossfun(yHat,y)
batchLoss.append(loss.item())

optimizer.zero_grad()
loss.backward()
optimizer.step()

loss[epochi] = np.mean(batchLoss)

with torch.no_grad():
train_predictions = []
train_labels = []
for x_train, y_train in train_loader:
x_train = x_train.float()
y_train = y_train.float()
train_pred = net(x_train)
train_predictions.append(train_pred)
train_labels.append(y_train)

train_predictions = torch.cat(train_predictions)
train_labels = torch.cat(train_labels)

train_acc = 100 * torch.mean((np.abs(train_predictions - train_labels) &lt; 1).float())
trainacc.append(train_acc.item())

X,y = next(iter(test_data))
X = X.float() # 将 X 转换为浮点数用于测试数据
y = y.float() # 将 y 转换为浮点数用于测试数据
with torch.no_grad():
yHat = net(X)

testacc= 100*torch.mean((np.abs(yHat-y)&lt; 1).float())

return trainacc,testacc,losses,net

trainAcc, testAcc, loss , net = trainModel()


模型有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78975293/ffn-model-achieving-100-accuracy-in-predicting-sums</guid>
      <pubDate>Wed, 11 Sep 2024 18:43:48 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenCV 改进这种图像自然背景扩展方法？</title>
      <link>https://stackoverflow.com/questions/78969286/how-can-i-improve-this-approach-for-natural-background-extension-in-an-image-usi</link>
      <description><![CDATA[我正在使用 Python 中的 OpenCV 扩展图像的背景。我目前的方法是复制边框并对扩展区域应用高斯模糊以将它们混合到原始图像中。目标是使背景扩展看起来更自然，尤其是对于具有一致纹理的图像。
这是我当前使用的代码：
import cv2
import numpy as np

def expand_image_with_smart_blend(image_path, top=50, bottom=50, left=50, right=50):
img = cv2.imread(image_path)
original_h, original_w = img.shape[:2]

expanded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_REPLICATE)

blured_img = expand_img.copy()

if top &gt; 0:
blured_img[0:top, :] = cv2.GaussianBlur(expanded_img[0:top, :], (51, 51), 0)

如果底部 &gt; 0:
blured_img[original_h + top:original_h + top + bottom, :] = cv2.GaussianBlur(expanded_img[original_h + top:original_h + top + bottom, :], (51, 51), 0)

如果左侧 &gt; 0:
blured_img[:, 0:left] = cv2.GaussianBlur(expanded_img[:, 0:left], (51, 51), 0)

如果右侧 &gt; 0:
blured_img[:, original_w + left:original_w + left + right] = cv2.GaussianBlur(expanded_img[:, original_w + left:original_w + left + right], (51, 51), 0)

cv2.namedWindow(&quot;智能混合扩展图像&quot;, cv2.WINDOW_NORMAL)
cv2.namedWindow(&quot;原始图像&quot;, cv2.WINDOW_NORMAL)
cv2.imwrite(&#39;expanded_smart_blended_image.jpg&#39;, blured_img)
cv2.imshow(&#39;智能混合扩展图像&#39;, blured_img)
cv2.imshow(&quot;原始图像&quot;, img)
cv2.waitKey(0)
cv2.destroyAllWindows()

expand_image_with_smart_blend(&#39;test_img.jpg&#39;, top=100, bottom=100, left=100, right=100)

我尝试过的方法：
cv2.BORDER_REPLICATE：我使用它将原始图像的边缘复制到新扩展的区域中。
高斯模糊：应用于扩展区域以柔化原始图像和新区域之间的过渡。
问题：
结果在某种程度上是可以接受的，但过渡仍然看起来不像我想要的那样自然。特别是：
某些区域的过度模糊使背景看起来不真实。
对于纹理更复杂的图像，边缘复制并不总是有效。
原始图像 结果图像
问题：
在 OpenCV 或其他库中，是否有更复杂的方法来扩展图像的背景，从而产生更自然、无缝的结果？我愿意接受涉及高级图像处理技术或机器学习的方法。任何使用扩散模型的方法都可以。]]></description>
      <guid>https://stackoverflow.com/questions/78969286/how-can-i-improve-this-approach-for-natural-background-extension-in-an-image-usi</guid>
      <pubDate>Tue, 10 Sep 2024 11:32:09 GMT</pubDate>
    </item>
    <item>
      <title>自定义模型聚合器 TensorFlow Federated</title>
      <link>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</link>
      <description><![CDATA[我正在尝试使用 TensorFlow Federated，使用 FedAvg 算法模拟训练过程。
def model_fn():
# 包装 Keras 模型以用于 TensorFlow Federated
keras_model = get_uncompiled_model()

# 对于联合过程，模型必须是未编译的
return tff.learning.models. functional_model_from_keras(
keras_model,
loss_fn=tf.keras.losses.BinaryCrossentropy(),
input_spec=(
tf.TensorSpec(shape=[None, X_train.shape[1]], dtype=tf.float32),
tf.TensorSpec(shape=[None], dtype=tf.int32)
),
metrics_constructor=collections.OrderedDict(
accuracy=tf.keras.metrics.BinaryAccuracy,
precision=tf.keras.metrics.Precision,
recall=tf.keras.metrics.Recall,
false_positives=tf.keras.metrics.FalsePositives,
false_negatives=tf.keras.metrics.FalseNegatives,
true_positives=tf.keras.metrics.TruePositives,
true_negatives=tf.keras.metrics.TrueNegatives
)
)

trainer = tff.learning.algorithms.build_weighted_fed_avg(
model_fn= model_fn(),
client_optimizer_fn=client_optimizer,
server_optimizer_fn=server_optimizer
)

我想使用自定义权重来聚合客户端的更新，而不是使用它们的样本。我知道 tff.learning.algorithms.build_weighted_fed_avg() 有一个名为 client_weighting 的参数，但唯一接受的值来自类 tff.learning.ClientWeighting，它是一个枚举。
因此，唯一的方法似乎是编写自定义 WeightedAggregator。我尝试按照本教程进行操作，该教程解释了如何编写无加权聚合器，但我无法将其转换为加权聚合器。
这是我尝试做的：
@tff.tensorflow.computation
def custom_weighted_aggregate(values, weights):
# 规范化客户端权重
total_weight = tf.reduce_sum(weights)
normalized_weights = weights / total_weight

# 计算客户端更新的加权总和
weighted_sum = tf.nest.map_structure(
lambda v: tf.reduce_sum(normalized_weights * v, axis=0),
values
)

return weighted_sum

class CustomWeightedAggregator(tff.aggregators.WeightedAggregationFactory):
def __init__(self):
pass

def create(self, value_type, weight_type):
@tff.federated_computation
def initialize():
return tff.federated_value(0.0, tff.SERVER)

@tff.federated_computation(
initialize.type_signature.result,
tff.FederatedType(value_type, tff.CLIENTS),
tff.FederatedType(weight_type, tff.CLIENTS)
)
def next(state, value, weight):
aggregate_value = tff.federated_map(custom_weighted_aggregate, (value, weight))
return tff.templates.MeasuredProcessOutput(
state,aggregate_value,tff.federated_value((),tff.SERVER)
)

return tff.templates.AggregationProcess(initialize,next)

@property
def is_weighted(self):
return True

但是我得到了以下错误：
AggregationPlacementError：next_fn 返回类型的“result”属性必须放置在 SERVER 中，但发现 {&lt;float32[7],float32,float32[1],float32&gt;}@CLIENTS。]]></description>
      <guid>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</guid>
      <pubDate>Mon, 05 Aug 2024 16:06:48 GMT</pubDate>
    </item>
    <item>
      <title>无法更改嵌入维度以将其传递给 gpt2</title>
      <link>https://stackoverflow.com/questions/74996908/cant-change-embedding-dimension-to-pass-it-through-gpt2</link>
      <description><![CDATA[我正在练习图像字幕，在张量的不同维度方面遇到了一些问题。所以我的图像嵌入又名大小 [1, 512]，但我用于字幕生成的 GPT2 需要大小 [n, 768]，其中 n 是字幕开头的标记数。我不知道如何更改图像嵌入的维度以使其通过 GPT2。
我认为用零填充图像嵌入是个好主意，因此大小将是 [1, 768]，但我认为这会对结果字幕产生负面影响。
谢谢你的帮助！
我曾尝试用零填充图像嵌入，使其大小为 [1, 768]，但我认为这不会有太大帮助]]></description>
      <guid>https://stackoverflow.com/questions/74996908/cant-change-embedding-dimension-to-pass-it-through-gpt2</guid>
      <pubDate>Tue, 03 Jan 2023 17:50:56 GMT</pubDate>
    </item>
    <item>
      <title>批次内的数据是否应该平衡？</title>
      <link>https://stackoverflow.com/questions/48200136/should-the-data-in-batch-be-balanced</link>
      <description><![CDATA[我正在训练一个深度学习模型，通过输入推文内容来预测三种情绪（快乐、悲伤、愤怒）。
我遇到的一个问题是，我的模型在悲伤、快乐方面学习得很好，但在快乐方面学习得很糟糕。

我认为原因是我的训练数据集不平衡。
快乐中的数据大小：196952，悲伤：29407，愤怒：42420
因此，在训练模型时，批量大小包含太多快乐数据集，这使得模型只能猜测答案是快乐而不是其他。
我想通过平衡每个数据集中的数据来解决这个问题批次。
也就是说批次大小为 128，我们随机选择相同数量的三种情绪数据。防止模型被快乐数据所主导。
问题是：批次中的数据应该平衡吗？
另一个问题是，我随机选择了数据集，这是否违反了 epoch 的定义？
因为 epoch 意味着读取所有训练数据集。当随机选择时，可能某些数据集在某些 epoch 中不会被选择。或者只需训练更多 epoch 即可解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/48200136/should-the-data-in-batch-be-balanced</guid>
      <pubDate>Thu, 11 Jan 2018 05:09:56 GMT</pubDate>
    </item>
    <item>
      <title>推荐相关项目的推荐算法</title>
      <link>https://stackoverflow.com/questions/43029589/recommender-algorithm-to-recommend-associated-items</link>
      <description><![CDATA[我有一个场景，我想提供建议。为简单起见，假设我有包含购买和购买项目记录的数据库表。每个购买项目都描述了应用程序向用户呈现的每件商品的数量、描述（鞋子、裤子、衬衫）以及总价。通常，与特定购买相关的购买项目或一系列购买项目会自动触发添加其他相关购买项目以进行类似类型的购买（也许他们想添加帽子或手套）。
需要注意的是，没有为这些类型的项目分配类别。它们可能完全不相关，除非它们定期一起应用于购买（衬衫、食物和相机不相关，但可能经常一起购买）。也就是说，这是用户的购买习惯，而不是（推荐帽子，因为它们是一种像衬衫一样的服装）。
我尝试了推荐算法，但不完全了解如何在这种情况下应用它。这是我应该关注的正确算法吗？]]></description>
      <guid>https://stackoverflow.com/questions/43029589/recommender-algorithm-to-recommend-associated-items</guid>
      <pubDate>Sun, 26 Mar 2017 13:52:13 GMT</pubDate>
    </item>
    <item>
      <title>多步预测神经网络</title>
      <link>https://stackoverflow.com/questions/10327260/multi-step-prediction-neural-networks</link>
      <description><![CDATA[我一直在使用 matlab 神经网络工具包。这里我使用的是 NARX 网络。我有一个数据集，其中包含某个物品的价格以及在一段时间内购买的物品数量。本质上，这个网络执行一步预测，其数学定义如下：
y(t)= f (y(t −1),y(t −2),...,y(t −ny),x(t −1),x(t −2),...,x(t −nx))
这里 y(t) 是时间 t 时的价格，x 是金额。所以我使用的输入特征是价格和金额，目标是时间 t+1 时的价格。假设我有 100 条此类交易的记录，每笔交易都包含价格和金额。那么我的神经网络基本上可以预测第 101 笔交易的价格。这对于一步预测来说效果很好。但是，如果我想进行多步预测，比如说我想预测未来 10 笔交易（第 110 笔交易），那么我假设我对价格进行一步预测，然后将其反馈给神经网络。我一直这样做，直到达到第 110 次预测。但是，在这种情况下，在我预测第 101 个价格后，我可以将此价格输入神经网络以预测第 102 个价格，但是，我不知道第 101 笔交易的物品数量。我该怎么做？我正在考虑将我的目标设置为比当前交易提前 10 笔交易的交易价格，这样当我预测第 101 笔交易时，我基本上就是在预测第 110 笔交易的价格。这是一个可行的解决方案吗，还是我完全错误地处理了这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/10327260/multi-step-prediction-neural-networks</guid>
      <pubDate>Thu, 26 Apr 2012 04:28:45 GMT</pubDate>
    </item>
    </channel>
</rss>