<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 05 Dec 2024 21:16:59 GMT</lastBuildDate>
    <item>
      <title>减少模型训练期间的 CTC 损失</title>
      <link>https://stackoverflow.com/questions/79255724/decreasing-ctc-loss-during-model-training</link>
      <description><![CDATA[我是手写识别模型，在训练过程中，我遇到了高 ctc 损失，我应该怎么做才能减少损失并使我的模型识别任何文本，因为截至目前它无法识别任何东西。
链接到 Google-collab 笔记本：https://colab.research.google.com/drive/16VDTTcCGsQrpURZXCxL5695jZ5Xxze4O?usp=sharing[在此处输入图片描述](https://i.sstatic.net/GswZOriQ.jpg)在此处输入图片描述https://colab.research.google.com/drive/16VDTTcCGsQrpURZXCxL5695jZ5Xxze4O?usp=sharing
def ctc_loss(y_true, y_pred):
&quot;&quot;&quot;
计算真实标签和预测输出之间的 CTC 损失。

参数：
y_true：真实标签 (int32/int64)。
y_pred：模型预测 (logits)。

返回：
整个批次的平均 CTC 损失。
&quot;&quot;&quot;
# 确保 y_true 是整数类型
y_true = tf.cast(y_true, dtype=tf.int32)

input_length = tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1]) # 时间步骤
label_length = tf.reduce_sum(tf.cast(y_true != -1, tf.int32), axis=1) # 非填充标签长度

# 计算 CTC 损失
loss = tf.nn.ctc_loss(
labels=y_true,
logits=y_pred,
label_length=label_length,
logit_length=input_length,
blank_index=-1, # 指定用于填充的空白索引
logits_time_major=False # Logits 是批量主序列
)

return tf.reduce_mean(loss)

# 优化器使用学习率调度程序
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
initial_learning_rate=1e-3,
decay_steps=10000,
decay_rate=0.9
)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)

model.compile(optimizer=optimizer, loss=ctc_loss)

# 模型训练
history = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))

# 可视化损失
import matplotlib.pyplot as plt

plt.plot(history.history[&#39;loss&#39;], label=&#39;Training Loss&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;Validation Loss&#39;)
plt.legend()
plt.title(&quot;Training and Validation Loss&quot;)
plt.xlabel(&quot;Epoch&quot;)
plt.ylabel(&quot;Loss&quot;)
plt.show()

我尝试使用学习率调度程序来降低损失率，但没有成功。请更正我的代码并提出改进建议以修复训练过程]]></description>
      <guid>https://stackoverflow.com/questions/79255724/decreasing-ctc-loss-during-model-training</guid>
      <pubDate>Thu, 05 Dec 2024 17:39:56 GMT</pubDate>
    </item>
    <item>
      <title>包括 GNN 训练数据中每个类的节点数</title>
      <link>https://stackoverflow.com/questions/79255547/including-number-of-nodes-per-class-in-gnn-training-data</link>
      <description><![CDATA[我正在使用 pytorch-geometric 来训练二元节点分类模型，其中我提供了一个带有一些属性的节点的图，模型返回每个节点的类别。我的训练数据集包含 1000 多个图。没有使用掩码。我的数据是这样的，对于每个图，我可以计算每个类的节点数。因此，这可以看作是一个模型，它只是根据图的节点属性将类 1 的节点分布在图中的所有节点之间。
因为我可以计算出每个图中每个类有多少个节点，所以我想到我可以将该数字用作模型的额外输入。这样，我至少会从每个类的正确节点数开始，训练将专注于分配它们。
我的模型由两个图卷积层 GCNConv 组成，每个层都有一个 tanh 激活。之后我有一个 Linear 层来进行分类。所以，
class GCN(torch.nn.Module):
def __init__(self):
super(GCN, self).__init__()
torch.manual_seed(12345)
self.conv1 = GCNConv(3, 4)
self.conv2 = GCNConv(4, 4)
self.classifier = Linear(4, 2)

def forward(self, x, edge_index):
h = self.conv1(x, edge_index)
h = h.tanh()
h = self.conv2(h, edge_index)
h = h.tanh() # 最终 GNN 嵌入空间。
h = self.classifier(h)

out = F.log_softmax(h,dim=1)

return out, h

问题是：将有关每个类的节点数的信息纳入模型的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79255547/including-number-of-nodes-per-class-in-gnn-training-data</guid>
      <pubDate>Thu, 05 Dec 2024 16:42:22 GMT</pubDate>
    </item>
    <item>
      <title>根据二元变量制定信用评级（最好使用 R 语言）[关闭]</title>
      <link>https://stackoverflow.com/questions/79254610/develop-credit-rating-based-on-binary-variable-in-r-preferably</link>
      <description><![CDATA[我在 R 中处理信用数据。我有一个贷款数据集，其中包含借款人和信用特定变量以及二元指标违约/不违约。第一步，我进行 logit 并获取违约概率 (PD)。接下来我想做什么：

根据这些 PD 建立类似信用评级模型的东西。例如，类别 1：PD&lt;0.2，类别 2：0.2&lt;PD&lt;0.4 等。
此类别数应在计量经济学/统计上达到最优。
找到类别之间的这些阈值。

我该如何实现？
我认为基于 PD 和其他一些特征，我可以进行聚类分析，然后将这些聚类用作有序 logit 中的因变量或类似的东西。但我害怕相关性和其他东西。我也对证明最佳聚类数感到困惑。我该如何选择？
如果您能提供 R 代码思路就更好了，但不一定]]></description>
      <guid>https://stackoverflow.com/questions/79254610/develop-credit-rating-based-on-binary-variable-in-r-preferably</guid>
      <pubDate>Thu, 05 Dec 2024 12:05:53 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ML/DL 模型找到两个矩阵之间的映射，其中矩阵一的形状为 [B，n，特征]，另一个矩阵的形状为 [B，m，特征] [关闭]</title>
      <link>https://stackoverflow.com/questions/79254384/how-to-find-mapping-between-two-matrices-where-matrix-one-is-of-shape-b-n-fea</link>
      <description><![CDATA[我正在研究一个问题，需要将一个矩阵映射到另一个矩阵。考虑 X 和 Y，如下所示
X 的形状为 [batch_size, seq_len_1, feature_dim]
Y 的形状为 [batch_size, seq_len_2, feature_dim]
这里，seq_len_1 和 seq_len_2 可以在不同的样本中变化，我需要在这些矩阵之间执行序列到序列的映射。我想训练一个 ml/dl 模型，该模型将矩阵 X 作为输入，并将矩阵 Y 作为输出。
这些矩阵包含数值，准确地说，将第一个矩阵视为一种语言的梅尔频谱图表示，将第二个矩阵视为另一种语言的梅尔频谱图表示。
到目前为止，我已经训练了一个简单的神经网络，它将在给定输入中的矩阵 X 的情况下在输出中给出一个矩阵，我只是在 output_matrix 和 Y 之间找到了 mse 损失。]]></description>
      <guid>https://stackoverflow.com/questions/79254384/how-to-find-mapping-between-two-matrices-where-matrix-one-is-of-shape-b-n-fea</guid>
      <pubDate>Thu, 05 Dec 2024 11:02:10 GMT</pubDate>
    </item>
    <item>
      <title>Azure ML 实验指标限制：无法在实验比较中查看所有记录的指标</title>
      <link>https://stackoverflow.com/questions/79254263/azure-ml-experiment-metric-limit-unable-to-see-all-logged-metrics-in-experiment</link>
      <description><![CDATA[我正在开展一项实验，记录 Azure 机器学习 (Azure ML) 中 12 种不同机器学习模型的指标。每个模型都有 680 个指标。这是因为我有一个包含 150 多个类别的多分类模型，每个类别都有准确率、召回率、f1 分数和支持值。
以下是我记录指标的方式：
# 使用 mlflow 记录指标
mlflow.log_metric(f&#39;{metric_name}&#39;, metric_value)

这些指标显示在“运行详细信息”选项卡中，查看单个运行时可以查看所有指标。但是，当我切换到实验级别来比较模型时，我只能在所有运行中看到最多 250 个指标。
运行指标

实验指标

这似乎是 Azure ML 的一个限制，但我不确定它是否是服务问题、我的配置问题，或者 Azure ML 或 MLflow 中是否存在关于可在实验级别显示的指标数量的已知限制。
我尝试过的方法：

验证所有指标是否在运行详细信息中正确记录。
确保指标在 mlflow.log_metric() 调用中可用。
检查了 Azure ML 文档和配置设置，但找不到任何有关指标限制的引用。

问题：

在 Azure ML 的实验比较视图中可显示的指标数量是否存在已知限制？
是否有任何配置更改或解决方法来克服此限制？
我是否应该考虑对大量指标使用不同的日志记录方法？
]]></description>
      <guid>https://stackoverflow.com/questions/79254263/azure-ml-experiment-metric-limit-unable-to-see-all-logged-metrics-in-experiment</guid>
      <pubDate>Thu, 05 Dec 2024 10:26:57 GMT</pubDate>
    </item>
    <item>
      <title>为什么通用 Windows 应用程序中的调试器显示 modelgen 为空</title>
      <link>https://stackoverflow.com/questions/79254069/why-is-the-debugger-in-universal-windows-app-showing-modelgen-is-null</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79254069/why-is-the-debugger-in-universal-windows-app-showing-modelgen-is-null</guid>
      <pubDate>Thu, 05 Dec 2024 09:33:13 GMT</pubDate>
    </item>
    <item>
      <title>torch.utils.data.random_split 中的随机性与基于 numpy 的拆分</title>
      <link>https://stackoverflow.com/questions/79254043/randomness-in-torch-utils-data-random-split-vs-numpy-based-splitting</link>
      <description><![CDATA[我正在使用 PyTorch Lightning 基础架构在 PyTorch 中实现卷积神经网络。
我的合作者要求我使用 kfold 交叉验证来评估跨数据集性能（是的，我知道数据泄漏和过度拟合）。
我的问题是基于 numpy 的随机分割产生的性能明显比 torch.utils.data.random_split 差。我使用 train/val/test 方法的测试性能约为 AUC 0.95，使用 8:1:1 分割。

使用手动 numpy 分割的 5 倍 CV 产生 0.88 的 AUC
使用自动 pytorch random_split 的 5 倍 CV 产生 0.95 的 AUC

用于计算 AUC 的样本量约为 10,000，因此两种方法之间的 AUC 差异很大。
我的问题是为什么 pytorch random_split 函数返回更好的结果？在我的 numpy 实现中，属于给定折叠的概率在观察之间是独立的。 random_split 函数是否也是如此，或者它是否使用随机样本间隔？
Kfold 实验
当我使用 numpy 对数据集进行子集化时，我获得的性能约为 AUC=0.88。这是我的代码：
fold = 1
indices = np.arange(X.shape[0])
sampler = np.random.permutation(indices) % 5 # 5 倍 CV
X_train, X_test = X[(sampler!=fold)], X[(sampler==fold)]
y_train, y_test = y[(sampler!=fold], y[(sampler==fold)]

dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1])
test_dataset = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))

当我对数据集进行子集化时在 pytorch 中使用 random_split 我获得了大约 AUC=0.95 的性能。这是我的代码：
fold = 1
dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))
five_folds = torch.utils.data.random_split(dataset, [1/5]*5) # 5 倍 CV
test_dataset = five_folds[fold]
train_dataset = torch.utils.data.ConcatDataset(
[five_folds[i] for i in range(5) if i != fold]
)
train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [0.9, 0.1])
]]></description>
      <guid>https://stackoverflow.com/questions/79254043/randomness-in-torch-utils-data-random-split-vs-numpy-based-splitting</guid>
      <pubDate>Thu, 05 Dec 2024 09:28:01 GMT</pubDate>
    </item>
    <item>
      <title>文本类别预测 [关闭]</title>
      <link>https://stackoverflow.com/questions/79252250/text-category-prediction</link>
      <description><![CDATA[我需要创建某种方法，以便预测文本的类别。
现在我正在使用这样的 PHP-ML：
 $texts = $queriesService-&gt;getTexts();

foreach($texts as $key =&gt; $text){
$string = preg_replace(&#39;/\s+/&#39;, &#39; &#39;, strip_tags($text[&#39;text&#39;]));
$string = str_replace(&#39;&quot;&#39;, &#39;&#39;, $string);
$samples[] = $string;
$labels[] = $text[&#39;category&#39;]; 
}

$tokenize = new WordTokenizer();
$vectorizer = new TokenCountVectorizer($tokenize);

$vectorizer-&gt;fit($samples);
$vectorizer-&gt;transform($samples);

$transformer = new TfIdfTransformer($samples);
$transformer-&gt;transform($samples);

$classifier = new NaiveBayes();
$classifier-&gt;train($samples, $labels);

$testSamples = [
&#39;关于某些产品的示例文本&#39;,
&#39;这是关于糟糕的服务&#39;,
&#39;由于某种原因电子设备无法正常工作&#39;,
];

$vectorizer-&gt;transform($testSamples);
$transformer-&gt;transform($testSamples);

$predictions = $classifier-&gt;predict($testSamples);

这样运行良好，但问题是 - 它占用了大量内存。所讨论的文本各不相同，从 5 个单词到 200 个单词，大约有 100k 个。
即使训练分类器并将其保存到文件 - 文件大小超过 10GB，因此读取它需要大量内存。有没有更好的方法来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/79252250/text-category-prediction</guid>
      <pubDate>Wed, 04 Dec 2024 17:55:51 GMT</pubDate>
    </item>
    <item>
      <title>当我的训练和测试数据大小不同时，如何像使用 sklearn 模型一样使用拟合和预测函数创建神经网络类？</title>
      <link>https://stackoverflow.com/questions/79249247/how-do-i-make-a-neural-network-class-with-fit-and-predict-functions-like-with-sk</link>
      <description><![CDATA[我正在尝试创建一个可以回答线性回归问题的神经网络模型（我已经使用 sklearn 的 LinearRegression 建立了一个模型，我想比较一下这两个模型）。最终，我想创建一个包含 fit 和 predict 函数的类，就像 sklearn 中的模型一样，这样我就可以创建一个循环来测试我在项目中使用的所有模型。
为此，我遵循了此问题答案中的代码：编写一个具有模型拟合和预测功能的 pytorch 神经网络类。
经过一些修改，我得到了以下结果：
import torch
import torch.nn as nn
import torch.optim as optim

class MyNeuralNet(nn.Module):
def __init__(self):
super().__init__()
self.layer1 = nn.Linear(2, 4, bias=True)
self.layer2 = nn.Linear(4, 1, bias=True)
self.loss = nn.MSELoss()
self.compile_()

def forward(self, x):
x = self.layer1(x)
x = self.layer2(x)
return x.squeeze()

def fit(self, x, y):
x = torch.tensor(x.values, dtype=torch.float32)
y = torch.tensor(y.values, dtype=torch.float32)
loss = []
for epoch in range(100):
## 推理
res = self.forward(x)#self(self,x)
loss_value = self.loss(res,y)

## 反向传播
loss_value.backward() # 计算梯度
self.opt.zero_grad() # 刷新前一个 epoch 的梯度
self.opt.step() # 使用上面的梯度执行迭代

## 日志记录
loss.append(loss_value.item())

def compile_(self):
self.opt = optim.SGD(self.parameters(), lr=0.01)

def predict(self, x_test):
self.eval()
y_test_hat = self(x_test)
return y_test_hat.detach().numpy()
# self.train()

注意，您还需要 numpy，我只是没有在这里，因为此代码已放入单独的 .py 文件中。
导入我的类后，这是我使用模型的方式：
model = MyNeuralNet()
X_train = # pandas 数据框，包含 1168 行和 49 列
y_train = # pandas 数据框，包含 1168 行和 1 列
X_test = # pandas 数据框，包含 292 行和 49 列
model.fit(X_train, y_train)
pred = model.predict(X_test)
print(pred)

我得到的错误是 RuntimeError：mat1 和 mat2 形状无法相乘（1168x49 和 2x4），在 fit 步骤。我理解这与我的网络线性层的参数有关。我认为，如果我将第一个线性层的输入大小更改为 49，将第二个线性层的输出大小更改为 1168，那么它将适用于 fit 步骤（或至少类似的步骤，以匹配训练数据的大小）。但是，我的测试数据的大小不同，我很确定 predict 步骤将不起作用。
是否可以创建一个训练和测试数据大小不同的神经网络类？]]></description>
      <guid>https://stackoverflow.com/questions/79249247/how-do-i-make-a-neural-network-class-with-fit-and-predict-functions-like-with-sk</guid>
      <pubDate>Tue, 03 Dec 2024 21:41:10 GMT</pubDate>
    </item>
    <item>
      <title>convLSTM 教程的输入[关闭]</title>
      <link>https://stackoverflow.com/questions/79248815/inputs-of-a-convlstm-tutorial</link>
      <description><![CDATA[我找到了以下教程“https://medium.com/neuronio-br/uma-introdu%C3%A7%C3%A3o-a-convlstm-c14abf9ea84a”教授有关 ConvLSTM 模型的知识。
我创建了一个算法来构建模型输入：
import os
import cv2
import numpy as np
import pandas as pd

# 设置
video_folder = &#39;train&#39; # 包含视频的文件夹的路径
output_folder = &#39;train_npy&#39; # 保存 .npy 文件的文件夹
csv_file = &#39;train.csv&#39; # CSV 文件的路径
frames_per_video = 16 # 每个视频的帧数（时间）
pixels_x, pixels_y = 112, 112 # 帧尺寸

# 使用视频名称和类别加载 CSV
df = pd.read_csv(csv_file) # 列：&#39;video_name&#39;, &#39;tag&#39;

# 根据“tag”列中的唯一值确定的类数
unique_tags = df[&#39;tag&#39;].unique()
num_categories = len(unique_tags) # 根据数据定义类别数量

# 将类别映射到索引的字典
tag_to_index = {tag: idx for idx, tag in enumerate(unique_tags)}

# 如果不存在则创建输出文件夹
os.makedirs(output_folder, exist_ok=True)

# 提取和处理帧的函数
def extract_frames(video_path, num_frames=16, size=(pixels_x, pixels_y)):
cap = cv2.VideoCapture(video_path)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
frame_interval = max(total_frames // num_frames, 1)

frames = []
for i in range(num_frames):
cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)
ret, frame = cap.read()
if not ret:
break
frame = cv2.resize(frame, size)
frames.append(frame)

# 必要时用零帧填充
while len(frames) &lt; num_frames:
frames.append(np.zeros((pixels_x, pixels_y, 3), dtype=np.uint8))

cap.release()
return np.array(frames) # 形状：(frames, pixels_x, pixels_y, 3)

# 循环处理所有视频
for idx, row in df.iterrows():
video_name = row[&#39;video_name&#39;]
class_label = row[&#39;tag&#39;]

# 视频的完整路径
video_path = os.path.join(video_folder, video_name)

# 检查视频是否存在
if not os.path.exists(video_path):
print(f&quot;Video {video_name} not found!&quot;)
continue

# 处理帧并保存为 .npy
scene_data = extract_frames(video_path, frames_per_video)
scene_data = scene_data.transpose(0, 3, 1, 2) # 将顺序改为 (帧、通道、行、列)
np.save(os.path.join(output_folder, f&#39;scene_{idx}.npy&#39;), scene_data)

# 创建并保存类别（独立输出）
category_data = np.zeros((num_categories, 1, frames_per_video, 1))
category_data[tag_to_index[class_label], 0, :, 0] = 1 # 将所有帧的类别标记为 1
np.save(os.path.join(output_folder, f&#39;category_{idx}.npy&#39;), category_data)

print(&quot;处理完成！&quot;)

然而，在训练模型时，我收到以下错误：
ValueError: Arguments `target`和 `output` 必须具有相同的形状。 
收到：target.shape=(None, 1, 16), output.shape=(None, 16, 1)

而形状符合教程：
scene_0.npy.shape = (16, 3, 112, 112)
category_0.npy.shape = (5, 1, 16, 1)

为什么目标和输出的形状不同？是教程错误还是输入格式有错误？]]></description>
      <guid>https://stackoverflow.com/questions/79248815/inputs-of-a-convlstm-tutorial</guid>
      <pubDate>Tue, 03 Dec 2024 18:34:23 GMT</pubDate>
    </item>
    <item>
      <title>无法提高 Tiny VGG CNN 模型的准确性 [关闭]</title>
      <link>https://stackoverflow.com/questions/79247334/failing-to-improve-accuracy-in-tiny-vgg-cnn-model</link>
      <description><![CDATA[设置

我试图学习如何在 PyTorch 中编写 CNN 的基础知识。
我几乎是一丝不苟地遵循 05. PyTorch Going Modular 来学习如何以脚本方式建模，写下 CNN Explainer 中描述的 CNN 模型 Tiny VGG。

我正在练习的数据，如 05. PyTorch Going Modular 是 pizza-steak-sushi 数据集；加载和转换方式与 此处 相同。

再次，该模型是这里中的模型。
问题

经过多次尝试设置以下超参数：
NUM_EPOCHS = 20
BATCH_SIZE = 20
HIDDEN_UNITS = 10
LEARNING_RATE = 0.0005

并使用一对损失函数和优化器：
loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(tiny_vgg.parameters(), lr=LEARNING_RATE)

我无法提高模型在训练和（大部分）测试数据集中的准确率，目前如下所示：
Epoch：1 | train_loss：1.1018 | train_acc：0.2875 | test_loss：1.0947 | test_acc：0.4500
Epoch：2 | train_loss：1.0974 | train_acc：0.3833 | test_loss：1.0970 | test_acc：0.3125
Epoch：3 | train_loss：1.0960 | train_acc：0.3375 | test_loss：1.0995 | test_acc：0.3125
Epoch：4 | train_loss：1.0895 | train_loss：0.3500 | test_loss：1.1013 | test_acc：0.3125
Epoch：5 | train_loss：1.0626 | train_acc：0.3917 | test_loss：1.0755 | test_acc：0.3250
Epoch：6 | train_loss：1.0566 | train_acc：0.3750 | test_loss：1.0666 | test_acc：0.3375
Epoch：7 | train_loss：1.0105 | train_loss：0.5542 | test_loss：1.0133 | test_acc：0.4208
Epoch：8 | train_loss：0.9488 | train_acc：0.5708 | test_loss：1.0114 | test_acc：0.4792
时期：9 | train_loss：0.8834 | train_acc：0.5625 | test_loss：0.9973 | test_acc：0.4208
时期：10 | train_loss：0.8751 | train_acc：0.5667 | test_loss：1.0350 | test_acc：0.4542
时期：11 | train_loss：0.8098 | train_acc：0.6375 | test_loss：0.9834 | test_acc：0.4458
时期：12 | train_loss：0.8136 | train_acc：0.6333 | test_loss：1.0400 | test_acc：0.3792
时期：13 |训练损失：0.8042 | 训练误差：0.6833 | 测试损失：0.9929 | 测试损失：0.4458
时期：14 | 训练损失：0.7932 | 训练误差：0.6292 | 测试损失：1.0383 | 测试损失：0.4333
时期：15 | 训练损失：0.7796 | 训练损失：0.6458 | 测试损失：1.0025 | 测试损失：0.4833
时期：16 | 训练损失：0.7654 | 训练损失：0.6708 | 测试损失：1.0344 | 测试损失：0.4375
时期：17 | 训练损失：0.7412 | 训练损失：0.6833 | test_loss：1.0358 | test_acc：0.4625
时期：18 | train_loss：0.7168 | train_acc：0.6667 | test_loss：1.0759 | test_acc：0.4125
时期：19 | train_loss：0.7364 | train_acc：0.6500 | test_loss：1.0393 | test_acc：0.4583
时期：20 | train_loss：0.7228 | train_acc：0.7167 | test_loss：1.0826 | test_acc：0.4500


因此，我想知道我是否缺少了架构方面的某些东西、超参数设置，或者其他一些东西，例如此类模型缺乏数据等等。]]></description>
      <guid>https://stackoverflow.com/questions/79247334/failing-to-improve-accuracy-in-tiny-vgg-cnn-model</guid>
      <pubDate>Tue, 03 Dec 2024 11:10:18 GMT</pubDate>
    </item>
    <item>
      <title>出现“TemplateNotFound：index.html”错误 - 面临路线问题</title>
      <link>https://stackoverflow.com/questions/79164308/getting-templatenotfound-index-html-error-facing-issue-with-routes</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79164308/getting-templatenotfound-index-html-error-facing-issue-with-routes</guid>
      <pubDate>Wed, 06 Nov 2024 21:07:11 GMT</pubDate>
    </item>
    <item>
      <title>sklearn中RepeatedStratifiedKFold和StratifiedKFold有什么区别？</title>
      <link>https://stackoverflow.com/questions/71181291/what-is-the-difference-between-repeatedstratifiedkfold-and-stratifiedkfold-in-sk</link>
      <description><![CDATA[我尝试阅读 RepeatedStratifiedKFold 和 StratifiedKFold 的文档，但无法分辨这两种方法之间的区别，除了 RepeatedStratifiedKFold 重复 StratifiedKFold n 次，每次重复的随机化方式不同。
我的问题是：这两种方法是否返回相同的结果？在执行 GridSearchCV 时，我应该使用哪一个方法来分割不平衡的数据集，选择该方法的理由是什么？]]></description>
      <guid>https://stackoverflow.com/questions/71181291/what-is-the-difference-between-repeatedstratifiedkfold-and-stratifiedkfold-in-sk</guid>
      <pubDate>Sat, 19 Feb 2022 00:22:41 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的‘适合’是什么意思？[关闭]</title>
      <link>https://stackoverflow.com/questions/62976233/what-is-fit-in-machine-learning</link>
      <description><![CDATA[机器学习中的“适合”是什么意思？我注意到在某些情况下它是训练的同义词。
有人能用外行人能理解的术语解释一下吗？]]></description>
      <guid>https://stackoverflow.com/questions/62976233/what-is-fit-in-machine-learning</guid>
      <pubDate>Sun, 19 Jul 2020 04:18:19 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的 OOF 方法是什么？</title>
      <link>https://stackoverflow.com/questions/52396191/what-is-oof-approach-in-machine-learning</link>
      <description><![CDATA[我在许多 kaggle 笔记本中看到人们在使用 K-Fold 验证进行机器学习时谈论 oof 方法。什么是 oof？它与 k-fold 验证有关吗？此外，您能否推荐一些有用的资源来详细了解这个概念？
谢谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/52396191/what-is-oof-approach-in-machine-learning</guid>
      <pubDate>Wed, 19 Sep 2018 00:12:58 GMT</pubDate>
    </item>
    </channel>
</rss>