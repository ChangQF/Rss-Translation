<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 20 Apr 2024 15:13:34 GMT</lastBuildDate>
    <item>
      <title>在机器学习中处理大于 5% 的异常值？</title>
      <link>https://stackoverflow.com/questions/78358607/handling-outliers-greater-than-5-in-machine-learning</link>
      <description><![CDATA[在我的机器学习项目中，我有超过 5% 的问题，我可以克服什么方法来获取数据而不减少或删除一些数据
q1 = data.quantile(0.25)
q3 = 数据.分位数(0.75)
IQR = q3 - q1
磅 = q1 - (1.5*IQR)
ub = q3 + (1.5*IQR)
outlier_sum = ((数据 &gt; ub)|(数据 &lt; lb)).sum()
异常值百分比 = (异常值总和/len(数据))*100
print(&#39;离群值总和:\n&#39;, outlier_sum)
print(&#39;\n 异常值百分比:\n&#39;,outliers_percentage)

异常值总和：
 0岁
性别 139
总胆红素 75
直接胆红素 80
碱性磷酸酶69
谷氨酰胺转氨酶72
天冬氨酸转氨酶 64
总蛋白质 3
白蛋白0
白蛋白球蛋白比率 0
目标0
数据类型：int64

 异常值百分比：
 年龄 0.000000
性别 24.428822
总胆红素 13.181019
直接胆红素 14.059754
碱性磷酸酶 12.126538
谷氨酰胺转氨酶 12.653779
天冬氨酸转氨酶 11.247803
总蛋白质 0.527241
白蛋白 0.000000
白蛋白球蛋白比率 0.000000
目标 0.00000

我用这个方法计算了，有人能解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78358607/handling-outliers-greater-than-5-in-machine-learning</guid>
      <pubDate>Sat, 20 Apr 2024 14:36:51 GMT</pubDate>
    </item>
    <item>
      <title>多类问题的层次分类方法</title>
      <link>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-mutlclass-problem</link>
      <description><![CDATA[有一个多类分类任务。我的目标是使用每父节点本地分类器 (LCPN) 方法来解决这个问题。
让我解释一下如何使用 MWE。
假设我有这个虚拟数据集：
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification
从 scipy.cluster 导入层次结构

X, y = make_classification(n_samples=1000, n_features=10, n_classes=5,
                             n_信息=4）

我想出了这些类之间的距离矩阵：
d = np.array(
[[ 0.、201.537、197.294、200.823、194.517]、
 [201.537, 0., 199.449, 202.941, 196.703],
 [197.294, 199.449, 0., 198.728, 192.354],
 [200.823, 202.941, 198.728, 0., 195.972],
[[194.517, 196.703, 192.354, 195.972, 0.]]
）

因此，我确定了类层次结构，如下所示：
hc = hierarchy.linkage(d, method=&#39;complete&#39;)

得到的树状图如下：
dendrogram = hierarchy.dendrogram(hc, labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;, &#39;D&#39;, &#39;F&#39;])
树状图


我用树状结构来说明：

我的问题：
如何按照 LCPN 方法在每个内部节点（包括根）处安装分类器，例如 DecisionTreeClassifier 或 SVM，以像在树中一样进行上图？]]></description>
      <guid>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-mutlclass-problem</guid>
      <pubDate>Sat, 20 Apr 2024 14:08:05 GMT</pubDate>
    </item>
    <item>
      <title>如何进行标准化我无法理解为什么平均值和方差没有得到应用</title>
      <link>https://stackoverflow.com/questions/78358310/how-to-do-normalization-i-am-failing-to-understand-why-the-mean-and-variance-are</link>
      <description><![CDATA[代码应该如何工作
我的代码::
从tensorflow.keras.layers导入标准化
标准化器 = 标准化(均值= 5 , 方差= 4) # 标准化对象
Normalized_tns1 = tf.constant([[3,4,5,6,7]])
打印（归一化_tns1.shape）
print(&quot;\n输出:\n&quot;)
归一化器（归一化_tns1）

我正在做的事情与上图相同，但是当我这样做时，我收到了重塑错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
[88] 中的单元格，第 7 行
      5 打印（归一化_tns1.shape）
      6 print(&quot;\n输出:\n&quot;)
----&gt; 7 标准化器(normalized_tns1)

文件 c:\Users\Sujal07\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于filter_traceback..error_handler(*args, **kwargs)
    第 119 章
    120 # 要获取完整的堆栈跟踪，请调用：
    121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
    123最后：
    124 删除filtered_tb

文件c:\Users\Sujal07\anaconda3\Lib\site-packages\tensorflow\python\eager\execute.py:53，在quick_execute（op_name，num_outputs，输入，attrs，ctx，名称）中
     51 尝试：
     52 ctx.ensure_initialized()
---&gt; 53 张量 = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54 个输入、属性、输出数）
     55 除了 core._NotOkStatusException 为 e：
     56 如果名称不是 None：

InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} 重塑的输入是一个值为 1 的张量，但请求的形状有 5 [Op:Reshape]

请帮忙 // **感谢提前回复的人**
重塑可以工作，但我不想随着矩阵的变化而这样做]]></description>
      <guid>https://stackoverflow.com/questions/78358310/how-to-do-normalization-i-am-failing-to-understand-why-the-mean-and-variance-are</guid>
      <pubDate>Sat, 20 Apr 2024 13:00:57 GMT</pubDate>
    </item>
    <item>
      <title>如何计算尖峰神经网络电路实现中的“每个尖峰能量”？</title>
      <link>https://stackoverflow.com/questions/78358095/how-to-calculate-energy-per-spike-in-spiking-neural-networks-circuit-implemen</link>
      <description><![CDATA[在一些科学文献中（例如this和this) 关于 LIF（一种尖峰神经网络）的模拟电路实现，作者提到了“每尖峰能量”和“每个概要的能量”作为评价参数之一。有谁知道如何从电路级实现中计算它？
我想知道如何计算“每次尖峰能量”来自电路植入的参数。]]></description>
      <guid>https://stackoverflow.com/questions/78358095/how-to-calculate-energy-per-spike-in-spiking-neural-networks-circuit-implemen</guid>
      <pubDate>Sat, 20 Apr 2024 11:56:58 GMT</pubDate>
    </item>
    <item>
      <title>在 keras 中，当模型拟合 epochs=5000 时，代码看起来非常巨大</title>
      <link>https://stackoverflow.com/questions/78358018/in-keras-while-model-fitting-with-epochs-5000-the-code-looks-so-huge</link>
      <description><![CDATA[所以，我正在尝试深度学习中的梯度下降。代码是这样的。
model=keras.Sequential([keras.layers.Dense(1, input_shape= (2,),activation=&#39;sigmoid&#39;, kernel_initializer=&#39;ones&#39;,bias_initializer=&#39;zeros&#39;)])

model.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

model.fit(x_train_scaled,y_train, epochs=5000)

当 epochs=5000 时，我应该获得约 90% 的准确率。但它在 jupyter 中占有重要地位。
我试图在较短的空间内达到 90% 的准确率。因为当我想引用上面的代码时，有很多东西需要滚动。]]></description>
      <guid>https://stackoverflow.com/questions/78358018/in-keras-while-model-fitting-with-epochs-5000-the-code-looks-so-huge</guid>
      <pubDate>Sat, 20 Apr 2024 11:30:52 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试使用 grad-cam，但得到 ValueError: The layerequential has never be called and 因此没有定义的输入</title>
      <link>https://stackoverflow.com/questions/78357635/i-am-trying-to-use-grad-cam-but-get-valueerror-the-layer-sequential-has-never</link>
      <description><![CDATA[导入tensorflow为tf

将 numpy 导入为 np

导入CV2

从 keras.models 导入 load_model

模型 = load_model(r&#39;.\CNN.keras&#39;)

# 定义感兴趣的层（例如，conv2d_19）

图层名称 = &#39;conv2d_1&#39;

# 创建一个模型来输出所选层的激活

activation_model = tf.keras.Model(输入=model.input, 输出=model.get_layer(layer_name).output)

# 生成示例输入图像（替换为您的实际数据）

input_image = np.random.rand(1, 120, 120, 3) # 示例形状，根据需要调整

# 获取输出相对于所选层的梯度

使用 tf.GradientTape() 作为磁带：

    最后的卷积层输出 = 激活模型（输入图像）

    磁带.watch(last_conv_layer_output)

    preds = 模型（输入图像）

    top_class = tf.argmax(preds[0])

# 计算顶层类相对于所选层的梯度

grads = Tape.gradient(preds[:, top_class], last_conv_layer_output)[0]

# 计算热图的权重

权重 = tf.reduce_mean(梯度, 轴=(0, 1))

# 将特征图乘以权重以获得热图

热图 = tf.reduce_sum(last_conv_layer_output * 权重，轴=-1)

# 调整热图大小以匹配输入图像大小

heatmap = cv2.resize(heatmap.numpy(), (input_image.shape[1], input_image.shape[2]))

# 标准化热图

热图 = np.maximum(热图, 0) / np.max(热图)

# 将热图叠加在原始图像上

热图 = cv2.applyColorMap(np.uint8(255 * 热图), cv2.COLORMAP_JET)

superimpose_img = cv2.addWeighted(input_image[0], 0.6, 热图, 0.4, 0)

# 显示或保存叠加图像

cv2.imshow(“Grad-CAM”, superimulated_img)

cv2.waitKey(0)

cv2.destroyAllWindows()

这是我的代码，但我明白了
ValueError：层顺序从未被调用，因此没有定义的输入。

我希望从 grad-cam 获取热图，以便我可以可视化触发模型预测的图像部分]]></description>
      <guid>https://stackoverflow.com/questions/78357635/i-am-trying-to-use-grad-cam-but-get-valueerror-the-layer-sequential-has-never</guid>
      <pubDate>Sat, 20 Apr 2024 09:07:26 GMT</pubDate>
    </item>
    <item>
      <title>警告：由于元数据条目“名称”无效而跳过 C:\Users\abhis\AppData\Roaming\Python\Python312\site-packages\jupyter_client-8.6.0.dist-info [已关闭]</title>
      <link>https://stackoverflow.com/questions/78357455/warning-skipping-c-users-abhis-appdata-roaming-python-python312-site-packages</link>
      <description><![CDATA[[] [参考图片]我遇到了错误，希望得到解决该错误的帮助。有人可以提供有关如何解决此问题的指导吗？
它应该显示我的系统中安装的所有库列表，没有任何错误]]></description>
      <guid>https://stackoverflow.com/questions/78357455/warning-skipping-c-users-abhis-appdata-roaming-python-python312-site-packages</guid>
      <pubDate>Sat, 20 Apr 2024 08:09:54 GMT</pubDate>
    </item>
    <item>
      <title>在自动训练中将适配器与模型合并时出错</title>
      <link>https://stackoverflow.com/questions/78357392/error-in-merging-adapter-with-model-in-autotrain</link>
      <description><![CDATA[我正在尝试使用 Hugging Face 中的自动训练来微调某些模型。由于我没有大量的计算资源，因此我尝试微调模型 EleutherAI/pythia-14m 和此数据集。但我收到了这条消息：
无法合并适配器权重：为 PeftModelForCausalLM 加载 state_dict 时出错：
    base_model.model.gpt_neox.embed_in.weight 的大小不匹配：从检查点复制形状为 torch.Size([50280, 128]) 的参数，当前模型中的形状为 torch.Size([50277, 128])。
    base_model.model.embed_out.weight 的大小不匹配：从检查点复制形状为 torch.Size([50280, 128]) 的参数，当前模型中的形状为 torch.Size([50277, 128])。

当我启动此脚本时发生此错误，该脚本只是用 Jupiter 编写的终端命令。
!auto​​train llm \
     - 火车 \
    --模型“EleutherAI/pythia-14m” \
    --项目名称“my-llm” \
    --数据路径数据/ \
    --text-列文本 \
    --批量大小“4” \
    --lr“2e-5” \
    --纪元“3” \
    --块大小“1024” \
    --预热比率“0.03” \
    --lora-r“16” \
    --lora-alpha“32”； \
    --lora-dropout“0.05” \
    --权重衰减“0”。 \
    --梯度累积“4” \
    --logging-steps“10” \
    --use-peft \
    --合并适配器\

此外，当我尝试在拥抱脸部空间中进行自动训练时，也出现了同样的问题。
我没有机器学习经验，所以我无法想象，什么会导致这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/78357392/error-in-merging-adapter-with-model-in-autotrain</guid>
      <pubDate>Sat, 20 Apr 2024 07:38:52 GMT</pubDate>
    </item>
    <item>
      <title>从 torchensemble 中的基本模型获取嵌入</title>
      <link>https://stackoverflow.com/questions/78355585/getting-embeddings-from-the-base-model-in-torchensemble</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78355585/getting-embeddings-from-the-base-model-in-torchensemble</guid>
      <pubDate>Fri, 19 Apr 2024 18:25:20 GMT</pubDate>
    </item>
    <item>
      <title>在序列模型中使用归一化层时，adapt() 会出错吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78355246/adapt-gives-error-while-using-normalization-layer-in-sequential-models</link>
      <description><![CDATA[在顺序模型中使用归一化层时，通过适应（），我收到了未绑定错误：
这是错误
我做了以下事情：
标准化器 = 标准化()
标准化器.adapt(X_train)

但这给出了以下错误：
未绑定错误：赋值之前引用了局部变量“input_shape”。
为什么我会收到此错误？如果不是这样，还有其他方法可以标准化神经网络中的数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/78355246/adapt-gives-error-while-using-normalization-layer-in-sequential-models</guid>
      <pubDate>Fri, 19 Apr 2024 17:04:02 GMT</pubDate>
    </item>
    <item>
      <title>使用 CNN 进行音频分类总是预测错误</title>
      <link>https://stackoverflow.com/questions/78354074/audio-classification-using-cnn-predicting-wrong-all-the-time</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78354074/audio-classification-using-cnn-predicting-wrong-all-the-time</guid>
      <pubDate>Fri, 19 Apr 2024 13:39:10 GMT</pubDate>
    </item>
    <item>
      <title>获取边界框问题</title>
      <link>https://stackoverflow.com/questions/78353726/getting-bounding-box-issue</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78353726/getting-bounding-box-issue</guid>
      <pubDate>Fri, 19 Apr 2024 12:40:40 GMT</pubDate>
    </item>
    <item>
      <title>LMST模型敏感性——初学者抗运气</title>
      <link>https://stackoverflow.com/questions/78349854/lmst-model-sensitivity-beginners-anti-luck</link>
      <description><![CDATA[我一直在尝试使用艾伯塔省电力市场的一些非常基本的数据，并尝试使用时间序列数据的 LMST 模型来尝试预测价格。我确实得到“可能”这是我的模型的结果，而且它似乎确实出现了我们可以预期的一些波动（仅根据我自己的市场经验）。
但是，我正在寻求更好地理解我遇到的一些陷阱。
从 keras.models 导入顺序
从 keras.layers 导入 LSTM
从 keras.layers 导入 Dropout
从 keras.layers 导入密集
将 pandas 导入为 pd
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.model_selection 导入 train_test_split
从 sklearn.metrics 导入mean_absolute_error,mean_squared_error
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns
导入作业库

# 加载数据
# 加载数据

# 加载数据
csv_file_path = &#39;Frankenstein.csv&#39; # 使用您的实际文件路径更新
df = pd.read_csv(csv_file_path)

# 将“日期/时间”转换为日期时间并提取数据集中存在的组件
如果 df.columns 中的“日期/时间”：
    df[&#39;日期&#39;] = pd.to_datetime(df[&#39;日期/时间&#39;])
    df[&#39;年份&#39;] = df[&#39;日期&#39;].dt.year
    df[&#39;月份&#39;] = df[&#39;日期&#39;].dt.月份
    df[&#39;日期&#39;] = df[&#39;日期&#39;].dt.day
    df[&#39;小时&#39;] = df[&#39;日期&#39;].dt.小时
    df.drop([&#39;日期/时间&#39;, &#39;日期&#39;], axis=1, inplace=True)

# 假设“价格”是目标变量
features = df.drop([&#39;价格&#39;], axis=1)
目标 = df[&#39;价格&#39;]

# 标准化特征和目标
缩放器特征 = MinMaxScaler()
features_scaled = scaler_features.fit_transform(features)
缩放器目标 = MinMaxScaler()
target_scaled = scaler_target.fit_transform(target.values.reshape(-1, 1))

# 创建序列函数
def create_sequences（特征，目标，time_steps = 100）：
    X、y = []、[]
    对于范围内的 i(len(features) - time_steps)：
        X.append(特征[i:(i + time_steps)])
        y.append(目标[i + time_steps])
    返回 np.array(X), np.array(y)

# 使用整个数据集创建序列
X, y = create_sequences(features_scaled, target_scaled.flatten())

# 模型配置
input_shape = (X.shape[1], X.shape[2]) # (time_steps, num_features)

# 定义LSTM模型
模型=顺序（[
    LSTM（单位= 100，return_sequences = True，input_shape = input_shape），
    辍学（0.1），
    LSTM（单位=100），
    辍学（0.1），
    密集（单位=100，激活=&#39;elu&#39;），
    Dense(1) # 预测单个值
]）

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)

# 在整个数据集上训练模型
历史= model.fit（X，y，纪元= 150，batch_size = 20，validation_split = 0.1）

# 情节训练&amp;验证损失值
plt.figure(figsize=(10, 6))
plt.plot(history.history[&#39;loss&#39;], label=&#39;火车&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;验证&#39;)
plt.title(&#39;模型损失&#39;)
plt.ylabel(&#39;损失&#39;)
plt.xlabel(&#39;纪元&#39;)
plt.legend(loc=&#39;右上&#39;)
plt.show()

# 保存LSTM模型
model_save_path = &#39;trained_lstm_model.h5&#39;
model.save(model_save_path)
print(f&quot;模型已保存到 {model_save_path}&quot;)
joblib.dump(scaler_features, &#39;scaler_features.pkl&#39;)
joblib.dump(scaler_target, &#39;scaler_target.pkl&#39;)

有人可以给绝对的初学者一些建议吗？主要是为了更好地理解我应该如何设置它。我有一个每小时的数据集，是过去三年的历史生成和交换。我正在寻找方法让我的模型对供应与价格的变化更具反应性。]]></description>
      <guid>https://stackoverflow.com/questions/78349854/lmst-model-sensitivity-beginners-anti-luck</guid>
      <pubDate>Thu, 18 Apr 2024 19:18:18 GMT</pubDate>
    </item>
    <item>
      <title>对于表格数据模型中的过度拟合我该怎么办</title>
      <link>https://stackoverflow.com/questions/78333191/what-can-i-do-about-overfitting-in-tabular-data-model</link>
      <description><![CDATA[我建立了一个预测模型，用于根据所提供数据中的某些特征来预测结果。
该模型是一个利用 fastai 的表格学习器。
该数据集包含约 300 条记录，分为训练集、验证集和测试集。
我已经实现了解决过度拟合的技术，例如提前停止和权重衰减，但在对未见过的数据进行评估时，模型仍然似乎过度拟合。
此外，我还尝试调整学习率和批量大小等超参数，但没有改善。我怀疑我的模型架构或预处理管道的某些方面可能会导致该问题，但我不确定从哪里开始调查。
鉴于该项目的敏感性，我无法提供有关数据集或预测任务的具体细节，但我可以分享当前模型的预处理和结构。
这是训练的输出：

&lt;标题&gt;

纪元
train_loss
valid_loss
准确度
时间


&lt;正文&gt;

0
0.752707
0.579501
0.776119
00:00


1
0.699270
0.833771
0.776119
00:00


2
0.652438
0.598243
0.791045
00:00


3
0.621083
3.889398
0.776119
00:00


4
0.591348
0.632366
0.791045
00:00


5
0.580582
6.670314
0.791045
00:00



&lt;块引用&gt;
自 epoch 2 以来没有任何改进：提前停止

这是预处理的代码（在我构建了我不能透露的功能之后）。
features 列表定义每个特征，包括有效值范围和权重（feature、range_ 和 weight 如下面的标准化函数中所使用的那样）。
def custom_normalize(df, 特征, range_, 权重):
    df[特征] = 归一化(df[特征], range_)
    df[特征] = df[特征] * 权重
    返回df

分割 = RandomSplitter(valid_pct=0.2)(range_of(df))

procs = [分类，填充缺失]

对于功能，features.items() 中的信息：
    # 确定训练时选择值的范围。
    procs.append(partial(custom_normalize, feature=feature, range_=info[&#39;range&#39;],weight=info[&#39;weight&#39;]))

据我所知，构建模型和训练是非常标准的：
to = TabularPandas(df, procs=procs,
                   cat_names = cat_vars,
                   连续名称=连续变量，
                   y_names=dep_var,
                   分裂=分裂）

dls = to.dataloaders(bs=64)

Early_stop = EarlyStoppingCallback(监视器=&#39;准确度&#39;, min_delta=0.01, 耐心=3)

学习 = tabular_learner(dls, 指标=准确度, wd=0.1)
学习.lr_find()

# 绘制学习率。
learn.recorder.plot_lr_find()

# 根据情节选择学习率。
lr = learn.recorder.lrs[np.argmin(learn.recorder.losses)]

learn.fit_one_cycle(15, lr, cbs=early_stop)
学习.show_results()

# 如果模型不存在则只保存模型
# TODO 将保存包装在条件中，以防止模型存在时保存。
如果不是 os.path.exists(model_fname):
    学习.保存(model_fname)
]]></description>
      <guid>https://stackoverflow.com/questions/78333191/what-can-i-do-about-overfitting-in-tabular-data-model</guid>
      <pubDate>Tue, 16 Apr 2024 08:38:01 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：使用 `bitsandbytes` 8 位量化需要加速：`pip install Accelerate`</title>
      <link>https://stackoverflow.com/questions/78040978/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</link>
      <description><![CDATA[我正在尝试使用开源数据集微调 llama2-13b-chat-hf。
我一直使用此模板，但现在收到此错误：
导入错误：使用bitsandbytes 8位量化需要加速：pip install Accelerate和最新版本的bitsandbytes：pip install -i https://pypi .org/simple/bitsandbytes
我安装了所需的所有软件包，这些是版本：
加速@git+https://github.com/huggingface/accelerate.git@97d2168e5953fe7373a06c69c02c5a00a84d5344
    位和字节==0.42.0
    数据集==2.17.1
    拥抱脸集线器==0.20.3
    佩夫特==0.8.2
    分词器==0.13.3
    火炬==2.1.0+cu118
    火炬音频==2.1.0+cu118
    火炬视觉==0.16.0+cu118
    变形金刚==4.30.0
    trl==0.7.11

有人知道这是不是版本问题吗？
你是如何解决这个问题的？
我尝试安装其他版本，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78040978/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</guid>
      <pubDate>Thu, 22 Feb 2024 12:37:11 GMT</pubDate>
    </item>
    </channel>
</rss>