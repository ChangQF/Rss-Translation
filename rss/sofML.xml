<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 25 Sep 2024 18:22:21 GMT</lastBuildDate>
    <item>
      <title>尽管 np.where(np.isnan(X)) 没有返回任何内容，sklearn 仍会引发错误“输入 X 包含 NaN”</title>
      <link>https://stackoverflow.com/questions/79024134/sklearn-raises-error-input-x-contains-nan-despite-np-wherenp-isnanx-return</link>
      <description><![CDATA[我正在尝试为著名的泰坦尼克号数据集制作一个简单的 Knearestneighbor 算法。尽管我的数据框似乎没有任何 NaN 值，但我还是不断收到标题中的错误。我迷路了，希望能得到一些帮助。
y = train_data[&quot;Survived&quot;]
features = [&quot;Fare&quot;, &quot;Sex&quot;]

X = train_data[features]
X.loc[:, &#39;Sex&#39;] = X.loc[:, &#39;Sex&#39;].apply(weigh_sex)
X = ct.fit_transform(X)

X_test = test_data[features]
X_test.loc[:, &quot;Sex&quot;] = X_test.loc[:, &quot;Sex&quot;].apply(weigh_sex)
X_test = ct.fit_transform(X_test)

print(np.where(np.isnan(X))) #即使放在 clf.fit() 之后，也不会返回任何内容

clf.fit(X, y)

predictions = clf.predict(X_test) #此行引发错误
output = pd.DataFrame({&#39;PassengerId&#39;: test_data.PassengerId, &#39;Survived&#39;: predictions})
output.to_csv(&#39;submission1.csv&#39;, index=False)

编辑完整回溯
回溯（最近一次调用最后一次）：
文件“C:\pystuff\StatsSandbox\Titanic.py”，第 65 行，位于&lt;module&gt;
predictions = clf.predict(X_test) #此行引发错误
文件“C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\neighbors\_classification.py”，第 266 行，在 predict 中
neigh_ind = self.kneighbors(X, return_distance=False)
文件“C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\neighbors\_base.py”，第 804 行，在 kneighbors 中
X = self._validate_data(X, accept_sparse=&quot;csr&quot;, reset=False, order=&quot;C&quot;)
文件“C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\base.py”，第 605 行，在 _validate_data 中
out = check_array(X, input_name=&quot;X&quot;, **check_params)
文件 &quot;C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\utils\validation.py&quot;, 第 957 行, 在 check_array 中
_assert_all_finite(
文件 &quot;C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\utils\validation.py&quot;, 第 122 行, 在 _assert_all_finite
_assert_all_finite_element_wise(
文件 &quot;C:\pystuff\learn\StatsSandbox\lib\site-packages\sklearn\utils\validation.py&quot;, 第 171 行, 在 _assert_all_finite_element_wise
raise ValueError(msg_err)
ValueError: 输入 X 包含 NaN。
 ]]></description>
      <guid>https://stackoverflow.com/questions/79024134/sklearn-raises-error-input-x-contains-nan-despite-np-wherenp-isnanx-return</guid>
      <pubDate>Wed, 25 Sep 2024 17:26:01 GMT</pubDate>
    </item>
    <item>
      <title>如果训练集和测试集完全不相交，那么可以使用哪些 ML/DL 模型来玩 Hangman 游戏？</title>
      <link>https://stackoverflow.com/questions/79023327/what-ml-dl-models-can-be-used-to-play-the-game-hangman-given-the-training-and-t</link>
      <description><![CDATA[Hangman 是一款猜字游戏，玩家需要逐个字母猜出隐藏的单词。每次猜对都会显示单词中该字母的所有实例，而猜错则会减少玩家剩余的尝试次数。目标是在尝试次数用完之前猜出单词。隐藏单词的长度是可变的，最大尝试次数为 5 次。
我必须想出一个模型/算法来猜测隐藏字符串的字母（例如，如果隐藏单词是“mathematics”，输入是 _ a t h e _ a t i _ s，则模型理想情况下应该猜测“m”或“c”）。测试将通过模拟从完全隐藏的单词（仅下划线）开始的绞刑游戏来完成。
训练数据是来自英语的单词列表，测试数据将不包含任何这些给定的单词。
机器学习方法是首选，但不是强制性的。
我尝试在通过随机删除给定单词中的字母实例生成的数据上训练 BiLSTM 模型，然后为输入字符串中的每个位置（包括已经显示的单词）输出 27 个类别的 softmax（每个字母 1 个，填充字符 1 个）。由于训练集中的最大单词数为 32，因此我将输入字符串填充为 32 个字符。这并没有给出令人满意的结果。
还尝试了字节对编码 (BPE) 来提取训练词汇表中的相关子词，然后尝试从测试集中猜测单词是 BPE 词汇表中三个单词的组合。
我还能尝试什么？这会属于哪种问题？我在哪里可以阅读有关此类问题的更多信息？]]></description>
      <guid>https://stackoverflow.com/questions/79023327/what-ml-dl-models-can-be-used-to-play-the-game-hangman-given-the-training-and-t</guid>
      <pubDate>Wed, 25 Sep 2024 14:03:42 GMT</pubDate>
    </item>
    <item>
      <title>Jupyter Notebook 和 Python 脚本返回不同的 YOLOV8 实例分割掩码</title>
      <link>https://stackoverflow.com/questions/79023273/jupyter-notebook-and-python-script-return-different-yolov8-instance-segmentation</link>
      <description><![CDATA[我一直在运行预先训练的 YOLOV8 模型来进行实例分割。为了进行测试和开发，我一直在使用 Anaconda Jupyter Notebook，在部署之前我将其转换为 Python 脚本。
这是来自 Jupyter Notebook 的（正确）掩码：

这是来自 Python 脚本的结果：

您可以看到 Python 结果在边缘处有奇怪的块我的头和手机。

两个版本的代码完全相同
Python 虚拟环境直接从 Jupyter 环境导出创建（我仔细检查了：Python 版本相同，所有库的版本也相同）
CUDA 版本相同
硬件相同（两个示例都在同一台机器上执行）

有人知道为什么会发生这种情况或我该如何修复它吗？]]></description>
      <guid>https://stackoverflow.com/questions/79023273/jupyter-notebook-and-python-script-return-different-yolov8-instance-segmentation</guid>
      <pubDate>Wed, 25 Sep 2024 13:53:07 GMT</pubDate>
    </item>
    <item>
      <title>我正在实现 qwen_2-vl +byaldi，用于基于视觉的 ocr，并将其托管为基于 stremlit 的 Web 应用程序，但它无法正常工作，一直崩溃</title>
      <link>https://stackoverflow.com/questions/79022489/i-am-doing-an-implementation-of-qwen-2-vl-byaldi-for-vision-based-ocr-and-hosti</link>
      <description><![CDATA[我正在开发一个 Streamlit 应用程序，该应用程序利用多模态模型进行图像搜索和文本提取。但是，在尝试下载提取的文本时，该应用程序经常在文本提取过程中崩溃。以下是相关代码，以及重现问题的步骤。
requirement.txt
pdf2image

git+https://github.com/huggingface/transformers.git

qwen-vl-utils

#flash-attn

byaldi

qwen_vl_utils

transformers

重现步骤

使用提供的代码运行 Streamlit 应用。
上传有效的图像文件 (JPG、JPEG、PNG)。
在提供的输入框中输入文本查询。
单击“搜索并提取文本”按钮。

import streamlit as st
import base64
from huggingface_hub import notebook_login
from byaldi import RAGMultiModalModel
from transformers import Qwen2VLForConditionalGeneration、AutoTokenizer、AutoProcessor
从 PIL 导入图像
从 io 导入 BytesIO
导入 torch
导入 re

@st.cache_resource
def load_models():
RAG = RAGMultiModalModel.from_pretrained(&quot;vidore/colpali&quot;, verbose=10)
model = Qwen2VLForConditionalGeneration.from_pretrained(
&quot;Qwen/Qwen2-VL-2B-Instruct&quot;,
torch_dtype=torch.float16,
device_map=&quot;auto&quot;,
)
processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen2-VL-2B-Instruct&quot;)
return RAG、model、processor

RAG、model、processor = load_models()

st.title(&quot;多模态图像搜索和文本提取App&quot;)

uploaded_file = st.file_uploader(&quot;选择图片&quot;, type=[&quot;jpg&quot;, &quot;jpeg&quot;, &quot;png&quot;])

如果 uploaded_file 不为 None:
image = Image.open(uploaded_file)
st.image(image, caption=&#39;Uploaded Image&#39;, use_column_width=True)

temp_image_path = &quot;uploaded_image.jpeg&quot;
image.save(temp_image_path)

@st.cache_data
def create_rag_index(image_path):
RAG.index(
input_path=image_path,
index_name=&quot;image_index&quot;,
store_collection_with_index=True,
overwrite=True,
)

create_rag_index(temp_image_path)

text_query = st.text_input(&quot;输入您的文本查询&quot;)

if st.button(&quot;搜索并提取文本&quot;):
if text_query:
results = RAG.search(text_query, k=1, return_base64_results=True)

image_data = base64.b64decode(results[0].base64)
image = Image.open(BytesIO(image_data))
st.image(image, caption=&quot;结果图像&quot;, use_column_width=True)

messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: [
{&quot;type&quot;: &quot;image&quot;},
{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;extract text&quot;}
]
}
]

text_prompt = processing.apply_chat_template(messages, add_generation_prompt=True)

input = processing(
text=[text_prompt],
images=[image],
padding=True,
return_tensors=&quot;pt&quot;
)

输入 = 输入.to(model.device)

使用 torch.no_grad():
输出_ids = 模型.generate(**输入, max_new_tokens=1024)

生成_ids = 输出_ids[:, 输入.输入_ids.shape[1]:]

输出_文本 = 处理器.batch_decode(
生成_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True
)[0]

# 突出显示查询的文本
def 突出显示_文本(文本, 查询):
突出显示_文本 = 文本
for word in query.split():
模式 = re.compile(re.escape(word), re.IGNORECASE)
突出显示_文本 = 模式.sub(lambda m: f&#39;&lt;span style=&quot;background-color: yellow;&quot;&gt;{m.group()}&lt;/span&gt;&#39;, highlight_text)
return highlight_text

highlight_output = highlight_text(output_text, text_query)

st.subheader(&quot;提取的文本（查询突出显示）：&quot;)
st.markdown(highlighted_output, unsafe_allow_html=True)
else:
st.warning(&quot;请输入查询。&quot;)
else:
st.info(&quot;上传图片以开始。&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79022489/i-am-doing-an-implementation-of-qwen-2-vl-byaldi-for-vision-based-ocr-and-hosti</guid>
      <pubDate>Wed, 25 Sep 2024 10:56:35 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 AI / OCR 检测和验证文档中的复选框[关闭]</title>
      <link>https://stackoverflow.com/questions/79022347/how-to-detect-and-validate-checkboxes-in-documents-using-ai-ocr</link>
      <description><![CDATA[我需要开发可以自动验证称为“W 合同”的文档的软件。这些合同包含许多需要填写的复选框和字段，我的目标是准确识别哪些框被选中并根据特定规则和参数验证值。
我尝试使用 OpenAI 的 API 来检测这些元素，但我发现它在处理文档中的大量复选框和选项时遇到了困难。它经常错误识别结构或无法正确识别复选框。
我正在寻找可以帮助我更准确地执行此任务的技术、库或 API 的建议，最好使用 OCR 和计算机视觉。理想的解决方案应该能够：
检测 PDF 或图像文档中的复选框和输入字段。
准确确定复选框是否被选中。
读取字段并根据某些规则验证数据。
任何关于可以促进这项任务的工具或方法的建议都将不胜感激。我对基于 AI 的解决方案持开放态度，但也对传统的 OCR 方法感兴趣。
]]></description>
      <guid>https://stackoverflow.com/questions/79022347/how-to-detect-and-validate-checkboxes-in-documents-using-ai-ocr</guid>
      <pubDate>Wed, 25 Sep 2024 10:22:52 GMT</pubDate>
    </item>
    <item>
      <title>将双线性采样特征映射回体素</title>
      <link>https://stackoverflow.com/questions/79022319/map-bilinear-sampled-features-to-voxel-back</link>
      <description><![CDATA[我正在开发一个专门用于双线性采样的类。我的目标是将从双线性采样过程中提取的特征映射到创建的体素网格中的适当位置。
实施步骤：
B = 批次

C = 通道

S = 视图数

D = 深度

H = 高度

W = 宽度

3 = x,y,z

2 = x,y

我创建了一个具有此形状 [B,D,H,W,3] 的体素
并且我有 360 个图像视图样本，尺寸为 [B,S,C,H,W]
第一步，我使用外部和内部将体素点（点云）投影到每个图像。然后我过滤了图像之外的点。
结果我得到了 valid_points = [B,S,H,W,2]
我从 valid_points 中对我的点进行了归一化，并创建了大小为 [B,S,H,W,2] 的网格，请注意，W 是有效点的数量，H = 1
然后我所做的是应用双线性采样，如代码所示：
valid_points = cur_coords[:, on_img[1]]

######### 将有效点归一化在 [-1, 1] 之间 ########
normalized_points = torch.zeros_like(valid_points)
normalized_points[:,:, 0] = 2.0 * (valid_points[:, :, 0] / (H_img - 1)) - 1.0 # 归一化y 坐标
normalized_points[:,:, 1] = 2.0 * (valid_points[:, :, 1] / (W_img - 1)) - 1.0 # [N, M&#39;,2]

grid = normalized_points.unsqueeze(1).cuda() # 形状 [S, H_out, W_out, 2]
sampled_features_with_location_list = []
for i in range(0,N):
img_s =camera_view_tensor[i].unsqueeze(0).permute(0, 3, 1, 2) #[B, C, H_in, W_in]
grid_s = grid[i].unsqueeze(0)
sampled_points = F.grid_sample(img_s, grid_s,mode=&#39;bilinear&#39;,
align_corners=None) # (B,N,C,H_out,W_out)
sampled_features_list.append(sampled_pointson)
sampled_points = torch.stack(sampled_features_list, dim=1) #[B = 1,S = 6,C= 3,H= 1,W =22965]

现在我想应用逆映射来提取 bev 特征。怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/79022319/map-bilinear-sampled-features-to-voxel-back</guid>
      <pubDate>Wed, 25 Sep 2024 10:14:39 GMT</pubDate>
    </item>
    <item>
      <title>在不断演变的特征空间中采用反馈进行自适应预测的技术</title>
      <link>https://stackoverflow.com/questions/79022083/techniques-for-adaptive-prediction-with-feedback-in-an-evolving-feature-space</link>
      <description><![CDATA[我正在研究一个预测问题，其中目标变量 𝑦 来自正态分布，连续特征空间 𝑋 和 𝑦 之间的关系随时间保持稳定。但是，目标值（例如平均值和标准差）会随着系统的变化而随时间变化。我事先并不了解真正的目标值，因此我利用在线回归和强化学习 (RL) 等技术根据反馈迭代调整我的预测。
反馈机制仅指示我的预测是高估还是低估，并且此反馈是在延迟后提供的。误差大小未知，并且仅提供方向性反馈（高估/低估）。我目前正在使用增量方法，根据反馈向上或向下调整预测以更新预测。
关于改进此方法，我有两个主要问题：
(1) 改进自适应调整技术：

我目前正在使用在线回归和 RL，反馈会告诉我是否增加或减少先前的预测。除了简单的增量/减量之外，我是否应该探索更先进的技术来进行更智能的调整？具体来说，当反馈仅限于过度/不足指示时，是否有方法可以进行更细微的调整，并且随着时间的推移，这可能导致更快的收敛或更好的预测？

(2) 自适应更新值的有效管理：

为了增强我目前的方法，我考虑保持预测的动态上限和下限，并根据反馈调整这些界限（即缩小过度/不足估计之间的范围）。我探索的另一种策略是使用先前调整的指数加权移动平均线 (EWMA)，其中反复的低估会导致逐渐增大的校正。但是，在大型特征空间中管理这些调整在计算上是昂贵的。
我最初使用字典将特征 𝑋 映射到这些更新值（例如界限或 EWMA 调整）的方法随着特征空间的增长变得不切实际。我也尝试将这些值映射到回归模型，但效果不佳，可能是由于更新的非平稳性质。
鉴于这些调整值不是静态目标，而是基于反馈动态变化的，在高维特征空间中有效管理或建模它们的最佳方法是什么？在这样的设置中，是否有更合适的自适应更新策略，可能涉及函数逼近技术或内存高效的数据结构？
]]></description>
      <guid>https://stackoverflow.com/questions/79022083/techniques-for-adaptive-prediction-with-feedback-in-an-evolving-feature-space</guid>
      <pubDate>Wed, 25 Sep 2024 09:24:03 GMT</pubDate>
    </item>
    <item>
      <title>SBERT 微调总是在完成所有 epoch 之前停止</title>
      <link>https://stackoverflow.com/questions/79021064/sbert-fine-tuning-always-stops-before-finish-all-epochs</link>
      <description><![CDATA[我正在使用 SBERT 预训练模型（特别是 MiniLM）进行一个包含 995 个分类的文本分类项目。我大部分时间都在按照此处列出的步骤进行操作，一切似乎都运行正常。
我的问题出现在实际训练模型时。无论我在训练参数中设置什么值，训练似乎总是提前结束，并且永远不会完成所有批次。例如，我设置了 num_train_epochs=1，但它最多只能达到 0.49 个 epoch。如果 num_train_epochs=4，它总是在 3.49 个 epoch 处结束。
这是我的代码：
from datasets import load_dataset
from sentence_transformers import (
SentenceTransformer,
SentenceTransformerTrainer,
SentenceTransformerTrainingArguments,
SentenceTransformerModelCardData,
)
from sentence_transformers.losses import BatchAllTripletLoss
from sentence_transformers.training_args import BatchSamplers
from sentence_transformers.evaluation import TripletEvaluator

model = SentenceTransformer(
&quot;nreimers/MiniLM-L6-H384-uncased&quot;,
model_card_data=SentenceTransformerModelCardData(
language=&quot;en&quot;,
license=&quot;apache-2.0&quot;,
model_name=&quot;all-MiniLM-L6-v2&quot;,
)
)

loss = BatchAllTripletLoss(model)
# 损失概述：https://www.sbert.net/docs/sentence_transformer/loss_overview.html
# 此特定损失方法：https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#batchalltripletloss

# 训练参数：https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
args = SentenceTransformerTrainingArguments(
# 必需参数：
output_dir=&quot;finetune/model20240924&quot;,
# 可选训练参数：
num_train_epochs=1,
max_steps = -1,
per_device_train_batch_size=8,
per_device_eval_batch_size=8,
learning_rate=1e-5,
warmup_ratio=0.1,
fp16=True, # 如果您收到 GPU 无法在 FP16 上运行的错误，请设置为 False
bf16=False, # 如果您拥有支持 BF16 的 GPU，请设置为 True
batch_sampler=BatchSamplers.GROUP_BY_LABEL, # 
# 可选的跟踪/调试参数：
eval_strategy=&quot;no&quot;,
eval_steps=100,
save_strategy=&quot;epoch&quot;,
# save_steps=100,
save_total_limit=2,
logs_steps=100,
run_name=&quot;miniLm-triplet&quot;, # 如果在 W&amp;B 中使用`wandb` 已安装
)

trainer = SentenceTransformerTrainer(
model=model,
args=args,
train_dataset=trainDataset,
eval_dataset=devDataset,
loss=loss,
#evaluator=dev_evaluator,
)
trainer.train()

请注意，我没有使用评估器，因为我们正在创建模型，并在事后使用专用的测试值集对其进行测试。我的数据集结构如下：
Dataset({
features: [&#39;Title&#39;, &#39;Body&#39;, &#39;label&#39;],
num_rows: 23961
})

与 dev 数据集具有相同的结构，只是行数较少。这将提供以下输出：
 [1473/2996 57:06 &lt; 59:07，0.43 it/s，Epoch 0/1]
步骤训练损失
100 1.265600
200 0.702700
300 0.633900
400 0.505200
500 0.481900
600 0.306800
700 0.535600
800 0.369800
900 0.265400
1000 0.345300
1100 0.516700
1200 0.372600
1300 0.392300
1400 0.421900

TrainOutput(global_step=1473, training_loss=0.5003972503496366, metrics={&#39;train_runtime&#39;: 3427.9198, &#39;train_samples_per_second&#39;: 6.99, &#39;train_steps_per_second&#39;: 0.874, &#39;total_flos&#39;: 0.0, &#39;train_loss&#39;: 0.5003972503496366, &#39;epoch&#39;: 0.4916555407209613})

无论我如何调整值，我都无法让它完成所有批次。如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79021064/sbert-fine-tuning-always-stops-before-finish-all-epochs</guid>
      <pubDate>Wed, 25 Sep 2024 03:55:44 GMT</pubDate>
    </item>
    <item>
      <title>如何将 CIFAR10 模型的准确率提高到 80％ 以上？[关闭]</title>
      <link>https://stackoverflow.com/questions/79020893/how-to-increase-accurracy-for-cifar10-model-above-80-accuracy</link>
      <description><![CDATA[有人能帮助我吗？我使用来自 tensorflow 数据集的 CIFAR10 数据集训练我的机器学习模型，但我无法将模型准确率提高到 80% 以上...
有人能给我一个建议吗？
import tensorflow as tf
import time
import tensorflow_datasets as tfds
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()

def normalize(train_images, test_images):
normalized_train_dataset = tf.cast(train_images, tf.float32) / 255.0
normalized_test_dataset = tf.cast(test_images, tf.float32) / 255.0
返回 normalized_train_dataset, normalized_test_dataset

# Normalisasi Dataset
train_dataset, test_dataset = normalize(train_images, test_images)

def visualization(image, image_sample=2):
for i in range (image_sample):

print(f&quot;弯曲图像：{np.shape(image)}&quot;)
print(f&quot;弯曲数据：{image[i].dtype}&quot;)
print(f&quot;Nilai 最大图像：{np.max(image[i])}&quot;)
print(f&quot;Nilai 最小图像：{np.min(image[i])}&quot;)

plt.figure(figsize=(6,6))
plt.imshow(image[i])
plt.axis(&#39;off&#39;)
plt.colorbar()
plt.title(&quot;Gambar CIFAR-10&quot;)
plt.grid(False)
plt.show()

visualization(train_dataset)

train_labels = np.squeeze(train_labels)
test_labels = np.squeeze(test_labels)

print(f&quot;Shape Of Train Label : {train_labels.shape}&quot;)

print(f&quot;Shape Of Test_Label : {test_labels.shape}&quot;)

train_labels= to_categorical(train_labels, num_classes=10)
test_labels = to_categorical(test_labels, num_classes=10)

从 tensorflow.keras.preprocessing.image 导入 ImageDataGenerator

datagen = ImageDataGenerator(
rotation_range=20,
width_shift_range=0.2,
height_shift_range=0.2,
sheath_range=0.2,
zoom_range=0.2,
Horizo​​ntal_flip=True,
fill_mode=&#39;nearest&#39;
)

model = tf.keras.models.Sequential([
tf.keras.layers.Conv2D(32, (3,3), padding=&#39;same&#39;,activation=tf.nn.relu, input_shape=(32, 32, 3)),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(64, (3,3), padding=&#39;same&#39;,activation=tf.nn.relu),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(128, (3,3), padding=&#39;same&#39;, 激活=tf.nn.relu),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(128, (3,3), padding=&#39;same&#39;, 激活=tf.nn.relu),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.MaxPool2D((2,2), strides=2),

tf.keras.layers.Conv2D(512, (3,3)，padding=&#39;same&#39;，activation=tf.nn.relu，kernel_regularizer=tf.keras.regularizers.l2(0.01))，
tf.keras.layers.BatchNormalization()，
tf.keras.layers.MaxPool2D((2,2)，strides=2)，

tf.keras.layers.Flatten()，
tf.keras.layers.Dense(512，activation=tf.nn.relu)，
tf.keras.layers.Dropout(0.3)，

tf.keras.layers.Dense(128，activation=tf.nn.relu)，
tf.keras.layers.Dropout(0.5)，

tf.keras.layers.Dense(10，激活=tf.nn.softmax)
])

model.summary()

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

early_stopping = tf.keras.callbacks.EarlyStopping(
monitor=&#39;val_loss&#39;,
patience=5,
restore_best_weights=True
)

reducer_lr = tf.keras.callbacks.ReduceLROnPlateau(
monitor=&#39;val_loss&#39;,
factor=0.2,
patience=3,
verbose=1,
min_lr=0.00001
)

callbacks = [early_stopping, reducer_lr]

start_time = time.time()

history = model.fit(datagen.flow(
train_dataset,
train_labels,
batch_size=64),
epochs=30,
validation_data=(test_dataset, test_labels),
callbacks=callbacks,
verbose=1
)

end_time = time.time()
training_time = end_time - start_time
print(f&quot;训练时间：{training_time/60:.2f} 分钟&quot;)

model.save(&#39;hand_gesture_detect.keras&#39;)

# 评估模型
loss_val, accuracy_val = model.evaluate(test_dataset, test_labels)
print(f&quot;损失：{loss_val}&quot;)
print(f&quot;准确率：{accuracy_val}&quot;)

来自 tensorflow.keras.applications 导入 ResNet50

base_model = ResNet50(weights=&#39;ImageNet&#39;, include_top=False, input_tensor=(32, 32, 3))

我已经使我的模型复杂化，但准确率仍然只有 77-80%，我不知道如何提高我的模型准确率]]></description>
      <guid>https://stackoverflow.com/questions/79020893/how-to-increase-accurracy-for-cifar10-model-above-80-accuracy</guid>
      <pubDate>Wed, 25 Sep 2024 01:55:02 GMT</pubDate>
    </item>
    <item>
      <title>如何在 GPU 上运行 gridSearchCV 或 randonizedSerchCV</title>
      <link>https://stackoverflow.com/questions/79020888/how-to-run-gridsearchcv-or-randonizedserchcv-on-gpu</link>
      <description><![CDATA[我想运行 gridSearchCV 或 randonizedSerchCV 来使用 GPU 在 Colab 环境中调整超参数。
但我找不到这些函数与 GPU 兼容的实现。
在这种情况下，我该如何调整超参数？
因此，由于我找不到在 GPU 上调整超参数的函数，我尝试实现 randonizedSerchCV。但我认为一定有一种方法可以做到这一点，而无需手动实现该函数。]]></description>
      <guid>https://stackoverflow.com/questions/79020888/how-to-run-gridsearchcv-or-randonizedserchcv-on-gpu</guid>
      <pubDate>Wed, 25 Sep 2024 01:53:20 GMT</pubDate>
    </item>
    <item>
      <title>如何将预测值合并回数据集？[关闭]</title>
      <link>https://stackoverflow.com/questions/79018990/how-to-merge-predicted-value-back-to-the-data-set</link>
      <description><![CDATA[我已经在 Python 中训练了一个 XGboost 模型，并将概率列表作为输出。我如何将这些概率带到原始数据集，以便在一个 DF 中拥有数据 + 预测值？假设我的原始原始测试 df 称为 df_raw。
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)
model = XGBClassifier(n_estimators=1500, max_depth=5, n_jobs=-1, min_child_weight=2, 
early_stopping_rounds=25)
model.fit(X_train, y_train, eval_set=[(X_test, y_test)])
test_outputs = model.predict_proba(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/79018990/how-to-merge-predicted-value-back-to-the-data-set</guid>
      <pubDate>Tue, 24 Sep 2024 14:08:01 GMT</pubDate>
    </item>
    <item>
      <title>提取哪些特征来聚类文本？</title>
      <link>https://stackoverflow.com/questions/78974474/what-features-to-extract-to-cluster-text</link>
      <description><![CDATA[我想为文本制作一个分类器，进一步用于为给定的文本推荐最相似的文本。
应用程序的流程如下：

使用 llm 从文本中提取 10 个主要主题（它可以从 150 个词池中选择）
我将词向量设为二进制向量，基本上在 150 维空间中工作，其中每个文本都有一个坐标，例如 [1, 0, 1, ..., 0]
然后我使用 cosine 距离找到最近的邻居（我想扩展到 3-5，但为了简单起见，我们假设只有一个）
我收到了最接近的文本

问题是文本非常不同，并且 llm 可以很好地提供主题，但是建议的文本并不完全符合我的预期。我尝试根据重要性对主题进行排序，并使向量非二进制（[10, 0, 0, 9, ..., 1]），但这似乎没有太大帮助。
我想知道这种方法是否不适合我的问题，或者我是否应该使用其他参数或其他任何东西来对我的文本进行分组。]]></description>
      <guid>https://stackoverflow.com/questions/78974474/what-features-to-extract-to-cluster-text</guid>
      <pubDate>Wed, 11 Sep 2024 14:56:49 GMT</pubDate>
    </item>
    <item>
      <title>如何在 AWS 免费套餐上分配 GPU？</title>
      <link>https://stackoverflow.com/questions/60668849/how-to-allocate-gpus-on-aws-free-tier</link>
      <description><![CDATA[是否可以在 AWS 免费套餐上分配 GPU？如果可以，有人可以解释一下步骤吗？我尝试在 Amazon EC2 上分配 GPU。]]></description>
      <guid>https://stackoverflow.com/questions/60668849/how-to-allocate-gpus-on-aws-free-tier</guid>
      <pubDate>Fri, 13 Mar 2020 10:33:16 GMT</pubDate>
    </item>
    <item>
      <title>如何使 FeatureUnion 返回 Dataframe</title>
      <link>https://stackoverflow.com/questions/36652196/how-to-make-featureunion-return-dataframe</link>
      <description><![CDATA[所以我目前有一个包含大量客户转换器的管道：
p = Pipeline([
(&quot;GetTimeFromDate&quot;,TimeTransformer(&quot;Date&quot;)), #添加 [&quot;time&quot;] 列的自定义转换器
(&quot;GetZipFromAddress&quot;,ZipTransformer(&quot;Address&quot;)), #添加 [&quot;zip&quot;] 列的自定义转换器
(&quot;GroupByTimeandZip&quot;,GroupByTransformer([&quot;time&quot;,&quot;zip&quot;]) #添加 onehot 列的自定义转换器
])

每个转换器都接收一个 pandas 数据框并返回包含一个或多个新列的相同数据框。它实际上运行得很好，但我如何并行运行“GetTimeFromDate”和“GetZipFromAddress”步骤？
我想使用 FeatureUnion：
f = FeatureUnion([
(&quot;GetTimeFromDate&quot;,TimeTransformer(&quot;Date&quot;)), #添加 [&quot;time&quot;] 列的自定义转换器
(&quot;GetZipFromAddress&quot;,ZipTransformer(&quot;Address&quot;)), #添加 [&quot;zip&quot;] 列的自定义转换器])
])

p = Pipeline([
(&quot;FeatureUnionStep&quot;,f),
(&quot;GroupByTimeandZip&quot;,GroupByTransformer([&quot;time&quot;,&quot;zip&quot;]) #添加 onehot 列的自定义转换器
])

但问题是 FeatureUnion 返回的是 numpy.ndarray，而“GroupByTimeandZip”步骤需要数据框。
有没有办法让 FeatureUnion 返回 pandas 数据框？]]></description>
      <guid>https://stackoverflow.com/questions/36652196/how-to-make-featureunion-return-dataframe</guid>
      <pubDate>Fri, 15 Apr 2016 16:18:14 GMT</pubDate>
    </item>
    <item>
      <title>开源神经网络库 [关闭]</title>
      <link>https://stackoverflow.com/questions/11477145/open-source-neural-network-library</link>
      <description><![CDATA[我正在寻找一个开源神经网络库。到目前为止，我已经研究过 FANN、WEKA 和 OpenNN。我还应该看看其他的吗？当然，标准是文档、示例和易用性。]]></description>
      <guid>https://stackoverflow.com/questions/11477145/open-source-neural-network-library</guid>
      <pubDate>Fri, 13 Jul 2012 19:32:11 GMT</pubDate>
    </item>
    </channel>
</rss>