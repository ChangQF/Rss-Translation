<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 06 Mar 2024 15:15:38 GMT</lastBuildDate>
    <item>
      <title>高斯过程数据误差 MATLAB</title>
      <link>https://stackoverflow.com/questions/78115412/gaussian-process-data-errors-matlab</link>
      <description><![CDATA[我想问如何在matlab的fitrgp函数中插入误差数组来进行高斯过程回归。我有一个数组 x、其他数组 y 以及与 y 关联的标准差数组 delta_y。
只有 x 和 y，我可以使用 gprMdl = fitrgp(x,y)，但是如何添加 delta_y 数组作为 y 的误差条？
谢谢！
我不知道如何插入数据集的误差条以在matlab上进行GP回归]]></description>
      <guid>https://stackoverflow.com/questions/78115412/gaussian-process-data-errors-matlab</guid>
      <pubDate>Wed, 06 Mar 2024 15:02:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 RGB 图像中应用多头注意力？</title>
      <link>https://stackoverflow.com/questions/78115215/how-to-apply-multiheadattention-in-rgb-image</link>
      <description><![CDATA[我想在卷积网络中使用多头注意力。
将(3,360,360)（RGB、高、宽）的图像作为多头注意力的输入，然后连接到Conv2d。但我有一些问题。
首先，从输入为 2D（seq、embedding 或 d_model）的情况开始，如 (6, 512)。
shape = (6,512) #（seq、嵌入或 d_model）
# 定义输入层
输入矩阵=输入（形状=形状）

# 使用 MultiHeadAttention 进行自注意力
层= MultiHeadAttention（num_heads = 4，key_dim = 2，use_bias = False）
注意输出，权重=层（输入矩阵，输入矩阵，return_attention_scores = True）

# 创建模型
模型 = tf.keras.Model(输入=input_matrix, 输出=attention_output)

weight_names = [&#39;查询&#39;, &#39;键&#39;, &#39;值&#39;, &#39;项目&#39;]
对于名称，在 zip(weight_names,layer.get_weights()) 中输出：
    print(名称, 输出形状)
打印（权重.形状）
--------------------
查询 (512, 4, 2) （为什么这里是嵌入数？, Heads, d_k）
键（512、4、2）
值 (512, 4, 2)
项目 (4, 2, 512)
（无、4、6、6）（批次、头、seq、seq）

这有助于我理解。
在此处输入图像描述
这是我的代码
将张量流导入为 tf
从tensorflow.keras.layers导入输入、Conv2D、MultiHeadAttention、Reshape
从tensorflow.keras导入后端
backend.set_image_data_format(&#39;channels_first&#39;)

形状 = (3,450,450) # (RGB, 高, 宽)
# 定义输入层
输入矩阵=输入（形状=形状）

# 使用 MultiHeadAttention 进行自注意力
层= MultiHeadAttention（num_heads = 4，key_dim = 2，use_bias = False，attention_axes =（1,2））
注意输出，权重=层（输入矩阵，输入矩阵，return_attention_scores = True）

# 使用Conv2D进行卷积运算
conv_output = Conv2D(filters=128, kernel_size=(2, 2),strides=(2,2),activation=&#39;relu&#39;)(attention_output)

# 创建模型
模型= tf.keras.Model（输入= input_matrix，输出= conv_output）

它可以运行，但我不确定它是否按照我的想法运行。因为权重（q，k，v）和attention_scores维度让我很困惑。首先，我认为 (None, 4, 3, 450, 3, 450) 应该是 (None, 4, 450, 450, 450, 450) (batch, Heads, height, width, height, width)，类似于 ( None,4,6,6)(batch,heads,seq,seq)，表示单词之间的关系。图像大小应该代表像素之间的关系。我该如何修改我的代码？其次，为什么权重(q,k,v)的维度没有改变？谢谢。
weight_names = [&#39;查询&#39;, &#39;键&#39;, &#39;值&#39;, &#39;项目&#39;]
对于名称，在 zip(weight_names,layer.get_weights()) 中输出：
    print(名称, 输出形状)
打印（权重.形状）
--------------------------------
查询 (450, 4, 2)
键（450、4、2）
值（450、4、2）
项目（4、2、450）
（无、4、3、450、3、450）
]]></description>
      <guid>https://stackoverflow.com/questions/78115215/how-to-apply-multiheadattention-in-rgb-image</guid>
      <pubDate>Wed, 06 Mar 2024 14:34:58 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降权重不断变大</title>
      <link>https://stackoverflow.com/questions/78115138/gradient-descent-weights-keep-getting-larger</link>
      <description><![CDATA[为了熟悉梯度下降算法，我尝试创建自己的线性回归模型。对于少数数据点来说它效果很好。但是当尝试使用更多数据来拟合它时，w0 和 w1 的大小总是增加。有人可以解释一下这种现象吗？
类线性回归：
    def __init__(自身, x_向量, y_向量):

        self.x_vector = np.array(x_vector, dtype=np.float64)
        self.y_向量 = np.array(y_向量, dtype=np.float64)
        自身.w0 = 0
        自身.w1 = 0

    def _get_predicted_values(self, x):
        公式 = lambda x: self.w0 + self.w1 * x
        返回公式(x)

    def_get_gradient_matrix（自身）：
        预测 = self._get_predicted_values(self.x_vector)
        w0_hat = sum((self.y_向量 - 预测))
        w1_hat = sum((self.y_向量 - 预测) * self.x_向量)

        梯度矩阵 = np.array([w0_hat, w1_hat])
        梯度矩阵 = -2 * 梯度矩阵

        返回梯度矩阵

    def fit(自我，step_size=0.001，num_iterations=500)：
        for _ in range(1, num_iterations):
            梯度矩阵 = self._get_gradient_matrix()
            self.w0 -= 步长大小 * (梯度矩阵[0])
            self.w1 -= 步长大小 * (梯度矩阵[1])

    def _show_coeffiecients（自身）：
        print(f&quot;w0: {self.w0}\tw1: {self.w1}\t&quot;)

    def 预测（自身，x）：
        y = 自身.w0 + 自身.w1 * x
        返回y

# 这工作正常
x = [x 表示 x 在范围 (-3, 3) 内]
f = 拉姆达 x: 5 * x - 7
y = [f(x_val) for x_val in x]

模型 = 线性回归(x, y)
模型.fit(num_iterations=3000)

model.show_coeffiecients() #输出：w0：-6.99999999999994 w1：5.00000000000002

#虽然这不是
x = [x for x in range(-50, 50)] # 增加 x 值的数量
f = 拉姆达 x: 5 * x - 7
y = [f(x_val) for x_val in x]

模型 = 线性回归(x, y)
模型.fit(num_iterations=3000)

model.show_coefficients()
#这会产生一个警告
”“”
运行时警告：乘法中遇到溢出
w1_hat = sum((self.y_向量 - 预测) * self.x_向量)
公式 = lambda x: self.w0 + self.w1 * x
”“”
]]></description>
      <guid>https://stackoverflow.com/questions/78115138/gradient-descent-weights-keep-getting-larger</guid>
      <pubDate>Wed, 06 Mar 2024 14:22:16 GMT</pubDate>
    </item>
    <item>
      <title>如何将一系列分类特征传递给 CatBoost</title>
      <link>https://stackoverflow.com/questions/78114854/how-to-pass-an-array-of-categorical-features-to-catboost</link>
      <description><![CDATA[我有一个整数数组作为功能。
它代表项目的标签（因此内部必须被视为分类特征）。
示例：[11,2344,17]
如何将它们传递给 Catboost 并告诉它像处理一系列分类特征一样使用它？]]></description>
      <guid>https://stackoverflow.com/questions/78114854/how-to-pass-an-array-of-categorical-features-to-catboost</guid>
      <pubDate>Wed, 06 Mar 2024 13:40:10 GMT</pubDate>
    </item>
    <item>
      <title>在机器学习模型中按时间对训练实例进行加权[关闭]</title>
      <link>https://stackoverflow.com/questions/78114524/weighting-training-instances-by-time-in-machine-learning-models</link>
      <description><![CDATA[我正在根据数据训练一个神经网络，我认为这些数据的相关性会根据每个实例过去的时间而减弱。我已经看过了，一种方法似乎是根据新近度对训练实例进行“加权”，使用某种指数衰减函数。
我的问题是如何解释这样一个事实：一旦模型在这样的加权数据集上进行训练：

测试实例与训练数据集的距离（在时间上）并不完全相等。举例来说，我使用 2012-2017 年的时间加权数据进行训练，并使用 2018-2019 年作为测试，2018 年的实例将比 2019 年的实例“更接近”训练集……不会有固有的“衰减”吗当我们的预测与训练数据的距离越来越远时，模型的预测能力会如何？

与此相关，更新训练模型时的做法是什么？考虑一下我的示例，是否有一个程序可以决定让使用 2012-2017 年数据训练的模型进行预测，然后再决定重新训练或更新？


我尝试在网上搜索，有些人在这个论坛上提出了相关问题，但我认为没有找到解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78114524/weighting-training-instances-by-time-in-machine-learning-models</guid>
      <pubDate>Wed, 06 Mar 2024 12:50:33 GMT</pubDate>
    </item>
    <item>
      <title>如何使用自定义选择器正确堆叠 sklearn 管道</title>
      <link>https://stackoverflow.com/questions/78114397/how-to-correctly-stack-sklearn-pipeline-with-custom-selector</link>
      <description><![CDATA[我想使用 sklearn 中的 StackingClassifier 类来创建一个集成学习器。
具体来说，我有两种不同类型的数据：二维图像和经典表格数据。我想对 2D 图像使用 MLP 分类器，对表格数据使用 RandomForest 分类器。然后我想使用随机森林分类器将上述两个模型的预测结合到我的最终预测中。
由于无法为 StackingClassifier 实例提供单独的训练数据，我采用了以下技巧：我使用自定义选择器创建了两个 Pipeline从字典返回一个值。虽然我可以单独拟合和预测管道，但当我尝试拟合和预测堆叠模型时，出现了 ValueError: Found input Variables with不一致数量的样本。
下面是我使用的代码：
将 numpy 导入为 np
从 sklearn.model_selection 导入 train_test_split
从 sklearn.neural_network 导入 MLPClassifier
从 sklearn.base 导入 BaseEstimator、TransformerMixin
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.ensemble 导入 StackingClassifier
从 sklearn.datasets 导入 load_iris
从 sklearn.metrics 导入 precision_score
从 sklearn.pipeline 导入管道

#定制选择器
类 DictionarySelector(BaseEstimator, TransformerMixin):
    def __init__(self, in_dict, key):

        self.in_dict = in_dict
        self.key = 密钥
        
    def fit(self, x, y = None):
        返回（自己）
    
    def 变换（自身，键）：
        return(self.in_dict[self.key])
    
    
        

# 生成一些示例数据
# 出于演示目的，我使用 Iris 数据集
数据 = load_iris()
X_images = data.data[:, :2] # 假设这些是我的图像
X_vector = data.data[:, 2:] # 一维向量
y = data.target # 目标类（3类）

# 将数据分为训练集和测试集
X_images_train，X_images_test，X_vector_train，X_vector_test，y_train，y_test = train_test_split（
    X_图像、X_向量、y、test_size=0.2、random_state=42
）

# 定义图像的 MLP
image_model = MLPClassifier(hidden_​​layer_sizes=(64, 32), 激活=“relu”, max_iter=1000)

# 定义向量的随机森林
矢量模型 = RandomForestClassifier(n_estimators=100, criteria=“基尼”, max_depth=None)
#用两种数据类型创建字典
in_dict = {&#39;图像&#39;：X_images_train，&#39;矢量&#39;：X_vector_train}
#MLP 图像管道
管道图像=管道（[
    (&#39;选择&#39;, DictionarySelector(in_dict, &#39;图像&#39;)),
    （&#39;clf&#39;，图像模型）
]）

#检查：它有效
打印（pipe_images.fit（X_images_train，y_train）.预测（X_images_test））

#特征管道
管道向量=管道（[
    (&#39;选择&#39;, DictionarySelector(in_dict, &#39;向量&#39;)),
    （&#39;clf&#39;，向量模型）
]）
#检查：它有效
打印（pipe_images.fit（X_vector_train，y_train）.predict（X_vector_test））

# 创建一个堆叠模型
stacked_model = StackingClassifier(
    估计量=[
        （“image_mlp”，pipe_images），
        (“向量_rf”，管道_向量)，
    ],
    Final_estimator=RandomForestClassifier(n_estimators=100),
）

#堆叠模型抛出ValueError
stacked_model.fit(in_dict, y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/78114397/how-to-correctly-stack-sklearn-pipeline-with-custom-selector</guid>
      <pubDate>Wed, 06 Mar 2024 12:29:37 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中生成 CSV 和 Excel 数据库的列名称？</title>
      <link>https://stackoverflow.com/questions/78114334/how-does-one-generate-column-names-for-csvs-and-excel-databases-in-python</link>
      <description><![CDATA[有一堆没有列名称的逗号分隔值 (CSV) 和 Excel 数据库，有没有办法使用列值通过 Python 自动生成名称？
示例：
输入：
迈克尔，18 岁
安娜，19 岁
彼得，20 岁
输出：
姓名年龄
迈克尔，18 岁
安娜，19 岁
彼得，20 岁
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78114334/how-does-one-generate-column-names-for-csvs-and-excel-databases-in-python</guid>
      <pubDate>Wed, 06 Mar 2024 12:20:25 GMT</pubDate>
    </item>
    <item>
      <title>为每月数据编写时间滞后，以合并训练测试分割模型的情绪和市场分析</title>
      <link>https://stackoverflow.com/questions/78113308/coding-a-time-lag-for-monthly-data-to-merge-a-sentiment-and-market-analysis-for</link>
      <description><![CDATA[我们的教授为我们提供了以下代码：
代码1：
sentiment=pd.read_csv(&#39;/content/drive/MyDrive/Colab Notebooks/Python Code/Sentiment_quarterly.csv&#39;).drop(&#39;未命名：0&#39;,axis=1)
情绪
从日期时间导入日期时间
数据[&#39;数据&#39;]=0

代码2：
for k in tqdm(range(len(data[&#39;Datum&#39;])))：
    test_date=str(int(data.loc[k,&#39;bewertungsjahr&#39;]))+&#39;.&#39;+str(int(data.loc[k,&#39;bewertungsquartal&#39;]*3))
    data.loc[k,&#39;Datum&#39;]=str(pd.Timestamp(datetime.strptime(test_date, &#39;%Y.%m&#39;).date()).to_period(&#39;Q&#39;))

数据[&#39;数据&#39;][0]

代码3：
for k in tqdm(range(len(sentiment[&#39;Time&#39;]))):
    test_date=str(int(sentiment.loc[k,&#39;时间&#39;][0:4]))+&#39;.&#39;+str(int(sentiment.loc[k,&#39;时间&#39;][5])*3)
    情感.loc[k,&#39;Time&#39;]=pd.Timestamp(datetime.strptime(test_date, &#39;%Y.%m&#39;).date()).to_period(&#39;Q&#39;)

情绪[&#39;时间&#39;][0]

我们不想使用季度情绪.csv，而是使用每月.csv
季度数据中的时间格式是例如2014Q1 和月度数据显示 2014-01、2014-02...
如何调整代码以使其正常工作？
我尝试添加此 &#39;.&#39;+str(int(data.loc[k,&#39;bewertungsquartal&#39;]*12)) 这样也许我也考虑了月份。]]></description>
      <guid>https://stackoverflow.com/questions/78113308/coding-a-time-lag-for-monthly-data-to-merge-a-sentiment-and-market-analysis-for</guid>
      <pubDate>Wed, 06 Mar 2024 09:47:41 GMT</pubDate>
    </item>
    <item>
      <title>获取“ValidationError：VectorstoreIndexCreator 嵌入的 1 个验证错误”</title>
      <link>https://stackoverflow.com/questions/78112934/getting-validationerror-1-validation-error-for-vectorstoreindexcreator-embeddi</link>
      <description><![CDATA[我正在尝试构建一个 pdf 聊天机器人，您可以在其中上传 pdf 并询问与 pdf 相关的问题。为此，我正在考虑基于 RAG 的应用程序。所以我想为我的输入 pdf 创建矢量嵌入，但是当我这样做时，
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embed_model = HuggingFaceEmbedding(model_name=“BAAI/bge-small-en-v1.5”)
index_creator = VectorstoreIndexCreator(
    矢量store_cls = 卡桑德拉，
    嵌入=嵌入模型，
    text_splitter = RecursiveCharacterTextSplitter(
        块大小 = 400,
        块重叠 = 30
    ),

    矢量store_kwargs={
        &#39;会话&#39;：会话，
        &#39;键空间&#39;：键空间，
        &#39;表名&#39;：表名
    }
）

我收到验证错误。
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ValidationError Traceback（最近一次调用最后一次）
&lt;ipython-input-17-b83dc7fd1587&gt;在&lt;细胞系：4&gt;()
      2 keyspace = “pdf_qa_name”
      3
----&gt; 4 index_creator = VectorstoreIndexCreator(
      5 矢量store_cls = 卡桑德拉，
      6 嵌入 = embed_model,

/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py 在 __init__(__pydantic_self__, **data)
    第 339 部分
    第340章
--&gt;第341章
    第342章
    第343章

ValidationError：VectorstoreIndexCreator 出现 1 个验证错误
嵌入
  预期 Embeddings 的实例（type=type_error.任意_type；expected_centric_type=Embeddings）

有什么想法吗？
尝试了 2 个不同的模型（Jina 和 BAAI/bge）。错误不会继续。我正在使用 open ai gpt 3.5 api。]]></description>
      <guid>https://stackoverflow.com/questions/78112934/getting-validationerror-1-validation-error-for-vectorstoreindexcreator-embeddi</guid>
      <pubDate>Wed, 06 Mar 2024 08:47:33 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降最小二乘代码问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78112607/problem-with-gradient-descent-least-squares-code</link>
      <description><![CDATA[我正在尝试在数据集上使用梯度下降。我写的是
&lt;前&gt;&lt;代码&gt;导入numpy
将 pandas 导入为 pd
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

数据 = pd.read_csv(&#39;C:/Users/Teacher/Downloads/data.csv&#39;)
X = data.iloc[:, 0] # 选择 data 中第一列的所有数据
Y = data.iloc[:, 1]
plt.scatter(X,Y)
plt.show()
n = 长度 (X)

a = 0
b = 0
L = .001

对于范围（1000）内的 i：
    y_预测 = a * X + b
    pd_a = (1 / n) * sum((y_预测 - Y) * X)
    pd_b = (1 / n) * sum(y_预测 - Y)
    a = a - L * pd_a
    b = b - L * pd_b
打印（a，b）
plt.scatter(X, Y)
c, d = numpy.polyfit(X, Y, 1)
打印（c，d）
plt.plot([min(X), max(X)], [a * x + b for x in [min(X), max(X)]], [c * x + d for x in [min( X), 最大值(X)]])
plt.show()

如果我定义 X 和 Y = np.random.rand(20)，那么一切似乎都工作正常，所以我相信问题就在那里。然而，X 和 Y 的散点图仍然很好，即使我将它们定义为数据集的第一列和第二列，所以我不确定发生了什么。
编辑：这是定义 X = data.iloc[:, 0] 后的散点图图像
Y = data.iloc[:, 1]

这是代码末尾的绘图和线条的图像。
]]></description>
      <guid>https://stackoverflow.com/questions/78112607/problem-with-gradient-descent-least-squares-code</guid>
      <pubDate>Wed, 06 Mar 2024 07:48:04 GMT</pubDate>
    </item>
    <item>
      <title>在 randomForestClassifier 上使用 GridsearchCV 时遇到的问题</title>
      <link>https://stackoverflow.com/questions/78112023/problem-faced-while-using-gridsearchcv-on-randomforestclassifier</link>
      <description><![CDATA[我正在使用 RandomForestClassifier 处理与心脏病相关的分类问题。在对 RandomForestClassifier 执行超参数调整时，我面临以下问题。我使用 sklearn Pipeline 和 ColumnTransformer 进行预处理。
错误：总共 2160 次拟合中有 720 次失败。
这些参数的训练测试分区的分数将设置为 nan。
如果这些失败不是预期的，您可以尝试通过设置 error_score=&#39;raise&#39; 来调试它们。
用户警告：一项或多项测试分数是非有限的

numerical_pipeline = 管道(
步骤=[(&#39;缩放器&#39;,StandardScaler())]
）

分类管道=管道（
步骤=[(&#39;编码器&#39;,OneHotEncoder(handle_unknown=&#39;忽略&#39;))]
）

预处理器 = ColumnTransformer(
[(&#39;numerical_pipeline&#39;,numerical_pipeline,numerical_features),
 (&#39;categorical_pipeline&#39;,categorical_pipeline,categorical_features)]`

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)`

scaled_X_train = 预处理器.fit_transform(X_train)
scaled_X_test = 预处理器.transform(X_test)`

param_grid={&#39;max_depth&#39;:[3,5,10,无],
          &#39;n_estimators&#39;:[10,100,200],
          &#39;最大特征&#39;:[1,3,5,7],
          &#39;min_samples_leaf&#39;:[1,2,3],
          &#39;min_samples_split&#39;:[1,2,3]
       }

grid = GridSearchCV(RandomForestClassifier(),param_grid=param_grid,cv=5,scoring=&#39;准确性&#39;,verbose=True,n_jobs=-1)
grid.fit(scaled_X_train,y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/78112023/problem-faced-while-using-gridsearchcv-on-randomforestclassifier</guid>
      <pubDate>Wed, 06 Mar 2024 05:36:25 GMT</pubDate>
    </item>
    <item>
      <title>如何消除在张量流的 Tape.gradient 方法中将虚数转换为实值的警告？</title>
      <link>https://stackoverflow.com/questions/77185089/how-to-remove-this-warning-of-casting-imaginary-into-real-values-within-tape-gra</link>
      <description><![CDATA[我正在使用tape.gradient方法来优化一些神经网络。它按预期工作，但当我在单次迭代中多次使用 Tape.gradients 计算梯度时，不断发出此警告。这意味着在单个循环内，在执行 back prop 时，它会在某个地方摆弄复数。
警告：tensorflow：您正在将complex64类型的输入转换为不兼容的dtype float64。这将丢弃虚部，并且可能不是您想要的。

cost_progress=[]
跟踪进度=[]
对于我在范围内（次数）：

  使用 tf.GradientTape() 作为磁带：
    磁带.watch(参数)
    损失，跟踪 = 成本（参数，比率）
    trace_progress.append(trace)
    cost_progress.append(损失)

  梯度 = Tape.gradient(loss, params)
  opt.apply_gradients(zip([渐变], [参数]))

现在，所有参数和损失都是 tf.float64，但仍在 Tape.gradient() 中给出了一些复杂类型，我想手动将它们转换为真实值，以便此警告停止显示在我的屏幕上。但我无法找到如何投射以免弄乱。
强制gradients = tf.cast(tape.gradient(loss, params),tf.float64)不起作用。我已验证 gradients = Tape.gradient(loss, params) 发出警告，并且 loss 和 params 均为 tf.float64 类型。]]></description>
      <guid>https://stackoverflow.com/questions/77185089/how-to-remove-this-warning-of-casting-imaginary-into-real-values-within-tape-gra</guid>
      <pubDate>Wed, 27 Sep 2023 06:26:47 GMT</pubDate>
    </item>
    <item>
      <title>更改 onnx 模型中的输入和输出形状</title>
      <link>https://stackoverflow.com/questions/75867115/change-input-and-output-shapes-in-onnx-model</link>
      <description><![CDATA[我对机器学习很陌生，所以我提前为这个问题表示歉意。
我有一个预先训练的 onnx 模型，具有定义的输入和输出形状。
是否可以更改这些值？
我研究了可能的解决方案，尝试使用例如onnxruntime.tools.make_dynamic_shape_fixed，但由于模型已经具有固定的形状，因此失败。
我还在考虑将模型转换为张量流格式，并尝试从那里修改形状，但我没有取得多大进展。
任何帮助将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/75867115/change-input-and-output-shapes-in-onnx-model</guid>
      <pubDate>Tue, 28 Mar 2023 13:46:08 GMT</pubDate>
    </item>
    <item>
      <title>Handpose tfjs 错误 - 在注册表中找不到后端</title>
      <link>https://stackoverflow.com/questions/62134812/handpose-tfjs-error-no-backend-found-in-registry</link>
      <description><![CDATA[尝试运行 Handpose tfjs 演示项目时，出现以下错误。

我的 package.json 文件具有以下依赖项：

&lt;前&gt;&lt;代码&gt;{
“名称”：“tensorflowJs”，
“版本”：“1.0.0”，
“描述”： ””，
“主要”：“index.js”，
“脚本”：{
  &quot;watch&quot;: &quot;跨环境 NODE_ENV=开发包index.html --no-hmr &quot;,
  &quot;build&quot;: &quot;cross-env NODE_ENV=生产包构建index.html --public-url ./&quot;
 },
“浏览器”：{
“加密”：假
 },
“关键字”：[]，
“作者”： ””，
“许可证”：“ISC”，
“依赖项”：{
  “@tensorflow-models/handpose”：“0.0.4”，
  &quot;@tensorflow/tfjs-backend-wasm&quot;: &quot;^2.0.0&quot;,
  &quot;@tensorflow/tfjs-converter&quot;: &quot;^1.7.4&quot;,
  &quot;@tensorflow/tfjs-core&quot;: &quot;^2.0.0&quot;,
  &quot;@tensorflow/tfjs-node&quot;: &quot;^2.0.0&quot;,
  &quot;引导程序&quot;: &quot;^4.5.0&quot;,
  “跨环境”：“^7.0.2”
 },
“开发依赖项”：{
  &quot;@babel/cli&quot;: &quot;^7.10.1&quot;,
  &quot;@babel/core&quot;: &quot;^7.10.2&quot;,
  &quot;@babel/plugin-transform-runtime&quot;: &quot;^7.10.1&quot;,
  &quot;@babel/polyfill&quot;: &quot;^7.10.1&quot;,
  &quot;@babel/preset-env&quot;: &quot;^7.10.2&quot;,
  &quot;babel-preset-env&quot;: &quot;^1.7.0&quot;,
  “包裹捆绑器”：“^1.12.4”
 }
}

注册表问题应该在版本 0.10.3 之后得到解决，但即使对于版本 2，我仍然面临这个问题。有谁知道为什么会出现这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/62134812/handpose-tfjs-error-no-backend-found-in-registry</guid>
      <pubDate>Mon, 01 Jun 2020 14:46:38 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch DataLoader内存未释放</title>
      <link>https://stackoverflow.com/questions/52015010/pytorch-dataloader-memory-is-not-released</link>
      <description><![CDATA[我想在google collaboratory上的pythorch上实现SRGAN，但是DataLoader的内存似乎被释放了，所以如果你转动epoch，就会出现内存错误。
如果您告诉我如何执行此操作以释放每批内存，我将不胜感激。
这是代码的 GitHub 链接
已经 48 了，1 个 echoch 发生内存错误。如果将批量大小设置为 8 的 1/6，则大约在第 6 个 epoch 时会出现错误。
我正在使用以下代码读取高分辨率和低分辨率图像。扩展图像文件夹。但是例如，即使执行学习时出现错误，GPU的内存也不会被释放
类 DownSizePairImageFolder(ImageFolder):
    def __init__(自身，根，变换=无，large_size=256，small_size=64，**kwds)：
        super().__init__(根，变换=变换，**kwds)
        self.large_resizer = 变换.Scale(large_size)
        self.small_resizer = 变换.Scale(small_size)
    
    def __getitem__(自身，索引)：
        路径, _ = self.imgs[索引]
        img = self.loader(路径)
        Large_img = self.large_resizer(img)
        Small_img = self.small_resizer(img)
        如果 self.transform 不是 None：
            Large_img = self.transform(large_img)
            小img = self.transform(small_img)
        返回小img、大img


train_data = DownSizePairImageFolder(&#39;./lfw-deepfunneled/train&#39;,transform=transforms.ToTensor())
test_data = DownSizePairImageFolder(&#39;./lfw-deepfunneled/test&#39;,transform=transforms.ToTensor())
批量大小 = 8
train_loader = DataLoader(train_data,batch_size,shuffle=True)
test_loader = DataLoader(test_data,batch_size,shuffle=False)
]]></description>
      <guid>https://stackoverflow.com/questions/52015010/pytorch-dataloader-memory-is-not-released</guid>
      <pubDate>Sat, 25 Aug 2018 07:25:26 GMT</pubDate>
    </item>
    </channel>
</rss>