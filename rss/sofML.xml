<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 16 Dec 2024 12:36:45 GMT</lastBuildDate>
    <item>
      <title>具有小区域估计的机器学习</title>
      <link>https://stackoverflow.com/questions/79284337/machine-learning-with-small-area-estimation</link>
      <description><![CDATA[
我能否使用调查数据结果和辅助变量（不包括调查协变量）、训练集和测试集构建合适的机器学习模型？
个人和家庭层面的样本总数为 5000 个，聚合数据有 300 个聚类区域。是否可以将随机森林用于此类数据结构？
在聚类级别聚合这些数据后，我只得到了 300 个样本和区域。我的分析重点是区域级别。训练集和随机森林的测试集使用哪个样本量？

我尝试使用调查样本数据来构建模型，但我遇到了一些挑战，对这个过程感到困惑。具体来说，我不确定我是否应用了适当的方法。我需要更多关于如何使用这些数据有效地构建模型的说明或指导。]]></description>
      <guid>https://stackoverflow.com/questions/79284337/machine-learning-with-small-area-estimation</guid>
      <pubDate>Mon, 16 Dec 2024 10:25:16 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 aiplatform.BatchPredictionJob.create() 在 Vertex AI 中配置模型监控？</title>
      <link>https://stackoverflow.com/questions/79283938/how-to-configure-model-monitoring-in-vertex-ai-using-aiplatform-batchpredictionj</link>
      <description><![CDATA[我在使用 aiplatform SDK 设置 Vertex AI 模型监控时遇到了问题，特别是在配置 BatchPredictionJob.create() 时。文档不清楚，缺少定义 model_monitoring_objective_config 和 model_monitoring_alert_config 的示例。由于引用不完整和参数映射不清楚，这导致了兼容性问题。
主要混淆是因为这些配置在 SDK 中没有很好的记录，需要探索 SDK 源代码才能了解正确的结构。此外，SDK 需要来自 aiplatform.model_monitoring 的配置，这在官方指南中没有明确说明。
我尝试过的方法：
我最初参考了 SDK 文档，并根据看似合乎逻辑的内容尝试了各种配置，假设所有组件都可以直接使用 SDK 的类进行配置。但是，这会导致多个类型和参数不匹配错误。
我的预期：
我预期清晰明了的文档，展示如何在使用 aiplatform.BatchPredictionJob.create() 创建批量预测作业时定义和传递监控配置。
实际发生的情况：
我遇到了由于 SDK 和 GAPIC API 之间的参数不匹配而导致的错误。在探索源代码后，我意识到必须使用 aiplatform.model_monitoring 类定义所需的配置。这种跨库依赖关系没有记录。]]></description>
      <guid>https://stackoverflow.com/questions/79283938/how-to-configure-model-monitoring-in-vertex-ai-using-aiplatform-batchpredictionj</guid>
      <pubDate>Mon, 16 Dec 2024 07:45:27 GMT</pubDate>
    </item>
    <item>
      <title>利用贝叶斯优化进行多类分类</title>
      <link>https://stackoverflow.com/questions/79283333/multiclass-classification-with-bayesian-optimisation</link>
      <description><![CDATA[无法让这部分代码运行，而且速度真的很慢。
我只想创建一个模型，将叶子图像分为 4 种类型（无，然后是 3 种疾病类型）
我想使用 F1 作为损失函数，然后使用贝叶斯优化来获取叶子类模型的最佳参数，但它没有运行。或者它运行得非常慢然后失败了...
我在 CPU 上运行，因为我没有 GPU
# Optuna 的目标函数
def objective(trial, train_dataloader, val_dataloader, device):
# 使用 Optuna 的建议函数定义超参数搜索空间
conv1_filters = trial.suggest_categorical(&quot;conv1_filters&quot;, [16, 32, 64])
conv2_filters = trial.suggest_categorical(&quot;conv2_filters&quot;, [64, 128, 256])
kernel_size = trial.suggest_categorical(&quot;kernel_size&quot;, [3, 5, 7])
hidden_​​units = trial.suggest_categorical(&quot;hidden_​​units&quot;, [256, 512, 1024])
dropout_rate = trial.suggest_float(&quot;dropout_rate&quot;, 0.1, 0.5)
learning_rate = trial.suggest_float(&quot;learning_rate&quot;, 1e-5, 1e-2, log=True)
num_epochs = trial.suggest_int(&quot;num_epochs&quot;, 10, 50)

# 使用给定的参数初始化模型
model = LeafCNN(
conv1_filters=conv1_filters,
conv2_filters=conv2_filters,
kernel_size=kernel_size,
hidden_​​units=hidden_​​units,
dropout_rate=dropout_rate
).to(device)

# 设置优化器和损失函数
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)

# 提前停止参数
patient = 10 # 允许 10 个 epoch 不进行改进
delta = 0.001 # 算作改进的最小变化
best_val_loss = float(&quot;inf&quot;)
patience_counter = 0 # 从 0 开始计数器

# 训练循环
for epoch in range(num_epochs):
model.train()
for X_batch, y_batch in train_dataloader:
X_batch, y_batch = X_batch.to(device), y_batch.to(device)
optimizer.zero_grad()
outputs = model(X_batch)
loss = loss_fn(outputs, y_batch)
loss.backward()
optimizer.step()

# 验证损失
model.eval()
val_loss = 0.0
使用 torch.no_grad():
对于 val_dataloader 中的 X_batch、y_batch：
X_batch、y_batch = X_batch.to(device)、y_batch.to(device)
输出 = 模型 (X_batch)
损失 = loss_fn(outputs、y_batch)
val_loss += loss.item()

val_loss /= len(val_dataloader)
打印 (f&quot;Epoch {epoch + 1}/{num_epochs}, Val Loss: {val_loss}&quot;)

如果 val_loss &lt; best_val_loss - delta:
best_val_loss = val_loss
waiting_counter = 0
else:
waiting_counter += 1

if waiting_counter &gt;= waiting:
print(f&quot;在 epoch {epoch + 1} 触发提前停止&quot;)
break

# 使用 F1 分数进行评估
model.eval()
all_preds, all_labels = [], []
with torch.no_grad():
for X_batch, y_batch in val_dataloader:
X_batch, y_batch = X_batch.to(device), y_batch.to(device)
output = model(X_batch)
_, predict = torch.max(outputs, 1)
all_preds.extend(predicted.cpu().numpy())
all_labels.extend(y_batch.cpu().numpy())

f1 = f1_score(all_labels, all_preds, average=&quot;weighted&quot;)
return f1 # 我们的目标是最大化 F1 分数

# 运行 Optuna 优化的主要函数
def optuna_search(train_dataloader, val_dataloader, device, num_trials):
# 创建 Optuna 研究
study = optuna.create_study(direction=&quot;maximize&quot;) # 最大化 F1 分数
study.optimize(lambda trial: objective(trial, train_dataloader, val_dataloader, device), n_trials=num_trials)

# 打印最佳超参数
print(&quot;找到最佳超参数：&quot;, study.best_params)
print(&quot;最佳 F1 分数：&quot;, study.best_value)

return study.best_params, study.best_value

# 主程序
if __name__ == &quot;__main__&quot;:

# 创建 DataLoaders
train_dataset = TensorDataset(X_train_split, y_train_split)
train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)

val_dataset = TensorDataset(X_val, y_val)
val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# 运行 Optuna 优化
best_params, best_f1 = optuna_search(train_dataloader, val_dataloader, device, num_trials=10)
print(&quot;最佳参数：&quot;, best_params)
print(&quot;最佳 F1 分数：&quot;, best_f1)
]]></description>
      <guid>https://stackoverflow.com/questions/79283333/multiclass-classification-with-bayesian-optimisation</guid>
      <pubDate>Sun, 15 Dec 2024 23:07:57 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 停留在图像生成上</title>
      <link>https://stackoverflow.com/questions/79283140/lstm-stuck-on-image-generation</link>
      <description><![CDATA[我创建了一个 LSTM 来生成序列中的下一张图像（我知道 CNN 是用于图像生成的，但我需要整个图像，而不仅仅是提供给序列下一次迭代的过滤器）。所以我有一个数据集，它包含图像（电影中的帧），我创建了它的序列，就像 1 个场景包含例如。 n 个图像，我有 s 个序列长度，那么输入将是 image_1 到 image_s，输出是 image_s+1，下一个输入是 image_2 到 image_s+1，输出是 image_s+2，依此类推。
模型如下：
class LSTM(nn.Module):
def __init__(self, input_len, hidden_​​size, num_layers):
super(LSTM, self).__init__()
self.hidden_​​size = hidden_​​size
self.num_layers = num_layers
self.lstm = nn.LSTM(input_len, hidden_​​size, num_layers, batch_first=True)
self.output_layer = nn.Linear(hidden_​​size, input_len)
self.dropout = nn.Dropout(.2)

def forward(self, X):
hidden_​​states = torch.zeros(self.num_layers, X.size(0), self.hidden_​​size, device=device)
cell_states = torch.zeros(self.num_layers, X.size(0), self.hidden_​​size, device=device)
out, _ = self.lstm(X, (hidden_​​states, cell_states))
out = self.dropout(out)
out = self.output_layer(out[:, -1, :])
return out

训练是：
def train(num_epochs, model, loss_func, optimizer):
total_steps = loader.getSizeWithBatch()

for epoch in range(num_epochs):
loader.reset()
for item in range(total_steps-1):
element = loader.next()[0]
x_images,y_image = element
x_images = x_images.reshape(-1,sequence_len,input_len)
output = model(x_images)
y_image = y_image.reshape(-1,input_len)
loss = loss_func(output, y_image)

optimizer.zero_grad()
loss.backward()
optimizer.step()

if (item + 1) % 1 == 0:
print(f&#39;Epoch: {epoch + 1};批次：{item + 1} / {total_steps};损失：{loss.item():&gt;4f}&#39;)

if (epoch + 1) % int(config[&#39;SAVE&#39;][&#39;model_save_interval&#39;]) == 0:
if (epoch + 1) % int(config[&#39;SAVE&#39;][&#39;clean_save_interval&#39;]) == 0:
torch.save(model.state_dict(), os.path.join(config[&#39;PATH&#39;][&#39;model_path&#39;], config[&#39;PATH&#39;][&#39;model_name&#39;] + str(epoch+1)))
else:
torch.save(model.state_dict(), os.path.join(config[&#39;PATH&#39;][&#39;model_path&#39;], config[&#39;PATH&#39;][&#39;model_name&#39;]))

Loader 以张量的形式引导图像由于内存使用，从文件中预先排序。
我使用 MSE 损失和 Adam 作为优化器。
问题是，当我训练它时，错误达到 0.003，这是目标，因为我通过将它们除以 255 来规范化值，但当我预测它时，它会产生一种模糊的场景图像，并且无论输入如何，预测图像始终相同，即使输入来自其他场景，它也会创建相同的图像，当我减去不同输出图像的颜色值时，该值为 0，因此每个输出图像都完全相同
我尝试添加 Droput，增加隐藏大小的神经元（现在是 128），尝试增加层数，不同的时期会创建相同的图像，只是模糊程度略低，但是一样的，我将学习率从 .001 降低到 .0001，一切都一样]]></description>
      <guid>https://stackoverflow.com/questions/79283140/lstm-stuck-on-image-generation</guid>
      <pubDate>Sun, 15 Dec 2024 20:28:51 GMT</pubDate>
    </item>
    <item>
      <title>针对 Lllama 3.2 1B 的分层学生-教师优化</title>
      <link>https://stackoverflow.com/questions/79283119/layer-wise-student-teacher-optimization-for-lllama-3-2-1b</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79283119/layer-wise-student-teacher-optimization-for-lllama-3-2-1b</guid>
      <pubDate>Sun, 15 Dec 2024 20:21:32 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我扫描模型参数时，我的 GPU 内存不断增加？</title>
      <link>https://stackoverflow.com/questions/79283083/why-does-my-gpu-memory-keep-increasing-when-i-sweep-over-model-parameters</link>
      <description><![CDATA[我正在尝试评估特定架构下具有不同丢弃率的模型分类错误率。当我这样做时，内存使用量会增加，而且我无法阻止这种情况发生（有关详细信息，请参阅下面的代码）：
N=2048 split 0 内存使用量
{&#39;current&#39;: 170630912, &#39;peak&#39;: 315827456}
{&#39;current&#39;: 345847552, &#39;peak&#39;: 430210560}
{&#39;current&#39;: 530811136, &#39;peak&#39;: 610477568}
...
{&#39;current&#39;: 1795582208, &#39;peak&#39;: 1873805056}
N=2048 split 1 内存使用量
{&#39;current&#39;: 1978317568, &#39;peak&#39;: 2056609280}
{&#39;current&#39;: 2157136640，&#39;峰值&#39;：2235356160}
...
2024-12-15 18:55:04.141690：W external/local_xla/xla/tsl/framework/bfc_allocator.cc:497] 分配器 (GPU_0_bfc) 在尝试分配 op 请求的 52.00MiB（四舍五入为 54531328）时内存不足
...
2024-12-15 18:55:04.144298：I tensorflow/core/framework/local_rendezvous.cc:405] 本地会合正在中止，状态为：RESOURCE_EXHAUSTED：尝试分配 54531208 字节时内存不足。
...

这是我正在运行的代码的相关部分，包括每次迭代后清除内存的一些不成功的尝试。
import tensorflow as tf
import tensorflow_datasets as tfds
import gc

batch_size = 128
sizes = [2048 + n * batch_size * 5 for n in range(10)]
dropout_points = 10

vals_ds = tfds.load(
&#39;mnist&#39;,
split=[f&#39;train[{k}%:{k+10}%]&#39; for k in range(0, 100, 10)],
as_supervised=True,
)
trains_ds = tfds.load(
&#39;mnist&#39;,
split=[f&#39;train[:{k}%]+train[{k+10}%:]&#39; for k in range(0, 100, 10)],
as_supervised=True,
)
_, ds_info = tfds.load(&#39;mnist&#39;, with_info=True)

def normalize_img(image, label):
return tf.cast(image, tf.float32) / 255., label

for N in sizes:
for i, (ds_train, ds_test) in enumerate(zip(trains_ds, vals_ds)):
ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.shuffle(ds_info.splits[&#39;train&#39;].num_examples)
ds_train = ds_train.batch(128)

ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(128)

print(f&quot;N={N} split {i} 内存使用情况&quot;)
with open(f&quot;out_{N}_{i}.csv&quot;, &quot;w&quot;) as f:
f.write((&quot;retention_rate,&quot;
&quot;train_loss,&quot;
&quot;train_err,&quot;
&quot;test_loss,&quot;
&quot;test_err,&quot;
&quot;epochs\n&quot;))
for p in range(dropout_points):
dropout_rate = p / dropout_points

layers = [tf.keras.layers.Flatten(input_shape=(28, 28))]
for i in range(4):
layers.append(tf.keras.layers.Dense(N,activation=&#39;relu&#39;))
layers.append(tf.keras.layers.Dropout(dropout_rate))
layers.append(tf.keras.layers.Dense(10))

with tf.device(&#39;/GPU:0&#39;):
model = tf.keras.models.Sequential(layers)
model.compile(
optimizer=tf.keras.optimizers.Adam(0.001),
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

callback = tf.keras.callbacks.EarlyStopping(monitor=&#39;loss&#39;, waiting=3)
history = model.fit(
ds_train,
epochs=100,
validation_data=ds_test,
verbose=0,
callbacks=[callback]
)

train_loss, train_acc = model.evaluate(ds_train, verbose=0)
test_loss, test_acc = model.evaluate(ds_test, verbose=0)
epochs = len(history.history[&#39;loss&#39;])
f.write((
f&quot;{1 - dropout_rate},&quot;
f&quot;{train_loss},&quot;
f&quot;{1 - train_acc},&quot;
f&quot;{test_loss},&quot;
f&quot;{1 - test_acc},&quot;
f&quot;{epochs}\n&quot;))
del model
tf.keras.backend.clear_session()
gc.collect()
print(tf.config.experimental.get_memory_info(&#39;GPU:0&#39;))

如何才能有效地执行此循环而不增加内存使用量？]]></description>
      <guid>https://stackoverflow.com/questions/79283083/why-does-my-gpu-memory-keep-increasing-when-i-sweep-over-model-parameters</guid>
      <pubDate>Sun, 15 Dec 2024 19:58:11 GMT</pubDate>
    </item>
    <item>
      <title>自定义环境：接住沿抛物线轨迹飞行的球</title>
      <link>https://stackoverflow.com/questions/79282335/custom-env-catching-a-ball-flying-by-a-parabolic-trajectory</link>
      <description><![CDATA[我正在练习使用 Farama Gymnasium 和 PyGame 实现自己的环境，
源代码可在此处获取
我实现了课程中的物理和策略梯度方法：https://huggingface.co/learn/deep-rl-course/unit4/hands-on
相同的代码能够解决推车杆问题。
但是我正在努力制作一个代理来移动球拍来接球。
环境：

一个球以一定的初始速度从左向右抛出
在游戏场地的右侧有一个可以上下移动的球拍
当球击中场地的另一侧时会分配奖励。如果击中球拍，则 +10，如果未击中，则 -10

我使用的是具有一个隐藏层的网络。
训练后，平台要么上升，要么下降并停留在那里。
我的想法是让它学习重力和抛物线的概念，或者至少尝试跟随球的 Y 坐标，
但这并没有发生。
你对如何调整我的参数或奖励函数有什么提示吗？
任何帮助都值得感激。]]></description>
      <guid>https://stackoverflow.com/questions/79282335/custom-env-catching-a-ball-flying-by-a-parabolic-trajectory</guid>
      <pubDate>Sun, 15 Dec 2024 12:40:38 GMT</pubDate>
    </item>
    <item>
      <title>如何为任何数据集创建强大的预处理函数？[关闭]</title>
      <link>https://stackoverflow.com/questions/79282246/how-to-create-a-robust-preprocessing-function-for-any-dataset</link>
      <description><![CDATA[我正在开展一个项目，根据患者的症状预测合适的医生专业。
该项目包括一项功能，研究人员可以上传自己的数据集并使用预先训练的机器学习模型对其进行评估。在对上传的数据进行训练后，将显示准确率、召回率和精确率等结果。
我需要编写一个通用预处理函数，在对模型进行训练之前处理研究人员上传的任何数据集。
到目前为止，我已经使用标签编码和独热编码对分类数据进行编码，但我担心处理具有不同特征的数据集。以下是我预见到的一些挑战：
噪声数据
不正确的数据类型
缺失值
多重共线性
我的问题：

一个预处理函数能否处理任何给定数据集的所有这些问题？
是否有标准技术可以以通用方式检测和解决噪声数据、不正确的类型或缺失值等问题？
我是否应该考虑为不同类型的数据集创建多个预处理管道，或者是否有一种可以根据数据集进行调整的动态方法？
]]></description>
      <guid>https://stackoverflow.com/questions/79282246/how-to-create-a-robust-preprocessing-function-for-any-dataset</guid>
      <pubDate>Sun, 15 Dec 2024 11:46:39 GMT</pubDate>
    </item>
    <item>
      <title>URL 中损坏的访问控制的数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/79282134/dataset-for-broken-access-control-in-url</link>
      <description><![CDATA[我是一名学生，正在接受一个项目任务，该项目涉及使用机器学习检测 URL 中损坏的访问控制（如 IDOR）。我一直在寻找可用于训练机器学习的数据集，但找不到与我的主题最相关的数据集。你们知道我可以在哪里找到数据集吗？
我尝试使用每个相关关键字搜索有关损坏的访问控制。]]></description>
      <guid>https://stackoverflow.com/questions/79282134/dataset-for-broken-access-control-in-url</guid>
      <pubDate>Sun, 15 Dec 2024 10:22:25 GMT</pubDate>
    </item>
    <item>
      <title>纠正抽样偏差（寻找数据科学或统计方法）？</title>
      <link>https://stackoverflow.com/questions/79282031/correcting-sampling-bias-looking-for-data-science-or-statistical-approach</link>
      <description><![CDATA[假设我正在查看一组学生及其在 5 个科目中取得的成绩。90% 的样本偏向于表现最差的学生，10% 的样本是随机的。
我可以使用什么简单的方法来纠正样本中的偏差，以近似估计无偏差的成绩总体？]]></description>
      <guid>https://stackoverflow.com/questions/79282031/correcting-sampling-bias-looking-for-data-science-or-statistical-approach</guid>
      <pubDate>Sun, 15 Dec 2024 08:39:26 GMT</pubDate>
    </item>
    <item>
      <title>模型部署：交叉验证和超参数调整</title>
      <link>https://stackoverflow.com/questions/79275593/model-deployment-cross-validation-and-hyperparameter-tuning</link>
      <description><![CDATA[我正在使用 Prophet（时间序列的元模型），我有一个与模型部署相关的问题，该问题也扩展到其他机器学习算法。因此，我使用了元文档中提供的代码进行超参数调整。它的工作原理类似于网格搜索，并基于交叉验证输出模型中使用的最佳超参数组合。该最佳组合的 RMSE 约为 11.15。因此，考虑到我想部署模型并发送到生产，我应该使用网格搜索提供的超参数组合在整个数据集上重新训练模型，还是应该将使用交叉验证训练的模型发送到生产？

我问这个问题是因为当我使用网格搜索中的超参数在整个数据集上训练模型时，RMSE 比交叉验证的更高（更差）。
]]></description>
      <guid>https://stackoverflow.com/questions/79275593/model-deployment-cross-validation-and-hyperparameter-tuning</guid>
      <pubDate>Thu, 12 Dec 2024 15:01:43 GMT</pubDate>
    </item>
    <item>
      <title>Pyannote：离线加载和应用说话人区分</title>
      <link>https://stackoverflow.com/questions/78820971/pyannote-load-and-apply-speaker-diarization-offline</link>
      <description><![CDATA[我尝试离线使用 Pyannotes 模型。
我是这样加载和应用模型的：
from pyannote.audio import Pipeline

access_token = &#39;xxxxxxxxxxx&#39;

model = Pipeline.from_pretrained(
&quot;pyannote/speaker-diarization-3.1&quot;,
use_auth_token=access_token)

path_in = &#39;blabla/1-137-A-32.wav&#39;

num_speakers = 1

model(path_in,
num_speakers=num_speakers).labels()

这样就没问题了。
但是现在我按照离线使用的说明操作：https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/applying_a_pipeline.ipynb
我的目录结构如下：
src-
     |-pyannote_offline_config.yaml
     |-pyannote_pytorch_model.bin
---- YAML ----
version: 3.1.0

pipeline:
name: pyannote.audio.pipelines.SpeakerDiarization
params:
clustering: AgglomerativeClustering
embedding: pyannote/wespeaker-voxceleb-resnet34-LM
embedding_batch_size: 32
embedding_exclude_overlap: true
分段：src/pyannote_pytorch_model.bin
分段批处理大小：32

参数：
聚类：
方法：质心
min_cluster_size：12
阈值：0.7045654963945799
分段：
min_duration_off：0.0

---- 正在加载模型 ----
path_yaml = &#39;src/pyannote_offline_config.yaml&#39;

model = Pipeline.from_pretrained(path_yaml)

path_in = &#39;blabla/1-137-A-32.wav&#39;

num_speakers = 1

model(path_in,
num_speakers=num_speakers).labels()

但结果却是：“必须先使用 pipeline.instantiate(parameters) 实例化管道，然后才能应用它。”
好的，下次尝试：
---- 加载模型 ----
path_yaml = &#39;src/pyannote_offline_config.yaml&#39;

model = Pipeline.from_pretrained(path_yaml)

params = {&#39;clustering&#39;:
{&#39;method&#39;: &#39;centroid&#39;,
&#39;min_cluster_size&#39;: 12,
&#39;threshold&#39;: 0.7045654963945799},
&#39;segmentation&#39;:
{&#39;min_duration_off&#39;: 0.0}}

pipeline = model.instantiate(params)

path_in = &#39;blabla/1-137-A-32.wav&#39;

num_speakers = 1

pipeline(path_in,
num_speakers=num_speakers).labels()

但结果是：“必须先使用 pipeline.instantiate(parameters) 实例化管道，然后才能应用它。”
我不明白问题所在。
如果我这样做，它就会起作用：
---- 加载模型 ----
path_yaml = &#39;src/pyannote_offline_config.yaml&#39;

model = Pipeline.from_pretrained(&quot;pyannote/speaker-diarization-3.1&quot;, path_yaml)

path_in = &#39;blabla/1-137-A-32.wav&#39;

num_speakers = 1

model(path_in,
num_speakers=num_speakers).labels()

但上传到 gitlab 后，测试管道显示：“无法下载‘pyannote/speaker-diarization-3.1’管道。
这可能是因为管道是私有的或封闭的，因此请确保进行身份验证。访问 https://hf.co/settings/tokens
创建您的访问令牌并重试：
Pipeline.from_pretrained(&#39;pyannote/speaker-diarization-3.1&#39;,
... use_auth_token=YOUR_AUTH_TOKEN)&quot;
因此，似乎我的本地计算机上有一些东西没有通过 pip 安装下载。例如，如果我不使用 yaml 加载它，而只使用 model = Pipeline.from_pretrained(&quot;pyannote/speaker-diarization-3.1&quot;)，它也会起作用。]]></description>
      <guid>https://stackoverflow.com/questions/78820971/pyannote-load-and-apply-speaker-diarization-offline</guid>
      <pubDate>Thu, 01 Aug 2024 12:28:41 GMT</pubDate>
    </item>
    <item>
      <title>使用 Conda + Poetry 有意义吗？</title>
      <link>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</link>
      <description><![CDATA[在机器学习项目中使用 Conda + Poetry 是否有意义？请允许我分享我的（新手）理解，请纠正或启发我：
据我所知，Conda 和 Poetry 有不同的用途，但在很大程度上是多余的：

Conda 主要是一个环境管理器（实际上不一定是 Python），但它也可以管理包和依赖项。
Poetry 主要是一个 Python 包管理器（例如，pip 的升级），但它也可以创建和管理 Python 环境（例如，Pyenv 的升级）。

我的想法是同时使用两者并划分它们的角色：让 Conda 成为环境管理器，让 Poetry 成为包管理器。我的理由是（听起来）Conda 最适合管理环境，可用于编译和安装非 Python 包，尤其是 CUDA 驱动程序（用于 GPU 功能），而 Poetry 作为 Python 包管理器比 Conda 更强大。
我已经设法通过在 Conda 环境中使用 Poetry 相当轻松地完成这项工作。诀窍是不使用 Poetry 来管理 Python 环境：我没有使用 poetry shell 或 poetry run 之类的命令，只使用 poetry init、poetry install 等（在激活 Conda 环境后）。
为了全面披露，我的 environment.yml 文件（用于 Conda）如下所示：
name: N

channels:
- defaults
- conda-forge

dependencies:
- python=3.9
- cudatoolkit
- cudnn

我的 poetry.toml 文件如下所示：
[tool.poetry]
name = &quot;N&quot;
authors = [&quot;B&quot;]

[tool.poetry.dependencies]
python = &quot;3.9&quot;
torch = &quot;^1.10.1&quot;

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

说实话，我这样做的原因之一是，在没有 Conda 的情况下，我很难安装 CUDA（用于 GPU 支持）。
这个项目设计对你来说合理吗？]]></description>
      <guid>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</guid>
      <pubDate>Tue, 25 Jan 2022 15:09:43 GMT</pubDate>
    </item>
    <item>
      <title>数据解析和特征工程管道的设计模式</title>
      <link>https://stackoverflow.com/questions/63788496/design-pattern-for-a-data-parsingfeature-engineering-pipeline</link>
      <description><![CDATA[我知道这个问题过去在这些论坛上被问过几次，但我有一个更通用的版本，将来可能也适用于其他人的项目。
简而言之 - 我正在构建一个 ML 系统（使用 Python，但在这种情况下语言选择并不十分关键），其 ML 模型位于正在发生的操作管道的末尾：

数据上传
数据解析
特征工程
特征工程（与上一步的逻辑括号不同）
特征工程（与上一步的逻辑括号不同）

...（更多步骤，如最后 3 个）

数据传递给 ML 模型

上述每个步骤都有其必须采取的一系列操作，以便构建正确的输出，然后将其用作下一个步骤的输入，等等。这些子步骤反过来可以完全彼此分离，或者其中一些可能需要先完成大步骤中的某些步骤，以生成后续步骤使用的数据。
现在的问题是，我需要构建一个自定义管道，这样就可以非常轻松地将新步骤（无论大小）添加到组合中，而不会破坏现有的步骤。
到目前为止，从架构的角度来看，我对它的外观有这样一个概念，如下所示：

在查看此架构时，我立即想到了一个责任链设计模式，它管理大步骤（1、2、...、n），并且每个大步骤都有自己的小版本的责任链，这些责任链在它们的内部发生，对于 NO_REQ 步骤独立发生，然后对于 REQ 步骤独立发生（REQ 步骤循环进行，直到它们全部完成）。如果有一个共享接口用于在大步骤和小步骤内运行逻辑，那么它可能会运行得相当整齐。
但是，我想知道，是否有更好的方法？此外，我不喜欢责任链，因为它需要一个人添加新的大/小步骤，总是编辑逻辑设置步骤包的“核心”，以手动包含新添加的步骤。我很想构建一些东西，它只会扫描每个大步骤下特定于步骤的文件夹，并自行构建 NO_REQ 和 REQ 步骤列表（以坚持开放/封闭 SOLID 原则）。
我将不胜感激任何想法。]]></description>
      <guid>https://stackoverflow.com/questions/63788496/design-pattern-for-a-data-parsingfeature-engineering-pipeline</guid>
      <pubDate>Tue, 08 Sep 2020 06:49:39 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：'_IncompatibleKeys' 对象不可调用</title>
      <link>https://stackoverflow.com/questions/59041918/typeerror-incompatiblekeys-object-is-not-callable</link>
      <description><![CDATA[我正在训练 CNN 以解决多标签分类问题，并使用 torch.save(model.state_dict(), &quot;model.pt&quot;) 保存了我的 .pt 模型。出于某种原因，当我使用以图像数组为输入的自定义函数 predict(x) 测试模型时，我收到以下错误：TypeError: &#39;_IncompatibleKeys&#39; object is not callable。它指出了下面代码的最后一部分：y_test_pred = model(images_tensors)。您知道这里的问题可能是什么吗？ 
导入 numpy 作为 np
导入 cv2
导入 torch
从 torch 导入 nn
导入 torch.nn. functional 作为 F
导入 os

类 Net(nn.Module):
def __init__(self, classes_number):
super().__init__()
self.ConvLayer1 = nn.Sequential(
nn.Conv2d(1, 8, 5), # inp (1, 512, 512)
nn.MaxPool2d(2),
nn.ReLU() # op (8, 254, 254)
)
self.ConvLayer2 = nn.Sequential(
nn.Conv2d(8, 16, 3), # inp (8, 254, 254)
nn.MaxPool2d(2),
nn.ReLU(),
            nn.BatchNorm2d(16) # 操作 (16, 126, 126)
        ）
        self.ConvLayer3 = nn.Sequential(
            nn.Conv2d(16, 32, 3), # inp (16, 126, 126)
            nn.MaxPool2d(2),
            ReLU(),
            nn.BatchNorm2d(32) # 操作 (32, 62, 62)
        ）
        self.ConvLayer4 = nn.Sequential(
            nn.Conv2d(32, 64, 3), # inp (32, 62, 62)
            nn.MaxPool2d(2),
            nn.ReLU() # 运算 (64, 30, 30)
        ）
        self.Lin1 = nn.Linear(30 * 30 * 64, 1500)
self.drop = nn.Dropout(0.5)
self.Lin2 = nn.Linear(1500, 150)
self.drop = nn.Dropout(0.3)
self.Lin3 = nn.Linear(150, classes_number)

def forward(self, x):
x = self.ConvLayer1(x)
x = self.ConvLayer2(x)
x = self.ConvLayer3(x)
x = self.ConvLayer4(x)
x = x.view(x.size(0), -1)
x = F.relu(self.Lin1(x))
x = self.drop(x)
x = F.relu(self.Lin2(x))
x = self.drop(x)
x = self.Lin3(x)
out = torch.sigmoid(x)
return out

def predict(x):
# 在考试中，x 将是指向我们保留集图像的所有路径的列表
images = []
for img_path in x:
img = cv2.imread(img_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # 转换为灰度
img = cv2.resize(img, (512, 512))
images.append(img)
images = np.array(images)
images = images.reshape(len(images), 1, images.shape[1], images.shape[1]) # converting(n,512,512)&gt;(n,1,512,512)
images_tensors = torch.FloatTensor(np.array(images))
images_tensors = images_tensors.to(device)
classes = [&quot;red blood细胞”、“困难”、“配子体”、“滋养体”、“环”、“裂殖体”、“白细胞”]
model = Net(len(classes))
model = model.load_state_dict(torch.load(&#39;model.pt&#39;))

y_test_pred = model(images_tensors)
y_test_pred[y_test_pred &gt; 0.49] = 1
y_test_pred[y_test_pred &lt; 0.5] = 0

return y_test_pred.cpu().detach()
]]></description>
      <guid>https://stackoverflow.com/questions/59041918/typeerror-incompatiblekeys-object-is-not-callable</guid>
      <pubDate>Tue, 26 Nov 2019 00:16:13 GMT</pubDate>
    </item>
    </channel>
</rss>