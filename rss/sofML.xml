<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 27 Sep 2024 12:33:33 GMT</lastBuildDate>
    <item>
      <title>如何才能准确填补数据集中的缺失值？</title>
      <link>https://stackoverflow.com/questions/79030256/how-can-i-achieve-accurate-imputation-of-missing-values-in-a-dataset</link>
      <description><![CDATA[我正在处理一个包含二手车详细信息的数据集，我发现 Fuel_Type 列中缺少几个值。可能的值包括“汽油”、“E85 混合燃料”、“混合动力”、“柴油”等。目前，我的数据中有超过 4,000 辆电动汽车、不到 50 辆汽油车和一些缺少 Fuel_Type 条目的混合动力汽车。此外，一些条目包含非标准值，如“–”和“不支持”。准确填充这些缺失值对我的分析至关重要，因为它们会显著影响结果。
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# 示例 DataFrame
data = {
&#39;Car&#39;: [&#39;Toyota&#39;, &#39;Honda&#39;, &#39;Tesla&#39;, None, &#39;Ford&#39;],
&#39;Fuel_Type&#39;: [&#39;Gasoline&#39;, &#39;E85 Flex Fuel&#39;, np.nan, &#39;Hybrid&#39;, None],
&#39;Transmission&#39;: [&#39;Automatic&#39;, None, &#39;Automatic&#39;, &#39;Manual&#39;, &#39;Manual&#39;]
}

df = pd.DataFrame(data)

# 初始插补尝试
imputer = SimpleImputer(strategy=&#39;most_frequent&#39;)
df[&#39;Fuel_Type&#39;] = imputer.fit_transform(df[[&#39;Fuel_Type&#39;]])
print(df)
]]></description>
      <guid>https://stackoverflow.com/questions/79030256/how-can-i-achieve-accurate-imputation-of-missing-values-in-a-dataset</guid>
      <pubDate>Fri, 27 Sep 2024 07:15:40 GMT</pubDate>
    </item>
    <item>
      <title>当有多个场景切换时，有没有办法让 SAM2 跨场景跟踪同一个人？</title>
      <link>https://stackoverflow.com/questions/79029852/is-there-a-way-to-have-sam2-track-the-same-person-across-scenes-when-there-are-m</link>
      <description><![CDATA[使用 Meta 的 SAM2 演示，当场景切换时，面具通常会切换到不同的玩家身上。
我知道手动重新标记每个场景中的玩家是一种选择，但我想探索是否有可用的自动化解决方案。

我尝试使用 Meta 的 SAM2 演示，网址为 https://sam2.metademolab.com/
我希望它能在整个视频中跟踪勒布朗
我发现它只在第一个场景中这样做，偶尔当勒布朗是镜头中唯一的人或主要人物时
]]></description>
      <guid>https://stackoverflow.com/questions/79029852/is-there-a-way-to-have-sam2-track-the-same-person-across-scenes-when-there-are-m</guid>
      <pubDate>Fri, 27 Sep 2024 04:47:00 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在 rust 中导入 python 制作的 ML 模型 (.pkl) 吗？</title>
      <link>https://stackoverflow.com/questions/79029841/can-we-import-a-python-made-ml-model-pkl-in-rust</link>
      <description><![CDATA[我之前用 Python 构建了一个项目，但由于它占用了太多资源并且缺乏并发性，所以我改用了 rust。现在我不知道如何正确迁移它，大多数代码已经迁移，但我无法导入导出为 .pkl 文件的 ml 模型。]]></description>
      <guid>https://stackoverflow.com/questions/79029841/can-we-import-a-python-made-ml-model-pkl-in-rust</guid>
      <pubDate>Fri, 27 Sep 2024 04:43:46 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练推荐系统算法</title>
      <link>https://stackoverflow.com/questions/79029441/trying-to-train-an-algorithm-for-a-recommender-system</link>
      <description><![CDATA[我一直收到以下错误：

ratings_matrix[which_train, ] 中的错误：维度数不正确

&gt; which_train &lt;- sample(x = c(TRUE, FALSE),
+ size = (ratings_matrix),
+ replace = TRUE,
+ prob = c(0.8, 0.2))
&gt; recc_data_train &lt;- ratings_matrix[which_train, ]
]]></description>
      <guid>https://stackoverflow.com/questions/79029441/trying-to-train-an-algorithm-for-a-recommender-system</guid>
      <pubDate>Thu, 26 Sep 2024 23:51:08 GMT</pubDate>
    </item>
    <item>
      <title>如何保存/查看树状图中的信息？</title>
      <link>https://stackoverflow.com/questions/79028907/how-can-you-save-look-at-the-information-in-a-dendogram</link>
      <description><![CDATA[我正在尝试分析数据以根据树状图确定结果。问题是我主要有 2 组数据“H”和“U”，两者一起进行分析。在树状图的末尾，我需要知道哪个区域的“H”更多和“U”。
我尝试使用树状图的字典信息，但当我打开函数时，我注意到它根本没有包含我通过链接输入的信息。
以下是我正在使用的代码：
import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, set_link_color_palette
from scipy.cluster import Hierarchy
import pandas as pd
from sklearn.decomposition import PCA

df=pd.read_csv(name+&quot;.csv&quot;, index_col=None, header=0)

normalized_df=(df-df.mean())/df.std()

pca=PCA(n_components=6, n_oversamples=6)
principalComponents = pca.fit_transform(df)
principalDF = pd.DataFrame(data = principalComponents, columns = [&#39;principal component 1&#39;, &#39;principal component 2&#39;, &#39;principal component 3&#39;, &#39;principal component 4&#39;, &#39;principal component 5&#39;, &#39;principal component 6&#39;])

#将类型 (U/H) 添加到数据框
finalDF = pd.concat([principalDF, full_df[[&#39;type&#39;]]], axis = 1)

#从这里开始，我开始遇到问题
data = list(zip(finalDF[&#39;principal component 1&#39;], finalDF[&#39;principal component 2&#39;]))

linkage_data = linkage(data, method=&#39;ward&#39;, metric=&#39;euclidean&#39;)
hierarchy.set_link_color_palette([&#39;r&#39;,&#39;g&#39;,&#39;b&#39;,&#39;w&#39;])
den=dendrogram(linkage_data)
plt.title(&quot;Attempt #1&quot;)
plt.show()

树状图看起来与预期一致。问题是我无法区分每个点的类型（H 或 U）。
如前所述，我尝试查看树状图的属性，例如 icoord 和 dcoord，但我无法弄清楚它的含义。
我使用的数据本身是 6 个不同的列，具有不同的数字，这是我标准化的数据((df-df.mean())/df.std)，然后取主成分。
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79028907/how-can-you-save-look-at-the-information-in-a-dendogram</guid>
      <pubDate>Thu, 26 Sep 2024 19:43:38 GMT</pubDate>
    </item>
    <item>
      <title>相关变量的系数是什么意思？[关闭]</title>
      <link>https://stackoverflow.com/questions/79028217/what-do-the-coefficients-on-correlated-variables-mean</link>
      <description><![CDATA[不相关变量的系数表示其中的独特信息对最终变量的影响程度。但相关变量的系数意味着什么 - 谁的“拉锯战”程度如何？（请不要进行数学运算）]]></description>
      <guid>https://stackoverflow.com/questions/79028217/what-do-the-coefficients-on-correlated-variables-mean</guid>
      <pubDate>Thu, 26 Sep 2024 16:11:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 LOOCV 进行模拟退火[关闭]</title>
      <link>https://stackoverflow.com/questions/79027782/simulated-annealing-using-loocv</link>
      <description><![CDATA[我正尝试使用 LOOCV（留一交叉验证）作为交叉验证方法，对 25 个预测变量实施模拟退火回归模型，进行 100 次迭代。
然而，问题是代码已经运行了 10 天，但仍然没有完成。以下代码可能存在问题？
# 库
library(caret)
library(foreach)
library(pROC)
library(randomForest)
library(parallel)
library(doParallel)

# 设置并行后端
Mycluster &lt;- makeCluster(detectCores() - 2) # 使用可用核心减 2
registerDoParallel(Mycluster) # 注册并行执行

stime = system.time({
# 训练函数的回归控制
reg.ctrl &lt;- trainControl(method = &quot;LOOCV&quot;, 
number = 10, 
repeats = 5, 
allowParallel = TRUE)

# 模拟退火控制设置
safs.ctrl &lt;- safsControl(functions = caretSA, 
method = &quot;LOOCV&quot;, 
number = 10,
metric = c(internal = &quot;RMSE&quot;, external = &quot;RMSE&quot;),
maximize = c(internal = FALSE, external = FALSE),
holdout = 0.2, 
improve = 5,
allowParallel = TRUE, 
verbose = TRUE)

# 模拟退火特征选择
sa_100 &lt;- safs(x = MEs[, 1:dim(MEs)[[2]]],
y = gsva[1, ],
iters = 100, 
metric = &quot;RMSE&quot;,
trControl = reg.ctrl,
safsControl = safs.ctrl)
})[3]

stime[1]

# 计算完成后停止集群
stopCluster(Mycluster)
registerDoSEQ() # 重置为顺序执行
]]></description>
      <guid>https://stackoverflow.com/questions/79027782/simulated-annealing-using-loocv</guid>
      <pubDate>Thu, 26 Sep 2024 14:21:10 GMT</pubDate>
    </item>
    <item>
      <title>多头自注意力中的梯度爆炸（NaN 训练损失和验证损失） - Vision Transformer</title>
      <link>https://stackoverflow.com/questions/79027142/exploding-gradient-nan-training-loss-and-validation-loss-in-multi-head-self-at</link>
      <description><![CDATA[此多头自注意力代码导致训练损失和验证损失变为 NaN，但当我删除此部分时，一切都恢复正常。我知道当训练损失和验证损失变为 NaN 时，这意味着那里有一个爆炸梯度。但是，我不知道我的代码出了什么问题导致梯度爆炸。当我将它与官方 PyTorch 代码进行比较时，它看起来很相似。当我使用 nn.MultiheadSelfAttention 时，梯度不会爆炸，但当我使用我自己的代码时，梯度开始爆炸。没有显示任何错误消息。有人知道我下面的代码有什么问题吗？
class MultiHeadAttention(nn.Module):
def __init__(self, in_dim, num_heads=8, dropout=0):
super().__init__()
self.num_heads = num_heads
self.head_dim = in_dim // num_heads
self.conv_q = nn.Conv2d(in_dim, in_dim, kernel_size=1)
self.conv_k = nn.Conv2d(in_dim, in_dim, kernel_size=1)
self.conv_v = nn.Conv2d(in_dim, in_dim, kernel_size=1)
self.att_drop = nn.Dropout(dropout)
self.proj = nn.Conv2d(in_dim, in_dim，kernel_size=1)
self.proj_drop = nn.Dropout(dropout)

def forward(self, x):

b, _, h, w = x.shape

q = self.conv_q(x)
k = self.conv_k(x)
v = self.conv_v(x)

q = rearrange(q，“b (nh hd) h w -&gt; b nh (h w) hd”，nh=self.num_heads)
k = rearrange(k，“b (nh hd) h w -&gt; b nh (h w) hd”，nh=self.num_heads)
v = rearrange(v，“b (nh hd) h w -&gt; b nh (h w) hd”，nh=self.num_heads)

att_score = q @ k.transpose(2, 3) ** (self.head_dim ** -0.5)
att_score = F.softmax(att_score, dim=-1)
att_score = self.att_drop(att_score)

x = att_score @ v

x = rearrange(x, &#39;b nh (h w) hd -&gt; b (nh hd) h w&#39;, h=h, w=w)

x = self.proj(x)
x = self.proj_drop(x)

返回 x
]]></description>
      <guid>https://stackoverflow.com/questions/79027142/exploding-gradient-nan-training-loss-and-validation-loss-in-multi-head-self-at</guid>
      <pubDate>Thu, 26 Sep 2024 11:48:15 GMT</pubDate>
    </item>
    <item>
      <title>如何在多个 gpu 上运行 Qwen2-VL 模型？</title>
      <link>https://stackoverflow.com/questions/79027046/how-to-run-qwen2-vl-models-on-multiple-gpus</link>
      <description><![CDATA[我有 4 个 gpu，我想运行 Qwen2 VL 模型，但我收到“设备端断言已触发。使用 TORCH_USE_CUDA_DSA 进行编译以启用设备端断言”错误。
model_name=&quot;Qwen/Qwen2-VL-2B-Instruct&quot;
model = Qwen2VLForConditionalGeneration.from_pretrained(
model_name, torch_dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)
model = nn.DataParallel(model)
processor = AutoProcessor.from_pretrained(model_name)

messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: [
{
&quot;type&quot;: &quot;image&quot;,
&quot;image&quot;: file
},
{
&quot;type&quot;: &quot;text&quot;,
&quot;text&quot;: &quot;&quot;&quot;描述图像&quot;&quot;&quot;
}
]
}
]
text = processing.apply_chat_template(
messages, tokenize=False, add_generation_prompt=True)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processing(
text=[text],
images=image_inputs,
videos=video_inputs,
padding=True,
return_tensors=&quot;pt&quot;,
)
使用 torch.no_grad():
generated_ids = model.module.generate(**inputs, max_new_tokens=128)

但我总是得到：
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [35,0,0], thread: [31,0,0] 断言 `-sizes[i] &lt;= index &amp;&amp; index &lt; sizes[i] &amp;&amp; &quot;index out of bounds&quot;` 失败。
错误：CUDA 错误：设备端断言已触发
使用 `TORCH_USE_CUDA_DSA` 进行编译以启用设备端断言。

回溯（最近一次调用）：
文件“/home/ubuntu/projects/mistral-qaC/services/VisionService.py”，第 104 行，位于 ask_vision
generated_ids = self.model.module.generate(
^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/utils/_contextlib.py”，第 116 行，位于 decorate_context
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^
文件“/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/generation/utils.py”，第2015，在 generate 中
result = self._sample(
^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/generation/utils.py&quot;，第 2965 行，在 _sample 中
output = self(**model_inputs, return_dict=True)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;，第 1553 行，在 _wrapped_call_impl 中
return self._call_impl(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;，第 1562 行，在 _call_impl 中
return forward_call(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/accelerate/hooks.py&quot;，第 169 行，在 new_forward 中
output = module._old_forward(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py&quot;，第 1598 行，正向
输入_embeds[image_mask] = image_embeds
~~~~~~~~~~~~~^^^^^^^^^^^^
RuntimeError：CUDA 错误：设备端断言已触发
使用 `TORCH_USE_CUDA_DSA` 进行编译以启用设备端断言。

我尝试了什么？

使用 CUDA_LAUNCH_BLOCKING=1 python script.py 运行我的 python 脚本，但它也不起作用。
打印输入和模型设备：模型设备：cuda:0
输入设备：cuda:0
torch.cuda.synchronize() 和 torch.cuda.empty_cache() 在生成之前。

输入的形状：

input_ids 的形状：torch.Size([1, 759])
attention_mask 的形状：torch.Size([1, 759])
 pixel_values: torch.Size([2940, 1176])
image_grid_thw 的形状：torch.Size([1, 3])

我的 transformers 和 pytorch 版本是：
transformers==4.45.0.dev0
torch==2.4.1+cu124

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79027046/how-to-run-qwen2-vl-models-on-multiple-gpus</guid>
      <pubDate>Thu, 26 Sep 2024 11:21:59 GMT</pubDate>
    </item>
    <item>
      <title>机器学习与分类神经网络中的数据重叠[关闭]</title>
      <link>https://stackoverflow.com/questions/79026738/dataoverlap-in-machine-learning-with-classification-neurnal-network</link>
      <description><![CDATA[嗨，我正在做一个关于体积脑图像的机器学习项目
但我没有 ML 背景（有一点编码……但不多），所以我一直在用 pytorch lightning 学习 ML。这很有趣，但有时很难……
所以我现在的问题是我的模型在很大程度上对数据集过度拟合，这使得我在训练中的三个类准确度达到每类 0.85 - ~1.0 之间的准确度，并且损失以良好的曲线下降。遗憾的是验证准确度不足。对于第一类，准确度稳定在 0.05 左右，而其他类稳定在 0.3 左右徘徊。此外，损失从 1.2 略微下降到略高于 1。
（一些医学内容，让您了解数据重叠问题所在）
这些类别的图像是接受 CT 灌注的患者的灰度图像。它显示了患者动脉闭塞或阻塞时大脑灌注的变化。
对于非医学方面的人来说，你可以把它看作是一棵树的树枝，上面有叶子。如果树枝有阻塞，那么该树枝上的叶子就会枯萎。
我的类别是 ICA-T、M1 和最后一类 M2
ICA-T 分支在 M1 中。这意味着体积图像可能包含这些片段的信号相似性。所以也许模型认为由于重叠，ICA-T 病例被猜测为 M1??
这就是类别背景。原始数据集总体如下：
类别 0 (ICA-T)：94，
类别 1 (M1)：366，
类别 2 (M2)：119
总计 579
这显然在类别 1 中占比过高。我对此有疑问，因此我将类别 1 减少到 157，没有考虑任何策略。
0 类 (ICA-T)：94，
1 类 (M1)：157，
2 类 (M2)：119
总计 370
我有一个训练数据集、验证数据集和测试数据集，这些数据集是我通过分层随机分割获得的，这给了我
训练：259 - 0 类：66，1 类：110，2 类：83
验证：37 - 0 类：9，1 类：16，2 类：12
测试：74 - 0 类：19，1 类：31，2 类：24
进一步的信息是，图像是体积图像，空间大小为 256,256,32。
我认为问题在于类相似性。但我不确定是否如此。
有人遇到过类似的问题吗？他们解决了吗？或者有人可以指导我找到答案吗？
遗憾的是我的项目很快就要完成了，所以我没有时间。如果有人需要代码，请询问，但我认为这对我的问题来说没有必要？..
这是我使用的结构：
class CNN5_Mod(L.LightningModule): # 模型定义
def __init__(self, num_classes):
super(CNN5_Mod, self).__init__()
from Project1.MyFile.ConfigMain import Config
config = Config()
# 特征 - x y z
# 1 - 256 256 32
&quot;&quot;&quot; Block 1 &quot;&quot;&quot;
self.conv1 = nn.Conv3d(1, config.c1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=1) # 16 - 256 256 32
self.relu1 = nn.ReLU()
self.bt_nm1 = nn.BatchNorm3d(config.c1)

self.conv2 = nn.Conv3d(config.c1, config.c2, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=1) # 32 - 128 128 16
self.relu2 = nn.ReLU()
self.bt_nm2 = nn.BatchNorm3d(config.c2)

self.dropout1 = nn.Dropout3d(config.dropout1)
self.pooling1 = nn.MaxPool3d(kernel_size=2, stride=2)
# 输出 (64, 64,64,8)

&quot;&quot;&quot; 块 2 &quot;&quot;&quot;
self.conv3 = nn.Conv3d(config.c2, config.c3, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=1) # 32 32 4
self.relu3 = nn.ReLU()
self.bt_nm3 = nn.BatchNorm3d(config.c3)

self.conv4 = nn.Conv3d(config.c3, config.c4, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=1) # 16 16 2
self.relu4 = nn.ReLU()
self.bt_nm4 = nn.BatchNorm3d(config.c4)

self.dropout2 = nn.Dropout3d(config.dropout2)
self.pooling2 = nn.MaxPool3d(kernel_size=2, stride=2)
# 输出 (256, 8,8,1)

&quot;&quot;&quot; 扁平化 &quot;&quot;&quot;
self.flatten = nn.Flatten(1)

&quot;&quot;&quot; 隐藏层 &quot;&quot;&quot;
self.fc1 = nn.Linear(config.in_fc, 64)
self.relu5 = nn.ReLU()
self.fc_dropout = nn.Dropout3d(config.fc_dropout)

&quot;&quot;&quot; 输出 &quot;&quot;&quot;
self.fc2 = nn.Linear(64, num_classes)

&quot;&quot;&quot; 预测 &quot;&quot;&quot;
self.softmax = nn.Softmax(dim=1)

我改变了 dropout rate 和其他一些值。但我不知所措。。]]></description>
      <guid>https://stackoverflow.com/questions/79026738/dataoverlap-in-machine-learning-with-classification-neurnal-network</guid>
      <pubDate>Thu, 26 Sep 2024 10:13:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中训练眼睛验证（而非识别）模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</link>
      <description><![CDATA[我想知道我们如何训练一对一图像验证模型。模型拍摄两张图像并验证它们是否相同。我想要训练眼睛认证模型的软件算法。
我在网上搜索过，但只能找到关于识别软件算法（一对多）的答案。
如何在 PyTorch 代码中训练验证模型？
需要澄清的是，相同是指眼睛相同，意味着它们属于同一个人。这是一个验证模型。]]></description>
      <guid>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</guid>
      <pubDate>Tue, 24 Sep 2024 18:10:07 GMT</pubDate>
    </item>
    <item>
      <title>使用斑点检测计数黑色圆形种子</title>
      <link>https://stackoverflow.com/questions/79016356/counting-black-round-seeds-with-blob-detection</link>
      <description><![CDATA[我想玩一下 OpenCV 或类似技术，以便能够计算简单的黑色球体（种子，在本例中为拟花椒）。
其他时候，种子中间的白色反射较少，但主要特征是它是“圆形”的、黑色的，几乎总是具有相同的尺寸，并且可以（或有时没有）一个白色的小斑点。





我应该从哪里开始才能让 5 张照片的种子数量大致相同（或者最好是完全相同）？（种子数量相同，我只是在拍摄照片之间摇晃了一下容器）
CV 还是 ML？从哪里开始？
附言：如果有帮助，我也可以尝试物理去除较小的黑色棍子和不好的种子……但理论上，如果可以有可靠的方法可以忽略这些小的“非种子”暗元素，那就太好了……
附言：如果这也能有帮助，我还可以修改拍照的方式……]]></description>
      <guid>https://stackoverflow.com/questions/79016356/counting-black-round-seeds-with-blob-detection</guid>
      <pubDate>Mon, 23 Sep 2024 21:30:43 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenCV 进行 OCR 和文本检测和识别</title>
      <link>https://stackoverflow.com/questions/69647125/how-to-use-opencv-to-do-ocr-and-text-detect-and-recognition</link>
      <description><![CDATA[我正在开发一个测试应用程序，使用 Google Collab 在 Python 中开发一个小型文本检测和识别应用程序。您能提供一些代码示例来实现这一点吗？我的要求是我应该能够使用 OpenCV 检测和识别图像中的文本。
请提供建议。]]></description>
      <guid>https://stackoverflow.com/questions/69647125/how-to-use-opencv-to-do-ocr-and-text-detect-and-recognition</guid>
      <pubDate>Wed, 20 Oct 2021 13:40:59 GMT</pubDate>
    </item>
    <item>
      <title>xgboost.plot_tree：二元特征解释</title>
      <link>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</link>
      <description><![CDATA[我构建了一个 XGBoost 模型，并试图检查各个估计量。作为参考，这是一个二元分类任务，具有离散和连续输入特征。输入特征矩阵是 scipy.sparse.csr_matrix。
然而，当我去检查一个单独的估计量时，我发现很难解释二元输入特征，例如下面的 f60150。最底部图表中的实值 f60150 很容易解释 - 其标准在该特征的预期范围内。但是，对二元特征 &lt;X&gt; &lt; -9.53674e-07 进行的比较没有意义。这些特征中的每一个要么是 1，要么是 0。-9.53674e-07 是一个非常小的负数，我想这只是 XGBoost 或其底层绘图库中的一些浮点特性，但当特征始终为正时使用这种比较是没有意义的。有人能帮我理解哪个方向（即 是、缺失 与 否 对应这些二进制特征节点的哪一侧为真/假吗？
这是一个可重现的示例：
import numpy as np
import scipy.sparse
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from xgboost import plot_tree, XGBClassifier
import matplotlib.pyplot as plt

def booleanize_csr_matrix(mat):
&#39;&#39;&#39; 将具有正整数元素的稀疏矩阵转换为 1 &#39;&#39;&#39;
nnz_inds = mat.nonzero()
keep = np.where(mat.data &gt; 0)[0]
n_keep = len(keep)
result = scipy.sparse.csr_matrix(
(np.ones(n_keep), (nnz_inds[0][keep], nnz_inds[1][keep])),
shape=mat.shape
)
返回结果

### 设置数据集
res = fetch_20newsgroups()

text = res.data
outcome = res.target

### 使用 CountVectorizer 的默认参数创建初始计数矩阵
vec = CountVectorizer()
X = vec.fit_transform(text)

# 是否“布尔化”输入矩阵
booleanize = True

# 是否在“布尔化”之后将数据类型转换为与 `vec.fit_transform(text)` 返回的内容相匹配
to_int = True

如果 booleanize 和 to_int:
X = booleanize_csr_matrix(X)
X = X.astype(np.int64)

# 使其成为二元分类问题
y = np.where(outcome == 1, 1, 0)

# 随机状态确保我们能够一致地比较树及其特征
model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

将 booleanize 和 to_int 设置为 True 并运行上述程序，将生成以下图表：

将 booleanize 和 to_int 设置为 False 并运行上述程序，将生成以下图表：

哎呀，即使我做了一个非常简单的例子，我也会得到“正确”的结果，无论 X 或 y 是整数还是浮点类型。
X = np.matrix(
[
[1,0],
[1,0],
[0,1],
[0,1],
[1,1],
[1,0],
[0,0],
[0,0],
[1,1],
[0,1]
]
)

y = np.array([1,0,0,0,1,1,1,0,1,1])

model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</guid>
      <pubDate>Thu, 13 Sep 2018 13:06:06 GMT</pubDate>
    </item>
    <item>
      <title>真实世界参数优化</title>
      <link>https://stackoverflow.com/questions/14013266/realworld-parameter-optimization</link>
      <description><![CDATA[我需要对我的最新研究项目进行参数优化。我有一个算法，目前有 5 个参数（四个双精度 [0,1] 和一个具有 3 个值的名义参数）。该算法使用这些参数来计算一些东西，然后我计算准确率、召回率和 FMeasure。一次运行大约需要 1.8 秒。目前，我正在以 0.1 的步长遍历每个参数，这向我展示了全局最大值的大致位置。但我想找到精确的全局最大值。我研究过梯度下降，但我真的不知道如何将其应用于我的算法（如果可能的话）。有人可以指导我如何实现这样的算法吗，因为我对这类工作很陌生。
干杯，
丹尼尔]]></description>
      <guid>https://stackoverflow.com/questions/14013266/realworld-parameter-optimization</guid>
      <pubDate>Sun, 23 Dec 2012 17:55:54 GMT</pubDate>
    </item>
    </channel>
</rss>