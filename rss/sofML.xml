<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 21 Oct 2024 15:19:05 GMT</lastBuildDate>
    <item>
      <title>是否有一种方法或函数可以通过调试、记录器或库获取 Python 神经网络代码中每个变量的值？</title>
      <link>https://stackoverflow.com/questions/79110588/is-there-a-way-or-a-function-in-which-through-debugging-or-logger-or-a-library-i</link>
      <description><![CDATA[我想了解模型在每个步骤中对其每个变量（如权重、损失、偏差）做了什么，从一个层到另一个层，甚至在批次级别。
是否有任何预建库或方法可以让我获得完整的细分？
下面是我当前的代码，我正在尝试构建所有自定义回调，但我想完全摆脱它，并从分配权重等的第 0 阶段的最基础级别理解模型。
来自 keras.models 导入 Sequential
来自 keras.layers 导入 Dense
来自 keras.callbacks 导入 Callback
导入 numpy 作为 np

步骤 1：自定义回调以了解模型内部工作原理
class DebuggingCallback(Callback):
def on_epoch_begin(self, epoch, logs=None):
print(f&quot;\n--- Epoch {epoch + 1} Start ---&quot;)

def on_epoch_end(self, epoch, logs=None):
print(f&quot;--- Epoch {epoch + 1} End ---&quot;)
for layer_index, layer in enumerate(self.model.layers):
weights, biases = layer.get_weights()
print(f&quot;Layer {layer_index + 1}: Weights\n{weights}&quot;)
print(f&quot;Layer {layer_index + 1}: Biases\n{biases}&quot;)

def on_batch_end(self, batch, logs=None):
loss = logs.get(&#39;loss&#39;)
accuracy = logs.get(&#39;accuracy&#39;)
print(f&quot;Batch {batch + 1} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}&quot;)

步骤 2：定义 ANN 模型
classifier = Sequential()
定义输入和第一个隐藏层层
classifier.add(Dense(units=3, input_dim=numFeatures, kernel_initializer=&#39;uniform&#39;, activity=&#39;relu&#39;))

定义第二个隐藏层
classifier.add(Dense(units=2, kernel_initializer=&#39;uniform&#39;, activity=&#39;relu&#39;))

定义输出层
classifier.add(Dense(units=1, kernel_initializer=&#39;uniform&#39;, activity=&#39;sigmoid&#39;))

编译模型
classifier.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

步骤3：拟合模型并使用自定义回调
survivalANN_Model = classifier.fit(X_train, y_train, 
batch_size=30, 
epochs=5, 
verbose=1, 
callbacks=[DebuggingCallback()])

步骤 4：打印训练后的最终权重
print(&quot;\nFinal Weights After Training:&quot;)
for i, layer in enumerate(classifier.layers):
weights, biases = layer.get_weights()
print(f&quot;Layer {i + 1}:&quot;)
print(&quot;Weights:&quot;, weights)
print(&quot;Biases:&quot;, biases)
]]></description>
      <guid>https://stackoverflow.com/questions/79110588/is-there-a-way-or-a-function-in-which-through-debugging-or-logger-or-a-library-i</guid>
      <pubDate>Mon, 21 Oct 2024 14:50:13 GMT</pubDate>
    </item>
    <item>
      <title>将数据集拆分为训练/测试 - 但保证某个键/标识符在两者中都不存在</title>
      <link>https://stackoverflow.com/questions/79109880/split-dataste-into-train-test-but-guarantee-that-a-certain-key-identifier-does</link>
      <description><![CDATA[我需要做的是将数据集拆分为训练/测试数据集（python），但请考虑以下几点：

遵循近似的训练/测试比例拆分（例如 70/30）
数据集中的每一行都有一个键。这些键不是唯一的，这意味着我可能有 1000 行带有 1 个键，另外 2 行带有另一个键。我想保证训练和测试中不存在相同的键。

我尝试了网上的所有方法（包括此处的类似主题），例如 GroupKFold 等。但似乎我无法找到任何方法来做到这一点。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79109880/split-dataste-into-train-test-but-guarantee-that-a-certain-key-identifier-does</guid>
      <pubDate>Mon, 21 Oct 2024 11:39:07 GMT</pubDate>
    </item>
    <item>
      <title>更新 Python 中的傅里叶项以进行未来预测</title>
      <link>https://stackoverflow.com/questions/79109109/updating-fourier-terms-in-python-for-future-predicitons</link>
      <description><![CDATA[我在 Python 中构建了一个 XGBoost 预测模型，该模型结合了滞后特征、日期时间特征和傅里叶项。我的目标是对每个新的工作日进行预测。该模型使用 5 倍交叉验证进行训练，我从调整过程中保存了最佳超参数。之后，我使用这些最佳参数重新训练整个模型。
我面临的挑战是使用预测日的正确特征更新未来数据框：
对于日期时间特征，我可以轻松更新它们。但是，更新傅里叶项 (FT) 和滞后是我遇到的问题。以下是我尝试过的方法：
我将历史数据框与新的预测数据框结合起来，将 Y 列重命名为 pred。对于第一个预测日，我使用最后已知的实际值。对于后续的预测日，我需要使用历史数据和新预测值的组合来更新傅里叶项和滞后。尽管尝试了几次，我还是无法让更新过程正常工作。傅立叶项没有按预期对齐，我很难根据历史数据和预测数据的组合调整滞后。
问题：如何使用历史预测和新预测的组合正确更新未来几天的傅立叶项和滞后？以下是数据框。 pred 列是实际历史数据的 Y 变量（国家计数）的副本。

# 使用 NaN 初始化预测列
future_df_all_countries[&#39;pred&#39;] = np.nan

# 使用已经训练过的 `best_model` 对未来数据进行预测
# 循环遍历 future_df_all_countries 中的每一行
for i in range(len(future_df_all_countries)):
if not future_df_all_countries[&#39;is_actual&#39;].iloc[i]: # 仅预测未来数据
if i == 0:
# 对于第一个预测天，使用最后的实际计数
last_actual_count = future_df_all_countries.loc[future_df_all_countries[&#39;is_actual&#39;] == True, &#39;Country count&#39;].iloc[-1]
future_df_all_countries.at[future_df_all_countries.index[i], &#39;pred&#39;] = last_actual_count
else:
# 使用已经训练好的模型对当天进行预测
future_df_all_countries.at[future_df_all_countries.index[i], &#39;pred&#39;] = best_model.predict(future_df_all_countries[FEATURES].iloc[[i]])[0]

# 计算傅里叶变换的函数
def calculate_fourier_transform(group, components):
data_FT = group[[&#39;pred&#39;]] # 使用 &#39;pred&#39; 列进行 FT 计算
if data_FT[&#39;pred&#39;].isnull().all():
return pd.DataFrame(index=group.index) # 如果全部为 NaN，则返回空 DF
country_count_fft = np.fft.fft(np.asarray(data_FT[&#39;pred&#39;].tolist()))
ifft_results = pd.DataFrame(index=group.index)

# 使用指定的列名进行更新
for i, num_ in enumerate(components):
fft_list = np.copy(country_count_fft)
fft_list[num_:-num_] = 0
ifft_results[f&#39;ifft_{num_}_components&#39;] = np.fft.ifft(fft_list).real

return ifft_results

# 用于傅里叶变换的组件列表
component_list = [70, 80, 90] # 使用指定的组件

#使用“pred”列计算所有行的 FT
fourier_results = calculate_fourier_transform(future_df_all_countries, component_list)

# 使用 FT 结果更新 future_df_all_countries
for col in fourier_results.columns:
future_df_all_countries[col] = fourier_results[col]

# 根据“pred”创建滞后特征
def create_lag_features(df, target_col=&#39;pred&#39;, group_col=&#39;SHIP_TO_COUNTRY&#39;, lags=[5, 10, 15, 30]):
for lag in lags:
df[f&#39;lag_{lag}_day&#39;] = df.groupby(group_col)[target_col].shift(lag)
return df

# 应用滞后特征创建
future_df_all_countries = create_lag_features(future_df_all_countries)

# 打印结果 future_df_all_countries，其中包含更新后的预测和傅里叶分量
print(future_df_all_countries)
]]></description>
      <guid>https://stackoverflow.com/questions/79109109/updating-fourier-terms-in-python-for-future-predicitons</guid>
      <pubDate>Mon, 21 Oct 2024 08:03:58 GMT</pubDate>
    </item>
    <item>
      <title>从处理后的图像中提取文本</title>
      <link>https://stackoverflow.com/questions/79108131/extract-text-from-processed-image</link>
      <description><![CDATA[我正在尝试从任何欧盟车牌的裁剪图像中提取文本。我尝试使用 easyocr，但结果对我来说太不准确了。我已经训练了一个 YOLOV8 模型来检测车牌，然后我裁剪了图像并对其进行了一些处理，如灰度和阈值处理。有人能帮我完成最后一步，从裁剪和处理过的车牌中提取文本吗？我的代码目前看起来像这样
from ultralytics import YOLO
import cv2
from inference import get_model
import easyocr

reader = easyocr.Reader([&#39;en&#39;])

plate_model = get_model(model_id=&quot;plate-recogniser/3&quot;, api_key=&lt;API_KEY&gt;)

# 加载图像
image_file = r&quot;C:\Users\46723\Desktop\Plate Recognition\samples\swedish.jpg&quot;
image = cv2.imread(image_file)

# 执行推理以检测车牌
results = plate_model.infer(image)[0]

for prediction in results.predictions:
x_center = prediction.x
y_center = prediction.y
width = prediction.width
height = prediction.height

# 计算边界框坐标
x1 = int(x_center - width / 2)
y1 = int(y_center - height / 2)
x2 = int(x_center + width / 2)
y2 = int(y_center + height / 2)

# 裁剪检测到的车牌区域
license_plate_crop = image[y1:y2, x1:x2]

# 转换为灰度
license_plate_crop_gray = cv2.cvtColor(license_plate_crop, cv2.COLOR_BGR2GRAY)

# 自适应阈值化
license_plate_crop_thresh = cv2.adaptiveThreshold(
license_plate_crop_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
cv2.THRESH_BINARY_INV, 11, 2
)

text_results = reader.readtext(license_plate_crop_thresh)

for (bbox, text, prob) in text_results:
print(f&quot;检测到的车牌文本：{text}，置信度：{prob:.2f}&quot;)

cv2.imshow(&quot;裁剪后的车牌&quot;, license_plate_crop)
cv2.waitKey(0)

cv2.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/79108131/extract-text-from-processed-image</guid>
      <pubDate>Sun, 20 Oct 2024 21:56:49 GMT</pubDate>
    </item>
    <item>
      <title>如何有效实施人工智能（AI）和机器学习（ML）系统的测试自动化策略？[关闭]</title>
      <link>https://stackoverflow.com/questions/79107998/how-can-i-effectively-implement-test-automation-strategies-for-artificial-intell</link>
      <description><![CDATA[我的任务是自动测试一个复杂的 AI/ML 系统，该系统涉及随着时间推移从数据中学习的各种算法和模型。鉴于 AI/ML 输出的不确定性以及模型验证所涉及的复杂性，我正在努力创建一个强大的测试自动化框架，以确保功能和性能。
我尝试过的方法：到目前为止，我已经探索了以下技术：

为单个算法和函数编写单元测试。
使用统计方法根据预期结果验证模型输出。
进行性能测试以评估不同数据负载下的模型响应时间。

但是，我面临着几个挑战，包括：

难以为基于训练数据进行调整的模型定义预期结果。
管理和验证训练和测试所需的大型数据集。
考虑到模型预测的固有可变性，确保测试的可靠性和可重复性。

预期结果：我正在寻求先进的策略和实施 AI/ML 系统测试自动化的最佳实践，重点关注：
有效测试 AI 模型功能和性能的方法：

以自动化方式验证模型准确性和稳健性的技术。
在不影响模型性能的情况下将测试集成到 ML 管道 (CI/CD) 中的策略。
]]></description>
      <guid>https://stackoverflow.com/questions/79107998/how-can-i-effectively-implement-test-automation-strategies-for-artificial-intell</guid>
      <pubDate>Sun, 20 Oct 2024 20:23:55 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：'LlamaForCausalLM' 对象没有属性 'invoke'</title>
      <link>https://stackoverflow.com/questions/79107923/attributeerror-llamaforcausallm-object-has-no-attribute-invoke</link>
      <description><![CDATA[我正在尝试对块进行一些总结。尝试使用“meta-llama/Meta-Llama-3.1-8B-Instruct”。代码如下
%%capture
from huggingface_hub import login

login(token=&quot;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&quot;)
quantization_config = BitsAndBytesConfig(load_in_4bit=True,
llm_int4_enable_fp32_cpu_offload=True)

llm_llama_model = AutoModelForCausalLM.from_pretrained(
&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;,
torch_dtype=torch.float32,
temperature =0,
device_map=&#39;auto&#39;,
quantization_config=quantization_config
)

llm_llama_tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;)

然后调用它
prompt = PromptTemplate(
template = &quot;&quot;&quot;写一个简明的总结。总结应该是项目要点的列表。总结不能超过 5 个项目要点。文本是：
{text}
简明总结：&quot;&quot;&quot;,
input_variables=[&quot;text&quot;]
)

chunkSummaries = []

for split in splits:
response = llm_llama_model.invoke(prompt.format(text=split.page_content))
chunkSummaries.append(response.content)

我看到了 AttributeError。尝试谷歌搜索，我找到的唯一答案是使用 unsloth。尝试使用它，但有太多其他错误。非常感谢任何解决 attributeerror 的见解]]></description>
      <guid>https://stackoverflow.com/questions/79107923/attributeerror-llamaforcausallm-object-has-no-attribute-invoke</guid>
      <pubDate>Sun, 20 Oct 2024 19:39:28 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：使用 MSE 时无法将字符串转换为浮点数：'？'</title>
      <link>https://stackoverflow.com/questions/79107349/valueerror-could-not-convert-string-to-float-while-working-with-mse</link>
      <description><![CDATA[我正在使用 auto-mpg 数据集。我在下面给出了数据集的链接：
https://www.kaggle.com/datasets/uciml/autompg-dataset
我在下面给出了代码：
df = pd.read_csv(&#39;data/auto-mpg.csv&#39;)

df.head()
df = df.drop(&#39;car name&#39;, axis=1)
X = df

X.head()
y = df[&#39;mpg&#39;]

y.head()
SEED = 1
# 将数据分成 70% 训练和 30% 测试
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)

# 实例化 DecisionTreeRegressor dt
dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)

# 计算包含 10 倍 CV MSE 的数组
MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv = 10,
scoring = &#39;neg_mean_squared_error&#39;, n_jobs = 1)

RMSE_CV = (MSE_CV_scores.mean())**(1/2)

#Error
ValueError:
所有 10 次拟合均失败。
很可能是您的模型配置错误。
您可以尝试通过设置 error_score=&#39;raise&#39; 来调试错误。
以下是有关失败的更多详细信息：
--------------------------------------------------------------------------------
10 次拟合失败，错误如下：

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError：无法将字符串转换为浮点数：&#39;？&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/79107349/valueerror-could-not-convert-string-to-float-while-working-with-mse</guid>
      <pubDate>Sun, 20 Oct 2024 14:49:44 GMT</pubDate>
    </item>
    <item>
      <title>Light Gradient Boosting Machine 无法使用 GPU</title>
      <link>https://stackoverflow.com/questions/79107131/light-gradient-boosting-machine-can-not-use-the-gpu</link>
      <description><![CDATA[我正在尝试使用 Pycaret 进行一些练习。当我尝试仅使用 CPU 使用 setup 时，比较模型和调整模型需要很长时间。因此，我在 setup 函数中使用了 use_gpu=True。这样我就可以继续进行 pycaret 操作了。
但是我得到了错误
[LightGBM] [Fatal] 此版本中未启用 CUDA Tree Learner
请使用 CMake 选项 -DUSE_CUDA=1 重新编译

然后使用 lightgbm 进行调整变得非常慢。似乎使用例如 catboost 创建其他模型仍然很快。
可能发生了什么，如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/79107131/light-gradient-boosting-machine-can-not-use-the-gpu</guid>
      <pubDate>Sun, 20 Oct 2024 13:17:01 GMT</pubDate>
    </item>
    <item>
      <title>如何估计 CoreML 模型的参数数量？</title>
      <link>https://stackoverflow.com/questions/79103126/how-to-estimate-the-number-of-parameters-for-coreml-models</link>
      <description><![CDATA[我正在比较修剪对 CoreML 模型的影响。
虽然我可以轻松测量文件大小的变化（以 kB 为单位），但我很难估计模型参数数量的变化，因为 CoreML 没有提供像 PyTorch 的 model.parameters() 这样的直接方法。我如何估计或计算修剪后的 CoreML 模型中的参数数量？]]></description>
      <guid>https://stackoverflow.com/questions/79103126/how-to-estimate-the-number-of-parameters-for-coreml-models</guid>
      <pubDate>Fri, 18 Oct 2024 17:35:25 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：将输入绑定到 tf.function 失败，无法将 input_tensor TensorSpec 转换为 TensorSpec</title>
      <link>https://stackoverflow.com/questions/79094829/typeerror-binding-inputs-to-tf-function-failed-can-not-cast-input-tensor-tenso</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79094829/typeerror-binding-inputs-to-tf-function-failed-can-not-cast-input-tensor-tenso</guid>
      <pubDate>Wed, 16 Oct 2024 15:43:54 GMT</pubDate>
    </item>
    <item>
      <title>训练T5时如何添加EOS？</title>
      <link>https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5</guid>
      <pubDate>Tue, 15 Oct 2024 04:22:59 GMT</pubDate>
    </item>
    <item>
      <title>使用 tf.keras.metrics.R2Score 导致 Tensorflow 出现错误</title>
      <link>https://stackoverflow.com/questions/78056806/using-tf-keras-metrics-r2score-results-in-an-error-in-tensorflow</link>
      <description><![CDATA[我正在使用 Tensorflow 制作回归模型，但是当我使用 tf.keras.metrics.R2Score() 作为指标时，它在第一个 epoch 之后失败，并出现 ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.0&gt;。 （但在此之前工作正常）但是，如果我使用不同的指标（tf.keras.metrics.RootMeanSquaredError()），它工作正常。
import pandas as pd

weather_states = pd.read_sql(&quot;SELECT stations.id, stations.capacity_kw, start, wind_speed_10m, wind_direction_10m, wind_speed_80m, wind_direction_80m, wind_speed_180m, wind_direction_180m FROM stations INNER JOIN weather_states ON stations.id = weather_states.station WHERE weather_states.source = &#39;openmeteo_forecast/history/best&#39; AND stations.source = &#39;wind&#39;&quot;, db_client)

grid_states = pd.read_sql(&quot;SELECT start, wind来自 grid_states&quot;, db_client)

def create_x_y(df: tuple[Any, pd.DataFrame]):
start = df[1][&quot;start&quot;].iloc[0]
res = df[1].sort_values(&quot;id&quot;).drop([&quot;id&quot;, &quot;start&quot;], axis=1)
temp_wind = grid_states.loc[grid_states[&quot;start&quot;] == start][&quot;wind&quot;].to_list()
wind_kw = temp_wind if len(temp_wind) &gt;= 1 else None
res_flat_df = pd.DataFrame(res.to_numpy().reshape((1, -1)))
res_flat_df[&quot;wind_kw&quot;] = wind_kw
返回res_flat_df

data = pd.concat(map(create_x_y, weather_states.groupby(&quot;start&quot;))).dropna()
来自 sklearn.model_selection 导入 train_test_split

data = data.astype(&quot;float32&quot;)
train, test = train, test = train_test_split(data.dropna(), test_size=0.2)

train_y = train.pop(&quot;wind_kw&quot;)
train_x = train

test_y = test.pop(&quot;wind_kw&quot;)
test_x = test

norm = tf.keras.layers.Normalization()
norm.adapt(train_x)

model = tf.keras.Sequential([
norm,
tf.keras.layers.Dense(16,activation=&quot;linear&quot;),
tf.keras.layers.Dropout(0.3),
tf.keras.layers.Dense(1, 激活=&quot;线性&quot;),
])

model.compile(
优化器=tf.keras.optimizers.legacy.Adam(0.001),
指标=[tf.keras.metrics.R2Score(dtype=tf.float32)],
损失=tf.keras.losses.MeanSquaredError(),
)

model.fit(train_x, train_y, epochs=7, batch_size=2)

tf.keras.models.save_model(model, &#39;wind.keras&#39;)

print(data.describe())
 0 1 2 3 4 ... 241 242 243 244 wind_kw
计数 1896.0 1896.000000 1896.000000 1896.000000 1896.000000 ... 1896.000000 1896.000000 1896.000000 1896.000000 1896.000000
平均值 144000.0 4.315717 189.610759 5.791377 193.830169 ... 3.881292 145.420359 4.572205 143.642405 1292.576958
标准差 0.0 2.482439 113.178764 2.926497 113.685887 ... 2.612259 93.293471 2.775681 94.721086 611.333721
最小值 144000.0 0.100000 1.000000 0.100000 1.000000 ... 0.100000 2.000000 0.000000 1.000000 34.263000
25% 144000.0 2.110000 88.000000 3.487500 90.000000 ... 1.900000 67.000000 2.500000 63.000000 793.109500
50% 144000.0 4.110000 199.000000 5.500000 231.000000 ... 3.075000 137.000000 3.940000 135.000000 1251.590000
75% 144000.0 6.220000 291.000000 7.882500 294.000000 ... 5.502500 205.000000 6.082500 205.000000 1761.926750
最大144000.0 11.670000 360.000000 15.210000 360.000000 ... 14.460000 360.000000 16.980000 360.000000 3008.125000

print(type(data))
#&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
print(data.dtypes)
#0 float32
#1 float32
#2 float32
#3 float32
#4 float32
# ... 
#241 float32
#242 float32
#243 float32
#244 float32
#wind_kw float32
#Length: 246, dtype: object
print(data.shape)
#(1896, 246)

我似乎无法在网上找到有关使用 R2Score 时出现此错误的任何信息 - 您对问题可能是什么有任何想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78056806/using-tf-keras-metrics-r2score-results-in-an-error-in-tensorflow</guid>
      <pubDate>Sun, 25 Feb 2024 16:40:38 GMT</pubDate>
    </item>
    <item>
      <title>如何将分类变量添加到百分比堆积条形图？</title>
      <link>https://stackoverflow.com/questions/77068899/how-to-add-categorical-variables-to-a-percentage-stacked-bar-chart</link>
      <description><![CDATA[第一次在这里发帖，如果我遗漏了通常包含的任何细节，请告诉我。
我正在使用 ggplot2 和 ggdendro 制作带有分层聚类树的堆叠条形百分比图，其中每个节点都与我的一个条形图相关联。

如您所见，我或多或少已经弄清楚了这一点（请注意，这只是我的数据的一个子集。我现在想将一个分类变量与我的每个条形图关联起来，其中每个变量都用一种颜色表示（在我的情况下，这是 HIV+ 或 HIV-，每个条形图代表给定类别中细胞的百分比）。此外，我想弄清楚如何将样本名称添加到每个树状图节点，但这个问题不那么紧迫。
以下是我正在使用的代码块。
library(ggplot2)
library(ggdendro)

# 加载表型图数据
TotalPercentage &lt;- read.csv(&quot;~/TotalPercentage.csv&quot;, header=TRUE)

#生成树
tree &lt;- hclust(dist(TotalPercentage))
tree &lt;- dendro_data(tree)

数据 &lt;- cbind(TotalPercentage, x = match(rownames(TotalPercentage), tree$labels$label))

# 在堆积条形图下方绘制，位于 &quot;data = tidyr::pivot_longer(data, c(2...&quot;包括
## 所有列（集群），但排除列 1，因为该值是我们的样本 ID

scale &lt;- .5
p &lt;- ggplot() +
geom_col(
data = tidyr::pivot_longer(data, c(2, 3 , 4, 5, 6, 7, 8)),
aes(x = x,
y = value, fill = factor(name)),
) +
labs(title=&quot;无监督聚类表型图输出&quot;,
x =&quot;集群表示 (%)&quot;, y = &quot;参与者样本&quot;
) +
geom_segment(
data = tree$segments,
aes(x = x, y = -y * scale, xend = xend, yend = -yend * scale)
)

p

这是一个样本数据集，包含较少行以便简单起见
data.frame(
`参与者 ID` = c(&quot;123&quot;, &quot;456&quot;, &quot;789&quot;),
`1` = c(.1933, .1721, 34.26),
`2` = c(20.95, 4.97, 2.212),
`3` = c(11.31, 35.34, .027),
`4` = c(35.55, 15.03, 0),
`5` = c(.26, .87, 7.58),
`6` = c(12.85, 33.44, .033),
`7` = c(2.04, 3.77, 4.32)
)

患者一和三感染 HIV，但患者二感染 HIV 阴性
最后，这是我最终尝试制作的一个例子
(https://i.sstatic.net/uAWxR.png)
我已经到处查看如何执行此操作，但我对 R 还不熟悉，所以我有点不知所措，不知道下一步该怎么做。提前感谢任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/77068899/how-to-add-categorical-variables-to-a-percentage-stacked-bar-chart</guid>
      <pubDate>Fri, 08 Sep 2023 17:36:00 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何正确使用 torch.compile？</title>
      <link>https://stackoverflow.com/questions/75886125/how-should-i-use-torch-compile-properly</link>
      <description><![CDATA[我目前正在尝试使用 pytorch 2.0 来提升我的项目的训练性能。我听说 torch.compile 可能会提升一些模型的性能。
所以我的问题（目前）很简单；我应该如何使用带有大型模型的 torch.compile？
例如，我应该像这样使用 torch.model 吗？
class BigModel(nn.Module):
def __init__(self, ...):
super(BigModel, self).__init__()
self.model = nn.Sequential(
SmallBlock(), 
SmallBlock(), 
SmallBlock(), 
...
)
...

class SmallBlock(nn.Module):
def __init__(self, ...):
super(SmallBlock, self).__init__()
self.model = nn.Sequential(
...some small model...
)

model = BigModel()
model_opt = torch.compile(model)

，或者像这样？
class BigModel(nn.Module):
def __init__(self, ...):
super(BigModel, self).__init__()
self.model = nn.Sequential(
SmallBlock(), 
SmallBlock(), 
SmallBlock(), 
...
)
...

class SmallBlock(nn.Module):
def __init__(self, ...):
super(SmallBlock, self).__init__()
self.model = nn.Sequential(
...一些小模型...
)
self.model = torch.compile(self.model)

model = BigModel()
model_opt = torch.compile(model)

总结一下，

应该编译每一层吗？或者 torch.compile 会自动执行此操作？
有没有什么关于正确使用 torch.compile 的技巧？

说实话，我都试过了，但没有什么区别。
而且，它并没有显著加速，我只是检查了我的模型的加速率大约为 5 ~ 10%。]]></description>
      <guid>https://stackoverflow.com/questions/75886125/how-should-i-use-torch-compile-properly</guid>
      <pubDate>Thu, 30 Mar 2023 08:59:07 GMT</pubDate>
    </item>
    <item>
      <title>决策树回归模型获取准确率最高的模型的max_depth值</title>
      <link>https://stackoverflow.com/questions/63922690/decision-tree-regressor-model-get-max-depth-value-of-the-model-with-highest-accu</link>
      <description><![CDATA[使用默认参数，从 X_train 集和 Y_train 标签构建决策树回归模型。将模型命名为dt_reg。
在训练数据集上评估模型准确率并打印其分数。
在测试数据集上评估模型准确率并打印其分数。
预测X_test集前两个样本的房价并打印出来。（提示：使用predict()函数）
在X_train数据和Y_train标签上拟合多个决策树回归器，max_depth参数值从2变为5。
在测试数据集上评估每个模型的准确率。
提示：使用for循环

打印准确率最高的模型的max_depth值。

import sklearn.datasets as datasets
from sklearn.model_selection import train_test_split 
from sklearn.tree import DecisionTreeRegressor
import numpy as np
np.random.seed(100) 
boston = datasets.load_boston()
X_train, X_test, Y_train, Y_test = train_test_split(boston.data, boston.target, random_state=30)
print(X_train.shape)
print(X_test.shape)

dt_reg = DecisionTreeRegressor() 
dt_reg = dt_reg.fit(X_train, Y_train) 
print(dt_reg.score(X_train,Y_train))
print(dt_reg.score(X_test,Y_test))
y_pred=dt_reg.predict(X_test[:2])
print(y_pred)

我想要得到打印具有最高值的模型的max_depth值准确率。但是 fresco 播放未提交，请告诉我错误是什么。
max_reg = None
max_score = 0 
t=()
for m in range(2, 6) :
rf_reg = DecisionTreeRegressor(max_depth=m)
rf_reg = rf_reg.fit(X_train, Y_train) 
rf_reg_score = rf_reg.score(X_test,Y_test)
print (m, rf_reg_score ,max_score) 
if rf_reg_score &gt; max_score :
max_score = rf_reg_score
max_reg = rf_reg
t = (m,max_score) 
print (t)
]]></description>
      <guid>https://stackoverflow.com/questions/63922690/decision-tree-regressor-model-get-max-depth-value-of-the-model-with-highest-accu</guid>
      <pubDate>Wed, 16 Sep 2020 14:53:20 GMT</pubDate>
    </item>
    </channel>
</rss>