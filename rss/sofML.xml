<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 18 May 2024 09:14:51 GMT</lastBuildDate>
    <item>
      <title>计算错误的神经网络比正确计算的神经网络更好</title>
      <link>https://stackoverflow.com/questions/78497893/neural-network-with-incorrect-calculation-better-than-correct-one</link>
      <description><![CDATA[我设计了自己的神经网络并发现了一个错误。在反向传播期间，我没有将 Z 值插入到激活函数的导数中，而是插入了 A 值。结果是，当我使用 A 值时，神经网络比使用 Z 值进行计算学习得更快、更稳定。计算应该是不正确的。那么为什么它效果更好，产生更好的结果和更稳定的结果呢？
Z=x×w+b A=激活函数(Z)
错误的计算却有更好的结果：
dA/dZ=activationfunction_derivative(A)
delta = deltas[-1].dot(self.weights[i].T) * self.leaky_relu_derivative(self.activations[i])
正确的计算，但结果更差：
dA/dZ=activationfunction_derivative(Z)
错误的代码具有更好的结果：
defleaky_relu(self, x, alpha=0.01):
    返回 np.where(x &gt; 0, x, alpha * x)

defleaky_relu_derivative(self, x, alpha=0.01):
    返回 np.where(x &gt; 0, 1, alpha)
def 线性（自身，x）：
    返回x

def 线性导数（自身）：
    返回1

defforward_propagation(self, X):
    自我激活 = []
    激活=X
    self.activations.append(激活)
    对于权重，zip中的偏差(self.weights[:-1], self.biases[:-1])：
        激活 = self.leaky_relu(np.dot(激活, 权重) + 偏差)
        self.activations.append(激活)
    激活 = self.线性(np.dot(激活, self.weights[-1]) + self.biases[-1])
    self.activations.append(激活)
    返回激活

def msnq(self, y_true, y_pred):
    返回 np.mean(np.square(y_true - y_pred))

def 反向传播(self, y_true, t):
    增量 = []
    错误= y_true - self.activations[-1]
    delta = 误差 * self.linear_derivative()
    deltas.append(delta)
    对于范围内的 i(len(self.activations) - 2, 0, -1)：
        delta = deltas[-1].dot(self.weights[i].T) * self.leaky_relu_derivative(self.activations[i])
        deltas.append(delta)
]]></description>
      <guid>https://stackoverflow.com/questions/78497893/neural-network-with-incorrect-calculation-better-than-correct-one</guid>
      <pubDate>Fri, 17 May 2024 20:48:33 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 - 如何将数据清理纳入训练模型中</title>
      <link>https://stackoverflow.com/questions/78497891/machine-learning-how-to-incorporate-data-cleansing-into-trained-model</link>
      <description><![CDATA[我一直在尝试寻找这个问题的答案，但还没有找到任何与之相关的东西。
问题是，如果我清理数据并将中值归结为 NaN 值，我是否应该以某种方式将其合并到将用于测试数据的模型中。换句话说，我的测试数据是否也需要清理和归结，或者训练会处理这个问题。我想说它需要被纳入，因为否则 NaN 值会破坏模型，而且任何偏斜都不会得到解决。
特别是：
用中位数替换 NaN：
data = data.fillna(data.median())

使用分位数变换处理偏斜，以遵循每个特征的正态分布（以下仅针对其中之一）。
qualtile_transformer = QuantileTransformer(output_distribution=&#39;normal&#39;, random_state=0&#39;)
data[&#39;feat_0&#39;] = quantile_transformer.fit_transform(data[&#39;feat_0&#39;].values.reshape(-1,1)).flatten()

模型：
from sklearn.linear_model import LinearRegression
linear_regr = LinearRegression()
linear_regr.fit(Xtrain,Ytrain)

预测：
# 使用测试集进行预测
Ypred = linear_regr.predict(Xtest)

谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78497891/machine-learning-how-to-incorporate-data-cleansing-into-trained-model</guid>
      <pubDate>Fri, 17 May 2024 20:47:39 GMT</pubDate>
    </item>
    <item>
      <title>在 python 中为我的标签应用程序处理数据库锁</title>
      <link>https://stackoverflow.com/questions/78497797/handle-database-locks-in-python-for-my-labeling-app</link>
      <description><![CDATA[我希望我的应用程序的用户检索一些要标记的数据。在第一个版本中，我没有实现锁定，因此多个用户可以同时访问相同的数据，因此第二个版本会覆盖第一个版本的标签。
我正在使用 python fastAPI sqlite 后端。
我最初想出了为标签添加“is-being-labelized”值的想法，以便下一个提议的数据不一样。我不喜欢它，因为我不知道如何处理用户在没有标记数据的情况下退出应用程序（或其他应用程序）的情况。目前，我最好的方法是添加一个包含检索时间时间戳的列，并实现一个逻辑，假设检索后 30 秒，我们检查标签是否不再是 None。如果它仍然是 None （意味着该人没有标记），我们删除时间戳的值。我也不完全高兴，因为它不能处理人们冥想然后回来标记数据的情况。
您还有更好的建议吗？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78497797/handle-database-locks-in-python-for-my-labeling-app</guid>
      <pubDate>Fri, 17 May 2024 20:19:31 GMT</pubDate>
    </item>
    <item>
      <title>我用自己的数据集训练yolo模型但没有测试结果</title>
      <link>https://stackoverflow.com/questions/78497575/i-train-yolo-model-with-my-own-data-set-but-there-is-no-test-result</link>
      <description><![CDATA[我正在使用 Yolov3 模型以及从 Kaggle 收到的数据集来训练模型。模型训练已完成，我将新权重添加到备份文件夹中。我运行了我训练过的一种水果进行测试，但没有发生对象检测。同一图像显示为 Prediction.jpg。训练看起来不错，但我不明白为什么它不能检测物体。请帮助我。
火车站代码：
./darknet探测器列车 /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/Desktop/Projects/Bitirmeprojesi/yolov3.weights

测试终端代码：
./darknet探测器测试 /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/darknet/backup/yolov3_final.weights -thresh 0.25 -out预测.jpg

我设置并编辑了 obj.data、obj.names 和 yolov3.cfg 文件。
我有 3 个类别：苹果、香蕉和橙子。我已经根据3个类在cfg文件中正确设置了filter和class值等值。
cfg 文件
[网]
# 测试
批次=64
细分=1
＃ 训练
细分=16
宽度= 608
高度=608
通道=3
动量=0.9
衰减=0.0005
角度=0
饱和度=1.5
曝光=1.5
色调=0.3

学习率=0.001
烧入=1000
max_batches = 6000 # 类数 * 2000
政策=步骤
步骤=3600,4800 # max_batches num %80, %90
尺度=.1,.1

数据集中除了.jpg图片外，还有yolo格式的同名.txt文件。
在此处输入图像描述文件图像
包含所有图像路径的 train.txt 和 test.txt 文件也已准备就绪。
当我在终端中运行测试命令时，它可以工作，但图片看起来相同，没有检测对象的边界框。我确定我已经安装了 Opencv。我正在使用 macOS。为什么它没有检测到它？请有人帮忙。我多次通过 make clean 清理暗网，并通过 make opencv = 1 运行它，但结果没有改变。
[yolo]参数：iou损失：mse（2），iou_norm：0.75，obj_norm：1.00，cls_norm：1.00，delta_norm：1.00，scale_x_y：1.00
总 BFLOPS 137.613
平均输出 = 1052318
正在从 /Users/melisabagcivan/darknet/backup/yolov3_final.weights 加载权重...
 看过 64 个，训练过：32013 个 K 图像（500 Kilo-batches_64）
完毕！从权重文件加载 107 层
输入图像路径：/Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg
 检测层：82-类型=28
 检测层数：94-型=28
 检测层：106-类型=28
/Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg：预测为 6738.129000 毫秒。

我尝试了很多图像，但它没有在任何图像中绘制方框。我不明白是它无法检测到它还是我在测试时犯了错误。请帮忙，我快疯了。]]></description>
      <guid>https://stackoverflow.com/questions/78497575/i-train-yolo-model-with-my-own-data-set-but-there-is-no-test-result</guid>
      <pubDate>Fri, 17 May 2024 19:21:32 GMT</pubDate>
    </item>
    <item>
      <title>从 Orange 导出的模型在 Orange 中运行良好，但在 Python 中运行不佳</title>
      <link>https://stackoverflow.com/questions/78497427/model-exported-from-orange-works-well-in-orange-but-not-in-python</link>
      <description><![CDATA[我在 Orange 中训练了一个机器学习模型，可以非常准确地对狗和猫进行分类。但是，当我将模型导出到 pickle 文件并将其加载到 Python 中时，无论输入数据如何，它都会一致地预测“cat”。
这是我在 Python 中编写的：
import pickle
from PIL import Image
import numpy as np

modello = &#39;modelli/catDogsLogisticRegression.pkcls&#39;

def load_model_from_pickle(modello):
try:
with open(modello, &#39;rb&#39;) as file_pickle:
model = pickle.load(file_pickle)
return model
except FileNotFoundError:
print(f&quot;File {modello} non trovato.&quot;)
return None

def preprocess_image(image_path):
# 加载图片
img = Image.open(image_path)
# 调整图片尺寸并将其转换为灰色版本
img = img.resize((32, 64)).convert(&#39;L&#39;)
# 将图片转换为 numpy 数组，并将维度转换为单个向量
img_array = np.array(img).reshape(1, -1)
return img_array

# 示例
loaded_model = load_model_from_pickle(modello)
if loaded_model:
print(&quot;模型成功加载&quot;)
# 示例
# 加载和预处理图片
image_path = &#39;cane.jpg&#39;
new_data = preprocess_image(image_path)
# 预测新示例的类别
predict_class = loaded_model.predict(new_data)[0]
print(&quot;预测类别:&quot;, &#39;Gatto&#39; if predict_class == 0 else &#39;Cane&#39;)
else:
print(&quot;Errore nel caricamento del modello.&quot;)

知道为什么模型在 Python 中的行为不同吗？我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78497427/model-exported-from-orange-works-well-in-orange-but-not-in-python</guid>
      <pubDate>Fri, 17 May 2024 18:43:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用决策树算法和bert算法对文本进行分类</title>
      <link>https://stackoverflow.com/questions/78497176/how-to-use-decision-tree-algorithm-with-bert-algorithm-to-classify-a-text</link>
      <description><![CDATA[我想集成并使用 BERT 和决策树两种算法进行文本分类，因此我需要该领域的指导和帮助。
如果有人有这个领域的源代码或文章，请提供给我。或者即使朋友有更好的建议将 BERT 算法与任何其他算法结合起来]]></description>
      <guid>https://stackoverflow.com/questions/78497176/how-to-use-decision-tree-algorithm-with-bert-algorithm-to-classify-a-text</guid>
      <pubDate>Fri, 17 May 2024 17:44:40 GMT</pubDate>
    </item>
    <item>
      <title>如何在流模式下分割拥抱脸部数据集而不将其加载到内存中？</title>
      <link>https://stackoverflow.com/questions/78497069/how-to-split-a-hugging-face-dataset-in-streaming-mode-without-loading-it-into-me</link>
      <description><![CDATA[我正在处理 Hugging Face 数据集，需要将数据集分成训练集和验证集。我的主要要求是数据集应该以流式模式处理，因为我不想将整个数据集加载到内存中。
from datasets import load_dataset, DatasetDict

# 从 Hugging Face 加载数据集
dataset = load_dataset(&#39;squad&#39;, split=&#39;train&#39;)

# 将数据集拆分为训练集和验证集
# 指定测试集（验证集）的分数
train_val_split = dataset.train_test_split(test_size=0.1)

# 提取训练和验证数据集
train_dataset = train_val_split[&#39;train&#39;]
val_dataset = train_val_split[&#39;test&#39;]

# 打印数据集的大小
print(f&quot;训练集大小：{len(train_dataset)}&quot;)
print(f&quot;验证集大小：{len(val_dataset)}&quot;)

# 如果需要，保存数据集
# train_dataset.save_to_disk(&#39;path/to/train_dataset&#39;)
# val_dataset.save_to_disk(&#39;path/to/val_dataset&#39;)

有没有办法在流模式下分割 Hugging Face 数据集？任何对我的代码的建议或改进都将不胜感激。
参考文献：

https://discuss.huggingface.co/t/how-to-split-a-dataset-into-train-test-and-validation/1238
https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090/21
https://discuss.huggingface.co/t/possible-to-stream-and-create-new-splits/67214
https://huggingface.co/docs/datasets/v1.11.0/splits.html
https://discuss.huggingface.co/t/how-to-split-a-hugging-face-dataset-in-streaming-mode-without-loading-it-into-memory/87205
]]></description>
      <guid>https://stackoverflow.com/questions/78497069/how-to-split-a-hugging-face-dataset-in-streaming-mode-without-loading-it-into-me</guid>
      <pubDate>Fri, 17 May 2024 17:18:18 GMT</pubDate>
    </item>
    <item>
      <title>学习率不更新</title>
      <link>https://stackoverflow.com/questions/78496983/learning-rate-not-updating</link>
      <description><![CDATA[def make_prediction(x0,t0):
    输入 = torch.vstack([x0,t0])
    Layer_1 = torch.matmul(w0,输入)
    返回层_1

损失1 = nn.MSELoss()
def loss_function():
            u_t=(make_prediction(x,t+inf_s)-make_prediction(x,t))/inf_s
            u_x=(make_prediction(x+inf_s,t)-make_prediction(x,t))/inf_s
            u_xx=(make_prediction(x+inf_s,t)-2*make_prediction(x,t)+make_prediction(x-inf_s,t))/inf_s**2
            返回 (1/N_i)*(loss1(make_prediction(x0IC,t0IC), u0IC))+(1/N_b)*(loss1(make_prediction(x0BC1,t0BC1), u0BC1))
            +(1/N_b)*(loss1(make_prediction(x0BC2,t0BC2), u0BC2))+(1/N_f)*(np.pi/0.01)*(loss1(u_xx-u_t-make_prediction(x,t)*u_x , 0))

def train_step(w,b, 学习率):
    可训练变量 = [w,b]
    优化器 = torch.optim.SGD(trainable_variables, lr=learning_rate,momentum=0.9)
    调度程序 = torch.optim.lr_scheduler.ExponentialLR(优化器, gamma=0.01)
    损失 = loss_function()
    loss.backward()
    使用 torch.no_grad()：
        w -= 学习率 * w.grad
        b -= 学习率 * b.grad
        w.grad.zero_()
        b.grad.zero_()
    优化器.step()
    调度程序.step()
train_step(w,偏差,学习率)

我运行此代码（通过scheduler.ExponentialLR），但学习率没有变化。
您认为问题从何而来？
我写了完整的代码...感谢您的帮助]]></description>
      <guid>https://stackoverflow.com/questions/78496983/learning-rate-not-updating</guid>
      <pubDate>Fri, 17 May 2024 16:56:52 GMT</pubDate>
    </item>
    <item>
      <title>尝试在自定义 autograd 函数中的模型上调用 autograd.grad，在初始化时有效，但在设置权重/偏差时无效</title>
      <link>https://stackoverflow.com/questions/78496967/trying-to-call-autograd-grad-on-a-model-inside-of-a-custom-autograd-function-wo</link>
      <description><![CDATA[class BernoulliApproximator(nn.Module):
def __init__(self, hidden_​​dim):
super().__init__()
self.linear1 = nn.Linear(2, hidden_​​dim)
self.linear2 = nn.Linear(hidden_​​dim, hidden_​​dim)
self.linear3 = nn.Linear(hidden_​​dim, hidden_​​dim)
self.linear4 = nn.Linear(hidden_​​dim, 1)
self.relu = nn.ReLU()

def forward(self, x):
out = self.relu(self.linear1(x))
out = self.relu(self.linear2(out))
out = self.relu(self.linear3(out))
out = torch.sigmoid(self.linear4(out))
return out

model = torch.load(&#39;bernoullimodel9.pth&#39;,map_location=device)

class BernoulliSampleFunction(torch.autograd.Function):
@staticmethod
def forward(ctx, probabilities, random_numbers):
result = torch.zeros_like(probabilities)
输入 = []
输出 = []
for i in range(probabilities.shape[1]):
使用 torch.enable_grad():
输入 = torch.cat((probabilities[:, i].unsqueeze(1).double(), random_numbers[:, i].unsqueeze(1).double()), dim=1).clone().requires_grad_(True)
输入.append(input)
输出 = model(input)
输出.append(output)
result[:, i] = output.squeeze().detach()
输入长度 = torch.tensor(len(输入))
输入.extend(输出)
ctx._dict = model.state_dict()
ctx.save_for_backward(输入长度，*输入)
返回结果

@staticmethod
def behind(ctx, grad_output):
print(&quot;grad_output: &quot;,grad_output)
输入长度，*输入 = ctx.saved_tensors
输出 = 输入[输入长度：]
输入 = 输入[：输入长度]
toReturn = torch.zeros_like(grad_output)
toReturn2 = torch.zeros_like(grad_output)
torch.set_grad_enabled(True)
使用 torch.enable_grad():
模型 = BernoulliApproximator(32)
# model.load_state_dict(ctx._dict)
for param in model.parameters():
print(&quot;PARAM 1: &quot;,param)
for i in range(toReturn.shape[1]):
input = input[i].float()
print(&quot;input: &quot;,input)
output = model(input)
print(&quot;output: &quot;,output)
delta, *g_pars = autograd.grad(output, [input] + list(model.parameters()), grad_output[:,i].unsqueeze(1).requires_grad_(),allow_unused=True)
print(&quot;delta: &quot;,delta)
toReturn[:,i] = delta[:,0]
toReturn2[:,i] = delta[:,1]
print(&quot;toReturn: &quot;,toReturn)
return toReturn, toReturn2

尝试通过在模型内部重新创建模型来获取相对于模型输入的梯度torch.enable_grad()，向其提供相同的输入，然后调用 autograd.grad。autograd.grad 仅在模型已初始化时才能正常工作，但如果我对其权重/偏差的值进行任何更改，则无论如何梯度都是 0。我尝试了将训练模型的权重和偏差复制到新实例的每种方法；.copy_() 与 torch.no_grad()，param.data = savedParamTensor。如果我直接在其上调用 .copy_()，它会导致就地修改错误。在保存的输出上调用 autograd.grad 也会产生相同的结果。
autograd.grad 仅在使用新初始化的模型时起作用，但无法更改权重并更新计算图。包括如果您在 .init 调用中更改它们。]]></description>
      <guid>https://stackoverflow.com/questions/78496967/trying-to-call-autograd-grad-on-a-model-inside-of-a-custom-autograd-function-wo</guid>
      <pubDate>Fri, 17 May 2024 16:52:11 GMT</pubDate>
    </item>
    <item>
      <title>我的逻辑回归机器学习模型有问题</title>
      <link>https://stackoverflow.com/questions/78493918/i-am-having-problem-with-my-logistics-regression-machine-learning-model</link>
      <description><![CDATA[我的模型精度很差。此数据取自 https://archive.ics.uci .edu/dataset/15/breast+cancer+wisconsin+original 显示逻辑回归模型的准确度为 96%，所以问题确实出在我的模型中。我在 R 中构建了以下模型。
# 导入数据集
tumor_study &lt;- read.csv(“breast-cancer-wisconsin.data”, header = FALSE, na.strings = “NA”)

# 添加列名
特征&lt;-c(“id_number”，“ClumpThickness”，“Uniformity_CellSize”，
              “Uniformity_CellShape”、“边缘粘附”、
              “SingleEpithelial_CellSize”、“BareNuclei”、“Bland_Chromatin”、
              “Normal_Nucleoli”、“Mitoses”、“Class”）

colnames(tumor_study) &lt;- 特征

# 清洗数据
# 删除第一列（id_number）
肿瘤研究 &lt;- 肿瘤研究[,-1]

# 转换“?” BareNuclei 列中为 NA，然后为数字
tumor_study$BareNuclei[tumor_study$BareNuclei == &quot;?&quot;] &lt;- NA
tumor_study$BareNuclei &lt;- as.numeric(tumor_study$BareNuclei)

# 删除 BareNuclei 中缺失值的行
tumor_study &lt;-tumor_study[!is.na(tumor_study$BareNuclei),]

# 将类转换为因子
tumor_study$Class &lt;- 因子(tumor_study$Class, level = c(2, 4), labels = c(“良性”, “恶性”))

# 将数据集分为训练集和测试集
库（caTools）
设置.种子(123)
split &lt;-sample.split(tumor_study$Class, SplitRatio = 0.8)
Training_set &lt;-tumor_study[split == TRUE,]
test_set &lt;-tumor_study[split == FALSE,]

# 应用特征缩放
训练集[, 1:9] &lt;- 比例(训练集[, 1:9])
test_set[, 1:9] &lt;- 比例(test_set[, 1:9])

# 构建逻辑回归模型
分类器 &lt;- glm(公式 = Class ~ ., family = 二项式, data = Training_set)

# 预测训练集的概率
prob_y_train &lt;- 预测（分类器，类型 = &#39;响应&#39;，newdata = Training_set[,-10]）
Predicted_y_training &lt;- ifelse(prob_y_train &gt;= 0.5,“良性”,“恶性”)

# 使用 test_set 进行预测
prob_y_test &lt;- 预测（分类器，类型 = &#39;响应&#39;，newdata = test_set[,-10]）
Predicted_y_test &lt;- ifelse(prob_y_test &gt;= 0.5,“良性”,“恶性”)

# 使用混淆矩阵检查准确性
cm_test &lt;- 表(test_set[,10], Predicted_y_test)
打印（厘米_测试）

## 如果你检查准确度...它接近 2%

如何找出模型中的问题？]]></description>
      <guid>https://stackoverflow.com/questions/78493918/i-am-having-problem-with-my-logistics-regression-machine-learning-model</guid>
      <pubDate>Fri, 17 May 2024 06:43:30 GMT</pubDate>
    </item>
    <item>
      <title>具有 n 维卫星图像的多类 UNet</title>
      <link>https://stackoverflow.com/questions/78491587/multiclass-unet-with-n-dimensional-satellite-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78491587/multiclass-unet-with-n-dimensional-satellite-images</guid>
      <pubDate>Thu, 16 May 2024 17:28:46 GMT</pubDate>
    </item>
    <item>
      <title>我应该在 GitHub 上分享我的自我项目吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78484980/should-i-share-my-self-projects-on-github</link>
      <description><![CDATA[我一直在考虑是否应该在 GitHub 上分享我自己的项目。这些项目主要涉及机器学习，重点关注回归和分类任务。虽然我已经付出了努力，但我不确定是否值得清理代码并上传它。展示这些小项目实际上是否有益，或者最终只是浪费时间？]]></description>
      <guid>https://stackoverflow.com/questions/78484980/should-i-share-my-self-projects-on-github</guid>
      <pubDate>Wed, 15 May 2024 15:26:31 GMT</pubDate>
    </item>
    <item>
      <title>Kmeans 算法的特征缩放</title>
      <link>https://stackoverflow.com/questions/57507584/feature-scaling-for-kmeans-algorithm</link>
      <description><![CDATA[我知道下定义的 KMeans 算法需要特征缩放
sklearn.cluster.KMeans
我的问题是，在使用 KMeans 之前是否需要手动完成，或者 KMeans 会自动执行特征缩放？如果是自动的，请告诉我它在 KMeans 算法中指定的位置，因为我无法在此处的文档中找到它：
https://scikit-learn.org/stable /modules/ generated/sklearn.cluster.KMeans.html
顺便说一句，人们说 Kmeans 本身负责特征缩放。]]></description>
      <guid>https://stackoverflow.com/questions/57507584/feature-scaling-for-kmeans-algorithm</guid>
      <pubDate>Thu, 15 Aug 2019 09:27:46 GMT</pubDate>
    </item>
    <item>
      <title>F1 分数 vs ROC AUC</title>
      <link>https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc</link>
      <description><![CDATA[我有以下 2 个不同案例的 F1 和 AUC 分数

&lt;块引用&gt;
  模型 1：精度：85.11 召回率：99.04 F1：91.55 AUC：69.94
模型 2：精度：85.1 召回率：98.73 F1：91.41 AUC：71.69

我的问题的主要动机是正确预测正例，即减少假负例（FN）。我应该使用 F1 分数并选择模型 1 还是使用 AUC 并选择模型 2。谢谢 ]]></description>
      <guid>https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc</guid>
      <pubDate>Thu, 25 May 2017 04:14:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 NLP 和 python 从文档中提取特定内容，例如姓名或出生日期？ [关闭]</title>
      <link>https://stackoverflow.com/questions/37967282/how-to-extract-specific-content-like-name-or-dob-from-a-document-using-nlp-and</link>
      <description><![CDATA[我想从文档（例如简历）中提取非常具体的内容，如姓名、地址和出生日期。假设我有 1000 份这样的文档，我想使用机器学习和自然语言处理来自动化它。最好是 Python。
我该怎么做？或者我从哪里开始？
更新：我知道 NER，但我希望从文档中提取非常具体的信息，这些信息可以加载到 excel 或其他东西中。
示例：从项目报告中，我想提取项目的主题、团队成员姓名和任期。]]></description>
      <guid>https://stackoverflow.com/questions/37967282/how-to-extract-specific-content-like-name-or-dob-from-a-document-using-nlp-and</guid>
      <pubDate>Wed, 22 Jun 2016 11:52:43 GMT</pubDate>
    </item>
    </channel>
</rss>