<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 07 Mar 2024 09:12:39 GMT</lastBuildDate>
    <item>
      <title>sklearn.multiclass.OneVsRestClassifier 中的回调</title>
      <link>https://stackoverflow.com/questions/78119978/callbacks-in-sklearn-multiclass-onevsrestclassifier</link>
      <description><![CDATA[我想使用回调和 eval_set 等。
但我有一个问题：
from sklearn.multiclass import OneVsRestClassifier
导入lightgbm

&lt;前&gt;&lt;代码&gt;详细 = 100
参数 = {
    “目标”：“二元”，
    “n_估计器”：500，
    “详细”：0
}
适合参数= {
    “eval_set”：eval_数据集，
    “回调”：[CustomCallback（详细）]
}

clf = OneVsRestClassifier(lightgbm.LGBMClassifier(**params))
clf.fit(X_train, y_train, **fit_params)

我如何将 fit_params 交给我的估算器？
&lt;小时/&gt;
---&gt; 13 clf.fit(X_train, y_train, **fit_params)
类型错误：OneVsRestClassifier.fit() 获得意外的关键字参数“eval_set”]]></description>
      <guid>https://stackoverflow.com/questions/78119978/callbacks-in-sklearn-multiclass-onevsrestclassifier</guid>
      <pubDate>Thu, 07 Mar 2024 08:59:29 GMT</pubDate>
    </item>
    <item>
      <title>使用目标列识别机器学习问题</title>
      <link>https://stackoverflow.com/questions/78119976/identify-machine-learning-problem-using-target-column</link>
      <description><![CDATA[我正在解决一个问题，该问题使用给定的输入数据和选择的目标列自动定义预测类型。
现在的问题是如何区分目标列是多类分类还是二类分类或回归？
我尝试阅读来自 DSML 平台的各种实现的博客，但需要一些清晰度。]]></description>
      <guid>https://stackoverflow.com/questions/78119976/identify-machine-learning-problem-using-target-column</guid>
      <pubDate>Thu, 07 Mar 2024 08:59:26 GMT</pubDate>
    </item>
    <item>
      <title>如何用奇数样本大小批量训练神经网络？</title>
      <link>https://stackoverflow.com/questions/78119974/how-to-train-nn-in-batches-with-odd-examples-size</link>
      <description><![CDATA[我是神经网络领域的新手，正在使用 pytorch 进行一些训练。
我决定做一个简单的普通神经网络。
我使用了一个包含 2377 个数字特征和 6277 个示例的个人数据集。
我的第一次尝试是让神经网络预测每个示例，因此伪代码如下所示
对于范围内的 i(...)：
    X = ... # 特征
    y = ... # 结果
    y_pred = 模型(X[i])
    损失=标准(y_pred, y)

    y_pred.size # [1,1]
    y.尺寸#[1,1]

每个时期大约需要 10 秒，我决定使用小批量来改进它。
所以我在开始时定义了批量大小，Pytorch 中的神经网络是这样定义的
&lt;前&gt;&lt;代码&gt;batch_size = 30
n_inputs = X.size[1] #2377

## 2 个隐藏层
模型 = nn.Sequential(
    nn.Linear(n_inputs, 1024),
    ReLU(),
    nn.线性(1024, 512),
    ReLU(),
    nn.线性(512, 356),
    ReLU(),
    nn.Linear(356,batch_size),
    ReLU(),
）

然后我分批进行训练
对于范围（5）内的纪元：
    总损失 = 0
    排列 = torch.randperm(X.size()[0])
    对于范围内的 i（0，X.size（）[0]，batch_size）：
        优化器.zero_grad()
        索引 = 排列[i:i+batch_size]
        batch_x, batch_y = x[索引], y[索引]

        ypred = 模型(batch_x)
        损失=标准(ypred,batch_y)
        总损失 += loss.item()
        
        ## 更新权重
        loss.backward()
        优化器.step()

现在的问题是我的神经网络总是输出 100 个值但最后的批量大小可能会有所不同。
事实上，如果我选择 100 作为批量大小，最后一批将由 77 个示例组成 (6277%100)。
我确信有一种方法可以解决这个问题，并且我的结构中有一个错误，但我看不到它。
您能帮助我概括批量训练以处理任意数量的示例和批量大小吗？]]></description>
      <guid>https://stackoverflow.com/questions/78119974/how-to-train-nn-in-batches-with-odd-examples-size</guid>
      <pubDate>Thu, 07 Mar 2024 08:58:57 GMT</pubDate>
    </item>
    <item>
      <title>高级机器学习算法？</title>
      <link>https://stackoverflow.com/questions/78119578/advanced-ml-algorithms</link>
      <description><![CDATA[我学习了所有机器学习算法，包括回归、分类和聚类，例如线性回归、物流、决策树、随机森林、SVM、XGBoost、AdaBoost、LightBoost、KNN、KMeans、KMeans+、DBSCAN、分层聚类、Mini- Batch KMeans 等也。
和降维技术。
现在，我的问题是：我应该学习哪些更高级的技术？]]></description>
      <guid>https://stackoverflow.com/questions/78119578/advanced-ml-algorithms</guid>
      <pubDate>Thu, 07 Mar 2024 07:46:25 GMT</pubDate>
    </item>
    <item>
      <title>如何将 BERT 输出转换回 token ID？</title>
      <link>https://stackoverflow.com/questions/78119353/how-to-transform-bert-output-back-into-token-ids</link>
      <description><![CDATA[我试图通过加载一个简单的 BERT 模型、输入一些文本并获取输出来学习 .NET 中机器学习的基础知识。很简单，对吧？
不。
该模型似乎输出两列“onnx::Gather_1269”和“onnx::Gather_1269”。和“1272”。无论哪种情况，输出都是 FLOATS 数组？？？！？！ ...并且它们的负载有负值??????!!!!!
什么？我到底应该如何将这些数据恢复到要取消标记的 tokenID 的 int[] 中？
我觉得我在某个地方错过了整个步骤。救命！
使用 BERTTokenizers；
使用 Microsoft.ML；
使用 Microsoft.ML.Data；

命名空间 TFLibrary；

公共静态类沙箱
{
    公共静态无效DoThing（字符串提示）
    {
        MLContext mlContext = new();
        var pipeline = mlContext.Transforms
            .ApplyOnnxModel(modelFile: @&quot;Models\bert_Opset18.onnx&quot;,
                shapeDictionary: new Dictionary;
                {
                    { “input_ids”, new[] { 1, 128 } },
                    { “attention_mask”, new[] { 1, 128 } },
                    { “onnx::Gather_1269”, new[] { 1, 128, 768 } },
                    { “1272”, 新[] { 1, 768 } },
                },
                inputColumnNames: new[] { “input_ids”, “attention_mask” },
                outputColumnNames: new[] { “onnx::Gather_1269”, “1272” },
                GPU设备ID: 0,
                FallbackToCpu: true);

        var model = pipeline.Fit(mlContext.Data.LoadFromEnumerable(new List()));

        var tokenizer = new BertBaseTokenizer();
        var 编码 = tokenizer.Encode(128, 提示);

        var 输入 = 新的 ModelInput()
        {
            InputIds = 编码.Select(t =&gt; t.InputIds).ToArray(),
            AttentionMask = 编码.Select(t =&gt; t.AttentionMask).Select(m =&gt; m / (float)long.MaxValue).ToArray()
        };

        var dataView = mlContext.Data.LoadFromEnumerable(new[] { input });

        var 输出 = model.Transform(dataView);
        列表&lt;浮动&gt; tkFloats = output.GetColumn(“onnx::Gather_1269”).SelectMany(f =&gt; f).ToList();

        int[] tokenIds = { 0 }; //我如何填充这个？模型结果 (tkFloats) 均为浮点数，且它们的载荷均为负值
        //我究竟该如何将其恢复到令牌IDS？
        //为什么输出不是那些ID？

        Console.WriteLine(string.Join(&#39; &#39;, tokenizer.Untokenize(tokenIds.Select(i =&gt; tokenizer.IdToToken(i)).ToList())));
    }

    公共类模型输入
    {
        [矢量类型(1, 128)]
        [列名(“input_ids”)]
        公共长[] InputIds { 获取;放; }

        [矢量类型(1, 128)]
        [列名(“attention_mask”)]
        公共浮动[] AttentionMask { 得到;放; }
    }

    //无法弄清楚我在哪里需要这个，如果我需要的话
    /*公共类模型输出
    {
        [矢量类型(1, 128, 768)]
        [ColumnName(“onnx::Gather_1269”)]
        公共浮动[] Gather_1269 { 得到;放; }

        [矢量类型(1, 768)]
        [列名(“1272”)]
        公共浮动[] _1272 { 得到;放; }
    }*/
}
]]></description>
      <guid>https://stackoverflow.com/questions/78119353/how-to-transform-bert-output-back-into-token-ids</guid>
      <pubDate>Thu, 07 Mar 2024 07:06:18 GMT</pubDate>
    </item>
    <item>
      <title>神经语言模型：出现错误 - ValueError：无法将大小为 380 的数组重塑为形状 (1,1,10)</title>
      <link>https://stackoverflow.com/questions/78119223/neural-language-model-getting-error-valueerror-cannot-reshape-array-of-size</link>
      <description><![CDATA[我正在尝试遵循基于字符的神经语言模型的教程，该模型尝试预测“基于特定单词的序列中的单词”。
按照指示，我已将文本序列生成到文件中，定义语言模型并保存模型以及映射字符（如 *.pkl）。
接下来，为了生成文本（使用保存的映射），将文本编码为整数，并使用 predict_classes() 运行代码以按顺序预测字符。使用以下函数（下面的代码）使用种子文本来预测字符序列。
但是，当我运行脚本时遇到以下错误：
&lt;块引用&gt;
ValueError：无法将大小为 380 的数组重塑为形状 (1,1,10)

这是我收到错误的部分：
defgenerate_seq（模型，映射，seq_length，seed_text，n_chars）：
    in_text = 种子文本
    # 生成固定数量的字符
    对于 _ 在范围内（n_chars）：
        # 将字符编码为整数
        编码 = [in_text 中字符的映射 [字符]]
        # 将序列截断为固定长度
        编码 = pad_sequences([编码], maxlen=seq_length, 截断=&#39;pre&#39;)
        # 一种热编码
        编码= to_categorical（编码，num_classes = len（映射））
        编码 = 编码.reshape(1, 编码.shape[0], 编码.shape[1]) # &lt;--- 错误行
        # 预测字符
        yhat = model.predict_classes（编码，详细=0）
        # 反向映射整数到字符
        输出字符 = &#39;&#39;
        对于 char，mapping.items() 中的索引：
            如果索引 == yhat：
                输出字符 = 字符
                休息
        # 附加到输入
        输入文本 += 输出字符
    返回 in_text

该函数正在被调用：
print(generate_seq(model, mapping, 10, &#39;唱儿子&#39;, 20))

为什么我会收到此错误？]]></description>
      <guid>https://stackoverflow.com/questions/78119223/neural-language-model-getting-error-valueerror-cannot-reshape-array-of-size</guid>
      <pubDate>Thu, 07 Mar 2024 06:41:41 GMT</pubDate>
    </item>
    <item>
      <title>训练有素的多智能体强化学习模型在增加智能体时崩溃</title>
      <link>https://stackoverflow.com/questions/78119078/trained-multi-agent-rl-model-crashes-when-increasing-agents</link>
      <description><![CDATA[我正在 Open AI Gym 中制作一个具有稳定基线的自定义 Boid 植绒环境 3。 
错误1：
我已经创建了环境并针对 3 个 boids 进行了测试，该模型是使用 3 个 boids 制作的，但是当我在不更改任何内容的情况下测试 10 个 boids 时，会出现错误：
错误 1
错误2：
我测试了 boids 的不同初始化位置，但获得了奖励 600200，并且它们按预期移动。
但是，当我为了一致性而使用模型进行重新训练和测试时，它的表现不佳，并且我的剧集奖励大多为负。虽然我什么也没改变。
我的模型是训练有素的模型，是侥幸还是过度拟合？因为它对于不同的职位都有足够的表现。
奖励]]></description>
      <guid>https://stackoverflow.com/questions/78119078/trained-multi-agent-rl-model-crashes-when-increasing-agents</guid>
      <pubDate>Thu, 07 Mar 2024 06:04:41 GMT</pubDate>
    </item>
    <item>
      <title>将数据帧写入功能存储时出现 ConcurrentAppendException</title>
      <link>https://stackoverflow.com/questions/78118939/concurrentappendexception-while-writing-dataframe-to-feature-store</link>
      <description><![CDATA[我正在尝试将 Spark(pyspark) 数据帧写入特征存储
fs = FeatureStoreClient()

    尝试：
        fs.get_table(fs_name)
    除了值错误：
        fs.create_table(
            名称=文件系统名称，
            Primary_keys=pri_key_cols,
            df=df,
            时间戳_键=时间戳_列，
            描述=表描述，
            标签=tags_dict，
        ）
    别的：
        fs.write_table(name=fs_name, mode=“覆盖”, df=df)

此 Databricks 笔记本附加到 ADF 管道，当它运行时有时会出现异常并且管道失败。
ConcurrentAppendException：文件已通过并发更新添加到表的根中。请重试该操作。
冲突的提交：
]]></description>
      <guid>https://stackoverflow.com/questions/78118939/concurrentappendexception-while-writing-dataframe-to-feature-store</guid>
      <pubDate>Thu, 07 Mar 2024 05:23:31 GMT</pubDate>
    </item>
    <item>
      <title>运行带有校准的构建模型后，如何在 H2o Flow 中找到校准概率、绘图、brier 分数？</title>
      <link>https://stackoverflow.com/questions/78118859/how-can-i-find-the-calibration-probabilities-plot-brier-score-in-h2o-flow-afte</link>
      <description><![CDATA[指定在单独的校准集上使用 platt 来校准模型，但我在输出中没有看到任何与校准相关的内容。 H2o Flow 中不存在这个吗？
查看了交叉验证输出和预测输出]]></description>
      <guid>https://stackoverflow.com/questions/78118859/how-can-i-find-the-calibration-probabilities-plot-brier-score-in-h2o-flow-afte</guid>
      <pubDate>Thu, 07 Mar 2024 04:53:57 GMT</pubDate>
    </item>
    <item>
      <title>加载图像边界框输出相同大小的错误</title>
      <link>https://stackoverflow.com/questions/78118283/loading-image-bounding-boxes-outputs-equal-size-error</link>
      <description><![CDATA[这是我在这里发表的第一篇文章，如果我需要更多数据或解释，请告诉我。
我正在尝试为我的数据集创建一个 PyTorch 数据加载器。每个图像都有一定数量的汽车和每个汽车的边界框，但并非所有图像都具有相同数量的边界框。
您可能无法运行它，但这里有一些信息。
这是我的数据加载器
类 AGR_Dataset（数据集）：
    def __init__(self,annotations_root,img_root,transform=None):
        ”“”
        论据：
            annotations_root（字符串）：带有注释的 csv 文件的路径。
            img_root（字符串）：包含所有图像的目录。
            变换（可调用，可选）：要应用的可选变换
                在样品上。
        ”“”
        self.annotations_root = 注释_root
        self.img_root = img_root
        self.transform = 变换

    def __len__(自身):
        返回 len(self.annotations_root)
    
    def __getitem__(self, idx):
        # idx 可能是索引或图像名称，我认为图像 naem
        如果 torch.is_tensor(idx):
            idx = idx.tolist()
        
        idx_name = os.listdir(self.img_root)[idx]
        # 打印(idx_name)
        
        img_name = os.path.join(self.img_root, idx_name)
        Comments_data = os.path.join(self.annotations_root, f&quot;{idx_name.removesuffix(&#39;.jpg&#39;)}.txt&quot;)
        # print(img_name, 注释数据)

        图像 = io.imread(img_name)

        以 open(annotation_data, &#39;r&#39;) 作为文件：
            行= file.readlines()
            图像数据 = []
            img_标签 = []
            对于行中行：
                line = line.split(&#39;,&#39;)
                line = [i.strip() for i in line]
                line = [float(num) for num in line[0].split()]
                img_labels.append(int(行[0]))
                img_data.append(行[1:])

        框 = tv_tensors.BoundingBoxes(img_data, format=&#39;CXCYWH&#39;, canvas_size=(image.shape[0], image.shape[1]))

        # 样本 = {&#39;image&#39;: 图像, &#39;bbox&#39;: 盒子, &#39;labels&#39;: img_labels}
        样本= {&#39;image&#39;：图像，&#39;bbox&#39;：盒子}

        如果自我变换：
            样本 = self.transform(样本)

        打印（样本[&#39;图像&#39;].shape）
        打印（样本[&#39;bbox&#39;].shape）
        # print(样本[&#39;标签&#39;].shape)
        返回样品

我运行转换并创建数据加载器
data_transform = v2.Compose([
    v2.ToImage(),
    # v2.调整大小(680),
    v2.RandomResizedCrop(大小=(680, 680), 抗锯齿=True),
    # v2.ToDtype(torch.float32,scale=True),
    v2.ToTensor()
]）

Transformed_dataset = AGR_Dataset(f&#39;{annotations_path}/test/&#39;,
                        f&#39;{img_path}/测试/&#39;,
                        变换=数据变换）

数据加载器=数据加载器(transformed_dataset,batch_size=2,
                        洗牌=假，num_workers=0）

然后我尝试用它迭代它，并最终使用边界框查看和图像。
对于 i，枚举中的示例（dataloader）：
    打印（我，样本）
    print(i, 样本[&#39;image&#39;].size(), 样本[&#39;bbox&#39;].size())

    如果我==4：
        休息

批处理大小为 1 时，它运行正常，批处理大小为 2 时，出现此错误
torch.Size([3, 680, 680])
火炬.Size([12, 4])

火炬.Size([3, 680, 680])
火炬.Size([259, 4])

RuntimeError: 堆栈期望每个张量大小相等，但在条目 0 处得到 [12, 4]，在条目 1 处得到 [259, 4]


我认为这是由于边界框的数量不相等造成的，但如何克服这个问题？
我的变换中需要 ToTensor 吗？我开始认为我不这样做，因为 v2 使用 ToImage()，并且 ToTensor 正在被贬值。

如有任何其他意见或帮助，我们将不胜感激。
我不确定如何创建一个工作示例，我会继续尝试。
我尝试过的
我尝试通过注释数据加载器中的 tv_tensors.BoundingBoxes 行来不将边界框加载为张量，但由于某种原因，我的调整大小无法正常工作。
我刚刚尝试在数据加载器中像这样分割框和图像
样本 = 图像
    目标= {&#39;bbox&#39;：盒子，&#39;标签&#39;：img_labels}

运气不好]]></description>
      <guid>https://stackoverflow.com/questions/78118283/loading-image-bounding-boxes-outputs-equal-size-error</guid>
      <pubDate>Thu, 07 Mar 2024 01:30:38 GMT</pubDate>
    </item>
    <item>
      <title>TypeError: fit() 缺少 1 个必需的位置参数 Y [关闭]</title>
      <link>https://stackoverflow.com/questions/78118068/typeerror-fit-missing-1-required-positional-argument-y</link>
      <description><![CDATA[代码：
def _forward(self, x):
    self.trained_model = self.model.fit(x)
    返回 self.trained_model.labels_

我在这个项目中使用了很多模型，代码与 minibatchkmeans 和birthch 完美配合，但是在添加 kneighborclassifier 后，它开始显示错误消息，如下所示：
回溯（最近一次调用最后一次）：
  文件“E:\Desktop\customer_analysis\customer_analysis-main\main.py”，第 39 行，位于  中。
    主要的（）
  文件“E:\Desktop\customer_analysis\customer_analysis-main\main.py”，第 15 行，在 main 中
    预测标签 = cluster_model._forward(data_array)
  文件“E:\Desktop\customer_analysis\customer_analysis-main\dnn_module\dnn_model.py”，第 193 行，位于 _forward
    self.trained_model = self.model.fit(x)
类型错误：fit() 缺少 1 个必需的位置参数：&#39;y&#39;

如果我添加更多参数，例如纪元和批量大小，它会显示：
类型错误：fit() 获得意外的关键字参数“batch_size”

如果我添加另一个参数，例如 []：
self.trained_model = self.model.fit(x, [])

它会导致值错误。]]></description>
      <guid>https://stackoverflow.com/questions/78118068/typeerror-fit-missing-1-required-positional-argument-y</guid>
      <pubDate>Thu, 07 Mar 2024 00:06:38 GMT</pubDate>
    </item>
    <item>
      <title>训练预训练 CNN 模型时出现矩阵大小不兼容错误</title>
      <link>https://stackoverflow.com/questions/78116865/matrix-size-incompatible-error-while-training-a-pretrained-cnn-model</link>
      <description><![CDATA[矩阵大小不兼容：In[0]：[32,7776]，In[1]：[18816,512]
[[{{节点顺序_1/activation_1/aiRelu}}]]
[操作：__inference_train_function_3421]

值得注意的是，理解这个错误它可以在更少的数据集和更少的类的情况下顺利运行
# # 建模开始使用 CNN。

模型=顺序（）
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = &#39;相同&#39;,activation =&#39;relu&#39;, input_shape = (224,224,3)))
model.add(MaxPooling2D(pool_size=(2,2)))


model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = &#39;相同&#39;,activation =&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))


model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = &#39;相同&#39;,activation = &#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = &#39;相同&#39;,activation =&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

模型.add(压平())
模型.add（密集（512））
model.add(激活(&#39;relu&#39;))
model.add（密集（5，激活=“softmax”））

文本]]></description>
      <guid>https://stackoverflow.com/questions/78116865/matrix-size-incompatible-error-while-training-a-pretrained-cnn-model</guid>
      <pubDate>Wed, 06 Mar 2024 19:09:37 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的训练损失和验证损失非常低，但新数据集的均方误差却很高？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78116573/why-my-training-loss-and-validation-loss-very-low-but-the-mse-for-a-new-dataset</link>
      <description><![CDATA[来自tensorflow.keras导入层
从tensorflow.keras.layers导入Dense
从tensorflow.keras.layers导入LSTM
从tensorflow.keras.models导入顺序
从tensorflow.keras导入正则化器

模型=顺序（[
       LSTM(单位=64, input_shape=(len(x.columns),1),
       kernel_regularizer=regularizers.l2(0.01)),
       密集(1)
     ]）
model.compile(loss=tf.keras.losses.MeanSquaredError(),optimizer=tf.keras.optimizers.Adam())
历史= model.fit（x_train，y_train，validation_data =（x_val，y_val），epochs = 100，verbose = 1，batch_size = 32）

mse=model.evaluate(x_test, y_test)
打印（毫秒）

这是我得到的损失图：

我得到的新数据集的 MSE 值非常高 (30.2344)。代码中可能存在什么问题。是否过度拟合？建议进行一些更改。]]></description>
      <guid>https://stackoverflow.com/questions/78116573/why-my-training-loss-and-validation-loss-very-low-but-the-mse-for-a-new-dataset</guid>
      <pubDate>Wed, 06 Mar 2024 18:14:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在 ADBench 中集成其他自定义算法？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78008005/how-do-i-integrate-other-custom-algorithms-in-adbench</link>
      <description><![CDATA[我想测试不同的隔离林类型。但是，考虑到拟合/模型和运行文件，我不太明白如何将其集成到 ADB 中。特别是算法的哪一部分应该放入文件中。
有人可以给我建议或解决方案吗？
我尝试使用扩展隔离林https://github.com/ sahandha/eif/blob/master/eif_old.py。为此，我使用旧的（非 cython）版本并将其放入模型中。但是，我不确定要在运行和拟合文件中放入什么内容......]]></description>
      <guid>https://stackoverflow.com/questions/78008005/how-do-i-integrate-other-custom-algorithms-in-adbench</guid>
      <pubDate>Fri, 16 Feb 2024 14:21:16 GMT</pubDate>
    </item>
    <item>
      <title>无法创建 Spark 会话</title>
      <link>https://stackoverflow.com/questions/55971395/unable-to-create-spark-session</link>
      <description><![CDATA[&lt;块引用&gt;
  当我创建 Spark 会话时，它抛出错误


无法创建 Spark 会话
使用pyspark，代码片段：

ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-13-2262882856df&gt;在&lt;模块&gt;()中
     37 如果 __name__ == &quot;__main__&quot;:
     38conf = SparkConf（）
---&gt; 39 sc = SparkContext（conf=conf）
     40 # 打印（sc.版本）
     41 # sc = SparkContext(conf=conf)

〜/anaconda3/lib/python3.5/site-packages/pyspark/context.py 在 __init__(self、master、appName、sparkHome、pyFiles、环境、batchSize、序列化器、conf、网关、jsc、profiler_cls)
    131 “注意此选项将在 Spark 3.0 中删除”）
    132
--&gt; 133 SparkContext._ensure_initialized（自我，网关=网关，conf=conf）
    134 尝试：
    第135章

_ensure_initialized 中的 ~/anaconda3/lib/python3.5/site-packages/pyspark/context.py（cls、实例、网关、conf）
    330 “由 %s 在 %s:%s 创建”
    331 %（当前应用程序名称、当前主控、
--&gt;第332章
    第333章：
    第334章

ValueError：无法同时运行多个 SparkContext；由 __init__ 在 :33 创建的现有 SparkContext(app=pyspark-shell, master=local[*])



进口


&lt;前&gt;&lt;代码&gt;
从 pyspark 导入 SparkConf、SparkContext


我尝试了这种替代方法，但也失败了：

spark = SparkSession(sc).builder.appName(&quot;检测恶意 URL 应用程序&quot;).getOrCreate()

这会引发另一个错误，如下所示：
NameError：名称“SparkSession”未定义
]]></description>
      <guid>https://stackoverflow.com/questions/55971395/unable-to-create-spark-session</guid>
      <pubDate>Fri, 03 May 2019 14:02:42 GMT</pubDate>
    </item>
    </channel>
</rss>