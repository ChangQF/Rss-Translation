<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 24 Feb 2025 18:24:17 GMT</lastBuildDate>
    <item>
      <title>多标签分类任务的自动编码器</title>
      <link>https://stackoverflow.com/questions/79463813/autoencoder-for-multi-label-classification-task</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79463813/autoencoder-for-multi-label-classification-task</guid>
      <pubDate>Mon, 24 Feb 2025 14:17:53 GMT</pubDate>
    </item>
    <item>
      <title>LLM是否在令牌集中包含很少使用的单词或字符？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79463243/do-llms-include-very-rarely-used-words-or-characters-in-the-token-set</link>
      <description><![CDATA[我看到LLM几乎用所有语言给出答案，而且我很少看到使用英语词汇以及很少使用的汉字（我本人作为本地语说话的人甚至都不使用该角色）。
我的问题是：
当模型预测接下来的令牌时，它会计算概率分布。但这是多少个令牌的分布？
该概率分布的维度是多少？
如果它包含许多语言中的所有可能的单词或字符，则数组的长度太大。
如果他们使用相对较小的令牌集，那么这些稀有单词和汉字如何在答案中弹出？从这个意义上讲，考虑到许多语言的词汇和字符的数量，即使是令牌的大小也很小。。
他们用来解决此问题的技术方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79463243/do-llms-include-very-rarely-used-words-or-characters-in-the-token-set</guid>
      <pubDate>Mon, 24 Feb 2025 10:45:19 GMT</pubDate>
    </item>
    <item>
      <title>候选算法算法实施无法正常工作</title>
      <link>https://stackoverflow.com/questions/79463186/candidate-elimination-algorithm-implementation-not-working-as-expected</link>
      <description><![CDATA[我正在尝试实现候选算法，以从数据集中学习概念。但是，我的代码没有产生预期的结果。以下是我的实现和我正在使用的数据集。
 我的代码： 
 将大熊猫作为pd导入
导入numpy作为NP
导入OS，系统

df = pd.read_csv（&#39;chumma.csv; quot;）

val_dict = {
    ＆quot” 0：[[一些，“许多”，“]，”，
    ＆quot“ 1＆quot”：[&#39;small&#39;，&#39;big&#39;，&#39;Medive&#39;]，
    &#39;2&#39;：[&#39;no&#39;]，
    &#39;3&#39;：[“负担得起”，“昂贵”]，
    &#39;4&#39;：[&#39;一个&#39;，&#39;许多&#39;，&#39;少数&#39;]
}

x = df.iloc [：，： -  1]。值
y = df.iloc [：， -  1]。值

s = [[&#39;$&#39;]*5]
对于我的范围（len（y））：
    如果y [i] ==&#39;是&#39;：
        s = x [i] .copy（）
        休息
    
g = [[[＆quot;？]*len（s）]

def is_consistent（x_data，y_data，g_hyp）：
    out_hyp = []
    对于g_hyp中的催眠：
        is_valid = true
        对于我的范围（len（x_data））：
            matches_hyp = true
            对于J范围（Len（Len）（Hyp））：
                如果用演[J]！=;？＆quot;和hyp [j]！= x_data [i] [j]：
                    matches_hyp = false
                    休息
            if（y_data [i] ==&#39;是&#39;而不是匹配的_hyp）或（y_data [i] ==&#39;no&#39;和matches_hyp）：
                is_valid = false
                休息
            
        如果IS_VALID和HYP不在out_hyp中：
            out_hyp.append（hyp）
            
    返回out_hyp
    
    
对于我，实例在枚举（x）中：
    如果y [i] ==&#39;是&#39;：
        对于J范围（LEN（S））的J：
            如果s [j]！=实例[J]：
                s [j] =;？？＆quot;
    别的：
        g_hyp = []
        对于G中的G：
            对于J范围（Len（g））的J：
                如果g [j] ==;
                    d_vals = val_dict [str（j）]
                    对于范围内的k（0，len（d_vals））：
                        如果d_va​​ls [k]！=实例[J]：
                            new_g = g.copy（）
                            new_g [j] = d_vals [k]
                            打印（new_g）
                            g_hyp.append（new_g）
            打印（）
            休息
        g = is_consistent（x [：i+1]，y [：i+1]，g_hyp）
        

打印（“最终特定假设：”，S）
打印（“最终的一般假设：”，G）
 
 数据集（chumma.csv）： 
 引用，尺寸，无盲，价格，版本，购买
有些，小，不，负担得起，一个，没有
许多，大，不，昂贵，很多，是的
许多，中等，不昂贵，很少，是的
有些，小，不，负担得起，一个，没有
许多，大，不，昂贵，很多，是的
许多，中等，不昂贵，很少，是的
 
 预期输出
该算法应输出特定的假设和一般假设（G），该假设代表从数据集中学到的概念
 问题
该代码似乎无法正确更新特定的假设（S）和一般假设（G）。具体：

 s尚未正确地为积极的例子正确概括。
 g无法正确专门用于负面示例。

我实施候选算法是什么问题，我该如何修复代码以正确更新S和G？]]></description>
      <guid>https://stackoverflow.com/questions/79463186/candidate-elimination-algorithm-implementation-not-working-as-expected</guid>
      <pubDate>Mon, 24 Feb 2025 10:26:17 GMT</pubDate>
    </item>
    <item>
      <title>语言模型的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79463184/probl%c3%a8me-avec-un-mod%c3%a8le-de-langage</link>
      <description><![CDATA[ valueerror：太多值无法打开包装（预期2）
追溯：
file＆quot＆quot c：\ user \ chawk \ pycharmprojects \ pfe.venv \ lib \ lib \ site-packages \ runlit \ runtime \ runtime \ scriptrunner \ exec_code.code.code.py.py＆quote＆quort＆quote＆quort＆quort of exec_func_with_error_error_error_error_error_error_error_error_error_error_error_error_error_error_error_error_handling in
结果= func（）]]></description>
      <guid>https://stackoverflow.com/questions/79463184/probl%c3%a8me-avec-un-mod%c3%a8le-de-langage</guid>
      <pubDate>Mon, 24 Feb 2025 10:24:56 GMT</pubDate>
    </item>
    <item>
      <title>它只是说这个问题，请帮助如何克服这个[关闭]</title>
      <link>https://stackoverflow.com/questions/79462841/it-just-says-this-problem-please-help-how-to-overcome-this</link>
      <description><![CDATA[ 在此处输入图像描述为什么它不说属性svc？我在这里做什么？我总是遇到这些类型的问题。
请帮助我我绝望。预先感谢您
我尝试了其他解决方案，但没有任何帮助]]></description>
      <guid>https://stackoverflow.com/questions/79462841/it-just-says-this-problem-please-help-how-to-overcome-this</guid>
      <pubDate>Mon, 24 Feb 2025 08:23:04 GMT</pubDate>
    </item>
    <item>
      <title>grpotrainer不支持iterabledataset</title>
      <link>https://stackoverflow.com/questions/79462501/iterabledataset-not-supported-on-grpotrainer</link>
      <description><![CDATA[执行时以下程序崩溃
 从数据集导入iterabledataset，数据集
来自TRL Import grpoconfig，grpotrainer

提示= [hi＆quot;
def data_generator（）：
    而真：
        在提示中s：
            产生{提示; ：S}
dataset = iterabledataset.from_generator（data_generator）


triending_args = grpoconfig（
    output_dir =＆quot; tmp＆quort;
    max_steps = 1000，
）

培训师= grpotrainer（
    型号=“ Facebook/opt-350m”
    Reward_funcs = Lambda提示，完成，** Kwargs：[1]*8，
    train_dataset =数据集，
    args =训练_args，
）

Trainer.Train（）
 
导致以下迹线：
  trackback（最近的最新通话）：
  file＆quot＆quort＆quot＆quode/code/code/cs234/starter_code/trl_testing.py&quot;，第24行，in＆lt; module＆gt;
    Trainer.Train（）
  file＆quot＆quort＆quort＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/transformers/trainer.py&amp;py&quot;，第2241号线
    return innion_training_loop（
  file＆quot＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/transformers/trainer.py&quot; line 2500，in _inner_training_training_loop in
    batch_samples，num_items_in_batch = self.get_batch_samples（epoch_iterator，num_batches）
  file＆quot＆quot＆quort＆quot＆quot＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/transformers/trainer.py&amp;py&quot; line 5180，在get_batch_samples中
    batch_samples += [next（epoch_iterator）]
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    next_batch，next_batch_info = self._fetch_batches（main_iterator）
  file＆quot＆quort＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/accelerate/data_loader.py＆quot＆quot; line 812，in _fetch_batches
    批处理=连接（批次，dim = 0）
  file＆quot＆quort＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot congatenate in Compatenate in Compatenate
    返回honador_type（data [0]，（condenenate（数据中的d [d [i]），dim = dim = dim）for range（len（data [0]））））））））））））））））
  file＆quot＆quort＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回类型（OBJ）（生成器）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回honador_type（data [0]，（condenenate（数据中的d [d [i]），dim = dim = dim）for range（len（data [0]））））））））））））））））
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回类型（data [0]）（{k：condenate（数据中的d [d [k]），data [0] .keys（）}的k中的k = dim = dim）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回类型（data [0]）（{k：condenate（数据中的d [d [k]），data [0] .keys（）}的k中的k = dim = dim）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    提高typeerror（f＆quot“只能连接张量，但得到{type（data [0]）};）
typeError：只能连接张量，但可以得到＆lt; class&#39;Str&#39;＆gt;
 
但是，用类似的 dataset 替换 iterabledataset 解决了问题：
 从数据集导入iterabledataset，数据集
来自TRL Import grpoconfig，grpotrainer

提示= [hi＆quot;
dataset = dataset.from_dict（{提示＆quot;：提示}）

triending_args = grpoconfig（
    output_dir =＆quot; tmp＆quort;
    max_steps = 1000，
）

培训师= grpotrainer（
    型号=“ Facebook/opt-350m”
    Reward_funcs = Lambda提示，完成，** Kwargs：[1]*8，
    train_dataset =数据集，
    args =训练_args，
）

Trainer.Train（）
 
这已经在2个截然不同的系统上复制了，因此这不太可能是原因。
我想念什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/79462501/iterabledataset-not-supported-on-grpotrainer</guid>
      <pubDate>Mon, 24 Feb 2025 05:08:12 GMT</pubDate>
    </item>
    <item>
      <title>如何从PDF中提取表并将其转换为结构化的HTML（<table>，<tr>，<td>），同时保持原始布局和格式化？</title>
      <link>https://stackoverflow.com/questions/79430117/how-to-extract-tables-from-a-pdf-and-convert-them-into-structured-html-table</link>
      <description><![CDATA[  1] doc1的原始页面包含4个表 
  1] output .html doc1 of doc1 of doc1 of doc1 of doc1 of doc1 of doc1 of docc1 of doc1 notection .html doct of dectuts of dectuts of dectuts of dectuts dectuts dotection dotect of table，有时从表中提取文本 
  2] doc2的原始页面在表中包含图像     
  2]输出.html doc2的of Doc2，复杂的表具有带有嵌入式图像的复杂表格，无法正确处理＆amp;有时，对于简单表而言，在.html文件中也无法获得适当图像描述在这里“ src =” https://i.sstatic.net/feqdv0vo.jpg“ /&gt;  
我正在从PDF文件中提取内容，并将其转换为HTML格式，同时维护原始结构和格式。我正在为此目的使用文档库。
我正在获取与 .html 文件中原始PDF文件相同的内容流的输出。但是，我在保留输出html文件中保存表结构时面临问题。
 我期望发生的事情： 

 从PDF中提取正确的行和列结构。

 将表布局保留在＆lt; table; ，＆lt; tr＆gt; 和＆lt; td＆gt;  html标签中。 

 保持原始格式，对齐和单元格内容，如PDF所示。


 实际发生的事情： 

 表未正确检测到表 - 表显示在＆lt; p＆gt; 标签中，而不是正确的＆lt; table&gt;＆gt; 结构。

 未对准的表格 - 细胞内部的图像出现在表面 表。。

 带有嵌入式图像的复杂表无法正确保留。


 使用的代码： 
 来自docling.document_converter import docuctonverter，pdfformatoption
从docling.datamodel.pipeline_options导入pdfpipelineOptions
来自docling.datamodel.base_models导入inputformat
来自docling_core.types.doc导入imagerefmode
从pathlib导入路径
导入记录

＃设置记录
logging.basicconfig（level = logging.info）
log = logging.getLogger（__名称__）＃更正：_name_--＆gt; __姓名__

＃配置图像设置
image_resolution_scale = 2.0

＃通往PDF文件的路径
source = path（r＆quot; c：\ users \ downloads \ journal.pdf;）
output_path =路径（r＆quot; c：\ users \ desktop \ output20.html＆quort;）

＃配置图像处理的管道选项
pipeline_options = pdfpipelineOptions（）
pipeline_options.images_scale = image_resolution_scale
pipeline_options.generate_page_images = true
pipeline_options.generate_picture_images = true

＃创建带有图像选项的转换器实例
转换器= documentConverter（
    format_options = {
        inputformat.pdf：pdfformatoption（pipeline_options = pipeline_options）
    }
）

＃将PDF转换为文档
结果= converter.convert（源）

＃用嵌入式图像保存HTML
result.document.save_as_html（output_path，image_mode = imagerefmode.embedded）

log.Info（f＆quot&#39;html文件，创建的嵌入式图像：{output_path}＆quot;）
 
 我到目前为止尝试过的是： 

 检查了提取的HTML输出 - 缺少或错误地显示。

 在 pdfpipelineoptions（）中尝试了不同的管道选项以查看它们是否影响表提取。

 将输出与文档智能库进行了比较 - 它可以更好地提取标题/页脚，但仍在使用复杂的表格。


 关键挑战： 

 表不保留在＆lt; table; 标签中。

 细胞分裂问题 - 数据被错位或分为多个部分。

 表中的图像放错了位置（出现在上方/下方而不是内部表单元格）。


 其他观察： 

提取的HTML文件中的内容流与原始PDF文件**匹配，但是**表结构未正确格式。

 问题： 
我如何才能从PDF正确提取表并将其转换为结构化的HTML（＆lt; table; gt; ，＆lt; tr＆gt; ，＆lt; td＆gt＆gt; ）在维护原始布局和使用文档库的格式化时？]]></description>
      <guid>https://stackoverflow.com/questions/79430117/how-to-extract-tables-from-a-pdf-and-convert-them-into-structured-html-table</guid>
      <pubDate>Tue, 11 Feb 2025 13:21:50 GMT</pubDate>
    </item>
    <item>
      <title>我如何成功设置和检索元数据信息以在Huggingface Hub上的HuggingFacedatAset？</title>
      <link>https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat</link>
      <description><![CDATA[我有许多数据集，我是从诸如此类的字典中创建的：
  info = datasetinfo（
        Description =&#39;我的快乐LIL数据集
        版本=; 0.0.1＆quot;
        homepage =＆quot; https：//www.myhomepage.co.uk＆quot;
    ）
train_dataset = dataset.from_dict（prepary_data（data [＆quot; train;]），info = info）
test_dataset = dataset.from_dict（prepary_data（数据[test; test;]），info = info）
验证_DATASET = DATASET.FROM_DICT（prepary_data（data [data [＆quot; quartation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quote = info = info）
 
 i然后将它们集合到数据集中。
 ＃创建一个datasetDict
dataset = datasetDict（
    {train＆quort＆quot; train_dataset，&#39;test;：test_dataset，;
）
 
到目前为止，一切都很好。如果我访问 dataset [&#39;train&#39;]。info.description 我看到“我的快乐lil dataset”的预期结果。
所以我推到轮毂上，就像：
  dataset.push_to_hub（f＆quot {agrompome}/{repo_name}＆quits＆quits_message =＆quort; some some commin
 
这也成功了。
但是，当我来将数据集从集线器中拉回并访问与之关联的信息时，而不是获取数据集的描述时，我只会得到一个空字符串；喜欢：
  pulled_data = full = load_dataset（＆quot; f {agrommy}/{repo_name}＆quort＆quort; use_auth_token = true）

＃我希望以下内容打印出来“我的快乐LIL数据集”。
print（pulled_data [&#39;train;]。info.Description）
＃但是，它返回&#39;&#39;
 
我是否错误地从集线器加载数据？我是只推出数据集而不是以某种方式推出信息吗？
我觉得我缺少一些明显的东西，但我真的不确定。]]></description>
      <guid>https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat</guid>
      <pubDate>Wed, 17 Jul 2024 13:23:04 GMT</pubDate>
    </item>
    <item>
      <title>如何在NLTK中下载Punkt Tokenizer？</title>
      <link>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</link>
      <description><![CDATA[我使用安装了NLTK库
  PIP安装NLTK
 
使用lib 
 来自nltk.tokenize导入send_tokenize 
send_tokenize（文本）
 
我遇到此错误
  lookuperror： 
****************************************************** ********************
  找不到资源朋克。
  请使用NLTK下载器获取资源：

  ＆gt;＆gt;＆gt;导入NLTK
  ＆gt;＆gt;＆gt; nltk.download（&#39;punkt&#39;）
  
  有关更多信息，请参见：https：//www.nltk.org/data.html

  尝试加载dokenizers/punkt/English.pickle

  在：
     - &#39;c：\\用户\\ adars/nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ share \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ lib lib \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\漫游\\ nltk_data&#39;
     - &#39;c：\\ nltk_data&#39;
     - &#39;d：\\ nltk_data&#39;
     - &#39;e：\\ nltk_data&#39;
     - &#39;&#39;&#39;
 
因此，为了解决此错误，我尝试了
 导入NLTK
nltk.download（&#39;punkt&#39;）
 
但是我无法下载此软件包，因为每次运行时，我都会收到错误的错误
  [nltk_data]错误加载punkt：＆lt; urlopen错误[WinError 10060] a
[nltk_data]连接尝试失败，因为连接的聚会
[nltk_data]一段时间后没有正确响应，或者
[nltk_data]建立的连接失败，因为连接的主机
[nltk_data]未能响应＆gt;
 
请在这里帮助我]]></description>
      <guid>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</guid>
      <pubDate>Tue, 19 Sep 2023 04:36:59 GMT</pubDate>
    </item>
    <item>
      <title>在微调过程中，如何正确设置垫子令牌（不是EOS），以避免模型不预测EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch Runtimeerror：Mat1＆Mat2形状无法乘以</title>
      <link>https://stackoverflow.com/questions/75693007/pytorch-runtimeerror-mat1-mat2-shapes-cannot-be-multiplied</link>
      <description><![CDATA[我正在Pytorch上建立CNN并收到以下错误消息：

 RuntimeError：MAT1和MAT2形状无法乘以（32x32768和
512x256）

我已经构建了以下模型：
  def classifier_block（输入，输出，kernel_size，stride，last_layer = false）：
  如果不是last_layer：
    x = nn。
        nn.conv2d（输入，输出，kernel_size，大步，填充= 3），
        nn.batchnorm2d（输出），，
        nn.leakyrelu（0.2，intplophe = true）
    ）
  别的：
    x = nn。
        nn.conv2d（输入，输出，kernel_size，大步），
        nn.maxpool2d（kernel_size = 3，步幅= 2，填充= 1）
    ）
  返回x

类分类器（nn.module）：
  def __init __（self，input_dim，输出）：
    超级（分类器，self）.__ init __（）
    self.classifier = nn。
        classifier_block（input_dim，64、7、2），
        classifier_block（64、64、3、2），
        classifier_block（64、128、3、2），
        classifier_block（128，256，3，2），
        classifier_block（256，512，3，2，true）
    ）
    打印（&#39;clf：&#39;，self.classifier）
    
    self.linear = nn.Sequepention（
        nn.linear（512，256），
        nn.relu（inplace = true），
        nn.linear（256，128），
        nn.relu（inplace = true），
        nn.linear（128，64），
        nn.relu（inplace = true），
        nn.linear（64，输出）
    ）
    打印（&#39;linear：&#39;，self.linear）
  
  向前（自我，图像）：
    打印（&#39;img：&#39;，image.shape）
    x = self.classifier（图像）
    打印（&#39;X：&#39;，X.Shape）
    返回self.linear（x.View（len（x），-1））
 
输入图像是大小 512x512 。这是我的训练障碍：
  loss_train = []
loss_val = []

对于范围（时期）的时期：
  print（&#39;epoch：{}/{}&#39;。格式（epoch，epochs））
  total_train = 0
  CRORCE_TRAIN = 0
  cumloss_train = 0
  classifier.train（）
  对于枚举（x，y）的批次（train_loader）：
    x = x.to（设备）
    打印（X.Shape）
    打印（y.形）
    输出=分类器（x）
    损失=标准（输出，y.to（设备））
    优化器.zero_grad（）
    loss.backward（）
    优化器.step（）

    打印（&#39;损失：{}&#39;。格式（损失））
 
任何建议都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/75693007/pytorch-runtimeerror-mat1-mat2-shapes-cannot-be-multiplied</guid>
      <pubDate>Fri, 10 Mar 2023 06:48:16 GMT</pubDate>
    </item>
    <item>
      <title>如何在虚线文本验证码中找到轮廓图像</title>
      <link>https://stackoverflow.com/questions/71261999/how-to-find-contours-in-dotted-text-captcha-image</link>
      <description><![CDATA[我是OpenCV的新手。我正在尝试找到验证码图像的轮廓。仅当我的验证码包含虚线文本时，它不起作用。
我已经完成了以下代码：
 导入numpy作为NP
导入CV2作为CV
导入imgaug.augmenters为IAA

im = cv.imread（&#39;dataset/1.jpg&#39;）
imgray = cv.cvtcolor（im，cv.color_bgr2gray）

imgray = cv.threshold（Imgray，127，255，0）[1]

dst = cv.canny（imgray，0,150）
bluret = cv.blur（dst，（5,5），0）
img_thresh = cv.AdaptivEthreshold（Blud，255，cv.Adaptive_thresh_gaussian_c，cv.thresh_binary_inv，11，2）

内核= cv.getStructuringElement（cv.morph_rect，（3,3））
阈值= cv.morphologyex（img_thresh，cv.morph_close，kernel）

轮廓，层次结构= cv.findcontours（dst，cv.retr_tree，cv.chain_approx_simple）
打印（Len（Contours））
＃cv.drawContours（IM，轮廓，-1，（0，255，0），3）

cv.imshow（“ img_thresh”，img_thresh）
cv.imshow（dst&#39;dst）
cv.imshow（“阈值”，阈值）
CV.Waitkey（0）
cv.destroyallwindows（）
 
有人可以帮忙吗？有什么方法可以在此图像中找到轮廓？
  ]]></description>
      <guid>https://stackoverflow.com/questions/71261999/how-to-find-contours-in-dotted-text-captcha-image</guid>
      <pubDate>Fri, 25 Feb 2022 06:39:32 GMT</pubDate>
    </item>
    <item>
      <title>如何找到稀疏矢量的最近邻居</title>
      <link>https://stackoverflow.com/questions/34611337/how-to-find-the-nearest-neighbor-of-a-sparse-vector</link>
      <description><![CDATA[我有大约500个向量，每个向量是1500维矢量，
几乎每个向量都很稀疏 - 我的意思是，矢量的30-70维度不是0。
现在，问题在于，这里是一个给定的向量，也是1500个维度，我需要将其与500个向量进行比较，以查找500个最接近的矢量。（在Euclidean距离中）。。
毫无疑问，蛮力方法是一种解决方案，但是我需要计算500次的距离，这需要很长时间。
昨天，我读了一篇文章“用大词汇和快速的空间匹配”的文章，它说使用倒置索引会有所帮助，它说：
   
但是，在我的测试之后，几乎没有任何意义，想象一个1500矢量，其中50个尺寸并不为零，当涉及另一个尺寸时，它们可能总是具有相同的尺寸，而不是零。换句话说，这种算法只能排除一个小矢量，我仍然需要与剩下的许多向量进行比较。
我的问题：

 此算法是否有意义？

 还有其他方法可以做我想做的事吗？例如Flann或KD-Tree？
但是我想要精确的准确的最近邻居，大约是一个不够的

]]></description>
      <guid>https://stackoverflow.com/questions/34611337/how-to-find-the-nearest-neighbor-of-a-sparse-vector</guid>
      <pubDate>Tue, 05 Jan 2016 12:07:01 GMT</pubDate>
    </item>
    <item>
      <title>数据归一化</title>
      <link>https://stackoverflow.com/questions/21554301/data-normalization</link>
      <description><![CDATA[当我想分类“好”时或“最佳”然后，我可以使用Facebook的计数或Twitter转发计数的计数。
但是有些社区的用户群很大，因此他们的链接获得了更多的喜欢或转发。我该如何“归一化”这些巨大的社区喜欢例如，像count这样的小得多的社区的类似新闻项目链接之类的链接？
这被称为正常化吗？我可以在哪种书籍中学习有关“质量”的这类算法。 （例如，在这种情况下）？无论如何，我想做什么？]]></description>
      <guid>https://stackoverflow.com/questions/21554301/data-normalization</guid>
      <pubDate>Tue, 04 Feb 2014 13:47:45 GMT</pubDate>
    </item>
    <item>
      <title>数据归一化的参考文献[关闭]</title>
      <link>https://stackoverflow.com/questions/5652357/references-for-data-normalization</link>
      <description><![CDATA[对于NNS和其他机器学习算法，将数据标准化（不确定是否正确）的最佳实践是什么？  我的意思是您如何表示NN/Algo的数据。
例如，您如何表示商店代码？  商店555不大于或小于554，它只是一个分类。 NNS/ALGO模型只是单独过滤出来，还是您需要使它们进行分类而不是数学上的区别？
 编辑：感谢大家的答案。  我一直在挖掘很多数据挖掘书，尽管我发现了一些在预处理的数据主题上花了一两章的时间，但我对最掩饰的效果最大感到有些惊讶。  再次感谢。]]></description>
      <guid>https://stackoverflow.com/questions/5652357/references-for-data-normalization</guid>
      <pubDate>Wed, 13 Apr 2011 16:17:41 GMT</pubDate>
    </item>
    </channel>
</rss>