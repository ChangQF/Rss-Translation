<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 23 Mar 2024 00:56:35 GMT</lastBuildDate>
    <item>
      <title>TPU 连接问题 训练 TF 模型 Google Colab</title>
      <link>https://stackoverflow.com/questions/78209293/tpu-connectivity-issue-training-tf-model-google-colab</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78209293/tpu-connectivity-issue-training-tf-model-google-colab</guid>
      <pubDate>Fri, 22 Mar 2024 23:57:10 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用训练集还是验证集来进行参数优化？</title>
      <link>https://stackoverflow.com/questions/78209231/should-i-use-training-or-validation-set-for-parameter-otimization</link>
      <description><![CDATA[我正在使用决策树和参数优化来训练模型。
我了解到验证集的目标是评估训练期间的模型性能并帮助调整参数。
考虑到这一点，我不应该使用 grid_search.fit 上的验证集而不是训练集吗？
param_grid = {
    &#39;最大深度&#39;: [3, 5, 7, 10],
    &#39;min_samples_split&#39;: [2, 5, 10],
    &#39;min_samples_leaf&#39;: [1, 2, 4]
}

clf = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(clf, param_grid, cv=5, 评分=&#39;准确度&#39;)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(&quot;最佳参数：&quot;, best_params)
打印(“\n”)

＃验证
best_clf = grid_search.best_estimator_
val_accuracy = best_clf.score(X_val, y_val)
print(“最佳模型的验证准确性：”, val_accuracy)
打印(“\n”)

＃测试
y_test_pred = best_clf.predict(X_test)
test_accuracy = precision_score(y_test, y_test_pred)
test_ precision = precision_score(y_test, y_test_pred)
test_recall=recall_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred)
print(“具有最佳模型的测试集上的决策树测量：”)
print(&quot;准确度：&quot;, test_accuracy)
print(&quot;精度：&quot;, test_ precision)
print(&quot;召回：&quot;, test_recall)
print(&quot;F1 分数：&quot;, test_f1)
打印(“---------------------------------------------- ---------”）
]]></description>
      <guid>https://stackoverflow.com/questions/78209231/should-i-use-training-or-validation-set-for-parameter-otimization</guid>
      <pubDate>Fri, 22 Mar 2024 23:27:32 GMT</pubDate>
    </item>
    <item>
      <title>连体网络反向传播</title>
      <link>https://stackoverflow.com/questions/78208649/siamese-network-backpropagation</link>
      <description><![CDATA[我正在尝试使用 Triplet-loss 构建一个跨视图本地化网络。
我想要查询照片（锚点），通过“model1”，以及负片和“model1”。积极通过“model2” （因为他们来自不同的领域，我不想分享他们的权重）。
我想知道当我计算 2 个不同模型的特征的损失时，如何正确反向传播。
这段代码正确吗？：
criterion_triplet = nn.TripletMarginLoss（margin=args.margin，p=2，reduction=“sum”）
模型 1 = MyNet(参数)
模型2 = MyNet(参数)
优化器1 = torch.optim.Adam(...)
优化器2 = torch.optim.Adam(...)

...训练循环...
   features1 = model1(images_anchor.to(args.device))
   features2 = model2(images_satelite.to(args.device))
   loss_triplet += criteria_triplet(features1,
                                     特点2,
                                     特点2)
   优化器1.zero_grad()
   优化器2.zero_grad()
   loss_triplet.backward()
   优化器1.step()
   优化器2.step()
]]></description>
      <guid>https://stackoverflow.com/questions/78208649/siamese-network-backpropagation</guid>
      <pubDate>Fri, 22 Mar 2024 20:15:29 GMT</pubDate>
    </item>
    <item>
      <title>MLP a2c 策略抱怨 0 不大于 0，或者无穷大不大于 0？</title>
      <link>https://stackoverflow.com/questions/78208624/mlp-a2c-policy-complaining-that-0-isnt-greater-than-0-or-infinity-isnt-greate</link>
      <description><![CDATA[当我训练一些火炬模型时出现以下错误：
ValueError(&#39;分布Normal(loc: torch.Size([1, 4]))的预期参数尺度（形状为(1, 4)的张量），scale: torch.Size([1, 4] )) 以满足约束 GreaterThan(lower_bound=0.0)，但发现无效值：\ntensor([[inf, inf, 0., 0.]])&#39;)。

我的行为具有形状 (4,) 和观察 (3,)。
它是否认为无穷大不&gt;0，或者0不大于0？我不知道为什么会出现这种情况。它是简单地使用 model.learn 在稳定的基线 3 中训练模型。然而，它学习了一段时间，但在这一步失败了：
~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py 学习中（self、total_timesteps、callback、log_interval、tb_log_name、reset_num_timesteps、progress_bar）
    第257章
    [第 258 章]总时间步数：
--&gt;第 259 章
    260
    261 如果 continue_training 为 False：

〜\ anaconda3 \ envs \ lib \ site-packages \ stable_baselines3 \ common \ on_policy_algorithm.py在collect_rollouts中（self，env，callback，rollout_buffer，n_rollout_steps）
    167 # 转换为pytorch张量或TensorDict
    第168章
--&gt; 169 个动作，值，log_probs = self.policy(obs_tensor)
    170 个动作 = actions.cpu().numpy()
    171

_call_impl 中的 ~\anaconda3\envs\\lib\site-packages\torch\nn\modules\module.py(self, *input, **kwargs)
   第1192章
   第1193章
-&gt;第1194章
   第1195章
   第1196章

〜\anaconda3\envs\\lib\site-packages\stable_baselines3\common\policies.py 向前（自我，obs，确定性）
    第624章
    625 个值 = self.value_net(latent_vf)
--&gt; [第 626 章]
    第627章 行动=distribution.get_actions(确定性=确定性)
    第628章

~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\policies.py 在 _get_action_dist_from_latent(self, Latent_pi)
    第654章
    第655章
--&gt;第656章
    第657章
    第658章

proba_distribution 中的 ~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\distributions.py(self,mean_actions,log_std)
    第162章 162
    第 163 章
--&gt;第164章
    第165章 回归自我
    166

~\anaconda3\envs\\lib\site-packages\torch\distributions\normal.py 在 __init__(self, loc, scale, validate_args)
     54 其他：
     55 batch_shape = self.loc.size（）
---&gt; 56 super(普通，自我).__init__(batch_shape, validate_args=validate_args)
     57
     58 def Expand(self,batch_shape,_instance=None):

__init__ 中的 ~\anaconda3\envs\\lib\site-packages\torch\distributions\distribution.py(self、batch_shape、event_shape、validate_args)
     55 如果无效.all():
     56 引发值错误（
---&gt; 57 f“预期参数{param}”
     58 f&quot;({type(value).__name__}，形状为{tuple(value.shape)})&quot;
     59 f”分布{repr(self)}”

请记住我的操作是 0&lt;=a&lt;=1。我需要将其设置为 0
我很难知道它到底在抱怨什么，因为这段代码深入稳定的基线3。这可能是他们的包中的一个小故障吗？我希望它更新权重并继续运行，但它却抱怨 0 不大于 0.. 我不知道为什么我关心这个，但它应该继续运行，不是吗？
感谢您的浏览。]]></description>
      <guid>https://stackoverflow.com/questions/78208624/mlp-a2c-policy-complaining-that-0-isnt-greater-than-0-or-infinity-isnt-greate</guid>
      <pubDate>Fri, 22 Mar 2024 20:07:29 GMT</pubDate>
    </item>
    <item>
      <title>为什么 torch.nn.function.linear 中重量的维度是 (out,in) 而不是 (in,out)</title>
      <link>https://stackoverflow.com/questions/78208603/why-are-the-dimension-of-the-weight-in-torch-nn-functional-linear-out-in-inste</link>
      <description><![CDATA[在 torch.nn.function.linear 的文档中（https://pytorch.org/docs/stable/ generated/torch.nn.function.linear.html），权重输入的维度为（out_features，in_features），然后计算时对权重矩阵进行转置输出：y=xA^T+b。为什么他们这样做而不是采用维度矩阵 W（in_features、out_features）并执行 y=xW+b？
通过执行 y=xW+b 尺寸将匹配，因此我找不到上述的明确原因。]]></description>
      <guid>https://stackoverflow.com/questions/78208603/why-are-the-dimension-of-the-weight-in-torch-nn-functional-linear-out-in-inste</guid>
      <pubDate>Fri, 22 Mar 2024 20:01:42 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow Lite：导入错误：libusb-1.0.so.0：无法打开共享对象文件：没有这样的文件或目录</title>
      <link>https://stackoverflow.com/questions/78208486/tensorflow-lite-importerror-libusb-1-0-so-0-cannot-open-shared-object-file-n</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78208486/tensorflow-lite-importerror-libusb-1-0-so-0-cannot-open-shared-object-file-n</guid>
      <pubDate>Fri, 22 Mar 2024 19:31:01 GMT</pubDate>
    </item>
    <item>
      <title>以下 RandomizedSearchCV 的实现正确吗？</title>
      <link>https://stackoverflow.com/questions/78208446/is-the-following-implementation-of-randomizedsearchcv-right</link>
      <description><![CDATA[我正在通过这本书学习机器学习&quot;动手机器学习”，并且有一个练习来实现SelectFromModel。书中给出的练习的解决方案运行以下代码：
param_distribs = {
        &#39;svr__kernel&#39;: [&#39;线性&#39;, &#39;rbf&#39;],
        &#39;svr__C&#39;: loguniform(20, 200_000),
        &#39;svr__gamma&#39;: 指数(scale=1.0),
    }

svr_pipeline = 管道([(“预处理”, 预处理), (“svr”, SVR())])
rnd_search = RandomizedSearchCV(svr_pipeline,
                                param_distributions=param_distribs,
                                n_iter=50，CV=3，
                                评分=&#39;neg_root_mean_squared_error&#39;,
                                详细=2，
                                随机状态=42）

rnd_search.fit(housing.iloc[:5000], housing_labels.iloc[:5000])

选择器管道=管道（[
    （&#39;预处理&#39;，预处理），
    (&#39;选择器&#39;, SelectFromModel(RandomForestRegressor(random_state=42),
                                 阈值=0.005)), # 最小特征重要性
    (&#39;svr&#39;, SVR(C=rnd_search.best_params_[&quot;svr__C&quot;],
                gamma=rnd_search.best_params_[“svr__gamma”],
                kernel=rnd_search.best_params_[“svr__kernel”])),
]）

selector_rmses = -cross_val_score(selector_pipeline,
                                  住房.iloc[:5000],
                                  housing_labels.iloc[:5000],
                                  评分=“neg_root_mean_squared_error”，
                                  简历=3）
pd.Series(selector_rmses).describe()

计数 3.000000
平均值 56211.362086
标准1922.002802
分钟 54150.008629
25% 55339.929909
50% 56529.851189
75% 57242.038815
最大 57954.226441
数据类型：float64

好的。这个结果比之前的练习更糟糕，作者建议：“哦，特征选择似乎没有帮助。”但也许这只是因为我们使用的阈值不是最佳的。也许尝试使用随机搜索或网格搜索来调整它？”
嗯，出于学习的目的，我尝试着实施这个建议。以下是我的尝试：
param_distribs = {
    &#39;选择器__阈值&#39;：统一（0，0.05），
    &#39;svr__C&#39;: loguniform(20, 200_000),
    &#39;svr__gamma&#39;: 指数(scale=1.0),
    &#39;svr__kernel&#39;: [&#39;线性&#39;, &#39;rbf&#39;]
}

选择器管道=管道（[
    （&#39;预处理&#39;，预处理），
    (&#39;选择器&#39;, SelectFromModel(RandomForestRegressor(random_state=42))),
    (&#39;svr&#39;, SVR()),
]）

rnd_search = RandomizedSearchCV(selector_pipeline,
                                param_distributions=param_distribs,
                                n_iter=50，CV=3，
                                评分=&#39;neg_root_mean_squared_error&#39;,
                                详细=2，
                                随机状态=42）

rnd_search.fit(housing.iloc[:5000], housing_labels.iloc[:5000])

# 获取最佳参数
best_threshold = rnd_search.best_params_[&#39;selector__threshold&#39;]
best_svr_C = rnd_search.best_params_[&#39;svr__C&#39;]
best_svr_gamma = rnd_search.best_params_[&#39;svr__gamma&#39;]
best_svr_kernel = rnd_search.best_params_[&#39;svr__kernel&#39;]

# 使用最佳参数更新管道
选择器管道.set_params(
    选择器__阈值=最佳阈值，
    svr__C=best_svr_C,
    svr__gamma=best_svr_gamma,
    svr__kernel=best_svr_kernel
）

但是最后一个输出似乎比前一个输出更糟糕：
selector_rmses = -cross_val_score(selector_pipeline,
                                  住房.iloc[:5000],
                                  housing_labels.iloc[:5000],
                                  评分=“neg_root_mean_squared_error”，
                                  简历=3）
pd.Series(selector_rmses).describe()

计数 3.000000
平均值 56713.692587
标准2011.579891
分钟 55241.366017
25% 55567.689915
50% 55894.013813
75% 57449.855872
最大 59005.697931
数据类型：float64

我只想解释结果：也许阈值的RandomizedSearchCV不需要解决方案，也许优化阈值无论如何都无济于事。但对我来说更根本的问题是：我的实施正确吗？我不能 100% 确定这是正确的方法。就像我说的，我正在研究和学习，然后我不确定代码。如果这是正确的，下一步可以做什么来尝试更好的性能？]]></description>
      <guid>https://stackoverflow.com/questions/78208446/is-the-following-implementation-of-randomizedsearchcv-right</guid>
      <pubDate>Fri, 22 Mar 2024 19:22:22 GMT</pubDate>
    </item>
    <item>
      <title>源代码显示的不同输出（机器学习）（Python）</title>
      <link>https://stackoverflow.com/questions/78207935/different-output-showing-from-a-source-code-machine-learning-python</link>
      <description><![CDATA[我目前正在尝试开展一个小型图像机器学习项目。我找到了这个人的 Kaggle 代码，并尝试从头开始复制它。然而，即使在主要部分，我也已经遇到了错误。
我确信我的结局一定存在本地化问题，但我不知道是什么。
我的代码：
#导入库

#数据处理模块
将 pandas 导入为 pd
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
导入CV2
#文件目录模块
将 glob 导入为 gb
导入操作系统
#训练和测试（机器学习）模块
将张量流导入为 tf
导入keras

#将图像导入到代码中

trainDataset = &#39;melanoma_cancer_dataset/train&#39;
testDataset = &#39;melanoma_cancer_dataset/test&#39;
预测数据集 = &#39;melanoma_cancer_dataset/skinTest&#39;

#为要处理的图像创建空列表
训练列表 = []
测试列表 = []
#为良性和恶性这两个键制作一个分类字典
#用于插入图像
词典 = {&#39;良性&#39;: 0, &#39;恶性&#39;: 1}

#读取文件夹的长度内容
对于 os.listdir(trainDataset) 中的文件夹：
    数据 = gb.glob(路径名=str(trainDataset + 文件夹 + &#39;/*.jpg&#39;))
    print(f&#39;{len(data)} 在文件夹 {fold}&#39;)
    #读取图像，按照统一的顺序调整它们的大小，并将它们存储在空列表中
    对于数据中的数据：
        图像 = cv2.imread(数据)
        imageList = cv2.resize(图像(120,120))
        Training_List.append(列表(imageList))

笔记本的输出显示该文件夹中存储了 0 个图像/内容。现在我有点怀疑这里发生了什么，并且希望得到一些答案。提前致谢。我也在使用自己的 VScode。
这是我的文件的屏幕截图：
]]></description>
      <guid>https://stackoverflow.com/questions/78207935/different-output-showing-from-a-source-code-machine-learning-python</guid>
      <pubDate>Fri, 22 Mar 2024 17:27:51 GMT</pubDate>
    </item>
    <item>
      <title>用于API结构转换的AI/ML模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78207358/ai-ml-model-for-api-structure-conversion</link>
      <description><![CDATA[是否有一个 AI 模型可以将不同 API 的响应转换为单一 JSON 格式？
所有 API 响应都有不同的结构。我正在寻找一种将响应转换为通用格式的模型。所有API都与数据集相关。
我正在尝试将不同的结构化 API 响应转换为单一格式，以便更容易使用。目前我创建了一个程序来做到这一点，但它需要针对不同的 API 进行少量的手动工作。]]></description>
      <guid>https://stackoverflow.com/questions/78207358/ai-ml-model-for-api-structure-conversion</guid>
      <pubDate>Fri, 22 Mar 2024 15:43:09 GMT</pubDate>
    </item>
    <item>
      <title>Azure ML Studio Web 服务始终返回相同的预测</title>
      <link>https://stackoverflow.com/questions/78207131/azure-ml-studio-web-service-always-returns-the-same-prediction</link>
      <description><![CDATA[我目前正在为我的课程开发一个小型项目，但遇到了障碍，希望能得到一些帮助。在 Azure 机器学习工作室中训练 SVM 模型并将其部署为 Web 服务后，我遇到了一个特殊问题 - 无论输入数据如何，该服务都会返回相同的预测。
可能出了什么问题？
https://gallery.cortanaintelligence.com/Experiment/Binary-Classifier -SVM-Web
https://gallery.cortanaintelligence.com/Experiment/Binary-Classifiers-SVM
这是我到目前为止所做的事情：

确保所有预处理步骤与模型训练阶段使用的步骤相同（包括数据标准化和缺失值处理）。
仔细检查模型是否使用输入数据进行评分。
验证了网络服务的配置，特别是在构建响应方面，以确保不返回任何静态值。
确保输入数据的格式与预期架构匹配。该模型在 ML Studio 环境中表现良好，并根据测试数据进行准确预测。但是，部署的 Web 服务似乎没有反映此行为并输出恒定值。
]]></description>
      <guid>https://stackoverflow.com/questions/78207131/azure-ml-studio-web-service-always-returns-the-same-prediction</guid>
      <pubDate>Fri, 22 Mar 2024 15:06:25 GMT</pubDate>
    </item>
    <item>
      <title>R 混淆矩阵 - 错误：“数据”和“参考”应该是具有相同级别的因素[关闭]</title>
      <link>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</link>
      <description><![CDATA[尽管还有其他针对相同错误消息的报告，但没有一个对我的情况有帮助。
我已经准备了自己的数据，分割如下，但无法获得混淆矩阵。
test_index &lt;- createDataPartition(y =workingData$PM10, times = 1, p = 0.5, list = FALSE)
train_set &lt;-工作数据[-test_index,]
test_set &lt;-工作数据[test_index,]

train_knn &lt;- train(PM10 ~. , method= &quot;knn&quot; , data = train_set)

y_hatknn &lt;- 预测(train_knn, train_set, type = “raw”)

fusionMatrix(y_hatknn, test_set$PM10)

上面最后一行给出
错误：“data”和“reference”应该是具有相同级别的因素。

我想上传数据进行复制，但可以提供基本的：
&lt;前&gt;&lt;代码&gt;str(工作数据)
“数据帧”：3653 obs。 3 个变量：
&#39; $ 日期 : 数字 2e+07 2e+07 2e+07 2e+07 2e+07 ...
&#39; $ Rain_mm: 数字 0.1 6.7 0 1.4 0.8 1.8 15.3 0 2.6 3.8 ...
&#39; $ PM10 : 数字 -1 -1 -1 -1 -1 ...

PM10 是污染 PM10 水平。
如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</guid>
      <pubDate>Fri, 22 Mar 2024 09:39:08 GMT</pubDate>
    </item>
    <item>
      <title>给定标签集之外的短 2-3 个标记文本或用户搜索查询的序列标签 [关闭]</title>
      <link>https://stackoverflow.com/questions/78204207/sequence-labelling-for-short-2-3-token-text-or-user-search-queries-out-of-given</link>
      <description><![CDATA[我正在使用FLAIR模块解决序列标签问题。
我有包含 3 种不同类型实体的虚拟电子商务数据，每个实体都有大约 1K 个子实体。训练数据（大小约为 200K）是通过 3K 标签的组合综合创建的。
我尝试使用查询分类模型（带有 3K 标签）验证 FLAIR 序列标签。 FLAIR 模型（F1-score：60%）严重低于分类模型（F1-score：80%） &gt;).
我不愿意开发序列标签模块，因为我希望序列标签器也能够检测并提出新实体。
你能帮我了解哪里可能出错以及我可以尝试哪些其他模型吗？]]></description>
      <guid>https://stackoverflow.com/questions/78204207/sequence-labelling-for-short-2-3-token-text-or-user-search-queries-out-of-given</guid>
      <pubDate>Fri, 22 Mar 2024 05:30:33 GMT</pubDate>
    </item>
    <item>
      <title>Spark如何应用于深度学习模型训练阶段？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78204101/how-can-spark-be-used-in-deep-learning-model-training-phase</link>
      <description><![CDATA[我注意到 Apache Spark 被大量用于训练数据准备，但我很好奇它与 PyTorch/TensorFlow 一起在训练阶段的潜在作用，特别是在同时具有 CPU 和 GPU 的环境中。我想知道 Spark 在数据加载或缓存方面是否比 PyTorch/TensorFlow 有任何优势，特别是在分布式训练场景中。
虽然 PyTorch 和 TensorFlow 都支持分布式训练，但我很想知道 Spark 的功能是否可以提高性能或减少延迟，特别是在处理可能超出 GPU 内存容量的大型数据集时。]]></description>
      <guid>https://stackoverflow.com/questions/78204101/how-can-spark-be-used-in-deep-learning-model-training-phase</guid>
      <pubDate>Fri, 22 Mar 2024 04:51:30 GMT</pubDate>
    </item>
    <item>
      <title>Yolov8 超参数调优</title>
      <link>https://stackoverflow.com/questions/78196468/yolov8-hyperparameter-tunning</link>
      <description><![CDATA[我尝试改进模型中的 mAP 结果。你能帮我调整yolov8中的超参数吗
如何调整 Yolov8 中的超参数？有什么方法可以在 Colab 中用 Python 代码对其进行调优吗？或者我应该与默认一起工作。 yaml 文件，我应该如何使用这个文件？
另外，我想提高准确性。我怎样才能通过调整来做到这一点？
这是我的问题]]></description>
      <guid>https://stackoverflow.com/questions/78196468/yolov8-hyperparameter-tunning</guid>
      <pubDate>Wed, 20 Mar 2024 22:05:12 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch中的forward函数到底输出什么？</title>
      <link>https://stackoverflow.com/questions/64987430/what-exactly-does-the-forward-function-output-in-pytorch</link>
      <description><![CDATA[此示例逐字取自 PyTorch 文档。现在我确实对深度学习有一些总体背景，并且知道很明显，forward 调用表示前向传递，穿过不同的层并最终到达终点，在本例中有 10 个输出，然后获取前向传递的输出并使用定义的损失函数计算损失。现在，我忘记了在这种情况下 forward() 传递的输出到底给我带来了什么。
我认为神经网络的最后一层应该是某种激活函数，例如 sigmoid() 或 softmax() ，但我没有看到这些是定义在任何地方，而且，当我现在做项目时，我发现后来调用了 softmax() 。所以我只是想澄清 outputs = net(inputs) 给我的到底是什么 link，在我看来，默认情况下 PyTorch 模型前向传递的输出是 logits？
transform = Transforms.Compose(
    [transforms.ToTensor(),
     变换.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True,
                                        下载=真，变换=变换）
trainloader = torch.utils.data.DataLoader(trainset,batch_size=4,
                                          洗牌=真，num_workers=2）

将 torch.nn 导入为 nn
导入 torch.nn.function 作为 F


类 Net(nn.Module):
    def __init__(自身):
        超级（网络，自我）.__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def 前向（自身，x）：
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        返回x


净=净()

导入 torch.optim 作为 optim

标准 = nn.CrossEntropyLoss()
优化器 = optim.SGD(net.parameters(), lr=0.001, 动量=0.9)

for epoch in range(2): # 多次循环数据集

    运行损失 = 0.0
    对于 i，enumerate(trainloader, 0) 中的数据：
        # 获取输入；数据是[输入，标签]的列表
        输入，标签=数据

        # 将参数梯度归零
        优化器.zero_grad()

        # 前向+后向+优化
        输出 = 净值（输入）
        打印（输出）
        休息
        损失=标准（输出，标签）
        loss.backward()
        优化器.step()

        # 打印统计数据
        running_loss += loss.item()
        if i % 2000 == 1999: # 每 2000 个小批量打印一次
            print(&#39;[%d, %5d] 损失: %.3f&#39; %
                  (epoch + 1, i + 1, running_loss / 2000))
            运行损失 = 0.0

print(&#39;训练完成&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/64987430/what-exactly-does-the-forward-function-output-in-pytorch</guid>
      <pubDate>Tue, 24 Nov 2020 13:21:18 GMT</pubDate>
    </item>
    </channel>
</rss>