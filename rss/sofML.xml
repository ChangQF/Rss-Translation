<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 15 Jul 2024 09:17:17 GMT</lastBuildDate>
    <item>
      <title>交叉验证和 MICE 归因 [关闭]</title>
      <link>https://stackoverflow.com/questions/78748357/cross-validation-and-mice-imputation</link>
      <description><![CDATA[我正在研究一个二元分类问题，其中有一些缺失数据。我最初的想法是使用 MiceForest。我还使用了分层 k 折技术（数据不平衡）。我还想尽量减少数据泄漏。

我应该何时使用 MiceForest 填补缺失值？针对每个折？还是一开始就填补整个数据集？
我应该使用 SMOTE 来解决每个折中的类别不平衡问题吗？因为我得到了很多误报（当仅使用分层 k 折而没有过度采样时）。

当我对整个数据集进行 mice 填补，用 smote 解决类别不平衡问题，然后进行交叉验证时，我获得了非常好的性能。我觉得这是过度拟合？这是因为数据泄漏吗？（我对这个领域有点陌生）]]></description>
      <guid>https://stackoverflow.com/questions/78748357/cross-validation-and-mice-imputation</guid>
      <pubDate>Mon, 15 Jul 2024 06:37:02 GMT</pubDate>
    </item>
    <item>
      <title>HuggingFace：Llama-3-8B 合作检查点碎片加载进度在 25% 处停止</title>
      <link>https://stackoverflow.com/questions/78748213/huggingface-loading-checkpoint-shards-in-collab-for-llama-3-8b-stops-at-25</link>
      <description><![CDATA[我尝试使用 huggingface 在我的 Colab 笔记本中本地运行 Llama-3-8B 模型。加载模型时，检查点分片在 25% 处停止加载。我不明白问题可能是什么。
from transformers import AutoModelForCausalLM, AutoTokenizer

# 定义模型名称（这是一个占位符，请替换为实际模型名称）
model_name = &quot;meta-llama/Meta-Llama-3-8B&quot;

!huggingface-cli login --token $HF_TOKEN
# 加载 tokenizer 和模型
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 如果模型很大，将其移动到 GPU 可能会有所帮助
model.to(&#39;cuda&#39;)

HF_Token 已定义，出于隐私原因，此处未提及。
提示以下错误：
您的 token 已保存到 /root/.cache/huggingface/token
登录成功
/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: 
您的 Colab secrets 中不存在 secret `HF_TOKEN`。
要使用 Hugging Face Hub 进行身份验证，请在设置选项卡 (https://huggingface.co/settings/tokens) 中创建一个令牌，将其设置为 Google Colab 中的机密，然后重新启动会话。
您将能够在所有笔记本中重复使用此机密。
请注意，建议进行身份验证，但仍然可以选择访问公共模型或数据集。
warnings.warn(
词汇表中已添加特殊令牌，请确保对相关的词嵌入进行了微调或训练。
正在加载 检查点 分片：  25%
 1/4 [00:22&lt;01:07, 22.37s/it]
]]></description>
      <guid>https://stackoverflow.com/questions/78748213/huggingface-loading-checkpoint-shards-in-collab-for-llama-3-8b-stops-at-25</guid>
      <pubDate>Mon, 15 Jul 2024 05:44:13 GMT</pubDate>
    </item>
    <item>
      <title>如何对视频中对象执行的具体动作进行分类。（不是仅使用一帧，而是使用一组帧）[关闭]</title>
      <link>https://stackoverflow.com/questions/78747441/how-to-classify-what-specific-actions-an-object-performs-in-a-video-not-using</link>
      <description><![CDATA[我想创建一个人工智能模型，通过查看帧集合来确定对象行为的结果，而不是通过在家打高尔夫球时查看单个帧来确定高尔夫球是进入还是离开。
我尝试使用 ultralytics，但 ultralytics 按帧对对象进行分类，因此它不符合我的目的。我想知道如何创建一个区分视频中行为分类的模型。
如何制作一个分析帧而不是帧的人工智能模型。]]></description>
      <guid>https://stackoverflow.com/questions/78747441/how-to-classify-what-specific-actions-an-object-performs-in-a-video-not-using</guid>
      <pubDate>Sun, 14 Jul 2024 20:58:11 GMT</pubDate>
    </item>
    <item>
      <title>如何制作一个自动检查工程图形答题纸并相应分配分数的系统[关闭]</title>
      <link>https://stackoverflow.com/questions/78747310/how-to-a-make-system-which-automatically-check-the-engineering-graphics-answer-s</link>
      <description><![CDATA[如何制作一个系统，自动检查工程图形答题纸并根据其正确性分配分数此类图形图像需要分配分数
尝试进行图像处理但无法做到。你能告诉我如何制作这个项目以及我应该使用哪些python库以及如何使用吗？]]></description>
      <guid>https://stackoverflow.com/questions/78747310/how-to-a-make-system-which-automatically-check-the-engineering-graphics-answer-s</guid>
      <pubDate>Sun, 14 Jul 2024 19:38:24 GMT</pubDate>
    </item>
    <item>
      <title>IndexError：目标 32 超出范围。运行时损失 = 标准（y_pred，y_train）[关闭]</title>
      <link>https://stackoverflow.com/questions/78747258/indexerror-target-32-is-out-of-bounds-while-running-loss-criteriony-pred-y</link>
      <description><![CDATA[我正在运行一个简单的神经网络，其中包含一些大约 1112 行、23 个输入、2 个隐藏层和 31 个可能输出的 csv 数据。在前向训练之后，在以下代码执行过程中，我收到以下错误消息
在行 loss = criterion(y_pred, y_train)
错误：
-----------------------------------------------------------------------------
IndexError Traceback（最近一次调用最后一次）
&lt;ipython-input-64-47488b841fa2&gt; 在 &lt;cell line: 5&gt;()
8 
9 
---&gt; 10 loss = criterion(y_pred, y_train)
11 
12 #loss.append(loss.detach().numpy())

3 帧
/usr/local/lib/python3.10/dist-packages/torch/nn/ functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)
3084 如果 size_average 不为 None 或 reduce 不为 None:
3085 reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 3086 返回 torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
3087 
3088 

IndexError：目标 32 超出范围。

有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78747258/indexerror-target-32-is-out-of-bounds-while-running-loss-criteriony-pred-y</guid>
      <pubDate>Sun, 14 Jul 2024 19:10:09 GMT</pubDate>
    </item>
    <item>
      <title>需要有关针对医疗领域的计算机视觉问题采取哪些措施的建议[关闭]</title>
      <link>https://stackoverflow.com/questions/78747070/need-advice-on-what-steps-to-take-regarding-a-computer-vision-problem-in-the-med</link>
      <description><![CDATA[我在实验室工作，遇到了一个问题，想得到一些指导。我在实验室的角色大致是计算机视觉科学家。我的任务是将一个领域的图像转换为另一个领域的图像，而不会丢失精细的细节。我需要将此图像转换为此图像。至少在结构上。我不想使用诸如 GAN 之类的工具，因为它们往往会添加多余的细节并以对我的任务有害的方式产生幻觉。我想尽可能地保留甚至带出图像的精细细节。有人能告诉我如何做到这一点吗？
我尝试过的一种方法是使用高斯展开或高斯近似。我的想法是，我添加并处理多个高斯函数，这样当我将它们相互添加时，它会创建一个近似的内核，我可以用它来卷积图像，从而最大化某种图像相似度得分（我目前正在使用 SSIM）。这是该方法的伪代码。这里有任何有效性吗？这不是确切的方法，因为我使用 scipy optimize 来处理优化步骤。
for i in N:
for j in N:
for m in N:
g1 = produce_gaussian(i)
g2 = produce_gaussian(j)
g3 = produce_gaussian(m)

final_gaussian = g1 + g2 + g3 
convolution = convolve2d(input_image, final_gaussian)
if SSIM(convolution, target_image)
set_values(g1, g2, g3)

我还尝试了维纳反卷积（image），总的来说，结果相当不错。有人能为我提供解决这个问题的指导吗？我想知道接下来可以采取哪些好的措施。]]></description>
      <guid>https://stackoverflow.com/questions/78747070/need-advice-on-what-steps-to-take-regarding-a-computer-vision-problem-in-the-med</guid>
      <pubDate>Sun, 14 Jul 2024 17:52:46 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow/Keras `load_img` 错误：“无法导入 PIL.Image”</title>
      <link>https://stackoverflow.com/questions/78746872/tensorflow-keras-load-img-error-could-not-import-pil-image</link>
      <description><![CDATA[尽管下载了枕头模块，我仍然遇到导入错误，是的，尽管我遇到了错误，但我还是卸载了枕头。有什么办法可以修复它吗？https://i.sstatic.net/eAhcTExv.png
我希望包含所有图像的文件夹被导入，这样我就可以通过导入的图像训练数据]]></description>
      <guid>https://stackoverflow.com/questions/78746872/tensorflow-keras-load-img-error-could-not-import-pil-image</guid>
      <pubDate>Sun, 14 Jul 2024 16:20:23 GMT</pubDate>
    </item>
    <item>
      <title>无法将 (Dimension(None)、Dimension(80)) 的元素转换为张量</title>
      <link>https://stackoverflow.com/questions/78746638/failed-to-convert-elements-of-dimensionnone-dimension80-to-tensor</link>
      <description><![CDATA[我正在尝试阅读 LibRecommender 中有关模型训练过程的教程：https://librecommender.readthedocs.io/en/latest/tutorial.html
我停在了训练模型阶段，代码如下：
model = WideDeep(
task=&quot;ranking&quot;,
data_info=data_info,
embed_size=16,
n_epochs=2,
loss_type=&quot;cross_entropy&quot;,
lr={&quot;wide&quot;: 0.05, &quot;deep&quot;: 7e-4},
batch_size=2048,
use_bn=True,
hidden_​​units=(128, 64, 32),
)

model.fit(
train_data,
neg_sampling=True, # 对训练和评估数据执行负抽样
verbose=2,
shuffle=True,
eval_data=eval_data,
metrics=[&quot;loss&quot;, &quot;roc_auc&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;ndcg&quot;],
)

我收到错误：
TypeError：调用 Flatten.call() 时遇到异常。

无法将 (Dimension(None)、Dimension(80)) 的元素转换为 Tensor。请考虑将元素转换为受支持的类型。请参阅 https://www.tensorflow.org/api_docs/python/tf/dtypes 了解受支持的 TF 数据类型。

Flatten.call() 接收的参数：
• 输入=tf.Tensor(shape=(?, 5, 16), dtype=float32)

我不知道为什么会收到此错误？我假设本教程中没有错误，我按照所示按 1:1 执行。
我在 PyCharm 环境中工作并使用 Jupyter 笔记本。]]></description>
      <guid>https://stackoverflow.com/questions/78746638/failed-to-convert-elements-of-dimensionnone-dimension80-to-tensor</guid>
      <pubDate>Sun, 14 Jul 2024 14:37:06 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow 中无需编译新模型即可增加神经网络层的大小</title>
      <link>https://stackoverflow.com/questions/78746621/increasing-the-size-of-a-neural-network-layer-without-compiling-a-new-model-in-t</link>
      <description><![CDATA[我正在训练一个窄 3 层 TensorFlow 神经网络，其层大小为 (input_size, small_number_hidden_​​units, output_size)，并使用学习到的权重作为更宽 3 层网络的初始条件的蓝图，其层大小为 (input_size, large_number_hidden_​​units, output_size)。我的目标是利用窄模型找到的解决方案，降低更宽模型的训练成本。除了隐藏单元的数量外，这两个模型都具有相同的架构。
是否可以通过使用单个模型并在需要时向其隐藏层添加单元来避免创建和编译两个模型的开销？例如，是否可以采用层大小为 (input_size, small_number_hidden_​​units, output_size) 的未经训练的网络，关闭大部分隐藏单元，以便在前向和后向传递过程中忽略它们，在该状态下训练网络以节省计算时间，然后在训练过程中的某个时间点打开所有隐藏单元并完成训练？
我曾考虑使用掩码关闭隐藏单元，如这篇文章中所述，但我不清楚这是否真的会降低计算成本。]]></description>
      <guid>https://stackoverflow.com/questions/78746621/increasing-the-size-of-a-neural-network-layer-without-compiling-a-new-model-in-t</guid>
      <pubDate>Sun, 14 Jul 2024 14:28:44 GMT</pubDate>
    </item>
    <item>
      <title>多线程 TFRecord 写入在 kaggle 笔记本上突然停止[关闭]</title>
      <link>https://stackoverflow.com/questions/78746204/multithreading-tfrecord-writing-stops-abruptly-on-kaggle-notebook</link>
      <description><![CDATA[我正在寻求有关在 Kaggle 笔记本中 TFRecord 写入的多线程方面的帮助。我正在研究 VGGFace2 数据集，旨在将图像对转换为 TFRecords，用于训练、验证和测试集，但该过程在单线程上运行速度过慢。
TFRecord 文件包含此配对图像的 protobuf 示例，以表示同一个人和不同的人。
挑战和我尝试过的方法：

单线程处理速度慢：即使是处理数据集的有限子集（每个目录 5 个图像对用于训练，每个目录 2 个图像对用于验证/测试），使用单线程也需要大量时间（可能长达 24 小时）。我已尽可能优化代码，因此我开始探索多线程以提高性能。

多线程问题：当我使用 concurrent.futures.ThreadPoolExecutor 实现多线程时，会话突然停止，没有任何错误消息。此外，所有变量都丢失，需要从头开始完全重新启动。有趣的是，使用单个目录时，多线程可以完美运行（因为我实现的多线程是同时处理不同的目录，所以即使使用多线程池执行器对象，处理单个目录也不再是多线程，而是单线程进程），但即使使用两个目录也会导致与上述相同的问题（VGGFace2 大约有 8631 个目录）。


我的问题：

潜在原因：这些多线程问题背后的原因可能是什么？这是 Kaggle 资源的内存限制吗？

替代方法：其他人是否遇到过类似的挑战？在 Kaggle 环境中，是否有其他方法可以加速 TFRecord 写入？


附加说明：

我正在使用 contextlib.ExitStack 库来打开许多写入器并同时写入。

我在 Kaggle 讨论和 Stack Overflow 上广泛搜索解决方案，但没有找到针对这种情况的具体解决方案。


您对 Kaggle 笔记本中的多线程有什么见解或经验，特别是在处理像 VGGFace2 这样的数据集时？]]></description>
      <guid>https://stackoverflow.com/questions/78746204/multithreading-tfrecord-writing-stops-abruptly-on-kaggle-notebook</guid>
      <pubDate>Sun, 14 Jul 2024 11:10:50 GMT</pubDate>
    </item>
    <item>
      <title>BERT 嵌入余弦相似度看起来非常随机且无用</title>
      <link>https://stackoverflow.com/questions/78744975/bert-embedding-cosine-similarities-look-very-random-and-useless</link>
      <description><![CDATA[我以为你可以使用 BERT 嵌入来确定语义相似性。我试图用这个将一些单词分组，但结果很糟糕。
例如，这是一个关于动物和水果的小例子。注意到相似度最高的是猫和香蕉吗？
import torch
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity

tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;, output_hidden_​​states=True).eval()

def gen_embedding(word):
encoding = tokenizer(word, return_tensors=&#39;pt&#39;)
with torch.no_grad():
output = model(**encoding)

token_embeddings = output.last_hidden_​​state.squeeze()
token_embeddings = token_embeddings[1 : -1]
word_embedding = token_embeddings.mean(dim=0)
return word_embedding

words = [
&#39;cat&#39;,
&#39;seagull&#39;,
&#39;mango&#39;,
&#39;banana&#39;
]

embs = [gen_embedding(word) for word in words]

print(cosine_similarity(embs))

# array([[1. , 0.33929926, 0.7086487 , 0.79372996],
# [0.33929926, 1.0000001 , 0.29915804, 0.4000572 ],
# [0.7086487 , 0.29915804, 1. , 0.7659105 ],
# [0.79372996, 0.4000572 , 0.7659105 , 0.99999976]], dtype=float32)

我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78744975/bert-embedding-cosine-similarities-look-very-random-and-useless</guid>
      <pubDate>Sat, 13 Jul 2024 20:58:49 GMT</pubDate>
    </item>
    <item>
      <title>BART 配备所有虚拟功能</title>
      <link>https://stackoverflow.com/questions/78731704/bart-with-all-dummy-features</link>
      <description><![CDATA[我正尝试将 BART 应用于分类问题，其中预测变量是虚拟变量以及 y 变量。我知道这是一种不常见的设置，但不幸的是这就是设置。实际上，0 和 1 值是从 -4 到 4 的分类变量中获得的，将负值设置为 0，将正值添加到 1。我还有数据的分类版本，以防它有用。
现在，我的预测变量包含大量 NA 值（即 70%），由 648x48 的 0-1 虚拟变量矩阵组成。我的 y 变量不包含缺失值，有 648 个值。
我目前正在使用 RStudio 在 R 中工作。然而，当我执行下面的代码时，结果却令人失望：
bart_machine = build_bart_machine(predictors, response_var,use_missing_data = TRUE, use_missing_data_dummies_as_covars = TRUE)
bart_machine$confusion_matrix

也就是说，我获得了一个 NULL 混淆矩阵，并且
bartMachine v1.3.4.1 用于回归

缺失数据功能开启
训练数据大小：n = 638 和 p = 96
在 8 个核心、50 棵树、250 个 burn-in 和 1000 个 post 上构建，耗时 5.8 秒。样本

事先对 y 的 sigsq 估计：0.016

老化后的平均 sigsq 估计：0.00314

样本内统计数据：
L1 = 9.91
L2 = 1.01
rmse = 0.04
Pseudo-Rsq = 0.9547
残差的 shapiro-wilk 正态性检验的 p-val：0

零均值噪声的 p-val：0.99451

现在我的问题是：

您是否认为我有太多 NA 值而无法执行 BART？

您认为我的设置至少应该产生一个混淆矩阵吗？

您认为数据的分类版本在这里可能更有帮助还是上述令人失望的结果是由于更深层次的原因吗？


编辑：我实际上已经进行了预测，但它们相当令人失望：
# 提取特征名称
feature_names &lt;- bart_machine[[&quot;training_data_features_with_missing_features&quot;]]

# 删除&quot;M_&quot;前缀
feature_names &lt;- gsub(&quot;^M_&quot;, &quot;&quot;, feature_names)

# 使用重命名的特征名称更新 bart_machine 对象
bart_machine[[&quot;training_data_features_with_missing_features&quot;]] &lt;- feature_names

# 检查 bart_machine 训练数据中的特征名称
training_data &lt;- bart_machine[[&quot;model_matrix_training_data&quot;]]
training_feature_names &lt;- colnames(training_data)

# 删除 &quot;M_&quot;列名中的前缀
training_feature_names &lt;- gsub(&quot;^M_&quot;, &quot;&quot;, training_feature_names)

# 使用重命名的列更新 bart_machine 对象
colnames(training_data) &lt;- training_feature_names
bart_machine[[&quot;model_matrix_training_data&quot;]] &lt;- training_data

# 使用相同方法在 albany2005_predictors 中插入缺失数据
imputed_data_2005 &lt;- mice(albany2005[, -1], m = 5, method = &#39;pmm&#39;, maxit = 50, seed = 500)
complete_data_2005 &lt;- complete(imputed_data_2005, 1)

# 删除 &quot;M_&quot; albany2005_predictors 中列名的前缀
colnames(complete_data_2005) &lt;- gsub(&quot;^M_&quot;, &quot;&quot;, colnames(complete_data_2005))

# 确保 albany2005_predictors 具有与训练数据相同的列，但不包括“y_remaining”
required_cols &lt;- setdiff(training_feature_names, &quot;y_remaining&quot;)
albany2005_predictors &lt;- complete_data_2005[, required_cols, drop = FALSE]

# 为 albany2005_response 中的非 NA 值创建逻辑向量
non_na_indices &lt;- !is.na(albany2005_response)

# 子集预测值和albany2005_response 使用 non_na_indices
non_na_predicted_values &lt;- predict_values[non_na_indices]
non_na_actual_values &lt;- albany2005_response[non_na_indices]

# 使用 bartMachine 模型对 2005 年数据进行预测
predicted_values &lt;- predict(bart_machine, albany2005_predictors, type = &quot;class&quot;)

# 计算 RMSE
# 将预测值转换为数字
non_na_predicted_values &lt;- as.numeric(as.character(non_na_predicted_values))

# 将实际值转换为数字
non_na_actual_values &lt;- as.numeric(as.character(non_na_actual_values))

rmse &lt;- sqrt(mean((non_na_predicted_values - non_na_actual_values)^2))

# 打印 RMSE
print(paste(&quot;RMSE: &quot;, rmse))。###非常高的 RMSE!!!
]]></description>
      <guid>https://stackoverflow.com/questions/78731704/bart-with-all-dummy-features</guid>
      <pubDate>Wed, 10 Jul 2024 16:18:19 GMT</pubDate>
    </item>
    <item>
      <title>什么是 x_train.reshape() 以及它的作用是什么？</title>
      <link>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</link>
      <description><![CDATA[使用 MNIST 数据集
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist

# MNIST 数据集参数
num_classes = 10 # 总类别（0-9 位数字）
num_features = 784 # 数据特征（图像形状：28*28）

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 转换为 float32
x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)

# 将图像展平为 784 个特征（28*28）的一维向量
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])

# 将图像值从 [0, 255] 标准化为 [0, 1]
x_train, x_test = x_train / 255., x_test / 255.

在这些代码的第 15 行中，
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])。我无法理解这些重塑在我们的数据集中到底起什么作用..?? 请解释一下。]]></description>
      <guid>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</guid>
      <pubDate>Sat, 02 May 2020 06:44:34 GMT</pubDate>
    </item>
    <item>
      <title>分类交叉熵和二元交叉熵之间的差异</title>
      <link>https://stackoverflow.com/questions/52965686/difference-between-categorical-and-binary-cross-entropy</link>
      <description><![CDATA[使用 keras，我必须训练一个模型来预测图像是属于类别 0 还是类别 1。我对 binary 和 categorical_cross_entropy 感到困惑。我已经搜索过但还是很困惑。有人提到，当我们试图预测多类时，我们只使用分类交叉熵，并且我们应该为此使用独热编码器向量。所以这意味着当我们要使用 binary_cross_entrpoy 进行训练时，我们不需要任何独热编码向量标签。有人建议将 binary_cross_entropy 的 one_hot 向量表示为 [0. 1.]（如果类别为 1）或 [1. 0.]（如果类别为 0）。
我使用独热编码器 [0 1] 或 [1 0] 和分类交叉熵。我的最后一层是
model.add(Dense(num_classes,activation=&#39;softmax&#39;))

# 编译模型
model.compile(loss=&#39;categorical_crossentropy&#39;, 
optimizer=&#39;adadelta&#39;, 
metrics=[&#39;accuracy&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/52965686/difference-between-categorical-and-binary-cross-entropy</guid>
      <pubDate>Wed, 24 Oct 2018 09:36:45 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 聚类：预测（X）与 fit_predict（X）</title>
      <link>https://stackoverflow.com/questions/37106983/scikit-learn-clustering-predictx-vs-fit-predictx</link>
      <description><![CDATA[在 scikit-learn 中，一些聚类算法同时具有 predict(X) 和 fit_predict(X) 方法，例如 KMeans 和 MeanShift，而其他算法仅具有后者，例如 SpectralClustering。根据文档：
fit_predict(X[, y]): 对 X 执行聚类并返回聚类标签。
predict(X): 预测 X 中每个样本所属的最接近聚类。

我不太明白这两者之间的区别，在我看来它们似乎是等价的。]]></description>
      <guid>https://stackoverflow.com/questions/37106983/scikit-learn-clustering-predictx-vs-fit-predictx</guid>
      <pubDate>Mon, 09 May 2016 02:25:29 GMT</pubDate>
    </item>
    </channel>
</rss>