<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 30 Mar 2024 18:17:22 GMT</lastBuildDate>
    <item>
      <title>支持向量机分类的增量学习</title>
      <link>https://stackoverflow.com/questions/78248733/incremental-learning-for-support-vector-machines-classification</link>
      <description><![CDATA[我目前正在尝试为 Cawenbergs 和 Poggio 的增量和减量 SVM 算法找到一个好的实现。我找到了这个： Incremental-SVM-Learning-in- MATLAB，但我不太明白该算法在MatLab代码中是如何实现的。
我试图深入分析它并研究它，但仍然没有成功地理解它。我正在寻求一些帮助来理解代码，甚至寻求其他具有更清晰实现的解决方案。
非常欢迎任何帮助，你会给我一个很大的帮助！
这是我目前关注的算法：
Cawenbergs 和 Poggio 算法]]></description>
      <guid>https://stackoverflow.com/questions/78248733/incremental-learning-for-support-vector-machines-classification</guid>
      <pubDate>Sat, 30 Mar 2024 16:45:09 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 数据加载器中的 Snuffle</title>
      <link>https://stackoverflow.com/questions/78248552/snuffle-in-pytorch-dataloader</link>
      <description><![CDATA[我在 PyTorch 中有一个返回文本和图像的自定义数据集。我有一个关于数据加载器中的鼻烟的问题。他能不能把文字和对应的图片混合起来，也就是让图片和文字不匹配？
我不知道我能用它做什么]]></description>
      <guid>https://stackoverflow.com/questions/78248552/snuffle-in-pytorch-dataloader</guid>
      <pubDate>Sat, 30 Mar 2024 15:54:18 GMT</pubDate>
    </item>
    <item>
      <title>Pyarrow：导入错误：/lib/x86_64-linux-gnu/libc.so.6：找不到版本“GLIBC_2.28”</title>
      <link>https://stackoverflow.com/questions/78248485/pyarrow-importerror-lib-x86-64-linux-gnu-libc-so-6-version-glibc-2-28-not</link>
      <description><![CDATA[我正在尝试重现液体 s4 纸的结果，但遇到了无法导入 Pyarrow 的问题。整个错误消息为：import pyarrow as pa
回溯（最近一次调用最后一次）：
  文件“”，第 1 行，位于  中。
  文件“/home/nfs/state_space_model/.local/lib/python3.10/site-packages/pyarrow/__init__.py”，第 65 行，在  中
    将 pyarrow.lib 导入为 _lib
ImportError：/lib/x86_64-linux-gnu/libc.so.6：找不到版本“GLIBC_2.28”（/home/nfs/path/.local/lib/python3.10/site-packages/pyarrow/需要） libarrow.so.1500)

这似乎与我的linux系统有关。但我不知道从哪里开始。以下是我当前的环境设置和有关我的 GPU 的信息。
&lt;前&gt;&lt;代码&gt;absl-py==1.4.0
aiohttp==3.9.3
aiosignal==1.3.1
antlr4-python3-runtime==4.9.3
应用程序目录==1.4.4
异步超时==4.0.3
属性==23.2.0
证书==2024.2.2
字符集标准化器==3.3.2
切克斯==0.1.6
点击==8.1.7
cmake==3.29.0.1
轮廓py==1.2.0
循环仪==0.12.1
数据集==2.18.0
莳萝==0.3.8
dm-树==0.1.8
docker-pycreds==0.4.0
埃诺普斯==0.7.0
文件锁==3.13.1
字体工具==4.50.0
冻结列表==1.4.1
fsspec==2024.2.0
未来==1.0.0
gitdb==4.0.11
GitPython==3.1.42
googleapis-common-protos==1.63.0
grpcio==1.62.1
拥抱脸集线器==0.21.4
Hydra 核心==1.3.2
idna==3.6
Jinja2==3.1.3
joblib==1.3.2
keopscore==2.2.2
奇异解算器==1.4.5
闪电实用程序==0.11.2
降价==3.6
markdown-it-py==3.0.0
标记安全==2.1.5
matplotlib==3.8.3
mdurl==0.1.2
ml-dtypes==0.3.2
mpmath==1.3.0
消息包==1.0.8
多重字典==6.0.5
多进程==0.70.16
网络x==3.2.1
numpy==1.26.4
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu12==2.19.3
nvidia-nvjitlink-cu12==12.4.99
nvidia-nvtx-cu12==12.1.105
omegaconf==2.3.0
opt-einsum==3.3.0
光通量==0.1.7
包装==24.0
熊猫==2.2.1
枕头==10.2.0
承诺==2.3
协议缓冲区==3.20.3
psutil==5.9.8
pyarrow==15.0.2
pyarrow-修补程序==0.6
pybind11==2.12.0
pyDeprecate==0.3.1
pydub==0.25.1
皮格门斯==2.17.2
pykeops==2.2.2
pyparsing==3.1.2
python-dateutil==2.9.0.post0
pytorch-闪电==1.5.10
pytz==2024.1
PyYAML==6.0.1
正则表达式==2023.12.25
请求==2.31.0
丰富==13.7.1
安全张量==0.4.2
scikit-learn==1.4.1.post1
scipy==1.12.0
哨兵-sdk==1.44.0
setproctitle==1.3.3
六==1.16.0
smmap==5.0.1
sympy==1.12
张量板==2.16.2
张量板数据服务器==0.7.2
张量流数据集==4.5.2
张量流元数据==1.14.0
术语颜色==2.4.0
threadpoolctl==3.4.0
分词器==0.15.2
工具z==0.12.1
火炬==2.2.1
火炬音频==2.2.1
火炬数据==0.7.1
火炬测量==1.3.2
火炬文本==0.17.1
火炬视觉==0.17.1
tqdm==4.66.2
变形金刚==4.39.2
海卫一==2.2.0
打字扩展==4.10.0
tzdata==2024.1
urllib3==2.2.1
万宝==0.16.5
武器==3.0.1
xxhash==3.4.1
雅尔==1.9.4

我使用的是 RTX3060 GPU。
&lt;前&gt;&lt;代码&gt;+-------------------------------------------------------- ----------------------------------+
| NVIDIA-SMI 525.60.13 驱动程序版本：525.60.13 CUDA 版本：12.0 |
|------------------------------------------+----------------- ---+----------------------+
| GPU 名称持久性-M|总线 ID Disp.A |挥发性未校正。 ECC |
|风扇温度性能功率：使用/上限|内存使用情况 | GPU-Util 计算 M。
| | |米格·M。
|================================+================== ====+======================|
| 0 NVIDIA GeForce ... 开 | 00000000:01:00.0 关闭 |不适用 |
| 30% 33C P8 13W / 170W | 1MiB / 12288MiB | 0% 默认 |
| | |不适用 |
+--------------------------------------------+----------------- ---+----------------------+
                                                                               
+------------------------------------------------ ----------------------------+
|流程： |
| GPU GI CI PID 类型 进程名称 GPU 内存 |
| ID ID 用途 |
|=================================================== ============================|
|未找到正在运行的进程 |
+------------------------------------------------ ----------------------------+

有谁知道可能出现什么问题以及如何修复它们？任何帮助表示赞赏。如果您需要更多信息，请告诉我。]]></description>
      <guid>https://stackoverflow.com/questions/78248485/pyarrow-importerror-lib-x86-64-linux-gnu-libc-so-6-version-glibc-2-28-not</guid>
      <pubDate>Sat, 30 Mar 2024 15:30:54 GMT</pubDate>
    </item>
    <item>
      <title>建议将 Gymnasium 与神经网络结合使用，以避免 model.fit 和 model.predict 中的开销</title>
      <link>https://stackoverflow.com/questions/78248300/recommended-way-to-use-gymnasium-with-neural-networks-to-avoid-overheads-in-mode</link>
      <description><![CDATA[我正在尝试 Sutton &amp; 的情景半梯度 Sarsa。 Barto 第 10 章（第二版）使用 Gymnasium 的 CartPole 问题。对于函数逼近，我使用带有 keras 的神经网络。然而，忠实地实现该算法迫使我使用批量大小 1 进行拟合和预测，这会导致代码极其缓慢。另一种方法是首先运行代码从体育馆收集数据，然后使用这些数据离线训练神经网络。是否建议这样做 - 如果我理解正确的话，它会离线但仍然符合政策）？或者是否有其他标准方法可以在 Gymnasium 中使用神经网络而不影响性能？

我当前尝试的概述 -
将gymnasium导入为gym
从 numpy.random 导入选择作为 random_choice
从 numpy 导入数组，argmax

我将算法编写为以下 python 代码：
env =gym.make(&#39;CartPole-v1&#39;)

对于范围内的 ep_idx（num_episodes）：
    终止=假
    状态, _ = env.reset()
    动作 = env.action_space.sample()
    未终止时：
        action_=policy.take_action（状态，qvalue，ep_idx）
        状态_，奖励，终止，_，_ = env.step(action_)
        如果终止：
            qvalue.update(状态、动作、奖励、无、无)
        别的：
            qvalue.update(状态、动作、奖励、state_、action_)
        状态，动作 = 状态_，动作_

对于函数逼近，我决定使用 Keras。这是在 qvalue.update 内部实现的，如下所示：
类 QValueFunction：
    def __init__(自我、折扣、学习率、num_actions、*state_vector_dim):
        # 为简洁起见，此处未显示
    def __call__(自我，状态，动作=无)：
        # 为简洁起见，此处未显示
    def更新（自我，s，a，r，s_，a_）：
        model = self._model # keras.models.Model 的实例
        gamma = self._discount # 浮动
        update_targets = self._update_targets # 预分配的 numpy 数组
        q = 自身
        update_targets[:] = q(s, 无)
        self._s[:] = s
        s = self._s
        如果 s_ 为 None 并且 a_ 为 None：
            update_targets[0, a] = r
        别的：
            update_targets[0, a] = r + gamma * q(s_, a_)
        model.fit(s、update_targets、batch_size=1、verbose=0)

并且 policy 是 EpsilonGreedyPolicy 的实例：
类 EpsilonGreedyPolicy：
    def __init__(self, epsilon):
        self.eps = epsilon
    def take_action(self, 状态, qvalue, ep=None):
        num_actions = qvalue.num_actions
        如果可调用(self.eps): eps = self.eps(ep+1)
        否则： eps = self.eps
        如果 rand() &lt;每股收益：
            返回随机选择(num_actions)
        别的：
            qvalues = qvalue(状态)
            返回 argmax(qvalues)

上面的代码在我的笔记本电脑上以每 10 秒大约 1 集的速度运行（仅 CPU）。为了检查代码实际运行的速度，我尝试使用随机策略 (eps=1) 生成 1000 个剧集的数据，生成 20000 多个 (s, a, r, s_, a_) 元组 。这仅需要大约 10 秒。接下来，我使用这些数据分别训练神经网络，通过将所有数据一次传递到 model.predict 和 model.fit&lt;，每 10000 个数据点大约需要 1 秒Keras 的 /code&gt;。本质上，使用批量大小为 1 的 model.fit 和 model.predict 忠实地按照算法运行代码需要 10000 秒，而按 (i) 首先生成数据 (ii) 接下来训练神经网络的方式运行则需要 10 到 100 秒秒。
有没有推荐的方法在 Gymnasium 中使用神经网络来避免如此沉重的开销？]]></description>
      <guid>https://stackoverflow.com/questions/78248300/recommended-way-to-use-gymnasium-with-neural-networks-to-avoid-overheads-in-mode</guid>
      <pubDate>Sat, 30 Mar 2024 14:32:09 GMT</pubDate>
    </item>
    <item>
      <title>我如何在 coursera 上编辑“唤醒词检测笔记本”，使其适合我自己的单词？</title>
      <link>https://stackoverflow.com/questions/78248216/how-can-i-edit-the-wake-word-detection-notebook-on-coursera-so-it-fit-my-own-w</link>
      <description><![CDATA[我已经理解并解决了 Coursera 上 Andrew Ng 提供的深度学习专业化（序列模型课程）笔记本。
在笔记本中，他提供了构建唤醒词检测模型的详细演练。然而，最后，他加载了一个针对“激活”一词进行训练的预训练模型。
我尝试使用 Google Colab 和我自己的数据。我收集了 369 个人们说“Alexa”的声音。这些都可以在 Kaggle 上找到。然而，它们的采样率为 16000KHz。
我还使用 Google 语音命令作为负面声音，并从 YouTube 收集了一些包含各种环境声音的剪辑。
我完全按照说明执行了所有步骤，并且我正在使用 google colab 进行训练。但是当我尝试创建数据集时，RAM 很快就填满了，并且我无法创建 Andrew 在他的笔记本中提到的 4000 个样本。
这是我的“create_training_examples”代码：
&lt;前&gt;&lt;代码&gt;n 样本 = 4000
X_train = []
Y_train=[]
X_测试 = []
Y_测试 = []
火车计数 = 0
测试计数 = 0

测试=假
对于范围内的 i（0，nsamples）：

    如果我％500==0：
      打印（一）
    兰特 = random.randint(0,61)

    如果 i%5 == 0:


      x, y = create_data_example(backgrounds_list[rand], alexa_list, negatives_list, Ty, name=str(i),to_test = True)
      X_test.append(x.swapaxes(0,1))
      Y_test.append(y.swapaxes(0,1))
      测试计数+=1

    别的：

      x, y = create_data_example(backgrounds_list[rand], alexa_list, negatives_list, Ty, name=str(i),to_test = False)
      X_train.append(x.swapaxes(0,1))
      Y_train.append(y.swapaxes(0,1))
      火车计数+=1


print(&quot;训练样本数：&quot;, train_count)
print(&quot;测试样本数：&quot;, test_count)


X_train = np.array(X_train)
Y_train = np.array(Y_train)

np.save(&#39;XY_train/X_train.npy&#39;, X_train)
np.save(&#39;XY_train/Y_train.npy&#39;, Y_train)


X_test = np.array(X_test)
Y_test = np.array(Y_test)
np.save(&#39;XY_test/X_test.npy&#39;, X_test)
np.save(&#39;XY_test/Y_test.npy&#39;, Y_test)


print(&#39;保存完毕&#39;)

print(&#39;X_train.shape: &#39;,X_train.shape)
print(&#39;Y_train.shape: &#39;,Y_train.shape)



这是我使用的模型：
def 模型(input_shape):

    
    X_input = 输入（形状 = input_shape）
    

    X = Conv1D(196,15,步长=4)(X_输入)
    X = BatchNormalization()(X)
    X = 激活(&#39;relu&#39;)(X)
    X = 辍学(0.8)(X)

    
    X = GRU(128,return_sequences = True)(X)
    X = 辍学(0.8)(X)
    X = BatchNormalization()(X)
    
    
    X = GRU(128,return_sequences=True)(X)
    X = 辍学(0.8)(X)
    X = BatchNormalization()(X)
    X = 辍学(0.8)(X)
    
    X = TimeDistributed(Dense(1,activation = &quot;sigmoid&quot;))(X) # 时间分布 (sigmoid)


    模型 = 模型（输入 = X_输入，输出 = X）
    
    返回模型



以及训练：
opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, 衰减=0.01)
model.compile(loss=&#39;binary_crossentropy&#39;, 优化器=opt, 指标=[“准确度”])
model.fit（X_train，Y_train，batch_size = 5，epochs = 20，validation_data =（X_test，Y_test））



我按原样使用笔记本，甚至采用相同的方法进行特征提取。 我唯一修改的是训练数据，使用的是我自己的数据。但是，RAM 很快就被填满了。当我将样本数量减少到 1600KHz 或 8000KHz 时，我根本没有得到好的结果。
我还尝试编辑batch_size、learning_rate。
没有任何改变..
我做错了什么吗？
请问您有什么意见或建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78248216/how-can-i-edit-the-wake-word-detection-notebook-on-coursera-so-it-fit-my-own-w</guid>
      <pubDate>Sat, 30 Mar 2024 14:03:37 GMT</pubDate>
    </item>
    <item>
      <title>python中的批量梯度下降算法返回巨大的值</title>
      <link>https://stackoverflow.com/questions/78248203/batch-gradient-descent-algorithm-in-python-is-returning-huge-values</link>
      <description><![CDATA[我正在尝试在 python 中实现批量梯度下降算法，该算法将训练集、学习率和迭代次数作为输入参数，并返回权重。然而，当我运行它时，在几次迭代内，参数的值呈指数级增长，最终返回“nan”。
x = [[2104] [1600] [2400] [1416] [3000] [1985] [1534] [1427] [1380] [1494] [1940] [2000] [1890] [4478] [1268 ] [2300] [1320] [1236] [2609] [3031] [1767] [1888] [1604] [1962] [3890] [1100] [1458] [2526] [2200] [2637] [1839] [ 1000][2040][3137][1811][1437][1239][2132][4215][2162][1664][2238][2567][1200][852][1852][1203]]
y = [399900 329900 369000 232000 539900 299900 314900 198999 212000 242500 239999 347000 329999 699900 259900 449900 299900 199900 499998 599000 252900 255000 242900 259900 573900 249900 464500 469000 475000 299900 349900 169900 314900 579900 28590 0 249900 229900 345000 549000 287000 368500 329900 314000 299000 179900 299900 239500]
a = 0.01
num_iter = 100
def BGD ( x, y, a, num_iter):
    m = len(x) #样本数
    n = x.shape[1] #特征数量
    p = np.zeros(n)
    b = 0
    对于 _ 在范围内（num_iter）：
        sum_p = np.zeros(n)
        总和 = 0
        对于范围 (m) 内的 i：
            sum_p = sum_p + ((np.dot(p,x[i])+b) - y[i]) * x[i]
            sum_b = sum_b + (((np.dot(p,x[i])+b) - y[i]))
        p = p - (a * (1/m) * sum_p)
        b = b - (a * (1/m) * sum_b)
    返回 p、b

p, b = BGD(x, y, 0.01, 100)
打印（页）
打印(b)

我得到以下信息：
RuntimeWarning：add 中遇到溢出
sum_p = sum_p + ((np.dot(p,x[i])+b) - y[i]) * x[i]
RuntimeWarning：减法中遇到无效值
p = p - (a * (1/m) * sum_p)
[楠]
楠]]></description>
      <guid>https://stackoverflow.com/questions/78248203/batch-gradient-descent-algorithm-in-python-is-returning-huge-values</guid>
      <pubDate>Sat, 30 Mar 2024 14:00:37 GMT</pubDate>
    </item>
    <item>
      <title>Word2Vec 向量作为其他特征之一</title>
      <link>https://stackoverflow.com/questions/78248032/word2vec-vectors-as-one-feature-among-others</link>
      <description><![CDATA[我正在构建一个工具来根据单元格值、坐标和格式来标记发票字段。所述发票采用 Excel 格式，因此我迭代每个单元格以获取其属性并将它们存储在数据框中。坐标按 MinMax 缩放，而格式则变成虚拟的。然而，值是通过 Word2Vec 进行矢量化的。
现在我需要将向量添加回数据帧，但我不知道该怎么做。正如您在下图中看到的，这些集群相当可区分，但它们需要其他功能才能像我需要的那样工作。
矢量散点图
列表（称为“文本”）和数据框列如下所示：[&#39;fl&#39;, &#39;三&#39;], [&#39;四&#39;], [&#39;pc&#39;], [&#39;onetwo&#39;], [&#39;三十三&#39;] &#39;]、[&#39;一&#39;]、[&#39;十二&#39;]、[&#39;五&#39;]...]
这是通过 word2vec 传递列表的代码。我现在需要将向量发送回数据帧或其 x/y 分量。你们认为哪个最好？我该怎么做？
texts = df_separado[&#39;valor&#39;].str.split(&#39; &#39;).tolist()

矢量大小 = 100
MIN_COUNT = 5
窗口 = 1
SG = 1

模型 = Word2Vec(
    句子=文本，
    矢量大小=矢量大小，
    min_count=MIN_COUNT,
    sg=SG
）

def reduce_dimensions（模型）：
    组件数量 = 2

    向量 = np.asarray(model.wv.向量)
    标签 = np.asarray(model.wv.index_to_key)

    tsne = TSNE(n_components=num_components, random_state=0)
    向量_tsne = tsne.fit_transform（向量）

    缩放器 = MinMaxScaler(feature_range=(0, 1))
    向量 = scaler.fit_transform(vectors_tsne)

    x_vals = [v[0] 对于向量中的 v]
    y_vals = [v[1] 对于向量中的 v]
    返回 x_vals、y_vals、标签


defplot_embeddings（x_vals，y_vals，标签）：
    导入plotly.graph_objs作为go
    图 = go.Figure()
    跟踪 = go.Scatter(x=x_vals, y=y_vals, mode=&#39;markers&#39;, text=labels)
    图.add_trace(跟踪)
    Fig.update_layout(title=&quot;Word2Vec - TSNE&quot;)
    图.show()
    返回无花果

x_vals、y_vals、标签=reduce_dimensions(模型)

情节=情节嵌入（x_vals，y_vals，标签）
]]></description>
      <guid>https://stackoverflow.com/questions/78248032/word2vec-vectors-as-one-feature-among-others</guid>
      <pubDate>Sat, 30 Mar 2024 13:04:09 GMT</pubDate>
    </item>
    <item>
      <title>在 M2 GPU 上进行 PyTorch 训练比 Colab CPU 慢</title>
      <link>https://stackoverflow.com/questions/78247818/pytorch-training-on-m2-gpu-slower-than-colab-cpu</link>
      <description><![CDATA[我对深度学习/机器学习相当陌生，我一直在尝试使用我的 Apple M2 在 Jupyter 笔记本中加速我的 CNN 训练，但是，我发现尽管使用“mps”，但每个周期的执行速度要慢得多（ 1m 30-40 秒）比 Google Colab CPU 每个周期花费 40 秒。
我不确定为什么会出现这种情况，并且想知道是否有人可以帮助我理解为什么会这样或者我可能做错了什么。
我正在检查可用性和 PyTorch 版本并相应地使用它：
# 检查 PyTorch 是否可以访问 MPS（Metal Performance Shader，Apple 的 GPU 架构）
print(f“MPS（金属性能着色器）构建了吗？{torch.backends.mps.is_built()}”)
print(f&quot;MPS 可用吗？{torch.backends.mps.is_available()}&quot;)

# 设置设备
设备=“mps”； if torch.backends.mps.is_available() else “cpu”
print(f“使用设备：{device}”)

print(&quot;版本：&quot;, torch.__version__)
=======================================
MPS（金属性能着色器）是否已构建？真的
MPS 可用吗？真的
使用设备：mps
版本：2.2.2
]]></description>
      <guid>https://stackoverflow.com/questions/78247818/pytorch-training-on-m2-gpu-slower-than-colab-cpu</guid>
      <pubDate>Sat, 30 Mar 2024 11:57:27 GMT</pubDate>
    </item>
    <item>
      <title>无法解决 Pandas 中的 KeyError</title>
      <link>https://stackoverflow.com/questions/78247772/cant-resolve-keyerror-in-pandas</link>
      <description><![CDATA[我编写了此代码来在从 HuggingFace 导入的数据集上微调 gpt2：
从 Transformers 导入 Trainer、TrainingArguments、GPT2LMHeadModel、GPT2Tokenizer
model_name = “gpt2”;
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
模型 = GPT2LMHeadModel.from_pretrained(model_name)

训练参数 = 训练参数（
输出目录=“./输出”,
num_train_epochs=3,
per_device_train_batch_size=8，
logging_dir=&quot;./logs&quot;,
）

教练=教练（
型号=型号，
参数=训练参数，
train_dataset=火车，
分词器=分词器，
）
训练师.train()`

当我运行此命令时，我收到 KeyError:9198，尽管 9198 包含在我的数据集的数据框中（我检查了两次）
这是我遇到的错误：
KeyError Traceback（最近一次调用最后一次）
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method,lerance)
第3801章 试试：
-&gt;第3802章
第3803章
11帧
pandas._libs.hashtable.PyObjectHashTable.get_item() 中的 pandas/_libs/hashtable_class_helper.pxi
pandas/_libs/hashtable_class_helper.pxi 在 pandas._libs.hashtable.PyObjectHashTable.get_item()
密钥错误：9198
上述异常是导致以下异常的直接原因：
KeyError Traceback（最近一次调用最后一次）
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method,lerance)
第3802章
第3803章
-&gt;第3804章
第3805章
第3806章
密钥错误：9198]]></description>
      <guid>https://stackoverflow.com/questions/78247772/cant-resolve-keyerror-in-pandas</guid>
      <pubDate>Sat, 30 Mar 2024 11:39:34 GMT</pubDate>
    </item>
    <item>
      <title>我应该用一组图片作为一个输入数据来训练我的模型，还是需要使用 Pytorch 裁剪成小图片</title>
      <link>https://stackoverflow.com/questions/78247685/should-i-train-my-model-with-a-set-of-pictures-as-one-input-data-or-i-need-to-cr</link>
      <description><![CDATA[我看到两种训练模型的方法：

基于一张图片进行训练以及我对决策的真实预测
基于包含一组图片的一张大图片进行训练，并根据我对决策的真实预测进行类似的训练

预测输入可能是一张图片或一组图片
如果您有建议，很高兴听到您的最佳实践
如何正确提供培训输入
如何正确给出预测输入
现在，我继续使用输入作为集合和大的分离图像来训练我的模型。
我收到基于一张图片或一组预测输入的答案。]]></description>
      <guid>https://stackoverflow.com/questions/78247685/should-i-train-my-model-with-a-set-of-pictures-as-one-input-data-or-i-need-to-cr</guid>
      <pubDate>Sat, 30 Mar 2024 11:11:48 GMT</pubDate>
    </item>
    <item>
      <title>无法打开 shape_predictor_68_face_landmarks.dat</title>
      <link>https://stackoverflow.com/questions/78247670/unable-to-open-shape-predictor-68-face-landmarks-dat</link>
      <description><![CDATA[我正在尝试使用 dlib 裁剪图像来检测人脸，但我不断收到此错误，错误
我已将 .dat 文件下载到存储 .ipynb 文件的同一目录中，但问题仍然存在。我从 dlib.net 下载了该文件。]]></description>
      <guid>https://stackoverflow.com/questions/78247670/unable-to-open-shape-predictor-68-face-landmarks-dat</guid>
      <pubDate>Sat, 30 Mar 2024 11:04:21 GMT</pubDate>
    </item>
    <item>
      <title>R 中没有为任何变量提供梯度</title>
      <link>https://stackoverflow.com/questions/78247273/no-gradients-provided-for-any-variable-in-r</link>
      <description><![CDATA[我正在使用 R 和 MNIST 数据集进行深度学习。
我编写了这段代码来存储训练和测试数据，并定义和拟合模型：
库（keras）

#获取数据
mnist &lt;- dataset_mnist()
train_data &lt;- mnist$train$x
train_labels &lt;- mnist$train$y
test_data &lt;- mnist$test$x
test_labels &lt;- mnist$test$y

#重塑&amp;正常化
train_data &lt;- array_reshape(train_data,c(nrow(train_data), 784))
训练数据 &lt;- 训练数据 / 255
test_data &lt;- array_reshape(test_data,c(nrow(test_data), 784))
测试数据 &lt;- 测试数据 / 255

#一个热编码 train_labels &lt;- to_categorical(train_labels, 10)
测试标签 &lt;- to_categorical(测试标签, 10)

＃模型
模型 &lt;- keras_model_sequential()
模型％&gt;％layer_dense（单位= 128，激活=“relu”，input_shape = c（784））％&gt;％
            Layer_dropout(率=0.3) %&gt;%
            Layer_dense(单位=64，激活=“relu”)%&gt;%
            Layer_dropout(率=0.2) %&gt;%
            Layer_dense（单位= 10，激活=“softmax”）

#编译
模型%&gt;%编译(loss=“categorical_crossentropy”,
                    优化器=“rmsprop”，
                    指标=“准确性”）
    
＃火车
历史 &lt;- 模型 %&gt;% fit(train_data,
                        火车标签，
                        纪元=10，
                        批量大小=784，
                        验证分割=0.2，
                        详细=2)
    
#评估与预测
模型 %&gt;% 评估(test_data, test_labels)
pred &lt;- 模型 %&gt;% 预测(test_data)
打印（表（预测= pred，实际= test_labels））

在R studio中运行时，出现以下错误：
ValueError：没有为任何变量提供渐变：([&#39;dense_124/kernel:0&#39;, &#39;dense_124/bias:0&#39;, &#39;dense_123/kernel:0&#39;, &#39;dense_123/bias:0&#39;, &#39;密集_122 /内核：0&#39;，&#39;密集_122 /偏差：0&#39;]，）。假设 `grads_and_vars` 为 ((None, ), (None, ), (无, ), (无, ), (无, ), (无, ))。

我认为问题可能在于输入数据和输入的形状冲突，但不知道如何解决这个问题。
感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78247273/no-gradients-provided-for-any-variable-in-r</guid>
      <pubDate>Sat, 30 Mar 2024 08:30:30 GMT</pubDate>
    </item>
    <item>
      <title>从二维输入预测多个输出的回归问题</title>
      <link>https://stackoverflow.com/questions/78247231/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</link>
      <description><![CDATA[我有几个二维图表，每个图表都有七个独特的数字特征，可用于生成这些图表。我以大量 CSV 文件的形式获得了所有这些图表的 x 和 y 坐标及其数值特征。我想通过使用机器学习或深度学习模型来预测每个图的数值特征（通过使用图的图像或使用每个图的点的坐标）
例如，这是我的一张图表：

该图的独特数字特征是 [8.76e15, 8e-1, 5e-2, 5e-3, 5e-2, 9.65e-1, 2.1e-9] （我有该图所有点的坐标对 (x, y) 以两列 CSV 文件的形式存在，我也可以使用它们）。
到目前为止，我已经寻找了很多预训练的模型，并在 HuggingFace 等网站上搜索了此类模型，还在 GitHub 代码中搜索了很多。我还在 Papers with Code 网站上搜索了做过同样事情的文章，但不幸的是，我仍然没有找到任何东西！我曾多次尝试自己编写一个网络，但由于这样做的复杂性以及对于如何设置网络的超参数以达到预期结果的知识不够，我遇到了很多错误并且无法做到这一点！
例如，我编写了以下代码：
X = []
y = []
目录=“数据”；
对于 os.listdir（目录）中的 csv_file：
    data = pd.read_csv(f&quot;{目录}/{csv_file}&quot;)
    X.append(data.iloc[1:, :2].astype(float).values)
    y.append(data.iloc[0, 2:].astype(float).values)
X = np.array(X, dtype=np.float64) # X.shape: (50000, 253, 2)
y = np.array(y, dtype=np.float64) # y.shape: (50000, 7)

X_train = X[:40000,:,:]
X_val = X[40000:, :, :]
y_train = y[:40000,:]
y_val = y[40000:, :]

定标器=标准定标器()
X_train_scaled = 缩放器.fit_transform(X_train)
X_val_scaled = 缩放器.fit_transform(X_val)

输入 = keras.layers.Input(shape=(X.shape[1], X.shape[2]))
lstm_out = keras.layers.LSTM(32)(输入)
输出 = keras.layers.Dense(7)(lstm_out)

模型= keras.Model（输入=输入，输出=输出）
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=“mse”)
模型.summary()

历史=模型.fit(
    x=X_train,
    y = y_train，
    纪元=10，
）

损失非常高，而且一点也不好。
我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78247231/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</guid>
      <pubDate>Sat, 30 Mar 2024 08:11:00 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-Learn 排列和更新 Polars DataFrame</title>
      <link>https://stackoverflow.com/questions/78246736/scikit-learn-permutating-and-updating-polars-dataframe</link>
      <description><![CDATA[我正在尝试重写scikit-learn 排列重要性要实现的：

与 Polar 的兼容性
与功能集群的兼容性

将极坐标导入为 pl
将 Polars.selectors 导入为 cs
将 numpy 导入为 np

从 sklearn.datasets 导入 make_classification
从 sklearn.model_selection 导入 train_test_split

X, y = make_classification(
    n_样本=1000，
    n_特征=10，
    n_信息=3，
    n_冗余=0，
    n_重复=0，
    n_classes=2,
    随机状态=42，
    随机播放=假，
）
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)
feature_names = [f&quot;feature_{i}&quot;;对于范围内的 i(X.shape[1])]

X_train_polars = pl.DataFrame(X_train, schema=feature_names)
X_test_polars = pl.DataFrame(X_test, schema=feature_names)
y_train_polars = pl.Series(y_train, schema=[“目标”])
y_test_polars = pl.Series(y_test, schema=[“目标”])

为了获得一组特征的未来重要性，我们需要同时排列一组特征，然后传递给评分器以与基线分数进行比较。
但是，在检查特征簇时，我正在努力替换多个极坐标数据框列：
from sklearn.utils import check_random_state
随机状态=检查随机状态(42)
random_seed = random_state.randint(np.iinfo(np.int32).max + 1)

X_train_permuted = X_train_polars.clone()
shuffle_arr = np.array(X_train_permuted[:, [“feature_0”, “feature_1”]])

random_state.shuffle(shuffle_arr)
X_train_permuted.replace_column( # 这个操作到位
                0,
                pl.Series(name=“feature_0”,values=shuffle_arr))

通常，shuffle_arr 的形状为 (n_samples,)，可以使用 polars.DataFrame.replace_column() 轻松替换 Polars 数据帧中的相关列。在这种情况下，shuffle_arr 的多维形状为（簇中的 n_samples，n_features）。替换相关列的有效方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78246736/scikit-learn-permutating-and-updating-polars-dataframe</guid>
      <pubDate>Sat, 30 Mar 2024 03:20:39 GMT</pubDate>
    </item>
    <item>
      <title>解决从 Jupyter Notebook 到 .py 文件的自定义管道类转换中的 OneHotEncoder 问题</title>
      <link>https://stackoverflow.com/questions/78219825/troubleshooting-onehotencoder-issue-in-custom-pipeline-class-conversion-from-jup</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78219825/troubleshooting-onehotencoder-issue-in-custom-pipeline-class-conversion-from-jup</guid>
      <pubDate>Mon, 25 Mar 2024 14:32:03 GMT</pubDate>
    </item>
    </channel>
</rss>