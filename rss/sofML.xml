<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 02 Feb 2024 18:17:52 GMT</lastBuildDate>
    <item>
      <title>VertexAIException - 使用 liteLLM 调用 Gemini-Pro 时列表索引超出范围错误</title>
      <link>https://stackoverflow.com/questions/77928395/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-using</link>
      <description><![CDATA[我正在使用Python的liteLLM包以连续的方式调用Google Gemini-Pro API（大约每分钟50个查询）。我相信我已经正确设置了我的 VertexAI 项目和凭据。当我使用的连续查询数量相对较小时（有时总共大约 330 个查询，有时总共大约 56 个查询），查询会运行完毕，并且会很好地收到响应。但是，一旦查询数量超过上述栏，就会出现以下错误：
litellm.exceptions.APIError：VertexAIException - 列表索引超出范围



回溯（最近一次调用最后一次）：
  文件“/Users/mike/anaconda3/envs/chat/lib/python3.11/site-packages/litellm/llms/vertex_ai.py”，第 390 行，已完成
    完成响应=响应.文本
                          ^^^^^^^^^^^^^^
  文件“/Users/mike/anaconda3/envs/chat/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py”，第 1315 行，文本
    返回 self.candidates[0].text
           ~~~~~~~~~~~~~~~^^^
IndexError：列表索引超出范围

这是什么原因造成的？我已将 VertexAI 服务器位置设置为： litellm.vertex_location = “us-central1”，据我所知，该位置的配额应该为 60 个查询/分钟。由于我连续执行 API 调用，但低于每分钟 60 次查询的速率，因此我认为我处于使用正常范围。我目前正在使用免费的 VertexAI 试用帐户（有 300 美元的免费信用）。]]></description>
      <guid>https://stackoverflow.com/questions/77928395/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-using</guid>
      <pubDate>Fri, 02 Feb 2024 16:11:12 GMT</pubDate>
    </item>
    <item>
      <title>即使两个句子相同，图像字幕模型的 CIDEr 评估指标也会输出 0.0</title>
      <link>https://stackoverflow.com/questions/77927272/cider-evaluation-metric-for-image-captioning-model-outputs-0-0-even-if-both-sent</link>
      <description><![CDATA[从 pycocoevalcap.cider.cider 导入 Cider
导入 json

Reference_file = &#39;/content/ref.json&#39;
候选文件 = &#39;/content/preds.json&#39;

将 open(reference_file, &#39;r&#39;) 作为 f：
    引用 = json.load(f)

以 open(candidate_file, &#39;r&#39;) 作为 f：
    候选人 = json.load(f)
打印（候选人）
# 创建 CIDEr 记分器
cider_scorer = 苹果酒()

# 计算 CIDEr 分数
cider_score, cider_scores = cider_scorer.compute_score(参考文献, 候选人)

# 打印 CIDEr 分数
print(&quot;CIDEr 分数：&quot;, cider_score)`

它给出输出 0.0。]]></description>
      <guid>https://stackoverflow.com/questions/77927272/cider-evaluation-metric-for-image-captioning-model-outputs-0-0-even-if-both-sent</guid>
      <pubDate>Fri, 02 Feb 2024 13:18:02 GMT</pubDate>
    </item>
    <item>
      <title>Rust 中的神经网络针对 ReLu 和 SoftMax 收敛到零</title>
      <link>https://stackoverflow.com/questions/77927151/neural-network-in-rust-converging-to-zeros-for-relu-and-softmax</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77927151/neural-network-in-rust-converging-to-zeros-for-relu-and-softmax</guid>
      <pubDate>Fri, 02 Feb 2024 12:58:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 CNN + LSTM 组合的视频分类损失没有减少，指标也没有改善</title>
      <link>https://stackoverflow.com/questions/77926420/video-classification-using-cnn-lstm-combination-loss-isnt-reducing-metrics-a</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77926420/video-classification-using-cnn-lstm-combination-loss-isnt-reducing-metrics-a</guid>
      <pubDate>Fri, 02 Feb 2024 10:49:47 GMT</pubDate>
    </item>
    <item>
      <title>KerasRL：没有名为“keras.utils.generic_utils”的模块</title>
      <link>https://stackoverflow.com/questions/77925862/kerasrl-no-module-named-keras-utils-generic-utils</link>
      <description><![CDATA[在 Python 3.10.10 中，我尝试遵循 https: //github.com/keras-rl/keras-rl，但是当我这样做时，我收到错误：
ModuleNotFoundError：没有名为“keras.utils.generic_utils”的模块

通过运行“dqn_cartpole.py”中的源代码一行一行，我发现导入了
从 rl.agents.dqn 导入 DQNAgent

触发此错误。
我尝试用 KerasRL2 替换 KerasRL，但这样做会产生另一个错误：
导入错误：无法从“tensorflow.keras”导入名称“__version__”

我不知道为什么会发生这个错误，但我认为 KerasRL 的安装有问题，尽管我遵循了官方文档。有什么帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/77925862/kerasrl-no-module-named-keras-utils-generic-utils</guid>
      <pubDate>Fri, 02 Feb 2024 09:22:02 GMT</pubDate>
    </item>
    <item>
      <title>洪水预测模型</title>
      <link>https://stackoverflow.com/questions/77925474/flood-prediction-model</link>
      <description><![CDATA[我有一个带有列的河水测量站的数据集
每天的“water_level_8am”、“water_level_9am”、“24hr_rainfall”，我有洪水风险级别。
我需要建立一个机器学习模型，考虑最近 7 天的水位和降雨量数据，并在不知道第 8 天的数据的情况下预测第 8 天的水位。然后根据预测的水位可以判断是否存在洪水风险。任何人都可以帮忙解决这个问题
我尝试了一些机器学习模型，例如
KNN分类器
逻辑回归
支持向量分类
决策树分类
随机森林分类器。
但我不知道如何正确应用它们]]></description>
      <guid>https://stackoverflow.com/questions/77925474/flood-prediction-model</guid>
      <pubDate>Fri, 02 Feb 2024 08:01:16 GMT</pubDate>
    </item>
    <item>
      <title>我自己的 MNIST 数据集神经网络的成本函数没有减少</title>
      <link>https://stackoverflow.com/questions/77925060/cost-function-not-decreasing-for-my-own-neural-network-for-mnist-dataset</link>
      <description><![CDATA[类 MLP():
    
    def __init__(自身、输入节点、隐藏节点、输出节点):
        self.input_nodes = input_nodes
        self.hidden_​​nodes = 隐藏节点
        self.output_nodes = 输出节点
        
        
        self.input_weights = np.random.randn(self.hidden_​​nodes, self.input_nodes) / np.sqrt(self.input_nodes)
        self.hidden_​​weights = np.random.rand(self.output_nodes, self.hidden_​​nodes) / np.sqrt(self.hidden_​​nodes)
        self.hidden_​​bias = np.zeros((self.output_nodes, 1))
        self.input_bias = np.zeros((self.hidden_​​nodes, 1))
         
        
    def feed_forward(自身,X):
        
        self.Z1 = np.dot(self.input_weights, X) + self.input_bias
        self.A1 = self.sigmoid(self.Z1)
        self.Z2 = np.dot(self.hidden_​​weights, self.A1) + self.hidden_​​bias
        输出 = softmax(self.Z2)
        
        返回
        
        
        
    def sigmoid(自身, x):
        返回 np.where(x &gt;= 0,
                    1 / (1 + np.exp(-x)),
                    np.exp(x) / (1 + np.exp(x)))
        
        
        
    def backprop（自身，inputs_list，targets_list）：
        
        m = 目标列表大小
        输出 = self.feed_forward(inputs_list)
        输出错误 = 输出 - 目标列表
        self.binary_loss = -np.mean(targets_list * np.log(outs) + (1 - Targets_list) * np.log(1 -outs))
        打印（self.binary_loss）
        soft_error = 出局数 * (1-出局数)
        dz2 = 输出错误 * 软错误
        dw2 = 1/m * np.dot(dz2, self.A1.T)
        db2 = 1/m * dz2
        da1 = np.dot(self.hidden_​​weights.T, dz2)
        dz1 = da1 * 自身.A1*(1-自身.A1)
        dw1 = 1/m * np.dot(dz1, input_list.T)
        db1 = 1/m * dz1
        
        
        
        返回 dw1、dw2、db1、db2
    
    
    def get_predictions(self, X2):
        返回 np.argmax(X2, 0)
    
    
    def 火车（自我，inputs_list，targets_list，epochs = 100，lr = 0.001）：
        
        对于范围内的 i（纪元）：
            #print(“迭代：” + str(i))
            
            
            input_weights，hidden_​​weights，input_bias，hidden_​​bias = self.backprop（inputs_list，targets_list）
            self.input_weights = self.input_weights - (lr*input_weights)
            self.hidden_​​weights = self.hidden_​​weights - (lr*hidden_​​weights)
            self.input_bias = self.input_bias - (lr*input_bias)
            self.hidden_​​bias = self.hidden_​​bias - (lr*hidden_​​bias)
            
        输出 = self.feed_forward(inputs_list)
            
        返回 self.get_predictions(输出)

这就是我传递来训练神经网络的内容。为了测试我的神经网络，我训练了1000 个示例。 X_training 只是从 mnist 数据集中读取 train.csv。
X_data = np.array(X_training).T
X_train = X_data[1:785]
X_train.shape



X_train_1 = X_train[:, 0:1000]
X_train_1.shape


Y_train = X_data[0]
Y_targs = np.eye(10)[Y_train]

Y_targs = Y_targs.T
Y_targs.shape


Y_targs_1 = Y_targs[:, 0:1000]
Y_targs_1.shape


c = MLP(784, 100, 10) # 创建具有 784 个输入节点、100 个隐藏层和 10 个输出节点的 MLP 对象。

b = c.train(X_train_1, Y_targs_1, 500, 0.01) # 这给出了成本函数的输出，其永远不会减少



如您所见，在训练 mnist 数据集时，我的成本函数没有减少。它适用于成本函数起作用的一个示例，但是，当存在一个或多个示例时，成本永远不会下降。我想知道我的反向传播方法是否有问题，但看起来没问题？我使用 scipy 库中的 softmax。我将不胜感激任何建议！我基本上对每个输出都进行了热编码，因此我的尺寸看起来也正确。
&lt;前&gt;&lt;代码&gt;0.9239303973120006
0.923930392942585
0.9239303885732107
0.9239303842038776
0.923930379834586
0.9239303754653354
0.9239303710961263
0.9239303667269585
0.9239303623578322
0.923930357988747
0.9239303536197034
0.9239303492507012
0.9239303448817403
0.9239303405128209
0.9239303361439426
0.9239303317751062
0.9239303274063112
0.9239303230375575
0.9239303186688456
0.9239303143001749
0.9239303099315459
0.9239303055629583
0.9239303011944123
0.923930296825908
0.9239302924574451
0.9239302880890238
0.9239302837206442
0.9239302793523064

]]></description>
      <guid>https://stackoverflow.com/questions/77925060/cost-function-not-decreasing-for-my-own-neural-network-for-mnist-dataset</guid>
      <pubDate>Fri, 02 Feb 2024 06:29:54 GMT</pubDate>
    </item>
    <item>
      <title>IBM Watson Annotator for Clinical Data 是否仍然可以使用？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77924873/is-ibm-watson-annotator-for-clinical-data-still-available-for-use</link>
      <description><![CDATA[我尝试访问 IBM 网站，但没有该名称的产品，并且它不断引导我进入一些错误页面
我希望它用于从我的文本数据中提取相关的临床文本，或者您有其他建议。]]></description>
      <guid>https://stackoverflow.com/questions/77924873/is-ibm-watson-annotator-for-clinical-data-still-available-for-use</guid>
      <pubDate>Fri, 02 Feb 2024 05:35:14 GMT</pubDate>
    </item>
    <item>
      <title>OpenPCDet 与 Windows 兼容还是仅与 Linux 兼容？</title>
      <link>https://stackoverflow.com/questions/77924600/is-openpcdet-compatible-with-windows-or-only-linux</link>
      <description><![CDATA[我正在使用 OpenPCDet 开发一个项目，在 Windows 环境中设置它时遇到一些问题。官方文档主要参考Linux环境。我想知道 OpenPCDet 是否本质上与 Windows 不兼容，或者是否有特定步骤使其在 Windows 上工作。
是否有人在 Windows 上成功运行 OpenPCDet，如果是，您能否提供一些设置指导或资源？或者，如果 OpenPCDet 仅限于 Linux 环境，是否有针对 Windows 用户的推荐解决方法？
我尝试设置环境，但是当我运行 python setup.pydevelopment 时遇到错误。]]></description>
      <guid>https://stackoverflow.com/questions/77924600/is-openpcdet-compatible-with-windows-or-only-linux</guid>
      <pubDate>Fri, 02 Feb 2024 03:56:47 GMT</pubDate>
    </item>
    <item>
      <title>Imagnet-21K 访问类标签？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77923909/imagnet-21k-accessing-the-class-labels</link>
      <description><![CDATA[是否可以访问 21K 版本 ImageNet 的类标签？
我正在尝试实现一个图像分类网络应用程序。
我正在使用 image net 1K，因为我只能在网上找到那些类标签，但版本很糟糕，它认为猫头鹰是黄鼠狼......有人帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/77923909/imagnet-21k-accessing-the-class-labels</guid>
      <pubDate>Thu, 01 Feb 2024 23:21:18 GMT</pubDate>
    </item>
    <item>
      <title>在google colab上使用tensorflow的问题</title>
      <link>https://stackoverflow.com/questions/77922080/problems-with-using-tensorflow-on-google-colab</link>
      <description><![CDATA[我对这一切都很陌生。有人可以告诉我发生了什么事吗？那么我该如何解决这个问题呢？
在此处输入图像描述
基本上，我想使用 tf，但我被卡住了。因为我以前从未使用过它。
我想将它与我的检测模型一起使用。
感谢大家的帮助。
谨此致以崇高敬意，祝大家好运。]]></description>
      <guid>https://stackoverflow.com/questions/77922080/problems-with-using-tensorflow-on-google-colab</guid>
      <pubDate>Thu, 01 Feb 2024 17:05:18 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：调用层“time_distributed_4”时遇到异常（类型 TimeDistributed）</title>
      <link>https://stackoverflow.com/questions/77909132/valueerror-exception-encountered-when-calling-layer-time-distributed-4-type</link>
      <description><![CDATA[我尝试使用 Kvasir 数据集制作 CNN-LSTM 模型。我使用 image_dataset_from_directory 分割数据集，如下所示：
dataset_path = “/kaggle/working/dataset”
图像大小 = 224, 224
批量大小 = 64

train_ds = image_dataset_from_directory(
  数据集_路径，
  验证分割=0.2，
  子集=“训练”，
  label_mode =“分类”，
  种子=23，
  图像大小=图像大小，
  color_mode =“rgb”，
  批量大小=批量大小）


val_ds = image_dataset_from_directory(
  数据集_路径，
  验证分割=0.2，
  子集=“验证”，
  label_mode =“分类”，
  种子=23，
  图像大小=图像大小，
  color_mode=“RGB”，
  批量大小=批量大小）

这个函数给了我一个 BatchDataset。然后我将基数设置如下：
val_batches = tf.data.experimental.cardinality(val_ds)
test_ds = val_ds.take(val_batches // 2)
val_ds = val_ds.skip(val_batches // 2)

然后
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)

这段代码还给了我一个预取数据集。当我运行 print(train_ds) 时它给出：
&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 8), dtype=tf .float32，名称=无））&gt;

然后我添加了我的模型，
 model = tf.keras.models.Sequential([
    # 具有批量归一化和最大池化的卷积层
    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, (3, 3), 激活=无,input_shape=(224, 224,3))),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3), 激活=无)),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),



    # 压平输出并添加密集层
    tf.keras.layers.TimeDistributed(Flatten()),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.LSTM(256,激活=&#39;tanh&#39;),
    
    
    
    tf.keras.layers.Dense(256),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),

    # 具有 8 个节点的输出层用于分类
    tf.keras.layers.Dense(8, 激活=&#39;softmax&#39;)
]）

# 编译模型

model.compile(优化器=AdamW(learning_rate=0.001,weight_decay=0.004,beta_1=0.9,beta_2=0.999,epsilon=1e-8),
          损失=分类交叉熵(),
          指标=[&#39;准确性&#39;])

当我适合这个模型时它没有运行并且出现错误：
ValueError：调用层“time_distributed_4”（类型 TimeDistributed）时遇到异常。
    
    层“conv2d_2”的输入0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、224、3）
    
    调用层“time_distributed_4”接收的参数（类型 TimeDistributed）：
      输入= tf.Tensor（形状=（无，224，224，3），dtype = float32）
      • 训练=真
      • 掩码=无

我不知道如何解决这个问题，你能帮助我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77909132/valueerror-exception-encountered-when-calling-layer-time-distributed-4-type</guid>
      <pubDate>Tue, 30 Jan 2024 20:05:50 GMT</pubDate>
    </item>
    <item>
      <title>Tesseract 的训练自定义数据集以及版本之间的差异</title>
      <link>https://stackoverflow.com/questions/77894617/training-custom-dataset-for-tesseract-and-difference-between-versions</link>
      <description><![CDATA[我正在尝试构建自定义数据集，然后对其进行训练，以改进tesseract 的 OCR。
然而，我很难理解确切的步骤或正确的方法。请注意，我在机器学习方面的经验很少，尤其是在神经网络方面。
在开始我的问题之前，我想说，我认为 LSTM 是从 Tesseract 4 开始添加的，并且对于较低版本，默认使用自适应分类器。
我在这里查看了文档：https://github.com/tesseract-ocr/tessdoc  但老实说，我很困惑，因为我的经验为零，并且通过查看一些教程，我发现了很多差异。
我的问题是：

在为 tesseract 4 及更高版本创建数据集时，我只需要图像、box 文件和真实数据文件？
对于 4 以下的超正方体，我只需要图像和盒子文件，而不需要真实数据文件？

我尝试识别的图像上的文本如下：








如果您还可以提供有关我的案例场景的确切步骤的详细说明，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77894617/training-custom-dataset-for-tesseract-and-difference-between-versions</guid>
      <pubDate>Sun, 28 Jan 2024 11:33:15 GMT</pubDate>
    </item>
    <item>
      <title>线性回归 RMSE [关闭]</title>
      <link>https://stackoverflow.com/questions/77892345/linear-regression-rmse</link>
      <description><![CDATA[尝试比较不同多项式次数的均方根误差，但最终得到相同的 RMSE。对于不同的学位，我应该得到不同的 RMSE，但我对所有学位都得到相同的 RMSE。我不明白为什么。
train_rmse_errors=[]
test_rmse_errors=[]

对于范围 (1,20) 中的 d：
    
    poly_converter = 多项式特征（度=d，include_bias=False）
    poly_fearures = poly_converter.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.33, random_state=42)
    模型=线性回归()
    model.fit(X_train,y_train)
    
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    
    train_rmse = np.sqrt(mean_squared_error(y_train,train_pred))
    test_rmse = np.sqrt(mean_squared_error(y_test,test_pred))
    
    train_rmse_errors.append(train_rmse)
    test_rmse_errors.append(test_rmse)
]]></description>
      <guid>https://stackoverflow.com/questions/77892345/linear-regression-rmse</guid>
      <pubDate>Sat, 27 Jan 2024 18:38:33 GMT</pubDate>
    </item>
    <item>
      <title>如何在 scikit-learn 中显示每次迭代的成本函数？</title>
      <link>https://stackoverflow.com/questions/38179687/how-do-you-show-cost-function-per-iteration-in-scikit-learn</link>
      <description><![CDATA[我最近一直在运行一些线性/逻辑回归模型，我想知道如何输出每次迭代的成本函数。 sci-kit LinearRegression 中的参数之一是“maxiter”，但实际上您需要查看成本与迭代，以找出该值真正需要的值，即是否值得花费计算时间来运行更多迭代等。
我确信我遗漏了一些东西，但我会认为有一种方法可以输出此信息？
提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/38179687/how-do-you-show-cost-function-per-iteration-in-scikit-learn</guid>
      <pubDate>Mon, 04 Jul 2016 08:08:35 GMT</pubDate>
    </item>
    </channel>
</rss>