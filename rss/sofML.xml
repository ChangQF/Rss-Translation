<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 16 Aug 2024 12:29:55 GMT</lastBuildDate>
    <item>
      <title>PyTorch：重塑 RuntimeError：形状'[1, 2]'对于大小为 100 的输入无效？</title>
      <link>https://stackoverflow.com/questions/78878681/pytorch-reshape-runtimeerror-shape-1-2-is-invalid-for-input-of-size-100</link>
      <description><![CDATA[我正在 Pytorch 中构建一个简单的足球比分预测模型。
我的步骤：

清理和处理数据
使用球队名称作为输入，将分数作为输出
对球队名称进行独热编码
将两个球队名称传递给模型
添加非线性
（问题就在这里）接收两个分数作为输出

但是我的模型多次返回两个分数
tensor([[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
        [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5 232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4845], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4 844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844],
        [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5 232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844]], grad_fn=&lt;SigmoidBackward0&gt;)

这是我的 NN 的输出，但我不确定为什么当我的输出层大小为 2 个神经元时，它会返回 50x2。
我的模型：
class EstimatorModel(nn.Module):
def __init__(self,
input_neurons:int,
hidden_​​neurons:int,
output_neurons:int):

super().__init__()
self.input_neurons= nn.Linear(in_features=input_neurons, out_features=hidden_​​neurons)
self.hidden_​​neurons= nn.Linear(in_features=hidden_​​neurons, out_features=hidden_​​neurons)
self.output_neurons= nn.Linear(in_features=hidden_​​neurons, out_features=output_neurons)

# 激活函数
self.sigmoid = nn.Sigmoid()

def forward(self, x):
x = self.input_neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.hidden_​​neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.hidden_​​neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.hidden_​​neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.output_neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

return x

使用带有 Adam 优化器的 MSEloss。
训练循环：
 for e in range(epochs):
x=0
for input, output in zip(input_data, output_data):
input = stack(inputs, dim=1)
output = stack(outputs, dim=1)
#outputs = tensor(outputs

print(outputs.shape)
print(outputs)

prediction:Tensor = model(inputs)
prediction_processed = model.process_model_output(prediction)

print(prediction.shape)
print(prediction)

loss = loss_func(prediction, output.float())

optim.zero_grad()
loss.backward()
optim.step()

quit()

x+=1
# 如果您不喜欢太冗长，请注释此打印语句
#print(f&quot;[INFO] {x+1} 对完成。损失：{loss}&quot;)

因此，任何建议都值得赞赏。
尝试改变输入传递的方式，例如，正常传递，作为单个张量传递，最后作为堆栈传递。没有运气。]]></description>
      <guid>https://stackoverflow.com/questions/78878681/pytorch-reshape-runtimeerror-shape-1-2-is-invalid-for-input-of-size-100</guid>
      <pubDate>Fri, 16 Aug 2024 10:20:05 GMT</pubDate>
    </item>
    <item>
      <title>如何在实时物体检测中使用 YOLO 区分两个相似物体？</title>
      <link>https://stackoverflow.com/questions/78878680/how-to-distinguish-two-similar-objects-using-yolo-in-real-time-object-detection</link>
      <description><![CDATA[我正在使用 YOLOv8 和 Python 开发实时对象检测系统。我的系统旨在检测经过的工业零件，并确保它们属于指定的批次类型。每个零件都有一个标签，格式为“X123.4567.122.12”，其中“X123.4567”表示零件的类型和所属的组，在该组中，所有零件几乎相同，只有很小的差异，“122.12”是特定零件的唯一标识符。
我在区分非常相似的物体时遇到了挑战。具体来说，我需要区分具有非常相似特征的零件，而唯一的差异可能是细微的变化。例如，考虑两个几乎完全相同的零件，它们的特征只有细微的差异。 [正如您所见，这是两个不同的部分，唯一的区别是两个孔位移部分 1 ]。
[部分 2：]
我向训练数据集添加了更多类似部分的图像，但模型仍然难以区分细微的变化。并应用了各种增强技术来提高泛化能力，例如旋转和缩放，但并没有显着提高区分这些部分的准确性。并且 nth 确实有效。我对此很陌生，所以我不知道应该采用什么样的方法来处理类似的部分。]]></description>
      <guid>https://stackoverflow.com/questions/78878680/how-to-distinguish-two-similar-objects-using-yolo-in-real-time-object-detection</guid>
      <pubDate>Fri, 16 Aug 2024 10:19:53 GMT</pubDate>
    </item>
    <item>
      <title>在 TensorFlow 中重塑数据数组是否会保留方向？</title>
      <link>https://stackoverflow.com/questions/78878433/does-reshaping-data-arrays-in-tensorflow-preserve-orientation</link>
      <description><![CDATA[我有一个形状为 (100, 51, 128, 128, 3) 的数据集，其中 100 是不同的样本，51 是时间戳，128、128 是网格点，3 是不同的群体。当我将这些数据输入深度学习架构时，我会将此数据集重塑为
tf.reshape(data, [-1, 128]),

这样它就变成了一个适合密集层的 2D 数组（密集层是将数据压缩到特定潜在空间所必需的）。
我的深度学习架构是一个自动编码器结构，我想重建原始数据矩阵。因此，我想知道何时将数据集重塑为 2D 矩阵，并在重塑后的矩阵上使用密集层，然后使用以下命令将此 2D 矩阵重塑回我的原始数据集：
tf.reshape(2Dmatrix, [-1, 51, 128, 128, 3])。

这样做会保留我的数据方向吗？还是这个矩阵的特征会混淆，我将无法保留原始方向？由于我的数据是网格结构，因此原始方向对我很重要。使用密集层后，有没有办法保留数据的方向？
将数据转换为 2D 矩阵，然后将其重塑回原始形状后，这样获得的数据集具有相似的值，但绘制时似乎与原始数据完全不同，好像没有捕获任何模式。]]></description>
      <guid>https://stackoverflow.com/questions/78878433/does-reshaping-data-arrays-in-tensorflow-preserve-orientation</guid>
      <pubDate>Fri, 16 Aug 2024 09:19:21 GMT</pubDate>
    </item>
    <item>
      <title>如何提高使用短期时间序列数据预测累计能源消耗的准确性？</title>
      <link>https://stackoverflow.com/questions/78878351/how-to-improve-accuracy-in-predicting-cumulative-energy-consumption-using-short</link>
      <description><![CDATA[问题：
我正在研究基于短期时间序列数据的电动汽车累计能耗预测模型。数据集包含大约 1000 个样本，每个样本包含 30 个时间步长的传感器数据，每个时间步长有 4 个特征。目标值是最后一个时间步长的累计能耗。
目前的方法：
我已经尝试了基于 CNN (ResNet) 和基于 RNN (LSTM) 的架构。
但是，这两种方法的预测精度都不令人满意。
问题：
我的目标是专门针对最后一个时间步长的累计能耗优化预测。
我担心仅针对最后一个时间步进行优化可能会导致问题，或者无法充分利用数据的顺序性。
问题：
架构：对于此任务，我是否应考虑进行任何特定的架构调整？例如，CNN 和 RNN 架构的组合（例如 ConvLSTM）是否更有效？
数据：鉴于序列长度较短（30 个时间步），是否有任何数据预处理或增强技术可以帮助提高模型性能？
损失函数：我是否应该考虑使用自定义损失函数来更加强调最后一个时间步的准确性，或者是否有更适合这种类型预测的替代损失函数？
其他注意事项：是否有任何其他建模技术、正则化方法或优化策略可能特别有助于提高短期时间序列数据中累积预测的准确性？
任何见解或建议都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78878351/how-to-improve-accuracy-in-predicting-cumulative-energy-consumption-using-short</guid>
      <pubDate>Fri, 16 Aug 2024 08:57:01 GMT</pubDate>
    </item>
    <item>
      <title>如何在 android kotlin 中实现输出形状为 [1, 25200, 6],[1, 2, 640, 640],[1, 2, 640, 640] 的 yolop 模型（分割模型）</title>
      <link>https://stackoverflow.com/questions/78878176/how-can-i-implement-yolop-modelsegemntation-model-with-output-shapes-1-25200</link>
      <description><![CDATA[我目前正在开发一个 Android 项目，需要使用 YOLOp 模型进行分割。我已成功将模型转换为 onnx/tflite 格式，但在处理输出形状时遇到了困难。该模型包含三个输出，形状分别为 [1, 25200, 6]、[1, 2, 640, 640] 和 [1, 2, 640, 640]
如果您能提供任何有关如何在我的 Android 应用程序中解释和处理此输出形状的指导，我将不胜感激。具体来说，我需要帮助来理解输出张量的结构并提取对象的边界框坐标和分割蒙版。
任何与在 Android 中使用此特定输出形状实现 YOLOP TFLite 相关的代码片段、建议或资源都将非常有帮助。提前感谢您的帮助！
我搜索了基于 yolop 的解决方案。但没有找到]]></description>
      <guid>https://stackoverflow.com/questions/78878176/how-can-i-implement-yolop-modelsegemntation-model-with-output-shapes-1-25200</guid>
      <pubDate>Fri, 16 Aug 2024 08:05:49 GMT</pubDate>
    </item>
    <item>
      <title>如何在@tensorflow-models/body-segmentation 中的 toBinaryMask 中编辑轮廓颜色？</title>
      <link>https://stackoverflow.com/questions/78878172/how-can-i-edit-the-contour-colour-in-tobinarymask-in-tensorflow-models-body-seg</link>
      <description><![CDATA[我正在使用@tensorflow-models/body-segmentation，轮廓颜色始终默认为 rgb(0,255,255)。我使用 toBinaryMask 函数将人物与背景区分开来，我希望轮廓为 rgb(0,0,0)。
这是我使用的代码。我可以通过“drawContour”切换轮廓是否可见，但不能切换颜色。从 ImageData 运行函数后，是否可以更改颜色？欢迎提出任何想法。
 const foregroundColor = { r: 255, g: 255, b: 255, a: 255 };
const backgroundColor = { r: 205, g: 205, b: 205, a: 0 };
const drawContour = true;
const foregroundThreshold = 0.2;
const opacity = 1;
const maskBlurAmount = 0;
const flipHorizo​​ntal = false;

const backgroundDarkeningMask = await bodySegmentation.toBinaryMask(
segations,
foregroundColor,
backgroundColor,
drawContour,
foregroundThreshold
);

await bodySegmentation.drawMask(
outputCanvas,
img,
backgroundDarkeningMask,
opacity,
maskBlurAmount,
flipHorizo​​ntal
);

我尝试编辑使用 patch-package 处理轮廓颜色的包，但什么都没有改变。]]></description>
      <guid>https://stackoverflow.com/questions/78878172/how-can-i-edit-the-contour-colour-in-tobinarymask-in-tensorflow-models-body-seg</guid>
      <pubDate>Fri, 16 Aug 2024 08:04:22 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关优化 500 个摄像头的计算机视觉集成的建议 [关闭]</title>
      <link>https://stackoverflow.com/questions/78877907/seeking-advice-on-optimizing-a-computer-vision-ensemble-for-500-cameras</link>
      <description><![CDATA[希望你们都一切顺利。我正在联系这个社区，就我目前正在进行的一个项目寻求你们的建议和见解。
提前感谢您提供的任何帮助或建议！
我有三个计算机视觉神经网络的组合，我正在努力连接 500 多个摄像头。目前，我正在使用 5 个摄像头进行测试，使用 OpenCV 库接收 RTSP 流。然后，我使用 Ultralytics 库加载神经网络并通过它们处理每个帧。结果用 OpenCV 绘制为边界框，我使用 Streamlit 显示图像。我正在一台配备 12 GB GPU 的简单 PC 上测试此设置，同时等待一台配备两个 GPU（总计 98 GB）的服务器。但是，性能非常慢，有时系统会完全冻结。
作为计算机视觉领域的初学者，考虑到未来 500 个摄像头的负载，您能否推荐一些库或方法来提高性能？我计划从 OpenCV 切换到支持 GPU 的库。将模型导出到 ONNX 并通过 ONNX Runtime 运行它们是否有助于提高性能？有人组织过类似的系统吗？任何建议都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78877907/seeking-advice-on-optimizing-a-computer-vision-ensemble-for-500-cameras</guid>
      <pubDate>Fri, 16 Aug 2024 06:52:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Android 中使用 Google ML Kit 对同一个人的图像进行分组？</title>
      <link>https://stackoverflow.com/questions/78877628/how-to-use-google-ml-kit-for-grouping-images-of-the-same-person-in-android</link>
      <description><![CDATA[我正在开发一个 Android 应用程序，需要对包含同一个人脸的图像进行分组。我计划使用 Google ML Kit 进行人脸检测和识别，但我不确定如何实现比较人脸和对人脸进行分组的逻辑。
有人可以解释一下使用 Google ML Kit 实现此目的所需的步骤或逻辑吗？我还没有示例代码，因此详细的解释或任何指示都会非常有帮助。
提前致谢！
你尝试了什么？
到目前为止，我已将 Google ML Kit 集成到我的 Android 项目中并成功实现了人脸检测。但是，我不知道如何继续比较检测到的人脸以确定它们是否属于同一个人，然后相应地对它们进行分组。
你期待什么？
我期望找到一种方法来匹配检测到的人脸并创建同一个人的图像汇总。理想情况下，我想了解使用 Google ML Kit 实现此逻辑的最佳方法，无论是通过人脸嵌入、特征提取还是任何其他可能有效的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78877628/how-to-use-google-ml-kit-for-grouping-images-of-the-same-person-in-android</guid>
      <pubDate>Fri, 16 Aug 2024 04:38:24 GMT</pubDate>
    </item>
    <item>
      <title>如何分析和检测 CAN 总线跟踪文件 (.blf) 中的模式以进行 DTC 诊断？[关闭]</title>
      <link>https://stackoverflow.com/questions/78877182/how-to-analyze-and-detect-patterns-in-can-bus-trace-files-blf-for-dtc-diagnos</link>
      <description><![CDATA[我正在开展一个项目，分析 CAN 总线跟踪文件 (.blf 和 .asc)，以检测与车辆 ECU 中的诊断故障代码 (DTC) 相关的模式。目标是识别故障发生时的常见信号模式，以自动进行故障检测和诊断。我们有 VW MQB 和 MEB UNECE 车辆架构车辆跟踪。我也有这些的 DBC 文件。我们正在使用 UDS 和 OBD 作为基于 CAN 的诊断的一部分。
我真的不知道具体要查看哪些特定信号。任何帮助都将不胜感激。
我正在努力识别信号数据中在故障发生之前或发生时出现的一致模式。我不确定如何有效地提取相关特征或应用聚类/无监督学习技术来检测这些模式。任何关于如何处理这个问题的指导，或类似分析的例子，都将不胜感激。
跟踪文件很大，包含来自多个 ECU 的数据，因此很难隔离相关信号。我正在寻找有关如何预处理数据和有效应用机器学习技术的建议。]]></description>
      <guid>https://stackoverflow.com/questions/78877182/how-to-analyze-and-detect-patterns-in-can-bus-trace-files-blf-for-dtc-diagnos</guid>
      <pubDate>Thu, 15 Aug 2024 23:40:15 GMT</pubDate>
    </item>
    <item>
      <title>对列应用对数变换</title>
      <link>https://stackoverflow.com/questions/78873685/applying-log-transformation-to-a-column</link>
      <description><![CDATA[
我已使用 OneHotEncoder 对性别列进行编码。我想仅对 Female[0] 列应用对数转换，但它却对所有列都应用了对数转换 — 为什么？
我的代码：
import pandas as p
from sklearn.preprocessing import FunctionTransformer, OneHotEncoder
from sklearn.compose import ColumnTransformer
import numpy as n

customer=p.read_csv(&#39;/content/Customers.csv&#39;)
customer.drop([&#39;CustomerID&#39;,&#39;Profession&#39;,&#39;Family Size&#39;,&#39;Work Experience&#39;],axis=1,inplace=True)
column=ColumnTransformer(
[
(&#39;ohe_gender&#39;,OneHotEncoder(sparse=False,dtype=n.int32),[0])
],remainder=&#39;passthrough&#39;
)
function=ColumnTransformer(
[
(&#39;function&#39;,FunctionTransformer(n.log1p),[0,1])
],remainder=&#39;passthrough&#39;
)
s=column.fit_transform(customer)
function.fit_transform(s)

输出：
 array([[0.00000000e+00, 6.93147181e-01, 1.90000000e+01, 1.50000000e+04, 3.90000000e+01],
[0.00000000e+00, 6.93147181e-01, 2.10000000e+01, 3.50000000e+04, 8.10000000e+01],
[6.93147181e-01, 0.00000000e+00, 2.00000000e+01, 8.60000000e+04, 6.00000000e+00],
...,
[0.00000000e+00, 6.93147181e-01, 8.70000000e+01, 9.09610000e+04, 1.40000000e+01],
[0.00000000e+00, 6.93147181e-01, 7.70000000e+01, 1.82109000e+05, 4.00000000e+00],
[0.00000000e+00, 6.93147181e-01, 9.00000000e+01, 1.10610000e+05, 5.20000000e+01]]

在 FunctionTransformer 之前进行编码 (OHE) 后，输出为
array([[ 0, 1, 19, 15000, 39],
[ 0, 1, 21, 35000, 81],
[ 1, 0, 20, 86000, 6],
...,
[ 0, 1, 87, 90961, 14],
[ 0, 1, 77, 182109, 4],
[ 0, 1, 90, 110610, 52]])

我确实想在上述数组的第 0 个索引中应用对数变换，但正如您在第一个输出中看到的那样，它应用于所有值，尽管我在列变换器中指定了 [0]，为什么？我希望输出只有 [0] 索引的对数。]]></description>
      <guid>https://stackoverflow.com/questions/78873685/applying-log-transformation-to-a-column</guid>
      <pubDate>Thu, 15 Aug 2024 04:58:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在不使用 for 循环的情况下直接从 Claude API 对多个完成（n）进行采样？</title>
      <link>https://stackoverflow.com/questions/78873304/how-to-sample-multiple-completions-n-directly-from-claude-api-without-a-for-lo</link>
      <description><![CDATA[我正在使用 Anthropic Claude API，并尝试在单个 API 调用中为给定的提示生成多个完成（n 个完成）。 OpenAI 的 API 在其采样设置中提供了一个 n 参数来实现这一点，但我在 Claude API 中找不到等效选项。
我目前的方法：
我目前正在使用重试机制来处理 API 调用期间的潜在错误，如下所示：
from tenacity import retry, stop_after_attempt, wait_exponential

def before_sleep(retry_state):
print(f&quot;(Tenacity) Retry, error that cause it: {retry_state.outcome.exception()}&quot;)

def retry_error_callback(retry_state):
exception = retry_state.outcome.exception()
exception_str = str(exception)
if &quot;prompt is too long&quot; in exception_str and &quot;400&quot;在 exception_str 中：
引发异常
返回“没有需要我们提前退出的错误。”

@retry(stop=stop_after_attempt(20), wait=wait_exponential(multiplier=2, max=256), 
before_sleep=before_sleep, retry_error_callback=retry_error_callback)
def call_to_anthropic_client_api_with_retry(gen: AnthropicGenerator, prompt: str) -&gt;; dict:
response = gen.llm.messages.create(
model=gen.model,
max_tokens=gen.sampling_params.max_tokens,
system=gen.system_prompt,
messages=[
{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: prompt}]}
],
temperature=gen.sampling_params.temperature,
top_p=gen.sampling_params.top_p,
n=gen.sampling_params.n, # 旨在生成多个完成
stop_sequences=gen.sampling_params.stop[:3],
)
return response

问题：
我在 Anthropic API 中找不到 n 参数允许在一次请求中生成多个完成的文档。
问题：

Claude API 是否支持在一次 API 调用中直接生成多个完成（n 个完成）？
如果没有，是否有推荐的解决方法或最佳实践来实现这一点，而无需循环多个请求？

跨 discord：https://discord.com/channels/1072196207201501266/1213976011998498816/threads/1273440866861846549
交叉：https://dev.to/brando90/how-to-sample-multiple-completions-n-directly-from-claude-api-without-a-for-loop-2m1e

现在这样做：
@retry(stop=stop_after_attempt(20), wait=wait_exponential(multiplier=2, max=256), 
before_sleep=before_sleep, retry_error_callback=retry_error_callback)
def call_to_anthropic_client_api_with_retry(gen: AnthropicGenerator, prompt: str) -&gt; dict:
# max_tokens=8192, # Claude 3.5 的 max_tokens https://docs.anthropic.com/en/docs/about-claude/models#model-comparison
# client = anthropic.Anthropic(api_key=gen.api_key)
# response = client.messages.create(
# response_text: str = gen.llm.messages.create(
# model=gen.sampling_params.model,
# max_tokens=gen.sampling_params.max_tokens,
# #temperature=temperature, # 注意提示生成器不会将其作为输入
# system=gen.sampling_params.system,
# messages=[
# {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: prompt}]}
# ],
#temperature=gen.sampling_params.temperature,
#top_p=gen.sampling_params.top_p,
#n=gen.sampling_params.n,
#stop=gen.sampling_params.stop[:3],
# ).content[0].text
if not hasattr(gen.sampling_params, &#39;n&#39;):
gen.sampling_params.n = 1
content: list[dict] = [] 
for _ in range(gen.sampling_params.n):
response = gen.llm.messages.create(
model=gen.model,
max_tokens=gen.sampling_params.max_tokens,
system=gen.system_prompt,
messages=[
{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: prompt}]}
],
temperature=gen.sampling_params.temperature,
top_p=gen.sampling_params.top_p,
n=gen.sampling_params.n,
stop_sequences=gen.sampling_params.stop[:3],
)
content.append(response)
response = dict(content=content)
# 消息示例：https://docs.anthropic.com/en/api/messages-examples
返回响应
]]></description>
      <guid>https://stackoverflow.com/questions/78873304/how-to-sample-multiple-completions-n-directly-from-claude-api-without-a-for-lo</guid>
      <pubDate>Thu, 15 Aug 2024 00:37:53 GMT</pubDate>
    </item>
    <item>
      <title>层“顺序”需要 1 个输入，但它收到了 48 个输入张量</title>
      <link>https://stackoverflow.com/questions/78872766/layer-sequential-expects-1-inputs-but-it-received-48-input-tensors</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78872766/layer-sequential-expects-1-inputs-but-it-received-48-input-tensors</guid>
      <pubDate>Wed, 14 Aug 2024 20:06:08 GMT</pubDate>
    </item>
    <item>
      <title>语法错误：无效的不可打印字符 U+00A0</title>
      <link>https://stackoverflow.com/questions/78872236/syntaxerror-invalid-non-printable-character-u00a0</link>
      <description><![CDATA[我想建立一个基本的逻辑回归模型，但当我尝试将数据输入模型时，出现了不可打印字符错误
model.fit(X_train, y_train
SyntaxError: 无效的不可打印字符 U+00A0

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78872236/syntaxerror-invalid-non-printable-character-u00a0</guid>
      <pubDate>Wed, 14 Aug 2024 17:20:55 GMT</pubDate>
    </item>
    <item>
      <title>Epoch 1/3 ^C - model.fit() 以此行终止，且没有任何错误</title>
      <link>https://stackoverflow.com/questions/78858484/epoch-1-3-c-model-fit-was-terminated-with-this-line-and-without-any-error</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78858484/epoch-1-3-c-model-fit-was-terminated-with-this-line-and-without-any-error</guid>
      <pubDate>Sun, 11 Aug 2024 13:47:36 GMT</pubDate>
    </item>
    <item>
      <title>如何在现场生成数据时实现神经网络的训练？</title>
      <link>https://stackoverflow.com/questions/78677993/how-do-i-implement-training-a-neural-network-when-generating-the-data-on-spot</link>
      <description><![CDATA[我想构建一个物理模拟的替代模型。这样我就可以自己生成数据了。数据本身非常大，因此生成一些数据样本（例如一批 124 个）然后立即将其用于训练是有意义的，而不是生成数百万个样本，因为我无法保存这样的数据集。通过这种方式，我可以生成一个批次，临时保存它/将其发送到等待数据的神经网络，然后在对其进行训练后丢弃它。
我通常使用 pytorch 实现我的模型，我也熟悉 torch.utils.data.DataLoader，它可以将批次提取到 RAM 中，但它需要目录结构和文件 ID，即整个数据集需要保存在某个地方。]]></description>
      <guid>https://stackoverflow.com/questions/78677993/how-do-i-implement-training-a-neural-network-when-generating-the-data-on-spot</guid>
      <pubDate>Thu, 27 Jun 2024 13:31:24 GMT</pubDate>
    </item>
    </channel>
</rss>