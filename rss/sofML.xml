<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 29 Apr 2024 03:15:43 GMT</lastBuildDate>
    <item>
      <title>屏蔽 pytorch 张量时减少 TPU RAM 使用量</title>
      <link>https://stackoverflow.com/questions/78400256/reduce-tpu-ram-usage-when-masking-pytorch-tensors</link>
      <description><![CDATA[目前正在致力于合并 LLM 并在 Tensor 中计算其任务向量。 （通过获取前 80% 的向量）但是每次当我尝试从张量获取值时，它都会超过 Colab TPU 的 RAM 使用量。张量很大，但只需要大约 60GB 的 RAM。现阶段有什么办法可以减少TPU RAM的使用吗？
def mask（张量）：
    使用 torch.no_grad()：
        d=张量.形状[1]
        掩码=int(d*0.8)
        张量_abs=张量.abs()
    
        #RAM 使用情况良好，直到下一行
        
        kth_values，_=tensor_abs.kthvalue（掩码，dim = 1，keepdim = True）
        mask=tensor_abs&gt;=kth_values
        返回张量*掩码

我尝试过使用 torch.no_grad。有什么办法可以将张量分成不同的批次并分别处理吗？]]></description>
      <guid>https://stackoverflow.com/questions/78400256/reduce-tpu-ram-usage-when-masking-pytorch-tensors</guid>
      <pubDate>Mon, 29 Apr 2024 02:40:02 GMT</pubDate>
    </item>
    <item>
      <title>SVM 训练时间过长</title>
      <link>https://stackoverflow.com/questions/78400254/svm-training-taking-too-long</link>
      <description><![CDATA[我有一个包含 41 个特征的数据集，其中 4 个是文本特征。我得到了“词袋”这四个特征的 numpy 数组 (npz)，我将其与其他数值特征结合起来训练 SVM 模型。总共有 100000 条记录和 41 个特征，其中 4 个特征如前所述进行了矢量化。
该模型现已训练 45 分钟:)。有没有办法减少训练时间？我预处理数据集的方式（特别是结合 npz 和现有的数值特征）有什么问题吗？我还可以探索其他选择吗？
title_feature = load_npz(&#39;train_title_bow.npz&#39;)
Overview_feature = load_npz(&#39;train_overview_bow.npz&#39;)
tagline_feature = load_npz(&#39;train_tagline_bow.npz&#39;)
Production_companies_feature = load_npz(&#39;train_product_companies_bow.npz&#39;)

numeric_features = df_train[df_train.columns.difference([&#39;标题&#39;, &#39;概述&#39;, &#39;标语&#39;, &#39;生产公司&#39;, &#39;rate_category&#39;, &#39;average_rate&#39;, &#39;original_language&#39;])]
text_features = np.hstack([title_feature.toarray()、overview_feature.toarray()、tagline_feature.toarray()、生产_companies_feature.toarray()])
svm_X_train = np.hstack([数字特征, 文本特征])
svm_y_train = df_train[&#39;rate_category&#39;]

svm_classifier = SVC(kernel=&#39;linear&#39;) # 使用线性核，也可以选择其他核
svm_classifier.fit(svm_X_train, svm_y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/78400254/svm-training-taking-too-long</guid>
      <pubDate>Mon, 29 Apr 2024 02:39:09 GMT</pubDate>
    </item>
    <item>
      <title>GKE 上的 GPU 时间共享</title>
      <link>https://stackoverflow.com/questions/78400223/gpu-time-sharing-on-gke</link>
      <description><![CDATA[我正在尝试使用 说明中的 GPU 时间共享此处，但是我的工作负载不会在启用分时的节点上运行。
我有一个具有 GPU 配置的节点池，启用了策略分时的 GPU 共享以及“每个 GPU 的最大共享客户端数”。如 48 所示。节点运行良好，但我无法使用记录的 nodeSelector 配置为我的工作负载运行工作负载，例如
节点选择器：
  cloud.google.com/gke-accelerator：“nvidia-tesla-t4”
  cloud.google.com/gke-max-shared-clients-per-gpu：“48”
  cloud.google.com/gke-gpu-sharing-strategy：分时

这样，我的 Pod 就会陷入挂起状态，并显示消息xnodes did not match Pod&#39;s nodeaffinity/selector。如果我删除 gke-max-shared-clients-per-gpu 和 gke-gpu-sharing-strategy 密钥对，pod 会正常调度并运行。
当我检查 GPU 分时节点池中节点上的 kubernetes 标签时，它们不包含这些标签，并且我无法手动添加它们，因为 GCP 阻止了它。
如有任何建议，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78400223/gpu-time-sharing-on-gke</guid>
      <pubDate>Mon, 29 Apr 2024 02:20:41 GMT</pubDate>
    </item>
    <item>
      <title>是否仍然建议手动使用 `del` 或 `torch.cuda.empty_cache()` ？</title>
      <link>https://stackoverflow.com/questions/78399631/is-it-still-recommended-to-use-del-or-torch-cuda-empty-cache-manually</link>
      <description><![CDATA[许多在线机器学习代码仍然使用这个。但到了2024年，还推荐吗？如果没有的话，有什么具体的案例可以推荐吗？ （例如调试？）
我希望更多地了解为什么这些以前如此常见以及它们是否仍然有必要]]></description>
      <guid>https://stackoverflow.com/questions/78399631/is-it-still-recommended-to-use-del-or-torch-cuda-empty-cache-manually</guid>
      <pubDate>Sun, 28 Apr 2024 20:56:20 GMT</pubDate>
    </item>
    <item>
      <title>java中的机器学习是否可行？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78399466/machine-learning-in-java-is-possible-or-not</link>
      <description><![CDATA[
我们可以使用 Java 进行机器学习吗？
如何使用它？
Java 中有哪些可用的机器学习库？
Java 中的机器学习比 Python 更高效吗？
如果java有机器学习功能，它比python更有价值吗？

我在谷歌上搜索了上述问题并得到了几个答案，但我必须从工作专业人士那里得到答案，这样我才能消除我的疑虑......]]></description>
      <guid>https://stackoverflow.com/questions/78399466/machine-learning-in-java-is-possible-or-not</guid>
      <pubDate>Sun, 28 Apr 2024 19:45:18 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：X 有 3 个特征，但 MultinomialNB 期望 235 个特征作为输入</title>
      <link>https://stackoverflow.com/questions/78399309/valueerror-x-has-3-features-but-multinomialnb-is-expecting-235-features-as-inp</link>
      <description><![CDATA[我正在尝试使用 Flask 制作一个 Web 应用程序来预测 HTML 表单的输入。但我越来越
ValueError：X 有 3 个特征，但 MultinomialNB 期望 235 个特征作为输入

这是我的 Python 代码片段：
from sklearn.model_selection import train_test_split
从 sklearn.feature_extraction.text 导入 TfidfVectorizer
从 sklearn.naive_bayes 导入 MultinomialNB
从 sklearn.metrics 导入 precision_score

X_train, X_test, y_train, y_test = train_test_split(df[&#39;Aduan&#39;], df[&#39;Bag_Pelayanan&#39;], test_size=0.2, random_state=42)

向量化器 = TfidfVectorizer()
X_train_vec = 矢量化器.fit_transform(X_train)
X_test_vec = 矢量化器.transform(X_test)

NB = 多项式NB()
NB.fit(X_train_vec, y_train)

y_pred = NB.predict(X_test_vec)

NB_accuracy = precision_score(y_test, y_pred)

这是我的 Flask 代码片段：
def preprocess_text(text):
    文本 = 文本.lower()
    文本 = 删除标点符号（文本）
    文本 = 删除停用词（文本）
    文本=stem_text(文本)
    返回文本.strip()

def 删除标点符号（文本）：
    return text.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation))

def remove_stopwords(文本):
    return &#39; &#39;.join([如果单词不在 stop_words 中则在 text.split() 中逐字逐句])

def 词干文本(文本):
    返回stemmer.stem(文本)

@app.route(&#39;/&#39;, method=[&quot;GET&quot;, &quot;POST&quot;])
定义索引（）：
    if request.method == &quot;POST&quot;;和 request.form 中的“用户名”：
        用户名 = request.form[&#39;用户名&#39;]
        aduan = request.form[&#39;aduan&#39;]

        preprocessed_aduan = preprocess_text(aduan)
        aduan_vec = 矢量化器.fit_transform(preprocessed_aduan)

        预测 = model.predict(aduan_vec)

        游标 = mysql.connection.cursor(MySQLdb.cursors.DictCursor)
        cursor.execute(&#39;INSERT INTO pengaduan (用户名, bag_pelayanan, aduan) VALUES (%s, %s, %s)&#39;, (用户名, 预测[0], aduan))
        mysql.connection.commit()
        光标.close()
        
        return render_template(“success.html”,预测=预测)
    
    返回 render_template(“index.html”)

这是Flask 应用程序中的错误 我&#39;我遇到了。
此错误的原因是什么以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78399309/valueerror-x-has-3-features-but-multinomialnb-is-expecting-235-features-as-inp</guid>
      <pubDate>Sun, 28 Apr 2024 18:57:44 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：Q-learning 中以 10 为基数的 int() 的文字无效：“”</title>
      <link>https://stackoverflow.com/questions/78399063/valueerror-invalid-literal-for-int-with-base-10-in-q-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78399063/valueerror-invalid-literal-for-int-with-base-10-in-q-learning</guid>
      <pubDate>Sun, 28 Apr 2024 17:29:11 GMT</pubDate>
    </item>
    <item>
      <title>我正在寻找这些列类型的什么类型的机器学习？</title>
      <link>https://stackoverflow.com/questions/78398611/what-type-of-machine-learning-am-i-looking-for-with-these-column-types</link>
      <description><![CDATA[我一直在学习一些关于机器学习的知识，并使用了一些模型类型（xgboost、LogisticRegression）和一些测试数据。我使用这些模型的次数越多，我就越意识到它们处理的是一种特定类型的数据，即可以转换为数字的列。甚至像汽车的品牌/型号之类的东西也可以转化为数字，因为它们是有限的并且在数据集中重复。
我真正想要使用的数据集包含名字和姓氏、公司名称、电子邮件地址等唯一的字符串。这是一个例子

&lt;标题&gt;

名字和姓氏
公司名称
电子邮件地址
是欺诈


&lt;正文&gt;

全食 CVS 评估
全食/CVS 评估
laime.barry9989@gmail.com
正确


全食店
全食店
laimeb.a.r.ry9989@gmail.com
正确


蒂娜·罗森
最佳商品鞋
tina.rosen@gmail.com
错误


乔约翰
全食品市场调查
wholefoodsmark.et.l.inc@gmail.com
正确


史黛西帕克特
S Parket 奥特莱斯
sales@parkeroutlet.com
错误


迈克尔·费兰
克罗格
b.ill.h.o.rt2.2@gmail.com
正确



这是我拥有的一小部分数据，但您可以看到它不适合我所了解和使用的模型的正常数据集。我尝试过诸如 OneHotEncoder 和 LabelEncoder 之类的东西，但它们将它们转换为实际上没有任何意义的整数，因为它们不重复。
我知道看到该示例很容易想到“哦，只需自己编写验证器来查找电子邮件中的多个句点、名称中的特定单词等”即可。但有数千个重复的欺诈帐户不适合。
所以我的问题是，是否有一种机器学习模型可以接收这些电子邮件地址/名称等内容并了解欺诈电子邮件地址/名称的样子？]]></description>
      <guid>https://stackoverflow.com/questions/78398611/what-type-of-machine-learning-am-i-looking-for-with-these-column-types</guid>
      <pubDate>Sun, 28 Apr 2024 15:01:24 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的数据增强：我应该对验证集应用数据增强吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78398283/data-augmentation-in-machine-learning-should-i-apply-data-augmentation-for-vali</link>
      <description><![CDATA[我正在皮肤损伤医学图像数据集上训练卷积神经网络。原始数据集由训练和测试文件夹组成。因此，我将一些图像从训练文件夹移动到新的有效文件夹进行验证。由于数据集不平衡，我随后将数据增强应用于训练集。我现在的问题是：我应该使用之前创建的验证集，还是通过从增强训练集中提取图像来创建新的验证集？另外，如果我使用准确性作为验证指标，验证集是否应该保持不平衡（我知道准确性需要平衡的数据集），或者每个类的样本数量是否需要相同？
使用的数据集由九个类组成。到目前为止，仅当数据增强应用于训练集和验证集时，我才能够在训练集和验证集上获得良好的准确性性能。但当我进入测试阶段时，结果却不太令人满意。我目前正在尝试使用 Keras Tuner 在平衡数据集和原始验证集上搜索各种模型。]]></description>
      <guid>https://stackoverflow.com/questions/78398283/data-augmentation-in-machine-learning-should-i-apply-data-augmentation-for-vali</guid>
      <pubDate>Sun, 28 Apr 2024 13:14:45 GMT</pubDate>
    </item>
    <item>
      <title>决策树信息增益与特征重要性</title>
      <link>https://stackoverflow.com/questions/78398063/decision-trees-information-gain-vs-feature-importance</link>
      <description><![CDATA[我基于Sklearn用Python编写了一个决策树，但是当我计算结果并显示决策树（以及20个最重要的特征）时，特征“A”被忽略了。最重要，也作为根节点。
但是，当我计算每个特征的信息增益并将结果显示在列表中时，特征“B”会出现。具有最高的信息增益(特征“A”也具有相当高的信息增益，但不如特征B)。尽管如此，特征 A 被用作根节点......所以我的问题是：我是否犯了编程错误，或者这是一种可能的情况（根据定义，具有最高信息增益的特征不被用作根节点）。 
在另一个主题中，有人写了以下内容：
&lt;块引用&gt;
对于使用信息增益的决策树，算法选择
提供最大信息增益的属性（这是
也是导致熵减少最大的属性）。

还有（尤其是这部分非常有趣）：
&lt;块引用&gt;
决策树算法是“贪婪的”算法。从某种意义上说，他们总是
选择产生最大信息增益的属性
正在考虑当前节点（分支），而无需稍后重新考虑
添加后续子分支后的属性。所以要回答你的
第二个问题：决策树算法尝试放置属性
在树根部附近信息增益最大。注意
由于算法的贪婪行为，决策树算法
不一定会产生一棵提供最大可能的树
熵的总体减少。

所以在这种情况下，它没有理由选择类别 B 而不是类别 A，这意味着我可能犯了一个编码错误..？]]></description>
      <guid>https://stackoverflow.com/questions/78398063/decision-trees-information-gain-vs-feature-importance</guid>
      <pubDate>Sun, 28 Apr 2024 11:50:34 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法评估模型是否能够识别有影响的变量（使用 make_classification 生成的变量）？</title>
      <link>https://stackoverflow.com/questions/78398017/is-there-a-way-to-evaluate-whether-a-model-is-able-to-identify-the-variables-tha</link>
      <description><![CDATA[我有一个关于 scikit-learn 的 make_classification 的问题。我使用 make_classification（二元分类任务）创建了一个数据集，目的是测试不同模型区分重要特征和不太重要特征的能力。
如何设置一个实验来评估模型是否能够识别有影响的变量？
我查看了 make_classification 的文档，但不幸的是我没有进一步了解。
我设置了以下内容：
X,y = make_classification(n_samples=50000, n_features=10, n_informative=5,
                    n_redundant=2、n_repeated=0、n_classes=2、n_clusters_per_class=2、
                          类间隔=1，
                   Flip_y=0.01，权重=[0.9,0.1]，shuffle=True，random_state=42）

谢谢您，我们非常感谢任何想法或建议。]]></description>
      <guid>https://stackoverflow.com/questions/78398017/is-there-a-way-to-evaluate-whether-a-model-is-able-to-identify-the-variables-tha</guid>
      <pubDate>Sun, 28 Apr 2024 11:37:08 GMT</pubDate>
    </item>
    <item>
      <title>我们如何以优雅的方式捕获使用optimizer.step()完成的更新？</title>
      <link>https://stackoverflow.com/questions/78392429/how-can-we-capture-update-done-with-optimizer-step-in-an-elegant-way</link>
      <description><![CDATA[我想实现一种方法，按照 Karpathy 视频中提到的想法，在使用 PyTorch 训练期间在 Tensorboard 中监控更新数据比率。我已经提出了一个解决方案，但我正在寻找一种更优雅且可配置的方法。
当前的实现直接修改训练循环如下：
对于步骤，在 data_loader 中进行批处理：
    x, y = 批次
    优化器.zero_grad()
    对于名称，model.named_pa​​rameters() 中的参数：
        如果 param.requires_grad 和“weight”是名称：
            param.data_before_step = param.data.clone()
    输出=模型(x)
    损失 = loss_fn(输出, y)
    loss.backward()
    优化器.step()
    lr_scheduler.step()
    对于名称，model.named_pa​​rameters() 中的参数：
        if hasattr(param, “data_before_step”):
            更新 = param.data - param.data_before_step
            update_to_data = (update.std() / param.data_before_step.std()).log10().item()
            summary_writer.add_scalar(f“更新：数据比率 {name}”，update_to_data，epoch * len(data_loader) + 步骤)
            param.data_before_step = param.data.clone()

但是，这种方法直接在训练循环中添加代码，这可能会使代码变得混乱，如果我们想要使其可配置，则需要 if-else 语句，这会使代码更加混乱。
我还探索过使用 PyTorch hooks 来实现这一点。我已经成功实现了一个钩子来跟踪梯度：
类 GradToDataRatioHook：
    def __init__(自身、名称、参数、start_step、summary_writer):
        self.name = 名字
        self.param = 参数
        self.summary_writer = 摘要_writer
        自我.毕业生 = []
        self.grads_to_data = []
        self.param.update_step = start_step

    def __call__(自我，毕业生)：
        self.grads.append(grad.std().item())
        self.grads_to_data.append((grad.std() / (self.param.data.std() + 1e-5)).log10().item())
        self.summary_writer.add_scalar(f&quot;Grad {self.name}&quot;, self.grads[-1], self.param.update_step)
        self.summary_writer.add_scalar(f&quot;梯度:数据比例{self.name}&quot;, self.grads_to_data[-1], self.param.update_step)
        self.param.update_step += 1

但是，实现类似的钩子来捕获更新似乎很棘手。据我了解， param.register_hook(...) 注册了钩子，该钩子在计算梯度时调用，即在 optimizer.step() 之前调用叫。虽然梯度和学习率为标准 SGD 提供了更新的直接值，但像 Adam 这样的现代优化器使更新过程变得更加复杂。我正在寻找一种以与优化器无关的方式捕获更新的解决方案，最好使用 PyTorch 挂钩。但是，任何建议或替代方法也将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78392429/how-can-we-capture-update-done-with-optimizer-step-in-an-elegant-way</guid>
      <pubDate>Fri, 26 Apr 2024 18:32:22 GMT</pubDate>
    </item>
    <item>
      <title>如何消除在张量流的 Tape.gradient 方法中将虚数转换为实值的警告？</title>
      <link>https://stackoverflow.com/questions/77185089/how-to-remove-this-warning-of-casting-imaginary-into-real-values-within-tape-gra</link>
      <description><![CDATA[我正在使用tape.gradient方法来优化一些神经网络。它按预期工作，但当我在单次迭代中多次使用 Tape.gradients 计算梯度时，不断发出此警告。这意味着在单个循环内，在执行 back prop 时，它会在某个地方摆弄复数。
警告：tensorflow：您正在将complex64类型的输入转换为不兼容的dtype float64。这将丢弃虚部，并且可能不是您想要的。

cost_progress=[]
跟踪进度=[]
对于我在范围内（次数）：

  使用 tf.GradientTape() 作为磁带：
    磁带.watch(参数)
    损失，跟踪 = 成本（参数，比率）
    trace_progress.append(trace)
    cost_progress.append(损失)

  梯度 = Tape.gradient(loss, params)
  opt.apply_gradients(zip([渐变], [参数]))

现在，所有参数和损失都是 tf.float64，但仍在 Tape.gradient() 中给出了一些复杂类型，我想手动将它们转换为真实值，以便此警告停止显示在我的屏幕上。但我无法找到如何投射以免弄乱。
强制gradients = tf.cast(tape.gradient(loss, params),tf.float64)不起作用。我已验证 gradients = Tape.gradient(loss, params) 发出警告，并且 loss 和 params 均为 tf.float64 类型。]]></description>
      <guid>https://stackoverflow.com/questions/77185089/how-to-remove-this-warning-of-casting-imaginary-into-real-values-within-tape-gra</guid>
      <pubDate>Wed, 27 Sep 2023 06:26:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中获取多类分类问题中每个类的 SHAP 值</title>
      <link>https://stackoverflow.com/questions/71753428/how-to-get-shap-values-for-each-class-on-a-multiclass-classification-problem-in</link>
      <description><![CDATA[我有以下数据框：
导入 pandas 作为 pd
随机导入

导入xgboost
导入形状

foo = pd.DataFrame({&#39;id&#39;:[1,2,3,4,5,6,7,8,9,10],
                   &#39;var1&#39;:random.sample(范围(1, 100), 10),
                   &#39;var2&#39;:random.sample(范围(1, 100), 10),
                   &#39;var3&#39;:random.sample(范围(1, 100), 10),
                   &#39;类&#39;: [&#39;a&#39;,&#39;a&#39;,&#39;a&#39;,&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;b&#39;,&#39;c&#39;,&#39;c&#39;,&#39;c&#39;]})

我想运行分类算法来预测 3 个类别。
因此，我将数据集分成训练集和测试集，并运行了 xgboost 分类
cl_cols = foo.filter(regex=&#39;var&#39;).columns
X_train, X_test, y_train, y_test = train_test_split(foo[cl_cols],
                                                        foo[[&#39;类&#39;]],
                                                        测试大小=0.33，随机状态=42）


模型= xgboost.XGBClassifier（目标=“二进制：逻辑”）
model.fit(X_train, y_train)

现在我想获取每个类的平均 SHAP 值，而不是从此代码生成的绝对 SHAP 值的平均值：
shap_values = shap.TreeExplainer(model).shap_values(X_test)
shap.summary_plot(shap_values, X_test)


此外，该图将 class 标记为 0,1,2。我怎么知道 0,1 和 0,1 属于哪一类？ 2与原文对应？
因为这段代码：
shap.summary_plot(shap_values, X_test,
                 类名= [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])

给出

和这段代码：
shap.summary_plot(shap_values, X_test,
                 类名= [&#39;b&#39;, &#39;c&#39;, &#39;a&#39;])

给出

所以我不再确定这个传说了。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/71753428/how-to-get-shap-values-for-each-class-on-a-multiclass-classification-problem-in</guid>
      <pubDate>Tue, 05 Apr 2022 14:21:03 GMT</pubDate>
    </item>
    <item>
      <title>批量读取Cifar10数据集</title>
      <link>https://stackoverflow.com/questions/37512290/reading-cifar10-dataset-in-batches</link>
      <description><![CDATA[我正在尝试读取 CIFAR10 数据集，这些数据集是从 https:// www.cs.toronto.edu/~kriz/cifar.html&gt;。我正在尝试使用 pickle 将其放入数据框中并读取其中的“数据”部分。但我收到此错误。
KeyError Traceback（最近一次调用最后一次）
&lt;ipython-input-24-8758b7a31925&gt;在&lt;模块&gt;()中
----&gt; 1 unpickle(&#39;数据集/cifar-10-batches-py/test_batch&#39;)

&lt;ipython-input-23-04002b89d842&gt;在unpickle（文件）中
      3 fo = 打开（文件，&#39;rb&#39;）
      4 dict = pickle.load(fo, 编码 =&#39;字节&#39;)
----&gt; 5 X = 字典[&#39;数据&#39;]
      6 fo.close()
      7 返回字典

密钥错误：“数据”。
我正在使用 ipython，这是我的代码：
def unpickle(文件):

 fo = 打开（文件，&#39;rb&#39;）
 dict = pickle.load(fo, 编码 =&#39;字节&#39;)
 X = 字典[&#39;数据&#39;]
 fo.close()
 返回字典

unpickle(&#39;数据集/cifar-10-batches-py/test_batch&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/37512290/reading-cifar10-dataset-in-batches</guid>
      <pubDate>Sun, 29 May 2016 16:29:55 GMT</pubDate>
    </item>
    </channel>
</rss>