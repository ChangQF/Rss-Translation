<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 12 May 2024 09:13:54 GMT</lastBuildDate>
    <item>
      <title>这个模型是否过度拟合或者数据质量太差？</title>
      <link>https://stackoverflow.com/questions/78467117/is-this-model-overfitting-or-is-the-quality-of-the-data-to-bad</link>
      <description><![CDATA[我目前正在从事一个机器学习项目。这是一个监督学习问题。我的目标是预测动物的给定数据（饲养、大小、重量……）成分（能量、维生素等……）。首先，我清理了数据并使用 LabelEncoding 对分类特征进行了编码。我选择随机森林作为算法，因为我读到树对于混合数据（分类和继续）很有用。所以我用几个参数训练了模型，我注意到我得到了很好的训练结果，但测试结果非常糟糕。在我看来，这表明过度拟合。该模型正在学习噪声。所以我知道我有两个选择：更多数据和降低模型的复杂性。但我尝试过PCA，删除了一些功能，更改了超参数（max_深度为15）。但这些行动都没有帮助。我已经减少了 max_深度，但随后我得到了更高的训练误差，但仍然是一个巨大的高测试误差。
那么这里的问题可能是什么？该模型是否过度拟合或数据噪音太大？
从 sklearn.model_selection 导入 GridSearchCV
从 sklearn.metrics 导入mean_absolute_error
从 sklearn.metrics 导入mean_squared_error, r2_score
从 sklearn.pipeline 导入 make_pipeline
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.neural_network 导入 MLPRegressor
从 sklearn.decomposition 导入 KernelPCA

参数网格 = {
    &#39;n_estimators&#39;: [i for i in range(50, 500, 50)],
    &#39;最大深度&#39;: [i 为范围 (5, 20, 5) 内的 i],
}

估计器 = RandomForestRegressor()
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=52)
X_train,scalerX = 归一化(X_train)
Y_train,scalerY = 归一化(Y_train)
X_test = scalerX.transform(X_test)
Y_test = scalerY.transform(Y_test)



gridModel = GridSearchCV(估计器=估计器，param_grid=param_grid，n_jobs=4，cv=5，评分=&#39;neg_mean_squared_error&#39;)
gridModel.fit(X_train,Y_train)


打印（gridModel.best_params_）

best_params：{&#39;max_深度&#39;：15，&#39;n_estimators&#39;：150}
将网格更改为 [i for i in range(5, 50, 5)] 时 best_params: {&#39;max_depth&#39;: 30, &#39;n_estimators&#39;: 50}
y_pred_test = gridModel.predict(X_test)
test_r2_score = r2_score(y_pred=y_pred_test,y_true=Y_test)

y_pred_train = gridModel.predict(X_train)
train_r2_score = r2_score(y_pred=y_pred_train,y_true=Y_train)

print(&quot;测试结果：&quot;,test_r2_score)
print(&quot;训练结果：&quot;,train_r2_score)

{&#39;最大深度&#39;：15，&#39;n_估计器&#39;：150}
结果测试：-2.952394644421328e+31
结果列车：0.8043381537451035
{&#39;最大深度&#39;：30，&#39;n_估计器&#39;：50}
结果测试：-7.37835882483847e+30
结果列车：0.9286384515560636]]></description>
      <guid>https://stackoverflow.com/questions/78467117/is-this-model-overfitting-or-is-the-quality-of-the-data-to-bad</guid>
      <pubDate>Sun, 12 May 2024 08:12:48 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError: 'QuantumCircuit' 对象没有属性 'cnot' 使用 qiskit.QuantumCircuit.cnot 时收到此错误</title>
      <link>https://stackoverflow.com/questions/78466923/attributeerror-quantumcircuit-object-has-no-attribute-cnot-receiving-this-e</link>
      <description><![CDATA[def ans(n, 深度):
    qc = 量子电路(n)
    对于范围（深度）内的 j：
        对于范围 (n) 内的 i：
            param_name = f&#39;theta_{j}_{i}&#39;
            theta_param = 参数(param_name)
            qc.rx(theta_param, i)
            qc.ry(theta_param, i)
            qc.rz(theta_param, i)
    对于范围 (n) 内的 i：
        如果我==n-1：
            qc.cnot(i, 0)
        别的：
            qc.cnot(i, i+1)
    返回质量控制

此方法似乎已被弃用并删除。我在尝试使用来自 此帖子。
我想要新版本的 qiskit 中提供的替代功能]]></description>
      <guid>https://stackoverflow.com/questions/78466923/attributeerror-quantumcircuit-object-has-no-attribute-cnot-receiving-this-e</guid>
      <pubDate>Sun, 12 May 2024 06:52:33 GMT</pubDate>
    </item>
    <item>
      <title>如何使用pytorch计算矩阵和向量的部分积？</title>
      <link>https://stackoverflow.com/questions/78466709/how-to-use-pytorch-to-compute-partial-products-of-a-matrix-and-a-vector</link>
      <description><![CDATA[我想实现一个速度快且 GPU 内存使用量较低的 MoE 模型，但我遇到了可以通过以下方式抽象的问题：
假设 A[1:1024][1:1024] 是一个矩阵，b[1:1024] 是一个向量，c[1:1024] 是一个稀疏向量，最多有 100 个非零元素。我需要计算由 d[i]=(A*b)[i] * c[i] 定义的 d[1:1024]。
我需要，对于一个固定的 A 和许多对 (b,c)，计算 d。我应该如何利用cuda的并行能力在pytorch下计算d？
取 A 的右行组成一个新矩阵？这可能很快，但会使用大量不必要的 GPU 内存，并使最大可能的批量大小太小。
使用 for 循环在右侧索引上一一计算 d 的元素？这可以节省 GPU 内存，但速度太慢。
如何既节省内存又提高速度？
这个 cuda 函数似乎可以完成工作。但我想在 pytorch 中工作，而不是 cuda 编程。
__global__ voidcomputeDots(float *A, float *b, float *c, float *out, int *mask) {
    int idx = blockIdx.x * 100 + threadIdx.x;
    if (threadIdx.x &lt; 100) {
        float *a = A + mask[idx] * VECTOR_SIZE；
        for (int i = 0; i &lt; VECTOR_SIZE; i++) {
            out[idx] += a[i] * b[idx+i];
        }
        输出[idx] *= c[idx];
    }
}
]]></description>
      <guid>https://stackoverflow.com/questions/78466709/how-to-use-pytorch-to-compute-partial-products-of-a-matrix-and-a-vector</guid>
      <pubDate>Sun, 12 May 2024 04:36:53 GMT</pubDate>
    </item>
    <item>
      <title>基本线性回归不预测 y=x+5 模式</title>
      <link>https://stackoverflow.com/questions/78466532/basic-linear-regression-not-predicting-y-x5-pattern</link>
      <description><![CDATA[然而，我的成本不断下降，但我的线性回归无法预测简单的模式 y=x+5。
我尝试了 alpha 的所有值，得到的最低成本是 3.00。
&lt;前&gt;&lt;代码&gt;x=[]
y=[]
对于范围（1000）内的 i：
    x.追加(i)
    y.追加(i+5)
def计算成本（x，y，w，b）：
    m=len(x)
    总成本=0
    成本=0
    对于范围 (m) 内的 i：
        f_wb = w*x[i]+b
        成本 += (f_wb - y[i])**2
    总成本 = 成本/(2*m)
    返回总成本
defgradient_descent(x,y,iter=200):
    m=len(x)
    w=0
    b=0
    阿尔法=0.0000001
    对于范围内的 i（iter）：
        dj_dw,dj_db=计算梯度(x,y,w,b)
        w-=alpha*dj_dw
        b-=alpha*dj_db
        如果（i%10==0）：
            成本=compute_cost(x,y,w,b)
            print(f“迭代 {i} 时的成本是 {cost}”)
    返回w,b
def Compute_gradient(x,y,w,b):
    m=len(x)
    dj_dw=0
    dj_db=0
    对于范围 (m) 内的 i：
        y_预测=w*x[i]+b
        dj_dw +=(y_predict-y[i])*x[i]
        dj_db +=(y_predict-y[i])
    dj_dw /=m
    dj_db /=m
    返回 dj_dw,dj_db
w,b=梯度下降(x,y)
q=w*10+b
打印（q）


这里是输出
迭代 0 时的成本为 157869.16787934472
第 10 次迭代的成本为 80221.1618112745
第 20 次迭代的成本为 40765.111675992295
第 30 次迭代的成本为 20715.91825636264
第 40 次迭代的成本为 10528.123091503312
迭代 50 时的成本为 5351.297861860537
迭代 60 时的成本为 2720.746400189429
迭代 70 时的成本为 1384.058239492339
迭代 80 时的成本为 704.8336487635617
迭代 90 时的成本为 359.6925315439946
迭代 100 时的成本为 184.31255755759477
迭代 110 时的成本为 95.19499412720666
迭代 120 时的成本为 49.910803477032324
迭代 130 时的成本为 26.900098542186733
迭代 140 时的成本为 15.20744043888423
迭代 150 时的成本为 9.265933534732453
迭代 160 时的成本为 6.246816032905587
迭代 170 时的成本为 4.712681193345336
迭代 180 时的成本为 3.93312529712553
迭代 190 时的成本为 3.537001070649068
10.064986594681566

出了什么问题？
代码是用 python 编写的，具有计算成本之类的函数来计算成本，梯度下降也是如此。我使用损失函数作为均方误差]]></description>
      <guid>https://stackoverflow.com/questions/78466532/basic-linear-regression-not-predicting-y-x5-pattern</guid>
      <pubDate>Sun, 12 May 2024 02:05:07 GMT</pubDate>
    </item>
    <item>
      <title>使用torch音频库创建数据集时出错</title>
      <link>https://stackoverflow.com/questions/78466420/error-when-using-torch-audio-library-to-create-a-data-set</link>
      <description><![CDATA[我正在学习 YT 课程，研究使用 torch 音频的城市 8k 数据集。作者编写了完全相同的代码，但在我收到错误时能够获得输出。
以下是我收到的错误：
运行时错误：找不到适当的后端来处理 uri C:\Users\hbhavnag\Documents\Hussain\ASU\collision detector\urban sound\UrbanSound8K\audio\5\100263-2-0-121。 wav 和格式 无。

以下是我的代码：
`从 torch.utils.data 导入数据集
将 pandas 导入为 pd
导入火炬音频
导入操作系统
类 UrbanSoundDataset（数据集）：
def __init__(self,annotation_file,audio_dir):
    self.annotations = pd.read_csv(annotation_file)
    self.audio_dir = 音频_dir

def __len__(自身):
    返回 len(self.annotations)

def __getitem__(自身，索引)：
    audio_sample_path = self._get_audio_sample_path(索引)
    标签 = self._get_audio_sample_label(索引)
    信号，sr = torchaudio.load（audio_sample_path）
    返回信号、标签

def _get_audio_sample_path（自身，索引）：
    Fold = f“fold{self.annotations.iloc[index,5]}”
    路径 = os.path.join(self.audio_dir, 折叠, self.annotations.iloc[index,0])
    返回路径

def _get_audio_sample_label（自身，索引）：
    返回 self.annotations.iloc[index,6]

如果 __name__ == “__main__”：
    注释_文件 = r“C:\Users\hbhavnag\Documents\Hussain\ASU\碰撞检测\城市声音\UrbanSound8K\metadata\UrbanSound8K.csv”
    audio_dir = r&quot;C:\Users\hbhavnag\Documents\Hussain\ASU\碰撞检测\城市声音\UrbanSound8K\audio&quot;
    usd = UrbanSoundDataset（注释文件，音频目录）
    print (f“数据集中有 {len(usd)} 个样本”)

信号，标签=美元[2]`

我尝试查找 torch audio 的文档，但不确定是否有任何内容可以直接帮助我，我假设存在一些版本兼容性问题。
我使用的是 Windows。]]></description>
      <guid>https://stackoverflow.com/questions/78466420/error-when-using-torch-audio-library-to-create-a-data-set</guid>
      <pubDate>Sun, 12 May 2024 00:34:24 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 HuggingFace Transformers Pipeline 在每个提示（如 vLLM）中生成多个文本补全而不触发错误？</title>
      <link>https://stackoverflow.com/questions/78466376/how-to-generate-multiple-text-completions-per-prompt-like-vllm-using-huggingfa</link>
      <description><![CDATA[我正在使用 HuggingFace Transformers Pipeline 库为给定提示生成多个文本完成。我的目标是利用像 GPT-2 这样的模型来生成不同的可能完成结果，例如 vLLM 中的默认值。但是，当我尝试指定 max_length 和 num_return_sequences 等参数时，我遇到了未使用 model_kwargs 的问题。
这是我正在使用的代码片段：
复制代码
从变压器导入 GPT2Tokenizer、GPT2LMHeadModel、管道
从输入导入列表，字典

def process_prompts(提示: List[str], 模型: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, num_completions: int = 3) -&gt;列表[列表[str]]：
    device = 0 if model.device.type == &#39;cuda&#39; else -1
    text_generator = pipeline(“文本生成”, model=model, tokenizer=tokenizer, device=device)
    输出 = []

    对于提示中的提示：
        尝试：
            结果= text_generator（提示，max_length = 50，num_return_sequences = num_completions，num_beams = num_completions）
            完成 = [结果[&#39;生成的文本&#39;] 结果中的结果]
            输出.追加（完成）
        除了异常 e：
            print(f&quot;处理错误提示{prompt}: {str(e)}&quot;)

    返回输出

如果 __name__ == “__main__”：
    tokenizer = GPT2Tokenizer.from_pretrained(“gpt2”)
    模型 = GPT2LMHeadModel.from_pretrained(“gpt2”)
    model.to(“cuda” if torch.cuda.is_available() else “cpu”)

    example_prompts = [“你好，你好吗？”]
    processed_outputs = process_prompts(example_prompts, model, tokenizer, num_completions=3)
    对于processed_outputs中的输出：
        打印（输出）

还有：
 results = text_generator(prompt, max_length=50, num_return_sequences=num_completions)

当我运行此程序时，出现以下错误：
模型不使用以下`model_kwargs`：[&#39;max_len&#39;]
注意：我知道生成参数中的拼写错误也可能触发此警告，但我已经检查并重新检查了参数名称。

和
 引发 ValueError(
ValueError：没有波束搜索的贪婪方法不支持不同于 1 的 `num_return_sequences`（得到 4）。

什么可能导致此错误，以及如何修复它以使用模型有效地生成多个完成？
交叉：https://discuss.huggingface.co/t/how-to-generate-multiple-text-completions-per-prompt-using-huggingface-transformers-pipeline-without -触发错误/86297]]></description>
      <guid>https://stackoverflow.com/questions/78466376/how-to-generate-multiple-text-completions-per-prompt-like-vllm-using-huggingfa</guid>
      <pubDate>Sun, 12 May 2024 00:06:07 GMT</pubDate>
    </item>
    <item>
      <title>通过 k 均值和分类进行聚类</title>
      <link>https://stackoverflow.com/questions/78466355/clustering-by-k-means-and-classification</link>
      <description><![CDATA[任务分配：
分类是根据类别进行的，这绝对是无监督的（有 5 个类别需要帮助）。
我被分配了这个任务，其中我有一个包含多列形式的数据集.. .
&lt;前&gt;&lt;代码&gt;` D1_16 D1_17 D1_18 D1_19 D1_20 D1_23 D1_24 D1_25 D1_26 D1_27 \
1 1 1 1 1 1 1 1 1 1 1
2 1 1 1 1 1 0 1 0 1 1
3 0 0 0 0 0 0 0 0 0 0
4 0 1 0 1 0 0 1 1 1 0
5 0 0 0 0 0 0 0 0 0 0
……………………………………
29502 0 0 0 0 0 0 0 0 0 0
29504 0 0 0 0 0 0 0 0 0 0
29505 0 0 0 0 0 0 0 0 0 0
29506 0 0 0 0 0 0 0 0 0 0
29507 0 0 0 0 0 0 0 0 0 0

       ... D68_29 D68_30 D68_31 D68_32 D68_33 D68_34 D68_35 D68_36 \
1 ... 0 0 1 0 0 0 0 0
2 ... 0 0 0 0 0 1 0 0
3 ... 0 0 0 0 0 0 0 0
4 ... 0 0 0 0 0 0 0 0
5 ... 0 0 0 0 0 0 0 0
………………………………
29502 ... 0 0 0 0 0 0 0 0
29504 ... 0 0 0 0 0 0 0 0
29505 ... 0 0 0 0 0 0 0 0
29506 ... 0 0 0 0 0 0 0 0
29507 ... 0 0 0 0 0 0 0 1`

我是否正确理解，首先需要进行聚类，例如根据k-means并选择一个合适的类，在此基础上我将数据分为两部分并标记“1”和“0”例如，然后对树进行分类？或者您需要以不同的方式执行聚类。老实说，我不太明白我要做什么，如果有任何想法，我将不胜感激。
我的程序：
best_score = -1
最佳_k = 0

对于范围 (2, 10) 内的 k：
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(df)
    Silhouette_avg = Silhouette_score(df, kmeans.labels_)
    

    如果 Silhouette_avg &gt;最佳得分：
        最佳得分 = 轮廓平均数
        最佳_k = k

kmeans = KMeans(n_clusters=best_k)
kmeans.fit(df)
df[&#39;cluster&#39;] = kmeans.labels_

best_cluster = np.argmax(np.bincount(kmeans.labels_))
df[&#39;目标&#39;] = np.where(df[&#39;簇&#39;] == best_cluster, 0, 1)

X_train, X_test, y_train, y_test = train_test_split(df.drop([&#39;集群&#39;, &#39;目标&#39;], axis=1), df[&#39;目标&#39;], test_size=0.2, random_state=42)

clf = 决策树分类器()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
准确度=准确度_分数（y_test，y_pred）

数据集：https://filetransfer.io/data-package/Aiawe648#link
我的所有程序： https://onecompiler.com/python/42cxfy2vz
有关聚类和分类的建议]]></description>
      <guid>https://stackoverflow.com/questions/78466355/clustering-by-k-means-and-classification</guid>
      <pubDate>Sat, 11 May 2024 23:54:57 GMT</pubDate>
    </item>
    <item>
      <title>eval(predvars, data, env) 中的错误：未找到对象“适配器”</title>
      <link>https://stackoverflow.com/questions/78466280/error-in-evalpredvars-data-env-object-adapter-not-found</link>
      <description><![CDATA[我正在尝试在 tf-idf 矩阵上训练随机森林分类器，其中的列是评论中的单词。
获得一个想法：
标签...1实际上是适配器
1 0 0.01495934 0.02880089
2 0 0.00000000 0.00000000
3 0 0.00000000 0.00000000

我使用 train_data 训练了模型，其中标签为 [0] 为负，[1] 为正。
这是代码：
set.seed(123)
random_forest_model &lt;- 训练（标签...1 ~ .,
               数据=训练数据，
               方法=“rf”，
               trControl = trainControl(方法 = &quot;cv&quot;, 数量 = 10),
               uneGrid = Expand.grid(mtry = 100),
               n树= 500，
               重要性=真）

我想使用经过训练的模型来预测另一个矩阵的评论是正面还是负面。
使用此代码：
# 对测试集进行预测
y_pred &lt;- 预测（random_forest_model，newdata = test_data）

问题是我收到此错误：
eval(predvars, data, env) 中的错误：未找到对象“适配器”

因为并非train_data中存在的所有单词（列）也存在于test_data中。对test_data的评价不同。
该模型的想法是预测在这种情况下评论是正面还是负面。不可能找到总是具有相同单词的矩阵。
我尝试输入 RF 模型数据框而不是矩阵，因为我读到它更好，但它没有解决问题。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78466280/error-in-evalpredvars-data-env-object-adapter-not-found</guid>
      <pubDate>Sat, 11 May 2024 22:55:59 GMT</pubDate>
    </item>
    <item>
      <title>用于预测前 k 个元素的平均倒数排名 (MRR) 理解</title>
      <link>https://stackoverflow.com/questions/78466130/mean-reciprocal-rank-mrr-understanding-for-predicting-top-k-elements</link>
      <description><![CDATA[我有一篇论文中的以下代码，他们已经实现了 MRR，以使用某种机器学习模型推荐 top-k 元素。
def MRR(test_y, pred_y, k=5):
    预测 = pd.DataFrame([])
    预测[“pred_y”] = pred_y
    预测[“y”] = test_y

    预测=预测.sort_values(“pred_y”,升序=False).reset_index(drop=True)
    预测[“pred_y_rank_index”] = (预测.index) + 1
    预测=预测.sort_values(“y”,升序=False)

    返回 sum(1 / 预测[“pred_y_rank_index”][:k]) / k

我不明白的是 - MRR 是如何根据代码在这里工作的。我从这个维基百科链接了解了 MRR。但是当我预测前 5 个元素时，在最好的情况下，如果预测元素的排名与实际元素的排名匹配 100%，那么 MRR 不应该产生结果 1 吗？但根据这段代码，前 5 个（k=5）元素的结果将是 ((1/1 + 1/2 + 1/3 + 1/4 + 1/5)/5) = 0.4566。
现在根据我的理解，在这种情况下，代码以错误的方式实现 MRR，或者 MRR 不是评估这种情况的正确指标。任何对此的了解或想法将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78466130/mean-reciprocal-rank-mrr-understanding-for-predicting-top-k-elements</guid>
      <pubDate>Sat, 11 May 2024 21:21:38 GMT</pubDate>
    </item>
    <item>
      <title>是否可以根据熟练程度/复杂性对文本进行有效分类？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78465977/is-it-possible-to-efficiently-classify-text-based-on-proficiency-complexity</link>
      <description><![CDATA[我目前正在开发一个项目，该项目需要一个包含大量文本的数据集，这些文本带有标记的文本复杂性/熟练程度、5/6 不同的复杂性级别。我尝试了多种方法，例如 API、可读性公式、搜索现有数据集等。但似乎没有任何效果。
我正在寻找基本文本，例如“她参观动物园”。她看到了很多动物。”，到精通的文本，例如：“他对行为经济学的深入研究细致地研究了影响消费者行为的认知偏差的复杂动态，提出了先进的预测模型来提高预测消费者购买模式的准确性。”&lt; /p&gt;
有人熟悉标记大量文本（50,000-100,000）吗？
如前所述，我尝试使用 API、可读性公式和现有数据集。但似乎没有任何作用。我无法使用任何模型，因为我没有数据集，这就是问题所在。]]></description>
      <guid>https://stackoverflow.com/questions/78465977/is-it-possible-to-efficiently-classify-text-based-on-proficiency-complexity</guid>
      <pubDate>Sat, 11 May 2024 20:11:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 python spacy 模块的词向量显示错误</title>
      <link>https://stackoverflow.com/questions/78464557/word-vectors-using-spacy-module-of-python-showing-error</link>
      <description><![CDATA[我正在尝试使用 python 的 spacy 模块获取词向量，并使用 en_core_web_lg 创建词向量。
从 sklearn.feature_extraction.text 导入 CountVectorizer
从 sklearn.feature_extraction.text 导入 TfidfVectorizer

tfidf=TfidfVectorizer(小写=False)

df1[&#39;问题1&#39;]=df1[&#39;问题1&#39;].apply(lambda x : str(x))
df1[&#39;问题2&#39;]=df1[&#39;问题2&#39;].apply(lambda x : str(x))

tot_ques=列表(df1[&#39;问题1&#39;]) + 列表(df1[&#39;问题2&#39;])

tfidf.fit(tot_ques)

idfscore=dict(zip(tfidf.get_feature_names_out(),tfidf.idf_))
打印（idf分数）

从 tqdm 导入 tqdm
导入spacy
nlp=spacy.load(&#39;en_core_web_lg&#39;)

向量1 = []
# 迭代每个问题1
对于 tqdm(list(df1[&#39;question1&#39;])) 中的 qu1：
doc1 = nlp（qu1）

# 初始化向量总和以及 IDF 总分
sum_vec = np.zeros(len(doc1[0].vector)) # 第一个单词的向量维度
Total_idf = 0.0 # 初始化IDF总分

# 遍历句子中的每个单词
对于 doc1 中的单词：
    vec = 词.向量
    
    # 计算单词的IDF分数
    尝试：
        idf = idfscore(str(单词))
    除了：
        idf = 0.0
    
    # 累加词向量的加权和
    sum_vec += vec * idf
    
    # 累计IDF总分
    总计 idf += idf

# 如果 IDF 总得分不为零，则计算均值向量
如果total_idf！= 0：
    平均向量 = 向量总和 / 总 idf
别的：
    mean_vec = sum_vec # 如果总 IDF 为零，则回退到 sum_vec

# 将均值向量附加到 vec1
vec1.append(mean_vec)

# 将计算出的向量分配给数据框中的新列“q1”
df1[&#39;q1&#39;] = vec1`

但是当我查看 q1 列的值时，每行都显示 0。]]></description>
      <guid>https://stackoverflow.com/questions/78464557/word-vectors-using-spacy-module-of-python-showing-error</guid>
      <pubDate>Sat, 11 May 2024 12:13:25 GMT</pubDate>
    </item>
    <item>
      <title>核逻辑回归 - 错误的预测</title>
      <link>https://stackoverflow.com/questions/78463519/kernel-logistic-regression-wrong-prediction</link>
      <description><![CDATA[我正在研究内核逻辑回归函数，但它们没有返回正确的预期预测。
它还会抛出指数溢出警告，目前已被抑制
&lt;前&gt;&lt;代码&gt;
    将 numpy 导入为 np
    进口警告

    warnings.filterwarnings(&#39;忽略&#39;)

    def monomial_kernel(d):
        def k(x, y, d=d):
            phi_x_y = 0
            prod_xy = np.dot(x.T,y)

            对于范围 (d+1) 中的 n：
                phi_x_y += (prod_xy ** n)

            返回 phi_x_y
            
        返回 k

    def rbf_kernel(西格玛):
        def k(x, y, 西格玛=西格玛):
            分子 = np.linalg.norm(x - y) **2
            分母 = 2 * (西格玛 ** 2)
            
            return np.exp(-分子/分母)

        返回 k

    定义 sigmoid(z):
        如果类型（z）== np.ndarray：
            z = z[0]
        尝试：
            返回 1 / (1 + np.exp(-z))
        除了：
            打印（z）

    def Logistics_regression_with_kernel(X, y, k, alpha, 迭代):

        n_samples, _ = X.shape
        偏差 = 0
        kernel_matrix = np.zeros((n_samples, n_samples))
        beta = np.zeros(n_samples)

        #创建核矩阵
        对于范围内的 i（n_samples）：
            对于 j 在范围内（n_samples）：
                kernel_matrix[i][j] = k(X[i], X[j])
        
        对于 _ 在范围内（迭代）：
            对于范围内的 i（n_samples）：
                总计 = 0
                对于 j 在范围内（n_samples）：
                    总计 += beta[j] * kernel_matrix[i][j]
                总计 += 偏差
                sigmoid_value = sigmoid(总计)
                t = y[i]

                beta += kernel_matrix[i] * alpha * (t - sigmoid_value)
                        
                偏差 += (alpha * (t - (sigmoid_value)))

        def 模型(x, beta=beta, 偏差=bias, k=k, ref=X):
            z = sum([k(ref[i], x) * beta[i] for i in range(ref.shape[0])]) + 偏差

            sig = sigmoid(z)
            # 打印（签名）
            回程(sig)
        返回模型



由于某种原因，它无法正确学习以下测试用例：
 def test4():
        
    f = lambda x, y, z, w: int(x*y*z - y**2*z*w/4 + x**4*w**3/8- y*w/2 &gt;= 0)

    训练示例 = [
        ([0.254, 0.782, 0.254, 0.569], 0),
        ([0.237, 0.026, 0.237, 0.638], 0),
        ([0.814, 0.18, 0.814, 0.707], 1),
        ([0.855, 0.117, 0.855, 0.669], 1),
        ([0.776, 0.643, 0.776, 0.628], 1),
        ([0.701, 0.71, 0.701, 0.982], 0),
        ([0.443, 0.039, 0.443, 0.356], 1),
        ([0.278, 0.105, 0.278, 0.158], 0),
        ([0.394, 0.203, 0.394, 0.909], 0),
        ([0.83, 0.197, 0.83, 0.779], 1),
        ([0.277, 0.415, 0.277, 0.357], 0),
        ([0.683, 0.117, 0.683, 0.455], 1),
        ([0.421, 0.631, 0.421, 0.015], 1)
    ]

    X, y = 地图(np.array, zip(*training_examples))

    h =logistic_regression_with_kernel(X, y, monomial_kernel(10), 0.01, 500)

    测试示例 = [
        ([0.157, 0.715, 0.787, 0.644], 0),
        ([0.79, 0.279, 0.761, 0.886], 1),
        ([0.903, 0.544, 0.138, 0.925], 0),
        ([0.129, 0.01, 0.493, 0.658], 0),
        ([0.673, 0.526, 0.672, 0.489], 1),
        ([0.703, 0.716, 0.088, 0.674], 0),
        ([0.276, 0.174, 0.69, 0.358], 1),
        ([0.199, 0.812, 0.825, 0.653], 0),
        ([0.332, 0.721, 0.148, 0.541], 0),
        ([0.51, 0.956, 0.023, 0.249], 0)
    ]
    print(f&quot;{&#39;x&#39;: ^30}{&#39;预测&#39;: ^11}{&#39;true&#39;: ^6}&quot;)
    对于 test_examples 中的 x、y：
        print(f&quot;{str(x) : ^30}{int(h(x)) : ^11}{y : ^6}&quot;)
    # x 预测为真
    # [0.157, 0.715, 0.787, 0.644] 0 0
    # [0.79, 0.279, 0.761, 0.886] 1 1
    # [0.903, 0.544, 0.138, 0.925] 0 0
    # [0.129, 0.01, 0.493, 0.658] 0 0
    # [0.673, 0.526, 0.672, 0.489] 1 1
    # [0.703, 0.716, 0.088, 0.674] 0 0
    # [0.276, 0.174, 0.69, 0.358] 1 1
    # [0.199, 0.812, 0.825, 0.653] 0 0
    # [0.332, 0.721, 0.148, 0.541] 0 0
    # [0.51, 0.956, 0.023, 0.249] 0 0

我的结果是：

我尝试将迭代次数增加到 4000，但仍然没有产生正确的结果]]></description>
      <guid>https://stackoverflow.com/questions/78463519/kernel-logistic-regression-wrong-prediction</guid>
      <pubDate>Sat, 11 May 2024 05:52:43 GMT</pubDate>
    </item>
    <item>
      <title>有序 Logit 回归的预测如何工作？</title>
      <link>https://stackoverflow.com/questions/78461070/how-does-prediction-for-ordered-logit-regression-work</link>
      <description><![CDATA[我正在学习有序 logit 回归，我想知道预测在数学上是如何工作的以及我如何自己在 python 中完成它。我知道在 python 中我可以简单地使用预测，但我想知道如何仅使用 model.summary() 中的 coef 进行预测。
导入 pandas 作为 pd
从 statsmodels.miscmodels.ordinal_model 导入 OrderedModel


数据 = pd.DataFrame({
    ‘分数’: [3.2, 4.5, 5.6, 6.7, 7.8, 8.9, 9.1],
    “评级”：[1,2,3,4,5,6,6]
})

X = 数据[[&#39;分数&#39;]]
y = 数据[&#39;评级&#39;]


ordinal_model = OrderedModel(y, X, distr=&#39;logit&#39;)


ordinal_results = ordinal_model.fit(method=&#39;bfgs&#39;)


打印（ordinal_results.summary（））


结果是：
时间：17:05:52
观察次数：7
Df 残差：1
DF型号：1
=================================================== ===========================
                 coef std err z P&gt;|z| [0.025 0.975]
-------------------------------------------------- ----------------------------
得分 66.3902 5669.125 0.012 0.991 -1.1e+04 1.12e+04
1/2 285.5835 2.56e+04 0.011 0.991 -4.98e+04 5.04e+04
2/3 4.2698 88.656 0.048 0.962 -169.493 178.032
3/4 4.1879 155.834 0.027 0.979 -301.241 309.617
4/5 4.3867 136.765 0.032 0.974 -263.668 272.442
5/6 3.4706 220.734 0.016 0.987 -429.161 436.102
=================================================== ===========================

使用 coef 向量如何获得与中相同的输出
ordinal_results.model.predict(ordinal_results.params, exog = (4.3))

&lt;预&gt;&lt;代码&gt;[[0.5264086 0.4735914 0.0.0.0.]]


我认为我应该对 coef 和新数据的线性和使用 softmax，但这不起作用]]></description>
      <guid>https://stackoverflow.com/questions/78461070/how-does-prediction-for-ordered-logit-regression-work</guid>
      <pubDate>Fri, 10 May 2024 15:16:52 GMT</pubDate>
    </item>
    <item>
      <title>损失值不断波动，MLP模型的一般问题</title>
      <link>https://stackoverflow.com/questions/78458260/the-value-of-loss-is-keeping-fluctuating-questions-about-mlp-model-in-general</link>
      <description><![CDATA[我正在为 ML 内容构建 MLP 模型，我对模型输出有一个基本问题。
这是我的源代码和结果
纪元 = 50
对于范围内的纪元（纪元）：
    对于输入，train_loader 中的标签：
        输出 = 模型（输入）
        损失=标准（输出，标签）

        优化器.zero_grad()
        loss.backward()
        优化器.step()

    loss_values.append(loss.item())
    print(f&#39;Epoch {epoch + 1}/{epochs}, 损失: {loss.item()}&#39;)

纪元 1/50，损失：0.2941759526729584
纪元 2/50，损失：0.2274172008037567
纪元 3/50，损失：0.1548108160495758
纪元 4/50，损失：0.09923569858074188
纪元 5/50，损失：0.07782179117202759
纪元 6/50，损失：0.08670808374881744
纪元 7/50，损失：0.1000475212931633
纪元 8/50，损失：0.08599527180194855
纪元 9/50，损失：0.06509505957365036
纪元 10/50，损失：0.0660080686211586
纪元 11/50，损失：0.07633966952562332
纪元 12/50，损失：0.06544400751590729
纪元 13/50，损失：0.07453220337629318
纪元 14/50，损失：0.0681438073515892
纪元 15/50，损失：0.07069016247987747
纪元 16/50，损失：0.05649592727422714
纪元 17/50，损失：0.05515648424625397
纪元 18/50，损失：0.05455780029296875
纪元 19/50，损失：0.06591354310512543
20/50 纪元，损失：0.06227065622806549
纪元 21/50，损失：0.050895411521196365
纪元 22/50，损失：0.05813339725136757
纪元 23/50，损失：0.05856814980506897
纪元 24/50，损失：0.056620728224515915
纪元 25/50，损失：0.05406007170677185
纪元 26/50，损失：0.05851085111498833
纪元 27/50，损失：0.04691702872514725
纪元 28/50，损失：0.036375436931848526
纪元 29/50，损失：0.043669767677783966
纪元 30/50，损失：0.047907356172800064
纪元 31/50，损失：0.04583781585097313
纪元 32/50，损失：0.044408515095710754
纪元 33/50，损失：0.04572493955492973
纪元 34/50，损失：0.0413966178894043
纪元 35/50，损失：0.047711536288261414
纪元 36/50，损失：0.046094246208667755
纪元 37/50，损失：0.03935185819864273
纪元 38/50，损失：0.036376748234033585
纪元 39/50，损失：0.04275327920913696
纪元 40/50，损失：0.04050033539533615
纪元 41/50，损失：0.03928723931312561
纪元 42/50，损失：0.038021307438611984
纪元 43/50，损失：0.039322346448898315
纪元 44/50，损失：0.03544142469763756
纪元 45/50，损失：0.03906610235571861
纪元 46/50，损失：0.03384337201714516
纪元 47/50，损失：0.040965259075164795
纪元 48/50，损失：0.038688428699970245
纪元 49/50，损失：0.041332412511110306
纪元 50/50，损失：0.03592131659388542

在此处输入图片描述
正如您所看到的，损失值总体上持续下降，但在某些点上仍然存在波动。这是我第一次构建 MLP 模型，因此我没有任何可以比较的经验。是正常现象吗？应该是在没有波动的情况下减少，还是波动很小很正常？
这是正常现象吗？应该是在没有波动的情况下减少，还是波动很小很正常？]]></description>
      <guid>https://stackoverflow.com/questions/78458260/the-value-of-loss-is-keeping-fluctuating-questions-about-mlp-model-in-general</guid>
      <pubDate>Fri, 10 May 2024 05:54:58 GMT</pubDate>
    </item>
    <item>
      <title>混淆矩阵错误：错误：“数据”和“参考”应该是具有相同级别的因素</title>
      <link>https://stackoverflow.com/questions/56995048/confusion-matrix-error-error-data-and-reference-should-be-factors-with-the</link>
      <description><![CDATA[我目前正在尝试构建一个神经网络来预测人们在数据中的排名。 
等级系统为：A、B、C、D、E
一切都运行得非常顺利，直到我到达我的混淆矩阵。我收到错误“错误：data 和 reference 应该是具有相同级别的因素。”。我在其他帖子中尝试了许多不同的方法，但似乎都不起作用。
NNPredictions 和 test$Rank 中的级别相同。我用 table() 检查了它们。
库（readxl）
库（插入符号）
图书馆（神经网络）
库（预测）
图书馆（tidyverse）
库（ggplot2）



间接 &lt;-read_excel(&quot;C:/Users/Abdulazizs/Desktop/Projects/Indirect/FIltered Indirect.xlsx&quot;,
    n_最大 = 500)

间接$Direct_or_Indirect &lt;- NULL


间接$parentaccount &lt;- NULL


sum(is.na(间接))


计数 &lt;- 表（间接$排名）



条形图（计数）

总结（计数）



第 2 部分 &lt;- createDataPartition(Indirect$Rank, ti​​mes = 1, p = .8, list = FALSE, groups = min(5, length(Indirect$Rank)))

火车 &lt;- 间接[part2, ]
测试 &lt;- 间接[-part2, ]

设置.种子(1234)

TrainingParameters &lt;- trainControl(方法 = &quot;repeatedcv&quot;, 数量 = 10, 重复 = 10)

as.data.frame(火车)
as.data.frame(测试)

NNModel &lt;- 训练(训练[,-7], 训练$Rank,
                  方法=“nnet”，
                  trControl= 训练参数，
                  预处理=c(&quot;尺度&quot;,&quot;中心&quot;),
                  na.action = na.omit
）

NNPredictions &lt;-预测（NNModel，测试，类型=“原始”）



摘要（NN预测）





fusionMatrix(NNPredictions, 测试$Rank)

长度（NN预测）
长度（测试$排名）

&lt;块引用&gt;
  长度（NN预测）
  [1] 98
  长度（测试$排名）
  [1]98

表（NNPredictions，测试$Rank，useNA =“ifany”）
NN预测 A B C D E
            1 0 0 0 0
            乙 0 6 0 0 0
            0 0 11 0 0
            d 0 0 0 18 0
            E 0 0 0 0 62]]></description>
      <guid>https://stackoverflow.com/questions/56995048/confusion-matrix-error-error-data-and-reference-should-be-factors-with-the</guid>
      <pubDate>Thu, 11 Jul 2019 18:05:03 GMT</pubDate>
    </item>
    </channel>
</rss>