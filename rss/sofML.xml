<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 14 Jan 2025 03:16:58 GMT</lastBuildDate>
    <item>
      <title>stable_baselines3：为什么比较 ep_info_buffer 与评估时奖励不匹配？</title>
      <link>https://stackoverflow.com/questions/79353843/stable-baselines3-why-the-reward-does-not-match-comparing-ep-info-buffer-vs-eva</link>
      <description><![CDATA[我正在使用 stable_baselines3 库，这时我发现了一些意想不到的东西。
这里有一个简单的代码来重现这个问题：
import gymnasium as gym

from stable_baselines3 import DQN

env = gym.make(&quot;CartPole-v1&quot;)

model = DQN(&quot;MlpPolicy&quot;, env, verbose=0, stats_window_size=100_000)
model.learn(total_timesteps=100_000)

看看最后一集的奖励：
print(model.ep_info_buffer[-1])


{&#39;r&#39;: 409.0, &#39;l&#39;: 409, &#39;t&#39;: 54.87983

但是如果我使用以下代码评估模型：
obs, info = env.reset()
total_reward = 0
while True:
action, _states = model.predict(obs, deterministic=True)
obs, reward, termed, truncated, info = env.step(action)
total_reward = total_reward + reward
if termed or truncated:
obs, info = env.reset()
break

print(&quot;total_reward {}&quot;.format(total_reward))


total_reward 196.0

我得到了不同的奖励，这是我没有预料到的。
我预计会得到与 409 相同的奖励model.ep_info_buffer[-1]。
为什么会有这种差异？.ep_info_buffer 与每集奖励不同吗？]]></description>
      <guid>https://stackoverflow.com/questions/79353843/stable-baselines3-why-the-reward-does-not-match-comparing-ep-info-buffer-vs-eva</guid>
      <pubDate>Tue, 14 Jan 2025 02:14:32 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练我的音频分类模型时出现错误 as_list()</title>
      <link>https://stackoverflow.com/questions/79353105/error-as-list-when-trying-to-train-my-audio-classification-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79353105/error-as-list-when-trying-to-train-my-audio-classification-model</guid>
      <pubDate>Mon, 13 Jan 2025 18:12:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python 为 UNO 纸牌游戏创建 AI 玩家？[关闭]</title>
      <link>https://stackoverflow.com/questions/79352641/how-do-i-create-an-ai-player-for-uno-card-game-in-python</link>
      <description><![CDATA[我熟悉 Python 的基础知识，我想创建一个 UNO 纸牌游戏，让 AI 与用户竞争。我唯一的经验是使用 Python 中的极小极大算法制作井字游戏机器人。
我读过，极小极大算法是不可行的，因为 UNO 有更大的移动范围。我读过一些关于蒙特卡洛树搜索、深度强化学习、神经网络等的文章。对于 1v1（AI 与玩家）的 UNO 游戏，哪种方法更容易实现？
我读过的一点资料建议在纸牌游戏中使用蒙特卡洛算法。我想知道这里的情况是否如此，这是否确实是最佳方法。]]></description>
      <guid>https://stackoverflow.com/questions/79352641/how-do-i-create-an-ai-player-for-uno-card-game-in-python</guid>
      <pubDate>Mon, 13 Jan 2025 15:15:46 GMT</pubDate>
    </item>
    <item>
      <title>尽管将 .mlmodel 添加到 Build Phases，但为什么 Xcode 中的自动链接不起作用？（找不到范围）</title>
      <link>https://stackoverflow.com/questions/79352561/why-is-auto-linking-of-mlmodel-not-working-in-xcode-despite-adding-it-to-build</link>
      <description><![CDATA[我正在开发一个使用 .mlmodel 文件的 Xcode 项目。我通过 Build Phases &gt; 添加了该文件复制资源，但自动链接似乎不起作用。
我到目前为止尝试过的方法：

确保 .mlmodel 文件已添加到目标成员资格下。
验证文件是否存在并可从其原始位置访问。
清理构建文件夹（Cmd + Shift + K）并删除 DerivedData 文件夹。
验证构建设置中的 CoreML 模型编译设置已启用。

]]></description>
      <guid>https://stackoverflow.com/questions/79352561/why-is-auto-linking-of-mlmodel-not-working-in-xcode-despite-adding-it-to-build</guid>
      <pubDate>Mon, 13 Jan 2025 14:50:23 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：'RasterIOSource'对象没有属性'urlpath'[关闭]</title>
      <link>https://stackoverflow.com/questions/79352075/attributeerror-rasteriosource-object-has-no-attribute-urlpath</link>
      <description><![CDATA[我正在运行一个使用detectron2和zenodo库检测树冠的代码。
print(cat_tc[&#39;sepilok_rgb&#39;]) # 打印目录条目详细信息
print(&quot;URL Path:&quot;, cat_tc[&#39;sepilok_rgb&#39;].urlpath) # 打印urlpath

# 使用rioxarray直接从urlpath打开数据集
tc_rgb = rioxarray.open_rasterio(cat_tc[&#39;sepilok_rgb&#39;].urlpath)

print(&#39;dims =&#39;, tc_rgb.dims, &#39;, number of bands =&#39;, len(tc_rgb.data_vars), &#39;, crs =&#39;, tc_rgb.rio.crs)

我想打印url路径，但我收到属性错误]]></description>
      <guid>https://stackoverflow.com/questions/79352075/attributeerror-rasteriosource-object-has-no-attribute-urlpath</guid>
      <pubDate>Mon, 13 Jan 2025 11:46:33 GMT</pubDate>
    </item>
    <item>
      <title>使用 FLOPS 和 FLOP 计算 GPU 执行时间</title>
      <link>https://stackoverflow.com/questions/79351271/calculate-gpu-excution-time-with-flops-and-flops</link>
      <description><![CDATA[我正在对深度学习模型进行性能分析，并想验证以下计算执行时间的公式：
公式：时间（秒）= 模型 FLOP/GPU FLOP 每秒
上下文：
我有一个总共 24,029,362,176 FLOP 的模型。
我正在两个不同的 GPU 上对其进行测试：

NVIDIA A100 (80 GB)，峰值性能为 312 TFLOP（FP32）。
NVIDIA Tesla T4（Colab GPU），峰值性能为 8.1 TFLOP（FP32）。

使用该公式，我计算了这些 GPU 的执行时间。但是，我想确认一下这个公式是否适用于估算比较 2 个 GPU 的执行时间。
这个公式对于估算执行时间模型是否准确？
]]></description>
      <guid>https://stackoverflow.com/questions/79351271/calculate-gpu-excution-time-with-flops-and-flops</guid>
      <pubDate>Mon, 13 Jan 2025 05:53:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在 pytorch 中并行计算不同权重和输入的神经网络？</title>
      <link>https://stackoverflow.com/questions/79350425/how-to-calculate-neural-net-for-different-weights-and-inputs-in-parallel-in-pyto</link>
      <description><![CDATA[我正在使用 pytorch 进行机器学习。
有一些实现神经网络的类继承自 nn.Module。实现了一些网络结构。
列表中存储了不同的参数（权重、偏差）：
parameters = [[...], ..., [...]] 。另一个列表中还存储了不同的输入：inputs = [[...], ..., [...]]。所有输入集的大小和维度都相同。
我需要计算输入和参数的网络输出，即我需要构建一个矩阵：




参数[0]
参数[1]
...
参数[n]




输入[0]
结果[0][0]
结果[0][1]
...
结果[0][n]


输入[1]
结果[ 1][0]
result[1][1]
...
result[1][n]


...
...
...
...
...


input[m]
result[m][0]
result[m][1]
...
result[m][n]



最明显的方法是使用两个循环。这对 CPU 来说很好。但如何为 GPU 实现它？每个 result[i][j] 都可以并行计算，所以我会使用某种批量计算。
你能给我一些解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/79350425/how-to-calculate-neural-net-for-different-weights-and-inputs-in-parallel-in-pyto</guid>
      <pubDate>Sun, 12 Jan 2025 18:00:35 GMT</pubDate>
    </item>
    <item>
      <title>使用矩阵在 PyTorch 上计算公式</title>
      <link>https://stackoverflow.com/questions/79350403/calculate-formulas-on-pytorch-using-matrix</link>
      <description><![CDATA[我有方程式：
$e_{ij} = \frac{X_i W^Q (X_j W^K + A^K_{ij}) }{\sqrt{D_z}}$
$\alpha_{ij} = softmax(e_{ij})$
$z_{i} = \sum_j \alpha_{ij} (X_j W^V + A^V_{ij})$

其中大小：
X：[B，S，H，D]
每个 W：[H，D，D]
每个 A：[S，S，H，D]

我如何通过矩阵运算计算它？
我有一个部分解决方案
import torch
import torch.nn. functional as F

B, S, H, D = X.shape
d_z = D # 为简单起见，假设 d_z 等于 D

W_Q = torch.randn(H, D, D)
W_K = torch.randn(H, D, D)
W_V = torch.randn(H, D, D)

a_K = torch.randn(S, S, H, D)
a_V = torch.randn(S, S, H, D)
}
XW_Q = torch.einsum(&#39;bshd,hde-&gt;bshe&#39;, X, W_Q) # [B, S, H, D] @ [H, D, D] -&gt; [B，S，H，D]
XW_K = torch.einsum(&#39;bshd,hde-&gt;bshe&#39;, X, W_K) # [B，S，H，D] @ [H，D，D] -&gt; [B，S，H，D]

e_ij_numerator = XW_Q.unsqueeze(2) @ (XW_K.unsqueeze(1) + a_K).transpose(-1, -2) # [B，S，1，H，D] @ [B，1，S，H，D] -&gt; [B，S，S，H，D]
e_ij = e_ij_numerator / torch.sqrt(torch.tensor(d_z, dtype=torch.float32)) # [B，S，S，H，D]

XW_V = torch.einsum(&#39;bshd,hde-&gt;bshe&#39;, X, W_V) # [B，S，H，D] @ [H，D，D] -&gt; [B，S，H，D]
alpha = F.softmax(e_ij, dim=2) # [B，S，S，H，D]

z_i = torch.einsum(&#39;bshij,bshjd-&gt;bshid&#39;, alpha, XW_V.unsqueeze(1) + a_V) # [B，S，S，H，D] @ [B，1，S，H，D] -&gt; [B, S, S, H, D]

但 z 应该是 [B, S, H,D]]]></description>
      <guid>https://stackoverflow.com/questions/79350403/calculate-formulas-on-pytorch-using-matrix</guid>
      <pubDate>Sun, 12 Jan 2025 17:48:23 GMT</pubDate>
    </item>
    <item>
      <title>如何在 GPU 中实现 KNNImputer？</title>
      <link>https://stackoverflow.com/questions/79350213/how-to-implement-knnimputer-in-gpu</link>
      <description><![CDATA[我正在处理 Kaggle 上的一个大型数据集，并希望通过使用 GPU 加速进行 KNN 插补来加快插补过程。我目前的方法使用 sklearn 的基于 CPU 的 KNNImputer，但它的速度太慢了，无法满足我的需求。
我听说 RAPIDS cuML 提供 GPU 加速的 KNN 插补。这是我到目前为止尝试的代码
import pandas as pd
import cudf
from cuml.experimental.preprocessing import KNNImputer

# 将 Pandas DataFrame 转换为 cuDF DataFrame
df_bad_cleaned_gpu = cudf.DataFrame.from_pandas(df_bad_cleaned)

# 使用邻居初始化 KNN imputer
knn_imputer_gpu = KNNImputer(n_neighbors=36)

# 拟合和转换
df_bad_knn_filled_gpu = knn_imputer_gpu.fit_transform(df_bad_cleaned_gpu)

# 转换回 Pandas DataFrame（如果需要）
df_bad_knn_filled = df_bad_knn_filled_gpu.to_pandas()

这是使用 RAPIDS 在 GPU 上实现 KNN 插补的正确方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79350213/how-to-implement-knnimputer-in-gpu</guid>
      <pubDate>Sun, 12 Jan 2025 16:00:11 GMT</pubDate>
    </item>
    <item>
      <title>DMIR 的 XMorpher 模型 - 数据集问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79180321/xmorpher-model-for-dmir-dataset-problem</link>
      <description><![CDATA[我正在尝试运行 GitHub 项目 XMorpher (https://github.com/Solemoon/XMorpher/tree/main)，但作者没有提供正确的数据集？他在代码中使用了：
 train_labeled_unlabeled_dir = &#39;data/train_labeled_unlabeled&#39;
train_unlabeled_unlabeled_dir = &#39;data/train_unlabeled_unlabeled&#39;
test_labeled_labeled_dir = &#39;data/test&#39;

但实际上数据文件夹仅包含 0_1.mat 文件，在 Matlab 中打开时，该文件包含 2 个变量：
fix_img # 大小：144x144x128 (int16)
mov_img # 大小：144x144x128 (int16)

由于一张图片包含 5308416 个字节，我无法看到变量的值... 还有其他方法可以运行此模型或我可以使用其他数据集吗？]]></description>
      <guid>https://stackoverflow.com/questions/79180321/xmorpher-model-for-dmir-dataset-problem</guid>
      <pubDate>Tue, 12 Nov 2024 08:25:50 GMT</pubDate>
    </item>
    <item>
      <title>删除边界框detectron2之外的所有内容</title>
      <link>https://stackoverflow.com/questions/78626157/delete-everything-outside-of-bounding-boxes-detectron2</link>
      <description><![CDATA[我已经训练了一个包含一个类的 detectron2 模型。现在我想将不在 bbox 内的所有内容设置为白色，其余部分保持原样。一张图片上可以有多个 bbox，它们可以叠加。
我阅读了 detectron2 的文档以及 cv2，但我找不到解决问题的方法。
这是我的预测代码：
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
import cv2

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(&#39;COCO-Detection/tmp&#39;))
cfg.MODEL.WEIGHTS = &#39;tmp&#39;

预测器 = DefaultPredictor(cfg)

img = cv2.imread(&#39;tmp&#39;)

out = 预测器(img)
]]></description>
      <guid>https://stackoverflow.com/questions/78626157/delete-everything-outside-of-bounding-boxes-detectron2</guid>
      <pubDate>Sat, 15 Jun 2024 09:09:50 GMT</pubDate>
    </item>
    <item>
      <title>使用带有 max_new_tokens 的 LLM 进行不完整输出</title>
      <link>https://stackoverflow.com/questions/77061898/incomplete-output-with-llm-with-max-new-tokens</link>
      <description><![CDATA[我正在试验 Huggingface LLM 模型。
我注意到的一个问题是模型的输出突然结束，我理想情况下希望它完成它所在的段落/句子/代码。（或者尝试在某个固定数量的标记内完成答案）
虽然我提供了 max_new_tokens = 300，并且在提示中我写道：
“输出最多应为 300 个字。”
响应总是不完整的，并且突然结束。有什么方法可以要求在所需的输出标记数内完成输出？
代码：
checkpoint = “HuggingFaceH4/starchat-alpha”
device = “cuda” if torch.cuda.is_available() else “cpu”
class StarCoderModel:
def __init__(self):
self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)
# 如果需要 gpu，请确保在 docker run 命令中提供 `--gpus all`
self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=&#39;auto&#39;)

def infer(self, input_text, token_count):
输入 = self.tokenizer.encode(input_text, return_tensors=&quot;pt&quot;).to(device)
输出 = self.model.generate(inputs, max_new_tokens=token_count, pad_token_id=self.tokenizer.eos_token_id)
return self.tokenizer.decode(outputs[0])[len(input_text):]

示例输出：
private DataType FuntionName(String someId) {
// TODO：用利用 someId 获取信息的实现替换
return DataType.Value;
}

注释：

- 如果代码中存在 someId，则使用客户端的 getAPI 并以 someId 作为参数来获取一些信息。
- 如果

]]></description>
      <guid>https://stackoverflow.com/questions/77061898/incomplete-output-with-llm-with-max-new-tokens</guid>
      <pubDate>Thu, 07 Sep 2023 18:02:00 GMT</pubDate>
    </item>
    <item>
      <title>如何实现巴特沃斯滤波器</title>
      <link>https://stackoverflow.com/questions/74003337/how-to-implement-a-butterworth-filter</link>
      <description><![CDATA[我正在尝试使用 python 实现 butterworthfilter
数据来自 CSV 文件，名为 Samples.csv，如下所示
998,4778415
1009,209592
1006,619094
1001,785406
993,9426543
990,1408991
992,736118
995,8127334
...

该列调用欧几里得范数。数据范围从 0 到 1679.286158，共有 1838 行。
这是我使用的代码：
from scipy.signal import filtfilt
from scipy import stats

import csv
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import scipy
def plot():
data=pd.read_csv(&#39;Samples.csv&#39;,sep=&quot;;&quot;, decimal=&quot;,&quot;)
sensor_data=data[[&#39;Euclidian Norm&#39;]]
sensor_data=np.array(sensor_data)

time=np.linspace(0,1679.286158,1838)
plt.plot(time,sensor_data)
plt.show()

filtered_signal=bandPassFilter(sensor_data)
plt.plot(time,sensor_data)
plt.show()

def bandPassFilter(signal):
fs = 4000.0
lowcut=20.0
highcut=50.0

nyq=0.5*fs
low=lowcut/nyq
high=highcut/nyq

order =2

b,a=scipy.signal.butter(order,[low,high],&#39;bandpass&#39;,analog=False)

y=scipy.signal.filtfilt(b,a,signal,axis=0)

return(y)

plot()


我的问题是我的数据没有任何变化。它没有过滤我的数据。过滤数据的图表与源数据相同。有人知道哪里出了问题吗？
第一个图表是源数据，第二个图表是过滤后的图表，在我看来，它们看起来像是同一张图表。
]]></description>
      <guid>https://stackoverflow.com/questions/74003337/how-to-implement-a-butterworth-filter</guid>
      <pubDate>Sun, 09 Oct 2022 08:49:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们不应该在同一层使用多个激活函数？[关闭]</title>
      <link>https://stackoverflow.com/questions/63125782/why-shouldnt-we-use-multiple-activation-functions-in-the-same-layer</link>
      <description><![CDATA[我见过的所有应用神经网络的示例或案例都有一个共同点 - 它们在属于特定层的所有节点中使用特定类型的激活函数。
据我所知，每个节点都使用非线性激活函数来了解数据中的特定模式。如果是这样，为什么不使用多种类型的激活函数？
我确实找到了一个链接，它基本上说如果我们每层只使用一个激活函数，管理网络会更容易。还有其他好处吗？]]></description>
      <guid>https://stackoverflow.com/questions/63125782/why-shouldnt-we-use-multiple-activation-functions-in-the-same-layer</guid>
      <pubDate>Tue, 28 Jul 2020 01:28:55 GMT</pubDate>
    </item>
    <item>
      <title>weka java api stringtovector 异常</title>
      <link>https://stackoverflow.com/questions/6644191/weka-java-api-stringtovector-exception</link>
      <description><![CDATA[所以我有这个使用 Weka 的 Java API 的代码：
 String html = &quot;blaaah&quot;;
Attribute input = new Attribute(&quot;html&quot;,(FastVector) null);

FastVector inputVec = new FastVector();
inputVec.addElement(input);

Instances htmlInst = new Instances(&quot;html&quot;,inputVec,1);
htmlInst.add(new Instance(1)); 
htmlInst.instance(0).setValue(0, html);

System.out.println(htmlInst);

StringToWordVector filter = new StringToWordVector();
filter.setInputFormat(htmlInst);
Instances dataFiltered = Filter.useFilter(htmlInst, filter);

但是在 filter.setInputFormat(htmlInst) 行上，Java 抱怨该函数抛出了未处理的异常...
我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/6644191/weka-java-api-stringtovector-exception</guid>
      <pubDate>Sun, 10 Jul 2011 22:35:28 GMT</pubDate>
    </item>
    </channel>
</rss>