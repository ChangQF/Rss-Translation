<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 31 Jan 2024 15:13:29 GMT</lastBuildDate>
    <item>
      <title>Pytorch RuntimeError：函数“NativeBatchNormBackward0”在第 0 个输出中返回了 nan 值</title>
      <link>https://stackoverflow.com/questions/77914164/pytorch-runtimeerror-function-nativebatchnormbackward0-returned-nan-values-in</link>
      <description><![CDATA[我尝试从一篇提供 Tensorflow 代码的论文中实现一个卷积神经网络，并将其转换为 Pytorch。
使用 torch.autograd.detect_anomaly() 我收到错误：
RuntimeError：函数“NativeBatchNormBackward0”在第 0 个输出中返回了 nan 值。
第一次调用loss.backward()期间。
我无法找出此错误背后的原因，因为我在网络上找不到任何有关它的信息。
所讨论的架构是在 conv1d() 期间 -&gt; BatchNorm1d() -&gt;; ReLU() -&gt;; MaxPool1d() 顺序。如果我注释掉 BatchNorm1d() 层，则不会发生错误。
我运行一个自定义损失函数，根据三个特征的两个 BCELosses 和一个 MSELoss 计算加权损失。
损失本身以数字形式返回，并且表现符合预期。]]></description>
      <guid>https://stackoverflow.com/questions/77914164/pytorch-runtimeerror-function-nativebatchnormbackward0-returned-nan-values-in</guid>
      <pubDate>Wed, 31 Jan 2024 14:33:58 GMT</pubDate>
    </item>
    <item>
      <title>我在使用决策树分类器处理数据集时遇到此错误</title>
      <link>https://stackoverflow.com/questions/77913583/i-got-this-error-while-working-on-a-dataset-using-decision-tree-classifier</link>
      <description><![CDATA[&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ImportError Traceback（最近一次调用最后一次）
&lt;ipython-input-48-f146622b3284&gt;在&lt;细胞系：1&gt;()
----&gt; 1 从sklearn.tree导入DecisionTreeClassifier

1 帧
 中的 /usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py
     24 从 scipy.sparse 导入 issparse
     25
---&gt; 26 来自 ..base 导入 (
     27 基础估计器，
     28 分类器混合，

ImportError：无法从“sklearn.base”导入名称“_fit_context”（/usr/local/lib/python3.10/dist-packages/sklearn/base.py）




升级 scikit learn 并卸载或重新安装它。
]]></description>
      <guid>https://stackoverflow.com/questions/77913583/i-got-this-error-while-working-on-a-dataset-using-decision-tree-classifier</guid>
      <pubDate>Wed, 31 Jan 2024 13:09:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么我不能使用基于流程的并行性有效地并行化我的强化学习程序？</title>
      <link>https://stackoverflow.com/questions/77913525/why-cant-i-effectively-parallelize-my-reinforcement-learning-programs-using-pro</link>
      <description><![CDATA[我的目标是使用 Stable_Baselines3 库同时运行多个强化学习程序。我注意到，随着程序数量的增加，程序的迭代速度逐渐降低，这是相当令人惊讶的，因为每个程序应该运行在不同的进程（核心）上。
这是我的程序：
from joblib import 并行，延迟

进口健身房
# 从 sbx 导入 SAC
进口火炬

从 stable_baselines3 导入 SAC
定义火车（）：


    env =gym.make(“Humanoid-v4”)

    模型 = SAC(“MlpPolicy”, env, verbose=1)
    model.learn（total_timesteps=7e5，progress_bar=True）

def train_model():

    火车（）



如果 __name__ == &#39;__main__&#39;:
    程序数量 = 1
    并行(n_jobs=10)(延迟(train)() for i in range(num_of_programs))

num_of_programs 用于控制我尝试并行运行的程序数量。
以下是一些统计数据 -
 程序数量 迭代速度
1 1 ~102 次/秒
2 3 ~60 次/秒
3 10~20次/秒

我确保请求足够的资源，这样就不存在资源限制。这就是我使用 slurm 请求资源的方式 - srun --time=10:00:00 --nodes=1 --cpus-per-task=16 --mem=32G --partition=gpu --gres =gpu:a100-pcie:1 --pty /usr/bin/bash
因此我有 16 个 cpu、32G 内存和 40 GB GPU。
当我从 stable_baselines3 迁移到 sbx 时，我注意到了同样的问题。 stable_baselines3 使用 torch 作为其深度学习库，而后者则使用 JAX。]]></description>
      <guid>https://stackoverflow.com/questions/77913525/why-cant-i-effectively-parallelize-my-reinforcement-learning-programs-using-pro</guid>
      <pubDate>Wed, 31 Jan 2024 13:00:25 GMT</pubDate>
    </item>
    <item>
      <title>时间序列预测访问日期与客户类别图不准确</title>
      <link>https://stackoverflow.com/questions/77912045/time-series-forecasting-visit-dates-with-customer-classes-graph-not-accurate</link>
      <description><![CDATA[我正在尝试对一堆类和日期时间进行时间序列预测，但由于某种原因我的图表看起来像这样，我的完整代码如下：
从 google.colab 导入驱动器
drive.mount(&#39;/content/gdrive&#39;,force_remount = True)

将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.metrics 导入mean_squared_error
将 matplotlib.pyplot 导入为 plt

data = pd.read_csv(&#39;gdrive/My Drive/Colab_Notebooks/classproject/classdata.csv&#39;, parse_dates=[&#39;time_date&#39;], index_col=&#39;time_date&#39;)
类id = 数据[&#39;类id&#39;]
时间日期 = 数据.索引.日期
数据[&#39;日期&#39;] = data.index.日期

类id = 数据[&#39;类id&#39;]
time_date = data.index.to_series()
m1 = class_id.ne(class_id.shift())
m2 = time_date.dt.date.ne(time_date.dt.date.shift())
data[&#39;count&#39;] = data.groupby((m1 | m2).cumsum()).cumcount().add(1).values

out = data[data.groupby(data.index.date).transform(&#39;size&#39;).gt(1)]

!pip 安装 pandas-datareader

将 pandas_datareader.data 作为 web 导入
导入日期时间

将 pandas 导入为 pd
pd.set_option(&#39;display.max_columns&#39;, None)
pd.set_option(&#39;display.max_rows&#39;, None)

将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns

sns.set()

plt.ylabel(&#39;类别数量&#39;)
plt.xlabel(&#39;日期&#39;)
plt.xticks（旋转=45）

out.index = pd.to_datetime(out[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;)
plt.plot(out.index, out[&#39;count&#39;], )



而我从其中获取此时间序列代码的博客有这样的结果

所以我不确定是否应该继续XD
我的输入数据是这样的：
时间戳/class_id
2021-09-27 06:00:00 / A
2021-09-27 03:00:00 / A
2021-09-27 01:00:00 / A
2021-09-27 08:29:00 / C
2021-05-23 08:08:49 / B
2021-05-23 03:21:49 / B
2021-05-23 01:22:11 / C
处理它并添加计数和日期列后：
计数/时间戳/class_id/日期
1 / 2021-09-27 06:00:00 / A / 2021-09-27
2 / 2021-09-27 03:00:00 / A / 2021-09-27
3 / 2021-09-27 01:00:00 / A / 2021-09-27
1 / 2021-09-27 08:29:00 / C / 2021-09-27
1 / 2021-05-23 08:08:49 / B / 2021-05-23
2 / 2021-05-23 03:21:49 / B / 2021-05-23
1 / 2021-05-23 01:22:11 / C / 2021-05-23]]></description>
      <guid>https://stackoverflow.com/questions/77912045/time-series-forecasting-visit-dates-with-customer-classes-graph-not-accurate</guid>
      <pubDate>Wed, 31 Jan 2024 09:09:33 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Pytorch 在多节点 GPU 上结合模型和数据并行性</title>
      <link>https://stackoverflow.com/questions/77910976/how-to-combine-model-and-data-parallelism-on-multi-nodes-gpus-using-pytorch</link>
      <description><![CDATA[我有一个8节点的计算集群，每个节点有4个GPU（总共32个GPU，每个GPU只有16Gb内存）。我在我的项目中使用 Pytorch。我可以在所有 32 个 GPU 上使用 DistributedDataParallel 进行数据并行化，但我正在训练的神经网络模型太大，无法放入一个 GPU 内存中，因此数据并行化在这里没有帮助。我尝试进行模型并行化，将模型切割为 4 个部分，但我只能在一个具有 4 个 GPU 的节点上运行它，并进行数据并行化。但我不知道如何将多节点上的数据和模型并行化结合起来。有谁知道如何做到这一点的任何示例（最小可行示例）或想法？
目前，我认为推动这一进程的最佳方法是分两步（或两个级别）进行：
(1) 在一个节点（4个GPU）上进行模型并行化
（2）使用DistributedDataParallel将步骤一中的模型复制到所有8个节点，进行数据并行化
您认为上述想法是个好方法吗？有如何执行此操作的示例吗？]]></description>
      <guid>https://stackoverflow.com/questions/77910976/how-to-combine-model-and-data-parallelism-on-multi-nodes-gpus-using-pytorch</guid>
      <pubDate>Wed, 31 Jan 2024 05:24:42 GMT</pubDate>
    </item>
    <item>
      <title>使用 RTSP 摄像头进行面部定向</title>
      <link>https://stackoverflow.com/questions/77910835/face-orientor-using-rtsp-camera</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77910835/face-orientor-using-rtsp-camera</guid>
      <pubDate>Wed, 31 Jan 2024 04:36:18 GMT</pubDate>
    </item>
    <item>
      <title>系统生物中 ANN 模型预测的令人困惑的 SHAP 分析</title>
      <link>https://stackoverflow.com/questions/77910332/confusing-shap-analysis-of-ann-model-prediction-in-systems-bio</link>
      <description><![CDATA[我开发了一个 ANN 模型来根据 Elisa 数据预测蛋白质翻译后修饰模式。为了简单起见，如何、可行性和参数对于我的问题并不重要，并且省略了一些细节。
对于给定的蛋白质，我将其称为蛋白质 X，它具有泛素作为修饰，但没有磷酸化模式。
我用各种翻译后修饰模式训练了人工神经网络，但有一个关键信息：我的训练数据不包含任何泛素模式（假设有一个原因）
因此，当我使用一组抗体进行 ELISA 时，抗体 a 特异性针对泛素模式，抗体 b 特异性针对磷酸化模式。当我使用抗体 a、抗体 b（和其他抗体）预测蛋白质 x 修饰模式时，我们获得了相当好的准确性。
为了解释模型的工作原理，我运行了 SHAP 分析和二分图来显示特征重要性（抗体）和修改，但得到了令人困惑的结果

对于抗体 a，除泛素外，其他修饰模式的 SHAP 值存在正值和负值，泛素是其特异性的

对于抗体 b，我们还发现了除磷酸化之外的修饰模式的正向和负向 SHAP 值，而蛋白质 x 并不真正具有磷酸化。


那么我该如何解释为什么 SHAP 产生这种模式：1）抗体 a 与其靶标泛素没有任何 SHAP 相关性，但对其其他靶标有 SHAP 相关性，2）抗体 b 与其靶标也没有 SHAP 相关性，但与其他抗体有 SHAP 相关性。其他人代替。
这又是令人困惑的，因为我预计抗体 a 与泛素有 SHAP 相关性，但与其他蛋白没有 SHAP 相关性，然后抗体 b 不应该有任何 SHAP 相关性，因为它的目标是磷酸化，但蛋白 x 没有磷酸化。
我不太相信或无法将 SHAP 的一些限制联系起来，因为它显示了模型的隐藏模式/关系，而不是我们在“现实生活”中所期望的
有人可以对这个观察到的 SHAP 分析提供更细致的见解吗？]]></description>
      <guid>https://stackoverflow.com/questions/77910332/confusing-shap-analysis-of-ann-model-prediction-in-systems-bio</guid>
      <pubDate>Wed, 31 Jan 2024 01:27:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的机器学习模型总是预测相同的错误答案，即使预测已经在我的数据集中？</title>
      <link>https://stackoverflow.com/questions/77910272/why-is-my-machine-learning-model-always-predicting-the-same-wrong-answer-even-wh</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77910272/why-is-my-machine-learning-model-always-predicting-the-same-wrong-answer-even-wh</guid>
      <pubDate>Wed, 31 Jan 2024 01:07:06 GMT</pubDate>
    </item>
    <item>
      <title>在Python中查找特征列的哪些过滤集导致最大目标列</title>
      <link>https://stackoverflow.com/questions/77910177/find-which-filter-sets-of-feature-columns-leads-to-maximum-target-column-in-pyth</link>
      <description><![CDATA[我无法找到可以解决我的问题的机器学习模型或分类类型。我本以为这可能相当简单，但也许不是。
假设我有 10 个特征列和一个二进制目标列。目标列的数据集中应该有大致相等数量的 0 和 1。整组数据并不是强相关的，所以线性回归、逻辑回归、朴素贝叶斯等都没有得出强相关的模型。然而，我所寻找的是哪个数据系列导致目标列的最大平均值。
例如。对于特征集 A 到 J 如果我按（C = True、D = false、J = true）过滤数据集，则目标 X 的平均值现在为 56%。我正在寻找一种算法，可以找到导致最大目标列均值的方程。
我觉得这可以通过蛮力来完成（循环遍历所有可能的组合），但我希望有一种方法可以在现有的众多数据科学库之一中做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/77910177/find-which-filter-sets-of-feature-columns-leads-to-maximum-target-column-in-pyth</guid>
      <pubDate>Wed, 31 Jan 2024 00:30:49 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：调用层“time_distributed_4”时遇到异常（类型 TimeDistributed）</title>
      <link>https://stackoverflow.com/questions/77909132/valueerror-exception-encountered-when-calling-layer-time-distributed-4-type</link>
      <description><![CDATA[我尝试使用 Kvasir 数据集制作 CNN-LSTM 模型。我使用 image_dataset_from_directory 分割数据集，如下所示：
dataset_path = “/kaggle/working/dataset”
图像大小 = 224, 224
批量大小 = 64

train_ds = image_dataset_from_directory(
  数据集_路径，
  验证分割=0.2，
  子集=“训练”，
  label_mode =“分类”，
  种子=23，
  图像大小=图像大小，
  color_mode =“rgb”，
  批量大小=批量大小）


val_ds = image_dataset_from_directory(
  数据集_路径，
  验证分割=0.2，
  子集=“验证”，
  label_mode =“分类”，
  种子=23，
  图像大小=图像大小，
  color_mode=“RGB”，
  批量大小=批量大小）

这个函数给了我一个 BatchDataset。然后我将基数设置如下：
val_batches = tf.data.experimental.cardinality(val_ds)
test_ds = val_ds.take(val_batches // 2)
val_ds = val_ds.skip(val_batches // 2)

然后
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)

这段代码还给了我一个预取数据集。当我运行 print(train_ds) 时它给出：
&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 8), dtype=tf .float32，名称=无））&gt;

然后我添加了我的模型，
 model = tf.keras.models.Sequential([
    # 具有批量归一化和最大池化的卷积层
    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, (3, 3), 激活=无,input_shape=(224, 224,3))),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3), 激活=无)),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),



    # 压平输出并添加密集层
    tf.keras.layers.TimeDistributed(Flatten()),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.LSTM(256,激活=&#39;tanh&#39;),
    
    
    
    tf.keras.layers.Dense(256),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),

    # 具有 8 个节点的输出层用于分类
    tf.keras.layers.Dense(8, 激活=&#39;softmax&#39;)
]）

# 编译模型

model.compile(优化器=AdamW(learning_rate=0.001,weight_decay=0.004,beta_1=0.9,beta_2=0.999,epsilon=1e-8),
          损失=分类交叉熵(),
          指标=[&#39;准确性&#39;])

当我适合这个模型时它没有运行并且出现错误：
ValueError：调用层“time_distributed_4”（类型 TimeDistributed）时遇到异常。
    
    层“conv2d_2”的输入0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、224、3）
    
    调用层“time_distributed_4”接收的参数（类型 TimeDistributed）：
      输入= tf.Tensor（形状=（无，224，224，3），dtype = float32）
      • 训练=真
      • 掩码=无

我不知道如何解决这个问题，你能帮助我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77909132/valueerror-exception-encountered-when-calling-layer-time-distributed-4-type</guid>
      <pubDate>Tue, 30 Jan 2024 20:05:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在多维复杂数据上训练模型？</title>
      <link>https://stackoverflow.com/questions/77825016/how-to-train-a-model-on-multidimensional-complex-data</link>
      <description><![CDATA[我有一个输入数据数组，它们是 5 个不同长度的数组。如何构建正确的张量和形式进行训练？
&lt;前&gt;&lt;代码&gt;[
[
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ],],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
[
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ],],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]
],
...
],
]
]]></description>
      <guid>https://stackoverflow.com/questions/77825016/how-to-train-a-model-on-multidimensional-complex-data</guid>
      <pubDate>Tue, 16 Jan 2024 10:22:33 GMT</pubDate>
    </item>
    <item>
      <title>我使用 XGB 分类器训练了数据集</title>
      <link>https://stackoverflow.com/questions/77776124/ive-trained-dataset-using-xgb-classifier</link>
      <description><![CDATA[我从我的队友那里得到了我们项目的这部分代码，我在本地遇到了这个错误，我已经使用 XGB 分类器训练了数据集。
我的代码是：
# XGBoost 分类器模型
从 xgboost 导入 XGBClassifier

# 实例化模型
xgb = XGBClassifier()

# 拟合模型
xgb.fit(X_train,y_train)

然后我得到了这个错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ValueError Traceback（最近一次调用最后一次）
[70] 中的单元格，第 8 行
      5 xgb = XGBClassifier()
      7#拟合模型
----&gt; 8 xgb.fit(X_train,y_train)

文件 ~/anaconda3/envs/project/lib/python3.10/site-packages/xgboost/core.py:730，在 require_keyword_args..throw_if..inner_f(*args, **kwargs ）
    728 k, arg in zip(sig.parameters, args)：
    第729章
--&gt;第730章

文件〜/anaconda3/envs/project/lib/python3.10/site-packages/xgboost/sklearn.py:1471，在XGBClassifier.fit（self，X，y，sample_weight，base_margin，eval_set，eval_metric，early_stopping_rounds，verbose， xgb_model、sample_weight_eval_set、base_margin_eval_set、feature_weights、回调）
   第1466章
   第1467章
   第1468章
   第1469章
   第1470章
-&gt;第1471章
   攀上漂亮女局长之后1472 ”
   第1473章
   第1474章
   第1476章
   第1478章

ValueError：从“y”的唯一值推断出无效的类。

预期：[0 1]，得到[-1 1]，，我听说 y_train 必须在较新的更新中进行编码，但我对这些事情有点陌生，我也不知道如何做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/77776124/ive-trained-dataset-using-xgb-classifier</guid>
      <pubDate>Mon, 08 Jan 2024 03:09:32 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：缓冲区数据类型不匹配，预期为“double_t”，但得到“float” - hdbscanvalidity_index</title>
      <link>https://stackoverflow.com/questions/76242882/valueerror-buffer-dtype-mismatch-expected-double-t-but-got-float-hdbscan</link>
      <description><![CDATA[我使用hdbscan包中的有效性索引，它根据以下论文实现DBCV评分：
https://www.dbs.ifi.lmu.de /~zimek/publications/SDM2014/DBCV.pdf
我正在做一个人脸聚类项目，使用有效性索引后提示错误。
这是代码：
dbcv_score_output = hdbscan.validity.validity_index(feature_vectors, archive_labels)
dbcv_分数_输出

完整错误：
hdbscan/validity.py:30: Ru​​ntimeWarning: 电源遇到溢出
  距离矩阵[距离矩阵！= 0] = (1.0 / 距离矩阵[

-------------------------------------------------- ------------------------
ValueError Traceback（最近一次调用最后一次）
文件〜/anaconda3/lib/python3.9/site-packages/hdbscan/validity.py:371，在validation_index（X，标签，指标，d，per_cluster_scores，mst_raw_dist，详细，** kwd_args）
    第356章 继续
    第358章
    [第 359 章]
    360X，
   （...）
    第367章
    第368章）
    [第 370 章]
--&gt;第371章
    [第 372 章]
    [第 374 章]

文件〜/anaconda3/lib/python3.9/site-packages/hdbscan/validity.py:165，在internal_minimum_spanning_tree(mr_distances)中
    136 def 内部最小跨度树（mr_distances）：
    第137章
    第138章
    139 个可达距离。给定最小生成树“内部”
   （...）
...
    167 为索引，枚举中的行(min_span_tree[1:], 1)：

文件 hdbscan/_hdbscan_linkage.pyx:15，在 hdbscan._hdbscan_linkage.mst_linkage_core()

ValueError：缓冲区数据类型不匹配，预期为“double_t”，但得到“float”

快速浏览输入及其类型：

特点：
dtype=float32
形状：（70201、320）


档案/集群（它是标签编码的）：
形状：(70201,)


当我尝试将功能类型更改为 double/float64 时，它显示了不同类型的错误：
hdbscan/validity.py:33: RuntimeWarning: true_divide 中遇到无效值
  结果 /= distance_matrix.shape[0] - 1
-------------------------------------------------- ------------------------
ValueError Traceback（最近一次调用最后一次）
文件〜/anaconda3/lib/python3.9/site-packages/hdbscan/validity.py:372，在validation_index（X，标签，指标，d，per_cluster_scores，mst_raw_dist，详细，** kwd_args）
    第358章
    [第 359 章]
    360X，
   （...）
    第367章
    第368章）
    [第 370 章]
    第371章
--&gt; [第 372 章]
    [第 374 章]
    第376章

文件〜/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:40，在_amax（a，轴，out，keepdims，初始，其中）
     38 def _amax(a, axis=None, out=None, keepdims=False,
     39 初始=_NoValue，其中=True）：
---&gt; 40 return umr_maximum(a, axis, None, out, keepdims, initial, where)

ValueError：零大小数组到没有标识的缩减操作最大值

我检查了存储库中的所有相关问题和修复，但没有效果。有什么建议或修复吗？]]></description>
      <guid>https://stackoverflow.com/questions/76242882/valueerror-buffer-dtype-mismatch-expected-double-t-but-got-float-hdbscan</guid>
      <pubDate>Sat, 13 May 2023 13:04:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 GridSearchCV 和 f2 分数评估多个模型</title>
      <link>https://stackoverflow.com/questions/67147606/evaluation-of-multiple-models-using-gridsearchcv-and-f2-score</link>
      <description><![CDATA[我正在尝试使用 GridSearchCV 对一些机器学习模型进行二元分类。我想根据分数、最佳参数和 f2 分数对模型进行分类。
对于分数和最佳参数，我使用了代码
分数 = []

对于 model_name，model_params.items() 中的 mp：
    clf = GridSearchCV(mp[&#39;model&#39;], mp[&#39;params&#39;], cv=5, return_train_score=False)
    clf.fit(X_测试, y_测试)
    分数.append({
        &#39;模型&#39;：模型名称，
        &#39;最佳得分&#39;: clf.best_score_,
        &#39;best_params&#39;：clf.best_params_
    })
    
df = pd.DataFrame(分数,列=[&#39;模型&#39;,&#39;best_score&#39;,&#39;best_params&#39;])

它给出了所有模型的best_score和best_params，但我无法找到所有模型的f2分数。代码应该是什么？]]></description>
      <guid>https://stackoverflow.com/questions/67147606/evaluation-of-multiple-models-using-gridsearchcv-and-f2-score</guid>
      <pubDate>Sun, 18 Apr 2021 10:38:43 GMT</pubDate>
    </item>
    <item>
      <title>声明嵌入层时出现 ResourceExhaustedError (Keras)</title>
      <link>https://stackoverflow.com/questions/52547568/resourceexhaustederror-when-declaring-embeddings-layer-keras</link>
      <description><![CDATA[我正在为 NLP 创建一个神经网络，从嵌入层开始（使用预先训练的嵌入）。但是当我在 Keras（Tensorflow 后端）中声明 Embedding 层时，出现 ResourceExhaustedError ：
ResourceExhaustedError：通过分配器 GPU_0_bfc 在 /job:localhost/replica:0/task:0/device:GPU:0 上分配形状为 [137043,300] 的张量并键入 float 时出现 OOM
 [[{{node embedding_4/random_uniform/RandomUniform}} = RandomUniform[T=DT_INT32，dtype=DT_FLOAT，seed=87654321，seed2=9524682，_device=&quot;/job:localhost/replica:0/task:0/device:GPU :0&quot;](embedding_4/random_uniform/shape)]]
 提示：如果您想在 OOM 发生时查看分配的张量列表，请将 report_tensor_allocations_upon_oom 添加到 RunOptions 以获取当前分配信息。

我已经查过Google：大多数ResourceExhaustedError发生在训练时，并且是因为GPU的RAM不够大。它可以通过减少批量大小来修复。
但就我而言，我什至没有开始训练！这行是问题所在：
q1 = 嵌入(nb_words + 1,
             参数[&#39;embed_dim&#39;].value,
             权重=[word_embedding_matrix],
             input_length=param[&#39;sentence_max_len&#39;].value)(问题1)

这里，word_embedding_matrix 是一个大小为 (137043, 300) 的矩阵，即预训练的嵌入。
据我所知，这不会占用大量内存（不像此处）：
137043 * 300 * 4 字节 = 53 kiB
这是使用的 GPU：

&lt;前&gt;&lt;代码&gt; +-------------------------------------------------------- ----------------------------------+
 | NVIDIA-SMI 396.26 驱动程序版本：396.26 |
 |------------------------------------------+----------------- ---+----------------------+
 | GPU 名称持久性-M|总线 ID Disp.A |挥发性未校正。 ECC |
 |风扇温度性能功率：使用/上限|内存使用情况 | GPU-Util 计算 M。
 |================================+================== ====+======================|
 | 0 GeForce GTX 108...关闭 | 00000000:02:00.0 关闭 |不适用 |
 | 23% 32C P8 16W / 250W | 6956MiB / 11178MiB | 0% 默认 |
 +--------------------------------------------+----------------- ---+----------------------+
 | 1 个 GeForce GTX 108...关闭 | 00000000:03:00.0 关闭 |不适用 |
 | 23% 30C P8 16W / 250W | 530MiB / 11178MiB | 0% 默认 |
 +--------------------------------------------+----------------- ---+----------------------+
 | 2 GeForce GTX 108...关闭| 00000000:82:00.0 关闭 |不适用 |
 | 23% 34C P8 16W / 250W | 333MiB / 11178MiB | 0% 默认 |
 +--------------------------------------------+----------------- ---+----------------------+
 | 3 GeForce GTX 108...关闭| 00000000:83:00.0 关闭 |不适用 |
 | 24% 46C P2 58W / 250W | 4090MiB / 11178MiB | 23% 默认 |
 +--------------------------------------------+----------------- ---+----------------------+

 +------------------------------------------------ ----------------------------+
 |进程：GPU 内存 |
 | GPU PID 类型 进程名称 用法 |
 |=================================================== ===========================|
 | 0 1087 C uwsgi 1331MiB |
 | 0 1088 C uwsgi 1331MiB | 0 1088 C uwsgi 1331MiB
 | 0 1089 C uwsgi 1331MiB | 0 1089 C uwsgi 1331MiB
 | 0 1090 C uwsgi 1331MiB | 0 1090 C uwsgi 1331MiB
 | 0 1091 C uwsgi 1331MiB | 0 1091 C uwsgi 1331MiB
 | 0 4176 C /usr/bin/python3 289MiB | 0 4176 C /usr/bin/python3 289MiB
 | 1 2631 C ...e92/venvs/wordintent_venv/bin/python3.6 207MiB |
 | 1 4176 C /usr/bin/python3 313MiB | 1 4176 C /usr/bin/python3 313MiB
 | 2 4176 C /usr/bin/python3 323MiB | 2 4176 C /usr/bin/python3 323MiB
 | 3 4176 C /usr/bin/python3 347MiB | 3 4176 C /usr/bin/python3 347MiB
 | 3 10113 C 蟒蛇 1695MiB |
 | 3 13614 C python3 1347MiB |
 | 3 14116 C 蟒蛇 689MiB |
 +------------------------------------------------ ----------------------------+

有谁知道我为什么会遇到这个异常？]]></description>
      <guid>https://stackoverflow.com/questions/52547568/resourceexhaustederror-when-declaring-embeddings-layer-keras</guid>
      <pubDate>Fri, 28 Sep 2018 02:41:56 GMT</pubDate>
    </item>
    </channel>
</rss>