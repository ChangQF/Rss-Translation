<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 04 Dec 2024 01:22:30 GMT</lastBuildDate>
    <item>
      <title>为特定任务和具体交付成果实施 AI/ML 任务 [关闭]</title>
      <link>https://stackoverflow.com/questions/79249348/implementing-ai-ml-tasks-for-specific-taskst-and-concrete-deliverables</link>
      <description><![CDATA[我所在的公司正在大力推动人工智能，我已经看到许多空缺职位，包括人工智能/机器学习架构师、具有人工智能/机器学习经验的人等。我想了解更多，但我仍然不确定如何以个人贡献者的身份使用这项新技术。
例如，我目前工作中的一项任务可能是开发 powerbi 仪表板或 SSRS 报告（及其相关的 sql）。另一项任务可能是修复执行时中断的存储过程。更大的用户故事可能是创建一个读取一些 JSON 并将其转储到 sql 表的 Web 服务。这些都是具有具体可交付成果的特定任务（即交付仪表板/报告、成功运行 SP、检查表是否具有解析的 JSON）。
按照这种思路，哪些具体任务会涉及人工智能/机器学习？例如，我如何使用 Amazon Bedrock、SageMaker 或 Azure OpenAI 来完成任务？那么可交付成果是什么？
我粘贴了关于 AI 和 ML 的一般概念，它很有意义，但听起来不像是个人贡献者会做的事情。

AI 是一个通用术语，用于试图模仿人类行为及其智能的领域。任何能够做到这一点的方法或方式都属于 AI。


机器学习是 AI 的一个子集，它通过从数据中学习模式来实现 AI，然后根据这些模式进行预测。
]]></description>
      <guid>https://stackoverflow.com/questions/79249348/implementing-ai-ml-tasks-for-specific-taskst-and-concrete-deliverables</guid>
      <pubDate>Tue, 03 Dec 2024 22:23:50 GMT</pubDate>
    </item>
    <item>
      <title>当我的训练和测试数据大小不同时，如何像使用 sklearn 模型一样使用拟合和预测函数创建神经网络类？</title>
      <link>https://stackoverflow.com/questions/79249247/how-do-i-make-a-neural-network-class-with-fit-and-predict-functions-like-with-sk</link>
      <description><![CDATA[我正在尝试创建一个可以回答线性回归问题的神经网络模型（我已经使用 sklearn 的 LinearRegression 建立了一个模型，我想比较一下这两个模型）。最终，我想创建一个包含 fit 和 predict 函数的类，就像 sklearn 中的模型一样，这样我就可以创建一个循环来测试我在项目中使用的所有模型。
为此，我遵循了此问题答案中的代码：编写一个具有模型拟合和预测功能的 pytorch 神经网络类。
经过一些修改，我得到了以下结果：
import torch
import torch.nn as nn
import torch.optim as optim

class MyNeuralNet(nn.Module):
def __init__(self):
super().__init__()
self.layer1 = nn.Linear(2, 4, bias=True)
self.layer2 = nn.Linear(4, 1, bias=True)
self.loss = nn.MSELoss()
self.compile_()

def forward(self, x):
x = self.layer1(x)
x = self.layer2(x)
return x.squeeze()

def fit(self, x, y):
x = torch.tensor(x.values, dtype=torch.float32)
y = torch.tensor(y.values, dtype=torch.float32)
loss = []
for epoch in range(100):
## 推理
res = self.forward(x)#self(self,x)
loss_value = self.loss(res,y)

## 反向传播
loss_value.backward() # 计算梯度
self.opt.zero_grad() # 刷新前一个 epoch 的梯度
self.opt.step() # 使用上面的梯度执行迭代

## 日志记录
loss.append(loss_value.item())

def compile_(self):
self.opt = optim.SGD(self.parameters(), lr=0.01)

def predict(self, x_test):
self.eval()
y_test_hat = self(x_test)
return y_test_hat.detach().numpy()
# self.train()

注意，您还需要 numpy，我只是没有在这里，因为此代码已放入单独的 .py 文件中。
导入我的类后，这是我使用模型的方式：
model = MyNeuralNet()
X_train = # pandas 数据框，包含 1168 行和 49 列
y_train = # pandas 数据框，包含 1168 行和 1 列
X_test = # pandas 数据框，包含 292 行和 49 列
model.fit(X_train, y_train)
pred = model.predict(X_test)
print(pred)

我得到的错误是 RuntimeError：mat1 和 mat2 形状无法相乘（1168x49 和 2x4），在 fit 步骤。我理解这与我的网络线性层的参数有关。我认为，如果我将第一个线性层的输入大小更改为 49，将第二个线性层的输出大小更改为 1168，那么它将适用于 fit 步骤（或至少类似的步骤，以匹配训练数据的大小）。但是，我的测试数据的大小不同，我很确定 predict 步骤将不起作用。
是否可以创建一个训练和测试数据大小不同的神经网络类？]]></description>
      <guid>https://stackoverflow.com/questions/79249247/how-do-i-make-a-neural-network-class-with-fit-and-predict-functions-like-with-sk</guid>
      <pubDate>Tue, 03 Dec 2024 21:41:10 GMT</pubDate>
    </item>
    <item>
      <title>想要构建一个验证码求解器但不知道如何做？[关闭]</title>
      <link>https://stackoverflow.com/questions/79248824/want-to-built-an-captcha-solver-but-dont-know-how</link>
      <description><![CDATA[我有一个来自我大学网站的 1000 张带标签的 CAPTCHA 图像数据集，我想训练一个模型，该模型可以在未见过的数据上准确解决类似的 CAPTCHA。CAPTCHA 通常由 6 个字母数字字符（A-Z、0-9）组成。尽管尝试了几种方法，但该模型仍无法实现高精度。
数据集：

样本数量：1000 张带标签的图像。
CAPTCHA 类型：6 个字母数字字符。
图像示例：[附加示例 CAPTCHA 图像]。
我尝试过的方法：
卷积神经网络 (CNN)：

我创建了一个具有多个卷积层和密集层的 CNN 模型。
将输出展平，以为每个字符输入单独的密集层。
在未见过的 CAPTCHA 上实现了较差的泛化。
迁移学习：
使用 MobileNetV2 作为带有自定义头的特征提取器。
调整输入大小（灰度到 RGB 转换）。
由于数据有限，该模型容易过度拟合
我想训练一个模型：

可以高精度处理未见过的 CAPTCHA。
有效地解码字母数字字符序列。

解决此类 CAPTCHA 的最佳方法或架构是什么？我应该使用：

CNN 用于特征提取，然后 LSTM 用于序列解码？
完全卷积架构（例如 CRNN）？
对于有限的数据集，还有其他更好的方法吗？

如果可能，您能否建议一个完整的模型架构、预处理步骤或可能有帮助的损失函数设置？对于这项任务，有没有什么技巧可以增强小型数据集？]]></description>
      <guid>https://stackoverflow.com/questions/79248824/want-to-built-an-captcha-solver-but-dont-know-how</guid>
      <pubDate>Tue, 03 Dec 2024 18:37:33 GMT</pubDate>
    </item>
    <item>
      <title>溢出 CUDA 内存错误但有可用空间[关闭]</title>
      <link>https://stackoverflow.com/questions/79248530/overflowing-cuda-memory-error-but-have-free-space</link>
      <description><![CDATA[我遇到这个问题已经有一段时间了，当我尝试将嵌入暗度从 64 加倍到 128 或将批处理大小从 1 增加时，我总是收到此错误。这是一个具有 120 万个参数的语言模型。它使用相同数据的 30 万个参数，但如果我使用包含 20 万个样本的较大数据集，它就会崩溃。当前数据集有 4k 个样本。样本是堆叠在一起的 50 种蛋白质序列。如果有人能帮忙，我将不胜感激。
错误代码是：
torch.cuda.OutOfMemoryError：CUDA 内存不足。尝试分配 150.00 MiB（GPU 0；总容量 31.74 GiB；已分配 21.32 GiB；空闲 9.86 GiB；允许 21.58 GiB；PyTorch 总共保留 21.37 GiB）
以下是参数：
nb_blocks=6, embed_dim=128, nb_heads=4, nb_epochs=20, warmup_steps=300, learning_rate=0.0001, check_val_every=1000, batch_size=1
我使用的是 NVIDIA V100 PCIe 32 GB GPU 的集群设置。
Lightning 设置为：
slurm_args = {
&quot;accelerator&quot;: &quot;gpu&quot;,
&quot;devices&quot;: int(os.environ[&quot;SLURM_GPUS_ON_NODE&quot;]),
&quot;num_nodes&quot;: int(os.environ[&quot;SLURM_NNODES&quot;]),
&quot;strategy&quot;: &quot;ddp&quot;,
&quot;precision&quot;: 16,
}

...

accelerator = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
trainer_args = {
&quot;max_epochs&quot;: args.nb_epochs,
&quot;log_every_n_steps&quot;: LOGGING_STEPS,
&quot;val_check_interval&quot;: VAL_CHECK_STEPS,
&quot;logger&quot;: wandb_logger,
&quot;callbacks&quot;: callbacks,
&quot;accelerator&quot;: accelerater,
**slurm_args,
}

我尝试过的方法：

将精度设置为 16（从 32 降低）：将内存溢出从 300MiB 减半到 150MiB（至少取得了一些进展，但没有解决问题问题）
torch.cuda.set_per_process_memory_fraction(0.7)：释放了空间，但该空间未分配用于内存溢出，我无法弄清楚如何分配该备用内存以用于上述问题
手动覆盖分配并将权重、数据和模型移动到 CPU。没有做任何事情，因为几乎所有的初始化都已初始化到 CPU。
将 slurm_args 中的 accelerator 设置为 cpu。解决了内存问题，但每次迭代从 1.2 秒增加到 25 秒。换句话说，它太慢了，根本没用
将策略从 ddp 更改为 FSDPStrategy(cpu_offload=True)，但没有任何变化
尝试设置 PYTORCH_CUDA_ALLOC_CONF，但没有任何效果
在参数中使用不同的值设置 accumulate_grad_batches，没有变化
]]></description>
      <guid>https://stackoverflow.com/questions/79248530/overflowing-cuda-memory-error-but-have-free-space</guid>
      <pubDate>Tue, 03 Dec 2024 16:51:36 GMT</pubDate>
    </item>
    <item>
      <title>保留验证集-超参数调整</title>
      <link>https://stackoverflow.com/questions/79247785/holdout-validation-set-hyperparameter-tuning</link>
      <description><![CDATA[我有一个大型数据集，我将其拆分为：

训练集 (80%)
验证集 (10%)
测试集 (10%)

在每个集合上，我执行了缺失值插补和特征选择（在训练集上训练，并复制到验证和测试集中）以避免数据泄露。
现在，我想用 Python 训练 XGBoost 模型，并希望使用训练集执行超参数调整，并使用验证集评估每个参数集。我如何使用 RandomizedSearchCV 等随机方法执行此操作，以便不运行所有参数集？
如果我是正确的，GridSearch 和 RandomizedSearchCV 仅允许交叉验证，这不是我想要的，因为将预处理的训练集拆分成几层会导致数据泄露。
我知道我可以构建一个 sklearn 管道，在其中对每个折叠进行预处理，但我想避免后一种选择。
我只能考虑像在 GridSearch 中一样运行每个参数集的代码：
from sklearn.model_selection import ParameterGrid
import xgboost as xgb

# 定义你的超参数网格
param_grid = {
&#39;max_depth&#39;: [3, 5, 7],
&#39;learning_rate&#39;: [0.01, 0.1, 0.2],
&#39;n_estimators&#39;: [100, 200, 300]
}

best_score = -1
best_params = {}

for params in ParameterGrid(param_grid):
model = xgb.XGBClassifier(**params)
model.fit(X_train, y_train)
val_score = model.score(X_val, y_val) # 或者使用更具体的指标

if val_score &gt; best_score:
best_score = val_score
best_params = params

# 使用最佳超参数训练最终模型
best_model = xgb.XGBClassifier(**best_params)
best_model.fit(X_train, y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/79247785/holdout-validation-set-hyperparameter-tuning</guid>
      <pubDate>Tue, 03 Dec 2024 13:26:56 GMT</pubDate>
    </item>
    <item>
      <title>获取文本分类的 Captum 文本解释时出错</title>
      <link>https://stackoverflow.com/questions/79247672/error-in-getting-captum-text-explanations-for-text-classification</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79247672/error-in-getting-captum-text-explanations-for-text-classification</guid>
      <pubDate>Tue, 03 Dec 2024 12:47:45 GMT</pubDate>
    </item>
    <item>
      <title>Python 版本 3.8.2，迁移至 Python 版本 3.11.9。（PKL 文件兼容性问题）</title>
      <link>https://stackoverflow.com/questions/79247110/python-version-3-8-2-migrate-it-to-python-version-3-11-9-pkl-file-compatiblity</link>
      <description><![CDATA[我有一个使用 Python 版本 3.8.2 训练的 PKL 文件，但现在我需要将其迁移到 Python 版本 3.11.9。但是，当我在升级后的环境中执行它时，它会抛出一个错误，而在旧环境中它可以正常工作。
错误是：
TypeError：code() 参数 13 必须是 str，而不是 int

这是模型的路径：
在 python 3.8.2 中训练的所有模型中都出现类似的问题。
我已经尝试过 pickle 和 cloudpickle，
使用 subprocess 从 Python 3.11 环境执行 Python 3.8 脚本，但这不是永久的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/79247110/python-version-3-8-2-migrate-it-to-python-version-3-11-9-pkl-file-compatiblity</guid>
      <pubDate>Tue, 03 Dec 2024 10:03:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的逻辑回归的准确率只有 25%？</title>
      <link>https://stackoverflow.com/questions/79247069/why-my-logistic-regression-has-25-accuracy</link>
      <description><![CDATA[我正在实现逻辑回归。我知道已经有很多库可以实现它。但问题是我无法理解那些。所以我为它创建了自己的数据集。
它有 3 个东西，房价、标准 和 购买决策
标准 代表生活水平。
0 : 低
1 : 中
2 : 高
当房价非常低时，只有生活水平低（0）的人才会买房。
当房价非常高时，只有生活水平高（2）的人才会买房。
这是我的实现的数据集和 ipynb 文件
测试数据集有 array(1,1,0,0) 作为购买决策，但我的模型给出 array(0,1,1,1)
我知道我的数据集很小，但这一定不是准确率如此低的唯一原因。
我做错了什么？如何执行？]]></description>
      <guid>https://stackoverflow.com/questions/79247069/why-my-logistic-regression-has-25-accuracy</guid>
      <pubDate>Tue, 03 Dec 2024 09:54:08 GMT</pubDate>
    </item>
    <item>
      <title>anomalib 的零样本 winCLIP 不起作用</title>
      <link>https://stackoverflow.com/questions/79244492/zero-shot-winclip-from-anomalib-not-working</link>
      <description><![CDATA[随着异常分类/分割的最新进展，我想尝试新的 winCLIP 模型，anomalib 库也有一个实现。
如何测试零样本或为少样本提供几张“正常/健康”图像？由于这是一个零样本模型，我认为它很容易开箱即用，但我无法让它工作。这是我当前的代码：
from anomalib.models.image import WinClip
from anomalib.engine import Engine

# 导入数据模块
from anomalib.data import Folder
from anomalib.data.utils import TestSplitMode

# 创建数据模块
datamodule = Folder(
name=&quot;lasercut_plank&quot;,
root=&quot;./DATA_0shot&quot;,
normal_dir=&quot;normal&quot;,
test_split_mode=TestSplitMode.NONE
)

# 设置数据模块
datamodule.setup()

# 访问数据集
train_dataset = datamodule.train_data

# 访问数据加载器
train_dataloader = datamodule.train_dataloader()

# 创建模型和引擎
model = WinClip(class_name=&quot;lasercut_plank&quot;)
engine = Engine(task=&quot;segmentation&quot;)

# 在给定的数据模块上训练 Patchcore 模型
engine.train(datamodule=datamodule, model=model)
]]></description>
      <guid>https://stackoverflow.com/questions/79244492/zero-shot-winclip-from-anomalib-not-working</guid>
      <pubDate>Mon, 02 Dec 2024 14:43:56 GMT</pubDate>
    </item>
    <item>
      <title>为什么预先训练的 Swin Transformer 编码器在 TPU 上失败但在 Colab 中的 CPU 上可以运行？</title>
      <link>https://stackoverflow.com/questions/79244294/why-does-pre-trained-swin-transformer-encoder-fail-on-tpu-but-works-on-cpu-in-co</link>
      <description><![CDATA[我正在处理图像分割任务，并尝试使用预先训练的 Swin Transformer Large (Swin-L) 编码器作为特征提取主干。代码在 Colab 中的 CPU 上完美运行。但是，当切换到 TPU 时，它会抛出如下所示的错误。
代码：
from tensorflow.keras import layer, Model, Input
from tfswin import SwinTransformerLarge224

def load_swin_encoder(input_shape=(512, 512, 3)):
# 加载预训练的 Swin-L 模型
swin_encoder = SwinTransformerLarge224(include_top=False, weights=&#39;imagenet&#39;,
input_shape=input_shape)

# 冻结预训练层
for layer in swin_encoder.layers:
layer.trainable = False

# 从四个阶段提取输出
stage_outputs = [
swin_encoder.get_layer(&#39;normalize&#39;).output, # 从 0 阶段输出
swin_encoder.get_layer(&#39;layers.0&#39;).output, # 第一阶段的输出
swin_encoder.get_layer(&#39;layers.1&#39;).output, # 第二阶段的输出
swin_encoder.get_layer(&#39;layers.2&#39;).output, # 第三阶段的输出
swin_encoder.get_layer(&#39;layers.3&#39;).output, # 第四阶段的输出
]
return Model(swin_encoder.input, stage_outputs, name=&quot;SwinTransformerEncoder&quot;)

# 测试代码
encoder = load_swin_encoder(input_shape=(512, 512, 3))
dummy_input = tf.random.uniform((1, 512, 512, 3))
encoder_outputs =coder(dummy_input)

for i, output in enumerate(encoder_outputs):
print(f&quot;阶段 {i + 1} 输出形状：{output.shape}&quot;)


错误：
代码在 TPU 上抛出以下错误：
------------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-28-3cb122d32678&gt; 在 &lt;cell line: 2&gt;()
1 # 加载健全性检查
----&gt; 2 编码器 = load_swin_encoder(input_shape=(512, 512, 3))
3 dummy_input = tf.random.uniform((1, 512, 512, 3))
4 编码器输出 = 编码器(dummy_input)
5 

2 帧
/usr/local/lib/python3.10/dist-packages/keras/src/models/ functional.py in __init__(self, 输入, 输出, 名称, **kwargs)
117 for x in flat_inputs:
118 if not isinstance(x, backend.KerasTensor):
-&gt; 119 引发 ValueError(
120 “所有 `inputs` 值都必须是 KerasTensors。已收到：”
121 f“inputs={inputs} 包括无效值 {x}”

ValueError：所有 `inputs` 值都必须是 KerasTensors。已收到：inputs=KerasTensor(type_spec=TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=&#39;input_4&#39;), name=&#39;input_4&#39;, description=“由层 &#39;input_4&#39; 创建”) 包括无效值 KerasTensor(type_spec=TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=&#39;input_4&#39;), name=&#39;input_4&#39;, description=“由层创建” &#39;input_4&#39;&quot;) 类型为 &lt;class &#39;tf_keras.src.engine.keras_tensor.KerasTensor&#39;&gt;


问题：
为什么此代码在 Colab 中的 CPU 上有效，但在 TPU 上失败？我该如何修复此问题以使其与 TPU 执行兼容？
任何见解或指导都将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79244294/why-does-pre-trained-swin-transformer-encoder-fail-on-tpu-but-works-on-cpu-in-co</guid>
      <pubDate>Mon, 02 Dec 2024 13:35:57 GMT</pubDate>
    </item>
    <item>
      <title>set_transform 或 with_transform 之后 transformer 的数据集结构出现意外</title>
      <link>https://stackoverflow.com/questions/79241735/unexpected-transformers-dataset-structure-after-set-transform-or-with-transform</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79241735/unexpected-transformers-dataset-structure-after-set-transform-or-with-transform</guid>
      <pubDate>Sun, 01 Dec 2024 14:07:14 GMT</pubDate>
    </item>
    <item>
      <title>这些 `[0]` 在创建变量时是否有意义</title>
      <link>https://stackoverflow.com/questions/79236682/do-those-0-make-sense-in-making-the-variable</link>
      <description><![CDATA[使用 HuggingFace 工具集微调 Gemma 的指南位于：https://huggingface.co/blog/gemma-peft
链接到以下行：https://huggingface.co/blog/gemma-peft#:~:text=Quote%3A%20%7Bexample-,%5B%27quote%27%5D%5B0%5D,-%7D%5CnAuthor%3A
数据输入格式化函数是：
def formatting_func(example):
text = f&quot;Quote: {example[&#39;quote&#39;][0]}\nAuthor: {example[&#39;author&#39;][0]}&lt;eos&gt;&quot;
return [text]

这些 [0] 有意义吗？它们看起来不对，因为当打印出 text 变量时，我可以看到它们只是字符而不是字符串。]]></description>
      <guid>https://stackoverflow.com/questions/79236682/do-those-0-make-sense-in-making-the-variable</guid>
      <pubDate>Fri, 29 Nov 2024 10:14:43 GMT</pubDate>
    </item>
    <item>
      <title>尽管有多个 GPU，CUDA 仍出现内存不足错误</title>
      <link>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</link>
      <description><![CDATA[尝试运行 PyTorch 模型时，我遇到了 CUDA 内存不足错误，尽管我的系统有多个 NVIDIA GPU。
# 加载 tokenizer 和模型
tokenizer = AutoTokenizer.from_pretrained(&quot;MODEL_TYPE&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;MODEL_TYPE&quot;, output_attentions=True, device_map = &#39;auto&#39;, torch_dtype=torch.float16, low_cpu_mem_usage=True)

我有 8 个 GPU，模型分布在所有 GPU 上。但是，由于我的输入是长上下文（大约 20k 个 token）。尽管其他 GPU 中有很多空间，但我还是收到 GPU0 的 CUDA 内存错误。请注意，这是对批处理大小 1 的推断。
OutOfMemoryError：CUDA 内存不足。尝试分配 20.11 GiB。GPU 0 的总容量为 22.17 GiB，其中 16.06 GiB 是空闲的。包括非 PyTorch 内存在内，此进程使用了​​ 6.10 GiB 内存。在分配的内存中，5.57 GiB 由 PyTorch 分配，308.62 MiB 由 PyTorch 保留但未分配。如果保留但未分配的内存很大，请尝试设置 max_split_size_mb 以避免碎片化。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档

inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)
torch.cuda.empty_cache()
model.generation_config.temperature = temp
model.eval()
with torch.no_grad():
output = model.generate(inputs.input_ids, max_length=25000, output_attentions=False,output_scores=False, return_dict_in_generate=True)
print(&quot;temp:&quot;,model.generation_config.temperature)
tokens = tokenizer.convert_ids_to_tokens(inputs[&#39;input_ids&#39;][0])

response = tokenizer.batch_decode(output[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]

如何有效利用可用的 GPU 进行长上下文输入以避免内存不足错误？
我尝试将输入强制到其他 GPU，但没有成功：
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda:1&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</guid>
      <pubDate>Sat, 27 Jul 2024 01:14:45 GMT</pubDate>
    </item>
    <item>
      <title>为什么基于 Tensorflow.js 的天气预测模型无法预测正确的天气</title>
      <link>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</guid>
      <pubDate>Sun, 26 May 2024 18:36:09 GMT</pubDate>
    </item>
    <item>
      <title>在 SageMaker 上的 TensorFlow 推荐器中初始化 FactorizedTopK 时出错：“无法将‘计数器’转换为形状”</title>
      <link>https://stackoverflow.com/questions/78144515/error-initializing-factorizedtopk-in-tensorflow-recommenders-on-sagemaker-cann</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78144515/error-initializing-factorizedtopk-in-tensorflow-recommenders-on-sagemaker-cann</guid>
      <pubDate>Tue, 12 Mar 2024 03:28:18 GMT</pubDate>
    </item>
    </channel>
</rss>