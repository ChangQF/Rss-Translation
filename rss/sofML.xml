<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 26 Jan 2024 03:15:44 GMT</lastBuildDate>
    <item>
      <title>如何跟踪无人机图像中的单个点（例如角落）？</title>
      <link>https://stackoverflow.com/questions/77883796/how-to-track-a-single-point-e-g-corner-in-uav-images</link>
      <description><![CDATA[我正在尝试跟踪无人机图像中的单个点，这通常是一个角落。它还会有一个粗略的初始位置（与真实值的偏差约为 30 像素）。下图所示为初始位置得到的ROI，黄色标记为跟踪点。这是一个简单的场景，但仍然有很多错误。

一些要求：

您必须比手动更快地执行此任务，当然越快越好；

没有必要返回所有结果，可能只返回 100 张图像的 25-30 个结果，但尽可能准确很重要；

适应无人机图像的情况：噪声、大旋转和平移等仿射变换。


我目前的想法是：

选择一个参考图像，手动选择参考图像上的一个角点作为跟踪点，并使用某种描述符作为其特征向量；

遍历所有图像：



获取以初始位置为中心的ROI；

对 ROI 执行角点检测并计算描述符；

将描述符与参考点的描述符进行匹配，最相似的描述符就是跟踪点；


在我的实验中，我简单地尝试了OpenCV中的ORB检测器和BFMatcher来执行上述过程，但没有得到好的结果。我怀疑是匹配不准确造成的，所以我尝试利用SuperGlue[1]中提到的最优传输来解决匹配问题，但发现由于初始值在“垃圾箱”处而导致结果平平。以及 lambda 参数的值。
其余的尝试，包括切换到其他角点检测器（CSS，相位一致性）以及在匹配之前对参考图像执行角点检测，也并不令人满意。
考虑这个问题的正确方向是什么？
注释：

这个问题实际上是无人机图像地面控制点的自动检测；标准化控制点因其特定的形状而易于识别，但非标准化控制点难以检测和跟踪。有一篇文献[2]与此相关，但文献中的方法是：返回重投影点的最近角点。但显然，当重投影误差较大或者场景比较复杂时，这种方法就失效了；

如果初始点的位置偏差较大（例如200像素），如何进行精确跟踪？

地面控制点通常是路标的角点。


[1]P.-E。 Sarlin、D. DeTone、T. Malisiewicz 和 A. Rabinovich，“SuperGlue：使用图神经网络学习特征匹配”，2020 年 IEEE/CVF 计算机视觉和模式识别会议 (CVPR)，2020 年 6 月，第 4937 页–4946。
[2]Z。朱天宝，胡勇，龚建，“一种无人机图像非标准化地面控制点快速定位新方法”，遥感，2016年13、没有。 15，艺术。不。 2021 年 1 月 15 日，doi：10.3390/rs13152849。]]></description>
      <guid>https://stackoverflow.com/questions/77883796/how-to-track-a-single-point-e-g-corner-in-uav-images</guid>
      <pubDate>Fri, 26 Jan 2024 01:40:21 GMT</pubDate>
    </item>
    <item>
      <title>“java.lang.IllegalArgumentException：字节缓冲区的大小和形状不匹配。” - 但他们确实</title>
      <link>https://stackoverflow.com/questions/77883608/java-lang-illegalargumentexception-the-size-of-byte-buffer-and-the-shape-do-no</link>
      <description><![CDATA[我正在尝试使用 Android Studio 中的以下代码将 ML 模型集成到 Android 应用中：
predictBtn.setOnClickListener(new View.OnClickListener() {
            @覆盖
            公共无效onClick（查看视图）{
                尝试 {
                    ModelUnquant 模型 = ModelUnquant.newInstance(MainActivity.this);


                    // 检查位图是否不为空
                    if (位图！= null) {
                        // 创建输入以供参考。
                        TensorBuffer inputFeature0 = TensorBuffer.createFixedSize(new int[]{1, 244, 244, 3}, DataType.FLOAT32);

                        位图 = Bitmap.createScaledBitmap(位图, 244, 244, true);
                        inputFeature0.loadBuffer(TensorImage.fromBitmap(bitmap).getBuffer());

                        // 运行模型推理并获取结果。
                        ModelUnquant.Outputs 输出 = model.process(inputFeature0);
                        TensorBufferoutputFeature0=outputs.getOutputFeature0AsTensorBuffer();

                        // 在这里更改类号
                        result.setText(labels[getMax(outputFeature0.getFloatArray())]+“”);
                    } 别的 {
                        // 处理bitmap为null的情况
                        result.setText(“错误：无图像”);
                    }

                    // 如果不再使用则释放模型资源。
                    模型.close();
                } catch (IOException e) {
                    // TODO 处理异常
                }
            }
        });

返回以下错误：
致命异常：main
进程：com.example.archeyewitness，PID：15698

java.lang.IllegalArgumentException：字节缓冲区的大小和形状不匹配。

在 org.tensorflow.lite.support.common.SupportPreconditions.checkArgument（SupportPreconditions.java:104）

在 org.tensorflow.lite.support.tensorbuffer.TensorBuffer.loadBuffer(TensorBuffer.java:309)

在 org.tensorflow.lite.support.tensorbuffer.TensorBuffer.loadBuffer(TensorBuffer.java:328)

在 com.example.archeyewitness.MainActivity$3.onClick(MainActivity.java:96)

现在，我尝试通过打印字节缓冲区的大小和形状来调试它：
Log.d(&quot;DebugInfo&quot;, &quot;张量缓冲区大小：&quot; + tensorBufferSize);
Log.d(&quot;DebugInfo&quot;, &quot;字节缓冲区大小:&quot; + byteBufferSize);
Log.d(“DebugInfo”, “张量缓冲区形状：” + Arrays.toString(tensorBufferShape));
Log.d(“DebugInfo”, “预期形状：” + Arrays.toString(byteBufferShape));


返回以下内容：
张量缓冲区大小：150528
2024-01-25 18:29:33.965 14367-14367 DebugInfo com.example.archeyewitness D 字节缓冲区大小：150528
2024-01-25 18:29:33.965 14367-14367 DebugInfo com.example.archeyewitness D 张量缓冲区形状：[1, 224, 224, 3]
2024-01-25 18:29:33.965 14367-14367 DebugInfo com.example.archeyewitness D 预期形状：[1, 224, 224, 3]
2024-01-25 18:29:33.971 14367-14367 AndroidRuntime com.example.archeyewitness D 关闭虚拟机
2024-01-25 18:29:33.992 14367-14367 AndroidRuntime com.example.archeyewitness

如您所见，尺寸确实匹配，但我仍然遇到相同的错误。有人可以解释一下发生了什么事吗？我正在失去理智，而且显然不是最聪明的。]]></description>
      <guid>https://stackoverflow.com/questions/77883608/java-lang-illegalargumentexception-the-size-of-byte-buffer-and-the-shape-do-no</guid>
      <pubDate>Fri, 26 Jan 2024 00:16:55 GMT</pubDate>
    </item>
    <item>
      <title>如何采用在 Python 中训练并使用 Pickle 保存的机器学习模型，并在 C++ 中将其作为预测模型运行？</title>
      <link>https://stackoverflow.com/questions/77883553/how-do-i-take-a-machine-learning-model-that-i-trained-in-python-and-saved-with</link>
      <description><![CDATA[我有一个用Python训练的机器学习模型，我需要最好将该模型转换成可以在C++中运行的dll。我希望也许有一个可以读取 pickle 的 C++ 包，或者其他一些可以共享机器学习模型进行预测的 Python/C++ 库。
我发现了这个名为 Pickling tools 的库，它应该用于 Python 和 C++ 之间的交叉通信，其口号是“How can I pickle in C++?”但我很难找到将我的pickle 放入C++ 中的部分。
http://www.picklingtools.com/html/usersguide.html ]]></description>
      <guid>https://stackoverflow.com/questions/77883553/how-do-i-take-a-machine-learning-model-that-i-trained-in-python-and-saved-with</guid>
      <pubDate>Thu, 25 Jan 2024 23:54:37 GMT</pubDate>
    </item>
    <item>
      <title>管道并行神经网络[关闭]</title>
      <link>https://stackoverflow.com/questions/77883493/pipeline-parallel-neural-network</link>
      <description><![CDATA[图中的Cn、Cn-x、Cn+1、Cn-x+1是什么意思？我正在阅读有关 Pipedream 管道并行的内容，但无法理解该图像。
论文链接（图4）：
https://arxiv.org/pdf/1806.03377.pdf
pipedream 图像
论文中提到C是计算，但没有提到n,x, n+1,n-x是什么。]]></description>
      <guid>https://stackoverflow.com/questions/77883493/pipeline-parallel-neural-network</guid>
      <pubDate>Thu, 25 Jan 2024 23:32:35 GMT</pubDate>
    </item>
    <item>
      <title>如何保留 Oracle Machine Learning (OML) k-means 模型？</title>
      <link>https://stackoverflow.com/questions/77883357/how-can-i-persist-an-oracle-machine-learning-oml-k-means-model</link>
      <description><![CDATA[此示例不展示如何将 k-means 聚类模型保存到数据库中。有没有办法做到这一点并且有示例代码吗？
适用于 Python 的 Oracle 机器学习
这是我创建模型的方法，
# 创建KM模型对象并拟合。
km_mod = oml.km(n_clusters = 3, **设置).fit(数据)
]]></description>
      <guid>https://stackoverflow.com/questions/77883357/how-can-i-persist-an-oracle-machine-learning-oml-k-means-model</guid>
      <pubDate>Thu, 25 Jan 2024 22:46:42 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch LSTM 多目标维度错误</title>
      <link>https://stackoverflow.com/questions/77883294/pytorch-lstm-multi-target-dimension-error</link>
      <description><![CDATA[我几天来一直在尝试执行 LSTM 多目标，但没有成功，因为数据集的前 8 列是目标，其他列是特征，从而产生维度错误。挑战包括根据特征值预测 8 个整数值可以是 0、1 或 2 的目标。之前我成功创建了一个 LSTM 来预测单个目标列，该目标列是所有 8 列的总和。但这个总和在置信度得分中产生了不良结果。有什么错误吗？
导入 pandas 作为 pd
将 numpy 导入为 np

# 设置种子以实现可重复性
np.随机.种子(42)

# 创建数据框
data = {&#39;col_&#39; + str(i+1): np.random.choice([0, 1, 2], 100) 如果 i &lt; 8 else np.random.uniform(-0.99, 0.99, 100) for i in range(100)}
df = pd.DataFrame(数据)
df.head()

# 显示数据框
打印（df）

进口火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
从 torch.utils.data 导入 DataLoader，TensorDataset
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler

# 提取目标和特征
目标 = df.iloc[:, :8].values
特征 = df.iloc[:, 8:].values

# 缩放功能
定标器=标准定标器()
特征=scaler.fit_transform（特征）

# 将目标转换为 LongTensor
目标 = torch.tensor(targets, dtype=torch.long)

# 将特征转换为FloatTensor
特征 = torch.tensor(特征, dtype=torch.float32)

# 将数据分为训练集和测试集
features_train、features_test、targets_train、targets_test = train_test_split(
    特征、目标、test_size=0.2、random_state=None
）

# 创建数据加载器
train_dataset = TensorDataset（features_train，targets_train）
train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True)

# 定义LSTM模型
类 LSTMModel(nn.Module):
    def __init__(自身、输入大小、隐藏大小、层数、输出大小):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size,hidden_​​size,num_layers,batch_first=True)
        self.fc = nn.Linear(隐藏大小, 输出大小)

    def 前向（自身，x）：
        输出，_ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        返回

# 设置超参数
输入大小 = features.shape[1]
隐藏大小 = 64
层数 = 2
output_size = 8 # 目标类的数量
纪元数 = 10
学习率 = 0.001

# 实例化模型、损失函数和优化器
模型 = LSTMModel(输入大小、隐藏大小、层数、输出大小)
标准 = nn.CrossEntropyLoss()
优化器 = optim.Adam(model.parameters(), lr=learning_rate)

# 训练循环
对于范围内的纪元（num_epochs）：
    对于train_loader中的batch_features、batch_targets：
        优化器.zero_grad()
        输出=模型（batch_features）
        损失 = 标准（输出，batch_targets）
        loss.backward()
        优化器.step()

    print(f&#39;Epoch [{epoch+1}/{num_epochs}], 损失: {loss.item():.4f}&#39;)

# 在测试集上评估模型
模型.eval()
使用 torch.no_grad()：
    test_outputs = 模型（features_test）
    _, 预测 = torch.max(test_outputs, 1)

# 计算准确率
正确=(预测==targets_test).sum().item()
总计=targets_test.size(0)
准确率=正确率/总分
print(f&#39;测试准确度: {accuracy:.4f}&#39;)

运行时错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- ------------------------------- IndexError Traceback（最近一次调用最后一次）Cell In[4]，第 60 58 行，针对batch_features , train_loader 中的batch_targets: 59 Optimizer.zero_grad() ---&gt; 60 输出 = 模型（batch_features） 61 损失 = criteria（输出，batch_targets） 62 loss.backward（）

文件 c:\Users\Admin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py:1501，在 Module._call_impl(self, *args, **kwargs)第1496章 第1497章第1498章 1499、第1500章第1501章 return front_call(*args, **kwargs) 第1502章 使用jit时不要调用函数 第1503章 full_backward_hooks, non_full_backward_hooks = [], []

Cell In[4]，第 40 行 38 def forward(self, x): 39 out, _ = self.lstm(x) ---&gt; 40 out = self.fc(out[:, -1, :]) 41 返回 out

IndexError：2 维张量的索引过多

我尝试多次修改 LSTModel 类，但没有得到安全结果]]></description>
      <guid>https://stackoverflow.com/questions/77883294/pytorch-lstm-multi-target-dimension-error</guid>
      <pubDate>Thu, 25 Jan 2024 22:32:05 GMT</pubDate>
    </item>
    <item>
      <title>我的自定义 OCR 模型不起作用并且在每种情况下都预测错误</title>
      <link>https://stackoverflow.com/questions/77883259/my-custom-ocr-model-dont-work-and-predicts-wrong-in-every-case</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77883259/my-custom-ocr-model-dont-work-and-predicts-wrong-in-every-case</guid>
      <pubDate>Thu, 25 Jan 2024 22:22:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用逻辑算法对性别进行谓词？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77882972/how-to-make-predicate-to-gender-using-logistic-algorithm</link>
      <description><![CDATA[附有问题的 Data.csv
https://www.mediafire.com/file/8h65adtolvx6nyo/Data .csv/文件
我的 Excel 文件有 5 列

最喜欢的颜色：最喜欢的颜色（受访者报告的颜色分为暖色、冷色或中性色）。
最喜欢的音乐流派：最喜欢的广泛音乐流派。
最喜欢的饮料：最喜欢的酒精饮料。
最喜欢的软饮料：最喜欢的碳酸饮料
性别：受访者报告的二元性别

如何使用带有预处理和探索性数据分析的逻辑算法来预测性别？
我尝试使用 python Collab 进行的操作
如何完成代码，直到我用性别进行谓词？
df = pd.read_csv(&#39;Data.csv&#39;)

# 探索数据集
打印（df.head（））
打印（df.info（））
# ...执行进一步的探索性分析

# 第2步：预处理

# 处理缺失值
df = df.dropna() # 删除缺失值的行，或使用插补技术

我尝试更清晰的数据
df = pd.read_csv(&#39;Data.csv&#39;)

# 探索数据集
打印（df.head（））
打印（df.info（））
# 进行进一步的探索性分析

# 第2步：预处理

df = df.dropna()

那么如何使用 python 和 colab 机器学习来恢复代码以生成谓词性别？]]></description>
      <guid>https://stackoverflow.com/questions/77882972/how-to-make-predicate-to-gender-using-logistic-algorithm</guid>
      <pubDate>Thu, 25 Jan 2024 21:13:37 GMT</pubDate>
    </item>
    <item>
      <title>在 Cloud Functions 中运行 MobileNetV2</title>
      <link>https://stackoverflow.com/questions/77882843/running-mobilenetv2-in-cloud-functions</link>
      <description><![CDATA[我正在尝试使用 Cloud Functions v2、Python 3.11 和 Tensorflow 2.15 获取给定图像的特征。
由于某种原因，模型在获取预测时没有返回，因此我的函数超时。
我一直在添加更多的 CPU，确切地说是 8 个以及 32GB 内存，但我仍然找不到返回任何结果的方法。
这是我正在执行的代码的一部分：
从 keras.applications 导入 MobileNetV2
从 keras.preprocessing 导入图像
从 keras.applications.mobilenet_v2 导入 preprocess_input

# include_top=False - 排除最终的分类层，专注于提取特征。
# pooling=&#39;avg&#39; - 添加全局平均池化层以将特征图压缩为单个向量（嵌入）。
#weights=&#39;imagenet&#39; - 使用在 ImageNet 上预先训练的权重以实现更好的特征提取。
模型 = MobileNetV2(权重=&#39;imagenet&#39;, include_top=False, pooling=&#39;avg&#39;)

def extract_embeddings(image_url):
  
    响应 = requests.get(image_url)
    响应.raise_for_status()

    img = image.load_img(BytesIO(响应.内容), target_size=(224, 224))

    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, 轴=0)
    img_array = 预处理_输入(img_array)

    嵌入 = model.predict(img_array)
    print(“预测的嵌入”)
    返回 embeddings.flatten()

我在这里有点迷失，所以希望有人能帮忙
我尝试扩展我的资源，但没有成功]]></description>
      <guid>https://stackoverflow.com/questions/77882843/running-mobilenetv2-in-cloud-functions</guid>
      <pubDate>Thu, 25 Jan 2024 20:45:38 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn 机器学习 ANN 模型在 Flask API 调用期间未加载 [关闭]</title>
      <link>https://stackoverflow.com/questions/77881808/sklearn-machine-learning-ann-model-not-loading-during-an-flask-api-call</link>
      <description><![CDATA[我有2个Python代码，1个是使用tensorflow和sklearn训练ANN模型。另一种是加载训练好的模型并进行预测。如果我在本地运行它们并将第二段代码作为函数调用，它可以完美地工作并且能够产生预测。
但是，当我尝试公开加载模型并使用 Flask 预测 API 调用的代码时，模型未加载，并且我不断收到有关反序列化的错误消息。
我将代码上传到我的 GitHub 中。如果有人能帮助我对此进行调试，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77881808/sklearn-machine-learning-ann-model-not-loading-during-an-flask-api-call</guid>
      <pubDate>Thu, 25 Jan 2024 17:16:42 GMT</pubDate>
    </item>
    <item>
      <title>在机器学习中使用日期时间</title>
      <link>https://stackoverflow.com/questions/77881238/using-datetime-in-machine-learning</link>
      <description><![CDATA[我有一个具有各种功能的 pandas 数据集，包括日期时间功能。
它看起来像这样：
&lt;前&gt;&lt;代码&gt; DD SSCL1 SEG_CLASS_CODE FCLCLD PASS_BK SA AU DTD DAY_OF_YEAR
0 2018-01-01 C C 1 0 0 18 -1 1
1 2018-01-01 C C 0 0 7 26 -1 1
2 2018-01-01 C C 0 0 9 18 -1 1
3 2018-01-01 C C 1 10 0 18 -1 1
4 2018-01-01 C C 0 9 1 18 -1 1

我需要使用DD列来训练模型。问题是如何对这一列进行编码？
我无法使用循环特征编码，如下所述：
如何处理机器学习数据预处理中的日期变量- 处理
因为在我教授模型的领域，2020 年与 2018 年不同，2022 年 2 月也不是 2023 年 2 月。因此，年、月和日有时会有所不同。
我的想法是以某种方式将日期时间转换为整数。例如，要获取总天数、小时数、分钟数或秒数，但我不知道起点（也许像往常一样是 1970 年 1 月 1 日）。
最简单的使用方法：dataset[&#39;DD&#39;]).apply(lambda x: x.value)，所以我会得到这样的结果：
&lt;前&gt;&lt;代码&gt;0 1514764800000000000
1 1514764800000000000
2 1514764800000000000
3 1514764800000000000
4 1514764800000000000
                  ...
1450583 1577577600000000000
1450584 1577664000000000000
1450585 1577664000000000000
1450586 1577145600000000000
1450587 1577232000000000000
名称：DD，长度：1450588，数据类型：int64

之后我想使用 MinMaxScaler 或 Standardscaler。
有没有办法根据我的要求对日期时间进行编码？]]></description>
      <guid>https://stackoverflow.com/questions/77881238/using-datetime-in-machine-learning</guid>
      <pubDate>Thu, 25 Jan 2024 15:45:18 GMT</pubDate>
    </item>
    <item>
      <title>使用嵌入张量流模型的 pyinstaller 转换为可执行文件，但预测和预测图像未显示</title>
      <link>https://stackoverflow.com/questions/76063824/converting-to-executable-using-pyinstaller-having-tensorflow-model-embeded-but-p</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76063824/converting-to-executable-using-pyinstaller-having-tensorflow-model-embeded-but-p</guid>
      <pubDate>Thu, 20 Apr 2023 12:01:37 GMT</pubDate>
    </item>
    <item>
      <title>如何给出整数列表作为 Tensorflow 数据集中的输入？</title>
      <link>https://stackoverflow.com/questions/65270127/how-to-give-a-list-of-integers-as-input-in-tensorflow-dataset</link>
      <description><![CDATA[我们正在尝试使用张量流微调/训练预训练的 RoBERTa 模型。为此，我们必须从数据帧创建一个 tf.data.Dataset。
数据框如下所示：

其中三个选项都是编码字符串，答案是一个整数，对应选项 A、B 或 C。
我们尝试使用以下方法创建 tf.dataset：
features= [&#39;选项A&#39;, &#39;选项B&#39;, &#39;选项C&#39;]

训练数据集 = (
    tf.data.Dataset.from_tensor_slices(
        （
            tf.cast(train_data[特征].values, tf.float32),
            tf.cast(train_data[&#39;答案&#39;].values, tf.int32)
        ）
    ）
）

但是这不起作用，因为我们收到以下错误：
ValueError：无法将 NumPy 数组转换为张量（不支持的对象类型列表）。

我读到我们不能将列表用作 tf.dtype，我们现在在其中放置了“float32”。但我们也无法将数据框中的列表转换为浮点数。
我们如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/65270127/how-to-give-a-list-of-integers-as-input-in-tensorflow-dataset</guid>
      <pubDate>Sat, 12 Dec 2020 21:44:39 GMT</pubDate>
    </item>
    <item>
      <title>无法导入名称“ops”python</title>
      <link>https://stackoverflow.com/questions/51076277/cannot-import-name-ops-python</link>
      <description><![CDATA[我正在尝试运行一个应用程序。但是我收到错误：
from createDB import load_dataset
将 numpy 导入为 np
导入keras
从 keras.utils 导入到_categorical
将 matplotlib.pyplot 导入为 plt
从 sklearn.model_selection 导入 train_test_split
从 keras.models 导入顺序、输入、模型
从 keras.layers 导入密集、Dropout、Flatten
从 keras.layers 导入 Conv2D、MaxPooling2D
从 keras.layers.normalization 导入 BatchNormalization
从 keras.layers.advanced_activations 导入 LeakyReLU
################################################33
#显示数据集
X_train,y_train,X_test,y_test = load_dataset()
print(&#39;训练数据形状：&#39;, X_train.shape, y_train.shape)
print(&#39;测试数据形状：&#39;, X_test.shape, y_test.shape)
#################################################### ##########
# 从火车标签中找到唯一的数字
类 = np.unique(y_train)
nClasses = len(类)
print(&#39;输出总数：&#39;, nClasses)
print(&#39;输出类：&#39;, 类)
#################################################### #
#plt.figure(figsize=[5,5])
#
## 显示训练数据中的第一张图像
#plt.子图(121)
#plt.imshow(X_train[0,:,:], cmap=&#39;灰色&#39;)
#plt.title(&quot;基本事实：{}&quot;.format(y_train[0]))
#
## 显示测试数据中的第一张图像
#plt.子图(122)
#################################################### ######
#X_train.max()
#X_train.shape()
####################################
# 标准化和 float32
X_train = X_train.astype(&#39;float32&#39;)
X_test = X_test.astype(&#39;float32&#39;)
X_train = X_train / 255。
X_测试 = X_测试 / 255。
###############################3
#将标签从分类编码更改为one-hot编码
y_train_one_hot = to_categorical(y_train)
y_test_one_hot = to_categorical(y_test)

# 使用one-hot编码显示类别标签的变化
print(&#39;原标签：&#39;, y_train[25])
print(&#39;转换为one-hot后：&#39;, y_train_one_hot[25])
##############################################
# 训练分为训练和验证
X_train,X_valid,train_label,valid_label = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=13)
X_train.shape,
X_valid.shape,
火车标签.形状，
valid_label.shape
##########################
批量大小 = 64
历元 = 20
类数 = 3
####################
Fashion_model = 顺序()
Fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation=&#39;线性&#39;,input_shape=(28,28,3),padding=&#39;相同&#39;))
Fashion_model.add(LeakyReLU(alpha=0.1))
Fashion_model.add(MaxPooling2D((2, 2),padding=&#39;相同&#39;))
Fashion_model.add(Conv2D(64, (3, 3), 激活=&#39;线性&#39;,填充=&#39;相同&#39;))
Fashion_model.add(LeakyReLU(alpha=0.1))
Fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding=&#39;相同&#39;))
Fashion_model.add(Conv2D(128, (3, 3), 激活=&#39;线性&#39;,填充=&#39;相同&#39;))
Fashion_model.add(LeakyReLU(alpha=0.1))
Fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding=&#39;相同&#39;))
Fashion_model.add(压平())
Fashion_model.add（密集（128，激活=&#39;线性&#39;））
Fashion_model.add(LeakyReLU(alpha=0.1))
Fashion_model.add（密集（num_classes，激活=&#39;softmax&#39;））


&lt;块引用&gt;
  文件“F:\anaconda\install\envs\anaconda35\lib\site-packages\keras\backend\tensorflow_backend.py”，第 6 行，位于
      从tensorflow.python.framework导入ops作为tf_ops
导入错误：无法导入名称“ops”

如何解决此错误？]]></description>
      <guid>https://stackoverflow.com/questions/51076277/cannot-import-name-ops-python</guid>
      <pubDate>Thu, 28 Jun 2018 06:42:49 GMT</pubDate>
    </item>
    <item>
      <title>什么是逻辑？ softmax 和 softmax_cross_entropy_with_logits 有什么区别？</title>
      <link>https://stackoverflow.com/questions/34240703/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entrop</link>
      <description><![CDATA[在 tensorflow API 文档中，他们使用名为 logits 的关键字。它是什么？很多方法都是这样写的：
tf.nn.softmax(logits, name=None)

如果logits只是一个通用的Tensor输入，为什么它被命名为logits？
&lt;小时/&gt;
其次，下面两种方法有什么区别？
tf.nn.softmax(logits, name=None)
tf.nn.softmax_cross_entropy_with_logits（logits，标签，名称=无）

我知道 tf.nn.softmax 的作用，但不知道另一个。一个例子真的很有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/34240703/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entrop</guid>
      <pubDate>Sat, 12 Dec 2015 14:03:27 GMT</pubDate>
    </item>
    </channel>
</rss>