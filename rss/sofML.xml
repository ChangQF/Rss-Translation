<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 18 Jul 2024 03:20:24 GMT</lastBuildDate>
    <item>
      <title>双子座本月印刷日</title>
      <link>https://stackoverflow.com/questions/78762138/gemini-printing-days-of-the-month</link>
      <description><![CDATA[我尝试从下图中打印垃圾日，但无论我怎么尝试，都无法正确打印这些天。
我尝试了不同的温度和提示技术，但都不起作用。
我正在寻找双子座模型的提示
]]></description>
      <guid>https://stackoverflow.com/questions/78762138/gemini-printing-days-of-the-month</guid>
      <pubDate>Thu, 18 Jul 2024 01:57:06 GMT</pubDate>
    </item>
    <item>
      <title>XGBoostError：参数详细程度的值 -1 超出界限 [0,3]</title>
      <link>https://stackoverflow.com/questions/78761783/xgboosterror-value-1-for-parameter-verbosity-exceed-bound-0-3</link>
      <description><![CDATA[错误消息如标题所示。根据下面的代码，这对我来说毫无意义：
clf = xgboost.XGBClassifier(verbosity=1)
print (clf.__class__, clf.verbosity) 
# prints &lt;class &#39;xgboost.sklearn.XGBClassifier&#39;&gt; 1
clf.fit(X=train_data_iter[features].fillna(0), y=train_data_iter[&#39;y&#39;]) # 错误在这里出现

值显然是 1，但不知何故却变成了 -1？我不明白。]]></description>
      <guid>https://stackoverflow.com/questions/78761783/xgboosterror-value-1-for-parameter-verbosity-exceed-bound-0-3</guid>
      <pubDate>Wed, 17 Jul 2024 22:12:23 GMT</pubDate>
    </item>
    <item>
      <title>将 Tensorflow 模型转换为 CoreML 模型时出错</title>
      <link>https://stackoverflow.com/questions/78761234/error-converting-tensorflow-model-to-coreml-model</link>
      <description><![CDATA[NotImplementedError Traceback（最近一次调用最后一次）
Cell In[19]，第 1 行
----&gt; 1 mlmodel = ct.convert(model, convert_to=&quot;mlmodel&quot;, source=&quot;tensorflow&quot;)

文件 ~/anaconda3/lib/python3.11/site-packages/coremltools/converters/_converters_entry.py:551，在 convert(model, source, input, output, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline)
539 exact_target = _determine_target(convert_to, minimum_deployment_target)
540 _validate_conversion_arguments(
541 model,
542 exact_source,
(...)
549 minimum_deployment_target,
550 )
--&gt; 551 need_fp16_cast_pass = _need_fp16_cast_pass(compute_precision, exact_target)
553 如果 pass_pipeline 为 None:
554 pass_pipeline = PassPipeline()

文件 ~/anaconda3/lib/python3.11/site-packages/coremltools/converters/_converters_entry.py:624，位于 _need_fp16_cast_pass(compute_precision, convert_to)
620 def _need_fp16_cast_pass(
621 compute_precision: Optional[Union[precision, FP16ComputePrecision]], convert_to: Text
622 ) -&gt; bool:
623 if convert_to not in (&quot;mlprogram&quot;, &quot;neuralnetwork&quot;, &quot;milinternal&quot;, &quot;milpython&quot;):
--&gt; 624 raise NotImplementedError(f&quot;后端转换器 {convert_to} 未实现&quot;)
626 if compute_precision is None:
627 return convert_to != &quot;neuralnetwork&quot;

NotImplementedError: 后端转换器 mlmodel 未实现

不知道这个错误是什么意思。我正在尝试将使用 tensorflow 构建的 CNN 模型转换为 coreml 模型，但我一直收到上述错误。
mlmodel = ct.convert(model, convert_to=&quot;mlmodel&quot;, source=&quot;tensorflow&quot;)

这是在从保存的 .h5 文件导入模型后完成的。]]></description>
      <guid>https://stackoverflow.com/questions/78761234/error-converting-tensorflow-model-to-coreml-model</guid>
      <pubDate>Wed, 17 Jul 2024 19:08:55 GMT</pubDate>
    </item>
    <item>
      <title>为 Llama2 实现无需提示的少样本学习</title>
      <link>https://stackoverflow.com/questions/78761191/implementing-few-shot-learning-without-prompts-for-llama2</link>
      <description><![CDATA[我正在使用 Llama2 模型。我已经成功启动并微调了模型，并且我还使用了带和不带 LangChain 的 Few-Shot Prompting。但是，现在我正在寻找一种类似于 SetFit 的 Llama2 方法，允许在没有明确提示的情况下进行 Few-Shot Learning。有没有人有这方面的经验或可以提供如何实现这一点的指导？
我尝试了以下方法：

带 LangChain 的 Few-Shot Prompting。
不带 LangChain 的 Few-Shot Prompting。

我正在寻找详细的指南或示例，演示如何在不使用明确提示的情况下为 Llama2 实现 Few-Shot Learning 以提高模型的性能。]]></description>
      <guid>https://stackoverflow.com/questions/78761191/implementing-few-shot-learning-without-prompts-for-llama2</guid>
      <pubDate>Wed, 17 Jul 2024 18:56:51 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 golearn 在 GoLang 中训练随机森林</title>
      <link>https://stackoverflow.com/questions/78760263/how-to-train-random-forest-in-golang-using-golearn</link>
      <description><![CDATA[我正在使用“github.com/sjwhitworth/golearn”在我的数据集上训练随机森林，我已经正确配置了它，但我一直收到错误：
undefined: NewParameter
undefined: NewProblem
undefined: Train
undefined: Predict

如何修复它？
这是我目前的代码：
package main

import (
&quot;fmt&quot;
&quot;log&quot;

&quot;github.com/sjwhitworth/golearn/base&quot;
&quot;github.com/sjwhitworth/golearn/ensemble&quot;
&quot;github.com/sjwhitworth/golearn/evaluation&quot;
)
func trainAndTest() {
// 加载训练数据集
trainingData, err := base.ParseCSVToInstances(&quot;training.csv&quot;, true)
if err != nil {
log.Fatal(err)
}

// 加载测试数据集
testData, err := base.ParseCSVToInstances(&quot;test.csv&quot;, true)
if err != nil {
log.Fatal(err)
}

// 假设类标签位于 golearn 中的最后一列
//trainingData.Shuffle()

// 初始化一个新的 RandomForest 分类器
rf := ensemble.NewRandomForest(10, 2)

// 训练 RandomForest 分类器
rf.Fit(trainingData)

// 使用训练好的分类器预测测试数据集的标签
predictions, err := rf.Predict(testData)
if err != nil {
log.Fatal(err)
}

// 评估模型
fusionMat, err := evaluation.GetConfusionMatrix(testData, predictions)
if err != nil {
log.Fatal(err)
}

// 打印评估指标
fmt.Println(evaluation.GetSummary(confusionMat))

}
func main() {
trainAndTest()

}


我已经将 golang 版本更改为可能的解决方案，但不知道如何进一步处理。]]></description>
      <guid>https://stackoverflow.com/questions/78760263/how-to-train-random-forest-in-golang-using-golearn</guid>
      <pubDate>Wed, 17 Jul 2024 15:06:13 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Huggingface Hub 上成功设置和检索 HuggingfaceDataset 的元数据信息？</title>
      <link>https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat</link>
      <description><![CDATA[我有许多数据集，它们是从字典中创建的，如下所示：
info = DatasetInfo(
description=&quot;my happy lil dataset&quot;,
version=&quot;0.0.1&quot;,
homepage=&quot;https://www.myhomepage.co.uk&quot;
)
train_dataset = Dataset.from_dict(prepare_data(data[&quot;train&quot;]), info=info)
test_dataset = Dataset.from_dict(prepare_data(data[&quot;test&quot;]), info=info)
validation_dataset = Dataset.from_dict(prepare_data(data[&quot;validation&quot;]),info=info)

然后我将它们组合成一个 DatasetDict。
# 创建 DatasetDict
dataset = DatasetDict(
{&quot;train&quot;: train_dataset, &quot;test&quot;: test_dataset, &quot;validation&quot;: validation_dataset}
)

到目前为止，一切顺利。如果我访问 dataset[&#39;train&#39;].info.description，我会看到预期结果“My happy lil dataset”。
因此，我将其推送到集线器，如下所示：
dataset.push_to_hub(f&quot;{organization}/{repo_name}&quot;, commit_message=&quot;Some commit message&quot;)

这也成功了。
但是，当我从集线器拉回数据集并访问与其相关的信息时，我没有获取数据集的描述，而是只得到了一个空字符串；像这样：
pulled_data = full = load_dataset(&quot;f{organization}/{repo_name}&quot;, use_auth_token = True)

# 我希望以下内容打印出&quot;my happy lil dataset&quot;
print(pulled_data[&quot;train&quot;].info.description)
# 但是，它返回的是 &#39;&#39;

我是否错误地从集线器加载了数据？我是否只推送了我的数据集，而没有推送信息？
我觉得我遗漏了一些显而易见的东西，但我真的不确定。]]></description>
      <guid>https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat</guid>
      <pubDate>Wed, 17 Jul 2024 13:23:04 GMT</pubDate>
    </item>
    <item>
      <title>DSPy 无法检索 ChromaDB 中带有文本嵌入的段落</title>
      <link>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</link>
      <description><![CDATA[我正在使用 DSPy 和 ChromaDB 为 pdf 文件开发 RAG 应用程序。
首先，我从 pdf 中获取文本并将其作为块添加到 Chromadb。还添加了块的嵌入。并尝试使用 DSPy 检索与查询相关的块。但是它出现了错误
存储数据和嵌入
def store_document_in_chromadb(text):
chunks = chunk_document(text)
ids = [f&#39;chunk_{i}&#39; for i in range(len(chunks))]
embeddings = [get_embedding(chunk).tolist() for chunk in chunks]

collection.add(ids=ids, documents=chunks, embeddings=embeddings)

我尝试像这样检索相关块，
retriever_model = ChromadbRM(&quot;contracts_collection&quot;, &#39;db/&#39;, k=2)
dspy.settings.configure(lm=llama2_model, rm=retriever_model)

class GenerateAnswer(dspy.Signature): 
“”“”根据给出的上下文回答问题。“”“”
context = dspy.InputField(desc=&quot;可能包含相关上下文&quot;)
question = dspy.InputField()
answer = dspy.OutputField(desc=&quot;通常为 5 到 10 个单词&quot;)

class RAG(dspy.Module): 
def __init__(self, num_passages=2):
super().__init__()
self.retrieve = dspy.Retrieve(k=num_passages)
self.generate_answer = dspy.ChainOfThought(GenerateAnswer)

def forward(self, question):
context = self.retrieve(question).passages
prediction = self.generate_answer(context=context, question=question)
return dspy.Prediction(context=context, answer=prediction.answer)

with dspy.context(lm=llama2_model, rm=retriever_model):
module = RAG()
response = module(&quot;总支出是多少&quot;)
print(response)

当我运行此程序时，出现此错误
InvalidDimensionException：嵌入维度 384 与集合维度 768 不匹配
但是当我从 ChromaDB 中删除嵌入时，它会正确检索相关块。
为什么使用嵌入时没有出现此错误？]]></description>
      <guid>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</guid>
      <pubDate>Wed, 17 Jul 2024 08:03:30 GMT</pubDate>
    </item>
    <item>
      <title>请帮助我构建我的二元分类项目[关闭]</title>
      <link>https://stackoverflow.com/questions/78758049/please-help-me-to-structure-my-binary-classification-project</link>
      <description><![CDATA[我正在开发一个二元分类项目。最初，我得到了一个包含 3290 行和 15 列的真实数据的数据集。然后，我使用 CTGAN 网络生成了包含 100000 行的合成数据集。然后，我将这两个数据集混洗，得到 1 个数据集。我的目标变量高度不平衡（是：23175，否：76825）。我对我的项目有以下问题？

有 7 个分类预测因子，其中有 4 个二元分类列（性别、婚姻状况等），其他是非二元分类变量（省、区等）。我应该使用什么编码技术？

处理这里的数据不平衡问题是否重要？如果重要，我应该使用什么技术来处理这个不平衡问题？

我的数值变量都不是正态分布的。处理这个问题是否重要？如果是，我需要使用哪些技术（例如，如果需要，进行转换）？

我需要标准化或规范化我的数据吗？如果是，为什么？

我应该在这里使用哪些特征选择技术？

我的项目的顺序是什么。请按以下顺序排列。（数据不平衡问题处理/编码分类变量/转换数值数据/标准化或规范化数值数据/特征选择\建模）

最后我可以使用神经网络来实现这一点吗？如果可以，我可以使用哪些 NN 类型

]]></description>
      <guid>https://stackoverflow.com/questions/78758049/please-help-me-to-structure-my-binary-classification-project</guid>
      <pubDate>Wed, 17 Jul 2024 06:57:24 GMT</pubDate>
    </item>
    <item>
      <title>如何在 XGBoost 决策树的叶节点上显示预测类标签？</title>
      <link>https://stackoverflow.com/questions/78757774/how-do-i-display-predicted-class-labels-on-the-leaf-nodes-of-an-xgboost-decision</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78757774/how-do-i-display-predicted-class-labels-on-the-leaf-nodes-of-an-xgboost-decision</guid>
      <pubDate>Wed, 17 Jul 2024 05:22:10 GMT</pubDate>
    </item>
    <item>
      <title>如何将多头自注意力输出的形状更改为可以馈送到卷积层的形状？</title>
      <link>https://stackoverflow.com/questions/78757193/how-to-change-shape-of-multi-head-self-attention-output-to-a-shape-that-can-be-f</link>
      <description><![CDATA[我遇到了这样的错误：
MHSA（多头自注意力）的输出如下：
torch.Size([20, 197, 768])


批次大小为 20
序列长度为 197（之前为 196，添加类标记后变为 197）
嵌入维度为 768

我想将其重塑以适应以下格式，以便将其馈送到卷积层：
torch.Size([batch_size, channel, width, height])

我尝试通过使用以下方法添加新维度来实现此目的方法：
torch.unsqueeze(1)
torch.transpose(1, 3)

这成功地允许馈送到卷积层。但是，我不确定这种方法是否正确，如果不正确，请纠正我。
目前，我正在尝试一种不同的方法：
new_size = int(math.sqrt(sequence_length))
torch.transpose(1, 2).view(batch_size, embed_dim, new_size, new_size)

这导致错误，指出形状对于大小为 (some_number) 的输入无效。这是因为序列长度（197）不是完全平方的，得出的是一个十进制数，而视图函数需要输入一个整数，平方运算在转换为整数后得出 16，但 batch_size * 768 * 16 * 16 不等于 batch_size * 197 * 768，导致错误
我的分析正确吗？我该如何解决这个问题？还有没有更好的方法？]]></description>
      <guid>https://stackoverflow.com/questions/78757193/how-to-change-shape-of-multi-head-self-attention-output-to-a-shape-that-can-be-f</guid>
      <pubDate>Wed, 17 Jul 2024 00:24:06 GMT</pubDate>
    </item>
    <item>
      <title>当已知目标值的特征向量时，如何使用监督式机器学习进行时间序列预测？</title>
      <link>https://stackoverflow.com/questions/78757005/how-to-use-supervised-ml-for-time-series-predictions-when-the-feature-vector-for</link>
      <description><![CDATA[我尝试使用 LSTM 预测仪器的连续“偏移”校准值。这些偏移值之前已被证明与用作特征的一对温度值有很好的相关性。这些偏移值显示出周期性，因此为模型选择了 LSTM。但是，我发现的所有 LSTM 示例都使用来自先前数据点序列的特征向量来预测目标。我觉得这可能无法捕捉序列中每个特征向量与其偏移之间的关系。而且由于在预测中只使用了先前数据点序列的特征向量，因此无法有效地利用该温度偏移关系。
为了解决这个问题，我已经将当前数据点的特征向量添加到用于训练模型的向量序列中，当然，也添加到用于预测目标偏移的序列中。
但是，我如何才能对要预测的偏移的特征向量赋予更大的权重，以允许模型利用温度偏移关系？
创建序列的代码如下所示：
def create_sequences(x, y, time_steps = 24):
xs, ys = [], []
for i in range(len(x) - time_steps - 1):
x_temp = x[i:(i + time_steps + 1)] #在目标值之前创建一个 time_steps 序列
xs.append(x_temp)
ys.append(y[i + time_steps]) #目标值为序列之后的值
return np.array(xs), np.array(ys)
]]></description>
      <guid>https://stackoverflow.com/questions/78757005/how-to-use-supervised-ml-for-time-series-predictions-when-the-feature-vector-for</guid>
      <pubDate>Tue, 16 Jul 2024 22:49:31 GMT</pubDate>
    </item>
    <item>
      <title>即使经过数百个时期，pytorch AdamW 的 LR 仍未衰减</title>
      <link>https://stackoverflow.com/questions/78752899/lr-not-decaying-for-pytorch-adamw-even-after-hundreds-of-epochs</link>
      <description><![CDATA[我有以下使用 Pytorch 中的 AdamW 优化器的代码：
optimizer = AdamW(params=self.model.parameters(), lr=0.00005)

我尝试使用 wandb 进行登录，如下所示：
lrs = {f&#39;lr_group_{i}&#39;: param_group[&#39;lr&#39;]
for i, param_group in enumerate(self.optimizer.param_groups)}
wandb.log({&quot;train_loss&quot;: avg_train_loss, &quot;val_loss&quot;: val_loss, **lrs})

请注意 weight_decay 参数的默认值为 0.01（对于 AdamW）。
当我检查 wandb 仪表板时，它显示 AdamW 的 LR 即使在 200 个 epoch 之后也相同，并且根本没有衰减。我尝试了几次。

为什么 LR 衰减没有发生？
此外，它仅显示一个参数组的 LR。为什么会这样？似乎我在这里错过了一些基本的东西。有人可以指出吗？]]></description>
      <guid>https://stackoverflow.com/questions/78752899/lr-not-decaying-for-pytorch-adamw-even-after-hundreds-of-epochs</guid>
      <pubDate>Tue, 16 Jul 2024 06:09:44 GMT</pubDate>
    </item>
    <item>
      <title>我们可以使用 FastAPI 在 model.predict() 中直接使用 Pydantic 模型（BaseModel）吗？如果不行，为什么？</title>
      <link>https://stackoverflow.com/questions/71849683/can-we-use-pydantic-models-basemodel-directly-inside-model-predict-using-fas</link>
      <description><![CDATA[我正在使用带有 FastAPI 的 Pydantic 模型 (Basemodel)，并将输入转换为 dictionary，然后将其转换为 Pandas DataFrame，以便将其传递到 model.predict() 函数中进行机器学习预测，如下所示：
from fastapi import FastAPI
import uvicorn
from pydantic import BaseModel
import pandas as pd
from typing import List

class Inputs(BaseModel):
f1: float,
f2: float,
f3: str

@app.post(&#39;/predict&#39;)
def predict(features: List[Inputs]):
output = []

# 循环输入特征列表
for data in features:
result = {}

# 将数据转换为 dict()，然后转换为 DataFrame
data = data.dict()
df = pd.DataFrame([data])

# 获取预测
prediction = classifier.predict(df)[0]

# 获取概率
probability = classifier.predict_proba(df).max()

# 分配给字典 
result[&quot;prediction&quot;] = prediction
result[&quot;probability&quot;] = probability

# 将字典附加到列表（许多输出）
output.append(result)

返回输出

它运行良好，只是我不太确定它是否优化或是否是正确的方法，因为我将输入转换两次以获得预测。此外，我不确定在输入数量巨大的情况下它是否会快速地工作。对此有什么改进吗？如果有办法（甚至除了使用 Pydantic 模型之外），我可以直接工作并避免经过转换和循环。]]></description>
      <guid>https://stackoverflow.com/questions/71849683/can-we-use-pydantic-models-basemodel-directly-inside-model-predict-using-fas</guid>
      <pubDate>Tue, 12 Apr 2022 22:11:19 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中使用具有焦点损失的类权重来处理多类分类的不平衡数据集</title>
      <link>https://stackoverflow.com/questions/64751157/how-to-use-class-weights-with-focal-loss-in-pytorch-for-imbalanced-dataset-for-m</link>
      <description><![CDATA[我正在研究语言任务的多类分类（4 个类别），并且我正在使用 BERT 模型进行分类任务。我正在关注这篇博文NLP 迁移学习：针对文本分类的微调 BERT。 我的 BERT Fine Tuned 模型返回 nn.LogSoftmax(dim=1)。
我的数据非常不平衡，因此我使用 sklearn.utils.class_weight.compute_class_weight 来计算类别的权重，并使用 Loss 中的权重。
class_weights = compute_class_weight(&#39;balanced&#39;, np.unique(train_labels), train_labels)
weights= torch.tensor(class_weights,dtype=torch.float)
cross_entropy = nn.NLLLoss(weight=weights) 


我的结果不太好，因此我想用 Focal Loss 进行实验，并为 Focal Loss 编写了一个代码。
class FocalLoss(nn.Module):
def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):
super(FocalLoss, self).__init__()
self.alpha = alpha
self.gamma = gamma
self.logits = logits
self.reduce = reduce

def forward(self, input, target):
BCE_loss = nn.CrossEntropyLoss()(inputs, target)

pt = torch.exp(-BCE_loss)
F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss

if self.reduce:
return torch.mean(F_loss)
else:
return F_loss

我现在有 3 个问题。首先也是最重要的是

我应该将类权重与 Focal Loss 结合使用吗？
如果我必须在这个 Focal Loss 中实现权重，我可以在 nn.CrossEntropyLoss() 中使用 weights 参数吗？
如果这个实现不正确，那么包括权重在内的正确代码应该是什么（如果可能）
]]></description>
      <guid>https://stackoverflow.com/questions/64751157/how-to-use-class-weights-with-focal-loss-in-pytorch-for-imbalanced-dataset-for-m</guid>
      <pubDate>Mon, 09 Nov 2020 11:53:49 GMT</pubDate>
    </item>
    <item>
      <title>如何规范化混淆矩阵？</title>
      <link>https://stackoverflow.com/questions/20927368/how-to-normalize-a-confusion-matrix</link>
      <description><![CDATA[我使用 scikit-learn 中的 confusion_matrix() 为我的分类器计算了一个混淆矩阵。混淆矩阵的对角线元素表示预测标签等于真实标签的点数，而非对角线元素表示被分类器错误标记的点数。
我想规范化我的混淆矩阵，使其仅包含 0 到 1 之间的数字。我想从矩阵中读取正确分类的样本的百分比。
我找到了几种规范化矩阵的方法（行和列规范化），但我对数学不太了解，不确定这是否是正确的方法。]]></description>
      <guid>https://stackoverflow.com/questions/20927368/how-to-normalize-a-confusion-matrix</guid>
      <pubDate>Sat, 04 Jan 2014 22:10:34 GMT</pubDate>
    </item>
    </channel>
</rss>