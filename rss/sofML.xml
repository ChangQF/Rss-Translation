<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 02 May 2024 15:15:22 GMT</lastBuildDate>
    <item>
      <title>预测 = model.predict(email_features_array)</title>
      <link>https://stackoverflow.com/questions/78420048/prediction-model-predictemail-features-array</link>
      <description><![CDATA[ 预测 = model.predict(email_features_array)
                 ^^^^^^^^^^^^^^
AttributeError：“numpy.ndarray”对象没有属性“预测”


我已经构建并训练了 ML 模型，我只想通过 Flask 使用它，但每当它要进入 mode.predict 时，它都会给我这个问题
这是我的代码：
这些是我的进口
导入pickle
进口再
导入字符串
从 nltk.tokenize 导入 word_tokenize
从 nltk.corpus 导入停用词
从 sklearn.feature_extraction.text 导入 CountVectorizer
import numpy as np # 导入 NumPy

加载预训练模型
&lt;前&gt;&lt;代码&gt;
模型 = pickle.load(open(&#39;c:/Users/7rbe2/OneDrive/S/Main items/Grad project/Phishward/PhishWarden/app/Python/logistic_regression_model.pkl&#39;, &#39;rb&#39;))

# 初始化CountVectorizer
CV = CountVectorizer()

这里是清洁功能
def remove_special_characters(word):
    return word.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation))

def remove_stop_words(单词):
    stop_words = set(stopwords.words(&#39;英语&#39;))
    返回 [如果单词不在 stop_words 中，则返回单词中的单词]

def remove_hyperlink(word):
    return re.sub(r&quot;http\S+&quot;, &quot;&quot;, word)

def fit_count_vectorizer(文本):
    # 清理并标记文本
    clean_text = 删除_特殊_字符（文本）
    clean_text = 删除_超链接(cleaned_text)
    标记 = word_tokenize(cleaned_text)
    标记=remove_stop_words(标记)
    clean_text = &#39; &#39;.join(tokens)
    return clean_text # 返回清理后的文本

这是问题发生的预测
def 预测():
    email_text =“你好世界”
    clean_text = fit_count_vectorizer(email_text)
    cv.fit([cleaned_text]) # 让 CountVectorizer 适合已清理的文本
    email_features_array = cv.transform([cleaned_text]) # 使用transform而不是fit_transform
    # 假设`model`已经被定义和训练
    预测 = model.predict(email_features_array)

    # 应用预训练模型来预测电子邮件被钓鱼的概率
    概率 = model.predict_proba(email_features_array)
    如果预测[0] == 1：
        结果=“网络钓鱼”
        概率得分 = 概率[0][1] * 100
        打印（结果，概率分数）
    别的：
        结果=&#39;合法&#39;
        概率得分 = 概率[0][0] * 100
        打印（结果，概率分数）


# 用法示例
预测（）

，我不知道我能做些什么来解决这个问题，我几乎尝试了所有可能的方法]]></description>
      <guid>https://stackoverflow.com/questions/78420048/prediction-model-predictemail-features-array</guid>
      <pubDate>Thu, 02 May 2024 15:01:11 GMT</pubDate>
    </item>
    <item>
      <title>IndexError：数组索引太多：数组是一维的，但在从混淆矩阵计算 TP、TN、FP、FN 时对 2 个进行了索引</title>
      <link>https://stackoverflow.com/questions/78420043/indexerror-too-many-indices-for-array-array-is-1-dimensional-but-2-were-index</link>
      <description><![CDATA[我有几个混淆指标，然后我计算了所有 CM 的平均值。现在，我想计算 CM 的评估指标，例如 True Positive、True Negative、False Positive 和 False Negative。
# 计算ML模型的多个CM的平均值
avg_cm_dt = np.array(cm_dt, dtype=“对象”)
print(f&#39;CM 平均值：{avg_cm_dt}&#39;)


 # 这里我收到错误
TP = avg_cm_dt[0,0]
FP = avg_cm_dt[0,1]
FN = avg_cm_dt[1,0]
TN = avg_cm_dt[1,1]

当我运行上面的代码时，我得到了“IndexError：数组索引太多：数组是一维的，但有 2 个被索引”。
如何根据该代码的这些指标计算预期结果？]]></description>
      <guid>https://stackoverflow.com/questions/78420043/indexerror-too-many-indices-for-array-array-is-1-dimensional-but-2-were-index</guid>
      <pubDate>Thu, 02 May 2024 15:00:36 GMT</pubDate>
    </item>
    <item>
      <title>部署机器学习flask api（h5模型）</title>
      <link>https://stackoverflow.com/questions/78419889/deploying-machine-learning-flask-api-h5-models</link>
      <description><![CDATA[我有一个模型可以预测未来 30 年的二氧化碳排放量，我需要将其部署为 API，以便从可视化 Web 项目中以 json 形式检索预测数据，但是使用 pythonanywhere 是它无法访问 h5 模型文件，尽管它们位于同一目录中。
我尝试了另一种方法来访问模型，但它保持不变：
这是 API Flask 代码：
fromflask导入Flask，request，jsonify
将 pandas 导入为 pd
从 statsmodels.tsa.holtwinters 导入指数平滑
从先知导入先知
导入作业库
从flask_cors导入CORS

应用程序=烧瓶（__名称__）
CORS(app, resources={r&quot;/*&quot;: {&quot;origins&quot;: &quot;*&quot;}})

loaded_model_arima = joblib.load(&#39;model_arima.h5&#39;)
loaded_model_holt = joblib.load(&#39;model_holt.h5&#39;)
loaded_model_prophet = joblib.load(&#39;model_prophet.h5&#39;)

@app.route(&#39;/ARIMA&#39;,methods=[&#39;GET&#39;])
def Predict_arima():
    年 = int(30)
    预测=loaded_model_arima.forecast(步数=年)
    Forecast_index = [str(2022 + i) for i in range(1, 年 + 1)]
    返回 jsonify({&#39;annee&#39;: Forecast_index, &#39;预测&#39;: Forecast.tolist()})

@app.route(&#39;/HOLT-WINTERS&#39;,methods=[&#39;GET&#39;])
def Predict_holt_winters():
    年 = int(30)
    预测=loaded_model_holt.forecast（步骤=年）
    Forecast_index = [str(2022 + i) for i in range(1, 年 + 1)]
    返回 jsonify({&#39;annee&#39;: Forecast_index, &#39;预测&#39;: Forecast.tolist()})

@app.route(&#39;/prophet&#39;,methods=[&#39;GET&#39;])
def Predict_prophet():
    年= int(request.json[&#39;年&#39;])
    未来 = pd.DataFrame({&#39;ds&#39;: pd.date_range(start=&#39;2023-01-01&#39;, period=years, freq=&#39;A&#39;)})
    预测=loaded_model_prophet.predict(未来)
    Forecast_data = 预测[[&#39;ds&#39;, &#39;yhat&#39;]]
    Forecast_index = [str(date.year) for Forecast_data[&#39;ds&#39;] 中的日期]
    Forecast_values = Forecast_data[&#39;yhat&#39;].tolist()
    返回 jsonify({&#39;annee&#39;: Forecast_index, &#39;预测&#39;: Forecast_values})

如果 __name__ == &#39;__main__&#39;:
    应用程序运行（调试=真）

这是我得到的错误：
2024-04-30 17:43:57,222：运行 WSGI 应用程序时出错
2024-04-30 17:43:57,222：KeyError：92
2024-04-30 17:43:57,222：文件“/var/www/wildvcwduibv_pythonanywhere_com_wsgi.py”，第 16 行，在  中
2024-04-30 17:43:57,222：从flask_app导入应用程序作为应用程序#noqa
2024-04-30 17:43:57,223：
2024-04-30 17:43:57,223：文件“/home/wildvcwduibv/mysite/flask_app.py”，第 11 行，在  中
2024-04-30 17:43:57,223：loaded_model_arima = joblib.load（&#39;/home/wildvcwduibv/mysite/model_arima.h5&#39;）
2024-04-30 17:43:57,223：
2024-04-30 17:43:57,223：文件“/usr/local/lib/python3.10/site-packages/joblib/numpy_pickle.py”，第 587 行，加载中
2024-04-30 17:43:57,223：obj = _unpickle（fobj，文件名，mmap_mode）
2024-04-30 17:43:57,223：
2024-04-30 17:43:57,223：文件“/usr/local/lib/python3.10/site-packages/joblib/numpy_pickle.py”，第 506 行，位于 _unpickle
2024-04-30 17:43:57,223：obj = unpickler.load（）
2024-04-30 17:43:57,223：
2024-04-30 17:43:57,224：文件“/usr/local/lib/python3.10/pickle.py”，第 1213 行，加载中
2024-04-30 17:43:57,224：调度[key[0]]（自身）
2024-04-30 17:43:57,224: ************************************** **********
2024-04-30 17:43:57,224：如果您看到导入错误且不知道原因，
2024-04-30 17:43:57,224：我们有一个专门的帮助页面来帮助您调试：
2024-04-30 17:43:57,224：https://help.pythonanywhere.com/pages/DebuggingImportError/
2024-04-30 17:43:57,224: ************************************** **********

我尝试了不同的库，例如 h5py 来访问 h5 模型，但没有任何效果，我还尝试了不同的部署服务，遇到了相同的问题或依赖关系问题]]></description>
      <guid>https://stackoverflow.com/questions/78419889/deploying-machine-learning-flask-api-h5-models</guid>
      <pubDate>Thu, 02 May 2024 14:34:51 GMT</pubDate>
    </item>
    <item>
      <title>Visual Studio Code 中的 PyLance 无法识别 TensorFlow.keras 命名空间</title>
      <link>https://stackoverflow.com/questions/78419755/tensorflow-keras-namespace-not-recognized-by-pylance-in-visual-studio-code</link>
      <description><![CDATA[我在 Visual Studio Code 中遇到 PyLance 问题，其中tensorflow.keras 命名空间无法被识别，导致 IntelliSense 和自动完成不完整。看来这个问题源于 TensorFlow 利用延迟加载功能 (_KerasLazyLoader) 推迟加载不必要的包，直到需要它们为止。
我搜索了有关该主题的现有问题，但我发现的最新讨论可以追溯到 2023 年底。此外，提议的解决方案主要涉及从长远来看可能不可靠的解决方法。
是否有官方解决方案来确保 PyLance 正确识别 tensorflow.keras 命名空间？我无法想象延迟加载通常会破坏 VSCode 智能感知（尽管我肯定是错的）。或者，是否有任何可靠的解决方法在未来的更新中不太可能被破坏？任何见解或指导将不胜感激。
软件：
MacOS 索诺玛 14.1.2
Python 3.12
张量流2.16.1
参考之前发布的问题/问题：

https://github.com/microsoft/pylance-release/issues/3249 
VSCode 自动完成和建议 (IntelliSense) 不支持适用于 Tensorflow 和 Keras 库吗？
https://community.deeplearning.ai/t/unable-to-import-tensorflow-keras-pylinte0401-import-error-in-visual-studio-code/512586
https://jagaimox.wordpress.com/2020/12/28/configure-python-intellisense-on-vscode-for-tensorflow-1-14-or-1-15/ （张量流1.14/1.15）

我在上面的链接中尝试了几种解决方案，但它们对我不起作用。其中一些可能需要一些我需要做但我没有做的额外配置（例如，tensorflow.init 中引用了 _typing，但尚未定义 _typing - 可能已过时） .]]></description>
      <guid>https://stackoverflow.com/questions/78419755/tensorflow-keras-namespace-not-recognized-by-pylance-in-visual-studio-code</guid>
      <pubDate>Thu, 02 May 2024 14:11:58 GMT</pubDate>
    </item>
    <item>
      <title>MLPerf Docker 容器</title>
      <link>https://stackoverflow.com/questions/78419544/mlperf-docker-container</link>
      <description><![CDATA[我正在尝试复制 MLPerf 任务，但无法弄清楚他们想要哪个 Docker 容器。我对 Docker 的经验很少，而且我一筹莫展——我错过了什么？
情况是这样的：MLPerf 是一组机器学习基准。我对图像分类训练任务感兴趣。他们所有训练任务的参考实现可在 https://github.com/mlcommons/training/ 获取tree/master，特别是 install_cuda_docker。 sh 脚本。
但是，我找不到任何对我应该获取的 docker 容器的引用。 MLCommons（生产 MLPerf 的组织）拥有一系列 docker 容器，但没有一个描述似乎合适。 （最接近的是图像分割 docker 图像，但是（1）这是一个不同的问题，（2）它是为 PyTorch 构建的，但我知道参考图像分类代码使用 TensorFlow。我还偶然发现了对推理的引用 /em&gt; 容器，但想必这些也不适合训练。）
顺便说一句，其他一些训练任务目录（例如rnn_speech_recognition或graph_neural_network）包含Dockerfile，但图像分类目录中没有Dockerfile。]]></description>
      <guid>https://stackoverflow.com/questions/78419544/mlperf-docker-container</guid>
      <pubDate>Thu, 02 May 2024 13:37:09 GMT</pubDate>
    </item>
    <item>
      <title>找到返回最高精度的阈值</title>
      <link>https://stackoverflow.com/questions/78419199/find-the-threshold-that-returns-the-highest-precision</link>
      <description><![CDATA[我有这个数据集：
&lt;前&gt;&lt;代码&gt;(26.5625,0)
(29.5625,0)
(30.390625,0)
(18.640625,0)
(27.984375,0)
(26.984375,0)
(25.703125,0)
(25.78125,0)
(32.09375,0)
(25.59375,0)
(27.703125,0)
(30.828125,0)
(23.578125,0)
(21.890625,0)
(25.734375,0)
(24.65625,0)
(27.46875,0)
(31.640625,0)
(26.53125,0)
(25.078125,0)
(30.65625,0)
(24.515625,0)
(25.21875,0)
(21.78125,0)
(28.984375,0)
(29.765625,0)
(27.171875,1)
(30.46875,1)
(35.3125,1)
(27.90625,1)
(34.9375,1)
(33.4375,1)
(30.90625,1)
(31.671875,1)
(32.40625,1)
(26.078125,1)
(31.171875,1)
(36.21875,1)
(35.0625,1)
(35.65625,1)
(36.65625,1)
(37.96875,1)
(31.953125,1)
(33.15625,1)
(37.34375,1)

对应精度的排序为：
ordered_labels: [1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1 , 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

平均精度：0.7338

我正在尝试找到返回最高精度的阈值（例如 27.0）（在这种情况下为 0.7338），这意味着尽可能多的“1”位于左侧，并且“1”尽可能低0&#39;在右边。 （例如：[1, 1, 1, 0, 0, 0] 的精度为 1.0）我尝试过逻辑回归，但返回的阈值为“0.7”，而不是 27.0 等数字。
我应该对该数据集使用线性回归还是支持向量机？
我的输出：（下面的代码）
精度：[0.33333333 0.0.1.]
回想一下：[1。 0.0.0.]
阈值：[0.13154558 0.7006058 0.72969373]

这是我正在使用的代码：
导入 matplotlib.pyplot 作为 plt
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.datasets 导入 make_classification
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.model_selection 导入 train_test_split
从 sklearn.metrics 导入 precision_recall_curve
从 ast 导入文字_eval

# 创建一个简单的数据集
Scores_labels_path = &#39;数据.txt&#39;
X、y = []、[]
打开（scores_labels_path）作为文件：
    对于文件中的行：
        line =literal_eval(line.rstrip())
        X.append(行[0])
        y.append(行[1])

X = np.array(X).reshape(-1, 1)
y = np.array(y)
# X1, y1 = make_classification(n_samples=1000, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=7)
lr = 逻辑回归（random_state=42）
lr.fit(X_train, y_train)
y_scores = lr.predict_proba(X_test)
精度、召回率、阈值 = precision_recall_curve(y_test, y_scores[:, 1])

print(&quot;精度：{}&quot;.format(精度))
print(&quot;召回: {}&quot;.format(recall))
print(&quot;阈值: {}&quot;.format(threshold))
]]></description>
      <guid>https://stackoverflow.com/questions/78419199/find-the-threshold-that-returns-the-highest-precision</guid>
      <pubDate>Thu, 02 May 2024 12:36:04 GMT</pubDate>
    </item>
    <item>
      <title>ConvLSTM 模型的数据预处理过程中遇到问题</title>
      <link>https://stackoverflow.com/questions/78418879/having-trouble-during-preprocessing-of-data-for-a-convlstm-model</link>
      <description><![CDATA[我有 10 年（2014-24）的 GHRSST 数据，我将其分为两部分：训练（2014-2021）和测试（2021-2024）数据集。训练数据集大小为 4.18GB，测试数据集大小为 1.98GB。我正在尝试构建一个 ConvLSTM 模型来预测未来几天的 SST 数据，但是我似乎无法通过预处理阶段。我正在尝试在 google colab 上执行以下代码：
导入火炬
将 numpy 导入为 np
进口达斯克

# NetCDF 文件的路径
dataset_path = &#39;/content/drive/MyDrive/train_dataset_10.nc&#39;
# 使用 dask chunks 打开数据集
ds = xr.open_mfdataset(dataset_path, chunks={&#39;时间&#39;: 1, &#39;纬度&#39;: 50, &#39;经度&#39;: 50})

# 提取SST数据；假设变量名为“analysis_sst”，并使用计算将其转换为 numpy 来加载数据
sst_data = ds[&#39;analysisd_sst&#39;].compute() # 这将确保数据加载到内存中

上述代码需要 30 分钟执行，并占用 10.2/12.7GB 可用系统 RAM。这也会导致以下代码出现问题，该代码使用过多的 CPU（尽管 GPU 处于打开状态）并且由于缺少 RAM 而导致内核崩溃：
# 清除 CUDA 中所有未使用的内存
torch.cuda.empty_cache()

# 将数据转换为张量，出于兼容性原因确保它首先在 CPU 上
sst_tensor = torch.tensor(sst_data.values, dtype=torch.float32)

# 如果有可用的 GPU，则将张量传输到 GPU
如果 torch.cuda.is_available():
    sst_tensor = sst_tensor.to(&#39;cuda&#39;)

print(&quot;SST 张量的形状：&quot;, sst_tensor.shape)

defscale_data_gpu(data_tensor,batch_size):
    scaled_data = torch.full_like(data_tensor, float(&#39;nan&#39;)) # 为填充 NaN 的缩放数据初始化张量

    # 批量处理数据
    对于范围内的开始（0，data_tensor.shape [0]，batch_size）：
        结束 = 开始 + 批次大小
        批处理 = data_tensor[开始:结束]

        # 为有效（非 NaN）数据点创建掩码
        valid_mask = ~torch.isnan(batch)

        if valid_mask.any(): # 确保至少有一些有效数据
            data_min = torch.min(batch[valid_mask])
            data_max = torch.max(batch[valid_mask])

            # 缩放批次，但仅在有效的情况下应用
            batch_scaled = (batch - data_min) / (data_max - data_min)
            scaled_data[开始:结束][valid_mask] = batch_scaled[valid_mask]

    返回缩放数据

# 在 GPU 上应用缩放，仅考虑有效（非 NaN）值
使用 torch.no_grad()：
  sst_scaled_tensor = scale_adata_gpu（data_tensor = sst_tensor，batch_size = 64）

在上面的代码中，我试图掩盖标记为“NaN”的值因为它代表了我的数据集中的地形，然后缩放数据并批量处理它以准备训练。
我该如何进行这项工作？这个过程需要多长时间？有更有效的方法吗？
我尝试过使用 Dask，但没有成功，而且 GEE 不支持 CNN。我对机器学习还很陌生，非常感谢您提供的任何帮助。我不知道此后如何继续。]]></description>
      <guid>https://stackoverflow.com/questions/78418879/having-trouble-during-preprocessing-of-data-for-a-convlstm-model</guid>
      <pubDate>Thu, 02 May 2024 11:34:30 GMT</pubDate>
    </item>
    <item>
      <title>如何将我的 svm 模型部署到我的 flutter 应用程序中？</title>
      <link>https://stackoverflow.com/questions/78418707/how-can-i-deploy-my-svm-model-to-my-flutter-app</link>
      <description><![CDATA[将 SVM 模型部署到 Flutter 应用程序的最佳方式是什么？
我无法将我的代码重新实现为 TenserFlow lite，因为准确率下降到 15%。我的模型获取音频文件并分析它们。我的模型的输出是文本，我希望它按原样显示。我不知道如何使用抱脸。对我来说最好的方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78418707/how-can-i-deploy-my-svm-model-to-my-flutter-app</guid>
      <pubDate>Thu, 02 May 2024 11:04:31 GMT</pubDate>
    </item>
    <item>
      <title>如何将关系数据库集成到数据科学项目中？</title>
      <link>https://stackoverflow.com/questions/78418612/how-to-integrate-a-relational-database-into-a-data-science-project</link>
      <description><![CDATA[我是一名数据科学家，主要使用 CSV 文件进行数据分析，但我现在正在探索在我的项目中使用关系数据库。我想了解将关系数据库集成到我的工作流程中的最佳实践。
我应该如何将数据从关系数据库（例如 PostgreSQL、MySQL）导入到我的数据科学环境（例如 Python、R）中？我应该直接在数据库中执行连接和探索性数据分析，还是应该将数据导出到 CSV 文件，然后继续分析？
我过去主要使用 CSV 文件，但现在我正在着手一个现实世界的数据科学项目，我需要在其中使用关系数据库。不过，我对此还比较陌生，正在寻求有关如何有效地将数据库集成到我的工作流程中的指导。]]></description>
      <guid>https://stackoverflow.com/questions/78418612/how-to-integrate-a-relational-database-into-a-data-science-project</guid>
      <pubDate>Thu, 02 May 2024 10:49:01 GMT</pubDate>
    </item>
    <item>
      <title>IndexError：列表索引超出streamlit范围[关闭]</title>
      <link>https://stackoverflow.com/questions/78418486/indexerror-list-index-out-of-range-in-streamlit</link>
      <description><![CDATA[因此，我正在尝试构建一个 Streamlit RAG 应用程序，该应用程序从 url 中提取信息并从中学习，然后用户可以向模型询问与 url 中的文章相关的问题，模型将提供合适的答案。
我在我的笔记本上执行了此操作，它工作得很好，只是在我的 Streamlit 应用程序中遇到 IndexError: list index out of range 错误，我将 GoogleGenerativeAIEmbeddings 与 FAISS 结合使用。
这是代码块
embeddings = GoogleGenerativeAIEmbeddings(model = &#39;models/embedding-001&#39;)
矢量索引= FAISS.from_documents（文档，嵌入）

这是来自 stresmlit 应用程序的回溯
IndexError：列表索引超出范围
追溯：
文件“C:\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py”，第 584 行，位于 _run_script
    exec（代码，模块.__dict__）
文件“C:\Users\owner\Desktop\Projects\nlp\main.py”，第 84 行，在  中
    vectorstore_openai = FAISS.from_documents（文档，嵌入）
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_core\vectorstores.py”，第 550 行，from_documents
    返回 cls.from_texts(文本、嵌入、元数据=元数据、**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_community\vectorstores\faiss.py”，第 931 行，from_texts
    返回 cls.__from(
           ^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_community\vectorstores\faiss.py”，第 888 行，位于 __from
    索引 = faiss.IndexFlatL2(len(embeddings[0]))
                                  ~~~~~~~~~~^^^

就像我上面说的，这在我的笔记本上完美运行，我很困惑为什么会发生这种情况]]></description>
      <guid>https://stackoverflow.com/questions/78418486/indexerror-list-index-out-of-range-in-streamlit</guid>
      <pubDate>Thu, 02 May 2024 10:25:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么准确度有如此差异？</title>
      <link>https://stackoverflow.com/questions/78417156/why-such-difference-in-accuracy</link>
      <description><![CDATA[我正在使用 resnet 50 模型进行图像分类。运行模型几个时期后，以下是我得到的结果：
纪元 8/20
损失：0.5705 - 准确度：0.8785 - val_loss：0.9226 - val_accuracy：0.8135

纪元 9/20
损失：0.5509 - 准确度：0.8780 - val_loss：0.9321 - val_accuracy：0.7979

获得的这些结果可以改进，但我们暂时保留这些结果。
我获得了我期望的值：
# 评估模型
test_loss, test_acc = model.evaluate(val_data_flow)
print(&quot;测试准确度：&quot;, test_acc)

＃ 结果：
测试精度：0.7978515625

但是这里的准确性很糟糕：
从sklearn.metrics导入classification_report


# 对测试数据集生成预测
预测 = model.predict(val_data_flow)

# 将预测转换为类标签
Predicted_labels = np.argmax(预测，轴=1)

# 从测试数据集中提取真实标签
true_labels = val_data_flow.labels

# 生成分类报告
class_names = list(val_data_flow.class_indices.keys())
打印（分类报告（真实标签，预测标签，目标名称=类名称））


 精确召回率 f1-score 支持

       1 0.33 0.38 0.36 330
       2 0.00 0.00 0.00 14
       3 0.17 0.17 0.17 133
       4 0.09 0.11 0.10 102
       5 0.00 0.00 0.00 5
       6 0.00 0.00 0.00 21
       7 0.12 0.12 0.12 133
       8 0.18 0.16 0.17 154
       9 0.18 0.13 0.15 132

精度 0.21 1024
宏观平均 0.12 0.12 0.12 1024
加权平均值 0.20 0.21 0.21 1024

什么可能导致这样的问题？或者是因为功能的工作方式不同？
我的数据集非常不平衡。]]></description>
      <guid>https://stackoverflow.com/questions/78417156/why-such-difference-in-accuracy</guid>
      <pubDate>Thu, 02 May 2024 06:02:21 GMT</pubDate>
    </item>
    <item>
      <title>我的 LightGBM 模型中实际上有多少棵树？</title>
      <link>https://stackoverflow.com/questions/78416214/how-many-trees-do-i-actually-have-in-my-lightgbm-model</link>
      <description><![CDATA[我的代码看起来像这样
clf = lgb.LGBMClassifier(max_深度=3，详细程度=-1，n_estimators=3)
clf.fit(train_data[特征],train_data[&#39;y&#39;],sample_weight=train_data[&#39;权重&#39;])
print (f“我有 {clf.n_estimators_} 估计器”)
图，ax = plt.subplots（nrows = 4，figsize =（50,36），sharex = True）
lgb.plot_tree(clf, tree_index=7, dpi=600, ax=ax[0]) # 为什么有第七棵树？
lgb.plot_tree(clf, tree_index=8, dpi=600, ax=ax[1]) # 为什么它有第 8 棵树？
#lgb.plot_tree(clf, tree_index=9, dpi=600, ax=ax[2]) # 崩溃
#lgb.plot_tree(clf, tree_index=10, dpi=600, ax=ax[3]) # 崩溃

令我惊讶的是，尽管有 n_estimators=3，但我似乎有 9 棵树？我如何实际设置树的数量，以及与之相关的，n_estimators 是做什么的？我读过文档，我以为是树的数量，但似乎是别的东西。
另外，我如何解释单独的树及其顺序 0、1、2 等。我了解随机森林，以及每棵树如何同等重要。在 boosting 中，第一棵树最重要，下一棵树的重要性要低得多，下一棵树的重要性要低得多。所以在我的脑海中，当我查看树形图时，我该如何“模拟” LightGBM 推理过程？]]></description>
      <guid>https://stackoverflow.com/questions/78416214/how-many-trees-do-i-actually-have-in-my-lightgbm-model</guid>
      <pubDate>Wed, 01 May 2024 22:24:21 GMT</pubDate>
    </item>
    <item>
      <title>当不打乱测试数据时，Torchmetrics 的准确性问题。为什么？</title>
      <link>https://stackoverflow.com/questions/78415660/torchmetrics-accuracy-issue-when-dont-shuffle-test-data-why</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78415660/torchmetrics-accuracy-issue-when-dont-shuffle-test-data-why</guid>
      <pubDate>Wed, 01 May 2024 19:45:02 GMT</pubDate>
    </item>
    <item>
      <title>RFE 与 GBM 集成，用于特征选择和超参数调整</title>
      <link>https://stackoverflow.com/questions/78405164/integration-of-rfe-with-gbm-for-feature-selection-and-hyperparameter-tuning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78405164/integration-of-rfe-with-gbm-for-feature-selection-and-hyperparameter-tuning</guid>
      <pubDate>Mon, 29 Apr 2024 21:00:14 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 获取 One vs Rest SVC() 的模型参数？</title>
      <link>https://stackoverflow.com/questions/78395647/how-to-get-the-model-parameter-for-one-vs-rest-svc-using-python</link>
      <description><![CDATA[我尝试使用decision_function_shape= ovr制作onvsrest分类模型，但是当我将其更改为decision_function_shape= ovo时，它给了我与ovr相同的结果。结果我读到 svc() 正在使用 ovo 作为基础，无论它是作为 ovr 还是 ovo 启动的。那么我怎样才能改变我的代码，以便它给我一个 ovr 结果呢？
model3 = SVC(kernel = &#39;rbf&#39;, Decision_function_shape=&#39;ovr&#39;)
model3.fit(X_train, Y_train)
model3_predictions = model3.predict(X_test)

我尝试过使用 OneVsRestClassifier() 但不知道如何给出所有这些命令的输出，它总是出错并说 OneVsRestClassifier 没有这些命令。有没有办法用 OneVsRestClassifier 获取 cm、sm、sv、beta 和截距？
cm3 = fusion_matrix(Y_test, model3_predictions, labels=[-1,0,1])
sm3 = 分类报告（Y_测试，model3_预测）
support_vector3 = model3.support_
n_sv_model3 = model3.n_support_
alpha_model3 = pd.DataFrame(model3.dual_coef_)
b_model3 = pd.DataFrame(model3.intercept_)

希望有人能帮助我，先谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78395647/how-to-get-the-model-parameter-for-one-vs-rest-svc-using-python</guid>
      <pubDate>Sat, 27 Apr 2024 16:20:07 GMT</pubDate>
    </item>
    </channel>
</rss>