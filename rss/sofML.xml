<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 15 Aug 2024 18:20:44 GMT</lastBuildDate>
    <item>
      <title>使用只有一个数据点的数据集来训练神经网络无法完美预测 y 变量？</title>
      <link>https://stackoverflow.com/questions/78876176/using-a-dataset-of-only-one-data-point-to-train-neural-network-cannot-perfectly</link>
      <description><![CDATA[我的样本X_trn和y_trn都只有一个item，我搭建了一个神经网络进行分类，训练模型然后用X_trn进行预测，X_trn的预测结果和y_trn的预测结果不一样。这是否意味着我的神经网络有问题？
#=========================x_trn&amp;y_trn==========================#
print(X_trn)
mom12m
0 -0.334957

print(y_trn)
0 4.0
名称：ret_exc_lead1m_w，dtype：float64

#===========================model=============================#
mod = Sequential()
mod.add(Input(shape = (X_trn.shape[1],))) 
mod.add(Dense(1, 激活 = &#39;relu&#39;)) 
mod.add(Dense(1))
opt = Adam(learning_rate=0.1)
mod.compile(loss=&#39;binary_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;])
history = mod.fit(X_trn, y_trn, epochs=5, batch_size=1)

# 预测
y_pred.append(mod.predict(X_trn)[0])

print(y_pred)

[array([1.5509232], dtype=float32), 
array([1.6640353], dtype=float32), 
array([0.], dtype=float32), 
array([0.], dtype=float32)]


我希望模型预测 X_trn 应该得到一些接近的值到 y_trn。但这完全不同？我可能哪里错了？
即使我将损失函数更改为“mse”，预测值也在 0.5~1.6 左右，每次值都会发生变化，并且与 y_trn 不同。]]></description>
      <guid>https://stackoverflow.com/questions/78876176/using-a-dataset-of-only-one-data-point-to-train-neural-network-cannot-perfectly</guid>
      <pubDate>Thu, 15 Aug 2024 17:16:50 GMT</pubDate>
    </item>
    <item>
      <title>在 Python 中导入“catboost”包时出现问题</title>
      <link>https://stackoverflow.com/questions/78876149/problem-with-importing-catboost-package-in-python</link>
      <description><![CDATA[从我开始尝试解决这个问题已经过去了四天。我试图在 Python 中安装 catboost 包，一切都很顺利，直到我决定升级到 Python 3.12。升级后，我在尝试导入它时遇到了一个错误。：
numpy.dtype 大小已更改，可能表示二进制不兼容。预期来自 C 标头的 96，来自 PyObject 的 88
我尝试了各种方法，甚至降级到以前的版本，但问题仍然存在。然后我恢复到 Python 3.11 并再次安装包，但我仍然遇到同样的错误。此外，我安装了最新版本的 Numpy。
我希望有人能帮忙，因为我觉得我很快就要卖掉这台笔记本电脑了。

卸载 Python 3.12 并重新安装 3.11 版本，问题仍然存在。
卸载软件包并重新安装，错误仍然存​​在。
我尝试在 Python 3.12.4 的 conda 环境中运行它，一切正常。
]]></description>
      <guid>https://stackoverflow.com/questions/78876149/problem-with-importing-catboost-package-in-python</guid>
      <pubDate>Thu, 15 Aug 2024 17:06:29 GMT</pubDate>
    </item>
    <item>
      <title>TimeSeriesDataSet/TemporalFusionTransformer - 发现类“NoneType”错误，我认为它在“target_scale”数组中</title>
      <link>https://stackoverflow.com/questions/78876125/timeseriesdataset-temporalfusiontransformer-found-class-nonetype-error-i-th</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78876125/timeseriesdataset-temporalfusiontransformer-found-class-nonetype-error-i-th</guid>
      <pubDate>Thu, 15 Aug 2024 16:58:04 GMT</pubDate>
    </item>
    <item>
      <title>如何使用线图作为机器学习特征？</title>
      <link>https://stackoverflow.com/questions/78875992/how-to-use-line-plots-as-machine-learning-features</link>
      <description><![CDATA[在机器学习模型中，是否可以输入整个图表作为特征之一？为了便于理解，我希望创建一个机器学习模型，根据变星的光变曲线（光度与时间）对变星类型进行分类。我已经根据它们的 x 和 y（光度和时间）值提取了许多这样的光变曲线，作为坐标对 (x, y) (x, y)。但是要创建一个对线图进行分类的机器学习模型，我应该使用什么？我应该输入多个数据框，每个数据框都包含 x 和 y 值吗？我应该对图表进行傅里叶变换并输入其每个基频吗？还是我应该输入平均值、中位数、众数和范围等值？]]></description>
      <guid>https://stackoverflow.com/questions/78875992/how-to-use-line-plots-as-machine-learning-features</guid>
      <pubDate>Thu, 15 Aug 2024 16:18:35 GMT</pubDate>
    </item>
    <item>
      <title>在 JAX 中跨多个设备恢复优化器状态</title>
      <link>https://stackoverflow.com/questions/78875908/restoring-an-optimizer-state-across-multiple-devices-in-jax</link>
      <description><![CDATA[我正在四个 GPU 上使用 jax 和 optax 训练模型，需要保存和恢复优化器状态，​​但在加载时遇到了问题。优化器状态已初始化 --
optimizer = optax.adamw(0.0001)
class Runner():
self.opt_state = optimizer.init(self.params) # 初始化优化器
self.opt_state = jax.device_put_replicated(self.opt_state, devices) # 将 opt_state 复制到所有设备

然后仅在第一个设备上保存 --
def save(self, save_dir):
params_path = save_dir + &#39;/{}_params.pickle&#39;.format(self.train_step)
opt_state_path = save_dir + &#39;/{}_opt_state.pickle&#39;.format(self.train_step)

with open(params_path, &#39;wb&#39;) as file:
pickle.dump(jax.device_get(jax.tree_map(lambda x: x[0], self.params)), file)
with open(opt_state_path, &#39;wb&#39;) as file:
pickle.dump(jax.device_get(jax.tree_map(lambda x: x[0], self.opt_state)), file)

logs.info(&#39;saved to {}, step {}&#39;.format(save_dir, self.train_step))

稍后，加载参数和优化器状态
def restore(self, save_dir, step, restore_opt_state = True):
opt_state_path = save_dir + &#39;/{}_opt_state.pickle&#39;.format(step)
if restore_opt_state: # True
with open(opt_state_path, &#39;rb&#39;) as file:
opt_state = pickle.load(file)
replicate_opt_state = jax.device_put_replicated(opt_state, self.devices) # 将 opt_state 复制到所有设备
self.opt_state = update_dict(self.opt_state, replicate_opt_state)
logging.info(&#39;restored opt state from {}, step {}&#39;.format(save_dir, step))

其中 update_dict() 定义如下：
def update_dict(params1, params2):
assert type(params1) == type(params2)
if type(params1) == dict:
params1.update(params2)
elif type(params1) == FrozenDict:
p = dict(params1[&#39;params&#39;])
p.update(params2[&#39;params&#39;])
params1 = {&#39;params&#39;: FrozenDict(p)}
else:
raise ValueError(&#39;params1 type {} not implement&#39;.format(type(params1)))
return params1

当我在正确的 save_dir 和 step 上运行 restore 方法时，update_dict() 将其解释为元组，无法恢复优化器状态 (ValueError: params1 type &lt;class &#39;tuple&#39;&gt; not implement)。但我不确定这是为什么，因为 save() 明确只保存第一台设备上的优化器状态。此外，当从单个设备加载参数并将它们复制到多个设备时，此相同设置似乎有效。非常感谢您的指导！
我期望与 params 具有类似的性能，后者以字典形式保存和加载。我尝试过将另一个 jax.tree_map(lambda x: x[0] opt_state) 明确添加到 restore_opt_state 上下文管理器，但没有成功。看起来 opt_state 被保存为元组，而不是字典，而 params 则被保存为字典。将元组中的一个元素通过 update_dict() 传递就足够了吗？]]></description>
      <guid>https://stackoverflow.com/questions/78875908/restoring-an-optimizer-state-across-multiple-devices-in-jax</guid>
      <pubDate>Thu, 15 Aug 2024 15:55:30 GMT</pubDate>
    </item>
    <item>
      <title>在 kmeans 每次迭代中显示质心和图像</title>
      <link>https://stackoverflow.com/questions/78875750/displaying-the-centroid-and-the-image-at-each-iteration-of-kmeans</link>
      <description><![CDATA[我叫 Marvin。我最近使用 k-means 无监督学习算法实现了图像压缩，但我想在压缩过程的每次迭代中看到质心和图像，但我不知道该怎么做？
我还想知道这些图像是否可以用来形成动画，显示质心和图像从其初始状态变为最终状态？]]></description>
      <guid>https://stackoverflow.com/questions/78875750/displaying-the-centroid-and-the-image-at-each-iteration-of-kmeans</guid>
      <pubDate>Thu, 15 Aug 2024 15:15:05 GMT</pubDate>
    </item>
    <item>
      <title>迁移学习预训练模型</title>
      <link>https://stackoverflow.com/questions/78875648/transfer-learning-pretrained-model</link>
      <description><![CDATA[我目前正在 Google Colab 上拟合迁移学习模型。但是，我在代码中遇到了一条警告消息
Epoch 1/30
/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121：UserWarning：您的 PyDataset 类应在其构造函数中调用 super().__init__(**kwargs)。**kwargs 可以包括 workers、use_multiprocessing、max_queue_size。请勿将这些参数传递给 fit()，因为它们将被忽略。
self._warn_if_super_not_called()
在第一个 epoch 之后，我收到以下错误：

KeyboardInterrupt Traceback（最近一次调用最后一次）
in &lt;cell line: 16&gt;()
14 # 拟合模型
15 # 运行单元。执行需要一些时间
---&gt; 16 training_history = model_efficientnet.fit(
17 training_set,
18 validation_data=validate_set,
值得一提的是，我已经成功地拟合了其他六个迁移学习模型，没有任何问题，而且它们的准确度令人满意。
我将不胜感激任何有关如何解决此问题的指导或建议。
谢谢！
我希望获得训练准确度和验证准确度]]></description>
      <guid>https://stackoverflow.com/questions/78875648/transfer-learning-pretrained-model</guid>
      <pubDate>Thu, 15 Aug 2024 14:49:45 GMT</pubDate>
    </item>
    <item>
      <title>我对神经网络回归模型结果的解释</title>
      <link>https://stackoverflow.com/questions/78874398/interpretation-of-my-result-of-the-neural-network-regression-model</link>
      <description><![CDATA[我自定义的网络使用线性层，中间有 dropout 层。我知道在评估阶段，dropout 层不活跃，这通常会导致验证损失高于训练损失。为了更好地理解损失值，我计算了训练损失和验证损失之间的平均绝对差。是否有针对这种差异的一般规则或指导方针可用于改进我的模型？
我也使用此设置进行训练
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;)

计算绝对差异（距离）：
fold_dist_rmse = np.mean(np.abs(np.subtract(fold_train_losses, fold_val_losses)))
fold_dist_r2 = np.mean(np.abs(np.subtract(fold_train_r2, fold_val_r2)))

这是我的结果模型：
折叠 1/6：训练 RMSE：35.93554242793712 | 有效 RMSE：29.25876541711503 | 距离：6.769316131166423
折叠 1/6：训练 $R^2$：0.9707746378183365 | 有效 $R^2$：0.9887450106143951 | 距离：0.01803360450267792
折叠 2/6：训练 RMSE：33.979019073410804 | 有效 RMSE：37.212038090521546 |距离：6.741708392705943
折叠 2/6：训练 $R^2$：0.9723036136627198 | 有效 $R^2$：0.9756944441795349 | 距离：0.01469032382965088
折叠 3/6：训练 RMSE：32.49953081599383 | 有效 RMSE：42.565526526587355 | 距离：12.88033462681533
折叠 3/6：训练 $R^2$：0.9757700593471527 |有效 $R^2$：0.9610446383953094 | 距离：0.03205667233467102
折叠 4/6：训练 RMSE：32.936544826006646 | 有效 RMSE：55.71217012745017 | 距离：25.01426354134579
折叠 4/6：训练 $R^2$：0.9724288802146912 | 有效 $R^2$：0.9643870314359665 |距离：0.024146793484687804
我正在寻找某人来解释训练和验证损失之间的平均绝对差异是否是分析我的模型的良好指标。此外，我非常感谢任何有关如何改进我的模型以实现 RMSE 低于 10 的建议。]]></description>
      <guid>https://stackoverflow.com/questions/78874398/interpretation-of-my-result-of-the-neural-network-regression-model</guid>
      <pubDate>Thu, 15 Aug 2024 09:12:26 GMT</pubDate>
    </item>
    <item>
      <title>如何将带有自定义二进制分类头的“transformers.TFRobertaForSequenceClassification”放入没有新层的“tensorflow.keras.Model”中？</title>
      <link>https://stackoverflow.com/questions/78874099/how-to-put-transformers-tfrobertaforsequenceclassification-with-custom-binary</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78874099/how-to-put-transformers-tfrobertaforsequenceclassification-with-custom-binary</guid>
      <pubDate>Thu, 15 Aug 2024 07:41:56 GMT</pubDate>
    </item>
    <item>
      <title>ML 项目想法（机器学习中的推荐系统项目是否仍然值得做？在这样的项目中可以展示什么新颖之处？[关闭]</title>
      <link>https://stackoverflow.com/questions/78873904/ml-project-idea-are-recommender-systems-project-in-machine-learning-still-worth</link>
      <description><![CDATA[我目前是 B.Tech 的三年级学生，我必须为机器学习方法课程做一个小项目。我选择了推荐系统作为标题，但我怀疑在当今技术时代是否值得做这个？如果不值得，您有什么建议，可以学习并发展自己以将其添加到简历中的最佳项目创意。
我看到许多代码库都写了代码但没什么有趣的，我想听听您关于在这样的项目中可以引入哪些新奇事物的意见。]]></description>
      <guid>https://stackoverflow.com/questions/78873904/ml-project-idea-are-recommender-systems-project-in-machine-learning-still-worth</guid>
      <pubDate>Thu, 15 Aug 2024 06:39:08 GMT</pubDate>
    </item>
    <item>
      <title>对列应用对数变换</title>
      <link>https://stackoverflow.com/questions/78873685/applying-log-transformation-to-a-column</link>
      <description><![CDATA[
我已使用 OneHotEncoder 对性别列进行编码。我想仅对 Female[0] 列应用对数转换，但它却对所有列都应用了对数转换 — 为什么？
我的代码：
import pandas as p
from sklearn.preprocessing import FunctionTransformer, OneHotEncoder
from sklearn.compose import ColumnTransformer
import numpy as n

customer=p.read_csv(&#39;/content/Customers.csv&#39;)
customer.drop([&#39;CustomerID&#39;,&#39;Profession&#39;,&#39;Family Size&#39;,&#39;Work Experience&#39;],axis=1,inplace=True)
column=ColumnTransformer(
[
(&#39;ohe_gender&#39;,OneHotEncoder(sparse=False,dtype=n.int32),[0])
],remainder=&#39;passthrough&#39;
)
function=ColumnTransformer(
[
(&#39;function&#39;,FunctionTransformer(n.log1p),[0,1])
],remainder=&#39;passthrough&#39;
)
s=column.fit_transform(customer)
function.fit_transform(s)

输出：
 array([[0.00000000e+00, 6.93147181e-01, 1.90000000e+01, 1.50000000e+04, 3.90000000e+01],
[0.00000000e+00, 6.93147181e-01, 2.10000000e+01, 3.50000000e+04, 8.10000000e+01],
[6.93147181e-01, 0.00000000e+00, 2.00000000e+01, 8.60000000e+04, 6.00000000e+00],
...,
[0.00000000e+00, 6.93147181e-01, 8.70000000e+01, 9.09610000e+04, 1.40000000e+01],
[0.00000000e+00, 6.93147181e-01, 7.70000000e+01, 1.82109000e+05, 4.00000000e+00],
[0.00000000e+00, 6.93147181e-01, 9.00000000e+01, 1.10610000e+05, 5.20000000e+01]]

在 FunctionTransformer 之前进行编码 (OHE) 后，输出为
array([[ 0, 1, 19, 15000, 39],
[ 0, 1, 21, 35000, 81],
[ 1, 0, 20, 86000, 6],
...,
[ 0, 1, 87, 90961, 14],
[ 0, 1, 77, 182109, 4],
[ 0, 1, 90, 110610, 52]])

我确实想在上述数组的第 0 个索引中应用对数变换，但正如您在第一个输出中看到的那样，它应用于所有值，尽管我在列变换器中指定了 [0]，为什么？我希望输出只有 [0] 索引的对数。]]></description>
      <guid>https://stackoverflow.com/questions/78873685/applying-log-transformation-to-a-column</guid>
      <pubDate>Thu, 15 Aug 2024 04:58:50 GMT</pubDate>
    </item>
    <item>
      <title>python：“顺序”层需要 1 个输入，但它收到了 48 个输入张量</title>
      <link>https://stackoverflow.com/questions/78872766/python-layer-sequential-expects-1-inputs-but-it-received-48-input-tensors</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78872766/python-layer-sequential-expects-1-inputs-but-it-received-48-input-tensors</guid>
      <pubDate>Wed, 14 Aug 2024 20:06:08 GMT</pubDate>
    </item>
    <item>
      <title>Android 中的 Movenet Singlepose 照明模型：“不支持的图像格式：1”错误</title>
      <link>https://stackoverflow.com/questions/78636622/movenets-singlepose-lighting-model-in-android-unsupported-image-format-1-e</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78636622/movenets-singlepose-lighting-model-in-android-unsupported-image-format-1-e</guid>
      <pubDate>Tue, 18 Jun 2024 09:42:32 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow：你的输入数据不足</title>
      <link>https://stackoverflow.com/questions/59864408/tensorflowyour-input-ran-out-of-data</link>
      <description><![CDATA[我正在研究 seq2seq keras/tensorflow 2.0 模型。每次用户输入某些内容时，我的模型都会完美地打印响应。但是在每个响应的最后一行我都会得到这个：

您：警告：tensorflow：您的输入数据不足；中断训练。确保您的数据集或生成器至少可以生成 steps_per_epoch * epochs 个批次（在本例中为 2 个批次）。您可能需要在构建数据集时使用 repeat() 函数。

“您：”是我的最后一个输出，在用户应该输入新内容之前。该模型运行良好，但我想没有错误永远是好事，但我不太明白这个错误。它说“中断训练”，但我没有训练任何东西，这个程序加载了一个已经训练过的模型。我猜这就是为什么错误没有停止程序的原因？
如果有帮助的话，我的模型如下所示：
intent_model = keras.Sequential([
keras.layers.Dense(8, input_shape=[len(train_x[0])]), # 输入层
keras.layers.Dense(8), # 隐藏层
keras.layers.Dense(len(train_y[0]),activation=&quot;softmax&quot;), # 输出层
])

intent_model.compile(optimizer=&quot;adam&quot;, loss=&quot;categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;])
intent_model.fit(train_x, train_y, epochs=epochs)

test_loss, test_acc = intent_model.evaluate(train_x, train_y)
print(&quot;测试的 Acc:&quot;, test_acc)

intent_model.save(&quot;models/intent_model.h5&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/59864408/tensorflowyour-input-ran-out-of-data</guid>
      <pubDate>Wed, 22 Jan 2020 16:40:42 GMT</pubDate>
    </item>
    <item>
      <title>如何使 RandomForestClassifier 更快？</title>
      <link>https://stackoverflow.com/questions/43640546/how-to-make-randomforestclassifier-faster</link>
      <description><![CDATA[我正在尝试使用大约有 1M 原始数据的 Twitter 情绪数据从 kaggle 网站实现词袋模型。我已经清理了它，但在最后一部分，当我将特征向量和情绪应用于随机森林分类器时，它花费了太多时间。这是我的代码...
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 100,verbose=3)
forest = forest.fit( train_data_features, train[&quot;Sentiment&quot;] )

train_data_features 是 1048575x5000 稀疏矩阵。我试图将其转换为数组，但执行时显示内存错误。
我哪里做错了？有人可以建议我一些来源或其他更快的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/43640546/how-to-make-randomforestclassifier-faster</guid>
      <pubDate>Wed, 26 Apr 2017 17:09:55 GMT</pubDate>
    </item>
    </channel>
</rss>