<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 20 Mar 2024 15:14:52 GMT</lastBuildDate>
    <item>
      <title>Keras 模型对于相同的文本输入每次都会预测不同的值</title>
      <link>https://stackoverflow.com/questions/78194276/keras-model-predicts-different-value-everytime-for-the-same-input-of-text</link>
      <description><![CDATA[我已经加载了一个预训练的 LSTM 模型，在使用该模型进行预测时，对于相同的文本输入，它每次都会给出不同的值，我提供了下面的代码片段，任何人都可以帮助我识别问题吗？
我还检查了stateful并将其设置为False
]]></description>
      <guid>https://stackoverflow.com/questions/78194276/keras-model-predicts-different-value-everytime-for-the-same-input-of-text</guid>
      <pubDate>Wed, 20 Mar 2024 14:49:54 GMT</pubDate>
    </item>
    <item>
      <title>如何从shap值中只得到重要的词？</title>
      <link>https://stackoverflow.com/questions/78194233/how-can-get-only-important-word-from-shap-value</link>
      <description><![CDATA[我只想获取文字和值，而不获取图表。
shap_values[:,:,1].abs.mean(0) 根据重要性提供“单词”。然而，代码给出了一个仅由数字组成的数组。如果您使用 shap.plots.bar(shap_values[:,:,1].abs.mean(0))，您可以看到单词。在没有图表的情况下，如何获得考虑到其重要性的“单词”？
!pip 安装数据集
从数据集导入load_dataset

数据集 = load_dataset(“imdb”)
df = 数据集[&#39;测试&#39;].to_pandas()
Short_data = [v[:500] for v in df[“text”][:20]]

从转换器导入 AutoTokenizer、AutoModelForSequenceClassification、管道
t1okenizer = AutoTokenizer.from_pretrained(“lvwerra/distilbert-imdb”)
m1odel = AutoModelForSequenceClassification.from_pretrained(“lvwerra/distilbert-imdb”)
分类器 = pipeline(&#39;文本分类&#39;, device=0,return_all_scores=True, model=m1odel,tokenizer=t1okenizer)
分类器（短数据[：10]）

导入形状
解释器 = shap.Explainer(分类器)
shap_values = 解释器(short_data[:20])
shap.plots.bar(shap_values[:,:,1].abs.mean(0))
]]></description>
      <guid>https://stackoverflow.com/questions/78194233/how-can-get-only-important-word-from-shap-value</guid>
      <pubDate>Wed, 20 Mar 2024 14:44:04 GMT</pubDate>
    </item>
    <item>
      <title>机器学习移动网V2</title>
      <link>https://stackoverflow.com/questions/78193353/machine-learning-mobile-net-v2</link>
      <description><![CDATA[如何实施渐进学习？
我正在寻找一种训练大型数据集的方法，因此我需要每次在一小部分上训练模型，同时保留之前的训练结果......
询问 Gpt 后：我尝试在训练新模型时将以前的权重加载到基础模型中
input_tensor = 输入(形状=(IMG_SIZE, IMG_SIZE, ColorChannels))
baseModel = MobileNetV2(pooling=&#39;avg&#39;, include_top=False, input_tensor=input_tensor)
baseModel.load_weights(PROJECT_DIR+“/ModelWeights.h5”，by_name = True，skip_mismatch = True)
对于 baseModel.layers 中的图层：
    可训练层 = False
headModel = baseModel.输出
headModel = Dense(1, 激活=“sigmoid”)(headModel)
模型=模型（输入=baseModel.input，输出=headModel，名称=&#39;new_model&#39;）

print(&quot;正在编译模型...&quot;)
model.compile(loss=“binary_crossentropy”,
                优化器=&#39;亚当&#39;,
                指标=[“准确度”])
]]></description>
      <guid>https://stackoverflow.com/questions/78193353/machine-learning-mobile-net-v2</guid>
      <pubDate>Wed, 20 Mar 2024 12:35:11 GMT</pubDate>
    </item>
    <item>
      <title>确定 RTX 4090 训练性能不佳的原因</title>
      <link>https://stackoverflow.com/questions/78192841/identifying-the-cause-of-poor-training-performance-of-rtx-4090</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78192841/identifying-the-cause-of-poor-training-performance-of-rtx-4090</guid>
      <pubDate>Wed, 20 Mar 2024 11:16:15 GMT</pubDate>
    </item>
    <item>
      <title>pytorch和cuda安装问题[重复]</title>
      <link>https://stackoverflow.com/questions/78192733/problem-with-pytorch-and-cuda-installation</link>
      <description><![CDATA[我正在尝试在 Windows 11 上使用 Anaconda3 安装带有 Cuda 的 PyTorch
我的nvidia-smi输出驱动程序版本：551.76，CUDA版本：12.4
我的火炬版本是我从官方网站安装的 12.2
&lt;前&gt;&lt;代码&gt;火炬2.2.1+cu121
火炬音频2.2.1+cu121
火炬视觉 0.17.1+cu121

但问题是，当我运行 torch.cuda.is_available() 时，它显示 false 作为输出。这里出了什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/78192733/problem-with-pytorch-and-cuda-installation</guid>
      <pubDate>Wed, 20 Mar 2024 10:58:20 GMT</pubDate>
    </item>
    <item>
      <title>ViT 模型的 HuggingFace Inference API 问题 - “图像特征提取”错误</title>
      <link>https://stackoverflow.com/questions/78192634/issue-with-huggingface-inference-api-for-vit-model-image-feature-extraction</link>
      <description><![CDATA[我的 Vision Transformer (ViT) 模型 rshrott/vit-base-renovation2 的推理 API 遇到问题。
https://huggingface.co/rshrott/vit-base-renovation2 
当我尝试使用 API 时，收到以下错误：
&lt;前&gt;&lt;代码&gt;{
“错误”：“HfApiJson（反序列化（错误（“未知变体图像特征提取，预期音频分类，音频到音频，音频源分离，自动语音识别，特征提取之一，文本分类、标记分类、问答、翻译、摘要、文本生成、text2text-生成、填充掩模、零样本分类、零样本图像分类、会话、表格问答、图像分类、图像分割、图像到文本、文本到语音、...视觉问答、视频分类、文档问答、图像到图像、深度估计，行：1 ，栏目：318）））”
}

有趣的是，当我直接在 Python 中使用 Transformer 管道时，模型按预期工作：
从转换器导入管道
从 PIL 导入图像
导入请求

管道=管道（模型=“rshrott/vit-base-renovation2”）
url = &#39;https://example.com/image.jpeg&#39;
图像= Image.open(requests.get(url,stream=True).raw)
preds = 管道(图像)

此代码运行没有任何问题并返回预期的预测。但是，通过推理 API 使用同一模型时会遇到错误。我怀疑可能存在与预期任务类型相关的配置问题，但我不确定如何解决它。
为什么会出现此错误以及如何修复它？我已经检查了型号卡和配置，但我似乎无法找到“图像特征提取”的来源或原因。]]></description>
      <guid>https://stackoverflow.com/questions/78192634/issue-with-huggingface-inference-api-for-vit-model-image-feature-extraction</guid>
      <pubDate>Wed, 20 Mar 2024 10:44:57 GMT</pubDate>
    </item>
    <item>
      <title>用于在本地部署 70B 参数语言模型以服务 200 个并发用户的最佳基础设施设置 [关闭]</title>
      <link>https://stackoverflow.com/questions/78192362/optimal-infrastructure-setup-for-deploying-a-70b-parameter-language-model-locall</link>
      <description><![CDATA[我有一个由 OLLMA 提供的大型语言模型 (LLM)，包含大约700 亿个参数。我希望在本地部署此模型，以服务最多 200 个并发用户。考虑到有效运行此类模型的繁重计算要求，我正在探索设置必要基础设施的选项。
我应该选择虚拟机 (VM) 还是在本地部署模型？此外，我希望获得有关系统配置的建议，包括 RAM、GPU、存储容量和处理器等规格。
我在没有 GPU 系统的情况下尝试过此操作，但它不起作用。
我想运行700亿参数，为最多200个并发用户提供服务。]]></description>
      <guid>https://stackoverflow.com/questions/78192362/optimal-infrastructure-setup-for-deploying-a-70b-parameter-language-model-locall</guid>
      <pubDate>Wed, 20 Mar 2024 10:01:24 GMT</pubDate>
    </item>
    <item>
      <title>从波形中提取峰值和平均面积[关闭]</title>
      <link>https://stackoverflow.com/questions/78190852/peak-and-mean-area-extraction-from-a-waveform</link>
      <description><![CDATA[在此处输入图像描述是否有任何方法可以提取峰值和平均值从当前波形时间序列中提取&gt;
请帮忙。在此处输入图像描述
我尝试手动完成，但数据非常庞大。那么有没有人工智能工具或任何其他方法可以做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78190852/peak-and-mean-area-extraction-from-a-waveform</guid>
      <pubDate>Wed, 20 Mar 2024 04:27:02 GMT</pubDate>
    </item>
    <item>
      <title>对同一数据集的不同子组进行迁移学习[关闭]</title>
      <link>https://stackoverflow.com/questions/78190629/transfer-learning-on-different-subgroups-of-the-same-dataset</link>
      <description><![CDATA[我正在尝试根据回归任务的特定列中的值将原始数据集分为 6 个子组。每个子组中目标变量的分布非常相似。我的目标是通过首先对 5 个子组进行预训练，然后对最后一个子组进行微调来应用迁移学习。
对于预训练，我为每个子组设置了单独的训练、验证和测试集。预训练包括将5个子组的训练集和验证集结合起来，用它们来训练模型，验证模型，然后测量测试集上的损失。
随后，我使用预训练中的模型权重，并仅使用最终子组的训练集和验证集进行微调，并再次测量测试集上的损失。
但是，我遇到了一个问题，即我的模型对预训练数据过度拟合，导致微调过程中第一个周期的提前停止，因为微调集的验证误差会增加。
我的方法正确吗？]]></description>
      <guid>https://stackoverflow.com/questions/78190629/transfer-learning-on-different-subgroups-of-the-same-dataset</guid>
      <pubDate>Wed, 20 Mar 2024 02:45:43 GMT</pubDate>
    </item>
    <item>
      <title>我的 scikit-learn 代码序列正确吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78187495/is-my-scikit-learn-code-sequence-correct</link>
      <description><![CDATA[我已经构建了一个包含一些转换的管道并训练了一个 SVC 分类器。代码中构建模型的步骤顺序是否正确？ n次交叉验证效率较低。
我正在使用此处找到的processed.cleveland.data数据集：https： //archive.ics.uci.edu/dataset/45/heart+disease。
将 pandas 导入为 pd
将 numpy 导入为 np
导入操作系统
从 pathlib 导入路径

从 sklearn.model_selection 导入 train_test_split
从 sklearn.model_selection 导入 StratifiedKFold
从 sklearn.model_selection 导入 cross_val_score

从 sklearn.compose 导入 ColumnTransformer
从 sklearn.pipeline 导入管道
从 sklearn.preprocessing 导入 OneHotEncoder
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.impute 导入 SimpleImputer

从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.svm 导入 SVC
url =“C:/Users/.../processedcleveland.data”
名称 = [&#39;年龄&#39;, &#39;性别&#39;, &#39;cp&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;exang&#39;, &#39;oldpeak&#39;, &#39;slope&#39;, &#39;ca&#39; , &#39;thal&#39;, &#39;num&#39;]
def getData():
        返回 pd.read_csv(url, sep=&#39;,&#39;, 名称=名称)

输入 = 获取数据()
打印（输入.info（））
打印（输入.描述（））

数组=输入.值
X = 数组[:,0:13]
y = 数组[:,13]

dataframe = pd.DataFrame.from_records(X)
数据帧[[1, 2, 5, 6, 8]] = 数据帧[[1, 2, 5, 6, 8]].astype(str)

打印(dataframe.info())

numeric_ix = dataframe.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns
categorical_ix = dataframe.select_dtypes(include=[&#39;object&#39;, &#39;bool&#39;]).columns

打印（数字_ix）
打印（分类_ix）
&#39;&#39;&#39;
t = [(&#39;cat0&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;), [1, 2, 5, 6, 8]), (&#39;cat1&#39;, OneHotEncoder(), categorical_ix), (&#39;num0&#39;, SimpleImputer(strategy) =&#39;中位数&#39;), numeric_ix), (&#39;num1&#39;, MinMaxScaler(), numeric_ix)]
col_transform = ColumnTransformer(变压器=t)

管道 = 管道(步骤=[(&#39;t&#39;, col_transform)])
# 将管道拟合到转换后的数据上
结果 = pipeline.fit_transform(dataframe)

打印（类型（pd.DataFrame.from_records（结果）））
打印（pd.DataFrame.from_records（结果）.to_string（））
&#39;&#39;&#39;
X_train、X_validation、Y_train、Y_validation = train_test_split(X、y、test_size=0.20、random_state=1)


categorical_impute = 管道([
    （“mode_impute”，SimpleImputer（missing_values = np.nan，策略=&#39;most_frequent&#39;）），
    (“one_hot”, OneHotEncoder())
]）

numeric_impute = 管道([
    （“num_mode_impute”，SimpleImputer（missing_values = np.nan，策略=&#39;中位数&#39;）），
    (“min_max”, StandardScaler())
]）

预处理器 = ColumnTransformer([
    (“cat_impute”, categorical_impute, categorical_ix),
    (“num_impute”, numeric_impute, numeric_ix)
]，余数=“直通”）


模型 = SVC(伽玛=&#39;自动&#39;)

kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)

pipeline = Pipeline(steps=[(&#39;prep&#39;, 预处理器), (&#39;m&#39;, model)])

cv_results = cross_val_score(管道, X_train, Y_train, cv=kfold, 评分=&#39;准确度&#39;)
print(&#39;%s: %f (%f)&#39; % (&quot;SVC: &quot;, cv_results.mean(), cv_results.std()))
# 结果 = preprocessor.fit_transform(dataframe)
# print(pd.DataFrame.from_records(结果).to_string())

如上所述，分类器的效率非常低。顺序有问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78187495/is-my-scikit-learn-code-sequence-correct</guid>
      <pubDate>Tue, 19 Mar 2024 14:38:47 GMT</pubDate>
    </item>
    <item>
      <title>部分依赖图 - 使用缩放数据开发的模型，如何取消 PDP 缩放？</title>
      <link>https://stackoverflow.com/questions/78167199/partial-dependence-plot-model-developed-using-scaled-data-how-to-unscale-for</link>
      <description><![CDATA[我已经用Python制作了一个随机森林分类器模型，现在想要制作部分依赖图（PDP）。我使用缩放数据来训练和测试模型，并使 PDP 如下所示：
PartialDependenceDisplay.from_estimator(best_clf, X_test_final, best_features)。但是，x 轴值经过缩放，这限制了可解释性。
在调用 PartialDependenceDisplay 之前取消缩放数据 X_test_final 不起作用，有关如何将 x 轴值从缩放更改为未缩放的任何建议？我已使用 StandardScaler() 缩放了我的数据。]]></description>
      <guid>https://stackoverflow.com/questions/78167199/partial-dependence-plot-model-developed-using-scaled-data-how-to-unscale-for</guid>
      <pubDate>Fri, 15 Mar 2024 13:15:52 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“sklearn.neighbors._base”导入名称“_check_weights”</title>
      <link>https://stackoverflow.com/questions/75633185/importerror-cannot-import-name-check-weights-from-sklearn-neighbors-base</link>
      <description><![CDATA[我正在尝试将 Missforest 作为处理表数据中缺失值的方法。
导入sklearn
打印（sklearn.__version__）
-&gt;1.2.1

导入 sklearn.neighbors._base
导入系统
sys.modules[&#39;sklearn.neighbors.base&#39;] = sklearn.neighbors._base

!pip 安装缺少的py
从missingpy导入MissForest

到目前为止一切正常，但从昨天开始，出现了以下错误消息。
导入错误：无法从“sklearn.neighbors._base”导入名称“_check_weights”

我想知道如何处理这个错误。]]></description>
      <guid>https://stackoverflow.com/questions/75633185/importerror-cannot-import-name-check-weights-from-sklearn-neighbors-base</guid>
      <pubDate>Sat, 04 Mar 2023 01:48:43 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 生存模型</title>
      <link>https://stackoverflow.com/questions/75010397/xgboost-survival-model</link>
      <description><![CDATA[我正在尝试开发 XGBoost 生存模型。这是我的代码的快速快照：
X = df_High_School[[&#39;Gender&#39;, &#39;Lived_both_Parents&#39;, &#39;Moth_Born_in_Canada&#39;, &#39;Father_Born_in_Canada&#39;,&#39;Born_in_Canada&#39;,&#39;Aboriginal&#39;,&#39;Visible_Minority&#39;]] # 协变量
y = df_High_School[[&#39;time_to_event&#39;, &#39;event&#39;]] # 事件发生时间和事件指示器

#将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#开发模型
model = xgb.XGBRegressor(objective=&#39;survival:cox&#39;)

它给了我以下错误：
&lt;块引用&gt;
&lt;小时/&gt;

ValueError Traceback（最近一次调用最后一次）
 在
18
19 # 将模型拟合到训练数据
---&gt; 20 model.fit(X_train, y_train)
21
22 # 对测试集进行预测
2帧
_maybe_pandas_label（标签）中的/usr/local/lib/python3.8/dist-packages/xgboost/core.py
261 if isinstance（标签，DataFrame）：
[262] 第 262 章1：
--&gt; 263 raise ValueError（&#39;标签的数据帧不能有多列&#39;）
264
[第 265 章]
ValueError：标签的 DataFrame 不能有多列
由于这是一个生存模型，我需要两列 t 来指示事件和 time_to_event。我还尝试将 Dataframes 转换为 Numpy，但它也不起作用。
有什么线索吗？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/75010397/xgboost-survival-model</guid>
      <pubDate>Wed, 04 Jan 2023 19:32:04 GMT</pubDate>
    </item>
    <item>
      <title>如何使用带有灰度图像的预训练神经网络？</title>
      <link>https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images</link>
      <description><![CDATA[我有一个包含灰度图像的数据集，我想在它们上训练最先进的 CNN。我非常想微调预训练模型（例如 &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/slim#Pretrained&quot; rel=&quot;noreferrer ”在这里&lt;/a&gt;)。
问题是，我能找到权重的几乎所有模型都在包含 RGB 图像的 ImageNet 数据集上进行了训练。
我无法使用其中一个模型，因为它们的输入层需要一批形状 (batch_size, height, width, 3) 或 (64, 224, 224, 3)  就我而言，但我的图像批次是 (64, 224, 224)。
有什么方法可以使用这些模型之一吗？我曾想过在加载权重后删除输入层并添加我自己的权重（就像我们对顶层所做的那样）。这种做法正确吗？]]></description>
      <guid>https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images</guid>
      <pubDate>Fri, 24 Aug 2018 00:33:04 GMT</pubDate>
    </item>
    <item>
      <title>sklearn 中的 TfidfVectorizer 如何专门包含单词</title>
      <link>https://stackoverflow.com/questions/19753945/tfidfvectorizer-in-sklearn-how-to-specifically-include-words</link>
      <description><![CDATA[我对 TfidfVectorizer 有一些疑问。
我不清楚这些词是如何选择的。我们可以提供最低支持，但在那之后，什么将决定选择哪些功能（例如，更高的支持更多机会）？如果我们说 max_features = 10000，我们总是得到相同的结果吗？如果我们说 max_features = 12000，我们会得到相同的 10000 特征，但额外添加 2000 吗？ 
此外，有没有办法扩展例如 max_features=20000 功能？我将它放在一些文本上，但我知道一些肯定应该包含的单词，还有一些表情符号“:-)”等。如何将这些添加到 TfidfVectorizer 对象中，以便它将可以使用该对象，用它来拟合和预测
to_include = [&quot;:-)&quot;, &quot;:-P&quot;]
方法 = TfidfVectorizer(max_features=20000, ngram_range=(1, 3),
                      # 我知道停用词，但是包含单词怎么样？
                      stop_words=test.stoplist[:100],
                      # 包含单词 ??
                      分析器=&#39;词&#39;,
                      min_df=5)
方法.fit(训练数据)

寻求结果：
X = method.transform(traindata)
X
”的稀疏矩阵
 以压缩稀疏行格式存储了 1135520 个元素&gt;]，
 其中 N 是样本大小
]]></description>
      <guid>https://stackoverflow.com/questions/19753945/tfidfvectorizer-in-sklearn-how-to-specifically-include-words</guid>
      <pubDate>Sun, 03 Nov 2013 14:19:46 GMT</pubDate>
    </item>
    </channel>
</rss>