<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 31 Aug 2024 09:16:04 GMT</lastBuildDate>
    <item>
      <title>TensorFlow 验证拆分导致 75% 的类别过度拟合，25% 的类别为随机噪声</title>
      <link>https://stackoverflow.com/questions/78934378/tensorflow-validation-split-leading-to-75-of-classes-overfitted-25-random-noi</link>
      <description><![CDATA[我正在执行机器学习任务，其中验证分割似乎存在数据泄漏。如果我调整分割，它会移动过度拟合的类的数量。
X_train = resize_images(X_train, (512, 512))
y_train = label_enc.transform(y_train)
n_classes = len(list(label_enc.classes_))
X_train = np.asarray(X_train)
X_train, y_train = add_extra_dim(X_train, y_train, n_classes)
model = build_model()
model.fit(X_train, y_train, epochs=50, validation_split = 0.25, callbacks=[callback])
def build_model():
model = ATCNet(shape=(100, 100, 1), n_classes=n_classes)
top3 = tf.keras.metrics.TopKCategoricalAccuracy(k=3, name=&quot;top3&quot;)
top5 = tf.keras.metrics.TopKCategoricalAccuracy(k=5, name=&quot;top5&quot;)
model.compile(optimizer=Adam(), 
loss=&#39;categorical_crossentropy&#39;, 
metrics=[&#39;accuracy&#39;, top3, top5])
返回模型

[[输入图片描述在此输入图片描述在此描述](https://i.sstatic.net/nS2vvCJP.png)](https://i.sstatic.net/oCSLgoA4.png)
我已测试，不存在任何数据泄露。
我已将问题隔离到验证拆分，但我找不到任何有关如何修复它或我做错了什么的信息]]></description>
      <guid>https://stackoverflow.com/questions/78934378/tensorflow-validation-split-leading-to-75-of-classes-overfitted-25-random-noi</guid>
      <pubDate>Sat, 31 Aug 2024 05:41:32 GMT</pubDate>
    </item>
    <item>
      <title>transformers/LLM 与 pytorch 内存不足</title>
      <link>https://stackoverflow.com/questions/78934229/transformers-llm-with-pytorch-running-out-of-memory</link>
      <description><![CDATA[我改编了 https://pub.towardsai.net/build-your-own-large-language-model-llm-from-scratch-using-pytorch-9e9945c24858 中的转换器，用于将另一种语言翻译成我自己的两种形式语言（每种语言的词汇量为 23）。

最大输入长度约为 600，最大输出长度约为 3000，因此我选择了略大于 3000 的联合序列长度。
训练和验证的批量大小均为 1。
嵌入维度为 8，前馈维度为 32，头数为 8，块数为 6。
数据类型为 int64。

使用这些超参数，训练在我的 CPU 上使用 ~15 GiB RAM。一旦训练的批处理大小 &gt; 1，它就会尝试分配超过 40 GiB 并崩溃。因此，在 Colab 上进行免费训练是不可行的。

我做错了什么？我改编的转换器具有更大的超参数和词汇，网站声称可以在 Colab 上对其进行训练。也许其中一个原因是我的文本太长了？（我无法轻松地在测试较短的文本上进行训练，因为我的数据集不包含很多文本。）
如何减少内存占用？
]]></description>
      <guid>https://stackoverflow.com/questions/78934229/transformers-llm-with-pytorch-running-out-of-memory</guid>
      <pubDate>Sat, 31 Aug 2024 03:08:19 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch：实现批量损失（RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn）</title>
      <link>https://stackoverflow.com/questions/78934120/pytorch-implementing-batch-loss-runtimeerror-element-0-of-tensors-does-not-re</link>
      <description><![CDATA[我目前正在训练一个自动编码器，针对我的具体用例，我在 PyTorch 中实现了基于 Wasserstein 度量（地球移动度量）的自定义损失。我使用这个的一般想法是，下面代码片段中的 pred 和 target 是一批向量，比如 batch_size x vector_length
def wasserstein_distance(pred, target, num_bins=200):
# 获取列数（特征）
num_columns = pred.size(1)

# 计算每列的 Wasserstein 距离
distances = []
for i in range(num_columns):
# 获取列的预测值和目标值
pred_col = pred[:, i]
target_col = target[:, i]

# 确定列的直方图范围
min_val = min(pred_col.min().item(), target_col.min().item())
max_val = max(pred_col.max().item(), target_col.max().item())

# 计算预测值和目标值的直方图
pred_hist = torch.histc(pred_col, bins=num_bins, min=float(min_val), max=float(max_val))
target_hist = torch.histc(target_col, bins=num_bins, min=float(min_val), max=float(max_val))

# 将直方图标准化以形成概率分布
pred_hist /= pred_hist.sum()
target_hist /= target_hist.sum()

# 计算累积分布函数 (CDF)
pred_cdf = torch.cumsum(pred_hist, dim=0)
target_cdf = torch.cumsum(target_hist, dim=0)

# 计算 Wasserstein 距离（地球移动距离）
wasserstein_dist = torch.sum(torch.abs(pred_cdf - target_cdf))

distances.append(wasserstein_dist)

# 计算所有列的 Wasserstein 距离的平均值
mean_wasserstein_distance = torch.mean(torch.stack(distances))

return mean_wasserstein_distance

我在以下训练函数中使用它：
def train_model(model: nn.Module, data_loader: torch.utils.data.DataLoader, epoch_count: int, learning_rate: float) -&gt; np.ndarray:
print(&quot;##### 开始训练模型 #####&quot;)
model.train()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model.to(device)
loss = []

for epoch in range(epoch_count):
total_loss = 0.0
total_samples = 0

for batch in data_loader:
x = batch[0]
x = x.to(device)
optimizer.zero_grad()
pred_ae = model(x)
loss_ae = wasserstein_distance(pred_ae, x[:, 6:])

loss_ae.backward()
optimizer.step()

total_loss += loss_ae.item()
total_samples += x.size(0)

avg_loss = total_loss / total_samples
loss.append(avg_loss)

print(f&#39;Epoch: {epoch} Loss per unit: {avg_loss}&#39;)

print(&quot;##### FINISHED TRAINING OF MODEL #####&quot;)
return model, np.array(losses)


但是，我收到以下错误：
File &quot;&quot;, line 432, in &lt;module&gt;
model,losses = train_model(model,data,50,0.0001)
文件&quot;&quot;，第 237 行，在 train_model 中
loss_ae.backward()
文件&quot;C:\Users\aksha\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_tensor.py&quot;，第 522 行，在 Backward 中
torch.autograd.backward(
文件&quot;C:\Users\aksha\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\autograd\__init__.py&quot;，第 266 行，在 Backward 中
Variable._execution_engine.run_backward(# 调用 C++ 引擎运行反向传递
RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn


我确信每个 requires_grad 默认为 true（我没有手动更改任何内容）。我对 PyTorch 有点陌生，但我怀疑问题在于我为损失返回了一个值，而 Backward 期望每个张量都有一个值？？？但这不是我真正想要的训练方案，我希望每个更新都是特定于批次的（具体来说，学习每个批次中每列的分布）
如能提供帮助，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78934120/pytorch-implementing-batch-loss-runtimeerror-element-0-of-tensors-does-not-re</guid>
      <pubDate>Sat, 31 Aug 2024 01:05:47 GMT</pubDate>
    </item>
    <item>
      <title>在 colab 上安装 nvstrings</title>
      <link>https://stackoverflow.com/questions/78933173/installing-nvstrings-on-colab</link>
      <description><![CDATA[我尝试在 Google Colab 笔记本上运行 !pip install nvstrings，但遇到了错误
收集 nvstrings
使用缓存的 nvstrings-0.6.1.post1.tar.gz (1.2 kB)
准备元数据 (setup.py) ... 完成
为收集的包构建 wheel：nvstrings
错误：subprocess-exited-with-error

× python setup.py bdist_wheel 未成功运行。
│ 退出代码：1
╰─&gt; 请参阅上面的输出。

注意：此错误源自子进程，可能不是 pip 的问题。
为 nvstrings 构建 wheel (setup.py) ... 错误
错误：为 nvstrings 构建 wheel 失败
运行 setup.py clean 以清除 nvstrings
构建 nvstrings 失败
错误：错误：无法为某些基于 pyproject.toml 的项目 (nvstrings) 构建可安装的 wheel

如何克服这个问题？
我正在使用托管在 colab 免费层 gpu 上的 spacy en_core_web_trf 开发文档解析器模型，它需要 gpu 设备上的输入字符串，即为什么我需要安装 nvstrings 库。我已经在笔记本电脑上安装了 nvidia RAPIDS 库，但它似乎不包含 nvstrings 库。]]></description>
      <guid>https://stackoverflow.com/questions/78933173/installing-nvstrings-on-colab</guid>
      <pubDate>Fri, 30 Aug 2024 17:30:48 GMT</pubDate>
    </item>
    <item>
      <title>在 Microsoft Ai Hub 上应使用哪种 RAG 架构和流程来从超大文档生成内容</title>
      <link>https://stackoverflow.com/questions/78932671/what-rag-architecture-and-process-to-utilise-on-microsoft-ai-hub-to-generate-con</link>
      <description><![CDATA[我正在研究一个用例，旨在自动生成响应，该响应是对提案请求 (RFP) 的回复 - 该响应回答了请求中的问题，并且实际上包含了有关我们公司的详细信息。
外部企业首先向我们提供请求，目前人工需要花费很长时间来编写响应，响应可能长达 90 页。
在理想情况下，我会接受请求，然后自动回复 90 页，但目前没有一个 LLM 可以做到这一点。
Microsoft 建议在使用 RAG 之前将之前的响应拆分成小块，以便可以检索和生成所有信息。但我担心不同部分之间会丢失上下文和理解。
有人遇到过类似的问题吗？你是如何设计这个解决方案的？现在，我发现有必要给这些回复贴上标签，然后将它们分成小节，为每个小节生成内容并将它们缝合在一起（对请求也做类似的事情，但这些内容不太广泛，通常只有 3-4 页长）。
我使用 Microsoft Ai Hub 和 Prompt Flows 来完成大部分工作。]]></description>
      <guid>https://stackoverflow.com/questions/78932671/what-rag-architecture-and-process-to-utilise-on-microsoft-ai-hub-to-generate-con</guid>
      <pubDate>Fri, 30 Aug 2024 15:02:23 GMT</pubDate>
    </item>
    <item>
      <title>Mask r-cnn 模型未收敛，训练和验证的准确度在 NaN 和 0.09 之间波动[关闭]</title>
      <link>https://stackoverflow.com/questions/78932442/mask-r-cnn-model-not-converging-and-accuracies-for-training-and-validation-are-o</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78932442/mask-r-cnn-model-not-converging-and-accuracies-for-training-and-validation-are-o</guid>
      <pubDate>Fri, 30 Aug 2024 14:09:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 NEAT-Python 库重新利用训练的神经网络</title>
      <link>https://stackoverflow.com/questions/78932393/repurposing-neural-network-trained-using-the-neat-python-library</link>
      <description><![CDATA[我正在使用 NEAT-Python 库编写一个程序，以训练 AI 代理在 Python 中玩贪吃蛇游戏。训练后，我想导出网络，然后使用它来可视化它如何用另一种语言（例如 JavaScript）做出决策。是否可以导出网络，然后在 Python 之外使用它？我尝试查看 Uber Research 的 PyTorch-NEAT 库，但它并不是我想要的。]]></description>
      <guid>https://stackoverflow.com/questions/78932393/repurposing-neural-network-trained-using-the-neat-python-library</guid>
      <pubDate>Fri, 30 Aug 2024 13:57:57 GMT</pubDate>
    </item>
    <item>
      <title>如何使用机器学习来找到模式客户资料？[关闭]</title>
      <link>https://stackoverflow.com/questions/78932302/how-to-use-machine-learning-to-find-the-pattern-customer-profile</link>
      <description><![CDATA[我有一个数据集，其中包含从一家虚构公司购买产品的客户的个人特征。最初，我没有任何目标变量，只有他们的特征。我的目标是找到一种模式，该模式不一定是每列中最常见的特征。例如，是否可以使用 RandomForest 来做到这一点？或者我应该使用其他技术？
数据集具有类似于以下的结构。这些列都是 object 格式，并且有一些 NaN 值表示为 &#39;Blank&#39;:
日期姓名薪资职位年龄
&#39;05/10/2023&#39; &#39;Daniel&#39; &#39;10,000&#39; &#39;IT&#39; 32
&#39;05/12/2024&#39; &#39;John&#39; &#39;9,000&#39; &#39;Blank&#39; 27
&#39;03/01/2023&#39; &#39;Niel&#39; &#39;Blank&#39; &#39;数据科学家&#39; 21
&#39;03/01/2023&#39; &#39;Isa&#39; &#39;10,000&#39; &#39;工程师&#39; 51
&#39;05/10/2023&#39; &#39;Ana&#39; &#39;11,000&#39; &#39;数据科学家&#39; 52
&#39;05/12/2024&#39; &#39;Ian&#39; &#39;9,500&#39; &#39;Doctor&#39; 48
&#39;03/01/2023&#39; &#39;Fred&#39; &#39;Blank&#39; &#39;IT&#39; 21
&#39;03/01/2023&#39; &#39;Carol&#39; &#39;15,000&#39; &#39;Blank&#39; 30

我正在考虑返回输出，例如，说明构成最标准配置文件的特征，例如：
最标准的配置文件是：薪水 x、职位 y 和年龄 z。

我考虑过使用聚类，但我不认为这是最好的方法（例如，薪水的输出是一个简单的平均值）。我认为最好的方法是创建一个可能并不一定存在的配置文件，并且基于研究每个变量（薪水、职位和年龄）的模式。
# 编码分类变量
df[&#39;Position&#39;] = pd.Categorical(df[&#39;Position&#39;]).codes

# 执行聚类
kmeans = KMeans(n_clusters=1, random_state=42)
kmeans.fit(df[[&#39;Salary&#39;, &#39;Position&#39;, &#39;Age&#39;]])

# 获取聚类的质心
centroid = kmeans.cluster_centers_[0]

有没有更好的方法？NLP 或 RandomForest 是一种选择吗？]]></description>
      <guid>https://stackoverflow.com/questions/78932302/how-to-use-machine-learning-to-find-the-pattern-customer-profile</guid>
      <pubDate>Fri, 30 Aug 2024 13:39:34 GMT</pubDate>
    </item>
    <item>
      <title>zero123 的更大分辨率输出</title>
      <link>https://stackoverflow.com/questions/78932261/bigger-resolution-output-of-zero123</link>
      <description><![CDATA[我在 InstantMesh 环境中使用 zero123，我想知道 zero123 是否有可能输出更大分辨率的图像？
目前的分辨率是 320x320，对于从 InstantMesh 的 3D 重建管道获得良好的输出纹理来说，这个分​​辨率有点低。]]></description>
      <guid>https://stackoverflow.com/questions/78932261/bigger-resolution-output-of-zero123</guid>
      <pubDate>Fri, 30 Aug 2024 13:30:55 GMT</pubDate>
    </item>
    <item>
      <title>使用自己的多视图图像绕过 zero123 来增强 InstantMesh 3d 重建输出的纹理 [关闭]</title>
      <link>https://stackoverflow.com/questions/78931872/bypass-zero123-with-own-multiview-images-to-enhance-texture-of-instantmesh-3d-re</link>
      <description><![CDATA[我正在使用 InstantMesh，这是一个使用多视图模型 (zero123) 的 3D 重建管道，该模型可从一张输入图像生成多视图图像。
我和我的团队正在尝试增强 instantmesh 3D 重建纹理输出，经过多次尝试，我们发现最大的问题之一是 zero123 输出（输入重建管道）的分辨率太低。
我们现在的目标是使用我们自己的分辨率更高的多视图图像。
我现在的问题是：InstantMesh 重建管道是否接受更大的分辨率？如果是，代码中需要更改什么？如果没有，我们是否必须重新训练整个模型以考虑更大的分辨率？]]></description>
      <guid>https://stackoverflow.com/questions/78931872/bypass-zero123-with-own-multiview-images-to-enhance-texture-of-instantmesh-3d-re</guid>
      <pubDate>Fri, 30 Aug 2024 12:00:18 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hugging Face Transformers 训练 GPT-2 模型时如何修复分段错误？</title>
      <link>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</guid>
      <pubDate>Tue, 06 Aug 2024 21:47:06 GMT</pubDate>
    </item>
    <item>
      <title>了解 Vits 对 HiFi-GAN 的使用</title>
      <link>https://stackoverflow.com/questions/78625475/understanding-usage-of-hifi-gan-by-vits</link>
      <description><![CDATA[我正在（尝试）学习语音合成的 AI/ML，并尝试理解 Vits 如何使用 HiFi-GAN。
据我所知，Vits 会将文本输入转换为梅尔频谱图，然后由 HiFi-GAN 转换为音频波。
让我困惑的是为什么从 Vits 发送到 HiFi-GAN 的输入不是梅尔频谱图。
例如，当我测试其他模型并将以下代码添加到 HiFi-GAN 的正向方法时：
class Generator(torch.nn.Module):
...
def forward(self, x):
plot_spectrogram(x[0].cpu().detach().numpy(), &quot;mel_spec.png&quot;)
...
...

它保存了正确的图像，看起来像梅尔频谱图图像，但是，当我对 vits 执行相同操作时，保存的图像是纯绿色图像，当然不是梅尔频谱图的表示。
但生成的音频文件当然是有效的音频文件。
有人能向我解释一下吗？
我正在评估一些神经 tts 模型，我想要做的是保存由模型创建的梅尔频谱图，以便稍后进行比较，并通过不同的声码器运行它们以进行比较。
我注意到 vits repo 中的 HiFi-GAN 代码与原始 repo 略有不同，但我无法理解为什么。
有什么方法可以将输入参数 x 转换为梅尔频谱图表示，而无需先将其转换为音频，然后再将音频转换为梅尔？]]></description>
      <guid>https://stackoverflow.com/questions/78625475/understanding-usage-of-hifi-gan-by-vits</guid>
      <pubDate>Sat, 15 Jun 2024 02:17:36 GMT</pubDate>
    </item>
    <item>
      <title>我的神经网络在测试数据上的表现很差，但在训练数据上准确率却很高，这是什么原因造成的？</title>
      <link>https://stackoverflow.com/questions/75902400/what-can-be-the-cause-of-poor-performance-of-my-neural-network-on-testing-data</link>
      <description><![CDATA[我尝试开发一个简单的模型来解决多分类问题。我在 Kaggle 上找到了一个数据集，其中包含 25k+ 条带有文本情绪（积极、中性、消极）的推文。我处理了这些数据并提出了一个简单的网络模型。该网络在训练数据上的准确率约为 98-99%，但在测试数据上的准确率约为 60%。造成这种性能差异的原因可能是什么？我该如何优化模型以获得更好的评估性能？
代码如下：
df = pd.read_csv(&#39;Tweets.csv&#39;);

df = df.drop(columns=[&#39;textID&#39;, &#39;selected_text&#39;])

data = df[&#39;text&#39;]
labels = df[&#39;sentiment&#39;]

labels = np.unique(labels, return_inverse=True)
lookup = labels[0]
labels = labels[1]

data = np.array(data).astype(str)
tokenizer = keras_preprocessing.text.Tokenizer(num_words=10000)
tokenizer.fit_on_texts(data)
sequences = tokenizer.texts_to_sequences(data)
one_hot_results = tokenizer.texts_to_matrix(data, mode=&#39;binary&#39;)

all_tweets = one_hot_results[:20000]
all_labels =标签[:20000]

train_data = all_tweets[:10000]
test_data = all_tweets[10000:]

train_labels = all_labels[:10000]
test_labels = all_labels[10000:]

model = models.Sequential()
model.add(layers.Dense(64, 激活=&#39;relu&#39;, 输入形状=(10000, )))
model.add(layers.Dense(64, 激活=&#39;relu&#39;))
model.add(layers.Dense(3, 激活=&#39;softmax&#39;))
model.compile(优化器=&#39;rmsprop&#39;, 损失=&#39;sparse_categorical_crossentropy&#39;, 指标=[&#39;accuracy&#39;])

model.fit(train_data, train_labels, batch_size=512, epochs=20, validation_split=0.3)

test_loss, test_acc = model.evaluate(test_data, test_labels)
print(&quot;test_acc:&quot;, test_acc)

这是我的控制台输出：
Epoch 19/20
14/14 [===============================] - 0s 14ms/step - loss: 0.0423 - accuracy: 0.9903 - val_loss: 1.9351 - val_accuracy: 0.6063
Epoch 20/20
14/14 [================================] - 0s 14ms/step - loss: 0.0365 - 准确度：0.9920 - val_loss：2.0434 - val_accuracy：0.6013
313/313 [==============================] - 1s 3ms/step - 损失：1.9856 - 准确度：0.6086
test_acc：0.6086000204086304

我尝试过改变层数、层大小、批次大小、时期数，调整矢量化测试数据的词汇量，但似乎没有什么能改善评估。]]></description>
      <guid>https://stackoverflow.com/questions/75902400/what-can-be-the-cause-of-poor-performance-of-my-neural-network-on-testing-data</guid>
      <pubDate>Fri, 31 Mar 2023 20:09:00 GMT</pubDate>
    </item>
    <item>
      <title>残差图诊断以及如何改进回归模型</title>
      <link>https://stackoverflow.com/questions/62459677/residual-plot-diagnostic-and-how-to-improve-the-regression-model</link>
      <description><![CDATA[在为此住房数据集创建回归模型时，我们可以绘制实值函数中的残差。
from sklearn.linear_model import LinearRegression

X = housing[[&#39;lotsize&#39;]]
y = housing[[&#39;price&#39;]]

model = LinearRegression()
model.fit(X, y)

plt.scatter(y,model.predict(X)-y)


我们可以清楚地看到差异（预测 - 实际值）对于较低的价格主要为正，而对于较高的价格，差异为负。
对于线性回归来说也是如此，因为模型针对 RMSE 进行了优化（因此不考虑残差的符号）。
但是在执行 KNN 时
from sklearn.neighbors import KNeighborsRegressor
model = KNeighborsRegressor(n_neighbors = 3)

我们可以找到类似的图。

在这种情况下，我们可以给出什么解释，我们如何改进模型。
编辑：我们可以使用所有其他预测因子，结果类似。
housing = housing.replace(to_replace=&#39;yes&#39;, value=1, regex=True)
housing = housing.replace(to_replace=&#39;no&#39;, value=0, regex=True)
X = housing[[&#39;lotsize&#39;,&#39;bedrooms&#39;,&#39;stories&#39;,&#39;bathrms&#39;,&#39;bathrms&#39;,&#39;driveway&#39;,&#39;recroom&#39;,
&#39;fullbase&#39;,&#39;gashw&#39;,&#39;airco&#39;,&#39;garagepl&#39;,&#39;prefarea&#39;]]

下图为具有 3 个邻居的 KNN。如果有 3 个邻居，人们会预期过度拟合，我不明白为什么会有这种趋势。
]]></description>
      <guid>https://stackoverflow.com/questions/62459677/residual-plot-diagnostic-and-how-to-improve-the-regression-model</guid>
      <pubDate>Thu, 18 Jun 2020 21:27:35 GMT</pubDate>
    </item>
    <item>
      <title>多项式回归度增加后训练得分降低</title>
      <link>https://stackoverflow.com/questions/47717818/train-score-diminishes-after-polynomial-regression-degree-increases</link>
      <description><![CDATA[我尝试使用线性回归将多项式拟合到一组添加了噪声的正弦信号中的点，使用 sklearn 中的 linear_model.LinearRegression。
正如预期的那样，训练和验证分数随着多项式度数的增加而增加，但在 20 度左右之后，事情开始变得奇怪，分数开始下降，模型返回的多项式看起来根本不像我用来训练它的数据。
下面是一些可以看到这种情况的图表，以及生成回归模型和图表的代码：
在度数 = 17 之前，它是如何正常工作的。原始数据 VS 预测：

之后情况变得更糟：

验证曲线，增加多项式的次数：

from sklearn.pipeline import make_pipeline
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.learning_curve import验证曲线

def make_data(N, err=0.1, rseed=1):
rng = np.random.RandomState(1)
x = 10 * rng.rand(N)
X = x[:, None]
y = np.sin(x) + 0.1 * rng.randn(N)
if err &gt; 0:
y += err * rng.randn(N)
return X, y

def PolynomialRegression(degree=4):
return make_pipeline(PolynomialFeatures(degree),
LinearRegression())

X, y = make_data(400)

X_test = np.linspace(0, 10, 500)[:, None]
degrees = np.arange(0, 40)

plt.figure(figsize=(16, 8))
plt.scatter(X.flatten(), y)
for degree in degrees:
y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)
plt.plot(X_test, y_test, label=&#39;degre={0}&#39;.format(degree))
plt.title(&#39;原始数据 VS 不同预测值度&#39;)
plt.legend(loc=&#39;best&#39;);

degree = np.arange(0, 40)
train_score, val_score = validation_curve(PolynomialRegression(), X, y,
&#39;polynomialfeatures__degree&#39;,
degree, cv=7)

plt.figure(figsize=(12, 6))
plt.plot(degree, np.median(train_score, 1), marker=&#39;o&#39;, 
color=&#39;blue&#39;, label=&#39;training score&#39;)
plt.plot(degree, np.median(val_score, 1), marker=&#39;o&#39;,
color=&#39;red&#39;, label=&#39;validation score&#39;)
plt.legend(loc=&#39;best&#39;)
plt.ylim(0, 1)
plt.title(&#39;学习曲线，增加多项式的次数&#39;)
plt.xlabel(&#39;degree&#39;)
plt.ylabel(&#39;score&#39;);

我知道预期的事情是当模型的复杂性增加时，验证分数会下降，但为什么训练分数也会下降？我可能遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/47717818/train-score-diminishes-after-polynomial-regression-degree-increases</guid>
      <pubDate>Fri, 08 Dec 2017 15:53:36 GMT</pubDate>
    </item>
    </channel>
</rss>