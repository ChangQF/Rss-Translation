<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 26 Mar 2024 12:24:51 GMT</lastBuildDate>
    <item>
      <title>TensorFlowLite 错误：“Interpreter.GetOutputTensor(int)”由于其保护级别而无法访问</title>
      <link>https://stackoverflow.com/questions/78224205/tensorflowlite-error-interpreter-getoutputtensorint-is-inaccessible-due-to</link>
      <description><![CDATA[我有一个 tflite 模型，它将图像作为输入并预测其类别。我希望它在我的统一项目中使用。当我使用chatgpt给出的代码时，出现以下错误。任何人都可以帮忙，我对unity和c#不太了解
Assets\Samples\Detection\Scripts\PythonBridge.cs(72,9): 错误 CS0246: 找不到类型或命名空间名称“Tensor”（您是否缺少 using 指令或程序集引用？）
Assets\Samples\Detection\Scripts\PythonBridge.cs(72,43)：错误 CS0122：“Interpreter.GetOutputTensor(int)”由于其保护级别而无法访问
使用UnityEngine；
使用 TensorFlowLite；
使用系统.IO；
使用 System.Collections.Generic；

公共类对象检测：MonoBehaviour
{
    [序列化字段]
    [FilePopup(“*.tflite”)]
    公共字符串 modelPath = “model.tflite”;


    [序列化字段]
    私有 TextAsset 标签文件；

    [序列化字段]
    私有Texture2D输入图像；

    私人口译员；
    私有列表&lt;字符串&gt;标签;

    私有常量 int IMAGE_SIZE = 224;
    私有常量 int 通道 = 3;

    私有无效开始（）
    {
        加载模型();
        加载标签（）；
        预处理图像();
        运行推理（）；
    }

    私有无效LoadModel（）
    {
        解释器=新解释器(File.ReadAllBytes(modelPath));
    }

    私有无效 LoadLabels()
    {
        标签 = new List();
        使用 (StringReader reader = new StringReader(labelFile.text))
        {
            串线；
            while ((line = reader.ReadLine()) != null)
            {
                labels.Add(line.Trim());
            }
        }
    }

    私有无效 PreprocessImage()
    {
        Texture2D resizedImage = ResizeImage(inputImage, IMAGE_SIZE, IMAGE_SIZE);
        Color32[] 像素 = resizedImage.GetPixels32();

        float[] imgArray = new float[IMAGE_SIZE * IMAGE_SIZE * CHANNELS];
        for (int i = 0; i &lt; 像素.长度; i++)
        {
            imgArray[i * 3] = 像素[i].r / 255.0f；
            imgArray[i * 3 + 1] = 像素[i].g / 255.0f;
            imgArray[i * 3 + 2] = 像素[i].b / 255.0f;
        }

        解释器.SetInputTensorData(0, imgArray);
    }

    私有无效 RunInference()
    {
        解释器.Invoke();

        // 检索输出并处理预测
        张量输出Tensor =terpreter.GetOutputTensor(0);
        float[] 结果 = outputTensor.Data();

        // 寻找概率最高的类
        int 最大索引 = 0;
        浮动最大概率 = 0f;
        for (int i = 0; i &lt; results.Length; i++)
        {
            if (结果[i] &gt; 最大概率)
            {
                最大概率=结果[i]；
                最大索引 = i;
            }
        }

        字符串预测标签=标签[maxIndex];
        Debug.Log(“预测对象：”+predictedLabel);
    }

    私有Texture2D ResizeImage（Texture2D源，int宽度，int高度）
    {
        RenderTexture rt = RenderTexture.GetTemporary(宽度，高度，24);
        RenderTexture.active = rt;
        Graphics.Blit(源, rt);
        Texture2D 结果 = new Texture2D(宽度, 高度);
        结果.ReadPixels(new Rect(0, 0, 宽度, 高度), 0, 0);
        结果.Apply();
        RenderTexture.active = null;
        RenderTexture.ReleaseTemporary(rt);
        返回结果；
    }
}


我尝试在chatgpt中解决，但它没有更新。我在 c# 中使用了 .h5 和 python，并得到了输出。但导出为 apk 时不起作用。于是搜了一下，发现tensorflowlite可以解决这个问题]]></description>
      <guid>https://stackoverflow.com/questions/78224205/tensorflowlite-error-interpreter-getoutputtensorint-is-inaccessible-due-to</guid>
      <pubDate>Tue, 26 Mar 2024 09:24:43 GMT</pubDate>
    </item>
    <item>
      <title>此代码不适用于tensorflow 2.16.0+版本</title>
      <link>https://stackoverflow.com/questions/78223936/this-code-is-not-working-on-tensorflow-2-16-0-version</link>
      <description><![CDATA[检查点 = ModelCheckpoint(
    &#39;./base.model&#39;,
    监视器=&#39;val_accuracy&#39;,
    详细=1，
    save_best_only=真，
    模式=&#39;最大&#39;,
    save_weights_only=假,
    保存频率=1
）
提前停止=提前停止(
    监视器=&#39;val_loss&#39;,
    最小增量=0.001，
    耐心=30，
    详细=1，
    模式=&#39;自动&#39;
）

opt1 = tf.keras.optimizers.Adam()

回调= [检查点，提前停止]

这不适用于tensorflow 2.16.1
但是，正在 google colab 上开发 2.15.0
我如何修复我的代码或如何安装tensorflow 2.15.0？
我尝试了pip install tensorflow=2.15.0
但是，它显示错误]]></description>
      <guid>https://stackoverflow.com/questions/78223936/this-code-is-not-working-on-tensorflow-2-16-0-version</guid>
      <pubDate>Tue, 26 Mar 2024 08:41:28 GMT</pubDate>
    </item>
    <item>
      <title>无法在 LightGBM 中检索 best_iteration</title>
      <link>https://stackoverflow.com/questions/78223783/cant-retrieve-best-iteration-in-lightgbm</link>
      <description><![CDATA[我使用 Optuna 来优化我的 LightGBM 模型。同时，我使用 LightGBM 回调 early_stopping(50) 提前停止迭代。
这是我的代码：
def 目标（试验、train_set、valid_set、num_iterations）：
    
    参数 = {
        &#39;目标&#39;：&#39;二进制&#39;，
        &#39;任务&#39;:&#39;训练&#39;,
        &#39;提升&#39;：&#39;gbdt&#39;，
        &#39;度量&#39;：[&#39;auc&#39;]，
        &#39;n_estimators&#39;：num_iterations，
        “冗长”：-1，
        &#39;feature_pre_filter&#39;：假，
        &#39;学习率&#39;: Trial.suggest_float(&#39;学习率&#39;, 0.01, 0.3),
        &#39;num_leaves&#39;: Trial.suggest_int(&#39;num_leaves&#39;, 2, 256),
        &#39;最大深度&#39;: Trial.suggest_int(&#39;最大深度&#39;, 3, 12),
        &#39;min_data_in_leaf&#39;: Trial.suggest_int(&#39;min_data_in_leaf&#39;, 20, 10000),
        &#39;lambda_l1&#39;: Trial.suggest_float(&#39;lambda_l1&#39;, 1e-10, 10.0, log=True),
        &#39;lambda_l2&#39;: Trial.suggest_float(&#39;lambda_l2&#39;, 1e-10, 10.0, log=True),
        &#39;min_gain_to_split&#39;: Trial.suggest_float(&#39;min_gain_to_split&#39;, 0, 15),
        &#39;feature_fraction&#39;: Trial.suggest_float(&#39;feature_fraction&#39;, 0.2, 1.0),
        &#39;bagging_fraction&#39;: Trial.suggest_float(&#39;bagging_fraction&#39;, 0.2, 1.0),
        &#39;bagging_freq&#39;: Trial.suggest_int(&#39;bagging_freq&#39;, 1, 10),
    }

    
    pruning_callback = optuna.integration.LightGBMPruningCallback(Trial, &#39;auc&#39;, valid_name=&#39;valid_set&#39;)
    
    模型 = lgb.train(
        参数，
        训练集=训练集，
        valid_sets=[train_set, valid_set],
        valid_names=[&#39;train_set&#39;, &#39;valid_set&#39;],
        回调=[修剪_回调，
                   提前停止(100)
                  ]
    ）
    Trial.set_user_attr(key=&#39;best_booster&#39;, value=模型)

    prob_pred = model.predict(feature_test, num_iteration=model.best_iteration)
    返回 roc_auc_score(label_test, prob_pred, labels=[0,1])

func = lambda 试验：目标（试验=试验，train_set=train_set，valid_set=valid_set，num_iterations=num_iterations）

迭代次数 = 1000

研究 = optuna.create_study(
    修剪器=optuna.pruners.HyperbandPruner(),
    方向=&#39;最大化&#39;
）

我将最佳试验中的最佳模型设置为study对象中的user_attr。这是我的代码：
def save_best_booster（研究，试用）：
    如果 Study.best_Trial.number == Trial.number：
        Study.set_user_attr(key=&#39;best_booster&#39;, value=Trial.user_attrs[&#39;best_booster&#39;])
        Study.set_user_attr(key=&#39;best_eval_result&#39;, value=Trial.user_attrs[&#39;best_eval_result&#39;])

研究.优化(
    功能，
    n_试验=30，
    show_progress_bar=真，
    回调=[save_best_booster]
）

最后我从user_attr中检索到了最佳模型（最佳助推器）。这是我的代码：
试验 = Study.best_Trial

best_model=study.user_attrs[&#39;best_booster&#39;]

由于设置了 early_stopping 回调，训练输出日志显示如下内容：
提前停止，最佳迭代是：
[30]train_set的auc：0.982083 valid_set的auc：0.874471
训练直到 100 轮验证分数没有提高

假设高于 valid_set 的 auc: 0.874471 的 auc 值确实是所有迭代中的最佳值，则 best_iteration 应为 [30]，如上所示。
但是，我通过调用 best_model.best_iteration 得到了 -1，如下所示：
在： print(best_model.best_iteration)

输出：-1

我的问题是：如何从 study 对象检索的最佳模型中获取正确的 best_iteration 值？
感谢谁能解决我的问题！
期待您的回复:)]]></description>
      <guid>https://stackoverflow.com/questions/78223783/cant-retrieve-best-iteration-in-lightgbm</guid>
      <pubDate>Tue, 26 Mar 2024 08:12:26 GMT</pubDate>
    </item>
    <item>
      <title>有关正确/错误解决方案的 C 代码数据集</title>
      <link>https://stackoverflow.com/questions/78223653/c-code-dataset-on-correct-incorrect-solutions</link>
      <description><![CDATA[我正在寻找有关体育编程任务解决方案的公共数据集（例如来自 LeetCode、timus...）。问题是，为了微调我的模型，我需要正确和错误的解决方案。例如，在 Leetcode 上，人们可以找到通过每项任务的所有测试的解决方案（但不确定在未经许可的情况下使用它们是否公平），但根本没有公开开放的不正确解决方案那里。
所需的编程语言是 C。
有人可以帮我吗？
我检查了 HuggingFace 上的所有“代码”数据集，同样什么也没有。]]></description>
      <guid>https://stackoverflow.com/questions/78223653/c-code-dataset-on-correct-incorrect-solutions</guid>
      <pubDate>Tue, 26 Mar 2024 07:46:53 GMT</pubDate>
    </item>
    <item>
      <title>我们如何创建或者什么类型的机器学习或深度学习模型可用于创建用于安排比赛的模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78223241/how-can-we-create-or-what-type-of-machine-learning-or-deeplearning-models-be-use</link>
      <description><![CDATA[想要创建一个网球锦标赛时间表
我还没有尝试过，但想了解启动和训练模型的基本知识。我有一组过去的比赛数据，其中有各种限制，所以我想创建一个可以进行预测的模型。]]></description>
      <guid>https://stackoverflow.com/questions/78223241/how-can-we-create-or-what-type-of-machine-learning-or-deeplearning-models-be-use</guid>
      <pubDate>Tue, 26 Mar 2024 06:01:20 GMT</pubDate>
    </item>
    <item>
      <title>如何有效利用机器学习专业课程可选实验室（吴恩达先生）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78223041/how-to-effectively-use-optional-labs-in-machine-learning-specialization-course</link>
      <description><![CDATA[我应该复制实验室中给出的代码并在本地运行吗？另外，您如何使用可选实验室，请详细说明（提供分步路径）。
目前，我正在阅读代码并了解实验室中使用的库。
但我相信这不是理想的方式。我没有充分发挥实验室的优势。]]></description>
      <guid>https://stackoverflow.com/questions/78223041/how-to-effectively-use-optional-labs-in-machine-learning-specialization-course</guid>
      <pubDate>Tue, 26 Mar 2024 04:47:21 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Flower 和 Tensorflow 来结束联邦学习中服务器的额外参数？</title>
      <link>https://stackoverflow.com/questions/78221905/how-to-end-extra-parameters-to-server-in-federated-learning-with-flower-and-tens</link>
      <description><![CDATA[我想将带有模型更新的额外参数发送到服务器，然后将服务器中的这些额外参数用于其他目的。我在这个项目中使用 Flower 和 Tensorflow。在发送额外参数之前，我的模型运行良好。目前我有这些代码客户端模型 server.py。
如何在服务器中成功发送额外参数或值并接收它？
感谢您的帮助。
我尝试在 get_parameter 方法中发送附加参数，并使用 FedAvg 策略接收它。但我一次又一次地遇到这个错误。 错误]]></description>
      <guid>https://stackoverflow.com/questions/78221905/how-to-end-extra-parameters-to-server-in-federated-learning-with-flower-and-tens</guid>
      <pubDate>Mon, 25 Mar 2024 21:38:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 MS-COCO 数据集、标准化和灰度 [关闭]</title>
      <link>https://stackoverflow.com/questions/78221719/working-with-ms-coco-dataset-normalization-and-grayscale</link>
      <description><![CDATA[我正在尝试构建 SuperPoint 网络的修改版本以进行兴趣点检测。 SuperPoint 网络适用于灰度图像。我正在使用 MS-COCO 数据集进行训练。我的困惑是，我应该将图像转换为灰度，然后使用平均值和标准差（灰度数据集的）进行标准化，还是首先标准化 RGB（使用 RGB 数据集的平均值和标准差），然后转换为灰度？然后我需要将其缩放到 [-1,1]。
任何提示或解释将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78221719/working-with-ms-coco-dataset-normalization-and-grayscale</guid>
      <pubDate>Mon, 25 Mar 2024 20:52:13 GMT</pubDate>
    </item>
    <item>
      <title>如何删除 Huggingface 的 Transformer GPT2 预训练模型中的层？</title>
      <link>https://stackoverflow.com/questions/78219076/how-to-remove-layers-in-huggingfaces-transformers-gpt2-pre-trained-models</link>
      <description><![CDATA[我的代码：
从转换器导入 GPT2Config、GPT2Model
从变压器导入 AutoTokenizer、AutoModelForMaskedLM、AutoModelForCausalLM
模型 = AutoModelForCausalLM.from_pretrained(“openai-community/gpt2”)
打印（解码器）

这是控制台的输出，列出了模型架构：
&lt;前&gt;&lt;代码&gt;GPT2LMHeadModel(
  （变压器）：GPT2Model（
    (wte)：嵌入(50257, 768)
    (wpe)：嵌入(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): 模块列表(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        ）
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (动作): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        ）
      ）
    ）
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  ）
  （lm_head）：线性（in_features = 768，out_features = 50257，偏差= False）
）

我想删除第一层：
(wte)：嵌入(50257, 768)

我尝试过以下方法：
def deleteEncodingLayers(model, num_layers_to_keep): # 必须传入完整的bert模型
    oldModuleList = model.bert.encoder.layer
    newModuleList = nn.ModuleList()

    # 现在迭代所有层，只保留相关层。
    对于范围内的 i(0, len(num_layers_to_keep))：
        newModuleList.append(oldModuleList[i])

    # 创建模型的副本，使用新列表修改它，然后返回
    copyOfModel = copy.deepcopy(模型)
    copyOfModel.bert.encoder.layer = newModuleList

    返回模型副本

但是没有成功。谁知道怎么解决？]]></description>
      <guid>https://stackoverflow.com/questions/78219076/how-to-remove-layers-in-huggingfaces-transformers-gpt2-pre-trained-models</guid>
      <pubDate>Mon, 25 Mar 2024 12:28:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 MPI 优化 Optuna 参数</title>
      <link>https://stackoverflow.com/questions/78218072/optuna-parameter-optimisation-with-mpi</link>
      <description><![CDATA[我有一些机器学习代码，它使用 SVM（来自 scikit-learn）和预计算内核，我想使用 optuna 对其进行优化，因此代码简单地看起来有点像这样
def 目标（试用：试用，fast_check=True，target_meter=0，return_info=False）：
     #设置参数
     C = Trial.suggest_float(“C”,0.0​​1,5)
     tol = Trial.suggest_loguniform(“tol”,1e-4,1e-1)
     内核参数 = ...

     #构建火车内核
     内核训练 = ...

     #构建测试内核
     内核测试 = ...

     #火车服务
     svc = SVC(内核=“预计算”, C=C, tol=tol)
     svc.fit(kernel_train, train_labels)
     test_predict = svc.predict(kernel_test)
     test_auc = roc_auc_score(test_labels,test_predict)

     返回测试_auc

Study = optuna.create_study(direction=“最大化”,study_name=&#39;study_1&#39;)
研究.优化（目标，n_Trials=40）


但是，由于我正在计算的内核的复杂性，我使用 mpi4py 来并行计算，但同​​时使用 optuna 和 MPI 时遇到了一些问题。
显然，我想要多个处理器上的内核代码，但是当我创建研究并优化它时，我不想在处理器上创建多个不同的研究，我只想对根进行优化的一项研究（我假设？）。我已经尝试了下面的方法，它有效，但是当我不使用 MPI 时，它的优化效果不佳，我认为这正在创建多项研究并优化它们，这似乎效率不高。似乎更难以收敛到最佳参数。
从 mpi4py 导入 MPI

mpi_comm = MPI.COMM_WORLD
排名 = mpi_comm.Get_rank()
n_procs = mpi_comm.Get_size()
根=0

def目标（试验：试验，fast_check = True，target_meter = 0，return_info = False）：
     #设置参数
     C = Trial.suggest_float(“C”,0.0​​1,5)
     tol = Trial.suggest_loguniform(“tol”,1e-4,1e-1)
     内核参数 = ...

     #使用 MPI 构建训练内核
     内核训练 = ...

     #使用MPI构建测试内核
     内核测试 = ...

     #火车服务
     如果排名==根：
           svc = SVC(内核=“预计算”, C=C, tol=tol)
           svc.fit(kernel_train, train_labels)
           test_predict = svc.predict(kernel_test)
           test_auc = roc_auc_score(test_labels,test_predict)
     别的：
           测试_auc = 0
     test_auc = mpi_comm.bcast(test_auc, root=0)

如果排名==根：
     Study = optuna.create_study(direction=“最大化”,study_name=&#39;study_1&#39;)
别的：
     研究 = 0
 研究= mpi_comm.bcast（研究，根= 0）

研究.优化（目标，n_Trials=40）

这是一个非常小众的问题，但只是想知道是否有人对这些软件包有任何经验，并且可以帮助建议如何运行多处理代码，同时仅优化一个处理器上的参数。如果有任何术语不正确，我深表歉意，我是使用这两个软件包的新手，因此请耐心等待。]]></description>
      <guid>https://stackoverflow.com/questions/78218072/optuna-parameter-optimisation-with-mpi</guid>
      <pubDate>Mon, 25 Mar 2024 09:20:08 GMT</pubDate>
    </item>
    <item>
      <title>使用扩散模型和Detectron2进行图像分割调试</title>
      <link>https://stackoverflow.com/questions/78217946/image-segmentation-debugging-using-the-diffusion-model-and-detectron2</link>
      <description><![CDATA[我正在 Publaynet 数据集上训练一个基于 detectorron2 构建的扩散模型，用于实例分割。但多次迭代后我得到的输出是将整个文档分割为如图所示。它不会对文档中的表格和文本等单个元素进行分段。损失函数在学习率为 0.00005 时下降得非常好。总损失减少至 1.6。 Loss_bbox 约为 0.200，loss:giou 为 0.3573。
我进行了健全性检查，模型中的输入似乎是正确的。我可视化了输入和边界框，并检查了目标。一切似乎都是正确的。我不知道是否应该进一步训练模型或更改任何其他超参数。我尝试了多个学习率，这个学习率似乎不错。 NUM_PROPSALS 是 500。我应该提高到 1000。我应该特别关心哪些超参数。该代码不是从头开始的。我从这里获取仓库 https://github.com/chenhaoxing/DiffusionInst。所以我不是从头开始构建模型。如果有人有任何想法请告诉我。
以下是配置文件中的超参数列表：
&lt;前&gt;&lt;代码&gt;型号：
  META_ARCHITECTURE：“DiffusionInst”
  权重：“Detectron2://ImageNetPretrained/torchvision/R-50.pkl”
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  像素_STD：[58.395、57.120、57.375]
  骨干：
    名称：“build_resnet_fpn_backbone”
  资源网：
    OUT_FEATURES：[“res2”、“res3”、“res4”、“res5”]
  FPN：
    IN_FEATURES：[“res2”、“res3”、“res4”、“res5”]
  投资回报率_头：
    IN_FEATURES：[“p2”、“p3”、“p4”、“p5”]
  ROI_BOX_HEAD：
    POOLER_TYPE：“ROIAlignV2”
    POOLER_RESOLUTION：7
    POOLER_SAMPLING_RATIO：2
解算器：
  IMS_PER_BATCH：2
  BASE_LR：0.0000125
  步骤：（210000、250000）
  最大迭代次数：270000
  热身系数：0.01
  WARMUP_ITERS：1000
  WEIGHT_DECAY：0.0001
  优化器：“ADAMW”
  BACKBONE_MULTIPLIER: 1.0 # 与 BASE_LR 保持相同。
  CLIP_GRADIENTS：
    已启用：正确
    CLIP_TYPE：“完整模型”
    CLIP_VALUE：1.0
    标准类型：2.0
种子：40244023
输入：
  最小尺寸列车：（480、512、544、576、608、640、672、704、736、768、800）
  庄稼：
    启用：假
    类型：“绝对范围”
    尺寸：（384、600）
  格式：“RGB”
测试：
  EVAL_PERIOD：20000
数据加载器：
  FILTER_EMPTY_ANNOTATIONS：错误
  NUM_WORKERS：2
版本：2
]]></description>
      <guid>https://stackoverflow.com/questions/78217946/image-segmentation-debugging-using-the-diffusion-model-and-detectron2</guid>
      <pubDate>Mon, 25 Mar 2024 08:54:20 GMT</pubDate>
    </item>
    <item>
      <title>R 混淆矩阵 - 错误：“数据”和“参考”应该是具有相同级别的因素</title>
      <link>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</link>
      <description><![CDATA[尽管还有其他针对相同错误消息的报告，但没有一个对我的情况有帮助。
我已经准备了自己的数据，分割如下，但无法获得混淆矩阵。
test_index &lt;- createDataPartition(y =workingData$PM10, times = 1, p = 0.5, list = FALSE)
train_set &lt;-工作数据[-test_index,]
test_set &lt;-工作数据[test_index,]

train_knn &lt;- train(PM10 ~. , method= &quot;knn&quot; , data = train_set)

y_hatknn &lt;- 预测(train_knn, train_set, type = “raw”)

fusionMatrix(y_hatknn, test_set$PM10)

上面最后一行给出
错误：“data”和“reference”应该是具有相同级别的因素。

我想上传数据进行复制，但可以提供基本的：
&lt;前&gt;&lt;代码&gt;str（工作数据）
“数据帧”：3653 obs。 3 个变量：
&#39; $ 日期 : 数字 2e+07 2e+07 2e+07 2e+07 2e+07 ...
&#39; $ Rain_mm: 数字 0.1 6.7 0 1.4 0.8 1.8 15.3 0 2.6 3.8 ...
&#39; $ PM10 : 数字 -1 -1 -1 -1 -1 ...

PM10 是污染 PM10 水平。
如何解决？
添加更多信息：
在原始错误之后：
&lt;块引用&gt;
confusionMatrix(y_hatknn, test_set$PM10)
错误：data 和 reference 应该是具有相同水平的因素。

我尝试将其设置为因素...
&lt;块引用&gt;
confusionMatrix(y_hatknn, as.factor(test_set$PM10))
错误：data 和 reference 应该是具有相同水平的因素。

以预测为因素...
&lt;块引用&gt;
confusionMatrix(as.factor(y_hatknn), test_set$PM10)
错误：data 和 reference 应该是具有相同水平的因素。

以两个参数为因素...
&lt;块引用&gt;
confusionMatrix(as.factor(y_hatknn), as.factor(test_set$PM10))
fusionMatrix.default(as.factor(y_hatknn), as.factor(test_set$PM10)) 中的错误：
数据的级别不能多于参考

确实需要得到整理，Stack坚持关闭我的帖子，写下gmail中navarrodan007的解决方案]]></description>
      <guid>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</guid>
      <pubDate>Fri, 22 Mar 2024 09:39:08 GMT</pubDate>
    </item>
    <item>
      <title>从“y”的唯一值推断出的类无效。预期：[0 1 2 3 4 5]，得到[1 2 3 4 5 6]</title>
      <link>https://stackoverflow.com/questions/71996617/invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2-3-4-5-got</link>
      <description><![CDATA[我已经使用 XGB 分类器训练了数据集，但在本地出现了此错误。它在 Colab 上有效，而且我的朋友对相同的代码也没有任何问题。
我不知道这个错误意味着什么......
从 y 的唯一值推断出的类无效。预期：[0 1 2 3 4 5]，得到[1 2 3 4 5 6]
这是我的代码，但我想这不是原因。
start_time = time.time()
xgb = XGBClassifier（n_估计器 = 400，学习率 = 0.1，最大深度 = 3）
xgb.fit(X_train.values, y_train)
print(&#39;适合时间：&#39;, time.time() - start_time)
]]></description>
      <guid>https://stackoverflow.com/questions/71996617/invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2-3-4-5-got</guid>
      <pubDate>Mon, 25 Apr 2022 08:32:38 GMT</pubDate>
    </item>
    <item>
      <title>尽管验证准确度很高，为什么我的神经网络对属于某一类的测试图像预测出错误的类标签？</title>
      <link>https://stackoverflow.com/questions/71841718/why-does-my-neural-network-predict-the-incorrect-class-label-for-test-images-bel</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/71841718/why-does-my-neural-network-predict-the-incorrect-class-label-for-test-images-bel</guid>
      <pubDate>Tue, 12 Apr 2022 11:20:17 GMT</pubDate>
    </item>
    <item>
      <title>Python scipy/numpy 中相关性的层次聚类？</title>
      <link>https://stackoverflow.com/questions/2907919/hierarchical-clustering-on-correlations-in-python-scipy-numpy</link>
      <description><![CDATA[如何在 scipy/numpy 中的相关矩阵上运行层次聚类？我有一个 100 行 x 9 列的矩阵，我想根据 9 个条件中每个条目的相关性进行分层聚类。我想使用 1-pearson 相关性作为聚类的距离。假设我有一个包含 100 x 9 矩阵的 numpy 数组 X，我该怎么做？
我尝试使用 hcluster，基于此示例：
Y=pdist(X, &#39;seuclidean&#39;)
Z=联动(Y, &#39;单&#39;)
树状图(Z, color_threshold=0)

但是，pdist 不是我想要的，因为那是欧几里德距离。有什么想法吗？
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/2907919/hierarchical-clustering-on-correlations-in-python-scipy-numpy</guid>
      <pubDate>Tue, 25 May 2010 19:39:00 GMT</pubDate>
    </item>
    </channel>
</rss>