<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Thu, 27 Feb 2025 18:25:14 GMT</lastBuildDate>
    <item>
      <title>如何让pytorch学习系数变量，以使nn.module之外的丢失？</title>
      <link>https://stackoverflow.com/questions/79473125/how-to-let-pytorch-learn-coefficients-variables-for-the-loss-outside-of-nn-modul</link>
      <description><![CDATA[我有这样的损失弹性：
 损失= alpha * loss0 + beta * loss1 + gamma * loss2 + delta * loss3
 
我想制作Alpha，Beta，Gamma和Delta可学习的参数。请注意，Alpha，Beta，Gamma和Delta在NN.Module之外。我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/79473125/how-to-let-pytorch-learn-coefficients-variables-for-the-loss-outside-of-nn-modul</guid>
      <pubDate>Thu, 27 Feb 2025 15:16:32 GMT</pubDate>
    </item>
    <item>
      <title>没有或几个异常的异常检测ML训练</title>
      <link>https://stackoverflow.com/questions/79472624/anomaly-detection-ml-training-with-none-or-few-anomalies</link>
      <description><![CDATA[使用隔离森林对无监督的非序列数据进行异常检测时，局部离群因子具有低变化变量，例如温度和湿度。
，我们通过读取模型来训练该模型，这些读数可以被认为是我们案件正常的读数。因此，几乎没有我们的目标“异常”。
它会使模型更敏感地将值标记为异常吗？
可以通过“种植”来解决此类问题。拥有异常。
例如。从温度，二氧化碳，湿度等房间收集读数时，我们会燃烧与之旁边的匹配项，或者亲自更改数据。
 ps。这是对更清晰的问题进行了编辑的。]]></description>
      <guid>https://stackoverflow.com/questions/79472624/anomaly-detection-ml-training-with-none-or-few-anomalies</guid>
      <pubDate>Thu, 27 Feb 2025 12:07:53 GMT</pubDate>
    </item>
    <item>
      <title>BigQuery ML时间序列模型评估保持返回零</title>
      <link>https://stackoverflow.com/questions/79471261/bigquery-ml-time-series-model-evaluate-keeps-returning-null</link>
      <description><![CDATA[我正在使用BigQuery ML来训练ARIMA_PLUS模型，以预测CPU使用情况。该模型成功训练，但是当我运行ml。评估时，所有结果值均为null。
 模型训练查询 
 创建或替换模型`project.dataset.arima_model`
选项（
  model_type =&#39;arima_plus&#39;，
  time_series_timestamp_col =&#39;timestamp_column&#39;，
  time_series_id_col = [&#39;id_column_1&#39;，&#39;id_column_2&#39;]，
  time_series_data_col =&#39;data_column&#39;，
  forecast_limit_lower_bound = 0，
  forecast_limit_upper_bound = 100
） 作为
选择data_column，id_column_1，id_column_2，timestamp_column
来自`project.dataset.source_table`
在“ 2025-02-5”和&#39;2025-02-12&#39;之间的日期（timestamp_column）;
 
 评估查询 
 选择 * 
来自ml.evaluate（
  模型`project.dataset.arima_model`，
  （（
    选择data_column，id_column_1，id_column_2，timestamp_column
    来自`project.dataset.source_table`
    在“ 2025-02-13&#39;和&#39;2025-02-20&#39;之间的日期（timestamp_column）
  ），
  结构（
    true作为persim_gregation， 
    10作为地平线， 
    0.9作为信心_level
  ）
）；
 
 ml.Evaluate成功运行，但返回所有ID的null
查询结果 ]]></description>
      <guid>https://stackoverflow.com/questions/79471261/bigquery-ml-time-series-model-evaluate-keeps-returning-null</guid>
      <pubDate>Wed, 26 Feb 2025 23:29:23 GMT</pubDate>
    </item>
    <item>
      <title>NN回归训练损失初始增加[关闭]</title>
      <link>https://stackoverflow.com/questions/79471142/nn-regression-training-loss-initial-increase</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79471142/nn-regression-training-loss-initial-increase</guid>
      <pubDate>Wed, 26 Feb 2025 22:16:00 GMT</pubDate>
    </item>
    <item>
      <title>如何为AI模型培训构建服务器？ （GPU，硬件和远程访问）[关闭]</title>
      <link>https://stackoverflow.com/questions/79470714/how-to-build-a-server-for-ai-model-training-gpu-hardware-and-remote-access</link>
      <description><![CDATA[我想构建用于培训AI模型的计算机（服务器），包括：

微调聊天机器人LLMS 
机器学习和深度学习模型

我有几个问题：

 我应该购买哪个GPU？我知道GPU对于AI培训很重要，但我不确定哪一个是满足我需求的最佳选择。

 我还需要什么其他硬件？除GPU外，推荐的CPU，RAM，存储和其他组件是什么？

 如何远程控制此服务器？我希望能够从另一个位置访问和管理此服务器。我应该使用什么工具或方法？

 多人可以同时使用该服务器吗？如果我想与朋友或队友共享此服务器，我们如何一起训练模型？

]]></description>
      <guid>https://stackoverflow.com/questions/79470714/how-to-build-a-server-for-ai-model-training-gpu-hardware-and-remote-access</guid>
      <pubDate>Wed, 26 Feb 2025 18:16:44 GMT</pubDate>
    </item>
    <item>
      <title>在神经网络层中使用二进制（{0,1}）权重</title>
      <link>https://stackoverflow.com/questions/79469831/using-binary-0-1-weights-in-a-neural-network-layer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79469831/using-binary-0-1-weights-in-a-neural-network-layer</guid>
      <pubDate>Wed, 26 Feb 2025 13:02:20 GMT</pubDate>
    </item>
    <item>
      <title>如何准备使用LSTM进行分类的不规则间隔时间序列数据？</title>
      <link>https://stackoverflow.com/questions/79469782/how-to-prepare-irregularly-spaced-time-series-data-for-classification-using-lstm</link>
      <description><![CDATA[ i具有这样的可变的可变价值的数据，如下所示： processed_data 是一个大小为215×1的单元格数组，保存单元格，每个单元都包含给定一天的数据。每个单元（天）的观测值数量不同（平均约为12,000行）。每行代表一个观察值，其中：第一列包含自上一行以来经过的秒数（未归一化），第二列包含指定安全性的价格（使用z得分进行了归一化），第三列是目标变量，信号传递该时刻的价格是否会高0.01％（表示为1）60秒（表示为1）60秒或不表示为0（表示为0）。我将前两列用作预测指标。我将日子分开，因为小时在 processed_data {i，1} 的最后一行之间以及day processed_data {i+1，1} 的第一行。以下是任意日的数据示例：
  2.57500000000437 0.502515050312692 0
1.036000000006 0.469361050915526 1
1.05899999999383 0.386501335237771 1
0.838000000003376 0.436219680495852 0
1.1299999999738 0.469361050915526 0
0.824000000000524 0.369924327252462 1
 
我只是ML的初学者，而且我很难想象应该如何格式化LSTM层的数据。如果我正确，它需要3维数据，其中一个维度代表 channel ，另一个维度为时间步长，而另一个 batch 。我现在确定我已经完全误解了这些概念，并写了以下代码：
  %%分区数据。
train_data_length = round（长度（processed_data） * 0.9）;
train_data = processed_data（1：train_data_length）;
test_data = processed_data（train_data_length+1：end）;

%%培训设置
％将数据转换为dlarray的单元格数组。
train_x =单元格（size（train_data））;
train_y =单元格（size（train_data））;

对于一天= 1：长度（train_data）
    ％添加批处理尺寸（C×B×T，其中B = 1）。
    data = permute（train_data {day}（：，1：2）&#39;，[1 3 2]）; ％[2×1×T]
    train_x {day} = dlarray（data，＆quot; cbt; quot;）;
    
    ％将标签转换为单速编码的CBT格式[2×1×T]。
    labels = train_data {day}（：，3）&#39;; ％[1×T]
    One_hot_labels = OneHotEncode（标签，1，&#39;classNames&#39;，[0 1]）; ％[2×t]
    One_hot_labels = reshape（One_hot_labels，2，1，[]）; ％[2×1×T]
    train_y {day} = dlarray（single（One_hot_labels），“ CBT”）;
结尾

ds = combine（...
    arraydatastore（train_x，&#39;outputType&#39;，&#39;same&#39;），...
    arraydatastore（train_y，&#39;outputType&#39;，&#39;same&#39;）...
）；

％clearvars -Efcect ds test_data ml_method

num_features = 2;
num_hidden_​​units = 128;
num_classes = 2;
mini_batch_size = 32;

层= [
    sequenceInputlayer（num_features，“名称”，“输入”）
    lstmlayer（num_hidden_​​units，&#39;outputmode&#39;，&#39;sequence&#39;）
    完整连接的layerer（num_classes）
    SoftMaxlayer
];

net = dlnetwork（层）;

选项=训练（&#39;Adam&#39;，...
    “ Maxepochs”，30，...
    &#39;minibatchsize&#39;，mini_batch_size，...
    “序列长度”，“最长”，...
    “洗牌”，“每个段”，...
    “情节”，“训练过程”，...
    “ inputdataformats&#39;，&#39;cbt&#39;，...
    “冗长”，错误，...
    “执行环境”，“ gpu&#39;）;

net = trainnet（ds，net，&#39;crossentropy&#39;，选项）;
 
在上面的代码中，我试图将通道定义为预测变量的数量（在我的情况下为2，可能是我正确定义的唯一维度）。我将批次设置为1，因为我认为这意味着网络将使用一个观察结果来做出预测。我将时间步骤设置为一天的数据价值的第一列（自上次观察以来的秒数），因为我认为这实际上意味着及时的步骤。现在我知道我完全错了。我还必须将Mini_batch_size从128中将其更改为32，但我发现这太低了，但是否则，我会用尽内存。我想这是因为我的格式格式不正确（我不确定这是否是一个重要的细节，但是我将包括我的GPU，它是带有8GB内存的RTX2070 SUPER）。我的问题是：我应该如何根据目标格式化LSTM层的数据？否则我的目标是不现实的，我正在使用错误？
我想象这个网络能够对数据中的每个观察结果进行预测。]]></description>
      <guid>https://stackoverflow.com/questions/79469782/how-to-prepare-irregularly-spaced-time-series-data-for-classification-using-lstm</guid>
      <pubDate>Wed, 26 Feb 2025 12:49:01 GMT</pubDate>
    </item>
    <item>
      <title>可以使用Python比较多个ROC曲线与Delong的测试进行比较吗？</title>
      <link>https://stackoverflow.com/questions/79469653/it-is-possible-to-compare-more-than-two-roc-curves-with-delongs-test-using-pyth</link>
      <description><![CDATA[我有3条ROC曲线，该曲线使用了3种不同测试的数据计算出来，该测试旨在将患者归类为患病或健康。我已经计算了所有3个测试的AUC，我想比较此曲线，以查看使用DeLong测试之间是否存在统计差异。我发现了非常简单的实现，例如：
 来自mlstatkit.stats导入delong_test

z_score，p_value = delong_test（labels，scores_model1，scores_model2）

打印（f＆quot; Model 1 AUC：{roc_auc_score（标签，scores_model1）：。4f}＆quort;）
打印（F＆quot;模型2 AUC：{roc_auc_score（标签，scores_model2）：。4f}＆quort;）
print（f＆quot; z得分：{z_score：.4f}，p-value：{p_value：.4f}＆quot;）
 
但是，我发现的所有实现都是用于比较两条ROC曲线。有人知道该测试是否可以进行两条以上的曲线？我的想法正在进行3种不同的配对测试并比较P值，但我不知道。
有什么方法可以用3个ROC曲线执行Delong的测试，有人可以帮助我进行编码吗？]]></description>
      <guid>https://stackoverflow.com/questions/79469653/it-is-possible-to-compare-more-than-two-roc-curves-with-delongs-test-using-pyth</guid>
      <pubDate>Wed, 26 Feb 2025 12:07:00 GMT</pubDate>
    </item>
    <item>
      <title>改进职位描述，使用AI [封闭]</title>
      <link>https://stackoverflow.com/questions/79468885/improving-job-description-to-candidate-attribute-matching-for-talent-acquisition</link>
      <description><![CDATA[我正在研究AI驱动的人才获取解决方案，在该解决方案中，我们将恢复并将候选成就和职责转换为结构化属性。将主数据管理（MDM）用于作业角色和类别，我们生成这些属性。目标是将职位描述（JD）属性与候选属性匹配，以过滤给定作业的最相关的候选人。
这是我到目前为止尝试的：

  语义匹配：我使用语义相似性技术将职位描述属性与候选属性进行比较。

  向量嵌入：我尝试了嵌入技术（例如Word2Vec，句子变形金刚）以捕获属性的上下文含义。


但是，结果并不令人满意。传统方法（例如确切的关键字匹配或基于规则的方法）有局限性：

 他们无法捕获属性之间的语义相似性（例如Google AdWords和绩效营销）。

 他们不考虑同义词或相关技术（例如AWS和Amazon Web服务，或CI/CD Pipelines以及连续集成＆amp; exployment）。

 他们缺乏上下文感知的匹配，其中某些属性与特定的工作角色更相关。

 他们很难确定相关概念之间的语义相似性（例如，积极支持和客户健康监测）。

 他们没有有效地绘制同义词或相关术语（例如，预防和保留策略，或客户提高和扩张收入）。


我正在寻找改善匹配过程的建议或替代方法。具体：

 是否有更好的技术或模型来捕获语义相似性和上下文感知匹配？

 我如何有效地处理该域中的同义词和相关概念？

 是否有任何可以帮助解决此问题的预培训模型或数据集？

]]></description>
      <guid>https://stackoverflow.com/questions/79468885/improving-job-description-to-candidate-attribute-matching-for-talent-acquisition</guid>
      <pubDate>Wed, 26 Feb 2025 07:43:45 GMT</pubDate>
    </item>
    <item>
      <title>LSTM培训是否在恢复学习后重置？</title>
      <link>https://stackoverflow.com/questions/79461981/does-lstm-training-reset-after-resuming-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79461981/does-lstm-training-reset-after-resuming-learning</guid>
      <pubDate>Sun, 23 Feb 2025 21:11:38 GMT</pubDate>
    </item>
    <item>
      <title>LLM Studio无法下载错误：无法获得本地发行人证书</title>
      <link>https://stackoverflow.com/questions/78379820/llm-studio-fail-to-download-model-with-error-unable-to-get-local-issuer-certif</link>
      <description><![CDATA[在LLM Studio中，当我尝试下载任何模型时，我将面临以下错误：
下载失败：无法获取本地发行人证书
  &lt;img alt =“在此处输入图像描述” src =“ https：///i.sstatic.net/wkmzi.png”]]></description>
      <guid>https://stackoverflow.com/questions/78379820/llm-studio-fail-to-download-model-with-error-unable-to-get-local-issuer-certif</guid>
      <pubDate>Wed, 24 Apr 2024 16:03:26 GMT</pubDate>
    </item>
    <item>
      <title>在编译时，我该如何解决问题？（depthwise_conv.cc）</title>
      <link>https://stackoverflow.com/questions/78125296/how-can-i-solve-the-problem-during-mbed-compiledepthwise-conv-cc</link>
      <description><![CDATA[我正在研究使用Tinyml Book的机器学习。
我正在尝试编译，但它不起作用。
问题的情况如下：
本书提出了以下过程。
  make -f tensorflow/lite/micro/tools/make/makefile \
target = mbed tags =; cmsis-nn disco_f746ng; generate_micro_speech_mbed_project
 
目录。
  CD TensorFlow/Lite/Micro/Tools/Make/gen/Mbed_cortex-M4/prj/micro_speech/mbed
 
配置MBED盗贼根。
  mbed config root。
 
 mbed部署
  MBED部署
 
修改MBED配置文件以使用C ++11。
  python3 -c&#39;导入fileInput，glob;
for glob.glob中的文件名（“ mbed-os/tools/profiles/*。json＆quot”）：
    对于fileInput.input中的行（文件名，Inplace = true）：
        print(line.replace(&quot;\&quot;-std=gnu++98\&quot;&quot;,&quot;\&quot;-std=c++11\&quot;, \&quot;-fpermissive\&quot;&quot;))&#39;

 
和编译
  mbed compile -m disco_f746ng -t gcc_arm
 
但是，部署过程中存在一些问题。在部署过程中，发生了问题。在寻找解决方案时，我找到了一个建议，以修改make命令如下。
  make -f tensorflow/lite/micro/tools/make/makefile \
target = mbed tags =; cmsis-nn disco_f746ng; generate_micro_speech_mbed_project
 
进行修改后，我以相同的方式进行了编译过程，但遇到了以下错误。
 编译[82.7％]：depthwise_conv.cc
[错误] depthwise_conv.cc@178,9：从&#39;int&#39;到&#39;const cmsis_nn_dims*&#39;[-fpermissive]
[错误] depthwise_conv.cc@178,22：从&#39;int&#39;到&#39;const cmsis_nn_dims*&#39;[-fpermissive]
[error] depthwise_conv.cc@178,49：太多的参数无法函数&#39;int32_t arm_depthwise_conv_s8_opt_get_get_buffer_size（const cmsis_nn_dims*，const cmsis_nnn_dims*）&#39;
[错误] depthwise_conv.cc@184,34：无法将&#39;const&#39;consed char*&#39;转换为&#39;const cmsis_nn_context*&#39;
[错误] depthwise_conv.cc@195,9：在此范围中未声明&#39;arm_math_success&#39;；您的意思是&#39;ARM_MATH_DSP&#39;吗？
[错误] depthwise_conv.cc@184,34：无法将&#39;const&#39;consed char*&#39;转换为&#39;const cmsis_nn_context*&#39;
[error] depthwise_conv.cc@200,34：无法将&#39;const&#39;consed char*&#39;转换为&#39;const cmsis_nn_context*&#39;
[error] depthwise_conv.cc@212,9：在此范围中未声明&#39;arm_math_success&#39;；您的意思是&#39;ARM_MATH_DSP&#39;吗？
[error] depthwise_conv.cc@200,34：无法将&#39;const&#39;consed char*&#39;转换为&#39;const cmsis_nn_context*&#39;
[error] depthwise_conv.cc@272,5：&#39;arm_depthwise_conv_u8_basic_ver1&#39;在此范围中未声明；您的意思是&#39;ARM_DEPTHWIES_CONV_FAST_S16&#39;吗？
[错误]&#39;_queue.simplequeue&#39;对象没有属性&#39;队列&#39;
[mbed]错误：/usr/bin/python3＆quot返回的错误。
       代码：1
       路径：＆quot/home/ghjeon/tensorflow-lite/tensorflow/lite/micro/tools/make/gen/gen/mbed_cortex-m4/prj/prj/micro_speech/mbed;
       Command: &quot;/usr/bin/python3 -u /home/ghjeon/tensorflow-lite/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . -build ./build/disco_f746ng/gcc_arm&quot;
       提示：您可以用“ -v”重试最后一个命令。详细的详细输出
---

 
我无法解决这个问题。我一直无法解决这个问题2天。我要提前感谢任何可以提供帮助的人。
  [错误]&#39;_Queue.simplequeue&#39;对象没有属性&#39;queue&#39;
 
我已经看到了信息，表明可以使用Python 2.7解决上述错误。但是，我不确定这是否允许使用CLI1。因为ARM建议Cli1需要Python 3.7.x版本。]]></description>
      <guid>https://stackoverflow.com/questions/78125296/how-can-i-solve-the-problem-during-mbed-compiledepthwise-conv-cc</guid>
      <pubDate>Fri, 08 Mar 2024 02:44:08 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：不可用的类型：pd.get_dummies的“系列”</title>
      <link>https://stackoverflow.com/questions/70617092/typeerror-unhashable-type-series-for-pd-get-dummies</link>
      <description><![CDATA[我试图在我拥有的数据框中的某些名义数据上使用 pd.get_dummies （从Kaggle回归）。我将所有名义类别分为列名列表，&#39;obj_nominal&#39;。
我打电话
  pd.get_dummies（df，columns = obj_nominal）
 
我遇到了错误：
  typeError：不可用的类型：&#39;系列&#39;。
 
到目前为止，我唯一完成的预处理是删除数据集中的空值。我还尝试使用sklearn  onehotencoder ，并且会产生相同的错误。
我还尝试使用：进行单独的数据帧
  x = df.iloc [：,, obj_nominal]
 
和在数据框架上通过get_dummies：
  pd.get_dummies（data = x）
 
但仍然没有运气... 
The data is downloadable at https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data]]></description>
      <guid>https://stackoverflow.com/questions/70617092/typeerror-unhashable-type-series-for-pd-get-dummies</guid>
      <pubDate>Fri, 07 Jan 2022 05:51:22 GMT</pubDate>
    </item>
    <item>
      <title>来自_logits = try或false的含义是什么？</title>
      <link>https://stackoverflow.com/questions/55290709/what-does-from-logits-true-or-false-mean-in-sparse-categorical-crossentropy-of</link>
      <description><![CDATA[在Tensorflow 2.0中，
有一个称为的损失功能
  tf.keras.losses.sparse_categorical_crossentropy（标签，目标，from_logits = false）
 
设置 from_logits = true 或 false ？之间有什么区别
我的猜测是，当传入值是logits时，您将从_logits = true设置，并且如果传入值是概率（通过softmax等），则您只需设置from_logits = false（这是默认设置）。。
但是为什么？损失只是一些计算。为什么它需要通过其传入价值而有所不同？
我还在Google的Tensorflow教程中看到
 htttps://wwwww.tensorflow.org/alpha/alpha/alpha/tutorials/tutorials/sequences/sequences/sequences/sequences/textex_gentex_gentex_genert_genert_genert_generat_generatex_generatex_generatex_generatex_generatex_generatect_generation    &gt;
即使最后一层的传入值是logits，它也不会从_logits = true设置。
这是代码
 @tf.function
def train_step（INP，目标）：
  用tf.gradienttape（）作为磁带：
    预测=模型（INP）
    损失= tf.reduce_mean（
        tf.keras.losses.sparse_categorical_crossentropy（目标，预测））
  grads = tape.Gradient（损失，模型。Trainable_variables）
  优化器。Apply_gradients（zip（grads，model.trainable_variables））

  回报损失
 
模型为
 模型= tf.keras.Sequeential（[[[
    tf.keras.layers.embedding（vocab_size，embedding_dim， 
                              batch_input_shape = [batch_size，none]），），
    tf.keras.layers.lstm（rnn_units， 
                        return_sequences = true， 
                        状态= true， 
                        recurrent_initializer =&#39;glorot_uniform&#39;），
    tf.keras.layers.dense（vocab_size）
  ）））
 
没有softmax的最后一层。
（另外，在教程的另一部分中，它设置了 from_logits = true ）
所以，我是否将其设置为 true ？都没有关系？]]></description>
      <guid>https://stackoverflow.com/questions/55290709/what-does-from-logits-true-or-false-mean-in-sparse-categorical-crossentropy-of</guid>
      <pubDate>Thu, 21 Mar 2019 23:26:35 GMT</pubDate>
    </item>
    <item>
      <title>文档分析和标记[封闭]</title>
      <link>https://stackoverflow.com/questions/5107371/document-analysis-and-tagging</link>
      <description><![CDATA[假设我有一堆我想标记，分类等的论文（数千）。理想情况下，我想通过手动对几百个进行分类/标记来训练   ，然后让事物放松。 。
您会推荐哪些资源（书籍，博客，语言）进行此类任务？  我的一部分认为这非常适合 href =“ http://en.wikipedia.org/wiki/latent_semantic_analysis” rel =“ nofollow noreferrer”&gt;潜在的语义分析，但我对几个 ruby​​   gems 。。
贝叶斯分类器可以解决这样的事情吗？  我应该更多地研究语义分析/自然语言处理吗？  或者，我应该只是在寻找关键字密度和映射吗？]]></description>
      <guid>https://stackoverflow.com/questions/5107371/document-analysis-and-tagging</guid>
      <pubDate>Thu, 24 Feb 2011 16:20:43 GMT</pubDate>
    </item>
    </channel>
</rss>