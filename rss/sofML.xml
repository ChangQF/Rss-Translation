<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 07 Mar 2024 15:13:30 GMT</lastBuildDate>
    <item>
      <title>训练时验证损失没有变化</title>
      <link>https://stackoverflow.com/questions/78122336/validation-loss-not-varying-while-training</link>
      <description><![CDATA[我正在训练一个简单的神经网络，用于对具有约 2k 个查询的数据集进行查询评级，并且它们的评级以 CSV 的形式给出。我观察到，训练时验证损失没有变化，而且它为所有查询生成相同的输出（4. 最多 6 个小数位的评级相同）。为什么会发生这种情况？
我尝试更改学习率、层数、训练和测试数据的随机分割以及不同的超参数，但仍然没有变化。我什至尝试使用正则化，它没有改变任何东西。
导入重新
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.metrics 导入mean_absolute_error
将张量流导入为 tf
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入密集、嵌入、扁平化
从tensorflow.keras.preprocessing.text导入Tokenizer
从tensorflow.keras.preprocessing.sequence导入pad_sequences
从 nltk.corpus 导入停用词
从 nltk 导入 pos_tag
从 nltk.tokenize 导入 word_tokenize

REPLACE_BY_SPACE_RE = re.compile(&#39;[/(){}\[\]\|@,;]&#39;)
BAD_SYMBOLS_RE = re.compile(&#39;[^0-9a-z #+_]&#39;)
STOPWORDS = set(stopwords.words(&#39;english&#39;))

train_data = pd.read_csv(“train.csv”)

X = train_data[&#39;查询&#39;]
y = train_data[&#39;分数&#39;]

def clean_text(文本):
    文本 = re.sub(REPLACE_BY_SPACE_RE, &#39; &#39;, 文本)
    文本 = re.sub(BAD_SYMBOLS_RE, &#39;&#39;, 文本)
    单词 = text.lower().split()
    clean_text = &#39; &#39;.join(words)
    返回已清理的文本

X = X.应用(clean_text)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

分词器 = 分词器()
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)

X_train_padded = pad_sequences(X_train_seq)
X_val_padded = pad_sequences(X_val_seq, maxlen=X_train_padded.shape[1])

def pos_tagging（文本）：
    标记 = word_tokenize(文本)
    pos_tags = pos_tag(令牌)
    返回 pos_tags

X_train_pos = X_train.apply(pos_tagging)
X_val_pos = X_val.apply(pos_tagging)

常数因子 = 2.0

对于 i，枚举中的 pos_tags(X_train_pos)：
    对于单词，pos_tags 中的 pos：
        如果 pos == &#39;WP&#39; 或 pos == &#39;WRB&#39;:
            word_index = tokenizer.word_index.get(word.lower(), 0)
            如果单词索引！= 0：
                X_train_padded[i][X_train_padded[i] == word_index] *= Constant_factor

对于 i，枚举中的 pos_tags(X_val_pos)：
    对于单词，pos_tags 中的 pos：
        如果 pos == &#39;WP&#39; 或 pos == &#39;WRB&#39;:
            word_index = tokenizer.word_index.get(word.lower(), 0)
            如果单词索引！= 0：
                X_val_padded[i][X_val_padded[i] == word_index] *= Constant_factor

定标器=标准定标器()
y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_val_scaled = scaler.transform(y_val.values.reshape(-1, 1)).flatten()

模型=顺序（[
    嵌入（input_dim = len（tokenizer.word_index）+ 1，output_dim = 50，input_length = X_train_padded.shape [1]），
    展平（），
    密集（64，激活=&#39;relu&#39;），
    密集（1，激活=&#39;线性&#39;）
]）

model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_absolute_error&#39;)
model.fit(X_train_padded，y_train_scaled，epochs=10，batch_size=32，validation_data=(X_val_padded，y_val_scaled))

Predictions_scaled = model.predict(X_val_padded).flatten()
预测=scaler.inverse_transform(predictions_scaled)
mae = Mean_absolute_error(y_val, 预测)

print(f“验证集的平均绝对误差：{mae}”)。

这是我的代码，我认为给予更多权重“什么、谁、何时” wh- 词可能会帮助我找到更好的查询。有人可以告诉我该怎么做才能改变验证损失吗？即使损失很高，如何确保输出针对不同的输入而变化？]]></description>
      <guid>https://stackoverflow.com/questions/78122336/validation-loss-not-varying-while-training</guid>
      <pubDate>Thu, 07 Mar 2024 15:01:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Val 损失没有显示？如何显示它然后用训练损失绘制它</title>
      <link>https://stackoverflow.com/questions/78122308/why-val-loss-is-not-showing-how-to-display-it-then-plot-it-with-training-loss</link>
      <description><![CDATA[在数据集上微调 pytorch llm(IDEFICS9b) 后，训练结果未显示 val 损失，如何收集它，然后用训练损失绘制它？
training_args = TrainingArguments(
    output_dir=f“{model_name}-vqa1”，
    学习率=3e-5，
    fp16=正确，
    per_device_train_batch_size=16，
    per_device_eval_batch_size=16，
    梯度累积步数=1，
    dataloader_pin_memory=False,
    save_total_limit=3,
    评价_策略=“步骤”，
    save_strategy=&quot;步骤&quot;,
    保存步骤=10，
    评估步骤=10，
    记录步骤=10，
    最大步数=1000，
    删除_未使用的列=假，
    Push_to_hub=假,
    label_names=[“标签”],
    load_best_model_at_end=真，
    report_to=无，
    optim=&quot;paged_adamw_8bit&quot;,
）
教练=教练（
    型号=型号，
    参数=训练参数，
    train_dataset=train_ds,
    eval_dataset=eval_ds,
）

train_results = trainer.train()

训练结果如下：TrainOutput(global_step=10,training_loss=2.2117252349853516,metrics={&#39;train_runtime&#39;: 15.7995, &#39;train_samples_per_second&#39;: 10.127, &#39;train_steps_per_second&#39;: 0.633, &#39;total_flos&#39;: 470670709819 392.0, &#39;训练损失&#39; : 2.2117252349853516, &#39;纪元&#39;: 0.02})]]></description>
      <guid>https://stackoverflow.com/questions/78122308/why-val-loss-is-not-showing-how-to-display-it-then-plot-it-with-training-loss</guid>
      <pubDate>Thu, 07 Mar 2024 14:56:29 GMT</pubDate>
    </item>
    <item>
      <title>创建机器学习模型和部署[关闭]</title>
      <link>https://stackoverflow.com/questions/78121452/creating-a-machine-learning-model-and-deployment</link>
      <description><![CDATA[我需要创建一个机器学习模型并在测试其性能后部署它
我还没有尝试过，我需要了解如何创建一个可以解决科学、工程和技术不同领域的各种现实生活问题的模型]]></description>
      <guid>https://stackoverflow.com/questions/78121452/creating-a-machine-learning-model-and-deployment</guid>
      <pubDate>Thu, 07 Mar 2024 12:39:23 GMT</pubDate>
    </item>
    <item>
      <title>分析信用卡欺诈检测模型中的特征重要性[关闭]</title>
      <link>https://stackoverflow.com/questions/78121363/analyzing-feature-importance-in-credit-card-fraud-detection-models</link>
      <description><![CDATA[我最近完成了一个信用卡欺诈检测项目，非常感谢社区的反馈。
数据集概述：
该项目使用的数据集包含 3075 个条目和 12 列，包括“Merchant_id”、“Transaction_amount”和“isFradulent”等。删除了填充缺失值的“TransactionDate”列。该数据集混合了数值和分类数据，且“isFradulent”类别分布不平衡。
使用的算法：
使用了 3 种机器学习算法：

支持向量机(SVM)
K 最近邻（KNN）
决策树

为了评估我使用的各个功能的重要性：

相关性分析
模型中特征的重要性（SVM、KNN、决策树）

为了有效传达功能重要性，使用了以下策略：

混淆矩阵
学习曲线
GridSearchCV 结果
还重点关注准确率、召回率和 F1 分数的清晰解释，以帮助理解。

-可以应用哪些附加技术或工具来增强欺诈检测场景中的特征重要性评估？
- 在解释特征重要性结果时是否有任何常被忽视的陷阱或注意事项？
-就所选算法而言，您是否认为有潜在的改进或突出功能重要性的替代方法？
我专门寻求见解来完善我的项目的功能重要性分析方面。关于上述问题的任何反馈、建议或建议都非常有价值。]]></description>
      <guid>https://stackoverflow.com/questions/78121363/analyzing-feature-importance-in-credit-card-fraud-detection-models</guid>
      <pubDate>Thu, 07 Mar 2024 12:26:26 GMT</pubDate>
    </item>
    <item>
      <title>为新数据集创建梯度下降模型时出错</title>
      <link>https://stackoverflow.com/questions/78120856/error-while-creating-gradient-descent-model-for-a-new-dataset</link>
      <description><![CDATA[导入 pandas 作为 pd

文件 = pd.read_excel(&#39;slr06.xlsx&#39;)
＃ 数据
x = pd.DataFrame(文件, 列=[&#39;X&#39;])
y = pd.DataFrame(文件, 列=[&#39;Y&#39;])
＃ 参数
w = 0.0
b = 0.0
# 超参数
学习率 = 0.01

# 创建梯度下降

def Descend(x, y, w, b, 学习率):
    dl_dw = 0.0
    dl_db = 0.0
    n = x.形状[0]
    # 损失 = (y-yhat)**2
    对于 zip(x, y) 中的 xi, yi：
        dl_dw = -2*xi*(yi-(w*xi+b))
        dl_db = -2*(yi-(w*xi+b))
    # 进行更新
    w = w - 学习率*(1/n)*dl_dw
    b = b - 学习率*(1/n)*dl_db
    
    返回w,b
# 迭代更新
对于范围（500）内的纪元：
    w, b = 下降(x, y, w, b, 学习率)
    yhat = w * x + b
    损失 = np.divide(np.sum((y - yhat)**2, axis=0), x.shape[0])
    print(f&quot;{epoch} 损失是 {loss} 参数 w: {w} | b:{b}&quot;)


我是机器学习新手，在使用数据集进行练习时遇到了错误
这是数据集的图片 - 
这是错误图片 - 
我正在创建一个梯度下降模型，我之前已经使用不同的数据集练习过该模型。我尝试使用随机数据来练习并更好地理解梯度下降]]></description>
      <guid>https://stackoverflow.com/questions/78120856/error-while-creating-gradient-descent-model-for-a-new-dataset</guid>
      <pubDate>Thu, 07 Mar 2024 11:08:58 GMT</pubDate>
    </item>
    <item>
      <title>apt 安装的 cudnn 在终端之间不持久</title>
      <link>https://stackoverflow.com/questions/78120833/apt-installed-cudnn-not-persisting-between-terminaks</link>
      <description><![CDATA[安装 nvidia cudnn
sudo apt install nvidia-cudnn nvidia-cuda-toolkit

似乎没有在 shell 和重新启动 PC 之间保留某些变量：它在安装时工作正常，但在启动新 shell 时，如果程序需要 cudnn，则无法找到共享库。
操作系统是 Ubuntu 23.10，使用 Fish 作为我的主要 shell
重新安装可以解决此问题，但似乎不太理想]]></description>
      <guid>https://stackoverflow.com/questions/78120833/apt-installed-cudnn-not-persisting-between-terminaks</guid>
      <pubDate>Thu, 07 Mar 2024 11:05:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pyspark Pipelines 进行特征选择</title>
      <link>https://stackoverflow.com/questions/78120573/feature-selection-using-pyspark-pipelines</link>
      <description><![CDATA[我一直在查看 ML 模型的 Spark 文档，并在查看开箱即用的功能选择器时，我发现有一些选项。在我经常使用的数据集中，我混合了连续数据和分类数据以及用于分类的二进制目标。
最接近的似乎是单变量特征选择器，但文档表明（如下所示）它不能与连续特征和分类特征的混合一起使用，而应该是其中之一。

&lt;标题&gt;

特征类型
标签类型
评分函数


&lt;正文&gt;

分类
分类
卡方 (chi2)


连续
分类
方差分析测试（f_classif）


连续
连续
F值（f_regression）



我目前正在分析功能重要性并希望选择我的功能，但想知道 Spark 是否有任何功能可以满足此要求？]]></description>
      <guid>https://stackoverflow.com/questions/78120573/feature-selection-using-pyspark-pipelines</guid>
      <pubDate>Thu, 07 Mar 2024 10:30:30 GMT</pubDate>
    </item>
    <item>
      <title>sklearn.multiclass.OneVsRestClassifier 中的回调</title>
      <link>https://stackoverflow.com/questions/78119978/callbacks-in-sklearn-multiclass-onevsrestclassifier</link>
      <description><![CDATA[我想使用回调和 eval_set 等。
但我有一个问题：
from sklearn.multiclass import OneVsRestClassifier
导入lightgbm

&lt;前&gt;&lt;代码&gt;详细 = 100
参数 = {
    “目标”：“二元”，
    “n_估计器”：500，
    “详细”：0
}
适合参数= {
    “eval_set”：eval_数据集，
    “回调”：[CustomCallback（详细）]
}

clf = OneVsRestClassifier(lightgbm.LGBMClassifier(**params))
clf.fit(X_train, y_train, **fit_params)

我如何将 fit_params 交给我的估算器？我明白
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- --------------------------
---&gt; 13 clf.fit(X_train, y_train, **fit_params)

TypeError：OneVsRestClassifier.fit() 得到意外的关键字参数“eval_set”
]]></description>
      <guid>https://stackoverflow.com/questions/78119978/callbacks-in-sklearn-multiclass-onevsrestclassifier</guid>
      <pubDate>Thu, 07 Mar 2024 08:59:29 GMT</pubDate>
    </item>
    <item>
      <title>使用目标列识别机器学习问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78119976/identify-machine-learning-problem-using-target-column</link>
      <description><![CDATA[我正在努力使用给定的输入数据和选择的目标列自动定义预测类型。
如何区分目标列是多类分类还是二类分类或回归？
我尝试阅读来自 DSML 平台的各种实现的博客，但需要一些清晰度。]]></description>
      <guid>https://stackoverflow.com/questions/78119976/identify-machine-learning-problem-using-target-column</guid>
      <pubDate>Thu, 07 Mar 2024 08:59:26 GMT</pubDate>
    </item>
    <item>
      <title>如何用奇数样本大小批量训练神经网络？</title>
      <link>https://stackoverflow.com/questions/78119974/how-to-train-nn-in-batches-with-odd-examples-size</link>
      <description><![CDATA[我是神经网络领域的新手，正在使用 pytorch 进行一些训练。
我决定做一个简单的普通神经网络。
我使用了一个包含 2377 个数字特征和 6277 个示例的个人数据集。
我的第一次尝试是让神经网络预测每个示例，因此伪代码如下所示
对于范围内的 i(...)：
    X = ... # 特征
    y = ... # 结果
    y_pred = 模型(X[i])
    损失=标准(y_pred, y)

    y_pred.size # [1,1]
    y.尺寸#[1,1]

每个时期大约需要 10 秒，我决定使用小批量来改进它。
所以我在开始时定义了批量大小，Pytorch 中的神经网络是这样定义的
&lt;前&gt;&lt;代码&gt;batch_size = 30
n_inputs = X.size[1] #2377

## 2 个隐藏层
模型 = nn.Sequential(
    nn.Linear(n_inputs, 1024),
    ReLU(),
    nn.线性(1024, 512),
    ReLU(),
    nn.线性(512, 356),
    ReLU(),
    nn.Linear(356,batch_size),
    ReLU(),
）

然后我分批进行训练
对于范围（5）内的纪元：
    总损失 = 0
    排列 = torch.randperm(X.size()[0])
    对于范围内的 i（0，X.size（）[0]，batch_size）：
        优化器.zero_grad()
        索引 = 排列[i:i+batch_size]
        batch_x, batch_y = x[索引], y[索引]

        ypred = 模型(batch_x)
        损失=标准(ypred,batch_y)
        总损失 += loss.item()
        
        ## 更新权重
        loss.backward()
        优化器.step()

现在的问题是我的神经网络总是输出 100 个值但最后的批量大小可能会有所不同。
事实上，如果我选择 100 作为批量大小，最后一批将由 77 个示例组成 (6277%100)。
我确信有一种方法可以解决这个问题，并且我的结构中有一个错误，但我看不到它。
您能帮助我概括批量训练以处理任意数量的示例和批量大小吗？]]></description>
      <guid>https://stackoverflow.com/questions/78119974/how-to-train-nn-in-batches-with-odd-examples-size</guid>
      <pubDate>Thu, 07 Mar 2024 08:58:57 GMT</pubDate>
    </item>
    <item>
      <title>加载图像边界框输出相同大小的错误</title>
      <link>https://stackoverflow.com/questions/78118283/loading-image-bounding-boxes-outputs-equal-size-error</link>
      <description><![CDATA[我正在尝试为我的数据集创建一个 PyTorch 数据加载器。每个图像都有一定数量的汽车和每个汽车的边界框，但并非所有图像都具有相同数量的边界框。
您可能无法运行它，但这里有一些信息。
这是我的数据加载器
类 AGR_Dataset（数据集）：
    def __init__(self,annotations_root,img_root,transform=None):
        ”“”
        论据：
            comments_root（字符串）：带有注释的 csv 文件的路径。
            img_root（字符串）：包含所有图像的目录。
            变换（可调用，可选）：要应用的可选变换
                在样品上。
        ”“”
        self.annotations_root = 注释_root
        self.img_root = img_root
        self.transform = 变换

    def __len__(自身):
        返回 len(self.annotations_root)
    
    def __getitem__(self, idx):
        # idx 可能是索引或图像名称，我认为图像 naem
        如果 torch.is_tensor(idx):
            idx = idx.tolist()
        
        idx_name = os.listdir(self.img_root)[idx]
        # 打印(idx_name)
        
        img_name = os.path.join(self.img_root, idx_name)
        Comments_data = os.path.join(self.annotations_root, f&quot;{idx_name.removesuffix(&#39;.jpg&#39;)}.txt&quot;)
        # print(img_name, 注释数据)

        图像 = io.imread(img_name)

        以 open(annotation_data, &#39;r&#39;) 作为文件：
            行= file.readlines()
            图像数据 = []
            img_标签 = []
            对于行中行：
                line = line.split(&#39;,&#39;)
                line = [i.strip() for i in line]
                line = [float(num) for num in line[0].split()]
                img_labels.append(int(行[0]))
                img_data.append(行[1:])

        框 = tv_tensors.BoundingBoxes(img_data, format=&#39;CXCYWH&#39;, canvas_size=(image.shape[0], image.shape[1]))

        # 样本 = {&#39;image&#39;: 图像, &#39;bbox&#39;: 盒子, &#39;labels&#39;: img_labels}
        样本= {&#39;image&#39;：图像，&#39;bbox&#39;：盒子}

        如果自我变换：
            样本 = self.transform(样本)

        打印（样本[&#39;图像&#39;].shape）
        打印（样本[&#39;bbox&#39;].shape）
        # print(样本[&#39;标签&#39;].shape)
        返回样品

我运行转换并创建数据加载器
data_transform = v2.Compose([
    v2.ToImage(),
    # v2.调整大小(680),
    v2.RandomResizedCrop(大小=(680, 680), 抗锯齿=True),
    # v2.ToDtype(torch.float32,scale=True),
    v2.ToTensor()
]）

Transformed_dataset = AGR_Dataset(f&#39;{annotations_path}/test/&#39;,
                        f&#39;{img_path}/测试/&#39;,
                        变换=数据变换）

数据加载器=数据加载器(transformed_dataset,batch_size=2,
                        洗牌=假，num_workers=0）

然后我尝试用它迭代它，并最终使用边界框查看和图像。
对于 i，枚举（dataloader）中的示例：
    打印（我，样本）
    print(i, 样本[&#39;image&#39;].size(), 样本[&#39;bbox&#39;].size())

    如果我==4：
        休息

批处理大小为 1 时，它运行正常，批处理大小为 2 时，出现此错误
torch.Size([3, 680, 680])
火炬.Size([12, 4])

火炬.Size([3, 680, 680])
火炬.Size([259, 4])

RuntimeError: 堆栈期望每个张量大小相等，但在条目 0 处得到 [12, 4]，在条目 1 处得到 [259, 4]


我认为这是由于边界框数量不相等造成的，但如何克服这个问题？
我的变换中需要 ToTensor 吗？我开始认为我不这样做，因为 v2 使用 ToImage()，而 ToTensor 正在被贬值。

如有任何其他意见或帮助，我们将不胜感激。
我不确定如何创建一个工作示例，我会继续尝试。
我尝试过的
我尝试通过注释数据加载器中的 tv_tensors.BoundingBoxes 行来不将边界框加载为张量，但由于某种原因，我的调整大小无法正常工作。
我刚刚尝试在数据加载器中像这样分割框和图像
样本 = 图像
    目标= {&#39;bbox&#39;：盒子，&#39;标签&#39;：img_labels}

运气不好]]></description>
      <guid>https://stackoverflow.com/questions/78118283/loading-image-bounding-boxes-outputs-equal-size-error</guid>
      <pubDate>Thu, 07 Mar 2024 01:30:38 GMT</pubDate>
    </item>
    <item>
      <title>运行 Autotrain Advanced 时出现 UnicodeDecodeError [关闭]</title>
      <link>https://stackoverflow.com/questions/78115600/getting-unicodedecodeerror-while-running-autotrain-advanced</link>
      <description><![CDATA[我使用 Llama-2-7B-Chat-Hf 模型作为基础，Orca Math Word Problems 作为训练数据。我遇到错误 - UnicodeDecodeError: &#39;utf-8&#39; 编解码器无法解码位置 7 中的字节 0x92: 无效的起始字节。
如何解决这个问题？
这是我尝试的图像 - 我尝试构建的图像&lt; /p&gt;
这是完整的错误：
回溯（最近一次调用最后一次）：
  文件“/app/env/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py”，第 428 行，在 run_asgi 中
    结果=等待应用程序（＃类型：忽略[func-returns-value]
  文件“/app/env/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py”，第 78 行，在 __call__ 中
    返回等待 self.app（范围，接收，发送）
  文件“/app/env/lib/python3.10/site-packages/fastapi/applications.py”，第 1106 行，在 __call__ 中
    等待超级（）.__call__（范围，接收，发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/applications.py”，第 122 行，在 __call__ 中
    等待 self.middleware_stack（范围，接收，发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/errors.py”，第 184 行，在 __call__ 中
    提高执行力
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/errors.py”，第 162 行，在 __call__ 中
    等待 self.app（范围，接收，_发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/sessions.py”，第 86 行，在 __call__ 中
    等待 self.app（范围，接收，send_wrapper）
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/exceptions.py”，第 79 行，在 __call__ 中
    提高执行力
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/exceptions.py”，第 68 行，在 __call__ 中
    等待 self.app（范围、接收、发送者）
  文件“/app/env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py”，第 20 行，在 __call__ 中
    提高e
  文件“/app/env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py”，第 17 行，在 __call__ 中
    等待 self.app（范围、接收、发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/routing.py”，第 718 行，在 __call__ 中
    等待route.handle（范围，接收，发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/routing.py”，第 276 行，在句柄中
    等待 self.app（范围、接收、发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/routing.py”，第 66 行，在应用程序中
    响应=等待函数（请求）
  文件“/app/env/lib/python3.10/site-packages/fastapi/routing.py”，第 274 行，在应用程序中
    raw_response = 等待 run_endpoint_function(
  文件“/app/env/lib/python3.10/site-packages/fastapi/routing.py”，第 191 行，在 run_endpoint_function 中
    返回等待 dependent.call(**值)
  文件“/app/env/lib/python3.10/site-packages/autotrain/app.py”，第 452 行，handle_form
    dset = AutoTrainDataset(**dset_args)
  文件“”，第 13 行，位于 __init__ 中
  文件“/app/env/lib/python3.10/site-packages/autotrain/dataset.py”，第 204 行，在 __post_init__ 中
    self.train_df, self.valid_df = self._preprocess_data()
  文件“/app/env/lib/python3.10/site-packages/autotrain/dataset.py”，第 213 行，位于 _preprocess_data
    train_df.append(pd.read_csv(文件))
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py”，第 1026 行，在 read_csv 中
    返回_read（文件路径或缓冲区，kwds）
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py”，第 620 行，在 _read 中
    解析器 = TextFileReader(filepath_or_buffer, **kwds)
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py”，第 1620 行，位于 __init__ 中
    self._engine = self._make_engine(f, self.engine)
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py”，第 1898 行，在 _make_engine 中
    返回映射[引擎](f, **self.options)
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py”，第 93 行，在 __init__ 中
    self._reader = parsers.TextReader(src, **kwds)
  文件“parsers.pyx”，第 574 行，位于 pandas._libs.parsers.TextReader.__cinit__ 中
  文件“parsers.pyx”，第 663 行，位于 pandas._libs.parsers.TextReader._get_header
  文件“parsers.pyx”，第 874 行，位于 pandas._libs.parsers.TextReader._tokenize_rows 中
  文件“parsers.pyx”，第 891 行，位于 pandas._libs.parsers.TextReader._check_tokenize_status
  文件“parsers.pyx”，第 2053 行，位于 pandas._libs.parsers.raise_parser_error
UnicodeDecodeError：“utf-8”编解码器无法解码位置 7 中的字节 0x92：起始字节无效
]]></description>
      <guid>https://stackoverflow.com/questions/78115600/getting-unicodedecodeerror-while-running-autotrain-advanced</guid>
      <pubDate>Wed, 06 Mar 2024 15:30:30 GMT</pubDate>
    </item>
    <item>
      <title>使用LSTM模型进行充电数据负荷预测的时间序列交叉验证</title>
      <link>https://stackoverflow.com/questions/78107902/time-series-cross-validation-for-load-forecast-of-charging-data-using-lstm-model</link>
      <description><![CDATA[我目前正在构建一个 LSTM 模型，以使用包含 24 个特征和 12 个月目标列的数据集来预测电动汽车的消耗量。数据采用时间序列格式，间隔为 15 分钟，我想应用时间序列交叉验证来维护时间依赖性。
这是我到目前为止为训练和测试拆分编写的代码：
# 定义用于训练的列
特征 = [&#39;小时&#39;, &#39;季度小时&#39;, &#39;工作日&#39;, &#39;cal_week&#39;, &#39;月份&#39;, &#39;周末&#39;,
                  &#39;季节&#39;，&#39;假期&#39;，&#39;下一个假期&#39;，&#39;bridge_day&#39;，&#39;学校假期&#39;，&#39;quarter_sin&#39;，&#39;quarter_cos&#39;，&#39;hour_sin&#39;，&#39;hour_cos&#39;，
                  &#39;weekday_sin&#39;、&#39;weekday_cos&#39;、&#39;month_sin&#39;、&#39;month_cos&#39;、&#39;cal_week_sin&#39;、&#39;cal_week_cos&#39;、&#39;temp_day_avg&#39;、&#39;humidity_day_avg&#39;、&#39;wind_speed&#39;]

# 提取特征和目标变量
X = Bosch_data[功能].值
y = Bosch_data[&#39;aggregate_conspiration_kWh&#39;].values


# 定义窗口大小
validation_window_size = 7 * 24 * 4 # 7天* 24小时* 4个季度/小时（15T）
test_window_size = 31 * 24 * 4 # 31天* 24小时* 4个季度/小时（15T）

# 将训练集定义为除验证集和测试集之外的所有内容
train_window_size = len(Bosch_data) - validation_window_size - test_window_size

# 将数据分为训练集、验证集和测试集
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=validation_window_size + 31 * 24 * 4, shuffle=False)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=31 * 24 * 4, shuffle=False)

我有两个具体问题：

扩展窗口交叉验证：如何将扩展窗口交叉验证应用于此训练、验证和测试拆分？

选择时间步长：我的数据集呈现每周模式，我想预测未来 30 天的消耗量。考虑到 15 分钟的时间间隔，在这种情况下我的理想时间步长应该是多少？如果我使用 15 分钟的时间间隔作为一个时间步长，则会出现错误。


我愿意接受有关如何解决这些问题并扩展我的时间序列交叉验证代码的建议。任何帮助或指导将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78107902/time-series-cross-validation-for-load-forecast-of-charging-data-using-lstm-model</guid>
      <pubDate>Tue, 05 Mar 2024 12:58:27 GMT</pubDate>
    </item>
    <item>
      <title>无监督自动编码器产生输出维度 - 不同大小数据集的批次数</title>
      <link>https://stackoverflow.com/questions/78107646/unsupervised-autoencoder-produce-output-dimensions-number-of-batches-for-diffe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78107646/unsupervised-autoencoder-produce-output-dimensions-number-of-batches-for-diffe</guid>
      <pubDate>Tue, 05 Mar 2024 12:12:37 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练 Tensorflow.Net 模型时，未将对象引用设置为对象的实例[重复]</title>
      <link>https://stackoverflow.com/questions/78107434/object-reference-not-set-to-an-instance-of-an-object-when-attempting-to-train-te</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78107434/object-reference-not-set-to-an-instance-of-an-object-when-attempting-to-train-te</guid>
      <pubDate>Tue, 05 Mar 2024 11:37:44 GMT</pubDate>
    </item>
    </channel>
</rss>