<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 20 Mar 2024 03:15:11 GMT</lastBuildDate>
    <item>
      <title>对同一数据集的不同子组进行迁移学习</title>
      <link>https://stackoverflow.com/questions/78190629/transfer-learning-on-different-subgroups-of-the-same-dataset</link>
      <description><![CDATA[我目前正在尝试根据回归任务的特定列中的值将原始数据集分为 6 个子组。每个子组中目标变量的分布非常相似。我的目标是通过首先对 5 个子组进行预训练，然后对最后一个子组进行微调来应用迁移学习。
对于预训练，我为每个子组设置了单独的训练、验证和测试集。预训练包括将5个子组的训练集和验证集结合起来，用它们来训练模型，验证模型，然后测量测试集上的损失。
随后，我使用预训练中的模型权重，并仅使用最终子组的训练集和验证集进行微调，并再次测量测试集上的损失。
但是，我遇到了一个问题，即我的模型对预训练数据过度拟合，导致微调过程中第一个周期的提前停止，因为微调集的验证误差会增加。
我希望我已经清楚地解释了我的问题。
我的方法正确吗？任何建议将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78190629/transfer-learning-on-different-subgroups-of-the-same-dataset</guid>
      <pubDate>Wed, 20 Mar 2024 02:45:43 GMT</pubDate>
    </item>
    <item>
      <title>如何优化这个二元分类 ML 模型以给出更小的损失？</title>
      <link>https://stackoverflow.com/questions/78190265/how-can-i-optimize-this-binary-classification-ml-model-to-give-smaller-loss</link>
      <description><![CDATA[我已经尝试优化这个模型有一段时间了，但我无法弄清楚如何减少损失和训练所需的时间。损失需要很长时间才能减少，并且不会低于 0.27（大约该值）。
这是我使用 Kaggle 上提供的泰坦尼克号数据集训练自己的第一个分类模型。
请帮忙！
#定义导入
进口火炬
从火炬导入 nn
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd
将 numpy 导入为 np
导入io

X= df2[[ &#39;年龄&#39;, &#39;性别&#39;, &#39;Sib_And_Parch&#39;, &#39;Pclass&#39;]].to_numpy()
X_train= torch.from_numpy(X).type(torch.float32).to(device)


def precision_fn(y_true,y_pred):
  正确 = torch.eq(y_true,y_pred).sum().item()
  acc = (正确/len(y_true))*100
  返回帐户

# 编写模型
类 SurvivalPredictor(nn.Module):
  def __init__(自身):
    超级().__init__()
    self.layer_1 = nn.Linear(in_features=4, out_features=16)
    self.layer_2 = nn.Linear(in_features=16, out_features=32)
    self.layer_3 = nn.Linear(in_features=32, out_features=8)
    self.layer_4 = nn.Linear(in_features=8, out_features=1)
    self.relu = nn.ReLU()

  def 前向（自身，x）：
    返回 self.layer_4(self.relu(self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x))))))))

model_0 = SurvivalPredictor().to(设备)


# 设置损失函数
loss_fn = nn.BCEWithLogitsLoss()

# 设置优化器
优化器= torch.optim.SGD(params= model_0.parameters(),
                          lr=0.001)

纪元=100001

对于范围内的纪元（纪元）：

  优化器.zero_grad()


  ＃训练
  model_0.train()

  #前向传递
  y_logits = model_0(X_train).squeeze()
  y_preds =torch.argmax(torch.sigmoid(y_logits)) # logits-&gt;预测概率 -&gt;预测标签

  # 计算损失
  损失 = loss_fn(y_logits,y_train)

  loss.backward()

  优化器.step()


  如果纪元％10000 == 0：
    print(f&quot;历元: {epoch} | 损失: {loss:.5f} &quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78190265/how-can-i-optimize-this-binary-classification-ml-model-to-give-smaller-loss</guid>
      <pubDate>Wed, 20 Mar 2024 00:25:23 GMT</pubDate>
    </item>
    <item>
      <title>通过标准定标器实现前瞻偏差？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78189717/look-ahead-bias-via-standardscaler</link>
      <description><![CDATA[我非常确定存在前瞻偏差，因为标准缩放器之前看到过测试数据。但事情是这样的，我用大量不同的配置和参数设置运行了这个，它们的表现都很负面，这让我确信，如果存在前瞻偏差，一切都会看起来令人难以置信，并且参数负载将非常积极事实并非如此。
这是代码，我尝试对其进行一些标记：
scaler = StandardScaler()
X_scaled = 缩放器.fit_transform(X)

# 用于交叉验证的 TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)

# GridSearch 的 LightGBM 参数
参数网格 = {
    &#39;num_leaves&#39;: [--, --],
    &#39;学习率&#39;: [--, --],
    &#39;n_estimators&#39;: [--, --]
}

# 模型初始化
lgbm = lgb.LGBMRegressor(force_col_wise=True)

# 带有 TimeSeriesSplit 的网格搜索
grid_search = GridSearchCV(估计器=lgbm，param_grid=param_grid，cv=tscv，评分=&#39;neg_mean_absolute_error&#39;，详细=1)
grid_search.fit(X_scaled, y)

最佳模型 = grid_search.best_estimator_

# 使用最佳模型和早期停止进行训练和评估


feature_importances = best_model.feature_importances_
sorted_idx = np.argsort（feature_importances）
plt.figure(figsize=(10, 8))
plt.barh(范围(len(sorted_idx))，feature_importances[sorted_idx]，align=&#39;center&#39;)
plt.yticks(范围(len(sorted_idx)), np.array(features)[sorted_idx])
plt.title(&#39;功能重要性&#39;)
plt.show()

# 预测方向变化精度
方向准确度分数 = []

mae_scores = []

对于 tscv.split(X_scaled) 中的 train_index、test_index：
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    y_pred = best_model.predict(X_test)
    Direction_pred = np.sign(y_pred[1:] - y_pred[:-1])
    Direction_actual = np.sign(y_test.values[1:] - y_test.values[:-1])
    方向精度 = np.mean(方向预测 == 方向实际)
    Direction_accuracy_scores.append(direction_accuracy)
    mae_scores.append(mean_absolute_error(y_test, y_pred))

Average_mae = np.mean(mae_scores)
print(f&#39;平均绝对误差 (MAE): {average_mae}&#39;)
    
# 绘制方向变化精度
plt.figure(figsize=(10, 6))
plt.plot(direction_accuracy_scores, 标记=&#39;o&#39;)
plt.title(&#39;方向变化预测精度&#39;)
plt.xlabel(&#39;分割&#39;)
plt.ylabel(&#39;准确度&#39;)
plt.show()

print(f&#39;平均方向变化预测精度: {np.mean(direction_accuracy_scores)}&#39;)

# 预测下一个值
next_value_prediction = best_model.predict(X_scaled[-1].reshape(1, -1))
print(f&#39;预测下一个值：{next_value_prediction[0]}&#39;)

现在事情是这样的。模型获得了我最好的 MAE，但它并没有高得令人怀疑，并且需要进行大量的调整和其他工作才能达到这个目标。当我稍微改变叶子或分割或 n_estimators 的数量时，模型会变得明显更差，这样性能大约有一半的时间是正确的（与随机一样好）。
我是否疯狂地认为，即使缩放器提供了前瞻偏差，它也不会是一个巨大的偏差？ 70% 的时间预测都是朝着正确的方向发展的（并非不切实际），当我更改配置或拆分时，预测值会下降到 48 左右。但我很担心，因为当我实现此功能时：
对于 tscv.split(X) 中的 train_index、test_index：
    # 将数据分成每次折叠的训练集和测试集
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # 在没有前瞻偏差的情况下缩放数据
    定标器=标准定标器()
    X_train_scaled = 缩放器.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test) # 根据训练数据缩放测试数据
    
    # 根据缩放的训练数据拟合模型
    grid_search.fit(X_train_scaled, y_train)
    
    # 根据缩放测试数据评估模型
    y_pred = grid_search.best_estimator_.predict(X_test_scaled)
    Direction_pred = np.sign(y_pred[1:] - y_pred[:-1])
    Direction_actual = np.sign(y_test.values[1:] - y_test.values[:-1])
    方向精度 = np.mean(方向预测 == 方向实际)
    Direction_accuracy_scores.append(direction_accuracy)
    mae_scores.append(mean_absolute_error(y_test, y_pred))

模型 MAE 急剧恶化。我只是一个搞砸了定标器并引入了巨大的前瞻偏差的傻瓜吗？还是有更好的方法来测试定标器是否引起了巨大的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78189717/look-ahead-bias-via-standardscaler</guid>
      <pubDate>Tue, 19 Mar 2024 21:23:05 GMT</pubDate>
    </item>
    <item>
      <title>如何按 ROW 而不是数组元素读取包含数组的镶木地板文件？</title>
      <link>https://stackoverflow.com/questions/78189710/how-to-read-a-parquet-file-with-arrays-by-row-instead-of-array-elements</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78189710/how-to-read-a-parquet-file-with-arrays-by-row-instead-of-array-elements</guid>
      <pubDate>Tue, 19 Mar 2024 21:21:26 GMT</pubDate>
    </item>
    <item>
      <title>我应该选择哪种机器学习方法？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78189637/which-machine-learning-method-should-i-choose</link>
      <description><![CDATA[表中有数据|时间|-|罐压|.传感器以 0.05 秒的增量发送压力读数信号。该表适用于常规情况和紧急情况（当压力超出允许标准时）。我想训练该模型，使其能够识别紧急情况和相关信号。我应该选择哪种方法，我该怎么做？该模型将是一种传入数据流的过滤器
我通过pandas编译了一个数据集，并使用google colab进行训练。]]></description>
      <guid>https://stackoverflow.com/questions/78189637/which-machine-learning-method-should-i-choose</guid>
      <pubDate>Tue, 19 Mar 2024 21:04:58 GMT</pubDate>
    </item>
    <item>
      <title>序列分类中的通道重要性</title>
      <link>https://stackoverflow.com/questions/78189215/channel-importance-in-sequence-classification</link>
      <description><![CDATA[我有一个 ONNX 模型，它接受输入 [1, 35, 4]，即 [batch_size, num_channels, seq_len]，并输出 [1 , 3]，即[batch_size, num_classes]。我需要的是为每个通道分配一个数字，告诉我它对于模型做出的预测有多重要。我想我会使用 shap 的 PermutationExplainer 来达到此目的，但我不确定如何让它知道我不关心 seq_len&lt; /代码&gt;.
我尝试这样做：
导入 onnxruntime 作为 ort
导入形状
将 numpy 导入为 np

# 加载 ONNX 模型
model_path = &#39;解释示例.onnx&#39;
sess = ort.InferenceSession(model_path)

# 定义模型函数来处理批处理
定义模型(x):
    x = x.reshape(-1, 35, 4).astype(np.float32)
    # 模型期望输入 [1, 35, 4] 并返回 [1, 3]
    输出 = [sess.run(None, {&#39;input&#39;: x[i:i+1]})[0] for i in range(x.shape[0])]
    返回 np.concatenate(输出，轴=0)

# 创建样本输入
X = np.random.rand(1000, 35, 4).astype(np.float32)

输出名称 = [f&quot;输出_{i}&quot;;对于范围 (3) 内的 i]
feature_names = [f“频道 {i}”对于我在范围（35）]

def masker_fn(掩码, x):
    # 问题，掩码形状为 (140,)，x 形状为 (35, 4)
    masked_x = x.copy()
    对于范围内的 i(x.shape[1])：
        如果掩码[i] == 0：
            masked_x[:, i, :] = 0
    返回 masked_x

# 使用通道的自定义掩码声明解释器
解释器= shap.PermutationExplainer（模型，masker_fn，feature_names=feature_names，output_names=output_names）

# 计算形状值
shap_values = 解释器(X)

但问题是发送到掩码器的掩码具有形状 (140,) 而不是 (35,)。我当然可以以某种方式合并跨 seq_len 维度的掩码，但我认为 1. 解释器完成了不必要的工作，2. 也许这会以某种方式破坏其内部算法。
我怎样才能正确地告诉它，不，没有 140 个功能，而是 35 个？]]></description>
      <guid>https://stackoverflow.com/questions/78189215/channel-importance-in-sequence-classification</guid>
      <pubDate>Tue, 19 Mar 2024 19:34:59 GMT</pubDate>
    </item>
    <item>
      <title>R 中插入符号的训练函数中的中心和比例因子预测器</title>
      <link>https://stackoverflow.com/questions/78189166/center-and-scale-factor-predictors-within-train-function-from-caret-in-r</link>
      <description><![CDATA[我在使用 R caret 包的 train 函数的 preProc 参数方面遇到问题。我想居中并缩放我的预测变量，但忽略因子列。当我在火车之外进行预处理时，它工作正常，但我希望在火车功能内进行预处理。我错过了什么吗？
下面是一个示例，其中在训练之外使用 preProcess 时忽略因子预测器。
df &lt;- data.frame(
    分数 = runif(1000, 80, 110),
    var1 = as.factor(样本(0:1, 1000, 替换 = TRUE)),
    var2 = runif(1000, 5, 25)
）
预处理(df[-1], method=c(&#39;center&#39;,&#39;scale&#39;))

由 1000 个样本和 2 个变量创建

预处理：
  - 居中 (1)
  - 被忽略 (1)
  - 缩放 (1)

这是我在火车内部使用 preProc 时发生的情况
df &lt;- data.frame(
    分数 = runif(1000, 80, 110),
    var1 = as.factor(样本(0:1, 1000, 替换 = TRUE)),
    var2 = runif(1000, 5, 25)
）
mod &lt;- 训练（得分 ~., 数据 = df,
             方法=“lm”，
             preProc = c(“中心”,“比例”))
mod$预处理

由 1000 个样本和 2 个变量创建

预处理：
  - 居中 (2)
  - 被忽略 (0)
  - 缩放 (2)
]]></description>
      <guid>https://stackoverflow.com/questions/78189166/center-and-scale-factor-predictors-within-train-function-from-caret-in-r</guid>
      <pubDate>Tue, 19 Mar 2024 19:25:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么具有 Cartpole 健身房环境的 stable_baselines3 模型通过 sutton_barto_reward 提高了平均剧集奖励？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78188062/why-is-my-stable-baselines3-model-with-cartpole-gym-environment-improving-mean-e</link>
      <description><![CDATA[当我运行此代码时，我看到剧集长度平均值不断增加（就像一个好的模型应该的那样），而剧集平均值奖励保持在 -1 不变，这就是 sutton_barto_reward 系统的工作原理。
导入体育馆
从 cartpole 导入 CartPoleEnv
从 stable_baselines3 导入 PPO
从 stable_baselines3.ppo.policies 导入 MlpPolicy

env = CartPoleEnv(sutton_barto_reward=True)

模型 = PPO(“MlpPolicy”, env, gamma=1, verbose=1)
model.learn(total_timesteps=30000)

但是，我不明白为什么会这样，因为在gymnasium的GitHub文档中的Cartpole代码中似乎没有使用任何折扣率。既然剧集的累积奖励始终相同，那么剧集长度平均值难道不应该没有任何改善吗？]]></description>
      <guid>https://stackoverflow.com/questions/78188062/why-is-my-stable-baselines3-model-with-cartpole-gym-environment-improving-mean-e</guid>
      <pubDate>Tue, 19 Mar 2024 16:02:07 GMT</pubDate>
    </item>
    <item>
      <title>关于空闲时间的数据集？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78187717/data-set-about-free-time</link>
      <description><![CDATA[我想到了一个项目 - 一种用于组织与朋友外出活动的人工智能机器人。其中一部分是实际创建，甚至搜索（如果有的话）预先标记的数据集（这就是 chatgpt 告诉我的 - 我是全新的）。
我在哪里可以找到它，或者如何创建它？]]></description>
      <guid>https://stackoverflow.com/questions/78187717/data-set-about-free-time</guid>
      <pubDate>Tue, 19 Mar 2024 15:11:34 GMT</pubDate>
    </item>
    <item>
      <title>我的 scikit-learn 代码序列正确吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78187495/is-my-scikit-learn-code-sequence-correct</link>
      <description><![CDATA[我已经构建了一个包含一些转换的管道并训练了一个 SVC 分类器。代码中步骤的顺序是否正确？
我正在使用此处找到的processed.cleveland.data数据集：https： //archive.ics.uci.edu/dataset/45/heart+disease。
将 pandas 导入为 pd
将 numpy 导入为 np
导入操作系统
从 pathlib 导入路径

从 sklearn.model_selection 导入 train_test_split
从 sklearn.model_selection 导入 StratifiedKFold
从 sklearn.model_selection 导入 cross_val_score

从 sklearn.compose 导入 ColumnTransformer
从 sklearn.pipeline 导入管道
从 sklearn.preprocessing 导入 OneHotEncoder
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.impute 导入 SimpleImputer

从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.svm 导入 SVC
url =“C:/Users/.../processedcleveland.data”
名称 = [&#39;年龄&#39;, &#39;性别&#39;, &#39;cp&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;exang&#39;, &#39;oldpeak&#39;, &#39;slope&#39;, &#39;ca&#39; , &#39;thal&#39;, &#39;num&#39;]
def getData():
        返回 pd.read_csv(url, sep=&#39;,&#39;, 名称=名称)

输入 = 获取数据()
打印（输入.info（））
打印（输入.描述（））

数组=输入.值
X = 数组[:,0:13]
y = 数组[:,13]

dataframe = pd.DataFrame.from_records(X)
数据帧[[1, 2, 5, 6, 8]] = 数据帧[[1, 2, 5, 6, 8]].astype(str)

打印(dataframe.info())

numeric_ix = dataframe.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns
categorical_ix = dataframe.select_dtypes(include=[&#39;object&#39;, &#39;bool&#39;]).columns

打印（数字_ix）
打印（分类_ix）
&#39;&#39;&#39;
t = [(&#39;cat0&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;), [1, 2, 5, 6, 8]), (&#39;cat1&#39;, OneHotEncoder(), categorical_ix), (&#39;num0&#39;, SimpleImputer(strategy) =&#39;中位数&#39;), numeric_ix), (&#39;num1&#39;, MinMaxScaler(), numeric_ix)]
col_transform = ColumnTransformer(变压器=t)

管道 = 管道(步骤=[(&#39;t&#39;, col_transform)])
# 将管道拟合到转换后的数据上
结果 = pipeline.fit_transform(dataframe)

打印（类型（pd.DataFrame.from_records（结果）））
打印（pd.DataFrame.from_records（结果）.to_string（））
&#39;&#39;&#39;
X_train、X_validation、Y_train、Y_validation = train_test_split(X、y、test_size=0.20、random_state=1)


categorical_impute = 管道([
    （“mode_impute”，SimpleImputer（missing_values = np.nan，策略=&#39;most_frequent&#39;）），
    (“one_hot”, OneHotEncoder())
]）

numeric_impute = 管道([
    （“num_mode_impute”，SimpleImputer（missing_values = np.nan，策略=&#39;中位数&#39;）），
    (“min_max”, StandardScaler())
]）

预处理器 = ColumnTransformer([
    (“cat_impute”, categorical_impute, categorical_ix),
    (“num_impute”, numeric_impute, numeric_ix)
]，余数=“直通”）


模型 = SVC(伽玛=&#39;自动&#39;)

kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)

pipeline = Pipeline(steps=[(&#39;prep&#39;, 预处理器), (&#39;m&#39;, model)])

cv_results = cross_val_score(管道, X_train, Y_train, cv=kfold, 评分=&#39;准确度&#39;)
print(&#39;%s: %f (%f)&#39; % (&quot;SVC: &quot;, cv_results.mean(), cv_results.std()))
# 结果 = preprocessor.fit_transform(dataframe)
# print(pd.DataFrame.from_records(结果).to_string())

分类器的效率很低。有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78187495/is-my-scikit-learn-code-sequence-correct</guid>
      <pubDate>Tue, 19 Mar 2024 14:38:47 GMT</pubDate>
    </item>
    <item>
      <title>这样分割数据会不会造成数据泄露？</title>
      <link>https://stackoverflow.com/questions/78187069/is-there-data-leakage-while-splitting-the-data-this-way</link>
      <description><![CDATA[以这种方式分割数据集时是否存在数据泄漏的可能性：
def split_dataset(ds, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=True):
    # 获取数据集大小
    数据集大小 = len(ds)

    # 计算分割大小
    train_size = int(train_ratio * dataset_size)
    val_size = int(val_ratio * 数据集大小)
    测试大小 = 数据集大小 - 训练大小 - val_size

    # 如果需要的话，打乱数据集
    如果随机播放：
        ds = ds.shuffle(dataset_size)

    # 分割数据集
    train_dataset = ds.take(train_size)
    val_dataset = ds.skip(train_size).take(val_size)
    test_dataset = ds.skip(train_size + val_size).take(test_size)

    返回train_dataset、val_dataset、test_dataset

训练深度学习模型
只是对模型的准确率感到惊讶，它超过了 98%。]]></description>
      <guid>https://stackoverflow.com/questions/78187069/is-there-data-leakage-while-splitting-the-data-this-way</guid>
      <pubDate>Tue, 19 Mar 2024 13:37:37 GMT</pubDate>
    </item>
    <item>
      <title>选择模型训练参数（和层）</title>
      <link>https://stackoverflow.com/questions/78184146/selecting-model-training-params-and-layers</link>
      <description><![CDATA[在代码中（下面从tensorflow.org“针对初学者的 Tensorflow 2 快速入门”复制/粘贴了所有许多变量，例如：

纪元数
批量大小
学习率
层（或层的组合）
每层的激活函数
损失函数
单位（密集层的参数）
比率（针对 Dropout 层）
等等...

除了“猜测和猜测”之外，是否还有一些受过教育的程序？测试”，以便找到“最佳”方案。参数组合 --- 或者甚至知道从哪个参数组合开始？
我是否缺乏“专家”所需要的一些知识（或信息）？或博士学位将为我提供，这将使我有能力选择“最好的”。参数（第一次）？
我当然明白最后一层 Dense(10) 的一些参数需要是 10 个单位来表示 10 个类...而且我知道 10,000 层可能不太好...所以我不是在谈论极端值或最后一个密集层中的 10 之类的东西，而是其他参数。
含义 - 作为示例 - 在下面的代码中，假设我更改了行中的单位
tf.keras.layers.Dense（32，激活=&#39;relu&#39;）

到其他 64（而不是 32）。
对于下面的例子，
64 肯定比 32 更好（或更差）有什么理由吗？
是否有理由添加另一个密集层会更好（或更差）？
是否有明确的理由说明为什么 6 个 epoch 的训练会比 5 个 epoch 更好（或更差）？
我知道，如果我理解模型最终所做的所有计算，我就会得到我的答案......这意味着，因为“某人”写了那个代码，有能力知道这一点并不是不可能的......
所有教程和内容都令人非常沮丧。那里的网站关注的是如何，而不是为什么。
（任何建议/链接/资源将不胜感激）
提前非常感谢各位专家！
导入argparse
将张量流导入为 tf


def 过程（参数）：
    ( X_train, y_train ), ( X_test, y_test ) = tf.keras.datasets.mnist.load_data()
    X_train = X_train / 255.0
    X_测试 = X_测试 / 255.0

    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    模型 = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(10)
    ]）

    model.compile(optimizer=&#39;adam&#39;,loss=loss_fn,metrics=[&#39;accuracy&#39;])

    hist = model.fit(X_train,
                     y_火车，
                     批量大小=32，
                     纪元=5，
                     验证分割=0.2，
                     随机播放=真）


如果 __name__ == &#39;__main__&#39;:
    解析器 = argparse.ArgumentParser()
    parser.add_argument(&#39;--batch_size&#39;, type=int, default=32)
    args = parser.parse_args()
    过程（参数）

``
]]></description>
      <guid>https://stackoverflow.com/questions/78184146/selecting-model-training-params-and-layers</guid>
      <pubDate>Tue, 19 Mar 2024 03:47:31 GMT</pubDate>
    </item>
    <item>
      <title>TF2 和 python 中的 BERT 预处理器模型存在问题</title>
      <link>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</link>
      <description><![CDATA[我正在尝试使用 BERT 来做一个文本分类项目。但是我一直遇到这个错误
`
ValueError Traceback（最近一次调用最后一次）
单元格 In[37]，第 4 行
      2 text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
      3 bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
----&gt; 4 preprocessed_text = bert_preprocess(text_input)
      5 bert_encoder = hub.KerasLayer(encoder_url,
      6 可训练=真，
      7 名称=&#39;BERT_编码器&#39;)
      8 个输出 = bert_encoder(preprocessed_text)
ValueError：调用层“预处理”时遇到异常（类型 KerasLayer）。
KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。

调用层“预处理”接收的参数（类型 KerasLayer）：
  输入=
  • 培训=无

KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。



构建此模型时：
&lt;前&gt;&lt;代码&gt;
preprocess_url = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3&#39;
编码器网址 = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/bert-en-uncased-l-12-h-768-a-12/versions/2&#39;

# Bert 层
text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
预处理文本 = bert_preprocess(text_input)
bert_encoder = hub.KerasLayer(encoder_url,
                              可训练=真，
                              名称=&#39;BERT_编码器&#39;)
输出= bert_encoder（预处理文本）

# 神经网络层
l = tf.keras.layers.Dropout(0.1)(输出[&#39;pooled_output&#39;])
l = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;, name=&#39;输出&#39;)(l)

# 构建最终模型
模型 = tf.keras.Model(输入=[text_input], 输出=[l])

我看过无数的教程，甚至使用了张量流文档上的教程，即使我复制和粘贴，它们仍然不起作用。我尝试过不同版本的 tf、tf-text 和 tf-hub。我在这个项目中使用了tensorflow-gpu-jupyter docker 容器。
这是我安装库的方法：
!pip install “tensorflow-text”
!pip install “tf-models-official”
!pip install “tensorflow-hub”

版本是：
张量流：2.16.1
张量流文本：2.16.1
张量流中心：0.16.1
我看到的有关此问题的所有其他论坛都说要执行 tf.config.run_functions_eagerly(True) 但这不起作用。
任何事情都会有所帮助。如果您知道如何解决请回答。]]></description>
      <guid>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</guid>
      <pubDate>Tue, 19 Mar 2024 01:42:01 GMT</pubDate>
    </item>
    <item>
      <title>如何使用kaggle中的两个GPU在pytorch中进行训练？</title>
      <link>https://stackoverflow.com/questions/77094149/how-to-use-both-gpus-in-kaggle-for-training-in-pytorch</link>
      <description><![CDATA[我正在 Kaggle GPU 中训练模型。
但正如我所看到的，只有一个 GPU 正在工作。

我使用普通方法进行训练，例如
device = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;)
模型 = model.to(设备)

如何同时使用这两个 GPU？]]></description>
      <guid>https://stackoverflow.com/questions/77094149/how-to-use-both-gpus-in-kaggle-for-training-in-pytorch</guid>
      <pubDate>Wed, 13 Sep 2023 04:56:24 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 生存模型</title>
      <link>https://stackoverflow.com/questions/75010397/xgboost-survival-model</link>
      <description><![CDATA[我正在尝试开发 XGBoost 生存模型。这是我的代码的快速快照：
X = df_High_School[[&#39;Gender&#39;, &#39;Lived_both_Parents&#39;, &#39;Moth_Born_in_Canada&#39;, &#39;Father_Born_in_Canada&#39;,&#39;Born_in_Canada&#39;,&#39;Aboriginal&#39;,&#39;Visible_Minority&#39;]] # 协变量
y = df_High_School[[&#39;time_to_event&#39;, &#39;event&#39;]] # 事件发生时间和事件指示器

#将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#开发模型
model = xgb.XGBRegressor(objective=&#39;survival:cox&#39;)

它给了我以下错误：
&lt;块引用&gt;
&lt;小时/&gt;

ValueError Traceback（最近一次调用最后一次）
 在
18
19 # 将模型拟合到训练数据
---&gt; 20 model.fit(X_train, y_train)
21
22 # 对测试集进行预测
2帧
_maybe_pandas_label（标签）中的/usr/local/lib/python3.8/dist-packages/xgboost/core.py
261 if isinstance（标签，DataFrame）：
[262] 第 262 章1：
--&gt; 263 raise ValueError（&#39;标签的数据帧不能有多列&#39;）
264
[第 265 章]
ValueError：标签的 DataFrame 不能有多列
由于这是一个生存模型，我需要两列 t 来指示事件和 time_to_event。我还尝试将 Dataframes 转换为 Numpy，但它也不起作用。
有什么线索吗？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/75010397/xgboost-survival-model</guid>
      <pubDate>Wed, 04 Jan 2023 19:32:04 GMT</pubDate>
    </item>
    </channel>
</rss>