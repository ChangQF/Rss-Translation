<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 18 Dec 2023 18:17:28 GMT</lastBuildDate>
    <item>
      <title>解释 AutoGluon 表格分类器的文本特征</title>
      <link>https://stackoverflow.com/questions/77680786/explaining-text-features-of-autogluon-tabular-classifier</link>
      <description><![CDATA[我正在训练 AutoGluon 表格分类器。
我的所有数据都是表格格式，并且大多数特征都是数字。然而，训练中使用的一些特征是文本特征。是否可以解释文本特征如何影响预测？例如，我感兴趣的是是否有任何特定的单词会导致预测发生这样或那样的变化。我尝试使用 SHAP 的 KernelExplainer，但我不确定这是否是正确的方法或如何在这种情况下使用它。
我最初使用 AutoGluon 的 TabularPredictor.feature_importance，但这只能告诉我哪些特征在最终预测中权重最大。然而，权重最高的特征是文本特征。我无法找出 AutoGluon 在该功能中具体查看什么内容来进行预测，无论是特定单词还是其他单词。]]></description>
      <guid>https://stackoverflow.com/questions/77680786/explaining-text-features-of-autogluon-tabular-classifier</guid>
      <pubDate>Mon, 18 Dec 2023 17:28:53 GMT</pubDate>
    </item>
    <item>
      <title>如何使用自定义数据集格式训练自定义 Transformer 模型</title>
      <link>https://stackoverflow.com/questions/77680618/how-to-train-a-customized-transformer-model-with-custom-dataset-formatting</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77680618/how-to-train-a-customized-transformer-model-with-custom-dataset-formatting</guid>
      <pubDate>Mon, 18 Dec 2023 16:56:58 GMT</pubDate>
    </item>
    <item>
      <title>加权机器学习集成给出的结果很差[关闭]</title>
      <link>https://stackoverflow.com/questions/77680479/weighted-machine-learning-ensemble-giving-poor-results</link>
      <description><![CDATA[我正在为基于文本的 csv 文件情感分析实施加权集成模式。我尝试过根据准确度、精确度、召回率进行权重分配，但它给我的结果与单个 ML 或深度学习模型相似或更差 🤔
我已经制作了大约 23 个具有不同组合的模型
。
Python
输入是基于文本的推文
输出为3类分类
词嵌入是tfidf
我怎样才能提高性能以及我应该使用什么权重指标？重量度量的方程应该是..]]></description>
      <guid>https://stackoverflow.com/questions/77680479/weighted-machine-learning-ensemble-giving-poor-results</guid>
      <pubDate>Mon, 18 Dec 2023 16:33:10 GMT</pubDate>
    </item>
    <item>
      <title>如何将 OHLCV 交易数据加载到 TensorflowJS 卷积层中？</title>
      <link>https://stackoverflow.com/questions/77680420/how-to-load-ohlcv-trading-data-into-tensorflowjs-convolution-layer</link>
      <description><![CDATA[我的目标
获取本周前 5 个股票交易日的高位数据，看看我是否可以预测周末价格上涨或下跌。
数据
在交易中，我们使用 OHLCV 数据来描述交易日的情况：
（O=当日开盘价，H=当日最高价，L=当日最低价，C=当日收盘价，V=当日成交量）。
//1天的数据
[o、h、l、c、v]

// 例如
[10、12、9、11、100]

因此 5 天的数据如下：
//5天的数据
[[o、h、l、c、v]、[o、h、l、c、v]、[o、h、l、c、v]、[o、h、l、c、v]、[ o、h、l、c、v]]

// 例如
[[10, 12, 9, 11, 100], [11, 13, 11, 12, 200], [12, 12, 11, 11, 150], [11, 14, 11, 14, 300], [ 14, 14, 11, 12, 200]]

我读到，处理这些数据的最佳方法是使用卷积层，特别是 Tensorflow 中的 conv2d 层。据我了解，该层需要 4dTensor 作为输入。因此，我使用tensor4d()从包含4周训练数据（4倍5天数据）的数据数组创建这样的张量
// 4 次 5 天的 javascript 数组
常量数据 = [10, 12, 9, 11, 100, 11, 13, 11, 12, 200, 12, 12, 11, 11, 150, 11, 14, 11, 14, 300, 14, 14, 11, 12, 200, 10, 12, 9, 11, 100, 11, 13, 11, 12, 200, 12, 12, 11, 11, 150, 11, 14, 11, 14, 300, 14, 14, 11, 12, 200, 10, 12, 9, 11, 100, 11, 13, 11, 12, 200, 12, 12, 11, 11, 150, 11, 14, 11, 14, 300, 14, 14, 11, 12, 200, 10, 12, 9, 11, 100, 11, 13, 11, 12, 200, 12, 12, 11, 11, 150, 11, 14, 11, 14, 300, 14, 14, 11, 12, 200]

// 转换为 4d 张量
const input_tensor = tf.tensor4d(data, [data.length / (5 * 5), 5, 5, 1]);

目标由每周 1 个结果组成（周末交易的结果）。
const target = [-1, 2, -3, 1] // % 盈利或亏损

// 转换为一维张量
const target_tensor = tf.tensor1d(目标);

模型
我的模型看起来像这样，是根据此处找到的示例构建的：https:// blog.quantinsti.com/卷积神经网络/
const model = tf.sequential();
model.add(tf.layers.conv2d({

    激活：&#39;relu&#39;，
    过滤器：32，
    inputShape: [5, 5, 1], // 5 个 ohlcv，共 5 个值，每个值 1
    内核大小：1，
}));
model.add(tf.layers.maxPooling2d([2, 2]));
model.add(tf.layers.conv2d({

    激活：&#39;relu&#39;，
    过滤器：64，
    内核大小：1，
}));
model.add(tf.layers.maxPooling2d([2, 2]));
model.add(tf.layers.flatten());
model.add(tf.layers.dense({

    激活：&#39;relu&#39;，
    单位：64，
}));
model.add(tf.layers.dense({

    单位：1，
    激活：&#39;relu&#39;，
}));

我使用之前创建的张量训练模型
//训练模型
等待 model.fit(input_tensor, target_tensor, {
                    
    批量大小：32，
    纪元：50，
    随机播放：正确，
});

错误
在此步骤中，我收到以下错误
unhandledRejection: TypeError: 无法将 undefined 或 null 转换为对象

最后一步
model.predict([10, 12, 9, 11, 100], [5, 1])

问题

我使用卷积层是因为我想保留 1 天的 OHLCV 数据之间的关系，我的模型是否属于这种情况？有必要吗？
我正确创建了 4d 和 1d 张量吗？
内核大小为 [1, 1] 的卷积层是否有意义？似乎违背了卷积层的目的？
如何修复该错误？
我是走在正确的道路上还是完全看错了方向？在这种情况下，您能否为我指明如何构建模型的正确方向？
]]></description>
      <guid>https://stackoverflow.com/questions/77680420/how-to-load-ohlcv-trading-data-into-tensorflowjs-convolution-layer</guid>
      <pubDate>Mon, 18 Dec 2023 16:22:48 GMT</pubDate>
    </item>
    <item>
      <title>在手动交叉验证和 cross_val_score 之间获取不同的分数值</title>
      <link>https://stackoverflow.com/questions/77680320/getting-different-score-values-between-manual-cross-validation-and-cross-val-sco</link>
      <description><![CDATA[我创建了一个 python for 循环，将训练数据集分割成分层的 KFold，并在循环内使用分类器来训练它。然后使用经过训练的模型通过验证数据进行预测。使用此过程实现的指标与使用 cross_val_score 函数实现的指标完全不同。我期望使用这两种方法得到相同的结果。
此代码用于文本分类，我使用 TF-IDF 对文本进行矢量化
这是代码：
手动实现交叉验证的代码：
#导入指标函数来衡量模型的性能
从 sklearn.metrics 导入 f1_score、accuracy_score、 precision_score、recall_score
从 sklearn.model_selection 导入 StratifiedKFold
data_validation = [] # 用于存储使用交叉验证的模型验证结果的列表
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
准确度值 = []
f1_val = []

# 使用ravel函数将多维数组展平为一维
对于 (skf.split(X_train, y_train)) 中的 train_index、val_index：
    X_tr, X_val = X_train.ravel()[train_index], X_train.ravel()[val_index]
    y_tr, y_val = y_train.ravel()[train_index] , y_train.ravel()[val_index]
    tfidf=TfidfVectorizer()
    X_tr_vec_tfidf = tfidf.fit_transform(X_tr) # 对训练折叠进行向量化
    X_val_vec_tfidf = tfidf.transform(X_val) # 对验证折叠进行向量化
    #实例化模型
    模型= MultinomialNB(alpha=0.5, fit_prior=False)
    #用我们的训练数据集训练空模型
    model.fit(X_tr_vec_tfidf, y_tr)
    Predictions_val = model.predict(X_val_vec_tfidf) # 使用验证数据集进行预测
    acc_val = 准确度_分数（y_val，预测_val）
    Accuracy_val.append(acc_val)
    f_val = f1_score（y_val，预测值）
    f1_val.append(f_val)

avg_accuracy_val = np.mean(accuracy_val)
avg_f1_val = np.mean(f1_val)

# 存储指标的临时列表
temp = [&#39;NaiveBayes&#39;]
temp.append(avg_accuracy_val) #验证准确率分数
temp.append(avg_f1_val) #验证f1分数
data_validation.append(临时)
#使用数据帧创建一个表，其中包含所有经过训练和测试的 ML 模型的指标
result = pd.DataFrame(data_validation, columns = [&#39;算法&#39;,&#39;准确度分数：验证&#39;,&#39;F1-分数：验证&#39;])
result.reset_index(drop=True, inplace=True)
结果

输出：
 算法准确度分数：验证 F1-Score：验证
0 天真的贝叶斯 0.77012 0.733994

现在使用 cross_val_score 函数的代码：
从 sklearn.model_selection 导入 cross_val_score
从 sklearn.feature_extraction.text 导入 CountVectorizer、TfidfVectorizer
分数 = [&#39;准确度&#39;, &#39;f1&#39;]
#使用NLP技术TF-IDF对训练和测试数据集进行文本向量化
tfidf=TfidfVectorizer()
X_tr_vec_tfidf = tfidf.fit_transform(X_train)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
nb=多项式NB(alpha=0.5, fit_prior=False)
对于 [“accuracy”, “f1”] 中的分数：
    print (f&#39;{score}: {cross_val_score(nb,X_tr_vec_tfidf,y_train,cv=skf,scoring=score).mean()} &#39;)

输出：
准确度：0.7341283583255231
f1：0.7062017090972422

可以看出，使用这两种方法的准确性和 f1 指标有很大不同。当我使用 KNeighborsClassfier 时，指标的差异更加严重。]]></description>
      <guid>https://stackoverflow.com/questions/77680320/getting-different-score-values-between-manual-cross-validation-and-cross-val-sco</guid>
      <pubDate>Mon, 18 Dec 2023 16:05:36 GMT</pubDate>
    </item>
    <item>
      <title>处理 CNN 二元分类的分布外样本和异常</title>
      <link>https://stackoverflow.com/questions/77679785/handling-out-of-distribution-samples-and-anomalies-for-cnn-binary-classification</link>
      <description><![CDATA[我对机器学习领域比较陌生，并且对创建基本自定义模型有基本的了解。
分配给我的任务
我的经理要求我开发一种机器学习模型，能够在攻击性图像发送到服务器之前识别它们。这些图像可以分为不同的类别，例如武器或成人内容。
问题
但是，我在模型检测异常或分布外样本的能力方面遇到了问题。
我尝试过的
我尝试了不同的方法，利用&#39;binary_crossentropy&#39;作为我的损失函数，但遇到了同样的问题。我还构建了一个包含两个类的模型：

12,278 与武器相关的图片 class_name = 武器
3,000 张食物 商品图片 class_name = 其他

但是，我不确定 “其他” 类别中应包含哪些内容。使用 MobileNet 作为基础模型构建模型后，它成功地准确预测了武器，并提供了 0.4 到 0.6 范围内的食品预测，这似乎是可以接受的。然而，当我引入大象或汽车的图像时，模型倾向于将它们预测为武器，显示的置信度接近 1。
我不知道如何解决这个问题。我尝试研究这个问题，并发现了一些关于分布外检测的线索以及与异常或离群值相关的概念。当模型的输入包含不属于训练数据的图像时，如何获得不确定性值？
如果您能提供任何指导、建议，甚至参考能够有效解决此问题的视频或资源，我将不胜感激。感谢您的帮助。
数据加载：
train_ds, test_ds = keras.utils.image_dataset_from_directory(
    目录=“/内容/武器”，
    标签=“推断”，
    label_mode =“二进制”，
    批量大小=32，
    子集=“两者”，
    图像大小=(224,224),
    验证分割=0.2，
    随机播放=真，
    种子=1337
）

现在标准化输入图像数据：
def process（图像，标签）：
  图像=tf.cast(图像/255, tf.float32)
  返回图像、标签
train_ds = train_ds.map(进程)
test_ds = test_ds.map(进程)

创建 CNN 模型：
input_shape = (224,224,3)
mobilenet = MobileNet(input_shape,weights=&#39;imagenet&#39;,include_top=False)
模型=顺序（）
 
model.add（移动网络）
模型.add(压平())
model.add（密集（256，激活=&#39;relu&#39;））
模型.add(Dropout(0.5))
model.add（密集（1，激活=&#39;sigmoid&#39;））
模型.summary()



sgd = SGD(学习率=0.0001，动量=0.9，nesterov=True)
model.compile（损失=&#39;binary_crossentropy&#39;，优化器=sgd，指标=[&#39;准确性&#39;]）
历史= model.fit（train_ds，validation_data = test_ds，batch_size = 4，epochs = 6）


纪元 6/6
382/382 [==============================] - 58s 150ms/步 - 损失：0.0042 - 精度：0.9984 - val_loss ：0.0063 - val_accuracy：0.997
]]></description>
      <guid>https://stackoverflow.com/questions/77679785/handling-out-of-distribution-samples-and-anomalies-for-cnn-binary-classification</guid>
      <pubDate>Mon, 18 Dec 2023 14:34:38 GMT</pubDate>
    </item>
    <item>
      <title>使用YOLOv5旧模型进行预测</title>
      <link>https://stackoverflow.com/questions/77678969/using-yolov5-old-models-to-make-a-prediction</link>
      <description><![CDATA[我目前正在尝试使用 .pt 文件作为 YOLOv5 模型，该模型是在 2 年前训练的，每当我尝试这样做时，我都会收到以下错误。
运行时错误：PytorchStreamReader 读取 zip 存档失败：找不到中心目录。

每次我尝试使用不同的 model.pt 文件时，它都会下载最新版本（例如：yolov5mu.pt）。
我在 VSCode 和 anaconda 提示符下运行类似的命令：
yolo 检测预测 model=yolov5/yolov5m.pt source=&#39;datasets\Testing_images\20210823_112536.jpg&#39;，在 VSCode 上。

yolo任务=检测模式=预测模型=ganuza.pt源=&#39;test1.jpg&#39;show=True，在anaconda提示符下。

我按照教程进行操作，文件夹组织看起来不错。如果我使用另一种模型（例如它下载的模型），它就可以正常工作。
有谁知道发生了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/77678969/using-yolov5-old-models-to-make-a-prediction</guid>
      <pubDate>Mon, 18 Dec 2023 12:06:31 GMT</pubDate>
    </item>
    <item>
      <title>将问题标记为简单、中等和困难 [关闭]</title>
      <link>https://stackoverflow.com/questions/77678858/labelling-questions-as-easy-medium-and-hard</link>
      <description><![CDATA[我目前正在从事一个机器学习项目，专注于开发问题标签模型。 挑战在于确定如何将问题分类为“简单”、“中等”或“困难”。更具体地说，我正在寻求有关哪些指标可以准确确定问题难度级别的指导简单、中等或困难。如果有人有机器学习经验，我们将不胜感激您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/77678858/labelling-questions-as-easy-medium-and-hard</guid>
      <pubDate>Mon, 18 Dec 2023 11:44:50 GMT</pubDate>
    </item>
    <item>
      <title>将变量从一个模块导入到另一个模块</title>
      <link>https://stackoverflow.com/questions/77678535/importing-variables-from-one-module-to-other</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77678535/importing-variables-from-one-module-to-other</guid>
      <pubDate>Mon, 18 Dec 2023 10:44:18 GMT</pubDate>
    </item>
    <item>
      <title>分割模型推理延迟问题</title>
      <link>https://stackoverflow.com/questions/77678168/segmentation-model-inference-latency-issue</link>
      <description><![CDATA[我使用了 pyannote 的开源分割模型和 Diart diarization 的说话者二值化存储库，使用 diart==0.5.1，
在 diart/blocks/segmentation.py 中，我进行了以下更改::
 与 torch.no_grad()：
        wave=rearrange(self.formatter.cast(waveform),“批量采样通道-&gt;批量通道采样”)
        # 波火炬.Tensor (1, 1, 80000)
        打印（wave.get_device（））
        开始 = 时间.time()
        输出 = self.model(wave.to(self.device)).cpu()
        停止=时间.time()
        print(&#39;分段时间:&#39;)
        打印（停止-开始）
        # 输出：torch.Tensor (1, 293, 3)
    返回 self.formatter.restore_type(输出)

在输出中，seg timeL 0.4s
但是如果我尝试在 diart 存储库之外进行推断（在独立的存储库中）：
defegmentation_model(self,batch:np.ndarray) -&gt;; np.ndarray：
    块 = torch.tensor(batch)
    print(chunks.get_device()) # -1

    使用 torch.no_grad()：
        尝试：
            打印（块.形状）
            输出 = self.model(chunks.to(self.device)).cpu()
        除了 RuntimeError 作为例外：
            如果 is_oom_error（异常）：
                引发内存错误（
                    f&quot;batch_size ({self.batch_size: d}) 可能太大。 ”
                    f“尝试使用较小的值，直到内存错误消失。”
                ）
            别的：
                引发异常

    返回输出.numpy()

此处分段时间：10s
资源、输入格式、形状、类型一切都是相同的
为什么延迟不同？
期望延迟相同]]></description>
      <guid>https://stackoverflow.com/questions/77678168/segmentation-model-inference-latency-issue</guid>
      <pubDate>Mon, 18 Dec 2023 09:38:17 GMT</pubDate>
    </item>
    <item>
      <title>运行 fmri 深度学习模型时出现错误</title>
      <link>https://stackoverflow.com/questions/77677389/getting-error-while-running-fmri-deep-learning-model</link>
      <description><![CDATA[我正在处理 fmri 数据，我有两组数据集疾病和正常数据的形状是正常数据形状：(91, 109, 91, 1200)
疾病数据形状：(91, 109, 91, 210)
我写了python脚本
将 numpy 导入为 np
将 nibabel 导入为 nib
从 sklearn.model_selection 导入 train_test_split


# 定义正常和疾病数据文件夹的路径
疾病数据路径 = glob(&#39;/media/aish/Backup Plus1/ABIDE/scan_data001/**/**/**/**/**/**/**/**/swa*&#39;)
Normal_data_path = glob(&#39;/media/aish/rs2/hcp/s*&#39;)

# 加载正常和疾病数据
正常数据 = []
疾病数据 = []
对于normal_data_path[0:400]中的文件：
    Normal_data.append(nib.load(file).get_data())
对于疾病数据路径中的文件：
    疾病数据.append(nib.load(文件).get_data())

print(“正常数据形状：”, normal_data[1].shape)
print(“疾病数据形状：”,疾病数据[1].shape)

# 创建标签
标签 = np.concatenate((np.zeros(len(normal_data)), np.ones(len(disease_data))))

# 将数据分为训练集和测试集
X_train，X_test，y_train，y_test = train_test_split（np.array（正常数据+疾病数据），标签，test_size = 0.2，random_state = 42）

但是我收到错误
完整错误是
ValueError Traceback（最近一次调用最后一次）
[7]，第 32 行中的单元格
     29 个标签 = np.concatenate((np.zeros(len(normal_data)), np.ones(len(disease_data))))
     31 # 将数据分为训练集和测试集
---&gt; 32 X_train，X_test，y_train，y_test = train_test_split（np.array（正常数据+疾病数据），标签，test_size = 0.2，random_state = 42）
     34 # 定义CNN模型
     35 模型 = 顺序()

ValueError：无法将输入数组从形状 (91,109,91,1200) 广播到形状 (91,109,91)

我无法解决此错误]]></description>
      <guid>https://stackoverflow.com/questions/77677389/getting-error-while-running-fmri-deep-learning-model</guid>
      <pubDate>Mon, 18 Dec 2023 06:53:58 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 中的数据采样方法</title>
      <link>https://stackoverflow.com/questions/77578111/data-sample-methods-in-lightgbm</link>
      <description><![CDATA[我的问题
我不太清楚所有参数的用法以及它们如何相互交互（或应该使用）。
我所知道的
据我了解，LightGBM中有3种算法：

GBDT，默认的，使用 boosting
DART 是一种带有 dropout 的 boosting 算法
随机森林，不使用增强（确实如此，但仅在一次迭代中）

并且有两种数据采样策略：

Bagging，用于集成学习。这是默认值，但关联参数的值（例如 bagging_freq）会使 bagging 停用。
GOSS 选择更多对误差梯度贡献最大的数据（我们的想法是，我们需要对远离基线的数据进行更多训练），而对“弱”数据进行更少的训练。数据点（对误差梯度贡献较小的数据点）。

问题
所以我的问题如下：

为什么 Bagging 和 GOSS 不兼容？它们似乎不会影响同一件事。
LightGBM 的主要创新似乎是 GOSS，但它并不是默认选择，这样做的动机是什么？
最后，我们能够将 boosting_type=goss 作为参数传递。当我们这样做时会发生什么？算法会是GBDT，而数据样本策略是goss吗？

非常感谢您抽出时间。
祝你有美好的一天。]]></description>
      <guid>https://stackoverflow.com/questions/77578111/data-sample-methods-in-lightgbm</guid>
      <pubDate>Thu, 30 Nov 2023 11:34:21 GMT</pubDate>
    </item>
    <item>
      <title>LLM模型非常慢</title>
      <link>https://stackoverflow.com/questions/77199972/llm-model-is-very-slow</link>
      <description><![CDATA[我正在 nvidia g5 上运行 34b LLM 模型。 8xlarge 实例（1 个 Nvidia A10G GPU、24GB GPU RAM、32 个 vCPU、128GB RAM）
这是推理代码
从变压器导入 AutoTokenizer,LlamaForCausalLM, AutoConfig, AutoModelForCausalLM
从加速导入 infer_auto_device_map, init_empty_weights
进口火炬

model_path = “Phind/Phind-CodeLlama-34B-v2”

model = LlamaForCausalLM.from_pretrained(model_path, device_map=“自动”, offload_folder=“卸载”, torch_dtype=torch.float16, offload_state_dict = True)
tokenizer = AutoTokenizer.from_pretrained(model_path)

defgenerate_one_completion（提示：str）：
    tokenizer.pad_token = tokenizer.eos_token
    输入=分词器（提示，return_tensors =“pt”，截断= True，max_length = 4096）

    ＃ 产生
    generate_ids = model.generate(inputs.input_ids.to(“cuda”), max_new_tokens=384, do_sample=True, top_p=0​​.75, top_k=10, 温度=0.1)
    完成= tokenizer.batch_decode（generate_ids，skip_special_tokens = True，clean_up_tokenization_spaces = False）[0]
    完成 = 完成.replace(prompt, &quot;&quot;).split(&quot;\n\n\n&quot;)[0]

    返回完成

text = “你好，请问是你吗？”
打印（生成_一个_完成（文本））


加载检查点分片 - 这需要 4 分钟。如何才能加快速度？
即使是简单的推理也需要 60 秒以上。代码填写/提示需要 10 多分钟。可以在此 ec2 实例上加速吗？
]]></description>
      <guid>https://stackoverflow.com/questions/77199972/llm-model-is-very-slow</guid>
      <pubDate>Fri, 29 Sep 2023 06:54:14 GMT</pubDate>
    </item>
    <item>
      <title>如何使用GAN生成拉曼光谱的合成数据样本？</title>
      <link>https://stackoverflow.com/questions/76906588/how-to-generate-synthetic-data-samples-of-raman-spectroscopy-by-using-gan</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76906588/how-to-generate-synthetic-data-samples-of-raman-spectroscopy-by-using-gan</guid>
      <pubDate>Tue, 15 Aug 2023 14:05:29 GMT</pubDate>
    </item>
    <item>
      <title>无法在python中安装lap==0.4.0库</title>
      <link>https://stackoverflow.com/questions/76463707/unable-to-install-lap-0-4-0-library-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76463707/unable-to-install-lap-0-4-0-library-in-python</guid>
      <pubDate>Tue, 13 Jun 2023 09:55:26 GMT</pubDate>
    </item>
    </channel>
</rss>