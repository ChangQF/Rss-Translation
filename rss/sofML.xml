<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 31 Mar 2025 12:36:19 GMT</lastBuildDate>
    <item>
      <title>与Skywise -Slate仪表板相关的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79545514/issue-related-to-skywise-slate-dashboard</link>
      <description><![CDATA[我正在努力修改空客Skywise中现有的仪表板。我的任务涉及：
 提取数据： 
用户单击“获取”时按钮，系统应从用户指定的列中获取数据并将其存储以供以后使用。
 手动描述输入： 
在“手动描述”中单元格，用户将手动粘贴来自空中客车数据库的文本，对应于特定的FUID ID。
 生成TDD评估： 
用户点击“生成评估”时，系统应：

使用先前获取的数据和手册说明
参考。
利用ML/AI模型来自动生成TDD评估
针对不同错误类型的预定义参考脚本。

我正在寻找有关如何实施此功能的指导。具体来说，我需要以下帮助：

提取和存储用户指定的列数据。
处理手动说明并将其链接到Fuid ID。
使用使用ML/AI基于ML/AI的自动生成TDD评估
参考脚本。
]]></description>
      <guid>https://stackoverflow.com/questions/79545514/issue-related-to-skywise-slate-dashboard</guid>
      <pubDate>Mon, 31 Mar 2025 04:58:02 GMT</pubDate>
    </item>
    <item>
      <title>如何使用R中的自定义内核进行SVM？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79545276/how-to-do-svm-with-a-custom-kernel-in-r</link>
      <description><![CDATA[ 内核方程  
我试图使用上面的自定义内核进行SVM，但我不知道该如何编码，有人知道我应该在哪里看吗？哪些软件包允许包括自定义内核？该内核应允许更高阶的相互作用，而不仅仅是常规多项式内核。]]></description>
      <guid>https://stackoverflow.com/questions/79545276/how-to-do-svm-with-a-custom-kernel-in-r</guid>
      <pubDate>Sun, 30 Mar 2025 23:33:57 GMT</pubDate>
    </item>
    <item>
      <title>当给出额外的较大V12权重（Yolov12x）时，Yolo为什么要下载Nano V11型号（Yolov11n）？</title>
      <link>https://stackoverflow.com/questions/79545081/why-does-yolo-download-a-nano-v11-model-yolov11n-when-given-the-extra-larger-v</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79545081/why-does-yolo-download-a-nano-v11-model-yolov11n-when-given-the-extra-larger-v</guid>
      <pubDate>Sun, 30 Mar 2025 19:43:01 GMT</pubDate>
    </item>
    <item>
      <title>将型号的重量从旧的Keras版本转换为Pytorch</title>
      <link>https://stackoverflow.com/questions/79544819/conversion-of-model-weights-from-old-keras-version-to-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79544819/conversion-of-model-weights-from-old-keras-version-to-pytorch</guid>
      <pubDate>Sun, 30 Mar 2025 15:41:21 GMT</pubDate>
    </item>
    <item>
      <title>如何保存Dynamax Gussianhmm型号？</title>
      <link>https://stackoverflow.com/questions/79544766/how-do-i-save-a-dynamax-gussianhmm-model</link>
      <description><![CDATA[我一直在基于本教程开发模型：
 https://probml.github.github.io/dynamax/namax/neamax/notebooks/notebooks/notebooks/gaussian/gaussian_phmmm.phmm.hmm.hmm.hmm.hmm.hmm.hmm.hmm.hmmm.hmm.hmm.hmm.hmt 但是，由于现在我的代码可能需要几天的运行，所以我希望将模型保存在中间，以便以后加载。
我尝试了JSON，Pickle和其他几种GPT建议，但无济于事。
因此，在我尝试保存模型的所有参数作为字符串之前，并在加载模型时将其转换回它们，我想知道是否有更轻松的选择。
这是我的代码的示例：
 来自dynamax.hidden_​​markov_model import import gaussianhmm
导入jax.random作为JR

嗯= Gaussianhmm（5，3）
param，properties = hmm.initialize（jr.prngkey（10））
 
为了保存我尝试的模型，例如：
 导入jax.numpy作为jnp
导入JAX
进口泡菜

def backup_hmm（params，props，filename =; hmm_backup_jax.pkl＆quort;）：
    ＃提取阵列安全
    params_flat，params_tree = jax.tree_util.tree_flatten（params）
    props_flat，props_tree = jax.tree_util.tree_flatten（props）

    ＃转换为普通列表
    params_flat = [jnp.array（p）for params_flat中的p]
    props_flat = [p props_flat中的p的jnp.array（p）]

    ＃保存到泡菜
    用开放式（文件名，“ wb”）为f：
        pickle.dump（{
            ＆quot&#39;params;：params_flat，
            ＆quot&#39;params_tree＆quort;：params_tree，
            ＆quot“ props”：props_flat，
            ＆quot“ props_tree”：props_tree
        }，f）

    打印（“备份完成。”

Def Restore_hmm（filename =＆quot; hmm_backup_jax.pkl＆quort;）：
    用开放式（文件名，rb＆quot）为f：
        data = pickle.load（f）

    ＃ 恢复
    params_flat = [jnp.array（p）for p in data [&#39;params; quot;]]
    props_flat = [jnp.Array（p）for p in Data [＆quots&#39;props; quot;]]

    ＃重建原始结构
    params = jax.tree_util.tree_unflatten（data [; params_tree; quord; quart&#39;; params_flat）
    props = jax.tree_util.tree_unflatten（data [＆quot; props_tree＆quot; quor＆quot; props_flat）

    打印（“恢复已完成。”
    返回参数，道具

 ]]></description>
      <guid>https://stackoverflow.com/questions/79544766/how-do-i-save-a-dynamax-gussianhmm-model</guid>
      <pubDate>Sun, 30 Mar 2025 14:57:55 GMT</pubDate>
    </item>
    <item>
      <title>l如何解决VCC16的输入数据大小问题</title>
      <link>https://stackoverflow.com/questions/79544595/how-can-l-solve-the-input-data-size-problem-of-vcc16</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79544595/how-can-l-solve-the-input-data-size-problem-of-vcc16</guid>
      <pubDate>Sun, 30 Mar 2025 12:34:18 GMT</pubDate>
    </item>
    <item>
      <title>TrackNet：使用TrackNet实时跟踪Shuttlecock，并更新分数[闭合]</title>
      <link>https://stackoverflow.com/questions/79543955/tracknet-tracking-a-shuttlecock-in-real-time-with-a-webcam-using-tracknet-and-u</link>
      <description><![CDATA[我正在尝试为项目开发羽毛球实时跟踪和评分系统。我可以使用踪迹获取实时数据吗？
我正在使用 https://github.com/qaz812345/tracknetv3 这个模型
检测视频很好，但是当我尝试使用我从chatgpt获得的实时跟踪代码时，它显示错误：
  runtimeerror：tracknet加载state_dict中的错误（s）：

down_block_1.conv_1.conv.的尺寸不匹配。
 
也有SEQ LENG的另一个错误
如果不可能，还有其他方法吗？
它可以与设置缓冲区一起加载一些实时框架，然后在持续的周期中对其进行处理？]]></description>
      <guid>https://stackoverflow.com/questions/79543955/tracknet-tracking-a-shuttlecock-in-real-time-with-a-webcam-using-tracknet-and-u</guid>
      <pubDate>Sat, 29 Mar 2025 22:07:02 GMT</pubDate>
    </item>
    <item>
      <title>在图像中检测表情符号[封闭]</title>
      <link>https://stackoverflow.com/questions/79543795/detecting-emojis-in-an-image</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79543795/detecting-emojis-in-an-image</guid>
      <pubDate>Sat, 29 Mar 2025 19:27:22 GMT</pubDate>
    </item>
    <item>
      <title>Importerror：使用“ BitsandBytes” 8位量化需要加速</title>
      <link>https://stackoverflow.com/questions/78595127/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate</link>
      <description><![CDATA[从HuggingFace下载模型时，我遇到了错误。它在Google Colab上工作，但不在我的Windows机器上工作。我正在使用python 3.10.0。
错误代码如下：
  e：\ internships \ consciusai \ .venv \ lib \ lib \ site-packages \ huggingface_hub \ file _download.py：1132：1132：futureWarning：futureWarning：`remume_download&#39;s depected and depected and depected and将在版本1.0.0.0.0.0.0.0中删除。下载总是在可能的情况下恢复。如果要强制新下载，请使用`force_download = true`。
  警告。
未使用的Kwargs：[&#39;_load_in_4bit&#39;，&#39;_load_in_8bit&#39;，&#39;QUAT_METHOD&#39;]。这些Kwargs在＆lt; class&#39;Transformers.utils.quantization_config.bitsandbytesconfig&#39;＆gt;中不使用这些夸大。
e：\ internships \ consciusai \ .venv \ lib \ lib \ lib \ site-packages \ transformers \ ventrizers \ venterizers \ auto.py：159：userWarning：您通过`jentization_config`或等效参数to`from_pretrenained&#39;&#39;&#39;但您正在加载的模型已经加载了量化量化量化量化量化。将使用来自模型的`Quantization_Config`。
  WARNINGS.WARN（WARNNING_MSG）
Trackback（最近的最新电话）：
  file＆quort e：\ induthships \ consciusai \ emaim_2.py”，第77行，in＆lt; module＆gt;
    主要的（）
  file＆quot e：\ internships \ consciusai \ emaim_2.py; ,，第71行，主要
    摘要= summarize_email（内容）
  file＆quot e：\ internships \ consciusai \ emaim_2.py”，第22行，在summarize_email中
    管道=变形金刚。Pipeline（
  file＆quot&#39;e：\ internships \ consciusai \ .venv \ lib \ lib \ site-packages \ transformers \ pipelines \ pipelines \ __ init __ init __ in Int __.py＆quort&#39;＆quort&#39;＆quort&#39;＆quort&#39;＆quote＆quote＆quote＆quort in 906
    框架，型号= peash_framework_load_model（
  file＆quot; e：\实习\ consciusai \ .venv \ lib \ lib \ site-packages \ Transformers \ pipelines \ pipelines \ base.py＆quid＆quot＆quot＆quot＆quid＆quort＆quot＆quot＆quot＆quot＆quid＆quot＆quid＆quot＆quot＆quot＆quot＆quid＆quid＆quot＆quot＆quid＆quot＆quid＆quot＆quid＆quort of 283
    model = model_class.from_pretrataining（模型，** kwargs）
  file＆quot; e：\ internships \ consciusai \ .venv \ lib \ lib \ site-packages \ transformers \ transformers \ models \ auto \ auto \ auto_factory.py;
    返回model_class.from_pretaining（
  file＆quort;
    hf_quantizer.validate_environment（
  file＆quot;
    提高侵居者（
Importerror：使用`bitsandBytes` 8位量化都需要加速：`pip安装加速&#39;和最新版本的bitsandbytes：`pip install -i https：//pypi.org/simple/ bitsandble/ bitsandbytes&#39;
 
这是我使用的代码：
  def summarize_email（content）：
    model_id =＆quot&#39;unsploth/llama-3-8b-instruct-bnb-4bit;

    管道=变形金刚。Pipeline（
        ＆quot“文字生成”
        模型= model_id，
        model_kwargs = {
            ＆quot“ torch_dtype”：torch.float16，
            ＆quot&#39;ventalization_config;：{&#39;load_in_4bit＆quort;：true}，
            ＆quot“ low_cpu_mem_usage＆quot”：true，
        }，，
    ）

    消息= [
        {&#39;&#39;：＆quot; quot“ system; quot” content; quot; quot; quot;
        {&#39;：＆quot“ user quot” contents“ contents”：’汇总给我的电子邮件+内容}，
    这是给出的

    提示= pipeline.tokenizer.apply_chat_template（
            消息，
            tokenize = false，
            add_generation_prompt = true
    ）

    终结者= [
        pipeline.tokenizer.eos_token_id，
        pipeline.tokenizer.convert_tokens_to_ids（“”）
    这是给出的

    输出=管道（
        迅速的，
        max_new_tokens = 256，
        eos_token_id =终止者，
        do_sample = true，
        温度= 0.6，
        top_p = 0.9，
    ）
 
我正在尝试使用“不舒服/Llama-3-8B-Instruct-Bnb-4bit”总结文本。来自拥抱面。
它确实在Google Colab和Kaggle上总结了文本，但没有在本地机器上进行。]]></description>
      <guid>https://stackoverflow.com/questions/78595127/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate</guid>
      <pubDate>Sat, 08 Jun 2024 08:37:18 GMT</pubDate>
    </item>
    <item>
      <title>无法下载MMCV 1.3.0并构建车轮</title>
      <link>https://stackoverflow.com/questions/77479005/not-able-to-download-mmcv-1-3-0-and-build-wheels</link>
      <description><![CDATA[当我尝试安装 mmcv-full == 1.3.0 时，它无法下载并构建轮子（我已经更新了车轮）
  错误无法为MMCV-Full构建车轮，这是安装pyproject.toml项目所需的
 但是当我尝试使用时
 MIM安装mmcv-full  
错误消息：
  RROR：MMCV-Full的建筑轮失败
  运行设置。
无法构建mmcv-full
错误：无法为MMCV-Full构建车轮，这是安装pyproject.toml的项目所需的
 
可以下载 mmcv-full 的最新版本，但是由于我试图克隆的存储库需要使用 MMCV版本1.3.0 。
我正在使用 Windows 11 ，想知道我应该如何下载版本。]]></description>
      <guid>https://stackoverflow.com/questions/77479005/not-able-to-download-mmcv-1-3-0-and-build-wheels</guid>
      <pubDate>Tue, 14 Nov 2023 08:00:38 GMT</pubDate>
    </item>
    <item>
      <title>我正在创建一个扑朔迷离的应用程序来检测手势并在文本中转换为聋人和愚蠢的人[封闭]</title>
      <link>https://stackoverflow.com/questions/75481225/i-am-creating-a-flutter-app-to-detect-hand-gestures-and-convert-them-in-text-for</link>
      <description><![CDATA[是否有人可以为我提供有关如何启动项目，从我可以获取数据集以及如何将手势转换为文本的路线图？还是有任何SDK或API，如果您能为我提供git链接，那就太好了]]></description>
      <guid>https://stackoverflow.com/questions/75481225/i-am-creating-a-flutter-app-to-detect-hand-gestures-and-convert-them-in-text-for</guid>
      <pubDate>Fri, 17 Feb 2023 07:21:32 GMT</pubDate>
    </item>
    <item>
      <title>pytorch` thorch.no_grad` vs`torch.inference_mode`</title>
      <link>https://stackoverflow.com/questions/69543907/pytorch-torch-no-grad-vs-torch-inference-mode</link>
      <description><![CDATA[ pytorch具有新的功能 torch.inference_mode ，如V1.9，它是&#39;https：///pytorch.org/docs/stable/generate/torgh..autograd.autograd.grad_mode.grad_mode.Inference一下 torch.no_grad  ...在此模式下运行的代码通过禁用视图跟踪和版本计数器颠簸而获得更好的性能。
如果我只是在测试时间评估我的模型（即未培训），是否有任何情况， torch.no_grad 比 torch.inferey_mode 更可取？我计划用后者替换前者的每个实例，我希望将运行时错误用作护栏（即，我相信任何问题都会表明自己是运行时错误，如果它不会表现为运行时错误，我假设它确实比使用 torch.inference_mode_mode ）。
在 pytorch开发者podcast 。]]></description>
      <guid>https://stackoverflow.com/questions/69543907/pytorch-torch-no-grad-vs-torch-inference-mode</guid>
      <pubDate>Tue, 12 Oct 2021 16:21:23 GMT</pubDate>
    </item>
    <item>
      <title>如何计算Bernoulli幼稚贝叶斯的联合日志样本</title>
      <link>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</link>
      <description><![CDATA[使用 bernoullinb ，如何对关节进行定量。联合的可能性由下方公式计算，其中y（d）是实际输出的数组（不是预测值），而x（d）是特征的数据集。
我阅读 href =“ https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/naive_bayes.py#l839“ rel =“ nofollow noreferrer”&gt; Documentation&gt; Documentation 但并没有完全满足我的目的。请有人可以 
帮助。 &lt;img alt = &lt;img alt =“ Enter Image Description在此处”]]></description>
      <guid>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</guid>
      <pubDate>Wed, 17 Oct 2018 18:08:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在Python中从头开始选择逻辑回归的功能？</title>
      <link>https://stackoverflow.com/questions/48403445/how-to-select-features-for-logistic-regression-from-scratch-in-python</link>
      <description><![CDATA[我一直在尝试从头开始编码逻辑回归，但是我正在使用乳腺癌数据集中的所有功能，并且我想选择一些功能（特别是我发现Scikit-Learn在与之进行比较并在数据上使用其功能选择时选择的功能）。但是，我不确定在我的代码中在哪里做，我目前拥有的是：
  x_train = [&#39;texture_mean&#39;，&#39;Smoothness_mean&#39;，&#39;compactness_mean&#39;，&#39;symmetry_mean&#39;，&#39;radius_se&#39;，&#39;symmetry_se&#39;
&#39;fractal_dimension_se&#39;，&#39;radius_worst&#39;，&#39;texture_worst&#39;，&#39;reage_worst&#39;，&#39;smoothness_worst&#39;，&#39;compactness_worst&#39;]
x_test = [&#39;texture_mean&#39;，&#39;Smoothness_mean&#39;，&#39;compactness_mean&#39;，&#39;symmetry_mean&#39;，&#39;radius_se&#39;，&#39;symmetry_se&#39;
&#39;fractal_dimension_se&#39;，&#39;radius_worst&#39;，&#39;texture_worst&#39;，&#39;reage_worst&#39;，&#39;smoothness_worst&#39;，&#39;compactness_worst&#39;]

Def Sigmoid（Z）：
    返回1/（1 + np.exp（-z））

DEF假设（Theta，X）：   
    返回sigmoid（x @ theta）

def cost_function（x，y，theta，m）：
    嗨=假设（theta，x）
    _y = y.Reshape（-1，1）
    j = 1/float（m） * np.sum（-_ y * np.log（hi） - （1-_y） * np.log（1-hi））
    返回j

def cost_function_derivivative（x，y，theta，m，alpha）：
    嗨=假设（theta，x）
    _y = y.Reshape（-1，1）
    j = alpha/float（m） * x.t @（hi -_y）
    返回j

def gradient_descent（x，y，theta，m，alpha）：
    new_theta = theta -cost_function_derivitive（x，y，theta，m，alpha）
    返回new_theta

缺陷准确性（theta）：
    正确= 0
    长度= len（x_test）
    预测=（假设（theta，x_test）＆gt; 0.5） 
    _y = y_test.Reshape（-1，1）
    正确=预测== _y
    my_accuracy =（np.sum（正确） /长度）*100
    打印（&#39;lr精度：&#39;，my_accuracy，“％＆quot”）

def logistic_regression（x，y，alpha，theta，num_iters）：
    m = len（y）
    对于x范围（num_iters）：
        new_theta = gradient_descent（x，y，theta，m，alpha）
        theta = new_theta
        如果x％100 == 0：
            打印＃（&#39;Theta：&#39;，Theta）    
            打印＃（&#39;COST：&#39;，cost_function（x，y，theta，m））
    准确性（theta）
EP = .012 
oniration_theta = np.random.rand（x_train.shape [1]，1） * 2 * EP -EP
alpha = 0.5
迭代= 10000
logistic_regression（x_train，y_train，alpha，initial_theta，迭代）
 
我假设如果我手动更改功能x_train和x_test的功能，其中包含在内，但是我会在 prinity_theta  line上遇到错误：
  attributeError：&#39;列表&#39;对象没有属性&#39;shape&#39; 
 ]]></description>
      <guid>https://stackoverflow.com/questions/48403445/how-to-select-features-for-logistic-regression-from-scratch-in-python</guid>
      <pubDate>Tue, 23 Jan 2018 13:55:31 GMT</pubDate>
    </item>
    <item>
      <title>训练后如何用分布的时间来替换嵌入层？</title>
      <link>https://stackoverflow.com/questions/39532572/how-to-replace-an-embedding-layer-with-a-time-distributed-dense-after-training</link>
      <description><![CDATA[我有以下问题：

 我想使用LSTM网络进行文本分类。为了加快训练的速度并使代码更加清楚，我想沿沿 keras.tokenizer 嵌入层以训练我的模型。 
 一旦我训练了我的模型 - 我想计算输出W.R.T.的显着性图。输入。为此，我决定将嵌入层替换为 timeDistributedDense 。 

您知道什么是最好的方法。对于一个简单的模型，我可以简单地使用已知权重的模型来重建模型 - 但我想使其尽可能通用 - 例如替换模型结构的未来并使我的框架尽可能不可知。]]></description>
      <guid>https://stackoverflow.com/questions/39532572/how-to-replace-an-embedding-layer-with-a-time-distributed-dense-after-training</guid>
      <pubDate>Fri, 16 Sep 2016 13:21:25 GMT</pubDate>
    </item>
    </channel>
</rss>