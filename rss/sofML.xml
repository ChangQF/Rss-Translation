<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 25 Jul 2024 18:20:07 GMT</lastBuildDate>
    <item>
      <title>B-cos 网络：我们只需要协调</title>
      <link>https://stackoverflow.com/questions/78794761/b-cos-networks-alignment-is-all-we-need</link>
      <description><![CDATA[我对解缠结和生成模型等主题非常感兴趣，最近读了这篇论文：https://arxiv.org/abs/2205.10268
我想知道在生成环境中，用 B-cos 变换（强制权重和输入之间的对齐）替换仿射变换是否有益。此外，使用这些新层来查找解缠结的潜在因子是否有帮助？我查看了所有引用，但没有人在生成框架中使用 B-cos 网络。也许有人对这个主题有更多的直觉，或者听说过已经尝试过类似内容的相关论文，他可以分享。
我看到的第一个（也是唯一的）问题是，权重输入对齐是通过在分类问题中最大化类逻辑来实现的。在生成环境中，我们可能会遇到完全不同的损失函数/优化问题。
我很乐意展开讨论，听取和谈论您/我的想法和意见！]]></description>
      <guid>https://stackoverflow.com/questions/78794761/b-cos-networks-alignment-is-all-we-need</guid>
      <pubDate>Thu, 25 Jul 2024 17:42:04 GMT</pubDate>
    </item>
    <item>
      <title>XGboost 的特征重要性仅返回 1000 个特征中的 1 个特征</title>
      <link>https://stackoverflow.com/questions/78794708/feature-importance-with-xgboost-returns-only-1-feature-out-of-1000-feature</link>
      <description><![CDATA[我的 XGClassifier 或 XGboost 在打印 get_fscore() 时总是只返回 1000 个特征中的一个特征。
xgbc.get_booster().get_fscore()

output- {feature_m:98&gt;
xgbc= XGBClassifier() 
model = xgbc.fit( X_train_scaled, Y_train)

我尝试更新不同的参数，但结果总是相同的。任何线索都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78794708/feature-importance-with-xgboost-returns-only-1-feature-out-of-1000-feature</guid>
      <pubDate>Thu, 25 Jul 2024 17:27:27 GMT</pubDate>
    </item>
    <item>
      <title>需要具有 10k 行独特上下文的合成 PII 数据集</title>
      <link>https://stackoverflow.com/questions/78794440/need-synthetic-pii-dataset-with-unique-contexts-for-10k-lines</link>
      <description><![CDATA[我正在寻找一个包含 10,000 行数据的合成数据集，其中包含各种类型的个人身份信息 (PII)，用于分类问题。数据应按段落格式化，并且每个段落应具有唯一的上下文。
我需要涵盖不同类型的 PII 数据，例如
[&quot;地址&quot;,
&quot;银行 • 帐号&quot;
&quot;信用卡 • 卡号&quot;
&quot;电子邮件地址&quot;
&quot;政府 - 身份证号码&quot;
&quot;个人姓名&quot;
&quot;密码&quot;
&quot;电话号码&quot;
&quot;密钥•（又称私钥）&quot;
121]
&quot;用户 ID&quot;,
&quot;出生日期&quot;,&quot;性别&quot;]

此外，每个段落在上下文中都是不同的，这一点至关重要。我尝试过使用 Faker，但它依赖于占位符模板，例如：
templates = [
&quot;{intro} {name} 出生于 {dob}，住在 {address}。您可以通过电子邮件 {email} 或电话 {phone} 联系他们。{closing}&quot;,
&quot;{intro} {name} 的社会安全号码是 {ssn}，护照号码是 {passport}。他们的信用卡号是 {ccn}。{closing}&quot;,
&quot;{intro} {name} 在 {license_year} 年获得了驾照号码 {dl}。 {closing}&quot;,
&quot;{intro} {name} 的电子邮件地址是 {email}，家庭住址是 {address}。他们出生于 {dob}，电话号码是 {phone}。{closing}&quot;,
&quot;{intro} {name} 的全名是 {name}，出生于 {dob}。他们的联系信息包括电话号码 {phone} 和电子邮件 {email}。他们居住在 {address}。{closing}&quot;
]

问题是这些句子在模板中重复出现，导致上下文变化有限。
我也尝试过使用 Kaggel，但它的结果没有涵盖所有必需的 PII 数据类型。还检查了 Github 存储库，但没有找到任何可靠的解决方案。
我正在寻找一种生成完全随机且唯一段落的方法。有人可以建议一种方法或工具来创建这样的数据集，或者提供有关生成具有多样化和独特背景的合成 PII 数据的指导吗？]]></description>
      <guid>https://stackoverflow.com/questions/78794440/need-synthetic-pii-dataset-with-unique-contexts-for-10k-lines</guid>
      <pubDate>Thu, 25 Jul 2024 16:19:35 GMT</pubDate>
    </item>
    <item>
      <title>训练模型检测垃圾邮件</title>
      <link>https://stackoverflow.com/questions/78794378/training-model-to-detect-spam</link>
      <description><![CDATA[我想训练一个模型来检测垃圾评论。不基于消息本身的内容，而是基于用户消息的频率和其他元数据。你认为我需要输入人工智能模型的关键数据是什么
我还没有特别建立模型]]></description>
      <guid>https://stackoverflow.com/questions/78794378/training-model-to-detect-spam</guid>
      <pubDate>Thu, 25 Jul 2024 16:00:26 GMT</pubDate>
    </item>
    <item>
      <title>使用变换对测试集进行留一编码</title>
      <link>https://stackoverflow.com/questions/78793796/leave-one-out-encoding-on-test-set-with-transform</link>
      <description><![CDATA[上下文：使用 sklearn 预处理数据集时，您会在训练集上使用 fit_transform，并在测试集上使用 transform，以避免数据泄露。使用留一法 (LOO) 编码时，您需要目标变量值来计算特征值的编码值。在管道中使用 LOO 编码器时，您可以使用 fit_transform 函数将其应用于训练集，该函数接受特征 (X) 和目标值 (y)。
在知道 transform 不接受目标变量值作为参数的情况下，如何使用相同的管道计算测试集的 LOO 编码？我对此很困惑。transform 函数确实转换了列，但没有考虑目标的值，因为它没有该信息。]]></description>
      <guid>https://stackoverflow.com/questions/78793796/leave-one-out-encoding-on-test-set-with-transform</guid>
      <pubDate>Thu, 25 Jul 2024 14:05:44 GMT</pubDate>
    </item>
    <item>
      <title>Flask 岭回归模型预测</title>
      <link>https://stackoverflow.com/questions/78793708/flask-ridge-regression-model-prediction</link>
      <description><![CDATA[当我运行 application.py 时，它将运行，当我将预测的数据点提供给模型，然后提交模型。然后将显示错误，如下所示
内部服务器错误
服务器遇到内部错误，无法完成您的请求。服务器超载或应用程序中出现错误。
我如何运行此应用程序并在给出值时给出预测点。我正在提供代码，请给我解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78793708/flask-ridge-regression-model-prediction</guid>
      <pubDate>Thu, 25 Jul 2024 13:48:04 GMT</pubDate>
    </item>
    <item>
      <title>在 pythonanywhere 中输入后预测页面未显示</title>
      <link>https://stackoverflow.com/questions/78793658/prediction-page-not-showing-after-giving-input-in-pythonanywhere</link>
      <description><![CDATA[我使用 pythonanywhere 实现了 flask 项目。我能够在 localhost 上实现它，但无法在 pythonanywhere 中实现它。只显示输入页面，提交后需要很长时间，并且不显示下一页。
可能的原因是什么，整个文件大小仅低于 2MB
我希望预测显示在结果页面中。索引页仍在加载.. 服务器日志中显示信号 9]]></description>
      <guid>https://stackoverflow.com/questions/78793658/prediction-page-not-showing-after-giving-input-in-pythonanywhere</guid>
      <pubDate>Thu, 25 Jul 2024 13:37:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 load_model 函数加载 HDF5 模型不起作用</title>
      <link>https://stackoverflow.com/questions/78793440/loading-hdf5-model-with-load-model-function-is-not-working</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78793440/loading-hdf5-model-with-load-model-function-is-not-working</guid>
      <pubDate>Thu, 25 Jul 2024 12:55:24 GMT</pubDate>
    </item>
    <item>
      <title>使用 CNN 进行单类物体检测获得假阳性</title>
      <link>https://stackoverflow.com/questions/78793283/single-class-object-detection-using-cnn-getting-false-positive</link>
      <description><![CDATA[在这里，我尝试使用 cnn 构建一个 Manhole 物体检测，在这个模型中，经过训练我得到了 95% 的准确率。我得到的是假阳性，例如，如果我用人孔（训练对象）测试图像进​​行检测，它将绘制边界框，而我测试没有训练对象的随机图像，则会出现一个随机边界框，这就是问题所在，在实时网络摄像头测试中也是如此，但在这里，如果对象甚至没有被检测到，则会在框架中获取一些随机边界框。这里我提供我的代码，请帮助
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Conv2D, Input, BatchNormalization``, Flatten, MaxPool2D, Dense
from pathlib import Path

train_img = Path(&quot;DATASET/train/Manhole&quot;)
val_img = Path(&quot;DATASET\valid\Manhole&quot;)

train_csv = pd.read_csv(&#39;DATASET/train/Manhole/_annotations.csv&#39;) 
val_csv = pd.read_csv(&#39;DATASET/valid/_annotations.csv&#39;)
#print(train_csv)
train_csv[[&#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;]] = train_csv[[&#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;]].fillna(0)
train_csv[[&#39;xmin&#39;,&#39;ymin&#39;,&#39;xmax&#39;,&#39;ymax&#39;]] = train_csv[[&#39;xmin&#39;,&#39;ymin&#39;,&#39;xmax&#39;,&#39;ymax&#39;]].astype(int)
train_csv.drop_duplicates(subset=&#39;filename&#39;,inplace=True, ignore_index=True)
val_csv.drop_duplicates(subset=&#39;filename&#39;, inplace=True, ignore_index=True)

def datagenerator(df ,batch_size ,path):
while True:
images = np.zeros((batch_size,640,640,3))
bounding_box_coords = np.zeros((batch_size, 4))

for i in range(batch_size):
rand_index = np.random.randint(0, train_csv.shape[0])
row = df.loc[rand_index, :]
images[i] = cv2.imread(str(path/row.filename)) / 255.
bounding_box_coords[i] = np.array([row.xmin, row.ymin, row.xmax, row.ymax])

产生 {&#39;filename&#39;: images}, {&#39;coords&#39;: bounding_box_coords}

# example, label = next(datagenerator(batch_size=16))
# img = example[&#39;filename&#39;][0]
# bbox_coords = label[&#39;coords&#39;][0] 

# x1, y1, x2, y2 = map(int, bbox_coords)
# print(&#39;bbox cords&#39;,x1,y1,x2,y2)
# cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)
# cv2.putText(img, &#39;&#39;, (x1,y1-10),cv2.FONT_HERSHEY_DUPLEX, 0.8, (0, 0, 255), 2)
# # plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
# plt.imshow(img)
# plt.show()

input_ = 输入(shape=[640, 640, 3], name=&#39;filename&#39;)

x = input_
x = Conv2D(16, (3,3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2, 填充=&#39;same&#39;)(x)

x = Conv2D(32, (3,3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2, 填充=&#39;same&#39;)(x)

x = Conv2D(64, (3,3),激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(128，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(256，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(312，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(500，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(580，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(680，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Flatten()(x)
x = Dense(256，激活=&#39;relu&#39;)(x)
x = Dense(32, 激活=&#39;relu&#39;)(x)
输出坐标 = Dense(4, 名称=&#39;coords&#39;)(x)

模型 = tf.keras.models.Model(input_,output_coords)

模型摘要()

模型编译(loss={&#39;coords&#39;: &#39;mse&#39;},
优化器=tf.keras.optimizers.Adam(5e-5), 
指标={&#39;coords&#39;: &#39;accuracy&#39;})

检查点回调 = ModelCheckpoint(&#39;model_Checkpoint.h5&#39;, 监视器=&#39;val_loss&#39;, save_best_only=True, 模式=&#39;min&#39;)

模型拟合(数据生成器(df=train_csv,batch_size=6,path=train_img), 
epochs=80, steps_per_epoch=150,
validation_data=datagenerator(df=val_csv,batch_size=6,path=val_img), 
validation_steps=240, 
callbacks=[checkpoint_callback])

model.save(&#39;model2.h5&#39;)

我需要代码来在实时网络摄像头中正确检测训练过的对象，而不会出现任何边界框，并从 cnn 接收置信度值，这样我就可以设置检测的阈值]]></description>
      <guid>https://stackoverflow.com/questions/78793283/single-class-object-detection-using-cnn-getting-false-positive</guid>
      <pubDate>Thu, 25 Jul 2024 12:22:40 GMT</pubDate>
    </item>
    <item>
      <title>EMA 衰减和 LR 衰减之间的实际差异</title>
      <link>https://stackoverflow.com/questions/78793123/practical-difference-between-ema-decay-and-lr-decay</link>
      <description><![CDATA[我无法理解 EMA 衰减和 LR 衰减在实践中的差异。
我觉得它们都以不同的方式完成了相同的事情（以下内容可能是错误的，所以我提前道歉，如果我的理解完全错误，请纠正我）：

使用 EMA，在训练期间保留模型的单独副本，每 N 步使用原始模型权重的平均值更新模型。
使用 LR 衰减，原始模型的权重在训练期间更新的次数总是较少，但只有一个模型得到有效训练。

现在，给定一个包含 32 个样本的数据集，我可以想象这是两次训练的进行方式：
训练 A（无 EMA）
给定以下超参数：

LR 1e-5
批次大小为 4
线性调度程序

经过 4 个步骤后，模型将看到 16 个样本，LR 将下降到最终 LR 的一半。
实际上，模型已更新 4 次。
训练 B (EMA)
给定以下超参数：

LR 为 1e-5
批次大小为 1
恒定调度程序
EMA 衰减为 0.9999
EMA 更新步骤为 4

经过 16 个步骤后，模型将看到 16 个样本，EMA 衰减将上升到最终 EMA 的一半衰减。
实际上，原始模型已更新 16 次，EMA 模型已更新 4 次。
问题
最终，两个模型都更新了 4 次，我能看到的唯一区别是训练 A 直接更新了权重，而训练 B 更新了原始模型和 EMA 模型的权重。
为什么人们决定选择训练 B 而不是训练 A？]]></description>
      <guid>https://stackoverflow.com/questions/78793123/practical-difference-between-ema-decay-and-lr-decay</guid>
      <pubDate>Thu, 25 Jul 2024 11:49:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 Gratz 大学 LSM 模型预测 Lorenz 吸引子</title>
      <link>https://stackoverflow.com/questions/78792982/predicting-lorenz-attractor-using-the-gratz-university-lsm-model</link>
      <description><![CDATA[我目前正在研究 LSM，在阅读了几篇论文后，我认为我掌握了有关这种 Reservoir Computing 模型的主要信息。
但是，我很难用我用于 Lorenz Attractor 的模型获得良好的结果。
目前，我正试图仅预测 X 分量，但结果很糟糕。
我尝试了 NEST 模拟器 中的两种生成器。spike_generator 和 step_current_generator。
使用 step_current_generator 我有更好的结果，但我想坚持使用 W.Maas 模型并使用 spike_generator。
这是我所做的：
def generate_spike_times_lorenz(data, stim_times, gen_burst, scale_factor=100.0):
&quot;&quot;&quot;
根据 Lorenz X 分量生成尖峰时间。

参数：
- data：类似数组，Lorenz X 分量的值。
- stim_times：类似数组，刺激的时间。
- gen_burst：函数，在给定时间附近生成一连串尖峰。
- scale_factor：浮点数，缩放尖峰时间的因子。

返回：
- inp_spikes：数组列表，每个数组包含一个输入神经元的脉冲时间。
“” “”

inp_spikes = []

data = data * scale_factor

for value, t in zip(data, stim_times):
spike_count = int(value)
if spike_count &gt; 0:
spikes = np.concatenate([t + gen_burst() for _ in range(spike_count)])

# 缩放并调整尖峰时间
spikes *= 10
spikes = spikes.round() + 1.0
spikes = spikes / 10.0

spikes = np.sort(spikes)

inp_spikes.append(spikes)
else:
inp_spikes.append(np.array([]))

return inp_spikes

def injection_spikes(inp_spikes, neuron_targets):
spike_generators = nest.Create(&quot;spike_generator&quot;, len(inp_spikes))

for sg, sp in zip(spike_generators, inp_spikes):
nest.SetStatus(sg, {&#39;spike_times&#39;: sp})

C_inp = 100 # int(N_E / 20) # 每个输入神经元的传出输入突触数量

def generate_delay_normal_clipped(mu=10., sigma=20., low=3., high=200.):
delay = np.random.normal(mu, sigma)
delay = max(min(delay, high), low)
return delay

nest.Connect(spike_generators, neuron_targets,
{&#39;rule&#39;: &#39;fixed_outdegree&#39;,
&#39;outdegree&#39;: C_inp},
{&#39;synapse_model&#39;: &#39;static_synapse&#39;,
&#39;delay&#39;: generate_delay_normal_clipped(),
&#39;weight&#39;: nest.random.uniform(min=2.5 * 10 * 5.0, max=7.5 * 10 * 5.0)})

有人能提供一些见解或建议，告诉我如何改进我的模型，特别是在使用 spike_generator 来提高性能方面吗？我可能忽略了哪些特定的参数或技术？]]></description>
      <guid>https://stackoverflow.com/questions/78792982/predicting-lorenz-attractor-using-the-gratz-university-lsm-model</guid>
      <pubDate>Thu, 25 Jul 2024 11:11:09 GMT</pubDate>
    </item>
    <item>
      <title>hmmlearn 中的隐马尔可夫模型不收敛</title>
      <link>https://stackoverflow.com/questions/78791079/hidden-markov-model-in-hmmlearn-not-converging</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78791079/hidden-markov-model-in-hmmlearn-not-converging</guid>
      <pubDate>Thu, 25 Jul 2024 00:56:12 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch LSTM 模型上的 CrossEntropyLoss 每个时间步进行一个分类</title>
      <link>https://stackoverflow.com/questions/78781313/crossentropyloss-on-pytorch-lstm-model-with-one-classification-per-timestep</link>
      <description><![CDATA[我正在尝试制作一个 LSTM 模型，用于检测时间序列数据中的异常。它需要 5 个输入并产生 1 个布尔输出（如果检测到异常则为 True/False）。异常模式通常连续出现在 3 - 4 个时间步之间。与大多数 LSTM 示例不同，它们预测未来数据或对整个数据序列进行分类，我试图在每个时间步长输出一个 True/False 检测标志（如果检测到，则在模式中的最后一个时间步长点输出 True）。
不幸的是，CrossEntropyLoss 似乎不允许超过 1D 的输出张量，在这种情况下它将是 2D [序列数，序列长度和布尔数据]
以下是我正在尝试生成的一些示例代码：
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# 定义 LSTM 分类器模型
class LSTMClassifier(nn.Module):
def __init__(self, input_size, hidden_​​size, num_layers, output_size):
super(LSTMClassifier, self).__init__()
self.hidden_​​size = hidden_​​size
self.num_layers = num_layers
self.lstm = nn.LSTM(input_size, hidden_​​size, num_layers, batch_first=True)
self.fc = nn.Linear(hidden_​​size, output_size)

def forward(self, x):
h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_​​size).to(x.device)
c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_​​size).to(x.device)
out, _ = self.lstm(x, (h0, c0))
out = self.fc(out[:, -1, :])
return out

# 输入 - 100 个示例，每个时间步包含 5 个数据点（其中有 10 个时间步）
X_train = np.random.rand(100, 10, 5)
# 输出 - 100 个示例，每个时间步包含 1 个 True/False 输出以匹配输入
y_train = np.random.choice(a=[True, False], size=(100, 10)) # 二进制标签（True 或 False）

# 将数据转换为 PyTorch 张量
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.bool)

# 定义模型参数
input_size = X_train.shape[2] # 每个时间步 5 个输入
hidden_​​size = 4 # 我们尝试检测的模式通常为 4 个时间步长
num_layers = 1
output_size = 1 # True/False

# 实例化模型
model = LSTMClassifier(input_size, hidden_​​size, num_layers, output_size)

# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练模型
num_epochs = 10
for epoch in range(num_epochs):
optimizer.zero_grad()
output = model(X_train_tensor)
loss = criterion(outputs, y_train_tensor)
loss.backward()
optimizer.step()
print(f&#39;Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}&#39;)

# 测试模型
X_test = np.random.rand(10, 10, 5) # 生成一些测试数据 - 与输入相同的维度
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
with torch.no_grad():
predictions = model(X_test_tensor)
predict_outputs = torch.argmax(predictions, dim=1)
print(&quot;Predicted Outputs:&quot;, predict_outputs)

我是否需要重新调整输出（或者使 LSTM 的输出数量等于序列长度），或者使用不同的损失函数，或者使用 LSTM 以外的模型？]]></description>
      <guid>https://stackoverflow.com/questions/78781313/crossentropyloss-on-pytorch-lstm-model-with-one-classification-per-timestep</guid>
      <pubDate>Tue, 23 Jul 2024 02:11:04 GMT</pubDate>
    </item>
    <item>
      <title>karateclub MUSAE 嵌入产生奇怪的列数</title>
      <link>https://stackoverflow.com/questions/78623717/karateclub-musae-embedding-produces-strange-number-of-columns</link>
      <description><![CDATA[我正在试验属性节点嵌入和结构嵌入，但 karateclub 实现返回的矩阵具有奇怪的列数。
MUSAE 给出 128 个“特征”，而不是请求的 32 个。当我请求 32 个时，GLEE 给出了 33 个。我遗漏了什么吗？
import random
import numpy as np
import networkx as nx
from scipy.sparse import coo_matrix

from karateclub.node_embedding.attributed import MUSAE
from karateclub.node_embedding.neighbourhood import GLEE

g = nx.newman_watts_strogatz_graph(50, 10, 0.2)

X = {i: random.sample(range(150),50) for i in range(50)}

row = np.array([k for k, v in X.items() for val in v])
col = np.array([val for k, v in X.items() for val in v])
data = np.ones(50*50)
shape = (50, 150)

X = coo_matrix((data, (row, col)), shape=shape)

model = MUSAE(dimensions=32)
model.fit(g, X)
emb = model.get_embedding()
print(emb.shape)

model = GLEE(dimensions=32)
model.fit(g)
emb = model.get_embedding()
print(emb.shape)

输出：
(50, 128)
(50, 33)
]]></description>
      <guid>https://stackoverflow.com/questions/78623717/karateclub-musae-embedding-produces-strange-number-of-columns</guid>
      <pubDate>Fri, 14 Jun 2024 15:02:08 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 示例与 SequenceExample</title>
      <link>https://stackoverflow.com/questions/46857596/tensorflow-example-vs-sequenceexample</link>
      <description><![CDATA[TensorFlow 文档中没有提供太多信息：
https://www.tensorflow.org/api_docs/python/tf/train/Example
https://www.tensorflow.org/api_docs/python/tf/train/SequenceExample
它只是向您推荐：
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto
每个都有简短的描述以及示例；但是，我仍然不太明白为什么你会选择其中一个而不是另一个？有人可以帮忙解释一下吗？]]></description>
      <guid>https://stackoverflow.com/questions/46857596/tensorflow-example-vs-sequenceexample</guid>
      <pubDate>Fri, 20 Oct 2017 21:28:52 GMT</pubDate>
    </item>
    </channel>
</rss>