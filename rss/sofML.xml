<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 08 Aug 2024 01:08:15 GMT</lastBuildDate>
    <item>
      <title>如何使用 Sarimax 预测日期？</title>
      <link>https://stackoverflow.com/questions/78845748/how-to-project-forecast-dates-with-sarimax</link>
      <description><![CDATA[我正在使用 Sarimax 进行预测：
# 仅使用 VENDA 过滤数据集
df_int_aux = pd.DataFrame(df_internal[&#39;VENDA&#39;])

# 使用训练数据创建模型
train = round(len(df_int_aux) * 0.85) 
test = len(df_int_aux) - train

model_val = sm.tsa.statespace.SARIMAX(df_int_aux[&quot;VENDA&quot;][:train], order=(0,0,1), seasonal_order=(1, 1, 1, 4), exog=df_internal[&#39;C_EF_VENDA&#39;][:train])

# 拟合模型
model_val_fit = model_val.fit()

# 预测测试数据
validation = model_val_fit.get_forecast(steps=test, exog=df_internal[&#39;C_EF_VENDA&#39;][-test:]) 
validation_mean = validation.predicted_mean

但是，validation_mean 数据集未显示未来日期。它显示的数字索引范围从 101 到 118。数据集有 109 行。我使用前 100 行进行训练，因此第 101 行到第 118 行是模型的预测值。
为什么没有显示预计日期？我该如何解决这个问题？
以下是数据集的示例。可能是因为日期没有遵循特定的频率或模式，所以没有显示日期？
DATE VENDA C_EF_VENDA
2022-01-01 6.004414 12.122044
2022-01-11 10.933905 22.073975
2022-01-18 11.589626 23.397781
2022-01-25 21.005069 42.406200
2022-02-01 8.639416 14.461015
2022-02-08 16.847755 28.200475
2022-02-15 17.289413 28.939740
2022-02-22 16.966222 28.398770
]]></description>
      <guid>https://stackoverflow.com/questions/78845748/how-to-project-forecast-dates-with-sarimax</guid>
      <pubDate>Wed, 07 Aug 2024 21:30:19 GMT</pubDate>
    </item>
    <item>
      <title>数值数据中的异常值检测问题</title>
      <link>https://stackoverflow.com/questions/78845677/issues-with-outlier-detection-in-numerical-data</link>
      <description><![CDATA[我目前正在进行一个数据分析项目，其中我使用 Z 分数来检测数据集数值列中的异常值。但是，我遇到了一个问题，合法的数据点被标记为异常值，我不确定为什么会发生这种情况。
这是我正在做的事情：

缺失值的插补：我使用 sklearn.impute 中的 IterativeImputer 来填充数值列中的缺失值。

异常值检测：我计算每个数值列的 Z 分数，以使用阈值 3 来检测异常值。
例如，我有一条关于埃及古典式摔跤运动员 Yasser Abdel Rahman Sakr 的记录，其属性如下：

体重：120 公斤
身高：180 厘米



尽管这些是合理的测量值，但该记录被标记为我的代码中的异常值。其他记录也出现了此问题。
以下是我的代码的相关部分：
import numpy as np
import pandas as pd
from sklearn.impute import IterativeImputer

# 假设“数据”已定义并加载
numeric_cols = data.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]).columns
categorical_cols = data.select_dtypes(include=[&#39;object&#39;]).columns

# 在数字列中插入缺失值
mice_imputer = IterativeImputer(max_iter=10, random_state=0)
df_numeric = pd.DataFrame(mice_imputer.fit_transform(data[numeric_cols]), columns=numeric_cols)

# 将插入的数字列与原始分类列合并列
df_MICE = pd.concat([df_numeric, data[categorical_cols]], axis=1)

# 存储异常信息的字典
outliers_info = {}

for col in numeric_cols:
# 计算平均值和标准差
mean = df_MICE[col].mean()
std_dev = df_MICE[col].std()

# 如果 std_dev 为零，则避免除以零
if std_dev == 0:
print(f&quot;列 {col} 的标准差为零。跳过异常值检测。)
继续

# 计算 Z 分数
z_scores = (df_MICE[col] - mean) / std_dev

# 定义异常值阈值
阈值 = 3

# 查找异常值
outliers = df_MICE[np.abs(z_scores) &gt;阈值]

# 将异常值的数量和异常值样本存储在字典中
outliers_info[col] = {
&#39;count&#39;: len(outliers),
&#39;sample&#39;: outliers.head(1) # 一个异常值的样本
}

# 打印每个数值列的异常值数量和样本
for col, info in outliers_info.items():
print(f&#39;Column: {col}&#39;)
print(f&#39;Number of outliers: {info[&quot;count&quot;]}&#39;)
if info[&#39;count&#39;] &gt; 0：
print(&#39;样本异常值：&#39;)
print(info[&#39;sample&#39;])
else:
print(&#39;无异常值。&#39;)
print() # 打印空白行以提高可读性

问题：
尽管是真实且可信的记录，但 Yasser Abdel Rahman Sakr 的体重和身高被标记为异常值。其他记录也会出现此问题。
问题：

什么原因导致合法数据点被标记为异常值？
是否有任何改进或替代方法可以更好地处理此情况下的异常值检测？
我是否应该考虑其他因素或异常值检测方法？
]]></description>
      <guid>https://stackoverflow.com/questions/78845677/issues-with-outlier-detection-in-numerical-data</guid>
      <pubDate>Wed, 07 Aug 2024 21:07:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Mediapipe 根据特定面部区域过滤面部标志坐标？</title>
      <link>https://stackoverflow.com/questions/78845589/how-to-filter-face-landmark-coordinates-by-specific-facial-regions-using-mediapi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78845589/how-to-filter-face-landmark-coordinates-by-specific-facial-regions-using-mediapi</guid>
      <pubDate>Wed, 07 Aug 2024 20:41:07 GMT</pubDate>
    </item>
    <item>
      <title>YOLov8 无法在本地机器上做出正确的预测[关闭]</title>
      <link>https://stackoverflow.com/questions/78845245/yolov8-not-making-correct-prediction-on-local-machine</link>
      <description><![CDATA[我尝试在本地机器上运行 yolo v8，但做出了错误的预测，即它在图像顶部预测了很多 100% 的 bbox。所有 bbox 都在那里
本地机器结果
但是，如果我在 kaggle/colab 上运行它，它工作正常
vm 结果
vm/local 上的两台机器都是基于 cpu 的，没有使用 gpu。]]></description>
      <guid>https://stackoverflow.com/questions/78845245/yolov8-not-making-correct-prediction-on-local-machine</guid>
      <pubDate>Wed, 07 Aug 2024 18:44:35 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Matlab 中使用 knnsearch 设置 k 值</title>
      <link>https://stackoverflow.com/questions/78844904/how-to-set-k-value-using-knnsearch-in-matlab</link>
      <description><![CDATA[我有一个代码来对图像进行分类。
training1 = xlsread(&#39;Data Train&#39;);

% 提及训练数据矩阵在 excel 文件中的位置
training = [training1(:,1) training1(:,2) training1(:,3) training1(:,4) training1(:,5) training1(:,6) training1(:,7) training1(:,8) training1(:,9) training1(:,10) training1(:,11) training1(:,12) training1(:,13) training1(:,14) training1(:,15) training1(:,16) training1(:,17) training1(:,18) training1(:,19) training1(:,20) training1(:,21) training1(:,22) training1(:,23) training1(:,24)];

% 提及输入数据变量
Z=[MeanR MeanG MeanB MeanH MeanS MeanV VarRed VarGreen VarBlue VarH VarS VarV RangeR RangeG RangeB RangeH RangeS RangeV sdR sdG sdB sdH sdS sdV];

%执行 knn 分类
result = knnsearch(training,Z);

if (result&gt;=1 &amp;&amp; result&lt;=20)
set(handles.EditBox,&#39;string&#39;,&#39;Raw&#39;);
elseif (result&gt;=21 &amp;&amp; result&lt;=40)
set(handles.EditBox,&#39;string&#39;,&#39;Undercook&#39;);
elseif (result&gt;=41 &amp;&amp; result&lt;=60)
set(handles.EditBox,&#39;string&#39;,&#39;Cook&#39;);
elseif (result&gt;=61 &amp;&amp; result&lt;=80)
set(handles.EditBox,&#39;string&#39;,&#39;Rotten&#39;);
end

knnsearch语法是否只默认k值为1？
如何才能让knnsearch中的k值为5？
当我尝试将其更改为
k = 5;
result = knnsearch(training,Z,&#39;K&#39;,k); 

系统不显示分类结果。]]></description>
      <guid>https://stackoverflow.com/questions/78844904/how-to-set-k-value-using-knnsearch-in-matlab</guid>
      <pubDate>Wed, 07 Aug 2024 17:01:04 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降算法中的学习率</title>
      <link>https://stackoverflow.com/questions/78844901/learning-rate-in-gradient-descent-algorithm</link>
      <description><![CDATA[在梯度下降算法中，我根据它们的导数更新B和M值，然后将它们与学习率值相乘，但是当我对L使用相同的值，例如0.0001时，它不能正常工作。减小或增加L值不起作用。作为一种解决方法，我不得不为b和m值设置不同的L值。这是正常的还是有错误？
import pandas as pd
import matplotlib.pyplot as plt
import time
import random

# Veri seti
veri_seti = &quot;study_score_decreasing.csv&quot; #study_score_decreasing.csv #study_score_increasing.csv 
data = pd.read_csv(veri_seti)

# 梯度下降 Fonksiyonu
def gradient_descent(m_next, b_next, points, L):
m_gradient = 0
b_gradient = 0
n = len(points)

for i in range(n):
x = points.iloc[i].study_time
y = points.iloc[i].score

m_gradient += -(2/n) * x * (y - (m_next * x + b_next))
b_gradient += -(2/n) * (y - (m_next * x + b_next))

m = m_next - m_gradient * 0.0001 #(L = 0.0001)
b = b_next - b_gradient * 0.1 #(L = 0.1)

return m, b

# 图形选项 图表
def show_graph(m, b):
plt.scatter(data.study_time, data.score, color=&quot;red&quot;)
x_range = range(int(data.study_time.min()), int(data.study_time.max()) + 1)
plt.plot(x_range, [m * x + b for x in x_range], color=&quot;blue&quot;)
plt.xlabel(&#39;学习时间&#39;)
plt.ylabel(&#39;分数&#39;)
plt.title(&#39;学习时间与分数&#39;)
plt.show()
time.sleep(0.001)
print(&quot;=&gt; F(X):&quot;, round(m, 1), &quot;X +&quot;, round(b, 3))

# Ana Fonksiyon
def main(m, b, L, epochs):
print(&quot;=&gt; F(X):&quot;, m, &quot;X&quot;, b)

for i in range(epochs):
m, b = gradient_descent(m, b, data, L)
show_graph(m, b)

# 基础说明
main(random.uniform(-1, 110), random.uniform(-10, 10), 0.1, 250)

我逐个更新了L值，得到了合乎逻辑的结果，但是用一个共同的L值，为什么解看起来不合逻辑？]]></description>
      <guid>https://stackoverflow.com/questions/78844901/learning-rate-in-gradient-descent-algorithm</guid>
      <pubDate>Wed, 07 Aug 2024 16:59:10 GMT</pubDate>
    </item>
    <item>
      <title>使用 7 个类别训练图像分类器，但我的模型过度拟合，导致模型的准确性在训练过程中表现异常</title>
      <link>https://stackoverflow.com/questions/78844629/training-image-classifier-with-7-classes-but-my-model-is-overfitting-resulting-t</link>
      <description><![CDATA[我正在为特定汽车发动机部件的 7 种不同模型类型训练图像分类器。每个类别都有 308 张灰度图像，分辨率均为 1014x760。这些图像主要由白色屏幕上的发动机部件组成，每次拍照后都会旋转 60 度，因此数据集由看起来彼此非常相似的图片组成。我想训练我的模型 50 个 epoch，但在第 30 个 epoch 之后，准确率达到 1.0，而验证准确率停留在 0.2 左右。为什么结果这么奇怪？是不是图片太相似了？
import numpy as np
import pickle
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.utils import to_categorical
import time

name = &quot;Core_Classifier_{}&quot;.format(int(time.time()))

tensorboard = TensorBoard(log_dir=&quot;logs/{}&quot;.format(name))

X = pickle.load(open(&quot;X.pickle&quot;, &quot;rb&quot;))
y = pickle.load(open(&quot;y.pickle&quot;, &quot;rb&quot;))

X = X/255.0 # 标准化颜色值
y = to_categorical(y, num_classes=7)

model = Sequential()

model.add(Conv2D(64, (3, 3), input_shape = X.shape[1:]))
model.add(Activation(&quot;relu&quot;))
model.add(MaxPooling2D(pool_size = (2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation(&quot;relu&quot;))

model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Flatten())

model.add(Dense(64))
model.add(Activation(&quot;relu&quot;)) # idk 是否需要

model.add(Dense(7))
model.add(Activation(&quot;softmax&quot;))

model.compile(loss = &quot;categorical_crossentropy&quot;,
optimizer = &quot;adam&quot;,
metrics = [&quot;accuracy&quot;])

model.fit(X, y, batch_size = 64, epochs = 50, validation_split = 0.1, callbacks = [tensorboard])

以下是通过 tensorboard 表示的图形
我添加了2个model.add(Dropout(0.2))函数，但结果没有太大变化。
import numpy as np
import pickle
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.utils import to_categorical
import time

name = &quot;Core_Classifier_{}&quot;.format(int(time.time()))

tensorboard = TensorBoard(log_dir=&quot;logs/{}&quot;.format(name))

X = pickle.load(open(&quot;X.pickle&quot;, &quot;rb&quot;))
y = pickle.load(open(&quot;y.pickle&quot;, &quot;rb&quot;))

X = X/255.0 # 标准化颜色值
y = to_categorical(y, num_classes=7)

model = Sequential()

model.add(Conv2D(64, (3, 3), input_shape = X.shape[1:]))
model.add(Activation(&quot;relu&quot;))
model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(64, (3, 3)))
model.add(Activation(&quot;relu&quot;))
model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Dropout(0.2))
model.add(Flatten())

model.add(Dense(64))
model.add(Activation(&quot;relu&quot;)) # idk 是否需要

model.add(Dense(7))
model.add(Activation(&quot;softmax&quot;))

model.compile(loss = &quot;categorical_crossentropy&quot;,
optimizer = &quot;adam&quot;,
metrics = [&quot;accuracy&quot;])

model.fit(X, y, batch_size = 64, epochs = 50, validation_split = 0.1, callbacks = [tensorboard])

训练结果反馈退出函数]]></description>
      <guid>https://stackoverflow.com/questions/78844629/training-image-classifier-with-7-classes-but-my-model-is-overfitting-resulting-t</guid>
      <pubDate>Wed, 07 Aug 2024 15:51:31 GMT</pubDate>
    </item>
    <item>
      <title>构建模拟 SVM 模型的自定义分类器</title>
      <link>https://stackoverflow.com/questions/78843755/building-a-custom-classifier-that-simulates-svm-model</link>
      <description><![CDATA[我在代码中使用了以下 SVM：
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classes_report, confusion_matrix, f1_score

# 加载数据
data = pd.read_csv(&#39;data.csv&#39;)

# 分离特征 (X) 和目标变量 (y)
X = data.drop(columns=&#39;label&#39;)
y = data[&#39;label&#39;]

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 标准化特征
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 定义小网格搜索的参数网格
param_grid = {
&#39;C&#39;: [0.1, 1, 10],
&#39;gamma&#39;: [&#39;scale&#39;, 0.01, 0.1]
}

# 执行带有交叉验证的网格搜索
grid_search = GridSearchCV(SVC(kernel=&#39;rbf&#39;), param_grid, cv=3,scoring=&#39;f1_weighted&#39;, verbose=2, n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)

# 来自网格搜索的最佳参数
best_params = grid_search.best_params_
print(f&#39;Best parameters: {best_params}\n&#39;)

# 训练 SVM 模型使用最佳参数
svm_best = SVC(kernel=&#39;rbf&#39;, C=best_params[&#39;C&#39;], gamma=best_params[&#39;gamma&#39;])
svm_best.fit(X_train_scaled, y_train)

# 对测试集进行预测
y_pred_best = svm_best.predict(X_test_scaled)

# 对改进模型的评估
print(&quot;改进的 SVM 模型评估&quot;)
print(confusion_matrix(y_test, y_pred_best))
print(classification_report(y_test, y_pred_best))
improved_f1 = f1_score(y_test, y_pred_best, average=&#39;weighted&#39;)
print(f&#39;改进的加权 F1 分数： {improved_f1}\n&#39;)


如您所见，我直接使用来自 sklearn 的 SVM 模型。我如何创建一个名为“分类器”的类，它将执行相同的操作并获得相同的结果？这可能吗？
我尝试创建类并使用每个函数的参数，但结果总是更糟。]]></description>
      <guid>https://stackoverflow.com/questions/78843755/building-a-custom-classifier-that-simulates-svm-model</guid>
      <pubDate>Wed, 07 Aug 2024 12:39:48 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“datachain.lib”的模块；“datachain”不是一个包</title>
      <link>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</link>
      <description><![CDATA[
为什么我会遇到 datachain.lib 模块的 ModuleNotFoundError？
我需要采取其他步骤才能在项目中正确使用 datachain 包吗？

我正在开发一个 Python 项目，在尝试导入模块时遇到以下错误：
import os
os.environ[&quot;PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION&quot;] = &quot;python&quot;
import tensorflow as tf
import numpy as np
from PIL import Image
from datachain.lib.dc import Column, DataChain

错误消息：
ModuleNotFoundError：没有名为“datachain.lib”的模块； &#39;datachain&#39; 不是包

详细信息：

我已使用 pip 安装了 datachain：pip install datachain。
通过运行 pip list 可看到 datachain 的安装版本为 0.2.18。
我已验证包已正确安装并位于我的 Python 环境中。
]]></description>
      <guid>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</guid>
      <pubDate>Wed, 07 Aug 2024 09:53:07 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 的暹罗网络指南[关闭]</title>
      <link>https://stackoverflow.com/questions/78842114/guidelines-for-siamese-network-using-r</link>
      <description><![CDATA[我的数据库中存储了 20,000 多张艺术品图片（绘画、雕塑、罐子等）。实际作品分布在多个仓库中。理想情况下，实物应该贴上标签（带有其 ID、QR 码等），这些标签是纸质的，因此可能会受损、印刷不良、无法读取、完全丢失甚至放错位置。我的目标是创建一个模型，该模型接收输入（任何仓库的某人发送的图像），从可用数据中识别完全相同的艺术品并返回其 ID、详细信息等。
在我的例子中，样本是静态的、固定的（不会有“新”艺术品，除非客户购买更多），因此模型永远不会“看到”新图像，这让我认为过度拟合可能是模型最理想的结果（这意味着大量的数据增强和大量的 epoch）。
请注意，每个类（艺术作品）只有一个图像可用。这就是无法改变的情况。
所选的编程语言是 R，主要是 tensorflow 和 keras3 库。
话虽如此，我很难找到解决方案，因为每个文档都依赖于相同的 cat vs dogs 或 mnist 数据集。我的问题是：

暹罗网络是否是用于此目的的正确算法？
我可以采取哪些方法来提高准确性？

仅出于测试目的，我抽取了 10 个样本，从每个样本中生成了 9 个其他样本（数据增强、应用旋转、垂直/水平翻转、随机饱和度因子、随机亮度因子等）。后来，为每个类别创建了 5 个正对和 5 个负对。最后，我运行了一个暹罗网络，但准确率似乎停留在 49%。]]></description>
      <guid>https://stackoverflow.com/questions/78842114/guidelines-for-siamese-network-using-r</guid>
      <pubDate>Wed, 07 Aug 2024 06:29:33 GMT</pubDate>
    </item>
    <item>
      <title>什么是代币、Top K 和 Top P？[关闭]</title>
      <link>https://stackoverflow.com/questions/78841275/what-are-tokens-top-k-and-top-p</link>
      <description><![CDATA[我正在学习使用 Google AI Studio，在生成代码片段时，我遇到了这些术语：
constgenerationConfig = {
temperature: 1,
topP: 0.95,
topK: 64,
maxOutputTokens: 8192,
responseMimeType:&quot;text/plain&quot;,
};

我很难理解这些术语的含义。topP、topK 和 maxOutputTokens 是什么。我想了解这些，以便正确使用它们。]]></description>
      <guid>https://stackoverflow.com/questions/78841275/what-are-tokens-top-k-and-top-p</guid>
      <pubDate>Tue, 06 Aug 2024 22:55:28 GMT</pubDate>
    </item>
    <item>
      <title>yolov9 在自定义数据上进行训练</title>
      <link>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</link>
      <description><![CDATA[我正尝试在 PyCharm 而不是 google colab 上用一些自定义数据训练 yolov9。我该怎么做？
将存储库克隆到我的计算机后，我在虚拟环境中安装了所有要求。然后我创建了训练脚本，但我觉得有些短。
这是我的训练脚本：
import os
import subprocess

dataset_path = &#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject&#39;

def train_yolov5(train_images_path, val_images_path, yaml_file_path, weights_path=&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main/yolov9-c.pt&#39;, epochs=50):

# 获取 yolov5 目录的绝对路径
yolov9_dir = os.path.abspath(&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main&#39;)

# 将当前工作目录更改为 yolov9 目录
os.chdir(yolov9_dir)
# 训练 yolov9 模型
command = f&#39;python train.py --workers 8 --device cpu --batch 16 --data {dataset_path}/sfdV2_musa.yaml --img 640 --cfg models/detect/yolov9-c.yaml --weights yolov9-c --hyp hyp.scratch-high.yaml --min-items 0 --epochs 5 --close-mosaic 15&#39;

# 执行命令
process = subprocess.Popen(command, shell=True)
process.wait()

if __name__ == &quot;__main__&quot;:
TRAIN_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/train&#39;)
VAL_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/val&#39;)
YAML_FILE_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/sfdV2_musa.yaml&#39;)

# 训练 YOLOv9 模型
train_yolov5(TRAIN_IMAGES_PATH, VAL_IMAGES_PATH, YAML_FILE_PATH)`

我在运行训练脚本时收到此未来错误，我正在努力解决该错误：
FutureWarning：torch.cuda.amp.autocast(args...) 已弃用。
请改用 torch.amp.autocast(&#39;cuda&#39;, args...)。使用 torch.cuda.amp.autocast(amp)
]]></description>
      <guid>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</guid>
      <pubDate>Mon, 05 Aug 2024 12:28:07 GMT</pubDate>
    </item>
    <item>
      <title>无法抑制来自 transformers/src/transformers/modeling_utils.py 的警告</title>
      <link>https://stackoverflow.com/questions/78827482/cant-suppress-warning-from-transformers-src-transformers-modeling-utils-py</link>
      <description><![CDATA[我对 AutoModel AutoTokenizer 类的实现相当简单：
from transformers import AutoModel, AutoTokenizer
import numpy as np
from rank_bm25 import BM25Okapi
from sklearn.neighbors import NearestNeighbors

class EmbeddingModels:

def bert(self, model_name, text):
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
input = tokenizer(text, return_tensors=&quot;pt&quot;, truncation=True, padding=True)
output = model(**inputs)
embeddings = output.last_hidden_​​state.mean(dim=1).detach().numpy()
return embeddings

def create_chunks(self, text, chunk_size):
return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

但我无法让这个警告消失：
包含“beta”的参数名称将在内部重命名为“bias”。
请使用其他名称来抑制此警告。
包含“gamma”的参数名称将在内部重命名为“weight”。
请使用其他名称来抑制此警告。

我的存储库中没有任何地方提到 beta 或 gamma 这个词。
更新软件包，使用 import warnings 抑制警告]]></description>
      <guid>https://stackoverflow.com/questions/78827482/cant-suppress-warning-from-transformers-src-transformers-modeling-utils-py</guid>
      <pubDate>Fri, 02 Aug 2024 23:04:25 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TensorFlow Pipeline 中对大型数据集应用图像增强？</title>
      <link>https://stackoverflow.com/questions/78816835/how-to-apply-image-augmentations-in-tensorflow-pipeline-for-large-dataset</link>
      <description><![CDATA[我有一个图像数据集，每个图像包含一个 1 到 5 个字母的单词。我想使用深度学习对每个图像中组成单词的字符进行分类。这些图像的标签格式如下：
totalcharacter_indexoffirstchar_indexofsecondchar_.._indexoflastchar
我正尝试将这些图像加载到 TensorFlow 管道中，以降低由于内存限制而导致的复杂性。下面是我从目录加载和处理图像和标签的代码：
def process_img(file_path):
label = get_label(file_path)
image = tf.io.read_file(file_path)
image = tf.image.decode_png(image, channels=1) 
image = tf.image.convert_image_dtype(image, tf.float32) 
target_shape = [695, 1204]
image = tf.image.resize_with_crop_or_pad(image, target_shape[0], target_shape[1])

# 对标签进行编码
coded_label = tf.py_function(func=encode_label, inp=[label], Tout=tf.float32)
coded_label.set_shape([5, len(urdu_alphabets)])

return image,coded_label
input_dir = &#39;/kaggle/input/dataset/Data/*&#39;
images_ds = tf.data.Dataset.list_files(input_dir, shuffle=True)

train_count = int(tf.math.round(len(images_ds) * 0.8))
train_ds = images_ds.take(train_count)
test_ds = images_ds.skip(train_count)
train_ds = train_ds.map(process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_ds = test_ds.map(process_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_ds = test_ds.batch(32)
train_ds = train_ds.cache()
test_ds = test_ds.cache()
train_ds = train_ds.shuffle(len(train_ds))
test_ds = test_ds.prefetch(tf.data.AUTOTUNE)
print(train_ds)
print(test_ds)

train_ds 如下所示：
&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 695, 1204, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5, 39), dtype=tf.float32, name=None))&gt;
现在，我想对图像应用简单的增强，例如旋转、剪切、侵蚀和扩张。我最初使用了以下函数：
def augment(image, label):
image = tf.image.random_flip_left_right(image)
image = tf.image.random_flip_up_down(image)
image = tf.keras.preprocessing.image.random_rotation(image, rg=15, row_axis=0, col_axis=1, channel_axis=2, fill_mode=&#39;nearest&#39;, cval=0.0, interpolation_order=1)
image = tf.image.random_zoom(image, [0.85, 0.85])
image = tf.image.random_shear(image, 0.3)
image = tf.image.random_shift(image, 0.1, 0.1)
return image, label

train_augmented_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
train_augmented_ds = train_augmented_ds.prefetch(buffer_size=tf.data.AUTOTUNE)

但是，tf.image 中的许多函数都已弃用。如何以高效的方式在 TensorFlow 管道中将这些增强应用于图像？
注意：我可以通过不使用 TensorFlow 管道使用 NumPy 数组加载图像来执行这些增强，但我的数据集非常大（110 万张图像），因此我需要一种高效的方法来执行此操作。]]></description>
      <guid>https://stackoverflow.com/questions/78816835/how-to-apply-image-augmentations-in-tensorflow-pipeline-for-large-dataset</guid>
      <pubDate>Wed, 31 Jul 2024 14:11:01 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中的神经元应该是异步的吗？</title>
      <link>https://stackoverflow.com/questions/38250558/should-the-neurons-in-a-neural-network-be-asynchronous</link>
      <description><![CDATA[我正在设计一个神经网络，并试图确定我是否应该以这样一种方式编写它，即每个神经元都是 Erlang 中的自己的“进程”，或者我是否应该只使用 C++ 并在一个线程中运行一个网络（我仍然会通过在每个网络自己的线程中运行一个实例来使用我的所有核心）。
是否有充分的理由放弃 C++ 的速度而选择 Erlang 提供的异步神经元？]]></description>
      <guid>https://stackoverflow.com/questions/38250558/should-the-neurons-in-a-neural-network-be-asynchronous</guid>
      <pubDate>Thu, 07 Jul 2016 16:17:43 GMT</pubDate>
    </item>
    </channel>
</rss>