<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 25 May 2024 12:25:36 GMT</lastBuildDate>
    <item>
      <title>强化学习在完成数据分类后给予奖励，而不是一一进行，基于CNN的强化学习</title>
      <link>https://stackoverflow.com/questions/78532315/reinforcement-learning-give-reward-after-finishing-the-data-classification-inste</link>
      <description><![CDATA[我正在尝试编写一个基于强化的交易系统，在尝试这样做时，我能做的唯一方法是奖励每个动作的模型，但它实际上执行了糟糕的结果，并且无法添加我想要的所有参数。 F.E 我想添加这些参数，但为了“一一执行”模型中，我无法添加这些参数作为奖励，因为所有这些参数都可以在所有长空持有分类完成后返回
&lt;前&gt;&lt;代码&gt;
如果 roi_percent &lt; 30：
    f = 11.8
elif roi_percent &lt;&lt; 70：
    f = 9.5
elif roi_percent &lt;&lt; 140：
    f = 7.6
elif roi_percent &lt;&lt; 250：
    f = 6.4
elif roi_percent &lt;&lt; 340：
    f = 5
elif roi_percent &lt;&lt; 600：
    f = 3.6
elif roi_percent &lt;&lt; 1000：
    f = 2.46
elif roi_percent &lt;&lt; 1600：
    f = 1.34
别的：
    f = 0.7

sayi = self.条目号

如果萨伊 == 20：
    萨伊 = 70
埃利夫·萨伊10：
    萨伊 = 30 - (10*(10-萨伊))
埃利夫·萨伊20：
    萨伊 = 70 - (4*(20-萨伊))
别的：
    萨伊 = 70 - (1.6*(萨伊 - 20))

奖励 = (roi_percent * f) + (win_loss_ratio * 45) - ((self.max_drawdown * 440) / (93 - self.max_drawdown)) + (sharpe_ratio * 64) + sayi

我们还可以通过 DQN、DDQN、PPO、A2C 模型创建强化学习模型，但我想知道我们如何使用 GRU、LSTM、CNN 等模型来做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/78532315/reinforcement-learning-give-reward-after-finishing-the-data-classification-inste</guid>
      <pubDate>Sat, 25 May 2024 11:31:19 GMT</pubDate>
    </item>
    <item>
      <title>在OpenAI Gym中，MuJoCo Fetch版本1和版本2的结构是否相同？</title>
      <link>https://stackoverflow.com/questions/78531991/in-openai-gym-is-the-mujoco-fetch-version-1-is-the-same-structure-as-version-2</link>
      <description><![CDATA[我目前正在使用 OpenAI Gym MuJoCo Fetch 环境。由于 MuJoCo 已经开源，Fetch 环境的第 2 版很好地支持了这一点。但网上仍有许多实现使用 Fetch 的第 1 版（需要手动安装 MuJoCo，而不是从 pip 安装）。我只是想确定第 1 版和第 2 版之间的区别。除了 MuJoCo 安装之外，环境本身是否有任何修改？非常感谢您的帮助。我将不胜感激您的回复。
在重新实现过程中，安装旧版本的 MuJoCo 对我来说很复杂且困难。所以我只是用 -v2s 替换了 Fetch-v1s，我不确定它是否正确，因为它一直失败。问题也可能是由于 gym 版本而发生的，但 MuJoCo 版本也可能影响我的重新实现过程。]]></description>
      <guid>https://stackoverflow.com/questions/78531991/in-openai-gym-is-the-mujoco-fetch-version-1-is-the-same-structure-as-version-2</guid>
      <pubDate>Sat, 25 May 2024 09:22:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在缺失值的情况下训练模型并使用预测函数</title>
      <link>https://stackoverflow.com/questions/78531918/how-to-train-a-model-and-use-predict-function-while-having-missing-values</link>
      <description><![CDATA[我正在开发一个 ML 项目，我正在尝试训练一些分类模型，然后对测试 df 进行一些预测。
如何训练一个能够使用每个可用观察值的模型，无论它是否有缺失值？我如何做出预测？
我的 df 的大多数观察结果至少有一个缺失值。
为了训练我的模型，我使用了 caret 库。
例如，给定此模型：
control &lt;- trainControl(method=“cv”，number=10，search=“grid”，summaryFunction = TwoClassSummary，classProbs = TRUE)
unegrid &lt;- Expand.grid(.mtry=c(1:6))
rf &lt;- train(Target~.，data=train，method=“rf”，metric=“ROC”，tuneGrid=tunegrid，ntree=100，trControl=control)

然后我这样做出预测：
test$pred&lt;-predict(rf,test,&#39;prob&#39;)[,2]

在训练时，我已经尝试过这个 na.action 选项：

na.omit;

-na.exclude;

na.pass。

前两个工作正常，但如果我使用 na.pass 我会收到此错误：
出了点问题；所有 ROC 指标值均缺失：
      ROC Sens 规格    
 分钟。   ：NA 最小值。   ：NA 最小值。   : 不适用  
 第一季度：不适用 第一季度：不适用 第一季度：不适用  
 中位数 : NA 中位数 : NA 中位数 : NA  
 平均值：NaN 平均值：NaN 平均值：NaN  
 第三季度：不适用 第三季度：不适用 第三季度：不适用  
 最大限度。   ：不适用 最大。   ：不适用 最大。   : 不适用  
 不适用 :6 不适用 :6 不适用 :6    
错误：停止
警告()
1：Fold01 的模型拟合失败：mtry=1 randomForest.default(x, y, mtry = param$mtry, ...) 中的错误： 
  预测变量中不允许使用 NA

如果我使用前两个之一，当我进行预测时，我会得到与此类似的错误：
set(x, j = name, value = value) 中的错误： 
  提供了 199 个项目，分配给 5425 个项目

]]></description>
      <guid>https://stackoverflow.com/questions/78531918/how-to-train-a-model-and-use-predict-function-while-having-missing-values</guid>
      <pubDate>Sat, 25 May 2024 08:52:03 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们不能只使用Keys来计算self-attention？</title>
      <link>https://stackoverflow.com/questions/78531893/why-cant-we-use-only-keys-to-calculate-self-attention</link>
      <description><![CDATA[我读过关于自注意力机制的文章，论文建议计算 3 项内容：键、查询和值。据我所知，之所以有值，是为了根据上下文调整初始嵌入（位置编码之后）（这是直观的）。但是，我不明白为什么我们需要查询，为什么我们不能只使用键来计算相似度？提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78531893/why-cant-we-use-only-keys-to-calculate-self-attention</guid>
      <pubDate>Sat, 25 May 2024 08:42:13 GMT</pubDate>
    </item>
    <item>
      <title>通过 python 脚本使用模型</title>
      <link>https://stackoverflow.com/questions/78531849/using-a-model-via-python-script</link>
      <description><![CDATA[我可能听不懂复杂的建议和答案，但这是我的大学，我没有时间学习基础知识。
我正在尝试使用我通过 GTZAN 数据集创建的模型 - 音乐流派分类 (https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification)
模型具有很高的准确性，但我没有得到令人满意的输出。我不知道需要多少信息，但我觉得我在使用的脚本中犯了一个错误。这是脚本。
导入tensorflow为tf
导入库
将 numpy 导入为 np

# 加载预训练模型
模型 = tf.keras.models.load_model(&#39;C:/Users/VOLKAN/Desktop/SonProject/model.keras&#39;)

# 定义流派（假设您有模型预测的固定流派列表）
types = [&#39;Blues&#39;, &#39;Classical&#39;, &#39;Country&#39;, &#39;Disco&#39;, &#39;Hip-hop&#39;, &#39;Jazz&#39;, &#39;Metal&#39;, &#39;Pop&#39;, &#39;Reggae&#39;, &#39;Rock&#39;] # 替换为实际流派名字

def extract_features(文件路径):
    y，sr = librosa.load（文件路径，持续时间= 30）
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=58)
    mfccs = np.mean(mfccs.T, 轴=0)
    特征 = mfccs[np.newaxis, ...]
    返回特征



def Predict_genre(文件路径):
    特征 = extract_features(文件路径)
    预测 = model.predict(features)
    Genre_index = np.argmax(预测，轴=1)[0]
    返回类型[genre_index]

# 用法示例
audio_file = &#39;C:/Users/VOLKAN/Desktop/Data/genres_original/classical/classical.00059.wav&#39; # 替换为您的音频文件路径
预测流派 = 预测流派（音频文件）
print(f&#39;预测的类型是：{predicted_genre}&#39;)


`
在模型中，我使用了CNN。模型具有 .keras 扩展名。]]></description>
      <guid>https://stackoverflow.com/questions/78531849/using-a-model-via-python-script</guid>
      <pubDate>Sat, 25 May 2024 08:21:11 GMT</pubDate>
    </item>
    <item>
      <title>基于 Python 的模型学习，通过使用 TF、Keras 和 NLTK 进行标记化的意图</title>
      <link>https://stackoverflow.com/questions/78531788/python-based-model-learning-through-intents-using-tf-keras-and-nltk-for-tokeniz</link>
      <description><![CDATA[我使用 Tensorflow、keras 和 nltk 进行标记化，用 Python 开发了一个聊天机器人模型。当我在 vs 终端中运行它时，它会显示时间戳和模型提供答案所需的时间，但我试图在使用 React 设计的网站中显示它。如何从输出中删除日志。我已经尝试了一切，包括抑制日志，除非它们很关键，但我仍然无法删除它们。
我尝试使用这个，但它不起作用，它仍然显示它们。我知道日志不是警告，因此它们可能不会因此被删除。
导入 os os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78531788/python-based-model-learning-through-intents-using-tf-keras-and-nltk-for-tokeniz</guid>
      <pubDate>Sat, 25 May 2024 08:01:27 GMT</pubDate>
    </item>
    <item>
      <title>MODIS图像增强优化模型</title>
      <link>https://stackoverflow.com/questions/78531465/image-enhancement-optimum-model-for-modis</link>
      <description><![CDATA[我正在尝试将超分辨率模型应用于 MODIS 500m 图像，以便将其分辨率缩小到 Sentinel-2 的 60m 光谱带。
我知道这是一项非常具有挑战性的任务，因为我的数据集仅包含 20000 张图像，而且到目前为止我还没有在文献中发现类似的内容。
我尝试过实现多种架构，从简单的 CNN 到更复杂的 SRGAN，但我的结果与预期输出相去甚远。您有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78531465/image-enhancement-optimum-model-for-modis</guid>
      <pubDate>Sat, 25 May 2024 05:21:54 GMT</pubDate>
    </item>
    <item>
      <title>A3C 代理（连续动作空间）没有经过适当的训练，只能达到</title>
      <link>https://stackoverflow.com/questions/78531464/a3c-agent-continuous-action-space-not-being-trained-properly-and-only-reach</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78531464/a3c-agent-continuous-action-space-not-being-trained-properly-and-only-reach</guid>
      <pubDate>Sat, 25 May 2024 05:21:08 GMT</pubDate>
    </item>
    <item>
      <title>在 CNN 推理中跳过零乘法</title>
      <link>https://stackoverflow.com/questions/78531437/skipping-zero-multiplications-in-cnn-inference</link>
      <description><![CDATA[我在 MNIST 上有一个预训练的 CNN 模型，每次都会加载经过训练的权重和偏差来运行推理。有什么方法可以仅在推理阶段跳过 conv 和 fc 层中的零操作（我不想重新训练它，因此它不需要反向传播）？
由于 MNIST 图像很稀疏，因此我预计跳过零操作时的执行时间会少得多。工作的最优性对我来说并不是那么重要，我只是想看看输入的不同零率下执行时间有多少差异。
我尝试了一些用于 sprase 卷积的存储库，但他们正在考虑您在之后重新训练模型。我期望在 Pytorch 代码中找到一个简单的更改，只跳过零操作。还尝试找到一种方法来更改 Pytorch 的 C++ 代码库，但我无法弄清楚。]]></description>
      <guid>https://stackoverflow.com/questions/78531437/skipping-zero-multiplications-in-cnn-inference</guid>
      <pubDate>Sat, 25 May 2024 05:06:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 xgboost 推断 csv 数据时出现问题</title>
      <link>https://stackoverflow.com/questions/78531267/there-was-a-problem-infering-csv-data-with-xgboost</link>
      <description><![CDATA[我在使用 xgboost 进行推断时遇到问题：
简单来说，我想使用 xgboost 执行回归任务，由多个 csv 数据集组成。我将它们拼接成一个数据帧，并使用 train_test_split 分割训练/验证/测试。该模型运行良好（mae：0.6）。但是当我手动拆分训练集和测试集（我挑选了一部分 csv 并将其放入测试文件夹中）时，结果变得非常差（mae：12+）。
我真的很想知道这里发生了什么？我已经发布了下面的一些代码。
1：这是带有train_test_split的分割代码：
# 准备好数据
数据集 = []
路径=&#39;../data/low_fidelity_chips_res&#39;
对于 os.listdir(path) 中的文件名：
    if filename.endswith(“.csv”)：
        数据集 = ThermalDataset(os.path.join(路径，文件名))
        数据集.append(数据集)

# 合并数据集
[merged_dataset = pd.concat([pd.DataFrame(dataset.X) 用于数据集中的数据集])
merged_targets = pd.concat([数据集中的数据集的pd.DataFrame(dataset.y)])
X_scaled = scaler.fit_transform(merged_dataset)
y_scaled = 缩放器.fit_transform(merged_targets)

# 除法
X_train，X_test，y_train，y_test = train_test_split（X_scaled，y_scaled，test_size = 0.2，random_state = 11）
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=11)

# 构建xgboost模型
模型= xgb.XGBRegressor（tree_method =&#39;gpu_hist&#39;，gpu_id = device.index，n_estimators = 500，learning_rate = 0.05，max_depth = 8）
model.fit(X_train, y_train)

＃ 评估
y_val_pred = scaler.inverse_transform(y_val_pred_scaled.reshape(-1, 1)).flatten()
y_test_pred = 缩放器.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()
y_val_original = scaler.inverse_transform(y_val.reshape(-1, 1)).flatten()
y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
val_mse =mean_squared_error(y_val_original, y_val_pred)
test_mse =mean_squared_error(y_test_original, y_test_pred)
val_mae = Mean_absolute_error(y_val_original, y_val_pred)
test_mae = Mean_absolute_error(y_test_original, y_test_pred)]

2：这是我在代码后的手动划分：
# 训练数据集
数据集 = []
路径=&#39;../data/low_fidelity_chips_res&#39;
对于 os.listdir(path) 中的文件名：
    if filename.endswith(“.csv”)：
        数据集 = ThermalDataset(os.path.join(路径，文件名))
        数据集.append(数据集)

# 测试数据，这是我从原始数据中手动分区的测试集的 csv
测试=[]
test_file = &#39;../data/test_xgboost/Thermal014withMidPos.csv&#39;
测试 = ThermalDataset(test_file)
测试.追加（测试）

＃ 结合
merged_dataset = pd.concat([数据集中的数据集的pd.DataFrame(dataset.X)])
merged_targets = pd.concat([数据集中的数据集的pd.DataFrame(dataset.y)])
test_x = pd.concat([pd.DataFrame(test.X) 用于测试中的测试])
test_y = pd.concat([pd.DataFrame(test.y) 用于测试中的测试])

# 标准化
X_scaled = scaler.fit_transform(merged_dataset)
y_scaled = 缩放器.fit_transform(merged_targets)
x_fill = 缩放器.fit_transform(test_x)
y_fill = 缩放器.fit_transform(test_y)

＃ 分裂
X_train，X_test，y_train，y_test = train_test_split（X_scaled，y_scaled，test_size = 0.05，random_state = 11）
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=11)

＃ 火车
模型= xgb.XGBRegressor（tree_method =&#39;gpu_hist&#39;，gpu_id = device.index，n_estimators = 500，learning_rate = 0.05，max_depth = 8）
model.fit(X_train, y_train)

＃ 评估
y_val_pred_scaled = model.predict(X_val)
y_test_pred_scaled = model.predict(X_test)
y_fill_res = model.predict(x_fill)

# inverse_transform 获取原始数据
y_val_pred = scaler.inverse_transform(y_val_pred_scaled.reshape(-1, 1)).flatten()
y_test_pred = 缩放器.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()
y_pre = scaler.inverse_transform(y_fill_res.reshape(-1, 1)).flatten()
y_val_original = scaler.inverse_transform(y_val.reshape(-1, 1)).flatten()
y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
y_fill = scaler.inverse_transform(y_fill.reshape(-1, 1)).flatten()
val_mse =mean_squared_error(y_val_original, y_val_pred)
test_mse =mean_squared_error(y_test_original, y_test_pred)
val_mae = Mean_absolute_error(y_val_original, y_val_pred)
test_mae = Mean_absolute_error(y_pre, y_fill)`

我希望能够对单个 csv 文件进行正确推理，并获得与训练中一样好的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78531267/there-was-a-problem-infering-csv-data-with-xgboost</guid>
      <pubDate>Sat, 25 May 2024 02:40:08 GMT</pubDate>
    </item>
    <item>
      <title>langchain RetrievalQA 错误：ValueError：缺少一些输入键：{'query'}</title>
      <link>https://stackoverflow.com/questions/78530745/langchain-retrievalqa-error-valueerror-missing-some-input-keys-query</link>
      <description><![CDATA[在RAG项目中，我使用langchain，当我使用查询输入运行qa链时，这个错误不断出现！
这是错误：
----&gt;结果 = qa_chain({&#39;查询&#39;: 问题})
ValueError：缺少一些输入键：{&#39;query&#39;}
这是我的代码：
from langchain.chains import RetrievalQA
从 langchain.prompts 导入 PromptTemplate

# 构建提示
template = &quot;&quot;&quot; 根据以下上下文回答问题。
    语境：
    {语境}
    ------------------
    问题：{查询}
    答案：“”

# 法学硕士链
QA_CHAIN_PROMPT = PromptTemplate.from_template(模板)
qa_chain = RetrievalQA.from_chain_type(
    嗯，
    检索器=vectordb.as_retriever(),
    return_source_documents=真，
    chain_type_kwargs={“提示”: QA_CHAIN_PROMPT}
）

Question =“这篇研究论文使用了什么方法？”

结果 = qa_chain({&#39;查询&#39;: 问题})

# 查看查询结果
结果[“结果”]
# 检查我们所在的源文档 
结果[“源文档”][0]

]]></description>
      <guid>https://stackoverflow.com/questions/78530745/langchain-retrievalqa-error-valueerror-missing-some-input-keys-query</guid>
      <pubDate>Fri, 24 May 2024 21:23:49 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“sklearn.utils”导入名称“_get_column_indices”</title>
      <link>https://stackoverflow.com/questions/78524575/importerror-cannot-import-name-get-column-indices-from-sklearn-utils</link>
      <description><![CDATA[尝试为 RandomOverSampler 导入 imblearn.over_sampling 时出现导入错误。我相信问题不在于我的代码，而在于库冲突，但我不确定。
导入 pandas 作为 pd
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np
从 sklearn.preprocessing 导入 StandardScaler #actually scikit-learn
从 imblearn.over_sampling 导入 RandomOverSampler

使用 StandardScaler 和 RandomOverSampler 的代码：
def scale_dataset(dataframe, oversample=False):
    X = dataframe[dataframe.columns[:-1]].values
    Y = dataframe[dataframe.columns[-1]].values

    定标器=标准定标器() 
    X = 缩放器.fit_transform(X) 

    如果过采样：
        ros = RandomOverSampler()
        X, Y = ros.fit_resample(X,Y) 
    数据 = np.hstack((X, np.reshape(Y, (-1, 1))))
    返回数据，X，Y

print(len(train[train[“班级”]==1]))
print(len(train[train[“班级”]==0]))

训练，X_train，Y_train =scale_dataset（训练，True）

我尝试完全导入sklearn，卸载并重新安装scipi和sklearn（作为scikit-learn），安装Tensorflow。
我确实安装了 numpy、scipy、pandas 和其他依赖库。]]></description>
      <guid>https://stackoverflow.com/questions/78524575/importerror-cannot-import-name-get-column-indices-from-sklearn-utils</guid>
      <pubDate>Thu, 23 May 2024 16:54:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 SHAP 解释学习到的潜在空间位置</title>
      <link>https://stackoverflow.com/questions/78517488/using-shap-to-explain-learned-latent-space-position</link>
      <description><![CDATA[我在 MNIST 数据集上的 pytorch 中实现了一个监督自动编码器。
我在潜在空间（大小 8）上使用分类层对其进行监督。在训练期间，我优化了 MSE 重建损失和分类损失 (BCE)。我在潜在空间中有单个实例，这些实例很有趣，我想找到它们不同位置的解释。
所以我的问题是，在潜在维度上使用 SHAP 值是否是一种有效的方法（它有效，我得到了值，但我不确定这是否有意义）。
更具体地说：我想比较例如实例 A 和实例 B。假设在潜在空间中它们相距很远，例如在潜在维度 3 of 8 中。现在我想找到输入中可以解释这种现象的像素。因此，我计算实例 A 和 B 的潜在表示的 SHAP 值，并比较两者的维度 3 的 SHAP 值。这是有效的吗？我认为它与解释多输出回归没有太大不同，对吧？但我还没有看到任何 SHAP 的应用来解释潜在位置。]]></description>
      <guid>https://stackoverflow.com/questions/78517488/using-shap-to-explain-learned-latent-space-position</guid>
      <pubDate>Wed, 22 May 2024 12:17:33 GMT</pubDate>
    </item>
    <item>
      <title>每个时期 Retinanet 模型内的数据流</title>
      <link>https://stackoverflow.com/questions/78516393/flow-of-data-inside-the-retinanet-model-in-each-epoch</link>
      <description><![CDATA[通过提供batch_size、epochs和每个epoch的步骤，向retinanet model_network提供了多少数据？
到目前为止，我认为步长的计算方式如下：
step_size = (total_number_of_data/batch_size)*epochs

而在keras-retinanet中，它以batch_size、epochs和steps_per_epoch作为参数，这与上述情况不同。我怀疑计算是如何进行的。]]></description>
      <guid>https://stackoverflow.com/questions/78516393/flow-of-data-inside-the-retinanet-model-in-each-epoch</guid>
      <pubDate>Wed, 22 May 2024 09:04:14 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的分段似乎没有保存？关于totalsegmentator</title>
      <link>https://stackoverflow.com/questions/78516029/why-my-segmentations-dont-seem-to-be-saved-about-totalsegmentator</link>
      <description><![CDATA[我一步一步按照你的教程操作，但得到的结果类似于分割未保存。
这是我输入的语句和得到的结果：
(d:\totalsegmentotar.conda) D:\totalsegmentotar&gt;TotalSegmentator -i hip_left.nii.gz -o fragmentations -ta hip_implant

如果您使用此工具，请引用：https://pubs.rsna.org/doi/10.1148/ryai.230024

未检测到 GPU。在 CPU 上运行。这可能非常慢。&#39;--fast&#39; 或 --roi_subset 选项可以帮助减少运行时间。
生成粗略的身体分割...
重新采样...
在 1.93 秒内重新采样
预测...
d:\totalsegmentotar.conda\Lib\site-packages\nnunetv2\utilities\plans_handling\plans_handler.py:37: UserWarning：检测到旧的 nnU-Net 计划格式。尝试重建网络架构参数。如果失败，请为您的数据集重新运行 nnUNetv2_plan_experiment。如果您使用自定义架构，请将 nnU-Net 降级到您实施的版本或更新您的实施 + 计划。
warnings.warn(&quot;检测到旧的 nnU-Net 计划格式。尝试重建网络架构&quot;
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 1.12it/s]
预计 12.95 秒
重新采样...
警告：无法裁剪，因为未检测到前景
从 (333, 333, 539) 裁剪到 (333, 333, 539)
预测...
d:\totalsegmentotar.conda\Lib\site-packages\nnunetv2\utilities\plans_handling\plans_handler.py:37: UserWarning：检测到旧的 nnU-Net 计划格式。尝试重建网络架构参数。如果失败，请为您的数据集重新运行 nnUNetv2_plan_experiment。如果您使用自定义架构，请将 nnU-Net 降级到您实施的版本或更新您的实施 + 计划。
warnings.warn(&quot;检测到旧的 nnU-Net 计划格式。尝试重建网络架构&quot;
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [04:27&lt;00:00, 4.18s/it]
预测时间为 288.96s
正在保存分段...
0%| | 0/1 [00:00&lt;?, ?it/s]正在创建 hip_implant.nii.gz
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04&lt;00:00, 4.61s/it]
保存于 6.80s

可以看到分割没有保存，而且我用过切片软件看确实没有预测结果，什么都没有显示。
当我使用 `-ta total 时，分割器进度条发生了变化，但遗憾的是它似乎没有保存分割的结果。这是我的输出，以及在切片器 5.6.2 中打开的输出文件夹和图像，什么都没有显示出来。
这是我的 powershell 输出
这是我的输出文件夹和在切片器 5.6.2 中打开的图像]]></description>
      <guid>https://stackoverflow.com/questions/78516029/why-my-segmentations-dont-seem-to-be-saved-about-totalsegmentator</guid>
      <pubDate>Wed, 22 May 2024 07:52:18 GMT</pubDate>
    </item>
    </channel>
</rss>