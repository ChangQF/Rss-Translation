<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 17 Jan 2024 21:13:03 GMT</lastBuildDate>
    <item>
      <title>如何改进时间序列的建模？</title>
      <link>https://stackoverflow.com/questions/77835401/how-to-improve-the-modeling-of-a-time-series</link>
      <description><![CDATA[我正在对各种机器学习模型进行建模来预测可可价格。我使用的数据集是 2018 年至 2024 年的时间序列以及这些价格。然而，到 2023 年底，出现了非常夸张的增长，我的模型无法跟上。我希望有人帮助我提出如何改进拟合和预测的想法。
我首先导入并预处理数据集，如下所示：
df = pd.read_csv(&quot;https://raw.githubusercontent.com/nunesisabella/Analise-Preditiv-Cacau/main/ICCO_2018-2024.csv&quot;,sep=&quot;;&quot;)

df[&quot;Valor_Londres&quot;] = df[&quot;Valor_Londres&quot;].str.replace(&quot;,&quot;, &quot;&quot;).astype(float)
df[&quot;Valor_NY&quot;] = df[&quot;Valor_NY&quot;].str.replace(&quot;,&quot;, &quot;&quot;).astype(float)
df[&quot;ICCO_USD&quot;] = df[&quot;ICCO_USD&quot;].str.replace(&quot;,&quot;, &quot;&quot;).astype(float)
df[&quot;ICCO_EUR&quot;] = df[&quot;ICCO_EUR&quot;].str.replace(&quot;,&quot;, &quot;&quot;).astype(float)

df[&#39;Data&#39;] = pd.to_datetime(df[&#39;Data&#39;], format=&#39;%d.%m.%Y&#39;)

df.set_index(&#39;数据&#39;, inplace=True)

df = df.sort_values(&#39;数据&#39;)

然后，我分离数据集：
train = df.loc[df.index &lt;; &#39;01-01-2023&#39;]
测试 = df.loc[df.index &gt;= &#39;01-01-2023&#39;]

创建新功能：
def create_feature(df):
    ”“”
    Cria 作为 variaveis independetes da serie baseadas 无索引时间
    ”“”
    df[&#39;Dia&#39;] = df.index.day
    df[“Dia_Semana”] = df.index.dayofweek
    df[&#39;Mês&#39;] = df.index.month
    df[&#39;Ano&#39;] = df.index.year
    返回df

火车 = 创建特征（火车）
测试=创建_功能（测试）

并获取分离后的数据：
feature = [&#39;Valor_Londres&#39;, &#39;Valor_NY&#39;, &#39;ICCO_EUR&#39;, &#39;Dia&#39;, &#39;Dia_Semana&#39;, &#39;Mês&#39;, &#39;Ano&#39;]
目标=&#39;ICCO_USD&#39;

X_train = 训练[特征]
y_train = 训练[目标]

X_test = 测试[特征]
y_test = 测试[目标]

使用 XGBoost 模型：
model_XGB = xgb.XGBRegressor(n_estimators=1000)

# 模型制作
model_XGB.fit(X_train, y_train,
          eval_set=[(X_train, y_train), (X_test, y_test)],
          详细=100)

我得到了下图，其中红线显示了预测问题。我可以使用什么方法来改善这一点？
]]></description>
      <guid>https://stackoverflow.com/questions/77835401/how-to-improve-the-modeling-of-a-time-series</guid>
      <pubDate>Wed, 17 Jan 2024 20:40:26 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 前向传播速度取决于变量</title>
      <link>https://stackoverflow.com/questions/77835175/pytorch-forward-pass-speed-depending-on-variables</link>
      <description><![CDATA[我想问为什么前向传递通过 MLP 模型的时间取决于输入数据的大小（在采样和创建较小批次之前）。
例如，如果我从大小为 100（整个数据大小）的原始数据集中选择 10（批量大小）数据，则前向传递所需的时间不应取决于 100，而是取决于 10。
我发现，当我增加批次大小时，前向传递时间也会增加。但是，当我增加总数据集的数量时，也会发生同样的情况。
我有一个模型，可以训练一定批量大小的输入（在本例中为 100），这些输入是从连续整数数组中采样的，其形式为，
# 生成输入
时间0 = 时间.time()

input_data = np.zeros([n,1]).astype(np.float64)

对于范围内的 i(len(input_array))：
    输入数据[i, 0] = -1.5 + 3 / (n-1) * i

时间1 = 时间.time()



＃ 火车
批量大小 = 100

对于范围（1000）内的train_step：
   indexTotal = [num for num in range(n)]
   batchIndices = random.sample(indexTotal, batchSize)
   input_data_0 = input_data[batchIndices, :] # 样本数据

   input_data_combined = np.concatenate([input_data_0] * 3, axis=0) # 合并数据
   input_data_combined = input_data_combined + shiftMat # 添加偏差
   input_data_combined = torch.tensor(input_data_combined, require_grad=True, device=device) # 转换为张量
   时间2 = 时间.时间()

   output_combined = model(input_data_combined) # 前向传递
   time3 = time.time()
   
   // 反向传播
   

基本上，它将多个 numpy 数组组合成一个数组，添加一个偏差（也是一个 numpy 数组），然后转发到模型。该模型是一个简单的 MLP 模型，其条件是如果接收到的张量在特定边界内，则返回零。
# 定义神经网络
类 myModel(nn.Module):
    def __init__(自身):
        超级（myModel，自我）.__init__()
        self.fc1 = nn.Linear(1, 31) # (x, y) 输入
        self.fc2 = nn.Linear(31, 31)
        self.fc3 = nn.Linear(31, 1)

    def 前向（自身，x）：
        # 检查每一行是否在 epsilon (=0.01) 内接近 [1.1, 0]
        ε = 0.01
        isClose2Target = torch.all(torch.abs(x - torch.tensor([1.1, 0], device=device)) &lt; epsilon, dim=1)
        
        x = self.fc1(x)
        x = 火炬.tanh(x)
        x = self.fc2(x)
        x = 火炬.tanh(x)
        x = self.fc3(x)

        # 对于接近 [1.1, 0] 的行，将输出设置为 0
        x[isClose2Target] = torch.tensor([0])

        return x # 单个 Eb 输出

如果增加batchSize，前向传递所需的时间就会增加。当前的问题是，如果我增加 n （完整数据集的大小），前向传递时间也会增加，这是没有意义的（因为模型接收到 batchSize 的输入大小，不是n）。
我认为这可能是 random.sample() 函数的问题，它可能存储有关 n 的信息，但我不确定......
您能否建议哪部分代码可能会导致此问题？
谢谢。
我在 GPU 和 CPU 上都进行了尝试，但它们都给了我相同的结果，其中前向传递速度取决于 n 和 batchSize。
我目前正在使用时间模块 time.time() 测量时间，如上面代码中所写。]]></description>
      <guid>https://stackoverflow.com/questions/77835175/pytorch-forward-pass-speed-depending-on-variables</guid>
      <pubDate>Wed, 17 Jan 2024 19:56:39 GMT</pubDate>
    </item>
    <item>
      <title>测试数据集的最佳数量是多少</title>
      <link>https://stackoverflow.com/questions/77834854/what-is-the-optimal-number-of-testing-dataset</link>
      <description><![CDATA[我目前从事胶质瘤相关研究。尽管采用了 SMOTE、RUS 或 ROS 等各种技术，但我的机器学习模型的结果似乎偏向于多数类别。鉴于模型是在不平衡数据集上训练的，这个问题可能是预料之中的。但是，我不确定问题是否出在所使用的技术上，或者是否是由于测试数据集不足造成的。
分割我的训练集和测试/验证集是否是一个可行的解决方案，特别是考虑到我的数据集的大小有限？作为上下文，我的数据由 19 名患者的 MRI 图像组成，每个患者 15 张图像。我使用 15 张图像进行训练，使用 4 张图像进行测试。
非常感谢，]]></description>
      <guid>https://stackoverflow.com/questions/77834854/what-is-the-optimal-number-of-testing-dataset</guid>
      <pubDate>Wed, 17 Jan 2024 18:53:39 GMT</pubDate>
    </item>
    <item>
      <title>在多类分类上训练模型时出现 XGboost 错误“SoftmaxMultiClassObj：标签必须位于 [0, num_class)”</title>
      <link>https://stackoverflow.com/questions/77834714/xgboost-error-while-training-a-model-on-multi-class-classification-softmaxmulti</link>
      <description><![CDATA[我正在使用 dask XGboost 库 (xgboost.dask.DaskXGBClassifier) 在具有多个类的数据上训练模型。这些类如下：{1,2,3,4,5,6,21,25}。在使用 multi:softprob 目标拟合分类器时，我收到以下错误 SoftmaxMultiClassObj：标签必须位于 [0, num_class)。有趣的是，我之前也用 xgboost 分类器训练过模型，但没有使用 dask (xgboost.XGBClassifier)，但没有出现这个问题。可能是什么原因？ [我使用的是xgboost==1.5.0版本]
我使用编码将标签保持在 [0, num_class) 内，但我有巨大的数据集，我认为将数据加载到内存中并对标签进行编码将导致我想避免的内存问题，这是唯一的原因我正在与 dask 库对齐。
如何解决这个问题？
以下是示例代码
def main(客户端):
    将 dask.dataframe 导入为 dd
    ddf = dd.read_csv(df_path)

    # 删除非数值数据、NaN、Inf 和位置特定数据
    数据= ddf.drop（discarded_columns，轴= 1，错误=&#39;忽略&#39;）
    
    数据 = data.replace([np.inf, -np.inf], np.nan).dropna()

    train_x = data.drop([&#39;预测&#39;], axis=1)
    train_y = 数据[&#39;预测&#39;]

    从 dask_ml.model_selection 导入 train_test_split
    (train_x, test_x, train_y, test_y) = train_test_split(
        train_x，train_y，test_size=TEST_SIZE，
        random_state=RANDOM_STATE
    ）

    尝试：
        将 xgboost 导入为 xgb
        print(&quot;创建分类器：&quot;)
        
        分类器= xgb.dask.DaskXGBClassifier（客户端，random_state=RANDOM_STATE，n_jobs=-1，详细程度=1）

        # 预定义的调整超参数
        调整参数 = {
            &#39;colsample_bytree&#39;：0.8，&#39;eval_metric&#39;：&#39;mlogloss&#39;，&#39;伽玛&#39;：0，
            “学习率”：0.15，“最大深度”：8，“最小儿童权重”：1，
            &#39;n_estimators&#39;：800，&#39;目标&#39;：&#39;multi：softprob&#39;，&#39;tree_method&#39;：&#39;hist&#39;}
        print(&quot;设置参数：&quot;)
        classifier.set_params(**tuned_pa​​rams)
        分类器.client = 客户端
        print(“拟合模型：”)
        classifier.fit(train_x, train_y, eval_set=[(train_x, train_y)])
        bst = classifier.get_booster()
        历史 = classifier.evals_result()

        print(&quot;评估历史记录：&quot;, History)

    除了异常 e：
        打印(e)


如果 __name__ == “__main__”：
    从 dask.distributed 导入客户端、LocalCluster
    以 LocalCluster() 作为集群：
        以客户端（集群）作为客户端：
            主要（客户端）
]]></description>
      <guid>https://stackoverflow.com/questions/77834714/xgboost-error-while-training-a-model-on-multi-class-classification-softmaxmulti</guid>
      <pubDate>Wed, 17 Jan 2024 18:25:07 GMT</pubDate>
    </item>
    <item>
      <title>人工智能日历的算法建议利用机器学习和自动化来优化日历，就像 trevorai 和 reclaim 所做的那样？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77834687/algorithm-suggestions-for-an-ai-calendar-utilizing-machine-learning-and-automati</link>
      <description><![CDATA[我们计划创建一个关于个性化人工智能调度程序/日历应用程序的有趣的业余项目，该应用程序利用机器学习和自动化来优化调度。想想 reclaim.ai、motion、顺时针、trevorai 等，但都是在移动设备上由学生制作的，哈哈。您能否建议我们可以在项目中使用的算法，尤其是 ML 算法，或者您是否认为除了 ML 之外还有更好的方法？
例如，我要求 AI 将我的会议从下午 4 点开始移至第二天，然后 AI 会检查日历，如果第二天下午 4 点有活动，如果有，则会将其移到第二天下午 4 点。或者，人工智能可以添加建议事件，例如为第二天即将举行的考试而学习，您可以选择接受或不接受建议。诸如此类的事情。
根据我的研究，我发现了调度算法、自动化的使用、遗传算法、优化算法，但没有一个适合我的特定应用程序。]]></description>
      <guid>https://stackoverflow.com/questions/77834687/algorithm-suggestions-for-an-ai-calendar-utilizing-machine-learning-and-automati</guid>
      <pubDate>Wed, 17 Jan 2024 18:19:48 GMT</pubDate>
    </item>
    <item>
      <title>召回分数！=使用confusion_matrix手动计算</title>
      <link>https://stackoverflow.com/questions/77834628/recall-score-manual-calculation-using-confusion-matrix</link>
      <description><![CDATA[我遇到了一个问题，即使用 recall_score(y, y_pred) 获得的召回分数与使用 confusion_matrix 手动计算的值不匹配。
不仅如此，召回率与特异性的值完全相同，我也在下面手动计算了该值。
这是我正在使用的相关代码：
recall = recall_score(y, y_pred) # &lt;-- 不同的分数

conf_matrix = fusion_matrix(y, y_pred)
tn, fp, fn, tp = conf_matrix.ravel()
Manual_recall = tp / (tp + fn) # &lt;-- 达到这个分数
特异性 = tn / (tn + fp) # &lt;-- 与上面的分数相同

这是发生这种情况的终端中打印的混淆矩阵的示例：
&lt;前&gt;&lt;代码&gt;[[34 6]
 [20 20]]

科学套件召回：0.85
手动召回：0.5
或
&lt;前&gt;&lt;代码&gt;[[29 11]
 [9 31]]

科学套件召回：0.725
手动召回：0.775
问题：
scikit-learn 返回的召回和手动召回不会产生相同的值。
问题：
为什么recall_score和使用confusion_matrix的手动计算可能会产生不同的召回分数结果？
更多信息...

这是一个二元分类问题。

我正在使用 recall_score 的默认阈值。

我尝试确定混淆表是否准确（确实如此）。

]]></description>
      <guid>https://stackoverflow.com/questions/77834628/recall-score-manual-calculation-using-confusion-matrix</guid>
      <pubDate>Wed, 17 Jan 2024 18:08:23 GMT</pubDate>
    </item>
    <item>
      <title>训练时张量流损失不会超过 100 [关闭]</title>
      <link>https://stackoverflow.com/questions/77833956/tensorflow-loss-doesnt-get-past-100-when-training</link>
      <description><![CDATA[我有一个用于股票预测人工智能的张量流代码，由于某种原因，我无法让损失低于 100。我已经尝试了各种方法，例如增加模型大小、减少、学习率调度、提前停止、密集、调整学习速率和纪元。但我还是没能突破 100 的亏损大关。有人可以告诉我我是否做错了什么并帮助我解决问题。
这是代码：
导入 csv
将 numpy 导入为 np
从张量流导入keras
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入密集，批量标准化，丢弃
从tensorflow.keras.optimizers导入Adam
从tensorflow.keras.callbacks导入ReduceLROnPlateau，EarlyStopping

# 用于训练或评估的用户输入
choice = input(&#39;火车或 eva: &#39;)

如果选择==“火车”：
    # 初始化列表来存储输入数组和输出数组
    输入数组 = []
    输出数组 = []

    # 打开 CSV 文件和格式。
    以 open(r&quot;C:\Users\eldo7\Downloads\HistoricalData_1698728285806.csv&quot;, newline=&#39;&#39;) 作为 csvfile：
        读者 = csv.DictReader(csvfile)

        收盘价 = []

        对于反转行（列表（读者））：
            close_price_str = row.get(“收盘/最后”)
            如果 close_price_str 不为 None：
                close_price = float(close_price_str.strip(&#39;$&#39;).replace(&#39;,&#39;, &#39;&#39;))
                close_prices.append(close_price)

                # 如果我们收集了 6 个值（5 个用于输入，1 个用于输出），将它们添加到各自的数组中
                如果 len(close_prices) == 6:
                    input_arrays.append(close_prices[:5])
                    output_array.append(close_prices[-1])
                    close_prices.pop(0) # 删除第一个值以保持5个连续值

    # 打乱数据同时保持对的顺序
    索引 = 列表（范围（len（input_arrays）））
    np.random.shuffle(索引)

    shuffled_input_arrays = [input_arrays[i] 对于索引中的 i]
    shuffled_output_array = [output_array[i] 对于索引中的 i]

    # 将输入和输出数据转换为NumPy数组
    shuffled_input_arrays = np.array(input_arrays, dtype=np.float32)
    shuffled_output_array = np.array(output_array, dtype=np.float32)

    # 将数据分为训练集和验证集
    分割比率 = 0.8
    split_index = int(split_ratio * len(shuffled_input_arrays))

    train_input = shuffled_input_arrays[:split_index]
    train_output = shuffled_output_array[:split_index]
    val_input = shuffled_input_arrays[split_index:]
    val_output = shuffled_output_array[split_index:]

    自定义学习率 = 0.0001

    优化器 = Adam(learning_rate=custom_learning_rate)

    模型=顺序（[
        密集（128，激活=&#39;relu&#39;，input_shape=（5，）），
        批量归一化(),
        辍学率（0.3），

        密集（64，激活=&#39;relu&#39;），
        批量归一化(),
        辍学率（0.3），

        密集（32，激活=&#39;relu&#39;），
        批量归一化(),
        辍学率（0.3），

        密集（16，激活=&#39;relu&#39;），
        批量归一化(),
        辍学率（0.3），

        密集（8，激活=&#39;relu&#39;），
        批量归一化(),
        辍学率（0.3），

        密集(1)
    ]）

    # 使用自定义优化器和 MSE 作为损失函数来编译模型
    model.compile（优化器=优化器，损失=&#39;mean_squared_error&#39;，指标=[&#39;mae&#39;]）

    模型.summary()

    # 学习率调度
    reduce_lr =ReduceLROnPlateau(监视器=&#39;val_loss&#39;,因子=0.5,耐心=100,min_lr=0.00001,min_delta=100)

    # 提前停止回调
    Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 100，restore_best_weights = True）

    历史= model.fit（train_input，train_output，epochs = 3000，batch_size = 64，validation_data =（val_input，val_output），callbacks = [reduce_lr，early_stopping]）

    model.save(“stockPredictorV2.h5”)

    # 绘制训练和验证指标
    将 matplotlib.pyplot 导入为 plt

    plt.plot(history.history[&#39;loss&#39;], label=&#39;训练损失&#39;)
    plt.plot(history.history[&#39;val_loss&#39;], label=&#39;验证损失&#39;)
    plt.图例()
    plt.show()

elif 选择==“eva”：
    模型 = keras.models.load_model(“stockPredictorV2.h5”)
    # 创建自定义输入数组
    custom_input = np.array([193.60, 193.05, 193.15, 193.58, 192.53], dtype=np.float32)

    # 使用自定义输入进行预测
    自定义输入 = 自定义输入.reshape(1, 5)
    自定义预测 = model.predict(自定义输入)

    print(&quot;自定义输入：&quot;, custom_input)
    # 打印自定义预测
    print(&quot;自定义预测：&quot;, custom_prediction)
]]></description>
      <guid>https://stackoverflow.com/questions/77833956/tensorflow-loss-doesnt-get-past-100-when-training</guid>
      <pubDate>Wed, 17 Jan 2024 16:22:15 GMT</pubDate>
    </item>
    <item>
      <title>我目前在 google colab 上运行 FAISS 并且遇到了这个问题</title>
      <link>https://stackoverflow.com/questions/77833829/i-am-currently-running-faiss-on-google-colab-and-i-encountered-this-problem</link>
      <description><![CDATA[从 langchain.vectorstores 导入 FAISS
vectorStore = FAISS.from_texts(块，嵌入)

基本上我正在尝试在我的 mac (google-colab) 上运行它。 (https://github.com/Selim321/Langchain-QnA -falcon/blob/main/Langchain_QnA_falcon.ipynb)
并且 google colab 会无限期地运行。难道我做错了什么？给定的代码有优化吗？我已经导入了faiss_cpu、langchain和sentence_transformers。]]></description>
      <guid>https://stackoverflow.com/questions/77833829/i-am-currently-running-faiss-on-google-colab-and-i-encountered-this-problem</guid>
      <pubDate>Wed, 17 Jan 2024 16:03:00 GMT</pubDate>
    </item>
    <item>
      <title>如何训练一个接受十六进制代码的模型，并且它应该输出给定数量的对比或相似的十六进制代码</title>
      <link>https://stackoverflow.com/questions/77830997/how-do-i-train-a-model-that-takes-in-a-hex-code-and-it-should-output-a-given-num</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77830997/how-do-i-train-a-model-that-takes-in-a-hex-code-and-it-should-output-a-given-num</guid>
      <pubDate>Wed, 17 Jan 2024 08:40:22 GMT</pubDate>
    </item>
    <item>
      <title>查找输入列中的值的优化组合，以生成输出列中的值[关闭]</title>
      <link>https://stackoverflow.com/questions/77830783/find-optimized-combinations-of-values-in-input-columns-that-produces-the-values</link>
      <description><![CDATA[我有多个生成输出列的输入列。我想找到输入列中的值的优化组合，以生成输出列中的值。
例如：像这个表所以输出将是这样的：
抄送-&gt;输出4
AA，XX -&gt;输出1
等等。
我正在尝试使用关联规则学习来查找多个输入列和一个输出列（所有文本列）之间的规则或关系。我尝试过 pycaret.arules 但在 pycaret==2.3.10 之后它不可用，并且此版本或以下版本与 Python 3.10 不兼容。所以，我不能使用它。
有没有其他方法可以解决这个问题。我尝试过决策树，但我想要一些完全适合我的数据而无需任何额外节点的东西。
到目前为止，我已经研究了 apyori、efficient-apriori 等不同的 apriori 实现。它们都不完全符合我想要的。]]></description>
      <guid>https://stackoverflow.com/questions/77830783/find-optimized-combinations-of-values-in-input-columns-that-produces-the-values</guid>
      <pubDate>Wed, 17 Jan 2024 08:01:36 GMT</pubDate>
    </item>
    <item>
      <title>如何仅使用组件在 azure ml Designer 中训练和部署 ml 模型？</title>
      <link>https://stackoverflow.com/questions/77827691/how-to-train-and-deploy-ml-models-in-azure-ml-designer-just-using-components</link>
      <description><![CDATA[我在 azure ml Designer 中创建了一个训练管道。现在，我需要通过添加用于注册和部署的组件来部署此模型。我想我可以使用“执行 python 脚本”组件来执行此操作。但是我不知道如何将“训练的最佳模型”（“调整模型超参数”组件的输出）与“执行 python 脚本”组件连接起来。那么，知道如何完成这项任务吗？我将非常感谢您的帮助。
这是我的管道：
训练管道]]></description>
      <guid>https://stackoverflow.com/questions/77827691/how-to-train-and-deploy-ml-models-in-azure-ml-designer-just-using-components</guid>
      <pubDate>Tue, 16 Jan 2024 17:39:49 GMT</pubDate>
    </item>
    <item>
      <title>当尝试使用tuner.search运行GridTuner类时我遇到了问题</title>
      <link>https://stackoverflow.com/questions/77821202/when-trying-to-run-gridtuner-class-using-tuner-search-%c4%b1-am-having-problem</link>
      <description><![CDATA[类 GridTuner(keras_tuner.GridSearch):
def __init__(self, 超模型, \*\*kwargs):
super().__init__(超级模型，\*\*kwargs)

    def run_Trial(自我, 审判, *args, **kwargs):
        hp = 试验.超参数
        模型 = self.hypermodel.build(hp)
        返回 self.hypermodel.fit(hp, model, *args, **kwargs)
                调谐器 = GridTuner(
                构建模型，
                目标=&#39;val_loss&#39;,
                覆盖=真，
                目录=“D:\\kaggle\\working\\hyperparameters”,
                project_name=f“driams-{有机体}-{抗菌剂}”，
）
                

                tuner.search_space_summary()



                调谐器. 搜索(
                    X_火车，
                    y_火车，
                    验证数据=（X_val，y_val），
                    批量大小=128，
                    纪元=100，
                    类别权重=类别权重，
                    回调=[提前停止(耐心=15)]
）

            best_hp =tuner.get_best_hyperparameters()[0]
            best_model =tuner.hypermodel.build(best_hp)
            best_model.summary()

我正在尝试运行有关超参数的试验，但出现以下错误：
文件“C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\streamlit\\runtime\\scriptrunner\ \script_runner.py”，第 534 行，在 \_run_script exec(code, module.__dict__) 文件“C:\\Users\\90507\\OneDrive\\Masaüstü\\demo\\app.py”，第 266 行，在\&lt;模块\&gt;; main() 文件“C:\\Users\\90507\\OneDrive\\Masaüstü\\demo\\app.py”，第 234 行，在 maintuner.search( 文件“C:\\Users\\90507”中\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py”，第 234 行，在搜索 self.on_Trial_end(Trial)文件“C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py”，第 338 行，在 on_trial_end self.oracle.end_trial(Trial) 文件 &quot;C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner \\src\\engine\\oracle.py”，第 108 行，wrapped_func ret_val = func(\*args, \*\*kwargs) ^^^^^^^^^^^^^^^^^^ ^^^ 文件“C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\tuners\\gridsearch.txt” py”，第 318 行，在 end_Trial super().end_Trial(Trial) 文件“C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages \\keras_tuner\\src\\engine\\oracle.py”，第 108 行，wrapped_func ret_val = func(\*args, \*\*kwargs) ^^^^^^^^^^^^^^^^ ^^^^^^ 文件“C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\src\\engine\ \oracle.py”，第 586 行，end_Trial self.\_check_consecutive_failures() 文件“C:\\Users\\90507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site- packages\\keras_tuner\\src\\engine\\oracle.py”，第 543 行，在 \_check_consecutive_failures 中引发 RuntimeError( ValueError: 无法将 NumPy 数组转换为张量（不支持的对象类型 NoneType）。
我尝试查看数据集内部，检查导入、库并尝试更改代码。我正在尝试这个 https://www.kaggle.com/code/ hlysine/driams-maldi-tof-classifier 代码来制作有关 ML 的 Web 应用程序，我正在使用 Streamlit。]]></description>
      <guid>https://stackoverflow.com/questions/77821202/when-trying-to-run-gridtuner-class-using-tuner-search-%c4%b1-am-having-problem</guid>
      <pubDate>Mon, 15 Jan 2024 16:43:50 GMT</pubDate>
    </item>
    <item>
      <title>在使用pywinauto生成的多个系统中使用Wrapper_objects</title>
      <link>https://stackoverflow.com/questions/77819573/using-wrapper-objects-in-multiple-systems-generated-using-pywinauto</link>
      <description><![CDATA[我在使用 pywinauto 执行操作时得到了一个包装器。
现在我想在不同的系统上使用同一应用程序上的相同包装器来播放它。
可能吗？
我正在使用它创建包装对象：
from ctypes.wintypes import tagPOINT
导入 pywinauto
导入时间
时间.睡眠(2)
def get_ElementFromPoint(x,y):
     elem = pywinauto.uia_defines.IUIA().iuia.ElementFromPoint(tagPOINT(x, y))
     元素 = pywinauto.uia_element_info.UIAElementInfo(elem)
     包装器 = pywinauto.controls.uiawrapper.UIAWrapper(元素)
     返回包装器

创建的对象示例如下：


如何在不同的系统中使用此包装器来自动执行我的任务？]]></description>
      <guid>https://stackoverflow.com/questions/77819573/using-wrapper-objects-in-multiple-systems-generated-using-pywinauto</guid>
      <pubDate>Mon, 15 Jan 2024 11:49:34 GMT</pubDate>
    </item>
    <item>
      <title>在微调期间如何正确设置 pad token（不是 eos）以避免模型无法预测 EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>Precision、Recall 和 F1 可以是相同的值吗？</title>
      <link>https://stackoverflow.com/questions/54068401/can-the-precision-recall-and-f1-be-the-same-value</link>
      <description><![CDATA[我正在研究 ML 分类问题，并使用 sklearn 库的以下导入和相应代码来计算精度、召回率和 F1，如下所示。
从 sklearn.metrics 导入 precision_recall_fscore_support

打印（ precision_recall_fscore_support（y_test，prob_pos，average =&#39;加权&#39;））

结果
&lt;预&gt;&lt;代码&gt;0.8806451612903226、0.8806451612903226、0.8806451612903226

机器学习分类问题的精确率、召回率和 F1 这三项是否有可能获得相同的值？]]></description>
      <guid>https://stackoverflow.com/questions/54068401/can-the-precision-recall-and-f1-be-the-same-value</guid>
      <pubDate>Mon, 07 Jan 2019 03:58:04 GMT</pubDate>
    </item>
    </channel>
</rss>