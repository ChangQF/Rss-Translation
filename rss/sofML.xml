<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 04 Dec 2023 15:15:09 GMT</lastBuildDate>
    <item>
      <title>使用机器学习进行高光谱图像分类</title>
      <link>https://stackoverflow.com/questions/77600329/hyperspectral-image-classification-using-machine-learning</link>
      <description><![CDATA[问题陈述：我有一个人脸高光谱图像数据集，总参与者为42人，压力类别为4-情绪压力基线，情绪压力，身体压力基线，PS1（在特定时间测量的身体压力） ，PS2（在其他时间测量的身体压力）。使用这个超直肠图像数据集，我们必须将其分类为身体或情绪压力。该数据集包含 mat 文件。我们必须将其分类为身体或情绪压力。对于分类，我们必须使用SVM。您能否建议在jupyter笔记本中实现它的代码
文字
数据集链接-文本
导入 pandas 作为 pd
将 numpy 导入为 np
导入操作系统
从 sklearn.impute 导入 SimpleImputer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.svm 导入 SVC

# CSV 文件所在的目录
目录 = &#39;驱动器/我的驱动器/StO2_mat(size513_911)/&#39;

# 初始化空列表来存储数据和文件名
数据数组 = []
文件名 = []

# 循环遍历目录下的所有CSV文件
对于 os.listdir（目录）中的文件名：
    if filename.endswith(&#39;.csv&#39;):
        file_path = os.path.join(目录, 文件名)
        df = pd.read_csv(文件路径)
        data_array = df.values.ravel()
        data_arrays.append(data_array)
        file_names.append(文件名)

# 从一维 NumPy 数组列表创建一个 DataFrame
数据 = pd.DataFrame(data_arrays)

# 添加“目标列”包含原始文件名
数据[&#39;目标列&#39;] = 文件名

# 检查是否有足够的唯一样本用于分割
if len(data[&#39;target_column&#39;].unique()) &lt;= 1:
    print(“没有足够的唯一样本用于训练-测试分割。”)
别的：
    # 分离非数字和数字数据列
    non_numeric_data = data.select_dtypes(&#39;字符串&#39;)
    numeric_data = data.select_dtypes(include=[&#39;number&#39;])

    # 估算数值数据中的缺失值
    imputer = SimpleImputer(策略=&#39;均值&#39;)
    numeric_data_impulated = imputer.fit_transform(numeric_data)
    numeric_data_impulated_df = pd.DataFrame(numeric_data_impulated)

    # 合并非数值数据和估算数值数据
    impulated_data = pd.concat([non_numeric_data, numeric_data_impulated_df], axis=1)

    # 将数据分为训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(impulated_data.drop(&#39;target_column&#39;, axis=1), impulated_data[&#39;target_column&#39;], test_size=0.2, random_state=42)

    # 在训练数据上训练模型
    clf = SVC(内核=&#39;线性&#39;)
    clf.fit(X_train, y_train)

    # 对测试数据进行预测
    y_pred = clf.predict(X_test)

    # 评估模型性能
    准确度 = np.mean(y_pred == y_test)
    print(&#39;准确度：&#39;, 准确度)


我收到这样的错误
KeyError Traceback（最近一次调用最后一次）
&lt;ipython-input-6-0e3b82a51446&gt;在&lt;细胞系：31&gt;()
     45
     46 # 将数据拆分为训练集和测试集
---&gt; 47 X_train, X_test, y_train, y_test = train_test_split(impulated_data.drop(&#39;target_column&#39;, axis=1), impulated_data[&#39;target_column&#39;], test_size=0.2, random_state=42)
     48
     49 # 在训练数据上训练模型

5帧
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py 中 drop(self, labels, error)
   第6932章
   第6933章
-&gt;第6934章
   第6935章
   第6936章

KeyError：“在轴中找不到[&#39;target_column&#39;]”

在这段代码中，我尝试对高光谱图像进行分类。
我已经尝试解决，但到目前为止无法解决。请帮助解决错误。基本目标是打印准确性。
在此处输入图片描述
我试图打印准确性但出现错误]]></description>
      <guid>https://stackoverflow.com/questions/77600329/hyperspectral-image-classification-using-machine-learning</guid>
      <pubDate>Mon, 04 Dec 2023 14:45:22 GMT</pubDate>
    </item>
    <item>
      <title>后端的大.pkl数据没有推送到github中</title>
      <link>https://stackoverflow.com/questions/77600252/large-pkl-data-for-backend-is-not-pushed-in-github</link>
      <description><![CDATA[我正在学习机器学习。最近，我从 tmdb 数据集制作了电影推荐模型，我使用 .pkl （二进制）文件中的模型处理数据。使用该数据制作后端，但是数据太大，无法推送到 github，我无法托管网站。
我正在尝试将已处理的数据推送到后端，但无法部署，因为它超出了文件大小的限制]]></description>
      <guid>https://stackoverflow.com/questions/77600252/large-pkl-data-for-backend-is-not-pushed-in-github</guid>
      <pubDate>Mon, 04 Dec 2023 14:33:48 GMT</pubDate>
    </item>
    <item>
      <title>张量流形状错误</title>
      <link>https://stackoverflow.com/questions/77600065/tensorflow-shape-bug</link>
      <description><![CDATA[我正在进行深度 Q 学习，当我将图像提供给我的模型（来自硒驱动程序）时，一切都很好，但是当我尝试拟合我的模型时，它给了我这个错误
ValueError：层“顺序”的输入 0与图层不兼容：预期形状=(无, 500, 400, 1)，发现形状=(无, 4, 500, 400, 1)


这是我的图像、合身度和模型的代码
 def get_image(驱动程序):
      屏幕 = driver.get_screenshot_as_png()
      img = Image.open(BytesIO(屏幕))
      返回 np.array(img)[500:900, 400:900]

    def load_model(自身):
        ”“”
        从文件加载模型。
        ”“”

        if os.path.isfile(“model.h5”):
            self.model = load_model(“model.h5”)
            print(“模型已加载。”)

        模型=顺序（）
        #32个尺寸为3x3的滤波器的卷积层，具有relu激活函数
        model.add(Conv2D(32, (3, 3), input_shape=(500, 400, 1)))
        model.add(激活(&#39;relu&#39;))
        #池化层大小为2x2
        model.add(MaxPooling2D(pool_size=(2, 2)))

        #32个尺寸为3x3的滤波器的卷积层，具有relu激活函数
        model.add(Conv2D(32, (3, 3)))
        model.add(激活(&#39;relu&#39;))
        #池化层大小为2x2
        model.add(MaxPooling2D(pool_size=(2, 2)))

        #隐藏层64个神经元
        模型.add(压平())
        model.add(密集(64))
        model.add(激活(&#39;relu&#39;))
        #输出层有 5 个神经元，每个神经元对应一个可能的动作
        model.add(密集(5))

        #编译模型
        model.compile(loss=&#39;categorical_crossentropy&#39;,
                      优化器=&#39;亚当&#39;,
                      指标=[&#39;准确性&#39;])


        返回模型

      def fit_model(自身):
        ”“”
        将模型拟合到重播内存中。
        ”“”

        #如果回放内存不够满，则不要训练模型
        如果 len(self.replay_memory) &lt; self.MIN_REPLAY_MEMORY_SIZE：
            返回

        print(&quot;拟合模型&quot;)

        #从重放内存中获取过渡的随机样本
        样本 = random.sample(self.replay_memory, self.MINIBATCH_SIZE)

        #从样本中获取当前状态
        current_states = np.array([样本中的转换的转换[0]])
        #预测当前状态的q值
        current_qs_list = self.model.predict(current_states)

        #从样本中获取未来状态
        future_states = np.array([样本中的转换的转换[3]])
        #预测未来状态的q值
        future_qs_list = self.model.predict(future_states)

        X = []
        y = []

        #对于样本中的每个转换
        对于枚举（样本）中的索引（current_state、action、reward、future_state、done）：
            #如果过渡不是样本中的最后一个
            如果没有完成：
                #计算所采取行动的新q值
                max_future_q = np.max(future_qs_list[索引])
                new_q = 奖励 + self.DISCOUNT * max_future_q
            别的：
                #如果转变是样本中的最后一个，则将新的 q 值设置为奖励
                new_q = 奖励

            #更新所采取操作的q值
            current_qs = current_qs_list[索引]
            当前_qs[操作] = 新_q

            #将当前状态和新的q值添加到训练数据中
            X.append(当前状态)
            y.append(current_qs)

        #将模型拟合到训练数据上
        self.model.fit(np.array(X)，np.array(y)，batch_size=self.MINIBATCH_SIZE，verbose=0，shuffle=False)

        #更新目标模型
        如果 self.target_update_counter &gt; self.UPDATE_TARGET_EVERY：
            self.target_model.set_weights(self.model.get_weights())
            self.target_update_counter = 0
            self.save_model()
        别的：
            self.target_update_counter += 1```


我已经尝试对我的图像进行 .shape，但它不起作用。我还尝试将我的过渡[0]和过渡[3]直接添加到列表中，并制作预测列表，但它不起作用并告诉我 current_qs[action] 超出范围。这很奇怪，因为我在创建 futur_qs_list 和 current_qs_list 时想要预测的图像似乎与我用来预测动作的图像形状不同。然而，它与我的代码中的格式完全相同，所以我不知道该怎么办。]]></description>
      <guid>https://stackoverflow.com/questions/77600065/tensorflow-shape-bug</guid>
      <pubDate>Mon, 04 Dec 2023 14:08:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在第二次运行 Optuna 中重试失败的试验？</title>
      <link>https://stackoverflow.com/questions/77599820/how-can-i-retry-fail-trials-in-optuna-in-a-second-run</link>
      <description><![CDATA[我正在使用 Optuna 进行网格搜索，但失败的试验不会在第二次运行中重复。相反，已经完成的试验被无用地重复。
这里我分别描述一下两个问题：

当试验失败（例如缺乏计算资源）时，第二次启动网格搜索（Python 文件）时不会重复。这可以使用以下独立代码进行测试，其中我通过启动异常来模拟问题。注释这些行并再次重新运行，可以看到组合 x=2 和 y=2 没有重复。

导入时间
导入奥图纳
从 optuna.storages 导入 RetryFailedTrialCallback
将 numpy 导入为 np


定义目标（试用）：
    # 获取值
    参数 = {
                &#39;x&#39;: Trial.suggest_categorical(&#39;x&#39;, [0, 1, 2, 3]),
                &#39;y&#39;: Trial.suggest_categorical(&#39;y&#39;, [0, 1, 2, 3])
            }
    # 打印它
    print(&#39;用 x=&#39; 进行测试&#39; + str(params[&#39;x&#39;]), &#39;y=&#39; + str(params[&#39;y&#39;]))

    ##########################################
    # 首次运行后评论此部分#
    ##########################################
    如果 params[&#39;x&#39;] == 2 且 params[&#39;y&#39;] == 2：
        引发 ValueError(&quot;x==2, y==2&quot;)
    ##########################################

    ＃ 返回
    返回参数[&#39;x&#39;] ** 2 - 参数[&#39;y&#39;]



def optuna_search_space():
    # 定义搜索空间
    返回 {
        &#39;x&#39;：范围（3），
        &#39;y&#39;：范围（3），
    }



def optuna_grid():
    # 定义网址
    URL = &#39;mysql://&lt;用户&gt;:&lt;密码&gt;@:&lt;端口&gt;&#39;
    # 获取搜索空间
    搜索空间 = optuna_search_space()
    # 定义存储
    存储 = optuna.storages.RDBStorage(
        url=f&quot;{URL}/prove_optuna&quot;,
        failed_trial_callback=重试失败TrialCallback(max_retry=3),
    ）
    # 定义研究
    研究 = optuna.load_study(
        研究名称=“测试1”，
        采样器 = optuna.samplers.GridSampler(search_space),
        存储=存储，
    ）
    ＃ 跑步
    研究.优化（目标）
    ＃ 打印
    打印（研究.best_Trial）



如果 __name__ == “__main__”：
    ＃ 跑步
    optuna_grid()


当我重新运行代码时，它会重复已经执行的试验（或更多）。我不希望这样，因为这是计算资源的损失。

在 Optuna 仪表板上可以看到，在多次重新运行组合 (x=2, y=2) 后，它永远不会重复（即使第一次失败），并且组合 (x= 0, y=1) 已测试多次（无用）。

如何解决这些问题？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/77599820/how-can-i-retry-fail-trials-in-optuna-in-a-second-run</guid>
      <pubDate>Mon, 04 Dec 2023 13:32:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在colab中查找数据集的某一列中有多少个不同的数据</title>
      <link>https://stackoverflow.com/questions/77599408/how-to-find-how-many-different-data-are-in-a-column-of-a-data-set-in-colab</link>
      <description><![CDATA[我有一个大约由 400000 行和 8 列组成的数据集，我只想知道一列中有多少种不同类型的数据，我该怎么做？列中的数据是字符串的形式，我需要给它们分配数字，所以我需要找出该列中有多少个不同的单词。我不知道我应该做什么]]></description>
      <guid>https://stackoverflow.com/questions/77599408/how-to-find-how-many-different-data-are-in-a-column-of-a-data-set-in-colab</guid>
      <pubDate>Mon, 04 Dec 2023 12:22:17 GMT</pubDate>
    </item>
    <item>
      <title>如何创建机器人[关闭]</title>
      <link>https://stackoverflow.com/questions/77599245/how-to-create-a-bot</link>
      <description><![CDATA[我很想知道制作 Alexa 这样的机器人的代码是什么
我尝试过云计算，但我仍然对这个过程是如何完成的有些怀疑。我对云计算非常感兴趣，并对它进行了彻底的简短研究。我注意到 C3loud 计算是为了获得客户或用户的评论而完成的。云计算有可能永远是对的吗？就像云计算总是会显示正确的输出一样吗？]]></description>
      <guid>https://stackoverflow.com/questions/77599245/how-to-create-a-bot</guid>
      <pubDate>Mon, 04 Dec 2023 11:56:20 GMT</pubDate>
    </item>
    <item>
      <title>LLM 微调和推理所需的廉价云计算平台 [关闭]</title>
      <link>https://stackoverflow.com/questions/77599001/cheap-cloud-computing-platform-needed-for-llm-fine-tuning-and-inference</link>
      <description><![CDATA[我是一名刚毕业的人工智能毕业生，现在在一家非常小的初创公司工作，探索（并尝试实施）人工智能可以在公司软件中使用的地方。公司里没有其他人做人工智能，这就是为什么我想在这里问一个问题（也是因为我在谷歌上找不到具体的答案）。
基本上，我正在尝试使用 HuggingFace 来尝试一些法学硕士，以便我可以找到适合我的想法的法学硕士。问题是我的笔记本电脑不够强大，无法在 LLM 上运行推理，因为我只有 GTX 1650。我尝试使用 Google Colab，但只成功运行了一个小型 3B 参数模型，该模型表现不佳。
我的问题是：在哪里可以找到最便宜的云计算平台，该平台仍然强大到足以运行推理并可能对中小型法学硕士进行微调？如果有帮助的话，我目前正在尝试找到一个可以进行自定义命名实体识别的模型，因此该模型可能不需要太大，我也不需要进行训练。
问题是，由于我工作的公司是一家小型初创公司，他们无法为一个人提供像 AWS 或 Azure 这样的东西（我尝试研究了这方面的成本，我认为每月大约 2500 美元） .
我非常感谢您对此的帮助！感谢您的宝贵时间:)]]></description>
      <guid>https://stackoverflow.com/questions/77599001/cheap-cloud-computing-platform-needed-for-llm-fine-tuning-and-inference</guid>
      <pubDate>Mon, 04 Dec 2023 11:17:32 GMT</pubDate>
    </item>
    <item>
      <title>机器学习预测想法[关闭]</title>
      <link>https://stackoverflow.com/questions/77598931/machine-learning-forecast-ideas</link>
      <description><![CDATA[虚拟数据
我必须预测此数据未来 3 个月的故事点，我该如何开始？
我必须预测未来 3 个月的故事点，我可以使用哪种 ML 算法
如何分析数据以及数据的趋势]]></description>
      <guid>https://stackoverflow.com/questions/77598931/machine-learning-forecast-ideas</guid>
      <pubDate>Mon, 04 Dec 2023 11:07:11 GMT</pubDate>
    </item>
    <item>
      <title>高分辨率数据集和 Nvidia MX 150 GPU 的 YOLOv8 自定义模型性能问题</title>
      <link>https://stackoverflow.com/questions/77598734/yolov8-custom-model-performance-issue-with-high-resolution-dataset-and-nvidia-mx</link>
      <description><![CDATA[我最近开发了一个定制的 YOLOv8 模型来检测家庭环境中的眼镜。为此，我创建了一个包含 1000 张图像的数据集，这些图像是使用 Galaxy S22 Ultra 相机以 3000x4000 的分辨率拍摄的。该数据集中的每个图像都已进行相应注释。我在此数据集上训练了 YOLOv8 模型 100 个 epoch。
但是，当使用相同分辨率的视频测试模型时，我注意到检测性能存在明显的滞后。我正在尝试查明此问题的原因，并考虑两种可能性：
&lt;强&gt;1。训练数据集的高分辨率：
训练图像的高分辨率 (3000x4000) 是否会导致检测滞后？如果是这样，在训练之前将图像大小调整为较低的分辨率是否会提高性能而不显着影响检测精度？
&lt;强&gt;2。 GPU 能力：
我使用 Nvidia MX 150 GPU 进行训练和推理。在这种情况下，GPU 的功能是否会成为限制因素？如果是，有效训练和运行 YOLOv8 模型的推荐 GPU 规格是什么？
此外，如果您能提供有关在自定义数据集上训练 YOLOv8 模型的任何见解或最佳实践，尤其是在数据集准备和硬件要求方面，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77598734/yolov8-custom-model-performance-issue-with-high-resolution-dataset-and-nvidia-mx</guid>
      <pubDate>Mon, 04 Dec 2023 10:33:46 GMT</pubDate>
    </item>
    <item>
      <title>实施文本分类注意力机制的问题</title>
      <link>https://stackoverflow.com/questions/77598187/issue-with-implementing-attention-mechanisms-for-text-classification</link>
      <description><![CDATA[我第一次尝试注意力机制
我无法理解 keras 中 Attention 和 MultiHeadAttention 的使用。 （实现上混乱，概念上很清楚）
我的一些疑问。

想知道如何准确使用它们吗？

我发现像这样的线条
model.add（MultiHeadAttention（num_heads = 8，key_dim = 16，attention_axes =（1, 1）））
或者
model.add(Attention(use_scale=True))

以类似的方式，我从头开始找到代码，但它们都不能直接工作，所以可以使用哪些代码？

我在 ML/DL 中创建文本分类模型的一些工作背景
我当前的模型有
&lt;前&gt;&lt;代码&gt;模型 = 顺序()
model.add(嵌入(max_features, 128))
model.add(双向(LSTM(64)))
model.add（密集（1，激活=&#39;sigmoid&#39;））

现在我想添加注意力机制来改进它
任何参考资料或帮助都可能非常有帮助]]></description>
      <guid>https://stackoverflow.com/questions/77598187/issue-with-implementing-attention-mechanisms-for-text-classification</guid>
      <pubDate>Mon, 04 Dec 2023 08:47:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 生成头像图像的良好路线图/工作流程是什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77597420/what-is-a-good-roadmap-workflow-for-generating-a-headshot-image-using-python</link>
      <description><![CDATA[我想使用 Python AI/ML 创建头像图像，因此有人建议我如何创建它的工作流程，并建议我使用哪种深度学习/机器学习模型
我希望上传简单的五张图像并输出专业头像]]></description>
      <guid>https://stackoverflow.com/questions/77597420/what-is-a-good-roadmap-workflow-for-generating-a-headshot-image-using-python</guid>
      <pubDate>Mon, 04 Dec 2023 05:47:06 GMT</pubDate>
    </item>
    <item>
      <title>在本地设备中使用 MMOCR 进行文本识别推理期间出现“FileNotFoundError”</title>
      <link>https://stackoverflow.com/questions/77597246/filenotfounderror-during-text-recognition-inference-using-mmocr-in-local-devic</link>
      <description><![CDATA[SAR 文本识别模型用于自定义训练车牌数据集以识别尼泊尔字符。我使用 Google Drive 训练模型来识别文本。现在我想在本地设备上使用权重（以 .pth 扩展名结尾的文件）进行推理。 text_rec_model包含.pth文件的路径
infer = TextRecInferencer(weights=text_rec_model)

我在 PyCharm 终端中抛出此错误。
FileNotFoundError: [Errno 2] 没有这样的文件或目录: &#39;/content/drive/MyDrive/mmocr_tut/mmocr/configs/textrecog/sar/../../../dicts/english_digits_symbols.txt &#39;

配置文件是否保存在.pth 文件中？如果是这样，那么我如何编辑以便可以使用最新的检查点（在本例中为 epoch_85.pth 文件）运行推理]]></description>
      <guid>https://stackoverflow.com/questions/77597246/filenotfounderror-during-text-recognition-inference-using-mmocr-in-local-devic</guid>
      <pubDate>Mon, 04 Dec 2023 04:35:47 GMT</pubDate>
    </item>
    <item>
      <title>MLPClassifier 适合二元分类吗？</title>
      <link>https://stackoverflow.com/questions/77596591/is-mlpclassifier-appropriate-for-binary-classification</link>
      <description><![CDATA[我编写了一个使用 MLPClassifier 来解决二元分类问题的程序。它有点有效，但我不相信这是正确的模型。
我有 1300 个整数的十六进制数要放入两个类之一：类 0 和类 1。一个潜在的问题是，在我的训练数据中，98% 属于类 0，所以我将从 &quot; 获得 98% 的准确率“预测函数”总是返回“class 0”与输入无关。
是否有专为此类问题设计的机器学习模型？
================================================== ===============
TLDR？
我的数据如下：
X = 数组([[ 0, 11, 51, 13, 0, 9],
       [51,13,0,9,0,11],
       [ 0, 8, 0, 10, 0, 13],
       ...,
       [ 0, 11, 61, 12, 0, 8],
       [ 0, 8, 0, 0, 60, 11],
       [30, 11, 0, 6, 0, 9]])

目标是 y，一个包含 1300 个 0 和 1 的列表。我使用 MLPClassifier 并获得了 98% 的预测准确率。这时我突然想到，98% 的元组恰好属于 0 类，因此，如果我不费心进行任何机器学习，而是猜测类始终为 0，那么我将获得 98% 的准确率。
我检查了拟合度，看看它在 1 类元组上的表现如何，发现其中 82% 的预测正确，因此准确度为 98% 的 82%，即大约 80%，我想改进，但是怎么办？
除了盲目增加层的大小/数量之外，我不知道如何更改 MLPClassifier 的参数，但我突然想到，我很可能使用完全错误的模型来解决带有“是/”的学习问题没有分类。另外，六元组中的整数不是任意的，我想到这也可能与模型的选择有关。特别是，六个输入中的三个始终在 0 - 15 范围内，另外三个是两位数代码，第一位数字有三种可能，第二位数字有两种可能。
感谢您的任何想法。
代码：
m = MLPClassifier(hidden_​​layer_sizes = (256, 128, 64), max_iter=10000) # 从我在网上找到的示例粘贴:(
_ = m.fit(X, y)
yhat = m.predict(X)
cm = 混淆矩阵(y, yhat)
print( &#39;准确率 = &#39;, np.mean( y == yhat ) )
打印（厘米）

输出：
准确度 = 0.9816653934300993
[[1267 13]
 [11 18]]

(pdb) class1 = [ i for i in range(len(y)) if y[i] == 1]
(pdb) z = m.predict(np.row_stack((X[q] for q in class1)))
(pdb) z
数组([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 , 1, 1, 1, 0, 1])
(pdb) len(z), 总和(z)
29, 24
]]></description>
      <guid>https://stackoverflow.com/questions/77596591/is-mlpclassifier-appropriate-for-binary-classification</guid>
      <pubDate>Sun, 03 Dec 2023 23:53:22 GMT</pubDate>
    </item>
    <item>
      <title>在TF中实现FedAvg算法的问题</title>
      <link>https://stackoverflow.com/questions/77596548/problem-in-implementing-fedavg-algorithm-in-tf</link>
      <description><![CDATA[我在 google colab 中使用 TF 实现 FL（不使用 TFF），我使用 Bot-IoT 数据集，并将数据划分为 10 个客户端，以便每个客户端学习所有类型的课程，以便可以在所有课程上进行训练。 
我面临的问题是聚合从每个客户端接收到的模型权重，以获得全局准确率和召回率，在我的代码中为 0%。
这是聚合函数：
&lt;块引用&gt;
def federated_averaging(client_weights):
新权重 = []
# 模型层数

# 权重求和
forweights_list_tuple in zip(*client_weights): # 迭代每一层
    layer_mean = np.mean(np.array([np.array(weights) 用于weights_list_tuple中的权重]), axis=0)
    
    new_weights.append(layer_mean)

返回新的权重

我的代码是收集每轮的模型权重，然后对权重进行平均，得到用于计算全局准确率的平均权重。]]></description>
      <guid>https://stackoverflow.com/questions/77596548/problem-in-implementing-fedavg-algorithm-in-tf</guid>
      <pubDate>Sun, 03 Dec 2023 23:31:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 svm 进行高光谱图像分类</title>
      <link>https://stackoverflow.com/questions/77594411/hyperspectral-image-classification-using-svm</link>
      <description><![CDATA[将 pandas 导入为 pd
将 numpy 导入为 np
导入操作系统
从 sklearn.impute 导入 SimpleImputer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.metrics 导入 precision_score
从 imblearn.over_sampling 导入 SMOTE
从 imblearn.under_sampling 导入 RandomUnderSampler



# CSV 文件所在的目录
目录 = &#39;驱动器/我的驱动器/StO2_mat(size513_911)/&#39;

# 初始化空列表来存储数据和文件名
数据数组 = []
文件名 = []

# 循环遍历目录下的所有CSV文件
对于 os.listdir（目录）中的文件名：
    if filename.endswith(&#39;.csv&#39;):
        file_path = os.path.join(目录, 文件名)
        df = pd.read_csv(文件路径)
        data_array = df.values.ravel()
        data_arrays.append(data_array)
        file_names.append(文件名)

# 从一维 NumPy 数组列表创建一个 DataFrame
数据 = pd.DataFrame(data_arrays)

# 添加“目标列”包含原始文件名
数据[&#39;目标列&#39;] = 文件名

# 检查是否有足够的唯一样本用于分割
if len(data[&#39;target_column&#39;].unique()) &lt;= 1:
    print(“没有足够的唯一样本用于训练-测试分割。”)
别的：
    # 分离非数字和数字数据列
    non_numeric_data = data.select_dtypes(&#39;字符串&#39;)
    numeric_data = data.select_dtypes(include=[&#39;number&#39;])

    # 估算数值数据中的缺失值
    imputer = SimpleImputer(策略=&#39;均值&#39;)
    numeric_data_impulated = imputer.fit_transform(numeric_data)
    numeric_data_impulated_df = pd.DataFrame(numeric_data_impulated)

    # 合并非数值数据和估算数值数据
    impulated_data = pd.concat([non_numeric_data, numeric_data_impulated_df], axis=1)

    # 将数据分为训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(impulated_data.drop(&#39;target_column&#39;, axis=1), impulated_data[&#39;target_column&#39;], test_size=0.1, random_state=42)

   # 将 SMOTE 应用于训练数据
    smote = SMOTE(random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

    # 使用 RandomForestClassifier （如您的示例中所示）
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train_resampled, y_train_resampled)

    # 对测试数据进行预测
    y_pred = clf.predict(X_test)

    # 评估模型性能
    准确度=准确度_得分(y_test, y_pred)
    print(&#39;准确度：&#39;, 准确度)

我尝试过欠采样、不同的 ckassifiers，如 svm、knn 和随机森林分类器（对数据 imabalance 不太敏感）。仍然无法解决该错误。
错误-KeyError Traceback（最近一次调用最后一次）
 在&lt;细胞系：43&gt;()
43、如果len(imput_data)==1：
44#处理单个样品箱
---&gt; 45 X_train = impulated_data.drop(&#39;target_column&#39;, axis=1)
46 y_train = impulated_data[&#39;target_column&#39;]
47
5帧
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py 中 drop(self, labels, error)
第6932章
第6933章
-&gt;第6934章
第6935章
第6936章
KeyError：“在轴中找不到[&#39;target_column&#39;]”]]></description>
      <guid>https://stackoverflow.com/questions/77594411/hyperspectral-image-classification-using-svm</guid>
      <pubDate>Sun, 03 Dec 2023 13:04:03 GMT</pubDate>
    </item>
    </channel>
</rss>