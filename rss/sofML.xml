<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 07 May 2024 18:18:46 GMT</lastBuildDate>
    <item>
      <title>需要使用 Rand_forest 和 h2o 进行预测的指导</title>
      <link>https://stackoverflow.com/questions/78444040/need-guidance-on-predictions-with-rand-forest-and-h2o-with-r</link>
      <description><![CDATA[我有一个随机森林模型，我正在尝试更好地理解它。
为了举例，假设我们有一片蓝莓灌木丛。我们感兴趣的是预测特定灌木丛中腐烂蓝莓的产量以及各个灌木丛中所有蓝莓的收获量。
每个灌木都有一个识别名称：bush_name，例如&#39;bush001&#39;，我们希望根据每个单独的灌木进行预测。例如，我想知道 Bush025 是否在 2/2/22 生产了腐烂的浆果。
为了本示例，输入位于具有以下虚拟结构的 df 中：
train_data &lt;- data.frame(date = c(&quot;2022-01-01&quot;, &quot;2022-01-07&quot;, &quot;2022-02-09&quot;, &quot;2022-05&quot; -01”、“2022-11-01”、“2022-11-02”)、
                   Bush_name = c(“bush001”、“bush001”、“bush001”、“bush043”、“bush043”、“bush043”),
                   错误 = c(2, 0, 1, 0, 3, 1),
                   有腐烂的浆果 = c(1, 0, 0, 1, 1, 0),
                   浆果计数 = c(12, 1, 7, 100, 14, 4),
                   天气 = c(1, 0, 2, 0, 1, 1))

我已经建立了一个随机森林模型，并进行了以下高级设置：
库(agua)
图书馆（防风草）
图书馆（水）

h2o.init(n线程 = -1)

model_fit &lt;- rand_forest(mtry = 10, trees = 100) %&gt;%
  set_engine(“h2o”) %&gt;%
  set_mode(“分类”) %&gt;%
  适合（has_rotten_berry ~ .,
      数据 = train_data) %&gt;%
  step_dummy(灌木名称) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_normalize(all_predictors())

我想知道的是：
当我尝试预测训练模型中的新数据时，似乎我只能使用我已经训练过的灌木丛的 Bush_names 输入新的测试数据。 我假设该模型正在创建特定于灌木丛的预测是否正确？因此必须在训练中输入新的灌木丛信息才能输出这些新灌木丛的未来预测？
示例：我种植了一棵新灌木，bush700，它不存在于原始训练数据集中。如果我尝试使用新的灌木丛数据进行预测，但训练数据中不存在该数据，则会向我传达一条消息：数据中有新的级别。所以我假设因为这些预测似乎是特定于灌木丛的，并且我们无法为新添加的灌木丛获得任何新的灌木丛预测。
这个假设正确吗？
谢谢您，对于可能令人困惑的隐喻，我深表歉意。也欢迎您对该模型可能有的任何其他反馈。]]></description>
      <guid>https://stackoverflow.com/questions/78444040/need-guidance-on-predictions-with-rand-forest-and-h2o-with-r</guid>
      <pubDate>Tue, 07 May 2024 16:58:00 GMT</pubDate>
    </item>
    <item>
      <title>model.fit 对使用 tf.data.experimental.make_csv_dataset 创建的张量流数据集给出错误</title>
      <link>https://stackoverflow.com/questions/78443975/model-fit-gives-error-with-tensorflow-dataset-created-with-tf-data-experimental</link>
      <description><![CDATA[我是张量流新手。我正在尝试从 CSV 文件读取值并将其加载为张量流数据集。但是，当我尝试运行 model.fit 时，它给出以下错误 -
输入“input_39”缺少数据。您传递了一个带有键 [&#39;Age&#39;, &#39;Number&#39;, &#39;Start&#39;] 的数据字典。需要以下键：[&#39;input_39&#39;]
这是我的代码-
将 numpy 导入为 np
将 pandas 导入为 pd
将张量流导入为 tf

input_file=&#39;kyphosis.csv&#39;

all_dataset = tf.data.experimental.make_csv_dataset(input_file,batch_size=1,label_name=“Kyphosis”,num_epochs=1)

模型=tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(3))
model.add(tf.keras.layers.Dense(10))
model.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;))

model.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,run_eagerly=True)

model.fit(all_dataset,epochs=10)

请让我知道我在这里做错了什么。 Tensorflow版本是2.11.0。
我尝试使用 tf.data.Dataset.from_tensor_slices 但遇到相同的错误-
df=pd.read_csv(&#39;kyphosis.csv&#39;)
X=df.drop(&#39;脊柱后凸&#39;,axis=1)
y=df[&#39;脊柱后凸&#39;]

all_dataset=tf.data.Dataset.from_tensor_slices((X.to_dict(orient=&#39;list&#39;),y))
all_dataset = all_dataset.batch(1)

模型=tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(3))
model.add(tf.keras.layers.Dense(10))
model.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;))

model.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;)
model.fit(all_dataset,epochs=3)

错误-
ValueError：输入“input_41”缺少数据。您传递了一个带有键 [&#39;Age&#39;, &#39;Number&#39;, &#39;Start&#39;] 的数据字典。需要以下键：[&#39;input_41&#39;]]]></description>
      <guid>https://stackoverflow.com/questions/78443975/model-fit-gives-error-with-tensorflow-dataset-created-with-tf-data-experimental</guid>
      <pubDate>Tue, 07 May 2024 16:45:38 GMT</pubDate>
    </item>
    <item>
      <title>如何在保持梯度的同时提取“MaskedTensor”的值？</title>
      <link>https://stackoverflow.com/questions/78443158/how-to-extract-the-value-of-a-maskedtensor-while-maintaining-the-gradient</link>
      <description><![CDATA[我有一个 MaskedTensor（参见文档）
&lt;前&gt;&lt;代码&gt;打印（结果）
&gt;&gt;&gt;&gt;&gt;
掩蔽张量(
  [
    [ 1.7866, 2.5468, 1.6330],
    [2.2041、2.5388、2.3315]
  ]
）

我正在尝试用这个张量计算损失
loss = 5 - torch.sum(result_summed)

但我收到 TypeError: unsupported operand type(s) for -: &#39;int&#39; and &#39;MaskedTensor&#39;
问题似乎是，即使我调用 torch.sum(result_summed) 我也会得到一个“MaskedTensor”对象：
print(torch.sum(result_summed))
&gt;&gt;&gt;&gt;&gt;
掩码张量（13.0409，真）

如何提取值（此处为13.0409）来计算损失，同时保持梯度？]]></description>
      <guid>https://stackoverflow.com/questions/78443158/how-to-extract-the-value-of-a-maskedtensor-while-maintaining-the-gradient</guid>
      <pubDate>Tue, 07 May 2024 14:18:58 GMT</pubDate>
    </item>
    <item>
      <title>大语言模型提示重复数据删除</title>
      <link>https://stackoverflow.com/questions/78443093/large-language-model-prompt-for-deduping-data</link>
      <description><![CDATA[我正在尝试使用 LLM 提示对以下数据进行重复数据删除
 产品线：DTV，销售渠道：间接，总增加量 =51
    产品线：BYOD，销售渠道：ONLINE2，总添加量=100
    产品线：BYOD，销售渠道：ONLINE1，总添加量=200
    产品线：BYOD，销售渠道：ONLINE3，总增加量=400
    产品线：BYOD，销售渠道：ONLINE4，总添加量=500
    产品线：BYOD，销售渠道：空，总添加量=300

重复数据删除标准：
1.识别所有Sales Channel=null记录并
看看它们是否重复
2.如果是相同的产品，则它们是重复的
任何其他销售渠道的总增加额与总增加额等于
空记录总添加量。
================================================== =======================
示例：
对于  产品线：BYOD，销售渠道：null，总添加数：300，
产品线：BYOD，销售渠道：空，总增加量：300 =

产品线：BYOD，销售渠道：ONLINE2，总添加量=100
+
产品线：BYOD，销售渠道：ONLINE1，总添加量=200

，其中 300==300
所以我需要将销售渠道：null，总添加= 300识别为重复
如何针对此类问题有效创建LLM提示？
我还需要从类似的数据集中识别趋势，有效的方法是什么？
我计划使用 Mistral/DBRX 模型。]]></description>
      <guid>https://stackoverflow.com/questions/78443093/large-language-model-prompt-for-deduping-data</guid>
      <pubDate>Tue, 07 May 2024 14:09:17 GMT</pubDate>
    </item>
    <item>
      <title>最适合实时季节性数据峰值检测的 ML 模型或统计指标是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/78442853/what-are-the-most-suited-ml-models-or-statistical-indicators-for-peak-detection</link>
      <description><![CDATA[我目前正在尝试创建一种用于实时功耗数据分析的算法。目标很简单：实时检测建筑物中的功耗峰值/尖峰，并采取必要的措施吸收峰值。
数据以功率值 (kW) 流的形式出现，每 10 分钟采样一次。我有一年多的功耗数据集。
数据具有很强的季节性，建筑物的耗电量根据天气、人流量、节假日的不同而变化很大：许多未知参数。
我可以将功率峰值定义为相对值功率的快速激增，与建筑物的季节性趋势无关，持续时间不会超过一个小时（例如：超出正常值的异常）。
这个定义是非常相对的。在图表上，很容易区分峰值。有了数学规则，它就会变得更加复杂，特别是考虑到单个阈值或标准差。
这是两天的数据图表。蓝色部分为功耗。红色表示手动解释的峰值。
正如您所看到的，定义峰值比定义常态或“季节性”更难。一月的峰值与七月的峰值不同。它们没有相同的价值，也没有相同的起源。
这使得很难使用标准差、平均值或阈值等基本工具进行识别。
有几种检测峰值的方法，基于统计和机器学习。我尝试了两者，但我有点迷失了。
第一种方法是我所说的统计方法：比较一个样本的标准差（z 分数）、高于平均值或指数平均值的阈值。 （例如：此处）
那里的问题是不可避免地引入滞后，这使得模型对功耗的快速但季节性的增加以及对“噪声”的敏感性敏感。以及所述峰值对均值的不良影响。基本上，它无法处理建筑物的基本行为。
机器学习对于应对趋势很有用。这让我想到了异常检测模型。乍一看，他们可以了解什么是正常行为并检测异常值+他们大多不受监督，当现实生活中异常值的定义很粗略时，这是一件好事。
当然，我尝试了一下。我使用了 PySad 库并开始尝试以下模型 指南。到目前为止，移动窗口上的 IForest 模型似乎工作得最好，AUROC 分数接近 1。（如果该值低于样本平均值，我通过清空异常分数来消除坑）。
我还尝试了 LOFP 和 KNNCAD，但结果很差。
这里的问题是模型的准确度根据数据集的不同而变化很大，在不太明显的峰值上低至 (AUROC)0,65。
我现在的问题如下：您对用于实时准确检测峰值的合适的 ML 模型或统计方法有什么见解或想法吗？ （自动编码器、CWT、LOF、SVM？）或者您知道更好的方法吗？
我计划结合多种方法，但我觉得将方法准确性的随机性放在一起不会给我带来更好的结果。
我还必须考虑到调整 SARIMA 等高级模型的超参数是不可能的，因为使用该算法的人不是机器学习专家或数据分析师。
这个项目是我的计算机科学硕士学位的一部分。
谢谢你，]]></description>
      <guid>https://stackoverflow.com/questions/78442853/what-are-the-most-suited-ml-models-or-statistical-indicators-for-peak-detection</guid>
      <pubDate>Tue, 07 May 2024 13:26:16 GMT</pubDate>
    </item>
    <item>
      <title>如何在 python 中为一个奇一问题创建 SPoSE 模型？</title>
      <link>https://stackoverflow.com/questions/78442722/how-can-i-create-s-spose-model-in-python-for-one-odd-one-out-problem</link>
      <description><![CDATA[我需要了解如何为三元组问题创建稀疏正相似嵌入，但我不知道如何开始
我正在考虑开始从它们创建嵌入，然后计算它们之间的距离，这是正确的方法吗？谁能建议一个代码]]></description>
      <guid>https://stackoverflow.com/questions/78442722/how-can-i-create-s-spose-model-in-python-for-one-odd-one-out-problem</guid>
      <pubDate>Tue, 07 May 2024 13:03:28 GMT</pubDate>
    </item>
    <item>
      <title>由于导入，在 python 程序中使用 bash 脚本运行 python 代码时出现高延迟</title>
      <link>https://stackoverflow.com/questions/78442377/high-latency-while-running-python-code-using-bash-script-in-a-python-program-due</link>
      <description><![CDATA[我有一个 python 应用程序，它使用子进程来运行 bash 脚本。 bash 脚本依次运行一个 python 文件，其中包含一些导入（视网膜面部、深层面部库）。该应用程序需要花费大量时间来运行，因为每次运行子进程时，都需要 20-30 秒来加载/导入视网膜/深脸模块。有没有办法可以加快速度？
注意：更改设置是不可能的，即我无法直接从原始 python 代码调用 python 代码。
我不确定如何解决这个问题，感谢任何帮助。谢谢。
我尝试使用系统模块的缓存版本，但这不起作用。虽然，我不确定如何正确使用它。]]></description>
      <guid>https://stackoverflow.com/questions/78442377/high-latency-while-running-python-code-using-bash-script-in-a-python-program-due</guid>
      <pubDate>Tue, 07 May 2024 12:09:10 GMT</pubDate>
    </item>
    <item>
      <title>各种模型预测的组合都会得出类似的基本事实</title>
      <link>https://stackoverflow.com/questions/78442079/various-combination-of-model-predictions-yields-to-similar-ground-truth</link>
      <description><![CDATA[我有一个模型（3DUnet，回归问题）可以预测值 PD 和 T1，其中 PD 和 T1 是基于输入的 qMRI 输出。根据这些预测，我使用以下公式计算 T1_Weighted_image：Weighted_images = PD (1 - exp(-1 / (T1 + epsilon)))*，其中 epsilon 很小值以防止被零除和 T1=&gt;0 。在训练期间，我用于损失计算的基本事实是 T1_Weighted_groundtruth，但我也有 PD 和 T1 的基本事实值，尽管它们不直接用于损失计算。它们用于确保 PD 和 T1 预测值的正确性。损失是使用 T1_Weighted_predict 和 T1_Weighted_groundtruth 之间的损失函数计算的。
但是，存在各种 PD 或 T1 组合，可以为 T1_Weighted 产生类似的结果。例如，我的模型可能预测 PD 的非常低的值（例如在 CSF 中作为一个明显的例子），而不是预测 T1 的高值（这是正确的答案）。有没有一种方法可以迫使我的模型预测正确的值，或者至少预测（任何）可能的组合？]]></description>
      <guid>https://stackoverflow.com/questions/78442079/various-combination-of-model-predictions-yields-to-similar-ground-truth</guid>
      <pubDate>Tue, 07 May 2024 11:19:19 GMT</pubDate>
    </item>
    <item>
      <title>如何训练我的图像识别模型，使其像奖励惩罚系统一样工作，让我可以分辨出它无法识别的人是谁？</title>
      <link>https://stackoverflow.com/questions/78441996/how-do-i-train-my-image-recognition-model-to-work-like-a-reward-punishment-syste</link>
      <description><![CDATA[我正在研究制作考勤系统的方法，教授点击几张照片（2到3张）
并上传到应用程序，大约 80 名学生会自动出勤。我的训练数据有限，这是我们需要应对的最大缺点和主要问题。我制作了一个用于训练和标记出勤率的基本模型。
我如何训练它像奖励惩罚系统一样工作，我可以手动告诉它它无法识别的人是谁，以便它在途中学习。]]></description>
      <guid>https://stackoverflow.com/questions/78441996/how-do-i-train-my-image-recognition-model-to-work-like-a-reward-punishment-syste</guid>
      <pubDate>Tue, 07 May 2024 11:03:00 GMT</pubDate>
    </item>
    <item>
      <title>“管道”对象没有属性“_check_fit_params”</title>
      <link>https://stackoverflow.com/questions/78440449/pipeline-object-has-no-attribute-check-fit-params</link>
      <description><![CDATA[来自 imblearn.over_sampling 导入 SMOTE
从 imblearn.under_sampling 导入 RandomUnderSampler
从 imblearn.pipeline 导入管道

# 定义特征和目标
X = df.drop(&#39;感染&#39;, axis=1)
y = df[&#39;感染&#39;]

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义重采样策略
over = SMOTE(sampling_strategy=0.5) # 将少数类过采样到多数类的 50%
under = RandomUnderSampler(sampling_strategy=0.8) # 将多数类欠采样至其原始大小的 80%

管道 = 管道(步骤=[(&#39;o&#39;, 上), (&#39;u&#39;, 下)])

# 应用重采样
X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)

# 显示新的类分布
print(“重采样的类分布：”, pd.Series(y_resampled).value_counts())

这是我的代码
这是我遇到的错误
AttributeError Traceback（最近一次调用最后一次）
单元格 In[7]，第 19 行
     16 pipeline = Pipeline(steps=[(&#39;o&#39;, over), (&#39;u&#39;, under)])
     18 # 应用重采样
---&gt; 19 X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)
     21 # 显示新的班级分布
     22 print(&quot;重采样的类分布：&quot;, pd.Series(y_resampled).value_counts())

文件 ~\anaconda3\Lib\site-packages\imblearn\pipeline.py:372，在 Pipeline.fit_resample(self, X, y, **fit_params)
    第342章
    第343章
    第344章 一个接一个地安装所有变压器/采样器并且
   （...）
    第369章 变形的目标。
    第370章
    第371章
--&gt;第372章
    第373章
    第374章

AttributeError：“管道”对象没有属性“_check_fit_params”

我已经尝试了一切。我的所有包都已更新。我尝试使用的所有方法都在 sklearn 和 imblearn 这两个网站上查看。]]></description>
      <guid>https://stackoverflow.com/questions/78440449/pipeline-object-has-no-attribute-check-fit-params</guid>
      <pubDate>Tue, 07 May 2024 06:12:34 GMT</pubDate>
    </item>
    <item>
      <title>如何训练我的图像识别模型，使其像奖励惩罚系统一样工作，在其中我教它不认识的人是谁？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78439584/how-do-i-train-my-image-recognition-model-to-work-like-a-reward-punishment-syste</link>
      <description><![CDATA[我正在研究制作考勤系统的方法，教授点击几张照片（2到3张）
并上传到应用程序，大约 80 名学生会自动出勤。我的训练数据有限，这是我们需要应对的最大缺点和主要问题。我制作了一个用于训练和标记出勤率的基本模型。
我如何训练它像奖励惩罚系统一样工作，我可以手动告诉它它无法识别的人是谁，以便它在途中学习。]]></description>
      <guid>https://stackoverflow.com/questions/78439584/how-do-i-train-my-image-recognition-model-to-work-like-a-reward-punishment-syste</guid>
      <pubDate>Tue, 07 May 2024 00:12:44 GMT</pubDate>
    </item>
    <item>
      <title>如何基于掩码相乘矩阵并排除元素？</title>
      <link>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</guid>
      <pubDate>Mon, 29 Apr 2024 19:07:12 GMT</pubDate>
    </item>
    <item>
      <title>用最少层数训练绝对函数的神经网络</title>
      <link>https://stackoverflow.com/questions/78311513/train-neural-network-for-absolute-function-with-minimum-layers</link>
      <description><![CDATA[我正在尝试训练神经网络来学习 y = |x|功能。我们知道，绝对函数有两条不同的线在零点处相互连接。所以我尝试使用以下顺序模型：
隐藏层：
2 致密层（激活relu）
输出层：
1 致密层
训练模型后，它只拟合函数的一半边。大多数时候是右手边，有时是左手边。一旦我在隐藏层中再添加 1 层，那么我就用 3 层代替 2 层，它就完全符合该功能了。谁能解释为什么当绝对函数只有一次切割时需要额外的一层？
这是代码：
将 numpy 导入为 np


X = np.linspace(-1000,1000,400)
np.random.shuffle(X)
Y = np.abs(X)

# 重塑数据以适应模型输入
X = X.reshape(-1, 1)
Y = Y.重塑(-1, 1)

将张量流导入为 tf
将张量流导入为 tf
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

# 构建模型
模型 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(2, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(1)
]）

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;,metrics=[&#39;mae&#39;])
model.fit(X, Y, epochs=1000)
# 使用模型进行预测
Y_pred = model.predict(X)

# 绘制结果
plt.scatter(X, Y, color=&#39;blue&#39;, label=&#39;实际&#39;)
plt.scatter(X, Y_pred, color=&#39;red&#39;, label=&#39;预测&#39;)
plt.title(&#39;实际与预测&#39;)
plt.xlabel(&#39;X&#39;)
plt.ylabel(&#39;Y&#39;)
plt.图例()
plt.show()

2 个密集层的绘图：

3 个密集层的绘图：
]]></description>
      <guid>https://stackoverflow.com/questions/78311513/train-neural-network-for-absolute-function-with-minimum-layers</guid>
      <pubDate>Thu, 11 Apr 2024 15:34:01 GMT</pubDate>
    </item>
    <item>
      <title>如何通过 LSTM 模型预测具有多个特征的一个输出？</title>
      <link>https://stackoverflow.com/questions/72595250/how-to-forecast-one-output-with-multiple-features-by-lstm-model</link>
      <description><![CDATA[我正在研究一些股票的时间序列数据，并尝试使用多变量特征预测趋势。下面是我拥有的示例数据集，其中包括不同的技术指标，包括每只股票的移动平均线、抛物线转向等。从不同的在线来源来看，他们中的大多数都使用一个特征（例如一次“收盘价”）来预测一只股票。我如何利用所有股票的特征来预测一个输出，比如说标准普尔的收盘价。我知道这可能无助于提高预测准确性，但我不确定我现在正在训练什么，希望对 LSTM 模型有更多的了解。

基本上，我把整个数据集放进去，然后进行缩放和训练。如何将预测指定在一列上？
代码：
scaler = MinMaxScaler(feature_range = (0,1))
scaled_feature_data = scaler.fit_transform(feature_data)
X_train, y_train = training_set[:, :-1], training_set[:, -1]
X_test, y_test = testing_set[:, :-1], testing_set[:, -1]
X_train = X_train.reshape((X_train.shape[0],1,X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0],1,X_test.shape[1]))
model_lstm.add(LSTM(50, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))

模型：
model_lstm.add(LSTM(50, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))
model_lstm.add(Dropout(0.2))
model_lstm.add(LSTM(units=50, return_sequences=True))
model_lstm.add(Dropout(0.2))
model_lstm.add(LSTM(units=50, return_sequences=True))
model_lstm.add(Dropout(0.2))
model_lstm.add(LSTM(units=50))
model_lstm.add(Dropout(0.2))
model_lstm.add(Dense(units=1,激活=&#39;relu&#39;))
]]></description>
      <guid>https://stackoverflow.com/questions/72595250/how-to-forecast-one-output-with-multiple-features-by-lstm-model</guid>
      <pubDate>Sun, 12 Jun 2022 19:24:42 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch，切片张量导致 RuntimeError:: 梯度计算所需的变量之一已被就地操作修改：</title>
      <link>https://stackoverflow.com/questions/60869124/pytorch-slicing-tensor-causes-runtimeerror-one-of-the-variables-needed-for-gr</link>
      <description><![CDATA[我用 Pycharm 编写了一个带有 LSTM 单元的 RNN。该网络的特点是，RNN 的输出被输入到使用 Runge-kutta 计算的积分运算中。
积分需要一些输入，并及时将其向前传播一步。为了做到这一点，我需要沿批处理维度对特征张量 X 进行切片，并将其传递给 Runge-kutta。
class MyLSTM(torch.nn.Module):
def __init__(self, ni, no, samples_interval, nh=10, nlayers=1):
super(MyLSTM, self).__init__()

self.device = torch.device(&quot;cpu&quot;)
self.dtype = torch.float
self.ni = ni
self.no = no
self.nh = nh
self.nlayers = nlayers

self.lstms = torch.nn.ModuleList(
[torch.nn.LSTMCell(self.ni, self.nh)] + [torch.nn.LSTMCell(self.nh, self.nh) for i in range(nlayers - 1)])
self.out = torch.nn.Linear(self.nh, self.no)
self.do = torch.nn.Dropout(p=0.2)
self.actfn = torch.nn.Sigmoid()
self.sampling_interval = samples_interval
self.scaler_states = None
# 选项

# 整个块的描述
def forward(self, x, h0, train=False, integration_ode=True):
x0 = x.clone().requires_grad_(True)
hs = x # 启动隐藏状态

if h0 is None:
h = torch.zeros(hs.shape[0], self.nh, device=self.device)
c = torch.zeros(hs.shape[0], self.nh, device=self.device)
else:
(h, c) = h0

# LSTM 单元
for i in range(self.nlayers):
h, c = self.lstms[i](hs, (h, c))
if train:
hs = self.do(h)
else:
hs = h

# 输出层
# y = self.actfn(self.out(hs))
y = self.out(hs)

if integration_ode:
p = y
y = self.integrate(x0, p)
return y, (h, c)

def integration(self, x0, p):
# 每个间隔 RK4 步骤
M = 4
DT = self.sampling_interval / M
X = x0
# X = self.scaler_features.inverse_transform(x0)

for b in range(X.shape[0]):
xx = X[b, :]
for j in range(M):
k1 = self.ode(xx, p[b, :])
k2 = self.ode(xx + DT / 2 * k1, p[b, :])
k3 = self.ode(xx + DT / 2 * k2, p[b, :])
k4 = self.ode(xx + DT * k3, p[b, :])
xx = xx + DT / 6 * (k1 + 2 * k2 + 2 * k3 + k4)
X_all[b, :] = xx
return X_all

def ode(self, x0, y):
# 这里我是一个动态模型


我得到这个错误：
RuntimeError：梯度计算所需的变量之一已被就地操作修改：[torch.FloatTensor []]，它是 SelectBackward 的输出 0，处于版本 64；预期版本为 63。提示：启用异常检测以查找无法计算其梯度的操作，使用 torch.autograd.set_detect_anomaly(True)。

问题出在操作 xx = X[b, :] 和 p[b,:]。我知道因为我选择了批处理维度 1，所以我可以用 xx=X 和 p 替换前两个方程，这样就可以了。如何在不丢失梯度的情况下分割张量？]]></description>
      <guid>https://stackoverflow.com/questions/60869124/pytorch-slicing-tensor-causes-runtimeerror-one-of-the-variables-needed-for-gr</guid>
      <pubDate>Thu, 26 Mar 2020 14:06:24 GMT</pubDate>
    </item>
    </channel>
</rss>