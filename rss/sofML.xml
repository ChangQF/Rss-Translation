<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 05 Jun 2024 03:19:03 GMT</lastBuildDate>
    <item>
      <title>如何使用 Keras 创建小批量？</title>
      <link>https://stackoverflow.com/questions/78577993/how-to-create-minibatch-with-keras</link>
      <description><![CDATA[假设我有 20 个 5*5 张量。我该如何创建一个 batchsize = 20 的数据集？
读了很多帖子后，我觉得最有可能的解决方案是：
步骤 1.使用 tf.dataset.from_tensors 创建 20 个数据集，每个数据集包含一个 5*5 张量。
步骤 2.使用 tf.dataset.zips 将 20 个数据集压缩为一个大数据集。
步骤 3.不要在 model.fit 中声明 batch_size=20，因为从官方文档中可以看到，“如果您的数据是数据集、生成器或 keras.utils.PyDataset 实例的形式，请不要指定 batch_size（因为它们会生成批次）”
我不明白批次是从上述步骤生成的。是不是因为数据集的形状（2055）意味着小批次大小应该是 20，这等于第一个参数？
如果以下陈述是正确的。假设我想将批次大小减少到 10。我需要做的就是在步骤 1 之后先将两个数据集压缩 10 次，然后将新的 10 个数据集压缩为最终数据集。这是正确的吗？
此外，我尝试在第 2 步之后使用 tf.dataset.batch(20) 应用大数据集。在我 tf.print 执行此批处理命令之前和之后的最终数据集后，我得到的输出是：
之前：压缩数据集 (array(shape(55), array(shape(55)), …)
之后：批处理数据集 (array(shape(none55), array(shape(none55)), …)
形状不同。输出值后，我注意到应用 tf.dataset.batch(20) 后的实际形状为每个数组变为 1*5*5。
这个命令 tf.dataset.batch(20) 在我的情况下是无用的吗？还是我应该在第 2 步之后使用它？
官方文档仅使用一个例子是无量纲数组。所以我不知道这个命令对高阶张量如何起作用。]]></description>
      <guid>https://stackoverflow.com/questions/78577993/how-to-create-minibatch-with-keras</guid>
      <pubDate>Tue, 04 Jun 2024 22:21:52 GMT</pubDate>
    </item>
    <item>
      <title>R 中的 Firth 模型“重采样性能指标中的缺失值”</title>
      <link>https://stackoverflow.com/questions/78577799/firths-model-in-r-missing-values-in-resampled-performance-measures</link>
      <description><![CDATA[以下是带有样本数据的示例
library(caret)
library(logistf)
library(data.table)
library(ggplot2)

# 创建合成数据集
set.seed(123)
n &lt;- 100
data &lt;- data.table(
predictor1 = rnorm(n),
predictor2 = rnorm(n),
predictor3 = rnorm(n),
group = factor(sample(c(&quot;control&quot;, &quot;AD&quot;), n, replace = TRUE))
)

# 标准化预测变量
data_normalized &lt;- as.data.table(scale(data[, .SD, .SDcols = -&quot;group&quot;]))
data_normalized[, group := data$group]

# 分为训练集和测试集
train_index &lt;- createDataPartition(data_normalized$group, p = .75, list = FALSE)
train &lt;- data_normalized[train_index, ]
test &lt;- data_normalized[-train_index, ]

# 标准化训练数据
train_proc &lt;- predict(preProcess(train, method = c(&quot;center&quot;, &quot;scale&quot;), verbose = TRUE), train)
train_proc[, group := factor(group, levels = c(&quot;control&quot;, &quot;AD&quot;))]

# 定义训练控制
train_control &lt;- trainControl(method = &quot;repeatedcv&quot;, 
number = 3, repeats = 3,
savePredictions = TRUE,
classProbs = TRUE)

# 定义自定义模型函数
firth_model &lt;- list(
type = &quot;Classification&quot;,
library = &quot;logistf&quot;,
loop = NULL,
parameters = data.frame(parameter = c(&quot;none&quot;), class = c(&quot;character&quot;), label = c(&quot;none&quot;)),
grid = function(x, y, len = NULL, search = &quot;grid&quot;) {
data.frame(none = &quot;none&quot;)
},
fit = function(x, y, wts, param, lev, last, classProbs, ...) {
data &lt;- as.data.frame(x)
data$group &lt;- y
logistf(group ~ ., data = data, control = logistf.control(maxit = 100), ...)
},
predict = function(modelFit, newdata, submodels = NULL) {
as.factor(ifelse(predict(modelFit, newdata, type = &quot;response&quot;) &gt; 0.5, &quot;AD&quot;, &quot;control&quot;))
},
prob = function(modelFit, newdata, submodels = NULL) {
preds &lt;- predict(modelFit, newdata, type = &quot;response&quot;)
data.frame(control = 1 - preds, AD = preds)
}
)

# 训练模型
set.seed(123)
firth.logist.model &lt;- train(train_proc[, .SD, .SDcols = !c(&quot;group&quot;)],
train_proc$group,
method = firth_model,
trControl = train_control)

print(firth.logist.model)


收到以下错误:
&gt; firth.logist.model &lt;- train(train_proc[, .SD, .SDcols = !c(&quot;group&quot;)],
+ train_proc$group,
+ method = firth_model,
+ trControl = train_control)
出现问题；所有准确度指标值均缺失：
准确度 Kappa 
最小值：NA 最小值：NA 
第 1 组：NA 第 1 组：NA 
中位数：NA 中位数：NA 
平均值：NaN 平均值：NaN 
第 3 组：NA 第 3 组：NA 
最大值：NA 最大值: NA 
NA :1 NA :1 
错误：正在停止
此外：警告消息：
在 normTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
中，重新采样的性能指标中缺少值。


我试图比较不同模型的性能，当我使用 caret 中的逻辑回归或随机森林时，模型都有效（使用相同的训练、交叉验证和测试数据），所以不确定 Firth 模型在这里失败的原因是什么。]]></description>
      <guid>https://stackoverflow.com/questions/78577799/firths-model-in-r-missing-values-in-resampled-performance-measures</guid>
      <pubDate>Tue, 04 Jun 2024 21:24:40 GMT</pubDate>
    </item>
    <item>
      <title>Julia MLJ Forest Load：错误：MethodError：没有与 BetaML.Bmlj.RandomForestRegressor() 匹配的方法</title>
      <link>https://stackoverflow.com/questions/78577415/julia-mlj-forest-loaderror-methoderror-no-method-matching-betaml-bmlj-randomf</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78577415/julia-mlj-forest-loaderror-methoderror-no-method-matching-betaml-bmlj-randomf</guid>
      <pubDate>Tue, 04 Jun 2024 19:40:11 GMT</pubDate>
    </item>
    <item>
      <title>如何枚举 ML.NET 训练管道中的所有训练师？</title>
      <link>https://stackoverflow.com/questions/78576996/how-to-enumerate-over-all-trainers-in-an-ml-net-training-pipeline</link>
      <description><![CDATA[是否可以从 ML.NET 应用程序中的特定训练器类别中获取所有训练器并对它们进行迭代？例如，使类似这样的工作（没有）。
var allTrainers = new object[]
{
mlContext.MulticlassClassification.Trainers.SdcaNonCalibrated(),
mlContext.MulticlassClassification.Trainers.LbfgsMaximumEntropy()
};

foreach (var trainer in allTrainers)
{
var trainingPipeline = dataProcessPipeline
.Append(trainer)
.Append(mlContext.Transforms.Conversion.MapKeyToValue(&quot;PredictedLabel&quot;));
}

它们似乎没有使用可以放入列表并分配给 Append() 的通用基类型。]]></description>
      <guid>https://stackoverflow.com/questions/78576996/how-to-enumerate-over-all-trainers-in-an-ml-net-training-pipeline</guid>
      <pubDate>Tue, 04 Jun 2024 17:52:31 GMT</pubDate>
    </item>
    <item>
      <title>为什么MobileNetv2微调时Conv中的数组形状不匹配？</title>
      <link>https://stackoverflow.com/questions/78576851/why-the-shape-of-array-in-conv-when-fine-tuning-mobilenetv2-doesnt-match</link>
      <description><![CDATA[我正在尝试从检查点（https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet，请参阅“Mobilenet V2 Imagenet 检查点”下的“float_v2_1.4_224”）对 MobileNetv2 模型进行微调，以完成使用 Deeplabv3 进行图像分割的任务。我按照 deeplab repo 中描述的方法，将 pascal VOC 数据集替换为我自己的数据集，该数据集由大小为 224x224 的图像组成（与用于训练该检查点的图像大小相同），然后使用此脚本作为参考，以确定要设置哪些标志来训练 mobilenetv2。（https://github.com/tensorflow/models/blob/master/research/deeplab/local_test_mobilenetv2.sh）
这是我正在使用的命令：
 python deeplab/train.py
--logtostderr 
--train_split=&quot;train&quot; 
--model_variant=&quot;mobilenet_v2&quot;
--output_stride=16
--train_crop_size=&quot;225,225&quot; #图像为 224x224，因此根据 tensorflow/research/deeplab 常见问题解答将其设置为此尺寸
--train_batch_size=4 
--training_number_of_steps=100
--fine_tune_batch_norm=true
--dataset=&quot;custom&quot; #正确定义为其自己的数据集
--tf_initial_checkpoint=&quot;./deeplab/data_resized/initial_checkpoint/mobilenet_v2_1.4_224.ckpt&quot;
--train_logdir=&quot;./deeplab/data_resized/checkpoint&quot;
--dataset_dir=&quot;./deeplab/data_resized/ltfrecord&quot;

但我一直收到以下错误：
 回溯（最近一次调用）：
文件 &quot;deeplab/train.py&quot;，第 464 行，位于 &lt;module&gt;
tf.app.run()
文件“C:\Users\Luca\.pyenv\pyenv-win\versions\3.7.6\lib\site-packages\tensorflow_core\python\platform\app.py”，第 40 行，运行中
_run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
文件“C:\Users\Luca\.pyenv\pyenv-win\versions\3.7.6\lib\site-packages\absl\app.py”，第 308 行，运行中
_run_main(main, args)
文件“C:\Users\Luca\.pyenv\pyenv-win\versions\3.7.6\lib\site-packages\absl\app.py”，第 254 行，运行中
sys.exit(main(argv))
文件 &quot;deeplab/train.py&quot;，第 444 行，在 main
ignore_missing_vars=True)
文件 &quot;C:\Users\Luca\Documents\marathon\facade_ml\training_gitclone\models-master\research\deeplab\utils\train_utils.py&quot;，第 221 行，在 get_model_init_fn
ignore_missing_vars=ignore_missing_vars)
文件 &quot;C:\Users\Luca\.pyenv\pyenv-win\versions\3.7.6\lib\site-packages\tensorflow_core\contrib\framework\python\ops\variables.py&quot;，第 690 行，在assign_from_checkpoint
(ckpt_name, str(ckpt_value.shape), str(var.get_shape())))
ValueError: Total对于 MobilenetV2/Conv/weights，新数组的大小必须保持不变 lh_shape: [(3, 3, 3, 48)], rh_shape: [(3, 3, 3, 32)]

我尝试将其他配置值（例如将“initialize_last_layer”更改为 false 并将“last_layers_contain_logits_only”更改为 false）更改为 false，但这没有帮助，而且 Google 和 GitHub 上似乎都不存在有关此问题的信息。一位用户建议将“depth_multiplier”标志更改为 0.5，但除了将 rh 形状更改为 [3,3,3,16] 之外，它没有任何效果。
感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78576851/why-the-shape-of-array-in-conv-when-fine-tuning-mobilenetv2-doesnt-match</guid>
      <pubDate>Tue, 04 Jun 2024 17:13:08 GMT</pubDate>
    </item>
    <item>
      <title>skopt.gbrt_minimize（梯度提升回归树）——如果损失函数不可微，如何计算其梯度？</title>
      <link>https://stackoverflow.com/questions/78576495/skopt-gbrt-minimize-gradient-boosted-regression-trees-how-does-it-calculate</link>
      <description><![CDATA[我正在为 Jansen &amp; Rit 全脑模型优化 11 个参数，如下所示：
search_space = [
Integer(1, 10, name=&quot;A&quot;), Integer(10, 40, name=&quot;B&quot;), Integer(90, 200, name=&quot;C&quot;), 等...
]
gbrt_minimize(find_bold_loss, search_space, n_calls=80, n_initial_points=32, initial_point_generator=&#39;sobol&#39;)。
我不明白的是，我的损失函数非常复杂且不可微，因为它首先需要运行模型（使用欧拉方法并在每次迭代中引入高斯噪声）。那么，如果 gbrt 无法计算每次迭代的损失梯度，它如何工作？我对 skopt 和 GBRT 还不熟悉，所以如果能提供一些深入的解释就更好了！
注意：我已经用 gbrt 运行了模型，效果很好（比贝叶斯优化更好），所以它一定做对了什么……]]></description>
      <guid>https://stackoverflow.com/questions/78576495/skopt-gbrt-minimize-gradient-boosted-regression-trees-how-does-it-calculate</guid>
      <pubDate>Tue, 04 Jun 2024 15:50:01 GMT</pubDate>
    </item>
    <item>
      <title>'InvalidArgumentError' 的解决方案是什么</title>
      <link>https://stackoverflow.com/questions/78576371/what-is-the-solution-to-an-invalidargumenterror</link>
      <description><![CDATA[尝试在扩散模型上训练我的数据以生成新数据时，最后一个用于训练的单元输出此错误。请问解决方案是什么？
在预测和运行噪声和去噪后，现在训练 UNet。尽管为 train 创建了一个函数
def train(R=50):
bar = trange(R)
total = 100
for i in bar:
for j in range(total):
x_img = X_train[np.random.randint(len(X_train), size=BATCH_SIZE).tolist()]
loss = train_one(x_img)
pg = (j / total) * 100
if j % 5 == 0:
bar.set_description(f&#39;loss: {loss:.5f}, p: {pg:.2f}%&#39;)

上传的图像是出现错误的单元格，具体来说是调用 train 函数的那一行]]></description>
      <guid>https://stackoverflow.com/questions/78576371/what-is-the-solution-to-an-invalidargumenterror</guid>
      <pubDate>Tue, 04 Jun 2024 15:27:10 GMT</pubDate>
    </item>
    <item>
      <title>如果我的模型检测到多个物体（有时是假的），我该如何使用 mAP、精度、召回率和 f1 验证我的 mask rcnn 模型</title>
      <link>https://stackoverflow.com/questions/78576156/how-do-i-validate-my-mask-rcnn-model-with-map-precision-recall-and-f1-if-my-mo</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78576156/how-do-i-validate-my-mask-rcnn-model-with-map-precision-recall-and-f1-if-my-mo</guid>
      <pubDate>Tue, 04 Jun 2024 14:49:04 GMT</pubDate>
    </item>
    <item>
      <title>PySpark 错误要求失败：A 和 B 维度不匹配</title>
      <link>https://stackoverflow.com/questions/78575590/pyspark-error-requirement-failed-a-b-dimension-mismatch</link>
      <description><![CDATA[我正在使用 PySpark 中的房屋贷款还款数据构建神经网络。执行了 EDA、数据预处理和特征工程步骤，但当涉及到模型时，我收到错误：

java.lang.IllegalArgumentException：要求失败：A 和 B 维度不匹配！

代码如下：
# 要转换为数字的列的列表 -&gt;解决不支持字符串的错误
# 将列转换为双精度
columns_to_cast = [&#39;AMT_REQ_CREDIT_BUREAU_HOUR&#39;, &#39;AMT_REQ_CREDIT_BUREAU_DAY&#39;, &#39;AMT_REQ_CREDIT_BUREAU_WEEK&#39;,
&#39;AMT_REQ_CREDIT_BUREAU_MON&#39;, &#39;AMT_REQ_CREDIT_BUREAU_QRT&#39;, &#39;AMT_REQ_CREDIT_BUREAU_YEAR&#39;]

for col_name in columns_to_cast:
train_df = train_df.withColumn(col_name, col(col_name).cast(&quot;double&quot;))
test_df = test_df.withColumn(col_name, col(col_name).cast(&quot;double&quot;))

print(&quot;转换完成。正在检查schema:&quot;)
train_df.printSchema()

# 组装特征向量
ohe_columns = [col + &#39;_ohe&#39; for col in categorical_columns]
numerical_columns = [col for col in train_df.columns if col not in ohe_columns + [&#39;SK_ID_CURR&#39;, &#39;TARGET&#39;]]

print(f&quot;Numerical columns: {numerical_columns}&quot;)
print(f&quot;One-hotcoded columns: {ohe_columns}&quot;)

# 删除现有的 &#39;scaled_features&#39; 列（如果存在）-&gt; 重新运行代码后导致
# 我们需要运行两次才能玩
# 稍后可能会删除。
如果 train_df.columns 中有 &#39;features&#39;:
train_df = train_df.drop(&#39;features&#39;)
否则:
通过
如果 test_df.columns 中有 &#39;features&#39;:
test_df = test_df.drop(&#39;features&#39;)
否则:
通过
如果 train_df.columns 中有 &#39;scaled_features&#39;:
train_df = train_df.drop(&#39;scaled_features&#39;)
否则:
通过
如果 test_df.columns 中有 &#39;scaled_features&#39;:
test_df = test_df.drop(&#39;scaled_features&#39;)
否则:
通过

assembler = VectorAssembler(inputCols=numerical_columns + ohe_columns, outputCol=&quot;features&quot;)
train_df = assembler.transform(train_df)
test_df = assembler.transform(test_df)
print(&quot;特征向量组装完成。&quot;)

print(&quot;检查组装的特征向量模式：&quot;)
train_df.select(&quot;features&quot;).show(5, truncate=False)

# 缩放特征
scaler = StandardScaler(inputCol=&quot;features&quot;, outputCol=&quot;scaled_features&quot;)
scaler_model = scaler.fit(train_df)
train_df = scaler_model.transform(train_df)
test_df = scaler_model.transform(test_df)
print(&quot;缩放完成。&quot;)

print(&quot;检查缩放的特征模式：&quot;)
train_df.select(&quot;scaled_features&quot;).show(5, truncate=False)

# 定义神经网络结构

层 = [
173, # 输入特征的数量 -&gt;从上面的语句中检查
64, # 隐藏层大小
32, # 隐藏层大小
2 # 类数
]

print(f&quot;神经网络层：{layers}&quot;)

# 初始化多层感知器分类器
mlp = MultilayerPerceptronClassifier(
featuresCol=&#39;scaled_features&#39;,
labelCol=&#39;TARGET&#39;,
maxIter=100,
layer=layers,
blockSize=128,
seed=1234
)

print(&quot;训练模型...&quot;)
# 训练模型
mlp_model = mlp.fit(train_df)
print(&quot;模型训练完成。&quot;)

print(&quot;对训练集进行预测...&quot;)
# 对训练集进行预测（用于评估目的）
train_predictions = mlp_model.transform(train_df)

print(&quot;评估模型...&quot;)
# 评估模型
evaluator = BinaryClassificationEvaluator(labelCol=&#39;TARGET&#39;, rawPredictionCol=&#39;rawPrediction&#39;, metricName=&#39;areaUnderROC&#39;)
auc_train = evaluator.evaluate(train_predictions)
print(f&#39;Training AUC: {auc_train}&#39;)

print(&quot;对测试集进行预测...&quot;)
# 对测试集进行预测 --------此处错误 
test_predictions = mlp_model.transform(test_df)

# 显示预测
test_predictions.select(&#39;SK_ID_CURR&#39;, &#39;prediction&#39;, &#39;probability&#39;).show()

print(&quot;准备提交文件...&quot;)
# 准备提交文件
submission = test_predictions.select(&#39;SK_ID_CURR&#39;, &#39;prediction&#39;)
submission.show()

# 将预测保存到 CSV 文件
submission.write.csv(&#39;./prediction&#39;, header=True)
print(&quot;Prediction file saved.&quot;)

请注意，我添加了打印语句来帮助我识别错误发生的位置，并且看起来，错误是在模型尝试对测试数据框进行预测时产生的，而它在训练数据框上工作
（错误就在这里）
这是什么意思？]]></description>
      <guid>https://stackoverflow.com/questions/78575590/pyspark-error-requirement-failed-a-b-dimension-mismatch</guid>
      <pubDate>Tue, 04 Jun 2024 13:07:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用随机森林计算预测的置信区间？</title>
      <link>https://stackoverflow.com/questions/78572538/how-can-i-calculate-confidence-interval-for-forecast-using-random-forest</link>
      <description><![CDATA[我正在计算名为 &quot;spot&quot; 的变量的预测（数据的未来结果）。我正在使用随机森林和名为 &quot;DTCI&quot; 的独立变量来协助预测 &quot;spot&quot;。
预测按月进行，与数据频率相同。我想根据每个月的上限和下限获得每个预测月份的置信区间。它与附图中所做的类似，带有绿色限制。

我尝试使用 GradientBoostingRegressor 构建间隔，如下所示：
# 设置下分位数和上分位数
inf = 0.1
sup = 0.9

# 每个模型必须独立
lower_model = GradientBoostingRegressor(loss=&quot;quantile&quot;, alpha=inf)
upper_model = GradientBoostingRegressor(loss=&quot;quantile&quot;, alpha=sup)

lower_model.fit(X_train, y_train)
upper_model.fit(X_train, y_train)

predictions = pd.DataFrame(y_hat_forecast_spot)

predictions[&quot;inf&quot;] = lower_model.predict(X_fore)
predictions[&quot;sup&quot;] = upper_model.predict(X_fore)

然而，结果并没有我预期的趋势。由于它是一个时间序列，我想象（如上图所示）限值应该以置信区域变大的方式增长。换句话说，日期越远，预测就越困难，因此与之相关的误差或间隔就越大。
我使用 GradientBoostingRegressor 得到的结果（如下所示）的间隔会随时间变化而不是增长。

GradientBoostingRegressor 适合时间序列吗？或者有其他函数可以更好地理解时间序列吗？]]></description>
      <guid>https://stackoverflow.com/questions/78572538/how-can-i-calculate-confidence-interval-for-forecast-using-random-forest</guid>
      <pubDate>Mon, 03 Jun 2024 21:17:18 GMT</pubDate>
    </item>
    <item>
      <title>处理图像以提取神经网络的手写文本</title>
      <link>https://stackoverflow.com/questions/78571522/process-the-image-to-extract-handwritten-text-for-the-neural-network</link>
      <description><![CDATA[我想识别图像中的方块并提取其中的字母，但我无法获得整个字母。我只得到像下面显示的图像中的片段。
img = cv2.imread(img_path)
img_path = &quot;img.png&quot;

gray = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY)

# 平滑边缘以方便锐化
blur = cv2.GaussianBlur(gray, (5, 5), 0)
# sharpen_kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
sharpen_kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
sharpen = cv2.filter2D(blur, -1, sharpen_kernel)

# 应用自适应阈值创建二值图像并缩小间隙
thresh = cv2.adaptiveThreshold(sharpen, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV, 11, 2)
kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
close = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=2)

# 检测轮廓并根据面积大小进行过滤
contours = cv2.findContours(close, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)
contours = contours[0] if len(contours) == 2 else contours[1]
min_area = 350
max_area = 6200
image_number = 0
lower_blue = np.array([90, 50, 20]) 
upper_blue = np.array([150, 255, 255]) 
# 避免检测到的小/大随机轮廓
contours = filter(lambda contour: min_area &lt; cv2.contourArea(contour) &lt; max_area, contours)
# 对轮廓进行排序，使其变成“线”
sorted_contours = sorted(contours, key=lambda contour: cv2.boundingRect(contour)[0])
selected_rects = []
for c in sorted_contours:
x, y, w, h = cv2.boundingRect(c)
# 确保矩形不与已选定的矩形重叠
if not any(x &gt; rect[0] and y &gt; rect[1] and x + w &lt; rect[0] + rect[2] and y + h &lt; rect[1] + rect[3] for rect in selected_rects):
ROI = image[y:y + h, x:x + w]
ROI = cv2.GaussianBlur(ROI, (3, 3), 0)
hsv = cv2.cvtColor(ROI, cv2.COLOR_BGR2HSV)
mask = cv2.inRange(hsv, lower_blue, upper_blue)
blue_pixels = cv2.countNonZero(mask)
# 确保矩形内有蓝色，以避免出现随机矩形
if blue_pixels &gt; 100：
binary_image = cv2.resize(mask, (28, 28))
cv2.imwrite(f&#39;./resources/ROI_{image_number}.png&#39;, binary_image)
image_number += 1
selected_rects.append((x, y, w, h))

初始图像

一些提取的 ROI

不完整的蒙版

我尝试移除灰色边框，然后将所有非白色的像素转换为亮蓝色。但是，我无法在每种情况下都移除所有边框，而且有时由于图像是扫描的，所以会出现随机的黄色斑点。]]></description>
      <guid>https://stackoverflow.com/questions/78571522/process-the-image-to-extract-handwritten-text-for-the-neural-network</guid>
      <pubDate>Mon, 03 Jun 2024 16:27:31 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习方法选择加权平均的权重集</title>
      <link>https://stackoverflow.com/questions/78551292/using-machine-learning-approach-to-select-weight-sets-for-weighted-average</link>
      <description><![CDATA[我有多个风速预测，我试图为每个预测分配不同的权重，以获得最佳的“混合”预测，从而获得最佳的预测性能。假设每个风速预测在温度和湿度等外部条件下的表现不同。
我知道这可以建模为优化问题，但我很好奇这是否可以通过机器学习来解决，因为 NN 基本上是在解决优化问题，并且由权重和偏差组成。
因此，给定预测，并且可能给定外部条件（例如温度），模型将返回最佳权重选择。
你们觉得呢？]]></description>
      <guid>https://stackoverflow.com/questions/78551292/using-machine-learning-approach-to-select-weight-sets-for-weighted-average</guid>
      <pubDate>Wed, 29 May 2024 18:08:08 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：'Flags' 对象没有属性 'c_contiguous'</title>
      <link>https://stackoverflow.com/questions/77615883/attributeerror-flags-object-has-no-attribute-c-contiguous</link>
      <description><![CDATA[我正在按照 Aurélien Géron 的《机器学习入门》一书进行操作，并遇到了以下错误。
代码：
y_train_large = (y_train.astype(&quot;int&quot;) &gt;= 7)
y_train_odd = (y_train.astype(&quot;int&quot;) % 2 == 1)
y_multilabel = np.c_[y_train_large, y_train_odd]

#model
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_multilabel)

y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)

最后一行产生以下错误：
{
AttributeError: &#39;Flags&#39;对象没有属性“c_contiguous”
}

由于我按照书上的方法操作，我预计这段代码可以正常工作。我尝试过 Google Bard 和 Claude AI 聊天机器人的解决方案，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77615883/attributeerror-flags-object-has-no-attribute-c-contiguous</guid>
      <pubDate>Wed, 06 Dec 2023 19:42:47 GMT</pubDate>
    </item>
    <item>
      <title>优化欺诈检测不平衡数据的指标</title>
      <link>https://stackoverflow.com/questions/77444565/optimize-metrics-for-fraud-detection-imbalanced-data</link>
      <description><![CDATA[我需要您的帮助来提高我的模型性能。就像大多数欺诈检测一样，我有一个不平衡的数据集（0.1/0.9）。我想优化目标 1 和 0 的召回率，因为一方面我想避免欺诈检测，另一方面我想限制将非欺诈客户定位为欺诈的成本，因为 5% 的错误分类会使我的收入减少 3000 欧元（而定位正确的欺诈者会让我为每个检测到的客户节省 1000 欧元的损失）。
我的第一个问题是：您会根据这个问题考虑哪些指标？我更关注召回率，但我会阅读您的意见。
第二个问题：我如何提高模型性能？
到目前为止，我在不降低阈值的情况下获得的最佳结果是：
准确率：0.89
混淆矩阵：
[[3153 279]
[ 145 297]]
分类报告：
准确率 召回率 f1 分数 支持
 0 0.96 0.92 0.94 3432
1 0.52 0.67 0.58 442

准确率 0.89 3874

而如果我降低阈值以增加目标 1 的召回率：
准确率：0.61
混淆矩阵：
[[1959 1473]
[ 42 400]]
分类报告：
准确率 召回率 f1 分数 支持率
 0 0.98 0.57 0.72 3432
1 0.21 0.90 0.35 442

准确率 0.61 3874

我尝试了几种模型：
线性回归、XGBoost、随机森林和 SVM
此外，甚至过采样/反采样技术（仅在训练集上）
RandomOverSampling、RandomUnderSampling、SMOTE
您还有其他建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77444565/optimize-metrics-for-fraud-detection-imbalanced-data</guid>
      <pubDate>Wed, 08 Nov 2023 10:08:43 GMT</pubDate>
    </item>
    <item>
      <title>用于欺诈检测的特征工程</title>
      <link>https://stackoverflow.com/questions/50330778/feature-engineering-for-fraud-detection</link>
      <description><![CDATA[我正在为学术目的进行一些欺诈检测研究。
我想具体了解从交易数据集中进行特征选择\工程化的技术。
更详细地说，给定一个交易数据集（例如信用卡），选择什么样的特征用于模型以及如何设计它们？
我遇到的所有论文都关注模型本身（SVM、NN 等），并没有真正涉及这个主题。
此外，如果有人知道未匿名的公共数据集 - 这也会有所帮助。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/50330778/feature-engineering-for-fraud-detection</guid>
      <pubDate>Mon, 14 May 2018 12:50:06 GMT</pubDate>
    </item>
    </channel>
</rss>