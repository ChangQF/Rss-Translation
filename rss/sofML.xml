<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 13 Dec 2023 15:14:37 GMT</lastBuildDate>
    <item>
      <title>Llama2 回归语言模型 (huggingface)</title>
      <link>https://stackoverflow.com/questions/77654285/llama2-language-model-for-regression-huggingface</link>
      <description><![CDATA[我尝试利用给定整个输入序列的模型的最后一个隐藏状态，调整 Llama2 来解决回归任务。
如果随后提出问题“2+2 的答案是什么”，则应回答 4（虚拟问题，用于解释问题）。&lt; /p&gt;
为此，我将在 pytorch 模型中使用它
导入火炬
将 torch.nn 导入为 nn
从 Transformer 导入 LlamaModel、LlamaTokenizer

类 TransformerModel(nn.Module):
    def __init__(self, 模型名称:str, 附加层大小:int = 1):
        super(TransformerModel, self).__init__()
        self.transformer = LlamaModel.from_pretrained(model_name, torch_dtype=torch.float32, cache_dir=“hugginface_cache/models”)
        self.tokenizer = LlamaTokenizer.from_pretrained(model_name,cache_dir=“hugginface_cache/tokenizer”)

        # 添加一个带有一个输出的附加层
        self.additional_layer = nn.Linear(self.transformer.config.hidden_​​size,additional_layer_size)
        
    defforward(self, input_text):
        # 对输入文本进行标记
        input_ids = self.tokenizer(input_text, return_tensors=“pt”).input_ids.to(“cuda”)
        打印（“输入ID：”，输入ID）

        # 获取变压器的输出
        输出 = self.transformer(input_ids)
        
        # 使用整个最后的隐藏状态作为附加层的输入
        最后隐藏状态 = 输出.最后隐藏状态
        打印（&#39;last_hidden_​​state_shape：&#39;，last_hidden_​​state.size（））

        # 应用附加层
        附加输出= self.附加层（最后隐藏状态）

        返回额外的输出


model_url = “meta-llama/Llama-2-7b-hf”

模型 = TransformerModel(model_url)

但是，对于给定的输入模型（“Hello world！”），输出是大小为 1,4,1 的张量。
我可以验证标记生成器是否将字符串拆分为 4 个标记，我预计这会导致问题。但是，我不确定如何解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77654285/llama2-language-model-for-regression-huggingface</guid>
      <pubDate>Wed, 13 Dec 2023 14:22:48 GMT</pubDate>
    </item>
    <item>
      <title>像 Google Photo-Image 这样的图像搜索被赋予模型[关闭]</title>
      <link>https://stackoverflow.com/questions/77653804/image-search-like-google-photo-image-with-face-is-given-to-model</link>
      <description><![CDATA[当用户上传自拍照时，模型会在多人图像数据集中搜索同一个人，并返回包含该人的所有图像。
第 1 步：从图像数据集中检测所有人脸，并通过 DeepFace(RetinaFace) 和 OpenCV 保存裁剪后的人脸。
第2步：通过OpenCV读取所有裁剪后的面，调整其大小，存储到numpy数组中并标准化。
第 3 步：将其输入 VGG16(Keras) 模型，不包括具有 0 个可训练参数的顶层（使用权重），并从图像 numpy 数组中提取特征。
第4步：将这些提取的特征输入聚类模型（无监督）
我尝试了 KMeans 和凝聚层次聚类，但没有达到预期效果。
聚类根本不准确，更不准确。
任何人都可以帮助我解决这个问题，如何完成这项任务，我在哪一步犯了错误......
图像/数据未标记，想要构建无监督的聚类模型。
这里有 C# 的开源框架吗？]]></description>
      <guid>https://stackoverflow.com/questions/77653804/image-search-like-google-photo-image-with-face-is-given-to-model</guid>
      <pubDate>Wed, 13 Dec 2023 13:04:01 GMT</pubDate>
    </item>
    <item>
      <title>我应该在 Adam 优化器的顶部应用余弦退火调度程序吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77653513/should-i-apply-a-cosine-annealing-scheduler-on-the-top-of-the-adam-optimizer</link>
      <description><![CDATA[Adam 优化器以其自适应调整学习率的能力而闻名，那么余弦退火调度器还有用吗？
我还没有完成实验，部分原因是我没有干净的余弦退火代码示例。特别感谢那些可以提供代码片段的人。]]></description>
      <guid>https://stackoverflow.com/questions/77653513/should-i-apply-a-cosine-annealing-scheduler-on-the-top-of-the-adam-optimizer</guid>
      <pubDate>Wed, 13 Dec 2023 12:16:11 GMT</pubDate>
    </item>
    <item>
      <title>执行与自然语言处理相关的代码时出错[关闭]</title>
      <link>https://stackoverflow.com/questions/77653463/error-in-execution-of-code-related-to-natural-language-processing</link>
      <description><![CDATA[此代码显示了图像中给出的错误。我无法理解其中的原因。

我不知道如何在Python中使用导入命令。我尝试了所有可能的方法进行检查，包括删除带有其他新闻门户名称的“路透社”，但没有任何效果。现在，如果有人帮助我正确编写代码的“导入”部分，那就更好了。我认为其他部分没问题，因为没有显示其他消息。]]></description>
      <guid>https://stackoverflow.com/questions/77653463/error-in-execution-of-code-related-to-natural-language-processing</guid>
      <pubDate>Wed, 13 Dec 2023 12:07:23 GMT</pubDate>
    </item>
    <item>
      <title>朴素贝叶斯分类器需要估计多少个参数？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77652867/how-many-parameters-need-to-be-estimated-for-na%c3%afve-bayes-classifier</link>
      <description><![CDATA[这是来自 GATE 数据科学与人工智能 (DA) 样本论文的 MCQ 问题：
Q.28
&lt;块引用&gt;
给定一个包含𝑁点的𝐾类数据集，其中描述了样本点
使用 𝐷 离散特征，每个特征都能够取 𝑉 值，如何
朴素贝叶斯分类器需要估计很多参数？
(A) (𝑉^D)𝐾 
(B) (𝐾^V)^D 
(C) 𝑉𝐷𝐾 + K 
(D) 𝐾(𝑉 + 𝐷)

根据chatgpt正确答案是(A)：
总参数=类的数量×(每个特征的值的数量)^特征的数量+类的数量
而 Bard 通过取 K、V 和 D 的一些值给出了正确答案 (C)。
我想知道这是否正确，如果可能的话，进一步阅读有关该主题的内容会很好。]]></description>
      <guid>https://stackoverflow.com/questions/77652867/how-many-parameters-need-to-be-estimated-for-na%c3%afve-bayes-classifier</guid>
      <pubDate>Wed, 13 Dec 2023 10:38:01 GMT</pubDate>
    </item>
    <item>
      <title>用转换层替换平均池层</title>
      <link>https://stackoverflow.com/questions/77652246/replacing-a-avg-pool-layer-with-a-conv-layer</link>
      <description><![CDATA[我有一个内核大小为 (4,3) 的平均池层，其步长为 (4,3) 和 0 填充。我想将其转换为等效的转换层，以便在我的神经网络中实现。
所以，在我的模型中，我替换了这个：
self.pool = nn.AvgPool2d((4, 3))

使用这一行（32 是输入通道数）：
self.pool = nn.Conv2d(32, 32, (4, 3), stride=(4, 3), 偏差=False)

对我来说，这两个层似乎应该是等效的，并且这两个层的输出大小都是 (1, 32, 32, 13) （它有 32 个通道）；对于大小为 (1, 32, 129, 40) 的输入（32 个通道，X 维度 129，Y 维度 40）。
我不确定我对池化层和转换层的基础知识是否有错误的理解。有人可以帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77652246/replacing-a-avg-pool-layer-with-a-conv-layer</guid>
      <pubDate>Wed, 13 Dec 2023 09:06:48 GMT</pubDate>
    </item>
    <item>
      <title>SEEM 模型在向其传递图像及其推理时面临的问题</title>
      <link>https://stackoverflow.com/questions/77651185/facing-issues-with-seem-model-in-passing-images-to-it-and-its-inferencing</link>
      <description><![CDATA[我正在研究 SEEM 模型，这是一种可推广的交互式模型，用于一次性分割图像中任何地方的所有内容。由于资源有限，我在将图像传递给它及其推理时面临问题。我已经安装了所有依赖项并单独加载了 SEEM 模型。有谁对此有任何想法并相应地指导我。
我期待有人能够根据他们的知识指导我，我可以尽快解决这个问题]]></description>
      <guid>https://stackoverflow.com/questions/77651185/facing-issues-with-seem-model-in-passing-images-to-it-and-its-inferencing</guid>
      <pubDate>Wed, 13 Dec 2023 04:53:29 GMT</pubDate>
    </item>
    <item>
      <title>尝试将自定义模型部署到终端节点时从 AWS Sagemaker 获取（超时）ModelError</title>
      <link>https://stackoverflow.com/questions/77649384/getting-timed-out-modelerror-from-aws-sagemaker-when-trying-to-deploy-a-custom</link>
      <description><![CDATA[我目前在使用 PyTorchModel 和我的自定义模型在 AWS SageMaker 上部署终端节点时遇到问题。尽管遵循 Amazon 官方网站（所有代码均可在一个 Github 上找到），我遇到以下错误：
&lt;块引用&gt;
sagemaker 调用 InvokeEndpoint 操作时发生错误 (ModelError)：从主容器收到服务器错误 (0)，并显示消息您的调用在等待容器主容器的响应时超时。检查 Amazon CloudWatch 中每个容器的延迟指标，解决问题，然后重试。

我尝试通过查看 Amazon CloudWatch 中的日志来解决该问题，但无法找到解决方案。我还将 SAGEMAKER_MODEL_SERVER_TIMEOUT 更改为 180，但它不起作用。请让我知道如何修复它。我已经尝试过 AWS 网站上的其他示例和文档，但没有一个对我有用。]]></description>
      <guid>https://stackoverflow.com/questions/77649384/getting-timed-out-modelerror-from-aws-sagemaker-when-trying-to-deploy-a-custom</guid>
      <pubDate>Tue, 12 Dec 2023 22:37:01 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 openai 和 langchain 将已创建的 chromadb 集合与法学硕士一起使用？</title>
      <link>https://stackoverflow.com/questions/77642444/how-can-you-use-an-already-created-chromadb-collection-with-a-llm-using-openai-a</link>
      <description><![CDATA[我已经使用其文档和元数据创建了 chromadb 集合。
问题是当我想使用 langchain 创建 llm 并传递此 chromadb 集合以用作知识库时。
langchain_chroma = 色度(
客户端=持久客户端，
集合名称=集合.名称,
embedding_function = openai_ef，
）

llm_model =“gtp35turbo-最新”

llm = AzureChatOpenAI(
   api_key=openai_api_key,
   api_version=openai_api_version,
   azure_endpoint=openai_api_base,
   模型=llm_模型）

qa_chain = RetrievalQA.from_chain_type(
   嗯，
   检索器=langchain_chroma.as_retriever(),
   chain_type=&quot;精炼&quot;
）

当我想跑步时：
qa_chain.run(“对象检测问题需要多少数据科学家”)

我收到此错误：
AttributeError Traceback（最近一次调用最后一次）
&lt;ipython-input-81-3cdb65aeb43e&gt;在&lt;细胞系：1&gt;()
----&gt; 1 qa.run(“对象检测问题需要多少数据科学家”)

9帧
/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py 中相似性_search_with_score(self, query, k, filter, where_document, **kwargs)
    第430章）
    第431章：
--&gt;第432章
    第433章
    第434章

AttributeError：“OpenAIEmbeddingFunction”对象没有属性“embed_query”

如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/77642444/how-can-you-use-an-already-created-chromadb-collection-with-a-llm-using-openai-a</guid>
      <pubDate>Mon, 11 Dec 2023 21:17:23 GMT</pubDate>
    </item>
    <item>
      <title>调整图像分割模型（来自 TF 教程）以进行二元掩蔽</title>
      <link>https://stackoverflow.com/questions/77635064/adjust-image-segmentaion-model-from-tf-tutorial-for-binary-masking</link>
      <description><![CDATA[我需要 Tensorflow 的图像分割模型。输入为图像和掩码（二进制、掩码或非掩码），输出为带有 0 和 1 的图像掩码。
我遵循了 https://www.tensorflow.org/tutorials/ 中的图像分割教程图像/分割
但现在我想在我的数据集上运行它的二进制掩码（没有边框类）
新数据集已准备好并输入到 model.fit 中。应该没问题吧。
如何将此模型更改为只有 2 个类（非屏蔽和屏蔽）？
base_model: keras.Model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)

# 使用这些层的激活
图层名称 = [
    &#39;block_1_expand_relu&#39;, # 64x64
    &#39;block_3_expand_relu&#39;, # 32x32
    &#39;block_6_expand_relu&#39;, # 16x16
    &#39;block_13_expand_relu&#39;, # 8x8
    &#39;block_16_project&#39;, # 4x4
]
base_model_outputs = [base_model.get_layer(name).layer_names 中名称的输出]

# 创建特征提取模型
down_stack = 模型（输入=base_model.输入，输出=base_model_outputs）

down_stack.trainable = False

上层堆栈 = [
    pix2pix.upsample(512, 3), # 4x4 -&gt; 8x8
    pix2pix.upsample(256, 3), # 8x8 -&gt; 16x16
    pix2pix.upsample(128, 3), # 16x16 -&gt; 32x32
    pix2pix.upsample(64, 3), # 32x32 -&gt; 64x64
]

def unet_model(output_channels:int):
  输入 = 层.Input(形状=[128, 128, 3])

  # 通过模型进行下采样
  跳过= down_stack（输入）
  x = 跳过[-1]
  跳过 = 反转(跳过[:-1])

  # 上采样并建立跳跃连接
  对于 up，在 zip 中跳过（up_stack，skips）：
    x = 上(x)
    concat = 层.Concatenate()
    x = concat([x, 跳过])

  # 这是模型的最后一层
  最后=层.Conv2DTranspose(
      过滤器=output_channels，kernel_size=3，步长=2，
      padding=&#39;相同&#39;) #64x64 -&gt; 128x128

  x = 最后一个(x)

  返回模型（输入=输入，输出=x​​）

输出类 = 3

模型 = unet_model(output_channels=OUTPUT_CLASSES)

model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;binary_crossentropy&#39;,
              指标=[&#39;准确性&#39;])

当我将 OUTPUT_CLASSES 更改为 2 时，出现错误：
W tensorflow/core/kernels/data/generator_dataset_op.cc:108] 完成 GeneratorDataset 迭代器时发生错误：FAILED_PRECONDITION：Python 解释器状态未初始化。该过程可以被终止。

当OUTPUT_CLASSES为1时，预测掩码为空。
也许还必须改变其他东西？我还不熟悉神经网络架构，所以我可能看不到明显的东西。
编辑：
我已将activation=&#39;sigmoid&#39;添加到输出层
 最后 = tf.keras.layers.Conv2DTranspose(
      过滤器=output_channels，kernel_size=3，步长=2，
      填充 = &#39;相同&#39;, 激活 = &#39;sigmoid&#39;) #64x64 -&gt; 128x128

  x = 最后一个(x)

和OUTPUT_CLASSES = 1
奇怪的行为是下一个：
预期的掩模是当我在一个非常小的数据集上训练它时（该数据集中包含的测试的图片和掩模，只是为了测试它如何检测所看到的图像），我在第一个时期得到了一些东西。但纪元越多，结果越差。然而，准确度约为 0.99。
预期掩码：

预测掩码纪元 0：

如果打开图像，您可能会在预期的遮罩部分看到轻微的阴影。
预测掩码纪元1：

...
纪元 4：

所以每次迭代都会变得更糟。
数据集包含不应显示任何蒙版的图像。也许这就是问题所在？ （编辑：从数据集中排除没有掩码的数据 - 没有帮助）
编辑2：
x = tf.keras.layers.BatchNormalization()(x)

有帮助，虽然不完美，但是有所帮助]]></description>
      <guid>https://stackoverflow.com/questions/77635064/adjust-image-segmentaion-model-from-tf-tutorial-for-binary-masking</guid>
      <pubDate>Sun, 10 Dec 2023 13:55:04 GMT</pubDate>
    </item>
    <item>
      <title>minMax TicTacToe 代码返回错误的最佳可能移动</title>
      <link>https://stackoverflow.com/questions/77596870/minmax-tictactoe-code-returning-the-wrong-best-possible-move</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77596870/minmax-tictactoe-code-returning-the-wrong-best-possible-move</guid>
      <pubDate>Mon, 04 Dec 2023 02:07:13 GMT</pubDate>
    </item>
    <item>
      <title>ValidationError：StuffDocumentsChain __root__ 出现 1 个验证错误</title>
      <link>https://stackoverflow.com/questions/76776695/validationerror-1-validation-error-for-stuffdocumentschain-root</link>
      <description><![CDATA[我收到此错误ValidationError：在 llm_chain input_variables 中找不到 StuffDocumentsChain __root__ document_variable_name 上下文的 1 个验证错误：[&#39;chat_history&#39;、&#39;user_query&#39;、&#39;relevant_context&#39;] (type=value_error)
在使用 load_qa_chain 时，我搜索了此错误，但没有找到与此相关的任何内容。谁能告诉我这里缺少什么。
代码：
template = &quot;&quot;&quot;您是一个正在与人类对话的聊天机器人。

给定长文档和问题的以下提取部分，创建最终答案。

{相关上下文}

{聊天记录}
人类：{user_query}
聊天机器人：“”“”

提示=提示模板(
input_variables=[“chat_history”, “user_query”, “relevant_context”],
模板=模板
）

内存 = ConversationBufferMemory(memory_key=“chat_history”, input_key=“user_query”)

llm = OpenAI()
llm_chain = LLMChain(
    llm=llm,
    提示=提示，
    内存=内存，
）

链 = load_qa_chain(
    llm, chain_type=“东西”, 内存=内存, 提示=提示
）
]]></description>
      <guid>https://stackoverflow.com/questions/76776695/validationerror-1-validation-error-for-stuffdocumentschain-root</guid>
      <pubDate>Thu, 27 Jul 2023 05:20:25 GMT</pubDate>
    </item>
    <item>
      <title>Google Colab 免费套餐：使用自定义数据集微调 LLAMA 2 时，代码停止在 51,000 个示例</title>
      <link>https://stackoverflow.com/questions/76765564/google-colab-free-tier-code-stops-at-51-000-examples-while-fine-tuning-llama-2</link>
      <description><![CDATA[我在使用自定义数据集在 Google Colab 上微调 Llama 2 时遇到问题。在训练过程中，代码恰好在 51,000 个示例处停止，尽管我的数据集包含 61,609 个示例。奇怪的是，当我使用更大的数据集测试代码时，它运行得非常好。我按照 YouTube 上的教程对 Llama 2 进行了微调，您可以在下面找到原始 Colab 和教程链接：
教程链接：YouTube 教程
原始 Colab：Google Colab
数据集链接：我的自定义数据集
代码：
!pip install -q -U trl 变压器加速 git+https://github.com/huggingface/peft.git
!pip install -q 数据集 BitsandBytes einops wandb

从数据集导入load_dataset
从 Transformer 导入 AutoTokenizer、TrainingArguments
从 peft 导入 LoraConfig，get_peft_model
从 trl 导入 SFTTrainer

# 加载数据集
dataset_name = &#39;harpyerr/merged-pf&#39;
数据集 = load_dataset(dataset_name, split=&quot;train&quot;)

# 定义model_name、lora_alpha、lora_dropout、lora_r等配置
model_name = “your_pretrained_model_name” # 替换为你的预训练模型的名称
劳拉阿尔法 = 16
劳拉_dropout = 0.1
劳拉_r = 64

# 初始化分词器
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# 定义 LoraConfig
peft_config = LoraConfig（
    劳拉_阿尔法=劳拉_阿尔法,
    lora_dropout=lora_dropout,
    r=劳拉_r,
    偏差=“无”，
    task_type=“CAUSAL_LM”
）

# 定义训练参数
输出目录=“./结果”
每个设备训练批次大小 = 4
梯度累积步数 = 4
optim = “paged_adamw_32bit”
保存步数 = 100
记录步骤 = 10
学习率 = 2e-4
最大梯度范数 = 0.3
最大步数 = 100
预热比率 = 0.03
lr_scheduler_type = “常量”

训练参数 = 训练参数（
    输出目录=输出目录，
    per_device_train_batch_size = per_device_train_batch_size，
    梯度累积步数=梯度累积步数，
    优化=优化，
    保存步骤=保存步骤，
    日志记录步骤=日志记录步骤，
    学习率=学习率，
    fp16=正确，
    max_grad_norm=max_grad_norm,
    最大步数=最大步数，
    Warmup_ratio=warmup_ratio,
    group_by_length=真，
    lr_scheduler_type = lr_scheduler_type，
）

# 初始化 SFTTrainer
最大序列长度 = 512
训练师 = SFTTrainer(
    型号=型号，
    train_dataset=数据集，
    pft_config=peft_config,
    dataset_text_field=&quot;文本&quot;,
    max_seq_length = max_seq_length，
    分词器=分词器，
    args=训练参数，
）

# 将所有标准化层转换为 float32
进口火炬
对于名称，trainer.model.named_modules() 中的模块：
    如果“正常”名称：
        模块 = module.to(torch.float32)

# 开始训练
训练师.train()

我尝试使用较大尺寸的不同数据集来检查问题是否特定于我的自定义数据集。令人惊讶的是，当我使用其他更大的数据集时，代码运行得非常好，没有任何停止问题。因此，我推断问题不在于代码或训练器，而可能与我的自定义数据集的具体特征有关。]]></description>
      <guid>https://stackoverflow.com/questions/76765564/google-colab-free-tier-code-stops-at-51-000-examples-while-fine-tuning-llama-2</guid>
      <pubDate>Tue, 25 Jul 2023 18:25:00 GMT</pubDate>
    </item>
    <item>
      <title>HuggingFace AutoModelForCasualLM “仅解码器架构”警告，即使在设置 padding_side='left' 后也是如此</title>
      <link>https://stackoverflow.com/questions/74748116/huggingface-automodelforcasuallm-decoder-only-architecture-warning-even-after</link>
      <description><![CDATA[我正在使用
AutoModelForCausalLM 和 AutoTokenizer 使用 DialoGPT 生成文本输出。
无论出于何种原因，即使使用 Huggingface 提供的示例，我也会收到此警告：
&lt;块引用&gt;
正在使用仅解码器架构，但检测到右填充！为了正确的生成结果，请在初始化分词器时设置 padding_side=&#39;left&#39;。

从变压器导入 AutoModelForCausalLM, AutoTokenizer
进口火炬


tokenizer = AutoTokenizer.from_pretrained(“microsoft/DialoGPT-medium”)
模型 = AutoModelForCausalLM.from_pretrained(“microsoft/DialoGPT-medium”)

# 我们聊5行吧
对于范围（5）中的步骤：
    # 对新的用户输入进行编码，添加 eos_token 并在 Pytorch 中返回一个张量
    new_user_input_ids = tokenizer.encode(input(“&gt;&gt;用户:”) + tokenizer.eos_token, return_tensors=&#39;pt&#39;)

    # 将新的用户输入标记附加到聊天历史记录中
    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) 如果步骤 &gt; 0 其他 new_user_input_ids

    # 生成响应，同时将总聊天历史记录限制为 1000 个令牌，
    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)

    # 漂亮地打印机器人最后的输出令牌
    print(“DialoGPT: {}”.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0],skip_special_tokens=True)))

代码由 微软在 Huggingface 的模型卡上
我尝试将 padding_side=&#39;left&#39; 添加到标记生成器中，但这不会改变任何内容。
显然（从一些阅读来看）DialoGPT 想要在右侧填充？
我无法弄清楚这一点，当我尝试谷歌搜索时几乎没有结果。
我能够像这样抑制警告：
from Transformers.utils 导入日志记录

记录.set_verbosity_info()

但这似乎不是最好的答案？]]></description>
      <guid>https://stackoverflow.com/questions/74748116/huggingface-automodelforcasuallm-decoder-only-architecture-warning-even-after</guid>
      <pubDate>Fri, 09 Dec 2022 20:39:39 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI 健身房玩家模式</title>
      <link>https://stackoverflow.com/questions/46762083/openai-gym-player-mode</link>
      <description><![CDATA[有谁知道如何作为玩家运行 OpenAI 健身房环境之一。就像让人类玩家玩一轮车竿一样？我已经看到有 env.mode = &#39; human&#39; 但我无法让它正常运行。我尝试按照此处给出的示例 https://www.pinchofintelligence.com/getting -started-openai-gym/ 但它似乎对我不起作用。
如果您能提供任何帮助，我们将不胜感激。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/46762083/openai-gym-player-mode</guid>
      <pubDate>Mon, 16 Oct 2017 02:21:38 GMT</pubDate>
    </item>
    </channel>
</rss>