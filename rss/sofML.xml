<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 24 Jun 2024 18:19:47 GMT</lastBuildDate>
    <item>
      <title>如何通过 LSTM 预测未来价值？</title>
      <link>https://stackoverflow.com/questions/78664040/how-to-predict-the-future-value-thoriugh-lstm</link>
      <description><![CDATA[我如何预测不在数据框中的未来值。以下代码预测训练数据集和验证数据集上的值，但不预测未来值。我想通过查看过去 100 天的数据来预测未来 30 天的数据。我该怎么做呢？谢谢
df2 = df.reset_index()[&#39;High&#39;]
scaler = MinMaxScaler()
df2 = scaler.fit_transform(np.array(df2).reshape(-1, 1))

train_size = int(len(df2)*0.65)
test_size = len(df2) - train_size
train_data,test_data = df2[0:train_size,:],df2[train_size:len(df2),:1]

def create_dataset(dataset, time_step = 1):
dataX,dataY = [],[]
for i in range(len(dataset)-time_step-1):
a = dataset[i:(i+time_step),0]
dataX.append(a)
dataY.append(dataset[i + time_step,0])
return np.array(dataX),np.array(dataY)

time_step = 100
X_train,Y_train = create_dataset(train_data,time_step)
X_test,Y_test = create_dataset(test_data,time_step)

model = Sequential()
model.add(LSTM(50,return_sequences = True,input_shape = (X_train.shape[1],1)))
model.add(LSTM(50,return_sequences = True))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss = &#39;mean_squared_error&#39;,optimizer = &#39;adam&#39;)
model.fit(X_train,Y_train,validation_data = (X_test,Y_test),epochs = 100,batch_size = 64 ,verbose = 1)

train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)
Y_test = scaler.inverse_transform(np.array(Y_test).reshape(-1, 1))

look_back = 100
trainPredictPlot = np.empty_like(df2)
trainPredictPlot[:,:] = np.nan
trainPredictPlot[look_back : len(train_predict)+look_back,:] = train_predict

testPredictPlot = np.empty_like(df2)
testPredictPlot[:,:] = np.nan
testPredictPlot[len(train_predict)+(look_back)*2 + 1 : len(df2) - 1,:] = test_predict

plt.plot(scaler.inverse_transform(df2))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show() 此处

我试图通过回顾 100 天来一次性预测未来 30 天。我尝试了一些示例代码，但它们已经过时且已弃用。]]></description>
      <guid>https://stackoverflow.com/questions/78664040/how-to-predict-the-future-value-thoriugh-lstm</guid>
      <pubDate>Mon, 24 Jun 2024 18:17:00 GMT</pubDate>
    </item>
    <item>
      <title>mlflow 在记录图像时不会自动记录工件</title>
      <link>https://stackoverflow.com/questions/78663805/mlflow-doesnt-autolog-artifacts-while-logging-images</link>
      <description><![CDATA[对 mlflow 相当陌生。我正在使用 dagshub 免费 mlflow 服务器进行实验。我偶然发现了一个奇怪的行为。当我运行一个简单的 keras 模型时，使用 mlflow autolog 进行拟合，如下所示：
mlflow.tensorflow.autolog()
dagshub.init(&#39;my_experiment_name&#39;,
&#39;my_user_name&#39;,
mlflow=True)

# 定义参数。
num_epochs = 10
batch_size = 256

# 训练模型。
history = model.fit(X_train,
y_train,
epochs=num_epochs,
batch_size=batch_size,
validation_data=(X_test, y_test))
mlflow.end_run()

这会产生预期的行为。我可以在工件和指标中看到模型。

但是，当我尝试添加包含训练准确率和损失的图形时，工件仅包含图像。
mlflow.tensorflow.autolog()
dagshub.init(&#39;my_experiment_name&#39;,
&#39;my_user_name&#39;,
mlflow=True)

# 定义参数。
num_epochs = 10
batch_size = 256

# 训练模型。
history = model.fit(X_train,
y_train,
epochs=num_epochs,
batch_size=batch_size,
validation_data=(X_test, y_test))

##_________ 有问题的部分
fig, ax = plt.subplots(1,2,figsize=(10,4))
ax[0].plot(history.history[&#39;accuracy&#39;], label=&#39;Accuracy&#39; )
ax[0].plot(history.history[&#39;val_accuracy&#39;], label=&#39;Val Accuracy&#39; )
ax[0].set_title(&#39;Accuracy&#39;)
ax[0].legend(loc=&#39;best&#39;)
ax[1].plot(history.history[&#39;loss&#39;], label=&#39;Loss&#39; )
ax[1].plot(history.history[&#39;val_loss&#39;], label=&#39;Val Loss&#39; )
ax[1].set_title(&#39;Loss&#39;)
ax[1].legend(loc=&#39;best&#39;)
mlflow.log_figure(fig,&#39;training_history.png&#39;)
# _________

mlflow.end_run()

模型工件不存在。指标也没有记录。
我是否遗漏了一些简单的东西。

请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78663805/mlflow-doesnt-autolog-artifacts-while-logging-images</guid>
      <pubDate>Mon, 24 Jun 2024 17:12:13 GMT</pubDate>
    </item>
    <item>
      <title>单头 Transformer 模型，输出和目标张量大小不匹配</title>
      <link>https://stackoverflow.com/questions/78663722/single-headed-transformer-model-output-and-target-tensor-sizes-dont-match</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78663722/single-headed-transformer-model-output-and-target-tensor-sizes-dont-match</guid>
      <pubDate>Mon, 24 Jun 2024 16:50:02 GMT</pubDate>
    </item>
    <item>
      <title>执行训练过程中遇到错误</title>
      <link>https://stackoverflow.com/questions/78663639/facing-errors-while-executing-the-training-process</link>
      <description><![CDATA[这是我的代码片段
from transformers import TrainingArguments, Trainer
定义训练参数
args = TrainingArguments(
output_dir=&#39;./content/drive/MyDrive/phase ii/result&#39;, # 保存结果的目录
evaluation_strategy=&quot;epoch&quot;, # 在每个 epoch 结束时进行评估
save_strategy=&quot;epoch&quot;, # 在每个 epoch 结束时保存检查点
save_total_limit=3, # 限制检查点的总数
learning_rate=1e-5, # 设置学习率
per_device_train_batch_size=32, # 训练的批次大小
per_device_eval_batch_size=32, # 评估的批次大小
num_train_epochs=10, # 训练次数epochs
report_to=&quot;none&quot;, # 禁用向外部系统报告
load_best_model_at_end=True, # 训练完成后加载最佳模型
metric_for_best_model=&quot;f1&quot; # 用于选择最佳模型的指标
)
执行上述代码时遇到此错误
我该如何解决给定的问题。您能简要解释一下必要的解决方案吗]]></description>
      <guid>https://stackoverflow.com/questions/78663639/facing-errors-while-executing-the-training-process</guid>
      <pubDate>Mon, 24 Jun 2024 16:31:02 GMT</pubDate>
    </item>
    <item>
      <title>使用 OpenCV 和 DETR 优化视频处理</title>
      <link>https://stackoverflow.com/questions/78663466/optimizing-video-processing-with-opencv-and-detr</link>
      <description><![CDATA[我正在编写一个 Python 脚本，使用 OpenCV 处理视频文件，使用预训练的 DETR 模型注释检测到的对象，并保存注释后的视频。问题是处理部分需要 5 分钟，即使是 4 秒的视频。这是我找到的解决方案：
DEVICE = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
CHECKPOINT = &#39;facebook/detr-resnet-50&#39;
CONFIDENCE_THRESHOLD = 0.5
BATCH_SIZE = 4
image_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)

id2label = {
0: &#39;potholes&#39;,
1: &#39;pothole&#39;
}

model = DetrForObjectDetection.from_pretrained(&quot;Models/potholes-model&quot;)
model.to(DEVICE)
model.eval()
box_annotator = sv.BoxAnnotator()

def process_video(video_path, output_path):
cap = cv2.VideoCapture(video_path)

if not cap.isOpened():
print(&quot;Error: 无法打开视频。&quot;)
return

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

fourcc = cv2.VideoWriter_fourcc(*&#39;mp4v&#39;)
out = cv2.VideoWriter(os.path.join(output_path, &quot;results.mp4&quot;), fourcc, fps, (width, height))

if not out.isOpened():
print(&quot;Error: 无法打开 VideoWriter。&quot;)
return

frames = []
while True:
ret, frame = cap.read()
if not ret:
break

frames.append(frame)
if len(frames) == BATCH_SIZE:
process_and_write_batch(frames, out)
frames = []

if frames:
process_and_write_batch(frames, out)

cap.release()
out.release()
cv2.destroyAllWindows()

我当时尝试处理一批帧，而不是一次处理一个：
def process_and_write_batch(frames, out):
with torch.no_grad():
输入 = image_processor(images=frames, return_tensors=&#39;pt&#39;).to(DEVICE)
输出 = model(**inputs)

target_sizes = torch.tensor([frame.shape[:2] for frame in frames]).to(DEVICE)
结果 = image_processor.post_process_object_detection(
output=outputs,
Threshold=CONFIDENCE_THRESHOLD,
target_sizes=target_sizes
)

for frame, result in zip(frames, results):
try:
detections = sv.Detections.from_transformers(transformers_results=result).with_nms(threshold=0.5)
labels = [f&quot;{id2label[class_id]} {confidence:.2f}&quot; for _, confidence, class_id, _ in detections]
annotated_frame = box_annotator.annotate(scene=frame.copy(), detections=detections, labels=labels)
out.write(annotated_frame)
except:
out.write(frame)

即使对于短视频，该过程仍然需要很长时间。我可以做些什么来优化该过程？]]></description>
      <guid>https://stackoverflow.com/questions/78663466/optimizing-video-processing-with-opencv-and-detr</guid>
      <pubDate>Mon, 24 Jun 2024 15:46:23 GMT</pubDate>
    </item>
    <item>
      <title>有没有一种方法可以通过比率来测量用于创建第三个特征的两个特征的初始形状值？</title>
      <link>https://stackoverflow.com/questions/78662828/is-there-a-way-of-measuring-the-initial-shap-value-of-two-features-that-have-bee</link>
      <description><![CDATA[在 Python 机器学习、回归或分类等背景下，我有时会对两个特征进行比率计算以生成第三个特征，并删除前两个特征。
示例：living_surface_ratio = residence_building__total_living_surface / building__footprint_surface

因此，在我的 data_processing_pipe 末尾，我只有 living_surface_ratio 作为一个特征。
我想知道是否有办法测量 residence_building__total_living_surface 和 building__footprint_surface 的形状值，尽管这些特征已被删除，因此不再可用。
也许：
shape(living_surface_ratio) = 0.3 * shape(residential_building__total_living_surface) + 0.7µbuilding__footprint_surface

我考虑过用两个特征，然后只使用比率，但困扰我的是，我并没有真正提供具有特征比率的模型的可解释性。
我看过形状值的实现，但修改起来似乎很复杂。
如果您有任何想法，我很乐意听取您的意见！
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/78662828/is-there-a-way-of-measuring-the-initial-shap-value-of-two-features-that-have-bee</guid>
      <pubDate>Mon, 24 Jun 2024 13:36:28 GMT</pubDate>
    </item>
    <item>
      <title>Yolov5 上的模型蒸馏</title>
      <link>https://stackoverflow.com/questions/78661658/model-distillation-on-yolov5</link>
      <description><![CDATA[有人能指导我如何对 yolov5 进行模型蒸馏吗？其中 yolov5n 是学生模型，yolov5l 是教师模型。如果您能详细解释步骤和所涉及的代码，将会很有帮助，因为我无法在网上找到合适的指南。
提前谢谢
我曾尝试在线搜索如何在 yolov5 上执行模型蒸馏，但资源不足。我期待完整的指南，包括在 yolov5n 上执行模型蒸馏所涉及的代码，其中 yolov5l 是教师模型。]]></description>
      <guid>https://stackoverflow.com/questions/78661658/model-distillation-on-yolov5</guid>
      <pubDate>Mon, 24 Jun 2024 09:43:48 GMT</pubDate>
    </item>
    <item>
      <title>如何将 JSON 中的标记坐标叠加到 JPG 图像中以进行 CNN 训练？</title>
      <link>https://stackoverflow.com/questions/78661583/how-to-overlay-labeled-coordinates-from-json-into-jpg-images-for-cnn-training</link>
      <description><![CDATA[我正在开展一个计算机视觉项目，该项目涉及检测和分割 MRI 扫描中的骨折。作为该项目的一部分，我让专家直接在图像上标记骨折区域。此过程会生成一个 JSON 文件，其中包含以下信息：

标记区域的坐标
标记区域的名称
标记图像的名称

我面临的挑战是将这些坐标从 JSON 文件转移到相应的 JPG 图像上，以准备进行 CNN 训练。
以下是我的 JSON 文件结构示例：
&quot;item&quot;: {
&quot;name&quot;: &quot;img-00003-00082.jpg&quot;,
&quot;team&quot;: {
&quot;name&quot;: &quot;Mask&quot;,
&quot;slug&quot;: &quot;mask&quot;
&quot;file_name&quot;: &quot;img-00003-00082.jpg&quot;,
&quot;annotations&quot;: [
{
&quot;bounding_box&quot;: {
&quot;h&quot;: 142.16649999999993,
&quot;w&quot;: 124.14549999999997,
&quot;x&quot;: 679.8006,
&quot;y&quot;: 425.7789
},
&quot;name&quot;: &quot;Broken&quot;,
&quot;polygon&quot;: {
&quot;paths&quot;: [
[
{
&quot;x&quot;: 695.1519,
&quot;y&quot;: 567.9454
},
{
&quot;x&quot;: 679.8006,
&quot;y&quot;: 530.5683
},


到目前为止，我已经设法从 JSON 文件中提取了必要的坐标。但是，我很难将这些坐标叠加到 JPG 图像上以生成 CNN 的训练数据。
我的问题：

如何准确地将 JSON 文件中的坐标叠加到相应的 JPG 图像上？
是否有任何推荐的 Python 库或方法专门适合此任务？
]]></description>
      <guid>https://stackoverflow.com/questions/78661583/how-to-overlay-labeled-coordinates-from-json-into-jpg-images-for-cnn-training</guid>
      <pubDate>Mon, 24 Jun 2024 09:29:08 GMT</pubDate>
    </item>
    <item>
      <title>与前一天相比，同一特征的 MSE 的负和正百分比增加</title>
      <link>https://stackoverflow.com/questions/78660957/negative-and-positive-increase-in-mse-for-a-same-feature-over-previous-day</link>
      <description><![CDATA[我有一个包含过去几天的数据和当天数据的数据框。
示例列 [cases、mobility、temp、rh、cases_1、mobility_1、temp_1、rh_1、cases_2、mobility_2、temp_2、rh_2 等。。]。我的目标列 (Y) 是“cases”，col_i 表示当前日期前 i 天的参数。
%inc mse 的代码如下所示。这会导致某些特征（如 temp）具有正值，而 temp_1 具有负值，temp_2 也可以具有正值或负值，其他特征也是如此。我该如何解释这些结果并找到过去几天的列的综合累积效应？
from tabulate import tabulate
feature_importance_df = pd.DataFrame({&#39;Feature Name&#39;: X_pastdays_test.columns}) 

y_pred_baseline = rf_regressor.predict(X_pastdays_test)
baseline_mse = mean_squared_error(y_test, y_pred_baseline)

percent_inc_mse_list = []
for feature_name in X_pastdays_test.columns:
# 排列测试数据中特征的值
x_test_perturbed = X_pastdays_test.copy()
x_test_perturbed[feature_name] = np.random.permutation(x_test_perturbed[feature_name])

# 使用扰动数据进行预测数据
y_pred_perturbed = rf_regressor.predict(x_test_perturbed)

# 使用扰动数据计算均方误差
perturbed_mse = mean_squared_error(y_test, y_pred_perturbed)

# 将 %Inc MSE 计算为 MSE 的百分比增加
percent_inc_mse = ((perturbed_mse - baseline_mse) / baseline_mse) * 100
percent_inc_mse_list.append(percent_inc_mse)

feature_importance_df[&#39;%Inc MSE&#39;] = percent_inc_mse_list

feature_importance_df = feature_importance_df.sort_values(by=&#39;%Inc MSE&#39;, accending=False)

print(feature_importance_df)
]]></description>
      <guid>https://stackoverflow.com/questions/78660957/negative-and-positive-increase-in-mse-for-a-same-feature-over-previous-day</guid>
      <pubDate>Mon, 24 Jun 2024 07:02:32 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 时，损失为 Nan</title>
      <link>https://stackoverflow.com/questions/78660903/loss-is-nan-with-tensorflow</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78660903/loss-is-nan-with-tensorflow</guid>
      <pubDate>Mon, 24 Jun 2024 06:50:02 GMT</pubDate>
    </item>
    <item>
      <title>我正在做一个足球分析跟踪机器学习项目，我得到了速度和距离估计器的导入错误</title>
      <link>https://stackoverflow.com/questions/78659710/i-am-making-a-football-analysis-tracking-machine-learning-project-i-am-getting-i</link>
      <description><![CDATA[ImportError: 无法从 
&#39;speed_and_distance_estimator.speed_and_distance_estimator&#39; 
(c:\Users...\Football analysis\speed_and_distance_estimator\speed_and_distance_estimator.py) 导入名称 &#39;Speed_and_Distance_Estimator&#39;

在我的文件 speed_and_distance_estimator.py 中
sys.path.append(&#39;../&#39;)
from utils import measure_distance, get_foot_position

class Speed_and_Distance_Estimator:
pass

在我的 init.py 中
from .speed_and_distance_estimator import Speed_and_Distance_Estimator

我预计我的 main.py 中不会出现任何错误
from utils import read_video, save_video
from trackers import Tracker
import cv2
import numpy as np
from team_assigner import TeamAssigner
from player_ball_assigner import PlayerBallAssigner
from camera_movement_estimator import CameraMovementEstimator
from view_transformer import ViewTransformer
from speed_and_distance_estimator import Speed_and_Distance_Estimator

def main():
# 读取视频
video_frames = read_video(&#39;input_videos/08fd33_4.mp4&#39;)

# 初始化跟踪器
tracker = Tracker(&#39;models/best.pt&#39;)

tracks = tracker.get_object_tracks(video_frames,
read_from_stub=True,
stub_path=&#39;stubs/track_stubs.pkl&#39;)
# 获取对象位置 
tracker.add_position_to_tracks(tracks)

# 相机运动估计器
camera_movement_estimator = CameraMovementEstimator(video_frames[0])
camera_movement_per_frame = camera_movement_estimator.get_camera_movement(video_frames,
read_from_stub=True,
stub_path=&#39;stubs/camera_movement_stub.pkl&#39;)
camera_movement_estimator.add_adjust_positions_to_tracks(tracks,camera_movement_per_frame)

# 视图转换器
view_transformer = ViewTransformer()
view_transformer.add_transformed_position_to_tracks(tracks)

# 插入球位置
tracks[&quot;ball&quot;] = tracker.interpolate_ball_positions(tracks[&quot;ball&quot;])

# 速度和距离估算器
speed_and_distance_estimator = Speed_and_Distance_Estimator()
speed_and_distance_estimator.add_speed_and_distance_to_tracks(tracks)

# 分配球员队伍
team_assigner = TeamAssigner()
team_assigner.assign_team_color(video_frames[0], 
tracks[&#39;players&#39;][0])

for frame_num, player_track in enumerate(tracks[&#39;players&#39;]):
for player_id, track in player_track.items():
team = team_assigner.get_player_team(video_frames[frame_num], 
track[&#39;bbox&#39;],
player_id)
tracks[&#39;players&#39;][frame_num][player_id][&#39;team&#39;] = team 
tracks[&#39;players&#39;][frame_num][player_id][&#39;team_color&#39;] = team_assigner.team_colors[team]

# 分配球获取
player_assigner =PlayerBallAssigner()
team_ball_control= []
for frame_num, player_track in enumerate(tracks[&#39;players&#39;]):
ball_bbox = tracks[&#39;ball&#39;][frame_num][1][&#39;bbox&#39;]
assigned_player = player_assigner.assign_ball_to_player(player_track, ball_bbox)

if assignment_player != -1:
tracks[&#39;players&#39;][frame_num][assigned_player][&#39;has_ball&#39;] = True
team_ball_control.append(tracks[&#39;players&#39;][frame_num][assigned_player][&#39;team&#39;])
else:
team_ball_control.append(team_ball_control[-1])
team_ball_control= np.array(team_ball_control)

# 绘制输出 
## 绘制对象轨迹
output_video_frames = tracker.draw_annotations(video_frames, tracks,team_ball_control)

## 绘制摄像机运动
output_video_frames = camera_movement_estimator.draw_camera_movement(output_video_frames,camera_movement_per_frame)

## 绘制速度和距离
speed_and_distance_estimator.draw_speed_and_distance(output_video_frames,tracks)

# 保存视频
save_video(output_video_frames, &#39;output_videos/output_video.avi&#39;)

if __name__ == &#39;__main__&#39;:
main()
]]></description>
      <guid>https://stackoverflow.com/questions/78659710/i-am-making-a-football-analysis-tracking-machine-learning-project-i-am-getting-i</guid>
      <pubDate>Sun, 23 Jun 2024 20:05:06 GMT</pubDate>
    </item>
    <item>
      <title>如何在使用训练有素的 YOLO-V8 实例分割模型进行预测时将边界框值添加到标签文本文件中？</title>
      <link>https://stackoverflow.com/questions/78649611/how-to-add-the-bounding-box-values-to-the-labels-text-files-during-prediction-wi</link>
      <description><![CDATA[我训练了一个 YOLO-V8 实例分割模型来分割类标签为 0 的对象。我使用 CLI 实例化训练后的模型并根据测试数据进行预测。
!yolo task=segment mode=predict model=&#39;/weights/best.pt&#39; conf=0.25 source=&#39;/test/images&#39; imgsz=1024 save=True save_txt=True save_conf=True

预测后，标签文件将以 .txt 格式存储。这些标签文件包含类索引，后跟多边形坐标，最后是边界框预测的置信度分数。但是，边界框坐标（即 x 中心、y 中心、宽度、高度）不包含在标签文件中。我还想将这些边界框坐标包含到每个标签文件中，因为我想稍后使用这些边界框坐标进行后期处理。示例标签文件内容如下所示：
0 0.21582 0.0898438 0.214844 0.0908203 0.213867 0.0908203 0.210938 0.09375 0.210938 0.0947266 0.203125 0.102539 0.203125 0.103516 0.201172 0.105469 0.200195 0.105469 0.199219 0.106445 0.199219 0.113281 0.200195 0.114258 0.200195 0.115234 0.203125 0.115234 0.204102 0.116211 0.223633 0.116211 0.224609 0.117188 0.227539 0.117188 0.228516 0.118164 0.230469 0.118164 0.231445 0.119141 0.234375 0.119141 0.235352 0.120117 0.248047 0.120117 0.249023 0.121094 0.251953 0.121094 0.25293 0.12207 0.254883 0.0927734 0.260742 0.0917969 0.256836 0.0917969 0.255859 0.0908203 0.233398 0.0908203 0.232422 0.0898438 0.910849

我没有将预测保存到任何“结果”变量中，并且我只在 CLI 中运行预测。]]></description>
      <guid>https://stackoverflow.com/questions/78649611/how-to-add-the-bounding-box-values-to-the-labels-text-files-during-prediction-wi</guid>
      <pubDate>Thu, 20 Jun 2024 21:15:36 GMT</pubDate>
    </item>
    <item>
      <title>如何修复 Huggingface 训练器的学习率？</title>
      <link>https://stackoverflow.com/questions/77792137/how-to-fix-the-learning-rate-for-huggingface%c2%b4s-trainer</link>
      <description><![CDATA[我正在使用以下参数训练模型：
Seq2SeqTrainingArguments(
output_dir = &quot;./out&quot;, 
overwrite_output_dir = True,
do_train = True,
do_eval = True,

per_device_train_batch_size = 2, 
gradient_accumulation_steps = 4,
per_device_eval_batch_size = 8, 

learning_rate = 1.25e-5,
warmup_steps = 1,

save_total_limit = 1,

evaluation_strategy = &quot;epoch&quot;,
save_strategy = &quot;epoch&quot;,
logs_strategy = &quot;epoch&quot;, 
num_train_epochs = 5, 

gradient_checkpointing = True,
fp16 = True, 

predict_with_generate = True,
generation_max_length = 225,

report_to = [&quot;tensorboard&quot;],
load_best_model_at_end = True,
metric_for_best_model = &quot;wer&quot;,
greater_is_better = False,
push_to_hub = False,
)

我假设 warmup_steps=1 固定了学习率。
但是，训练结束后，我查看文件 trainer_state.json，发现学习率似乎没有固定。
以下是 learning_rate 和 step 的值：
learning_rate，steps
1.0006 e-05 1033
7.5062 e-06 2066
5.0058 e-06 3099
2.5053 e-06 4132
7.2618 e-09 5165

学习率似乎没有固定在 1.25e-5（步骤 1 之后）。我遗漏了什么？如何修复学习率。]]></description>
      <guid>https://stackoverflow.com/questions/77792137/how-to-fix-the-learning-rate-for-huggingface%c2%b4s-trainer</guid>
      <pubDate>Wed, 10 Jan 2024 09:14:26 GMT</pubDate>
    </item>
    <item>
      <title>我的 C++ 代码无法正确检测物体 yolov5</title>
      <link>https://stackoverflow.com/questions/74536510/my-c-code-is-not-detecting-objects-correctly-yolov5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/74536510/my-c-code-is-not-detecting-objects-correctly-yolov5</guid>
      <pubDate>Tue, 22 Nov 2022 17:06:39 GMT</pubDate>
    </item>
    <item>
      <title>将 onnx 模型转换为 keras</title>
      <link>https://stackoverflow.com/questions/58395644/convert-onnx-model-to-keras</link>
      <description><![CDATA[我尝试将 ONNX 模型转换为 Keras，但当我调用转换函数时，我收到以下错误消息 “TypeError：不可哈希类型：&#39;google.protobuf.pyext._message.RepeatedScalarContainer&#39;”
ONNX 模型输入：input_1
您可以在此处查看 ONNX 模型：https://ibb.co/sKnbxWY
import onnx2keras
from onnx2keras import onnx_to_keras
import keras
import onnx

onnx_model = onnx.load(&#39;onnxModel.onnx&#39;)
k_model = onnx_to_keras(onnx_model, [&#39;input_1&#39;])

keras.models.save_model(k_model,&#39;kerasModel.h5&#39;,overwrite=True,include_optimizer=True)


 文件“C:/../onnx2Keras.py”，第 7 行，位于 &lt;module&gt;
k_model = onnx_to_keras(onnx_model, [&#39;input_1&#39;])
文件“..\site-packages\onnx2keras\converter.py”，第 80 行，位于 onnx_to_keras
weights[onnx_extracted_weights_name] = numpy_helper.to_array(onnx_w)
TypeError：不可哈希类型：&#39;google.protobuf.pyext._message.RepeatedScalarContainer&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/58395644/convert-onnx-model-to-keras</guid>
      <pubDate>Tue, 15 Oct 2019 13:15:06 GMT</pubDate>
    </item>
    </channel>
</rss>