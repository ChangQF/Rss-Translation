<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 14 Aug 2024 12:30:01 GMT</lastBuildDate>
    <item>
      <title>training=False 和 training =True 之间有什么区别？[重复]</title>
      <link>https://stackoverflow.com/questions/78870866/whats-the-difference-between-training-false-and-training-true</link>
      <description><![CDATA[def train_step(self, batch):
# 获取数据 
real_images = batch
fake_images = self.generator(tf.random.normal((128, 128, 1)), training=False)

# 训练鉴别器
with tf.GradientTape() as d_tape: 

yhat_real = self.discriminator(real_images, training=True) 
yhat_fake = self.discriminator(fake_images, training=True)
yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)

y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)

# 为 TRUE 输出添加一些噪声
noise_real = 0.15*tf.random.uniform(tf.shape(yhat_real))
* noise_fake = -0.15*tf.random.uniform(tf.shape(yhat_fake))
y_realfake += tf.concat([noise_real, noise_fake], axis=0)

total_d_loss = self.d_loss(y_realfake, yhat_realfake)

dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) 
self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))

使用 tf.GradientTape() 作为 g_tape: 

gen_images = self.generator(tf.random.normal((128,128,1)), training=True)

predicted_labels = self.discriminator(gen_images, training=False)

total_g_loss = self.g_loss(tf.zeros_like(predicted_labels),predicted_labels) 

ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)
self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))

return {&quot;d_loss&quot;:total_d_loss, &quot;g_loss&quot;:total_g_loss}

我正在构建一个 GAN。如上面的代码所示，在计算生成器的梯度时，鉴别器设置为 training =False。这是否意味着在计算梯度时，我们不想计算相对于鉴别器参数的损失梯度，也不想将梯度应用于鉴别器？
我还发现，这是训练模式和推理模式之间的区别。显然，批量归一化和 dropout 层对这两种模式的作用不同。这是什么意思？]]></description>
      <guid>https://stackoverflow.com/questions/78870866/whats-the-difference-between-training-false-and-training-true</guid>
      <pubDate>Wed, 14 Aug 2024 12:18:37 GMT</pubDate>
    </item>
    <item>
      <title>建模时我需要标准化 Y 变量吗？</title>
      <link>https://stackoverflow.com/questions/78870125/do-i-need-to-standardize-the-y-variable-when-modelling</link>
      <description><![CDATA[我正在构建机器学习回归模型。y 变量是每只股票所有月份的超额收益。
我对股票进行了 1% 和 99% 的缩尾处理。 y 变量的图：

y_trn.hist(bins=50)
训练样本图

y_vld.hist(bins=50)
有效样本图

y_tst.hist(bins=50)
测试样本图


因为现在我的模型不够稳健，测试样本的 r2 远大于训练样本的 r2。
这是我的另一个问题：
前馈网络回归预测几乎相同的值
我将所有 X 变量标准化为 [-1,1]，我需要标准化 y 变量吗？也许我不应该对 y 变量进行缩尾？]]></description>
      <guid>https://stackoverflow.com/questions/78870125/do-i-need-to-standardize-the-y-variable-when-modelling</guid>
      <pubDate>Wed, 14 Aug 2024 09:20:20 GMT</pubDate>
    </item>
    <item>
      <title>将 `transformers` 中的 `TFRobertaForSequenceClassification` 与 `tensorflow.keras.Model` 结合使用</title>
      <link>https://stackoverflow.com/questions/78870106/using-tfrobertaforsequenceclassification-from-transformers-with-tensorflow</link>
      <description><![CDATA[我已经按照此处所示为 TFRobertaForSequenceClassification 定义了带有两个标签的自定义分类器头，以便能够针对我的下游任务对其进行微调，即将句子分类为来自一组有限的独立标签。
from transformers import TFRobertaForSequenceClassification
roberta_model = TFRobertaForSequenceClassification.from_pretrained(pretrained_model_path, from_pt=True, num_labels=2)

我想将 roberta_model 成为 tensorflow.keras.Model。以下是我现在使用 tensorflow.keras.layers.Identity 作为最终/输出层的结果，因为 roberta 的分类主管已经处理好了它。
import numpy as np
import tensorflow as tf

MAX_SEQUENCE_LENGTH = 256

# 定义输入 ID、注意掩码和输入类型 ID
input_word_ids = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name=&#39;input_word_ids&#39;)
input_mask = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name=&#39;input_mask&#39;)
input_type_ids = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name=&#39;input_type_ids&#39;)

# 启动预训练模型
roberta_model = TFRobertaForSequenceClassification.from_pretrained(pretrained_model_path, from_pt=True, num_labels=2)
x = roberta_model(input_ids=input_word_ids,tention_mask=input_mask, token_type_ids=input_type_ids, labels=np.array([0, 1]))

# `x` 具有损失并将 logits 作为键。因此，将损失和对数传递到前面。
# 添加任务所需的最终层
out = tf.keras.layers.Identity(activation=&#39;softmax&#39;)(x)

# 构建模型
model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], output=out)

# 编译模型
model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=tf.keras.optimizers.Adam(lr=1e-5), metrics=[&#39;accuracy&#39;, tf.keras.metrics.F1Score()])

我需要将模型放在 Keras 中，因为我有处理历史记录的下游代码tf.keras.Model.fit 返回。
我发现我现有方法的问题是，如源代码所示 (https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1210-L1232)，末尾已经附加了一个损失，因此，我添加了一个 softmax 损失，只是为了使其成为 tensorflow.keras.Model 并且有两个损失函数看起来不正确。此外，我在最终的身份层中不能有任何激活吗？
我在其他地方做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78870106/using-tfrobertaforsequenceclassification-from-transformers-with-tensorflow</guid>
      <pubDate>Wed, 14 Aug 2024 09:18:14 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML 算法对图像进行分类时，如何修复“找到具有 dim 4 的数组”错误</title>
      <link>https://stackoverflow.com/questions/78869863/how-fix-found-array-with-dim-4error-when-using-ml-algorthims-to-classify-image</link>
      <description><![CDATA[我有一个简单的 ML 分类问题。我有 8 个文件夹，每个文件夹代表一个类，因此我首先从文件夹中加载这些图像并分配标签，然后将其保存为 csv 文件（代码如下）
def load_images_from_folder(root_folder):`
image_paths = []
images = []
labels = []
for label in os.listdir(root_folder):
label_path = os.path.join(root_folder, label)
if os.path.isdir(label_path):
for filename in os.listdir(label_path):
img_path = os.path.join(label_path, filename)
if os.path.isfile(img_path) and (filename.endswith(&quot;.jpg&quot;):
img = Image.open(img_path)
img = img.resize((128, 128))
img_array = np.array(img)
image_paths.append(img_path)
images.append(img_array)
labels.append(label)
return image_paths, images, labels
if __name__ == &quot;__main__&quot;:
root_folder_path = &quot;./Datasets_1&quot;
image_paths, images, labels = load_images_from_folder(root_folder_path)

然后我将图像和标签转换为 DataFrame 并加载它
data = {&quot;Images&quot;: image_paths, &quot;Labels&quot;: labels}
df = pd.DataFrame(data)
df.to_csv(&quot;original_data.csv&quot;, index=False)
csv_file = &quot;original_data.csv&quot;
df = pd.read_csv(csv_file)

我还将向 DataFrame 添加一个带有编码标签的新列“Encoded_Labels”，并将“Encoded_Labels”列转换为整数
df[&#39;Encoded_Labels&#39;] =coded_labels
df[&#39;Encoded_Labels&#39;] = df[&#39;Encoded_Labels&#39;].astype(int)

最后，我将数据集拆分为训练集和测试集，并对训练图像进行预处理
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
def load_and_preprocess_images(file_paths, target_size=(128, 128)):
images = []
for file_path in file_paths:
img = Image.open(file_path)
img = img.resize(target_size)
img_array = np.array(img) / 255.0 # 标准化像素值
images.append(img_array)
return np.array(images)

X_train = load_and_preprocess_images(train_df[&#39;Images&#39;].values)
y_train = train_df[&#39;Encoded_Labels&#39;].values
X_test = load_and_preprocess_images(test_df[&#39;Images&#39;].values)
y_test = test_df[&#39;Encoded_Labels&#39;].values**your text**

X_train 的输出形状是
(20624, 128, 128, 3)`

对于这一点我没有问题，我可以使用它与 DL 模型一起使用没有问题，但是当尝试使用 ML 模型（例如 KNN、SVM、DT 等）时。示例代码如下
from sklearn.svm import SVC
svc = SVC(kernel=&#39;linear&#39;,gamma=&#39;auto&#39;)
svc.fit(X_train, y_train)`

或
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train)
y_pred = knn_clf.predict(X_test)
accuracy = metrics.accuracy_score(y_test, y_pred)
print(&quot;Accuracy of KNN Classifier : %.2f&quot; % (准确率*100))

我收到此错误
“ValueError：找到 dim 为 4 的数组。SVC 预期 &lt;= 2。”
如何修复此错误？
使用 ML 训练模型]]></description>
      <guid>https://stackoverflow.com/questions/78869863/how-fix-found-array-with-dim-4error-when-using-ml-algorthims-to-classify-image</guid>
      <pubDate>Wed, 14 Aug 2024 08:26:27 GMT</pubDate>
    </item>
    <item>
      <title>xgboost 是否使用有放回抽样、无放回抽样或者其他完全不同的抽样方法？</title>
      <link>https://stackoverflow.com/questions/78869855/does-xgboost-use-sampling-with-replacement-sampling-without-replacement-or-some</link>
      <description><![CDATA[在Coursera 上学习这门课程，据说它像传统的集成树一样使用替换采样。我知道 xgboost 在第一次迭代后会为错误分类的示例赋予更多权重，但是第一次迭代呢？即使在网上，我也得到了不同的信息。
课程片段
尝试了 GPT、Gemini 和在线资源。]]></description>
      <guid>https://stackoverflow.com/questions/78869855/does-xgboost-use-sampling-with-replacement-sampling-without-replacement-or-some</guid>
      <pubDate>Wed, 14 Aug 2024 08:23:38 GMT</pubDate>
    </item>
    <item>
      <title>在 Keras 3 中从“.keras”文件加载模型时出现反序列化错误，密集层可能存在问题</title>
      <link>https://stackoverflow.com/questions/78869745/deserializing-error-when-loading-models-from-keras-files-in-keras-3-possible</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78869745/deserializing-error-when-loading-models-from-keras-files-in-keras-3-possible</guid>
      <pubDate>Wed, 14 Aug 2024 07:58:35 GMT</pubDate>
    </item>
    <item>
      <title>NotImplementedError：调用 Lambda.call() 时遇到异常。我们无法自动推断 Lambda 输出的形状</title>
      <link>https://stackoverflow.com/questions/78869440/notimplementederror-exception-encountered-when-calling-lambda-call-we-could</link>
      <description><![CDATA[NotImplementedError Traceback（最近一次调用最后一次）
&lt;ipython-input-36-138183a2a830&gt; 在 &lt;cell line: 2&gt;()
1 # 在推理模式下创建模型对象。
----&gt; 2 model = modellib.MaskRCNN(mode=&quot;inference&quot;, model_dir=MODEL_DIR, config=config)
3 
4 # 加载在 MS-COCO 上训练的权重
5 model.load_weights(COCO_MODEL_PATH, by_name=True)

4 帧
/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/lambda_layer.py in compute_output_shape(self, input_shape)
93 return nest.map_structure(lambda x: x.shape, output_spec)
94 except:
---&gt; 95 raise NotImplementedError(
96 “我们无法自动推断 Lambda 输出的形状”
97 “请指定 `output_shape`”

NotImplementedError：调用 Lambda.call() 时遇到异常。

我们无法自动推断 Lambda 输出的形状。请为此 Lambda 层指定 `output_shape` 参数。

Lambda.call() 收到的参数：
• args=(&#39;&lt;KerasTensor shape=(None, 1000, 1, 1, 1024), dtype=float32, sparse=False, name=keras_tensor_9478&gt;&#39;,)
• kwargs={&#39;mask&#39;: &#39;None&#39;}

如何解决在 google colab 中运行 MRCNN 的问题]]></description>
      <guid>https://stackoverflow.com/questions/78869440/notimplementederror-exception-encountered-when-calling-lambda-call-we-could</guid>
      <pubDate>Wed, 14 Aug 2024 06:33:59 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-learn 版本不匹配问题。我不知道应该安装哪个版本</title>
      <link>https://stackoverflow.com/questions/78869112/scikit-learn-version-mismatch-problem-and-i-dont-know-which-version-should-be</link>
      <description><![CDATA[我正在努力解决涉及 Scikit-learn 的版本不匹配问题，事实证明这非常成问题。每当我尝试安装不同版本的 Scikit-learn 时，我都会遇到一系列错误，这些错误似乎因我尝试的每个版本而异。

问题的核心似乎是 Scikit-learn 与其依赖项（例如 NumPy 和 SciPy）之间的不兼容性。这些依赖项对于 Scikit-learn 正常运行至关重要，找到可以协同工作的正确版本已成为一项艰巨的任务。尽管我付出了努力，但我还是无法找到一个可以解决错误并与我现有设置很好地集成的 Scikit-learn 版本。反复试验的过程只会导致越来越多的挫败感，因为每个新版本都会带来一系列问题，而不是解决核心问题。
这个版本不匹配严重影响了我在项目中有效使用 Scikit-learn 的能力。缺乏关于将 Scikit-learn 与其依赖项的兼容版本对齐的明确指导增加了我的困难，使我很难继续工作并实现预期结果。]]></description>
      <guid>https://stackoverflow.com/questions/78869112/scikit-learn-version-mismatch-problem-and-i-dont-know-which-version-should-be</guid>
      <pubDate>Wed, 14 Aug 2024 04:15:49 GMT</pubDate>
    </item>
    <item>
      <title>runs\train\exp10 不是目录</title>
      <link>https://stackoverflow.com/questions/78868439/runs-train-exp10-is-not-a-directory</link>
      <description><![CDATA[我正在尝试使用我的自定义数据训练 YoloV5 模型。我正在尝试在自己的电脑上进行训练（因为如果我离开，Google Colab 就会断开连接，而且我的数据集大约有 3000 张图像，所以它真的很大），但我一直收到此错误：
train: weights=yolov5s.pt, cfg=models/yolov5s.yaml, data=data.yaml, hyp=data\hyps\hyp.scratch-low.yaml, epochs=300, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data\hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, worker=8, project=runs\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, pains=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False
github：跳过检查（不是 git 存储库），有关更新，请参阅 https://github.com/ultralytics/yolov5
YOLOv5 2024-7-15 Python-3.12.2 torch-2.3.1+cpu CPU
超参数：lr0=0.01、lrf=0.01、momentum=0.937、weight_decay=0.0005、warmup_epochs=3.0， warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, Translation=0.1, scale=0.5, Shelf=0.0, Perspective=0.0, flipud=0.0, fliplr=0.5, Mosaic=1.0, mixup=0.0, copy_paste=0.0
TensorBoard：以“tensorboard --logdir runs\train”开始，在 http://localhost:6006/ 查看
回溯（最近一次调用）：
文件&lt;module&gt; 中的“C:\Users\Usuário\yolov5_work2024\yolov5-master\train.py”，第 986 行
main(opt)
文件 &quot;C:\Users\Usuário\yolov5_work2024\yolov5-master\train.py&quot;，第 688 行，在 main 中
train(opt.hyp, opt, device, callbacks)
文件 &quot;C:\Users\Usuário\yolov5_work2024\yolov5-master\train.py&quot;，第 180 行，在 train 中
loggers = Loggers(
^^^^^^^^^
文件 &quot;C:\Users\Usuário\yolov5_work2024\yolov5-master\utils\loggers\__init__.py&quot;，第 121 行，在 __init__ 中
self.tb = SummaryWriter(str(s))
^^^^^^^^^^^^^^^^^^^^^^
文件&quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\tensorboard\writer.py&quot;，第 249 行，在 __init__ 中
self._get_file_writer()
文件 &quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\tensorboard\writer.py&quot;，第 281 行，在 _get_file_writer 中
self.file_writer = FileWriter(
^^^^^^^^^^^
文件 &quot;C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\utils\tensorboard\writer.py&quot;，第 75 行，在 __init__ 中
self.event_writer = EventFileWriter(**
^^^^^^^^^^^^^^^^^
文件“C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorboard\summary\writer\event_file_writer.py”，第 72 行，在 __init__ 中
tf.io.gfile.makedirs(logdir)
文件“C:\Users\Usuário\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\lib\io\file_io.py”，第 513 行，在 recursive_create_dir_v2 中
_pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path))
tensorflow.python.framework.errors_impl.FailedPreconditionError: runs\train\exp10 不是目录

我尝试从预先训练的模型训练我的模型（正如 yolov5 文档所推荐的那样），如下所示：
python train.py --img 640 --batch 32 --epochs 300 --data data.yaml --weights yolov5s.pt
或者从头开始，如下所示：
python train.py --img 640 --batch 32 --epochs 300 --data data.yaml --cfg models/yolov5s.yaml
我还看到了其他问题，例如 GitHub 上的问题 #12008 和 Stack Overflow 上的这个问题 tensorflow.python.framework.errors_impl.FailedPreconditionError: runs\train\exp3 不是目录，但还没有找到任何解决方案]]></description>
      <guid>https://stackoverflow.com/questions/78868439/runs-train-exp10-is-not-a-directory</guid>
      <pubDate>Tue, 13 Aug 2024 21:58:27 GMT</pubDate>
    </item>
    <item>
      <title>具有大数据集的 Adaboost：初始权重接近于零</title>
      <link>https://stackoverflow.com/questions/78867381/adaboost-with-large-dataset-initial-weight-are-near-zero</link>
      <description><![CDATA[我正在尝试实现 adaboost。我选择的数据集是 mlpack 包中的 Covertype。问题在于计算数据集的初始权重为 1/n_elem，其中 n_elem (406709) 是一个非常大的数字，因此除法接近于 0。此外，类别非常不平衡：

标签 0 出现 148378 次。
标签 1 出现 198219 次。
标签 2 出现 25086 次。
标签 3 出现 1935 次
标签 4 出现 6656 次。
标签 5 出现 12181 次。
标签 6 出现 14254 次。

因此，当我规范化权重时，会发生分段错误

我想到的唯一想法是对数据集进行分区，但我不知道是否要保留这种类别分布或平衡它？这可能是个好主意吗？还有其他可能性吗？
提前感谢你的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78867381/adaboost-with-large-dataset-initial-weight-are-near-zero</guid>
      <pubDate>Tue, 13 Aug 2024 16:29:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 创建聊天机器人，使用 AWS Bedrock 中的模型与 Excel 或 PDF 文件进行交互 [关闭]</title>
      <link>https://stackoverflow.com/questions/78866843/create-a-chatbot-using-llm-for-interacting-with-excel-or-pdf-files-using-models</link>
      <description><![CDATA[我想创建一个聊天机器人，它可以根据我输入的 excel 或 PDF 文件回答问题。我有 AWS 基本访问权限，可以在其中使用多个模型。我的 excel 文件有 20k 行和 10 列。有人可以建议我应该使用哪个最好的 LLM 模型吗？或者如果有人可以分享一些相关资源，那将非常有帮助。
我尝试了一些模型，但要么我得到了 token 错误，要么输出不准确。]]></description>
      <guid>https://stackoverflow.com/questions/78866843/create-a-chatbot-using-llm-for-interacting-with-excel-or-pdf-files-using-models</guid>
      <pubDate>Tue, 13 Aug 2024 14:26:01 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv8 进行大量错误检测</title>
      <link>https://stackoverflow.com/questions/78820748/alot-of-incorrect-detection-using-yolov8</link>
      <description><![CDATA[我尝试使用 Visual Code Studio 运行 YOLOv8。安装了 ultralytics 并在 vs code 终端上运行了 yolo predict model=yolov8n.pt source=&#39;https://ultralytics.com/images/bus.jpg&#39;。
但是我收到的输出是
2 个人、1 辆自行车、5 辆汽车、10 辆摩托车、73 艘船、3 个停车标志、1 只狗、10 匹马、10 头牛、32 只熊、1 只长颈鹿、63 把雨伞、6 个手提包、9 个飞盘、15 块滑雪板、5 块冲浪板、12 把刀、5 张床、37 张餐桌

这些显然不是这张图片的一部分。

当我第一次安装 ultralytics 并尝试运行 torch 时，出现了缺少依赖项的错误。fbgemm.ddl 丢失。后来，当我安装 vs_BuildTools 时，这个问题得到了解决。然后我继续在虚拟环境中运行代码，其中使用 torch 的程序运行没有任何错误。然后我继续输入此代码片段并遇到此问题。我也尝试使用命令提示符和 jupyter 笔记本运行，但同样的问题仍然存在。
我也检查了版本是否兼容，结果是兼容的。我还没有安装 cuda，是因为这个原因还是还有其他我不知道的问题？请有人帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78820748/alot-of-incorrect-detection-using-yolov8</guid>
      <pubDate>Thu, 01 Aug 2024 11:33:58 GMT</pubDate>
    </item>
    <item>
      <title>我无法从“typing_extensions”导入名称“TypeAliasType”</title>
      <link>https://stackoverflow.com/questions/77450322/i-cannot-import-name-typealiastype-from-typing-extensions</link>
      <description><![CDATA[我是 Python 新手，发现了以下这样的错误。非常感谢您的评论。谢谢
我尝试将 Gradio 库导入为 gr
我尝试了几个现有的建议，但结果都是徒劳的。我不知道该怎么办]]></description>
      <guid>https://stackoverflow.com/questions/77450322/i-cannot-import-name-typealiastype-from-typing-extensions</guid>
      <pubDate>Thu, 09 Nov 2023 03:38:10 GMT</pubDate>
    </item>
    <item>
      <title>Torch Geometric - RuntimeError: mat1 和 mat2 形状无法相乘（1479x1 和 1479x1024）</title>
      <link>https://stackoverflow.com/questions/70844354/torch-geometric-runtimeerror-mat1-and-mat2-shapes-cannot-be-multiplied-1479x</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/70844354/torch-geometric-runtimeerror-mat1-and-mat2-shapes-cannot-be-multiplied-1479x</guid>
      <pubDate>Tue, 25 Jan 2022 06:39:32 GMT</pubDate>
    </item>
    <item>
      <title>如何使 RandomForestClassifier 更快？</title>
      <link>https://stackoverflow.com/questions/43640546/how-to-make-randomforestclassifier-faster</link>
      <description><![CDATA[我正在尝试使用大约有 1M 原始数据的 Twitter 情绪数据从 kaggle 网站实现词袋模型。我已经清理了它，但在最后一部分，当我将特征向量和情绪应用于随机森林分类器时，它花费了太多时间。这是我的代码...
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 100,verbose=3)
forest = forest.fit( train_data_features, train[&quot;Sentiment&quot;] )

train_data_features 是 1048575x5000 稀疏矩阵。我试图将其转换为数组，但执行时显示内存错误。
我哪里做错了？有人可以建议我一些来源或其他更快的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/43640546/how-to-make-randomforestclassifier-faster</guid>
      <pubDate>Wed, 26 Apr 2017 17:09:55 GMT</pubDate>
    </item>
    </channel>
</rss>