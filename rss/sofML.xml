<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 31 Jul 2024 15:16:12 GMT</lastBuildDate>
    <item>
      <title>YoloTinyNet 如何对特定图像进行推理？</title>
      <link>https://stackoverflow.com/questions/78816855/how-does-the-yolotinynet-make-inference-on-a-specific-image</link>
      <description><![CDATA[我有一个图像weirdobject.jpg和一个通过带注释的图像训练的模型，以识别该图像中的对象（注释是通过roboflow完成的，并导出到.ckpt文件中）。基本上这就是我使用 YoloTinyNet 对输入图像进行推理的方式：
class_names = [&quot;aeroplane&quot;, &quot;bicycle&quot;, &quot;bird&quot;, &quot;boat&quot;, &quot;bottle&quot;, &quot;bus&quot;, &quot;car&quot;, &quot;cat&quot;, &quot;chair&quot;, &quot;cow&quot;, &quot;diningtable&quot;,
&quot;dog&quot;, &quot;horse&quot;, &quot;motorbike&quot;, &quot;person&quot;, &quot;pottedplant&quot;, &quot;sheep&quot;, &quot;sofa&quot;, &quot;train&quot;, &quot;tvmonitor&quot;,
&quot;small_ball&quot;]
_common_params = {&#39;image_size&#39;: 448, &#39;num_classes&#39;: len(classes_name),
&#39;batch_size&#39;: 16} # 批次大小 1
_net_params = {&#39;cell_size&#39;: 7, &#39;boxes_per_cell&#39;: 2, &#39;weight_decay&#39;: 0.0005}
image = tf.placeholder(tf.float32, (1, 448, 448, 3))
_net =YoloTinyNet(_common_params, _net_params, test=True)
# 图像不应该包含实际的 .jpg 文件吗？或者此时占位符就足够了？
_net .inference(image)
sess = tf.Session()
saver = tf.train.Saver(_net.trainable_collection)
saver.restore(sess, modelFile)
src_img = cv2.imread(&quot;./weirdobject.jpg&quot;)
resized_img = cv2.resize(src_img, (448, 448))
np_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)
#我在此阶段仅包含图像数据（在此之前不应该包含吗？）
np_predict = sess.run(object_predicts, feed_dict={image: np_img})

np_predict 结果似乎与我的预期不一致，因为返回的结果包括许多不在图像中的对象类别，而真实对象也包含在结果中。
我的问题是，这是通过固定图像对训练模型进行预测以检测对象的正确方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78816855/how-does-the-yolotinynet-make-inference-on-a-specific-image</guid>
      <pubDate>Wed, 31 Jul 2024 14:15:32 GMT</pubDate>
    </item>
    <item>
      <title>嵌入问题：使用本体为下游 ML 任务创建分类数据的连续表示</title>
      <link>https://stackoverflow.com/questions/78816609/embedding-problem-using-ontologies-to-create-a-continuous-representation-of-cat</link>
      <description><![CDATA[简化设置：我有多个二分图。第一个节点类型是不同种类的食物，例如苹果、香蕉、豌豆、胡萝卜……第二个节点类型是分类/本体，可以将我的数据设置为关系，例如苹果、香蕉是水果，豌豆、胡萝卜是蔬菜等。我想对表格数据集执行线性回归/ xgboost，这些食物也存储在其中。我不想只进行独热编码，因为我会失去食物之间的关系。现在我可以构建一个二分图并使用例如 node2vec 来创建嵌入。然后我的表中会有很多列，并且可能在下游 ML 之后我会丢失有关特征重要性的信息。那么我可以使用嵌入来学习相似性/聚类，将它们放在 1-100 的范围内，然后将其用作我的数据集中的一列，这样我就可以从分类变为连续了吗？或者这是一个愚蠢的想法。有没有关于此的出版物或它有名字吗？
还没有尝试，更多的是一个概念性问题。]]></description>
      <guid>https://stackoverflow.com/questions/78816609/embedding-problem-using-ontologies-to-create-a-continuous-representation-of-cat</guid>
      <pubDate>Wed, 31 Jul 2024 13:25:23 GMT</pubDate>
    </item>
    <item>
      <title>在 mlflow 中使用 lightgbm 进行数据集记录（自动记录）时速度严重减慢</title>
      <link>https://stackoverflow.com/questions/78816040/severe-slowdown-when-using-dataset-logging-autologging-with-lightgbm-in-mlflow</link>
      <description><![CDATA[在使用 mlflow 进行训练期间跟踪数据集时，我遇到了严重的减速（通常需要 10-50 倍的时间）。
代码如下所示：
mlflow.set_tracking_uri(&quot;http://localhost:5000&quot;)
mlflow.set_experiment(&quot;My Experiment&quot;)

mlflow.lightgbm.autolog()
train, val = get_datasets() # 返回 lightgbm.Dataset 对象
train_model(train, val) # 调用 lightgbm.train

通过设置 mlflow.lightgbm.autolog(log_datasets=False) 禁用数据集跟踪时，我没有遇到明显的减速。
有人知道为什么会这样吗？情况如何？
我预计不会出现明显的减速，因为 mlflow 仅跟踪数据集的一些基本统计数据（或至少看起来是这样。用户界面和文档并未表明其他情况）]]></description>
      <guid>https://stackoverflow.com/questions/78816040/severe-slowdown-when-using-dataset-logging-autologging-with-lightgbm-in-mlflow</guid>
      <pubDate>Wed, 31 Jul 2024 11:30:39 GMT</pubDate>
    </item>
    <item>
      <title>如何创建边界框以在对象检测中检测不到任何内容？[关闭]</title>
      <link>https://stackoverflow.com/questions/78815620/how-do-i-create-my-bounding-boxes-to-detect-nothing-in-object-detection</link>
      <description><![CDATA[我正在开发一个计算机视觉项目，其中有猫和狗的类别，但我也有一些图像（占我数据集的 50% 以上）既不包含猫也不包含狗。我想教我的模型在这些情况下检测“无”，但我不确定如何为这些输出创建边界框。
我尝试使用带有 torch.tensor([]) 的空张量，但它与其他边界框的大小不同。我还尝试使用 [None, None, None, None] 创建一个边界框，但我不确定它是否会起作用。
我正在考虑创建一个整个图像大小的边界框，例如 [0, 0, 224, 224]，但我想知道这是否是唯一的方法。
我在网上搜索过，但没有找到明确的解决方案。有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78815620/how-do-i-create-my-bounding-boxes-to-detect-nothing-in-object-detection</guid>
      <pubDate>Wed, 31 Jul 2024 09:48:09 GMT</pubDate>
    </item>
    <item>
      <title>如何使用苹果 MLX 框架正确保存微调模型</title>
      <link>https://stackoverflow.com/questions/78815544/how-to-correctly-save-a-fine-tuned-model-using-apple-mlx-framework</link>
      <description><![CDATA[我们使用 MLX 来微调从 hugging face 获取的模型。
from transformers import AutoModel
model = AutoModel.from_pretrained(&#39;deepseek-ai/deepseek-coder-6.7b-instruct&#39;)

我们使用 python -m mlx_lm.lora --config lora_config.yaml 等命令对模型进行了微调，配置文件如下所示：
# 本地模型目录或 Hugging Face repo 的路径。
model: &quot;deepseek-ai/deepseek-coder-6.7b-instruct&quot;
# 训练过的适配器权重的保存/加载路径。
adapter_path: &quot;adapters&quot;

当微调后生成适配器文件时，我们通过类似脚本对模型进行评估
from mlx_lm.utils import *
model,tokenizer = load(path_or_hf_repo =&quot;deepseek-ai/deepseek-coder-6.7b-instruct&quot;,
adapter_path = &quot;adapters&quot; # path to new training adaptor
)
text = &quot;Tell sth about New York&quot;
response = generate(model, tokenizer, prompt=text, verbose=True, temp=0.01, max_tokens=100)

并且它按预期工作。
但是，在我们保存模型并使用 mlx_lm.generate 进行评估后，模型运行不佳。 （该行为与使用 generate(model, tokenizer, prompt=text, verbose=True, temp=0.01, max_tokens=100) 调用模型完全不同。
mlx_lm.fuse --model &quot;deepseek-ai/deepseek-coder-6.7b-instruct&quot; --adapter-path &quot;adapters&quot; --save-path new_model
mlx_lm.generate --model new_model --prompt &quot;Tell sth about New York&quot; --adapter-path &quot;adapters&quot; --temp 0.01
]]></description>
      <guid>https://stackoverflow.com/questions/78815544/how-to-correctly-save-a-fine-tuned-model-using-apple-mlx-framework</guid>
      <pubDate>Wed, 31 Jul 2024 09:35:34 GMT</pubDate>
    </item>
    <item>
      <title>模型选择 [关闭]</title>
      <link>https://stackoverflow.com/questions/78814603/model-selection</link>
      <description><![CDATA[我有一个数据框：

我想训练一个模型，在这个模型中我可以提供数据列 1 和列 2，即系统 A 中的设备名称和警报。该模型应该预测输出并给出列 3 和列 4。显然这只是一个示例数据。如果我的模型可以预测输出，并且如果有多个系统受到影响，它也可以给出输出，那就更好了。
更多解释：
系统 A 以某种方式连接到系统 B，因此如果 A 中出现任何问题，系统 B 可能会出现警报。下面的数据也是如此。
我的问题是我应该为这种类型的数据选择哪种模型，以便获得最大的准确性？]]></description>
      <guid>https://stackoverflow.com/questions/78814603/model-selection</guid>
      <pubDate>Wed, 31 Jul 2024 05:52:58 GMT</pubDate>
    </item>
    <item>
      <title>导入 pywrap_saved_model 时 DLL 加载失败：找不到指定的过程</title>
      <link>https://stackoverflow.com/questions/78814370/dll-load-failed-while-importing-pywrap-saved-model-the-specified-procedure-coul</link>
      <description><![CDATA[我在导入 tflite-model-maker 时遇到问题，
我已经使用 cmd 管理员安装了它，它完全完成了
问题是当我将其导入到我的代码中时
我尝试使用 pip install，完全没有问题，
我不知道，顺便说一下，我使用的是 py 版本 3.8
在此处输入图片说明
在此处输入图片说明
我尝试使用 tensorflow 的 tflite-model-maker 制作音频分类模型
并将其用于我的语音识别项目]]></description>
      <guid>https://stackoverflow.com/questions/78814370/dll-load-failed-while-importing-pywrap-saved-model-the-specified-procedure-coul</guid>
      <pubDate>Wed, 31 Jul 2024 03:52:53 GMT</pubDate>
    </item>
    <item>
      <title>优化序列以最小化涉及求和与序列长度的自定义评分函数</title>
      <link>https://stackoverflow.com/questions/78814217/optimizing-a-sequence-to-minimize-a-custom-score-function-involving-summation-an</link>
      <description><![CDATA[问题定义
我试图找到一个序列 S，使得对于从 1 到 n 的每个数字 i，使用以下公式计算并最小化分数 f(i)：
f(i) = a * |S| + (1-a) * N(S, i)
其中：

|S| 是序列 S 的长度。
N(S, i) 是 S 中加起来达到目标​​ i 所需的最小元素数。序列 S 中的元素可以重复使用。
a 是介于 0 和 1 之间的参数，用于平衡 |S| 的贡献并将 N(S, i) 添加到分数中。

目标是找到这样一个序列 S，使得从 1 到 n 的每个 i 产生的最大分数 f(i) 尽可能低。
我尝试过的方法
我最初尝试使用蛮力方法来生成可能的序列并对其进行评估，但这种方法计算成本高，并且对于较大的数字（例如 n = 20 及以上）不可行。
问题

是否有更有效的算法或方法来解决这个问题，可能使用动态规划或其他优化技术？
是否有类似于此问题结构的已知问题或数学框架，可以指导解决方案的开发？
你们知道如何解决这个问题吗？

想法
我考虑过使用随机近似来近似解决方案，但我无法执行这个想法。
观察：

我不知道如何证明，但{1,2,3,…,N}的序列可以形成1和（1 + N）N / 2之间的任何数字。因此，如果 a=1，则最佳序列应为 {1,2,3,…,x}，其中 argminx (1+x)x/2 &gt;= target

如果 a=0，(1-a)=1，则 N(S,i) 始终为 1，因为我们可以形成一个序列 {1,2,3,…,T}，这样我们就可以仅使用单个数字本身来表示低于目标的任何数字

根据观察 1 和观察 2，我猜测最佳序列应介于 {1,2,…x} 和 {1,2,…,T} 的组合之间，并且总共会有 T-x+1 次这样的迭代。


]]></description>
      <guid>https://stackoverflow.com/questions/78814217/optimizing-a-sequence-to-minimize-a-custom-score-function-involving-summation-an</guid>
      <pubDate>Wed, 31 Jul 2024 02:23:03 GMT</pubDate>
    </item>
    <item>
      <title>如何在嘈杂的回归数据上产生过度拟合？</title>
      <link>https://stackoverflow.com/questions/78814212/how-can-i-generate-overfitting-on-noisy-regression-data</link>
      <description><![CDATA[如何使用 PyTorch 在嘈杂的回归数据上生成过度拟合？尽管进行了各种尝试，但 PyTorch 倾向于泛化数据，这使得记住（过度拟合）数据变得具有挑战性。我修改了权重初始化，使用了 batch_size=1，使学习率变得灵活，并增加了层数和参数数量，但它仍然具有泛化能力。
是否有任何参数需要更改？

def __init__(self):
super().__init__()
inSize = 1
layer = []
for h in hiddenSizes:
layer.append(nn.Linear(inSize, h))
layer.append(nn.ELU()) 
inSize = h 
layer.append(nn.Linear(inSize, 1)) 
self.rede = nn.Sequential(*layers)
#self.apply(self._init_weights)
def _init_weights(self, m):
if isinstance(m, nn.Linear):
#nn.init.uniform_(m.weight, a=-2.0, b=2.0)
#nn.init.kaiming_normal_(m.weight, mode=&#39;fan_in&#39;, nonlinearity=&#39;relu&#39;)
#nn.init.xavier_uniform_(m.weight)
nn.init.normal_(m.weight, mean=0.0, std=1.)
if m.bias is not None:
#nn.init.uniform_(m.bias, a=-2.0, b=2.0)
#nn.init.zeros_(m.bias)
nn.init.normal_(m.bias, mean=0.0, std=1.0)
def forward(self, x): 
return self.rede(x)

hiddenSizes = [500, 500 ,250 , 250, 125] # 层和参数

lossFn = nn.MSELoss()
dataloader = DataLoader(dataset, batch_size=1, shuffle=True) # 我尝试了其他值
optimizer = torch.optim.Adam(model.parameters(), lr = .01)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1300, gamma=.9)
# scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=.01, step_size_up=200, mode=&#39;triangular&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78814212/how-can-i-generate-overfitting-on-noisy-regression-data</guid>
      <pubDate>Wed, 31 Jul 2024 02:16:21 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn 预测需要很长时间</title>
      <link>https://stackoverflow.com/questions/78814028/sklearn-prediction-takes-forever</link>
      <description><![CDATA[我在 sklearn 中的几种常见机器学习方法中遇到了严重的性能问题。我正在研究二元分类问题，数据集包含 500 万个观测值和 100 个特征，使用 sklearn 中的 LogisticRegression()、MLPClassifier()、RandomForestClassifier() 和 LinearSVC() 等模型。
例如，这是我用于 L2 逻辑回归的设置，使用交叉验证从网格 c_grid = [1e-15, 1e-10, 1e-5, 1e-1, 10] 中找到最佳正则化项 C：
lr = LogisticRegression(class_weight=class_weight,
solver=&#39;sag&#39;, # 我还尝试了 &#39;liblinear&#39;
max_iter=10000,
tol=0.1,
random_state=seed,
penalty=&#39;l2&#39;)

C = [1e-15, 1e-10, 1e-5, 1e-1, 10]
c_grid = {&quot;C&quot;: C}
c_grid = {k: v for k, v in c_grid.items() if v is not None}

...

cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True) 
clf = GridSearchCV(estimator=lr, 
param_grid=c_grid, 
scoring=&#39;roc_auc&#39;,
cv=cv, 
return_train_score=True).fit(X_train, Y_train) 
best_model = clf.best_estimator_
prob = clf.predict_proba(X_train)[:, 1]
pred = clf.predict(X_train)

但是，整个训练过程花费了近 20 个小时。对于这种规模的数据集，这是否正常，或者可能是由于参数或设置不正确？例如，我调整了 LogisticRegression 中的各种参数，但似乎都没有改善这种情况。
此外，当我尝试使用 best_model 来计算测试结果时
prob = clf.predict_proba(X_test)[:, 1]
pred = clf.predict(X_test)

这似乎需要很长时间才能完成。我尝试使用类似这样的方法并行化该过程
X_test_batches = np.array_split(X_test, N)
args = [(best_model, batch) for batch in X_test_batches]

with Pool(N) as pool:
prob_batches = pool.map(predict_batch, args)
prob = np.concatenate(prob_batches)
pred = (prob &gt;= 0.5)

但它也没有太大帮助，所以最终我不得不手动实现我自己的预测函数（显然它只适用于逻辑回归，但不适用于我想要测试的其他模型）：
z = np.dot(X_test, best_model.coef_.T) + best_model.intercept_
prob = 1 / (1 + np.exp(-z))

鉴于训练和测试都花费了不合理的长时间，我猜测问题可能出在 clf.predict_proba() 和 clf.predict() 上。不过，我希望 sklearn 能够有效处理包含数百万个观测值的数据集。]]></description>
      <guid>https://stackoverflow.com/questions/78814028/sklearn-prediction-takes-forever</guid>
      <pubDate>Wed, 31 Jul 2024 00:04:22 GMT</pubDate>
    </item>
    <item>
      <title>如何让 Python 脚本在后台运行同时仍与前端交互？[关闭]</title>
      <link>https://stackoverflow.com/questions/78813752/how-to-have-a-python-script-run-in-the-background-while-still-interacting-with-t</link>
      <description><![CDATA[我有这个网站，它允许人们添加新的预测。首先，他们点击一个新的预测和广告数据（csvs），然后选择他们想要使用的机器学习模型，然后它应该运行python代码并将csv输出到具有相关预测参数的数据库。
我的问题是我应该如何运行python代码，因为这个代码可能需要几个小时才能运行，然后我必须考虑许多试图做出新预测的用户。我研究过任务调度和redis。我也听说我可以使用一些AWS服务，但我不确定这里最好的选择是什么，因为我想尽可能地防止内存错误和超时，同时确保代码在稳定的环境中运行。
顺便说一下，我使用flask作为后端并在前端做出反应]]></description>
      <guid>https://stackoverflow.com/questions/78813752/how-to-have-a-python-script-run-in-the-background-while-still-interacting-with-t</guid>
      <pubDate>Tue, 30 Jul 2024 21:37:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 opencv 检测该焊接带中的缺陷（例如孔洞）[关闭]</title>
      <link>https://stackoverflow.com/questions/78813507/how-can-i-detect-defectssuch-as-holes-in-this-welding-strip-using-opencv</link>
      <description><![CDATA[参考图：带孔的焊条：https://i.sstatic.net/VaVQX3th.jpg
正常焊条：https://i.sstatic.net/MBcyyIyp.jpg
我在使用 opencv 检测缺陷（如孔、不均匀性）时遇到问题。我是 opencv 新手，尝试过轮廓检测、边缘检测，但没有得到想要的结果。我想使用 opencv 构建一个算法，检测这些孔并标记它们，而不标记任何其他不必要的东西，这些东西不是缺陷。
这是我在代码中使用的方法
import cv2
import numpy as np
from matplotlib import pyplot as plt

# 加载图像
image_path = &quot;sample weld strip.jpg&quot;
image = cv2.imread(image_path)

# 将图像转换为 HSV 颜色空间
hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

# 定义焊缝条的 HSV 范围
lower_hsv = (1, 1, 1)
upper_hsv = (177, 255, 255)

# 应用 HSV 掩码
mask = cv2.inRange(hsv_image, lower_hsv, upper_hsv)
masked_image = cv2.bitwise_and(image, image, mask=mask)

# 转换为灰度
gray_image = cv2.cvtColor(masked_image, cv2.COLOR_BGR2GRAY)

# 应用高斯模糊
blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)

# 使用 Canny 进行边缘检测
edges = cv2.Canny(blurred_image, 50, 150)

# 查找轮廓
contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 在重要轮廓（孔洞）周围绘制边界框
holes_image = image.copy()
for contour in contours:
area = cv2.contourArea(contour)
if 10 &lt;area &lt; 25：# 根据您的需要调整此阈值
x, y, w, h = cv2.boundingRect(contour)
cv2.rectangle(holes_image, (x, y), (x+w, y+h), (0, 0, 255), 2)

我正在寻求有关此问题的帮助或指导。]]></description>
      <guid>https://stackoverflow.com/questions/78813507/how-can-i-detect-defectssuch-as-holes-in-this-welding-strip-using-opencv</guid>
      <pubDate>Tue, 30 Jul 2024 20:06:30 GMT</pubDate>
    </item>
    <item>
      <title>如何进行布尔分类处理？</title>
      <link>https://stackoverflow.com/questions/78813351/how-to-boolean-categorical-proccessing</link>
      <description><![CDATA[将 pandas 导入为 pd
从 sklearn.impute 导入 SimpleImputer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler、OneHotEncoder、OrdinalEncoder
从 sklearn.pipeline 导入 Pipeline

data = pd.read_csv(&#39;Datasets/StudentScore.csv&#39;)

target = &#39;MathScore&#39;
x = data.drop(data[[target, &#39;Unnamed: 0&#39;]], axis=1)
y = data[target]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

# 数值处理
num_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)),
(&#39;scaler&#39;, StandardScaler())
])

x_train[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]] = num_transformer.fit_transform(x_train[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]])
x_test[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]] = num_transformer.transform(x_test[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]])

# 序数处理
education_levels = [&quot;high school&quot;, &quot;some high school&quot;, &quot;some college&quot;, &quot;associate&#39;s degree&quot;, &quot;bachelor&#39;s degree&quot;,
&quot;master&#39;s degree&quot;]

ord_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encoder&#39;, OrdinalEncoder(categories=[education_levels])),
])

x_train[[&#39;ParentEduc&#39;]] = ord_transformer.fit_transform(x_train[[&#39;ParentEduc&#39;]])
x_test[[&#39;ParentEduc&#39;]] = ord_transformer.transform(x_test[[&#39;ParentEduc&#39;]])

# 名义处理
nom_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encoder&#39;, OneHotEncoder())
])

x_train[[&#39;EthnicGroup&#39;]] = nom_transformer.fit_transform(x_train[[&#39;EthnicGroup&#39;]])
x_test[[&#39;EthnicGroup&#39;]] = nom_transformer.transform(x_test[[&#39;EthnicGroup&#39;]])

# 布尔处理
bool_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encoder&#39;, OneHotEncoder(sparse_output=False)),
])

x_train[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]] = bool_transformer.fit_transform(
x_train[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]])
x_test[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]] = bool_transformer.transform(x_train[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]])

我在尝试创建管道来处理布尔分类特征时遇到错误。具体来说，在训练集和测试集中的特征的 fit_transform 步骤中，我在名义处理和布尔处理部分中收到了
ValueError：列的长度必须与键的长度相同

。如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78813351/how-to-boolean-categorical-proccessing</guid>
      <pubDate>Tue, 30 Jul 2024 19:12:18 GMT</pubDate>
    </item>
    <item>
      <title>优化大数据集上的 Pandas 性能</title>
      <link>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</link>
      <description><![CDATA[我正在使用 pandas 处理一个大型数据集（约 1000 万行和 50 列），在数据操作和分析过程中遇到了严重的性能问题。这些操作包括过滤、合并和聚合数据，目前执行时间太长。
我读过几种优化技术，但不确定哪种技术最有效且适用于我的情况。以下是有关我的工作流程的一些细节：
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台具有 16GB RAM 的机器上运行分析。
社区能否分享优化 pandas 在大型数据集上的性能的最佳实践？
1.内存管理技术。
2.执行 groupby 和 apply 的有效方法。
3.处理大型数据集的 pandas 替代方案。
4. 有没有关于并行处理或有效利用多核的技巧。
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台有 16GB RAM 的机器上运行分析。]]></description>
      <guid>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</guid>
      <pubDate>Tue, 16 Jul 2024 02:24:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在线托管 TensorFlow 模型</title>
      <link>https://stackoverflow.com/questions/76187823/how-to-host-tensorflow-model-online</link>
      <description><![CDATA[我正在尝试使用 TensorFlow Serving 将 ML 模型作为 REST API 提供服务。
我想知道是否有办法在线而不是本地托管模型？
提前谢谢您。
我需要托管一个 ML 模型，在进行预测时，该模型会与字符串 id 进行映射。
该模型是一个 .h5 文件。
该程序在笔记本中运行。但在开发移动应用程序时，我不知道如何进行托管。]]></description>
      <guid>https://stackoverflow.com/questions/76187823/how-to-host-tensorflow-model-online</guid>
      <pubDate>Sat, 06 May 2023 08:09:39 GMT</pubDate>
    </item>
    </channel>
</rss>