<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 25 Jan 2024 15:15:25 GMT</lastBuildDate>
    <item>
      <title>在 Pytorch 中分析给定模型的所有层</title>
      <link>https://stackoverflow.com/questions/77880408/profile-all-layers-of-a-given-model-in-pytorch</link>
      <description><![CDATA[我正在学习使用 Pytorch profiler (https://pytorch.org/ Tutorials/recipes/recipes/profiler_recipe.html）来分析不同的模型。
它与示例配合得非常好。但是当我尝试不同的模型时，输出并不是我所期望的。
我想获取不同层（q_proj、k_proj、v_proj、softmax 等）所花费的时间。查看模型的代码（使用 https://github.com/kingoflolz/mesh -transformer-jax），这些层被定义为普通的pytorch层（nn.Linear（self.embed_dim，self.embed_dim，bias=False），nn.function.softmax等）。
在此类模型中如何使用 Pytorch 的正确方法？我需要修改模型吗？
例如，当使用 Pytorch profiler 和模型 GPT-J 时（来自 https:// Huggingface.co/docs/transformers/model_doc/gptj），我得到以下输出，其中没有显示任何层，但显示其他辅助功能：
&lt;前&gt;&lt;代码&gt;------------------------ ------------ ---------- -- ------------ ------------ ------------ ------------
                  名称 自身 CPU % 自身 CPU CPU 总计 % CPU 总 CPU 时间 平均调用次数
---------------------- ------------ ------------ ---- -------- ------------ ------------ ------------
               正向 90.29% 558.000us 94.34% 583.000us 583.000us 1
           aten::零 5.02% 31.000us 5.66% 35.000us 35.000us 1
          aten::解绑 1.62% 10.000us 2.43% 15.000us 15.000us 1
          aten::分离 0.49% 3.000us 1.29% 8.000us 8.000us 1
          aten::选择 0.65% 4.000us 0.81% 5.000us 5.000us 1
                分离 0.81% 5.000us 0.81% 5.000us 5.000us 1
           aten::空 0.65% 4.000us 0.65% 4.000us 2.000us 2
           aten::zero_ 0.16% 1.000us 0.16% 1.000us 1.000us 1
      aten::as_strided 0.16% 1.000us 0.16% 1.000us 1.000us 1
              aten::至 0.16% 1.000us 0.16% 1.000us 1.000us 1
    aten::resolve_conj 0.00% 0.000us 0.00% 0.000us 0.000us 1
     aten::resolve_neg 0.00% 0.000us 0.00% 0.000us 0.000us 1
---------------------- ------------ ------------ ---- -------- ------------ ------------ ------------
自CPU时间总计：618.000us

我使用的代码是：
导入火炬
导入 torchvision.models 作为模型
从 torch.profiler 导入配置文件、record_function、ProfilerActivity

从 Transformer 导入 AutoModelForCausalLM、AutoTokenizer

模型 = AutoModelForCausalLM.from_pretrained(“EleutherAI/gpt-j-6B”)
tokenizer = AutoTokenizer.from_pretrained(“EleutherAI/gpt-j-6B”)

Prompt = (“令人震惊的发现，科学家发现了一群生活在偏远地区的独角兽，”
           “安第斯山脉中以前未经探索的山谷。更令人惊讶的是“
           “研究人员发现独角兽能说一口流利的英语。”
        ）

input_ids = tokenizer(提示, return_tensors=“pt”).input_ids
gen_tokens = model.generate(input_ids,
                            do_sample=真，
                            温度=0.9，
                            最大长度=100)

将 profile(activities=[ProfilerActivity.CPU], record_shapes=True) 作为教授：
    使用 record_function(“forward”)：
       gen_text = tokenizer.batch_decode(gen_tokens)[0]


print(prof.key_averages().table(sort_by=“cpu_time_total”, row_limit=100))

print(&quot;-----按输入形状分组&quot;)
print(prof.key_averages(group_by_input_shape=True).table(sort_by=“cpu_time_total”, row_limit=10))

prof.export_chrome_trace(“trace.json”)
]]></description>
      <guid>https://stackoverflow.com/questions/77880408/profile-all-layers-of-a-given-model-in-pytorch</guid>
      <pubDate>Thu, 25 Jan 2024 13:38:21 GMT</pubDate>
    </item>
    <item>
      <title>我从头开始构建了我的神经网络，但它没有按预期学习</title>
      <link>https://stackoverflow.com/questions/77880094/i-built-my-nn-from-scratch-but-it-is-not-learning-as-expected</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77880094/i-built-my-nn-from-scratch-but-it-is-not-learning-as-expected</guid>
      <pubDate>Thu, 25 Jan 2024 12:46:29 GMT</pubDate>
    </item>
    <item>
      <title>SHAP特征选择[关闭]</title>
      <link>https://stackoverflow.com/questions/77879894/shap-feature-selection</link>
      <description><![CDATA[形状特征选择是通过计算形状值并将特征重要性最低的特征一一剔除来进行的。
先求特征重要性，去掉重要性最低的特征，然后再求特征重要性，一一去掉最低的特征，这样正确吗？
以ROC-AUC分数作为衡量指标，该值反复下降和上升。所以，我想知道根据最初获得的特征重要性（无需再次计算特征重要性）按照重要性最低的顺序将它们一一删除是否正确。
xgb.fit(X_train, y_train)
解释器_xgb = shap.TreeExplainer(xgb)
shap_values_xgb = 解释器_xgb(X_test)

df_shap_values_xgb = pd.DataFrame(data = shap_values_xgb.values, columns=X_test_xgb.columns)
df_feature_importance_xgb = pd.DataFrame(columns=[&#39;feature&#39;,&#39;importance&#39;])
对于 df_shap_values_xgb.columns 中的 col_xgb：
    important_xgb = df_shap_values_xgb[col_xgb].abs().mean()
    df_feature_importance_xgb.loc[len(df_feature_importance_xgb)] = [col_xgb,importance_xgb]
df_feature_importance_xgb = df_feature_importance_xgb11.sort_values(&#39;重要性&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/77879894/shap-feature-selection</guid>
      <pubDate>Thu, 25 Jan 2024 12:14:10 GMT</pubDate>
    </item>
    <item>
      <title>我可以摆脱这个吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77879676/can-i-get-rid-of-this</link>
      <description><![CDATA[我是编码初学者。
我正在尝试某种第一个机器学习项目，并且发生了此错误。
告诉我如何解决这个问题，“AttributeError: module &#39;pandas&#39; has no attribute &#39;Dataframe&#39;”
还告诉我将来如何解决此类错误：
]]></description>
      <guid>https://stackoverflow.com/questions/77879676/can-i-get-rid-of-this</guid>
      <pubDate>Thu, 25 Jan 2024 11:43:10 GMT</pubDate>
    </item>
    <item>
      <title>如何从 AutoModelForSequenceClassification 重置参数？</title>
      <link>https://stackoverflow.com/questions/77879635/how-to-reset-parameters-from-automodelforsequenceclassification</link>
      <description><![CDATA[目前，要重新初始化 AutoModelForSequenceClassification 的模型，我们可以这样做：
从变压器导入 AutoModel、AutoConfig、AutoModelForSequenceClassification

m =“moussaKam/frugalscore_tiny_bert-base_bert-score”
config = AutoConfig.from_pretrained(m)
model_from_scratch = AutoModel（配置）

model_from_scratch.save_pretrained(“frugalscore_tiny_bert-from_scratch”)

模型 = AutoModelForSequenceClassification(
  “frugalscore_tiny_bert-from_scratch”，local_files_only=True
）

是否有某种方法可以重新初始化模型权重，而不保存使用 AutoConfig 初始化的新预训练模型？
模型 = AutoModelForSequenceClassification(
  “moussaKam/frugalscore_tiny_bert-base_bert-score”，
  local_files_only=真
  reinitialize_weights=True
）

或者类似的东西：
模型 = AutoModelForSequenceClassification(
  “moussaKam/frugalscore_tiny_bert-base_bert-score”，
  local_files_only=真
）

model.reinitialize_parameters()
]]></description>
      <guid>https://stackoverflow.com/questions/77879635/how-to-reset-parameters-from-automodelforsequenceclassification</guid>
      <pubDate>Thu, 25 Jan 2024 11:34:07 GMT</pubDate>
    </item>
    <item>
      <title>获取“运行时错误：计算出的每个通道的填充输入大小：(2)。内核大小：(10)。内核大小不能大于实际输入大小”</title>
      <link>https://stackoverflow.com/questions/77879568/getting-runtimeerror-calculated-padded-input-size-per-channel-2-kernel-siz</link>
      <description><![CDATA[我创建了一个Python代码来将印地语音频文件转换为文本。
我正在使用“theainerd/Wav2Vec2-large-xlsr-hindi”转换模型。
下面是我的代码：
从变压器导入 Wav2Vec2Processor, Wav2Vec2ForCTC
导入声音文件
进口火炬

# 加载模型和处理器
模型 = Wav2Vec2ForCTC.from_pretrained(&quot;theainerd/Wav2Vec2-large-xlsr-hindi&quot;)
处理器 = Wav2Vec2Processor.from_pretrained(“theainerd/Wav2Vec2-large-xlsr-hindi”)

# 加载并准备音频

soundfile.write(“resampled_audio.wav”, 音频, 16000)

音频，sampling_rate = soundfile.read(“resampled_audio.wav”)
输入=处理器（音频，采样率=采样率，return_tensors=“pt”）

# 转录音频
使用 torch.no_grad()：
    输出=模型（**输入）
    Predicted_ids = torch.argmax(outputs.logits, dim=-1)
    转录=处理器.batch_decode(predicted_ids)

# 打印转录的文本
打印（转录[0]）

当我在 jupyter 笔记本中运行此代码时收到错误消息：
运行时错误：计算出的每个通道的填充输入大小：(2)。内核大小：(10)。内核大小不能大于实际输入大小
请帮助我如何修复此运行时错误。
提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/77879568/getting-runtimeerror-calculated-padded-input-size-per-channel-2-kernel-siz</guid>
      <pubDate>Thu, 25 Jan 2024 11:25:21 GMT</pubDate>
    </item>
    <item>
      <title>如何在Python中测量纯分类数据的Huang K-modes聚类模型的性能？</title>
      <link>https://stackoverflow.com/questions/77879280/how-to-measure-performance-of-huang-k-modes-clustering-model-for-pure-categorica</link>
      <description><![CDATA[如何衡量Huang K-modes聚类模型的性能？
我正在运行 Huang K-modes 聚类模型，其中我的主要数据帧都是分类数据（字符串）。如何衡量该模型的性能？
我尝试测量数据矩阵中每一列与簇的调整后兰德分数，然后取平均值，这是正确的，但不确定这对我有任何意义！
如果您能分享您的意见并表示感谢，我将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/77879280/how-to-measure-performance-of-huang-k-modes-clustering-model-for-pure-categorica</guid>
      <pubDate>Thu, 25 Jan 2024 10:43:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 textattack 进行 Bert 攻击并获取 ValueError(f"Failed to import file {args.dataset_from_file}")</title>
      <link>https://stackoverflow.com/questions/77878231/bert-attack-using-textattack-and-getting-valueerrorffailed-to-import-file-arg</link>
      <description><![CDATA[我想在我的预训练模型和数据集上使用文本攻击来生成对抗样本。所以我运行这个命令：
文本攻击攻击 --model-from-file model --dataset-from-file data/data.csv --attack-recipe bert-attack --log-to-txt data/output.txt

但是，我收到此错误
raise ValueError(f“无法导入文件 {args.dataset_from_file}”)
ValueError：无法导入文件 data/data.csv`

我认为这可能是文件路径，所以我使用了绝对路径，但仍然是同样的错误。我的模型文件包含模型和分词器。我还检查了我的 csv 内容，我认为没有任何问题。
我的 csv 内容：
&lt;前&gt;&lt;代码&gt;文本
“我和妻子上周住在芝加哥阿菲尼亚酒店的单间套房，不值得额外付费，一开始就绝对是一场噩梦，因为房间被列为“超大”，而它与我们之前住过的类型完全不同住在酒店，普通客房只小了大约 2 平方英尺，我相信唯一的额外空间是在衣柜里，这不是很有用，除非你打算住在那里，我们有两个儿子和我们在一起，所以我们不得不额外的睡眠空间，我们的大儿子睡在沙发上，多次醒来，抱怨有虫子在四处走动，他一定也很不舒服，因为我白天坐在沙发上，就像一块石头一样，客房服务很糟糕晚上 10 点左右，我花了 47 分钟才给我妻子送来一个枕头，她不得不等待，因为我的家人永远不会再回到这家酒店，我强烈建议您也远离这家酒店
”
“阿菲尼亚芝加哥酒店是我住过的最糟糕的酒店之一，我作为客人受到的待遇如此差劲，当我要求无烟房间时，他们在我的房间里犯了一个错误，前台非常不通融。”预订时，由于某种奇怪的原因，没有服务员，所以我不得不把所有行李搬到电梯上，然后自己沿着长长的走廊到我的房间，如果这不是一次糟糕的住宿，我叫了客房服务，花了一个多小时如果房间里没有空调，我会说，如果您前往芝加哥进行任何类型的商务旅行，那么这次住宿的一切都非常痛苦，我希望您决定不选择这家酒店很惊讶我喜欢芝加哥这个城市，但这次住宿绝对让我的旅行变得非常负面的经历

所以我希望在运行命令后得到对抗性输出，但我遇到了错误。]]></description>
      <guid>https://stackoverflow.com/questions/77878231/bert-attack-using-textattack-and-getting-valueerrorffailed-to-import-file-arg</guid>
      <pubDate>Thu, 25 Jan 2024 07:42:54 GMT</pubDate>
    </item>
    <item>
      <title>在 pyspark 数据帧的 groupby 上实现机器学习算法，然后获得组合结果</title>
      <link>https://stackoverflow.com/questions/77878122/implement-machine-learning-algorithm-on-groupby-from-pyspark-dataframe-and-then</link>
      <description><![CDATA[我尝试在完整的数据帧上实现机器学习算法，下面是代码，但我希望在 groupby 基础上应用该算法，因为我们有不同的组，例如 group_cols=[“col1”，“col2”， “col3”]并且会有不同的组合，因此需要将相似的组分组在一起并对其应用算法并获得具有异常值分数的最终数据帧。
spark = SparkSession.builder.appName(“LOFExample”).getOrCreate()

# 假设您有一个具有功能的 DataFrame &#39;df_actual_final&#39;
feature_columns = [&quot;col5&quot;] # 根据你实际的特征列进行调整
汇编器= VectorAssembler（inputCols = feature_columns，outputCol =“特征”）
df_assembled = assembler.transform(df_actual_final)

# 提取特征作为列表
extract_features_udf = F.udf(lambda 特征: features[0].item(), DoubleType())
df_features = df_assembled.withColumn(“feature_value”, extract_features_udf(col(“features”)))

# 将特征转换为 NumPy 数组
numpy_array = np.array(df_features.select(“feature_value”).collect())


# 训练局部离群因子模型
lof = LocalOutlierFactor(contamination=0.01) # 根据需要调整污染
outlier_scores = lof.fit_predict(numpy_array)
outlier_scores_list = outlier_scores.tolist()
outlier_df = Spark.createDataFrame(enumerate(outlier_scores_list), [“id”, “outlier_scores”])
   
df_assembled_pd = df_assembled.toPandas()
outlier_df_pd=outlier_df.toPandas()
df_concat = pd.concat([df_assembled_pd, outlier_df_pd], axis=1)
result_df=spark.createDataFrame(df_concat)

result_df = result_df.withColumn(“local_outlier_flag”, when(col(“outlier_scores”) ==-1, 1).otherwise(0))

]]></description>
      <guid>https://stackoverflow.com/questions/77878122/implement-machine-learning-algorithm-on-groupby-from-pyspark-dataframe-and-then</guid>
      <pubDate>Thu, 25 Jan 2024 07:18:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 Top-N 特征方法去除特征的随机森林分类器</title>
      <link>https://stackoverflow.com/questions/77877253/random-forest-classifier-removing-features-using-top-n-features-method</link>
      <description><![CDATA[我正在尝试使用随机森林分类器来预测 NBA 比赛的获胜者。我试图删除和修改我的功能列表，以便提高准确性并减少噪音。
我实现了此处找到的解决方案：https:// datascience.stackexchange.com/questions/57697/decision-trees-should-we-discard-low-importance-features，其中我将循环遍历前 N 个最重要的特征并绘制出最终的准确性。在我的所有功能都经过该循环之后，我留下了一个如下所示的图：

正如您所看到的，生成的图表有点乱。我是否要删除具有负斜率的要素？或者说删除特征的门槛是多少？有没有更好的方法来计算噪声？鉴于我有如此多的特征，并且对训练数据上的模型准确性产生如此多的影响，我如何获得最准确的模型？]]></description>
      <guid>https://stackoverflow.com/questions/77877253/random-forest-classifier-removing-features-using-top-n-features-method</guid>
      <pubDate>Thu, 25 Jan 2024 02:32:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Flux `withgradient` 计算的损失与我计算的不匹配？</title>
      <link>https://stackoverflow.com/questions/77876955/why-doesnt-the-loss-calculated-by-flux-withgradient-match-what-i-have-calcula</link>
      <description><![CDATA[我正在尝试使用 Flux 训练一个简单的 CNN，但遇到了一个奇怪的问题...在训练过程中，损失似乎下降了（表明它正在工作），但尽管损失曲线表明“训练过的”模型仍然有效，模型输出非常糟糕，当我手动计算损失时，我注意到它与训练表明的结果不同（它表现得好像根本没有经过训练）。
然后我开始计算梯度内部与外部返回的损失，经过大量挖掘，我认为问题与 BatchNorm 层有关。考虑以下最小示例：
使用 Flux
x = rand(100,100,1,1) #假设一个灰度图像 100x100，具有 1 个通道（灰度）和 1 个批次
y = @。 5*x + 3 #输出图像，与输入值的一些关系（与此无关）
m = Chain(BatchNorm(1),Conv((1,1),1=&gt;1)) #非常简单的模型（实际上没有做任何事情，但说明了问题）
l_init = Flux.mse(m(x),y) #模型创建后的初始损失
l_grad, grad = Flux.withgradient(m -&gt; Flux.mse(m(x),y), m) #梯度计算的loss
l_final = Flux.mse(m(x),y) #使用模型再次计算损失（没有更新参数）
println(&quot;初始损失：$l_init&quot;)
println(&quot;用梯度计算的损失：$l_grad&quot;)
println(&quot;最终损失：$l_final&quot;)

上面所有的损失都会有所不同，有时会非常显着（刚才运行时我得到了 22.6、30.7 和 23.0），而我认为它们应该是相同的？
有趣的是，如果我删除 BatchNorm 层，输出都是相同的，即运行：
使用 Flux
x = rand(100,100,1,1) #假设一个灰度图像 100x100，具有 1 个通道（灰度）和 1 个批次
y = @。 5*x + 3 #输出图像
m = 链(Conv((1,1),1=&gt;1))
l_init = Flux.mse(m(x),y) #模型创建后的初始损失
l_grad, grad = Flux.withgradient(m -&gt; Flux.mse(m(x),y), m)
l_final = Flux.mse(m(x),y)
println(&quot;初始损失：$l_init&quot;)
println(&quot;用梯度计算的损失：$l_grad&quot;)
println(&quot;最终损失：$l_final&quot;)

每次损失计算都会产生相同的数字。
为什么包含 BatchNorm 层会像这样改变损失值？
我（有限）的理解是，这只是为了标准化输入值，我知道这可能会影响非标准化和标准化情况之间的损失，但我不明白为什么它会产生不同的损失值同一模型上的相同输入值，而没有更新该模型的任何参数？]]></description>
      <guid>https://stackoverflow.com/questions/77876955/why-doesnt-the-loss-calculated-by-flux-withgradient-match-what-i-have-calcula</guid>
      <pubDate>Thu, 25 Jan 2024 00:30:43 GMT</pubDate>
    </item>
    <item>
      <title>变压器架构的输入大小问题[关闭]</title>
      <link>https://stackoverflow.com/questions/77876582/transformer-architectures-input-size-problem</link>
      <description><![CDATA[我读了“你所需要的就是注意力”论文，描述了变压器的架构。在 Transformer 中，有一个叫做 masked multi-head Attention 的组件，仅在解码器部分使用。
问题是，解码器的输入是编码器的输出以及之前生成的标记。并且之前每次迭代生成的token数量不同，但是线性层的神经元数量是相同的。因此，我们必须使用“pad tokens”来实现。屏蔽注意力用于对这些 pad token 给予 0 注意力。
编码器也是如此。输入可以是不同的大小，所以我们还必须使用填充令牌，但在这里，我们不使用屏蔽注意力，我很好奇，为什么？
或者我们不在那里使用填充令牌，而是使用其他东西？]]></description>
      <guid>https://stackoverflow.com/questions/77876582/transformer-architectures-input-size-problem</guid>
      <pubDate>Wed, 24 Jan 2024 22:28:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyTorch 训练 VGG16 模型进行图像分类</title>
      <link>https://stackoverflow.com/questions/77872605/training-the-vgg16-model-for-image-classification-using-pytorch</link>
      <description><![CDATA[我正在使用 PyTorch 进行图像分类。
我编写了以下适用于简单线性模型的训练函数：
criterion = nn.CrossEntropyLoss()
def train（模型、数据加载器、纪元）：
模型.to（设备）
优化器 = torch.optim.Adam(model.parameters(), lr=1e-3)
运行损失，运行加速 = 0., 0.
损失历史记录 = []
precision_history = [](data_train):.2f}%&quot;)

对于范围内的 i(1, 纪元 + 1)：
  模型.train()
  对于输入，数据加载器中的目标：
      输入，目标 = 输入.to(设备), 目标.to(设备)
      输出 = 模型（输入）
      损失=标准（输出，目标）
      优化器.zero_grad()
      loss.backward()
      优化器.step()
      preds = torch.argmax(输出, 1)
      running_loss += loss.item()
      running_acc += torch.sum(preds == Targets).item()

  print(f&quot;[TRAIN epoch {i}] 损失: {running_loss/len(data_train):.2f} Acc: {100 * running_acc/len
 

我有预训练的 VGG16 模型，我想更改其最后一层的权重：
model_vgg = models.vgg16(weights=&#39;DEFAULT&#39;)
model_vgg.classifier[6] = nn.Linear(4096, 2)

对于 model_vgg.parameters() 中的参数：
    param.requires_grad = False
model_vgg.classifier[-1].requires_grad = True

火车（model_vgg，train_loader，2）

但是，在训练时出现以下错误：
RuntimeError Traceback（最近一次调用最后一次）

&lt;定时评估&gt;在&lt;模块&gt;中

&lt;ipython-input-27-1f64686a5cfd&gt;火车中（模型、数据加载器、纪元）
     39 损失 = 标准（输出，目标）
     40 优化器.zero_grad()
---&gt; 41loss.backward()
     42 优化器.step()
     43 preds = torch.argmax(输出, 1)

/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py 向后（张量，grad_tensors，retain_graph，create_graph，grad_variables，输入）
--&gt; 251 Variable._execution_engine.run_backward( # 调用 C++ 引擎来运行向后传递
    252个张量，
    第253章

RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77872605/training-the-vgg16-model-for-image-classification-using-pytorch</guid>
      <pubDate>Wed, 24 Jan 2024 11:21:50 GMT</pubDate>
    </item>
    <item>
      <title>梯度消失会导致“没有为任何变量提供梯度”</title>
      <link>https://stackoverflow.com/questions/77870522/can-vanishing-gradients-cause-no-gradients-provided-for-any-variable</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77870522/can-vanishing-gradients-cause-no-gradients-provided-for-any-variable</guid>
      <pubDate>Wed, 24 Jan 2024 04:01:23 GMT</pubDate>
    </item>
    <item>
      <title>相关矩阵的累积 AOC 计算</title>
      <link>https://stackoverflow.com/questions/77830147/cumulative-aoc-calculation-for-a-correlation-matrix</link>
      <description><![CDATA[我正在使用一个非常简单的数据集（胎儿健康分类）进行练习，使用支持向量机、相关性指标和典型模型指标（没什么特别的）进行一些练习。我想做以下事情：

采用（与目标）最相关的变量并计算 SVM 模型；然后保留 AUC 结果。
采用第二个最相关的变量（与目标）并使用第一个和第二个变量，计算 SVM 模型；然后保留 AUC 结果。
依此类推......直到到达最后一个变量

之后，我需要创建一个显示以下信息的图表：

X轴：累计变量数
Y 轴：模型中包含的每个变量数量对应的 AUC

我有以下代码；我认为这是合理的。然而，它被卡住了。我不得不中断迭代，因为它们似乎没有结束。关于如何修复循环有什么建议吗？
**导入参考文件的一些行**

df = pd.read_csv(&quot;ASI_casoPractico.csv&quot;, sep = &quot;;&quot;)

# 导入库

将 pandas 导入为 pd
从 sklearn.svm 导入 SVC
从 sklearn.metrics 导入 roc_auc_score
从 sklearn.model_selection 导入 train_test_split
将 matplotlib.pyplot 导入为 plt

# 相关矩阵

corr_matrix = df.corr().abs()
排序校正=
corr_matrix[&#39;目标&#39;].sort_values(升序=False)

# 创建按相关性排序的变量列表

Sorted_vars = Sorted_corr.index.tolist()

# 为结果创建空列表

结果=[]

# 使用变量进行迭代并使用 SVM 生成模型

对于范围内的 i(1, len(sorted_vars) + 1)：

  # 选择相关性最好的变量
  选定的变量 = 排序的变量[:i]

  # 训练和测试的数据分开

  X_train = df[selected_vars]
  y_train = df[&#39;目标&#39;]

  # 训练支持向量机

  svm = SVC(内核=&#39;线性&#39;, 概率=True)
  svm.fit(X_train, y_train)

  # 计算AUC
  y_pred = svm.predict_proba(X_train)[:, 1]
  auc = roc_auc_score(y_train, y_pred)

  # 将值添加到列表中
  结果.append([i, auc])

# 为结果创建数据框
results_df = pd.DataFrame(结果, columns=[&#39;变量&#39;, &#39;AUC&#39;])

# 图
results_df.plot(x=&#39;变量&#39;, y=&#39;AUC&#39;, kind=&#39;线&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/77830147/cumulative-aoc-calculation-for-a-correlation-matrix</guid>
      <pubDate>Wed, 17 Jan 2024 05:28:50 GMT</pubDate>
    </item>
    </channel>
</rss>