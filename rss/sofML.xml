<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 02 Apr 2024 21:12:59 GMT</lastBuildDate>
    <item>
      <title>当重新训练循环神经网络模型时，val_loss比loss大得多</title>
      <link>https://stackoverflow.com/questions/78263413/when-retraining-a-recurrent-neural-network-model-val-loss-is-much-bigger-than-l</link>
      <description><![CDATA[我想使用循环神经网络来分析时间序列。我第一次使用常规正弦函数的时间序列。我想构建一个模型并在后续数据上对其进行测试。
将 numpy 导入为 np
将张量流导入为 tf
从 sklearn.model_selection 导入 train_test_split

# Подготовка данных
x = np.arange(1, 100001)
y = np.sin(0.01*x)

x = x.reshape(-1, 1, 1)

x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)
模型 = tf.keras.Sequential([
    tf.keras.layers.Embedding（input_dim = 200001，output_dim = 64），
    tf.keras.layers.Conv1D(filters=64，kernel_size=3，padding=&#39;same&#39;，activation=&#39;relu&#39;),
    tf.keras.layers.Dropout(0.3),
    
    tf.keras.layers.LSTM(128, return_sequences=True),
    tf.keras.layers.Dropout(0.3),
    
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dropout(0.3),
    
    tf.keras.layers.Dense(1)
]）
#model.compile(优化器=‘adam’,loss=‘mean_squared_error’)

# Компиляция модели
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)


model.fit（x_train，y_train，epochs = 60，batch_size = 1024，validation_data =（x_val，y_val））# Обучаем модель на данных x и y

结果，我的模型被重新训练。
纪元 12/60
79/79 [================================] - 1s 9ms/步 - 损耗：0.0140 - val_loss：0.5431
13/60 纪元
79/79 [================================] - 1s 9ms/步 - 损耗：0.0132 - val_loss：0.5430
14/60 纪元
79/79 [================================] - 1s 9ms/步 - 损耗：0.0125 - val_loss：0.5427

但是val_loss比loss大得多。我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78263413/when-retraining-a-recurrent-neural-network-model-val-loss-is-much-bigger-than-l</guid>
      <pubDate>Tue, 02 Apr 2024 18:41:45 GMT</pubDate>
    </item>
    <item>
      <title>Autograd 返回 None</title>
      <link>https://stackoverflow.com/questions/78263388/autograd-returning-none</link>
      <description><![CDATA[我正在尝试创建一个收缩自动编码器，我在几篇论文中读到，其主要思想是使用编码器输出相对于其输入的雅可比行列式的范数。
换句话说，我试图在使用原始输入的同时获取编码器输出的梯度。
到目前为止，我有这样的事情：
梯度 = torch.autograd.grad(输出 = Latent_X, 输入 = X, grad_outputs = torch.ones_like(latent_X),
                                create_graph = True，allow_unused = True）[0]

print(渐变) # 无！
frobenius_norm = torch.mean(torch.norm(梯度, p = &#39;fro&#39;, dim = (1,2)))

Contractive_penalty = self.args[&#39;lambda&#39;] * frobenius_norm
总损失 += 收缩惩罚

计算第一行中的梯度时会出现问题。由于某种原因，它返回 None，所以我首先尝试的是查看数据是什么样的。
输入数据X：(32 x 2866)
张量([[0.4663, 0.3859, 0.6573, ..., 0.7819, 0.0822, 0.3332],
        [0.4204, 0.8448, 0.6168, ..., 0.2698, 0.3503, 0.3372],
        [0.6329, 0.4084, 0.7437, ..., 0.3490, 0.4902, 0.8333],
        ...,
        [0.3004, 0.6908, 0.7698, ..., 0.8115, 0.9253, 0.1996],
        [0.6895, 0.6812, 0.4595, ..., 0.8959, 0.6600, 0.5660],
        [0.5647, 0.2448, 0.5046, ..., 0.6494, 0.4483, 0.5269]],
       device=&#39;cuda:0&#39;, grad_fn=)

编码器的输出：(32 x 32)
张量([[0.3837, 0.3975, 0.5000, ..., 0.6480, 0.9503, 0.8660],
        [0.4182, 0.5000, 0.8683, ..., 0.4916, 0.7044, 0.5293],
        [0.5000, 0.7034, 0.5588, ..., 0.3750, 0.5000, 0.5000],
        ...,
        [0.4598, 0.4478, 0.9167, ..., 0.8179, 0.7026, 0.5000],
        [0.5000, 0.5370, 0.4786, ..., 0.4529, 0.3132, 0.4245],
        [0.4134, 0.5000, 0.4898, ..., 0.4799, 0.5000, 0.7334]],
       device=&#39;cuda:0&#39;, grad_fn=)

所以这两个张量确实有一个计算图...所有这些张量也将 requires_grad_ 设置为 True。
关于自动编码器架构，它只是一个又一个的线性层，我的前向函数如下所示：
defforward(self, x)：
    编码 = self.encoder(x)
    解码 = self.decoder(编码)
    返回解码后的、编码后的

至于训练部分：
x_batch = torch.tensor(tr_model.X_train[b]).to(self.device)

x_batch.requires_grad_(True)
x_batch.retain_grad()

# h 是编码器的输出
x_pred_batch, h = tr_model.model.forward(x_batch)

# 这里我们计算收缩损失
损失 = tr_model.compute_model_loss(x_pred_batch, x_batch, h)

如果有帮助，当我使用普通自动编码器执行 loss.backward() 时，一切正常。 （虽然我在那里不使用编码器的输出）
非常感谢您阅读本文！！]]></description>
      <guid>https://stackoverflow.com/questions/78263388/autograd-returning-none</guid>
      <pubDate>Tue, 02 Apr 2024 18:36:19 GMT</pubDate>
    </item>
    <item>
      <title>如何修复此错误没有名为“llama_index.llms.llama_cpp”的模块</title>
      <link>https://stackoverflow.com/questions/78263004/how-to-fix-this-error-no-module-named-llama-index-llms-llama-cpp</link>
      <description><![CDATA[我尝试将 mixtral-8x7b 与我自己的数据一起使用，但没有成功。这是我的代码
导入火炬
从 llama_index.llms.llama_cpp 导入 LlamaCPP
从 llama_index.llms.llama_cpp.llama_utils 导入 messages_to_prompt、completion_to_prompt
llm = 骆驼CPP(
    model_url=None, # 我们将在本地加载。
    model_path=&#39;./Models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf&#39;, # 4 位模型
    温度=0.1，
    max_new_tokens=1024, # 增加以支持更长的响应
    context_window=8192, # Mistral7B 有一个 8K 上下文窗口
    生成_kwargs={},
    # 至少设置为 1 才能使用 GPU
    model_kwargs={“n_gpu_layers”: 40}, # 40 对于 RTX 3090 来说是一个很好的层数，如果您的 VRAM 少于 24GB，您可能需要减少层数
    messages_to_prompt=messages_to_prompt,
    completion_to_prompt=completion_to_prompt,
    详细=真
）

这给出了错误“没有名为“llama_index.llms.llama_cpp”的模块”。
我已经安装了 llama_index，使用了我的 MAC Mini 以及 Google Colab 的 GPU
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78263004/how-to-fix-this-error-no-module-named-llama-index-llms-llama-cpp</guid>
      <pubDate>Tue, 02 Apr 2024 17:10:19 GMT</pubDate>
    </item>
    <item>
      <title>从各种语言到英语的机器翻译[关闭]</title>
      <link>https://stackoverflow.com/questions/78262917/machine-translation-from-various-languages-into-english</link>
      <description><![CDATA[我对使用机器翻译模型将不同语言（例如西班牙语）翻译成英语的可能性感兴趣。我花了几个小时在互联网上搜索，但我设法想出的只是以下代码：
model_name_marianMT = “Helsinki-NLP/opus-mt-es-en”
tokenizer_marianMT = MarianTokenizer.from_pretrained(model_name_marianMT)

text_ES =“”“这是西班牙语中的示例。 Les deseo a todos un buen día。”“”
marianMT_model = MarianMTModel.from_pretrained(model_name_marianMT)

tokenized_source_text=tokenizer_marianMT(text_ES, return_tensors=“pt”, padding=True)
Translation_tokenized_text = marianMT_model.generate(**tokenized_source_text)

translated_text_marianMT = tokenizer_marianMT.decode(translated_tokenized_text[0],skip_special_tokens=True)
打印（translated_text_marianMT）

上面的代码适用于 MarianMT 模型，但不幸的是，以下代码不适用于 mBART：
从转换器导入 MBartForConditionalGeneration、MBart50Tokenizer

model_name =“facebook/mbart-large-50-many-to-many-mmt”；
模型 = MBartForConditionalGeneration.from_pretrained(model_name)
tokenizer = MBart50Tokenizer.from_pretrained(model_name)

spanish_text = [&quot;Esta es una oración de ejemplo en español. Les deseo a todos un buen día。”]

输入 = tokenizer(spanish_text, return_tensors=“pt”, max_length=1024, truncation=True)

translated_ids = model.generate(**输入，forced_bos_token_id=tokenizer.lang_code_to_id[“en_XX”])
translated_text = tokenizer.decode(translated_ids[0],skip_special_tokens=True)

print(&quot;翻译后的文本：&quot;,translated_text)

我也尝试过使用T5模型，但也失败了。
我想测试其他模型，例如 mBart 或 T5（或其他值得考虑的模型），但经过长时间的搜索，我无法确定是否可行。如果是这样，请给我任何提示。]]></description>
      <guid>https://stackoverflow.com/questions/78262917/machine-translation-from-various-languages-into-english</guid>
      <pubDate>Tue, 02 Apr 2024 16:55:29 GMT</pubDate>
    </item>
    <item>
      <title>下采样数据的评估指标</title>
      <link>https://stackoverflow.com/questions/78262276/evaluation-metrics-for-downsampled-data</link>
      <description><![CDATA[我有原始和下采样的结果。例如，原始和 20% 下采样结果采用具有 30 个值的数组格式。

由于它们的尺度相差很大，如何比较或找出它们之间的误差？
除了统计指标之外，还有什么可以比较的吗？ （绘图、RMSE、MAPE）
]]></description>
      <guid>https://stackoverflow.com/questions/78262276/evaluation-metrics-for-downsampled-data</guid>
      <pubDate>Tue, 02 Apr 2024 15:07:49 GMT</pubDate>
    </item>
    <item>
      <title>尽管我在最后一个密集层使用了 Sigmoid 函数，但 Tensorflow 二元分类仅预测 1</title>
      <link>https://stackoverflow.com/questions/78261873/tensorflow-binary-classification-predict-just-1-although-i-use-sigmoid-function</link>
      <description><![CDATA[我创建了一个二元分类模型来训练猫与狗数据集。我使用 RMSprop 进行优化，使用二元交叉熵作为损失函数。经过训练的模型后，当我尝试预测图像时，预测矩阵一致地给出了 [[1.]]。尽管我的最后一个密集层使用了 sigmoid 函数。这是我的模型：
def create_model():

  模型 = tf.keras.models.Sequential([
      tf.keras.layers.Conv2D(16, (3,3), 激活=&#39;relu&#39;, input_shape=(150,150,3)),
      tf.keras.layers.MaxPooling2D(2,2),
      tf.keras.layers.Conv2D(32, (3,3), 激活=&#39;relu&#39;),
      tf.keras.layers.MaxPooling2D(2,2),
      tf.keras.layers.Conv2D(64, (3,3), 激活=&#39;relu&#39;),
      tf.keras.layers.MaxPooling2D(2,2),

      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense（512，激活=&#39;relu&#39;），
      tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;),
  ]）

  model.compile(优化器=&#39;RMSprop&#39;,
                  损失=&#39;binary_crossentropy&#39;,
                  指标=[&#39;准确性&#39;])


  返回模型

&lt;前&gt;&lt;代码&gt;模型 = create_model()

# 训练模型
# 请注意，这可能需要一些时间。
历史 = model.fit(train_generator,
                    纪元=15，
                    详细=1，
                    验证数据=验证生成器）

然后对单个图像进行预测
&lt;前&gt;&lt;代码&gt;
从 keras.preprocessing 导入图像
将 numpy 导入为 np

img = image.load_img(“/tmp/cats-v-dogs/test/dogs/dog2.jpg”, target_size=(150,150))
x = image.img_to_array(img)
x = np.expand_dims(x, 轴=0)
图像 = np.vstack([x])
类= model.predict（图像，batch_size = 10）
打印（类[0]）


1/1 [================================] - 0s 18ms/步
[1.]
这里有什么问题吗？这就是我问的原因。]]></description>
      <guid>https://stackoverflow.com/questions/78261873/tensorflow-binary-classification-predict-just-1-although-i-use-sigmoid-function</guid>
      <pubDate>Tue, 02 Apr 2024 14:05:24 GMT</pubDate>
    </item>
    <item>
      <title>在另一个类中通过该对象的 ID 使用该对象</title>
      <link>https://stackoverflow.com/questions/78261679/use-an-object-by-the-id-of-that-in-another-class</link>
      <description><![CDATA[在“Random_forest.process()”中创建“Decision_Tree”对象并给出对象的 id，加上“self.id_estimators”
接下来，当我们从“Random_forest.make_predict()”中的 id 返回对象时，我们会收到错误
Random_forest 类的一部分：
类 Random_Forest:

    def __init__(self, min_sample=1, n_estimators=1, max_depth=5):
        self.id_estimators = 无
        self.n_estimators = n_estimators
        self.最大深度 = 最大深度
        self.min_sample = min_sample
        self.max_features = 无
        self.data = None # 数据
        self.target_data = None # 目标特征
        self.columns = 无

    def fit_model(self, X_tr, y_tr):
        self.data, self.target_data = X_tr, y_tr
        self.columns = self.data.columns
        self.max_features = int(sqrt(len(self.data.columns)))
        ＃ 输入（）

        self.id_estimators = self.process_estimator()
        打印（self.id_estimators）

    def process_estimator（自身）：
        id_列表 = []

        columns_selected = self.pick_random()
        X_tr = self.data[columns_selected]
        X_tr_sample, X_tst_sample, y_tr_sample, y_tst_sample = train_test_split(X_tr, self.target_data,
                                                                                    测试大小=0.2，随机状态=42）
        估计器 = Decision_Tree(min_sample=self.min_sample, max_depht=self.max_深度)

        estimator.fit_model(X_tr_sample, y_tr_sample)

        id_list.append(id(估计器))

        返回id_list

    def pick_random(自身):

        index_list = [i for i in range(len(self.columns))]
        所选列 = []
        对于范围内的 i(self.max_features)：
            col_index = random.choice(index_list)
            index_list.remove(col_index)

            columns_selected.append(self.columns[col_index])
        返回所选列

    def make_predict(self, X_tst):
        预测列表 = []
        # 遍历测试数据行
        对于 X_tst.iloc 中的 i：
            对于 self.id_estimators 中的 j：
                q = self.return_obj_from_id(j)
                打印（q.data）
                输入（）

    @静态方法
    def return_obj_from_id(id2):
        返回 ctypes.cast(id2, ctypes.py_object).value

错误：
回溯（最近一次调用最后一次）：
  文件“C:\Users\Ashil-Rayan\PycharmProjects\Random_forest\main.py”，第 16 行，在  中
    model.make_predict(X_test)
  文件“C:\Users\Ashil-Rayan\PycharmProjects\Random_forest\random_forest.py”，第 60 行，在 make_predict 中
    打印（q.data）
AttributeError：“Decision_Tree”对象没有属性“data”


进程已完成，退出代码为 -1073741819 (0xC0000005)


如果您想查看完整的代码：
GitHub
感谢您的回答]]></description>
      <guid>https://stackoverflow.com/questions/78261679/use-an-object-by-the-id-of-that-in-another-class</guid>
      <pubDate>Tue, 02 Apr 2024 13:30:41 GMT</pubDate>
    </item>
    <item>
      <title>MNIST 中 0 的误报太多：使用 Logistic 回归进行多类分类 [关闭]</title>
      <link>https://stackoverflow.com/questions/78260773/way-too-many-false-positives-for-0-in-mnist-multiclass-classification-using-log</link>
      <description><![CDATA[我编写了一个基本上使用 MNIST 数据集的代码，并使用逻辑回归对 0 到 9 的数字进行分类。我使用了多个逻辑回归单元并单独训练它们，保存它们的权重，然后找到每个逻辑单元的预测标签。然后我简单地取给出最大值的逻辑单元，并将与该逻辑单元对应的数字视为预测数字
但是当我看到混淆矩阵时，它在第一列中显示了许多误报，即大多数逻辑单元进行了许多错误分类，认为它们为零。这是我获得的混淆矩阵（附为照片）：
混淆矩阵
我不确定我在这里做错了什么。我将在此处留下笔记本的链接：
https://colab.research.google.com/drive/1rDCW4Tpf4AMLzjxYjTrqv4c1_e0oOY75?usp=sharing
我尝试使用 MNIST 数据集实现逻辑回归并执行多类分类。
我在 Kaggle 中浏览了一下，发现了类似的东西，但他们的混淆矩阵结果是这样的：
链接到在 MNIST 分类器上使用逻辑回归实现的多类分类：
https://www.kaggle.com/code/hamzaboulahia/logistic-regression-mnist-classification]]></description>
      <guid>https://stackoverflow.com/questions/78260773/way-too-many-false-positives-for-0-in-mnist-multiclass-classification-using-log</guid>
      <pubDate>Tue, 02 Apr 2024 10:57:45 GMT</pubDate>
    </item>
    <item>
      <title>如何有效地使用神经网络进行回归？</title>
      <link>https://stackoverflow.com/questions/78260668/how-can-i-effectively-use-a-neural-net-for-regression</link>
      <description><![CDATA[我有训练数据和相应的目标数据来训练。我的数据的形状是 [1,272] - 只是 272 个值的数组。我有少量数据可供使用，但我相信这是可行的。
我的目标是捕获第一个数组中的每个值与目标数组中相同索引处的值之间的关系。
例如：使用 X: [1,5,3] Y: [2,10,6] 训练的网络将预测输入：[5, 10, 20]，预测 [10,20,40] 
在我的例子中，这些值将具有非线性关系
我的最终目标是从每个值的直接邻居中获取一些影响，为此我尝试了 conv1d 层并向后移动。
当我确实有一个模型训练并实现相对较低的损失时，当我尝试使用新的、未见过的数组进行预测时，无论输入如何，结果通常都非常接近。
我假设我在这里尝试的东西有一个名称，也许还有一条不同的路线（不是神经网络）。即使是一些正确的术语也会对我有很大帮助 - 即使谷歌也不知所措。
我正在使用tensorflowjs。我尝试过：
尝试1
仅使用 2 个密集层，均具有 272 个单元，其中一个使用 tf.eye(272) 进行权重，第二个我让权重自然初始化。尝试使用可训练和不可训练的第一层。目标是迫使模型在此处找到相应值之间的某种关系，而不受数组中远处值的影响
结果 1
可训练密集层的权重显然会受到各个方面的影响，以实现最低的损失。
尝试2
使用 conv1d 层（填充：“相同”、kernelSize：1、stride：1、filters：272），后跟不可训练的密集层，如前所述。
结果 2
无论配置/激活功能如何，损失都不会下降。
在这些架构中，我尝试了激活函数的所有组合，并将一层或另一层设置为不可训练，useBias：true/false等。尽管我感觉我在这里走错了路。]]></description>
      <guid>https://stackoverflow.com/questions/78260668/how-can-i-effectively-use-a-neural-net-for-regression</guid>
      <pubDate>Tue, 02 Apr 2024 10:36:31 GMT</pubDate>
    </item>
    <item>
      <title>我在尝试计算 PGN 的最终距离时遇到问题，IndexError：索引 695 超出尺寸 695 的维度 1 的范围</title>
      <link>https://stackoverflow.com/questions/78259116/i-am-facing-problem-while-trying-to-calculate-final-dist-for-pgn-indexerror-in</link>
      <description><![CDATA[我从代码中得到的错误是
回溯（最近一次调用）：
  文件“/Users/sagar/PycharmProjects/TouchFYP/V2_main.py”，第 201 行，在  中
    火车（模型，train_loader，val_loader，优化器，标准，调度器，设备，5，）
  文件“/Users/sagar/PycharmProjects/TouchFYP/V2_main.py”，第 167 行，列车中
    损失，_ = 模型（文章，ext_enc_inp，摘要[:, :-1]，摘要[:, 1:]，teacher_forcing_ratio=0.5）
  文件“/Users/sagar/PycharmProjects/TouchFYP/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1511 行，位于 _wrapped_call_impl 中
    返回 self._call_impl(*args, **kwargs)
  文件“/Users/sagar/PycharmProjects/TouchFYP/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1520 行，位于 _call_impl 中
    返回forward_call(*args, **kwargs)
  文件“/Users/sagar/PycharmProjects/TouchFYP/my_V2.py”，第 283 行，向前
    Final_dist = self.get_final_distribution(enc_input_ext, p_gen, p_vocab, attn, self.max_oov,
  文件“/Users/sagar/PycharmProjects/TouchFYP/my_V2.py”，第 359 行，位于 get_final_distribution
    attn_dist_extended[b, vocab_idx] +=attention_weighted[b, idx]
IndexError：索引 695 超出尺寸 695 的维度 1 的范围

这就是我的代码：
 defforward(self, enc_input, enc_input_ext, dec_input, target=None, Teacher_forcing_ratio=0.5):
        enc_output, enc_hidden = self.encoder(enc_input)

        dec_hidden = enc_hidden
        batch_size, seq_len = dec_input.size()
        输出 = torch.zeros(batch_size, seq_len, self.vocab_size).to(
            enc_input.device) # 相应调整 self.vocab_size

        如果自我训练：
            损失=0
            for t in range(seq_len - 1): # 假设 dec_input 包含 ; &lt;eos&gt;
                dec_input_t = dec_input[:, t].unsqueeze(1)
                true_output = 目标[:, t + 1]
                p_vocab，dec_hidden，p_gen = self.decoder（dec_input_t，dec_hidden，enc_output）
                context_vector, attn = self.attention(dec_hidden, enc_output)
                Final_dist = self.get_final_distribution(enc_input_ext, p_gen, p_vocab, attn, self.max_oov) # 简化

                # Final_dist = self.get_final_distribution(enc_input_ext, p_gen, p_vocab, attn, self.max_oov) # 简化

                损失+= f.cross_entropy(final_dist, true_output,ignore_index=self.pad_token_id)
            回波损耗 / (seq_len - 1)
        别的：
            生成的令牌 = []
            dec_input_t = torch.tensor([[self.sos_token_id]] * batch_size,
                                       device=enc_input.device) # 以  开头代币
            对于范围内的 t(self.max_length)：
                p_vocab，dec_hidden，p_gen = self.decoder（dec_input_t，dec_hidden，enc_output）
                context_vector, attn = self.attention(dec_hidden, enc_output)
                Final_dist = self.get_final_distribution(enc_input_ext, p_gen, p_vocab,attn, self.max_oov) # 简化

                topi = Final_dist.max(1)[1]
                generated_tokens.append(topi.squeeze().detach().cpu().numpy())

                if (topi == self.sos_token_id).all():
                    休息
                dec_input_t = topi.unsqueeze(1)

            返回 generated_tokens

    def get_final_distribution（自我，x，p_gen，p_vocab，attention_weights，max_oov）：
        批量大小 = x.size(0)
        # 裁剪概率以避免损失计算中的 log(0)
        p_gen = torch.clamp(p_gen, 0.001, 0.999)
        p_vocab_weighted = p_gen * p_vocab
        注意力权重 = (1 - p_gen) * 注意力权重

        扩展名 = torch.zeros((batch_size, max_oov)).float().to(x.device)
        p_vocab_extended = torch.cat([p_vocab_weighted, 扩展名], 暗淡=1)
        # print(&quot;得到最终分布的x值&quot;,x.shape)
        # print(&quot;x 中的最大索引:&quot;, x.max().item())
        # print(&quot;p_vocab_extend 的第二个维度：&quot;, p_vocab_extend.size(1))
        # print(&quot;得到最终分布的attention_weighted&quot;, attention_weighted.shape)
        # print(&quot;来自 x 的样本值：&quot;, x[0, :10])
        # print(&quot;attention_weighted 的样本值:&quot;, Attention_weighted[0, :10])

        Final_distribution = p_vocab_extended.scatter_add_(1, x, 注意权重)

        返回最终分布
]]></description>
      <guid>https://stackoverflow.com/questions/78259116/i-am-facing-problem-while-trying-to-calculate-final-dist-for-pgn-indexerror-in</guid>
      <pubDate>Tue, 02 Apr 2024 05:29:10 GMT</pubDate>
    </item>
    <item>
      <title>视觉变压器：运行时错误：mat1和mat2形状不能相乘（32x1000和768x32）[关闭]</title>
      <link>https://stackoverflow.com/questions/78253997/vision-transformers-runtimeerror-mat1-and-mat2-shapes-cannot-be-multiplied-32</link>
      <description><![CDATA[我正在尝试对视觉变换器模型进行回归，但无法用回归层替换最后一层分类
类RegressionViT(nn.Module)：
    def __init__(self, in_features=224 * 224 * 3, num_classes=1, pretrained=True):
        super(RegressionViT, self).__init__()
        self.vit_b_16 = vit_b_16(预训练=预训练)
        # 从 vit_b_16 访问实际输出特征尺寸
        self.regressor = nn.Linear(self.vit_b_16.heads[0].in_features, num_classes * batch_size)

    def 前向（自身，x）：
        x = self.vit_b_16(x)
        x = self.regressor(x)
        返回x


＃ 模型
模型 = RegressionViT(num_classes=1)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
模型.to（设备）

criteria = nn.MSELoss() # 使用适当的损失函数进行回归
优化器 = optim.Adam(model.parameters(), lr=0.0001)


当我尝试初始化并运行模型时收到此错误
运行时错误：mat1 和 mat2 形状无法相乘（32x1000 和 768x32）

问题是回归层和vit_b_16模型层之间不匹配，解决此问题的正确方法是什么]]></description>
      <guid>https://stackoverflow.com/questions/78253997/vision-transformers-runtimeerror-mat1-and-mat2-shapes-cannot-be-multiplied-32</guid>
      <pubDate>Mon, 01 Apr 2024 06:33:32 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：使用序列设置数组元素。尝试用 Python 制作星空图</title>
      <link>https://stackoverflow.com/questions/78245302/valueerror-setting-an-array-element-with-a-sequence-trying-to-make-a-skymap-in</link>
      <description><![CDATA[# 日期时间库
从日期时间导入日期时间
从 geopy.geocoders 导入 Nominatim
从 tzwhere 导入 tzwhere
从 pytz 导入时区，UTC

# matplotlib 帮助显示我们的星图
将 matplotlib.pyplot 导入为 plt
# 恒星数据的天空场
从 skyfield.api 导入 Star、负载、wgs84
从 skyfield.data 导入 hipparcos
从 skyfield.projections 导入 build_steregraphic_projection

# de421 显示地球和太阳在太空中的位置
eph = 负载(&#39;de421.bsp&#39;)
# hipparcos 数据集包含恒星位置数据
将 load.open(hipparcos.URL) 作为 f：
星星= hipparcos.load_dataframe(f)
位置 = &#39;纽约时代广场，纽约&#39;
当 = &#39;2023-01-01 00:00&#39;

定位器 = Nominatim(user_agent=&#39;myGeocoder&#39;)
位置 = locator.geocode(位置)
lat, long = location.latitude, location.longitude 代码在这里
 # 将日期字符串转换为日期时间对象
 dt = datetime.strptime(当, &#39;%Y-%m-%d %H:%M&#39;)
 导入pytz
 从 pytz 导入时区，UTC
 # 定义日期时间并根据我们的时区转换为 utc
 #def 时区（纬度、经度、DT）：
 tzw = tzwhere.tzwhere()
 timezone_str = tzw.tzNameAt(纬度, 经度)
 本地 = pytz.timezone(timezone_str)
 #tzw = tzwhere.tzwhere()
 #timezone_str = tzw.tzNameAt(纬度, 经度)
 #local = pytz.local(timezone_str)

# 从本地时区和日期时间获取UTC
#dt = datetime.strptime(when, &#39;%Y-%m-%d %H:%M&#39;)
local_dt = local.localize(dt, is_dst=None)
utc_dt = local_dt.astimezone(utc)
打印（utc_dt）&#39;

上面的代码给出了这个错误。
ValueError：使用序列设置数组元素。请求的数组在二维后具有不均匀的形状。检测到的形状为(1, 2)+不均匀部分。
错误位于 tzw = tzwhere.tzwhere() 行
我应该如何纠正这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/78245302/valueerror-setting-an-array-element-with-a-sequence-trying-to-make-a-skymap-in</guid>
      <pubDate>Fri, 29 Mar 2024 18:14:24 GMT</pubDate>
    </item>
    <item>
      <title>解决从 Jupyter Notebook 到 .py 文件的自定义管道类转换中的 OneHotEncoder 问题</title>
      <link>https://stackoverflow.com/questions/78219825/troubleshooting-onehotencoder-issue-in-custom-pipeline-class-conversion-from-jup</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78219825/troubleshooting-onehotencoder-issue-in-custom-pipeline-class-conversion-from-jup</guid>
      <pubDate>Mon, 25 Mar 2024 14:32:03 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 自定义和默认目标和评估函数</title>
      <link>https://stackoverflow.com/questions/78109955/xgboost-custom-default-objective-and-evaluation-functions</link>
      <description><![CDATA[我正在训练 BDT 以进行信号/背景的二元分类（我从事粒子物理学工作）。我的模型（用 python 实现）如下所示：
导入 xgboost 为 xgb
train = xgb.DMatrix(data=train_df[特征],label=train_df[“标签”],
                    缺失=“inf”，feature_names=特征，权重=(np.array(train_df[&#39;label&#39;].array)*-0.99+1))
测试= xgb.DMatrix(数据=test_df[特征],标签=test_df[“标签”],
                   缺失=“inf”，feature_names=特征，权重=(np.array(test_df[&#39;label&#39;].array) *-0.99+1))

参数 = {}


# 助推器参数
param[&#39;eta&#39;] = 0.1 # 学习率
param[&#39;max_depth&#39;] = 10 # 树的最大深度
param[&#39;subsample&#39;] = 0.5 # 训练树的事件分数
param[&#39;colsample_bytree&#39;] = 0.5 # 训练树的特征分数

# 学习任务参数
param[&#39;objective&#39;] = &#39;binary:logistic&#39; # 目标函数
param[&#39;eval_metric&#39;] = &#39;error&#39; # 交叉验证的评估指标
param = list(param.items()) + [(&#39;eval_metric&#39;, &#39;logloss&#39;)] + [(&#39;eval_metric&#39;, &#39;rmse&#39;)]


num_trees = 50 # 要制作的树的数量
booster = xgb.train(参数,train,num_boost_round=num_trees)

该模型表现良好，但我想稍微修改一下成本/损失函数，以便误报比真报受到更多惩罚。我正在寻找背景事件尽可能少的选择，即使这意味着牺牲信号的很大一部分。
根据自定义目标和评估函数的文档，我成功添加教程案例。
哪些是标准目标和评估函数（或者我可能已经在模型中实现的函数）？
由于模型已经表现良好，我只想为已经实现的功能添加一个额外的术语。
作为参考，我正在使用的软件包版本：
python 版本：3.10.12（主要，2023 年 11 月 20 日，15:14:05）[GCC 11.4.0]
XGBoost版本：2.0.2
熊猫版本：2.1.4
Numpy 版本：1.26.2
]]></description>
      <guid>https://stackoverflow.com/questions/78109955/xgboost-custom-default-objective-and-evaluation-functions</guid>
      <pubDate>Tue, 05 Mar 2024 18:53:51 GMT</pubDate>
    </item>
    <item>
      <title>掩蔽者在 SHAP 包中真正做了什么并让他们适合训练或测试？</title>
      <link>https://stackoverflow.com/questions/66560839/what-do-maskers-really-do-in-shap-package-and-fit-them-to-train-or-test</link>
      <description><![CDATA[我一直在尝试使用 shap 包。我想从我的逻辑回归模型中确定形状值。与TreeExplainer相反，LinearExplainer需要一个所谓的掩码器。这个掩码器到底有什么作用？独立掩码器和分区掩码器有什么区别？
此外，我对测试集中的重要功能很感兴趣。然后我是否将掩蔽器安装在训练集或测试集上？您可以在下面看到一段代码。
模型 = LogisticRegression(random_state = 1)
model.fit(X_train, y_train)

masker = shap.maskers.Independent(data = X_train)
**或者**
masker = shap.maskers.Independent(data = X_test)

解释器 = shap.LinearExplainer(模型, masker = masker)
shap_val = 解释器(X_test)```

]]></description>
      <guid>https://stackoverflow.com/questions/66560839/what-do-maskers-really-do-in-shap-package-and-fit-them-to-train-or-test</guid>
      <pubDate>Wed, 10 Mar 2021 08:20:08 GMT</pubDate>
    </item>
    </channel>
</rss>