<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 09 Aug 2024 21:14:30 GMT</lastBuildDate>
    <item>
      <title>我该如何解释这个训练/验证损失曲线？[关闭]</title>
      <link>https://stackoverflow.com/questions/78854439/how-can-i-interpret-this-training-validation-loss-curve</link>
      <description><![CDATA[我实现了机器学习算法进行分类，并生成了训练/验证损失曲线。
这里是：
训练/验证损失曲线
我该如何解释它，我从未见过这样的曲线。它是否显示了一个好的模型？感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78854439/how-can-i-interpret-this-training-validation-loss-curve</guid>
      <pubDate>Fri, 09 Aug 2024 19:57:12 GMT</pubDate>
    </item>
    <item>
      <title>使用基于随机傅里叶特征 (RFF) 的核 LMS 进行在线联邦学习模拟时出现意外 MSE 行为</title>
      <link>https://stackoverflow.com/questions/78854316/unexpected-mse-behavior-in-online-federated-learning-simulation-using-random-fou</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78854316/unexpected-mse-behavior-in-online-federated-learning-simulation-using-random-fou</guid>
      <pubDate>Fri, 09 Aug 2024 19:13:39 GMT</pubDate>
    </item>
    <item>
      <title>MMpose 推断器不适用于 MP4 文件</title>
      <link>https://stackoverflow.com/questions/78854257/mmpose-inferencer-not-working-for-mp4-files</link>
      <description><![CDATA[我正在尝试使用 MMPose 在视频中的个人的 3D 空间中查找关键点。我使用的代码（之前在 2d 中运行过）是：
from mmpose.apis import MMPoseInferencer
from pathlib import Path
import os

data_folder = Path(&quot;x/videos&quot;)
for filename in os.listdir(data_folder):
if not filename.endswith(&#39;.mp4&#39;):
continue
img_path = os.path.join(data_folder, filename)

inferencer = MMPoseInferencer(pose3d=&quot;human3d&quot;)

result_generator = inferencer(img_path, out_dir=&#39;output&#39;)

每当我尝试访问 results_generator（如 results = [result for result in result_generator]）时，我都会遇到段错误，我猜是因为 result_generator 中没有任何内容。我还希望输出文件夹中有可视化效果和数据，但该文件夹是空的。我有什么明显的错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78854257/mmpose-inferencer-not-working-for-mp4-files</guid>
      <pubDate>Fri, 09 Aug 2024 18:55:46 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试使用 DecisionTreeClassifier 在 Python 中从 SPSS 复制决策树</title>
      <link>https://stackoverflow.com/questions/78854246/i-am-trying-to-replicate-a-decision-tree-from-spss-in-python-using-decisiontreec</link>
      <description><![CDATA[我尝试使用 DecisionTreeClassifier 在 Python 中复制 SPSS 中的决策树。我无法执行以下操作。

无法使用功能进行第一次强制拆分。
如果我使用相同的变量，则无法获得相似的模型拟合。我们可以在不更改正在复制的模型中已给出的变量或参数的情况下对其进行微调吗？
3.我可以在这棵树上对新数据进行评分，并将节点分配给新数据中的每一行吗？
还有其他更灵活的库可以使用吗？

任何帮助都将非常有帮助。
我使用这段代码来提供相同的参数并运行模型：
 criterion=&#39;gini&#39;, # CART 方法
max_depth=5, 
min_samples_split=100, 
min_samples_leaf=50
)
clf = clf.fit(X_train, y_train)

模型拟合与此模型不匹配。]]></description>
      <guid>https://stackoverflow.com/questions/78854246/i-am-trying-to-replicate-a-decision-tree-from-spss-in-python-using-decisiontreec</guid>
      <pubDate>Fri, 09 Aug 2024 18:51:54 GMT</pubDate>
    </item>
    <item>
      <title>如何下载机器学习的数据集但仅限于我的 python 文件中？</title>
      <link>https://stackoverflow.com/questions/78854017/how-do-i-download-datasets-for-machine-learning-but-only-in-my-python-file</link>
      <description><![CDATA[我需要下载音频数据集。我正在关注此 Tensorflow 教程，但我不知道如何获取像教程中用于设置原点的链接。我正在使用 Google Collab。
DATASET_PATH = &#39;data/mini_speech_commands&#39;

data_dir = pathlib.Path(DATASET_PATH)
if not data_dir.exists():
tf.keras.utils.get_file(
&#39;mini_speech_commands.zip&#39;,
origin=&quot;http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip&quot;,
extract=True,
cache_dir=&#39;.&#39;, cache_subdir=&#39;data&#39;)

commands = np.array(tf.io.gfile.listdir(str(data_dir)))
commands = commands[(commands != &#39;README.md&#39;) &amp; (commands != &#39;.DS_Store&#39;)]
print(&#39;Commands:&#39;, command)

按照教程中的所有内容，它可以正常工作，但我不明白将音频数据集放入我的 python 文件中的过程以及如何浏览每个音频。]]></description>
      <guid>https://stackoverflow.com/questions/78854017/how-do-i-download-datasets-for-machine-learning-but-only-in-my-python-file</guid>
      <pubDate>Fri, 09 Aug 2024 17:32:08 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用 Movenet 来检测正确和不正确的姿势？</title>
      <link>https://stackoverflow.com/questions/78853603/how-would-i-use-movenet-to-detect-correct-and-incorrect-poses</link>
      <description><![CDATA[我正在尝试使用 Tensorflow、Keras 和 Movenet 检测正确和不正确的坐姿。目前，我在训练、有效和测试文件夹中有好姿势和坏姿势文件夹，其中包含相应的姿势。我对编码模型的方法有一些想法，但我不确定哪种方法真正有效/最合适：

保留好姿势/坏姿势文件夹，使用数据构建和训练 CNN，然后构建/训练微调神经网络以对好姿势和坏姿势进行分类

仅保留好姿势图像，使用斜率计算某些身体部位的正确位置，不构建 CNN。我能想到的唯一问题是，如果某个身体部位阻碍了另一个身体部位怎么办？例如，我有一张正确姿势的照片，我的手臂放在桌子上，准备打字，但我还有另一张正确姿势的照片，我的手臂放在身体两侧，前臂向下不可见。或者姿势不正确，如果有人身体倾斜，脸靠在手上，身体的某些部位就看不见了怎么办？我或 Movenet 会如何处理这种情况？

完全不同的方法

]]></description>
      <guid>https://stackoverflow.com/questions/78853603/how-would-i-use-movenet-to-detect-correct-and-incorrect-poses</guid>
      <pubDate>Fri, 09 Aug 2024 15:33:58 GMT</pubDate>
    </item>
    <item>
      <title>如何完成遮挡的形状？</title>
      <link>https://stackoverflow.com/questions/78853333/how-to-complete-occluded-shapes</link>
      <description><![CDATA[我正在做一个项目来补全遮挡形状。
我浏览了互联网，发现我们可以使用插值方法来完成不完整的曲线，但唯一的问题是，在我使用的数据中（在包含每条曲线的点的 csv 文件中），曲线是封闭的……有什么想法可以做到这一点吗？

就像下面给出的这张图片一样，输入是一个 csv 文件，其中包含 3 条曲线的数据点，这些曲线都是封闭的。如果我单独绘制被遮挡的椭圆的输入点，它们将像第二张照片一样出现，而不仅仅是不完整的椭圆边界曲线。

输出应如下所示：
]]></description>
      <guid>https://stackoverflow.com/questions/78853333/how-to-complete-occluded-shapes</guid>
      <pubDate>Fri, 09 Aug 2024 14:32:17 GMT</pubDate>
    </item>
    <item>
      <title>无法设置张量：获取 STRING 类型的值，但输入 0 应为 FLOAT32 类型，名称：serving_default_args_0:0</title>
      <link>https://stackoverflow.com/questions/78853060/cannot-set-tensor-got-value-of-type-string-but-expected-type-float32-for-input</link>
      <description><![CDATA[当我尝试加载模型时，我收到一条错误消息
这是我的代码
interpreter = tf.lite.Interpreter(model_path=&quot;/content/colab_model.tflite&quot;)
interpreter.allocate_tensors()

# 获取输入和输出张量。
input_details = explainer.get_input_details()
output_details = explainer.get_output_details()
# 打印输入详细信息以了解其预期类型和形状
for input_detail in input_details:
print(f&quot;Name: {input_detail[&#39;name&#39;]}, Shape: {input_detail[&#39;shape&#39;]}, Dtype: {input_detail[&#39;dtype&#39;]}&quot;)

# 将用户 ID 转换为字符串
user_id_value = np.array([&#39;42&#39;], dtype=np.str_) 

# 将配方 ID 转换为字符串
recipe_id_values = np.array([&#39;49&#39;, &#39;66&#39;, &#39;62&#39;], dtype=np.str_)

# 使用字符串设置张量值
interpreter.set_tensor(input_details[0][&#39;index&#39;], np.array([recipe_id_values[0]])) 
interpreter.set_tensor(input_details[1][&#39;index&#39;], user_id_value)

# 调用模型
interpreter.invoke()

# 获取输出
output_data = interpretationer.get_tensor(output_details[0][&#39;index&#39;])
print(output_data)

错误
ValueError: 无法设置张量：得到的值类型为 STRING，但输入 0 的预期类型为 FLOAT32，名称为：serving_default_args_0:0

当我尝试先将其转换为浮点 32 时，出现错误
RuntimeError: 将浮点转换为字符串是错误的支持的委托内核未初始化节点号 19 (TfLiteFlexDelegate) 准备失败。委托内核未初始化节点号 19 (TfLiteFlexDelegate) 准备失败。委托内核未初始化节点号 19
(TfLiteFlexDelegate) 准备失败。
]]></description>
      <guid>https://stackoverflow.com/questions/78853060/cannot-set-tensor-got-value-of-type-string-but-expected-type-float32-for-input</guid>
      <pubDate>Fri, 09 Aug 2024 13:30:18 GMT</pubDate>
    </item>
    <item>
      <title>集成一项功能来识别手绘形状并实时重新绘制[关闭]</title>
      <link>https://stackoverflow.com/questions/78852946/integrate-a-feature-to-recognize-hand-drawn-shapes-and-redraw-it-in-real-time</link>
      <description><![CDATA[我正在构建一个项目，它可以跟踪我的手指运动并根据我的运动在屏幕上绘图。为此，我使用了 mediaPipe Hands 解决方案。我想集成一个功能来识别圆形、矩形等形状，并以理想的形式重新绘制它。
（当然是视频流）
我实现了一个功能来识别使用 OpenCV 在屏幕上绘制的形状。我尝试在绘图模式下捕获手指跟踪的点，并使用这些点来检测所绘制的形状是圆形还是矩形。我期望它在完成后准确识别形状。然而，实际发生的情况是，一旦我进入绘图模式，系统就开始将我的手指检测为圆形，并在手指移动到的每个点周围绘制一个圆圈。
def understand_shape(points):
if len(points) &lt; 5：
return None

# 计算点的边界框
x_coords, y_coords = zip(*points)
min_x, max_x = min(x_coords), max(x_coords)
min_y, max_y = min(y_coords), max(y_coords)
width, height = max_x - min_x, max_y - min_y

# 使用半径方差检查圆
center_x, center_y = np.mean(x_coords), np.mean(y_coords)
radii = [distance.euclidean((x, y), (center_x, center_y)) for x, y in points]
mean_radius = np.mean(radii)
radius_variance = np.var(radii)

if radius_variance &lt; 1000：# 调整阈值以提高准确度
return (&quot;circle&quot;, (int(center_x), int(center_y), int(mean_radius)))

# 使用纵横比检查矩形
if 0.9 &lt; width / height &lt; 1.1：# 允许正方形略有偏差
if all(min_x &lt;= x &lt;= max_x and min_y &lt;= y &lt;= max_y for x, y in points):
return (&quot;rectangle&quot;, (min_x, min_y, max_x, max_y))

return None

def draw_shape(shape, imgCanvas):
if shape[0] == &quot;circle&quot;:
_, (center_x, center_y, radius) = shape
cv2.circle(imgCanvas, (center_x, center_y), radius, (0, 0, 255), 2)
elif shape[0] == &quot;rectangle&quot;:
_, (min_x, min_y, max_x, max_y) = shape
cv2.rectangle(imgCanvas, (min_x, min_y), (max_x, max_y), (0, 0, 255), 2)
]]></description>
      <guid>https://stackoverflow.com/questions/78852946/integrate-a-feature-to-recognize-hand-drawn-shapes-and-redraw-it-in-real-time</guid>
      <pubDate>Fri, 09 Aug 2024 13:06:37 GMT</pubDate>
    </item>
    <item>
      <title>如何找到参数之间的因果关系？[关闭]</title>
      <link>https://stackoverflow.com/questions/78852790/how-to-find-causal-relationship-between-parameters</link>
      <description><![CDATA[我的公司生产嵌入式设备，我们从这些设备中收集了一堆参数（几百个），保存在 CSV 文件中。现在我想找到一个重要的错误参数和所有其他参数之间的因果关系。到目前为止，我所做的就是训练一个 SVM 模型，以重要的错误参数为目标，其余参数为特征，然后找到对目标参数贡献最大的特征。我对结果非常满意，因为贡献最大的标签实际上是有意义的，并且该模型在测试数据上的准确率超过了 90%。
但我不知道如何从这些结果继续找到特定事件中的贡献参数。例如，如果触发了这个错误参数 - 我如何使用我训练过的模型知道哪个参数是贡献最大的参数（假设所有其他参数的上下文）？目前，模型只能告诉我触发错误参数的可能性 - 但不能反过来。]]></description>
      <guid>https://stackoverflow.com/questions/78852790/how-to-find-causal-relationship-between-parameters</guid>
      <pubDate>Fri, 09 Aug 2024 12:31:24 GMT</pubDate>
    </item>
    <item>
      <title>云管理矢量数据库选项或图像匹配选项</title>
      <link>https://stackoverflow.com/questions/78852684/cloud-managed-vector-database-options-or-image-matching-options</link>
      <description><![CDATA[我们有一个项目，将图像转换为矢量并存储在 Milvus 数据库中。之后，我们检查图像之间的相似性以找到矢量中距离较小的图像。
目前，我们自己管理 Milvus 数据库集群。我正在寻找矢量数据库的云托管解决方案。是否有任何云提供商提供此类解决方案，例如用于 SQL 数据库的 AWS RDS。
我还检查了 AWS Rekognize，它可以识别图像中的脸部。市场上还有其他解决方案可以提供矢量图像中的距离吗？]]></description>
      <guid>https://stackoverflow.com/questions/78852684/cloud-managed-vector-database-options-or-image-matching-options</guid>
      <pubDate>Fri, 09 Aug 2024 12:10:49 GMT</pubDate>
    </item>
    <item>
      <title>使用 device_map 选择可用的 GPU 设备</title>
      <link>https://stackoverflow.com/questions/78852192/choose-available-gpu-devices-with-device-map</link>
      <description><![CDATA[from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
model_id,
torch_dtype=torch.bfloat16,
device_map=&quot;cuda:3&quot;,
)

服务器上有很多 GPU，但我只能使用其中两个。我应该如何配置 device_map（或其他参数）才能让模型在两个 GPU 上运行？]]></description>
      <guid>https://stackoverflow.com/questions/78852192/choose-available-gpu-devices-with-device-map</guid>
      <pubDate>Fri, 09 Aug 2024 10:04:34 GMT</pubDate>
    </item>
    <item>
      <title>将图像数据地理配准到 Google staelite 地图的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/78852106/what-is-the-best-way-to-georeference-image-data-to-google-staelite-map</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78852106/what-is-the-best-way-to-georeference-image-data-to-google-staelite-map</guid>
      <pubDate>Fri, 09 Aug 2024 09:42:51 GMT</pubDate>
    </item>
    <item>
      <title>我该如何修剪这个神经网络？</title>
      <link>https://stackoverflow.com/questions/78851708/how-can-i-prune-this-neural-network</link>
      <description><![CDATA[我正在使用 TensorFlow 在 Python 中创建一个用于 PA 行为建模的传统神经网络。该模型接收输入 I 和 Q 值并预测放大器输出。所以这是一个包含两列的 .csv 文件。我的目标之一是修剪（或以任何方式优化）我构建的模型。原始模型运行良好。但是，我在修剪创建、训练和测试的模型时遇到了问题。
以下是原始模型：
import os

os.environ[&#39;TF_ENABLE_ONEDNN_OPTS&#39;] = &#39;0&#39;

import keras
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model, load_model, save_model, clone_model, Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.metrics import Accuracy
import tensorflow_model_optimization as tfmot
import time

# 加载数据
data_input = pd.read_csv(&#39;Input_TimeAligned.csv&#39;,header = None)
data_input.columns = [&#39;i&#39;,&#39;q&#39;]
data_input.describe()
data_input.head()

data_input_arr = data_input.to_numpy()
print(&#39;以数组形式输入数据：&#39;,data_input_arr)
print(len(data_input_arr))
print()

data_output = pd.read_csv(&#39;Output_TimeAligned.csv&#39;,header = None)
data_output.columns = [&#39;i&#39;,&#39;q&#39;]
data_output.describe()
data_output.head()

data_output_arr = data_output.to_numpy()
print(&#39;以数组形式输出数据：&#39;,data_output_arr)
print(len(data_output_arr))
print()

data_input_tr = data_input_arr[0:122879,:]
data_output_tr = data_output_arr[0:122879,:]

data_input_test = data_input_arr[122880:491519,:]

data_output_test = data_output_arr[122880:491519,:]

X_train = data_input_tr
y_train = data_output_tr

X_test = data_input_test
y_test = data_output_test

# 定义模型架构。
开始 = time.time()
模型 = keras.Sequential([
keras.layers.InputLayer(input_shape = (2,)),
keras.layers.Dense(单位 = 128, 激活 = &#39;tanh&#39;, 名称 = &#39;layer_1&#39;),
keras.layers.Dense(单位 = 256, 激活 = &#39;tanh&#39;, 名称 = &#39;layer_2&#39;),
keras.layers.Dense(单位 = 512, 激活 = &#39;tanh&#39;, 名称 = &#39;layer_3&#39;),
keras.layers.Dense(单位 = 256, 激活 = &#39;tanh&#39;, 名称 = &#39;layer_4&#39;),
keras.layers.Dense(单位 = 128, 激活 = &#39;tanh&#39;, 名称 = &#39;layer_5&#39;),
keras.layers.Dense(单位 = 2, 激活 = &#39;tanh&#39;, name = &#39;output_layer&#39;),
])

model.compile(loss = &#39;mean_squared_error&#39;, optimizer = &#39;adam&#39;, metrics = [&#39;Accuracy&#39;])
end = time.time()
print(&#39;编译模型所用时间为：&#39;, end - start)
print()

print(model.summary())
print()

start = time.time()
model.fit(X_train, y_train, epochs = 3, batch_size = 32) 
end = time.time()
print(&#39;训练模型所用时间为：&#39;, end - start)
print()

start = time.time()
y_hat = model.predict(X_test)
end = time.time()
print(&#39;测试模型所用时间为：&#39;, end - start)
print(&#39;预测输出为：&#39;, y_hat)
print()

start = time.time()
model.evaluate(X_test, y_test)
end = time.time()
print(&#39;评估模型所用时间为：&#39;, end - start)
print()

我尝试了如下所示的修剪：
prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude

# 定义修剪模型。
pruning_params = {
&#39;pruning_schedule&#39;: tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.0,
final_sparsity=0.50,
begin_step=0,
end_step=1000)
}

model_for_pruning = prune_low_magnitude(model, **pruning_params)

# `prune_low_magnitude` 需要重新编译。
model_for_pruning.compile(optimizer=&#39;adam&#39;,
loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=[&#39;accuracy&#39;])

model_for_pruning.summary()

start = time.time()
model.compile(
loss=&quot;mse&quot;,
optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)
)
end = time.time()
print(&#39;编译修剪模型所花费的时间为：&#39;, end - start)
print()

print(model.summary())
print()

start = time.time()
model.fit(
X_train, 
y_train, 
epochs=3, 
batch_size=32, 
#callbacks= pruning_callback, 
# verbose=1
)
end = time.time()
print(&#39;训练修剪模型所花费的时间为：&#39;, end - start)

start = time.time()
y_hat = model.predict(X_test)
end = time.time()
print(&#39;测试修剪模型所花费的时间为：&#39;, end - start)
print(&#39;预测输出为：&#39;, y_hat)
print()

start = time.time()
model.evaluate(X_test, y_test)
end = time.time()
print(&#39;评估修剪模型所花费的时间为：&#39;, end - start)
print()

这是我收到的错误：
ValueError: `prune_low_magnitude` 只能修剪以下类型的对象：keras.models.Sequential、keras 函数模型、keras.layers.Layer、keras.layers.Layer 列表。您传递了一个类型为 Sequential 的对象。

我在这里做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78851708/how-can-i-prune-this-neural-network</guid>
      <pubDate>Fri, 09 Aug 2024 07:50:59 GMT</pubDate>
    </item>
    <item>
      <title>无需深度学习或 Tesseract 的文本图像二元分类器</title>
      <link>https://stackoverflow.com/questions/78842184/text-image-binary-classifier-without-deep-learning-or-tesseract</link>
      <description><![CDATA[我有 20k 张小标签图像，每张图像都有单词“Back”或“Front”。
图像分辨率为全部 (200px, 25px)

我可以使用 tesseract_OCR 对这些图像进行 100% 准确率的分类。
 txt = pytesseract.image_to_string(img, lang=&#39;eng&#39;)
if &quot;Front&quot; in txt:
return &quot;Front&quot;
if &quot;Back&quot; in txt:
return &quot;Back&quot;

问题是，它太慢了（20k 张图像需要 1 小时）并且需要安装 OCR 包。
我知道即使是 3 层的简单 CNN 也能很好地运行，但我认为这个问题似乎可以用简单的算法解决，而不需要复杂的技术。
你能给我推荐一种新方法吗？
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78842184/text-image-binary-classifier-without-deep-learning-or-tesseract</guid>
      <pubDate>Wed, 07 Aug 2024 06:46:36 GMT</pubDate>
    </item>
    </channel>
</rss>