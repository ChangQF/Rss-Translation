<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Wed, 26 Mar 2025 18:25:14 GMT</lastBuildDate>
    <item>
      <title>在两个nn.modulelist上使用zip（）</title>
      <link>https://stackoverflow.com/questions/79536891/using-zip-on-two-nn-modulelist</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79536891/using-zip-on-two-nn-modulelist</guid>
      <pubDate>Wed, 26 Mar 2025 18:02:29 GMT</pubDate>
    </item>
    <item>
      <title>在机器学习和数据科学方面需要帮助[关闭]</title>
      <link>https://stackoverflow.com/questions/79536719/need-help-in-machine-learning-and-data-science</link>
      <description><![CDATA[我是初学者，现在正在进行Kaggle ML课程。任何人都可以告诉我下一步该怎么做或任何路线图，并在可能的情况下请提供资源（免费）。   还建议我一些好的项目，这些项目解决了新的主题，例如决策者和RandomerForestRegressor等 
只是想学习新事物并变得更好！]]></description>
      <guid>https://stackoverflow.com/questions/79536719/need-help-in-machine-learning-and-data-science</guid>
      <pubDate>Wed, 26 Mar 2025 16:53:21 GMT</pubDate>
    </item>
    <item>
      <title>初学者如何在知识论坛中有效地提出大型语言模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79536492/how-can-a-beginner-effectively-present-large-language-models-in-a-knowledge-foru</link>
      <description><![CDATA[我是大型语言模型（LLM）领域的初学者，并已被邀请为我的工作场所的知识贡献论坛做出贡献。目的是分享与LLM有关的见解和经验。
由于我仍在学习，所以我想确保我的演讲准确，引人入胜且易于理解，这些人也可能是该主题的新手。]]></description>
      <guid>https://stackoverflow.com/questions/79536492/how-can-a-beginner-effectively-present-large-language-models-in-a-knowledge-foru</guid>
      <pubDate>Wed, 26 Mar 2025 13:00:21 GMT</pubDate>
    </item>
    <item>
      <title>Jupyter |内核似乎已经死亡。它将自动重新启动| keras.models.Sequinential</title>
      <link>https://stackoverflow.com/questions/79536194/jupyter-the-kernel-appears-to-have-died-it-will-restart-automatically-keras</link>
      <description><![CDATA[我正在使用顺序模型构建我的深度学习模型，而我正在执行 model.fit 我的jupyter笔记本与消息死亡内核似乎已经死亡。它将自动重新启动。
  model_class = tf.keras.models.sequeential（）
model_class.add（tf.keras.layers.dense（11，activation =&#39;relu&#39;））
model_class.add（tf.keras.layers.dense（8，activation =&#39;relu&#39;））
model_class.add（tf.keras.layers.dense（8，activation =&#39;softmax&#39;））
＃编译模型
model_class.compile（lose =; cancorical_crossentropy＆quot＆quort＆quort = [＆quord&#39;cocucre＆quort＆quort＆quort＆quort＆quotizer =; sgd; sgd＆quort;

＃适合模型
model_class.fit（x = xc_train，y = yc_train，batch_size = 20，epochs = 100，versitation_data =（xc_val，yc_val））
 ]]></description>
      <guid>https://stackoverflow.com/questions/79536194/jupyter-the-kernel-appears-to-have-died-it-will-restart-automatically-keras</guid>
      <pubDate>Wed, 26 Mar 2025 10:57:03 GMT</pubDate>
    </item>
    <item>
      <title>如何同时使用ROS收集实时数据并应用机器学习算法？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79535669/how-to-collect-real-time-data-and-apply-a-machine-learning-algorithm-using-ros-a</link>
      <description><![CDATA[我正在使用Turtlebot3从事一个研究项目。该机器人配备了Raspberry Pi，OpenCR和LIDAR传感器。我需要收集实时数据（WiFi RSSI值以及X，Y坐标，然后将机器学习算法应用于实时占用检测。
我正在使用ROS（机器人操作系统）来管理机器人，我计划使用数据实时检测占用。
我感谢有关：的指导

 如何构造我的ROS以进行实时数据收集和ML处理。

 在机器人收集数据时，如何并行运行机器学习模型。

 有效执行此操作的推荐工具，软件包或架构是什么？


我已经写了一个ROS节点来收集WiFi RSSI值，以及机器人的X和Y坐标以及时间戳。我基于时间戳并创建无线电环境图合并了这些数据。
我现在想扩展此设置，以使用机器学习算法执行实时占用检测。我不确定如何执行系统，以便机器人可以同时收集数据并运行ML模型。
我已经尝试研究与ROS节点一起运行Python ML脚本，但是我对从哪里开始以及如何实时整合所有内容感到困惑。]]></description>
      <guid>https://stackoverflow.com/questions/79535669/how-to-collect-real-time-data-and-apply-a-machine-learning-algorithm-using-ros-a</guid>
      <pubDate>Wed, 26 Mar 2025 07:14:16 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用Python在这张图片中区分（绿色）之间的发芽和非发芽种子（红色）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79535638/how-can-i-use-python-to-differentiate-green-between-germinated-and-non-germina</link>
      <description><![CDATA[我如何编写python脚本以区分发芽的种子（绿色）和非发芽种子（红色）或使用ImageJ区分它们的步骤
我不知道哪种是在24孔板中区分这些种子的最佳方法。我之前曾尝试过ImageJ]]></description>
      <guid>https://stackoverflow.com/questions/79535638/how-can-i-use-python-to-differentiate-green-between-germinated-and-non-germina</guid>
      <pubDate>Wed, 26 Mar 2025 07:00:07 GMT</pubDate>
    </item>
    <item>
      <title>如何在没有自动差异的神经网络的情况下实施反向传播？</title>
      <link>https://stackoverflow.com/questions/79535519/how-to-implement-backpropagation-without-auto-differentiation-for-a-feedforward</link>
      <description><![CDATA[我正在研究一个深度学习任务，该任务需要使用仅使用 numpy （没有Tensorflow，Pytorch或其他自动分辨率工具）实现 feedforward神经网络（FNN）。该网络具有三层（2048、512、10个神经元），使用 relu和SoftMax激活，并用 mini-Batch随机渐变（SGD）进行了优化。
分配需要手动实施向前传播，反向传播和重量更新，以确保我使用 BackPropagation AlgorithM  确保我正确计算每个层的梯度。。
但是，我正在努力正确地计算每个层的梯度，尤其是当处理 relu激活的衍生物和 softmax 时。我想确认我的梯度计算和重量更新是正确的。
 导入numpy作为NP

def relu（x）：
    返回np.maximum（0，x）

def relu_derivative（x）：
    返回（x＆gt; 0）.astype（float）＃relu derivative（1如果x＆gt; 0，else 0）

def softmax（x）：
    exp_x = np.exp（x -np.max（x，axis = 1，keepdims = true））＃稳定技巧
    返回exp_x / np.sum（exp_x，axis = 1，keepdims = true）

def cross_entropy_loss（y_pred，y_true）：
    返回-np.mean（np.sum（y_true * np.log（y_pred + 1e -9），轴= 1））＃防止log（0）

def softmax_cross_entropy_grad（y_pred，y_true）：
    返回y_pred -y_true＃softmax +跨渗透衍生物

def gradient_check（）：
    np.random.seed（42）＃确保可重复性
    
    ＃假输入（batch_size = 3，input_dim = 5）
    x = np.random.randn（3，5）
    w1 = np.random.randn（5，4）
    b1 = np.zeros（（1，4））

    ＃假式单壁编码标签（batch_size = 3，num_classes = 4）
    y = np.Array（[[[0，1，0，0]， 
                  [1，0，0，0]， 
                  [0，0，1，0]]）

    z1 = np.dot（x，w1） + b1
    a1 = relu（z1）
    y_pred = softmax（a1）
    损失= cross_entropy_loss（y_pred，y）

    dl_da1 = softmax_cross_entropy_grad（y_pred，y）＃渐变W.R.T. SoftMax输出
    dl_dz1 = dl_da1 * relu_derivative（z1）＃与relu的链条规则

    打印（y_pred（SoftMax输出）：\ n＆quort y_pred）
    打印（“损失：; quot”损失）
    打印（&#39;梯度W.R.T. SoftMax输出（DL/DA1）：\ n＆quort; dl_da1）
    打印（``relu derivative&#39;&#39;之后的渐变（dl/dz1）：\ n＆quort; dl_dz1）

gradient_check（）
 
我仅使用 numpy 并手动计算的反向传播实现了 feedforward神经网络（FNN）。我期望损失会降低和的准确性，但相反，损失波动，准确性保持 low ，有时梯度 爆炸或成为nan 。我怀疑 relu的导数和 softmax +交叉渗透梯度计算的问题，需要验证我的反向传播是否正确。]]></description>
      <guid>https://stackoverflow.com/questions/79535519/how-to-implement-backpropagation-without-auto-differentiation-for-a-feedforward</guid>
      <pubDate>Wed, 26 Mar 2025 05:59:43 GMT</pubDate>
    </item>
    <item>
      <title>我使用densenet-169创建了一个使用组织病理学图像来预测结肠癌的模型，我该怎么做才能将我的F1分数从75％提高[封闭]</title>
      <link>https://stackoverflow.com/questions/79535469/i-used-densenet-169-in-creating-a-model-for-predicting-colon-cancer-using-histop</link>
      <description><![CDATA[我使用Python在10,000张图像的数据集上使用Densenet169创建了一个模型，以预测结肠癌，两个子文件夹有5000张图像，每张图像5000张图像用于良性和癌组织。训练持续了8小时，训练后的F1分数为0.75，这是不可取的。我试图查看我可以做出的更改以改善指标，但是我没有任何更改的方法，并且需要帮助确定可以在哪里进行更改以提高F1分数。这是我第一次从事这样的事情，我发现没有在线材料进行深度学习可以帮助我解决这个问题
 将TensorFlow导入为TF
来自Tensorflow.keras.applications导入Densenet169
来自tensorflow.keras.layers导入密集，globalaveration -pooling2d，Randomflip，RandomRotation，辍学
来自Tensorflow.keras.models导入模型，顺序
来自Tensorflow.keras.optimizer导入Adam
从tensorflow.keras.regulinizer导入l2
来自tensorflow.keras.applications.densenet导入preprocess_input
来自sklearn.metrics导入混淆_matrix，f1_score
导入numpy作为NP
进口海洋作为SNS
导入matplotlib.pyplot作为PLT
导入操作系统

＃定义常数
img_size =（224，224）＃匹配densenet169输入大小
batch_size = 32
时代= 15
data_dir =＆quot; colon_images＆quot;
class_names = [＆quast; cancyous&#39;&#39;正常＆quot;]
split_ratio = 0.2

＃直接加载数据集在224x224
def create_dataset（子集）：
    返回tf.keras.utils.image_dataset_from_directory（
        data_dir，
        验证_split = split_ratio，
        子集=子集
        种子= 42，
        image_size = img_size，＃直接以目标大小加载
        batch_size = batch_size，
        label_mode =&#39;binary&#39;
    ）

train_ds = create_dataset（“训练”）
val_ds = create_dataset（&#39;验证＆quot;）

＃通过增强进行预处理（仅在培训期间活跃）
预处理=顺序（[
    Randomflip（“水平”，“），
    随机旋转（0.1），
    tf.keras.layers.lambda（preprocess_input）＃正确归一化
）））

＃构建模型
base_model = densenet169（
    权重=&#39;Imagenet&#39;，
    include_top = false，
    input_shape = img_size +（3，）
）

输入= tf.keras.input（shape = img_size +（3，））
X =预处理（输入）
x = base_model（x）
x = globalaveragepooling2d（）（x）
x =密集（512，激活=&#39;relu&#39;，kernel_regularizer = l2（0.01））（x）
x =辍学（0.5）（x）＃正则化
输出=密集（1，激活=&#39;Sigmoid&#39;）（x）
模型=模型（输入，输出）

＃阶段1：火车顶层
base_model.trainable = false
model.compile（
    优化器= ADAM（1E-3），
    损失=&#39;binary_crossentropy&#39;，
    量表= [&#39;fecycy&#39;，tf.keras.metrics.auc（name =&#39;auc&#39;），
             tf.keras.metrics.precision（name =&#39;precision&#39;），
             tf.keras.metrics.Recall（name =&#39;recember&#39;）]
）

＃用回调监视AUC的火车
早期_stop = tf.keras.callbacks.earlystopping（
    Monitor =&#39;Val_auc&#39;，耐心= 3，模式=&#39;max&#39;，详细= 1
）
检查点= tf.keras.callbacks.modelcheckpoint（
    &#39;best_model.h5&#39;，save_best_only = true，monitor =&#39;val_auc&#39;，mode =&#39;max&#39;
）

历史= model.fit（
    train_ds，
    验证_data = val_ds，
    时代= epochs，
    回调= [早期_STOP，检查点]
）

＃阶段2：微调整个模型
base_model.trainable = true
model.compile（
    优化器= ADAM（1E-5），＃非常低的学习率
    损失=&#39;binary_crossentropy&#39;，
    量表= [&#39;facer&#39;，tf.keras.metrics.auc（name =&#39;auc&#39;）]
）

history_fine = model.fit（fit）（
    train_ds，
    验证_data = val_ds，
    时代= epochs，
    onirome_epoch = history.epoch [-1]，
    回调= [早期_STOP，检查点]
）

＃ 评估
y_pred =（model.predict（val_ds）＆gt; 0.5）.astype（int）
y_true = np.concatenate（[y for _，y in val_ds]，axis = 0）

打印（f＆quot f1分数：{f1_score（y_true，y_pred）：。3f}＆quot;）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79535469/i-used-densenet-169-in-creating-a-model-for-predicting-colon-cancer-using-histop</guid>
      <pubDate>Wed, 26 Mar 2025 05:27:58 GMT</pubDate>
    </item>
    <item>
      <title>我的加权动态时间扭曲算法中是否存在错误[封闭]</title>
      <link>https://stackoverflow.com/questions/79535435/is-there-an-error-in-my-weighted-dynamic-time-warping-algorithm</link>
      <description><![CDATA[使用平均归一化，并为算法中的输入进行修改的逻辑权重函数。不确定我的代码或我的数据是否弄乱了，使用了两个不同人的步态数据数据集，而我的距离最终是：0.06495056266749273 
我将以下公式用于WDTW和修改后的逻辑权重函数：
      def stripfunction（i_j，g，w_max）：#calculates修改后的逻辑权重函数
  尺寸= len（x）
  m_c = size // 2

  返回w_max /（1 + np.exp（-g*（i_j -m_c）））

def euclidean_distances（x，y）：
      返回np.sqrt（np.sum（（（x -y）** 2）））

def witeeddtw（x，y，vector = true）：
  w_max = 3

  n，m = X.Shape [0]，y.Shape [0]
  成本= np r.zeros（（（n，m））

  成本[0,0] = euclidean_distances（x [0]，y [0]）

  对于我在范围（1，n）中：
    成本[i，0] =成本[I-1，0] + Euclidean_distances（x [i]，y [0]）

  对于J范围（1，m）的J：
    成本[0，j] =成本[0，j-1] + euclidean_distances（x [0]，y [j]）


  对于我在范围（1，n）中：
    对于J范围（1，m）的J：
      i_j = euclidean_distances（x [i]，y [j]）
      w =重量函数（I_J，0.4，W_MAX）
      min_value = min（[成本[I-1，J-1]，成本[I-1，J]，成本[I，J-1]]）
      成本[i，j] = w * i_j + min_value

    max_dist = w_max *（n + m -2）
    norm_dist = cop [n-1，m-1] / max_dist


  退货成本[N-1，M-1]，NORM_DIST

x_norm =（x -np.mean（x，axis = 0）） /（np.max（x，axis = 0）-np.min（x，axis = 0））

y_norm =（y -np.mean（y，axis = 0）） /（np.max（y，axis = 0）-np.min（y，axis = 0））


＃计算WDTW
距离，归一化=加权dtw（x_norm [：，0]，y_norm [：，0]）

打印（f＆quot&#39;wdtw距离：{demand}＆quot;）
打印（f＆quot“归一化wdtw距离：{normolized_distance}＆quort”）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79535435/is-there-an-error-in-my-weighted-dynamic-time-warping-algorithm</guid>
      <pubDate>Wed, 26 Mar 2025 04:58:34 GMT</pubDate>
    </item>
    <item>
      <title>GOCV和Python OpenCV之间的推断结果不同</title>
      <link>https://stackoverflow.com/questions/79533406/different-inference-results-between-gocv-and-python-opencv</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79533406/different-inference-results-between-gocv-and-python-opencv</guid>
      <pubDate>Tue, 25 Mar 2025 10:48:10 GMT</pubDate>
    </item>
    <item>
      <title>有什么方法可以查看python中bert的内部Q，k，v配置</title>
      <link>https://stackoverflow.com/questions/79530683/is-there-any-way-to-view-the-internal-q-k-v-configurations-of-bert-in-python</link>
      <description><![CDATA[我正在进行一个NLP项目，我需要分析该过程的工作方式的各个方面。本质上潜入黑匣子。我需要使用BERT为数据集生成嵌入（我已经完成了）。但是虽然这样做，但它具有Q，K和V矩阵和注意力评分 - 我必须获得，解释和可视化。从本质上讲，它可以闯入伯特而不打破它。我将如何处理？
我生成了嵌入式和可以可视化的嵌入，但是伯特的内部配置我什至不知道从哪里开始可视化它们。我不想为此使用chatgpt，所以这就是为什么我在这里。]]></description>
      <guid>https://stackoverflow.com/questions/79530683/is-there-any-way-to-view-the-internal-q-k-v-configurations-of-bert-in-python</guid>
      <pubDate>Mon, 24 Mar 2025 09:31:26 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的代码不导致张量的错误“元素0”不需要毕业，也不需要grad_fn'</title>
      <link>https://stackoverflow.com/questions/79524465/why-does-not-my-code-cause-the-error-element-0-of-tensors-does-not-require-grad</link>
      <description><![CDATA[给定下面的代码片段，我使用 model.fc1.requires_grad_（false） and  model.fc2.requires_grad_（false）冻结神经网络的权重。
如果我使用损失=标准（输出，y_batch）来计算损失，则训练很好。为什么它不会导致错误 RuntimeError：张量的元素0不需要grad，也不需要Grad_fn ？
我认为这应该导致错误，因为神经网络的所有层都已冻结。
 导入火炬
导入Torch.nn作为nn
导入Torch.optim作为最佳
来自Torch.utils.data导入数据载体，TensordataSet

＃定义一个简单的神经网络
类SimpleNet（nn.Module）：
    def __init __（self，input_size，hidden_​​size，output_size）：
        超级（SimpleNet，Self）.__ INIT __（）
        self.fc1 = nn.linear（input_size，hidden_​​size）
        self.fc2 = nn.linear（hidden_​​size，output_size）

    def向前（self，x）：
        x = self.fc1（x）
        x = self.fc2（x）
        返回x

input_size = 3
hidden_​​size = 3
output_size = 3
batch_size = 16
num_epochs = 1
Learning_rate = 0.01
num_samples = 1000 
x_train = torch.randn（num_samples，input_size，quirenes_grad = true）  
y_train = torch.ones（（num_samples，output_size））
dataset = tensordataset（x_train，y_train）
dataloader = dataloader（数据集，batch_size = batch_size，shuffle = true）

model = simpleNet（input_size，hidden_​​size，output_size）


标准= nn.Crossentropyloss（）
优化器= Optim.SGD（model.parameters（），lr = Learning_rate）

对于范围（num_epochs）的时代：
    total_loss = 0
    对于emumerate（dataloader）中的batch_idx（x_batch，y_batch）：
        优化器.zero_grad（）
        输出=模型（x_batch）
        损失=标准（输出，y_batch）
        打印（损失。_grad_fn）
        loss.backward（） 
        优化器.step（） 
                
        if（batch_idx + 1）％10 == 0：
            print（f&#39;batch [{batch_idx + 1}/{len（dataloader）}]，损失：{loss.item（）：。4f}&#39;）
            model.fc1.requires_grad_（false）
            model.fc2.requires_grad_（false）
                        
打印（“训练完成！”）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79524465/why-does-not-my-code-cause-the-error-element-0-of-tensors-does-not-require-grad</guid>
      <pubDate>Fri, 21 Mar 2025 02:40:41 GMT</pubDate>
    </item>
    <item>
      <title>节点/边缘归因的有向图嵌入</title>
      <link>https://stackoverflow.com/questions/79508535/node-edge-attributed-directed-graph-embedding</link>
      <description><![CDATA[  graph2vec  gl2vec  in  karateclub 需要什么？它确实提到应该没有字符串功能，但是无论有没有它，我都会遇到以下代码的错误：
 导入NetworkX为Nx
导入Karateclub为KC
导入matplotlib.pyplot作为PLT

g = nx.digraph（）

g.add_node（0，label =; a;
g.add_node（1，label =; b＆quot; feature = [1.2]）
g.add_node（2，label =; c＆quot;特征= [0.8]）

g.add_edge（0，1）
g.add_edge（0，2）
g.add_edge（1，2）

nx.draw_networkx（g，with_labels = true）
plt.show（）

图= [g]

型号= kc.graph_embedding.graph2vec（）
型号（图）
嵌入= model.get_embedding（）
打印（嵌入）
 
错误： RuntimeError：您必须在训练模型之前先构建词汇 
我在 word2vec 中看到了一个选项，但是如何为 graph2vec ？执行此操作。
是否有任何替代软件包，最好使用 KarateClub 等更简单的实现，我可以用来生成定向节点/edge属性列表 network&gt; networkx  Graphs 图形而无需定义训练/测试集吗？]]></description>
      <guid>https://stackoverflow.com/questions/79508535/node-edge-attributed-directed-graph-embedding</guid>
      <pubDate>Fri, 14 Mar 2025 08:52:01 GMT</pubDate>
    </item>
    <item>
      <title>神经网络拟合乳腺癌数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/52358505/neural-network-underfitting-breast-cancer-dataset</link>
      <description><![CDATA[我正在尝试在乳腺癌数据集上创建一个用于二进制分类的神经网络：
  https://wwwww.kaggle.com/uciml/uciml/uciml/breast-cancer-cancer-wisconsin-data-data-data-data-data-data 
我的神经网络由3层组成（不包括输入层）：

 第一层：6个带有Tanh激活的神经元。

 第二层：6个带有Tanh激活的神经元。

 最终层：1个神经元，带有Sigmoid激活。


不幸的是，我在训练示例中仅获得约44％的精度，在测试示例中的精度约为23％。
这是我的python代码：
 导入numpy作为NP
导入大熊猫作为pd
导入matplotlib.pyplot作为PLT

data = pd.read_csv（&#39;data.csv; quot;）
data = data.drop（[&#39;id&#39;]，轴= 1）
data = data.drop（data.columns [31]，轴= 1）
data = data.replace（{&#39;m&#39;：1，&#39;b&#39;：0}）

x =数据
x = x.drop（[&#39;诊断&#39;]，轴= 1）
x = np.array（x）

x_mean = np.mean（x，axis = 1，keepdims = true）
x_std = np.std（x，axis = 1，keepdims = true）
x_n =（x -x_mean） / x_std
y = np.array（数据[&#39;诊断&#39;]）
y = y.Reshape（569，1）
M = 378
y_train = y [：m，：]
y_test = y [m：，：]

x_train = x_n [：M，：]
x_test = x_n [m :，：]

Def Sigmoid（Z）：
  返回1 /（1 + np.exp（-z））

def dsigmoid（z）：
  返回np.multiply（z，（1 -z））

def tanh（z）：
  返回（np.exp（z）-np.exp（-z）） /（np.exp（z） + np.exp（-z））

def dtanh（z）：
  返回1 -np.square（tanh（z））

def成本（a，y）：
  m = y.形[0]
  返回 - （1.0/m） *np.sum（np.dot（y.t，np.log（a）） + np.dot（（（1 -y）.t，np，np.log（1 -a）））

def train（x，y，型号，epocs，a）：
  W1 =模型[&#39;W1&#39;]
  W2 =模型[&#39;W2&#39;]
  W3 =模型[&#39;W3&#39;]
  
  B1 =模型[&#39;B1&#39;]
  B2 =模型[&#39;B2&#39;]
  B3 =模型[&#39;B3&#39;]
  
  费用= []
  
  对于我的范围（EPOC）：
    
    ＃前传播

    z1 = np.dot（x，w1） + b1
    a1 = tanh（z1）

    z2 = np.dot（a1，w2） + b2
    a2 = tanh（z2）

    z3 = np.dot（a2，w3） + b3
    A3 = Sigmoid（Z3）
    
    costs.append（成本（A3，y））

    #back繁殖
    
    dz3 = z3 -y
    d3 = np.multiply（dz3，dsigmoid（z3））
    dw3 = np.dot（a2.t，d3）
    db3 = np.sum（d3，axis = 0，keepdims = true）

    d2 = np.multiply（np.dot（d3，w3.t），dtanh（z2））
    dw2 = np.dot（a1.t，d2）
    db2 = np.sum（d2，轴= 0，keepdims = true）

    d1 = np.multiply（np.dot（d2，w2.t），dtanh（z1））
    dw1 = np.dot（x.T，d1）
    db1 = np.sum（d1，axis = 0，keepdims = true）

    W1  -  =（A / M） * DW1
    W2- =（A / M） * DW2
    w3- =（a / m） * dw3

    B1  -  =（A / M） * DB1
    b2  -  =（a / m） * db2
    B3  -  =（A / M） * DB3
    
  cache = {&#39;w1&#39;：w1，&#39;w2&#39;：w2，&#39;w3&#39;：w3，&#39;b1&#39;：b1&#39;：b1，&#39;b2&#39;：b2，&#39;b3&#39;：b3}
  返回缓存，成本

np.random.seed（0）

型号= {&#39;w1&#39;：np.random.rand（30，6） * 0.01，&#39;w2&#39;：np.random.rand（6，6） * 0.01，&#39;w3&#39;：np.random.rand（6，1） &#39;b3&#39;：np.random.rand（1，1）}

型号，成本=火车（x_train，y_train，型号，1000，0.1）

plt.plot（[i在范围内（1000）]，费用）
打印（费用[999]）
plt.show（）



def预测（x，y，模型）：
  W1 =模型[&#39;W1&#39;]
  W2 =模型[&#39;W2&#39;]
  W3 =模型[&#39;W3&#39;]
  
  B1 =模型[&#39;B1&#39;]
  B2 =模型[&#39;B2&#39;]
  B3 =模型[&#39;B3&#39;]
  
  z1 = np.dot（x，w1） + b1
  a1 = tanh（z1）

  z2 = np.dot（a1，w2） + b2
  a2 = tanh（z2）

  z3 = np.dot（a2，w3） + b3
  A3 = Sigmoid（Z3）
  
  m = a3.形[0]
  y_predict = np.zeros（（M，1））
  
  对于我的范围（m）：
    y_predict = 1如果a3 [i，0]＆gt; 0.5其他0
  返回y_predict
 ]]></description>
      <guid>https://stackoverflow.com/questions/52358505/neural-network-underfitting-breast-cancer-dataset</guid>
      <pubDate>Sun, 16 Sep 2018 21:24:54 GMT</pubDate>
    </item>
    <item>
      <title>训练后如何用分布的时间来替换嵌入层？</title>
      <link>https://stackoverflow.com/questions/39532572/how-to-replace-an-embedding-layer-with-a-time-distributed-dense-after-training</link>
      <description><![CDATA[我有以下问题：

 我想使用LSTM网络进行文本分类。为了加快训练的速度并使代码更加清楚，我想沿沿 keras.tokenizer 嵌入层以训练我的模型。 
 一旦我训练了我的模型 - 我想计算输出W.R.T.的显着性图。输入。为此，我决定将嵌入层替换为 timeDistributedDense 。 

您知道什么是最好的方法。对于一个简单的模型，我可以简单地使用已知权重的模型来重建模型 - 但我想使其尽可能通用 - 例如替换模型结构的未来并使我的框架尽可能不可知。]]></description>
      <guid>https://stackoverflow.com/questions/39532572/how-to-replace-an-embedding-layer-with-a-time-distributed-dense-after-training</guid>
      <pubDate>Fri, 16 Sep 2016 13:21:25 GMT</pubDate>
    </item>
    </channel>
</rss>