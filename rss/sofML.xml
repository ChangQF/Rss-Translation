<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 31 May 2024 09:16:46 GMT</lastBuildDate>
    <item>
      <title>多输出分类器低分</title>
      <link>https://stackoverflow.com/questions/78559087/multioutputclassifier-low-score</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78559087/multioutputclassifier-low-score</guid>
      <pubDate>Fri, 31 May 2024 09:01:14 GMT</pubDate>
    </item>
    <item>
      <title>即使指定了某些列，Pandas 也会获取数据框的所有列</title>
      <link>https://stackoverflow.com/questions/78559070/pandas-takes-all-columns-of-a-dataframe-even-when-some-columns-are-specified</link>
      <description><![CDATA[我正在尝试使用 Scikit-Learn 训练 KMeans 模型。
我在这个问题上被困了 2 天。
尽管我指定了 2 列，但 Pandas 还是选择了数据框的所有列。
以下是代码：
cols=[&#39;area&#39;, &#39;perimeter&#39;, &#39;compactness&#39;, &#39;length&#39;, &#39;width&#39;, &#39;asymmetry&#39;, &#39;groove&#39;, &#39;class&#39;]

x = &#39;perimeter&#39;
y = &#39;asymmetry&#39;
z = df[[x, y]].values

kmeans = KMeans(n_clusters=3).fit(z)

clusters = kmeans.labels_

print(clusters)

cluster_df = pd.DataFrame(np.hstack((z, clusters.reshape(-1, 1))), columns=[x, y, &quot;class&quot;])

sns.scatterplot(x=x, y=y, hue=&#39;class&#39;, data=cluster_df)
plt.show()

sns.scatterplot(x=x, y=y, hue=&#39;class&#39;, data=df)
plt.show()

这是原始数据集的图
这是预测图
我原本以为预测图会像原始数据集图一样。
预测图看起来像 sns 将所有不同的列绘制在一起。
PS：我是初学者，所以我可能不太了解，所以请不要不喜欢。]]></description>
      <guid>https://stackoverflow.com/questions/78559070/pandas-takes-all-columns-of-a-dataframe-even-when-some-columns-are-specified</guid>
      <pubDate>Fri, 31 May 2024 08:59:38 GMT</pubDate>
    </item>
    <item>
      <title>如何在多标签分类中实现类别权重采样？</title>
      <link>https://stackoverflow.com/questions/78559061/how-to-implement-class-weight-sampling-in-multi-label-classification</link>
      <description><![CDATA[我正在研究一个多标签分类问题，需要一些使用 Scikit-Learn 计算类别权重的指导。
问题背景：
我有一个包含 9973 个训练样本的数据集。标签是独热编码的，代表 13 个不同的类别。我的训练标签的形状是 (9973, 13)。
我想使用此代码：
import numpy as np
from sklearn.utils.class_weight import compute_class_weight

y_integers = np.argmax(y, axis=1)
class_weights = compute_class_weight(&#39;balanced&#39;, np.unique(y_integers), y_integers)
d_class_weights = dict(enumerate(class_weights))

这不起作用，因为位置参数太多。我的训练样本如下所示：
 [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],

我该如何在多类分类问题中实现，以便解决我的数据集不平衡问题？]]></description>
      <guid>https://stackoverflow.com/questions/78559061/how-to-implement-class-weight-sampling-in-multi-label-classification</guid>
      <pubDate>Fri, 31 May 2024 08:57:17 GMT</pubDate>
    </item>
    <item>
      <title>成本函数最小值太低</title>
      <link>https://stackoverflow.com/questions/78558974/cost-function-minimum-is-too-low</link>
      <description><![CDATA[下面是我正在尝试编写的神经网络的一段 Matlab 代码。这是我第一次尝试与机器学习相关的任何事情。我在这里跟随 Michael Nielson 的书：http://neuralnetworksanddeeplearning.com/chap2.html
我正在加载一组 60000 张 28x28 灰度手写数字图像，并尝试训练这个神经网络来识别它们。还有一个包含 10000 张图像的测试数据集。该网络有 784 个输入神经元（28^2），两个隐藏层，每个隐藏层有 16 个神经元，输出层有 10 个神经元。我将权重和偏差初始化为 -0.5 和 0.5 之间的随机值。
我将成本函数评估为 C = 0.5*(a-y).^2。它似乎取得了一定成功，因为它从 C=1.35 开始，在 C=0.46 结束，然后基本趋于平稳（大约 75 个时期）。但是，错误仍然很高，只有 12% 的时间能猜出正确的数字，这几乎是随机的。我反复检查了数学，但找不到错误。我想一定有一个我没有看到的。下面的代码是主训练循环中的所有内容，因此任何错误都应该在那里。我没有将图像分成更小的批次，而是在每个时期一次处理整个 60k 图像。由于每张图像只有 28x28 像素，因此无需将其分开就足够快了。输入神经元 a_0 是一个 784x60000 的双精度数组，值介于 0 和 1 之间。我获取了原始图像，其中每个像素都是一个 uint8，然后将其转换为双精度，然后除以 255 得到 a_0。我在代码中对层进行编号，其中第 0 层是输入层，第 1 层和第 2 层是隐藏层，第 3 层是输出层。
a_0 = training_images;
epoch = 0;
while epoch &lt; 5 || C(epoch - 1) - C(epoch) &gt; 0.001 
epoch = epoch + 1;

%向前传播
z_1 = weights_1*a_0 + biases_1;
a_1 = sigmoid(z_1);
z_2 = weights_2*a_1 + biases_2;
a_2 = sigmoid(z_2);
z_3 = weights_3*a_2 + biases_3;
a_3 = sigmoid(z_3);

%评估成本函数
C(epoch) = 0.5*mean(sum((a_3-y).^2, 1));

%向后传播
sigmoid_d1 = a_1 .* (1-a_1); %Sigmoid 导数
sigmoid_d2 = a_2 .* (1-a_2);
sigmoid_d3 = a_3 .* (1-a_3);
delta_3 = (a_3-y).*sigmoid_d3;
delta_2 = weights_3.&#39;*delta_3 .* sigmoid_d2;
delta_1 = weights_2.&#39;*delta_2 .* sigmoid_d1;

%计算梯度
for image_index = 1:num_images
dC_dw3(:, :, image_index) = delta_3(:, image_index) * a_2(:, image_index).&#39;;
dC_dw2(:, :, image_index) = delta_2(:, image_index) * a_1(:, image_index).&#39;;
dC_dw1(:, :, image_index) = delta_1(:, image_index) * a_0(:, image_index).&#39;;
end

%计算调整
training_rate = 0.1;
adjust_biases_1 = -training_rate * mean(delta_1, 2);
adjust_biases_2 = -training_rate * mean(delta_2, 2);
adjust_biases_3 = -training_rate * mean(delta_3, 2);
调整权重1 = -训练速率 * 平均值(dC_dw1, 3);
调整权重2 = -训练速率 * 平均值(dC_dw2, 3);
调整权重3 = -训练速率 * 平均值(dC_dw3, 3);
偏差1 = 偏差1 + 调整偏差1;
偏差2 = 偏差2 + 调整偏差2;
偏差3 = 偏差3 + 调整偏差3;
权重1 = 权重1 + 调整权重1;
权重2 = 权重2 + 调整权重2;
权重3 = 权重3 + 调整权重3;
]]></description>
      <guid>https://stackoverflow.com/questions/78558974/cost-function-minimum-is-too-low</guid>
      <pubDate>Fri, 31 May 2024 08:39:40 GMT</pubDate>
    </item>
    <item>
      <title>通过单一数字指标训练 XGBoost</title>
      <link>https://stackoverflow.com/questions/78558863/training-xgboost-over-a-single-number-metric</link>
      <description><![CDATA[假设我正在用 Python（xgboost 版本 2.0.3）构建一个 XGBoost 模型（这里是回归还是分类完全不重要）来预测股票市场时间序列分析中的目标变量。
例如，目标可能是：时间序列中的下一个值或二进制变量，如果下一个值高于前一个值，则设置为 1，否则设置为 0。
为了训练模型，是否可以使用回归问题中的 MSE 或分类问题中的“二元逻辑”。
训练后，可以根据测试集中模型的输出对策略进行回测并计算总体回报。
我的问题是：使用 xgboost scikit-learn 接口，是否可以在用于回测策略的性能指标上训练模型？
例如：按照策略最大化训练集中的总体回报规则。
在xgboost库网站上，展示了如何使用自定义损失函数来训练模型：
def softprob_obj(labels: np.ndarray, predt: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:
rows = labels.shape[0]
classes = predt.shape[1]
grad = np.zeros((rows, classes), dtype=float)
hess = np.zeros((rows, classes), dtype=float)
eps = 1e-6
for r in range(predt.shape[0]):
target = labels[r]
p = softmax(predt[r, :])
for c in range(predt.shape[1]):
g = p[c] - 1.0 if c == target else p[c]
h = max((2.0 * p[c] * (1.0 - p[c])).item(), eps)
grad[r, c] = g
hess[r, c] = h

grad = grad.reshape((rows * classes, 1))
hess = hess.reshape((rows * classes, 1))
return grad, hess

clf = xgb.XGBClassifier(tree_method=&quot;hist&quot;, objective=softprob_obj)

目标函数需要计算梯度和 hessian。
假设函数定义如下：
def maximum_performance_metric(y_true: np.ndarray, y_pred: np.ndarray):
# 指标计算（例如：使用 y_pred 计算总体回报
overall_return = get_overall_return(y_pred, real_prices, ...) #overall_return 是浮点数
return grad, hess


是否可以根据总体回报计算梯度和 hessian，然后使用此自定义损失训练模型函数？
函数maximize_performance_metric如何访问包含real_prices的变量（需要整体回报计算？]]></description>
      <guid>https://stackoverflow.com/questions/78558863/training-xgboost-over-a-single-number-metric</guid>
      <pubDate>Fri, 31 May 2024 08:14:07 GMT</pubDate>
    </item>
    <item>
      <title>如何将网络安全应用于机器学习算法</title>
      <link>https://stackoverflow.com/questions/78558772/how-could-i-apply-network-security-to-machine-learning-algorithm</link>
      <description><![CDATA[毕业设计
大家好，我是一名计算机科学专业的学生，​​我正在做一个毕业设计，主题是基于机器学习算法的再生相关基因研究，它将分析数据并确定某些生物体中的特定基因以加速或控制再生，但是我想在项目中加入一些网络安全方面的内容，你们知道我该怎么做吗？
我想做云计算，然后封装数据供用户访问。然而，这将无关紧要，因为用户将无法输入任何数据。我只需要训练算法来找到特定的基因]]></description>
      <guid>https://stackoverflow.com/questions/78558772/how-could-i-apply-network-security-to-machine-learning-algorithm</guid>
      <pubDate>Fri, 31 May 2024 07:53:15 GMT</pubDate>
    </item>
    <item>
      <title>NLP输入数据集质量评估</title>
      <link>https://stackoverflow.com/questions/78558228/nlp-input-dataset-quality-evaluation</link>
      <description><![CDATA[是否有任何方法可以评估将用于 LLM 任务的文本输入数据的质量（使用 F1 分数或其他指标）？
我听说我们可以使用 GenAI 来完成这项任务，但没有详细的解释。
（我是这个领域的新手）]]></description>
      <guid>https://stackoverflow.com/questions/78558228/nlp-input-dataset-quality-evaluation</guid>
      <pubDate>Fri, 31 May 2024 05:23:07 GMT</pubDate>
    </item>
    <item>
      <title>使用句子相似度 NLP 匹配相似度较高的字符串</title>
      <link>https://stackoverflow.com/questions/78558021/matching-strings-with-high-similarity-using-sentence-similarity-nlp</link>
      <description><![CDATA[因此，目前我的数据库中有一个向量列表，并且我从 API 获取数据，从该 API 我循环遍历提供的每个字符串，将其转换为向量并匹配数据库中最相似的字符串。问题是，API 提供的名称与我存储在数据库中的名称不同，尽管它们是相同的。
例如，在我的数据库中，我有两所大学，分别名为 SUNY College of Technology at Alfred 和 Alfred University。从 API 中我返回的大学名称是 Alfred State College 和 Alfred University。显然，句子相似度将为 Alfred University 提供完美的相似度，但 Alfred State College 不会与 SUNY College of Technology at Alfred 匹配，而是与 Alfred University 匹配，我明白为什么它们尚未匹配，尽管名称不同，但它们是同一所大学。我能做些什么来使系统更准确？
我尝试将大学所在州添加到向量中，然后根据大学名称和州匹配一个向量，但这两所大学都是同一个州，所以这是一条死路。我正在考虑创建一些函数，如果有多个匹配项，它将暂缓处理该数据，然后将其推送到数组中。它会继续，直到找到相似度为 1 的匹配项，然后它会区分两者，并将最不准确的结果给予相似度较低的匹配项。这会起作用吗？它会叫什么？
我能做什么？]]></description>
      <guid>https://stackoverflow.com/questions/78558021/matching-strings-with-high-similarity-using-sentence-similarity-nlp</guid>
      <pubDate>Fri, 31 May 2024 03:46:06 GMT</pubDate>
    </item>
    <item>
      <title>训练 YOLOv8 进行裁判姿势估计</title>
      <link>https://stackoverflow.com/questions/78557557/training-yolov8-for-pose-estimation-of-referees</link>
      <description><![CDATA[我目前正在使用预训练的 YOLOv8 模型训练姿势估计模型。我的目标是检测图像中的体育裁判并确定他们的姿势。我想知道我是否可以简单地标记图像中的裁判，让 YOLO 模型将他们识别为人，然后确定关键点，而不是手动注释数据集图像中每个裁判的关键点。
这种方法可行吗？如果可行，最好的实现方法是什么？任何指导或建议都将不胜感激。
到目前为止，我已经注释了裁判的标签并使用了物体检测模型。但是，首先使用物体检测，然后对裁判标签应用姿势估计会变得非常耗费计算资源。]]></description>
      <guid>https://stackoverflow.com/questions/78557557/training-yolov8-for-pose-estimation-of-referees</guid>
      <pubDate>Thu, 30 May 2024 23:15:49 GMT</pubDate>
    </item>
    <item>
      <title>XGBtree 模型准确率报告 1</title>
      <link>https://stackoverflow.com/questions/78557288/xgbtree-model-reporting-1-in-accuracy-metric</link>
      <description><![CDATA[我正在尝试使用此代码和 {caret} 包训练一个 XGB 模型，用于对正面或负面评论进行分类。
这似乎是正确的，但我得到的准确率全是 1？
这个模型是否过度拟合？
#install.packages(&quot;xgboost&quot;)
library(xgboost)
library(readxl)
library(caret)
library(e1071)

# 加载带有标签的原始 DataFrame
df_1 &lt;- read_excel(&quot;path/to/excel&quot;)

# 将标签转换为因子
df_1$label &lt;- as.factor(df_1$label)

train_control &lt;- trainControl(method = &quot;repeatedcv&quot;, 
number = 5, 
repeats = 10,
summaryFunction = defaultSummary,
classProb = TRUE,
savePredictions = &quot;final&quot;)

scale_pos_weight &lt;- c(&quot;X0&quot; = 1.53, &quot;X1&quot; = 1) 

xgb_model &lt;- train(x = as.matrix(sapply(df_1, as.numeric), weights = scale_pos_weight), 
y = df_1$label,
method = &quot;xgbTree&quot;,
trControl = train_control)

如何修复此问题？]]></description>
      <guid>https://stackoverflow.com/questions/78557288/xgbtree-model-reporting-1-in-accuracy-metric</guid>
      <pubDate>Thu, 30 May 2024 21:19:31 GMT</pubDate>
    </item>
    <item>
      <title>fastapi 机器学习脚本中的结果为空</title>
      <link>https://stackoverflow.com/questions/78556222/null-results-in-a-fastapi-machine-learning-script</link>
      <description><![CDATA[我正在开发一个 fastAPI 简单机器学习脚本。我已经成功训练了管道，它按预期工作。但现在我正尝试使用 fastAPI 和 uvicorn 来部署它。我不知道为什么它总是返回一个空值作为结果。这是一个二元分类。一些数据输入，1 或 0 应该输出。现在我这样做了：
从 pydantic 导入 BaseModel
从 fastapi 导入 FastAPI
导入 pandas 作为 pd
导入 joblib

导入 uvicorn

# 实例
app = FastAPI()

# 数据模型
class DataModel(BaseModel):
id: str
radius_mean: float
...
# 除必须预测的目标变量之外的所有其他特征

class ItemOut(BaseModel):
id: str
诊断：int

def load_weights():
model = joblib.load(&quot;models/RF/RF_weights_v1.0.pkl&quot;)
返回模型

def predict(data):
model = load_weights()
preprocessing = model.named_steps[&quot;preprocessing&quot;]
perceived_data = preprocessing.transform(data)
classifier = model.named_steps[&quot;classifier&quot;]
predict_values = classifier.predict(transformed_data)
return predict_values

@app.post(&quot;/predict&quot;, response_model=ItemOut)
async def predict(dataModel: DataModel):
data = pd.DataFrame([dataModel.dict()])
predictions = predict(data)
return {&quot;id&quot;: &quot;id&quot;, &quot;diagnosis&quot;: predictions}

if __name__ == &quot;__main__&quot;:
uvicorn.run(app, host=&quot;0.0.0.1&quot;, port=8000)

现在，我不知道模型的权重是否正确加载。最重要的是，预测的结果应该是 1 维数组（在这种情况下只有一个值），根据分类结果为 0 或 1。
但返回的却是空值。
我使用的模型是“SVC”（我用随机森林做了第二个模型，没什么特别的）。
我做错了什么？
同样，我不知道这是否是最好的方法，或者例如，最佳实践应该是加载包含这些值的 json 或加载包含多个值的 json 文件。]]></description>
      <guid>https://stackoverflow.com/questions/78556222/null-results-in-a-fastapi-machine-learning-script</guid>
      <pubDate>Thu, 30 May 2024 16:27:16 GMT</pubDate>
    </item>
    <item>
      <title>具有不平衡类别的 U-Net 分割的图像块提取</title>
      <link>https://stackoverflow.com/questions/78555784/image-patch-extraction-for-u-net-segmentation-with-imbalanced-classes</link>
      <description><![CDATA[我正在使用 U-Net 进行多类图像分割项目。
我的数据集的类别分布不平衡。有些类别几乎出现在每幅图像中，而其他类别则很少见。我不确定图像修补的最佳方法：
在每个补丁内做出相等的类别表示？在这种情况下，对于某些类别，我将不得不使用数据增强技术。
还是保持补丁内原始的不平衡比例？]]></description>
      <guid>https://stackoverflow.com/questions/78555784/image-patch-extraction-for-u-net-segmentation-with-imbalanced-classes</guid>
      <pubDate>Thu, 30 May 2024 14:57:49 GMT</pubDate>
    </item>
    <item>
      <title>相册强度增强会破坏图像</title>
      <link>https://stackoverflow.com/questions/78477344/albumentations-intensity-augmentations-disrupt-the-image</link>
      <description><![CDATA[我使用预处理的 z 分数标准化列表作为数据集的来源。
这是 Albumentations 增强的图像拼贴图：
在此处输入图像描述
这是我的 Compose：
augmentation = A.Compose([
A.Horizo​​ntalFlip(),
A.RandomBrightnessContrast(brightness_limit=(-0.0001, 0.0001),contrast_limit=(-0.01, 0.01)),
A.CoarseDropout(8, 0.1, 0.1),
A.Rotate(limit=15),
A.Affine(shear=(-2, 2), scale=(0.95, 1.05)),
&gt;! ToTensorV2()
])

在 50% 的图像上，即使使用非常小的参数，应用 RandBrightnessContrast 后，图像的整个分布也会压缩到 [0, 1]（从 z 分数标准化图像的预期值 ~-2,~2）。
有什么办法吗？
也许我应该在这些之后执行 z 分数标准化，但我的初衷是将所有确定性步骤（调整大小、标准化等）与增强步骤分开以提高效率。]]></description>
      <guid>https://stackoverflow.com/questions/78477344/albumentations-intensity-augmentations-disrupt-the-image</guid>
      <pubDate>Tue, 14 May 2024 10:11:54 GMT</pubDate>
    </item>
    <item>
      <title>用于多标签分类的 CLIP</title>
      <link>https://stackoverflow.com/questions/74927358/clip-for-multi-label-classification</link>
      <description><![CDATA[我正在使用 CLIP 来确定单词和图像之间的相似性。
目前我正在使用此 repo 和以下代码，对于分类，它给出了很好的结果。我需要它来进行多标签分类，其中我需要使用 sigmoid 而不是 softmax。
import torch
from PIL import Image
import open_clip

model, _, preprocess = open_clip.create_model_and_transforms(&#39;ViT-B-32-quickgelu&#39;, pretrained=&#39;laion400m_e32&#39;)
tokenizer = open_clip.get_tokenizer(&#39;ViT-B-32-quickgelu&#39;)

image = preprocess(Image.open(&quot;CLIP.png&quot;)).unsqueeze(0)
text = tokenizer([&quot;a diagram&quot;, &quot;a dog&quot;, &quot;a cat&quot;])

with torch.no_grad(), torch.cuda.amp.autocast():
image_features = model.encode_image(image)
text_features = model.encode_text(text)
image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)

text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)

print(&quot;Label probs:&quot;, text_probs) # prints: [[1., 0., 0.]]

现在我想将它用于多类。例如，如果我们在图像上有狗和猫，我希望两者都有较高的概率，所以我需要用 sigmoid 来运行它。但是这给我的结果都在 0.55 左右，正确的类别是 0.56，错误的是 0.54，所以结果是这样的 [0.54, 0.555, 0.56]。使用 S 形函数后，我希望得到 [0.01, 0.98, 0.99] 这样的值。
我做错了什么？我怎样才能得到我想要的结果？]]></description>
      <guid>https://stackoverflow.com/questions/74927358/clip-for-multi-label-classification</guid>
      <pubDate>Tue, 27 Dec 2022 08:56:22 GMT</pubDate>
    </item>
    <item>
      <title>如何将文档分成训练集和测试集？</title>
      <link>https://stackoverflow.com/questions/42471570/how-can-i-split-documents-into-training-set-and-test-set</link>
      <description><![CDATA[我正在尝试构建一个分类模型。本地文件夹中有 1000 个文本文档。我想将它们分成训练集和测试集，分割比例为 70:30（70 → 训练和 30 → 测试）。有什么更好的方法吗？我正在使用 Python。

我想要一种以编程方式分割训练集和测试集的方法。首先读取本地目录中的文件。其次，建立这些文件的列表并对其进行随机排序。第三，将它们分成训练集和测试集。
我尝试了几种使用内置 Python 关键字和函数的方法，但都失败了。最后，我想到了接近它的想法。此外，交叉验证也是构建一般分类模型的一个不错的选择。]]></description>
      <guid>https://stackoverflow.com/questions/42471570/how-can-i-split-documents-into-training-set-and-test-set</guid>
      <pubDate>Sun, 26 Feb 2017 17:08:29 GMT</pubDate>
    </item>
    </channel>
</rss>