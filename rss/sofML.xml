<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 06 May 2024 06:20:03 GMT</lastBuildDate>
    <item>
      <title>如何在tensorflow中读入csv文件</title>
      <link>https://stackoverflow.com/questions/78434695/how-to-read-into-a-csv-file-in-tensorflow</link>
      <description><![CDATA[我正在使用tensorflow在conda环境中进行机器学习。在我的代码中，当我尝试读入我的 train.csv 和 test.csv 时，出现问题，如下代码所示：
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39;
将张量流导入为 tf
从张量流导入keras
从tensorflow.keras导入层，正则化器
从tensorflow.keras.datasets导入mnist
将 pandas 导入为 pd
从tensorflow.keras.optimizers.legacy导入Adam

#超级参数

批量大小 = 64
WEIGHT_DECAY = 0.001
学习率 = 0.001

train_df = pd.read_csv(“train.csv”)
test_df = pd.read_csv(“test.csv”)
train_images = os.getcwd() + “/train_images/” + train_df.iloc[:, 0].values
test_images = os.getcwd() + “/test_images/” + test_df.iloc[:, 0].values

train_labels = train_df.iloc[:, 1:].values
test_labels = test_df.iloc[:, 1:].values

def read_image(图像路径, 标签):
  图像 = tf.io.read_file(image_path)
  图像 = tf.image.decode_image(图像, 通道=1, dtype=tf.float32)

  图像.set_shape((64, 64, 1))
  标签[0].set_shape([])
  标签[1].set_shape([])

  标签 = {“first_num”：标签[0]，“second_num”：标签[1]}
  返回图像、标签

自动调谐 = tf.data.experimental.AUTOTUNE
train_dataset = tf.data.Dataset.from_tensor_slices(
  （火车图像，火车标签）
）
训练数据集 = (
  train_dataset.shuffle(buffer_size=len(train_labels))
  .map(读取图像)
  .batch(batch_size=BATCH_SIZE)
  .prefetch(buffer_size=AUTOTUNE)
）

test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))
测试数据集 = (
  test_dataset.map(read_image)
  .batch(batch_size=BATCH_SIZE)
  .prefetch(buffer_size=AUTOTUNE)
）

输入 = keras.Input(形状= (64, 64, 1))
x = 层.Conv2D(
  过滤器=32，
  内核大小=3，
  填充=&#39;相同&#39;，
  kernel_regularizer=regularizers.l2(WEIGHT_DECAY),
)(输入)
x = 层.BatchNormalization()(x)
x = keras.activations.relu(x)
x = 层.Conv2D(
  64, 3, kernel_regularizer=regularizers.l2(WEIGHT_DECAY),
）（X）
x = 层.BatchNormalization()(x)
x = keras.activations.relu(x)
x = 层数.MaxPooling2D()(x)
x = 层.Conv2D(
  64、
  3、
  激活=&#39;relu&#39;,
  kernel_regularizer=regularizers.l2(WEIGHT_DECAY),
）（X）

x= 层.Conv2D(128, 3, 激活=&#39;relu&#39;)(x)
x = 层数.MaxPooling2D()(x)
x = 层.Flatten()(x)
x = 层.Dense(128, 激活=&#39;relu&#39;)(x)
x = 层.Dropout(0.5)(x)
x = 层.Dense(64, 激活=&#39;relu&#39;)(x)
输出1 = 层.Dense(10, 激活=&#39;softmax&#39;, 名称=&#39;first_num&#39;)(x)
输出2 = 层.Dense(10, 激活=&#39;softmax&#39;, 名称=&#39;second_num&#39;)(x)
模型= keras.Model（输入=输入，输出=[输出1，输出2]）

打印（模型.摘要（））

模型.编译(
  损失=[keras.losses.SparseCategoricalCrossentropy(),
        keras.losses.SparseCategoricalCrossentropy(),
        ],
  指标=[“准确度”,“准确度”],
）

model.fit(train_dataset, epochs=5, verbose=2)
model.evaluate(train_dataset, verbose=2)

错误开始于...
&lt;前&gt;&lt;代码&gt;
2024-05-06 06:09:12.539105: E external/local_tsl/tsl/platform/windows/windows_file_system.cc:363] 错误：GetSymbolicLinkTarget 无法打开 \\?\C:\Users\USER\CODE\PycharmProjects\ 的文件pythonProject2Conda\train_images\129_97.png GetLastError：2


对于要读取的不同图像，以下错误类似，并以以下结尾：
回溯（最近一次调用最后一次）：
  文件“C:\Users\USER\CODE\PycharmProjects\pythonProject2Conda\main.py”，第 94 行，在  中
    model.fit(train_dataset, epochs=5, verbose=2)
  文件“C:\Users\USER\anaconda3\envs\tf_cpu\Lib\site-packages\keras\src\utils\traceback_utils.py”，第 123 行，在 error_handler 中
    从 None 引发 e.with_traceback(filtered_tb)
  文件“C:\Users\USER\anaconda3\envs\tf_cpu\Lib\site-packages\tensorflow\python\eager\execute.py”，第 53 行，在 Quick_execute 中
    张量 = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError：图执行错误：

在定义于（最近一次调用最后）的节点 ReadFile 处检测到：
&lt;堆栈跟踪不可用&gt;
NewRandomAccessFile 无法创建/打开：C:\Users\USER\CODE\PycharmProjects\pythonProject2Conda/train_images/129_97.png ：系统找不到指定的文件。
;没有这样的文件或目录
     [[{{节点读取文件}}]]
     [[IteratorGetNext]] [操作：__inference_one_step_on_iterator_2745]


请问我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78434695/how-to-read-into-a-csv-file-in-tensorflow</guid>
      <pubDate>Mon, 06 May 2024 05:33:44 GMT</pubDate>
    </item>
    <item>
      <title>如何将我训练过的模型移动到另一个 Python 笔记本中？</title>
      <link>https://stackoverflow.com/questions/78434627/how-can-i-move-my-trained-model-to-another-python-notebook</link>
      <description><![CDATA[我有一个Python笔记本，其中包含多个函数，例如pre_process，..并且它包含我训练的模型。现在我应该提交一个 python 笔记本，其中包含我训练过的模型以进行进一步测试。我怎样才能做到这一点？
我知道在运行单元格时，它会保存所有单元格使用的变量，我可以链接这两个文件并从那里调用模型吗？]]></description>
      <guid>https://stackoverflow.com/questions/78434627/how-can-i-move-my-trained-model-to-another-python-notebook</guid>
      <pubDate>Mon, 06 May 2024 05:04:24 GMT</pubDate>
    </item>
    <item>
      <title>关于 onehotencoder 空间成本</title>
      <link>https://stackoverflow.com/questions/78434421/regarding-onehotencoder-space-cost</link>
      <description><![CDATA[为什么 onehotencoding 不使用基于位的编码？占用的内存不是少很多吗？我的意思是，当您对四个城市进行编码时，您可以像 onehotencoder 那样将一列扩展到 4 或在一列中这样进行：
 第一个城市 = 0(base10) = 00000000
    第二个城市 = 1(base10) = 00000001
    第三个城市 = 2(base10) = 00000010
    第四个城市 = 3(base10) = 00000011

。
就内存成本而言，这不是更有效吗？或者编码技术是否迫使它占用更多空间？]]></description>
      <guid>https://stackoverflow.com/questions/78434421/regarding-onehotencoder-space-cost</guid>
      <pubDate>Mon, 06 May 2024 03:24:38 GMT</pubDate>
    </item>
    <item>
      <title>如何在多个 GPU 上进行训练？</title>
      <link>https://stackoverflow.com/questions/78433502/how-do-i-train-on-multiple-gpus</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78433502/how-do-i-train-on-multiple-gpus</guid>
      <pubDate>Sun, 05 May 2024 19:27:19 GMT</pubDate>
    </item>
    <item>
      <title>向 Python Streamlit 饮食推荐应用程序添加饮食偏好按钮</title>
      <link>https://stackoverflow.com/questions/78433479/adding-dietary-preference-button-to-python-streamlit-diet-recommendation-app</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78433479/adding-dietary-preference-button-to-python-streamlit-diet-recommendation-app</guid>
      <pubDate>Sun, 05 May 2024 19:19:19 GMT</pubDate>
    </item>
    <item>
      <title>将 PyG 数据对象列表转换为 PyG 数据集？</title>
      <link>https://stackoverflow.com/questions/78433332/turning-a-list-of-pyg-data-objects-into-a-pyg-dataset</link>
      <description><![CDATA[我有一个 torch_geometric.data.Data 对象的 python 列表（每个对象代表一个图形）。我没有简单的方法来访问这些数据的原始文件：我只有列表。我需要将此数据对象列表转换为 torch_geometric.data.InMemoryDataset 或 torch_geometric.data.Dataset 对象，以便将其与我未编写的更大代码库集成。我该怎么做？
需要明确的是，我知道可以使用一系列数据对象来创建 torch_geometric.data.DataLoader 对象。但是，我特别需要一个 Dataset 对象，而不是 DataLoader 对象，因为较大的代码库在将 Dataset 对象转换为加载器之前会对它们执行一些额外的处理步骤。
我不明白为什么 PyG 让这变得如此困难。难道没有一种非常简单的方法可以做到这一点吗？
我尝试使用一个简单的 CustomDataset 类
类 CustomDataset(InMemoryDataset):
    def __init__(自身，数据)：
        超级().__init__()
        self.data = 数据
    
    def __len__(自身):
        返回 len(self.data)
    
    def __getitem__(self, idx):
        样本 = self.data[idx]
        返回样品

当尝试获取索引 0 处的 Data 对象时，它给了我一个 KeyIndex 错误。我还尝试了上述代码的一个版本，其中超类是 Dataset 而不是 InMemoryDataset，但我不知道如何制作整理方法有效。]]></description>
      <guid>https://stackoverflow.com/questions/78433332/turning-a-list-of-pyg-data-objects-into-a-pyg-dataset</guid>
      <pubDate>Sun, 05 May 2024 18:30:03 GMT</pubDate>
    </item>
    <item>
      <title>如何在自定义估计器上使用 GridSearchCV？</title>
      <link>https://stackoverflow.com/questions/78433029/how-to-use-gridsearchcv-on-a-customized-estimator</link>
      <description><![CDATA[我使用 sklearn BaseEstimator 和 ClassifierMixin 构建了一个自定义 Estimator。但是当涉及到交叉验证时，GridSearchCV 给我的分数是 nan 值。
这是估计器的代码：
class RegressionClassifier(ClassifierMixin, BaseEstimator):
    def __init__(self, 回归器=RidgeCV(cv=10, fit_intercept=True), alpha=0, n_components=1):
        self.alpha = alpha
        self.n_组件 = n_组件
        self.regressor = 回归器
        自估计器=无

    def fit(自身, X, y):
        管道=管道（步骤=[
            (&#39;估算&#39;，SimpleImputer(missing_values=np.nan，策略=“中位数”))，
            （&#39;缩放&#39;，StandardScaler（）），
            (&#39;减少&#39;, PCA(self.n_components)),
            (&#39;回归&#39;, self.regressor)
        ]）
        self.estimator = pipeline.fit(X, y)
        返回自我
        
    def 预测（自身，X）：
        预测 = self.estimator.predict(X)
        转换器 = [
        预测&lt; -自我阿尔法，
        (-self.alpha &lt;= 预测) &amp; （预测&lt; self.alpha），
        预测 &gt;= self.alpha
         ]
        类 = [2, 0, 0]
        Predicted_class = np.select（转换器，类）
        返回预测类

    def 分数(自身, X, y):
        y_true = train_new_y[y.index]
        返回准确度分数（y_true，y_true）


这应该返回 1 作为分数，因为我计算了相同预测的准确性。
估算器的工作原理如下：

管道（进行线性回归）-&gt;输出（回归结果向量）

转换器（获取管道结果）-&gt;输出（对回归结果进行一些神奇操作的类）

分数应该采用生成的类以及这些类和目标向量之间的输出准确性


网格搜索结果：
对 100 名候选者每人拟合 5 次，总共 500 次拟合
[CV 1/5] END ........alpha=0.0, n_components=50.0;, 分数=nan 总时间= 0.2s
[CV 2/5] END ........alpha=0.0, n_components=50.0;, 分数=nan 总时间= 0.1s
[CV 3/5] END ........alpha=0.0, n_components=50.0;, 分数=nan 总时间= 0.2s
]]></description>
      <guid>https://stackoverflow.com/questions/78433029/how-to-use-gridsearchcv-on-a-customized-estimator</guid>
      <pubDate>Sun, 05 May 2024 16:51:52 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Torchvison Mixup 解决多标签问题？</title>
      <link>https://stackoverflow.com/questions/78432917/how-to-use-torchvison-mixup-with-a-multilabel-problem</link>
      <description><![CDATA[我想使用 MixUp 作为多标签图像分类问题的数据增强，但它似乎只支持形状为 (batch_size,) 的标签。我该如何修改它以允许使用 [0.0, 1.0, 0.0, 1.0] 这样的标签
我尝试在我的标签向量 [0.0, 1.0, 0.0, 1.0] 上调用 argmax，这给了我一个索引。这使函数工作，但它只对单个类进行混合。]]></description>
      <guid>https://stackoverflow.com/questions/78432917/how-to-use-torchvison-mixup-with-a-multilabel-problem</guid>
      <pubDate>Sun, 05 May 2024 16:17:28 GMT</pubDate>
    </item>
    <item>
      <title>在自定义数据上训练 Mask R-CNN，但训练不会停止并且不会产生输出或错误</title>
      <link>https://stackoverflow.com/questions/78430026/training-mask-r-cnn-on-custom-data-but-the-training-doesnt-stop-and-produces-n</link>
      <description><![CDATA[以下是我的流程的简要概述：

我通过将边界框的 SAM 掩码应用到我的图像，使用 PyTorch 生成数据集。
创建数据集后，我将其分为训练集和测试集。
我使用 torch.utils.data.DataLoader 加载这两个集合。
我使用的是包含 11 个类别的预训练模型。

但是，我在训练过程中遇到了问题。该过程似乎花费了异常长的时间，并且我没有看到任何可从中进行故障排除的进度或错误消息。
我的数据集

可能出了什么问题或者如何改进我的培训过程？]]></description>
      <guid>https://stackoverflow.com/questions/78430026/training-mask-r-cnn-on-custom-data-but-the-training-doesnt-stop-and-produces-n</guid>
      <pubDate>Sat, 04 May 2024 18:34:21 GMT</pubDate>
    </item>
    <item>
      <title>显示检测到但无法识别的面孔的名称时出现问题</title>
      <link>https://stackoverflow.com/questions/78429558/issues-with-displaying-the-name-of-detected-faces-that-arent-recognized</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78429558/issues-with-displaying-the-name-of-detected-faces-that-arent-recognized</guid>
      <pubDate>Sat, 04 May 2024 15:47:26 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：无法同步创建数据集（名称已存在）</title>
      <link>https://stackoverflow.com/questions/78429387/valueerror-unable-to-synchronously-create-dataset-name-already-exists</link>
      <description><![CDATA[当我尝试将模型另存为 h5 时
caption_model.save(“/kaggle/working/mymodel.h5”)

我发现了这个错误
ValueError Traceback（最近一次调用最后一次）
[19] 中的单元格，第 1 行
----&gt; 1 title_model.save(“/kaggle/working/mymodel.h5”)

文件 /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122，位于filter_traceback..error_handler(*args, **kwargs)
    第 119 章
    120 # 要获取完整的堆栈跟踪，请调用：
    121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
    123 最后：
    124 删除filtered_tb

文件 /opt/conda/lib/python3.10/site-packages/h5py/_hl/group.py:183，在 Group.create_dataset(self, name, shape, dtype, data, **kwds)
    第180章 180
    [第 181 回]
--&gt;第183章
    184 dset = 数据集. 数据集（dsid）
    185 返回数据集

文件 /opt/conda/lib/python3.10/site-packages/h5py/_hl/dataset.py:163，在 make_new_dset(parent、shape、dtype、数据、名称、块、压缩、shuffle、fletcher32、maxshape、compression_opts 中、 fillvalue、scaleoffset、track_times、external、track_order、dcpl、dapl、efile_prefix、virtual_prefix、allow_unknown_filter、rdcc_nslots、rdcc_nbytes、rdcc_w0)
    160 其他：
    161 sid = h5s.create_simple（形状，maxshape）
--&gt;第163章
    165 if (data is not None) and (not isinstance(data, Empty)):
    166 dset_id.write（h5s.ALL，h5s.ALL，数据）

文件 h5py/_objects.pyx:54，在 h5py._objects.with_phil.wrapper() 中

文件 h5py/_objects.pyx:55，在 h5py._objects.with_phil.wrapper() 中

文件h5py/h5d.pyx:137，在h5py.h5d.create()中

ValueError：无法同步创建数据集（名称已存在）

你们以前遇到过这个问题或者知道如何解决吗？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78429387/valueerror-unable-to-synchronously-create-dataset-name-already-exists</guid>
      <pubDate>Sat, 04 May 2024 14:51:33 GMT</pubDate>
    </item>
    <item>
      <title>从tensorflow.compat.v1导入估计器作为tf_estimator导入错误：无法从'tensorflow.compat.v1导入名称'估计器'</title>
      <link>https://stackoverflow.com/questions/78428547/from-tensorflow-compat-v1-import-estimator-as-tf-estimator-importerror-cannot-i</link>
      <description><![CDATA[我收到此错误
ImportError：无法从“tensorflow.compat.v1”（/root/.local/lib/python3.10/site-packages/tensorflow/_api/v2/compat/v1/__init__.py）导入名称“estimator”

当我尝试运行此代码时
python Tensorflow\models\research\object_detection\model_main_tf2.py --model_dir=Tensorflow\workspace\models\my_ssd_mobnet --pipeline_config_path=Tensorflow\workspace\models\my_ssd_mobnet\pipeline.config --num_train_steps=10000
]]></description>
      <guid>https://stackoverflow.com/questions/78428547/from-tensorflow-compat-v1-import-estimator-as-tf-estimator-importerror-cannot-i</guid>
      <pubDate>Sat, 04 May 2024 09:58:47 GMT</pubDate>
    </item>
    <item>
      <title>COCO分段json格式的合并和减去注释</title>
      <link>https://stackoverflow.com/questions/78427741/merge-and-subtract-annotations-in-coco-segmentation-json-format</link>
      <description><![CDATA[我是 Python 和机器学习新手，遇到以下问题：我以 COCO .json 格式注释了数据。在这种情况下，水下照片上的珊瑚表面区域是活的，而珊瑚的部分区域是死的。有时，蒙版会重叠。
我想减去我注释为“死区”的区域来自我注释为“活着”的区域。在每张照片上，只有一个珊瑚被注释，有时我会分多个部分进行注释，因此我也想在减法之前合并每个类别的蒙版。注释类“死”了。仅出现在图像的子集中。
有人能指出我如何做到这一点的正确方向吗？
我添加了一张示例照片：黄色表示“活着”类别，绿色表示“死亡”类别。

非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/78427741/merge-and-subtract-annotations-in-coco-segmentation-json-format</guid>
      <pubDate>Sat, 04 May 2024 04:11:41 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 GAN 模型训练在 90 次迭代时停止？</title>
      <link>https://stackoverflow.com/questions/78425198/why-does-my-gan-model-training-stop-at-90-iterations</link>
      <description><![CDATA[我正在训练一个cycleGAN模型，每个域40张图片。 epoch 应该为 50，batch_size=1，总迭代次数为 2000，batch_per_epoch 为 40，但是模型在迭代 90 次后停止训练，不存在甚至将 90 视为 true 且模型停止的条件.
def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, 数据集, epochs=1):
    # 定义训练运行的属性
    n_epochs, n_batch, = epochs, 1 #batch size固定为1，如论文中建议的
    # 确定鉴别器的输出正方形形状
    n_patch = d_model_A.output_shape[1]
    # 解压数据集
    trainA, trainB = 数据集
    # print(trainA.shape,trainB.shape)
    # 为假图像准备图像池
    池A，池B = 列表（），列表（）
    # 计算每个训练时期的批次数
    bat_per_epo = int(len(trainA) / n_batch)
    # 计算训练迭代次数
    n_steps = bat_per_epo * n_epochs
    
    打印（n_steps，bat_per_epo）
    # 手动枚举纪元
    对于范围内的 i（n_steps）：
        # 从每个域（A和B）中选择一批真实样本
        X_realA, y_realA =generate_real_samples(trainA, n_batch, n_patch)
        X_realB, y_realB =generate_real_samples(trainB, n_batch, n_patch)
        # 使用 B to A 和 A to B 生成器生成一批假样本。
        X_fakeA, y_fakeA =generate_fake_samples(g_model_BtoA, X_realB, n_patch)
        X_fakeB, y_fakeB =generate_fake_samples(g_model_AtoB, X_realA, n_patch)
        # 更新池中的假图像。请记住，论文建议使用 50 个图像的缓冲区
        X_fakeA = update_image_pool(poolA, X_fakeA)
        X_fakeB = update_image_pool(poolB, X_fakeB)
        # 通过复合模型更新生成器 B-&gt;A
        # print(类型(X_realA),类型(X_realB),类型(X_fakeA),类型(X_fakeB),类型(y_realA),类型(y_realB),类型(y_fakeA),类型(y_fakeB))
        
        g_loss2 = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])
        
        # 更新 A 的鉴别器 -&gt; [真/假]
        dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)
        dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)
        
        # 通过复合模型更新生成器 A-&gt;B
        g_loss1 = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])
        # 更新 B 的鉴别器 -&gt; [真/假]
        dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)
        dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)
        
        # 总结性能
        #由于我们的批量大小=1，迭代次数将与数据集的大小相同。
        #在一个纪元中，迭代次数等于图像数量。
        #如果你有 100 张图像，那么 1 epoch 就是 100 次迭代
        print(&#39;迭代&gt;%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]&#39; % (i+1, dA_loss1,dA_loss2, dB_loss1 ,dB_loss2, g_loss1,g_loss2))
        # 定期评估模型性能
        #如果批量大小（总图像）=100，则每 75 次迭代后将总结性能。
        如果 (i+1) % (bat_per_epo * 1) == 0:
            # 绘制 A-&gt;B 翻译
            总结性能（i，g_model_AtoB，trainA，&#39;AtoB&#39;）
            # 情节 B-&gt;A 翻译
            总结性能（i，g_model_BtoA，trainB，&#39;BtoA&#39;）
        如果 (i+1) % (bat_per_epo * 5) == 0:
            # 保存模型
            # #如果批量大小（总图像）=100，模型将在之后保存
            #每 75 次迭代 x 5 = 375 次迭代。
            save_models(i, g_model_AtoB, g_model_BtoA)

这是训练函数，历元传递为 50，最后一个条件：
如果 (i+1) % (bat_per_epo * 5) == 0:
    # 保存模型
    # #如果批量大小（总图像）=100，模型将在之后保存
    #每 75 次迭代 x 5 = 375 次迭代。
    save_models(i, g_model_AtoB, g_model_BtoA)

没有被执行。
随着生成器损失的减少，模型似乎正在训练和改进，生成的图像并没有那么糟糕，因为它只训练了 40 次迭代和 80 次迭代，在第 40 次和 80 次之后生成了图片并保存了模型迭代，然后停止。
这是我得到的结果：
40 次迭代后：

80次迭代后：
]]></description>
      <guid>https://stackoverflow.com/questions/78425198/why-does-my-gan-model-training-stop-at-90-iterations</guid>
      <pubDate>Fri, 03 May 2024 14:13:14 GMT</pubDate>
    </item>
    <item>
      <title>如何绘制以下代码片段的ROC曲线？</title>
      <link>https://stackoverflow.com/questions/64962372/how-to-draw-roc-curve-for-the-following-code-snippet</link>
      <description><![CDATA[我是深度学习新手。我正在尝试为以下代码生成 ROC 曲线。我正在使用喀拉拉。
类大小为 10，图像为大小为 1001003 的 RGB 图像。
&lt;前&gt;&lt;代码&gt;target_size=(100,100,3)

train_generator = train_datagen.flow_from_directory(&#39;路径&#39;,
    目标大小=目标大小[:-1],
    批量大小=16，
    class_mode=&#39;分类&#39;,
    子集=&#39;训练&#39;,
    种子=随机种子）

有效生成器 = ...

测试生成器 = ...
n_classes = len(set(train_generator.classes))

打印（n_类）

input_layer = keras.layers.Input(shape=target_size)

conv2d_1 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=1, padding=&#39;same&#39;,
激活=&#39;relu&#39;,
                           kernel_initializer=&#39;he_normal&#39;)(input_layer)

batchnorm_1 = keras.layers.BatchNormalization()(conv2d_1)
maxpool1=keras.layers.MaxPool2D(pool_size=(2,2))(batchnorm_1)


conv2d_2 = keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=1, padding=&#39;same&#39;,
激活=&#39;relu&#39;,
                           kernel_initializer=&#39;he_normal&#39;)(maxpool1)
batchnorm_2 = keras.layers.BatchNormalization()(conv2d_2)

maxpool2=keras.layers.MaxPool2D(pool_size=(2,2))(batchnorm_2)


展平 = keras.layers.Flatten()(maxpool2)
稠密_1 = keras.layers.Dense(256, 激活=&#39;relu&#39;)(展平)

稠密_2 = keras.layers.Dense(n_classes, 激活=&#39;softmax&#39;)(dense_1)



模型= keras.models.Model（input_layer，dense_3）

model.compile(优化器=keras.optimizers.Adam(0.001),
          损失=&#39;分类交叉熵&#39;，
          指标=[&#39;acc&#39;])
模型.summary()

model.fit_generator（生成器=train_generator，validation_data=valid_generator，
                纪元=200）
                
分数 = model.evaluate_generator(test_generator)

打印（分数）

我想看到曲线并生成 ROC 曲线。请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/64962372/how-to-draw-roc-curve-for-the-following-code-snippet</guid>
      <pubDate>Mon, 23 Nov 2020 03:58:45 GMT</pubDate>
    </item>
    </channel>
</rss>