<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 04 Nov 2024 06:26:20 GMT</lastBuildDate>
    <item>
      <title>训练LLM时出现问题，3D attn_mask的形状错误</title>
      <link>https://stackoverflow.com/questions/79154375/problem-when-training-llm-shape-of-3d-attn-mask-is-wrong</link>
      <description><![CDATA[我目前正在尝试使用 PyTorch 库训练 LLM，但我遇到了一个无法解决的问题。我不知道如何修复此错误。也许有人可以帮助我。在帖子中，我将包含错误的屏幕截图。
这是我的训练单元的代码：
for epoch in range(1): 
for batch in train_loader:

input_ids = batch[&#39;input_ids&#39;]
tention_mask = batch[&#39;attention_mask&#39;]

target = input_ids[:, 1:]
input_ids = input_ids[:, :-1]
input_attention_mask =tention_mask[:, :-1].to(torch.float).transpose(0, 1)
target_attention_mask =tention_mask[:, 1:].to(torch.float).transpose(0, 1)

input_ids, target, target_attention_mask, input_attention_mask = input_ids.to(device), target.to(device), target_attention_mask.to(device), input_attention_mask.to(device)

logits = model(input_ids, target, input_attention_mask, target_attention_mask)

loss = criterion(logits.view(-1, vocal_size), target.view(-1))

optimizer.zero_grad()
loss.backward()
optimizer.step()

if epoch % 25 == 0:
loss =estimate_loss()
print(f&quot;Epoch {epoch}, Loss Train: {losses[&#39;train_loader&#39;]:.3f}, Loss Validation: {losses[&#39;validation_loader&#39;]:.3f}&quot;)
if epoch % 250 == 0:
torch.save(model.state_dict(), &quot;model.pth&quot;)
#再次加载model.load_state_dict(torch.load(&quot;model.pth&quot;))

这是我的前向函数：
import torch.nn as nn
import torch

class TransformerLanguageModel(nn.Module):
def __init__(self, vocab_size, embedding_dim=512, num_heads=8, num_layers=8, hidden_​​dim=2048):
super().__init__()

self.token_embedding = nn.Embedding(vocab_size, embedding_dim)
self.position_embedding = nn.Embedding(1023, embedding_dim)

self.encoder_layers = nn.ModuleList([
nn.TransformerEncoderLayer(
d_model=embedding_dim,
nhead=num_heads, 
dim_feedforward=hidden_​​dim
) for _ in range(num_layers)
])

self.decoder_layers = nn.ModuleList([
nn.TransformerDecoderLayer(
d_model=embedding_dim, 
nhead=num_heads, 
dim_feedforward=hidden_​​dim
) for _ in range(num_layers)
])

self.fc_out = nn.Linear(embedding_dim, vocab_size)

def forward(self, input_ids, target, input_attention_mask=None, target_attention_mask=None):
input_token_embeddings = self.token_embedding(input_ids)
input_positions = torch.arange(0, input_ids.size(1), device=input_ids.device).unsqueeze(0)
input_position_embeddings = self.position_embedding(input_positions)
coder_embeddings = input_token_embeddings + input_position_embeddings

for layer in self.encoder_layers:
coder_embeddings = layer(encoder_embeddings, src_key_padding_mask=input_attention_mask)

target_token_embeddings = self.token_embedding(targets)
target_positions = torch.arange(0, target.size(1), device=targets.device).unsqueeze(0)
target_position_embeddings = self.position_embedding(target_positions)
coder_embeddings = target_token_embeddings + target_position_embeddings

seq_len = target.size(1)
causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=targets.device)).unsqueeze(0)

for layer in self.decoder_layers:
coder_embeddings = layer(decoder_embeddings,coder_embeddings, tgt_key_padding_mask=target_attention_mask,memory_key_padding_mask=input_attention_mask, tgt_mask=causal_mask)

logits = self.fc_out(decoder_embeddings)
return logits

这是错误
3D attn_mask 的形状是torch.Size([1, 1023, 1023])，但应该是 (8184, 32, 32)。)

我尝试重塑注意力掩码以适应尺寸，我也尝试运行没有注意力掩码的训练代码，因为我在前向函数中定义，它也应该在没有注意力掩码的情况下工作。我对这两个选项都没有运气。重塑没有起作用，因为我得到了不同的错误，我无法将其重塑为正确的张量尺寸。此外，当只是尝试在没有参数 input_attention_mask 和 target_attention_mask 的情况下拟合模型时，发生了与屏幕截图中相同的错误。我有点不知道还能尝试什么。我也有点困惑，因为 attn_mask 从未被定义过，只有注意力掩码、input_attention_mask 和 target_attention_mask。]]></description>
      <guid>https://stackoverflow.com/questions/79154375/problem-when-training-llm-shape-of-3d-attn-mask-is-wrong</guid>
      <pubDate>Mon, 04 Nov 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>选择处理缺失值的最佳技术[关闭]</title>
      <link>https://stackoverflow.com/questions/79153897/choosing-the-best-techniques-for-handling-missing-values</link>
      <description><![CDATA[有很多处理缺失值的技术。例如，平均值/中位数/众数插补、随机样本插补、分布末端插补等。现在我很困惑：我应该何时使用哪种技术？
也许随机样本插补可能是最好的，但并非在所有方面都是如此。]]></description>
      <guid>https://stackoverflow.com/questions/79153897/choosing-the-best-techniques-for-handling-missing-values</guid>
      <pubDate>Mon, 04 Nov 2024 00:09:55 GMT</pubDate>
    </item>
    <item>
      <title>如何评估SRGAN模型</title>
      <link>https://stackoverflow.com/questions/79153606/how-to-evaluate-the-srgan-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79153606/how-to-evaluate-the-srgan-model</guid>
      <pubDate>Sun, 03 Nov 2024 20:34:58 GMT</pubDate>
    </item>
    <item>
      <title>coremltools 错误：ValueError：perm 的长度应与 rank(x) 相同：3 != 2</title>
      <link>https://stackoverflow.com/questions/79153512/coremltools-error-valueerror-perm-should-have-the-same-length-as-rankx-3</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79153512/coremltools-error-valueerror-perm-should-have-the-same-length-as-rankx-3</guid>
      <pubDate>Sun, 03 Nov 2024 19:49:49 GMT</pubDate>
    </item>
    <item>
      <title>是否有可能制作一个二元期权交易机器人？</title>
      <link>https://stackoverflow.com/questions/79153210/is-it-possible-to-make-a-trading-bot-for-binary-options</link>
      <description><![CDATA[我想自己创建一个人工智能机器人来帮助我的交易，或者至少确保信号。
我一直在学习如何使用 LSTM 并通过微调对其进行训练，但我的问题是：有没有更好的模型可以开始，如果有，如果训练正确，每个模型的准确率是多少？
以下是我从 chatGPT 获得的一些信息，我不确定是否应该使用它。
名称胜率
长短期记忆 (LSTM)：60–70
卷积神经网络 (CNN)：55–65
支持向量机 (SVM)：50–60

我想决定选择一个模型或混合模型，这样我就可以开始学习了。]]></description>
      <guid>https://stackoverflow.com/questions/79153210/is-it-possible-to-make-a-trading-bot-for-binary-options</guid>
      <pubDate>Sun, 03 Nov 2024 16:53:43 GMT</pubDate>
    </item>
    <item>
      <title>表现出训练不稳定性最小的神经网络[关闭]</title>
      <link>https://stackoverflow.com/questions/79152950/smallest-neural-network-exhibiting-training-instability</link>
      <description><![CDATA[我遇到过训练发散（例如，损失函数的发散）导致我使用过的一些深度神经网络训练不稳定的情况。这似乎是深度网络的一个常见特征，即使不是普遍特征，也有很多实用的方法来处理它。
我的问题是：哪一个最小的神经网络已被证明表现出训练发散导致不稳定？我想尝试在最简单的测试用例中了解根本原因。
我浏览了已发表的文献，发现了很多关于这种现象的论文（例如，https://www.sciencedirect.com/science/article/abs/pii/S1568494624001091 和 https://arxiv.org/abs/2110.04369 等），但我不清楚表现出这种现象的最小可能网络是什么。]]></description>
      <guid>https://stackoverflow.com/questions/79152950/smallest-neural-network-exhibiting-training-instability</guid>
      <pubDate>Sun, 03 Nov 2024 14:44:32 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用函数 shap.Explainer 会根据输入的不同顺序获得不同的 shap 值？</title>
      <link>https://stackoverflow.com/questions/79152799/why-i-get-different-shap-values-according-to-the-different-order-of-inputs-by-us</link>
      <description><![CDATA[我训练了一个二分类模型，并想使用 shap.Explainer 来分析特征贡献。
代码如下：
def f(x):
return model.predict_proba(x)[:, 1]

X100 = shap.utils.sample(X_train, 100)

explainer = shap.Explainer(f, X100, seed=2023)
shap_values = explainer(data.iloc[[0,1,2,3], :])

shap_values.values 的结果如下：




Feature 1
...




sample 0
-0.009703
...


样本 1
-0.009297
...


样本 2
-0.007699
...


样本 3
0.032624
...



但是当输入顺序改变时：
def f(x):
return model.predict_proba(x)[:, 1]

X100 = shap.utils.sample(X_train, 100)

explainer = shap.Explainer(f, X100, seed=2023)
shap_values = explainer(data.iloc[[1,0,2,3], :])

样本 0 和样本 1 的结果已更改：




特征 1
...




样本 1
-0.010012
...


样本0
-0.008277
...


样本 2
-0.007699
...


样本 3
0.032624
...



我不知道有什么区别。]]></description>
      <guid>https://stackoverflow.com/questions/79152799/why-i-get-different-shap-values-according-to-the-different-order-of-inputs-by-us</guid>
      <pubDate>Sun, 03 Nov 2024 13:22:12 GMT</pubDate>
    </item>
    <item>
      <title>使用 ml 的人脸识别项目[关闭]</title>
      <link>https://stackoverflow.com/questions/79152234/face-recognition-project-using-ml</link>
      <description><![CDATA[做什么 代码显示错误
此代码使用 OpenCV 和 LBPH 算法实现了人脸识别系统。它由三个主要组件组成。
数据收集（生成数据集）：
初始化 harr Cascade 分类器以检测人脸。
定义辅助函数 face cropped，将帧转换为灰度，检测人脸并裁剪它们。
从网络摄像头捕获视频并连续读取帧。
如果检测到人脸，它会调整大小并将裁剪的人脸转换为灰度，使用唯一 ID 保存它，然后显示图像。
在 200 张图像后或按下 Enter 键时，该过程停止。
训练分类器（训练分类器）：
从指定目录读取图像并准备进行训练。
将图像转换为灰度并提取其 ID。
初始化 LBPH 人脸识别器，使用人脸数据对其进行训练，并将模型保存为 classifier.xml。
实时人脸识别：
从网络摄像头捕获视频帧并使用 haar Cascade 检测人脸。
调用绘制边界函数在检测到的人脸周围绘制矩形，并使用训练有素的分类器预测其身份。
根据置信度显示识别的名称或“未知”，实现实时人脸检测和识别。]]></description>
      <guid>https://stackoverflow.com/questions/79152234/face-recognition-project-using-ml</guid>
      <pubDate>Sun, 03 Nov 2024 07:21:47 GMT</pubDate>
    </item>
    <item>
      <title>LCD 7 段数字无法正确识别</title>
      <link>https://stackoverflow.com/questions/79151393/lcd-7-segment-digits-not-recognized-correctly</link>
      <description><![CDATA[我选择读取我的热系统 LCD 7 段显示屏作为学习 CNN 的首要任务。
我能够正确读取大多数数字，但数字 6 大多数情况下被检测为 5。
有人可以建议我使用 MNIST 数据集执行该任务的方法是否正确，我应该找到更好的超参数以使其按预期工作吗？
这是我的代码，包含更多上下文：https://github.com/tkdcpl/cnn-lcd-digits]]></description>
      <guid>https://stackoverflow.com/questions/79151393/lcd-7-segment-digits-not-recognized-correctly</guid>
      <pubDate>Sat, 02 Nov 2024 18:48:18 GMT</pubDate>
    </item>
    <item>
      <title>ImageDataGenerator 在预处理函数中发送图像数组，而不是文件路径</title>
      <link>https://stackoverflow.com/questions/79151089/imagedatagenerator-sending-array-of-image-instead-of-file-path-in-preprocessing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79151089/imagedatagenerator-sending-array-of-image-instead-of-file-path-in-preprocessing</guid>
      <pubDate>Sat, 02 Nov 2024 16:29:00 GMT</pubDate>
    </item>
    <item>
      <title>需要从图像中分别分割出每个数字</title>
      <link>https://stackoverflow.com/questions/79147122/need-to-segment-each-number-from-the-image-separately</link>
      <description><![CDATA[我使用 MNIST 数据集创建了一个 CNN 模型。我想对图像中存在的数字序列进行预测。该技术涉及分割每张图像并将其输入到模型中，但我在从图像中分割数字时遇到了困难，因为存在两种不同类型的图像。我需要一种强大的技术来消除图像中存在的所有噪音和阴影并分别分割每个数字。
我也在这里分享这些图片。
我正在寻找强大的技术和代码。


我尝试了此代码和技术，但它对附加的图像不起作用
def fragment_and_display_digits(image_path):# Read imageimg = cv2.imread(image_path)gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
# 获取图像尺寸
height, width = gray.shape
total_area = height * width

# 应用自适应阈值
thresh = cv2.adaptiveThreshold(
gray,
255,
cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
cv2.THRESH_BINARY_INV,
21, # 块大小
10 # C 常数
)

# 查找轮廓
contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 根据面积过滤轮廓
valid_contours = []
min_area = total_area * 0.001 # 图像面积的最小 0.1%
max_area = total_area * 0.5 # 图像面积的最大 50%

for cnt in contours:
area = cv2.contourArea(cnt)
if min_area &lt; area &lt; max_area:
x, y, w, h = cv2.boundingRect(cnt)
aspects_ratio = w / float(h)
# 检查数字的纵横比是否合理（不要太宽或太高）
if 0.2 &lt; aspects_ratio &lt; 2：
valid_contours.append(cnt)

# 从左到右对轮廓进行排序
valid_contours = sorted(valid_contours, key=lambda x: cv2.boundingRect(x)[0])

# 提取并显示数字
digits = []
padding = int(min(height, width) * 0.02) # 根据图像大小进行自适应填充

for cnt in valid_contours:
x, y, w, h = cv2.boundingRect(cnt)
# 添加填充，同时保持在图像范围内
x1 = max(0, x - padding)
y1 = max(0, y - padding)
x2 = min(width, x + w + padding)
y2 = min(height, y + h + padding)
digit = img[y1:y2, x1:x2]
digits.append(digit)

# 显示结果
if digits:
# 创建带有检测到的数字的原始图像的可视化
img_with_boxes = img.copy()
for cnt in valid_contours:
x, y, w, h = cv2.boundingRect(cnt)
cv2.rectangle(img_with_boxes, (x, y), (x+w, y+h), (0, 255, 0), 2)

# 绘制带有方框和分割数字的原始图像
plt.figure(figsize=(15, 5))

# 带有方框的原始图像
plt.subplot(2, 1, 1)
plt.imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))
plt.title(&#39;Detected Digits&#39;)
plt.axis(&#39;off&#39;)

#分割的数字
plt.subplot(2, 1, 2)
for i, digit in enumerate(digits):
plt.subplot(2, len(digits), len(digits) + i + 1)
plt.imshow(cv2.cvtColor(digit, cv2.COLOR_BGR2RGB))
plt.axis(&#39;off&#39;)
plt.title(f&#39;Digit {i+1}&#39;)

plt.tight_layout()
plt.show()
else:
print(&quot;图像中未找到数字&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79147122/need-to-segment-each-number-from-the-image-separately</guid>
      <pubDate>Fri, 01 Nov 2024 06:27:46 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 Candle 从 NV-Embed 获取嵌入？</title>
      <link>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</link>
      <description><![CDATA[我想要做的是一个输出任意输入的嵌入的 CLI 程序。
为此，我想使用嵌入模型进行推理，我选择了 NV-Embed-v2。我选择的框架是 Candle，但我也查看了 Mistral-RS。
基本上，我想做的是这个代码片段：
https://huggingface.co/nvidia/NV-Embed-v2
但使用 Rust 和 Candle。
我尝试从 Mistral Candle 的示例，因为 NV-Embed 的 HF 页面显示：模型详细信息/仅基础解码器 LLM：Mistral-7B-v0.1。
我将原始代码中的模型 ID 替换为 nvidia/NV-Embed-v2，并能够从 Hugging Face 下载权重，但在加载配置时，我得到了这个：
错误：缺少第 101 行第 1 列的字段“vocab_size”

然后我将从 HF 加载的 JSON 配置中的值硬编码到新创建的 candle_transformers::models::mistral::Config 实例中。之后，Mistral::new(&amp;config, vb) 失败，并显示：
错误：找不到张量 model.embed_tokens.weight

有没有办法解决这个问题 — 也许还有其他一些基于 Candle 的开源作品可以作为我的灵感？或者，也许这是一个很容易诊断的常见错误？]]></description>
      <guid>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</guid>
      <pubDate>Thu, 31 Oct 2024 15:55:49 GMT</pubDate>
    </item>
    <item>
      <title>DQN 性能波动</title>
      <link>https://stackoverflow.com/questions/79141566/dqn-performance-swinging</link>
      <description><![CDATA[我使用 DDQN 和经验重放，就像本教程中一样 https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
除了我通过模糊 x_dot 和 theta_t（推车速度和杆的角速度）使问题变得更难一些。然后，我根据当前状态计算前一个 x_dot、theta_dot、x_dot_dot 和 theta_dot_dot，并使用此状态空间进行学习过程：(x, prev_x_dot, prev_prev_x_dot_dot, theta, prev_theta_dot, prev_prev_theta_dot_dot)。
无论如何，我的主要问题是，通过使用上面链接的教程中描述的 DQN 算法，算法不会收敛。如果最后 100 集的平均长度 &gt; 450，我认为学习成功。执行时，我可能会看到 50-60 个连续的 500 集长集，但随后集长随机波动并下降到甚至 20!?!? 我需要从某个范围内（对于每个 x、theta）的任意起始位置开始，使问题更加棘手，但到目前为止，结果并不乐观。
对于像 DQN 这样的算法来说，这是一种正常行为吗？我知道，作为基于先前执行计算的策略，损失函数可能存在一些收敛问题，但这是否包括性能的严重波动？
我使用的网络有 3 个非线性层，线性层尺寸为 256x256。]]></description>
      <guid>https://stackoverflow.com/questions/79141566/dqn-performance-swinging</guid>
      <pubDate>Wed, 30 Oct 2024 14:36:28 GMT</pubDate>
    </item>
    <item>
      <title>如何在 ML.NET 中使用 CenterFace？模型预期形状为 (10, 3, 32, 32)</title>
      <link>https://stackoverflow.com/questions/79122749/how-to-use-centerface-in-ml-net-model-expects-shape-10-3-32-32</link>
      <description><![CDATA[我尝试在 ML.NET 中使用 CenterFace ONNX，但一直出现各种错误，主要是关于输入大小的错误。
CenterFace 元数据指出，它应该有一个 10, 3, 32, 32 的输入，这对于图像检测来说已经毫无意义了 - 为算法提供 10 个批次（每个批次 32x32 像素）有什么意义？
这是我的主要代码：
 string modelPath = &quot;centerface.onnx&quot;;
var mlContext = new MLContext();

string imagePath = &quot;photo1.jpg&quot;;

var img = Image.FromFile(imagePath);
var DH = (int)(Math.Ceiling((float)img.Height / 32) * 32);
var DW = (int)(Math.Ceiling((float)img.Width / 32) * 32);

var inputData = new[] { new ModelInput { ImagePath = imagePath } };
IDataView imageData = mlContext.Data.LoadFromEnumerable(inputData);

var pipeline = mlContext.Transforms.LoadImages(outputColumnName: &quot;input.1&quot;, imageFolder: &quot;&quot;, inputColumnName: nameof(ModelInput.ImagePath))
.Append(mlContext.Transforms.ResizeImages(outputColumnName: &quot;input.1&quot;, imageWidth: DW, imageHeight: DH))
.Append(mlContext.Transforms.ExtractPixels(outputColumnName: &quot;input.1&quot;))
.Append(mlContext.Transforms.ApplyOnnxModel(
outputColumnNames: [&quot;537&quot;, &quot;538&quot;, &quot;539&quot;, &quot;540&quot;],
inputColumnNames: [&quot;input.1&quot;],
modelFile: modelPath
));

var model = pipeline.Fit(imageData);
var predictionEngine = mlContext.Model.CreatePredictionEngine&lt;ModelInput, ModelOutput&gt;(mo​​del);
var prediction = predictionEngine.Predict(new ModelInput { ImagePath = imagePath });

使用我的 2 个模型类：
 public class ModelInput
{
public string ImagePath { get; set; }
}

public class ModelOutput
{
[ColumnName(&quot;537&quot;)] 
public float[] HeatMap { get; set; }

[ColumnName(&quot;538&quot;)]
public float[] Scale { get; set; }

[ColumnName(&quot;539&quot;)]
public float[] Offset { get; set; }

[ColumnName(&quot;540&quot;)]
public float[] Landmarks { get; set; }
}

但我确实一直收到有关输入大小的错误：

System.ArgumentException：“内存长度（3686400）必须与尺寸乘积（30720）匹配。”

30720 显然是 10x3x32x32。但同样，这有什么意义呢？
我认为我的 ONNX 坏了，但我确实有一个使用 OpenCVSharp 的工作实现：
// 这是计算机 DW 和 DH，与 ML.NET 示例中的方式相同
CenterFaceParams p = new(image, resizedSize.Width, resizedSize.Height, scoreThreshold, nmsThreshold);
Size size = new(p.DW, p.DH);

使用 Mat input = new();
Cv2.Resize(image, input, size);

使用 Mat blobInput = CvDnn.BlobFromImage(input, 1.0, size, new Scalar(0, 0, 0), true, false);
_net.SetInput(blobInput, &quot;input.1&quot;);

使用 (Mat heatMap = new())
使用 (Mat scale = new())
使用 (Mat offset = new())
使用 (Mat skylines = new())
{
_net.Forward([heatMap, scale, offset, skylines], [&quot;537&quot;, &quot;538&quot;, &quot;539&quot;, &quot;540&quot;]);

CenterFaceDecodercoder = new(heatMap, scale, offset, skylines, p);
returncoder.GetOutput();
}

这个实现给了我所有 4 个层，建模后我得到了我想要的值。]]></description>
      <guid>https://stackoverflow.com/questions/79122749/how-to-use-centerface-in-ml-net-model-expects-shape-10-3-32-32</guid>
      <pubDate>Thu, 24 Oct 2024 15:52:45 GMT</pubDate>
    </item>
    <item>
      <title>为机器学习模型创建标记图像数据集</title>
      <link>https://stackoverflow.com/questions/52848947/create-labeled-image-dataset-for-machine-learning-models</link>
      <description><![CDATA[我的问题是如何为机器学习创建带标签的图像数据集？
我一直使用已有的数据集，因此我面临着如何标记图像数据集的困难（就像我们在猫与狗分类中所做的那样）。
我必须进行标记以及图像分割，在网上搜索后，我找到了一些手动标记工具，例如LabelMe和LabelBox。LabelMe 很好，但它以 XML 文件的形式返回输出。
现在我再次担心如何将 XML 文件输入神经网络？我根本不擅长图像处理任务，所以我需要一个替代建议。
编辑：我有学位证书和普通文件的扫描件，我必须制作一个分类器，将学位证书分类为 1，将非学位证书分类为 0。所以我的标签将是这样的：
Degree_certificate -&gt; y(1)
Non_degree_cert -&gt; y(0)]]></description>
      <guid>https://stackoverflow.com/questions/52848947/create-labeled-image-dataset-for-machine-learning-models</guid>
      <pubDate>Wed, 17 Oct 2018 06:56:31 GMT</pubDate>
    </item>
    </channel>
</rss>