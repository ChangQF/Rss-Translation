<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 29 May 2024 01:06:11 GMT</lastBuildDate>
    <item>
      <title>为什么加载 AutoTokenizer 会占用这么多的 RAM？</title>
      <link>https://stackoverflow.com/questions/78546693/why-loading-autotokenizer-takes-so-much-ram</link>
      <description><![CDATA[我测量了脚本使用的 RAM，惊讶地发现它占用了大约 300Mb 的 RAM，而 tokenizer 文件本身大约只有 9MB。这是为什么？
我试过：
from transformers import AutoTokenizer
from memory_profiler import profile

@profile
def load_tokenizer():
path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
tokenizer = AutoTokenizer.from_pretrained(path)

return tokenizer

load_tokenizer()

输出：
行 # 内存使用量 增量 发生次数 行内容
==================================================================
4 377.4 MiB 377.4 MiB 1 @profile
5 def load_tokenizer():
6 377.4 MiB 0.0 MiB 1 path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
7 676.6 MiB 299.2 MiB 1 tokenizer = AutoTokenizer.from_pretrained(path)
8 
9 
10 676.6 MiB 0.0 MiB 1 返回 tokenizer
]]></description>
      <guid>https://stackoverflow.com/questions/78546693/why-loading-autotokenizer-takes-so-much-ram</guid>
      <pubDate>Tue, 28 May 2024 22:44:10 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么机器学习方法来找到最佳卷积？</title>
      <link>https://stackoverflow.com/questions/78546274/what-machine-learning-approach-should-i-use-to-find-optimal-convolutions</link>
      <description><![CDATA[因此，我想寻找最适合某个物理实验的熵编码波形的最佳卷积运算。您可以想象，搜索空间非常大，我需要一种机器学习方法来找到一个好的最小值。我有大量数据和一台超级计算机可供使用。
我曾考虑使用与 AlphaTensor 类似的方法（有没有开源替代方案？），但我对 ML 算法不太熟悉，因此任何建议都将不胜感激。
我搜索过非 ML 方法，但认为其中一种方法效果最好。]]></description>
      <guid>https://stackoverflow.com/questions/78546274/what-machine-learning-approach-should-i-use-to-find-optimal-convolutions</guid>
      <pubDate>Tue, 28 May 2024 20:16:15 GMT</pubDate>
    </item>
    <item>
      <title>使用并行神经网络实现动态权重分配，提升 CNN 性能</title>
      <link>https://stackoverflow.com/questions/78545298/improving-cnn-performance-with-a-parallel-neural-network-for-dynamic-weight-assi</link>
      <description><![CDATA[我试图通过为所有数据点分配 0-1 之间的权重来改进我的回归模型，即卷积神经网络，这反过来又告诉我特定数据点在进行预测时有多好。关键是有些数据点很嘈杂并且会做出更糟糕的预测，我希望这些数据点具有较低的权重，以便在最后和训练期间忽略它们。
我所做的：尝试实现一个并行神经网络 (nn)，输出 0 到 1 之间的权重，并将此权重分配给相关的数据点/图像。
两个网络都接收相同的数据点作为输入，nn 输出权重，而回归 cnn 输出值 Y_i。然后将结果连接起来并传递给两个网络的相互自定义损失函数，该函数将预测值和目标值之间的平方差与并行 nn 中的给定权重相乘：sum((Y(:,i)-T(:,i))^2)*W(i)。
问题是模型似乎向较小的权重 W(i) 收敛，因为这将最大限度地减少损失，我该如何解决这个问题？
我尝试添加与权重大小成比例的正则化项来惩罚较小的权重。它有所帮助，但结果仍然不如没有并行 nn 时那么好。
# functional api
inputs = Input(shape=(16,150,1) )

x2 = Conv2D(64, 3, 1,activation=&#39;relu&#39;,data_format=&quot;channels_last&quot;,name=&#39;Conv1&#39;)(inputs)

x2 = MaxPooling2D()(x2)
x2 = BatchNormalization()(x2)

x2 = Conv2D(74, 2,activation=&#39;relu&#39;)(x2)
x2 = MaxPooling2D()(x2)
x2 = BatchNormalization()(x2)

x2 = Conv2D(128, 2,activation=&#39;relu&#39;)(x2)
x2 = Flatten()(x2)

x2 = Dense(256,activation=&#39;relu&#39;, name=&quot;FC1&quot;)(x2)
x2 = Dense(256,activation=&#39;relu&#39;,name=&#39;FC2&#39;)(x2)
regression_output = Dense(1,name=&#39;Output&#39;)(x2)

# 权重 NN 
x1 = Flatten()(inputs) # 与上面的 CNN 相同的输入
x1 = Dense(64,activation=&#39;relu&#39;,kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)
x1 = Dense(64,activation=&#39;relu&#39;,kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)
x1 = Dense(64,activation=&#39;relu&#39;)(x1)

weight_output = Dense(1,activation=&#39;sigmoid&#39;,name=&#39;weight_output&#39;)(x1) # 权重作为输出
weight_output = tf.maximum(weight_output, 0.1) # 确保权重至少为 0.1
combined_output = Concatenate()([regression_output, weight_output])
model = Model(inputs=inputs, output=combined_output)

def custom_loss(y_true, y_pred):
regression = y_pred[:, 0]
weight = y_pred[:, 1]
# 与之前一样对权重进行 L2 惩罚
#weight_penalty = tf.reduce_mean(tf.square(weight))
#l1_penalty = tf.reduce_sum(tf.abs(weight)) # L1 惩罚
#lambda_l1 = 0.10 # L1 的正则化强度
return tf.reduce_mean(weight * tf.square(y_true - return))
model.compile(optimizer=&#39;adam&#39;, loss=custom_loss, metrics = [&#39;mae&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/78545298/improving-cnn-performance-with-a-parallel-neural-network-for-dynamic-weight-assi</guid>
      <pubDate>Tue, 28 May 2024 16:06:38 GMT</pubDate>
    </item>
    <item>
      <title>将 Spark 中的大数据导入到 Feast Feature Store</title>
      <link>https://stackoverflow.com/questions/78544969/ingesting-big-data-from-spark-into-feast-feature-store</link>
      <description><![CDATA[我目前正在为 MLOps 项目构建大数据管道，该管道用于批处理。
这是当前设置：

我将原始结构化数据存储在 Hive 中。
Spark 作业提取原始数据并对其进行处理。
我打算使用 feast 和 Apache Cassandra 作为离线存储，用于存储由我的 Spark 作业产生的计算和策划特征。

我想高效地将数据从 spark 作业传递到 feast 和 Cassandra，我不确定在将处理后的数据传递给 feast 以存储在离线存储中之前，是否需要中间数据持久性解决方案来保存处理后的数据，在我的情况下有必要吗？]]></description>
      <guid>https://stackoverflow.com/questions/78544969/ingesting-big-data-from-spark-into-feast-feature-store</guid>
      <pubDate>Tue, 28 May 2024 15:00:31 GMT</pubDate>
    </item>
    <item>
      <title>预测多元时间序列时 VARIMA 模型的模型漂移</title>
      <link>https://stackoverflow.com/questions/78544041/model-drift-for-varima-model-when-forecasting-multivariate-time-series</link>
      <description><![CDATA[我目前正在尝试在多变量时间序列数据上训练 VARIMA 模型，该数据是关于冷却系统的 5 种不同类型的传感器测量的。数据具有周期性，因此完全相同的模式每 25 个数据点左右就会重复出现一次。我有一个包含 5 个不同组件的数据集，其中每分钟有一个数据点。我使用了 2 周的数据来训练模型，然后让它合成数据。我使用的 VARIMA 模型是从 Darts 包导入的，我这样定义模型：model_VARIMA = VARIMA(p=12, d=0, q=0, trend=&quot;n&quot;)。该模型是在 2 周的训练数据上训练的。当预测未来 30 个点或更多时，预测显然开始显示模型漂移。所有组件都没有产生周期性的多变量时间序列数据，因为一条不再有价值的直线。我想知道是否有人对此有解释并有解决问题的方法。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78544041/model-drift-for-varima-model-when-forecasting-multivariate-time-series</guid>
      <pubDate>Tue, 28 May 2024 12:15:20 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中如何按行处理不同数量的解释变量？</title>
      <link>https://stackoverflow.com/questions/78543787/how-should-different-numbers-of-explanatory-variables-be-handled-by-row-in-machi</link>
      <description><![CDATA[我在制作预测模型时遇到问题，所以留下一个问题。
我正在尝试使用随机森林、xgboost 等机器学习方法创建一个预测模型。
此时，y 值是分化后的月度时间序列数据，x 值是分化后的每日时间序列数据。
作为参考，t（表示时间）的时间与美国股市的交易日同步。
我的模型由以下格式组成。
预测值 = y_(t+21) - y_(t)
解释值 = y(t) - y(t-1), y(t-1) - y(t-2) ... y(t-p) - y(t-p-1)
此时，p 是该月的最后一个交易日。
这里的问题是每个月都有交易日数不同
例如1980年1月有23个交易日，而1981年2月有20个交易日，而且有假期的月份可能更少。
这种情况下，在构建用于预测因变量的解释变量数据集时，可能会对逐行中的某些值生成NaN值。
这种情况下，应该如何普遍处理？或者有没有术语或论文提到这个问题？
y_(t+21) - y_(t)有两种情况。一种是微分月末值，一种是微分月平均值。因此，目前还没有触及。]]></description>
      <guid>https://stackoverflow.com/questions/78543787/how-should-different-numbers-of-explanatory-variables-be-handled-by-row-in-machi</guid>
      <pubDate>Tue, 28 May 2024 11:21:17 GMT</pubDate>
    </item>
    <item>
      <title>二元分类获取预测值大于 1 [重复]</title>
      <link>https://stackoverflow.com/questions/78543310/binary-classification-get-predict-value-greater-than-1</link>
      <description><![CDATA[

有人能帮我吗，为什么我的模型返回的预测值大于 1。即使我使用的是具有 1 个单位和 S 型激活函数的密集层。我创建了一个二元分类模型。
我正在使用 tensorflow 和 keras 调谐器进行超参数调整。]]></description>
      <guid>https://stackoverflow.com/questions/78543310/binary-classification-get-predict-value-greater-than-1</guid>
      <pubDate>Tue, 28 May 2024 09:53:40 GMT</pubDate>
    </item>
    <item>
      <title>提高人脸识别性能并扩大检测范围 [关闭]</title>
      <link>https://stackoverflow.com/questions/78543297/improving-face-recognition-performance-and-extending-detection-range</link>
      <description><![CDATA[我正在使用 Python、dlib 和 OpenCV 开发人脸识别系统，但是在检测到多张人脸时会遇到性能问题，并且我需要扩大检测范围以检测 3 米以外距离的人脸。以下是我当前设置和挑战的摘要：
当前设置：

使用 Logitech Brio 4K Ultra HD 网络摄像头捕获视频流。
使用 dlib 的正面人脸检测器进行人脸检测。
利用 Dlib ResNet 模型进行人脸识别。
使用 OpenCV 处理视频流。
将已知的人脸特征存储在 CSV 文件中以供比较。

挑战：

当帧中有多个人脸时，性能会显著下降，
导致丢帧和 FPS 降低。
需要从 3 米以上的距离检测人脸，同时
保持准确性。

是否有任何硬件加速技术或优化我应该考虑以更快的速度计算？]]></description>
      <guid>https://stackoverflow.com/questions/78543297/improving-face-recognition-performance-and-extending-detection-range</guid>
      <pubDate>Tue, 28 May 2024 09:51:28 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Kaggle 上以 .h5 格式保存深度学习模型</title>
      <link>https://stackoverflow.com/questions/78541201/unable-to-save-deep-learning-model-in-h5-format-on-kaggle</link>
      <description><![CDATA[我在尝试将深度学习模型以 .h5 格式保存在 Kaggle 上时遇到问题。尽管遵循了标准程序，但保存过程始终失败。在此处输入图片说明我已添加代码和面临的问题。
在此处输入图片说明
我将格式指定为 .keras，但模型无法保存。但是，代码在 Google Colab 上运行良好。不幸的是，Google Colab 的内存不足以有效运行我的代码。
任何解决此问题并确保在 Kaggle 平台上成功以 .h5 格式保存我的模型的见解或潜在解决方案都将对我非常有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78541201/unable-to-save-deep-learning-model-in-h5-format-on-kaggle</guid>
      <pubDate>Mon, 27 May 2024 21:49:55 GMT</pubDate>
    </item>
    <item>
      <title>在与我之前训练过的数据集不同的数据集上训练 yolov8 变得非常慢 [关闭]</title>
      <link>https://stackoverflow.com/questions/78539266/training-yolov8-on-a-different-data-set-than-i-had-previously-trained-it-on-beca</link>
      <description><![CDATA[我正在尝试在与我之前训练过的数据集不同的数据集上训练 yolov8。即使这是一个较小的数据集，即使是 1 个 epoch 也需要很长时间才能完成。还有其他人遇到过这个问题吗，我可能哪里出错了？
我正在尝试在与我之前训练过的数据集不同的数据集上训练 yolov8。即使这是一个较小的数据集，即使是 1 个 epoch 也需要很长时间才能完成。]]></description>
      <guid>https://stackoverflow.com/questions/78539266/training-yolov8-on-a-different-data-set-than-i-had-previously-trained-it-on-beca</guid>
      <pubDate>Mon, 27 May 2024 13:08:40 GMT</pubDate>
    </item>
    <item>
      <title>层“dense_4”的输入 0 与层不兼容：预期输入形状的轴 -1 具有值 1，但收到的输入形状为 (None, 6)</title>
      <link>https://stackoverflow.com/questions/78538382/input-0-of-layer-dense-4-is-incompatible-with-the-layer-expected-axis-1-of-i</link>
      <description><![CDATA[我正在尝试实现多元回归模型。
使用以下代码：
all_normalizer = keras.layers.Normalization(input_shape=(1, ), axis=-1)
all_normalizer.adapt(x_train_all)

nn_model = tf.keras.Sequential([
all_normalizer,
tf.keras.layers.Dense(32,activation=&#39;relu&#39;),
tf.keras.layers.Dense(32,activation=&#39;relu&#39;),
tf.keras.layers.Dense(1)
])

nn_model.compile(keras.optimizers.Adam(learning_rate=0.001), loss=&#39;mean_squared_error&#39;)

history = nn_model.fit(
x_train_Temp,y_train_Temp,
validation_data=(x_val_Temp, y_val_Temp),
verbose=0, epochs=100
)

我收到以下错误：
 文件“C:\~ai.py”，第 330 行，位于 &lt;module&gt;
history = nn_model.fit(
^^^^^^^^^^^^^^
文件 &quot;C:\~\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\utils\traceback_utils.py&quot;，第 122 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
文件 &quot;C:\~PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\input_spec.py&quot;，第 227 行，位于 assert_input_compatibility 中
引发 ValueError(
ValueError：调用 Sequential.call() 时遇到异常。

层的输入 0 &quot;dense_4&quot; 与层不兼容：预期输入形状的轴 -1 具有值 1，但收到的输入形状为 (None, 6)

Sequential.call() 收到的参数：
• 输入=tf.Tensor(shape=(None, 1), dtype=float32)
• 训练=True
• 掩码=None

我应该在代码中更改/实现什么来解决此错误？
PS：我是初学者，所以我可能不了解一些东西，所以请不要不喜欢。]]></description>
      <guid>https://stackoverflow.com/questions/78538382/input-0-of-layer-dense-4-is-incompatible-with-the-layer-expected-axis-1-of-i</guid>
      <pubDate>Mon, 27 May 2024 09:54:27 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 保存模型有效，但加载模型无效</title>
      <link>https://stackoverflow.com/questions/78535919/tensorflow-saving-model-works-but-loading-it-doesnt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78535919/tensorflow-saving-model-works-but-loading-it-doesnt</guid>
      <pubDate>Sun, 26 May 2024 16:58:03 GMT</pubDate>
    </item>
    <item>
      <title>基于 Python 的模型学习，使用 TF、Keras 和 NLTK 进行标记</title>
      <link>https://stackoverflow.com/questions/78531788/python-based-model-learning-through-intents-using-tf-keras-and-nltk-for-tokeniz</link>
      <description><![CDATA[我已经使用 tensorflow、keras 和 nltk 在 Python 中开发了一个聊天机器人模型，用于标记化。当我在 vs 终端中运行它时，它会显示时间戳和模型提供答案所需的时间，但我试图在使用 React 设计的网站中显示它。如何从输出中删除日志。我尝试了所有方法，包括隐藏日志（除非它们至关重要），但我仍然无法删除它们。
我试过用这个，但没有用，它仍然显示它们。我知道日志不是警告，所以它们可能不会被删除。
import os os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78531788/python-based-model-learning-through-intents-using-tf-keras-and-nltk-for-tokeniz</guid>
      <pubDate>Sat, 25 May 2024 08:01:27 GMT</pubDate>
    </item>
    <item>
      <title>librosa、MFCC 中的 TypeError</title>
      <link>https://stackoverflow.com/questions/75775979/typeerror-in-librosa-mfcc</link>
      <description><![CDATA[我有以下代码，它获取一个数据集（GTZAN）并将其转换为字典中的 MFCC：
DATASET_PATH = &#39;/content/drive/MyDrive/ColabNotebooksNew/PROJECT/ProjectMusic/Data/genres_original&#39;
JSON_PATH = &quot;data_10.json&quot;
SAMPLE_RATE = 22050 #每首歌曲时长 30 秒，采样率为 22,050 Hz
TRACK_DURATION = 30 # 以秒为单位
SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION #=661,500

def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):

# 用于存储映射、标签和 MFCC 的字典
data = {
&quot;mapping&quot;: [], #标签名称。size - (10,)
&quot;labels&quot;: [], #存储“真实”歌曲类型（值从 0-9）。 size - (5992,)
&quot;mfcc&quot;: [] #存储 mfccs.size - (5992, 216, 13)
}

samples_per_segment = int(SAMPLES_PER_TRACK / num_segments) #=110250
num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length) #=216(math.ceil of 215.332)
# 循环遍历所有流派子文件夹
for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):

# 确保我们正在处理流派子文件夹级别
if dirpath is not dataset_path:

# 在映射中保存流派标签（即子文件夹名称）
semantic_label = dirpath.split(&quot;/&quot;)[-1]
data[&quot;mapping&quot;].append(semantic_label)
print(&quot;\nProcessing: {}&quot;.format(semantic_label))
# 处理类型子目录中的所有音频文件
for f in filenames:

# 加载音频文件

file_path = os.path.join(dirpath, f)

if file_path != &#39;/content/drive/MyDrive/ColabNotebooksNew/PROJECT/ProjectMusic/Data/genres_original/jazz/jazz.00054.wav&#39;: 
&quot;&quot;&quot;fileError: 错误打开 &#39;/content/drive/MyDrive/ColabNotebooksNew/PROJECT/ProjectMusic/Data/genres_original/jazz/jazz.00054.wav&#39;: 文件包含未知数据格式。&quot;&quot;&quot;

signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE) #signal=音频文件中有多少样本, sample rate =音频文件的采样率, sample=22050

#处理音频文件的所有片段
for d in range(num_segments):

#计算当前片段的开始和结束样本
start = samples_per_segment * d
finish = start + samples_per_segment

#提取mfcc
mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length) #mfcc - 时间和 Coef(13 因为 num_mfcc=13), 
mfcc = mfcc.T #[216,13]
#仅存储具有预期向量数量的 mfcc 特征
if len(mfcc) == num_mfcc_vectors_per_segment: #==216
data[&quot;mfcc&quot;].append(mfcc.tolist())
data[&quot;labels&quot;].append(i-1)
print(&quot;{},segment:{}&quot;.format(file_path, d+1))
# 将 MFCC 保存到 json 文件
with open(json_path, &quot;w&quot;) as fp:
json.dump(data, fp, indent=4) # 将所有内容放入 Json 文件中

# 运行数据处理 
save_mfcc(DATASET_PATH, JSON_PATH, num_segments=6)

我已经使用这个代码很长一段时间了，它一直运行良好，直到今天我收到以下错误：
TypeError Traceback (most最近调用最后一次)
&lt;ipython-input-10-4a9371926618&gt; 在 &lt;module&gt;
1 # 运行数据处理
----&gt; 2 save_mfcc(DATASET_PATH, JSON_PATH, num_segments=6)

&lt;ipython-input-9-8ba1c6e78747&gt; 在 save_mfcc(dataset_path, json_path, num_mfcc, n_fft, hop_length, num_segments)
56 
57 # 提取 mfcc
---&gt; 58 mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length) #mfcc - 时间和 Coef（13 因为 num_mfcc=13），
59 mfcc = mfcc.T #[216,13]
60 # 仅存储具有预期向量数量的 mfcc 特征

TypeError：mfcc() 接受 0 个位置参数，但给出了 2 个位置参数（和 1 个仅关键字参数）

关于 save_mfcc 函数：
从音乐数据集中提取 MFCC 并将它们与流派标签一起保存到 json 文件中。
 :param dataset_path (str)：数据集路径
:param json_path (str)：用于保存的 json 文件的路径MFCCs
:param num_mfcc (int)：要提取的系数数量
:param n_fft (int)：我们考虑应用 FFT 的间隔。以样本数量为单位测量
:param hop_length (int)：FFT 的滑动窗口。以样本数量为单位测量
:param: num_segments (int)：我们要将样本轨迹划分成的段数
:return:

我不明白为什么今天才出现这个问题，以及如何解决它。
我该如何解决这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/75775979/typeerror-in-librosa-mfcc</guid>
      <pubDate>Sat, 18 Mar 2023 12:46:34 GMT</pubDate>
    </item>
    <item>
      <title>从头开始实现 dropout</title>
      <link>https://stackoverflow.com/questions/54109617/implementing-dropout-from-scratch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/54109617/implementing-dropout-from-scratch</guid>
      <pubDate>Wed, 09 Jan 2019 11:57:47 GMT</pubDate>
    </item>
    </channel>
</rss>