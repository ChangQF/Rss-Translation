<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 20 Oct 2024 03:26:49 GMT</lastBuildDate>
    <item>
      <title>Python 中的 Tensorflow predict() 时间序列对齐</title>
      <link>https://stackoverflow.com/questions/79106002/tensorflow-predict-timeseries-alignment-in-python</link>
      <description><![CDATA[假设我在 Tensorflow 中创建一个顺序输入 LSTM，如下所示：
def Sequential_Input_LSTM(df, input_sequence):
df_np = df.to_numpy()
X = []
y = []

for i in range(len(df_np) - input_sequence):
row = [a for a in df_np[i:i + input_sequence]]
X.append(row)
label = df_np[i + input_sequence]
y.append(label)

return np.array(X), np.array(y)

X, y = Sequential_Input_LSTM(df_data , 10) # pandas DataFrame df_data 包含我们的数据

在此示例中，我将数据切片X（输入向量）和 y（标签），例如前 10 个值（序列长度）用作 X，第 11 个值用作第一个 y。然后，将 10 个值的窗口向右移动一步（再移动一个时间步），我们再次为 X 取 10 个值，并将第二行之后的值作为下一个 y，依此类推。
然后假设我将 X 的一部分作为我的 X_test，并使用 LSTM model 进行时间序列预测，例如 predictions = model.predict(X_test)。
当我实际尝试此操作并绘制 predict(X_test) 的结果时，它看起来像 y 数组，并且预测结果是同步的，无需进一步调整。我预计在将预测数组与标签一起绘制时，我必须手动将预测数组向右移动 10 个时间步，因为我无法解释预测的前 10 个时间戳来自哪里。
由于模型尚未收到 10 个输入序列值，X_test 的前 10 个时间步的预测来自哪里？Tensorflow 是否使用 X_test 中的最后几个时间步来创建前 10 个值的预测，还是一开始的预测只是纯粹的猜测？]]></description>
      <guid>https://stackoverflow.com/questions/79106002/tensorflow-predict-timeseries-alignment-in-python</guid>
      <pubDate>Sat, 19 Oct 2024 21:37:05 GMT</pubDate>
    </item>
    <item>
      <title>MAPPO 无法达到良好的 PPO 性能</title>
      <link>https://stackoverflow.com/questions/79105876/unable-to-achieve-as-good-ppo-performance-with-mappo</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79105876/unable-to-achieve-as-good-ppo-performance-with-mappo</guid>
      <pubDate>Sat, 19 Oct 2024 20:29:44 GMT</pubDate>
    </item>
    <item>
      <title>具有独家类别的多标签 Keras CNN</title>
      <link>https://stackoverflow.com/questions/79105715/multi-label-keras-cnn-with-exclusive-classes</link>
      <description><![CDATA[我正在使用 Keras 构建一个多标签图像分类模型，该模型将包含 5 个类别。
前 3 个类别（我们称之为 A、B 和 C）是互斥的。我已经使用 sparse_categorical_crossentropy 损失函数和 softmax 激活函数构建了一个相当准确的模型。
我现在开始研究接下来的两个类，D 和 E。每个类都与其他类正交，即任何图像都应该恰好具有标签 A、B 或 C 中的一个，并且可以选择具有 D 或 E 或两者。
我首先在 D 上训练一个简单的模型，该模型使用 binary_crossentropy 损失函数和 sigmoid 激活函数。
现在我想知道我是否应该将它们组合成一个模型，或者维护 3 个独立的、有针对性的模型：一个用于 A/B/C，一个用于 D，一个用于 E。它们都将用于对同一组图像进行分类，因此似乎有一个可以为我提供所有 5 个类的预测的单一模型是有意义的。
应该如果我将第一个模型从 sparse_categorical_crossentropy / softmax 移至 binary_crossentropy / sigmoid 模型（其中每个类别被单独考虑，并且只选择置信度最高的类别），我预计会失去第一个模型的准确性？
在 Keras 的背景下，我应该考虑这两种方法之间的权衡吗？]]></description>
      <guid>https://stackoverflow.com/questions/79105715/multi-label-keras-cnn-with-exclusive-classes</guid>
      <pubDate>Sat, 19 Oct 2024 19:02:40 GMT</pubDate>
    </item>
    <item>
      <title>.pth 到 .onnx 的转换破坏了模型 u2net</title>
      <link>https://stackoverflow.com/questions/79105371/pth-to-onnx-conversion-break-the-model-u2net</link>
      <description><![CDATA[任务如下：网站需要添加从汽车磁盘图像中删除背景的功能。
我决定使用 rembg 库作为基础：https://github.com/danielgatis/rembg
此库又以 u2net 为基础：https://github.com/xuebinqin/U-2-Net
但是，标准 u2net 模型会从外部删除所有背景，而磁盘内部的空间保持不变 - 辐条、孔等之间。
在谷歌搜索了一下之后，我得出结论，我可以进一步训练 u2net 模型以用于我的特定需要。
操作算法如下：

进一步训练模型
将其加载到rembg中
使用自定义模型进行裁剪

我设法训练了标准u2net模型，它完美地按照我的需要裁剪出黑白蒙版。
但是，当将模型从.pth转换为.onnx格式（这是在rembg中工作所必需的）时，它开始工作不佳。
蒙版模糊且有肥皂味。我尝试转换标准的未训练的u2net模型
并在rembg中使用它 - 结果是一样的，蒙版模糊，背景裁剪不起作用。
因此，结论是训练成功了。
问题在于转换。
因此，以下是我训练过的模型的掩码示例。
原始图像
我训练过的模型生成的掩码
我训练过的模型转换为 .onnx 格式后生成的掩码
要在 u2net 中生成掩码，我使用：
python3 u2net_test.py

要在 rembg 中生成掩码，我使用命令：
rembg i -om -m u2net_custom -x &#39;{&quot;model_path&quot;: &quot;~/.u2net/u2net_custom.onnx&quot;}&#39; 55.jpg 55.png

我尝试转换完成的模型。以下是转换代码：
import torch
import torch.onnx
from model.u2net import U2NET

def load_model(model_path, model_class):
checkpoint = torch.load(model_path, map_location=&#39;cpu&#39;)
if isinstance(checkpoint, dict) and &#39;state_dict&#39; in checkpoint:
model = model_class()
model.load_state_dict(checkpoint[&#39;state_dict&#39;])
else:
model = model_class()
model.eval()
return model

def convert_to_onnx(model, output_path):
dummy_input = torch.randn(1, 3, 320, 320)
torch.onnx.export(model, dummy_input, output_path, opset_version=12,
dynamic_axes={&#39;input&#39;: {0: &#39;batch_size&#39;, 2: &#39;height&#39;, 3: &#39;width&#39;},
&#39;output&#39;: {0: &#39;batch_size&#39;, 2: &#39;height&#39;, 3: &#39;width&#39;}})
print(f&quot;success {output_path}&quot;)

if __name__ == &quot;__main__&quot;:
import argparse

parser = argparse.ArgumentParser(description=&quot;conversion PyTorch to ONNX&quot;)
parser.add_argument(&#39;--model-path&#39;, type=str, required=True, help=&#39;path to .pth file&#39;)
parser.add_argument(&#39;--output-path&#39;, type=str, required=True, help=&#39;save ONNX file&#39;)

args = parser.parse_args()
model = load_model(args.model_path, U2NET)
convert_to_onnx(model, args.output_path)

并尝试在训练过程中保存模型：
 if ite_num % save_frq == 0:
timestamp = int(time.time())
filePath = model_dir + model_name+&quot;_%d_%d.&quot; % (ite_num, timestamp)

torch.save(net.state_dict(), filePath + &#39;pth&#39;)

dummy_input = torch.randn(1, 3, 320, 320)
net.eval()
torch.onnx.export(net, dummy_input, filePath + &#39;onnx&#39;, opset_version=12)

running_loss = 0.0
running_tar_loss = 0.0
net.train() # 恢复训练
ite_num4val = 0

我尝试更改设置、更改库版本、更改 opset_version 以及 chatGpt 建议的所有其他操作。
结果总是一样的。
转换后模型停止工作。
我犯了什么错误？]]></description>
      <guid>https://stackoverflow.com/questions/79105371/pth-to-onnx-conversion-break-the-model-u2net</guid>
      <pubDate>Sat, 19 Oct 2024 16:07:45 GMT</pubDate>
    </item>
    <item>
      <title>将 BCELossWithLogits 中某个像素类的损失清零</title>
      <link>https://stackoverflow.com/questions/79105307/zero-out-loss-for-a-certain-pixel-class-in-bcelosswithlogits</link>
      <description><![CDATA[我正在对宠物（狗和猫）数据集执行二元语义分割，每个像素都有一个类别。共有 3 个类别，前景（1.0）、背景（0.0）和未分类像素（0.5020）。我只关心模型预测前景和背景像素的效果如何。所以我的想法是将“未分类”像素的损失设置为 0。这样这些像素就不会在反向传播过程中影响梯度。为了做到这一点，我的想法是创建一个掩码，对于前景类（1.0）和背景类（0.0）的像素，该掩码为 1.0（真），对于未分类类的像素，该掩码为 0.0（假）。下面你可以看到我的实现。
import torch
import torch.nn as nn

class CustomBCEWithLogitsLoss(nn.Module):
def __init__(self, ignore_class_value=0.5020):
super(CustomBCEWithLogitsLoss, self).__init__()
# 使用 logits 初始化二元交叉熵损失
self.bce_loss = nn.BCEWithLogitsLoss(reduction=&#39;none&#39;)
# 存储忽略类值
self.ignore_class_value = torch.tensor(ignore_class_value)

def forward(self, output, labels):
# 打印标签中的唯一值以供调试
print(&quot;Unique labels:&quot;, torch.unique(labels))

# 创建一个用于忽略未分类像素的掩码
mask = (labels != self.ignore_class_value).float()
print(&quot;唯一掩码值：&quot;, torch.unique(mask))

# 为前景类 (1.0) 创建二进制标签
binary_labels = (labels == 1.0).float()

# 计算损失
loss = self.bce_loss(outputs, binary_labels)
# 将掩码应用于损失
loss = loss * mask
# 根据有效像素数对损失进行标准化
loss = loss.sum() / mask.sum()

return loss

问题是，运行代码时我得到以下结果：
print(torch.unique(labels))
tensor([0.0000, 0.5020, 1.0000], device=&#39;cuda:0&#39;)
print(torch.unique(mask))
tensor([1.], device=&#39;cuda:0&#39;)

这不正确，因为标签显然包含类 0.0，所以我的掩码应该包含类型 1.0 和 0.0 的值。
我真的不明白为什么它不起作用。我尝试使用 torch.isClose 来检查问题是否与浮点问题有关，但这也没有解决问题。现在我陷入困境，正在试图了解问题所在。]]></description>
      <guid>https://stackoverflow.com/questions/79105307/zero-out-loss-for-a-certain-pixel-class-in-bcelosswithlogits</guid>
      <pubDate>Sat, 19 Oct 2024 15:34:57 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Heroku 上的 Python Flask 后端使用大型模型文件部署 YOLOv8 模型？</title>
      <link>https://stackoverflow.com/questions/79104416/how-to-deploy-yolov8-model-in-python-flask-backend-on-heroku-with-large-model-fi</link>
      <description><![CDATA[我使用 Python Flask 后端和 React Native 前端开发了一个用于图像检测和分类的移动应用程序。我使用 Google Colab 训练了我的 YOLOv8 模型，并下载了用于对象检测和分类的 best.pt 和 bestc.pt 文件。我将这些模型文件放在 Flask 应用程序的后端文件夹 (/backend/models/) 中。
我正尝试在 Heroku 上部署后端，但遇到了问题，因为模型的文件大小很大。当我尝试在 Heroku 上部署应用程序时，出现错误，提示无法检测到 ultralytics 包。安装 ultralytics 后，我的应用程序的大小增加到 3GB，超过了免费版 Heroku 允许的 500MB 限制。
如果没有模型和 ultralytics 包，后端大小只有 10MB。这是我的后端文件夹结构：
 ═── backend/
│ ═── models/
│ │ ═── bestc.pt
│ │ └── best.pt
│ └── app.py

from flask import Flask, request, jsonify
from ultralytics import YOLO
from PIL import Image, ImageOps
from flask_cors import CORS
import io
import os

app = Flask(__name__)
CORS(app)

# 启动时加载一次模型
yolo_model = YOLO(&#39;models/best.pt&#39;) # 用于对象检测和裁剪的 YOLO 模型
classification_model = YOLO(&#39;models/bestc.pt&#39;) # 用于分类的模型

CONFIDENCE_THRESHOLD = 0.5 # 设置有效置信度阈值检测

@app.route(&#39;/classify&#39;, methods=[&#39;POST&#39;])
def predict():
try:
file = request.files[&#39;image&#39;]
img = Image.open(file.stream).convert(&quot;RGB&quot;) # 确保它是 RGB 格式

# 运行 YOLO 进行对象检测
detection_results = yolo_model(img)

# 调试：打印检测结果
print(&quot;Detection Results:&quot;, detection_results)

# 检查是否有任何检测
if len(detection_results) == 0 or len(detection_results[0].boxes) == 0:
return jsonify({&#39;message&#39;: &#39;No object sent in the image.&#39;}), 200

# 过滤以仅保留高于阈值的最可信检测
best_box = max(detection_results[0].boxes, key=lambda box: box.conf)
如果 best_box.conf &lt; CONFIDENCE_THRESHOLD:
return jsonify({&#39;message&#39;: &#39;未检测到有效的蛇。请上传更清晰的图像。&#39;}), 200

box_coords = best_box.xyxy[0].tolist() # 转换为坐标列表
print(f&quot;裁剪坐标：{box_coords}&quot;) # 调试：打印裁剪坐标

cropped_img = img.crop(box_coords) # 使用边界框坐标进行裁剪

# 为裁剪后的图像添加填充以防止失真
padded_img = ImageOps.pad(cropped_img, (224, 224), method=Image.Resampling.LANCZOS)

# 对填充后的图像进行分类
classes_results = classes_model(padded_img)

# 处理分类结果
predictions = []
for result in分类结果：
top_class = result.names[result.probs.top1]
top_confidence = result.probs.top1conf.item()

# 如果分类置信度低于阈值，则拒绝结果
if top_confidence &lt; CONFIDENCE_THRESHOLD:
return jsonify({
&#39;message&#39;: &#39;未检测到有效的蛇。请上传更清晰的图像。&#39;,
&#39;class&#39;: top_class,
&#39;probability&#39;: &quot;{:.2%}&quot;.format(top_confidence)
}), 200

# 确定毒液状态
venom_status = get_venom_status(top_class)

# 将概率格式化为带有两个小数点的百分比
formatted_prob = &quot;{:.2%}&quot;.format(top_confidence)

# 附加到预测
predictions.append({
&#39;class&#39;: top_class,
&#39;probability&#39;: formatted_prob,
&#39;venom_status&#39;: venom_status
})

return jsonify({&#39;predictions&#39;: predictions}), 200

except Exception as e:
print(f&quot;Error: {str(e)}&quot;)
return jsonify({&#39;error&#39;: &#39;处理过程中发生错误。&#39;, &#39;details&#39;: str(e)}), 500

def get_venom_status(class_name):
venom_status_map = {
&#39;Common Indian Krait&#39;: &#39;有毒&#39;,
&#39;Python&#39;: &#39;无毒&#39;,
&#39;Hump Nosed Viper&#39;: &#39;有毒&#39;,
&#39;Green Vine Snake&#39;: &#39;无毒&#39;,
&#39;Russells Viper&#39;: &#39;有毒&#39;,
&#39;Indian Cobra&#39;: &#39;有毒&#39;
}
return venom_status_map.get(class_name, &#39;Unknown&#39;)

if __name__ == &#39;__main__&#39;:
app.run(host=&#39;0.0.0.0&#39;, port=int(os.environ.get(&#39;PORT&#39;, 5000)))


如何在 Heroku 上部署这个具有大模型（约 3GB）的 Flask 应用，同时又不超出 Heroku 500MB 的免费套餐限制？
在 Heroku 上部署时，是否有管理大模型的最佳实践，或者是否有其他云解决方案可以解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79104416/how-to-deploy-yolov8-model-in-python-flask-backend-on-heroku-with-large-model-fi</guid>
      <pubDate>Sat, 19 Oct 2024 07:39:42 GMT</pubDate>
    </item>
    <item>
      <title>在 Colab 中微调 Llama 3.1 时的上下文长度限制</title>
      <link>https://stackoverflow.com/questions/79104305/context-length-limitation-when-fine-tuning-llama-3-1-in-colab</link>
      <description><![CDATA[我正在通过 Unsloth 库，使用带有自定义数据集（使用 LoRA 技术）的 A100 GPU 在 Google Colab Pro 中对 Llama 3.1 模型进行微调。下面是我正在使用的 LoRA 代码：
max_seq_length = 2048
model = FastLanguageModel.get_peft_model(
model,
r=16, # 选择任意数字 &gt; 0 ！建议 8、16、32、64、128
target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
&quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down​​_proj&quot;],
lora_alpha=16,
lora_dropout=0, # 支持任意，但 = 0 是经过优化的
bias=&quot;none&quot;, # 支持任意，但 = &quot;none&quot; 是经过优化的

use_gradient_checkpointing=&quot;unsloth&quot;, # 对于非常长的上下文，为 True 或 &quot;unsloth&quot;
random_state=3407,
use_rslora=False, # 我们支持等级稳定的 LoRA
loftq_config=None, # 和 LoftQ
)
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
train_dataset=dataset,
dataset_text_field=&quot;text&quot;,
max_seq_length=max_seq_length,
dataset_num_proc=2,
packing=False, # 可以使短序列的训练速度提高 5 倍。
args=TrainingArguments(
per_device_train_batch_size=2,
gradient_accumulation_steps=4,
warmup_steps=5,
# num_train_epochs = 1, # 将其设置为 1 次完整的训练运行。
max_steps=60,
learning_rate=2e-4,
fp16=not is_bfloat16_supported(),
bf16=is_bfloat16_supported(),
logs_steps=1,
optim=&quot;adamw_8bit&quot;,
weight_decay=0.01,
lr_scheduler_type=&quot;linear&quot;,
seed=3407,
output_dir=&quot;outputs&quot;,
),
)

加载模型时，我们必须指定最大序列长度，这会限制其上下文窗口。 Llama 3.1 支持高达 128k 的上下文长度，但在本例中我将其设置为 2048，因为它消耗更多的计算和 VRAM。此外，dtype 参数会自动检测您的 GPU 是否支持 BF16 格式，以便在训练期间获得更高的稳定性（此功能仅限于 Ampere 和较新的 GPU）。
我的问题：

如果我在训练时将 max_seq_length 设置为 2048，那么训练后我的模型的上下文长度是多少，128k 还是 2048？
训练模型后，我们可以使用 128k 的上下文长度吗，还是仍然限制为 2048？
]]></description>
      <guid>https://stackoverflow.com/questions/79104305/context-length-limitation-when-fine-tuning-llama-3-1-in-colab</guid>
      <pubDate>Sat, 19 Oct 2024 05:59:26 GMT</pubDate>
    </item>
    <item>
      <title>numpy nd 数组连接用于创建数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/79104179/numpy-nd-array-concatenation-for-dataset-creation</link>
      <description><![CDATA[我正在处理一个数据集，其中训练数据具有此形状（33104,6,128,1），标签具有此形状（33104,1），我该如何连接标签以适合其最后一列？
我该如何连接标签以适合其最后一列？]]></description>
      <guid>https://stackoverflow.com/questions/79104179/numpy-nd-array-concatenation-for-dataset-creation</guid>
      <pubDate>Sat, 19 Oct 2024 04:01:10 GMT</pubDate>
    </item>
    <item>
      <title>训练模型预测不一致</title>
      <link>https://stackoverflow.com/questions/79104144/trained-model-prediction-are-not-consistent</link>
      <description><![CDATA[我用一个小型数据集训练了一个 VGG16 (224x224x3) 迁移学习图像分类模型，该数据集包含来自 3 位艺术家的画作。该数据集有大约 80 幅画作用于训练，25 幅用于测试。图像上有增强。图像被标记为真（属于艺术家）或假（不属于艺术家），比例约为 50-50。训练补丁总数（224x224x3）约为 20000。该模型旨在判断一幅画是否属于艺术家。这是训练损失和准确度图：

该图表明该模型在测试数据集上表现良好。然后将壁灯等图像（模型从未见过这些图像）输入模型，并询问此壁灯图像是否属于艺术家。显然答案是否定的。但最初有几次模型确实给出了“否”的答案，但现在它始终给出“是”的答案，这显然是错误的。我对训练模型性能的经验有限。什么会导致训练模型的预测不一致？训练数据集太小还是其他原因？]]></description>
      <guid>https://stackoverflow.com/questions/79104144/trained-model-prediction-are-not-consistent</guid>
      <pubDate>Sat, 19 Oct 2024 03:19:59 GMT</pubDate>
    </item>
    <item>
      <title>决策树，Knn 算法 [关闭]</title>
      <link>https://stackoverflow.com/questions/79103501/decision-tree-knn-algorithms</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79103501/decision-tree-knn-algorithms</guid>
      <pubDate>Fri, 18 Oct 2024 19:59:23 GMT</pubDate>
    </item>
    <item>
      <title>在 XGBoost 预测模型中更新未来预测的傅里叶项和滞后 [关闭]</title>
      <link>https://stackoverflow.com/questions/79102211/updating-fourier-terms-and-lags-for-future-predictions-in-xgboost-forecasting-mo</link>
      <description><![CDATA[我在 Python 中构建了一个 XGBoost 预测模型，该模型结合了滞后特征、日期时间特征和傅立叶项。我的目标是对每个新的工作日进行预测。该模型使用 5 倍交叉验证进行训练，我从调整过程中保存了最佳超参数。之后，我使用这些最佳参数重新训练整个模型。
我面临的挑战是使用预测日的正确特征更新未来数据框：
对于日期时间特征，我可以轻松更新它们。
但是，更新傅立叶项 (FT) 和滞后是我遇到的难题。
这是我尝试过的方法：
我将历史数据框与新的预测数据框结合起来，将 Y 列重命名为 pred。
对于第一个预测日，我使用最后已知的实际值。
对于后续的预测日，我需要使用历史数据和新预测值的组合来更新傅立叶项和滞后。
尽管尝试了几次，我还是无法让更新过程正常工作。傅立叶项没有按预期对齐，我很难根据历史数据和预测数据的组合调整滞后。
问题：如何使用历史预测和新预测的组合正确更新未来几天的傅立叶项和滞后？以下是数据框。 pred 列是实际历史数据的 Y 变量（国家计数）的副本。

# 使用 NaN 初始化预测列
future_df_all_countries[&#39;pred&#39;] = np.nan

# 使用已经训练过的 `best_model` 对未来数据进行预测
# 循环遍历 future_df_all_countries 中的每一行
for i in range(len(future_df_all_countries)):
if not future_df_all_countries[&#39;is_actual&#39;].iloc[i]: # 仅预测是否为未来数据
if i == 0:
# 对于第一个预测日，使用最后一个实际计数
last_actual_count = future_df_all_countries.loc[future_df_all_countries[&#39;is_actual&#39;] == True, &#39;Country count&#39;].iloc[-1]
future_df_all_countries.at[future_df_all_countries.index[i], &#39;pred&#39;] = last_actual_count
else:
# 使用已经训练好的模型对当天进行预测
future_df_all_countries.at[future_df_all_countries.index[i], &#39;pred&#39;] = best_model.predict(future_df_all_countries[FEATURES].iloc[[i]])[0]

# 计算傅里叶变换的函数
def calculate_fourier_transform(group, components):
data_FT = group[[&#39;pred&#39;]] # 使用 &#39;pred&#39; 列进行 FT 计算
if data_FT[&#39;pred&#39;].isnull().all():
return pd.DataFrame(index=group.index) # 如果全部为 NaN，则返回空 DF
country_count_fft = np.fft.fft(np.asarray(data_FT[&#39;pred&#39;].tolist()))
ifft_results = pd.DataFrame(index=group.index)

# 使用指定的列名进行更新
for i, num_ in enumerate(components):
fft_list = np.copy(country_count_fft)
fft_list[num_:-num_] = 0
ifft_results[f&#39;ifft_{num_}_components&#39;] = np.fft.ifft(fft_list).real

return ifft_results

# 用于傅里叶变换的组件列表
component_list = [70, 80, 90] # 使用指定的组件

#使用“pred”列计算所有行的 FT
fourier_results = calculate_fourier_transform(future_df_all_countries, component_list)

# 使用 FT 结果更新 future_df_all_countries
for col in fourier_results.columns:
future_df_all_countries[col] = fourier_results[col]

# 根据“pred”创建滞后特征
def create_lag_features(df, target_col=&#39;pred&#39;, group_col=&#39;SHIP_TO_COUNTRY&#39;, lags=[5, 10, 15, 30]):
for lag in lags:
df[f&#39;lag_{lag}_day&#39;] = df.groupby(group_col)[target_col].shift(lag)
return df

# 应用滞后特征创建
future_df_all_countries = create_lag_features(future_df_all_countries)
]]></description>
      <guid>https://stackoverflow.com/questions/79102211/updating-fourier-terms-and-lags-for-future-predictions-in-xgboost-forecasting-mo</guid>
      <pubDate>Fri, 18 Oct 2024 13:07:36 GMT</pubDate>
    </item>
    <item>
      <title>使用 RAG 和 FastAPI 的 WebApp [关闭]</title>
      <link>https://stackoverflow.com/questions/79099986/webapp-using-rag-and-fastapi</link>
      <description><![CDATA[我正在开展一个项目，需要实现一个检索增强生成 (RAG) 后端系统，该系统可以分析和比较两个提供的调查结果数据集。目标是创建一个具有 Python FastAPI 后端和 ReactJS 前端的 Web 应用程序。该应用程序应允许用户：
探索两个数据集。
交叉比较数据集。
从用户查询中生成 AI 驱动的见解。
后端应根据用户查询从数据集中检索相关数据，并将检索到的数据传递给文本生成模型以生成上下文响应。
这两个数据集都是 Excel 格式，但我已将它们转换为 CSV，甚至尝试使用 SQL 和 JSON 来处理数据。数据集复杂且嵌套，标题在行和列中重复，使数据看起来像矩阵，这使处理变得复杂。
最大的问题似乎是数据集本身的复杂性。它包含带有行和列标题的嵌套信息，因此很难提取有意义的文本。检索和生成的组合并没有产生相关的见解。GPT-2 倾向于重复自己或提供通用的、非信息性的响应，例如：

圣诞节是我们所有人庆祝和庆祝的时刻。这是我们所有人庆祝的时刻。这是一个庆祝的时刻……

我如何处理具有重复行和列标题的复杂数据集，并将其转换为 RAG 管道可用的格式？
任何帮助或指导都将不胜感激！
GitHub 存储库：https://github.com/drrahulsuresh/bounce/tree/dev]]></description>
      <guid>https://stackoverflow.com/questions/79099986/webapp-using-rag-and-fastapi</guid>
      <pubDate>Thu, 17 Oct 2024 22:12:24 GMT</pubDate>
    </item>
    <item>
      <title>在 mask-rcnn 上使用自定义数据集进行训练时 lambda 函数出现问题</title>
      <link>https://stackoverflow.com/questions/78623475/issue-with-the-lambda-function-when-training-with-coustom-dataset-on-mask-rcnn</link>
      <description><![CDATA[回溯（最近一次调用最后一次）：
文件“D:\Local Disk E\Project-mask-rcnn\Mask_RCNN\custom.py”，第 238 行，位于
model = modellib.MaskRCNN(mode=&quot;training&quot;, config=config,
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“D:\Local Disk E\Project-mask-rcnn\Mask_RCNN\mrcnn\model.py”，第 1838 行，位于 init
self.keras_model = self.build(mode=mode, config=config)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“D:\Local Disk E\Project-mask-rcnn\Mask_RCNN\mrcnn\model.py&quot;，第 1876 行，在 build 中
gt_boxes = KL.Lambda(lambda x: norm_boxes_graph(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;d:\Local Disk E\Project-mask-rcnn\mask-rcnn\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;，第 122 行，在 erroler 中
raise e.with_traceback(filtered_tb) from None
文件 &quot;d:\Local Disk E\Project-mask-rcnn\mask-rcnn\Lib\site-packages\keras\src\layers\core\lambda_layer.py&quot;，第 95 行，在cooutput_shape
raise NotImplementedError(
NotImplementedError: 调用 Lambda.call() 时遇到异常。

我们无法自动推断 Lambda 输出的形状。请为此 Lambda 层指定 output_shape 参数。

Lambda.call() 收到的参数：
• args=(&#39;&lt;KerasTensor shape=(None, None, 4), dtype=float32, sparse=None, name=input_gt_boxes&gt;&#39;,)
• kwargs={&#39;mask&#39;: &#39;None&#39;}

如何消除此错误？
如何指定输出形状以及我应该给出什么输出形状，因为我正在我的自定义数据集上对其进行训练。]]></description>
      <guid>https://stackoverflow.com/questions/78623475/issue-with-the-lambda-function-when-training-with-coustom-dataset-on-mask-rcnn</guid>
      <pubDate>Fri, 14 Jun 2024 14:13:21 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络自定义数据</title>
      <link>https://stackoverflow.com/questions/78049931/graph-neural-network-custom-data</link>
      <description><![CDATA[我正在遵循本教程。
https://colab.research.google.com/github/AntonioLonga/PytorchGeometricTutorial/blob/main/Tutorial3/Tutorial3.ipynb
我想在自定义数据集上尝试一下。如何使用我自己的数据集而不是 Cora？我很想知道任何想法。]]></description>
      <guid>https://stackoverflow.com/questions/78049931/graph-neural-network-custom-data</guid>
      <pubDate>Fri, 23 Feb 2024 20:02:48 GMT</pubDate>
    </item>
    <item>
      <title>Android 上的 Yolov8 TFLite 模型</title>
      <link>https://stackoverflow.com/questions/77634474/yolov8-tflite-model-on-android</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77634474/yolov8-tflite-model-on-android</guid>
      <pubDate>Sun, 10 Dec 2023 10:47:01 GMT</pubDate>
    </item>
    </channel>
</rss>