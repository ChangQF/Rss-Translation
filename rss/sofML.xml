<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 19 Dec 2023 21:10:21 GMT</lastBuildDate>
    <item>
      <title>AWS Sagemaker/ML 操作</title>
      <link>https://stackoverflow.com/questions/77687831/aws-sagemaker-ml-ops</link>
      <description><![CDATA[我在尝试使用 AWS Sagemaker 端点进行推理时遇到 AWS 实例问题。我需要的图像 ml.g5.12xlargem 不在我的配额范围内。我需要这个，或者我的模型尺寸太大。当我开票时，他们只是告诉我使用当前的配额，但我没有现金可以浪费。
现在我在 Colab Notebook 中微调了 Llama-2-7b-chat，并手动将其上传到 s3 存储桶中。
有什么办法可以适当增加配额吗？致电 AWS Support 对您有用吗？我的s3存储桶包含model.tar.gz，可能格式不正确，因此太大。
解决方案可能是按照 Sagemaker Studio 中的说明进行部署：
https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/
但如果我不在 Sagemaker Studio 中进行训练，这是否可能：
https： //github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/llama-2-finetuning.ipynb
这可能有效，但重新训练需要时间。我仍然会遇到同样的问题，因为该实例不在我的配额内。
或者我应该使用不同的文本生成模型，称为 Phi-2。它的性能比 llama 2 稍好一些，是 2.7B 参数，比 7B Llama 模型少很多。它可能能够运行成本低得多且可用的计算。它需要迁移到 Azure AI Studio，并对功能进行完整的重新培训，以及学习曲线。

增加配额或减小模型大小的某种方法
在 Sagemaker studio 中以略有不同的方式训练和运行推理
使用不同的文本生成模型 (Phi-2)，并在 Azure AI Studio 中执行此操作（我计划在将来执行此操作，只有在必要时我才会立即执行此操作）
]]></description>
      <guid>https://stackoverflow.com/questions/77687831/aws-sagemaker-ml-ops</guid>
      <pubDate>Tue, 19 Dec 2023 20:26:02 GMT</pubDate>
    </item>
    <item>
      <title>新的 ML 模型具有较低的历元损失，但平均 RMSE 较高。如何/为什么？</title>
      <link>https://stackoverflow.com/questions/77687730/new-ml-model-with-lower-epoch-loss-but-higher-average-rmse-how-why</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77687730/new-ml-model-with-lower-epoch-loss-but-higher-average-rmse-how-why</guid>
      <pubDate>Tue, 19 Dec 2023 20:04:25 GMT</pubDate>
    </item>
    <item>
      <title>为什么 MLNet 没有得出合理的结论</title>
      <link>https://stackoverflow.com/questions/77687481/why-is-mlnet-not-coming-to-reasonable-conclusions</link>
      <description><![CDATA[我有一个模型，它目前基于合理的推理，然后对其进行测试来支持它。问题是预测多人游戏中的获胜者。
所以我希望使用一些机器学习来改进它。第一次尝试是做大小玩家的游戏，数据如下col0[标签],col1-col10[玩家一数据],col11-20[玩家二数据]...
数据是平衡的，因此每个类别的可能性与其他类别的可能性相同。
当样本用完时，这能够以 24% 的准确率进行预测，但问题是我知道，如果你只按每个玩家的第一个列排序，你会在样本中获得 27% 的准确率（并且你需要向你的老板为什么会这样）。我还知道所有其他数据都具有预测能力，可以解释原因并通过回归证明这一点，并且它们都在现实世界中发挥了与测试相同的准确性。
我尝试简化 MLNet 的问题，将其变成二元分类问题，并让它预测两个玩家的相对成功，数据再次平衡。这与第一列的执行方式几乎相同，但我可以看到它使用其他参数，如果我使用它们，我会得到比这产生的更好的结果。
经过几个小时的训练，我终于有了一个 6 人游戏的模型，它与第一个排序排序达到了同等水平，但我觉得我不能相信它，因为它无意中接触到了样本外的数据适者生存，淘汰在样本外表现不佳的模型。
我使用“Microsoft.ML”在本地运行了这个但我尝试过不同的自动化机器学习提供商 azure、amazon、google，但它们都未能产生更好的结果。当人们不断告诉我 ML 很棒，你可以向其扔数据，但就这个问题而言，它只是不如人类洞察力时，我是否遗漏了一些东西。
请注意，六人游戏有 1,000,000 条记录，我将其分解为两人游戏的 10,000,000 条记录。
关于提高性能有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77687481/why-is-mlnet-not-coming-to-reasonable-conclusions</guid>
      <pubDate>Tue, 19 Dec 2023 19:11:11 GMT</pubDate>
    </item>
    <item>
      <title>用于优化的多变量梯度上升问题</title>
      <link>https://stackoverflow.com/questions/77687410/problem-with-multivariable-gradient-ascent-for-optimization</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77687410/problem-with-multivariable-gradient-ascent-for-optimization</guid>
      <pubDate>Tue, 19 Dec 2023 18:56:50 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 LightGBM.LGBMRanker 执行交叉验证，同时将组保持在一起？</title>
      <link>https://stackoverflow.com/questions/77687360/how-to-perform-cross-validation-with-lightgbm-lgbmranker-while-keeping-groups-t</link>
      <description><![CDATA[我遇到了搜索问题，我有一个查询和网址数据集。对于给定的查询，每对（查询、网址）都有一个相关性（目标），即一个应保留网址顺序的浮点数。
我想对我的 lightgbm.LGBMRanker 模型进行交叉验证，目标为 ndcg。
我浏览了文档，发现将实例保留在同一个组中非常重要，因为实例实际上是一个包含所有关联 URL 的查询。
然而，我对此有一个问题，因为我收到以下错误：
ValueError: 计算 NDCG 仅当存在超过 1 个文档时才有意义。反而得到了1。

我使用了调试器，虽然我的数据集中没有任何大小小于 2 的组，但在 _feval 函数中我有较小的组，这意味着 cv() 函数实际上并未将组保持在一起。
在 lightgbm.cv 我看到没有 LGBMRanker。
但我可以看到函数 lightbm.cv 精确地指出通过参数传递的值优先于通过参数提供的值。我的理解是这个值被传递给cv函数的底层模型。
这是我到目前为止的代码：
def eval_model(
    自己，
    模型：lightgbm.LGBMRanker，
    k_fold: int = 3,
    种子：int = 42，
）：
    “”“”用 NDCG 进行评估“”“”

    def _feval(y_pred: np.ndarray, lgb_dataset: lightgbm.basic.Dataset):
        y_true = lgb_dataset.get_label()
        serp_sizes = lgb_dataset.get_group()

        ndcg_值 = []
        开始=0
        对于 serp_sizes 中的大小：
            结束=开始+大小
            y_true_serp, y_pred_serp = y_true[开始:结束], y_pred[开始:结束]
            ndcg_serp = sklearn.metrics.ndcg_score(
                [y_true_serp]，[y_pred_serp]，k=10
            ）
            ndcg_values.append(ndcg_serp)
            开始=结束

        eval_name =“my-ndcg”；
        eval_result = np.mean(ndcg_values)
        更大更好=真
        返回 eval_name、eval_result、greater_is_better

    lgb_dataset = lightgbm.Dataset(data=self.X, label=self.y, group=self.serp_sizes)
    cv_结果 = lightgbm.cv(
        params={**model.get_params(), &quot;group&quot;: self.serp_sizes},
        train_set=lgb_dataset,
        num_boost_round=1_000,
        nfold=k_fold,
        分层=假，
        种子=种子，
        费瓦尔=_费瓦尔,
    ）
    ndcg = np.mean(cv_results[“my-ndcg”])

    返回 NDCG

我的错误/误解在哪里？
是否有一个简单的解决方法可以使用 lightgbm.LGBMRanker 执行交叉验证，并将组保持在一起？]]></description>
      <guid>https://stackoverflow.com/questions/77687360/how-to-perform-cross-validation-with-lightgbm-lgbmranker-while-keeping-groups-t</guid>
      <pubDate>Tue, 19 Dec 2023 18:47:13 GMT</pubDate>
    </item>
    <item>
      <title>python中的矩阵乘法得到成本函数</title>
      <link>https://stackoverflow.com/questions/77686666/matrix-multiplication-in-python-to-get-cost-function</link>
      <description><![CDATA[谁能告诉我为什么我的代码给出了错误的输出？
评论的是我的。
# 分级函数：cofi_cost_func
#UNQ_C1

def cofi_cost_func(X, W, b, Y, R, lambda_):
    ”“”
    返回基于内容的过滤的成本
    参数：
      X (ndarray (num_movies,num_features))：项目特征矩阵
      W (ndarray (num_users,num_features)) ：用户参数矩阵
      b (ndarray (1, num_users) ：用户参数向量
      Y (ndarray (num_movies,num_users) ：电影用户评分矩阵
      R (ndarray (num_movies,num_users) ：矩阵，其中 R(i, j) = 1 如果第 i 个电影由第 j 个用户评分
      lambda_ (float): 正则化参数
    返回：
      J（浮点数）：成本
    ”“”
    nm, nu = Y.shape
    J = 0
    ### 从这里开始代码 ###
    
# J+= np.sum(np.square((R*(X@(W.T)+b)-Y)))/2
# J+= (lambda_/2)*np.sum(np.square(W))
# J+= (lambda_/2)*np.sum(np.square(X))
        
    对于 j 在范围内（nu）：
        w = W[j,:]
        b_j = b[0,j]
        对于范围内的 i（nm）：
            x = X[i,:]
            y = Y[i,j]
            r = R[i,j]
            J += np.square(r * (np.dot(w,x) + b_j - y ) )
    J = J/2
    J += (lambda_/2) * (np.sum(np.square(W)) + np.sum(np.square(X)))
            
            
            
    
    
    ### 在此结束代码 ###

    返回J

我不知道为什么使用我的成本函数代码测试会失败，我只是尝试在不使用循环的情况下做同样的事情。
 J+= np.sum(np.square((R*(X@(W.T)+b)-Y)))/2
    J+= (lambda_/2)*np.sum(np.square(W))
    J+= (lambda_/2)*np.sum(np.square(X))
]]></description>
      <guid>https://stackoverflow.com/questions/77686666/matrix-multiplication-in-python-to-get-cost-function</guid>
      <pubDate>Tue, 19 Dec 2023 16:34:22 GMT</pubDate>
    </item>
    <item>
      <title>机器学习后指标结果相同的问题</title>
      <link>https://stackoverflow.com/questions/77686328/problem-with-identical-metrics-results-after-machine-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77686328/problem-with-identical-metrics-results-after-machine-learning</guid>
      <pubDate>Tue, 19 Dec 2023 15:36:36 GMT</pubDate>
    </item>
    <item>
      <title>有什么想法可以让 scikit 学得更快吗？它不断使内核崩溃</title>
      <link>https://stackoverflow.com/questions/77686255/any-ideas-how-to-make-scikit-learn-faster-it-keeps-crashing-the-kernel</link>
      <description><![CDATA[我的内核在这个单元格中不断死亡：
hyper_params = {&#39;outlier_neighbors&#39;:3, &#39;base_lambda&#39;:1e9, &#39;base_p&#39;:0.0005, &#39;smoothing_window&#39;:2,&#39;smoothing_order&#39;:1,
                &#39;diff_ma_window&#39;：2，&#39;diff_rhl_window&#39;：2，&#39;eps&#39;：0.001，&#39;ms&#39;：2，&#39;regress_portion&#39;：（0,1）}
#parameters 我建议采样频率为 1 分钟的数据

outlier_neighbors = hyper_params[&#39;outlier_neighbors&#39;]
base_lambda = hyper_params[&#39;base_lambda&#39;]
base_p = hyper_params[&#39;base_p&#39;]
smoothing_window = hyper_params[&#39;smoothing_window&#39;]
smoothing_order = hyper_params[&#39;smoothing_order&#39;]
diff_ma_window = hyper_params[&#39;diff_ma_window&#39;]
diff_rhl_window = hyper_params[&#39;diff_rhl_window&#39;]

污染物 = &#39;CO2&#39; #此处指定浓度时间序列的列名称
date_time = &#39;日期/时间&#39; #and 用于您的时间戳列

有没有办法让 scikit learn 库运行得更快或更有效，这样就不会杀死内核？]]></description>
      <guid>https://stackoverflow.com/questions/77686255/any-ideas-how-to-make-scikit-learn-faster-it-keeps-crashing-the-kernel</guid>
      <pubDate>Tue, 19 Dec 2023 15:22:52 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Matlab 中从线性 SVM 导出权重和偏移量 [关闭]</title>
      <link>https://stackoverflow.com/questions/77685684/how-to-derive-weights-and-offsets-from-linear-svm-in-matlab</link>
      <description><![CDATA[Matlab训练的线性SVM模型如下：
阿尔法
测试版
偏差
穆
西格玛
支持向量
支持向量标签
请问如何使用上述参数来计算超平面的权重和偏移？
超平面：f(x) = 符号((w,x) + b)]]></description>
      <guid>https://stackoverflow.com/questions/77685684/how-to-derive-weights-and-offsets-from-linear-svm-in-matlab</guid>
      <pubDate>Tue, 19 Dec 2023 13:52:53 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Hubert 模型获得音频嵌入</title>
      <link>https://stackoverflow.com/questions/77685045/how-to-get-audio-embeddings-using-hubert-model</link>
      <description><![CDATA[示例代码
进口火炬
从变压器导入 Wav2Vec2Processor、HubertForCTC
从数据集导入load_dataset

处理器 = Wav2Vec2Processor.from_pretrained(“facebook/hubert-large-ls960-ft”)
模型 = HubertForCTC.from_pretrained(“facebook/hubert-large-ls960-ft”)
input_values = 处理器(&#39;来自音频文件的数组。, return_tensors=“pt”).input_values


之后如何获得嵌入？模型中没有最后一个隐藏状态。
尝试了我提到的代码块，此外，尝试创建没有最后一层的模型，然后将其提供给输入。我的另一个问题是，假设我有不同时间维度的剪辑，那么如何创建固定嵌入。是沿着时间轴平均还是需要不同的方法。]]></description>
      <guid>https://stackoverflow.com/questions/77685045/how-to-get-audio-embeddings-using-hubert-model</guid>
      <pubDate>Tue, 19 Dec 2023 12:04:19 GMT</pubDate>
    </item>
    <item>
      <title>如何为此笔记本创建 Sagemaker 端点？</title>
      <link>https://stackoverflow.com/questions/77660996/how-can-i-create-a-sagemaker-endpoint-for-this-notebook</link>
      <description><![CDATA[我创建了一个 VectorDB (FAISS) 并将 PDF 输入到其中。然后我使用 AWS Bedrock 的 Langchain 包装器来调用它。我知道现在存在 Kowledge Base，但至少在 SageMaker 笔记本中，我有更多的控制权。该模型在 SageMaker Notebook 中完美运行，当我提出问题时，它会返回答案。
我想做的是创建一个小网页（并通过 HTTP/REST API），只需在文本字段中提交问题并在文本字段中接收答案。我猜如果链中某个地方没有 Lambda 函数，这很难做到，或者也许不是？
当我查看 Sagemaker 控制台的推理选项卡下时，没有模型或没有端点，或者没有&lt; /strong&gt; 端点配置（因为我没有从 Sagemaker 选择模型，所以我只是在 Python 笔记本中使用 langchain LLM 和 Bedrock，如下所示）。
&lt;前&gt;&lt;代码&gt;导入boto3
导入 json

bedrock = boto3.client(service_name=&quot;bedrock&quot;)
bedrock_runtime = boto3.client(service_name=“bedrock-runtime”)



从 langchain.llms.bedrock 导入 Bedrock
从 langchain.chains 导入 RetrievalQA
从 langchain.prompts 导入 PromptTemplate

嵌入 = BedrockEmbeddings(model_id=“amazon.titan-embed-text-v1”,
                               客户端=bedrock_runtime）

最终我将文档嵌入到 FAISS Vector 数据库中，我查询的就是这个数据库
db = FAISS.from_documents（文档，嵌入）


模型泰坦 = {
    “最大令牌计数”：512，
    “停止序列”：[]，
    “温度”：0.0，
    “顶部P”：0.5
}

# 亚马逊泰坦模型
llm = 基岩(
    model_id=&quot;amazon.titan-text-express-v1&quot;,
    客户端=bedrock_runtime，
    model_kwargs=model_titan,
）

然后定义一个提示......
提示 = 提示模板(
    template=prompt_template, input_variables=[“上下文”, “问题”]
）

并查询数据库：
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=“东西”，
    检索器=db.as_retriever(
        search_type=“相似度”，
    ),
    return_source_documents=真，
    chain_type_kwargs={“提示”: 提示},
）



query =“未来的技术是什么样的？”

结果 = qa({“查询”: 查询})

print(f&#39;查询: {结果[“查询”]}\n&#39;)
print(f&#39;结果: {结果[“结果”]}\n&#39;)
print(f&#39;上下文文档：&#39;)
对于结果 [“source_documents”] 中的 srcdoc：
      打印（f&#39;{srcdoc}\n&#39;）

这恰好返回了我在 Sagemaker 中需要的内容，我只需要从外部查询数据库即可。
我不想让 lambda 函数每次都重建链。我考虑的是效率，我需要的只是在 lambda 函数中传递查询并返回结果。]]></description>
      <guid>https://stackoverflow.com/questions/77660996/how-can-i-create-a-sagemaker-endpoint-for-this-notebook</guid>
      <pubDate>Thu, 14 Dec 2023 14:49:20 GMT</pubDate>
    </item>
    <item>
      <title>Optuna 分数与 Cross_val_score？</title>
      <link>https://stackoverflow.com/questions/71778333/optuna-score-vs-cross-val-score</link>
      <description><![CDATA[optuna 的准确度分数和 cross_val_score 的分数不同。为什么会发生这种情况以及我应该选择哪个分数？
我在 cross_val_score 中使用了 optuna 中获得的超参数。
def Objective_lgb（试用版）：
    num_leaves = Trial.suggest_int(“num_leaves”, 2, 1000)
    最大深度 = Trial.suggest_int(“最大深度”, 2, 100)
    学习率 = Trial.suggest_float(&#39;学习率&#39;, 0.001, 1)
    n_estimators = Trial.suggest_int(&#39;n_estimators&#39;, 100, 2000)
    min_child_samples = Trial.suggest_int(&#39;min_child_samples&#39;, 3, 1000)
    子样本 = Trial.suggest_float(&#39;子样本&#39;, 0.000001, 1)
    colsample_bytree = Trial.suggest_float(&#39;colsample_bytree&#39;, 0.00000001, 1)
    reg_alpha = Trial.suggest_float(&#39;reg_alpha&#39;, 0, 400)
    reg_lambda = Trial.suggest_float(“reg_lambda”, 0, 400)
    important_type = Trial.suggest_categorical(&#39;importance_type&#39;, [&quot;split&quot;, &quot;gain&quot;])

    lgb_clf = lgb.LGBMClassifier(random_state=1,
                         目标=“多类”，
                         类数 = 3,
                         重要性类型=重要性类型，
                         num_leaves=num_leaves,
                         最大深度=最大深度，
                         学习率=学习率，
                         n_估计器=n_估计器，
                         min_child_samples=min_child_samples,
                         子样本=子样本，
                         colsample_bytree=colsample_bytree,
                         reg_alpha=reg_alpha,
                         reg_lambda=reg_lambda
                         ）
    分数 = cross_val_score(lgb_clf, train_x, train_y, n_jobs=-1, cv=KFold(n_splits=10, shuffle=True, random_state=1), 评分=&#39;准确度&#39;)
    mean_score = 分数.mean()
    返回平均值
lgb_study = optuna.create_study(方向=“最大化”)
lgb_study.optimize(objective_lgb, n_Trials=1500)

lgb_试验 = lgb_study.best_试验
print(“准确度：”, lgb_Trial.value)
打印（）
print(&quot;最佳参数：&quot;, lgb_Trial.params)
=================================================== =======
def light_check(x,params):
     模型 = lgb.LGBMClassifier()
     分数 = cross_val_score(模型,x,y,cv=KFold(n_splits=10, shuffle=True, random_state=1),n_jobs=-1)
     平均值 = 分数.mean()
     返回分数，平均值
light_check(x,{&#39;num_leaves&#39;: 230, &#39;max_depth&#39;: 53, &#39;learning_rate&#39;: 0.04037430031226232, &#39;n_estimators&#39;: 1143, &#39;min_child_samples&#39;: 381, &#39;子样本&#39;: 0.12985990464862135, &#39;colsample_bytree&#39;: 0.8914118949904919, &#39;reg_alpha&#39; : 31.869348047391053, &#39;reg_lambda&#39;: 17.45653692887209, &#39;importance_type&#39;: &#39;分割&#39;})
]]></description>
      <guid>https://stackoverflow.com/questions/71778333/optuna-score-vs-cross-val-score</guid>
      <pubDate>Thu, 07 Apr 2022 07:56:04 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn cross_val_score 给出的数字与 model.score 明显不同？</title>
      <link>https://stackoverflow.com/questions/61937500/sklearn-cross-val-score-gives-significantly-different-number-than-model-score</link>
      <description><![CDATA[我有一个二元分类问题
首先，我将数据训练测试分割为：
X_train、X_test、y_train、y_test = train_test_split(X、y、random_state=42)

我检查了 y_train，它基本上有两个类 (1,0) 的 50/50 分割，这就是数据集的方式
当我尝试基本模型时，例如：
模型 = RandomForestClassifier()
model.fit(X_train, y_train)
model.score(X_train, y_train)

输出为 0.98 或 1% 的差异，具体取决于训练测试分割的随机状态。 
但是，当我尝试 cross_val_score 时，例如：
cross_val_score(model, X_train, y_train, cv=StratifiedKFold(shuffle=True), rating=&#39;accuracy&#39;)

输出为
数组([0.65, 0.78333333, 0.78333333, 0.66666667, 0.76666667])

数组中没有一个分数接近 0.98？
当我尝试得分 = &#39;r2&#39; 时，我得到了
&gt;&gt;&gt;&gt;cross_val_score(model, X_train, y_train, cv=StratifiedKFold(shuffle=True), rating=&#39;r2&#39;)
数组([-0.20133482,-0.00111235,-0.2,-0.2,-0.13333333])

有谁知道为什么会这样吗？我尝试过 Shuffle = True 和 False 但没有帮助。
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/61937500/sklearn-cross-val-score-gives-significantly-different-number-than-model-score</guid>
      <pubDate>Thu, 21 May 2020 15:04:38 GMT</pubDate>
    </item>
    <item>
      <title>我是给 cross_val_score() 整个数据集还是只提供训练集？</title>
      <link>https://stackoverflow.com/questions/52249158/do-i-give-cross-val-score-the-entire-dataset-or-just-the-training-set</link>
      <description><![CDATA[由于该类的文档不是很清楚。我不明白我赋予它什么价值。

&lt;块引用&gt;
  cross_val_score（估计器，X，y=无）

这是我的代码：
clf = LinearSVC(random_state=seed, **params)
cvscore = cross_val_score(clf, 特征, 标签)

我不确定这是否正确，或者我是否需要提供 X_train 和 y_train 而不是特征和标签。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/52249158/do-i-give-cross-val-score-the-entire-dataset-or-just-the-training-set</guid>
      <pubDate>Sun, 09 Sep 2018 22:39:32 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中参数、特征和类之间的区别</title>
      <link>https://stackoverflow.com/questions/35819869/difference-between-parameters-features-and-class-in-machine-learning</link>
      <description><![CDATA[我是机器学习和自然语言处理方面的新手。
我总是对这三个术语感到困惑？
据我了解：
class：我们的模型输出的各种类别。给出一个人的名字，确定他/她是男性还是女性？
假设我正在使用朴素贝叶斯分类器。
我的功能和参数是什么？
此外，上述单词的一些可互换使用的别名是什么？]]></description>
      <guid>https://stackoverflow.com/questions/35819869/difference-between-parameters-features-and-class-in-machine-learning</guid>
      <pubDate>Sat, 05 Mar 2016 21:02:05 GMT</pubDate>
    </item>
    </channel>
</rss>