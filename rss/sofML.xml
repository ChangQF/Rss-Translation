<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 13 Sep 2024 06:23:39 GMT</lastBuildDate>
    <item>
      <title>AttributeError：'Dense' 对象在 TensorFlow/Keras 的 ann_visualizer 中没有属性 'output_shape'</title>
      <link>https://stackoverflow.com/questions/78980852/attributeerror-dense-object-has-no-attribute-output-shape-in-ann-visualizer</link>
      <description><![CDATA[`我有这个错误
Epoch 1/2
1/1 ━━━━━━━━━━━━━━━━━━━━━ 0s 391ms/step - 损失：6272135168.0000
Epoch 2/2
1/1 ━━━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - 损失：6272133632.0000
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step
均方误差：7431671762.639743
回溯（最近一次调用）：
文件 ~\anaconda3\Lib\site-packages\spyder_kernels\py3compat.py:356 in compat_exec
exec(code, globals, locals)
文件 c:\users\mouli.spyder-py3\temp.py:27
ann_viz(model,title=&#39;Linear Regression&#39;)
文件~\anaconda3\Lib\site-packages\ann_visualizer\visualize.py:42 in ann_viz
input_layer = int(str(layer.input_shape).split(&quot;,&quot;)[1][1:-1]);
AttributeError: &#39;Dense&#39; 对象没有属性 &#39;input_shape&#39;
为此
`
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import pandas as pd`
`# 导入数据集
dataset = pd.read_csv(r&quot;Salary_Data (1).csv&quot;)
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = keras.Sequential([
keras.layers.Input(shape=(1,)),
keras.layers.Dense`(1)
])`
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)
model.fit(`your text`X_train, y_train, epochs=2, batch_size=32, verbose=1)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f&quot;均方误差：{mse}&quot;)`

# 可视化数据和回归线

#可视化神经网络
来自 ann_visualizer.visualize 导入 ann_viz
来自 graphviz `your text`导入 Source
ann_viz(model,title=&#39;线性回归&#39;)
graph_source=Source.from_file(&#39;network.gv&#39;)

]]></description>
      <guid>https://stackoverflow.com/questions/78980852/attributeerror-dense-object-has-no-attribute-output-shape-in-ann-visualizer</guid>
      <pubDate>Fri, 13 Sep 2024 06:14:36 GMT</pubDate>
    </item>
    <item>
      <title>我如何创建一个模型来根据过去的选择预测会选择哪个选项？</title>
      <link>https://stackoverflow.com/questions/78980646/how-do-i-create-a-model-to-predict-which-option-will-be-chosen-based-on-past-cho</link>
      <description><![CDATA[我有一个数据集，其中包含一系列涉及用户选择的购买。每个数据集包括：

1 个或多个可供购买的商品（商品 + 价格 + 每个商品的详细信息）
用户从可用商品中选择了哪个商品

通常，只有一件商品可供购买，但有时会有多个。对于有多个商品的情况，我希望建立一个可以预测将购买哪些商品的模型。我对预测型算法（和机器学习）还不熟悉，因此欢迎提出建议。]]></description>
      <guid>https://stackoverflow.com/questions/78980646/how-do-i-create-a-model-to-predict-which-option-will-be-chosen-based-on-past-cho</guid>
      <pubDate>Fri, 13 Sep 2024 04:40:07 GMT</pubDate>
    </item>
    <item>
      <title>加载照片时“路径应该是路径类或 io.BytesIO，而不是 <class 'PIL.JpegImagePlugin.JpegImageFile'>”</title>
      <link>https://stackoverflow.com/questions/78980063/during-loading-photo-path-should-be-path-like-or-io-bytesio-not-class-pil-j</link>
      <description><![CDATA[我尝试将照片加载到 ML 项目中，但出现错误：路径应为路径类或 io.BytesIO，而不是 &lt;class &#39;PIL.JpegImagePlugin.JpegImageFile&#39;&gt;
def load_image_pixels(filename, shape):
# 加载图像以获取其形状
image = load_img(filename)
width, height = image.size
# 加载具有所需大小的图像
image = load_img(filename, target_size=shape)
# 转换为 numpy 数组
image = img_to_array(image)
# 将像素值缩放到 [0, 1]
image = image.astype(&#39;float32&#39;)
image /= 255.0
# 添加一个维度，以便我们有一个样本
image = expand_dims(image, 0)
return image, width, height

photo_filename = Image.open(r&#39;C:\ML\Pies.jpg&#39;)
# 定义模型的预期输入形状
input_w, input_h = 167, 221
# 加载并准备图像
image, image_w, image_h = load_image_pixels(photo_filename, (net_w, net_w))

我看到照片已加载，但函数无法处理。如何解决这个问题。
在我使用的其他代码中，以这种方式打开的图像可以像将图像转换为数组然后处理它一样进行处理。
image = Image.open(r&#39;C:\ML\Pies.jpg&#39;)
image = np.array(image)
]]></description>
      <guid>https://stackoverflow.com/questions/78980063/during-loading-photo-path-should-be-path-like-or-io-bytesio-not-class-pil-j</guid>
      <pubDate>Thu, 12 Sep 2024 21:57:11 GMT</pubDate>
    </item>
    <item>
      <title>线性模型中的规范化</title>
      <link>https://stackoverflow.com/questions/78979243/normalization-in-linear-models</link>
      <description><![CDATA[我知道对于线性模型来说，有必要对所有数据进行归一化。但是如果我们得到一个预测，它也将被归一化。我怎样才能在原始尺度上得到这个预测？
例如，如果我进行标准化（减去平均值并除以方差），我认为当我得到模型预测（假设为 X）时，我可以执行 X_original = X * 方差 + 平均值。但是有没有内置的方法可以做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78979243/normalization-in-linear-models</guid>
      <pubDate>Thu, 12 Sep 2024 17:03:00 GMT</pubDate>
    </item>
    <item>
      <title>使用电子邮件 ID 识别垃圾邮件用户和真正用户 [关闭]</title>
      <link>https://stackoverflow.com/questions/78976869/identifying-spam-users-and-genuine-users-using-email-ids</link>
      <description><![CDATA[我正在开发一种机器学习算法，用于检测电子邮件 ID 是真是假。我已经探索了一些可以帮助我区分两者的功能。我在网上搜索了相关的研究论文，以便了解真实用户邮件 ID 中的字符估计数量、预期的字母与数字的比率等，但找不到。有人能帮我吗？
我探索了一些可以使用的功能。我在下面列出了这些功能-
1.用户名的长度：用户名部分（&#39;@&#39; 符号之前）的字符数。真正的电子邮件 ID 可能遵循一定的长度模式，可能与假冒电子邮件 ID 不同。
2.特殊字符数：电子邮件 ID 中特殊字符的数量，例如点、下划线和连字符。总计数和用户名或域内的计数都可以提供深刻的见解。
3.域名：电子邮件的域名（&#39;@&#39;符号后）可以是一个强有力的指标，特别是如果您可以识别通常与临时或一次性电子邮件服务相关的域名。
4.顶级域名（TLD）：顶级域名（.com、.org、.net 等）也可能提供线索，因为某些 TLD 可能在真实或虚假的电子邮件 ID 中更为普遍。
5.数字与字母的比率：用户名部分的数字与字母的比率。假冒电子邮件 ID 的比率可能高于或低于真实电子邮件 ID。
6.连续字符的存在：连续相同字符的存在和数量，这可能在随机生成的（假）电子邮件 ID 中更常见。
7.域名使用频率：如果您有电子邮件 ID 数据集，每个域名出现的频率可能有助于识别不太常见、可能可疑的域名。对于这些，您是否对真实用户和虚假用户进行了有效的研究，或者您可以给我一篇文章的链接。]]></description>
      <guid>https://stackoverflow.com/questions/78976869/identifying-spam-users-and-genuine-users-using-email-ids</guid>
      <pubDate>Thu, 12 Sep 2024 07:05:16 GMT</pubDate>
    </item>
    <item>
      <title>建议一些可微调的预训练对象检测模型以使用自定义数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/78976857/suggest-some-fine-tunable-pretrained-object-detection-model-for-using-custom-dat</link>
      <description><![CDATA[我需要找到一个用于识别家庭内物体的物体检测模型。该模型应该经过预先训练，并且应该可以针对家居物品进行微调。您能推荐一些好的模型吗？
我尝试了 Yolov10、EfficientDET 和 RetinaNet。对它们的结果不满意。]]></description>
      <guid>https://stackoverflow.com/questions/78976857/suggest-some-fine-tunable-pretrained-object-detection-model-for-using-custom-dat</guid>
      <pubDate>Thu, 12 Sep 2024 07:02:43 GMT</pubDate>
    </item>
    <item>
      <title>可以对研究论文进行情绪分析吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78976695/can-a-sentiment-analysis-on-research-paper-be-done</link>
      <description><![CDATA[我开始开发一个情感分析工具，它将分析研究论文中表达的情感，尽管研究论文中写的语言非常微妙和温和。
我正在使用各种在线资源，但仍然很难达到我想要的输出。
我正在使用预先训练的 BERT 模型，因为我已经决定将其规模非常小，这是我大学最后一年的项目。
请分享任何有用的见解、经验和知识。
请分享任何有用的见解、经验和知识。]]></description>
      <guid>https://stackoverflow.com/questions/78976695/can-a-sentiment-analysis-on-research-paper-be-done</guid>
      <pubDate>Thu, 12 Sep 2024 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>在 ML Studio 中使用本地计算机作为 Azure AutoML 的计算目标</title>
      <link>https://stackoverflow.com/questions/78976497/using-local-computer-as-a-compute-target-for-azure-automl-in-ml-studio</link>
      <description><![CDATA[是否可以将本地计算机作为计算目标连接到 Azure 机器学习工作区，并将其用作 Azure ML Studio 中 AutoML 的计算目标，而不是使用计算群集？我尝试浏览文档，但找不到任何有用的东西。有人可以指导我完成这个过程或分享代码片段/任何相关文档吗？]]></description>
      <guid>https://stackoverflow.com/questions/78976497/using-local-computer-as-a-compute-target-for-azure-automl-in-ml-studio</guid>
      <pubDate>Thu, 12 Sep 2024 04:44:02 GMT</pubDate>
    </item>
    <item>
      <title>测量肩角[关闭]</title>
      <link>https://stackoverflow.com/questions/78976329/measuring-shoulder-angle</link>
      <description><![CDATA[https://i.sstatic.net/tC8S69ry.jpg
我必须用 Python 测量每个肩膀的角度。什么是以最高精度测量的最佳方法？我对此很陌生，所以欢迎提出任何建议。
我尝试了 Media pipe，但它只提供骨架点。我需要测量肩部边缘的角度。（如图所示）]]></description>
      <guid>https://stackoverflow.com/questions/78976329/measuring-shoulder-angle</guid>
      <pubDate>Thu, 12 Sep 2024 03:05:15 GMT</pubDate>
    </item>
    <item>
      <title>尽管分类报告很好，但模型无法正确预测</title>
      <link>https://stackoverflow.com/questions/78976316/model-cant-predict-correctly-even-though-has-a-good-classification-report</link>
      <description><![CDATA[我尝试从链接运行此模型：
https://www.kaggle.com/code/alexfordna/garbage-classification-mobilenetv2-92-accuracy/notebook
当我在 colab 上使用类似数据集（但较小，2100 张图片到 6 个类）执行此操作时，效果很好。但是当我添加此代码来预测输入图像时：
from google.colab import files
from PIL import Image

def process_uploaded_image(image_path, target_size=(224, 224)):
img = Image.open(image_path)
img = img.resize(target_size) 
img_array = np.array(img) 

if img_array.shape[-1] == 4: 
img_array = img_array[..., :3]

img_array = img_array / 255.0 
img_array = np.expand_dims(img_array, axis=0) 
img_array = mobilenetv2.preprocess_input(img_array) 

return img_array

uploaded = files.upload()

for fn in uploaded.keys(): 
processed_image = process_uploaded_image(fn, target_size=IMAGE_SIZE) 
preds = model.predict(processed_image)
pred_class = np.argmax(preds, axis=1)

plt.imshow(Image.open(fn)) # 显示上传的图片
plt.title(f&#39;预测的类别：{categories[pred_class[0]]}&#39;)
plt.axis(&#39;off&#39;)
plt.show()
print(f&#39;文件 {fn} 被预测为：{categories[pred_class[0]]}&#39;)

结果是错误的预测。例如，模型总是将我的输入预测为“垃圾”类。当我停止运行时，它会更改为另一个类，但它仍然处于错误的预测中。
我还添加了此代码来检查预测概率：
preds = model.predict(processed_image)
pred_probs = preds[0] # 获取第一个（也是唯一一个）批次的预测概率
print(&quot;Prediction probabilities:&quot;, pred_probs)
pred_class = np.argmax(pred_probs)
print(&quot;Predicted class:&quot;, categories[pred_class])

输出：
**1/1** ━━━━━━━━━━━━━━━━━━━━━━ **0s** 24ms/步 预测概率：\[0.31027108 0.12315894 0.47848797 0.00863316 0.07789086 0.00155797\] 
预测类别：金属 

为什么会发生这种情况，我的模型如何正确预测结果？]]></description>
      <guid>https://stackoverflow.com/questions/78976316/model-cant-predict-correctly-even-though-has-a-good-classification-report</guid>
      <pubDate>Thu, 12 Sep 2024 02:58:40 GMT</pubDate>
    </item>
    <item>
      <title>在 Azure 中创建数据资产但出现此错误</title>
      <link>https://stackoverflow.com/questions/78976058/creating-a-data-asset-in-azure-but-getting-this-error</link>
      <description><![CDATA[我正在尝试创建 ML 数据资产，我正在关注此链接：https://microsoftlearning.github.io/mslearn-ai-fundamentals/Instructions/Labs/01-machine-learning.html
以完成此操作，但我不断收到此错误

我知道我的文件与示例中的文件不完全相同，但即使我使用这些文件，我也会收到相同的错误。所以我不确定问题是什么，因为这是我第一次在 Azure 中使用 ML。]]></description>
      <guid>https://stackoverflow.com/questions/78976058/creating-a-data-asset-in-azure-but-getting-this-error</guid>
      <pubDate>Thu, 12 Sep 2024 00:30:32 GMT</pubDate>
    </item>
    <item>
      <title>从测量位置数据集中分离系统误差和随机性</title>
      <link>https://stackoverflow.com/questions/78975875/separating-systematic-errors-and-randomness-from-a-measured-position-dataset</link>
      <description><![CDATA[我正在尝试找出一种方法，将系统误差从一组数据集中分离出来，这些数据集表示机械平台的位置和位置误差。数据在 pandas 数据框中。
背景：平台在 2D 平面中移动。测量报告平台位置的 X 和 Y 坐标以及 X 和 Y 方向的位置误差。2D 平面上有一个相距 1nm 的标记网格，平台平稳移动到这些标记，并且仅在这些标记处进行测量。通过算法减去这些标记的真实位置和测量的平台位置来计算误差。
数据格式：这里的倾斜表示误差。



StageCoords_X
StageCoords_Y
Skew_X
Skew_Y




118760606
112836409
-29
-45


118760622
112836426
-18
5



**数据预处理：**我正在使用 nm 尺度进行工作，每次我扫描相同的“标记”位置时，测量的舞台位置都会在这些“标记”位置周围略有不同。这就是为什么我想在 X 和 Y 方向上对数据进行分类，以便每个矩形箱（结合 X 和 Y 轴箱宽度）将覆盖用于特定标记的数据点。我可以计算每个箱的平均和峰峰值误差并绘制它们。
# 根据箱宽度定义箱边界
bin_width = 1000000
x_bins = np.arange(df[&#39;StageCoordsNM.X&#39;].min(), df[&#39;StageCoordsNM.X&#39;].max() + bin_width, bin_width)
y_bins = np.arange(df[&#39;StageCoordsNM.Y&#39;].min(), df[&#39;StageCoordsNM.Y&#39;].max() + bin_width, bin_width)

df[&#39;X_bin&#39;] = pd.cut(df[&#39;StageCoordsNM.X&#39;], bins=x_bins, labels=False)
df[&#39;Y_bin&#39;] = pd.cut(df[&#39;StageCoordsNM.Y&#39;], bins=y_bins, labels=False)

我想要做的：现在假设，我已经将舞台扫过同一区域 25 次。在 x 和 y 方向上进行分箱后，每个箱将有 25 个数据点，意味着每个箱有 25 个测量舞台位置误差。现在对于每个箱，我想提取误差的系统或可重复部分。每个箱 25 个数据点可能不足以得出任何可靠的结论。因此使用机器学习很困难。我想找出一种更具统计性的方法来做到这一点。
我到目前为止所做的：计算了“归一化加权调整重复性指数”。这应该表明我对特定分箱误差是否可重复的信心。忽略 &#39;_before&#39; 下标。
#*__Pk-to-Pk X 和 Y__
df[&#39;Error_pk2pk_X&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.X&#39;].transform(lambda x: x.max() - x.min())
df[&#39;Error_pk2pk_Y&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.Y&#39;].transform(lambda x: x.max() - x.min())

df[&#39;Mean_SkewNM_X_before&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.X&#39;].transform(lambda x: x.mean())
df[&#39;Mean_SkewNM_Y_before&#39;] = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;])[&#39;SkewNM.Y&#39;].transform(lambda x: x.mean())

def compute_confidence_X_before(group):
group = group.reset_index(drop=True)
group[&#39;WARI_x_before&#39;] = (w_a * np.abs(group[&#39;Mean_SkewNM_X_before&#39;].mean()) ) / ( w_p * (np.abs(group[&#39;Error_pk2pk_X&#39;].mean()) + epsilon) )
group[&#39;NWARI_x_before&#39;] = (group[&#39;WARI_x_before&#39;]) / (1+group[&#39;WARI_x_before&#39;])

group[&#39;Confidence_X_before&#39;] = ( np.abs(group[&#39;Mean_SkewNM_X_before&#39;].mean()) - np.abs(group[&#39;Error_pk2pk_X&#39;].mean()) ) / np.abs(group[&#39;SkewNM.X&#39;].mean())
group[&#39;Confidence_X_before&#39;] = group[&#39;Confidence_X_before&#39;].apply(lambda x: max(x, -1))
return group

def compute_confidence_Y_before(group):
group = group.reset_index(drop=True)
group[&#39;WARI_y_before&#39;] = (w_a * np.abs(group[&#39;Mean_SkewNM_Y_before&#39;].mean()) ) / ( w_p * (np.abs(group[&#39;Error_pk2pk_Y&#39;].mean()) + epsilon) )
group[&#39;NWARI_y_before&#39;] = (group[&#39;WARI_y_before&#39;]) / (1+group[&#39;WARI_y_before&#39;])

group[&#39;Confidence_Y_before&#39;] = ( np.abs(group[&#39;Mean_SkewNM_Y_before&#39;].mean()) - np.abs(group[&#39;Error_pk2pk_Y&#39;].mean()) ) / np.abs(group[&#39;SkewNM.Y&#39;].mean())
group[&#39;Confidence_Y_before&#39;] = group[&#39;Confidence_Y_before&#39;].apply(lambda x: max(x, -1))
返回组

df = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;], group_keys=False).apply(compute_confidence_X_before)
df = df.groupby([&#39;X_bin&#39;, &#39;Y_bin&#39;], group_keys=False).apply(compute_confidence_Y_before)
df[&#39;NWARI_X_before&#39;] = df[&#39;NWARI_x_before&#39;]
df[&#39;NWARI_Y_before&#39;] = df[&#39;NWARI_y_before&#39;]

但我不确定这是否是正确的方法。或者是否有其他方法可以验证这一点。
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78975875/separating-systematic-errors-and-randomness-from-a-measured-position-dataset</guid>
      <pubDate>Wed, 11 Sep 2024 22:23:55 GMT</pubDate>
    </item>
    <item>
      <title>无监督图像聚类：无法获得正确结果</title>
      <link>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</link>
      <description><![CDATA[我正在开展一个个人项目，该项目采用一组图像（金属螺母）并确定是否存在缺陷（着色、划痕、弯曲、翻转和良好）。
我使用 VGG16 模型提取特征，使用 PCA 降低维数，然后将降维后的特征输入到简单的 k 均值算法（k=5）中以识别聚类。
我遇到的问题归结为：从模型中提取的特征对于解决手头的问题并不是很有效。
更具体地说，如果我想识别特定的“翻转”金属螺母（只是制造时齿朝向错误的螺母），提取的特征确实很有效。因此，集群最终是 4 个随机集，然后是 1 组刚翻转的螺母。
我的问题是，我可以做些什么来修改我的模型/提取的特征，使它们更适合我的问题（识别所有 5 个类别的金属螺母）？我甚至很高兴能够从“有缺陷”中识别出“好”的螺母。
我尝试过的事情：

在“好”图像的训练集上训练模型（即只是普通的金属螺母）
从模型的较早层（第 10 层）而不是倒数第二层获取输出
使用不同的模型（我最初使用的是 ResNet18）
]]></description>
      <guid>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</guid>
      <pubDate>Wed, 11 Sep 2024 19:16:38 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenCV 改进这种图像自然背景扩展方法？</title>
      <link>https://stackoverflow.com/questions/78969286/how-can-i-improve-this-approach-for-natural-background-extension-in-an-image-usi</link>
      <description><![CDATA[我正在使用 Python 中的 OpenCV 扩展图像的背景。我目前的方法是复制边框并对扩展区域应用高斯模糊以将它们混合到原始图像中。目标是使背景扩展看起来更自然，尤其是对于具有一致纹理的图像。
这是我当前使用的代码：
import cv2
import numpy as np

def expand_image_with_smart_blend(image_path, top=50, bottom=50, left=50, right=50):
img = cv2.imread(image_path)
original_h, original_w = img.shape[:2]

expanded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_REPLICATE)

blured_img = expand_img.copy()

if top &gt; 0:
blured_img[0:top, :] = cv2.GaussianBlur(expanded_img[0:top, :], (51, 51), 0)

如果底部 &gt; 0:
blured_img[original_h + top:original_h + top + bottom, :] = cv2.GaussianBlur(expanded_img[original_h + top:original_h + top + bottom, :], (51, 51), 0)

如果左侧 &gt; 0:
blured_img[:, 0:left] = cv2.GaussianBlur(expanded_img[:, 0:left], (51, 51), 0)

如果右侧 &gt; 0:
blured_img[:, original_w + left:original_w + left + right] = cv2.GaussianBlur(expanded_img[:, original_w + left:original_w + left + right], (51, 51), 0)

cv2.namedWindow(&quot;智能混合扩展图像&quot;, cv2.WINDOW_NORMAL)
cv2.namedWindow(&quot;原始图像&quot;, cv2.WINDOW_NORMAL)
cv2.imwrite(&#39;expanded_smart_blended_image.jpg&#39;, blured_img)
cv2.imshow(&#39;智能混合扩展图像&#39;, blured_img)
cv2.imshow(&quot;原始图像&quot;, img)
cv2.waitKey(0)
cv2.destroyAllWindows()

expand_image_with_smart_blend(&#39;test_img.jpg&#39;, top=100, bottom=100, left=100, right=100)

我尝试过的方法：
cv2.BORDER_REPLICATE：我使用它将原始图像的边缘复制到新扩展的区域中。
高斯模糊：应用于扩展区域以柔化原始图像和新区域之间的过渡。
问题：
结果在某种程度上是可以接受的，但过渡仍然看起来不像我想要的那样自然。特别是：
某些区域的过度模糊使背景看起来不真实。
对于纹理更复杂的图像，边缘复制并不总是有效。
原始图像 结果图像
问题：
在 OpenCV 或其他库中，是否有更复杂的方法来扩展图像的背景，从而产生更自然、无缝的结果？我愿意接受涉及高级图像处理技术或机器学习的方法。任何使用扩散模型的方法都可以。]]></description>
      <guid>https://stackoverflow.com/questions/78969286/how-can-i-improve-this-approach-for-natural-background-extension-in-an-image-usi</guid>
      <pubDate>Tue, 10 Sep 2024 11:32:09 GMT</pubDate>
    </item>
    <item>
      <title>MS Azure autoML 准备中出现错误 - 文件格式/编码错误？</title>
      <link>https://stackoverflow.com/questions/57096415/error-in-the-ms-azure-automl-preparation-wrong-file-format-encoding</link>
      <description><![CDATA[我正尝试按照以下 Github 示例部署 MS Azure 自动机器学习：
https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning/classification-bank-marketing
我更改了那里的代码以向其提供我的数据，但在执行 autoML 运行时出现以下错误：
automl.client.core.common.exceptions.DataprepException：无法执行指定的转换。

来自：
文件“/azureml-envs/azureml_e9e27206cd19de471f4e5c7a1171037e/lib/python3.6/site-packages/azureml/automl/core/dataprep_utilities.py”，第 50 行，在 try_retrieve_pandas_dataframe_adb 中

现在，我认为我的数据有问题，但随后我使用原始 csv 文件执行了以下实验：
第一次执行与 Github 示例相同，直接基于 http 链接构建数据流
第二次执行基于相同的 csv 构建数据流，但下载到我的共享。
在第二种情况下，我得到了与我的数据相同的错误。这意味着 Azure autoML 运行/数据流/准备过程仅接受特定文件格式，该格式在保存到我的驱动器时发生了更改。
我不确定这是否与编码或其他任何内容有关。
您能提供建议吗？
###########################################
#Case 1，返回错误

data= &quot;\\\dwdf219\\...\\bankmarketing_train.csv&quot;
dflow = dprep.auto_read_file(data)
dflow.get_profile()
X_train = dflow.drop_columns(columns=[&#39;y&#39;])
y_train = dflow.keep_columns(columns=[&#39;y&#39;],validate_column_exists=True)
dflow.head()

# 训练
automl_settings = {
&quot;iteration_timeout_minutes&quot;: 10,
&quot;iterations&quot;: 5,
&quot;n_cross_validations&quot;: 2,
&quot;primary_metric&quot;: &#39;AUC_weighted&#39;,
&quot;preprocess&quot;: True,
&quot;max_concurrent_iterations&quot;: 5,
&quot;verbosity&quot;:logging.INFO,
}

automl_config = AutoMLConfig(task = &#39;classification&#39;,
debug_log = &#39;automl_errors.log&#39;,
path = project_folder,
run_configuration=conda_run_config,
X = X_train,
y = y_train,
**automl_settings
) 

remote_run = experiment.submit(automl_config, show_output = True)

###########################################
#Case 2，一切正常

data = &quot;https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv&quot;
dflow = dprep.auto_read_file(data)
dflow.get_profile()
X_train = dflow.drop_columns(columns=[&#39;y&#39;])
y_train = dflow.keep_columns(columns=[&#39;y&#39;],validate_column_exists=True)
dflow.head()

# 训练...
##################################### 
]]></description>
      <guid>https://stackoverflow.com/questions/57096415/error-in-the-ms-azure-automl-preparation-wrong-file-format-encoding</guid>
      <pubDate>Thu, 18 Jul 2019 14:04:38 GMT</pubDate>
    </item>
    </channel>
</rss>