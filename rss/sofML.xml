<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 03 Jul 2024 01:05:25 GMT</lastBuildDate>
    <item>
      <title>如何利用小型神经网络高效地做出大量预测？</title>
      <link>https://stackoverflow.com/questions/78699009/how-to-efficiently-make-a-lot-of-predictions-with-small-neural-network</link>
      <description><![CDATA[我需要用小型神经网络（100-150 个参数）进行大量预测。我在 TensorFlow 中实现了它，但遇到了效率问题。这是伪代码：
for my_dense_netowrk,my_lstm_netowrk in networks_list
my_dense_netowrk.paramters = 100
my_lstm_netowrk.paramters = 150
for images in data[:60]:
@tf.function
def tf_wrapper(images, state):
model_data = meta_model(images)
data_prepared = image_preparation(model_data)

results = my_dense_netowrk(data_prepared)
results.shape = (19000,1,1)

better_results, state = my_lstm_netowrk(results, state)
return better_results, state

better_results, state = tf_wrapper(images, state)

my_dense_netowrk_n2.paramters = 100
my_lstm_netowrk_n2.paramters = 100

并继续...


我使用 tensorflow 数据管道 api，实际上所有必需的数据（数据变量）都可以分配到我的内存中。
在构建和将数据作为一大堆（批处理大小为 19000）插入神经网络以并行化所有内容时，我没有为我的神经网络指定批处理大小。即使是 lstm 也不会受到序列处理的瓶颈，因为它必须一次处理 19000 个输入。但是当我将神经网络参数增加 10 倍（我不需要）时，我的代码几乎没有注意到它认为批处理大小相当大。
@tf.fcuntion 加快了一切速度。
我尝试了分析，但由于发生了太多操作，未能找到瓶颈。我发现内核启动只花费了一半的时间，因为通常 tensorflow 预计这个过程会花费大量时间，所以我猜它没有针对此类任务进行优化，因为当我将循环从 60 增加到 6000 时，每次循环的效率都会提高 10 倍！似乎需要时间进行准备。
image_preparation() 函数仅使用 tf 操作，如重塑、堆叠、平铺，我无法提前准备数据。
我使用带有 M3 Max 芯片的 macOS，使用 GPU 或 CPU 没有区别。我尝试了 python 3.8、3.9、3.10、3.11、3.12。

因此，似乎 tensorflow 并没有被我的模型所限制，这很奇怪，而且互联网上没有太多关于如何有效地从小模型中获得大量预测的讨论，每个人都将这样的库用于大型 NN。虽然我认为我的管道应该会从中受益，因为我使用了大批量，但 gpu 根本没有帮助。所以我真的很难找到一个好的解决方案来解决我的问题，并想寻求建议。也许有更好的 ml 框架可以解决我的问题（PyTorch、Jax，也许还有其他？）或者我只是不擅长分析？或者我应该尝试用汇编语言构建自己的内核吗？我不知道]]></description>
      <guid>https://stackoverflow.com/questions/78699009/how-to-efficiently-make-a-lot-of-predictions-with-small-neural-network</guid>
      <pubDate>Tue, 02 Jul 2024 19:52:03 GMT</pubDate>
    </item>
    <item>
      <title>机器过期域名 gnews</title>
      <link>https://stackoverflow.com/questions/78698880/machine-expired-domains-gnews</link>
      <description><![CDATA[我需要创建一台机器，一个程序，通过网站 https://www.expireddomains.net/ 从过期域名列表中获取仅属于 google 新闻的域名。
我需要检测 2019 年 12 月之前属于 google 新闻的 google 新闻域名
我曾尝试手动检查过期域名以查看它们是否属于 Google 新闻，但此过程非常耗时且效率低下。我希望找到一种更自动化的方法来交叉引用过期域名及其在 2019 年 12 月之前的 Google 新闻状态。
但是，我还没有找到可以有效完成此任务的现有工具或方法。我需要一个可以自动化此过程的解决方案，从而节省我的时间并确保准确性。]]></description>
      <guid>https://stackoverflow.com/questions/78698880/machine-expired-domains-gnews</guid>
      <pubDate>Tue, 02 Jul 2024 19:15:05 GMT</pubDate>
    </item>
    <item>
      <title>我的感知器和 sklearn 感知器的区别</title>
      <link>https://stackoverflow.com/questions/78698399/difference-in-my-perceptron-and-sklearn-perceptron</link>
      <description><![CDATA[我从头开始编写感知器算法，并将训练后获得的权重与训练 sklearn 感知器模型后获得的权重进行比较。我相信 sklearn 模型将权重和偏差初始化为零向量，我选择学习率 eta0=1 来匹配我的感知器代码。 （注意：我的代码中的偏差是向量 w_b 中的最后一项）
我的代码：
def perceptron(X_train, y_train):
#将权重初始化为 0
w = np.zeros(len(X_train.columns))
b = 0
w_b = np.append(w, b)
while True:
misclassifications = 0 
for X , Y in zip(X_train.values, y_train.values):
X_i = np.append(X, 1)
if Y*(np.dot(X_i,w_b)) &lt;= 0:
w_b = w_b + Y*X_i
misclassifications += 1
if misclassifications == 0:
break
return w_b

w_b = perceptron(X_train, y_train)

结果：[-3. 6.7 -1. ]
sklearn 代码：
perceptron = Perceptron(max_iter=1000, eta0=1,random_state=42) 
perceptron.fit(X_train, y_train)

print(&quot;weights are&quot;,perceptron.coef_)
print(&quot;bias is&quot;,perceptron.intercept_)

结果：weights are [[-4.7 10.1]] bias is [-2.]
我期望权重相同，但事实并非如此。有什么线索可以解释原因吗？]]></description>
      <guid>https://stackoverflow.com/questions/78698399/difference-in-my-perceptron-and-sklearn-perceptron</guid>
      <pubDate>Tue, 02 Jul 2024 17:04:46 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中使用不同分辨率图像训练 DeepLabV3 的最佳实践</title>
      <link>https://stackoverflow.com/questions/78698316/best-practice-to-train-deeplabv3-with-different-resolution-images-in-pytorch</link>
      <description><![CDATA[我正在尝试在 COCO 2017 数据集 上训练 PyTorch 的 DeepLabV3 进行语义分割，但我不确定如何处理不同分辨率的图像。我知道 DeepLab 的架构可以毫无问题地处理它们，但由于它们的分辨率，我无法将它们分批堆叠。处理此问题的最佳做法是什么？我是否将它们调整为固定大小？我是否随机裁剪固定大小？我知道有很多解决方案可以解决此问题，但我真的不知道在语义分割训练的背景下最佳做法是什么。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78698316/best-practice-to-train-deeplabv3-with-different-resolution-images-in-pytorch</guid>
      <pubDate>Tue, 02 Jul 2024 16:39:00 GMT</pubDate>
    </item>
    <item>
      <title>chemprop：RuntimeError：在“目标”中检测到以下值：张量（[0，12]），但仅预期以下值[0，1]</title>
      <link>https://stackoverflow.com/questions/78698076/chemprop-runtimeerror-detected-the-following-values-in-target-tensor-0-1</link>
      <description><![CDATA[运行 Chemprop 脚本时出现运行时错误。
所有脚本均可在此处找到：
https://github.com/chemprop/chemprop/blob/main/examples/training.ipynb
我遇到错误的部分是：
trainer.fit(mpnn, train_loader, val_loader)
两个月前它还可以正常工作。
我已经更新了所有软件包]]></description>
      <guid>https://stackoverflow.com/questions/78698076/chemprop-runtimeerror-detected-the-following-values-in-target-tensor-0-1</guid>
      <pubDate>Tue, 02 Jul 2024 15:44:02 GMT</pubDate>
    </item>
    <item>
      <title>使用 OpenCV 和自适应阈值检测和掩盖图像中的花朵</title>
      <link>https://stackoverflow.com/questions/78697737/struggling-to-detect-and-mask-flowers-in-images-using-opencv-and-adaptive-thresh</link>
      <description><![CDATA[我正在做一个项目，需要使用 OpenCV 和自适应阈值检测和掩盖图像中的花朵。尽管我付出了努力，但结果并不一致。有些花朵被很好地掩盖了，但其他花朵要么被部分掩盖，要么根本检测不到。我在 TensorFlow 中使用 Oxford Flowers 102 数据集来完成这项任务。下面是我正在使用的代码：
(train_dataset, test_dataset, validation_dataset), ds_info = tfds.load(&#39;oxford_flowers102&#39;, split=[&#39;test&#39;, &#39;train&#39;,&#39;validation&#39;], with_info=True, as_supervised=True)

def normalize_img(image, label):
image = tf.image.resize(image, (256, 256))
return tf.cast(image, tf.float32) / 255.0, label

train_dataset = train_dataset.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
train_dataset = train_dataset.cache()
train_dataset = train_dataset.shuffle(ds_info.splits[&#39;train&#39;].num_examples)
train_dataset = train_dataset.batch(32)
train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)

def detect_flowers_and_mask(image):
image_np = (image.numpy() * 255).astype(np.uint8)
gray_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY) 
binary_image = cv2.adaptiveThreshold(gray_image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 2) 
contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL，cv2.CHAIN_APPROX_SIMPLE)

如果轮廓：
max_contour = max(contours，key=cv2.contourArea)
mask = np.zeros_like(gray_image，dtype=np.uint8)
cv2.drawContours(mask，[max_contour]，-1，(255，255，255)，厚度=cv2.FILLED)
masked_image = cv2.bitwise_and(image_np，image_np，mask=mask)
else:
masked_image = image_np

返回 masked_image

plt.figure(figsize=(15, 15)) 
for i, (image_batch, label_batch) in enumerate(train_dataset.take(20)):
for image, label in zip(image_batch, label_batch):
masked_image = detect_flowers_and_mask(image)
plt.subplot(4, 5, i+1)
plt.imshow(masked_image)
plt.title(ds_info.features[&#39;label&#39;].int2str(label.numpy())) 
plt.axis(&quot;off&quot;)
plt.tight_layout()
plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/78697737/struggling-to-detect-and-mask-flowers-in-images-using-opencv-and-adaptive-thresh</guid>
      <pubDate>Tue, 02 Jul 2024 14:34:33 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOV8 best.pt 文件的对象跟踪算法，该文件是在自定义数据集上训练的？[关闭]</title>
      <link>https://stackoverflow.com/questions/78696932/object-tracking-algo-with-yolov8-best-pt-file-which-is-trained-on-custom-dataset</link>
      <description><![CDATA[我已经使用 YOLOV8 在自定义数据集上训练了一个模型。从中获得了 best.pt 文件。我想使用一些跟踪器算法来跟踪该对象。有人可以给我推荐一些算法吗？也请附上一些链接，以便我可以参考。
此外，如何查看此组合模型的性能指标？
除了 DeepSORT，因为我已经尝试过了。]]></description>
      <guid>https://stackoverflow.com/questions/78696932/object-tracking-algo-with-yolov8-best-pt-file-which-is-trained-on-custom-dataset</guid>
      <pubDate>Tue, 02 Jul 2024 12:08:35 GMT</pubDate>
    </item>
    <item>
      <title>Pyspark 中的 RankingMetrics 未按预期工作</title>
      <link>https://stackoverflow.com/questions/78695397/rankingmetrics-in-pyspark-not-working-as-expected</link>
      <description><![CDATA[嗨，我有一个像这样的 pyspark df。
 (&#39;recs&#39;, &#39;array&lt;string&gt;&#39;),
(&#39;model_pred_score&#39;, &#39;array&lt;double&gt;&#39;),
(&#39;ground_truth_score&#39;, &#39;array&lt;double&gt;&#39;),
(&#39;model_ranked_items&#39;, &#39;array&lt;string&gt;&#39;),
(&#39;actual_ranked_items&#39;, &#39;array&lt;string&gt;&#39;)]

其中 recs 包含向客户推荐的商品列表，大小为 n，它将向客户推荐相关和不相关的商品。
model_pred_score 是模型预测的分数，为 int， ground_truth_score 是基于交互计算的浮点数，如果客户根本没有与项目交互，它也可以有 0。
现在我已经对 recs 进行了排序，这将为我们提供 model_ranked_items 和 actual_ranked_items，它们是根据其分数排名的项目。它们都将具有相等大小的 recs 数组列
现在我想计算这些项目的 NDCG 和 Precision，因此我像这样使用 Python 中的 RankingMetrics 库。
predictions_and_labels_rdd = df_grp.rdd.map(lambda row: (row[&quot;model_ranked_items&quot;], row[&quot;actual_ranked_items&quot;]))
ranking_metrics = RankingMetrics(predictions_and_labels_rdd)

ranking_metrics.ndcgAt(10)
ranking_metrics.precisionAt(10)

我不知道我在这里遗漏了什么，但我总是得到所有指标的 1。我后来还为 ndcg 添加了相关性列，但所有指标的得分仍然为 1。
我是否不应该将不相关的项目添加到 ground_truth_set？]]></description>
      <guid>https://stackoverflow.com/questions/78695397/rankingmetrics-in-pyspark-not-working-as-expected</guid>
      <pubDate>Tue, 02 Jul 2024 06:42:59 GMT</pubDate>
    </item>
    <item>
      <title>精确而强大的角点检测（噪声图像、脏污物体）</title>
      <link>https://stackoverflow.com/questions/78694749/precise-and-robust-corner-detection-noisy-image-dirty-object</link>
      <description><![CDATA[鉴于一定的质量控制要求，我们实施了一个自动化系统来测量钢板生产线的某些尺寸。问题是，有时系统不够强大，系统选择的像素不能反映我们人类推理认为的真实角落。该图像大约为 17 MPixels-
此简化的代码片段应代表我们的测量过程：
defcontrast_stretch(image, multiplier=1.0):
min_val = np.min(image)
max_val = np.max(image)
stretched = (image - min_val) * (255 / (max_val - min_val) * multiplier)
stretched = np.clip(stretched, 0, 255).astype(np.uint8)
returnstretched

def distance(pt1, pt2):
return math.sqrt((pt2[0] - pt1[0]) ** 2 + (pt2[1] - pt1[1]) ** 2)

defmeasure_diagonals(image, contours, px_to_mm):
refined_corners = []
for cnt in轮廓：
rect = cv2.minAreaRect(cnt)
box = cv2.boxPoints(rect)
box = np.int0(box)
corners = cv2.cornerSubPix(image, np.float32(box), (5, 5), (-1, -1), (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.1))
refined_corners.append(corners)
如果 len(refined_corners) &gt;= 2:
d1 = distance(refined_corners[0][0], refined_corners[0][2])
d2 = distance(refined_corners[1][0], refined_corners[1][2])
m1 = d1 * px_to_mm
m2 = d2 * px_to_mm
diff = abs(m2 - m1)
返回 m1, m2, diff
返回 None, None, None

# 加载校准数据
calibration_data = load_calibration_data(calibration_data_path)
mtx = calibration_data[&quot;mtx&quot;]
dist = calibration_data[&quot;dist&quot;]

# 加载图像
frame = cv2.imread(img_path)

# 不失真图像
frame_undistorted = cv2.undistort(frame, mtx, dist, None, mtx)

# 转换为灰度
gray = cv2.cvtColor(frame_undistorted, cv2.COLOR_BGR2GRAY)

# 应用双边滤波器
filtered_image = cv2.bilateralFilter(gray, 9, 125, 25)

# 增强对比度
enhanced_image =对比度拉伸（过滤后的图像）

# 检测轮廓
_, edge = cv2.threshold(enhanced_image, 140, 255, cv2.THRESH_BINARY)
contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 测量对角线
px_to_mm = 0.1 # 示例转换因子
m1, m2, diff = measure_diagonals(enhanced_image, contours, px_to_mm)

这是一个正确选择角的示例：
ROI 生成阈值（白点），然后是圆角子像素（黑点）：

这是一个错误选择的角落的例子：
红色部分是我们知道的真正角落

我知道我们应该改善照明。我们正在测量一个大面积（&gt;4 米），要有一个能产生明亮、均匀图像的照明系统极具挑战性，因此我们应用了大量软件校正，例如双边滤波器、增益和对比度增强器。]]></description>
      <guid>https://stackoverflow.com/questions/78694749/precise-and-robust-corner-detection-noisy-image-dirty-object</guid>
      <pubDate>Tue, 02 Jul 2024 01:35:14 GMT</pubDate>
    </item>
    <item>
      <title>HTML 文件输出未生成</title>
      <link>https://stackoverflow.com/questions/78694578/html-file-output-not-generating</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78694578/html-file-output-not-generating</guid>
      <pubDate>Mon, 01 Jul 2024 23:43:32 GMT</pubDate>
    </item>
    <item>
      <title>使用 SAM-LSTM-RESNET</title>
      <link>https://stackoverflow.com/questions/78694533/using-sam-lstm-resnet</link>
      <description><![CDATA[我使用的是 SAM-LSTM-RESNET 包，遇到了这个形状的问题。
包中附带的示例图片也出现了同样的问题，所以它不能正常工作，这很奇怪。有人知道如何在不修改包代码的情况下修复此问题吗？
from sam_lstm import SalMap
import os

if not os.path.exists(&quot;\\samples&quot;):
os.makedirs(&quot;\\samples&quot;)

SalMap.auto()

这是回溯
-------------------------------------------------------------------------------------------
ValueError Traceback (most recent call last)
Input In [5], in &lt;cell line: 9&gt;()
6 if not os.path.exists(&quot;\\samples&quot;):
7 os.makedirs(&quot;\\samples&quot;)
----&gt; 9 SalMap.auto()

文件 ~\anaconda3\lib\site-packages\sam_lstm\__init__.py:48，位于 SalMap.auto(cls)
45 @classmethod
46 def auto(cls):
47 salmap = cls()
---&gt; 48 salmap.compile()
49 salmap.load_weights()
50 salmap.predict_maps()

文件 ~\anaconda3\lib\site-packages\sam_lstm\__init__.py:59，位于 SalMap.compile(self)
57 def compile(self):
58 self.model = Model(
---&gt; 59 输入=[self.x, self.x_maps], 输出=sam_resnet([self.x, self.x_maps])
60 )
61 self.model.compile(
62 RMSprop(learning_rate=1e-4),
63 损失=[kl_divergence, correlation_coefficient, nss],
64 损失权重=[10, -2, -1],
65 )

文件~\anaconda3\lib\site-packages\sam_lstm\models.py:192，在 sam_resnet(x) 中
190 def sam_resnet(x):
191 # 扩张卷积网络
--&gt; 192 dcn = dcn_resnet(input_tensor=x[0])
193 conv_feat = Conv2D(512, (3, 3), padding=&quot;same&quot;,activation=&quot;relu&quot;)(dcn.output)
194 # 注意力卷积 LSTM

文件 ~\anaconda3\lib\site-packages\sam_lstm\dcn_resnet.py:259, 在 dcn_resnet(input_tensor) 中
252 # 加载权重
253 weights_path = get_file(
254 &quot;resnet50_weights_th_dim_ordering_th_kernels_notop.h5&quot;,
255 TH_WEIGHTS_PATH_NO_TOP,
256 cache_subdir=&quot;weights&quot;,
257 file_hash=&quot;f64f049c92468c9affcd44b0976cdafe&quot;,
258 )
--&gt; 259 model.load_weights(weights_path)
261 返回模型

文件 ~\anaconda3\lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 中引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 ~\anaconda3\lib\site-packages\keras\src\backend\common\variables.py:226，位于 KerasVariable.assign(self, value)
224 value = self._convert_to_tensor(value, dtype=self.dtype)
225 if not shape_equal(value.shape, self.shape):
--&gt; 226 raise ValueError(
227 “目标变量的形状和”
228 ““`variable.assign(value)` 中的目标值的形状必须匹配。”
229 “`variable.assign(value)` 中目标值的形状必须匹配。”
230 f“variable.shape={self.value.shape},”
231 f“收到：value.shape={value.shape}。”
232 f“目标变量：{self}”
233 )
234 if in_stateless_scope():
235 scope = get_stateless_scope()

ValueError: `variable.assign(value)` 中目标变量的形状和目标值的形状必须匹配。variable.shape=(7, 7, 3, 64), 收到：value.shape=(64, 3, 7, 7)。目标变量：&lt;KerasVariable shape=(7, 7, 3, 64), dtype=float32, path=conv1/kernel&gt;

你看这个错误基本上是形状以某种方式被转置了，但我不知道如何修复它？]]></description>
      <guid>https://stackoverflow.com/questions/78694533/using-sam-lstm-resnet</guid>
      <pubDate>Mon, 01 Jul 2024 23:22:27 GMT</pubDate>
    </item>
    <item>
      <title>如何在 macOS 10.12 上运行 Core ML 模型？</title>
      <link>https://stackoverflow.com/questions/78694076/how-can-one-run-a-core-ml-model-on-macos-10-12</link>
      <description><![CDATA[https://developer.apple.com/documentation/coreml 提到 macOS 10.13+：

如何在 macOS 10.12 上运行 Core ML 模型？

在 Ubuntu 20.04 上使用 Hugging Face 的 Exporters lib:
git clone https://github.com/huggingface/exporters.git
cd exporters
pip install -e .
python -m exporters.coreml --model=distilbert-base-uncasederated/ --quantize=float32 
]]></description>
      <guid>https://stackoverflow.com/questions/78694076/how-can-one-run-a-core-ml-model-on-macos-10-12</guid>
      <pubDate>Mon, 01 Jul 2024 20:13:24 GMT</pubDate>
    </item>
    <item>
      <title>使用嵌入技术从数据库中进行人脸识别</title>
      <link>https://stackoverflow.com/questions/78688976/face-recognize-from-the-database-using-embedding-technique</link>
      <description><![CDATA[我目前正在开展一个项目，旨在识别大学记录中是否存在任何个人的照片。所提出的方法涉及将每个学生照片的嵌入及其详细信息存储在矢量数据库中。当需要比较照片时，系统将生成该照片的嵌入值，然后将该值与数据库进行比较。如果该值在特定阈值内，则表明该个人存在于记录中。
我正在寻求专家建议，以确定这种方法是否可行。如果对此方法有任何疑虑，我将不胜感激最佳解决方案的建议。]]></description>
      <guid>https://stackoverflow.com/questions/78688976/face-recognize-from-the-database-using-embedding-technique</guid>
      <pubDate>Sun, 30 Jun 2024 15:09:55 GMT</pubDate>
    </item>
    <item>
      <title>如何修复 Huggingface 训练器的学习率？</title>
      <link>https://stackoverflow.com/questions/77792137/how-to-fix-the-learning-rate-for-huggingface%c2%b4s-trainer</link>
      <description><![CDATA[我正在使用以下参数训练模型：
Seq2SeqTrainingArguments(
output_dir = &quot;./out&quot;, 
overwrite_output_dir = True,
do_train = True,
do_eval = True,

per_device_train_batch_size = 2, 
gradient_accumulation_steps = 4,
per_device_eval_batch_size = 8, 

learning_rate = 1.25e-5,
warmup_steps = 1,

save_total_limit = 1,

evaluation_strategy = &quot;epoch&quot;,
save_strategy = &quot;epoch&quot;,
logs_strategy = &quot;epoch&quot;, 
num_train_epochs = 5, 

gradient_checkpointing = True,
fp16 = True, 

predict_with_generate = True,
generation_max_length = 225,

report_to = [&quot;tensorboard&quot;],
load_best_model_at_end = True,
metric_for_best_model = &quot;wer&quot;,
greater_is_better = False,
push_to_hub = False,
)

我假设 warmup_steps=1 固定了学习率。
但是，训练结束后，我查看文件 trainer_state.json，发现学习率似乎没有固定。
以下是 learning_rate 和 step 的值：
learning_rate，steps
1.0006 e-05 1033
7.5062 e-06 2066
5.0058 e-06 3099
2.5053 e-06 4132
7.2618 e-09 5165

学习率似乎没有固定在 1.25e-5（步骤 1 之后）。我遗漏了什么？如何修复学习率。]]></description>
      <guid>https://stackoverflow.com/questions/77792137/how-to-fix-the-learning-rate-for-huggingface%c2%b4s-trainer</guid>
      <pubDate>Wed, 10 Jan 2024 09:14:26 GMT</pubDate>
    </item>
    <item>
      <title>Spark MLlib 中 DataFrame 的‘rawPrediction’和‘probability’列是什么意思？</title>
      <link>https://stackoverflow.com/questions/37903288/what-do-columns-rawprediction-and-probability-of-dataframe-mean-in-spark-mll</link>
      <description><![CDATA[我训练了一个 LogisticRegressionModel 之后，用它对测试数据 DF 进行了变换，得到了预测 DF。然后当我调用 prediction.show() 时，输出的列名为：[label | features | rawPrediction | probability | prediction]。我知道 label 和 featrues 是什么意思，但是我该如何理解 rawPrediction|probability|prediction？]]></description>
      <guid>https://stackoverflow.com/questions/37903288/what-do-columns-rawprediction-and-probability-of-dataframe-mean-in-spark-mll</guid>
      <pubDate>Sun, 19 Jun 2016 02:00:47 GMT</pubDate>
    </item>
    </channel>
</rss>