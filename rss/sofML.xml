<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 07 Sep 2024 01:10:04 GMT</lastBuildDate>
    <item>
      <title>在函数中创建 VLLM 对象时会导致内存错误，即使明确清除 GPU 缓存也是如此，只有共享引用才能使代码不会崩溃</title>
      <link>https://stackoverflow.com/questions/78959131/vllm-objects-cause-memory-errors-when-created-in-a-function-even-when-explicitly</link>
      <description><![CDATA[我在 Python 中使用 VLLM 库时遇到了问题。具体来说，当我在函数内部创建 VLLM 模型对象时，我遇到了内存问题，并且无法有效清除 GPU 内存，即使在删除对象并使用 torch.cuda.empty_cache() 之后也是如此。
当我尝试在函数内部实例化 LLM 对象时会出现问题，但如果我在父进程或全局范围内实例化该对象，则不会发生这种情况。这表明 VLLM 在函数中创建和管理对象时存在问题，从而导致内存保留和 GPU 耗尽。
示例代码
以下是代码的简化版本：
import torch
import gc
from vllm import LLM

def run_vllm_eval(model_name, samples_params, path_2_eval_dataset):
# 在函数中实例化 LLM
llm = LLM(model=model_name, dtype=torch.float16, trust_remote_code=True)

# 在此处运行一些 VLLM 推理或评估（简化）
result = llm.generate([path_2_eval_dataset], samples_params)

# 推理后清理
del llm
gc.collect()
torch.cuda.empty_cache()

# 在此之后，GPU 内存未正确清除并导致 OOM 错误
run_vllm_eval()
run_vllm_eval()
run_vllm_eval()

但是
llm = run_vllm_eval2()
llm = run_vllm_eval2(llm)
llm = run_vllm_eval2(llm)

有效。
即使明确删除 LLM 对象并清除缓存后，GPU 内存仍未正确释放，导致在尝试加载或运行同一脚本中的另一个模型时出现内存不足 (OOM) 错误。
我尝试过的方法
使用 del 删除 LLM 对象。
运行 gc.collect() 以触发 Python 的垃圾收集。
使用 torch.cuda.empty_cache() 清除 CUDA 内存。
确保父进程中没有实例化 VLLM 对象。
当在函数内创建 LLM 对象时，这些似乎都无法解决问题。
问题
有人在函数内创建 VLLM 对象时遇到过类似的内存问题吗？
是否有推荐的方法来管理或清除函数中的 VLLM 对象以防止 GPU 内存保留？
在这种情况下，是否存在与标准 Hugging Face 或 PyTorch 模型不同的特定 VLLM 处理技术？]]></description>
      <guid>https://stackoverflow.com/questions/78959131/vllm-objects-cause-memory-errors-when-created-in-a-function-even-when-explicitly</guid>
      <pubDate>Sat, 07 Sep 2024 00:58:59 GMT</pubDate>
    </item>
    <item>
      <title>向量近似程序故障</title>
      <link>https://stackoverflow.com/questions/78958485/vector-approximation-program-malfunction</link>
      <description><![CDATA[我和一个朋友创建了一个程序，该程序应该近似一个向量，输入数据是该向量中的一些点。我的朋友写了一行我无法理解的代码，它出现故障：
model = make_pipeline(PolynomialFeatures(degree), RidgeCV(alphas=ridge_alpha,normalize=True,cv=5))

我尝试执行这行代码，但结果出错了：
C:\Users\USER\OneDrive\Desktop\programming\python\vector approximation&gt; &amp; &quot;c:/Users/USER/OneDrive/Desktop/programming/python/vector approximation/.venv/Scripts/python.exe&quot; &quot;c:/Users/USER/OneDrive/Desktop/programming/python/vector approximation/file.py&quot;
50
回溯（最近一次调用）：
文件“c:\Users\USER\OneDrive\Desktop\programming\python\vector approximation\file.py”，第 78 行，位于&lt;module&gt;
model = make_pipeline(PolynomialFeatures(degree), RidgeCV(alphas=ridge_alpha,normalize=True,cv=5))
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError：_BaseRidgeCV.__init__() 获得了意外的关键字参数“normalize”

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78958485/vector-approximation-program-malfunction</guid>
      <pubDate>Fri, 06 Sep 2024 19:17:51 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 `sklearn` 管道中的 `ravel()` 或 `to_numpy()` 转换目标变量？</title>
      <link>https://stackoverflow.com/questions/78958361/is-it-possible-to-transform-a-target-variable-using-ravel-or-to-numpy-in</link>
      <description><![CDATA[我在 R markdown 文档中使用 RStudio 和 tidymodels。我想整合一些来自 scikit-learn 的模型。将数据从 R 代码块传输到 Python 代码块效果很好，但是当我使用以下代码训练和测试模型时：
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

log_reg_pipe = Pipeline([
(&#39;Logistic Regression&#39;, LogisticRegression())
])

log_reg_pipe.fit(X_train, y_train).score(X_val, y_val)

我收到错误
DataConversionWarning：当预期为 1d 数组时，传递了列向量 y。
请将 y 的形状更改为 (n_samples, )，例如使用 ravel()。

我可以通过使用 y_train[&#39;clinical_course&#39;].to_numpy() 训练数据来解决这个问题，但我希望这直接在管道中完成。这可能吗？
请注意，上面的代码只是一个简单的示例来展示我的问题。在这种情况下，X_train 有四列，y_train 有一列。
如上所述，我尝试使用 .to_numpy()，但我想要一个在管道内完成所有转换的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78958361/is-it-possible-to-transform-a-target-variable-using-ravel-or-to-numpy-in</guid>
      <pubDate>Fri, 06 Sep 2024 18:31:45 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 中的 Autograd Trainstep 中的 Lightning</title>
      <link>https://stackoverflow.com/questions/78956646/autograd-in-pytorch-lightning-in-trainstep</link>
      <description><![CDATA[我想实现一个基于 Pytorch Lightning 的 ML 训练，其中我使用 autograd 功能进行训练损失计算：
def training_step(self, batch, batch_idx):
x, y = batch
y_hat = self(x)
loss = self.loss_function(y_hat, y)
return loss


X 的每个样本 x 都是一个二维向量 x=[v,a]
在训练步骤中，我想计算 y_hat 相对于 的梯度。到 v。
损失进一步通过以下公式计算：
loss = mse(y,y_hat) + mse(gradient,gradient_hat)，其中给出了 (true) 梯度
到目前为止还在尝试，但无法正常工作
def training_step(self, batch, batch_idx):
x, y = batch
x.requires_grad_(True) # 确保我们跟踪 x 的梯度
y_hat = self(x)

# 计算 y_hat 相对于 v 的梯度 (x[:, 0])
v = x[:, 0]
grads = torch.autograd.grad(y_hat, v, grad_outputs=torch.ones_like(y_hat), create_graph=True)[0] # ...
]]></description>
      <guid>https://stackoverflow.com/questions/78956646/autograd-in-pytorch-lightning-in-trainstep</guid>
      <pubDate>Fri, 06 Sep 2024 10:08:53 GMT</pubDate>
    </item>
    <item>
      <title>将 Google 机器学习模型放入 Touch Designer 中的问题</title>
      <link>https://stackoverflow.com/questions/78956583/problem-with-putting-google-machine-learning-model-in-touch-designer</link>
      <description><![CDATA[我尝试将一个以 .tox 结尾的文件放入 Touch Designer 程序中，此文件允许将 Google 机器学习模型放入 Touch 设计器中，但当我放入它时，我收到消息
错误加载 modelTypeError：无法读取未定义的属性（读取“加载”）

据说当我放入文件时，我可以放入模型，然后会出现一个相机，它可以识别我训练它的东西。]]></description>
      <guid>https://stackoverflow.com/questions/78956583/problem-with-putting-google-machine-learning-model-in-touch-designer</guid>
      <pubDate>Fri, 06 Sep 2024 09:53:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在单列中存储多个值以用于 ML 算法</title>
      <link>https://stackoverflow.com/questions/78955781/how-to-store-multiple-values-in-single-column-for-use-in-ml-algorithm</link>
      <description><![CDATA[我有 6 个不同的数据集，具有 5 个不同的分类值。这些分类值的总数为 250k。
我的问题定义是预测偏头痛发作的结束时间。
例如，我使用二进制编码将 40000 种不同的药物转换为数字数据。
meds_0,meds_1,meds_2,meds_3,meds_4,meds_5,meds_6,meds_7,meds_8,meds_9,meds_10,meds_11,meds_12,meds_13,meds_14,meds_15

这就是列显示的方式。
但是，用户可以使用多种药物，我必须在一行中显示这一点，因为他使用的药物直接影响他的偏头痛发作的结束时间。
如下所示。
[0 0 0],[0 0 0],[0 0 0],[0 0 0],[0 0 0],[0 0 0],[0 0 1],[0 0 0],[0 0 0],[0 0 1],[0 0 1],[1 0 0],[0 0 0],[1 0 1],[0 1 0],[0 1 0],[0 1 0]

例如，我有另一个名为症状的特征，该用户可以有 95 种不同的症状。我需要将它们保存在一行中的单个列中。症状列从 symptom_0 到 symptom_15。
我想将它们保存在一个数组中，但我的数组长度不是固定的，它是可变的。但是，我不应该添加值（如 0）来使数组长度固定，否则类别将被破坏。我无法弄清楚如何在 ml 算法中使用它。
总之，我想将多个值保留在单个列中，并将其转换为可以在 ml 算法中使用的结构。]]></description>
      <guid>https://stackoverflow.com/questions/78955781/how-to-store-multiple-values-in-single-column-for-use-in-ml-algorithm</guid>
      <pubDate>Fri, 06 Sep 2024 05:54:28 GMT</pubDate>
    </item>
    <item>
      <title>无法加载贝叶斯神经网络</title>
      <link>https://stackoverflow.com/questions/78955532/unable-to-load-bayesian-nueral-network</link>
      <description><![CDATA[我的贝叶斯神经网络运行良好，但在保存模型后我无法加载它，因为它给了我一些错误。
这是我的模型代码：
import tensorflow as tf
import tensorflow_probability as tfp

kernel_divergence_fn = lambda q, p, _: tfp.distributions.kl_divergence(q, p)
bias_divergence_fn = lambda q, p, _: tfp.distributions.kl_divergence(q, p)

inputs = tf.keras.Input(shape=(1024, 2, 1))

x = tfp.layers.Convolution2DFlipout(
filters=32,
kernel_size=(3, 1),
padding=&#39;same&#39;,
activation=&#39;relu&#39;,
kernel_divergence_fn=kernel_divergence_fn,
bias_divergence_fn=bias_divergence_fn
)(输入)
x = tf.keras.layers.Dropout(0.2)(x)

x = tfp.layers.Convolution2DFlipout(
filters=16,
kernel_size=(3, 2),
padding=&#39;same&#39;,
activation=&#39;relu&#39;,
kernel_divergence_fn=kernel_divergence_fn,
bias_divergence_fn=bias_divergence_fn
)(x)
x = tf.keras.layers.Dropout(0.2)(x)

x = tf.keras.layers.Flatten()(x)

解释1 = tfp.layers.DenseFlipout(
128,
bias_posterior_fn=tfp.layers.default_mean_field_normal_fn(),
bias_prior_fn=tfp.layers.default_mean_field_normal_fn(), 
kernel_divergence_fn=kernel_divergence_fn,
bias_divergence_fn=bias_divergence_fn,
activation=&quot;relu&quot;
)(x)

interpretation2 = tfp.layers.DenseFlipout(
64,
bias_posterior_fn=tfp.layers.default_mean_field_normal_fn(),
bias_prior_fn=tfp.layers.default_mean_field_normal_fn(), 
kernel_divergence_fn=kernel_divergence_fn,
bias_divergence_fn=bias_divergence_fn,
activation=&quot;relu&quot;
)(interpretation1)

outputs = tf.keras.layers.Dense(24,activation=&#39;softmax&#39;)(interpretation2)

model = tf.keras.Model(inputs=inputs,outputs=outputs)
model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

model.summary()

我添加了自定义对象部分，因为它为自定义层抛出了错误，但现在它只是说“str”对象不可调用，所以现在我非常困惑。]]></description>
      <guid>https://stackoverflow.com/questions/78955532/unable-to-load-bayesian-nueral-network</guid>
      <pubDate>Fri, 06 Sep 2024 03:41:34 GMT</pubDate>
    </item>
    <item>
      <title>React Native 中的实时面部和空中识别[关闭]</title>
      <link>https://stackoverflow.com/questions/78955519/real-time-facial-aerial-recognition-in-react-native</link>
      <description><![CDATA[如何使用自定义模型在 React Native 中实现实时面部和空中物体识别？
我正在开发一个 React Native 应用，它需要实时面部识别和空中物体识别（例如，从无人机信息中识别建筑物、车辆或其他物体）。我想在不完全依赖 Google ML Kit 或 Face API 等第三方库的情况下实现这一点。我需要一个支持自定义机器学习模型的解决方案。
我已成功集成 react-native-camera 来访问设备摄像头。
我已经探索了 tensorflow.js 和 onnx 进行模型推理，但我在实时性能方面遇到了困难，尤其是空中识别。
我尝试使用 mediapipe 进行面部特征识别，但还没有弄清楚如何以流畅、高效的方式结合面部识别和空中识别。
我的问题：

如何在 React Native 中优化实时面部识别和空中识别，尤其是使用自定义模型？

什么是实时运行这些过程的一些性能考虑或最佳实践，尤其是对于较大的空中识别模型？

我应该使用本机模块来处理繁重的计算，还是可以使用 tensorflow.js 或 onnx 等库在 JavaScript 中高效处理所有事情？

如何在摄像头供稿上叠加边界框和其他视觉提示（例如，面部边框、对象标签），并确保两种识别（面部 + 空中）同时工作？


相关代码：
import { Camera } from &#39;react-native-camera&#39;;
import * as tf from &#39;@tensorflow/tfjs&#39;;
import * as faceLandmarks from &#39;@mediapipe/face_landmarks&#39;;
import airlineRecognitionModel from &#39;./aerialModel&#39;; // 自定义空中识别模型 

const CameraScreen = () =&gt; {

const [predictions, setPredictions] = useState([]); 

const handleCameraStream = async (cameraStream) =&gt; {
const faceResults = await faceLandmarks.estimateFaces(cameraStream);

const airlineResults = await airlineRecognitionModel.predict(cameraStream); 

setPredictions([...faceResults, ...aerialResults]);
}; 

return (&lt;Camera onFrame={handleCameraStream} style={{ flex: 1 }} /&gt;);
};

附加请求：
如果有人参与过类似的项目或知道任何解决面部和空中识别的开源项目，我很乐意看到一些参考资料 - 任何相关资源、教程或 GitHub 存储库。
系统信息：
React Native：0.74.1
平台：iOS 和 Android
模型：自定义训练模型（TensorFlow 格式）
]]></description>
      <guid>https://stackoverflow.com/questions/78955519/real-time-facial-aerial-recognition-in-react-native</guid>
      <pubDate>Fri, 06 Sep 2024 03:30:57 GMT</pubDate>
    </item>
    <item>
      <title>如何微调人脸识别预训练模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78954958/how-can-i-fine-tune-pre-trained-model-for-face-recognition</link>
      <description><![CDATA[我正在开发一个使用人脸识别的自动考勤系统，需要有关最佳方法的建议：
模型选择：哪种高级人脸识别模型（例如 FaceNet、DeepFace、ArcFace）最适合在自定义数据集上进行微调？
数据集管理：我应该如何构建和预处理我的数据集以实现最佳训练和验证？
上下文：
目标：使用人脸识别识别需要出勤的个人。
当前设置：使用 TensorFlow/Keras 和按单个文件夹组织的数据集。
任何指导或提示都将不胜感激！
谢谢！
我尝试制作一个人脸识别模型，我希望知道在自己的数据集中训练其他预训练模型的准确度。]]></description>
      <guid>https://stackoverflow.com/questions/78954958/how-can-i-fine-tune-pre-trained-model-for-face-recognition</guid>
      <pubDate>Thu, 05 Sep 2024 21:54:39 GMT</pubDate>
    </item>
    <item>
      <title>无法将 tensorflow 导入 jupyter 并验证</title>
      <link>https://stackoverflow.com/questions/78954574/can-not-import-tensorflow-to-jupyter-and-verify</link>
      <description><![CDATA[这是它的屏幕截图，我已经安装在正确的路径中，我的 python 版本是 3.9.6，pip 已升级，我不知道为什么我无法导入它，我也尝试运行验证
python3 -c &quot;import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))&quot;

但是它什么都没显示，只是卡在那里，我使用的是 macOS。知道为什么会发生这种情况吗？
尝试创建新的虚拟环境并卸载、重新安装、安装其他版本]]></description>
      <guid>https://stackoverflow.com/questions/78954574/can-not-import-tensorflow-to-jupyter-and-verify</guid>
      <pubDate>Thu, 05 Sep 2024 19:34:12 GMT</pubDate>
    </item>
    <item>
      <title>Python 神经网络中的 DeprecationWarning</title>
      <link>https://stackoverflow.com/questions/78954495/deprecationwarning-in-neural-network-with-python</link>
      <description><![CDATA[我最近一直在学习神经网络实现，在 Dence、Sequential 和预测函数之后，当我想获得结果时，VS Code 会给出此警告：
DeprecationWarning：将带有 ndim \&gt; 0 的数组转换为标量已被弃用，
并且将来会出错。在执行此操作之前，请确保从数组中提取单个元素。 （已弃用 NumPy 1.25。）

p[i,0] = my_sequence(X[i],W1,b1,W2,b2)

def my_dense(a_in,W,b): # 矩阵的大写 W
units = W.shape[1]
a_out = np.zeros(units)
for j in range(units):
w = W[:,j]
z = np.dot(w,a_in) + b[j]
a_out[j] = sigmoid(z)
return a_out
def my_sequence(a0,W1,b1,W2,b2): # a0 : x（输入）
a1 = my_dense(a0,W1,b1)
a2 = my_dense(a1,W2,b2)
return a2
def my_predict(X,W1,b1,W2,b2):
m = X.shape[0]
p = np.zeros((m,1))
for i in range(m):
p[i,0] = my_sequence(X[i],W1,b1,W2,b2)
return(p) # 预测矩阵中的 p
X = np.array([[210,17],
[190,20],
[240,19]])
W1 = np.array( [[-8.93, 0.29, 12.9 ],
[-0.1, -7.32, 10.81]] )
b1 = np.array( [-9.82, -9.28, 0.96] )
W2 = np.array( [[-31.18],
[-27.59],
[-32.56]] )
b2 = np.array( [15.41] )
norm_l.adapt(X)
X_n = norm_l(X)
predictions = my_predict(X_n,W1_tmp,b1_tmp,W2_tmp,b2_tmp)
]]></description>
      <guid>https://stackoverflow.com/questions/78954495/deprecationwarning-in-neural-network-with-python</guid>
      <pubDate>Thu, 05 Sep 2024 19:03:34 GMT</pubDate>
    </item>
    <item>
      <title>具有高基数分类列的聚类数据[关闭]</title>
      <link>https://stackoverflow.com/questions/78952979/clustering-data-which-has-high-cardinal-categorical-columns</link>
      <description><![CDATA[我有一个数据 - 例如：电器商店中的商品数据集。
让我们考虑以下是我的列：
ITEM_NO（唯一标识商品的编号）
、品牌（制造品牌）
、类别
、材料
、工艺
、重量
、价格
、类别特定属性（假设您有某个类别的特定属性，而其他商品可能不存在这些属性）
让我们假设材料、工艺的基数非常高（大约 5k）
我的最终目标：我想对类似类型的商品进行聚类
注意：每个材料值和工艺值都具有独特的含义，因此不能将其分组在一起
我尝试通过采用每个类别对数据进行分组，然后进行聚类（在缩放、编码和 PCA 之后），但效果不佳。由于材料和工艺的基数非常高，因此这没有奏效。 [我旋转了材料和工艺的列]。
下一个方法：我根据材料对项目进行分组，并选取构成初始研发项目数量最多的材料，并将这些项目创建为数据集。接下来，我选取上述项目数据集（即属于项目数量最多的材料的项目）的工艺列（以逗号分隔的字符串，例如 P1、P2、P3 作为值）。因此，我使用 Jaccard 指数根据工艺成对查找每个项目之间的相似性，然后选取 Jaccard 指数值大于 0.7 的项目对，但这花费了很长时间。此外，由于材料的基数非常高，我必须对每一种材料重复此练习。
我想要一个可扩展的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78952979/clustering-data-which-has-high-cardinal-categorical-columns</guid>
      <pubDate>Thu, 05 Sep 2024 12:15:35 GMT</pubDate>
    </item>
    <item>
      <title>运行 Jenkins 管道时如何修复“脚本返回退出代码 15”</title>
      <link>https://stackoverflow.com/questions/78885552/how-to-fix-script-returned-exit-code-15-when-running-jenkins-pipeline</link>
      <description><![CDATA[我正在使用 Jenkins 定义管道。我正在开发一个文本摘要器项目，并使用 jenkins 进行 CICD。触发管道后，在 CD 阶段我收到以下错误：
ssh -o StrictHostKeyChecking=no -l ubuntu 3.226.221.21 &#39;cd /home/ubuntu/ &amp;&amp; wget https://raw.githubusercontent.com/mishraatharva/textsummarization/main/docker-compose.yml &amp;&amp; export IMAGE_NAME=${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/textsum:latest &amp;&amp; aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com &amp;&amp; docker compose up -d &#39;
Shell 脚本
1.6 秒
+ ssh -o StrictHostKeyChecking=no -l ubuntu 3.226.221.21 cd /home/ubuntu/ &amp;&amp; wget https://raw.githubusercontent.com/mishraatharva/textsummarization/main/docker-compose.yml &amp;&amp; export IMAGE_NAME=****:latest &amp;&amp; aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ****.dkr.ecr.us-east-1.amazonaws.com &amp;&amp; docker compose up -d 
--2024-08-18 19:19:08-- https://raw.githubusercontent.com/mishraatharva/textsummarization/main/docker-compose.yml
正在解析 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...
正在连接到 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... 已连接。
HTTP 请求已发送，正在等待响应... 200 OK
长度：95 [text/plain]
保存至：‘docker-compose.yml.10’
0K 100% 3.77M=0s
2024-08-18 19:19:08 (3.77 MB/s) - ‘docker-compose.yml.10’ 已保存 [95/95]
警告！您的密码将以未加密形式存储在 /home/ubuntu/.docker/config.json 中。
配置凭据助手以删除此警告。请参阅
https://docs.docker.com/engine/reference/commandline/login/#credential-stores
登录成功
yaml：第 229 行：此上下文中不允许映射值
脚本返回退出代码 15
Jenkins 2.462.1

我不知道如何解决这个问题：
我正在共享包含 jenkins 文件的 github repo。
https://github.com/mishraatharva/textsummarization
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78885552/how-to-fix-script-returned-exit-code-15-when-running-jenkins-pipeline</guid>
      <pubDate>Sun, 18 Aug 2024 20:03:08 GMT</pubDate>
    </item>
    <item>
      <title>自定义模型聚合器 TensorFlow Federated</title>
      <link>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</link>
      <description><![CDATA[我正在尝试使用 TensorFlow Federated，使用 FedAvg 算法模拟训练过程。
trainer = tff.learning.algorithms.build_weighted_fed_avg(
model_fn= tff_model,
client_optimizer_fn=client_optimizer,
server_optimizer_fn=server_optimizer
)

我想使用自定义权重来聚合客户端的更新，而不是使用它们的样本数量。我知道 tff.learning.algorithms.build_weighted_fed_avg() 有一个名为 client_weighting 的参数，但唯一接受的值来自类 tff.learning.ClientWeighting，它是一个枚举。
因此，唯一的方法似乎是编写自定义 WeightedAggregator。我尝试按照本教程 进行操作，该教程解释了如何编写非加权聚合器，但我无法将其转换为加权聚合器。
加权和非加权聚合器是否具有相同的结构？有没有使用自定义权重的简单方法？]]></description>
      <guid>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</guid>
      <pubDate>Mon, 05 Aug 2024 16:06:48 GMT</pubDate>
    </item>
    <item>
      <title>在 sklearn 数字示例中使用自己的图像</title>
      <link>https://stackoverflow.com/questions/41666627/using-own-image-in-sklearn-digits-example</link>
      <description><![CDATA[因此，我一直在尝试使用 sklearn 和 python，并尝试了解机器学习的工作原理。我得到了基本示例，但有一件事让我很困惑。
例如，假设我正在使用数字数据集，一旦我准备好分类器并进行测试。在该示例中，我将如何使用自己的手写图像？
我设法加载图像并使用 matplotlib 读取其像素，但我从中得到了一个包含 (8,8,3) 的数组，而来自数字数据集的样本的形状为 (8,8)。
这是我用来训练分类器的代码
digits = load_digits()
x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.20)

clf = svm.SVC(gamma=0.001, C=100)

clf.fit(x_train, y_train)

img = mpimg.imread(&#39;handwritten.jpg&#39;)

这是我得到的打印件来自 print(img)
[[[245 251 255]
[ 51 55 82]
[ 41 56 87]
[ 18 58 109]
[ 11 65 125]
[ 20 64 101]
[242 255 255]
[255 255 239]]

[[249 253 255]
[249 254 255]
[239 255 255]
[221 255 255]
[209 255 255]
[ 16 60 105]
[242 255 255]
[255 253 242]]

 [[250 254 255] [250 255 255] [241 255 255] [218 255 255] [ 10 69 137] [ 10 57 111] [241 255 255] [255 253 250]] [[252 255 253] [251 255 252] [ 44 61 77] [ 16 60 109] [ 3 63 136] [ 13 61 123] [240 255 255] [255 253 255]] [[251 255 249] [252 255 250] [239 255 255] [ 19 63 112] [ 3 63 136] [ 16 64 128] [240 255 255] [255 252 255]] [[249 255 253] [249 255 253] [240 255 255] 8 255 255] [ 3 59 133] [ 17 62 121] [242 255 255] [255 252 255]] [[245 255 255] [245 255 255] [236 254 255]
  [220 255 255] [ 14 67 135] [ 19 59 111] [245 255 255] [255 253 250]] [[241 255 255] [ 46 58 74] [ 38 58 83] [ 21 61 110] [ 9 60 123] [224 255 255] [246 255 255] [255 254 243]]] (64,) [[[245 251 255] [ 51 55 82] [ 41 56 87] [ 18 58 109] [ 11 65 125] [ 20 64 101] [242 255 255] [255 255 239]] [[249 253 255] [249 254 255] [239 255 255] [221 255 255] [209 255 255] [ 16 60 105] [ 242 255 255] [255 253 242]] [[250 254 255] [250 255 255] [241 255 255] [218 255 255] [ 10 69 137] [ 10 57 111]
  [241 255 255] [255 253 250]] [[252 255 253] [251 255 252] [ 44 61 77] [ 16 60 109] [ 3 63 136] [ 13 61 123] [240 255 255] [255 253 2 55]] [[251 255 249] [252 255 250] [239 255 255] [ 19 63 112] [ 3 63 136] [ 16 64 128] [240 255 255] [255 252 255]] [[249 255 253] [249 255 253] [240 255 255] [218 255 255] [ 3 59 133] [ 17 62 121] [242 255 255] [255 252 255]] [[245 255 255] [245 255 255] [236 254 255] [220 255 255] [ 14 67 135] [ 19 59 111] [245 255 255] [255 253 250]] [[241 255 255]
[ 46 58 74]
[ 38 58 83]
[ 21 61 110]
[ 9 60 123]
[224 255 255]
[246 255 255]
[255 254 243]]]

这是示例数字中的一个
[ 0. 0. 5. 13. 9. 1. 0. 0. 0. 0. 13. 15. 10. 15. 5.
0. 0. 3. 15. 2. 0. 11. 8. 0. 0. 4. 12. 0. 0. 8.
8. 0. 0. 5. 8. 0. 0. 9. 8. 0. 0. 4. 11. 0. 1.
12. 7. 0. 0. 2. 14. 5. 10. 12. 0. 0. 0. 0. 6. 13.
10. 0. 0. 0.]

正如您所见，不仅形状不同，而且特征的值也不同，数据集中的样本仅包含从 0 到 16 的整数，而我的样本具有 RGB 值。
那么我该如何“规范化”我的数据以便能够在其上使用我的分类器呢？]]></description>
      <guid>https://stackoverflow.com/questions/41666627/using-own-image-in-sklearn-digits-example</guid>
      <pubDate>Sun, 15 Jan 2017 21:39:28 GMT</pubDate>
    </item>
    </channel>
</rss>