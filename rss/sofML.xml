<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 01 Jul 2024 12:30:02 GMT</lastBuildDate>
    <item>
      <title>使用黑盒评分进行无监督反向传播</title>
      <link>https://stackoverflow.com/questions/78692093/unsupervised-backpropagation-with-blackbox-scoring</link>
      <description><![CDATA[我有一个向量集，它对 x 进行评分，分数是一个浮点数，而黑盒是另一个程序。我想尝试通过更改向量化来提高分数。我不知道更好的向量化是什么样子，因此它是无监督的，应该从获得的分数中学习。你会建议什么机器学习方法？损失函数是负分数吗？]]></description>
      <guid>https://stackoverflow.com/questions/78692093/unsupervised-backpropagation-with-blackbox-scoring</guid>
      <pubDate>Mon, 01 Jul 2024 11:58:12 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 pytorch 提取视觉变换器的倒数第二层输出</title>
      <link>https://stackoverflow.com/questions/78691616/cannot-extract-penultimate-layer-output-of-a-vision-transformer-with-pytorch</link>
      <description><![CDATA[我有以下模型，我使用自己的数据集通过 DataParallel 训练对其进行了调整
model = timm.create_model(&#39;vit_base_patch16_224&#39;, pretrained=False)
model.head = nn.Sequential(nn.Linear(768, 512),nn.ReLU(),nn.BatchNorm1d(512),nn.Dropout(p=0.2),nn.Linear(512, 141))
checkpoint = torch.load(&#39;vit_b_16v3.pth&#39;)
checkpoint = {k.partition(&#39;module.&#39;)[2]: v for k, v in checkpoint.items()}
# 加载参数
model.load_state_dict(checkpoint)

但是，我不知道如何获取这种视觉转换器的倒数第二层输出。我尝试了本教程，但不起作用。我只想输入一张图片，并有一个 512 维向量来描述它。使用 Tensorflow 做这件事很容易，但在 Pytorch 中我却很挣扎。
P.s：我的最后一层如下
(norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(fc_norm): Identity()
(head_drop): Dropout(p=0.0, inplace=False)
(head): Sequential(
(0): Linear(in_features=768, out_features=512, bias=True)
(1): ReLU()
(2): BatchNorm1d(512, eps=1e-05, motivation=0.1, affine=True, track_running_stats=True)
(3): Dropout(p=0.2, inplace=False)
(4): Linear(in_features=512, out_features=141,bias=True)
)
)
]]></description>
      <guid>https://stackoverflow.com/questions/78691616/cannot-extract-penultimate-layer-output-of-a-vision-transformer-with-pytorch</guid>
      <pubDate>Mon, 01 Jul 2024 10:16:39 GMT</pubDate>
    </item>
    <item>
      <title>Roboflow Vs. Darknet 用于生成权重文件和创建模型</title>
      <link>https://stackoverflow.com/questions/78691574/roboflow-vs-darknet-for-generating-weight-file-and-creating-the-model</link>
      <description><![CDATA[我有一个 YoloV8 数据文件格式，它是手动完成的数据（图像）注释。
生成模型并因此产生权重文件的最有效和最直接的方法是什么？是通过以下命令使用 darknet 吗：
darknet.exe detector train data/obj.data yolo-obj.cfg backup\yolo-obj_2000.weights

然后使用类似下面的命令生成关联模型：
python tools/model_converter/convert.py cfg/yolov3.cfg weights/yolov3.weights weights/yolov3.h5

或者通过以下命令使用 Roboflow：
version.deploy(model_type=&quot;yolov8&quot;, model_path=f”{HOME}/runs/detect/train/&quot;)

在我看来，darknet 更难安装。]]></description>
      <guid>https://stackoverflow.com/questions/78691574/roboflow-vs-darknet-for-generating-weight-file-and-creating-the-model</guid>
      <pubDate>Mon, 01 Jul 2024 10:07:36 GMT</pubDate>
    </item>
    <item>
      <title>学习 Python 的最佳书籍 [关闭]</title>
      <link>https://stackoverflow.com/questions/78690958/best-book-to-learn-python</link>
      <description><![CDATA[寻求具有最新更新的最佳 Python 书籍推荐
我渴望从头开始学习 Python，并随时了解该语言的最新发展。随着 Python 的快速发展，我想确保自己学习的是最新的功能、最佳实践和行业标准。
您能否推荐一本全面且适合初学者的书籍，涵盖 Python 3.x（最好是最新版本，Python 3.10 或 3.11），并包含以下主题：
核心 Python 概念：变量、数据类型、控制结构、函数、面向对象编程等
数据分析和可视化：NumPy、Pandas、Matplotlib 和 Seaborn
Web 开发：Flask 或 Django、HTML、CSS 和 JavaScript 基础知识
机器学习和人工智能：scikit-learn、TensorFlow 和 Keras
最佳实践和编码标准：代码组织、调试和测试
]]></description>
      <guid>https://stackoverflow.com/questions/78690958/best-book-to-learn-python</guid>
      <pubDate>Mon, 01 Jul 2024 07:35:55 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的可解释 AI：在 shap.Explainer() 中，我应该输入 classifier 还是 classifier.predict？那么，如何获取 shap 值？[关闭]</title>
      <link>https://stackoverflow.com/questions/78690391/explainable-ai-in-python-in-shap-explainer-should-i-input-classifier-or-clas</link>
      <description><![CDATA[我正在使用 SHAP（Shapley Additive Explanations）。我理解工作流程必须是：将我的数据拆分为训练和测试，训练我的模型，运行 SHAP 解释器，获取 shap 值。
当然，我见过使用不同方法做同样事情的代码（有点像 Perl 哲学，但很好）。我搞不懂它们之间的区别。我已阅读 SHAP 文档，但未能理解正确的方法。
我已看到两者：

explainer = shap.Explainer(clf)
explainer = shap.Explainer (clf.predict, X train)

后来，为了获取 shap 值，我看到了：

explainer(X)
explainer(X_test)
explainer.shap_values(X_test) - 这是我唯一理解差异的。它返回一个 numpy 数组，而不是 shap 解释对象。

下面，我复制了一些我见过的例子。
在Towards Data Science中：
X_train, X_test, y_train, y_test = train_test_split(X, y)
clf.fit(X_train, y_train)

explainer = shap.Explainer(clf.predict, X_test)
shap_values = explainer(X_test)

在 SHAP 官方GitHub 页面（不是他们的文档）
explainer = shap.Explainer(clf)
shap_values = explainer(X)

在geeks for geeks和datacamp
X_train, X_test, y_train, y_test = train_test_split(X, y)
clf.fit(X_train, y_train)

解释器 = shap.Explainer(clf)
shap_values = explainer(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/78690391/explainable-ai-in-python-in-shap-explainer-should-i-input-classifier-or-clas</guid>
      <pubDate>Mon, 01 Jul 2024 03:49:45 GMT</pubDate>
    </item>
    <item>
      <title>如何使用我自己的数据集训练万寿菊模型进行单目深度图估计？</title>
      <link>https://stackoverflow.com/questions/78690038/how-can-i-train-the-marigold-model-for-monocular-depth-map-estimation-using-my-o</link>
      <description><![CDATA[我偶然发现了这篇论文“重新利用基于扩散的图像生成器进行单目深度估计”，并在 GitHub 上找到了它的代码。
我目前正在尝试使用自己的数据集训练模型，但感觉有点卡住，不确定需要进行哪些修改。
在 /dataset 目录中，有几个 .yaml 配置文件。我应该为我的数据集创建新的 train.yaml 和 val.yaml 文件吗，还是最好修改现有的 dataset_train.yaml 和 dataset_val.yaml 文件并保留其他文件？
如能提供任何有关如何正确设置这些配置文件的指导，我将不胜感激！
论文：https://arxiv.org/abs/2312.02145
GitHub：https://github.com/prs-eth/Marigold]]></description>
      <guid>https://stackoverflow.com/questions/78690038/how-can-i-train-the-marigold-model-for-monocular-depth-map-estimation-using-my-o</guid>
      <pubDate>Sun, 30 Jun 2024 23:25:07 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 DB 扫描来聚类椭圆？[关闭]</title>
      <link>https://stackoverflow.com/questions/78689202/is-it-possible-to-use-db-scan-to-cluster-ellipses</link>
      <description><![CDATA[我有一组椭圆形的轮廓。这些轮廓以特定角度倾斜，我想对这些轮廓进行聚类。如果两个相邻细菌的质心相距小于距离 R，并且它们的运动方向相差小于角度 A，我们将这两个细菌定义为同一簇的成员。
我的轮廓如下所示：

我们可以针对这种情况使用 DB-scan 算法吗？据我了解，我们定义一个半径为 R 的小圆，如果这些点的总数符合我们之前设定的标准，则该圆中重叠的所有点都将成为核心点。从这些核心点开始，集群不断扩展，然后包含非核心点。
我不明白如何将角度的其他条件纳入其中。我是否应该首先使用 DB 扫描基于 R 查找集群，然后使用其他可以处理角度的算法过滤掉这个结果？或者有没有办法使用 DB 扫描本身来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78689202/is-it-possible-to-use-db-scan-to-cluster-ellipses</guid>
      <pubDate>Sun, 30 Jun 2024 16:41:42 GMT</pubDate>
    </item>
    <item>
      <title>如何优化 PyTorch 和 Ultralytics Yolo 代码以利用 GPU？</title>
      <link>https://stackoverflow.com/questions/78687946/how-to-optimize-pytorch-and-ultralytics-yolo-code-to-utilize-gpu</link>
      <description><![CDATA[我正在做一个涉及对象检测和跟踪的项目。对于对象检测，我使用 yolov8，对于跟踪，我使用 SORT 跟踪器。运行以下代码后，我的 GPU 使用率始终低于 10%，而 CPU 使用率始终超过 40%。我安装了 cuda、cudnn，并使用 cuda 安装了 torch。我还编译了支持 cuda 的 opencv。我正在使用 RTX 4060 ti，但看起来它没有被使用。
有没有办法进一步优化下面的代码，以便所有工作都由 GPU 而不是 CPU 处理？
from src.sort import *
import cv2
import time
import torch
import numpy as np
from ultralytics import YOLO

device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
print(f&quot;Using device: {device}&quot;)
sort_tracker = Sort(max_age=20, min_hits=2, iou_threshold=0.05)
model = YOLO(&#39;yolov8s.pt&#39;).to(device)

cap = cv2.VideoCapture(0)

while True:
ret, frame = cap.read() 
if not ret:
print(&quot;**未收到帧**&quot;)
继续

results = model(frame)
dets_to_sort = np.empty((0, 6))
for result in results:
for obj in result.boxes:
bbox = obj.xyxy[0].cpu().numpy().astype(int)
x1, y1, x2, y2 = bbox

conf = obj.conf.item()
class_id = int(obj.cls.item())
dets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, class_id])))

tracked_dets = sort_tracker.update(dets_to_sort)
for det in tracked_dets:
x1, y1, x2, y2 = [int(i) for i in det[:4]]
track_id = int(det[8]) if det[8] 不为 None else 0
class_id = int(det[4])
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
cv2.putText(frame, f&quot;{track_id}&quot;, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)

frame = cv2.resize(frame, (800, int(frame.shape[0] * 800 / frame.shape[1])), interpolation=cv2.INTER_NEAREST)
cv2.imshow(&quot;Frame&quot;, frame)
key = cv2.waitKey(1)
如果 key == ord(&quot;q&quot;):
break
如果 key == ord(&quot;p&quot;):
cv2.waitKey(-1)

cap.release()
cv2.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/78687946/how-to-optimize-pytorch-and-ultralytics-yolo-code-to-utilize-gpu</guid>
      <pubDate>Sun, 30 Jun 2024 07:43:52 GMT</pubDate>
    </item>
    <item>
      <title>如何在 FastAPI 中加载深度学习模型</title>
      <link>https://stackoverflow.com/questions/78687792/how-to-load-deep-learning-model-in-fastapi</link>
      <description><![CDATA[我无法使用 Keras 和 FAstAPI 成功加载以 .keras 格式保存的机器学习模型。尽管按照标准程序使用 tensorflow.keras.models.load_model() 加载模型，但应用程序仍会抛出错误，指示未找到文件、与 FastAPI 和 TensorFlow 版本的兼容性问题或依赖项冲突。我已验证文件路径并确保安装了所有必要的依赖项，但问题仍然存在。详细的调试尝试包括检查 FastAPI、Keras 和 TensorFlow 之间的版本兼容性，以及确认模型文件可访问且格式正确。尽管做出了这些努力，但模型加载过程始终失败，阻碍了机器学习功能进一步集成到我的 FastAPI 应用程序中。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78687792/how-to-load-deep-learning-model-in-fastapi</guid>
      <pubDate>Sun, 30 Jun 2024 06:14:54 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python sdk v2 在 Azure ML for Pipeline 中加载已注册的组件</title>
      <link>https://stackoverflow.com/questions/78679016/load-registered-component-in-azure-ml-for-pipeline-using-python-sdk-v2</link>
      <description><![CDATA[我正在 Azure 机器学习工作室中创建将在管道中一起运行的组件。在这个基本示例中，我有一个 python 脚本和一个 yml 文件，它们构成了我的组件，还有一个用于定义、实例化和运行管道的笔记本。请参阅下面此组件的文件夹结构概述。
📦component
┣ 📜notebook.ipynb
┣ 📜component_script.py
┗ 📜component_def.yml

然后，在我的笔记本中，我可以使用下面的代码加载组件并将其注册到工作区（请注意，这里我已经实例化了我的 ml_client 对象）。
# 导入组件包
from azure.ai.ml import load_component

# 从 yml 文件加载组件
component = load_component(&quot;component_def.yml&quot;)

# 现在我们将组件注册到工作区
component = ml_client.create_or_update(component)

然后我可以成功地将此组件传递到管道中。我的问题是，既然我已经注册了组件，我就不再需要使用 component = load_component(&quot;component_def.yml&quot;) 来实例化我的组件对象，这需要访问 yml 文件。我应该能够从已注册的组件实例化我的组件对象。我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78679016/load-registered-component-in-azure-ml-for-pipeline-using-python-sdk-v2</guid>
      <pubDate>Thu, 27 Jun 2024 17:10:34 GMT</pubDate>
    </item>
    <item>
      <title>Flutter 中的 TensorFlowInferenceInterface</title>
      <link>https://stackoverflow.com/questions/78677544/tensorflowinferenceinterface-in-flutter</link>
      <description><![CDATA[我在 Android 中有一个对象检测代码，我想将其转换为 Flutter 以便也用于 IOS
public static Classifier create(
AssetManager assetManager,
String modelFilename,
String[] labels,
int inputSize,
int imageMean,
float imageStd,
String inputName,
String outputName) {
final TensorFlowImageClassifier c = new TensorFlowImageClassifier();
c.inputName = inputName;
c.outputName = outputName;

// 将标签名称读入内存。
Collections.addAll(c.labels, labels);

c.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);

// 输出的形状为 [N, NUM_CLASSES]，其中 N 是批次大小。
final Operation operation = c.inferenceInterface.graphOperation(outputName);
final int numClasses = (int) operation.output(0).shape().size(1);

// 理想情况下，可以从输入操作的形状中检索 inputSize。唉，
// 通常使用的 graphdef 中输入的占位符节点未指定形状，因此它
// 必须作为参数传入。
c.inputSize = inputSize;
c.imageMean = imageMean;
c.imageStd = imageStd;

// 预分配缓冲区。
c.outputNames = new String[]{outputName};
c.intValues = new int[inputSize * inputSize];
c.floatValues = new float[inputSize * inputSize * 3];
c.outputs = new float[numClasses];

return c;
}

调用：
create(
getAssets(),
&quot;file:///android_asset/xxx&quot;,
getResources().getStringArray(R.array.yyy),
INPUT_SIZE,
128,
128,
&quot;input&quot;,
&quot;InceptionV3/Predictions/Reshape_1&quot;)

org.tensorflow.contrib.android.TensorFlowInferenceInterface
在 Flutter 中找不到这个类，我应该用什么来替换它？]]></description>
      <guid>https://stackoverflow.com/questions/78677544/tensorflowinferenceinterface-in-flutter</guid>
      <pubDate>Thu, 27 Jun 2024 12:06:57 GMT</pubDate>
    </item>
    <item>
      <title>wandb 的超频算法在哪些时期检查改进？</title>
      <link>https://stackoverflow.com/questions/78530549/at-which-epochs-does-the-hyperband-algorithm-of-wandb-checks-for-improvement</link>
      <description><![CDATA[我正在尝试使用 wandb 库的超参数调整（又名扫描）功能（链接到其官方页面）。我正在尝试应用贝叶斯超带算法。
现在，正如这些页面中提到的那样（如何定义扫描配置），（与提前终止选项相关的参数是什么），在提前终止下，我们必须提到 4 个参数（一般），它们是 min_iter、s、eta 和 max_iter，它看起来如下所示。
我的疑问总结：
总之，我想知道的是，
给定所有 4 个：- min_iter、s、eta 和max_iter

超频带算法将在哪些时期检查改进？

考虑到我正在尝试做贝叶斯超频带，第一个括号中将评估多少次运行，连续的括号中将评估多少次运行？

是否有任何方法或经验法则来决定这 4 个参数（min_iter、s、eta 和 max_iter）的取值？

请更详细地解释一下参数 s 和 eta（特别是 eta），即使用一些基础数学知识（如果可能，请保持简单）。


我对什么有疑问？ （更详细/上下文解释）：
（这里），他们有点解释了在哪些时期（他们的）超频带算法实现检查改进并决定是否终止运行。
当我们只关注每次运行的最小迭代次数时
当我们只关注每次运行的最小迭代次数时
但是对于我们关注的是每次运行的最小和最大迭代次数的情况？
就像下面这个...
#在 yaml 文件中，由 wanbd 在 python 中使用
early_terminate:
type: hyperband
min_iter: 10
s: 3
eta: 4
max_iter: 50

我已经尝试过的：
我甚至尽我所能阅读原始论文并试图了解发生了什么（或可能发生什么）（链接到 hyperband 算法的原始论文），但无法得到满意的答案。
我甚至尝试访问他们的 github 页面，那里有示例，但他们只展示了如何编写配置，并没有深入解释它的作用。]]></description>
      <guid>https://stackoverflow.com/questions/78530549/at-which-epochs-does-the-hyperband-algorithm-of-wandb-checks-for-improvement</guid>
      <pubDate>Fri, 24 May 2024 20:25:42 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：不可散列类型：pd.get_dummies 的“Series”</title>
      <link>https://stackoverflow.com/questions/70617092/typeerror-unhashable-type-series-for-pd-get-dummies</link>
      <description><![CDATA[我尝试对我拥有的数据框中的一些名义数据（来自 Kaggle 的 House Regression）使用 pd.get_dummies。我将所有名义类别分成列名列表，&#39;obj_nominal&#39;。
当我调用
pd.get_dummies(df, columns=obj_nominal)

我收到错误：
TypeError：不可哈希类型：&#39;Series&#39;。

到目前为止，我所做的唯一预处理是删除数据集中的空值。我也尝试过使用 Sklearn OneHotEncoder，但它会产生相同的错误。
我也尝试过使用以下方法制作单独的数据框：
x = df.iloc[:, obj_nominal]

并在数据框上传递 get_dummies：
pd.get_dummies(data = x)

但还是没运气……
数据可在 https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data 下载]]></description>
      <guid>https://stackoverflow.com/questions/70617092/typeerror-unhashable-type-series-for-pd-get-dummies</guid>
      <pubDate>Fri, 07 Jan 2022 05:51:22 GMT</pubDate>
    </item>
    <item>
      <title>我的项目中没有安装 face_recognition，显示这些类型的错误？</title>
      <link>https://stackoverflow.com/questions/58379962/face-recognition-is-not-install-in-my-project-showing-these-types-of-error</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/58379962/face-recognition-is-not-install-in-my-project-showing-these-types-of-error</guid>
      <pubDate>Mon, 14 Oct 2019 15:36:16 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中处理 nan/null 的分类器</title>
      <link>https://stackoverflow.com/questions/30317119/classifiers-in-scikit-learn-that-handle-nan-null</link>
      <description><![CDATA[我想知道 scikit-learn 中是否有分类器可以处理 nan/null 值。我以为随机森林回归器可以处理这个问题，但当我调用 predict 时却出现了错误。
X_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])
y_train = np.array([1, 2])
clf = RandomForestRegressor(X_train, y_train)
X_test = np.array([7, 8, np.nan])
y_pred = clf.predict(X_test) # 失败！

我不能用任何带有缺失值的 scikit-learn 算法调用预测吗？
编辑。
现在我想想，这很有道理。这不是训练期间的问题，但当您预测变量为空时如何分支时？也许您可以同时拆分两种方式并取结果的平均值？只要距离函数忽略空值，k-NN 似乎就可以正常工作。
编辑 2（年纪大了，更聪明了）
一些 gbm 库（例如 xgboost）使用三叉树而不是二叉树正是出于这个目的：2 个子节点用于是/否决策，1 个子节点用于缺失决策。sklearn 使用二叉树]]></description>
      <guid>https://stackoverflow.com/questions/30317119/classifiers-in-scikit-learn-that-handle-nan-null</guid>
      <pubDate>Tue, 19 May 2015 05:02:35 GMT</pubDate>
    </item>
    </channel>
</rss>