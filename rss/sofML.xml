<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 10 May 2024 12:26:09 GMT</lastBuildDate>
    <item>
      <title>是否有任何网站允许仅放置代码并接受来自我的手机的输入？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78459442/is-there-any-site-that-allows-to-just-place-the-code-and-accepts-input-from-my-p</link>
      <description><![CDATA[只需编辑这一个。如果我不知道如何写或解释这个，我很抱歉。从昨天开始我就没有足够的睡眠。
几个月来我一直被这个问题困扰。我有一个开发应用程序的项目。我在 jupyter 笔记本中编写代码。我想将我的代码放在网上或服务器上并接受来自手机的输入。
我的应用程序是识别图像上的疾病是否是这种疾病。我已经编写了代码，它正在运行。我还在其中编写了一个单元，它接受单个图像并使用已识别的模型输出可能的结果。
现在，您可能会将我重定向到一些 Tflite 或 TensorFlow ml 模型，但我的模型是定制的，与这些东西不兼容。我使用多层感知器编写了一个定制的机器学习模型，但没有办法将其转换为与 Tflite 兼容的代码。
我最后的办法是开发一个应用程序，将图像发送到该网站，在那里处理我的代码，并将结果返回到我的手机。就这么简单，但我不知道它是否简单，但我没有办法做到这一点。
有没有网站允许我这样做？或者有其他方法可以做到这一点？
任何回复或帮助都非常感谢。我们的论文的截止日期是下周，我现在压力很大，很担心。非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/78459442/is-there-any-site-that-allows-to-just-place-the-code-and-accepts-input-from-my-p</guid>
      <pubDate>Fri, 10 May 2024 10:14:31 GMT</pubDate>
    </item>
    <item>
      <title>进行线性回归时结果不佳</title>
      <link>https://stackoverflow.com/questions/78459352/bad-results-while-doing-linear-regression</link>
      <description><![CDATA[我试图使用一组数据进行多元线性回归。我尝试使用用于生成回归系数的同一组 X 来预测 Y。虽然一组数据的实际值与预测值之间的差异较小（正如预期的那样），但另一组数据的实际值与预测值之间的差异较大。两个数据集代表同一组参数（相同的物理量）。我是否做错了什么或者我可以做些什么来改进？
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd
将 statsmodels.api 导入为 sm
……
……
# 构建设计矩阵
X = np.column_stack((tir1, tir1_z, bt_diff, bt_diff_z, bt_diff_sst, s_theta, np.ones_like(tir1)))

sst = np.array([i+273.15 for i in selected_buoy_sst])

# 拟合OLS模型
模型 = sm.OLS(sst, X)
结果 = model.fit()

Predicted_sst_same_data = results.predict(X)

# 计算实际SST和预测SST之间的差异
差异 = sst - 预测的 sst_same_data`


# 使用不同的数据
X_n = np.column_stack((tir1, tir1_z, bt_diff, bt_diff_z, bt_diff_sst, s_theta, np.ones_like(tir1)))
sst = 皮肤温度数组
# 拟合OLS模型
模型 = sm.OLS(sst_n, X_n)
结果 = model.fit()

# 打印回归结果的摘要
打印（结果.summary（））

Predicted_sst_same_data = results.predict(X)

# 计算实际SST和预测SST之间的差异
差异 = sst - 预测的 sst_same_data


如果需要的话我可以上传数据。这取决于我的 x 和 Y 值吗？]]></description>
      <guid>https://stackoverflow.com/questions/78459352/bad-results-while-doing-linear-regression</guid>
      <pubDate>Fri, 10 May 2024 10:02:13 GMT</pubDate>
    </item>
    <item>
      <title>交叉熵成本值是否实际用于更新神经网络的权重，或者仅用于显示网络的性能如何？</title>
      <link>https://stackoverflow.com/questions/78459178/is-the-cross-entropy-cost-value-practically-used-in-updating-weights-of-a-neural</link>
      <description><![CDATA[我创建了一个具有 10 个输出的神经网络，最终由 Leaky RELU 函数激活。我用于该网络的成本函数是均方误差函数乘以 0.5，以使导数的计算变得简单。 0.5（预期输出 - 网络输出）^2。其导数是（网络输出 - 预期输出）。请注意，我创建的网络将在一个热编码值标签值上进行训练。因此，输出 1 到 10 将代表 10 个类，并且输出将映射到类，如果识别到该类，则输出 1，如果未识别到，则输出 0。
现在我一直在阅读并试图理解交叉熵损失函数，这方面的信息有点令人困惑，因为交叉熵损失函数的导数本质上与 0.5 * 均方误差的导数相同功能。所以这最终让我理解，也许是错误的，交叉熵损失函数可能是理论推导，表明通过网络发送错误信息的最佳方法是使用基于信息论的（网络输出 - 预期输出） 。几乎就像证明这是从哪里来的？除此之外，我没有看到交叉熵损失函数计算出的实际值在哪里使用，除了用单个数字指示网络的性能。
我已向 chat gpt 询问过此问题，但确信它用于告知权重应如何更改，但未能向我展示在何处以及如何更改。]]></description>
      <guid>https://stackoverflow.com/questions/78459178/is-the-cross-entropy-cost-value-practically-used-in-updating-weights-of-a-neural</guid>
      <pubDate>Fri, 10 May 2024 09:28:54 GMT</pubDate>
    </item>
    <item>
      <title>我正在训练一个暹罗网络，我有 gtx 1650ti 但张量流仅使用 CPU。如何利用GPU</title>
      <link>https://stackoverflow.com/questions/78458890/i-am-training-a-siamese-network-i-have-gtx-1650ti-but-tensorflow-is-only-using</link>
      <description><![CDATA[从tensorflow.python.client导入device_lib

def get_available_devices():
    local_device_protos = device_lib.list_local_devices()
    返回 [local_device_protos 中 x 的 x.name]

打印（获取可用设备（））

输出：[&#39;/设备:CPU:0&#39;]

所以 GPU 甚至没有显示，是否可以利用 GPU ？ ，如果是的话请指导我。
我认为我没有安装 cudnn 或 cuda。
我是张量流和机器学习方面的新手。
谢谢
我在没有使用 GPU 的情况下训练了模型，但它没有给出准确的结果。
所以我必须使用更多数据集重新训练，所以我认为使用 GPU 会让工作更快。
关于 GPU 利用率，我没有编写任何代码，因为 GPU 甚至没有显示
从tensorflow.python.client导入device_lib
def get_available_devices():
    local_device_protos = device_lib.list_local_devices()
    返回 [local_device_protos 中 x 的 x.name]

打印（获取可用设备（））

输出：[&#39;/设备:CPU:0&#39;]
]]></description>
      <guid>https://stackoverflow.com/questions/78458890/i-am-training-a-siamese-network-i-have-gtx-1650ti-but-tensorflow-is-only-using</guid>
      <pubDate>Fri, 10 May 2024 08:29:50 GMT</pubDate>
    </item>
    <item>
      <title>当文件位于单独的 URL 中时，进行网页抓取和下载吗？</title>
      <link>https://stackoverflow.com/questions/78458393/webscraping-and-downloading-files-when-they-are-in-seperate-urls</link>
      <description><![CDATA[我正在尝试为正在做的一个项目下载澳大利亚法律案例文件。我尝试抓取的网站是 https://www.austlii.edu.au/。该网站提供 rtf 文档供下载，这很不错，但要访问它们，您必须单击链接进入网站，然后才能从那里继续。问题是，我真的不确定如何设计一个高效的程序来执行此操作，例如，我不必手动提供不同的 URL 来获取文件。
有人知道如何实现一个程序来执行此操作吗？您可以点击下面的链接，它确实很好地列出了每个案例，但我不确定如何才能获得尽可能多的数据。
https://www.austlii.edu.au/cgi-bin/viewtoc/au/cases/nsw/NSWSC/2012/
我尝试了一个简单的网页抓取应用程序，但它太慢而且效率低下。]]></description>
      <guid>https://stackoverflow.com/questions/78458393/webscraping-and-downloading-files-when-they-are-in-seperate-urls</guid>
      <pubDate>Fri, 10 May 2024 06:31:57 GMT</pubDate>
    </item>
    <item>
      <title>如何基于另一个张量创建张量 - 在实践中学习 PyTorch？</title>
      <link>https://stackoverflow.com/questions/78457370/how-to-create-a-tensor-based-on-another-one-studying-pytorch-in-practice</link>
      <description><![CDATA[我正在使用 PyTorch 学习 IA 并实现一些玩具示例。
首先，我创建了一个一维张量 (X) 和一个从第一个张量导出的第二个张量 (y)：
X = torch.arange(0, 100, 1.0).unsqueeze(dim=1)
y = X * 2

所以我有类似的东西
X = 张量([[0.], [1.], [2.], [3.], [4.], [5.], ...
y = 张量([[ 0.], [ 2.], [ 4.], [ 6.], [ 8.], [10.], ...

然后，我训练了一个模型来预测 y，它运行良好。
现在，我想要一些不同的东西。 X 为 2D，y 为 1D。 y 通过对 X 的元素进行运算来计算：
如果 x[0] + x[1] &gt; 0？ y = 10: y -10 
X = 张量([[ 55.5348, -97.7608],
            [29.0493，-52.1908]，
            [47.1722，-43.1151]，
            [11.1242，-62.8652]，
            [ 44.8067, 80.8335],...
y = 张量([[-10.], [-10.], [ 10.], [-10.], [ 10.],...

第一个问题，这对于机器学习有意义吗？
第二个...
我正在使用 numpy 生成张量。我可以用更聪明的方式来做吗？
# Criar X valores de entrada para testes
X_numpy = np.random.uniform(低=-100, 高=100, 大小=(1000,2))
打印（“X”，X_numpy）

#y_numpy = np.array([[ (n[0]+n[1]) &gt;= 0 ? 10:-10] for n in X_numpy])
y_numpy = np.empty(形状=[0, 1])
对于 X_numpy 中的 n：
    如果 n[0] + n[1] &gt;= 0：
        y_numpy = np.append(y_numpy, [[10.]], 轴=0)
    elif n[0] + n[1] &lt; 0:
        y_numpy = np.append(y_numpy, [[-10.]], 轴=0)
]]></description>
      <guid>https://stackoverflow.com/questions/78457370/how-to-create-a-tensor-based-on-another-one-studying-pytorch-in-practice</guid>
      <pubDate>Thu, 09 May 2024 23:28:50 GMT</pubDate>
    </item>
    <item>
      <title>用于确定点平面象限的 Kohonen 神经网络</title>
      <link>https://stackoverflow.com/questions/78456978/kohonen-neural-network-for-determining-the-quadrant-of-a-point-plane</link>
      <description><![CDATA[我需要实现 Kohonen 神经网络（没有老师）来确定点的平面（从 1 到 8）的象限。坐标 [x, y, z] 的向量被馈送到神经网络的输入。在输出中，我们得到一个向量，通过该向量可以确定该点属于哪个象限。
这是我训练神经网络的代码：
将 numpy 导入为 np
将 numpy.typing 导入为 npt
将 matplotlib.pyplot 导入为 plt


def 生成数据集（
        大小：整数，
        高：浮动= 10.0
）-&gt;列表[元组[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]]:
    数据集 = []

    象限乘数 = {
        1: (1, 1, 1),
        2: (-1, 1, 1),
        3: (-1, -1, 1),
        4: (1, -1, 1),
        5: (1, 1, -1),
        6: (-1, 1, -1),
        7：（-1，-1，-1），
        8：（1，-1，-1）
    }

    对于范围 (1, 8 + 1) 中的象限：
        数据集 += [
            （
                np.array([[x, y, z]]) *quadrants_multipliers[象限],
                np.eye(8)[象限 - 1].reshape(1, 8)
            ) 对于 zip 中的 x、y、z(
                np.random.uniform（低= 0.0，高=高，大小=大小），
                np.random.uniform（低= 0.0，高=高，大小=大小），
                np.random.uniform(低=0.0，高=高，大小=大小)
            ）
        ]

    返回数据集

定义火车（
        数据集：list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]],
        学习率：浮动，
        纪元：int
）-&gt; npt.NDArray[npt.NDArray[浮点]]：

    W = np.random.randn(3, 8)

    对于范围（1，纪元 + 1）中的纪元：
        random_indexes = np.random.choice(len(数据集)，size=len(数据集)，replace=False)
        对于 random_indexes 中的索引：
            x = 数据集[索引][0] # [[x, y, z]]
            y = 数据集[索引][1]

            x_normalized = x / np.sqrt(np.sum(x ** 2))

            z = np.dot(x_归一化，W)

            W[0][z.argmax()] += 学习率 * (x[0][0] - W[0][z.argmax()])
            W[1][z.argmax()] += 学习率 * (x[0][1] - W[1][z.argmax()])
            W[2][z.argmax()] += 学习率 * (x[0][2] - W[2][z.argmax()])

        学习率 = 学习率 / 时期

    返回W

def calc_accuracy(
        数据集：list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]],
        W: npt.NDArray[npt.NDArray[float]]) -&gt;漂浮：
    正确 = 0
    对于数据集中的 x、y：
        z = np.dot(x, W)
        如果 z.argmax() == y.argmax():
            正确+=1
    返回（正确/len（数据集））* 100


def draw_accuracy_epoch_plots(
        数据集：list[list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]]],
        epoch_number：int
）-&gt;没有任何：
    对于索引，枚举中的数据集（数据集）：
        准确度列表 = []
        准确度列表_2 = []
        对于范围（1，epoch_number + 1）中的纪元：
            W = 训练（数据集，learning_rate=0.7，epochs=epochs）
            precision_list.append(calc_accuracy(数据集，W))
            precision_list_2.append(calc_accuracy(generate_dataset(15), W))

        plt.subplot(3, 2, (2 * 索引) + 1)
        plt.plot(accuracy_list)

        plt.subplot(3, 2, (2 * 索引) + 2)
        plt.plot(accuracy_list_2)


def main() -&gt;;没有任何：
    draw_accuracy_epoch_plots([generate_dataset(250),generate_dataset(500),generate_dataset(1000)],200)

    plt.tight_layout()
    plt.show()


如果 __name__ == “__main__”：
    主要的（）


在代码中，我计算了不同时期神经网络预测的准确性。结果，我得到的准确度为 0 到 45%，这不适合我。同时，神经网络经常产生 0% 的匹配。
如果更改学习率和历元没有帮助，我该如何解决这个问题？我可以添加图层吗（我还没有找到这个问题的答案）？或者也许我错误地构建了学习算法？]]></description>
      <guid>https://stackoverflow.com/questions/78456978/kohonen-neural-network-for-determining-the-quadrant-of-a-point-plane</guid>
      <pubDate>Thu, 09 May 2024 21:05:03 GMT</pubDate>
    </item>
    <item>
      <title>如何纠正 r 中光栅堆栈的方向</title>
      <link>https://stackoverflow.com/questions/78454706/how-do-i-correct-the-orientation-of-a-raster-stack-in-r</link>
      <description><![CDATA[我有一个每日数据的 NetCDF 文件。我将其转换为光栅堆栈，但其方向不正确（我已附上图像）。我该如何纠正它。我还将我的代码附在本文中。 [raster_stack 图像和 r 代码](https://i.sstatic.net/UmI4kSNE.png)
另外，请告诉是否有人知道，我如何从这些栅格文件中提取年度数据到 Excel 格式（在 arcGIS 或 r 中）。我有 1955 年到 2023 年的栅格文件，其中包含每日降雨量数据，我想根据我拥有的管理形状文件提取年降雨量数据。
我在 r 中运行了代码，但没有取得任何进展。
库（光栅）
库（ncdf4）

ncfile &lt;- nc_open(“E:/IMD2/Rainfall/netcdfFiles_0.25 Degree/RF25_ind1955_rfp25.nc”)

variable_name &lt;- “RAINFALL”
降雨数据 &lt;- ncvar_get(ncfile, 变量名称)
lon &lt;- ncvar_get(ncfile, “经度”)
lat &lt;- ncvar_get(ncfile, “纬度”)
时间 &lt;- ncvar_get(ncfile, “TIME”)
lon_range &lt;- 范围(lon)
lat_range &lt;- 范围(lat)

raster_stack &lt;- stack()
for (i in 1:dim(rainfall_data)[3]) {
  raster_layer &lt;- raster(matrix(rainfall_data[,,i], nrow = nrow(rainfall_data), ncol = ncol(rainfall_data)),
                         xmn = lon_range[1], xmx = lon_range[2], ymn = lat_range[1], ymx = lat_range[2],
                         crs =“+proj=longlat +datum=WGS84”）
  raster_stack &lt;- addLayer(raster_stack, raster_layer)
}

&lt;前&gt;&lt;代码&gt;绘图（raster_stack）
翻转 = t(翻转(raster_stack))
情节（翻转）
]]></description>
      <guid>https://stackoverflow.com/questions/78454706/how-do-i-correct-the-orientation-of-a-raster-stack-in-r</guid>
      <pubDate>Thu, 09 May 2024 13:18:03 GMT</pubDate>
    </item>
    <item>
      <title>如何将FastAI分类器集成到sklearn VotingClassifier中？</title>
      <link>https://stackoverflow.com/questions/78435090/how-to-integrate-fastai-classifier-into-sklearn-votingclassifier</link>
      <description><![CDATA[我有一堆表格数据，我设法用它们训练了一个 RandomForestClassifier、一个 GradientBoostingClassifier 和一个深度学习模型（来自 fastai 的表格学习器）。我在结果中注意到，每个模型在特定标签上的表现都比其他模型要好，每个模型都不一样。我想知道我是否可以将所有模型放入 VotingClassifier（来自 sklearn 的模型）。我对 RandomForestClassifier 和 GradientBoostingClassifier 没有问题，但我没有找到有关将表格学习器放入 VotingClassifier 中的内容。可以这样做吗？]]></description>
      <guid>https://stackoverflow.com/questions/78435090/how-to-integrate-fastai-classifier-into-sklearn-votingclassifier</guid>
      <pubDate>Mon, 06 May 2024 07:21:01 GMT</pubDate>
    </item>
    <item>
      <title>IndexError：列表索引超出 Streamlit 范围</title>
      <link>https://stackoverflow.com/questions/78418486/indexerror-list-index-out-of-range-in-streamlit</link>
      <description><![CDATA[因此，我正在尝试构建一个 Streamlit RAG 应用程序，该应用程序从 URL 中提取信息并从中学习。然后，用户可以向模型询问与 URL 中的文章相关的问题，模型将提供合适的答案。
我在笔记本上执行了此操作，效果非常好，只是在我的 Streamlit 应用程序中遇到 IndexError: list index out of range 错误。我将 GoogleGenerativeAIEmbeddings 与 FAISS 结合使用。
这是代码块：
 main_placeholder = sl.empty()
    llm = ChatGoogleGenerativeAI(模型 = &#39;gemini-pro&#39;)
    如果 process_url_clicked:
        加载器 = UnstructedURLLoader(urls = urls)
        main_placeholder.text(&quot;数据加载...开始...✅✅✅&quot;)
        数据 = 加载器.load()
        text_splitter = RecursiveCharacterTextSplitter(
            分隔符 = [&#39;\n&#39;,&#39;\n\n&#39;,&#39;.&#39;,&#39;,&#39;],
            块大小 = 1000,
            块重叠 = 200
        ）
        main_placeholder.text(&quot;文本分割器...开始...✅✅✅&quot;)
        文档 = text_splitter.split_documents(数据)
        嵌入 = GoogleGenerativeAIEmbeddings(模型 = &#39;models/embedding-001&#39;)
        矢量索引= FAISS.from_documents（文档，嵌入）

这是来自 Streamlit 应用程序的回溯：
IndexError：列表索引超出范围
追溯：
文件“C:\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py”，第 584 行，位于 _run_script
    exec（代码，模块.__dict__）
文件“C:\Users\owner\Desktop\Projects\nlp\main.py”，第 84 行，在  中
    vectorstore_openai = FAISS.from_documents（文档，嵌入）
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_core\vectorstores.py”，第 550 行，from_documents
    返回 cls.from_texts(文本、嵌入、元数据=元数据、**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_community\vectorstores\faiss.py”，第 931 行，from_texts
    返回 cls.__from(
           ^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_community\vectorstores\faiss.py”，第 888 行，位于 __from
    索引 = faiss.IndexFlatL2(len(embeddings[0]))
                                  ~~~~~~~~~~^^^

这在我的笔记本上完美运行，我很困惑为什么会发生这种情况。]]></description>
      <guid>https://stackoverflow.com/questions/78418486/indexerror-list-index-out-of-range-in-streamlit</guid>
      <pubDate>Thu, 02 May 2024 10:25:27 GMT</pubDate>
    </item>
    <item>
      <title>如何标记未标记的文本数据</title>
      <link>https://stackoverflow.com/questions/74815891/how-to-label-unlabeled-text-data</link>
      <description><![CDATA[我收集了一些有关食品评论的社交媒体评论，我计划进行方面基础情感分析。作为初始过程，我想将那些未标记的数据标记为一些预定义的主题，例如价格、质量、口味等。
由于我是机器学习的新手，不确定是否可以在没有标记测试数据的情况下进行标记。感谢您的帮助
我尝试过 LDA 主题建模，但觉得这不是正确的方法。]]></description>
      <guid>https://stackoverflow.com/questions/74815891/how-to-label-unlabeled-text-data</guid>
      <pubDate>Thu, 15 Dec 2022 18:17:08 GMT</pubDate>
    </item>
    <item>
      <title>sklearn ImportError：无法导入名称plot_roc_curve</title>
      <link>https://stackoverflow.com/questions/60321389/sklearn-importerror-cannot-import-name-plot-roc-curve</link>
      <description><![CDATA[我正在尝试按照示例。但是，以下导入在 python2 和 python3 中都会出现 ImportError。
从 sklearn.metrics 导入plot_roc_curve

错误：
回溯（最近一次调用最后一次）：
  文件“”，第 1 行，在  中
导入错误：无法导入名称plot_roc_curve

python-2.7 sklearn 版本：0.20.2。
python-3.6 sklearn 版本：0.21.3。
我发现以下导入工作正常，但与 plot_roc_curve 不太一样。
from sklearn.metrics import roc_curve

plot_roc_curve 是否已弃用？有人可以尝试一下代码并让我知道 sklearn 版本是否有效吗？]]></description>
      <guid>https://stackoverflow.com/questions/60321389/sklearn-importerror-cannot-import-name-plot-roc-curve</guid>
      <pubDate>Thu, 20 Feb 2020 13:44:41 GMT</pubDate>
    </item>
    <item>
      <title>如何在Python中添加L1标准化？</title>
      <link>https://stackoverflow.com/questions/48782758/how-to-add-l1-normalization-in-python</link>
      <description><![CDATA[我正在尝试从头开始编写逻辑回归代码。在这段代码中，我认为我的成本导数是我的正则化，但我的任务是添加 L1norm 正则化。你如何在Python中添加这个？是否应该在我定义成本导数的地方添加此内容？感谢任何正确方向的帮助。
def Sigmoid(z):
    返回 1/(1 + np.exp(-z))

def 假设(theta, X):
    返回 Sigmoid(X @ theta)

def Cost_Function(X,Y,theta,m):
    hi = 假设(theta, X)
    _y = Y.reshape(-1, 1)
    J = 1/float(m) * np.sum(-_y * np.log(hi) - (1-_y) * np.log(1-hi))
    返回J

def Cost_Function_Derivative(X,Y,theta,m,alpha):
    hi = 假设(theta,X)
    _y = Y.reshape(-1, 1)
    J = alpha/float(m) * X.T @ (hi - _y)
    返回J

def Gradient_Descent(X,Y,θ,m,alpha):
    new_theta = theta - Cost_Function_Derivative(X,Y,theta,m,alpha)
    返回 new_theta

定义精度(theta):
    正确 = 0
    长度 = len(X_test)
    预测=（假设（theta，X_test）&gt; 0.5）
    _y = Y_test.reshape(-1, 1)
    正确=预测==_y
    my_accuracy = (np.sum(正确) / 长度)*100
    print (&#39;LR 精度:&#39;, my_accuracy, &quot;%&quot;)

def Logistic_Regression(X,Y,alpha,theta,num_iters):
    m = 长度（Y）
    对于范围内的 x（num_iters）：
        new_theta = Gradient_Descent(X,Y,theta,m,alpha)
        θ = 新_θ
        如果 x % 100 == 0：
            打印 #(&#39;θ: &#39;, θ)
            print #(&#39;成本：&#39;, Cost_Function(X,Y,theta,m))
    精度(θ)
ep = .012
初始_theta = np.random.rand(X_train.shape[1],1) * 2 * ep - ep
阿尔法 = 0.5
迭代次数 = 10000
Logistic_Regression(X_train,Y_train,alpha,initial_theta,迭代)
]]></description>
      <guid>https://stackoverflow.com/questions/48782758/how-to-add-l1-normalization-in-python</guid>
      <pubDate>Wed, 14 Feb 2018 08:34:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow Estimator 打印额外的训练指标</title>
      <link>https://stackoverflow.com/questions/45353389/printing-extra-training-metrics-with-tensorflow-estimator</link>
      <description><![CDATA[有没有办法让 Tensorflow 在使用 Estimator API 时打印额外的训练指标（例如批量准确性）？
人们可以添加摘要并在 Tensorboard 中查看结果（请参阅另一篇文章），但我想知道是否有一种优雅的方法可以在训练时打印标量摘要值。这已经发生在训练损失中，例如：
损失 = 0.672677，步长 = 2901（52.995 秒）

但是如果有这样的例子就好了
损失 = 0.672677，准确度 = 0.54678，步长 = 2901（52.995 秒）

没有太多麻烦。我知道大多数时候绘制测试集准确性更有用（我已经使用验证监视器执行此操作），但在这种情况下，我也对训练批次准确性感兴趣。]]></description>
      <guid>https://stackoverflow.com/questions/45353389/printing-extra-training-metrics-with-tensorflow-estimator</guid>
      <pubDate>Thu, 27 Jul 2017 14:14:04 GMT</pubDate>
    </item>
    <item>
      <title>多变量梯度下降</title>
      <link>https://stackoverflow.com/questions/24411315/multi-variable-gradient-descent</link>
      <description><![CDATA[我正在学习梯度下降来计算系数。以下是我正在做的事情：
#!/usr/bin/Python

 将 numpy 导入为 np


   # 这里 m 表示示例的数量，而不是特征的数量
 defgradientDescent(x, y, theta, alpha, m, numIterations):
     xTrans = x.transpose()
     对于范围内的 i(0, numIterations)：
        假设 = np.dot(x, theta)
        损失 = 假设 - y
        # 每个示例的平均成本（2*m 中的 2 在这里并不重要。
        # 但为了与渐变保持一致，我将其包括在内）
        成本 = np.sum(损失 ** 2) / (2 * m)
        #print(&quot;迭代 %d | 成本：%f&quot; % (i, 成本))
        # 每个示例的平均梯度
        梯度 = np.dot(xTrans, 损失) / m
        ＃ 更新
        θ = θ - α * 梯度
     返回θ

 X = np.array([41.9,43.4,43.9,44.5,47.3,47.5,47.9,50.2,52.8,53.2,56.7,57.0,63.5,65.3,71.1,77.0,77.8])
 y = np.array([251.3,251.3,248.3,267.5,273.0,276.5,270.3,274.9,285.0,290.0,297.0,302.5,304.5,309.3,321.7,330.7,349.0])
 n = np.max(X.形状)
 x = np.vstack([np.ones(n), X]).T
 m, n = np.shape(x)
 迭代次数= 100000
 阿尔法 = 0.0005
 θ = np.ones(n)
 theta = 梯度下降(x, y, theta, alpha, m, numIterations)
 打印（θ）

现在我上面的代码工作正常了。如果我现在尝试多个变量并将 X 替换为 X1，如下所示：

&lt;预&gt;&lt;代码&gt; X1 = np.array([[41.9,43.4,43.9,44.5,47.3,47.5,47.9,50.2,52.8,53.2,56.7,57.0,63.5,65.3,71.1,77.0,77.8], [ 29.1,29.3,29.5,29.7,29.9,30.3,30.5,30.7,30.8,30.9,31.5,31.7,31.9,32.0,32.1,32.5,32.9]])

然后我的代码失败并显示以下错误：
 JustTestingSGD.py:14: RuntimeWarning: square 遇到溢出
  成本 = np.sum(损失 ** 2) / (2 * m)
  JustTestingSGD.py:19: RuntimeWarning: 减法中遇到无效值
  θ = θ - α * 梯度
  [楠楠楠]

谁能告诉我如何使用X1进行梯度下降？我使用 X1 的预期输出是：

&lt;前&gt;&lt;代码&gt;[-153.5 1.24 12.08]

我也对其他 Python 实现持开放态度。我只想要 X1 和 y 的系数（也称为 thetas）。]]></description>
      <guid>https://stackoverflow.com/questions/24411315/multi-variable-gradient-descent</guid>
      <pubDate>Wed, 25 Jun 2014 14:24:39 GMT</pubDate>
    </item>
    </channel>
</rss>