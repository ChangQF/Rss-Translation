<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 09 Sep 2024 18:21:49 GMT</lastBuildDate>
    <item>
      <title>我应该如何为我的黑客马拉松构建 ML 模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/78966011/how-should-i-build-an-ml-model-for-my-hackathon</link>
      <description><![CDATA[我和我的团队必须构建一个 ML 模型，以使用 OCR 从标签中读取详细信息。
我是 ML 领域的新手，我该如何学习以及在哪个平台上构建我的模型？
Azure AI、AWS 或 Vertex AI 是执行此操作的好平台吗？它们中的哪一个或其他东西可以构建我的模型？
我尝试访问 Vertex AI、Azure AI，所有内容都要求预先提供银行详细信息，我想在继续之前修复平台。如果有人能在这方面提供帮助，那将非常有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78966011/how-should-i-build-an-ml-model-for-my-hackathon</guid>
      <pubDate>Mon, 09 Sep 2024 14:41:08 GMT</pubDate>
    </item>
    <item>
      <title>为自定义 MLP Keras 模型实现域自适应网络 (DAN)</title>
      <link>https://stackoverflow.com/questions/78965799/implementing-domain-adaptation-network-dan-for-a-custom-mlp-keras-model</link>
      <description><![CDATA[我正在尝试使用 Keras 为 MLP 模型实现 DAN（域自适应网络）。
def build_network():
source_input = layer.Input(shape=(config_dict[&#39;patch_size&#39;], config_dict[&#39;patch_size&#39;], config_dict[&#39;patch_size&#39;], 1), name=&#39;input_1&#39;)
target_input = layer.Input(shape=(config_dict[&#39;patch_size&#39;], config_dict[&#39;patch_size&#39;], 1), name=&#39;input_2&#39;)
source_x = layer.Flatten()(source_input)
target_x = layer.Flatten()(target_input)
source_y1 = density_block(num_nodes=128, l_name=&quot;fc1&quot;)(source_x)
source_y2 = 密集块（num_nodes=128，l_name=“fc2”）（source_y1）
source_y3 = 密集块（num_nodes=128，l_name=“fc3”）（source_y2）
source_y4 = 密集块（num_nodes=128，l_name=“fc4”）（source_y3）
y_source = 层。密集（units=config_dict[&#39;output_dim&#39;]，activation=“tanh”，name=“fc5”）（source_y4）
target_y1 = 密集块（num_nodes=128，l_name=“fc6”）（target_x）
target_y2 = 密集块（num_nodes=128，l_name=“fc7”）（target_y1）
target_y3 = density_block(num_nodes=128, l_name=&quot;fc8&quot;)(target_y2)
target_y4 = density_block(num_nodes=128, l_name=&quot;fc9&quot;)(target_y3)
y_target = layer.Dense(units=config_dict[&#39;output_dim&#39;],activation=&quot;tanh&quot;, name=&quot;fc10&quot;)(target_y4)
model = Model([source_input, target_input], y_source)

return model

model = build_network()
model.compile(optimizer=&#39;adam&#39;, loss=tf.keras.losses.huber, metrics[&#39;mean_absolute_error&#39;])

然后我从预先训练的模型中加载一些权重。
print(&#39;-------------加载model-----------------&#39;)
m = Custommetrics()
mlp = models.load_model(os.path.join(config_dict[&#39;checkpoint_dir&#39;],
weights_file_name), custom_objects={&quot;r2_metric&quot;: m.r2_metric, &quot;geodesic_metric&quot;: m.geodesic_metric})
for layer1 in mlp.layers[1:]:
for layer2 in model.layers[1:]:
if layer1.name == layer2.name:
layer2.set_weights(layer1.get_weights())
layer2.trainable = False

最后，我训练模型：
source_x = np.load(os.path.join(source_dir, &#39;x_train.npy&#39;))
target_x = np.load(os.path.join(target_dir, &#39;x_train.npy&#39;))
source_y_train = np.load(os.path.join(target_dir, &#39;y_train.npy&#39;))
history = model.fit([source_x, target_x],
source_y_train,
batch_size=32, epochs=10,
validation_data=([source_x, target_x], source_y_train),
callbacks=[cp_callback])

当我打印 model.summary() 时，我得到以下内容：
模型：“model”
层（类型）输出形状参数 # 连接到
input_1（输入层）[（无，40，40，40，1）] 0 [] 

flatten（Flatten）（无，64000）0 [&#39;input_1[0][0]&#39;] 

fc1（密集）（无，128）8192128 [&#39;flatten[0][0]&#39;] 

fc2（密集）（无，128）16512 [&#39;fc1[0][0]&#39;] 

fc3（密集）（无，128）16512 [&#39;fc2[0][0]&#39;] 

fc4（密集）（无，128）16512 [&#39;fc3[0][0]&#39;]

input_2 (InputLayer) [(None, 40, 40, 40, 1)] 0 [] 

fc5 (Dense) (None, 6) 774 [&#39;fc4[0][0]&#39;] 

总参数：8242438 (31.44 MB)
可训练参数：0 (0.00 字节)
不可训练参数：8242438 (31.44 MB)

已成功训练。但我有两个问题：

为什么摘要中没有包含目标分支 (fc6-10)？
我应该如何逐层添加 MMD（即 fc1 和 fc6、fc2 和 fc7、fc3 和 fc8、fc4 和 fc9）？
]]></description>
      <guid>https://stackoverflow.com/questions/78965799/implementing-domain-adaptation-network-dan-for-a-custom-mlp-keras-model</guid>
      <pubDate>Mon, 09 Sep 2024 13:49:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 进行自然语言处理</title>
      <link>https://stackoverflow.com/questions/78965606/natural-language-processing-using-r</link>
      <description><![CDATA[我对自然语言处理很感兴趣。我有一个路线图，比如获取韩语（任何语言，英语除外）的 PDF，然后在应用一些操作（例如：数据导入、清理、标记化、删除停用词）后，我想将该语言翻译成英语。
我正在使用 R。您能建议我路线图需要进行哪些更改以及哪些库很重要吗？]]></description>
      <guid>https://stackoverflow.com/questions/78965606/natural-language-processing-using-r</guid>
      <pubDate>Mon, 09 Sep 2024 13:03:37 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中如何避免 RGB 图像训练数据的内存泄漏？</title>
      <link>https://stackoverflow.com/questions/78965096/how-to-avoid-memory-leaks-in-training-data-for-rgb-images-in-reinforcement-learn</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78965096/how-to-avoid-memory-leaks-in-training-data-for-rgb-images-in-reinforcement-learn</guid>
      <pubDate>Mon, 09 Sep 2024 10:51:59 GMT</pubDate>
    </item>
    <item>
      <title>图像分类中的错误预测</title>
      <link>https://stackoverflow.com/questions/78964898/incorrect-prediction-in-image-classification</link>
      <description><![CDATA[我正在使用 Kaggel 数据集 https://www.kaggle.com/datasets/praveengovi/coronahack-chest-xraydataset/data 来开发图像分类模型，但我没有获得所需的预测准确度，我的模型只学习一个类并只预测一个类，准确度不佳。
这是我的 CNN 类
import torch.nn as nn
import torch.nn. functional as F
import torch

class XrayCNN(nn.Module):
def __init__(self,num_classes=2):
super(XrayCNN,self).__init__()

#convolution layer
self.conv1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,padding=1)
self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1)
self.conv3 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3, padding=1)
self.conv4 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1)
self.conv5 = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3, padding=1)

#池化层
self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

#连接层
self.fc1 = nn.Linear(512*9*9,1024)
self.fc2 = nn.Linear(1024,512)
self.fc3 = nn.Linear(512,num_classes)

#dropout
self.dropout = nn.Dropout(0.25)

self.bn1 = nn.BatchNorm2d(32)
self.bn2 = nn.BatchNorm2d(64)
self.bn3 = nn.BatchNorm2d(128)
self.bn4 = nn.BatchNorm2d(256)
self.bn5 = nn.BatchNorm2d(512)

def forward(self,x):
x = self.pool(F.relu(self.bn1(self.conv1(x))))
x = self.pool(F.relu(self.bn2(self.conv2(x))))
x = self.pool(F.relu(self.bn3(self.conv3(x))))
x = self.pool(F.relu(self.bn4(self.conv4(x))))
x = self.pool(F.relu(self.bn5(self.conv5(x))))

# print(x.shape)

size = x.size()[1:]
num_features = 1
for s in size:
num_features *= s

x = x.view(-1,num_features)

x = F.relu(self.fc1(x))
x = self.dropout(x)
x = F.relu(self.fc2(x))
x = self.dropout(x)

x = self.fc3(x)

return x

num_classes = 2
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; )
model = XrayCNN(num_classes=num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

我试图预测 Normal 和 Pnemonia 图像，但它将所有图像都预测为 Pnemonia，在我的数据集中，我将 Normal 和 Pnemonoia 图像分别下采样为 400 张图像进行分类，但它不起作用，即使 val 准确率为 87%，它在测试数据上也惨不忍睹
## 数据增强
image_transform =变换。Compose（[
变换。调整大小（（300,300）），
变换。随机水平翻转（p=0.5），
变换。随机垂直翻转（p=0.5），
变换。随机旋转（度=35），
变换。灰度（num_output_channels=1），
变换。ToTensor（），
变换。随机Affine（度=0，平移=（0.2,0.2）），
变换。正则化（平均值=[0.485]，标准差=[0.229]）

])

class CustomXrayDataset（Dataset）：
def __init__（self，image_label_dict，train_dir，transforms=None）：
self.image_label_dict = image_label_dict
self.train_dir = train_dir
self.transforms = image_transform
self.image_list = list(image_label_dict.keys())
#self.label_list = list(image_label_dict.values())

def __len__(self):
return len(self.image_list)

def label_encoding(self, label):
label_dict = {
&#39;Normal&#39;:0,
# &#39;COVID-19&#39;:1,
&#39;Pnemonia&#39;:1

}
return label_dict[label]

def __getitem__(self,idx):
image_name = self.image_list[idx]

image = Image.open(os.path.join(self.train_dir,image_name))
label = self.image_label_dict[image_name]
label = self.label_encoding(label)
if self.transforms:
image = self.transforms(image)
return image,label

想知道哪里出了问题，我开始一个简单的模型，但添加了层以期解决分类问题。
我的 Kaggle Notebook 链接：https://www.kaggle.com/code/siddharthsehgal/notebooka8a9efe8fa
我在 Collab 中运行了它]]></description>
      <guid>https://stackoverflow.com/questions/78964898/incorrect-prediction-in-image-classification</guid>
      <pubDate>Mon, 09 Sep 2024 09:55:19 GMT</pubDate>
    </item>
    <item>
      <title>将 PyTorch 模型导出到 ONNX</title>
      <link>https://stackoverflow.com/questions/78964817/export-pytorch-model-to-onnx</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78964817/export-pytorch-model-to-onnx</guid>
      <pubDate>Mon, 09 Sep 2024 09:33:30 GMT</pubDate>
    </item>
    <item>
      <title>是否可以通过SVM选出距离超平面最远的那些错误分类的样本？</title>
      <link>https://stackoverflow.com/questions/78964423/is-it-possible-to-select-those-misclassified-samples-by-svm-that-are-farthest-fr</link>
      <description><![CDATA[我正在使用 R 中的 e1071 库中的 SVM 模型，我的问题是我是否可以识别模型错误分类且距离 SVM 超平面最远的样本。
我的代码如下：
best_model &lt;- list(
model = svm(x = omicDataReduced[, -which(colnames(omicDataReduced) == classVariable)],
y = omicDataReduced[[classVariable]],
kernel = best_kernel,
cost = best_C,
probability = TRUE,
decision.values = TRUE),
error = best_error,
cost = best_C,
kernel = best_kernel
)

我的数据集有 790 个样本和 18,710 个预测变量。
编辑：
我发现使用“decision.values”属性，您可以获得到超平面的距离。我现在的问题是：我如何检查样本是否超出了边界？
这是我目前拥有的代码：
pred &lt;- predict(best_model$model, omicDataReduced[, -which(colnames(omicDataReduced) == classVariable)], decision.value=T)

decision_values &lt;- attr(pred, &quot;decision.values&quot;)

misclassified_indices &lt;- which(as.numeric(pred) != as.numeric(omicDataReduced[[classVariable]]))

misclassified_decision_values &lt;- abs(decision_values[misclassified_indices])

selected_indices &lt;- misclassified_indices[order(misclassified_decision_values, increasing = TRUE)][1:round(diagnosticChangeProbability * length(misclassified_indices))]
]]></description>
      <guid>https://stackoverflow.com/questions/78964423/is-it-possible-to-select-those-misclassified-samples-by-svm-that-are-farthest-fr</guid>
      <pubDate>Mon, 09 Sep 2024 07:52:54 GMT</pubDate>
    </item>
    <item>
      <title>具有主导变量或仅 1 个变量的 ML 模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78964182/ml-model-with-a-dominant-or-just-1-variable</link>
      <description><![CDATA[您能否解释使用具有主导变量的机器学习模型（这些主导变量可以解释目标中的大部分方差）或仅使用一个变量构建的模型的效果？具体来说，我有兴趣了解与这些方法相关的潜在缺点、偏见或局限性。]]></description>
      <guid>https://stackoverflow.com/questions/78964182/ml-model-with-a-dominant-or-just-1-variable</guid>
      <pubDate>Mon, 09 Sep 2024 06:46:08 GMT</pubDate>
    </item>
    <item>
      <title>为什么 nn.Linear(in_features, out_features) 在 PyTorch 中使用形状为 (out_features, in_features) 的权重矩阵？</title>
      <link>https://stackoverflow.com/questions/78963755/why-does-nn-linearin-features-out-features-use-a-weight-matrix-of-shape-out</link>
      <description><![CDATA[我试图理解为什么 PyTorch 的 nn.Linear(in_features, out_features) 层的权重矩阵具有形状 (out_features, in_features) 而不是 (in_features, out_features)。
从基本矩阵乘法的角度来看，具有形状 (in_features, out_features) 似乎可以消除在乘法过程中转置权重矩阵的需要。例如，对于形状为 (batch_size, in_features) 的输入张量 x，与形状为 (in_features, out_features) 的权重矩阵相乘将直接产生形状为 (batch_size, out_features) 的输出，而无需转置操作。
但是，PyTorch 将权重矩阵定义为 (out_features, in_features)，这意味着它在前向传递过程中会被转置。这种设计有什么好处？它如何与线性代数和神经网络实现的更广泛原则保持一致？这种选择背后是否有任何效率或一致性考虑使其更可取？]]></description>
      <guid>https://stackoverflow.com/questions/78963755/why-does-nn-linearin-features-out-features-use-a-weight-matrix-of-shape-out</guid>
      <pubDate>Mon, 09 Sep 2024 03:08:49 GMT</pubDate>
    </item>
    <item>
      <title>结合语音、面部表情和文本数据进行实时心理健康监测的挑战 [关闭]</title>
      <link>https://stackoverflow.com/questions/78963600/challenges-in-combining-speech-facial-expression-and-text-data-for-real-time-m</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78963600/challenges-in-combining-speech-facial-expression-and-text-data-for-real-time-m</guid>
      <pubDate>Mon, 09 Sep 2024 01:01:00 GMT</pubDate>
    </item>
    <item>
      <title>安装 Snap ML 时出错 | 未找到与 snapml 匹配的发行版 [关闭]</title>
      <link>https://stackoverflow.com/questions/78960743/error-on-installing-snap-ml-no-matching-distribution-found-for-snapml</link>
      <description><![CDATA[我在 MacOS 机器上（2.6 GHz Intel Core i7）
我试图安装库“snapml”，但一直出错。
有人知道问题可能是什么吗？
我的终端上有几行：
danilovpavel@Pauls-MacBook-Pro ~ % pip install snapml
错误：找不到满足要求 snapml 的版本（来自版本：无）
错误：未找到与 snapml 匹配的发行版]]></description>
      <guid>https://stackoverflow.com/questions/78960743/error-on-installing-snap-ml-no-matching-distribution-found-for-snapml</guid>
      <pubDate>Sat, 07 Sep 2024 17:39:40 GMT</pubDate>
    </item>
    <item>
      <title>自定义模型聚合器 TensorFlow Federated</title>
      <link>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</link>
      <description><![CDATA[我正在尝试使用 TensorFlow Federated，使用 FedAvg 算法模拟训练过程。
def model_fn():
# 包装 Keras 模型以用于 TensorFlow Federated
keras_model = get_uncompiled_model()

# 对于联合过程，模型必须是未编译的
return tff.learning.models. functional_model_from_keras(
keras_model,
loss_fn=tf.keras.losses.BinaryCrossentropy(),
input_spec=(
tf.TensorSpec(shape=[None, X_train.shape[1]], dtype=tf.float32),
tf.TensorSpec(shape=[None], dtype=tf.int32)
),
metrics_constructor=collections.OrderedDict(
accuracy=tf.keras.metrics.BinaryAccuracy,
precision=tf.keras.metrics.Precision,
recall=tf.keras.metrics.Recall,
false_positives=tf.keras.metrics.FalsePositives,
false_negatives=tf.keras.metrics.FalseNegatives,
true_positives=tf.keras.metrics.TruePositives,
true_negatives=tf.keras.metrics.TrueNegatives
)
)

trainer = tff.learning.algorithms.build_weighted_fed_avg(
model_fn= model_fn(),
client_optimizer_fn=client_optimizer,
server_optimizer_fn=server_optimizer
)

我想使用自定义权重来聚合客户端的更新，而不是使用它们的样本。我知道 tff.learning.algorithms.build_weighted_fed_avg() 有一个名为 client_weighting 的参数，但唯一接受的值来自类 tff.learning.ClientWeighting，它是一个枚举。
因此，唯一的方法似乎是编写自定义 WeightedAggregator。我尝试按照本教程中的说明编写无加权聚合器，但我无法将其转换为加权聚合器。
这是我尝试做的：
class CustomWeightedAggregator(tff.aggregators.WeightedAggregationFactory):
def __init__(self):
pass

def create(self, value_type, weight_type):

@tff.federated_computation
def initialize(self):
return tff.federated_value(0.0, tff.SERVER)

@tff.federated_computation(
lambda self: self.initialize.type_signature.result,
tff.FederatedType(tff.types.to_type(np.float32), tff.CLIENTS),
tff.FederatedType(tff.types.to_type(np.float32), tff.CLIENTS)
)
def next(self, state, value, weight):
aggregate_value = tff.federated_map(custom_weighted_aggregate, (value, weight))
return state,aggregate_value

@property
def is_weighted(self):
return True

return tff.templates.AggregationProcess(self.initialize, self.next)

我必须更改什么才能使其工作并使用自定义权重？]]></description>
      <guid>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</guid>
      <pubDate>Mon, 05 Aug 2024 16:06:48 GMT</pubDate>
    </item>
    <item>
      <title>如何使用预训练的 BERT 词嵌入向量来微调（初始化）其他网络？</title>
      <link>https://stackoverflow.com/questions/65802782/how-to-use-pretrained-bert-word-embedding-vector-to-finetune-initialize-other</link>
      <description><![CDATA[当我以前使用 textcnn 进行分类工作时，我曾使用 Word2Vec 和 fasttext 等预训练词嵌入对 textcnn 进行微调。我使用这个过程：

在 textcnn 中创建一个嵌入层
通过 Word2Vec 或 fasttext 加载这次使用的单词的嵌入矩阵
由于嵌入层的向量值在训练过程中会发生变化，因此网络正在进行微调。

最近我也想尝试 BERT 来做这件事。我想，“由于使用 BERT 预训练嵌入来初始化其他网络的嵌入层和微调应该没有什么不同，所以应该很容易！”但事实上昨天我试了一整天还是不行。
我发现的事实是，由于BERT的嵌入是上下文嵌入，特别是在提取词嵌入时，每个句子中每个词的向量都会有所不同，所以似乎没有办法像往常一样使用该嵌入来初始化另一个网络的嵌入层……
最后，我想到了一种“微调”的方法，步骤如下：

首先，不要在textcnn中定义嵌入层。
在网络训练部分，我没有使用嵌入层，而是首先将序列标记传递给预训练的BERT模型并获取每个句子的词嵌入。
将2中的BERT词嵌入放入textcnn并训练textcnn网络。

通过这种方法我终于能够进行训练了，但认真想想，我觉得我根本没有进行微调……
因为正如你所看到的，每次我开始新的训练循环时，从 BERT 生成的词嵌入总是相同的向量，所以只需将这些不变的向量输入到 textcnn 中，就不会让 textcnn 进行微调，对吧？

更新：
我想到了一种新方法，可以使用 BERT 嵌入并一起“训练”BERT 和 textcnn。
我的部分代码如下：
 BERTmodel = AutoModel.from_pretrained(&#39;bert- 
base-uncased&#39;,output_hidden_​​states=True).to(device)
TextCNNmodel = TextCNN(EMBD_DIM, CLASS_NUM, KERNEL_NUM, 
KERNEL_SIZES).to(device)
optimizer = torch.optim.Adam(TextCNNmodel.parameters(), lr=LR)
loss_func = nn.CrossEntropyLoss()

 for epoch in range(EPOCH):
TextCNNmodel.train()
BERTmodel.train()
for step, (token_batch, seg_batch, y_batch) in enumerate(train_loader):
token_batch = token_batch.to(device)
y_batch = y_batch.to(device)

BERToutputs = BERTmodel(token_batch)
# 我想使用倒数第二个隐藏层作为嵌入，因此
x_batch = BERToutputs[2][-2]

output = TextCNNmodel(x_batch)
output = output.squeeze()
loss = loss_func(output, y_batch)

optimizer.zero_grad()
loss.backward()
optimizer.step()

我认为通过在获取嵌入时启用 BERTmodel.train() 并删除 torch.no_grad()，损失梯度可以反向到 BERTmodel， TextCNNmodel的训练过程也很顺利。
为了以后使用这个模型，我保存了TextCNNmodel和BERTmodel的参数。
然后为了实验BERTmodel是否真的被训练和改变了，在另一个程序中我加载了BERTModel，并输入一句话来测试BERTModel是否真的被训练了。
但是我发现原始的&#39;bert-base-uncased&#39;模型和我的&#39;BERTmodel&#39;的输出（嵌入）是一样的，这很令人失望......
我真的不知道为什么BERTmodel部分没有变化......]]></description>
      <guid>https://stackoverflow.com/questions/65802782/how-to-use-pretrained-bert-word-embedding-vector-to-finetune-initialize-other</guid>
      <pubDate>Wed, 20 Jan 2021 03:43:21 GMT</pubDate>
    </item>
    <item>
      <title>如何暂时禁用 MLFlow？</title>
      <link>https://stackoverflow.com/questions/61088651/how-to-disable-mlflow-temporarily</link>
      <description><![CDATA[是否可以暂时禁用 MLFlow 以调试代码或添加新功能？如果不禁用，它会保存大量实际上无用的执行或未完成的执行。
或者最好的策略是使用不调用 mlflow.start_run() 的类似代码？]]></description>
      <guid>https://stackoverflow.com/questions/61088651/how-to-disable-mlflow-temporarily</guid>
      <pubDate>Tue, 07 Apr 2020 20:14:15 GMT</pubDate>
    </item>
    <item>
      <title>CNN 的面部表情识别数据准备</title>
      <link>https://stackoverflow.com/questions/37452073/facial-expression-recognition-data-preparation-for-cnn</link>
      <description><![CDATA[我正在研究通过深度学习，特别是 CNN 进行面部表情识别。我对准备和/或预处理数据有一些疑问。
我有正面面部表情的分段视频（例如，根据某人的注释，2-3 秒的视频中某人表达了快乐情绪）。
注意：我的参与者表现出的表情强度很低（不是夸张的表情/微表情）
一般问题：现在，我应该如何准备数据以使用 CNN 进行训练（我有点倾向于使用深度学习库 TensorFlow）？
问题 1：我读过一些基于深度学习的面部表情识别 (FER) 论文，这些论文建议取该表情的峰值（很可能是单个图像）并将该图像用作训练数据的一部分。我如何知道表情的峰值？我的依据是什么？如果我要拍摄单张图片，参与者表现出的微妙表情的一些重要帧会不会丢失？
问题 2：或者，在 OpenCV 中执行分段视频以检测（例如 Viola-Jones）、裁剪并保存每帧的面部，并将这些图像与相应的标签一起用作我的训练数据的一部分，这样是否也正确？我猜有些面部帧是多余的。但是，由于我们知道数据中的参与者表现出低强度的表情（微表情），因此面部的一些动作也可能很重要。]]></description>
      <guid>https://stackoverflow.com/questions/37452073/facial-expression-recognition-data-preparation-for-cnn</guid>
      <pubDate>Thu, 26 May 2016 05:11:23 GMT</pubDate>
    </item>
    </channel>
</rss>