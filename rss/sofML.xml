<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 19 Jun 2024 12:28:24 GMT</lastBuildDate>
    <item>
      <title>了解 ICML 2013 鲸鱼挑战赛 - 露脊鲸重现中的数据泄露文章</title>
      <link>https://stackoverflow.com/questions/78642417/understanding-the-data-leakage-article-on-the-icml-2013-whale-challenge-right</link>
      <description><![CDATA[我在尝试进一步了解数据泄露时偶然发现了这篇文章。 https://www.kaggle.com/competitions/the-icml-2013-whale-challenge-right-whale-redux/discussion/4865
虽然这里确实突出了要点，但我仍然不明白数据泄露发生在哪里。需要一些帮助来了解这篇文章如何识别数据泄露吗？]]></description>
      <guid>https://stackoverflow.com/questions/78642417/understanding-the-data-leakage-article-on-the-icml-2013-whale-challenge-right</guid>
      <pubDate>Wed, 19 Jun 2024 12:05:53 GMT</pubDate>
    </item>
    <item>
      <title>机器学习混淆矩阵 矩阵解释</title>
      <link>https://stackoverflow.com/questions/78641974/machine-learning-confusion-matrix-explanation-of-matrix</link>
      <description><![CDATA[`下面显示了几个混淆矩阵。TP、FP、FN 和 TN 已标记（使用 ChatGPT）
我知道对角线代表 TP。但是 FP、FN 和 TN 又如何呢？ TN 被识别（或区分）了吗？
请用矩阵解释您的答案
# [[TP=12 FP=0 FN=0]
# [FN=0 TP=7 FP=2]
# [FN=0 FN=0 TP=9]]

# [[TP=12 FP=1 FP=2]
# [FN=3 TP=7 FP=4]
# [FN=5 FP=1 TP=9]]

# [[TP_0=12 FP_1=1 FP_2=2 TN_3=0]
# [FN_0=3 TP_1=7 FP_2=4 FP_3=1]
# [FN_0=5 FP_1=1 TP_2=9 FP_3=2]
# [TN_0=0 FN_1=2 FN_2=3 TP_3=11]]

我试过 ChatGPT。
它用 TP、FP、FN 和 TN 标记矩阵。但没有得到适当的解释
解释我们如何在混淆矩阵中标记 FP、FN 和 TN`]]></description>
      <guid>https://stackoverflow.com/questions/78641974/machine-learning-confusion-matrix-explanation-of-matrix</guid>
      <pubDate>Wed, 19 Jun 2024 10:32:46 GMT</pubDate>
    </item>
    <item>
      <title>PyGAD 遗传手段 - 惩罚簇大小</title>
      <link>https://stackoverflow.com/questions/78641636/pygad-genetic-means-punishing-cluster-size</link>
      <description><![CDATA[我正在研究一个问题，从我之前的研究来看，这个问题困扰了我所在行业的许多人。在市政工程中，特别是光纤电缆，房屋连接 (HC) 需要分组为连接到网络分配器 (ND) 的集群。直观地讲，人们可能会认为这对于简单的聚类算法（例如 kmeans）来说是一个问题，然而，问题出现在基础设施强加的每 ND 20 个 HC 的限制上。如果某个区域的 HC 过于密集，kmeans 等会构建超出此限制的集群。
据我所知，唯一可用的商业解决方案是 ArcGIS 的构建平衡区域算法，但是，为了 FOSS，我个人希望避免使用 ArcGIS。不过，值得庆幸的是，ESRI 确实提供了有关 bbs 如何工作的背景信息，这就是我一直在阅读遗传算法并最终使用 Gad, A.F. 的 PyGAD 的原因。他的教程对我帮助很大，但是，我正在努力调整适应度函数以满足我的需求，即惩罚导致集群超过 20 个种群的解决方案。
原始函数如下，并根据其产生的集群密度对解决方案进行评分，即它相当于 kmeans：
def fitness_func(solution, solution_idx):
cluster_centers, all_clusters_dists, cluster_indices, clusters, clusters_sum_dist = cluster_data(solution, solution_idx)

fitness = 1.0 / (np.sum(clusters_sum_dist) + 0.00000001)

返回 fitness

最佳解决方案是 [ 6.76857432 87.25666069 82.82788371 84.71676014 17.01672128 41.12036676
6.71771791 9.91987939 83.17725224 28.87028453 87.64197362 53.89675135
29.77371507 64.36472386 58.77172181 59.66950147 40.07732671 83.47739475
59.29050269 13.28871487]
最佳解决方案的适应度为 0.0004500494908839888
100 代后找到的最佳解决方案

根据 Gad, A.F. 生成的 K-means-clusters
我本想为超过 20 的簇大小添加一个惩罚项，但是，我这样做的方式似乎会平等地惩罚所有染色体（我希望我正确使用这个术语，我对遗传算法仍然有点陌生）。
def capped_clusters(solution, solution_idx):
cluster_centers, all_clusters_dists, cluster_indices, clusters, clusters_sum_dist = cluster_data(solution, solution_idx)

fitness = 1.0 / (np.sum(clusters_sum_dist) + 0.00000001) #0 为最优

if any(element &gt; 20 for element in [len(clu​​ster) for cluster in clusters]):
fitness = 1
else:
pass

return fitness

最佳解决方案是 [66.16381384 94.42791405 34.78040731 64.66917082 90.50249179 24.68926304
7.54327271 66.90232116 55.22703763 88.59661014 1.87161791 93.35067003
39.58200881 57.61364249 65.91865205 16.62963165 4.22783108 67.18573653
15.57759318 9.46596451]
最佳解决方案的适应度为 1
0 代后找到最佳解决方案

我如何有选择地惩罚超过 20 的聚类大小，以推动算法朝着有利于 20 个以下成员的聚类问题解决方案的方式发展？
注意：我已降级到 PyGAD 2.10.0，以便与上面链接中的教程代码兼容。这要求我手动从源代码中删除对过时的 numpy.int 和 numpy.float 的任何提及。
干杯，非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/78641636/pygad-genetic-means-punishing-cluster-size</guid>
      <pubDate>Wed, 19 Jun 2024 09:27:12 GMT</pubDate>
    </item>
    <item>
      <title>GPU 张量上的内向旋转环面</title>
      <link>https://stackoverflow.com/questions/78641329/inwards-spinning-torus-on-gpu-tensors</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78641329/inwards-spinning-torus-on-gpu-tensors</guid>
      <pubDate>Wed, 19 Jun 2024 08:28:38 GMT</pubDate>
    </item>
    <item>
      <title>如何使用树提升模型处理多输出回归的 3d 输入数据？</title>
      <link>https://stackoverflow.com/questions/78641199/how-to-handle-3d-input-data-with-tree-boosting-models-for-multi-output-regressio</link>
      <description><![CDATA[我有 32 种商品 x 27 种特征 x 3000 个时间步长的数据用于股票预测。出于某种原因，我必须使用各种 树提升模型（lightgbm、xgboost、adaboost）来比较回归任务的性能：预测单个时间步长上每只股票的回报率，并对之前时间步长的样本进行训练。预期的数据流将是：在 n 个时间步长上训练的模型，输出单个时间步长（例如 t+2）上每种商品的回报率向量（32 个元素向量）。但是，我不确定如何构造数据以进行有效的训练。
现在，我沿着商品和特征维度展平数据以创建 3000x864 数据框，但我认为这不是可行的方法，因为我怀疑我丢失了一些信息。
我使用 sklearn 的 MultiOutputRegressor 来适应 lgbm，但 mse 约为 0.5，对于预测在 -1 和 1 之间标准化的值非常不利，因为数据包含许多易于预测的 0。
这种展平方法是处理 3d 数据的正确方法吗？还是我需要使用其他方法？]]></description>
      <guid>https://stackoverflow.com/questions/78641199/how-to-handle-3d-input-data-with-tree-boosting-models-for-multi-output-regressio</guid>
      <pubDate>Wed, 19 Jun 2024 08:00:34 GMT</pubDate>
    </item>
    <item>
      <title>机器学习——对数据进行反复训练会导致过度拟合吗？</title>
      <link>https://stackoverflow.com/questions/78640262/machine-learning-does-repeatedly-training-on-data-cause-overfit</link>
      <description><![CDATA[所以我有一个已经训练好的机器学习模型，没有过拟合或欠拟合，这个模型是完美的，可以正确检测物体。但是，之后我尝试如果我再次反复训练这个模型，结果发生了过拟合，在一个已经完美的模型上反复训练模型会影响模型吗？所以原本正常的变成了过拟合。
我希望得到关于我的情况的解释]]></description>
      <guid>https://stackoverflow.com/questions/78640262/machine-learning-does-repeatedly-training-on-data-cause-overfit</guid>
      <pubDate>Wed, 19 Jun 2024 02:24:34 GMT</pubDate>
    </item>
    <item>
      <title>神经网络近似代码中的值错误</title>
      <link>https://stackoverflow.com/questions/78640203/value-error-in-code-for-neural-network-approximation</link>
      <description><![CDATA[import numpy as np
import matplotlib.pyplot as plt

# 定义目标函数 f(x) = sin(x)^2
def f(x):
return np.sin(x)**2

# 生成训练数据集
np.random.seed(0) # 为了可重复性
num_samples = 100
x_train = np.linspace(0, 2*np.pi, num_samples)
y_train = f(x_train)

# 初始化系数
n = len(x_train)
a = np.random.randn(n) # 线性组合系数
b = np.random.randn() # 常数项
m = 10 # 神经元数量
alphas = np.random.randn(m, n) # ReLU 神经元的权重
ts = np.random.randn(m) # ReLU 阈值神经元
cs = np.random.randn(m) # ReLU 神经元的系数

# 实现神经网络近似
def relu(x):
return np.maximum(0, x)

def f_N(x):
return np.dot(a, x) + b + np.sum(cs[:, np.newaxis] * relu(np.dot(alphas, x) - ts[:, np.newaxis]), axis=0)

# 定义损失函数
def loss_function(a, b, cs):
predictions = f_N(x_train)
return np.mean((predictions - y_train)**2)

# 梯度下降优化
def gradient_descent(learning_rate=0.01, num_iterations=1000):
loss = []
for i in range(num_iterations):
# 计算梯度
predictions = f_N(x_train)
误差 = 预测 - y_train

grad_a = np.outer(误差, x_train) / num_samples
grad_b = np.mean(误差)

relu_grad = np.dot(cs[:, np.newaxis] * (np.dot(alphas, x_train) - ts[:, np.newaxis] &gt; 0), alphas)
grad_cs = np.sum(error[:, np.newaxis] * relu_grad, axis=0) / num_samples

# 更新系数
a -= learning_rate * grad_a
b -= learning_rate * grad_b
cs -= learning_rate * grad_cs

# 裁剪或投影 cs 以确保它们保持在界限内
cs = np.clip(cs, -1, 1) # 在 -1 和 1 之间裁剪的示例

# 计算当前损失
current_loss = loss_function(a, b, cs)
loss.append(current_loss)

# 打印损失以进行监控
if i % 100 == 0:
print(f&quot;Iteration {i}, Loss: {current_loss}&quot;)

return loss

# 执行梯度下降
losses = gradient_descent()

# 绘制结果
plt.figure(figsize=(12, 6))

# 绘制目标函数
plt.plot(x_train, y_train, label=&#39;目标函数：$f(x) = \sin(x)^2$&#39;, color=&#39;blue&#39;)

# 绘制神经网络近似
y_pred = f_N(x_train)
plt.plot(x_train, y_pred, label=&#39;神经网络近似&#39;, linestyle=&#39;--&#39;, color=&#39;red&#39;)

plt.title(&#39;神经网络近似 vs 目标函数&#39;)
plt.xlabel(&#39;x&#39;)
plt.ylabel(&#39;y&#39;)
plt.legend()
plt.grid(True)
plt.show()

# 绘制损失曲线
plt.figure(figsize=(10, 5))
plt.plot(losses, label=&#39;训练Loss&#39;)
plt.title(&#39;Training Loss over Iterations&#39;)
plt.xlabel(&#39;Iteration&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()
plt.grid(True)
plt.show()


代码的目标如下：
定义目标函数 f(x) = sin(x)^2，并生成训练数据集 {(x_i, y_i)}，其中 y_i = f(x_i)，针对一系列 x_i 值。
用随机值初始化神经网络系数 a、b 和 c_i，并从标准高斯分布生成样本 (alpha_1,t_1),...,(alpha_m,t_m)。
实现神经网络近似 fN(x) = a^Tx+b+sum{i=1}^{m} ci\sigma(alpha{i}^T-t_i) 如图所示，使用初始化系数。
定义损失函数 L(a, b, c_1, ..., c_m)，用于测量神经网络近似值 f_N(x_i) 与训练数据点 (x_i, y_i) 的目标函数 f(x_i) 之间的差异。例如，您可以使用均方误差：L(a, b, c_1, ..., c_m) = Σ_i (f_N(x_i) - y_i)^2
使用线性回归最小化关于系数 a、b 和 c_i 的损失函数 L。这可以使用梯度下降或其他优化算法来完成。
在优化过程中，通过裁剪或投影系数到边界定义的可行区域，确保系数保持在提供的边界内。
我收到以下错误，无法调试它：

-----------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[30]，第 68 行
65 returnloss
67 # 执行梯度下降
---&gt; 68loss = gradient_descent()
70 # 绘制结果
71 plt.figure(figsize=(12, 6))

Cell In[30]，第 41 行
38 for i in range(num_iterations):
39 # 计算梯度
40 predictions = f_N(x_train)
---&gt; 41 错误 = 预测 - y_train
43 grad_a = np.outer(error, x_train) / num_samples
44 grad_b = np.mean(error)

ValueError：操作数不能与形状 (10,) (100,) 一起广播

如果有人能帮助我，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78640203/value-error-in-code-for-neural-network-approximation</guid>
      <pubDate>Wed, 19 Jun 2024 01:52:20 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络 (CNN) 在决策中使用黑色背景 - LIME</title>
      <link>https://stackoverflow.com/questions/78640044/convolutional-neural-network-cnn-using-black-background-in-decision-making-l</link>
      <description><![CDATA[我是一名正在做学校项目的学生，需要帮助。
我的二元分类卷积神经网络在验证数据上具有非常高的准确率 (&gt;96%)，在测试数据集上的表现也同样出色。然而，当我使用 LIME 可视化图像中对其决策很重要的部分时，它往往会突出显示背景。所以我的问题是：
为什么会这样，以前有人见过吗？
当它在做决定时实际上是看着黑色面具时，它是如何达到 96% 的准确率的？
我在图像上应用黑色面具的原因是，我得到的整个数据集具有完全相同的背景，即白色滚轮，并且正如您从我上传的其中一张图片中看到的那样，该模型在决策过程中严重依赖滚轮，因此我将背景预处理为完全黑色 (0, 0, 0)RGB 像素，但现在模型似乎以某种方式使用了它。
我只是被难住了，非常感谢任何帮助！
模型架构
滚轮问题示例
我尝试了各种架构，其中一些使用 keras 层构建，甚至尝试了预训练的 ResNet50。我还改变了大多数重要的超参数，但行为仍然存在。如果有帮助，我可以提供任何细节。
我很感激任何提前提供的帮助！:)
]]></description>
      <guid>https://stackoverflow.com/questions/78640044/convolutional-neural-network-cnn-using-black-background-in-decision-making-l</guid>
      <pubDate>Wed, 19 Jun 2024 00:05:44 GMT</pubDate>
    </item>
    <item>
      <title>我是否需要在每个 Spark 节点上安装 PyTorch，还是只需要主节点来运行示例？</title>
      <link>https://stackoverflow.com/questions/78639778/do-i-need-pytorch-on-each-spark-node-or-just-the-master-for-running-examples</link>
      <description><![CDATA[我是否需要在 Apache Spark 集群的每个节点上安装 PyTorch，还是仅在主节点上安装它就足以顺利执行示例？
我尝试在 Apache Spark 集群上执行 PyTorch 示例，而无需在所有节点上安装 PyTorch，期望仅在主节点上安装就足以实现无缝执行。]]></description>
      <guid>https://stackoverflow.com/questions/78639778/do-i-need-pytorch-on-each-spark-node-or-just-the-master-for-running-examples</guid>
      <pubDate>Tue, 18 Jun 2024 21:47:13 GMT</pubDate>
    </item>
    <item>
      <title>如何处理 ADB ML Workload 中的内存不足错误</title>
      <link>https://stackoverflow.com/questions/78637717/how-to-handle-out-of-memory-error-in-adb-ml-workload</link>
      <description><![CDATA[我的预测数据框有近 200 个特征，因此我得到了 OOM（内存不足错误）。我正在使用 Azure databricks，我该如何处理这个错误？
我已经进行了特征选择以减少特征数量，但我仍然必须使用 100 多个特征。我尝试过广播，但没有解决问题]]></description>
      <guid>https://stackoverflow.com/questions/78637717/how-to-handle-out-of-memory-error-in-adb-ml-workload</guid>
      <pubDate>Tue, 18 Jun 2024 13:19:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 scikit-learn 在 Python 中对未标记数据实现层次聚类？</title>
      <link>https://stackoverflow.com/questions/78625589/how-to-implement-hierarchical-clustering-in-python-with-scikit-learn-for-unlabel</link>
      <description><![CDATA[我正在学习聚类，在尝试查找带有标记数据的数据库时遇到了一些问题，这对我来说是一个限制，因为我发现了非常有趣的未标记数据集。我读过各种无监督聚类技术，并想实现层次聚类。
我将数据加载到 pandas DataFrame 中，对数据进行标准化并应用层次聚类。然后我可视化了树状图，但我不确定如何解释结果或我是否使用了正确的参数。]]></description>
      <guid>https://stackoverflow.com/questions/78625589/how-to-implement-hierarchical-clustering-in-python-with-scikit-learn-for-unlabel</guid>
      <pubDate>Sat, 15 Jun 2024 04:03:59 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 Elixir Nx/Schorar 中进行 ELISA 分析？</title>
      <link>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</link>
      <description><![CDATA[我已阅读 Medium 上的文章 ELISA Analysis in Python。
上述文章使用 SciPy 的 curve_fit 函数根据 4 参数逻辑回归 (4PL) 模型找到近似曲线，如下所示：
from scipy.optimize import curve_fit

x = [1.95, 3.91, 7.381, 15.63, 31.25, 62.5, 125,250, 500, 1000]
y = [0.274, 0.347, 0.392, 0.420, 0.586, 1.115, 1.637, 2.227, 2.335, 2.372]

def log4pl(x, A, B, C, D):
return(((A - D) / (1.0 + ((x / C) ** B))) + D)

params, _ = curve_fit(log4pl, x, y)
A, B, C, D = params[0], params[1], params[2], params[3]

我想使用 Nx/Scholar 库。
可能吗？如果您能给我任何提示，我将不胜感激。

[更新]
快速浏览一下 Python scipy.optimize 源代码，似乎 curve_fit 在内部使用了 Fortran 的 MINPACK 库。
据我所知，没有简单的方法可以从 Elixir 使用 MINPACK。
因此，我得出结论，目前在 Elixir 中进行 ELISA 分析很困难。
欢迎提供任何其他信息。]]></description>
      <guid>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</guid>
      <pubDate>Sun, 02 Jun 2024 04:29:18 GMT</pubDate>
    </item>
    <item>
      <title>如何从句子中删除不带有积极或消极情绪的单词？</title>
      <link>https://stackoverflow.com/questions/71284177/how-to-remove-words-from-a-sentence-that-carry-no-positive-or-negative-sentiment</link>
      <description><![CDATA[我正在尝试一种基于情感分析的方法来分析 youtube 评论，但评论中很多时候都有像 mrbeast、tiger/&#39;s、lion/&#39;s、pewdiepie、james 等这样的词，这些词并没有给句子增添任何感觉。我已经尝试过 nltk 的 average_perception_tagger，但效果不佳，因为它给出了如下结果
我的输入：
&quot;mrbeast james lion tigers bad sad clickbait fight nice good&quot;

我的句子中需要的单词：
&quot;bad sad clickbait fight nice good&quot;

我使用 average_perception_tagger 得到的结果：
[(&#39;mrbeast&#39;, &#39;NN&#39;),
(&#39;james&#39;, &#39;NNS&#39;),
(&#39;lion&#39;, &#39;JJ&#39;),
(&#39;tigers&#39;, &#39;NNS&#39;),
(&#39;bad&#39;, &#39;JJ&#39;),
(&#39;sad&#39;, &#39;JJ&#39;),
(&#39;clickbait&#39;, &#39;NN&#39;),
(&#39;fight&#39;, &#39;NN&#39;),
(&#39;nice&#39;, &#39;RB&#39;),
(&#39;good&#39;, &#39;JJ&#39;)]


因此，如您所见，如果我删除 mrbeast 即 NN，clickbait、fight 等词也会被删除，最终会从该句子中删除表达。]]></description>
      <guid>https://stackoverflow.com/questions/71284177/how-to-remove-words-from-a-sentence-that-carry-no-positive-or-negative-sentiment</guid>
      <pubDate>Sun, 27 Feb 2022 10:59:01 GMT</pubDate>
    </item>
    <item>
      <title>如何计算空白标记预测的 transformers 损失？</title>
      <link>https://stackoverflow.com/questions/66518375/how-is-transformers-loss-calculated-for-blank-token-predictions</link>
      <description><![CDATA[我目前正在尝试实现一个转换器，但无法理解其损失计算。
我的编码器输入查找 batch_size=1 和 max_sentence_length=8，如下所示：
[[Das, Wetter, ist, gut, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]

我的解码器输入如下所示（德语到英语）：
[[&lt;start&gt;, The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;]]

假设我的转换器预测了这些类概率（仅显示类概率最高的类的单词）：
[[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]

现在我使用以下方法计算损失：
loss = categorical_crossentropy(
[[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]],
[[The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
)

这是计算损失的正确方法吗？我的转换器总是预测下一个单词的空白标记，我认为这是因为我的损失计算有误，在计算损失之前必须对空白标记进行一些处理。]]></description>
      <guid>https://stackoverflow.com/questions/66518375/how-is-transformers-loss-calculated-for-blank-token-predictions</guid>
      <pubDate>Sun, 07 Mar 2021 15:51:16 GMT</pubDate>
    </item>
    <item>
      <title>sample.int(m, k) 中的错误：无法抽取大于总体的样本</title>
      <link>https://stackoverflow.com/questions/25745215/error-in-sample-intm-k-cannot-take-a-sample-larger-than-the-population</link>
      <description><![CDATA[首先，我要说的是，我对机器学习、kmeans 和 r 还很陌生，这个项目是一种学习更多这方面知识的方法，也是将这些数据呈现给我们的 CIO 的一种方式，这样我就可以在开发新的帮助台系统时使用它。
我有一个 60K 行的文本文件。该文件包含教师在 3 年期间输入的帮助台工单的标题。
我想创建一个 r 程序，获取这些标题并创建一组类别。例如，与打印问题相关的术语，或与投影仪灯泡相关的一组术语。我使用 r 打开文本文档，清理数据，删除停用词和其他我认为不必要的词。我已获得频率 &gt;= 400 的所有术语列表，并将其保存到文本文件中。
但现在我想将（如果可以完成或合适）kmeans 聚类应用于同一数据集，看看我是否可以提出类别。
下面的代码包括将写出使用的术语列表 &gt;= 400 的代码。它位于末尾，并被注释掉。
library(tm) #加载文本挖掘库
library(SnowballC)
options(max.print=5.5E5) 
setwd(&#39;c:/temp/&#39;) #将 R 的工作目录设置为靠近我的文件的位置
ae.corpus&lt;-Corpus(DirSource(&quot;c:/temp/&quot;),readerControl=list(reader=readPlain))
summary(ae.corpus) #检查输入了什么
ae.corpus &lt;- tm_map(ae.corpus, tolower)
ae.corpus &lt;- tm_map(ae.corpus, removePunctuation)
ae.corpus &lt;- tm_map(ae.corpus, removeNumbers)
ae.corpus &lt;- tm_map(ae.corpus, stemDocument, language = &quot;english&quot;) 
myStopwords &lt;- c(stopwords(&#39;english&#39;), &lt;a very long list of other words&gt;)
ae.corpus &lt;- tm_map(ae.corpus, removeWords, myStopwords) 

ae.corpus &lt;- tm_map(ae.corpus, PlainTextDocument)

ae.tdm &lt;- DocumentTermMatrix(ae.corpus, control = list(minWordLength = 5))

dtm.weight &lt;- weightTfIdf(ae.tdm)

m &lt;- as.matrix(dtm.weight)
rownames(m) &lt;- 1:nrow(m)

#euclidian 
norm_eucl &lt;- function(m) {
m/apply(m,1,function(x) sum(x^2)^.5)
}
m_norm &lt;- norm_eucl(m)

results &lt;- kmeans(m_norm,25)

#list clusters

clusters &lt;- 1:25
for (i in clusters){
cat(&quot;Cluster &quot;,i,&quot;:&quot;,findFreqTerms(dtm.weight[results$cluster==i],400,&quot;\n\n&quot;))
}

#inspect(ae.tdm)
#fft &lt;- findFreqTerms(ae.tdm, lowfreq=400)

#write(fft, file = &quot;dataTitles.txt&quot;,
# ncolumns = 1,
# append = FALSE, sep = &quot; &quot;)

#str(fft)

#inspect(fft)

当我使用 RStudio 运行此程序时，我得到：
&gt; 结果 &lt;- kmeans(m_norm,25)


sample.int(m, k) 中的错误：当“replace = FALSE”时无法获取大于总体的样本

我不太清楚这是什么意思，而且我在网上没有找到很多关于此的信息。有什么想法吗？
TIA]]></description>
      <guid>https://stackoverflow.com/questions/25745215/error-in-sample-intm-k-cannot-take-a-sample-larger-than-the-population</guid>
      <pubDate>Tue, 09 Sep 2014 12:55:28 GMT</pubDate>
    </item>
    </channel>
</rss>