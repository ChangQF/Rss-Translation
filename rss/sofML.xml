<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 27 Jul 2024 15:15:05 GMT</lastBuildDate>
    <item>
      <title>如何将 Nural 函数 1.X Tensorflow 更改为 2.X Tensorflow？</title>
      <link>https://stackoverflow.com/questions/78800789/how-do-i-change-nural-function-1-x-tensorflow-to-2-x-tensorflow</link>
      <description><![CDATA[layer_1 = tf.nn.relu(tf.add(tf.matmul(x, w_1), b_1))
layer_1_b = tf.layers.batch_normalization(layer_1)
layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1_b, w_2), b_2))
layer_2_b = tf.layers.batch_normalization(layer_2)
layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2_b, w_3), b_3))
layer_3_b = tf.layers.batch_normalization(layer_3)
y = tf.nn.relu(tf.add(tf.matmul(layer_3, w_4), b_4))
g_q_action = tf.argmax(y, axis=1)

# 计算损失
g_target_q_t = tf.placeholder(tf.float32, None, name=&quot;target_value&quot;)
g_action = tf.placeholder(tf.int32, None, name=&#39;g_action&#39;)
action_one_hot = tf.one_hot(g_action, n_output, 1.0, 0.0, name=&#39;action_one_hot&#39;)
q_acted = tf.reduce_sum(y * action_one_hot, reduction_indices=1, name=&#39;q_acted&#39;)

g_loss = tf.reduce_mean(tf.square(g_target_q_t - q_acted), name=&#39;g_loss&#39;)
optim = tf.train.RMSPropOptimizer(learning_rate=0.001, motivation=0.95, epsilon=0.01).minimize(g_loss)

错误声明：
回溯（最近一次调用最后一次）：
文件“C:\Users\T\PycharmProjects\Project1\.venv\main_test.py”，第 139 行，位于&lt;module&gt;
layer_1 = tf.compat.v1.nn.relu(tf.add(tf.matmul(x, w_1), b_1))
^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\T\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\ops\weak_tensor_ops.py&quot;，第 142 行，在包装器中
return op(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\T\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\util\traceback_utils.py&quot;，第 153 行，在 error_handler 中
raise e.with_traceback(filtered_tb)来自 None
文件“C:\Users\T\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\framework\ops.py”，第 1037 行，位于 _create_c_op
引发 ValueError(e.message)

将 1.x 更改为 2.x Tensorflow。]]></description>
      <guid>https://stackoverflow.com/questions/78800789/how-do-i-change-nural-function-1-x-tensorflow-to-2-x-tensorflow</guid>
      <pubDate>Sat, 27 Jul 2024 07:23:19 GMT</pubDate>
    </item>
    <item>
      <title>尽管有多个 GPU，CUDA 仍出现内存不足错误</title>
      <link>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</link>
      <description><![CDATA[尝试运行 PyTorch 模型时，我遇到了 CUDA 内存不足错误，尽管我的系统有多个 NVIDIA GPU。
# 加载 tokenizer 和模型
tokenizer = AutoTokenizer.from_pretrained(&quot;MODEL_TYPE&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;MODEL_TYPE&quot;, output_attentions=True, device_map = &#39;auto&#39;, torch_dtype=torch.float16, low_cpu_mem_usage=True)

我有 8 个 GPU，模型分布在所有 GPU 上。但是，由于我的输入是长上下文（大约 20k 个 token）。尽管其他 GPU 中有很多空间，但我还是收到 GPU0 的 CUDA 内存错误。请注意，这是对批处理大小 1 的推断。
OutOfMemoryError：CUDA 内存不足。尝试分配 20.11 GiB。GPU 0 的总容量为 22.17 GiB，其中 16.06 GiB 是空闲的。包括非 PyTorch 内存在内，此进程使用了​​ 6.10 GiB 内存。在分配的内存中，5.57 GiB 由 PyTorch 分配，308.62 MiB 由 PyTorch 保留但未分配。如果保留但未分配的内存很大，请尝试设置 max_split_size_mb 以避免碎片化。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档

 input = tokenizer(prompt, return_tensors=&quot;pt&quot;)
torch.cuda.empty_cache()
model.generation_config.temperature = temp
model.eval()
with torch.no_grad():
output = model.generate(inputs.input_ids, max_length=25000, output_attentions=False,output_scores=False, return_dict_in_generate=True)
print(&quot;temp:&quot;,model.generation_config.temperature)
tokens = tokenizer.convert_ids_to_tokens(inputs[&#39;input_ids&#39;][0])

response = tokenizer.batch_decode(output[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]

如何有效利用可用的 GPU 进行长上下文输入以避免内存不足错误？
我尝试将输入强制到其他 GPU，但没有成功：
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda:1&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</guid>
      <pubDate>Sat, 27 Jul 2024 01:14:45 GMT</pubDate>
    </item>
    <item>
      <title>来自segmentation_models_pytorch 的 Unet 在训练中停滞</title>
      <link>https://stackoverflow.com/questions/78798820/unet-from-segmentation-models-pytorch-stalling-in-training</link>
      <description><![CDATA[我一直在遵循一个关于在自定义数据集上训练分割模型的教程，但它拒绝在训练模型方面取得任何进展。
这是我的模型设置
import fragmentation_models_pytorch as smp
import torch

ENCODER = &#39;efficientnet-b0&#39;
ENCODER_WEIGHTS = &#39;imagenet&#39;
CLASSES = [&#39;ship&#39;]
ACTIVATION = &#39;sigmoid&#39;
DEVICE = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

model = smp.Unet(
coder_name=ENCODER, 
coder_weights=ENCODER_WEIGHTS, 
classes=len(CLASSES), 
activation=ACTIVATION,
).to(DEVICE)

from fragmentation_models_pytorch import utils as smp_utils

loss = smp_utils.losses.DiceLoss()
metrics = [
smp_utils.metrics.IoU(threshold=0.5),
]

optimizer = torch.optim.Adam([ 
dict(params=model.parameters(), lr=0.0001),
])


和 epochs 运行器
train_epoch = smp_utils.train.TrainEpoch(
model, 
loss=loss, 
metrics=metrics, 
optimizer=optimizer,
device=DEVICE,
verbose=True,
)

valid_epoch = smp_utils.train.ValidEpoch(
model, 
loss=loss, 
metrics=metrics, 
device=DEVICE,
verbose=True,
)

并且，当我运行训练，模型只是停留在第一个 epoch 上，没有任何进展
max_score = 0

for i in range(0, 40):

print(&#39;\nEpoch: {}&#39;.format(i))
train_logs = train_epoch.run(train_loader)
valid_logs = valid_epoch.run(valid_loader)

# 执行某些操作（保存模型、更改 lr 等）
if max_score &lt; valid_logs[&#39;iou_score&#39;]:
max_score = valid_logs[&#39;iou_score&#39;]
torch.save(model, &#39;./best_model.pth&#39;)
print(&#39;模型已保存！&#39;)

if i == 25:
optimizer.param_groups[0][&#39;lr&#39;] = 1e-5
print(&#39;将解码器学习率降低至 1e-5！&#39;)

结果：
Epoch：0
train：0%| | 0/3851 [00:00&lt;?, ?it/s]

我这样把它放了 3 个小时，它一点变化都没有
我在 CPU (i7-10710U) 上运行（我知道它比 GPU 慢得多，但我的 GPU (GeForce 1650mq) 不支持 cuda），内存为 32 GB，我之前运行过类似的模型，没有任何问题。
有人能帮帮我吗？也许我漏掉了什么？也许有一个更轻的模型可以在我的系统上运行？
我已经尝试了一些其他设置和模型，YOLOv8 和 YOLOv3 也拒绝训练。]]></description>
      <guid>https://stackoverflow.com/questions/78798820/unet-from-segmentation-models-pytorch-stalling-in-training</guid>
      <pubDate>Fri, 26 Jul 2024 15:14:45 GMT</pubDate>
    </item>
    <item>
      <title>决策树分类器给出错误结果</title>
      <link>https://stackoverflow.com/questions/78797339/decision-tree-classifier-gives-wrong-results</link>
      <description><![CDATA[我正在学习一门机器学习课程，其中的作业是实现 DecisionTreeClassifier 的拟合方法。
这是我的代码：
import numpy as np
import pandas as pd

class MyTreeClf:
def __init__(self, max_depth=5, min_samples_split=2, max_leafs=20):
self.max_depth = max_depth
self.min_samples_split = min_samples_split
self.max_leafs = max_leafs
self.tree = None
self.leafs_cnt = 0

def node_entropy(self, probs):
return -np.sum([p * np.log2(p) for p in probs if p &gt; 0])

def node_ig(self, x_col, y, split_value):
left_mask = x_col &lt;= split_value
right_mask = x_col &gt; split_value

如果 len(x_col[left_mask]) == 0 或 len(x_col[right_mask]) == 0:
返回 0

left_probs = np.bincount(y[left_mask]) / len(y[left_mask])
right_probs = np.bincount(y[right_mask]) / len(y[right_mask])

entropy_after = len(y[left_mask]) / len(y) * self.node_entropy(left_probs) + len(y[right_mask]) / len(y) * self.node_entropy(right_probs)
entropy_before = self.node_entropy(np.bincount(y) / len(y))

返回 entropy_before - entropy_after

def get_best_split(self, X: pd.DataFrame，y：pd.Series）：
best_col，best_split_value，best_ig = None，None，-np.inf

对于 X.columns 中的 col：
sorted_unique_values = np.sort(X[col].unique())

对于 range(1，len(sorted_unique_values)) 中的 i：
split_value = (sorted_unique_values[i - 1] + sorted_unique_values[i]) / 2

ig = self.node_ig(X[col]，y，split_value)

如果 ig &gt; best_ig:
best_ig = ig
best_col = col
best_split_value = split_value

返回 best_col、best_split_value、best_ig

def fit(self, X: pd.DataFrame, y: pd.Series,depth=0):
如果depth == 0:
self.tree = {}

best_col、best_split_value、best_ig = self.get_best_split(X, y)

如果depth &lt; self.max_depth 和 len(y) &gt;= self.min_samples_split 和 self.leafs_cnt &lt; self.max_leafs 和 best_col 不为 None:
left_mask = X[best_col] &lt;= best_split_value
right_mask = X[best_col] &gt; best_split_value

self.tree[depth] = {&#39;col&#39;: best_col, &#39;split&#39;: best_split_value, &#39;left&#39;: {}, &#39;right&#39;: {}}

self.fit(X[left_mask], y[left_mask],depth + 1)
self.fit(X[right_mask], y[right_mask],depth + 1)
else:
class_label = y.mode()[0]
self.tree[depth] = {&#39;class&#39;: class_label}
self.leafs_cnt += 1
df = pd.read_csv(&#39;c:\\Users\\Nijat\\Downloads\\banknote+authentication.zip&#39;, header=None)
df.columns = [&#39;variance&#39;, &#39;skewness&#39;, &#39;curtosis&#39;, &#39;entropy&#39;, &#39;target&#39;]
X, y = df.iloc[:,:4], df[&#39;target&#39;]

model = MyTreeClf()
model.fit(X, y)

print(model.leafs_cnt)

在 fit 方法中，使用 X 和 y 来构建树，应该计算叶子的数量。
树的构建如下：
根节点：从根节点开始，遍历每个属性。
阈值选择过程：
对于每个属性，选择唯一值并对其进行排序。
形成阈值列表以拆分值。
对于每个阈值，将数据集拆分为两个子集（左和右）。
评估每次拆分的信息增益。
选择具有最高信息增益的属性和阈值，并将它们保存在分层结构中。
递归拆分：
将数据集拆分为两个子集。
如果子集可以进一步分割，则递归重复该过程。
如果不能，则将子集声明为叶子并保存第一个类的概率。
约束：
当满足以下条件之一时停止分割：
最大树深度
节点中的最小实例数
最大叶子数
即使达到约束，也要通过创建必要的叶子来完成树。
叶子数保存在 leafs_cnt 变量中。该方法不返回任何内容。
此外，我可以提供一些链接和对“钞票认证”数据集的额外检查。]]></description>
      <guid>https://stackoverflow.com/questions/78797339/decision-tree-classifier-gives-wrong-results</guid>
      <pubDate>Fri, 26 Jul 2024 09:48:47 GMT</pubDate>
    </item>
    <item>
      <title>线性模型的 SHAP 值与手动计算的值不同</title>
      <link>https://stackoverflow.com/questions/78796974/shap-values-for-linear-model-different-from-those-calculated-manually</link>
      <description><![CDATA[我训练一个线性模型来预测房价，然后我手动比较 Shapley 值计算结果与 SHAP 库返回的值，发现它们略有不同。
我的理解是，对于线性模型，Shapley 值由以下公式给出：
coeff * features for obs - coeffs * mean(features in training set)

或者如 SHAP 文档中所述：coef[i] * (x[i] - X.mean(0)[i])，其中 i 是一个特征。
问题是，为什么 SHAP 返回的值与手动计算不同？
代码如下：
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import LinearRegression
from sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 MinMaxScaler
导入 shap

X, y = fetch_california_housing(return_X_y=True, as_frame=True)
X = X.drop(columns = [&quot;Latitude&quot;, &quot;Longitude&quot;, &quot;AveBedrms&quot;])

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.3, random_state=0,
)

scaler = MinMaxScaler().set_output(transform=&quot;pandas&quot;).fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

linreg = LinearRegression().fit(X_train, y_train)
coeffs = pd.Series(linreg.coef_, index=linreg.feature_names_in_)

X_test.reset_index(inplace=True, drop=True)
obs = 6188

# 手动 shapley 计算
effect = coeffs * X_test.loc[obs]
effect - coeffs * X_train.mean()

返回结果：
MedInc 0.123210
HouseAge -0.459784
AveRooms -0.128162
Population 0.032673
AveOccup -0.001993
dtype: float64

SHAP 库返回的结果略有不同：
explainer = shap.LinearExplainer(linreg, X_train)
shap_values = explainer(X_test)
shap_values[obs]

结果如下：
.values =
array([ 0.12039244, -0.47172515, -0.12767778, 0.03473923, -0.00251017])

.base_values =
2.0809714707337523

.data =
array([0.25094137, 0.01960784, 0.06056066, 0.07912217, 0.00437137])

设置为忽略交互：
explainer.feature_perturbation

返回
&#39;interventional&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/78796974/shap-values-for-linear-model-different-from-those-calculated-manually</guid>
      <pubDate>Fri, 26 Jul 2024 08:22:09 GMT</pubDate>
    </item>
    <item>
      <title>GradientBoostedClassifier() 中的 min_samples_leaf 行为怪异</title>
      <link>https://stackoverflow.com/questions/78796671/min-samples-leaf-in-gradientboostedclassifier-having-weird-behavior</link>
      <description><![CDATA[尝试在 GradientBoostedClassifer() 中调整 min_samples_leaf。我看到了偏差/方差权衡的预期结果。但是，为了测试边界，我让 min_samples_leaf &gt;训练数据集中有 n_samples，预计会出现错误或其他问题，但我仍然得到与模型调整时类似的结果：
df = df_a # 样本数 = 347
df=df.sample(frac=1) 
train_proportion = 0.8 
n = len(df)
t = int(train_proportion * n)

# 单独的训练和测试集
y = df[&#39;detected&#39;]
X = df.loc[:, ~df.columns.isin([&#39;detected&#39;])]

# 训练集中的样本
train_x = X.iloc[:t,:].reset_index().iloc[:,1:]
# 测试集中的样本
test_x = X.iloc[t:,:].reset_index().iloc[:,1:]
# 训练集中的目标
train_y = pd.Series(y[:t].reset_index().iloc[:,1:].iloc[:,0])
#测试集中的目标
test_y = pd.Series(y[t:].reset_index().iloc[:,1:].iloc[:,0])

clf = GradientBoostingClassifier(n_estimators = 100, max_depth = 10, random_state= 0, min_samples_leaf=500)
clf.fit(train_x,train_y)
print(clf.score(train_x,train_y))
print(clf.score(test_x,test_y))

输出：
0.924187725631769
0.9142857142857143

为什么会这样？我预计会出现错误或不会进行拆分。文档中似乎没有说明如果 min_samples_leaf &gt; n_samples 会发生什么。对 int 的唯一要求是范围 [1,inf]。对此也没有其他说明。
我当时想也许它会将 min_samples_leaf 重置为某个可用值，但所有子树都没有深度，也没有进行拆分：subtree]]></description>
      <guid>https://stackoverflow.com/questions/78796671/min-samples-leaf-in-gradientboostedclassifier-having-weird-behavior</guid>
      <pubDate>Fri, 26 Jul 2024 07:05:30 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：不支持 y 的稀疏多标签指标 - 如何处理具有稀疏数据的多标签分类？</title>
      <link>https://stackoverflow.com/questions/78795297/valueerror-sparse-multilabel-indicator-for-y-is-not-supported-how-to-handle-m</link>
      <description><![CDATA[我只是个初学者，我仍在学习稀疏矩阵。
这是我遇到的问题，在网上搜索后找不到合适的答案。
我使用默认参数sparse_output=True对分类标签进行了 OneHotEncoded，
当我尝试在训练测试拆分后使用transformed_X和目标y拟合 RandomForestClassifier 时，它显示了此错误。
ValueError: 不支持 y 的稀疏多标签指示器。

使用列转换器进行独热编码
#seed
np.random.seed(42)

#独热编码导入
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer as ct

#数据分割
X = f_data.drop(&#39;attended&#39;, axis = 1)
y = f_data[&#39;attended&#39;]

#选择列
cat_col = [&#39;days_before&#39;,&#39;day_of_week&#39;,&#39;time&#39;,&#39;category&#39;]

#初始化编码器
enc = OneHotEncoder()

#使用 ct 进行编码器拟合
transformer = ct([(&#39;enc&#39;,enc,cat_col)], remainder = &#39;passthrough&#39;)
transformed_X = transformer.fit_transform(X)
transformed_X

经过独热编码后的 transformed_X
&lt;1480x36 稀疏矩阵，类型为 &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;压缩稀疏行格式中存储了 10360 个元素&gt;
使用 RandomForestClassifier 拟合 transformed_X 和 y
#BaseLine 模型
np.random.seed(42)

#imports
from sklearn.model_selection import train_test_split as tts
from sklearn.ensemble import RandomForestClassifier

#splitting
X_train,Y_train,X_test,Y_test = tts(transformed_X,y, test_size = 0.2)

#model fitting
model = RandomForestClassifier()
model.fit(X_train,Y_train)

完整错误
--------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[416]，第 13 行
11 #modelling
12 model = RandomForestClassifier()
---&gt; 13 model.fit(X_train,Y_train)
15 #模型得分
16 blsc = model.score(X_test,Y_test)

文件 G:\Md Jaffer\UDEMY\Machine Learning Course ZTM\Projects\HeartDesease_Classification\env\Lib\site-packages\sklearn\base.py:1474，在 _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)
1467 estimator._validate_params()
1469 使用 config_context(
1470 skip_parameter_validation=(
1471 prefer_skip_nested_validation 或 global_skip_validation
1472 )
1473 ):
-&gt; 1474 返回 fit_method(estimator, *args, **kwargs)

文件 G:\Md Jaffer\UDEMY\Machine Learning Course ZTM\Projects\HeartDesease_Classification\env\Lib\site-packages\sklearn\ensemble\_forest.py:361，位于 BaseForest.fit(self, X, y, sample_weight)
359 # 验证或转换输入数据
360 if issparse(y):
--&gt; 361 引发 ValueError(&quot;不支持 y 的稀疏多标签指标。&quot;)
363 X, y = self._validate_data(
364 X,
365 y,
(...)
369 force_all_finite=False,
370 )
371 # _compute_missing_values_in_feature_mask 检查 X 是否有缺失值，
372 # 如果底层树基础估计器无法处理缺失值，则会引发错误。
373 # 仅需标准即可确定树是否支持
374 # 缺失值。

ValueError: 不支持 y 的稀疏多标签指标。

我尝试设置 sparse_output=False，但结果显示样本数量不一致。标签编码后的实际形状为 (1480 x 36)
---------------------------------------------------------------------------
ValueError Traceback (most recent call last)
Cell In[444], line 13
11 #modelling
12 model = RandomForestClassifier()
---&gt; 13 model.fit(X_train,Y_train)
15 #model score
16 blsc = model.score(X_test,Y_test)

ValueError: 发现输入变量的样本数量不一致: [1184, 296]
]]></description>
      <guid>https://stackoverflow.com/questions/78795297/valueerror-sparse-multilabel-indicator-for-y-is-not-supported-how-to-handle-m</guid>
      <pubDate>Thu, 25 Jul 2024 20:21:08 GMT</pubDate>
    </item>
    <item>
      <title>具有 10k 行独特上下文的合成 PII 数据集 [关闭]</title>
      <link>https://stackoverflow.com/questions/78794440/synthetic-pii-dataset-with-unique-contexts-for-10k-lines</link>
      <description><![CDATA[我正在寻找一个包含 10,000 行数据的合成数据集，其中包含各种类型的个人身份信息 (PII)，用于分类问题。数据应按段落格式化，并且每个段落应具有唯一的上下文。
我需要涵盖不同类型的 PII 数据，例如
[&quot;地址&quot;,
&quot;银行 • 帐号&quot;
&quot;信用卡 • 卡号&quot;
&quot;电子邮件地址&quot;
&quot;政府 - 身份证号码&quot;
&quot;个人姓名&quot;
&quot;密码&quot;
&quot;电话号码&quot;
&quot;密钥•（又称私钥）&quot;
121]
&quot;用户 ID&quot;,
&quot;出生日期&quot;,&quot;性别&quot;]

此外，每个段落在上下文中都是不同的，这一点至关重要。我尝试过使用 Faker，但它依赖于占位符模板，例如：
templates = [
&quot;{intro} {name} 出生于 {dob}，住在 {address}。您可以通过电子邮件 {email} 或电话 {phone} 联系他们。{closing}&quot;,
&quot;{intro} {name} 的社会安全号码是 {ssn}，护照号码是 {passport}。他们的信用卡号是 {ccn}。{closing}&quot;,
&quot;{intro} {name} 在 {license_year} 年获得了驾照号码 {dl}。 {closing}&quot;,
&quot;{intro} {name} 的电子邮件地址是 {email}，家庭住址是 {address}。他们出生于 {dob}，电话号码是 {phone}。{closing}&quot;,
&quot;{intro} {name} 的全名是 {name}，出生于 {dob}。他们的联系信息包括电话号码 {phone} 和电子邮件 {email}。他们居住在 {address}。{closing}&quot;
]

问题是这些句子在模板中重复出现，导致上下文变化有限。
我也尝试过使用 Kaggel，但它的结果没有涵盖所有必需的 PII 数据类型。还检查了 Github 存储库，但没有找到任何可靠的解决方案。
我正在寻找一种生成完全随机且唯一段落的方法。有人可以建议一种以编程方式创建具有多样化和独特上下文的合成 PII 数据的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78794440/synthetic-pii-dataset-with-unique-contexts-for-10k-lines</guid>
      <pubDate>Thu, 25 Jul 2024 16:19:35 GMT</pubDate>
    </item>
    <item>
      <title>在 pythonanywhere 中输入后预测页面未显示[关闭]</title>
      <link>https://stackoverflow.com/questions/78793658/prediction-page-not-showing-after-giving-input-in-pythonanywhere</link>
      <description><![CDATA[我使用 pythonanywhere 来实现 flask 项目。我能够在 localhost 上实现它，但无法在 pythonanywhere 中实现它。只显示输入页面，提交后需要很长时间，并且不显示下一页。
可能的原因是什么，整个文件大小仅在 2MB 以下
我希望预测显示在结果页面中。
输入并加载多次后，网站显示如下图片
错误日志图片
服务器日志 2024-07-27 10:05:15 *** 于 [2024 年 7 月 27 日星期六 10:05:12] 启动 uWSGI 2.0.20 (64bit) ***
2024-07-27 10:05:15 编译版本：9.4.0 于 2022 年 7 月 22 日 18:35:26
2024-07-27 10:05:15操作系统：Linux-5.15.0-1044-aws #49~20.04.1-Ubuntu SMP 2023 年 8 月 21 日星期一 17:09:32 UTC
2024-07-27 10:05:15 节点名称：green-liveweb27
2024-07-27 10:05:15 机器：x86_64
2024-07-27 10:05:15 时钟源：unix
2024-07-27 10:05:15 pcre jit 已禁用
2024-07-27 10:05:15 检测到的 CPU 核心数：4
2024-07-27 10:05:15 当前工作目录：/home/ANCYPADMANABHAN1
2024-07-27 10:05:15 检测到的二进制路径：/usr/local/bin/uwsgi
2024-07-27 10:05:15 *** 转储内部路由表 ***
2024-07-27 10:05:15 [规则：0] 主题：path_info regexp：.svgz$ 操作：addheader：Content-Encoding：gzip
2024-07-27 10:05:15 *** 内部路由表结束 ***
2024-07-27 10:05:15 chdir() 到 /home/ANCYPADMANABHAN1/mysite
2024-07-27 10:05:15 您的进程数限制为 256
2024-07-27 10:05:15 您的内存页面大小为 4096字节
2024-07-27 10:05:15 检测到的最大文件描述符数：123456
2024-07-27 10:05:15 从文件 /etc/mime.types 构建 mime-types 字典...
2024-07-27 10:05:15 找到 567 个条目
2024-07-27 10:05:15 锁定引擎：pthread 健壮互斥锁
2024-07-27 10:05:15 thunder 锁定：已禁用（您可以使用 --thunder-lock 启用它）
2024-07-27 10:05:15 Python 版本：3.10.5（main，2022 年 7 月 22 日，17:09:35）[GCC 9.4.0]
2024-07-27 10:05:15 *** Python 线程支持已被禁用。您可以使用 --enable-threads *** 启用它
2024-07-27 10:05:15 Python 主解释器在 0x55c2aa1a5e80 处初始化
2024-07-27 10:05:15 您的服务器套接字监听积压限制为 100 个连接
2024-07-27 10:05:15 您对工人优雅操作的怜悯是 60 秒
2024-07-27 10:05:15 将请求主体缓冲大小设置为 65536 字节
2024-07-27 10:05:15 为 1 个核心映射了 334256 字节（326 KB）
2024-07-27 10:05:15 *** 操作模式：单进程 ***
2024-07-27 10:05:15 初始化 38 个指标
2024-07-27 10:05:15 WSGI 应用程序 0 (mountpoint=&#39;&#39;) 在解释器 0x55c2aa1a5e80 pid: 1 (默认应用程序) 上 2 秒内准备就绪
2024-07-27 10:05:15 *** uWSGI 正在多解释器模式下运行 ***
2024-07-27 10:05:15 正常 (RE) 生成 uWSGI 主进程 (pid: 1)
2024-07-27 10:05:15 生成 uWSGI 工作进程 1 (pid: 12, 核心: 1)
2024-07-27 10:05:15 指标收集器线程已启动
2024-07-27 10:05:15 生成2 个用于 uWSGI worker 1 的卸载线程
2024-07-27 10:15:36 星期六 7 月 27 日 10:15:35 2024 - *** HARAKIRI ON WORKER 1 (pid: 12, try: 1) ***
2024-07-27 10:15:36 星期六 7 月 27 日 10:15:35 2024 - HARAKIRI !!! worker 1 状态 !!!
2024-07-27 10:15:36 星期六 7 月 27 日 10:15:35 2024 - HARAKIRI [core 0] 10.0.0.20 - POST /预测自 1722074734
2024-07-27 10:15:36 星期六 7 月 27 日 10:15:35 2024 - HARAKIRI !!! 工人 1 状态结束 !!!
2024-07-27 10:15:36 该死！工作者 1 (pid: 12) 死亡，被信号 9 杀死 :( 尝试重生...
2024-07-27 10:15:36 重生 uWSGI 工作者 1 (新 pid: 17)
2024-07-27 10:15:36 为 uWSGI 工作者 1 生成 2 个卸载线程
2024-07-27 10:15:36 星期六 7 月 27 日 10:15:36 2024 - SIGPIPE：根据请求 /favicon.ico (ip 10.0.0.20) 写入已关闭的管道/套接字/fd（可能是客户端断开连接）!!!
2024-07-27 10:15:36 星期六 7 月 27 日 10:15:36 2024 - uwsgi_response_writev_headers_and_body_do(): GET /favicon.ico (10.0.0.20) 期间管道 [core/writer.c 第 306 行] 损坏
2024-07-27 10:15:36 宣布我对皇帝的忠诚..]]></description>
      <guid>https://stackoverflow.com/questions/78793658/prediction-page-not-showing-after-giving-input-in-pythonanywhere</guid>
      <pubDate>Thu, 25 Jul 2024 13:37:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 JSON 数据降低掩码质量以训练 U 网模型</title>
      <link>https://stackoverflow.com/questions/78786001/down-quality-of-mask-with-json-data-for-train-u-net-model</link>
      <description><![CDATA[我想用 json 格式屏蔽我的图像作为 u net 训练模型的数据。
我使用下面的代码来屏蔽它们：

import json
import numpy as np
import cv2
import os

# 包含 JSON 文件的文件夹路径
json_folder = os.path.expanduser(&#39;~/Desktop/jeson&#39;) # 包含 JSON 文件的文件夹
# 用于保存掩码图像的文件夹路径
mask_folder = os.path.expanduser(&#39;~/Desktop/masks&#39;) # 用于保存掩码的文件夹

# 确保用于保存掩码的文件夹存在
os.makedirs(mask_folder, exist_ok=True)

# 列出文件夹中的所有 JSON 文件
for filename in os.listdir(json_folder):
if filename.endswith(&#39;.json&#39;):
json_file = os.path.join(json_folder, filename)

# 从 JSON 文件加载数据
with open(json_file) as f:
data = json.load(f)

# 创建一个空的掩码
mask = np.zeros((data[&#39;imageHeight&#39;], data[&#39;imageWidth&#39;]), dtype=np.uint8)

# 将区域添加到掩码
for shape in data[&#39;shapes&#39;]:
points = np.array(shape[&#39;points&#39;], dtype=np.int32)
if len(points) &gt; 0:
# 用白色填充由点定义的区域
cv2.fillPoly(mask, [points], 49)

# 使用 OpenCV 将掩码保存为 PNG 图像
mask_filename = os.path.splitext(filename)[0] + &#39;_mask.png&#39;
cv2.imwrite(os.path.join(mask_folder, mask_filename), mask)

print(&quot;转换完成，掩码图像已保存在 &#39;masks&#39; 文件夹中！&quot;)


但有一个有趣的问题。它在开始时只很好地掩盖了其中的几个，但其他的只用一条细线掩盖了。当我再次尝试使用此代码时，它会用一条细线掩盖所有数据。
例如在 labelme 工具中处理之前的图像：
在 lamelme 中处理之前的图像
例如屏蔽的 jeson 数据：
jeson 被屏蔽
我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78786001/down-quality-of-mask-with-json-data-for-train-u-net-model</guid>
      <pubDate>Wed, 24 Jul 2024 00:14:38 GMT</pubDate>
    </item>
    <item>
      <title>PipeOp classif.avg (mlr3) 错误：对“prob”的断言失败：包含缺失值（元素 1）</title>
      <link>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</guid>
      <pubDate>Thu, 18 Jul 2024 07:56:11 GMT</pubDate>
    </item>
    <item>
      <title>如何在 SKLearn Estimator 上使用 Sagemaker HyperparameterTuner？</title>
      <link>https://stackoverflow.com/questions/77573670/how-do-i-use-sagemaker-hyperparametertuner-on-a-sklearn-estimator</link>
      <description><![CDATA[我正在关注 Amazon Sagemaker 研讨会，尝试利用 Sagemaker 的几个实用程序，而不是像我目前所做的那样在 Notebook 上运行所有内容。
问题是，在研讨会上，他们教你如何使用来自 AWS 的现成 XGBoost 图像来使用 HyperparameterTuner，而我的大多数管道都在使用 Scikit-Learn 模型，例如 GradientBoostingClassifier 或 RandomForest，所以我正在实例化一个像这样的估算器 此示例文件:
sklearn = SKLearn(entry_point=&quot;train.py&quot;, 
framework_version=&quot;1.2-1&quot;, 
instance_type=&quot;ml.m5.xlarge&quot;, 
role=role,
hyperparameters=fixed_hyperparameters
)

之后，我将使用刚刚创建的估算器实例化 HyperparameterTuner 作业，其中包含我想要的超参数范围测试。
hyperparameters_ranges = {
&quot;n_estimators&quot;: ContinuousParameter(100, 500),
&quot;learning_rate&quot;: ContinuousParameter(1e-2, 1e-1),
&quot;max_depth&quot;: IntegerParameter(2, 5),
&quot;subsample&quot;: ContinuousParameter(0.6, 1),
&quot;max_df&quot;: ContinuousParameter(0.4, 1),
&quot;max_features&quot;: IntegerParameter(5, 25),
&quot;use_idf&quot;: CategoricalParameter([True, False])
}

metric = &quot;validation:f1&quot;

tuner = HyperparameterTuner(
sklearn,
metric,
hyperparameters_ranges,
max_jobs=2,
max_parallel_jobs=2
)

我的问题是，我没有找到任何关于如何访问“train.py”文件中 SKLearn 估算器中传递的超参数的信息。我也没有找到最佳超参数存储在哪里，以便我可以将它们用于最终模型。有人能告诉我这是否可行，或者提供替代方案，看看是否有其他更简单的方法可以做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/77573670/how-do-i-use-sagemaker-hyperparametertuner-on-a-sklearn-estimator</guid>
      <pubDate>Wed, 29 Nov 2023 18:27:13 GMT</pubDate>
    </item>
    <item>
      <title>超时错误：[WinError 10060]</title>
      <link>https://stackoverflow.com/questions/76484091/timeouterror-winerror-10060</link>
      <description><![CDATA[ DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot;
HOUSING_PATH = os.path.join(&quot;datasets&quot;, &quot;housing&quot;)
HOUSING_URL = DOWNLOAD_ROOT + &quot;datasets/housing/housing.tgz&quot;

 def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
if not os.path.isdir(housing_path):
os.makedirs(housing_path)
tgz_path = os.path.join(housing_path, &quot;housing.tgz&quot;)
urllib.request.urlretrieve(housing_url, tgz_path)
housing_tgz = tarfile.open(tgz_path)
housing_tgz.extractall(path=housing_path)
housing_tgz.close()

 fetch_housing_data()

 import pandas as pd

 def load_housing_data(housing_path=HOUSING_PATH):
csv_path = os.path.join(housing_path,&quot;housing.csv&quot;)
return pd.read_csv(csv_path)

 housing = load_housing_data()

我运行了这段代码，但出现了错误
&quot;TimeoutError: [WinError 10060] 连接尝试失败，因为连接方在一段时间后没有正确响应，或者由于连接的主机未能响应而建立的连接失败&gt;&quot;
&quot;URLError: &lt;urlopen error [WinError 10060] 连接尝试失败，因为连接方在一段时间后没有正确响应，或者由于连接的主机未能响应而建立的连接失败&gt;&quot;
有人能帮我解决这个代码吗？]]></description>
      <guid>https://stackoverflow.com/questions/76484091/timeouterror-winerror-10060</guid>
      <pubDate>Thu, 15 Jun 2023 17:20:49 GMT</pubDate>
    </item>
    <item>
      <title>视觉框架（iOS）：VNDetectFaceLandmarksRequest 和 VNDetectFaceRectanglesRequest 有何不同？</title>
      <link>https://stackoverflow.com/questions/66367963/vision-framework-ios-how-are-vndetectfacelandmarksrequest-and-vndetectfacerec</link>
      <description><![CDATA[您可以使用两种不同的请求通过 iOS Vision Framework 执行人脸检测任务：VNDetectFaceLandmarksRequest 和 VNDetectFaceRectanglesRequest。它们都返回一个 VNFaceObservation 数组，每个检测到的人脸对应一个数组。VNFaceObservation 具有多种可选属性，包括 boundingBox 和 landmarks。landmarks 对象还包括可选属性，例如 nose、innerLips、leftEye 等。
这两种不同的 Vision 请求在执行人脸检测的方式上是否有所不同？
似乎 VNDetectFaceRectanglesRequest 只能找到一个边界框（可能还有其他一些属性），但找不到任何地标。另一方面，VNDetectFaceLandmarksRequest 似乎可以同时找到边界框和地标。
是否存在一种请求类型可以找到面部而另一种则找不到的情况？ VNDetectFaceLandmarksRequest 是否 优于 VNDetectFaceRectanglesRequest，或者后者在 性能 或 可靠性 方面可能更具优势？
以下是如何使用这两个 Vision 请求的示例代码：
let faceLandmarkRequest = VNDetectFaceLandmarksRequest()
let faceRectangleRequest = VNDetectFaceRectanglesRequest()
let requestHandler = VNImageRequestHandler(ciImage: image, options: [:])
try requestHandler.perform([faceRectangleRequest, faceLandmarkRequest])
if let rectangleResults = faceRectangleRequest.results as? [VNFaceObservation] {
let boundingBox1 = rectangleResults.first?.boundingBox //这是一个可选类型
}
if let landmarkResults = faceLandmarkRequest.results as? [VNFaceObservation] {
let boundingBox2 = landmarkResults.first?.boundingBox //这是一个可选类型
let landmarks = landmarks //这是一个可选类型
}
]]></description>
      <guid>https://stackoverflow.com/questions/66367963/vision-framework-ios-how-are-vndetectfacelandmarksrequest-and-vndetectfacerec</guid>
      <pubDate>Thu, 25 Feb 2021 11:53:04 GMT</pubDate>
    </item>
    <item>
      <title>Py4JJavaError：调用 z:org.apache.spark.api.python.PythonRDD.runJob 时发生错误。ModuleNotFoundError：没有名为“numpy”的模块</title>
      <link>https://stackoverflow.com/questions/59152894/py4jjavaerror-an-error-occurred-while-calling-zorg-apache-spark-api-python-pyt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/59152894/py4jjavaerror-an-error-occurred-while-calling-zorg-apache-spark-api-python-pyt</guid>
      <pubDate>Tue, 03 Dec 2019 08:28:51 GMT</pubDate>
    </item>
    </channel>
</rss>