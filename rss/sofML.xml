<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 11 Dec 2024 03:33:27 GMT</lastBuildDate>
    <item>
      <title>无法访问自由变量“fig”，因为它与封闭范围内的值没有关联</title>
      <link>https://stackoverflow.com/questions/79270292/cannot-access-free-variable-fig-where-it-is-not-associated-with-a-value-in-enc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79270292/cannot-access-free-variable-fig-where-it-is-not-associated-with-a-value-in-enc</guid>
      <pubDate>Wed, 11 Dec 2024 02:04:10 GMT</pubDate>
    </item>
    <item>
      <title>如何设置 PyTorch 模型（例如 GFPGAN）进行跟踪？</title>
      <link>https://stackoverflow.com/questions/79270125/how-do-i-set-up-pytorch-model-e-g-gfpgan-for-tracing</link>
      <description><![CDATA[我在设置 GFPGAN PyTorch 模型进行跟踪时遇到了麻烦，因此我将其转换为 CoreML 模型。我的理解是，我还需要定义一个从 Torch.nn.Module 继承的类，该类定义了 GPFGAN 模型的结构，并且 .pth 文件包含权重。但我不知道在哪里可以找到定义从 Torch.nn.Module 继承的类的信息。我是不是漏掉了什么？
我在 GFPGAN python 库中找到了从 Torch.nn.Module 继承的一些类，但它们在尝试将状态字典加载到其中时都会导致错误。
这是我的代码：
import torch
from gfpgan.archs import gfpganv1_clean_arch

model_path = &#39;GFPGANv1.4.pth&#39;
traceable_model = gfpganv1_clean_arch.GFPGANv1Clean(out_size=512)
traceable_model.load_state_dict(torch.load(model_path)) # 此处发生错误。
traceable_model.eval()
trace = torch.jit.trace(traceable_model, input_batch)

我应该如何加载 GFPGANv1.4 模型进行跟踪？]]></description>
      <guid>https://stackoverflow.com/questions/79270125/how-do-i-set-up-pytorch-model-e-g-gfpgan-for-tracing</guid>
      <pubDate>Wed, 11 Dec 2024 00:00:30 GMT</pubDate>
    </item>
    <item>
      <title>如何加载使用（version ='latest'）框架训练的 Sagemaker XGBoost 模型？</title>
      <link>https://stackoverflow.com/questions/79269787/how-to-load-sagemaker-xgboost-model-which-was-trained-using-version-latest</link>
      <description><![CDATA[管道中有一个使用此容器创建的现有 xgboost 模型
sagemaker.image_uris.retrieve(&#39;xgboost&#39;, sagemaker.Session().boto_region_name, version=&#39;latest&#39;)

输出：
&#39;{accountid}.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest&#39;

我从模型工件中提取了 model.tar.gz 并加载了 xgboost-model 文件
但它给出了这个错误
XGBoostError: basic_string::resize

我运行了一个 shell 脚本，使用所有可用的 XGBoost 版本加载模型，但没有任何效果。
我只想使用 model.get_score 检查特征重要性。]]></description>
      <guid>https://stackoverflow.com/questions/79269787/how-to-load-sagemaker-xgboost-model-which-was-trained-using-version-latest</guid>
      <pubDate>Tue, 10 Dec 2024 20:57:42 GMT</pubDate>
    </item>
    <item>
      <title>奇怪的是损失法学硕士的跳高</title>
      <link>https://stackoverflow.com/questions/79269535/strangely-high-jump-in-loss-llm</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79269535/strangely-high-jump-in-loss-llm</guid>
      <pubDate>Tue, 10 Dec 2024 19:31:33 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：无法将字符串转换为浮点数：'N/'</title>
      <link>https://stackoverflow.com/questions/79268711/valueerror-could-not-convert-string-to-float-n</link>
      <description><![CDATA[我正在 Jupyter Notebook 上开展一个机器学习项目，该项目用于预测电动汽车的价格。
我在 Jupyter Notebook 上运行此单元：
p = regressor.predict(df2)

我收到此错误：
ValueError Traceback（最近一次调用最后一次）
~\AppData\Local\Temp\ipykernel_16424\818753220.py in &lt;module&gt;
----&gt; 1 p = regressor.predict(df2)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\tree\_classes.py in predict(self, X, check_input)
465 &quot;&quot;&quot;
466 check_is_fitted(self)
--&gt; 467 X = self._validate_X_predict(X, check_input)
468 proba = self.tree_.predict(X)
469 n_samples = X.shape[0]

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\tree\_classes.py in _validate_X_predict(self, X, check_input)
431 &quot;&quot;&quot;验证预测（概率）上的训练数据。&quot;&quot;&quot;
432 if check_input:
--&gt; 433 X = self._validate_data(X, dtype=DTYPE, accept_sparse=&quot;csr&quot;, reset=False)
434 if issparse(X) and (
435 X.indices.dtype != np.intc 或 X.indptr.dtype != np.intc

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)
564 raise ValueError(&quot;应在 X、y 或两者上进行验证。&quot;)
565 elif not no_val_X and no_val_y:
--&gt; 566 X = check_array(X, **check_params)
567 out = X
568 elif no_val_X 且 not no_val_y:

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, Ensure_2d, allow_nd, Ensure_min_samples, Ensure_min_features, estimator)
744 array = array.astype(dtype, casting=&quot;unsafe&quot;, copy=False)
745 else:
--&gt; 746 array = np.asarray(array, order=order, dtype=dtype)
747 except ComplexWarning as complex_warning:
748 raise ValueError(

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\pandas\core\generic.py in __array__(self, dtype)
1991 
1992 def __array__(self, dtype: NpDtype | None = None) -&gt; np.ndarray:
-&gt; 1993 return np.asarray(self._values, dtype=dtype)
1994 
1995 def __array_wrap__(

ValueError: 无法将字符串转换为浮点数：&#39;N/&#39;

我做了什么尝试？
我尝试使用以下代码：
uv = np.nanpercentile(df2[&#39;Base MSRP&#39;], [99])[0]*2
df2[&#39;Base MSRP&#39;][(df2[&#39;Base MSRP&#39;]&gt;uv)] = uv
le = preprocessing.LabelEncoder()
cols = [&#39;County&#39;, &#39;City&#39;, &#39;State&#39;, &#39;ZIP Code&#39;, &#39;Model Year&#39;, &#39;Make&#39;, &#39;Model&#39;, &#39;Electric Vehicle Type&#39;, &#39;Clean Alternative Fuel Vehicle (CAFV) Eligibility&#39;]
for col in cols:
le.fit(t[col])
df2[col] = le.transform(df2[col]) 
print(le.classes_)
regressor.fit(x, y)
p = regressor.predict(df2)

这是我的笔记本的链接：https://github.com/SteveAustin583/electric-vehicle-price-prediction-revengers/blob/main/revengers.ipynb
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79268711/valueerror-could-not-convert-string-to-float-n</guid>
      <pubDate>Tue, 10 Dec 2024 15:00:14 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Keras 组合预先训练好的层将多个模型合并为一个</title>
      <link>https://stackoverflow.com/questions/79267084/how-to-combine-multiple-models-into-one-by-combining-pre-trained-layers-using-ke</link>
      <description><![CDATA[我有多个经过预训练的分类模型。我想将它们组合成一个模型。每个模型都包含相同的 EfficentNet 和 Resnet 架构，然后是连接层和 Conv 层，然后是密集层。我想从每个模型中删除 Resnet 和 EfficentNet，并仅对整个“final_model”使用一个。之前，模型是在冻结的 EfficentNet 和 Resnet 上使用 imagenet 权重进行训练的。
我尝试过这样的解决方案，但当我尝试通过在测试数据集上运行此模型来验证权重是否相同时，它给出了随机输出。
shared_input = Input(shape=(224, 224, 3), name=&quot;shared_input&quot;)

efficient_net = EfficientNetB7(weights=&#39;imagenet&#39;, include_top=False, input_shape=(SIZE, SIZE, 3))
resnet = ResNet152(weights=&#39;imagenet&#39;, include_top=False, input_shape=(SIZE, SIZE, 3))

efficient_net = efficient_net(shared_input)
resnet = resnet(shared_input)

shared_conc = Concatenate()([efficient_net, resnet])

for i in范围（模型数量）：
模型 = 加载模型（f“model_nb_{i}.keras”）
小模型 = 模型（模型.layers[4].输入，模型.layers[-1].输出）
小模型.append（小模型（shared_conc））

组合输出 = 连接（名称=“最终输出”（轻量级输出）
最终模型 = 模型（输入=[shared_input]，输出=[组合输出]）
]]></description>
      <guid>https://stackoverflow.com/questions/79267084/how-to-combine-multiple-models-into-one-by-combining-pre-trained-layers-using-ke</guid>
      <pubDate>Tue, 10 Dec 2024 05:19:07 GMT</pubDate>
    </item>
    <item>
      <title>我想研究第三磨牙。如何通过机器学习来唯一地识别牙齿？[关闭]</title>
      <link>https://stackoverflow.com/questions/79267013/i-want-to-work-with-3rd-molar-teeth-how-can-i-uniquely-identify-the-teeth-throu</link>
      <description><![CDATA[一般来说，第三磨牙有 4 颗。我想唯一地识别每颗牙齿。如果缺少任何一颗，我也想通过机器学习来识别它。
我曾尝试注释其中一张牙齿图像。但我没有得到任何有价值的结果。]]></description>
      <guid>https://stackoverflow.com/questions/79267013/i-want-to-work-with-3rd-molar-teeth-how-can-i-uniquely-identify-the-teeth-throu</guid>
      <pubDate>Tue, 10 Dec 2024 04:26:21 GMT</pubDate>
    </item>
    <item>
      <title>处理 Llama 3.2：3b-Instruct 模型中的令牌限制问题（最多 2048 个令牌）</title>
      <link>https://stackoverflow.com/questions/79267003/handling-token-limit-issues-in-llama-3-23b-instruct-model-2048-tokens-max</link>
      <description><![CDATA[我正在使用 Llama 3.2:3b-instruct 模型，并遇到以下错误：
此模型的最大上下文长度为 2048 个标记。但是，您请求了 
2049 个标记（消息中为 1681 个，完成中为 368 个）。

我理解这是由于超出令牌限制造成的，但我想知道：

为什么存在此令牌限制，此特定限制是否有技术原因？
是否有任何最佳实践或技术可以减少令牌使用量，而不会丢失消息或完成中的关键上下文？
是否有关于此令牌限制的官方文档或开发人员更新，或者是否有在未来版本中增加此限制的潜在计划？
是否有其他人在类似场景中有效地使用此模型的替代策略（例如分块、总结或其他技巧）？
]]></description>
      <guid>https://stackoverflow.com/questions/79267003/handling-token-limit-issues-in-llama-3-23b-instruct-model-2048-tokens-max</guid>
      <pubDate>Tue, 10 Dec 2024 04:19:17 GMT</pubDate>
    </item>
    <item>
      <title>GFPGAN CoreML 模型无法正常工作</title>
      <link>https://stackoverflow.com/questions/79266909/gfpgan-coreml-model-isnt-working-correctly</link>
      <description><![CDATA[我尝试从 hoseins77/gfpgan 和 TheMurusTeam/coreml-upscaler-gfpgan 下载 GFPGAN CoreML 模型，但是当我在我的项目中使用它时，输出的图像不正确。 GFPGAN 似乎总体上工作正常，因为它可以在此处在线运行，但 CoreML 模型似乎不起作用。
以下是几个示例：


下面是运行该模型的代码。我是否需要进行一些其他配置才能使 GFPGAN CoreML 模型正常工作？
func getPhotoSharpened(_ photo: CGImage) async -&gt; CGImage? {
guard let vnModel = await getVNModelFromStorage() else { return nil }

let coreMLRequest = VNCoreMLRequest(model: vnModel)
coreMLRequest.imageCropAndScaleOption = .scaleFit

do {
let handler = VNImageRequestHandler(cgImage: photo, options: [:])
try handler.perform([coreMLRequest])
} catch {
print(&quot;Error sharpening photo: \(error)&quot;)
}

guard
let results = coreMLRequest.results as? [VNPixelBufferObservation],
let outputPixelBuffer = results.first?.pixelBuffer
else {
print(&quot;无法从模型获取输出&quot;); return nil
}

let ciImage = CIImage(cvPixelBuffer: outputPixelBuffer)
let desireAspectRatio = CGFloat(photo.width) / CGFloat(photo.height)
// cropped 函数在其他地方定义。
let croppedImage = cropped(ciImage, toAspectRatio: desireAspectRatio)

let context = CIContext()
guard let sharpenedImage = context.createCGImage(croppedImage, from: croppedImage.extent) else {
print(&quot;无法从 CIImage 创建锐化 CGImage&quot;)
return nil
}

return sharpenedImage
}

private func getVNModelFromStorage() async -&gt; VNCoreMLModel? {
do {
// gpfganFileUrl 在其他地方定义。
let mlModel = try await MLModel.load(contentsOf: gpfganFileUrl, configuration: .init())
return try VNCoreMLModel(for: mlModel)
} catch {
print(&quot;无法从文件加载 GFPGAN 模型，错误为：\(error)&quot;); return nil
}
}

]]></description>
      <guid>https://stackoverflow.com/questions/79266909/gfpgan-coreml-model-isnt-working-correctly</guid>
      <pubDate>Tue, 10 Dec 2024 02:53:43 GMT</pubDate>
    </item>
    <item>
      <title>时间序列运动捕捉数据的 PCA 图聚类问题</title>
      <link>https://stackoverflow.com/questions/79263104/pca-plot-clustering-issue-with-time-series-motion-capture-data</link>
      <description><![CDATA[我正在使用 PCA 对手部动作捕捉数据的时间序列数据集进行降维，并遇到了意外的聚类行为。以下是我的过程和我面临的问题的详细信息。

上下文：
我有一个使用智能手套捕捉的手部动作记录数据集，每个手指上有 4 个传感器。每个传感器提供：

位置值：X、Y、Z
旋转值：X、Y、Z、W

数据记录在单独的文件中，每个文件包含 10 次手势重复。这些重复随后被分割成单独的文件（每个文件 1 个重复），贴上标签，并合并成一个大型数据集。
在对数据进行标签编码和缩放后，我使用以下代码应用 PCA 进行降维：
# PCA 用于降维
pca_components = 3
pca = PCA(n_components=pca_components)
x_train_pca = pca.fit_transform(x_train_scaled)

为了可视化结果，我创建了一个 PCA 摘要图，其中每个数据段都表示为一个点。目标是查看每个手势的 10 个点的聚类。以下是总结 PCA 结果的代码：
# 将 PCA 结果转换为 DataFrame 以关联标签
x_train_pca_df = pd.DataFrame(x_train_pca, columns=[&#39;PC1&#39;, &#39;PC2&#39;, &#39;PC3&#39;])
x_train_pca_df[&#39;label&#39;] = y_train_data
x_train_pca_df[&#39;segment&#39;] = train_data[&#39;segment&#39;].values.ravel()

# 计算每个数据集的 PC1 和 PC2 的平均值（将每个数据集总结为一个点）
summary_train_points = x_train_pca_df.groupby([&#39;label&#39;, &#39;segment&#39;]).mean().reset_index()

以下是我用来绘制总结 PCA 的方法数据：
def plot_3d_pca_matplotlib(summary_points, title=&#39;3D PCA Plot&#39;):
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111,projection=&#39;3d&#39;)

# 对于每个唯一数据集（标签），分散点并分配图例条目
unique_labels = summary_points[&#39;label&#39;].unique()

for label in unique_labels:
# 过滤当前标签的数据
filtered_data = summary_points[summary_points[&#39;label&#39;] == label]

# 当前标签点的散点图
ax.scatter(filtered_data[&#39;PC1&#39;],
filtered_data[&#39;PC2&#39;],
filtered_data[&#39;PC3&#39;],
label=label)

ax.set_xlabel(&#39;PCA 组件 1&#39;)
ax.set_ylabel(&#39;PCA 组件 2&#39;)
ax.set_zlabel(&#39;PCA 组件 3&#39;)
ax.set_title(title)
ax.legend(title=&quot;Labels&quot;, loc=&quot;center left&quot;, bbox_to_anchor=(1.05, 0.7))

plt.subplots_adjust(left=0.05, right=0.75)
plt.show()


问题：
当我为相同手势记录一组新数据并绘制 PCA 结果时，我希望看到每个手势有 20 个点（10 个来自原始数据的点 + 10 个来自新数据的点）的聚类。
相反，PCA 图显示每个手势有两个独立的 10 点簇。这表明，即使手势相同，新数据也未与 PCA 空间中的原始数据对齐。

问题：
什么原因导致相同手势被分离为不同的簇？
可能与以下情​​况有关：

缩放过程？
传感器校准不一致？
在应用 PCA 之前是否需要对齐或对数据进行额外的预处理？
]]></description>
      <guid>https://stackoverflow.com/questions/79263104/pca-plot-clustering-issue-with-time-series-motion-capture-data</guid>
      <pubDate>Sun, 08 Dec 2024 18:31:23 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 和 LGBM 模型的大小取决于给定参数集的训练数据大小，而 Catboost 则不然</title>
      <link>https://stackoverflow.com/questions/79261222/xgboost-and-lgbm-models-size-depends-on-training-data-size-for-a-given-set-of-pa</link>
      <description><![CDATA[我正在使用 Python 3.11 在前向交叉验证设置中比较模型。对于给定的一组超参数，xgboost 和 LGBM 模型大小（使用库保存方法进行 pickle 或保存时）随训练大小单调增加，而对于 CatBoost，它保持不变，正如我所料，因为树的数量、max_depth 和所有内容都相同。
您是否知道 xgboost/lgbm 模型是否保存了可以删除以进行推理的任何东西，例如梯度？我遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79261222/xgboost-and-lgbm-models-size-depends-on-training-data-size-for-a-given-set-of-pa</guid>
      <pubDate>Sat, 07 Dec 2024 19:09:52 GMT</pubDate>
    </item>
    <item>
      <title>保留验证集-超参数调整</title>
      <link>https://stackoverflow.com/questions/79247785/holdout-validation-set-hyperparameter-tuning</link>
      <description><![CDATA[我有一个大型数据集，我将其拆分为：

训练集 (80%)
验证集 (10%)
测试集 (10%)

在每个集合上，我执行了缺失值插补和特征选择（在训练集上训练，并复制到验证和测试集中）以避免数据泄露。
现在，我想用 Python 训练 XGBoost 模型，并希望使用训练集执行超参数调整，并使用验证集评估每个参数集。我如何使用 RandomizedSearchCV 等随机方法执行此操作，以便不运行所有参数集？
如果我是正确的，GridSearch 和 RandomizedSearchCV 仅允许交叉验证，这不是我想要的，因为将预处理的训练集拆分成几层会导致数据泄露。
我知道我可以构建一个 sklearn 管道，在其中对每个折叠进行预处理，但我想避免后一种选择。
我只能考虑像在 GridSearch 中一样运行每个参数集的代码：
from sklearn.model_selection import ParameterGrid
import xgboost as xgb

# 定义你的超参数网格
param_grid = {
&#39;max_depth&#39;: [3, 5, 7],
&#39;learning_rate&#39;: [0.01, 0.1, 0.2],
&#39;n_estimators&#39;: [100, 200, 300]
}

best_score = -1
best_params = {}

for params in ParameterGrid(param_grid):
model = xgb.XGBClassifier(**params)
model.fit(X_train, y_train)
val_score = model.score(X_val, y_val) # 或者使用更具体的指标

if val_score &gt; best_score:
best_score = val_score
best_params = params

# 使用最佳超参数训练最终模型
best_model = xgb.XGBClassifier(**best_params)
best_model.fit(X_train, y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/79247785/holdout-validation-set-hyperparameter-tuning</guid>
      <pubDate>Tue, 03 Dec 2024 13:26:56 GMT</pubDate>
    </item>
    <item>
      <title>使用 XGBoost 对未来 24 小时进行多时间步长预测</title>
      <link>https://stackoverflow.com/questions/79190223/multi-time-step-forecasting-for-next-24-h-ahead-using-xgboost</link>
      <description><![CDATA[我的项目是使用 XGBoost 对未来 24 小时进行多时间步长预测。
以下两张图是结果：


为什么第一张图中的测试数据与第二张图（橙色）不同，它们应该是相同的？
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# 逆变换预测和测试数据（如果缩放）
y_pred_actual = scaler_y.inverse_transform(y_pred.reshape(-1, 1)) # 确保形状为 
(-1, 
1)
y_test_inv = scaler_y.inverse_transform(y_test)

# 计算指标
mse = mean_squared_error(y_test_inv, y_pred_actual)
mae = mean_absolute_error(y_test_inv, y_pred_actual)
rmse = np.sqrt(mse)
mpe = np.mean((y_test_inv - y_pred_actual) / y_test_inv) * 100

# 计算 MAPE
mape = np.mean(np.abs((y_test_inv - y_pred_actual) / y_test_inv)) * 100

# 打印评估指标
print(&#39;&#39;)
print(&#39;-----------------------------------&#39;)
print(f&#39;TL_model_loaded MAE for test set : {round(mae, 3)}&#39;)
print(f&#39;TL_model_loaded MSE for test set : {round(mse, 3)}&#39;)
print(f&#39;TL_model_loaded RMSE for test set : {round(rmse, 3)}&#39;)
print(f&#39;TL_model_loaded MPE for test set : {round(mpe, 3)} %&#39;)
print(f&#39;TL_model_loaded MAPE for test set : {round(mape, 3)} %&#39;)
print(&#39;-----------------------------------&#39;)
print(&#39;&#39;)

# 调用plot_results_xgboost 函数
plot_results_xgboost(y_pred_actual, y_test_inv, evals_result, &#39;XG&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79190223/multi-time-step-forecasting-for-next-24-h-ahead-using-xgboost</guid>
      <pubDate>Thu, 14 Nov 2024 19:49:04 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 提前停止轮次</title>
      <link>https://stackoverflow.com/questions/79189607/xgboost-early-stopping-rounds</link>
      <description><![CDATA[下面的代码一直在崩溃，我不知道发生了什么
import optuna
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 假设 `X` 和 `y` 是你的特征矩阵和目标数组
X_train, X_valid, y_train, y_valid = train_test_split(df_combined, y, test_size=0.2, random_state=42)

# 为 Optuna 定义目标函数
def objective(trial):
# 为超参数建议值
params = {
&quot;objective&quot;: &quot;reg:squarederror&quot;,
&quot;eval_metric&quot;: &quot;rmse&quot;,
&quot;tree_method&quot;: &quot;hist&quot;, # 使用 hist 方法
&quot;device&quot;: &quot;cuda&quot;, # 指定使用 GPU
&quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 0.01, 0.3, log=True),
&quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 3, 10),
&quot;min_child_weight&quot;: trial.suggest_float(&quot;min_child_weight&quot;, 1, 10),
&quot;gamma&quot;: trial.suggest_float(&quot;gamma&quot;, 0, 1),
&quot;subsample&quot;: trial.suggest_float(&quot;subsample&quot;, 0.5, 1.0),
&quot;colsample_bytree&quot;: trial.suggest_float(&quot;colsample_bytree&quot;, 0.5, 1.0),
&quot;lambda&quot;: trial.suggest_float(&quot;lambda&quot;, 1e-3, 10.0, log=True),
&quot;alpha&quot;: trial.suggest_float(&quot;alpha&quot;, 1e-3, 10.0, log=True),
&quot;n_estimators&quot;: 1000 # 在模型初始化中定义 n_estimators
}

# 初始化模型
model = xgb.XGBRegressor(**params)

# 使用早期停止回调训练模型
model.fit(
X_train,
y_train,
eval_set=[(X_valid, y_valid)],
verbose=False,
early_stopping_rounds=50 # 如果之后没有改进则停止50 轮
)

# 预测并计算验证集的 RMSE
preds = model.predict(X_valid)
rmse = mean_squared_error(y_valid, preds, squared=False)

return rmse # Optuna 将其最小化

# 设置 Optuna 研究
study = optuna.create_study(direction=&quot;minimize&quot;)

# 优化超参数
study.optimize(objective, n_trials=100, n_jobs=40) # 100 次试验，40 次并行作业

# 显示最佳试验
print(&quot;最佳试验：&quot;)
trial = study.best_trial
print(f&quot;值 (RMSE)：{trial.value}&quot;)
print(&quot; Params: &quot;)
for key, value in trial.params.items():
print(f&quot; {key}: {value}&quot;)

我得到
TypeError: XGBModel.fit() 得到一个意外的关键字参数 &#39;early_stopping_rounds&#39;

我已更新所有内容以确保我拥有所有更新的库。
提前停止轮次是正确的（我认为），但由于某种原因，它只是爆炸了。]]></description>
      <guid>https://stackoverflow.com/questions/79189607/xgboost-early-stopping-rounds</guid>
      <pubDate>Thu, 14 Nov 2024 16:14:40 GMT</pubDate>
    </item>
    <item>
      <title>了解 XGboost 中的 nrounds</title>
      <link>https://stackoverflow.com/questions/78939808/understanding-nrounds-in-xgboost</link>
      <description><![CDATA[我不明白参数 nrounds。我正在使用 R 并使用包 xgboost 创建一个“极端”梯度提升回归模型。
有一个名为 nrounds 的参数，我的理解是它设置了梯度提升模型（也称为提升迭代）中的树的数量。但是当我设置 nrounds = 0 时，我并没有发现所有预测都等于 base_score（梯度提升模型中的初始猜测）。
为什么会这样？
注意：
我并不是想要一个有 0 棵树的 xgboost 模型，但我尝试用它来检查我对 nrounds 的理解。
我有一个 data.frame，我将其转换为 xgb.DMatrix，然后训练模型：
params &lt;- list(objective = &quot;reg:squarederror&quot;, eval_metric = &quot;rmse&quot;, base_score = 0.5)

bst_model &lt;- xgb.train(params = params, data = dtrain, nrounds = 0, verbose = 1)

使用预测我得到：
pred &lt;- predict(bst_model, newdata = dtrain)
print(pred[1:10])

[1] 1052663 874001 1498940 1991579 2316396 1086949 874001 1432935 1086949 2351261

我预期的位置：
[1] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5
]]></description>
      <guid>https://stackoverflow.com/questions/78939808/understanding-nrounds-in-xgboost</guid>
      <pubDate>Mon, 02 Sep 2024 09:36:49 GMT</pubDate>
    </item>
    </channel>
</rss>