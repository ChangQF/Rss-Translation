<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 05 Mar 2024 21:13:10 GMT</lastBuildDate>
    <item>
      <title>自动编码器模型错误？</title>
      <link>https://stackoverflow.com/questions/78110545/wrong-autoencoder-model</link>
      <description><![CDATA[我正在尝试使用张量流创建一个用于异常检测的自动编码器，该模型可以运行，但仅此而已。
这是模型：`
int_vectorizer=layers.TextVectorization(
    最大令牌=10000,
    输出模式=&#39;int&#39;,
    输出序列长度=140
）
int_vectorizer.adapt(adapt_data)

模型 = tf.keras.Sequential([
    int_向量化器，
    层.密集（256，激活=“relu”），
    层.密集（128，激活=“relu”），
    层.密集（64，激活=“relu”），
    层.密集（32，激活=“relu”），
    层.密集（64，激活=“relu”），
    层.密集（128，激活=“sigmoid”），`
]）

label_converter=layers.StringLookup(output_mode=“int”)
label_converter.adapt(数据)

model.compile(optimizer=&#39;adam&#39;, loss=“mae”)
model.fit(数据, label_converter(数据), epochs=200)

测试时，我得到以下 2 个输入的相同百分比水平：
Inpus
我尝试更改层、节点数量、单元数量和不同的激活。]]></description>
      <guid>https://stackoverflow.com/questions/78110545/wrong-autoencoder-model</guid>
      <pubDate>Tue, 05 Mar 2024 21:05:27 GMT</pubDate>
    </item>
    <item>
      <title>使用 LSTM 的多变量时间序列</title>
      <link>https://stackoverflow.com/questions/78110519/multiple-multipvariate-time-series-with-lstm</link>
      <description><![CDATA[我正在处理的数据集可以在下面找到。
https://www.kaggle.com/datasets/behrad3d/nasa -cmaps/数据
数据是多个多元时间序列数据。该数据集跟踪发动机整个生命周期的 26 个特征，直至发生故障。它为 100 种不同的引擎执行此操作。有一个时间周期功能，它记录直到故障为止的时间周期数，从 1 开始。在我们的训练集中，我们可以从引擎当前的生命周期中减去引擎的最大时间周期，以获得 RUL，剩余可用生命周期，即我们将预测的功能。我们的测试集不包含 100 个引擎的完整生命周期数据。模型必须利用有限的数据来预测发动机的RUL。
我的数据集的当前形状是 20631, 129（我进行了特征工程，得到了 129 个特征）。这 20631 行包含 100 个引擎的时间序列数据。我知道如何将它们分成训练集和测试集，同时保持每个引擎数据的时间顺序。但需要注意的一件事是，引擎没有相同数量的 Time_Cycle。
有人可以指导我如何训练 LSTM，使其将每个引擎视为多元时间序列，并训练我将用于对所有 100 个引擎进行预测的模型吗？]]></description>
      <guid>https://stackoverflow.com/questions/78110519/multiple-multipvariate-time-series-with-lstm</guid>
      <pubDate>Tue, 05 Mar 2024 20:58:38 GMT</pubDate>
    </item>
    <item>
      <title>与YOLO模型的预测模型</title>
      <link>https://stackoverflow.com/questions/78110123/prediction-model-with-yolo-model</link>
      <description><![CDATA[我想知道是否可以使用 yolo 模型，并对其进行更改，使其成为实时图像输入的预测模型，并使用我自己的图像数据集训练模型。最终应该是一个具有实时图像输入的模型，使用我的数据集进行训练，并输出最有可能的百分比。
如果您有任何可能有帮助的建议，我很感激！]]></description>
      <guid>https://stackoverflow.com/questions/78110123/prediction-model-with-yolo-model</guid>
      <pubDate>Tue, 05 Mar 2024 19:26:48 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 自定义和默认目标和评估函数</title>
      <link>https://stackoverflow.com/questions/78109955/xgboost-custom-default-objective-and-evaluation-functions</link>
      <description><![CDATA[我正在训练 BDT 以进行信号/背景的二元分类（我从事粒子物理学工作）。我的模型（用 python 实现）如下所示：
导入 xgboost 为 xgb
train = xgb.DMatrix(data=train_df[特征],label=train_df[“标签”],
                    缺失=“inf”，feature_names=特征，权重=(np.array(train_df[&#39;label&#39;].array)*-0.99+1))
测试= xgb.DMatrix(数据=test_df[特征],标签=test_df[“标签”],
                   缺失=“inf”，feature_names=特征，权重=(np.array(test_df[&#39;label&#39;].array) *-0.99+1))

参数 = {}


# 助推器参数
param[&#39;eta&#39;] = 0.1 # 学习率
param[&#39;max_depth&#39;] = 10 # 树的最大深度
param[&#39;subsample&#39;] = 0.5 # 训练树的事件分数
param[&#39;colsample_bytree&#39;] = 0.5 # 训练树的特征分数

# 学习任务参数
param[&#39;objective&#39;] = &#39;binary:logistic&#39; # 目标函数
param[&#39;eval_metric&#39;] = &#39;error&#39; # 交叉验证的评估指标
param = list(param.items()) + [(&#39;eval_metric&#39;, &#39;logloss&#39;)] + [(&#39;eval_metric&#39;, &#39;rmse&#39;)]


num_trees = 50 # 要制作的树的数量
booster = xgb.train(参数,train,num_boost_round=num_trees)

该模型表现良好，但我想稍微修改一下成本/损失函数，以便误报比真报受到更多惩罚。我正在寻找背景事件尽可能少的选择，即使这意味着牺牲信号的很大一部分。
根据自定义目标和评估函数的文档，我成功添加教程案例。
我的问题是：标准目标和评估函数是什么？ （或者我可能已经在模型中实现的那些，我仍在学习基础知识）。
由于模型已经表现良好，我只想为已经实现的功能添加一个额外的术语。
作为参考，我正在使用的软件包版本：
python 版本：3.10.12（主要，2023 年 11 月 20 日，15:14:05）[GCC 11.4.0]
XGBoost版本：2.0.2
熊猫版本：2.1.4
Numpy 版本：1.26.2
]]></description>
      <guid>https://stackoverflow.com/questions/78109955/xgboost-custom-default-objective-and-evaluation-functions</guid>
      <pubDate>Tue, 05 Mar 2024 18:53:51 GMT</pubDate>
    </item>
    <item>
      <title>mac - yolov9 - 为什么 epoch02 这么慢？</title>
      <link>https://stackoverflow.com/questions/78109909/mac-yolov9-why-epoch02-is-so-slow</link>
      <description><![CDATA[我正在 Mac M2 芯片上训练 YOLOV9。第一个 epoch 花了 7 分钟，但第二个 epoch 花了 2 小时。为什么两个 epoch 的训练时间相差这么大？
我的代码
# DDP模式
如果不是 torch.backends.mps.is_available():
    如果不是 torch.backends.mps.is_built():
        print(“MPS 不可用，因为当前 PyTorch 安装不是”
              “在启用 MPS 的情况下构建。”）
    别的：
        print(&quot;MPS 不可用，因为当前的 MacOS 版本不是 12.3+ &quot;
              “和/或您在此计算机上没有支持 MPS 的设备。”）
    设备= select_device(opt.device,batch_size=opt.batch_size)
别的：
    device = torch.device(“mps”)
如果 LOCAL_RANK ！= -1：
    msg = &#39;与 YOLO 多 GPU DDP 训练不兼容&#39;
    断言不是 opt.image_weights, f&#39;--image-weights {msg}&#39;
    断言不是 opt.evolve, f&#39;--evolve {msg}&#39;
    断言 opt.batch_size != -1, f&#39;AutoBatch with --batch-size -1 {msg}，请传递有效的 --batch-size&#39;
    断言 opt.batch_size % WORLD_SIZE == 0, f&#39;--batch-size {opt.batch_size} 必须是 WORLD_SIZE&#39; 的倍数
    断言 torch.backends.mps.is_available(),“MPS 不可用。检查 PyTorch 版本、macOS 版本和硬件”
    # 断言 torch.cuda.device_count() &gt; LOCAL_RANK，“用于 DDP 命令的 CUDA 设备不足”
    device = torch.device(“mps”)
    dist.init_process_group(backend=“nccl” if dist.is_nccl_available() else “gloo”)

批量大小 = 8
我将批量大小设置为 8。
您可以看到GPU确实正在被使用。
在此处输入图片描述
在此处输入图像描述]]></description>
      <guid>https://stackoverflow.com/questions/78109909/mac-yolov9-why-epoch02-is-so-slow</guid>
      <pubDate>Tue, 05 Mar 2024 18:43:43 GMT</pubDate>
    </item>
    <item>
      <title>Accuracy_score 出现错误</title>
      <link>https://stackoverflow.com/questions/78109873/erroring-in-accuracy-score</link>
      <description><![CDATA[我在编码时遇到问题。
我想在 jupyter 笔记本的数据框中使用 precision_score 但出现此错误：
“ValueError：不支持连续”
我写道：
从sklearn.metrics导入accuracy_score
train_prediction = model.predict(X_train)
训练准确度 = 准确度得分（y_train ，
     训练预测）
print(“分数：”，training_accuracy)` [?]
]]></description>
      <guid>https://stackoverflow.com/questions/78109873/erroring-in-accuracy-score</guid>
      <pubDate>Tue, 05 Mar 2024 18:38:58 GMT</pubDate>
    </item>
    <item>
      <title>不平衡文本数据集中的准确率较高，但精度和召回率较低</title>
      <link>https://stackoverflow.com/questions/78109777/good-accuracy-but-low-precision-and-recall-in-imbalanced-text-dataset</link>
      <description><![CDATA[文本数据集极其不平衡，在总共 1200 条记录中，少数数据集约为 120 条。数据集有两个类别“Y”和“N”。
我已经尝试过

数据已预处理。
使用 n_grams 的 TfIdfVectorizer 来获取更有意义的数据。
随机森林分类器、XGB分类器、SVC。
上采样和下采样

尽管如此，我在上述任何方法中都没有获得良好的精确度或召回率。
我原以为 UpSampling 在这种情况下应该可以正常工作，但事实并非如此。
我错过了什么吗？任何建议将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78109777/good-accuracy-but-low-precision-and-recall-in-imbalanced-text-dataset</guid>
      <pubDate>Tue, 05 Mar 2024 18:20:46 GMT</pubDate>
    </item>
    <item>
      <title>对于具有协变量和缺失日期的多元时间序列插补，我应该选择哪种模型？</title>
      <link>https://stackoverflow.com/questions/78109616/which-model-should-i-select-for-a-multivariate-time-series-imputation-where-i-ha</link>
      <description><![CDATA[我有两年时间范围内商店的销售数据。样本数据：

&lt;标题&gt;

日期
目标商店销售额
目标商店促销
Sales_Store1
Promotion_Store1
Sales_Store2
Promotion_Store1


&lt;正文&gt;

2022-06-27
30.0
否
29.0
是
34.0
是


2022-06-28
42.0
否
空
否
39.0
否


2022-06-29
37.0
否
26.0
是
37.0
是


2022-07-02
45.0
否
44.0
否
空
否


2022-07-03
29.0
否
空
是
24.0
是


2022-07-05
34.0
否
40.0
否
42.0
否



为了扩展我的（希望是正确的）问题标题，我想做的是使用上表作为我的训练集来预测我的 Target 商店销售额的基线，希望找到特定促销的提升。该表已根据“目标商店促销”列进行筛选，因此它仅包含没有促销的日期。促销活动每天进行。 Sales_Store1和Promotion_Store1包含类似商店的销售和促销状态。我对他们的选择相当有信心。
示例表的其余部分如下：

&lt;标题&gt;

日期
目标商店销售额
目标商店促销
Sales_Store1
Promotion_Store1
Sales_Store2
Promotion_Store1


&lt;正文&gt;

2022-06-30
55.0
是
34.0
是
67.0
是


2022-07-04
47.0
是
66.0
否
55.0
是



数据具有很强的季节性，但目标商店和所使用的类似商店也有许多缺失值。商店的销售天数约为 600 天。
此外，值得注意的是，使用综合控制的 A/B 测试在这里会失败，因为促销在商店之间是高度同步的。也就是说，如果我的目标商店有正在进行的促销活动，那么很有可能类似的商店也会这样做。 （但不一定）
我希望结果表看起来像这样：

&lt;标题&gt;

日期
目标商店销售额
没有促销的目标商店销售额
目标商店促销
Sales_Store1
Promotion_Store1
Sales_Store2
Promotion_Store1


&lt;正文&gt;

2022-06-30
55.0
45.0
是
34.0
是
67.0
是


2022-07-04
47.0
32.0
是
66.0
否
55.0
是



您知道哪种模型或技术可以帮助我预测此基线吗？我正在研究新的 Tide，但在深入研究之前并尝试使用它，我想我应该问问是否有人遇到过类似的挑战。预先感谢！
我最初尝试了典型的 A/B 测试，同时消除协变量，但剩余的数据样本太少了。正如我提到的，大多数促销活动在各商店同时进行。
我尝试使用上述功能（以及月份和工作日等功能）训练 XGBoost 回归器，希望能够捕获季节性，但结果平庸，MAPE 约为 0.15。]]></description>
      <guid>https://stackoverflow.com/questions/78109616/which-model-should-i-select-for-a-multivariate-time-series-imputation-where-i-ha</guid>
      <pubDate>Tue, 05 Mar 2024 17:48:57 GMT</pubDate>
    </item>
    <item>
      <title>修改ML预处理函数以在Google Cloud TPU上进行训练</title>
      <link>https://stackoverflow.com/questions/78109089/modify-ml-preprocessing-function-to-train-on-google-cloud-tpu</link>
      <description><![CDATA[在此处输入图像描述
请帮助我提高 TPU 上的 CPU 使用率。
目前仅使用了 0.5%（附截图）
&lt;前&gt;&lt;代码&gt;最大输入长度 = 128
最大目标长度 = 128

source_lang = &quot;en&quot;;
target_lang =“嗨”

def preprocess_function（示例）：
输入 = \[ex\[source_lang\] 示例中的 ex\[“翻译”\]\]
目标 = \[ex\[target_lang\] 示例中的 ex\[“翻译”\]\]
model_inputs = tokenizer(输入, max_length=max_input_length, 截断=True)

# 为目标设置标记器
使用 tokenizer.as_target_tokenizer()：

标签=分词器（目标，max_length = max_target_length，截断= True）

model_inputs[“labels”] = labels[“input_ids”]
返回模型输入

请帮我修改代码以增加CPU使用率:)
我尝试增加批处理大小，但 RAM 使用量只会增加。
训练时间保持不变且没有减少。]]></description>
      <guid>https://stackoverflow.com/questions/78109089/modify-ml-preprocessing-function-to-train-on-google-cloud-tpu</guid>
      <pubDate>Tue, 05 Mar 2024 16:13:37 GMT</pubDate>
    </item>
    <item>
      <title>尝试建立一个lstm模型</title>
      <link>https://stackoverflow.com/questions/78108041/trying-to-build-an-lstm-model</link>
      <description><![CDATA[**构建 lstm 模型并获取形状错误**
模型=顺序（）
词汇大小 = 10000
嵌入尺寸 = 100
最大序列长度 = 21
model.add(嵌入(input_dim=vocab_size,
output_dim=embedding_dim))
model.add(LSTM(单位=50,return_sequences=False))
model.add（密集（单位= 1，激活=&#39;sigmoid&#39;））
model.compile(loss=&#39;binary_crossentropy&#39;, 优化器=&#39;adam&#39;,
指标=[&#39;准确性&#39;])
模型.summary()

这是错误
输入 Tensor 的输入形状无效(“sequential_4_1/Cast:0”,
形状=（无，21），dtype=float32）。预期形状（无、9144、
21)，但输入的形状不兼容（无，21）
Sequential.call() 收到的参数：
输入=tf.Tensor（形状=（无，21），dtype=int64）
• 训练=真
•掩码=无]]></description>
      <guid>https://stackoverflow.com/questions/78108041/trying-to-build-an-lstm-model</guid>
      <pubDate>Tue, 05 Mar 2024 13:22:01 GMT</pubDate>
    </item>
    <item>
      <title>无监督自动编码器产生特定的输出维度</title>
      <link>https://stackoverflow.com/questions/78107646/unsupervised-autoencoder-produce-specific-output-dimensions</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78107646/unsupervised-autoencoder-produce-specific-output-dimensions</guid>
      <pubDate>Tue, 05 Mar 2024 12:12:37 GMT</pubDate>
    </item>
    <item>
      <title>强化学习神经网络概率没有改变[关闭]</title>
      <link>https://stackoverflow.com/questions/78106390/reinforcement-learning-neural-network-probabilities-arent-changing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78106390/reinforcement-learning-neural-network-probabilities-arent-changing</guid>
      <pubDate>Tue, 05 Mar 2024 08:48:07 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 3D X 和 2D y 进行训练</title>
      <link>https://stackoverflow.com/questions/78099801/unable-to-use-3d-x-and-2d-y-for-training</link>
      <description><![CDATA[我正在训练一个模型来区分真实数据和虚假数据。
我有一个真实的数据集和一个假的数据集，由于我不知道如何在这两个数据集上训练单个模型，所以我现在正在训练一个真实的模型和一个假的模型。
问题是训练任何模型似乎都是不可能的，因为我收到此错误：
ValueError：输入 X 包含无穷大或对于 dtype(&#39;float16&#39;) 来说太大的值

对于此代码
导入 pandas 作为 pd
将 numpy 导入为 np
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.metrics 导入 precision_score
从 sklearn.utils 导入洗牌
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.preprocessing 导入 RobustScaler
从 sklearn.metrics 导入分类报告
从sklearn.metrics导入confusion_matrix

定标器 = RobustScaler()
X = np.load(“fake_x.npy”)
x_data_flat = X.reshape(X.shape[0], -1)
x_data_scaled = 缩放器.fit_transform(x_data_flat)
y = np.load(“fake_y.npy”)
x_data_flat, y = 随机播放(x_data_flat, y)
X_train,X_test,y_train,y_test = train_test_split(x_data_scaled,y,test_size=0.1, train_size=0.1)

clf = 随机森林分类器()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

准确度=准确度_得分(y_test, y_pred)
print(“准确度：”, 准确度)

print(&quot;分类报告：&quot;)
打印（分类报告（y_test，y_pred））


conf_matrix = fusion_matrix(y_test, y_pred)
print(&quot;混淆矩阵：&quot;)
打印（conf_matrix）

数据肯定太大了，x.shape = (750,1998,101) 和 y.shape = (750,496)。
数据链接：https://drive.google.com/drive/folders/1EYnOIOWP17ALs -903ESFM-02_6VszJEx
对于分类本身，我会在输入上使用相似度分数，无论谁获得更高的相似度分数，输入都将属于其类别。
使用最初是 real.npz 但提取到 real_x.npy 和 real_y.npy 的二维频谱图以使其更容易，模型应该识别输入是属于真实的还是属于我提取到 fake_x 的 fake.npz。 npy 和 fake_y.npy]]></description>
      <guid>https://stackoverflow.com/questions/78099801/unable-to-use-3d-x-and-2d-y-for-training</guid>
      <pubDate>Mon, 04 Mar 2024 08:20:52 GMT</pubDate>
    </item>
    <item>
      <title>wandb 表中的彩色文本</title>
      <link>https://stackoverflow.com/questions/75900664/colored-text-in-a-wandb-table</link>
      <description><![CDATA[对于错误分析（对于我的 ASR 模型），我想向 wandb 报告一个表，其中文本采用颜色编码，以显示预测的转录本和真实情况之间的差异（例如删除的单词为灰色，插入为蓝色，替换为红色）。
我尝试了 HTML，即
def _html_c(color: str) -&gt;字符串：
    return f&#39;&#39;

和 ASCII 颜色代码，即
def _c_ascii(color: str) -&gt;字符串：
    如果颜色 == EQUAL_C：
        返回&#39;\033[0m&#39;
    elif 颜色 == REPLACE_C:
        返回&#39;\033[91m&#39;
    elif 颜色 == DELETE_C:
        返回&#39;\033[95m&#39;
    elif 颜色 == INSERT_C:
        返回&#39;\033[94m&#39;
    别的：
        引发 ValueError(f&#39;未知颜色：{color}&#39;)

但是，该表显示的是 kebab 字形而不是 ASCII 颜色代码，并且显示 HTML 代码而不是颜色（请参阅 https://github.com/wandb/wandb/issues/5253）。
那么如何对 wandb 表中的文本进行颜色编码？
我觉得必须有一个解决方案，因为预测和真实情况之间的颜色编码文本本身就表明了这一点（至少 ASR 在视觉上接近通用指标 WER）。但我的搜索毫无结果。
&lt;小时/&gt;
有关计算表格和差异的详细信息：
EQUAL_C = &#39;黑色&#39;
REPLACE_C = &#39;红色&#39;
DELETE_C = &#39;粉红色&#39;
INSERT_C = &#39;橙色&#39;

def Visualize_differences（标签，预测）：
    断言 len(预测) == len(标签)
    返回 [[索引 + 1, Visualize_difference(标签, 预测), 标签]
            对于索引，枚举（zip（标签，预测））中的（标签，预测）]

def Visualize_difference（标签，预测）：
    split_label = _split(标签)
    split_prediction = _split(预测)
    matcher = difflib.SequenceMatcher(无, split_label, split_prediction)
    结果=“”
    对于 matcher.get_opcodes() 中的标签 i1、i2、j1、j2：
        如果标签==&#39;等于&#39;：
            结果+=&#39;&#39;.join(split_label[i1:i2])
        elif 标签 == &#39;替换&#39;:
            结果+= _html_c(REPLACE_C) + &#39;&#39;.join(split_prediction[j1:j2]) + _html_c(EQUAL_C)
        elif 标签 == &#39;删除&#39;:
            结果+= _html_c(DELETE_C) + &#39;&#39;.join(split_label[i1:i2]) + _html_c(EQUAL_C)
        elif 标签 == &#39;插入&#39;:
            结果+= _html_c(INSERT_C) + &#39;&#39;.join(split_prediction[j1:j2]) + _html_c(EQUAL_C)
    返回结果

def _split（输入字符串）：
    模式 = r&quot;(\w+|\s+|[^\w\s]+)&quot;
    返回 re.findall(pattern, input_string)

diff_wandb_table = wandb.Table(data=visualize_differences(ground_truths, 预测), columns=[&#39;样本&#39;, &#39;差异&#39;, &#39;标签&#39;])
    [create_word_error_table 中的行的 diff_wandb_table.add_data(*row)]
]]></description>
      <guid>https://stackoverflow.com/questions/75900664/colored-text-in-a-wandb-table</guid>
      <pubDate>Fri, 31 Mar 2023 16:13:01 GMT</pubDate>
    </item>
    <item>
      <title>在训练和测试数据分割之前或之后对数据进行归一化？</title>
      <link>https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data</link>
      <description><![CDATA[我想将数据分成训练集和测试集，我应该在分割之前还是之后对数据应用归一化？它在构建预测模型时有什么区别吗？]]></description>
      <guid>https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data</guid>
      <pubDate>Fri, 23 Mar 2018 07:13:09 GMT</pubDate>
    </item>
    </channel>
</rss>