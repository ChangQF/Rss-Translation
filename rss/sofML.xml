<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 31 Jan 2024 21:14:08 GMT</lastBuildDate>
    <item>
      <title>TensorFlow edit_distance 文本预处理</title>
      <link>https://stackoverflow.com/questions/77916416/tensorflow-edit-distance-text-preprocessing</link>
      <description><![CDATA[我正在尝试在 TensorFlow 中构建一个模型，该模型采用两个字符串，通过计算两个字符串之间的编辑距离来预处理字符串，然后使用结果数字作为模型的输入。这个想法是，在部署中，模型将接受这两个字符串作为输入，处理它们，然后进行评估。下面是我想要做的伪代码。
将张量流导入为 tf
从 nltk.metrics.distance 导入 edit_distance

# 我正在尝试做的事情：
string1 = “约翰·J·多伊”
string2 = “约翰·詹姆斯·多伊”

dist = edit_distance(字符串1,字符串2)
打印（分布）
# 6（这将作为输入传递给模型）

我已经尝试过类似的方法，但是得到一个 ValueError: Shape (1, None, 1) must haverank 1 并且我似乎无法得到任何结果。
&lt;代码&gt;
string1 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)
string2 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)

类 EditDistanceLayer(tf.keras.layers.Layer):
    def 调用（自身，输入）：
        字符串 1、字符串 2 = 输入

        假设 = tf.sparse.SparseTensor(indices=[[0, 0]],values=[string1],dense_shape=[1, 1])
        真值 = tf.sparse.SparseTensor(indices=[[0, 0]]，values=[string2]，dense_shape=[1, 1])

        edit_distance = tf.edit_distance（假设，真相，归一化= True）

        返回编辑距离

# 这里发生错误
edit_distance_output = EditDistanceLayer()([string1_input, string2_input])

我尝试构建一个自定义预处理层来计算两个字符串之间的编辑距离。]]></description>
      <guid>https://stackoverflow.com/questions/77916416/tensorflow-edit-distance-text-preprocessing</guid>
      <pubDate>Wed, 31 Jan 2024 21:04:46 GMT</pubDate>
    </item>
    <item>
      <title>MONAI DiceMetric</title>
      <link>https://stackoverflow.com/questions/77916384/monai-dicemetric</link>
      <description><![CDATA[我正在训练 MONAI model =SegResNet( out_channels=2) 来执行分割任务。我有两个类，前景和背景。真实分割/标签是 1 通道图像。
我使用 MONAI DiceLoss(softmax=True, include_background=False, to_onehot_y=True) ，它似乎有效，它减少了，并且预测看起来不错。 （include_background 是 False，因为背景比前景大得多。）但是我似乎不知道如何使用 DiceMetric(include_background=False,duction=“mean” ;, get_not_nans=False).
它要么给出错误，要么给出 1 或 0 或大于 1 的数字。
我阅读了我能找到的所有教程，并尝试复制这些教程，但没有成功......
我这样使用损失：
 vloss = loss_fn(voutputs, vlabels)
在教程“AsDiscreted”中这是一个常见的步骤。我不想使用 MONAI 转换，所以这就是我尝试过的：
voutputs_bin=voutputs
voutputs_bin=torch.nn.function.softmax(voutputs_bin,dim=1)
#voutputs_bin=torch.argmax(voutputs_bin,dim=1)
voutputs_bin = torch.nn.function.one_hot(voutputs_bin.to(torch.int64), num_classes=-1)
#voutputs_bin=voutputs_bin[:,1:,:,:]
#voutputs_bin=torch.nn.function.threshold(voutputs_bin,0.5,1)
vlabels_bin=torch.nn.function.one_hot(vlabels.to(torch.int64), num_classes=-1)

指标（y_pred=voutputs_bin，y=vlabels_bin）

我包含了注释行，因为这是我尝试过的以及它的不同组合。
在验证循环之后我会这样做：
vmetric=metric.aggregate().item()
print(f&#39; 骰子指标: {vmetric}&#39;)
指标.reset()

您能告诉我并解释一下，我应该如何正确使用 DiceMetric？]]></description>
      <guid>https://stackoverflow.com/questions/77916384/monai-dicemetric</guid>
      <pubDate>Wed, 31 Jan 2024 20:59:41 GMT</pubDate>
    </item>
    <item>
      <title>生物系统中 ANN 模型预测的 SHAP 分析令人困惑，需要帮助</title>
      <link>https://stackoverflow.com/questions/77915752/confusing-shap-analysis-of-ann-model-prediction-in-systems-bio-need-assistance</link>
      <description><![CDATA[我开发了一个 ANN 模型来根据 Elisa 数据预测蛋白质翻译后修饰模式。为了简单起见，如何、可行性和参数对于我的问题并不重要，并且省略了一些细节。
对于给定的蛋白质，我将其称为蛋白质 X，它具有泛素作为修饰，但没有磷酸化模式。
我用各种翻译后修饰模式训练了人工神经网络，但有一个关键信息：我的训练数据不包含任何泛素模式（假设有一个原因）
因此，当我使用一组抗体进行 ELISA 时，抗体 a 特异性针对泛素模式，抗体 b 特异性针对磷酸化模式。当我使用抗体 a、抗体 b（和其他抗体）预测蛋白质 x 修饰模式时，我们获得了相当好的准确性。
为了解释模型的工作原理，我运行了 SHAP 分析和二分图来显示特征重要性（抗体）和修改，但得到了令人困惑的结果
对于抗体 a，除泛素外，其他修饰模式都有正值和负值 SHAP 值，泛素是其特异性的
对于抗体 b，我们还发现除磷酸化之外的修饰模式的正 SHAP 值和负 SHAP 值，而蛋白质 x 并不真正具有磷酸化。
那么我如何解释为什么 SHAP 产生这种模式：1）抗体 a 与其目标泛素没有任何 SHAP 相关性，但对其其他目标有任何 SHAP 相关性，2）抗体 b 也与其目标没有任何 SHAP 相关性，而是与其他目标相关。 
这又是令人困惑的，因为我预计抗体 a 与泛素有 SHAP 相关性，但与其他蛋白没有 SHAP 相关性，然后抗体 b 不应该有任何 SHAP 相关性，因为它的目标是磷酸化，但蛋白 x 没有磷酸化。
我不太相信或无法将 SHAP 的一些限制联系起来，因为它显示了模型的隐藏模式/关系，而不是我们在“现实生活”中所期望的
有人可以对这个观察到的 SHAP 分析提供更细致的见解吗？]]></description>
      <guid>https://stackoverflow.com/questions/77915752/confusing-shap-analysis-of-ann-model-prediction-in-systems-bio-need-assistance</guid>
      <pubDate>Wed, 31 Jan 2024 18:44:06 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程模型预测所有数据点的相同 STD [关闭]</title>
      <link>https://stackoverflow.com/questions/77914742/gaussian-process-model-predicts-the-same-std-for-all-datapoints</link>
      <description><![CDATA[我用 gpytorch 建立了一个 gp 模型。我创建了一个自定义可能性，它使用连续预测来计算可能性。因此，它不直接使用预测，而是使用连续预测之间的差异。
当我在训练后使用模型进行预测时，它预测了逻辑边界中的所有均值，但预测的标准在所有数据点上完全相同。另外，当我从后部抽取样本时，我观察到非常奇怪、反向和无意义的样本。所以最终，预测方法完全正确，样本不存在，并且所有具有如此高值的数据点的标准差都是相同的。
这是我迄今为止尝试过的：

不同大小和价值的诱导点
不同的学习率和迭代次数
不同内核、双内核等
不同/固定长度尺度、输出尺度，
改变先验
可能性中的不同/固定噪声
锚定预测

对目前的情况有什么了解吗？]]></description>
      <guid>https://stackoverflow.com/questions/77914742/gaussian-process-model-predicts-the-same-std-for-all-datapoints</guid>
      <pubDate>Wed, 31 Jan 2024 16:00:47 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch RuntimeError：函数“NativeBatchNormBackward0”在其第 0 个输出中返回了 nan 值[关闭]</title>
      <link>https://stackoverflow.com/questions/77914164/pytorch-runtimeerror-function-nativebatchnormbackward0-returned-nan-values-in</link>
      <description><![CDATA[我尝试从一篇提供 Tensorflow 代码的论文中实现一个卷积神经网络，并将其转换为 Pytorch。
使用 torch.autograd.detect_anomaly() 我收到错误：
RuntimeError：函数“NativeBatchNormBackward0”在第 0 个输出中返回了 nan 值。
第一次调用loss.backward()期间。
我无法找出此错误背后的原因，因为我在网络上找不到任何有关它的信息。
所讨论的架构是在 conv1d() 期间 -&gt; BatchNorm1d() -&gt;; ReLU() -&gt;; MaxPool1d() 顺序。如果我注释掉 BatchNorm1d() 层，则不会发生错误。
我运行一个自定义损失函数，根据三个特征的两个 BCELosses 和一个 MSELoss 计算加权损失。
损失本身以数字形式返回，并且表现符合预期。
这是一个使用第一个卷积层和自定义损失重现代码的示例：
导入火炬
将 torch.nn 导入为 nn
将 numpy 导入为 np


类 CustomLoss(torch.nn.Module):
    def __init__(自身):
        超级().__init__()

    def 前向（自我，y_pred，y_true，n_splits，weight_prob = 1.0，weight_loc = 1.0，weight_area = 1.0）：
        y_true = torch.Tensor(y_true)
        y_pred = torch.Tensor(y_pred)

        pred_prob、pred_loc、pred_area = torch.tensor_split(y_pred、n_splits、dim=1)
        true_prob、true_loc、true_area = torch.tensor_split(y_true、n_splits、dim=1)

        掩码 = true_prob.eq(1.)

        prob_loss = torch.nn.BCELoss()(true_prob, pred_prob)
        loc_loss = torch.nn.BCELoss()(
            torch.masked_select(true_loc, mask), torch.masked_select(pred_loc, mask))
        面积损失 = torch.nn.MSELoss()(
            torch.masked_select(true_area, mask), torch.masked_select(pred_area, mask))

        返回 （
                概率损失 * 权重概率 +
                loc_loss * 权重_loc +
                面积损失 * 重量面积
        ）


类 PeakDetection(nn.Module):
    def __init__(自身):
        超级().__init__()
        self.n_splits = 3
        self.conv_block1 = nn.Sequential(
            nn.Conv1d(in_channels=1,
                      输出通道=3，
                      内核大小=9，
                      步幅=2，
                      填充=4),
            nn.BatchNorm1d(3),
            ReLU(),
            nn.MaxPool1d(kernel_size=16)
        ）

    def 前向（自身，x）：
        输出 = self.conv_block1(x)
        输出 = self.CustomActivation(输出)
        返回输出

    def CustomActivation（自身，输入）：
        pred, loc, 区域 = torch.tensor_split(输入, self.n_splits, dim=1)
        pred = torch.sigmoid(pred)
        loc = torch.sigmoid(loc)
        return torch.concat([pred,loc,area],dim=1)


torch.autograd.detect_anomaly(True)
# torch.manual_seed(42)
模型 = PeakDetection()

优化器 = torch.optim.Adam(params=model.parameters(), lr=0.01)

X = np.ones((32, 1, 8192))
y = np.ones((32, 3, 256))

使用 torch.autograd.detect_anomaly()：
    y_pred = 模型(火炬.张量(X))
    损失 = CustomLoss()(y_pred, torch.Tensor(y), n_splits=3)
    优化器.zero_grad()
    loss.backward()
    优化器.step()

manual_seed(42) 总是会产生相关的 NativeBatchNormBackward0 错误。]]></description>
      <guid>https://stackoverflow.com/questions/77914164/pytorch-runtimeerror-function-nativebatchnormbackward0-returned-nan-values-in</guid>
      <pubDate>Wed, 31 Jan 2024 14:33:58 GMT</pubDate>
    </item>
    <item>
      <title>我在使用决策树分类器处理数据集时遇到此错误</title>
      <link>https://stackoverflow.com/questions/77913583/i-got-this-error-while-working-on-a-dataset-using-decision-tree-classifier</link>
      <description><![CDATA[&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ImportError Traceback（最近一次调用最后一次）
&lt;ipython-input-48-f146622b3284&gt;在&lt;细胞系：1&gt;()
----&gt; 1 从sklearn.tree导入DecisionTreeClassifier

1 帧
 中的 /usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py
     24 从 scipy.sparse 导入 issparse
     25
---&gt; 26 来自 ..base 导入 (
     27 基础估计器，
     28 分类器混合，

ImportError：无法从“sklearn.base”导入名称“_fit_context”（/usr/local/lib/python3.10/dist-packages/sklearn/base.py）




升级 scikit learn 并卸载或重新安装它。
]]></description>
      <guid>https://stackoverflow.com/questions/77913583/i-got-this-error-while-working-on-a-dataset-using-decision-tree-classifier</guid>
      <pubDate>Wed, 31 Jan 2024 13:09:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么我不能使用基于流程的并行性有效地并行化我的强化学习程序？</title>
      <link>https://stackoverflow.com/questions/77913525/why-cant-i-effectively-parallelize-my-reinforcement-learning-programs-using-pro</link>
      <description><![CDATA[我的目标是使用 Stable_Baselines3 库同时运行多个强化学习程序。我注意到，随着程序数量的增加，程序的迭代速度逐渐降低，这是相当令人惊讶的，因为每个程序应该运行在不同的进程（核心）上。
这是我的程序：
from joblib import 并行，延迟

进口健身房
# 从 sbx 导入 SAC
进口火炬

从 stable_baselines3 导入 SAC
定义火车（）：


    env =gym.make(“Humanoid-v4”)

    模型 = SAC(“MlpPolicy”, env, verbose=1)
    model.learn（total_timesteps=7e5，progress_bar=True）

def train_model():

    火车（）



如果 __name__ == &#39;__main__&#39;:
    程序数量 = 1
    并行(n_jobs=10)(延迟(train)() for i in range(num_of_programs))

num_of_programs 用于控制我尝试并行运行的程序数量。
以下是一些统计数据 -
 程序数量 迭代速度
1 1 ~102 次/秒
2 3 ~60 次/秒
3 10~20次/秒

我确保请求足够的资源，这样就不存在资源限制。这就是我使用 slurm 请求资源的方式 - srun --time=10:00:00 --nodes=1 --cpus-per-task=16 --mem=32G --partition=gpu --gres =gpu:a100-pcie:1 --pty /usr/bin/bash
因此我有 16 个 cpu、32G 内存和 40 GB GPU。
当我从 stable_baselines3 迁移到 sbx 时，我注意到了同样的问题。 stable_baselines3 使用 torch 作为其深度学习库，而后者则使用 JAX。]]></description>
      <guid>https://stackoverflow.com/questions/77913525/why-cant-i-effectively-parallelize-my-reinforcement-learning-programs-using-pro</guid>
      <pubDate>Wed, 31 Jan 2024 13:00:25 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Pytorch 在多节点 GPU 上结合模型和数据并行性</title>
      <link>https://stackoverflow.com/questions/77910976/how-to-combine-model-and-data-parallelism-on-multi-nodes-gpus-using-pytorch</link>
      <description><![CDATA[我有一个8节点的计算集群，每个节点有4个GPU（总共32个GPU，每个GPU只有16Gb内存）。我在我的项目中使用 Pytorch。我可以在所有 32 个 GPU 上使用 DistributedDataParallel 进行数据并行化，但我正在训练的神经网络模型太大，无法放入一个 GPU 内存中，因此数据并行化在这里没有帮助。我尝试进行模型并行化，将模型切割为 4 个部分，但我只能在一个具有 4 个 GPU 的节点上运行它，并进行数据并行化。但我不知道如何将多节点上的数据和模型并行化结合起来。有谁知道如何做到这一点的任何示例（最小可行示例）或想法？
目前，我认为推动这一进程的最佳方法是分两步（或两个级别）进行：
(1) 在一个节点（4个GPU）上进行模型并行化
（2）使用DistributedDataParallel将步骤一中的模型复制到所有8个节点，进行数据并行化
您认为上述想法是个好方法吗？有如何执行此操作的示例吗？]]></description>
      <guid>https://stackoverflow.com/questions/77910976/how-to-combine-model-and-data-parallelism-on-multi-nodes-gpus-using-pytorch</guid>
      <pubDate>Wed, 31 Jan 2024 05:24:42 GMT</pubDate>
    </item>
    <item>
      <title>在Python中查找特征列的哪些过滤集导致最大目标列</title>
      <link>https://stackoverflow.com/questions/77910177/find-which-filter-sets-of-feature-columns-leads-to-maximum-target-column-in-pyth</link>
      <description><![CDATA[我无法找到可以解决我的问题的机器学习模型或分类类型。我本以为这可能相当简单，但也许不是。
假设我有 10 个特征列和一个二进制目标列。目标列的数据集中应该有大致相等数量的 0 和 1。整组数据并不是强相关的，所以线性回归、逻辑回归、朴素贝叶斯等都没有得出强相关的模型。然而，我所寻找的是哪个数据系列导致目标列的最大平均值。
例如。对于特征集 A 到 J 如果我按（C = True、D = false、J = true）过滤数据集，则目标 X 的平均值现在为 56%。我正在寻找一种算法，可以找到导致最大目标列均值的方程。
我觉得这可以通过蛮力来完成（循环遍历所有可能的组合），但我希望有一种方法可以在现有的众多数据科学库之一中做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/77910177/find-which-filter-sets-of-feature-columns-leads-to-maximum-target-column-in-pyth</guid>
      <pubDate>Wed, 31 Jan 2024 00:30:49 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：调用层“time_distributed_4”时遇到异常（类型 TimeDistributed）</title>
      <link>https://stackoverflow.com/questions/77909132/valueerror-exception-encountered-when-calling-layer-time-distributed-4-type</link>
      <description><![CDATA[我尝试使用 Kvasir 数据集制作 CNN-LSTM 模型。我使用 image_dataset_from_directory 分割数据集，如下所示：
dataset_path = “/kaggle/working/dataset”
图像大小 = 224, 224
批量大小 = 64

train_ds = image_dataset_from_directory(
  数据集_路径，
  验证分割=0.2，
  子集=“训练”，
  label_mode =“分类”，
  种子=23，
  图像大小=图像大小，
  color_mode =“rgb”，
  批量大小=批量大小）


val_ds = image_dataset_from_directory(
  数据集_路径，
  验证分割=0.2，
  子集=“验证”，
  label_mode =“分类”，
  种子=23，
  图像大小=图像大小，
  color_mode=“RGB”，
  批量大小=批量大小）

这个函数给了我一个 BatchDataset。然后我将基数设置如下：
val_batches = tf.data.experimental.cardinality(val_ds)
test_ds = val_ds.take(val_batches // 2)
val_ds = val_ds.skip(val_batches // 2)

然后
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)

这段代码还给了我一个预取数据集。当我运行 print(train_ds) 时它给出：
&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 8), dtype=tf .float32，名称=无））&gt;

然后我添加了我的模型，
 model = tf.keras.models.Sequential([
    # 具有批量归一化和最大池化的卷积层
    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, (3, 3), 激活=无,input_shape=(224, 224,3))),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3), 激活=无)),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),



    # 压平输出并添加密集层
    tf.keras.layers.TimeDistributed(Flatten()),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.LSTM(256,激活=&#39;tanh&#39;),
    
    
    
    tf.keras.layers.Dense(256),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),

    # 具有 8 个节点的输出层用于分类
    tf.keras.layers.Dense(8, 激活=&#39;softmax&#39;)
]）

# 编译模型

model.compile(优化器=AdamW(learning_rate=0.001,weight_decay=0.004,beta_1=0.9,beta_2=0.999,epsilon=1e-8),
          损失=分类交叉熵(),
          指标=[&#39;准确性&#39;])

当我适合这个模型时它没有运行并且出现错误：
ValueError：调用层“time_distributed_4”（类型 TimeDistributed）时遇到异常。
    
    层“conv2d_2”的输入0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、224、3）
    
    调用层“time_distributed_4”接收的参数（类型 TimeDistributed）：
      输入= tf.Tensor（形状=（无，224，224，3），dtype = float32）
      • 训练=真
      • 掩码=无

我不知道如何解决这个问题，你能帮助我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77909132/valueerror-exception-encountered-when-calling-layer-time-distributed-4-type</guid>
      <pubDate>Tue, 30 Jan 2024 20:05:50 GMT</pubDate>
    </item>
    <item>
      <title>ValidationError：StuffDocumentsChain __root__ 出现 1 个验证错误</title>
      <link>https://stackoverflow.com/questions/76776695/validationerror-1-validation-error-for-stuffdocumentschain-root</link>
      <description><![CDATA[我收到此错误ValidationError：在 llm_chain input_variables 中找不到 StuffDocumentsChain __root__ document_variable_name 上下文的 1 个验证错误：[&#39;chat_history&#39;、&#39;user_query&#39;、&#39;relevant_context&#39;] (type=value_error)
在使用 load_qa_chain 时，我搜索了此错误，但没有找到与此相关的任何内容。谁能告诉我这里缺少什么。
代码：
template = &quot;&quot;&quot;您是一个正在与人类对话的聊天机器人。

给定长文档和问题的以下提取部分，创建最终答案。

{相关上下文}

{聊天记录}
人类：{user_query}
聊天机器人：“”“”

提示=提示模板(
input_variables=[“chat_history”, “user_query”, “relevant_context”],
模板=模板
）

内存= ConversationBufferMemory（memory_key =“聊天历史记录”，input_key =“用户查询”）

llm = OpenAI()
llm_chain = LLMChain(
    llm=llm,
    提示=提示，
    内存=内存，
）

链 = load_qa_chain(
    llm，chain_type =“东西”，内存=内存，提示=提示
）
]]></description>
      <guid>https://stackoverflow.com/questions/76776695/validationerror-1-validation-error-for-stuffdocumentschain-root</guid>
      <pubDate>Thu, 27 Jul 2023 05:20:25 GMT</pubDate>
    </item>
    <item>
      <title>部署时，SageMaker 无法提取容器的模型数据存档 tar.gz</title>
      <link>https://stackoverflow.com/questions/65881699/sagemaker-failed-to-extract-model-data-archive-tar-gz-for-container-when-deployi</link>
      <description><![CDATA[我正在尝试在 Amazon Sagemaker 中部署现有的 Scikit-Learn 模型。所以这个模型不是在 SageMaker 上训练的，而是在我的机器上本地训练的。
在我的本地（Windows）计算机上，我已将模型保存为 model.joblib 并将模型压缩为 model.tar.gz。
接下来，我已将此模型上传到我的 S3 存储桶 (&#39;my_bucket&#39;)，路径为 s3://my_bucket/models/model.tar.gz。我可以在 S3 中看到 tar 文件。
但是当我尝试部署模型时，它不断给出错误消息“无法提取模型数据存档”。
.tar.gz 是通过在 powershell 命令窗口中运行“tar -czf model.tar.gz model.joblib”在我的本地计算机上生成的。
上传到S3的代码
&lt;前&gt;&lt;代码&gt;导入boto3
s3 = boto3.client(“s3”,
              Region_name=&#39;eu-central-1&#39;,
              aws_access_key_id=AWS_KEY_ID,
              aws_secret_access_key=AWS_SECRET)
s3.upload_file(文件名=&#39;model.tar.gz&#39;, Bucket=my_bucket, Key=&#39;models/model.tar.gz&#39;)

用于创建估计器和部署的代码：
&lt;前&gt;&lt;代码&gt;导入boto3
从 sagemaker.sklearn.estimator 导入 SKLearnModel

...

model_data = &#39;s3://my_bucket/models/model.tar.gz&#39;
sklearn_model = SKLearnModel(model_data=model_data,
                             角色=角色，
                             Entry_point =“my-script.py”，
                             Framework_version =“0.23-1”）
预测器= sklearn_model.deploy（instance_type =“ml.t2.medium”，initial_instance_count = 1）

错误信息：
&lt;块引用&gt;
错误消息：UnexpectedStatusException：托管端点错误
sagemaker-scikit-learn-2021-01-24-17-24-42-204：失败。原因：失败
提取容器“container_1”的模型数据档案来自网址
“s3://my_bucket/models/model.tar.gz”。请确保对象
位于 URL 处的是有效的 tar.gz 存档

有没有办法查看存档无效的原因？]]></description>
      <guid>https://stackoverflow.com/questions/65881699/sagemaker-failed-to-extract-model-data-archive-tar-gz-for-container-when-deployi</guid>
      <pubDate>Mon, 25 Jan 2021 09:03:31 GMT</pubDate>
    </item>
    <item>
      <title>重新训练 Tensorflow 模型</title>
      <link>https://stackoverflow.com/questions/52769607/retrain-tensorflow-model</link>
      <description><![CDATA[我有一个使用对象检测 SSD 移动网络训练的张量流模型。
训练现已完成，我导出了模型推理以进行测试。我的问题是，如果我想稍后使用新的图像数据集重新训练模型，我现在应该在这个阶段做什么以使权重渗透到模型中，以便我可以从那时起重新训练它。我知道有一个冻结脚本，我必须使用它吗？ 
谢谢
阿亚德]]></description>
      <guid>https://stackoverflow.com/questions/52769607/retrain-tensorflow-model</guid>
      <pubDate>Thu, 11 Oct 2018 22:12:07 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用具有流输入和输出的机器学习库？ [关闭]</title>
      <link>https://stackoverflow.com/questions/50850497/is-it-possible-to-use-a-machine-learning-library-with-streaming-inputs-and-outpu</link>
      <description><![CDATA[我想将机器学习纳入我一直在从事的项目中，但我还没有看到任何关于我的预期用例的信息。看起来旧的潘多拉魔盒项目做了类似的事情，但是有文本输入和输出。
我想实时训练一个模型并使用它（然后当它运行良好时将其从测试切换到实时 api 端点。）
但我发现的每个库的工作方式都类似于“输入数据块，得到答案”
我希望能够将数据传输到其中：
而不是给它“5,4,3,4,3,2,3,4,5”，它说“1”或“-1”或“0”
我想给它“5”然后“4”然后“3”然后“4”等等，每次它响应时。
我什至不确定“流媒体”是否是正确的词。请帮忙！]]></description>
      <guid>https://stackoverflow.com/questions/50850497/is-it-possible-to-use-a-machine-learning-library-with-streaming-inputs-and-outpu</guid>
      <pubDate>Thu, 14 Jun 2018 05:54:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中实现多元线性回归？</title>
      <link>https://stackoverflow.com/questions/48257144/how-do-i-implement-multiple-linear-regression-in-python</link>
      <description><![CDATA[我正在尝试从头开始编写一个多元线性回归模型来预测影响 Facebook 上歌曲观看次数的关键因素。关于每首歌曲，我们收集这些信息，即我正在使用的变量：

&lt;前&gt;&lt;代码&gt;df.dtypes
单击 int64
Listened_5s int64 已听
Listened_20s int64 已听
视图 int64
已听百分比 float64
反应总数 int64
共享歌曲 int64
评论 int64
平均听时间 int64
歌曲长度 int64
喜欢 int64
已收听_稍后 int64

我使用视图数作为因变量，并将数据集中的所有其他变量作为独立变量。该模型发布如下：

&lt;前&gt;&lt;代码&gt; #df_x --&gt;自变量的新数据框
  df_x = df.drop([&#39;视图&#39;], 1)

  #df_y --&gt;因变量视图的新数据框
  df_y = df.ix[:, [&#39;视图&#39;]]

  名称 = [i 代表列表中的 i(df_x)]

  regr = Linear_model.LinearRegression()
  x_train，x_test，y_train，y_test = train_test_split（df_x，df_y，test_size = 0.2）

   #将模型拟合到训练数据集
   regr.fit(x_train,y_train)
   regr.intercept_
   print(&#39;系数: \n&#39;, regr.coef_)
   print(&quot;均方误差(MSE): %.2f&quot;
         % np.mean((regr.predict(x_test) - y_test) ** 2))
   print(&#39;方差分数: %.2f&#39; % regr.score(x_test, y_test))
   regr.coef_[0].tolist()

此处输出：
 regr.intercept_
 数组([-1173904.20950487])
 微信：19722838329246.82
 方差得分：0.99

看起来出了什么严重错误。
尝试 OLS 模型：
 import statsmodels.api as sm
   从 statsmodels.sandbox.regression.predstd 导入 wls_prediction_std
   模型=sm.OLS(y_train,x_train)
   结果=模型.fit()
   打印(结果.summary())

输出：

&lt;前&gt;&lt;代码&gt; R 平方：0.992
     F 统计量：6121。

                      coef std err t P&gt;|t| [95.0% 浓度国际]


点击 0.3333 0.012 28.257 0.000 0.310 0.356
Listened_5s -0.4516 0.115 -3.944 0.000 -0.677 -0.227
已听 20 秒 1.9015 0.138 13.819 0.000 1.631 2.172
已听百分比 7693.2520 1.44e+04 0.534 0.594 -2.06e+04 3.6e+04
反应总数 8.6680 3.561 2.434 0.015 1.672 15.664
共享歌曲 -36.6376 3.688 -9.934 0.000 -43.884 -29.392
评论 34.9031 5.921 5.895 0.000 23.270 46.536
平均听时间 1.702e+05 4.22e+04 4.032 0.000 8.72e+04 2.53e+05
歌曲长度 -6309.8021 5425.543 -1.163 0.245 -1.7e+04 4349.413
喜欢 4.8448 4.194 1.155 0.249 -3.395 13.085
稍后收听 -2.3761 0.160 -14.831 0.000 -2.691 -2.061


综合巴士：233.399 杜宾-沃森：
1.983
概率（综合）： 0.000 Jarque-Bera (JB)：
2859.005
偏差：1.621 概率（JB）：
0.00
峰度：14.020 条件。不。
2.73e+07

警告：
[1] 标准误差假设误差的协方差矩阵已正确指定。
[2] 条件数很大，2.73e+07。这可能表明存在很强的多重共线性或其他数值问题。

仅通过查看此输出就可以看出出现了严重错误。
我认为训练/测试集和创建两个不同的数据框 x 和 y 出了问题，但无法弄清楚是什么。这个问题必须可以通过使用多元回归来解决。难道不是线性的吗？您能帮我找出问题所在吗？]]></description>
      <guid>https://stackoverflow.com/questions/48257144/how-do-i-implement-multiple-linear-regression-in-python</guid>
      <pubDate>Mon, 15 Jan 2018 04:58:03 GMT</pubDate>
    </item>
    </channel>
</rss>