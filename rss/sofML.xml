<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 08 Apr 2024 21:12:26 GMT</lastBuildDate>
    <item>
      <title>寻求最新的图像分类模型以实现高精度和微调代码</title>
      <link>https://stackoverflow.com/questions/78294759/seeking-latest-image-classification-models-for-high-accuracy-and-fine-tuning-cod</link>
      <description><![CDATA[我目前正在从事一个专注于图像分类的研究项目，特别强调利用机器学习模型的最新进展。我的主要目标是在图像分类任务中实现最高水平的准确性。为了实现这一目标，我正在寻求 2020 年后发布的最新图像分类模型的指导，这些模型与早期模型相比已表现出卓越的准确性。此外，我已将数据集上传到 Google 云端硬盘，并需要帮助使用这些尖端模型微调此数据集，以获得尽可能高的准确度。
在努力解决这个研究问题的过程中，我探索了与图像分类模型相关的各种资源和文档。然而，由于该领域的快速发展，我正在寻求有关最新模型的更新信息和指导，这些模型已显示出显着的准确性改进。我希望找到全面的文档或资源，详细介绍最新模型及其微调程序，以达到最高的准确度水平。然而，到目前为止我所了解到的信息还比较有限，这促使我向社区寻求帮助和见解。]]></description>
      <guid>https://stackoverflow.com/questions/78294759/seeking-latest-image-classification-models-for-high-accuracy-and-fine-tuning-cod</guid>
      <pubDate>Mon, 08 Apr 2024 19:35:40 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络背景下的图信号</title>
      <link>https://stackoverflow.com/questions/78294645/graph-signal-in-the-context-of-graph-neural-network</link>
      <description><![CDATA[我正在阅读有关图神经网络论文的一些有关图信号处理的材料，我发现图信号被定义为向量，因此每个节点信号都是标量。在实践中，节点通常具有向量作为其特征，所以我想知道图形信号如何扩展到这种情况。如果您能在答案中包含参考文献，我将不胜感激。我正在阅读的包含图形信号定义的示例材料之一是 https： //www.seas.upenn.edu/~ese2240/labs/1200_ch_9_signal_processing_on_graphs.pdf]]></description>
      <guid>https://stackoverflow.com/questions/78294645/graph-signal-in-the-context-of-graph-neural-network</guid>
      <pubDate>Mon, 08 Apr 2024 19:02:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 HDF5 可以保存哪些 ML 模型？</title>
      <link>https://stackoverflow.com/questions/78294583/which-ml-models-can-be-saved-using-hdf5</link>
      <description><![CDATA[我已阅读 HDF5 格式 可用于保存机器学习模型。但是，当使用经过训练的 CNNClassifier来自 sktime 的  实例：
导入 h5py
从 sktime.classification.deep_learning.cnn 导入 CNNClassifier
cnn = CNNClassifier(n_epochs=100, batch_size=4) # 它运行 Ne epochs
与 h5py.File(
        “测试.h5”，
        “w”
    ) 作为 f:
    dataset_cnn = f.create_dataset(“cnn”, data=cnn)

但是，我收到以下错误：
回溯（最近一次调用最后一次）：
  文件“”，第 5 行，位于  中。
  文件“/home/tapyu/.cache/pypoetry/virtualenvs/tscnn-PNsUTi5L-py3.10/lib/python3.10/site-packages/h5py/_hl/group.py”，第 183 行，在 create_dataset 中
    dsid = dataset.make_new_dset(组、形状、dtype、数据、名称、**kwds)
  文件“/home/tapyu/.cache/pypoetry/virtualenvs/tscnn-PNsUTi5L-py3.10/lib/python3.10/site-packages/h5py/_hl/dataset.py”，第 86 行，在 make_new_dset 中
    tid = h5t.py_create(dtype, 逻辑=1)
  文件“h5py/h5t.pyx”，第 1658 行，位于 h5py.h5t.py_create 中
  文件“h5py/h5t.pyx”，第 1682 行，位于 h5py.h5t.py_create 中
  文件“h5py/h5t.pyx”，第 1742 行，位于 h5py.h5t.py_create 中
类型错误：对象 dtype dtype(&#39;O&#39;) 没有本机 HDF5 等效项

毕竟，哪些机器学习模型可以使用 HDF5 保存？]]></description>
      <guid>https://stackoverflow.com/questions/78294583/which-ml-models-can-be-saved-using-hdf5</guid>
      <pubDate>Mon, 08 Apr 2024 18:50:35 GMT</pubDate>
    </item>
    <item>
      <title>我必须使用哪种机器学习模型以及如何使用？</title>
      <link>https://stackoverflow.com/questions/78294353/which-ml-model-i-have-to-use-and-how</link>
      <description><![CDATA[我使用图像数据集训练了 cnn，其中一个文件夹 real 包含此图像在此处输入图像描述 全部都像这样，其他文件夹包含不同的图像，如下在此处输入图像描述。现在我已经给出了一张照片在此处输入图像描述，该照片不是真实的，因此 moel 有给出假作为输出，但它给出真实。如何克服它。还有其他模型可以对此进行分类吗？你给我的帮助将会非常有帮助。
我想要训练哪个模型以及逐步的过程]]></description>
      <guid>https://stackoverflow.com/questions/78294353/which-ml-model-i-have-to-use-and-how</guid>
      <pubDate>Mon, 08 Apr 2024 17:57:17 GMT</pubDate>
    </item>
    <item>
      <title>人工智能根据文本输入揭示图像识别的具体细节</title>
      <link>https://stackoverflow.com/questions/78293860/ai-revealing-specific-details-in-image-recognition-based-on-a-text-input</link>
      <description><![CDATA[我正在尝试执行以下操作：

我想使用某种 AI（免费）API 来执行自动图像识别
AI 的研究（并提取随后的具体细节）必须基于提供的文本输入，说明在图像分析过程中要关注哪些细节。

例如如果我向人工智能提供一张充满慢跑者的游乐场的图像，左上角有一个人骑着红色自行车，右下角有一个人骑着绿色自行车，我还必须能够向它提供文本“告诉我你在左上角看到的自行车的颜色”并且输出必须是“红色”。
或者，如果它是冬季游乐场的图像，我需要能够对人工智能说“看看左上角的树，识别它的特征，它看起来是光秃秃的还是茂盛的？”并且输出应该是“裸”。
由于我没有找到任何 API 能够一次性执行我所要求的操作，因此我尝试手动使用 Google Cloud Vision API 提供的图像分析提供的文本输入和标签。
我正在研究的特定领域是基于对某些鸟类的性别的识别，基于其身体的色彩细节
# 定义兼容详细信息的预定义列表（这只是一个示例）
Compatible_details = [“黄色宝箱”,“绿色宝箱”,“红头”,“蓝头”]

# 处理文本输入以提取详细信息
def process_textual_input(textual_input):
    提取的详细信息 = []
    有关兼容详细信息的详细信息：
        如果textual_input中有详细信息：
            extract_details.append（详细信息）
    返回提取的详细信息

# 分析图像并匹配细节
defanalyze_image_with_details(image_path,extracted_details):
    # 使用图像分析API获取标签
    # 使用提取的细节仅关注有趣的细节
    以 open(image_path, “rb”) 作为 image_file：
        内容 = image_file.read()

    图像 = 视觉.Image(内容=内容)

    # 通过Google云视觉API分析图像
    响应 = client.label_detection(图像=图像)
    标签=response.label_annotations

    # 将提取的详细信息与识别的标签进行比较
    结果=[]
    有关 extract_details 中的详细信息：
        对于标签中的标签：
            如果 label.description.lower() 中的detail.lower()：
                results.append((detail, 如果detail 为“黄色”，则为“男性”。lower() else “女性”))
    返回结果

＃ 主要的
如果 __name__ == “__main__”：
    # 文本输入示例
    textual_input = “看小鸟的胸部。如果它是黄色的，那么它就是雄性。”
    
    # 处理文本输入以获取详细信息
    提取的详细信息 = process_textual_input(textual_input)
    
    # 要分析的图像列表
    图片 = [“image1.jpg”、“image2.jpg”、“image3.jpg”]
    
    # 分析提取细节的图像
    对于图像中的 image_path：
        结果=analyze_image_with_details(image_path,extracted_details)
        打印（f“图像：{image_path}”）
        详细信息，结果中的性别：
            print(f&quot;详细信息: {detail}, 性别: {gender}&quot;)

但是，当然，正如你所看到的，我只是从文本中静态提取关键字，而不是让人工智能思考和理解它。
我该如何执行此操作？]]></description>
      <guid>https://stackoverflow.com/questions/78293860/ai-revealing-specific-details-in-image-recognition-based-on-a-text-input</guid>
      <pubDate>Mon, 08 Apr 2024 16:12:09 GMT</pubDate>
    </item>
    <item>
      <title>如何训练机器学习模型，该模型可以从虹膜物种的测量中学习并对它们进行分类</title>
      <link>https://stackoverflow.com/questions/78293619/how-to-train-a-machine-learning-model-that-can-learn-from-the-measurements-of-th</link>
      <description><![CDATA[鸢尾花有三个品种； setosa、versicolor 和 virginica，根据它们的不同而有所不同
测量。现在假设您根据以下公式测量了鸢尾花的尺寸
他们的物种，在这里你的任务是训练一个机器学习模型，可以从
测量虹膜种类并对它们进行分类。
虽然 Scikit-learn 库提供了用于鸢尾花分类的数据集，但您也可以
从此处下载相同的数据集，用于使用机器进行鸢尾花分类的任务
学习。
# 从数据集中导入库。
从 sklearn.datasets 导入 load_iris
从 sklearn.model_selection 导入 train_test_split
从 sklearn.tree 导入 DecisionTreeClassifier
从sklearn.metrics导入accuracy_score，classification_report

# 加载鸢尾花数据集
虹膜 = load_iris()
X = iris.data # 用于对鸢尾花物种进行分类的特征
y = iris.target # 用于预测的标签

# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 初始化模型
clf = DecisionTreeClassifier(random_state=42)

# 训练模型
clf.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = clf.predict(X_test)

# 评估模型
准确度=准确度_得分(y_test, y_pred)
print(f&#39;准确率: {准确率*100:.2f}%&#39;)

# 详细分类报告
打印（分类报告（y_test，y_pred，target_names = iris.target_names））

]]></description>
      <guid>https://stackoverflow.com/questions/78293619/how-to-train-a-machine-learning-model-that-can-learn-from-the-measurements-of-th</guid>
      <pubDate>Mon, 08 Apr 2024 15:29:58 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 中的验证准确率超过 100%？</title>
      <link>https://stackoverflow.com/questions/78293441/validation-accuracy-going-over-100-in-pytorch</link>
      <description><![CDATA[我目前正在做一个项目，尝试对乳腺癌肿瘤的图像进行正确分类，以预测它们是良性（正常）还是恶性（危险），我正在使用 Pytorch。我已经设置了扩展 ResNet18 模型的网络并添加了更多层。对于超参数，本来是我自己设置的，但想用随机搜索等方法来找到最好的超参数。然而，在运行验证循环管道以获得准确性后，我发现它超过了 100%？
我像这样加载数据：
&lt;前&gt;&lt;代码&gt;
导入火炬.utils
导入 torch.utils.data
从 torchvision.transforms 导入变换
导入 torch.utils.data 作为数据

Transformed_data = Transforms.Compose([ # 使用 Compose() 链接数据转换
    Transforms.ToTensor()
]）



Training_dataset = DataSetClass(split=“训练”，transform=transformed_data，download=下载)
#testing_dataset = DataSetClass(split=“测试”,transform=transformed_data,download=下载)

train_size = int(training_ratio * len(training_dataset))
validation_size = int(validation_ratio * len(training_dataset))
测试大小 = len(训练数据集) - 训练大小 - 验证大小

train_set、validation_set、testing_dataset = torch.utils.data.random_split(
    训练数据集，[训练大小，验证大小，测试大小]
）

# 将数据转换为dataloader形式
train_loader = data.DataLoader(数据集=train_set,batch_size=BATCH_SIZE,shuffle=True)
validation_loader = data.DataLoader(数据集=validation_set,batch_size=2*BATCH_SIZE,shuffle=False)
test_loader = data.DataLoader(数据集=testing_dataset,batch_size=BATCH_SIZE,shuffle=False)


打印（训练数据集）
打印（“========================”）
打印（测试数据集）

我尝试创建一个列表，存储每个超参数的不同值，如下所示，同时执行验证循环：
导入 torch.optim 作为 optim
将 numpy 导入为 np

# 定义你的超参数空间
lr_空间 = [0.01, 0.02, 0.03, 0.04, 0.05]
纪元空间 = [10, 20, 30, 40, 50]
批量大小空间 = [32, 64, 128, 256]

# 初始化你的网络
网络 = 扩展网络(resnet18)
网络.to(设备=设备)

# 定义你的损失函数
loss_function = nn.BCEWithLogitsLoss()

网络.eval()

最佳准确度 = 0
最佳超参数 = 无
验证准确度 = 0


# 执行随机搜索
对于 _ 在范围（100）内：
    lr = np.random.choice(lr_space)
    epochs = np.random.choice(epochs_space)
    batch_size = np.random.choice(batch_size_space)

    # 使用选定的超参数来训练您的模型
    优化器 = optim.Adam(network.parameters(), lr=lr)
    
    对于范围内的纪元（纪元）：
        使用 torch.no_grad()：
            对于输入，validation_loader 中的目标：
                输入，目标 = 输入.to(设备), 目标.to(设备)

                ＃ 向前
                输出 = 网络（输入）

                # 计算并累加准确率
                验证准确度 += 准确度（输出，目标）

        # 计算所有验证批次的平均准确度
        验证准确度 /= len(validation_loader)

        print(&#39;验证准确度：&#39;,validation_accuracy)

        # 如果当前模型优于之前所有模型，则更新最佳精度和最佳超参数
        如果validation_accuracy&gt;最佳准确度：
            最佳准确度 = 验证准确度
            best_hyperparameters = {&#39;lr&#39;：lr，&#39;epochs&#39;：epochs，batch_size&#39;：batch_size}

print(&#39;最佳准确度：&#39;, best_accuracy)
print(&#39;最佳超参数：&#39;, best_hyperparameters)

这会导致结果：
验证准确度：0.045871559633027525
验证精度：0.09174311926605505
...
验证准确度：135.73394495412717
验证准确度：135.77981651376018

最佳准确度：135.77981651376018
最佳超参数：{&#39;lr&#39;: 0.03, &#39;epochs&#39;: 40, &#39;batch_size&#39;: 256}

但是准确率应该限制在 100% 吗？我不明白我的错误是否来自我加载数据的方式或验证循环管道
编辑：

在纪元内移动了validation_accuracy（我之前做过，但仍然得到一个既不增加也不减少的恒定准确度，类似于“验证准确度：0.3761467889908257”

这是否是由于我使用自己设置的批量大小并将其用于加载器变量（例如train_loader、validation_loader、test_loader）的方式，但后来我想使用批量大小列表？我应该使用加载器内的 batch_size_space 值吗？

]]></description>
      <guid>https://stackoverflow.com/questions/78293441/validation-accuracy-going-over-100-in-pytorch</guid>
      <pubDate>Mon, 08 Apr 2024 15:03:29 GMT</pubDate>
    </item>
    <item>
      <title>排列检验中的 R 平方</title>
      <link>https://stackoverflow.com/questions/78293145/r-squared-in-permutation-test</link>
      <description><![CDATA[在排列测试期间，回归模型得到负值（-0.5 左右）是否正常（交叉验证中的正常值约为 0.8）？
我用 100 个排列运行了测试，我预计分数约为 0，但也许我不理解回归模型的此测试的目的，因为对于分类问题，我预计准确度分数为 0.5（完全随机预测） 。
您认为这个结果是完全错误的还是有有价值的意义？]]></description>
      <guid>https://stackoverflow.com/questions/78293145/r-squared-in-permutation-test</guid>
      <pubDate>Mon, 08 Apr 2024 14:12:47 GMT</pubDate>
    </item>
    <item>
      <title>YOLO 中的调整大小如何影响模型精度？</title>
      <link>https://stackoverflow.com/questions/78293078/how-resize-in-yolo-impact-to-model-accuracy</link>
      <description><![CDATA[我使用 YOLOv8X 进行照片中的物体检测，最近我一直想知道图像大小对检测质量的影响。我通常使用 Roboflow 中的数据集，并将大小调整为 1280x1280（这比标准大小调整为 640x640 的性能明显更好），但原始照片的长宽比通常远离正方形，调整大小会改变照片中对象的比例。数据集中的注释会根据新的照片尺寸重新计算，但更改比例会降低检测质量吗？我可以在不调整输入照片大小的情况下训练模型吗？如果我可以在训练期间简单地指定参数 imgsz=1280，是否需要在 Roboflow 上调整大小？
我在 Roboflow 上应用和不应用调整大小的情况下训练了模型，质量指标相似。数据集中的一些照片在大小调整到 1280x1280 后出现不自然的拉伸，这可能会由于对象形状的变化而降低模型识别对象的能力。]]></description>
      <guid>https://stackoverflow.com/questions/78293078/how-resize-in-yolo-impact-to-model-accuracy</guid>
      <pubDate>Mon, 08 Apr 2024 14:02:20 GMT</pubDate>
    </item>
    <item>
      <title>CNN 教程模型根本拒绝训练</title>
      <link>https://stackoverflow.com/questions/78293057/cnn-tutorial-models-refusing-to-train-at-all</link>
      <description><![CDATA[我遇到的问题涉及在一些众所周知的数据集上创建和使用 CNN 模型。问题始于我的一项家庭作业，我们应该创建一个 CNN 并在 CIFAR10 数据集上运行它。然而这个问题似乎更加严重，我怀疑可能出了什么问题。
我注意到，在运行我自己的模型时，无论我制作什么模型或调整什么超参数，性能都与随机猜测一致。
为了更好地了解什么是好的模型，我在 CIFAR10 和 MNIST 上在线下载了一些教程。教程页面、网站等上的这些模型都报告了不错的准确度，范围在 60-80% 之间。
然而事情就变得奇怪了。当我使用相同的数据预处理等运行这些完全相同的模型（我下载了文件，因此没有复制/粘贴错误）时，我得到了与我自己的模型相同的结果，没有学习发生，准确性保持在 10%。这种行为与我在网上找到的至少四个不同的教程示例是一致的。
我尝试创建一个新的 conda 环境，以便我可以全新安装 tensorflow-gpu，并确保尽可能使用与教程相同的版本，但无论出于何种原因，模型似乎拒绝训练为我。下面是一个最小的可重现示例，我只是从 tensorflow.org 上的 CNN 教程示例和我的结果中复制/粘贴了该示例。
导入tensorflow为tf

从tensorflow.keras导入数据集、图层、模型
将 matplotlib.pyplot 导入为 plt

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# 将像素值标准化为 0 到 1 之间
训练图像，测试图像 = 训练图像 / 255.0，测试图像 / 255.0


模型 = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), 激活=&#39;relu&#39;, input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), 激活=&#39;relu&#39;))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), 激活=&#39;relu&#39;))


model.add(layers.Flatten())
model.add(layers.Dense(64,activation=&#39;relu&#39;))
model.add(layers.Dense(10))

model.compile(优化器=&#39;亚当&#39;,
          损失=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
          指标=[&#39;准确性&#39;])

历史 = model.fit(train_images, train_labels, epochs=10,
                验证数据=（测试图像，测试标签））

1563/1563 [================================] - 250s 11ms/步 - 损失：2.3027 - 精度：0.0977 - val_loss ：2.3026 - val_accuracy：0.1000
纪元 2/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3028 - 准确度：0.0999 - val_loss ：2.3027 - val_accuracy：0.1000
纪元 3/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3027 - 准确度：0.1011 - val_loss ：2.3027 - val_accuracy：0.1000
纪元 4/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3028 - 精度：0.0993 - val_loss ：2.3027 - val_accuracy：0.1000
纪元 5/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3027 - 准确度：0.0957 - val_loss ：2.3026 - val_accuracy：0.1000
纪元 6/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3028 - 准确度：0.0979 - val_loss ：2.3026 - val_accuracy：0.1000
纪元 7/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3027 - 准确度：0.0997 - val_loss ：2.3026 - val_accuracy：0.1000
]]></description>
      <guid>https://stackoverflow.com/questions/78293057/cnn-tutorial-models-refusing-to-train-at-all</guid>
      <pubDate>Mon, 08 Apr 2024 13:58:28 GMT</pubDate>
    </item>
    <item>
      <title>YOLOV8 预测相反类别</title>
      <link>https://stackoverflow.com/questions/78292691/yolov8-predicting-opposite-classes</link>
      <description><![CDATA[我正在尝试使用 Yolov8n.pt 检测对象。我已经使用 RoboFlow 分配了课程。该模型预测的类别非常相反。为什么？如果有请给出解决方案。谢谢。
我尝试改进 epoch 并尝试删除其他类。]]></description>
      <guid>https://stackoverflow.com/questions/78292691/yolov8-predicting-opposite-classes</guid>
      <pubDate>Mon, 08 Apr 2024 12:57:12 GMT</pubDate>
    </item>
    <item>
      <title>CIFAR10 数据集上的 Tensorflow CNN 模型无法学习</title>
      <link>https://stackoverflow.com/questions/78289990/tensorflow-cnn-model-on-cifar10-dataset-not-learning</link>
      <description><![CDATA[首先，我最近一直在做一项家庭作业，在 CIFAR10 灰度数据集上实现 CNN 模型，以展示其与之前作业中的 FNN 相比的准确性。
但是，这不是我的作业问题，因为我想讨论的训练 CNN 时似乎遇到了更深层次的系统性问题。
值得注意的是，当我运行 CNN 时，似乎无论什么参数、数据集或任何细节都会改变我的模型根本无法学习的事实。
例如，当我在 youtube 上下载与本教程相关的 jupyter 笔记本并自己运行它时，我首先怀疑有更深层次的错误：
Github 链接
对于 FNN，我得到的结果与笔记本/视频中显示的结果完全相同。然而，当我运行 CNN 的单元时，我发现模型没有进行训练，并且损失了 Nan，准确度为 10%，这与随机猜测一致。视频/笔记本说我应该获得 70% 左右的准确度，所以这似乎非常错误。
我下载的另一个教程也发生了这种情况。他们报告在 MNIST 数据集上的准确度为 90%，但是当我运行完全相同的代码时，该模型仅保持在 10% 左右。
如果相关，我可以提供 tf、numpy 等的版本。首先，我使用 GPU 版本的 Tensorflow 在 conda 环境中运行这一切。
下面是我的问题的一个最小示例。我从教程中得到了这个精确的模型、数据处理等，该教程应该给出大约 90% 的准确率，但当我训练模型时，它保持在 10% 左右。
导入tensorflow为tf
将 numpy 导入为 np
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 plt
将 matplotlib.image 导入为 mpimg
导入迭代工具

打印（tf.__版本__）

cifar10 = tf.keras.datasets.cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

y_train = y_train.flatten()
y_test = y_test.flatten()

输入形状 = (32, 32, 3)

x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 3)
x_train=x_train / 255.0
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 3)
x_test=x_test / 255.0

y_train = tf.one_hot(y_train.astype(np.int32), 深度=10)
y_test = tf.one_hot(y_test.astype(np.int32), 深度=10)

批量大小 = 32
班级数 = 10
纪元 = 50

模型 = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, 3, 填充=&#39;相同&#39;, input_shape=x_train.shape[1:], 激活=&#39;relu&#39;),
    tf.keras.layers.Conv2D(32, 3, 激活=&#39;relu&#39;),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Dropout(0.25),

    tf.keras.layers.Conv2D(64, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;),
    tf.keras.layers.Conv2D(64, 3, 激活=&#39;relu&#39;),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.BatchNormalization(),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense（512，激活=&#39;relu&#39;），
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense（num_classes，激活=&#39;sigmoid&#39;），
]）


model.compile（优化器=tf.keras.optimizers.RMSprop（learning_rate=0.0001，衰减=1e-06），
                损失=&#39;categorical_crossentropy&#39;，指标=[&#39;acc&#39;]）

历史 = model.fit(x_train, y_train, batch_size=batch_size,
                    纪元=纪元）
]]></description>
      <guid>https://stackoverflow.com/questions/78289990/tensorflow-cnn-model-on-cifar10-dataset-not-learning</guid>
      <pubDate>Mon, 08 Apr 2024 02:56:44 GMT</pubDate>
    </item>
    <item>
      <title>澄清：模型评估 - Tran 和 Val 损失 [关闭]</title>
      <link>https://stackoverflow.com/questions/78288712/clarification-model-evaluation-tran-and-val-loss</link>
      <description><![CDATA[我不确定我的模型是否表现良好。我的理解是，如果我的模型在训练和验证损失之间进行调整，也表明没有欠拟合或过拟合。然而，我的教授不同意，并说我的模型没有经过充分的训练，这表明图表本身是不正确的。
我不确定问题出在哪里或者到底出了什么问题。有人可以提供一些指导或帮助吗？
我的模型的训练和 val 损失图
我通过在嵌入层中添加一些噪声来训练模型。
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/78288712/clarification-model-evaluation-tran-and-val-loss</guid>
      <pubDate>Sun, 07 Apr 2024 17:35:17 GMT</pubDate>
    </item>
    <item>
      <title>py_call_impl 中的错误（可调用，call_args$未命名，call_args$命名）</title>
      <link>https://stackoverflow.com/questions/78283892/error-in-py-call-implcallable-call-argsunnamed-call-argsnamed</link>
      <description><![CDATA[我正在尝试在 R 中构建 CNN 模型来对三种类型的图像进行分类。 R 中的库似乎创建了一个 Python 虚拟环境并调用它来执行一些张量流任务。我的 R 代码是
库（keras）
库（EBImage）
库（字符串）
库（pb应用）
# 设置图像大小
宽度 &lt;- 50
高度 &lt;- 50

extract_feature &lt;- 函数(dir_path, 宽度, 高度, labelsExist = T) {
  img_size &lt;- 宽度 * 高度
  
  # 列出路径中的图像
  images_names &lt;- list.files(dir_path)
  
  如果（标签存在）{
    # 从文件名中提取类标签
    class_labels &lt;- unique(basename(images_names) %&gt;% str_extract(&quot;^[^_]+&quot;))
    
    # 创建关键向量
    key &lt;- setNames(as.integer(seq_along(class_labels)), class_labels)
    
    # 设置标签
    y &lt;- key[basename(images_names) %&gt;% str_extract(&quot;^[^_]+&quot;)]
  }
  
  print(paste(“开始处理”, length(images_names), “图像”))
  
  # 处理图像
  feature_list &lt;- pblapply(images_names, function(imgname) {
    img &lt;- readImage(file.path(dir_path, imgname))
    img_resized &lt;- 调整大小(img, w = 宽度, h = 高度)
    greyimg &lt;- 通道(img_resized, “灰色”)
    img_matrix &lt;-grayimg@.Data
    img_vector &lt;- as.vector(t(img_matrix))
    返回（img_向量）
  })
  
  feature_matrix &lt;- do.call(rbind, feature_list)
  feature_matrix &lt;- as.data.frame(feature_matrix)
  名称（feature_matrix）&lt;-paste0（“像素”，c（1：img_size））
  
  如果（标签存在）{
    返回（列表（X =特征矩阵，y = y））
  } 别的 {
    返回（特征矩阵）
  }
}

# 准备数据
#train_data &lt;- extract_feature(“C:/Users/aryaj/OneDrive/Desktop/project 6/data/Training/train/”，宽度，高度)
#write.csv(train_data,&#39;C:/Users/aryaj/OneDrive/Desktop/project 6/data/train_data.csv&#39;)
train_data &lt;- read.csv(&#39;C:/Users/aryaj/OneDrive/Desktop/project 6/data/train_data.csv&#39;)
X_train &lt;- train_data$X
# 将标签转换为one-hot编码
unique_labels &lt;- unique(train_data$y)
y_train &lt;- sapply(train_data$y, 函数(x) {
  label_vector &lt;-rep(0, length(unique_labels))
  label_vector[which(unique_labels == x)] &lt;- 1
  返回（标签向量）
})
y_train &lt;- t(y_train)

#test_data &lt;- extract_feature(“C:/Users/aryaj/OneDrive/Desktop/project 6/data/Testing/test/”, 宽度, 高度, labelsExist = F)
#write.csv(test_data,&#39;C:/Users/aryaj/OneDrive/Desktop/project 6/data/test_data.csv&#39;)
test_data &lt;- read.csv(&#39;C:/Users/aryaj/OneDrive/Desktop/project 6/data/test_data.csv&#39;)
X_test &lt;- test_data

# 定义模型
模型 &lt;- keras_model_sequential()
型号%&gt;%
  Layer_dropout(率 = 0.25) %&gt;%
  层_展平（）％&gt;％
  Layer_dense(单位= 50，激活=“relu”)%&gt;%
  Layer_dropout(率 = 0.25) %&gt;%
  Layer_dense（单位=长度（unique_labels），激活=“softmax”）

摘要（模型）

# 编译模型
模型%&gt;%编译(
  损失=&#39;分类交叉熵&#39;，
  优化器=优化器_adam(),
  指标 = c(&#39;准确度&#39;)
）

# 训练模型
历史 &lt;- 模型 %&gt;% 拟合(
  x = X_train，
  y = y_train，
  历元 = 20,
  批量大小 = 100,
  验证数据=列表（X_测试，y_测试）
）

# 评估模型
分数 &lt;- 模型 %&gt;% 评估(X_test, y_test)
print(paste(“测试损失：”, Score[1]))
print(paste(“测试准确度：”, Score[2]))

# 绘制训练历史
情节（历史）

我收到的错误是这样的
&lt;前&gt;&lt;代码&gt;&gt;型号%&gt;%
+ Layer_dropout(率 = 0.25) %&gt;%
+ Layer_flatten() %&gt;%
+ Layer_dense(单位= 50，激活=“relu”) %&gt;%
+ Layer_dropout(率 = 0.25) %&gt;%
+ Layer_dense（单位=长度（unique_labels），激活=“softmax”）
2024-04-06 14:59:23.781982：I tensorflow/core/platform/cpu_feature_guard.cc:210] 此 TensorFlow 二进制文件经过优化，可以在性能关键型操作中使用可用的 CPU 指令。
要启用以下指令：AVX2 FMA，在其他操作中，使用适当的编译器标志重建 TensorFlow。
py_call_impl(callable, call_args$unnamed, call_args$named) 中的错误：
  ValueError：只有输入张量可以作为位置参数传递。以下参数值应作为关键字参数传递： （类型）
运行 `reticulate::py_last_error()` 了解详细信息。

我尝试按如下方式更改模型构建步骤
模型 &lt;- keras_model_sequential() %&gt;%
  Layer_dropout(率 = 0.25) %&gt;%
  层_展平（）％&gt;％
  Layer_dense(单位= 50，激活=“relu”)%&gt;%
  Layer_dropout(率 = 0.25) %&gt;%
  Layer_dense（单位=长度（unique_labels），激活=“softmax”）

但即使这个也一直给出同样的错误。]]></description>
      <guid>https://stackoverflow.com/questions/78283892/error-in-py-call-implcallable-call-argsunnamed-call-argsnamed</guid>
      <pubDate>Sat, 06 Apr 2024 09:49:16 GMT</pubDate>
    </item>
    <item>
      <title>2类的组合</title>
      <link>https://stackoverflow.com/questions/78246119/combination-of-2-classes</link>
      <description><![CDATA[# 创建单独的数据生成器用于训练和验证

train_data = data_generator.flow_from_directory(
    火车路径，
    目标大小=(img_size,img_size),
    批量大小=批量大小_训练，
    class_mode=&#39;分类&#39;,
    类=[&#39;AKIEC et BCC&#39;,&#39;VASC et DF&#39;,&#39;MEL &amp; NV&amp; BKL&#39;]
）

错误：
找到属于 3 个类别的 0 张图片。]]></description>
      <guid>https://stackoverflow.com/questions/78246119/combination-of-2-classes</guid>
      <pubDate>Fri, 29 Mar 2024 22:04:14 GMT</pubDate>
    </item>
    </channel>
</rss>