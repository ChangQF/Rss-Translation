<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 26 Nov 2024 18:24:53 GMT</lastBuildDate>
    <item>
      <title>Visual Studio Code 中未找到 Conda 内核</title>
      <link>https://stackoverflow.com/questions/79227753/conda-kernel-not-found-in-visual-studio-code</link>
      <description><![CDATA[我正在尝试重新开始机器学习。六个月前我尝试过，但我的操作系统崩溃了，我不得不完全重新安装它，这有点可惜！
从那时起，该软件可能进行了一些更新，这可能是我遇到麻烦的原因。我正在尝试使用 Visual Studio Code 选择一个内核，但我不确定我是否做得对。我遵循了 VSCode 提供的方法，但我仍然停留在内核选择上。
我很高兴地说，安装扩展和创建 Conda 环境进展顺利！但是，当我选择内核时，我收到此消息：
错误通知
我想分享我安装的扩展列表，以防有帮助：
我的扩展列表
我在网上做了很多研究，但遗憾的是，我找到的解决方案都没有用。]]></description>
      <guid>https://stackoverflow.com/questions/79227753/conda-kernel-not-found-in-visual-studio-code</guid>
      <pubDate>Tue, 26 Nov 2024 17:37:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在机器学习中有效处理不平衡数据集？</title>
      <link>https://stackoverflow.com/questions/79227104/how-can-i-handle-imbalanced-datasets-effectively-in-machine-learning</link>
      <description><![CDATA[我正在研究一个二元分类问题，数据集严重不平衡（类别分布为 90:10）。使用标准准确度指标无法提供对模型性能的有意义的见解。
我尝试过采样少数类，欠采样多数类，但这种方法要么导致过度拟合，要么导致信息丢失。我也尝试过成本敏感型学习，但结果并没有显著改善。
有哪些有效的技术或策略，如高级采样方法、集成学习或度量优化，可以帮助解决这个问题？您还可以推荐一些专门用于解决不平衡问题的 Python 库吗？
我正在寻找处理不平衡数据集的高级策略，包括集成方法、度量优化或预处理技术。]]></description>
      <guid>https://stackoverflow.com/questions/79227104/how-can-i-handle-imbalanced-datasets-effectively-in-machine-learning</guid>
      <pubDate>Tue, 26 Nov 2024 14:25:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用数据科学技术来识别机器学习模型中的偏见，以及这些偏见的伦理影响是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79227101/how-can-data-science-techniques-be-used-to-identify-biases-in-machine-learning-m</link>
      <description><![CDATA[数据科学方法如何发现和解决由于数据集不平衡、算法有缺陷或人类偏见而可能存在于机器学习模型中的偏见。它还深入探讨了当这些偏见影响招聘、医疗保健或刑事司法等领域的关键决策时出现的道德问题。通过解决这个问题，讨论强调了数据科学家开发公平、透明和平等的人工智能系统的责任。
一个关于数据科学在识别和解决机器学习偏见方面的作用的深思熟虑的问题。我的目的是鼓励对该领域的技术和道德层面进行批判性思考。我希望它能引发有意义的讨论或澄清数据科学的一个细微方面。]]></description>
      <guid>https://stackoverflow.com/questions/79227101/how-can-data-science-techniques-be-used-to-identify-biases-in-machine-learning-m</guid>
      <pubDate>Tue, 26 Nov 2024 14:23:54 GMT</pubDate>
    </item>
    <item>
      <title>为什么（远程） Jupyter 在 ML 训练期间很忙，但实际上却没有做任何事情？</title>
      <link>https://stackoverflow.com/questions/79226995/why-is-remote-jupyter-busy-during-ml-training-but-not-actually-doing-anything</link>
      <description><![CDATA[我正在使用 PyTorch 在自己的专用远程服务器上训练 ML 模型，使用 Jupyter 作为我的 IDE。
大约 120 个 epoch（训练大约 2 小时），Jupyter 单元停止更新输出，但状态栏仍显示内核状态为 busy，SSH 连接仍处于活动状态。
我认为训练可能仍在继续，但输出单元停止更新，因为它包含太多输出。为了验证这个假设，我昨晚让 Jupyter 运行了大约 7 个小时。当我醒来时，它已经在 123 个 epoch 时停止更新输出单元，当我终止执行并打印出当前 epoch 数时，它只达到了 126 个 epoch。
知道是什么原因造成的吗？]]></description>
      <guid>https://stackoverflow.com/questions/79226995/why-is-remote-jupyter-busy-during-ml-training-but-not-actually-doing-anything</guid>
      <pubDate>Tue, 26 Nov 2024 13:55:11 GMT</pubDate>
    </item>
    <item>
      <title>将请求上下文从 FastAPI 传递到用于 OpenAI 集成的 Microsoft Semantic Kernel 插件</title>
      <link>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</link>
      <description><![CDATA[我正在 FastAPI 应用程序中将 Microsoft Semantic Kernel 与 OpenAI 集成。我有一个聊天/端点，我从请求中收到一个 session_id，我需要将此 session_id 与 openai_client 一起传递给插件。但是，我不确定如何在内核的执行过程中将 FastAPI 请求中的 session_id 正确传递给插件。
以下是设置内核和插件的相关代码：
# 内核和服务设置
kernel = Kernel()

execution_settings = AzureChatPromptExecutionSettings(tool_choice=&quot;auto&quot;)
execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={})

openai_client = OpenAI(api_key=api_key)
chat_completion_service = OpenAIChatCompletion(
ai_model_id=model_id, 
api_key=api_key, 
service_id=service_id 
)

# 添加服务和插件
kernel.add_service(chat_completion_service)
kernel.add_plugin(MovesPlugin(openai_client), plugin_name=&#39;MovesPlugin&#39;)

在我的 FastAPI 端点内，我想在调用内核进行聊天响应时将 session_id 传递给插件：
# 在 FastAPI 端点内
@app.post(&quot;/chat/&quot;)
async def chat(request: Request):
session_id = await request.json().get(&#39;session_id&#39;)

# 获取聊天完成服务
_chat_completion_service = kernel.get_service(type=ChatCompletionClientBase)

# 获取聊天完成响应
response = await _chat_completion_service.get_chat_message_content(
chat_history=chat_history,
kernel=kernel,
settings=execution_settings
)

return响应

如何将请求上下文 (session_id) 从 FastAPI 请求传递到 MovesPlugin，并确保它与语义内核执行中的 openai_client 一起正确使用？
如能得到任何指导或建议，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</guid>
      <pubDate>Tue, 26 Nov 2024 12:51:34 GMT</pubDate>
    </item>
    <item>
      <title>构建 Python ML 项目目录的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/79226565/what-is-the-best-way-to-structure-the-directories-a-python-ml-project</link>
      <description><![CDATA[我想知道为 ML Python 项目构建目录的适当方法；以使项目易于阅读、维护并使包的导入正常工作。（即，从命令行和 IDE 运行时导入都能顺利运行，而无需在 PYTHONPATH 或类似的东西中硬编码更改）
我对此很陌生，我已经搜索过但还没有找到标准方法。
例如，我将以这种方式构建一个项目：（注意：这只是一个示例，用于提供想法，文件夹可能包含更多 .py 文件、具有类定义的文件......）
+---configuration_files
+---data
+---models
+---README.md
+---report.md
+---requirements.txt
+---shell_scripts
\---src
|数据清理.py
| 数据预处理.py
| 预处理数据分析.py
| 原始数据分析.py
| 测试.py
| 训练.py
|
\---模块
| __init__.py
|
+---数据分析
| 数据分析实用程序.py
| __init__.py
|
+---数据处理
| 数据加载器.py
| __init__.py
|
+---数据预处理
| 数据预处理实用程序.py
| __init__.py
|
+---通用实用程序
| 通用实用程序.py
| __init__.py
|
+---测试
| 测试实用程序.py
| __init__.py
| \---train
trainer.py.py
train_functions_1.py
train_functions_2.py
__init__.py

您对此有何看法？
例如，将必须直接运行的脚本放在 src 目录中，使导入更容易；
但是我觉得将它们放在 data_preprocessing、data_analysis、train、test 等文件夹中，每个文件夹都有各自的包和模块，这样会更简洁。但是，这会在从应该由所有人共享的 general_utils 包导入时引入问题。
此外，src、modules、utils 等使用名称的约定是什么？我应该有一个 main.py  脚本吗？ （即使我有不同的阶段）
一般来说，在行业中，构建此类项目的最有效和最广泛使用的方式是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79226565/what-is-the-best-way-to-structure-the-directories-a-python-ml-project</guid>
      <pubDate>Tue, 26 Nov 2024 11:44:32 GMT</pubDate>
    </item>
    <item>
      <title>对患者同时发生的医学症状进行聚类[关闭]</title>
      <link>https://stackoverflow.com/questions/79226363/clustering-co-occuring-medical-symptoms-for-patients</link>
      <description><![CDATA[我获得了一个数据集，其中跟踪了不同患者的 48 种不同医学症状。这些症状每天都会记录下来，每天的严重程度评级为 0-4。
我需要将一年中不同时间点同时出现的症状聚集在一起。
哪个无监督模型能够按时间拆分数据并提供输出来解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79226363/clustering-co-occuring-medical-symptoms-for-patients</guid>
      <pubDate>Tue, 26 Nov 2024 10:50:08 GMT</pubDate>
    </item>
    <item>
      <title>何时使用 mlflow.set_tag() 与 mlflow.log_params()？</title>
      <link>https://stackoverflow.com/questions/72473826/when-to-use-mlflow-set-tag-vs-mlflow-log-params</link>
      <description><![CDATA[我对 mlflow.set_tag() 与 mlflow.log_params() 的用例感到困惑，因为两者都采用键和值对。目前，我使用 mlflow.set_tag() 来设置数据版本、代码版本等的标签，使用 mlflow.log_params() 来设置模型训练参数，如损失、准确率、优化器等。]]></description>
      <guid>https://stackoverflow.com/questions/72473826/when-to-use-mlflow-set-tag-vs-mlflow-log-params</guid>
      <pubDate>Thu, 02 Jun 2022 09:23:32 GMT</pubDate>
    </item>
    <item>
      <title>什么是传统的加性模型以及这些模型与机器学习模型之间的区别？</title>
      <link>https://stackoverflow.com/questions/71879305/what-is-traditional-additive-models-and-the-differences-between-these-models-and</link>
      <description><![CDATA[我读到一篇SCI(PMID: 32437368)论文中说

目前的中风风险评估工具假设风险因素的影响是线性和累积的。然而，传统的加性模型很难揭示新的风险因素及其对中风发病率的影响。

什么是传统的加性模型，这些模型与机器学习模型有什么区别？]]></description>
      <guid>https://stackoverflow.com/questions/71879305/what-is-traditional-additive-models-and-the-differences-between-these-models-and</guid>
      <pubDate>Fri, 15 Apr 2022 02:02:44 GMT</pubDate>
    </item>
    <item>
      <title>将机器学习或深度学习模型部署到生产中的最佳实践是什么？</title>
      <link>https://stackoverflow.com/questions/53588943/what-are-the-best-practices-for-deploying-machine-learning-or-deep-learning-mode</link>
      <description><![CDATA[请指导我一些用于将机器学习和深度学习模型部署到生产环境的最佳工具，参考链接到云来部署模型。 ]]></description>
      <guid>https://stackoverflow.com/questions/53588943/what-are-the-best-practices-for-deploying-machine-learning-or-deep-learning-mode</guid>
      <pubDate>Mon, 03 Dec 2018 07:03:37 GMT</pubDate>
    </item>
    <item>
      <title>xgboost.plot_tree：二元特征解释</title>
      <link>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</link>
      <description><![CDATA[我构建了一个 XGBoost 模型，并试图检查各个估计量。作为参考，这是一个二元分类任务，具有离散和连续输入特征。输入特征矩阵是 scipy.sparse.csr_matrix。
然而，当我去检查一个单独的估计量时，我发现很难解释二元输入特征，例如下面的 f60150。最底部图表中的实值 f60150 很容易解释 - 其标准在该特征的预期范围内。但是，对二元特征 &lt;X&gt; &lt; -9.53674e-07 进行的比较没有意义。这些特征中的每一个要么是 1，要么是 0。-9.53674e-07 是一个非常小的负数，我想这只是 XGBoost 或其底层绘图库中的一些浮点特性，但当特征始终为正时使用这种比较是没有意义的。有人能帮我理解哪个方向（即 是、缺失 与 否 对应这些二进制特征节点的哪一侧为真/假吗？
这是一个可重现的示例：
import numpy as np
import scipy.sparse
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from xgboost import plot_tree, XGBClassifier
import matplotlib.pyplot as plt

def booleanize_csr_matrix(mat):
&#39;&#39;&#39; 将具有正整数元素的稀疏矩阵转换为 1 &#39;&#39;&#39;
nnz_inds = mat.nonzero()
keep = np.where(mat.data &gt; 0)[0]
n_keep = len(keep)
result = scipy.sparse.csr_matrix(
(np.ones(n_keep), (nnz_inds[0][keep], nnz_inds[1][keep])),
shape=mat.shape
)
返回结果

### 设置数据集
res = fetch_20newsgroups()

text = res.data
outcome = res.target

### 使用 CountVectorizer 的默认参数创建初始计数矩阵
vec = CountVectorizer()
X = vec.fit_transform(text)

# 是否“布尔化”输入矩阵
booleanize = True

# 是否在“布尔化”之后将数据类型转换为与 `vec.fit_transform(text)` 返回的内容相匹配
to_int = True

如果 booleanize 和 to_int:
X = booleanize_csr_matrix(X)
X = X.astype(np.int64)

# 使其成为二元分类问题
y = np.where(outcome == 1, 1, 0)

# 随机状态确保我们能够一致地比较树及其特征
model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

将 booleanize 和 to_int 设置为 True 并运行上述程序，将生成以下图表：

将 booleanize 和 to_int 设置为 False 并运行上述程序，将生成以下图表：

哎呀，即使我做了一个非常简单的例子，我也会得到“正确”的结果，无论 X 或 y 是整数还是浮点类型。
X = np.matrix(
[
[1,0],
[1,0],
[0,1],
[0,1],
[1,1],
[1,0],
[0,0],
[0,0],
[1,1],
[0,1]
]
)

y = np.array([1,0,0,0,1,1,1,0,1,1])

model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</guid>
      <pubDate>Thu, 13 Sep 2018 13:06:06 GMT</pubDate>
    </item>
    <item>
      <title>统计学、数据科学或机器学习中 SPATIAL 和 TEMPORAL 术语的定义是什么</title>
      <link>https://stackoverflow.com/questions/51410033/what-is-the-definition-of-the-terms-spatial-and-temporal-in-terms-of-statistics</link>
      <description><![CDATA[空间和时间的确切定义是什么？我在很多地方都看到人们使用这两个术语，例如空间向量、时间向量、时间因子、空间位置。
我在 StackOverflow 中搜索，找到了这个 - 图像处理中的空间和时间特征有什么区别？ 
到目前为止，我所理解的是，术语“空间”与空间有关，术语“时间”与时间有关。不过，这对我来说还是很抽象的。同样，我也不确定这两个术语的用途。所以，和上面链接中问的人一样，我想问同样的问题 - 这两个术语是什么意思，我们为什么要关心这两个术语？]]></description>
      <guid>https://stackoverflow.com/questions/51410033/what-is-the-definition-of-the-terms-spatial-and-temporal-in-terms-of-statistics</guid>
      <pubDate>Wed, 18 Jul 2018 19:52:53 GMT</pubDate>
    </item>
    <item>
      <title>特征工程和呈现ML模型的常用技术有哪些？</title>
      <link>https://stackoverflow.com/questions/46841795/what-are-the-common-techniques-for-feature-engineering-and-presenting-the-ml-mod</link>
      <description><![CDATA[我正在做一个 ML 语言识别项目（Python），需要具有高维特征输入的多类分类模型。
目前，我能做的就是通过反复试验来提高准确性。盲目地结合可用的特征提取算法和可用的 ML 模型，看看我是否会走运。
我在问是否有一个普遍接受的工作流程可以系统地找到 ML 解决方案。
这个想法可能很幼稚，但我在想我是否可以以某种方式可视化这些高维数据和我的模型的决策边界。希望这种可视化可以帮助我进行一些调整。在 MATLAB 中，经过训练后，我可以从所有特征中选择任意两个特征，MATLAB 将相应地给出决策边界。我可以用 Python 做到这一点吗？
此外，我正在寻找一些可以在演示中用来介绍我的模型和特征的图表类型。该领域最常用的图表是什么？]]></description>
      <guid>https://stackoverflow.com/questions/46841795/what-are-the-common-techniques-for-feature-engineering-and-presenting-the-ml-mod</guid>
      <pubDate>Fri, 20 Oct 2017 03:02:39 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 - 图像的特征设计</title>
      <link>https://stackoverflow.com/questions/37236590/machine-learning-features-design-for-images</link>
      <description><![CDATA[我有一个机器学习项目，其中我必须开发一个 QR 码定位程序，以便可以检测和读取任何旋转角度的 QR 码。开发将使用 Python 完成。
计划是收集不同角度和不同背景的 QR 码的各种图像。由此，我想创建一个数据集，用于使用神经网络进行训练，然后进行测试。
我遇到的问题是，我似乎无法找出数据集的正确特征设计，以及如何从图像中识别 QR 码以进行特征处理。我会使用地面实况图像来隔离 QR 码或边缘幅度图吗？图像的特征设计似乎让我感到困惑。]]></description>
      <guid>https://stackoverflow.com/questions/37236590/machine-learning-features-design-for-images</guid>
      <pubDate>Sun, 15 May 2016 09:26:11 GMT</pubDate>
    </item>
    <item>
      <title>R 中的 xgboost：xgb.cv 如何将最佳参数传递到 xgb.train</title>
      <link>https://stackoverflow.com/questions/35050846/xgboost-in-r-how-does-xgb-cv-pass-the-optimal-parameters-into-xgb-train</link>
      <description><![CDATA[我一直在探索 R 中的 xgboost 包，并浏览了多个演示和教程，但这仍然让我感到困惑：在使用 xgb.cv 进行交叉验证后，最佳参数如何传递给 xgb.train？还是应该根据xgb.cv的输出来计算理想的参数（例如nround，max.depth）？
param &lt;- list(&quot;objective&quot; = &quot;multi:softprob&quot;,
&quot;eval_metric&quot; = &quot;mlogloss&quot;,
&quot;num_class&quot; = 12)
cv.nround &lt;- 11
cv.nfold &lt;- 5
mdcv &lt;-xgb.cv(data=dtrain,params = param,nthread=6,nfold = cv.nfold,nrounds = cv.nround,verbose = T)

md &lt;-xgb.train(data=dtrain,params = param,nround = 80,watchlist = list(train=dtrain,test=dtest),nthread=6)
]]></description>
      <guid>https://stackoverflow.com/questions/35050846/xgboost-in-r-how-does-xgb-cv-pass-the-optimal-parameters-into-xgb-train</guid>
      <pubDate>Thu, 28 Jan 2016 00:31:41 GMT</pubDate>
    </item>
    </channel>
</rss>