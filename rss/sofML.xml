<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 10 Sep 2024 01:12:25 GMT</lastBuildDate>
    <item>
      <title>跨多个超市类别树的类别映射</title>
      <link>https://stackoverflow.com/questions/78967341/category-mapping-across-multiple-supermarket-category-trees</link>
      <description><![CDATA[我有来自多家超市的数据，包括其产品数据库和按产品描述分类的销售记录。我的数据被组织成一个自定义树结构，有 700 个叶节点和 7 个级别，所有产品都位于这些叶子中。我合作的一些超市有自己的分类，有些多达 2,000 个叶子。
我的目标是将我的类别无缝映射到超市的类别。由于他们想要按照他们的分类标准而不是我的分类标准来获取数据，因此手动映射 40 多家超市是不可行的。
最初，我考虑为每个超市训练一个单独的模型来预测我数据库中的所有描述并重新分类，但管理 40 多个模型并不实​​际。另一种策略是使用来自 neuromind/bert-base-portuguese-cased 模型的嵌入（因为数据是葡萄牙语）。我连接了树的各个层级（例如，食物 - 冷食 - 奶酪 - 穆扎雷拉奶酪），并使用 sklearn.metrics.pairwise 中的余弦相似度来查找超市结构中最相似的类别。然而，结果令人失望，即使有明显相似的类别，类别匹配也经常不准确。
我正在考虑大型语言模型方法（可能使用 LangChain 之类的方法）是否可以改进嵌入过程，或者是否有更好的方法。
是否有人解决过类似的问题，或者您对更有效的方法有什么建议？
def get_embedding(text):
inputs = tokenizer(text, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=512)
with torch.no_grad():
outputs = model(**inputs)
return outputs.last_hidden_​​state.mean(dim=1).squeeze().numpy()
]]></description>
      <guid>https://stackoverflow.com/questions/78967341/category-mapping-across-multiple-supermarket-category-trees</guid>
      <pubDate>Mon, 09 Sep 2024 22:14:57 GMT</pubDate>
    </item>
    <item>
      <title>神经元组织学图像分割的最佳方法</title>
      <link>https://stackoverflow.com/questions/78966320/optimal-methods-for-segmenting-histology-images-of-neurons</link>
      <description><![CDATA[我正在研究分割用 H&amp;E 染色的神经元组织学图像。
我很好奇分割这些图像的最有效方法。
我希望从图像中准确地分割出神经元。
我以前使用过细胞姿势分割，但它似乎对我的数据不太适用。
]]></description>
      <guid>https://stackoverflow.com/questions/78966320/optimal-methods-for-segmenting-histology-images-of-neurons</guid>
      <pubDate>Mon, 09 Sep 2024 16:00:24 GMT</pubDate>
    </item>
    <item>
      <title>为自定义 MLP Keras 模型实现域自适应网络 (DAN)</title>
      <link>https://stackoverflow.com/questions/78965799/implementing-domain-adaptation-network-dan-for-a-custom-mlp-keras-model</link>
      <description><![CDATA[我正在尝试使用 Keras 为 MLP 模型实现 DAN（域自适应网络）。
def build_network():
source_input = layer.Input(shape=(config_dict[&#39;patch_size&#39;], config_dict[&#39;patch_size&#39;], config_dict[&#39;patch_size&#39;], 1), name=&#39;input_1&#39;)
target_input = layer.Input(shape=(config_dict[&#39;patch_size&#39;], config_dict[&#39;patch_size&#39;], 1), name=&#39;input_2&#39;)
source_x = layer.Flatten()(source_input)
target_x = layer.Flatten()(target_input)
source_y1 = density_block(num_nodes=128, l_name=&quot;fc1&quot;)(source_x)
source_y2 = 密集块（num_nodes=128，l_name=“fc2”）（source_y1）
source_y3 = 密集块（num_nodes=128，l_name=“fc3”）（source_y2）
source_y4 = 密集块（num_nodes=128，l_name=“fc4”）（source_y3）
y_source = 层。密集（units=config_dict[&#39;output_dim&#39;]，activation=“tanh”，name=“fc5”）（source_y4）
target_y1 = 密集块（num_nodes=128，l_name=“fc6”）（target_x）
target_y2 = 密集块（num_nodes=128，l_name=“fc7”）（target_y1）
target_y3 = density_block(num_nodes=128, l_name=&quot;fc8&quot;)(target_y2)
target_y4 = density_block(num_nodes=128, l_name=&quot;fc9&quot;)(target_y3)
y_target = layer.Dense(units=config_dict[&#39;output_dim&#39;],activation=&quot;tanh&quot;, name=&quot;fc10&quot;)(target_y4)
model = Model([source_input, target_input], y_source)

return model

model = build_network()
model.compile(optimizer=&#39;adam&#39;, loss=tf.keras.losses.huber, metrics[&#39;mean_absolute_error&#39;])

然后我从预先训练的模型中加载一些权重。
print(&#39;-------------加载model-----------------&#39;)
m = Custommetrics()
mlp = models.load_model(os.path.join(config_dict[&#39;checkpoint_dir&#39;],
weights_file_name), custom_objects={&quot;r2_metric&quot;: m.r2_metric, &quot;geodesic_metric&quot;: m.geodesic_metric})
for layer1 in mlp.layers[1:]:
for layer2 in model.layers[1:]:
if layer1.name == layer2.name:
layer2.set_weights(layer1.get_weights())
layer2.trainable = False

最后，我训练模型：
source_x = np.load(os.path.join(source_dir, &#39;x_train.npy&#39;))
target_x = np.load(os.path.join(target_dir, &#39;x_train.npy&#39;))
source_y_train = np.load(os.path.join(target_dir, &#39;y_train.npy&#39;))
history = model.fit([source_x, target_x],
source_y_train,
batch_size=32, epochs=10,
validation_data=([source_x, target_x], source_y_train),
callbacks=[cp_callback])

当我打印 model.summary() 时，我得到以下内容：
模型：“model”
层（类型）输出形状参数 # 连接到
input_1（输入层）[（无，40，40，40，1）] 0 [] 

flatten（Flatten）（无，64000）0 [&#39;input_1[0][0]&#39;] 

fc1（密集）（无，128）8192128 [&#39;flatten[0][0]&#39;] 

fc2（密集）（无，128）16512 [&#39;fc1[0][0]&#39;] 

fc3（密集）（无，128）16512 [&#39;fc2[0][0]&#39;] 

fc4（密集）（无，128）16512 [&#39;fc3[0][0]&#39;]

input_2 (InputLayer) [(None, 40, 40, 40, 1)] 0 [] 

fc5 (Dense) (None, 6) 774 [&#39;fc4[0][0]&#39;] 

总参数：8242438 (31.44 MB)
可训练参数：0 (0.00 字节)
不可训练参数：8242438 (31.44 MB)

已成功训练。但我有两个问题：

为什么摘要中没有包含目标分支 (fc6-10)？
我应该如何逐层添加 MMD（即 fc1 和 fc6、fc2 和 fc7、fc3 和 fc8、fc4 和 fc9）？
]]></description>
      <guid>https://stackoverflow.com/questions/78965799/implementing-domain-adaptation-network-dan-for-a-custom-mlp-keras-model</guid>
      <pubDate>Mon, 09 Sep 2024 13:49:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 进行自然语言处理</title>
      <link>https://stackoverflow.com/questions/78965606/natural-language-processing-using-r</link>
      <description><![CDATA[我对自然语言处理很感兴趣。我有一个路线图，比如获取韩语（任何语言，英语除外）的 PDF，然后在应用一些操作（例如：数据导入、清理、标记化、删除停用词）后，我想将该语言翻译成英语。
我正在使用 R。您能建议我路线图需要进行哪些更改以及哪些库很重要吗？]]></description>
      <guid>https://stackoverflow.com/questions/78965606/natural-language-processing-using-r</guid>
      <pubDate>Mon, 09 Sep 2024 13:03:37 GMT</pubDate>
    </item>
    <item>
      <title>将 PyTorch 模型导出到 ONNX</title>
      <link>https://stackoverflow.com/questions/78964817/export-pytorch-model-to-onnx</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78964817/export-pytorch-model-to-onnx</guid>
      <pubDate>Mon, 09 Sep 2024 09:33:30 GMT</pubDate>
    </item>
    <item>
      <title>具有主导变量或仅 1 个变量的 ML 模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78964182/ml-model-with-a-dominant-or-just-1-variable</link>
      <description><![CDATA[您能否解释使用具有主导变量的机器学习模型（这些主导变量可以解释目标中的大部分方差）或仅使用一个变量构建的模型的效果？具体来说，我有兴趣了解与这些方法相关的潜在缺点、偏见或局限性。]]></description>
      <guid>https://stackoverflow.com/questions/78964182/ml-model-with-a-dominant-or-just-1-variable</guid>
      <pubDate>Mon, 09 Sep 2024 06:46:08 GMT</pubDate>
    </item>
    <item>
      <title>为什么 nn.Linear(in_features, out_features) 在 PyTorch 中使用形状为 (out_features, in_features) 的权重矩阵？</title>
      <link>https://stackoverflow.com/questions/78963755/why-does-nn-linearin-features-out-features-use-a-weight-matrix-of-shape-out</link>
      <description><![CDATA[我试图理解为什么 PyTorch 的 nn.Linear(in_features, out_features) 层的权重矩阵具有形状 (out_features, in_features) 而不是 (in_features, out_features)。
从基本矩阵乘法的角度来看，具有形状 (in_features, out_features) 似乎可以消除在乘法过程中转置权重矩阵的需要。例如，对于形状为 (batch_size, in_features) 的输入张量 x，与形状为 (in_features, out_features) 的权重矩阵相乘将直接产生形状为 (batch_size, out_features) 的输出，而无需转置操作。
但是，PyTorch 将权重矩阵定义为 (out_features, in_features)，这意味着它在前向传递过程中会被转置。这种设计有什么好处？它如何与线性代数和神经网络实现的更广泛原则保持一致？这种选择背后是否有任何效率或一致性考虑使其更可取？]]></description>
      <guid>https://stackoverflow.com/questions/78963755/why-does-nn-linearin-features-out-features-use-a-weight-matrix-of-shape-out</guid>
      <pubDate>Mon, 09 Sep 2024 03:08:49 GMT</pubDate>
    </item>
    <item>
      <title>在函数中创建 VLLM 对象时会导致内存错误，即使明确清除 GPU 缓存也是如此，只有共享引用才能使代码不会崩溃</title>
      <link>https://stackoverflow.com/questions/78959131/vllm-objects-cause-memory-errors-when-created-in-a-function-even-when-explicitly</link>
      <description><![CDATA[我在 Python 中使用 VLLM 库时遇到了问题。具体来说，当我在函数内部创建 VLLM 模型对象时，我遇到了内存问题，并且无法有效清除 GPU 内存，即使在删除对象并使用 torch.cuda.empty_cache() 之后也是如此。
当我尝试在函数内部实例化 LLM 对象时会出现问题，但如果我在父进程或全局范围内实例化该对象，则不会发生这种情况。这表明 VLLM 在函数中创建和管理对象时存在问题，从而导致内存保留和 GPU 耗尽。
以下是代码的简化版本：
import torch
import gc
from vllm import LLM

def run_vllm_eval(model_name, samples_params, path_2_eval_dataset):
# 在函数中实例化 LLM
llm = LLM(model=model_name, dtype=torch.float16, trust_remote_code=True)

# 在此处运行一些 VLLM 推理或评估（简化）
result = llm.generate([path_2_eval_dataset], samples_params)

# 推理后清理
del llm
gc.collect()
torch.cuda.empty_cache()

# 在此之后，GPU 内存不会被清除正确并导致 OOM 错误
run_vllm_eval()
run_vllm_eval()
run_vllm_eval()

但是
llm = run_vllm_eval2()
llm = run_vllm_eval2(llm)
llm = run_vllm_eval2(llm)

有效。
即使明确删除 LLM 对象并清除缓存后，GPU 内存仍未正确释放，导致在尝试加载或运行同一脚本中的另一个模型时出现内存不足 (OOM) 错误。
我尝试过的方法：

使用 del 删除 LLM 对象。
运行 gc.collect() 以触发 Python 的垃圾集合。
使用 torch.cuda.empty_cache() 清除 CUDA 内存。
确保父进程中没有实例化 VLLM 对象。

当在函数内创建 LLM 对象时，这些似乎都无法解决问题。
问题：

在函数内创建 VLLM 对象时，有人遇到过类似的内存问题吗？
是否有推荐的方法来管理或清除函数中的 VLLM 对象以防止 GPU 内存保留？
在这种情况下，是否存在与标准 Hugging Face 或 PyTorch 模型不同的特定 VLLM 处理技术？
]]></description>
      <guid>https://stackoverflow.com/questions/78959131/vllm-objects-cause-memory-errors-when-created-in-a-function-even-when-explicitly</guid>
      <pubDate>Sat, 07 Sep 2024 00:58:59 GMT</pubDate>
    </item>
    <item>
      <title>自定义模型聚合器 TensorFlow Federated</title>
      <link>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</link>
      <description><![CDATA[我正在尝试使用 TensorFlow Federated，使用 FedAvg 算法模拟训练过程。
def model_fn():
# 包装 Keras 模型以用于 TensorFlow Federated
keras_model = get_uncompiled_model()

# 对于联合过程，模型必须是未编译的
return tff.learning.models. functional_model_from_keras(
keras_model,
loss_fn=tf.keras.losses.BinaryCrossentropy(),
input_spec=(
tf.TensorSpec(shape=[None, X_train.shape[1]], dtype=tf.float32),
tf.TensorSpec(shape=[None], dtype=tf.int32)
),
metrics_constructor=collections.OrderedDict(
accuracy=tf.keras.metrics.BinaryAccuracy,
precision=tf.keras.metrics.Precision,
recall=tf.keras.metrics.Recall,
false_positives=tf.keras.metrics.FalsePositives,
false_negatives=tf.keras.metrics.FalseNegatives,
true_positives=tf.keras.metrics.TruePositives,
true_negatives=tf.keras.metrics.TrueNegatives
)
)

trainer = tff.learning.algorithms.build_weighted_fed_avg(
model_fn= model_fn(),
client_optimizer_fn=client_optimizer,
server_optimizer_fn=server_optimizer
)

我想使用自定义权重来聚合客户端的更新，而不是使用它们的样本。我知道 tff.learning.algorithms.build_weighted_fed_avg() 有一个名为 client_weighting 的参数，但唯一接受的值来自类 tff.learning.ClientWeighting，它是一个枚举。
因此，唯一的方法似乎是编写自定义 WeightedAggregator。我尝试按照本教程中的说明编写无加权聚合器，但我无法将其转换为加权聚合器。
这是我尝试做的：
class CustomWeightedAggregator(tff.aggregators.WeightedAggregationFactory):
def __init__(self):
pass

def create(self, value_type, weight_type):

@tff.federated_computation
def initialize(self):
return tff.federated_value(0.0, tff.SERVER)

@tff.federated_computation(
lambda self: self.initialize.type_signature.result,
tff.FederatedType(tff.types.to_type(np.float32), tff.CLIENTS),
tff.FederatedType(tff.types.to_type(np.float32), tff.CLIENTS)
)
def next(self, state, value, weight):
aggregate_value = tff.federated_map(custom_weighted_aggregate, (value, weight))
return state,aggregate_value

@property
def is_weighted(self):
return True

return tff.templates.AggregationProcess(self.initialize, self.next)

我必须更改什么才能使其工作并使用自定义权重？]]></description>
      <guid>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</guid>
      <pubDate>Mon, 05 Aug 2024 16:06:48 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络（CNN）在决策中使用黑色背景——LIME [关闭]</title>
      <link>https://stackoverflow.com/questions/78640044/convolutional-neural-network-cnn-using-black-background-in-decision-making-l</link>
      <description><![CDATA[我的二元分类卷积神经网络在验证数据上具有非常高的准确率 (&gt;96%)，在测试数据集上的表现也同样出色。然而，当我使用 LIME 可视化图像中对其决策很重要的部分时，它往往会突出显示背景。所以我的问题是：
为什么会这样，以前有人见过吗？
当它在做决定时实际上是看着黑色面具时，它是如何达到 96% 的准确率的？
我在图像上应用黑色面具的原因是，我得到的整个数据集具有完全相同的背景，即白色滚轮，并且正如您从我上传的其中一张图片中看到的那样，该模型在决策过程中严重依赖滚轮，因此我将背景预处理为完全黑色 (0, 0, 0)RGB 像素，但现在模型似乎以某种方式使用了它。
我只是被难住了，非常感谢任何帮助！
模型架构
滚轮问题示例
我尝试了各种架构，其中一些使用 keras 层构建，甚至尝试了预训练的 ResNet50。我还改变了大多数重要的超参数，但行为仍然存在。如果有帮助，我可以提供任何细节。
]]></description>
      <guid>https://stackoverflow.com/questions/78640044/convolutional-neural-network-cnn-using-black-background-in-decision-making-l</guid>
      <pubDate>Wed, 19 Jun 2024 00:05:44 GMT</pubDate>
    </item>
    <item>
      <title>准确度和验证预训练模型的准确度没有改变[关闭]</title>
      <link>https://stackoverflow.com/questions/73032032/accuracy-and-validation-accuracy-of-pretrained-model-is-not-changed</link>
      <description><![CDATA[我有 600 000 张图像，我想使用 keras 对它们进行分类。我只是在灰度图像上尝试预训练模型。我正在尝试使用预训练模型的模型架构，如 resnet50、inceptionv3 等。但模型的准确度和验证准确度没有改变，停留在 67%。我尝试改变网络，应用更多 epoch，也改变预训练模型，但我总是得到相同的结果，比如 67% 的准确度和验证准确度。我不明白为什么我会得到相同的结果。请推荐一些关于如何解决这个问题的想法。这是我的代码。在此 steps_per_epochs = no。图像/批次大小，批次大小为 128。训练数据集中的图像数量为 479369，验证数据集中的图像数量为 136962。这是代码的输出。]]></description>
      <guid>https://stackoverflow.com/questions/73032032/accuracy-and-validation-accuracy-of-pretrained-model-is-not-changed</guid>
      <pubDate>Tue, 19 Jul 2022 06:22:43 GMT</pubDate>
    </item>
    <item>
      <title>如何暂时禁用 MLFlow？</title>
      <link>https://stackoverflow.com/questions/61088651/how-to-disable-mlflow-temporarily</link>
      <description><![CDATA[是否可以暂时禁用 MLFlow 以调试代码或添加新功能？如果不禁用，它会保存大量实际上无用的执行或未完成的执行。
或者最好的策略是使用不调用 mlflow.start_run() 的类似代码？]]></description>
      <guid>https://stackoverflow.com/questions/61088651/how-to-disable-mlflow-temporarily</guid>
      <pubDate>Tue, 07 Apr 2020 20:14:15 GMT</pubDate>
    </item>
    <item>
      <title>我的卷积神经网络过度拟合</title>
      <link>https://stackoverflow.com/questions/59998763/my-convolutional-neural-network-is-overfitting</link>
      <description><![CDATA[我构建了一个简单的卷积神经网络，用于手势图像识别，使用背景减法使手在屏幕上呈现白色形状，背景为黑色。它主要使用 keras Conv2D 构建。我的数据集有 1000 张用于训练的图片和 100 张用于验证和测试的图片。奇怪的是，问题在第一个 epoch 之后立即出现，在此期间模型的损失大幅下降。它通常会在第二个 epoch 开始时从 183 这样的大数字下降到 1。数据集中的所有图片都是使用 cv2 从我自己的手上获取的，但我只用自己的手进行了测试，所以这应该不会有什么问题。如果数据集是问题所在，我尝试获取 3 个不同的数据集，一个使用 cv2 的 Canny 方法，该方法本质上是描绘手的一条线，并将图片的其余部分变为黑色，以查看是否有差异。无论如何，同样的事情继续发生。此外，我在不同的地方添加了多个 Dropout 层来观察效果，​​总是发生同样的事情，即损失急剧减少，并且显示出过度拟合的迹象。我还实现了 EarlyStopping 和多层来查看是否有帮助，但似乎总是会出现相同的结果。
model = Sequential()
model.add(Conv2D(32, (3,3),activation = &#39;relu&#39;,
input_shape = (240, 215, 1)))
model.add(MaxPooling2D((2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3,3),activation = &#39;relu&#39;))
model.add(Conv2D(64, (3,3),activation = &#39;relu&#39;))
model.add(MaxPooling2D((2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(128, (3,3),activation = &#39;relu&#39;))
model.add(MaxPooling2D((2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(256, (3,3), 激活 = &#39;relu&#39;))
model.add(MaxPooling2D((2,2)))
model.add(Dropout(0.25))
#model.add(Conv2D(256, (3,3), 激活 = &#39;relu&#39;))
#model.add(MaxPooling2D((2,2)))
#model.add(Conv2D(128, (3,3), 激活 = &#39;relu&#39;))
#model.add(MaxPooling2D((2,2)))
#model.add(Conv2D(64, (3,3), 激活 = &#39;relu&#39;))
#model.add(MaxPooling2D((2,2)))
model.add(Flatten())
model.add(Dense(150, 激活 = &#39;relu&#39;))
#model.add(Dropout(0.25))
#model.add(Dense(1000, 激活 = &#39;relu&#39;))
model.add(Dropout(0.75))
model.add(Dense(6, 激活 = &#39;softmax&#39;))
model.summary()
model.compile(optimizer = &#39;adam&#39;, loss = &#39;categorical_crossentropy&#39;,
metrics = [&#39;acc&#39;])
callbacks_list = [EarlyStopping(monitor = &#39;val_loss&#39;, waiting = 10),
ModelCheckpoint(filepath = &#39;model.h6&#39;, monitor = &#39;val_loss&#39;,
save_best_only = True),]

代码中注释的部分是我尝试实施的更改。我还对 Dropout 值和它们的位置进行了很大的改变，但没有什么重大变化。有人能提供一些建议，说明为什么我的模型会这么快过度拟合吗？]]></description>
      <guid>https://stackoverflow.com/questions/59998763/my-convolutional-neural-network-is-overfitting</guid>
      <pubDate>Fri, 31 Jan 2020 06:14:18 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能提高 CNN 的准确性？[关闭]</title>
      <link>https://stackoverflow.com/questions/57834240/how-can-i-improve-my-cnns-accuracy-evolution</link>
      <description><![CDATA[因此，我正在尝试创建一个 CNN，它可以预测胸部 x 射线图像中是否有任何“支撑设备”，但在训练我的模型时，它似乎没有学到任何东西。
我正在使用一个名为“CheXpert”的数据集，它有超过 200,000 张图像可供使用。经过一些“清理”后，最终数据集最终有 100,000 张图像。
就模型而言，我导入了 vgg16 预训练模型的卷积基，并自行添加了 2 个完全连接的层。然后，我冻结了所有卷积基，只训练完全连接的层。这是代码：
from keras.layers import GlobalAveragePooling2D
from keras.models import Model

pretrained_model = VGG16(weights=&#39;imagenet&#39;, include_top=False)

pretrained_model.summary()

for layer in pretrained_model.layers:
layer.trainable = False

x = pretrained_model.output
x = GlobalAveragePooling2D()(x)

dropout = Dropout(0.25)

# 让我们添加一个全连接层
x = Dense(1024,activation=&#39;relu&#39;)(x)
x = dropout(x)
x = Dense(1024,activation =&#39;relu&#39;)(x)
predictions = Dense(1,activation=&#39;sigmoid&#39;)(x)

final_model = Model(inputs=pretrained_model.input, output=predictions)

final_model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;rmsprop&#39;, metrics=[&#39;accuracy&#39;])

据我所知，正常行为应该是准确率从低开始，然后随着时期的推移而增长。但在这里，它只在相同的值（0.93 和 0.95）之间波动。很抱歉，我无法上传图片来向您展示图表。
总而言之，我想知道准确率的微小差异是否意味着模型没有学到任何东西。
我有一个假设：在数据集的所有 100,000 张图像中，95,000 张带有标签“1”，只有 5,000 张带有标签“0”。我认为，如果将带有“1”的图像减少并使其与带有“0”的图像相等，结果就会改变。]]></description>
      <guid>https://stackoverflow.com/questions/57834240/how-can-i-improve-my-cnns-accuracy-evolution</guid>
      <pubDate>Sat, 07 Sep 2019 13:42:46 GMT</pubDate>
    </item>
    <item>
      <title>什么是 Killed:9 以及如何在 macOS 终端中修复？</title>
      <link>https://stackoverflow.com/questions/51833310/what-is-killed9-and-how-to-fix-in-macos-terminal</link>
      <description><![CDATA[我有一段用于机器学习项目的简单 Python 代码。我有一个相对较大的自发语音数据库。我开始训练我的语音模型。由于这是一个庞大的数据库，我让它连夜工作。早上我醒来时看到终端中出现一个神秘的
Killed: 9
行。没有其他内容。没有其他错误消息或需要处理的内容。代码运行良好约 6 小时，占整个过程的 75%，所以我真的不明白哪里出了问题。
什么是 Killed:9 以及如何修复它？浪费数小时的计算时间非常令人沮丧……
如果这很重要，我正在使用 macOS Mojave 测试版。提前谢谢您！]]></description>
      <guid>https://stackoverflow.com/questions/51833310/what-is-killed9-and-how-to-fix-in-macos-terminal</guid>
      <pubDate>Tue, 14 Aug 2018 03:28:58 GMT</pubDate>
    </item>
    </channel>
</rss>