<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 24 Jul 2024 01:07:24 GMT</lastBuildDate>
    <item>
      <title>创建具有标量特征和 N 维坐标向量特征的机器学习 Numpy 数组</title>
      <link>https://stackoverflow.com/questions/78785643/create-a-machine-learning-numpy-array-with-scalar-features-and-an-n-dimensional</link>
      <description><![CDATA[我正在尝试为 ML 程序格式化我的数据。有 33,000 个事件，每个事件都有 3 个我想考虑的事情：质量、能量、坐标。
质量的形状为 (33000,)，看起来像：[188.9 189.0 125.7 ... 127.4 201.0 210.1]。
能量也是 (33000,)，看起来相同：[1 2 3 ... 8 9 10]。然后，我还有一个形状为 (33000,10) 的 10 维坐标向量
每个坐标都是一个包含 10 个坐标点的 10 维向量：
坐标数组：
 [[19.9 613.0 6.5 127.4 486.4 54.3 194.0 19.4 194.0 32.3]
[1.89 1.01 4.9 ... 2.3 2.3 2.3]
[1.2 6.1 4.0 ... 1.7 1.7 1.7]
...
]

我想将这些输入到机器学习程序中。但是，我不想创建一个将 10 维坐标压缩为一组平面浮点值的数组，如下所示：
 [188.9 1 19.9 613.0 6.5 127.4 486.4 54.3 194.0 19.4 194.0 32.3\]
[189.0 2 1.89 1.01 4.9 ... 2.3 2.3 2.3\]
...

这会丢失最后 10 个值本质上联系在一起的信息，因为它们是一个坐标。相反，我想创建一个 numpy 数组，该数组中间有一个向量
 [188.9 1 [19.9 613.0 6.5 127.4 486.4 54.3 194.0 19.4 194.0 32.3]]
[189.0 2 [1.89 1.01 4.9 ... 2.3 2.3 2.3]]
...

这样，机器学习程序就知道将坐标向量视为其自身的特征，而不是一组不同的特征。因此实际形状可能是 (33000,3) 而不是 (33000,13) 这可能吗？
我尝试过 dstack、concatenate、stack 等。所有方法都存在“轴必须完全匹配”的问题。在我的例子中，轴不匹配。一个特征的轴为 10，而其他特征要么没有轴（33000，），要么有 1 个轴（33000,1）（如果你强制它有一个轴）。我不确定我是否遗漏了某个 numpy 数组事实，或者这是否是不可能的。]]></description>
      <guid>https://stackoverflow.com/questions/78785643/create-a-machine-learning-numpy-array-with-scalar-features-and-an-n-dimensional</guid>
      <pubDate>Tue, 23 Jul 2024 21:28:52 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 RNN 不能收敛到一个简单的任务？</title>
      <link>https://stackoverflow.com/questions/78784723/why-my-rnn-does-not-converge-to-a-simple-task</link>
      <description><![CDATA[我想创建一个递归模型来解决我所知道的最简单的序列，算术级数。以 a 为基数，以 d 为步长，序列如下：
a、a+d、a+2d、a+3d、a+4d、...
为了解决这个问题，将隐藏状态表示为 h，模型必须学习一个简单的 2*2 矩阵。这实际上是设置 h1 = t0。

换句话说，你也可以这样看：

所以这个带有 2*2 全连接层的模型应该能够学习这个矩阵：
class Model(nn.Module):
def __init__(self):
super(Model, self).__init__()
self.fc1 = nn.Linear(2, 2, bias=False)

def forward(self, x):
x = self.fc1(x)
return x

但令我惊讶的是它并没有收敛！我的设置应该有问题。如果你能帮我找到它，我将不胜感激。我怀疑问题应该出在我的训练循环中。
附言：我现在故意将批处理大小设置为 1。我想稍后再处理输入数据。无论如何，模型应该可以在没有批次的情况下进行学习。
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np

class CustomDataset(Dataset):
def __init__(self, size):
self.size = size

def __len__(self):
return self.size

def __getitem__(self, index):
a0 = (np.random.rand() - 0.5) * 200
d = (np.random.rand() - 0.5) * 40
length = np.random.randint(2, MAX_Length_sequence + 1)

serial = np.arange(length) * d + a0
next_number = serial[-1] + d

return length, torch.tensor(sequence, dtype=torch.float32), torch.tensor(next_number, dtype=torch.float32)

class Model(nn.Module):
def __init__(self):
super(Model, self).__init__()
self.fc1 = nn.Linear(2, 2, bias=False)

def forward(self, x):
x = self.fc1(x)
return x

# 超参数
EPOCHS = 10
BATCH_SIZE = 1
LEARNING_RATE = 0.001
DATASET_SIZE = 10000
criterion = nn.MSELoss()

# 模型
model = Model()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)


我的训练循环：
for epoch in范围（EPOCHS）：
数据集 = CustomDataset（DATASET_SIZE）
数据加载器 = DataLoader（数据集，batch_size=BATCH_SIZE）
模型。训练（）
总损失 = 0

对于长度、序列、数据加载器中的下一个编号：
优化器。zero_grad（）
损失 = 0
h = torch.zeros（BATCH_SIZE）

对于范围（长度）中的 i：
x = torch.cat（[h，sequence[0，i].unsqueeze(0)])
y = 序列[0，i + 1] 如果 i != length - 1 else next_number[0]

输出 = 模型（x）
h，y_hat = output[0].unsqueeze(0), output[1]

损失 += 标准（y_hat，y）

损失。backward（）
优化器。step（）
总损失 += 损失。item（）

打印（f&#39;Epoch {epoch+1}，损失：{total_loss/len(dataloader)}&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78784723/why-my-rnn-does-not-converge-to-a-simple-task</guid>
      <pubDate>Tue, 23 Jul 2024 16:59:29 GMT</pubDate>
    </item>
    <item>
      <title>HPCC 系统 ECL：使用 LinearRegression 时出现访问权限不足错误</title>
      <link>https://stackoverflow.com/questions/78784047/hpcc-systems-ecl-insufficient-access-rights-error-using-linearregression</link>
      <description><![CDATA[我正在使用 HPCC Systems 和 ECL 进行机器学习项目以执行线性回归。我的数据集包含心理健康患病率数据。在尝试使用 LinearRegression.OLS 训练我的模型时，我遇到了以下错误：
错误：访问权限不足，无法使用嵌入代码 
(125, 34 - C:\Users\LENOVO\AppData\Roaming\HPCCSystems\bundles_versions\PBblas\V3_0_2\PBblas\internal\Converted.ecl)

这意味着包含在 ECL 脚本中的代码需要特殊权限或访问级别才能运行。
错误指向 HPCC Systems 安装中的特定文件，在本例中是 PBblas 库 (Converted.ecl) 的一部分。
我的代码：
IMPORT ML_Core;
IMPORT LinearRegression;
IMPORT $;

// 输入数据集的记录结构
MentalHealthRecord := RECORD
STRING Entity;
STRING Code;
INTEGER Year;
REAL8 Schizophrenia;
REAL8 Depression;
REAL8 Anxiety;
REAL8 Bipolar;
REAL8 EatingDisorders;
END;

// 根据记录输入数据集
MentalHealthDs := DATASET(&#39;~asn::testing::1-mental-illnesses-prevalence.csv&#39;,
MentalHealthRecord,
CSV(HEADING(1),
SEPARATOR(&#39;,&#39;),
TERMINATOR([&#39;\n&#39;, &#39;\r\n&#39;, &#39;\n\r&#39;])));

OUTPUT(MentalHealthDs, NAMED(&#39;InputDataset&#39;)); 

// 数据集中的记录数。训练：测试的分割比率为小数。
recordCount := COUNT(MentalHealthDs);
splitRatio := 0.8; // 80% 用于训练，20% 用于测试

// 继承包含随机数的数据集记录的记录结构
Shuffler := RECORD
MentalHealthRecord;
UNSIGNED4 rnd; // 一个随机数
END;

// 为包含随机数的数据添加一个属性
newDs := PROJECT(MentalHealthDs, TRANSFORM(Shuffler, SELF.rnd := RANDOM(), SELF := LEFT));

// 根据随机数对数据集进行排序，进行随机排序
shuffledDs := SORT(newDs, rnd);

// 拆分训练和测试数据集，同时仅获取输入记录属性
TrainDs := PROJECT(shuffledDs[1..(recordCount * splitRatio)], RECORDOF(MentalHealthDs));
TestDs := PROJECT(shuffledDs[(recordCount * splitRatio + 1)..recordCount], RECORDOF(MentalHealthDs));

OUTPUT(TrainDs, NAMED(&#39;TrainDataset&#39;));
OUTPUT(TestDs, NAMED(&#39;TestDataset&#39;));

// 将顺序 ID 附加到训练和测试数据集
ML_Core.AppendSeqID(TrainDs, id, newTrain);
ML_Core.AppendSeqID(TestDs, id, newTest);

OUTPUT(newTrain, NAMED(&#39;TrainDatasetID&#39;));
OUTPUT(newTest, NAMED(&#39;TestDatasetID&#39;));

// 将数据集转换为数字字段以进行训练
ML_Core.ToField(newTrain, TrainNF);
ML_Core.ToField(newTest, TestNF);

OUTPUT(TrainNF, NAMED(&#39;TrainNumericField&#39;));
OUTPUT(TestNF, NAMED(&#39;TestNumericField&#39;));

// 根据独立列的数量拆分转换后的数字字段数据集，以获取用于训练的 X 和 Y 
independent_cols := 4; // 精神分裂症、焦虑症、躁郁症、饮食失调症

X_train := TrainNF(number &lt; independent_cols + 1);
y_train := PROJECT(TrainNF(number = independent_cols + 1), TRANSFORM(RECORDOF(LEFT), SELF.number := 1, SELF := LEFT));

X_test := TestNF(number &lt; independent_cols + 1);
y_test := PROJECT(TestNF(number = independent_cols + 1), TRANSFORM(RECORDOF(LEFT), SELF.number := 1, SELF := LEFT));

OUTPUT(y_test, NAMED(&#39;ActualY&#39;));

// 通过拟合模型构建回归器并使用测试数据集进行预测
regressor := LinearRegression.OLS(X_train, y_train).GetModel;
predicted := LinearRegression.OLS().Predict(X_test, regressor);

OUTPUT(predicted, NAMED(&#39;PredictedY&#39;));


我已检查我的访问权限，但问题仍然存在。
为什么我会收到“访问权限不足”错误以及如何解决？
如何正确使用 LinearRegression.OLS 来训练模型？
HPCC Systems ECL 中是否有其他方法或配置可以执行线性回归而不会遇到访问权限问题？]]></description>
      <guid>https://stackoverflow.com/questions/78784047/hpcc-systems-ecl-insufficient-access-rights-error-using-linearregression</guid>
      <pubDate>Tue, 23 Jul 2024 14:29:53 GMT</pubDate>
    </item>
    <item>
      <title>pointnet++的实现</title>
      <link>https://stackoverflow.com/questions/78783587/implementation-of-pointnet</link>
      <description><![CDATA[这是我的错误。掩码张量的形状与被索引的张量的形状不匹配。具体来说，掩码张量的形状为 [8, 512, 3]，而被索引的张量的形状为 [8, 512, 16]。如何解决这个问题？
/usr/bin/python3.10 /home/aniruddha/PycharmProjects/Pointnet_Pointnet2_pytorch/centerline_pn.py 
回溯（最近一次调用最后一次）：
文件 &quot;/home/aniruddha/PycharmProjects/Pointnet_Pointnet2_pytorch/centerline_pn.py&quot;，第 265 行，位于 &lt;module&gt;
main()
文件 &quot;/home/aniruddha/PycharmProjects/Pointnet_Pointnet2_pytorch/centerline_pn.py&quot;，第 217 行，在 main
输出，_ = model(points)
文件 &quot;/home/aniruddha/.local/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;，第 1532 行，在 _wrapped_call_impl
返回 self._call_impl(*args, **kwargs)
文件 &quot;/home/aniruddha/.local/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;，第 1541 行，在 _call_impl
返回 forward_call(*args, **kwargs)
文件&quot;/home/aniruddha/PycharmProjects/Pointnet_Pointnet2_pytorch/models/pointnet2_cls_msg.py&quot;，第 32 行，在 forward 中
l1_xyz, l1_points = self.sa1(xyz, norm)
文件 &quot;/home/aniruddha/.local/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;，第 1532 行，在 _wrapped_call_impl 中
return self._call_impl(*args, **kwargs)
文件 &quot;/home/aniruddha/.local/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;，第 1541 行，在 _call_impl 中
return forward_call(*args, **kwargs)
文件&quot;/home/aniruddha/PycharmProjects/Pointnet_Pointnet2_pytorch/models/pointnet2_utils.py&quot;，第 283 行，在 forward 中
group_idx = query_ball_point(radius, K, xyz, new_xyz)
文件 &quot;/home/aniruddha/PycharmProjects/Pointnet_Pointnet2_pytorch/models/pointnet2_utils.py&quot;，第 148 行，在 query_ball_point 中
group_idx[mask] = group_first[mask]
IndexError：索引 2 处的掩码 [8, 512, 3] 的形状与索引 2 处的索引张量 [8, 512, 16] 的形状不匹配

进程以退出代码 1 结束


def query_ball_point(radius, nsample, xyz, new_xyz):
&quot;&quot;&quot;
输入：
radius：局部区域半径
nsample：局部区域最大样本数
xyz：所有点，[B, N, 3]
new_xyz：查询点，[B, S, 3]
返回：
group_idx：分组点索引，[B, S, nsample]
&quot;&quot;&quot;
device = xyz.device
B, N, C = xyz.shape
_, S, _ = new_xyz.shape
group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])
sqrdists = square_distance(new_xyz, xyz)
group_idx[sqrdists &gt; radius ** 2] = N
group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]
group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample])
mask = group_idx == N
group_idx[mask] = group_first[mask]
return group_idx

def square_distance(src, dst):
&quot;&quot;&quot;
计算每两点之间的欧几里得距离。

src^T * dst = xn * xm + yn * ym + zn * zm；
sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;
sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;
dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2
= sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst

输入：
src：源点，[B, N, C]
dst：目标点，[B, M, C]
输出：
dist：每个点的平方距离，[B, N, M]
&quot;&quot;&quot;
B, N, _ = src.shape
_, M, _ = dst.shape
dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))
dist += torch.sum(src ** 2, -1).view(B, N, 1)
dist += torch.sum(dst ** 2, -1).view(B, 1, M)
返回 dist

]]></description>
      <guid>https://stackoverflow.com/questions/78783587/implementation-of-pointnet</guid>
      <pubDate>Tue, 23 Jul 2024 12:58:50 GMT</pubDate>
    </item>
    <item>
      <title>多输出回归可根据 ROAS 和其他功能预测成本和收入</title>
      <link>https://stackoverflow.com/questions/78783100/multi-output-regression-to-predict-cost-and-revenue-from-roas-and-other-features</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78783100/multi-output-regression-to-predict-cost-and-revenue-from-roas-and-other-features</guid>
      <pubDate>Tue, 23 Jul 2024 11:10:59 GMT</pubDate>
    </item>
    <item>
      <title>如何防止人工智能模型检测特定对象</title>
      <link>https://stackoverflow.com/questions/78781272/how-to-prevent-the-ai-model-from-detecting-a-specific-object</link>
      <description><![CDATA[我正在做一个 AI 项目。有一个特定的物体我想让它不被检测到，所以正在寻找解决方案。
已经完成了两个物体的标记，并使用这两个标签训练了模型。还没有对物体进行标记，我很快就会这样做。
我认为这可能有效：

在使用我想要不被检测到的物体的标签训练预训练模型后，每当使用 AI 模型时，我都不会包含该物体的标签。

不确定这是否正确。你能对此提出任何意见吗？]]></description>
      <guid>https://stackoverflow.com/questions/78781272/how-to-prevent-the-ai-model-from-detecting-a-specific-object</guid>
      <pubDate>Tue, 23 Jul 2024 01:48:35 GMT</pubDate>
    </item>
    <item>
      <title>更改 YoloV8 分割颜色</title>
      <link>https://stackoverflow.com/questions/78776522/change-yolov8-segmentation-color</link>
      <description><![CDATA[我是 YoloV8 训练任务的新手，想了解如何更改模型执行的分割颜色。
如果有人有一些代码示例并可以分享，请分享。
任何帮助指导我的帮助都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78776522/change-yolov8-segmentation-color</guid>
      <pubDate>Sun, 21 Jul 2024 23:11:03 GMT</pubDate>
    </item>
    <item>
      <title>具有多输出回归和自定义损失函数的 LightGBM</title>
      <link>https://stackoverflow.com/questions/78310990/lightgbm-with-multi-output-regression-and-custom-loss-function</link>
      <description><![CDATA[我可以将 LightGBM 与多输出回归和自定义损失函数一起使用吗？
问题是我必须使用 LightGBM。
我知道我可以使用 sklearn 的 MultiOutputRegression 来包装 LightGBM，但这不允许我定义自定义损失函数，因为我可以使用 Keras 来做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/78310990/lightgbm-with-multi-output-regression-and-custom-loss-function</guid>
      <pubDate>Thu, 11 Apr 2024 14:15:26 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用函数转换器对我的目标列进行特征转换，但我不知道如何将其传递给函数？</title>
      <link>https://stackoverflow.com/questions/78274143/i-am-doing-feature-transformation-of-my-target-column-using-function-transformer</link>
      <description><![CDATA[该代码用于获取我的目标列，即 Time_taken(min)，其值为 (min) 36、(min) 54、(min) 65 ... 等等。所以我想创建一个新列“Time Taken”其值将为 36、54、65....并使用 Function Transformers 删除 Time_taken(min)。
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline

class TimeTakenTransformer(BaseEstimator, TransformerMixin):
def __init__(self, input_column):
self.input_column = input_column
def fit(self, X, y=None):
return self

def transform(self, X):
X = X.copy()
op = []
for i in X[self.input_column]:
a = i.split()
op.append(int(a[1]))
X[&#39;Time_taken&#39;] = op
X.drop([self.input_column], axis=1, inplace=True)
return X

Target_column = Pipeline([(&#39;替换值&#39;, TimeTakenTransformer(input_column=&quot;TARGET_COLUMN_NAME&quot;))])

Target_column = Pipeline([(&#39;替换值&#39;, TimeTakenTransformer(input_column=&quot;TARGET_COLUMN_NAME&quot;))])

TARGET_COLUMN_NAME = &quot;Time_taken(min)&quot; # 假设这是正确的列名

# 假设 df 是您的 DataFrame，应用转换
df_transformed = Target_column.fit_transform(df[[TARGET_COLUMN_NAME]])

# 用转换后的列替换原始列
df[&quot;Time_Taken&quot;] = df_transformed

我无法确定传递输入的正确方法。如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78274143/i-am-doing-feature-transformation-of-my-target-column-using-function-transformer</guid>
      <pubDate>Thu, 04 Apr 2024 13:19:38 GMT</pubDate>
    </item>
    <item>
      <title>将自动编码器转变为另一个模型</title>
      <link>https://stackoverflow.com/questions/78190758/turning-an-autoencoder-into-another-model</link>
      <description><![CDATA[根据我目前所读和所见，自动编码器神经网络的一个优点和用途是找到/挑选有用的特征。如果自动编码器训练良好，那么包含压缩数据的内层（即潜在层或瓶颈）中就有非常有价值的数据。
此层可用作另一个网络的输入，使另一个网络训练更容易/更快/更准确。
换句话说，我们摆脱了解码器部分，并将新层附加到编码器部分。
问题：

新层可以/应该具有不同的激活函数吗？

在潜在层之后，模型是否有任何合理的理由在每个层上添加更多节点数？ （我的意思是，当使用潜在作为新网络的输入时，再次扩大网络是否有意义？）

如何使用 keras 完成此过程？
我知道如何训练模型和自动编码器，但我究竟如何将它们结合起来？


我已经训练了自动编码器，并且知道新网络所需输出的形式。我只知道如何在 keras 和 tensorflow 中使用顺序方法创建网络。]]></description>
      <guid>https://stackoverflow.com/questions/78190758/turning-an-autoencoder-into-another-model</guid>
      <pubDate>Wed, 20 Mar 2024 03:42:57 GMT</pubDate>
    </item>
    <item>
      <title>Pycaret：目标列中出现缺失值错误</title>
      <link>https://stackoverflow.com/questions/78099026/pycaret-got-missing-value-error-in-target-col</link>
      <description><![CDATA[如果目标列包含 NaN，并且当将其作为 Pycaret 中的目标列传递时，它会显示缺失值错误；所有可用的插补方法都适用于其余列，而不适用于所选目标列。
s = setup(df, target = &#39;Life expectancy&#39;, numeric_imputation=&quot;mean&quot;)


ValueError: 在目标列中发现 10 个缺失值：预期寿命。要继续，请从数据中删除相应的行。


目标列包含 NaN，当在 Pycaret 中将其作为目标列传递时，它会显示缺失值错误，如何处理目标列中的缺失值？]]></description>
      <guid>https://stackoverflow.com/questions/78099026/pycaret-got-missing-value-error-in-target-col</guid>
      <pubDate>Mon, 04 Mar 2024 04:54:10 GMT</pubDate>
    </item>
    <item>
      <title>训练特征矩阵 vs 真实输入</title>
      <link>https://stackoverflow.com/questions/77977567/training-feature-matrix-vs-real-input</link>
      <description><![CDATA[我在尝试将我的模型应用于实际场景时遇到了问题。用于训练的原始特征矩阵大于输入数据。
请纠正我，我知道实际应用中的输入可能在大小上要小得多，并且在更糟糕的情况下具有一些不同的特征。
示例：我的数据集是数千个文本文件，它们有两个类别（备忘录 (0) 或字母 (1)）。我使用 linearSVC 训练模型来对这些文件进行分类。
使用 train_test_split 的结果很棒，现在我想用实际场景测试它。实际场景中的输入将是一个文件。该文件将具有较少的特征，并且可能具有不同的特征。在我的上一次测试中，我使用 4500 个特征进行训练，而实际场景中的输入有 350 个特征。
ValueError：X 有 350 个特征，但 LinearSVC 需要 4500 个特征作为输入。
我该如何处理此类问题？]]></description>
      <guid>https://stackoverflow.com/questions/77977567/training-feature-matrix-vs-real-input</guid>
      <pubDate>Sun, 11 Feb 2024 16:49:49 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 自动编码器数据缩放指南</title>
      <link>https://stackoverflow.com/questions/76866677/guidance-on-data-scaling-for-lstm-autoencoder</link>
      <description><![CDATA[我正在处理一个包含 9 个特征的数据集。其中 8 个特征的值范围为 0-255，但一个特征的值明显不同。我正在将此数据集与 LSTM 自动编码器一起使用以进行异常检测，并且对缩放有几个问题：
虽然建议使用 RobustScaler 来处理异常值，但我发现 StandardScaler 在我的测试中表现更好。您能解释一下为什么会这样吗？
我尝试使用 RobustScaler 来处理发散特征，使用 StandardScaler 来处理其余特征。这种方法似乎很有希望。您会推荐这种混合缩放方法吗，还是我应该坚持对所有特征使用一个缩放器？]]></description>
      <guid>https://stackoverflow.com/questions/76866677/guidance-on-data-scaling-for-lstm-autoencoder</guid>
      <pubDate>Wed, 09 Aug 2023 10:06:05 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 使用 RNN 生成路径 - 与输入、输出、隐藏和批量大小混淆</title>
      <link>https://stackoverflow.com/questions/62305941/pytorch-path-generation-with-rnn-confusion-with-input-output-hidden-and-batc</link>
      <description><![CDATA[我按照 RNN 的句子生成教程进行操作，并尝试对其进行修改以生成位置序列，但是我在定义正确的模型参数（例如 input_size、output_size、hidden_​​dim、batch_size）时遇到了麻烦。
背景：
我有 596 个 x、y 位置序列，每个序列看起来像 [[x1,y1],[x2,y2],...,[xn,yn]]。每个序列代表车辆的 2D 路径。我想训练一个模型，给定一个起点（或部分序列），就可以生成其中一个序列。
-我已经填充/截断了序列，使它们的长度都为 50，这意味着每个序列都是形状为 [50,2] 的数组
-然后我将这些数据分为 input_seq 和 target_seq：
input_seq：torch.Size([596, 49, 2]) 的张量。包含所有 596 个序列，每个序列都没有其最后一个位置。
target_seq：torch.Size([596, 49, 2]) 的张量。包含所有 596 个序列，每个序列都没有其第一个位置。
模型类：
class Model(nn.Module):
def __init__(self, input_size, output_size, hidden_​​dim, n_layers):
super(Model, self).__init__()
# 定义一些参数
self.hidden_​​dim = hidden_​​dim
self.n_layers = n_layers
# 定义层
# RNN 层
self.rnn = nn.RNN(input_size, hidden_​​dim, n_layers, batch_first=True)
# 完全连接层
self.fc = nn.Linear(hidden_​​dim, output_size)

def forward(self, x):
batch_size = x.size(0) 
# 使用下面定义的方法初始化第一个输入的隐藏状态
hidden = self.init_hidden(batch_size)
#将输入和隐藏状态传入模型并获取输出
out, hidden = self.rnn(x, hidden)
# 重塑输出，使其适合全连接层
out = out.contiguous().view(-1, self.hidden_​​dim)
out = self.fc(out) 
return out, hidden

def init_hidden(self, batch_size):
# 此方法生成第一个隐藏状态为零的函数，我们将在前向传递中使用它
# 我们还将保存隐藏状态的张量发送到我们之前指定的设备
hidden = torch.zeros(self.n_layers, batch_size, self.hidden_​​dim)
return hidden

我使用以下参数实例化模型：
input_size 为 2（[x,y] 位置）
output_size 为 2（[x,y]位置）
hidden_​​dim 为 2（[x,y] 位置）（或者这应该是 50，就像完整序列的长度一样？）
model = Model(input_size=2, output_size=2, hidden_​​dim=2, n_layers=1)
n_epochs = 100
lr=0.01
# 定义损失、优化器
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# 训练运行
for epoch in range(1, n_epochs + 1):
optimizer.zero_grad() # 清除上一个 epoch 的现有梯度
output, hidden = model(input_seq)
loss = criterion(output, target_seq.view(-1).long())
loss.backward() # 是否反向传播并计算梯度
optimizer.step() # 相应地更新权重
if epoch%10 == 0:
print(&#39;Epoch: {}/{}.............&#39;.format(epoch, n_epochs), end=&#39; &#39;)
print(&quot;Loss: {:.4f}&quot;.format(loss.item()))

当我运行训练循环时，它会失败并出现此错误：
ValueError Traceback (most recent call last)
&lt;ipython-input-9-ad1575e0914b&gt; in &lt;module&gt;
3 optimizer.zero_grad() # 清除上一个 epoch 的现有梯度
4 output, hidden = model(input_seq)
----&gt; 5 loss = criterion(output, target_seq.view(-1).long())
6 loss.backward() # 进行反向传播并计算梯度
7 optimizer.step() # 相应地更新权重
...

ValueError: 预期输入 batch_size (29204) 与目标 batch_size (58408) 匹配。

我尝试修改 input_size、output_size、hidden_​​dim 和 batch_size 并重塑张量，但我尝试得越多，就越困惑。有人能指出我做错了什么吗？
此外，由于批次大小在 Model.forward(self,x) 中定义为 x.size(0)，这意味着我只有一个大小为 596 的批次，对吗？拥有多个较小批次的正确方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/62305941/pytorch-path-generation-with-rnn-confusion-with-input-output-hidden-and-batc</guid>
      <pubDate>Wed, 10 Jun 2020 14:22:24 GMT</pubDate>
    </item>
    <item>
      <title>将堆叠的 RNN 输出馈入全连接层</title>
      <link>https://stackoverflow.com/questions/49811006/feed-stacked-rnn-output-into-fully-connected-layer</link>
      <description><![CDATA[我正在尝试使用 TensorFlow 中的堆叠 RNN 解决回归问题。RNN 输出应输入到完全连接层以进行最终预测。目前，我正在努力研究如何将 RNN 输出输入到最终的完全连接层。
我的输入形状为 [batch_size, max_sequence_length, num_features]
RNN 层创建如下：
cells = []
for i in range(num_rnn_layers):
cell = tf.contrib.rnn.LSTMCell(num_rnn_units)
cells.append(cell)

multi_rnn_cell = tf.contrib.rnn.MultiRNNCell(cells)

outputs, states = tf.nn.dynamic_rnn(cell=multi_rnn_cell,
input=Bx_rnn,dtype=tf.float32)

输出形状为 [batch_size, max_sequence_length, num_rnn_units]
我尝试仅使用最后一个时间步的输出，例如这个：
final_outputs = tf.contrib.layers.fully_connected(
output[:,-1,:],
n_targets,
activation_fn=None)

我还找到了一些例子和书籍，建议像这样重塑输出和目标：
rnn_outputs = tf.reshape(outputs, [-1, num_rnn_units])
y_reshaped = tf.reshape(y, [-1])

由于我目前使用的批量大小为 500，序列长度为 10000，这会导致矩阵巨大，训练时间非常长，内存消耗巨大。 
我还发现许多文章建议将输入拆分并再次堆叠输出，但由于形状不匹配，我无法实现这一点。
将 RNN 输出馈送到全连接层的正确方法是什么？或者我应该使用 RNN 状态而不是输出？
编辑：
澄清一下：我确实需要这些长序列，因为我正在尝试建模一个物理系统。输入是一个单一特征，由白噪声组成。我有多个输出（在这个特定系统中有 45 个）。脉冲影响系统状态大约 10.000 个时间步骤。
即，目前我正在尝试建模一个由振动器动画化的汽车齿轮桥接。 15 个加速度传感器测量 3 个方向（X、Y 和 Z）的输出。
批量大小 500 是任意选择的。
无论长序列可能消失的梯度或潜在的内存问题如何，我都对如何正确提供数据感兴趣。我们确实有合适的硬件（即 Nvidia Titan V）。此外，我们已经能够通过经典 DNN 以 &gt;3000 个时间步长的滞后以良好的精度对系统行为进行建模。]]></description>
      <guid>https://stackoverflow.com/questions/49811006/feed-stacked-rnn-output-into-fully-connected-layer</guid>
      <pubDate>Fri, 13 Apr 2018 06:56:11 GMT</pubDate>
    </item>
    </channel>
</rss>