<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Fri, 28 Mar 2025 12:35:39 GMT</lastBuildDate>
    <item>
      <title>使用Sklearn的SeceentialFeaturesElector，Local（Ubuntu VM）和Databrick之间的不同特征选择结果</title>
      <link>https://stackoverflow.com/questions/79541286/different-feature-selection-results-between-local-ubuntu-vm-and-databricks-usi</link>
      <description><![CDATA[我正在使用VM上的Ubuntu在Databricks上使用Ubuntu在VS代码中运行我的机器学习管道。当我使用相同的代码测试相同的数据集时，我从SeecentionFeaturesElector获得了不同的选定功能，这会导致不同的最终模型输出。
调试，我尝试了以下内容：

圆形的X和Y到4个小数点，以检查是否有少量阅读差异。
设置全局种子（np.random.seed（seed），andand.seed（seed））以控制随机性。
明确设置Random_State = sequentialFeaturesElector中的kfold中的种子。
单独运行RIDGECV（没有特征选择，没有标准标准器（）），并确认它在这两台机器上都会产生相同的结果。
确保Python和所有库的版本相同。

 观察：

当我只运行RidgeCV时，我在这两台机器上都会获得相同的结果。
当我运行sequentialFeaturesElector时，它会在本地与数据括号上选择不同的功能集，从而导致不同的模型输出。
我怀疑我尚未考虑SFS或交叉验证中可能存在一个随机性问题。

尽管使用了相同的数据和种子，但SequentialFeaturesElector为什么在本地和数据映中会产生不同的结果？
 ＃设置全局种子
种子= 42
np.random.seed（种子）
随机种子（种子）

＃山脊回归模型
ridge_model = ridgecv（
    alpha = np.logspace（-10，2，200），＃alpha =正则化强度
    fit_intercept = true，  
    store_cv_values = false）

＃型号管道：标准化 +脊回归
model_pipeline = make_pipeline（standardscaler（），ridge_model）

＃顺序特征选择（SFS）
sfs = SeceentialFeaturesElector（
    model_pipeline，
    n_features_to_select =&#39;auto&#39;，
    方向=&#39;向前&#39;，
    评分=&#39;r2&#39;，
    cv = kfold（n_splits = 2，Random_state = seed，shuffle = true））

＃适合SFS
sfs.fit（x，y）

＃获得选定的功能
predictors = sfs.get_feature_names_out（）。tolist（）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79541286/different-feature-selection-results-between-local-ubuntu-vm-and-databricks-usi</guid>
      <pubDate>Fri, 28 Mar 2025 11:22:54 GMT</pubDate>
    </item>
    <item>
      <title>用不同的二进制滤波器培训Tesseract会影响ENG.TRAINDATA吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79539916/would-training-tesseract-with-different-binarization-filters-affect-eng-trainedd</link>
      <description><![CDATA[我正在与Tesseract OCR合作，并想尝试不同的二进制方法，例如OTSU的阈值和其他自定义过滤器，以提高文本识别精度。
但是，我担心使用这些不同的预处理技术培训可能会修改或覆盖Eng.trainedData，我想保持完整。
我的问题是：

 培训新模型是否会影响现有的Eng.trainedData文件？

 如何在不修改默认英语模型的情况下安全地使用新过滤器训练Tesseract？

 是否有推荐的方法在保持启动时训练tesseract在预处理图像上。


 我尝试的是： 
用三个样本更新了我当前的eng_new.traineddata，每个样本都应用了过滤器otsu，ottu_tresh_binary，otsu_tresh_binary_inv
在第一次1000次迭代之后，我在初始训练和目标培训之间有所区别。DATA
但受到训练的目标。达塔的结果稍差。
  lstMtraining -continue_from/home/j/tribnedcurrenteng/data/checkpoints/eng_training -trainedData/home/j/trainingcurrenteng/data/data/data/eng.traineddata-train_train_train_listfile/tristfile/home/home/home/j/j/j/j/trainingcurrentengeng gultial_listefile_list_list_listfile-ev /home/j/trainingcurrenteng/data/list.eval--model_output/home/j/trainingcurrenteng/data/checkpoints/eng_training -learning_rate_rate 0.0001 -debug_interval 10 -m -max_interations 600
 
  tesseract otsu_tresh_binary_inv.tiff output_text -l eng -tessdata -dir/home/j/j/triendingcurrenteng/data -psm 7
 
  cat output_text.txt

ABCD123
 
  tesseract otsu_tresh_binary_inv.tiff output_text_1 -l eng_trained -tessdata -dir/home/home/j/j/triendingcurrenteng/data -psm 7
 
  cat output_text_1.txt
ABC
 
如何在不干扰现有模型的情况下训练自定义模型？]]></description>
      <guid>https://stackoverflow.com/questions/79539916/would-training-tesseract-with-different-binarization-filters-affect-eng-trainedd</guid>
      <pubDate>Thu, 27 Mar 2025 19:57:58 GMT</pubDate>
    </item>
    <item>
      <title>使用分层k折后代码：我们需要指定哪个折叠？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79539911/code-after-using-stratified-k-fold-do-we-need-to-specify-which-fold</link>
      <description><![CDATA[我一直在使用简单的技术来拆分数据集。但是，我很想通过更先进的技术进行进步。例如分层的k倍数据不平衡数据。但是，一旦将数据拆分，例如我提供的代码。我们如何继续进行预处理阶段？
示例：我们无法使用整个数据集估算NAN值。我们如何编码？
 ＃假设sem_df是您的数据框，“目标”是目标列
x = sem_df.drop（&#39;target&#39;，轴= 1）
y = sem_df [&#39;target&#39;]

n_total = len（sem_df）

＃1。将数据分为80％的培训和20％的临时培训（将分为验证和测试）
x_train，x_test，y_train，y_test = train_test_split（
    x，y， 
    test_size = 0.20， 
    分层= y， 
    Random_State = 42
）

＃印刷形状具有百分比信息
print（f＆quot; train设置形状：{x_train.shape}（{x_train.shape [0] / n_total * 100：.1f}％）＆quot&#39;）
打印（f＆quot;测试集形状：{x_test.shape}（{x_test.shape [0] / n_total * 100：.1f}％）＆quot; quot&#39;）
 
这是插补问题：
 ＃注意：火车和测试的不同值
train_num_medians = x_train [num_cols] .median（）
train_cat_modes = x_train [cat_cols] .mode（）。iLoc [0]

＃注意：火车和测试的不同值
test_num_medians = x_test [num_cols] .median（）
test_cat_modes = x_test [cat_cols] .mode（）。iLoc [0]

＃使用.loc且不在内地的训练集中估算丢失值
x_train.loc [：，num_cols] = x_train.loc [：，num_cols] .fillna（train_num_medians）
x_train.loc [：，cat_cols] = x_train.loc [：，cat_cols] .fillna（train_cat_modes）

＃使用训练值值将测试集中的缺失值算
x_test.loc [：，num_cols] = x_test.loc [：，num_cols] .fillna（test_num_medians）
x_test.loc [：，cat_cols] = x_test.loc [：，cat_cols] .fillna（test_cat_modes）
 
，甚至对于标准化部分：
 ＃现在安全执行缩放：
sualer = StandardScaler（）
x_train [norm_col] = scaler.fit_transform（x_train [norm_col]）
＃x_val.loc [：，norm_col] = scaler.transform（x_val [to_norm]）
x_test [norm_col] = scaler.transform（x_test [norm_col]）
 
执行与交叉验证的数据拆分时，我们必须更改经典步骤吗？
火车 - ＆gt; fit_transform 
测试 - ＆GT;变换]]></description>
      <guid>https://stackoverflow.com/questions/79539911/code-after-using-stratified-k-fold-do-we-need-to-specify-which-fold</guid>
      <pubDate>Thu, 27 Mar 2025 19:56:19 GMT</pubDate>
    </item>
    <item>
      <title>在两个nn.modulelist上使用zip（）</title>
      <link>https://stackoverflow.com/questions/79536891/using-zip-on-two-nn-modulelist</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79536891/using-zip-on-two-nn-modulelist</guid>
      <pubDate>Wed, 26 Mar 2025 18:02:29 GMT</pubDate>
    </item>
    <item>
      <title>初学者如何在知识论坛中有效地提出大型语言模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79536492/how-can-a-beginner-effectively-present-large-language-models-in-a-knowledge-foru</link>
      <description><![CDATA[我是大型语言模型（LLM）领域的初学者，并已被邀请为我的工作场所的知识贡献论坛做出贡献。目的是分享与LLM有关的见解和经验。
由于我仍在学习，所以我想确保我的演讲准确，引人入胜且易于理解，这些人也可能是该主题的新手。]]></description>
      <guid>https://stackoverflow.com/questions/79536492/how-can-a-beginner-effectively-present-large-language-models-in-a-knowledge-foru</guid>
      <pubDate>Wed, 26 Mar 2025 13:00:21 GMT</pubDate>
    </item>
    <item>
      <title>Jupyter |内核似乎已经死亡。它将自动重新启动| keras.models.Sequinential</title>
      <link>https://stackoverflow.com/questions/79536194/jupyter-the-kernel-appears-to-have-died-it-will-restart-automatically-keras</link>
      <description><![CDATA[我正在使用顺序模型构建我的深度学习模型，而我正在执行 model.fit 我的jupyter笔记本与消息死亡内核似乎已经死亡。它将自动重新启动。
  model_class = tf.keras.models.sequeential（）
model_class.add（tf.keras.layers.dense（11，activation =&#39;relu&#39;））
model_class.add（tf.keras.layers.dense（8，activation =&#39;relu&#39;））
model_class.add（tf.keras.layers.dense（8，activation =&#39;softmax&#39;））
＃编译模型
model_class.compile（lose =; cancorical_crossentropy＆quot＆quort＆quort = [＆quord&#39;cocucre＆quort＆quort＆quort＆quort＆quotizer =; sgd; sgd＆quort;

＃适合模型
model_class.fit（x = xc_train，y = yc_train，batch_size = 20，epochs = 100，versitation_data =（xc_val，yc_val））
 ]]></description>
      <guid>https://stackoverflow.com/questions/79536194/jupyter-the-kernel-appears-to-have-died-it-will-restart-automatically-keras</guid>
      <pubDate>Wed, 26 Mar 2025 10:57:03 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用机器学习来计算此图片中的发芽（绿色）和非发芽种子（红色）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79535638/how-can-i-use-machine-learning-to-count-germinated-green-and-non-germinated-se</link>
      <description><![CDATA[这张照片中如何使用机器学习来计算发芽（绿色）和非发芽种子（红色）？ 在此处输入图像描述 
我以前尝试过使用imagej，但是辐射的颜色与背景太相似，没有合适的阈值来区分它们]]></description>
      <guid>https://stackoverflow.com/questions/79535638/how-can-i-use-machine-learning-to-count-germinated-green-and-non-germinated-se</guid>
      <pubDate>Wed, 26 Mar 2025 07:00:07 GMT</pubDate>
    </item>
    <item>
      <title>如何在没有自动差异的神经网络的情况下实施反向传播？</title>
      <link>https://stackoverflow.com/questions/79535519/how-to-implement-backpropagation-without-auto-differentiation-for-a-feedforward</link>
      <description><![CDATA[我正在研究一个深度学习任务，该任务需要使用仅使用 numpy （没有Tensorflow，Pytorch或其他自动分辨率工具）实现 feedforward神经网络（FNN）。该网络具有三层（2048、512、10个神经元），使用 relu和SoftMax激活，并用 mini-Batch随机渐变（SGD）进行了优化。
分配需要手动实施向前传播，反向传播和重量更新，以确保我使用 BackPropagation AlgorithM  确保我正确计算每个层的梯度。。
但是，我正在努力正确地计算每个层的梯度，尤其是当处理 relu激活的衍生物和 softmax 时。我想确认我的梯度计算和重量更新是正确的。
 导入numpy作为NP

def relu（x）：
    返回np.maximum（0，x）

def relu_derivative（x）：
    返回（x＆gt; 0）.astype（float）＃relu derivative（1如果x＆gt; 0，else 0）

def softmax（x）：
    exp_x = np.exp（x -np.max（x，axis = 1，keepdims = true））＃稳定技巧
    返回exp_x / np.sum（exp_x，axis = 1，keepdims = true）

def cross_entropy_loss（y_pred，y_true）：
    返回-np.mean（np.sum（y_true * np.log（y_pred + 1e -9），轴= 1））＃防止log（0）

def softmax_cross_entropy_grad（y_pred，y_true）：
    返回y_pred -y_true＃softmax +跨渗透衍生物

def gradient_check（）：
    np.random.seed（42）＃确保可重复性
    
    ＃假输入（batch_size = 3，input_dim = 5）
    x = np.random.randn（3，5）
    w1 = np.random.randn（5，4）
    b1 = np.zeros（（1，4））

    ＃假式单壁编码标签（batch_size = 3，num_classes = 4）
    y = np.Array（[[[0，1，0，0]， 
                  [1，0，0，0]， 
                  [0，0，1，0]]）

    z1 = np.dot（x，w1） + b1
    a1 = relu（z1）
    y_pred = softmax（a1）
    损失= cross_entropy_loss（y_pred，y）

    dl_da1 = softmax_cross_entropy_grad（y_pred，y）＃渐变W.R.T. SoftMax输出
    dl_dz1 = dl_da1 * relu_derivative（z1）＃与relu的链条规则

    打印（y_pred（SoftMax输出）：\ n＆quort y_pred）
    打印（“损失：; quot”损失）
    打印（&#39;梯度W.R.T. SoftMax输出（DL/DA1）：\ n＆quort; dl_da1）
    打印（``relu derivative&#39;&#39;之后的渐变（dl/dz1）：\ n＆quort; dl_dz1）

gradient_check（）
 
我仅使用 numpy 并手动计算的反向传播实现了 feedforward神经网络（FNN）。我期望损失会降低和的准确性，但相反，损失波动，准确性保持 low ，有时梯度 爆炸或成为nan 。我怀疑 relu的导数和 softmax +交叉渗透梯度计算的问题，需要验证我的反向传播是否正确。]]></description>
      <guid>https://stackoverflow.com/questions/79535519/how-to-implement-backpropagation-without-auto-differentiation-for-a-feedforward</guid>
      <pubDate>Wed, 26 Mar 2025 05:59:43 GMT</pubDate>
    </item>
    <item>
      <title>通过使用Densenet169结构进行结肠癌预测的模型进行微调模型[闭幕]</title>
      <link>https://stackoverflow.com/questions/79535469/fine-tuning-a-model-utilizing-densenet169-architecture-for-colon-cancer-predicti</link>
      <description><![CDATA[我使用Python在10,000张图像的数据集上使用Densenet169创建了一个模型，以预测结肠癌，两个子文件夹有5000张图像，每张图像5000张图像用于良性和癌组织。训练持续了8小时，训练后的F1分数为0.75，这是不可取的。我试图查看我可以做出的更改以改善指标，但是我没有任何更改的方法，并且需要帮助确定可以在哪里进行更改以提高F1分数。这是我第一次从事这样的事情，我发现没有在线材料进行深度学习可以帮助我解决这个问题
 将TensorFlow导入为TF
来自Tensorflow.keras.applications导入Densenet169
来自tensorflow.keras.layers导入密集，globalaveration -pooling2d，Randomflip，RandomRotation，辍学
来自Tensorflow.keras.models导入模型，顺序
来自Tensorflow.keras.optimizer导入Adam
从tensorflow.keras.regulinizer导入l2
来自tensorflow.keras.applications.densenet导入preprocess_input
来自sklearn.metrics导入混乱_matrix，f1_score
导入numpy作为NP
进口海洋作为SNS
导入matplotlib.pyplot作为PLT
导入操作系统

＃定义常数
img_size =（224，224）＃匹配densenet169输入大小
batch_size = 32
时代= 15
data_dir =＆quot; colon_images＆quot;
class_names = [＆quast; cancyous&#39;&#39;正常＆quot;]
split_ratio = 0.2

＃直接加载数据集在224x224
def create_dataset（子集）：
    返回tf.keras.utils.image_dataset_from_directory（
        data_dir，
        验证_split = split_ratio，
        子集=子集
        种子= 42，
        image_size = img_size，＃直接以目标大小加载
        batch_size = batch_size，
        label_mode =&#39;binary&#39;
    ）

train_ds = create_dataset（“训练”）
val_ds = create_dataset（&#39;验证＆quot;）

＃通过增强进行预处理（仅在培训期间活跃）
预处理=顺序（[
    Randomflip（“水平”，“），
    随机旋转（0.1），
    tf.keras.layers.lambda（preprocess_input）＃正确归一化
）））

＃构建模型
base_model = densenet169（
    权重=&#39;Imagenet&#39;，
    include_top = false，
    input_shape = img_size +（3，）
）

输入= tf.keras.input（shape = img_size +（3，））
X =预处理（输入）
x = base_model（x）
x = globalaveragepooling2d（）（x）
x =密集（512，激活=&#39;relu&#39;，kernel_regularizer = l2（0.01））（x）
x =辍学（0.5）（x）＃正则化
输出=密集（1，激活=&#39;Sigmoid&#39;）（x）
模型=模型（输入，输出）

＃阶段1：火车顶层
base_model.trainable = false
model.compile（
    优化器= ADAM（1E-3），
    损失=&#39;binary_crossentropy&#39;，
    量表= [&#39;fecycy&#39;，tf.keras.metrics.auc（name =&#39;auc&#39;），
             tf.keras.metrics.precision（name =&#39;precision&#39;），
             tf.keras.metrics.Recall（name =&#39;recember&#39;）]
）

＃用回调监视AUC的火车
早期_stop = tf.keras.callbacks.earlystopping（
    Monitor =&#39;Val_auc&#39;，耐心= 3，模式=&#39;max&#39;，详细= 1
）
检查点= tf.keras.callbacks.modelcheckpoint（
    &#39;best_model.h5&#39;，save_best_only = true，monitor =&#39;val_auc&#39;，mode =&#39;max&#39;
）

历史= model.fit（
    train_ds，
    验证_data = val_ds，
    时代= epochs，
    回调= [早期_STOP，检查点]
）

＃阶段2：微调整个模型
base_model.trainable = true
model.compile（
    优化器= ADAM（1E-5），＃非常低的学习率
    损失=&#39;binary_crossentropy&#39;，
    量表= [&#39;facer&#39;，tf.keras.metrics.auc（name =&#39;auc&#39;）]
）

history_fine = model.fit（fit）（
    train_ds，
    验证_data = val_ds，
    时代= epochs，
    onirome_epoch = history.epoch [-1]，
    回调= [早期_STOP，检查点]
）

＃ 评估
y_pred =（model.predict（val_ds）＆gt; 0.5）.astype（int）
y_true = np.concatenate（[y for _，y in val_ds]，axis = 0）

打印（f＆quot f1分数：{f1_score（y_true，y_pred）：。3f}＆quot;）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79535469/fine-tuning-a-model-utilizing-densenet169-architecture-for-colon-cancer-predicti</guid>
      <pubDate>Wed, 26 Mar 2025 05:27:58 GMT</pubDate>
    </item>
    <item>
      <title>CATBOOST模型的串联TF-IDF数据和分类数据</title>
      <link>https://stackoverflow.com/questions/79531266/concatenating-tf-idf-data-and-categorical-data-for-catboost-model</link>
      <description><![CDATA[我一直在尝试将TF-IDF数据与分类数据相连。但是，当串联时，默认情况下，分类数据会自动转换为float。由于catboost不支持分类特征的浮动，因此由于不再被认为是分类数据而导致稀疏数据的错误。
有解决这个问题的解决方案吗？请在下面找到我的代码以供参考：
 导入numpy作为NP
导入大熊猫作为pd
来自Catboost Import CatboostClassifier
来自sklearn.feature_extraction.text导入tfidfvectorizer
从Sklearn.Preprocessing Import LabElenCoder
来自scipy.sparse导入hstack，csr_matrix

text_data = [
    “我喜欢机器学习和数据科学”
    “深度学习是机器学习的子集”
    “自然语言处理是惊人的”
    “ AI正在改变世界”
    “大数据和AI正在彻底改变行业”。
这是给出的

pecorical_data = {
    “ cantory”：“ tech; quot” tech&#39;tech&#39;nlp’s&#39;&#39;
    “地区”：“欧洲”，“亚洲”欧洲“欧洲”
}

y = np.Array（[0，1，0，1，1]）

df_cat = pd.dataframe（centorical_data）

vectorizer = tfidfvectorizer（）
x_tfidf = vectorizer.fit_transform（text_data）

df_cat_encoded = df_cat.apply（labelencoder（）。fit_transform）

x_categorical = csr_matrix（df_cat_encoded.values）

x_combind = hstack（[x_tfidf，x_categorical]）

model = catboostClassifier（迭代= 100，Learning_rate = 0.1，深度= 5，冗长= 0）

model.fit（x_combined，y，cat_features = [x_tfidf.shape [1]，x_tfidf.shape [1] + 1]）

预测= model.predict（x_combined）

打印（预测）
 
错误：
  catboostror：&#39;data&#39;是scipy.sparse.spmatrix floating Point数值类型， 
这意味着没有分类功能，但是“ cat_features”参数指定非零 
分类功能的数量
 ]]></description>
      <guid>https://stackoverflow.com/questions/79531266/concatenating-tf-idf-data-and-categorical-data-for-catboost-model</guid>
      <pubDate>Mon, 24 Mar 2025 13:53:36 GMT</pubDate>
    </item>
    <item>
      <title>有什么方法可以查看python中bert的内部Q，k，v配置[封闭]</title>
      <link>https://stackoverflow.com/questions/79530683/is-there-any-way-to-view-the-internal-q-k-v-configurations-of-bert-in-python</link>
      <description><![CDATA[我正在进行一个NLP项目，我需要分析该过程的工作方式的各个方面。本质上潜入黑匣子。我需要使用BERT为数据集生成嵌入（我已经完成了）。但是虽然这样做，但它具有Q，K和V矩阵和注意力评分 - 我必须获得，解释和可视化。从本质上讲，它可以闯入伯特而不打破它。我将如何处理？
我生成了嵌入式和可以可视化的嵌入，但是伯特的内部配置我什至不知道从哪里开始可视化它们。我不想为此使用chatgpt，所以这就是为什么我在这里。]]></description>
      <guid>https://stackoverflow.com/questions/79530683/is-there-any-way-to-view-the-internal-q-k-v-configurations-of-bert-in-python</guid>
      <pubDate>Mon, 24 Mar 2025 09:31:26 GMT</pubDate>
    </item>
    <item>
      <title>如何使用OpenAI的API（GPT）处理PDF？</title>
      <link>https://stackoverflow.com/questions/77469097/how-can-i-process-a-pdf-using-openais-apis-gpts</link>
      <description><![CDATA[ CHATGPT的Web界面具有简单的PDF上传。 是否有Openai的API可以接收PDF？ 
我知道有第三方库可以读取PDF，但是鉴于PDF中有图像和其他重要信息，如果像GPT 4 Turbo这样的模型直接喂食实际PDF可能会更好。。
我将说明我的用例以添加更多上下文。我打算做抹布。在下面的代码中，我处理PDF和提示。通常，我会在提示结束时附加文本。如果我手动提取其内容，我仍然可以用PDF做到这一点。
以下代码从此处获取 https://platform.openai.com/doces.com/docs/assistans/assistans/tools/tools/tools/code-interterpreter-这是我应该做的吗？
 ＃＃用“助手”上传文件。目的
file = client.files.files.create（
  file = open（示例。
  目的=&#39;助手&#39;
）

＃使用文件ID创建助手
助理= client.beta.assistants.greate（
  说明=“您是个人数学老师。当被问到一个数学问题时，编写并运行代码以回答问题。
  Model =＆quort; gpt-4-1106-Preview＆quot; quot;
  工具= [{{&#39;type;：＆quod_interpreter;}]，
  file_ids = [file.id]
）
 
也有一个上传端点，但是这些端点的目的似乎是针对微调和助手的。我认为抹布用例是普通的，不一定与助手有关。]]></description>
      <guid>https://stackoverflow.com/questions/77469097/how-can-i-process-a-pdf-using-openais-apis-gpts</guid>
      <pubDate>Sun, 12 Nov 2023 13:25:44 GMT</pubDate>
    </item>
    <item>
      <title>总和和平均值在.backward（）计算损失和通过网络反向传播时的差异</title>
      <link>https://stackoverflow.com/questions/72429838/difference-between-sum-and-mean-in-backward-in-calculating-the-loss-and-backp</link>
      <description><![CDATA[我知道我们应该在向后应用之前将张量转换为标量（），但是何时和何时表示？
  some_loss_function.sum（）。backward（）
-或者-
some_loss_function.mean（）。backward（）
 ]]></description>
      <guid>https://stackoverflow.com/questions/72429838/difference-between-sum-and-mean-in-backward-in-calculating-the-loss-and-backp</guid>
      <pubDate>Mon, 30 May 2022 06:13:42 GMT</pubDate>
    </item>
    <item>
      <title>神经网络拟合乳腺癌数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/52358505/neural-network-underfitting-breast-cancer-dataset</link>
      <description><![CDATA[我正在尝试在乳腺癌数据集上创建一个用于二进制分类的神经网络：
  https://wwwww.kaggle.com/uciml/uciml/uciml/breast-cancer-cancer-wisconsin-data-data-data-data-data-data 
我的神经网络由3层组成（不包括输入层）：

 第一层：6个带有Tanh激活的神经元。

 第二层：6个带有Tanh激活的神经元。

 最终层：1个神经元，带有Sigmoid激活。


不幸的是，我在训练示例中仅获得约44％的精度，在测试示例中的精度约为23％。
这是我的python代码：
 导入numpy作为NP
导入大熊猫作为pd
导入matplotlib.pyplot作为PLT

data = pd.read_csv（&#39;data.csv; quot;）
data = data.drop（[&#39;id&#39;]，轴= 1）
data = data.drop（data.columns [31]，轴= 1）
data = data.replace（{&#39;m&#39;：1，&#39;b&#39;：0}）

x =数据
x = x.drop（[&#39;诊断&#39;]，轴= 1）
x = np.array（x）

x_mean = np.mean（x，axis = 1，keepdims = true）
x_std = np.std（x，axis = 1，keepdims = true）
x_n =（x -x_mean） / x_std
y = np.array（数据[&#39;诊断&#39;]）
y = y.Reshape（569，1）
M = 378
y_train = y [：m，：]
y_test = y [m：，：]

x_train = x_n [：M，：]
x_test = x_n [m :，：]

Def Sigmoid（Z）：
  返回1 /（1 + np.exp（-z））

def dsigmoid（z）：
  返回np.multiply（z，（1 -z））

def tanh（z）：
  返回（np.exp（z）-np.exp（-z）） /（np.exp（z） + np.exp（-z））

def dtanh（z）：
  返回1 -np.square（tanh（z））

def成本（a，y）：
  m = y.形[0]
  返回 - （1.0/m） *np.sum（np.dot（y.t，np.log（a）） + np.dot（（（1 -y）.t，np，np.log（1 -a）））

def train（x，y，型号，epocs，a）：
  W1 =模型[&#39;W1&#39;]
  W2 =模型[&#39;W2&#39;]
  W3 =模型[&#39;W3&#39;]
  
  B1 =模型[&#39;B1&#39;]
  B2 =模型[&#39;B2&#39;]
  B3 =模型[&#39;B3&#39;]
  
  费用= []
  
  对于我的范围（EPOC）：
    
    ＃前传播

    z1 = np.dot（x，w1） + b1
    a1 = tanh（z1）

    z2 = np.dot（a1，w2） + b2
    a2 = tanh（z2）

    z3 = np.dot（a2，w3） + b3
    A3 = Sigmoid（Z3）
    
    costs.append（成本（A3，y））

    #back繁殖
    
    dz3 = z3 -y
    d3 = np.multiply（dz3，dsigmoid（z3））
    dw3 = np.dot（a2.t，d3）
    db3 = np.sum（d3，axis = 0，keepdims = true）

    d2 = np.multiply（np.dot（d3，w3.t），dtanh（z2））
    dw2 = np.dot（a1.t，d2）
    db2 = np.sum（d2，轴= 0，keepdims = true）

    d1 = np.multiply（np.dot（d2，w2.t），dtanh（z1））
    dw1 = np.dot（x.T，d1）
    db1 = np.sum（d1，axis = 0，keepdims = true）

    W1  -  =（A / M） * DW1
    W2- =（A / M） * DW2
    w3- =（a / m） * dw3

    B1  -  =（A / M） * DB1
    b2  -  =（a / m） * db2
    B3  -  =（A / M） * DB3
    
  cache = {&#39;w1&#39;：w1，&#39;w2&#39;：w2，&#39;w3&#39;：w3，&#39;b1&#39;：b1&#39;：b1，&#39;b2&#39;：b2，&#39;b3&#39;：b3}
  返回缓存，成本

np.random.seed（0）

型号= {&#39;w1&#39;：np.random.rand（30，6） * 0.01，&#39;w2&#39;：np.random.rand（6，6） * 0.01，&#39;w3&#39;：np.random.rand（6，1） &#39;b3&#39;：np.random.rand（1，1）}

型号，成本=火车（x_train，y_train，型号，1000，0.1）

plt.plot（[i在范围内（1000）]，费用）
打印（费用[999]）
plt.show（）



def预测（x，y，模型）：
  W1 =模型[&#39;W1&#39;]
  W2 =模型[&#39;W2&#39;]
  W3 =模型[&#39;W3&#39;]
  
  B1 =模型[&#39;B1&#39;]
  B2 =模型[&#39;B2&#39;]
  B3 =模型[&#39;B3&#39;]
  
  z1 = np.dot（x，w1） + b1
  a1 = tanh（z1）

  z2 = np.dot（a1，w2） + b2
  a2 = tanh（z2）

  z3 = np.dot（a2，w3） + b3
  A3 = Sigmoid（Z3）
  
  m = a3.形[0]
  y_predict = np.zeros（（M，1））
  
  对于我的范围（m）：
    y_predict = 1如果a3 [i，0]＆gt; 0.5其他0
  返回y_predict
 ]]></description>
      <guid>https://stackoverflow.com/questions/52358505/neural-network-underfitting-breast-cancer-dataset</guid>
      <pubDate>Sun, 16 Sep 2018 21:24:54 GMT</pubDate>
    </item>
    <item>
      <title>训练后如何用分布的时间来替换嵌入层？</title>
      <link>https://stackoverflow.com/questions/39532572/how-to-replace-an-embedding-layer-with-a-time-distributed-dense-after-training</link>
      <description><![CDATA[我有以下问题：

 我想使用LSTM网络进行文本分类。为了加快训练的速度并使代码更加清楚，我想沿沿 keras.tokenizer 嵌入层以训练我的模型。 
 一旦我训练了我的模型 - 我想计算输出W.R.T.的显着性图。输入。为此，我决定将嵌入层替换为 timeDistributedDense 。 

您知道什么是最好的方法。对于一个简单的模型，我可以简单地使用已知权重的模型来重建模型 - 但我想使其尽可能通用 - 例如替换模型结构的未来并使我的框架尽可能不可知。]]></description>
      <guid>https://stackoverflow.com/questions/39532572/how-to-replace-an-embedding-layer-with-a-time-distributed-dense-after-training</guid>
      <pubDate>Fri, 16 Sep 2016 13:21:25 GMT</pubDate>
    </item>
    </channel>
</rss>