<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 04 Jan 2025 01:14:30 GMT</lastBuildDate>
    <item>
      <title>如何检查随机森林模型是否过度拟合？</title>
      <link>https://stackoverflow.com/questions/79327542/how-to-check-if-the-model-is-overfitting-for-random-forest</link>
      <description><![CDATA[因此，我已经为数据集实现了随机森林，并且平衡了数据，我使用了 80-10-10、70-15-15、60-20-20 和 80-20 方法。我还使用了特征重要性，并在 41 个独立特征中使用了 10 个 imp 特征、15 个 imp 特征、24 个 imp 特征和 34 个 imp 特征。所有上述方法的平均召回率为 95.8%，平均准确率为 96.6%，精确率为 97%。交叉验证召回率得分（我主要关注召回率）为 95.5%。
我使用训练数据对训练数据本身进行预测，得到了 99.8%
我还使用了热图并删除了 3 个高度相关的特征，但我得到了 80-10-10 的相同分数（热图之后）。
我的模型是否过度拟合？如何检查是否过度拟合？]]></description>
      <guid>https://stackoverflow.com/questions/79327542/how-to-check-if-the-model-is-overfitting-for-random-forest</guid>
      <pubDate>Fri, 03 Jan 2025 19:56:33 GMT</pubDate>
    </item>
    <item>
      <title>如何修复具有可变时间维度的 3D U-Net 中的跳过连接维度不匹配问题？</title>
      <link>https://stackoverflow.com/questions/79326988/how-to-fix-skip-connection-dimension-mismatch-in-3d-u-net-with-variable-temporal</link>
      <description><![CDATA[我正在开发一个用于 3D 数据的 U-Net 模型，其中输入有三个维度：（高度、宽度、时间）。第三个维度 时间 是可变的，这给编码器和解码器之间的跳过连接带来了挑战。
出现此问题的原因是编码器和解码器特征图在某些层中的时间维度不匹配。例如：

编码器输出形状：tf.Tensor(shape=(1, 32, 32, 16, 256), dtype=float32)
解码器输出形状：tf.Tensor(shape=(1, 32, 32, 15, 256), dtype=float32)

时间维度（16 vs. 15）不匹配，导致连接期间出现维度不匹配错误。
可能的解决方案：

裁剪编码器特征图：减少编码器特征图的时间维度以与解码器对齐。
填充解码器特征图：向解码器特征图的时间维度添加填充以匹配编码器。

是否有一种首选方法（裁剪与填充）来解决这种不匹配问题，特别是在语义分割中？是否有任何其他最佳实践来处理 3D U-Net 中的可变时间维度？
非常感谢任何见解或建议！]]></description>
      <guid>https://stackoverflow.com/questions/79326988/how-to-fix-skip-connection-dimension-mismatch-in-3d-u-net-with-variable-temporal</guid>
      <pubDate>Fri, 03 Jan 2025 15:53:08 GMT</pubDate>
    </item>
    <item>
      <title>CNN 模型在实验室图像上的准确率始终保持在 0％ [关闭]</title>
      <link>https://stackoverflow.com/questions/79326802/cnn-model-is-staying-constant-at-0-accuracy-with-lab-images</link>
      <description><![CDATA[我的 cnn 有问题。我必须构建一个 cnn，它预测 13x13 像素图像中心像素的 a 和 b 值。我只使用了卷积层和 sigmoid 作为激活函数，但我不想改变这两者，因为这些属于我的论文。我使用 PyTorch 作为主库。所有图像均在 LAB 颜色空间中
我的模型如下所示：
filter = 3
（stride 在 PyTorch 中为标准：(1,1))
conv_2d(3, 8, filter=filter)
sigmoid()
conv_2d(8, 16)
sigmoid()
conv_2d(16, 32)
sigmoid()
conv_2d(32, 64)
sigmoid()
conv_2d(64, 128)
sigmoid()
conv_2d(128, 256)
sigmoid()
flatten(..., 1)
Linear(256, 64)
sigmoid()
Linear(64, 32)
sigmoid()
Linear(32, 8)
sigmoid()
Linear(8, 2)
sigmoid()

训练：
我用 Pytorch 创建了一个自定义数据集，其中有一个目录，13x13 像素图像位于其中，还有一个 train.csv 文件，其中包含文件名以及相关图像中心像素的 a 和 b 值。我使用 64 的批处理大小和 10000 张图像进行训练。数据集中的数据按 128 倍缩放，然后网络输出按比例缩小回原始数据。
自定义数据集：
def target_transform(label):
label_tensor = torch.tensor(label, dtype=torch.float)

normalized_label = label_tensor / 128.0 # 按 128 倍缩放
return normalized_label

class ABSectionDataset(Dataset):
def __init__(self, csv_file, image_dir, section_size=13, transform=None, target_transform=None):
self.data = pd.read_csv(csv_file)
self.image_dir = image_dir
self.transform = transform
self.target_transform = target_transform

def __len__(self):
return len(self.data)

def __getitem__(self, idx):
img_path = os.path.join(self.image_dir, self.data.iloc[idx, 0])

image = Image.open(img_path)

label = self.data.iloc[idx, 1:3].values
label = label.astype(np.float32)

if self.transform:
image = self.transform(image)
if self.target_transform:
label = self.target_transform(label)

return image, label

我尝试了不同的卷积，在网络内部使用不同的输入和输出通道。还有不同的训练大小和批处理大小。
我还检查了模型的输出，无论我给网络哪个输入，它们都是相同的。但输入数据不一样。因此错误不可能是由于输入数据造成的。
这些是前 6 个输入输出对：
输出：tensor([[84.5234, 63.5912]], grad_fn=&lt;SliceBackward0&gt;)
标签：tensor([[162., 190.]])

输出：tensor([[84.5234, 63.5912]], grad_fn=&lt;SliceBackward0&gt;)
标签：tensor([[100., 165.]])

输出：tensor([[84.5234, 63.5912]], grad_fn=&lt;SliceBackward0&gt;)
标签：张量（[[149., 154.]])

输出：张量（[[84.5234, 63.5912]], grad_fn=&lt;SliceBackward0&gt;)
标签：张量（[[155., 62.]])

输出：张量（[[84.5234, 63.5912]], grad_fn=&lt;SliceBackward0&gt;)
标签：张量（[[166., 195.]])

输出：张量（[[84.5234, 63.5912]], grad_fn=&lt;SliceBackward0&gt;)
标签：张量（[[159., 187.]])
]]></description>
      <guid>https://stackoverflow.com/questions/79326802/cnn-model-is-staying-constant-at-0-accuracy-with-lab-images</guid>
      <pubDate>Fri, 03 Jan 2025 14:43:59 GMT</pubDate>
    </item>
    <item>
      <title>Kubernetes 中的多模型（机器学习）部署</title>
      <link>https://stackoverflow.com/questions/79326485/multi-model-machine-learning-deployment-in-kubernetes</link>
      <description><![CDATA[我正在尝试在 Kubernetes 集群中部署多个小型模型。我正在探索多个选项，例如 Seldon Core
有人可以建议将模型部署到 Kubernetes 集群的最流行方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79326485/multi-model-machine-learning-deployment-in-kubernetes</guid>
      <pubDate>Fri, 03 Jan 2025 12:38:11 GMT</pubDate>
    </item>
    <item>
      <title>PPO 不断学习在网格世界环境中什么也不做</title>
      <link>https://stackoverflow.com/questions/79326093/ppo-constantly-learns-to-do-nothing-in-a-grid-world-setting</link>
      <description><![CDATA[我正在尝试使用 PPO 解决自定义网格世界环境。网格为 5x5，其中一个单元格是仓库，代理从这里开始。随着时间的推移，物品会随机出现在网格上，并在那里停留 15 个时间步骤，然后再次消失。代理的目标是收集尽可能多的物品并将它们带到仓库。代理的容量为 1（即，他一次只能携带一件物品），并且可以决定向上、向下、向右、向左或不做任何事情（请注意，拾取或放下不是单独的操作，因为这些事情在物品单元格或仓库上时会自动发生）。如果成功拾取和放下物品，他将获得 +15 的总奖励（分为拾取 +7.5 和放下物品 +7.5）。每一步没有拾取或放下都会产生 -1，除非所选动作是什么都不做，在这种情况下代理会收到 0 的奖励。
我选择用大小为 25 的向量表示代理或物品位置等指标（即，每个单元格的一个条目，如果有物品/代理，则为 1 或任何其他相关数字，否则为 0）。因此，我的观察空间由以下内容组成：可用容量、代理位置、物品位置、剩余时间、目标位置、到物品和目标的曼哈顿距离、到最近物品的剩余时间和距离、从最近物品到目标的距离、到墙壁的距离。
参与者和评论家网络均由 4 个隐藏层组成，包含 128 个神经元和 ReLU 激活函数。至于我的超参数，我选择如下：
learning_rate：0.0001
gamma：0.99
lam：0.95
clip_ratio：0.2
value_coef：0.5
entropy_coef：0.5
num_trajectories：5
num_epochs：4
num_minibatches：4
max_grad_norm：0.5

现在，在运行此程序时，PPO 无法学习任何可用的策略，最终什么也不做。虽然这是一个合理的学习策略，因为它至少不会产生负的总奖励，但显然不是理想的/最佳的。
由于这有点像 PPO 陷入的局部最优，我认为这可能是一个超参数/探索问题。因此，我增加了熵系数以增加探索，并同样增加了每个策略推出的轨迹数量，以便代理在更新网络时拥有更多可用经验。但是，我尝试的方法似乎都没有奏效。我甚至运行了 WandB 扫描，但该扫描的所有 100 次运行都没有获得超过 0 的总奖励。观察到这一点后，我认为代码中一定存在某种错误，这就是为什么我一遍又一遍地检查代码以试图找出问题所在。但是，我无法发现实施中的任何错误（这并不是说存在错误，我只是在检查了 x 次代码后没有发现任何错误）。
有人知道是什么阻止 PPO 学习好的策略吗？显然，代理在连接拿起物品和放下物品的动作时存在问题。但是，我不明白为什么会这样，因为由于取货和送货的奖励是分开的，代理应该很容易就能弄清楚，在有空闲容量的情况下，他应该去任何物品单元，而在满负荷的情况下，他应该去仓库。
如果需要或感兴趣，您可以通过 pastebin 在这里找到整个代码：https://pastebin.com/zuRprVWR。
我还能尝试什么来解决这个问题？您认为问题实际上源于实现/逻辑错误，还是还有其他原因？或者 PPO 毕竟无法解决这个问题，而其他算法可能是更好的选择？]]></description>
      <guid>https://stackoverflow.com/questions/79326093/ppo-constantly-learns-to-do-nothing-in-a-grid-world-setting</guid>
      <pubDate>Fri, 03 Jan 2025 09:59:51 GMT</pubDate>
    </item>
    <item>
      <title>虚拟变量为布尔值而不是整数[关闭]</title>
      <link>https://stackoverflow.com/questions/79325633/dummy-variable-as-boolean-rather-than-integer</link>
      <description><![CDATA[我正在用 Python 开发一个机器学习项目。使用 pandas pd.get_dummies，我尝试为数据中的分类列创建虚拟变量，但变量被转换为布尔值而不是整数，这使得 statsmodels 无法拟合 OLS 模型。我尝试将布尔值转换为整数，但一直出现错误。我该如何解决这个问题？？
我使用了 .astype(int) 方法，我也尝试使用 numpy 转换为整数。
ocean_proximity_dummies = pd.get_dummies(data[&#39;ocean_proximity&#39;], prefix= &#39;ocean_proximity&#39;)

data = pd.concat([data.drop(&quot;ocean_proximity&quot;, axis =1), ocean_proximity_dummies], axis=1)

ocean_proximity_dummies = ocean_proximity_dummies.astype(int)
ocean_proximity_dummies
]]></description>
      <guid>https://stackoverflow.com/questions/79325633/dummy-variable-as-boolean-rather-than-integer</guid>
      <pubDate>Fri, 03 Jan 2025 06:15:51 GMT</pubDate>
    </item>
    <item>
      <title>LSTM模型预测不会随着输入的不同而改变</title>
      <link>https://stackoverflow.com/questions/79323808/lstm-model-prediction-does-not-change-with-different-inputs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79323808/lstm-model-prediction-does-not-change-with-different-inputs</guid>
      <pubDate>Thu, 02 Jan 2025 13:06:55 GMT</pubDate>
    </item>
    <item>
      <title>无法访问自由变量“fig”，因为它与封闭范围内的值没有关联</title>
      <link>https://stackoverflow.com/questions/79270292/cannot-access-free-variable-fig-where-it-is-not-associated-with-a-value-in-enc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79270292/cannot-access-free-variable-fig-where-it-is-not-associated-with-a-value-in-enc</guid>
      <pubDate>Wed, 11 Dec 2024 02:04:10 GMT</pubDate>
    </item>
    <item>
      <title>图像批次在线方差的有效算法[关闭]</title>
      <link>https://stackoverflow.com/questions/75545944/efficient-algorithm-for-online-variance-over-image-batches</link>
      <description><![CDATA[我有大量多维数据，想计算所有数据中某个轴的方差。从内存角度来看，我无法创建一个大型数组来一步计算方差。因此，我需要分批加载数据，并需要在每个批次之后以在线方式更新当前方差。
示例
最后，按批次更新的 online_var 应该与 correct_var 匹配。
但是，我很难找到一种有效的算法。
import numpy as np
np.random.seed(0)
# 正确计算方差
all_data = np.random.randint(0, 9, (9, 3)) # &lt;-- 不适合内存
correct_var = all_data.var(axis=0)
# 创建批次
batches = all_data.reshape(-1, 3, 3)

online_var = 0
for batch in batches:
batch_var = batch.var(axis=0)
online_var = ? # 如何正确更新
assert np.allclose(correct_var, online_var)


我发现了Welford 在线算法，但是它非常慢，因为它只更新单个新值的方差，即它不能一次处理整个批次。当我处理图像时，每个像素和每个通道都需要更新。

如何以有效的方式更新多个新观测值的方差，同时考虑整个批次？]]></description>
      <guid>https://stackoverflow.com/questions/75545944/efficient-algorithm-for-online-variance-over-image-batches</guid>
      <pubDate>Thu, 23 Feb 2023 14:10:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在医疗数据上训练 tensorflow.js</title>
      <link>https://stackoverflow.com/questions/65135003/how-to-train-tensorflow-js-on-medical-data</link>
      <description><![CDATA[我正在使用 tfjs 和一些（假）医疗数据（乳腺癌）编写 POC 脚本。数据如下所示：
[206, 293, 140, 126, 117, 27, 35, 152, 239, 79] 结果 (ys) 为 [1]，其中 1 表示恶性，0 表示良性。
脚本似乎可以进行训练，但准确率/损失从未改变，无论数据如何，我都会得到相同的结果。我已经验证了数据/格式。脚本如下：
 const formedData = _.shuffle(data).map(util.transformRow);

// 定义模型。
const model = tf.sequence();
// 设置网络层
model.add(tf.layers.dense({units: 10,activation: &#39;relu&#39;,inputShape: [10]}));
model.add(tf.layers.dense({units: 150,activation: &#39;relu&#39;}));
model.add(tf.layers.dense({units: 250,activation: &#39;relu&#39;}));
model.add(tf.layers.dense({units: 250,activation: &#39;relu&#39;}));
model.add(tf.layers.dense({units: 250,activation: &#39;relu&#39;}));
model.add(tf.layers.dense({units: 1,activation: &#39;softmax&#39;,outputShape: [1]}));
// 定义优化器
const optimizer = tf.train.adam(LEARNING_RATE);
// 初始化模型
model.compile({
optimizer: optimizer,
loss: &#39;meanSquaredError&#39;,
metrics: [&#39;accuracy&#39;],
});

const ys = formedData.map(d =&gt; [d.ys]);
const xs = formedData.map(d =&gt; d.xs);

let xTrain = tf.tensor2d(xs.slice(0,500), [xs.slice(0,500).length, 10]); // [[123,234,345...], [...]...]
let yTrain = tf.tensor2d(ys.slice(0,500), [ys.slice(0,500).length, 1]); // [[1], [0]...]

console.log(&#39;准备开始训练模型&#39;);
const history = await model.fit(xTrain, yTrain, {
epochs: EPOCHS,
validationData: [xTrain, yTrain],
}) 

需要说明的是，我现在并不关心超准确的结果或优化，我只希望脚本能够真正训练模型。]]></description>
      <guid>https://stackoverflow.com/questions/65135003/how-to-train-tensorflow-js-on-medical-data</guid>
      <pubDate>Thu, 03 Dec 2020 22:17:12 GMT</pubDate>
    </item>
    <item>
      <title>时间序列预测的最佳算法？[关闭]</title>
      <link>https://stackoverflow.com/questions/64544725/best-algorithm-for-time-series-prediction</link>
      <description><![CDATA[我必须每天预测某个区域的总需水量，并根据包含以下内容的 4 个 CVS 文件创建一个模型：

汇总形式的需水量（每日粒度的时间序列，2 年数据）
进入该区域蓄水池的水量（每日粒度的时间序列，2 年数据）
离开该区域蓄水池的水量（每日粒度的时间序列，2 年数据）
来自该区域 4,000 个测量点的用水请求（每日粒度的时间序列，2 年数据）。

您认为，使用现有数据和特征，哪个模型可以很好地预测该区域的需水量？我只能想到 LSTM 或 MLP，我不知道像 ARIMA 或 (SARIMA) 这样的东西在这种情况下是否有用，因为我有很多功能但时间不多。]]></description>
      <guid>https://stackoverflow.com/questions/64544725/best-algorithm-for-time-series-prediction</guid>
      <pubDate>Mon, 26 Oct 2020 20:45:57 GMT</pubDate>
    </item>
    <item>
      <title>K 近邻分类器 - 训练测试分割的随机状态导致不同的准确度分数</title>
      <link>https://stackoverflow.com/questions/63410524/k-nearest-neighbour-classifier-random-state-for-train-test-split-leads-to-diff</link>
      <description><![CDATA[我一直在 python 的 sklearn 模块中对乳腺癌数据集进行一些 KNN 分类分析。我有以下代码，它尝试找到目标变量分类的最佳 k。
来自 sklearn.datasets 导入 load_breast_cancer
来自 sklearn.model_selection 导入 train_test_split
来自 sklearn.neighbors 导入 KNeighborsClassifier
导入 matplotlib.pyplot 作为 plt

breast_cancer_data = load_breast_cancer()

training_data, validation_data, training_labels, validation_labels = train_test_split(breast_cancer_data.data, breast_cancer_data.target, test_size = 0.2, random_state = 40)
results = []

for k in range(1,101):
classifier = KNeighborsClassifier(n_neighbors = k)
classifier.fit(training_data, training_labels)
results.append(classifier.score(validation_data, validation_labels))

k_list = range(1,101)
plt.plot(k_list, results)
plt.ylim(0.85,0.99)
plt.xlabel(&quot;k&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.title(&quot;Breast Cancer Classifier Accuracy&quot;)
plt.show()

代码循环遍历 1 到 100，并生成 100 个 KNN 模型，其中“k”设置为 1 到 100 范围内的增量值。每个模型的性能都保存到一个列表中，并生成一个图，在 x 轴上显示“k”，在 y 轴上显示模型性能。
我遇到的问题是，当我在将数据拆分为训练和测试分区时更改 random_state 参数时，导致完全不同的图，表明不同数据集分区的不同“k”值具有​​不同的模型性能。
对我来说，这使得很难决定哪个“k”是最佳的，因为算法对使用不同随机状态的不同“k”执行不同。这当然并不意味着对于这个特定的数据集，“k”是任意的？有人可以解释一下吗？


]]></description>
      <guid>https://stackoverflow.com/questions/63410524/k-nearest-neighbour-classifier-random-state-for-train-test-split-leads-to-diff</guid>
      <pubDate>Fri, 14 Aug 2020 09:51:57 GMT</pubDate>
    </item>
    <item>
      <title>我必须使用哪种机器学习算法进行序列预测？[关闭]</title>
      <link>https://stackoverflow.com/questions/59543222/which-machine-learning-algorithm-i-have-to-use-for-sequence-prediction</link>
      <description><![CDATA[我有一个如下所示的数据集。我有 datetime 列作为索引，type 是带有序列的列。例如；R、C、D、D、D、R、R 是一个序列。
start_time type

2019-12-14 09:00:00 RCDDDRR 
2019-12-14 10:00:00 CCRD 
2019-12-14 11:00:00 DDRRCC 
2019-12-14 12:00:00 ? 

我想预测 12:00:00 时的下一个序列是什么？哪种算法是预测下一个序列的最佳算法？
我知道我们可以使用马尔可夫链来预测可能的序列。但是，还有其他更好的算法吗？]]></description>
      <guid>https://stackoverflow.com/questions/59543222/which-machine-learning-algorithm-i-have-to-use-for-sequence-prediction</guid>
      <pubDate>Tue, 31 Dec 2019 11:03:17 GMT</pubDate>
    </item>
    <item>
      <title>提高预测算法的准确性</title>
      <link>https://stackoverflow.com/questions/58399563/improving-accuracy-of-prediction-algorithm</link>
      <description><![CDATA[我正在研究预测问题。这是一个包含几列数据的 Excel 电子表格：
https://drive.google.com/file/d/1fWf6dX8kOCRB3GpX42AF6UvTmd0g9zXp/view?usp=sharing
我试图根据 A 列到 E 列的值预测 F 列的值。代码如下
import numpy as np
import pandas as pn
from keras.layers import Dense, Activation
from keras.models import Sequential
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import linear_model
import matplotlib.pyplot as plt
dataset = pn.read_excel(r&quot;G:\Machine learning\data\database.xlsx&quot;, &quot;Sheet5&quot;)
dataset.columns = [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;,&#39;F&#39;]

print (dataset)
#check= dataset.iloc[0:,3 :13]
X = dataset.iloc[0:,0 :5]
print(X)
Y = dataset.iloc[0:, 5 :6]

print(Y)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 0)
print(X_test)
print(Y_test)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
#
model = Sequential()
##
### 添加输入层和第一个隐藏层
model.add(Dense(32,activation =&#39;relu&#39;,input_dim = 5, kernel_initializer=&#39;normal&#39;))
##
### 添加第二个隐藏层
model.add(Dense(units = 16,activation =&#39;relu&#39;))
model.add(Dense(units = 64,activation = &#39;relu&#39;))
model.add(Dense(units = 8,activation = &#39;relu&#39;))
model.add(Dense(units = 16,activation = &#39;relu&#39;))
#model.add(Dense(units = 8,activation = &#39;linear&#39;))
###
#### 添加第三个隐藏层
#model.add(Dense(units = 16,activation = &#39;relu&#39;))
#model.add(Dense(units = 16,activation = &#39;relu&#39;))
#model.add(Dense(units = 16,activation = &#39;relu&#39;))
##
### 添加输出层
model.add(Dense(units = 1))
##
model.add(Dense(units = 1))
##
model.add(Dense(1))
### 编译 ANN
model.compile(optimizer = &#39;nadam&#39;,loss = &#39;mean_squared_error&#39;,metrics= [&#39;accuracy&#39;])
##
### 将 ANN 拟合到训练集
history = model.fit(X_train, Y_train, epochs=125, batch_size=5, verbose=1, validation_split=0.1)
##
y_pred = model.predict(X_test)
##

y_pred1 = model.predict(X_train)
print (y_pred1)
Y_test.reset_index(drop= True, inplace= True)
print (Y_train)
Y_train.reset_index(drop= True, inplace= True)
plt.plot(y_pred1)
plt.plot(Y_train)
plt.show()
plt.plot(y_pred)
plt.plot(Y_test)
plt.show()
print (y_pred)
print (Y_test)
plt.plot((Y_test-y_pred)*100/Y_test)
plt.show()

我从此代码中获得的拟合如下所示。
￼￼拟合
现在当我预测时，在某些情况下误差很大，如下所示
预测
有人可以指导我改进代码以获得更好的预测吗？
￼￼]]></description>
      <guid>https://stackoverflow.com/questions/58399563/improving-accuracy-of-prediction-algorithm</guid>
      <pubDate>Tue, 15 Oct 2019 17:05:58 GMT</pubDate>
    </item>
    <item>
      <title>单词预测算法</title>
      <link>https://stackoverflow.com/questions/18728290/word-prediction-algorithm</link>
      <description><![CDATA[考虑以下情况：

我们有一个可用的词典
我们输入了许多段落的单词，我希望能够根据此输入预测句子中的下一个单词。

假设我们有几个句子，例如“你好，我叫汤姆”、“他叫杰瑞”、“他去没有水的地方”。我们检查哈希表是否存在单词。如果不存在，我们为其分配一个唯一的 ID 并将其放入哈希表中。这样，就不用存储“链”了单词作为一串字符串，我们可以只拥有一个 uniqueID 列表。
上面，例如 (0, 1, 2, 3, 4)、(5, 2, 3, 6) 和 (7, 8, 9, 10, 3, 11, 12)。请注意，3 表示“是”，我们在发现新单词时添加了新的唯一 ID。假设我们得到一个句子“她的名字是”，这将是 (13, 2, 3)。我们想知道，在给定此上下文的情况下，下一个单词应该是什么。这是我想到的算法，但我认为它效率不高：

我们有 N 个链（观察到的句子）的列表，其中一个链可能是 ex。 3,6,2,7,8。
每个链的平均大小为 M，其中 M 是平均句子长度
我们得到一个大小为 S 的新链，例如 13、2、3，我们想知道最有可能的下一个单词是什么？

算法：

首先扫描整个链列表，查找包含完整 S 输入的链（本例中为 13、2、3）。由于我们必须扫描 N 个链，每个链长度为 M，并且每次比较 S 个字母，因此其复杂度为 O(NMS)。

如果我们的扫描中没有包含完整 S 的链，则通过删除最不重要的单词（即第一个单词，因此删除 13）进行下一次扫描。现在，扫描 (2,3)，如 1 中的 ...我们对最坏情况进行 S 次扫描（13、2、3，然后 2、3，然后 3，共 3 次扫描 = S）。因此，总复杂度为 O(S^2 * M * N)。
因此，如果我们有 100,000 条链，平均句子长度为 10 个单词，则我们需要 1,000,000*S^2 才能获得最佳单词。显然，N &gt;&gt; M，因为句子长度通常与观察到的句子数量不成比例，所以 M 可以是一个常数。然后，我们可以将复杂度降低到 O(S^2 * N)。不过，O(S^2 * M * N) 可能更有助于分析，因为 M 可以是一个相当大的“常数”。
对于此类问题，这可能是完全错误的方法，但我想分享我的想法，而不是公然寻求帮助。我之所以这样扫描，是因为我只想扫描我必须扫描的内容。如果没有完整的 S，就继续修剪 S，直到某些链匹配。如果它们永远不匹配，我们就不知道下一个单词该预测什么！有没有关于时间/空间复杂度较低的解决方案的建议？]]></description>
      <guid>https://stackoverflow.com/questions/18728290/word-prediction-algorithm</guid>
      <pubDate>Tue, 10 Sep 2013 20:35:11 GMT</pubDate>
    </item>
    </channel>
</rss>