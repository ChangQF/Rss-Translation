<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 05 Aug 2024 06:23:15 GMT</lastBuildDate>
    <item>
      <title>机器学习聚类模型（类似K-means但功能不同）</title>
      <link>https://stackoverflow.com/questions/78833002/machine-leraning-model-for-clusteringsimilar-with-k-means-but-different-functio</link>
      <description><![CDATA[当我研究几种机器学习模型时，
我看到了几种聚类算法，包括 K-Means。
据我所知，K-Means 使用欧几里得距离作为自己的计算方法，
我想要的不是使用欧几里得距离，而是数据的值。
例如，样本分布很广（如坐标），坐标有自己的值。
我想找到平均值较高的 N 个聚类。
有没有其他适合此图的算法，或者我是否只处理 K-Means 算法中的几个参数即可完成此操作。
谢谢
有没有其他适合此图的算法，或者我是否只处理 K-Means 算法中的几个参数即可完成此操作。]]></description>
      <guid>https://stackoverflow.com/questions/78833002/machine-leraning-model-for-clusteringsimilar-with-k-means-but-different-functio</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>即使没有问题，也会显示意外的缩进错误[关闭]</title>
      <link>https://stackoverflow.com/questions/78832777/showing-unexpected-indent-error-even-though-there-is-no-problem</link>
      <description><![CDATA[当我按下 Enter 并从 IDE 中转到带有缩进的下一行时，它一直显示意外的缩进错误，当我删除缩进并尝试手动按下 Tab 按钮来放置缩进时，它显示相同的错误
我已经使用几乎相同的代码完成了训练函数（我只是稍微改变了名称等），并且没有错误，我尝试手动提供缩进也没有成功
def test_step(model: torch.nn.Module,
data_loader:torch.utils.data.DataLoader,
loss_fn:torch.nn.Module,
accuracy_fn,
device:torch.device=device
):

&quot;&quot;&quot; 使用模型执行测试，尝试在 data_loader&quot;&quot;&quot; 上学习
test_loss,test_acc=0,0 &lt;------此处显示缩进错误
model.eval()

with torch.inference_mode():
for X,y in test_dataloader:
#将数据发送到目标设备
X,y=X.to(device),y.to(device)

#1. 前向传递
test_pred=model(X)

#2. 计算损失
test_loss+=loss_fn(test_pred,y)

#3.计算准确率
test_acc+=accuracy_fn(y_true=y,
y_pred=test_pred.argmax(dim=1))

#计算每批次的测试损失平均值
test_loss/=len(test_dataloader)

#计算每批次的测试准确率平均值
test_acc/=len(test_dataloader)

##打印出正在发生的事情
print(f&quot;Train Loss: {train_loss:.5f} | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%&quot;)``

]]></description>
      <guid>https://stackoverflow.com/questions/78832777/showing-unexpected-indent-error-even-though-there-is-no-problem</guid>
      <pubDate>Mon, 05 Aug 2024 04:45:36 GMT</pubDate>
    </item>
    <item>
      <title>createDataPartition 给出异常不均匀的测试和训练集</title>
      <link>https://stackoverflow.com/questions/78832537/createdatapartition-gives-abnormally-uneven-test-and-train-sets</link>
      <description><![CDATA[我正在尝试使用 caret 包将我的数据拆分为测试集和训练集。我有 77 行，每列都有完整数据。函数“createDataPartition”导致训练数据为 4 行，测试数据为 73 行，这似乎不对。任何帮助都将不胜感激。这是我的代码：
&gt; # 将数据拆分为训练和测试
&gt; set.seed(123)
&gt; data.full &lt;- data.full %&gt;% select(fasting_status, a1c, glu, uc_ratio)
&gt; training.samples &lt;- data.full %&gt;% 
+ createDataPartition(p = 0.8, list = FALSE)
警告消息：
1：在 createDataPartition(., p = 0.8, list = FALSE) 中：
某些类没有记录 ( )，这些将被忽略
2：在 createDataPartition(., p = 0.8, list = FALSE) 中：
某些类只有一条记录 ( )，这些将被选为样本
&gt; train.data &lt;- data.full[training.samples, ]
&gt; test.data &lt;- data.full[-training.samples, ] ```

这是我的可重现数据：

```&gt; dput(data.full) 结构(列表(fasting_status = 结构(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L), 级别 = c(&quot;1&quot;, &quot;2&quot;), 类别 = &quot;因素&quot;), 
a1c = c(4.3, 4.5, 4.4, 2.9, 4.3, 4.4, 4.2, 4.5, 4.2, 4.2, 
4.5, 4.5, 4.8, 4.5, 5.2, 4.9, 4.6, 4.2, 4.4, 4.9, 4.6, 4.5, 
    4.4、4.8、4.5、4.1、3.8、3.1、4.3、4.6、4.7、4.9、4.6、4.4、3.1、4.6、4.4、4.2、4.4、5.2、4.4、5.1、4.6、4.7、5.2、4.7、4。 7、4.6、4.4、4.4、4.2、4.5、4.6、4.4、3.2、4.8、5.2、5.2、4.6、4.9、5.6、4.6、4.9、4.5、5.1、4.6、4.9、4.6、4.3、4.6、
4.6, 4.3, 4.6, 4.3, 4.6, 6.5, 4.8), glu = c(88.5, 98, 117.5, 
53, 108.5, 106, 105, 101, 91, 99.5, 128.5, 113, 114, 121.5, 
121, 131.5, 160.5, 96, 110, 140, 119.5, 115.3, 112, 143.5, 
116.5, 116.5, 111, 139.5, 123.5, 131, 113, 137, 114, 98.5, 
    124.5、123.5、111.5、111、127、123、137.5、119、107、130.5、142.5、115、133.5、119、148.3、125.5、138.5、106.5、153.5、 .5、179、145、143、124.5、134、146.5、127.5、124.5、123、129、145.3、125.5、146.5、153.5、115.5、128、110.5、131、 
139.5, 124, 154, 94, 76.3), uc_ratio = c(30.65603924, 15.32801962, 
60.59075991, 7.39973361, 57.84661317, 27.46781116, 16.0944206, 
6.131207848, 94.61568474, 19.50838861, 7.803355443, 19.41549152, 
7.464079119, 19.67095851, 29.50643777, 62.94706724, 80.472103、25.75107296、73.57449418、39.01677721、41.13018598、10.62933697、7.803355443、30.04291845、32.75355771、 9416、5.969860273、22.72153497、7.153075823、75.61823012、23.50296342、53.64806867、11.19611891、38.25340549、 88.36152487、51.50214592、9.196811772、41.98544505、6.35828962、9.196811772、94.87237407、12.87553648、6.035407725、7.3997 3361、10.72961373、11.70503316、9.035464197、16.34988759、11.68917269、35.11509949、61.85306741、11.36076748、 
    12.2624157、7.153075823、14.30615165、10.40447392、3.901677721、52.11526671、21.45922747、30.49469166、81.06819266、 38861、34.33476395、8.0472103、24.94635193、9.754194304、64.3776824、9.196811772、11.92179304、34.87124464、 74.39198856, 124.4635193, 
13.79521766, 5.722460658, 66.76204101, 69.9757432, 19.50838861
)), row.names = c(NA, -77L), class = &quot;data.frame&quot;)```
]]></description>
      <guid>https://stackoverflow.com/questions/78832537/createdatapartition-gives-abnormally-uneven-test-and-train-sets</guid>
      <pubDate>Mon, 05 Aug 2024 02:29:00 GMT</pubDate>
    </item>
    <item>
      <title>用于 NLP 的 MLP 与 Transformer 架构</title>
      <link>https://stackoverflow.com/questions/78832485/mlp-vs-transformer-architecture-for-nlp</link>
      <description><![CDATA[我不太明白在 NLP 中使用经典 MLP 与自注意力转换器之间的区别。自注意力转换器能做什么而 MLP 不能？它与仅仅添加更多隐藏层有何不同？我理解发送键和查询然后创建注意力权重的要点，但对我来说，直觉上（我知道我错了），这似乎是一种额外的抽象，可以做 MLP 用更多隐藏层可以做的事情。转换器修复了 MLP 的 NLP 架构的哪些根本错误？]]></description>
      <guid>https://stackoverflow.com/questions/78832485/mlp-vs-transformer-architecture-for-nlp</guid>
      <pubDate>Mon, 05 Aug 2024 01:51:18 GMT</pubDate>
    </item>
    <item>
      <title>如何对 Keras 模型的计算使用情况进行基准测试</title>
      <link>https://stackoverflow.com/questions/78832141/how-to-benchmark-keras-model-computational-use</link>
      <description><![CDATA[我正在做一个涉及板载图像分类系统的项目。由于它的硬件非常有限，我想找到一种方法来“基准测试”模型预测如何影响我的 CPU 和内存。我正在使用 google colab 上的 Keras 训练 CNN 模型，并希望获取有关 CPU 使用率和内存使用率的信息，我只想要有关预测部分的信息，而不是训练，因为它不会在板载完成。
出于某种原因，我很难在互联网上找到基准测试这些参数的项目，我找到的是 psutil 和 memory_profiler python 库。所以我想知道使用这些库是否是对这些参数的良好估计，或者在我的硬件上加载模型之前是否有其他更好的方法来测量它。]]></description>
      <guid>https://stackoverflow.com/questions/78832141/how-to-benchmark-keras-model-computational-use</guid>
      <pubDate>Sun, 04 Aug 2024 21:12:56 GMT</pubDate>
    </item>
    <item>
      <title>未找到检查点</title>
      <link>https://stackoverflow.com/questions/78831928/no-checkpoint-found</link>
      <description><![CDATA[# 训练模型并创建检查点以存储权重
import os

checkpoint_dir = &#39;./training_checkpoints&#39;

# 检查点文件的名称
checkpoint_prefix = os.path.join(checkpoint_dir, &quot;ckpt_{epoch}.weights.h5&quot;) # 将 &#39;.weights.h5&#39; 添加到文件名
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only = True)

EPOCHS = 10
history = model.fit(dataset, epochs = EPOCHS, callbacks = [checkpoint_callback])

checkpoint_dir = &#39;/content/training_checkpoints&#39;
latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)

if latest_checkpoint:
print(f&quot;最新检查点发现：{latest_checkpoint}&quot;)
model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)
model.load_weights(latest_checkpoint) 
model.build(tf.TensorShape([1, None]))
else:
print(&quot;未找到检查点。&quot;)

输出：
未找到检查点。

我原本期望在完成 10 个 epoch 后生成最新的检查点，但却没有得到任何检查点……似乎代码无法检测到最新的检查点。为什么以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78831928/no-checkpoint-found</guid>
      <pubDate>Sun, 04 Aug 2024 19:16:56 GMT</pubDate>
    </item>
    <item>
      <title>将来自 OneHotencoder 的稀疏矩阵转换为数组，然后转换为 DataFrame [关闭]</title>
      <link>https://stackoverflow.com/questions/78831669/covert-a-sparse-matrix-arrived-from-onehotencoder-into-an-array-and-then-into-a</link>
      <description><![CDATA[我在将 OneHotencoder 传来的稀疏矩阵转换为数组，然后再转换为 DataFrame 的过程中遇到了错误，即
ValueError: 必须传递 2-D 输入。shape=()

实际上，它之前运行良好。但在重新启动内核后重新运行 shell 后遇到了这个问题。
对于以下代码：
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(handle_unknown=&#39;ignore&#39;)
temp = ohe.fit_transform(df[[&#39;encoded_data&#39;]])

temp = pd.DataFrame(np.array(temp))
temp
]]></description>
      <guid>https://stackoverflow.com/questions/78831669/covert-a-sparse-matrix-arrived-from-onehotencoder-into-an-array-and-then-into-a</guid>
      <pubDate>Sun, 04 Aug 2024 17:09:05 GMT</pubDate>
    </item>
    <item>
      <title>python-error-the-truth-value-of-a-dataframe-is-ambiguous [关闭]</title>
      <link>https://stackoverflow.com/questions/78831608/python-error-the-truth-value-of-a-dataframe-is-ambiguous</link>
      <description><![CDATA[我无法使用以下代码解决错误：
main.html 不会返回推荐的产品，并在数据框上抛出此错误。
def content_based_recommendations(train_data, item_name, top_n=10):
if item_name not in train_data[&#39;Name&#39;].values:
print(f&quot;Item &#39;{item_name}&#39; not found in the training data.&quot;)
return pd.DataFrame()

tfidf_vectorizer = TfidfVectorizer(stop_words=&#39;english&#39;)
tfidf_matrix_content = tfidf_vectorizer.fit_transform(train_data[&#39;Tags&#39;])
cosine_similarities_content = cosine_similarity(tfidf_matrix_content, tfidf_matrix_content)
item_index = train_data[train_data[&#39;名称&#39;] == item_name].index[0]
similar_items = list(enumerate(cosine_similarities_content[item_index]))
similar_items = sorted(similar_items, key=lambda x: x[1], reverse=True)
top_similar_items = similar_items[1:top_n+1]
Recommended_item_indices = [x[0] for x in top_similar_items]
Recommended_items_details = train_data.iloc[recommended_item_indices][[&#39;名称&#39;, &#39;评论数&#39;, &#39;品牌&#39;, &#39;图片网址&#39;, &#39;评分&#39;]]
return Recommended_items_details

@app.route(&quot;/recommendations&quot;, methods=[&#39;POST&#39;])
def suggestions():
try:
prod = request.form.get(&#39;prod&#39;)
nbr = request.form.get(&#39;nbr&#39;)

如果不是 prod 或不是 nbr:
return render_template(&#39;main.html&#39;, message=&quot;缺少产品名称或推荐数量。&quot;)

try:
nbr = int(nbr)
except ValueError:
return render_template(&#39;main.html&#39;, message=&quot;无效的推荐数量。&quot;)

df = pd.read_csv(&#39;clean_data.csv&#39;)
print(df.to_string())
content_based_rec = content_based_recommendations(train_data, prod, top_n=nbr)

if content_based_rec.empty:
return render_template(&#39;main.html&#39;, message=&quot;没有可用于该产品的推荐。&quot;)
else:
random_product_image_urls = [random.choice(random_image_urls) for _ in range(len(content_based_rec))]
price = [40, 50, 60, 70, 100, 122, 106, 50, 30, 50]
return render_template(&#39;main.html&#39;, content_based_rec=content_based_rec, truncate=truncate,
random_product_image_urls=random_product_image_urls,
random_price=random.choice(price))

except Exception as e:
return render_template(&#39;main.html&#39;, message=f&quot;An error occurred: {str(e)}&quot;)

如何解决？
我期望模板上显示的是类似商品的列表。]]></description>
      <guid>https://stackoverflow.com/questions/78831608/python-error-the-truth-value-of-a-dataframe-is-ambiguous</guid>
      <pubDate>Sun, 04 Aug 2024 16:40:42 GMT</pubDate>
    </item>
    <item>
      <title>Ultralytics YOLOv8 姿势估计在自定义数据集上绘制边/颜色时不尊重顶点顺序</title>
      <link>https://stackoverflow.com/questions/78831319/ultralytics-yolov8-pose-estimation-not-respecting-vertices-order-when-drawing-ed</link>
      <description><![CDATA[我按照官方文档，使用 Python 中的 ultralytics 在自定义数据集上进行人体姿势估计训练。我有一个由我标记的 1 幅图像组成的小型数据集，预测工作正常，因此顶点识别和类别识别工作正常。
问题是程序绘制顶点的方式，包括边缘和颜色，与我的骨架不匹配。YOLO 训练中的鼻子被画在我的脚踝上。预测实际上工作正常，但它非常丑陋，因为它绘制了不应该存在的边缘（例如将膝盖顶点与肩部顶点连接起来）。此外，我的姿势估计对象与人类非常相似，因此我认为如果我有相同的骨架，训练会更快。
我已经做过和尝试过的事情：

密切关注官方文档中的标签，0 代表鼻子，1 代表左耳，依此类推。
尝试另一种标签，看看文档是否过时。这会导致 txt 中 train 和 val 的坐标顺序发生变化，因此我认为它会改变输出的颜色和边缘。它什么都不做，这对我来说很奇怪。
读取标准数据集 yaml 选项，例如 super-gradients on coco 中的选项，并复制“edge_links”、“edge_colors”和“keypoint_colors”选项。这样，即使我的顺序不同，它也应该解析这些选项并正确绘制它们。它也没有做任何事情，在我看来，我误解了 yaml 的编写方式。但我不知道是什么。
重要提示，我没有使用任何自动数据集创建工具，而是手动创建所有这些文件。由于图像版权原因，我无法使用 Roboflow，因此我使用本地注释器。我检查了在 Roboflow 中创建的数据集（来自 ultralytics 的 tiger 示例）以查看差异，但我没有看到它们，它们甚至没有定义这些东西，但结果只显示顶点，没有任何边缘。这对我来说也很奇怪。
我正在使用当前版本的 ultralytics，几天前刚刚安装了 pip。

我拥有的代码（由文档提取）是这样的。我的 yaml 是“checa.yaml”。
yaml_of_model=“yolov8n-pose.yaml”
model_to_load =“best.pt”
# 加载模型
model = YOLO(yaml_of_model) # 从 YAML 构建新模型
model = YOLO(model_to_load) # 加载预训练模型（建议用于训练）
model = YOLO(yaml_of_model).load(model_to_load) # 从 YAML 构建并传输权重
results = model.train(data=&quot;./checa.yaml&quot;, epochs=4, imgsz=1920)
results = model(&#39;./screenshot_1.jpg&#39;)

for result in results:
result.show()

我的 yaml 摘录：
# 数据集根目录
path: ...

# 训练和验证目录
train: ...
val: ...

# 关键点和维度的数量（x,y 为 2，x,y,visible 为 3）
kpt_shape: [17, 3]
flip_indexes: [ 0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15,]

edge_links:
- [0, 1]
- ...

edge_colors:
- [214, 39, 40] # Nose -&gt; LeftEye
- ...

keypoint_colors:
- [148, 103, 189]
- ...

# Classes 字典
names:
0: FirstClass
1: ...
]]></description>
      <guid>https://stackoverflow.com/questions/78831319/ultralytics-yolov8-pose-estimation-not-respecting-vertices-order-when-drawing-ed</guid>
      <pubDate>Sun, 04 Aug 2024 14:26:27 GMT</pubDate>
    </item>
    <item>
      <title>生成 512x512 照片的模型</title>
      <link>https://stackoverflow.com/questions/78831225/model-to-generate-512x512-photos</link>
      <description><![CDATA[我如何让这个模型生成 512x512 像素或更大的图像？现在它生成 64x64 像素的图像。我尝试更改模型中的某些值，但没有成功。这些卷积层如何工作，尤其是 Conv2D 和 Conv2DTranspose？我不明白图像在这些层中是如何调整大小的。
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layer
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt

cd /content/drive/MyDrive

dataset = keras.preprocessing.image_dataset_from_directory(
directory = &#39;Humans&#39;, label_mode = None, image_size = (64,64), batch_size = 32,
shuffle = True
).map(lambda x: x/255.0)

discriminator = keras.models.Sequential(
[
keras.Input(shape = (64,64,3)),
layer.Conv2D(64, kernel_size = 4, strides = 2, padding = &#39;相同&#39;),
layers.LeakyReLU(0.2),
layers.Conv2D(128, kernel_size = 4, strides = 2, padding = &#39;相同&#39;),
layers.LeakyReLU(0.2),
layers.Conv2D(128, kernel_size = 4, strides = 2, padding = &#39;相同&#39;),
layers.LeakyReLU(0.2),
layers.Flatten(),
layers.Dropout(0.2),
layers.Dense(1,activation = &#39;sigmoid&#39;)
]
)

latent_dim = 128
generator = keras.models.Sequential(
[
layers.Input(shape = (latent_dim,)),
layers.Dense(8*8*128),
layers.Reshape((8,8,128)),
layers.Conv2DTranspose(128, kernel_size = 4, strides = 2, padding = &#39;same&#39;),
layers.LeakyReLU(0.2),
layers.Conv2DTranspose(256, kernel_size = 4, strides = 2, padding = &#39;same&#39;),
layers.LeakyReLU(0.2),
layers.Conv2DTranspose(512, kernel_size = 4, strides = 2, padding = &#39;same&#39;),
layers.LeakyReLU(0.2),
layers.Conv2D(3, kernel_size = 5,padding = &#39;same&#39;,activation = &#39;sigmoid&#39;)
]
)

opt_gen = keras.optimizers.Adam(1e-4)
opt_disc = keras.optimizers.Adam(1e-4)
loss_fn = keras.losses.BinaryCrossentropy()

for epoch 在 range(500) 中：
对于 idx，real 在 enumerate(tqdm(dataset)) 中：
batch_size = real.shape[0]
random_latent_vectors = tf.random.normal(shape = (batch_size,latent_dim))
fake = generator(random_latent_vectors)

如果 idx % 50 == 0：
img = keras.preprocessing.image.array_to_img(fake[0])
img.save(f&#39;gen_images/generated_img{epoch}_{idx}_.png&#39;)

使用 tf.GradientTape() 作为 disc_tape：
loss_disc_real = loss_fn(tf.ones((batch_size,1)), discriminator(real))
loss_disc_fake = loss_fn(tf.zeros(batch_size,1), discriminator(fake))
loss_disc = (loss_disc_real+loss_disc_fake)/2

grads = disc_tape.gradient(loss_disc, discriminator.trainable_weights)

opt_disc.apply_gradients(
zip(grads, discriminator.trainable_weights)
)

with tf.GradientTape() as gen_tape:
fake = generator(random_latent_vectors)
output = discriminator(fake)
loss_gen = loss_fn(tf.ones(batch_size,1),output)

grads = gen_tape.gradient(loss_gen, generator.trainable_weights)
opt_gen.apply_gradients(
zip(grads, generator.trainable_weights)
)

我尝试更改图像大小和卷积层中的某些值，但它不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/78831225/model-to-generate-512x512-photos</guid>
      <pubDate>Sun, 04 Aug 2024 13:42:23 GMT</pubDate>
    </item>
    <item>
      <title>如何获得更高的余弦相似度分数</title>
      <link>https://stackoverflow.com/questions/78830868/how-to-get-higher-score-for-cosine-similarity</link>
      <description><![CDATA[我已经使用 nltk 清理了我的数据，我的数据是完全干净的，但我仍然无法获得更高的相似度得分，我正在制作一个食谱推荐系统，该系统需要配料并返回我们可以烹饪的食谱
https://colab.research.google.com/drive/1YnM0tUyWhhTQIXZipWdzIFfZGJHViFpi?usp=sharing
帮忙
我正在尝试获得更高的相似度得分，但我仍然停留在 0.62，数据集有超过 6000 行]]></description>
      <guid>https://stackoverflow.com/questions/78830868/how-to-get-higher-score-for-cosine-similarity</guid>
      <pubDate>Sun, 04 Aug 2024 10:55:14 GMT</pubDate>
    </item>
    <item>
      <title>检测并定位图像中的大量不同物体[关闭]</title>
      <link>https://stackoverflow.com/questions/78830813/detect-and-localize-a-large-number-of-different-objects-in-an-image</link>
      <description><![CDATA[我有多个 Match 3 游戏的图片，例如 Candy Crush。下面给出了一个示例图片。我想检测和定位图片中所有不同颜色的物体。定位的意思是获取图片中每个物体的精确坐标。我尝试过不同的方法，例如边缘检测技术、轮廓和其他一些方法，但到目前为止效果并不理想。无法使用模板匹配，因为图像大小不同。我正在考虑切换到 ML 技术，特别是 CNN，但为此我必须创建一个庞大的数据集，而且不确定这是否可行。那么有没有可以解决上述问题的计算机视觉方法呢？
]]></description>
      <guid>https://stackoverflow.com/questions/78830813/detect-and-localize-a-large-number-of-different-objects-in-an-image</guid>
      <pubDate>Sun, 04 Aug 2024 10:33:33 GMT</pubDate>
    </item>
    <item>
      <title>如何将风格化效果应用于视频？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78830209/how-can-i-apply-a-stylized-effect-to-a-video</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78830209/how-can-i-apply-a-stylized-effect-to-a-video</guid>
      <pubDate>Sun, 04 Aug 2024 03:36:28 GMT</pubDate>
    </item>
    <item>
      <title>通过 OpenCV 快速查找重复图像[关闭]</title>
      <link>https://stackoverflow.com/questions/78829582/fast-finding-of-the-duplicate-images-via-opencv</link>
      <description><![CDATA[我正在尝试通过 C# 添加一种在 MongoDB 中存储和查找重复图片的方法。我有一个应用程序，允许用户创建图像并将其保存在我的网站上。我将它们存储在 MongoDB 中，现在想查找重复项，但由于图像数量众多，我无法只用其他图像检查新图像。我希望有一些索引和标志来快速找到可能的重复项，然后完全检查它们。我使用 OpenCV 查找图片的描述符并存储它们，以便之后我可以轻松地用新图片检查它们。我可以使用什么作为标志或索引来不搜索整个数据库？有没有快速的方法？这是我的代码示例，它允许我找到描述符：
Mat img1 = Cv2.ImRead(&quot;somepic&quot;, ImreadModes.Grayscale); 
var orb = ORB.Create(); 
KeyPoint[] keyPoints; 
Mat descriptors = new Mat(); 
orb.DetectAndCompute(img1, null, out keyPoints, descriptors);

有一个通过描述符比较两幅图像的示例（左侧原始图像和右侧裁剪后的图像）。所以这种方法确实有效。但是如何在数百万张图片中快速做到这一点？
比较图像
我听说我可以将描述符分成 4-8 个部分并将它们用作索引，但无法保证我会找到可能的重复项。我也听说过 k-means，但我也不了解如何将其与描述符一起使用。或者也许还有其他没有机器学习的方法？
附言：我尝试过 PHash，但它对裁剪后的图片效果很糟糕。]]></description>
      <guid>https://stackoverflow.com/questions/78829582/fast-finding-of-the-duplicate-images-via-opencv</guid>
      <pubDate>Sat, 03 Aug 2024 19:14:23 GMT</pubDate>
    </item>
    <item>
      <title>在音频 AI 中实现迁移学习</title>
      <link>https://stackoverflow.com/questions/78828884/implementing-transfer-learning-in-audio-ai</link>
      <description><![CDATA[虽然我的任务是从音频中检测疾病，但我能否通过迁移一些预训练网络来实现 CNN 神经网络（以 MFCC 为输入）来进行语音处理？
我的意思是，虽然这两项任务彼此不同，但原则上这样做可以吗？
因为一般来说，我看到的迁移学习是当模型经过预训练时检测人，然后你进行微调以检测你想要的人]]></description>
      <guid>https://stackoverflow.com/questions/78828884/implementing-transfer-learning-in-audio-ai</guid>
      <pubDate>Sat, 03 Aug 2024 14:06:15 GMT</pubDate>
    </item>
    </channel>
</rss>