<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 11 Dec 2023 06:18:44 GMT</lastBuildDate>
    <item>
      <title>使用形状值分析模型时刻度标签中的字形错误</title>
      <link>https://stackoverflow.com/questions/77637695/glyph-errors-in-tick-labels-when-using-shap-values-to-analysis-my-model</link>
      <description><![CDATA[我正在 python 中使用 shap 包为我的模型重新创建一些图表。其中之一是瀑布图，来自 手册我按照使用以下代码生成的（完整代码太长，请查看手册）。
shap.waterfall_plot(shap_explainer_values[4652]) 
但是，我的图表的减号缺失，并出现警告消息“当前字体中缺少 Glyph 8722 (\N{MINUS SIGN})”。

stackoverflow上有很多与这个问题相关的问题，都可以通过来解决
plt.rcParams[&#39;axes.unicode_minus&#39;] = False

但是，我不能。有人可以帮助解决这个特定问题吗？非常感谢。
我也尝试了shutil.rmtree(matplotlib.get_cachedir())。]]></description>
      <guid>https://stackoverflow.com/questions/77637695/glyph-errors-in-tick-labels-when-using-shap-values-to-analysis-my-model</guid>
      <pubDate>Mon, 11 Dec 2023 05:54:34 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python AI/ML/DL 生成专业的头像图像应用程序？</title>
      <link>https://stackoverflow.com/questions/77637655/how-to-generating-a-professional-headshot-image-application-using-python-ai-ml-d</link>
      <description><![CDATA[我想使用 Python AI/ML 创建头像图像，因此有人建议我如何创建它的工作流程，并建议我使用哪种深度学习/机器学习模型
我希望上传简单的五张图像，并使用 AI/ML 输出专业头像图像。]]></description>
      <guid>https://stackoverflow.com/questions/77637655/how-to-generating-a-professional-headshot-image-application-using-python-ai-ml-d</guid>
      <pubDate>Mon, 11 Dec 2023 05:39:33 GMT</pubDate>
    </item>
    <item>
      <title>OpenCv 和 Numpy 无法兼容</title>
      <link>https://stackoverflow.com/questions/77637559/opencv-and-numpy-cannot-be-resloved</link>
      <description><![CDATA[我一直在Python中使用指纹匹配系统，并且我已经安装了所有库，但我无法导入这些库，我的pylance正在发送一条无法解析库的错误消息
在此处输入图像描述
我不想搞乱我迄今为止所做的工作]]></description>
      <guid>https://stackoverflow.com/questions/77637559/opencv-and-numpy-cannot-be-resloved</guid>
      <pubDate>Mon, 11 Dec 2023 05:03:30 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow 中组合图像和表格数据</title>
      <link>https://stackoverflow.com/questions/77637432/combining-image-and-tabular-data-in-tensorflow</link>
      <description><![CDATA[我一直在尝试将图像（胸部 X 光）和表格数据（年龄、性别、BMI 等）结合起来形成二元预测模型（疾病：0 或 1）。我有使用顺序的 2D-CNN，但在合并表格数据时遇到困难。我在网上探索了一些资源（例如 https://machinelearningmastery.com/keras-function -api-deep-learning/），但大多数都已经过时了 - 我还没有找到任何好的例子。 函数式 API 是最好的方法吗？您能否指导我如何针对下面的数据处理此问题？
导入tensorflow为tf
从tensorflow.keras.layers导入输入、Conv2D、MaxPooling2D、展平、密集、连接
从tensorflow.keras.models导入模型

label_encoder = LabelEncoder()
df[&#39;疾病&#39;] = label_encoder.fit_transform(df[&#39;疾病&#39;]).astype(str)

# 将 DataFrame 拆分为训练集和测试集
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df[&#39;疾病&#39;])

# 图像数据生成器
image_datagen = ImageDataGenerator（重新缩放=1/255）

# 训练图像生成器
train_image_generator = image_datagen.flow_from_dataframe(
    训练数据，
    x_col=&#39;文件名&#39;,
    y_col=&#39;疾病&#39;,
    目标大小=(224, 224),
    批量大小=32，
    class_mode=&#39;二进制&#39;,
    dtype=&#39;float32&#39;
）

# 测试图像生成器
test_image_generator = image_datagen.flow_from_dataframe(
    测试数据，
    x_col=&#39;文件名&#39;,
    y_col=&#39;疾病&#39;,
    目标大小=(224, 224),
    批量大小=32，
    class_mode=&#39;二进制&#39;,
    dtype=&#39;float32&#39;
）

# 将“性别”特征转换为数值
train_data[&#39;性别&#39;] = train_data[&#39;性别&#39;].map({&#39;F&#39;: 0, &#39;M&#39;: 1})
test_data[&#39;性别&#39;] = test_data[&#39;性别&#39;].map({&#39;F&#39;: 0, &#39;M&#39;: 1})

# 用于训练的表格特征
train_tabular_features = train_data[[&#39;年龄&#39;, &#39;性别&#39;, &#39;身高&#39;, &#39;体重&#39;]].values.astype(float)

# 用于测试的表格特征
test_tabular_features = test_data[[&#39;年龄&#39;, &#39;性别&#39;, &#39;身高&#39;, &#39;体重&#39;]].values.astype(float)

# 定义图像输入层
img_input = 输入(形状=(224, 224, 3), 名称=&#39;image_input&#39;)
x1 = Conv2D(16, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;)(img_input)
x1 = MaxPooling2D()(x1)
x1 = Conv2D(32, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;)(x1)
x1 = MaxPooling2D()(x1)
x1 = 展平()(x1)

# 定义表格输入层
tabular_input = 输入(形状=(4,), name=&#39;tabular_input&#39;)
x2 = 密集（16，激活=&#39;relu&#39;）（tabular_input）
x2 = 密集(32, 激活=&#39;relu&#39;)(x2)
x2 = 展平()(x2)

# 连接图像和表格分支的输出
连接=连接（[x1，x2]）

# 组合特征的公共层
x = 密集（128，激活=&#39;relu&#39;）（连接）
输出层=密集（1，激活=&#39;sigmoid&#39;，名称=&#39;输出&#39;）（x）

# 创建模型
模型 = 模型(输入=[img_input, tabular_input], 输出=output_layer)

# 编译模型
模型.编译(
    优化器=&#39;亚当&#39;,
    损失=&#39;binary_crossentropy&#39;,
    指标=[&#39;准确性&#39;]
）

历史=模型.fit(
    x={
        &#39;image_input&#39;：train_image_generator，
        &#39;表格输入&#39;：train_tabular_features
    },
    y=训练标签，
    纪元=20，
    验证数据=(
        {
            &#39;图像输入&#39;：测试图像生成器，
            &#39;tabular_input&#39;：test_tabular_features
        },
        测试标签
    ）
）


我在处理问题的方式中遇到了各种错误，主要是以下错误。我相信我的方法是错误的，因此寻找任何资源或指南。
&lt;小时/&gt;
ValueError Traceback（最近一次调用最后）
---&gt; 79 历史 = 模型.fit(
80 x={
81&#39;图像输入&#39;：train_image_generator，
82&#39;tabular_input&#39;：train_tabular_features
83}，
ValueError：无法找到可以处理输入的数据适配器：(包含{&quot;&quot;}键和{&quot;&quot;, &quot;&quot;} 值), ]]></description>
      <guid>https://stackoverflow.com/questions/77637432/combining-image-and-tabular-data-in-tensorflow</guid>
      <pubDate>Mon, 11 Dec 2023 04:05:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 CodeT5+ 或其他转换器根据方法的特征生成方法名称</title>
      <link>https://stackoverflow.com/questions/77637422/how-to-generate-method-name-by-methods-features-using-codet5-or-other-transfor</link>
      <description><![CDATA[如何使用 CodeT5+ 或其他转换器根据方法的功能生成方法名称？特征是方法的主体、访问修饰符、参数的类型和名称以及返回类型。我需要从一些 Java 代码中提取这些数据，并使用 CodeT5+ 或其他模型来预测方法的名称，然后将它们与真实的方法名称进行比较。您能给我提供解决此任务的 Python 代码吗？
我尝试了这段代码，但它产生了无意义的结果：
defgenerate_java_function_name（主体，修饰符，参数，return_type）：
    tokenizer = RobertaTokenizer.from_pretrained(&#39;Salesforce/codet5-small&#39;)
    模型 = T5ForConditionalGeneration.from_pretrained(&#39;Salesforce/codet5-small&#39;)

    input_text = f“{modifier.replace(&#39;,&#39;, &#39;&#39;)} {return_type} functionName({params}) {body}”

    input = tokenizer.encode(“函数名称：” + input_text, return_tensors=“pt”, max_length=512, truncation=True)
    输出= model.generate（输入，max_length = 50，num_beams = 5，length_penalty = 0.6，early_stopping = True）

    generated_function_name = tokenizer.decode(outputs[0],skip_special_tokens=True)
    返回生成的函数名称
]]></description>
      <guid>https://stackoverflow.com/questions/77637422/how-to-generate-method-name-by-methods-features-using-codet5-or-other-transfor</guid>
      <pubDate>Mon, 11 Dec 2023 04:01:24 GMT</pubDate>
    </item>
    <item>
      <title>针对我的 TSA 用例的自定义张量流 CNN 实现</title>
      <link>https://stackoverflow.com/questions/77637163/custom-tensorflow-cnn-implemetnation-for-my-tsa-use-case</link>
      <description><![CDATA[我想在时间序列数据集中找到某些特征和模式。我尝试过使用 HMM 聚类，但它没有产生好的结果。我想要实现类似于 RCNN 在 2D 情况下所做的事情，但在一维情况下。我可以在其中为模型提供示例，它会在序列中找到更多示例，并标记找到它的时间范围。给定一个时间序列序列 T，假设我想通过它运行 CNN，并且假设我想要对多个模式进行多类分类，每个模式可以发生多次。在我的具体实现中，我希望能够识别发生特定模式的时间帧 t，因此将卷积开始和结束的日期时间信息保留为特征图中的另一层将很有用。我想保留有关何时发现它的日期时间范围信息。我知道 python 不允许重载，但是我是否可以修改 conv1D 的代码以在特征映射中保留第二层并保留每个特征的时间戳，然后最终可能有一个 [start_time, end_time] 对于特定模式。因此，特征图将是一个 XY2 张量，并且在每次卷积操作之后，新图将包含 [oldest_start_time,newest_end_time] 的日期时间。到目前为止，我的训练是否有任何问题，我是否使用了错误的工具（也许是基于 RNN 的架构？）？在tensorflow中自定义实现通常是如何完成的，我需要编写自己的GD实现吗？
到目前为止还没有尝试过任何事情，只是处于想法阶段。尝试看看我是否使用了错误的工具，可能使用的是 RNN 类型架构。在我承诺编码之前，我需要确保我的想法是正确的。或者，我正在考虑首先进行多类分类，然后为每个命中的类运行自定义 1D-CNN 网络，然后 bin 搜索将大小缩小到时间范围内。]]></description>
      <guid>https://stackoverflow.com/questions/77637163/custom-tensorflow-cnn-implemetnation-for-my-tsa-use-case</guid>
      <pubDate>Mon, 11 Dec 2023 02:14:57 GMT</pubDate>
    </item>
    <item>
      <title>发生错误：需要 2D 数组，却得到 1D 数组：[关闭]</title>
      <link>https://stackoverflow.com/questions/77636896/an-error-occurred-expected-2d-array-got-1d-array-instead</link>
      <description><![CDATA[在这一行中：
stacked_predictions_2d = ensemble_predictions(X_test、X_train、y_test、y_train、rf_model、svm_model、gru_model、lstm_model、xgb_model、neural_network_model)`

我遇到这个问题：
发生错误：需要二维数组，却得到一维数组：array=[4.185 4.22 3.975 4.055 4.0475 4.015 4.0925 4.28 4.3375
4.4175
 4.46 4.1675 4.145 4.1525 4.0525 4.1375 4.1 4.05 4.2125 4.21
 4.1575 4.165 4.2925 4.23 4.3875 4.3975 4.2975 4.3125 4.29 4.44
 4.525 4.365 4.3125 4.3825 4.4725 4.3225 4.335 4.335 4.39 4.4075
 4.35 4.305 4.37 4.2625 4.29 4.22 4.2475 4.325 4.415 4.47
 4.5025 4.265 4.245 4.26 4.15 4.1675 4.11 4.15 4.0325 4.04
 4.1625 3.9275 3.885 3.6875 3.625 3.55 3.49 3.64 3.6 3.67]。
如果数据具有单个特征，则使用 array.reshape(-1, 1) 重塑数据；如果数据包含单个样本，则使用 array.reshape(1, -1) 重塑数据。

我不知道为什么会遇到这个问题，也不知道为什么它不告诉我问题出在哪个数组中。
X_TRAIN 的形状：(70, 17)
X_TEST 的形状：(30, 17)
Y_TRAIN 的形状：(70,)
Y_TEST 的形状：(30,)
累积特征的形状 (1, 19)

y 测试和训练应该是 1d，因为它们只包含样本。我不知道问题出在哪个数组中，但我确实知道这不是训练或测试，因为他们正在预测在获得此数据时价值 400 美元的股票的股价。
问题可能是什么以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/77636896/an-error-occurred-expected-2d-array-got-1d-array-instead</guid>
      <pubDate>Mon, 11 Dec 2023 00:03:35 GMT</pubDate>
    </item>
    <item>
      <title>在google colab上安装cuML时出现问题</title>
      <link>https://stackoverflow.com/questions/77636413/problem-when-installing-cuml-on-google-colab</link>
      <description><![CDATA[我在 google colab 上安装 RAPIDS 和 cuML 时遇到此错误：https://i。 stack.imgur.com/7Q12u.png
我按照此链接的说明进行操作：https://docs.rapids .ai/deployment/stable/platforms/colab/
我检查过，我已连接到 T4 GPU，并且我也尝试使用其他 GPU：
https://i.stack.imgur.com/BszUI.png
我的python版本似乎没问题，我使用的是python 3.10.12
我尝试使用--no-cache-dir选项，但它似乎没有改变任何东西。
我还尝试按照说明使用 conda 安装 RAPIDS，但在安装 cuml 时遇到了同样的问题]]></description>
      <guid>https://stackoverflow.com/questions/77636413/problem-when-installing-cuml-on-google-colab</guid>
      <pubDate>Sun, 10 Dec 2023 20:54:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit-learn 在没有预先存在标签的评论中进行情感分类的策略 [关闭]</title>
      <link>https://stackoverflow.com/questions/77635747/strategies-for-emotion-classification-in-comments-without-pre-existing-labels-us</link>
      <description><![CDATA[我正在开发一个项目，涉及使用 Python 和 scikit-learn 对文本评论中的情感进行分类。但是，我的数据集没有预先存在的情感标签。
我正在寻求有关在没有预先存在的标签的情况下进行情绪分类的可能策略的建议。如何定义情绪类别？我可以使用哪些类型的特征或文本表示方法来捕捉评论中的情感方面？
有任何代码示例、推荐的库或经过验证的方法的参考吗？]]></description>
      <guid>https://stackoverflow.com/questions/77635747/strategies-for-emotion-classification-in-comments-without-pre-existing-labels-us</guid>
      <pubDate>Sun, 10 Dec 2023 17:29:53 GMT</pubDate>
    </item>
    <item>
      <title>为什么tensorflow的AudioIOTensor的WAV文件的张量输出与decode_wav的输出不同？</title>
      <link>https://stackoverflow.com/questions/77628394/why-does-tensor-output-for-wav-file-from-tensorflows-audioiotensor-differ-from</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77628394/why-does-tensor-output-for-wav-file-from-tensorflows-audioiotensor-differ-from</guid>
      <pubDate>Fri, 08 Dec 2023 17:55:59 GMT</pubDate>
    </item>
    <item>
      <title>具有不同输入形状的 3D 深度学习输入 [关闭]</title>
      <link>https://stackoverflow.com/questions/77602918/3d-deep-learning-input-with-varying-input-shapes</link>
      <description><![CDATA[如何将可变维度的数据集输入到深度学习模型中。
我正在使用可变切片进行 3D 医学成像，我使用了 PCA 和其他切片选择技术以及填充以使模型具有相同的形状
但我想知道是否有任何用于深度学习模型的可变输入形状的技术。
下面是代码：
来自tensorflow.keras导入层

#输入形状 = (200, 200, 60, 1)
输入形状=像素数组[1:]
输入 = keras.Input(shape=(pixel_arrays.shape[1:]))
x=layers.Conv3D(filters=16,kernel_size=3,activation=&#39;relu&#39;,padding=&#39;same&#39;)(输入)
x=layers.Conv3D(filters=32,kernel_size=3,activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
x=layers.Conv3D(filters=64,kernel_size=3,activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
#x = 层.Conv3D(filters=32, kernel_size=3, 激活=&#39;relu&#39;, padding=&#39;same&#39;)(x)
]]></description>
      <guid>https://stackoverflow.com/questions/77602918/3d-deep-learning-input-with-varying-input-shapes</guid>
      <pubDate>Mon, 04 Dec 2023 22:33:53 GMT</pubDate>
    </item>
    <item>
      <title>如何在colab中查找数据集的列中有多少个不同的数据[关闭]</title>
      <link>https://stackoverflow.com/questions/77599408/how-to-find-how-many-different-data-are-in-a-column-of-a-data-set-in-colab</link>
      <description><![CDATA[我有一个大约由 400000 行和 8 列组成的数据集，我只想知道一列中有多少种不同类型的数据，我该怎么做？列中的数据是字符串的形式，我需要给它们分配数字，所以我需要找出该列中有多少个不同的单词。我不知道我应该做什么]]></description>
      <guid>https://stackoverflow.com/questions/77599408/how-to-find-how-many-different-data-are-in-a-column-of-a-data-set-in-colab</guid>
      <pubDate>Mon, 04 Dec 2023 12:22:17 GMT</pubDate>
    </item>
    <item>
      <title>如何修复我的感知器来识别数字？</title>
      <link>https://stackoverflow.com/questions/77594625/how-can-i-fix-my-perceptron-to-recognize-numbers</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77594625/how-can-i-fix-my-perceptron-to-recognize-numbers</guid>
      <pubDate>Sun, 03 Dec 2023 14:03:49 GMT</pubDate>
    </item>
    <item>
      <title>使用 tidymodels 进行特征消除以筛选多个模型</title>
      <link>https://stackoverflow.com/questions/72896969/feature-elimination-to-screen-for-multiple-models-using-tidymodels</link>
      <description><![CDATA[我目前正在执行回归建模，数据集的特征数量 (p) 高于观测值 (n)。
通常为 p = 10000 和 n = 30。此外，我想测试许多模型并找到最好的一个。
我现在要做的就是首先消除这些功能。将其从 10K 减少到 20-30，使用
step_select_mrmr() 或 step_select_vip()。我通过将其放在管道的顶部来实现这一点。
然后我会继续测试许多模型。
这种做法合理吗？]]></description>
      <guid>https://stackoverflow.com/questions/72896969/feature-elimination-to-screen-for-multiple-models-using-tidymodels</guid>
      <pubDate>Thu, 07 Jul 2022 11:23:56 GMT</pubDate>
    </item>
    <item>
      <title>Python：如何从 Optuna LightGBM 研究中检索最佳模型？</title>
      <link>https://stackoverflow.com/questions/62144904/python-how-to-retrieve-the-best-model-from-optuna-lightgbm-study</link>
      <description><![CDATA[我希望获得最佳模型，以便稍后在笔记本中使用，以使用不同的测试批次进行预测。
可重现的示例（取自 Optuna Github）：
导入 lightgbm 为 lgb
将 numpy 导入为 np
导入 sklearn.datasets
导入 sklearn.metrics
从 sklearn.model_selection 导入 train_test_split

导入奥图纳


# 仅供参考：目标函数可以接受额外的参数
#（https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args）。
定义目标（试用）：
    数据，目标 = sklearn.datasets.load_breast_cancer(return_X_y=True)
    train_x、valid_x、train_y、valid_y = train_test_split（数据、目标、test_size=0.25）
    dtrain = lgb.Dataset(train_x, label=train_y)
    dvalid = lgb.Dataset(valid_x, label=valid_y)

    参数 = {
        “目标”：“二进制”，
        “公制”：“auc”，
        “详细程度”：-1，
        &quot;boosting_type&quot;: &quot;gbdt&quot;,
        &quot;lambda_l1&quot;: Trial.suggest_loguniform(&quot;lambda_l1&quot;, 1e-8, 10.0),
        &quot;lambda_l2&quot;: Trial.suggest_loguniform(&quot;lambda_l2&quot;, 1e-8, 10.0),
        &quot;num_leaves&quot;: Trial.suggest_int(&quot;num_leaves&quot;, 2, 256),
        &quot;feature_fraction&quot;: Trial.suggest_uniform(&quot;feature_fraction&quot;, 0.4, 1.0),
        &quot;bagging_fraction&quot;: Trial.suggest_uniform(&quot;bagging_fraction&quot;, 0.4, 1.0),
        &quot;bagging_freq&quot;: Trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),
        &quot;min_child_samples&quot;: Trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),
    }

    # 添加用于修剪的回调。
    pruning_callback = optuna.integration.LightGBMPruningCallback（试用版，“auc”）
    gbm = lgb.train(
        参数，dtrain，valid_sets = [dvalid]，verbose_eval = False，callbacks = [pruning_callback]
    ）

    preds = gbm.predict(valid_x)
    pred_labels = np.rint(preds)
    准确度 = sklearn.metrics.accuracy_score(valid_y, pred_labels)
    返回精度


我的理解是，下面的研究将调整准确性。我想以某种方式从研究中检索最佳模型（不仅仅是参数）而不将其保存为泡菜，我只想在笔记本中的其他地方使用该模型。 

&lt;前&gt;&lt;代码&gt;
如果 __name__ == &quot;__main__&quot;:
    研究 = optuna.create_study(
        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)，方向=“最大化”
    ）
    研究.优化（目标，n_Trials=100）

    print(&quot;最佳试用：&quot;)
    试验 = 研究.best_试验

    print(&quot; 参数: &quot;)
    对于 Trial.params.items() 中的键、值：
        print(&quot; {}: {}&quot;.format(key, value))


期望的输出是
best_model = ~上面的模型~
new_target_pred = best_model.predict(new_data_test)
指标.accuracy_score(new_target_test, new__target_pred)

]]></description>
      <guid>https://stackoverflow.com/questions/62144904/python-how-to-retrieve-the-best-model-from-optuna-lightgbm-study</guid>
      <pubDate>Tue, 02 Jun 2020 04:35:05 GMT</pubDate>
    </item>
    </channel>
</rss>