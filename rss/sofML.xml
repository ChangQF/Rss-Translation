<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 28 Nov 2024 06:26:40 GMT</lastBuildDate>
    <item>
      <title>我想训练一个简单的人工智能模型用于实践。它可以非常简单。对我来说，最好的操作步骤是什么？</title>
      <link>https://stackoverflow.com/questions/79232599/i-want-to-train-a-simple-ai-model-for-practise-purposes-it-can-be-very-simple</link>
      <description><![CDATA[我是一名大学生，我想训练一个简单的人工智能模型，只是为了好玩。它可以像绘制几个点并估计与图相对应的函数一样简单。
我有编程和计算机硬件的知识基础。我只是对构建一个简单的人工智能模型的步骤感到困惑，以便它可以训练输入数据并输出它所学到的东西。我听说Python是人工智能的语言。设置Python环境后我应该做什么？是否有某个网站上的AI模板可供我使用？我在Youtube上搜索过，但没有很多关于从头开始训练人工智能模型的有用信息。如果您能给我一些建议，我将不胜感激。
我想学习构建一个简单的人工智能模型的过程，该模型可以通过输入数据进行训练并输出一些东西。]]></description>
      <guid>https://stackoverflow.com/questions/79232599/i-want-to-train-a-simple-ai-model-for-practise-purposes-it-can-be-very-simple</guid>
      <pubDate>Thu, 28 Nov 2024 04:32:11 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中的 GaussianProcessRegressor 估计器可以在多核上并行化吗？</title>
      <link>https://stackoverflow.com/questions/79232519/is-the-gaussianprocessregressor-estimator-in-scikit-learn-able-to-be-parallelize</link>
      <description><![CDATA[在具有 8 个内核（16 个线程）的机器上使用 GaussianProcessRegressor 时，我没有注意到任何性能改进，尽管我只使用物理内核。所以我想知道，sklearn.gaussian_process 中的 GaussianProcessRegressor 类是否能够利用多个处理器/内核/线程？
#当前场景
4 个内核情况下的时间：0.57
8 个内核情况下的时间：0.56
加速不明显。这次只是将 fit_transform 作用于数据块。因此没有计时开销。]]></description>
      <guid>https://stackoverflow.com/questions/79232519/is-the-gaussianprocessregressor-estimator-in-scikit-learn-able-to-be-parallelize</guid>
      <pubDate>Thu, 28 Nov 2024 03:36:56 GMT</pubDate>
    </item>
    <item>
      <title>通过建模预测需求</title>
      <link>https://stackoverflow.com/questions/79232517/demand-prediction-through-modeling</link>
      <description><![CDATA[在此处输入图片说明我的公司正在开展一个按客户预测需求的项目。客户分为 6 个行业组，我们有按客户划分的过去需求数据。当我按行业组检查模式时，结果如下，由于确定了特定模式，我对数据进行了预处理（时间序列分解），似乎可以看到特定的趋势和周期。由于残差不是随机的，我认为我需要添加解释变量，但即使我添加变量，我也不认为我可以使其完全随机。因为它是时间序列数据，并且需求模式的特征因行业组而异，所以我想按行业组进行拆分并应用每个模型。我正在考虑将 SARIMAX 作为候选模型，
问题是：

为什么 LSTM 不适合这种情况
我应该如何预处理和建模？
我还应该考虑哪些其他因素？

2 年的需求模式和时间序列分解（趋势、季节性）
在此处输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/79232517/demand-prediction-through-modeling</guid>
      <pubDate>Thu, 28 Nov 2024 03:35:48 GMT</pubDate>
    </item>
    <item>
      <title>为什么有些模型架构使用加法运算符而不是减法，反之亦然？</title>
      <link>https://stackoverflow.com/questions/79232424/why-do-some-model-architectures-use-the-addition-operator-instead-of-subtraction</link>
      <description><![CDATA[为什么有些模型架构使用加法运算符而不是减法运算符，反之亦然？例如，在 ResNet 中，更改运算符是否会影响模型（F(x) + x -&gt; F(x) - x）？模型是否只需通过翻转符号就可以轻松学习？]]></description>
      <guid>https://stackoverflow.com/questions/79232424/why-do-some-model-architectures-use-the-addition-operator-instead-of-subtraction</guid>
      <pubDate>Thu, 28 Nov 2024 02:39:18 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Hugging Face Trainer 或 SFT Trainer 中记录第零步的训练损失？</title>
      <link>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</link>
      <description><![CDATA[我正在使用 Hugging Face Trainer（或 SFTTrainer）进行微调，我想在步骤 0（在执行任何训练步骤之前）记录训练损失。我知道有一个用于评估的 eval_on_start 选项，但我找不到在训练开始时记录训练损失的直接等效方法。
是否有办法使用 Trainer 或 SFTTrainer 在步骤 0（在任何更新之前）记录初始训练损失？理想情况下，我希望使用类似于 eval_on_start 的方法。
以下是我迄今为止尝试过的方法：
解决方案 1：自定义回调
我实现了自定义回调，以在训练开始时记录训练损失：
from transformers import TrainerCallback

class TrainOnStartCallback(TrainerCallback):
def on_train_begin(self, args, state, control, logs=None, **kwargs):
# 在步骤 0 记录训练损失
logs = logs or {}
logs[&quot;train/loss&quot;] = None # 如果可用，用初始值替换 None
logs[&quot;train/global_step&quot;] = 0
self.log(logs)

def log(self, logs):
print(f&quot;Logging at start: {logs}&quot;)
wandb.log(logs)

# 将回调添加到 Trainer
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
train_dataset=train_dataset,
eval_dataset=eval_dataset,
args=training_args,
optimizers=(optimizer, scheduler),
callbacks=[TrainOnStartCallback()],
)

这有效，但感觉有点过头了。它会在训练开始时记录任何步骤之前的指标。
解决方案 2：手动记录
或者，我在开始训练之前手动记录训练损失：
wandb.log({&quot;train/loss&quot;: None, &quot;train/global_step&quot;: 0})
trainer.train()

问题：
Trainer 或 SFTTrainer 中是否有任何内置功能可以在第 0 步记录训练损失？或者自定义回调或手动记录是这里的最佳解决方案吗？如果是这样，是否有更好的方法来实现此功能？与 eval_on_start 类似，但 train_on_start？
交叉：https://discuss.huggingface.co/t/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer/128188]]></description>
      <guid>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</guid>
      <pubDate>Thu, 28 Nov 2024 00:23:35 GMT</pubDate>
    </item>
    <item>
      <title>如何摆脱 Unity ML 中找不到类型或命名空间名称“Keypoint”的错误</title>
      <link>https://stackoverflow.com/questions/79231502/how-to-get-rid-of-the-type-or-namespace-name-keypoint-could-not-be-found-error</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79231502/how-to-get-rid-of-the-type-or-namespace-name-keypoint-could-not-be-found-error</guid>
      <pubDate>Wed, 27 Nov 2024 18:36:11 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么 ML/优化算法来找到最大输出的最佳输入值？[关闭]</title>
      <link>https://stackoverflow.com/questions/79229880/what-ml-optimization-algorithm-should-i-use-to-find-the-optimum-input-values-for</link>
      <description><![CDATA[我有一些数据，需要找到 X*Y 的最佳组合，以便因变量 A、B、C 达到最大值。
所有这些都是简单的数值，就像科学实验的观察结果一样。因此，A、B、C 是在同一范围内具有不同值的性能变量，而 X 和 Y 是两个测量变量。
基本上，我想制作一个程序来不断处理未来实验中涌入的任何数据。
我遇到过有人使用 ANN 回归来解决类似的问题，还有人建议使用梯度下降。由于我必须从头开始完成这项任务，如果我能建议我应该从哪种算法开始，我将非常高兴。]]></description>
      <guid>https://stackoverflow.com/questions/79229880/what-ml-optimization-algorithm-should-i-use-to-find-the-optimum-input-values-for</guid>
      <pubDate>Wed, 27 Nov 2024 10:35:42 GMT</pubDate>
    </item>
    <item>
      <title>Python 和 Maple SVR 结果中的差异 [关闭]</title>
      <link>https://stackoverflow.com/questions/79229459/discrepancy-in-python-and-maple-svr-results</link>
      <description><![CDATA[我正在使用最小二乘 SVR 方法求解积分方程。我应该将下面链接中提供的 Maple 代码转换为 Python。我已编写如下所示的 Python 代码，但无论我做什么，都无法获得准确的结果。您能告诉我如何让我的代码产生与 Maple 相同的结果吗？
https://github.com/alirezaafzalaghaei/LSSVR-FIE/blob/master/paper-examples/example-4/CLS-SVR-dual.mw
import numpy as np
from scipy.integrate import quad
from scipy.special import legendre
from scipy.optimize import minimal
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from scipy.stats import qmc
from scipy.integrate import quad
from scipy.integrate import dblquad
import time
from tensorflow.keras import regularizers
from tensorflow.keras.losses import MeanSquaredError

# 设置精度
np.set_printoptions(precision=15)

# 定义积分函数
def Quad(f, a, b):
# 注意：这是一个简化版本，您可能需要根据要使用的具体求积方法进行调整。
return dblquad(f, a, b,a,b)[0]

# 定义区间
a, b = 0, 1

# 定义精确函数
def exact(x,y):
return 1/((x+y+1)**2)

# 定义函数 f
def f(x,y):
return 1/((x+y+1)**2)-(x/(6*(8+y)))

# 定义函数 k
def k(x, y,t,s):
return (x/((8+y)*(1+t+s)))

# 示例用法（替换为您的具体用例）
result_quad = Quad(f, a, b)
print(result_quad)
import numpy as np
from scipy.special import legendre
from scipy.optimize import fsolve

def shift(x):
return (2 * x - a - b) / (b - a)

gamma = 10 ** 8
M = 4
N = M + 1

a, b = 0, 1 # 定义 a 和 b

# 计算勒让德多项式的根
train1 = fsolve(lambda x: legendre(N)(shift(x)), np.linspace(0, 1, N))
train2 = fsolve(lambda y: legendre(N)(shift(y)), np.linspace(0, 1, N))

# 使用 NumPy 的 meshgrid 和 stacking 创建 N x 2 矩阵
X, Y = np.meshgrid(train1, train2)
train = np.stack((X.flatten(), Y.flatten()), axis=1)

print(train)
print(train.shape) # 输出形状以确认
phi = []
L_phi = []
for kindx in range((M + 1) * (M + 1)):
i = kindx // (M + 1) + 1
j = kindx % (M + 1) + 1
#print(i,j)
# 创建 i 次勒让德多项式
p = legendre(i)
# 使用嵌套函数定义 phi[i] 以创建新范围
def make_phi(p=p): # 在嵌套函数的参数中捕获 p
return lambda x: p(shift(x))
phi.append(make_phi())
L_phi.append(lambda x,y: phi[i](x)*phi[j](y) - dblquad(lambda t,s: k(x,y, t,s) 
* phi[i](t)*phi[j](s), a, b,a,b)[0])

train = np.array([train1, train2]).T #假设 train1 和 train2 是列表或 
数组
A = np.zeros((M+1, M+1))
for m in range(M+1):
for n in range(M+1):
A[m, n] = L_phi[m](train[n, 0], train[n, 1])
print(A)
Omega = np.dot(A.T, A) + np.identity(M+1) / gamma
GAMMA = np.array([f(train1[i],train2[i]) for i in range(M+1)]).reshape(M+1, 1)
alpha = resolve(Omega, GAMMA)
import numpy as np
l=[]
import numpy as np
from scipy.linalg import resolve

# ... (其他导入和函数) ...

def u_tilde(x, y):

# 仅对 x 和 y 处 phi 中的前 N ​​个勒让德多项式进行求值
# 这与 A 的维度一致
P_x = np.array([phi[i](x) for i in range(M+1)]).reshape(-1, 1) # 此处更改
P_y = np.array([phi[i](y) for i in range(M+1)]).reshape(-1, 1) # 此处更改

# 使用逐元素乘法和求和计算 u˜(x)
u_tilde_x = alpha.T @ A.T @ (P_x * P_y) # 此处更改

return u_tilde_x[0, 0] # 从结果中提取标量值

# ...（其余代码）...# 从结果中提取标量值

return result
# 示例用法：
x_value = train1
y_value = train2
for k in range(M+1):
u_tilde_at_x = u_tilde(x_value[k],y_value[k])
l.append(u_tilde_at_x)
print(f&quot;u˜({x_value[k]}) = {u_tilde_at_x}&quot;)
#u_tilde_at_x = u_tilde(x_value)
#print(f&quot;u˜({x_value}) = {u_tilde_at_x}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79229459/discrepancy-in-python-and-maple-svr-results</guid>
      <pubDate>Wed, 27 Nov 2024 08:30:38 GMT</pubDate>
    </item>
    <item>
      <title>寻找洞察聚类机器学习项目[关闭]</title>
      <link>https://stackoverflow.com/questions/79228750/looking-for-insights-clustering-machine-leaning-prohect</link>
      <description><![CDATA[我正在为高中开展一个涉及聚类（k 均值和 DBSCAN）的机器学习项目
我抓取了一个电子商务网站 (StockX)，并对某些商品（包括设计师品牌等）的零售价值 (x) 和转售价值 (y) 进行聚类。然后对它们进行聚类。
最终的聚类结果非常基础，有 3 个聚类 - 围绕低零售/转售、中等零售/转售和高零售/转售价格。
我想知道你们是否对我可以用数据和我的项目做的更细微的事情有什么建议。如果有更多有趣的发现，我可以尝试挖掘出来。]]></description>
      <guid>https://stackoverflow.com/questions/79228750/looking-for-insights-clustering-machine-leaning-prohect</guid>
      <pubDate>Wed, 27 Nov 2024 01:58:21 GMT</pubDate>
    </item>
    <item>
      <title>大型多 GPU ML 训练作业的 GPU 间流量 [关闭]</title>
      <link>https://stackoverflow.com/questions/79228728/inter-gpu-traffic-for-large-multi-gpu-ml-training-jobs</link>
      <description><![CDATA[对于具有不同并行类型（如数据、张量、管道等）的分布式多 GPU 大型机器学习作业，我正在寻找点对点、全对全、全归约等 GPU 间流量的模式和百分比。是否有关于此的研究/数据？
大多数研究都讨论数据并行，其中全归约类型的流量占分配梯度的大多数。]]></description>
      <guid>https://stackoverflow.com/questions/79228728/inter-gpu-traffic-for-large-multi-gpu-ml-training-jobs</guid>
      <pubDate>Wed, 27 Nov 2024 01:44:11 GMT</pubDate>
    </item>
    <item>
      <title>Keras 神经网络回归模型优先考虑 2 个输出值，如何才能让它更好地概括？[关闭]</title>
      <link>https://stackoverflow.com/questions/79228286/keras-neural-network-regression-model-prioritizes-2-output-values-over-the-rest</link>
      <description><![CDATA[我正在尝试使用其他特征预测附加数据集中的“温度”值。执行代码时，模型对两个值有明显的偏差。我正在使用 plotly 图显示预测的准确性，其中包含真实值和预测值以及表示最佳预测的线。我的预测准确性
因此，如您所见，有两个主要的信息集群，表明我的模型主要选择这两个值作为输出。我不知道为什么会发生这种情况，也不知道我可以做些什么来补救。我将链接我正在使用的 .csv 文件和代码（google Drive / Colab）
此外，当删除预处理步骤时，它会产生类似的效果，但有三个主要集群而不是两个。 https://drive.google.com/drive/folders/1uSHTVAmW-UXhutZa5-Tf1QcB_e9cl_DR?usp=sharing
我尝试过：

删除预处理步骤。
添加特征工程和通过相关性进行数据选择。
排除分类值。
我已经试验了神经网络的大小和密度，增加或减少以查看它是否是欠拟合问题。
我已经引入了最多 100 次试验的自动超参数选择。
我在神经网络中添加了批量和特征规范化。
我已经手动调整了超参数的值，例如时期、激活函数等。
我使用了 KMeans 来降低密度。
我尝试了不同的数据分割。

我预计分布会略有变化，但它总是水平聚集（与预测值一起）
总之，这是我的神经网络还是预处理的问题？]]></description>
      <guid>https://stackoverflow.com/questions/79228286/keras-neural-network-regression-model-prioritizes-2-output-values-over-the-rest</guid>
      <pubDate>Tue, 26 Nov 2024 21:09:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么（远程） Jupyter 在 ML 训练期间很忙，但实际上却没有做任何事情？</title>
      <link>https://stackoverflow.com/questions/79226995/why-is-remote-jupyter-busy-during-ml-training-but-not-actually-doing-anything</link>
      <description><![CDATA[我正在使用 PyTorch 在自己的专用远程服务器上训练 ML 模型，使用 Jupyter 作为我的 IDE。
大约 120 个 epoch（训练大约 2 小时），Jupyter 单元停止更新输出，但状态栏仍显示内核状态为 busy，SSH 连接仍处于活动状态。
我认为训练可能仍在继续，但输出单元停止更新，因为它包含太多输出。为了验证这个假设，我昨晚让 Jupyter 运行了大约 7 个小时。当我醒来时，它已经在 123 个 epoch 时停止更新输出单元，当我终止执行并打印出当前 epoch 数时，它只达到了 126 个 epoch。
知道是什么原因造成的吗？]]></description>
      <guid>https://stackoverflow.com/questions/79226995/why-is-remote-jupyter-busy-during-ml-training-but-not-actually-doing-anything</guid>
      <pubDate>Tue, 26 Nov 2024 13:55:11 GMT</pubDate>
    </item>
    <item>
      <title>将请求上下文从 FastAPI 传递到用于 OpenAI 集成的 Microsoft Semantic Kernel 插件</title>
      <link>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</link>
      <description><![CDATA[我正在 FastAPI 应用程序中将 Microsoft Semantic Kernel 与 OpenAI 集成。我有一个聊天/端点，我从请求中收到一个 session_id，我需要将此 session_id 与 openai_client 一起传递给插件。但是，我不确定如何在内核的执行过程中将 FastAPI 请求中的 session_id 正确传递给插件。
以下是设置内核和插件的相关代码：
# 内核和服务设置
kernel = Kernel()

execution_settings = AzureChatPromptExecutionSettings(tool_choice=&quot;auto&quot;)
execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={})

openai_client = OpenAI(api_key=api_key)
chat_completion_service = OpenAIChatCompletion(
ai_model_id=model_id, 
api_key=api_key, 
service_id=service_id 
)

# 添加服务和插件
kernel.add_service(chat_completion_service)
kernel.add_plugin(MovesPlugin(openai_client), plugin_name=&#39;MovesPlugin&#39;)

在我的 FastAPI 端点内，我想在调用内核进行聊天响应时将 session_id 传递给插件：
# 在 FastAPI 端点内
@app.post(&quot;/chat/&quot;)
async def chat(request: Request):
session_id = await request.json().get(&#39;session_id&#39;)

# 获取聊天完成服务
_chat_completion_service = kernel.get_service(type=ChatCompletionClientBase)

# 获取聊天完成响应
response = await _chat_completion_service.get_chat_message_content(
chat_history=chat_history,
kernel=kernel,
settings=execution_settings
)

return响应

如何将请求上下文 (session_id) 从 FastAPI 请求传递到 MovesPlugin，并确保它与语义内核执行中的 openai_client 一起正确使用？
如能得到任何指导或建议，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</guid>
      <pubDate>Tue, 26 Nov 2024 12:51:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么基于 Tensorflow.js 的天气预测模型无法预测正确的天气</title>
      <link>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</guid>
      <pubDate>Sun, 26 May 2024 18:36:09 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Windows 机器上安装 Rasa</title>
      <link>https://stackoverflow.com/questions/78483192/unable-to-install-rasa-in-windows-machine</link>
      <description><![CDATA[我尝试在我的 Windows 10 笔记本电脑上使用命令 pip install rasa 安装 Rasa。
我安装了 Python 3.11 版。
但是我收到以下错误：
获取构建 wheel 的要求未成功运行。
│ 退出代码：1
╰─&gt; [20 行输出]
回溯（最近一次调用最后一次）：
文件“C:\python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，行 
353，在&lt;module&gt; 中
main()
文件“C:\python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，第 335 行，
在 main 中 
json_out[&#39;return_val&#39;] = hook(**hook_input[&#39;kwargs&#39;])
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

我遗漏了什么？之前，我尝试安装 chatterbot 模块，也遇到了类似的错误。
此外，我也尝试使用 pip install chatterbot，但仍然出现错误。]]></description>
      <guid>https://stackoverflow.com/questions/78483192/unable-to-install-rasa-in-windows-machine</guid>
      <pubDate>Wed, 15 May 2024 10:10:42 GMT</pubDate>
    </item>
    </channel>
</rss>