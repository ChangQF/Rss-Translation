<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 01 Apr 2024 12:26:10 GMT</lastBuildDate>
    <item>
      <title>我正在使用 GAN 制作一个关于 DR 检测的项目，并尝试运行“models_from_json”，但它不起作用</title>
      <link>https://stackoverflow.com/questions/78255418/i-am-making-a-project-on-dr-detection-using-gan-and-trying-to-run-models-from-j</link>
      <description><![CDATA[所以在我的代码中，我尝试基本上从 json 中获取现有模型，但它不起作用
从 keras.models 导入顺序
从 keras.layers 导入 Convolution2D
从 keras.layers 导入 MaxPooling2D
从 keras.layers 导入扁平化
从 keras.layers 导入密集、激活、BatchNormalization
**从 keras.models 导入 model_from_json**


with open(&#39;model/train.json&#39;, &quot;r&quot;) 作为 json_file:
    load_model_json = json_file.read()
    加载模型 = model_from_json(加载模型_json)

loaded_model.load_weights(“模型/train.h5”)
loaded_model._make_predict_function()
打印（loaded_model.summary（））

我收到此错误：-
 with open(&#39;model/train.json&#39;, &quot;r&quot;) 作为 json_file：
FileNotFoundError：[Errno 2]没有这样的文件或目录：&#39;model/train.json&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/78255418/i-am-making-a-project-on-dr-detection-using-gan-and-trying-to-run-models-from-j</guid>
      <pubDate>Mon, 01 Apr 2024 12:16:19 GMT</pubDate>
    </item>
    <item>
      <title>train_test_split 机器学习中的 random_state</title>
      <link>https://stackoverflow.com/questions/78255303/random-state-in-train-test-split-machine-learning</link>
      <description><![CDATA[我看到了一个视频，他使用下面的循环来找到最佳的 random_state：
&#39;
#找到具有最高分数的TrainTestSplit随机状态的模型
#使用后移除
分数=[]
对于范围（1000）内的 i：
X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=i)
lr=线性回归()
管道=make_pipeline(column_tran,lr)
管道.fit(X_train,y_train)
y_pred=pipe.predict(X_test)
分数.append(r2_score(y_test,y_pred))
print(&#39;最高分由 i=&#39;,np.argmax(scores),&#39;得分为 &#39;,scores[np.argmax(scores)])
&#39;
我在网上看到的任何其他项目都没有这样做，我不确定是否找到最佳随机状态。
在使用它之前，我的 R2_SCORE 是 69.09，使用它之后，我的 R2_SCORE 达到了 89.1]]></description>
      <guid>https://stackoverflow.com/questions/78255303/random-state-in-train-test-split-machine-learning</guid>
      <pubDate>Mon, 01 Apr 2024 11:51:34 GMT</pubDate>
    </item>
    <item>
      <title>随机森林项目</title>
      <link>https://stackoverflow.com/questions/78255277/randomforest-project</link>
      <description><![CDATA[我对机器学习非常陌生，这是我作为大学课程的一部分正在从事的第一个项目。我选择了英国足球比赛。我选择使用随机森林。
使用不同的来源，我成功地获得了 20 年的上述比赛数据，清理了数据并构建了我的模型。
但是，我被困住了。我如何真正让模型对未来的比赛进行预测？
谢谢
我尝试加载模型，然后使用仅填充“日期”、“Home_Team”和“Away_Team”列的 CSV 文件，将其他列留空，以便模型预测这些值 - 这是执行此操作的正确方法吗？ 
更新：
谢谢 - 请参阅用于构建模型的代码；
从 sklearn.ensemble 导入 RandomForestClassifier
train = matches[matches[“日期”] &lt; &#39;2012-06-01&#39;]
测试=匹配[匹配[“日期”]&gt; &#39;2012-06-01&#39;]
预测器 = [&#39;Home_Team&#39;、&#39;Away_Team&#39;、&#39;HT_Winner&#39;、&#39;FT_Winner&#39;、&#39;match_result&#39;、&#39;ht_match_result&#39;、&#39;HomeShots&#39;、&#39;AwayShots&#39;、&#39;HomeCorners&#39;、&#39;AwayCorners&#39;]
rf.fit(train[预测变量], train[“FT_Winner”])
preds = rf.predict(测试[预测变量])

用于未来预测的新 CSV：
导入 pandas 作为 pd

new_data_df = pd.read_csv(..)
预测 = model.predict(new_data_df)
]]></description>
      <guid>https://stackoverflow.com/questions/78255277/randomforest-project</guid>
      <pubDate>Mon, 01 Apr 2024 11:46:02 GMT</pubDate>
    </item>
    <item>
      <title>C++ 如何创建带有游戏训练的神经网络？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78255012/c-how-to-create-a-neural-network-with-game-training</link>
      <description><![CDATA[互联网上关于如何用 C++ 创建神经网络的信息很少。我对 C++ 基础非常熟悉。我的目标是用 C++ 创建神经网络，以便它们可以玩特定的游戏。例如，您可以从使用井字游戏开始，然后是战舰、国际象棋，以及使用更复杂的游戏，如大富翁和其他游戏。
我使用了来自互联网、YouTube 来源观看视频的更多信息。我了解了不同类型的神经网络，其中一些对某些动作以分数形式给予奖励。
我了解 CNN 模型、RNN、GAN 等架构。我知道LSTM、GRU是用来存储上下文的，充当内存。权重在神经元中用于某种目的，各种激活函数有 ReLu、sigmoid、tanh。
我还想为需要 2 名以上玩家的游戏创建竞争性神经网络，例如大富翁和其他游戏。将代码从 Python 翻译为 C++ 并不是最好的解决方案。
更新：
我需要有关如何制作能够玩游戏以及制作竞争性神经网络的神经网络的信息。问题是如何在 C++ 中实现这一点。我刚刚用 JavaScript 编写了一个更简单的神经网络。
更新：
我问 OpenAI 的 ChatGPT 如何完成这个任务，他回答说：
要为井字游戏创建这样的神经网络，您需要编写 291 个“if else”块来记住游戏所有动作的模式。

实现国际象棋神经网络不可能做到这一点，因为国际象棋有超过 20 万个对手和你的走法模板。

我用 C++ 编写了一些代码。
#include ;
#include &lt;向量&gt;
#include &lt;字符串&gt;
#include &lt;功能&gt;

枚举类型激活{
    正弦，
    雷鲁，
    乙状结肠
};

类实用程序{
    模板&lt;类型名称 T&gt;
    bool static every(const std::vector&gt;&amp;mtx, const std::function cb) {
        for (const auto&amp; row : mtx) {
            for (const auto&amp; i : row) {
               if (cb(i) == 0) 返回 false；
            }
        }
        
       返回真；
    }
};

类代理{
民众：
    代理（）=默认；
 
私人的：
    浮重；
    类型激活激活；
    浮动平均值； // 消除？
};

类游戏{
民众：
    游戏（）=默认；
    〜游戏（）{}；
    
    bool walk(int x, int y, std::string 值) {
        if (x &gt;= 1 &amp;&amp; x &lt;= 3) {
            if (_mtx[x][y] == &quot; &quot;) {
                _mtx[x][y] = 值；
                返回真；
            }
        }
        
        返回假；
    }
    
    布尔 isEmpty() {
        auto = utils::every(_mtx, [](const std::string&amp; str) {
            返回 str == ” ”;
        });
        
        返回的是；
    }
    
私人的：
    std::vector&gt;&gt; _mtx = {
        {” ”、“ ”、“ ”},
        {” ”、“ ”、“ ”},
        {” ”、“ ”、“ ”}
    };
};

int main() {
    游戏游戏；
    代理代理；
    
    
    返回0；
}
]]></description>
      <guid>https://stackoverflow.com/questions/78255012/c-how-to-create-a-neural-network-with-game-training</guid>
      <pubDate>Mon, 01 Apr 2024 10:47:09 GMT</pubDate>
    </item>
    <item>
      <title>Adagrad/Rmsprop/Adam 关于变化方向的困惑</title>
      <link>https://stackoverflow.com/questions/78254992/confusion-about-adagrad-rmsprop-adam-about-the-direction-of-change</link>
      <description><![CDATA[你好，我现在正在学习优化器，
我可以理解动量部分（类似于物理世界），
但对不同参数的不同学习率感到困惑，
对于 Adagrad/Rmsprop，如果 ∂L/∂w_1 很大，则学习率
因为w_1很小，如果∂L/∂w_1很小，那么学习率
因为 w_1 很大。（如何使用 Latex 编码？）
但从数学上讲，-梯度是最陡的方向
值减小，对于 Adagrad/Rmsprop，它本质上改变了这一点
方向变为其他方向，本质上改变了更新方向
更多偏导数较小（如果∂L/∂w_1很小）
这是为什么呢？我的解释是，由于 Adagrad/Rmsprop 本质上改变了更新方向
更倾向于那些偏导数较小的情况，比如 w_1（如果 ∂L/∂w_1 很小），
等于在 -gradient 方向迈出一步，然后在 w_1 方向迈出额外一步，因为 w_1 方向更平坦，因此在 w_1 方向迈出额外一步的风险较小？]]></description>
      <guid>https://stackoverflow.com/questions/78254992/confusion-about-adagrad-rmsprop-adam-about-the-direction-of-change</guid>
      <pubDate>Mon, 01 Apr 2024 10:43:46 GMT</pubDate>
    </item>
    <item>
      <title>如何修复错误：索引错误：标量变量的索引无效</title>
      <link>https://stackoverflow.com/questions/78254954/how-to-fix-error-index-error-invalid-index-to-scalar-variable</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78254954/how-to-fix-error-index-error-invalid-index-to-scalar-variable</guid>
      <pubDate>Mon, 01 Apr 2024 10:35:50 GMT</pubDate>
    </item>
    <item>
      <title>我只想在 python 中从该图像中提取图形部分，我该怎么做？</title>
      <link>https://stackoverflow.com/questions/78254412/i-want-to-extract-only-the-graph-parts-from-this-image-in-python-how-do-i-go-abo</link>
      <description><![CDATA[我想将右侧的两个图一起提取，也不能单独提取，两者都可以提取为一个（https://i.stack.imgur.com/RqwkB.jpg)
我不知道该尝试什么，我对此很陌生。我正在使用 python，我从图像中提取文本并将其保存在 csv 中
&lt;前&gt;&lt;代码&gt;导入cv2
将 numpy 导入为 np

# 加载图像
image_path = r&#39;C:\Prarthana\PROJECTS\GitHub\MajorProject\Images\1.jpg&#39;
图像 = cv2.imread(image_path)

# 将图像转换为灰度图
灰色 = cv2.cvtColor(图像, cv2.COLOR_BGR2GRAY)

# 应用高斯模糊来减少噪音
模糊 = cv2.GaussianBlur(灰色, (5, 5), 0)

# 应用 Canny 边缘检测
边缘 = cv2.Canny(模糊, 50, 150)

# 在边缘检测图像中查找轮廓
轮廓，_ = cv2.findContours（边缘，cv2.RETR_EXTERNAL，cv2.CHAIN_APPROX_SIMPLE）

# 根据面积过滤轮廓，找到最大的轮廓（假设图形是面积最大的）
轮廓=排序（轮廓，键= cv2.contourArea，反向= True）[：1]

# 创建一个掩码来提取图形区域
mask = np.zeros_like(灰色)
cv2.drawContours(蒙版, 轮廓, -1, (255, 255, 255), 厚度=cv2.FILLED)

# 将掩模应用于原始图像以提取图形
图= cv2.bitwise_and（图像，图像，掩码=掩码）

# 保存提取的图形图像
cv2.imwrite(r&#39;C:\Prarthana\PROJECTS\GitHub\MajorProject\Images\output\extracted_graph.jpg&#39;, graph)
]]></description>
      <guid>https://stackoverflow.com/questions/78254412/i-want-to-extract-only-the-graph-parts-from-this-image-in-python-how-do-i-go-abo</guid>
      <pubDate>Mon, 01 Apr 2024 08:33:52 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：使用 `bitsandbytes` 8 位量化需要加速：`pip install Accelerate` 和最新版本的 Bitsandbytes：`pip install</title>
      <link>https://stackoverflow.com/questions/78254344/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78254344/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</guid>
      <pubDate>Mon, 01 Apr 2024 08:15:53 GMT</pubDate>
    </item>
    <item>
      <title>视觉 Transformer 模型的回归</title>
      <link>https://stackoverflow.com/questions/78253997/regression-on-the-vision-transformers-model</link>
      <description><![CDATA[我正在尝试对视觉变换器模型进行回归，但无法用回归层替换最后一层分类
当我尝试初始化模型时收到此错误
&lt;前&gt;&lt;代码&gt;
类 RegressionViT(nn.Module):
    def __init__(self, in_features=224 * 224 * 3, num_classes=1, pretrained=True):
        super(RegressionViT, self).__init__()
        self.vit_b_16 = vit_b_16(pretrained=pretrained) # 加载预训练权重

        # 用回归头替换最终的分类层
        self.regressor = nn.Linear(self.vit_b_16.heads.in_features, num_classes)

    def 前向（自身，x）：
        x = self.vit_b_16(x)
        x = self.regressor(x)
        返回x
]]></description>
      <guid>https://stackoverflow.com/questions/78253997/regression-on-the-vision-transformers-model</guid>
      <pubDate>Mon, 01 Apr 2024 06:33:32 GMT</pubDate>
    </item>
    <item>
      <title>NLP 中的序列分类/标记</title>
      <link>https://stackoverflow.com/questions/78253915/sequence-classification-labelling-in-nlp</link>
      <description><![CDATA[我正在尝试 nlp 的序列分类/标记。我有一些逻辑问题需要专家的回答。
这些问题与下面链接中给出的数据集相关。
数据集：https://huggingface.co/datasets /surrey-nlp/PLOD-CW/viewer/default/train

预处理：我正在删除停用词、标点符号和词形还原。这是正确的方法吗？

在“tokens”列的预处理过程中，我是否需要相应地更新 pos_tags 和 ner_tags 列，还是保持其余列相同？

接下来，我是否需要仅对预处理数据进行矢量化和构建模型？ （新的“tokens”、“pos_tags”、“ner_tags”列）


我做了什么：
我只预处理了“tokens”列，并保持其他列与原始数据集中相同。
我所期望的：
我在可视化数据时意识到其余列也应该更新]]></description>
      <guid>https://stackoverflow.com/questions/78253915/sequence-classification-labelling-in-nlp</guid>
      <pubDate>Mon, 01 Apr 2024 06:05:32 GMT</pubDate>
    </item>
    <item>
      <title>我的 PyTorch 回归机器学习程序没有学习</title>
      <link>https://stackoverflow.com/questions/78253278/my-pytorch-machine-learning-program-for-regression-is-not-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78253278/my-pytorch-machine-learning-program-for-regression-is-not-learning</guid>
      <pubDate>Mon, 01 Apr 2024 01:08:49 GMT</pubDate>
    </item>
    <item>
      <title>Keras相似度计算。枚举两个张量之间的距离，以列表形式表示</title>
      <link>https://stackoverflow.com/questions/78252612/keras-similarity-calculation-enumerating-distance-between-two-tensors-which-in</link>
      <description><![CDATA[我遵循 Nicholas Renotte 教程“构建深度面部识别应用程序”(Python)。但在第 4 部分我遇到了一个问题，代码如下：
连体 L1 距离等级
类L1Dist（图层）：

    # Init方法-继承
    def __init__(self, **kwargs):
        超级().__init__()

    # 魔法发生在这里 - 相似度计算
    def 调用（自身，input_embedding，validation_embedding）：
        返回 tf.math.abs（输入嵌入 - 验证嵌入）

类型错误：不支持的操作数类型 -：“列表”和“列表”
在视频中一切都很好，但在我的例子中，函数无法进行减法（input_embedding - valid_embedding）
L1Dist.call() 收到的参数：
args=([&#39;&#39;]，[&#39;&#39;])

尝试修改：
def 调用（自身、input_embedding、validation_embedding）：
        input_embedding = tf.convert_to_tensor(input_embedding)
        validation_embedding = tf.convert_to_tensor(validation_embedding)
        input_embedding = tf.squeeze(input_embedding, axis=0) # 删除潜在的第一维
        validation_embedding = tf.squeeze(validation_embedding, axis=0)
        返回 tf.math.abs（输入嵌入 - 验证嵌入）

但是失败了
第108行，在convert_to_eager_tensor中
    返回 ops.EagerTensor(值, ctx.device_name, dtype)
ValueError：TypeError：“KerasTensor”类型的对象没有 len()

尝试过 tf.keras.layers.Subtract()([input_embedding,validation_embedding])
但是 AttributeError: 调用 Subtract.call() 时遇到异常。
“list”对象没有属性“shape”
使用 keras.ops.subtract（input_embedding，validation_embedding）
面临：ValueError(f“无效的 dtype：{dtype}”)
ValueError：无效的数据类型：列表]]></description>
      <guid>https://stackoverflow.com/questions/78252612/keras-similarity-calculation-enumerating-distance-between-two-tensors-which-in</guid>
      <pubDate>Sun, 31 Mar 2024 19:39:46 GMT</pubDate>
    </item>
    <item>
      <title>Optuna Hyperband 算法不遵循预期的模型训练方案</title>
      <link>https://stackoverflow.com/questions/78251318/optuna-hyperband-algorithm-not-following-expected-model-training-scheme</link>
      <description><![CDATA[我在 Optuna 中使用 Hyperband 算法时发现了一个问题。根据 Hyperband 算法，当 min_resources = 5、ma​​x_resources = 20 且 reduction_factor = 2 时，搜索应以 支架 1 的初始空间为 4 个模型，每个模型在第一轮中接收 5 epoch。随后，每轮模型的数量减少 2 倍，下一个括号的搜索空间也应减少 2 倍，即括号 2 将进行初始搜索2 个模型的空间，并且剩余模型的 epoch 数量在后续的每一轮中加倍。因此预计模型总数应为 11，但它正在训练很多模型。
文章链接：- https://arxiv.org/pdf/1603.06560.pdf
导入 optuna
将 numpy 导入为 np
将 pandas 导入为 pd
从tensorflow.keras.layers导入密集，扁平化，丢弃
将张量流导入为 tf
从tensorflow.keras.models导入顺序


# 玩具数据集生成
defgenerate_toy_dataset():
    np.随机.种子(0)
    X_train = np.random.rand(100, 10)
    y_train = np.random.randint(0, 2, 大小=(100,))
    X_val = np.random.rand(20, 10)
    y_val = np.random.randint(0, 2, 大小=(20,))
    返回 X_train、y_train、X_val、y_val

X_train、y_train、X_val、y_val =generate_toy_dataset（）

# 模型构建函数
def build_model（试用）：
    模型=顺序（）
    model.add(Dense(units=Trial.suggest_int(&#39;unit_input&#39;, 20, 30),
                    激活=&#39;selu&#39;,
                    input_shape=(X_train.shape[1],)))

    num_layers = Trial.suggest_int(&#39;num_layers&#39;, 2, 3)
    对于范围内的 i（num_layers）：
        单位 = Trial.suggest_int(f&#39;num_layer_{i}&#39;, 20, 30)
        激活 = Trial.suggest_categorical(f&#39;activation_layer_{i}&#39;, [&#39;relu&#39;, &#39;selu&#39;, &#39;tanh&#39;])
        model.add（密集（单位=单位，激活=激活））
        if Trial.suggest_categorical(f&#39;dropout_layer_{i}&#39;, [True, False]):
            model.add(Dropout(rate=0.5))

    model.add（密集（1，激活=&#39;sigmoid&#39;））

    Optimizer_name = Trial.suggest_categorical(&#39;optimizer&#39;, [&#39;adam&#39;, &#39;rmsprop&#39;])
    如果优化器名称==&#39;亚当&#39;：
        优化器 = tf.keras.optimizers.Adam()
    别的：
        优化器 = tf.keras.optimizers.RMSprop()

    model.compile(optimizer=optimizer,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;,tf.keras.metrics.AUC(name=&#39;val_auc&#39;)])

    返回模型

定义目标（试用）：
    模型 = build_model(试用)
    # 假设你已经准备好数据
    # 修改拟合方法以包含 AUC 指标
    历史= model.fit（X_train，y_train，validation_data =（X_val，y_val），详细= 1）
    
    # 检查&#39;val_auc&#39;是否被记录
    auc_key = 无
    对于history.history.keys()中的键：
        if key.startswith(&#39;val_auc&#39;):
            auc_key = 密钥
            print(f&quot;auc_key 是 {auc_key}&quot;)
            休息
    
    如果 auc_key 为 None：
        raise ValueError(“历史记录中未找到 AUC 指标。确保在训练期间记录它。”)
    
    # 报告每个模型的验证 AUC
    
    如果 auc_key ==“val_auc”：
        步长=0
    别的：
        步骤 = int(auc_key.split(&#39;_&#39;)[-1])
    
    auc_value=history.history[auc_key][0]
    试验.报告（auc_value，步骤=步骤）
    print(f&quot;是否修剪:-{Trial.should_prune()}&quot;)
    如果审判.should_prune():
        引发 optuna.TrialPruned()

    返回历史记录.history[auc_key]

# Optuna 研究创建
研究 = optuna.create_study(
    方向=&#39;最大化&#39;,
    修剪器=optuna.pruners.HyperbandPruner(
        最小资源=5，
        最大资源=20,
        减少因子=2
    ）
）

# 开始优化
研究.优化（目标）

]]></description>
      <guid>https://stackoverflow.com/questions/78251318/optuna-hyperband-algorithm-not-following-expected-model-training-scheme</guid>
      <pubDate>Sun, 31 Mar 2024 12:38:07 GMT</pubDate>
    </item>
    <item>
      <title>在运行时访问记录的值</title>
      <link>https://stackoverflow.com/questions/65392269/access-logged-values-during-runtime</link>
      <description><![CDATA[如何在运行完成之前从 wandb 检索记录的值？
导入操作系统
导入万数据库
wandb.init(项目=&#39;someproject&#39;)


def loss_a():
    # do_stuff 和日志：
    wandb.log({“loss_a”: 1.0})
    
def loss_b():
    # do_stuff 和日志：
    wandb.log({“loss_b”: 2.0})

对于范围（2）中的纪元：
    损失_a（）
    损失_b()
    
    # 以某种方式检索loss_a和loss_b并在此处打印它们：
    print(f&#39;loss_a={??}, loss_b={??}&#39;)


运行完成后，我可以使用 wandb.Api 找到它以获取 run.history。但似乎在 run 完成之前，访问 run.history 不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/65392269/access-logged-values-during-runtime</guid>
      <pubDate>Mon, 21 Dec 2020 11:51:10 GMT</pubDate>
    </item>
    <item>
      <title>ALS 算法 Spark MLlib - 我如何获得自己的“个人推荐”（我未排名的电影排名）</title>
      <link>https://stackoverflow.com/questions/54592009/als-algorithm-spark-mllib-how-do-i-get-my-own-personal-recomendations-rank</link>
      <description><![CDATA[我在 Azure Databricks 中使用 PySpark。我使用 Sparks MLlib 库 ALS 算法来预测电影评级，效果很成功。但是，我正在尝试添加一个数据框，其中包含我对 10 部随机选择的电影的评分。当我这样做时，我只会获得我已经排名的电影的预测排名。 
我希望能够使用该模型根据排名获得推荐。
我有执行以下任务的 Spark 代码：

导入数据（RatingsSmall、MoviesSmall、RatingsLarge、Movies Large）
将小评分与小电影合并，将大评分与大电影合并
一起附加到两个新数据集
删除不相关的列时间戳和流派

我现在有一个干净的表，其中包含 MovieID、标题（电影名称）、UserID 和排名。我将从现在开始展示代码。如果您想要之前的代码，那么我也可以提交。

将数据拆分为训练集和测试集（0.80、0.20）
ALS算法
显示预测。

希望以上内容可以帮助您指导我所附的代码。
我只能获得对我已提交的排名的预测。
我尝试将我的排名加入到训练集中。从这里我想获得数据集中其他电影的推荐或预测。
我的尝试：
导入了一个带有我自己的排名的DF。
将此 (UnionAll) 附加到训练集中。
得到预测（但仅限于我已经排名的电影）
代码：
#分割数据集

    训练，测试 = All_Movies.randomSplit([0.8, 0.2])
    从 pyspark.ml.recommendation 导入 ALS

    从 pyspark.ml.evaluation 导入回归评估器

#设置模型

    ALS = ALS(maxIter=10, regParam=0.01, userCol = &quot;userId&quot;,itemCol=&quot;movieId&quot;, ratingCol=&quot;评级&quot;, ColdStartStrategy=&quot;drop&quot;)

#将模型适合训练集并附上个人建议


     model = ALS.fit(training.unionAll(PersonalDF)) #PersonalDF是我的排名

#获取测试集的预测
    预测 = model.transform(test).dropna()

#到这里为止一切都很好。

#尝试获取我的电影的预测排名
    mySampledMovies = model.transform(PersonalDF)
    mySampledMovies.registerTempTable(&quot;mySampledMovies&quot;)

    显示（sqlContext.sql（“从 mySampledMovies 中选择 userId、movieId、评分、标题、预测”））

我期望一个 DataFrame 能够显示我的用户 ID、电影 ID、排名、预测。对于我还没有看到的电影，排名为 N/A 或 Null，而预测具有价值。
非常感谢]]></description>
      <guid>https://stackoverflow.com/questions/54592009/als-algorithm-spark-mllib-how-do-i-get-my-own-personal-recomendations-rank</guid>
      <pubDate>Fri, 08 Feb 2019 12:00:58 GMT</pubDate>
    </item>
    </channel>
</rss>