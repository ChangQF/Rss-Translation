<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 19 Oct 2024 18:21:16 GMT</lastBuildDate>
    <item>
      <title>.pth 到 .onnx 的转换破坏了模型 u2net</title>
      <link>https://stackoverflow.com/questions/79105371/pth-to-onnx-conversion-break-the-model-u2net</link>
      <description><![CDATA[我是一名 PHP 和 Golang 开发人员。我从未使用过 Python。
但我必须在工作中处理它。
那么我需要什么呢？
任务如下：网站需要添加从汽车磁盘图像中删除背景的功能。
我决定使用 rembg 库作为基础：https://github.com/danielgatis/rembg
此库又基于 u2net 工作：https://github.com/xuebinqin/U-2-Net
但是，标准 u2net 模型会从外部删除所有背景，而磁盘内部的空间保持不变 - 在辐条、孔等之间。
在谷歌搜索了一下之后，我得出结论，我可以进一步训练u2net 模型来满足我的具体需求。
操作算法如下：

进一步训练模型
将其加载到 rembg 中
使用自定义模型进行裁剪

我设法训练了标准 u2net 模型，它完美地按照我的需要裁剪出黑白蒙版。
但是，当将模型从 .pth 转换为 .onnx 格式（这是在 rembg 中工作所必需的）时，它开始工作不佳。
蒙版模糊且有肥皂味。我尝试转换标准的未训练 u2net 模型
并在 rembg 中使用它 - 结果是一样的，蒙版模糊，背景裁剪不起作用。
因此，结论是训练成功了。
问题在于转换。
因此，以下是我训练过的模型的掩码示例。
原始图像：
在此处输入图像说明
我训练过的模型生成的掩码：
在此处输入图像说明
我训练过的模型转换为 .onnx 格式后生成的掩码
在此处输入图像说明
要在 u2net 中生成掩码，我使用：
python3 u2net_test.py
要生成掩码在 rembg 中我使用以下命令：
rembg i -om -m u2net_custom -x &#39;{&quot;model_path&quot;: &quot;~/.u2net/u2net_custom.onnx&quot;}&#39; 55.jpg 55.png
我尝试转换完成的模型。以下是转换代码：
import torch
import torch.onnx
from model.u2net import U2NET

def load_model(model_path, model_class):
checkpoint = torch.load(model_path, map_location=&#39;cpu&#39;)
if isinstance(checkpoint, dict) and &#39;state_dict&#39; in checkpoint:
model = model_class()
model.load_state_dict(checkpoint[&#39;state_dict&#39;])
else:
model = model_class()
model.eval()
return model

def convert_to_onnx(model, output_path):
dummy_input = torch.randn(1, 3, 320, 320)
torch.onnx.export(model, dummy_input, output_path, opset_version=12,
dynamic_axes={&#39;input&#39;: {0: &#39;batch_size&#39;, 2: &#39;height&#39;, 3: &#39;width&#39;},
&#39;output&#39;: {0: &#39;batch_size&#39;, 2: &#39;height&#39;, 3: &#39;width&#39;}})
print(f&quot;success {output_path}&quot;)

if __name__ == &quot;__main__&quot;:
import argparse

parser = argparse.ArgumentParser(description=&quot;conversion PyTorch to ONNX&quot;)
parser.add_argument(&#39;--model-path&#39;, type=str, required=True, help=&#39;path to .pth file&#39;)
parser.add_argument(&#39;--output-path&#39;, type=str, required=True, help=&#39;save ONNX file&#39;)

args = parser.parse_args()
model = load_model(args.model_path, U2NET)
convert_to_onnx(model, args.output_path)

并尝试在训练过程中保存模型：
 if ite_num % save_frq == 0:
timestamp = int(time.time())
filePath = model_dir + model_name+&quot;_%d_%d.&quot; % (ite_num, timestamp)

torch.save(net.state_dict(), filePath + &#39;pth&#39;)

dummy_input = torch.randn(1, 3, 320, 320)
net.eval()
torch.onnx.export(net, dummy_input, filePath + &#39;onnx&#39;, opset_version=12)

running_loss = 0.0
running_tar_loss = 0.0
net.train() # 恢复训练
ite_num4val = 0

我尝试更改设置、更改库版本、更改 opset_version 以及 chatGpt 建议的所有其他操作。
结果总是一样的。
转换后模型停止工作。
我犯了什么错误？]]></description>
      <guid>https://stackoverflow.com/questions/79105371/pth-to-onnx-conversion-break-the-model-u2net</guid>
      <pubDate>Sat, 19 Oct 2024 16:07:45 GMT</pubDate>
    </item>
    <item>
      <title>将 BCELossWithLogits 中某个像素类的损失清零</title>
      <link>https://stackoverflow.com/questions/79105307/zero-out-loss-for-a-certain-pixel-class-in-bcelosswithlogits</link>
      <description><![CDATA[我正在对宠物（狗和猫）数据集执行二元语义分割，每个像素都有一个类别。共有 3 个类别，前景（1.0）、背景（0.0）和未分类像素（0.5020）。我只关心模型预测前景和背景像素的效果如何。所以我的想法是将“未分类”像素的损失设置为 0。这样这些像素就不会在反向传播过程中影响梯度。为了做到这一点，我的想法是创建一个掩码，对于前景类（1.0）和背景类（0.0）的像素，该掩码为 1.0（真），对于未分类类的像素，该掩码为 0.0（假）。下面你可以看到我的实现。
import torch
import torch.nn as nn

class CustomBCEWithLogitsLoss(nn.Module):
def __init__(self, ignore_class_value=0.5020):
super(CustomBCEWithLogitsLoss, self).__init__()
# 使用 logits 初始化二元交叉熵损失
self.bce_loss = nn.BCEWithLogitsLoss(reduction=&#39;none&#39;)
# 存储忽略类值
self.ignore_class_value = torch.tensor(ignore_class_value)

def forward(self, output, labels):
# 打印标签中的唯一值以供调试
print(&quot;Unique labels:&quot;, torch.unique(labels))

# 创建一个用于忽略未分类像素的掩码
mask = (labels != self.ignore_class_value).float()
print(&quot;唯一掩码值：&quot;, torch.unique(mask))

# 为前景类 (1.0) 创建二进制标签
binary_labels = (labels == 1.0).float()

# 计算损失
loss = self.bce_loss(outputs, binary_labels)
# 将掩码应用于损失
loss = loss * mask
# 根据有效像素数对损失进行标准化
loss = loss.sum() / mask.sum()

return loss

问题是，运行代码时我得到以下结果：
print(torch.unique(labels))
tensor([0.0000, 0.5020, 1.0000], device=&#39;cuda:0&#39;)
print(torch.unique(mask))
tensor([1.], device=&#39;cuda:0&#39;)

这不正确，因为标签显然包含类 0.0，所以我的掩码应该包含类型 1.0 和 0.0 的值。
我真的不明白为什么它不起作用。我尝试使用 torch.isClose 来检查问题是否与浮点问题有关，但这也没有解决问题。现在我陷入困境，正在试图了解问题所在。
任何帮助都非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/79105307/zero-out-loss-for-a-certain-pixel-class-in-bcelosswithlogits</guid>
      <pubDate>Sat, 19 Oct 2024 15:34:57 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Heroku 上的 Python Flask 后端使用大型模型文件部署 YOLOv8 模型？</title>
      <link>https://stackoverflow.com/questions/79104416/how-to-deploy-yolov8-model-in-python-flask-backend-on-heroku-with-large-model-fi</link>
      <description><![CDATA[我使用 Python Flask 后端和 React Native 前端开发了一个用于图像检测和分类的移动应用程序。我使用 Google Colab 训练了我的 YOLOv8 模型，并下载了用于对象检测和分类的 best.pt 和 bestc.pt 文件。我将这些模型文件放在 Flask 应用程序的后端文件夹 (/backend/models/) 中。
我正尝试在 Heroku 上部署后端，但遇到了问题，因为模型的文件大小很大。当我尝试在 Heroku 上部署应用程序时，出现错误，提示无法检测到 ultralytics 包。安装 ultralytics 后，我的应用程序的大小增加到 3GB，超过了免费版 Heroku 允许的 500MB 限制。
如果没有模型和 ultralytics 包，后端大小只有 10MB。这是我的后端文件夹结构：
 ═── backend/
│ ═── models/
│ │ ═── bestc.pt
│ │ └── best.pt
│ └── app.py

from flask import Flask, request, jsonify
from ultralytics import YOLO
from PIL import Image, ImageOps
from flask_cors import CORS
import io
import os

app = Flask(__name__)
CORS(app)

# 启动时加载一次模型
yolo_model = YOLO(&#39;models/best.pt&#39;) # 用于对象检测和裁剪的 YOLO 模型
classification_model = YOLO(&#39;models/bestc.pt&#39;) # 用于分类的模型

CONFIDENCE_THRESHOLD = 0.5 # 设置有效置信度阈值检测

@app.route(&#39;/classify&#39;, methods=[&#39;POST&#39;])
def predict():
try:
file = request.files[&#39;image&#39;]
img = Image.open(file.stream).convert(&quot;RGB&quot;) # 确保它是 RGB 格式

# 运行 YOLO 进行对象检测
detection_results = yolo_model(img)

# 调试：打印检测结果
print(&quot;Detection Results:&quot;, detection_results)

# 检查是否有任何检测
if len(detection_results) == 0 or len(detection_results[0].boxes) == 0:
return jsonify({&#39;message&#39;: &#39;No object sent in the image.&#39;}), 200

# 过滤以仅保留高于阈值的最可信检测
best_box = max(detection_results[0].boxes, key=lambda box: box.conf)
如果 best_box.conf &lt; CONFIDENCE_THRESHOLD:
return jsonify({&#39;message&#39;: &#39;未检测到有效的蛇。请上传更清晰的图像。&#39;}), 200

box_coords = best_box.xyxy[0].tolist() # 转换为坐标列表
print(f&quot;裁剪坐标：{box_coords}&quot;) # 调试：打印裁剪坐标

cropped_img = img.crop(box_coords) # 使用边界框坐标进行裁剪

# 为裁剪后的图像添加填充以防止失真
padded_img = ImageOps.pad(cropped_img, (224, 224), method=Image.Resampling.LANCZOS)

# 对填充后的图像进行分类
classes_results = classes_model(padded_img)

# 处理分类结果
predictions = []
for result in分类结果：
top_class = result.names[result.probs.top1]
top_confidence = result.probs.top1conf.item()

# 如果分类置信度低于阈值，则拒绝结果
if top_confidence &lt; CONFIDENCE_THRESHOLD:
return jsonify({
&#39;message&#39;: &#39;未检测到有效的蛇。请上传更清晰的图像。&#39;,
&#39;class&#39;: top_class,
&#39;probability&#39;: &quot;{:.2%}&quot;.format(top_confidence)
}), 200

# 确定毒液状态
venom_status = get_venom_status(top_class)

# 将概率格式化为带有两个小数点的百分比
formatted_prob = &quot;{:.2%}&quot;.format(top_confidence)

# 附加到预测
predictions.append({
&#39;class&#39;: top_class,
&#39;probability&#39;: formatted_prob,
&#39;venom_status&#39;: venom_status
})

return jsonify({&#39;predictions&#39;: predictions}), 200

except Exception as e:
print(f&quot;Error: {str(e)}&quot;)
return jsonify({&#39;error&#39;: &#39;处理过程中发生错误。&#39;, &#39;details&#39;: str(e)}), 500

def get_venom_status(class_name):
venom_status_map = {
&#39;Common Indian Krait&#39;: &#39;有毒&#39;,
&#39;Python&#39;: &#39;无毒&#39;,
&#39;Hump Nosed Viper&#39;: &#39;有毒&#39;,
&#39;Green Vine Snake&#39;: &#39;无毒&#39;,
&#39;Russells Viper&#39;: &#39;有毒&#39;,
&#39;Indian Cobra&#39;: &#39;有毒&#39;
}
return venom_status_map.get(class_name, &#39;Unknown&#39;)

if __name__ == &#39;__main__&#39;:
app.run(host=&#39;0.0.0.0&#39;, port=int(os.environ.get(&#39;PORT&#39;, 5000)))


如何在 Heroku 上部署这个具有大模型（约 3GB）的 Flask 应用，同时又不超出 Heroku 500MB 的免费套餐限制？
在 Heroku 上部署时，是否有管理大模型的最佳实践，或者是否有其他云解决方案可以解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79104416/how-to-deploy-yolov8-model-in-python-flask-backend-on-heroku-with-large-model-fi</guid>
      <pubDate>Sat, 19 Oct 2024 07:39:42 GMT</pubDate>
    </item>
    <item>
      <title>在 Colab 中微调 Llama 3.1 时的上下文长度限制</title>
      <link>https://stackoverflow.com/questions/79104305/context-length-limitation-when-fine-tuning-llama-3-1-in-colab</link>
      <description><![CDATA[我正在通过 Unsloth 库，使用带有自定义数据集（使用 LoRA 技术）的 A100 GPU 在 Google Colab Pro 中对 Llama 3.1 模型进行微调。下面是我正在使用的 LoRA 代码：
max_seq_length = 2048
model = FastLanguageModel.get_peft_model(
model,
r=16, # 选择任意数字 &gt; 0 ！建议 8、16、32、64、128
target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
&quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down​​_proj&quot;],
lora_alpha=16,
lora_dropout=0, # 支持任意，但 = 0 是经过优化的
bias=&quot;none&quot;, # 支持任意，但 = &quot;none&quot; 是经过优化的

use_gradient_checkpointing=&quot;unsloth&quot;, # 对于非常长的上下文，为 True 或 &quot;unsloth&quot;
random_state=3407,
use_rslora=False, # 我们支持等级稳定的 LoRA
loftq_config=None, # 和 LoftQ
)
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
train_dataset=dataset,
dataset_text_field=&quot;text&quot;,
max_seq_length=max_seq_length,
dataset_num_proc=2,
packing=False, # 可以使短序列的训练速度提高 5 倍。
args=TrainingArguments(
per_device_train_batch_size=2,
gradient_accumulation_steps=4,
warmup_steps=5,
# num_train_epochs = 1, # 将其设置为 1 次完整的训练运行。
max_steps=60,
learning_rate=2e-4,
fp16=not is_bfloat16_supported(),
bf16=is_bfloat16_supported(),
logs_steps=1,
optim=&quot;adamw_8bit&quot;,
weight_decay=0.01,
lr_scheduler_type=&quot;linear&quot;,
seed=3407,
output_dir=&quot;outputs&quot;,
),
)

加载模型时，我们必须指定最大序列长度，这会限制其上下文窗口。 Llama 3.1 支持高达 128k 的上下文长度，但在本例中我将其设置为 2048，因为它消耗更多的计算和 VRAM。此外，dtype 参数会自动检测您的 GPU 是否支持 BF16 格式，以便在训练期间获得更高的稳定性（此功能仅限于 Ampere 和较新的 GPU）。
我的问题：

如果我在训练时将 max_seq_length 设置为 2048，那么训练后我的模型的上下文长度是多少，128k 还是 2048？
训练模型后，我们可以使用 128k 的上下文长度吗，还是仍然限制为 2048？
]]></description>
      <guid>https://stackoverflow.com/questions/79104305/context-length-limitation-when-fine-tuning-llama-3-1-in-colab</guid>
      <pubDate>Sat, 19 Oct 2024 05:59:26 GMT</pubDate>
    </item>
    <item>
      <title>numpy nd 数组连接用于创建数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/79104179/numpy-nd-array-concatenation-for-dataset-creation</link>
      <description><![CDATA[我正在处理一个数据集，其中训练数据具有此形状（33104,6,128,1），标签具有此形状（33104,1），我该如何连接标签以适合其最后一列？
我该如何连接标签以适合其最后一列？]]></description>
      <guid>https://stackoverflow.com/questions/79104179/numpy-nd-array-concatenation-for-dataset-creation</guid>
      <pubDate>Sat, 19 Oct 2024 04:01:10 GMT</pubDate>
    </item>
    <item>
      <title>训练模型预测不一致</title>
      <link>https://stackoverflow.com/questions/79104144/trained-model-prediction-are-not-consistent</link>
      <description><![CDATA[我用一个小型数据集训练了一个 VGG16 (224x224x3) 迁移学习图像分类模型，该数据集包含来自 3 位艺术家的画作。该数据集有大约 80 幅画作用于训练，25 幅用于测试。图像上有增强。图像被标记为真（属于艺术家）或假（不属于艺术家），比例约为 50-50。训练补丁总数（224x224x3）约为 20000。该模型旨在判断一幅画是否属于艺术家。这是训练损失和准确度图：

该图表明该模型在测试数据集上表现良好。然后将壁灯等图像（模型从未见过这些图像）输入模型，并询问此壁灯图像是否属于艺术家。显然答案是否定的。但最初有几次模型确实给出了“否”的答案，但现在它始终给出“是”的答案，这显然是错误的。我对训练模型性能的经验有限。什么会导致训练模型的预测不一致？训练数据集太小还是其他原因？]]></description>
      <guid>https://stackoverflow.com/questions/79104144/trained-model-prediction-are-not-consistent</guid>
      <pubDate>Sat, 19 Oct 2024 03:19:59 GMT</pubDate>
    </item>
    <item>
      <title>我们如何知道Ranker（Weka）要设置哪些参数？[关闭]</title>
      <link>https://stackoverflow.com/questions/79103647/how-do-we-know-what-parameters-for-ranker-weka-to-set</link>
      <description><![CDATA[我正在使用 Weka。所以我有几个问题无法通过手册弄清楚：

我们如何知道 Ranker 应该设置哪些参数？我的意思是，阈值和 numToSelect。对此有什么解释吗？
当我通过资源管理器选择属性并保存修改后的数据集时，它始终是 N+1 属性（N 个选定的属性 + 类/标签）。为什么？标签/类不也是属性吗？
]]></description>
      <guid>https://stackoverflow.com/questions/79103647/how-do-we-know-what-parameters-for-ranker-weka-to-set</guid>
      <pubDate>Fri, 18 Oct 2024 20:54:50 GMT</pubDate>
    </item>
    <item>
      <title>高效的 Net V2 M ONNX 模型在小输入上的推断速度明显较慢 [关闭]</title>
      <link>https://stackoverflow.com/questions/79103592/efficient-net-v2-m-onnx-model-infers-significantly-slower-on-small-input</link>
      <description><![CDATA[当我将 Efficient net v2 m 模型从 Pytorch 转换为不同大小输入的 Onnx 时，我注意到一种奇怪且无法解释的行为。
在我的 RTX 4090 上，1280X1280 大小图像上的 ONNX 模型在 35 毫秒内推断出批处理大小为 1。当我将图像大小缩小到大约 192X192（批处理大小相同为 1）时，运行时间几乎保持不变。这是可以理解的，因为固定开销占主导地位，例如初始化时间、线程池预热、与 GPU 之间的低效数据传输，最重要的是，计算库针对 GPU 上的矢量化和 SIMD 指令进行了优化。
然而，令人困惑的是，一旦我开始将输入图像大小减小到 192X192 以下，运行时间就会急剧增加。对于 64X64 图像，批处理大小为 1 时运行时间为 &gt;100ms。我完全理解为什么在较小的图像上推理不应该更快，但我不明白为什么它会更慢（而且慢得多）。
当我增加较小图像的批处理大小时，每批的运行时间会大幅改善（不仅仅是每幅图像的运行时间）。对于批处理大小为 16 的图像，推理 192X192 图像每批需要 25 毫秒（每幅图像不到 2 毫秒），而批处理大小为 1 时则需要 &gt;100ms。同样，我对此没有任何解释。固定开销和优化的 SIMD 矢量化将决定每幅图像的摊销运行时间应该随着批处理大小的增加而改善。但是，我观察到整个批次的运行时间也得到了改善。
对于较大的图像（例如 1280X1280），增加批次大小会增加每个批次的运行时间（尽管是亚线性的，这是完全可以预料的 - 随着批次大小的增加，每个图像的运行时间仍然会缩短到一定限度，之后，对于几乎无法放入 GPU 内存的更高批次大小，每个图像的运行时间也会增加约 10%）。
但是在 CPU 上运行时，处理时间会随着输入大小的增加而单调增加，正如预期的那样。
当我要求它对所有输入进行处理时，我已经验证了 ONNX 模型在 GPU 上成功运行。事实上，对于小输入，CPU 推理时间比 GPU 更快（这是可以理解的，因为有固定的 I/O 和其他开销）
注意：由于我在整个实验过程中将动态轴设置为 None，因此我为具有不同输入大小的同一 torch 模型保存了多个版本的 ONNX 模型。使用或不使用 onnx-sim 几乎不会对运行时间产生影响（处理速度差异小于 10-15%）。我在 C++ 中以 OrtCUDAProviderOptions 作为执行提供程序运行 onnx 模型，使用或不使用 GraphOptimizationLevel 几乎没有区别。
神经网络的输出对于所有输入都符合预期，因此我不希望我的代码中出现任何错误。
TL;DR
我的 ONNX 模型对于中等大小图像的运行速度比 GPU 上的小图像更快。对于较小的图像，增加批次大小会大幅减少每批次的处理时间（而不仅仅是 SIMD 并行化所预期的每张图像的摊销时间）。]]></description>
      <guid>https://stackoverflow.com/questions/79103592/efficient-net-v2-m-onnx-model-infers-significantly-slower-on-small-input</guid>
      <pubDate>Fri, 18 Oct 2024 20:32:42 GMT</pubDate>
    </item>
    <item>
      <title>决策树，Knn 算法 [关闭]</title>
      <link>https://stackoverflow.com/questions/79103501/decision-tree-knn-algorithms</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79103501/decision-tree-knn-algorithms</guid>
      <pubDate>Fri, 18 Oct 2024 19:59:23 GMT</pubDate>
    </item>
    <item>
      <title>我应该在正弦位置编码中交错正弦和余弦吗？</title>
      <link>https://stackoverflow.com/questions/79103455/should-i-interleave-sin-and-cosine-in-sinusoidal-positional-encoding</link>
      <description><![CDATA[我正在尝试实现正弦位置编码。我发现了两个给出不同编码的解决方案。我想知道其中一个是错误的还是两个都是正确的。我展示了两种选项的编码结果的视觉图。
class SinusoidalPosEmb(nn.Module):
def __init__(self, dim):
super().__init__()
self.dim = dim
def forward(self, x):
device = x.device
half_dim = self.dim // 2
emb = math.log(10000) / (half_dim - 1)
emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
emb = x[:, None] * emb[None, :]
emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
return emb


2)
class TransformerPositionalEmbedding(nn.Module):
&quot;&quot;&quot;
摘自论文《Attention Is All You Need》第 3.5 节
&quot;&quot;&quot;
def __init__(self, dimension, max_timesteps=1000):
super(TransformerPositionalEmbedding, self).__init__()
assert dimension % 2 == 0, &quot;Embedding 维度必须是偶数&quot;
self.dimension = dimension
self.pe_matrix = torch.zeros(max_timesteps, dimension)
# 收集嵌入向量中的所有偶数维度
even_indices = torch.arange(0, self.dimension, 2)
# 使用对数变换计算项以加快计算速度
# (https://stackoverflow.com/questions/17891595/pow-vs-exp-performance)
log_term = torch.log(torch.tensor(10000.0)) / self.dimension
div_term = torch.exp(even_indices * -log_term)
# 根据奇数/偶数时间步长预先计算位置编码矩阵
timesteps = torch.arange(max_timesteps).unsqueeze(1)
self.pe_matrix[:, 0::2] = torch.sin(timesteps * div_term)
self.pe_matrix[:, 1::2] = torch.cos(timesteps * div_term)
def forward(self, timestep):
# [bs, d_model]
return self.pe_matrix[timestep]

]]></description>
      <guid>https://stackoverflow.com/questions/79103455/should-i-interleave-sin-and-cosine-in-sinusoidal-positional-encoding</guid>
      <pubDate>Fri, 18 Oct 2024 19:35:08 GMT</pubDate>
    </item>
    <item>
      <title>如何估计 CoreML 模型的参数数量？</title>
      <link>https://stackoverflow.com/questions/79103126/how-to-esitmate-the-number-of-parameters-for-coreml-models</link>
      <description><![CDATA[我正在比较修剪对 CoreML 模型的影响。
虽然我可以轻松测量文件大小的变化（以 kB 为单位），但我很难估计模型参数数量的变化，因为 CoreML 没有提供像 PyTorch 的 model.parameters() 这样的直接方法。我如何估计或计算修剪后的 CoreML 模型中的参数数量？]]></description>
      <guid>https://stackoverflow.com/questions/79103126/how-to-esitmate-the-number-of-parameters-for-coreml-models</guid>
      <pubDate>Fri, 18 Oct 2024 17:35:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 RAG 和 FastAPI 的 WebApp [关闭]</title>
      <link>https://stackoverflow.com/questions/79099986/webapp-using-rag-and-fastapi</link>
      <description><![CDATA[我正在开展一个项目，需要实现一个检索增强生成 (RAG) 后端系统，该系统可以分析和比较两个提供的调查结果数据集。目标是创建一个具有 Python FastAPI 后端和 ReactJS 前端的 Web 应用程序。该应用程序应允许用户：
探索两个数据集。
交叉比较数据集。
从用户查询中生成 AI 驱动的见解。
后端应根据用户查询从数据集中检索相关数据，并将检索到的数据传递给文本生成模型以生成上下文响应。
这两个数据集都是 Excel 格式，但我已将它们转换为 CSV，甚至尝试使用 SQL 和 JSON 来处理数据。数据集复杂且嵌套，标题在行和列中重复，使数据看起来像矩阵，这使处理变得复杂。
最大的问题似乎是数据集本身的复杂性。它包含带有行和列标题的嵌套信息，因此很难提取有意义的文本。检索和生成的组合并没有产生相关的见解。GPT-2 倾向于重复自己或提供通用的、非信息性的响应，例如：

圣诞节是我们所有人庆祝和庆祝的时刻。这是我们所有人庆祝的时刻。这是一个庆祝的时刻……

我如何处理具有重复行和列标题的复杂数据集，并将其转换为 RAG 管道可用的格式？
任何帮助或指导都将不胜感激！
GitHub 存储库：https://github.com/drrahulsuresh/bounce/tree/dev]]></description>
      <guid>https://stackoverflow.com/questions/79099986/webapp-using-rag-and-fastapi</guid>
      <pubDate>Thu, 17 Oct 2024 22:12:24 GMT</pubDate>
    </item>
    <item>
      <title>将 ONNX 模型从版本 9 升级到 11</title>
      <link>https://stackoverflow.com/questions/69899046/upgrade-onnx-model-from-version-9-to-11</link>
      <description><![CDATA[我正在使用 ONNX 模型，需要对其进行量化以减小其大小，为此，我遵循官方文档中的说明：
import onnx
from onnxruntime.quantization import quantize_dynamic, QuantType

model_fp32 = &#39;path/to/the/model.onnx&#39;
model_quant = &#39;path/to/the/model.quant.onnx&#39;
quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8)

但是当我运行它时，我收到以下警告：
警告：root：原始模型 opset 版本为 9，不支持量化。请将模型更新为 opset &gt;= 11。自动将模型更新为 opset 11。请验证量化模型。

我测试了量化模型，但没有成功，它生成此错误：
 INVALID_GRAPH：从 model_a2_quant.onnx 加载模型失败：这是一个无效的模型。 Node:Upsample__477 中的错误：为 Upsample 注册的 Op 在 domain_version 11 中已弃用

此时我有什么替代方案可以量化模型？
我从这个 repo 中获得了张量流中的原始模型：https://github.com/ciber-lab/pictor-ppe
并使用此代码将其转换为 ONNX：
# 输入和输出
input_tensor = 输入（shape=(input_shape[0], input_shape[1], 3) ) # 输入
num_out_filters = ( num_anchors//3 ) * ( 5 + num_classes ) # 输出

## 构建并加载模型
model = yolo_body(input_tensor, num_out_filters)

weight_path = &#39;ONNX_demo/models/pictor-ppe-v302-a1-yolo-v3-weights.h5&#39;

model.load_weights( weight_path )

tf.saved_model.save(model, &quot;ONNX_demo/models/save_model&quot;)

# 将其转换为 ONNX 格式：

python3 -m tf2onnx.convert --saved-model &quot;ONNX_demo/models/save_model&quot; --output &quot;ONNX_demo/models/model.onnx&quot;
]]></description>
      <guid>https://stackoverflow.com/questions/69899046/upgrade-onnx-model-from-version-9-to-11</guid>
      <pubDate>Tue, 09 Nov 2021 13:31:13 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 生存模型预测</title>
      <link>https://stackoverflow.com/questions/53521427/xgboost-prediction-for-survival-model</link>
      <description><![CDATA[Xgboost 的文档暗示，使用 Cox PH 损失训练的模型的输出将是个人预测乘数的指数（相对于基线风险）。有没有办法从这个模型中提取基线风险，以预测每个人的整个生存曲线？

survival:cox：右删失生存时间数据的 Cox 回归
（负值被视为右删失）。请注意，预测
是在风险比尺度上返回的（即，比例风险函数 h(t) = h0(t) * HR 中的 HR = exp(marginal_prediction)）
]]></description>
      <guid>https://stackoverflow.com/questions/53521427/xgboost-prediction-for-survival-model</guid>
      <pubDate>Wed, 28 Nov 2018 14:13:15 GMT</pubDate>
    </item>
    <item>
      <title>sample.int(m, k) 中的错误：无法抽取大于总体的样本</title>
      <link>https://stackoverflow.com/questions/25745215/error-in-sample-intm-k-cannot-take-a-sample-larger-than-the-population</link>
      <description><![CDATA[首先，我要说的是，我对机器学习、kmeans 和 r 还很陌生，这个项目是一种学习更多这方面知识的方法，也是将这些数据呈现给我们的 CIO 的一种方式，这样我就可以在开发新的帮助台系统时使用它。
我有一个 60K 行的文本文件。该文件包含教师在 3 年期间输入的帮助台工单的标题。
我想创建一个 r 程序，获取这些标题并创建一组类别。例如，与打印问题相关的术语，或与投影仪灯泡相关的一组术语。我使用 r 打开文本文档，清理数据，删除停用词和其他我认为不必要的词。我已获得频率 &gt;= 400 的所有术语列表，并将其保存到文本文件中。
但现在我想将 kmeans 聚类（如果可以完成或合适）应用于同一数据集，看看我是否可以提出类别。
下面的代码包括将写出使用的术语列表 &gt;= 400 的代码。它位于末尾，并被注释掉。
library(tm) #加载文本挖掘库
library(SnowballC)
options(max.print=5.5E5) 
setwd(&#39;c:/temp/&#39;) #将 R 的工作目录设置为靠近我的文件的位置
ae.corpus&lt;-Corpus(DirSource(&quot;c:/temp/&quot;),readerControl=list(reader=readPlain))
summary(ae.corpus) #检查发生了什么在
ae.corpus &lt;- tm_map(ae.corpus, tolower)
ae.corpus &lt;- tm_map(ae.corpus, removePunctuation)
ae.corpus &lt;- tm_map(ae.corpus, removeNumbers)
ae.corpus &lt;- tm_map(ae.corpus, stemDocument, language = &quot;english&quot;) 
myStopwords &lt;- c(stopwords(&#39;english&#39;), &lt;a very long list of other words&gt;)
ae.corpus &lt;- tm_map(ae.corpus, removeWords, myStopwords) 

ae.corpus &lt;- tm_map(ae.corpus, PlainTextDocument)

ae.tdm &lt;- DocumentTermMatrix(ae.corpus, control = list(minWordLength = 5))

dtm.weight &lt;- weightTfIdf(ae.tdm)

m &lt;- as.matrix(dtm.weight)
rownames(m) &lt;- 1:nrow(m)

#euclidian 
norm_eucl &lt;- function(m) {
m/apply(m,1,function(x) sum(x^2)^.5)
}
m_norm &lt;- norm_eucl(m)

results &lt;- kmeans(m_norm,25)

#list clusters

clusters &lt;- 1:25
for (i in clusters){
cat(&quot;Cluster &quot;,i,&quot;:&quot;,findFreqTerms(dtm.weight[results$cluster==i],400,&quot;\n\n&quot;))
}

#inspect(ae.tdm)
#fft &lt;- findFreqTerms(ae.tdm, lowfreq=400)

#write(fft, file = &quot;dataTitles.txt&quot;,
# ncolumns = 1,
# append = FALSE, sep = &quot; &quot;)

#str(fft)

#inspect(fft)

当我使用 RStudio 运行此程序时，我得到：
&gt;结果 &lt;- kmeans(m_norm,25)


sample.int(m, k) 中的错误：当“replace = FALSE”时无法获取大于总体的样本

我不太清楚这是什么意思，而且我在网上没有找到很多关于这方面的信息。有什么想法吗？
TIA]]></description>
      <guid>https://stackoverflow.com/questions/25745215/error-in-sample-intm-k-cannot-take-a-sample-larger-than-the-population</guid>
      <pubDate>Tue, 09 Sep 2014 12:55:28 GMT</pubDate>
    </item>
    </channel>
</rss>