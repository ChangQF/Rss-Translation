<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 22 Apr 2024 21:13:29 GMT</lastBuildDate>
    <item>
      <title>Python 中用于二元分类机器学习模型的 LazyClassifier 中的 ValueError？</title>
      <link>https://stackoverflow.com/questions/78368809/valueerror-in-lazyclassifier-in-python-for-binary-classification-machine-learnin</link>
      <description><![CDATA[我尝试使用 LazyClassifier 构建具有二进制目标变量的机器学习模型。

X_train 有 114074 个观测值和 15 列
X_test 有 48890 个观测值
y_train 有 114074 个观测值和 15 列
y_test 有 48890 个观测值

我创建了如下所示的 custom_metic 函数：
def custom_metric(y_train_true, y_train_pred, y_test_true, y_test_pred):
    准确度训练 = 准确度分数(y_train_true, y_train_pred)
    准确度测试 = 准确度分数(y_test_true, y_test_pred)
    
    f1_beta1_train = f1_score(y_train_true, y_train_pred, beta=1)
    f1_beta1_test = f1_score(y_test_true, y_test_pred, beta=1)
    
    f1_beta2_train = f1_score(y_train_true, y_train_pred, beta=2)
    f1_beta2_test = f1_score(y_test_true, y_test_pred, beta=2)
    
    auc_train = roc_auc_score(y_train_true, y_train_pred)
    auc_test = roc_auc_score(y_test_true, y_test_pred)
    
    precision_train = precision_score(y_train_true, y_train_pred)
    precision_test = precision_score(y_test_true, y_test_pred)
    
    召回训练 = 召回分数(y_train_true, y_train_pred)
    召回测试 = 召回分数(y_test_true, y_test_pred)
    
    返回 {
        &#39;accuracy_train&#39;：accuracy_train，&#39;accuracy_test&#39;：accuracy_test，
        &#39;f1_beta1_train&#39;：f1_beta1_train，&#39;f1_beta1_test&#39;：f1_beta1_test，
        &#39;f1_beta2_train&#39;：f1_beta2_train，&#39;f1_beta2_test&#39;：f1_beta2_test，
        &#39;auc_train&#39;：auc_train，&#39;auc_test&#39;：auc_test，
        &#39;精度训练&#39;：精度训练，&#39;精度测试&#39;：精度测试，
        &#39;recall_train&#39;：recall_train，&#39;recall_test&#39;：recall_test
    }

然后我尝试使用 LazyClassifier：
cls = LazyClassifier(ignore_warnings=True, custom_metric=custom_metric)
模型，预测 = cls.fit(X_train, X_test, y_train, y_test)

尽管如此，我收到了如下错误：
ValueError：所有数组的长度必须相同


我该如何避免该错误？]]></description>
      <guid>https://stackoverflow.com/questions/78368809/valueerror-in-lazyclassifier-in-python-for-binary-classification-machine-learnin</guid>
      <pubDate>Mon, 22 Apr 2024 20:49:06 GMT</pubDate>
    </item>
    <item>
      <title>在使用 LR finder 代码后，如何获得使用循环学习率方法找到的最佳 LR 的学习率 (LR) 范围（最小值和最大值）？</title>
      <link>https://stackoverflow.com/questions/78368740/how-to-get-learning-rate-lr-bounds-min-and-max-values-wrt-optimal-lr-found-t</link>
      <description><![CDATA[我使用以下代码来获取给定神经网络模型的最佳学习率 - https://github.com/beringresearch/lrfinder/blob/master/lrfinder/lrfinder.py - 特别是通过 get_best_lr 函数。因此，在获得最佳学习率的值后，如何以编程方式找出使用循环学习率 (CLR) 方法找到的最佳 LR 的 LR 边界（最小值和最大值）值 (https://arxiv.org/abs/1506.01186)?
来自引用的 GitHub 存储库的代码：
导入数学

将 matplotlib.pyplot 导入为 plt
导入tensorflow.keras.backend为K
将 numpy 导入为 np

从tensorflow.keras.callbacks导入LambdaCallback


LRFinder 类：
    ”“”
    训练的循环学习率中详细介绍了学习率范围测试
    神经网络 作者：Leslie N. Smith。学习率范围测试是一个测试
    这提供了有关最佳学习率的有价值的信息。期间
    预训练运行时，学习率线性增加或
    两个边界之间呈指数关系。较低的初始学习率允许
    网络开始收敛，并且随着学习率的增加
    最终会太大并且网络会发散。
    ”“”

    def __init__(自我，模型)：
        self.model = 模型
        自我损失= []
        自我学习率 = []
        self.best_loss = 1e9

    def on_batch_end（自身，批次，日志）：
        lr = K.get_value(self.model.optimizer.lr)
        self.learning_rates.append（lr）

        损失=日志[&#39;损失&#39;]
        self.losses.append(损失)

        如果批次&gt; 5 且 (math.isnan(loss) 或 loss &gt; self.best_loss * 4)：
            self.model.stop_training = True
            返回

        如果损失&lt; self.best_loss：
            self.best_loss = 损失

        lr *= self.lr_mult
        K.set_value(self.model.optimizer.lr, lr)

    def find(自我, 数据集, start_lr, end_lr, epochs=1,
             steps_per_epoch=无，**kw_fit）：
        如果steps_per_epoch为None：
            引发异常（&#39;正确训练数据生成器，&#39;
                            “steps_per_epoch”不能为“None”。”
                            &#39;你可以将其计算为&#39;
                            &#39;`np.ceil(len(TRAINING_LIST) / BATCH)`&#39;)

        self.lr_mult = (浮点(end_lr) /
                        浮动（start_lr））**（浮动（1）/
                                             浮点数（纪元*steps_per_epoch））
        初始权重 = self.model.get_weights()

        Original_lr = K.get_value(self.model.optimizer.lr)
        K.set_value(self.model.optimizer.lr, start_lr)

        回调 = LambdaCallback(on_batch_end=lambda 批次,
                                  日志：self.on_batch_end（批次，日志））

        self.model.fit（数据集，
                       纪元=纪元，回调=[回调]，**kw_fit）
        self.model.set_weights(initial_weights)

        K.set_value(self.model.optimizer.lr,original_lr)

    def get_learning_rates(自我):
        返回（自我学习率）

    def get_losses(自身):
        返回（自我损失）

    def get_derivatives(self, sma):
        断言 sma &gt;= 1
        导数 = [0] * sma
        对于范围内的 i(sma, len(self.learning_rates))：
            衍生品.append((self.losses[i] - self.losses[i - sma]) / sma)
        回报衍生品

    def get_best_lr（自身，sma，n_skip_beginning = 10，n_skip_end = 5）：
        衍生品 = self.get_derivatives(sma)
        best_der_idx = np.argmin(导数[n_skip_beginning:-n_skip_end])
        返回 self.learning_rates[n_skip_beginning:-n_skip_end][best_der_idx]
]]></description>
      <guid>https://stackoverflow.com/questions/78368740/how-to-get-learning-rate-lr-bounds-min-and-max-values-wrt-optimal-lr-found-t</guid>
      <pubDate>Mon, 22 Apr 2024 20:33:17 GMT</pubDate>
    </item>
    <item>
      <title>语码转换/混合的情感分析[关闭]</title>
      <link>https://stackoverflow.com/questions/78368584/sentiment-analysis-of-code-switching-mixing</link>
      <description><![CDATA[Stack Overflow 社区您好，
我目前正在从事一个涉及代码切换/代码混合的情感分析的项目。我的目标是在检测和解释混合语言文本中的情感方面实现高精度。然而，我在寻找适合此任务的强大数据集方面面临着挑战。
有人对我可以用来训练代码转换或混合语言文本的情感分析的数据集和 ML/DL 模型有建议吗？此外，如果您有任何可以帮助提高此类分析准确性的提示或技术，我将非常感谢您的见解。
预先感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78368584/sentiment-analysis-of-code-switching-mixing</guid>
      <pubDate>Mon, 22 Apr 2024 19:50:03 GMT</pubDate>
    </item>
    <item>
      <title>即使添加更多层，训练损失也不会减少</title>
      <link>https://stackoverflow.com/questions/78368391/training-loss-does-not-decrease-even-when-adding-more-layers</link>
      <description><![CDATA[我正在使用 Keras 的顺序模型来训练包含约 10000 个数据点和 16 个特征的小型数据集。目标在 0 到 1 之间。理论上，当我们通过添加更多层来增加模型复杂性时，训练损失应该会减少，并且可能会过度拟合。就我而言，我使用均方误差，训练误差饱和在 0.14 左右（远未达到过拟合），这比 XGBoost、RandomForest 等误差接近 0 的其他算法要差得多。
如果您能给出一些可能的原因，我将不胜感激。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78368391/training-loss-does-not-decrease-even-when-adding-more-layers</guid>
      <pubDate>Mon, 22 Apr 2024 19:04:25 GMT</pubDate>
    </item>
    <item>
      <title>UnicodeEncodeError：“charmap”编解码器无法对位置 19-38 中的字符进行编码：字符映射到 <未定义></title>
      <link>https://stackoverflow.com/questions/78367946/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-19-38-c</link>
      <description><![CDATA[我正在开发一个基于 Flask 的 Web 应用程序，用户可以上传图像以使用机器学习模型进行预测。上传的图像存储在本地目录中，并使用预先训练的模型进行预测。然而，当我点击预测按钮时
是什么导致了这个 UnicodeEncodeError？
如何解决此问题以确保我的应用程序能够正确处理图像上传和预测？
是否有在 Flask 环境中处理字符编码的最佳实践，尤其是在 Windows 上？
==app.py====
@app.route(&#39;/uploadimage&#39;,methods=[&#39;GET&#39;, &#39;POST&#39;])
def upload_image():
    如果不是 session.get(&#39;logged_in&#39;):
        flash(&quot;您需要登录才能上传图片。&quot;, &quot;错误&quot;)
        返回重定向（url_for（&#39;登录&#39;））
    
    如果 request.method == &#39;POST&#39;:
        # 从表单中获取文件
        如果“my_image”不在 request.files 中：
            flash(“请求中没有文件部分。”,“错误”)
            返回重定向(request.url)

        文件 = request.files[&#39;my_image&#39;]

        # 检查文件是否上传
        if file.filename == &#39;&#39;:
            flash(“未选择文件。”, “错误”)
            返回重定向(request.url)

        # 验证文件类型（假设图像文件是图像）
        如果不是 file.filename.lower().endswith((&#39;.png&#39;, &#39;.jpg&#39;, &#39;.jpeg&#39;)):
            flash(“仅允许 PNG、JPG 或 JPEG 文件。”, “错误”)
            返回重定向(request.url)

        # 将文件保存到临时位置
        img_path = os.path.join(&#39;images&#39;, file.filename) # 您可能需要创建 &#39;temp&#39; 目录
        文件.保存（img_path）

        # 获取预测结果
        预测标签 = 预测标签(img_path)

        # 返回预测的标签和一条提示信息
        flash(f&quot;预测：{predicted_label}&quot;, &quot;成功&quot;)
        os.remove(img_path) # 处理后删除临时文件

        return redirect(request.url) # 重新加载页面以避免重新提交

    return render_template(&#39;uploadimage.html&#39;) # 对于 GET 请求，渲染表单

]]></description>
      <guid>https://stackoverflow.com/questions/78367946/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-19-38-c</guid>
      <pubDate>Mon, 22 Apr 2024 17:25:20 GMT</pubDate>
    </item>
    <item>
      <title>异特龙图书馆</title>
      <link>https://stackoverflow.com/questions/78367284/allosaurus-library</link>
      <description><![CDATA[我正在使用 Allosaurus 库，该库没有用于设置整个训练模型的批量大小的参数，在这种情况下我该怎么办。我们需要为此更改训练器代码吗？
请提出任何选择。我还想在 GPU 上执行特征提取。这怎么可能？]]></description>
      <guid>https://stackoverflow.com/questions/78367284/allosaurus-library</guid>
      <pubDate>Mon, 22 Apr 2024 15:19:18 GMT</pubDate>
    </item>
    <item>
      <title>最大似然估计初始参数问题</title>
      <link>https://stackoverflow.com/questions/78366722/maximum-likelihood-estimation-initial-parameters-issue</link>
      <description><![CDATA[我的数据集由 x 和 y 变量组成，我想执行最大似然估计 (MLE) 来拟合 sigmoid 均值函数 μ(X;β)=β0​+1+e−(X−β2​) β1​​ 和数据的线性标准偏差函数 σ(X;α)=α0​+α1​⋅X。我使用最大似然函数 L(θ∣X,Y)=i=1ΣN​logf(Yi​∣Xi​;θ) 估计 beta 和 alpha 参数，以便观察数据中的均值和 sigma 趋势。
可能性 = -np.sum(np.log(norm.pdf(Y, mu, sigma)))
最后，每次遇到以下情况时，使用 result = minusminimum(likelihood, initial_params, args=(X,Y), method=&#39;L-BFGS-B&#39;,options={&#39;maxiter&#39;: 100}) 执行似然优化当传递不同的初始值时获得不同的平均值和西格玛值的问题时，我发现我的模型对初始参数变得敏感。有什么方法可以解决这种情况，以便我可以获得自动适合我的模型的均值和西格玛的最佳值？
附：我也应用了不同的优化方法，但没有效果。
我通过其他技术实现它已经取得了所需的结果，但我想使用 MLE 来实现它。我该如何解决这个问题？
我尝试过不同的优化方法]]></description>
      <guid>https://stackoverflow.com/questions/78366722/maximum-likelihood-estimation-initial-parameters-issue</guid>
      <pubDate>Mon, 22 Apr 2024 13:47:56 GMT</pubDate>
    </item>
    <item>
      <title>用于回归问题的 PyTorch 模型，每个样本 4 个图像，图像之间有时间间隔</title>
      <link>https://stackoverflow.com/questions/78366460/pytorch-model-for-regression-problem-with-4-images-per-sample-with-time-gap-betw</link>
      <description><![CDATA[我正在使用一个数据集，其中每个样本对应于以已知延迟拍摄的 4 个图像，并且每组 4 个图像都有一个目标预测，该目标预测是一个数字（不是分类）。我目前已经制作了下面的模型，但它根本没有给出好的结果。有什么建议吗？
class SimpleModel(nn.Module)：
    def __init__(自身):
        超级(SimpleModel, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(8)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout1 = nn.Dropout(p=0.25)
        
        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(16)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout2 = nn.Dropout(p=0.25)
        
        self.conv3 = nn.Conv2d（in_channels = 16，out_channels = 32，kernel_size = 3，stride = 1，padding = 1）
        self.bn3 = nn.BatchNorm2d(32)
        self.pool3 = nn.MaxPool2d(kernel_size=5, stride=2)
        self.dropout3 = nn.Dropout(p=0.25)
        
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28800, 512)
        self.dropout4 = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(512, 1) # 单输出

    def 前向（自身，x）：
        x = torch.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        x = self.dropout1(x)
        
        x = torch.relu(self.bn2(self.conv2(x)))
        x = self.pool2(x)
        x = self.dropout2(x)
        
        x = torch.relu(self.bn3(self.conv3(x)))
        x = self.pool3(x)
        x = self.dropout3(x)
        
        x = self.展平(x)
        x = torch.relu(self.fc1(x))
        x = self.dropout4(x)
        
        x = self.fc2(x) # 输出层，无回归激活函数
        返回x

此外，目标预测值通常非常小，有时甚至大得多，例如从 1e-9 到 1e2 左右。我已将对数刻度应用于目标预测，以减少这种影响，以尝试改进学习，但不确定它有多大帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78366460/pytorch-model-for-regression-problem-with-4-images-per-sample-with-time-gap-betw</guid>
      <pubDate>Mon, 22 Apr 2024 13:04:42 GMT</pubDate>
    </item>
    <item>
      <title>预测值与目标值/实际值之间没有相关性[关闭]</title>
      <link>https://stackoverflow.com/questions/78365619/no-correlation-between-predicted-values-and-target-value-real-values</link>
      <description><![CDATA[我正在执行回归任务。当我绘制预测值与实际值时，我发现变量之间没有相关性。我猜这意味着模型无法拟合数据（类似于分类模型预测最常见的类别）。


我尝试了很多方法，但没有一个能够使模型适合数据：

我尝试用对数函数转换目标值：

y_train = np.log(y_train)
y_test = np.log(y_test)



我对目标变量应用了平方根函数，但它不起作用：

y_train = (y_train)**0.5
y_测试 = (y_测试)**0.5



我什至尝试标准化目标函数，但也不起作用

def preprocess_data_standard_regression(数据):
    定标器=标准定标器()
    X = data[[data.columns 中的 col 的 col
              如果不是 col.startswith(&quot;POSTOP_&quot;)
              并且 col !=“in_患者_id”
              和 col !=“in_laterity”]]
    y = 数据[“POSTOP_MAN_vault_posto”]
    y = scaler.fit_transform(y.values.reshape(-1,1)).flatten()
    缩放器 = MinMaxScaler()
    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)
    X_train = 缩放器.fit_transform(X_train)
    X_test = 缩放器.transform(X_test)
    返回 X_train、X_test、y_train、y_test


我的数据集的形状是 545 行 vs 24 列。]]></description>
      <guid>https://stackoverflow.com/questions/78365619/no-correlation-between-predicted-values-and-target-value-real-values</guid>
      <pubDate>Mon, 22 Apr 2024 10:41:06 GMT</pubDate>
    </item>
    <item>
      <title>多类问题的层次分类方法</title>
      <link>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</link>
      <description><![CDATA[有一个多类分类任务。我的目标是使用每父节点本地分类器 (LCPN) 方法来解决这个问题。
让我解释一下如何使用 MWE。
假设我有这个虚拟数据集：
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification
从 scipy.cluster 导入层次结构

X, y = make_classification(n_samples=1000, n_features=10, n_classes=5,
                             n_信息=4）

我想出了这些类之间的距离矩阵：
d = np.array(
[[ 0.、201.537、197.294、200.823、194.517]、
 [201.537, 0., 199.449, 202.941, 196.703],
 [197.294, 199.449, 0., 198.728, 192.354],
 [200.823, 202.941, 198.728, 0., 195.972],
[[194.517, 196.703, 192.354, 195.972, 0.]]
）

因此，我确定了类层次结构，如下所示：
hc = hierarchy.linkage(d, method=&#39;complete&#39;)

得到的树状图如下：
dendrogram = hierarchy.dendrogram(hc, labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;, &#39;D&#39;, &#39;F&#39;])
树状图


我使用hierarchy.to_tree()以树状结构进行说明：

我的问题：
如何按照 LCPN 方法在每个内部节点（包括根）处安装分类器，例如 DecisionTreeClassifier 或 SVM，以像在树中一样进行上图？]]></description>
      <guid>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</guid>
      <pubDate>Sat, 20 Apr 2024 14:08:05 GMT</pubDate>
    </item>
    <item>
      <title>在weka中重新采样过滤器</title>
      <link>https://stackoverflow.com/questions/78356992/resample-filter-in-weka</link>
      <description><![CDATA[我的数据集中的数据实例数量很少。所以，我尝试了“重新采样” Weka中的过滤器可以增加数据量，从而提高模型性能。样本量百分比设置为200可以吗？因为那时我在交叉验证测试中获得了良好的相关系数。
我想知道将样本大小百分比设置为 200 时，重新采样过滤器是否工作正常。
使用此过滤器后，我的模型会准确预测吗？
由于数据量较少，是否有其他增强方法可以增强模型的性能？]]></description>
      <guid>https://stackoverflow.com/questions/78356992/resample-filter-in-weka</guid>
      <pubDate>Sat, 20 Apr 2024 04:29:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么数组形状会出现这个错误，有其他解决方案吗？</title>
      <link>https://stackoverflow.com/questions/75301595/why-does-this-error-of-array-shape-is-there-other-solutions</link>
      <description><![CDATA[我不断收到此错误，但我通过重塑数组解决了问题：data = data.reshape(-1, 1)
我的输出：
回溯（最近一次调用最后一次）：
  文件“C:\Users\USER\Desktop\python\machine-learning\bot4.py”，第 93 行，在  中
    预测 = model.predict(data)
  文件“C:\Users\USER\Desktop\python\machine-learning\machine-learningVenv\lib\site-packages\sklearn\naive_bayes.py”，第 105 行，在预测中
    X = self._check_X(X)
  文件“C:\Users\USER\Desktop\python\machine-learning\machine-leaningVenv\lib\site-packages\sklearn\naive_bayes.py”，第 579 行，位于 _check_X 中
    返回 self._validate_data(X,accept_sparse=“csr”,reset=False)
  文件“C:\Users\USER\Desktop\python\machine-learning\machine-learningVenv\lib\site-packages\sklearn\base.py”，第 546 行，位于 _validate_data
    X = check_array(X, input_name=“X”, **check_params)
  文件“C:\Users\USER\Desktop\python\machine-learning\machine-learningVenv\lib\site-packages\sklearn\utils\validation.py”，第 902 行，在 check_array 中
    引发值错误（
ValueError：需要 2D 数组，却得到 1D 数组：
array=[&#39;猫正在阳光下睡觉。&#39; “狗对着月亮狂吠。”]。
如果数据具有单个特征，则使用 array.reshape(-1, 1) 重塑数据；如果数据包含单个样本，则使用 array.reshape(1, -1) 重塑数据。

我期待输出：
[{“猫”:“睡觉”,“狗”:“吠叫”}]]]></description>
      <guid>https://stackoverflow.com/questions/75301595/why-does-this-error-of-array-shape-is-there-other-solutions</guid>
      <pubDate>Tue, 31 Jan 2023 18:27:25 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“tensorflow.keras”的模块，库不匹配</title>
      <link>https://stackoverflow.com/questions/72409779/modulenotfounderror-no-module-named-tensorflow-keras-libraries-mismatching</link>
      <description><![CDATA[软件包1
packages2
这些是我在 anaconda 上的包。我在最后一张照片上收到此错误。我尝试了 stackoverflow 和 github 上的几乎所有内容。我尝试了各种方法来导入 keras 和 tensorflow 。我降级了tensorflow，keras，但我遇到了任何其他错误，例如numpy不兼容等。我将numpy降级为，但这次keras需要更高的版本。
从 keras.preprocessing.text 导入分词器
/从keras.preprocessing.sequence导入pad_sequences
这些是我尝试导入的行。
导入keras
回溯（最近一次调用最后一次）：

  在&lt;单元格行：1&gt;中输入[6]
    导入keras

   中的文件 ~\anaconda3\lib\site-packages\keras\__init__.py:3
    从 。导入实用程序

   中的文件 ~\anaconda3\lib\site-packages\keras\utils\__init__.py:26
    从 .vis_utils 导入 model_to_dot

   中的文件 ~\anaconda3\lib\site-packages\keras\utils\vis_utils.py:7
    从 ..models 导入模型

   中的文件 ~\anaconda3\lib\site-packages\keras\models.py:12
    从 .engine.training 导入模型

   中的文件 ~\anaconda3\lib\site-packages\keras\engine\__init__.py:7
    从 .network 导入 get_source_inputs

   中的文件 ~\anaconda3\lib\site-packages\keras\engine\network.py:15
    从 。进口储蓄

   中的文件 ~\anaconda3\lib\site-packages\keras\engine\ saving.py:21
    从 .. 导入优化器

   中的文件 ~\anaconda3\lib\site-packages\keras\optimizers\__init__.py:1
    从tensorflow.keras.optimizers导入*

ModuleNotFoundError：没有名为“tensorflow.keras”的模块
]]></description>
      <guid>https://stackoverflow.com/questions/72409779/modulenotfounderror-no-module-named-tensorflow-keras-libraries-mismatching</guid>
      <pubDate>Fri, 27 May 2022 18:37:11 GMT</pubDate>
    </item>
    <item>
      <title>当权重存在时，glmnet 如何标准化变量？</title>
      <link>https://stackoverflow.com/questions/41122803/how-does-glmnet-standardize-variables-when-weights-are-present</link>
      <description><![CDATA[glmnet 允许用户通过 weights 参数输入观察权重向量。 glmnet 还标准化（默认）预测变量以具有零均值和单位方差。我的问题是：当提供 weights 时，glmnet 是否使用每列的加权平均值（和标准差）或未加权平均值（和标准差）来标准化预测变量？]]></description>
      <guid>https://stackoverflow.com/questions/41122803/how-does-glmnet-standardize-variables-when-weights-are-present</guid>
      <pubDate>Tue, 13 Dec 2016 13:50:42 GMT</pubDate>
    </item>
    <item>
      <title>pyspark：名称错误：名称“spark”未定义</title>
      <link>https://stackoverflow.com/questions/39541204/pyspark-nameerror-name-spark-is-not-defined</link>
      <description><![CDATA[我是从官方文档网站复制pyspark.ml示例：
http://spark.apache.org /docs/latest/api/python/pyspark.ml.html#pyspark.ml.Transformer
data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),(Vectors.dense([9.0, 8.0]),) , (Vectors.dense([8.0, 9.0]),)]
df = Spark.createDataFrame(数据, [“特征”])
kmeans = KMeans(k=2, 种子=1)
模型 = kmeans.fit(df)

但是，上面的示例无法运行并给出以下错误：

&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
NameError Traceback（最近一次调用最后一次）
&lt;ipython-input-28-aaffcd1239c9&gt;在&lt;模块&gt;()中
      1 从 pyspark 导入 *
      2 数据 = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),(Vectors.dense([9.0, 8.0]),), (Vectors.dense ([8.0, 9.0]),)]
----&gt; 3 df = Spark.createDataFrame(数据, [“特征”])
      4 kmeans = KMeans(k=2, 种子=1)
      5 模型 = kmeans.fit(df)

NameError：名称“spark”未定义

需要设置哪些附加配置/变量才能运行示例？]]></description>
      <guid>https://stackoverflow.com/questions/39541204/pyspark-nameerror-name-spark-is-not-defined</guid>
      <pubDate>Fri, 16 Sep 2016 23:05:11 GMT</pubDate>
    </item>
    </channel>
</rss>