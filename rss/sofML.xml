<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 19 Mar 2024 21:11:52 GMT</lastBuildDate>
    <item>
      <title>我应该选择哪种机器学习方法？</title>
      <link>https://stackoverflow.com/questions/78189637/which-machine-learning-method-should-i-choose</link>
      <description><![CDATA[我请求帮助=!!表中有数据 |时间|-|罐压|.传感器以 0.05 秒的增量发送压力读数信号。该表适用于常规情况和紧急情况（当压力超出允许标准时）。我想训练该模型，使其能够识别紧急情况和相关信号。我应该选择哪种方法，该怎么做？该模型将是一种传入数据流的过滤器
我通过pandas编译了一个数据集，并使用googlcolab进行训练]]></description>
      <guid>https://stackoverflow.com/questions/78189637/which-machine-learning-method-should-i-choose</guid>
      <pubDate>Tue, 19 Mar 2024 21:04:58 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit 学习回归</title>
      <link>https://stackoverflow.com/questions/78189577/using-scikit-learn-regression</link>
      <description><![CDATA[我正在尝试使用如下所示的简单函数来学习scikit-learn回归：
导入 matplotlib.pyplot 作为 plt
将 numpy 导入为 np
从 sklearn.ensemble 导入 GradientBoostingRegressor、RandomForestRegressor
从 sklearn.gaussian_process 导入 GaussianProcessRegressor
从 sklearn.gaussian_process.kernels 导入 RBF
从 sklearn.metrics 导入mean_squared_error
从 sklearn.model_selection 导入 train_test_split
从 sklearn.multioutput 导入 MultiOutputRegressor
从 sklearn.neural_network 导入 MLPRegressor
从 sklearn.svm 导入 SVR


def func(x: np.ndarray):
    # x (N, 24), y (N, 2)
    x_mean = np.mean(x, 轴=1)
    a = np.sin(x_mean)
    b = np.cos(x_mean)
    y = np.array([a, b]).T
    返回y


np.随机.种子(0)
x = np.random.rand(1000, 24) * np.pi * 10
y = 函数(x)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

型号=[
    梯度提升回归器（），
    随机森林回归器(),
    高斯过程回归器(RBF()),
    MLPRegressor(max_iter=1000),
    支持向量机（），
]

plt.figure(figsize=(5, len(模型) * 3))

对于 i，枚举（模型）中的 base_model：
    名称 = 基础模型.__class__.__name__
    模型 = MultiOutputRegressor(base_model)
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    mse = 均方误差(y_test, y_pred)

    x_mean = np.mean(x_test, 轴=1)
    plt.subplot(len(模型), 1, i + 1)
    标题 = f&quot;{名称}. MSE: {mse:.2e}”
    打印（标题）
    plt.标题（标题）
    plt.plot(x_mean, y_test[:, 0], “.”, label=“y_test[:, 0]”)
    plt.plot(x_mean, y_test[:, 1], “.”, label=“y_test[:, 1]”)
    plt.plot(x_mean, y_pred[:, 0], “.”, label=“y_pred[:, 0]”)
    plt.plot(x_mean, y_pred[:, 1], “.”, label=“y_pred[:, 1]”)
    plt.图例()

plt.tight_layout()
plt.savefig(“regression_test.png”)


但是结果并不好：

我认为我没有正确使用这些回归器。
我应该如何修改我的代码？
更新
通过减少 x 的暗度，我得到了一些好的结果：
我应该如何更改 GP 的代码以支持高亮度输入？
]]></description>
      <guid>https://stackoverflow.com/questions/78189577/using-scikit-learn-regression</guid>
      <pubDate>Tue, 19 Mar 2024 20:49:54 GMT</pubDate>
    </item>
    <item>
      <title>序列分类中的通道重要性</title>
      <link>https://stackoverflow.com/questions/78189215/channel-importance-in-sequence-classification</link>
      <description><![CDATA[我有一个 ONNX 模型，它接受输入 [1, 35, 4]，即 [batch_size, num_channels, seq_len]，并输出 [1 , 3]，即[batch_size, num_classes]。我需要的是为每个通道分配一个数字，告诉我它对于模型做出的预测有多重要。我想我会使用 shap 的 PermutationExplainer 来达到此目的，但我不确定如何让它知道我不关心 seq_len&lt; /代码&gt;.
我尝试这样做：
导入 onnxruntime 作为 ort
导入形状
将 numpy 导入为 np

# 加载 ONNX 模型
model_path = &#39;解释示例.onnx&#39;
sess = ort.InferenceSession(model_path)

# 定义模型函数来处理批处理
定义模型(x):
    x = x.reshape(-1, 35, 4).astype(np.float32)
    # 模型期望输入 [1, 35, 4] 并返回 [1, 3]
    输出 = [sess.run(None, {&#39;input&#39;: x[i:i+1]})[0] for i in range(x.shape[0])]
    返回 np.concatenate(输出，轴=0)

# 创建样本输入
X = np.random.rand(1000, 35, 4).astype(np.float32)

输出名称 = [f&quot;输出_{i}&quot;;对于范围 (3) 内的 i]
feature_names = [f“频道 {i}”对于我在范围（35）]

def masker_fn(掩码, x):
    # 问题，掩码形状为 (140,)，x 形状为 (35, 4)
    masked_x = x.copy()
    对于范围内的 i(x.shape[1])：
        如果掩码[i] == 0：
            masked_x[:, i, :] = 0
    返回 masked_x

# 使用通道的自定义掩码声明解释器
解释器= shap.PermutationExplainer（模型，masker_fn，feature_names=feature_names，output_names=output_names）

# 计算形状值
shap_values = 解释器(X)

但问题是发送到掩码器的掩码具有形状 (140,) 而不是 (35,)。我当然可以以某种方式合并跨 seq_len 维度的掩码，但我认为 1. 解释器完成了不必要的工作，2. 也许这会以某种方式破坏其内部算法。
第一个问题：我怎样才能正确地告诉它，不，没有 140 个功能，而是 35 个？
第二个问题：你会如何解决这个问题？您是否有推荐的不同/更好/实际关心的图书馆？我正在考虑编写一个自定义排列解释器，但如果存在更好的可用解决方案，为什么还要麻烦呢。]]></description>
      <guid>https://stackoverflow.com/questions/78189215/channel-importance-in-sequence-classification</guid>
      <pubDate>Tue, 19 Mar 2024 19:34:59 GMT</pubDate>
    </item>
    <item>
      <title>R 中插入符号的训练函数中的中心和比例因子预测器</title>
      <link>https://stackoverflow.com/questions/78189166/center-and-scale-factor-predictors-within-train-function-from-caret-in-r</link>
      <description><![CDATA[我在使用 R caret 包的 train 函数的 preProc 参数方面遇到问题。我想居中并缩放我的预测变量，但忽略因子列。当我在火车之外进行预处理时，它工作正常，但我希望在火车功能内进行预处理。我错过了什么吗？
下面是一个示例，其中在训练之外使用 preProcess 时忽略因子预测器。
df &lt;- data.frame(
    分数 = runif(1000, 80, 110),
    var1 = as.factor(样本(0:1, 1000, 替换 = TRUE)),
    var2 = runif(1000, 5, 25)
）
预处理(df[-1], method=c(&#39;center&#39;,&#39;scale&#39;))

由 1000 个样本和 2 个变量创建

预处理：
  - 居中 (1)
  - 被忽略 (1)
  - 缩放 (1)

这是我在火车内部使用 preProc 时发生的情况
df &lt;- data.frame(
    分数 = runif(1000, 80, 110),
    var1 = as.factor(样本(0:1, 1000, 替换 = TRUE)),
    var2 = runif(1000, 5, 25)
）
mod &lt;- 训练（分数〜.，数据= df，
             方法=“lm”，
             preProc = c(“中心”,“比例”))
mod$预处理

由 1000 个样本和 2 个变量创建

预处理：
  - 居中 (2)
  - 被忽略 (0)
  - 缩放 (2)

提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78189166/center-and-scale-factor-predictors-within-train-function-from-caret-in-r</guid>
      <pubDate>Tue, 19 Mar 2024 19:25:11 GMT</pubDate>
    </item>
    <item>
      <title>测试数据是否应该与训练数据唯一不同并且来自不同的源/数据集？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78188497/should-the-testing-data-be-uniquely-distinct-and-come-from-different-source-data</link>
      <description><![CDATA[我正在使用 CNN 构建音频分类系统。我的数据集由我录制的不同音频组成，并拼接为相同的时间长度。与任何其他常见的 ML 或 DL 任务一样，我要将数据集拆分为训练数据和测试数据，并评估我的模型。然而，我的教授说测试数据一定不能来自与训练数据相同的环境/来源。由于我录制了所有音频样本并将它们拼接并分割为训练和测试数据，因此它们本质上来自相同的环境。他基本上认为测试数据本质上必须是“在野外”。
但是，我对此的看法是，只要我的模型没有训练并且在模型拟合期间没有将测试数据作为输入，那么测试数据就被认为是“看不见的”和“野外的”。有人可以指出我正确的方向吗？]]></description>
      <guid>https://stackoverflow.com/questions/78188497/should-the-testing-data-be-uniquely-distinct-and-come-from-different-source-data</guid>
      <pubDate>Tue, 19 Mar 2024 17:11:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么具有 Cartpole 健身房环境的 stable_baselines3 模型通过 sutton_barto_reward 提高了平均剧集奖励？</title>
      <link>https://stackoverflow.com/questions/78188062/why-is-my-stable-baselines3-model-with-cartpole-gym-environment-improving-mean-e</link>
      <description><![CDATA[当我运行此代码时，我看到剧集长度平均值不断增加（就像一个好的模型应该的那样），而剧集平均奖励保持在 -1 不变，这就是 sutton_barto_reward 系统的工作原理。
导入体育馆
从 cartpole 导入 CartPoleEnv
从 stable_baselines3 导入 PPO
从 stable_baselines3.ppo.policies 导入 MlpPolicy

env = CartPoleEnv(sutton_barto_reward=True)

模型 = PPO(“MlpPolicy”, env, gamma=1, verbose=1)
model.learn(total_timesteps=30000)


但是，我不明白为什么会这样，因为在gymnasium的GitHub文档中的Cartpole代码中似乎没有使用任何折扣率。既然剧集的累积奖励始终相同，那么剧集长度平均值难道不应该没有任何改善吗？]]></description>
      <guid>https://stackoverflow.com/questions/78188062/why-is-my-stable-baselines3-model-with-cartpole-gym-environment-improving-mean-e</guid>
      <pubDate>Tue, 19 Mar 2024 16:02:07 GMT</pubDate>
    </item>
    <item>
      <title>关于空闲时间的数据集？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78187717/data-set-about-free-time</link>
      <description><![CDATA[我是这里的初学者:)。做了 cs50p，一个项目，现在我想深入 ML 领域。
我想到了一个项目 - 一种用于组织与朋友外出活动的人工智能机器人。其中一部分是实际创建，甚至搜索（如果有的话）预先标记的数据集（这就是 chatgpt 告诉我的 - 我是全新的）。
所以问题是，我在哪里可以找到它，或者如何创建它？任何相关资源都会有很大帮助。
谢谢！
我刚刚开始潜水，甚至不知道该怎么做:)）））]]></description>
      <guid>https://stackoverflow.com/questions/78187717/data-set-about-free-time</guid>
      <pubDate>Tue, 19 Mar 2024 15:11:34 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 - scikit-learn 代码序列正确吗？</title>
      <link>https://stackoverflow.com/questions/78187495/machine-learning-scikit-learn-code-sequnce-correct</link>
      <description><![CDATA[我已经构建了一个包含一些转换的管道并训练了一个 SVC 分类器。代码中步骤的顺序是否正确？
我正在使用此处找到的processed.cleveland.data数据集：https： //archive.ics.uci.edu/dataset/45/heart+disease。
导入 pandas 作为 pd
将 numpy 导入为 np
导入操作系统
从 pathlib 导入路径

从 sklearn.model_selection 导入 train_test_split
从 sklearn.model_selection 导入 StratifiedKFold
从 sklearn.model_selection 导入 cross_val_score

从 sklearn.compose 导入 ColumnTransformer
从 sklearn.pipeline 导入管道
从 sklearn.preprocessing 导入 OneHotEncoder
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.impute 导入 SimpleImputer

从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.svm 导入 SVC
url =“C:/Users/.../processedcleveland.data”
名称 = [&#39;年龄&#39;, &#39;性别&#39;, &#39;cp&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;exang&#39;, &#39;oldpeak&#39;, &#39;slope&#39;, &#39;ca&#39; , &#39;thal&#39;, &#39;num&#39;]
def getData():
        返回 pd.read_csv(url, sep=&#39;,&#39;, 名称=名称)

输入 = 获取数据()
打印（输入.info（））
打印（输入.描述（））

数组=输入.值
X = 数组[:,0:13]
y = 数组[:,13]

dataframe = pd.DataFrame.from_records(X)
数据帧[[1, 2, 5, 6, 8]] = 数据帧[[1, 2, 5, 6, 8]].astype(str)

打印(dataframe.info())

numeric_ix = dataframe.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns
categorical_ix = dataframe.select_dtypes(include=[&#39;object&#39;, &#39;bool&#39;]).columns

打印（数字_ix）
打印（分类_ix）
&#39;&#39;&#39;
t = [(&#39;cat0&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;), [1, 2, 5, 6, 8]), (&#39;cat1&#39;, OneHotEncoder(), categorical_ix), (&#39;num0&#39;, SimpleImputer(strategy) =&#39;中位数&#39;), numeric_ix), (&#39;num1&#39;, MinMaxScaler(), numeric_ix)]
col_transform = ColumnTransformer(变压器=t)

管道 = 管道(步骤=[(&#39;t&#39;, col_transform)])
# 将管道拟合到转换后的数据上
结果 = pipeline.fit_transform(dataframe)

打印（类型（pd.DataFrame.from_records（结果）））
打印（pd.DataFrame.from_records（结果）.to_string（））
&#39;&#39;&#39;
X_train、X_validation、Y_train、Y_validation = train_test_split(X、y、test_size=0.20、random_state=1)


categorical_impute = 管道([
    （“mode_impute”，SimpleImputer（missing_values = np.nan，策略=&#39;most_frequent&#39;）），
    (“one_hot”, OneHotEncoder())
]）

numeric_impute = 管道([
    （“num_mode_impute”，SimpleImputer（missing_values = np.nan，策略=&#39;中位数&#39;）），
    (“min_max”, StandardScaler())
]）

预处理器 = ColumnTransformer([
    (“cat_impute”, categorical_impute, categorical_ix),
    (“num_impute”, numeric_impute, numeric_ix)
]，余数=“直通”）


模型 = SVC(伽玛=&#39;自动&#39;)

kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)

pipeline = Pipeline(steps=[(&#39;prep&#39;, 预处理器), (&#39;m&#39;, model)])

cv_results = cross_val_score(管道, X_train, Y_train, cv=kfold, 评分=&#39;准确度&#39;)
print(&#39;%s: %f (%f)&#39; % (&quot;SVC: &quot;, cv_results.mean(), cv_results.std()))
# 结果 = preprocessor.fit_transform(dataframe)
# print(pd.DataFrame.from_records(结果).to_string())


分类器的效率很低。有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78187495/machine-learning-scikit-learn-code-sequnce-correct</guid>
      <pubDate>Tue, 19 Mar 2024 14:38:47 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch LSTM 不使用隐藏层</title>
      <link>https://stackoverflow.com/questions/78187211/pytorch-lstm-not-using-hidden-layer</link>
      <description><![CDATA[我正在使用 PyTorch 的 LSTM api，但遇到了一些问题。我使用 LSTM 作为虚拟 AI 模型。该模型的任务是如果前一个数字小于当前数字，则返回 1。
因此，对于像 [0.7, 0.3, 0.9, 0.99] 这样的数组，预期输出是 [1.0, 0.0, 1.0, 1.0]。无论如何，第一个输出应该是 1.0。
我设计了以下网络来尝试这个问题：
# network.py

进口火炬

N_输入 = 1
N_STACKS = 1
N_隐藏 = 3

LR = 0.001


网络类（torch.nn.Module）：

    # 参数：自身
    def __init__(自身):
        超级（网络，自我）.__init__()

        self.lstm = torch.nn.LSTM(
            输入大小=N_INPUT，
            隐藏大小=N_HIDDEN,
            num_layers=N_STACKS,
        ）

        self.线性 = torch.nn.Linear(N_HIDDEN, 1)
        self.relu = torch.nn.ReLU()

        self.optim = torch.optim.Adam(self.parameters(), lr=LR)
        self.loss = torch.nn.MSELoss()

    # 参数：自身、预测、预期
    def backprop(self, xs, es):

        # 执行反向传播
        self.optim.zero_grad()
        l = self.loss(xs, torch.tensor(es))
        l.backward()
        自我优化.step()

        返回l

    # params: self, data (作为Python数组)
    def 前向（自身，数据）：

        out, _ = self.lstm(torch.tensor(dat))

        输出 = self.relu(输出)
        输出 = 自.线性(输出)

        返回

我从这个文件中调用它：
# main.py

进口网络

将 numpy 导入为 np

# 创建一个新网络
n: 网络.网络 = 网络.网络()


# 创建一些数据
def rand_array():

    # 一堆随机数
    a = [[np.random.uniform(0, 1)] for i in range(1000)]

    # 现在，如果前一个数字更大，我们的预期值为 0，否则为 1
    预期 = [0.0 如果 a[i - 1][0] &gt; a[i][0] 否则 1.0 for i in range(len(a))]
    Expected[0] = 1.0 # 使第一个元素始终为 1.0

    返回[a，预期]


# 一堆随机数组
数据 = [rand_array() for i in range(1000)]

# 100 个纪元
对于范围（100）内的 i：
    对于数据中的 i：

        预测 = n(i[0])
        损失 = n.backprop(pred, i[1])
        print(&quot;损失: {:.5f}&quot;.format(loss))

现在，当我运行这个程序时，我只是得到了大约 0.25 的损失，而且一旦到达那里，它就不会真正改变。我认为该模型只是为每个输入选取 0 和 1 (0.5) 的平均值。
这让我相信模型无法看到以前的数据；数据只是随机数（尽管预期输出基于这些随机数），并且模型不记得之前发生了什么。
我的问题是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78187211/pytorch-lstm-not-using-hidden-layer</guid>
      <pubDate>Tue, 19 Mar 2024 13:59:08 GMT</pubDate>
    </item>
    <item>
      <title>这样分割数据会不会造成数据泄露？</title>
      <link>https://stackoverflow.com/questions/78187069/is-there-data-leakage-while-splitting-the-data-this-way</link>
      <description><![CDATA[以这种方式分割数据集时是否存在数据泄漏的可能性：
def split_dataset(ds, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=True):
    # 获取数据集大小
    数据集大小 = len(ds)

    # 计算分割大小
    train_size = int(train_ratio * dataset_size)
    val_size = int(val_ratio * 数据集大小)
    测试大小 = 数据集大小 - 训练大小 - val_size

    # 如果需要的话，打乱数据集
    如果随机播放：
        ds = ds.shuffle(dataset_size)

    # 分割数据集
    train_dataset = ds.take(train_size)
    val_dataset = ds.skip(train_size).take(val_size)
    test_dataset = ds.skip(train_size + val_size).take(test_size)

    返回train_dataset、val_dataset、test_dataset

训练深度学习模型
只是对模型的准确率感到惊讶，它超过了 98%。]]></description>
      <guid>https://stackoverflow.com/questions/78187069/is-there-data-leakage-while-splitting-the-data-this-way</guid>
      <pubDate>Tue, 19 Mar 2024 13:37:37 GMT</pubDate>
    </item>
    <item>
      <title>多类数据集预处理出现“nan”错误</title>
      <link>https://stackoverflow.com/questions/78186997/multiclass-dataset-preprocessing-gave-nan-error</link>
      <description><![CDATA[在预处理我面临的数据集时
&lt;前&gt;&lt;代码&gt;DF
    ID 年龄 性别 身高 体重 BMI 标签
0 1 25 男性 175.0 80 25.3 正常体重
1 2 30 女性 160.0 60 22.5 正常体重
2 3 35 男性 180.0 90 27.3 超重
3 4 40 女性 150.0 50 20.0 体重不足
4 5 45 男性 190.0 100 31.2 肥胖
…………………………
103 106 11 男性 175.0 10 3.9 体重不足
104 107 16 女性 160.0 10 3.9 体重不足
105 108 21 男性 180.0 15 5.6 体重不足
106 109 26 女性 150.0 15 5.6 体重不足
107 110 31 男性 190.0 20 8.3 体重不足


DF.isnull().sum()

DF[&#39;身高&#39;]=DF[&#39;身高&#39;].fillna(DF[&#39;身高&#39;].median())

most_frequent_category = DF[&#39;性别&#39;].mode().iloc[0]
DF[&#39;性别&#39;].fillna(most_frequent_category, inplace = True)

从 sklearn.preprocessing 导入 LabelEncoder
le = 标签编码器()

DF[&#39;性别&#39;]=le.fit_transform(DF[&#39;性别&#39;])
DF[&#39;标签&#39;]=le.fit_transform(DF[&#39;标签&#39;])

value_count = DF.groupby(&#39;Label&#39;).size().reset_index(name = &#39;count&#39;)
打印（值_计数）

z_score = (DF[&#39;年龄&#39;]-DF[&#39;年龄&#39;].mean())/DF[&#39;年龄&#39;].std()
打印（z_分数）

对于 z_score 中的 i：
  如果我&lt;-3：
    print (“我们有异常值”, i)
  elif i&gt;3：
    print (“我们有异常值”, i)
  别的：
    继续

将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns

sns.set(font_scale = 2) #将可视化的字体大小固定为2

plt.子图（图大小=（15,15））

heat_plot = sns.heatmap(DF.corr(method=&#39;pearson&#39;), annot=True, cmap=&#39;Spectral&#39;,annot_kws={&#39;size&#39;:20})

plt.yticks（字体大小= 15）
plt.xticks（字体大小= 15）
plt.show()

相关性 = DF.corr(method=&#39;pearson&#39;)
print(correlations[&#39;Label&#39;].sort_values(ascending = False).to_string())

DF[&#39;标签&#39;].iloc[0:150]

从 sklearn.utils 导入洗牌
shuffled_DF = 洗牌(DF)

reranged_DF = shuffled_DF.reset_index(drop = True)
X = reranged_DF.drop(columns=[&#39;Label&#39;])
y = 重新排列_DF[&#39;标签&#39;]

从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.preprocessing 导入 StandardScaler

定标器=标准定标器()
Standard_Scaled_X = 缩放器.fit_transform(X)
打印（Standard_Scaled_X）

从 sklearn.model_selection 导入 train_test_split

x_train、x_test、y_train、y_test = train_test_split（Standard_Scaled_X、y、test_size = 0.20）


print(&quot;训练数据大小（特征）：&quot;, len(x_train))
print(&quot;训练数据大小（目标）：&quot;, len(y_train))

print(&quot;测试数据大小(特征):&quot;, len(x_test))
print(&quot;测试数据大小（目标）：&quot;, len(y_test))

打印（x_train）

从 sklearn.naive_bayes 导入 GaussianNB
NB_Classifier = GaussianNB(先验=无，var_smoothing=1e-09)

从 sklearn.model_selection 导入 cross_val_score
从 sklearn.model_selection 导入 KFold

k_fold = KFold(10)

准确度 = cross_val_score(NB_Classifier, x_train,y_train, cv = k_fold, 评分 = &#39;准确度&#39;)
精度 = cross_val_score(NB_Classifier, x_train,y_train, cv = k_fold, 评分 = &#39;精度&#39;)
召回= cross_val_score（NB_Classifier，x_train，y_train，cv = k_fold，评分=&#39;召回&#39;）
f1_score = cross_val_score(NB_Classifier, x_train,y_train, cv = k_fold, 评分 = &#39;f1&#39;)
AUC = cross_val_score(NB_Classifier, x_train,y_train, cv = k_fold, 评分 = &#39;roc_auc&#39;)

ValueError：目标是多类，但平均值=“二进制”。请选择其他
平均设置，[无、&#39;微观&#39;、&#39;宏观&#39;、&#39;加权&#39;]之一。

打印（准确度）
总体准确度 = sum(准确度)/len(准确度)

打印（总体准确率）

打印（精度）
总体精度 = sum(精度)/len(精度)
打印（总体精度）

输出：南

我必须在这里添加整个代码。
准确度值给了我正确的输出，但精度/召回/f1_score/AUC 值给了我 nan 作为输出。我该如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78186997/multiclass-dataset-preprocessing-gave-nan-error</guid>
      <pubDate>Tue, 19 Mar 2024 13:29:17 GMT</pubDate>
    </item>
    <item>
      <title>如何下载 EMNIST Letters 数据集</title>
      <link>https://stackoverflow.com/questions/78186856/how-to-downlad-emnist-letters-dataset</link>
      <description><![CDATA[# 导入库
import numpy as np # 更正缩进和拼写
进口火炬
将 torch.nn 导入为 nn
import torch.nn.functionas F # 修正了额外的空间
from torch.utils.data import DataLoader, TensorDataset # 删除多余的点
导入副本
from sklearn.model_selection import train_test_split # 更正缩进
# 用于导入数据
导入火炬视觉
将 matplotlib.pyplot 导入为 plt
从 IPython 导入显示
display.set_matplotlib_formats(&#39;svg&#39;) # 更正了缩进并删除了多余的空格

# 下载数据集
cdata = torchvision.datasets.EMNIST(root=&#39;emnist&#39;, split=&#39;letters&#39;, download=True)

错误：
&lt;块引用&gt;
URLError: 

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78186856/how-to-downlad-emnist-letters-dataset</guid>
      <pubDate>Tue, 19 Mar 2024 13:07:56 GMT</pubDate>
    </item>
    <item>
      <title>如何设计神经网络来进行突破性检测[关闭]</title>
      <link>https://stackoverflow.com/questions/78180337/how-to-design-a-neural-network-to-do-breakthrough-detection</link>
      <description><![CDATA[摘要：
我正在设计一个用于大鼠开颅手术的自动钻孔系统。我希望当神经网络检测到突破时钻头会停止。照片显示了该系统。
系统概述
手术结果
我尝试的：

力传感器和加速度计实时收集信号（采样率为3kHz）。所以我用它们记录了一些钻孔过程（以恒定的进给速度和旋转速度），并用Python绘制它们，如下所示：强制数据（蓝色代表原始数据，橙色代表黄油低通滤波数据）。来自 NIDAQmx 的振动数据
我希望使用神经网络进行检测（因为我可以轻松识别图中“forcedata”中蓝色的突破点）。所以我训练一个CNN-LSTM，输入0.1秒内收集到的数据，输出一个0-1的数字，其中0表示不钻，1表示钻。（因为我认为突破点是1和0的交界处）。所以我手动将数据分为标签为1的数据集和标签为0的数据集如图所示。红色代表钻孔过程。标签1组是通过每0.1s分割区域来制作的
CNN-LSTM 网络代码如下：

类 CNNLSTMClassifier(nn.Module):
    def __init__(自身，hidden_​​size，num_layers)：
        超级（CNNLSTMClassifier，自我）.__init__（）
        self.cnn = nn.Sequential(
            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=4, padding=1),
            ReLU(),
            nn.MaxPool1d(kernel_size=2, 步长=2),
            nn.Conv1d（in_channels = 64，out_channels = 128，kernel_size = 4，padding = 1），
            ReLU(),
            nn.MaxPool1d(kernel_size=2, 步长=2),
            nn.Conv1d（in_channels = 128，out_channels = 256，kernel_size = 4，padding = 1），
            ReLU(),
            nn.MaxPool1d（内核大小=2，步长=2）
        ）
        self.lstm = nn.LSTM(input_size=256,hidden_​​size=hidden_​​size,num_layers=num_layers,batch_first=True)
        self.fc = nn.Linear(hidden_​​dim, 1)
        self.sigmoid = nn.Sigmoid()

    def 前向（自身，x）：
        x = x.unsqueeze(1)
        x = self.cnn(x)
        x = x.permute(0, 2, 1)
        lstm_out, _ = self.lstm(x)
        out = self.fc(lstm_out[:, -1, :])
        输出 = self.sigmoid(输出)
        返回

我明白了
2024-03-18 16:25:50.708972 Epoch 1，训练损失 0.6764530851605625
2024-03-18 16:25:55.804250 Epoch 5，训练损失 0.6663112645497138
2024-03-18 16:26:02.211953 Epoch 10，训练损失 0.660625655507836
...................................................... ......................
2024-03-18 16:28:04.148844 Epoch 100，训练损失 0.6572404681613006

列车精度：0.63
准确度测试：0.66

所以效果很差。
我的期望：
我希望无论使用什么方法，神经网络都能检测到突破（可能是另一个神经网络或只是另一种方法）。
补充说明：我认为二元分类的想法是错误的。也许我应该检查的是每0.1s的数据是否有足够的整体下降趋势。]]></description>
      <guid>https://stackoverflow.com/questions/78180337/how-to-design-a-neural-network-to-do-breakthrough-detection</guid>
      <pubDate>Mon, 18 Mar 2024 12:52:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 tf.compat.v1.summary.merge_all() 合并摘要时出错</title>
      <link>https://stackoverflow.com/questions/78131106/error-while-merging-summaries-using-tf-compat-v1-summary-merge-all</link>
      <description><![CDATA[由于某种原因，self._summaries 对象为 None。
这会导致以下错误：
 文件“main.py”，第 216 行，在  中
    tf.compat.v1.app.run()
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/platform/app.py”，第 40 行，运行中
    _run（主=主，argv=argv，flags_parser=_parse_flags_tolerate_undef）
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/absl/app.py”，第 312 行，运行中
    _run_main（主要，参数）
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/absl/app.py”，第 258 行，位于 _run_main
    sys.exit(主(argv))
  文件“main.py”，第 203 行，在 main 中
    setup_training（hps.mode，生成器，鉴别器，generator_batcher，discriminator_batcher，generator_val_batcher，discriminator_val_batcher）
  文件“main.py”，第 160 行，位于 setup_training
    trainer.adversarial_train（生成器、鉴别器、generator_batcher、discriminator_batcher、generator_val_batcher、discriminator_val_batcher、summary_writer、sess_context_manager）
  文件“/home/20bce120/tf2/trainer.py”，第 106 行，adversarial_train
    result_train = 生成器.run_train_step(sess, 批处理)
  文件“/home/20bce120/tf2/generator.py”，第 636 行，在 run_train_step 中
    返回 sess.run(to_return, feed_dict)
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 968 行，运行中
    运行元数据指针）
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 1176 行，位于 _run
    self._graph、获取、feed_dict_tensor、feed_handles=feed_handles）
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 487 行，在 __init__ 中
    self._fetch_mapper = _FetchMapper.for_fetch(获取)
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 270 行，在 for_fetch 中
    返回_DictFetchMapper(获取)
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 419 行，位于 __init__ 中
    _FetchMapper.for_fetch(fetch) 用于在 fetches.values() 中获取
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 419 行，在  中
    _FetchMapper.for_fetch(fetch) 用于在 fetches.values() 中获取
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 265 行，在 for_fetch 中
    （获取，类型（获取）））
类型错误：获取参数 None 具有无效类型 

代码：
 def build_graph(self):
        
        tf.logging.info(&#39;构建图表...&#39;)
        t0 = 时间.time()
        self._add_placeholders()
        self._add_seq2seq()
        self.global_step = tf.Variable(0, name=&#39;global_step&#39;, trainable=False)
        如果 self._hps.mode == &#39;train&#39; 或 self._hps.mode == &#39;pretrain&#39;:
            self._add_train_op()
            self._rollout()
            
        self._summaries = tf.compat.v1.summary.merge_all()
        tf.logging.info(&#39;构建图表的时间：%i秒&#39;, time.time() - t0)

我尝试在调用 tf.compat.v1.summary.merge_all() 之前添加一个 tf.scalar_summary()，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78131106/error-while-merging-summaries-using-tf-compat-v1-summary-merge-all</guid>
      <pubDate>Sat, 09 Mar 2024 03:00:37 GMT</pubDate>
    </item>
    <item>
      <title>Python 中具有正系数的线性回归</title>
      <link>https://stackoverflow.com/questions/35986627/linear-regression-with-positive-coefficients-in-python</link>
      <description><![CDATA[我正在尝试找到一种方法来拟合具有正系数的线性回归模型。
我发现的唯一方法是sklearn的Lasso模型，它有一个 positive=True 参数，但不建议与 alpha=0 一起使用（意味着对权重没有其他限制）。
您知道另一种模型/方法/方式吗？]]></description>
      <guid>https://stackoverflow.com/questions/35986627/linear-regression-with-positive-coefficients-in-python</guid>
      <pubDate>Mon, 14 Mar 2016 11:43:38 GMT</pubDate>
    </item>
    </channel>
</rss>