<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 19 Dec 2023 12:25:04 GMT</lastBuildDate>
    <item>
      <title>目标变量中有 80% 缺失值</title>
      <link>https://stackoverflow.com/questions/77685075/80-of-missing-values-in-target-variable</link>
      <description><![CDATA[我的目标变量中有 80% 的缺失值，这是一个回归问题；请问我该怎么办？
该表大约有 788 行，目标列上有 707 行缺失值，目标标签上的输入是价格，但其中 80% 包含 Nan。]]></description>
      <guid>https://stackoverflow.com/questions/77685075/80-of-missing-values-in-target-variable</guid>
      <pubDate>Tue, 19 Dec 2023 12:08:46 GMT</pubDate>
    </item>
    <item>
      <title>我实现了一个 lenet CNN 模型来对人类情感进行分类，但它只正确地分类了“悲伤”组</title>
      <link>https://stackoverflow.com/questions/77682900/i-implemented-a-lenet-cnn-model-to-classify-human-emotion-but-it-only-classifie</link>
      <description><![CDATA[我开发了一个 Lenet 模型来在 TensorFlow 中对人脸情感进行分类。我想可视化我的数据、真实标签和预测标签，但我面临各种错误。为了方便起见，我附上了所有代码。 jupyter 笔记本代码的最后一个块是我想要应用可视化的地方。
链接到我的代码：
https://github.com/mirpouya/TensorFlow -Tutorial/blob/main/Human_Emotion_Detection(可视化%20问题).ipynb
我尝试了各种具有不同参数的 Lenet 模型。]]></description>
      <guid>https://stackoverflow.com/questions/77682900/i-implemented-a-lenet-cnn-model-to-classify-human-emotion-but-it-only-classifie</guid>
      <pubDate>Tue, 19 Dec 2023 04:20:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中正确释放 GPU 内存</title>
      <link>https://stackoverflow.com/questions/77682343/how-to-properly-free-gpu-memory-in-pytorch</link>
      <description><![CDATA[我正在使用 Pytorch 和 Pytorch-Lightning 来训练和验证模型。但在测试期间（test_step），我遇到了 CUDA 内存错误，我不知道如何解决。该模型训练和验证良好，这不是批量大小、数据大小或模型大小的问题。因此，有关此主题的大多数其他答案不适用于我。
我需要在测试期间保持某种状态。数据属于某些 id，我收集多个时期的张量，如果它们属于同一 id，则将它们连接起来。然后，我计算对该级联张量的一些兴趣度量，并继续收集下一个张量，直到 id 发生变化。我尝试将批处理大小减少到 1，但这只会使流程进展得更远，然后会引发类似的错误。
我尝试了一些技巧，例如 gc.collect、torch.cuda.empty_cache() 和 del var 其中 var&lt; /code&gt; 是我收集张量以便稍后连接的变量。
连接本身不是问题，因为当我检查日志时，该过程会在多个时期内正常进行，直到在结束前的几个 id 处抛出以下错误。在我看来，我用来维护状态的张量并没有被释放，并且在每个纪元之后占用越来越多的空间。
torch.cuda.OutOfMemoryError：CUDA 内存不足。
尝试分配 12.48 GiB（GPU 0；39.56 GiB 总容量；
已分配 25.35 GiB；
9.62 GiB 免费；
PyTorch 总共预留了 29.44 GiB）
如果保留内存是&gt;&gt;分配的内存尝试设置 max_split_size_mb 以避免碎片。
请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档

我已经不知道该做什么了。我已经尝试了这个问题中提到的几乎所有技巧，包括这个评论 https://stackoverflow.com/a/75779114/228177 .
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77682343/how-to-properly-free-gpu-memory-in-pytorch</guid>
      <pubDate>Tue, 19 Dec 2023 00:11:12 GMT</pubDate>
    </item>
    <item>
      <title>LSTM多类蛋白质分类模型（keras）模型疯狂的低精度[关闭]</title>
      <link>https://stackoverflow.com/questions/77682312/lstm-multiclass-protien-classification-model-keras-model-crazy-low-accuracy</link>
      <description><![CDATA[我正在做一项蛋白质分类作业。我得到了一个包含 5 个类别的不平衡数据集。每个条目都有一个总科名称和相应的序列。
我一直在遵循我的大学教学，了解使用 keras 和 sklearn 模块的正确方法和编码实践。然而，经过多次参数调整、添加和删除图层。我的模型始终预测精度较低（val_accuracy 0.2）。我依次使用 2 个双向 LSTM，然后是 relu 密集层、dropout 和带有 softmax 激活的最终密集层。
请参阅下面的代码：
导入 pandas 作为 pd
从 sklearn 导入预处理，model_selection
导入keras
将 numpy 导入为 np
将张量流导入为 tf
将tensorflow_addons导入为tfa
从 sklearn.utils 导入 class_weight
从 keras.layers 导入嵌入、Conv1D、MaxPooling1D
从 keras.layers 导入 Dropout、Flatten、Dense
从 keras.layers 导入 LSTM、双向、GRU、SimpleRNN、MaxPooling1D、SpatialDropout1D
从 scikeras.wrappers 导入 KerasClassifier
从 sklearn.model_selection 导入 GridSearchCV、StratifiedKFold
从 imblearn.over_sampling 导入 RandomOverSampler

种子 = 123

# 读入数据并删除索引列
数据= pd.read_csv(“./results/processedData.csv”,index_col=False)

# 对 seq 进行标记化，以便 CNN 可以使用
tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)

打印（数据.描述（））

y = 数据.名称

# 将标记化值设置为 x
x = tokenizer.texts_to_sequences(data.seq)
最大长度 = 580
x = keras.preprocessing.sequence.pad_sequences(
    x，MAX_LENGTH，截断=“前”）

# 设置模型参数
令牌 = len(tokenizer.word_index) + 1
课程 = 5
单位 = 64
丢包率 = 0.2


def buildLSTM():
    模型 = keras.Sequential([
        嵌入（TOKENS，32，input_length = MAX_LENGTH，mask_zero = True），

        双向（LSTM（单位，return_sequences = True）），
        双向（LSTM（UNITS // 2）），


        密集（UNITS // 2，激活=“relu”），
        辍学率（DROPOUT_RATE），

        密集（类，激活=“softmax”）
    ]）
    model.compile(loss=“categorical_crossentropy”,
                  优化器=keras.optimizers.Adam(1e-4)，指标=[tf.keras.metrics.CategoricalAccuracy()])
    返回模型


# # 过采样
过采样 = RandomOverSampler(sampling_strategy=&#39;少数&#39;)

skf = StratifiedKFold(n_splits=5, shuffle=True)

对于 skf.split(x, y) 中的 trainI、testI：
    训练X，训练Y = x [训练I]，y [训练I]
    测试X，测试Y = x [测试I]，y [测试I]

    # 将输出设置为分类器
    labelBinarize = 预处理.LabelBinarizer()
    labelBinarize.fit(trainY)
    trainY = labelBinarize.transform(trainY)
    testY = labelBinarize.transform(testY)

    # 过采样训练集
    trainX, trainY = oversample.fit_resample(trainX, trainY)

    # 构建模型并拟合数据
    模型 = buildLSTM()
    model.fit(trainX,trainY,validation_data=(
        测试X，测试Y），纪元= 5，batch_size = 64）

    # 评估分数
    分数 = model.evaluate(testX, testY, verbose=0)
    准确度 = 分数[1] * 100
    print(f&#39;准确度：{准确度}&#39;)

# 打印摘要
打印（模型.摘要）


请参阅下面的纪元示例：
39/39 [================================] - 48s 944ms/步 - 损耗：1.6088 - 分类准确度：0.2923 - 验证损失：1.6090 - 验证分类准确度：0.0761
纪元 2/5
39/39 [================================] - 32s 825ms/步 - 损失：1.6075 - categorical_accuracy：0.2987 - val_loss ：1.6085 - val_categorical_accuracy：0.0761
纪元 3/5
39/39 [================================] - 33s 838ms/步 - 损失：1.6062 - categorical_accuracy：0.2987 - val_loss ：1.6081 - val_categorical_accuracy：0.0761
纪元 4/5
39/39 [==============================] - 34s 877ms/步 - 损失：1.6050 - categorical_accuracy：0.2987 - val_loss ：1.6077 - val_categorical_accuracy：0.0761
纪元 5/5
39/39 [================================] - 33s 845ms/步 - 损失：1.6037 - categorical_accuracy：0.2987 - val_loss ：1.6073 - val_categorical_accuracy：0.0761

如上面的代码所示，我尝试将过采样与 stratifiedkfold 一起使用来尝试纠正这些数据不平衡。此外，我尝试了不同范围的时期、批量大小和学习率。]]></description>
      <guid>https://stackoverflow.com/questions/77682312/lstm-multiclass-protien-classification-model-keras-model-crazy-low-accuracy</guid>
      <pubDate>Mon, 18 Dec 2023 23:56:24 GMT</pubDate>
    </item>
    <item>
      <title>sklearn 管道中的标准缩放器有缺陷吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77682306/standard-scaler-in-sklearn-pipeline-has-a-flaw</link>
      <description><![CDATA[当管道在 pipeline.fit() 上训练管道时，标准缩放器将根据训练数据适合 std/meanDev，对吧？现在，如果测试数据与训练数据有很大不同...在 pipeline.predict() 上，相同的 std/meanDev 将应用于由标准缩放器在训练数据上提取的测试数据！这是 sklearn 管道中的一个巨大缺陷吗？
我一直在研究这个问题，我有两个不同的数据集来解决同一问题，当我在一个数据集上训练管道并在另一个数据集上测试它时，它给我所有肯定或所有否定的答案（二元分类），当我分别缩放管道外部的两个数据集，它开始给出正确的结果。]]></description>
      <guid>https://stackoverflow.com/questions/77682306/standard-scaler-in-sklearn-pipeline-has-a-flaw</guid>
      <pubDate>Mon, 18 Dec 2023 23:54:16 GMT</pubDate>
    </item>
    <item>
      <title>对点击流进行分类并了解特定的点击流如何导致特定的用户行为操作？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77682089/categorize-stream-of-clicks-and-understand-how-a-specific-stream-of-clicks-resul</link>
      <description><![CDATA[我有来自汽车拍卖网站的点击流数据，现在我想对点击流进行分类，并了解特定的点击流如何导致特定的用户行为操作，例如查看特定汽车、搜索特定汽车、将汽车添加到监视列表，竞标各种汽车？]]></description>
      <guid>https://stackoverflow.com/questions/77682089/categorize-stream-of-clicks-and-understand-how-a-specific-stream-of-clicks-resul</guid>
      <pubDate>Mon, 18 Dec 2023 22:35:23 GMT</pubDate>
    </item>
    <item>
      <title>查找文本的所有标记概率[关闭]</title>
      <link>https://stackoverflow.com/questions/77682073/finding-all-token-probabilities-of-text</link>
      <description><![CDATA[我正在玩 ROBERTA，试图找到输入文本中不可能的部分。例如，我想潜在地检测奇怪的单词选择、不正确的语法等。
我当前的方法是循环序列中的所有标记，将它们交换为掩码标记，然后运行模型。然后我可以获得 logits 的 softmax，并查找实际标记的概率：
导入火炬
从变压器导入 RobertaTokenizer、RobertaForMaskedLM

def get_token_probabilities(句子、模型、分词器):

    token_ids = tokenizer.encode(sentence, return_tensors=&#39;pt&#39;)
    结果=[]

    for i in range(1, token_ids.size(1) - 1): # 排除开始和结束标记
        masked_token_ids = token_ids.clone()
        masked_token_ids[0, i] = tokenizer.mask_token_id

        使用 torch.no_grad()：
            输出=模型（masked_token_ids）

        softmax = torch.softmax(outputs.logits[0, i], dim=0)

        原始令牌 ID = 令牌 ID[0, i]
        token_prob = softmax[original_token_id].item()

        结果.追加({
            “令牌”：tokenizer.decode([original_token_id]),
            “概率”：token_prob，
        })

    返回结果


def main():
    model_name = &#39;罗伯塔基地&#39;
    tokenizer = RobertaTokenizer.from_pretrained(model_name)
    模型 = RobertaForMaskedLM.from_pretrained(model_name)

    例子= [
        “安和保罗正在准备考试。”,
        “安和保罗正在准备考试。”,
        “安和保罗正在为考试而学习。”,
    ]

    对于示例中的句子：
        结果 = get_token_probabilities(句子、模型、分词器)
        对于结果中的条目：
            print(f&#39;{entry[&quot;token&quot;]}: {entry[&quot;probability&quot;]:.6f}&#39;)
        打印（）

如果 __name__ == &#39;__main__&#39;:
    主要的（）

结果：
&lt;前&gt;&lt;代码&gt;安：0.007430
 和：0.982790
 保罗：0.012045
 研究：0.002692
 为：0.973533
 他们的：0.077359
 考试：0.008554
.: 0.691542

安：0.005499
 和：0.871503
 保罗：0.006089
 研究：0.006135
 为：0.954329
 那里：0.000004
 考试：0.008875
.: 0.545155

安：0.003608
 和：0.965940
 保罗：0.004815
 研究：0.000095
 为：0.009223
 他们：0.013043
是：0.000293
 考试：0.000020
.: 0.220154

问题是这种方法效率不高，因为它必须针对输入中的每个标记运行。有没有更好的方法来做到这一点，也许只运行模型一次？]]></description>
      <guid>https://stackoverflow.com/questions/77682073/finding-all-token-probabilities-of-text</guid>
      <pubDate>Mon, 18 Dec 2023 22:30:18 GMT</pubDate>
    </item>
    <item>
      <title>KeyError：“[Index(['names_of_categorical_columns'], dtype='object', name='name_of_index_column')] 都不在 [index] 中”</title>
      <link>https://stackoverflow.com/questions/77681457/keyerror-none-of-indexnames-of-categorical-columns-dtype-object-name</link>
      <description><![CDATA[我编写这段代码是为了获取一个变量中的所有分类列，并使用 OrdinalEncoder() 对它们进行编码。
我的代码：
s = (X_train.dtypes == &#39;对象&#39;)
object_cols = 列表(s[s].index)

ordinal_encoder = OrdinalEncoder(handle_unknown = &#39;use_encoded_value&#39;,unknown_value = 999)

# 进行复制以避免更改原始数据
label_X_train = X_train.copy()
label_X_valid = X_valid.copy()

# 将序数编码器应用于具有分类数据的每一列
label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])
label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])

我收到此错误（在问题标题中）并且无法处理它。您能告诉我如何解决这个问题吗？
我尝试使用 OrdinalEncoder() 对分类数据进行编码，然后再使用它来训练 XGBRegressor() 模型。]]></description>
      <guid>https://stackoverflow.com/questions/77681457/keyerror-none-of-indexnames-of-categorical-columns-dtype-object-name</guid>
      <pubDate>Mon, 18 Dec 2023 19:59:08 GMT</pubDate>
    </item>
    <item>
      <title>在手动交叉验证和 cross_val_score 之间获取不同的分数值</title>
      <link>https://stackoverflow.com/questions/77680320/getting-different-score-values-between-manual-cross-validation-and-cross-val-sco</link>
      <description><![CDATA[我创建了一个 python for 循环，将训练数据集分割成分层的 KFold，并在循环内使用分类器来训练它。然后使用经过训练的模型通过验证数据进行预测。使用此过程实现的指标与使用 cross_val_score 函数实现的指标完全不同。我期望使用这两种方法得到相同的结果。
此代码用于文本分类，我使用 TF-IDF 对文本进行矢量化
这是代码：
手动实现交叉验证的代码：
#导入指标函数来衡量模型的性能
从 sklearn.metrics 导入 f1_score、accuracy_score、 precision_score、recall_score
从 sklearn.model_selection 导入 StratifiedKFold
data_validation = [] # 用于存储使用交叉验证的模型验证结果的列表
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
准确度值 = []
f1_val = []

# 使用ravel函数将多维数组展平为一维
对于 (skf.split(X_train, y_train)) 中的 train_index、val_index：
    X_tr, X_val = X_train.ravel()[train_index], X_train.ravel()[val_index]
    y_tr, y_val = y_train.ravel()[train_index] , y_train.ravel()[val_index]
    tfidf=TfidfVectorizer()
    X_tr_vec_tfidf = tfidf.fit_transform(X_tr) # 对训练折叠进行向量化
    X_val_vec_tfidf = tfidf.transform(X_val) # 对验证折叠进行向量化
    #实例化模型
    模型= MultinomialNB(alpha=0.5, fit_prior=False)
    #用我们的训练数据集训练空模型
    model.fit(X_tr_vec_tfidf, y_tr)
    Predictions_val = model.predict(X_val_vec_tfidf) # 使用验证数据集进行预测
    acc_val = 准确度_分数（y_val，预测_val）
    Accuracy_val.append(acc_val)
    f_val = f1_score（y_val，预测值）
    f1_val.append(f_val)

avg_accuracy_val = np.mean(accuracy_val)
avg_f1_val = np.mean(f1_val)

# 存储指标的临时列表
temp = [&#39;NaiveBayes&#39;]
temp.append(avg_accuracy_val) #验证准确率分数
temp.append(avg_f1_val) #验证f1分数
data_validation.append(临时)
#使用数据帧创建一个表，其中包含所有经过训练和测试的 ML 模型的指标
result = pd.DataFrame(data_validation, columns = [&#39;算法&#39;,&#39;准确度分数：验证&#39;,&#39;F1-分数：验证&#39;])
result.reset_index(drop=True, inplace=True)
结果

输出：
 算法准确度分数：验证 F1-Score：验证
0 天真的贝叶斯 0.77012 0.733994

现在使用 cross_val_score 函数的代码：
从 sklearn.model_selection 导入 cross_val_score
从 sklearn.feature_extraction.text 导入 CountVectorizer、TfidfVectorizer
分数 = [&#39;准确度&#39;, &#39;f1&#39;]
#使用NLP技术TF-IDF对训练和测试数据集进行文本向量化
tfidf=TfidfVectorizer()
X_tr_vec_tfidf = tfidf.fit_transform(X_train)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
nb=多项式NB(alpha=0.5, fit_prior=False)
对于 [“accuracy”, “f1”] 中的分数：
    print (f&#39;{score}: {cross_val_score(nb,X_tr_vec_tfidf,y_train,cv=skf,scoring=score).mean()} &#39;)

输出：
准确度：0.7341283583255231
f1：0.7062017090972422

可以看出，使用这两种方法的准确性和 f1 指标有很大不同。当我使用 KNeighborsClassfier 时，指标的差异更加严重。]]></description>
      <guid>https://stackoverflow.com/questions/77680320/getting-different-score-values-between-manual-cross-validation-and-cross-val-sco</guid>
      <pubDate>Mon, 18 Dec 2023 16:05:36 GMT</pubDate>
    </item>
    <item>
      <title>处理 CNN 二元分类的分布外样本和异常</title>
      <link>https://stackoverflow.com/questions/77679785/handling-out-of-distribution-samples-and-anomalies-for-cnn-binary-classification</link>
      <description><![CDATA[我对机器学习领域比较陌生，并且对创建基本自定义模型有基本的了解。
分配给我的任务
我的经理要求我开发一种机器学习模型，能够在攻击性图像发送到服务器之前识别它们。这些图像可以分为不同的类别，例如武器或成人内容。
问题
但是，我在模型检测异常或分布外样本的能力方面遇到了问题。
我尝试过的
我尝试了不同的方法，利用&#39;binary_crossentropy&#39;作为我的损失函数，但遇到了同样的问题。我还构建了一个包含两个类的模型：

12,278 与武器相关的图片 class_name = 武器
3,000 张食物 商品图片 class_name = 其他

但是，我不确定“其他” 类别中应包含哪些内容。使用 MobileNet 作为基础模型构建模型后，它成功地准确预测了武器，并提供了 0.4 到 0.6 范围内的食品预测，这似乎是可以接受的。然而，当我引入大象或汽车的图像时，模型倾向于将它们预测为武器，显示的置信度接近 1。
我不知道如何解决这个问题。我尝试研究这个问题，并发现了一些关于分布外检测的线索以及与异常或离群值相关的概念。当模型的输入包含不属于训练数据的图像时，如何获得不确定性值？
如果您能提供任何指导、建议，甚至参考能够有效解决此问题的视频或资源，我将不胜感激。感谢您的帮助。
数据加载：
train_ds, test_ds = keras.utils.image_dataset_from_directory(
    目录=“/内容/武器”，
    标签=“推断”，
    label_mode =“二进制”，
    批量大小=32，
    子集=“两者”，
    图像大小=(224,224),
    验证分割=0.2，
    随机播放=真，
    种子=1337
）

现在标准化输入图像数据：
def process（图像，标签）：
  图像=tf.cast(图像/255, tf.float32)
  返回图像、标签
train_ds = train_ds.map(进程)
test_ds = test_ds.map(进程)

创建 CNN 模型：
input_shape = (224,224,3)
mobilenet = MobileNet(input_shape,weights=&#39;imagenet&#39;,include_top=False)
模型=顺序（）
 
model.add（移动网络）
模型.add(压平())
model.add（密集（256，激活=&#39;relu&#39;））
模型.add(Dropout(0.5))
model.add（密集（1，激活=&#39;sigmoid&#39;））
模型.summary()



sgd = SGD(学习率=0.0001，动量=0.9，nesterov=True)
model.compile（损失=&#39;binary_crossentropy&#39;，优化器=sgd，指标=[&#39;准确性&#39;]）
历史= model.fit（train_ds，validation_data = test_ds，batch_size = 4，epochs = 6）


纪元 6/6
382/382 [==============================] - 58s 150ms/步 - 损失：0.0042 - 精度：0.9984 - val_loss ：0.0063 - val_accuracy：0.997
]]></description>
      <guid>https://stackoverflow.com/questions/77679785/handling-out-of-distribution-samples-and-anomalies-for-cnn-binary-classification</guid>
      <pubDate>Mon, 18 Dec 2023 14:34:38 GMT</pubDate>
    </item>
    <item>
      <title>是否可以直接在张量流上运行使用 mediapipe 重新训练的对象检测模型，而不是使用 mediapipe？</title>
      <link>https://stackoverflow.com/questions/77666162/is-it-possible-to-run-object-detection-models-retrained-with-mediapipe-on-tensor</link>
      <description><![CDATA[我正在使用此 mediapipe 指南来重新训练对象检测模型，并将其导出到 tflite 模型。我想在反应原生中使用该模型。不幸的是，mediapipe 没有直接的 React-Native 实现，但我有一个可以在 RN 中运行任何 .tflite 模型的库。
起初我以为我只需要使用 mediapipe 来重新训练我的模型，但现在我在示例中意识到我还需要 mediapipe 来进行检测部分。所以我想知道是否也可以直接在张量流中运行使用 mediapipe 创建的模型？
第一次测试后，我得到了以下形状的对象检测模型的“位置”输出：和“分数”：
&lt;前&gt;&lt;代码&gt;((1, 19125, 4), (1, 19125, 4))

位置的形状对我来说很有意义，但是我应该如何解释“分数”？数据？或者在没有媒体管道的情况下运行模型没有意义吗？]]></description>
      <guid>https://stackoverflow.com/questions/77666162/is-it-possible-to-run-object-detection-models-retrained-with-mediapipe-on-tensor</guid>
      <pubDate>Fri, 15 Dec 2023 12:08:03 GMT</pubDate>
    </item>
    <item>
      <title>如何修复“TypeError：无法将值转换为 TensorFlow DType”？</title>
      <link>https://stackoverflow.com/questions/67134610/how-to-fix-typeerror-cannot-convert-the-value-to-a-tensorflow-dtype</link>
      <description><![CDATA[我尝试为手写数字数据集构建 GAN（生成对抗网络），但遇到与张量数据类型错误相关的问题。

第 78 行，I 使用“tf.cast”将所有图像转换为 float32。
对于生成器和判别器损失，我使用了“tf.losses.BinaryCrossentropy”。

我不知道错误发生的位置。
这是完整的代码
导入tensorflow为tf
将 matplotlib.pyplot 导入为 plt
将 scipy.io 导入为 sio
从 sklearn.model_selection 导入 train_test_split
将 numpy 导入为 np

（火车，标签），（测试，label_test）= tf.keras.datasets.mnist.load_data（）

train_images = train.reshape(train.shape[0], 28, 28, 1)
训练图像 = (训练图像-127.5)/127.5

BUFFER_SIZE = train.shape[0]
批量大小 = 100
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

def 鉴别器():
    模型 = tf.keras.Sequential()
    model.add(tf.keras.layers.Conv2D(7, (3,3), padding=&#39;相同&#39;, input_shape=(28, 28, 1)))
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.LeakyReLU(0.1))
    model.add（tf.keras.layers.Dense（128，激活=&#39;relu&#39;））
    model.add(tf.keras.layers.Dense(1))
    返回模型

dis_model = 判别器()
disk_opt = tf.optimizers.Adam()

def discriminator_loss(y_pred_real, y_pred_fake):
    真实 = tf.sigmoid(y_pred_real)
    假 = tf.sigmoid(y_pred_fake)
    fake_loss = tf.losses.binary_crossentropy(tf.ones_like(y_pred_real), y_pred_real)
    real_loss = tf.losses.binary_crossentropy(tf.zeros_like(y_pred_fake), y_pred_fake)
    返回假损失+真实损失

def 生成器():
    模型 = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(7*7*256, input_shape=(100,)))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Reshape((7, 7, 256)))
    model.add(tf.keras.layers.Conv2DTranspose(128, (3, 3), padding=&#39;相同&#39;))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Conv2DTranspose(64, (3,3), strides=(2,2), padding=&#39;相同&#39;))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Conv2DTranspose(1, (3,3), strides=(2,2), padding=&#39;相同&#39;))
    返回模型

gen_model = 生成器()
gen_opt = tf.optimizers.Adam()

def生成器损失（fake_pred）：
    损失 = tf.sigmoid(fake_pred)
    fake_loss = tf.losses.BinaryCrossentropy(tf.zeros_like(fake_pred), fake_pred)
    返回假损失

def train_steps(图像):
    fake_noise = np.random.randn(BATCH_SIZE, 100).astype(&#39;float32&#39;)
    将 tf.GradientTape() 作为 gen_tape，tf.GradientTape() 作为 disk_tape：
        生成的图像 = gen_model(假噪声)

        fake_output = dis_model(生成的图像)
        real_output = dis_model(图像)

        gen_loss = 生成器_loss(fake_output)
        Disc_loss = discriminator_loss(real_output, fake_output)

        gen_gradient = gen_tape.gradient(gen_loss, gen_model.trainable_variables)
        Disc_gradient = Disc_tape.gradient(disc_loss, dis_model.trainable_variables)

        gen_opt.apply_gradients(zip(gen_gradient, gen_model.trainable_variables))
        disk_opt.apply_gradients(zip(disc_gradient, dis_model.trainable_variables))

        print(&#39;disc_loss: &#39;, np.mean(disc_loss))
        print(&#39;gen_loss: &#39;, np.mean(gen_loss))

def train（数据集，纪元）：
    对于范围（纪元）内的 j：
        对于数据集中的图像：
            图像 = tf.cast(图像, tf.dtypes.float32)
            train_steps（图像）


火车（火车数据集，2）

发生错误
回溯（最近一次调用最后一次）：
  文件“D:/GANs_handwriting/model.py”，第 81 行，位于  中。
    火车（火车数据集，2）
  文件“D:/GANs_handwriting/model.py”，第 78 行，训练中
    train_steps（图像）
  文件“D:/GANs_handwriting/model.py”，第 66 行，train_steps 中
    gen_gradient = gen_tape.gradient（gen_loss，gene.trainable_variables）
  文件“C:\Users\Devanshu\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\eager\backprop.py”，第 1047 行，渐变
    如果不是 backprop_util.IsTrainable(t)：
  文件“C:\Users\Devanshu\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\eager\backprop_util.py”，第 30 行，在 IsTrainable 中
    dtype = dtypes.as_dtype(dtype)
  文件“C:\Users\Devanshu\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\framework\dtypes.py”，第 649 行，在 as_dtype 中
    raise TypeError(“无法将值 %r 转换为 TensorFlow DType。” %
类型错误：无法转换值 到 TensorFlow DType。
]]></description>
      <guid>https://stackoverflow.com/questions/67134610/how-to-fix-typeerror-cannot-convert-the-value-to-a-tensorflow-dtype</guid>
      <pubDate>Sat, 17 Apr 2021 04:16:49 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn cross_val_score 给出的数字与 model.score 明显不同？</title>
      <link>https://stackoverflow.com/questions/61937500/sklearn-cross-val-score-gives-significantly-different-number-than-model-score</link>
      <description><![CDATA[我有一个二元分类问题
首先，我将数据训练测试分割为：
X_train、X_test、y_train、y_test = train_test_split(X、y、random_state=42)

我检查了 y_train，它基本上有两个类 (1,0) 的 50/50 分割，这就是数据集的方式
当我尝试基本模型时，例如：
模型 = RandomForestClassifier()
model.fit(X_train, y_train)
model.score(X_train, y_train)

输出为 0.98 或 1% 的差异，具体取决于训练测试分割的随机状态。 
但是，当我尝试 cross_val_score 时，例如：
cross_val_score(model, X_train, y_train, cv=StratifiedKFold(shuffle=True), rating=&#39;accuracy&#39;)

输出为
数组([0.65, 0.78333333, 0.78333333, 0.66666667, 0.76666667])

数组中没有一个分数接近 0.98？
当我尝试得分 = &#39;r2&#39; 时，我得到了
&gt;&gt;&gt;&gt;cross_val_score(model, X_train, y_train, cv=StratifiedKFold(shuffle=True), rating=&#39;r2&#39;)
数组([-0.20133482,-0.00111235,-0.2,-0.2,-0.13333333])

有谁知道为什么会这样吗？我尝试过 Shuffle = True 和 False 但没有帮助。
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/61937500/sklearn-cross-val-score-gives-significantly-different-number-than-model-score</guid>
      <pubDate>Thu, 21 May 2020 15:04:38 GMT</pubDate>
    </item>
    <item>
      <title>我是给 cross_val_score() 整个数据集还是只提供训练集？</title>
      <link>https://stackoverflow.com/questions/52249158/do-i-give-cross-val-score-the-entire-dataset-or-just-the-training-set</link>
      <description><![CDATA[由于该类的文档不是很清楚。我不明白我赋予它什么价值。

&lt;块引用&gt;
  cross_val_score（估计器，X，y=无）

这是我的代码：
clf = LinearSVC(random_state=seed, **params)
cvscore = cross_val_score(clf, 特征, 标签)

我不确定这是否正确，或者我是否需要提供 X_train 和 y_train 而不是特征和标签。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/52249158/do-i-give-cross-val-score-the-entire-dataset-or-just-the-training-set</guid>
      <pubDate>Sun, 09 Sep 2018 22:39:32 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何最佳实践来为基于文本的分类准备特征？</title>
      <link>https://stackoverflow.com/questions/22087407/is-there-any-best-practice-to-prepare-features-for-text-based-classification</link>
      <description><![CDATA[我们收到了许多来自客户的反馈和问题报告。它们是纯文本。我们正在尝试为这些文档构建一个自动分类器，以便未来反馈/问题可以自动路由到正确的支持团队。除了文本本身之外，我认为我们应该将客户资料、案例提交区域等内容纳入分类器中。我认为这可以为分类器做出更好的预测提供更多线索。
目前，所有选择用于训练的特征都是基于文本内容的。如何包含上述元特征？
添加1
我目前的做法是首先对原始文本（包括标题和正文）进行一些典型的预处理，例如删除停用词、词性标记和提取重要词。然后，我将标题和正文转换为单词列表，并以某种稀疏格式存储它们，如下所示：
&lt;块引用&gt;
实例 1：单词 1：单词 1 计数，单词 2：单词 2 计数，...
实例 2：wordX:word1 计数，wordY:word2 计数，...

对于其他非文本功能，我计划将它们添加为单词“columns”之后的新列。所以最终的实例将如下所示：
&lt;块引用&gt;
实例 1：word1:word1 计数，...，特征 X:值，特征 Y:值
]]></description>
      <guid>https://stackoverflow.com/questions/22087407/is-there-any-best-practice-to-prepare-features-for-text-based-classification</guid>
      <pubDate>Fri, 28 Feb 2014 05:57:21 GMT</pubDate>
    </item>
    </channel>
</rss>