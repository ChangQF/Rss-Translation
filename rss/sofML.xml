<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 08 Aug 2024 15:16:42 GMT</lastBuildDate>
    <item>
      <title>如何在 sklearn 管道中反转序数编码？</title>
      <link>https://stackoverflow.com/questions/78848893/how-can-i-reverse-ordinal-encoding-within-a-sklearn-pipeline</link>
      <description><![CDATA[我正在尝试清理一个数据集，该数据集包含来自不同特征（包括数字和分类）的大量缺失值。我的想法如下：

使用 OrdinalEncoder 只保留数值并将缺失值保留为 NaN（不能使用 OneHotEncoder，因为它会创建新的列，这些列为 NaN）
使用 KNNImputer 估算缺失值
反转编码，因为没有合理的理由在类别中绘制某种顺序

这是我到目前为止的代码：
import pandas as pd
import numpy as np
from sklearn import set_config
set_config(transform_output=&quot;pandas&quot;)
from sklearn.preprocessing import OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import KNNImputer
from sklearn.base import BaseEstimator, TransformerMixin

类 RoundingToIntegerTransformer(BaseEstimator, TransformerMixin):
def fit(self, X, y=None):
return self

def transform(self, X):
return np.round(X) 

类 ReverseOrdinalEncoder(BaseEstimator, TransformerMixin):
def __init__(self,coder_name, categorical_columns):
self.encoder_name =coder_name
self.categorical_columns = categorical_columns

def fit(self, X, y=None):
return self

def transform(self, X, y=None):
X_ = X.copy()
coder = self.encoder_name
X_categorical = X_[categorical_columns] 
X_categorical =coder.inverse_transform(X_categorical)
X_[categorical_columns] = X_categorical
return X_

data = pd.DataFrame({
&#39;类别 1&#39;: [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, np.nan, &#39;c&#39;, &#39;b&#39;],
&#39;类别 2&#39;: [&#39;x&#39;, &#39;y&#39;, &#39;x&#39;, &#39;z&#39;, np.nan, &#39;y&#39;],
&#39;数值 1&#39;: [1.1, np.nan, 3.3, 4.4, 5.5, np.nan],
&#39;数值 2&#39;: [np.nan, 2.2, np.nan, 4.4, 5.5, 6.6]
})

numerical_columns = data.select_dtypes(include=&#39;number&#39;).columns.tolist()
categorical_columns = data.select_dtypes(include=&#39;object&#39;).columns.tolist()

ordinal_encoder = OrdinalEncoder(encoded_missing_value=np.nan)

first_encoder = ColumnTransformer(transformers=[
(&#39;ordinal_cat&#39;, ordinal_encoder, categorical_columns)
],
remainder=&#39;passthrough&#39;,
verbose_feature_names_out=False)

imputer = Pipeline([
(&#39;KNN_imputing&#39;, KNNImputer()),
(&#39;rounding&#39;, ColumnTransformer(transformers=[(&#39;round_cat&#39;, RoundingToIntegerTransformer() , categorical_columns)],
remainder=&#39;passthrough&#39;,
verbose_feature_names_out=False))
])

reverse_encoder = ReverseOrdinalEncoder(
编码器名称=first_encoder.transformers[0][1], 
categorical_columns=categorical_columns
)

preprocessor = Pipeline([
(&#39;encoding&#39;, first_encoder),
(&#39;imputing&#39;, imputer),
(&#39;decoding&#39;, reverse_encoder)
])

我尝试了一段时间，但从未成功，我总是收到以下错误：

NotFittedError：此 OrdinalEncoder 实例尚未安装。使用此估算器之前，请使用适当的参数调用“fit”。

我理解这个错误，但我认为由于序数编码器在管道中出现得早，因此当它在反向编码器中使用时会安装好。有什么办法可以让它工作吗？
非常感谢]]></description>
      <guid>https://stackoverflow.com/questions/78848893/how-can-i-reverse-ordinal-encoding-within-a-sklearn-pipeline</guid>
      <pubDate>Thu, 08 Aug 2024 14:07:04 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：名称“model”在模型中使用了 2 次。所有层名称都应唯一</title>
      <link>https://stackoverflow.com/questions/78848605/valueerror-the-name-model-is-used-2-times-in-the-model-all-layer-names-shoul</link>
      <description><![CDATA[我尝试在联合学习管道中引入两个重新训练的客户端并计算其权重的平均值。第一次训练时成功了，但现在第二次出现此错误，我尝试修复，但无法找出问题所在。
以下错误源自行 main(model_FTM_path, model_Stanford_path, save_path)
ValueError：名称“model”在模型中使用了 2 次。所有层名称都应唯一。

# 将两个经过训练的客户端加权到一个文件中

import tensorflow as tf
from Client_Trainer import CustomLoss

# 停用 GPU
tf.config.set_visible_devices([], &#39;GPU&#39;)

# 自定义损失函数
loss_function = CustomLoss.MaskedMSE

def load_model(model_path, prefix):
model = tf.keras.models.load_model(model_path, custom_objects = {&#39;MaskedMSE&#39;: loss_function})

return model

def ensemble(model_FTM, model_Stanford):
# 创建输入层
input_shape = model_FTM.input.shape[1:]
ensemble_input = tf.keras.layers.Input(shape = input_shape, name = &#39;ensemble_input&#39;)

# 获取输出两个模型的
output_FTM = model_FTM(ensemble_input)
output_Stanford = model_Stanford(ensemble_input)

# 平均输出
averaged_output = tf.keras.layers.Average(name = &quot;ensemble_average&quot;)([output_FTM, output_Stanford])

# 创建集成模型
ensemble_model = tf.keras.models.Model(inputs = ensemble_input, output = averaged_output, name = &quot;ensemble_model&quot;)

return ensemble_model

def main(model_FTM_path, model_Stanford_path, save_path):
# 加载模型
model_FTM = load_model(model_FTM_path, &quot;FTM&quot;)
model_Stanford = load_model(model_Stanford_path, &quot;Stanford&quot;)

#构建集成模型
ensemble_model = ensemble(model_FTM, model_Stanford)

# 编译集成模型
ensemble_model.compile(optimizer = &#39;adam&#39;, loss = loss_function, metrics = [&#39;mse&#39;])

# print(&quot;\nEnsemble Model Summary:&quot;)
# ensemble_model.summary()

# 保存集成模型
ensemble_model.save(save_path)

if __name__ == &quot;__main__&quot;:
model_FTM_path = &#39;FTMRetrained_round2.h5&#39;
model_Stanford_path = &#39;StanfordRetrained_round2.h5&#39;
save_path = &#39;weighted_2clients_round2.h5&#39;

main(model_FTM_path, model_Stanford_path, save_path)
]]></description>
      <guid>https://stackoverflow.com/questions/78848605/valueerror-the-name-model-is-used-2-times-in-the-model-all-layer-names-shoul</guid>
      <pubDate>Thu, 08 Aug 2024 13:08:35 GMT</pubDate>
    </item>
    <item>
      <title>由于损失函数，我的模型无法训练</title>
      <link>https://stackoverflow.com/questions/78848428/my-model-would-not-train-because-of-the-loss-function</link>
      <description><![CDATA[--------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-37-9a9cd8046c6f&gt; in &lt;cell line: 18&gt;()
16 loss=tf.keras.losses.CategoricalCrossentropy(),
17 metrics=[&#39;accuracy&#39;])
---&gt; 18 history1=model1.fit(train_data,epochs=5,validation_data=valid_data)

1 frames
/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py in categorical_crossentropy(target, output, from_logits, axis)
552 )
553 if len(target.shape) != len(output.shape):
--&gt; 554 raise ValueError(
555 &quot;参数 `target` 和 `output` 必须具有相同的等级 &quot;
556 &quot;(ndim)。已收到： &quot;

ValueError：参数 `target` 和 `output` 必须具有相同的等级 (ndim)。已收到：target.shape=(None,)，output.shape=(None, 19)

所以，我不明白问题是什么？？
但是当我使用 SpareCategoricalCrossentropy 训练我的模型时，它工作正常，但准确率很差，在有效数据中为零
我尝试使用 SpareCategorical，但准确率很低。我的数据集来自 kaggle，所以我刚刚下载了它并给了它一个图像大小
train_dir=&quot;Dataset/train/&quot; test_dir=&quot;Dataset/test/&quot; train_data= Images(train_dir,image_size=(150,150),batch_size=32) valid_data= Images(test_dir,image_size=(150,150),batch_size=32)
我又想起了这一点。现在有人能告诉我该怎么办吗？]]></description>
      <guid>https://stackoverflow.com/questions/78848428/my-model-would-not-train-because-of-the-loss-function</guid>
      <pubDate>Thu, 08 Aug 2024 12:29:54 GMT</pubDate>
    </item>
    <item>
      <title>我正在训练一个 VAE，但在我的批次中，我的 KLD 术语在执行几个步骤后就消失了</title>
      <link>https://stackoverflow.com/questions/78848344/im-training-a-vae-and-my-kld-term-is-vanishing-after-just-a-couple-steps-inside</link>
      <description><![CDATA[这是我的 VAE 损失代码：
def loss_function(x, x_hat, mean, logvar, beta=1.0):
    criterion = nn.MSELoss(reduction=&quot;mean&quot;)
    rebuilding_loss = criterion(x_hat, x)

    KLD = - 0.5 * torch.mean(1+ logvar - mean.pow(2) - logvar.exp())
    print(f&quot;KLD = {KLD}&quot;)
    return rebuilding_loss + KLD*beta

这是我的 VAE（简化版）：
class VariationalAutoEncoder(nn.Module):
def __init__(self, latent_shape):
super(VariationalAutoEncoder, self).__init__()

self.encoder = nn.Sequential(

nn.Linear(18,5184), # new
nn.LeakyReLU(0.2),
nn.Linear(5184,128), # new

)

# 潜在均值和方差对数 

self.mean_layer = nn.Linear(128, latent_shape)
self.logvar_layer = nn.Linear(128,latent_shape)

# 解码器
self.decoder = nn.Sequential(
nn.Linear(latent_shape,128),

nn.LeakyReLU(0.2),
nn.Linear(128,5184),

nn.LeakyReLU(0.2),
nn.Linear(5184,18), # new
nn.Sigmoid(),

)
def encode(self, x):
x = self.encoder(x)
mean, logvar = self.mean_layer(x), self.logvar_layer(x)
return mean, logvar

def reparameterization(self, mean, logvar):
std = torch.exp(0.5 * logvar)
epsilon = torch.randn_like(std).to(device) 
z = mean + std*epsilon
return z

def decrypt(self, z):
return self.decoder(z)

def forward(self, x):
mean, logvar = self.encode(x)
z = self.reparameterization(mean, logvar)

x_hat = self.decode(z)
return x_hat, mean, logvar


在我使用 10^16 的 beta 值后，KLD 的值会达到 10^-7 的数量级。我不确定为什么会这样。我该怎么办？
我目前的超参数是：Adm Optimizer 的权重衰减为 10，lr 为 0.01，ReduceLROnPlateau 调度程序的耐心为 3，因子为 0.5。我在代码中使用梯度裁剪，max_norm 为 0.5，训练 25 个时期。
潜在形状也是 10。
感谢您的帮助！
我尝试使用较小的值进行 beta 退火和循环退火，甚至使用 10^16 作为 beta 值的 beta KLD，但 KLD 项仍然消失为 10^-8。这就像它在我的任务上随机运行一样。我尝试添加一个卷积层，因为输入是一张大小约为 18 x 18 的图像。没有变化。]]></description>
      <guid>https://stackoverflow.com/questions/78848344/im-training-a-vae-and-my-kld-term-is-vanishing-after-just-a-couple-steps-inside</guid>
      <pubDate>Thu, 08 Aug 2024 12:10:40 GMT</pubDate>
    </item>
    <item>
      <title>Huggingface 自动训练失败</title>
      <link>https://stackoverflow.com/questions/78848134/huggingface-autotrain-fail</link>
      <description><![CDATA[我正在尝试 Huggingface Autotrain。我的数据集：

以下是我的配置：

当我开始训练时，我收到此错误：

我做错了什么？我必须提到分类列和数字列吗？如果是，它们是用逗号分隔的吗？我如何决定哪些列放在哪里？]]></description>
      <guid>https://stackoverflow.com/questions/78848134/huggingface-autotrain-fail</guid>
      <pubDate>Thu, 08 Aug 2024 11:26:32 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft Azure 自定义图像分析模型 - 错误识别产品</title>
      <link>https://stackoverflow.com/questions/78848095/microsoft-azure-custom-image-analysis-model-mistakenly-identifies-products</link>
      <description><![CDATA[我确实需要一些帮助和指导。
我正在为一个产品训练一个自定义图像分析模型。我遵循以下准则 - https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/model-customization?tabs=studio
https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/shelf-model-customization
不幸的是，在我训练了几个模型并对数据集进行了不同的配置后，我没有看到模型输出有任何改进。
一个数据集包括41 张图片和 19 张图片中的一张。我还用产品本身的图片训练了一个模型。我训练模型的预算是 1 小时，昨晚我训练模型的预算是 7 小时。
但是，当我将输出结果叠加在图像上时，我得到的结果完全相同 - 附上图片。该模型是在带有绿色箭头的产品上进行训练的，但它错误地将带有白色包装的货架产品识别为相同的产品。
]]></description>
      <guid>https://stackoverflow.com/questions/78848095/microsoft-azure-custom-image-analysis-model-mistakenly-identifies-products</guid>
      <pubDate>Thu, 08 Aug 2024 11:18:59 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 改进 Telegram 机器人中的关键字识别和提取</title>
      <link>https://stackoverflow.com/questions/78847420/improving-keyword-recognition-and-extraction-in-a-telegram-bot-using-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78847420/improving-keyword-recognition-and-extraction-in-a-telegram-bot-using-python</guid>
      <pubDate>Thu, 08 Aug 2024 08:52:23 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：层“dense_2”需要 1 个输入，但它收到了 2 个输入张量</title>
      <link>https://stackoverflow.com/questions/78846949/valueerror-layer-dense-2-expects-1-inputs-but-it-received-2-input-tensors</link>
      <description><![CDATA[我无法加载我的模型，它一直显示错误
ValueError：层“dense_2”需要 1 个输入，但它收到了 2 个输入张量。收到的输入：[&lt;KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, name=keras_tensor_2896&gt;, &lt;KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, name=keras_tensor_2897&gt;]
这是我的代码
image_generator = ImageDataGenerator(
rescale=1./255,
rotation_range=20,
zoom_range=0.2,
width_shift_range=0.2,
height_shift_range=0.2,
Horizo​​ntal_flip=True,
validation_split=0.2
)

train_dataset = image_generator.flow_from_directory(
directory=path_to_dataset,
target_size=(224, 224),
batch_size=32,
subset=&#39;training&#39;
)

validation_dataset = image_generator.flow_from_directory(
directory=path_to_dataset,
target_size=(224, 224),
batch_size=32,
subset=&#39;validation&#39;
)

# 加载数据集中子文件夹中的 (num_classes) 类
num_classes = len(train_dataset.class_indices)

from tensorflow.keras.applications.mobilenet import MobileNet

# 加载 MobileNet 模型
pre_trained_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),
include_top=False,
weights=&#39;imagenet&#39;)

pre_trained_model.summary()

# 打印数据集信息以供调试
print(f&quot;训练数据集形状：{train_dataset.image_shape}&quot;)
print(f&quot;验证数据集形状：{validation_dataset.image_shape}&quot;)

pre_trained_model.trainable = False

# 为预训练模型添加自定义层
model = tf.keras.Sequential([
pre_trained_model,
tf.keras.layers.GlobalAveragePooling2D(),
tf.keras.layers.Dense(1024,activation=&#39;relu&#39;),
tf.keras.layers.Dropout(0.5),
tf.keras.layers.Dense(num_classes,activation=&#39;softmax&#39;) 
])

# 编译模型
#from tensorflow.keras.optimizers import RMSprop
model.compile(optimizer=Adam(learning_rate=0.0001),
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

# batch=40
# history = model.fit(train_dataset,
# validation_data=validation_dataset,
# epochs=20,
# steps_per_epoch = train_dataset.samples//batch,
# validation_steps = validation_dataset.samples//batch,
# verbose = 1
# )

epochs = 20
batch_size = 32

for epoch in range(epochs):
print(f&quot;Epoch {epoch + 1}/{epochs}&quot;)

# 训练 
# 使用 model.fit 进行训练，而不是手动迭代
history = model.fit(
train_dataset,
epochs=1, # 在外循环中一次训练一个 epoch
validation_data=validation_dataset,
steps_per_epoch=train_dataset.samples // batch_size,
validation_steps=validation_dataset.samples // batch_size,
verbose=1
)

# 验证 - 此部分可以保持不变
val_loss, val_accuracy = model.evaluate(validation_dataset)
print(f&quot;验证 - 损失：{val_loss:.4f}, 准确率： {val_accuracy:.4f}&quot;)

print(&quot;训练完成。&quot;)from keras.models import load_model
model_save_path = &#39;/content/drive/MyDrive/Machine Learning/saved_models/model_plastik.h5&#39;
model.save(model_save_path,save_format=&#39;keras&#39;)

model.summary()
print(f&#39;Model disimpan di: {model_save_path}&#39;)

# 加载模型
model_save_path = &#39;/content/drive/MyDrive/Machine Learning/saved_models/model_plastik.h5&#39;

# 加载模型，确保在需要时对其进行编译
loaded_model = tf.keras.models.load_model(model_save_path) 

# 现在您可以根据需要修改已加载的模型
# 例如，如果您想要提取子模型：
input_layer_index = 0 # 用实际索引替换
dense_2_index = 3 # 用实际索引替换
loaded_model = tf.keras.models.Model(inputs=loaded_model.layers[input_layer_index].input, 
outputs=loaded_model.layers[dense_2_index].output)

# 检查已加载模型的配置
for i, layer in enumerate(loaded_model.layers):
print(f&quot;Layer {i}: {layer.name} - 输入形状：{layer.input_shape} - 输出形状：{layer.output_shape}&quot;)

print(&quot;修订后的模型已成功加载。&quot;)

我尝试加载模型，并希望它已加载以进行测试]]></description>
      <guid>https://stackoverflow.com/questions/78846949/valueerror-layer-dense-2-expects-1-inputs-but-it-received-2-input-tensors</guid>
      <pubDate>Thu, 08 Aug 2024 07:06:54 GMT</pubDate>
    </item>
    <item>
      <title>无法在 AWS 或 Vercel 上部署 ML 项目</title>
      <link>https://stackoverflow.com/questions/78846779/unable-to-deploy-ml-project-on-aws-or-vercel</link>
      <description><![CDATA[我创建了一个简单的 ML 项目，根据性别、体重和身高预测一个人的 BMI，这是 GitHub 代码存储库的链接
https://github.com/sid111nov/BMIProject
我可以在本地机器上运行它，但无法将其部署到 AWS Bean Stack 或 Vercel。然后我认为我的项目可能存在一些问题，但构建似乎没问题。非常感谢任何解决此问题的意见，提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78846779/unable-to-deploy-ml-project-on-aws-or-vercel</guid>
      <pubDate>Thu, 08 Aug 2024 06:23:07 GMT</pubDate>
    </item>
    <item>
      <title>我正在编写决策树修剪算法[关闭]</title>
      <link>https://stackoverflow.com/questions/78846680/i-am-writing-a-decision-tree-pruning-algorithm</link>
      <description><![CDATA[我正在尝试修剪决策树，这是我的代码，它没有按照我的预期工作，这里到底出了什么问题，
我试图实现的是，我试图根据 2 个标准（纯度阈值和人口阈值）修剪树，下面的算法没有给我我想要的结果，我这里缺少什么条件或检查，如何使其更强大？
# 使用前序遍历和标准修剪树的函数
def prune_preorder(inner_tree, index, purity_threshold=95,population_threshold=5):
# 处理当前节点
node_samples = inner_tree.n_node_samples[index]
# 修复修剪节点的特征确定逻辑
feature = feature_names[inner_tree.feature[index]] if inner_tree.feature[index] != -1 else &quot;Leaf节点”
阈值 = f” &lt;= {inner_tree.threshold[index]:.2f}”如果 inner_tree.feature[index] != -1 否则””
值 = inner_tree.value[index][0]
多数类别计数 = max(值)
纯度 = (多数类别计数 / 总和(值)) * 100 如果节点样本 &gt; 0 否则 0
人口百分比 = (节点样本 / 总样本) * 100 如果节点样本 &gt; 0 else 0

# 在修剪决策之前打印节点统计信息
print(f&quot;Node {feature} {index} : Samples={node_samples}, Purity={purity:.2f}%, Population={population_pct:.2f}%&quot;)

# 检查是否应修剪节点
if purity &gt;= purity_threshold orpopulation_pct &lt;人口阈值：
print(f&quot;修剪节点 {feature} {index} {threshold} (纯度： {purity:.2f}%, 人口： {population_pct:.2f}%)&quot;)
# 通过将子指针设置为 -1 将节点标记为叶子
inner_tree.children_left[index] = -1
inner_tree.children_right[index] = -1
else:
# 如果存在左子树，则遍历左子树
if inner_tree.children_left[index] != -1:
prune_preorder(inner_tree, inner_tree.children_left[index], purity_threshold, 人口阈值)

# 如果存在右子树，则遍历右子树
if inner_tree.children_right[index] != -1:
prune_preorder(inner_tree, inner_tree.children_right[index], purity_threshold,人口阈值)

# 使用前序遍历从根节点应用修剪
prune_preorder(clf.tree_, 0, purity_threshold=95,population_threshold=5)

# 为修剪后的树创建节点标签
node_labels_pruned = custom_node_labels(clf, feature_names, xmtrain, ymtrain)

理想情况下，树在满足修剪标准的任何地方都不应有子节点，但我看到一些标记为叶子的节点没有样本，还有更多子节点，理想情况下不应该这样。]]></description>
      <guid>https://stackoverflow.com/questions/78846680/i-am-writing-a-decision-tree-pruning-algorithm</guid>
      <pubDate>Thu, 08 Aug 2024 05:47:13 GMT</pubDate>
    </item>
    <item>
      <title>未找到与 torch==1.9.1 匹配的分布</title>
      <link>https://stackoverflow.com/questions/78846461/no-matching-distribution-found-for-torch-1-9-1</link>
      <description><![CDATA[尝试使用 google colab 安装 torchmeta，但它依赖于 torch&lt;1.10.0 and &gt;=1.4.0，每当我尝试安装 torch 1.9.0 或任何版本的 torch&lt;1.10.0 and &gt;=1.4.0 时，都会出现以下错误：

错误：找不到满足要求 torch==1.9.1 的版本（来自版本：1.11.0、1.12.0、1.12.1、1.13.0、1.13.1、2.0.0、2.0.1、2.1.0、2.1.1、2.1.2、2.2.0、2.2.1、2.2.2， 2.3.0、2.3.1、2.4.0)错误：未找到与 torch==1.9.1 匹配的发行版

如何解决此问题？
我正在使用 Google colab，Python 版本 3.10.12。
如果我遇到的问题无法解决，请告诉我使用 colab 安装 torchmeta 的其他方法。
我正在尝试安装 torch&lt;1.10.0 和 &gt;1.4.0，然后安装依赖于我提到的 torch 版本 torch&lt;1.10.0 和 &gt;1.4.0 的 torchmeta。]]></description>
      <guid>https://stackoverflow.com/questions/78846461/no-matching-distribution-found-for-torch-1-9-1</guid>
      <pubDate>Thu, 08 Aug 2024 03:54:54 GMT</pubDate>
    </item>
    <item>
      <title>face_recognition 模块以某种方式严重干扰了 Speech_recognition 模块（python）</title>
      <link>https://stackoverflow.com/questions/78845929/face-recognition-module-somehow-badly-interfering-with-speech-recognition-module</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78845929/face-recognition-module-somehow-badly-interfering-with-speech-recognition-module</guid>
      <pubDate>Wed, 07 Aug 2024 22:40:22 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Sarimax 预测日期？</title>
      <link>https://stackoverflow.com/questions/78845748/how-to-project-forecast-dates-with-sarimax</link>
      <description><![CDATA[我正在使用 Sarimax 进行预测：
# 仅使用 VENDA 过滤数据集
df_int_aux = pd.DataFrame(df_internal[&#39;VENDA&#39;])

# 使用训练数据创建模型
train = round(len(df_int_aux) * 0.85) 
test = len(df_int_aux) - train

model_val = sm.tsa.statespace.SARIMAX(df_int_aux[&quot;VENDA&quot;][:train], order=(0,0,1), seasonal_order=(1, 1, 1, 4), exog=df_internal[&#39;C_EF_VENDA&#39;][:train])

# 拟合模型
model_val_fit = model_val.fit()

# 预测测试数据
validation = model_val_fit.get_forecast(steps=test, exog=df_internal[&#39;C_EF_VENDA&#39;][-test:]) 
validation_mean = validation.predicted_mean

但是，validation_mean 数据集未显示未来日期。它显示的数字索引范围从 101 到 118。数据集有 109 行。我使用前 100 行进行训练，因此第 101 行到第 118 行是模型的预测值。
为什么没有显示预计日期？我该如何解决这个问题？
以下是数据集的示例。可能是因为日期没有遵循特定的频率或模式，所以没有显示日期？
DATE VENDA C_EF_VENDA
2022-01-01 6.004414 12.122044
2022-01-11 10.933905 22.073975
2022-01-18 11.589626 23.397781
2022-01-25 21.005069 42.406200
2022-02-01 8.639416 14.461015
2022-02-08 16.847755 28.200475
2022-02-15 17.289413 28.939740
2022-02-22 16.966222 28.398770
]]></description>
      <guid>https://stackoverflow.com/questions/78845748/how-to-project-forecast-dates-with-sarimax</guid>
      <pubDate>Wed, 07 Aug 2024 21:30:19 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 和 Opacus 用于差异隐私</title>
      <link>https://stackoverflow.com/questions/78839246/pytorch-and-opacus-for-differential-privacy</link>
      <description><![CDATA[在使用 Jupyter Notebook（可从此处获取）测试来自 TensorFlow 网站的示例代码时，我遇到了一个错误。您可以在此处找到我关于该错误的 SO 问题。
因此，我决定使用 PyTorch、Opacus 和 PySyft 为相同功能编写等效实现。然而，不幸的是，我又遇到了另一个错误。
下面是实现与 TensorFlow 网站中的示例代码相同功能的代码，但使用 PyTorch 和 Opacus 和 PySyft，以及错误消息。
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from opacus import PrivacyEngine

# 定义一个简单的模型
class SimpleCNN(nn.Module):
def __init__(self):
super(SimpleCNN, self).__init__()
self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
self.fc1 = nn.Linear(32*26*26, 10)

def forward(self, x):
x = torch.relu(self.conv1(x))
x = x.view(-1, 32*26*26)
x = self.fc1(x)
return torch.log_softmax(x, dim=1)

# 数据加载器
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST(&#39;.&#39;, train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# 初始化模型、优化器和损失函数
model = SimpleCNN()
optimizer = optim.SGD(model.parameters(), lr=0.01)
criterion = nn.NLLLoss()

# 初始化 PrivacyEngine
privacy_engine = PrivacyEngine(
model,
batch_size=64,
sample_size=len(train_loader.dataset),
epochs=1,
max_grad_norm=1.0,
)

privacy_engine.attach(optimizer)

# 训练循环
model.train()
for epoch in range(1):
for data, target in train_loader:
optimizer.zero_grad()
output = model(data)
loss = criterion(output, target)
loss.backward()
optimizer.step()

# 打印隐私统计数据
epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(1e-5)
print(f&quot;Epsilon: {epsilon}, Delta: 1e-5&quot;)

错误：
-------------------------------------------------------------------------------
TypeError Traceback (最近一次调用最后一次)
Cell In[1]，第 32 行
29 criterion = nn.NLLLoss()
31 # 初始化 PrivacyEngine
---&gt; 32 privacy_engine = PrivacyEngine(
33 model,
34 batch_size=64,
35 sample_size=len(train_loader.dataset),
36 epochs=1,
37 max_grad_norm=1.0,
38 )
40 privacy_engine.attach(optimizer)
42 # 训练循环

TypeError: PrivacyEngine.__init__() 获得了意外的关键字参数“batch_size”
]]></description>
      <guid>https://stackoverflow.com/questions/78839246/pytorch-and-opacus-for-differential-privacy</guid>
      <pubDate>Tue, 06 Aug 2024 13:17:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 SKforecast 时出错：[Int64Index([48, ...],\n dtype='int64', name='date_time')] 均不在 [index] 中</title>
      <link>https://stackoverflow.com/questions/77013878/error-when-using-skforecast-none-of-int64index48-n-dtype-int64-na</link>
      <description><![CDATA[我有使用 groupby（基于日期和组）的数据集，结果如下 Dataframe:
| 日期 | 组 | 值 |
|:---- |:------:| -----:|
| 2022-01-01 | 12 | 25.2|
| 2022-01-01 | 15 | 36.54|
| 2022-02-01 | 12 | 55.3|
| 2022-02-01 | 15 | 69.2|

最后我有 177 行。
我的第一个问题是我无法应用 data.asfreq
data = data.asfreq(&#39;MS&#39;) 

我收到此错误：
ValueError：无法在具有重复标签的轴上重新索引

我的第二个问题是当我想应用 xgboost 时我使用了此代码：
data = data.set_index(&#39;date_time&#39;)
#data = data.asfreq(&#39;MS&#39;)
data = data.sort_index()

end_train = &#39;2023-05-01&#39;
end_validation = &#39;2023-06-01&#39;
data_train = data.loc[: end_train, :]
data_val = data.loc[end_train:end_validation, :]
data_test = data.loc[end_train:, :]

print(f&quot;日期训练：{data_train.index.min()} --- {data_train.index.max()} (n={len(data_train)})&quot;)
print(f&quot;日期验证：{data_val.index.min()} --- {data_val.index.max()} (n={len(data_val)})&quot;)
print(f&quot;日期测试：{data_test.index.min()} --- {data_test.index.max()} (n={len(data_test)})&quot;)

forecaster = ForecasterAutoreg(
regressor = XGBRegressor(random_state=123),
lags = 24
)

# 回归器超参数
param_grid = {
&#39;n_estimators&#39;: [100, 500],
&#39;max_depth&#39;: [3, 5, 10],
&#39;learning_rate&#39;: [0.01, 0.1]
}

# 用作预测器的滞后值
lags_grid = [48, 72]

results_grid = grid_search_forecaster(
Forecaster = Forecaster,
y = data.loc[:end_validation, &#39;value&#39;], # 训练和验证数据
param_grid = param_grid,
lags_grid = lags_grid,
steps = 36,
refit = False,
metric = &#39;mean_squared_error&#39;,
initial_train_size = len(data_train),
fixed_train_size = False,
return_best = True,
n_jobs = &#39;auto&#39;,
verbose = False
)

我收到此错误：
KeyError: &quot;None of [Int64Index([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64,\n 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n 82, 83, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61、62、\n 63、64、65、66、67、68、69、70、71、72、73、74、75、76]、\n dtype=&#39;int64&#39;, name=&#39;date_time&#39;)] 在 [index]&quot;

我尝试更改 lags_grid，但出现了几乎相同的错误。
在此先感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/77013878/error-when-using-skforecast-none-of-int64index48-n-dtype-int64-na</guid>
      <pubDate>Thu, 31 Aug 2023 07:59:30 GMT</pubDate>
    </item>
    </channel>
</rss>