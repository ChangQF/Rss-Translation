<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Tue, 25 Feb 2025 15:20:39 GMT</lastBuildDate>
    <item>
      <title>在线性调度程序训练DDPM后，如何实现其他类型的调度程序？</title>
      <link>https://stackoverflow.com/questions/79465469/how-do-i-implement-a-different-type-of-scheduler-after-training-my-ddpm-on-a-lin</link>
      <description><![CDATA[我使用以下代码定义的线性调度程序训练了扩散模型（DDPM）：
 调度程序= ddpmscheduler（
    num_train_timesteps = 1000，
    beta_start = 0.0001，
    beta_end = 0.02，
    beta_schedule =＆quot&#39;线性
）
 
但是，在训练此模型并使用代码加载之后：
  def load_checkpoint（模型，优化器，checkpoint_）：
    ““从检查点加载模型和优化器词典”。“”。
    checkpoint = torch.load（checkpoint_path，map_location =设备）
    model.load_state_dict（checkpoint [&#39;model_state_dict&#39;]）
    Optimizer.load_state_dict（checkpoint [&#39;Optimizer_state_dict&#39;]）
    start_epoch = checkpoint [&#39;epoch&#39;] + 1＃从下一个时代恢复。
    打印（f＆quot“从epoch {start_epoch}＆quot恢复）
    返回start_epoch
 
当我尝试切换到余弦调度程序以使用以下代码推理：
 调度程序= ddpmscheduler（
    num_train_timesteps = 1000，
    beta_schedule =＆quot; 
）
 
我有以下错误：
  notimplementedError：cosine未针对＆lt; class&#39;dribfusers.schedulers.scheduling_ddpm.ddpmm.ddpmscheduler&#39;＆gt; gt;
 
我尝试将DDPM调度程序更改为DDIM调度程序，将代码更改为：
 调度程序= ddimscheduler（
    num_train_timesteps = 1000，
    beta_schedule =＆quot;
）
 
，但我仍然有一个非常相似的错误：
  notimplementedError：cosine未针对＆lt; class&#39;drifusers.schedulers.scheduling_ddim.ddimscheduler&#39;＆gt; gt;
 
我不明白我在做什么错，因为我从多个来源读到，在一个调度程序上训练扩散模型是正常的（在我的情况下是线性），然后更改为推理的另一个调度程序（在我的情况，余弦）。我如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79465469/how-do-i-implement-a-different-type-of-scheduler-after-training-my-ddpm-on-a-lin</guid>
      <pubDate>Tue, 25 Feb 2025 05:14:09 GMT</pubDate>
    </item>
    <item>
      <title>ASR的单方言语音语料库[关闭]</title>
      <link>https://stackoverflow.com/questions/79464867/single-dialect-speech-corpus-for-asr</link>
      <description><![CDATA[我正在进行有关减轻自动语音识别系统中音调偏差的研究。我的研究涉及测试流行的语音助手和创建的ASR系统。但是，我正在努力寻找用于测试和培训数据集的语音数据集。因为我专注于音调，所以我想通过找到一个每个人都具有相同口音/英语方言的数据集来缩小变异性。由于大多数语音语料库都专注于ASR培训的不同演讲者，因此很难遇到这一点。我发现了一个免费的语料库，其中包含来自不列颠群岛的6个方言（）。但是，由于我仅利用一种方言，即使是最坚固的方言也没有提供来自高音的声音的足够数据。我正在寻找大约80-350 Hz的一系列音高。一些录音确实降落在300-350上方的范围内，但只有大约12个，这对我的数据集还不够。我花了无数小时的时间进行搜索，并努力寻找合适的语料库。我没有钱可以花在演讲语料库上，所以有人对我能做什么有任何建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79464867/single-dialect-speech-corpus-for-asr</guid>
      <pubDate>Mon, 24 Feb 2025 21:45:16 GMT</pubDate>
    </item>
    <item>
      <title>多标签分类任务的自动编码器</title>
      <link>https://stackoverflow.com/questions/79463813/autoencoder-for-multi-label-classification-task</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79463813/autoencoder-for-multi-label-classification-task</guid>
      <pubDate>Mon, 24 Feb 2025 14:17:53 GMT</pubDate>
    </item>
    <item>
      <title>grpotrainer不支持iterabledataset</title>
      <link>https://stackoverflow.com/questions/79462501/iterabledataset-not-supported-on-grpotrainer</link>
      <description><![CDATA[执行时以下程序崩溃
 从数据集导入iterabledataset，数据集
来自TRL Import grpoconfig，grpotrainer

提示= [hi＆quot;
def data_generator（）：
    而真：
        在提示中s：
            产生{提示; ：S}
dataset = iterabledataset.from_generator（data_generator）


triending_args = grpoconfig（
    output_dir =＆quot; tmp＆quort;
    max_steps = 1000，
）

培训师= grpotrainer（
    型号=“ Facebook/opt-350m”
    Reward_funcs = Lambda提示，完成，** Kwargs：[1]*8，
    train_dataset =数据集，
    args =训练_args，
）

Trainer.Train（）
 
导致以下迹线：
  trackback（最近的最新通话）：
  file＆quot＆quort＆quot＆quode/code/code/cs234/starter_code/trl_testing.py&quot;，第24行，in＆lt; module＆gt;
    Trainer.Train（）
  file＆quot＆quort＆quort＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/transformers/trainer.py&amp;py&quot;，第2241号线
    return innion_training_loop（
  file＆quot＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/transformers/trainer.py&quot; line 2500，in _inner_training_training_loop in
    batch_samples，num_items_in_batch = self.get_batch_samples（epoch_iterator，num_batches）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/transformers/trainer.py&amp;py&quot; line 5180，在get_batch_samples中
    batch_samples += [next（epoch_iterator）]
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    next_batch，next_batch_info = self._fetch_batches（main_iterator）
  file＆quot＆quort＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/accelerate/data_loader.py＆quot＆quot; line 812，in _fetch_batches
    批处理=连接（批次，dim = 0）
  file＆quot＆quort＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot congatenate in Compatenate in Compatenate
    返回honador_type（data [0]，（condenenate（数据中的d [d [i]），dim = dim = dim）for range（len（data [0]））））））））））））））））
  file＆quot＆quort＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回类型（OBJ）（生成器）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回honador_type（data [0]，（condenenate（数据中的d [d [i]），dim = dim = dim）for range（len（data [0]））））））））））））））））
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回类型（data [0]）（{k：condenate（数据中的d [d [k]），data [0] .keys（）}的k中的k = dim = dim）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回类型（data [0]）（{k：condenate（数据中的d [d [k]），data [0] .keys（）}的k中的k = dim = dim）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    提高typeerror（f＆quot“只能连接张量，但得到{type（data [0]）};）
typeError：只能连接张量，但可以得到＆lt; class&#39;Str&#39;＆gt;
 
但是，用类似的 dataset 替换 iterabledataset 解决了问题：
 从数据集导入iterabledataset，数据集
来自TRL Import grpoconfig，grpotrainer

提示= [hi＆quot;
dataset = dataset.from_dict（{提示＆quot;：提示}）

triending_args = grpoconfig（
    output_dir =＆quot; tmp＆quort;
    max_steps = 1000，
）

培训师= grpotrainer（
    型号=“ Facebook/opt-350m”
    Reward_funcs = Lambda提示，完成，** Kwargs：[1]*8，
    train_dataset =数据集，
    args =训练_args，
）

Trainer.Train（）
 
这已经在2个截然不同的系统上复制了，因此这不太可能是原因。
我想念什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/79462501/iterabledataset-not-supported-on-grpotrainer</guid>
      <pubDate>Mon, 24 Feb 2025 05:08:12 GMT</pubDate>
    </item>
    <item>
      <title>如何使用Docling库从DOCX文件中提取页面上的HTML内容，以检测页面断路？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79461458/how-to-extract-page-wise-html-content-from-docx-files-using-docling-library-by-d</link>
      <description><![CDATA[我已经成功地使用 docling&gt; docling 和 pypdf2 。这是我当前代码对PDF的作用：

使用PYPDF2将PDF分为单个页面
使用Docling的 Document Converter  将每个页面转换为HTML
用嵌入式图像提取HTML内容
添加元数据（页码，文档ID，文件名）
将所有内容保存到JSON结构

重要说明：我首先将PDF划分为单个页面的原因是因为Docling的 save_as_html（） and  export_to_to_html（）函数在完整的文档对象上工作，而不是在单个页面。要获取页面html内容，我需要创建临时的单页PDF并分别转换一个。
这是我每页获得的示例JSON结构：
  {
    ＆quot“ page”：“第1页”
    ＆quot“ content＆quot”：＆quot＆lt; html内容＆gt;＆quot ,,
    “元数据：{{
        ＆quot“ docutsId”：; quot“ uuid; quot”
        “文件名”：“ document.pdf”
        &#39;page_number＆quot”：1，
        ＆quot“ total_pages”：总计
    }
}
 
现在，我需要为DOCX文件实现相同的功能。据我说，DOCX文件包含标题，页脚和页面断路等元素，我们可以使用这些页面中断将内容分为页面。
我正在使用Docling库进行转换（DOCX到HTML），但是我无法识别或检测到DOCX文件中的页面中断。由于Docling的HTML转换在完整的文档上起作用，因此我需要根据页面断路首先将DOCX内容拆分，类似于我处理PDF的方式。
问题：

如何使用Docling在DOCX文件中检测页面中断？
是否有一种方法可以根据这些页面断开以创建单独的文档对象来拆分DOCX内容？
如果Docling不直接支持此内容，是否还有其他python库，我应该与文档一起使用以检测和拆分页面中断？

我尝试查看文档文档，但找不到有关DOCX文件中处理页面中断的信息。
任何帮助或指导将不胜感激！
我正在使用的相关库：

  docling 
  python-docx （如果需要）
]]></description>
      <guid>https://stackoverflow.com/questions/79461458/how-to-extract-page-wise-html-content-from-docx-files-using-docling-library-by-d</guid>
      <pubDate>Sun, 23 Feb 2025 15:19:41 GMT</pubDate>
    </item>
    <item>
      <title>如何使用FBProphet使用多个回归剂预测值？</title>
      <link>https://stackoverflow.com/questions/79456368/how-to-forecast-values-with-multiple-regressors-using-fbprophet</link>
      <description><![CDATA[我想使用FBProphet模型预测小时温度值。到目前为止，我已经对DS和Y变量进行了培训，这给了我良好的结果。但是现在我想添加额外的回归器，然后执行预测。
将模型与额外的回归器拟合后，我在测试数据集上对其进行了测试，这使我准确。但是主要问题是如何预测未来（我的测试集超出我的测试集）
这是我到目前为止所做的。
 ＃数据准备和功能工程

temp = df [[[＆quot; weverip; quot; quot; quot;]]。应用（kelvintodegc）.copy（）
temp [&#39;hourlylag＆quot;] = temp [温度＆quot＆quot。shift（1）.bfill（）
temp [&#39;dailylag&#39;&#39;] = temp [温度＆quot＆quot。shift（24）.bfill（）
temp [＆quot; weeklylag＆quot;] = temp [温度＆quot＆quot＆quot。shift（24*7）.bfill（）
temp [movmean＆quot;] = temp [温度＆quot＆quot＆quot＆quot;
temp [mstd＆quot;] = temp [温度＆quot＆quot＆quot＆quot;
temp [ub＆quot; quot&#39;] = temp [movmean＆quot;] +（1.6 * temp [mstd;]）
temp [lb＆quot;] = temp [movmean;]  - （1.6 * temp [mstd;]）
temp [＆quot; devfromean＆quot; quot; temp [movemean＆quord; temp [温度＆quort; quort&#39;&#39;]
temp [＆quot; devfromub＆quot;] = temp [ub quot; quot; temp [temp;
temp [devfromlb; quot; quot; quot temp [lb＆quot&#39;]  - 温度；
temp [小时;
temp [&#39;Dayofyear＆quort;] = temp.index.day
temp [; quot; quot; quot＆quot temp.index.month
temp = temp.Reset_index（）
temp.rename（columns = {; date;：＆quord ds; quot; quot; quot; quot; quot; quot; quot; y y y}

模型=先知（）
model.Add_regressor（“ hourlylag”）
model.Add_regressor（“ Dailylag”）
model.Add_regressor（“每周”
model.add_regressor（“ movmean;）
Model.Add_Regressor（“ MSTD”）
model.Add_regressor（&#39;ub＆quot;）
model.add_regressor（&#39;lb＆quot;）
model.add_regressor（“ Devfromean＆quot”）
Model.Add_Regressor（“ DevFromub”）
Model.Add_regressor（&#39;Devfromlb＆quort;）
model.add_regressor（“小时”）
model.Add_regressor（“ Dayofyear”）
model.add_regressor（“月”）

型号（火车）

这些是MAE和MAPE分数
MAE：0.00
Mape：0.17％

现在未来= model.make_future_dataframe（周期= 24 * 365 * 3，freq =; h＆quot;）

我有这个错误

ValueError Trackback（最近的最新电话）
[68]中的单元，第2行
      1未来= model.make_future_dataframe（周期= 8760，freq =; h＆quot;）
----＆gt; 2预测=模型。预定（未来）

文件C：\ USER \ 5923imtiaz \ AppData \ local \ local \ anaconda3 \ envs \ ai \ ai \ lib \ lib \ site-packages \ prophet \ forecaster.py.py.py.py：1270，in Prophet.prophet.predt.predict.predict.predict.predict.predict（self，df，df，vectorized，vectorized）
   1268如果DF.Shape [0] == 0：
   1269提高价值Error（“数据框架没有行。”）
 - ＆gt; 1270 df = self.setup_dataframe（df.copy（））
   1272 DF [&#39;趋势&#39;] = self.predict_trend（df）
   1273 sipersal_components = self.predict_seasonal_components（df）

文件C：\ USER \ 5923imtiaz \ AppData \ local \ local \ anaconda3 \ envs \ ai \ ai \ lib \ lib \ site-packages \ prophet \ forecaster.py.py.py：297，in PropHet.set.setup.dataframe（self，ddf，diredize_scales）
    295在self.extra_regressor中名称：
    296如果不在DF中的名字：
 - ＆gt; 297提高价值Error（
    298&#39;回归器{name！r} dataframe中缺少
    299 .format（名称=名称）
    300）
    301 df [name] = pd.to_numeric（df [name]）
    302如果DF [name] .isnull（）。任何（）：

valueerror：dataFrame中缺少回归器“小时lag”
 ]]></description>
      <guid>https://stackoverflow.com/questions/79456368/how-to-forecast-values-with-multiple-regressors-using-fbprophet</guid>
      <pubDate>Fri, 21 Feb 2025 04:48:53 GMT</pubDate>
    </item>
    <item>
      <title>为什么拥抱面提供的DeepSeek代码会导致“未知量化类型”错误？</title>
      <link>https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t</link>
      <description><![CDATA[我正在使用huggingface的此代码：
此代码直接从 deepseek上的huggingface网站页面上要插件代码：

 来自变形金刚导入管道

消息= [
{&#39;&#39;：＆quot“ user quot”内容“：;
这是给出的
pipe =管道（＆quot&#39;text-generation＆quot; deepseek-ai/deepseek-r1＆quort; trust_remote_code = true）
管道（消息）
 

，但我无法加载模型。当我这样做时，我会得到这个问题：

 file＆quot＆lt; ...＆gt;/site-packages/transformers/quantizers/auto.py&quot;，第97行，in_dict 
 提高ValueError（

ValueError：未知量化类型，获得FP8-支持类型为： 
[&#39;awq&#39;，&#39;bitsandbytes_4bit&#39;，&#39;bitsandbytes_8bit&#39;，&#39;gptq&#39;，&#39;aqlm&#39;，&#39;quanto&#39;，&#39;eetq&#39;，&#39;eetq&#39;， 
&#39;HQQ&#39;，“压缩张量”，“ fbgemm_fp8&#39;，&#39;torchao&#39;，&#39;bitnet&#39;]
 

我尝试了不同的代码：
 导入火炬
generate_text = pipeline（model =; deepSeek-ai/deepSeek-r1; torch_dtype = torch.bfloat16，trust_remote_code = true，device_map =; auto;
generate_text（消息）
 
这给出以下错误：

提高ValueError（valueError：未知量化类型，获得FP8-支持类型为：[&#39;awq&#39;，&#39;bitsandbytes_4bit&#39;，&#39;bitsandbytes_8bit&#39;，gptq&#39;，&#39;gptq&#39;，&#39;aqlm&#39;&#39;aqlm&#39;&#39;，&#39;aqlm&#39;，&#39; &#39;，&#39;hqq&#39;，&#39;compressed Tensors&#39;，&#39;fbgemm_fp8&#39;， &#39;torchao&#39;，&#39;bitnet&#39;，&#39;vptq&#39;] 

我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t</guid>
      <pubDate>Sun, 09 Feb 2025 03:05:30 GMT</pubDate>
    </item>
    <item>
      <title>我如何成功设置和检索元数据信息以在Huggingface Hub上的HuggingFacedatAset？</title>
      <link>https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat</link>
      <description><![CDATA[我有许多数据集，我是从诸如此类的字典中创建的：
  info = datasetinfo（
        Description =&#39;我的快乐LIL数据集
        版本=; 0.0.1＆quot;
        homepage =＆quot; https：//www.myhomepage.co.uk＆quot;
    ）
train_dataset = dataset.from_dict（prepary_data（data [＆quot; train;]），info = info）
test_dataset = dataset.from_dict（prepary_data（数据[test; test;]），info = info）
验证_DATASET = DATASET.FROM_DICT（prepary_data（data [data [＆quot; quartation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quote = info = info）
 
 i然后将它们集合到数据集中。
 ＃创建一个datasetDict
dataset = datasetDict（
    {train＆quort＆quot; train_dataset，&#39;test;：test_dataset，;
）
 
到目前为止，一切都很好。如果我访问 dataset [&#39;train&#39;]。info.description 我看到“我的快乐lil dataset”的预期结果。
所以我推到轮毂上，就像：
  dataset.push_to_hub（f＆quot {agrompome}/{repo_name}＆quits＆quits_message =＆quort; some some commin
 
这也成功了。
但是，当我来将数据集从集线器中拉回并访问与之关联的信息时，而不是获取数据集的描述时，我只会得到一个空字符串；喜欢：
  pulled_data = full = load_dataset（＆quot; f {agrommy}/{repo_name}＆quort＆quort; use_auth_token = true）

＃我希望以下内容打印出来“我的快乐LIL数据集”。
print（pulled_data [&#39;train;]。info.Description）
＃但是，它返回&#39;&#39;
 
我是否错误地从集线器加载数据？我是只推出数据集而不是以某种方式推出信息吗？
我觉得我缺少一些明显的东西，但我真的不确定。]]></description>
      <guid>https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat</guid>
      <pubDate>Wed, 17 Jul 2024 13:23:04 GMT</pubDate>
    </item>
    <item>
      <title>如何在NLTK中下载Punkt Tokenizer？</title>
      <link>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</link>
      <description><![CDATA[我使用安装了NLTK库
  PIP安装NLTK
 
使用lib 
 来自nltk.tokenize导入send_tokenize 
send_tokenize（文本）
 
我遇到此错误
  lookuperror： 
****************************************************** ********************
  找不到资源朋克。
  请使用NLTK下载器获取资源：

  ＆gt;＆gt;＆gt;导入NLTK
  ＆gt;＆gt;＆gt; nltk.download（&#39;punkt&#39;）
  
  有关更多信息，请参见：https：//www.nltk.org/data.html

  尝试加载dokenizers/punkt/English.pickle

  搜索：
     - &#39;c：\\用户\\ adars/nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ share \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ lib lib \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\漫游\\ nltk_data&#39;
     - &#39;c：\\ nltk_data&#39;
     - &#39;d：\\ nltk_data&#39;
     - &#39;e：\\ nltk_data&#39;
     - &#39;&#39;&#39;
 
因此，为了解决此错误，我尝试了
 导入NLTK
nltk.download（&#39;punkt&#39;）
 
但是我无法下载此软件包，因为每次运行时，我都会收到错误的错误
  [nltk_data]错误加载punkt：＆lt; urlopen错误[WinError 10060] a
[nltk_data]连接尝试失败，因为连接的聚会
[nltk_data]一段时间后没有正确响应，或者
[nltk_data]建立的连接失败，因为连接的主机
[nltk_data]未能响应＆gt;
 
请在这里帮助我]]></description>
      <guid>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</guid>
      <pubDate>Tue, 19 Sep 2023 04:36:59 GMT</pubDate>
    </item>
    <item>
      <title>在微调过程中，如何正确设置垫子令牌（不是EOS），以避免模型不预测EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch Runtimeerror：Mat1＆Mat2形状无法乘以</title>
      <link>https://stackoverflow.com/questions/75693007/pytorch-runtimeerror-mat1-mat2-shapes-cannot-be-multiplied</link>
      <description><![CDATA[我正在Pytorch上建立CNN并收到以下错误消息：

 RuntimeError：MAT1和MAT2形状无法乘以（32x32768和
512x256）

我已经构建了以下模型：
  def classifier_block（输入，输出，kernel_size，stride，last_layer = false）：
  如果不是last_layer：
    x = nn。
        nn.conv2d（输入，输出，kernel_size，大步，填充= 3），
        nn.batchnorm2d（输出），，
        nn.leakyrelu（0.2，intplophe = true）
    ）
  别的：
    x = nn。
        nn.conv2d（输入，输出，kernel_size，大步），
        nn.maxpool2d（kernel_size = 3，步幅= 2，填充= 1）
    ）
  返回x

类分类器（nn.module）：
  def __init __（self，input_dim，输出）：
    超级（分类器，self）.__ init __（）
    self.classifier = nn。
        classifier_block（input_dim，64、7、2），
        classifier_block（64、64、3、2），
        classifier_block（64、128、3、2），
        classifier_block（128，256，3，2），
        classifier_block（256，512，3，2，true）
    ）
    打印（&#39;clf：&#39;，self.classifier）
    
    self.linear = nn.Sequepention（
        nn.linear（512，256），
        nn.relu（inplace = true），
        nn.linear（256，128），
        nn.relu（inplace = true），
        nn.linear（128，64），
        nn.relu（inplace = true），
        nn.linear（64，输出）
    ）
    打印（&#39;linear：&#39;，self.linear）
  
  向前（自我，图像）：
    打印（&#39;img：&#39;，image.shape）
    x = self.classifier（图像）
    打印（&#39;X：&#39;，X.Shape）
    返回self.linear（x.View（len（x），-1））
 
输入图像是大小 512x512 。这是我的训练障碍：
  loss_train = []
loss_val = []

对于范围（时期）的时期：
  print（&#39;epoch：{}/{}&#39;。格式（epoch，epochs））
  total_train = 0
  CRORCE_TRAIN = 0
  cumloss_train = 0
  classifier.train（）
  对于枚举（x，y）的批次（train_loader）：
    x = x.to（设备）
    打印（X.Shape）
    打印（y.形）
    输出=分类器（x）
    损失=标准（输出，y.to（设备））
    优化器.zero_grad（）
    loss.backward（）
    优化器.step（）

    打印（&#39;损失：{}&#39;。格式（损失））
 
任何建议都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/75693007/pytorch-runtimeerror-mat1-mat2-shapes-cannot-be-multiplied</guid>
      <pubDate>Fri, 10 Mar 2023 06:48:16 GMT</pubDate>
    </item>
    <item>
      <title>如何在虚线文本验证码中找到轮廓图像</title>
      <link>https://stackoverflow.com/questions/71261999/how-to-find-contours-in-dotted-text-captcha-image</link>
      <description><![CDATA[我是OpenCV的新手。我正在尝试找到验证码图像的轮廓。仅当我的验证码包含虚线文本时，它不起作用。
我已经完成了以下代码：
 导入numpy作为NP
导入CV2作为CV
导入imgaug.augmenters为IAA

im = cv.imread（&#39;dataset/1.jpg&#39;）
imgray = cv.cvtcolor（im，cv.color_bgr2gray）

imgray = cv.threshold（Imgray，127，255，0）[1]

dst = cv.canny（imgray，0,150）
bluret = cv.blur（dst，（5,5），0）
img_thresh = cv.AdaptivEthreshold（Blud，255，cv.Adaptive_thresh_gaussian_c，cv.thresh_binary_inv，11，2）

内核= cv.getStructuringElement（cv.morph_rect，（3,3））
阈值= cv.morphologyex（img_thresh，cv.morph_close，kernel）

轮廓，层次结构= cv.findcontours（dst，cv.retr_tree，cv.chain_approx_simple）
打印（Len（Contours））
＃cv.drawContours（IM，轮廓，-1，（0，255，0），3）

cv.imshow（“ img_thresh”，img_thresh）
cv.imshow（dst&#39;dst）
cv.imshow（“阈值”，阈值）
CV.Waitkey（0）
cv.destroyallwindows（）
 
有人可以帮忙吗？有什么方法可以在此图像中找到轮廓？
  ]]></description>
      <guid>https://stackoverflow.com/questions/71261999/how-to-find-contours-in-dotted-text-captcha-image</guid>
      <pubDate>Fri, 25 Feb 2022 06:39:32 GMT</pubDate>
    </item>
    <item>
      <title>如何找到稀疏矢量的最近邻居</title>
      <link>https://stackoverflow.com/questions/34611337/how-to-find-the-nearest-neighbor-of-a-sparse-vector</link>
      <description><![CDATA[我有大约500个向量，每个向量是1500维矢量，
几乎每个向量都很稀疏 - 我的意思是，矢量的30-70维度不是0。
现在，问题在于，这里是一个给定的向量，也是1500个维度，我需要将其与500个向量进行比较，以查找500个最接近的矢量。（在Euclidean距离中）。。
毫无疑问，蛮力方法是一种解决方案，但是我需要计算500次的距离，这需要很长时间。
昨天，我读了一篇文章“用大词汇和快速的空间匹配”的文章，它说使用倒置索引会有所帮助，它说：
   
但是，在我的测试之后，几乎没有任何意义，想象一个1500矢量，其中50个尺寸并不为零，当涉及另一个尺寸时，它们可能总是具有相同的尺寸，而不是零。换句话说，这种算法只能排除一个小矢量，我仍然需要与剩下的许多向量进行比较。
我的问题：

 此算法是否有意义？

 还有其他方法可以做我想做的事吗？例如Flann或KD-Tree？
但是我想要精确的准确的最近邻居，大约是一个不够的

]]></description>
      <guid>https://stackoverflow.com/questions/34611337/how-to-find-the-nearest-neighbor-of-a-sparse-vector</guid>
      <pubDate>Tue, 05 Jan 2016 12:07:01 GMT</pubDate>
    </item>
    <item>
      <title>数据归一化[关闭]</title>
      <link>https://stackoverflow.com/questions/21554301/data-normalization</link>
      <description><![CDATA[当我想分类“好”时或“最佳”然后，我可以使用Facebook的计数或Twitter转发计数的计数。
但是有些社区的用户群很大，因此他们的链接获得了更多的喜欢或转发。我该如何“归一化”这些巨大的社区喜欢例如，像count这样的小得多的社区的类似新闻项目链接之类的链接？
这被称为正常化吗？我可以在哪种书籍中学习有关“质量”的这类算法。 （例如，在这种情况下）？无论如何，我想做什么？]]></description>
      <guid>https://stackoverflow.com/questions/21554301/data-normalization</guid>
      <pubDate>Tue, 04 Feb 2014 13:47:45 GMT</pubDate>
    </item>
    <item>
      <title>数据归一化的参考文献[关闭]</title>
      <link>https://stackoverflow.com/questions/5652357/references-for-data-normalization</link>
      <description><![CDATA[对于NNS和其他机器学习算法，将数据标准化（不确定是否正确）的最佳实践是什么？  我的意思是您如何表示NN/Algo的数据。
例如，您如何表示商店代码？  商店555不大于或小于554，它只是一个分类。 NNS/ALGO模型只是单独过滤出来，还是您需要使它们进行分类而不是数学上的区别？
 编辑：感谢大家的答案。  我一直在挖掘很多数据挖掘书，尽管我发现了一些在预处理的数据主题上花了一两章的时间，但我对最掩饰的效果最大感到有些惊讶。  再次感谢。]]></description>
      <guid>https://stackoverflow.com/questions/5652357/references-for-data-normalization</guid>
      <pubDate>Wed, 13 Apr 2011 16:17:41 GMT</pubDate>
    </item>
    </channel>
</rss>