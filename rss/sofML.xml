<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 19 Sep 2024 06:23:57 GMT</lastBuildDate>
    <item>
      <title>为什么我的“pd.get_dummies()”返回布尔值而不是 0 和 1？</title>
      <link>https://stackoverflow.com/questions/79001092/why-is-my-pd-get-dummies-returning-booleans-instead-of-0-and-1</link>
      <description><![CDATA[我可以发誓df = pd.get_dummies(df, columns=categorical_cols)用于输出二进制值（0 和 1）。当我将鼠标悬停在 get_dummies 上时，它甚至会显示出来。

为什么我的输出是布尔值（True/False）？
这是我的代码：
import pandas as pd

# 加载数据
script_dir = os.path.dirname(__file__)
data_path = os.path.join(script_dir, &quot;path/to/my/raw/data.csv&quot;)
df = pd.read_csv(data_path)

#示例
feature_cols = [&quot;list&quot;, &quot;of&quot;, &quot;feature&quot;, &quot;cols&quot;]
categorical_cols = [&quot;list&quot;, &quot;of&quot;, &quot;categorical&quot;, &quot;cols&quot;]
target = &quot;target_col&quot;

X = df[feature_cols].copy()
y = df[target]

# 将分类列转换为类别类型

# 应用 get_dummies
X[categorical_cols] = X[categorical_cols].astype(&quot;category&quot;)
X = pd.get_dummies(X, columns=categorical_cols)

预期输出：虚拟变量为二进制形式（0 和 1）。
实际输出：输出包含布尔值（True/False）。
采取的步骤：
我必须通过添加 dtype=int 将其转换为 0 和 1。
`df = pd.get_dummies(df, columns=categorical_cols, dtype=int)`

我的问题：

为什么 pd.get_dummies() 默认为布尔值 (True/False) 而不是 (0/1)？
在 Pandas 的新版本中是否进行了更新，还是我做错了什么？
我应该一直使用 dtype=int 吗？
]]></description>
      <guid>https://stackoverflow.com/questions/79001092/why-is-my-pd-get-dummies-returning-booleans-instead-of-0-and-1</guid>
      <pubDate>Thu, 19 Sep 2024 06:12:09 GMT</pubDate>
    </item>
    <item>
      <title>实值复函数的自动微分</title>
      <link>https://stackoverflow.com/questions/79001014/automatic-differentiation-of-real-valued-complex-function</link>
      <description><![CDATA[我正在研究一个 MLP，其中权重和偏差是复数矩阵，学习率是一个复数，输入和输出是实数矩阵。我对这两个权重使用修改后的 ReLU 和 softmax 函数，对损失使用交叉熵。我正尝试使用 Flux.jl 在 Julia 中实现这一点。我应该怎么做？
我不知道如何初始化复值权重。我能够定义修改后的激活函数。Zygote 文档说应该可以进行复数微分，但我的损失函数是一个实值复数函数，所以我不确定如何将它们结合在一起。我可以明确地写出偏微分。我可以通过将复数矩阵分解为实部和虚部的实数矩阵，在库之外编写自定义训练循环实现，但这不是我的目标。]]></description>
      <guid>https://stackoverflow.com/questions/79001014/automatic-differentiation-of-real-valued-complex-function</guid>
      <pubDate>Thu, 19 Sep 2024 05:37:15 GMT</pubDate>
    </item>
    <item>
      <title>缩放极右偏斜的受抚养者</title>
      <link>https://stackoverflow.com/questions/79000716/scaling-a-super-right-skewed-dependant</link>
      <description><![CDATA[我正在研究在线文章的数据集，并根据文章的分享次数来衡量受欢迎程度。因变量（分享次数）向右严重倾斜，因为中位数是 800，最小值为 0，最大值为 800,000。
我尝试过对数缩放，但没有帮助。
我的问题是：
如果我使用 Z 分数（与平均值的距离）缩放因变量，然后使用 MinMax Scalar，会起作用吗？这应该会导致均匀分布。
我的第二个选择是使用 Z 分数将因变量更改为类，这样我就可以更改进入每个类的值的范围]]></description>
      <guid>https://stackoverflow.com/questions/79000716/scaling-a-super-right-skewed-dependant</guid>
      <pubDate>Thu, 19 Sep 2024 02:59:57 GMT</pubDate>
    </item>
    <item>
      <title>核密度估计，通过交叉验证选择带宽</title>
      <link>https://stackoverflow.com/questions/79000625/kernel-density-estimation-bandwidth-selection-via-cross-validation</link>
      <description><![CDATA[我尝试使用核密度估计来非参数化平滑期权隐含密度。分布的形状高度依赖于带宽。出于研究目的，我想在一系列带宽上使用交叉验证（使用全局带宽时我遇到了虚假振荡问题）
我相信我有一个工作模型，但我注意到网格搜索总是在我的带宽候选数组中选择最大的带宽，这显然是不正确的。我尝试了一系列临时技术，但无济于事。这是我的代码。
使用带宽 #1 绘图
使用带宽 #2 绘图
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KernelDensity
from sklearn.model_selection import GridSearchCV

##选项数据硬编码。
strike_prices = np.array([20000, 25000, 26000, 28000, 30000, 31000, 32000, 33000, 34000, 35000, 36000, 
37000, 38000, 39000, 40000, 41000, 42000, 44000, 45000, 46000, 48000, 50000, 
52000, 54000, 55000, 56000, 58000, 60000, 65000, 70000, 75000, 80000, 85000])

option_prices=np.array([
17703, 12679.6, 11684.7, 9694.98, 7705.26, 6710.4, 5724.03, 4749.47, 
3774.91, 2814.05, 1928.94, 1064.75, 489.491, 188.266, 85.9672, 63.398, 
40.8287, 22.4558, 13.2693, 9.41329, 9.41329, 9.41329, 9.41329, 
9.41329, 9.41329, 9.41329, 9.41329, 9.41329, 9.41329, 9.41329, 9.41329])

##通过对行使价进行两次数值微分获得期权隐含密度
first_derivative=np.gradient(option_prices,strike_prices)
option_pdf=np.gradient(first_derivative,strike_prices)
option_pdf = np.clip(option_pdf, a_min=0, a_max=None) #微小的负值被移位为零。

#绘制初步分布
plt.figure(figsize=(10, 6))
plt.scatter(strike_prices, option_pdf, color=&#39;blue&#39;, s=100, alpha=0.6, edgecolor=&#39;k&#39;, label=&#39;Option PDF&#39;)
plt.xlabel(&quot;执行价格&quot;, fontsize=14)
plt.ylabel(&quot;概率&quot;, fontsize=14)
plt.title(&quot;期权隐含密度&quot;, fontsize=16)

###########################################核密度帮助###############################
###尝试使用核密度估计来平滑此分布，并通过交叉验证选择带宽。

x = strike_prices.reshape(-1,1) # 对于 sklearn，x 值必须是 2D
bandwidths = np.linspace(10, 1000, 250) # 我们探索从 10 到 1000 的带宽。
#bandwidths = np.linspace(10, 5000, 250) # 我们探索从 10 到 5000 的带宽。

# 执行交叉验证以选择最佳带宽
grid = GridSearchCV(KernelDensity(kernel=&quot;gaussian&quot;), {&#39;bandwidth&#39;: broadbands}, cv=5)
grid.fit(x, sample_weight=option_pdf) # 由于我们已经有了分布，我根据每个观察值的发生概率对其进行加权。

best_bandwidth = grid.best_params_[&#39;bandwidth&#39;]
print(f&quot;最佳带宽：{best_bandwidth}&quot;)

kde = KernelDensity(bandwidth=best_bandwidth)
kde.fit(x, sample_weight=option_pdf)

x_fine = np.linspace(strike_prices[0], strike_prices[-1], 1000).reshape(-1,1) # 创建更密集 x 值的 2D 数组

# 在更精细的网格上评估 KDE
log_density = kde.score_samples(x_fine)
smooth_pdf = np.exp(log_density) 

plt.figure(figsize=(10, 6))
plt.plot(x_fine, smooth_pdf, label=&#39;KDE Smoothed PDF&#39;, color=&#39;green&#39;, lw=2)
plt.scatter(strike_prices, option_pdf, color=&#39;blue&#39;, s=100, alpha=0.6, edgecolor=&#39;k&#39;, label=&#39;Raw Data&#39;)

# 在标题中显示最佳带宽（格式化）
plt.title(f&quot;KDE with Best Bandwidth {best_bandwidth:.2f}&quot;, fontsize=16)
plt.xlabel(&quot;Strike Prices&quot;, fontsize=14)
plt.ylabel(&quot;Probability&quot;, fontsize=14)
plt.legend()
]]></description>
      <guid>https://stackoverflow.com/questions/79000625/kernel-density-estimation-bandwidth-selection-via-cross-validation</guid>
      <pubDate>Thu, 19 Sep 2024 01:53:24 GMT</pubDate>
    </item>
    <item>
      <title>CustomScaler 类型的对象未在 gurobi_ml 中注册/支持</title>
      <link>https://stackoverflow.com/questions/79000471/object-of-type-customscaler-is-not-registered-supported-with-gurobi-ml</link>
      <description><![CDATA[我在 sklearn.pipeline.make_pipeline 中构建了一个 CustomScaler 来缩放我的数据。它包含 transform 和 inversetransform 函数。当我使用 gurobipy 优化回归模型时，我得到了上述错误。Gurobi 文档中没有明确提到他们不接受 CustomScaler，但提到他们接受 StandardScalar 的很多地方。我是否需要使用不同的方法来制定我的问题，或者我可以在 gurobipy_ml 中使用 CustomScalar？我想绝对使用 gurobi 框架，因为它具有商业可行性和专业经验。
我采用了最简单的方法，使用 StandardScalar，但用作 CustomeScalar，并得到 NotRegistred：CustomScaler 类型的对象未在 gurobi_ml 中注册/支持。
class CustomScaler(BaseEstimator, TransformerMixin):
def __init__(self):
self.scaler = StandardScaler()

def fit(self, X, y=None):
return self.scaler.fit(X)

def transform(self, X):
return self.scaler.transform(X)

def inverse_transform(self, X):
return self.scaler.inverse_transform(X)
]]></description>
      <guid>https://stackoverflow.com/questions/79000471/object-of-type-customscaler-is-not-registered-supported-with-gurobi-ml</guid>
      <pubDate>Wed, 18 Sep 2024 23:50:16 GMT</pubDate>
    </item>
    <item>
      <title>无法从 Pytorch Dataset 的 __get_item__ 返回布尔变量</title>
      <link>https://stackoverflow.com/questions/79000230/unable-to-return-a-boolean-variable-from-pytorch-datasets-get-item</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79000230/unable-to-return-a-boolean-variable-from-pytorch-datasets-get-item</guid>
      <pubDate>Wed, 18 Sep 2024 21:33:47 GMT</pubDate>
    </item>
    <item>
      <title>Jupyter 笔记本无法通过 Anaconda Navigator 打开，尝试打开 Jupyter 笔记本时出现以下错误，请提供解决方案</title>
      <link>https://stackoverflow.com/questions/78999330/jupyter-notebook-is-not-opening-via-anaconda-navigator-and-following-error-is-th</link>
      <description><![CDATA[[W 2024-09-18 21:59:36.813 ServerApp] 在 jupyter_lsp 中未找到 _jupyter_server_extension_points 函数。相反，找到了 _jupyter_server_extension_paths 函数，目前将使用该函数。此函数名称将在 Jupyter Server 的未来版本中弃用。
[W 2024-09-18 21:59:36.996 ServerApp] 在 notebook_shim 中未找到 _jupyter_server_extension_points 函数。相反，找到了 _jupyter_server_extension_paths 函数，目前将使用该函数。此函数名称将在 Jupyter Server 的未来版本中弃用。
[I 2024-09-18 21:59:38.680 ServerApp] 扩展包 panel.io.jupyter_server_extension 导入耗时 1.6916 秒
[I 2024-09-18 21:59:38.680 ServerApp] jupyter_lsp | 扩展已成功链接。
[I 2024-09-18 21:59:38.696 ServerApp] jupyter_server_terminals | 扩展已成功链接。
[I 2024-09-18 21:59:38.710 ServerApp] jupyterlab | 扩展已成功链接。
[I 2024-09-18 21:59:38.711 ServerApp] notebook | 扩展已成功链接。
[I 2024-09-18 21:59:39.355 ServerApp] notebook_shim | 扩展已成功链接。
[I 2024-09-18 21:59:39.355 ServerApp] panel.io.jupyter_server_extension | 扩展已成功链接。
[I 2024-09-18 21:59:39.428 ServerApp] notebook_shim | 扩展已成功加载。
[I 2024-09-18 21:59:39.443 ServerApp] jupyter_lsp | 扩展已成功加载。
[I 2024-09-18 21:59:39.443 ServerApp] jupyter_server_terminals | 扩展已成功加载。
[I 2024-09-18 21:59:39.457 LabApp] JupyterLab 扩展已从 C:\ProgramData\anaconda3\Lib\site-packages\jupyterlab 加载
[I 2024-09-18 21:59:39.458 LabApp] JupyterLab 应用程序目录为 C:\ProgramData\anaconda3\share\jupyter\lab
[I 2024-09-18 21:59:39.459 LabApp] 扩展管理器为“pypi”。
[I 2024-09-18 21:59:39.461 ServerApp] jupyterlab | 扩展已成功加载。
[I 2024-09-18 21:59:39.470 ServerApp] notebook | 扩展已成功加载。
[I 2024-09-18 21:59:39.470 ServerApp] panel.io.jupyter_server_extension | 扩展已成功加载。
[I 2024-09-18 21:59:39.474 ServerApp] 从本地目录提供笔记本：C:\Users\Avi
[I 2024-09-18 21:59:39.474 ServerApp] Jupyter Server 2.14.1 正在运行：
[I 2024-09-18 21:59:39.474 ServerApp] http://localhost:8888/tree?token=435b978e976c033e2b96cb8ce465e12d2ec469809cf2ab35
[I 2024-09-18 21:59:39.474 ServerApp] http://127.0.0.1:8888/tree?token=435b978e976c033e2b96cb8ce465e12d2ec469809cf2ab35
[I 2024-09-18 21:59:39.474 ServerApp] 使用 Control-C 停止此服务器并关闭所有内核（两次以跳过确认）。
[E 2024-09-18 21:59:39.475 ServerApp] 无法将服务器信息写入 C:\Users\Avi\AppData\Roaming\jupyter\runtime\jpserver-3148.json：PermissionError(13, &#39;权限被拒绝&#39;)
回溯（最近一次调用）：
文件“C:\ProgramData\anaconda3\Scripts\jupyter-notebook-script.py”，第 10 行，位于
sys.exit(main())
^^^^^^
文件“C:\ProgramData\anaconda3\Lib\site-packages\jupyter_server\extension\application.py”，第 623 行，位于 launch_instance
serverapp.start()
文件“C:\ProgramData\anaconda3\Lib\site-packages\jupyter_server\serverapp.py”，第3119，在 start 中
self.start_app()
文件“C:\ProgramData\anaconda3\Lib\site-packages\jupyter_server\serverapp.py”，第 3023 行，在 start_app 中
self.write_browser_open_files()
文件“C:\ProgramData\anaconda3\Lib\site-packages\jupyter_server\serverapp.py”，第 2890 行，在 write_browser_open_files 中
self.write_browser_open_file()
文件“C:\ProgramData\anaconda3\Lib\site-packages\jupyter_server\serverapp.py”，第 2913 行，在 write_browser_open_file 中
使用 open(self.browser_open_file, “w”, encoding=“utf-8”) 作为f:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
PermissionError：[Errno 13] 权限被拒绝：&#39;C:\Users\Avi\AppData\Roaming\jupyter\runtime\jpserver-3148-open.html&#39;
我尝试通过 Anaconda Navigator 打开 Jupyter Notebook，但在打开应用程序时遇到问题，此外，我想让您知道，安装后 Jupyter Notebook 可以打开，但关闭 Windows 后无法打开。]]></description>
      <guid>https://stackoverflow.com/questions/78999330/jupyter-notebook-is-not-opening-via-anaconda-navigator-and-following-error-is-th</guid>
      <pubDate>Wed, 18 Sep 2024 16:41:36 GMT</pubDate>
    </item>
    <item>
      <title>我应该在 XGBoost 中对具有不平衡类别的分区模型进行集合或平均 SHAP 值吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78997484/should-i-ensemble-or-average-shap-values-across-partitioned-models-with-imbalanc</link>
      <description><![CDATA[我使用 XGBoost 构建了一个分类模型，但我的类别高度不平衡。我有大约 2,000 个正类实例和 130,000 个负类实例。我没有对负类进行欠采样以匹配正类的大小，而是决定将 130,000 个实例划分为 65 个子集以避免数据丢失。我还使用 SHAP 值和增益（即特征为其贡献的分支带来的准确度改进）来解释特征重要性。我的目标是根据这两个指标比较特征重要性。
但是，我不确定是否应该在 65 个模型中应用模型集成以获得平均性能，还是独立运行 65 个模型，然后计算平均 SHAP 和增益值。
我看到 StackOverflow 和 GitHub 上的讨论表明可以对模型之间的 SHAP 值进行平均，但前提是样本相同。由于我的分区中的负类不同，因此我不确定在模型之间比较 SHAP 值是否仍然合理。]]></description>
      <guid>https://stackoverflow.com/questions/78997484/should-i-ensemble-or-average-shap-values-across-partitioned-models-with-imbalanc</guid>
      <pubDate>Wed, 18 Sep 2024 09:27:31 GMT</pubDate>
    </item>
    <item>
      <title>损失偏差巨大原因何在？[关闭]</title>
      <link>https://stackoverflow.com/questions/78997392/what-are-the-reasons-for-the-huge-deviation-of-loss</link>
      <description><![CDATA[我的loss
我的loss函数
这是我的loss值，我觉得它看起来不稳定。
请问loss偏差这么大的原因是什么？
可能是loss函数的设计问题？还是数据集的质量问题？
非常感谢您的回答。]]></description>
      <guid>https://stackoverflow.com/questions/78997392/what-are-the-reasons-for-the-huge-deviation-of-loss</guid>
      <pubDate>Wed, 18 Sep 2024 09:05:41 GMT</pubDate>
    </item>
    <item>
      <title>面临 MLC-LLM Android 应用程序构建问题</title>
      <link>https://stackoverflow.com/questions/78996973/facing-build-issue-in-the-mlc-llm-android-app</link>
      <description><![CDATA[我尝试使用以下链接运行 MLC-LLM Android 应用程序，但由于以下屏幕截图中显示的错误，我无法运行该应用程序：
Git 存储库

我已克隆并运行 MLC-AI/MLC-LLC，但它没有运行，我得到了它，帮我解决这个问题。
有人在 android 上运行过这个吗，请帮我解决这个问题。
在 android 目录中，有 3 个文件夹：

mlc4j
MLCChat
MLCEngineExample

我们需要使用哪一个？我该如何使用它？]]></description>
      <guid>https://stackoverflow.com/questions/78996973/facing-build-issue-in-the-mlc-llm-android-app</guid>
      <pubDate>Wed, 18 Sep 2024 07:17:43 GMT</pubDate>
    </item>
    <item>
      <title>ImportError: 导入 o​​nnx_cpp2py_export 时 DLL 加载失败：动态链接库 (DLL) 初始化例程失败</title>
      <link>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</guid>
      <pubDate>Wed, 18 Sep 2024 07:08:40 GMT</pubDate>
    </item>
    <item>
      <title>将safetensors模型格式（LLaVA模型）转换为gguf格式</title>
      <link>https://stackoverflow.com/questions/78763327/convert-safetensors-model-formatllava-model-into-gguf-format</link>
      <description><![CDATA[我想在 ollama 中进行 LLaVA 推理，因此我需要将其转换为 gguf 文件格式。
我的模型具有文件格式 safetensors。（使用 lora 训练）
似乎 ollama 仅支持 llama，但不支持 llava，如下所示，
https://github.com/ollama/ollama/blob/main/docs/import.md
我遵循了 llama.cpp 的说明，并在此处使用了代码 convert_lora_to_gguf.py，
https://github.com/ggerganov/llama.cpp/blob/master/convert_lora_to_gguf.py
但是我收到如下错误：
ERROR:lora-to-gguf:不支持 Model LlavaLlamaForCausalLM

如果我在模型文件的 config.json 中写入 llama 模型并运行以下代码，则会收到另一个错误。
model_instance.gguf_writer.add_string(gguf.Keys.General.TYPE, gguf.GGUFType.ADAPTER)
model_instance.gguf_writer.add_string(gguf.Keys.Adapter.TYPE, &quot;lora&quot;)
model_instance.gguf_writer.add_float32(gguf.Keys.Adapter.LORA_ALPHA, float(alpha))
model_instance.gguf_writer.add_quantization_version(gguf.GGML_QUANT_VERSION)
logger.info(&quot;Exporting model...&quot;)
model_instance.write()
logger.info(f&quot;模型已成功导出至 {model_instance.fname_out}&quot;)

Traceback (most recent call last):
File &quot;C:\Users\jjjy2\OneDrive\Desktop\VLM_FastAPI\ollama\convert_lora_to_gguf.py&quot;, line 373, in &lt;module&gt;
model_instance.gguf_writer.add_string(gguf.Keys.General.FILE_TYPE, gguf.GGUFType.ADAPTER)
AttributeError: module &#39;gguf&#39; has no attribute &#39;GGUFType&#39;

似乎所有代码和 gguf 包都不支持 llava，只支持 llama。我必须将我自己训练的模型转换为 gguf。我无法使用 hugging face 的 gguf llava 模型进行推理。
有没有办法转换它？]]></description>
      <guid>https://stackoverflow.com/questions/78763327/convert-safetensors-model-formatllava-model-into-gguf-format</guid>
      <pubDate>Thu, 18 Jul 2024 08:47:53 GMT</pubDate>
    </item>
    <item>
      <title>文本分割器输出不可 JSON 序列化</title>
      <link>https://stackoverflow.com/questions/76890207/text-splitter-output-is-not-json-serializable</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76890207/text-splitter-output-is-not-json-serializable</guid>
      <pubDate>Sat, 12 Aug 2023 16:34:50 GMT</pubDate>
    </item>
    <item>
      <title>无法更改嵌入维度以将其传递给 gpt2</title>
      <link>https://stackoverflow.com/questions/74996908/cant-change-embedding-dimension-to-pass-it-through-gpt2</link>
      <description><![CDATA[我正在练习图像字幕，在张量的不同维度方面遇到了一些问题。所以我的图像嵌入又名大小 [1, 512]，但我用于字幕生成的 GPT2 需要大小 [n, 768]，其中 n 是字幕开头的标记数。我不知道如何更改图像嵌入的维度以使其通过 GPT2。
我认为用零填充图像嵌入是个好主意，因此大小将是 [1, 768]，但我认为这会对结果字幕产生负面影响。
谢谢你的帮助！
我曾尝试用零填充图像嵌入，使其大小为 [1, 768]，但我认为这不会有太大帮助]]></description>
      <guid>https://stackoverflow.com/questions/74996908/cant-change-embedding-dimension-to-pass-it-through-gpt2</guid>
      <pubDate>Tue, 03 Jan 2023 17:50:56 GMT</pubDate>
    </item>
    <item>
      <title>如何处理一列包含图像名称、另一列包含图像路径的 csv 文件数据集？</title>
      <link>https://stackoverflow.com/questions/72773807/how-to-handle-dataset-which-is-a-csv-file-that-contains-image-names-in-one-colum</link>
      <description><![CDATA[我是 Python 和机器学习的新手。我只是在练习模型训练和数据集。我偶然发现了这个数据集，它有测试和训练文件夹。该文件夹中有几个包含不同图像的文件（这是一个乐器数据集，因此每个乐器都按名称分类在不同的文件夹中）。csv 文件包含乐器的名称及其在文件夹中的路径，如下所示：Instrument.csv
现在我的问题是如何处理这个数据集？我应该遍历训练和测试文件夹还是使用这个 csv 文件？
如果我想选择文件夹选项，那么如何遍历每个子文件夹并访问图像？
这是数据集的链接：https://www.kaggle.com/datasets/gpiosenka/musical-instruments-image-classification
如果问题没有任何意义或太容易回答，我很抱歉。我承认我是菜鸟]]></description>
      <guid>https://stackoverflow.com/questions/72773807/how-to-handle-dataset-which-is-a-csv-file-that-contains-image-names-in-one-colum</guid>
      <pubDate>Mon, 27 Jun 2022 14:30:58 GMT</pubDate>
    </item>
    </channel>
</rss>