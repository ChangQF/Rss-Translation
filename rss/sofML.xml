<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 12 Oct 2024 06:21:31 GMT</lastBuildDate>
    <item>
      <title>混合 TensorFlow LSTM-SVM 预测在测试集上全为零</title>
      <link>https://stackoverflow.com/questions/79080166/hybrid-tensorflow-lstm-svm-predictions-are-all-zeros-on-test-set</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79080166/hybrid-tensorflow-lstm-svm-predictions-are-all-zeros-on-test-set</guid>
      <pubDate>Sat, 12 Oct 2024 04:07:58 GMT</pubDate>
    </item>
    <item>
      <title>如何仅知道输出的总和来训练 NN？</title>
      <link>https://stackoverflow.com/questions/79079993/how-to-train-a-nn-by-only-knowing-the-sum-of-the-outputs</link>
      <description><![CDATA[在我的 ML 项目中，我需要调整某个函数 f，其中在我的数据集中我只知道 f 个评估的总和，例如在这种情况下：
f(a1,b1,c1) + f(a2,b2,c2) + … = S
我只知道总和 S，所以我只能根据总和预测计算损失。我的数据集由 30 到 90 个项的不同总和组成，具有不同的输入组合。
如何在 PyTorch 中正确实现这一点？梯度会“通过总和转移”吗？
我尝试使用常规 MLP 并仅根据预测输出的总和计算损失，但效果不佳，我不知道梯度是否计算正确。]]></description>
      <guid>https://stackoverflow.com/questions/79079993/how-to-train-a-nn-by-only-knowing-the-sum-of-the-outputs</guid>
      <pubDate>Sat, 12 Oct 2024 00:30:49 GMT</pubDate>
    </item>
    <item>
      <title>cuda 中如何处理 bf16 和 float32？[关闭]</title>
      <link>https://stackoverflow.com/questions/79078710/how-are-bf16-and-float32-handled-in-cuda</link>
      <description><![CDATA[不幸的是，下面的注释和代码对我来说毫无意义。grad_thresholds[] 被定义为浮点数组。我认为因为这是一个 .cu 文件，一个 CUDA 文件，所以这些本质上是 C 声明。
但是有一个预处理器指令检查 FP_32；在这种情况下，数组值会再次设置。
注释中还提到了 bf16，但我认为“大脑浮点”毫无用处16.
有人能解释一下这段代码与注释的关系吗？
它来自这个文件：
此外，我在学习 CUDA/机器学习时做了一些笔记。这些笔记位于代码下方。
 // 在 fp32 中一次性比较所有参数的梯度
// 我通过检查每个张量的几个元素的梯度差异来手动设置公差。bf16 看起来还不错，但在这里并不惊人。
// 可能有潜在的错误，或者可能是 bf16。不是 100% 确定。
// 此外，如果代码发生变化，并且其中一些被触发，如果影响不是太大，那么可能没问题，
// 因为我们使用的随机舍入会增加一些非确定性的“胡椒噪声”。
// 在这种情况下，在人工审核后，可以稍微延长容差。
// 此外，不同的 GPU 可能使用不同的矩阵乘法算法，因此
// 实际错误可能与硬件有关。

float grad_thresholds[NUM_PARAMETER_TENSORS] = {5e-1f, 4e-3f, 1e-1f, 3.5e-2f, 2e-2f, 3e-2f, 5e-2f, 5e-2f, 5e-2f, 1.5e-2f, 5e-4f, 8e-3f, 1.5e-3f, 2.5e-3f, 1e-1f, 2e-2f};
#如果已定义 (ENABLE_FP32)
for (int i = 0; i &lt; NUM_PARAMETER_TENSORS; i++) {
grad_thresholds[i] = 1e-6f; // 在 FP32 中我们可以更加精确
}
#endif

注释

CUDA 同时使用 C 和 C++ 语法；C++ 是 C 的超集。CUDA 是 C++ 的超集。
全局阈值被分解为组以进行梯度比较。
更新建议

根据平均值和标准差确定阈值。
为噪声添加 10% 的缓冲区。
重构动态阈值的逻辑
记录任何故障
创建单元测试。测试测试？


类型
BF16 代表大脑浮点 16。BF16 有一个 8 位指数，与 FP32 相同，允许非常大和非常小的数字。它有一个 7 位尾数。
FP32 代表浮点 32。

试图理解此代码的用途。]]></description>
      <guid>https://stackoverflow.com/questions/79078710/how-are-bf16-and-float32-handled-in-cuda</guid>
      <pubDate>Fri, 11 Oct 2024 14:46:17 GMT</pubDate>
    </item>
    <item>
      <title>训练LSTM-Attention时的NaN损失</title>
      <link>https://stackoverflow.com/questions/79078535/nan-loss-when-training-lstm-attention</link>
      <description><![CDATA[在模型训练过程中，loss值突然变成Nan，虽然我修改了很多参数，但还是失败了。
我在训练时检查了错误，错误打印在输出中，而不是输入中，所以我认为错误出在参数或我的模型中。
&gt;&gt;&gt; model_name: atae_lstm
&gt;&gt;&gt; 数据集：comment
&gt;&gt;&gt; 优化器：&lt;class &#39;torch.optim.adam.Adam&#39;&gt;
&gt;&gt;&gt; 初始化器：&lt;function kaiming_normal_ at 0x0000022D0817E0C0&gt;
&gt;&gt;&gt; lr: 1e-08
&gt;&gt;&gt; dropout: 0.1
&gt;&gt;&gt; l2reg：0.01
&gt;&gt;&gt; num_epoch：10
&gt;&gt;&gt; batch_size：8
&gt;&gt;&gt; log_step：10
&gt;&gt;&gt; embed_dim：300
&gt;&gt;&gt; hidden_​​dim：300
&gt;&gt;&gt; bert_dim：768
&gt;&gt;&gt; pretrained_bert_name：bert-base-uncased
&gt;&gt;&gt; max_seq_len：85
&gt;&gt;&gt; polarities_dim：3
&gt;&gt;&gt; hops：3
&gt;&gt;&gt; 耐心：5
&gt;&gt;&gt; 设备：cuda
&gt;&gt;&gt;种子：1234
&gt;&gt;&gt; valset_ratio：0
&gt;&gt;&gt; local_context_focus：cdm
&gt;&gt;&gt; SRD：3
&gt;&gt;&gt; model_class：&lt;class &#39;models.atae_lstm.ATAE_LSTM&#39;&gt;
&gt;&gt;&gt; 数据集文件：{&#39;train&#39;：&#39;./datasets/comment/train.raw&#39;，&#39;test&#39;：&#39;./datasets/comment/test.raw&#39;}
&gt;&gt;&gt; 输入列：[&#39;text_indices&#39;， &#39;aspect_indices&#39;]
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
时期：0
损失：1.4378，acc：0.1375
损失：1.4600，acc：0.1125
损失：1.4595，acc：0.1167
损失：1.4558，acc：0.1187
损失：1.4623，acc：0.1125
损失：1.4695，acc：0.1062
损失：1.4511，acc：0.1232
损失：1.4533，acc：0.1203
损失：1.4610，acc：0.1139
损失：1.4598，acc：0.1150
损失：1.4659，acc：0.1102
**损失：nan，acc：0.1073
损失： nan，acc：0.1077
loss：nan，acc：0.1152
loss：nan，acc：0.1133**
[[ 74 0 0]
[ 90 0 0]
[367 0 0]]
&gt; val_acc：0.1394，val_f1：0.0815
&gt;&gt; saved: state_dict/atae_lstm_comment_val_acc_0.1394

模型输入
text_indices = tokenizer.text_to_sequence(text_left + &quot; &quot; + aspects + &quot; &quot; + text_right)
text_indices = text_indices[:tokenizer.max_seq_len] # 如果长度超过 max_seq_len，则截断
aspect_indices = tokenizer.text_to_sequence(aspect)
aspect_indices = aspects_indices[:tokenizer.max_seq_len]

input_colses = &#39;atae_lstm&#39;: [&#39;text_indices&#39;, &#39;aspect_indices&#39;],

这是 LSTM-ATTENTION 模型
来自layer.attention 导入 Attention、NoQueryAttention
来自 layer.dynamic_rnn 导入 DynamicLSTM
导入 torch
导入 torch.nn 作为 nn
来自 layer.squeeze_embedding 导入 SqueezeEmbedding

class ATAE_LSTM(nn.Module):
def __init__(self, embedding_matrix, opt):
super(ATAE_LSTM, self).__init__()
self.opt = opt
self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
self.squeeze_embedding = SqueezeEmbedding()
self.lstm = DynamicLSTM(opt.embed_dim * 2, opt.hidden_​​dim, num_layers=1, batch_first=True)
self.attention = NoQueryAttention(opt.hidden_​​dim + opt.embed_dim, score_function=&#39;bi_linear&#39;)
self.dense = nn.Linear(opt.hidden_​​dim, opt.polarities_dim)
#self.activation = nn.ReLU() 
self.activation = nn.Tanh() 

def forward(self, input):
text_indices, aspects_indices = 输入[0], 输入[1]
x_len = torch.sum(text_indices != 0, dim=-1)
x_len_max = torch.max(x_len)
aspects_len = torch.sum(aspect_indices != 0, dim=-1).float()

x = self.embed(text_indices)
x = self.squeeze_embedding(x, x_len)
aspects = self.embed(aspect_indices)
aspects_pool = torch.div(torch.sum(aspect, dim=1), aspects_len.unsqueeze(1))
aspects = aspects_pool.unsqueeze(1).expand(-1, x_len_max, -1)
x = torch.cat((aspect, x), dim=-1)

# 确保 x_len 在 CPU 上且类型为 int64
h, (_, _) = self.lstm(x, x_len.cpu().to(torch.int64))
ha = torch.cat((h, aspects), dim=-1)
_, score = self.attention(ha)
output = torch.squeeze(torch.bmm(score, h), dim=1)

output = self.activation(output) 

out = self.dense(output)
return out

这是我更改的参数
 if opt.model_name.lower() == &#39;atae_lstm&#39;:
opt.batch_size = 8
opt.num_epoch = 10
opt.lr = 1e-8
opt.initializer= &#39;kaiming_normal_&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/79078535/nan-loss-when-training-lstm-attention</guid>
      <pubDate>Fri, 11 Oct 2024 13:59:15 GMT</pubDate>
    </item>
    <item>
      <title>网格搜索为带有 LOGO 或 LOO 的 best_score 给出 nan，而不是 k 倍 CV</title>
      <link>https://stackoverflow.com/questions/79076968/grid-search-gives-nan-for-best-score-with-logo-or-loo-not-k-fold-cv</link>
      <description><![CDATA[我有一个使用网格搜索的 nan R2 分数问题。
FDODB=pd.read_excel(&#39;Final Training Set for LOGO.xlsx&#39;)
array = FDODB.values
X = array[:,2:126]
Y = array[:,1]
Compd = array[:,0]

scaler = StandardScaler()
X = scaler.fit_transform(X)

params = {
&#39;max_depth&#39;: [5,7,9],
&#39;learning_rate&#39;: [0.03,0.05,0.07],
&#39;n_estimators&#39;: [200,300,400],
&#39;min_child_weight&#39;: [5,7,9],
&#39;subsample&#39;: [0.3, 0.5, 0.7],
&#39;base_score&#39;: [0.4, 0.5, 0.6]
}

xgb_reg = XGBRegressor(tree_method=&#39;hist&#39;, device=&#39;cuda&#39;)
logo = LeaveOneGroupOut()

grid_search = GridSearchCV(
estimator=xgb_reg,
param_grid=params,
scoring=&#39;r2&#39;,
error_score=&quot;raise&quot;,
cv=logo,
verbose=2,
n_jobs=-1,
return_train_score=True
)

grid_search.fit(X, Y, groups=Compd)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(&#39;最佳超参数：&#39;, best_params)
print(&#39;最佳 R2 分数：&#39;, best_score)

输出：
c:\\Anaconda\\envs\\machinelearning\\Lib\\site-packages\\sklearn\\model_selection_search.py​​:1102：UserWarning：一个或多个测试分数是非有限的：\[nan\]
warnings.warn(
最佳超参数：{&#39;base_score&#39;: 0.5, &#39;learning_rate&#39;: 0.03, &#39;max_depth&#39;: 8, &#39;min_child_weight&#39;: 6, &#39;n_estimators&#39;: 200, &#39;subsample&#39;: 0.3}
最佳 R2 分数：nan

当我使用 10 倍 CV 进行网格搜索时，我很难获得 R2 分数。
但每当我尝试 LOO 或 LOGO 时CV，所有 R2 分数都只是 nan。我检查了我的训练集是否有 nan 值，但结果很好。
这可能的原因是什么以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/79076968/grid-search-gives-nan-for-best-score-with-logo-or-loo-not-k-fold-cv</guid>
      <pubDate>Fri, 11 Oct 2024 06:01:41 GMT</pubDate>
    </item>
    <item>
      <title>将 TensorFlow 权重和模型转换为 PyTorch（修改后的 EffectiveNet）</title>
      <link>https://stackoverflow.com/questions/79076436/converting-tensorflow-weights-and-model-to-pytorch-modified-efficientnet</link>
      <description><![CDATA[我正尝试在 pytorch 中模拟一个经过修改的 efficientnet TF 模型。我在 pytorch 中对模型进行了架构更改，转储了 TF 模型权重，然后将其重新加载到新的 pytorch 模型中。使用以下代码在 TF 中转储权重：
model = tf.saved_model.load(model_path)
ws = []
for i in range(len(model.variables)):
ws.append((i, model.variables[i].name, model.variables[i].numpy()))

with open(&quot;manually_dumped_contentnet_weights.pkl&quot;, &quot;wb&quot;) as ofile:
pickle.dump(ws, ofile)

pytorch 中的权重形状似乎与架构和导入的权重相匹配（在 conv2d 和深度 conv2d 之间进行转换之后）。我可以毫无错误地运行模型。但输出结果与 TF 模型的输出结果大不相同。
我注意到在 TF 代码中，模型不是直接加载的，而是在 tf Session 中加载的：
with Session(graph=Graph(), config=ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:
saved_model.loader.load(sess, [saved_model.tag_constants.SERVING], model_path)
patch_feature, patch_label = sess.run(output_nodes,feed_dict={input_node: patch})

现在我想知道我最初转储模型权重的尝试是否做得不正确。或者如果我遗漏了其他内容。
我在加载数据时进行的转置是 conv2d 的 (3,2,0,1) 和深度 conv2d 的 (2,3,0,1)：
def reload_conv2d(layer, weights):
### weights 是一个元组，其中每个元素都由一个三元组组成：(1) 索引号，(2) TF 中权重转储的层的名称，以及 (3) 权重
count = 0
if (
&quot;/conv2d/kernel&quot; not in weights[0][1]
and &quot;/conv2d_1/kernel&quot; not in weights[0][1]
and &quot;depthwise_conv2d/depthwise_kernel&quot; not in weights[0][1]
and &quot;final_conv2d/final_conv2d&quot; 不在 weights[0][1] 中 :
raise ValueError(
f&quot;需要在第一个索引上有 conv2d/kernel，但得到了 {weights[0][1]}&quot;
)
transpose_shape = (2,3,0,1) if &quot;depthwise&quot;在 weights[0][1] 中否则（3、2、0、1）
transposed_weights = torch.from_numpy（weights[0][2].transpose（transpose_shape[0]、transpose_shape[1]、transpose_shape[2]、transpose_shape[3]））
layer.weight.data = transposed_weights
count += 1
如果 layer.bias 不是 None 或 layer.bias:
如果（
&quot;/conv2d/bias&quot; 不在 weights[1][1] 中
并且 &quot;/conv2d_1/bias&quot; 不在 weights[1][1] 中
）：
引发 ValueError（
f&quot;需要在第二个索引上有 conv2d/bias 但得到了 {weights[1][1]}&quot;
)
layer.bias.data = (
torch.from_numpy(weights[1][2])
如果type(weights[1][2]) == np.ndarray
else torch.from_numpy(weights[1][2])
)
count += 1
return layer, count

为什么 pytorch 和 TF 模型对相同输入给出完全不同的结果？是因为权重倾倒，还是权重加载……或者是模型架构变化？输入 TF 权重（在模型更改和转置之后）加载正常，我可以毫无问题地运行模型，但这对于调试它没有任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/79076436/converting-tensorflow-weights-and-model-to-pytorch-modified-efficientnet</guid>
      <pubDate>Thu, 10 Oct 2024 23:37:12 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练 yolo 自定义模型时，data.yaml 文件中的相对路径出现问题</title>
      <link>https://stackoverflow.com/questions/79075311/issue-with-relative-paths-in-data-yaml-file-when-trying-to-train-yolo-custom-mod</link>
      <description><![CDATA[我正在尝试创建一个训练管道，以使用用户输入的带标签图像来训练自定义 yolov9 模型。
我遇到了一个问题，如果我使用相对路径来设置 data.yaml 文件，就会出现错误：
 RuntimeError：数据集“OIT_model/customOIT/customdatasetyolo/data.yaml”错误
数据集“OIT_model/customOIT/customdatasetyolo/data.yaml”图像未找到，缺少路径“C:\GitHub\Anomaly_detection_combine\OIT_model\Anomaly_detection_combine\OIT_model\customOIT\customdatasetyolo\Anomaly_detection_combine\OIT_model\customOIT\customdatasetyolo\val”

更奇怪的是，错误路径提及，
&#39;C:\\GitHub\\Anomaly_detection_combine\\OIT_model\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\val&#39;

不是存在或正在任何地方请求的路径。实际路径是
&#39;C:\\GitHub\\Anomaly_detection_combine\\OIT_model\\customOIT\\customdatasetyolo\\val&#39;

由于某种原因，它重复了路径的第一部分 3 次。
这是 data.yaml 文件：
 path: OIT_model/customOIT/customdatasetyolo
train: OIT_model/customOIT/customdatasetyolo/train
val: OIT_model/customOIT/customdatasetyolo/val
nc: 1
names: [&#39;5&#39;]

这是开始训练的代码：

def train_custom_dataset_yolo(data_path, epochs=100, imgsz=64, verbose=True):
model = YOLO(&quot;OIT_model/yolov9c.pt&quot;)
# 指定训练运行的保存目录
save_dir = &#39;OIT_model/customOIT/yolocustomtrainoutput&#39;
if os.path.exists(save_dir):
for file in os.listdir(save_dir):
file_path = os.path.join(save_dir, file)
if os.path.isfile(file_path) or os.path.islink(file_path):
os.unlink(file_path)
elif os.path.isdir(file_path):
shutter.rmtree(file_path)
os.makedirs(save_dir, exist_ok=True)
model.train(data=data_path, epochs=epochs, imgsz=imgsz, verbose=verbose, save_dir=save_dir)
return
train_custom_dataset_yolo(&#39;OIT_model/customOIT/customdatasetyolo/data.yaml&#39;, epochs=1,imgsz=64, verbose=True)

然而，非常奇怪的是，当我用绝对路径替换相对路径时，如下所示：
 path: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo
train: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo/train
val: C:/GitHub/fix/Anomaly_detection_combine/OIT_model/customOIT/customdatasetyolo/val
nc: 1
names: [&#39;5&#39;]

训练没有问题。对于我来说，使用绝对路径不是一个选择，因为这个应用程序需要在其他机器上可重现。]]></description>
      <guid>https://stackoverflow.com/questions/79075311/issue-with-relative-paths-in-data-yaml-file-when-trying-to-train-yolo-custom-mod</guid>
      <pubDate>Thu, 10 Oct 2024 16:13:09 GMT</pubDate>
    </item>
    <item>
      <title>卷积-反卷积，同时保持原始图像大小</title>
      <link>https://stackoverflow.com/questions/79074519/convolution-deconvolution-while-maintaining-the-original-image-size</link>
      <description><![CDATA[我正在尝试在 tensorflow 中实现 pspnet。
它需要一个池化模块来接收输入，以及几个内核大小：

使用每个内核AveragePooling2D对输入进行平均池化

进行 1x1 卷积，之后使用 UpSampling2D


最后将所有不同的 conv-deconv 输出连接在一起并前馈
def pyramid_pooling_module(x, pool_sizes):
pool_outputs = []
for pool_size in pool_sizes:
pooled=layers.AveragePooling2D(pool_size)(x)
pooled=layers.Conv2D(512, (1,1), padding=&#39;same&#39;)(pooled)
pooled=layers.UpSampling2D(size=pool_size, interpolation=&#39;bilinear&#39;)(pooled)
print(pool_size)
pool_outputs.append(pooled)
return layer.Concatenate()(pool_outputs)

输入的维度为 68, 120
因此，所使用的内核 (1x1, 2x2, 3x3, 6x6) 会与 3x3, 6x6 的平均池化层产生舍入误差
因此这些层的最终池化输出为 (66, 120)
我不确定如何修复此问题，是否应该将输入调整为可以被 6x6 整除的大小？还有其他方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79074519/convolution-deconvolution-while-maintaining-the-original-image-size</guid>
      <pubDate>Thu, 10 Oct 2024 13:00:23 GMT</pubDate>
    </item>
    <item>
      <title>如何使用深度学习来解决由合成数据组成的拼图游戏？[关闭]</title>
      <link>https://stackoverflow.com/questions/79072271/how-can-i-use-deep-learning-to-solve-a-jigsaw-puzzle-composed-of-synthetic-data</link>
      <description><![CDATA[我花了一些时间研究 Python 中的拼图生成器，该生成器接收图像、行数 (M) 和列数 (N)，并将原始图像分解为 M*N 个 png 图像块输出到文件夹中。这些图像是正方形，带有不规则形状的制表符和空格，因此每个块只能放在一个位置。
接下来，我想创建一个拼图解算器来接收这些 PNG 图像，提取关键特征并确定它们的位置。
这里是图像的示例。如果您对它的实现方式感到好奇，也可以查看生成器代码。
我遇到过制作拼图解算器的不同方法，但我对使用深度学习感兴趣。我的主要问题是我不知道从哪里开始。截至目前，这些碎片没有任何旋转，但我希望将来能够解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79072271/how-can-i-use-deep-learning-to-solve-a-jigsaw-puzzle-composed-of-synthetic-data</guid>
      <pubDate>Wed, 09 Oct 2024 22:50:24 GMT</pubDate>
    </item>
    <item>
      <title>如何从 PDF 研究论文中准确提取标题、标题和副标题？</title>
      <link>https://stackoverflow.com/questions/79050147/how-to-accurately-extract-title-headings-and-subheadings-from-pdf-research-pap</link>
      <description><![CDATA[我试图从 PDF 格式的研究论文中提取标题、标题和副标题。我尝试了各种方法，但都无法获得准确的结果。以下是我采取的步骤：
1. 尝试使用 PyMuPDF (fitz)
我使用 PyMuPDF (fitz) 从 PDF 中提取文本。虽然我能够获取文本，但问题是格式丢失了（例如，标题和副标题不容易区分）。文档的其他部分（如引文和脚注）也存在额外的噪音。
2. 提示语言模型
我还尝试使用提示语言模型 (LLM) 来分析提取的文本。我使用 Ollama 进行离线处理，但结果不够准确。当我尝试 OpenAI 的 GPT 和 Gemini 时，它们提供了准确的输出，但我想要一个可以离线工作的解决方案。
我尝试过的：

PyMuPDF (fitz)
Ollama (llama3.1、gemma)
OpenAI GPT 和 Gemini 可实现准确提取，但它们需要在线使用。
PyPDF2 和类似的库，但它们也会返回非结构化文本。

我需要的：

从 PDF 研究论文中准确提取标题、标题和副标题。
离线解决方案。
来自引用、页码等额外内容的噪音最小。

是否有可靠的离线方法或我可以采取的一些额外步骤至：

识别并准确提取标题、标题和副标题。
尽量减少输出中的噪音和不相关内容。
]]></description>
      <guid>https://stackoverflow.com/questions/79050147/how-to-accurately-extract-title-headings-and-subheadings-from-pdf-research-pap</guid>
      <pubDate>Thu, 03 Oct 2024 10:37:10 GMT</pubDate>
    </item>
    <item>
      <title>使用 transformer 和 tensorflow 时内核崩溃</title>
      <link>https://stackoverflow.com/questions/79042488/kernel-crashing-while-using-transformer-and-tensorflow</link>
      <description><![CDATA[我正在研究一个开源数据集，并决定使用一个文本摘要库，为此我安装了 transformers。后来我发现有一些必备库我应该安装，因此通过浏览 stackoverflow 中的一些论坛，我在 anaconda prompt 中使用 pip 命令安装了以下软件包，并提到了版本：

transformers --&gt; 4.45.1
tensorflow --&gt; 2.17.0
numpy --&gt; 1.26.4
keras --&gt; 3.5.0

我的代码中的导入库命令如下所示：
import pandas as pd
from transformers import pipeline
import os

os.environ[&#39;TF_ENABLE_ONEDNN_OPTS&#39;] = &#39;0&#39;
import tensorflow as tf

当我尝试运行以下代码时，我收到错误消息，如下所示
未提供默认为 google-t5/t-5-small 和修订版 df1b051 的模型（https://huggingface.co/google-t5/t5-small。不建议在生产中使用未指定模型名称和修订版的管道

然后内核崩溃并显示此错误消息
由于不同计算顺序的浮点舍入，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量&#39;TF_ENABLE_ONEDNN_OPTS&#39; = &#39;0&#39;** 如屏幕截图所示 --\&gt; [未提供模型错误](https://i.sstatic.net/p5L7e0fg.png)。[内核崩溃错误](https://i.sstatic.net/KPgtb84G.png)

#loading summarization pipeline

summarizer = pipeline(&quot;summarization&quot;)

text = &quot;Capcom 的最新挑战者来了！《街头霸王™ 6》将于 2023 年 6 月 2 日在全球推出，代表了《街头霸王™》系列的下一个发展！《街头霸王 6》涵盖三种不同的游戏模式，包括世界巡回赛、格斗场和战斗中心。&quot;

#summarize text

summary = summaryr(text , max_length = 30 , min_length = 10 , do_sample = False)

print(summary)

我使用下面的代码来解决环境变量问题，但即便如此，我仍然收到此错误消息：
os.environ[&#39;TF_ENABLE_ONEDNN_OPTS&#39;] = &#39;0&#39;
import tensorflow as tf

问题是我收到了两种错误 - 一种表示没有可用于管道的模型，另一种与环境变量有关。
如何解决它们？]]></description>
      <guid>https://stackoverflow.com/questions/79042488/kernel-crashing-while-using-transformer-and-tensorflow</guid>
      <pubDate>Tue, 01 Oct 2024 09:27:45 GMT</pubDate>
    </item>
    <item>
      <title>我如何在第二次运行中重试 Optuna 中的失败试验？</title>
      <link>https://stackoverflow.com/questions/77599820/how-can-i-retry-fail-trials-in-optuna-in-a-second-run</link>
      <description><![CDATA[我正在使用 Optuna 进行网格搜索，但第二次运行中不会重复失败试验。相反，已经完成的试验会毫无意义地重复。
在这里我分别描述这两个问题：

当试验失败（例如缺乏计算资源）时，第二次启动网格搜索（Python 文件）时不会重复。这可以使用以下自包含代码进行测试，其中我通过启动异常来模拟问题。注释这些行并再次运行，以查看 x=2 和 y=2 的组合是否没有重复。

import time
import optuna
from optuna.storages import RetryFailedTrialCallback
import numpy as np

def objective(trial):
# 获取值
params = {
&#39;x&#39;: trial.suggest_categorical(&#39;x&#39;, [0, 1, 2, 3]),
&#39;y&#39;: trial.suggest_categorical(&#39;y&#39;, [0, 1, 2, 3])
}
# 打印
print(&#39;Testing with x=&#39; + str(params[&#39;x&#39;]), &#39;y=&#39; + str(params[&#39;y&#39;]))

########################################
# 在第一部分之后注释此部分运行 #
###########################################
如果 params[&#39;x&#39;] == 2 且 params[&#39;y&#39;] == 2:
引发 ValueError(&quot;x==2, y==2&quot;)
########################################

# 返回
返回 params[&#39;x&#39;] ** 2 - params[&#39;y&#39;]

def optuna_search_space():
# 定义搜索空间
返回 {
&#39;x&#39;: range(3),
&#39;y&#39;: range(3),
}

def optuna_grid():
# 定义 URL
URL = &#39;mysql://&lt;USER&gt;:&lt;PASSWORD&gt;@&lt;IP&gt;:&lt;PORT&gt;&#39;
# 获取搜索空间
search_space = optuna_search_space()
# 定义存储
storage = optuna.storages.RDBStorage(
url=f&quot;{URL}/prove_optuna&quot;,
failed_trial_callback=RetryFailedTrialCallback(max_retry=3),
)
# 定义研究
study = optuna.load_study(
study_name=&quot;test1&quot;,
sampler = optuna.samplers.GridSampler(search_space),
storage = storage,
)
# 运行
study.optimize(objective)
# 打印
print(study.best_trial)

if __name__ == &quot;__main__&quot;:
# 运行
optuna_grid()


当我重新运行代码时，它会重复已经执行过的一次（或更多次）试验。我不想这样，因为这是计算资源的损失。

在 Optuna 仪表板上，可以看到，经过多次重新运行组合 (x=2, y=2) 后，它再也不会重复（即使第一次失败），并且组合 (x=0, y=1) 已经测试了几次（无用）。

我该如何解决这些问题？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/77599820/how-can-i-retry-fail-trials-in-optuna-in-a-second-run</guid>
      <pubDate>Mon, 04 Dec 2023 13:32:29 GMT</pubDate>
    </item>
    <item>
      <title>如何调整随机森林模型中的特征权重？</title>
      <link>https://stackoverflow.com/questions/67417292/how-to-adjust-feature-weights-in-random-forest-model</link>
      <description><![CDATA[我正在使用 Scikit-Learn 的随机森林库，我想知道是否可以更改特征权重，以便特定特征产生更大的影响。我查看了随机森林文档，但我只看到我不感兴趣的类别的权重变化。
除了重写代码本身之外，还有其他方法可以做到这一点吗？任何建议都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/67417292/how-to-adjust-feature-weights-in-random-forest-model</guid>
      <pubDate>Thu, 06 May 2021 11:33:58 GMT</pubDate>
    </item>
    <item>
      <title>添加我自己的密集层后，vgg16模型的可训练参数发生了变化</title>
      <link>https://stackoverflow.com/questions/65651051/trainable-parameters-of-vgg16-model-get-changed-after-adding-my-own-dense-layer</link>
      <description><![CDATA[vgg16_model = tf.keras.applications.vgg16.VGG16()

model= Sequential()

for layer in vgg16_model.layers[:-1]:

model.add(layer)

model.summary() #最后一个密集层到现在为止已被移除 


for layer in model.layers:

layer.trainable=False #对于迁移学习，我已冻结了这些层

model.add(Dense(2,activation=&#39;softmax&#39;))

model.summary() #现在当我添加密集层时，模型的可训练参数会发生变化

]]></description>
      <guid>https://stackoverflow.com/questions/65651051/trainable-parameters-of-vgg16-model-get-changed-after-adding-my-own-dense-layer</guid>
      <pubDate>Sun, 10 Jan 2021 07:39:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 迁移学习的边界框回归准确率为 0%。具有 Sigmoid 激活的输出层仅输出 0 或 1</title>
      <link>https://stackoverflow.com/questions/65459399/bounding-box-regression-using-keras-transfer-learning-gives-0-accuracy-the-out</link>
      <description><![CDATA[我正在尝试创建一个对象定位模型来检测汽车图像中的车牌。我使用了 VGG16 模型并排除了顶层以添加我自己的密集层，最后一层有 4 个节点和 S 形激活以获得 (xmin、ymin、xmax、ymax)。
我使用 keras 提供的函数读取图像，并将其调整为 (224, 244, 3)，还使用 ​​preprocess_input() 函数来处理输入。我还尝试通过使用填充调整大小来手动处理图像以保持比例，并通过除以 255 对输入进行规范化。
当我训练时，似乎什么都不起作用。我的训练和测试准确率为 0%。下面是我为该模型编写的代码。
def get_custom(output_size, optimizer, loss):

vgg = VGG16(weights=&quot;imagenet&quot;, include_top=False, input_tensor=Input(shape=IMG_DIMS))

vgg.trainable = False

flatten = vgg.output
flatten = Flatten()(flatten)

bboxHead = Dense(128,activation=&quot;relu&quot;)(flatten)
bboxHead = Dense(32,activation=&quot;relu&quot;)(bboxHead)

bboxHead = Dense(output_size,activation=&quot;sigmoid&quot;)(bboxHead)

model = Model(inputs=vgg.input,outputs=bboxHead)
model.compile(loss=loss,optimizer=optimizer,metrics=[&#39;accuracy&#39;])

return模型

X 和 y 分别为 (616, 224, 224, 3) 和 (616, 4)。我将坐标除以相应边的长度，因此 y 中的每个值都在 (0,1) 范围内。
我将在下面链接我的 github 中的 python 笔记本，以便您可以看到完整的代码。我正在使用 google colab 来训练模型。
https://github.com/gauthamramesh3110/image_processing_scripts/blob/main/License_Plate_Detection.ipynb]]></description>
      <guid>https://stackoverflow.com/questions/65459399/bounding-box-regression-using-keras-transfer-learning-gives-0-accuracy-the-out</guid>
      <pubDate>Sat, 26 Dec 2020 18:24:41 GMT</pubDate>
    </item>
    </channel>
</rss>