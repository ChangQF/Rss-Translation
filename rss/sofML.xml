<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 11 Jun 2024 06:21:23 GMT</lastBuildDate>
    <item>
      <title>如何训练模型根据部分输入预测完整的产品名称？</title>
      <link>https://stackoverflow.com/questions/78605611/how-to-train-a-model-to-predict-full-product-names-from-partial-input</link>
      <description><![CDATA[我正在开展一个机器学习项目，需要根据给定的部分产品名称预测完整的产品名称。例如：
完整产品名称：“Women Viscose Rayon Kurta Pant Dupatta Set”
部分名称：“Women Viscose Rayon Kur”。
我有一个庞大的数据集来训练模型。最初，我尝试使用余弦相似度，但结果并不令人满意。我正在寻找一种更好的方法，最好是通过微调现有的开源模型。
要求：

模型类型：可以根据部分输入生成或完成文本的模型。
数据集：足够大以训练强大的模型。
开源：最好使用开源工具和库。

我目前的方法：

余弦相似度：尝试使用余弦相似度进行匹配，但发现它不足以生成全名。
探索语言模型：考虑微调预先训练的语言模型，但需要有关最佳方法和实践的指导。
]]></description>
      <guid>https://stackoverflow.com/questions/78605611/how-to-train-a-model-to-predict-full-product-names-from-partial-input</guid>
      <pubDate>Tue, 11 Jun 2024 05:54:00 GMT</pubDate>
    </item>
    <item>
      <title>国际象棋中有效和无效走法的分类</title>
      <link>https://stackoverflow.com/questions/78605587/classification-of-valid-and-invalids-moves-in-chess</link>
      <description><![CDATA[我已经开始学习二元分类的基本机器学习概念。我创建了一个问题陈述，用于识别国际象棋中特定白棋的有效/无效走法。我创建了自己的数据集，如下所示。我使用了编号而不是 a1、a2 格式

white_pawn_moves.csv
--------------------
current_position, next_position, valid
12, 17, 0
13, 19, 0
16, 20, 0
12, 28, 1
4, 13, 0

来自 ml.net cli 的命令：
-----------
mlnet 分类 --dataset &quot;white_pawn_moves.csv&quot; --label-col 2 --has-header true --train-time 10

我在数据集中添加了 100 行，即不同的组合。如果我们考虑 8x8 棋盘，可能的组合将是 64*64 = 4096。当我尝试使用分类技术运行时，它似乎没有给出正确的预测。我有以下问题

对于这类问题，多少数据即行足够？因为
我们知道最大可能的组合不能超过 4096，而其他一些
问题可能有 100 万，识别数据集的好经验法则是什么？它是基于特征数量吗？
一般来说，当我知道输入的范围和输出的范围时，我应该如何知道多少数据是足够的？假设特征 1 可以从 1 到 1000
而特征 2 可以从 2 到 20？我们是否可以限制或规定特征输入范围
不能超过或低于某些值？是不是应该永远训练模型？

有时甚至 12,17 的预测也为 1，尽管样本数据将该记录标记为 0。链接为 https://learn.microsoft.com/en-us/dotnet/machine-learning/automate-training-with-cli]]></description>
      <guid>https://stackoverflow.com/questions/78605587/classification-of-valid-and-invalids-moves-in-chess</guid>
      <pubDate>Tue, 11 Jun 2024 05:44:14 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 PyTorch 和 TensorFlow 实现 ResNet-50 和 SSD300 模型</title>
      <link>https://stackoverflow.com/questions/78605564/how-to-implement-resnet-50-and-ssd300-models-using-pytorch-and-tensorflow</link>
      <description><![CDATA[我正在尝试使用 PyTorch 框架实现 ResNet-50 和 SSD300 模型以执行对象检测任务。我遇到了一个问题，我的 SSD300 模型仅关注图像的中心并预测所有图像的相同边界框。我需要帮助来识别和解决此问题。
1-我在 PyTorch 框架中将 resnet50 作为主干与 SSD300 模型一起实现以执行对象检测任务。
2-我遵循了输入图像的推荐预处理步骤，包括调整大小和规范化。
3-我使用了预先训练的模型，并期望它们能够很好地推广到提供的输入图像
4-我使用了数据增强技术。
我期望 SSD300 模型能够检测图像不同部分中的对象，并根据每幅图像的内容返回不同的边界框。具体来说，我预计该模型不会只关注图像的中心，而是会对图像中各个位置的物体提供准确的预测。
无论实际内容如何，​​SSD300 模型始终会预测位于图像中间的边界框。所有输入图像的边界框都相同，这表明模型的学习或推理过程可能存在问题。]]></description>
      <guid>https://stackoverflow.com/questions/78605564/how-to-implement-resnet-50-and-ssd300-models-using-pytorch-and-tensorflow</guid>
      <pubDate>Tue, 11 Jun 2024 05:35:00 GMT</pubDate>
    </item>
    <item>
      <title>如何按照物体在最顶层的顺序检测和识别它们，然后对它们进行分层并为它们分配 ID？</title>
      <link>https://stackoverflow.com/questions/78605533/how-to-detect-and-identify-objects-in-the-order-that-they-are-on-top-then-layer</link>
      <description><![CDATA[在此处输入图片描述
我尝试过过滤掉它们的线条，现在如何确定哪个物体被隐藏了
我也尝试过使用 yoloV8 过滤物体，但我仍然无法确定哪个物体被另一个物体遮挡了，有人能帮我吗？
CODE:
import cv2
import numpy as np

# 加载图像 + 蒙版、灰度、高斯模糊、Otsu 阈值
image = cv2.imread(&quot;./anhtest/11.png&quot;) # 这是原始图像
original = image.copy()
mask = cv2.imread(&quot;./anhtest/11.png&quot;) # 这是从生成的蒙版U-2-Net
gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
bg_removed = cv2.bitwise_and(image, image, mask=thresh)

# HSV 颜色阈值
hsv = cv2.cvtColor(bg_removed, cv2.COLOR_BGR2HSV)
lower = np.array([0, 0, 0])
upper = np.array([179, 33, 255])
hsv_mask = cv2.inRange(hsv, lower, upper)
isolated = cv2.bitwise_and(bg_removed, bg_removed, mask=hsv_mask)
isolated = cv2.cvtColor(isolated, cv2.COLOR_BGR2GRAY)
isolated = cv2.threshold(isolated, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# 变形操作以去除小伪影和噪音
open_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))
opening = cv2.morphologyEx(isolated, cv2.MORPH_OPEN, open_kernel, iterations=1)
close_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))
close = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, close_kernel, iterations=1)

# 查找轮廓并按最大轮廓面积排序
cnts = cv2.findContours(close, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
cnts = cnts[0] if len(cnts) == 2 else cnts[1]
cnts = sorted(cnts, key=cv2.contourArea, reverse=True)
for c in cnts:
cv2.drawContours(original, [c], -1, (36,255,12), 3)
break

cv2.imshow(&quot;bg_removed&quot;, bg_removed)
cv2.imshow(&quot;hsv_mask&quot;, hsv_mask)
cv2.imshow(&#39;isolated&#39;,隔离)
cv2.imshow(&#39;original&#39;, original)
cv2.waitKey()
]]></description>
      <guid>https://stackoverflow.com/questions/78605533/how-to-detect-and-identify-objects-in-the-order-that-they-are-on-top-then-layer</guid>
      <pubDate>Tue, 11 Jun 2024 05:23:24 GMT</pubDate>
    </item>
    <item>
      <title>机器学习算法的分类</title>
      <link>https://stackoverflow.com/questions/78605421/classification-on-machine-learning-algorithms</link>
      <description><![CDATA[过去三个月我一直在研究机器学习，并遇到了这样的说法“机器学习模型可以分为生成式或描述式、概率式与非概率式以及参数式与非参数式。”
我尝试根据我的理解对所有模型进行分类，如表所示。
表格
有人可以确认我是否正确完成了此分类吗？我理解内核是非参数的，但其余两个类别呢？]]></description>
      <guid>https://stackoverflow.com/questions/78605421/classification-on-machine-learning-algorithms</guid>
      <pubDate>Tue, 11 Jun 2024 04:38:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyArmNN 在 Raspberry Pi 上进行 ML 推理失败</title>
      <link>https://stackoverflow.com/questions/78604851/ml-inference-on-raspberry-pi-with-pyarmnn-failure</link>
      <description><![CDATA[我尝试按照此文档 (https://developer.arm.com/documentation/102107/0000/Before-you-begin) 在 Raspberry PI 4 中构建 Arm NN。
在步骤
scons extra_cxx_flags=&quot;-fPIC&quot; benchmark_tests=0 validation_tests=0 neon=1 ，
它失败并出现错误。我已附上错误图片。如果有人能提供有关此错误的任何见解，我将非常高兴。 
注意；我无法从此链接 https://dl.bintray.com/boostorg/release/1.64.0/source/boost_1_64_0.tar.bz2 安装 boost，因为它不再存在。相反，我使用了这个：https://www.boost.org/users/history/version_1_64_0.html]]></description>
      <guid>https://stackoverflow.com/questions/78604851/ml-inference-on-raspberry-pi-with-pyarmnn-failure</guid>
      <pubDate>Mon, 10 Jun 2024 23:33:11 GMT</pubDate>
    </item>
    <item>
      <title>Predict.boosting 问题：newmfinal 必须是 1<newmfinal<mfinal</title>
      <link>https://stackoverflow.com/questions/78604530/predict-boosting-problem-newmfinal-must-be-1newmfinalmfinal</link>
      <description><![CDATA[我尝试使用 adabag 包的 predict.boosting() 函数通过 Adaboost 算法进行预测，但出现错误：

&quot;error in predict.boosting(adaboost, train01_new, newmfinal = 9) :
newmfinal must be 1&lt;newmfinal&lt;mfinal&quot;

以下是脚本：
install.packages(&quot;adabag&quot;)
library(&quot;adabag&quot;)
adaboost &lt;- boosting.cv(factor_new ~ RFS +LI+SDI+LDI+DR+DBT+FCT+FII+DITP+ADCG+ADDG+ROA+ROI+ROS+ROE,data = train01_new, boos = TRUE，mfinal = 10，v=5，par=TRUE，control = rpart.control(cp=.001))
predict_adaboost_cv_train &lt;- predict.boosting(adaboost, train01_new, newmfinal = 9)

我按照错误提示使用了 newmfinal=9，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/78604530/predict-boosting-problem-newmfinal-must-be-1newmfinalmfinal</guid>
      <pubDate>Mon, 10 Jun 2024 21:09:02 GMT</pubDate>
    </item>
    <item>
      <title>从 Azure 机器学习工作室笔记本直接将数据写入 Blob 存储</title>
      <link>https://stackoverflow.com/questions/78603383/write-data-directly-to-blob-storage-from-an-azure-machine-learning-studio-notebo</link>
      <description><![CDATA[我正在 Azure 机器学习笔记本中进行一些交互式开发，我想将一些数据直接从 pandas DataFrame 保存到我默认连接的 blob 存储帐户中的 csv 文件中。我目前正在按以下方式加载一些数据：
import pandas as pd

uri = f&quot;azureml://subscriptions/&lt;sub_id&gt;/resourcegroups/&lt;res_grp&gt;/workspaces/&lt;workspace&gt;/datastores/&lt;datastore_name&gt;/paths/&lt;path_on_datastore&gt;&quot;
df = pd.read_csv(uri)

加载这些数据没有问题，但经过一些基本转换后，我想将这些数据保存到我的存储帐户中。我发现的大多数（如果不是全部）解决方案都建议将此文件保存到本地目录，然后将此保存的文件上传到我的存储帐户。我发现的最佳解决方案是以下解决方案，它使用 tmpfile，因此我不必事后删除任何“本地”文件：
from azureml.core import Workspace
import tempfile

ws = Workspace.from_config()
datastore = ws.datastores.get(&quot;exampleblobstore&quot;)

with tempfile.TemporaryDirectory() as tmpdir:
tmpath = f&quot;{tmpdir}/example_file.csv&quot;
df.to_csv(tmpath)
datastore.upload_files([tmpath], target_path=&quot;path/to/target.csv&quot;, overwrite=True)

这是一个合理的解决方案，但我想知道是否有任何方法可以直接写入我的存储帐户，而无需先保存文件。理想情况下，我想做一些简单的事情：
target_uri = f&quot;azureml://subscriptions/&lt;sub_id&gt;/resourcegroups/&lt;res_grp&gt;/workspaces/&lt;workspace&gt;/datastores/&lt;datastore_name&gt;/paths/&lt;path_on_datastore&gt;&quot;
df.to_csv(target_uri)

读了一些资料后，我认为 AzureMachineLearningFileSystem 类可能允许我读取和写入数据到我的数据存储，方式类似于我在本地机器上开发时的方式，但是，这个类似乎不允许我写入数据，只能检查“文件系统”并从中读取数据。]]></description>
      <guid>https://stackoverflow.com/questions/78603383/write-data-directly-to-blob-storage-from-an-azure-machine-learning-studio-notebo</guid>
      <pubDate>Mon, 10 Jun 2024 16:05:39 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用哪种 ML 模型进行销售预测？</title>
      <link>https://stackoverflow.com/questions/78602784/which-ml-model-should-i-use-for-sales-prediction</link>
      <description><![CDATA[给定披萨店的数据进行训练，其中包含历史销售数据在此处输入图片说明。



X 形状
Y 形状




(1582, 13)
(1582,)



我正在使用顺序 NN，其中有 6 个密集层，带有“relu”，1 个输出层，带有“线性”激活函数 (128,64,32,16,8,4,1)。
我尝试添加 l2 正则化，但它仍然无法正常工作。
Epoch 999/1000
20/20 [==============================] - 0s 6ms/step - 损失：40284.5391 - mae：163.4370 - val_loss：96699.7188 - val_mae：260.4841
Epoch 1000/1000
20/20 [==============================] - 0s 19ms/step - 损失：40112.4141 - mae：163.2846 - val_loss：98381.9688 - val_mae： ]]></description>
      <guid>https://stackoverflow.com/questions/78602784/which-ml-model-should-i-use-for-sales-prediction</guid>
      <pubDate>Mon, 10 Jun 2024 14:05:42 GMT</pubDate>
    </item>
    <item>
      <title>如何比较两个多维张量</title>
      <link>https://stackoverflow.com/questions/78602074/how-to-compare-two-multi-dimension-tensors</link>
      <description><![CDATA[我有以下张量
import torch as t
a = t.tensor([[[[1.0, 2.0], [3.0, 2.0]],[[1.0, 2.0], [2.0, 3.0]],[[1.0, 2.0], [2.0, 3.0]]], [[[1.0, 2.0], [3.0, 2.0]],[[1.0, 2.0], [2.0, 3.0]],[[1.0, 2.0], [2.0, 3.0]],[[1.0, 2.0], [3.0, 2.0]],[[1.0, 2.0], [2.0, 3.0]],[[1.0, 2.0], [2.0, 3.0]],[[1.0, 2.0], [2.0, 3.0]]]])

b = t.tensor([[[[1.0, 2.0], [3.0, 2.0]],[[1.0, 2.0], [2.0, 3.0]],[[1.0, 2.0], [2.0, 3.0]]], [[[1.0, 2.0], [3.0, 2.0]],[[1.0, 2.0], [2.0, 2.0]],[[1.0, 2.0], [2.0, 2.0]],[[1.0, 2.0], [2.0, 3.0]]],[[[1.0, 1.0], [3.0, 2.0]],[[1.0, 2.0], [2.0, 3.0]]],[[1.0, 2.0], [2.0, 3.0]]])

答案 = t.all(a.eq(b)).sum()

print(ans)

预期值为 1，因为第一个 (3,2,2) 的所有值都相等。但它总是返回零。]]></description>
      <guid>https://stackoverflow.com/questions/78602074/how-to-compare-two-multi-dimension-tensors</guid>
      <pubDate>Mon, 10 Jun 2024 11:41:30 GMT</pubDate>
    </item>
    <item>
      <title>在预处理 CT 扫描图像系列数据以训练 CNN 模型以获得更好的准确性时，我应该如何具体地关注我感兴趣的区域？</title>
      <link>https://stackoverflow.com/questions/78601522/how-specific-should-i-be-with-my-region-of-interest-while-preprocessing-a-ct-sca</link>
      <description><![CDATA[我正在尝试训练一个 3D CNN 模型，以在一个数据集上对癌症分期进行分类，该数据集由头部到颈部的 CT 图像系列组成，分为 5 个类别，与癌症的分期相对应。每个阶段都有与每位患者相对应的文件夹，每个文件夹包含 120 帧 CT 图像系列。我想将一个图像立方体输入到模型中，以考虑空间分辨率和深度分辨率，并将图像立方体归类为五个类别之一。
在 120 张图像中，大约有 10 帧存在癌症。
我将整个图像集（每个患者 120 张）作为 3D 图像立方体传入 3D 卷积模型，对数据进行归一化处理，其结构如下所示）：
num_classes = 5
model = Sequential([
tf.keras.Input(shape=(255, 255, 120, 1)),
# 卷积层 1
Conv3D(16, (3, 3, 3),activation=&#39;relu&#39;),
BatchNormalization(),

MaxPooling3D((2, 2, 1), strides=(2, 2, 1), padding=&quot;same&quot;),

# 卷积层 2
Conv3D(32, (3, 3, 3),activation=&#39;relu&#39;),
BatchNormalization(),
MaxPooling3D((2, 2, 2), strides=(2, 2, 2), padding=&quot;same&quot;),

# 卷积层 3
Conv3D(32, (3, 3, 3), activity=&#39;relu&#39;),
BatchNormalization(),
MaxPooling3D((2, 2, 2), strides=(2, 2, 2),padding=&quot;same&quot;),

# 卷积层 4
Conv3D(64, (3, 3, 3), activity=&#39;relu&#39;),
BatchNormalization(),
MaxPooling3D((2, 2, 2), strides=(2, 2, 2),padding=&quot;same&quot;),

# 卷积层 5
Conv3D(128, (3, 3, 3), activity=&#39;relu&#39;),
BatchNormalization(),

# 卷积层 6
Conv3D(128, (3, 3, 3),activation=&#39;relu&#39;),
BatchNormalization(),
MaxPooling3D((2, 2, 2), strides=(2, 2, 2),padding=&quot;same&quot;),
#Dropout(0.25),
# 扁平层
Flatten(),

# 密集层 1
Dense(256,activation=&#39;relu&#39;, kernel_initializer = &#39;glorot_uniform&#39;, kernel_regularizer=tf.keras.regularizers.L2(0.01)),
BatchNormalization(),
#Dropout(0.35),
# 密集层 2
Dense(128,activation=&#39;relu&#39;, kernel_initializer = &#39;glorot_uniform&#39;, kernel_regularizer=tf.keras.regularizers.L2(0.01)),
BatchNormalization(),
#Dropout(0.25),
# 输出层
Dense(num_classes,activation=&#39;softmax&#39;)
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.000001),loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

这是我将 DICOM 图像加载到输入 (X) 和输出 (Y) 标签中的方式：
def load_dicom_images(folder_path):
images = []
for file in sorted(os.listdir(folder_path)):
ds = pydicom.dcmread(os.path.join(folder_path,file))
# 转换为灰度图像并调整大小为 255x255
image = ds.pixel_array
image = cv2.resize(image, (255, 255))
# 标准化图像
normalized_images= (image.astype(np.float32)-image.mean())/image.std()
images.append(normalized_images)
# 将列表转换为 numpy 数组
images = np.array(images)
return images

def load_data(stage_folder): #Stage 文件夹包含与类别相关的五个文件夹
X = []
y = [] 
# 将阶段映射到标签
stage_to_label = {&#39;Stage I&#39;: 0, &#39;Stage II&#39;: 1, &#39;Stage III&#39;: 2, &#39;Stage IVA&#39;: 3, &#39;Stage IVB&#39;: 4 }

for stage in os.listdir(stage_folder):
stage_path = os.path.join(stage_folder, stage)
label = stage_to_label[stage]
forp​​atient_id in os.listdir(stage_path):
patient_folder = os.path.join(stage_path,patient_id)
selected_images = load_dicom_images(patient_folder)

X.append(selected_images)
y.append(label)

X = np.array(X)
y = np.array(y)

# 将 y 转换为分类（独热编码）
y = to_categorical(y, num_classes=5)

return X, y

history = model.fit(X_train, y_train, batch_size=1, epochs=50, validation_data=(X_test, y_test), verbose = True,回调=回调)

这会导致训练准确率 (21%) 和验证准确率较低，并且验证损失会随着每个时期而增加。我根据 CNN 的输入重塑了数据。
我是否需要进一步处理我的数据并仅包含癌变帧，过滤掉其余帧，还是应该包含整个数据以保留深度分辨率并寻找不同的方法来提高准确率？]]></description>
      <guid>https://stackoverflow.com/questions/78601522/how-specific-should-i-be-with-my-region-of-interest-while-preprocessing-a-ct-sca</guid>
      <pubDate>Mon, 10 Jun 2024 09:39:03 GMT</pubDate>
    </item>
    <item>
      <title>如何提高多标签分类的准确度得分？</title>
      <link>https://stackoverflow.com/questions/78598665/how-to-improve-accuracy-score-in-multilabel-classification</link>
      <description><![CDATA[我想知道如何在多标签分类问题中提高准确率并降低损失。
如果你查看 sklearn 参考，你会发现在多类和多输出算法中提到了多标签，我现在正在测试它。
（https://scikit-learn.org/stable/modules/multiclass.html）
样本数据使用sklearn.datasets中的make_multilabel_classification有10个特征，通过修改n_classes创建一个数据集。
当multilabel有两个类时，似乎准确率和损失都比较令人满意。
from numpy import mean
from numpy import std
from sklearn.datasets import make_multilabel_classification
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score, hamming_loss

# define dataset
X, y = make_multilabel_classification(n_samples=10000, n_features=10, n_classes=2, random_state=1)

# 总结数据集形状
print(X.shape, y.shape)
# 总结前几个示例
for i in range(10):
print(X[i], y[i])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
print(scaler.mean_)
print(scaler.var_)

x_train_std = scaler.transform(X_train)
x_test_std = scaler.transform(X_test)

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train_std, y_train)

pred = knn.predict(x_test_std)

print(accuracy_score(y_test, pred))
print(hamming_loss(y_test, pred))

accuracy_score: 0.8345, hamming_loss: 0.08875
但是，随着类别数超过3，准确率得分逐渐下降，损失增加。
# define dataset
X, y = make_multilabel_classification(n_samples=10000, n_features=10, n_classes=3, random_state=1)

n_classes= 3 --&gt; accuracy_score: 0.772, hamming_loss: 0.116
n_classes= 4 --&gt; accuracy_score: 0.4875, hamming_loss: 0.194125
使用 RandomForestClassifier 算法和 MLPClassifier 算法时也会出现类似情况，如参考文献所示，或者使用 ClassifierChain(estimator=SVC) 使用不支持 Multilabel 分类的算法时也会出现类似情况。
我想知道为什么会出现这种情况，如何调整超参数来提高准确率？]]></description>
      <guid>https://stackoverflow.com/questions/78598665/how-to-improve-accuracy-score-in-multilabel-classification</guid>
      <pubDate>Sun, 09 Jun 2024 13:03:31 GMT</pubDate>
    </item>
    <item>
      <title>为神经网络 MATLAB 实现岭回归方程</title>
      <link>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network-matlab</link>
      <description><![CDATA[我试图在 MATLAB 中复制以下方程，以使用岭回归训练找到神经网络的最佳输出权重矩阵。
使用岭回归训练后的神经网络输出权重矩阵：

我的尝试如下。请注意，y_i 是一个 T x 1 向量，而 y_i_target 也是一个 T x 1 向量。 Wout_i 是一个 N x 1 向量，其中 N 是神经网络中的节点数。我为每个 i^th 目标训练信号生成三个 Ny x 1 向量 Wout_i,y_i,y_i_target，其中 Ny 是训练信号的数量。为了简单起见，我没有在我的可重现示例中计算 Wout_i,y_i,y_i_target。
Ny = 1000; % 训练信号数量
T = 100; % 每个训练信号的时间长度
reg = 10^-4; % 岭回归系数
outer_sum = 0;
for i = 1:Ny
Wouts{i} = Wout_i; % 为每个第 i 个目标训练信号收集每个 Wout_i 的单元矩阵
inner_sum = sum(((y_i-y_i_target).^2)+reg*norm(Wout_i)^2);
outer_sum(i) = inner_sum;
end
outer_sum = outer_sum.*(1/Ny);
[minval, minidx] = min(outer_sum);
Wout = cell2mat(Wouts(minidx));

我对 Wout 的最终答案是 N 乘以 1，正如它应该的那样，但我对我的答案不确定。我特别不确定我是否正确地完成了 Wout 运算的双重求和和 arg min。有什么方法可以验证我的答案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network-matlab</guid>
      <pubDate>Sat, 08 Jun 2024 22:31:47 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 Elixir Nx/Schorar 中进行 ELISA 分析？</title>
      <link>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</link>
      <description><![CDATA[我已阅读 Medium 上的文章 ELISA Analysis in Python。
上述文章使用 SciPy 的 curve_fit 函数根据 4 参数逻辑回归 (4PL) 模型找到近似曲线，如下所示：
from scipy.optimize import curve_fit

x = [1.95, 3.91, 7.381, 15.63, 31.25, 62.5, 125,250, 500, 1000]
y = [0.274, 0.347, 0.392, 0.420, 0.586, 1.115, 1.637, 2.227, 2.335, 2.372]

def log4pl(x, A, B, C, D):
return(((A - D) / (1.0 + ((x / C) ** B))) + D)

params, _ = curve_fit(log4pl, x, y)
A, B, C, D = params[0], params[1], params[2], params[3]

我想使用 Nx/Scholar 库。
可能吗？如果您能给我任何提示，我将不胜感激。

[更新]
快速浏览一下 Python scipy.optimize 源代码，似乎 curve_fit 在内部使用了 Fortran 的 MINPACK 库。
据我所知，没有简单的方法可以从 Elixir 使用 MINPACK。
因此，我得出结论，目前在 Elixir 中进行 ELISA 分析很困难。
欢迎提供任何其他信息。]]></description>
      <guid>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</guid>
      <pubDate>Sun, 02 Jun 2024 04:29:18 GMT</pubDate>
    </item>
    <item>
      <title>部署失败：此部署的自动回滚已禁用</title>
      <link>https://stackoverflow.com/questions/75362415/failed-to-deploy-automatic-rollback-disabled-for-this-deployment</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/75362415/failed-to-deploy-automatic-rollback-disabled-for-this-deployment</guid>
      <pubDate>Mon, 06 Feb 2023 14:10:27 GMT</pubDate>
    </item>
    </channel>
</rss>