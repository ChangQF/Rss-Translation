<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 13 May 2024 09:15:20 GMT</lastBuildDate>
    <item>
      <title>微调 EasyOcr 需要很长时间</title>
      <link>https://stackoverflow.com/questions/78471072/fine-tuning-easyocr-is-taking-very-long-timr</link>
      <description><![CDATA[嗨，我正在尝试微调 Easyocr，并尝试在 RTX 3050 12GB 上进行少量迭代 3K、批量大小 128 和 24K 图像。但是，训练需要很长时间。以下是我正在使用的配置文件.
手册种子：1111
工人：4
批量大小：128 #32
数量：3000
验证间隔：200
英国《金融时报》：错误
optim: False # 默认为 Adadelta
左：1。
贝塔1：0.9
ρ：0.95
每股收益：0.00000001
毕业剪辑：5
＃数据处理
select_data: &#39;e&#39; # 这是train_data中的数据集文件夹
批次比例：&#39;1&#39;
总数据使用率：1.0
批量最大长度：35
图像高度: 64
图片尺寸：600
RGB：假
对比度调整：假
敏感：真实
垫：真实
对比度调整：0.0
data_filtering_off：假
# 模型架构
转变：“无”
特征提取：“ResNet”
序列建模：“BiLSTM”
预测：“CTC”
基准数：20
输入通道：1
输出通道：512
隐藏大小：512
解码：“贪婪”
新预测：错误
freeze_FeatureFxtraction：假
freeze_SequenceModeling：假
]]></description>
      <guid>https://stackoverflow.com/questions/78471072/fine-tuning-easyocr-is-taking-very-long-timr</guid>
      <pubDate>Mon, 13 May 2024 08:49:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 模糊视频中的文本</title>
      <link>https://stackoverflow.com/questions/78470906/blur-text-in-video-using-python</link>
      <description><![CDATA[模糊视频中的文本
我想对高分辨率视频中的所有文本应用模糊效果，包括移动文本。
我已成功使用 Python Deface 库来模糊视频中的脸部，但它似乎不适用于模糊文本。我还尝试使用 OpenCV 进行文本检测和模糊，但它不能准确识别文本。我的目标是模糊视频中存在的所有文本。]]></description>
      <guid>https://stackoverflow.com/questions/78470906/blur-text-in-video-using-python</guid>
      <pubDate>Mon, 13 May 2024 08:15:52 GMT</pubDate>
    </item>
    <item>
      <title>在 Python ML 中，我的 RMSE 和 MAE 始终计算为 0 尽管步骤正确，但我不确定问题出在哪里 对于下一步该做什么有什么建议吗？</title>
      <link>https://stackoverflow.com/questions/78470290/in-python-ml-both-my-rmse-mae-are-consistently-calculated-as-0-despite-correct</link>
      <description><![CDATA[# 这是我的代码

X = store1.drop([&#39;商店&#39;,&#39;日期&#39;,&#39;Holiday_Flag&#39;,&#39;天数&#39;,&#39;温度&#39;], axis=1)
y = store1[&#39;Weekly_Sales&#39;]

# 缩放预测数据
从 sklearn.preprocessing 导入 StandardScaler
sc = 标准缩放器()
X_sc = sc.fit_transform(X)

从 sklearn.model_selection 导入 train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_sc, y, test_size=0.2, random_state=21)

从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.metrics 导入mean_absolute_error，mean_squared_error
lin_reg = 线性回归()
lin_reg.fit(X_train, y_train)
y_pred = lin_reg.predict(X_test)

print(&quot;MAE: {}&quot; .format(mean_absolute_error(y_test, y_pred)))
print(&quot;RMSE: {}&quot; .format(mean_squared_error(y_test, y_pred)))

笔记本图片(https://i.sstatic.net/Z48nX8dm.png ）
这似乎是正确的，但如此高的准确性确实是我寻求知识渊博的指导的一个问题。]]></description>
      <guid>https://stackoverflow.com/questions/78470290/in-python-ml-both-my-rmse-mae-are-consistently-calculated-as-0-despite-correct</guid>
      <pubDate>Mon, 13 May 2024 05:55:23 GMT</pubDate>
    </item>
    <item>
      <title>我想在输入问题时预测标签</title>
      <link>https://stackoverflow.com/questions/78469857/i-want-to-predict-tags-when-input-the-questions</link>
      <description><![CDATA[`问题=“facetgrid数据标签seaborn”
标签 = “python pandas seaborn”
我在小数据集中使用 MLPClassifier，但实际数据形状是 262529。为此使用哪种算法。我提供我的代码
从 sklearn.neural_network 导入 MLPClassifier
来自 sklearn.feature_extraction.text
导入CountVectorizer
来自 sklearn.model_selection
导入train_test_split
来自 sklearn.metrics
导入准确度_分数
df_half = df.iloc[:int(len(df)*0.1)]q = df_half[&#39;Question&#39;]t = [tags.split() for df_half[&#39;Tags&#39;]]向量化器中的标签= countvectorizer（）x_vectorized1 = vectorizer.fit_transform（q）label_binarizer = multiLabelBinarizer（）y_binarized1 = label_binarizer.fit_transform（t） 2，Random_State = 42）分类器= mlpClassifier (hidden_​​layer_sizes=(100,), max_iter=100, alpha=0.0001,solver=&#39;adam&#39;, verbose=10, random_state=42, tol=0.0001)classifier.fit(X_train1, y_train1)y_pred1 = classifier.predict(X_train1) 
准确度=准确度_得分（y_train1，y_pred1）
print(“准确度：”, 准确度)
精度：0.9964`
我想预测问题​​明智的标签以及算法使用的大数据集中的问题。我正在尝试
TF-IDF
纳维贝叶斯
线性支持向量机
随机森林（RF）`]]></description>
      <guid>https://stackoverflow.com/questions/78469857/i-want-to-predict-tags-when-input-the-questions</guid>
      <pubDate>Mon, 13 May 2024 02:42:40 GMT</pubDate>
    </item>
    <item>
      <title>我如何进一步推进这个 AI/ML 项目？</title>
      <link>https://stackoverflow.com/questions/78469835/how-can-i-proceed-further-in-this-ai-ml-project</link>
      <description><![CDATA[我有 10 个数据集 (.csv)，每个数据集有 100,000 行，每行包含 5 个输入（-4.0f 到 +4.0f）和一个输出列 (0/1)。我想使用它来训练神经网络并预测给定的测试数据集（也有 100,000 行，但没有填充输出列）。
我想创建一个 5--(reLU)--&gt; 32 --(reLU)--&gt; 32 --（乙状结肠） --&gt; 1 个神经网络并用这样的奖励系统对其进行训练 [if (expec.op ==0)reward=1- o/pfromNN; if (expec.op ==1) 奖励= o/pfromNN].
如何使用此调整 NN 的权重或如何进一步进行？我是 NN 的新手。
我想过像体育馆的月球着陆器模块一样这样做，但由于这里没有涉及任何州，我很困惑]]></description>
      <guid>https://stackoverflow.com/questions/78469835/how-can-i-proceed-further-in-this-ai-ml-project</guid>
      <pubDate>Mon, 13 May 2024 02:25:15 GMT</pubDate>
    </item>
    <item>
      <title>如何将 standardscaler() 用于具有多列的单行的 Predict() 函数？</title>
      <link>https://stackoverflow.com/questions/78469729/how-to-use-standardscaler-for-the-predict-function-for-a-single-row-having-m</link>
      <description><![CDATA[很抱歉，如果我的描述含糊不清。
我正在尝试建立一个房价预测系统。数据有异常值并且是非高斯的，对于目标特征 y，使用对数变换。在进行一项热编码之前，我已经使用 StandardScaler() 来适合我的模型。代码如下所示：
numerical_features = df4[[&#39;bhk&#39;, &#39;面积&#39;, &#39;price_lakhs&#39;, &#39;price_per_sqft&#39;]]
categorical_features = df4.select_dtypes(include=[&#39;object&#39;])

定标器=标准定标器()
Standardized_features = scaler.fit_transform(numerical_features)

std_df4 = pd.DataFrame(standardized_features, columns=numerical_features.columns)
std_df4.head()

现在为了预测新值，我使用了这个 Predict_price() 函数。我很难理解如何像上面的代码块一样做到这一点。我将数值和分类值分开。我不能在下面做同样的事情。这段代码工作错误，我认为列 x[3:] 中的一个热编码值也可能已被缩放，这不是上层代码的工作方式。我使用的任何回归模型[下面代码中的 clf.predict()] 对于下面的 Predict() 输入的不同值给出相同的答案。
def Predict_price（bhk，面积，price_per_sqft，类型，区域）：
    
    house_type_loc_index = np.where(X.columns == &#39;type_&#39; + type)[0][0]
    打印（房屋类型位置索引）
    
    Region_loc_index = np.where(X.columns == &#39;region_&#39; + 区域)[0][0]
    打印（region_loc_index）

    x = np.zeros(len(X.columns))
    x[0] = bhk
    x[1] = 面积
    x[2] = 每平方英尺价格
    
    如果 house_type_loc_index &gt;= 0：
        x[房屋类型位置索引] = 1
        
    如果region_loc_index &gt;= 0：
        x[区域位置索引] = 1
    
    列 = X.列
    x = x.reshape(1, -len(列))

    定标器=标准定标器()
    标准化特征 = 缩放器.fit_transform(x)
    数据 = pd.DataFrame(standardized_features, columns = columns)
    
    打印（数据）
    
    ans = clf.predict(数据)[0]
    返回exp(ans)

我期望模型能够根据我给预测函数的输入来预测值。预测函数的调用如下。
predict_price(bhk = 2，面积 = 2000，price_per_sqft = 35，类型 = &#39;公寓&#39;，区域 = &#39;Airoli&#39;)

我得到的答案是：42.6103853222858455
predict_price(bhk = 3，面积 = 600，price_per_sqft = 70，类型 = &#39;别墅&#39;，区域 = &#39;Vashi&#39;)

我得到的答案是：42.6103853222858455
我进一步检查，对于上面的 Predict_price() 行，它收到的每个值都是 0。这就是为什么我强烈认为我在 Predict() 中错误地使用了 standardScaler()
bhk面积价格_每尺户型_公寓户型_独立屋\
0 0.0 0.0 0.0 0.0 0.0

   类型_顶层公寓类型_单间公寓类型_别墅区_阿格里帕达\
0 0.0 0.0 0.0 0.0

   地区_艾罗利 ... 地区_瓦赛 地区_瓦希 地区_维赫罗利 \
0 0.0 ... 0.0 0.0 0.0

   地区_Ville Parle East 地区_Ville Parle West 地区_Virar \
0 0.0 0.0 0.0

   地区_Virar West 地区_Wadala 地区_Worli 地区_other
0 0.0 0.0 0.0 0.0
]]></description>
      <guid>https://stackoverflow.com/questions/78469729/how-to-use-standardscaler-for-the-predict-function-for-a-single-row-having-m</guid>
      <pubDate>Mon, 13 May 2024 01:25:36 GMT</pubDate>
    </item>
    <item>
      <title>随机森林机器学习</title>
      <link>https://stackoverflow.com/questions/78469722/random-forest-machine-learning</link>
      <description><![CDATA[我做了一个模型预测，准确率达到 80%。您可以在此处.
现在，我想要进行案例实现，所以我制作随机数据帧，其中包含 100 行数据，其与训练数据具有完全相同的特征，但是当我运行它时，它会抛出这样的错误...
`ValueError：调用输入形状与训练期间提供的输入形状不同的模型：训练模型时输入单个数组 Tensor(“inputs:0”, shape=(None, 27), dtype=float32)在{&#39;SeniorCitizen&#39;：，&#39;合作伙伴&#39;：，&#39;家属&#39;：，&#39;PhoneService&#39;：、&#39;MultipleLines&#39;: 、&#39;OnlineSecurity&#39;: 、&#39;OnlineBackup&#39;: 、&#39;DeviceProtection&#39;: 、&#39;TechSupport&#39;: 、&#39;StreamingTV&#39;: 、&#39;StreamingMovies&#39;: 、&#39;无纸化账单&#39;: &lt; ;Semantic.NUMERICAL: 1&gt;, &#39;gender_Female&#39;: , &#39;gender_Male&#39;: , &#39;InternetService_DSL&#39;: , &#39;InternetService_Fiber_optic&#39; : 、&#39;InternetService_No&#39;: 、&#39;Contract_Month-to-month&#39;: 、&#39;Contract_One_year&#39;: , &#39;Contract_Two_year&#39;: , &#39;PaymentMethod_Bank_transfer_(automatic)&#39;: , &#39;PaymentMethod_Credit_card_(automatic)&#39;: , &#39;PaymentMethod_Electronic_check &#39;: , &#39;PaymentMethod_Mailed_check&#39;: , &#39;tenure&#39;: , &#39;MonthlyCharges&#39;: , “TotalCharges”：&lt;语义.数值：1&gt;}。
调用层“random_forest_model”接收的参数（类型 RandomForestModel）：
  输入=tf.Tensor（形状=（无，27），dtype=float32）
  • 训练=False`

我很确定train和随机df的列数是相同的，列的名称是相同的，dtype是相同的。我不知道该怎么办了，请帮助我。
您可以在此处查看所有错误消息
我用来制作随机 100 行数据框的代码此处
解决问题]]></description>
      <guid>https://stackoverflow.com/questions/78469722/random-forest-machine-learning</guid>
      <pubDate>Mon, 13 May 2024 01:17:18 GMT</pubDate>
    </item>
    <item>
      <title>更改层数据类型后的 Keras分段_模型 nan 损失</title>
      <link>https://stackoverflow.com/questions/78469320/keras-segmentation-model-nan-loss-after-changing-layer-dtype</link>
      <description><![CDATA[我正在尝试解决二进制分割问题，我需要从背景中对树进行分类。我的数据预处理函数非常简单，如下所示：
def get_data(a, 路径, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=3):
  输出 = np.zeros((len(a), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)
  对于 i，enumerate(a) 中的 image_id：
    图像路径 = 路径 + 图像 ID
    图像 = np.array(Image.open(path_image))
    图像=调整大小（图像，（IMG_HEIGHT，IMG_WIDTH），模式=&#39;常量&#39;，保留_范围=真）
    如果 image.shape[-1] == IMG_WIDTH:
      图像 = np.expand_dims(图像, 轴=-1)
    输出[i] = 图像
  返回

图像采用 .tif 格式。
X_train = np.zeros((len(img_list), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)
Y_train = np.zeros((len(mask_list), IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)
X_test = np.zeros((len(img_test), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)

X_train = get_data(img_list, “/content/train/images/”)
Y_train = get_data(mask_list, “/content/train/gt/”, IMG_CHANNELS=1)
X_test = get_data(img_test, “/content/public_test/images/”)

数据形状：
X_train.shape、Y_train.shape、X_test.shape

((2828, 256, 256, 3), (2828, 256, 256, 1), (707, 256, 256, 3))

起初，我的所有数据都是 uint8 类型。但是，我在调用 model.fit(...) 时收到此错误：
类型错误：“Mul”运算的输入“y”的类型为 uint8，与参数“x”的 float32 类型不匹配。

但是当我将数据更改为 np.float32 时，我的 Google Colab 会话在训练时崩溃了。我想知道是否可以将模型数据类型从 float32 更改为占用更少空间的内容，例如 float16 并检查它是否有帮助。我尝试这样做：
a = model.get_config()
对于 a[&#39;layers&#39;] 中的层：
  层[&#39;config&#39;][&#39;dtype&#39;]=&#39;float16&#39;

模型 = model.from_config(a)
model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;)

当我尝试训练模型时，输出如下（RAM 持续缓慢增长）：
纪元 1/100
160/160 [================================] - ETA：0秒 - 损失：nan
Epoch 1：val_loss 没有从 inf 改善
160/160 [================================] - 73s 180ms/步 - 损失：nan - val_loss：nan
纪元 2/100
160/160 [================================] - ETA：0秒 - 损失：nan
Epoch 2：val_loss 没有从 inf 改善
160/160 [================================] - 19s 121ms/步 - 损失：nan - val_loss：nan
纪元 3/100
 45/160 [=======&gt;.................................] - 预计到达时间：13 秒 - 损失：nan

所以我的问题是：为什么损失总是 nan 以及解决该问题的其他方法是什么？
提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78469320/keras-segmentation-model-nan-loss-after-changing-layer-dtype</guid>
      <pubDate>Sun, 12 May 2024 21:16:48 GMT</pubDate>
    </item>
    <item>
      <title>准确的时间序列异常检测</title>
      <link>https://stackoverflow.com/questions/78469052/accurate-time-series-anomaly-detection</link>
      <description><![CDATA[尝试以最高精度对时间序列数据执行异常检测。您最近遇到的任何框架或 python 库。或者请告诉我您遇到过或使用过的最好的图书馆。
问候，
K
尝试过像 PyCaret 这样的库，但错误率很高。]]></description>
      <guid>https://stackoverflow.com/questions/78469052/accurate-time-series-anomaly-detection</guid>
      <pubDate>Sun, 12 May 2024 19:18:40 GMT</pubDate>
    </item>
    <item>
      <title>K-Means：如何解决错误：scatter() 得到参数“c”的多个值</title>
      <link>https://stackoverflow.com/questions/78468674/k-means-how-to-solve-error-scatter-got-multiple-values-for-argument-c</link>
      <description><![CDATA[我是机器学习新手，我有一项任务要求我执行无监督学习，因此我决定使用 K-Means。
我使用Python来编码。我已将数据（我的数据来自 csv 文件）导入到 Google Colab 中。我的数据有 7 个特征，我需要绘制簇，但出现错误：scatter() 获得了参数“c”的多个值。
这是我的代码：
这部分是我决定 k 值的方法。我使用肘部法。
%matplotlib 内联
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns； sns.set()
将 numpy 导入为 np
将 pandas 导入为 pd

从 sklearn.cluster 导入 KMeans

数据=“/内容/信息.csv”
df = pd.read_csv（数据，标题=0）
data = list(zip(x_train[“日期”], x_train[“a”], x_train[“b”], x_train[“c”], x_train[“d”], x_train[“” e&quot;], x_train[&quot;f&quot;]))
打印（数据）

惯性 = []

对于范围 (1,40) 内的 i：
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(数据)
    惯性.append(kmeans.inertia_)

plt.plot（范围（1,40），惯性，标记=&#39;o&#39;）
plt.title(&#39;弯头法&#39;)
plt.xlabel(&#39;簇数&#39;)
plt.ylabel(&#39;惯性&#39;)
plt.show()

这就是出错的地方：
kmeans = KMeans(n_clusters=5)
kmeans.fit(数据)
plt.scatter(x_train[“日期”], x_train[“a”], x_train[“b”], x_train[“c”], x_train[“d”], x_train[“e” ], x_train[&quot;f&quot;], c=kmeans.labels_)
plt.show()

该错误似乎表明出错的部分位于 plt.scatter() 行。
我尝试了 2 个功能，它可以工作，但是当涉及 7 个功能时，我收到错误消息。可能出了什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/78468674/k-means-how-to-solve-error-scatter-got-multiple-values-for-argument-c</guid>
      <pubDate>Sun, 12 May 2024 17:07:02 GMT</pubDate>
    </item>
    <item>
      <title>将 keras_facenet 模型转换为 tflite</title>
      <link>https://stackoverflow.com/questions/78462866/converting-keras-facenet-model-to-tflite</link>
      <description><![CDATA[如何将keras_facenet模型转换为tensorflow lite以进行人脸检测任务。模型已预先训练好，只需转换即可
回购链接 - https://github.com/Kartikgc9/faceRecognition_Labs/tree/master&lt; /a&gt;
我尝试到处搜索此内容，但在查找 .H5 文件时遇到问题。上面提供了回购链接]]></description>
      <guid>https://stackoverflow.com/questions/78462866/converting-keras-facenet-model-to-tflite</guid>
      <pubDate>Fri, 10 May 2024 23:08:22 GMT</pubDate>
    </item>
    <item>
      <title>不平衡学习管道的哪些部分应用于测试集？</title>
      <link>https://stackoverflow.com/questions/78462616/which-parts-of-the-imbalanced-learn-pipeline-are-applied-to-the-test-set</link>
      <description><![CDATA[我创建了一个由 RobustScaler、SMOTE-NC、 组成的 imbalanced-learn Pipeline随机欠采样和随机森林分类器。
RandomSearchCV 用于选择最佳的超参数。
我想在我的测试集上测试最佳估计器。
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)

缩放器 = RobustScaler(quantile_range=(25.0, 75.0))
smote = SMOTENC(categorical_features=categorical_features,抽样策略=0.35,random_state=42)
rus = RandomUnderSampler(sampling_strategy=0.35, random_state=42)
分类器 = RandomForestClassifier(random_state=42)

管道=不平衡_make_pipeline（缩放器，smote，rus，分类器）

random_search = RandomizedSearchCV(管道, param_distributions=param, 评分=scoring_metric, cv=cv, n_iter=10, random_state=42, n_jobs=-1)

best_model = random_search.fit(X_train, y_train).best_estimator_

y_pred = best_model.predict(X_test)

据我了解，只有缩放（通过X_train获得的设置）和分类器应该应用于测试集。 SMOTE 和 RandomUndersampling 不应应用于 X_test。
这是由 imbalanced-learn 管道保证的还是我必须考虑其他事情？]]></description>
      <guid>https://stackoverflow.com/questions/78462616/which-parts-of-the-imbalanced-learn-pipeline-are-applied-to-the-test-set</guid>
      <pubDate>Fri, 10 May 2024 21:28:24 GMT</pubDate>
    </item>
    <item>
      <title>如何配置作业yaml和Yolov8数据集ymal来访问Azure上的数据资产？</title>
      <link>https://stackoverflow.com/questions/78453842/how-to-configure-job-yaml-and-yolov8-dataset-ymal-to-access-data-asset-on-azure</link>
      <description><![CDATA[我目前正在使用 Azure ML CLI v2 在 Azure ML 工作室中使用 Yolov8 训练自定义模型。
问题：
当我在 Azure ML 上运行作业时，出现“权限被拒绝”错误
错误代码：ScriptExecution.StreamAccess.Authentication
本机错误：来自输入数据源的流式传输错误
    StreamError(PermissionDenied(Some(此请求无权使用此权限执行此操作。)))
=&gt;访问流时权限被拒绝。原因：一些（该请求无权使用该权限执行该操作。）
    PermissionDenied(Some(此请求无权使用此权限执行此操作。))
错误消息：尝试访问流时身份验证失败。确保您设置了正确的权限。好的（该请求无权使用该权限执行该操作。）| session_id=da7b713c-6cc8-4f6d-b24f-b54ab37e14ef

Yaml 文件：

job.yaml：

$schema：https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
实验名称：yolov8-实验

命令： |
  sed -i“s|路径：.*$|路径：${{inputs.training_data}}|”自定义数据集.yaml
  # 训练模型
  yolo 任务=检测训练数据=custom_dataset.yaml 模型=${{inputs.model_to_train }} epochs=10 项目=yolov8-实验名称=实验

输入：
  训练数据：
    类型：uri_文件夹
    模式：ro_mount
    path: azure:data_asset:1 #我已经从本地文件创建了数据资产。
  模型到训练：
    类型：自定义模型
    路径：azureml:yolov8l:1

code: /training-code/ #custom_dataset.yaml 存储在本地的路径
环境：azureml：yolov8-环境：1
计算：azureml：compute_cluster


custom_dataset.yaml 文件：

路径：../数据集
火车：/图像/火车/
测试：/图像/测试/
值：/图像/测试/

NC=2

# 类名
名称：[class1，class2]

我参考了这些文章：
中-文章-yolov8-training-azure-cli
在 Azure ML 上训练模型使用 CLI v2 - Microsoft 培训
我在确定继续进行 Azure 设置所需的权限时遇到问题。我已使用“az login”成功登录，并成功创建了各种组件，例如环境、计算集群和数据资产。
但是，我不确定进一步操作所需的具体权限。我的设置需要授予哪些权限才能正常运行？]]></description>
      <guid>https://stackoverflow.com/questions/78453842/how-to-configure-job-yaml-and-yolov8-dataset-ymal-to-access-data-asset-on-azure</guid>
      <pubDate>Thu, 09 May 2024 10:35:51 GMT</pubDate>
    </item>
    <item>
      <title>每个示例使用多个类别对分类特征进行编码</title>
      <link>https://stackoverflow.com/questions/57752264/encode-a-categorical-feature-with-multiple-categories-per-example</link>
      <description><![CDATA[我正在处理一个数据集，该数据集的一个特征是单个示例具有多个类别。
该功能如下所示：- 

&lt;前&gt;&lt;代码&gt;功能
0 [类别 1、类别 2、类别 2、类别 4、类别 5]
1 [类别 11、类别 20、类别 133]
2 [类别2、类别9]
3 [类别1000、类别1200、类别2000]
4 [类别12]

该问题与发布的问题类似：- 每个示例使用多个类别对分类特征进行编码 - sklearn
现在，我想向量化这个特征。一种解决方案是按照上述类似问题的答案中的建议使用 MultiLabelBinarizer。但是，大约有 2000 个类别，这导致编码数据稀疏且维数非常高。
还有其他可以使用的编码吗？或者这个问题的任何可能的解决方案。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/57752264/encode-a-categorical-feature-with-multiple-categories-per-example</guid>
      <pubDate>Mon, 02 Sep 2019 06:25:01 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的“平均”f1 分数是什么？</title>
      <link>https://stackoverflow.com/questions/45604888/what-is-mean-f1-score-in-machine-learning</link>
      <description><![CDATA[我知道使用精度和召回率的 f1 分数。
但是，平均 f1 分数中的“平均”是什么？
我们什么时候使用它以及如何计算“平均值”？
编辑以明确解释我的问题：
我知道 f1 分数是精确率和召回率的调和平均值。
而当我们计算f1分数时，需要多个分类结果来计算精度和召回率。
例如，如果我们有一个由 1000 个实例组成的数据集，我们可以获得 1000 个分类结果。然后我们将其放入列联表中，这样我们就可以计算出f1分数。
现在这就是我对“平均”f1 分数感到困惑的地方。我们根据列联表计算 f1 分数，但“平均值”是什么？只有我能计算的是f1分数，那么什么是“平均值”以及如何计算“平均值”f1分数？]]></description>
      <guid>https://stackoverflow.com/questions/45604888/what-is-mean-f1-score-in-machine-learning</guid>
      <pubDate>Thu, 10 Aug 2017 05:06:35 GMT</pubDate>
    </item>
    </channel>
</rss>