<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 12 Feb 2024 03:14:20 GMT</lastBuildDate>
    <item>
      <title>为什么重新运行 jupyter 笔记本单元会增加内存使用量？</title>
      <link>https://stackoverflow.com/questions/77979163/why-does-rerunning-a-jupyter-notebook-cell-increase-memory-usage</link>
      <description><![CDATA[我不确定这是 Pytorch 还是 jupyter 笔记本问题。
我正在使用 Pytorch 和 jupyter 笔记本在 CPU 上训练一些神经网络。我可以运行整个 jupyter 笔记本，但是当我重新运行某些单元（例如训练代码）时，我会遇到内存错误或崩溃。
这是为什么呢？我重新运行的单元格没有在任何地方积累对象。他们只是重新加载数据加载器并重新训练模型。
额外的内存从哪里来？]]></description>
      <guid>https://stackoverflow.com/questions/77979163/why-does-rerunning-a-jupyter-notebook-cell-increase-memory-usage</guid>
      <pubDate>Mon, 12 Feb 2024 03:09:53 GMT</pubDate>
    </item>
    <item>
      <title>如何为每个患者创建一个单独的图，显示其“订单金额”和“订单数量”与“订单时间”变量的关系？</title>
      <link>https://stackoverflow.com/questions/77979145/how-do-i-create-a-separate-plot-for-each-patient-showing-their-order-amount-an</link>
      <description><![CDATA[我现在面临的问题是代码返回一个图，该图显示与用户输入日期匹配的最新数据。我想一起研究数据，因此我想要图中的趋势，而不是最新数据的单个水平线。然而，它不显示与相同“患者ID”相对应的过去数据和最新数据的合并数据。我想创建两个图，其中一个图是“订单金额”与“订单时间”的对比，另一个图是“订单数量”与“订单时间”的对比。 (https://i.stack.imgur.com/hXLgl.jpg)&lt; /p&gt;
数据按以下方式分组：(https://i.stack.imgur. com/qJHGA.jpg)
代码想要解决四个主要部分：

将最新数据与输入日期进行匹配，我已经根据输入日期过滤了数据帧。

将之前数据的相同 ID 与最新数据进行匹配，并将数据串在一起：为了实现这一点，我们需要识别每个患者 ID 的最新数据，然后将其与该患者之前的数据合并。相同的患者 ID。

在共享相同“患者 ID”和“订单时间”的两行中选取“订单金额”和“订单数量”值最高的行：将最近的数据与先前的数据合并后，我们可以比较每个患者 ID 的订单金额和订单数量的值，并选择具有最高值的行。 - 我被困在这个阶段:)


例如，第 27 行和第 28 行显示第 28 行的“订单金额”和“订单数量”分别为 659.8 和 21，以及 671.677 和 13。在这种情况下，我们将选择第 28 行作为“订单数量”值似乎要低得多，而“订单金额”值似乎彼此接近。在不同的场景中，当第 29 行和第 30 行具有相同的“患者 ID”时，我们会选择第 30 行，因为其“订单金额”值要低得多，并且其“订单数量”值与第 30 行的值相差小于“订单”数量&#39;。第 32 行和第 33 行是替代示例。如果“订单金额”和“订单数量”的值相同，我们可以选择第 32 行作为两者之间的第一行。如果只有一个例外，我们将选择单行。第 31 行就是一个示例。
(https://i.stack.imgur.com/irDL0.jpg)

将每个患者 ID 的“订单金额”与“订单时间”和“订单数量”与“订单时间”的点绘制在一起。这意味着每个患者 ID 一个图。

导入 pandas 作为 pd
导入日期时间
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
导入urllib
将 matplotlib.dates 导入为 mdates

input_date = input(&quot;请输入日期（格式：yyyy/mm/dd）：&quot;)
input_date = datetime.datetime.strptime(input_date, &quot;%Y/%m/%d&quot;)

df[&#39;订单时间&#39;] = pd.to_datetime(df[&#39;订单时间&#39;])

df[&#39;time_diff&#39;] = (df[&#39;订单时间&#39;] - input_date).dt.days

df_filtered = df[(df[&#39;time_diff&#39;] &gt;= -30) &amp; (df[&#39;time_diff&#39;] &lt;= 30)]

centre_data = df_filtered.sort_values(by=&#39;订单时间&#39;).groupby(&#39;患者 ID&#39;).last()

merged_data = pd.merge（df，recent_data，on =&#39;患者ID&#39;，后缀=（&#39;_prev&#39;，&#39;&#39;））

## 到目前为止似乎还不错，尽管将后缀包含为“_prev”很奇怪。

idx_amount = merged_data.groupby(&#39;患者ID&#39;)[&#39;订单金额&#39;].idxmax()
idx_quantity = merged_data.groupby(&#39;患者 ID&#39;)[&#39;订单数量&#39;].idxmax()

idx_highest = idx_amount[idx_amount.isin(idx_quantity)]

Filtered_data = merged_data.loc[idx_highest]

defplot_filtered_data（数据）：

    grouped_data = data.groupby(&#39;患者 ID&#39;)
    
    对于 grouped_data 中的 Patient_id、group_data：
        plt.plot(group_data[&#39;订单时间&#39;], group_data[&#39;订单金额&#39;],marker=&#39;o&#39;, label=f&#39;患者{patent_id}&#39;)
    
    plt.xlabel(&#39;下单时间&#39;)
    plt.ylabel(&#39;订单金额&#39;)
    plt.title(&#39;患者的订单金额与订单时间&#39;)
    plt.图例()
    plt.xticks（旋转=45）
    plt.tight_layout()
    plt.show()

绘图过滤数据（过滤数据）


我知道代码中存在错误，但我的编辑经验很少。有人可以指出我的错误吗？我知道这听起来有点复杂。]]></description>
      <guid>https://stackoverflow.com/questions/77979145/how-do-i-create-a-separate-plot-for-each-patient-showing-their-order-amount-an</guid>
      <pubDate>Mon, 12 Feb 2024 02:57:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 model.evaluate() 和 model.predict() 时的准确性不同</title>
      <link>https://stackoverflow.com/questions/77979085/different-accuracy-when-using-model-evaluate-and-model-predict</link>
      <description><![CDATA[我正在使用 4 个标签执行多类分类，使用 model.predict() 和 model.evaluate() 时有不同的结果，这里是我的代码：
test_loss, test_accuracy = model.evaluate(testing_generator)

这段代码的结果是
测试损失：0.0561
测试准确率：99.31%
y_true=testing_generator.classes
y_pred_probs = model.predict(testing_generator)
y_pred = np.argmax(y_pred_probs, 轴=1)

准确度=准确度_得分(y_true, y_pred)

这段代码的结果是
准确率：25.17%
我使用包含测试图像的相同testing_generator，为什么会发生这种情况？有谁知道怎么解决吗？
我陷入了这个问题，我希望准确性是相同的值，因为我使用相同的testing_generator]]></description>
      <guid>https://stackoverflow.com/questions/77979085/different-accuracy-when-using-model-evaluate-and-model-predict</guid>
      <pubDate>Mon, 12 Feb 2024 02:29:26 GMT</pubDate>
    </item>
    <item>
      <title>在 qiskit.algorithms.optimizers.ADAM 优化过程中获取中间步骤的方法</title>
      <link>https://stackoverflow.com/questions/77978874/ways-to-get-intermediate-steps-during-optimization-process-in-qiskit-algorithms</link>
      <description><![CDATA[使用 Qiskit 内部 ADAM 优化器时，有什么方法可以获取中间步骤（类似于 Tensorflow 或 Pytorch Adams 优化器中的回调函数）？
我正在实现变分量子电路 (VQC)，以使用 Qiskit 的 Adam 优化器训练量子机器学习模型。
我浏览了 ADAM 的文档 Qiskit 中的优化器，但我找不到合适的方法来获取它们。
我可以使用 Pytorch Adam 优化器获取中间训练信息，但我想知道是否可以使用 Qiskit 来做到这一点。
我正在使用qiskit v0.45.2。]]></description>
      <guid>https://stackoverflow.com/questions/77978874/ways-to-get-intermediate-steps-during-optimization-process-in-qiskit-algorithms</guid>
      <pubDate>Mon, 12 Feb 2024 00:13:43 GMT</pubDate>
    </item>
    <item>
      <title>整洁的Python基因组的计算并没有真正相加</title>
      <link>https://stackoverflow.com/questions/77978427/the-calculation-of-the-neat-python-genome-doesnt-really-add-up</link>
      <description><![CDATA[我为 XOR 制作了一个非常简单的整洁的 python。这是基因组的摘要。
最佳基因组：
钥匙：141
健身：2.993003425787766
节点：
    0 DefaultNodeGene（键= 0，偏差= -0.8080802310263379，响应= 1.0，激活= sigmoid，聚合=总和）
连接：
    DefaultConnectionGene(键=(-2, 0)，权重=1.5655941592328844，启用=True)
    DefaultConnectionGene(key=(-1, 0)，权重=1.2986799185483175，启用=True

但是当我尝试计算它时，它并没有真正意义。
以下是评估结果：
 输入 (0.0, 0.0)，预期输出 (0.0,)，得到 [0.017286340618090416]
  输入（0.0，1.0），预期输出（1.0，），得到[0.9778511014280682]
  输入（1.0，0.0），预期输出（1.0，），得到[0.920780444438713]
  输入（1.0，1.0），预期输出（0.0，），得到[0.9999657218870016]

这就是我计算输入 (1, 1) 的方法：
activation_func( sum( 输入 * 权重 ) + 偏差 )

对于这种情况：
sigmoid( 1 * 1.2986799185483175 + 1 * 1.5655941592328844 + (-0.8080802310263379) )
= 0.88657197794273698476
但是根据评估应该是0.9999657218870016
我错过了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/77978427/the-calculation-of-the-neat-python-genome-doesnt-really-add-up</guid>
      <pubDate>Sun, 11 Feb 2024 21:04:52 GMT</pubDate>
    </item>
    <item>
      <title>当图片是几个函数的绘图时的图像识别</title>
      <link>https://stackoverflow.com/questions/77978372/image-recognition-when-the-picture-is-a-plot-of-a-few-functions</link>
      <description><![CDATA[
你好：
您能给我一些建议吗？
目标是对多个对象进行监督分类。每个对象都由两个函数的图来描述。每个对象的绘图尺寸 (b - a) 和 T 大致相同，看起来它们可以标准化为 (0, 1) 和 (0, 1)。
这是一个图像分类问题吗？我这么问是因为大多数像素都是白色的。
无论如何，您会推荐什么方法来解决这个问题？
TIA]]></description>
      <guid>https://stackoverflow.com/questions/77978372/image-recognition-when-the-picture-is-a-plot-of-a-few-functions</guid>
      <pubDate>Sun, 11 Feb 2024 20:46:45 GMT</pubDate>
    </item>
    <item>
      <title>BERT 如何理解 [CLS] 令牌？</title>
      <link>https://stackoverflow.com/questions/77978283/how-does-bert-understand-the-cls-token</link>
      <description><![CDATA[我已经阅读了论坛上的很多答案和媒体上的文章，但我仍然不完全理解模型如何准确地从我们有分类任务的 [CLS] 标记中识别出来。 [CLS] 令牌的处理是在代码的开头部分，还是来自编码器中处理令牌的注意层？]]></description>
      <guid>https://stackoverflow.com/questions/77978283/how-does-bert-understand-the-cls-token</guid>
      <pubDate>Sun, 11 Feb 2024 20:17:52 GMT</pubDate>
    </item>
    <item>
      <title>字符串“loss”被传递给 metric_name() 而不是指标名称</title>
      <link>https://stackoverflow.com/questions/77977110/string-loss-being-passed-to-metric-name-instead-of-metric-name</link>
      <description><![CDATA[我正在遵循教程，但收到错误：
ValueError：无法解释指标标识符：丢失
在：
\keras\src\metrics\__init__.py:205，在 get(identifier) 中
看起来在wrapper.py第532行metric_name(key)应该接收损失函数的名称，但它实际上接收字符串“loss”
以下是错误的相关代码：
def buildNetwork():
    分类器=顺序（）
    classificator.add（密集（单位= 20，激活=&#39;relu&#39;，kernel_initializer =&#39;random_uniform&#39;，input_shape =（30，）））
    classificator.add（密集（单位= 20，激活=&#39;relu&#39;，kernel_initializer =&#39;random_uniform&#39;））
    classificator.add（密集（单位= 1，激活=&#39;sigmoid&#39;））
    优化器= keras.optimizers.Adam(learning_rate=0.001,weight_decay=0.000001)
    classificator.compile（优化器=优化器，损失=&#39;binary_crossentropy&#39;，指标=[&#39;binary_accuracy&#39;]）
    返回分类器


分类器 = KerasClassifier(model=buildNetwork,batch_size=10,epochs=100, loss=&#39;binary_crossentropy&#39;)

分数 = cross_val_score(估计器=分类器, X=数据, y=真相, cv=10, error_score=&#39;raise&#39;)

如果在keras_metric_get中我手动设置identifier =“binary_crossentropy”它工作正常
我不知道是否是兼容性问题，但我有 Keras 3.0.8、TF 2.15.0 和 SciKeras 0.12.0]]></description>
      <guid>https://stackoverflow.com/questions/77977110/string-loss-being-passed-to-metric-name-instead-of-metric-name</guid>
      <pubDate>Sun, 11 Feb 2024 14:32:54 GMT</pubDate>
    </item>
    <item>
      <title>关于进化图卷积网络的机器学习[关闭]</title>
      <link>https://stackoverflow.com/questions/77977043/machine-learning-about-evolve-graph-convolution-network</link>
      <description><![CDATA[如何使用 Jupyter Notebook 的 Evolve Graph 卷积网络代码，使用 Kaggle 上的 Elliptic_Bitcoin_Heist 数据。
我无法编码。谁能帮我？我已经阅读了该程序 https://github.com/IBM/EvolveGCN.git 但我不知道如何使用它。我尝试在 ubuntu 上运行，但总是出错，我无法安装它。我想用jupyter笔记本编码]]></description>
      <guid>https://stackoverflow.com/questions/77977043/machine-learning-about-evolve-graph-convolution-network</guid>
      <pubDate>Sun, 11 Feb 2024 14:10:41 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 自定义转换器从底层模型中抛出 NotFittedError</title>
      <link>https://stackoverflow.com/questions/77976770/scikit-learn-custom-transformer-throws-notfittederror-from-underlying-model</link>
      <description><![CDATA[我想创建自己的 scikit-learn 转换器，用于对包含分类的数字特征进行编码，例如邮政编码或行业代码（NAICS、MCC 等）。在这些类型的代码中有一个结构：例如MCC 3000-3999 是“旅行和娱乐”，它进一步细分为更细粒度的类别，例如“航空公司”、“汽车租赁”等。我们不能将它们用作序数特征，但如果我们将它们视为纯分类特征（例如，通过 One -Hot-Encoding）我们需要选择在代码结构的哪个级别应用特征编码。
为了解决这个问题，我创建了自己的 scikit-learn 变压器，它是 TargetEncoder 使用决策树。代码如下所示。重要的是要认识到，在模型训练期间，应使用样本外决策树回归分数来避免过度拟合。因此，我实现了自己的 fit_transform 函数来生成这些样本外分数：
从 sklearn.tree 导入 DecisionTreeRegressor

从 sklearn.base 导入 TransformerMixin、BaseEstimator
从 sklearn.model_selection 导入 cross_val_predict

类 TaxonomyEncoder（TransformerMixin，BaseEstimator）：

def __init__(自身, n_leafs=10, cv=3):
    self.n_leafs = n_leafs
    自我简历 = 简历

def fit(self, X, y=None):
    self.tree_ = DecisionTreeRegressor(max_leaf_nodes=self.n_leafs).fit(X,y)
    返回自我

def 变换（自身，X）：
    返回 self.tree_.predict(X).reshape(-1,1)

def fit_transform(self, X, y=None):
    self.tree_ = DecisionTreeRegressor(max_leaf_nodes=self.n_leafs)
    返回 cross_val_predict(self.tree_, X, y, cv=self.cv).reshape(-1,1)

转换器工作正常，除非在 ColumnTransformer 中使用：
从 sklearn.compose 导入 ColumnTransformer

变压器 = ColumnTransformer([(&#39;分类法&#39;, TaxonomyEncoder(), [&#39;mcc&#39;])])
变压器.fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
Transformer.transform(df[[&#39;mcc&#39;]])

然后我得到决策树尚未拟合的错误：
NotFittedError：此 DecisionTreeRegressor 实例尚未安装。在使用此估计器之前，请使用适当的参数调用“fit”。

显然，scikit-learn 在导致此错误的表面下进行了一些检查。请注意，实际上没有理由需要拟合决策树，因为决策树是在 cross_val_predict 函数中重新拟合的。我该如何解决这个问题？
下面显示了重现该错误的完整工作示例：
导入 pandas 作为 pd
df = pd.DataFrame({&#39;mcc&#39;:[3000,3500,7339], &#39;y&#39;:[0,0,1]})

te = TaxonomyEncoder().fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
te.transform(df[[&#39;mcc&#39;]])

给出：
数组([[0.],
       [0.],
       [1.]])

并且 fit_transform 也给出了预期的结果：
te.fit_transform(df[[&#39;mcc&#39;]], df[&#39;y&#39;])

数组([[0.],
       [0.],
       [0.]])

但是当包装在 ColumnTransformer 中时，事情就会出错：
transformer = ColumnTransformer([(&#39;taxonomy&#39;, TaxonomyEncoder(), [&#39;mcc&#39;])])
变压器.fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
Transformer.transform(df[[&#39;mcc&#39;]])
]]></description>
      <guid>https://stackoverflow.com/questions/77976770/scikit-learn-custom-transformer-throws-notfittederror-from-underlying-model</guid>
      <pubDate>Sun, 11 Feb 2024 12:34:35 GMT</pubDate>
    </item>
    <item>
      <title>我在实现 Graycomatrix 方法时收到“JpegImageFile”对象不可下标错误</title>
      <link>https://stackoverflow.com/questions/77976714/i-am-getting-an-jpegimagefile-object-is-not-subscriptable-error-while-implemen</link>
      <description><![CDATA[代码
错误
我试图获取 2 个图像的 GLCM，每个图像分为 4 个补丁，但发生了此错误。这2张图片都是jpg的。该方法还可以获得补丁之间的相异性和相关性。我从这里获取了代码 https://scikit-image.org/文档/stable/auto_examples/features_detection/plot_glcm.html]]></description>
      <guid>https://stackoverflow.com/questions/77976714/i-am-getting-an-jpegimagefile-object-is-not-subscriptable-error-while-implemen</guid>
      <pubDate>Sun, 11 Feb 2024 12:14:45 GMT</pubDate>
    </item>
    <item>
      <title>分类任务问题[关闭]</title>
      <link>https://stackoverflow.com/questions/77976439/classification-task-issues</link>
      <description><![CDATA[我有一个关于二元变量分类任务的概念性问题。我可用的数据集如下：

id，设备 ID
性别，目标变量
日
小时
内容、具有许多唯一值的分类变量

这些数据代表客户进行的网络搜索，并且出现了两个问题：

目标变量高度不平衡，女性较多，男性较少。
内容变量有许多唯一值。

考虑到我的时间有限，并且模型结果背后的推理比模型本身更重要，任何人都可以提出解决这两个问题的最佳方法吗？
我正在寻求这两个问题的解决方案，以下是我的考虑：

对内容变量应用诸如单热编码之类的技术会导致维度问题。我可以考虑设置阈值并考虑最常见的值，同时将其他值分类为“其他”。
应用目标编码会反映出类别不平衡的问题。
]]></description>
      <guid>https://stackoverflow.com/questions/77976439/classification-task-issues</guid>
      <pubDate>Sun, 11 Feb 2024 10:35:58 GMT</pubDate>
    </item>
    <item>
      <title>如何在苹果 M1 Pro 芯片组上的 XGBoost 中启用 GPU</title>
      <link>https://stackoverflow.com/questions/77975756/how-to-enable-gpu-in-xgboost-on-apple-m1-pro-chipset</link>
      <description><![CDATA[我尝试在带有设备 = cuda 的 Windows 上使用 GPU 进行 XGBoost 训练，它有效并且训练时间大大减少，现在我想在我的 Mac M1 Pro 上进行此实验。
如何在 m1 pro 芯片组上启用 GPU 的 XGBoost。
我尝试查看文档，但找不到信息。]]></description>
      <guid>https://stackoverflow.com/questions/77975756/how-to-enable-gpu-in-xgboost-on-apple-m1-pro-chipset</guid>
      <pubDate>Sun, 11 Feb 2024 05:51:44 GMT</pubDate>
    </item>
    <item>
      <title>如何使用多个图像训练序列模型</title>
      <link>https://stackoverflow.com/questions/77973831/how-to-train-sequential-model-with-multiple-images</link>
      <description><![CDATA[我在传递多个图像进行训练时遇到错误。但是当只传递一张图像时就很好了。图像大小相同。
这是代码：
导入tensorflow为tf
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np
从张量流导入keras
从tensorflow.keras导入图层、数据集、模型

# 加载模板图像
template_image = tf.keras.preprocessing.image.load_img(&#39;模板.jpg&#39;)
template_array = tf.keras.preprocessing.image.img_to_array(template_image)

# 加载实际图像
实际图像 = tf.keras.preprocessing.image.load_img(&#39;实际.jpg&#39;)
实际数组 = tf.keras.preprocessing.image.img_to_array(实际图像)

# 创建模型
模型 = tf.keras.Sequential([
  层.InputLayer(input_shape=(template_array.shape)),
  层.Conv2D(16, (3, 3), 激活=&#39;relu&#39;),
  层.MaxPooling2D((2, 2)),
  层.Conv2D(32, (3, 3), 激活=&#39;relu&#39;),
  层.MaxPooling2D((2, 2)),
  层.Flatten(),
  层.Dense(64, 激活=&#39;relu&#39;),
  层.Dense(2, 激活=&#39;softmax&#39;),
]）

# 编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
模型.summary()
对于 model.layers 中的图层：
    打印（层.output_shape）

template_array = template_array.reshape((1, 549, 549, 3))
实际数组 = 实际数组.reshape((1, 549, 549, 3))
train_x = [模板数组，实际数组]
y_train = np.array([1,0])
y_train = y_train.reshape(1,2)
train_y = [y_train, y_train]

print(&quot;X 形状是：&quot;)
打印（模板_数组.形状）
print(&quot;Y 形状是：&quot;)
打印（y_train）

# 训练模型
model.fit(x=train_x, y=train_y, epochs=10)

＃ 作出预测
预测 = model.predict([actual_array])

# 检查是否有错误或缺失的部分
对于范围内的 i(len(预测))：
  如果预测[i][0]&gt;预测[i][1]：
    print(&#39;第 {} 部分丢失或不正确&#39;.format(i))


收到错误：
ValueError：层“sequential_28”预计有 1 个输入，但它收到了 2 个输入张量。收到的输入：
[&lt;tf.Tensor &#39;IteratorGetNext:0&#39; shape=(None, 549, 549, 3) dtype=float32&gt;,
]

如果我传递 x=template_array 和 y = y_train 它运行良好。但这意味着我只使用一张图像进行训练。
我是否无法使图像数组和相应的分类数组同时通过？我如何立即传递所有列车数据？]]></description>
      <guid>https://stackoverflow.com/questions/77973831/how-to-train-sequential-model-with-multiple-images</guid>
      <pubDate>Sat, 10 Feb 2024 16:28:24 GMT</pubDate>
    </item>
    <item>
      <title>R 机器学习中 Ranger 模型的错误[重复]</title>
      <link>https://stackoverflow.com/questions/77970324/an-error-in-ranger-model-in-machine-learning-in-r</link>
      <description><![CDATA[我正在运行生存模型的机器学习代码。我的 pred_prob 代码有错误。谁能帮我？先感谢您
我的错误是：
&lt;代码&gt;&gt; pred_prob &lt;- rowMeans(ranger_predict$train_data[, 1:dim(ranger_predict$train_data)[2]])
h(simpleError(msg, call)) 中的错误：
  在为函数“rowMeans”选择方法时评估参数“x”时出错：$ 运算符对于原子向量无效

我的代码是：
库(ranger) 库(生存) 库(caret) 库(dplyr) 库(pec) df2 &lt;- df2 %&gt;% mutate(status = time_15year-1) # 0 = 审查，1 = 死亡 df2 &lt; ;- na.omit(df2) df2$time_15year &lt;- Floor(df2$time_15year) 对(df2 %&gt;% dplyr::select(time_15year,BS_death), main = &quot;NCCTG 脑卒中数据&quot;) 折叠 &lt; - 2 # 用于交叉验证 cvIndex &lt;- createFolds(factor(df2$BS_death),folds, returnTrain = T)container_model &lt;-vector(“list”,length(cvIndex))container_pred &lt;-container_model for (i in 1:length(cvIndex)) { train_data &lt;- df2[cvIndex[[i]], } testing_data &lt;- df2[-cvIndex[[i]],] train_data &lt;- train_data[complete.cases(train_data) , ] 测试数据 &lt;- 测试数据[完整.案例(测试数据), ] rangermodel &lt;- ranger(Surv(time_15year, BS_death) ~ 年龄 + 性别 + 教育 + 地点 + cvahis+ mihis + bphis +heartdis + smok + Pastsmok+ 被动+活动+waterpip +cvatype，数据= train_data）图（rangermodel$unique.death.times，rangermodel$survival[1，]）ranger_predict &lt;-预测（rangermodel，数据=testing_data）pred_prob &lt;-rowMeans（ranger_predict$train_data[， 1:dim(ranger_predict$train_data)[2]]) pred_prob[pred_prob&gt;median(pred_prob)]=1 pred_prob[pred_prob&lt;=median(pred_prob)]=0 fusionMatrix(as.factor(testing_data$BS_death), as.factor (pred_prob))]]></description>
      <guid>https://stackoverflow.com/questions/77970324/an-error-in-ranger-model-in-machine-learning-in-r</guid>
      <pubDate>Fri, 09 Feb 2024 19:06:49 GMT</pubDate>
    </item>
    </channel>
</rss>