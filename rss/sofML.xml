<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 21 Mar 2024 18:17:00 GMT</lastBuildDate>
    <item>
      <title>Seq-to-seq LSTM 无法正确学习</title>
      <link>https://stackoverflow.com/questions/78201576/seq-to-seq-lstm-not-learning-properly</link>
      <description><![CDATA[我正在尝试使用 Pytorch 中的 LSTM 解决 seq-to-seq 问题。具体来说，我采用 5 个元素的序列来预测接下来的 5 个元素。我关心的是数据转换。我有大小为 [bs, seq_length, features] 的张量，其中 seq_length = 5 和 features = 1。每个特征都是一个介于 0 和 3 之间的整数（两者都包含在内）。
我认为输入数据必须使用 MinMaxScaler 转换为浮点范围 [0, 1]，以便使 LSTM 学习过程更容易。之后，我应用一个线性层，它将隐藏状态转换为相应的输出，其大小为特征。我在 Pytorch 中对 LSTM 网络的定义：
类 LSTM(nn.Module):
    def __init__(自身、input_dim、hidden_​​dim、output_dim、num_layers、dropout_prob):
        super(LSTM, self).__init__()
        self.lstm_layer = nn.LSTM(input_dim,hidden_​​dim,num_layers,dropout=dropout_prob)
        self.output_layer = nn.Linear（hidden_​​dim，output_dim）

    ...

    def 向前（自身，X）：
        out, (隐藏, 单元格) = self.lstm_layer(X)
        输出 = self.output_layer(输出)
        返回

我用来进行训练循环的代码如下：
def train_loop(t, checkpoint_epoch, dataloader, model, loss_fn, optimizationr):
    大小 = len(dataloader.dataset)
    对于批处理，枚举中的 X（数据加载器）：
        X = X[0].type(torch.float).to(设备)

        # X = torch.Size([batch_size, 10, input_dim])
        # 将序列拆分为输入和目标
        输入 = 变换(X[:, :5, :]) # 输入 = [batch_size, 5, input_dim]
        目标 = 变换(X[:, 5:, :]) # 目标 = [batch_size, 5, input_dim]

        # 预测（前向传递）
        使用自动转换（）：
            pred = 模型(输入) # pred = [batch_size, 5, input_dim]
            损失 = loss_fn(pred, 目标)

        # 反向传播
        优化器.zero_grad()
        scaler.scale(loss).backward()
        缩放器.step（优化器）
        定标器.update()

        如果批次 % 100 == 0:
            损失，当前 = loss.item(), 批次 * len(X)
            #print(f&quot;当前损耗：{loss:&gt;7f}, [{current:&gt;5d}/{size:&gt;5d}]&quot;)

        # 删除变量并清空缓存
        del X、输入、目标、预测
        torch.cuda.empty_cache()

    回波损耗

我用于预处理数据的代码：
def main():
    代理数量 = 2
    # 打开HDF5文件
    将 h5py.File(&#39;dataset_&#39; + str(num_agents) + &#39;UAV.hdf5&#39;, &#39;r&#39;) 作为 f：
        # 访问数据集
        数据 = f[&#39;数据&#39;][:]
        # 转换为 PyTorch 张量
        data_tensor = torch.tensor(数据)

        大小 = data_tensor.size()
        序列长度 = 10
        重塑 = data_tensor.view(-1, 大小[2], 大小[3])

        r_size = reshape.size()
        重塑 = 重塑[:, :, 1:]
        reshape_v2 = reshape.view(r_size[0], -1)

        数据集 = create_dataset(reshape_v2.numpy(), seq_length)

        f.close()

    数据集 = TensorDataset(数据集)

    # 将数据集分为训练集和验证集
    train_size = int(0.8 * len(dataset)) # 80% 用于训练
    val_size = len(dataset) - train_size # 20% 用于验证
    train_dataset, val_dataset = random_split(数据集, [train_size, val_size])

    train_dataloader = DataLoader（train_dataset，batch_size = params [&#39;batch_size&#39;]，shuffle = True，pin_memory = True）
    val_dataloader = DataLoader（val_dataset，batch_size = params [&#39;batch_size&#39;]，shuffle = False，pin_memory = True）

尝试这个，模型没有正确学习，所以我想也许可以直接计算 targets （范围 [0, 1] 内的浮点值）和 pred 之间的损失code&gt; （我认为由于 LSTM 层的 tanh 激活函数，浮点值在 [-1, 1] 范围内），具有不同的尺度可能是错误的。然后，我尝试在前向传递中的线性层之后应用 sigmoid 激活函数，但也没有正确学习。我尝试了许多超参数组合的执行，但没有一个产生“正常”的结果。训练曲线。我还附上了 5000 epoch 的屏幕截图来说明训练过程：

我的问题是：

我的训练过程中似乎存在什么问题？
我所说的话是否被认为是错误的？
]]></description>
      <guid>https://stackoverflow.com/questions/78201576/seq-to-seq-lstm-not-learning-properly</guid>
      <pubDate>Thu, 21 Mar 2024 16:43:58 GMT</pubDate>
    </item>
    <item>
      <title>如何以我的 k 折叠分割方式适应 lgb.cv？</title>
      <link>https://stackoverflow.com/questions/78201471/how-to-adapt-lgb-cv-in-my-k-folds-splitting-way</link>
      <description><![CDATA[我设计了一个方法，将数据分成5折，然后我想用它来执行5折交叉验证。
来自 load_data 导入 load_data
折叠、test_samples、input_shape = load_data()

folds[0].keys()
# dict_keys([&#39;训练&#39;, &#39;val&#39;, &#39;测试&#39;])

要使用特定的 5 倍来优化 GBM 模型，对于任何优化方法（例如 RandomSearch、GridSearch...），我需要为每个超参数配置训练 5 个模型，然后评估模型性能。
一种方法是，我使用迭代每次折叠来训练模型
early_stopping = lgb.early_stopping(stopping_rounds=10)
模型 = lgb.LGBMClassifier()
model.fit(X, y, 回调=[early_stopping],...)

我发现它的另一种方式是 lgb.cv，它不允许我的折叠 适合。
有人知道如何在不使用分割的情况下实现 lgb.cv 吗？
这是 1 个配置的片段代码
从 sklearn.metrics 导入 roc_auc_score
从 sklearn.metrics 导入 precision_score
from timeit import default_timer 作为计时器

对于 i，折叠枚举（折叠）：
    print(&#39;折叠&#39;, i+1)
    训练，验证，测试=折叠[折叠].values()
    Early_stopping = lgb.early_stopping(stopping_rounds=10)
    模型 = lgb.LGBMClassifier()
    
    开始=定时器()
    model.fit(训练[&#39;x&#39;], 训练[&#39;y&#39;],
              回调=[early_stopping],
              评估集=[
                  (火车[&#39;x&#39;], 火车[&#39;y&#39;]),
                  (val[&#39;x&#39;], val[&#39;y&#39;]),
                  （测试[&#39;x&#39;]，测试[&#39;y&#39;]）]，
              eval_names=[&#39;训练&#39;, &#39;val&#39;, &#39;测试&#39;],
              eval_metric=[&#39;auc&#39;, &#39;binary_logloss&#39;],
              功能名称=功能名称）
    train_time = 计时器() - 开始

    ＃ 作出预测
    预测 = model.predict_proba(val[&#39;x&#39;])
    auc = roc_auc_score(val[&#39;y&#39;], 预测[:, 1])
    acc = precision_score(val[&#39;y&#39;], np.argmax(预测, axis=1))
    
    print(&#39;验证集上的验证准确度为{:.4f}.&#39;.format(acc))
    print(&#39;验证集上的验证auc为{:.4f}.&#39;.format(auc))
    print(&#39;训练时间为{:.4f}秒&#39;.format(train_time))

我如何以优化的方式调整它（例如，RandomSearch）？]]></description>
      <guid>https://stackoverflow.com/questions/78201471/how-to-adapt-lgb-cv-in-my-k-folds-splitting-way</guid>
      <pubDate>Thu, 21 Mar 2024 16:26:34 GMT</pubDate>
    </item>
    <item>
      <title>使用 Colab 在 Google Drive 上保存训练数据和模型</title>
      <link>https://stackoverflow.com/questions/78200960/saving-training-data-and-model-on-google-drive-with-colab</link>
      <description><![CDATA[最近我正在尝试使用 Google Colab 训练一些机器学习模型。我将支付 Colab Pro 计划的费用，每月花费 10 美元。我想将我的训练数据和训练后的模型保存在我的 Google Drive 上，默认情况下只有 15Gb 空间，而我已经使用了 12Gb。当我训练模型时，我会将我的 Google Drive 安装到 Colab。我非常确定剩余的 3Gb 不足以容纳我的数据和模型。
我的问题是，Colab pro 是否会在我的 Google 云端硬盘上为我提供更多空间？如果没有，这是否意味着我需要同时为 Colab Pro 和 Google One 支付额外的存储空间费用？另一方面，是否有另一种方式来“永久”存储数据和模型？并且可以轻松地从 Google Colab 访问？
非常感谢您的投入。
祝一切顺利，
极光]]></description>
      <guid>https://stackoverflow.com/questions/78200960/saving-training-data-and-model-on-google-drive-with-colab</guid>
      <pubDate>Thu, 21 Mar 2024 15:07:42 GMT</pubDate>
    </item>
    <item>
      <title>找到每个类别的图像原型？</title>
      <link>https://stackoverflow.com/questions/78200700/find-the-image-protoypes-for-each-class</link>
      <description><![CDATA[我尝试实现一种方法，该方法采用所有图像特征和相应的标签来为每个类生成图像原型。我在网上搜索过，但找不到任何教程或可用代码来验证我所做的是否正确或不正确。此外，我尝试计算加权图像原型，其中每个图像都有自己的权重。但是，我仍然不确定这种方法是否准确。
labels = torch.arange(num_classes)
weights = torch.tesnor([......]) ## 大小 N 的权重 = num_images
类均值 = []
对于可用标签中的 i：
    idx = (伪标签 == i)
    样本计数 = idx.float().sum().item()
    如果样本数&gt; 0.0：
        壮举=特征[idx]
        class_emebdding = torch.sum(feat*weights[idx],dim=0)
        class_emebdding /= class_emebdding.norm()
原型 = torch.stack(class_means, 0)
]]></description>
      <guid>https://stackoverflow.com/questions/78200700/find-the-image-protoypes-for-each-class</guid>
      <pubDate>Thu, 21 Mar 2024 14:31:36 GMT</pubDate>
    </item>
    <item>
      <title>在基于品种的作物产量预测模型中查找每个品种的准确性</title>
      <link>https://stackoverflow.com/questions/78199996/finding-the-accuracy-for-each-variety-in-a-variety-based-crop-yield-prediction-m</link>
      <description><![CDATA[我一直在使用回归研究田间作物的产量预测模型。我的输入特征包括 30 多个特定于作物的变量，这些变量是我使用 Google Earth Engine 针对每个由单个多边形标记的田地得出的。我还通过调查了解了每块田地种植的农作物的品种（具体是两种类型）。我想了解每个品种的模型准确性如何。我们以后如何确定模型对每个品种的准确性？
品种 1 - 有 100 个样品
品种 2 - 有 60 个样品
我正在考虑做这样的事情：

如果我遵循 70-30% 的分割，我就有 48 个测试样本。根据每个样本绘制预测产量。
根据多样性将样本分为几类。
通过找出误差差异来计算每个类别的 RMSE/MAE。

我不太确定这种方法。任何建议都会非常有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78199996/finding-the-accuracy-for-each-variety-in-a-variety-based-crop-yield-prediction-m</guid>
      <pubDate>Thu, 21 Mar 2024 12:40:06 GMT</pubDate>
    </item>
    <item>
      <title>尝试在自注释数据集上训练用于虹膜识别的神经网络</title>
      <link>https://stackoverflow.com/questions/78199835/trying-to-train-a-neural-network-for-iris-recognition-on-a-self-annotated-datase</link>
      <description><![CDATA[classloss = tf.keras.losses.BinaryCrossentropy()
回归损失 = 本地化损失
类损失（y\[0\]，类）

在此处输入图像描述
模型 = FaceTracker(facetracker)
model.compile(opt, classloss, regressloss)
logdir=&#39;日志&#39;
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)
hist = model.fit(train, epochs=15,validation_data=val,callbacks=\[tensorboard_callback\])


model.fit 抛出以下错误：
为什么会出现错误。在此处输入图像描述]]></description>
      <guid>https://stackoverflow.com/questions/78199835/trying-to-train-a-neural-network-for-iris-recognition-on-a-self-annotated-datase</guid>
      <pubDate>Thu, 21 Mar 2024 12:07:19 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 numpy 函数计算以下 hessian 矩阵以加快计算速度？</title>
      <link>https://stackoverflow.com/questions/78199806/how-can-i-compute-the-following-hessian-using-numpy-functions-to-speed-up-the-co</link>
      <description><![CDATA[我必须实现一个等效函数来计算逻辑损失的 hessian，写为指数项对数之和。我在Python中实现了以下功能：
def hessian(self,w,hess_trick=0):
        赫斯 = 0
        对于 zip(self.data, self.labels) 中的 x_i,y_i:
            hess += np.exp(y_i * np.dot(w.T, x_i))/((1 + np.exp(y_i * np.dot(w.T,x_i)))**2) * np.outer(x_i, x_i.T)
        返回hess + lambda_reg * np.identity(w.shape[0]) + hess_trick * 10**(-12) * np.identity(w.shape[0])

我的问题是如何在不使用慢速 python 的情况下编写等效但更快的函数？
由于我对 numpy 不太有信心，我尝试编写以下函数：
 def new_hessian(self, w, hess_trick=0):
        exp_term = np.exp(self.labels * np.dot(self.data, w))
        sigmoid_term = 1 + exp_term
        inv_sigmoid_sq = 1 / sigmoid_term ** 2

        diag_elements = np.sum((exp_term * inv_sigmoid_sq)[:, np.newaxis] * self.data ** 2, axis=0)
        off_diag_elements = np.dot((exp_term * inv_sigmoid_sq) * self.data.T, self.data)
        hess = np.diag(diag_elements) + off_diag_elements
        正则化 = lambda_reg * np.identity(w.shape[0])

        hess += hess_trick * 1e-12 * np.identity(w.shape[0])

        返回 hess + 正则化

通过调试这个函数，我发现存在一个根本性的问题。对于特征数量较小的值（例如小于 200），hessian 的两种实现不相等。当我增加特征数量时，这两个函数似乎是相等的。问题在于，当使用牛顿方法来优化对数损失来测试这些实现时，较快的实现会比第一个实现（但在运行时速度方面较慢）实现更多的迭代收敛。]]></description>
      <guid>https://stackoverflow.com/questions/78199806/how-can-i-compute-the-following-hessian-using-numpy-functions-to-speed-up-the-co</guid>
      <pubDate>Thu, 21 Mar 2024 12:02:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用 Tensorflow 时 Python 产生的结果比 kotlin 更准确？</title>
      <link>https://stackoverflow.com/questions/78199511/why-does-python-produce-a-more-accurate-result-than-kotlin-when-using-tensorflow</link>
      <description><![CDATA[我目前正在制作一个应用程序，它将检测不同数字系统中不同的手写数学表达式。截至目前，阻碍任何进展的主要因素是 kotlin 在使用 Tensorflow lite 时产生的不准确性 - 大约 10% 正确。我的 Python 代码非常相似，但它使用常规张量流，并且更加准确 - 大约 70% 正确。
我的想法是图像从 OpenCV Mat 转换为 Tensorbbuffer 的方式导致了一些问题，或者预处理的处理方式导致了差异。
我的代码片段如下：

提取边界矩形后，进行预处理和标准化。

val image_roi = Mat(tmp,boundRect)
Imgproc.cvtColor(image_roi, image_roi, Imgproc.COLOR_RGB2GRAY) Imgproc.GaussianBlur(image_roi, image_roi, Size(3.0,3.0), 0.0)
Imgproc.dilate(image_roi, image_roi, Imgproc.getStructuringElement(Imgproc.MORPH_RECT, Size(4.0, 4.0)))
Imgproc.threshold(image_roi, image_roi, 90.0, 255.0, Imgproc.THRESH_BINARY);
Imgproc.resize(image_roi, image_roi, 大小(28.0,28.0))
Core.normalize(image_roi, image_roi, 0.0, 255.0, Core.NORM_MINMAX);
image_roi.convertTo(image_roi, CvType.CV_8UC1)
提取.add(image_roi)


运行预测，将 OpenCV Mat 转换为 Tensorbuffer（第 3 步）

for（提取的img）{
       val 张量缓冲区 = extractBytes(img)
       val 输出 = model.process(tensorBuffer)
       valoutputFeature0=outputs.outputFeature0AsTensorBuffer
       valconf=outputFeature0.floatArray
       out += getLanguageText(conf, 数字)
 
}


将 Mat 转换为 Tensorbbuffer

私有乐趣 extractBytes(img: Mat): TensorBuffer{
        val inputFeature = TensorBuffer.createFixedSize(intArrayOf(1, 28, 28, 1), DataType.FLOAT32)
        val byteBuffer = ByteBuffer.allocateDirect(28 * 28 * 4) // 每个浮点数 4 个字节
        byteBuffer.order(ByteOrder.nativeOrder())
        byteBuffer.rewind()
 
        for (i 从 0 到 28) {
            for (j in 0 到 28) {
                val temp = img.get(i, j)[0].toFloat() // 假设单通道（灰色）
                byteBuffer.putFloat(临时)
            }
        }
 
        inputFeature.loadBuffer(byteBuffer)
        返回输入特征
    }

在下面的粘贴箱中，我也包含了我的 pythin 代码。我需要一些帮助来弄清楚为什么我的模型无法通过 Kotlin 准确预测，但可以通过 Python 准确预测。
https://pastebin.com/BACzTkq6
以下是在 Python 和 Kotlin 中使用相同图像的差异示例：
通过 Kotlin 显示预测的图像
通过 python 显示预测的图像
我尝试了将 Matrix 转换为 Tensorbuffer 的不同方法，我尝试删除大部分（如果不是全部）图像预处理，我尝试让 python 在 Android studio 中工作（但这并没有成功。）
此外，我也曾多次向其他地方寻求过帮助，但都没有得到任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78199511/why-does-python-produce-a-more-accurate-result-than-kotlin-when-using-tensorflow</guid>
      <pubDate>Thu, 21 Mar 2024 11:18:54 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络可以逆向工程吗？</title>
      <link>https://stackoverflow.com/questions/78197953/can-a-convolutional-neural-network-be-reverse-engineered</link>
      <description><![CDATA[可以对用于目标检测的现代复杂卷积神经网络进行逆向工程来获取原始图像训练数据吗？谢谢。
我确实尝试在线搜索我的问题，但答案并没有完全回答我的问题]]></description>
      <guid>https://stackoverflow.com/questions/78197953/can-a-convolutional-neural-network-be-reverse-engineered</guid>
      <pubDate>Thu, 21 Mar 2024 06:38:32 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 矩阵乘法形状错误：“RuntimeError：mat1 和 mat2 形状无法相乘”</title>
      <link>https://stackoverflow.com/questions/78196998/pytorch-matrix-multiplication-shape-error-runtimeerror-mat1-and-mat2-shapes-c</link>
      <description><![CDATA[我是 PyTorch 的新手，正在创建一个多输出线性回归模型，根据字母为单词着色。 （这将帮助有字素颜色联觉的人更轻松地阅读。）它接收单词并输出 RGB 值。每个单词都表示为 45 个浮点数 [0,1] 的向量，其中 (0, 1] 代表字母，0 代表该位置不存在字母。每个样本的输出应该是一个向量 [r-value, g -值，b-值]。
我懂了
&lt;块引用&gt;
运行时错误：mat1 和 mat2 形状无法相乘（90x1 和 45x3）

当我尝试在训练循环中运行我的模型时。
查看现有的 Stack Overflow 帖子，我认为这意味着我需要重塑我的数据，但我不知道如何/在哪里以解决此问题的方式进行此操作。特别是考虑到我不知道那个 90x1 矩阵来自哪里。
我的模型
我一开始很简单；在我可以让单个层发挥作用之后，可以出现多个层。
类 ColorPredictor(torch.nn.Module):
    #构造函数
    def __init__(自身):
        super(ColorPredictor, self).__init__()
        self.linear = torch.nn.Linear(45, 3, device= device) #编码词向量的长度 &amp; r,g,b 向量的大小
        
    ＃ 预言
    defforward(self, x: torch.Tensor) -&gt;;火炬.张量：
        y_pred = self.线性(x)
        返回 y_pred

我如何加载数据
# 数据集类
数据类（数据集）：
    # 构造函数
    def __init__(自身，输入，输出)：
        self.x = input # 编码词向量列表
        self.y = 输出 # 将 r、g、b 值转换为火炬张量的 Pandas 数据帧
        self.len = len(输入)
    
    # 吸气剂
    def __getitem__(自身，索引)：
        返回 self.x[索引], self.y[索引]
    
    # 获取样本数
    def __len__(自身):
        返回 self.len

# 创建训练/测试分割
train_size = int(0.8 * len(数据))
train_data = 数据(输入[:train_size], 输出[:train_size])
test_data = 数据(输入[train_size:], 输出[train_size:])

# 为训练和测试集创建 DataLoaders
train_loader = DataLoader（数据集= train_data，batch_size = 2）
test_loader = DataLoader（数据集= test_data，batch_size = 2）

发生错误的测试循环
对于范围内的纪元（纪元）：
    ＃ 火车
    model.train() #训练模式
    对于 train_loader 中的 x,y：
        y_pred = model(x) #此处错误
        损失=标准(y_pred, y)
        优化器.zero_grad()
        loss.backward()
        优化器.step()
      

错误回溯


新尝试：
将 45x1 输入张量更改为 2x45 输入张量，第二列全为零。这适用于第一次运行 train_loader 循环，但在第二次运行 train_loader 循环期间，我得到另一个矩阵乘法错误，这次是大小为 90x2 和 45x3 的矩阵。]]></description>
      <guid>https://stackoverflow.com/questions/78196998/pytorch-matrix-multiplication-shape-error-runtimeerror-mat1-and-mat2-shapes-c</guid>
      <pubDate>Thu, 21 Mar 2024 01:00:23 GMT</pubDate>
    </item>
    <item>
      <title>层顺序从未被调用，因此没有定义的输入</title>
      <link>https://stackoverflow.com/questions/78196623/the-layer-sequential-has-never-been-called-and-thus-has-no-defined-input</link>
      <description><![CDATA[我正在 Anaconda 虚拟环境中运行一个简单的脚本
从 deepface 导入 DeepFace

face_analysis = DeepFace.analyze(img_path = “face3.jpeg”)
打印（面部分析）

但我不断收到此错误。
行动：年龄：25%|██████████████████████████▊ | 1/4 [00:02&lt;00:06, 2.08s/it]
回溯（最近一次调用最后一次）：
  文件“C:\Users\Ctrend.pk\Cheer-Check\test2.py”，第 9 行，在  中
    分析 = DeepFace.analyze(img_path)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\DeepFace.py”，第 222 行，在分析中
    返回人口统计分析（
           ^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\modules\demography.py”，第 157 行，位于分析
    表观年龄 = modeling.build_model(“年龄”).predict(img_content)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\modules\modeling.py”，第 57 行，位于构建模型
    model_obj[模型名称] = model()
                            ^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\extendedmodels\Age.py”，第 32 行，位于__在里面__
    self.model = load_model()
                 ^^^^^^^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\deepface\extendedmodels\Age.py”，第 61 行，位于加载模型
    年龄模型=模型（输入=模型.输入，输出=基本模型输出）
                             ^^^^^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\ops\operation.py”，第 228 行，在输入中
    返回 self._get_node_attribute_at_index(0, “input_tensors”, “input”)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^
  文件“C:\Users\Ctrend.pk\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\ops\operation.py”，第 259 行，在 _get_node_attribute_at_index 中
    引发值错误（
ValueError：层equential_1从未被调用，因此没有定义的输入。

Deepface版本：0.0.87
张量流
版本：2.16.1
我认为它获取了年龄，但随后没有继续。我错过了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78196623/the-layer-sequential-has-never-been-called-and-thus-has-no-defined-input</guid>
      <pubDate>Wed, 20 Mar 2024 22:50:10 GMT</pubDate>
    </item>
    <item>
      <title>启动 ML 项目指南 [关闭]</title>
      <link>https://stackoverflow.com/questions/78196528/guide-to-starting-a-ml-project</link>
      <description><![CDATA[我正在致力于创建机器学习模型，学习如何将传入电子邮件分类到文件夹中，主要重点是模型必须自主学习，而无需了解电子邮件习惯，并随着时间的推移在分类方面做得更好。
有关如何启动此项目的任何提示、视频、链接、知识以及如何创建此自主分类的策略？ （我正在使用安然语料库数据集）。
在启动项目时需要帮助]]></description>
      <guid>https://stackoverflow.com/questions/78196528/guide-to-starting-a-ml-project</guid>
      <pubDate>Wed, 20 Mar 2024 22:20:07 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit 学习回归</title>
      <link>https://stackoverflow.com/questions/78189577/using-scikit-learn-regression</link>
      <description><![CDATA[我正在尝试使用如下所示的简单函数来学习 scikit-learn 回归：
导入 matplotlib.pyplot 作为 plt
将 numpy 导入为 np
从 sklearn.ensemble 导入 GradientBoostingRegressor、RandomForestRegressor
从 sklearn.gaussian_process 导入 GaussianProcessRegressor
从 sklearn.gaussian_process.kernels 导入 RBF
从 sklearn.metrics 导入mean_squared_error
从 sklearn.model_selection 导入 train_test_split
从 sklearn.multioutput 导入 MultiOutputRegressor
从 sklearn.neural_network 导入 MLPRegressor
从 sklearn.svm 导入 SVR


def func(x: np.ndarray):
    # x (N, 24), y (N, 2)
    x_mean = np.mean(x, 轴=1)
    a = np.sin(x_mean)
    b = np.cos(x_mean)
    y = np.array([a, b]).T
    返回y


np.随机.种子(0)
x = np.random.rand(1000, 24) * np.pi * 10
y = 函数(x)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

型号=[
    梯度提升回归器（），
    随机森林回归器(),
    高斯过程回归器(RBF()),
    MLPRegressor(max_iter=1000),
    支持向量机（），
]

plt.figure(figsize=(5, len(模型) * 3))

对于 i，枚举（模型）中的 base_model：
    名称 = 基础模型.__class__.__name__
    模型 = MultiOutputRegressor(base_model)
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    mse = 均方误差(y_test, y_pred)

    x_mean = np.mean(x_test, 轴=1)
    plt.subplot(len(模型), 1, i + 1)
    标题 = f&quot;{名称}. MSE: {mse:.2e}”
    打印（标题）
    plt.标题（标题）
    plt.plot(x_mean, y_test[:, 0], “.”, label=“y_test[:, 0]”)
    plt.plot(x_mean, y_test[:, 1], “.”, label=“y_test[:, 1]”)
    plt.plot(x_mean, y_pred[:, 0], “.”, label=“y_pred[:, 0]”)
    plt.plot(x_mean, y_pred[:, 1], “.”, label=“y_pred[:, 1]”)
    plt.图例()

plt.tight_layout()
plt.savefig(“regression_test.png”)

但是结果并不好：

我认为我没有正确使用这些回归器。
我应该如何修改我的代码？
更新
通过减少 x 的暗度，我得到了一些好的结果：
我应该如何更改 GP 的代码以支持高亮度输入？
]]></description>
      <guid>https://stackoverflow.com/questions/78189577/using-scikit-learn-regression</guid>
      <pubDate>Tue, 19 Mar 2024 20:49:54 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 的 Google Colab Bert 实例化错误</title>
      <link>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</link>
      <description><![CDATA[我正在尝试在 Colab 上使用 Tensorflow 构建 Bert 模型。这段代码几周前就可以完美运行。现在，如果我尝试实例化模型，则会收到以下错误：
初始化 TF 2.0 模型 TFBertModel 时未使用 PyTorch 模型的某些权重：[&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls .predictions.transform.LayerNorm.weight&#39;、&#39;cls.predictions.bias&#39;、&#39;cls.seq_relationship.bias&#39;、&#39;cls.predictions.transform.dense.bias&#39;、&#39;cls.seq_relationship.weight&#39;]
- 如果您从在其他任务或其他架构上训练的 PyTorch 模型初始化 TFBertModel（例如，从 BertForPreTraining 模型初始化 TFBertForSequenceClassification 模型），这是预期的。
- 如果您从希望完全相同的 PyTorch 模型初始化 TFBertModel（例如，从 BertForSequenceClassification 模型初始化 TFBertForSequenceClassification 模型），则不会出现这种情况。
TFBertModel 的所有权重都是从 PyTorch 模型初始化的。
如果您的任务与检查点模型训练的任务类似，您就可以使用 TFBertModel 进行预测，而无需进一步训练。
-------------------------------------------------- ------------------------
TypeError Traceback（最近一次调用最后一次）
&lt;ipython-input-14-b0e769ef7​​890&gt;在&lt;细胞系：7&gt;()
      5 SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
      6 SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
----&gt; 7 SC_pooler_output = SC_bert_model(SC_input_layer, Attention_mask=SC_mask_layer)[1] # 第二个输出，che è il pooler_output
      8
      9 # 辍学层的Aggiungi

36帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py 在 type_spec_from_value(value) 中
   1002 3，“无法将 %r 转换为张量：%s” % (类型(值).__name__, e))
   1003
-&gt;第1004章
   第1005章 1005
   1006

TypeError：调用层“嵌入”时遇到异常（类型 TFBertEmbeddings）。

无法为名称构建 TypeSpec：“tf.debugging.assert_less_5/assert_less/Assert/Assert”
op：“断言”
输入：“tf.debugging.assert_less_5/assert_less/All”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_0”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_1”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_2”
输入：“占位符”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_4”
输入：“tf.debugging.assert_less_5/assert_less/y”
属性{
  键：“总结”
  价值 {
    我：3
  }
}
属性{
  键：“T”
  价值 {
    列表 {
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_INT32
      类型：DT_STRING
      类型：DT_INT32
    }
  }
}
 不支持的类型。

调用层“embeddings”接收的参数（类型 TFBertEmbeddings）：
  • input_ids=
  •position_ids=无
  • token_type_ids=
  • input_embeds=无
  •过去的键值长度=0
  • 训练=False

模型的代码是：
SC_input_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“input_ids”)
SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
SC_pooler_output = SC_bert_model（SC_input_layer，attention_mask = SC_mask_layer）[1]

# Dropout 层的Aggiungi
SC_dropout_layer = Dropout(dropout_rate)(SC_pooler_output)
SC_output_layer = 密集（6，激活=&#39;sigmoid&#39;）（SC_dropout_layer）
SC_model = 模型(输入=[SC_input_layer, SC_mask_layer], 输出=SC_output_layer)

我发现安装tensorflow 2.10.0可以工作，但是使用Google Colab时我的CUDA版本有问题，并且使用tensorflow 2.10它无法识别GPU。
该代码几周前就可以工作，有人有解决方案吗？
编辑：Kaggle 上也出现同样的错误。]]></description>
      <guid>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</guid>
      <pubDate>Sun, 17 Mar 2024 17:03:42 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 几何：张量大小存在问题</title>
      <link>https://stackoverflow.com/questions/63610626/pytorch-geometric-having-issues-with-tensor-sizes</link>
      <description><![CDATA[这是我第一次使用Pytorch和Pytorch几何。我正在尝试使用 Pytorch Geometric 创建一个简单的图神经网络。我正在通过遵循 Pytorch Geometric 文档并扩展 InMemoryDataset 创建自定义数据集。之后，我将数据集分为训练数据集、验证数据集和测试数据集，其大小分别为（3496、437、439）。这些是每个数据集中的图表数量。这是我的简单神经网络
类 Net(torch.nn.Module):
def __init__(自身):
    超级（网络，自我）.__init__()
    self.conv1 = GCNConv(dataset.num_node_features, 10)
    self.conv2 = GCNConv(10, dataset.num_classes)

def 转发（自身，数据）：
    x，edge_index，batch = data.x，data.edge_index，data.batch
    x = self.conv1(x, 边缘索引)
    x = F.relu(x)
    x = F.dropout(x, 训练=self.training)
    x = self.conv2(x, 边缘索引)

    返回 F.log_softmax(x, 暗淡=1)

我在训练模型时收到此错误，这表明我的输入尺寸存在一些问题。也许原因在于我的批量大小？
RuntimeError：TorchScript 解释器中的以下操作失败。
TorchScript 的回溯（最近一次调用最后一次）：
文件“E:\Users\abc\Anaconda3\lib\site-packages\torch_scatter\scatter.py”，第 22 行，位于 scatter_add 中
        大小[dim] = int(index.max()) + 1
    out = torch.zeros(大小, dtype=src.dtype, device=src.device)
    返回 out.scatter_add_(dim, 索引, src)
           ~~~~~~~~~~~~~~~~ &lt;--- 这里
别的：
    返回 out.scatter_add_(dim, 索引, src)
运行时错误：索引 13654 超出尺寸 678 的维度 0 的范围

该错误专门发生在神经网络中的这行代码上，
x = self.conv1(x, 边缘索引)

编辑：添加了有关edge_index的更多信息，并更详细地解释了我正在使用的数据。
这是我试图传递的变量的形状
x: torch.Size([678, 43])
边缘索引: torch.Size([2, 668])
torch.max(edge_index): 张量(541690)
torch.min(edge_index): 张量(1920)

我使用的数据列表包含 Data(x=node_features, edge_index=edge_index, y=labels) 对象。当我将数据集拆分为训练、验证和测试数据集时，我分别在每个数据集中获得 (3496, 437, 439) 图。最初，我尝试从数据集中创建一个图表，但我不确定它如何与 Dataloader 和小批量一起使用。
train_loader = DataLoader(train_dataset,batch_size=batch_size)
val_loader = DataLoader(val_dataset,batch_size=batch_size)
test_loader = DataLoader(test_dataset,batch_size=batch_size)

这是从数据帧生成图形的代码。我尝试创建一个简单的图，其中只有一些顶点和一些连接它们的边。我可能忽略了一些事情，这就是我遇到这个问题的原因。创建此图时，我尝试遵循 Pytorch 几何文档（Pytorch几何：创建您自己的数据集)
def 进程（自身）：
        数据列表 = []

        分组 = df.groupby(&#39;EntityId&#39;)
        对于 id，分组中的组：
            node_features = torch.tensor(group.drop([&#39;Labels&#39;], axis=1).values)
            source_nodes = group.index[1:].values
            target_nodes = group.index[:-1].values
            标签 = torch.tensor(group.Labels.values)
            edge_index = torch.tensor([源节点, 目标节点])

            数据=数据（x=节点特征，边缘索引=边缘索引，y=标签）
            data_list.append(数据)

        如果 self.pre_filter 不是 None：
            data_list = [data_list 中的数据 if self.pre_filter(data)]

        如果 self.pre_transform 不是 None：
            data_list = [self.pre_transform(data) for data_list中的数据]

        数据，切片= self.collat​​e（data_list）
        torch.save((数据, 切片), self.processed_pa​​ths[0])

如果有人可以帮助我在任何类型的数据上创建图表并将其与 GCNConv 一起使用，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/63610626/pytorch-geometric-having-issues-with-tensor-sizes</guid>
      <pubDate>Thu, 27 Aug 2020 06:52:23 GMT</pubDate>
    </item>
    </channel>
</rss>