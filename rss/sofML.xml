<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 20 Jun 2024 21:15:34 GMT</lastBuildDate>
    <item>
      <title>具有二元结果预测的单一时间序列</title>
      <link>https://stackoverflow.com/questions/78648701/single-time-series-with-binary-outcome-prediction</link>
      <description><![CDATA[我有全天股票价格的时间序列和与这些价格相关的二元结果变量（行动），即买入/无行动。
目标是预测股票价格和行动变量之间是否存在相关性。
不确定逻辑回归或决策树是否是可行的方法。你认为哪一个更有意义？
考虑到没有太多节点可以使用，决策树是否可行？
我认为随机森林会有点矫枉过正。
仅供参考，我不熟悉 NN。
请就对此类数据建模的最佳方法提供建议。]]></description>
      <guid>https://stackoverflow.com/questions/78648701/single-time-series-with-binary-outcome-prediction</guid>
      <pubDate>Thu, 20 Jun 2024 16:33:58 GMT</pubDate>
    </item>
    <item>
      <title>可从头定制的神经网络</title>
      <link>https://stackoverflow.com/questions/78648642/neural-network-customisable-from-scratch</link>
      <description><![CDATA[我从头开始创建了一个神经网络。仅使用 numpy 库。但损失函数没有收敛，并且准确率不稳定。请帮助我找出问题所在。如果您能给我解决方案的话
这是我的代码
# 计算分类器准确率的函数
def accuracy(output, y):
a = np.argmax(output, axis=0)

return np.mean(a == y)

# 返回初始权重和基准的函数
def initialisation(t):
parameters = {}
np.random.seed(1)
for i in range(len(t) - 1):
parameters[&#39;w&#39; + str(i + 1)] = np.random.randn(t[i + 1], t[i])
parameters[&#39;b&#39; + str(i + 1)] = np.zeros((t[i + 1], 1))
返回参数

def relu_activation(x):
返回 np.maximum(0, x)

def relu_derivative(x):
返回 np.where(x &gt; 0, 1, 0)

def softmax_activation(z):
z = z - np.max(z, axis=0, keepdims=True)
z = np.exp(z)
softmax = z / np.sum(z, axis=0, keepdims=True)
返回 softmax

def sigmoid_activation(x):
返回 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
返回 np.exp(-x) / (1 + np.exp(-x)) ** 2

def derive(x, num):
if num == 1:
return relu_derivative(x)
elif num == 3:
return sigmoid_derivative(x)

#根据数字 n 选择激活函数的函数
def activate(z, n):
if n == 1:
return relu_activation(z)
elif n == 2:
return softmax_activation(z)
elif n == 3:
return sigmoid_activation(z)

#计算损失函数的函数
def loss_function(a_last, y):
output = np.clip(a_last, 1e-7, 1 - 1e-7)
num = y.shape[1]
loss = -np.mean(np.log(output[np.argmax(y, axis=0), np.arange(num)]))
return loss

def Z(x, w, b):
return np.dot(w, x) + b

#实现前向传播的函数
def forward(x, parameters, configuration, t):
cache = {&#39;a0&#39;: x}
for i in range(len(t) - 1):
cache[&#39;z&#39; + str(i + 1)] = Z(cache[&#39;a&#39; + str(i)], parameters[&#39;w&#39; + str(i + 1)], parameters[&#39;b&#39; + str(i + 1)])
cache[&#39;a&#39; + str(i + 1)] = activate(cache[&#39;z&#39; + str(i + 1)], configuration[i])
return cache

#实现反向传播的函数传播
def behind(cache, t, configuration, parameters, y):
m = cache[&#39;a&#39; + str(len(t) - 1)].shape[1]
grad = {&#39;dw&#39; + str(len(t) - 1): np.dot((cache[&#39;a&#39; + str(len(t) - 1)] - y), cache[&#39;a&#39; + str(len(t) - 2)].T) / m,
&#39;db&#39; + str(len(t) - 1): np.sum(cache[&#39;a&#39; + str(len(t) - 1)] - y, axis=1, keepdims=True) / m,
&#39;dz&#39; + str(len(t) - 1): cache[&#39;a&#39; + str(len(t) - 1)] - y}

for i in range(len(t) - 2, 0, -1):
grad[&#39;dz&#39; + str(i)] = np.dot(parameters[&#39;w&#39; + str(i + 1)].T, grad[&#39;dz&#39; + str(i + 1)] * derive(cache[&#39;z&#39; + str(i + 1)], configuration[i - 1]))
grad[&#39;dw&#39; + str(i)] = np.dot(grad[&#39;dz&#39; + str(i)], cache[&#39;a&#39; + str(i - 1)].T) / m
grad[&#39;db&#39; + str(i)] = np.sum(grad[&#39;dz&#39; + str(i)], axis=1, keepdims=True) / m

return grad

# 更新权重和基准的函数
def update(learning_rate, parameters, grad, t):
for i in range(len(t) - 1):
parameters[&#39;w&#39; + str(i + 1)] = np.clip(parameters[&#39;w&#39; + str(i + 1)] - learning_rate * grad[&#39;dw&#39; + str(i + 1)], -10, 10)
parameters[&#39;b&#39; + str(i + 1)] = np.clip(parameters[&#39;b&#39; + str(i + 1)] - learning_rate * grad[&#39;db&#39; + str(i + 1)], -10, 10)
return parameters

#学习函数
def learning(x, y, t, configuration, nb, learning_rate):
param = initialisation(t)
los = []
for i in tqdm.tqdm(range(nb)):
cache = forward(x, param, configuration, t)
grad = behind(cache, t, configuration, param, y)
loss = loss_function(cache[&#39;a&#39; + str(len(t) - 1)], y)
param = update(learning_rate, param, grad, t)
los = np.append(los, loss)
if i % (nb // 20) == 0:
print(f&quot;loss={loss}, accuracy={100 * accuracy(cache[&#39;a&#39; + str(len(t) - 1)], y)}%&quot;)
plt.plot(np.arange(nb), los)
print(np.argmax(cache[&#39;a&#39; + str(len(t) - 1)], axis=0), np.argmax(y, axis=0))
return param

def predict(x, parameters, configuration, t):
cache = forward(x, parameters, configuration, t)
return cache[&#39;a&#39; + str(len(t) - 1)]
import tensorflow as tf
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
#了解训练数据的形状
x_train.shape
x=x_train.reshape(60000,784).T/255.0
y=y_train.reshape(-1,60000)
t=np.array([x.shape[0],128,64,10])
configuration=np.array([1,1,2])
arr_1d=y

arr_2d = np.zeros((10,60000))

#将相应的列索引设置为 1
for i in range(len(arr_1d)):
arr_2d[y[0, i], i] = 1

para=learning(x,arr_2d,t,configuration,2000,0.0001)
]]></description>
      <guid>https://stackoverflow.com/questions/78648642/neural-network-customisable-from-scratch</guid>
      <pubDate>Thu, 20 Jun 2024 16:22:18 GMT</pubDate>
    </item>
    <item>
      <title>如何将 200 万份简历高精度地与 200 个职位匹配？[关闭]</title>
      <link>https://stackoverflow.com/questions/78647918/how-to-match-2-million-resumes-to-200-jobs-with-high-accuracy</link>
      <description><![CDATA[我面临的挑战是将 200 万份简历与 200 个活跃职位空缺进行匹配，目标准确率为 80%。我的目标是简化我们的招聘流程，确保候选人与职位的准确匹配。以下是详细信息：
问题陈述
目标：有效地将大量简历与较少数量的职位描述进行匹配。
目标：
高精度：目标准确率至少为 80%。
可扩展性：处理大量数据。
自动化：最大限度地减少人工干预。
可能的技术和方法
自然语言处理 (NLP)：
文本预处理：清理、标记化和规范化。
特征提取：TF-IDF、Word2Vec、GloVe、BERT 嵌入。
机器学习模型：
监督学习：对标记数据进行训练。
无监督学习：聚类技术。
深度学习模型：
基于 Transformer 的模型：BERT、RoBERTa，用于更好地理解上下文。
相似度测量：
余弦相似度。
高级指标：针对数据定制的自定义指标。
使用的工具
矢量数据库
OpenAI 的 Ada-002 嵌入模型
专业知识请求
有人处理过类似的问题或使用过上述技术吗？任何关于实现我们准确度目标的最佳方法的见解或建议都将不胜感激。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78647918/how-to-match-2-million-resumes-to-200-jobs-with-high-accuracy</guid>
      <pubDate>Thu, 20 Jun 2024 13:56:09 GMT</pubDate>
    </item>
    <item>
      <title>超分辨率 GAN 训练中生成器和鉴别器损失之间的不平衡</title>
      <link>https://stackoverflow.com/questions/78647617/imbalance-between-generator-and-discriminator-losses-in-gan-training-for-super-r</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78647617/imbalance-between-generator-and-discriminator-losses-in-gan-training-for-super-r</guid>
      <pubDate>Thu, 20 Jun 2024 12:57:24 GMT</pubDate>
    </item>
    <item>
      <title>时间序列预测，其中历史值也会由于滞后而更新</title>
      <link>https://stackoverflow.com/questions/78646921/time-series-forecasting-where-historical-values-also-gets-updated-due-to-lag</link>
      <description><![CDATA[我正在对未来 4 周的 covid 病例进行时间序列预测。

传入数据频率：每周
要预测的周数：4

数据的主要问题是，数据滞后约 8 周，这意味着特定周的值将在接下来的 8 周内更新 8 周，同时添加下周的值数据。
特定周（Epiweek 1）的值如下所示：
Load_Date 值
1-Jan-24 为 10 ，
8-Jan-24 为 11 ，
15-Jan-24 为 14 ，
22-Jan-24 为 15 ，
29-Jan-24 为 16 ，
6-Feb-24 为 18 ，
13-Feb-24 为23 ,
20-Feb-24 是 26 ,
27-Feb-24 是 26 ,
6-Mar-24 是 26 ,

在这种情况下：
未修订值为 10
修订值为 26

如您在以上数据中看到的 - 数据在第 9 周稳定下来。同样，我们有其他一周的值。
当我应用 Auto_Arima、Garch 模型时，当我将我的预测与未修订的数据进行比较时，我可以看到良好的结果，但是当我将它们与修订值（在建模时不可用）进行比较时，我看到更多的 MAPE。
考虑到历史值也会更新 7-8 周，我该如何改善结果？]]></description>
      <guid>https://stackoverflow.com/questions/78646921/time-series-forecasting-where-historical-values-also-gets-updated-due-to-lag</guid>
      <pubDate>Thu, 20 Jun 2024 10:31:39 GMT</pubDate>
    </item>
    <item>
      <title>Mobilenet 与 resnet [关闭]</title>
      <link>https://stackoverflow.com/questions/78646834/mobilenet-vs-resnet</link>
      <description><![CDATA[Q1-为什么我们不像在 mobile-net v2 中那样在 resnet50 中添加 skip connection 后移除 relu 以获得更好的性能？
Q2-为什么我们没有在 skip connection 中使用卷积层来匹配 mobile-net v2 中层尺寸变化时的维度，就像我们在 resnet 中那样，当层尺寸变化时匹配输出通道？
我尝试在 web 和 chatgpt 上搜索，但答案并不令人满意。它们都像“架构是以那种方式提出的”。]]></description>
      <guid>https://stackoverflow.com/questions/78646834/mobilenet-vs-resnet</guid>
      <pubDate>Thu, 20 Jun 2024 10:13:24 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 - 年龄和性别检测 [关闭]</title>
      <link>https://stackoverflow.com/questions/78646157/machine-learning-age-and-gender-detection</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78646157/machine-learning-age-and-gender-detection</guid>
      <pubDate>Thu, 20 Jun 2024 07:54:19 GMT</pubDate>
    </item>
    <item>
      <title>所有时期的损失和准确率相同</title>
      <link>https://stackoverflow.com/questions/78645720/same-loss-and-accuracy-for-all-epochs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78645720/same-loss-and-accuracy-for-all-epochs</guid>
      <pubDate>Thu, 20 Jun 2024 06:01:17 GMT</pubDate>
    </item>
    <item>
      <title>对训练数据集使用决策树模型后仅生成一个节点</title>
      <link>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</link>
      <description><![CDATA[1我正在尝试构建一个决策树模型，该模型基于预测变量预测结果变量（名为：结果）。实际上，我已经对一些&quot;&gt;2 级&quot;变量应用了独热编码，以便稍微扩展预测变量的 n [我的数据]。
我首先探索了数据，然后将其拆分为 80/20 拆分并运行模型，但在训练数据集上运行的模型最终只有一个节点，没有分支。查看类似的帖子，我发现我的数据不平衡，因为通过检查类分配的 prop.table（结果变量），大多数是负面的，而不是正面的。关于在此数据上创建正确树的任何建议
这是我的代码：
将数据拆分为测试和训练数据（80％训练和20％测试数据）
set.seed(1234)
pd &lt;- sample(2, nrow(data_hum_mod), replace = TRUE, prob = c(0.8,0.2))
data_hum_train &lt;- data_hum_mod[pd==1,]
data_hum_test&lt;- data_hum_mod[pd==2,]

拆分后的数据探索
检查数据维度
dim(data_hum_train); dim(data_hum_test)
#确保分离后的数据在每个结果类别（即阳性/阴性 toxo）中的 n 值是平衡的
prop.table(table(data_hum_train$Results)) * 100
prop.table(table(data_hum_test$Results)) *100

这给出了以下结果：
(训练)
阴性 阳性 
75.75758 24.24242

和
(测试)
阴性 阳性 
54.54545 45.45455

检查缺失值
anyNA(data_hum_mod)
#确保没有任何变量为零或接近零方差。
nzv(data_hum_mod)
构建模型（使用 party 包）
install.packages(&#39;party&#39;)
library(party)

data_human_train_tree&lt;- ctree(Results ~., data = data_hum_train,
controls = ctree_control(mincriterion = 0.1))
data_human_train_tree
plot(data_human_train_tree)

使用此代码，我获得了此图
使用其他包（如 C50 和 rpart）也得到了相同的结果
您能对此提出建议吗？我读到了关于多数类的子采样（这里是负面结果），如何在 R 中实现这一点？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</guid>
      <pubDate>Thu, 20 Jun 2024 01:04:59 GMT</pubDate>
    </item>
    <item>
      <title>Excel 中的逻辑回归</title>
      <link>https://stackoverflow.com/questions/78644668/logistic-regression-in-excel</link>
      <description><![CDATA[我有两个优化模型：
LR-P1：

LR-P2：

我期望两个模型都得到相同的最优值，但我无法计算模型 LR-P1。我正在进行所有计算，但 excel 求解器无法找到最优值。当我将所有系数设为 0.1 时，求解器只会说找到了最佳值，但不会更改决策变量。
我的问题是，我正在进行所有计算，但 excel 给出了 NUM 错误，而模型 LR-P2 没有。这是因为目标函数对于 LR-P1 来说太小，以至于 excel 求解器无法对其进行交换，从而导致数值问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78644668/logistic-regression-in-excel</guid>
      <pubDate>Wed, 19 Jun 2024 21:13:58 GMT</pubDate>
    </item>
    <item>
      <title>将图像置于中心并在导出时添加背景</title>
      <link>https://stackoverflow.com/questions/78581619/center-an-image-and-adding-a-background-at-export</link>
      <description><![CDATA[我想自动完成所有这些操作：

选择图像中的对象
在此对象上裁剪我的图像
裁剪为 1:1 的宽高比，在此对象周围留出一点空隙
以 800x800px 的 JPG 格式导出我的图像，我的对象位于图像中心，背景为白色。

我在 win11 64 位上
我做了什么：

安装 Python 并创建环境
安装opencv-python-headless、pillow、numpy、Pytorch以用于 CUDA 11.8
克隆存储库 segment-anything.git 并使用 PIP 安装它
下载sam_vit_b_01ec64.pth

像这样对 py 文件进行编码：
import os
import cv2
import numpy as np
from PIL import Image
from fragment_anything import sam_model_registry, SamAutomaticMaskGenerator

def load_image(image_path):
return cv2.imread(image_path)

def save_image(image, path):
cv2.imwrite(path + &#39;.jpg&#39;, image)

def select_object(image):
sam = sam_model_registry[&quot;vit_b&quot;](checkpoint=&quot;sam_vit_b_01ec64.pth&quot;)
mask_generator = SamAutomaticMaskGenerator(sam)
mask = mask_generator.generate(image)
largest_mask = max(masks, key=lambda x: x[&#39;area&#39;])
返回 largest_mask[&#39;segmentation&#39;]

def crop_to_object(image, mask):
x, y, w, h = cv2.boundingRect(mask.astype(np.uint8))
padding = 5
x = max(0, x - padding)
y = max(0, y - padding)
w = min(image.shape[1] - x, w + 2 * padding)
h = min(image.shape[0] - y, h + 2 * padding)

cropped_image = image[y:y+h, x:x+w]
返回 cropped_image

def resize_to_square(image, size=800):
h, w = image.shape[:2]
scale = size / max(h, w)
new_h, new_w = int(h * scale), int(w * scale)
resized_image = cv2.resize(image, (new_w, new_h), 插值=cv2.INTER_LANCZOS4)

new_image = np.ones((size, size, 3), dtype=np.uint8) * 255

top = (size - new_h) // 2
left = (size - new_w) // 2
bottom = top + new_h
right = left + new_w

new_image[top:top+new_h, left:left+new_w] = resized_image

return new_image

def process_image(image_path, output_path):

image = load_image(image_path)
mask = select_object(image)
cropped_image = crop_to_object(image, mask)
final_image = resize_to_square(cropped_image, 800)
save_image(final_image, output_path + &#39;.jpg&#39;)

def process_folder(input_folder, output_folder):

如果 os.path.exists(output_folder):
os.makedirs(output_folder)

对于 root, _, files in os.walk(input_folder):
对于 filename in files:
如果 filename.lower().endswith((&#39;.png&#39;, &#39;.jpg&#39;, &#39;.jpeg&#39;, &#39;.bmp&#39;, &#39;.tiff&#39;)):
input_path = os.path.join(root, filename)

relative_path = os.path.relpath(input_path, input_folder)
output_path = os.path.join(output_folder,relative_path)

output_dir = os.path.dirname(output_path)
如果 os.path.exists(output_dir):
os.makedirs(output_dir)

尝试:
process_image(input_path, output_path)
print(f&quot;已处理 {input_path}&quot;)
except Exception as e:
print(f&quot;无法处理 {input_path}：{e}&quot;)

if __name__ == &quot;__main__&quot;:
input_folder = &quot;&quot;
output_folder = &quot;&quot;
process_folder(input_folder, output_folder)

发生了什么：
我导入了基本图像，我想要预期结果，并且我获得了结果
我得到了一些不同的基本结果：

基本白色背景 -&gt; 结果
Base-nobg -&gt; 结果

有人能帮我理解我错过了什么吗？
提前谢谢，
Cyril]]></description>
      <guid>https://stackoverflow.com/questions/78581619/center-an-image-and-adding-a-background-at-export</guid>
      <pubDate>Wed, 05 Jun 2024 14:13:43 GMT</pubDate>
    </item>
    <item>
      <title>是否可以训练神经网络来输入随机森林分类器或任何其他类型的分类器（如 XGBoost 或决策树）？</title>
      <link>https://stackoverflow.com/questions/78461828/is-it-possible-to-train-a-neural-network-to-feed-into-a-random-forest-classifier</link>
      <description><![CDATA[我想创建一个模型架构来预测未来的股价走势，如下所示：

该模型的目标是预测未来 3 个月内价格是上涨还是下跌。
我尝试过一些模型，例如 Logistic 回归、神经网络、XGBoost 等。我得到了一些不错的结果。通过使用随机森林分类器，我得到了迄今为​​止最好的结果。我如何使用神经网络对数据进行编码，然后将这些值传递给随机森林分类器进行分类，而不是使用如图所示的使用 S 型函数的最终输出层（使用 Python、Keras 和 SKlearn）。
我对 Keras 不是很熟悉，所以我想知道是否有可能训练一个输入到单独分类器的神经网络，如果可以，该怎么做。]]></description>
      <guid>https://stackoverflow.com/questions/78461828/is-it-possible-to-train-a-neural-network-to-feed-into-a-random-forest-classifier</guid>
      <pubDate>Fri, 10 May 2024 17:53:36 GMT</pubDate>
    </item>
    <item>
      <title>如果数据只有一个特征，则使用 array.reshape(-1, 1) 重塑数据；如果数据只有一个样本，则使用 array.reshape(1, -1)</title>
      <link>https://stackoverflow.com/questions/58663739/reshape-your-data-either-using-array-reshape-1-1-if-your-data-has-a-single-fe</link>
      <description><![CDATA[当我从我的数据中预测一个样本时，它给出了重塑错误，但我的模型具有相同的行数。这是我的代码：
import pandas as pd
from sklearn.linear_model import LinearRegression
import numpy as np
x = np.array([2.0 , 2.4, 1.5, 3.5, 3.5, 3.5, 3.5, 3.7, 3.7])
y = np.array([196, 221, 136, 255, 244, 230, 232, 255, 267])

lr = LinearRegression()
lr.fit(x,y)

print(lr.predict(2.4))

错误是
如果它包含单个样本。格式（数组））
ValueError：预期为 2D 数组，但得到的是标量数组：
array=2.4。
如果您的数据只有一个特征，则使用 array.reshape(-1, 1) 重塑数据；如果它包含一个样本，则使用 array.reshape(1, -1)。
]]></description>
      <guid>https://stackoverflow.com/questions/58663739/reshape-your-data-either-using-array-reshape-1-1-if-your-data-has-a-single-fe</guid>
      <pubDate>Fri, 01 Nov 2019 17:56:16 GMT</pubDate>
    </item>
    <item>
      <title>如何在 pyspark 上创建分层分割训练、验证和测试集？</title>
      <link>https://stackoverflow.com/questions/58014693/how-to-create-stratified-split-training-validation-and-test-set-on-pyspark</link>
      <description><![CDATA[我有一个小数据集（140K），我想将其分成验证集、验证集测试集，使用目标变量和另一个字段来限制这些分割。]]></description>
      <guid>https://stackoverflow.com/questions/58014693/how-to-create-stratified-split-training-validation-and-test-set-on-pyspark</guid>
      <pubDate>Thu, 19 Sep 2019 15:45:28 GMT</pubDate>
    </item>
    <item>
      <title>Python 中更快的 kNN 分类算法</title>
      <link>https://stackoverflow.com/questions/51688568/faster-knn-classification-algorithm-in-python</link>
      <description><![CDATA[我想从头开始编写自己的 kNN 算法，原因是我需要对特征进行加权。问题是，尽管删除了 for 循环并使用了内置的 numpy 功能，我的程序仍然很慢。
有人能建议一种加快速度的方法吗？我没有使用 np.sqrt 来计算 L2 距离，因为它没有必要，而且实际上会减慢整个过程。
class GlobalWeightedKNN:
&quot;&quot;&quot;
具有特征权重的 k-NN 分类器

返回：k-NN 的预测。
&quot;&quot;&quot;

def __init__(self):
self.X_train = None
self.y_train = None
self.k = None
self.weights = None
self.predictions = list()

def fit(self, X_train, y_train, k, weights): 
self.X_train = X_train
self.y_train = y_train
self.k = k
self.weights = weights

def predict(self, testing_data):
&quot;&quot;&quot;
获取查询案例的 2d 数组。

返回 k-NN 分类器的预测列表
&quot;&quot;&quot;

np.fromiter((self.__helper(qc) for qc in testing_data), float) 
return self.predictions

def __helper(self, qc):
neighbours = np.fromiter((self.__weighted_euclidean(qc, x) for x in self.X_train), float)
neighbours = np.array([neighbours]).T 
indexes = np.array([range(len(self.X_train))]).T
neighbours = np.append(indexes, neighbours, axis=1)

# 按第二列排序 - 距离
neighbours = neighbours[neighbours[:,1].argsort()] 
k_cases = neighbours[ :self.k]
indexes = [x[0] for x in k_cases]

y_answers = [self.y_train[int(x)] for x in indexes]
answer = max(set(y_answers), key=y_answers.count) # 获取最常见的值
self.predictions.append(answer)

def __weighted_euclidean(self, qc, other):
&quot;&quot;&quot;
自定义加权欧几里得距离

返回：浮点数
&quot;&quot;&quot;

return np.sum( ((qc - other)**2) * self.weights )
]]></description>
      <guid>https://stackoverflow.com/questions/51688568/faster-knn-classification-algorithm-in-python</guid>
      <pubDate>Sat, 04 Aug 2018 18:39:24 GMT</pubDate>
    </item>
    </channel>
</rss>