<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 23 Apr 2024 12:26:22 GMT</lastBuildDate>
    <item>
      <title>在 Pytorch 中冻结现有模型的层</title>
      <link>https://stackoverflow.com/questions/78372187/freeze-layers-of-existing-model-in-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78372187/freeze-layers-of-existing-model-in-pytorch</guid>
      <pubDate>Tue, 23 Apr 2024 12:00:09 GMT</pubDate>
    </item>
    <item>
      <title>修复从头构建的 LSTM 模型的训练函数</title>
      <link>https://stackoverflow.com/questions/78371853/fixing-the-training-function-for-an-lstm-model-built-from-scratch</link>
      <description><![CDATA[所以，我目前正在尝试构建一个定制的 LSTM 模型。为此，我尝试首先从头开始构建自定义 LSTM，然后慢慢修改它。然而，现在我很难让模型学习。
这是我的 LSTM 函数的代码。
类 LSTM_Model(nn.Module):
    def __init__(自身、input_sz、hidden_​​sz、out_sz、bs)：
        超级().__init__()
        self.input_sz = input_sz
        self.hidden_​​size = 隐藏_sz
        self.W = nn.Parameter(torch.Tensor(input_sz,hidden_​​sz * 4))
        self.U = nn.Parameter(torch.Tensor(hidden_​​sz,hidden_​​sz * 4))
        self.bias = nn.Parameter(torch.Tensor(hidden_​​sz * 4))
        self.fc1 = nn.Linear(hidden_​​sz, out_sz)
        self.init_weights()

    def init_weights(自身):
        stdv = 1.0 / math.sqrt(self.hidden_​​size)
        对于 self.parameters() 中的权重：
            重量.data.uniform_(-stdv, stdv)

    defforward(self, x, init_states=None):
        bs, seq_sz, _ = x.size()
        隐藏序列 = []
        如果 init_states 为 None：
            h_t, c_t = (torch.zeros(bs, self.hidden_​​size).to(x.device),
                        torch.zeros(bs, self.hidden_​​size).to(x.device))
        别的：
            h_t, c_t = 初始化状态

        对于范围内的 t(seq_sz)：
            x_t = x[:, t, :]
            如果 x_t.size()[0] != bs:
                fill_in = torch.zeros(bs-x_t[0], seq_sz)
                x_t - torch.cat((x_t, fill_in), 0)

            门 = torch.matmul(x_t, self.W) + torch.matmul(h_t, self.U) + self.bias
            i_t, f_t, g_t, o_t = Gates.chunk(4, 1)
            i_t, f_t, g_t, o_t = torch.sigmoid(i_t), torch.sigmoid(f_t), torch.tanh(g_t), torch.sigmoid(i_t)
            c_t = f_t * c_t + i_t * g_t
            h_t = o_t * torch.tanh(c_t)
            hide_seq.append(h_t)
        输出 = self.fc1(h_t)
        返回，(h_t, c_t)

这是我的训练函数。
def train_model(data_loader、模型、loss_function、优化器)：
    num_batches = len(data_loader)
    总损失= 0
    模型.train()
    隐藏 = 无
    #torch.autograd.set_detect_anomaly(True)
    对于 data_loader 中的 X、y：
        输出，隐藏 = 模型（X，隐藏）
        损失=损失函数（输出，y）

        优化器.zero_grad()
        loss.backward()
        #loss.backward(retain_graph=True)
        优化器.step()

        总损失 += loss.item()

    avg_loss = 总损失 / num_batches
    print(f&quot;列车损失：{avg_loss}&quot;)
    返回平均损失

运行它会收到一条初始错误消息，这要求我放置retain_graph(True)，但之后我收到更多错误。 （还要澄清数据是按这个顺序[批次、序列、特征]）
我还注意到，删除训练函数的隐藏变量，程序会给出输出，但模型根本不学习，可能是因为它抛弃了我认为的隐藏状态。
我尝试做一些研究，但一切似乎都进入了死胡同。从训练函数中删除隐藏状态部分会使程序运行，但它不再学习任何内容并像静态线一样输出（我正在根据股票市场数据训练我的模型）。然而，解决这些问题会让我遇到各种不同的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78371853/fixing-the-training-function-for-an-lstm-model-built-from-scratch</guid>
      <pubDate>Tue, 23 Apr 2024 11:07:26 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用 Twitter API？</title>
      <link>https://stackoverflow.com/questions/78371731/how-can-i-twitter-api</link>
      <description><![CDATA[从 textblob 导入 TextBlob

导入tweepy

访问令牌=&#39;####&#39;
access_token_secret=&#39;####&#39;

消费者密钥 = &#39;####&#39;
Consumer_key_secret = &#39;####&#39;


bearer_token = &#39;####&#39;

# 使用不记名令牌身份验证进行身份验证
auth = tweepy.Client(bearer_token,consumer_key,consumer_key_secret,Acess_token,Acess_token_secret)

# 创建一个 Tweepy v2 客户端
客户端 = tweepy.Client(auth)

# 执行最近的推文搜索
查询=&#39;新闻&#39;
推文 = client.search_recent_tweets(query=query, max_results=10)

# 打印每条推文的文本
对于 tweets.data 中的推文：
    打印（推文.文本）

我正在尝试访问 Twitter API，但它一直显示此错误
tweepy.errors.Unauthorized: 401 未经授权
未经授权
我已经尝试了 Oauthhandler 等所有方法，但仍然是同样的事情？
谁能帮我解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78371731/how-can-i-twitter-api</guid>
      <pubDate>Tue, 23 Apr 2024 10:47:02 GMT</pubDate>
    </item>
    <item>
      <title>类型错误：L2.__init__() 得到意外的关键字参数“l”</title>
      <link>https://stackoverflow.com/questions/78371566/typeerror-l2-init-got-an-unexpected-keyword-argument-l</link>
      <description><![CDATA[&lt;前&gt;&lt;代码&gt;
img_size = (224, 224)
频道 = 3
img_shape = (img_size[0], img_size[1], 通道)
class_count = len(list(train_gen.class_indices.keys()))
base_model = tf.keras.applications.efficientnet.EfficientNetB7(include_top=False,weights=“imagenet”,input_shape=img_shape,pooling=&#39;max&#39;)
base_model.trainable = False
模型=顺序（[
基本模型，
BatchNormalization（轴=-1，动量=0.99，epsilon=0.001），
密集（128，kernel_regularizer=regularizers.l2（l=0.016），activity_regularizer=regularizers.l1（0.006），
bias_regularizer=regularizers.l1(0.006), 激活=&#39;relu&#39;),
辍学率（率=0.45，种子=123），
密集（class_count，激活=&#39;softmax&#39;）
])
model.compile(Adamax(learning_rate=0.002),loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
模型.summary()
&lt;前&gt;&lt;代码&gt;

-------------------------------------------------- ------------------------
TypeError Traceback（最近一次调用最后一次）
[70] 中的单元格，第 16 行
      9 base_model = tf.keras.applications.efficientnet.EfficientNetB7(include_top=False,weights=“imagenet”,input_shape=img_shape,pooling=&#39;max&#39;)
     11 基础模型.trainable = False
     13 模型 = 顺序（[
     14 基本模型，
     15 BatchNormalization（轴=-1，动量=0.99，epsilon=0.001），
---&gt; 16 密集（128，kernel_regularizer=regularizers.l2（l=0.016），activity_regularizer=regularizers.l1（0.006），
     17bias_regularizer=regularizers.l1(0.006), 激活=&#39;relu&#39;),
     18 Dropout（比率=0.45，种子=123），
     19 密集（class_count，激活=&#39;softmax&#39;）
     20]）
     22 model.compile(Adamax(learning_rate=0.002),loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
     25 模型.summary()

类型错误：L2.__init__() 得到意外的关键字参数“l”
]]></description>
      <guid>https://stackoverflow.com/questions/78371566/typeerror-l2-init-got-an-unexpected-keyword-argument-l</guid>
      <pubDate>Tue, 23 Apr 2024 10:17:26 GMT</pubDate>
    </item>
    <item>
      <title>测试二元分割模型并计算 IoU 分数</title>
      <link>https://stackoverflow.com/questions/78371428/testing-binary-segmentation-model-and-calculating-iou-score</link>
      <description><![CDATA[我是机器学习编码新手，正在使用 Pytorch 训练二进制分割模型。我有大约 120 张测试图像。我已经加载了训练好的模型。谁能帮我看看我的测试和计算 IoU 分数代码是否正确？
&lt;前&gt;&lt;代码&gt;batch_size = 8
test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)
# 加载保存的模型
模型 = EncoderDecoder(in_channels=1, out_channels=1)
model.load_state_dict(torch.load(&#39;best_model.pth&#39;))
model.eval() # 设置模型为评估模型

# 定义列表来存储预测掩模和地面真实标签
预测掩码 = []
ground_truth_labels = []
原始图像 = []

# 迭代测试数据集
对于输入，test_loader 中的标签：
    # 将数据移至GPU
    输入，标签=输入.to（设备），标签.to（设备）
    原始图像.append（输入）

    # 前向传递
    使用 torch.no_grad()：
        输出 = 模型（输入）

    # 使用阈值将输出转换为二进制掩码
    Predicted_masks.append((outputs &gt; 0.5).float()) # 直接应用阈值并保持为浮点张量
    
    # 直接将标签附加为张量
    ground_truth_labels.append(标签)

# 连接真实标签
ground_truth_labels = torch.cat(ground_truth_labels, 暗淡=0)

# 连接原始图像
原始图像= torch.cat（原始图像，暗淡= 0）

# 将预测的掩模堆叠到单个张量中
Predicted_masks = torch.cat(predicted_masks, 暗淡=0)

torch.Size([120, 1, 650, 1250]) #predicted_masks 的形状

///计算IoU分数
def iou_pytorch(输入：torch.Tensor，目标：torch.Tensor，平滑：float = 1e-6):
    输入=输入.连续().视图(-1)
    目标 = 目标.view(-1)

    交集 = (输入 * 目标).sum().float()
    总计 = (输入 + 目标).sum().float()
    并集 = 总计 - 交集

    iou =（交集+平滑）/（并集+平滑）
    
    返回 iou.item()
]]></description>
      <guid>https://stackoverflow.com/questions/78371428/testing-binary-segmentation-model-and-calculating-iou-score</guid>
      <pubDate>Tue, 23 Apr 2024 09:56:48 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 TimeSeriesSplit 处理面板数据？</title>
      <link>https://stackoverflow.com/questions/78370489/how-to-use-timeseriessplit-for-panel-data</link>
      <description><![CDATA[我一直在尝试使用TimeSeriesSplit对于面板数据。我所说的面板数据是指人口的年度图片。我对多年来的数据分割很感兴趣。该人口正在不断变化，每年的人口规模并不相同。因此，直接使用 TimeSeriesSplit 是不可能的。
基本上我试图获得以下简历方案：

我设法使用以下代码来做到这一点：
将 pandas 导入为 pd，将 numpy 导入为 np
将seaborn导入为sns，将matplotlib.pyplot导入为plt

从 sklearn.datasets 导入 make_regression
从 sklearn.dummy 导入 DummyRegressor
从 sklearn.metrics 导入mean_squared_error
从 sklearn.model_selection 导入 TimeSeriesSplit

X_测试，y_测试 = []，[]

开始年份 = 2010
年底 = 2020 年

对于 np.arange(start_year, end_year+1) 中的年份：
    X_year, y_year = make_regression(n_samples=5+year-start_year, n_features=2, 偏差=100, 噪声=1, random_state=year)
    X_year = pd.DataFrame(X_year).rename(columns={0:&#39;X1&#39;, 1:&#39;X2&#39;})
    X_year[&#39;年份&#39;] = 年
    y_year = pd.Series(y_year)
    X_test.append(X_year)
    y_test.append(y_year)
    
X_test, y_test = pd.concat(X_test), pd.concat(y_test)

# 建模

X = X_测试
y = y_测试
年 = np.unique(X_test[&#39;year&#39;])

# 建模
模型= DummyRegressor（策略=“平均值”）
指标=均方误差
cv = TimeSeriesSplit(n_splits=len(年)-1)

年数=[]
分辨率=[]

对于 i，枚举（cv.split（years））中的（train_year，test_year）：
    
    print(f&quot;折叠 {i}:&quot;)
    print(f&quot;火车：索引={years[train_year]}&quot;)
    print(f&quot;测试：index={years[test_year]}&quot;)
    
    years_folds.append((years[train_year],years[test_year]))
    
    train_filter = X[&#39;year&#39;].isin(years[train_year])
    test_filter = X[&#39;year&#39;].isin(years[train_year])
    
    X_train, y_train = X.loc[train_filter.values], y[train_filter.values]
    X_test, y_test = X.loc[test_filter.values], y[test_filter.values]
    
    model.fit(X_train, y_train)
    分数 = 指标(model.predict(X_test), y_test)
    print(f&#39; {score=:.3}&#39;)
    res.append((年份[test_year][0], 分数))

情节_年份_折叠（年份_折叠）
    
Folds_res = pd.DataFrame(res,columns=[&#39;test_year&#39;, metric.__name__])
Folds_res.plot.scatter(x=&#39;test_year&#39;, y=metric.__name__, title=f&#39;{metric.__name__} over test_year&#39;);

注意：我使用虚拟数据集和虚拟模型是为了提供运行示例。这不是我帖子的主题。
正如您所注意到的，我必须使用一个技巧：我分割年份而不是数据。我想知道：是否有一种标准方法可以使用 sklearn cv 对象分割数据？目标是在 cross_val_score 函数中使用它。]]></description>
      <guid>https://stackoverflow.com/questions/78370489/how-to-use-timeseriessplit-for-panel-data</guid>
      <pubDate>Tue, 23 Apr 2024 07:14:30 GMT</pubDate>
    </item>
    <item>
      <title>基于兴趣的匹配数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/78370399/interest-based-matching-dataset</link>
      <description><![CDATA[寻求资源以使用机器学习构建基于兴趣的匹配约会应用程序。在哪里可以找到这方面的数据集、教程和代码示例？任何指导表示赞赏！需要数据集、教程视频和代码方面的帮助。呃]]></description>
      <guid>https://stackoverflow.com/questions/78370399/interest-based-matching-dataset</guid>
      <pubDate>Tue, 23 Apr 2024 07:00:04 GMT</pubDate>
    </item>
    <item>
      <title>Keras 模型加载问题</title>
      <link>https://stackoverflow.com/questions/78370261/keras-model-loading-issue</link>
      <description><![CDATA[我已经下载了一个模型 .keras 文件，当尝试加载它时，我收到此错误，
ValueError：无法识别的关键字参数传递给 ConvLSTM2D：
{&#39;batch_input_shape&#39;: [无, 5, 240, 240, 3], &#39;time_major&#39;: False}

这是我的代码，因为在添加 custom_objects 之前它给出了一些其他错误，我询问了聊天 gpt 并向其中添加了一些错误。
from keras.models import load_model
从 keras.layers 导入 LeakyReLU
从 keras.initializers 导入正交
从 keras.layers 导入 ConvLSTM2D
从 keras.models 导入顺序

custom_objects = {&#39;LeakyReLU&#39;：LeakyReLU，&#39;正交&#39;：正交，&#39;ConvLSTM2D&#39;：ConvLSTM2D}

模型 = load_model(&#39;./Models/CNN_Model.keras&#39;, custom_objects=custom_objects)

我需要加载这个模型，现在就足够了。]]></description>
      <guid>https://stackoverflow.com/questions/78370261/keras-model-loading-issue</guid>
      <pubDate>Tue, 23 Apr 2024 06:30:59 GMT</pubDate>
    </item>
    <item>
      <title>学生分数所需的数据申请/建议[关闭]</title>
      <link>https://stackoverflow.com/questions/78370196/student-marks-data-application-suggestions-required</link>
      <description><![CDATA[我有学生分数和人口统计数据，我想在实用程序应用程序中使用它。到目前为止，我已经提出了“标记预测”应用程序，它使用线性回归模型来预测标记。我想制作一个实用程序，学生可以在其中进行交互，提供他们的分数历史/人口统计数据以预测未来课程的分数等。
这些是我的一些想法，我愿意接受有关此类数据的更多想法/应用的建议。还请推荐工具。我可以在这里使用预制的法学硕士来进行fintune吗？还有其他工具吗？GPT？我的数据在CSV文件中以字符串和整数的形式存在。]]></description>
      <guid>https://stackoverflow.com/questions/78370196/student-marks-data-application-suggestions-required</guid>
      <pubDate>Tue, 23 Apr 2024 06:15:17 GMT</pubDate>
    </item>
    <item>
      <title>张量流联合错误模块“tensorflow_federated.python.learning”没有属性“build_federated_averaging_process”</title>
      <link>https://stackoverflow.com/questions/78370048/tensorflow-federated-error-module-tensorflow-federated-python-learning-has-no</link>
      <description><![CDATA[我有一个机器学习代码
将 pandas 导入为 pd
将 numpy 导入为 np
从 sklearn.datasets 导入 load_iris
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler

将张量流导入为 tf
将tensorflow_federated导入为tff

虹膜 = load_iris()
df = pd.DataFrame(iris.data,列=iris.feature_names)
df[&#39;物种&#39;]=iris.target

# 将数据帧拆分为输入特征和目标变量

x = df.drop(&#39;物种&#39;,axis=1)
y = df[&#39;物种&#39;]
# 创建客户端数据集的函数（假设数据已预先分区）
def create_tf_dataset(client_data):
  “”“”根据提供的客户端数据（特征、标签）创建 tf.data.Dataset。“”“”
  特征，标签= client_data
  返回 tf.data.Dataset.from_tensor_slices((特征，标签))

# 将数据拆分为clientdatasets（模拟数据分区）
客户端数据集 = []
客户数量 = 5
对于范围内的 i（num_clients）：
  start_index = int(i * (len(x) / num_clients))
  end_index = int((i + 1) * (len(x) / num_clients))
  client_features = x[开始索引:结束索引]
  client_labels = y[开始索引:结束索引]
  client_datasets.append(create_tf_dataset((client_features, client_labels)))
  # 定义模型架构（替换为您想要的模型复杂度）
def model_fn(输入):
   features, _ = input # 我们只使用特征进行分类
   密集1=tf.keras.layers.Dense（10，激活=&#39;relu&#39;）（特征）
   稠密2 = tf.keras.layers.Dense(3,activation=&#39;softmax&#39;)(dense1) # 3个鸢尾花类的3个单元
   返回 tf.keras.Model(输入=特征，输出=dense2)

# 定义客户端优化器
client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)

# 定义服务器优化器（用于服务器端聚合）
server_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
fed_learning_model = tff.learning.build_federated_averaging_process(
     模型_fn，
     client_optimizer_fn=client_optimizer,
     server_optimizer_fn=服务器优化器）

但我一直收到此错误。
------------------------------------------------ ----------------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
&lt;ipython-input-13-e5966e29fc79&gt;在&lt;细胞系：1&gt;()
----&gt; 1 fed_learning_model = tff.learning.build_federated_averaging_process(
      2 模型_fn，
      3 client_optimizer_fn=client_optimizer,
      4 server_optimizer_fn=server_optimizer)

AttributeError：模块“tensorflow_federated.python.learning”没有属性“build_federated_averaging_process”

我不知道我的版本是否适合。我的tensorflow联合版本是0.76.0
我的tensorflow版本是2.14.1，python版本是3.10.12
当我在互联网上搜索时，我发现此代码不支持 tensorflow 版本 0.21.0 及以上版本。但我不知道在最新版本中使用什么]]></description>
      <guid>https://stackoverflow.com/questions/78370048/tensorflow-federated-error-module-tensorflow-federated-python-learning-has-no</guid>
      <pubDate>Tue, 23 Apr 2024 05:30:55 GMT</pubDate>
    </item>
    <item>
      <title>如何修改 Adam 优化器以在计算中不包含零？</title>
      <link>https://stackoverflow.com/questions/78369654/how-to-modify-the-adam-optimizer-to-not-include-zeros-in-the-calculations</link>
      <description><![CDATA[我在这个SO问题中找到了Adam的实现：
类 ADAMOptimizer(torch.optim.Optimizer):
    ”“”
    作为前面的步骤，实现 ADAM 算法。
    ”“”
    def __init__(自身，参数，lr=1e-3，betas=(0.9，0.999)，eps=1e-8，weight_decay=0)：
        默认值 = dict(lr=lr, betas=betas, eps=eps,weight_decay=weight_decay)
        super(ADAMOptimizer, self).__init__(参数, 默认值)

    def 步骤（自身）：
        ”“”
        执行单个优化步骤。
        ”“”
        损失=无
        对于 self.param_groups 中的组：

            对于组 [&#39;params&#39;] 中的 p：
                grad = p.grad.data
                状态 = self.state[p]

                # 状态初始化
                如果 len(状态) == 0:
                    状态[&#39;步骤&#39;] = 0
                    # 动量（梯度的指数 MA）
                    状态[&#39;exp_avg&#39;] = torch.zeros_like(p.data)

                    # RMS Prop 组件。 （平方梯度的指数 MA）。分母。
                    状态[&#39;exp_avg_sq&#39;] = torch.zeros_like(p.data)

                exp_avg, exp_avg_sq = 状态[&#39;exp_avg&#39;], 状态[&#39;exp_avg_sq&#39;]

                b1, b2 = 组[&#39;betas&#39;]
                状态[&#39;步骤&#39;] += 1

                # 添加权重衰减（如果有）
                如果组[&#39;weight_decay&#39;] != 0:
                    grad = grad.add(group[&#39;weight_decay&#39;], p.data)

                ＃ 势头
                exp_avg = torch.mul(exp_avg, b1) + (1 - b1)*grad
                
                # 有效值
                exp_avg_sq = torch.mul(exp_avg_sq, b2) + (1-b2)*(grad*grad)

                mhat = exp_avg / (1 - b1 ** 状态[&#39;步骤&#39;])
                vhat = exp_avg_sq / (1 - b2 ** 状态[&#39;步骤&#39;])
                
                denom = torch.sqrt( vhat + group[&#39;eps&#39;] )

                p.data = p.data - group[&#39;lr&#39;] * mhat / denom
                
                # 保存状态
                状态[&#39;exp_avg&#39;], 状态[&#39;exp_avg_sq&#39;] = exp_avg, exp_avg_sq

        回波损耗

我的问题是我的很多梯度都有 0 值，这会扰乱动量和速度项。我感兴趣的是修改代码，以便在计算动量和速度项（即第一和第二矩估计）时不考虑 0 值。
不过，我不确定该怎么做。如果它是一个简单的网络，其中梯度只是简单的维度，我可以检查是否 p.grad.data=0，但由于这将是一个多维张量，我不确定如何删除计算中的零并且不会弄乱其他东西（例如，剩余的更新）。]]></description>
      <guid>https://stackoverflow.com/questions/78369654/how-to-modify-the-adam-optimizer-to-not-include-zeros-in-the-calculations</guid>
      <pubDate>Tue, 23 Apr 2024 02:41:02 GMT</pubDate>
    </item>
    <item>
      <title>我在使用 djangorestframework 创建 REST API 来部署图像分类模型时遇到错误 [关闭]</title>
      <link>https://stackoverflow.com/questions/78365482/i-encounter-an-error-when-creating-a-rest-api-with-djangorestframework-to-deploy</link>
      <description><![CDATA[HTTP 500 内部服务器错误
允许：发布、选项
内容类型：application/json
变化：接受
{
“错误”：“找不到文件：filepath=saved_models/model.keras。请确保该文件是可访问的 .keras zip 文件。”
}
视图页面的脚本是：
&#39;&#39;&#39;Python
从rest_framework.views导入APIView
从rest_framework.response导入响应
从rest_framework导入状态
将张量流导入为 tf
从tensorflow.keras.models导入load_model
从tensorflow.keras.preprocessing导入图像
从张量流导入keras
从 process.models 导入 ImageModel
from process.serializers import ImageSerializer #ImageModelSerializer, ImagePredictionSerializer
将 numpy 导入为 np
从 io 导入 BytesIO
从 PIL 导入图像
从tensorflow.keras.applications.imagenet_utils导入preprocess_input
从rest_framework.exceptions导入ParseError
类 ImagePredictionAPIView(APIView):
def post(self, request, *args, **kwargs):
序列化器 = ImageSerializer(data=request.data)
如果序列化器.is_valid():
image_file = serializer.validated_data.get(&#39;image&#39;)
如果不是图像文件：
raise ParseError(“没有提交文件。”)
&lt;前&gt;&lt;代码&gt;尝试：
            model = load_model(“saved_models/model.keras”)
            img = image.load_img(image_file, target_size=(img_width, img_height))
            img_array = image.img_to_array(img)
            img_array = np.expand_dims(img_array, 轴=0)
            img_array = 预处理_输入(img_array)
            预测 = model.predict(img_array)
            预测标签=“好”如果预测[0][0]&gt; 0.5 否则“不好”
            返回响应（{&#39;预测&#39;：预测_标签}，状态=状态.HTTP_200_OK）
        除了文件未找到错误：
            return Response({&#39;error&#39;: &quot;Le modèle spécifié n&#39;a pas été trouvé.&quot;}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        除了异常 e：
            返回响应（{&#39;错误&#39;：str（e）}，状态= status.HTTP_500_INTERNAL_SERVER_ERROR）
    别的：
        返回响应（序列化器.错误，状态=状态.HTTP_400_BAD_REQUEST）

&#39;&#39;&#39;
序列化器页面脚本是：
&#39;&#39;&#39; 从rest_framework导入序列化器
导入uuid
导入base64
从 django.core.files.base 导入 ContentFile
从 process.models 导入 ImageProcess
类 Base64ImageField(serializers.ImageField):
def to_internal_value(自身, 数据):
if isinstance(data, str) 和 data.startswith(&#39;data:image&#39;):
# Base64 编码图像 - 解码
format, imgstr = data.split(&#39;;base64,&#39;) # 格式 ~= data:image/X,
ext = format.split(&#39;/&#39;)[-1] # 猜测文件扩展名
id = uuid.uuid4()
数据 = ContentFile(base64.b64decode(imgstr), name=id.urn[9:] + &#39;.&#39; + ext)
返回 super(Base64ImageField, self).to_internal_value(data)
类 ImageSerializer(serializers.ModelSerializer):
图像 = Base64ImageField()
类元：
模型 = 图像处理
字段=（&#39;pk&#39;，&#39;图像&#39;）
# read_only_fields = (&#39;照片&#39;,) &#39;
&#39;&#39;&#39;
网址脚本是：
&#39;&#39;&#39;
从 django.urls 导入路径
从 。导入视图
urlpatterns = [
路径(&#39;api/upload/&#39;,views.ImagePredictionAPIView.as_view(),name=&#39;image-upload&#39;),
]
&#39;&#39;&#39;
模型页面脚本是：
&#39;&#39;&#39;
从 django.db 导入模型
类 ImageModel(models.Model):
图像 = models.ImageField(upload_to=&#39;images/&#39;)
&#39;&#39;&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78365482/i-encounter-an-error-when-creating-a-rest-api-with-djangorestframework-to-deploy</guid>
      <pubDate>Mon, 22 Apr 2024 10:15:50 GMT</pubDate>
    </item>
    <item>
      <title>模块“keras.layers”没有属性“实验性”</title>
      <link>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</link>
      <description><![CDATA[你好，我试图调整我的数据集的大小和比例，如下所示，但我遇到了这个错误：
AttributeError：模块“keras.layers”没有属性“experimental”
&lt;前&gt;&lt;代码&gt;
resize_and_rescale= tf.keras.Sequential([
    图层.实验.预处理.调整大小(IMAGE_SIZE,IMAGE_SIZE),
    层.实验.预处理.重新缩放(1.0/255)
]）

]]></description>
      <guid>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</guid>
      <pubDate>Wed, 14 Dec 2022 00:43:49 GMT</pubDate>
    </item>
    <item>
      <title>MATLAB 中的自组织映射</title>
      <link>https://stackoverflow.com/questions/40830533/self-organizing-maps-in-matlab</link>
      <description><![CDATA[我正在尝试使用余弦而不是欧几里德距离对一些似乎可分离的数据进行聚类。如何使用 MATLAB 的 selforgmap 来实现此目的？我不相信这是通过“distanceFcn”选项实现的。 

&lt;块引用&gt;
  x = simplecluster_dataset；
  net = selforgmap([8 8],100,3,&#39;hextop&#39;,&#39;余弦&#39;);
  净=火车（净，x）；
  视图（净） y = 净（x）；
  类 = vec2ind(y);
]]></description>
      <guid>https://stackoverflow.com/questions/40830533/self-organizing-maps-in-matlab</guid>
      <pubDate>Sun, 27 Nov 2016 15:23:20 GMT</pubDate>
    </item>
    <item>
      <title>最大似然估计伪代码</title>
      <link>https://stackoverflow.com/questions/7718034/maximum-likelihood-estimate-pseudocode</link>
      <description><![CDATA[我需要编写一个最大似然估计器来估计一些玩具数据的均值和方差。我有一个包含 100 个样本的向量，是使用 numpy.random.randn(100) 创建的。数据应具有零均值和单位方差高斯分布。
我检查了维基百科和一些额外的资源，但我有点困惑，因为我没有统计背景。
是否有最大似然估计器的伪代码？我得到了 MLE 的直觉，但我不知道从哪里开始编码。
Wiki 表示采用对数似然的 argmax。我的理解是：我需要使用不同的参数来计算对数似然，然后我将采用给出最大概率的参数。我不明白的是：我首先在哪里可以找到参数？如果我随机尝试不同的平均值 &amp;方差以获得高概率，我什么时候应该停止尝试？]]></description>
      <guid>https://stackoverflow.com/questions/7718034/maximum-likelihood-estimate-pseudocode</guid>
      <pubDate>Mon, 10 Oct 2011 20:05:45 GMT</pubDate>
    </item>
    </channel>
</rss>