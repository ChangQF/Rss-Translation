<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 05 Feb 2025 12:33:05 GMT</lastBuildDate>
    <item>
      <title>如何通过 Keras、Tensorflow 实现重现性？</title>
      <link>https://stackoverflow.com/questions/79414036/how-to-have-reproducibility-with-keras-tensorflow</link>
      <description><![CDATA[每次运行以下代码时，我获得的准确率和损失都不一样。我按照之前帖子中的说明操作，但无法解决。问题可能出在哪里？
import os
os.environ[&#39;TF_ENABLE_ONEDNN_OPTS&#39;] = &#39;0&#39;
os.environ[&quot;TF_DETERMINISTIC_OPS&quot;] = &quot;1&quot;
os.environ[&quot;TF_CUDNN_DETERMINISTIC&quot;] = &quot;1&quot;
...
...

SEED=65
tf.keras.utils.set_random_seed(SEED) 
tf.config.experimental.enable_op_determinism()
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

训练，测试，训练目标，测试目标 = train_test_split((df.loc[:,&quot;input_alarm_1&quot;:&quot;input_alarm_&quot;+str(num_backtracking_events)]), df.loc[:,&quot;output_failure&quot;], test_size=0.25, random_state=SEED)

训练 = np.asarray(training).astype(&#39;float32&#39;)
训练目标 = np.asarray(trainingtarget).astype(&#39;float32&#39;)
test = np.asarray(test).astype(&#39;float32&#39;)
testtarget = np.asarray(testtarget).astype(&#39;float32&#39;)

initializer = tf.keras.initializers.GlorotUniform(seed=SEED)

model = keras.Sequential(
[
layer.Dense(600, 激活=&quot;relu&quot;, input_shape=(num_backtracking_events,), kernel_initializer=initializer),
layer.Dense(300, 激活=&quot;relu&quot;, kernel_initializer=initializer),
layer.Dense(100, 激活=&quot;relu&quot;, kernel_initializer=initializer),
layer.Dense(1, 激活=&quot;sigmoid&quot;, kernel_initializer=initializer),
#dropout?
]
)

#编译模型
model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;])

#fit
history = model.fit(training, trainingtarget, batch_size=10, epochs=50, validation_split=0.1)

# 评估 keras 模型
test_loss, test_acc = model.evaluate(test, testtarget)
print(&#39;Accuracy: %.2f&#39; % (test_acc*100))
print(&#39;Loss: %.2f&#39; % (test_loss*100))
]]></description>
      <guid>https://stackoverflow.com/questions/79414036/how-to-have-reproducibility-with-keras-tensorflow</guid>
      <pubDate>Wed, 05 Feb 2025 08:09:02 GMT</pubDate>
    </item>
    <item>
      <title>用 Java 编写的对偶数和奇数进行分类的人工智能无法工作</title>
      <link>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</guid>
      <pubDate>Wed, 05 Feb 2025 02:09:06 GMT</pubDate>
    </item>
    <item>
      <title>使用模型蒸馏来优化我的模型</title>
      <link>https://stackoverflow.com/questions/79413411/struggling-with-optimizing-my-model-using-model-distillation</link>
      <description><![CDATA[我有一个 NN 模型，用于学习端到端通信系统。它是一个自动编码器，其中编码器充当发射器；它采用 8 位并将其编码为 IQ 值，解码器充当接收器；它采用生成的 IQ 值并将其解码为 8 位。我还有一个通道模型，可以模拟噪声、频率/相位偏移等。
该模型经过训练，具有非常好的误码率 (BER)，但在进行推理时具有高延迟，因此我需要对其进行优化。我正在尝试遵循 pytorch 的知识提炼教程，但到目前为止，我无法让我的学生有效地学习。
我认为我的问题在于我的软损失函数不正确。在原始训练循环中，我使用 BinaryCrossEntropy 损失来对抗模型的预测位概率与真实输入位。从文档中可以看出，K.D 似乎包含了一个额外的损失，即采用学生和父母概率的 KL 散度损失。但是，在运行代码时，我的损失并没有改善。
我感到困惑的是，我的“软损失”应该是什么类型的损失函数，以及它应该获得什么输入类型（logit 或概率）。我尝试了不同的排列（将对数概率输入到 KL Div 中，使用 CrossEntropy 损失而不是 KL，即文档中显示的损失函数），但它们都没有以任何方式提高我的学生模型的性能。
这大致就是我正在使用的代码。它不是完整的代码；我只展示了父自动编码器和 K.D 循环，但这足以表达我的观点。
任何帮助都值得感激。
import torch
import torch.nn as nn
import torch.optim as optim

# 定义编码器
class Encoder(nn.Module):
def __init__(self):
super(Encoder, self).__init__()
self.fc1 = nn.Linear(8, 16) # 扩展特征空间
self.relu = nn.ReLU()
self.fc2 = nn.Linear(16, 10) # 输出 2 个值（IQ 表示）

def forward(self, x):
x = self.fc1(x)
x = self.relu(x)
x = self.fc2(x) # 输出原始 IQ 符号
return x

# 定义解码器
class Decoder(nn.Module):
def __init__(self):
super(Decoder, self).__init__()
self.fc1 = nn.Linear(100, 50) # 从 IQ 扩展回来
self.fc2 = nn.Linear(50, 30)
self.fc3 = nn.Linear(30, 16)
self.fc4 = nn.Linear(16, 8) # 输出 8 位恢复序列
self.relu = nn.ReLU()
self.sigmoid = nn.Sigmoid() # 确保输出在 (0,1) 范围内

def forward(self, x):
x = self.fc1(x)
x = self.relu(x)
x = self.fc2(x)
x = self.relu(x)
x = self.fc3(x)
x = self.relu(x)
x = self.fc4(x)
x = self.sigmoid() # 解释为概率
return x

# 定义自动编码器 (编码器 -&gt;通道 -&gt; 解码器)
class Autoencoder(nn.Module):
def __init__(self, noise_std=0.1):
super(Autoencoder, self).__init__()
self.encoder = Encoder()
self.decoder = Decoder()

def forward(self, x):
x = self.encoder(x) # 将 8 位编码为 2 个 IQ 符号
x = self.decoder(x) # 解码回 8 位序列
return x

ParentModel = Autoencoder(noise_std=0.1)

# 加载预先训练的权重
load_weights(model, path, optimizer)

def knowledge_distillation(teacher, student, T, epochs, batches, alpha):
ce_loss = nn.BCELoss()
kl_loss = nn.KLDivLoss(reduction=&quot;batchmean&quot;)
optimizer = optim.Adam(student.parameters(), lr = 1e-4)

teacher.eval() # 教师设置为评估模式
student.train() # 学生设置为训练模式

for epoch in range(epochs):
input_bits = generate_binary_tensor(8, batches) # 生成 [8, batch] 二进制张量

optimizer.zero_grad()

with torch.no_grad():
teacher_predictions = teacher(input_bits) # 教师前向传递

student_predictions = student(input_bits) # 学生前向传递

# 计算硬损失
hard_loss = ce_loss(student_predictions, input_bits)

# 计算软损失（不确定这部分）
soft_loss = kl_loss(student_predictions, teacher_predictions) * (T**2)

total_loss = alpha*soft_loss + (1-alpha)*hard_loss

total_loss.backward()
optimizer.step()

# 存储 BER

]]></description>
      <guid>https://stackoverflow.com/questions/79413411/struggling-with-optimizing-my-model-using-model-distillation</guid>
      <pubDate>Wed, 05 Feb 2025 00:55:20 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 的 seq2seq 教程解码器</title>
      <link>https://stackoverflow.com/questions/79413251/pytorchs-seq2seq-tutorial-decoder</link>
      <description><![CDATA[我正在通过 PyTorch 的 seq2seq 教程进行学习：https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
我对解码器有疑问

class DecoderRNN(nn.Module):
def __init__(self, hidden_​​size, output_size):
super(DecoderRNN, self).__init__()
self.embedding = nn.Embedding(output_size, hidden_​​size)
self.gru = nn.GRU(hidden_​​size, hidden_​​size, batch_first=True)
self.out = nn.Linear(hidden_​​size, output_size)

def forward(self,coder_outputs,coder_hidden,target_tensor=None):
batch_size =coder_outputs.size(0)
decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)
decoder_hidden =coder_hidden
decoder_outputs = []

for i in range(MAX_LENGTH):
decoder_output,decoder_hidden = self.forward_step(decoder_input,decoder_hidden)
decoder_outputs.append(decoder_output)

if target_tensor is not None:
# 教师强制：将目标作为下一个输入
decoder_input = target_tensor[:, i].unsqueeze(1) # 教师强制
else:
# 不使用教师强制：使用其自己的预测作为下一个输入
_,topi =decoder_output.topk(1)
decoder_input = topi.squeeze(-1).detach() # 从历史中分离作为输入

decoder_outputs = torch.cat(decoder_outputs, dim=1)
decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)
returncoder_outputs,coder_hidden, None # 我们返回 `None` 以在训练循环中保持一致性


为什么是“如果 target_tensor 不是 None”：
decoder_input = target_tensor[:, i].unsqueeze(1)

但是如果 target_tensor 是 None：
_, topi =coder_output.topk(1)
decoder_input = topi.squeeze(-1).detach()

具体来说，decoder_input 的形状在两者中不是不同的吗情况如何？
我觉得在第一种情况下，decoder_input 的形状是 2D 张量，但在第二种情况下是 1D
谢谢你的帮助]]></description>
      <guid>https://stackoverflow.com/questions/79413251/pytorchs-seq2seq-tutorial-decoder</guid>
      <pubDate>Tue, 04 Feb 2025 23:01:44 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法提高我的 CNN 准确性？[关闭]</title>
      <link>https://stackoverflow.com/questions/79412910/is-there-a-way-to-improve-my-cnns-accuracy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79412910/is-there-a-way-to-improve-my-cnns-accuracy</guid>
      <pubDate>Tue, 04 Feb 2025 20:07:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用带有分类头的 TensorFlow 主干模型？</title>
      <link>https://stackoverflow.com/questions/79411501/how-to-use-a-tensorflow-backbone-model-with-a-classification-head</link>
      <description><![CDATA[我尝试使用 tensorflow-models 库中的 ResNet3D，但在尝试运行该块时出现这个奇怪的错误
!pip install tf-models-official==2.17.0

Kaggle 笔记本上的 Tensorflow 版本是 2.18。
安装 tf-models-official 后
从 tensorflow.keras.callbacks 导入 EarlyStopping、ReduceLROnPlateau
从 tensorflow.keras.models 导入 Model
从 tensorflow.keras.layers 导入 Dense、GlobalAveragePooling3D、Input
从 tensorflow.keras.optimizers 导入 AdamW
导入 tensorflow_models 作为 tfm

def create_model():
base_model = tfm.vision.backbones.ResNet3D(model_id = 50,
temporary_strides= [3,3,3,3],
temporary_kernel_sizes = [(5,5,5),(5,5,5,5),(5,5,5,5,5,5),(5,5,5)],
input_specs=tf.keras.layers.InputSpec(shape=(None, None, IMG_SIZE, IMG_SIZE, 3))
)

# 取消冻结基础模型层
base_model.trainable = True

# 创建模型
input = Input(shape=[None, None, IMG_SIZE, IMG_SIZE, 3])
x = base_model(inputs) # B,1,7,7,2048
x = GlobalAveragePooling3D(data_format=&quot;channels_last&quot;, keepdims=False)(x)
x = Dense(1024,activation=&#39;relu&#39;)(x)
x = tf.keras.layers.Dropout(0.3)(x) # 添加 dropout 以防止过度拟合
outputs = Dense(NUM_CLASSES,activation=&#39;softmax&#39;)(x)

model = Model(inputs,outputs)

# 使用类权重编译模型
optimizer = AdamW(learning_rate=1e-4,weight_decay=1e-5)
model.compile(
optimizer=optimizer,
loss=&#39;sparse_categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;,tf.keras.metrics.AUC()]
)

返回模型

# 创建并显示模型
model = create_model()
model.summary()

当我运行此程序时，我收到错误以下：
---------------------------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
&lt;ipython-input-56-363271b4dda8&gt; in &lt;cell line: 39&gt;()
37 
38 # 创建并显示模型
---&gt; 39 model = create_model()
40 model.summary()

&lt;ipython-input-56-363271b4dda8&gt; in create_model()
18 # 创建模型
19 input = Input(shape=(None, None, IMG_SIZE, IMG_SIZE, 3))
---&gt; 20 x = base_model(输入) # B,1,7,7,2048

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py 在 __call__(self, *args, **kwargs) 中
586 layout_map_lib._map_subclass_model_variable(self, self._layout_map)
587 
--&gt; 588 返回 super().__call__(*args, **kwargs)

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/base_layer.py 在 __call__(self, *args, **kwargs) 中
1101 training=training_mode,
1102 ):
-&gt; 1103 input_spec.assert_input_compatibility(
1104 self.input_spec, input, self.name
1105 )

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/input_spec.py in assert_input_compatibility(input_spec, input, layer_name)
300 &quot;与层不兼容：&quot;
301 f&quot;预期形状={spec.shape}，&quot;
--&gt; 302 f&quot;发现形状={display_shape(x.shape)}&quot;
303 )
304 

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/input_spec.py 在 display_shape(shape) 中
305 
306 def display_shape(shape):
--&gt; 307 return str(tuple(shape.as_list()))
308 
309 

AttributeError: &#39;tuple&#39; 对象没有属性 &#39;as_list&#39;

我尝试将输入作为列表传递给 shape 参数，但仍然出现相同的错误。
错误发生在此
!pip install tf-models-official==2.17.0

import tensorflow as tf

inputs = tf.keras.Input(shape=[None, None, IMG_SIZE, IMG_SIZE, 3])
print(inputs.shape.as_list())

错误：
-------------------------------------------------------------------------------
AttributeError Traceback（最近一次调用last)
&lt;ipython-input-39-6e88680ff7df&gt; in &lt;cell line: 2&gt;()
1 输入 = tf.keras.Input(shape=[None, None, IMG_SIZE, IMG_SIZE, 3])
----&gt; 2 打印 (inputs.shape.as_list())

AttributeError: &#39;tuple&#39; 对象没有属性 &#39;as_list&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/79411501/how-to-use-a-tensorflow-backbone-model-with-a-classification-head</guid>
      <pubDate>Tue, 04 Feb 2025 11:22:41 GMT</pubDate>
    </item>
    <item>
      <title>为什么 2048 游戏的训练对我来说效果不佳？[关闭]</title>
      <link>https://stackoverflow.com/questions/79411336/why-is-training-for-the-game-2048-not-working-well-for-me</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79411336/why-is-training-for-the-game-2048-not-working-well-for-me</guid>
      <pubDate>Tue, 04 Feb 2025 10:28:14 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的分片是什么以及如何在 Tensorflow 中进行分片？</title>
      <link>https://stackoverflow.com/questions/68981874/what-is-sharding-in-machine-learning-and-how-to-do-sharding-in-tensorflow</link>
      <description><![CDATA[在机器学习的背景下，分片具体是什么（这里提出了一个更通用的古怪问题），以及它在 Tensorflow 中是如何实现的？
在谈论机器学习中的数据管道时，什么是分片，为什么我们需要分片？]]></description>
      <guid>https://stackoverflow.com/questions/68981874/what-is-sharding-in-machine-learning-and-how-to-do-sharding-in-tensorflow</guid>
      <pubDate>Mon, 30 Aug 2021 09:35:08 GMT</pubDate>
    </item>
    <item>
      <title>GridSearchCV 中的标准和评分有什么区别</title>
      <link>https://stackoverflow.com/questions/64675820/what-is-difference-between-criterion-and-scoring-in-gridsearchcv</link>
      <description><![CDATA[我创建了一个 GradientBoostingRegressor 模型。
我在 GridSearchCV 函数中使用 scoring 参数来返回 MSE 分数。
我想知道如果我在 param_grids 中使用 criterion 是否会改变我的模型？哪种方法才是正确的？
GBR = GradientBoostingRegressor()
param_grids = {
&#39;learning_rate&#39; : [0.01, 0.05, 0.07, 0.1, 0.3, 0.5 ],
&#39;n_estimators&#39; : [50,60,70,80,90,100],
&#39;max_depth&#39; : [1, 2, 3, 4],
&#39;min_samples_leaf&#39; : [1,2,3,5,10,15],
&#39;min_samples_split&#39;: [2,3,4,5,10], 
#&#39;criterion&#39; : [&#39;mse&#39;]
}

kf = KFold(n_splits=3, random_state=42, shuffle=True)
gs = GridSearchCV(estimator=GBR, param_grid = param_grids , cv = kf, n_jobs=-1, 
return_train_score=True, 评分=&#39;neg_mean_squared_error&#39;) 
]]></description>
      <guid>https://stackoverflow.com/questions/64675820/what-is-difference-between-criterion-and-scoring-in-gridsearchcv</guid>
      <pubDate>Wed, 04 Nov 2020 07:35:06 GMT</pubDate>
    </item>
    <item>
      <title>cross_val_score 和 gridsearchCV 如何工作？</title>
      <link>https://stackoverflow.com/questions/50629219/how-do-cross-val-score-and-gridsearchcv-work</link>
      <description><![CDATA[我一直在尝试弄清楚 gridsearchCV 和 cross_val_score 是如何工作的。
寻找赔率结果可以建立一种验证实验，但我仍然不明白我做错了什么。
为了简化，我使用 gridsearchCV 是最简单的方法，并尝试验证和理解正在发生的事情：
在这里：
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer
from sklearn.feature_selection import SelectKBest, f_regression, RFECV
from sklearn.decomposition import PCA
from sklearn.linear_model import RidgeCV,Ridge, LinearRegression
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.model_selection import GridSearchCV、KFold、TimeSeriesSplit、PredefinedSplit、cross_val_score
来自 sklearn.metrics 导入 mean_squared_error、make_scorer、r2_score、mean_absolute_error、mean_squared_error
来自 math 导入 sqrt

我创建了一个交叉验证对象（用于 gridsearchCV 和 cross_val_score）和一个用于管道和简单线性回归的训练/测试数据集。我已经检查过这两个数据集是相同的：
train_indices = np.full((15,), -1, dtype=int)
test_indices = np.full((6,), 0, dtype=int)
test_fold = np.append(train_indices, test_indices)
kf = PredefinedSplit(test_fold)

for train_index, test_index in kf.split(X):
print(&#39;TRAIN:&#39;, train_index, &#39;TEST:&#39;, test_index)
X_train_kf = X[train_index]
X_test_kf = X[test_index]

train_data = list(range(0,15))
test_data = list(range(15,21))

X_train, y_train=X[train_data,:],y[train_data]
X_test, y_test=X[test_data,:],y[test_data]

我的做法如下：
实例化一个简单的线性模型并将其与手动数据集一起使用
lr=LinearRegression()
lm=lr.fit(X,y)
lmscore_train=lm.score(X_train,y_train) 

结果：
r2=0.4686662249071524

lmscore_test=lm.score(X_test,y_test)

结果：
r2 0.6264021467338086

现在我尝试使用管道：
pipe_steps = ([(&#39;est&#39;, LinearRegression())])
pipe=Pipeline(pipe_steps)
p=pipe.fit(X,y)
pscore_train=p.score(X_train,y_train) 

结果：
r2=0.4686662249071524

pscore_test=p.score(X_test,y_test)

结果：
r2 0.6264021467338086

LinearRegression 和管道完美匹配
现在我尝试使用 cross_val_score 和预定义的分割来执行相同的操作kf
cv_scores = cross_val_score(lm, X, y, cv=kf) 

结果：
r2 = -1.234474757883921470e+01 # ?!?! （这应该是测试分数）

现在让我们尝试 gridsearchCV
scoring = {&#39;r_squared&#39;:&#39;r2&#39;}
grid_parameters = [{}] 
gridsearch=GridSearchCV(p, grid_parameters, verbose=3,cv=kf,scoring=scoring,return_train_score=&#39;true&#39;,refit=&#39;r_squared&#39;)
gs=gridsearch.fit(X,y)
results=gs.cv_results_

从 cv_results_ 我再次得到
mean_test_r_squared
# result
r2 -1.234474757883921292e+01

所以 cross_val_score 和 gridsearch 最终匹配一个另一个，但分数完全不对，与应有的分数不同。
你能帮我解开这个谜题吗？]]></description>
      <guid>https://stackoverflow.com/questions/50629219/how-do-cross-val-score-and-gridsearchcv-work</guid>
      <pubDate>Thu, 31 May 2018 16:50:39 GMT</pubDate>
    </item>
    <item>
      <title>预测性维护 - 如何将目标函数的贝叶斯优化与梯度下降的逻辑回归结合起来使用？</title>
      <link>https://stackoverflow.com/questions/48619800/predictive-maintenance-how-to-use-bayesian-optimization-with-objective-functio</link>
      <description><![CDATA[我试图重现 arimo.com 中显示的问题

这是一个如何为硬盘故障构建预防性维护机器学习模型的示例。我真正不明白的部分是如何将贝叶斯优化与自定义目标函数和逻辑回归与梯度下降结合使用。要优化的超参数是什么？问题的流程是什么？

正如我们之前的帖子所述，贝叶斯优化 [6] 用于
找到最佳超参数值。超参数调整中要优化的目标函数是在验证集上测量的以下分数：
S = alpha * fnr + (1 – alpha) * fpr
其中 fpr 和 fnr 是在验证集上获得的假阳性和假阴性率。我们的目标是保持假阳性率较低，因此我们使用 alpha = 0.2。由于验证集高度不平衡，我们发现标准分数（如准确率、F1 分数等）效果不佳。事实上，使用这个自定义分数对于模型获得良好的性能至关重要。
请注意，我们仅在运行贝叶斯优化时使用上述分数。为了训练逻辑回归模型，我们使用梯度下降法和通常的岭损失函数。

特征选择之前的数据框：
index date serial_number model capacity_bytes Failure 读取错误率 重新分配扇区数 通电时间 (POH) 温度 当前待处理扇区数 age yet_temperature yet_age yet_reallocated_sectors_count yet_read_error_rate yet_current_pending_sector_count yet_power_on_hours yet_failure
0 77947 2013-04-11 MJ0331YNG69A0A Hitachi HDS5C3030ALA630 3000592982016 0 0 0 4909 29 0 36348284.0 29.0 20799895.0 0.0 0.0 0.0 4885.0 0.0
1 79327 2013-04-11 MJ1311YNG7EWXA 日立 HDS5C3030ALA630 3000592982016 0 0 0 8831 24 0 36829839.0 24.0 21280074.0 0.0 0.0 0.0 8807.0 0.0
2 79592 2013-04-11 MJ1311YNG2ZD9A 日立 HDS5C3030ALA630 3000592982016 0 0 0 13732 26 0 36924206.0 26.0 21374176.0 0.0 0.0 0.0 13708.0 0.0
3 80715 2013-04-11 MJ1311YNG2ZDBA 日立 HDS5C3030ALA630 3000592982016 0 0 0 12745 27 0 37313742.0 27.0 21762591.0 0.0 0.0 0.0 12721.0 0.0
4 79958 2013-04-11 MJ1323YNG1EK0C 日立 HDS5C3030ALA630 3000592982016 0 524289 0 13922 27 0 37050016.0 27.0 21499620.0 0.0 0.0 0.0 13898.0 0.0
]]></description>
      <guid>https://stackoverflow.com/questions/48619800/predictive-maintenance-how-to-use-bayesian-optimization-with-objective-functio</guid>
      <pubDate>Mon, 05 Feb 2018 09:55:51 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 的超参数调整</title>
      <link>https://stackoverflow.com/questions/44181511/hyperparameter-tune-for-tensorflow</link>
      <description><![CDATA[我正在寻找一个用于直接在 Tensorflow（而不是 Keras 或 Tflearn）中编写的代码的超参数调整包。您能给我一些建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/44181511/hyperparameter-tune-for-tensorflow</guid>
      <pubDate>Thu, 25 May 2017 13:13:53 GMT</pubDate>
    </item>
    <item>
      <title>随机森林过度拟合</title>
      <link>https://stackoverflow.com/questions/33948946/random-forest-is-overfitting</link>
      <description><![CDATA[我正在使用带有分层 CV 的 scikit-learn 来比较一些分类器。
我正在计算：准确率、召回率、auc。
我使用带有 5 个 CV 的 GridSearchCV 进行参数优化。
RandomForestClassifier(warm_start= True, min_samples_leaf= 1, n_estimators= 800, min_samples_split= 5,max_features= &#39;log2&#39;, max_depth= 400, class_weight=None)

是来自 GridSearchCV 的 best_params。
我的问题是，我认为我真的过度拟合了。例如：

带标准差 (+/-) 的随机森林

精度：0.99 (+/- 0.06) 
敏感度：0.94 (+/- 0.06) 
特异性：0.94 (+/- 0.06) 
B_accuracy：0.94 (+/- 0.06)
AUC：0.94 (+/- 0.11)

带标准差 (+/-) 的逻辑回归

精度：0.88 (+/- 0.06) 
敏感度：0.79 (+/- 0.06) 
特异性： 0.68 (+/- 0.06) 
B_accuracy: 0.73 (+/- 0.06)
AUC: 0.73 (+/- 0.041)


其他的也看起来像逻辑回归（所以它们看起来没有过度拟合）。
我的 CV 代码是：
for i,j in enumerate(data):
X.append(data[i][0])
y.append(float(data[i][1]))
x=np.array(X)
y=np.array(y)

def SD(values):

mean=sum(values)/len(values)
a=[]
for i in range(len(values)):
a.append((values[i]-mean)**2)
erg=sum(a)/len(values)
SD=math.sqrt(erg)
return SD,mean

for name, clf in zip(titles,classifiers):
# 遍历所有分类器，计算 10 次折叠
# 下一个 for 循环应该再缩进 1 个制表符，抱歉，这里无法格式化
pre,sen,spe,ba,area=[],[],[],[],[]
for train_index, test_index in skf:
#print train_index, test_index
# 从所有 train_index 和 test_index 中获取索引
# 由于某些错误，将它们更改为列表
train=train_index.tolist()
test=test_index.tolist()
X_train=[]
X_test=[]
y_train=[]
y_test=[]
for i in train:
X_train.append(x[i])

for i in test:
X_test.append(x[i]) 

for i in train:
y_train.append(y[i])

for i in test:
y_test.append(y[i]) 

#clf=clf.fit(X_train,y_train)
#predicted=clf.predict_proba(X_test)
#... 其他代码，计算指标等等...
print name 
print(&quot;precision: %0.2f \t(+/- %0.2f)&quot; % (SD(pre)[1], SD(pre)[0]))
print(&quot;sensitivity: %0.2f \t(+/- %0.2f)&quot; % (SD(sen)[1], SD(pre)[0]))
print(&quot;specificity: %0.2f \t(+/- %0.2f)&quot; % (SD(spe)[1], SD(pre)[0]))
print(&quot;B_accuracy: %0.2f \t(+/- %0.2f)&quot; % (SD(ba)[1], SD(pre)[0]))
print(&quot;AUC: %0.2f \t(+/- %0.2f)&quot; % (SD(area)[1], SD(area)[0]))
print &quot;\n&quot;

如果我使用 scores = cross_validation.cross_val_score(clf, X, y, cv=10,scoring=&#39;accuracy&#39;) 方法，我不会得到这个“过度拟合”值。所以也许我使用的 CV 方法有问题？但它仅适用于 RF...
由于 cross_val_function 中特异性得分函数的滞后，我自己做了。]]></description>
      <guid>https://stackoverflow.com/questions/33948946/random-forest-is-overfitting</guid>
      <pubDate>Fri, 27 Nov 2015 00:55:00 GMT</pubDate>
    </item>
    <item>
      <title>在 sci-kit learn 中对随机森林分类器进行故障排除</title>
      <link>https://stackoverflow.com/questions/21963486/troubleshooting-random-forests-classifier-in-sci-kit-learn</link>
      <description><![CDATA[我尝试运行来自 sci-kit learn 的随机森林分类器，但输出结果可疑，只有不到 1% 的预测是正确的。该模型的表现比偶然性要差得多。我对 Python、ML 和 sci-kit learn（三重打击）还比较陌生，我担心的是缺少一些基本的东西，而不是需要微调参数。我希望有更多经验丰富的人来查看代码，看看设置是否有问题。
我尝试根据单词出现次数预测电子表格中行的类别，因此每行的输入都是一个数组，表示每个单词出现的次数，例如 [1 0 0 2 0 ... 1]。我使用 sci-kit learn 的 CountVectorizer 进行此处理，我向它输入包含每行单词的字符串，它输出单词出现次数数组。如果由于某种原因此输入不合适，则可能是出了问题，但我在网上或文档中没有找到任何表明情况如此的信息。
目前，森林的正确回答率约为 0.5%。使用完全相同的输入和 SGD 分类器可获得接近 80% 的结果，这表明我所做的预处理和矢量化没有问题 - 这是 RF 分类器特有的。我的第一反应是寻找过度拟合，但即使我在训练数据上运行模型，它仍然几乎完全出错。
我尝试过树的数量和训练数据量，但对我来说似乎没有太大变化。我试图仅显示相关代码，但如果有帮助，我可以发布更多代码。第一篇 SO 帖子，因此欢迎大家提出想法和反馈。
#拉入包来为每一行创建单词出现向量
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(min_df=1,charset_error=&#39;ignore&#39;)
X_train = vectorizer.fit_transform(train_file)
#转换为密集数组，这是随机森林分类器所需的输入类型
X_train = X_train.todense()

#拉入随机森林分类器并训练数据
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators = 100, compute_importances=True)
clf = clf.fit(X_train, train_targets)

#将测试数据转换为向量格式
testdata = vectorizer.transform(test_file)
testdata = testdata.todense()

#导出
with open(&#39;output.csv&#39;, &#39;wb&#39;) 作为 csvfile:
spamwriter = csv.writer(csvfile)
for item in clf.predict(testdata):
spamwriter.writerow([item])
]]></description>
      <guid>https://stackoverflow.com/questions/21963486/troubleshooting-random-forests-classifier-in-sci-kit-learn</guid>
      <pubDate>Sun, 23 Feb 2014 02:40:28 GMT</pubDate>
    </item>
    <item>
      <title>神经网络是一种懒惰的学习方法还是积极学习的方法？[关闭]</title>
      <link>https://stackoverflow.com/questions/5749867/is-a-neural-network-a-lazy-or-eager-learning-method</link>
      <description><![CDATA[神经网络是一种懒惰的还是积极学习的方法？不同的网页有不同的说法，所以我想得到一个可靠的答案，并有好的文献来支持它。最明显的书是米切尔著名的《机器学习》一书，但浏览整本书我找不到答案。谢谢 :)。]]></description>
      <guid>https://stackoverflow.com/questions/5749867/is-a-neural-network-a-lazy-or-eager-learning-method</guid>
      <pubDate>Thu, 21 Apr 2011 21:16:25 GMT</pubDate>
    </item>
    </channel>
</rss>