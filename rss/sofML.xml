<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 18 May 2024 21:12:34 GMT</lastBuildDate>
    <item>
      <title>建立预测模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78500710/build-a-prediction-model</link>
      <description><![CDATA[我想建立一个模型来预测游戏仍在玩时的结果。我的数据包括过去比赛的结果以及有关这些比赛状态的一些简单信息，例如每个球员的分数和位置。每分钟都会获取一次信息，直到比赛结束。我有很多游戏样本，其中每个样本都是结果+每分钟有关游戏的信息。最大分钟为 60 分钟，因此如果比赛在 60 分钟之前结束，则数据将只是初始值 0。
时间序列模型我在网上搜索到所有点都指向单个时间线，他们根据过去进行预测，但我希望我的模型根据游戏的进展（包括过去发生的事情）进行预测。
我会在比赛进行时使用我的模型，它会更新每分钟获胜的机会。因此，输入仅与某个时间点之前的先前数据相关。
我尝试使用 lstm 但它似乎效果不佳。我期望它像语言模型一样学习我的数据，尝试从过去到当前的信息预测游戏状态，但准确率不会超过 55%。我尝试调整参数，但似乎没有产生很大的差异。
我还可以使用哪些其他模型来实现此目的？或者我可以使用什么巧妙的方法？]]></description>
      <guid>https://stackoverflow.com/questions/78500710/build-a-prediction-model</guid>
      <pubDate>Sat, 18 May 2024 18:50:53 GMT</pubDate>
    </item>
    <item>
      <title>我如何为该调查计划中的这些答案选择分配数字权重？</title>
      <link>https://stackoverflow.com/questions/78500666/how-do-i-assign-numerical-weights-to-these-answer-choices-in-this-survey-program</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78500666/how-do-i-assign-numerical-weights-to-these-answer-choices-in-this-survey-program</guid>
      <pubDate>Sat, 18 May 2024 18:33:20 GMT</pubDate>
    </item>
    <item>
      <title>感知器的图像识别不起作用</title>
      <link>https://stackoverflow.com/questions/78500607/image-recognition-with-perceptrons-not-working</link>
      <description><![CDATA[我必须调整我的图像识别代码，以便它使用感知器来代替分类器进行图像识别。我按照讲师提供的示例执行了所有操作（并且我正在使用他们的感知器代码）。使用分类器，我的图像识别工作完美，没有出错，但是当我用感知器替换这些分类器时，无论我使用什么图像，它总是被识别为第二种类型。
我的图像识别代码：
learning_matrix = []
网络 = 感知器.PerceptronNetwork(6000, 1)
大小 = 75, 6000
Learning_matrix = np.zeros(大小, dtype=np.float32)
学习矩阵 = gabor_attributes
纪元数 = 0

对于范围（100）内的纪元：
    错误总数 = 0
    对于范围 (75) 内的 i：
        error_in_network = network.teachNetwork(learning_matrix[i],label_matrix[i])
        总错误数 += 网络中的错误数
    number_of_epochs += 1
    如果总错误数 == 0：
        休息

识别矩阵 = []
recognize_matrix = (GaborFilter(“Test.jpg”))

雷兹 = [10]
网络.giveNetworkAnswer(recognition_matrix, rez)

如果 rez[0] == 0:
    print(&quot;第一种类型&quot;)
elif rez[0] == 1:
    print(&quot;第二个管子&quot;)
elif rez[0] == 2:
    print(&quot;第三种类型&quot;)
别的：
    print(&quot;错误：&quot;+str(rez[0]))

感知器代码，由我的讲师提供：
### 单独的神经元（感知器）类###
神经元类：

    利亚姆达 = 0.001
    def __init__(this, input_nr):
        this.input_number = input_nr
        this.权重 = []

        对于范围内的 i（this.input_number）：
            this.weights.append(np.random.normal(0, 0.01))
        this.threshold = np.random.normal(0, 0.01)
        
    def 响应（此，输入信号）：
        总和 = 0
        对于范围内的 i（this.input_number）：
            sum += inputSignal[i] * this.weights[i]
        sum -= this.threshold
        如果总和 &gt; 返回 1 0 否则 0
            
    def changeWeights(this, inputSignal, targetResponse):

        实际响应 = this.response(inputSignal)
        错误 = 目标响应 - 实际响应
        
        如果错误！= 0：
            对于范围内的 i（this.input_number）：
                deltaSvoris = this.liambda * inputSignal[i] * 错误
                this.weights[i] += deltaSvoris
            this.threshold += this.liambda * 错误
        返回错误
           
########## 感知器 ANN 类 ############
类感知器网络：

    def __init__(this, 输入_nr, 输出_nr):
        this.input_number = input_nr
        this.neuronNumber = 输出编号
        this.outputLayer = []
        对于范围内的 i(this.neuronNumber)：
            神经元 = 神经元(this.input_number)
            this.outputLayer.append(神经元)

    def示教网络（这个，输入向量，输出向量）：
        错误编号输入输出 = 0
        对于范围内的 i(this.neuronNumber)：
            神经响应 = this.outputLayer[i].response(inputVector)
            如果神经元响应！= 输出向量[i]：
                错误= this.outputLayer [i] .changeWeights（inputVector，outputVector [i]）
                错误编号输入输出+=abs(错误)
        返回错误编号输入输出
    
    def GiveNetworkAnswer(this, inputVector, networkResponse):
        对于范围内的 i(this.neuronNumber)：
            networkResponse[i] = this.outputLayer[i].response(inputVector)
            打印（f“{networkResponse}”）

我尝试更改纪元数，打印出一些值以查看我使用的数据是否存在错误，但没有运气。]]></description>
      <guid>https://stackoverflow.com/questions/78500607/image-recognition-with-perceptrons-not-working</guid>
      <pubDate>Sat, 18 May 2024 18:12:53 GMT</pubDate>
    </item>
    <item>
      <title>计算损失函数的梯度</title>
      <link>https://stackoverflow.com/questions/78500380/calculate-graident-of-loss-function</link>
      <description><![CDATA[考虑如下所示的神经网络。
考虑我们有一个用于二元分类的交叉熵损失函数：
参考图

L=−[𝑦 ln(𝑎)+(1−𝑦) ln(1−𝑎)]，其中𝑎是输出层激活函数的概率。我们构建了网络的计算图，如下所示。下面的蓝色字母是中间变量标签，可以帮助你理解上面的网络架构图和计算图之间的联系。

当 𝑦=1 时，损失函数相对于梯度的梯度是多少。 𝑊11？将你的答案写到小数点后三位。
注：请使用计算图法。可以直接使用链式规则计算梯度，但如果根本不使用计算图，则无法正确评分。尝试填写上面的红色框。本题不需要编码，分析即可轻松得出答案。

我在解决这个问题时遇到问题]]></description>
      <guid>https://stackoverflow.com/questions/78500380/calculate-graident-of-loss-function</guid>
      <pubDate>Sat, 18 May 2024 16:44:46 GMT</pubDate>
    </item>
    <item>
      <title>TPU v3-8 TensorFlow CrossReplicaSum 错误</title>
      <link>https://stackoverflow.com/questions/78500291/tpu-v3-8-tensorflow-crossreplicasum-error</link>
      <description><![CDATA[当我适合我的模型时，我收到此错误。
tensorflow/core/tpu/kernels/tpu_compilation_cache_external.cc:112] 要求将动态维度从 hlo transpose.3750@{}@0 传播到 hlo %all-reduce.3755 = f32[&lt;= 70,256]{1,0}全归约(f32[&lt;=70,256]{1,0}%transpose.3750),replica_groups={{0,1,2,3,4,5,6,7}} ，to_apply=%sum.3751，metadata={op_type=“CrossReplicaSum” op_name=“CrossReplicaSum_33” source_file=“虚拟文件名” source_line=10}，未实现。

1013 tpu_program_group.cc:90] 检查失败：xla_tpu_programs.size() &gt; 0（0 对 0）

但是我传递显式输入形状，如下所示：
Config.COMPUTED_BATCH_SIZE = 128

使用strategy.scope()：
    模型 = my_model()
    输入形状 = [
        [配置.COMPUTED_BATCH_SIZE, 192],
        [配置.COMPUTED_BATCH_SIZE, 192],
        [计算通道、105、129、100]、
        [计算通道、105、129、100]、
        [计算通道、105、129、100]、
        [配置.COMPUTED_BATCH_SIZE, 70],
        [配置.COMPUTED_BATCH_SIZE, 320]
    ]
    model.build(input_shape=input_shapes)

编辑：
我已经追踪到这段代码：
hidden_​​size = 128
self.descriptor_embedding = 层.Dense(
    隐藏大小 * 2, #256
    激活=&#39;relu&#39;,
    input_shape=(Config.COMPUTED_BATCH_SIZE, 70)
）


学习描述符 = tf.expand_dims(
    self.descriptor_embedding（描述符），
    1
) # [BS, 1, HS * 2]

有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78500291/tpu-v3-8-tensorflow-crossreplicasum-error</guid>
      <pubDate>Sat, 18 May 2024 16:07:20 GMT</pubDate>
    </item>
    <item>
      <title>无法为 tflite 模型创建地图</title>
      <link>https://stackoverflow.com/questions/78500067/unable-to-create-a-map-for-tflite-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78500067/unable-to-create-a-map-for-tflite-model</guid>
      <pubDate>Sat, 18 May 2024 14:41:10 GMT</pubDate>
    </item>
    <item>
      <title>如何将数据清理纳入训练模型中</title>
      <link>https://stackoverflow.com/questions/78497891/how-to-incorporate-data-cleansing-into-trained-model</link>
      <description><![CDATA[如果我清理数据并将中值归入 NaN 值，我是否应该以某种方式将其合并到将用于测试数据的模型中？换句话说，我的测试数据是否也需要清理和估算，或者训练会处理这个问题吗？
我想说它需要被合并，因为否则 NaN 值会破坏模型，而且任何偏度都不会得到解决。
特别是：
用中位数替换 NaN：
data = data.fillna(data.median())

使用分位数变换处理偏度，使每个特征遵循正态分布（以下仅举一例）。
qualtile_transformer = QuantileTransformer(output_distribution=&#39;正常&#39;, random_state=0&#39;)
数据[&#39;feat_0&#39;] = quantile_transformer.fit_transform(数据[&#39;feat_0&#39;].values.reshape(-1,1)).flatten()

型号：
从 sklearn.linear_model 导入 LinearRegression
Linear_regr = 线性回归()
Linear_regr.fit(Xtrain,Ytrain)

预测：
# 使用测试集进行预测
Ypred = Linear_regr.predict(Xtest)

因此，最终，如果我要采用我的模型并将其用于类似但不同的数据，我如何才能确保它不会失败，并且 NaN 和分位数转换将在之前使用新数据进行处理预测已实现，所以不会失败？]]></description>
      <guid>https://stackoverflow.com/questions/78497891/how-to-incorporate-data-cleansing-into-trained-model</guid>
      <pubDate>Fri, 17 May 2024 20:47:39 GMT</pubDate>
    </item>
    <item>
      <title>在 python 中为我的标签应用程序处理数据库锁</title>
      <link>https://stackoverflow.com/questions/78497797/handle-database-locks-in-python-for-my-labeling-app</link>
      <description><![CDATA[我希望我的应用程序的用户检索一些要标记的数据。在第一个版本中，我没有实现锁定，因此多个用户可以同时访问相同的数据，因此第二个用户将覆盖第一个用户的标签。
我正在使用 python fastAPI sqlite 后端。
我最初想出了一个想法，为标签添加一个“正在被标记”值，以便下一个提议的数据不相同。我不喜欢它，因为我不知道如何处理用户退出应用程序（或其他）而没有标记数据的情况。目前，我最好的方法是添加一个带有时间戳检索时间的列，并实现一个逻辑，假设在检索后 30 秒，我们检查标签是否不再为 None。如果它仍然是 None（意味着该人没有标记），我们删除时间戳的值。我也不太满意，因为它没有处理人们冥想然后回来标记数据的情况。
您有更好的建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78497797/handle-database-locks-in-python-for-my-labeling-app</guid>
      <pubDate>Fri, 17 May 2024 20:19:31 GMT</pubDate>
    </item>
    <item>
      <title>我用自己的数据集训练yolo模型但没有测试结果</title>
      <link>https://stackoverflow.com/questions/78497575/i-train-yolo-model-with-my-own-data-set-but-there-is-no-test-result</link>
      <description><![CDATA[我正在使用 Yolov3 模型以及从 Kaggle 收到的数据集来训练模型。模型训练已完成，我将新权重添加到备份文件夹中。我运行了我训练过的一种水果进行测试，但没有发生对象检测。同一图像显示为 Prediction.jpg。训练看起来不错，但我不明白为什么它不能检测物体。请帮助我。
火车站代码：
./darknet探测器列车 /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/Desktop/Projects/Bitirmeprojesi/yolov3.weights

测试终端代码：
./darknet探测器测试 /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/darknet/backup/yolov3_final.weights -thresh 0.25 -out预测.jpg

我设置并编辑了 obj.data、obj.names 和 yolov3.cfg 文件。
我有 3 个类别：苹果、香蕉和橙子。我已经根据3个类在cfg文件中正确设置了filter和class值等值。
cfg 文件
[网]
# 测试
批次=64
细分=1
＃ 训练
细分=16
宽度= 608
高度=608
通道=3
动量=0.9
衰减=0.0005
角度=0
饱和度=1.5
曝光=1.5
色调=0.3

学习率=0.001
烧入=1000
max_batches = 6000 # 类数 * 2000
政策=步骤
步骤=3600,4800 # max_batches num %80, %90
尺度=.1,.1

数据集中除了.jpg图片外，还有yolo格式的同名.txt文件。
在此处输入图像描述文件图像
包含所有图像路径的 train.txt 和 test.txt 文件也已准备就绪。
当我在终端中运行测试命令时，它可以工作，但图片看起来相同，没有检测对象的边界框。我确定我已经安装了 Opencv。我正在使用 macOS。为什么它没有检测到它？请有人帮忙。我多次通过 make clean 清理暗网，并通过 make opencv = 1 运行它，但结果没有改变。
[yolo]参数：iou损失：mse（2），iou_norm：0.75，obj_norm：1.00，cls_norm：1.00，delta_norm：1.00，scale_x_y：1.00
总 BFLOPS 137.613
平均输出 = 1052318
正在从 /Users/melisabagcivan/darknet/backup/yolov3_final.weights 加载权重...
 看过 64 个，训练过：32013 个 K 图像（500 Kilo-batches_64）
完毕！从权重文件加载 107 层
输入图像路径：/Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg
 检测层：82-类型=28
 检测层数：94-型=28
 检测层：106-类型=28
/Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg：预测为 6738.129000 毫秒。

我尝试了很多图像，但它没有在任何图像中绘制方框。我不明白是它无法检测到还是我在测试时犯了错误。]]></description>
      <guid>https://stackoverflow.com/questions/78497575/i-train-yolo-model-with-my-own-data-set-but-there-is-no-test-result</guid>
      <pubDate>Fri, 17 May 2024 19:21:32 GMT</pubDate>
    </item>
    <item>
      <title>如何使用决策树算法和bert模型对文本进行分类？</title>
      <link>https://stackoverflow.com/questions/78497176/how-to-use-decision-tree-algorithm-with-bert-model-to-classify-a-text</link>
      <description><![CDATA[我想集成并使用 BERT 和决策树两种算法进行文本分类，因此我需要该领域的指导和帮助。
如果有人有这个领域的源代码或文章，请提供给我。或者即使朋友有更好的建议将 BERT 算法与任何其他算法结合起来]]></description>
      <guid>https://stackoverflow.com/questions/78497176/how-to-use-decision-tree-algorithm-with-bert-model-to-classify-a-text</guid>
      <pubDate>Fri, 17 May 2024 17:44:40 GMT</pubDate>
    </item>
    <item>
      <title>如何在流模式下分割拥抱脸部数据集而不将其加载到内存中？</title>
      <link>https://stackoverflow.com/questions/78497069/how-to-split-a-hugging-face-dataset-in-streaming-mode-without-loading-it-into-me</link>
      <description><![CDATA[我正在使用 Hugging Face 数据集，我需要将数据集拆分为训练集和验证集。我的主要要求是数据集应该以流模式处理，因为我不想将整个数据集加载到内存中。
从数据集导入load_dataset，DatasetDict

# 从 Hugging Face 加载数据集
数据集 = load_dataset(&#39;小队&#39;, split=&#39;训练&#39;)

# 将数据集分为训练集和验证集
# 指定测试集（验证集）的分数
train_val_split = dataset.train_test_split(test_size=0.1)

# 提取训练和验证数据集
train_dataset = train_val_split[&#39;train&#39;]
val_dataset = train_val_split[&#39;测试&#39;]

# 打印数据集的大小
print(f&quot;训练集大小: {len(train_dataset)}&quot;)
print(f&quot;验证集大小: {len(val_dataset)}&quot;)

# 如果需要的话保存数据集
# train_dataset.save_to_disk(&#39;路径/到/train_dataset&#39;)
# val_dataset.save_to_disk(&#39;路径/到/val_dataset&#39;)

是否有一种方法可以在流模式下分割 Hugging Face 数据集？
参考文献：

https ://discuss.huggingface.co/t/how-to-split-a-dataset-into-train-test-and-validation/1238
https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090/21
https://discuss.huggingface .co/t/possible-to-stream-and-create-new-splits/67214
https://huggingface.co/docs/datasets/v1.11.0 /splits.html
https://discuss.huggingface.co/t/how-to-split-a-hugging-face-dataset-in-streaming-mode-without-loading-it-into-memory /87205
]]></description>
      <guid>https://stackoverflow.com/questions/78497069/how-to-split-a-hugging-face-dataset-in-streaming-mode-without-loading-it-into-me</guid>
      <pubDate>Fri, 17 May 2024 17:18:18 GMT</pubDate>
    </item>
    <item>
      <title>学习率不更新</title>
      <link>https://stackoverflow.com/questions/78496983/learning-rate-not-updating</link>
      <description><![CDATA[def make_prediction(x0,t0):
输入 = torch.vstack([x0,t0])
layer_1 = torch.matmul(w0,inputs)
返回 layer_1

loss1 = nn.MSELoss()
def loss_function():
u_t=(make_prediction(x,t+inf_s)-make_prediction(x,t))/inf_s
u_x=(make_prediction(x+inf_s,t)-make_prediction(x,t))/inf_s
u_xx=(make_prediction(x+inf_s,t)-2*make_prediction(x,t)+make_prediction(x-inf_s,t))/inf_s**2
返回 (1/N_i)*(loss1(make_prediction(x0IC,t0IC), u0IC))+(1/N_b)*(loss1(make_prediction(x0BC1,t0BC1), u0BC1))
+(1/N_b)*(loss1(make_prediction(x0BC2,t0BC2), u0BC2))+(1/N_f)*(np.pi/0.01)*(loss1(u_xx-u_t-make_prediction(x,t)*u_x, 0))

def train_step(w,b, learning_rate):
trainable_variables = [w,b]
optimizer = torch.optim.SGD(trainable_variables, lr=learning_rate,momentum=0.9)
scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.01)
loss = loss_function()
loss.backward()
with torch.no_grad():
w -= learning_rate * w.grad
b -= learning_rate * b.grad
w.grad.zero_()
b.grad.zero_()
optimizer.step()
scheduler.step()
train_step(w,bias,learning_rate)

我运行此代码（通过scheduler.ExponentialLR），但学习率没有变化。
您认为问题出在哪里？
我写了完整的代码...感谢您的帮助]]></description>
      <guid>https://stackoverflow.com/questions/78496983/learning-rate-not-updating</guid>
      <pubDate>Fri, 17 May 2024 16:56:52 GMT</pubDate>
    </item>
    <item>
      <title>我的逻辑回归模型有问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78493918/i-am-having-problem-with-my-logistic-regression-model</link>
      <description><![CDATA[我的模型精度很差。此数据取自 https://archive.ics.uci .edu/dataset/15/breast+cancer+wisconsin+original 显示逻辑回归模型的准确度为 96%，所以问题确实出在我的模型中。我建立了以下模型：
# 导入数据集
tumor_study &lt;- read.csv(“breast-cancer-wisconsin.data”, header = FALSE, na.strings = “NA”)

# 添加列名
特征&lt;-c(“id_number”，“ClumpThickness”，“Uniformity_CellSize”，
              “Uniformity_CellShape”、“边缘粘附”、
              “SingleEpithelial_CellSize”、“BareNuclei”、“Bland_Chromatin”、
              “Normal_Nucleoli”、“Mitoses”、“Class”）

colnames(tumor_study) &lt;- 特征

# 清洗数据
# 删除第一列（id_number）
肿瘤研究 &lt;- 肿瘤研究[,-1]

# 转换“?” BareNuclei 列中为 NA，然后为数字
tumor_study$BareNuclei[tumor_study$BareNuclei == &quot;?&quot;] &lt;- NA
tumor_study$BareNuclei &lt;- as.numeric(tumor_study$BareNuclei)

# 删除 BareNuclei 中缺失值的行
tumor_study &lt;-tumor_study[!is.na(tumor_study$BareNuclei),]

# 将类转换为因子
tumor_study$Class &lt;- 因子(tumor_study$Class, level = c(2, 4), labels = c(“良性”, “恶性”))

# 将数据集分为训练集和测试集
库（caTools）
设置.种子(123)
split &lt;-sample.split(tumor_study$Class, SplitRatio = 0.8)
Training_set &lt;-tumor_study[split == TRUE,]
test_set &lt;-tumor_study[split == FALSE,]

# 应用特征缩放
训练集[, 1:9] &lt;- 比例(训练集[, 1:9])
test_set[, 1:9] &lt;- 比例(test_set[, 1:9])

# 构建逻辑回归模型
分类器 &lt;- glm(公式 = Class ~ ., family = 二项式, data = Training_set)

# 预测训练集的概率
prob_y_train &lt;- 预测（分类器，类型 = &#39;响应&#39;，newdata = Training_set[,-10]）
Predicted_y_training &lt;- ifelse(prob_y_train &gt;= 0.5,“良性”,“恶性”)

# 使用 test_set 进行预测
prob_y_test &lt;- 预测（分类器，类型 = &#39;响应&#39;，newdata = test_set[,-10]）
Predicted_y_test &lt;- ifelse(prob_y_test &gt;= 0.5,“良性”,“恶性”)

# 使用混淆矩阵检查准确性
cm_test &lt;- 表(test_set[,10], Predicted_y_test)
打印（厘米_测试）

但我的准确率接近 2%。
如何找出模型中的问题？]]></description>
      <guid>https://stackoverflow.com/questions/78493918/i-am-having-problem-with-my-logistic-regression-model</guid>
      <pubDate>Fri, 17 May 2024 06:43:30 GMT</pubDate>
    </item>
    <item>
      <title>反馈管理器需要具有单一签名推断的模型</title>
      <link>https://stackoverflow.com/questions/78493742/feedback-manager-requires-a-model-with-a-single-signature-inference</link>
      <description><![CDATA[我在尝试运行机器运行模型时遇到了此错误，该模型应该为驾驶员睡意检测项目提供动力
&lt;前&gt;&lt;代码&gt;W0000 00:00:1715924294.765512 2256 inference_feedback_manager.cc:114]
反馈管理器需要具有单一签名推断的模型。
禁用对反馈张量的支持。

模型架构如下：
&lt;前&gt;&lt;代码&gt;#**型号**
从 keras.layers 导入 BatchNormalization
模型 = tf.keras.models.Sequential()
# 输入形状是所需的图像大小 145 x 145，颜色为 3 字节

#这是第一个卷积
   model.add(Conv2D(16, 3, 激活=&#39;relu&#39;, input_shape=X_train.shape[1:]))
   model.add(BatchNormalization())
   model.add(MaxPooling2D())
   tf.keras.layers.Dropout(0.3)

# 第二次卷积
   model.add(Conv2D(32, 5, 激活=&#39;relu&#39;))
   model.add(BatchNormalization())
   model.add(MaxPooling2D())
   tf.keras.layers.Dropout(0.3)

# 第三次卷积
  model.add(Conv2D(64, 10, 激活=&#39;relu&#39;))
  model.add(BatchNormalization())
  model.add(MaxPooling2D())
  tf.keras.layers.Dropout(0.3)

# 第四次卷积
  model.add(Conv2D(128, 12, 激活=&#39;relu&#39;))
  model.add(BatchNormalization())

# 将结果压平以输入 DNN
  模型.add(压平())
  model.add（密集（128，激活=&#39;relu&#39;））
  模型.add(Dropout(0.25))
  model.add（密集（64，激活=&#39;relu&#39;））
# 只有 1 个输出神经元。
  model.add（密集（1，激活=&#39;sigmoid&#39;））

  model.compile(loss=“binary_crossentropy”，metrics=[“accuracy”]，optimizer=Adam(lr=0.001))
  历史= model.fit（train_generator，epochs = 10，batch_size = 32，validation_data = test_generator）

# 定义服务签名
  输入签名 = [
      tf.TensorSpec(shape=[None, 145, 145, 3], dtype=tf.float32, name=&#39;input_tensor&#39;)
  ]

@tf.function(input_signature=input_signature)
defserving_fn（输入）：
    返回模型（输入）

export_dir = &#39;E:\系统项目\项目&#39;
tf.saved_model.save（serving_fn，export_dir）

# 加载模型进行推理
load_model = tf.saved_model.load(&#39;my_model.keras&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78493742/feedback-manager-requires-a-model-with-a-single-signature-inference</guid>
      <pubDate>Fri, 17 May 2024 05:59:50 GMT</pubDate>
    </item>
    <item>
      <title>ARFaceAnchor如何确定顶点索引？</title>
      <link>https://stackoverflow.com/questions/63188856/how-arfaceanchor-determine-to-vertices-indices</link>
      <description><![CDATA[我使用过 ARKit 人脸追踪。通过使用它，我可以获得人脸特征点
x=faceAnchor.geometry.vertices[638][0]
  
y=faceAnchor.geometry.vertices[638][1]

z=faceAnchor.geometry.vertices[638][2]

我想到了一些简单的问题。
这就是“ARFaceAnchor 如何确定顶点索引？”
我搜索了许多网址（apple，stackoverflow），但我找不到真相。
我以为dlib之类的，机器学习用的？
我在哪里可以找到有关该信息（如何计算面向量指数）？
ARKit的人脸顶点比ARcore更准确
使用什么不同的方法？]]></description>
      <guid>https://stackoverflow.com/questions/63188856/how-arfaceanchor-determine-to-vertices-indices</guid>
      <pubDate>Fri, 31 Jul 2020 09:04:35 GMT</pubDate>
    </item>
    </channel>
</rss>