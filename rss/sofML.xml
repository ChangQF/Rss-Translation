<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 17 Jan 2024 03:16:15 GMT</lastBuildDate>
    <item>
      <title>随机获取.jpg文件的名称</title>
      <link>https://stackoverflow.com/questions/77829674/randomly-get-the-names-of-the-jpg-files</link>
      <description><![CDATA[我想随机获取.jpg文件的名称并分为2个文件：tranning.txt和test.txt。我使用的数据集是Caltech256、python和ggcolab。
我想知道是否有任何工具或方法可以帮助我快速完成此操作。
感谢您的关注]]></description>
      <guid>https://stackoverflow.com/questions/77829674/randomly-get-the-names-of-the-jpg-files</guid>
      <pubDate>Wed, 17 Jan 2024 02:25:26 GMT</pubDate>
    </item>
    <item>
      <title>改进多类分类模型？</title>
      <link>https://stackoverflow.com/questions/77829624/improving-multi-class-classification-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77829624/improving-multi-class-classification-model</guid>
      <pubDate>Wed, 17 Jan 2024 02:04:21 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 sigmoid 层会阻挡梯度？</title>
      <link>https://stackoverflow.com/questions/77829488/why-is-my-sigmoid-layer-blocking-gradients</link>
      <description><![CDATA[导入火炬
导入 torch.optim 作为 optim
将 torch.nn 导入为 nn

输入= torch.tensor([1.,2.],requires_grad=True)
sigmoid = nn.Sigmoid()

中间 = sigmoid(输入)

优化器 = optim.SGD([输入], lr=1, 动量=0.9)

对于范围（5）中的纪元：
    优化器.zero_grad()
    损失 = torch.linalg.vector_norm(interm - torch.tensor([2.,2.]))
    打印（纪元，损失，输入，中间）

    loss.backward(retain_graph=True)
    优化器.step()
    打印（期中毕业）

因此，我创建了这个简化的示例，其中输入进入 sigmoid 作为中间激活函数。
我正在尝试找到导致 interm = [2.,2.] 的输入
但是渐变没有通过。有谁知道为什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/77829488/why-is-my-sigmoid-layer-blocking-gradients</guid>
      <pubDate>Wed, 17 Jan 2024 01:13:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在 scikit-learn 中对线性回归模型使用交叉验证</title>
      <link>https://stackoverflow.com/questions/77829091/how-to-use-cross-validation-on-linear-regression-model-in-scikit-learn</link>
      <description><![CDATA[我想在 scikit learn 中使用网格搜索交叉验证进行训练线性回归模型可以说是 10 倍，就像我分享的图像中一样。
但是当我这样做时，我得到：
spipe = 管道([
    （&#39;缩放&#39;，StandardScaler（）），
    (&#39;模型&#39;, 线性回归())
]）

网格 = GridSearchCV(
    估计器=管道，
    简历=4
）

网格.fit(X,Y)

类型错误：GridSearchCV.__init__() 缺少 1 个必需参数：&#39;param_grid&#39;

所以我的理解是它想要迭代 LinearRegression 模型的可能参数，我应该将它们放入 param_grid 中。
但我不想为每次折叠调整参数。相反，我想简单地按照照片所示进行操作：进行 10 次折叠并对其进行 10 次训练和验证，以便模型微调 1 个线性回归多项式（我想这就是模型内部发生的情况）。
我尝试使用cross_val_score，但它似乎在 10 次折叠上训练 10 次，因为它返回 10 个分数而不是 1 个分数（所以我猜测 10 个线性回归多项式，每个折叠 1 个）。 
总而言之，如何将折叠交叉验证方法与线性回归结合使用？
如果有人需要，这里是设置：
从 sklearn.linear_model 导入 LinearRegression
从 sklearn.datasets 导入 fetch_california_housing
将 pandas 导入为 pd
从 sklearn.pipeline 导入管道
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.model_selection 导入 GridSearchCV

加利福尼亚州 = fetch_california_housing()

pd.set_option(&#39;显示.精度&#39;, 4)
pd.set_option(&#39;display.max_columns&#39;, 9)
pd.set_option(&#39;display.width&#39;, None)
california_df = pd.DataFrame(california.data,
                             列=加利福尼亚.feature_names）
california_df[&#39;MedHouseValue&#39;] = pd.Series(california.target)
X = 加利福尼亚州. 数据
Y = 加利福尼亚.目标
]]></description>
      <guid>https://stackoverflow.com/questions/77829091/how-to-use-cross-validation-on-linear-regression-model-in-scikit-learn</guid>
      <pubDate>Tue, 16 Jan 2024 22:44:50 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow ImportError：无法导入名称“dtensor”</title>
      <link>https://stackoverflow.com/questions/77828746/tensorflow-importerror-cannot-import-name-dtensor</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77828746/tensorflow-importerror-cannot-import-name-dtensor</guid>
      <pubDate>Tue, 16 Jan 2024 21:13:45 GMT</pubDate>
    </item>
    <item>
      <title>像 smote 或 adasyn 这样的过采样技术是否会将所有数据转换为单个类标签？</title>
      <link>https://stackoverflow.com/questions/77828073/does-a-oversampling-technique-like-smote-or-adasyn-convert-all-data-to-a-single</link>
      <description><![CDATA[我正在研究使用图像数据的深度学习模型。我的过采样器正在将所有样本转换为单个类。
最初我有 7909 个图像、2480 个 0 类图像和 5429 个 1 类图像。应用 smote 后（下面的代码）
#合成少数过采样技术
sm = SMOTE(采样策略=&#39;自动&#39;,random_state=56)

train_data_resampled, train_labels_resampled = sm.fit_resample(train_data.reshape(-1, IMG_SIZE * IMG_SIZE * 3), train_labels)

打印（train_data_resampled.shape，train_labels_resampled.shape）

我也尝试了adasyn（下面的代码）
从 imblearn.over_sampling 导入 ADASYN

adasyn = ADASYN(sampling_strategy=&#39;少数&#39;, random_state=89, n_neighbors=5)

train_data_resampled, train_labels_resampled = adasyn.fit_resample(train_data.reshape(-1, IMG_SIZE * IMG_SIZE * 3), train_labels)

打印（train_data_resampled.shape，train_labels_resampled.shape）

我的所有 train_labels 都转换为单类标签，即 0 类。任何人都可以帮忙吗？我正在 Kaggle 中编写我的代码。]]></description>
      <guid>https://stackoverflow.com/questions/77828073/does-a-oversampling-technique-like-smote-or-adasyn-convert-all-data-to-a-single</guid>
      <pubDate>Tue, 16 Jan 2024 18:45:59 GMT</pubDate>
    </item>
    <item>
      <title>仅支持一维向量将 MATLAB“double”转换为 Python</title>
      <link>https://stackoverflow.com/questions/77827946/converting-matlab-double-to-python-is-only-supported-for-1-dimensional-vectors</link>
      <description><![CDATA[源代码如下：
导入 py.sklearn.linear_model.LogisticRegression
导入 py.numpy.asarray


X = randn(100, 2);
Y = 兰迪([0, 1], 100, 1);

LR = py.sklearn. Linear_model.LogisticRegression(pyargs(&#39;class_weight&#39;, &#39;balanced&#39;, &#39;random_state&#39;, int64(0)));

模型 = LR.fit(X, Y);

错误：
使用 py.sklearn.linear_model._logistic.LogisticRegression/fit 时出错
仅支持一维向量将 MATLAB“double”转换为 Python。

错误 Untitled4（第 15 行）
模型 = LR.fit(X, Y);

如何在matlab中调用LR模型的拟合函数？]]></description>
      <guid>https://stackoverflow.com/questions/77827946/converting-matlab-double-to-python-is-only-supported-for-1-dimensional-vectors</guid>
      <pubDate>Tue, 16 Jan 2024 18:24:10 GMT</pubDate>
    </item>
    <item>
      <title>如何仅使用组件在 azure ml Designer 中训练和部署 ml 模型？</title>
      <link>https://stackoverflow.com/questions/77827691/how-to-train-and-deploy-ml-models-in-azure-ml-designer-just-using-components</link>
      <description><![CDATA[我在 azure ml Designer 中创建了一个训练管道。现在，我需要通过添加用于注册和部署的组件来部署此模型。我想我可以使用“执行 python 脚本”组件来执行此操作。但是我不知道如何将“训练的最佳模型”（“调整模型超参数”组件的输出）与“执行 python 脚本”组件连接起来。那么，知道如何完成这项任务吗？我将非常感谢您的帮助。
这是我的管道：
训练管道]]></description>
      <guid>https://stackoverflow.com/questions/77827691/how-to-train-and-deploy-ml-models-in-azure-ml-designer-just-using-components</guid>
      <pubDate>Tue, 16 Jan 2024 17:39:49 GMT</pubDate>
    </item>
    <item>
      <title>如何在提供 customGPT 之前处理大型 PDF 文件并构建数据 [关闭]</title>
      <link>https://stackoverflow.com/questions/77827516/how-to-process-large-pdf-file-and-structure-the-data-before-feeding-customgpt</link>
      <description><![CDATA[我正在学习机器学习和人工智能。我有大量 pdf、研究论文和书籍，我想将它们提供给 customGPT。我正在学习自然语言处理、标记化、词干提取等
我认为所有这些都可以通过编程方式完成，但我真正的问题是 - 在提取为“文本”后执行这些 PDF 文件吗？只有文件需要人工干预吗？我无法想象自己会浏览数千页，所以我对此表示怀疑。]]></description>
      <guid>https://stackoverflow.com/questions/77827516/how-to-process-large-pdf-file-and-structure-the-data-before-feeding-customgpt</guid>
      <pubDate>Tue, 16 Jan 2024 17:09:25 GMT</pubDate>
    </item>
    <item>
      <title>检测哪个图像不属于任何类 - 异常值 - Python</title>
      <link>https://stackoverflow.com/questions/77815892/detect-which-image-doesnt-belong-to-any-class-outlier-python</link>
      <description><![CDATA[我正在尝试用 python 编写一个程序，该程序将检测给定数据集中的哪个图像不属于任何类。
假设我们有一个包含 5 个类的数据集，每个类大约有 10 张图像：

汽车
城堡
数字
人
鱼

现在，在此数据集中有 3 个不属于任何类别的隐藏图像：

狗
球
坚持

我怎样才能制作一个程序来显示这三张图像？
我想就如何解决这个问题寻求建议。我不知道哪些 python 库可能有用，或者如果所使用的数据集中不存在这些类，我该如何创建自己的类？
这些类/图像可能不同，我只是给它们来解释问题。
我尝试基于 cifar10、cifar100 和 imagenet 制作自己的神经网络，但我发现它们给了我不需要的类，例如：我有 3 或 4 种不同类型的鱼，而不是 1 个名为“的类”鱼&#39;。
我了解到有一种叫做“异常值”的东西。我相信可以用非常不同的方式解决这个问题，但是我在互联网上没有找到任何对我有帮助的材料。]]></description>
      <guid>https://stackoverflow.com/questions/77815892/detect-which-image-doesnt-belong-to-any-class-outlier-python</guid>
      <pubDate>Sun, 14 Jan 2024 16:51:36 GMT</pubDate>
    </item>
    <item>
      <title>InvalidArgumentError：无法计算 MatMul，因为输入 #0（从零开始）预计是浮点张量，但实际上是双张量 [Op:MatMul]</title>
      <link>https://stackoverflow.com/questions/54255431/invalidargumenterror-cannot-compute-matmul-as-input-0zero-based-was-expected</link>
      <description><![CDATA[有人可以解释一下，TensorFlow 的 eager 模式是如何工作的吗？我正在尝试构建一个简单的回归，如下所示：
将张量流导入为 tf
将 numpy 导入为 np

tfe = tf.contrib.eager
tf.enable_eager_execution()

def make_model():
    net = tf.keras.Sequential()
    net.add(tf.keras.layers.Dense(4, 激活=&#39;relu&#39;))
    net.add(tf.keras.layers.Dense(1))
    回网

def计算损失（预测，实际）：
    返回 tf.reduce_mean(tf.square(tf.subtract(pred,actual)))

defcompute_gradient（模型，预测，实际）：
    “”“”用给定的噪声和输入“”“计算梯度”
    使用 tf.GradientTape() 作为磁带：
        损失=计算损失（预测，实际）
    grads = Tape.gradient(loss, model.variables)
    返回毕业生，损失

def apply_gradients（优化器，梯度，model_vars）：
    优化器.apply_gradients(zip(grads, model_vars))
    
模型 = make_model()
优化器 = tf.train.AdamOptimizer(1e-4)

x = np.linspace(0,1,1000)
y = x + np.random.normal(0,0.3,1000)
y = y.astype(&#39;float32&#39;)
train_dataset = tf.data.Dataset.from_tensor_slices((y.reshape(-1,1)))

纪元 = 2# 10
批量大小 = 25
itr = y.shape[0] # 批量大小
对于范围内的纪元（纪元）：
    对于 tf.contrib.eager.Iterator(train_dataset.batch(25)) 中的数据：
        preds = 模型（数据）
        梯度，损失=compute_gradient（模型，preds，数据）
        apply_gradients（优化器，梯度，模型.变量）
# 梯度输出：[无，无，无，无，无，无]

错误如下：
------------------------------------------------ ------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-3-a589b9123c80&gt;在&lt;模块&gt;中
     35 梯度，损失=compute_gradient（模型，preds，数据）
     36 打印（渐变）
---&gt; 37 apply_gradients（优化器，梯度，模型.变量）
     38 # 用 tf.GradientTape() 作为磁带：
     39 # 损失 = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(preds, data))))

&lt;ipython-input-3-a589b9123c80&gt;在 apply_gradients（优化器，梯度，model_vars）
     17 号
     18 def apply_gradients（优化器，梯度，model_vars）：
---&gt; 19 优化器.apply_gradients(zip(grads, model_vars))
     20
     21 模型 = make_model()

〜/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py 在 apply_gradients(self, grads_and_vars, global_step, name)
    第589章
    590 raise ValueError(“没有为任何变量提供梯度：%s。”%
--&gt;第591章
    第592章
    第593章

ValueError：没有为任何变量提供渐变：

编辑
我更新了我的代码。现在，问题出现在梯度计算中，它返回零。我检查过损失值不为零。]]></description>
      <guid>https://stackoverflow.com/questions/54255431/invalidargumenterror-cannot-compute-matmul-as-input-0zero-based-was-expected</guid>
      <pubDate>Fri, 18 Jan 2019 13:52:51 GMT</pubDate>
    </item>
    <item>
      <title>如何使随机森林分类器更快？</title>
      <link>https://stackoverflow.com/questions/43640546/how-to-make-randomforestclassifier-faster</link>
      <description><![CDATA[我正在尝试实现 kaggle 网站，包含大约 100 万个原始的 Twitter 情绪数据。我已经清理了它，但在最后一部分，当我将特征向量和情感应用于随机森林分类器时，它花费了很多时间。这是我的代码...
从 sklearn.ensemble 导入 RandomForestClassifier
森林 = RandomForestClassifier(n_estimators = 100,verbose=3)
森林 = Forest.fit( train_data_features, train[“情感”] )

train_data_features是1048575x5000稀疏矩阵。我尝试将其转换为数组，但这样做表明内存错误。
我哪里做错了？有人可以建议我一些来源或其他更快的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/43640546/how-to-make-randomforestclassifier-faster</guid>
      <pubDate>Wed, 26 Apr 2017 17:09:55 GMT</pubDate>
    </item>
    <item>
      <title>将复值图像输入神经网络</title>
      <link>https://stackoverflow.com/questions/40672094/feed-a-complex-valued-image-into-neural-network</link>
      <description><![CDATA[我正在努力“学习”一组大约 10 k 的复值输入图像（幅度/相位；实数/图像）与包含 48 个条目的实值输出向量之间的关系。该输出向量不是一组标签，而是一组数字，代表优化给定复值图像的视觉印象的最佳参数。这些参数是由算法生成的。数据中可能存在一些噪声（来自图像和生成参数向量的算法）
这些参数或多或少取决于输入图像的 FFT（快速傅里叶变换）。因此，我正在考虑用 FFT(complexImage) 的 1D 重塑版本为网络（5 个隐藏层，但架构现在不重要）提供数据 - 一些伪代码：
 // 离散化频谱
     obj_ft = fftshift(fft2(对象));
     
     obj_real_2d = 实数(obj_ft);
     obj_imag_2d = imag(obj_ft);
     
     // 将 2D 转换为 1D 行
     obj_real_1d = 重塑(obj_real_2d, 1, []);
     obj_imag_1d = 重塑(obj_imag_2d, 1, []);
     
     
     // 为 1d 对象创建复杂变量并连接
     obj_complx_1d(索引, :) = [obj_real_1d obj_imag_1d];
     
     opt_param_1D(索引, :) = get_opt_param(对象);
     

我想知道是否有更好的方法将复杂值图像输入深度网络。我想避免使用复杂的渐变，因为它并不是真的必要？！我“只是”尝试寻找“黑匣子”插入新图像后输出优化参数。
Tensorflow 获取输入：obj_complx_1d 和输出向量 opt_param_1D 进行训练。]]></description>
      <guid>https://stackoverflow.com/questions/40672094/feed-a-complex-valued-image-into-neural-network</guid>
      <pubDate>Fri, 18 Nov 2016 08:05:23 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 SVC 评分功能</title>
      <link>https://stackoverflow.com/questions/39735235/cant-get-svc-score-function-to-work</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/39735235/cant-get-svc-score-function-to-work</guid>
      <pubDate>Tue, 27 Sep 2016 22:22:08 GMT</pubDate>
    </item>
    <item>
      <title>解释自组织映射</title>
      <link>https://stackoverflow.com/questions/18233994/interpreting-a-self-organizing-map</link>
      <description><![CDATA[我一直在阅读有关自组织地图的内容，并且我理解该算法（我认为），但是仍然有一些事情让我困惑。
您如何解释经过训练的网络？
然后，您将如何实际使用它来执行分类任务（一旦您使用训练数据完成聚类）？
我似乎找到的所有材料（印刷版和数字版）都集中在算法的训练上。我相信我可能错过了一些重要的东西。]]></description>
      <guid>https://stackoverflow.com/questions/18233994/interpreting-a-self-organizing-map</guid>
      <pubDate>Wed, 14 Aug 2013 14:06:11 GMT</pubDate>
    </item>
    </channel>
</rss>