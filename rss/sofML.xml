<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 24 Oct 2024 18:22:36 GMT</lastBuildDate>
    <item>
      <title>时间序列预测：如何使用截至 t-1 计算的特征预测未来值而不会发生数据泄漏？</title>
      <link>https://stackoverflow.com/questions/79123110/time-series-forecasting-how-to-predict-future-values-using-features-calculated</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79123110/time-series-forecasting-how-to-predict-future-values-using-features-calculated</guid>
      <pubDate>Thu, 24 Oct 2024 17:39:23 GMT</pubDate>
    </item>
    <item>
      <title>在 Pytrees 上存储和 jax.vmap()</title>
      <link>https://stackoverflow.com/questions/79123001/storing-and-jax-vmap-over-pytrees</link>
      <description><![CDATA[我遇到了 Jax 的一个问题，如果我不解决它，我将不得不重写整个 20000 行的应用程序。
我有一个非 ML 应用程序，它依赖于 pytrees 来存储数据，并且 pytrees 很深 - 大约 6-7 层数据存储（class1 存储 class2，class2 存储 class3 的数组等）
我使用 python 列表来存储 pytrees 并希望对它们进行 vmap，但事实证明 jax 无法对列表进行 vmap。
（因此，一种解决方案是将每个数据类重写为结构化数组并从那里开始工作，可能将所有 6-7 层数据放入一个巨型数组中）
有没有办法避免重写？有没有办法将 pytree 类存储在 vmappable 状态，以便一切像以前一样工作？
如果有帮助的话，我的类已用 flax.struct.dataclass 标记。]]></description>
      <guid>https://stackoverflow.com/questions/79123001/storing-and-jax-vmap-over-pytrees</guid>
      <pubDate>Thu, 24 Oct 2024 17:07:05 GMT</pubDate>
    </item>
    <item>
      <title>将 Python 模型集成到 Power Bi</title>
      <link>https://stackoverflow.com/questions/79122864/integrate-python-model-to-power-bi</link>
      <description><![CDATA[我需要一些关于 Power bi 中 Python 的建议。
概述：- 在 power bi 中，我必须创建一个基于 NLP 的聊天机器人，就像我们在 power bi 中有 QnA visual 一样，它可以回答自然语言处理中提出的问题，但由于它的一些限制，我无法使用它。
我想使用 python 训练我的数据集并创建一个可以回答任何自然语言问题的模型。
但是问题是我无法将 python 脚本运行到 Power bi 中，因为它没有网关连接，所以它不会刷新，我不必在本地系统中执行此操作，只需将其发送给最终用户。任何我加载到 power bi 中的东西最终都会看起来像一张表格，我必须创建它的视觉效果。
我想知道以何种方式可以使用用 python 编写的 QnA 模型将 python 模型灌输到 power bi 中，以便用户可以用自然语言搜索他们想要问的问题。
有没有办法使用 python 让最终用户可以像聊天机器人一样输入他们的问题并得到他们的答案。
任何建议都会对我有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/79122864/integrate-python-model-to-power-bi</guid>
      <pubDate>Thu, 24 Oct 2024 16:24:05 GMT</pubDate>
    </item>
    <item>
      <title>如何在 ML.NET 中使用 CenterFace？模型预期形状为 (10, 3, 32, 32)</title>
      <link>https://stackoverflow.com/questions/79122749/how-to-use-centerface-in-ml-net-model-expects-shape-10-3-32-32</link>
      <description><![CDATA[我尝试在 ML.NET 中使用 CenterFace ONNX，但一直出现各种错误，主要是关于输入大小的错误。
CenterFace 元数据指出，它应该有一个 10, 3, 32, 32 的输入，这对于图像检测来说已经毫无意义了 - 为算法提供 10 个批次（每个批次 32x32 像素）有什么意义？
这是我的主要代码：
 string modelPath = &quot;centerface.onnx&quot;;
var mlContext = new MLContext();

string imagePath = &quot;photo1.jpg&quot;;

var img = Image.FromFile(imagePath);
var DH = (int)(Math.Ceiling((float)img.Height / 32) * 32);
var DW = (int)(Math.Ceiling((float)img.Width / 32) * 32);

var inputData = new[] { new ModelInput { ImagePath = imagePath } };
IDataView imageData = mlContext.Data.LoadFromEnumerable(inputData);

var pipeline = mlContext.Transforms.LoadImages(outputColumnName: &quot;input.1&quot;, imageFolder: &quot;&quot;, inputColumnName: nameof(ModelInput.ImagePath))
.Append(mlContext.Transforms.ResizeImages(outputColumnName: &quot;input.1&quot;, imageWidth: DW, imageHeight: DH))
.Append(mlContext.Transforms.ExtractPixels(outputColumnName: &quot;input.1&quot;))
.Append(mlContext.Transforms.ApplyOnnxModel(
outputColumnNames: [&quot;537&quot;, &quot;538&quot;, &quot;539&quot;, &quot;540&quot;],
inputColumnNames: [&quot;input.1&quot;],
modelFile: modelPath
));

var model = pipeline.Fit(imageData);
var predictionEngine = mlContext.Model.CreatePredictionEngine&lt;ModelInput, ModelOutput&gt;(mo​​del);
var prediction = predictionEngine.Predict(new ModelInput { ImagePath = imagePath });

使用我的 2 个模型类：
 public class ModelInput
{
public string ImagePath { get; set; }
}

public class ModelOutput
{
[ColumnName(&quot;537&quot;)] 
public float[] HeatMap { get; set; }

[ColumnName(&quot;538&quot;)]
public float[] Scale { get; set; }

[ColumnName(&quot;539&quot;)]
public float[] Offset { get; set; }

[ColumnName(&quot;540&quot;)]
public float[] Landmarks { get; set; }
}

但我确实一直收到有关输入大小的错误：

System.ArgumentException：“内存长度（3686400）必须与尺寸乘积（30720）匹配。”

30720 显然是 10x3x32x32。但同样，这有什么意义呢？
我认为我的 ONNX 坏了，但我确实有一个使用 OpenCVSharp 的工作实现：
// 这是计算机 DW 和 DH，与 ML.NET 示例中的方式相同
CenterFaceParams p = new(image, resizedSize.Width, resizedSize.Height, scoreThreshold, nmsThreshold);
Size size = new(p.DW, p.DH);

使用 Mat input = new();
Cv2.Resize(image, input, size);

使用 Mat blobInput = CvDnn.BlobFromImage(input, 1.0, size, new Scalar(0, 0, 0), true, false);
_net.SetInput(blobInput, &quot;input.1&quot;);

使用 (Mat heatMap = new())
使用 (Mat scale = new())
使用 (Mat offset = new())
使用 (Mat skylines = new())
{
_net.Forward([heatMap, scale, offset, skylines], [&quot;537&quot;, &quot;538&quot;, &quot;539&quot;, &quot;540&quot;]);

CenterFaceDecodercoder = new(heatMap, scale, offset, skylines, p);
returncoder.GetOutput();
}

这个实现给了我所有 4 个层，建模后我得到了我想要的值。]]></description>
      <guid>https://stackoverflow.com/questions/79122749/how-to-use-centerface-in-ml-net-model-expects-shape-10-3-32-32</guid>
      <pubDate>Thu, 24 Oct 2024 15:52:45 GMT</pubDate>
    </item>
    <item>
      <title>pytorch 的 mask r-cnn 推理 ONNX 模型从不起作用</title>
      <link>https://stackoverflow.com/questions/79122467/inferencing-onnx-model-of-pytorchs-mask-r-cnn-never-works</link>
      <description><![CDATA[问题：推理 onnx 模型给出空结果或形状奇怪的结果
我正在尝试：
pytorch 预训练 mask-rcnn -&gt; 在数据集上微调 -&gt; 另存为 onnx -&gt; 在 onnx 上推理 -&gt; 绘制结果
我目前拥有的一切都在推理之前有效。
我的 main.py 文件：https://pastebin.com/3jNfZdBi
获取有关我保存的 onnx 模型的信息
模型输入信息：
名称：输入
形状：[&#39;batch_size&#39;, 3, &#39;height&#39;, &#39;width&#39;]
类型：张量（浮点）

模型输出信息：
名称：boxes
形状：[&#39;Concatboxes_dim_0&#39;, 4]
类型：张量（浮点）
名称：labels
形状：[&#39;Gatherlabels_dim_0&#39;]
类型：张量（int64）
名称：分数
形状：[&#39;Gatherlabels_dim_0&#39;]
类型：张量（浮点）
名称：掩码
形状：[&#39;Unsqueezemasks_dim_0&#39;, &#39;Unsqueezemasks_dim_1&#39;, &#39;Unsqueezemasks_dim_2&#39;, &#39;Unsqueezemasks_dim_3&#39;]
类型：张量（浮点）

我的推理代码：https://pastebin.com/wxvp649G
我怀疑我要么：错误地将内容保存到 onnx，要么没有正确地预处理我的数据，要么我的推理代码是错误的（或其他我不知道的东西）
保存到 onnx 的代码
def save_model_onnx(models_file_path, model, torch_input):
#传统的导出方法。还有一种实验性的 dynamo_export 方法
torch.onnx.export(
model.cpu(),
torch_input.cpu(),
models_file_path, # 模型的完整路径，包括模型本身，即 ./models/model.onnx
export_params = True,
opset_version=15, # 选择支持的 ONNX opset 版本
do_constant_folding=True, # 折叠常量节点以进行优化
input_names = [&#39;input&#39;],
output_names = [&#39;boxes&#39;, &#39;labels&#39;, &#39;scores&#39;, &#39;masks&#39;],
dynamic_axes={
&quot;input&quot;: {0: &quot;batch_size&quot;, 2: &quot;height&quot;, 3: &quot;width&quot;}, 
}
)
logstash.info(f&quot;模型保存在{models_file_path}&quot;)

提前感谢大家]]></description>
      <guid>https://stackoverflow.com/questions/79122467/inferencing-onnx-model-of-pytorchs-mask-r-cnn-never-works</guid>
      <pubDate>Thu, 24 Oct 2024 14:41:59 GMT</pubDate>
    </item>
    <item>
      <title>机器学习的部署问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79121685/deployment-issues-on-machine-learning</link>
      <description><![CDATA[如何在没有 html 文件的情况下将 ML 模型部署到 github 操作中，我使用了 streamlit 应用程序？
我曾尝试找到一些使用 github 操作部署机器学习的源代码，但经常失败。
我有这个问题，该如何解决它。]]></description>
      <guid>https://stackoverflow.com/questions/79121685/deployment-issues-on-machine-learning</guid>
      <pubDate>Thu, 24 Oct 2024 11:31:36 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法将 MS COCO 2017 测试数据集中的图像分成包含小、中、大物体的图像？[关闭]</title>
      <link>https://stackoverflow.com/questions/79113120/is-there-a-way-to-segregate-images-in-ms-coco-2017-test-dataset-into-images-cont</link>
      <description><![CDATA[我正在研究在 MS COCO 测试数据集中分离包含小物体的图像。由于测试集没有注释，有什么方法可以完成此操作吗？
我尝试使用图像的宽度和高度参数，但在小图像部分只得到了 360 张图像。如何获取测试集的注释信息？]]></description>
      <guid>https://stackoverflow.com/questions/79113120/is-there-a-way-to-segregate-images-in-ms-coco-2017-test-dataset-into-images-cont</guid>
      <pubDate>Tue, 22 Oct 2024 08:33:45 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中实现贝叶斯逻辑回归和 SVM 进行分类</title>
      <link>https://stackoverflow.com/questions/79098217/how-to-implement-bayesian-logistic-regression-and-svm-for-classification-in-pyth</link>
      <description><![CDATA[我想在单个 Python 代码中实现贝叶斯逻辑回归和 SVM 进行分类，但我无法做到，请任何人告诉我。
我尝试使用 pymc3，但它不起作用。我也尝试过 theano tensor，但都没有用，有人能帮我解决这些吗]]></description>
      <guid>https://stackoverflow.com/questions/79098217/how-to-implement-bayesian-logistic-regression-and-svm-for-classification-in-pyth</guid>
      <pubDate>Thu, 17 Oct 2024 13:04:09 GMT</pubDate>
    </item>
    <item>
      <title>OpenCV 与 OpenVINO 后端：动态批次大小问题</title>
      <link>https://stackoverflow.com/questions/79097169/opencv-with-openvino-backend-problem-whit-dynamic-batch-size</link>
      <description><![CDATA[我正在使用 OpenCV（版本 4.10.0）和 OpenVINO（2023.0.1）后端编译来加载和处理深度学习模型。我已使用 ovc 和 omz_downloader 成功将模型从 Open Model Zoo 转换为 OpenVINO IR 格式。转换工作正常，但在将模型导入 OpenCV 进行推理时遇到了问题。
问题：
模型使用动态批处理大小（[-1, 3, 112, 112]）进行转换。当我尝试使用 cv::dnn::readNetFromModelOptimizer() 函数在 OpenCV 中加载此模型时，我在 OpenCV 源代码的这一部分中收到异常：
NetImplOpenVINO::createNetworkFromModelOptimizer(std::shared_ptr&lt;ov::Model&gt;&amp; ieNet) 函数
{
....
for (auto&amp; it : ieNet-&gt;get_parameters())
{
inputNames.push_back(it-&gt;get_friendly_name());
std::vector&lt;size_t&gt; dims = it-&gt;get_shape(); // 此处发生异常
inp_shapes.push_back(std::vector&lt;int&gt;(dims.begin(), dims.end()));
}
.....
}

在 OpenCV 代码中调用 it-&gt;get_shape() 时发生异常，可能是因为模型具有动态形状。
问题：
使用 OpenVINO 后端时，如何在 OpenCV 的 DNN 模块中处理具有动态批处理大小的模型？
是否有在 OpenCV 中加载具有动态输入形状的模型的解决方法，还是应该直接使用 OpenVINO 的推理引擎管理动态批处理？
环境：
OpenCV 4.10.0
OpenVINO 2023.1
Windows 11
环境 c++

静态批处理大小：我使用静态批处理大小 ([1, 3, 112, 112]) 转换了模型，并且它运行良好。但是，我需要为我的应用程序处理动态批次大小。
后端设置：我正在使用以下方法将后端设置为 OpenVINO：

`net.setPreferableBackend(cv::dnn::DNN_BACKEND_INFERENCE_ENGINE);
net.setPreferableTarget(cv::dnn::DNN_TARGET_CPU);`

任何有关使用 OpenVINO 处理 OpenCV 中的动态批次大小的帮助或见解都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79097169/opencv-with-openvino-backend-problem-whit-dynamic-batch-size</guid>
      <pubDate>Thu, 17 Oct 2024 08:22:34 GMT</pubDate>
    </item>
    <item>
      <title>通过索引为张量赋值后，值不匹配</title>
      <link>https://stackoverflow.com/questions/78949501/mismatch-of-values-after-assigning-values-to-a-tensor-by-index</link>
      <description><![CDATA[我正在编写一个 PyTorch 训练代码，它构建了一个算法类。其中有一个步骤需要为内部张量分配一些值。但是，即使代码只有两行，也有一个错误。我发现分配的张量的值与分配的值不同。
这是该类的代码：
class PRODEN(Algorithm):
&quot;&quot;&quot;
PRODEN
参考：部分标签学习的真实标签的渐进式识别，ICML 2020。
&quot;&quot;&quot;

def __init__(self, input_shape, train_givenY, hparams):
super(PRODEN, self).__init__(input_shape, train_givenY, hparams)
self.featurizer = networks.Featurizer(input_shape, self.hparams)
self.classifier = networks.Classifier(
self.featurizer.n_outputs,
self.num_classes)

self.network = nn.Sequential(self.featurizer, self.classifier)
self.optimizer = torch.optim.Adam(
self.network.parameters(),
lr=self.hparams[&quot;lr&quot;],
weight_decay=self.hparams[&#39;weight_decay&#39;]
)
train_givenY = torch.from_numpy(train_givenY)
tempY = train_givenY.sum(dim=1).unsqueeze(1).repeat(1, train_givenY.shape[1])
label_confidence = train_givenY.float()/tempY
self.label_confidence = label_confidence

def update(self, minibatches):
_, x, strong_x, partial_y, _, index = minibatches
loss = self.rc_loss(self.predict(x), index)
self.optimizer.zero_grad()
loss.backward()
self.optimizer.step()
self.confidence_update(x, partial_y, index)
return {&#39;loss&#39;: loss.item()}

def rc_loss(self, output, index):
device = &quot;cuda&quot; if index.is_cuda else &quot;cpu&quot;
self.label_confidence = self.label_confidence.to(device)
logsm_outputs = F.log_softmax(outputs, dim=1)
#print(self.label_confidence.is_cuda)
final_outputs = logsm_outputs * self.label_confidence[index, :]
average_loss = - ((final_outputs).sum(dim=1)).mean()
return average_loss

def predict(self, x):
return self.network(x)

def confidence_update(self, batchX, batchY, batch_index):
with torch.no_grad():
batch_outputs = self.predict(batchX)
temp_un_conf = F.softmax(batch_outputs, dim=1)
&#39;&#39;&#39;有问题的代码开始了&#39;&#39;&#39;
self.label_confidence[batch_index, :] = temp_un_conf * batchY # un_confidence 存储每个示例的权重
&#39;&#39;&#39;问题代码结束&#39;&#39;&#39;
base_value = self.label_confidence.sum(dim=1).unsqueeze(1).repeat(1, self.label_confidence.shape[1])
self.label_confidence = self.label_confidence / base_value

问题出在 confidence_update 上。我发现
self.label_confidence[batch_index, :]

的值与
temp_un_conf * batchY

在此分配之后
self.label_confidence[batch_index, :] = temp_un_conf * batchY

仅适用于少数示例，但适用于大多数示例。例如，对于 1024 的批次大小，第一次迭代时大约有 4 个示例，之后会变得更大。我对这个问题非常沮丧，尝试了很多方法：

这个问题只存在于 CIFAR10，但其他数据集不存在。

所有张量的数据类型都是 Float32。

所有张量都在 gpu 上。


我的代码有什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/78949501/mismatch-of-values-after-assigning-values-to-a-tensor-by-index</guid>
      <pubDate>Wed, 04 Sep 2024 15:41:44 GMT</pubDate>
    </item>
    <item>
      <title>Keras 的 one_hot 对不同的词产生相同的值</title>
      <link>https://stackoverflow.com/questions/78626998/one-hot-from-keras-producing-the-same-value-for-different-words</link>
      <description><![CDATA[我使用 keras 的 one_hot 函数将单词转换为数字。但出于某种原因，它会为不同的单词生成相同的数字。在下面的代码中，您可以看到 48 用于“amazing”，但 48 也用于“too”。这是为什么？
from tensorflow.keras.preprocessing.text import one_hot

reviews = [&#39;nice food&#39;,
&#39;amazing restaurant&#39;,
&#39;too good&#39;,
&#39;just loved it!&#39;,
&#39;will go again&#39;,
&#39;horrible food&#39;,
&#39;never go there&#39;,
&#39;poor service&#39;,
&#39;poor quality&#39;,
&#39;needs Improvement&#39;]

# 转换为 ont hot 向量 
encoded_reviews = [one_hot(d, vocab_size) for d in reviews]

当我打印coded_reviews 时，它显示：
[[13, 12],
[48, 44],
[48, 19],
[38, 28, 46],
[13, 29, 19],
[46, 12],
[19, 29, 4],
[18, 38],
[18, 35],
[42, 7]]
]]></description>
      <guid>https://stackoverflow.com/questions/78626998/one-hot-from-keras-producing-the-same-value-for-different-words</guid>
      <pubDate>Sat, 15 Jun 2024 15:16:01 GMT</pubDate>
    </item>
    <item>
      <title>在 PyCharm 中运行 Teachable Machines 对象识别器</title>
      <link>https://stackoverflow.com/questions/78596363/running-teachable-machines-object-recognizer-in-pycharm</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78596363/running-teachable-machines-object-recognizer-in-pycharm</guid>
      <pubDate>Sat, 08 Jun 2024 16:51:36 GMT</pubDate>
    </item>
    <item>
      <title>内核形状必须与输入具有相同的长度，但接收形状为 A 的内核和形状为 B 的输入</title>
      <link>https://stackoverflow.com/questions/78287794/kernel-shape-must-have-the-same-length-as-input-but-received-kernel-of-shape-a</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78287794/kernel-shape-must-have-the-same-length-as-input-but-received-kernel-of-shape-a</guid>
      <pubDate>Sun, 07 Apr 2024 12:27:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 c++ 通过 onnx 和 opencv 制作超分辨率图像？</title>
      <link>https://stackoverflow.com/questions/77328225/how-to-do-super-resolution-image-with-onnx-and-opencv-using-c</link>
      <description><![CDATA[我的超分辨率 ios/macos 应用程序有 animesr.onnx，所以我需要将我的 python 代码转换为 c++ 代码。这些是我的 python 代码：
session = onnxruntime.InferenceSession(&#39;animesr.onnx&#39;)

img = cv2.imread(&#39;imgs/naruto.jpg&#39;)
ori_h, ori_w, _ = img.shape
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img = cv2.resize(img, (512,512))
img = (np.array(img) / 255.0).astype(np.float32)
img = np.transpose(img, (2, 0, 1))
img = np.expand_dims(img, 0)

input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name

input_feed = {input_name: img}
output = session.run([output_name], input_feed)

output = output[0].clip(0, 1) * 255
output = output.astype(np.uint8)
output = np.squeeze(output)
output = np.transpose(output, (1, 2, 0))
output = cv2.cvtColor(output, cv2.COLOR_RGB2BGR)
output = cv2.resize(output, (ori_w*4, ori_h*4))
cv2.imwrite(&#39;naruto_animesr.jpg&#39;, output)

它工作正常，结果如下（左边是之前，右边是之后）

这是我的 c++ 代码：
Ort::Session session(env, ORT_TSTR(modelPath), sessionOptions);

cv::Mat inputImage = cv::imread(imagePath, cv::IMREAD_COLOR);
// cv::Mat blob = cv::dnn::blobFromImage(inputImage, 1.0/255, cv::Size(512,512), cv::Scalar(), true);
cv::Mat resizedImage;
cv::resize(inputImage, resizedImage, cv::Size(512,512));
cv::Mat floatImage;
resizedImage.convertTo(floatImage, CV_32FC3, 1.0/255.0);

Ort::MemoryInfo memoryInfo = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
std::vector&lt;int64_t&gt; inputShape = {1, 3, 512, 512};

Ort::Value inputTensor = Ort::Value::CreateTensor&lt;float&gt;(memoryInfo, (float*) floatImage.data, 3*512*512, inputShape.data(), inputShape.size());

std::vector&lt;const char*&gt; inputNames = {&quot;input&quot;};
std::vector&lt;const char*&gt; outputNames = {&quot;output&quot;};

std::vector&lt;Ort::Value&gt; outputTensor = session.Run(Ort::RunOptions{}, inputNames.data(), &amp;inputTensor, 1, outputNames.data(), outputNames.size());

Ort::TensorTypeAndShapeInfo outputInfo = outputTensor[0].GetTensorTypeAndShapeInfo();
int channels = outputInfo.GetShape()[1]; // 3
int height = outputInfo.GetShape()[2]; // 2048
int width = outputInfo.GetShape()[3]; // 2048

const float* outputData = outputTensor[0].GetTensorMutableData&lt;float&gt;();

cv::Mat outputImage(height, width, CV_32FC(channels), const_cast&lt;float*&gt;(outputData));
cv::Mat uint8Image;
outputImage.convertTo(uint8Image, CV_8UC3, 255);
cv::Mat bgrOutput;
cv::cvtColor(uint8Image, bgrOutput, cv::COLOR_RGB2BGR);
cv::imwrite(outputPath, bgrOutput);

这是结果

这是使用 cv::ddn::blobFromImage() 的结果

两者都是糟糕的结果，与我的 python 结果不同。
我认为问题在于在运行模型之前将图像作为输入进行预处理，并将输出处理为 cv 图像，我不擅长矩阵运算，如变换、转置，重塑等。
您能帮我提供使用 opencv c++ 的正确处理图像代码吗？]]></description>
      <guid>https://stackoverflow.com/questions/77328225/how-to-do-super-resolution-image-with-onnx-and-opencv-using-c</guid>
      <pubDate>Fri, 20 Oct 2023 03:48:27 GMT</pubDate>
    </item>
    <item>
      <title>将卫星图像与地图匹配</title>
      <link>https://stackoverflow.com/questions/74901142/matching-satellite-images-to-map</link>
      <description><![CDATA[我目前有点被一个问题难住了，听起来比实际容易（至少对我来说）：
假设你有从低地球轨道（LEO）拍摄的卫星图像，显示大约 1000 公里宽的区域（相机的图像轴或多或少垂直于地面）。图像中没有存储其他位置数据，因此无法直接提取拍摄图像的位置）。
我想要做的是编写一个程序（用 Python），可以通过将其与地球地图进行匹配来找到拍摄图像的位置。这应该自动完成（或多或少是实时的），以便计算拍摄图像的卫星的轨道。
一旦我有位置数据（即使非常嘈杂），使用基于扩展卡尔曼滤波器的技术，我可以毫无问题地计算轨道。
另一方面，仅使用图像数据将卫星图像与地球地图相匹配......老实说，我甚至不知道从哪里开始。
我知道这是一个非常不具体的问题，与特定问题无关，但也许有人可以给我指出正确的方向......
编辑：
只是为了让你了解未经处理的低地球轨道图像是什么样子，我附上了一些在地球轨道上拍摄的相当不错的图像。




图片是用 NIR 相机拍摄的。我所包含图片的分辨率只有 640x480（错误！），但图片分辨率应该在 4k 左右。
这些图片有一些瑕疵，因为它们是通过国际空间站的厚玻璃窗拍摄的 - 所以那里有一些反射……]]></description>
      <guid>https://stackoverflow.com/questions/74901142/matching-satellite-images-to-map</guid>
      <pubDate>Fri, 23 Dec 2022 15:08:14 GMT</pubDate>
    </item>
    </channel>
</rss>