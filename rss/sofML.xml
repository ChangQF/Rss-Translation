<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 19 Mar 2024 03:14:30 GMT</lastBuildDate>
    <item>
      <title>TF2 和 python 中的 BERT 预处理器模型存在问题</title>
      <link>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</link>
      <description><![CDATA[我正在尝试使用 BERT 来做一个文本分类项目。但是我一直遇到这个错误
`
ValueError Traceback（最近一次调用最后一次）
单元格 In[37]，第 4 行
      2 text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
      3 bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
----&gt; 4 preprocessed_text = bert_preprocess(text_input)
      5 bert_encoder = hub.KerasLayer(encoder_url,
      6 可训练=真，
      7 名称=&#39;BERT_编码器&#39;)
      8 个输出 = bert_encoder(preprocessed_text)
ValueError：调用层“预处理”时遇到异常（类型 KerasLayer）。
KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。

调用层“预处理”接收的参数（类型 KerasLayer）：
  输入=
  • 培训=无

KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。
调用层“预处理”接收的参数（类型 KerasLayer）：
• 输入=
• 训练=无`
构建此模型时：
&lt;前&gt;&lt;代码&gt;
preprocess_url = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3&#39;
编码器网址 = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/bert-en-uncased-l-12-h-768-a-12/versions/2&#39;

# Bert 层
text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
预处理文本 = bert_preprocess(text_input)
bert_encoder = hub.KerasLayer(encoder_url,
                              可训练=真，
                              名称=&#39;BERT_编码器&#39;)
输出= bert_encoder（预处理文本）

# 神经网络层
l = tf.keras.layers.Dropout(0.1)(输出[&#39;pooled_output&#39;])
l = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;, name=&#39;输出&#39;)(l)

# 构建最终模型
模型 = tf.keras.Model(输入=[text_input], 输出=[l])

我看过无数的教程，甚至使用了张量流文档上的教程，即使我复制和粘贴，它们仍然不起作用。我尝试过不同版本的 tf、tf-text 和 tf-hub。我在这个项目中使用了tensorflow-gpu-jupyter docker 容器。
这是我安装库的方法：
!pip install “tensorflow-text”
!pip install “tf-models-official”
!pip install “tensorflow-hub”

版本是：
张量流：2.16.1
张量流文本：2.16.1
张量流中心：0.16.1
我看到的有关此问题的所有其他论坛都说要执行 tf.config.run_functions_eagerly(True) 但这不起作用。
任何事情都会有所帮助。如果您知道如何解决请回答。]]></description>
      <guid>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</guid>
      <pubDate>Tue, 19 Mar 2024 01:42:01 GMT</pubDate>
    </item>
    <item>
      <title>分割接触谷物的最快方法：ML 模型还是传统图像处理？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78183577/fastest-approach-for-segmenting-touching-cereal-grains-ml-model-or-traditional</link>
      <description><![CDATA[对于分割接触谷物的图像，最快的方法是什么：训练 ML 模型还是使用传统图像处理工具开发算法？
我想分别获取每个颗粒的图像。
示例图像如下：
米粒]]></description>
      <guid>https://stackoverflow.com/questions/78183577/fastest-approach-for-segmenting-touching-cereal-grains-ml-model-or-traditional</guid>
      <pubDate>Mon, 18 Mar 2024 23:50:14 GMT</pubDate>
    </item>
    <item>
      <title>java中异常检测的最佳算法或库[关闭]</title>
      <link>https://stackoverflow.com/questions/78183122/best-algorithm-or-library-for-anomaly-detection-in-java</link>
      <description><![CDATA[我们正在做一个项目，尝试测量数据集中的异常情况，其中有我们公司的员工（大约 1000 名），并且每个人都拥有进入许多门的徽章。我们拥有完整的用户数据，例如职位、部门、职位代码、经理等。
每个人可能可以进入 10-100 扇门，我们正在尝试编写一个 java 程序来找到奇怪的人。可能是这样的情况，比如你是唯一一个拥有 5 号门的头衔的人，或者可能很多为 A 经理工作的人都有 10 号门，但为 B 经理工作的这个人也有它。
我们看到的大多数非结构化学习的例子都有数值。我们不这样做。事实上，像系统管理员和高级系统管理员这样的类似头衔是“接近”的。所以可能应该以某种方式考虑到它们是相似的。因此，如果一位 SR 还拥有 7 号门和 5 号门系统管理员，那可能并不是什么奇怪的事情。
理想的结果是列出似乎不合适的人-门组合。也许是一个数字置信值。门本身是一个固定值。但标准是诸如头衔、职位代码或经理层级（向上一级或四级）之类的词。
看来我们需要首先评估用户参数（如标题、部门）中的语言相似性，然后使用这些多值来运行门统计数据。
这似乎是机器学习应该能够做到的事情，但我们在这方面有点新手。有人有开源 java 库、想法或教程的链接吗？尤其是如何链接“相似”的内容。头衔或经理层次结构放在一起，以免将一个人的团队标记为完全异常。]]></description>
      <guid>https://stackoverflow.com/questions/78183122/best-algorithm-or-library-for-anomaly-detection-in-java</guid>
      <pubDate>Mon, 18 Mar 2024 21:35:09 GMT</pubDate>
    </item>
    <item>
      <title>用于西班牙语文本校正的人工智能模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78182982/ai-models-for-text-correction-in-spanish</link>
      <description><![CDATA[我们需要实现不同的人工智能模型来将音频转录为文本，然后将该文本转换为特定的 Word 格式。目前，这是使用从 Python 中的 Hugging Face 获得的模型来完成的，但我正在尝试找到一个可以让我纠正音频转录中的不准确之处的模型，因为这些模型往往非常字面意思以及说话者犯下的任何发音错误或错误保留在正文中。
所以，我需要一个带有人工智能的校正功能，可以进行西班牙语文本校正。我一直在寻找一些，但大部分都是英文的。您是否知道有任何免费或付费模型可以实现此目的？
我寻找了一些人工智能训练模型，但修正是英文的，我需要一个西班牙文的。我还在考虑为这项任务训练我自己的 T5 模型。]]></description>
      <guid>https://stackoverflow.com/questions/78182982/ai-models-for-text-correction-in-spanish</guid>
      <pubDate>Mon, 18 Mar 2024 20:58:11 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn 转换器输出返回更多列，其中一些列没有进行转换</title>
      <link>https://stackoverflow.com/questions/78182162/sklearn-transformer-output-returns-more-columns-with-some-columns-not-having-the</link>
      <description><![CDATA[我正在构建一个 scikit-learn 管道。我从在线机器学习存储库下载了一个数据集并为其生成了描述性统计数据。我正在使用此处找到的processed.cleveland.data数据集： https://archive .ics.uci.edu/dataset/45/heart+disease。
我手动添加了列名称，并根据需要将数字转换为字符串。我将 DataFrame 转换为 Numpy 数组以分隔预测变量和目标变量。之后，我检索管道的数字和分类变量列表。
我开发管道，然后总结 DataFrame。
这样做的结果是我没有生成额外的列。除了 OneHotEncoder 生成的列之外，为什么还有额外的列？
理想情况下，我的输出将包含来自原始数据集的相同数量的列，其中包含转换（简单输入器）以及 OneHotEncoder 为分类变量生成的列。标准化列仍包含空值，而数据集中的原始列包含中位数。
有人可以让我知道这些问题吗？
导入 pandas 作为 pd
将 numpy 导入为 np
导入操作系统
从 pathlib 导入路径

从 sklearn.compose 导入 ColumnTransformer
从 sklearn.pipeline 导入管道
从 sklearn.preprocessing 导入 OneHotEncoder
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.impute 导入 SimpleImputer

网址 = ...
名称 = [&#39;年龄&#39;, &#39;性别&#39;, &#39;cp&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;exang&#39;,
&#39;oldpeak&#39;、&#39;slope&#39;、&#39;ca&#39;、&#39;thal&#39;、&#39;num&#39;]

def getData():
    返回 pd.read_csv(url, sep=&#39;,&#39;, 名称=名称)

输入 = 获取数据()
打印（输入.info（））
打印（输入.描述（））

数组=输入.值
X = 数组[:,0:13]
y = 数组[:,13]

dataframe = pd.DataFrame.from_records(X)
数据帧[[1, 2, 5, 6, 8]] = 数据帧[[1, 2, 5, 6, 8]].astype(str)


numeric = dataframe.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns
categorical = dataframe.select_dtypes(include=[&#39;object&#39;, &#39;bool&#39;]).columns

打印（数字）
打印（分类）

t = [(&#39;cat0&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;), [1, 2, 5, 6, 8]), (&#39;cat1&#39;,
OneHotEncoder()，分类），（&#39;num0&#39;，SimpleImputer（strategy=&#39;median&#39;），数值），（&#39;num1&#39;，
MinMaxScaler()，数值）]
column_transforms = ColumnTransformer(transformers=t)

管道=管道(步骤=[(&#39;t&#39;,column_transforms)])
结果 = pipeline.fit_transform(dataframe)

打印（类型（pd.DataFrame.from_records（结果）））
打印（pd.DataFrame.from_records（结果）.to_string（））``

我期望 DataFrame 以相同的顺序返回（使用 SimpleImputer 和 StandardScaler）以及 OneHotEncoder 创建的新变量。]]></description>
      <guid>https://stackoverflow.com/questions/78182162/sklearn-transformer-output-returns-more-columns-with-some-columns-not-having-the</guid>
      <pubDate>Mon, 18 Mar 2024 17:57:04 GMT</pubDate>
    </item>
    <item>
      <title>从哪里可以找到 ML 神经网络的代码？</title>
      <link>https://stackoverflow.com/questions/78181767/from-where-i-can-find-the-codes-for-ml-neural-network</link>
      <description><![CDATA[我想在 Python 上研究机器学习神经网络，以预测遗传相互作用。我正在寻找有关编码和从公共数据生成结果的实践学习经验。
我发现的大多数都是商业或理论信息书籍，没有实用代码。如果您有任何消息来源可以指导我完成此操作，我真的很感激。]]></description>
      <guid>https://stackoverflow.com/questions/78181767/from-where-i-can-find-the-codes-for-ml-neural-network</guid>
      <pubDate>Mon, 18 Mar 2024 16:50:29 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn train_test_split 是否复制数据？</title>
      <link>https://stackoverflow.com/questions/78181518/does-scikit-learn-train-test-split-copy-data</link>
      <description><![CDATA[是否train_test_split&lt; /a&gt; scikit-learn 的方法复制数据？换句话说，如果我使用大型数据集 X, y，这是否意味着在执行类似
X_train，X_test，y_train，y_test = train_test_split（X，y，test_size = 0.2，random_state = 2023）

我的数据使用的内存是原始数据集的两倍？或者是否有一些 scikit-learn （或基本的 python）魔法可以阻止它？ （例如，使用  .to_numpy()并不一定会导致数据重复)
如果内存使用量翻倍，解决此问题的最佳实用方法是什么？也许，类似
X、X_test、y、y_test = train_test_split(X、y、test_size=0.2、random_state=2023)

？
备注
使用 np.shares_memor(X_train, X) 表明数据确实是重复的。]]></description>
      <guid>https://stackoverflow.com/questions/78181518/does-scikit-learn-train-test-split-copy-data</guid>
      <pubDate>Mon, 18 Mar 2024 16:08:22 GMT</pubDate>
    </item>
    <item>
      <title>“内存分配失败”的原因是什么</title>
      <link>https://stackoverflow.com/questions/78180875/whats-the-reason-of-memory-allocate-failed</link>
      <description><![CDATA[我创建了一个简单的逻辑回归类，其中包含一个gradient_ascent 函数。当我尝试使用它时，我的 IDE 会引发如下错误
无法为形状为 (86918, 86918) 且数据类型为 float64 的数组分配 56.3 GiB

我的数据大小是(86918,10)，标签大小是(86918,1)。我调试的时候发现在gradient_ascent函数中执行代码error = y_predicted - y时程序退出并报错。但是，我找不到原因。以下是我的代码。
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.metrics 导入 precision_score
从 sklearn.model_selection 导入 train_test_split


定义 sigmoid(z):
    返回 1 / (1 + np.exp(-z))


逻辑回归类：
    def __init__(self,learning_rate=0.01,number_iterations=1000,method=&#39;gradient_ascent&#39;):
        自我学习率 = 学习率
        self.number_iterations = number_iterations
        自重=无
        自我偏见=无
        self.method = 方法

    defgradient_ascent(自身, X, y):
        num_samples, num_features = X.shape
        self.weights = np.zeros((num_features, 1))
        自我偏见 = 0

        for _ in range(self.number_iterations):
            线性模型 = np.dot(X, self.weights) + self.bias
            y_预测 = sigmoid(线性模型)
            误差 = y_预测 - y
            dw = (1 / num_samples) * np.dot(X.T, 错误)
            db = (1 / num_samples) * np.sum(错误)

            self.weights += self.learning_rate * dw
            self.bias += self.learning_rate * db

    def stochastic_gradient_ascent(自身, X, y):
        num_samples, num_features = X.shape
        self.weights = np.zeros((num_features, 1))
        自我偏见 = 0

        for _ in range(self.number_iterations):
            对于范围内的 i（num_samples）：
                线性模型 = np.dot(X[i], self.weights) + self.bias
                y_预测 = sigmoid(线性模型)
                误差 = y_预测 - y[i]

                dw = X[i] * 误差
                数据库=错误

                self.weights += self.learning_rate * dw
                self.bias += self.learning_rate * db

    def fit(自身, X, y):
        如果 self.method == &#39;gradient_ascent&#39;:
            self.gradient_ascent(X, y)
        别的：
            self.stochastic_gradient_ascent(X, y)

    def 预测（自身，X）：
        y_predicted = sigmoid(np.dot(X, self.weights) + self.bias)
        y_predicted = [1 如果 i &gt; 0.5 else 0 for i in y_predicted]
        返回 y_预测值


路径=&#39;../data/KaggleCredit2.csv&#39;
数据= pd.read_csv（路径，index_col = 0）
data.dropna（轴= 0，就地= True）
y = 数据[&#39;SeriousDlqin2yrs&#39;]
X = data.drop(标签=&#39;SeriousDlqin2yrs&#39;, 轴=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

lr1 = 逻辑回归()
lr1.fit(X_train, y_train)
预测1 = lr1.预测(X_test)
lr1_score = precision_score(y_test, 预测1)

我尝试过查阅GPT并逐行调试代码，但仍然找不到错误。如果有人能给我一些指导，我将不胜感激。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78180875/whats-the-reason-of-memory-allocate-failed</guid>
      <pubDate>Mon, 18 Mar 2024 14:22:18 GMT</pubDate>
    </item>
    <item>
      <title>尝试复制一种收敛权重的算法，以近似合作差分博弈系统的联合成本函数</title>
      <link>https://stackoverflow.com/questions/78180810/trying-to-replicate-an-algorithm-that-converges-weights-for-approximate-a-joint</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78180810/trying-to-replicate-an-algorithm-that-converges-weights-for-approximate-a-joint</guid>
      <pubDate>Mon, 18 Mar 2024 14:10:32 GMT</pubDate>
    </item>
    <item>
      <title>预测客户的下一个购买日[关闭]</title>
      <link>https://stackoverflow.com/questions/78179087/predicting-customers-next-purchase-day</link>
      <description><![CDATA[我正在开发一个项目，需要预测客户的下一个购买日。我有一个数据集，其中包括客户购买历史记录、RFM（新近度、频率、货币）分数以及其他功能，例如：
最近三次购买之间的天数，
均值&amp;购买天数差异的标准差
您能否建议我可以用于此预测任务的最有效的模型或方法？我正在寻找一种可以利用这些功能以及数据的时间序列性质来提供准确预测的方法。
任何建议或见解将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78179087/predicting-customers-next-purchase-day</guid>
      <pubDate>Mon, 18 Mar 2024 09:17:12 GMT</pubDate>
    </item>
    <item>
      <title>为用户创建推荐系统 API 时出现问题</title>
      <link>https://stackoverflow.com/questions/78177747/issue-while-creating-an-api-for-recommendation-system-for-users</link>
      <description><![CDATA[fromflask导入Flask，jsonify，request
将 pandas 导入为 pd
将 numpy 导入为 np
从 sklearn.metrics.pairwise 导入 cosine_similarity
导入作业库

应用程序=烧瓶（__名称__）

# 加载预训练的用户-用户相似度矩阵
user_user_similarity_matrix = joblib.load(&#39;预测模型/user_user_similarity_matrix.pkl&#39;)

# 从 Excel 文件加载数据集（API 功能不需要此行）
data = pd.read_excel(&#39;预测模型/DDDD.xlsx&#39;)

# 将占位符列“PurchasedYes”添加到数据 DataFrame
数据[&#39;已购买&#39;] = 1

@app.route(&#39;/推荐&#39;, 方法=[&#39;POST&#39;])
def 推荐():
    user_id = request.json[&#39;user_id&#39;] # 从请求中获取用户ID
    推荐=generate_recommendations（user_user_similarity_matrix，数据，user_id，n_recommendations = 10）
    return jsonify({&#39;user_id&#39;: user_id, &#39;推荐&#39;: 推荐})


defgenerate_recommendations(user_similarity_matrix, data, user_id, n_recommendations=10):
    # 透视表以获得矩阵，其中行代表客户，列代表项目
    customer_item_matrix = data.pivot_table(index=&#39;客户&#39;, columns=&#39;SalesItem&#39;, value=&#39;PurchasedYes&#39;, fill_value=0)

    # 获取用户购买的商品
    user_purchases = customer_item_matrix.loc[user_id].values.reshape(1, -1) # 重塑为行向量

    # 计算用户-用户相似度分数（确保 user_similarity_matrix 与客户具有相同的行数）
    user_similarity_scores = user_similarity_matrix[user_id].reshape(1, -1) # 转置矩阵

    # 选项 1（确保 user_similarity_matrix 具有兼容的维度）
    # 检查如何加载“user_user_similarity_matrix”以使其具有与客户相同的行数

    # 选项 2（重塑 user_similarity_scores 以匹配 customer_item_matrix.columns）
    # user_similarity_scores = user_similarity_scores.reshape(-1, customer_item_matrix.shape[1])

    # 相似用户购买的加权总和
    加权购买 = customer_item_matrix.values.dot(user_similarity_scores.T)

    # 过滤掉用户已经购买过的商品
    加权购买[customer_item_matrix.loc[user_id] != 0] = -np.inf

    # 获取前n个推荐的索引
    top_indices = np.argsort(weighted_purchases.flatten())[::-1][:n_recommendations]

    # 获取推荐商品
    推荐项目 = [customer_item_matrix.columns[i] for i in top_indices]

    返回推荐的_项目


如果 __name__ == &#39;__main__&#39;:
    应用程序运行（调试=真）

对于此代码，我收到错误，我无法解决此错误。
错误：weighted_purchases = customer_item_matrix.values.dot(user_similarity_scores.T)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^
ValueError：形状 (35,3725) 和 (32,1) 未对齐：3725 (dim 1) != 32 (dim 0)

我确实尝试通过使两个矩阵的尺寸相同来修改代码。也用过chatgpt但是没用。如果您可以为我修改这段代码，请给我好的解决方案，这将是巨大的帮助。
这是我的数据集概述。

这也是 pkl 文件和数据集的驱动器链接。
文本]]></description>
      <guid>https://stackoverflow.com/questions/78177747/issue-while-creating-an-api-for-recommendation-system-for-users</guid>
      <pubDate>Mon, 18 Mar 2024 02:58:03 GMT</pubDate>
    </item>
    <item>
      <title>如何在火车上训练 11 个以上参数的模型，但在测试中使用 2 个参数 [关闭]</title>
      <link>https://stackoverflow.com/questions/78173009/how-to-train-model-on-11-parameters-on-train-but-using-on-2-param-in-test</link>
      <description><![CDATA[我的任务是在火车上我可以使用许多输入参数（11 个或更多），但在实际任务（测试）中输入始终是 2 个参数。因此，就如何做得更好、是否值得这样做等等提出一些建议。 简而言之，这是一个回归任务，当我需要预测用户从这台 ATM 取款的概率（用户 ID、和 ATM 位置），这些数据是其他人给我的，但我不明白如何使用比这两个更多的参数。
全局参数：
用户身份
自动提款机位置
我可以使用的火车上的输入：
h3_09
客户ID
日期时间_id
计数整数
总和、平均值、最小值、最大值、浮点数
MCC
等等
我已经尝试过在 11 个输入参数上训练神经网络，但我不明白如何在两个输入参数上使用它的电流，一般来说，我想使用一些梯度提升，但我也不明白如何充分利用所有参数...]]></description>
      <guid>https://stackoverflow.com/questions/78173009/how-to-train-model-on-11-parameters-on-train-but-using-on-2-param-in-test</guid>
      <pubDate>Sat, 16 Mar 2024 19:10:22 GMT</pubDate>
    </item>
    <item>
      <title>关于机器学习和数值训练数据的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78172418/question-about-machine-learning-and-numerical-training-data</link>
      <description><![CDATA[如果您使用 is_away 之类的内容作为数据中的数字字段，人工智能将对其进行训练以预测团队获胜的可能性。所以 1 代表 true，0 代表 false，那么是否应该将其更改为 is_home 之类的内容并翻转值，或者人工智能最终会在预测诸如获胜机会之类的内容时了解到 0 更有可能获胜？
我认为另一个很好的例子是海拔高度，而你的目标值是点。在大多数情况下，海拔越高，得分越少或赛道时间越慢。我假设人工智能理解海拔越高意味着它更有可能预测较低的目标值。在看到一些奇怪的预测后，我将玩家的高度提高了 5k，并在一场游戏中复制了所有其他字段，并且它总是预测该游戏的目标值更高。所以我对人工智能如何处理更高的数值感到困惑。
另请注意，我正在使用 relu 激活和 500k 行数据。我将随机更改单行的高度并使用相同的参数重新训练。与之前的训练数据相比，该行的预测值将从 20 变为 25，其他任何事情都不会发生变化...所以总结一下我的问题，应该反转对预测目标值产生负面影响的数值数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/78172418/question-about-machine-learning-and-numerical-training-data</guid>
      <pubDate>Sat, 16 Mar 2024 15:57:58 GMT</pubDate>
    </item>
    <item>
      <title>如何绘制 2x2 混淆矩阵，行中为预测值，列中为实际值？</title>
      <link>https://stackoverflow.com/questions/70176887/how-to-plot-2x2-confusion-matrix-with-predictions-in-rows-an-real-values-in-colu</link>
      <description><![CDATA[我知道我们可以使用以下示例代码使用 sklearn 绘制混淆矩阵。
from sklearn.metrics import fusion_matrix, ConfusionMatrixDisplay
将 matplotlib.pyplot 导入为 plt

y_true = [1, 0, 1, 1, 0, 1]
y_pred = [0, 0, 1, 1, 0, 1]

打印（f&#39;y_true：{y_true}&#39;）
打印(f&#39;y_pred: {y_pred}\n&#39;)

cm = 混淆矩阵(y_true, y_pred, labels=[0, 1])
印刷（厘米）
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
显示图()
plt.show()


我们拥有：
&lt;前&gt;&lt;代码&gt;TN | FP
前线 | TP

但我希望将预测标签放置在行或 y 轴中，并将真实值或实值标签放置在列或 x 轴中。我如何使用 Python 绘制此图？
我想要什么：
&lt;前&gt;&lt;代码&gt;TP | FP
前线 |总氮
]]></description>
      <guid>https://stackoverflow.com/questions/70176887/how-to-plot-2x2-confusion-matrix-with-predictions-in-rows-an-real-values-in-colu</guid>
      <pubDate>Tue, 30 Nov 2021 22:23:42 GMT</pubDate>
    </item>
    <item>
      <title>遗传算法如何利用数值数据演化出解决方案？</title>
      <link>https://stackoverflow.com/questions/61113622/how-can-genetic-algorithms-evolve-solutions-with-numerical-data</link>
      <description><![CDATA[我熟悉字符串或文本上下文中的 GA，但不熟悉数字数据。
对于字符串，我了解如何应用交叉和变异：
ParentA = abcdef
父级 B = uvwxyz

使用单点交叉：
ChildA = abwxyz（在第二个基因之后枢轴）
ChildB = uvcdef

使用随机基因突变（交叉后）：
ChildA = abwgyz（第四个基因突变）
ChildB = uvcdef（无基因突变）

对于字符串，我有一个离散的字母表可供使用，但是这些运算符如何应用于连续的数值数据？
例如，染色体表示为 4 空间中的点（每个轴是一个基因）：
ParentA = [19, 58, 21, 54]
父级 B = [65, 21, 59, 11]

通过为后代切换父母双方的轴来应用交叉是否合适？
ChildA = [19, 58, 59, 11]（在第二个基因之后旋转）
子 B = [65, 21, 21, 54]

我有一种感觉，这似乎不错，但我天真的突变概念，即随机化基因，似乎并不正确：
ChildA = [12, 58, 59, 11]（第一个基因突变）
ChildB = [65, 89, 34, 54]（第二个和第三个基因突变）

我只是不确定如何将遗传算法应用于这样的数字数据。我知道 GA 需要什么，但不知道如何应用运算符。例如，考虑在 4 维中最小化 Rastrigin 函数的问题：每个维度的搜索空间为 [-512, 512]，适应度函数为 Rastrigin 函数。我不知道我在这里描述的运算符如何帮助找到更合适的染色体。
就其价值而言，精英选择和群体初始化似乎很简单，我唯一的困惑来自交叉和变异算子。
赏金更新
我使用突变率和交叉率对连续数值数据进行了遗传算法的实现，正如我在这里所描述的。优化问题是二维的 Styblinski-Tang 函数，因为它很容易用图表表示。我还使用标准精英和锦标赛选择策略。
我发现群体最佳适应度确实能很好地收敛到解决方案，但平均适应度却不然。
在这里，我绘制了十代的搜索空间：黑点是候选解决方案，红色“x”是全局最优解：

正如我所描述的，交叉算子似乎工作得很好，但突变算子（随机化染色体的 x 或 y 位置，或者两者都不随机）似乎会创建十字准线和交叉影线图案。
我在 50 维中进行了一次运行以延长收敛时间（因为在二维中它会在一代内收敛）并绘制它：

这里的 y 轴表示解决方案与全局最优值的接近程度（因为最优值是已知的），它只是实际输出/预期输出的一小部分。这是一个百分比。绿线是总体最佳（大约 96-97% 目标），蓝色是总体平均（波动 65-85% 目标）。
这验证了我的想法：变异算子并没有真正对总体产生最好的影响，但确实意味着总体平均值永远不会收敛，并且会上下波动。
所以我的赏金问题是除了基因的随机化之外还可以使用哪些突变算子？
补充一下：我问这个问题是因为我对使用 GA 优化神经网络权重来训练网络来代替反向传播感兴趣。如果您对此有所了解，任何详细信息来源也可以回答我的问题。]]></description>
      <guid>https://stackoverflow.com/questions/61113622/how-can-genetic-algorithms-evolve-solutions-with-numerical-data</guid>
      <pubDate>Thu, 09 Apr 2020 03:45:51 GMT</pubDate>
    </item>
    </channel>
</rss>