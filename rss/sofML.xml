<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 19 Mar 2024 12:24:16 GMT</lastBuildDate>
    <item>
      <title>张量流 concat() 和 concatenate() 有什么区别？</title>
      <link>https://stackoverflow.com/questions/78185730/what-is-the-difference-between-tensorflow-concat-and-concatenate</link>
      <description><![CDATA[tf.concat() 和 tf.keras.layers.concatenate() 之间有什么区别（功能上）？
https://www.tensorflow.org/api_docs/python/tf/concat 
https://www.tensorflow.org/api_docs/python/tf/ keras/层/连接
此外，tf.nn.conv2d 和 tf.keras.layers.Conv2D 之间有什么区别？
https://www.tensorflow.org/api_docs/python /tf/keras/layers/Conv2D
https://www.tensorflow.org/api_docs/python/tf/nn/转换2d]]></description>
      <guid>https://stackoverflow.com/questions/78185730/what-is-the-difference-between-tensorflow-concat-and-concatenate</guid>
      <pubDate>Tue, 19 Mar 2024 10:09:45 GMT</pubDate>
    </item>
    <item>
      <title>在 pytorch 中解压火车数据加载器时出现类型错误</title>
      <link>https://stackoverflow.com/questions/78185614/typeerror-while-unpacking-train-data-loader-in-pytorch</link>
      <description><![CDATA[我尝试在扩展名为 .npy 的图像数据集上训练 CNN 模型，但在训练循环中出现 TypeError。
导入
导入火炬
从torchvision导入数据集，转换
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
导入全面质量管理

CNN模型
CNN 类（nn.Module）：
    def __init__(self) -&gt;; __init__(self) -&gt;没有任何：
        超级（CNN，自我）.__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)

        self.fc1 = nn.Linear(64*64*64, 128)
        self.fc2 = nn.Linear(128, 3)

        self.relu = nn.ReLU()

        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(p=0.2)
    
    defforward(self, image: torch.Tensor) -&gt;;火炬.张量：
        图像 = self.pool(self.relu(self.conv1(image)))
        图像 = self.pool(self.relu(self.conv2(image)))
        图像 = self.pool(self.relu(self.conv3(image)))
        图像 = image.view(-1, 64*64*64)
        图像 = self.dropout(self.relu(self.fc1(image)))
        图像 = self.fc2(图像)
        返回图像

超参数
train_path = &#39;数据集/火车&#39;
val_path = &#39;数据集/val&#39;
纪元数 = 11
类数 = 3
批量大小 = 64
学习率 = 0.002
训练测试比率 = 0.9

device = torch.device(&#39;mps&#39; if torch.backends.mps.is_available() else &#39;cpu&#39;)

加载列车数据
将 numpy 导入为 np

def npy_loader(路径):
    样本 = torch.from_numpy(np.load(path))
    返回样品

变换=transforms.Compose([transforms.ToPILImage(),transforms.ToTensor()])
数据集=数据集.DatasetFolder(
    根=火车路径，
    加载器=npy_loader，
    扩展名=&#39;.npy&#39;,
    变换=变换
）

trainloader = torch.utils.data.DataLoader(数据集,batch_size=batch_size,shuffle=True)

培训
模型 = CNN()
模型.to（设备）
优化器 = optim.Adam(model.parameters(), lr=learning_rate)
标准 = nn.CrossEntropyLoss()
模型.train()
对于范围内的纪元（num_epochs）：
    训练损失 = 0.0
    对于数据，tqdm(trainloader) 中的目标：
        数据 = 数据.to(设备)
        目标 = 目标.to(设备)
        优化器.zero_grad()
        输出=模型（数据）
        损失=标准（输出，目标）
        loss.backward()
        优化器.step()
        train_loss += loss.item() * data.size(0)
    train_loss = train_loss/len(trainloader.dataset)
    print(f&#39;Epoch: {epoch+1} \t训练损失: {train_loss:.6f}&#39;)

错误
类型错误：“模块”对象不可调用行中的数据，tqdm(trainloader)中的目标：
我尝试编写 MyDataset 自定义 Dataset 类，但这并没有解决我的错误。
从 torch.utils.data 导入数据集

类 MyDataset（数据集）：
    def __init__(self, np_file_paths, 变换=无):
        self.files = np_file_paths
        self.transform = 变换
    
    def __getitem__(自身，索引)：
        x = np.load(self.files[索引])
        x = torch.from_numpy(x).float()
        如果 self.transform 不是 None：
            图像 = self.transform(图像)
        返回x
    
    def __len__(自身):
        返回 len(self.files)
]]></description>
      <guid>https://stackoverflow.com/questions/78185614/typeerror-while-unpacking-train-data-loader-in-pytorch</guid>
      <pubDate>Tue, 19 Mar 2024 09:53:36 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中大量特征的分析[关闭]</title>
      <link>https://stackoverflow.com/questions/78185501/analysis-of-large-numbers-of-features-in-machine-learning</link>
      <description><![CDATA[我有一个包含 96 个特征的数据集。我的标签中有 90 个数字特征、6 个类别和 6 个分类。首先，我想通过计算相关性来进行特征选择，但是如果我分别使用 dyton 库进行分类特征选择，并使用 pandas 库的 corr() 函数进行数值特征选择，这是否正确？
您对此有什么建议吗？
其次，我想画一个配对图来查看分布。但是，我有 6 个类和分类数据，我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78185501/analysis-of-large-numbers-of-features-in-machine-learning</guid>
      <pubDate>Tue, 19 Mar 2024 09:37:00 GMT</pubDate>
    </item>
    <item>
      <title>如何解决在Python中导入TensorFlow时出现“unhashable type”错误？</title>
      <link>https://stackoverflow.com/questions/78185218/how-can-i-solve-the-unhashable-type-error-when-importing-tensorflow-in-python</link>
      <description><![CDATA[我在 Virtual Studio Code 中导入 TensorFlow 时遇到问题。我尝试执行我的代码，该代码从导入不同的模块开始。其中一行正在导入 Tensorflow：
将tensorflow导入为tf
这给了我错误：
不可哈希类型：“列表”
文件“链接”，第 10 行，位于
导入tensorflow as tf TypeError：不可散列类型：&#39;list&#39;
首先，我在另一台计算机上使用 Jupyter Notebook 编写了此代码，我可以毫无问题地执行它。在我想要执行它的计算机上遇到此错误后，我尝试重现该错误。我在第一台计算机上安装了 Virtual Studio Code，安装了所有模块并成功执行。好像是设置什么的问题。
为了测试我只执行了这个，这给了我错误：
将tensorflow导入为tf
我在两台计算机上都安装了 Python 3.9.0 和 Tensorflow 2.16.1。经过多次尝试（例如卸载并安装tensorflow，或重置Virtual Studio Code），我决定在这里询问。也许这里有人更了解这个问题:)]]></description>
      <guid>https://stackoverflow.com/questions/78185218/how-can-i-solve-the-unhashable-type-error-when-importing-tensorflow-in-python</guid>
      <pubDate>Tue, 19 Mar 2024 08:44:21 GMT</pubDate>
    </item>
    <item>
      <title>从朱莉娅的玻尔兹曼机采样[关闭]</title>
      <link>https://stackoverflow.com/questions/78184695/sampling-from-boltzmann-machine-in-julia</link>
      <description><![CDATA[对于给定的玻尔兹曼机，如何从玻尔兹曼机获取样本？
玻尔兹曼机是一种亵渎的概率分布

对于给定参数b和w。我需要来自 p(x) 的样本

我怎样才能在 Julia 中做到这一点？我假设系统N的大小是20或30。]]></description>
      <guid>https://stackoverflow.com/questions/78184695/sampling-from-boltzmann-machine-in-julia</guid>
      <pubDate>Tue, 19 Mar 2024 06:57:53 GMT</pubDate>
    </item>
    <item>
      <title>尝试从我的网络摄像头访问实时检测时，y python 代码中出现 ocr 错误 [关闭]</title>
      <link>https://stackoverflow.com/questions/78184460/ocr-error-in-y-python-code-while-trying-to-acces-the-realtime-detection-from-my</link>
      <description><![CDATA[这是我收到的错误：
&lt;小时/&gt;
错误回溯（最近一次调用最后一次）
单元格 In[79]，第 40 行
     37 除外：
     38 通
---&gt; 40 cv2.imshow(&#39;物体检测&#39;, cv2.resize(image_np_with_detections, (800, 600)))
     42 if cv2.waitKey(10) &amp; 0xFF == ord(&#39;q&#39;):
     43 cap.release()

错误：OpenCV(4.9.0) D:\a\opencv-python\opencv-python\opencv\modules\highgui\src\window.cpp:1272: 错误：(-2:未指定错误) 该功能未实现。使用 Windows、GTK+ 2.x 或 Cocoa 支持重建库。如果您使用的是 Ubuntu 或 Debian，请安装 libgtk2.0-dev 和 pkg-config，然后在函数“cvShowImage”中重新运行 cmake 或配置脚本

我尝试更换网络摄像头，但遇到同样的问题]]></description>
      <guid>https://stackoverflow.com/questions/78184460/ocr-error-in-y-python-code-while-trying-to-acces-the-realtime-detection-from-my</guid>
      <pubDate>Tue, 19 Mar 2024 05:44:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 MinMaxScaler() 进行特征缩放</title>
      <link>https://stackoverflow.com/questions/78184444/feature-scaling-with-minmaxscaler</link>
      <description><![CDATA[我有 31 个特征要输入到 ML 算法中。这 22 个特征值已经在 0 到 1 的范围内。其余 9 个特征在 0 到 750 之间变化。我的疑问是，如果我选择应用 MinMaxScaler() 并将范围设置为 (0,1)，是应该对所有特征进行缩放还是仅对所需范围之外的 9 个特征进行缩放？什么比较合适？]]></description>
      <guid>https://stackoverflow.com/questions/78184444/feature-scaling-with-minmaxscaler</guid>
      <pubDate>Tue, 19 Mar 2024 05:40:04 GMT</pubDate>
    </item>
    <item>
      <title>pytorch CUDA版本和CUDA实际如何工作？</title>
      <link>https://stackoverflow.com/questions/78184358/how-does-the-pytorch-cuda-version-and-cuda-actually-works</link>
      <description><![CDATA[这是我的 conda 环境的软件包列表：

如你所见，我还没有安装cudatoolkit，并且nvcc命令无法使用。但我确实安装了 CUDA 版本的 pytorch。
但是，当我在 python 中导入 torch 并检查 torch.cuda.is_available() 时，它返回 Ture。
我什至运行这个测试脚本：
导入火炬
从火炬导入 nn
从 torch.nn 导入模块
从 torch.optim.lr_scheduler 导入 LambdaLR


测试网类（模块）：
    def __init__(self) -&gt;; __init__(self) -&gt;没有任何：
        超级().__init__()
        self.线性 = nn.Linear(10,10)

    def 前向（自身，x）：
        返回自线性(x)
    

如果 __name__==“__main__”：
    如果 torch.cuda.is_available():
        设备=“cuda”；
    别的：
        设备=“CPU”
    print(f“使用设备 {device}”)
    test_samples = torch.rand([32,10]).to(设备)
    gt_matrix = torch.eye(10).to(设备)
    目标 = torch.matmul(test_samples, gt_matrix)

    模型 = TestNet().to(设备)

    优化器 = torch.optim.SGD(model.parameters(), lr=1)
    标准 = nn.MSELoss()
    调度程序 = LambdaLR(优化器, lr_lambda=lambda x: min(x, 24)/24)

    对于范围（128）内的纪元：
        logits = 模型（测试样本）
        损失=标准（logits，目标）
        Learning_rate = Optimizer.param_groups[0][“lr”]

        优化器.zero_grad()
        loss.backward()
        优化器.step()
        调度程序.step()

        print(f&quot;历元 {epoch+1}/{24}, 损失 {loss.item()}, lr {learning_rate}&quot;)
    
    print(&quot;学习矩阵：&quot;)
    打印（model.state_dict（）[“线性.权重”]）

并且运行成功。
所以我很好奇 pytorch CUDA 版本实际上是如何工作的？是否需要预装CUDA工具包？另外，通过 conda install cudatoolkit 和 conda install cuda 安装 CUDA 甚至通过图形安装程序安装有什么区别？]]></description>
      <guid>https://stackoverflow.com/questions/78184358/how-does-the-pytorch-cuda-version-and-cuda-actually-works</guid>
      <pubDate>Tue, 19 Mar 2024 05:18:07 GMT</pubDate>
    </item>
    <item>
      <title>选择模型训练参数（和层）</title>
      <link>https://stackoverflow.com/questions/78184146/selecting-model-training-params-and-layers</link>
      <description><![CDATA[在代码中（下面从tensorflow.org“针对初学者的 Tensorflow 2 快速入门”复制/粘贴了所有许多变量，例如：

纪元数
批量大小
学习率
层（或层的组合）
每层的激活函数
损失函数
单位（密集层的参数）
比率（针对 Dropout 层）
等等...

除了“猜测和猜测”之外，是否还有一些受过教育的程序？测试”，以便找到“最佳”方案。参数组合 --- 或者甚至知道从哪个参数组合开始？
我是否缺乏“专家”所需要的一些知识（或信息）？或博士学位将为我提供，这将使我有能力选择“最好的”。参数（第一次）？
我当然明白最后一层 Dense(10) 的一些参数需要是 10 个单位来表示 10 个类...而且我知道 10,000 层可能不太好...所以我不是在谈论极端值或最后一个密集层中的 10 之类的东西，而是其他参数。
含义 - 作为示例 - 在下面的代码中，假设我更改了行中的单位
tf.keras.layers.Dense（32，激活=&#39;relu&#39;）

到其他 64（而不是 32）。
对于下面的例子，
64 肯定比 32 更好（或更差）有什么理由吗？
是否有理由添加另一个密集层会更好（或更差）？
是否有明确的理由说明为什么 6 个 epoch 的训练会比 5 个 epoch 更好（或更差）？
我知道，如果我理解模型最终所做的所有计算，我就会得到我的答案......这意味着，因为“某人”写了那个代码，有能力知道这一点并不是不可能的......
所有教程和内容都令人非常沮丧。那里的网站关注的是如何，而不是为什么。
（任何建议/链接/资源将不胜感激）
提前非常感谢各位专家！
导入argparse
将张量流导入为 tf


def 过程（参数）：
    ( X_train, y_train ), ( X_test, y_test ) = tf.keras.datasets.mnist.load_data()
    X_train = X_train / 255.0
    X_测试 = X_测试 / 255.0

    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    模型 = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(10)
    ]）

    model.compile(optimizer=&#39;adam&#39;,loss=loss_fn,metrics=[&#39;accuracy&#39;])

    hist = model.fit(X_train,
                     y_火车，
                     批量大小=32，
                     纪元=5，
                     验证分割=0.2，
                     随机播放=真）


如果 __name__ == &#39;__main__&#39;:
    解析器 = argparse.ArgumentParser()
    parser.add_argument(&#39;--batch_size&#39;, type=int, default=32)
    args = parser.parse_args()
    过程（参数）

``
]]></description>
      <guid>https://stackoverflow.com/questions/78184146/selecting-model-training-params-and-layers</guid>
      <pubDate>Tue, 19 Mar 2024 03:47:31 GMT</pubDate>
    </item>
    <item>
      <title>TF2 和 python 中的 BERT 预处理器模型存在问题</title>
      <link>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</link>
      <description><![CDATA[我正在尝试使用 BERT 来做一个文本分类项目。但是我一直遇到这个错误
`
ValueError Traceback（最近一次调用最后一次）
单元格 In[37]，第 4 行
      2 text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
      3 bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
----&gt; 4 preprocessed_text = bert_preprocess(text_input)
      5 bert_encoder = hub.KerasLayer(encoder_url,
      6 可训练=真，
      7 名称=&#39;BERT_编码器&#39;)
      8 个输出 = bert_encoder(preprocessed_text)
ValueError：调用层“预处理”时遇到异常（类型 KerasLayer）。
KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。

调用层“预处理”接收的参数（类型 KerasLayer）：
  输入=
  • 培训=无

KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。
调用层“预处理”接收的参数（类型 KerasLayer）：
• 输入=
• 训练=无`
构建此模型时：
&lt;前&gt;&lt;代码&gt;
preprocess_url = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3&#39;
编码器网址 = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/bert-en-uncased-l-12-h-768-a-12/versions/2&#39;

# Bert 层
text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
预处理文本 = bert_preprocess(text_input)
bert_encoder = hub.KerasLayer(encoder_url,
                              可训练=真，
                              名称=&#39;BERT_编码器&#39;)
输出= bert_encoder（预处理文本）

# 神经网络层
l = tf.keras.layers.Dropout(0.1)(输出[&#39;pooled_output&#39;])
l = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;, name=&#39;输出&#39;)(l)

# 构建最终模型
模型 = tf.keras.Model(输入=[text_input], 输出=[l])

我看过无数的教程，甚至使用了张量流文档上的教程，即使我复制和粘贴，它们仍然不起作用。我尝试过不同版本的 tf、tf-text 和 tf-hub。我在这个项目中使用了tensorflow-gpu-jupyter docker 容器。
这是我安装库的方法：
!pip install “tensorflow-text”
!pip install “tf-models-official”
!pip install “tensorflow-hub”

版本是：
张量流：2.16.1
张量流文本：2.16.1
张量流中心：0.16.1
我看到的有关此问题的所有其他论坛都说要执行 tf.config.run_functions_eagerly(True) 但这不起作用。
任何事情都会有所帮助。如果您知道如何解决请回答。]]></description>
      <guid>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</guid>
      <pubDate>Tue, 19 Mar 2024 01:42:01 GMT</pubDate>
    </item>
    <item>
      <title>为用户创建推荐系统 API 时出现问题</title>
      <link>https://stackoverflow.com/questions/78177747/issue-while-creating-an-api-for-recommendation-system-for-users</link>
      <description><![CDATA[fromflask导入Flask，jsonify，request
将 pandas 导入为 pd
将 numpy 导入为 np
从 sklearn.metrics.pairwise 导入 cosine_similarity
导入作业库

应用程序=烧瓶（__名称__）

# 加载预训练的用户-用户相似度矩阵
user_user_similarity_matrix = joblib.load(&#39;预测模型/user_user_similarity_matrix.pkl&#39;)

# 从 Excel 文件加载数据集（API 功能不需要此行）
data = pd.read_excel(&#39;预测模型/DDDD.xlsx&#39;)

# 将占位符列“PurchasedYes”添加到数据 DataFrame
数据[&#39;已购买&#39;] = 1

@app.route(&#39;/推荐&#39;, 方法=[&#39;POST&#39;])
def 推荐():
    user_id = request.json[&#39;user_id&#39;] # 从请求中获取用户ID
    推荐=generate_recommendations（user_user_similarity_matrix，数据，user_id，n_recommendations = 10）
    return jsonify({&#39;user_id&#39;: user_id, &#39;推荐&#39;: 推荐})


defgenerate_recommendations(user_similarity_matrix, data, user_id, n_recommendations=10):
    # 透视表以获得矩阵，其中行代表客户，列代表项目
    customer_item_matrix = data.pivot_table(index=&#39;客户&#39;, columns=&#39;SalesItem&#39;, value=&#39;PurchasedYes&#39;, fill_value=0)

    # 获取用户购买的商品
    user_purchases = customer_item_matrix.loc[user_id].values.reshape(1, -1) # 重塑为行向量

    # 计算用户-用户相似度分数（确保 user_similarity_matrix 与客户具有相同的行数）
    user_similarity_scores = user_similarity_matrix[user_id].reshape(1, -1) # 转置矩阵

    # 选项 1（确保 user_similarity_matrix 具有兼容的维度）
    # 检查如何加载“user_user_similarity_matrix”以使其具有与客户相同的行数

    # 选项 2（重塑 user_similarity_scores 以匹配 customer_item_matrix.columns）
    # user_similarity_scores = user_similarity_scores.reshape(-1, customer_item_matrix.shape[1])

    # 相似用户购买的加权总和
    加权购买 = customer_item_matrix.values.dot(user_similarity_scores.T)

    # 过滤掉用户已经购买过的商品
    加权购买[customer_item_matrix.loc[user_id] != 0] = -np.inf

    # 获取前n个推荐的索引
    top_indices = np.argsort(weighted_purchases.flatten())[::-1][:n_recommendations]

    # 获取推荐商品
    推荐项目 = [customer_item_matrix.columns[i] for i in top_indices]

    返回推荐的_项目


如果 __name__ == &#39;__main__&#39;:
    应用程序运行（调试=真）

对于此代码，我收到错误，我无法解决此错误。
错误：weighted_purchases = customer_item_matrix.values.dot(user_similarity_scores.T)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^
ValueError：形状 (35,3725) 和 (32,1) 未对齐：3725 (dim 1) != 32 (dim 0)

我确实尝试通过使两个矩阵的尺寸相同来修改代码。也用过chatgpt但是没用。如果您可以为我修改这段代码，请给我好的解决方案，这将是巨大的帮助。
这是我的数据集概述。

这也是 pkl 文件和数据集的驱动器链接。
文本]]></description>
      <guid>https://stackoverflow.com/questions/78177747/issue-while-creating-an-api-for-recommendation-system-for-users</guid>
      <pubDate>Mon, 18 Mar 2024 02:58:03 GMT</pubDate>
    </item>
    <item>
      <title>Keras 中的形状不兼容，但 model.summary() 似乎没问题</title>
      <link>https://stackoverflow.com/questions/78066771/incompatible-shapes-in-keras-but-model-summary-seems-ok</link>
      <description><![CDATA[下面是我的模型
# 由于我们要预测每个时间步的值，因此我们设置 return_sequences=True
输入=输入(batch_shape=ip_shape)
mLSTM = LSTM（单位=32，return_sequences=True，stateful=True）（输入）
mDense = Dense(单位=32，激活=&#39;线性&#39;)(输入)
mSkip = Add()([mLSTM, mDense])

mSkip = 密集（单位=1，激活=&#39;线性&#39;）（mSkip）
模型=模型（输入，mSkip）

亚当 = 亚当(learning_rate=0.01)
model.compile(优化器=adam, 损失=total_loss)
模型.summary()

模型摘要
型号：“model_3”
_______________________________________________________________________________________________
 层（类型）输出形状参数#连接到
=================================================== ===============================================
 input_5 (输入层) [(104, 22050, 1)] 0 []
                                                                                                  
 lstm_4 (LSTM) (104, 22050, 32) 4352 [&#39;input_5[0][0]&#39;]
                                                                                                  
 稠密_5（密集）(104, 22050, 32) 64 [&#39;input_5[0][0]&#39;]
                                                                                                  
 add_3 (添加) (104, 22050, 32) 0 [&#39;lstm_4[0][0]&#39;,
                                                                     &#39;dense_5[0][0]&#39;]
                                                                                                  
 稠密_6（密集）(104, 22050, 1) 33 [&#39;add_3[0][0]&#39;]
                                                                                                  
=================================================== ===============================================
总参数：4449 (17.38 KB)
可训练参数：4449 (17.38 KB)
不可训练参数：0（0.00 字节）
_______________________________________________________________________________________________

我正在使用自定义损失函数。我不确定反向传播时是否会弄乱形状
def Total_loss(y_true, y_pred):
    比率 = 0.5
    dc_loss = math_ops.pow(math_ops.subtract(math_ops.mean(y_true, 0), math_ops.mean(y_pred, 0)), 2)
    dc_loss = math_ops.mean(dc_loss, axis=-1)
    dc_energy = math_ops.mean(math_ops.pow(y_true, 2), axis=-1) + 0.00001
    dc_loss = math_ops.div(dc_loss, dc_energy)

    esr_loss = math_ops.squared_difference(y_pred, y_true)
    esr_loss = math_ops.mean(esr_loss, axis=-1)
    esr_energy = math_ops.mean(math_ops.pow(y_true, 2), axis=-1) + 0.00001
    esr_loss = math_ops.div(esr_loss, esr_energy)

    返回（比率）*dc_loss +（1-比率）*esr_loss

最后的错误：让我知道是否需要整个回溯
InvalidArgumentError：图形执行错误：

...

不兼容的形状：[104,22050,32] 与 [32,22050,1]
     [[{{节点gradient_tape/total_loss/BroadcastGradientArgs}}]] [Op：__inference_train_function_9604]

设置 stateful=False 似乎有效，但我不明白为什么]]></description>
      <guid>https://stackoverflow.com/questions/78066771/incompatible-shapes-in-keras-but-model-summary-seems-ok</guid>
      <pubDate>Tue, 27 Feb 2024 10:22:55 GMT</pubDate>
    </item>
    <item>
      <title>如何利用 GPU 减少 xgboost 的处理时间？</title>
      <link>https://stackoverflow.com/questions/77643788/how-can-i-reduce-processing-time-with-xgboost-by-utilizing-my-gpu</link>
      <description><![CDATA[我正在关注数据营的本教程，他们有一件事提到的是利用 GPU 来加快处理时间。他们甚至说它“速度极快”。
然而，我看到了相反的结果。对于下面的代码块，在 10k 提升的情况下，我看到在我的 params 中传递 “hist” 大约需要 30 秒，而在 ” 中传递则只需一分多钟。 gpu_hist&quot; 与我的 params 一起传递。
使用 “gpu_hist” 时，我的 GPU 的使用率上限为 40%，使用 “hist” 时，所有 24 个逻辑核心的使用率上限为 100%
params = {“objective”: “reg:squarederror”, “tree_method”: “gpu_hist”, “subsample”: 0.8,
    “colsample_bytree”：0.8}

evals = [(dtrain_reg, “训练”),(dtest_reg, “验证”)]

n = 10000


模型 = xgb.train(
   参数=参数，
   dtrain=dtrain_reg,
   num_boost_round=n,
   评估=评估，
   详细评估=50，
）

我正在尝试在 jupyter 笔记本的 VSCode 中运行它。

我已安装 CUDA 工具包和 cuDNN
我已检查它们是否已添加到路径中
我已确保安装了正确版本的 xgboost 来使用 GPU。
数据集有 53k 行 10 列，所以我不认为数据集太小
我已确认兼容性（使用 RTX 2060）

我问过 chatGPT，在网上搜索过，甚至问过我正在学习的课程中的导师，但无法诊断为什么“gpu_hist”花费了这么长时间。 vs 只是“历史”。
4 个月前，Stack Overflow 上还有另一个类似问题其响应为零。]]></description>
      <guid>https://stackoverflow.com/questions/77643788/how-can-i-reduce-processing-time-with-xgboost-by-utilizing-my-gpu</guid>
      <pubDate>Tue, 12 Dec 2023 05:02:17 GMT</pubDate>
    </item>
    <item>
      <title>在链中堆叠分类器集合</title>
      <link>https://stackoverflow.com/questions/74801365/stacking-ensemble-of-classifiers-in-a-chain</link>
      <description><![CDATA[我有以下人类活动识别示例数据集：
df = pd.DataFrame(
    {
  &#39;平均速度&#39;: [40.01, 3.1, 2.88, 20.89, 5.82, 40.01, 33.1, 40.88, 20.89, 5.82, 40.018, 23.1],
  &#39;最大速度&#39;: [70.11, 6.71, 7.08, 39.63, 6.68, 70.11, 65.71, 71.08, 39.63, 13.68, 70.11, 35.71],
  &#39;max_acc&#39;: [17.63, 2.93, 3.32, 15.57, 0.94, 17.63, 12.93, 3.32, 15.57, 0.94, 17.63, 12.93],
  &#39;mean_acc&#39;: [5.15, 1.97, 0.59, 5.11, 0.19, 5.15, 2.97, 0.59, 5.11, 0.19, 5.15, 2.97],
  &#39;活动&#39;：[&#39;驾驶&#39;，&#39;步行&#39;，&#39;步行&#39;，&#39;骑行&#39;，&#39;步行&#39;，&#39;驾驶&#39;，&#39;摩托车&#39;，
               &#39;摩托车&#39;，&#39;骑行&#39;，&#39;骑行&#39;，&#39;摩托车&#39;，&#39;骑行&#39;]
}
）
df.head()
  mean_speed max_speed max_accmean_acc 活动
0 40.01 70.11 17.63 5.15 驾驶
1 3.10 6.71 2.93 1.97 步行
2 2.88 7.08 3.32 0.59 步行
3 20.89 39.63 15.57 5.11 骑行
4 5.82 6.68 0.94 0.19 步行

所以我想在管道中创建一系列机器学习分类器。基分类器首先预测活动是否为机动化（驾驶、摩托车）、&lt;强&gt;非机动（骑行、步行）。学习阶段应该这样进行：

因此，我添加了一列type，说明活动的位置是机动化的还是其他形式。
class_mapping = {&#39;驾驶&#39;:&#39;机动化&#39;, &#39;摩托车&#39;:&#39;机动化&#39;, &#39;步行&#39;:&#39;非机动化&#39;, &#39;骑行&#39; :&#39;非机动&#39;}
df[&#39;type&#39;] = df[&#39;activity&#39;].map(class_mapping)

df.head()
 mean_speed max_speed max_accmean_acc 活动类型
0 40.01 70.11 17.63 5.15 驾驶 机动化
1 3.10 6.71 2.93 1.97 步行 非机动
2 2.88 7.08 3.32 0.59 步行 非机动
3 20.89 39.63 15.57 5.11 乘坐非机动
4 5.82 6.68 0.94 0.19 步行 非机动

问题：
我想训练一个随机森林作为基本分类器，以预测一项活动是机动化还是非机动化，其中概率输出。然后是 2 个元分类器：用于预测活动是步行还是骑行的决策树，以及 SVC它可以预测活动是驾驶还是摩托车。元分类器（DT、SVC）将第一个分类器的 4 个特征 + 概率输出作为输入。显然，DT 和 SVC 只会采用与它们预测的类相对应的整个数据集的子集。
我对学习过程有这样的想法，但我不确定如何实现它。
有人可以展示如何做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/74801365/stacking-ensemble-of-classifiers-in-a-chain</guid>
      <pubDate>Wed, 14 Dec 2022 16:28:56 GMT</pubDate>
    </item>
    <item>
      <title>使用无标签机器学习进行异常检测</title>
      <link>https://stackoverflow.com/questions/44942551/anomaly-detection-with-machine-learning-without-labels</link>
      <description><![CDATA[我正在一段时间内跟踪多个信号，并将它们与时间戳相关联，如下所示：
&lt;前&gt;&lt;代码&gt;t0 1 10 2 0 1 0 ...
t1 1 10 2 0 1 0 ...
t2 3 0 9 7 1 1 ... //按下按钮更改模式
t3 3 0 9 7 1 1 ...
t4 3 0 8 7 1 1 ... // 按下按钮来调整某个特性，如温度（信号 3）

其中 t0 是时间戳，1 是信号 1 的值，10 是信号 2 的值，依此类推。
在该特定时间段内捕获的数据应被视为正常情况。现在应该可以检测到与正常情况的显着偏差。通过显着的推导，我并不是指一个信号值仅更改为跟踪阶段期间未见过的值，而是指许多值发生了尚未相互关联的更改。我不想对规则进行硬编码，因为将来可能会添加或删除更多信号，并且可能会添加或删除其他“modi”信号。可以实现具有其他信号值的。
这可以通过某种机器学习算法来实现吗？如果发生小的推导，我希望算法首先将其视为对训练集的微小更改，如果将来多次发生，则应该“学习”。主要目标是检测更大的变化/异常。]]></description>
      <guid>https://stackoverflow.com/questions/44942551/anomaly-detection-with-machine-learning-without-labels</guid>
      <pubDate>Thu, 06 Jul 2017 07:36:35 GMT</pubDate>
    </item>
    </channel>
</rss>