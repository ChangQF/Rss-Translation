<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 09 Jan 2024 09:14:09 GMT</lastBuildDate>
    <item>
      <title>过多的 padding 导致 NN 模型精度下降</title>
      <link>https://stackoverflow.com/questions/77785503/excessive-padding-causes-accuracy-decrease-to-nn-model</link>
      <description><![CDATA[我训练了一个简单的神经网络模型来进行二元分类，并能够区分真假新闻
#创建模型的类
类 FakeNewsDetectionModelV0(nn.Module):
     def __init__(自身, input_size):
        超级().__init__()
        
        self.layer_1=nn.Linear(in_features=input_size, out_features=8)
        self.layer_2=nn.Linear(in_features=8, out_features=1) #从前一层获取5个特征并输出单个特征

     #定义一个forward()用于前向传播
     def 前向（自身，x，掩码）：
        
        # 应用掩码来忽略某些值
        如果掩码不是 None：
            x = x * 掩码

        x = self.layer_1(x)
        x = self.layer_2(x)
        返回x




我使用 CountVectorizer 将文本转换为列表，然后转换为张量
从 sklearn.feature_extraction.text 导入 CountVectorizer

矢量化器 = CountVectorizer(min_df=0, 小写=False)
矢量化器.fit(df[&#39;文本&#39;])

X=vectorizer.fit_transform(df[&#39;text&#39;]).toarray()

问题在于，由于数据集有超过 9000 个条目，因此训练模型的输入大小非常大（大约 120000 个）。因此，当我尝试对单个句子进行预测时，由于大小明显较小，我需要过度填充句子以使其适合模型的输入，这极大地影响了模型的准确性。
from io 导入 StringIO
来自 torch.nn.function 导入垫
导入字符串
进口再
从 nltk.tokenize 导入 word_tokenize
从 nltk.corpus 导入停用词
导入nltk
从 keras.preprocessing.text 导入 Tokenizer
从 keras.preprocessing.sequence 导入 pad_sequences


尝试：
    #nltk.download(&#39;停用词&#39;)
    nltk.download(&#39;punkt&#39;)
除了：
    print(&quot;下载停用词时出错&quot;)

def normalise_text (文本):

  text = text.lower() # 小写
  text = text.replace(r&quot;\#&quot;,&quot;&quot;) # 替换主题标签
  text = text.replace(r&quot;http\S+&quot;,&quot;URL&quot;) # 删除 URL 地址
  text = text.replace(r&quot;@&quot;,&quot;&quot;)
  text = text.replace(r&quot;[^A-Za-z0-9()!?\&#39;\`\&quot;]&quot;, &quot; &quot;)
  text = text.replace(&quot;\s{2,}&quot;, &quot;&quot;)
  文本 = re.sub(r&#39;[^\w\s]&#39;, &#39;&#39;, 文本)
  返回文本

def fake_news_detection(df, model, model_input_size):
    预测=[]
    最大字数 = 10000
    最大长度 = 模型输入大小

    模型.eval()

    对于 df[&#39;text&#39;][:4000] 中的预测数据：
        预测数据=标准化文本（预测数据）

        #print([预测数据])



        # 使用CountVectorizer将文本数据转换为数组
        矢量化器 = CountVectorizer(min_df=0, 小写=False)
        Prediction_data_array = Vectorizer.fit_transform([prediction_data]).toarray()

        #tokenizer = Tokenizer(num_words=max_words)
        #tokenizer.fit_on_texts([预测数据])
        #sequences = tokenizer.texts_to_sequences([预测数据])


        #prediction_data_array = pad_sequences(序列, maxlen=max_length,value=-1.0)

        #print(预测数据数组.形状)

        # 检查转换后数据的形状
        当前输入大小=预测数据数组.形状[1]


        Prediction_data_tensor = torch.tensor(prediction_data_array, dtype=torch.float32)


        # 如果形状不匹配，则调整其大小
        如果当前输入大小！=模型输入大小：

            打印（当前输入大小）
            填充 = 模型输入大小 - 当前输入大小
            Prediction_data_tensor = pad(prediction_data_tensor, (0, 填充), &#39;常量&#39;, 值 = 0)
            mask_tensor = torch.ones_like(prediction_data_tensor)
            mask_tensor[:, -padding:] = 0 # 将填充区域中的值设置为 0
            #print(torch.unique(mask_tensor, return_counts=True))

            # 应用掩码来忽略某些值
            #预测数据张量 = 预测数据张量 * 掩码张量



        # 假设你的模型将 input_data 作为输入
        使用 torch.inference_mode()：
            预测 = torch.round(torch.sigmoid(model(prediction_data_tensor, mask_tensor))).squeeze()

        预测.append(round(预测.item()))

    print(f“我们的数据张量形状是 {prediction_data_tensor.shape}”)

    Predictions_tensor = torch.FloatTensor(预测)

    返回预测张量

有谁知道有什么解决方法可以让我将数据适合我的模型而不降低其准确性分数吗？
尝试：在对小尺寸数据进行预测时填充向量
预期：准确的预测类似于我在训练/评估过程中得到的结果
得到：预测不准确，准确度非常低（大约 43%）]]></description>
      <guid>https://stackoverflow.com/questions/77785503/excessive-padding-causes-accuracy-decrease-to-nn-model</guid>
      <pubDate>Tue, 09 Jan 2024 09:03:52 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中 NxM 密集层和 M 个独立 Nx1 密集层之间的梯度和优化差异</title>
      <link>https://stackoverflow.com/questions/77785480/gradient-and-optimization-differences-between-a-nxm-dense-layer-and-m-separate-n</link>
      <description><![CDATA[我很好奇这两种设计选择如何影响训练过程中的梯度计算和优化过程。
反向传播过程中每个结构的梯度计算会受到怎样的影响？梯度流回网络的方式有什么不同吗？这些不同的网络结构是否需要或受益于不同的优化算法或学习率？
我的目标是从梯度计算和优化的角度了解每种模型结构选择的潜在好处和可能的缺点。
对此问题的任何见解将不胜感激。谢谢。
我以为“渐变”是一样的，但结果是不同的。]]></description>
      <guid>https://stackoverflow.com/questions/77785480/gradient-and-optimization-differences-between-a-nxm-dense-layer-and-m-separate-n</guid>
      <pubDate>Tue, 09 Jan 2024 08:59:37 GMT</pubDate>
    </item>
    <item>
      <title>哪种评估指标最适合不平衡的多标签分类，确保准确的模型评估？</title>
      <link>https://stackoverflow.com/questions/77785250/which-evaluation-metric-suits-imbalanced-multi-label-classification-best-ensuri</link>
      <description><![CDATA[如何处理？
在处理多标签分类中的平均指标时，应优先考虑采用微观平均、宏观平均、加权平均或样本平均等平均技术来综合评估模型的性能]]></description>
      <guid>https://stackoverflow.com/questions/77785250/which-evaluation-metric-suits-imbalanced-multi-label-classification-best-ensuri</guid>
      <pubDate>Tue, 09 Jan 2024 08:20:55 GMT</pubDate>
    </item>
    <item>
      <title>计算混淆矩阵的精度和召回率</title>
      <link>https://stackoverflow.com/questions/77785162/calculate-precision-and-recall-on-confusion-matrix</link>
      <description><![CDATA[正如主题所说，我必须计算混淆矩阵的精度和召回率。

二元分类器经过训练，可以区分恶意网络流量 $(y=1)$ 与正常流量 $(y=-1)$
当我运行代码时：
 precision = 3805 / (3805 + 804) # 计算分类器的精度
recall = 3805 / (3805 + 14212) # 计算分类器的recall

print(“精度：{:.2f}”.format(精度))
print(&quot;召回: {:.2f}&quot;.format(recall))

我收到错误：
精度：0.83
召回率：0.21
✘ 精度不正确。你可以从困惑中得到它
矩阵为 TP/P*。
有人可以告诉我哪里出了问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77785162/calculate-precision-and-recall-on-confusion-matrix</guid>
      <pubDate>Tue, 09 Jan 2024 08:04:36 GMT</pubDate>
    </item>
    <item>
      <title>Mahout seqdirectory 无法读取输入 csv 文件</title>
      <link>https://stackoverflow.com/questions/77784930/mahout-seqdirectory-fails-to-read-input-csv-files</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77784930/mahout-seqdirectory-fails-to-read-input-csv-files</guid>
      <pubDate>Tue, 09 Jan 2024 07:19:53 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习技术的预测模型</title>
      <link>https://stackoverflow.com/questions/77784885/prediction-model-using-machine-learning-techniques</link>
      <description><![CDATA[我正在集群中运行 DNN 训练作业；这是我使用过的培训作业的链接。
https://github.com/pytorch/examples/blob /main/imagenet/main.py
在运行具有不同架构和不同批量大小的作业时，我可以收集执行时间。我在这里提到了一个例子。
因此，为了预测执行时间，我想使用机器学习技术来制作预测模型。
所以我想听听您关于收集数据点的建议。如果我收集不同架构的数据点，例如
模型架构：alexnet | convnext_base | convnext_large | convnext_small | convnext_tiny |密集网络121 |密集网络161 |密集网络169 |密集网络201 |高效网络_b0 |
高效网络_b1 |高效网络_b2 |高效网络_b3 |高效网络_b4 |高效网络_b5 |高效网_b6 |高效网_b7 |谷歌网 | inception_v3 | mnasnet0_5 | mnasnet0_75 | mnasnet0_75
mnasnet1_0 | mnasnet1_3 |移动网络_v2 | mobilenet_v3_large | mobilenet_v3_small | regnet_x_16gf | regnet_x_1_6gf | regnet_x_32gf | regnet_x_3_2gf | regnet_x_400mf | regnet_x_800mf |
regnet_x_8gf | regnet_y_128gf | regnet_y_16gf | regnet_y_1_6gf | regnet_y_32gf | regnet_y_3_2gf | regnet_y_400mf | regnet_y_800mf | regnet_y_8gf |资源网101 |资源网152 |资源网18 |
资源网34 |资源网50 | resnext101_32x8d | resnext50_32x4d | resnext50_32x4d | shufflenet_v2_x0_5 | shufflenet_v2_x1_0 | shufflenet_v2_x1_5 | shufflenet_v2_x2_0 |挤压网1_0 |挤压网1_1 | vgg11 |
vgg11_bn | vgg13 | vgg13_bn | vgg16 | vgg16_bn | vgg19 | vgg19_bn | vit_b_16 | vit_b_32 |维生素_l_16 | vit_l_32 | Wide_resnet101_2 | Wide_resnet50_2（默认：resnet18）
是否可以创建一个预测模型来预测执行时间？]]></description>
      <guid>https://stackoverflow.com/questions/77784885/prediction-model-using-machine-learning-techniques</guid>
      <pubDate>Tue, 09 Jan 2024 07:07:37 GMT</pubDate>
    </item>
    <item>
      <title>将 XGBoost 转换为 Excel 的 if-then 语句 [关闭]</title>
      <link>https://stackoverflow.com/questions/77784031/convert-xgboost-to-if-then-statements-for-excel</link>
      <description><![CDATA[我在 R 中制作了一个 XGBoost 模型。我想将该模型导出到 Excel，但遇到了一些后勤问题。是否有一种有效的方法将每个决策树转换为格式类似于 Excel 的 if-then 语句？另外，XGBoost模型中的最终分数是如何计算的。它是每棵树的所有单独分数的总和吗？]]></description>
      <guid>https://stackoverflow.com/questions/77784031/convert-xgboost-to-if-then-statements-for-excel</guid>
      <pubDate>Tue, 09 Jan 2024 02:13:30 GMT</pubDate>
    </item>
    <item>
      <title>无法合并负样本或背景图像来训练 MediaPipe 中的对象检测模型</title>
      <link>https://stackoverflow.com/questions/77783940/unable-to-incorporate-negative-samples-or-background-images-for-training-the-obj</link>
      <description><![CDATA[我面临的问题是 MediaPipe 中的对象检测模型在训练期间忽略了负样本的包含。例如，我有一个数据集，其中包含 500 张带有对象标签的图像以及另外 200 张标记为负样本的图像。但是，在检查训练数据大小时，它仅显示“train_data size: 500”，而不是预期的 700。我尝试了两种不同的方法来注释数据集中的负样本，使用以下 XML 格式：
&lt;注释已验证=“是”&gt;
   &lt;文件夹&gt;图像
   &lt;文件名&gt;10.jpeg
   &lt;路径&gt;/content/jpeg_images/train/images/10.jpeg
   &lt;来源&gt;
    &lt;数据库&gt;未知&lt;/数据库&gt;
   &lt;/来源&gt;
   &lt;尺寸&gt;
    &lt;宽度&gt;640
    &lt;高度&gt;480
    &lt;深度&gt;3&lt;/深度&gt;
   &lt;/尺寸&gt;
   &lt;分段&gt;0
&lt;/注释&gt;

&lt;注释已验证=“是”&gt;
    &lt;文件夹&gt;图像
    &lt;文件名&gt;1.jpeg
    &lt;路径&gt;/content/jpeg_images/train/images/1.jpeg
    &lt;来源&gt;
        &lt;数据库&gt;未知&lt;/数据库&gt;
    &lt;/来源&gt;
    &lt;尺寸&gt;
        &lt;宽度&gt;599
        &lt;高度&gt;399
        &lt;深度&gt;3&lt;/深度&gt;
    &lt;/尺寸&gt;
    &lt;分段&gt;0
    &lt;对象&gt;
        &lt;名称&gt;背景
        &lt;绑定框&gt;
            0
            &lt;ymin&gt;0&lt;/ymin&gt;
            &lt;xmax&gt;0&lt;/xmax&gt;0
            &lt;ymax&gt;0&lt;/ymax&gt;0
        &lt;/bndbox&gt;
    &lt;/对象&gt;
&lt;/注释&gt;


两者都被忽略了
MediaPipe 对象检测模型的预期行为是无缝处理和训练负样本，确保它们包含在训练数据集中。这种能力应该会产生一个全面的模型，能够准确地检测对象并在推理过程中将它们与负面实例区分开来]]></description>
      <guid>https://stackoverflow.com/questions/77783940/unable-to-incorporate-negative-samples-or-background-images-for-training-the-obj</guid>
      <pubDate>Tue, 09 Jan 2024 01:32:46 GMT</pubDate>
    </item>
    <item>
      <title>在 2D 图像上生成估计位置的方法 [关闭]</title>
      <link>https://stackoverflow.com/questions/77783783/approach-to-generate-estimated-position-on-2d-image</link>
      <description><![CDATA[我将 Python 与 OpenCV 结合使用，并使用我自己的数据训练了 YOLOv8 模型。
我有一个通过机器周围的对象检测获得的边界框，该边界框向外延伸并附加到对象上。我目前面临的问题是，我想估计机器在物体上的位置，如果机器要延伸的话，给定其由非延伸形式的边界框定义的位置。
本质上，我有机器和机器未扩展的对象（下面将详细描述）的 2D 图像。然后，我获得了机器的边界框，并希望估计其在机器延伸并附着到物体时所在的同一 2D 图像上的位置。
下面提供了一张图像作为示例，展示了它在程序中的外观；但是，给出的图像的对象位于右侧，值得注意的是，它也可以翻转，使对象位于左侧（多个摄像机角度）。
需要注意的是，机器不会每次都固定在完全相同的位置，因为物体在移动时会产生一些噪音。
我已经尝试解决这个问题有一段时间了，并尝试了许多不同的想法；然而，我对计算机视觉和机器学习非常陌生，当我尝试某些事情时，我很可能没有正确地做。
我尝试过使用计算机视觉技术来提取特征、透视线等来确定机器的运动，但效果不佳。再说一次，很可能是我没有正确处理它或没有使用正确的方法。
我当前的方法是使用“设置”当扩展机器连接到物体时，它会监视并记录扩展机器的位置。然后，当给定图像来估计位置时，它会采用机器的边界框并将其转换为记录的先前连接位置。这并没有真正按照我的预期工作，并且对于我的需要来说太硬编码了。
我希望获得一些关于如何解决这个问题的想法，我是否应该使用计算机视觉技术（以及如何使用），或者我是否应该尝试训练机器学习模型等。
本质上，我相对陷入困境，不确定从这里该何去何从。
下面是相机视图的示例。这是运行模型以生成机器周围的框后的有效图像。在此步骤中，我希望能够估计其连接时的位置。如图所示，左侧的机器向外延伸到右侧，连接到右侧的船上。
机器周围有边界框的图像]]></description>
      <guid>https://stackoverflow.com/questions/77783783/approach-to-generate-estimated-position-on-2d-image</guid>
      <pubDate>Tue, 09 Jan 2024 00:23:10 GMT</pubDate>
    </item>
    <item>
      <title>了解 GPT 的大嵌入大小与维数灾难的关系 [关闭]</title>
      <link>https://stackoverflow.com/questions/77783766/understanding-the-large-embedding-size-of-gpt-in-relation-to-the-curse-of-dimens</link>
      <description><![CDATA[我最近了解到，据报道 OpenAI 的 GPT-3 模型的嵌入大小为 12288，与机器学习模型中的典型嵌入相比，这似乎非常大。这提出了几个有趣的问题：

GPT-4 如何有效管理如此大的嵌入大小而不陷入维数灾难？维数灾难通常是指随着维数增加，空间体积增加如此之快，导致可用数据变得稀疏，导致模型可靠性降低的现象。鉴于此，GPT-4 是如何设计来应对与如此高维空间相关的挑战的？

如此大的嵌入大小有什么实际影响？这包括计算要求、训练数据量、泛化能力以及用于缓解维度问题的任何技术等方面。


任何有关大规模语言模型及其高维嵌入处理的见解或相关资源的参考，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/77783766/understanding-the-large-embedding-size-of-gpt-in-relation-to-the-curse-of-dimens</guid>
      <pubDate>Tue, 09 Jan 2024 00:15:17 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：使用 Keras 创建多视图变分自动编码器模型时，层“full_model”的输入 2 与该层不兼容</title>
      <link>https://stackoverflow.com/questions/77782475/valueerror-input-2-of-layer-full-model-is-incompatible-with-the-layer-when-cr</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77782475/valueerror-input-2-of-layer-full-model-is-incompatible-with-the-layer-when-cr</guid>
      <pubDate>Mon, 08 Jan 2024 18:18:19 GMT</pubDate>
    </item>
    <item>
      <title>Python LightGBM错误：joblib.externals.loky.process_executor.TermulatedWorkerError {SIGSEGV(-11)} [重复]</title>
      <link>https://stackoverflow.com/questions/77781970/python-lightgbm-error-joblib-externals-loky-process-executor-terminatedworkerer</link>
      <description><![CDATA[我正在使用 Microsoft 的 lightgbm (lgbm) 库。虽然我的 lgbm 脚本与 xgboost 和随机森林的脚本非常相似（两者都工作正常），但在使用 lgbm 时，我似乎在 Mac Book Pro 和 MacStudios（带有 M1 芯片）上始终收到以下错误：
joblib.externals.loky.process_executor.TermminateWorkerError：执行程序管理的工作进程意外终止。这可能是由于调用函数时出现分段错误或内存使用过多导致操作系统杀死工作线程造成的。
worker 的退出代码为 {SIGSEGV(-11)}
相关代码：
_train_x、_val_x、_train_y、_val_y = train_test_split(_train_x、_train_y、test_size = 0.2)
    
lgbm_model = LGBMClassifier（bagging_fraction = 0.75，bagging_freq = 5，random_state = 42，verbose = -1，force_col_wise = True）
    
kfoldcv = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)
    
lgbm_random_search = RandomizedSearchCV(估计器 = lgbm_model, param_distributions = self._param_dict, n_iter = self.num_searches, cv = kfoldcv, verbose=2, random_state=42, n_jobs=-1)
    
lgbm_random_search.fit(_train_x, _train_y)

self._CrossVal_largest_accscore = lgbm_random_search.best_score_
    
lgbm_model = LGBMClassifier(n_jobs=-1，verbose=-1，force_col_wise=True，bagging_fraction = 0.75，bagging_freq = 5，**lgbm_random_search.best_params_)

lgbm_model.fit(_train_x, _train_y, 回调=[early_stopping(50), log_evaluation(50)], eval_set=[(_val_x,_val_y)])

注意，当我简单地删除子句 njobs=-1 时，我的程序在运行以下行时就会终止：
lgbm_random_search.fit(_train_x, _train_y)
环境：
系统软件概述：
系统版本：macOS 14.0 (23A344)
内核版本：Darwin 23.0.0
启动卷：Macintosh
HD启动模式：正常
安全虚拟内存：已启用
系统完整性保护：已启用

硬件概述：
&lt;预&gt;&lt;代码&gt; 型号名称：MacBook Pro
  型号：MK1F3B/A
  芯片：苹果M1 Pro
  核心总数：10（8 个性能和 2 个效率）
  内存：16GB
  系统固件版本：10151.1.1
  操作系统加载程序版本：10151.1.1
  激活锁状态：已启用

应用软件
Visual Studio 代码==1.72.2
蟒蛇==3.10.121

Python 包
&lt;小时/&gt;
anaconda-client==1.12.0
anaconda-navigator==2.4.2
康达==23.7.2
conda-build==3.26.0
joblib==1.3.0
光GBM==4.0.0
matplotlib==3.7.1
matplotlib-内联==0.1.6
numpy==1.23.5
熊猫==2.0.3
scikit-image==0.20.0
scikit学习==1.3.0
scipy==1.11.1
统计模型==0.14.0
sympy==1.12
xgboost==2.0.0

我已经审查并尝试了以下网站中的一些建议解决方案，但无济于事（例如删除 njobs 参数；添加 &#39;pre_dispatch=2&#39; 参数；重新安装 anaconda、lightgbm、joblib；使用较少数量的估计器 30 到 300等）：

n_jobs=-1 的 GridSearchCV 不适用于决策树/随机森林分类

如何修复/调试 scikit learn 中引发的多进程终止工作错误

GridSearch 中的 TermatedWorkerError

由执行器意外终止

如何修复/调试 scikit learn 中抛出的多进程终止工作错误

TermatedWorkerError：托管工作进程被执行者意外终止

使用 RandomizedSearchCV 时的多个作业问题&lt; /p&gt;

]]></description>
      <guid>https://stackoverflow.com/questions/77781970/python-lightgbm-error-joblib-externals-loky-process-executor-terminatedworkerer</guid>
      <pubDate>Mon, 08 Jan 2024 16:42:21 GMT</pubDate>
    </item>
    <item>
      <title>带有我自己的预训练模型的 Sagemaker 批处理变压器</title>
      <link>https://stackoverflow.com/questions/77781734/sagemaker-batch-transformer-with-my-own-pre-trained-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77781734/sagemaker-batch-transformer-with-my-own-pre-trained-model</guid>
      <pubDate>Mon, 08 Jan 2024 15:54:18 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中具有 NxM 维度的密集层和 M 个独立的 Nx1 密集层之间的梯度和优化差异</title>
      <link>https://stackoverflow.com/questions/77773439/differences-in-gradient-and-optimization-between-a-dense-layer-with-nxm-dimensio</link>
      <description><![CDATA[我想知道这两种设计选择将如何影响训练期间的梯度计算和整体优化过程。

每个结构中反向传播过程中的梯度计算会受到怎样的影响？
梯度流回网络的方式会有差异吗？
这些不同的网络结构是否可能需要或受益于不同的优化算法或学习率？
我的目标是从梯度计算和优化的角度更好地理解每个模型结构选择的含义（潜在的好处和可能的缺点）。

对于此事的任何见解将不胜感激。谢谢。
我还尝试了一个实验，将两个模型的权重初始化为 1。然而，当我比较反向传播后的梯度时，它们是不同的。
张量([[ 0.0289026424, 0.0164458379, -0.0429413468, -0.0317727998,
         -0.0011618818、0.0309777111、0.0496413819、-0.0330999792、
         -0.0217525940, -0.0376570895, 0.0238987785, -0.0303875748,
          0.0422640890, -0.0327035226, -0.0046056593, 0.0095992200,
          0.0047464888, -0.0015923150, -0.0349406302, 0.0358588137,
          0.0109524736、0.0521549769、0.0092769144、0.0338292755、
          0.0418556146、0.0403830707、0.0027946709、-0.0142157376、
          0.0573743209、0.0421377942、0.0161724705、0.0135669028、
          0.0103750098、0.0048434297、-0.0176108982、-0.0011629635、
          0.0177134797, 0.0047528706, 0.0455351323, -0.0127471210,
         -0.0103122834, 0.0092379786, -0.0011389051, 0.0214950778,
          0.0423520021, 0.0157480091, 0.0166458990, -0.0457958765]])
张量([[ 0.0289026424, 0.0164458416, -0.0429413430, -0.0317727998,
         -0.0011618854、0.0309777092、0.0496413782、-0.0330999792、
         -0.0217525922、-0.0376570858、0.0238987822、-0.0303875823、
          0.0422640815, -0.0327035226, -0.0046056607, 0.0095992228,
          0.0047464883, -0.0015923139, -0.0349406339, 0.0358588099,
          0.0109524755、0.0521549769、0.0092769163、0.0338292755、
          0.0418556109、0.0403830633、0.0027946699、-0.0142157376、
          0.0573743209、0.0421377942、0.0161724724、0.0135669019、
          0.0103750126、0.0048434315、-0.0176109001、-0.0011629632、
          0.0177134816, 0.0047528730, 0.0455351323, -0.0127471173,
         -0.0103122853, 0.0092379786, -0.0011389013, 0.0214950833,
          0.0423520021, 0.0157480109, 0.0166459009, -0.0457958728]])
火炬大小([48])
第 12 层的渐变大小为 torch.Size([1, 48])
第 12 层的梯度相同吗？错误的
]]></description>
      <guid>https://stackoverflow.com/questions/77773439/differences-in-gradient-and-optimization-between-a-dense-layer-with-nxm-dimensio</guid>
      <pubDate>Sun, 07 Jan 2024 14:25:52 GMT</pubDate>
    </item>
    <item>
      <title>如何获得每个标记的困惑度而不是平均困惑度？</title>
      <link>https://stackoverflow.com/questions/77433100/how-to-get-perplexity-per-token-rather-than-average-perplexity</link>
      <description><![CDATA[我可以从这里获得整个句子的困惑：
设备=“cuda”
从 Transformers 导入 GPT2LMHeadModel、GPT2TokenizerFast

设备=“cuda”；
model_id =“gpt2”；
模型 = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
已发送 = &#39;生日快乐！&#39;
input_ids = tokenizer(已发送, return_tensors=&#39;pt&#39;)[&#39;input_ids&#39;]
target_ids = input_ids.clone()
输出=模型（input_ids.to（设备），标签=target_ids）
ppl = torch.exp(输出.loss)
打印（人）
&gt;&gt;&gt;张量(1499.6934, device=&#39;cuda:0&#39;, grad_fn=)

但是我怎样才能获得每个标记的困惑度值，而不是整个标记序列的平均困惑度呢？本例中的输入句子 &#39;Happybirthday!&#39; 由 3 个标记组成。基于困惑度公式：

这应该产生 3 个值：第一个标记的对数概率，给定第一个标记的第二个标记的对数概率，以及给定第一个标记的第三个标记的对数概率。每个值都应该取幂以获得以下的困惑度值每个令牌。
我目前有以下内容：
导入火炬
从 Transformers 导入 GPT2LMHeadModel、GPT2TokenizerFast

设备=“cuda”；
model_id =“gpt2”；
模型 = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)

已发送 = &#39;生日快乐！&#39;
input_ids = tokenizer(已发送, return_tensors=&#39;pt&#39;)[&#39;input_ids&#39;].to(设备)
target_ids = input_ids.clone()

# 初始化一个空列表来存储每个标记的困惑度
困惑=[]

# 计算每个 token 的困惑度
对于范围内的 i(input_ids.shape[1])：
    输出 = 模型(input_ids[:, :i+1], labels=target_ids[:, :i+1])
    log_prob = 输出.loss.item()
    困惑 = torch.exp(torch.tensor(log_prob))
    perplexities.append(perplexity.item())

# Perplexities 现在是一个包含每个标记的困惑度值的列表
对于 i，枚举中的令牌（[tokenizer.decode(i) for i in input_ids[0]]）：
    print(f&quot;令牌：{token}，困惑度：{perplexities[i]}&quot;)
    &gt;&gt;&gt;&gt;&gt;标记：快乐，困惑：nan
代币：生日，困惑度：54192.46484375
令牌：！，困惑度：1499.693359375

但我不确定我做错了什么，因为最后一个标记似乎与整个句子具有相同的复杂性。]]></description>
      <guid>https://stackoverflow.com/questions/77433100/how-to-get-perplexity-per-token-rather-than-average-perplexity</guid>
      <pubDate>Mon, 06 Nov 2023 17:30:20 GMT</pubDate>
    </item>
    </channel>
</rss>