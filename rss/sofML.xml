<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 23 Dec 2024 03:23:00 GMT</lastBuildDate>
    <item>
      <title>模型无法正常学习[关闭]</title>
      <link>https://stackoverflow.com/questions/79301299/model-cant-learn-normally</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79301299/model-cant-learn-normally</guid>
      <pubDate>Sun, 22 Dec 2024 16:24:38 GMT</pubDate>
    </item>
    <item>
      <title>单个卷积滤波器是否可以组合来自输入多个通道的值[关闭]</title>
      <link>https://stackoverflow.com/questions/79301125/can-single-convolutional-filter-combine-values-from-input-multiple-channels</link>
      <description><![CDATA[关于卷积神经网络的问题。
第 1 部分：假设输入是 RGB 图像，我们将其放入卷积层。人们是否曾经使用同时对输入的多个通道（例如 R 和 G）进行操作的过滤器，并在计算中结合两个通道的值？换句话说，过滤器矩阵是 3D（或更多）还是 2D？
第 2 部分：如果我错了，请纠正我，但有时在同一层中可以有多个过滤器。我的意思是，也许输入图像有 1 个通道（灰度），但层的输出有 2 个通道。这相当于有 2 个独立的过滤器，即 2 个不同的 3x3 矩阵，每个矩阵产生一个通道。
我理解过滤器是一个单一（例如 3x3）矩阵，我们在输入图像周围移动它并计算它“覆盖”的区域的某个加权平均值。]]></description>
      <guid>https://stackoverflow.com/questions/79301125/can-single-convolutional-filter-combine-values-from-input-multiple-channels</guid>
      <pubDate>Sun, 22 Dec 2024 14:04:26 GMT</pubDate>
    </item>
    <item>
      <title>我可以在 MacbookM4 上使用 CUDA 吗？[重复]</title>
      <link>https://stackoverflow.com/questions/79300848/can-i-use-cuda-on-macbookm4</link>
      <description><![CDATA[我正在尝试为我的本科论文创建一个动作检测系统。
我现在正尝试将 MMSkeleton 集成到我的项目管道中。https://github.com/open-mmlab/mmskeleton
要使用 MMSkeleton，我需要安装 PyTorch 和 torchvision（需要 CUDA），但据我所知，Mac 无法做到这一点。我收到此错误 OSError：编译 MMSkeleton 需要 CUDA！
有人知道我该如何克服这个障碍吗？]]></description>
      <guid>https://stackoverflow.com/questions/79300848/can-i-use-cuda-on-macbookm4</guid>
      <pubDate>Sun, 22 Dec 2024 10:49:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么当 refit=True 时，在 RandomizedSearchCV 之后会进行额外的拟合？</title>
      <link>https://stackoverflow.com/questions/79300159/why-is-an-additional-fitting-performed-after-randomizedsearchcv-when-refit-true</link>
      <description><![CDATA[我正在使用 scikit-learn 中的 RandomizedSearchCV 进行超参数调整，并注意到即使 refit 参数设置为 True，在找到最佳参数后也会执行额外的拟合步骤。
这是一个最小的可重现示例：
来自 sklearn.datasets 导入 make_classification
来自 sklearn.model_selection 导入 RandomizedSearchCV
来自 sklearn.ensemble 导入 RandomForestClassifier
来自 scipy.stats 导入 randint

# 生成合成数据集
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# 定义模型和参数分布
model = RandomForestClassifier(random_state=42)
param_dist = {
&#39;n_estimators&#39;: randint(10, 100),
&#39;max_depth&#39;: randint(3, 20),
}

# 执行 RandomizedSearchCV
search = RandomizedSearchCV(
model, param_dist, n_iter=10, cv=3, random_state=42, refit=True
)
search.fit(X, y)

# 访问最佳估计器
best_model = search.best_estimator_

print(best_model)

我理解 refit=True 标志表示在超参数调整后，应在整个数据集上重新拟合最佳模型。但是，为什么交叉验证中的“最佳模型”不直接作为 search.best_estimator_ 返回？对整个数据集执行另一个拟合步骤的原因是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79300159/why-is-an-additional-fitting-performed-after-randomizedsearchcv-when-refit-true</guid>
      <pubDate>Sat, 21 Dec 2024 21:49:41 GMT</pubDate>
    </item>
    <item>
      <title>使用 MaxViT 进行迁移学习时我的分类器应该是什么？</title>
      <link>https://stackoverflow.com/questions/79300055/what-should-be-my-classifier-in-transfer-learning-using-maxvit</link>
      <description><![CDATA[我正在尝试使用自定义数据集在 Pytorch 预训练模型上进行迁移学习。我已经能够使用 SqueezeNet 成功执行迁移学习。
对于 Squeezenet，我的分类器是，layers source
model.classifier = nn.Sequential(
nn.Dropout(p=0.2),
nn.Conv2d(512, len(class_names), kernel_size=1),
nn.ReLU(inplace=True),
nn.AdaptiveAvgPool2d((1, 1)))

对于 Efficientnet，我的分类器是，layers source
model.classifier = torch.nn.Sequential(
torch.nn.Dropout(p=0.2, inplace=True),
torch.nn.Linear(in_features=1280,
out_features=output_shape,
bias=True))

我也一直在尝试为 MaxViT 做类似的事情，我查看了源代码，发现参数中有 block_channels[-1]。我最近开始用这个，不知道它们是什么，layers source
self.classifier = nn.Sequential(
nn.AdaptiveAvgPool2d(1),
nn.Flatten(),
nn.LayerNorm(block_channels[-1]),
nn.Linear(block_channels[-1], block_channels[-1]),
nn.Tanh(),
nn.Linear(block_channels[-1], num_classes, bias=False),
)

如果需要，以下是我使用 squeezenet 执行迁移学习的完整代码，仅供参考。
weights = torchvision.models.SqueezeNet1_0_Weights.DEFAULT
model = torchvision.models.squeezenet1_0(weights=weights).to(device)
auto_transforms = weights.transforms()
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=d1,
test_dir=d2,
transform=auto_transforms,
batch_size=32)
for param in model.features.parameters():
param.requires_grad = False

torch.manual_seed(42)
torch.cuda.manual_seed(42)
output_shape = len(class_names)

model.classifier = nn.Sequential(
nn.Dropout(p=0.2),
nn.Conv2d(512, len(class_names), kernel_size=1),
nn.ReLU(inplace=True),
nn.AdaptiveAvgPool2d((1, 1))).to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
torch.manual_seed(42)
torch.cuda.manual_seed(42)
results = engine.train(model=model,
train_dataloader=train_dataloader,
test_dataloader=test_dataloader,
optimizer=optimizer,
loss_fn=loss_fn,
epochs=15,
device=device)

我的 MaxViT 分类器应该是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79300055/what-should-be-my-classifier-in-transfer-learning-using-maxvit</guid>
      <pubDate>Sat, 21 Dec 2024 20:24:49 GMT</pubDate>
    </item>
    <item>
      <title>ST-GCN 是否过时了？[关闭]</title>
      <link>https://stackoverflow.com/questions/79297114/is-st-gcn-outdated</link>
      <description><![CDATA[我正在尝试构建一个管道来跟踪视频监控录像中的异常情况。
为了对检测到的人的行为进行分类，我想使用 ST-GCN，但是我能找到的唯一文档是 5 年前更新的，但阅读最新研究 ST-GCN 仍在使用中。
有谁知道更新的文档或能给我一些提示，告诉我如何找到一些关于如何将其实现到我的管道中的信息？]]></description>
      <guid>https://stackoverflow.com/questions/79297114/is-st-gcn-outdated</guid>
      <pubDate>Fri, 20 Dec 2024 11:42:14 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：
&#39;super&#39; 对象没有属性 &#39;\_\_sklearn_tags__&#39;。

当我在 RandomizedSearchCV 对象上调用 fit 方法时会发生这种情况。我怀疑它可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我希望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>在 Google Cloud Functions 中部署 Keras 模型进行预测</title>
      <link>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</link>
      <description><![CDATA[我一直在尝试将一个非常简单的 Keras 玩具模型部署到 Cloud Functions，该模型可以预测图像的类别，但由于未知原因，当执行到 predict 方法时，它会卡住，不会抛出任何错误，最终会超时。
import functions_framework
import io
import numpy as np
import tensorflow as tf

from tensorflow.keras.models import load_model
from PIL import Image

model = load_model(&quot;gs://&lt;my-bucket&gt;/cifar10_model.keras&quot;)

class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]

def preprocess_image(image_file):
img = Image.open(io.BytesIO(image_file.read()))
img = img.resize((32, 32))
img = np.array(img)
img = img / 255.0
img = img.reshape(1, 32, 32, 3)
return img

@functions_framework.http
def predict(request):
image = preprocess_image(request.files[&#39;image_file&#39;])
print(image.shape) # 这会打印 OK
prediction = model.predict(image)
print(prediction) # 永远不会打印
predict_class = class_names[np.argmax(prediction)]
return f&quot;Predicted class: {predicted_class}&quot;

本地调试运行良好，预测速度如预期一样快（模型权重文件为 2MB）。我还在此过程中添加了几个打印（从上面的代码片段中删除），执行工作正常，直到 predict 方法。
即使最小计算配置应该可以工作，我还是尝试保留更多内存和 CPU，但没有任何效果。该模型托管在存储中，我尝试先下载它，但也没有用。我也尝试在 tf.device(&#39;/cpu:0&#39;) 上下文中进行预测，传递 step=1 参数并首先将图像数组转换为 Keras 数据集，如 ChatGPT 所建议的那样，结果相同。实际上，调用 predict 根本没有打印任何内容。调用 call 而不是 predict 没有任何效果。
我错过了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</guid>
      <pubDate>Tue, 17 Dec 2024 13:51:16 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch GRU 错误 RuntimeError：大小不匹配，m1：[1600 x 3]，m2：[50 x 20]</title>
      <link>https://stackoverflow.com/questions/66131870/pytorch-gru-error-runtimeerror-size-mismatch-m1-1600-x-3-m2-50-x-20</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/66131870/pytorch-gru-error-runtimeerror-size-mismatch-m1-1600-x-3-m2-50-x-20</guid>
      <pubDate>Wed, 10 Feb 2021 06:23:22 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的神经网络</title>
      <link>https://stackoverflow.com/questions/61845701/neural-network-in-python</link>
      <description><![CDATA[我最近开始尝试在不使用任何 NW 模块（如 Tensor Flow）的情况下创建自己的神经网络，但我无法将已定义的变量放入函数中，因此我将数据放入文本文件中，然后对其进行读写。虽然它不允许我将权重重新转换为 int，以便我可以将它们乘以它们的学习率。我收到一条错误消息，提示 int 不适用于基数。

你对此有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/61845701/neural-network-in-python</guid>
      <pubDate>Sun, 17 May 2020 01:07:59 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 对缺失标签的支持是什么</title>
      <link>https://stackoverflow.com/questions/58224649/what-is-lightgbms-support-for-missing-labels</link>
      <description><![CDATA[我们有一个数据集，其中某些标签缺失。我们最近才知道这一点，并且删除了这些行。这让我开始思考这到底是怎么回事？给 GBM 举一个没有标签的例子似乎没有意义。
有人能解释一下（双关语）LightGBM 如何处理缺少标签的行吗？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/58224649/what-is-lightgbms-support-for-missing-labels</guid>
      <pubDate>Thu, 03 Oct 2019 18:07:18 GMT</pubDate>
    </item>
    <item>
      <title>glove 和 word2vec 的主要区别是什么？</title>
      <link>https://stackoverflow.com/questions/56071689/whats-the-major-difference-between-glove-and-word2vec</link>
      <description><![CDATA[word2vec 和 glove 有什么区别？
这两种方法都是训练词向量的方法吗？如果是，那么我们该如何使用这两种方法？]]></description>
      <guid>https://stackoverflow.com/questions/56071689/whats-the-major-difference-between-glove-and-word2vec</guid>
      <pubDate>Fri, 10 May 2019 06:10:19 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中初始化权重？</title>
      <link>https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch</link>
      <description><![CDATA[如何初始化网络的权重和偏差（例如通过 He 或 Xavier 初始化）？]]></description>
      <guid>https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch</guid>
      <pubDate>Thu, 22 Mar 2018 16:34:42 GMT</pubDate>
    </item>
    <item>
      <title>神经网络（简单）</title>
      <link>https://stackoverflow.com/questions/46079541/neural-network-simple</link>
      <description><![CDATA[我很好奇为什么我没有打印任何输出，因为代码没有错误。
import numpy as np

class NN():
def _init_(self):
# 种子随机数生成器，因此每次程序运行时都会生成相同的数字
# np.random.seed(1)

# 模型单个神经元，具有 3 个输入连接和 1 个输出连接
# 将随机权重分配给 3x1 矩阵，值范围为 -1 到 1
# 平均值为 0
self.synaptic_weights = 2 * np.random.random((3, 1)) - 1

# 描述 s 形曲线我们传递输入的加权和
# 通过此函数将它们标准化为 0 和 1 之间
def __sigmoid(self, x):
return 1 / (1 + np.exp(-x))

# 梯度sigmoid 曲线
def __sigmoid_derivative(self, x):
return x * (1 - x)

def train(self, training_set_input, training_set_output, number_of_training_iterations):
for iteration in np.xrange(number_of_training_iterations):
# 将训练集通过神经网络
output = self.predict(training_set_input)

error = training_set_output - output

# 将误差乘以输入，再乘以 sigmoid 曲线的梯度
adjustment = np.dot(training_set_input.T, error * self.__sigmoid_derivative(output))

# 调整权重
self.synaptic_weights += adjustment

def predict(self, input):
# 将输入通过神经网络（单个神经元）
return self.__sigmoid(np.dot(inputs, self.synaptic_weights))

if __name__ == &quot;__NN__&quot;:
# 初始化单神经元神经网络
nn = NN()
weightz = nn.synaptic_weights
new_predict = nn.predict(np.array[1, 0, 0])

print(&quot;随机起始突触权重&quot;)
print(weightz)

# T 垂直翻转矩阵
training_set_input = np.array([0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1])
training_set_output = np.array([0, 1, 0, 0]).T

# 使用训练集训练网络
# 执行 10,000 次，每次进行小幅调整
nn.train(training_set_input, training_set_output, 10000)

print(&quot;新的起始突触权重&quot;)
print(weightz)

# 测试
print(&quot;预测&quot;)
print(new_predict)

将文件保存为 NN.py]]></description>
      <guid>https://stackoverflow.com/questions/46079541/neural-network-simple</guid>
      <pubDate>Wed, 06 Sep 2017 15:51:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 MLP 的神经网络分类器</title>
      <link>https://stackoverflow.com/questions/43238285/neural-network-classifier-using-mlp</link>
      <description><![CDATA[我正在开发一个 Python 应用程序，它使用一个数据集对扑克牌进行分类，我将发布一些片段。它似乎效果不佳。它无法正确地对牌进行分类。我得到了以下错误
第 298 行，在 fit 中
raise ValueError(&quot;Multioutput target data is not supports with &quot;
ValueError: Multioutput target data is not supports with label binarization

以下是我的代码：
import pandas as pnd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classes_report
training = pnd.read_csv(&quot;.idea/train.csv&quot;)
training.keys()
training.shape
X = np.array(training)
y = np.array(training)
X_train, X_test, y_train, y_test = train_test_split(X, y)
scaler = StandardScaler()
# 仅适合训练数据
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
mlp = MLPClassifier(hidden_​​layer_sizes=(30, 30, 30, 30, 30, 30, 30, 30, 30))
mlp.fit(X_train, y_train)
predictions = mlp.predict(X_test)
print(classification_report(y_test, predictions))
len(mlp.coefs_)
len(mlp.coefs_[0])
len(mlp.intercepts_[0])

以下是我使用的数据集示例：
图片在这里
这里是数据集的描述：
https://archive.ics.uci.edu/ml/datasets/Poker+Hand
有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/43238285/neural-network-classifier-using-mlp</guid>
      <pubDate>Wed, 05 Apr 2017 17:50:50 GMT</pubDate>
    </item>
    </channel>
</rss>