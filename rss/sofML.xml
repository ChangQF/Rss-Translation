<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 18 Dec 2023 03:15:29 GMT</lastBuildDate>
    <item>
      <title>如何避免非终端深度 Q 学习中每个动作的无限变化</title>
      <link>https://stackoverflow.com/questions/77676680/how-to-avoid-infinite-changes-per-action-in-non-terminal-deep-q-learning</link>
      <description><![CDATA[据我所知，梯度下降的 Deep Q 学习遵循以下过程：

初始化随机权重和偏差

从起始状态开始执行操作

确定奖励

通过梯度下降根据当前时间步长更改权重和偏差

通过梯度下降根据之前的时间步长来更改权重和偏差，但使用奖励 * 折扣因子 ^ 步数而不仅仅是奖励。

重复第 2 步


在无限的时间段内，这应该导致每一步都会导致权重和偏差发生变化，目标是梯度下降的当前奖励+预期未来回报*折扣因子，与贝尔曼匹配方程。然而，根据这种方法，在每个步骤中，我们需要进行与包含该步骤及其之前的每个步骤相同的更改量。在深度 Q 学习的非终结情况下，（据我所知）这应该会导致无限量的所需处理时间。
在我目前的案例中，我正在尝试在恐龙游戏上运行深度 Q 学习，并且假设恐龙可能永远不会死亡，因此可能会导致上述问题。
当折扣因子^步骤低于某个阈值时，潜在的解决方案可能只是简单地舍入为0，或者在某个点任意终止情节并重新开始，但这两种解决方案都没有似乎不完全正确。
Atari DQN 研究论文
在 Atari 论文中，他们似乎使用有限的内存大小来保存所需的所有状态，然后仅随机选择一个状态来执行梯度下降。这是正确的解释吗？这可能是我面临的问题的解决方案吗？还有其他可能的解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/77676680/how-to-avoid-infinite-changes-per-action-in-non-terminal-deep-q-learning</guid>
      <pubDate>Mon, 18 Dec 2023 02:35:47 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中使用短/跳过连接实现 RNN</title>
      <link>https://stackoverflow.com/questions/77676628/implementing-rnn-with-short-skip-connections-in-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77676628/implementing-rnn-with-short-skip-connections-in-pytorch</guid>
      <pubDate>Mon, 18 Dec 2023 02:07:05 GMT</pubDate>
    </item>
    <item>
      <title>TD(1) 如何等同于蒙特卡罗采样？</title>
      <link>https://stackoverflow.com/questions/77676320/how-td1-is-equivalent-to-monte-carlo-sampling</link>
      <description><![CDATA[我需要证明给定 λ=1，TD(λ) = Monte-Carlo。我在几个网站上阅读过，但我不知道如何得出证明。
我们知道以下内容：
TD(lambda) 和蒙特卡洛方程
但是，如果在 λ-&gt;0 时设置限制。 1 使用 TD(λ) 目标方程，您会得到如下结果：
lim(λ-&gt;1) (1-λ) ...
所以设λ = 1，(1-λ) = 0，则所有表达式都为0。
如果有人可以提供有关如何摆脱此 (1-λ) 的信息，以分析证明 TD(1) 相当于蒙特卡洛，我将非常感激。]]></description>
      <guid>https://stackoverflow.com/questions/77676320/how-td1-is-equivalent-to-monte-carlo-sampling</guid>
      <pubDate>Sun, 17 Dec 2023 23:17:25 GMT</pubDate>
    </item>
    <item>
      <title>在电脑上训练我的模型，然后在微控制器上使用它[关闭]</title>
      <link>https://stackoverflow.com/questions/77675723/train-my-model-on-pc-then-use-it-on-microcontroller</link>
      <description><![CDATA[如果我想在我的 PC 上训练一个模型（无论是 ML、NN 还是 CNN），因为我有强大的 GPU，是否可以在 Arduino 或 Raspberry Pi Pico 等微控制器上导出或保存这个训练模型以直接使用它？或者我需要从头开始重新训练这些模型？]]></description>
      <guid>https://stackoverflow.com/questions/77675723/train-my-model-on-pc-then-use-it-on-microcontroller</guid>
      <pubDate>Sun, 17 Dec 2023 19:33:23 GMT</pubDate>
    </item>
    <item>
      <title>从磁盘加载和使用 PyTorch 模型</title>
      <link>https://stackoverflow.com/questions/77675553/loading-and-using-a-pytorch-model-from-disk</link>
      <description><![CDATA[我是 PyTorch 的新手，在第一页上 - https:/ /pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html
该页面希望我在同一个文件中运行整个代码。即创建模型、训练模型、将其保存到磁盘、从磁盘加载并使用模型进行推理。
我发现推理代码依赖于训练期间完成的模型定义。我的问题是，我是否需要知道模型定义才能使用预训练模型？这似乎是错误的。我确信事实并非如此。那么，如何使用保存的模型进行推理呢？
谷歌搜索向我展示了使用 python 的导入函数导入模型的实例。但是，我找不到如何从磁盘加载模型并在不知道模型定义的情况下使用它的示例。请问有什么帮助吗？
我想将下面代码的模型加载和推理部分保存到另一个文件并运行它。我不知道该怎么做
导入火炬
从火炬导入 nn
从 torch.utils.data 导入 DataLoader
从 torchvision 导入数据集
从 torchvision.transforms 导入 ToTensor

# 从开放数据集中下载训练数据。
训练数据 = 数据集.FashionMNIST(
    根=“数据”，
    火车=真，
    下载=真，
    变换=ToTensor(),
）

# 从开放数据集中下载测试数据。
test_data = 数据集.FashionMNIST(
    根=“数据”，
    火车=假，
    下载=真，
    变换=ToTensor(),
）

批量大小 = 64

# 创建数据加载器。
train_dataloader = DataLoader(training_data,batch_size=batch_size)
test_dataloader = DataLoader(test_data,batch_size=batch_size)

对于 test_dataloader 中的 X、y：
    print(f&quot;X 的形状 [N, C, H, W]: {X.shape}&quot;)
    print(f&quot;y 的形状：{y.shape} {y.dtype}&quot;)
    休息


# 获取 cpu、gpu 或 mps 设备进行训练。
设备=（
    “cuda”
    如果 torch.cuda.is_available()
    否则“mps”
    如果 torch.backends.mps.is_available()
    否则“CPU”
）
print(f“使用 {device} 设备”)

# 定义模型
神经网络类（nn.Module）：
    def __init__(自身):
        超级().__init__()
        self.flatten = nn.Flatten()
        self. Linear_relu_stack = nn.Sequential(
            nn.线性(28*28, 512),
            ReLU(),
            nn.线性(512, 512),
            ReLU(),
            nn.线性(512, 10)
        ）

    def 前向（自身，x）：
        x = self.展平(x)
        logits = self. Linear_relu_stack(x)
        返回逻辑值

模型 = NeuralNetwork().to(设备)
print(model)# 获取 cpu、gpu 或 mps 设备进行训练。

loss_fn = nn.CrossEntropyLoss()
优化器 = torch.optim.SGD(model.parameters(), lr=1e-3)

def train（数据加载器，模型，loss_fn，优化器）：
    大小 = len(dataloader.dataset)
    模型.train()
    对于批处理，枚举（dataloader）中的（X，y）：
        X, y = X.to(设备), y.to(设备)

        # 计算预测误差
        预测值=模型(X)
        损失 = loss_fn(pred, y)

        # 反向传播
        loss.backward()
        优化器.step()
        优化器.zero_grad()

        如果批次 % 100 == 0:
            损失，当前 = loss.item(), (batch + 1) * len(X)
            print(f&quot;loss: {loss:&gt;7f} [{current:&gt;5d}/{size:&gt;5d}]&quot;)



def 测试（数据加载器、模型、loss_fn）：
    大小 = len(dataloader.dataset)
    num_batches = len(数据加载器)
    模型.eval()
    test_loss, 正确 = 0, 0
    使用 torch.no_grad()：
        对于数据加载器中的 X、y：
            X, y = X.to(设备), y.to(设备)
            预测值=模型(X)
            test_loss += loss_fn(pred, y).item()
            正确 += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    正确/=尺寸
    print(f&quot;测试误差:\n 准确度:{(100*正确):&gt;0.1f}%, 平均损失:{test_loss:&gt;8f}\n&quot;)



历元 = 5
对于范围内的 t（纪元）：
    print(f&quot;历元 {t+1}\n--------------------------------------------&quot;)
    火车（train_dataloader，模型，loss_fn，优化器）
    测试（test_dataloader，模型，loss_fn）
print(&quot;完成！&quot;)

torch.save(model.state_dict(), &quot;model.pth&quot;)
print(&quot;将 PyTorch 模型状态保存到 model.pth&quot;)


模型 = NeuralNetwork().to(设备)
model.load_state_dict(torch.load(“model.pth”))

类=[
    “T 恤/上衣”，
    “裤子”，
    “套头衫”，
    “礼服”，
    “外套”，
    “凉鞋”，
    “衬衫”，
    “运动鞋”，
    “袋子”，
    “踝靴”，
]

模型.eval()
x, y = 测试数据[0][0], 测试数据[0][1]
使用 torch.no_grad()：
    x = x.to(设备)
    预测值=模型(x)
    预测，实际=类[pred[0].argmax(0)]，类[y]
    print(f&#39;预测：“{预测}”，实际：“{实际}”&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/77675553/loading-and-using-a-pytorch-model-from-disk</guid>
      <pubDate>Sun, 17 Dec 2023 18:40:15 GMT</pubDate>
    </item>
    <item>
      <title>用于音频分类的 CNN 模型</title>
      <link>https://stackoverflow.com/questions/77675519/cnn-model-for-audio-classification</link>
      <description><![CDATA[我想构建一个可以使用 CNN 识别仪器的系统。但是，我对如何进行学习有一些疑问。

现在，其他研究使用的大多数方法都涉及使用频谱图作为 CNN 模型的输入。如果我使用原始值（即音频特征值数组）作为 CNN 的输入，我的系统性能会受到影响吗？这个办法可行吗？
我想为我的项目提取多种音频特征，例如 MFCC、梅尔频谱图、频谱质心、频谱通量等。我是否只需要选择一种特征来输入到我的 CNN 模型中，或者将它们组合起来实际上可行并将其作为我对 CNN 的输入？如果后者可行，我该怎么做？是否需要某种特征选择或降维技术（即 PCA/TSNE）？
]]></description>
      <guid>https://stackoverflow.com/questions/77675519/cnn-model-for-audio-classification</guid>
      <pubDate>Sun, 17 Dec 2023 18:32:22 GMT</pubDate>
    </item>
    <item>
      <title>我不明白如何训练元学习器或它如何预测</title>
      <link>https://stackoverflow.com/questions/77675237/i-dont-understand-how-to-train-a-meta-learner-or-how-it-predicts</link>
      <description><![CDATA[我一直在学习机器学习模型，并且遇到了元学习器的概念。
据我所知，这些方法通常通过将多个分类器的预测作为特征、将真实标签作为标签并在此基础上训练另一个模型来工作。
如果我有两个情感分类器，则会给出以下结果：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

文本
真实标签
CL 1 标签
CL 1 概率
CL 2 标签
CL 2 概率


&lt;正文&gt;

快乐
积极
积极
0.9
积极
0.7


悲伤
否定
积极
0.7
否定
0.9


生气
否定
否定
0.8
积极
0.6


内容
积极
积极
0.5
否定
0.8




我训练了一个新模型，将真实标签作为标签，将概率作为特征。新模型如何知道概率指的是哪个标签？基分类器可以以 0.9 的确定性预测任一标签。那么元学习器也应该包含基分类器标签吗？或者基分类器标签是否是元学习器的特征？
一旦元学习器经过训练，我假设它将在测试数据的文本部分上进行测试。但由于它没有学习文本中包含的任何特征，它到底在预测什么？]]></description>
      <guid>https://stackoverflow.com/questions/77675237/i-dont-understand-how-to-train-a-meta-learner-or-how-it-predicts</guid>
      <pubDate>Sun, 17 Dec 2023 16:54:36 GMT</pubDate>
    </item>
    <item>
      <title>根据新数据训练 NLP 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/77674787/training-nlp-model-on-new-data</link>
      <description><![CDATA[我正在开发一个 NLP 模型（使用 LSTM），该模型接收文本句子（例如谷歌地图评论）并预测星级评分。
我掌握了拥有超过 600 万条评论的 yelp 数据集，我正在用它来训练我的模型。数据集太大，导致我分批训练模型（10K 批审核）。
假设我完成了训练并且取得了合理的表现。随后，Yelp 发布了 100 万条新评论。如何在新数据上训练模型？
1- 我应该仅根据新数据训练模型吗？
或者
2- 将新数据与旧数据结合并重新训练模型？
3-如何避免灾难性遗忘？]]></description>
      <guid>https://stackoverflow.com/questions/77674787/training-nlp-model-on-new-data</guid>
      <pubDate>Sun, 17 Dec 2023 14:36:08 GMT</pubDate>
    </item>
    <item>
      <title>关于PyTorch中matmul的批量效果——什么时候减少？</title>
      <link>https://stackoverflow.com/questions/77673891/about-batched-effect-of-matmul-in-pytorch-when-to-reduce</link>
      <description><![CDATA[bp 在批处理 matmul 场景中如何工作？假设我们有一个线性，x的形状是[b,m,k]，W的形状是[k,n]：
&lt;前&gt;&lt;代码&gt; y = xW

然后：
 \frac{\partial L}{\partial W} = x^T\frac{\partial L}{\partial y}

那么W后面发生了什么？

我们保存完整的 $x^T$ 张量 [b,k,m]，并执行批处理的 matmul [b,k,m] x [b,m,n ]，最后将结果归约到[k,n]得到梯度。
我们直接保存平均$x^T$张量[k,m]，并执行批处理的matmul [k,m] x [m,n]获取梯度。

我已经阅读了 PyTorch 的源代码，但我在理解它时遇到了问题。]]></description>
      <guid>https://stackoverflow.com/questions/77673891/about-batched-effect-of-matmul-in-pytorch-when-to-reduce</guid>
      <pubDate>Sun, 17 Dec 2023 09:27:33 GMT</pubDate>
    </item>
    <item>
      <title>自定义神经网络输出层的梯度爆炸问题</title>
      <link>https://stackoverflow.com/questions/77673873/gradient-explosion-issue-with-custom-neural-network-output-layer</link>
      <description><![CDATA[我目前正在制作拟合函数来训练我的神经网络的自定义输出层。当将我的输出层拟合到数据集时，权重很快就会变得非常大。通过迭代 5-6，我已经获得了 Nan 值。
我尝试过提高学习率，但没有成功。我相当确定问题出在我用来计算调整权重步骤的公式中，因为它纯粹是我自己得出的，没有确认其正确性。
这是我的代码。我不确定是否应该包含 Neuron 类。
将 matplotlib 导入为 plt
从神经元导入*
导入时间
将 matplotlib.pyplot 导入为 plt

类输出层：
    def __init__(自身，input_shape，output_shape，activation_func=none)：
        神经元层 = np.array([])
        对于范围内的 x(output_shape)：
            神经元层 = np.append(神经元层,感知器(n_of_weights=input_shape, 步骤=activation_func, 激活=activation_func))
        self.neurons = 神经元_层
    def get_neuron(self, n):
        返回自身神经元[n]
    
    def get_neurons(self):
        返回自身神经元
    
    def set_neurons(self, adj_neurons):
        self.神经元 = adj_neurons
    
    defforward_pass（自身，X）：
        输出 = np.array([])
        对于 self.neurons 中的神经元：
            输出 = np.append(输出,neuron.step_pass(X))
        返回输出
    
    def relu（自身，输入）：
        如果输入&gt;0：
            返回输入
        别的：
            返回0
        
    def drelu（自身，输入）：
        如果输入&gt;0：
            返回1
        别的：
            返回0

    def sigmoid（自身，输入）：
        返回 1/(1+np.e**(-输入))

    def dsigmoid（自身，输入）：
        返回 self.sigmoid(输入)*(1-self.sigmoid(输入))
    
    def 失活（自身，神经元，输入）：
        激活=神经元.get_activation()
        如果激活==&#39;sigmoid&#39;：
            返回 dsigmoid(neuron.raw_pass(input))
        elif激活==&#39;relu&#39;：
            返回 drelu(neuron.raw_pass(input))
        别的：
            返回神经元.raw_pass(输入)
    
    def fit(自我, X, y, 学习率):
        Weight_change = [[w] for w in self.get_neurons()[0].get_weights()]
        错误 = [0]
        神经元 = self.get_neurons()
        对于范围内的 k(len(X))：
            对于神经元中的 n：
                权重 = n.get_weights()[:-1]
                print(&quot;通过：&quot;,self.forward_pass(X[k]))
                a = [(learning_rate*(self.forward_pass(X[k])-y[k])*self.dactivation(n, X[k])*X[k][x])[0] 对于 x 在范围内(长度(权重))]
                print(&quot;输出：&quot;,y[k])
                print(&quot;输入：&quot;,X[k])
                print(&quot;权重&quot;,n.get_weights())
                adj_weights = [权重[x]-(learning_rate*(self.forward_pass(X[k])-y[k])*self.dactivation(n, X[k])*X[k][x])[0 ] 对于范围内的 x(len(权重))]
                adj_weights.append(n.get_weights()[-1:][0]-(learning_rate*(self.forward_pass(X[k])-y[k])*self.dactivation(n, X[k])) [0]）
                n.change_weights(adj_weights)
                print(&#39;权重:&#39;,adj_weights)
                print(&#39;新通行证：&#39;,self.forward_pass(X[k]))
                对于范围内的 x(len(adj_weights))：
                    weight_change[x].append(adj_weights[x])
            error.append(y[k]-n.step_pass(X[k]))
            self.set_neurons(神经元)
        返回weight_change，错误```
]]></description>
      <guid>https://stackoverflow.com/questions/77673873/gradient-explosion-issue-with-custom-neural-network-output-layer</guid>
      <pubDate>Sun, 17 Dec 2023 09:19:07 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用 dataloader 来存储训练数据会改变模型的训练？</title>
      <link>https://stackoverflow.com/questions/77673297/why-does-the-usage-of-dataloader-for-train-data-change-the-training-of-the-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77673297/why-does-the-usage-of-dataloader-for-train-data-change-the-training-of-the-model</guid>
      <pubDate>Sun, 17 Dec 2023 04:17:54 GMT</pubDate>
    </item>
    <item>
      <title>端到端 ML 项目的模型训练器问题 - TypeError：initiate_model_training() 缺少 4 个必需的位置参数</title>
      <link>https://stackoverflow.com/questions/77673255/model-trainer-issue-on-end-to-end-ml-project-typeerror-initiate-model-trainin</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77673255/model-trainer-issue-on-end-to-end-ml-project-typeerror-initiate-model-trainin</guid>
      <pubDate>Sun, 17 Dec 2023 03:50:40 GMT</pubDate>
    </item>
    <item>
      <title>从时间和非时间数据进行预测[关闭]</title>
      <link>https://stackoverflow.com/questions/77667561/making-predictions-from-temporal-and-non-temporal-data</link>
      <description><![CDATA[我正在研究一个回归问题来预测由 2 个参数组成的目标。这两个参数将根据时间 (YYYY-MM-DD HH:MM) 和非时间数据组成的特征进行预测。
我开始基于“决策树回归”构建 Python 代码算法。尽管数据趋势看起来令人满意，但我的模型似乎产生了与用于训练/测试的数据范围相同的输出。我想知道我的方法好不好。我偶然发现了一些处理相同问题的论文，并使用 CNN-LSTM 模型等进行了解决。这里是我的代码（Google Colab 文件）的链接： https:// drive.google.com/file/d/1ar5Z8kMXx_9slc1e_MMzaAcQAvx5lmzr/view?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/77667561/making-predictions-from-temporal-and-non-temporal-data</guid>
      <pubDate>Fri, 15 Dec 2023 16:25:51 GMT</pubDate>
    </item>
    <item>
      <title>如何为此笔记本创建 Sagemaker 端点？</title>
      <link>https://stackoverflow.com/questions/77660996/how-can-i-create-a-sagemaker-endpoint-for-this-notebook</link>
      <description><![CDATA[我创建了一个 VectorDB (FAISS) 并将 PDF 输入到其中。然后我使用 AWS Bedrock 的 Langchain 包装器来调用它。我知道现在存在 Kowledge Base，但至少在 SageMaker 笔记本中，我有更多的控制权。该模型在 SageMaker Notebook 中完美运行，当我提出问题时，它会返回答案。
我想做的是创建一个小网页（并通过 HTTP/REST API），只需在文本字段中提交问题并在文本字段中接收答案。我猜如果链中某个地方没有 Lambda 函数，这很难做到，或者也许不是？
当我查看 Sagemaker 控制台的推理选项卡下时，没有模型或没有端点，或者没有&lt; /strong&gt; 端点配置（因为我没有从 Sagemaker 选择模型，所以我只是在 Python 笔记本中使用 langchain LLM 和 Bedrock，如下所示）。
&lt;前&gt;&lt;代码&gt;导入boto3
导入 json

bedrock = boto3.client(service_name=&quot;bedrock&quot;)
bedrock_runtime = boto3.client(service_name=“bedrock-runtime”)



从 langchain.llms.bedrock 导入 Bedrock
从 langchain.chains 导入 RetrievalQA
从 langchain.prompts 导入 PromptTemplate

嵌入 = BedrockEmbeddings(model_id=“amazon.titan-embed-text-v1”,
                               客户端=bedrock_runtime）

最终我将文档嵌入到 FAISS Vector 数据库中，我查询的就是这个数据库
db = FAISS.from_documents（文档，嵌入）


模型泰坦 = {
    “最大令牌计数”：512，
    “停止序列”：[]，
    “温度”：0.0，
    “顶部P”：0.5
}

# 亚马逊泰坦模型
llm = 基岩(
    model_id=&quot;amazon.titan-text-express-v1&quot;,
    客户端=bedrock_runtime，
    model_kwargs=model_titan,
）

然后定义一个提示......
提示 = 提示模板(
    template=prompt_template, input_variables=[“上下文”, “问题”]
）

并查询数据库：
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=“东西”，
    检索器=db.as_retriever(
        search_type=“相似度”，
    ),
    return_source_documents=真，
    chain_type_kwargs={“提示”: 提示},
）



query =“未来的技术是什么样的？”

结果 = qa({“查询”: 查询})

print(f&#39;查询: {结果[“查询”]}\n&#39;)
print(f&#39;结果: {结果[“结果”]}\n&#39;)
print(f&#39;上下文文档：&#39;)
对于结果 [“source_documents”] 中的 srcdoc：
      打印（f&#39;{srcdoc}\n&#39;）

这恰好返回了我在 Sagemaker 中需要的内容，我只需要从外部查询数据库即可。
我不想让 lambda 函数每次都重建链。我考虑的是效率，我需要的只是在 lambda 函数中传递查询并返回结果。]]></description>
      <guid>https://stackoverflow.com/questions/77660996/how-can-i-create-a-sagemaker-endpoint-for-this-notebook</guid>
      <pubDate>Thu, 14 Dec 2023 14:49:20 GMT</pubDate>
    </item>
    <item>
      <title>不使用 OpenAI Gym 环境的近端策略优化代码 [关闭]</title>
      <link>https://stackoverflow.com/questions/77641484/proximal-policy-optimization-code-without-using-openai-gym-environments</link>
      <description><![CDATA[我必须在 Python 中对使用在线物理系统收集的数据实施近端策略优化。我见过的所有示例都使用 OpenAI 的 Gym 环境。我将如何修改/设置使用来自我的收集系统的数据而不是健身房环境数据的实现？]]></description>
      <guid>https://stackoverflow.com/questions/77641484/proximal-policy-optimization-code-without-using-openai-gym-environments</guid>
      <pubDate>Mon, 11 Dec 2023 17:58:03 GMT</pubDate>
    </item>
    </channel>
</rss>