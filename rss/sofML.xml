<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 15 Apr 2024 12:25:13 GMT</lastBuildDate>
    <item>
      <title>预测模型 - Python - SARIMA</title>
      <link>https://stackoverflow.com/questions/78328211/forecast-model-python-sarima</link>
      <description><![CDATA[我在将 SARIMA 模型应用到 Python 中的数据集时遇到了问题 - 我正在使用百货商店的商店销售数据，并希望将明年分为几个季度进行预测。数据具有稳定性，我已将历史数据分成四分之一。数据来源截至2017年12月31日。
请参阅下面的 Python 代码和输出
&lt;前&gt;&lt;代码&gt;
从 statsmodels.tsa.statespace.sarimax 导入 SARIMAX
模型= SARIMAX（季度销售，订单=订单，季节性订单=季节性订单）
结果=模型。适合（）
&lt;前&gt;&lt;代码&gt;
预测 = results.get_forecast(steps=4)
Forecast_index = pd.date_range(start=&#39;2013-01-01&#39;, period=4, freq=&#39;Q&#39;)
Forecast_series = pd.Series(forecast.predicted_mean,index=forecast_index)`


打印（预测_系列）

# 绘制历史季度销售数据
quarterly_sales.plot(kind=&#39;bar&#39;, Figsize=(10, 6), label=&#39;历史季度销售额&#39;)

# 检查预测指数是否与预期的未来季度一致
打印（预测系列.索引）

# 以更高的可见性覆盖预测销售额
plt.plot(forecast_series.index,forecast_series,color=&#39;red&#39;,marker=&#39;o&#39;,linestyle=&#39;dashed&#39;,linewidth=2,label=&#39;预测季度销售额&#39;)


plt.ylim(0, max(quarterly_sales.max(), Forecast_series.max()) * 1.1)


plt.title(&#39;季度销售额及预测&#39;)
plt.xlabel(&#39;季度&#39;)
plt.ylabel(&#39;销售&#39;)
plt.xticks（旋转=45）


plt.图例()

plt.show()`


new_index = [f&quot;Q{date.quarter} {str(date.year)[-2:]}&quot;&quot;对于 Forecast_series.index 中的日期]
Forecast_series.index = new_index`

`将 matplotlib.pyplot 导入为 plt


Historical_index = [f&quot;Q{date.quarter} {str(date.year)[-2:]}&quot;&quot;对于quarterly_sales.index 中的日期]
季度销售指数 = 历史指数


quarterly_sales.plot(kind=&#39;bar&#39;, Figsize=(10, 6), label=&#39;历史季度销售额&#39;)


plt.plot(forecast_series.index,forecast_series,color=&#39;red&#39;,marker=&#39;o&#39;,linestyle=&#39;dashed&#39;,label=&#39;预测季度销售额&#39;)

plt.title(&#39;季度销售额及预测&#39;)
plt.xlabel(&#39;季度&#39;)
plt.ylabel(&#39;销售&#39;)
plt.xticks（旋转=45）


plt.图例()

plt.show()`

如上所述，我已经尝试过，但我在可视化图表上看不到任何我对未来 12 个月的预测，尽管它显示在图例的屏幕截图中。]]></description>
      <guid>https://stackoverflow.com/questions/78328211/forecast-model-python-sarima</guid>
      <pubDate>Mon, 15 Apr 2024 11:57:06 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Streamlit 中使推荐的书籍可点击以触发新的推荐？</title>
      <link>https://stackoverflow.com/questions/78328128/how-to-make-a-recommended-book-clickable-to-trigger-new-recommendations-in-strea</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78328128/how-to-make-a-recommended-book-clickable-to-trigger-new-recommendations-in-strea</guid>
      <pubDate>Mon, 15 Apr 2024 11:42:56 GMT</pubDate>
    </item>
    <item>
      <title>不标准化所有特征（回归）是不是很糟糕？</title>
      <link>https://stackoverflow.com/questions/78328094/is-it-bad-not-to-standardize-all-features-regression</link>
      <description><![CDATA[我正在使用具有两个隐藏层的神经网络来执行回归任务。我的训练集输出值从 0 到 2000 变化，测试集输出值从 0 到 600 变化。我的主要问题是过度拟合。
我标准化了一些具有更大值的输入特征（StandardScaler），但我没有标准化那些值在 -6 到 10 范围内的特征，因为我认为它们足够小，不会引起问题。

这是错误的吗？这会增加过度拟合吗？

我目前正在测试具有负斜率 (0.01) 的 ReLu，以某种方式考虑负输入（标准化或非标准化）。是否有更好的方法或更好的标准化激活函数组合来实现最佳测试集预测？


如果有任何解释或想法，我将不胜感激。
提前非常感谢您。]]></description>
      <guid>https://stackoverflow.com/questions/78328094/is-it-bad-not-to-standardize-all-features-regression</guid>
      <pubDate>Mon, 15 Apr 2024 11:34:22 GMT</pubDate>
    </item>
    <item>
      <title>句子相似度的加权输入</title>
      <link>https://stackoverflow.com/questions/78327620/weighted-input-for-sentence-similarity</link>
      <description><![CDATA[我正在使用点分数构建句子相似性的语言模型。目前，我正在使用 gte-large来自 Hugging Face 的语言模型。
我想知道是否有一种方法可以对文本进行加权输入。例如输入句子
蟋蟀昆虫
有没有办法在这里给予昆虫更多的权重，以便与大量的单词相比，它与昆虫比板球这项运动显示出更多的相似性？]]></description>
      <guid>https://stackoverflow.com/questions/78327620/weighted-input-for-sentence-similarity</guid>
      <pubDate>Mon, 15 Apr 2024 10:09:50 GMT</pubDate>
    </item>
    <item>
      <title>暹罗神经网络（机器学习）</title>
      <link>https://stackoverflow.com/questions/78327320/siamese-neural-network-machine-learning</link>
      <description><![CDATA[我正在尝试训练暹罗神经网络，但每当运行 train.py 文件时都会遇到此错误
加载成功键：[]……
成功加载密钥编号：0

无法加载密钥：[&#39;features.0.weight&#39;, &#39;features.0.bias&#39;, &#39;features.2.weight&#39;, &#39;features.2.bias&#39;, &#39;features.5.weight&#39;, &#39;features.5 .bias&#39;、&#39;features.7.weight&#39;、&#39;features.7.bias&#39;、&#39;features.10.weight&#39;、&#39;features.10.bias&#39;、&#39;features.12.weight&#39;、&#39;features.12.bias &#39;, &#39;features.14.weight&#39;, &#39;features.14.bias&#39;, &#39;features.17.weight&#39;, &#39;features.17.bias&#39;, &#39;features.19.weight&#39;, &#39;features.19.bias&#39;, &#39;features.21.weight&#39;, &#39;features.21.bias&#39;, &#39;features.24.weight&#39;, &#39;features.24.bias&#39;, &#39;features.26.weight&#39;, &#39;features.26.bias&#39;, &#39;fe ……
无法加载密钥编号：32

这是加载代码：
model = Siamese(input_shape, 预训练)
    如果 model_path != &#39;&#39;:
        如果 local_rank == 0：
            print(&#39;加载权重{}.&#39;.format(model_path))
        model_dict = model.state_dict()
        pretrained_dict = torch.load(model_path, map_location=device)
        load_key、no_load_key、temp_dict = []、[]、{}
        对于 pretrained_dict.items() 中的 k、v：
            如果 model_dict.keys() 中的 k 和 np.shape(model_dict[k]) == np.shape(v)：
                temp_dict[k] = v
                load_key.append(k)
            别的：
                no_load_key.append(k)
        model_dict.update(temp_dict)
        model.load_state_dict(model_dict)
        如果 local_rank == 0：
            print(&quot;\n成功加载密钥：&quot;, str(load_key)[:500], &quot;……\n成功加载密钥编号：&quot;, len(load_key))
            print(&quot;\n加载密钥失败:&quot;, str(no_load_key)[:500], &quot;……\n加载密钥失败:&quot;, len(no_load_key))

我看到了几篇关于 nn.Dataparallel 的帖子，但我不太确定如何去做。我对机器学习有点陌生]]></description>
      <guid>https://stackoverflow.com/questions/78327320/siamese-neural-network-machine-learning</guid>
      <pubDate>Mon, 15 Apr 2024 09:19:14 GMT</pubDate>
    </item>
    <item>
      <title>寻求建议：建立一个模型来区分手写数字和印刷数字</title>
      <link>https://stackoverflow.com/questions/78327181/seeking-advice-building-a-model-to-differentiate-handwritten-vs-printed-digits</link>
      <description><![CDATA[我正在开发一种机器学习模型，可以准确识别图像中的数字是手写的还是打印的。尽管可以访问 Char74K 和 MNIST 等数据集，但我仍在努力让我的模型表现良好。
我对基本机器学习模型有一些经验，但我发现这个特殊的挑战有点令人畏惧。
模型尝试：鉴于卷积神经网络 (CNN) 在基于图像的任务中取得的成功，我尝试使用它，但结果并不理想。
我正在寻求以下方面的建议：
最佳模型架构：是否有适合此类任务的特定架构或层？
有效的预处理：关于可能提高模型性能的预处理技术有什么建议吗？
数据集平衡和增强：有关如何平衡和增强这些数据集以获得更好训练结果的提示。
如果有人解决过类似问题或了解相关文献或教程，我将非常感谢您的见解。
非常感谢您花时间阅读并回复。任何指导或参考将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78327181/seeking-advice-building-a-model-to-differentiate-handwritten-vs-printed-digits</guid>
      <pubDate>Mon, 15 Apr 2024 08:52:41 GMT</pubDate>
    </item>
    <item>
      <title>基于人工智能的扩展的模型建议</title>
      <link>https://stackoverflow.com/questions/78326814/model-suggestion-for-ai-based-scaling</link>
      <description><![CDATA[我们正在探索根据给定大小缩放 UI 容器内元素的想法。
容器用json对象表示，例如：
&lt;前&gt;&lt;代码&gt;{
   “尺寸”：“小”，
   “元素”：[
      {
         “类型”：“图像”，
         “x”：348，
         “y”：36，
         “宽度”：607，
         “高度”：201
      },
      {
         “类型”：“文本”，
         “x”：45，
         “y”：100，
         “宽度”：100，
         “高度”：104
      }
   ]
}

我们将为 medium 和 large 尺寸提供类似的 JSON，并调整 x,y 坐标和 width 以及JSON 中每个元素的 height。
目前，这些调整是通过基于规则的算法来处理的，该算法有时效果不佳，我们必须手动调整坐标或尺寸。
是否有任何机器学习模型可以有效地解决这个问题，并在我们用足够的数据训练它的情况下建议坐标和尺寸？]]></description>
      <guid>https://stackoverflow.com/questions/78326814/model-suggestion-for-ai-based-scaling</guid>
      <pubDate>Mon, 15 Apr 2024 07:44:35 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法仅使用 LangChain 或其他 OpenAI API 上的口头指令来自动化整个机器学习/数据可视化流程工作流程？</title>
      <link>https://stackoverflow.com/questions/78326755/is-there-any-way-to-automate-the-entire-ml-data-visualization-process-workflow-u</link>
      <description><![CDATA[我打算在应用程序中使用开放式 AI API/插件来自动化整个机器学习工作流程，其中包括图像分类和基于规则的系统，以根据数据结果生成通用推荐和智能洞察。
我找到了通过 YouTube 实现的示例，但不确定如何将其用于定制应用程序。
导入操作系统
导入系统

导入openai
从 langchain.chains 导入 ConversationalRetrievalChain，RetrievalQA
从 langchain.chat_models 导入 ChatOpenAI
从 langchain.document_loaders 导入 DirectoryLoader、TextLoader
从 langchain.embeddings 导入 OpenAIEmbeddings
从 langchain.indexes 导入 VectorstoreIndexCreator
从 langchain.indexes.vectorstore 导入 VectorStoreIndexWrapper
从 langchain.llms 导入 OpenAI
从 langchain.vectorstores 导入 Chroma

导入常量

os.environ[“OPENAI_API_KEY”] = Constants.APIKEY

# 启用保存到磁盘&amp;重用模型（对相同数据进行重复查询）
坚持=错误

查询=无
如果 len(sys.argv) &gt; 1：
  查询 = sys.argv[1]

如果 PERSIST 和 os.path.exists(“persist”)：
  print(&quot;重用索引...\n&quot;)
  Vectorstore = Chroma(persist_directory=“持久”, embedding_function=OpenAIEmbeddings())
  索引 = VectorStoreIndexWrapper(向量存储=向量存储)
别的：
  #loader = TextLoader(&quot;data/data.txt&quot;) # 如果您只需要 data.txt，请使用此行
  loader = DirectoryLoader(“数据/”)
  如果坚持：
    index = VectorstoreIndexCreator(vectorstore_kwargs={“persist_directory”:“persist”}).from_loaders([loader])
  别的：
    索引 = VectorstoreIndexCreator().from_loaders([loader])

链 = ConversationalRetrievalChain.from_llm(
  llm=ChatOpenAI(模型=“gpt-3.5-turbo”),
  检索器=index.vectorstore.as_retriever(search_kwargs={“k”: 1}),
）

聊天记录 = []
而真实：
  如果没有查询：
    查询=输入（“提示：”）
  if 在 [&#39;quit&#39;, &#39;q&#39;, &#39;exit&#39;] 中查询：
    sys.exit()
  结果=链（{“问题”：查询，“聊天历史记录”：聊天历史记录}）
  打印（结果[&#39;答案&#39;]）

  chat_history.append((查询, 结果[&#39;答案&#39;]))
  查询=无

我不熟悉实现以及使用插件是否可行，因此欢迎任何建议。]]></description>
      <guid>https://stackoverflow.com/questions/78326755/is-there-any-way-to-automate-the-entire-ml-data-visualization-process-workflow-u</guid>
      <pubDate>Mon, 15 Apr 2024 07:33:28 GMT</pubDate>
    </item>
    <item>
      <title>在 VSCode 终端中卡住“git init”</title>
      <link>https://stackoverflow.com/questions/78325941/stuck-with-git-init-in-vscode-terminal</link>
      <description><![CDATA[我开始学习 ML，为此我使用 anaconda 提示符和 vscode 终端创建了一个 conda 环境。但是，当尝试将其链接到我的 github 存储库时，出现以下错误。这是错误运行“git init”时。
我尝试检查系统中的 PATH 变量，并且 git 已添加到路径中。想不出其他什么了。任何进一步的内容都会有很大帮助，谢谢！！]]></description>
      <guid>https://stackoverflow.com/questions/78325941/stuck-with-git-init-in-vscode-terminal</guid>
      <pubDate>Mon, 15 Apr 2024 03:21:57 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在随机森林的核心中使用 adaBoosting 而不是 bootstrap？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78319519/is-it-possible-to-use-adaboosting-in-the-core-of-random-forest-instead-of-bootst</link>
      <description><![CDATA[随机森林使用 Bagging（Bootstrapping）为其每棵树选择样本，对吗？是否可以使用 adaBoosting 代替？有什么优点和优点？缺点？为什么我没看到这个？
我看到 Sci-kit learn 中的 RandomForestClassifier 允许启用/禁用引导程序，否则没有选项可以用 boosting 替换它。
我这么问是因为我在大学里学过，在选择样本来创建树时，装袋和提升是两种可能的选择，但到目前为止我发现的似乎与此相反。]]></description>
      <guid>https://stackoverflow.com/questions/78319519/is-it-possible-to-use-adaboosting-in-the-core-of-random-forest-instead-of-bootst</guid>
      <pubDate>Sat, 13 Apr 2024 04:19:58 GMT</pubDate>
    </item>
    <item>
      <title>根据之前的元素，甚至不基于之前的元素，使用 LSTM 预测下一个元素</title>
      <link>https://stackoverflow.com/questions/78315830/predicting-next-elements-with-an-lstm-based-on-previous-ones-or-even-based-on</link>
      <description><![CDATA[我有一个时间序列问题，其中包括使用 LSTM 预测价格。该数据集是从 Python 库 yfinance 导入的。我在内置的极客教程中使用了示例代码Pytorch，并设法理解一切。我认为我不明白的唯一具体部分可以在这段代码片段中看到：
# 定义要预测的未来时间步数
预测步数 = 30

# 转换为 NumPy 并删除单一维度
sequence_to_plot = X_test.squeeze().cpu().numpy()

# 使用最后 30 个数据点作为起点
历史数据=序列到图[-1]
打印（历史数据.形状）

# 初始化一个列表来存储预测值
预测值 = []

# 使用训练好的模型来预测未来值
使用 torch.no_grad()：
    对于 _ 在范围内（num_forecast_steps*2）：
        # 准备历史数据张量
        历史数据张量 = torch.as_tensor(历史数据).view(1, -1, 1).float().to(设备)
        # 使用模型预测下一个值
        预测值 = 模型(历史数据张量).cpu().numpy()[0, 0]

        # 将预测值添加到forecasted_values列表中
        Forecasted_values.append(predicted_value[0])

        # 通过删除最旧的值并添加预测值来更新历史数据序列
        历史数据 = np.roll(历史数据, 移位=-1)
        历史数据[-1] = 预测值

        
# 生成未来日期
最后日期 = test_data.index[-1]

# 生成接下来的 30 个日期
future_dates = pd.date_range(start=last_date + pd.DateOffset(1), period=30)

# 将原始索引与未来日期连接起来
组合索引 = test_data.index.append(future_dates)

此外，模型的代码：
类 LSTMModel(nn.Module):
    # input_size ：每个时间步输入中的特征数量
    #hidden_​​size：LSTM 单元的数量
    # num_layers : LSTM 层数
    def __init__(自身、输入大小、隐藏大小、层数):
        super(LSTMModel, self).__init__() #初始化父类 nn.Module
        self.lstm = nn.LSTM(input_size,hidden_​​size,num_layers,batch_first=True)
        self.线性 = nn.Linear(hidden_​​size, 1)

    def front(self, x): # 定义神经网络的前向传播
        输出，_ = self.lstm(x)
        输出 = 自.线性(输出)
        返回

输入大小 = 1
层数 = 2
隐藏大小 = 64
输出大小 = 1

# 定义模型、损失函数和优化器
模型 = LSTMModel(input_size,hidden_​​size,num_layers).to(device)

loss_fn = torch.nn.MSELoss(reduction=&#39;mean&#39;)

优化器 = torch.optim.Adam(model.parameters(), lr=1e-3)
打印（模型）

根据教程内容，使用滚动预测。这意味着，如果我是对的，带有输入元素的数组将用作预测下一个元素的窗口，该窗口将附加到窗口，以预测下一个元素，依此类推。我的问题可能与概念或代码的其他部分有关，但我不太理解这行代码：
# 将预测值追加到 Forecasted_values 列表中
Forecasted_values.append(predicted_value[0])

如果我想根据之前的 30 个元素来预测下一个元素，为什么我应该采用预测数组的第一个预测元素？ （对我来说，这意味着前 30 个输入序列的第二个元素）。如果我需要基于动态生成的值构建一个序列窗口，并且之前没有先前的元素，我该如何修改此示例？
PS：下面有一个屏幕截图，显示了预测的图。]]></description>
      <guid>https://stackoverflow.com/questions/78315830/predicting-next-elements-with-an-lstm-based-on-previous-ones-or-even-based-on</guid>
      <pubDate>Fri, 12 Apr 2024 11:01:41 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法在同一个 Azure 机器学习管道中使用经典组件和自定义组件？</title>
      <link>https://stackoverflow.com/questions/78274833/is-there-any-way-to-use-classic-and-custom-components-in-the-same-azure-machine</link>
      <description><![CDATA[我正在尝试将管道从 Microsoft 机器学习工作室经典迁移到 Azure 机器学习。我的管道实际上在 Machine Learning Studio Classic 上运行，由以下经典和自定义组件组成。输入数据有 6 个，是使用导入数据组件从 Blob 存储帐户收集的。它们主要是 csv 文件，我使用一个自定义组件（R 脚本）来清理输入数据。这是因为“执行 R 脚本”只允许 2 个输入，但我需要 6 个输入，因为我有 6 个输入文件。之后，使用称为导出数据的组件将 csv 文件写入同一 Blob 存储帐户，但写入另一个文件夹中。我想在 Azure 机器学习中构建相同的管道，我可以在同一管道中使用自定义组件和默认组件吗？
如果我尝试使用经典组件创建管道，显然我无法使用我的自定义组件（又名 R 脚本），并且当我尝试创建自定义管道时，我无法使用经典组件，显然没有办法在我的自定义组件清理输入数据后创建自定义管道时导出我的数据。]]></description>
      <guid>https://stackoverflow.com/questions/78274833/is-there-any-way-to-use-classic-and-custom-components-in-the-same-azure-machine</guid>
      <pubDate>Thu, 04 Apr 2024 15:14:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 max_new_tokens 的 LLM 输出不完整</title>
      <link>https://stackoverflow.com/questions/77061898/incomplete-output-with-llm-with-max-new-tokens</link>
      <description><![CDATA[我正在尝试 Huggingface LLM 模型。
我注意到的一个问题是模型的输出突然结束，我理想地希望它完成其之间的段落/句子/代码。 （或者完全尝试在一些固定数量的标记内完成答案）
虽然我已经提供了 max_new_tokens = 300 并且在提示中我写道：
“输出最多应为 300 个字。”
响应总是不完整并且突然结束。我可以通过什么方式要求在所需数量的输出令牌内获得完整的输出？
代码：
检查点=“HuggingFaceH4/starchat-alpha”
设备=“cuda”； if torch.cuda.is_available() else “cpu”
StarCoderModel 类：
  def __init__(自身):
    self.tokenizer = AutoTokenizer.from_pretrained(检查点)
    # 如果需要 GPU，请确保 docker run 命令中提供了 `--gpus all`
    self.model = AutoModelForCausalLM.from_pretrained(检查点, device_map=&#39;auto&#39;)

  def infer(self, input_text, token_count):
    输入 = self.tokenizer.encode(input_text, return_tensors=“pt”).to(device)
    输出 = self.model.generate(输入, max_new_tokens=token_count, pad_token_id=self.tokenizer.eos_token_id)
    返回 self.tokenizer.decode(outputs[0])[len(input_text):]

样本输出：
私有数据类型FuntionName(String someId) {
    // TODO：替换为利用 someId 获取信息的实现
    返回数据类型.Value；
}


评论：

- 如果代码中存在 someId，则使用 Client 的 getAPI 以 someId 作为参数来获取一些信息。
- 如果

]]></description>
      <guid>https://stackoverflow.com/questions/77061898/incomplete-output-with-llm-with-max-new-tokens</guid>
      <pubDate>Thu, 07 Sep 2023 18:02:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 bootstrap 方法得出置信区间 AUC</title>
      <link>https://stackoverflow.com/questions/75309099/confidence-interval-auc-with-the-bootstrap-method</link>
      <description><![CDATA[今天我尝试制作一个 bootstrap 来获取各种不同 ML 算法 AUC 的区间置信度。
我使用了我的个人医疗数据集，其中包含 61 个特征，格式如下：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

年龄
女性


&lt;正文&gt;

65
1


45
0




例如，我使用了这种类型的算法：
X = data_sevrage.drop([&#39;Echec_sevrage&#39;], axis=1)

y = data_sevrage[&#39;Echec_sevrage&#39;]

X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.25, random_state=0)

lr = LogisticRegression(C=10 ,penalty=&#39;l1&#39;,solver=&#39;saga&#39;, max_iter=500).fit(X_train,y_train)
分数=roc_auc_score(y_test,lr.predict_proba(X_test)[:,1])
精度、召回率、阈值 = precision_recall_curve(y_test, lr.predict_proba(X_test)[:,1])
auc_ precision_recall = Metrics.auc(召回率, 精度)
y_pred = lr.predict(X_test)
print(&#39;ROC AUC 分数:&#39;,分数)
print(&#39;auc_ precision_recall :&#39;,auc_ precision_recall)

最后，当我使用 boostrap 方法获取置信区间时（我从其他主题获取代码：如何在Python中比较不同二元分类器的ROC AUC分数并评估统计显着性？）
def bootstrap_auc(clf, X_train, y_train, X_test, y_test, nsamples=1000):
    auc_值 = []
    对于范围内的 b (nsamples)：
        idx = np.random.randint(X_train.shape[0], size=X_train.shape[0])
        clf.fit(X_train[idx], y_train[idx])
        pred = clf.predict_proba(X_test)[:, 1]
        roc_auc = roc_auc_score(y_test.ravel(), pred.ravel())
        auc_values.append(roc_auc)
    返回 np.percentile(auc_values, (2.5, 97.5))

bootstrap_auc(lr, X_train, y_train, X_test, y_test, nsamples=1000)

我有这个错误：
&lt;块引用&gt;
“没有 [Int64Index([21, 22, 20, 31, 30, 13, 22, 1, 31, 3, 2, 9, 9, 18, 29, 30, 31,\n 31, 16 , 11, 23, 7, 19, 10, 14, 5, 10, 25, 30, 24, 8, 20],\n dtype=&#39;int64&#39;)] 在[列]中”

我使用了另一种方法，并且出现了几乎相同的错误：
&lt;前&gt;&lt;代码&gt; n_bootstraps = 1000
rng_seed = 42 # 控制再现性
bootstrapped_scores = []

rng = np.random.RandomState(rng_seed)
对于范围内的 i(n_bootstraps)：
    # 通过对预测索引进行替换采样来引导
    索引 = rng.randint(0, len(y_pred), len(y_pred))
    if len(np.unique(y_test[索引])) &lt; 2：
        # ROC AUC 至少需要 1 个正样本和 1 个负样本
        # 待定义：拒绝样本
        继续

    分数 = roc_auc_score(y_test[索引], y_pred[索引])
    bootstrapped_scores.append(分数)
    print(&quot;Bootstrap #{} ROC 区域: {:0.3f}&quot;.format(i + 1, Score))

&lt;块引用&gt;
“[6, 3, 12, 14, 10, 7, 9] 不在索引中”

你能帮我一下吗？我测试了很多解决方案，但每次都会出现此错误。
谢谢！
机器学习算法上 AUC 置信区间的 Bootstrap 方法。]]></description>
      <guid>https://stackoverflow.com/questions/75309099/confidence-interval-auc-with-the-bootstrap-method</guid>
      <pubDate>Wed, 01 Feb 2023 10:51:08 GMT</pubDate>
    </item>
    <item>
      <title>将 Scikit-Learn OneHotEncoder 与 Pandas DataFrame 结合使用</title>
      <link>https://stackoverflow.com/questions/58101126/using-scikit-learn-onehotencoder-with-a-pandas-dataframe</link>
      <description><![CDATA[我正在尝试使用 Scikit-Learn 的 OneHotEncoder 将 Pandas DataFrame 中包含字符串的列替换为 one-hot 编码的等效项。我的下面的代码不起作用：
从 sklearn.preprocessing 导入 OneHotEncoder
# 数据是 Pandas DataFrame

jobs_encoder = OneHotEncoder()
jobs_encoder.fit(data[&#39;职业&#39;].unique().reshape(1, -1))
data[&#39;职业&#39;] = jobs_encoder.transform(data[&#39;职业&#39;].to_numpy().reshape(-1, 1))

它会产生以下错误（列表中的字符串被省略）：
------------------------------------------------ ----------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-91-3a1f568322f5&gt;在&lt;模块&gt;()中
      3 jobs_encoder = OneHotEncoder()
      4 jobs_encoder.fit(data[&#39;职业&#39;].unique().reshape(1, -1))
----&gt; 5 数据[&#39;职业&#39;] = jobs_encoder.transform(数据[&#39;职业&#39;].to_numpy().reshape(-1, 1))

/usr/local/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py 变换（self，X）
    第730章 复制=真）
    第731章：
--&gt;第732章
    第733章
    第734章

/usr/local/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py 在 _transform_new(self, X)
    第678章
    第679章
--&gt;第680章
    第681章
    第682章

_transform 中的/usr/local/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py（self，X，handle_unknown）
    120 msg =（“在第{1}列中发现未知类别{0}”
    正文 正文_第 121 章
--&gt; 122 引发值错误（消息）
    123 其他：
    攀上漂亮女局长之后124

ValueError: 在转换过程中在第 0 列中发现未知类别 [&#39;...&#39;, ..., &#39;...&#39;]

以下是一些示例数据：
数据[&#39;职业&#39;] =

0 未知
1 个保险箱
2 接收
3 未知
4 导联
          ...
111988工业
111989 先生
111990 混乱
111991 先生
111992项目
名称：职业，长度：111993，dtype：对象

我究竟做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/58101126/using-scikit-learn-onehotencoder-with-a-pandas-dataframe</guid>
      <pubDate>Wed, 25 Sep 2019 14:47:16 GMT</pubDate>
    </item>
    </channel>
</rss>