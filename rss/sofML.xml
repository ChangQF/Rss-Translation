<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 14 Jul 2024 03:18:21 GMT</lastBuildDate>
    <item>
      <title>教导人工智能展现简单情绪以对抗孤独 [关闭]</title>
      <link>https://stackoverflow.com/questions/78745027/teaching-an-ai-to-show-simple-emotions-to-fight-loneliness</link>
      <description><![CDATA[我计划开发一种尽可能自然的计算机宠物。我认为神经网络可以很好地用于此，因为它们可以不断适应用户，并且不需要任何硬编码规则。
该程序应该记录用户的脸部，也许还会向 KI 传输一些额外的状态详细信息。
我在考虑一种 AI，它在最后选择某个状态，然后将其作为图像显示给用户。我的问题是我不知道从哪里获取
（对于监督学习）我应该从哪里获取训练数据或
（对于强化学习）我应该如何制作评估函数。
也许无监督学习也适用于这种情况。
最后，我想把整个东西喂给一个可爱的机器人，然后它只会让用户感觉到有人在那里感知你，所以除了一个有情绪反应的人工智能之外，我对其他任何东西都不感兴趣。
我在这个领域真的不太了解，所以另一个非常基本的问题：我也听说过一些关于“情绪-BICA”的事情，这可以用吗？或者首先应该如何设计一个神经、情绪网络？
在我的研究过程中，我遇到了莫夫林和其他一些人工宠物。但是，我看不出它们是如何工作的。
最后，我只想再说一遍，我只想学习一种构建对用户做出反应的人工智能的方法，而不是真正的宠物，我只是希望它比简单的算法更好。我还相信，当你让人工智能做它的事情时，它会特别强大，当你给它一些元规则时，它就会发展出复杂的行为。]]></description>
      <guid>https://stackoverflow.com/questions/78745027/teaching-an-ai-to-show-simple-emotions-to-fight-loneliness</guid>
      <pubDate>Sat, 13 Jul 2024 21:32:19 GMT</pubDate>
    </item>
    <item>
      <title>BERT 嵌入余弦相似度看起来非常随机且无用</title>
      <link>https://stackoverflow.com/questions/78744975/bert-embedding-cosine-similarities-look-very-random-and-useless</link>
      <description><![CDATA[我是这个领域的新手，所以也许我误解了一些东西。但是，我认为您可以使用 BERT 嵌入来确定语义相似性。我试图用这个将一些单词分组，但结果很糟糕。
例如，这是一个关于动物和水果的小例子。注意到相似度最高的是猫和香蕉吗？
import torch
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity

tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;, output_hidden_​​states=True).eval()

def gen_embedding(word):
encoding = tokenizer(word, return_tensors=&#39;pt&#39;)
with torch.no_grad():
output = model(**encoding)

token_embeddings = output.last_hidden_​​state.squeeze()
token_embeddings = token_embeddings[1 : -1]
word_embedding = token_embeddings.mean(dim=0)
return word_embedding

words = [
&#39;cat&#39;,
&#39;seagull&#39;,
&#39;mango&#39;,
&#39;banana&#39;
]

embs = [gen_embedding(word) for word in words]

print(cosine_similarity(embs))

# array([[1. , 0.33929926, 0.7086487 , 0.79372996],
# [0.33929926, 1.0000001 , 0.29915804, 0.4000572 ],
# [0.7086487 , 0.29915804, 1. , 0.7659105 ],
# [0.79372996, 0.4000572 , 0.7659105 , 0.99999976]], dtype=float32)

我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78744975/bert-embedding-cosine-similarities-look-very-random-and-useless</guid>
      <pubDate>Sat, 13 Jul 2024 20:58:49 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中对 NN 的输出求导</title>
      <link>https://stackoverflow.com/questions/78744355/taking-derivative-of-output-of-nn-wrt-to-inputs-in-pytorch</link>
      <description><![CDATA[我正在尝试使用 PyTorch 中的 NN 构建 ODE 求解器，模型的一部分涉及对模型输出相对于输入求导。
我研究过求解标量值函数的情况。在这种情况下，我的 NN 有 1 个输入节点和 1 个输出节点。根据一些论文，我使用
dy_dt = torch.autograd.grad(y,t, torch.ones_like(y), create_graph=True)[0]
来获得有效的梯度。因此，当我有一个形状为 [N,1] 的张量的训练集时，我得到的结果导数具有形状 [N,1]，这是有道理的。但是，当我尝试求解平面方程时遇到了问题。现在我的 NN 有 1 个输入节点和 2 个输出节点。当我输入一个形状为 [N,1] 的张量时，我从 NN 中得到了一个形状为 [N,2] 的张量，这是有道理的。然而，得到的 dy_dt 的形状为 [N,1]，而它应该是 [N,2]。我还尝试使用
dy_dt = torch.autograd. functional.jacobian(model, t)
它返回一个形状为 [N,2,N,1] 的张量，我认为可以从中访问正确的导数，尽管它不那么简单。我想知道是否有办法使用 autograd.grad 来获得正确的导数。也许它与 grad_output 参数有关？]]></description>
      <guid>https://stackoverflow.com/questions/78744355/taking-derivative-of-output-of-nn-wrt-to-inputs-in-pytorch</guid>
      <pubDate>Sat, 13 Jul 2024 16:01:15 GMT</pubDate>
    </item>
    <item>
      <title>无法在 databricks 中运行 Pysparkling</title>
      <link>https://stackoverflow.com/questions/78744050/unable-to-run-pysparkling-in-databricks</link>
      <description><![CDATA[!pip install h2o_pysparkling_3.5
from pysparkling import H2OConf,H2OContext
hc = H2OContext.getOrCreate()

我收到以下错误
IllegalArgumentException：不支持的参数：（spark.speculation，true）

我尝试了 spark.conf.set(&quot;spark.speculation&quot;, &quot;false&quot;)，但出现了以下错误
[CANNOT_MODIFY_CONFIG] 无法修改 Spark 配置的值：
&quot;spark.speculation&quot;
]]></description>
      <guid>https://stackoverflow.com/questions/78744050/unable-to-run-pysparkling-in-databricks</guid>
      <pubDate>Sat, 13 Jul 2024 14:02:12 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法在 winform 应用程序中使用 YoloV8 .Net Framework 4.7？</title>
      <link>https://stackoverflow.com/questions/78743181/is-there-anyway-to-use-yolov8-in-winform-application-net-framework-4-7</link>
      <description><![CDATA[我有一个基于 .net framework 4.7 的 c# winform 项目。
我想知道是否有办法在 winform 应用程序（.net framework 4.7）中使用 Yolov8。
每当我将引用添加到我的项目时，我都会收到以下错误：

YoloV8 使用 system.runtime 版本 6.0，高于 system.runtime 4.1
]]></description>
      <guid>https://stackoverflow.com/questions/78743181/is-there-anyway-to-use-yolov8-in-winform-application-net-framework-4-7</guid>
      <pubDate>Sat, 13 Jul 2024 07:33:51 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 指标显示“TypeError：‘property’对象不可迭代”</title>
      <link>https://stackoverflow.com/questions/78741833/tensorflow-metrics-is-showing-typeerror-property-object-is-not-iterable</link>
      <description><![CDATA[我正在建立一个 ANN 模型。当我运行以下代码时，它显示为
TypeError: &#39;property&#39; 对象不可迭代

如何修复此问题？
代码：
model=Sequential()
model.add(Dense(512,activation=tf.nn.relu))
model.add(Dense(256,activation=tf.nn.tanh))
model.add(Dense(128,activation=tf.nn.relu))
model.add(Dense(7))

# # 拟合模型

loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
accuracy=tf.keras.metrics.SparseCategoricalAccuracy
optimizer=tf.keras.optimizers.Adam()

model.compile(loss=loss,优化器=优化器，指标=[准确率])
history=model.fit(xtrain, ytrain, validation_data=(xval, yval), batch_size=64, epochs=100)
]]></description>
      <guid>https://stackoverflow.com/questions/78741833/tensorflow-metrics-is-showing-typeerror-property-object-is-not-iterable</guid>
      <pubDate>Fri, 12 Jul 2024 18:44:28 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 FAISS 减少大型人脸数据库的人脸识别中的误报？</title>
      <link>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</link>
      <description><![CDATA[我正在开发一个使用人脸识别的考勤跟踪系统。
该系统的工作原理如下：

1. 人脸检测：使用 Ultra Face 检测人脸。
2. 人脸编码：使用 FaceNet 对检测到的人脸进行编码。
3. 人脸比较：将编码的人脸与现有数据库进行比较以标记出勤率
4.使用的库：OpenCV 和 FAISS。
5.来源：CCTV摄像机镜头。

考勤系统说明：
当一个人走到摄像机前时，系统使用Ultra Face检测人脸，并使用FaceNet进行编码。然后将编码的人脸与现有数据库进行比较。如果相似度（余弦相似度）小于0.25，则标记出勤。
问题：
最初，数据库中的人数少于100人，比较时间是可以接受的。随着人数的增加，比较时间明显变长。每个人在数据库中都有5张图片。为了加快比较速度，我改用FAISS库。虽然FAISS显著缩短了比较时间，但也增加了误报（错误地标记出勤）。
人脸比较的旧方法：
for db_name, db_encode in encoding_dict.items():
尝试：
dist = cosine(db_encode, f_e[1])
除 ValueError 为 e 外：
print(&quot;&gt;&gt;&gt;&gt;&gt;&gt; : &quot;,f_e[1],&quot;\n&quot;,type(f_e[1]))
继续
if dist &lt;识别_t：
name = db_name
distance = dist

cv2.rectangle(img, (f_e[0][0], f_e[0][1]), (f_e[0][2], f_e[0][3]), (0, 255, 0), 1)
cv2.putText(img, f&#39;{name}:{distance - 1:.2f}&#39;, (f_e[0][0], f_e[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

使用 FAISS 的新方法：
class StaffCustManagement：
def __init__(self, staff_n_neighbours=4, identification_t=0.80):
self.staff_db：Custom_DB = Custom_DB（db_name =“mydatabase”，col_name =“staff”）
self.staff_names，self.staff_encodings = self.staff_load_encodings（）
self.staff_n_neighbours：int = staff_n_neighbours
self.staff_ini_faiss（）
self.recognition_t：float = identification_t

def staff_load_encodings（self） -&gt; Tuple[List[str], List[np.ndarray]]:
staff_names, staff_encodings = [], []
for document in self.staff_db.find_all_data():
staff_names.append(document[&#39;_id&#39;])
staff_encodings.append(ArrayEncDec.decode_from_base64(b64_str=document[&#39;encoding&#39;]))
return staff_names, staff_encodings

def staff_ini_faiss(self):
if self.staff_names and self.staff_encodings:
Dimensions = 128
self.staff_index_faiss = faiss.IndexFlatL2(dimensions)
faiss_embeddings = np.array(self.staff_encodings, dtype=&#39;float32&#39;)
faiss.normalize_L2(faiss_embeddings)
self.staff_index_faiss.add(faiss_embeddings)

def find_staff_cust(self, current_encode: np.ndarray) -&gt; Tuple[str, float]:
name = &quot;Unknown&quot;
distance = float(&quot;inf&quot;)
if len(self.staff_names) == 0:
return name, distance
target_rep = np.expand_dims(current_encode, axis=0)
# faiss.normalize_L2(target_rep)
distances, neighbours = self.staff_index_faiss.search(target_rep, self.staff_n_neighbours)
print(&quot;Distances&quot;, distances)
print(&quot;neighbors&quot;, neighbours)
if distances[0][0] &gt;= self.recognition_t:
return self.staff_names[neighbors[0][0]].split(&#39;-&#39;)[0], distances[0][0]
return name, distance

问题：
如何在使用 FAISS 进行人脸比较时减少误报我的出勤跟踪系统如何做到这一点？虽然 FAISS 大大缩短了比较时间，但准确性却受到影响，导致出勤标记不正确。是否有任何最佳实践或替代方法可以在大型数据库中保持高精度？]]></description>
      <guid>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</guid>
      <pubDate>Fri, 12 Jul 2024 10:33:51 GMT</pubDate>
    </item>
    <item>
      <title>通过向 CNN 输入添加位置和字符信息来增强文档布局分析</title>
      <link>https://stackoverflow.com/questions/78739816/enhancing-document-layout-analysis-by-adding-positional-and-character-informatio</link>
      <description><![CDATA[我正在研究文档布局分析，并一直在探索 CNN 和基于 Transformer 的网络来完成这项任务。通常，图像作为 3 通道 RGB 输入传递给这些网络。但是，我的数据源是 PDF 格式，我可以直接从中提取准确的位置和字符信息。
我担心将这些 PDF 数据转换为图像进行分析会导致宝贵的位置和字符信息丢失。我的想法是将 CNN 的输入维度从标准的 3 RGB 通道修改为包含这些额外位置和字符信息的更高维度输入。
我了解 CNN 的工作原理，并高度怀疑这种方法可能行不通，但我很感谢社区的任何反馈或建议。有没有人尝试过以这种方式增强输入通道，或者有没有人对将位置和字符数据直接集成到 CNN 中有什么见解？]]></description>
      <guid>https://stackoverflow.com/questions/78739816/enhancing-document-layout-analysis-by-adding-positional-and-character-informatio</guid>
      <pubDate>Fri, 12 Jul 2024 10:17:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 RAG 识别代码文件中的错误来源[关闭]</title>
      <link>https://stackoverflow.com/questions/78738937/using-rag-to-identify-the-source-of-error-in-a-code-file</link>
      <description><![CDATA[我正在尝试实现一个小工具，它可以自动识别一组代码文件（作为输入）中的哪一部分代码导致了执行期间显示的错误文本。
错误可能是语法错误，也可能是逻辑错误。我还在考虑利用 llms 的 api 调用来更正代码。
据我所知，RAG 是必要的，因为我不可能将所有代码文件的数据都放入提示中，因为它肯定会超出上下文窗口的大小。
哪种类型的 RAG 实现最有用？我想尽可能减少响应延迟？]]></description>
      <guid>https://stackoverflow.com/questions/78738937/using-rag-to-identify-the-source-of-error-in-a-code-file</guid>
      <pubDate>Fri, 12 Jul 2024 06:41:48 GMT</pubDate>
    </item>
    <item>
      <title>训练 PINN 来反演未知参数</title>
      <link>https://stackoverflow.com/questions/78730829/train-a-pinn-to-invert-for-unknown-parameters</link>
      <description><![CDATA[我使用 PINN 求解阻尼振荡器微分方程，同时以阻尼振荡器的噪声观测作为输入，找到后者的摩擦参数。我使用自定义训练程序在 Tensorflow 中编写了代码。问题是我定义的可训练参数没有接近我从噪声观测中知道的正确值。最终，PINN 的解决方案完全不正确。但是，我的代码运行得很好，不需要寻找可训练参数，也就是这里的摩擦参数。
以下函数的解释：

oscillator_system_data_loss：振荡器系统作为神经网络的实现，其中可学习参数 mu 传递给在 NN_osc_func 中实现的 ODE
train_NN_data_loss：自定义训练程序
plot_epochs_with_noise：与问题无关，但用于训练时监控

def rocksock_system_data_loss(t, net, func, params, mu, bc, t_data, u_data, lambda1):
t = t.reshape(-1,1)
t = tf.constant(t, dtype = tf.float32)
t_0 = tf.zeros((1,1))

使用 tf.GradientTape() 作为 outer_tape:
outer_tape.watch(t)

使用 tf.GradientTape() 作为 inner_tape:
inner_tape.watch(t)
x = net(t)

dx_dt = inner_tape.gradient(x, t) # 一阶导数

d2x_dt2 = outer_tape.gradient(dx_dt, t) # 二阶导数

bc_loss_1 = tf.square(net(t_0) - bc[0])
bc_loss_2 = tf.square(dx_dt[0] - bc[1])

ode_loss = d2x_dt2 - func(x, dx_dt, params[0], mu, params[2])

data_loss = u_data - net(t_data)

square_loss = tf.square(ode_loss) + lambda1*tf.square(data_loss) + bc_loss_1 + bc_loss_2
total_loss = tf.reduce_mean(square_loss)

return total_loss, mu

def train_NN_data_loss(epochs, optm, NN, func, bc, lambda1, train_t, train_u, data_t, data_u,
data_u_noised, test_t_plot, true_u_plot, testing_t):
train_loss_record = []
loss_tracker = plotting_points(epochs)

mu = tf.Variable(initial_value=tf.ones((1,1)), trainable=True, dtype=tf.float32)
mu_list = []

early_stop = 0

for itr in范围（epochs）：
使用 tf.GradientTape() 作为磁带：
train_loss，mu = 振荡器系统数据损失（train_t，NN，func，params，mu，bc，data_t，data_u_noised，lambda1）
train_loss_record.append（train_loss）

grad_w = 磁带。gradient（train_loss，NN.trainable_variables + [mu]）
optm.apply_gradients（zip（grad_w，NN.trainable_variables + [mu]））

如果 itr 在 loss_tracker 中：
print（train_loss.numpy()）
print（mu.numpy()）
plot_epochs_with_noise（train_t，train_u，data_t，data_u_noised，test_t_plot，true_u_plot，testing_t，itr，NN）

mu_list.append（mu.numpy()）

return train_loss_record, mu_list, early_stop

NN_osc_func = lambda x, dx_dt, k, d, m: -k/m*x - d/m*dx_dt

您可以在此处看到 6000 个 epoch 后的结果。神经网络正在收敛到一条水平线，误差为 5.76，参数估计为 0.84，尽管正确值为 4。这是我的阻尼振荡器设置：
k = 400
d = 4
m = 1
y0 = np.array([1.0, 0.0])

错误结果。
相应损失。
不幸的是，此时我不知道问题可能是什么。我尝试更改 NN_osc_func，并在两个函数中使用了 tape.gradient()。有什么帮助吗？
我的想法是，要么训练更长时间，要么我可能会遇到 PINN 容易出现的一些高频问题。]]></description>
      <guid>https://stackoverflow.com/questions/78730829/train-a-pinn-to-invert-for-unknown-parameters</guid>
      <pubDate>Wed, 10 Jul 2024 13:22:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在 TensorFlow 中使用 model.fit() 时会得到 ValueError：无法识别的数据类型：x=[...] (类型 <class 'list'>)？</title>
      <link>https://stackoverflow.com/questions/78271090/why-do-i-get-valueerror-unrecognized-data-type-x-of-type-class-list</link>
      <description><![CDATA[我尝试运行以下代码，该代码取自 CS50 的 AI 课程：
import csv
import tensorflow as tf
from sklearn.model_selection import train_test_split

# 从文件读取数据
with open(&quot;banknotes.csv&quot;) as f:
reader = csv.reader(f)
next(reader)

data = []
for row in reader:
data.append(
{
&quot;evidence&quot;: [float(cell) for cell in row[:4]],
&quot;label&quot;: 1 if row[4] == &quot;0&quot; else 0,
}
)

#将数据分为训练组和测试组
evidence = [row[&quot;evidence&quot;] for row in data]
labels = [row[&quot;label&quot;] for row in data]
X_training, X_testing, y_training, y_testing = train_test_split(
evidence, labels, test_size=0.4
)

# 创建神经网络
model = tf.keras.models.Sequential()

# 添加一个有 8 个单元的隐藏层，使用 ReLU 激活函数
model.add(tf.keras.layers.Dense(8, input_shape=(4,),activation=&quot;relu&quot;))

# 添加一个有 1 个单元的输出层，使用 sigmoid 激活函数
model.add(tf.keras.layers.Dense(1,activation=&quot;sigmoid&quot;))

# 训练神经网络
model.compile(
optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]
)
model.fit(X_training, y_training, epochs=20)

# 评估模型的表现
model.evaluate(X_testing, y_testing, verbose=2)

但是，我收到以下错误：
Traceback（最近一次调用最后一次）：
文件“C:\Users\Eric\Desktop\coding\cs50\ai\lectures\lecture5\banknotes\banknotes.py”，第 41 行，位于&lt;module&gt;
model.fit(X_training, y_training, epochs=20)
文件 &quot;C:\Users\Eric\Desktop\coding\cs50\ai\.venv\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;，第 122 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
文件 &quot;C:\Users\Eric\Desktop\coding\cs50\ai\.venv\Lib\site-packages\keras\src\trainers\data_adapters\__init__.py&quot;，第 113 行，位于 get_data_adapter 中
引发 ValueError(f&quot;无法识别的数据类型：x={x}（类型为 {type(x)}）&quot;)
ValueError：无法识别的数据类型：x=[...]（类型为 &lt;class &#39;list&#39;&gt;)

其中“...”是训练数据。
知道哪里出错了吗？我在 Windows 计算机上使用 Python 版本 3.11.8 和 TensorFlow 版本 2.16.1。
我尝试在 Google Colab 笔记本中运行相同的代码，并且成功了：问题仅发生在我的本地机器上。这是我期望的输出：
Epoch 1/20
26/26 [==============================] - 1s 2ms/step - 损失：1.1008 - 准确度：0.5055
Epoch 2/20
26/26 [===============================] - 0s 2ms/step - 损失：0.8588 - 准确度：0.5334
Epoch 3/20
26/26 [================================] - 0s 2ms/step - 损失：0.6946 - 准确度：0.5917
Epoch 4/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.5970 - 准确度：0.6683
纪元 5/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.5265 - 准确度：0.7120
纪元 6/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.4717 - 准确度：0.7655
纪元 7/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.4258 - 准确度：0.8177
纪元 8/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.3861 - 准确度：0.8433
纪元 9/20
26/26 [================================] - 0s 2ms/步 - 损失：0.3521 - 准确度：0.8615
纪元 10/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.3226 - 准确度：0.8870
纪元 11/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.2960 - 准确度：0.9028
纪元 12/20
26/26 [================================] - 0s 2ms/步 - 损失：0.2722 - 准确度：0.9125
纪元 13/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.2506 - 准确度：0.9283
纪元 14/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.2306 - 准确度：0.9514
纪元 15/20
26/26 [================================] - 0s 3ms/步 - 损失：0.2124 - 准确度：0.9660
纪元 16/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.1961 - 准确度：0.9769
纪元 17/20
26/26 [==============================] - 0s 2ms/步 - 损失：0.1813 - 准确度：0.9781
纪元 18/20
26/26 [================================] - 0s 2ms/步 - 损失：0.1681 - 准确度：0.9793
纪元 19/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.1562 - 准确度：0.9793
Epoch 20/20
26/26 [===============================] - 0s 2ms/步 - 损失：0.1452 - 准确度：0.9830
18/18 - 0s - 损失：0.1407 - 准确度：0.9891 - 187ms/epoch - 10ms/步
[0.14066053926944733, 0.9890710115432739]
]]></description>
      <guid>https://stackoverflow.com/questions/78271090/why-do-i-get-valueerror-unrecognized-data-type-x-of-type-class-list</guid>
      <pubDate>Thu, 04 Apr 2024 00:28:01 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Textract 中获取 BLOCK 类型 LAYOUT_TITLE、LAYOUT_SECTION_HEADER 和 LAYOUT_xx 的内容</title>
      <link>https://stackoverflow.com/questions/78252584/how-to-get-content-of-block-types-layout-title-layout-section-header-and-layout</link>
      <description><![CDATA[我正在尝试使用 textract 抓取多页 pdf。
需要抓取 pdf 并根据其部分、子部分、表格格式化为 json。
在尝试使用 LAYOUT 和 Table 进行 UI 演示时，它能够准确显示布局标题、布局部分、布局文本、布局页脚、页码
在从 UI 演示下载的 csv 文件中可以观察到相同的信息：layout.csv 文件。
在 json 文件中也是如此：analyzeDocResponse.json 也一样，但它包含所有内容（LINES、WORDS、LAYOUT_TITLE 和所有与布局相关的数据），我认为 textract 按顺序执行所有类型的块类型。
出于调试目的，我使用以下代码打印整个块字典。
以及块类型，后面跟着相应的文本。
如果对 pdf 文件感兴趣：其药物的 SmPC：SmPC 文件
代码 1：以 json 格式打印每个块。

def start_textract_job(bucket, document):
response = textract.start_document_analysis(
DocumentLocation={
&#39;S3Object&#39;: {
&#39;Bucket&#39;: bucket,
&#39;Name&#39;: document
}
},
FeatureTypes=[&quot;LAYOUT&quot;] # 您可以根据需要调整 FeatureTypes
)
return response[&#39;JobId&#39;]

def print_blocks(job_id):
next_token = None
while True:
if next_token:
response = textract.get_document_analysis(JobId=job_id, NextToken=next_token)
else:
response = textract.get_document_analysis(JobId=job_id)

for block in response.get(&#39;Blocks&#39;, []):
print(json.dumps(block, indent=4))

next_token = response.get(&#39;NextToken&#39;, None)
if not next_token:
break

它根据 UI Demo 打印类似信息，块类型 LINES、WORDS、LAYOUT_
但如果我尝试使用以下代码打印每种块类型的文本，它无法打印与 LAYOUT_ 相关的文本，不知道为什么，我是否遗漏了什么？
代码 2：打印块类型，然后打印其内容。

def start_textract_job 与上面的 LAYOUT 相同。

def print_blocks(job_id):
next_token = None
while True:
if next_token:
response = textract.get_document_analysis(JobId=job_id, NextToken=next_token)
else:
response = textract.get_document_analysis(JobId=job_id)

for block in response.get(&#39;Blocks&#39;, []):
print(f&quot;{block[&#39;BlockType&#39;]}: {block.get(&#39;Text&#39;, &#39;&#39;)}&quot;)

next_token = response.get(&#39;NextToken&#39;, None)
if not next_token:
break

我可以看到块类型 LINES、WORDS 的值
但 LAYOUT 为空，如下所示，我认为，它在块类型中识别，但不是其值。
LAYOUT_TITLE:
LAYOUT_FIGURE:
LAYOUT_TEXT:
LAYOUT_SECTION_HEADER:
LAYOUT_TEXT:
LAYOUT_SECTION_HEADER:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_TEXT:
LAYOUT_PAGE_NUMBER:
LAYOUT_FOOTER:
任何帮助都非常感谢，我查阅了文档和其他一些 StackOverflow 问题，但找不到任何帮助。
Tetract 新手，抱歉，如果是新手，请提问：)]]></description>
      <guid>https://stackoverflow.com/questions/78252584/how-to-get-content-of-block-types-layout-title-layout-section-header-and-layout</guid>
      <pubDate>Sun, 31 Mar 2024 19:28:51 GMT</pubDate>
    </item>
    <item>
      <title>NameError：名称“plot_confusion_matrix”未定义</title>
      <link>https://stackoverflow.com/questions/65651544/nameerror-name-plot-confusion-matrix-is-not-defined</link>
      <description><![CDATA[我正在尝试使用 VGG16 创建一个分类模型，但是在项目结束时，我在获取混淆矩阵时遇到了错误。下面给出了代码，
导入的包和模块是：
import os
import keras
import numpy as np
import tensorflow as tf
from keras.models import Model
import matplotlib.pyplot as plt
from keras.optimizers import Adam
from keras.applications import MobileNet
from sklearn.metrics import chaos_matrix
from keras.layers.core import Dense, Activation
from keras.metrics import categorical_crossentropy
from sklearn.model_selection import train_test_split
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.mobilenet import preprocess_input
from tensorflow.keras.preprocessing import image_dataset_from_directory

注意：对于简短地讲我只是跳过了链接的数据集
下面定义 VGG16：
vgg16_model = keras.applications.vgg16.VGG16()
vgg16_model.summary()

现在，定义模型：
model = Sequential()
for layer in vgg16_model.layers:
model.add(layer)

for layer in model.layers:
layer.trainable = False

model.add(Dense(2,activation=&#39;softmax&#39;))

编译模型：
model.compile(Adam(lr=.0001),loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

拟合模型：
model.fit_generator(train_batches, steps_per_epoch=4, validation_data=valid_batches, validation_steps=4, epochs=10, verbose=2)

现在是混淆矩阵：
test_imgs, test_labels = next(test_batches)
plots(test_imgs, titles=test_labels)
test_labels = test_labels[:,0] 
predictions = model.predict_generator(test_batches, steps=1, verbose=0)
cm = chaos_matrix(test_labels, np.round(predictions[:,0]))

下面我遇到了一个错误，请关注下面代码，
cm_plot_labels = [&#39;diseaseAffectedEggplant&#39;,&#39;freshEggplant&#39;]
plot_confusion_matrix(cm, cm_plot_labels, title=&quot;Confusion Matrix&quot;) // 这行，我遇到了一个错误

错误如下，
-------------------------------------------------------------------------------
NameError Traceback (most recent call last)
&lt;ipython-input-28-43b96d543746&gt; in &lt;module&gt;()
1 cm_plot_labels = [&#39;diseaseAffectedEggplant&#39;,&#39;freshEggplant&#39;]
----&gt; 2 plot_confusion_matrix(cm, cm_plot_labels, title=&quot;Confusion Matrix&quot;)

NameError: 名称 &#39;plot_confusion_matrix&#39; 未定义
]]></description>
      <guid>https://stackoverflow.com/questions/65651544/nameerror-name-plot-confusion-matrix-is-not-defined</guid>
      <pubDate>Sun, 10 Jan 2021 08:53:18 GMT</pubDate>
    </item>
    <item>
      <title>使用 NLP Python 对文本进行多分类 - 总类别中 2 个类别的召回率相对较低</title>
      <link>https://stackoverflow.com/questions/61279917/multi-classification-of-text-using-nlp-python-recall-is-relatively-very-less-f</link>
      <description><![CDATA[我拥有几乎平衡的数据集，包含 9 个独特类别，每个类别有近 2200 行，差异为 +/-100 行。为了创建模型，我使用了下面提到的 URL 方法，但在每种情况下，我的模型准确率都在 58% 左右，精确率/召回率也在 54% 左右。你能告诉我我做错了什么吗？
https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f
https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a
https://medium.com/@robert.salgado/multiclass-text-classification-from-start-to-finish-f616a8642538
我的数据集只有 2 列，1 列为特征，1 列为标签。
from pandas import ExcelFile

df = pd.read_excel(&#39;Prediction.xlsx&#39;, 
sheet_name=&#39;Sheet1&#39;)
df.head()
BAD_SYMBOLS_RE = re.compile(&#39;[^0-9a-z #+_]&#39;)
STOPWORDS = set(stopwords.words(&#39;english&#39;))
import sys
!{sys.executable} -m pip install lxml

def clean_text(text):
&quot;&quot;&quot;
text: 字符串

return: 修改后的初始字符串
&quot;&quot;&quot;
text = BeautifulSoup(text, &quot;html.parser&quot;).text # HTML 解码
text = text.lower() # 小写文本
text = REPLACE_BY_SPACE_RE.sub(&#39; &#39;, text) # 将文本中的 REPLACE_BY_SPACE_RE 符号替换为空格
text = BAD_SYMBOLS_RE.sub(&#39;&#39;, text) # 从文本中删除 BAD_SYMBOLS_RE 中的符号
text = &#39; &#39;.join(word for word in text.split() if word not in STOPWORDS) # 从文本中删除停用词
return text

df[&#39;notes_issuedesc&#39;] = df[&#39;notes_issuedesc&#39;].apply(clean_text)
print_plot(10)
df[&#39;notes_issuedesc&#39;].apply(lambda x: len(x.split(&#39; &#39;))).sum()
X = df.notes_issuedesc
y = df.final
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 42)
%%time
从 sklearn.naive_bayes 导入 MultinomialNB
从 sklearn.pipeline 导入 Pipeline
从 sklearn.feature_extraction.text 导入 TfidfTransformer

nb = Pipeline([(&#39;vect&#39;, CountVectorizer()),
(&#39;tfidf&#39;, TfidfTransformer()),
(&#39;clf&#39;, MultinomialNB()),
])
nb.fit(X_train, y_train)

来自 sklearn.metrics 导入分类报告
y_pred = nb.predict(X_test)

print(&#39;准确率 %s&#39; % 准确率得分(y_pred, y_test))
print(分类报告(y_test, y_pred,target_names=my_tags))
]]></description>
      <guid>https://stackoverflow.com/questions/61279917/multi-classification-of-text-using-nlp-python-recall-is-relatively-very-less-f</guid>
      <pubDate>Fri, 17 Apr 2020 20:12:37 GMT</pubDate>
    </item>
    <item>
      <title>打印张量的所有内容</title>
      <link>https://stackoverflow.com/questions/52673610/printing-all-the-contents-of-a-tensor</link>
      <description><![CDATA[我偶然发现了这个 PyTorch 教程（在 neuron_networks_tutorial.py 中），其中他们构建了一个简单的神经网络并运行推理。我想打印整个输入张量的内容以进行调试。当我尝试打印张量时，我得到的是类似这样的结果，而不是整个张量：

我看到了类似的 numpy 链接，但不确定哪个适用于 PyTorch。我可以将其转换为 numpy 并可能查看它，但我想避免额外的开销。有没有办法打印整个张量？]]></description>
      <guid>https://stackoverflow.com/questions/52673610/printing-all-the-contents-of-a-tensor</guid>
      <pubDate>Fri, 05 Oct 2018 21:41:40 GMT</pubDate>
    </item>
    </channel>
</rss>