<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 08 Jul 2024 03:18:15 GMT</lastBuildDate>
    <item>
      <title>如何将模板草图与制造零件的图像进行比较？</title>
      <link>https://stackoverflow.com/questions/78718809/how-do-i-compare-a-template-sketch-to-an-image-of-a-manufactured-part</link>
      <description><![CDATA[我有一个模板图像，用作基准来确定零件在电路板上的焊接位置。我无法保证图像具有相同的大小或比例。基准图像由外部来源提供给我，我自己焊接零件。焊接板的图像是我拍摄的。我想快速检查错误 - 应该放置但实际上没有放置的零件、应该跳过但实际上没有放置的零件等。
我希望在错误的零件上有一个边界框。我已附上两张描绘有问题的图像的图像（根据我的工作政策，我无法提供真实图像）。PCB 和 模板。
红色方框表示我不应该放置的部件，绿色方框表示我应该放置的部件。
我遵循了此处的信息。我尝试使用结构相似性指数。由于图像彼此之间差异很大，因此效果不佳。
我认为密集向量表示对我没用，因为这会得出相似性分数，而不是暴露所犯的任何错误。
理想情况下，我可以从 SSI 方法生成输出以显示遗漏了哪些问题。]]></description>
      <guid>https://stackoverflow.com/questions/78718809/how-do-i-compare-a-template-sketch-to-an-image-of-a-manufactured-part</guid>
      <pubDate>Mon, 08 Jul 2024 01:28:36 GMT</pubDate>
    </item>
    <item>
      <title>用于网络异常检测的开源 SOAR 平台</title>
      <link>https://stackoverflow.com/questions/78718800/open-source-soar-platforms-for-network-anomaly-detection</link>
      <description><![CDATA[我和我的小组成员计划在我们的 CAPSTONE 项目中使用机器学习 KNN 算法进行网络异常检测，但我们想找到一种方法来集成“响应”功能。经过进一步研究，我发现可以通过将其集成到 SOAR 平台来为其添加响应功能。
现在到了棘手的部分，我们很难找到可以使用的开源且免费的 SOAR 平台。不仅如此，我们还很难找到有关如何执行此操作的一些文档。所以我问你们，你们对我们可以使用什么有什么建议吗？如果可能的话，你们可以提供文档吗？
注意：我们将使用 Python 进行 KNN 算法，希望这些信息有所帮助]]></description>
      <guid>https://stackoverflow.com/questions/78718800/open-source-soar-platforms-for-network-anomaly-detection</guid>
      <pubDate>Mon, 08 Jul 2024 01:22:33 GMT</pubDate>
    </item>
    <item>
      <title>对键盘上的按键数据历史进行分类，以识别不适当的内容（对计算机有危险）</title>
      <link>https://stackoverflow.com/questions/78718328/classification-of-key-press-data-history-on-the-keyboard-to-identify-inappropria</link>
      <description><![CDATA[最近，我从计算机视觉转向了 NLP 任务。
我收集了一组以 ASCII 文本文件形式记录的按键数据历史记录，记录了用户在会话期间的每次击键。目标是对这些数据进行分类，以检测不适当的内容，例如尝试下载病毒、访问非法网站等。
注意：

高噪声水平：按键数据历史文件包含大量噪声，这是由于频繁的嘈杂按键（例如，用户在玩游戏时按“wasd”）造成的，因此很难提取有意义的文本。

多语言输入：我想为多种语言进行输入。
例如，用户可能会输入“crfxfnm dbhec” （另一种语言的短语，含义为“下载病毒”，使用经典英语键盘布局）


我尝试使用预训练的 BERT 模型对原始数据进行文本分类，预测两个类别：正上下文和负上下文。但是，这种方法不起作用。
此外，我还探索了原始数据（相同的预训练 BERT 模型）上的 token 分类结果，噪声数据中的真实单词被突出显示。仍然希望能够准确识别 token。
基于 Transformer 的分类器（例如 Hugging Face 的 BERT）能否有效处理和标记这种类型的噪声和多语言输入？token 分类任务是否可行？因此，我们有原始数据的 token 分类任务
是否应该清理数据集以删除这些噪声字符，如何清理？训练模型进行文本分类之后如何清理？因此，我们有两个主要步骤：过滤 + 文本分类任务。使用经典 ML + NN 进行过滤以进行文本分类？
是否有必要为每种语言训练单独的模型，在训练之前可能将字符转换为其真实语言？
如何为训练模型形成合成数据集？手动标记听起来很疯狂]]></description>
      <guid>https://stackoverflow.com/questions/78718328/classification-of-key-press-data-history-on-the-keyboard-to-identify-inappropria</guid>
      <pubDate>Sun, 07 Jul 2024 20:15:34 GMT</pubDate>
    </item>
    <item>
      <title>评估无监督学习 - 播放列表生成器应用程序[关闭]</title>
      <link>https://stackoverflow.com/questions/78718024/evaluate-unsupervised-learning-playlist-generator-app</link>
      <description><![CDATA[我正在为我的简历做一个项目，我想创建一个播放列表生成器应用程序。该应用程序将接收一个播放列表作为输入，并生成一个适合作为输出的播放列表。我的数据将包括从 Spotify for Developers API 中提取的歌曲名称、歌词和歌曲属性，数据集大小约为 150K 个条目。
我正在考虑使用 K-means 来完成这个无监督任务（或另一种无监督算法），以便对于给定的播放列表输入，我可以将歌曲分类为标签，然后从同一标签中选择相似的歌曲。然而，我开始认为这可能会有问题，因为我不知道事后如何评估我的模型。我无法手动创建带标签的测试数据，因为这对我来说不可行，即使可以，给定的播放列表也没有基本事实。
那么当我完成后，我怎么知道我的模型是否在发挥作用？]]></description>
      <guid>https://stackoverflow.com/questions/78718024/evaluate-unsupervised-learning-playlist-generator-app</guid>
      <pubDate>Sun, 07 Jul 2024 17:53:45 GMT</pubDate>
    </item>
    <item>
      <title>从 UNETR 获取预测后，在 matplotlib 中显示多类标签分割</title>
      <link>https://stackoverflow.com/questions/78717927/multi-class-label-segmentation-display-in-matplotlib-after-getting-preds-from-un</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78717927/multi-class-label-segmentation-display-in-matplotlib-after-getting-preds-from-un</guid>
      <pubDate>Sun, 07 Jul 2024 17:10:00 GMT</pubDate>
    </item>
    <item>
      <title>ValuerError：发现输入变量的样本数量不一致</title>
      <link>https://stackoverflow.com/questions/78717924/valuererror-found-input-variables-with-inconsistent-numbers-of-samples</link>
      <description><![CDATA[我编写了以下代码来学习机器学习方法中的分数。但是我收到了以下错误。原因是什么？？
ValueError: 发现输入变量的样本数量不一致：[6396, 1599]

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv(&#39;亚美尼亚市场汽车价格.csv&#39;)

df[&#39;汽车名称&#39;] = df[&#39;汽车名称&#39;].astype(&#39;category&#39;).cat.codes

df = df.join(pd.get_dummies(df.FuelType, dtype=int))
df = df.drop(&#39;FuelType&#39;, axis=1)

df[&#39;Region&#39;] = df[&#39;Region&#39;].astype(&#39;category&#39;).cat.codes

df[&#39;价格&#39;] = df.pop(&#39;Price&#39;)

X = df.drop(&#39;Price&#39;, axis=1)
y = df[&#39;Price&#39;]

来自 sklearn.model_selection 导入 train_test_split
来自 sklearn.linear_model 导入 LinearRegression

X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.2)
model = LinearRegression()

model.fit(X_train, y_train)

-------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[358]，第 1 行
----&gt; 1 model.fit(X_train, y_train)

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py:1473，在 _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)
1466 estimator._validate_params()
1468 使用 config_context(
1469 skip_parameter_validation=(
1470 prefer_skip_nested_validation 或 global_skip_validation
1471 )
1472 ):
-&gt; 1473 返回 fit_method(estimator, *args, **kwargs)

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_base.py:609，位于 LinearRegression.fit(self, X, y, sample_weight)
605 n_jobs_ = self.n_jobs
607 accept_sparse = False if self.positive else [&quot;csr&quot;, &quot;csc&quot;, &quot;coo&quot;]
--&gt; 609 X, y = self._validate_data(
610 X, y, accept_sparse=accept_sparse, y_numeric=True, multi_output=True
611 )
613 has_sw = sample_weight 不为 None
614 if has_sw:

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py:650，位于 BaseEstimator._validate_data(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)
648 y = check_array(y, input_name=&quot;y&quot;, **check_y_params)
649 else:
--&gt; 650 X, y = check_X_y(X, y, **check_params)
651 out = X, y
653 如果不是 no_val_X 且 check_params.get(&quot;ensure_2d&quot;, True):

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\validation.py:1291，在 check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, Ensure_2d, allow_nd, multi_output, Ensure_min_samples, Ensure_min_features, y_numeric, estimator)
1273 X = check_array(
1274 X,
1275 accept_sparse=accept_sparse,
(...)
1286 input_name=&quot;X&quot;,
1287 )
1289 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
-&gt; 1291 check_consistent_length(X, y)
1293 return X, y

文件 ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\validation.py:460，位于 check_consistent_length(*arrays)
458 uniques = np.unique(lengths)
459 if len(uniques) &gt; 1:
-&gt; 460 raise ValueError(
461 &quot;找到样本数量不一致的输入变量：%r&quot;
462 % [int(l) for l in lengths]
463 )

ValueError：找到样本数量不一致的输入变量：[6396, 1599]

我尝试了所有方法，但都没有用，或者我不知道如何解决问题。
Jupyternaut：

您提供的错误消息表明输入数据存在问题。具体来说，似乎有两个不同版本的输入数据，一个有 6396 个样本，另一个有 1599 个样本。这可能会在尝试拟合模型或对数据执行其他操作时导致问题。
要解决此问题，您可能需要检查代码并确保对每个操作使用正确版本的输入数据。您可能还想尝试清理输入数据，删除任何重复或不一致的数据。
]]></description>
      <guid>https://stackoverflow.com/questions/78717924/valuererror-found-input-variables-with-inconsistent-numbers-of-samples</guid>
      <pubDate>Sun, 07 Jul 2024 17:09:03 GMT</pubDate>
    </item>
    <item>
      <title>我在 WLASL 视频数据集上训练的用于预测手语手势的 ConvLSTM 模型在 10 个周期的训练阶段并没有提高准确率</title>
      <link>https://stackoverflow.com/questions/78717922/my-convlstm-model-for-predicting-sign-language-gestures-trained-on-wlasl-video-d</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78717922/my-convlstm-model-for-predicting-sign-language-gestures-trained-on-wlasl-video-d</guid>
      <pubDate>Sun, 07 Jul 2024 17:07:23 GMT</pubDate>
    </item>
    <item>
      <title>元特征分析：评估不同特征组合之间复杂性的中间结果</title>
      <link>https://stackoverflow.com/questions/78717886/meta-feature-analysis-intermediate-results-to-assess-complexity-among-different</link>
      <description><![CDATA[我正在使用 meta-feature 分析包。它提供了一种方便的方式来汇总元特征统计信息。
但是在这种情况下，我正在寻找一种通过成对特征组合来访问数据集中数据点复杂性分析的“中间”结果的方法。这是为了让我理解具有特征的数据集比其他数据集具有更高的类重叠（复杂性）。
例如，在只有 4 个特征（萼片长度、萼片宽度、花瓣长度、花瓣宽度）的鸢尾花数据集中：
pip install -U pymfe # package install

from sklearn.datasets import load_iris
from pymfe.mfe import MFE
data = load_iris()
X= data.data
y = data.target

而不是整体摘要：
extractor = MFE(features=[ &quot;f1&quot;], groups=[&quot;complexity&quot;],
summary=[&quot;mean&quot;, &quot;sd&quot;])
extractor.fit(X,y)
meta_feat = extractor.extract()

为了获得平均值和标准差，我想分析由于萼片长度 v 萼片宽度、萼片长度 v 花瓣长度、萼片长度 v 花瓣宽度等造成的复杂性...因为f1沿垂直轴投射数据点以组合二元特征。
如何实现这一目标？
如果可能的话，一些可视化帮助也将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78717886/meta-feature-analysis-intermediate-results-to-assess-complexity-among-different</guid>
      <pubDate>Sun, 07 Jul 2024 16:51:09 GMT</pubDate>
    </item>
    <item>
      <title>GNN 的损失没有减少</title>
      <link>https://stackoverflow.com/questions/78717784/the-loss-in-gnn-doesnt-decrease</link>
      <description><![CDATA[当我研究 GNN 时，我的训练代码无法训练模块，参数保持不变
def create_node_emb(num_node=34, embedding_dim=16):
# TODO：实现此函数，它将创建节点嵌入矩阵。
# 将返回一个 torch.nn.Embedding 层。您不需要更改 
# num_node 和 embedding_dim 的值。返回的 
# 层的权重矩阵应在均匀分布下初始化。

emb = None

############# 您的代码在此处 ############
emb = torch.nn.Embedding(num_embeddings=num_node,embedding_dim=embedding_dim)
shape = emb.weight.data.shape
emb.weight.data = torch.rand(shape)
############################################

return emb

这是正确的代码
def train(emb, loss_fn, sigmoid, train_label, train_edge):
# TODO：在此处训练嵌入层。您还可以更改 epoch 和 
# 学习率。一般来说，你需要实现：
#（1）获取 train_edge 中节点的嵌入
#（2）对每个节点对之间的嵌入进行点积
#（3）将点积结果输入 sigmoid
#（4）将 sigmoid 输出输入 loss_fn
#（5）打印每个 epoch 的损失和准确率
#（6）使用损失和优化器更新嵌入
#（作为健全性检查，损失应该在训练期间减少）
# epochs = 500
learning_rate = 0.1

optimizer = SGD(emb.parameters(), lr=learning_rate, motivation=0.9)

for i in range(epochs):
emb.train()
############# 您的代码在这里 ############
optimizer.zero_grad()

dot_pro = torch.sum(output[0]*output[1],dim=1)
pred = sigmoid(dot_pro)
loss = loss_fn(pred,train_label)
loss.backward()
optimizer.step()

acc = accuracy(pred,train_label)
if i%50 == 0:
print(&quot;acccracy ={},loss={}&quot;.format(acc,loss))
params = list(emb.parameters())
for i, param in enumerate(params):
print(&quot;Parameter {} shape: {}&quot;.format(i, param.shape))
print(param)

但是如果我像下面的代码一样更改训练代码，
def train(emb, loss_fn, sigmoid, train_label, train_edge):
# TODO：在此处训练嵌入层。您还可以更改时期和 
# 学习率。一般来说，你需要实现：
#（1）获取 train_edge 中节点的嵌入
#（2）对每个节点对之间的嵌入进行点积
#（3）将点积结果输入 sigmoid
#（4）将 sigmoid 输出输入 loss_fn
#（5）打印每个 epoch 的损失和准确率
#（6）使用损失和优化器更新嵌入
#（作为健全性检查，损失应该在训练期间减少）
# epochs = 500
learning_rate = 0.1

optimizer = SGD(emb.parameters(), lr=learning_rate, motivation=0.9)

for i in range(epochs):
emb.train()
############# 您的代码在这里 ############
optimizer.zero_grad()

node_list_fir,node_list_sec = (train_edge[0]),(train_edge[1])
emb_list_fir,emb_list_sec = emb(node_list_fir),emb(node_list_sec) 
emb_torch = torch.sum(emb_list_fir*emb_list_sec,dim=1)
emb_torch = sigmoid(emb_torch)

emb_res = torch.round(emb_torch)
loss = loss_fn(emb_res,train_label)

loss.backward()
optimizer.step()

acc = accuracy(emb_res,train_label)
if i%50 == 0:
print(&quot;acccracy ={},loss={}&quot;.format(acc,loss))
params = list(emb.parameters())
for i, param in enumerate(params):
print(&quot;参数 {} 形状: {}&quot;.format(i, param.shape))
print(param)


日志中的参数没有变化
这种现象的原因是什么，我是深度学习新手，为什么我将 train_edge 分成两部分然后输入到模型中，训练效果不好。我不知道背后的理论]]></description>
      <guid>https://stackoverflow.com/questions/78717784/the-loss-in-gnn-doesnt-decrease</guid>
      <pubDate>Sun, 07 Jul 2024 16:07:40 GMT</pubDate>
    </item>
    <item>
      <title>我是否应该为移动应用程序训练用于手写文本识别的 OCR 模型，还是仅使用 API 服务？</title>
      <link>https://stackoverflow.com/questions/78717411/should-i-train-an-ocr-model-for-handwritten-text-recognition-for-a-mobile-app-or</link>
      <description><![CDATA[为移动应用程序构建手写文本识别模型是否值得（需要在下个月完成此项目）？准确率必须至少达到 75%。
我正在构建一个简单的 Android 移动应用程序（React Native 前端），该应用程序能够识别手写文本，这是我学校项目的一部分。
基本上，这个概念是，孩子会在纸上写 15 个短语，应用程序必须至少在 75% 的时间内正确识别它。这些短语很简单，例如：“你好”、“早上好”、“你好吗？”等等。
它不适用于一般的手写文本识别目的，而是只需要识别 15 个特定短语。写这些短语的人是 10-14 岁的孩子。
从成本和性能方面来看，这里最好的选择是什么？我应该只使用 Google Vision 等 API 还是训练模型（例如使用 PaddleOCR 或 TrOCR 的模型）并将其部署到本地？如果是后者，我可以在哪个平台上进行训练，我可以针对我的情况使用哪个数据集？
我尝试使用 Google Vision 和 Microsoft API 进行文本识别，它们确实表现良好，但 1000 个请求的成本约为 2 美元。我还尝试了 Tesseract 和 PaddleOCR 推理模型，但它们的表现不佳。]]></description>
      <guid>https://stackoverflow.com/questions/78717411/should-i-train-an-ocr-model-for-handwritten-text-recognition-for-a-mobile-app-or</guid>
      <pubDate>Sun, 07 Jul 2024 13:21:19 GMT</pubDate>
    </item>
    <item>
      <title>如何转换日期格式</title>
      <link>https://stackoverflow.com/questions/78708002/how-to-convert-date-formet</link>
      <description><![CDATA[我正在用 Python 训练模型，下面是我的代码。
#将数据拆分为 x 和 y
x = tsla.drop(&quot;Date&quot;, axis=1)
y = tsla[&#39;Date&#39;]

#将数据拆分为训练和测试
x_train, x_test, y_train, y_tset = train_test_split(x,y, test_size=0.2, random_state=42)

# 模型 
model = LinearRegression()
model.fit(x_train, y_train)

我尝试在 x y 上训练模型。
但这是我的代码得到的错误：
ValueError：无法将字符串转换为浮点数：&#39;2020-03-24&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/78708002/how-to-convert-date-formet</guid>
      <pubDate>Thu, 04 Jul 2024 15:47:21 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Pytorch 从多幅图像中获取一定数量图像的梯度？而不是所有输入图像的梯度？</title>
      <link>https://stackoverflow.com/questions/78705415/how-can-i-get-the-gradient-of-a-certain-number-of-image-out-of-multiple-images-w</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78705415/how-can-i-get-the-gradient-of-a-certain-number-of-image-out-of-multiple-images-w</guid>
      <pubDate>Thu, 04 Jul 2024 06:39:42 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用免费资源在本地机器上的大数据集上训练 gpt2 [关闭]</title>
      <link>https://stackoverflow.com/questions/78686632/trying-to-train-gpt2-on-large-dataset-in-local-machine-with-free-resources</link>
      <description><![CDATA[是否可以在 colab、jupyter 或 kaggle 上对 1.5m 个数据点进行 gpt2 训练？
到目前为止，我尝试在 colab 中进行此操作，但在标记化过程中会耗尽存储空间，这是可以理解的。我也尝试了批处理技术。后来我尝试在 kaggle 上运行相同的算法，但目前它在加载转换器时显示错误。仍在尝试运行它。我只是想知道是否可以做到这一点！]]></description>
      <guid>https://stackoverflow.com/questions/78686632/trying-to-train-gpt2-on-large-dataset-in-local-machine-with-free-resources</guid>
      <pubDate>Sat, 29 Jun 2024 17:07:55 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我定义自己的调用函数而没有参数“training”时，我可以使用 model(x, training =True)？</title>
      <link>https://stackoverflow.com/questions/72716154/why-can-i-use-modelx-training-true-when-i-define-my-own-call-function-withou</link>
      <description><![CDATA[请注意，当我创建模型时，我使用参数 something = False 定义了调用函数，当我在函数 train_step 中使用该模型时，我输入了“something =True, training = True”，训练未在我的调用中定义，但它在默认的 tf.keras.model 调用中。
为什么我能够毫无错误地执行此操作？输出基本上打印了一堆“我的调用”。
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# 添加通道维度
x_train = x_train[..., tf.newaxis].astype(&quot;float32&quot;)
x_test = x_test[..., tf.newaxis].astype(&quot;float32&quot;)

train_ds = tf.data.Dataset.from_tensor_slices(
(x_train, y_train)).shuffle(10000).batch(32)

class MyModel(Model):
def __init__(self):
super(MyModel, self).__init__()
self.fl = Flatten()
self.d = Dense(10)

######我的问题#######
def call(self, x, something=False):
if something:
tf.print(&#39;my call&#39;)
x = self.fl(x)
return self.d(x)

model = MyModel()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam()

@tf.function
def train_step(X,Y):
with tf.GradientTape() as tape:
######我的问题#######
predictions = model(X, something =True, training = True)
loss = loss_object(Y, predictions)
gradients = tape.gradient(loss, model.trainable_variables)
optimizer.apply_gradients(zip(gradients, model.trainable_variables))

对于范围 (3) 中的 epoch：

对于 train_ds 中的 X、Y：
train_step(X,Y)
]]></description>
      <guid>https://stackoverflow.com/questions/72716154/why-can-i-use-modelx-training-true-when-i-define-my-own-call-function-withou</guid>
      <pubDate>Wed, 22 Jun 2022 13:10:23 GMT</pubDate>
    </item>
    <item>
      <title>我们应该在 train_test_split() 中为 random_state 使用什么值，以及在哪种情况下？[关闭]</title>
      <link>https://stackoverflow.com/questions/54264452/what-value-should-we-use-for-random-state-in-train-test-split-and-in-which-sce</link>
      <description><![CDATA[X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

在上面的代码中，random_state 使用的是 0。为什么我们不使用 1？]]></description>
      <guid>https://stackoverflow.com/questions/54264452/what-value-should-we-use-for-random-state-in-train-test-split-and-in-which-sce</guid>
      <pubDate>Sat, 19 Jan 2019 05:52:01 GMT</pubDate>
    </item>
    </channel>
</rss>