<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 30 Jul 2024 18:20:46 GMT</lastBuildDate>
    <item>
      <title>将两类图像分类器结合在一起</title>
      <link>https://stackoverflow.com/questions/78813144/combine-two-class-of-image-classifier-together</link>
      <description><![CDATA[我制作了两个模型，一个用于狗与猫的分类（它还告诉品种），另一个用于车辆分类（它还告诉汽车的型号），有没有办法将这两个文件结合起来，以便我可以用它来预测我想要的东西（使用 API 的概念）
我的意思是说，在进行预测时，我需要在特定模型中特别上传文件，但我想知道是否有第三种方法，我只需要上传图像，然后它就会提供预测]]></description>
      <guid>https://stackoverflow.com/questions/78813144/combine-two-class-of-image-classifier-together</guid>
      <pubDate>Tue, 30 Jul 2024 18:08:20 GMT</pubDate>
    </item>
    <item>
      <title>Python 错误（类的数量必须大于一；得到 1 个类）</title>
      <link>https://stackoverflow.com/questions/78812818/python-error-the-number-of-classes-has-to-be-greater-than-one-got-1-class</link>
      <description><![CDATA[我需要创建一个 GMM 模型，并且对于每个分布，我必须训练一个 svm 模型。这是目前为止的代码。但我收到此错误（类数必须大于 1；得到 1 个类）。
如何针对每个分布训练每个 svm？
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# 中心数（分布）
N = 3

# 生成具有不同聚类标准差的随机数据点
n_samples = 50000
n_features = 2
cluster_std = 1.0

X, y = make_blobs(n_samples=n_samples, centers=N, n_features=n_features, 
cluster_std=cluster_std, random_state=42)

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建并训练 GMM 模型
gmm = GaussianMixture(n_components=N, random_state=42)
gmm.fit(X_train)

# 预测训练集的聚类标签
y_train_pred = gmm.predict(X_train)

# 为每个分布训练一个 SVM 模型
svms = []
for i in range(N):
# 为当前分布过滤训练样本
X_train_i = X_train[y_train_pred == i]
y_train_i = y_train[y_train_pred == i]

# 为当前分布训练 SVM
svm = SVC(kernel=&#39;linear&#39;, random_state=42)
svm.fit(X_train_i, y_train_i)
svms.append(svm)

# 保存训练好的 SVM 模型以供将来使用
import joblib
for i, svm in enumerate(svms):
joblib.dump(svm, f&#39;svm_model_distribution_{i}.pkl&#39;)

print(&quot;SVM 训练已完成。&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78812818/python-error-the-number-of-classes-has-to-be-greater-than-one-got-1-class</guid>
      <pubDate>Tue, 30 Jul 2024 16:25:56 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ML/DL 模型预测范围 [0, 0.1] 内的极小十进制值？</title>
      <link>https://stackoverflow.com/questions/78812662/how-to-predict-extremely-small-decimal-values-in-range-0-0-1-with-an-ml-dl-mo</link>
      <description><![CDATA[我正在处理一个数据集，其中所有值都介于 0 和 0.1 之间，最小的非零值为
3.19526695102823×10^−8。我计划使用机器学习模型来预测这些值。
我正在考虑使用最小-最大缩放或对数转换来缩放数据，以提高模型性能。但是，我担心精度问题，因为浮点精度非常小且有限（比如说 float32 精度）。
问题：

即使在缩放后，LSTM 模型是否能够有效地预测这些小值？
基于树的模型可以很好地处理如此小的缩放值吗？
是否有任何最佳实践或替代方法可以确保模型能够准确预测这个小范围内的值？
]]></description>
      <guid>https://stackoverflow.com/questions/78812662/how-to-predict-extremely-small-decimal-values-in-range-0-0-1-with-an-ml-dl-mo</guid>
      <pubDate>Tue, 30 Jul 2024 15:51:33 GMT</pubDate>
    </item>
    <item>
      <title>我写了一个代码，运行时间太长</title>
      <link>https://stackoverflow.com/questions/78812154/i-wrote-a-code-and-the-running-time-is-too-long</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78812154/i-wrote-a-code-and-the-running-time-is-too-long</guid>
      <pubDate>Tue, 30 Jul 2024 13:59:27 GMT</pubDate>
    </item>
    <item>
      <title>从采访脚本中提取问题和答案[关闭]</title>
      <link>https://stackoverflow.com/questions/78812091/extracting-question-and-answer-from-the-interview-script</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78812091/extracting-question-and-answer-from-the-interview-script</guid>
      <pubDate>Tue, 30 Jul 2024 13:46:06 GMT</pubDate>
    </item>
    <item>
      <title>如何判断汽车配置的相似性？</title>
      <link>https://stackoverflow.com/questions/78811479/how-to-determine-the-similarity-of-a-cars-configuration</link>
      <description><![CDATA[有一个车辆配置的数据集：

ID
品牌
型号
代数
发动机类型
发动机排量
气缸数
车身类型
变速箱类型
发动机代码
制造年份

任务：
确定一个配置与另一个配置的相似程度。
我假设将数据集中的每个条目表示为一个向量，并计算向量之间的余弦相似度。
但是对于如何以数值形式表示值存在误解，例如车身类型：轿车、跨界车、轿跑车等。]]></description>
      <guid>https://stackoverflow.com/questions/78811479/how-to-determine-the-similarity-of-a-cars-configuration</guid>
      <pubDate>Tue, 30 Jul 2024 11:38:50 GMT</pubDate>
    </item>
    <item>
      <title>CVAE 合成数据分布范围过窄</title>
      <link>https://stackoverflow.com/questions/78809995/cvae-synthetic-data-distributed-too-narrowly</link>
      <description><![CDATA[我有一个包含三个特征的数据集，两个浮点特征和一个具有 33 个类别的分类特征。（此处称为 Float_A、Float_B 和 Cat_A）。
我正在尝试训练 CVAE 以生成合成数据。使用以下 sklearn 转换器转换数据：
df=df[[&quot;float_A&quot;,&quot;float_B&quot;,&quot;categorical_A&quot;]]

transformers=[(&#39;float_A&#39;,Pipeline(steps=[(&#39;imputer&#39;,SimpleImputer(strategy=&#39;mean&#39;,add_indicator=True)),
(&#39;scaler&#39;,RobustScaler(quantile_range=(5,95)))]),
[&#39;float_A&#39;]),
(&#39;float_B&#39;,
Pipeline( steps=[(&#39;imputer&#39;,SimpleImputer(strategy=&#39;mean&#39;,add_indicator=True)),
(&#39;scaler&#39;,MinMaxScaler())]),
[&#39;float_B&#39;]),
(&#39;cats&#39;,OneHotEncoder(),categorical_columns)]`

transformer=ColumnTransformer(transformers,remainder=&#39;passthrough&#39;)

transformed_df=transformer.fit_transform(df)

我的第二个浮点数有一个 S 形激活函数，声明如下：
Def sample(self,z):
reconstructed=self.decoder(z)
# 将 S 形激活应用于浮点数特征。
reconstructed[:,self.float_B_idx]=torch.sigmoid(reconstructed[:,self.float_B_idx])
returnreconstructed

Def forward(self,x):
z_mean,z_log_var=torch.chunk(self.encoder(x),2,dim=1)
z=self.reparameterize(z_mean,z_log_var)
reconstructed=self.decoder(z)
#将 sigmoid 激活应用于浮点特征。
reconstructed[:,self.float_B_idx]=torch.sigmoid(reconstructed[:,self.float_B_idx])
return reconstructed,z_mean,z_log_var

一旦 CVAE 经过训练（训练和验证损失似乎按应有的方式减少），我尝试使用以下方法生成随机样本：
random_latent_vectors=torch.randn(num_samples,latent_dim)

使用 torch.no_grad()：
gen_df=model.sample(random_latent_vectors).detach().cpu().numpy()

但是 gen_df 中的所有样本都非常“未展开”。
FloatA、FloatB、 Cat[0:2]…

[[0.11782782 0.286538 0.646666 0.266387 0.09747571]
[0.0963359 0.29775462 0.58443785 0.29296008 0.1101962]
[0.1300626 0.31274286 0.59086925 0.30710378 0.10169853]
[0.1232817 0.32317564 0.56470346 0.29102385 0.11446829]
[0.13240162 0.28100765 0.6230704 0.29497638 0.08924796]]

然后，当我在 gen_df 上调用 scaler.inverse_transform 时，我几乎在每一行上都得到了相同的结果。
我尝试了各种方法，我的一个类别非常占主导地位（~90%），因此使用 imblearn 进行了一些类别不平衡欠采样，使其仅占 50% 的主导地位，但仍然获得 100% 的样本。
我尝试为我的 CVAE 添加更多层和复杂性，但再次被证明是徒劳的。]]></description>
      <guid>https://stackoverflow.com/questions/78809995/cvae-synthetic-data-distributed-too-narrowly</guid>
      <pubDate>Tue, 30 Jul 2024 05:57:45 GMT</pubDate>
    </item>
    <item>
      <title>RNN 建模数据准备</title>
      <link>https://stackoverflow.com/questions/78809490/rnn-modelling-data-preparation</link>
      <description><![CDATA[我正在准备用于 rnn 模型的顺序数据，但我将时间数据放在不同的列中，其中天数格式为 0 表示工作日，1 表示周末。时间是否应采用单一数据格式列以用于模型？
此外，我还应该如何准备数据以计算与传感器数据的距离。我添加了数据和距离问题的屏幕截图。
在此处输入图片说明
在此处输入图片说明]]></description>
      <guid>https://stackoverflow.com/questions/78809490/rnn-modelling-data-preparation</guid>
      <pubDate>Tue, 30 Jul 2024 01:26:23 GMT</pubDate>
    </item>
    <item>
      <title>TFLM“Interpreter->Invoke()”问题导致硬故障</title>
      <link>https://stackoverflow.com/questions/78808999/issues-with-tflm-interpreter-invoke-causing-hard-fault</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78808999/issues-with-tflm-interpreter-invoke-causing-hard-fault</guid>
      <pubDate>Mon, 29 Jul 2024 20:48:04 GMT</pubDate>
    </item>
    <item>
      <title>语义分割中的数据增强[关闭]</title>
      <link>https://stackoverflow.com/questions/78798068/data-augmentation-in-semantic-segmentation</link>
      <description><![CDATA[我尝试对数据集执行数据增强，但得到的却是空白的白色图像，在蒙版上工作正常，但图像存在问题。
如何解决这个问题？
这是原始图像+蒙版
增强图像+蒙版
增强代码：
seed = 24
batch_size = 8

img_datagen_args = dict(rescale = 1/255,
rotation_range = 5,
zoom_range = 0.1,
Horizo​​ntal_flip = True,
Vertical_flip = True,
Fill_mode = &#39;nearest&#39;)

mask_datagen_args = dict(rescale = 1/255,
Rotation_range = 5,
Zoom_range = 0.1,
Horizo​​ntal_flip = True,
Vertical_flip = True,
Fill_mode = &#39;nearest&#39;,
Preprocessing_function = lambda x: np.where(x&gt;0, 1, 0).astype(x.dtype))

img_datagen = ImageDataGenerator(**img_datagen_args)
img_generator = img_datagen.flow_from_directory(&#39;/content/split_dataset/train/images/&#39;,
Seed = Seed,
Batch_size = Batch_size,
Class_mode = None)

mask_datagen = ImageDataGenerator(**mask_datagen_args)
mask_generator = mask_datagen.flow_from_directory(&#39;/content/split_dataset/train/masks/&#39;,
seed = seed,
batch_size = batch_size,
color_mode = &#39;grayscale&#39;,
class_mode = None)

valid_img_generator = img_datagen.flow_from_directory(&#39;/content/split_dataset/test/images/&#39;,
seed = seed,
batch_size = batch_size,
class_mode = None)

valid_mask_generator = mask_datagen.flow_from_directory(&quot;/content/split_dataset/test/masks/&quot;,
seed = seed,
batch_size = batch_size,
color_mode = &#39;grayscale&#39;,
class_mode = None)

train_generator = zip(img_generator, mask_generator)
valid_generator = zip(valid_img_generator, valid_mask_generator)

可视化代码：
import matplotlib.pyplot as plt

# 获取一批图像和掩码
img_batch, mask_batch = next(zip(img_generator, mask_generator))

# 绘制一些增强图像
fig, ax = plt.subplots(1, 4, figsize=(20, 5))
for i in range(4):
ax[i].imshow(img_batch[i]) # 对灰度图像使用 cmap=&#39;gray&#39;
ax[i].set_title(f&quot;Augmented Image {i+1}&quot;)
ax[i].axis(&#39;off&#39;)
plt.show()

# 绘制相应的掩码
fig, ax = plt.subplots(1, 4, figsize=(20, 5))
for i in range(4):
ax[i].imshow(mask_batch[i]) # 如果是灰度，则挤压以删除通道维度
ax[i].set_title(f&quot;增强蒙版 {i+1}&quot;)
ax[i].axis(&#39;off&#39;)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78798068/data-augmentation-in-semantic-segmentation</guid>
      <pubDate>Fri, 26 Jul 2024 12:32:26 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 展开和折叠：如何将这个图像张量重新组合在一起？</title>
      <link>https://stackoverflow.com/questions/64048720/pytorch-unfold-and-fold-how-do-i-put-this-image-tensor-back-together-again</link>
      <description><![CDATA[我尝试使用展开来过滤大小为 256x256 的单通道 2D 图像，以创建重叠为 8 的 16x16 块。如下所示：
# I = [256, 256] image
kernel_size = 16
stride = bx/2
patches = I.unfold(1, kernel_size, 
int(stride)).unfold(0, kernel_size, int(stride)) # size = [31, 31, 16, 16]


我已经开始尝试使用折叠将图像重新组合在一起，但还没有成功。我尝试使用视图让图像“适合”它应该的方式，但我不知道这将如何保留原始图像。也许我想太多了。
# patches.shape = [31, 31, 16, 16]
patches = = filt_data_block.contiguous().view(-1, kernel_size*kernel_size) # [961, 256]
patches = patches.permute(1, 0) # size = [951, 256]

如能得到任何帮助，我们将不胜感激。非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/64048720/pytorch-unfold-and-fold-how-do-i-put-this-image-tensor-back-together-again</guid>
      <pubDate>Thu, 24 Sep 2020 14:34:01 GMT</pubDate>
    </item>
    <item>
      <title>为 LeNet 创建数据集？</title>
      <link>https://stackoverflow.com/questions/36509340/create-dataset-for-lenet</link>
      <description><![CDATA[我正在做一个项目，我想创建一个绘制人脸的数据集（概念上类似于 CUFS 数据集）。除了手绘人脸之外，我该如何从“我已将这些图像文件上传到我的计算机并确保它们都具有相同的尺寸”到拥有一个随时可用的数据集？（我想用这个数据集训练/测试 LeNet。）我以前从未创建过自己的数据集，所以不太确定如何开始。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/36509340/create-dataset-for-lenet</guid>
      <pubDate>Fri, 08 Apr 2016 21:05:15 GMT</pubDate>
    </item>
    <item>
      <title>mAP 指标是什么以及如何计算？[关闭]</title>
      <link>https://stackoverflow.com/questions/36274638/what-is-the-map-metric-and-how-is-it-calculated</link>
      <description><![CDATA[在计算机视觉和物体检测中，一种常见的评估方法是mAP。
它是什么以及如何计算？]]></description>
      <guid>https://stackoverflow.com/questions/36274638/what-is-the-map-metric-and-how-is-it-calculated</guid>
      <pubDate>Tue, 29 Mar 2016 03:03:12 GMT</pubDate>
    </item>
    <item>
      <title>我如何检测实心圆圈的网格？</title>
      <link>https://stackoverflow.com/questions/35320416/how-can-i-detect-a-grid-of-filled-circles</link>
      <description><![CDATA[给定一个四子棋盘的图像，我想要识别并输出棋盘的状态（一个 6×7 的矩阵）。我尝试的第一种方法是基于找到圆，然后在它们的质心处寻找网格图案。
这是我使用的 open-cv 函数：
circles = cv2.HoughCircles(bw_im,
cv2.cv.CV_HOUGH_GRADIENT,
dp=DP,
minDist=MIN_DIST,
minRadius=MIN_RADIUS,
maxRadius=MAX_RADIUS)

我添加了非最大抑制，但结果并不好。
有没有比直接处理霍夫圆更好的方法，也许有某种我不知道的填充圆形形态学操作。
这是一个示例输入图像：

您可以假设输入图片已被裁剪，并且具有与上图类似的边距（我有另一段代码可以处理这个问题）。]]></description>
      <guid>https://stackoverflow.com/questions/35320416/how-can-i-detect-a-grid-of-filled-circles</guid>
      <pubDate>Wed, 10 Feb 2016 16:18:37 GMT</pubDate>
    </item>
    <item>
      <title>保存视频中每帧的边界框坐标</title>
      <link>https://stackoverflow.com/questions/30060567/saving-bounding-box-coordinates-for-each-frame-in-a-video</link>
      <description><![CDATA[我有一段摄像机拍摄的视频，场景中有人。我需要浏览该视频的每一帧，并手动保存场景中检测到的人的边界框的坐标（浏览每一帧并在每个人周围画出一个正方形）和头部中心的坐标 - 基本上就是左上、右下、头部中心坐标。边界框必须是正方形。
然后，附加程序将读取一个文件，其中包含正方形和头部中心的坐标以及帧号，并将这些框提取为图像。
对于任何有计算机视觉经验的人来说 - 是否有任何开源软件可以完成我的要求？如果没有，您会推荐使用什么技术来构建此工具？有入门代码吗？]]></description>
      <guid>https://stackoverflow.com/questions/30060567/saving-bounding-box-coordinates-for-each-frame-in-a-video</guid>
      <pubDate>Tue, 05 May 2015 18:27:04 GMT</pubDate>
    </item>
    </channel>
</rss>