<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 30 Jan 2024 12:23:17 GMT</lastBuildDate>
    <item>
      <title>考虑到所分析的问题，LSTM 的输入形状是否正确？</title>
      <link>https://stackoverflow.com/questions/77905948/is-the-input-shape-for-the-lstm-correct-considering-the-problem-under-analysis</link>
      <description><![CDATA[我有一个数据集，其中包含 5000 个模拟 x 21 个时间步长 x 49 个节点，总共 5145000 个观测值。该数据集是基于有限元模拟创建的。我正在尝试使用 LSTM 来预测每个节点的 x、y、z 坐标（每个节点对应于一个观察值）。
&lt;前&gt;&lt;代码&gt;OUTPUT_SHAPE = 3

模型=顺序（）
model.add(LSTM(num_neurons,activation=activation_function,input_shape=(x_train.shape[1],x_train.shape[2])))
model.add（密集（OUTPUT_SHAPE））

模型.编译(
    损失=“平均绝对误差”，
    优化器=tf.keras.optimizers.Adam(learning_rate=learning_rate),
    指标=[“平均绝对误差”]
）

以下是 1 次模拟的数据集示例：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

时间
节点位置
壮举3
壮举4
壮举5
壮举6
壮举7
壮举8
壮举9
壮举10


&lt;正文&gt;

0
0
...
...
...
...
...
...
...
...


0
1
...
...
...
...
...
...
...
...


...
...
...
...
...
...
...
...
...
...


0
48
...
...
...
...
...
...
...
...


--------
--------
--------
--------
--------
--------
--------
--------
--------
--------


...
...
...
...
...
...
...
...
...
...


--------
--------
--------
--------
--------
--------
--------
--------
--------
--------


20
0
...
...
...
...
...
...
...
...


20
1
...
...
...
...
...
...
...
...


...
...
...
...
...
...
...
...
...
...


20
48
...
...
...
...
...
...
...
...




由于我想预测每个观测值的坐标，因此 LSTM 的输入形式被定义为 n° 样本 x 1 x 10（10 是特征数量）。我使用 1 作为时间步长，因为每次模拟中我拥有的唯一信息是 t=0 的信息，因此我无法使用更多过去的观察结果来预测新的观察结果。
X_train.shape = (1039290, 1, 10)
y_train.shape = (1039290, 3)

问题是我没有唯一的时间序列，我有多个时间序列（每个模拟有 49 个对应于每个节点位移）。
那么这样考虑 LSTM 的输入是错误的吗？]]></description>
      <guid>https://stackoverflow.com/questions/77905948/is-the-input-shape-for-the-lstm-correct-considering-the-problem-under-analysis</guid>
      <pubDate>Tue, 30 Jan 2024 11:18:34 GMT</pubDate>
    </item>
    <item>
      <title>Earth Engine：尽管有有效的最小值/最大值，但陆地卫星频带中的空值，寻求诊断问题的帮助</title>
      <link>https://stackoverflow.com/questions/77905652/earth-engine-null-values-in-landsat-bands-despite-valid-min-max-seeking-assist</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77905652/earth-engine-null-values-in-landsat-bands-despite-valid-min-max-seeking-assist</guid>
      <pubDate>Tue, 30 Jan 2024 10:33:39 GMT</pubDate>
    </item>
    <item>
      <title>Ray集群无法调度资源</title>
      <link>https://stackoverflow.com/questions/77904953/ray-cluster-cant-schedule-resources</link>
      <description><![CDATA[我是机器学习的初学者。当我尝试重现 FedRolex 的实验时，我遇到了这个问题：它不断告诉我“警告：以下资源请求无法立即调度：{&#39;GPU &#39;：0.15，&#39;CPU&#39;：1.0}。这可能是由于所有集群资源都被参与者占用。考虑创建更少的参与者或向该 Ray 集群添加更多节点。”我以为没关系，但是这个实验到现在已经运行了大约3个小时，所以应该有问题。在此处输入图片描述
我在 https://github.com/AIoT-MLSys-Lab/ 中使用相同的程序联邦劳力士
我尝试更换显卡，也更换了新版本的Ray，但还是无法解决这个问题。希望有人能告诉我这个程序运行这么长时间是否正常，是吗？卡在某个地方或者只是需要运行很长时间。]]></description>
      <guid>https://stackoverflow.com/questions/77904953/ray-cluster-cant-schedule-resources</guid>
      <pubDate>Tue, 30 Jan 2024 08:43:42 GMT</pubDate>
    </item>
    <item>
      <title>将张量流中的非序列数据的掩蔽层和密集层结合起来</title>
      <link>https://stackoverflow.com/questions/77904938/combining-masking-and-dense-layers-for-non-sequential-data-in-tensorflow</link>
      <description><![CDATA[我正在研究一个分类问题，其中有不同长度的数据。每个样本都是 -1 到 1 之间的实数值的 numpy 数组。因为每个样本的长度不同，所以我使用填充，以便可以将其输入神经网络。根据tensorflow的文档，我还需要一个与填充结合使用的掩蔽层。然而，我还了解到，并非每一层都会传播掩码，我担心这可能是我的网络表现不佳的原因。我运行了带有和不带有填充+掩蔽层的分类器，结果相似。我想问您将遮罩层与致密层结合使用的最佳方法是什么。下面是我的网络的代码示例。
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入密集，掩蔽
从tensorflow.keras.utils导入pad_sequences
从 sklearn.model_selection 导入 train_test_split

X_padd = pad_sequences(X, padding=&#39;post&#39;, dtype=&#39;float32&#39;)

X_train, X_test, y_train, y_test = train_test_split(X_pangled, y, test_size=0.3, random_state=101, shuffle=True)

模型=顺序（）
model.add(掩蔽(mask_value=0.0, input_shape=(X_train.shape[1],)))
model.add（密集（1024，激活=&#39;relu&#39;））
model.add（密集（512，激活=&#39;relu&#39;））
model.add（密集（1，激活=&#39;sigmoid&#39;））

model.compile（优化器=&#39;adam&#39;，损失=&#39;binary_crossentropy&#39;，指标=[“准确性”]）

model.fit（x = X_train，y = y_train，epochs = 600，validation_data =（X_test，y_test））

训练数据看起来或多或少像这样：
&lt;预&gt;&lt;代码&gt;X = [[-0.1, 0.2, 0.1], [0.4, 0.3], [-0.2, -0.44, 0.32, 0.5], ...]
y = [0, 1, 1, ...]

感谢您的任何提示！]]></description>
      <guid>https://stackoverflow.com/questions/77904938/combining-masking-and-dense-layers-for-non-sequential-data-in-tensorflow</guid>
      <pubDate>Tue, 30 Jan 2024 08:41:04 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：层“conv3d”的输入 0 与该层不兼容 - Keras 3DConv 模型</title>
      <link>https://stackoverflow.com/questions/77904323/valueerror-input-0-of-layer-conv3d-is-incompatible-with-the-layer-keras-3dc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77904323/valueerror-input-0-of-layer-conv3d-is-incompatible-with-the-layer-keras-3dc</guid>
      <pubDate>Tue, 30 Jan 2024 06:38:27 GMT</pubDate>
    </item>
    <item>
      <title>d.预测和分类（医疗保健）</title>
      <link>https://stackoverflow.com/questions/77904188/d-predictions-and-classifications-health-care</link>
      <description><![CDATA[1.评估简介
根据 NHS 的说法，心血管疾病是影响心脏或血管的疾病的总称。它通常是由多种因素引起的，例如动脉内脂肪沉积物的堆积和血栓风险增加。它还与动脉和其他重要器官（如大脑、心脏、肾脏和眼睛）的损伤有关。例如，在英国，这是导致死亡的主要原因之一。
数字健康不仅正在改变人们的健康管理方式，而且有助于预防可避免的死亡，例如上面介绍的与心血管疾病相关的死亡。从将健康数据和护理转化为行动，它逐渐成为医疗保健提供者和患者自身管理健康相关问题的不可或缺的一部分。此外，智能手机和可穿戴设备等网络设备的出现使得移动医疗应用程序和软件成为可能，为临床医生使用可用数据和人工智能 (AI) 或机器学习 (ML) 做出临床决策提供支持。数字健康可以帮助管理心血管疾病，并利用人们的生命体征预测即将发生致命器官衰竭的可能性，帮助最大限度地减少死亡人数。
在此作业中，您获得了一个数据集，其中包含一些匿名患者的心脏相关重要信息。假设您已被医疗保健提供者聘用为数据科学家。您的部分工作是分析此数据集，然后设计和实现一个应用程序，可以帮助临床医生监测患者的生命体征（例如血压等）并预测即将发生的死亡。
根据模块的学习成果，作业将分两个不同的阶段进行评估，分别对应课程作业 1 和课程作业 2。第二个作业是第一个作业的延续，但使用了 OOP 概念、探索性数据分析和 Python 机器学习。您提交的内容必须采用已实施模块和 5 页报告的形式。
2.入门和一般规格
a.使用面向对象的概念实现模块
在此任务中，您将使用 OOP 概念设计和实现您的解决方案。您可以选择使用类、方法等来实现解决方案的全部或部分关键部分。请尽可能展示您对在整个实现过程中学到的关键面向对象编程概念的理解。
在此任务中，考虑数据集中的特征，您将设计、实现、训练和评估用于预测以下类别的机器学习模型：
1.贫血
2.高血压
3.死亡
对于上述每个类别，您将使用机器学习算法实现和评估 3 个分类模型。在评估模型时，使用混淆矩阵、精确度、召回率、准确度等指标来呈现您的结果并解释其含义。通过比较已实施模型的性能来可视化结果（例如精确度、召回率、准确度等）至关重要。例如，对于 d (1-3) 中的每个类，您将比较三个模型中每个模型的性能，并解释/证明为什么某些算法比其他算法表现更好或更差。]]></description>
      <guid>https://stackoverflow.com/questions/77904188/d-predictions-and-classifications-health-care</guid>
      <pubDate>Tue, 30 Jan 2024 06:03:12 GMT</pubDate>
    </item>
    <item>
      <title>具有多个输入和多个输出的 RNN</title>
      <link>https://stackoverflow.com/questions/77903939/rnn-with-multiple-inputs-and-multiple-outputs</link>
      <description><![CDATA[我是 RNN 新手，我想实现一个具有多个输入和输出的 RNN。我特别想实现如下图所示的功能。也就是说，在我处理完输入后，我想开始生成输出，是否可以在 pytorch RNN 模块中轻松实现这一点？
]]></description>
      <guid>https://stackoverflow.com/questions/77903939/rnn-with-multiple-inputs-and-multiple-outputs</guid>
      <pubDate>Tue, 30 Jan 2024 04:43:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit-learn 的 HalvingGridSearchCV 时未获得预期的 min_resources_</title>
      <link>https://stackoverflow.com/questions/77903538/not-getting-expected-min-resources-when-using-scikit-learns-halvinggridsearchc</link>
      <description><![CDATA[我正在尝试使用 scikit-learn 的 HalvingGridSearchCV 类调整模型超参数，但它使用的迭代对我来说似乎不正确。我使用默认的 min_resources=“exhaust”，因为我希望最后一次迭代使用整个训练集或尽可能多。就我而言，当我使用减半因子 2 测试超过 8 个超参数时，我会遇到两个问题之一。
第一个是，在某些情况下，它仅针对一个候选超参数集运行最后一次迭代。一些东西 文档指出它不应该这样做：
&lt;块引用&gt;
该过程在第一次迭代时停止，该迭代评估因子 = 2 个候选者：最佳候选者是这 2 个候选者中最好的。没有必要运行额外的迭代

生成了超过 1000 个样本，在 8 个可能的超参数组合的网格上运行因子为 2 的减半搜索，我预计这将在 n_candidates_=[8, 4, 2] 的 3 次迭代中完成 与 n_resources_=[250, 500, 1000]。然而，我得到的是 n_candidates_=[8, 4, 2, 1] 和 n_resources_=[125, 250, 500, 1000] ，其中第一次迭代是用低于预期的 min_resources 并且最后一个是不必要的。
当我对 100 个生成的样本运行相同的搜索时，而不是我预期的使用 n_resources_=[25, 50 进行 3 次 n_candidates_=[8, 4, 2] 迭代时，会出现另一个问题, 100]，我得到 n_candidates_=[8, 4, 2] 和 n_resources_=[20, 40, 80]。这里的迭代次数似乎是正确的，但初始的 n_resources_ 太低，并且在最后一次迭代中没有达到 min_resources=“exhaust” 应该达到的 100。 
这些问题似乎与数据相关，当我尝试对 Fashion-MNIST 数据集的子集运行小型测试时，我首先注意到迭代并不是我所期望的。在这种情况下，对 200 个样本运行相同的超参数搜索会得到 n_candidates_=[8, 4] 和 n_resources_=[100, 200]，在生成的样本上我得到 n_candidates_=[8, 4, 2, 1] 和 n_resources_=[25, 50, 100, 200]。两者都不是预期的 n_candidates_=[8, 4, 2] 和 n_resources_=[50, 100, 200]。
当我使用减半因子 3 或 4 时，我得到了预期的结果，我只看到了 2 的问题。
我不确定我是否误解了连续减半的预期行为，或者我是否以某种方式错误地使用了它。或者，如果这只是 scikit-learn 中的一个错误，我正在使用该库的 1.4.0 版本。
以下代码片段将生成 n_candidates_=[8, 4, 2, 1] 和 n_resources_=[125, 250, 500, 1000]，而不是n_candidates_=[8, 4, 2] 和 n_resources_=[250, 500, 1000] 我期望。还可以通过更改 num_samples 或取消注释 Fashion-MNIST 代码并注释掉 make_classification() 行来更改它以获得我突出显示的其他情况。
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.datasets 导入 fetch_openml、make_classification
从sklearn.experimental导入enable_halving_search_cv
从 sklearn.model_selection 导入 train_test_split，HalvingGridSearchCV
从 sklearn.tree 导入 DecisionTreeClassifier

rng = np.random.RandomState(0)
num_samples = 1000 # 或 100 或 200

# 如果在 Fashion-MNIST 上测试，这将下载数据集
# data_fash = fetch_openml(name=&quot;Fashion-MNIST&quot;)
# X_train, _, y_train, _ = train_test_split(
# data_fash.data, data_fash.target, train_size=num_samples, random_state=rng
＃）

X_train, y_train = make_classification(n_samples=num_samples, n_features=20, random_state=rng)

print(&quot;训练样本：&quot;, len(X_train))

param_grid = {“标准”：[“基尼”，“熵”]，
              “min_samples_split”：[2,3,4,5]}

cls = 决策树分类器()

网格 = HalfingGridSearchCV(
           cls，param_grid，因子=2，min_resources=“耗尽”，评分=“准确度”，cv=5
           ）

grid.fit(X_train, y_train)

打印（网格.n_candidates_）
打印（网格.n_resources_）
]]></description>
      <guid>https://stackoverflow.com/questions/77903538/not-getting-expected-min-resources-when-using-scikit-learns-halvinggridsearchc</guid>
      <pubDate>Tue, 30 Jan 2024 01:59:14 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 ValueError：在管道中的 PolynomialFeatuers 之前使用 Imputer 时，输入 X 包含 NaN？</title>
      <link>https://stackoverflow.com/questions/77903530/how-to-resolve-valueerror-input-x-contains-nan-when-using-imputer-before-polyno</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77903530/how-to-resolve-valueerror-input-x-contains-nan-when-using-imputer-before-polyno</guid>
      <pubDate>Tue, 30 Jan 2024 01:57:11 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 模型仅输出 1 个单值</title>
      <link>https://stackoverflow.com/questions/77903104/lstm-models-only-output-1-single-value</link>
      <description><![CDATA[我正在使用 LSTM 来研究时间序列，并尝试了一些变体，但我没有运气让模型为我提供与单个值不同的输出值。
这是我之前的数据：
临时订单检查：
总行数 = 10410 &amp;列车索引 = 7287 &amp;价值指数 = 8848
时间顺序正确：训练 &lt;验证&lt;测试
前向验证的数据分割完成：
训练集形状：(7287, 57) &amp;验证集形状：(1561, 57) &amp;测试集形状：(1562, 57)

重塑的数据形状对于训练、测试和验证集来说是正确的。
火车改造：(7280, 7, 38)
测试重塑：(1555,7,38)
瓦尔重塑：(1554, 7, 38)

修剪形状：
X_train 修剪：(7280, 7, 38)，y_train 修剪：(7280,)
X_测试修剪：（1554,7,38），y_测试修剪：（1554,）
X_val 修剪：(1554, 7, 38)，y_val 修剪：(1554,)

模型类型：lstm
模型类型的 DataFrame 运行状况检查：lstm

最后一次检查之前训练的变量： X_train: (7280, 7, 38) y_train: (7280,) X_test: (1555, 7, 38) y_test: (1554,) X_val: (1554, 7, 38）y_val：（1554，）

最终形状：X_train：（7280,7,38），y_train：（7280,），X_test：（1554,7,38），y_test：（1554,），X_val：（1554,7,38），y_val：（第1554章
数据已准备好用于模型训练。

我的模型结构如下：
def create_model(params):

    #-------------------------GPU设置-------------------- -----------------
    # 计算全局batch size并相应调整学习率
    #------------------------------------------------- ------------------------
    策略 = tf.distribute.MirroredStrategy()
    num_gpus = Strategy.num_replicas_in_sync # 可用 GPU 数量
    # global_batch_size = params[&#39;batch_size&#39;] * num_gpus
    # params[&#39;learning_rate&#39;] *= num_gpus # 缩放学习率
    
    input_shape = (params[&#39;timesteps&#39;], params[&#39;n_features&#39;])
    print(&#39;设备数量：{}&#39;.format(strategy.num_replicas_in_sync))
    
    使用strategy.scope()：
    #-------------------------GPU设置-------------------- -----------------
    # 计算全局batch size并相应调整学习率
    #------------------------------------------------- ------------------------
        
        # 正则化参数
        l1_reg = 0.0001 # L1正则化因子 0.005 0.001 0.01 0.02
        l2_reg = 0.0001 # L2正则化因子

        模型=顺序（）

        # LSTM层
        model.add(LSTM(64, input_shape=input_shape, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))

        模型.add(Dropout(0.2))

        # 用于特征学习的密集层
        model.add(密集(32, 激活=&#39;relu&#39;, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))

        模型.add(Dropout(0.2))
        
        model.add(密集(16, 激活=&#39;relu&#39;, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))

        # 最终输出层
        model.add（密集（1，激活=&#39;线性&#39;））`

训练后并尝试在这里预测我的测试：
警告：测试预测是恒定的。模型可能无法有效学习。
警告：验证预测是恒定的。模型可能无法有效学习。
-------------------------------------------------- -
样本测试预测：[0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942
 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942
 0.057942 0.057942 0.057942 0.057942]
样本验证预测：[0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942
 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942 0.057942
 0.057942 0.057942 0.057942 0.057942]

在输出中具有单个密集值，我希望预测下一个值，但它始终给出一个值。]]></description>
      <guid>https://stackoverflow.com/questions/77903104/lstm-models-only-output-1-single-value</guid>
      <pubDate>Mon, 29 Jan 2024 23:06:11 GMT</pubDate>
    </item>
    <item>
      <title>如何在 google colab 中使用更多 GPU RAM？</title>
      <link>https://stackoverflow.com/questions/77893929/how-do-i-use-more-of-the-gpu-ram-in-google-colab</link>
      <description><![CDATA[我正在 pytorch 中从事这个深度学习项目，其中我有 2 个完全连接的神经网络，我需要训练然后测试它们。但是当我在 google colab 中运行代码时，它并不比在我的 PC 上的 CPU 上运行快多少。顺便说一句，我有 colab pro。它还使用 A100 GPU 40GB GPU RAM 中的 0.6 个。
导入火炬
导入火炬视觉
导入 torchvision.transforms 作为变换
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim


设备 = torch.device(“cuda:0”)
# 定义变换
变换 = 变换.Compose([
    变换.ToTensor(),
    变换.Normalize((0.5,),(0.5,))
]）

# 加载 FashionMNIST 数据集
trainset = torchvision.datasets.FashionMNIST（&#39;./data&#39;，download=True，train=True，transform=transform）
测试集 = torchvision.datasets.FashionMNIST(&#39;./data&#39;, download=True, train=False, transform=transform)

# 创建数据加载器
trainloader = torch.utils.data.DataLoader(trainset,batch_size=1,shuffle=True,num_workers=2)
testloader = torch.utils.data.DataLoader(testset,batch_size=1,shuffle=False,num_workers=2)

# 为类定义常量
类 = (&#39;T 恤/上衣&#39;, &#39;裤子&#39;, &#39;套头衫&#39;, &#39;连衣裙&#39;, &#39;外套&#39;,
           “凉鞋”、“衬衫”、“运动鞋”、“包”、“踝靴”）




# 定义全连接神经网络
FCNN 类（nn.Module）：
    def __init__(自身, num_layers=1):
        超级（FCNN，自我）.__init__()
        self.num_layers = num_layers
        self.fc_layers = nn.ModuleList()
        如果 self.num_layers == 1:
            self.fc_layers.append(nn.Linear(28 * 28, 1024))
        elif self.num_layers == 2：
            self.fc_layers.append(nn.Linear(28 * 28, 1024))
            self.fc_layers.append(nn.Linear(1024, 1024))
        self.output_layer = nn.Linear(1024, 10)

    def 前向（自身，x）：
        x = x.view(-1, 28 * 28)
        对于 self.fc_layers 中的层：
            x = nn.function.relu(层(x))
        x = self.output_layer(x)
        返回x

# 修改train函数以将输入和标签移动到GPU
def train(网络, 标准, 优化器, epochs=15):
    对于范围内的纪元（纪元）：
        运行损失 = 0.0
        对于 i，enumerate(trainloader, 0) 中的数据：
            输入，标签=数据[0].to（设备），数据[1].to（设备）
            优化器.zero_grad()

            输出 = 净值（输入）
            损失=标准（输出，标签）
            loss.backward()
            优化器.step()

            running_loss += loss.item()
            如果我% 2000 == 1999：
                print(&#39;[%d, %5d] 损失: %.2f&#39; %
                      (epoch + 1, i + 1, running_loss / 2000))
                运行损失 = 0.0

# 定义函数来测试准确性
定义测试（净）：
    正确 = 0
    总计 = 0
    使用 torch.no_grad()：
        对于测试加载器中的数据：
            图像、标签=数据
            输出=净（图像）
            _, 预测 = torch.max(outputs.data, 1)
            总计 += labels.size(0)
            正确+=（预测==标签）.sum().item()

    print(&#39;准确率：%d %%&#39; % (
            100 * 正确/总计))

＃ 主功能
如果 __name__ == “__main__”：
    # 定义网络
    net1 = FCNN(num_layers=1)
    net2 = FCNN(num_layers=2)
    net2.to（设备）

    # 定义损失函数和优化器
    标准 = nn.CrossEntropyLoss()
    优化器1 = optim.SGD(net1.parameters(), lr=0.001, 动量=0.0)
    optimer2 = optim.SGD(net2.parameters(), lr=0.001, 动量=0.0)

    # 使用 1 个 FC 层训练和测试网络
    #print(“训练网络1层...”)
    #train(net1, 标准, 优化器1)
    #测试（网络1）

    # 使用 2 个 FC 层训练和测试网络
    print(&quot;2层训练网络...&quot;)
    训练（net2、标准、优化器2）
    测试（网络2）

尝试在 Google Colab 中使用不同的 GPU
尝试添加此行以始终使用 CUDA 核心：
device = torch.device(“cuda:0”)

并让网络使用该设备：
device = torch.device(“cuda:0”)
]]></description>
      <guid>https://stackoverflow.com/questions/77893929/how-do-i-use-more-of-the-gpu-ram-in-google-colab</guid>
      <pubDate>Sun, 28 Jan 2024 07:11:15 GMT</pubDate>
    </item>
    <item>
      <title>运行由 resipy 库组成的代码时出现错误 - from resipy import R2</title>
      <link>https://stackoverflow.com/questions/77893785/getting-error-while-running-the-code-which-consists-of-resipy-library-from-res</link>
      <description><![CDATA[无法导入 meshCalc 扩展，请参阅以下错误：
无法从“resipy.cext”（未知位置）导入名称“meshCalc”
-------------------------------------------------- ------------------------
ImportError Traceback（最近一次调用最后一次）
文件〜\ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ resipy \ meshTools.py：38
     37 尝试：
---&gt; 38 从 resipy.cext 导入 meshCalc 作为 mc
     39 除了异常 e：

ImportError：无法从“resipy.cext”（未知位置）导入名称“meshCalc”

在处理上述异常的过程中，又出现了一个异常：

异常回溯（最近一次调用最后一次）
[2] 中的单元格，第 9 行
      7 导入操作系统
      8 导入时间
----&gt; 9 从 resipy 导入 R2
     11 模型运行次数 = 1
     12 tic = 时间.time()

文件〜\ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ resipy \ __ init __.py：2
      1 名称 =“resipy”
----&gt; 2 从resipy.Project导入ResIPy_version、sysinfo
      3.从resipy.Project导入项目，R2
      4.从resipy.Surve导入Survey

文件〜\ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ resipy \ Project.py：40
     38 从 resipy.parsers 导入 geomParser
     39 从 resipy.r2in 导入 write2in
---&gt; 40 导入 resipy.meshTools 作为 mt
     41 从resipy.meshTools导入cropSurface
     42 从 resipy.template 导入 startAnmt、endAnmt

文件〜\ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ resipy \ meshTools.py：42
     40 print(&#39;无法导入 meshCalc 扩展，请参阅以下错误：&#39;)
     41 print(e)# 需要编译meshCalc
---&gt; 42 raise Exception(&#39;无法导入 meshCalc 扩展来解决问题尝试，&#39;\
     43&#39;更新 ResIPy、更新 Numpy 或重新编译扩展。&#39;)
     45 # 导入 pyvista（如果可用）
     46 尝试：

异常：无法导入 meshCalc 扩展来解决问题，请尝试更新 ResIPy、更新 Numpy 或重新编译扩展。

我收到此错误。

我正在尝试运行由 (from resipy import R2) 行组成的代码。我正在尝试执行，但以下软件包出现错误。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77893785/getting-error-while-running-the-code-which-consists-of-resipy-library-from-res</guid>
      <pubDate>Sun, 28 Jan 2024 06:02:30 GMT</pubDate>
    </item>
    <item>
      <title>对 MLH 奖学金代码示例有什么建议吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77890706/any-suggestions-for-mlh-fellowship-code-sample</link>
      <description><![CDATA[我计划今年申请 MLH 奖学金，我想展示一个强大的代码示例来展示我的全栈开发技能。我正在考虑的项目是任务管理器 Web 应用程序。该应用程序将包括用户身份验证、任务创建/编辑/删除、数据库集成、响应式用户界面和部署等功能。
我希望获得有关如何有效实施该项目的建议，以及能够给评选委员会留下深刻印象的任何具体技术或框架。此外，我们将非常感谢有关确保代码质量、安全实践以及任何可以使我的项目脱颖而出的额外功能的指导。
我的目标不仅仅是满足奖学金的要求，而是提供一个全面且精心制作的代码示例，反映我作为全栈开发人员的能力。有什么见解、技巧或建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77890706/any-suggestions-for-mlh-fellowship-code-sample</guid>
      <pubDate>Sat, 27 Jan 2024 09:45:26 GMT</pubDate>
    </item>
    <item>
      <title>安装斗争[重复]</title>
      <link>https://stackoverflow.com/questions/77889759/installation-struggle</link>
      <description><![CDATA[我们正在使用 Qiskit 工具包进行一个量子计算项目。但我们在导入或安装软件包和库时遇到了困难。在 Qiskit 中我们如何导入库和包？
澄清如何从外包安装库的疑问。]]></description>
      <guid>https://stackoverflow.com/questions/77889759/installation-struggle</guid>
      <pubDate>Sat, 27 Jan 2024 01:36:23 GMT</pubDate>
    </item>
    <item>
      <title>Sagemaker实例中的CUDA路径解决NameError：名称'_C'未使用GroundingDINO定义</title>
      <link>https://stackoverflow.com/questions/77888418/cuda-path-in-sagemaker-instances-to-solve-nameerror-name-c-is-not-defined-wi</link>
      <description><![CDATA[我正在尝试在 Sagemaker 实例（使用 GPU）中安装和使用 grounding dino ）但我收到错误：
NameError：名称“_C”未定义

我发现原因是因为变量CUDA_HOME没有配置，所以要解决这个问题我需要设置该变量，但在搜索答案后我找不到cuda在sagemaker实例中安装的路径。
cuda 安装在 sagemaker 实例中的什么位置以便我可以设置 CUDA_HOME？]]></description>
      <guid>https://stackoverflow.com/questions/77888418/cuda-path-in-sagemaker-instances-to-solve-nameerror-name-c-is-not-defined-wi</guid>
      <pubDate>Fri, 26 Jan 2024 18:45:06 GMT</pubDate>
    </item>
    </channel>
</rss>