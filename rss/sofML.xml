<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 03 Jan 2025 09:17:11 GMT</lastBuildDate>
    <item>
      <title>虚拟变量为布尔值而非整数</title>
      <link>https://stackoverflow.com/questions/79325633/dummy-variable-as-boolean-rather-than-integer</link>
      <description><![CDATA[我正在用 Python 开发一个机器学习项目（我的第一个项目）。我尝试使用 pandas(pd.get_dummies) 为数据中的分类列创建虚拟变量，但这些变量被转换为布尔值而不是整数，这使得 statsmodels 无法拟合 OLS 模型。我尝试将布尔值转换为整数，但一直出现错误。我该如何解决这个问题？
我使用了 .astype(int) 方法，还尝试使用 numpy 转换为整数]]></description>
      <guid>https://stackoverflow.com/questions/79325633/dummy-variable-as-boolean-rather-than-integer</guid>
      <pubDate>Fri, 03 Jan 2025 06:15:51 GMT</pubDate>
    </item>
    <item>
      <title>检测页面中的图像[关闭]</title>
      <link>https://stackoverflow.com/questions/79324425/detect-image-in-a-page</link>
      <description><![CDATA[我正在构建一个项目，我需要知道文档中哪些页面包含图像。我不想为此使用基于 llm 的视觉模型。请给我一些解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/79324425/detect-image-in-a-page</guid>
      <pubDate>Thu, 02 Jan 2025 16:48:36 GMT</pubDate>
    </item>
    <item>
      <title>WCSS 不会持续下降</title>
      <link>https://stackoverflow.com/questions/79324000/wcss-not-decreasing-constantly</link>
      <description><![CDATA[您好，我的 k-means 算法在此数据集上存在问题：
https://www.kaggle.com/datasets/youssefaboelwafa/clustering-penguins-species/data
我尝试从头开始实现它。似乎一切都正常，但当使用肘部法找到最佳 k 时，我得到了错误的结果：
k = 1 到 10 的第一个示例

k = 1 到 10 的第二个示例

但从数学上讲，当聚类质心收敛时，WCSS 应该总是减少，对吗？如您所见，在我实现的版本中并没有发生这种情况。总是有小的“颠簸”。
我尝试将我的结果与 sklearn 的结果进行比较：
k = 1 到 10 的第一个示例 (SKLEARN)

k = 1 到 10 的第二个示例 (SKLEARN)

如您所见，WCSS 不断减少。不过，WCSS 值要高得多。我不知道为什么。
这是我的笔记本：
https://github.com/Orivex/Cluster-Penguin-Species/blob/master/KMeansClustering.ipynb]]></description>
      <guid>https://stackoverflow.com/questions/79324000/wcss-not-decreasing-constantly</guid>
      <pubDate>Thu, 02 Jan 2025 14:23:01 GMT</pubDate>
    </item>
    <item>
      <title>请求合作伙伴/开发人员完成复杂的乐透预测算法 [关闭]</title>
      <link>https://stackoverflow.com/questions/79323951/request-for-partner-developer-to-finish-complex-lotto-prediction-algorithm</link>
      <description><![CDATA[各位开发者大家好，
我正在寻找合作伙伴或开发者同事，共同完成一项旨在对我国彩票系统进行逆向工程的算法。在发现彩票是伪随机生成的之后，我感到有必要开发一种算法来揭示其隐藏的模式。
我取得了重大进展，包括可视化数据、执行相关性分析、特征工程，甚至尝试机器学习技术。然而，我遇到了障碍，我相信，有了正确的合作，我们可以破解密码，完成我们已经开始的工作。
当前进展：
数据已经整理和清理。
我已经实现了 80% 的距离相关性，通过进一步调整，我将其提高到了 82%。
我的线性相关性也达到了 84%。
挑战：尽管取得了进步，但到目前为止，我应用的机器学习模型还没有提供我想要的结果，我正处于需要新见解或新方法的阶段。
我正在寻找：
在数据科学、统计分析和机器学习方面具有专业知识的合作伙伴。
愿意接受高潜在回报挑战的人。
具有逆向工程、预测模型或乐透系统是一个加分项，但不是必需的。
如果您愿意接受挑战并相信我们可以成功，那么让我们一起合作并共同完成这个项目。如果我们成功了，回报可能是巨大的！
在我的最终预测模型中，我试图获得小于 10 的 mae，但我得到的是超过 100 万的 mae]]></description>
      <guid>https://stackoverflow.com/questions/79323951/request-for-partner-developer-to-finish-complex-lotto-prediction-algorithm</guid>
      <pubDate>Thu, 02 Jan 2025 14:05:47 GMT</pubDate>
    </item>
    <item>
      <title>LSTM模型预测不会随着输入的不同而改变</title>
      <link>https://stackoverflow.com/questions/79323808/lstm-model-prediction-does-not-change-with-different-inputs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79323808/lstm-model-prediction-does-not-change-with-different-inputs</guid>
      <pubDate>Thu, 02 Jan 2025 13:06:55 GMT</pubDate>
    </item>
    <item>
      <title>将 onnx 文件编译为 hailo 可执行文件 (.hef) 时出现问题</title>
      <link>https://stackoverflow.com/questions/79323698/trouble-compiling-onnx-file-to-hailo-executable-hef</link>
      <description><![CDATA[按照 Cytron 网站 上的转换教程并运行命令后
hailomz compile yolov5s --ckpt=best.onnx --hw-arch hailo8l --calib-path yolov5/train/images --classes 1 --performance
我收到错误消息：ModuleNotFoundError：没有名为“hailo_platform”的模块。
我已完成的操作：

我按照教程复制了每个命令（仅将 hailoDFC 版本号中的 28 更改为 29）
已安装 Hailo model zoo 和 hailo dataflow 编译器
我已检查安装是否正确更正并以不同的顺序多次重做
我运行了 hailomz 和 hailo -h 并获得了成功的结果
我已经在干净的 Ubuntu 22.04 WSL 上完成了安装
训练数据的路径是正确的 yolov5/train/images
仅运行不带参数的 hailomz compile 会产生相同的结果
我已经安装了 HailoRT（因为 hailo_platform 是其中的一部分）并运行了相同的命令，错误消息不同：
ImportError：libhailort.so.4.19.0：无法打开共享对象文件：没有这样的文件或目录。

WSL 上的 Ubuntu 22.04
Python 2012年10月3日
HailoDFC 3.29.0
HailoMZ v2.14]]></description>
      <guid>https://stackoverflow.com/questions/79323698/trouble-compiling-onnx-file-to-hailo-executable-hef</guid>
      <pubDate>Thu, 02 Jan 2025 12:27:26 GMT</pubDate>
    </item>
    <item>
      <title>线性回归模型勉强优化了截距b</title>
      <link>https://stackoverflow.com/questions/79312660/linear-regression-model-barely-optimizes-the-intercept-b</link>
      <description><![CDATA[我从头开始编写了一个线性回归模型。我使用“残差平方和”作为梯度下降的损失函数。为了进行测试，我使用线性数据 (y=x)
运行算法时，截距 b 几乎没有变化。因此斜率 m 计算不正确。
%matplotlib qt5 
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

X = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
y = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12345)

class LinearRegression():
def __init__(self):
self.X = None
self.y = None

def ssr(self, m, b):
sum = 0
for i in range(len(self.X)):
sum += (self.y[i] - (m * self.X[i] + b) ) ** 2

return sum

def ssr_gradient(self, m, b):
sum_m = 0
sum_b = 0
n = len(self.X)
for i in range(n):
error = self.y[i] - (m * self.X[i] + b)
derivative_m = -(2/n) * self.X[i] * error # 相对于 m 的导数
derivative_b = -(2/n) * error # 相对于 m 的导数b
sum_m += derived_m
sum_b += derived_b

return sum_m, sum_b

def fit(self, X, y, m, b): # 梯度下降
self.X = X
self.y = y

M, B = np.meshgrid(np.arange(-10, 10, 0.1), np.arange(-10, 10, 0.1))
SSR = np.zeros_like(M)
for i in range(M.shape[0]):
for j in range(M.shape[1]):
SSR[i, j] = self.ssr(M[i, j], B[i, j])

fig, axis = plt.subplots(1, 2, figsize=(12, 6))
gd_model = fig.add_subplot(121,投影=“3d”，computed_zorder=False)
lin_reg_model = axis[1] 

current_pos = (m, b, self.ssr(m, b))
learning_rate = 0.001
min_step_size = 0.001
max_steps = 1000
current_steps = 0

while(current_steps &lt; max_steps):
M_derivative, B_derivative = self.ssr_gradient(current_pos[0], current_pos[1])
M_step_size, B_step_size = M_derivative * learning_rate, B_derivative * learning_rate

if abs(M_step_size) &lt; min_step_size 或 abs(B_step_size) &lt; min_step_size:
break

M_new, B_new = current_pos[0] - M_step_size, current_pos[1] - B_step_size

current_pos = (M_new, B_new, self.ssr(M_new, B_new))

print(f&quot;参数：m：{current_pos[0]}; b：{current_pos[1]}; SSR：{current_pos[2]}&quot;)

current_steps += 1

x = np.arange(0, 10, 1)
y = current_pos[0] * x + current_pos[1]
lin_reg_model.scatter(X_train, y_train, label=&quot;Train&quot;, s=75, c=&quot;#1f77b4&quot;)
lin_reg_model.plot(x, y)

gd_model.plot_surface(M, B, SSR, cmap=&quot;viridis&quot;, zorder=0)
gd_model.scatter(current_pos[0], current_pos[1], current_pos[2], c=&quot;red&quot;, zorder=1)
gd_model.set_xlabel(&quot;斜率 m&quot;)
gd_model.set_ylabel(&quot;截距 b&quot;)
gd_model.set_zlabel(&quot;残差平方和&quot;)

plt.tight_layout()
plt.pause(0.001)

gd_model.clear()
lin_reg_model.clear()

self.m = current_pos[0]
self.b = current_pos[1]

def predict(self, X_test):
return self.m * X_test + self.b

lin_reg_model = LinearRegression()
lin_reg_model.fit(X_train, y_train, 1, 10)


这是初始值 m=1 和 b=10 的结果：
参数：m：-0.45129949840919587；b：9.50972664859535；SSR：145.06534359577407

显然这不是最佳的，因为我的数据是线性的。因此最佳参数应该是 m=1 和 b=0
但我在代码中找不到问题。该算法根据初始值打印不同的结果，但只要 SSR 函数恰好有一个最小值，它就应该一遍又一遍地打印相同的结果。
我尝试使用不同的学习率，但问题仍然存在。]]></description>
      <guid>https://stackoverflow.com/questions/79312660/linear-regression-model-barely-optimizes-the-intercept-b</guid>
      <pubDate>Fri, 27 Dec 2024 19:40:21 GMT</pubDate>
    </item>
    <item>
      <title>如何对具有大量类别的商品特征进行编码以进行推荐</title>
      <link>https://stackoverflow.com/questions/79270683/how-to-encode-item-features-with-high-number-of-categories-for-recommendation</link>
      <description><![CDATA[对于我正在研究的推荐问题，有大约 50000 个独特品牌和 3 级产品类别，level_1_cat（50 个类别）、level_2_cat（100 个类别）和 level_3_cat（1000 个类别）。所有这些项目特征仅由整数表示。到目前为止，我已经为我的 lightfm 模型尝试了二进制编码、标签编码和目标编码。使用二进制编码和标签编码，结果比不使用任何项目特征更差。使用目标编码，结果与不使用任何项目特征相似。我想知道我还能尝试什么。]]></description>
      <guid>https://stackoverflow.com/questions/79270683/how-to-encode-item-features-with-high-number-of-categories-for-recommendation</guid>
      <pubDate>Wed, 11 Dec 2024 06:39:00 GMT</pubDate>
    </item>
    <item>
      <title>时间序列预测的最佳算法？[关闭]</title>
      <link>https://stackoverflow.com/questions/64544725/best-algorithm-for-time-series-prediction</link>
      <description><![CDATA[我必须每天预测某个区域的总需水量，并根据包含以下内容的 4 个 CVS 文件创建一个模型：

汇总形式的需水量（每日粒度的时间序列，2 年数据）
进入该区域蓄水池的水量（每日粒度的时间序列，2 年数据）
离开该区域蓄水池的水量（每日粒度的时间序列，2 年数据）
来自该区域 4,000 个测量点的用水请求（每日粒度的时间序列，2 年数据）。

您认为，使用现有数据和特征，哪个模型可以很好地预测该区域的需水量？我只能想到 LSTM 或 MLP，我不知道像 ARIMA 或 (SARIMA) 这样的东西在这种情况下是否有用，因为我有很多功能但时间不多。]]></description>
      <guid>https://stackoverflow.com/questions/64544725/best-algorithm-for-time-series-prediction</guid>
      <pubDate>Mon, 26 Oct 2020 20:45:57 GMT</pubDate>
    </item>
    <item>
      <title>K 近邻分类器 - 训练测试分割的随机状态导致不同的准确度分数</title>
      <link>https://stackoverflow.com/questions/63410524/k-nearest-neighbour-classifier-random-state-for-train-test-split-leads-to-diff</link>
      <description><![CDATA[我一直在 python 的 sklearn 模块中对乳腺癌数据集进行一些 KNN 分类分析。我有以下代码，它尝试找到目标变量分类的最佳 k。
来自 sklearn.datasets 导入 load_breast_cancer
来自 sklearn.model_selection 导入 train_test_split
来自 sklearn.neighbors 导入 KNeighborsClassifier
导入 matplotlib.pyplot 作为 plt

breast_cancer_data = load_breast_cancer()

training_data, validation_data, training_labels, validation_labels = train_test_split(breast_cancer_data.data, breast_cancer_data.target, test_size = 0.2, random_state = 40)
results = []

for k in range(1,101):
classifier = KNeighborsClassifier(n_neighbors = k)
classifier.fit(training_data, training_labels)
results.append(classifier.score(validation_data, validation_labels))

k_list = range(1,101)
plt.plot(k_list, results)
plt.ylim(0.85,0.99)
plt.xlabel(&quot;k&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.title(&quot;Breast Cancer Classifier Accuracy&quot;)
plt.show()

代码循环遍历 1 到 100，并生成 100 个 KNN 模型，其中“k”设置为 1 到 100 范围内的增量值。每个模型的性能都保存到一个列表中，并生成一个图，在 x 轴上显示“k”，在 y 轴上显示模型性能。
我遇到的问题是，当我在将数据拆分为训练和测试分区时更改 random_state 参数时，导致完全不同的图，表明不同数据集分区的不同“k”值具有​​不同的模型性能。
对我来说，这使得很难决定哪个“k”是最佳的，因为算法对使用不同随机状态的不同“k”执行不同。这当然并不意味着对于这个特定的数据集，“k”是任意的？有人可以解释一下吗？


]]></description>
      <guid>https://stackoverflow.com/questions/63410524/k-nearest-neighbour-classifier-random-state-for-train-test-split-leads-to-diff</guid>
      <pubDate>Fri, 14 Aug 2020 09:51:57 GMT</pubDate>
    </item>
    <item>
      <title>如何计算神经网络预测的置信度分数</title>
      <link>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</link>
      <description><![CDATA[我正在使用深度神经网络模型（在 keras 中实现）进行预测。类似这样的内容：
def make_model():
model = Sequential() 
model.add(Conv2D(20,(5,5),activation = &quot;relu&quot;))
model.add(MaxPooling2D(pool_size=(2,2))) 
model.add(Flatten())
model.add(Dense(20,activation = &quot;relu&quot;))
model.add(Lambda(lambda x: tf.expand_dims(x, axis=1)))
model.add(SimpleRNN(50,activation=&quot;relu&quot;))
model.add(Dense(1,activation=&quot;sigmoid&quot;)) 
model.compile(loss = &quot;binary_crossentropy&quot;,optimizer = adagrad,metrics = [&quot;accuracy&quot;])

返回模型

model = make_model()
model.fit(x_train,y_train,validation_data = (x_validation,y_validation), epochs = 25, batch_size = 25, verbose = 1)

##预测：
prediction = model.predict_classes(x)
probabilities = model.predict_proba(x) #我假设这些是被预测的类的概率

我的问题是分类（二元）问题。我希望计算每个预测的置信度分数，即我想知道 - 我的模型是否 99% 确定它是“0”，或者 58% 确定它是“0”。
我找到了一些关于如何做到这一点的观点，但无法实现它们。我希望遵循的方法是：“使用分类器，当你输出时，你可以将值解释为属于每个特定类别的概率。你可以使用它们的分布作为你对观察结果属于该类别的信心的粗略衡量标准。”
我应该如何使用类似上述模型的东西进行预测，以便获得对每个预测的信心？我希望有一些实际的例子（最好是在 Keras 中）。]]></description>
      <guid>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</guid>
      <pubDate>Wed, 22 Jan 2020 02:52:32 GMT</pubDate>
    </item>
    <item>
      <title>我必须使用哪种机器学习算法进行序列预测？</title>
      <link>https://stackoverflow.com/questions/59543222/which-machine-learning-algorithm-i-have-to-use-for-sequence-prediction</link>
      <description><![CDATA[我有一个如下所示的数据集。我有 datetime 列作为索引，type 是带有序列的列。例如；R、C、D、D、D、R、R 是一个序列。
start_time type

2019-12-14 09:00:00 RCDDDRR 
2019-12-14 10:00:00 CCRD 
2019-12-14 11:00:00 DDRRCC 
2019-12-14 12:00:00 ? 

我想预测 12:00:00 时的下一个序列是什么？哪种算法是预测下一个序列的最佳算法？
我知道我们可以使用马尔可夫链来预测可能的序列。但是，还有其他更好的算法吗？]]></description>
      <guid>https://stackoverflow.com/questions/59543222/which-machine-learning-algorithm-i-have-to-use-for-sequence-prediction</guid>
      <pubDate>Tue, 31 Dec 2019 11:03:17 GMT</pubDate>
    </item>
    <item>
      <title>提高预测算法的准确性</title>
      <link>https://stackoverflow.com/questions/58399563/improving-accuracy-of-prediction-algorithm</link>
      <description><![CDATA[我正在研究预测问题。这是一个包含几列数据的 Excel 电子表格：
https://drive.google.com/file/d/1fWf6dX8kOCRB3GpX42AF6UvTmd0g9zXp/view?usp=sharing
我试图根据 A 列到 E 列的值预测 F 列的值。代码如下
import numpy as np
import pandas as pn
from keras.layers import Dense, Activation
from keras.models import Sequential
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import linear_model
import matplotlib.pyplot as plt
dataset = pn.read_excel(r&quot;G:\Machine learning\data\database.xlsx&quot;, &quot;Sheet5&quot;)
dataset.columns = [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;,&#39;F&#39;]

print (dataset)
#check= dataset.iloc[0:,3 :13]
X = dataset.iloc[0:,0 :5]
print(X)
Y = dataset.iloc[0:, 5 :6]

print(Y)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 0)
print(X_test)
print(Y_test)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
#
model = Sequential()
##
### 添加输入层和第一个隐藏层
model.add(Dense(32,activation =&#39;relu&#39;,input_dim = 5, kernel_initializer=&#39;normal&#39;))
##
### 添加第二个隐藏层
model.add(Dense(units = 16,activation =&#39;relu&#39;))
model.add(Dense(units = 64,activation = &#39;relu&#39;))
model.add(Dense(units = 8,activation = &#39;relu&#39;))
model.add(Dense(units = 16,activation = &#39;relu&#39;))
#model.add(Dense(units = 8,activation = &#39;linear&#39;))
###
#### 添加第三个隐藏层
#model.add(Dense(units = 16,activation = &#39;relu&#39;))
#model.add(Dense(units = 16,activation = &#39;relu&#39;))
#model.add(Dense(units = 16,activation = &#39;relu&#39;))
##
### 添加输出层
model.add(Dense(units = 1))
##
model.add(Dense(units = 1))
##
model.add(Dense(1))
### 编译 ANN
model.compile(optimizer = &#39;nadam&#39;,loss = &#39;mean_squared_error&#39;,metrics= [&#39;accuracy&#39;])
##
### 将 ANN 拟合到训练集
history = model.fit(X_train, Y_train, epochs=125, batch_size=5, verbose=1, validation_split=0.1)
##
y_pred = model.predict(X_test)
##

y_pred1 = model.predict(X_train)
print (y_pred1)
Y_test.reset_index(drop= True, inplace= True)
print (Y_train)
Y_train.reset_index(drop= True, inplace= True)
plt.plot(y_pred1)
plt.plot(Y_train)
plt.show()
plt.plot(y_pred)
plt.plot(Y_test)
plt.show()
print (y_pred)
print (Y_test)
plt.plot((Y_test-y_pred)*100/Y_test)
plt.show()

我从此代码中获得的拟合如下所示。
￼￼拟合
现在当我预测时，在某些情况下误差很大，如下所示
预测
有人可以指导我改进代码以获得更好的预测吗？
￼￼]]></description>
      <guid>https://stackoverflow.com/questions/58399563/improving-accuracy-of-prediction-algorithm</guid>
      <pubDate>Tue, 15 Oct 2019 17:05:58 GMT</pubDate>
    </item>
    <item>
      <title>如何发现数据集中的哪些特征具有预测作用？</title>
      <link>https://stackoverflow.com/questions/21971709/how-can-you-discover-what-features-in-a-dataset-are-predictive</link>
      <description><![CDATA[我正在为此处提供的数据集开发机器学习算法。
有 26 列数据。其中大部分毫无意义。我如何才能有效快速地确定哪些特征是有趣的 - 哪些特征可以告诉我给定的 URL 是短暂的还是常青的（这是数据集中的因变量）？是否有智能、编程式的 Scikit 学习方法来做到这一点，或者它只是将每个特征与从属特征（“标签”，第 26 列）进行图形化并查看有什么影响的情况？
肯定有比这更好的方法吗？
编辑：我找到了一些分类器的代码 - 我如何打印出这里赋予每个特征的权重？
import numpy as np
import matplotlib.pyplot as plt
from sklearn import metrics,preprocessing,cross_validation
from sklearn.feature_extraction.text import TfidfVectorizer
import sklearn.linear_model as lm
import pandas as p
loadData = lambda f: np.genfromtxt(open(f,&#39;r&#39;), delimiter=&#39; &#39;)

print &quot;loading data..&quot;
traindata = list(np.array(p.read_table(&#39;train.tsv&#39;))[:,2])
testdata = list(np.array(p.read_table(&#39;test.tsv&#39;))[:,2])
y = np.array(p.read_table(&#39;train.tsv&#39;))[:,-1]

tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents=&#39;unicode&#39;, 
analyzer=&#39;word&#39;,token_pattern=r&#39;\w{1,}&#39;,ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)

rd = lm.LogisticRegression(penalty=&#39;l2&#39;, dual=True, tol=0.0001, 
C=1, fit_intercept=True,intercept_scaling=1.0, 
class_weight=None,random_state=None)

X_all = traindata + testdata
lentrain = len(traindata)

打印“拟合管道”
tfv.fit(X_all)
打印“转换数据”
X_all = tfv.transform(X_all)

X = X_all[:lentrain]
X_test = X_all[lentrain:]

打印“20 倍 CV 得分：”，np.mean(cross_validation.cross_val_score(rd,X,y,cv=20,scoring=&#39;roc_auc&#39;))

打印“在完整数据上训练”
rd.fit(X,y)
pred = rd.predict_proba(X_test)[:,1]
testfile = p.read_csv(&#39;test.tsv&#39;, sep=&quot;\t&quot;, na_values=[&#39;?&#39;], index_col=1)
pred_df = p.DataFrame(pred, index=testfile.index, columns=[&#39;label&#39;])
pred_df.to_csv(&#39;benchmark.csv&#39;)
print &quot;提交文件已创建..&quot;
]]></description>
      <guid>https://stackoverflow.com/questions/21971709/how-can-you-discover-what-features-in-a-dataset-are-predictive</guid>
      <pubDate>Sun, 23 Feb 2014 17:28:49 GMT</pubDate>
    </item>
    <item>
      <title>单词预测算法</title>
      <link>https://stackoverflow.com/questions/18728290/word-prediction-algorithm</link>
      <description><![CDATA[考虑以下情况：

我们有一个可用的词典
我们输入了许多段落的单词，我希望能够根据此输入预测句子中的下一个单词。

假设我们有几个句子，例如“你好，我叫汤姆”、“他叫杰瑞”、“他去没有水的地方”。我们检查哈希表是否存在单词。如果不存在，我们为其分配一个唯一的 ID 并将其放入哈希表中。这样，就不用存储“链”了单词作为一串字符串，我们可以只拥有一个 uniqueID 列表。
上面，例如 (0, 1, 2, 3, 4)、(5, 2, 3, 6) 和 (7, 8, 9, 10, 3, 11, 12)。请注意，3 表示“是”，我们在发现新单词时添加了新的唯一 ID。假设我们得到一个句子“她的名字是”，这将是 (13, 2, 3)。我们想知道，在给定此上下文的情况下，下一个单词应该是什么。这是我想到的算法，但我认为它效率不高：

我们有 N 个链（观察到的句子）的列表，其中一个链可能是 ex。 3,6,2,7,8。
每个链的平均大小为 M，其中 M 是平均句子长度
我们得到一个大小为 S 的新链，例如 13、2、3，我们想知道最有可能的下一个单词是什么？

算法：

首先扫描整个链列表，查找包含完整 S 输入的链（本例中为 13、2、3）。由于我们必须扫描 N 个链，每个链长度为 M，并且每次比较 S 个字母，因此其复杂度为 O(NMS)。

如果我们的扫描中没有包含完整 S 的链，则通过删除最不重要的单词（即第一个单词，因此删除 13）进行下一次扫描。现在，扫描 (2,3)，如 1 中的 ...我们对最坏情况进行 S 次扫描（13、2、3，然后 2、3，然后 3，共 3 次扫描 = S）。因此，总复杂度为 O(S^2 * M * N)。
因此，如果我们有 100,000 条链，平均句子长度为 10 个单词，则我们需要 1,000,000*S^2 才能获得最佳单词。显然，N &gt;&gt; M，因为句子长度通常与观察到的句子数量不成比例，所以 M 可以是一个常数。然后，我们可以将复杂度降低到 O(S^2 * N)。不过，O(S^2 * M * N) 可能更有助于分析，因为 M 可以是一个相当大的“常数”。
对于此类问题，这可能是完全错误的方法，但我想分享我的想法，而不是公然寻求帮助。我之所以这样扫描，是因为我只想扫描我必须扫描的内容。如果没有完整的 S，就继续修剪 S，直到某些链匹配。如果它们永远不匹配，我们就不知道下一个单词该预测什么！有没有关于时间/空间复杂度较低的解决方案的建议？]]></description>
      <guid>https://stackoverflow.com/questions/18728290/word-prediction-algorithm</guid>
      <pubDate>Tue, 10 Sep 2013 20:35:11 GMT</pubDate>
    </item>
    </channel>
</rss>