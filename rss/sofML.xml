<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 19 Mar 2024 00:57:16 GMT</lastBuildDate>
    <item>
      <title>分割接触谷物的最快方法：ML 模型还是传统图像处理？</title>
      <link>https://stackoverflow.com/questions/78183577/fastest-approach-for-segmenting-touching-cereal-grains-ml-model-or-traditional</link>
      <description><![CDATA[对于分割接触谷物的图像，最快的方法是什么，训练机器学习模型还是使用传统图像处理工具开发算法？
我想分别获取每个颗粒的图像。
示例图像如下：
米粒]]></description>
      <guid>https://stackoverflow.com/questions/78183577/fastest-approach-for-segmenting-touching-cereal-grains-ml-model-or-traditional</guid>
      <pubDate>Mon, 18 Mar 2024 23:50:14 GMT</pubDate>
    </item>
    <item>
      <title>java中异常检测的最佳算法或库[关闭]</title>
      <link>https://stackoverflow.com/questions/78183122/best-algorithm-or-library-for-anomaly-detection-in-java</link>
      <description><![CDATA[我们正在做一个项目，尝试测量数据集中的异常情况，其中有我们公司的员工（大约 1000 名），并且每个人都拥有进入许多门的徽章。我们拥有完整的用户数据，例如职位、部门、职位代码、经理等。
每个人可能可以进入 10-100 扇门，我们正在尝试编写一个 java 程序来找到奇怪的人。可能是这样的情况，比如你是唯一一个拥有 5 号门的头衔的人，或者可能很多为 A 经理工作的人都有 10 号门，但为 B 经理工作的这个人也有它。
我们看到的大多数非结构化学习的例子都有数值。我们不这样做。事实上，像系统管理员和高级系统管理员这样的类似头衔是“接近”的。所以可能应该以某种方式考虑到它们是相似的。因此，如果一位 SR 还拥有 7 号门和 5 号门系统管理员，那可能并不是什么奇怪的事情。
理想的结果是列出似乎不合适的人-门组合。也许是一个数字置信值。门本身是一个固定值。但标准是诸如头衔、职位代码或经理层级（向上一级或四级）之类的词。
看来我们需要首先评估用户参数（如标题、部门）中的语言相似性，然后使用这些多值来运行门统计数据。
这似乎是机器学习应该能够做到的事情，但我们在这方面有点新手。有人有开源 java 库、想法或教程的链接吗？尤其是如何链接“相似”的内容。头衔或经理层次结构放在一起，以免将一个人的团队标记为完全异常。]]></description>
      <guid>https://stackoverflow.com/questions/78183122/best-algorithm-or-library-for-anomaly-detection-in-java</guid>
      <pubDate>Mon, 18 Mar 2024 21:35:09 GMT</pubDate>
    </item>
    <item>
      <title>用于西班牙语文本校正的人工智能模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78182982/ai-models-for-text-correction-in-spanish</link>
      <description><![CDATA[我们需要实现不同的人工智能模型来将音频转录为文本，然后将该文本转换为特定的 Word 格式。目前，这是使用从 Python 中的 Hugging Face 获得的模型来完成的，但我正在尝试找到一个可以让我纠正音频转录中的不准确之处的模型，因为这些模型往往非常字面意思以及说话者犯下的任何发音错误或错误保留在正文中。
所以，我需要一个带有人工智能的校正功能，可以进行西班牙语文本校正。我一直在寻找一些，但大部分都是英文的。您是否知道有任何免费或付费模型可以实现此目的？
我寻找了一些人工智能训练模型，但修正是英文的，我需要一个西班牙文的。我还在考虑为这项任务训练我自己的 T5 模型。]]></description>
      <guid>https://stackoverflow.com/questions/78182982/ai-models-for-text-correction-in-spanish</guid>
      <pubDate>Mon, 18 Mar 2024 20:58:11 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn 转换器输出返回更多列，其中一些列没有进行转换</title>
      <link>https://stackoverflow.com/questions/78182162/sklearn-transformer-output-returns-more-columns-with-some-columns-not-having-the</link>
      <description><![CDATA[我正在构建一个 scikit-learn 管道。我从在线机器学习存储库下载了一个数据集并为其生成了描述性统计数据。我正在使用此处找到的processed.cleveland.data数据集： https://archive .ics.uci.edu/dataset/45/heart+disease。
我手动添加了列名称，并根据需要将数字转换为字符串。我将 DataFrame 转换为 Numpy 数组以分隔预测变量和目标变量。之后，我检索管道的数字和分类变量列表。
我开发管道，然后总结 DataFrame。
这样做的结果是我没有生成额外的列。除了 OneHotEncoder 生成的列之外，为什么还有额外的列？
理想情况下，我的输出将包含来自原始数据集的相同数量的列，其中包含转换（简单输入器）以及 OneHotEncoder 为分类变量生成的列。标准化列仍包含空值，而数据集中的原始列包含中位数。
有人可以让我知道这些问题吗？
导入 pandas 作为 pd
将 numpy 导入为 np
导入操作系统
从 pathlib 导入路径

从 sklearn.compose 导入 ColumnTransformer
从 sklearn.pipeline 导入管道
从 sklearn.preprocessing 导入 OneHotEncoder
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.impute 导入 SimpleImputer

网址 = ...
名称 = [&#39;年龄&#39;, &#39;性别&#39;, &#39;cp&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;exang&#39;,
&#39;oldpeak&#39;、&#39;slope&#39;、&#39;ca&#39;、&#39;thal&#39;、&#39;num&#39;]

def getData():
    返回 pd.read_csv(url, sep=&#39;,&#39;, 名称=名称)

输入 = 获取数据()
打印（输入.info（））
打印（输入.描述（））

数组=输入.值
X = 数组[:,0:13]
y = 数组[:,13]

dataframe = pd.DataFrame.from_records(X)
数据帧[[1,2,5,6,8]] =数据帧[[1,2,5,6,8]].astype(str)


numeric = dataframe.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns
categorical = dataframe.select_dtypes(include=[&#39;object&#39;, &#39;bool&#39;]).columns

打印（数字）
打印（分类）

t = [(&#39;cat0&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;), [1, 2, 5, 6, 8]), (&#39;cat1&#39;,
OneHotEncoder()，分类），（&#39;num0&#39;，SimpleImputer（strategy=&#39;median&#39;），数值），（&#39;num1&#39;，
MinMaxScaler()，数值）]
column_transforms = ColumnTransformer(transformers=t)

管道=管道(步骤=[(&#39;t&#39;,column_transforms)])
结果 = pipeline.fit_transform(dataframe)

打印（类型（pd.DataFrame.from_records（结果）））
打印（pd.DataFrame.from_records（结果）.to_string（））``

我期望 DataFrame 以相同的顺序返回（使用 SimpleImputer 和 StandardScaler）以及 OneHotEncoder 创建的新变量。]]></description>
      <guid>https://stackoverflow.com/questions/78182162/sklearn-transformer-output-returns-more-columns-with-some-columns-not-having-the</guid>
      <pubDate>Mon, 18 Mar 2024 17:57:04 GMT</pubDate>
    </item>
    <item>
      <title>从哪里可以找到 ML 神经网络的代码？</title>
      <link>https://stackoverflow.com/questions/78181767/from-where-i-can-find-the-codes-for-ml-neural-network</link>
      <description><![CDATA[我想在 Python 上研究机器学习神经网络，以预测遗传相互作用。我正在寻找有关编码和从公共数据生成结果的实践学习经验。
我发现的大多数都是商业或理论信息书籍，没有实用代码。如果您有任何消息来源可以指导我完成此操作，我真的很感激。]]></description>
      <guid>https://stackoverflow.com/questions/78181767/from-where-i-can-find-the-codes-for-ml-neural-network</guid>
      <pubDate>Mon, 18 Mar 2024 16:50:29 GMT</pubDate>
    </item>
    <item>
      <title>训练张量流模型来检测 .wav 文件中的静音</title>
      <link>https://stackoverflow.com/questions/78181530/train-a-tensorflow-model-to-detect-silence-in-wav-file</link>
      <description><![CDATA[我需要检测波形文件中的静音（轻微噪音，不是绝对静音）。所有波形文件（训练和检测）都是 16 位和单声道。
这是处理给定目录中所有静音文件的训练脚本。声音文件被分为0.1秒的块（1600帧）作为训练数据，对于特征检测，可以使用梅尔频率倒谱系数（MFCC）或短时傅里叶变换（STFT）。 （我都试过了）
这是训练脚本
# 使用tensorflow训练静音模型
导入全局
导入操作系统
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;1&#39;
os.environ[“TF_ENABLE_ONEDNN_OPTS”] = &#39;0&#39;

将张量流导入为 tf
从tensorflow.keras导入层、模型
将 numpy 导入为 np
导入操作系统
导入库

# 从音频文件中提取MFCC特征的函数
def extract_features(file_path, mfcc=True, hop_length=512, n_mfcc=13):
    信号，sr = librosa.load（文件路径，sr =无）
    block_size = 1600 # sr / 10 持续 0.1 秒
    num_blocks = len(signal) // 块大小
    
    特征=[]
    对于范围内的 i（num_blocks）：
        块 = 信号[i * 块大小: (i + 1) * 块大小]
        如果是 mfcc：
            mfccs = librosa.feature.mfcc(y=块，sr=sr，n_fft=1024，hop_length=hop_length，n_mfcc=n_mfcc)
            功能.append(mfccs.T)
        别的：
            features.append(np.abs(librosa.stft(块, n_fft=1024, hop_length=hop_length)))
    
    返回 np.array(特征)

# 包含静音和噪音声音文件的目录
silence_files = glob.glob(&#39;声音/沉默/沉默*.wav&#39;)

# 提取所有文件的特征
X = []
y = []
对于silence_files中的文件：
    特征 = extract_features(文件, mfcc=True)
    X.扩展（功能）
    y.extend([0] * len(features)) # 假设静音文件标记为 0

# 将列表转换为数组
X = np.array(X)
y = np.array(y)

# 定义并编译模型
模型 = models.Sequential([
    层.输入(形状=X[0].形状),
    Layers.Reshape(target_shape=(*X[0].shape, 1)), # 重塑以包含通道尺寸
    层.Conv2D(32, kernel_size=(3, 3), 激活=&#39;relu&#39;),
    层数.MaxPooling2D(pool_size=(2, 2)),
    层.Flatten(),
    层.Dense(1, 激活=&#39;sigmoid&#39;)
]）

# 编译模型
model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;binary_crossentropy&#39;,
              指标=[&#39;准确性&#39;])

# 训练模型
model.fit(X, y, epochs=10, batch_size=32)

# 将模型保存到外部文件
model.save(“models/silence_model.keras”)

这是检测脚本
# 使用张量流模型检测静音
导入操作系统
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;1&#39;
os.environ[“TF_ENABLE_ONEDNN_OPTS”] = &#39;0&#39;
将声音文件导入为 sf

将张量流导入为 tf
导入库
将 numpy 导入为 np

# 从音频帧中提取MFCC特征的函数
def extract_features_from_block(块, mfcc=True, hop_length=512, n_mfcc=13):
    如果是 mfcc：
        mfccs = librosa.feature.mfcc(y=块，sr=16000，n_fft=1024，hop_length=hop_length，n_mfcc=n_mfcc)
        # 重塑以匹配模型输入形状
        mfccs = mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1], 1)
        返回 MFCC
    别的：
        stft = librosa.stft(块, n_fft=1024, hop_length=hop_length)
        # 重塑以匹配模型输入形状
        stft = stft.reshape(1, stft.shape[0], stft.shape[1], 1)
        返回 np.abs(stft)


def remove_silence(输入文件、输出文件、模型、阈值=0.5):
    信号，sr = librosa.load（输入文件，sr =无）
    block_size = 1600 # sr / 10 持续 0.1 秒
    num_blocks = len(signal) // 块大小
    
    输出信号 = np.array([])
    
    对于范围内的 i（num_blocks）：
        块 = 信号[i * 块大小: (i + 1) * 块大小]
        特征 = extract_features_from_block(块, mfcc=True)
        预测 = model.predict(feature)[0][0]
        打印（预测）
        如果预测&lt;临界点：
            # 添加完整的静音块
            输出信号 = np.concatenate((输出信号，np.zeros_like(块)))
        别的：
            # 添加非静音块
            输出信号 = np.concatenate((输出信号，块))

    # 将处理后的信号写入输出文件
    sf.write（输出文件，输出信号，sr）

# 加载保存的模型
model = tf.keras.models.load_model(“models/silence_model.keras”)

# 使用示例
删除_静音（“./sounds/slience_test.wav”，“output_file.wav”，模型）

检测中的问题是每个区块的预测几乎等于 0.0。以下链接可下载 3 个用于训练的静默文件和一个用于测试的静默文件。
SILENCE.ZIP]]></description>
      <guid>https://stackoverflow.com/questions/78181530/train-a-tensorflow-model-to-detect-silence-in-wav-file</guid>
      <pubDate>Mon, 18 Mar 2024 16:10:37 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn train_test_split 是否复制数据？</title>
      <link>https://stackoverflow.com/questions/78181518/does-scikit-learn-train-test-split-copy-data</link>
      <description><![CDATA[是否train_test_split&lt; /a&gt; scikit-learn 的方法复制数据？换句话说，如果我使用大型数据集 X, y，这是否意味着在执行类似
X_train，X_test，y_train，y_test = train_test_split（X，y，test_size = 0.2，random_state = 2023）

我的数据使用的内存是原始数据集的两倍？或者是否有一些 scikit-learn （或基本的 python）魔法可以阻止它？ （例如，使用  .to_numpy()并不一定会导致数据重复)
如果内存使用量翻倍，解决此问题的最佳实用方法是什么？也许，类似
X、X_test、y、y_test = train_test_split(X、y、test_size=0.2、random_state=2023)

？
备注
使用 np.shares_memor(X_train, X) 表明数据确实是重复的。]]></description>
      <guid>https://stackoverflow.com/questions/78181518/does-scikit-learn-train-test-split-copy-data</guid>
      <pubDate>Mon, 18 Mar 2024 16:08:22 GMT</pubDate>
    </item>
    <item>
      <title>“内存分配失败”的原因是什么</title>
      <link>https://stackoverflow.com/questions/78180875/whats-the-reason-of-memory-allocate-failed</link>
      <description><![CDATA[我创建了一个简单的逻辑回归类，其中包含一个gradient_ascent 函数。当我尝试使用它时，我的 IDE 会引发如下错误
无法为形状为 (86918, 86918) 且数据类型为 float64 的数组分配 56.3 GiB

我的数据大小是(86918,10)，标签大小是(86918,1)。我调试的时候发现在gradient_ascent函数中执行代码error = y_predicted - y时程序退出并报错。但是，我找不到原因。以下是我的代码。
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.metrics 导入 precision_score
从 sklearn.model_selection 导入 train_test_split


定义 sigmoid(z):
    返回 1 / (1 + np.exp(-z))


逻辑回归类：
    def __init__(self,learning_rate=0.01,number_iterations=1000,method=&#39;gradient_ascent&#39;):
        自我学习率 = 学习率
        self.number_iterations = number_iterations
        自重=无
        自我偏见=无
        self.method = 方法

    defgradient_ascent(自身, X, y):
        num_samples, num_features = X.shape
        self.weights = np.zeros((num_features, 1))
        自我偏见 = 0

        for _ in range(self.number_iterations):
            线性模型 = np.dot(X, self.weights) + self.bias
            y_预测 = sigmoid(线性模型)
            误差 = y_预测 - y
            dw = (1 / num_samples) * np.dot(X.T, 错误)
            db = (1 / num_samples) * np.sum(错误)

            self.weights += self.learning_rate * dw
            self.bias += self.learning_rate * db

    def stochastic_gradient_ascent(自身, X, y):
        num_samples, num_features = X.shape
        self.weights = np.zeros((num_features, 1))
        自我偏见 = 0

        for _ in range(self.number_iterations):
            对于范围内的 i（num_samples）：
                线性模型 = np.dot(X[i], self.weights) + self.bias
                y_预测 = sigmoid(线性模型)
                误差 = y_预测 - y[i]

                dw = X[i] * 误差
                数据库=错误

                self.weights += self.learning_rate * dw
                self.bias += self.learning_rate * db

    def fit(自身, X, y):
        如果 self.method == &#39;gradient_ascent&#39;:
            self.gradient_ascent(X, y)
        别的：
            self.stochastic_gradient_ascent(X, y)

    def 预测（自身，X）：
        y_predicted = sigmoid(np.dot(X, self.weights) + self.bias)
        y_predicted = [1 如果 i &gt; 0.5 else 0 for i in y_predicted]
        返回 y_预测值


路径=&#39;../data/KaggleCredit2.csv&#39;
数据= pd.read_csv（路径，index_col = 0）
data.dropna（轴= 0，就地= True）
y = 数据[&#39;SeriousDlqin2yrs&#39;]
X = data.drop(标签=&#39;SeriousDlqin2yrs&#39;, 轴=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

lr1 = 逻辑回归()
lr1.fit(X_train, y_train)
预测1 = lr1.预测(X_test)
lr1_score = precision_score(y_test, 预测1)

我尝试过查阅GPT并逐行调试代码，但仍然找不到错误。如果有人能给我一些指导，我将不胜感激。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78180875/whats-the-reason-of-memory-allocate-failed</guid>
      <pubDate>Mon, 18 Mar 2024 14:22:18 GMT</pubDate>
    </item>
    <item>
      <title>尝试复制一种收敛权重的算法，以近似合作差分博弈系统的联合成本函数</title>
      <link>https://stackoverflow.com/questions/78180810/trying-to-replicate-an-algorithm-that-converges-weights-for-approximate-a-joint</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78180810/trying-to-replicate-an-algorithm-that-converges-weights-for-approximate-a-joint</guid>
      <pubDate>Mon, 18 Mar 2024 14:10:32 GMT</pubDate>
    </item>
    <item>
      <title>预测客户的下一个购买日[关闭]</title>
      <link>https://stackoverflow.com/questions/78179087/predicting-customers-next-purchase-day</link>
      <description><![CDATA[我正在开发一个项目，需要预测客户的下一个购买日。我有一个数据集，其中包括客户购买历史记录、RFM（新近度、频率、货币）分数以及其他功能，例如：
最近三次购买之间的天数，
均值&amp;购买天数差异的标准差
您能否建议我可以用于此预测任务的最有效的模型或方法？我正在寻找一种可以利用这些功能以及数据的时间序列性质来提供准确预测的方法。
任何建议或见解将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78179087/predicting-customers-next-purchase-day</guid>
      <pubDate>Mon, 18 Mar 2024 09:17:12 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 的 Google Colab Bert 实例化错误</title>
      <link>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</link>
      <description><![CDATA[我正在尝试在 Colab 上使用 Tensorflow 构建 Bert 模型。这段代码几周前就可以完美运行。现在，如果我尝试实例化模型，则会收到以下错误：
初始化 TF 2.0 模型 TFBertModel 时未使用 PyTorch 模型的某些权重：[&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls .predictions.transform.LayerNorm.weight&#39;、&#39;cls.predictions.bias&#39;、&#39;cls.seq_relationship.bias&#39;、&#39;cls.predictions.transform.dense.bias&#39;、&#39;cls.seq_relationship.weight&#39;]
- 如果您从在其他任务或其他架构上训练的 PyTorch 模型初始化 TFBertModel（例如，从 BertForPreTraining 模型初始化 TFBertForSequenceClassification 模型），这是预期的。
- 如果您从希望完全相同的 PyTorch 模型初始化 TFBertModel（例如，从 BertForSequenceClassification 模型初始化 TFBertForSequenceClassification 模型），则不会出现这种情况。
TFBertModel 的所有权重都是从 PyTorch 模型初始化的。
如果您的任务与检查点模型训练的任务类似，您就可以使用 TFBertModel 进行预测，而无需进一步训练。
-------------------------------------------------- ------------------------
TypeError Traceback（最近一次调用最后一次）
&lt;ipython-input-14-b0e769ef7​​890&gt;在&lt;细胞系：7&gt;()
      5 SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
      6 SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
----&gt; 7 SC_pooler_output = SC_bert_model(SC_input_layer, Attention_mask=SC_mask_layer)[1] # 第二个输出，che è il pooler_output
      8
      9 # 辍学层的Aggiungi

36帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py 在 type_spec_from_value(value) 中
   1002 3，“无法将 %r 转换为张量：%s” % (类型(值).__name__, e))
   1003
-&gt;第1004章
   第1005章 1005
   1006

TypeError：调用层“嵌入”时遇到异常（类型 TFBertEmbeddings）。

无法为名称构建 TypeSpec：“tf.debugging.assert_less_5/assert_less/Assert/Assert”
op：“断言”
输入：“tf.debugging.assert_less_5/assert_less/All”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_0”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_1”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_2”
输入：“占位符”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_4”
输入：“tf.debugging.assert_less_5/assert_less/y”
属性{
  键：“总结”
  价值 {
    我：3
  }
}
属性{
  键：“T”
  价值 {
    列表 {
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_INT32
      类型：DT_STRING
      类型：DT_INT32
    }
  }
}
 不支持的类型。

调用层“embeddings”接收的参数（类型 TFBertEmbeddings）：
  • input_ids=
  •position_ids=无
  • token_type_ids=
  • input_embeds=无
  •过去的键值长度=0
  • 训练=False

模型的代码是：
SC_input_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“input_ids”)
SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
SC_pooler_output = SC_bert_model（SC_input_layer，attention_mask = SC_mask_layer）[1]

# Dropout 层的Aggiungi
SC_dropout_layer = Dropout(dropout_rate)(SC_pooler_output)
SC_output_layer = 密集（6，激活=&#39;sigmoid&#39;）（SC_dropout_layer）
SC_model = 模型(输入=[SC_input_layer, SC_mask_layer], 输出=SC_output_layer)

我发现安装tensorflow 2.10.0可以工作，但是使用Google Colab时我的CUDA版本有问题，并且使用tensorflow 2.10它无法识别GPU。
该代码几周前就可以工作，有人有解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</guid>
      <pubDate>Sun, 17 Mar 2024 17:03:42 GMT</pubDate>
    </item>
    <item>
      <title>如何在火车上训练 11 个以上参数的模型，但在测试中使用 2 个参数 [关闭]</title>
      <link>https://stackoverflow.com/questions/78173009/how-to-train-model-on-11-parameters-on-train-but-using-on-2-param-in-test</link>
      <description><![CDATA[我的任务是在火车上我可以使用许多输入参数（11 个或更多），但在实际任务（测试）中输入始终是 2 个参数。因此，就如何做得更好、是否值得这样做等等提出一些建议。 简而言之，这是一个回归任务，当我需要预测用户从这台 ATM 取款的概率（用户 ID、和 ATM 位置），这些数据是其他人给我的，但我不明白如何使用比这两个更多的参数。
全局参数：
用户身份
自动提款机位置
我可以使用的火车上的输入：
h3_09
客户ID
日期时间_id
计数整数
总和、平均值、最小值、最大值、浮点数
MCC
等等
我已经尝试过在 11 个输入参数上训练神经网络，但我不明白如何在两个输入参数上使用它的电流，一般来说，我想使用一些梯度提升，但我也不明白如何充分利用所有参数...]]></description>
      <guid>https://stackoverflow.com/questions/78173009/how-to-train-model-on-11-parameters-on-train-but-using-on-2-param-in-test</guid>
      <pubDate>Sat, 16 Mar 2024 19:10:22 GMT</pubDate>
    </item>
    <item>
      <title>关于机器学习和数值训练数据的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78172418/question-about-machine-learning-and-numerical-training-data</link>
      <description><![CDATA[如果您使用 is_away 之类的内容作为数据中的数字字段，人工智能将对其进行训练以预测团队获胜的可能性。所以 1 代表 true，0 代表 false，那么是否应该将其更改为 is_home 之类的内容并翻转值，或者人工智能最终会在预测诸如获胜机会之类的内容时了解到 0 更有可能获胜？
我认为另一个很好的例子是海拔高度，而你的目标值是点。在大多数情况下，海拔越高，得分越少或赛道时间越慢。我假设人工智能理解海拔越高意味着它更有可能预测较低的目标值。在看到一些奇怪的预测后，我将玩家的高度提高了 5k，并在一场游戏中复制了所有其他字段，并且它总是预测该游戏的目标值更高。所以我对人工智能如何处理更高的数值感到困惑。
另请注意，我正在使用 relu 激活和 500k 行数据。我将随机更改单行的高度并使用相同的参数重新训练。与之前的训练数据相比，该行的预测值将从 20 变为 25，其他任何事情都不会发生变化...所以总结一下我的问题，应该反转对预测目标值产生负面影响的数值数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/78172418/question-about-machine-learning-and-numerical-training-data</guid>
      <pubDate>Sat, 16 Mar 2024 15:57:58 GMT</pubDate>
    </item>
    <item>
      <title>如何绘制 2x2 混淆矩阵，行中为预测值，列中为实际值？</title>
      <link>https://stackoverflow.com/questions/70176887/how-to-plot-2x2-confusion-matrix-with-predictions-in-rows-an-real-values-in-colu</link>
      <description><![CDATA[我知道我们可以使用以下示例代码使用 sklearn 绘制混淆矩阵。
from sklearn.metrics import fusion_matrix, ConfusionMatrixDisplay
将 matplotlib.pyplot 导入为 plt

y_true = [1, 0, 1, 1, 0, 1]
y_pred = [0, 0, 1, 1, 0, 1]

打印（f&#39;y_true：{y_true}&#39;）
打印(f&#39;y_pred: {y_pred}\n&#39;)

cm = 混淆矩阵(y_true, y_pred, labels=[0, 1])
印刷（厘米）
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
显示图()
plt.show()


我们拥有：
&lt;前&gt;&lt;代码&gt;TN | FP
前线 | TP

但我希望将预测标签放置在行或 y 轴中，并将真实值或实值标签放置在列或 x 轴中。我如何使用 Python 绘制此图？
我想要什么：
&lt;前&gt;&lt;代码&gt;TP | FP
前线 |总氮
]]></description>
      <guid>https://stackoverflow.com/questions/70176887/how-to-plot-2x2-confusion-matrix-with-predictions-in-rows-an-real-values-in-colu</guid>
      <pubDate>Tue, 30 Nov 2021 22:23:42 GMT</pubDate>
    </item>
    <item>
      <title>遗传算法如何利用数值数据演化出解决方案？</title>
      <link>https://stackoverflow.com/questions/61113622/how-can-genetic-algorithms-evolve-solutions-with-numerical-data</link>
      <description><![CDATA[我熟悉字符串或文本上下文中的 GA，但不熟悉数字数据。
对于字符串，我了解如何应用交叉和变异：
ParentA = abcdef
父级 B = uvwxyz

使用单点交叉：
ChildA = abwxyz（在第二个基因之后枢轴）
ChildB = uvcdef

使用随机基因突变（交叉后）：
ChildA = abwgyz（第四个基因突变）
ChildB = uvcdef（无基因突变）

对于字符串，我有一个离散的字母表可供使用，但是这些运算符如何应用于连续的数值数据？
例如，染色体表示为 4 空间中的点（每个轴是一个基因）：
ParentA = [19, 58, 21, 54]
父级 B = [65, 21, 59, 11]

通过为后代切换父母双方的轴来应用交叉是否合适？
ChildA = [19, 58, 59, 11]（在第二个基因之后旋转）
子 B = [65, 21, 21, 54]

我有一种感觉，这似乎不错，但我天真的突变概念，即随机化基因，似乎并不正确：
ChildA = [12, 58, 59, 11]（第一个基因突变）
ChildB = [65, 89, 34, 54]（第二个和第三个基因突变）

我只是不确定如何将遗传算法应用于这样的数字数据。我知道 GA 需要什么，但不知道如何应用运算符。例如，考虑在 4 维中最小化 Rastrigin 函数的问题：每个维度的搜索空间为 [-512, 512]，适应度函数为 Rastrigin 函数。我不知道我在这里描述的运算符如何帮助找到更合适的染色体。
就其价值而言，精英选择和群体初始化似乎很简单，我唯一的困惑来自交叉和变异算子。
赏金更新
我使用突变率和交叉率对连续数值数据进行了遗传算法的实现，正如我在这里所描述的。优化问题是二维的 Styblinski-Tang 函数，因为它很容易用图表表示。我还使用标准精英和锦标赛选择策略。
我发现群体最佳适应度确实能很好地收敛到解决方案，但平均适应度却不然。
在这里，我绘制了十代的搜索空间：黑点是候选解决方案，红色“x”是全局最优解：

正如我所描述的，交叉算子似乎工作得很好，但突变算子（随机化染色体的 x 或 y 位置，或者两者都不随机）似乎会创建十字准线和交叉影线图案。
我在 50 维中进行了一次运行以延长收敛时间（因为在二维中它会在一代内收敛）并绘制它：

这里的 y 轴表示解决方案与全局最优值的接近程度（因为最优值是已知的），它只是实际输出/预期输出的一小部分。这是一个百分比。绿线是总体最佳（大约 96-97% 目标），蓝色是总体平均（波动 65-85% 目标）。
这验证了我的想法：变异算子并没有真正对总体产生最好的影响，但确实意味着总体平均值永远不会收敛，并且会上下波动。
所以我的赏金问题是除了基因的随机化之外还可以使用哪些突变算子？
补充一下：我问这个问题是因为我有兴趣使用 GA 来优化神经网络权重来训练网络来代替反向传播。如果您对此有所了解，任何详细信息来源也可以回答我的问题。]]></description>
      <guid>https://stackoverflow.com/questions/61113622/how-can-genetic-algorithms-evolve-solutions-with-numerical-data</guid>
      <pubDate>Thu, 09 Apr 2020 03:45:51 GMT</pubDate>
    </item>
    </channel>
</rss>