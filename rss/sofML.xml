<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 20 Dec 2024 09:17:25 GMT</lastBuildDate>
    <item>
      <title>在 FastAPI 中提供多种机器学习模型的最佳实践：Docker 与使用 Redis 或其他方法的 Celery</title>
      <link>https://stackoverflow.com/questions/79296580/best-practice-for-serving-multiple-machine-learning-models-in-fastapi-docker-vs</link>
      <description><![CDATA[我正在开发一个 FastAPI 应用程序，该应用程序处理来自 Web 应用程序的请求，以使用多个机器学习模型（例如，糖尿病预测模型、面部分析模型等）进行预测。我正在尝试确定部署和管理这些模型的最佳架构。
以下是我正在考虑的方法：
选项 1：Docker + Flask 服务器
将每个模型打包在其自己的 Docker 映像中。
在各自的 Docker 容器内为每个模型运行一个 Flask 服务器。
FastAPI 通过维护每个模型的连接字符串与这些服务器通信（例如，一个用于糖尿病模型，另一个用于面部分析模型等）。

但是，这种方法意味着 FastAPI 必须管理多个连接字符串，随着模型数量的增加，这可能会变得混乱。
选项 2：Celery + Redis
使用 Celery 工作程序处理预测，每个工作程序负责一个特定模型。
使用 Redis 作为任务队列。
FastAPI 只会在 Redis 队列中注册任务，而无需知道各个工作程序的连接详细信息。

这种方法集中了任务管理，并消除了从 FastAPI 管理多个连接字符串的负担。
选项 3：其他方法
在这样的设置中，是否有用于管理和提供多个机器学习模型的替代架构或最佳实践？
我希望系统具有可扩展性、可维护性和高效性。如果您曾经使用过类似的设置，我将不胜感激您的见解！]]></description>
      <guid>https://stackoverflow.com/questions/79296580/best-practice-for-serving-multiple-machine-learning-models-in-fastapi-docker-vs</guid>
      <pubDate>Fri, 20 Dec 2024 08:19:43 GMT</pubDate>
    </item>
    <item>
      <title>iOS Swift 根据用户数据进行动态机器学习</title>
      <link>https://stackoverflow.com/questions/79295972/ios-swift-dynamic-machine-learning-from-user-data</link>
      <description><![CDATA[是否可以使用 Apple ML 框架动态学习应用中的用户行为？我已经使用 Create ML 应用程序训练了一个模型，然后我可以从 iOS 设备更新并重新训练吗？这就是我目前使用该模型的方式。
public func calculateMuscleRecoveryTime(_ workout: Workout) {
do {

let config = MLModelConfiguration()
let model = try MuscleRecoveryModel(configuration: config)

let allMuscleGroups = workout.exercises
.compactMap { $0.muscles } // 展平每个锻炼的肌肉数组
.reduce(Set&lt;MuscleGroup&gt;()) { $0.union($1) } // 联合以删除重复项

let uniqueMuscleGroups = Array(allMuscleGroups)

for muscleGroup in uniqueMuscleGroups {
let trainingIntensity = Int64(workout.intensity.intValue)
let lastTrainedTimestamp = workout.date
let timeAgo = timeAgoInSeconds(from: lastTrainedTimestamp)
let muscleName = muscleGroup.rawValue.lowercased()

let prediction = try model.prediction(muscle: muscleName, intense: trainingIntensity, lastTrained: timeAgo)
}
} catch let error {
print(&quot;Error: &quot;, error)
}
}
]]></description>
      <guid>https://stackoverflow.com/questions/79295972/ios-swift-dynamic-machine-learning-from-user-data</guid>
      <pubDate>Fri, 20 Dec 2024 01:10:59 GMT</pubDate>
    </item>
    <item>
      <title>关于将建议系统复杂性作为毕业设计的咨询 [关闭]</title>
      <link>https://stackoverflow.com/questions/79294470/consultion-about-a-recommendations-system-complexity-as-a-graduation-project</link>
      <description><![CDATA[我和我的两个项目伙伴正在做一个推荐系统作为毕业项目，我们想做一个混合系统，包括基于内容的、协作的和人口统计的推荐模块，同时利用强化学习来跟上新用户的数据
因为我们都没有经验，所以我们想向专业社区寻求建议
我们开始对数据进行预处理，但担心在 2 月 12 日截止日期之前是否能够真正实现所有内容，以及在开始训练模块之前我们应该了解的集成限制，这样我们就不必在完成一个模块后从头开始，因为如果我们要使用混合系统，我们应该做一些不同的事情
所以我想要关于如何做到这一点的建议，以及这一切是否能在截止日期之前完成，因为如果太多了，我们可以缩小规模，因为我们最初的计划只是一个基于内容的推荐系统，我们想知道我们的项目是否在我们的时间限制内可行]]></description>
      <guid>https://stackoverflow.com/questions/79294470/consultion-about-a-recommendations-system-complexity-as-a-graduation-project</guid>
      <pubDate>Thu, 19 Dec 2024 13:36:56 GMT</pubDate>
    </item>
    <item>
      <title>提取 LLaVa 转换器中的 hidden_​​states</title>
      <link>https://stackoverflow.com/questions/79294056/extracting-hidden-states-in-llavas-transformer</link>
      <description><![CDATA[我需要使用LLaVa来获取图像的嵌入+已提供给LLM的查询。
据我所知，我需要LLaVa模型在经过编码层之前的输出。
从StackOverflow上的这篇文章，我尝试使用pre-hook来保存每个层的输入：
def capture_hidden_​​states(module, module_input):
print(f&#39;module_input: {module_input}\n&#39;)

model_path = &quot;liuhaotian/llava-v1.6-vicuna-7b&quot;
model_name = get_model_name_from_path(model_path)

tokenizer, model, image_processor, context_len = load_pretrained_model(
model_path = model_path,
model_base = None,
model_name = model_name,
load_4bit = True, # 保存 GPU 内存
device = &quot;cuda:0&quot; # 默认
)
model.register_forward_pre_hook(capture_hidden_​​states)

但我的输出显示每个级别的输入都是空的：
module_input: ()

我看到 Hugging Face 的版本在 forward 方法中有一个 output_hidden_​​states 参数，但我目前无法让这个版本的 LLaVa 运行，所以我试图让它在我之前编写的代码上运行。]]></description>
      <guid>https://stackoverflow.com/questions/79294056/extracting-hidden-states-in-llavas-transformer</guid>
      <pubDate>Thu, 19 Dec 2024 11:11:59 GMT</pubDate>
    </item>
    <item>
      <title>ESP32 S3 MINI 1 上的 TinyML 部署问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/79293436/tinyml-deployment-issue-on-esp32-s3-mini-1</link>
      <description><![CDATA[我使用 TinyML 根据 6 轴加速度计和陀螺仪数据预测马的活动。数据通过 ESP32-S3 收集，我正尝试在 ESP32-S3 上部署 TensorFlow Lite (TFLite) 模型以进行实时预测。
但是，我遇到了几个库兼容性和部署问题。尽管遵循了标准的 TinyML 部署实践，但模型与 ESP32-S3 的集成似乎存在问题，尤其是在处理 TFLite Micro 运行时时。我尝试了一些优化，但仍然无法让一切顺利运行。
如果您能提供以下方面的任何建议，我将不胜感激：

如何在 ESP32-S3 上有效部署 TinyML 模型。
可能更适合此用例的替代方法或工具。
有关类似设置的任何提示、文档或经验
]]></description>
      <guid>https://stackoverflow.com/questions/79293436/tinyml-deployment-issue-on-esp32-s3-mini-1</guid>
      <pubDate>Thu, 19 Dec 2024 07:39:46 GMT</pubDate>
    </item>
    <item>
      <title>无论如何，PyTorch DeiT 模型都会持续预测一个类别</title>
      <link>https://stackoverflow.com/questions/79293139/pytorch-deit-model-keeps-predicting-one-class-no-matter-what</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79293139/pytorch-deit-model-keeps-predicting-one-class-no-matter-what</guid>
      <pubDate>Thu, 19 Dec 2024 04:52:07 GMT</pubDate>
    </item>
    <item>
      <title>如何以与“https://www.tensorflow.org/tfmodels/vision/object_detection”中类似的方式修改配置文件？</title>
      <link>https://stackoverflow.com/questions/79292447/how-can-i-modify-the-config-file-in-a-similar-way-used-in-https-www-tensorflo</link>
      <description><![CDATA[我正在寻找有关如何修改现有脚本以使用 EfficientDet D1 模型的指导。我按照教程操作，并使用默认脚本成功训练了自定义数据集。该脚本使用以下行来配置模型：
exp_config = exp_factory.get_exp_config(&#39;retinanet_resnetfpn_coco&#39;)

这对于默认的 RetinaNet 模型来说很好。但是，我想改用 EfficientDet D1 模型。我已经下载了 EfficientDet D1 配置文件，但不确定如何在脚本中引用它。
我尝试过的方法

检查了配置文件：我检查了 EfficientDet D1 配置文件中的参数，看它是否有任何明确的引用名称，可以与 exp_factory.get_exp_config() 一起使用。
检查了替代配置方法：我寻找了加载自定义模型配置的其他方法，但找不到任何明确的说明。

我正在寻找什么

如何修改 exp_factory.get_exp_config() 行以引用 EfficientDet D1 配置文件？
如果这种方法不可行，我该如何加载和引用手动修改配置文件？
配置文件本身是否需要进行任何特定更改才能使其正常工作？

如果需要，我愿意手动修改配置文件参数。并尝试通过此方法运行训练
!python model_main_tf2.py \
--model_dir=/content/trainingdemo/models/my_efficientDet_d0 \
--pipeline_config_path=/content/trainingdemo/models/my_efficientDet_d0/pipeline.config

但结果却是
2024-12-18 23:14:43.163543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] 无法注册 cuFFT 工厂：尝试注册插件 cuFFT 工厂，但已注册一个
警告：调用 absl::InitializeLog() 之前的所有日志消息都写入 STDERR
E0000 00:00:1734563683.182126 16153 cuda_dnn.cc:8310] 无法注册 cuDNN 工厂：尝试注册插件 cuDNN 工厂，但有一个工厂已注册
E0000 00:00:1734563683.187813 16153 cuda_blas.cc:1418] 无法注册 cuBLAS 工厂：尝试注册插件 cuBLAS 工厂，但有一个工厂已注册
回溯（最近一次调用）：
文件 &quot;/content/trainingdemo/model_main_tf2.py&quot;，第 32 行，位于 &lt;module&gt;
从 object_detection 导入 model_lib_v2
文件 &quot;/usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py&quot;，第 29 行，位于 &lt;module&gt;
从 object_detection 导入 eval_util
文件 &quot;/usr/local/lib/python3.10/dist-packages/object_detection/eval_util.py&quot;，第 35 行，位于 &lt;module&gt;
从 object_detection.metrics 导入 coco_evaluation
文件 &quot;/usr/local/lib/python3.10/dist-packages/object_detection/metrics/coco_evaluation.py&quot;，第 28 行，位于 &lt;module&gt;
从 object_detection.utils 导入 o​​bject_detection_evaluation
文件 &quot;/usr/local/lib/python3.10/dist-packages/object_detection/utils/object_detection_evaluation.py&quot;，第 46 行，在 &lt;module&gt;
从 object_detection.utils 导入 label_map_util
文件 &quot;/usr/local/lib/python3.10/dist-packages/object_detection/utils/label_map_util.py&quot;，第 29 行，在 &lt;module&gt;
从 object_detection.protos 导入 string_int_label_map_pb2
文件 &quot;/usr/local/lib/python3.10/dist-packages/object_detection/protos/string_int_label_map_pb2.py&quot;，第 33 行，在 &lt;module&gt;
_descriptor.EnumValueDescriptor(
文件 &quot;/usr/local/lib/python3.10/dist-packages/google/protobuf/descriptor.py&quot;，第 789 行，位于 __new__
_message.Message._CheckCalledFromGeneratedFile()
TypeError：无法直接创建描述符。
如果此调用来自 _pb2.py 文件，则您生成的代码已过期，必须使用 protoc &gt;= 3.19.0 重新生成。
如果您无法立即重新生成您的原型，其他一些可能的解决方法是：
1. 将 protobuf 包降级到 3.20.x 或更低版本。
2. 设置 PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python（但这将使用纯 Python 解析，速度会慢得多）。
]]></description>
      <guid>https://stackoverflow.com/questions/79292447/how-can-i-modify-the-config-file-in-a-similar-way-used-in-https-www-tensorflo</guid>
      <pubDate>Wed, 18 Dec 2024 20:49:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 ssd 和 mobilenetv2 进行对象检测时“目标”和“输出形状”不匹配</title>
      <link>https://stackoverflow.com/questions/79292180/mismatch-target-and-output-shape-on-object-detection-using-ssd-and-mobilenet</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79292180/mismatch-target-and-output-shape-on-object-detection-using-ssd-and-mobilenet</guid>
      <pubDate>Wed, 18 Dec 2024 18:50:54 GMT</pubDate>
    </item>
    <item>
      <title>“使用 YOLO 和 EasyOCR 进行车牌识别时遇到的文本识别问题” [关闭]</title>
      <link>https://stackoverflow.com/questions/79291987/text-recognition-issues-in-license-plate-recognition-using-yolo-and-easyocr</link>
      <description><![CDATA[问题
我正在开发一个车牌识别系统，使用 YOLOv8 进行检测，使用 EasyOCR 进行文本识别。虽然 YOLO 可以正确检测车牌区域，但 OCR 结果对于阿拉伯语文本和数字通常不准确。
检测到的文本示例：
检测到：“اباز”，这是无关紧要的。
检测到：“الراق”，与“العراق”部分匹配。
像“٢٦٠٤٩٩”这样的数字被准确检测到。
我尝试过的
管道设置：
YOLOv8 检测车牌并提取其边界框。
EasyOCR 处理裁剪后的车牌以进行文本识别。
文本校正：
使用 difflib.get_close_matches() 将 OCR 检测到的文本与预定义单词进行匹配（例如，“العراق”、“دهوك”）。
应用置信度阈值来过滤低置信度结果。
图像预处理：
将车牌区域转换为灰度。
调整区域大小以增强 OCR 性能。
最小可重现示例
import cv2
import easyocr
from ultralytics import YOLO

def detect_plate_with_yolo(image_path, model_path=&quot;yolov8n.pt&quot;):
model = YOLO(model_path)
img = cv2.imread(image_path)
results = model(img)
detections = results[0].boxes.xyxy.cpu().numpy()
if detections:
x1, y1, x2, y2 = map(int, detections[0])
return img[y1:y2, x1:x2]
return None

def perform_ocr_on_plate(plate_img):
reader = easyocr.Reader([&#39;ar&#39;, &#39;en&#39;], gpu=False)
plate_gray = cv2.cvtColor(plate_img, cv2.COLOR_BGR2GRAY)
return reader.readtext(plate_gray, detail=1)

plate_img = detect_plate_with_yolo(&quot;path/to/image.jpg&quot;)
if plate_img is not None:
detected_text = perform_ocr_on_plate(plate_img)
print(detected_text)

预期与实际行为
预期：正确识别阿拉伯语文本和数字（例如，&quot;العراق&quot;）。
实际：部分匹配（例如，&quot;الراق&quot;）或不相关的结果（例如，&quot;اباز&quot;）。
问题
如何使用 EasyOCR 提高阿拉伯语车牌的 OCR 准确率？
有没有比 difflib.get_close_matches() 更好的文本校正替代方案？
哪些额外的预处理步骤可能有助于提高 OCR 性能？]]></description>
      <guid>https://stackoverflow.com/questions/79291987/text-recognition-issues-in-license-plate-recognition-using-yolo-and-easyocr</guid>
      <pubDate>Wed, 18 Dec 2024 17:26:40 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：微调 llama 时，张量的元素 0 不需要 grad 且没有 grad_fn</title>
      <link>https://stackoverflow.com/questions/79277352/runtimeerror-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-gra</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79277352/runtimeerror-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-gra</guid>
      <pubDate>Fri, 13 Dec 2024 06:02:07 GMT</pubDate>
    </item>
    <item>
      <title>SAM 2.1 是什么导致 hydra.errors.MissingConfigException：未找到主配置模块“sam2”？</title>
      <link>https://stackoverflow.com/questions/79199682/sam-2-1-what-is-causing-hydra-errors-missingconfigexception-primary-config-modu</link>
      <description><![CDATA[我正在尝试使用此处给出的 roboflow 指南微调新的 SAM 2.1 分割模型：Sam 2.1 roboflow 指南
使用 google collab 时，此代码运行正常，没有遇到任何错误。当我在本地机器上运行完全相同的代码时，运行训练代码命令时会出现以下错误：
!python training/train.py -c &#39;configs/train.yaml&#39; --use-cluster 0 --num-gpus 1
在 Windows 10 上使用 vscode 运行时出现以下错误：
hydra.errors.MissingConfigException：未找到主配置模块“sam2”。
检查它是否正确并包含 __init__.py 文件

我的工作目录：
C:\..\SAM_2_1\sam2

]]></description>
      <guid>https://stackoverflow.com/questions/79199682/sam-2-1-what-is-causing-hydra-errors-missingconfigexception-primary-config-modu</guid>
      <pubDate>Mon, 18 Nov 2024 11:00:31 GMT</pubDate>
    </item>
    <item>
      <title>总参数：0，执行 model.summary() keras</title>
      <link>https://stackoverflow.com/questions/78462277/total-params-0-on-doing-model-summary-keras</link>
      <description><![CDATA[model = Sequential()
model.add(Embedding(283, 100, input_length=56))
model.add(LSTM(150))
model.add(LSTM(150))
model.add(Dense(283,activation=&#39;softmax&#39;))

model.compile(loss=&#39;categorical_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;])

model.summary()

Tensorflow 版本：2.16.1，
Keras 版本：3.3.3，
设备 - M3 pro macbook
我尝试使用虚拟数据集（有 282 个唯一单词，使用 tokenizer 检查）构建用于文本生成的 LSTM 模型，预期参数为非零，但输出每个层都有 0 个参数。]]></description>
      <guid>https://stackoverflow.com/questions/78462277/total-params-0-on-doing-model-summary-keras</guid>
      <pubDate>Fri, 10 May 2024 19:49:53 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：使用“bitsandbytes”8 位量化需要加速：“pip install accelerate”</title>
      <link>https://stackoverflow.com/questions/78040978/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</link>
      <description><![CDATA[我正在尝试使用开源数据集对 llama2-13b-chat-hf 进行微调。
我一直使用此模板，但现在出现此错误：
ImportError：使用 bitsandbytes 8 位量化需要 Accelerate：pip install accelerate 和最新版本的 bitsandbytes：pip install -i https://pypi.org/simple/ bitsandbytes
我安装了所有必需的软件包，这些是版本：
 accelerate @ git+https://github.com/huggingface/accelerate.git@97d2168e5953fe7373a06c69c02c5a00a84d5344
bitsandbytes==0.42.0
datasets==2.17.1
huggingface-hub==0.20.3
peft==0.8.2
tokenizers==0.13.3
torch==2.1.0+cu118
torchaudio==2.1.0+cu118
torchvision==0.16.0+cu118
transformers==4.30.0
trl==0.7.11

有人知道这是不是版本问题吗？
你是怎么解决的？
我尝试安装其他版本，但没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/78040978/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</guid>
      <pubDate>Thu, 22 Feb 2024 12:37:11 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ML 模型和 FastAPI 处理来自多个用户的请求？</title>
      <link>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</link>
      <description><![CDATA[我正在研究通过FastAPI分发人工智能模块的过程。
我创建了一个FastAPI应用，使用预先学习的机器学习模型来回答问题。
这种情况下，一个用户使用是没有问题的，但是多个用户同时使用的时候，响应可能会太慢。
那么，当多个用户输入一个问题的时候，有没有办法一次性复制模型并加载进去？
class sentencebert_ai():
def __init__(self) -&gt;无：
super().__init__()

def ask_query(self,query, topN):
startt = time.time()

ask_result = []
score = []
result_value = [] 
embedder = torch.load(model_path)
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)
query_embedding = embedder.encode(query, convert_to_tensor=True)
cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0] #torch.Size([121])121 表示该数据集为 10 ... cos_scores = cos_scores.cpu()

        top_results = np.argpartition(-cos_scores, range(topN))[0:topN]

        对于 top_results[0:topN] 中的 idx：        
            Ask_result.append(corpusid[idx].item())
            #.item()으로 접근하는 유는 张量(5)에서 해당 숫자에 접근하기 위한 방식다.
            score.append(round(cos_scores[idx].item(),3))

# 生成 json 数组并返回结果集
for i,e in zip(ask_result,score):
result_value.append({&quot;pred_id&quot;:i,&quot;pred_weight&quot;:e})
endd = time.time()
print(&#39;结果集&#39;,endd-startt)
return result_value
# return &#39;,&#39;.join(str(e) for e in ask_result),&#39;,&#39;.join(str(e) for e in score)

class Item_inference(BaseModel):
text : str
topN : Optional[int] = 1

@app.post(&quot;/retrieval&quot;, tags=[&quot;knowledge referral&quot;])
async def Knowledge_recommendation(item: Item_inference):

# db.append(item.dict())
item.dict()
results = _ai.ask_query(item.text, item.topN)

return results

if __name__ == &quot;__main__&quot;:
parser = argparse.ArgumentParser()
parser.add_argument(&quot;--port&quot;, default=&#39;9003&#39;, type=int)
# parser.add_argument(&quot;--mode&quot;, default=&#39;cpu&#39;, type=str, help=&#39;cpu for CPU mode, gpu for GPU mode&#39;)
args = parser.parse_args()

_ai = sentencebert_ai()
uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=args.port,workers=4)

更正版本
@app.post(&quot;/aaa&quot;) def your_endpoint(request: Request, item:Item_inference): start = time.time() model = request.app.state.model item.dict() # 测试结果 _ai = sentencebert_ai() results = _ai.ask_query(item.text, item.topN,model) end = time.time() print(end-start) return results ``` 
]]></description>
      <guid>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</guid>
      <pubDate>Fri, 25 Mar 2022 07:13:32 GMT</pubDate>
    </item>
    <item>
      <title>Google Cloud Vision API 和 Mobile Vision 有什么区别？</title>
      <link>https://stackoverflow.com/questions/44091577/what-is-the-difference-between-google-cloud-vision-api-and-mobile-vision</link>
      <description><![CDATA[我一直在使用 cloud vision API。我做了一些标签和面部检测。在这次 Google I/O 期间，有一个会议讨论了 mobile vision。我知道这两个 API 都与 Google Cloud 中的机器学习有关。
有人能解释（用例）何时使用其中一个而不是另一个吗？
我们可以同时使用这两个 API 来构建什么样的应用程序？]]></description>
      <guid>https://stackoverflow.com/questions/44091577/what-is-the-difference-between-google-cloud-vision-api-and-mobile-vision</guid>
      <pubDate>Sat, 20 May 2017 22:59:56 GMT</pubDate>
    </item>
    </channel>
</rss>