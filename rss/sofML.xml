<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 19 Aug 2024 18:20:26 GMT</lastBuildDate>
    <item>
      <title>这个 CNN 的最后三层使用的是全连接层还是卷积层？</title>
      <link>https://stackoverflow.com/questions/78888560/is-this-cnn-using-fully-connected-or-convolution-layers-for-the-final-three-laye</link>
      <description><![CDATA[在本文使用 CNN 准确检测唤醒词的开始和结束的第 2 部分中，图表显示了 CNN 架构。
最后三层标记为 FC，通常指完全连接层。但是，当它声明 FC1：3x3、FC2：1x1 和 FC3：1x1 时，似乎在每个完全连接层旁边都指定了内核大小。这可能意味着它们实际上是卷积层？
有人能给我解释一下最后三层吗？
以下是论文中指定的层：
Conv 1：9x5
72x60x96
Max Pooling：2x3
36x20x96

Conv2：7x3，步幅：3x1
10x18x192
Max Pooling：1z2
10x9x192

Conv3：4x3
7x7x192

Conv4：3x3
5x5x192

Conv5：3x3
3x3x192

FC1：3x3
1x1x500
FC2：1x1
1x1x500
FC3：1x1
1x1x500
Alexa
]]></description>
      <guid>https://stackoverflow.com/questions/78888560/is-this-cnn-using-fully-connected-or-convolution-layers-for-the-final-three-laye</guid>
      <pubDate>Mon, 19 Aug 2024 15:04:26 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 2.17.0 模型指标无法识别 - 显示 ['loss', 'compile_metrics'] 而不是预期指标</title>
      <link>https://stackoverflow.com/questions/78888263/tensorflow-2-17-0-model-metrics-not-recognized-showing-loss-compile-metri</link>
      <description><![CDATA[我正在尝试使用 TensorFlow 和 Keras 构建和训练一个简单的神经网络，但我遇到了一个问题，即我在 model.compile 中指定的指标无法正确识别。
具体来说，在编译和训练模型后，当我打印指标和 metrics_names 时，我看到的是 [&#39;loss&#39;, &#39;compile_metrics&#39;]，而不是预期的指标（[&#39;loss&#39;, &#39;accuracy&#39;, &#39;precision&#39;, &#39;recall&#39;]）。
这是我使用的代码
import tensorflow as tf
import numpy as np
from tensorflow import keras

# 创建一个简单的模型
def create_model():
model = tf.keras.Sequential([
tf.keras.layers.Dense(16,activation=&#39;relu&#39;,input_shape=(3,)),
tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)
])
返回模型

# 测试函数以演示 `metrics` 和 `metrics_names`
def test_metrics_properties():
# 创建并编译模型
model = create_model()
model.compile(optimizer=&#39;adam&#39;,
loss=keras.losses.BinaryCrossentropy(),
metrics=[
tf.keras.metrics.BinaryAccuracy(name=&#39;accuracy&#39;),
tf.keras.metrics.Precision(name=&#39;precision&#39;),
tf.keras.metrics.Recall(name=&#39;recall&#39;)
])

# 在训练前检查 `metrics` 和 `metrics_names`
print(&quot;Before training:&quot;)
print(&quot;Metrics:&quot;, [m.name for m in model.metrics])
print(&quot;Metrics names:&quot;, model.metrics_names)

# 生成虚拟数据
x = np.random.random((100, 3))
y = np.random.randint(0, 2, size=(100, 1))

# 训练模型
model.fit(x, y, epochs=5, verbose=0)

# 训练后检查 `metrics` 和 `metrics_names`
print(&quot;\n训练后：&quot;)
print(&quot;Metrics:&quot;, [m.name for m in model.metrics])
print(&quot;Metrics names:&quot;, model.metrics_names)

if __name__ == &quot;__main__&quot;:
test_metrics_properties()

这是我的输出看到：
/path/to/your/env/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87：UserWarning：不要将 input_shape/input_dim 参数传递给层。使用顺序模型时，最好使用 Input(shape) 对象作为模型中的第一层。
super().init(activity_regularizer=activity_regularizer, **kwargs)
训练前：
指标：[&#39;loss&#39;, &#39;compile_metrics&#39;]
指标名称：[&#39;loss&#39;, &#39;compile_metrics&#39;]
训练后：
指标：[&#39;loss&#39;, &#39;compile_metrics&#39;]
指标名称：[&#39;loss&#39;, &#39;compile_metrics&#39;]

我已确保指标在编译方法中得到正确定义。
对于我构建的其他神经网络模型，它有同样的问题，我可以访问训练历史中每个单独指标的值，但指标名称不是使用 model.metrics_name 返回的正确值。
]]></description>
      <guid>https://stackoverflow.com/questions/78888263/tensorflow-2-17-0-model-metrics-not-recognized-showing-loss-compile-metri</guid>
      <pubDate>Mon, 19 Aug 2024 13:58:12 GMT</pubDate>
    </item>
    <item>
      <title>nvidia-smi 命令的 GPU 实用程序和 GPU 内存使用情况</title>
      <link>https://stackoverflow.com/questions/78887886/gpu-util-and-gpu-memory-usage-for-nvidia-smi-command</link>
      <description><![CDATA[我正在使用 8 个 GPU 运行微调实验，nvidia-smi 命令给出了以下输出
2024 年 8 月 19 日星期一 12:16:17 
+-----------------------------------------------------------------------------------------------------+ 
| NVIDIA-SMI 535.161.08 驱动程序版本：535.161.08 CUDA 版本：12.2 | |-----------------------------------------------------+----------------------+----------------------+ 
| GPU 名称 Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | 
|============================================+======================================| 
| 0 NVIDIA A100-SXM4-40GB 开启 | 00000000:10:1C.0 关闭 | 0 | | N/A 61C P0 118W / 400W | 6223MiB / 40960MiB | 4% 默认 | | | | 已禁用 |
+-----------------------------------------+-----------+-------------------------+ 
| 1 NVIDIA A100-SXM4-40GB 开启 | 00000000:10:1D.0 关闭 | 0 | | N/A 52C P0 88W / 400W | 9153MiB / 40960MiB | 14% 默认 | | | | 已禁用 |
+-----------------------------------------+----------------------+----------------------+ 
| 2 NVIDIA A100-SXM4-40GB 开启 | 00000000:20:1C.0 关闭 | 0 | | N/A 64C P0 112W / 400W | 9153MiB / 40960MiB | 14% 默认 | | | | 已禁用 |
+-----------------------------------------+-------------------------+-------------------------+ 
| 3 NVIDIA A100-SXM4-40GB 开启 | 00000000:20:1D.0 关闭 | 0 | | N/A 53C P0 93W / 400W | 9153MiB / 40960MiB | 21% 默认 | | | | 已禁用 |
+-----------------------------------------+----------------------+----------------------+ 
| 4 NVIDIA A100-SXM4-40GB 开启 | 00000000:90:1C.0 关闭 | 0 | | N/A 62C P0 103W / 400W | 9153MiB / 40960MiB | 21% 默认 | | | | 已禁用 |
+-----------------------------------------+-------------------------+-------------------------+ 
| 5 NVIDIA A100-SXM4-40GB 开启 | 00000000:90:1D.0 关闭 | 0 | | N/A 53C P0 116W / 400W | 9153MiB / 40960MiB | 0% 默认 | | | | 已禁用 |
+-----------------------------------------+----------------------+----------------------+ 
| 6 NVIDIA A100-SXM4-40GB 开启 | 00000000:A0:1C.0 关闭 | 0 | | N/A 65C P0 388W / 400W | 9191MiB / 40960MiB | 6% 默认 | | | | 已禁用 |
+-----------------------------------------+-------------------------+-------------------------+ 
| 7 NVIDIA A100-SXM4-40GB 开启 | 00000000:A0:1D.0 关闭 | 0 | | N/A 44C P0 83W / 400W | 423MiB / 40960MiB | 0% 默认 | | | | 已禁用 |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------------------+ 
| 进程：| | GPU GI CI PID 类型 进程名称 GPU 内存 | | ID ID 使用情况 | 
|====================================================================================================| 
| 0 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 1 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 2 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 3 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 4 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 5 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 6 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 7 N/A N/A 198 C /usr/bin/python3 0MiB |
+-----------------------------------------------------------------------------------+

虽然 GPU 利用率超过 0%，但所有 8 个 GPU 的 GPU 内存使用率均为 0。我不明白为什么在 GPU 利用率上升时内存使用率会为 0？]]></description>
      <guid>https://stackoverflow.com/questions/78887886/gpu-util-and-gpu-memory-usage-for-nvidia-smi-command</guid>
      <pubDate>Mon, 19 Aug 2024 12:22:46 GMT</pubDate>
    </item>
    <item>
      <title>处理 CNN 数据集批次大小的非精确划分</title>
      <link>https://stackoverflow.com/questions/78887768/handling-non-exact-division-in-batch-size-for-datasets-in-cnn</link>
      <description><![CDATA[我正在开展一个 CNN 项目，需要一些帮助来调整步骤和批处理参数。以下是我的数据集摘要：
训练图像：5360
验证图像：1151
测试图像：1147

我已设置以下参数：
迭代次数：20
批次大小：16

要计算每个迭代的步数，我使用此代码：
nb_train_steps = np.ceil(train_data_gen.samples / batch_size).astype(int)
nb_validation_steps = np.ceil(valid_data_gen.samples / batch_size).astype(int)
nb_test_steps = np.ceil(test_data_gen.samples / batch_size).astype(int)

但是，我遇到了以下错误训练期间：
Epoch 2/20
2024-08-19 01:58:11.161693：I tensorflow/core/framework/local_rendezvous.cc:404] 本地会合正在中止，状态为：OUT_OF_RANGE：序列结束
[[{{node IteratorGetNext}}]]
C:\Users\anuja\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:158：UserWarning：您的输入数据不足；中断训练。确保您的数据集或生成器至少可以生成 `steps_per_epoch * epochs` 批次。构建数据集时可能需要使用 `.repeat()` 函数。
self.gen.throw(value)
2024-08-19 01:58:11.190224: I tensorflow/core/framework/local_rendezvous.cc:404] 本地会合正在中止，状态为：OUT_OF_RANGE：序列结束
[[{{node IteratorGetNext}}]]
回溯（最近一次调用最后一次）：
文件“c:\Users\anuja\Desktop\Project\Scripts\model.py”，第 140 行，位于 &lt;module&gt;
custom_model.fit(
文件 &quot;C:\Users\anuja\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;，第 122 行，位于 error_handler 中
raise e.with_traceback(filtered_tb) from None
文件 &quot;C:\Users\anuja\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\backend\tensorflow\trainer.py&quot;，第 354 行，位于 fit 中
&quot;val_&quot; + name: val for name, val in val_logs.items()
^^^^^^^^^^^^^^
AttributeError: &#39;NoneType&#39; 对象没有属性 &#39;items&#39;

问题似乎是验证和测试数据集有不能被批次大小完全整除的图像数量。例如：

对于验证集：1151 / 16 = 71.9375。当四舍五入到 72 时，程序会期望目录中不存在的额外图像。
我预计测试集也会出现类似的问题。

我考虑过删除图像，使总数很容易被 16 整除（例如，将 1151 减少到 1136，这样 1136 / 16 = 71），但这感觉很浪费。我知道 .repeat() 函数可能会有所帮助，但我不确定它会如何影响模型，特别是在过度拟合方面。
这是我的数据生成代码：
datagen = ImageDataGenerator(rescale=1./255)

train_data_gen = datagen.flow_from_directory(
directory=train_path,
target_size=(224, 224),
batch_size=batch_size,
class_mode=&#39;categorical&#39;
)

valid_data_gen = datagen.flow_from_directory(
directory=val_path,
target_size=(224, 224),
batch_size=batch_size,
class_mode=&#39;categorical&#39;
)

test_data_gen = datagen.flow_from_directory(
directory=test_path,
target_size=(224, 224),
batch_size=batch_size,
class_mode=&#39;categorical&#39;,
shuffle=False
)

我没有使用任何数据增强，因为我的数据集相当平衡。
这里是 model.fit():
custom_model.fit(
train_data_gen,
steps_per_epoch=nb_train_steps,
validation_data=valid_data_gen,
validation_steps=nb_validation_steps,
epochs=nb_epochs,
callbacks=[es, chkpt]
)

我的问题：

我应该如何在不删除任何数据的情况下处理这种情况？
使用 .repeat() 会如何影响模型，尤其是过度拟合？
有没有更好的方法来处理这个问题？

我使用：
TensorFlow 版本：2.17.0
Keras 版本：3.4.1
]]></description>
      <guid>https://stackoverflow.com/questions/78887768/handling-non-exact-division-in-batch-size-for-datasets-in-cnn</guid>
      <pubDate>Mon, 19 Aug 2024 11:53:50 GMT</pubDate>
    </item>
    <item>
      <title>如何优化 Google Colab 中的视频帧捕获和处理以实现实时 YOLO 对象检测？</title>
      <link>https://stackoverflow.com/questions/78887653/how-can-i-optimize-video-frame-capture-and-processing-in-google-colab-for-real-t</link>
      <description><![CDATA[我已经训练了一个 YOLO V 10X 模型来检测工业或建筑工人的安全参数，包括安全帽、夹克、靴子、手套等个人防护装备。但是，我无法在 Colab 环境中进行实时检测。我必须将该模型应用于通过网络摄像头进行视频捕获的实时检测。如何解决这个问题。
我尝试了 colab 环境来解决同样的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78887653/how-can-i-optimize-video-frame-capture-and-processing-in-google-colab-for-real-t</guid>
      <pubDate>Mon, 19 Aug 2024 11:27:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用基于机器学习的插件优化 Android Studio 中的实时能耗分析</title>
      <link>https://stackoverflow.com/questions/78887516/how-can-i-optimize-real-time-energy-consumption-analysis-in-android-studio-using</link>
      <description><![CDATA[我目前正在为 Android Studio 开发一个插件，该插件使用机器学习模型检测 Android 代码中的能源气味。该插件旨在在开发人员编写或修改代码时实时分析代码，识别潜在的能源效率低下并提供优化建议。
挑战：该插件需要执行复杂的分析，包括使用预先训练的 ML 模型进行实时特征提取和推理，而不会明显减慢 IDE 的速度或中断开发人员的工作流程。
关键问题：

实时处理：如何高效地实时处理代码更改，确保分析跟上开发人员的输入，而不会导致 IDE 出现明显的滞后或卡顿？
以下是我目前如何挂接代码编辑器的事件监听器来触发分析的片段。我已经使用 Kotlin 实现了这一点。

val editorListener = object : DocumentListener {
override fun documentChanged(event: DocumentEvent) {
val newText = event.document.text
analyzeCode(newText)
}
}

private fun analyzeCode(code: String) {
executorService.submit {
val features = extractFeatures(code)
val result = model.predict(features)
updateUI(result)
}
}


每次文档更改时都会触发 analyzeCode 方法，但随着代码库的增长，这可能会导致性能问题。

资源管理：可以采用哪些策略来最大限度地减少插件在能量气味检测期间的 CPU 和内存使用量，尤其是在处理大型代码库时？

目前，我正在尝试使用ExecutorService 用于后台处理，但保持插件轻量级仍然具有挑战性：
private val executorService = Executors.newSingleThreadExecutor()

private fun extractFeatures(code: String): FeatureVector {
// 此处为特征提取逻辑
}

private fun updateUI(result: AnalysisResult) {
// 使用分析结果更新 IDE，而不阻塞主线程
}



模型优化：是否有特定的技术可以优化 ML 模型（例如量化、模型修剪）以减少推理期间的计算开销，同时保持准确性？

我正在使用 TensorFlow Lite 进行推理，但即使使用轻量级模型，仍然存在一些滞后：
val explainer = Interpreter(loadModelFile())

private fun predict(features: FeatureVector): AnalysisResult {
val output = Array(1) { FloatArray(1) }
interpretation.run(features.toArray(), output)
return AnalysisResult(output[0][0])
}



异步操作：如何有效地实现异步操作或将密集型任务卸载到后台线程，而不会影响实时反馈循环？

当前方法将任务卸载到后台线程，但在提供反馈方面仍然存在明显的延迟：
executorService.submit {
val result = model.predict(features)
SwingUtilities.invokeLater {
updateUI(result)
}
}


我通过挂接到 Android Studio 代码编辑器的文档更改事件，将实时分析功能集成到插件中。我使用 ExecutorService 将 ML 模型推理卸载到后台线程，并实施 TensorFlow Lite 进行轻量级推理。目标是使分析与开发人员的代码更改保持同步，而不会导致任何明显的性能问题。
我希望插件能够实时高效地分析代码，在开发人员输入时立即提供有关能量气味的反馈，而不会导致 IDE 中出现任何滞后或卡顿。
虽然分析按预期运行，但处理过程中存在明显的延迟，尤其是在较大的项目中。IDE 偶尔会变得响应迟缓，影响开发人员的工作流程。尽管使用了后台线程和轻量级模型，但性能仍然不是最佳的，尤其是在快速连续进行多次代码更改时。]]></description>
      <guid>https://stackoverflow.com/questions/78887516/how-can-i-optimize-real-time-energy-consumption-analysis-in-android-studio-using</guid>
      <pubDate>Mon, 19 Aug 2024 10:49:55 GMT</pubDate>
    </item>
    <item>
      <title>我如何将自己的自定义图像转换为 vggface2 模型以供自己使用？[关闭]</title>
      <link>https://stackoverflow.com/questions/78886847/how-can-i-my-own-custom-images-to-vggface2-model-for-my-own-use</link>
      <description><![CDATA[我从 GitHub vggface2.pt 下载了 vggface 模型，我可以向该模型添加我自己的自定义图像和标签吗？例如，我有 20 张自己的图像，我想添加到 vggface 供自己使用，我可以这样做吗？
我下载了 vggface 模型，我想将自己的图像添加到模型中并对其进行训练]]></description>
      <guid>https://stackoverflow.com/questions/78886847/how-can-i-my-own-custom-images-to-vggface2-model-for-my-own-use</guid>
      <pubDate>Mon, 19 Aug 2024 07:50:24 GMT</pubDate>
    </item>
    <item>
      <title>如何从 yolov8 检测到的图像中的对象中提取边界框</title>
      <link>https://stackoverflow.com/questions/78886670/how-to-extract-bounding-boxes-from-the-object-detected-in-an-image-for-yolov8</link>
      <description><![CDATA[from ultralytics import YOLO
import cv2

def detect_and_visualize_objects_yolov8(image_path, model, confidence_threshold=0.5):
image = cv2.imread(image_path)
results = model(image)

boxes = []
confidences = []

for result in results:
for box in result.boxes:
if box.conf &gt; confidence_threshold:
# 提取边界框坐标
--&gt; x1, y1, x2, y2 = box.xyxy[0].item(), box.xyxy[1].item(), box.xyxy[2].item(), box.xyxy[3].item()
x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
w = x2 - x1
h = y2 - y1
boxes.append([x1, y1, w, h])
confidences.append(float(box.conf))

# 绘制边界框
cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
label = f&#39;{box.cls} {box.conf:.2f}&#39;
cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# 计算检测到的物体的总面积
total_area = sum(w * h for _, _, w, h in boxes)

# 显示带有边界框的图像
cv2.imshow(&#39;Detected Objects&#39;, image)
cv2.waitKey(0)
cv2.destroyAllWindows()

return total_area

# 加载 YOLOv8 模型

model = YOLO(&#39;yolov8n.pt&#39;)

# 用法

image_path = &#39;data.jpg&#39;
total_area = detect_and_visualize_objects_yolov8(image_path, model)
print(f&#39;Total area of​​detectedobjects: {total_area}&#39;)

我在提取边界框坐标时遇到了问题。我的目标是编写一段代码，用于提取在推理 yolov8 模型时检测到的对象的面积总和。运行上述代码会导致此错误
 RuntimeError：无法将具有 4 个元素的 Tensor 转换为 Scalar

这是图片...
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78886670/how-to-extract-bounding-boxes-from-the-object-detected-in-an-image-for-yolov8</guid>
      <pubDate>Mon, 19 Aug 2024 07:04:18 GMT</pubDate>
    </item>
    <item>
      <title>Val_accuracy 正在改变，有时它在补码之间交替（100％-val_acc）</title>
      <link>https://stackoverflow.com/questions/78885395/val-accuracy-inst-changing-and-sometimes-it-alternates-between-it-complement-10</link>
      <description><![CDATA[我被分配根据我读过的一篇论文来实现一个机器学习模型。
这篇论文实现了一个用于属性分类的多任务学习模型（带标签的图像是模型输入，带标签的意思是属性注释，每幅图像有 40 个）。
它是一个多任务学习模型，因为在模型输入层和 40 个属性分支之后有一个共享的密集层，每个分支都有自己的损失函数（所有分支的二元交叉熵）和自己的 S 型激活函数（在最后一层，用于预测 40 个属性中的每一个是否存在于图像中）。
经过大量艰苦的努力，它终于开始在所有分支上返回所有 S 型函数的概率，但只有 val_accuracy 的概率是错误的：val_loss 和损失（训练损失）越来越小，acc（训练准确度）也在正常的概率值范围内，除了 val_accuracy 总是相同的值或它的补码。
例如（仅举 5 个时期为例）：
40 个分支之一的一个属性预测的准确度：
5_o_Clock_Shadow_Accuracy
0 0.823665
1 0.891178
2 0.891178
3 0.891178

同一属性的损失：
 5_o_Clock_Shadow_loss
0 0.921046
1 0.701494
2 0.913597
3 0.765397
4 0.894950

val_loss：
val_5_o_Clock_Shadow_loss
0 730232.750000
1 300412.500000
2 376215.843750
3 0.747685
4 1.607191

最后是 val_Accuracy：
val_5_o_Clock_Shadow_Accuracy
0 0.882382
1 0.117618
2 0.882382
3   0.882382 4 0.882382  我的模型： def subnet(shared_layers_output, i): att_branch = Dense(512, name=&#39;dense_&#39;+str(i)+&#39;_1&#39;)(shared_layers_output) att_branch = ReLU()(att_branch) att_branch = BatchNormal ization()(att_branch) att_branch = Dropout(0.5)(att_branch) att_branch = Dense(512, name=&#39;dense_&#39;+str(i)+&#39;_2&#39;)(att_branch) att_branch = ReLU()(att_branch) att_branch = BatchNormalization()(att_branch) att_branch = Dropout(0.5)(att_branch)

branch_output = Dense(1, name=att_list[i],activation=&#39;sigmoid&#39;)(att_branch)

return branch_output

def multi_task_model():

#输入
input_layer = Input(shape=(512,), name=&#39;input_layer&#39;)

#共享网络（1 个网络）
shared_x = Dense(512, name=&#39;shared_dense_layer&#39;)(input_layer)
shared_x = ReLU()(shared_x)
shared_x = BatchNormalization()(shared_x)
shared_x = Dropout(0.5)(shared_x)

branch_outputs = list()
for i in range(40):
branch_outputs.append(subnet(shared_x, i))

model = Model(input_layer, branch_outputs, name=&#39;model&#39;)

返回模型


训练和测试输入形状：(n_samples, 512)
训练和测试标签输入形状：(40, n_samples)
学习率：1e-03
]]></description>
      <guid>https://stackoverflow.com/questions/78885395/val-accuracy-inst-changing-and-sometimes-it-alternates-between-it-complement-10</guid>
      <pubDate>Sun, 18 Aug 2024 18:38:14 GMT</pubDate>
    </item>
    <item>
      <title>回归决策树中用户定义的杂质</title>
      <link>https://stackoverflow.com/questions/78884108/user-defined-impurity-in-regression-decision-trees</link>
      <description><![CDATA[我正在从 R 迁移到 PySpark。我有一个创建回归树的过程，该树目前使用 R 的 rpart 算法构建。
在 PySpark 中配置时，我无法看到指定自定义
自定义杂质函数的选项。我有一个倾斜的数据集，我不想在公式中使用均值和方差/标准差作为节点杂质的标准，而是想使用更适合我的倾斜数据的指标。
如何在 PySpark 中定义自定义杂质函数？
我查看了决策树回归的文档，并且impurity 参数的文档仅提到对 variance 的支持

impurity = Param(parent=&#39;undefined&#39;, name=&#39;impurity&#39;, doc=&#39;用于信息增益计算的标准（不区分大小写）。支持的选项：方差&#39;)

是否有任何解决方法来定义自定义杂质函数？]]></description>
      <guid>https://stackoverflow.com/questions/78884108/user-defined-impurity-in-regression-decision-trees</guid>
      <pubDate>Sun, 18 Aug 2024 08:38:47 GMT</pubDate>
    </item>
    <item>
      <title>Cartpole 强化学习 Python</title>
      <link>https://stackoverflow.com/questions/78883565/cartpole-reinforcement-learning-python</link>
      <description><![CDATA[我已经为 cartpole 环境和强化学习编写了代码，但我不知道从哪里开始“保存”强化学习的进度，以便我可以重新运行该程序来继续训练它，以提高程序的性能。如何做到这一点？
我的程序：
import gym
import numpy as np
import time
env = gym.make(&quot;CartPole-v1&quot;, render_mode=&#39;human&#39;)
(state, _) = env.reset()
env.render()
env.step(0)
env.observation_space
env.observation_space.high
env.action_space
env.spec
env.spec.max_episode_steps
env.spec.reward_threshold
episodeNumber = 10000
timeSteps = 100
for episodeIndex in range(episodeNumber):
initial_state = env.reset
env.render()
appendedObservations = []
for timeIndex in range(timeSteps):
random_action = env.action_space.sample()
observation,奖励，终止，截断，信息 = env.step(random_action)
print(&quot;step&quot;, timeIndex, 观察，奖励，终止，信息)
appendedObservations. append(观察)
time.sleep(0.01)
if(终止)：
time.sleep(0.1)
break
env.close()
]]></description>
      <guid>https://stackoverflow.com/questions/78883565/cartpole-reinforcement-learning-python</guid>
      <pubDate>Sun, 18 Aug 2024 01:16:41 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn 的分类报告中的支持是否意味着原始数据集或输入模型的数据集中的出现？[重复]</title>
      <link>https://stackoverflow.com/questions/78883478/does-support-in-sklearns-classification-report-mean-occurences-within-original</link>
      <description><![CDATA[我实现了一个机器学习模型；为了获得有关模型性能的一些信息，我查看了来自 sklearn.metrics 的分类报告。
例如，这是我的分类报告：
分类报告图片
我有两个问题：

正类和负类旁边的支持值（56 和 3147）与底部宏和加权平均值旁边的支持值（3203 和 3203）有什么区别，我应该使用哪一个？
从这个 SO 问题，支持是每个类别中有多少个样本类。这是原始数据集中的样本，还是输入到机器学习模型中的样本？我之所以问这个问题，是因为我确实进行了重新采样，因为数据集是不平衡的。换句话说，正确的支持值是基于原始（不平衡）数据集还是输入到模型中的数据集（平衡）？

对于我的第一个问题，我相信“正确”的支持值是 3203 和 3203。这与我的第二个问题类似，因为我认为支持是基于输入到模型中的数据集，所以它应该是平衡的（因为模型如何“看到”原始数据集）？
顺便说一句，一切都在管道中，因此没有数据泄漏或模型“看到”测试数据，如果这可能相关的话。
我的问题与上面链接中的问题不是重复的，因为我问的是整个分类报告，而不仅仅是其中的一部分。]]></description>
      <guid>https://stackoverflow.com/questions/78883478/does-support-in-sklearns-classification-report-mean-occurences-within-original</guid>
      <pubDate>Sat, 17 Aug 2024 23:48:10 GMT</pubDate>
    </item>
    <item>
      <title>在sklearn的ClassificationReport中，科学“使用”的度量宏是平均值还是加权平均值？[关闭]</title>
      <link>https://stackoverflow.com/questions/78883328/in-sklearns-classificationreport-is-the-scientifically-used-metric-macro-ave</link>
      <description><![CDATA[在 Python 的 sklearn.metrics 中，我使用分类报告来帮助我解释不平衡数据集上的机器学习模型。例如，这是一个任意分类报告：
分类报告
在报告指标（例如精度）时，我们报告的是宏平均值（0.51）还是加权平均值（0.98）？我会假设是宏平均值，因为这会惩罚正类上较差的模型表现。我这样说对吗？
我不认为这是这个的重复问题，因为我基本上是在问使用宏还是加权平均值更有用（或在实践中使用更多）。]]></description>
      <guid>https://stackoverflow.com/questions/78883328/in-sklearns-classificationreport-is-the-scientifically-used-metric-macro-ave</guid>
      <pubDate>Sat, 17 Aug 2024 21:45:05 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 GPU 上加载一次 YOLO 模型并将其提供给多个 Python 进程？</title>
      <link>https://stackoverflow.com/questions/78603046/is-it-possible-to-load-a-yolo-model-on-gpu-once-and-give-it-to-multiple-python-p</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78603046/is-it-possible-to-load-a-yolo-model-on-gpu-once-and-give-it-to-multiple-python-p</guid>
      <pubDate>Mon, 10 Jun 2024 14:56:16 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：预期标量类型为 Long，但发现的是 Float（Pytorch）</title>
      <link>https://stackoverflow.com/questions/70279287/runtimeerror-expected-scalar-type-long-but-found-float-pytorch</link>
      <description><![CDATA[我尝试过多次来修复，也使用了 functional.py 中的示例代码，但仍然得到了相同的“loss”值。我该如何解决这个问题？
我的库：
导入 matplotlib.pyplot 作为 plt
导入 torch
导入 torch.nn 作为 nn
导入 numpy 作为 np
导入 matplotlib
导入 pandas 作为 pd
从 torch.autograd 导入变量
从 torch.utils.data 导入 DataLoader、TensorDataset
从 sklearn.model_selection 导入 train_test_split
导入警告
导入 os
导入 torchvision
导入 torchvision.datasets 作为 dsets
导入 torchvision.transforms 作为 transforms

train=pd.read_csv(&quot;train.csv&quot;,dtype=np.float32)

targets_numpy = train.label.values
features_numpy = train.loc[:,train.columns != &quot;label&quot;].values/255 # 标准化

features_train、features_test、targets_train、targets_test = train_test_split(features_numpy、targets_numpy、test_size = 0.2、random_state = 42)

featuresTrain=torch.from_numpy(features_train)
targetsTrain=torch.from_numpy(targets_train)

featuresTest=torch.from_numpy(features_test)
targetsTest=torch.from_numpy(targets_test) 

batch_size=100
n_iterations=10000
num_epochs=n_iterations/(len(features_train)/batch_size)
num_epochs=int(num_epochs)

train=torch.utils.data.TensorDataset(featuresTrain、targetsTrain)
test=torch.utils.data.TensorDataset(featuresTest,targetsTest)

print(type(train))

train_loader=DataLoader(train,batch_size=batch_size,shuffle=False)
test_loader=DataLoader(test,batch_size=batch_size,shuffle=False)
print(type(train_loader))

plt.imshow(features_numpy[226].reshape(28,28))
plt.axis(&quot;off&quot;)
plt.title(str(targets_numpy[226]))
plt.show()

class ANNModel(nn.Module):
def __init__(self,input_dim,hidden_​​dim,output_dim):
super(ANNModel,self).__init__()
self.fc1=nn.Linear(input_dim,hidden_​​dim)
self.relu1=nn.ReLU()
self.fc2=nn.Linear(hidden_​​dim,hidden_​​dim)
self.tanh2=nn.Tanh()
self.fc4=nn.Linear(hidden_​​dim,output_dim)

def forward (self,x): #forward 到上一层并返回 
out=self.fc1(x) 
out=self.relu1(out) 
out=self.fc2(out) 
out=self.tanh2(out)
out=self.fc4(out)
return out 

input_dim=28*28
hidden_​​dim=150 
output_dim=10

model=ANNModel(input_dim,hidden_​​dim,output_dim)

error=nn.CrossEntropyLoss()

learning_rate=0.02
optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)

count=0
loss_list=[]
iteration_list=[]
accuracy_list = []
for epoch in range(num_epochs):
for i,(images,labels) in enumerate(train_loader): 
train=Variable(images.view(-1,28*28))
labels=Variable(labels)
#print(labels)
#print(outputs) 
optimizer.zero_grad()

#forward propagation
output=model(train)

#outputs=torch.randn(784,10,requires_grad=True)
##labels=torch.randn(784,10).softmax(dim=1)
loss=error(outputs,labels)

loss.backward()

optimizer.step()

count+=1

if count % 50 == 0:
correct=0
total=0

for images,labels in test_loader:
test=Variable(images.view(-1,28*28)) 
output=model(test)

predict=torch.max(outputs.data,1)[1] #mantık???

total+= len(labels)

correct+=(predicted==labels).sum()

accuracy=100 *correct/float(total)

loss_list.append(loss.data)
iteration_list.append(count)
accuracy_list.append(accuracy)
if count % 500 == 0:
print(&#39;迭代：{} 损失：{} 准确度：{} %&#39;.format(count, loss.data, accuracy))

错误：
-------------------------------------------------------------------------------
RuntimeError Traceback（最近一次调用最后一次）
&lt;ipython-input-9-9e53988ad250&gt;在 &lt;module&gt;() 中
26 #outputs=torch.randn(784,10,requires_grad=True)
27 ##labels=torch.randn(784,10).softmax(dim=1)
---&gt; 28 loss=error(outputs,labels)
29 
30 

2 帧
/usr/local/lib/python3.7/dist-packages/torch/nn/ functional.py 在 cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing) 中
2844 如果 size_average 不为 None 或 reduce 不为 None:
2845 reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 2846 返回 torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
2847 
2848 

RuntimeError：预期标量类型为 Long，但发现为 Float

]]></description>
      <guid>https://stackoverflow.com/questions/70279287/runtimeerror-expected-scalar-type-long-but-found-float-pytorch</guid>
      <pubDate>Wed, 08 Dec 2021 17:29:11 GMT</pubDate>
    </item>
    </channel>
</rss>