<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 27 Nov 2024 15:18:24 GMT</lastBuildDate>
    <item>
      <title>我应该使用什么 ML/优化算法来找到最大输出的最佳输入值？</title>
      <link>https://stackoverflow.com/questions/79229880/what-ml-optimization-algorithm-should-i-use-to-find-the-optimum-input-values-for</link>
      <description><![CDATA[我有一些数据，需要找到 X*Y 的最佳组合，以便因变量 A、B、C 达到最大值。
所有这些都是简单的数值，就像科学实验的观察结果一样。因此，A、B、C 是在同一范围内具有不同值的性能变量，而 X 和 Y 是两个测量变量。
基本上，我想制作一个程序来不断处理未来实验中涌入的任何数据。
我遇到过有人使用 ANN 回归来解决类似的问题，还有人建议使用梯度下降。由于我必须从头开始完成这项任务，如果我能就我应该从哪种算法开始提出建议，我将非常高兴。]]></description>
      <guid>https://stackoverflow.com/questions/79229880/what-ml-optimization-algorithm-should-i-use-to-find-the-optimum-input-values-for</guid>
      <pubDate>Wed, 27 Nov 2024 10:35:42 GMT</pubDate>
    </item>
    <item>
      <title>Python 和 Maple SVR 结果中的差异</title>
      <link>https://stackoverflow.com/questions/79229459/discrepancy-in-python-and-maple-svr-results</link>
      <description><![CDATA[我正在使用最小二乘 SVR 方法求解积分方程。我应该将下面链接中提供的 Maple 代码转换为 Python。我已编写如下所示的 Python 代码，但无论我做什么，都无法获得准确的结果。您能告诉我如何让我的代码产生与 Maple 相同的结果吗？
https://github.com/alirezaafzalaghaei/LSSVR-FIE/blob/master/paper-examples/example-4/CLS-SVR-dual.mw
import numpy as np
from scipy.integrate import quad
from scipy.special import legendre
from scipy.optimize import minimal
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from scipy.stats import qmc
from scipy.integrate import quad
from scipy.integrate import dblquad
import time
from tensorflow.keras import regularizers
from tensorflow.keras.losses import MeanSquaredError

# 设置精度
np.set_printoptions(precision=15)

# 定义积分函数
def Quad(f, a, b):
# 注意：这是一个简化版本，您可能需要根据要使用的具体求积方法进行调整。
return dblquad(f, a, b,a,b)[0]

# 定义区间
a, b = 0, 1

# 定义精确函数
def exact(x,y):
return 1/((x+y+1)**2)

# 定义函数 f
def f(x,y):
return 1/((x+y+1)**2)-(x/(6*(8+y)))

# 定义函数 k
def k(x, y,t,s):
return (x/((8+y)*(1+t+s)))

# 示例用法（替换为您的具体用例）
result_quad = Quad(f, a, b)
print(result_quad)
import numpy as np
from scipy.special import legendre
from scipy.optimize import fsolve

def shift(x):
return (2 * x - a - b) / (b - a)

gamma = 10 ** 8
M = 4
N = M + 1

a, b = 0, 1 # 定义 a 和 b

# 计算勒让德多项式的根
train1 = fsolve(lambda x: legendre(N)(shift(x)), np.linspace(0, 1, N))
train2 = fsolve(lambda y: legendre(N)(shift(y)), np.linspace(0, 1, N))

# 使用 NumPy 的 meshgrid 和 stacking 创建 N x 2 矩阵
X, Y = np.meshgrid(train1, train2)
train = np.stack((X.flatten(), Y.flatten()), axis=1)

print(train)
print(train.shape) # 输出形状以确认
phi = []
L_phi = []
for kindx in range((M + 1) * (M + 1)):
i = kindx // (M + 1) + 1
j = kindx % (M + 1) + 1
#print(i,j)
# 创建 i 次勒让德多项式
p = legendre(i)
# 使用嵌套函数定义 phi[i] 以创建新范围
def make_phi(p=p): # 在嵌套函数的参数中捕获 p
return lambda x: p(shift(x))
phi.append(make_phi())
L_phi.append(lambda x,y: phi[i](x)*phi[j](y) - dblquad(lambda t,s: k(x,y, t,s) 
* phi[i](t)*phi[j](s), a, b,a,b)[0])

train = np.array([train1, train2]).T #假设 train1 和 train2 是列表或 
数组
A = np.zeros((M+1, M+1))
for m in range(M+1):
for n in range(M+1):
A[m, n] = L_phi[m](train[n, 0], train[n, 1])
print(A)
Omega = np.dot(A.T, A) + np.identity(M+1) / gamma
GAMMA = np.array([f(train1[i],train2[i]) for i in range(M+1)]).reshape(M+1, 1)
alpha = resolve(Omega, GAMMA)
import numpy as np
l=[]
import numpy as np
from scipy.linalg import resolve

# ... (其他导入和函数) ...

def u_tilde(x, y):

# 仅对 x 和 y 处 phi 中的前 N ​​个勒让德多项式进行求值
# 这与 A 的维度一致
P_x = np.array([phi[i](x) for i in range(M+1)]).reshape(-1, 1) # 此处更改
P_y = np.array([phi[i](y) for i in range(M+1)]).reshape(-1, 1) # 此处更改

# 使用逐元素乘法和求和计算 u˜(x)
u_tilde_x = alpha.T @ A.T @ (P_x * P_y) # 此处更改

return u_tilde_x[0, 0] # 从结果中提取标量值

# ...（其余代码）...# 从结果中提取标量值

return result
# 示例用法：
x_value = train1
y_value = train2
for k in range(M+1):
u_tilde_at_x = u_tilde(x_value[k],y_value[k])
l.append(u_tilde_at_x)
print(f&quot;u˜({x_value[k]}) = {u_tilde_at_x}&quot;)
#u_tilde_at_x = u_tilde(x_value)
#print(f&quot;u˜({x_value}) = {u_tilde_at_x}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79229459/discrepancy-in-python-and-maple-svr-results</guid>
      <pubDate>Wed, 27 Nov 2024 08:30:38 GMT</pubDate>
    </item>
    <item>
      <title>如何让康威生命游戏成为一个 AI/ML 项目？[关闭]</title>
      <link>https://stackoverflow.com/questions/79229377/how-to-make-conways-game-of-life-an-ai-ml-project</link>
      <description><![CDATA[所以我对我的 AI 模块进行了评估。我想开发一些类似于康威生命游戏的东西。当我对其进行了一些研究时，我认为默认版本本身并不是 AI/ML 项目。我想创建一个自我玩的模拟游戏。
我考虑过操纵生存的概念。例如：
细胞需要食物才能生存，只有当细胞有 1 块钱时才能吃食物，钱可以通过前往钱块来获得。每次移动到钱块都需要 1 能量。细胞必须进食才能获得能量。能量会随着时间的推移而耗尽。
我已经学习了一份人工智能概念列表，我必须从中将 2-3 个概念应用于项目。这些是：
树搜索
深度优先搜索
广度优先搜索
动态规划
统一成本搜索
学习成本
A* 搜索
松弛
马尔可夫决策过程
策略评估
价值迭代
强化学习
蒙特卡罗方法
游戏，expectimax
Minimax，expectiminimax
评估函数
Alpha-beta 剪枝
时间差异 (TD) 学习
同步游戏
最先进的技术
我对 AI 和 Python 还很陌生。对此了解不多。
有人能告诉我我的项目是否朝着正确的方向发展吗？还是我必须放弃它并寻找其他东西？在我添加上述变量后，我正在考虑的项目会成为 AI/ML 项目吗？]]></description>
      <guid>https://stackoverflow.com/questions/79229377/how-to-make-conways-game-of-life-an-ai-ml-project</guid>
      <pubDate>Wed, 27 Nov 2024 07:58:22 GMT</pubDate>
    </item>
    <item>
      <title>寻找洞察聚类机器学习项目[关闭]</title>
      <link>https://stackoverflow.com/questions/79228750/looking-for-insights-clustering-machine-leaning-prohect</link>
      <description><![CDATA[我正在为高中开展一个涉及聚类（k 均值和 DBSCAN）的机器学习项目
我抓取了一个电子商务网站 (StockX)，并对某些商品（包括设计师品牌等）的零售价值 (x) 和转售价值 (y) 进行聚类。然后对它们进行聚类。
最终的聚类结果非常基础，有 3 个聚类 - 围绕低零售/转售、中等零售/转售和高零售/转售价格。
我想知道你们是否对我可以用数据和我的项目做的更细微的事情有什么建议。如果有更多有趣的发现，我可以尝试挖掘出来。]]></description>
      <guid>https://stackoverflow.com/questions/79228750/looking-for-insights-clustering-machine-leaning-prohect</guid>
      <pubDate>Wed, 27 Nov 2024 01:58:21 GMT</pubDate>
    </item>
    <item>
      <title>大型多 GPU ML 训练作业的 GPU 间流量</title>
      <link>https://stackoverflow.com/questions/79228728/inter-gpu-traffic-for-large-multi-gpu-ml-training-jobs</link>
      <description><![CDATA[对于具有不同并行类型（如数据、张量、管道等）的分布式多 GPU 大型机器学习作业，我正在寻找点对点、全对全、全归约等 GPU 间流量的模式和百分比。是否有关于此的研究/数据？
大多数研究都讨论数据并行，其中全归约类型的流量占分配梯度的大多数。]]></description>
      <guid>https://stackoverflow.com/questions/79228728/inter-gpu-traffic-for-large-multi-gpu-ml-training-jobs</guid>
      <pubDate>Wed, 27 Nov 2024 01:44:11 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试创建多尺度 CNN，但遇到此错误：RuntimeError：mat1 和 mat2 形状无法相乘（32x4095 和 4096x4096）</title>
      <link>https://stackoverflow.com/questions/79228528/i-am-trying-to-create-multiscale-cnn-but-facing-this-error-runtimeerror-mat1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79228528/i-am-trying-to-create-multiscale-cnn-but-facing-this-error-runtimeerror-mat1</guid>
      <pubDate>Tue, 26 Nov 2024 23:06:22 GMT</pubDate>
    </item>
    <item>
      <title>Keras 神经网络回归模型优先考虑 2 个输出值，如何才能让它更好地概括？[关闭]</title>
      <link>https://stackoverflow.com/questions/79228286/keras-neural-network-regression-model-prioritizes-2-output-values-over-the-rest</link>
      <description><![CDATA[我正在尝试使用其他特征预测附加数据集中的“温度”值。执行代码时，模型对两个值有明显的偏差。我正在使用 plotly 图显示预测的准确性，其中包含真实值和预测值以及表示最佳预测的线。我的预测准确性
因此，如您所见，有两个主要的信息集群，表明我的模型主要选择这两个值作为输出。我不知道为什么会发生这种情况，也不知道我可以做些什么来补救。我将链接我正在使用的 .csv 文件和代码（google Drive / Colab）
此外，当删除预处理步骤时，它会产生类似的效果，但有三个主要集群而不是两个。 https://drive.google.com/drive/folders/1uSHTVAmW-UXhutZa5-Tf1QcB_e9cl_DR?usp=sharing
我尝试过：

删除预处理步骤。

添加特征工程和通过相关性进行数据选择。

排除分类值。


我尝试过神经网络的大小和密度，增加或减少以查看它是否是欠拟合问题。
我引入了自动超参数选择，最多100 次试验。
我在神经网络中添加了批处理和特征归一化。
我手动调整了超参数的值，例如时期、激活函数等。
我使用了 KMeans 来降低密度。
我尝试了不同的数据分割。
我预计分布会略有变化，但它总是水平聚集（与预测值一起）。]]></description>
      <guid>https://stackoverflow.com/questions/79228286/keras-neural-network-regression-model-prioritizes-2-output-values-over-the-rest</guid>
      <pubDate>Tue, 26 Nov 2024 21:09:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么（远程） Jupyter 在 ML 训练期间很忙，但实际上却没有做任何事情？</title>
      <link>https://stackoverflow.com/questions/79226995/why-is-remote-jupyter-busy-during-ml-training-but-not-actually-doing-anything</link>
      <description><![CDATA[我正在使用 PyTorch 在自己的专用远程服务器上训练 ML 模型，使用 Jupyter 作为我的 IDE。
大约 120 个 epoch（训练大约 2 小时），Jupyter 单元停止更新输出，但状态栏仍显示内核状态为 busy，SSH 连接仍处于活动状态。
我认为训练可能仍在继续，但输出单元停止更新，因为它包含太多输出。为了验证这个假设，我昨晚让 Jupyter 运行了大约 7 个小时。当我醒来时，它已经在 123 个 epoch 时停止更新输出单元，当我终止执行并打印出当前 epoch 数时，它只达到了 126 个 epoch。
知道是什么原因造成的吗？]]></description>
      <guid>https://stackoverflow.com/questions/79226995/why-is-remote-jupyter-busy-during-ml-training-but-not-actually-doing-anything</guid>
      <pubDate>Tue, 26 Nov 2024 13:55:11 GMT</pubDate>
    </item>
    <item>
      <title>将请求上下文从 FastAPI 传递到用于 OpenAI 集成的 Microsoft Semantic Kernel 插件</title>
      <link>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</link>
      <description><![CDATA[我正在 FastAPI 应用程序中将 Microsoft Semantic Kernel 与 OpenAI 集成。我有一个聊天/端点，我从请求中收到一个 session_id，我需要将此 session_id 与 openai_client 一起传递给插件。但是，我不确定如何在内核的执行过程中将 FastAPI 请求中的 session_id 正确传递给插件。
以下是设置内核和插件的相关代码：
# 内核和服务设置
kernel = Kernel()

execution_settings = AzureChatPromptExecutionSettings(tool_choice=&quot;auto&quot;)
execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={})

openai_client = OpenAI(api_key=api_key)
chat_completion_service = OpenAIChatCompletion(
ai_model_id=model_id, 
api_key=api_key, 
service_id=service_id 
)

# 添加服务和插件
kernel.add_service(chat_completion_service)
kernel.add_plugin(MovesPlugin(openai_client), plugin_name=&#39;MovesPlugin&#39;)

在我的 FastAPI 端点内，我想在调用内核进行聊天响应时将 session_id 传递给插件：
# 在 FastAPI 端点内
@app.post(&quot;/chat/&quot;)
async def chat(request: Request):
session_id = await request.json().get(&#39;session_id&#39;)

# 获取聊天完成服务
_chat_completion_service = kernel.get_service(type=ChatCompletionClientBase)

# 获取聊天完成响应
response = await _chat_completion_service.get_chat_message_content(
chat_history=chat_history,
kernel=kernel,
settings=execution_settings
)

return响应

如何将请求上下文 (session_id) 从 FastAPI 请求传递到 MovesPlugin，并确保它与语义内核执行中的 openai_client 一起正确使用？
如能得到任何指导或建议，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</guid>
      <pubDate>Tue, 26 Nov 2024 12:51:34 GMT</pubDate>
    </item>
    <item>
      <title>在 Python ML 项目中构建目录的最佳方法是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79226565/what-is-the-best-way-to-structure-the-directories-in-a-python-ml-project</link>
      <description><![CDATA[我想知道为 ML Python 项目构建目录的适当方法；以使项目易于阅读、维护并使包的导入正常工作。（即，从命令行和 IDE 运行时导入都能顺利运行，而无需在 PYTHONPATH 或类似的东西中硬编码更改）。
例如，我将以这种方式构建一个项目（这只是一个示例，用于说明，文件夹可能包含更多 .py 文件、具有类定义的文件等）：
+---configuration_files
+---data
+---models
+---README.md
+---report.md
+---requirements.txt
+---shell_scripts
\---src
| data_cleaning.py
| data_preprocessing.py
| preprocessed_data_analysis.py
| raw_data_analysis.py
| test.py
| train.py
|
\---modules
| __init__.py
|
+---data_analysis
| data_analysis_utils.py
| __init__.py
|
+---data_handling
| data_loaders.py
| __init__.py
|
+---data_preprocessing
| data_preprocessing_utils.py
| __init__.py
|
+---general_utils
| general_utils.py
| __init__.py
|
+---test
| test_utils.py
| __init__.py
|
\---train
trainer.py.py
train_functions_1.py
train_functions_2.py
__init__.py

您对此有何看法？
例如，将必须直接运行的脚本放在 src 目录中，使导入更容易；
但是我觉得将它们放在 data_preprocessing、data_analysis、train、test 等文件夹中，每个文件夹都有各自的包和模块，这样会更简洁。但是，这会在从应该由所有人共享的 general_utils 包导入时引入问题。
此外，src、modules、utils 等使用名称的约定是什么？我应该有一个 main.py  脚本吗？（即使我有不同的阶段）
一般来说，在行业中构建此类项目的最有效和最广泛使用的方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79226565/what-is-the-best-way-to-structure-the-directories-in-a-python-ml-project</guid>
      <pubDate>Tue, 26 Nov 2024 11:44:32 GMT</pubDate>
    </item>
    <item>
      <title>在使用 dataloader 测试数据集时，我们应该设置 shuffle=true 吗？或者这无所谓？</title>
      <link>https://stackoverflow.com/questions/79212687/in-testing-dataset-using-dataloader-should-we-set-shuffle-true-or-it-doesnt-m</link>
      <description><![CDATA[我有一个自定义数据集（披萨、寿司和牛排的图片）。
我正在使用 torch DataLoader 来处理它，现在在编写测试数据加载器自定义时，我们应该设置 shuffle=true 还是这无关紧要？？
我还没有看到区别，只是问一般情况。]]></description>
      <guid>https://stackoverflow.com/questions/79212687/in-testing-dataset-using-dataloader-should-we-set-shuffle-true-or-it-doesnt-m</guid>
      <pubDate>Thu, 21 Nov 2024 19:33:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在 google colab 中使用从 kaggle 加载的数据（实际使用它）</title>
      <link>https://stackoverflow.com/questions/79195592/how-to-use-loaded-data-from-kaggle-in-google-colab-to-actually-work-with-it</link>
      <description><![CDATA[因此，我最近从此 https://www.kaggle.com/datasets/mostafaabla/garbage-classification 网站导入了数据集。尽管我在 google colab 中的文件中有它（已解压和所有这些东西），但我不知道如何在代码本身中实现它。就像来自 tensorflow 的 Fashion mnist 教程 https://www.tensorflow.org/tutorials/keras/classification?hl 它加载为
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
如何将数据导入/加载到代码单元并通过分成类来处理它（因为在该教程数据集中有多个类，而在我的自定义数据集中有 12 个）
请问如何操作？
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# 定义训练和验证目录的路径
train_dir = &#39;garbage-classification/train&#39;
val_dir = &#39;garbage-classification/validation&#39;

# 创建 ImageDataGenerator 进行数据增强
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

# 从目录加载图像
train_generator = train_datagen.flow_from_directory(
train_dir,
target_size=(150, 150), # 根据需要调整图像大小
batch_size=32,
class_mode=&#39;categorical&#39; # 如果有多个类，请使用 &#39;categorical&#39;
)

validation_generator = val_datagen.flow_from_directory(
val_dir,
target_size=(150, 150),
batch_size=32,
class_mode=&#39;categorical&#39;
)

我使用 perplexity 尝试解决，结果得到了这个。显然它没有起作用，所以..]]></description>
      <guid>https://stackoverflow.com/questions/79195592/how-to-use-loaded-data-from-kaggle-in-google-colab-to-actually-work-with-it</guid>
      <pubDate>Sat, 16 Nov 2024 16:34:39 GMT</pubDate>
    </item>
    <item>
      <title>为什么基于 Tensorflow.js 的天气预测模型无法预测正确的天气</title>
      <link>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</guid>
      <pubDate>Sun, 26 May 2024 18:36:09 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法使用 XGBoostRegressor 获取预测的概率？</title>
      <link>https://stackoverflow.com/questions/55003557/is-there-a-way-to-get-the-probability-of-a-prediction-using-xgboostregressor</link>
      <description><![CDATA[我已经构建了一个 XGBoostRegressor 模型，该模型使用大约 200 个分类特征来预测连续的时间变量。
但我希望获得实际预测和该预测的概率作为输出。有没有办法从 XGBoostRegressor 模型中获取这些信息？
所以我同时想要  和 P(Y|X) 作为输出。有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/55003557/is-there-a-way-to-get-the-probability-of-a-prediction-using-xgboostregressor</guid>
      <pubDate>Tue, 05 Mar 2019 13:06:32 GMT</pubDate>
    </item>
    <item>
      <title>如何计算机器学习模型的复杂性</title>
      <link>https://stackoverflow.com/questions/53384906/how-to-compute-the-complexity-of-machine-learning-models</link>
      <description><![CDATA[我正在研究深度学习模型与车辆网络通信安全应用的比较。我想知道如何计算这些模型的复杂性，以了解我提出的模型的性能。我正在使用 tensorflow ]]></description>
      <guid>https://stackoverflow.com/questions/53384906/how-to-compute-the-complexity-of-machine-learning-models</guid>
      <pubDate>Tue, 20 Nov 2018 01:19:52 GMT</pubDate>
    </item>
    </channel>
</rss>