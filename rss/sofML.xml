<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 25 Jan 2024 12:27:12 GMT</lastBuildDate>
    <item>
      <title>SHAP特征选择</title>
      <link>https://stackoverflow.com/questions/77879894/shap-feature-selection</link>
      <description><![CDATA[形状特征选择是通过计算形状值并将特征重要性最低的特征一一剔除来进行的。
我的问题是，找到特征重要性，删除重要性最低的特征，然后再次找到特征重要性，并逐一删除最低的特征，这样正确吗？
以ROC-AUC分数作为衡量指标，该值反复下降和上升。所以，我想知道根据最初获得的特征重要性（无需再次计算特征重要性）按照重要性最低的顺序将它们一一删除是否正确。
xgb.fit(X_train, y_train)
解释器_xgb = shap.TreeExplainer(xgb)
shap_values_xgb = 解释器_xgb(X_test)

df_shap_values_xgb = pd.DataFrame(data = shap_values_xgb.values, columns=X_test_xgb.columns)
df_feature_importance_xgb = pd.DataFrame(columns=[&#39;feature&#39;,&#39;importance&#39;])
对于 df_shap_values_xgb.columns 中的 col_xgb：
    important_xgb = df_shap_values_xgb[col_xgb].abs().mean()
    df_feature_importance_xgb.loc[len(df_feature_importance_xgb)] = [col_xgb,importance_xgb]
df_feature_importance_xgb = df_feature_importance_xgb11.sort_values(&#39;重要性&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/77879894/shap-feature-selection</guid>
      <pubDate>Thu, 25 Jan 2024 12:14:10 GMT</pubDate>
    </item>
    <item>
      <title>我可以摆脱这个吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77879676/can-i-get-rid-of-this</link>
      <description><![CDATA[我是编码初学者。
我正在尝试某种第一个机器学习项目，并且发生了此错误。
告诉我如何解决这个问题，“AttributeError: module &#39;pandas&#39; has no attribute &#39;Dataframe&#39;”
还告诉我将来如何解决此类错误：
]]></description>
      <guid>https://stackoverflow.com/questions/77879676/can-i-get-rid-of-this</guid>
      <pubDate>Thu, 25 Jan 2024 11:43:10 GMT</pubDate>
    </item>
    <item>
      <title>如何从 AutoModelForSequenceClassification 重置参数？</title>
      <link>https://stackoverflow.com/questions/77879635/how-to-reset-parameters-from-automodelforsequenceclassification</link>
      <description><![CDATA[目前，要重新初始化 AutoModelForSequenceClassification 的模型，我们可以这样做：
从变压器导入 AutoModel、AutoConfig、AutoModelForSequenceClassification

m =“moussaKam/frugalscore_tiny_bert-base_bert-score”
config = AutoConfig.from_pretrained(m)
model_from_scratch = AutoModel（配置）

model_from_scratch.save_pretrained(“frugalscore_tiny_bert-from_scratch”)

模型 = AutoModelForSequenceClassification(
  “frugalscore_tiny_bert-from_scratch”，local_files_only=True
）

是否有某种方法可以重新初始化模型权重，而不保存使用 AutoConfig 初始化的新预训练模型？
模型 = AutoModelForSequenceClassification(
  “moussaKam/frugalscore_tiny_bert-base_bert-score”，
  local_files_only=真
  reinitialize_weights=True
）

或者类似的东西：
模型 = AutoModelForSequenceClassification(
  “moussaKam/frugalscore_tiny_bert-base_bert-score”，
  local_files_only=真
）

model.reinitialize_parameters()
]]></description>
      <guid>https://stackoverflow.com/questions/77879635/how-to-reset-parameters-from-automodelforsequenceclassification</guid>
      <pubDate>Thu, 25 Jan 2024 11:34:07 GMT</pubDate>
    </item>
    <item>
      <title>获取“运行时错误：计算出的每个通道的填充输入大小：(2)。内核大小：(10)。内核大小不能大于实际输入大小”</title>
      <link>https://stackoverflow.com/questions/77879568/getting-runtimeerror-calculated-padded-input-size-per-channel-2-kernel-siz</link>
      <description><![CDATA[我创建了一个Python代码来将印地语音频文件转换为文本。
我正在使用“theainerd/Wav2Vec2-large-xlsr-hindi”转换模型。
下面是我的代码：
从变压器导入 Wav2Vec2Processor, Wav2Vec2ForCTC
导入声音文件
进口火炬

# 加载模型和处理器
模型 = Wav2Vec2ForCTC.from_pretrained(&quot;theainerd/Wav2Vec2-large-xlsr-hindi&quot;)
处理器 = Wav2Vec2Processor.from_pretrained(“theainerd/Wav2Vec2-large-xlsr-hindi”)

# 加载并准备音频

soundfile.write(“resampled_audio.wav”, 音频, 16000)

音频，sampling_rate = soundfile.read(“resampled_audio.wav”)
输入=处理器（音频，采样率=采样率，return_tensors=“pt”）

# 转录音频
使用 torch.no_grad()：
    输出=模型（**输入）
    Predicted_ids = torch.argmax(outputs.logits, dim=-1)
    转录=处理器.batch_decode(predicted_ids)

# 打印转录的文本
打印（转录[0]）

当我在 jupyter 笔记本中运行此代码时收到错误消息：
运行时错误：计算出的每个通道的填充输入大小：(2)。内核大小：(10)。内核大小不能大于实际输入大小
请帮助我如何修复此运行时错误。
提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/77879568/getting-runtimeerror-calculated-padded-input-size-per-channel-2-kernel-siz</guid>
      <pubDate>Thu, 25 Jan 2024 11:25:21 GMT</pubDate>
    </item>
    <item>
      <title>如何在Python中测量纯分类数据的Huang K-modes聚类模型的性能？</title>
      <link>https://stackoverflow.com/questions/77879280/how-to-measure-performance-of-huang-k-modes-clustering-model-for-pure-categorica</link>
      <description><![CDATA[如何衡量Huang K-modes聚类模型的性能？
我正在运行 Huang K-modes 聚类模型，其中我的主要数据帧都是分类数据（字符串）。如何衡量该模型的性能？
我尝试测量数据矩阵中每一列与簇的调整后兰德分数，然后取平均值，这是正确的，但不确定这对我有任何意义！
如果您能分享您的意见并表示感谢，我将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/77879280/how-to-measure-performance-of-huang-k-modes-clustering-model-for-pure-categorica</guid>
      <pubDate>Thu, 25 Jan 2024 10:43:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 textattack 进行 Bert 攻击并获取 ValueError(f"Failed to import file {args.dataset_from_file}")</title>
      <link>https://stackoverflow.com/questions/77878231/bert-attack-using-textattack-and-getting-valueerrorffailed-to-import-file-arg</link>
      <description><![CDATA[我想在我的预训练模型和数据集上使用文本攻击来生成对抗样本。所以我运行这个命令：
文本攻击攻击 --model-from-file model --dataset-from-file data/data.csv --attack-recipe bert-attack --log-to-txt data/output.txt

但是，我收到此错误
raise ValueError(f“无法导入文件 {args.dataset_from_file}”)
ValueError：无法导入文件 data/data.csv`

我认为这可能是文件路径，所以我使用了绝对路径，但仍然是同样的错误。我的模型文件包含模型和分词器。我还检查了我的 csv 内容，我认为没有任何问题。
我的 csv 内容：
&lt;前&gt;&lt;代码&gt;文本
“我和妻子上周住在芝加哥阿菲尼亚酒店的单间套房，不值得额外付费，一开始就绝对是一场噩梦，因为房间被列为“超大”，而它与我们之前住过的类型完全不同住在酒店，普通客房只小了大约 2 平方英尺，我相信唯一的额外空间是在衣柜里，这不是很有用，除非你打算住在那里，我们有两个儿子和我们在一起，所以我们不得不额外的睡眠空间，我们的大儿子睡在沙发上，多次醒来，抱怨有虫子在四处走动，他一定也很不舒服，因为我白天坐在沙发上，就像一块石头一样，客房服务很糟糕晚上 10 点左右，我花了 47 分钟才给我妻子送来一个枕头，她不得不等待，因为我的家人永远不会再回到这家酒店，我强烈建议您也远离这家酒店
”
“阿菲尼亚芝加哥酒店是我住过的最糟糕的酒店之一，我作为客人受到的待遇如此差劲，当我要求无烟房间时，他们在我的房间里犯了一个错误，前台非常不通融。”预订时，由于某种奇怪的原因，没有服务员，所以我不得不把所有行李搬到电梯上，然后自己沿着长长的走廊到我的房间，如果这不是一次糟糕的住宿，我叫了客房服务，花了一个多小时如果房间里没有空调，我会说，如果您前往芝加哥进行任何类型的商务旅行，那么这次住宿的一切都非常痛苦，我希望您决定不选择这家酒店很惊讶我喜欢芝加哥这个城市，但这次住宿绝对让我的旅行变得非常负面的经历

所以我希望在运行命令后得到对抗性输出，但我遇到了错误。]]></description>
      <guid>https://stackoverflow.com/questions/77878231/bert-attack-using-textattack-and-getting-valueerrorffailed-to-import-file-arg</guid>
      <pubDate>Thu, 25 Jan 2024 07:42:54 GMT</pubDate>
    </item>
    <item>
      <title>在 pyspark 数据帧的 groupby 上实现机器学习算法，然后获得组合结果</title>
      <link>https://stackoverflow.com/questions/77878122/implement-machine-learning-algorithm-on-groupby-from-pyspark-dataframe-and-then</link>
      <description><![CDATA[我尝试在完整的数据帧上实现机器学习算法，下面是代码，但我希望在 groupby 基础上应用该算法，因为我们有不同的组，例如 group_cols=[“col1”，“col2”， “col3”]并且会有不同的组合，因此需要将相似的组分组在一起并对其应用算法并获得具有异常值分数的最终数据帧。
spark = SparkSession.builder.appName(“LOFExample”).getOrCreate()

# 假设您有一个具有功能的 DataFrame &#39;df_actual_final&#39;
feature_columns = [&quot;col5&quot;] # 根据你实际的特征列进行调整
汇编器= VectorAssembler（inputCols = feature_columns，outputCol =“特征”）
df_assembled = assembler.transform(df_actual_final)

# 提取特征作为列表
extract_features_udf = F.udf(lambda 特征: features[0].item(), DoubleType())
df_features = df_assembled.withColumn(“feature_value”, extract_features_udf(col(“features”)))

# 将特征转换为 NumPy 数组
numpy_array = np.array(df_features.select(“feature_value”).collect())


# 训练局部离群因子模型
lof = LocalOutlierFactor(contamination=0.01) # 根据需要调整污染
outlier_scores = lof.fit_predict(numpy_array)
outlier_scores_list = outlier_scores.tolist()
outlier_df = Spark.createDataFrame(enumerate(outlier_scores_list), [“id”, “outlier_scores”])
   
df_assembled_pd = df_assembled.toPandas()
outlier_df_pd=outlier_df.toPandas()
df_concat = pd.concat([df_assembled_pd, outlier_df_pd], axis=1)
result_df=spark.createDataFrame(df_concat)

result_df = result_df.withColumn(“local_outlier_flag”, when(col(“outlier_scores”) ==-1, 1).otherwise(0))

]]></description>
      <guid>https://stackoverflow.com/questions/77878122/implement-machine-learning-algorithm-on-groupby-from-pyspark-dataframe-and-then</guid>
      <pubDate>Thu, 25 Jan 2024 07:18:46 GMT</pubDate>
    </item>
    <item>
      <title>无法在 wsl2 上下载 deepspeed 库</title>
      <link>https://stackoverflow.com/questions/77877824/not-able-to-download-the-deepspeed-library-on-wsl2</link>
      <description><![CDATA[我无法在 WSL2 上下载 deepspeed 库。
我需要下载一个名为 SwissArmyTransformer 的库。但这个库需要deepspeed库。
我要下载的库是SwissArmyTransformer。该库需要 deepspeed 库。我在 WSL 上运行它。 deepspeed 库根本没有被下载。它无法获取元数据。我必须下载 deepspeed 0.11.0 或更高版本。请帮助我]]></description>
      <guid>https://stackoverflow.com/questions/77877824/not-able-to-download-the-deepspeed-library-on-wsl2</guid>
      <pubDate>Thu, 25 Jan 2024 06:09:03 GMT</pubDate>
    </item>
    <item>
      <title>使用 Top-N 特征方法去除特征的随机森林分类器</title>
      <link>https://stackoverflow.com/questions/77877253/random-forest-classifier-removing-features-using-top-n-features-method</link>
      <description><![CDATA[我正在尝试使用随机森林分类器来预测 NBA 比赛的获胜者。我试图删除和修改我的功能列表，以便提高准确性并减少噪音。
我实现了此处找到的解决方案：https:// datascience.stackexchange.com/questions/57697/decision-trees-should-we-discard-low-importance-features，其中我将循环遍历前 N 个最重要的特征并绘制出最终的准确性。在我的所有功能都经过该循环之后，我留下了一个如下所示的图：

正如您所看到的，生成的图表有点乱。我是否要删除具有负斜率的要素？或者说删除特征的门槛是多少？有没有更好的方法来计算噪声？鉴于我有如此多的特征，并且对训练数据上的模型准确性产生如此多的影响，我如何获得最准确的模型？]]></description>
      <guid>https://stackoverflow.com/questions/77877253/random-forest-classifier-removing-features-using-top-n-features-method</guid>
      <pubDate>Thu, 25 Jan 2024 02:32:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Flux `withgradient` 计算的损失与我计算的不匹配？</title>
      <link>https://stackoverflow.com/questions/77876955/why-doesnt-the-loss-calculated-by-flux-withgradient-match-what-i-have-calcula</link>
      <description><![CDATA[我正在尝试使用 Flux 训练一个简单的 CNN，但遇到了一个奇怪的问题...在训练过程中，损失似乎下降了（表明它正在工作），但尽管损失曲线表明“训练过的”模型仍然有效，模型输出非常糟糕，当我手动计算损失时，我注意到它与训练表明的结果不同（它表现得好像根本没有经过训练）。
然后我开始计算梯度内部与外部返回的损失，经过大量挖掘，我认为问题与 BatchNorm 层有关。考虑以下最小示例：
使用 Flux
x = rand(100,100,1,1) #假设一个灰度图像 100x100，具有 1 个通道（灰度）和 1 个批次
y = @。 5*x + 3 #输出图像，与输入值的一些关系（与此无关）
m = Chain(BatchNorm(1),Conv((1,1),1=&gt;1)) #非常简单的模型（实际上没有做任何事情，但说明了问题）
l_init = Flux.mse(m(x),y) #模型创建后的初始损失
l_grad, grad = Flux.withgradient(m -&gt; Flux.mse(m(x),y), m) #梯度计算的loss
l_final = Flux.mse(m(x),y) #使用模型再次计算损失（没有更新参数）
println(&quot;初始损失：$l_init&quot;)
println(&quot;用梯度计算的损失：$l_grad&quot;)
println(&quot;最终损失：$l_final&quot;)

上面所有的损失都会有所不同，有时会非常显着（刚才运行时我得到了 22.6、30.7 和 23.0），而我认为它们应该是相同的？
有趣的是，如果我删除 BatchNorm 层，输出都是相同的，即运行：
使用 Flux
x = rand(100,100,1,1) #假设一个灰度图像 100x100，具有 1 个通道（灰度）和 1 个批次
y = @。 5*x + 3 #输出图像
m = 链(Conv((1,1),1=&gt;1))
l_init = Flux.mse(m(x),y) #模型创建后的初始损失
l_grad, grad = Flux.withgradient(m -&gt; Flux.mse(m(x),y), m)
l_final = Flux.mse(m(x),y)
println(&quot;初始损失：$l_init&quot;)
println(&quot;用梯度计算的损失：$l_grad&quot;)
println(&quot;最终损失：$l_final&quot;)

每次损失计算都会产生相同的数字。
为什么包含 BatchNorm 层会像这样改变损失值？
我（有限）的理解是，这只是为了标准化输入值，我知道这可能会影响非标准化和标准化情况之间的损失，但我不明白为什么它会产生不同的损失值同一模型上的相同输入值，而没有更新该模型的任何参数？]]></description>
      <guid>https://stackoverflow.com/questions/77876955/why-doesnt-the-loss-calculated-by-flux-withgradient-match-what-i-have-calcula</guid>
      <pubDate>Thu, 25 Jan 2024 00:30:43 GMT</pubDate>
    </item>
    <item>
      <title>变压器架构的输入大小问题[关闭]</title>
      <link>https://stackoverflow.com/questions/77876582/transformer-architectures-input-size-problem</link>
      <description><![CDATA[我读了“你所需要的就是注意力”论文，描述了变压器的架构。在 Transformer 中，有一个叫做 masked multi-head Attention 的组件，仅在解码器部分使用。
问题是，解码器的输入是编码器的输出以及之前生成的标记。并且之前每次迭代生成的token数量不同，但是线性层的神经元数量是相同的。因此，我们必须使用“pad tokens”来实现。屏蔽注意力用于对这些 pad token 给予 0 注意力。
编码器也是如此。输入可以是不同的大小，所以我们还必须使用填充令牌，但在这里，我们不使用屏蔽注意力，我很好奇，为什么？
或者我们不在那里使用填充令牌，而是使用其他东西？]]></description>
      <guid>https://stackoverflow.com/questions/77876582/transformer-architectures-input-size-problem</guid>
      <pubDate>Wed, 24 Jan 2024 22:28:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyTorch 训练 VGG16 模型进行图像分类</title>
      <link>https://stackoverflow.com/questions/77872605/training-the-vgg16-model-for-image-classification-using-pytorch</link>
      <description><![CDATA[我正在使用 PyTorch 进行图像分类。
我编写了以下适用于简单线性模型的训练函数：
criterion = nn.CrossEntropyLoss()
def train（模型、数据加载器、纪元）：
模型.to（设备）
优化器 = torch.optim.Adam(model.parameters(), lr=1e-3)
运行损失，运行加速 = 0., 0.
损失历史记录 = []
precision_history = [](data_train):.2f}%&quot;)

对于范围内的 i(1, 纪元 + 1)：
  模型.train()
  对于输入，数据加载器中的目标：
      输入，目标 = 输入.to(设备), 目标.to(设备)
      输出 = 模型（输入）
      损失=标准（输出，目标）
      优化器.zero_grad()
      loss.backward()
      优化器.step()
      preds = torch.argmax(输出, 1)
      running_loss += loss.item()
      running_acc += torch.sum(preds == Targets).item()

  print(f&quot;[TRAIN epoch {i}] 损失: {running_loss/len(data_train):.2f} Acc: {100 * running_acc/len
 

我有预训练的 VGG16 模型，我想更改其最后一层的权重：
model_vgg = models.vgg16(weights=&#39;DEFAULT&#39;)
model_vgg.classifier[6] = nn.Linear(4096, 2)

对于 model_vgg.parameters() 中的参数：
    param.requires_grad = False
model_vgg.classifier[-1].requires_grad = True

火车（model_vgg，train_loader，2）

但是，在训练时出现以下错误：
RuntimeError Traceback（最近一次调用最后一次）

&lt;定时评估&gt;在&lt;模块&gt;中

&lt;ipython-input-27-1f64686a5cfd&gt;火车中（模型、数据加载器、纪元）
     39 损失 = 标准（输出，目标）
     40 优化器.zero_grad()
---&gt; 41loss.backward()
     42 优化器.step()
     43 preds = torch.argmax(输出, 1)

/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py 向后（张量，grad_tensors，retain_graph，create_graph，grad_variables，输入）
--&gt; 251 Variable._execution_engine.run_backward( # 调用 C++ 引擎来运行向后传递
    252个张量，
    第253章

RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77872605/training-the-vgg16-model-for-image-classification-using-pytorch</guid>
      <pubDate>Wed, 24 Jan 2024 11:21:50 GMT</pubDate>
    </item>
    <item>
      <title>相关矩阵的累积 AOC 计算</title>
      <link>https://stackoverflow.com/questions/77830147/cumulative-aoc-calculation-for-a-correlation-matrix</link>
      <description><![CDATA[我正在使用一个非常简单的数据集（胎儿健康分类）进行练习，使用支持向量机、相关性指标和典型模型指标（没什么特别的）进行一些练习。我想做以下事情：

采用（与目标）最相关的变量并计算 SVM 模型；然后保留 AUC 结果。
采用第二个最相关的变量（与目标）并使用第一个和第二个变量，计算 SVM 模型；然后保留 AUC 结果。
依此类推......直到到达最后一个变量

之后，我需要创建一个显示以下信息的图表：

X轴：累计变量数
Y 轴：模型中包含的每个变量数量对应的 AUC

我有以下代码；我认为这是合理的。然而，它被卡住了。我不得不中断迭代，因为它们似乎没有结束。关于如何修复循环有什么建议吗？
**导入参考文件的一些行**

df = pd.read_csv(&quot;ASI_casoPractico.csv&quot;, sep = &quot;;&quot;)

# 导入库

将 pandas 导入为 pd
从 sklearn.svm 导入 SVC
从 sklearn.metrics 导入 roc_auc_score
从 sklearn.model_selection 导入 train_test_split
将 matplotlib.pyplot 导入为 plt

# 相关矩阵

corr_matrix = df.corr().abs()
排序校正=
corr_matrix[&#39;目标&#39;].sort_values(升序=False)

# 创建按相关性排序的变量列表

Sorted_vars = Sorted_corr.index.tolist()

# 为结果创建空列表

结果=[]

# 使用变量进行迭代并使用 SVM 生成模型

对于范围内的 i(1, len(sorted_vars) + 1)：

  # 选择相关性最好的变量
  选定的变量 = 排序的变量[:i]

  # 训练和测试的数据分开

  X_train = df[selected_vars]
  y_train = df[&#39;目标&#39;]

  # 训练支持向量机

  svm = SVC(内核=&#39;线性&#39;, 概率=True)
  svm.fit(X_train, y_train)

  # 计算AUC
  y_pred = svm.predict_proba(X_train)[:, 1]
  auc = roc_auc_score(y_train, y_pred)

  # 将值添加到列表中
  结果.append([i, auc])

# 为结果创建数据框
results_df = pd.DataFrame(结果, columns=[&#39;变量&#39;, &#39;AUC&#39;])

# 图
results_df.plot(x=&#39;变量&#39;, y=&#39;AUC&#39;, kind=&#39;线&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/77830147/cumulative-aoc-calculation-for-a-correlation-matrix</guid>
      <pubDate>Wed, 17 Jan 2024 05:28:50 GMT</pubDate>
    </item>
    <item>
      <title>数据帧的 .corr() 方法不返回理想值仅返回 -1 或 1</title>
      <link>https://stackoverflow.com/questions/77214404/corr-method-for-dataframe-not-returning-ideal-values-only-returns-either-1-o</link>
      <description><![CDATA[理想情况下，它应该为每个单元格返回 -1 到 1 之间的值，但具有相同列名和行名的单元格需要具有 1 值
在执行 corr() 之前尝试用 0 替换 NaN，它返回正确的值，但这些值对于程序的目的来说是不准确的
&lt;前&gt;&lt;代码&gt;# df
            电影A 电影B 电影C 电影D 电影E
安吉 0.000000 南 -0.500000 0.500000 南
安尼维什 1.166667 -0.333333 -0.833333 NaN NaN
杰伊 1.166667 -0.333333 南 -0.833333 南
卡蒂克 0.000000 -1.500000 南 南 1.5
纳曼 南 0.250000 南 -0.250000 南

# df.T.corr()
          安吉·阿尼维什·杰伊·卡西克·纳曼
安吉 1.0 1.0 -1.0 南 南
安尼维什 1.0 1.0 1.0 1.0 NaN
杰伊 -1.0 1.0 1.0 1.0 1.0
卡蒂克 NaN 1.0 1.0 1.0 NaN
纳曼 南 南 1.0 南 1.0
]]></description>
      <guid>https://stackoverflow.com/questions/77214404/corr-method-for-dataframe-not-returning-ideal-values-only-returns-either-1-o</guid>
      <pubDate>Mon, 02 Oct 2023 09:15:04 GMT</pubDate>
    </item>
    <item>
      <title>使用属性在 scikit-learn 中进行分层训练/测试分割</title>
      <link>https://stackoverflow.com/questions/75516592/stratified-train-test-split-in-scikit-learn-using-an-attribute</link>
      <description><![CDATA[我需要将数据分为训练集 (80%) 和测试集 (20%)。我目前使用下面的代码来做到这一点：
StratifiedShuffleSplit(n_splits=10,test_size=.2, train_size=.8, random_state=0)

我如何需要指定一个特定的属性来进行分割。我做不到]]></description>
      <guid>https://stackoverflow.com/questions/75516592/stratified-train-test-split-in-scikit-learn-using-an-attribute</guid>
      <pubDate>Tue, 21 Feb 2023 05:28:10 GMT</pubDate>
    </item>
    </channel>
</rss>