<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 05 Mar 2024 00:56:36 GMT</lastBuildDate>
    <item>
      <title>如何使用 caret train() 函数对多个训练集使用重复的随机样本？</title>
      <link>https://stackoverflow.com/questions/78103028/how-to-use-repeated-random-samples-for-multiple-training-sets-with-caret-train</link>
      <description><![CDATA[我想使用重复的随机 80%/20% 分割作为训练集，因为我的只有约 800 个人，事件率为 5%。
#示例数据
dd_cleannames = data.frame(class = 样本(c(0,1),100,替换 = TRUE),var1 = 样本(c(1:5),100,替换= TRUE),var2 = 样本(c(10: 20),100,替换=真))

#为5个随机训练集创建数据分区
设置种子(100)
索引 &lt;- 插入符::createDataPartition(dd_cleannames$class, p = 0.8,times = 5,list = FALSE)

在另一个SO线程中，我找到了这个答案：https://stackoverflow.com/a/59276788/4685471&lt; /p&gt;
resample_data &lt;- tibble(
  training_sets = map(indices, ~ dd_cleannames[.x, ]),
  test_sets = map(索引, ~ dd_cleannames[-.x, ])
）

现在我创建我的控件：
ctrl = trainControl(method = &quot;LGOCV&quot;,
                    数量 = 5,
                    p = 0.8，
                    类概率 = TRUE,
                    摘要函数=两个类摘要）

但是当我尝试实现广义提升模型时，出现错误：
gbm = train(class ~ ., data = resample_data$training_sets,
            方法=“gbm”，
            trControl = ctrl,
            详细=假）
terms.formula(公式，数据 = 数据) 中的错误：
  使用“.”在数据框中重复名称“var1”

或者，除了在 createDataPartition 函数中使用 list = TRUE 之外，我尝试了相同的工作流程，但出现以下错误：
set.seed(100)
索引 &lt;- 插入符::createDataPartition(dd_cleannames$class, p = 0.8,times = 5,list = TRUE)

#training_sets = dd_cleannames[as.data.frame(indices)]

resample_data &lt;- tibble(
  training_sets = map(indices, ~ dd_cleannames[.x, ]),
  test_sets = map(索引, ~ dd_cleannames[-.x, ])
）


ctrl = trainControl(方法 = “LGOCV”,
                    数量 = 5,
                    p = 0.8，
                    类概率 = TRUE,
                    摘要函数=两个类摘要）


gbm = train(类 ~ ., 数据 = resample_data$training_sets,
            方法=“gbm”，
            trControl = ctrl,
            详细=假）
eval(predvars, data, env) 中的错误：未找到对象“Resample1.class”

然后我收到此错误：
eval(predvars, data, env) 中出现错误：未找到对象“Resample1.class”

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78103028/how-to-use-repeated-random-samples-for-multiple-training-sets-with-caret-train</guid>
      <pubDate>Mon, 04 Mar 2024 17:16:26 GMT</pubDate>
    </item>
    <item>
      <title>如何训练时间序列模型以包含最新数据[关闭]</title>
      <link>https://stackoverflow.com/questions/78103005/how-to-train-a-timeseries-model-to-include-latest-data</link>
      <description><![CDATA[这是与时间序列 ML/DL 模型相关的一般问题。我有一个模型，可以根据 10 年的历史数据进行训练，并根据 1 年的数据进行预测。编码器/输入块长度为 2 年。
假设我想要预测 2024 年。我将在 2012 年至 2021 年之间对数据进行训练，在 2022 年进行验证（需要提前停止）并在 2023 年进行测试。这意味着我计划在生产中部署的最终模型无法看到2 年的最新数据，在本例中为 2022 年和 2023 年。
有没有办法可以将晚年纳入培训中？在生产中部署的时间序列模型用于说明训练中的最新数据的最佳实践是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78103005/how-to-train-a-timeseries-model-to-include-latest-data</guid>
      <pubDate>Mon, 04 Mar 2024 17:12:56 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 会抛出除 1 之外的任何值的批量大小错误</title>
      <link>https://stackoverflow.com/questions/78102877/pytorch-throws-batch-size-error-with-any-value-but-1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78102877/pytorch-throws-batch-size-error-with-any-value-but-1</guid>
      <pubDate>Mon, 04 Mar 2024 16:51:12 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何使代码匹配以通过文档测试？</title>
      <link>https://stackoverflow.com/questions/78102706/how-should-i-make-the-code-match-up-to-pass-doctests</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78102706/how-should-i-make-the-code-match-up-to-pass-doctests</guid>
      <pubDate>Mon, 04 Mar 2024 16:23:09 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中利用 Nesterov Momentum 实现全批量梯度下降？</title>
      <link>https://stackoverflow.com/questions/78102637/how-to-implement-full-batch-gradient-descent-with-nesterov-momentum-in-pytorch</link>
      <description><![CDATA[我正在 PyTorch 中开展一个机器学习项目，我需要使用全批量梯度下降法来优化模型。关键要求是优化器应使用数据集中的所有数据点进行每次更新。我对现有 torch.optim.SGD 优化器的挑战是，它本身并不支持在一次更新中使用整个数据集。这对于我的项目至关重要，因为我需要优化过程来考虑所有数据点，以确保对模型参数进行最准确的更新。
此外，我想在优化过程中保留 Nesterov 动量的使用。据我所知，人们可能会修改批处理大小以等于整个数据集，从而使用 SGD 优化器模拟完整的批处理更新。然而，我感兴趣的是是否有一种更优雅或更直接的方法在 PyTorch 中实现真正的梯度下降优化器，同时也支持 Nesterov 动量。
理想情况下，我正在寻找有关如何在 PyTorch 中实现或配置满足以下条件的优化器的解决方案或指南：

利用整个数据集进行每次参数更新（真正的梯度下降行为）。
结合 Nesterov 动量以实现更高效的收敛。
通过子类化 torch.optim.Optimizer 与 PyTorch 生态系统的其余部分兼容
]]></description>
      <guid>https://stackoverflow.com/questions/78102637/how-to-implement-full-batch-gradient-descent-with-nesterov-momentum-in-pytorch</guid>
      <pubDate>Mon, 04 Mar 2024 16:10:18 GMT</pubDate>
    </item>
    <item>
      <title>如何将 AI 与 GitHub Pull Request 集成？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78102459/how-can-i-integrate-ai-with-github-pull-requests</link>
      <description><![CDATA[我的项目是开发一个人工智能驱动的工具，可以在 GitHub PR 上触发。应该以当前项目的代码库为基础，检查PR中发生变化的行，并在此基础上提出可能的改进代码质量的建议。重要的是，主要目标不是发现一般代码异味，而更像是基于存储库遵循编码约定/模式/建议的建议。
我不希望有人给我关于如何完成整个项目的确切指示。我正在寻找任何可能已经了解如何解决此问题、哪些工具最适合此用例或有任何建议的知识或想法的人。]]></description>
      <guid>https://stackoverflow.com/questions/78102459/how-can-i-integrate-ai-with-github-pull-requests</guid>
      <pubDate>Mon, 04 Mar 2024 15:39:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Optuna 中修剪随机森林回归？</title>
      <link>https://stackoverflow.com/questions/78101963/how-to-prune-random-forest-regression-in-optuna</link>
      <description><![CDATA[我正在研究机器学习模型并尝试使用 Optuna 调整超参数。我想尝试修剪，但我不知道如何实现这个功能。我正在使用随机森林回归器，一切正常。
def 目标（试用）：
    n_estimators = Trial.suggest_int(&#39;n_estimators&#39;, 100, 1000)
    最大深度 = Trial.suggest_int(&#39;最大深度&#39;, 5, 50)
    min_samples_split = Trial.suggest_int(&#39;min_samples_split&#39;, 2, 30)
    min_samples_leaf = Trial.suggest_int(&#39;min_samples_leaf&#39;, 1, 10)
    max_samples = Trial.suggest_float(&#39;max_samples&#39;, 0.5, 1.0)
    max_features = Trial.suggest_int(&#39;max_features&#39;, 5, 30)
    max_leaf_nodes = Trial.suggest_int(&#39;max_leaf_nodes&#39;, 100, 200)

    模型 = RandomForestRegressor(n_estimators=n_estimators,
                              最大深度=最大深度，
                              min_samples_split=min_samples_split,
                              min_samples_leaf=min_samples_leaf,
                              最大样本数=最大样本数，
                              最大特征=最大特征，
                              max_leaf_nodes=max_leaf_nodes)

    k 折叠 = K 折叠(n_splits=5)
    分数 = cross_val_score(模型, X_train_transformed, y_train, cv=kFold, 评分=&#39;r2&#39;, n_jobs=-1)
    mean_score = np.mean(分数)

    返回平均值


研究 = optuna.create_study(方向 = &#39;最大化&#39;,
                        采样器=optuna.samplers.TPESampler(multivariate=True))
研究.优化（目标，n_Trials=300）

如何对目标函数实施剪枝？]]></description>
      <guid>https://stackoverflow.com/questions/78101963/how-to-prune-random-forest-regression-in-optuna</guid>
      <pubDate>Mon, 04 Mar 2024 14:28:11 GMT</pubDate>
    </item>
    <item>
      <title>传递矩阵密钥时邻居索引错误</title>
      <link>https://stackoverflow.com/questions/78101850/neighbors-indexing-error-when-passing-matrix-key</link>
      <description><![CDATA[我正在创建一个衣服推荐系统，使用 NearestNeighbors，数据来自 2 个数据集，其中一个带有 ratings.csv，在本例中为 0 和 1，基于是否保存到愿望清单，以及包含所有衣服的衣服.csv，我想要传递服装的 ID 并获取推荐商品的列表，但我收到索引错误。
这是代码：
user_ ratings_df = pd.read_csv(“ ratings.csv”)

user_ ratings_df[&#39;IDGARMENT&#39;] = user_ ratings_df[&#39;IDGARMENT&#39;].astype(int)

# 读入数据；使用默认的 pd.RangeIndex，即 0、1、2 等作为列
Clothes_desc = pd.read_csv(“clothes.csv”, on_bad_lines=&#39;skip&#39;)
Clothing_metadata = Clothing_desc[[&#39;IDGARMENT&#39;, &#39;描述&#39;, &#39;类别&#39;, &#39;品牌&#39;, &#39;价格&#39;]]

衣服元数据[&#39;IDGARMENT&#39;] = 衣服元数据[&#39;IDGARMENT&#39;].astype(int)
Clothes_data = user_ ratings_df.merge(clothes_metadata, on=&#39;IDGARMENT&#39;)

user_item_matrix = user_ ratings_df.pivot(index=[&#39;USERID&#39;], columns=[&#39;IDGARMENT&#39;], value=&#39;RATING&#39;).fillna(0)
用户项矩阵

# 定义一个关于余弦相似度的 KNN 模型
cf_knn_model=NearestNeighbors(metric=&#39;cosine&#39;,algorithm=&#39;brute&#39;,n_neighbors=10,n_jobs=-1)
#lr.fit(x.reshape(-1, 1), y)

# 将模型拟合到我们的矩阵上
cf_knn_model.fit(user_item_matrix)


def dress_recommender_engine(garment_id, 矩阵, cf_model, n_recs):
    # 在矩阵上拟合模型
    cf_knn_model.fit（矩阵）
    
    # 计算邻居距离
    距离，索引 = cf_model.kneighbors(matrix[garment_id], n_neighbors=n_recs)
    Clothing_rec_ids = Sorted(list(zip(indices.squeeze().tolist(),distances.squeeze().tolist())),key=lambda x: x[1])[:0:-1]
    
    # 存储推荐的列表
    cf_recs = []
    对于我在 dress_rec_ids 中：
        cf_recs.append({&#39;Desc&#39;:clothes_desc[&#39;DESCRIPTION&#39;][i[0]],&#39;距离&#39;:i[1]})
    
    # 选择需要的最多推荐数量
    df = pd.DataFrame(cf_recs, 索引 = 范围(1,n_recs))
    返回df


n_recs = 10
dress_recommender_engine（54448，user_item_matrix，cf_knn_model，n_recs）

我得到的错误是：
&lt;前&gt;&lt;代码&gt;&gt; *keyError Traceback（最近一次调用最后）文件
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802,
&gt;在Index.get_loc（self，key，method，tolerance）3801中尝试：
&gt; -&gt; [第 3802 章] 第 3803 章
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:138,
&gt;在 pandas._libs.index.IndexEngine.get_loc() 文件中
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:165,
&gt;在 pandas._libs.index.IndexEngine.get_loc() 文件中
&gt; pandas/_libs/hashtable_class_helper.pxi:2263，在
&gt; pandas._libs.hashtable.Int64HashTable.get_item() 文件
&gt; pandas/_libs/hashtable_class_helper.pxi:2273，位于
&gt; pandas._libs.hashtable.Int64HashTable.get_item() KeyError：54448
&gt;上述异常是以下异常的直接原因：
&gt; KeyError Traceback（最近调用
&gt;最后）单元格 In[4]，第 64 行
&gt; 59 返回 df
&gt; 63 n_recs = 10
&gt; ---&gt; 64 dress_recommender_engine(54448, user_item_matrix, cf_knn_model, n_recs) 单元格 In[4]，第 48 行，in
&gt; dress_recommender_engine(garment_id, 矩阵, cf_model, n_recs)
&gt; 42 cf_knn_model.fit（矩阵）
&gt; 44 # 提取输入的电影ID
&gt;第45话
&gt; 46
&gt; 47 # 计算邻居距离
&gt; ---&gt; 48 个距离，索引 = cf_model.kneighbors(matrix[garment_id], n_neighbors=n_recs)
&gt;第49章 衣服
&gt; x: x[1])[:0:-1]
&gt; 51 # 存储推荐的列表 File ~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3807, in
&gt;第3805章1：
&gt;第3806章
&gt; -&gt;第3807章 第3808章 第3809章
&gt; 〜/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804，
&gt;在Index.get_loc（self，key，method，tolerance）3802返回
&gt; self._engine.get_loc(casted_key) 3803 除了 KeyError 为错误：
&gt; -&gt;第3804章 3805 错误：3806
&gt;第3807章否则我们会失败并重新加注
&gt;第3808章第3809章
&gt;密钥错误：54448*

我已将 IDGarment 转换为 int，但这似乎导致了某种问题，知道如何解决吗？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78101850/neighbors-indexing-error-when-passing-matrix-key</guid>
      <pubDate>Mon, 04 Mar 2024 14:12:06 GMT</pubDate>
    </item>
    <item>
      <title>如何使用人工神经网络（ANN）根据我收集的参数来预测导热率？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78101211/how-to-use-artificial-neural-network-ann-to-predict-thermal-conductivity-based</link>
      <description><![CDATA[我刚刚完成了一项关于特定纳米流体热导率的研究，收集了具有各种参数的数据集，例如纳米流体成分、温度和电导率值。我想使用人工神经网络 (ANN) 根据我收集的参数来预测热导率，我受到了几篇类似主题的研究论文的启发，我对启动前馈 ANN 有疑问，例如我将使用哪种软件以及我需要什么技能，所以请任何有开发类似 ANN 模型（如本研究论文中的模型）经验的人
https://sci-hub.se/10.1016/j.csite.2021.101055
如果您能让我大致了解如何启动人工神经网络 (ANN) 以根据我收集的参数预测导热率，请写一条关于如何启动的消息
没什么，我只是收集了特定水平的纳米流体成分、温度和电导率值等数据，我是纳米流体专家，而不是人工智能专家]]></description>
      <guid>https://stackoverflow.com/questions/78101211/how-to-use-artificial-neural-network-ann-to-predict-thermal-conductivity-based</guid>
      <pubDate>Mon, 04 Mar 2024 12:25:51 GMT</pubDate>
    </item>
    <item>
      <title>一维 CNN 预测图与实际时间序列图不匹配</title>
      <link>https://stackoverflow.com/questions/78100096/1d-cnn-predictions-plot-mismatch-with-actual-time-series-plot</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78100096/1d-cnn-predictions-plot-mismatch-with-actual-time-series-plot</guid>
      <pubDate>Mon, 04 Mar 2024 09:11:20 GMT</pubDate>
    </item>
    <item>
      <title>我在使用石榴时遇到问题，它不允许我使用 DiscreteDistribution [关闭]</title>
      <link>https://stackoverflow.com/questions/78100004/i-had-a-problem-using-pomegranate-that-it-wont-let-me-use-discretedistribution</link>
      <description><![CDATA[a = DiscreteDistribution({&#39;1&#39;: 1./10, &#39;0&#39;: 9./10})

在这一行中，代码不断抛出错误，在您说“是”之前，我已经安装了石榴并且也导入了它，但仍然显示此错误。我问过 chatgpt，据它说我的代码完全没问题。您认为问题出在哪里？石榴的版本会影响我的代码吗？我是菜鸟，这些东西我都不懂。
我试图创建贝叶斯网络，并且我试图获得一个简单图的概率。我在第一行写了： from pomegranate import * 。]]></description>
      <guid>https://stackoverflow.com/questions/78100004/i-had-a-problem-using-pomegranate-that-it-wont-let-me-use-discretedistribution</guid>
      <pubDate>Mon, 04 Mar 2024 08:55:30 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 询问脑电图分类的建议</title>
      <link>https://stackoverflow.com/questions/67236791/asking-advice-on-eeg-classification-using-keras</link>
      <description><![CDATA[我有一个脑电图数据集，形状如下：
&lt;前&gt;&lt;代码&gt;(11,1158, 200)

哪里
11为EEG通道数
1158是每个任务的编号
200是每个任务的时间间隔

例如，如果您绘制一个任务，您将得到（请注意，数据已标准化）：

该任务代表一个带有类的任务。 （例如，查看第 2 类的图片，我的数据集中的类总数为 5）。
现在我将数组转换为这种形状：
&lt;前&gt;&lt;代码&gt;(1158, 200, 11)

以便模型能够区分每个任务。这是我使用的模型：
opt = keras.optimizers.Adam(learning_rate=1e-4)

模型=顺序（）
model.add(Conv1D(filters=128, kernel_size=64,activation=&#39;relu&#39;, input_shape=(200, 11)))
model.add(Conv1D(filters=64, kernel_size=8,activation=&#39;relu&#39;))
模型.add(Dropout(0.5))
model.add(MaxPooling1D(pool_size=2))
模型.add(压平())
model.add（密集（100，激活=&#39;relu&#39;））
model.add（密集（5，激活=&#39;softmax&#39;））
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=opt, 指标=[&#39;accuracy&#39;])
model.fit（x_train，y_train，validation_data =（x_valid，y_valid），epochs = 50，batch_size = 16）

我尝试了许多不同的超参数，但我所有的结果都有点像这样：
纪元 50/50
58/58 [==============================] - 0s 5ms/步 - 损失：0.1281 - 准确度：0.9946 - val_loss ：2.7850 - val_accuracy：0.1897

训练准确率很高，但验证准确率在 20% 到 25% 之间（100/5 = 20，其中 5 是类数）；这基本上意味着模型预测随机的东西。我的做法有错吗？如果是这样，我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/67236791/asking-advice-on-eeg-classification-using-keras</guid>
      <pubDate>Fri, 23 Apr 2021 21:01:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit-learn OneHotEncoder 时如何处理分类数据中的缺失值 (NaN)？</title>
      <link>https://stackoverflow.com/questions/62409303/how-to-handle-missing-values-nan-in-categorical-data-when-using-scikit-learn-o</link>
      <description><![CDATA[我最近开始学习Python，以便使用机器学习方法为一个研究项目开发预测模型。我有一个由数值数据和分类数据组成的大型数据集。数据集有很多缺失值。我目前正在尝试使用 OneHotEncoder 对分类特征进行编码。当我阅读有关 OneHotEncoder 的内容时，我的理解是，对于缺失值 (NaN)，OneHotEncoder 会将 0 分配给所有功能的类别，如下所示：

&lt;前&gt;&lt;代码&gt;0 男
1 女
2 南

应用 OneHotEncoder 后：

&lt;前&gt;&lt;代码&gt;0 10
1 01
2 00

但是，当运行以下代码时：
 # 编码分类数据
    从 sklearn.compose 导入 ColumnTransformer
    从 sklearn.preprocessing 导入 OneHotEncoder


    ct = ColumnTransformer([(&#39;编码器&#39;, OneHotEncoder(handle_unknown=&#39;忽略&#39;), [1])],
                           余数=&#39;直通&#39;）
    obj_df = np.array(ct.fit_transform(obj_df))
    打印（obj_df）


我收到错误ValueError：输入包含NaN
所以我猜测我之前对 OneHotEncoder 如何处理缺失值的理解是错误的。
有没有办法让我获得上述功能？我知道在编码之前估算缺失值可以解决这个问题，但我不愿意这样做，因为我正在处理医疗数据，并且担心估算可能会降低模型的预测准确性。 
我发现这个问题类似，但答案没有提供关于如何处理 NaN 值的足够详细的解决方案。
请告诉我您的想法，谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/62409303/how-to-handle-missing-values-nan-in-categorical-data-when-using-scikit-learn-o</guid>
      <pubDate>Tue, 16 Jun 2020 13:09:16 GMT</pubDate>
    </item>
    <item>
      <title>在google colab中解压7z文件？</title>
      <link>https://stackoverflow.com/questions/49955814/unzip-a-7z-file-in-google-colab</link>
      <description><![CDATA[我正在尝试在 Kaggle 的 Amazon from Space 数据集上编写 CNN。我现在不能花钱。所以，我想使用Google colab。我已使用 kaggle cli 工具成功下载了数据集。但是，我无法提取数据。请帮助我。
]]></description>
      <guid>https://stackoverflow.com/questions/49955814/unzip-a-7z-file-in-google-colab</guid>
      <pubDate>Sat, 21 Apr 2018 12:33:48 GMT</pubDate>
    </item>
    <item>
      <title>对谷歌的离线语音识别支持[关闭]</title>
      <link>https://stackoverflow.com/questions/49699533/offline-speech-recognition-support-for-google</link>
      <description><![CDATA[是否有任何嵌入式语音识别不需要互联网连接即可使用阿拉伯语？
为什么谷歌手机上的语音识别功能可以离线和在线支持英语和其他语言，而不是仅在线模式下支持阿拉伯语和其他语言？]]></description>
      <guid>https://stackoverflow.com/questions/49699533/offline-speech-recognition-support-for-google</guid>
      <pubDate>Fri, 06 Apr 2018 19:06:13 GMT</pubDate>
    </item>
    </channel>
</rss>