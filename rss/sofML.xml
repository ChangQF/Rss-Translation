<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 26 Jul 2024 18:20:20 GMT</lastBuildDate>
    <item>
      <title>涅斯捷罗夫加速梯度的两种变体：它们是等效的吗？</title>
      <link>https://stackoverflow.com/questions/78799083/two-variants-of-nesterov-accelerated-gradient-are-they-equivalent</link>
      <description><![CDATA[我很困惑地发现 Paperswithcode 上对 Nesterov 加速梯度的描述，即：
v_t = beta * v_t-1 + eta * ∇ J(theta - beta * v_t-1)
theta_t = theta_t-1 + v_t

与原始 Sutskever 等人的论文第 133 页中的描述略有不同。 3：
v_t = beta * v_t-1 - eta * ∇ J(theta + beta * v_t-1)
theta_t = theta_t-1 + v_t

在软弱的时刻，我问过你知道是谁，他围绕前瞻和后瞻制定了一个答案，但我无法验证这一点。
这两个公式是否等价，如果等价，如何证明这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78799083/two-variants-of-nesterov-accelerated-gradient-are-they-equivalent</guid>
      <pubDate>Fri, 26 Jul 2024 16:21:10 GMT</pubDate>
    </item>
    <item>
      <title>如何区分机器学习算法和“普通”算法？</title>
      <link>https://stackoverflow.com/questions/78799030/how-to-distinguish-a-machine-learning-algorithm-and-a-normal-ones</link>
      <description><![CDATA[我正在写论文，我的同事正在使用 ML 模型，而我坚持使用变化点检测模型，而不是 ML。（我见过 SVM 可用于 CPD）。
我要做的是了解变化点检测算法（不是 ML）的当前最新技术，并将其用于我的代码。问题是...
我如何区分我正在查看的算法/模型是否是 ML？]]></description>
      <guid>https://stackoverflow.com/questions/78799030/how-to-distinguish-a-machine-learning-algorithm-and-a-normal-ones</guid>
      <pubDate>Fri, 26 Jul 2024 16:07:46 GMT</pubDate>
    </item>
    <item>
      <title>在搜索系统结果上计算 NDCG 时，处理假阳性和假阴性有哪些不同的方法？</title>
      <link>https://stackoverflow.com/questions/78798774/what-are-the-different-ways-to-handle-false-positives-and-false-negatives-when-c</link>
      <description><![CDATA[上下文：
我正在使用 NDCG（归一化折扣累积增益）来评估包含相关性分数的地面实况数据集上的语义搜索系统。我想为此使用 sklearn 的 ndcg_score()。
有什么方法可以处理

假阳性文档：对于给定的查询，这些文档存在于搜索系统的响应中，但不存在于地面实况数据中
假阴性文档：对于给定的查询，这些文档存在于地面实况数据中，但不存在于搜索系统的响应中

一种可能性是插入预测分数 = 0 来表示假阴性并忽略假阳性。但我并不完全确定这是否是正确的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78798774/what-are-the-different-ways-to-handle-false-positives-and-false-negatives-when-c</guid>
      <pubDate>Fri, 26 Jul 2024 15:02:50 GMT</pubDate>
    </item>
    <item>
      <title>解决医学 MRI 数据集中 DICOM 数据数量不平衡的问题</title>
      <link>https://stackoverflow.com/questions/78798297/solving-the-imbalanced-numbers-of-dicom-data-in-medical-mri-datasets</link>
      <description><![CDATA[有一个 DICOM 格式的脑部 MRI 数据集，我们想用它们来训练模型，每个 ID 都有 3 个文件夹，文件夹包含 16 到 20 个文件，现在我的问题是如何平衡它们以使用它们？
该算法可能会将给定的 T1 或 T2 或 T2-flair MRI-DICOM 数据排序为正常和异常]]></description>
      <guid>https://stackoverflow.com/questions/78798297/solving-the-imbalanced-numbers-of-dicom-data-in-medical-mri-datasets</guid>
      <pubDate>Fri, 26 Jul 2024 13:22:36 GMT</pubDate>
    </item>
    <item>
      <title>对图像中的手写和印刷文本进行分类、分割和提取</title>
      <link>https://stackoverflow.com/questions/78798241/classifying-segmenting-and-extracting-handwritten-and-printed-text-in-the-image</link>
      <description><![CDATA[该项目的目标是首先分割（或绘制边界框）并分类图像中的手写和印刷文本，然后从图像中提取手写和印刷文本。印刷文本可以轻松提取，但问题是难以准确提取手写文本。上图是将用于推理的示例图像。我使用 pytesseract 库提取文本，但它在手写文本上失败了。

示例输出在此处给出。（黄色-&gt;手写，绿色-&gt;印刷）但这里在每个单词上都画有 BB。是否可以在整行手写或印刷文本上分割或绘制 BB？如果没有，逐字分词或 BB 也可以。
因此，主要任务是使用模型对手写和印刷文本进行分类，然后使用 OCR 模型对手写文本进行准确度较高的分类。这些只是根据我的知识做出的假设。如果有其他最佳方法，请指导我。]]></description>
      <guid>https://stackoverflow.com/questions/78798241/classifying-segmenting-and-extracting-handwritten-and-printed-text-in-the-image</guid>
      <pubDate>Fri, 26 Jul 2024 13:11:48 GMT</pubDate>
    </item>
    <item>
      <title>语义分割中的数据增强</title>
      <link>https://stackoverflow.com/questions/78798068/data-augmentation-in-semantic-segmentation</link>
      <description><![CDATA[我尝试对数据集执行数据增强，但得到的却是空白的白色图像，在蒙版上工作正常，但图像存在问题。
如何解决这个问题？
这是原始图像+蒙版
增强图像+蒙版
增强代码：
seed = 24
batch_size = 8

img_datagen_args = dict(rescale = 1/255,
rotation_range = 5,
zoom_range = 0.1,
Horizo​​ntal_flip = True,
Vertical_flip = True,
Fill_mode = &#39;nearest&#39;)

mask_datagen_args = dict(rescale = 1/255,
Rotation_range = 5,
Zoom_range = 0.1,
Horizo​​ntal_flip = True,
Vertical_flip = True,
Fill_mode = &#39;nearest&#39;,
Preprocessing_function = lambda x: np.where(x&gt;0, 1, 0).astype(x.dtype))

img_datagen = ImageDataGenerator(**img_datagen_args)
img_generator = img_datagen.flow_from_directory(&#39;/content/split_dataset/train/images/&#39;,
Seed = Seed,
Batch_size = Batch_size,
Class_mode = None)

mask_datagen = ImageDataGenerator(**mask_datagen_args)
mask_generator = mask_datagen.flow_from_directory(&#39;/content/split_dataset/train/masks/&#39;,
seed = seed,
batch_size = batch_size,
color_mode = &#39;grayscale&#39;,
class_mode = None)

valid_img_generator = img_datagen.flow_from_directory(&#39;/content/split_dataset/test/images/&#39;,
seed = seed,
batch_size = batch_size,
class_mode = None)

valid_mask_generator = mask_datagen.flow_from_directory(&quot;/content/split_dataset/test/masks/&quot;,
seed = seed,
batch_size = batch_size,
color_mode = &#39;grayscale&#39;,
class_mode = None)

train_generator = zip(img_generator, mask_generator)
valid_generator = zip(valid_img_generator, valid_mask_generator)

可视化代码：
import matplotlib.pyplot as plt

# 获取一批图像和掩码
img_batch, mask_batch = next(zip(img_generator, mask_generator))

# 绘制一些增强图像
fig, ax = plt.subplots(1, 4, figsize=(20, 5))
for i in range(4):
ax[i].imshow(img_batch[i]) # 对灰度图像使用 cmap=&#39;gray&#39;
ax[i].set_title(f&quot;Augmented Image {i+1}&quot;)
ax[i].axis(&#39;off&#39;)
plt.show()

# 绘制相应的掩码
fig, ax = plt.subplots(1, 4, figsize=(20, 5))
for i in range(4):
ax[i].imshow(mask_batch[i]) # 如果是灰度，则挤压以删除通道维度
ax[i].set_title(f&quot;增强蒙版 {i+1}&quot;)
ax[i].axis(&#39;off&#39;)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78798068/data-augmentation-in-semantic-segmentation</guid>
      <pubDate>Fri, 26 Jul 2024 12:32:26 GMT</pubDate>
    </item>
    <item>
      <title>决策树分类器给出错误结果</title>
      <link>https://stackoverflow.com/questions/78797339/decision-tree-classifier-gives-wrong-results</link>
      <description><![CDATA[我正在学习一门机器学习课程，其中的作业是实现 DecisionTreeClassifier 的拟合方法。
这是我的代码：
import numpy as np
import pandas as pd

class MyTreeClf:
def __init__(self, max_depth=5, min_samples_split=2, max_leafs=20):
self.max_depth = max_depth
self.min_samples_split = min_samples_split
self.max_leafs = max_leafs
self.tree = None
self.leafs_cnt = 0

def node_entropy(self, probs):
return -np.sum([p * np.log2(p) for p in probs if p &gt; 0])

def node_ig(self, x_col, y, split_value):
left_mask = x_col &lt;= split_value
right_mask = x_col &gt; split_value

如果 len(x_col[left_mask]) == 0 或 len(x_col[right_mask]) == 0:
返回 0

left_probs = np.bincount(y[left_mask]) / len(y[left_mask])
right_probs = np.bincount(y[right_mask]) / len(y[right_mask])

entropy_after = len(y[left_mask]) / len(y) * self.node_entropy(left_probs) + len(y[right_mask]) / len(y) * self.node_entropy(right_probs)
entropy_before = self.node_entropy(np.bincount(y) / len(y))

返回 entropy_before - entropy_after

def get_best_split(self, X: pd.DataFrame，y：pd.Series）：
best_col，best_split_value，best_ig = None，None，-np.inf

对于 X.columns 中的 col：
sorted_unique_values = np.sort(X[col].unique())

对于 range(1，len(sorted_unique_values)) 中的 i：
split_value = (sorted_unique_values[i - 1] + sorted_unique_values[i]) / 2

ig = self.node_ig(X[col]，y，split_value)

如果 ig &gt; best_ig:
best_ig = ig
best_col = col
best_split_value = split_value

返回 best_col、best_split_value、best_ig

def fit(self, X: pd.DataFrame, y: pd.Series,depth=0):
如果depth == 0:
self.tree = {}

best_col、best_split_value、best_ig = self.get_best_split(X, y)

如果depth &lt; self.max_depth 和 len(y) &gt;= self.min_samples_split 和 self.leafs_cnt &lt; self.max_leafs 和 best_col 不为 None:
left_mask = X[best_col] &lt;= best_split_value
right_mask = X[best_col] &gt; best_split_value

self.tree[depth] = {&#39;col&#39;: best_col, &#39;split&#39;: best_split_value, &#39;left&#39;: {}, &#39;right&#39;: {}}

self.fit(X[left_mask], y[left_mask],depth + 1)
self.fit(X[right_mask], y[right_mask],depth + 1)
else:
class_label = y.mode()[0]
self.tree[depth] = {&#39;class&#39;: class_label}
self.leafs_cnt += 1

在 fit 方法中，该方法使用 X 和 y 来构建树，应该计算叶子的数量。
树的构建如下：
根节点：从根节点开始，遍历每个属性。
阈值选择过程：
对于每个属性，选择唯一值并对其进行排序。
形成阈值列表以拆分值。
对于每个阈值，将数据集拆分为两个子集（左和右）。
评估每次拆分的信息增益。
选择具有最高信息增益的属性和阈值，并将它们保存在层次结构中。
递归拆分：
将数据集拆分为两个子集。
如果子集可以进一步拆分，则递归重复该过程。
如果不可以，则将子集声明为叶子并保存第一个类的概率。
约束：
当满足以下条件之一时停止拆分：
最大树深度
节点中的最小实例数
最大叶子数
即使达到约束，也要通过创建必要的叶子来完成树。
叶子的数量保存在 leafs_cnt 变量中。该方法不返回任何内容。]]></description>
      <guid>https://stackoverflow.com/questions/78797339/decision-tree-classifier-gives-wrong-results</guid>
      <pubDate>Fri, 26 Jul 2024 09:48:47 GMT</pubDate>
    </item>
    <item>
      <title>线性模型的 SHAP 值与手动计算的值不同</title>
      <link>https://stackoverflow.com/questions/78796974/shap-values-for-linear-model-different-from-those-calculated-manually</link>
      <description><![CDATA[我训练一个线性模型来预测房价，然后我手动比较 Shapley 值计算结果与 SHAP 库返回的值，发现它们略有不同。
我的理解是，对于线性模型，Shapley 值由以下公式给出：
coeff * features for obs - coeffs * mean(features in training set)

或者如 SHAP 文档中所述：coef[i] * (x[i] - X.mean(0)[i])，其中 i 是一个特征。
问题是，为什么 SHAP 返回的值与手动计算不同？
代码如下：
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
来自 sklearn.preprocessing 导入 MinMaxScaler
导入 shap

X, y = fetch_california_housing(return_X_y=True, as_frame=True)

X = X.drop(columns = [&quot;Latitude&quot;, &quot;Longitude&quot;, &quot;AveBedrms&quot;])

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.3, random_state=0,
)

scaler = MinMaxScaler().set_output(transform=&quot;pandas&quot;).fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

linreg = LinearRegression().fit(X_train, y_train)
coeffs = pd.Series(linreg.coef_, index=linreg.feature_names_in_)

X_test.reset_index(inplace=True, drop=True)
obs = 6188

# 手动 shapley 计算
effect = coeffs * X_test.loc[obs]
effect - coeffs * X_train.mean()

返回结果：
MedInc 0.123210
HouseAge -0.459784
AveRooms -0.128162
Population 0.032673
AveOccup -0.001993
dtype: float64

SHAP 库返回的结果略有不同：
explainer = shap.LinearExplainer(linreg, X_train)
shap_values = explainer(X_test)
shap_values[obs]

结果如下：
.values =
array([ 0.12039244, -0.47172515, -0.12767778, 0.03473923, -0.00251017])

.base_values =
2.0809714707337523

.data =
array([0.25094137, 0.01960784, 0.06056066, 0.07912217, 0.00437137])

设置为忽略交互：
explainer.feature_perturbation

返回
&#39;interventional&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/78796974/shap-values-for-linear-model-different-from-those-calculated-manually</guid>
      <pubDate>Fri, 26 Jul 2024 08:22:09 GMT</pubDate>
    </item>
    <item>
      <title>在 Keras 中实现具有多个输入的循环神经网络训练算法</title>
      <link>https://stackoverflow.com/questions/78796429/implementation-of-training-algorithm-for-recurrent-neural-networks-with-multiple</link>
      <description><![CDATA[我正在阅读一本名为《使用 Scikit-Learn、Keras 和 TensorFlow 进行机器学习实践》的书中有关 RNN 的内容，该书的作者是 Aurelien Geron。我遇到了一个使用 Keras 的序列到序列 RNN 的非常简单的实现：
seq2seq_model = tf.keras.Sequential([
tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 5]),
tf.keras.layers.Dense(14)
])

这应该采取任意数量的时间步骤（在本例中为 56），每个步骤有 5 个特征，并创建一个向量，其中包含每个时间步骤每天未来 2 周的预测。
这是从 repo 中获取的：
https://github.com/ageron/handson-ml3/blob/main/15_processing_sequences_using_rnns_and_cnns.ipynb
但理解这个问题并不需要它。
时间序列数据以 56 的序列长度传递。因此，在调用 predict 之后，我们实际上得到了 56 个预测向量，每个向量有 14 天的预测（前 55 个用于训练，最后一个是实际预测）。
我的问题是：如果有 32 个输入神经元，每个神经元都采用前一个时间步骤，如果前 31 个时间步骤之前的时间步骤不足以作为输入传递给所有神经元，我们如何获得 56 个预测？所有缺失的特征都设置为 0 吗？在这些情况下，前向传递是如何完成的？当我们尝试训练模型使用完整的 56 个时间步骤数据进行预测时，这些情况下的反向传播有何用处？我正在寻找 Keras 特定的答案以及对此的一些一般见解。
我尝试运行此代码：
X = mulvar_valid.to_numpy()[np.newaxis, :seq_length]
y_pred_14 = seq2seq_model.predict(np.asarray(X).astype(np.float32))
print(len(y_pred_14[0]))

发现预测有 56 个向量。我预计一个 32 大小的窗口会在 56 个时间步骤中滚动以产生大约 20 个预测向量。]]></description>
      <guid>https://stackoverflow.com/questions/78796429/implementation-of-training-algorithm-for-recurrent-neural-networks-with-multiple</guid>
      <pubDate>Fri, 26 Jul 2024 06:00:48 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError: ‘list’ 对象没有属性 ‘shape’ 错误</title>
      <link>https://stackoverflow.com/questions/78796342/attributeerror-list-object-has-no-attribute-shape-error</link>
      <description><![CDATA[我正在尝试预测股票价格。这是我的代码：
import pandas as pd
import yfinance as web
import numpy as np

从 sklearn.preprocessing 导入 MinMaxScaler
从 tensorflow.python.keras.models 导入 Sequential
从 tensorflow.python.keras.layers 导入 Dense、Dropout
从 tensorflow.python.keras.layers.recurrent 导入 LSTM

company = &#39;TSLA&#39;

start=&#39;2012-01-01&#39;
end=&#39;2024-03-01&#39;

data = web.download(company, start=start, end=end)

scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(data[&#39;Close&#39;].values.reshape(-1,1))

prediction_days = 60

x_train = []
y_train = []

for x in range(prediction_days, len(scaled_data)):
x_train.append(scaled_data[x-prediction_days:x, 0])
y_train.append(scaled_data[x, 0])

model = Sequential()

model.add(LSTM(units = 50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units = 50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units = 50))
model.add(Dropout(0.2))

我期望它什么都不输入，但是我得到了这个错误：
但它说作为错误，
回溯（最近一次调用）：
文件
&quot;c:\Users\User1\OneDrive\Documents\Desktop\python\projects\machine\stock_price_predictor.py&quot;，
第 32 行，位于 &lt;module&gt;
model.add(LSTM(units = 50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
^^^^^^^^^^^^^
AttributeError: &#39;list&#39; 对象没有属性 &#39;shape&#39;

如何解决？我尝试将其转换为 np.array，但没有任何效果。这是我的尝试：
import pandas as pd
import yfinance as web
import numpy as np

从 sklearn.preprocessing 导入 MinMaxScaler
从 tensorflow.python.keras.models 导入 Sequential
从 tensorflow.python.keras.layers 导入 Dense、Dropout
从 tensorflow.python.keras.layers.recurrent 导入 LSTM

company = &#39;TSLA&#39;

start=&#39;2012-01-01&#39;
end=&#39;2024-03-01&#39;

data = web.download(company, start=start, end=end)

scaler = MinMaxScaler(feature_range=(0,1))
scaled_data=.fit_transform(data[&#39;Close&#39;].values.reshape(-1,1))

prediction_days = 60

x_train = np.array([])
y_train = np.array([])

对于范围（prediction_days，len（scaled_data））中的 x：
x_train = np.append(x_train，scaled_data[x-prediction_days：x，0])
y_train = np.append(y_train，scaled_data[x，0])

model = Sequential()

model.add(LSTM(units = 50，return_sequences=True，input_shape= 
(x_train.shape[1]，1)))
model.add(Dropout(0.2))
model.add(LSTM(units = 50，return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units = 50))
model.add(Dropout(0.2))

但我却得到了这个错误
回溯（最近一次调用）：
文件 
&quot;c:\Users\User1\OneDrive\Documents\Desktop\python\projects\machine 
learning\stock_price_predictor.py&quot;，第 32 行，位于 &lt;module&gt;
model.add(LSTM(units = 50, return_sequences=True, input_shape= 
(x_train.shape[1], 1))) 
~~~~~~~~~~~~~^^^
IndexError：元组索引超出范围
]]></description>
      <guid>https://stackoverflow.com/questions/78796342/attributeerror-list-object-has-no-attribute-shape-error</guid>
      <pubDate>Fri, 26 Jul 2024 05:31:21 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：参数 clone_function 和 input_tensors 仅支持顺序模型或功能模型</title>
      <link>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</link>
      <description><![CDATA[我正在使用Quantization perceived training，参考网上的lstm代码，想把QAT放进lstm，结果遇到了ValueError。
ValueError Traceback (most recent call last)
&lt;ipython-input-11-00669bb76f9d&gt; in &lt;cell line: 6&gt;()
4 return layer
5 
----&gt; 6 annotated_model = tf.keras.models.clone_model(
7 model,
8 clone_function=apply_quantization_to_dense,

/usr/local/lib/python3.10/dist-packages/tf_keras/src/models/cloning.py in clone_model(model, input_tensors, clone_function)
544 # 自定义模型类的情况
545 if clone_function or input_tensors:
--&gt; 546 raise ValueError(
547 &quot;参数 clone_function 和 input_tensors &quot;
548 &quot;仅支持 Sequential 模型 &quot;

ValueError: 参数 clone_function 和 input_tensors 仅支持 Sequential 模型或 Functional 模型。收到类型为“Sequential”的模型，其中 clone_function=&lt;function apply_quantization_to_dense 位于0x78b727ec4040&gt; 和 input_tensors=None

这是我的代码
import keras
从 keras.layers 导入 LSTM
从 keras.layers 导入 Dense、Activation
从 keras.datasets 导入 mnist
从 keras.models 导入 Sequential
从 keras.optimizers 导入 Adam

learning_rate = 0.001
training_iters = 20
batch_size = 128
display_step = 10

n_input = 28
n_step = 28
n_hidden = 128
n_classes = 10

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(-1, n_step, n_input)
x_test = x_test.reshape(-1, n_step, n_input)
x_train = x_train.astype(&#39;float32&#39;)
x_test = x_test.astype(&#39;float32&#39;)
x_train /= 255
x_test /= 255

y_train = keras.utils.to_categorical(y_train, n_classes)
y_test = keras.utils.to_categorical(y_test, n_classes)

model = Sequential()
model.add(LSTM(n_hidden,
batch_input_shape=(None, n_step, n_input),
unroll=True))

model.add(Dense(n_classes))
model.add(Activation(&#39;softmax&#39;))

adam = Adam(lr=learning_rate)
model.summary()
model.compile(optimizer=adam,
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

model.fit(x_train, y_train,
batch_size=batch_size,
epochs=training_iters,
verbose=1,
validation_data=(x_test, y_test))

scores = model.evaluate(x_test, y_test, verbose=0)
print(&#39;LSTM 测试分数：&#39;, scores[0])
print(&#39;LSTM 测试准确率：&#39;, scores[1])

def apply_quantization_to_dense(layer):
if isinstance(layer, tf.keras.layers.LSTM):
return tfmot.quantization.keras.quantize_annotate_layer(layer)
return layer

annotated_model = tf.keras.models.clone_model(
模型，
clone_function=apply_quantization_to_dense，
)
]]></description>
      <guid>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</guid>
      <pubDate>Fri, 26 Jul 2024 03:41:57 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：不支持 y 的稀疏多标签指标 - 如何处理具有稀疏数据的多标签分类？</title>
      <link>https://stackoverflow.com/questions/78795297/valueerror-sparse-multilabel-indicator-for-y-is-not-supported-how-to-handle-m</link>
      <description><![CDATA[我只是一个初学者，我还在学习稀疏矩阵以及它们如何与其他东西一起工作。
这是我遇到的问题，在网上搜索后找不到合适的答案。
我使用默认参数 sparse_output=True 对分类标签进行了 OneHotEncoded，
当我尝试在训练测试拆分后使用 transformed_X 和目标 y 拟合 RandomForestClassifier 时，它显示了此错误。
#seed
np.random.seed(42)

#one hot encoding imports
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer as ct

#Data splitting
X = f_data.drop(&#39;attended&#39;, axis = 1)
y = f_data[&#39;attended&#39;]

#select columns
cat_col = [&#39;days_before&#39;,&#39;day_of_week&#39;,&#39;time&#39;,&#39;category&#39;]

#初始化编码器 
enc = OneHotEncoder()

#使用 ct 拟合编码器
transformer = ct([(&#39;enc&#39;,enc,cat_col)], remainder = &#39;passthrough&#39;)
transformed_X = transformer.fit_transform(X)
transformed_X

&lt;1480x36 稀疏矩阵，类型为 &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;以压缩稀疏行格式存储 10360 个元素&gt;
#BaseLine 模型
np.random.seed(42)

#imports
from sklearn.model_selection import train_test_split as tts
from sklearn.ensemble import RandomForestClassifier

#splitting
X_train,Y_train,X_test,Y_test = tts(transformed_X,y, test_size = 0.2)

#model fitting
model = RandomForestClassifier()
model.fit(X_train,Y_train)

-------------------------------------------------------------------------------
ValueError Traceback (most recent call last)
Cell In[416], line 13
11 #modelling
12 model = RandomForestClassifier()
---&gt; 13 model.fit(X_train,Y_train)
15 #模型得分
16 blsc = model.score(X_test,Y_test)

文件 G:\Md Jaffer\UDEMY\Machine Learning Course ZTM\Projects\HeartDesease_Classification\env\Lib\site-packages\sklearn\base.py:1474，在 _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)
1467 estimator._validate_params()
1469 使用 config_context(
1470 skip_parameter_validation=(
1471 prefer_skip_nested_validation 或 global_skip_validation
1472 )
1473 ):
-&gt; 1474 返回 fit_method(estimator, *args, **kwargs)

文件 G:\Md Jaffer\UDEMY\Machine Learning Course ZTM\Projects\HeartDesease_Classification\env\Lib\site-packages\sklearn\ensemble\_forest.py:361，位于 BaseForest.fit(self, X, y, sample_weight)
359 # 验证或转换输入数据
360 if issparse(y):
--&gt; 361 引发 ValueError(&quot;不支持 y 的稀疏多标签指标。&quot;)
363 X, y = self._validate_data(
364 X,
365 y,
(...)
369 force_all_finite=False,
370 )
371 # _compute_missing_values_in_feature_mask 检查 X 是否有缺失值，并且
372 # 如果底层树基础估计器无法处理缺失值，则会引发错误。只需要标准来确定树是否支持
374 # 缺失值。

ValueError: 不支持 y 的稀疏多标签指标。

我尝试设置 sparse_output=False，但它给出了样本数量不一致的错误。标签编码后的实际形状为 (1480 x 36)
---------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[444]，第 13 行
11 #modelling
12 model = RandomForestClassifier()
---&gt; 13 model.fit(X_train,Y_train)
15 #model score
16 blsc = model.score(X_test,Y_test)

ValueError：发现输入变量的样本数量不一致：[1184, 296]
]]></description>
      <guid>https://stackoverflow.com/questions/78795297/valueerror-sparse-multilabel-indicator-for-y-is-not-supported-how-to-handle-m</guid>
      <pubDate>Thu, 25 Jul 2024 20:21:08 GMT</pubDate>
    </item>
    <item>
      <title>我在训练随机森林回归器时不断遇到这个问题</title>
      <link>https://stackoverflow.com/questions/78795096/i-keep-encountering-this-problem-training-a-random-forest-regressor</link>
      <description><![CDATA[/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: 
UserWarning：X 有特征名称，但 RandomForestRegressor 拟合时没有特征名称
warnings.warn(

我尝试添加 .values，但仍然标记错误。]]></description>
      <guid>https://stackoverflow.com/questions/78795096/i-keep-encountering-this-problem-training-a-random-forest-regressor</guid>
      <pubDate>Thu, 25 Jul 2024 19:18:50 GMT</pubDate>
    </item>
    <item>
      <title>Apache Flink 中的依赖管理和执行环境</title>
      <link>https://stackoverflow.com/questions/78780530/dependency-management-and-execution-environment-in-apache-flink</link>
      <description><![CDATA[我们正在评估 apache flink 是否可用于部署流式机器学习应用程序。
apache flink 中如何处理依赖管理，尤其是执行环境？
想象一下，具有不同依赖关系的 Python 任务应提交给 Flink 集群。
我们只看到 Flink 任务管理器可以使用 Python 虚拟环境处理依赖管理。当每个任务都有不同的依赖关系时，我们是否应该为每个任务部署一个新的任务管理器？
从容器设置开始，我们可以在单独的 Docker 映像中部署每个任务。
使用 apache flink 时通常如何处理这个问题？我们没有看到 Flink 擅长处理需要特定依赖关系的大量任务，但希望利用流式处理器。]]></description>
      <guid>https://stackoverflow.com/questions/78780530/dependency-management-and-execution-environment-in-apache-flink</guid>
      <pubDate>Mon, 22 Jul 2024 19:46:27 GMT</pubDate>
    </item>
    <item>
      <title>“keras.api._v2.keras”没有属性“mnist”</title>
      <link>https://stackoverflow.com/questions/74706683/keras-api-v2-keras-has-no-attribute-mnist</link>
      <description><![CDATA[我尝试安装 TensorFlow，我想尝试我在 youtube.com 上看到的东西，但我尝试了多种方法，但都不起作用，每次都出现不同的错误。`
import tensorflow as tf
import numpy as np
from keras.models import Sequential
import matplotlib.pyplot as plt
import time

(train_images,train_labels),(test_images,test_labels) = tf.keras.mnist.load_data()

plt.imshow(train_images[0])


这是错误
`
Traceback（最近一次调用最后一次）：
文件“c:\pyprojects\tensorflow\main.py”，第 7 行，在&lt;module&gt;
(train_images,train_labels),(test_images,test_labels) = tf.keras.mnist.load_data()
AttributeError: 模块“keras.api._v2.keras”没有属性“mnist”

`
我尝试了不同的方法，因为我说没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/74706683/keras-api-v2-keras-has-no-attribute-mnist</guid>
      <pubDate>Tue, 06 Dec 2022 17:23:14 GMT</pubDate>
    </item>
    </channel>
</rss>