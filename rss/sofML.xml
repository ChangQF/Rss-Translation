<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 16 Mar 2024 12:24:25 GMT</lastBuildDate>
    <item>
      <title>代码有问题还是我的数据有问题？</title>
      <link>https://stackoverflow.com/questions/78171320/is-there-something-wrong-with-the-code-or-will-the-problem-be-in-my-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78171320/is-there-something-wrong-with-the-code-or-will-the-problem-be-in-my-data</guid>
      <pubDate>Sat, 16 Mar 2024 10:00:18 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降算法中，如何导出-2*wx</title>
      <link>https://stackoverflow.com/questions/78171263/in-gradient-descent-algorithm-how-to-induce-2wx</link>
      <description><![CDATA[【梯度下降算法部分】
(https://i.stack.imgur.com/iTBz6.png)&lt; /p&gt;
this.updateWeights = function() {
 
  让wx;
  让w_deriv = 0;
  让 b_deriv = 0;

  for (让 i = 0; i &lt; this.points; i++) {
    wx = this.yArr[i] - (this.weight * this.xArr[i] + this.bias);
    w_deriv += -2 * wx * this.xArr[i];
    b_deriv += -2 * wx;
  }
  
  this.weight -= (w_deriv / this.points) * this.learnc;
  this.bias -= (b_deriv / this.points) * this.learnc;
}
            

请解释一下这部分！！
-2 * wx * this.xArr[i]

这部分是诱导出来的......？
如何通过数学公式归纳...]]></description>
      <guid>https://stackoverflow.com/questions/78171263/in-gradient-descent-algorithm-how-to-induce-2wx</guid>
      <pubDate>Sat, 16 Mar 2024 09:41:21 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中的超参数调整和模型评估</title>
      <link>https://stackoverflow.com/questions/78171227/hyperparameter-tuning-and-model-evaluation-in-scikit-learn</link>
      <description><![CDATA[我是机器学习领域的新手，对于如何正确使用超参数调整和模型评估感到有点困惑。
超参数调整应该在整个数据集上进行还是仅在训练集上进行？正确的操作顺序是什么？
您能否检查我的代码并建议我考虑该问题的最佳实践？
在这里，我首先对整个数据集使用超参数调整，然后仅在训练集上评估模型性能。这是对的吗？不会导致数据泄露吗？
超参数调优
numeric_features = X.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).columns
categorical_features = X.select_dtypes(include=[&#39;object&#39;, &#39;category&#39;]).columns

预处理器 = ColumnTransformer(
    变形金刚=[
        (&#39;num&#39;, StandardScaler(), numeric_features),
        (&#39;猫&#39;, OneHotEncoder(handle_unknown=&#39;忽略&#39;), categorical_features)
    ]
）

en_cv = ElasticNetCV(l1_ratio=np.arange(0, 1.1, 0.1),
                     alpha = np.arange(0, 1.1, 0.1),
                     随机状态=818，
                     职位数 = -1)

模型= make_pipeline（预处理器，en_cv）
模型.fit(X, y)

best_alpha = en_cv.alpha_
best_l1_ratio = en_cv.l1_ratio_

模型评估：
ElasticNet = make_pipeline(预处理器, ElasticNet(alpha=best_alpha, l1_ratio=l1_ratio))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=818)

ElasticNet.fit(X_train, y_train)
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mse = 均方误差(y_test, y_pred)

打印（r2，MSE）

提前致谢，祝您有美好的一天！
实际上，这段代码在包含约 80000 个观察值和约 150 列的数据集上运行大约需要 18 分钟。这是否足够？]]></description>
      <guid>https://stackoverflow.com/questions/78171227/hyperparameter-tuning-and-model-evaluation-in-scikit-learn</guid>
      <pubDate>Sat, 16 Mar 2024 09:27:26 GMT</pubDate>
    </item>
    <item>
      <title>我在梯度提升模型中收到此错误“AttributeError：'HalfSquaredError'对象没有属性'get_init_raw_predictions'”</title>
      <link>https://stackoverflow.com/questions/78171146/i-am-getting-this-error-attributeerror-halfsquarederror-object-has-no-attrib</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78171146/i-am-getting-this-error-attributeerror-halfsquarederror-object-has-no-attrib</guid>
      <pubDate>Sat, 16 Mar 2024 08:56:45 GMT</pubDate>
    </item>
    <item>
      <title>yolo 通过绘制边界点进行脊柱标记训练</title>
      <link>https://stackoverflow.com/questions/78170933/yolo-training-for-spine-labeling-by-plotting-boundary-points</link>
      <description><![CDATA[我正在检查如何通过使用 yolo 在 CT 脊柱图像上标记边界点来实现如图所示的预测。
我正在尝试使用 yolo v8，任何有关如何启动注释的指示都会有所帮助。谢谢
带有预测的结果/预期图像]]></description>
      <guid>https://stackoverflow.com/questions/78170933/yolo-training-for-spine-labeling-by-plotting-boundary-points</guid>
      <pubDate>Sat, 16 Mar 2024 07:32:54 GMT</pubDate>
    </item>
    <item>
      <title>使用 opencv / python / 任何 AI/ML 模型检测并提取二值图像中的四边形的四个点</title>
      <link>https://stackoverflow.com/questions/78170927/detect-and-extract-four-points-of-quadrilaterals-in-an-binary-image-using-opencv</link>
      <description><![CDATA[我有这样的二进制分段蒙版图像

我想从这样的图像中检测四边形的四个坐标

output object = [ ( (x11,y11), (x12,y12), (x13,y13), (x14,y14) ) , # 第一个四边形
                  ( (x21,y21), (x22,y22), (x23,y23), (x24,y24) ) ] # 第二个四边形

我尝试过使用边缘检测技术的文档扫描方法，但无法找到四边形的四个点。
我可以通过此代码获取角点的坐标，但无法从中创建四边形。
导入imutils
导入CV2
将 numpy 导入为 np

图像 = cv2.imread(&#39;../masks\cpu-b-model\segmentation4.png&#39;,0)

cnts = cv2.findContours(image.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)
cnts = imutils.grab_contours(cnts)
c = max(cnts, key=cv2.contourArea)
输出=图像.copy()
cv2.drawContours(输出, [c], -1, (0, 255, 0), 3)
(x, y, w, h) = cv2.boundingRect(c)
text = &quot;原始, num_pts={}&quot;.format(len(c))
# cv2.putText(输出, 文本, (x, y - 15), cv2.FONT_HERSHEY_SIMPLEX,0.9, (0, 255, 0), 2)
cv2.imshow(“原始轮廓”, 输出)
cv2.waitKey(0)
cv2.destroyAllWindows()
打印（文本）

对于 np.linspace(0.05, 0.1, 10) 中的 eps：
    peri = cv2.arcLength(c, True)
    近似 = cv2.approxPolyDP(c, eps * peri, True)
    打印（大约）
    输出=图像.copy()
    cv2.drawContours(输出, [大约], -1, (0, 255, 0), 3)
    文本 =“eps={:.4f}, num_pts={}”.format(eps, len(大约))
    cv2.putText（输出，文本，（x，y - 15），cv2.FONT_HERSHEY_SIMPLEX，
        0.9, (0, 255, 0), 2)
    print(“[INFO] {}”.format(text))
    cv2.imshow(“近似轮廓”, 输出)
    cv2.waitKey(0)
cv2.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/78170927/detect-and-extract-four-points-of-quadrilaterals-in-an-binary-image-using-opencv</guid>
      <pubDate>Sat, 16 Mar 2024 07:30:32 GMT</pubDate>
    </item>
    <item>
      <title>从二维输入预测多个输出的回归问题</title>
      <link>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</link>
      <description><![CDATA[我有几个二维图表，每个图表都有八个独特的数字特征，可用于生成这些图表。我以大量 .csv 文件的形式拥有所有这些图表的 x 和 y 坐标及其数值特征。我想通过使用机器学习或深度学习模型来预测每个图的数值特征（通过使用图的图像或使用每个图的点的坐标）
到目前为止，我已经寻找了许多预训练模型，并在 HuggingFace 等网站上搜索了此类模型，还在 GitHub 代码中搜索了很多。我还在 paperswithcode 网站上搜索了做过同样事情的文章，但不幸的是，我仍然没有找到任何东西！我曾多次尝试自己编写一个网络，但由于这样做的复杂性以及对如何设置网络的超参数以达到预期结果的了解不够，我遇到了很多错误并且无法做到这一点！
有人知道我该怎么做吗？如果您能为我提供任何帮助来完成此操作，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</guid>
      <pubDate>Sat, 16 Mar 2024 07:03:13 GMT</pubDate>
    </item>
    <item>
      <title>删除点云中的底层</title>
      <link>https://stackoverflow.com/questions/78170818/remove-base-floor-in-point-cloud</link>
      <description><![CDATA[我有沙堆 3d 点云的 .obj 文件。它包含底层点和沙堆点。我需要移除底层点并仅过滤沙堆。有相关的 Matlab 或 Python 代码吗？
我尝试了一些使用 python 的代码。但它不正确。]]></description>
      <guid>https://stackoverflow.com/questions/78170818/remove-base-floor-in-point-cloud</guid>
      <pubDate>Sat, 16 Mar 2024 06:31:22 GMT</pubDate>
    </item>
    <item>
      <title>深度学习 NCAAB 模型建议</title>
      <link>https://stackoverflow.com/questions/78170803/deep-learning-ncaab-model-suggestions</link>
      <description><![CDATA[我致力于学习和探索更深入地构建人工智能模型，并通过体育来实现这一目标。我正在根据上个赛季的数据预测一支球队将在哪一轮退出比赛。我已经做到了这一点，但我能达到的最高准确度约为 50-52%。我玩过很多很多功能，更多的层，神经元，纪元等等......请任何建议都会很棒。这是代码和头部数据。
from sklearn.model_selection import train_test_split
从 sklearn.preprocessing 导入 StandardScaler、LabelEncoder
将张量流导入为 tf
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入密集，输入，丢失
从tensorflow.keras.regularizers导入l2
从tensorflow.keras.optimizers导入Adam
从tensorflow.keras.callbacks导入EarlyStopping
从tensorflow.keras.callbacks导入LearningRateScheduler
从tensorflow.keras.layers导入BatchNormalization
从 sklearn.preprocessing 导入 MinMaxScaler


np.随机.种子(42)
tf.random.set_seed(42)


def lr_schedule(epoch, lr):
    如果纪元&gt; 5：
        lr = lr * 0.5 # 每5个epoch将学习率降低一半
    return max(lr, 1e-5) # 确保 lr 不会低于某个阈值

# 初始化学习率调度器回调
lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)

# 加载数据集
df = pd.read_csv(&#39;final_madness.csv&#39;)

# 过滤出2024年的数据
df_filtered = df[df[&#39;年份&#39;] != 2024]

# 特征和目标变量，不包括“TEAM”和“YEAR”
X = df_filtered.drop([&#39;ROUND&#39;, &#39;TEAM&#39;, &#39;YEAR&#39;, &quot;CONF&quot;, &quot;CONF ID&quot;, &quot;QUAD ID&quot;, &quot;TEAM NO&quot;, &quot;TEAM ID&quot;, &quot;&quot;四号”]，轴=1)
打印（X.head（））

y = df_filtered[&#39;圆形&#39;]
打印（y.head（））

# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)

# 实例化 StandardScaler
定标器=标准定标器()

# 在训练数据上拟合缩放器
X_train_scaled = 缩放器.fit_transform(X_train)

# 转换训练集和测试集
X_test_scaled = 缩放器.transform(X_test)

# 初始化编码器
编码器 = LabelEncoder()

# 拟合并变换 y_train 以对标签进行编码
y_train_encoded = 编码器.fit_transform(y_train)

# 使用相同的编码器转换 y_test
y_test_encoded = 编码器.transform(y_test)


# 检查编码后的目标
label_mapping = dict(zip(encoder.classes_, range(len(encoder.classes_))))

模型=顺序（[
    输入(形状=(X_train_scaled.shape[1],)),
    密集（64，kernel_regularizer = l2（0.001）），
    密集（32，kernel_regularizer = l2（0.001）），
    批量归一化(),
    Dropout(0.4), # 添加批量归一化后调整 dropout
    密集（len（np.unique（y_train_encoded）），激活=&#39;softmax&#39;）
]）

选择=亚当（学习率=0.001）
model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;],optimizer=opt)
打印（模型.摘要（））

Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 8，restore_best_weights = True）

# 将批量大小更新为 32
历史= model.fit（X_train_scaled，y_train_encoded，epochs = 100，batch_size = 64，validation_split = 0.2，callbacks = []）

plt.plot(history.history[&#39;准确度&#39;])
plt.plot(history.history[&#39;val_accuracy&#39;])
plt.title(&#39;模型精度&#39;)
plt.ylabel(&#39;准确率&#39;)
plt.xlabel(&#39;纪元&#39;)
plt.legend([&#39;训练&#39;, &#39;测试&#39;], loc=&#39;左上&#39;)
plt.show()

# 情节训练&amp;验证损失值
plt.plot(history.history[&#39;loss&#39;])
plt.plot(history.history[&#39;val_loss&#39;])
plt.title(&#39;模型损失&#39;)
plt.ylabel(&#39;损失&#39;)
plt.xlabel(&#39;纪元&#39;)
plt.legend([&#39;训练&#39;, &#39;测试&#39;], loc=&#39;左上&#39;)
plt.show()

# 根据测试数据评估模型
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_encoded)

print(&quot;测试损失：&quot;, test_loss)
print(&quot;测试准确度：&quot;, test_accuracy)

]]></description>
      <guid>https://stackoverflow.com/questions/78170803/deep-learning-ncaab-model-suggestions</guid>
      <pubDate>Sat, 16 Mar 2024 06:28:36 GMT</pubDate>
    </item>
    <item>
      <title>实时检测在YOLOv8中播放音频文件</title>
      <link>https://stackoverflow.com/questions/78170802/playing-audio-file-in-yolov8-in-real-time-detection</link>
      <description><![CDATA[我正在 YOLOv8 项目中工作，以检测困倦并在检测到困倦时播放警报音频文件。我面临的问题是我无法实时播放音频，因为我的检测首先存储在结果中。一旦我关闭检测窗口，它就会访问结果中存储的数据并连续播放音频。我该如何解决这个问题？
导入操作系统
从 ultralytics 导入 YOLO

进口火炬
导入 matplotlib
将 numpy 导入为 np
导入CV2
导入pygame

pygame.init()
sound_to_play = pygame.mixer.Sound(r&#39;D:\ML\同步警惕驱动程序\alarm.wav&#39;)
sound_to_play.play()

模型 = YOLO(r&#39;C:\Users\HP\Downloads\last.pt&#39;)

上限 = cv2.VideoCapture(0)
而真实：
    ret, 框架 = cap.read()

    结果 = model.predict(source=“0”,show=True)
    对于结果中的 r：
        如果 len(r.boxes.cls)&gt;0:
            dclass=r.boxes.cls[0].item()
            打印（d类）
            如果 dclass==2.0:
              sound_to_play.play()
    如果 cv2.waitKey(1) == ord(&#39;q&#39;):
        休息

pygame.quit()
cap.release()
cv2.destroyAllWindows()

&lt;块引用&gt;
问题是我的代码首先进行检测并将其存储在结果中，然后进入 for 循环
预期输出是它同时检测并检查类值
]]></description>
      <guid>https://stackoverflow.com/questions/78170802/playing-audio-file-in-yolov8-in-real-time-detection</guid>
      <pubDate>Sat, 16 Mar 2024 06:28:04 GMT</pubDate>
    </item>
    <item>
      <title>构建 SVM 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/78170776/building-an-svm-model</link>
      <description><![CDATA[我有一个由 254k 个实例组成的数据集，由于目标列不平衡，我决定对其进行过采样，结果得到了 436k 个实例。我应该考虑欠采样吗？或者我应该对数据集进行下采样，因为 SVM 往往在相对较小的数据集上工作得更好？或者 436k 可以吗？
尝试构建 SVM，但计算时间太长。]]></description>
      <guid>https://stackoverflow.com/questions/78170776/building-an-svm-model</guid>
      <pubDate>Sat, 16 Mar 2024 06:16:33 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：无法将字符串转换为浮点数：“alfa-romero”[关闭]</title>
      <link>https://stackoverflow.com/questions/78170755/valueerror-could-not-convert-string-to-float-alfa-romero</link>
      <description><![CDATA[当我弹出上述错误时，我正在尝试定义一个相关矩阵。
该列的 dtype 被定义为对象，但我不知道为什么它将它作为字符串。
感谢任何有关如何纠正此问题的线索
[Dtypes说明](https://i.stack.imgur.com/FNOpq.png)]]></description>
      <guid>https://stackoverflow.com/questions/78170755/valueerror-could-not-convert-string-to-float-alfa-romero</guid>
      <pubDate>Sat, 16 Mar 2024 06:04:12 GMT</pubDate>
    </item>
    <item>
      <title>加载 json 模型时 Python tensorflow keras 错误：无法找到类“Sequential”</title>
      <link>https://stackoverflow.com/questions/78170750/python-tensorflow-keras-error-when-load-a-json-model-could-not-locate-class-se</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78170750/python-tensorflow-keras-error-when-load-a-json-model-could-not-locate-class-se</guid>
      <pubDate>Sat, 16 Mar 2024 05:59:37 GMT</pubDate>
    </item>
    <item>
      <title>Pytroch 分割模型(.pt) 未转换为 CoreML</title>
      <link>https://stackoverflow.com/questions/78091161/pytroch-segmentation-model-pt-not-converting-to-coreml</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78091161/pytroch-segmentation-model-pt-not-converting-to-coreml</guid>
      <pubDate>Sat, 02 Mar 2024 01:26:48 GMT</pubDate>
    </item>
    <item>
      <title>scikeras.wrappers.KerasClassifier 返回 ValueError：无法解释指标标识符：loss</title>
      <link>https://stackoverflow.com/questions/78089332/scikeras-wrappers-kerasclassifier-returning-valueerror-could-not-interpret-metr</link>
      <description><![CDATA[我正在研究 KerasClassifier，因为我想将其插入 scikit-learn 管道中，但我收到了前面提到的 ValueError。
以下代码应该能够重现我遇到的错误：
从 sklearn.model_selection 导入 KFold，cross_val_score
从 sklearn.preprocessing 导入 StandardScaler
从 scikeras.wrappers 导入 KerasClassifier
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Dense
从 sklearn.datasets 导入 load_iris
将 numpy 导入为 np

数据 = load_iris()
X = 数据.数据
y = 数据.目标

def create_model():
    模型=顺序（）
    model.add（密集（8，input_dim = 4，激活=&#39;relu&#39;））
    model.add（密集（3，激活=&#39;softmax&#39;））
    model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,
                  优化器=&#39;亚当&#39;,
                  指标=[&#39;准确性&#39;])
    返回模型

clf = KerasClassifier(build_fn=create_model,
                      纪元=100，
                      批量大小=10，
                      详细=1)

管道=管道（[
    (&#39;缩放器&#39;, StandardScaler()),
    （&#39;clf&#39;，clf）
]）

kf = KFold(n_splits=5, shuffle=True, random_state=42)
结果= cross_val_score（管道，X，y，cv = kf）
print(&quot;交叉验证准确度：&quot;, np.mean(结果))

似乎我的模型正在随着纪元的运行而被编译。但是，之后我收到错误：
ValueError：无法解释指标标识符：丢失

tensorflow 和 scikeras 库的版本是：
scikeras==0.12.0
张量流==2.15.0
]]></description>
      <guid>https://stackoverflow.com/questions/78089332/scikeras-wrappers-kerasclassifier-returning-valueerror-could-not-interpret-metr</guid>
      <pubDate>Fri, 01 Mar 2024 17:03:39 GMT</pubDate>
    </item>
    </channel>
</rss>