<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 29 Oct 2024 01:17:40 GMT</lastBuildDate>
    <item>
      <title>评估模型预测性能时，mase() 错误无法索引数据以外的行</title>
      <link>https://stackoverflow.com/questions/79134999/error-with-mase-cant-indexes-rows-beyond-data-when-evaluating-model-forecasti</link>
      <description><![CDATA[我正在预测时间序列结果 Y，其预测因子滞后 t-12 天，以产生 12 天前的预测，样本外验证框架为 10 个不重叠的折叠。我想使用平均缩放误差作为性能指标。根据我对这篇文章的理解，我将朴素预测的时间步长设置为 12，以便朴素预测和我的模型预测在同一时间间隔。我理解这会将我的模型预测（使用滞后值预测）与 12 天前的 y 值进行比较。由于某种原因，mase() 函数似乎无法以 12 天的步长循环遍历数据，因为它会返回一条错误消息，指出它必须索引“负值行”，这是不可能的。我使用 yardstick 包 mase 函数 时也遇到了同样的情况。有谁知道如何修复该问题，或者可以指出我做错了什么吗？
查看带有模拟数据的示例
#load libraries#

library(tidiverse)

library(Metrics)

#create simulation data

set.seed(123)
y&lt;-sample(1:150,662,replace = T)
X1_L12&lt;-runif(662,-1,1)#假设 X1 滞后 12 天
X2_L12&lt;-runif(662,-1:1)#假设 X2 滞后 12 天
date&lt;-sample(1:662)

dat&lt;-data.frame(date,cases,X1_L12,X2_L12)

#create the function to compute mase setting the naive prediction at t-12

mase_lag16 &lt;- function(data, lev = NULL, model = NULL) {
data$pred &lt;- as.numeric(data$pred)
data$obs &lt;- as.numeric(data$obs)
masefunction = mase(data$obs,data$pred,12)
names(masefunction) &lt;- c(&#39;MASE&#39;)
masefunction
}

#OOS 验证框架
#创建时间片并定义性能指标
myTimeControlmase &lt;- trainControl(method = &quot;timeslice&quot;,
initialWindow = 53,
horizo​​n = 13,
skip=65,
fixedWindow = TRUE,
summaryFunction = mase_lag12)

#model
glmnetmod_lag16 = train(cases~X1_L12+X2_L12,
method = &quot;glmnet&quot;,
family=&quot;poisson&quot;,
trControl = myTimeControlmase,maximize=FALSE,
preProc = c(&quot;range&quot;),
data=dat)

在代码的最后一部分拟合模型时，它会返回以下错误消息：
actual[1:naive_end] 中的错误：只有 0 可以与负下标混合
]]></description>
      <guid>https://stackoverflow.com/questions/79134999/error-with-mase-cant-indexes-rows-beyond-data-when-evaluating-model-forecasti</guid>
      <pubDate>Mon, 28 Oct 2024 20:20:47 GMT</pubDate>
    </item>
    <item>
      <title>快速 AI 暹罗模型没有改进</title>
      <link>https://stackoverflow.com/questions/79134990/fast-ai-siamese-model-not-improving</link>
      <description><![CDATA[所以我按照这个教程：Fast Ai Siamese，然而在我完成它之后，我的准确率只有 50%。我尝试了很多方法，但都没有奏效。所以我认为问题可能出在 Siamese 实现本身，但我对 fast.ai 经验很少，也不知道如何修复它，甚至不知道从哪里开始。也许我遗漏了一些明显的东西？无论如何，任何评论都有帮助。这是我的代码
class SiameseImage(fastuple):

def show(self, ctx=None, **kwargs):
if len(self) &gt; 2：
img1，img2，相似度 = 自身
其他：
img1，img2 = 自身
相似度 = &#39;未确定&#39;

如果不是 isinstance(img1，Tensor)：
如果 img2.size != img1.size： img2 = img2.resize(img1.size)
t1 = 张量(img1)
t2 = 张量(img2)
t1 = t1.permute(2,0,1)
t2 = t2.permute(2,0,1)
其他：
t1 = img1
t2 = img2

line = t1.new_zeros(t1.shape[0]，t1.shape[1]，10)
返回 show_image(torch.cat([t1，line，t2]，dim=2)，title = 相似度，ctx=ctx， **kwargs) 

class ImageTuple(fastuple):

@classmethod
def create(cls, fns): return cls(tuple(PILImage.create(f) for f in fns))

def show(self, ctx=None, **kwargs): 
t1,t2 = self
if not isinstance(t1, Tensor) or not isinstance(t2, Tensor) or t1.shape != t2.shape: return ctx
line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)
return show_image(torch.cat([t1,line,t2], dim=2), ctx=ctx, **kwargs)

class SiameseModel(Module):

def __init__(self,coder, head):
self.encoder =coder
self.head = head

def forward(self, x1, x2):
ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim = 1)
return self.head(ftrs)

class SiameseTransform(Transform):

def __init__(self, files, splits, labels):
self.labels = labels
self.splbl2files = [{l: [f for f in files[splits[i]] if parent_label(f) == l] for l in labels}
for i in range(2)]
self.valid = {f: self._draw(f,1) for f in files[splits[1]]}

def encodes(self, f):
f2,same = self.valid.get(f, self._draw(f,0))
img1,img2 = PILImage.create(f),PILImage.create(f2)
return SiameseImage(img1, img2, int(same))

def _draw(self, f, split=0):
same = random.random() &lt; 0.5
cls = parent_label(f)
如果不相同：cls = random.choice(L(l for l in self.labels if l != cls)) 
返回 random.choice(self.splbl2files[split][cls])，相同

def get_x(t): 返回 t[:2]
def get_y(t): 返回 t[2]

def siamese_splitter(model):
返回 [params(model.encoder), params(model.head)]

def loss_func(out, targ):
返回 CrossEntropyLossFlat()(out, targ.long())

def train(dataset_path):
files = get_image_files(dataset_path)

labels = list(set(files.map(parent_label)))

coder = create_body(resnet50(), cut=-2)
head = create_head(2048*2, 2, ps=0.5)
model = SiameseModel(编码器，head)

splits = RandomSplitter()(文件)
tfm = SiameseTransform(文件，splits，标签)
tls = TfmdLists(文件，tfm，splits=splits)
dls = tls.dataloaders(
after_item=[Resize(256, method=&#39;squash&#39;), ToTensor], 
after_batch=[IntToFloatTensor, *aug_transforms(flip_vert=True, do_flip=True, max_rotate=50, max_warp=0.4, max_zoom=1.3), Normalize.from_stats(*imagenet_stats)],
bs = 8
)

torch.cuda.empty_cache()
learn = Learner(dls， model, loss_func=loss_func, splitter=siamese_splitter, metrics=accuracy)
learn.fine_tune(
epochs = 15,
base_lr=2.51e-5,
cbs=[SaveModelCallback(monitor=&#39;valid_loss&#39;), EarlyStoppingCallback(monitor=&#39;valid_loss&#39;, waiting=5)])
learn.export(&#39;siamese1.pkl&#39;)

我尝试更改批量大小、不同的架构、图像大小、epoch、各种批量转换和其他内容，但没有任何变化，我的准确率始终为 50%...]]></description>
      <guid>https://stackoverflow.com/questions/79134990/fast-ai-siamese-model-not-improving</guid>
      <pubDate>Mon, 28 Oct 2024 20:17:44 GMT</pubDate>
    </item>
    <item>
      <title>如何通过递归特征消除来选择最佳的特征数量和具有最高F1和ROC的最佳算法？</title>
      <link>https://stackoverflow.com/questions/79134937/how-to-select-the-best-number-of-feature-and-best-algorithm-with-highest-f1-and</link>
      <description><![CDATA[我想对我的数据的多个子集（即每个子集针对每个特定客户）运行递归特征消除，其中有 X 个特征（即 4-12）和 Y 个不同算法（即 DecisionTreeClassifier、GradientBoostingClassifier、LogisticRegression）。
如何创建一个代码，在每个子集上生成并显示具有最佳算法的最佳特征数量（最高精确度-召回率，其次是最高 ROC），而不是逐一查看结果？我的数据集不平衡。
下面是我尝试在逻辑回归中为不同数量的特征生成结果，但我仍然需要替换估算器中的算法并逐一检查结果。
for i in range(4, 13):
rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)
rfe.fit(X_resampled,Y_resampled)
selector = X_resampled.columns[rfe.support_]
X_train_selected = X_resampled[selector]
X_test_selected = X_test[selector]
log_reg_model = sm.Logit(Y_resampled, X_train_selected).fit()
pred_test = log_reg_model.predict(X_test_selected)
pred_test_1 = np.where(pred_test&gt;0.5,1,0)
logit_roc_auc = roc_auc_score(Y_test, pred_test)
fpr, tpr, 阈值 = roc_curve(Y_test, pred_test)
print(f&#39;特征数量：{i}，准确率得分：{accuracy_score(Y_test, pred_test_1)}&#39;)
print(f&#39;特征数量：{i}，ROC：&#39;，logit_roc_auc)
precision, recall, 阈值 = precision_recall_curve(Y_test, pred_test)
print(f&#39;特征数量：{i}，f1 得分：{f1_score(Y_test, pred_test_1)}&#39;)
print(f&#39;特征数量：{i}，PRC AUC：{auc(recall,precision)}&#39;)
precision = precision_score(Y_test, pred_test_1)
recall = recall_score(Y_test, pred_test_1)
print(f&#39;特征数量：{i},召回率：&#39;, recall)
print(f&#39;特征数量：{i},精确度：&#39;, precision)
]]></description>
      <guid>https://stackoverflow.com/questions/79134937/how-to-select-the-best-number-of-feature-and-best-algorithm-with-highest-f1-and</guid>
      <pubDate>Mon, 28 Oct 2024 19:57:20 GMT</pubDate>
    </item>
    <item>
      <title>性能权衡 - 降低速度以更好地利用 TTS 中的内存[关闭]</title>
      <link>https://stackoverflow.com/questions/79134305/performance-trade-off-reduce-speed-for-better-memory-usage-in-tts</link>
      <description><![CDATA[我正在使用 https://github.com/coqui-ai/tts/ 运行多个 TTS 模型。目前，我正在试验 your_tts 模型，这是一个较小的模型。我有 4 GB 内存 GPU（GTX 1050），支持 Cuda。一段时间内，它运行没有问题，而且速度非常快。过了一会儿，它抛出了内存错误。我想知道是否有办法降低速度并减少内存使用？通过 config.json 或类似的东西，也许在代码中通过一些参数？]]></description>
      <guid>https://stackoverflow.com/questions/79134305/performance-trade-off-reduce-speed-for-better-memory-usage-in-tts</guid>
      <pubDate>Mon, 28 Oct 2024 16:30:39 GMT</pubDate>
    </item>
    <item>
      <title>模型（输入，训练=True）和模型（输入，训练=False）之间的巨大差异[关闭]</title>
      <link>https://stackoverflow.com/questions/79134261/huge-difference-between-modelinput-training-true-and-modelinput-training-fa</link>
      <description><![CDATA[我被要求根据我读过的一篇文章实现一个机器学习模型。为此，该论文推荐了一种特定类型的预测可靠性度量：该模型运行 M 次随机前向传递，其中有 M 种不同的 dropout 模式（其中 alpha=0.5%）。但是，我注意到，模型（输入，训练=True）的 M 次随机运行的平均输出与模型（输入，训练=False）的 M 次随机运行的平均输出有很大不同。这是由于什么原因？如果不使用 dropout 模式，运行不应该收敛到相似的值吗？]]></description>
      <guid>https://stackoverflow.com/questions/79134261/huge-difference-between-modelinput-training-true-and-modelinput-training-fa</guid>
      <pubDate>Mon, 28 Oct 2024 16:16:55 GMT</pubDate>
    </item>
    <item>
      <title>当没有部分拟合选项时，如何训练分区数据集？</title>
      <link>https://stackoverflow.com/questions/79133844/how-do-i-train-a-partitioned-dataset-when-there-is-not-an-option-for-partial-fit</link>
      <description><![CDATA[我正在从包含 10 个分区的数据集训练 ML 模型，这样我就不会耗尽可用内存。我目前正在每个分区上训练 3 个不同的模型，然后将它们放入 VotingRegressor 中，然后再次进行拟合，但是由于它占用了大量内存，我无法将其拟合到整个训练集。这里有一个小片段
all_feature_cols = [f&quot;feature_{i:02d}&quot; for i in range(79)]

if TRAINING:
# 初始化列表以存储模型
lgbm_models = []
xgb_models = []
cat_models = []

# 逐步训练每个模型
for partion in range(10):
start_time = time.time()

# 为每个分区创建新的模型实例
lgbm = LGBMRegressor(num_leaves=127, n_estimators=200, max_depth=3, 
learning_rate=0.05, device_type=&#39;gpu&#39;, verbose=-1,
reg_alpha = 0.1, reg_lambda = 0.1)
xgb = XGBRegressor(n_estimators=200, min_child_weight=5, max_depth=7, 
learning_rate=0.01, device=&#39;cpu&#39;,
reg_alpha = 0.1, reg_lambda = 0.1)
cat = CatBoostRegressor(n_estimators=200, max_depth=7, learning_rate=0.05, 
reg_lambda = 0.1, task_type=&#39;GPU&#39;, verbose=False)

# 过滤当前分区并分别收集目标
partition_df = df.filter(pl.col(&quot;partition_id&quot;) ==partition)

# 在预处理之前提取并收集目标列
y =partition_df.select(&quot;responder_6&quot;).collect().to_numpy().ravel()

# 预处理特征（不包括目标）
X =partition_df.select(all_feature_cols).collect().to_numpy()

# 分成训练/验证并保持时间顺序
train_idx = int(len(X) * 0.8)
X_train, X_test = X[:train_idx], X[train_idx:]
y_train, y_test = y[:train_idx], y[train_idx:]

# 在此分区上训练模型
lgbm.fit(X_train, y_train)
xgb.fit(X_train, y_train)
cat.fit(X_train, y_train)

# 使用分区标识符保存训练好的模型
lgbm_models.append((f&#39;lgbm_{partition}&#39;, deepcopy(lgbm)))
xgb_models.append((f&#39;xgb_{partition}&#39;, deepcopy(xgb)))
cat_models.append((f&#39;cat_{partition}&#39;, deepcopy(cat)))

# 计算已用时间
end_time = time.time()
elapsed_time = end_time - start_time
elapsed_str = str(timedelta(seconds=int(elapsed_time)))
print(f&quot;分区 {partition} 在 {elapsed_str} 内完成&quot;)

# 清理内存
if partition &lt; 9:
del X, y, X_train, X_test, y_train, y_test, partition_df
else:
del X, y, X_train, y_train, partition_df
gc.collect()

# 创建并拟合 VotingRegressor 和所有经过训练的模型
model = VotingRegressor(lgbm_models + xgb_models + cat_models)

# 拟合模型
model.fit(X_test, y_test)

y_pred = model.predict(X_test)
print(r2_score(y_test, y_pred))

# 保存最终模型 
dump(model, &quot;/kaggle/working/JS_model.joblib&quot;)
```
]]></description>
      <guid>https://stackoverflow.com/questions/79133844/how-do-i-train-a-partitioned-dataset-when-there-is-not-an-option-for-partial-fit</guid>
      <pubDate>Mon, 28 Oct 2024 14:30:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在 IBM Watson Assistant 响应中动态包含来自 Watson Discovery 的完整且正确的 URL？[关闭]</title>
      <link>https://stackoverflow.com/questions/79133652/how-to-dynamically-include-full-and-correct-urls-from-watson-discovery-in-ibm-wa</link>
      <description><![CDATA[我正在使用 IBM Watson Assistant，使用 Llama 3.8 作为语言模型，我面临的一个问题是，模型始终无法在其响应中检索正确的 URL。我从 Watson Discovery 中的文档中动态提取这些 URL，并且每个响应都需要根据提出的问题包含不同的特定链接。尽管我的提示明确指示模型包含一个特定的完整链接，但模型的响应始终包含不正确或不完整的链接。
是否有人遇到过与 IBM Watson Assistant 或其他 LLM 类似的问题，即模型无法检索确切的指定 URL，尤其是在不同响应中需要不同的链接时？是否有任何已知的解决方法、配置或提示调整可以确保模型可靠地从 Watson Discovery 中检索并显示正确的链接？
以下是我尝试过的概述：
提示调整：我在提示中包含了明确的指示“包含整个链接而不缩短它”，我甚至尝试将链接直接放在提示中作为示例。但是，模型要么生成不完整的链接，要么生成完全错误的链接。
提示示例：
下面是我的提示的简化版本，它指示模型将 {DOCUMENTATION_LINK} 替换为与每个问题相关的文档的实际链接：
&quot;您是客户关系经理。您的目的是提供 CRM 流程的简要摘要。如果用户需要详细步骤，请回复：&#39;有关完整流程和所有详细步骤，请点击此链接：{DOCUMENTATION_LINK}&#39;。始终将“{DOCUMENTATION_LINK}”替换为问题中文档的实际链接。&quot;
配置详细信息：
温度：0.5
最大新令牌：900
停止序列：[&quot; &quot;]
重复惩罚：1]]></description>
      <guid>https://stackoverflow.com/questions/79133652/how-to-dynamically-include-full-and-correct-urls-from-watson-discovery-in-ibm-wa</guid>
      <pubDate>Mon, 28 Oct 2024 13:41:29 GMT</pubDate>
    </item>
    <item>
      <title>使用没有跳跃连接的 U-Net 进行图像到图像处理。这是真的吗？</title>
      <link>https://stackoverflow.com/questions/79132335/image-to-image-with-u-net-with-no-skip-connections-is-it-real</link>
      <description><![CDATA[如果我想从其他图像（图像到图像）获取一些图像，我可以使用没有跳过连接的 U-Net 吗？因为我不需要保留结构。例如，为了改变某些对象的相机视角。
例如，此模型为 256x256px。它适用于 3 对训练对（输入图像-输出图像），并带有角度增强（+/- 5 度、+/-10 度、+/-15 度），但不适用于 1.000 对。
# 编码器
c = layer.Conv2D(32, kernel_size=4, strides=2, padding=&quot;same&quot;)(inputs)
c = layer.LeakyReLU(negative_slope=0.2)(c) 

c = layer.Conv2D(64, kernel_size=4, strides=2, padding=&quot;same&quot;)(c)
c = layer.LeakyReLU(negative_slope=0.2)(c)

c = layer.Conv2D(128, kernel_size=4, strides=2, padding=&quot;same&quot;)(c)
c =层。LeakyReLU（负斜率=0.2）（c）

c = 层。Conv2D（256，kernel_size=4，strides=2，padding=“相同”（c）

c = 层。LeakyReLU（负斜率=0.2）（c）

c = 层。Conv2D（512，kernel_size=4，strides=2，padding=“相同”（c）

c = 层。LeakyReLU（负斜率=0.2）（c）

c = 层。Conv2D（1024，kernel_size=4，strides=2，padding=“相同”（c）

c = 层。LeakyReLU（负斜率=0.2）（c）

# 瓶颈
b = InstanceNormalization（）（c）

b =图层。重塑((-1,))(b)
b = 图层。密集(512*4*4, kernel_regularizer=l2_reg,)(b)
b = 图层。LeakyReLU(negative_slope=0.2)(b)

b = 图层。Dropout(0.2)(b)

b = 图层。密集(512*4*4*2, kernel_regularizer=l2_reg,)(b)
b = 图层。LeakyReLU(negative_slope=0.2)(b)
b = 图层。重塑((4,4,1024))(b)

# 解码器
d = 图层。UpSampling2D(size=(2, 2))(b)
d = 图层。Conv2D(1024, kernel_size=4, padding=&quot;same&quot;)(d)
d =层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2, 2））（d）

d = 层。Conv2D（512，内核大小=4，填充=“相同”）（d）

d = 层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2, 2））（d）

d = 层。Conv2D（256，内核大小=4，填充=“相同”）（d）

d = 层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2, 2））（d）

d = 层。Conv2D（128，内核大小=4，填充=“相同”）（d）

d =层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2，2））（d）

d = 层。Conv2D（64，kernel_size=4，padding=“相同”（d）

d = 层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2，2））（d）

d = 层。Conv2D（32，kernel_size=4，padding=“相同”（d）

d = 层。LeakyReLU（负斜率=0.2）（d）`

# 输出
输出 = 层。Conv2D（3，kernel_size=3，padding=“相同”，激活=“tanh”（d）

模型 = models.Model(inputs=inputs, output=outputs, name=&quot;build_unet&quot;)
return model`

无论输入图像是否旋转或扭曲，我都需要获取输出。]]></description>
      <guid>https://stackoverflow.com/questions/79132335/image-to-image-with-u-net-with-no-skip-connections-is-it-real</guid>
      <pubDate>Mon, 28 Oct 2024 07:00:00 GMT</pubDate>
    </item>
    <item>
      <title>删除所有人口后，NEAT 给出错误</title>
      <link>https://stackoverflow.com/questions/79130999/neat-giving-error-after-deleting-all-the-population</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79130999/neat-giving-error-after-deleting-all-the-population</guid>
      <pubDate>Sun, 27 Oct 2024 16:27:23 GMT</pubDate>
    </item>
    <item>
      <title>如何将 AWS Bedrock 与我的数据库集成以实现基于向量的 LLM 响应上下文检索？</title>
      <link>https://stackoverflow.com/questions/79130070/how-can-i-integrate-aws-bedrock-with-my-database-to-enable-vector-based-context</link>
      <description><![CDATA[我正在构建一个 AI 驱动的忠诚度应用程序，并希望利用大型语言模型 (LLM) 根据我的数据库内容提供响应。我目前的计划是：

将数据库数据转换为向量嵌入：我想将我的结构化数据转换为向量嵌入，以便 LLM 可以更轻松地使用它。
将嵌入存储在向量数据库中：这个想法是存储嵌入以实现基于相似性的高效检索。
使用 AWS Bedrock LLM：我想根据用户的查询从我的数据库中检索上下文，并使用 AWS Bedrock 将此上下文输入到 LLM 中以生成响应。

我将不胜感激任何有关以下方面的指导：

嵌入转换：是否有任何推荐的工具或与 AWS Bedrock 兼容的模型用于将关系数据库中的结构化数据转换为有用的嵌入？ Amazon Titan 是否适合这种情况，还是其他模型更好？

向量数据库选项：对于存储和查询嵌入，Amazon OpenSearch 是否合适，还是我应该考虑 FAISS 之类的东西？我的目标是实现高容量、实时的检索效率。

LLM 集成最佳实践：检索类似嵌入后，格式化并将此上下文传递给 AWS Bedrock 上的 LLM 的最佳方法是什么？有任何示例、文章或模板吗？

有关特定工具、模型或文章的建议，可以提供关于在 AWS Bedrock 上设置此工作流程的进一步见解。

]]></description>
      <guid>https://stackoverflow.com/questions/79130070/how-can-i-integrate-aws-bedrock-with-my-database-to-enable-vector-based-context</guid>
      <pubDate>Sun, 27 Oct 2024 07:10:23 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 真的需要可学习的自注意力层吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79120787/do-the-transformers-really-need-the-learnable-self-attention-layer</link>
      <description><![CDATA[Transformer 的核心组件是自注意力机制，它负责通过计算注意力分数来捕获 token 之间的依赖关系。这些分数通常通过可学习的投影（查询、键、值）计算得出。
我想知道自注意力层是否需要可学习。是否可以用更简单或不可学习的东西（例如固定或动态生成的表示）替换可学习的查询、键和值投影？这是否仍允许模型捕获足够的上下文信息以用于语言建模等任务？
我尝试删除所有可学习的投影（查询、键、值）并直接将点积应用于 token 本身。但是，我正在寻找一种替代方案，其中可以在不使用可学习参数的情况下将 token 投影到更有意义的维度。有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79120787/do-the-transformers-really-need-the-learnable-self-attention-layer</guid>
      <pubDate>Thu, 24 Oct 2024 07:21:36 GMT</pubDate>
    </item>
    <item>
      <title>如何在 p5.js 中更改 bodySegmentation-mask-body-parts 的默认背景？</title>
      <link>https://stackoverflow.com/questions/79110858/how-to-change-the-default-background-of-bodysegmentation-mask-body-parts-in-p5-j</link>
      <description><![CDATA[我试图更改检测到的身体部位背后的背景，但无法使其工作。即使我修改了 background(...) 函数，它仍然默认为白色。有人能解释为什么会发生这种情况吗？
这是我正在处理的文件的链接：
https://editor.p5js.org/speedyonion/sketches/X9mwX9XB9
let bodySegmentation;
let video;
let fragmentation;

let options = {
maskType: &quot;parts&quot;,
};

function preload() {
bodySegmentation = ml5.bodySegmentation(&quot;BodyPix&quot;, options);
}

function setup() {
createCanvas(640, 480);
// 创建视频
video = createCapture(VIDEO);
video.size(640, 480);
video.hide();

bodySegmentation.detectStart(video, gotResults);
}

function draw() {
background(0,0,0);
image(video, 0, 0);
if (segmentation) {
image(segmentation.mask, 0, 0, width, height);
}
}

// 身体分割回调函数
function gotResults(result) {
fragmentation = result;
}
]]></description>
      <guid>https://stackoverflow.com/questions/79110858/how-to-change-the-default-background-of-bodysegmentation-mask-body-parts-in-p5-j</guid>
      <pubDate>Mon, 21 Oct 2024 16:01:40 GMT</pubDate>
    </item>
    <item>
      <title>迁移学习预训练模型</title>
      <link>https://stackoverflow.com/questions/78875648/transfer-learning-pretrained-model</link>
      <description><![CDATA[我在 Google Colab 上拟合迁移学习模型。但是，我在代码中遇到了一条警告消息
Epoch 1/30
/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: 
UserWarning：您的 `PyDataset` 类应在其构造函数中调用 `super().__init__(**kwargs)`。`**kwargs` 可以包括 `workers`、`use_multiprocessing`、`max_queue_size`。
请勿将这些参数传递给 `fit()`，因为它们将被忽略。
self._warn_if_super_not_called()

在第一个 epoch 之后，我收到以下错误：
----------------------------------------------------------------------------------------
KeyboardInterrupt Traceback（最近一次调用最后一次）
&lt;ipython-input-23-962a870d4412&gt; in &lt;cell line: 16&gt;()
14 # 拟合模型
15 # 运行单元。执行需要一些时间
---&gt; 16 training_history = model_efficientnet.fit(
17 training_set,
18 validation_data=validate_set,

我已经成功地拟合了其他六个迁移学习模型，没有任何问题，它们的准确率令人满意。
如何解决这个问题？
我想获得训练准确率和验证准确率]]></description>
      <guid>https://stackoverflow.com/questions/78875648/transfer-learning-pretrained-model</guid>
      <pubDate>Thu, 15 Aug 2024 14:49:45 GMT</pubDate>
    </item>
    <item>
      <title>如何在spark集群环境下高效训练word2vec模型？</title>
      <link>https://stackoverflow.com/questions/34377742/how-to-train-word2vec-model-efficiently-in-the-spark-cluster-environment</link>
      <description><![CDATA[我想在我的 Spark 集群上训练 10G 新闻语料的 word2vec 模型。
以下是我的 spark 集群的配置：

一个 Master 和 4 个 Worker
每个都有 80G 内存和 24 个核心

但是我发现使用 Spark Mllib 训练 Word2vec 并没有充分利用集群的资源。
例如：
ubuntu 中的 top 命令图片
如上图所示，只有一个 worker 使用了 100% 的 cpu，其他三个 worker 未使用（因此不粘贴它们的图片）并且我刚刚如何训练一个关于 2G 新闻语料的 word2vec 模型，大约需要 6 小时，所以我想知道如何更有效地训练模型？提前谢谢大家:)

UPDATE1：以下命令是我在 spark-shell 中使用的

如何启动 spark-shell
spark-shell \
--master spark://ip:7077 \
--executor-memory 70G \
--driver-memory 70G \
--conf spark.akka.frameSize=2000 \
--conf spark.driver.maxResultSize=0 \
--conf spark.default.parallelism=180
以下命令是我在spark-shell中训练word2vec模型时使用的：
//导入相关包
import org.apache.spark._
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
//读取约10G newsdata 语料库
val newsdata = sc.textFile(&quot;hdfs://ip:9000/user/bd/newsdata/*&quot;,600).map(line =&gt; line.split(&quot; &quot;).toSeq)
//配置word2vec参数
val word2vec = new Word2Vec()
word2vec.setMinCount(10)
word2vec.setNumIterations(10)
word2vec.setVectorSize(200)
//训练模型
val model = word2vec.fit(newsdata)


更新2：
我已经训练模型大约24小时了，但还没有完成。集群运行如下：
只有一个worker使用了100%的cpu，其他三个worker没有像以前一样被使用。]]></description>
      <guid>https://stackoverflow.com/questions/34377742/how-to-train-word2vec-model-efficiently-in-the-spark-cluster-environment</guid>
      <pubDate>Sun, 20 Dec 2015 03:40:58 GMT</pubDate>
    </item>
    <item>
      <title>免费提供的真实公共数据[关闭]</title>
      <link>https://stackoverflow.com/questions/24962111/freely-available-real-public-data</link>
      <description><![CDATA[注意：我不是在寻找样本数据。
不同域中向公众免费公开的真实数据集：
例如：

FCM 的财务报告。
http://www.cftc.gov/MarketReports/FinancialDataforFCMs/HistoricalFCMReports/index.htm
YouTube 数据：（频道的受欢迎程度指标和统计数据）
https://developers.google.com/youtube/analytics/

如果有更多此类数据，请分享。 
可能与以下内容或其他任何可能有用的内容相关。
可能涉及医疗领域、药房、药品消费。
不同城市、道路等的交通、事故、伤亡。
不同地区的妇女安全指标。
食品/饮料消费、价格。
根据地区/公寓的垃圾收集量、洗手间。
有多少孤儿院以及他们获得了多少资助。
一个城市有多少个残疾人停车位等。
如果您认为这个问题不适合这种类型的平台，我将非常感激您为我推荐一个更好的论坛。]]></description>
      <guid>https://stackoverflow.com/questions/24962111/freely-available-real-public-data</guid>
      <pubDate>Fri, 25 Jul 2014 18:18:32 GMT</pubDate>
    </item>
    </channel>
</rss>