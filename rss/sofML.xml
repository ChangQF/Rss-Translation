<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 30 Mar 2024 00:55:48 GMT</lastBuildDate>
    <item>
      <title>jupyter笔记本中的tensorflow导入错误（ import tensorflow_io as tfio ）</title>
      <link>https://stackoverflow.com/questions/78246165/import-error-in-tensorflow-in-jupyter-notebook-import-tensorflow-io-as-tfio</link>
      <description><![CDATA[导入tensorflow_io as tfio 我在音频分类器深度学习项目的代码中遇到错误，有人可以帮助我吗？
来自nicholes renotte yt频道的项目，jupyter笔记本中的tensor_io代码存在问题。]]></description>
      <guid>https://stackoverflow.com/questions/78246165/import-error-in-tensorflow-in-jupyter-notebook-import-tensorflow-io-as-tfio</guid>
      <pubDate>Fri, 29 Mar 2024 22:24:20 GMT</pubDate>
    </item>
    <item>
      <title>2类的组合</title>
      <link>https://stackoverflow.com/questions/78246119/combination-of-2-classes</link>
      <description><![CDATA[创建单独的数据生成器以进行训练和验证
train_data = data_generator.flow_from_directory(
火车路径，
目标大小=(img_size,img_size),
批量大小=批量大小_训练，
class_mode=&#39;分类&#39;,
类=[&#39;AKIEC et BCC&#39;,&#39;VASC et DF&#39;,&#39;MEL &amp; NV&amp; BKL&#39;]
）
找到属于 3 个类别的 0 张图片。]]></description>
      <guid>https://stackoverflow.com/questions/78246119/combination-of-2-classes</guid>
      <pubDate>Fri, 29 Mar 2024 22:04:14 GMT</pubDate>
    </item>
    <item>
      <title>如何在Tensorflow中转换为对数梅尔谱图？</title>
      <link>https://stackoverflow.com/questions/78245969/how-to-convert-to-log-mel-spectrogram-in-tensorflow</link>
      <description><![CDATA[我尝试修改我的预处理函数来创建 log-mel-spectrogram，以便我的 CNN 模型可以在其上进行训练。
我尝试将我的频谱图转换为对数梅尔频谱图，但我无法做到这一点。但是，由于我使用的是tensorflow，所以这个转换过程需要使用tensorflow框架。
def 预处理（文件路径，标签）：
    wav = load_wav_16k_mono(文件路径)
    wav = wav[:8000]
    Zero_padding = tf.zeros([8000] - tf.shape(wav), dtype=tf.float32)
    wav = tf.concat([zero_padding, wav],0)
    
    频谱图 = tf.signal.stft(wav,frame_length=100,frame_step=20)
    频谱图 = tf.abs(频谱图)
    频谱图= tf.expand_dims（频谱图，轴= 2）
    
    返回频谱图、标签
]]></description>
      <guid>https://stackoverflow.com/questions/78245969/how-to-convert-to-log-mel-spectrogram-in-tensorflow</guid>
      <pubDate>Fri, 29 Mar 2024 21:10:07 GMT</pubDate>
    </item>
    <item>
      <title>了解 PyTorch 模型中的批处理</title>
      <link>https://stackoverflow.com/questions/78245568/understanding-batching-in-pytorch-models</link>
      <description><![CDATA[我有以下模型，它构成了我的整个模型管道中的步骤之一：
导入火炬
将 torch.nn 导入为 nn

类 NPB(nn.Module):
    def __init__(self, d, nhead, num_layers, dropout=0.1):
        超级（NPB，自我）.__init__()
            
        self.te = nn.TransformerEncoder(
            nn.TransformerEncoderLayer（d_model = d，nhead = nhead，dropout = dropout，batch_first = True），
            层数=层数，
        ）

        self.t_emb = nn.Parameter(torch.randn(1, d))
        
        self.L = nn.Parameter(torch.randn(1, d))

        self.td = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=d, nhead=nhead, dropout=dropout, batch_first=True),
            层数=层数，
        ）

        self.ffn = nn.Linear(d, 6)
    
    def 向前（自身，t_v，t_i）：
        打印（“--------------- t_v，t_i -----------------”）
        打印（&#39;t_v：&#39;，元组（t_v.shape））
        print(&#39;t_i: &#39;, 元组(t_i.shape))

        打印（“--------------- t_v + t_i + t_emb -----------------”）
        _x = t_v + t_i + self.t_emb
        打印（元组（_x.shape））

        print(&quot;---------------- 特 --------------&quot;)
        _x = self.te(_x)
        打印（元组（_x.shape））
        
        print(&quot;---------------- td ---------------&quot;)
        _x = self.td(self.L, _x)
        打印（元组（_x.shape））

        print(&quot;---------------- ffn --------------&quot;)
        _x = self.ffn(_x)
        打印（元组（_x.shape））

        返回_x

这里 t_v 和 t_i 是来自早期编码器块的输入。我将它们作为 (4,256) 的形状传递，其中 256 是特征数量，4 是批量大小。 t_emb 是时间嵌入。 L 表示学习矩阵，表示查询的嵌入。我使用以下代码测试了该模块块：
t_v = torch.randn((4,256))
t_i = torch.randn((4,256))
npb = NPB(d=256, nhead=8, num_layers=2)
npb（t_v，t_i）

输出：
&lt;前&gt;&lt;代码&gt;================ NPB ===============
--------------- t_v, t_i -----------------
电视: (4, 256)
t_i: (4, 256)
--------------- t_v + t_i + t_emb -----------------
(4, 256)
--------------- 特 ---------------
(4, 256)
--------------- TD ---------------
(1, 256)
--------------- ffn ---------------
(1, 6)

我期望输出的形状应为 (4,6)，大小为 6 的批次中每个样本有 6 个值。但输出的大小为(1,6)。经过大量调整后，我尝试将 t_emb 和 L 形状从 (1,d) 更改为 (4,d)&lt; /code&gt;，因为我不希望所有采样共享这些变量（通过广播：
self.t_emb = nn.Parameter(torch.randn(4, d)) # [n, d] = [4, 256]
self.L = nn.Parameter(torch.randn(4, d))

这给出了所需的形状输出（4,6：
&lt;前&gt;&lt;代码&gt;---------------------------- t_v, t_i -----------------
电视: (4, 256)
t_i: (4, 256)
--------------- t_v + t_i + t_emb -----------------
(4, 256)
--------------- 特 ---------------
(4, 256)
--------------- TD ---------------
(4, 256)
--------------- ffn ---------------
(4, 6)

我有以下疑问：
Q1. 到底为什么将 L 和 t_emb 形状从 (1,d) 更改为  (4,d) 有效吗？为什么它不能通过广播与(1,d)一起工作？
Q2.我是否以正确的方式进行批处理，或者输出是人为正确的，而在幕后它所做的事情与我预期的不同（预测大小为 4 的批次中每个样本的 6 个值）？&lt; /p&gt;]]></description>
      <guid>https://stackoverflow.com/questions/78245568/understanding-batching-in-pytorch-models</guid>
      <pubDate>Fri, 29 Mar 2024 19:24:58 GMT</pubDate>
    </item>
    <item>
      <title>如何在 lambda 函数中对时间戳和输出列表进行加法和减法？</title>
      <link>https://stackoverflow.com/questions/78245499/how-can-i-do-addition-and-substraction-of-timestamp-and-a-list-of-output-in-a-la</link>
      <description><![CDATA[RFM = sales_data.groupby([&#39;CLIENT_ID&#39;]).agg({
    &#39;CLIENT_ID&#39;: lambda x: (last_purchase_date - x.max()).days,
    &#39;Transaction_ID&#39;: &#39;计数&#39;,
    &#39;网络&#39;：&#39;总和&#39;
})

RFM.rename(columns={&#39;CLIENT_ID&#39;: &#39;Recency&#39;, &#39;Transaction_ID&#39;: &#39;Frequency&#39;, &#39;NET&#39;: &#39;MonetaryValue&#39;}, inplace= True)
显示(RFM)

我不知道如何修复我已经转换的这些东西，但它不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/78245499/how-can-i-do-addition-and-substraction-of-timestamp-and-a-list-of-output-in-a-la</guid>
      <pubDate>Fri, 29 Mar 2024 19:08:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在 python 中模拟 Microsoft Excel 的求解器功能（GRG 非线性）？</title>
      <link>https://stackoverflow.com/questions/78244486/how-can-i-emulate-microsoft-excels-solver-functionality-grg-nonlinear-in-pyth</link>
      <description><![CDATA[演示 Excel 求解器使用的屏幕截图：

我的任务是自动化某个 Excel 工作表。该工作表恰好使用名为 Solver 的 Excel 插件实现了逻辑。它使用单元格 $O$9 中的单个值 (-1.95624)（这是图中用红色和蓝色墨水突出显示的计算结果）作为输入值，然后使用名为的算法返回 C、B1 和 B2 的三个值“GRG非线性回归”。我的任务是用 Python 模拟这个逻辑。以下是我的尝试。主要问题是我没有得到与 Excel 的 Solver 插件计算出的 C、B1 和 B2 相同的值。
导入 numpy、scipy、matplotlib
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 plt
从 scipy.optimize 导入 curve_fit
从 scipy.optimize 导入 Differential_evolution
进口警告

xData = numpy.array([-2.59772914040242,-2.28665528866907,-2.29176070881848,-2.31163972446061,-2.28369414349715,-2.27911303233721,-2.282 22332344644,-2.39089535619106,-2.32144325648778,-2.17235002006179,-2.22906032068685,-2.42044014499938,-2.71639505549322,-2.65 462061336346,- 2.47330475191616,-2.33132910807216,-2.33025978869114,-2.61175064230516,-2.92916553244925,-2.987503044973,-3.00367414706232, -1.45507812104723]) # 使用与参数相同的表名
yData = numpy.array([0.0692847120775066,0.0922342111029099,0.0918076382491768,0.0901635409944003,0.0924824386284127,0.092867647175396, 0.092605957740688,20.0838696111204451,0.0893625419994501,0.102261091024881,0.097171046758256,70.0816272542472914,0.0620128251 290935,0.0657047909578125,0.0777509345715382,0.088561321341585,0.088647672874835,90.0683859871424735,0.0507304952495273,0.047 9936476914665,0.0472601632188253,0.18922126828463 ]) # 使用与参数相同的表名

def func(x, a, b, Offset): # 带偏移量的 Sigmoid A 来自 zunzun.com
    返回 1.0 / (1.0 + numpy.exp(-a * (x-b))) + 偏移量


# 遗传算法最小化（误差平方和）的函数
def sumOfSquaredError(parameterTuple):
    warnings.filterwarnings(“ignore”) # 不通过遗传算法打印警告
    val = func(xData, *parameterTuple)
    返回 numpy.sum((yData - val) ** 2.0)


defgenerate_Initial_Parameters():
    # 用于边界的最小值和最大值
    maxX = max(x数据)
    minX = min(x数据)
    maxY = max(y数据)
    minY = min(yData)

    参数范围 = []
    parameterBounds.append([minX, maxX]) # 的搜索范围
    parameterBounds.append([minX, maxX]) # b 的搜索范围
    parameterBounds.append([0.0, maxY]) # Offset 的搜索范围

    #“种子”用于可重复结果的 numpy 随机数生成器
    结果 = Differential_evolution(sumOfSquaredError,parameterBounds,seed=3)
    返回结果.x

# 生成初始参数值
遗传参数=generate_Initial_Parameters()

# 曲线拟合测试数据
参数，协方差 = curve_fit（func，xData，yData，遗传参数，maxfev = 50000）

# 将参数转换为Python内置类型
params = [float(param) for param in params] # 将 numpy float64 转换为 Python float
C、B1、B2 = 参数
OutputDataSet = pd.DataFrame({“C”：[C]，“B1”：[B1]，“B2”：[B2]，“ProType”：[input_value_1]，“RegType”：[input_value_2 ]})


有什么想法会有帮助吗？提前致谢
这是我的尝试：
鉴于 xData 和 yData 的这些数据集，正确的输出应该是：
C= -2.35443383，B1 = -14.70820051，B2 = 0.0056217]]></description>
      <guid>https://stackoverflow.com/questions/78244486/how-can-i-emulate-microsoft-excels-solver-functionality-grg-nonlinear-in-pyth</guid>
      <pubDate>Fri, 29 Mar 2024 15:08:18 GMT</pubDate>
    </item>
    <item>
      <title>哪种 ARIMA 模型最适合此数据？</title>
      <link>https://stackoverflow.com/questions/78244469/what-kind-of-arima-model-would-be-best-fit-for-this-data</link>
      <description><![CDATA[我正在尝试用Python学习时间序列预测和预测。我绘制了总电子含量的 ACF 和 PACF，它们具有季节性，即 TEC 值在白天达到最大值，在夜间达到最小值。总体数据没有上升或下降趋势，Adfuller检验的检验统计值为-3.67
我得到了以下图表，其中 ACF 逐渐减少，PACF 也逐渐减少，现在我很困惑哪个是 ARIMA 模型的最佳系数。
Timeseries TEC 的 ACF 和 PACF 图
注意：我想预测震后10天和震前20天，然后与实际值进行比较，得到差异值并显示地震对总电子含量的影响。
TEC 值也会受到地磁风暴的影响，因此接下来我将训练一个机器学习模型来对地震的影响和空间天气的影响进行分类。
如果有人想查看我的数据，我可以分享它。
谢谢！
我正在尝试拟合 ARIMA 模型来预测地震前 20 天和地震后 10 天的时间序列 TEC 值。我的目标是获得特定时间范围内的预报，然后将其与实际值进行比较，看看由于地震造成的差异有多大。
我对数据选择 AR 和 MA 系数感到惊讶。]]></description>
      <guid>https://stackoverflow.com/questions/78244469/what-kind-of-arima-model-would-be-best-fit-for-this-data</guid>
      <pubDate>Fri, 29 Mar 2024 15:04:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TensorFlow 中对多个类进行分类</title>
      <link>https://stackoverflow.com/questions/78243492/how-to-classify-multiple-classes-in-tensorflow</link>
      <description><![CDATA[我已经关注了 youtube 上的教程，该教程向我展示了如何对 2 个数据集进行分类（咳嗽，不是咳嗽），但现在我需要添加一个额外的类，即打喷嚏，因此需要训练 3 个类上（咳嗽，打喷嚏，其他），我不知道该怎么做。请帮忙！！！
在代码中，模型在 2 个类别（咳嗽、not_cough）上进行训练并且表现相当不错，但我无法让它在多个类别（例如咳嗽、打喷嚏、其他）上工作。
导入操作系统
从 matplotlib 导入 pyplot 作为 plt
将张量流导入为 tf
将tensorflow_io导入为tfio
从tensorflow.keras.models导入顺序，load_model
从tensorflow.keras.layers导入Conv2D、Dense、Flatten、MaxPool2D、Dropout、TimeDistributed、Reshape
从tensorflow.keras.optimizers.legacy导入Adam
从 keras 导入层
从 keras.utils 导入到_categorical

def load_wav_16k_mono(文件名):
    # 加载编码后的wav文件
    file_contents = tf.io.read_file(文件名)
    # 解码 wav（按通道的张量）
    wav，sample_rate = tf.audio.decode_wav（文件内容，desired_channels = 1）
    # 删除尾随轴
    wav = tf.squeeze(wav, 轴=-1)
    样本率 = tf.cast(样本率，dtype=tf.int64)
    # 从 44100Hz 到 16000Hz - 音频信号的幅度
    wav = tfio.audio.resample(wav,rate_in=sample_rate,rate_out=16000)
    返回波形

def 预处理（文件路径，标签）：
    wav = load_wav_16k_mono(文件路径)
    wav = wav[:8000]
    Zero_padding = tf.zeros([8000] - tf.shape(wav), dtype=tf.float32)
    wav = tf.concat([zero_padding, wav],0)
    
    频谱图 = tf.signal.stft(wav,frame_length=100,frame_step=20)
    频谱图 = tf.abs(频谱图)
    频谱图= tf.expand_dims（频谱图，轴= 2）
    返回频谱图、标签


def get_CNN(input_shape):
    模型=顺序（）
    model.add(Conv2D(16, (3,3), 激活=&#39;relu&#39;, input_shape=input_shape))
    model.add(Conv2D(16, (3,3), 激活=&#39;relu&#39;))
    model.add(MaxPool2D((2,2)))
    模型.add(压平())
    model.add（密集（128，激活=&#39;relu&#39;））
    model.add（密集（1，激活=&#39;softmax&#39;））
    
    model.compile(&#39;Adam&#39;, loss=&#39;BinaryCrossentropy&#39;, 指标=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision(),&#39;accuracy&#39;])
    model.summary() # 删除一些最大池层以减少参数
    返回模型
    

def main():
    POS_COUGH = “./data/咳嗽”
    NEG_COUGH =“./data/not_cough”
  
    #POS_SPEECH =“./数据/语音”

    pos_cough = tf.data.Dataset.list_files(POS_COUGH+&#39;\*.wav&#39;)
    neg_cough = tf.data.Dataset.list_files(NEG_COUGH+&#39;\*.wav&#39;)
    
    #pos_speech = tf.data.Dataset.list_files(POS_SPEECH +&#39;\*.wav&#39;)

    咳嗽标签 = tf.data.Dataset.from_tensor_slices(tf.ones(len(pos_cough)))
    
    not_cough_labels = tf.data.Dataset.from_tensor_slices(tf.ones(len(neg_cough)))
    
    # 添加标签并合并正负样本
    咳嗽 = tf.data.Dataset.zip((pos_cough, 咳嗽_标签))
    
    not_cough = tf.data.Dataset.zip((neg_cough, not_cough_labels))
   
    阴性= not_cough
    阳性=咳嗽
    # 连接两个相同的元素
    数据=正数.连接（负数）

    ### 2. 创建 Tensorflow 数据管道
    数据 = data.map(预处理)
    数据 = data.cache()
    数据 = data.shuffle(buffer_size=1000)
    数据 = 数据.batch(16)
    数据 = 数据.预取(8)
    
    ## 3. 将数据拆分为训练数据和测试数据
    火车 = data.take(int(len(数据) * 0.7))
    test = data.skip(int(len(data) * 0.7)).take(int(len(data) - len(data) * 0.7)) #test.as_numpy_iterator().next()

    输入形状频谱图 = (396, 65,1)
    模型 = get_CNN(input_shape_spectrogram)
    hist = model.fit(train, epochs=2,validation_data=test)
]]></description>
      <guid>https://stackoverflow.com/questions/78243492/how-to-classify-multiple-classes-in-tensorflow</guid>
      <pubDate>Fri, 29 Mar 2024 11:24:58 GMT</pubDate>
    </item>
    <item>
      <title>训练和测试分开，目标类别的每个名称和比例都出现在训练和测试中</title>
      <link>https://stackoverflow.com/questions/78242480/train-and-test-split-in-such-a-way-that-each-name-and-proportion-of-tartget-clas</link>
      <description><![CDATA[我正在尝试解决一个机器学习问题，如果一个人是否会交付订单。高度不平衡的数据集。这是我的数据集的一瞥
[{&#39;order_id&#39;: &#39;1bjhtj&#39;, &#39;送货员&#39;: &#39;约翰&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1aec&#39;, &#39;送货员&#39;: &#39;约翰&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1cgfd&#39;, &#39;送货员&#39;: &#39;约翰&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bceg&#39;, &#39;送货员&#39;: &#39;汤姆&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a2fg&#39;, &#39;送货员&#39;: &#39;汤姆&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1cbsf&#39;, &#39;送货员&#39;: &#39;汤姆&#39;, &#39;目标&#39;: 1},
 {&#39;order_id&#39;: &#39;1bc5&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a22&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bzc5&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1av22&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bsc5&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 1},
 {&#39;order_id&#39;: &#39;1a2t2&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bc5b&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a22a&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1c5bv&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;vb2er&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bs5s&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a22n&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;122a&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 1},
 {&#39;order_id&#39;: &#39;1cw5bv&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;vb=er&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1b5s&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a2n&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 1}]



这是我的桌子：
&lt;前&gt;&lt;代码&gt;|订单 ID |送货员 |目标|
|----------|--------------|--------|
| 1bjhtj |约翰 | 0 |
| 1aec |约翰 | 0 |
| 1cgfd |约翰 | 0 |
| 1bceg |汤姆| 0 |
| 1a2fg | 1a2fg汤姆| 0 |
| 1cbsf |汤姆| 1 |
| 1BC5 | 1BC5 |周杰伦 | 0 |
| 1a22 | 1a22周杰伦 | 0 |
| 1bzc5 | 1bzc5周杰伦 | 0 |
| 1av22 | 1av22周杰伦 | 0 |
| 1BSC5 |周杰伦 | 1 |
| 1a2t2 | 1a2t2 |周杰伦 | 0 |
| 1bc5b | 1bc5b |周杰伦 | 0 |
| 1a22a | 1a22a玛丽| 0 |
| 1c5bv | 1c5bv玛丽| 0 |
| VB2er |玛丽| 0 |
| 1bs5s | 1bs5s |玛丽| 0 |
| 1a22n | 1a22n |玛丽| 0 |
| 122a | 122a詹姆斯 | 1 |
| 1cw5bv | 1cw5bv詹姆斯 | 0 |
| vb=呃|詹姆斯 | 0 |
| 1b5s | 1b5s |詹姆斯 | 0 |
| 1a2n |詹姆斯 | 1 |


我希望我的机器学习模型能够理解每个人的属性并预测这两个
案例：
将传送“0”和
不会传送“1”
我想以这样的方式分割我的训练和测试，使其保留几行名称和几行目标类，以便它学习所有模式。
到目前为止我已经用过这个
X = df.drop(columns = “目标”)
y = df.目标
X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.7,stratify=y)

它确实给了我每个送货员的输出，但它错过了我们可以以“1”和“1”的方式分割“James”的部分。火车上将会有另一个“1”将在测试中。
谁能帮助我以不同的方式解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/78242480/train-and-test-split-in-such-a-way-that-each-name-and-proportion-of-tartget-clas</guid>
      <pubDate>Fri, 29 Mar 2024 07:21:32 GMT</pubDate>
    </item>
    <item>
      <title>Meta在视频流平台上的无缝通信模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78240811/metas-seamless-communication-model-on-video-streaming-platform</link>
      <description><![CDATA[我想知道如何在youtube或其他平台等视频流媒体平台上使用meta的无缝通信模型？有谁知道怎么做吗？
目前我可以使用存储的音频/视频在本地计算机上运行它。我不知道如何在流媒体平台上运行它。]]></description>
      <guid>https://stackoverflow.com/questions/78240811/metas-seamless-communication-model-on-video-streaming-platform</guid>
      <pubDate>Thu, 28 Mar 2024 20:51:05 GMT</pubDate>
    </item>
    <item>
      <title>“MENACE”井字棋电脑需要多少场比赛才能训练</title>
      <link>https://stackoverflow.com/questions/78219696/how-many-games-will-a-menace-tic-tac-toe-computer-take-to-train</link>
      <description><![CDATA[我最近读到了唐纳德·米奇 (Donald Michie) 设计的用火柴盒建造的“计算机”，它可以自学如何玩井字游戏。这是关于它的维基百科文章：
https://en.m.wikipedia.org/wiki/Matchbox_Educable_Noughts_and_Crosses_Engine 
我觉得它看起来很有趣，所以我决定用 Python 制作一个数字版本，以供娱乐和练习。它在对抗随机走棋时效果很好（我刚刚根据约 10,000 场比赛生成的数据再次运行了 5353 场比赛，它赢得了 5353 场比赛中的 4757 场），但它仍然经常输给我。
以下是完美答案应解决的一些问题：

需要玩多少场游戏才能让“火柴盒电脑”与 Michie 设计的电脑完全一样，才能完美地开始玩游戏？

带有实际火柴盒的原始计算机是否达到了完美状态
玩吗？

如果仅与计算机进行训练，计算机能否达到完美的发挥
随机移动？


编辑：
这个问题并不是寻求代码方面的帮助，但下面的评论表明包含代码可能会有所帮助。以下是我创建的 GitHub 存储库的链接，以便我可以在此处共享：
https://github.com/ACertainArchangel/ Recreation-Of-MENACE-Tic-Tac-Toe..git
抱歉，我知道这不太好并且不遵守约定；我只写了几个月的代码:)]]></description>
      <guid>https://stackoverflow.com/questions/78219696/how-many-games-will-a-menace-tic-tac-toe-computer-take-to-train</guid>
      <pubDate>Mon, 25 Mar 2024 14:12:37 GMT</pubDate>
    </item>
    <item>
      <title>TF2 和 python 中的 BERT 预处理器模型存在问题</title>
      <link>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</link>
      <description><![CDATA[我正在尝试使用 BERT 来做一个文本分类项目。但是我一直遇到这个错误
`
ValueError Traceback（最近一次调用最后一次）
单元格 In[37]，第 4 行
      2 text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
      3 bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
----&gt; 4 preprocessed_text = bert_preprocess(text_input)
      5 bert_encoder = hub.KerasLayer(encoder_url,
      6 可训练=真，
      7 名称=&#39;BERT_编码器&#39;)
      8 个输出 = bert_encoder(preprocessed_text)
ValueError：调用层“预处理”时遇到异常（类型 KerasLayer）。
KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。

调用层“预处理”接收的参数（类型 KerasLayer）：
  输入=
  • 培训=无

KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。



构建此模型时：
&lt;前&gt;&lt;代码&gt;
preprocess_url = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3&#39;
编码器网址 = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/bert-en-uncased-l-12-h-768-a-12/versions/2&#39;

# Bert 层
text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
预处理文本 = bert_preprocess(text_input)
bert_encoder = hub.KerasLayer(encoder_url,
                              可训练=真，
                              名称=&#39;BERT_编码器&#39;)
输出= bert_encoder（预处理文本）

# 神经网络层
l = tf.keras.layers.Dropout(0.1)(输出[&#39;pooled_output&#39;])
l = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;, name=&#39;输出&#39;)(l)

# 构建最终模型
模型 = tf.keras.Model(输入=[text_input], 输出=[l])

我看过无数的教程，甚至使用了张量流文档上的教程，即使我复制和粘贴，它们仍然不起作用。我尝试过不同版本的 tf、tf-text 和 tf-hub。我在这个项目中使用了tensorflow-gpu-jupyter docker 容器。
这是我安装库的方法：
!pip install “tensorflow-text”
!pip install “tf-models-official”
!pip install “tensorflow-hub”

版本是：
张量流：2.16.1
张量流文本：2.16.1
张量流中心：0.16.1
我看到的有关此问题的所有其他论坛都说要执行 tf.config.run_functions_eagerly(True) 但这不起作用。
任何事情都会有所帮助。如果您知道如何解决请回答。]]></description>
      <guid>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</guid>
      <pubDate>Tue, 19 Mar 2024 01:42:01 GMT</pubDate>
    </item>
    <item>
      <title>如何在 python 3.12.1 上安装 PyTorch</title>
      <link>https://stackoverflow.com/questions/77792551/how-to-install-pytorch-on-python-3-12-1</link>
      <description><![CDATA[我正在安装DARTS TimeSeries 库 但我遇到了依赖项安装的问题。在 DARTS 安装指南中，它说如果我们遇到这个问题，我们必须参考 PyTorch 的官方安装指南，然后尝试再次安装 Darts。然后，当我尝试在 python 3.12.1 上安装 torch 时，遇到此错误：
&lt;块引用&gt;
错误：找不到满足火炬要求的版本（来自版本：无）
错误：找不到火炬的匹配发行版。

如何解决？
我使用 PyCharm 作为 Python 代码编辑器。
我尝试了pip install darts，但它没有安装所有软件包并遇到此错误错误：subprocess-exited-with-error
 用于安装构建依赖项的 pip 子进程未成功运行。
  │ 退出代码：1
  ╰─&gt; 【136行输出】
      正在收集setuptools&gt;=64.0
        从 https://files.pythonhosted.org/packages 获取 setuptools&gt;=64.0 的依赖信息

然后，我尝试使用 pip install torch 安装 torch 并遇到此错误：
&lt;块引用&gt;
错误：找不到满足火炬要求的版本（来自版本：无）
错误：找不到火炬的匹配发行版
]]></description>
      <guid>https://stackoverflow.com/questions/77792551/how-to-install-pytorch-on-python-3-12-1</guid>
      <pubDate>Wed, 10 Jan 2024 10:16:06 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch安装</title>
      <link>https://stackoverflow.com/questions/77478747/pytorch-installation</link>
      <description><![CDATA[我正在尝试在 python 3.12.0 中安装 Pytorch 并在 Windows 11 中安装 cuda 12.1 ？但我收到错误
错误：找不到满足火炬要求的版本（来自版本：无）
错误：找不到火炬的匹配发行版
我也安装了 nvidia cuda 12.1
我尝试使用 Pytorch 网站安装 Pytorch，但它不起作用并且给我错误
错误：找不到满足火炬要求的版本（来自版本：无）
错误：找不到火炬的匹配发行版]]></description>
      <guid>https://stackoverflow.com/questions/77478747/pytorch-installation</guid>
      <pubDate>Tue, 14 Nov 2023 07:14:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 (V1) python SDK API 训练 Watson Discovery 不起作用</title>
      <link>https://stackoverflow.com/questions/59996251/training-watson-discovery-with-v1-python-sdk-apis-does-not-work</link>
      <description><![CDATA[我想使用 Watson discovery V1 API 进行相关性训练。我尝试了以下方法，但尚未得到想要的结果。下面详细描述问题：
我有一组文档，其中一些包含“云”或“大数据”一词。我想使用 query() api 搜索单词 &#39;hadoop&#39; 并获取这些文档，但发现查询不返回任何内容。
现在，我想提供以下训练示例来发现以更新相关性分数，以便我得到这些结果（我使用查询扩展来完成相同的任务并且它有效，现在我是对相关性培训感兴趣）。
我使用了 API add_training_data() 将查询 &#39;hadoop&#39; 与相关文档（由 ids 指定，包含 &#39;cloud &#39;，例如）。
现在训练数据如下所示：

&lt;前&gt;&lt;代码&gt;{
  &quot;natural_language_query&quot;: &quot;hadoop&quot;,
  “筛选”： ””，
  “例子”： [
    {
      “document_id”：“1ad6f551-e092-4ce9-b08c-eb4f4cbc9458”，
      “交叉引用”：“”，
      “相关性”：1，
      “创建”：“2020-01-30T23：16：19.674Z”，
      “更新”：“2020-01-30T23：16：19.716Z”
    },
    {
      “document_id”：“f1d11f51-31b2-414f-b359-d5336b019575”，
      “交叉引用”：“”，
      “相关性”：1，
      “创建”：“2020-01-30T23：16：19.674Z”，
      “更新”：“2020-01-30T23：16：19.722Z”
    },
    {
      “document_id”：“5bfcea6a-c925-4db5-a490-89a9d1de8d4c”，
      “交叉引用”：“”，
      “相关性”：1，
      “创建”：“2020-01-30T23：16：19.674Z”，
      “更新”：“2020-01-30T23：16：19.729Z”
    },
    {
      “document_id”：“bf07e701-6893-428c-ab16-c5446e821291”，
      “交叉引用”：“”，
      “相关性”：1，
      “创建”：“2020-01-30T23：16：19.674Z”，
      “更新”：“2020-01-30T23：16：19.735Z”
    },
    {
      “document_id”：“75082812-5c96-4d2e-b388-821a0434ad4c”，
      “交叉引用”：“”，
      “相关性”：1，
      “创建”：“2020-01-30T23：16：19.674Z”，
      “更新”：“2020-01-30T23：16：19.742Z”
    }
  ],
  “query_id”：“cc1d3677eeafe70929aeccfb462860439f61b051”，
  “创建”：“2020-01-30T23：16：19.677Z”，
  “更新”：“2020-01-30T23：16：19.677Z”
}

其中文档 ID 对应于集合中包含单词 &#39;cloud&#39; 的文档。例如
创建训练数据后，现在我想使用查询文本 &#39;hadoop&#39; 再次运行之前的查询，假设发现会自动训练自身以获取相关结果（因为我找不到任何我期待的像 &#39;train()&#39; 这样的 api）。但是，即使在提供了训练示例之后，发现查询仍然没有返回任何内容。 
我不知道出了什么问题。一些帮助将非常感激。]]></description>
      <guid>https://stackoverflow.com/questions/59996251/training-watson-discovery-with-v1-python-sdk-apis-does-not-work</guid>
      <pubDate>Thu, 30 Jan 2020 23:59:44 GMT</pubDate>
    </item>
    </channel>
</rss>