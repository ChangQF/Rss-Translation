<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 17 Aug 2024 18:19:44 GMT</lastBuildDate>
    <item>
      <title>训练时间随机变化，没有明显原因[关闭]</title>
      <link>https://stackoverflow.com/questions/78882116/training-time-randomly-varies-without-apparent-reason</link>
      <description><![CDATA[我正在 Nvidia RTX4090 GPU 中训练 Keras Tensorflow ResNet50 模型。我使用 Python 3.10、TF 2.10（我使用的是 Windows）、Keras 2.10、CUDA 12.5、CuDNN 8.9 和 PyCharm 作为解释器。该模型通常每轮大约需要 2 分钟。但是，我观察到有时在不更改任何超参数并使用完全相同的输入的情况下，一个轮次可能需要长达一个小时。此外，这种情况有时会在同一次运行中发生：第一个轮次需要 15 分钟，但其他轮次需要 2 分钟，或者模型以正常速度运行三​​个轮次，而第四个轮次需要半个多小时。在代码的开头，我使用了 tf.config.experimental.set_memory_growth(device, True) 和 tf.keras.backend.clear_session()，并且我检查了 GPU 使用情况，两种情况下都一样。我是机器学习和 Keras 的新手，有什么我遗漏的吗？
我希望使用相同输入的相同代码的训练时间相同]]></description>
      <guid>https://stackoverflow.com/questions/78882116/training-time-randomly-varies-without-apparent-reason</guid>
      <pubDate>Sat, 17 Aug 2024 11:40:40 GMT</pubDate>
    </item>
    <item>
      <title>Azure AI 自定义图像分析模型未返回性能指标</title>
      <link>https://stackoverflow.com/questions/78881768/azure-ai-custom-image-analysis-model-not-returning-performance-metrics</link>
      <description><![CDATA[成功训练并使用自定义图像分析模型后，我没有获得任何性能指标。在 Vision Studio 的 UI 和执行代码这两个地方，我都收到以下错误：
状态：错误（code={&#39;code&#39;: &#39;InternalServerError&#39;, &#39;message&#39;: &#39;批处理未成功完成。OutputResultWriteFailed：遇到意外错误。&#39;}, message=&#39;&#39;, target=&#39;&#39;, details=[])

您有什么建议，如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78881768/azure-ai-custom-image-analysis-model-not-returning-performance-metrics</guid>
      <pubDate>Sat, 17 Aug 2024 08:44:21 GMT</pubDate>
    </item>
    <item>
      <title>如何加速随机森林回归和SVR的训练？</title>
      <link>https://stackoverflow.com/questions/78881480/how-to-speed-up-training-of-random-forest-regression-and-svr</link>
      <description><![CDATA[我正在尝试使用以下数据集创建一个回归模型来预测比特币的收盘价：https://www.kaggle.com/datasets/prasoonkottarathil/btcinusd/data?select=BTC-2021min.csv
它有超过 60 万条记录，包含 15 个特征（其中一些是我创建的）。
我曾多次尝试在 google colab 和我的笔记本电脑上对其进行训练。我甚至把它放了一夜，但它花了太长时间。
有什么方法可以加快速度吗？
笔记本电脑规格：
CPU：Ryzen 7 5800H 
GPU：RTX 3050 
RAM：16 GB

这是训练代码：
models = {
&#39;线性回归&#39;：{
&#39;model&#39;：LinearRegression()，
&#39;params&#39;：{}
},
&#39;Ridge 回归&#39;：{
&#39;model&#39;：Riddom_state=42，
&#39;params&#39;：{&#39;alpha&#39;：[0.01, 0.1, 1, 5, 10, 50, 100]}
},
&#39;Lasso 回归&#39;：{
&#39;model&#39;： Lasso(random_state=42),
&#39;params&#39;: {&#39;alpha&#39;: [0.001, 0.01, 0.1, 1, 10]}
},
&#39;决策树&#39;: {
&#39;模型&#39;: DecisionTreeRegressor(random_state=42),
&#39;params&#39;: {&#39;max_depth&#39;: [None, 5, 10, 20], &#39;min_samples_split&#39;: [2, 5, 10]}
},
&#39;随机森林&#39;: {
&#39;模型&#39;: RandomForestRegressor(random_state=42),
&#39;params&#39;: {&#39;n_estimators&#39;: [50, 100, 200], &#39;max_depth&#39;: [None, 5, 10], &#39;min_samples_split&#39;: [2, 5, 10]}
},
&#39;支持向量回归&#39;: {
&#39;model&#39;: SVR(),
&#39;params&#39;: {&#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;], &#39;C&#39;: [0.1, 1, 10], &#39;epsilon&#39;: [0.01, 0.1, 1]}
}
}

results = {}

for model_name, model_data in models.items():
print(f&quot;Tuning {model_name}&quot;)
grid_search = GridSearchCV(model_data[&#39;model&#39;], model_data[&#39;params&#39;], cv=5,scoring=&#39;neg_mean_squared_error&#39;, verbose=1)
grid_search.fit(X_train, y_train)

# 获取最佳模型
best_model = grid_search.best_estimator_

# 预测
y_pred = best_model.predict(X_test)

# 性能指标
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

results[model_name] = {
&#39;MAE&#39;: mae,
&#39;MSE&#39;: mse,
&#39;RMSE&#39;: rmse,
&#39;R2&#39;: r2,
&#39;最佳模型&#39;: best_model,
&#39;最佳参数&#39;: grid_search.best_params_
}
]]></description>
      <guid>https://stackoverflow.com/questions/78881480/how-to-speed-up-training-of-random-forest-regression-and-svr</guid>
      <pubDate>Sat, 17 Aug 2024 05:30:15 GMT</pubDate>
    </item>
    <item>
      <title>参数逻辑模型的 AUC 如何能比 BART 模型更好？</title>
      <link>https://stackoverflow.com/questions/78881371/how-can-a-parametric-logit-model-have-a-better-auc-than-its-bart-equivalent</link>
      <description><![CDATA[我正在研究一个由大约 20k 个观察值和 620 个回归量组成的表格数据集，其中 25 个回归量是密集的，其余的则非常稀疏。大约 5-10 个回归量是连续变量，而其余的则是二分变量。同样，我的目标变量也是二分变量。我将通过标准 maxLH 方法估计的标准参数 logit 模型与带有 logit 链接器的简单贝叶斯加性回归树 (BART) 模型（包 BART - 函数 lbart）的结果进行了比较
从 Logit 回归中，我发现大多数回归量都不显著。即使是那些理论预测显著的回归量。这是内生性问题和可能的共线性（如果我没记错的话）的预期结果，这困扰着我的观察。有趣的是，参数 Logit 的 AUC 约为 0.7，这对于社会科学估计问题来说相对较高。
另一方面，BART 模型在理论认为重要的变量的平均处理效果上提供了预期的强可信区间。然而，AUC 低于参数 Logit！大约 0.68。
我想知道，我该如何调和这些相互矛盾的结果？具有明显病理规范的 Logit 模型的预测能力大大优于 BART 模型，后者在重要变量的可信区间上显示出相当令人信服的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78881371/how-can-a-parametric-logit-model-have-a-better-auc-than-its-bart-equivalent</guid>
      <pubDate>Sat, 17 Aug 2024 04:09:13 GMT</pubDate>
    </item>
    <item>
      <title>MatplotLib 未显示图像</title>
      <link>https://stackoverflow.com/questions/78881216/matplotlib-is-not-showing-the-image</link>
      <description><![CDATA[MatplotLib 不显示图像的原因是什么？
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
plt.figure(figsize=(10, 10))
plt.imshow(image_rgb)
plt.show()

它不显示任何图像。
更新：
以下是更新后的代码：
import matplotlib.pyplot as plt

image_path = &#39;/Users/johndoe/Desktop/machine-learning/traffic-light.png&#39;
image = cv2.imread(image_path)

plt.imshow(image)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78881216/matplotlib-is-not-showing-the-image</guid>
      <pubDate>Sat, 17 Aug 2024 01:24:40 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试训练 TensorFlow 模型时，出现“ValueError”</title>
      <link>https://stackoverflow.com/questions/78881211/i-get-a-valueerror-when-i-try-to-train-my-tensorflow-model</link>
      <description><![CDATA[这是我在第一个 epoch 调用 model.fit 时遇到的错误：
发生异常：ValueError
层“ functional”需要 2 个输入，但它收到了 1 个输入张量。收到的输入：[&lt;tf.Tensor &#39;data:0&#39; shape=(None, 128) dtype=float32&gt;]
文件 &quot;D:\workspace\Machine Learning 545\PSU_classes\cs445_group_project\code\Keras Music Genres Classification\encoder_decoder_feature_extractor.py&quot;，第 177 行，位于 train_encoder_decoder_model
model.fit(x = X_train,
文件 &quot;D:\workspace\Machine Learning 545\PSU_classes\cs445_group_project\code\Keras Music Genres Classification\encoder_decoder_feature_extractor.py&quot;，第 217 行，位于 &lt;module&gt;
train_encoder_decoder_model = train_encoder_decoder_model(encoder_decoder_model, X_train, y_train, X_test, y_test)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: 层“ functional”需要 2 个输入，但收到 1 个输入张量。收到的输入：[&lt;tf.Tensor &#39;data:0&#39; shape=(None, 128) dtype=float32&gt;]&quot;

这是我的模型：
def define_encoder_decoder_model(num_features):
# 定义编码器
coder_inputs = 输入（shape=(None, num_features))
coder_hidden1 = Dense(100, 激活=&#39;relu&#39;)(encoder_inputs)
coder_hidden2 = Dense(50, 激活=&#39;relu&#39;)(encoder_hidden1)
coder_lstm = LSTM(25, return_state=True)
coder_outputs, state_h, state_c =coder_lstm(encoder_hidden2)
coder_states = [state_h, state_c]

# 定义解码器
decoder_inputs = 输入(shape=(None, 25))
decoder_hidden1 = Dense(50, 激活=&#39;relu&#39;)(decoder_inputs)
decoder_hidden2 = Dense(100, 激活=&#39;relu&#39;)(decoder_hidden1)
decoder_lstm = LSTM(num_features, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decrypt_lstm(decoder_hidden2, initial_state=encoder_states)
decoder_dense = Dense(num_features,activation=&#39;softmax&#39;)
decoder_outputs =coder_dense(decoder_outputs)

# 定义将encoder_inputs和decoder_inputs转换为decoder_outputs的模型
model = Model([encoder_inputs,decoder_inputs],decoder_outputs)

# 编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;,&#39;precision&#39;,&#39;recall&#39;,&#39;f1_score&#39;])

# 模型摘要
model.summary()

在此处返回modeltype

这是我调用model.fit的方式：
def train_encoder_decoder_model(model,X_train, y_train, X_test, y_test):
&quot;&quot;&quot;
使用提供的数据训练编码器-解码器模型。

参数：
model：要训练的编码器-解码器模型。
X_train：输入训练数据。
y_train：目标训练数据。
X_test：输入测试数据。
y_test：目标测试数据。

返回：
训练好的编码器-解码器模型。
&quot;&quot;&quot;
print(&quot;shapes: X_train:&quot;, np.shape(X_train),&quot; y_train: &quot;, np.shape(y_train),&quot; X_test: &quot;, np.shape(X_test),&quot; y_test: &quot;, np.shape(y_test))
# 训练模型
# 将每个时期的训练日志附加到文件中
file_logger = FileLogger(&#39;training.log&#39;)
y_train_T = tf.convert_to_tensor(np.array([y_train]).T)
y_test_T = tf.convert_to_tensor(np.array([y_test]).T)
#x_train = tf.convert_to_tensor(X_train)
y_train = X_train
y_test = X_test
model.fit(x = X_train,
y= y_train,
batch_size=100,
epochs=100,
verbose=2,
validation_data=(X_test, y_test),
callbacks=[file_logger])
返回模型

这是一个编码器-解码器模型，因此 X_train 数据集等于 y_train，X_test、y_test 也一样。在第一种情况下，训练集的形状为 (799,128)，测试数据集为 (299,128)。特征表示为“float64”值。
我在 Visual Studio Code 下运行代码。我将数据预处理为标准化和缩放的数据集，然后将其分为训练数据集和测试数据集，构建我的编码器-解码器模型（参见上面的方法），然后尝试训练模型。我得到的是 model.fit 的这个输出“Epoch 1/100”和上面显示的错误消息。
这个错误是什么以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78881211/i-get-a-valueerror-when-i-try-to-train-my-tensorflow-model</guid>
      <pubDate>Sat, 17 Aug 2024 01:20:22 GMT</pubDate>
    </item>
    <item>
      <title>什么时候需要将数据居中？[关闭]</title>
      <link>https://stackoverflow.com/questions/78880910/when-is-it-necessary-to-center-the-data</link>
      <description><![CDATA[我正在尝试找出不同的缩放方法，我有以下问题。StandardScaler、RobustScaler 和类似的缩放器将数据相对于平均值或中位数居中，但 MinMaxScaler 不会这样做，它只会将数据传输到 0 到 1 的范围。什么时候应该将数据居中，什么时候不需要？在哪些情况下可以获得最佳准确度？
我还读到，当数据已经呈正态分布时，最好使用 StandardScaler，但如果只有一部分具有此属性怎么办？我应该对一半数据使用 StadardScaler，对另一半数据使用 MinMaxScaler（听起来不合逻辑）还是优先使用其中一种方法？]]></description>
      <guid>https://stackoverflow.com/questions/78880910/when-is-it-necessary-to-center-the-data</guid>
      <pubDate>Fri, 16 Aug 2024 21:39:50 GMT</pubDate>
    </item>
    <item>
      <title>如何配置 Co-DETR 来预测四边形而不是边界框？</title>
      <link>https://stackoverflow.com/questions/78880373/how-do-i-configure-co-detr-to-predict-quadrilaterals-instead-of-bounding-boxes</link>
      <description><![CDATA[我已经使用 MMDetection 框架在 COCO 数据集上训练了一个 Co-DETR 模型，该模型用于手写人口普查记录，其中注释是四边形，由于透视偏移或旋转，它们接近矩形但不完全是矩形。
该模型训练得很好，但当我运行推理时，模型输出的是边界框而不是四边形。当人口普查记录旋转或透视偏移时，这会出现问题，如绿色列所示：

从这里可以看出，列的左上角和绿色边界框匹配，右下角和边界框非常接近，但列的其他角与预测边界框的角偏离，因为这里的图像略有旋转。
这个框架是否有任何设置，使模型可以输出四边形预测（基本上每个类有 4 个关键点），这些预测与它所训练的注释相匹配，而不是边界盒子？]]></description>
      <guid>https://stackoverflow.com/questions/78880373/how-do-i-configure-co-detr-to-predict-quadrilaterals-instead-of-bounding-boxes</guid>
      <pubDate>Fri, 16 Aug 2024 18:17:01 GMT</pubDate>
    </item>
    <item>
      <title>如何打包使用SAM语义分割模型的Python项目？</title>
      <link>https://stackoverflow.com/questions/78880063/how-can-i-package-a-python-project-that-uses-the-sam-semantic-segmentation-model</link>
      <description><![CDATA[我有一个 Python 项目，我使用 Segment-Anything 模型的结果来确定通道内的气泡位置。现在，我需要打包应用程序，然后将其交给我的同事，但我不想在他们所有的电脑上安装和调试 pytorch 和 conda。
我目前正在尝试使用 Briefcase 和 PyOxidizer，一旦我尝试它们，我将尝试更新它。
我确实想支持 mac，因为他们主要使用 mac，但我可以强制他们使用 windows。
有什么技巧或窍门可以做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/78880063/how-can-i-package-a-python-project-that-uses-the-sam-semantic-segmentation-model</guid>
      <pubDate>Fri, 16 Aug 2024 16:33:51 GMT</pubDate>
    </item>
    <item>
      <title>实现 nn.Bilinear 层</title>
      <link>https://stackoverflow.com/questions/78870012/implementing-nn-bilinear-layer</link>
      <description><![CDATA[有人能帮我理解 nn.Bilinear 的实现吗？

根据文档，此函数实现 y = x1T * A * x2

取 x1 = (100,20) ， x2 = (100,30&#39;) ，假设 output_features = 50。矩阵 A 的维度为 [50,20,30]。
我发现很难将这些矩阵相乘以获得 output = [100,50]
根据 x1、x2 和 A 矩阵的大小，根据 y = x1T * A * x2 ，乘法似乎不兼容。我在这里遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78870012/implementing-nn-bilinear-layer</guid>
      <pubDate>Wed, 14 Aug 2024 09:01:18 GMT</pubDate>
    </item>
    <item>
      <title>加载权重时出现意外键错误</title>
      <link>https://stackoverflow.com/questions/78863529/getting-unexpected-keys-error-while-loading-weights</link>
      <description><![CDATA[import torch
from PIL import Image
import numpy as np
from effdet import get_efficientdet_config, EfficientDet

config = get_efficientdet_config(&#39;tf_efficientdet_d0&#39;)
model = EfficientDet(config, pretrained_backbone=True)
model.eval()

当我运行此程序时，我收到错误
加载预训练权重时发现意外键（bn2.bias、bn2.num_batches_tracked、bn2.running_mean、bn2.running_var、bn2.weight、classifier.bias、classifier.weight、conv_head.weight）。如果模型正在调整，则可能会出现这种情况。

我研究了一下，发现这是由于 timm builder 造成的，但没有找到任何解决方案。如何解决这个问题？
我想加载 efficientdet 权重，但结果出现了意外的键错误]]></description>
      <guid>https://stackoverflow.com/questions/78863529/getting-unexpected-keys-error-while-loading-weights</guid>
      <pubDate>Mon, 12 Aug 2024 20:42:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么我不能用 C++ 构建一个没有依赖关系的神经网络，即使它可以在 Numpy 中运行？</title>
      <link>https://stackoverflow.com/questions/78862784/why-cant-i-build-a-neural-network-in-c-with-no-dependencies-even-though-it-w</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78862784/why-cant-i-build-a-neural-network-in-c-with-no-dependencies-even-though-it-w</guid>
      <pubDate>Mon, 12 Aug 2024 16:55:51 GMT</pubDate>
    </item>
    <item>
      <title>以矩阵为输入、以矩阵中位置为输出的神经网络 - 强化</title>
      <link>https://stackoverflow.com/questions/78862449/neural-network-for-matrix-as-input-and-position-in-matrix-as-output-reinforcem</link>
      <description><![CDATA[我在体育馆中设置了一个非常基本的环境，由一个 nxn 矩阵组成，由 0 和 1 填充。神经网络现在应该输出一个向量，指向矩阵的一个特定条目。
现在，这应该只是一个带有 0 的条目，然后将其更改为 1：本质上，人工智能通过强化学习用 1 填充矩阵。稍后，我希望它找到合适的位置将形状放入矩阵中（有点像俄罗斯方块游戏，只是方块不会掉落）。
无论如何，到目前为止，我一直使用 DQN 模型，但这似乎不太适合这里。有人能告诉我这种输入/输出设置的更好方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78862449/neural-network-for-matrix-as-input-and-position-in-matrix-as-output-reinforcem</guid>
      <pubDate>Mon, 12 Aug 2024 15:23:25 GMT</pubDate>
    </item>
    <item>
      <title>使用相关的 Sagemaker HP 调整作业作为热启动父作业</title>
      <link>https://stackoverflow.com/questions/78861542/use-relevant-sagemaker-hp-tuning-jobs-as-warm-start-parent-jobs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78861542/use-relevant-sagemaker-hp-tuning-jobs-as-warm-start-parent-jobs</guid>
      <pubDate>Mon, 12 Aug 2024 11:58:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么剪枝后参数数量没有减少？</title>
      <link>https://stackoverflow.com/questions/78857798/why-are-the-number-of-parameters-not-decreasing-after-pruning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78857798/why-are-the-number-of-parameters-not-decreasing-after-pruning</guid>
      <pubDate>Sun, 11 Aug 2024 08:10:20 GMT</pubDate>
    </item>
    </channel>
</rss>