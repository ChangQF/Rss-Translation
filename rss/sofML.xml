<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 07 Jan 2025 01:16:36 GMT</lastBuildDate>
    <item>
      <title>请帮我找到 Xception、NASnet 和 DenseNet 迁移学习模型架构的绘制 IO 原始文件 [关闭]</title>
      <link>https://stackoverflow.com/questions/79334365/please-help-me-find-a-draw-io-raw-file-for-xception-nasnet-and-densenet-transf</link>
      <description><![CDATA[我正在开展一个项目，需要一张信息图来展示上述模型架构的不同部分，但我很难理解如何开始在 Draw IO 中绘图（尤其是对于较大的模型）。
有人可以为我提供一个包含上述迁移学习模型架构基本图的原始文件吗？]]></description>
      <guid>https://stackoverflow.com/questions/79334365/please-help-me-find-a-draw-io-raw-file-for-xception-nasnet-and-densenet-transf</guid>
      <pubDate>Mon, 06 Jan 2025 21:18:30 GMT</pubDate>
    </item>
    <item>
      <title>如何改进我的线性回归模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79333949/how-to-improve-my-linear-regression-model</link>
      <description><![CDATA[今天终于完成了线性回归的 Scratch 实现
filepath = f&quot;{path}/Food_Delivery_Times.csv&quot;
df = pd.read_csv(filepath)
df.head()

print(df.columns)
df.isnull().sum()

df.dropna(inplace = True , thresh=4)
df.drop_duplicates(inplace= True)
df[&#39;Courier_Experience_yrs&#39;] = df[&#39;Courier_Experience_yrs&#39;].interpolate()
columns_to_fill = [&#39;Weather&#39;, &#39;Traffic_Level&#39;, &#39;Time_of_Day&#39;]
for col in columns_to_fill:
mode_value = df[col].mode()[0]
df[col]= df[col].fillna(mode_value)
df.info()

print(df[&#39;Weather&#39;].unique())
print(df[&#39;Traffic_Level&#39;].unique())
print(df[&#39;Time_of_Day&#39;].unique())
print(df[&#39;Vehicle_Type&#39;].unique())

def onehot(df,column):
values = df[column].unique()
for val in values:
df[val] = (df[column] == val).astype(int)
return df

df = onehot(df,&#39;Weather&#39;)
df = onehot(df,&#39;Vehicle_Type&#39;)
df = onehot(df,&#39;Time_of_Day&#39;)
traffic_mapping = {&#39;低&#39;:0,&#39;中&#39;:1,&#39;高&#39;:2}
df[&#39;Traffic_encoded&#39;] = df[&#39;Traffic_Level&#39;].map(traffic_mapping)
df

categorical_col = [&#39;天气&#39;, &#39;Traffic_Level&#39;, &#39;时间&#39;,&#39;车辆类型&#39;]
col_new_df = [col for col in df.columns if col not in categorical_col ]
col_new_df

new_df = df[col_new_df]
new_df.info()

# 特征缩放
for col in new_df.columns:
if col not in [&#39;订单ID&#39;,&#39;交货时间分钟&#39;]:
new_df[col] = new_df[col].astype(&#39;float64&#39;)
mean = new_df[col].mean()
std = new_df[col].std()
new_df.loc[:,col] = (new_df[col]-mean)/std #转换为浮点数，因为在缩放浮点数值后将其分配给 int 数据类型，这会引发警告

new_df.describe()

train_df = new_df.sample(frac = 0.8,random_state =200)
test_df = new_df.drop(train_df.index)

train_df.reset_index(inplace=True)
test_df.reset_index(inplace=True)

train_df.drop(columns = [&#39;index&#39;],inplace=True)
test_df.drop(columns = [&#39;index&#39;],inplace=True)

columns_needed =列表（new_df.columns）
columns_needed.remove（&#39;订单 ID&#39;）
columns_needed.remove（&#39;送货时间分钟&#39;）
X = train_df[columns_needed].to_numpy()
Y = train_df[&#39;送货时间分钟&#39;].to_numpy()
Y_mean = train_df[&#39;送货时间分钟&#39;].mean()
Y_std = train_df[&#39;送货时间分钟&#39;].std()
Y = (Y-Y_mean)/Y_std

m = len(X)
np.random.seed(42)
W = np.random.randn(len(X[0]))
b = 0
alpha = 0.5
Lambda = 0.5
迭代 = 0
dW = np.zeros(len(X[0]))
当迭代 &lt; 100000：
f = np.dot(X,W) + b
损失 = (np.sum((f - Y)**2) + Lambda*np.sum(W*W))/(2*m)
dW = (np.dot(X.T,(f-Y)) + Lambda*W)/m
db = np.sum(f-Y)/m
W -= alpha*dW
b -= alpha*db
迭代 += 1
如果迭代 % 10000 == 0：
打印（迭代，“，“，“，损失）
如果（损失 &lt; 10**(-3)）：
中断

打印（“完成”）

#测试
sum = 0
sum2 =0
ymean = test_df[&#39;Delivery_Time_min&#39;].mean()
Y_original = test_df[&#39;Delivery_Time_min&#39;].to_numpy()
X_test = test_df[columns_needed].to_numpy()
Y_predicted = np.dot(X_test,W) + b
Y_predicted_ori = Y_predicted*Y_std + Y_mean
print(&quot;原始 ---- 预测 ---- 错误&quot;)
for i in range(len(Y_original)):
print(Y_original[i],&quot;---&quot;,Y_predicted_ori[i],&quot;---&quot;,Y_original[i]-Y_predicted_ori[i])
sum += (Y_original[i]-Y_predicted_ori[i])**2
sum2 += (Y_original[i] - ymean)**2

rscore = 1 - (sum/sum2)
msme = sum/len(Y_original)
print(rscore)
print(msme)

以上是它的实现

数据集 - 食品配送时间预测（Kaggle）
R2_score 为 0.754
MSME 为 133.17

那么它对初学者来说好吗？
可以做些什么来改进它？]]></description>
      <guid>https://stackoverflow.com/questions/79333949/how-to-improve-my-linear-regression-model</guid>
      <pubDate>Mon, 06 Jan 2025 18:12:55 GMT</pubDate>
    </item>
    <item>
      <title>通过 streamlit 执行时出现“短信垃圾邮件分类器”错误“实例未安装”[关闭]</title>
      <link>https://stackoverflow.com/questions/79333200/sms-spam-classifier-error-while-executing-it-via-streamlit-instance-not-fitte</link>
      <description><![CDATA[错误图片
这是我的第一个 ML 项目：“短信垃圾邮件分类器”，当我通过 streamlit 运行它时，我遇到了这个错误。请帮我调试一下
后端代码运行正常，但这是我部署后显示的内容
输出应该是垃圾邮件/非垃圾邮件]]></description>
      <guid>https://stackoverflow.com/questions/79333200/sms-spam-classifier-error-while-executing-it-via-streamlit-instance-not-fitte</guid>
      <pubDate>Mon, 06 Jan 2025 13:34:05 GMT</pubDate>
    </item>
    <item>
      <title>在 Rust 中的 Burn 张量中使用 f64</title>
      <link>https://stackoverflow.com/questions/79333072/using-f64-in-burn-tensor-in-rust</link>
      <description><![CDATA[我是 Rust 新手，我正在研究使用 Burn 移植一些 Python/Torch 代码，用于新的统计参数方法。
第一步：我想生成一个 (10, 1) 张量，其中包含从具有已知参数的柯西分布生成的随机值。Burn 中的分布非常有限，因此我使用 statrs。通过使用 statrs，我可以获得一个 Vec&lt;f64&gt;，然后我可以将其包装到 Burn 中的 TensorData 中，从而生成一个 Tensor。
我添加了一些类型签名，但 Burn 有 Float 而不是特定的 f64，我对此有点困惑。事实上，只是为了调试目的，我想从 Burn 张量中提取数据作为 Vec&lt;f64&gt; 来查看它（我应该从 vec: Vec&lt;f64&gt; 中看到相同的值）但我得到了运行时类型不兼容。
使用 rand::prelude::Distribution;
使用 statrs::distribution::Cauchy;
使用 rand_chacha::ChaCha8Rng;
使用 rand_core::SeedableRng;
使用 burn::tensor::{Tensor, TensorData, Float};
使用 burn::backend::Wgpu;

类型 Backend = Wgpu;

fn main() {
// 一些全局引用
let device = Default::default();
let mut rng: ChaCha8Rng = ChaCha8Rng::seed_from_u64(2);

// 使用 statrs 创建随机 vec，存储在 Vec&lt;f64&gt; 中
let dist: Cauchy = Cauchy::new(5.0, 2.0).unwrap();
let vec: Vec&lt;f64&gt; = dist.sample_iter(&amp;mut rng).take(10).collect();

// 将其包装到 Burn 张量中
let td: TensorData = TensorData::new(vec, [10, 1]);
let tensor: Tensor&lt;Backend, 2, Float&gt; = Tensor::&lt;Backend, 2, Float&gt;::from_data(td, &amp;device);

print!(&quot;{:?}\n&quot;, tensor.to_data().to_vec::&lt;f64&gt;().unwrap());
}


在上面运行时，我得到
thread &#39;main&#39; panicked at src/main.rs:23:55:
called `Result::unwrap()` on an `Err` value: TypeMismatch(&quot;Invalid target element type 
(expected F32, got F64)&quot;)

使用 to_vec::&lt;f32&gt; 有效，但我希望 Burn 张量具有 f64 值（torch 有这个），因为错误似乎意味着我在某个时候丢失了精度 - 不太好。
是否可以将 f64 存储在 Burn 张量中？]]></description>
      <guid>https://stackoverflow.com/questions/79333072/using-f64-in-burn-tensor-in-rust</guid>
      <pubDate>Mon, 06 Jan 2025 12:43:18 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法将手势识别集成到 .NET MAUI 中？[关闭]</title>
      <link>https://stackoverflow.com/questions/79332674/is-there-any-way-to-integrate-hand-gesture-recognition-in-net-maui</link>
      <description><![CDATA[我有一个名为“手语应用”的项目，我需要在其中集成手语手势识别。我不知道从哪里开始，因为我只具备 C# 基础知识。
我需要指南或建议，看看是否可行。]]></description>
      <guid>https://stackoverflow.com/questions/79332674/is-there-any-way-to-integrate-hand-gesture-recognition-in-net-maui</guid>
      <pubDate>Mon, 06 Jan 2025 09:55:54 GMT</pubDate>
    </item>
    <item>
      <title>检测器模型中框的正确损失函数</title>
      <link>https://stackoverflow.com/questions/79331211/correct-loss-function-for-bboxes-in-a-detector-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79331211/correct-loss-function-for-bboxes-in-a-detector-model</guid>
      <pubDate>Sun, 05 Jan 2025 17:31:42 GMT</pubDate>
    </item>
    <item>
      <title>如何指定使用集成分类器在网格搜索中进行迭代的级别？</title>
      <link>https://stackoverflow.com/questions/79330620/how-to-specify-the-levels-to-iterate-in-a-grid-search-with-an-ensemble-classifie</link>
      <description><![CDATA[我有以下设置，但我找不到在网格搜索中传递级别以探索 svm* 和 mlp* 的方法：
steps = [(&#39;preprocessing&#39;, StandardScaler()),
(&#39;feature_selection&#39;, SelectKBest(mutual_info_classif, k=15)),
(&#39;clf&#39;, VotingClassifier(estimators=[(&quot;mlp1&quot;, mlp1),
(&quot;mlp2&quot;, mlp2),
(&quot;mlp3&quot;, mlp3),
(&quot;svm1&quot;, svm1),
(&quot;svm2&quot;, svm2)
], voting=&#39;soft&#39;))
]

model = Pipeline(steps=steps)
params = [{
&#39;preprocessing&#39;: [StandardScaler(), MinMaxScaler(), MaxAbsScaler()],
&#39;feature_selection__score_func&#39;: [f_classif, mutual_info_classif]
}]

grid_search = GridSearchCV(model, params, cv=10,scoring=&#39;balanced_accuracy&#39;, verbose=1, n_jobs=20, refit=True)
]]></description>
      <guid>https://stackoverflow.com/questions/79330620/how-to-specify-the-levels-to-iterate-in-a-grid-search-with-an-ensemble-classifie</guid>
      <pubDate>Sun, 05 Jan 2025 11:22:57 GMT</pubDate>
    </item>
    <item>
      <title>如何将 pixelClassificationLayer() 更新为自定义损失函数？</title>
      <link>https://stackoverflow.com/questions/79328556/how-do-i-update-pixelclassificationlayer-to-a-custom-loss-function</link>
      <description><![CDATA[我在 Mathworks 官方网站上看到 pixelClassificationLayer() 函数，我应该使用以下代码将其更新为自定义损失函数：
function loss = modelLoss(Y,T) 
mask = ~isnan(T);
target(isnan(T)) = 0;
loss = crossentropy(Y,T,Mask=mask,NormalizationFactor=&quot;mask-included&quot;); 
end

netTrained = trainnet(images,net,@modelLoss,options); 

但是，我看不到任何关于输入“Classes”或“ClassWeights”的提及，我目前正使用它们来定义自定义 pixelClassificationLayer：
pixelClassificationLayer(&#39;Classes&#39;,classNames,&#39;ClassWeights&#39;,classWeights)，其中 classNames 是一个向量，以字符串形式包含每个类的名称，classWeights 是一个向量，包含每个类的权重，用于在训练数据中存在代表性不足的类时平衡类。
如何在自定义损失函数中包含这些参数？]]></description>
      <guid>https://stackoverflow.com/questions/79328556/how-do-i-update-pixelclassificationlayer-to-a-custom-loss-function</guid>
      <pubDate>Sat, 04 Jan 2025 09:05:32 GMT</pubDate>
    </item>
    <item>
      <title>负类的 precision_recall_curve 导致正相关的精度和召回率</title>
      <link>https://stackoverflow.com/questions/79327647/precision-recall-curve-of-negative-class-leading-to-positively-correlated-precis</link>
      <description><![CDATA[我有一个逻辑回归模型来预测二进制输出（0 或 1）。我想了解 0 类的 P/R，并生成相应的曲线。我使用这个代码：
clf = linear_model.LogisticRegression().fit(ohe_X_train, y_train)
# 预测独热编码测试集上的标签
clf_predictions = clf.predict(ohe_X_test)
y_scores = clf.predict_proba(ohe_X_test)

class_of_interest = 0

# 基于 precision_recall_fscore_support 的 P/R
precision, recall, fscore, support = precision_recall_fscore_support(y_test, clf_predictions, labels=[class_of_interest])

# 使用 precision_recall_curve 的 P/R 曲线
y_scores = clf.predict_proba(ohe_X_test)[:, class_of_interest]
precision_curve, recall_curve, Thresholds = precision_recall_curve(y_test, y_scores)

plt.plot(recall_curve, precision_curve, marker=&#39;.&#39;)
plt.xlabel(&#39;Recall&#39;)
plt.ylabel(&#39;Precision&#39;)
plt.title(&#39;Precision-Recall Curve for Class 0&#39;)
plt.grid(True)
plt.show()

在这种情况下，PR 曲线在提高召回率的同时提高了准确率。我做错了什么？当 class_of_interest = 1 时，它工作正常。]]></description>
      <guid>https://stackoverflow.com/questions/79327647/precision-recall-curve-of-negative-class-leading-to-positively-correlated-precis</guid>
      <pubDate>Fri, 03 Jan 2025 20:45:09 GMT</pubDate>
    </item>
    <item>
      <title>对随机森林进行修改，每次分割时都会评估某些特征</title>
      <link>https://stackoverflow.com/questions/79290974/modification-of-random-forest-to-always-evaluate-some-features-at-every-split</link>
      <description><![CDATA[我正在尝试更改随机森林分类器的功能。虽然通常每次分割都会随机选择特征，但我希望每次分割时都评估一个特定特征。我知道这会影响性能，但我想尝试一下这在非常具体的用例中是否是个好主意。因此，调整的结果应为：用于分割的特征是随机选择的（像往常一样），但始终会考虑一个特定特征（例如索引 15）（不一定使用）。
我不知道有哪些软件包允许开箱即用。有没有一个，或者也许有一个简单的解决方法来实现相同的效果？]]></description>
      <guid>https://stackoverflow.com/questions/79290974/modification-of-random-forest-to-always-evaluate-some-features-at-every-split</guid>
      <pubDate>Wed, 18 Dec 2024 11:48:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我扫描模型参数时，我的 GPU 内存不断增加？</title>
      <link>https://stackoverflow.com/questions/79283083/why-does-my-gpu-memory-keep-increasing-when-i-sweep-over-model-parameters</link>
      <description><![CDATA[我正在尝试评估特定架构下具有不同丢弃率的模型分类错误率。当我这样做时，内存使用量会增加，而且我无法阻止这种情况发生（有关详细信息，请参阅下面的代码）：
N=2048 split 0 内存使用量
{&#39;current&#39;: 170630912, &#39;peak&#39;: 315827456}
{&#39;current&#39;: 345847552, &#39;peak&#39;: 430210560}
{&#39;current&#39;: 530811136, &#39;peak&#39;: 610477568}
...
{&#39;current&#39;: 1795582208, &#39;peak&#39;: 1873805056}
N=2048 split 1 内存使用量
{&#39;current&#39;: 1978317568, &#39;peak&#39;: 2056609280}
{&#39;current&#39;: 2157136640，&#39;峰值&#39;：2235356160}
...
2024-12-15 18:55:04.141690：W external/local_xla/xla/tsl/framework/bfc_allocator.cc:497] 分配器 (GPU_0_bfc) 在尝试分配 op 请求的 52.00MiB（四舍五入为 54531328）时内存不足
...
2024-12-15 18:55:04.144298：I tensorflow/core/framework/local_rendezvous.cc:405] 本地会合正在中止，状态为：RESOURCE_EXHAUSTED：尝试分配 54531208 字节时内存不足。
...

这是我正在运行的代码的相关部分，包括每次迭代后清除内存的一些不成功的尝试。
import tensorflow as tf
import tensorflow_datasets as tfds
import gc

batch_size = 128
sizes = [2048 + n * batch_size * 5 for n in range(10)]
dropout_points = 10

vals_ds = tfds.load(
&#39;mnist&#39;,
split=[f&#39;train[{k}%:{k+10}%]&#39; for k in range(0, 100, 10)],
as_supervised=True,
)
trains_ds = tfds.load(
&#39;mnist&#39;,
split=[f&#39;train[:{k}%]+train[{k+10}%:]&#39; for k in range(0, 100, 10)],
as_supervised=True,
)
_, ds_info = tfds.load(&#39;mnist&#39;, with_info=True)

def normalize_img(image, label):
return tf.cast(image, tf.float32) / 255., label

for N in sizes:
for i, (ds_train, ds_test) in enumerate(zip(trains_ds, vals_ds)):
ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_train = ds_train.shuffle(ds_info.splits[&#39;train&#39;].num_examples)
ds_train = ds_train.batch(128)

ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.batch(128)

print(f&quot;N={N} split {i} 内存使用情况&quot;)
with open(f&quot;out_{N}_{i}.csv&quot;, &quot;w&quot;) as f:
f.write((&quot;retention_rate,&quot;
&quot;train_loss,&quot;
&quot;train_err,&quot;
&quot;test_loss,&quot;
&quot;test_err,&quot;
&quot;epochs\n&quot;))
for p in range(dropout_points):
dropout_rate = p / dropout_points

layers = [tf.keras.layers.Flatten(input_shape=(28, 28))]
for i in range(4):
layers.append(tf.keras.layers.Dense(N,activation=&#39;relu&#39;))
layers.append(tf.keras.layers.Dropout(dropout_rate))
layers.append(tf.keras.layers.Dense(10))

with tf.device(&#39;/GPU:0&#39;):
model = tf.keras.models.Sequential(layers)
model.compile(
optimizer=tf.keras.optimizers.Adam(0.001),
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

callback = tf.keras.callbacks.EarlyStopping(monitor=&#39;loss&#39;, waiting=3)
history = model.fit(
ds_train,
epochs=100,
validation_data=ds_test,
verbose=0,
callbacks=[callback]
)

train_loss, train_acc = model.evaluate(ds_train, verbose=0)
test_loss, test_acc = model.evaluate(ds_test, verbose=0)
epochs = len(history.history[&#39;loss&#39;])
f.write((
f&quot;{1 - dropout_rate},&quot;
f&quot;{train_loss},&quot;
f&quot;{1 - train_acc},&quot;
f&quot;{test_loss},&quot;
f&quot;{1 - test_acc},&quot;
f&quot;{epochs}\n&quot;))
del model
tf.keras.backend.clear_session()
gc.collect()
print(tf.config.experimental.get_memory_info(&#39;GPU:0&#39;))

如何才能有效地执行此循环而不增加内存使用量？]]></description>
      <guid>https://stackoverflow.com/questions/79283083/why-does-my-gpu-memory-keep-increasing-when-i-sweep-over-model-parameters</guid>
      <pubDate>Sun, 15 Dec 2024 19:58:11 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“sklearn.neighbors._base”导入名称“_check_weights”</title>
      <link>https://stackoverflow.com/questions/75633185/importerror-cannot-import-name-check-weights-from-sklearn-neighbors-base</link>
      <description><![CDATA[我正在尝试使用 Missforest 来处理表数据中的缺失值。
import sklearn
print(sklearn.__version__)
-&gt;1.2.1

import sklearn.neighbors._base
import sys
sys.modules[&#39;sklearn.neighbors.base&#39;] = sklearn.neighbors._base

!pip install missingpy
from missingpy import MissForest

到目前为止，它运行良好，但从昨天开始，出现了以下错误消息。
ImportError：无法从“sklearn.neighbors._base”导入名称“_check_weights”

我想知道如何处理这个错误。]]></description>
      <guid>https://stackoverflow.com/questions/75633185/importerror-cannot-import-name-check-weights-from-sklearn-neighbors-base</guid>
      <pubDate>Sat, 04 Mar 2023 01:48:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 Conda + Poetry 有意义吗？</title>
      <link>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</link>
      <description><![CDATA[在机器学习项目中使用 Conda + Poetry 是否有意义？请允许我分享我的（新手）理解，请纠正或启发我：
据我所知，Conda 和 Poetry 有不同的用途，但在很大程度上是多余的：

Conda 主要是一个环境管理器（实际上不一定是 Python），但它也可以管理包和依赖项。
Poetry 主要是一个 Python 包管理器（例如，pip 的升级），但它也可以创建和管理 Python 环境（例如，Pyenv 的升级）。

我的想法是同时使用两者并划分它们的角色：让 Conda 成为环境管理器，让 Poetry 成为包管理器。我的理由是（听起来）Conda 最适合管理环境，可用于编译和安装非 Python 包，尤其是 CUDA 驱动程序（用于 GPU 功能），而 Poetry 作为 Python 包管理器比 Conda 更强大。
我已经设法通过在 Conda 环境中使用 Poetry 相当轻松地完成这项工作。诀窍是不使用 Poetry 来管理 Python 环境：我没有使用 poetry shell 或 poetry run 之类的命令，只使用 poetry init、poetry install 等（在激活 Conda 环境后）。
为了全面披露，我的 environment.yml 文件（用于 Conda）如下所示：
name: N

channels:
- defaults
- conda-forge

dependencies:
- python=3.9
- cudatoolkit
- cudnn

我的 poetry.toml 文件如下所示：
[tool.poetry]
name = &quot;N&quot;
authors = [&quot;B&quot;]

[tool.poetry.dependencies]
python = &quot;3.9&quot;
torch = &quot;^1.10.1&quot;

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

说实话，我这样做的原因之一是，在没有 Conda 的情况下，我很难安装 CUDA（用于 GPU 支持）。
这个项目设计对你来说合理吗？]]></description>
      <guid>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</guid>
      <pubDate>Tue, 25 Jan 2022 15:09:43 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在同一层使用多个损失函数吗？</title>
      <link>https://stackoverflow.com/questions/62861773/can-we-use-multiple-loss-functions-in-same-layer</link>
      <description><![CDATA[我们可以在这个架构中使用多个损失函数吗：
我有两种不同类型的损失函数，并希望在最后一层使用它 [输出]
损失函数：

binary_crossentropy
自定义损失函数

我们可以这样做吗？
]]></description>
      <guid>https://stackoverflow.com/questions/62861773/can-we-use-multiple-loss-functions-in-same-layer</guid>
      <pubDate>Sun, 12 Jul 2020 13:37:51 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：Tensor 转换请求 dtype 为 float64，而 dtype 为 float32</title>
      <link>https://stackoverflow.com/questions/45111136/valueerror-tensor-conversion-requested-dtype-float64-for-tensor-with-dtype-floa</link>
      <description><![CDATA[我有一个 NN，它有两个相同的 CNN（类似于 Siamese 网络），然后合并输出，并打算在合并的输出上应用自定义损失函数，如下所示：
 ----------------- -----------------
| input_a | | input_b |
----------------- -----------------
| base_network | | base_network |
------------------------------------------------------
|processed_a_b |
------------------------------------------------------

在我的自定义损失函数中，我需要将 y 垂直分成两部分，然后在每部分上应用分类交叉熵损失。但是，我不断从损失函数中获取 dtype 错误，例如：
ValueError Traceback（最近一次调用最后一次）&lt;ipython-input-12-b01f2c4c71e3&gt; in &lt;module&gt;()
----&gt; 1 model.compile(loss=categorical_crossentropy_loss, optimizer=RMSprop())

/usr/local/lib/python3.5/dist-packages/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, **kwargs)
909 loss_weight = loss_weights_list[i]
910 output_loss = weighted_loss(y_true, y_pred,
--&gt; 911 sample_weight, mask)
912 if len(self.outputs) &gt; 1:
913 self.metrics_tensors.append(output_loss)

/usr/local/lib/python3.5/dist-packages/keras/engine/training.py in weighted(y_true, y_pred, weights, mask)
451 # 应用样本权重
452 如果权重不为 None:
--&gt; 453 score_array *= weights
454 score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))
455 返回 K.mean(score_array)

/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
827 if not isinstance(y, sparse_tensor.SparseTensor):
828 try:
--&gt; 829 y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=&quot;y&quot;)
830 except TypeError:
831 # 如果 RHS 不是张量，则可能是张量感知对象

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
674 name=name,
675 preferred_dtype=preferred_dtype,
-&gt; 676 as_ref=False)
677 
678 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py 在 internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
739 
740 如果 ret 为 None:
--&gt; 741 ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
742 
743 如果 ret 未实现：

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
612 引发 ValueError(
613 &quot;Tensor 转换请求 dtype %s 的 Tensor 具有 dtype %s：%r&quot;
--&gt; 614 % (dtype.name, t.dtype.name, str(t)))
615 返回 t
616 

ValueError：Tensor 转换请求 dtype float64 的 Tensor 具有 dtype float32： &#39;Tensor(&quot;processed_a_b_sample_weights_1:0&quot;, shape=(?,), dtype=float32)&#39;

以下是重现错误的 MWE：
import tensorflow as tf
from keras import backend as K
from keras.layers import Input, Dense, merge, Dropout
from keras.models import Model, Sequential
from keras.optimizers import RMSprop
import numpy as np

# 定义输入
input_dim = 10
input_a = Input(shape=(input_dim,), name=&#39;input_a&#39;)
input_b = Input(shape=(input_dim,), name=&#39;input_b&#39;)
# 定义 base_network
n_class = 4
base_network = Sequential(name=&#39;base_network&#39;)
base_network.add(Dense(8, input_shape=(input_dim,),activation=&#39;relu&#39;))
base_network.add(Dropout(0.1))
base_network.add(Dense(n_class,activation=&#39;relu&#39;))
processed_a = base_network(input_a)
processed_b = base_network(input_b)
# 合并左右部分
processed_a_b = merge([processed_a,processed_b],mode=&#39;concat&#39;,concat_axis=1,name=&#39;processed_a_b&#39;)
# 创建模型
model = Model(inputs=[input_a,input_b],outputs=processed_a_b)

# 自定义损失函数
def categorical_crossentropy_loss(y_true,y_pred):
# 拆分（取消合并）y_true 和 y_pred分成两部分
y_true_a, y_true_b = tf.split(value=y_true, num_or_size_splits=2, axis=1)
y_pred_a, y_pred_b = tf.split(value=y_pred, num_or_size_splits=2, axis=1)
loss = K.categorical_crossentropy(output=y_pred_a, target=y_true_a) + K.categorical_crossentropy(output=y_pred_b, target=y_true_b) 
return K.mean(loss)

# 编译模型
model.compile(loss=categorical_crossentropy_loss, optimizer=RMSprop())
]]></description>
      <guid>https://stackoverflow.com/questions/45111136/valueerror-tensor-conversion-requested-dtype-float64-for-tensor-with-dtype-floa</guid>
      <pubDate>Fri, 14 Jul 2017 20:28:09 GMT</pubDate>
    </item>
    </channel>
</rss>