<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 06 Oct 2024 21:14:35 GMT</lastBuildDate>
    <item>
      <title>是DataLoader的问题还是模型的问题？</title>
      <link>https://stackoverflow.com/questions/79059527/is-the-problem-with-the-dataloader-or-the-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79059527/is-the-problem-with-the-dataloader-or-the-model</guid>
      <pubDate>Sun, 06 Oct 2024 16:04:47 GMT</pubDate>
    </item>
    <item>
      <title>Amazon Sagemaker 无法识别 xgboost 中的超参数</title>
      <link>https://stackoverflow.com/questions/79059204/amazon-sagemaker-is-not-able-to-recognize-a-hyperparameter-in-xgboost</link>
      <description><![CDATA[我创建了一个 AWS 笔记本实例来对客户流失进行分类，并尝试训练模型：
hyperparameters = {
&quot;objective&quot;:&quot;binary:logistic&quot;,
&quot;num_round&quot;:&quot;50&quot;}

estimator = XGBoost(entry_point = &quot;train.py&quot;, 
framework_version=&#39;1.7-1&#39;,
hyperparameters=hyperparameters,
role=sagemaker.get_execution_role(),
instance_count=1,
instance_type=&#39;ml.m5.2xlarge&#39;,
output_path=output_path)

我收到此错误
参数“objective”应为以下选项之一

以及目标的相应选项，在我的情况下是客户流失，所以它要么是 0，要么是 1，所以我将它设置为
binary:logistic

这是一个有效的选项，但我收到了这个错误。如何解决？
我也尝试使用 estimator.set_hyperparameters() 设置超参数，仍然是同样的错误。]]></description>
      <guid>https://stackoverflow.com/questions/79059204/amazon-sagemaker-is-not-able-to-recognize-a-hyperparameter-in-xgboost</guid>
      <pubDate>Sun, 06 Oct 2024 13:32:12 GMT</pubDate>
    </item>
    <item>
      <title>在大学聊天机器人中微调 chatgpt 时出现问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/79059023/problems-in-finetuning-chatgpt-to-use-it-in-college-chatbot</link>
      <description><![CDATA[我无法进行微调...所有方法我都无法应用
我想要一个值得信赖的方法
我尝试了 YouTube 上的很多方法，但
所有方法都出现 git 错误
我需要一个可以在 google colab 中使用的代码
或者为我提供与我的项目相关的微调过程的网站。]]></description>
      <guid>https://stackoverflow.com/questions/79059023/problems-in-finetuning-chatgpt-to-use-it-in-college-chatbot</guid>
      <pubDate>Sun, 06 Oct 2024 12:04:45 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 NEAT 查找/改变神经元的潜力？</title>
      <link>https://stackoverflow.com/questions/79059007/how-to-find-change-the-potential-of-a-neuron-with-neat</link>
      <description><![CDATA[我正在使用 neat-python。我试图让 CTRNN 模拟正弦函数，并希望使用 CTRNN 演示神经元作为种子。然而，我尝试修改的神经元完全是非确定性的；除非我在 pop=neat.Population(config) 之前放置种子，否则每次运行代码时它们的行为都会改变。我唯一没有找到的值是“潜在”或神经元的衰减（如本CTRNN 文档所述。我查看了 github 源代码、配置文档，并使用 dir 命令查看了各种 Python 对象的对象描述。
这是具体代码，带有确定性的种子），使用config-ctrnn 来自单极平衡。
import neat
import numpy as np
import matplotlib.pyplot as plt
import random

simulationSteps = 100
timeConst = 1/100
goalFunc = lambda step: np.sin(step*25)

config = neat.Config(
neat.DefaultGenome,
neat.DefaultReproduction,
neat.DefaultSpeciesSet,
neat.DefaultStagnation,
r&quot;path_to\CTRNNconfig&quot;
)

random.seed(5)
pop = neat.Population(config)
pop.add_reporter(neat.StdOutReporter(False))

# 播种
for i in range(100, 200):
gen = pop.population[i+1]
n1, n2 = list(gen.nodes.keys())[1:]
for node, bias in zip((n1,n2), (-2.75/5, -1.75/5)):
# gen.nodes[node].aggregation = sum
# gen.nodes[node].activation = sigmoid_activation
gen.nodes[node].bias = bias
gen.nodes[node].response = 1
gen.add_connection(config.genome_config, n1, n1, 0.9, True)
gen.add_connection(config.genome_config, n1, n2, 0.2, True)
gen.add_connection(config.genome_config, n2, n1,-0.2, True)
gen.add_connection(config.genome_config, n2, n2, 0.9, True)

# 仅绘制最后一个种子基因组
network = neat.ctrnn.CTRNN.create(gen, config, timeConst)
network.set_node_value(n1, 0)
network.set_node_value(n2, 0)
network.set_node_value(0, 0.5)

actions = []
correct = []
for step in range(simulationSteps):
action = network.advance(
[], 
timeConst, 
timeConst,
)
action.append(action[0])
correct.append(goalFunc(step)/3 + 0.5)

plt.plot(actions, label=&quot;Network&quot;)
plt.plot(correct, label=&quot;Goal&quot;)
plt.legend()
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79059007/how-to-find-change-the-potential-of-a-neuron-with-neat</guid>
      <pubDate>Sun, 06 Oct 2024 11:52:31 GMT</pubDate>
    </item>
    <item>
      <title>无法运行 Github 存储库</title>
      <link>https://stackoverflow.com/questions/79058738/not-able-to-run-github-repository</link>
      <description><![CDATA[我的项目是“社交媒体中的多模态命名实体识别”（尝试将其扩展为聊天机器人）
我正尝试使用这个 github 存储库作为基础
https://github.com/Multimodal-NER/RpBERT
尽管尝试了几次，我还是无法运行它。
有人可以帮我吗？你能告诉我如何实现它的步骤吗？
我已经将它克隆到我的系统中。我的系统中的 loader.py 文件出现错误，但不知何故修复了它。我运行了 main.py 文件。它似乎正在运行。但我仍然有几个疑问。我想知道如何从头开始实现它。]]></description>
      <guid>https://stackoverflow.com/questions/79058738/not-able-to-run-github-repository</guid>
      <pubDate>Sun, 06 Oct 2024 09:38:24 GMT</pubDate>
    </item>
    <item>
      <title>分类神经网络不收敛[关闭]</title>
      <link>https://stackoverflow.com/questions/79058176/classification-neural-network-not-converging</link>
      <description><![CDATA[我为 MNIST 开发了一个基本的分类网络，但在训练期间，验证准确率在 10% 左右。我尝试过各种优化器（SGD、Adam、Nadam）以及不同的学习率（0.1、1e-3、1e-4、1e-5），但验证准确率在每个时期都保持在 10% 左右。
这是我的代码：
import tensorflow as tf
from tensorflow import keras

(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()
X_train, X_val = X_train[5000:]/255.0, X_train[:5000]/255.0
y_train, y_val = y_train[5000:]/255.0, y_train[:5000]/255.0

class_NN = keras.models.Sequential([
keras.layers.Input(shape = [28, 28]),
keras.layers.Flatten(),
keras.layers.Dense(300, 激活 = &quot;relu&quot;),
keras.layers.Dense(100, 激活 = &quot;relu&quot;),
keras.layers.Dense(10, 激活 = &quot;softmax&quot;)
])

class_NN.compile(loss = &quot;sparse_categorical_crossentropy&quot;,
optimizer = keras.optimizers.SGD(learning_rate = 1e-3), 
metrics = [&quot;accuracy&quot;])
class_NN.fit(X_train, y_train, epochs = 15,
validation_data = (X_val, y_val))

我在写这道题的时候，在开始训练之前从代码中去掉了/255.0。目前，验证的准确率随着训练的进行而提高。我的验证准确率达到了 96% 左右。是什么原因导致在训练过程中去掉/255.0后验证准确率有所提高？
X_train, X_val = X_train[5000:], X_train[:5000]
y_train, y_val = y_train[5000:], y_train[:5000]
]]></description>
      <guid>https://stackoverflow.com/questions/79058176/classification-neural-network-not-converging</guid>
      <pubDate>Sun, 06 Oct 2024 00:55:12 GMT</pubDate>
    </item>
    <item>
      <title>LogisticRegression 未返回正确结果</title>
      <link>https://stackoverflow.com/questions/79057824/logisticregression-not-returning-correct-results</link>
      <description><![CDATA[我试图使用基于训练数据的逻辑回归对测试数据中的一系列点进行分类预测。
我得到了一个没有错误的输出，但我被告知结果是错误的（错误的输出将被视为错误，但运行代码时没有错误）。
训练数据是 375 个点中 4 个不同类别的集合，每个点有 3 个变量，因此绘制在 3D 图形上。我在该集合上运行了一个计数程序，发现超过 50% 的点属于第 2 类。我的初步结果是所有测试点都属于第 2 类。我尝试将训练数据排序为几个不同的集合：随机选择 125 个条目（这是测试数据的大小），找到所有类别的最小数量，然后使用每个类别中相同数量的点创建一个训练集。
无排序 = 所有第 2 类
随机排序 = 所有第 2 类
等类数排序 = 给我一个答案，其中点被归类在所有 4 个类别中，但当我将它们插入在线最终测试表格时，我的准确率得分为 26%，这与随机机会相同。所以，我没有正确处理数据，我不确定在哪里。我希望有更多回归分类经验的人能为我指明正确的方向。
编辑：在调用 LogisticRegression 之前，我是否需要重新格式化（转换）train_X、train_y 和 test_X 数组？如果需要，该怎么做？也许我只是给它提供了格式错误的数据。
如果您想获取带有输入数据列表的代码，可以访问此处：text
如果您只想查看代码，请点击此处：
# 形成表格以推动逻辑回归
train_X = []
train_y = []
for i in range(len(train_table)):
train_X.append(
[train_table.x.iloc[i], train_table.y.iloc[i], train_table.z.iloc[i]]
)
train_y.append(train_table.label.iloc[i])

test_X = []
for k in range(len(test_table)):
test_X.append([test_table.x.iloc[k], test_table.y.iloc[k], test_table.z.iloc[k]])

# 尝试使用和不使用 normalize
clf = LogisticRegression().fit(normalize(train_X), train_y)
# clf = LogisticRegression().fit(train_X, train_y)

predict = clf.predict(test_X[:])
prob = clf.predict_proba(test_X[:])

results = pd.DataFrame(
sort_results(test_table, predict, prob),
columns=[&quot;&quot;, &quot;timestamp&quot;, &quot;UTC time&quot;, &quot;label&quot;, &quot;accuracy&quot;],
)
]]></description>
      <guid>https://stackoverflow.com/questions/79057824/logisticregression-not-returning-correct-results</guid>
      <pubDate>Sat, 05 Oct 2024 19:41:26 GMT</pubDate>
    </item>
    <item>
      <title>Azure AI | 机器学习实时推理 [关闭]</title>
      <link>https://stackoverflow.com/questions/79057387/azure-ai-machine-learning-real-time-inference</link>
      <description><![CDATA[我创建了一个 Azure ML 实验（带有拖放组件），其实际行为就像一个简单的 Web 服务。
基本上，我根据数据集上的输入（而不是使用经过训练的机器学习模型）进行查询，然后将行作为输出返回。
我创建了一个类似于 SQL 查询系统的管道，但它没有按预期工作。我尝试了几种方法来实现这一点（应用 SQL 转换、连接数据、执行 Python 脚本），但都没有成功。输出始终为空。
我不明白我做错了什么。
在所附图片/场景中，我的 t1 数据集包含超过 4500 行，具有以下字段：
PROC_CODE
FIELD_1
FIELD_2
FIELD_3
我的 t2（数据集中选择列的输出）仅包含 PROC_CODE。
错误显示：
ModuleExceptionMessage:InvalidSQLScript: SQL 查询“SELECT * FROM t1 WHERE t1.PROC_CODE = t2.PROC_CODE”不正确。异常消息：（sqlite3.OperationalError）没有这样的列：t2.PROC_CODE

[SQL：SELECT * FROM t1 WHERE t1.PROC_CODE = t2.PROC_CODE]
我想知道这（与类似 SQL 的查询系统类似的管道）是否可行，然后解决问题。]]></description>
      <guid>https://stackoverflow.com/questions/79057387/azure-ai-machine-learning-real-time-inference</guid>
      <pubDate>Sat, 05 Oct 2024 15:48:19 GMT</pubDate>
    </item>
    <item>
      <title>如何将自动编码器模型实现为 PMML？</title>
      <link>https://stackoverflow.com/questions/79038884/how-to-implement-an-autoencoder-model-as-pmml</link>
      <description><![CDATA[假设我们有以下模型：


我们如何构建这样的模型并将其导出为 PMML 文件？
PMML 能够对这样的模型结构进行编码吗？
PMML 中生成此模型中的 N 个输出节点所需的组件是什么？我理解对每个输出节点使用 &lt;OutputField ... feature=&quot;predictedValue&gt; 不会给出预期的结果。
]]></description>
      <guid>https://stackoverflow.com/questions/79038884/how-to-implement-an-autoencoder-model-as-pmml</guid>
      <pubDate>Mon, 30 Sep 2024 10:14:02 GMT</pubDate>
    </item>
    <item>
      <title>信用风险中的神经网络：召回率、准确度还是精确度？</title>
      <link>https://stackoverflow.com/questions/79019012/neural-network-in-credit-risk-recall-accuracy-or-precision</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79019012/neural-network-in-credit-risk-recall-accuracy-or-precision</guid>
      <pubDate>Tue, 24 Sep 2024 14:12:11 GMT</pubDate>
    </item>
    <item>
      <title>错误 conda.core.link:_execute(698): 安装包“defaults::icu-58.2-ha925a31_3”时发生错误</title>
      <link>https://stackoverflow.com/questions/63871492/error-conda-core-link-execute698-an-error-occurred-while-installing-package</link>
      <description><![CDATA[我使用 anaconda prompt conda create -n talkingbot python=3.5 创建了环境，然后安装了 pip install tensorflow==1.0.0（遵循与 udemy 课程中使用的相同命令），但是当我尝试使用 conda install spyder 安装 spyder 时，它给了我这个错误：
准备交易：完成
验证交易：完成
执行交易：完成
错误 conda.core.link:_execute(698)：安装包“defaults::icu-58.2-ha925a31_3”时发生错误。
回滚事务：完成

[Errno 13] 权限被拒绝：&#39;C:\\Users\\Lenovo\\anaconda3\\envs\\talkingbot\\Library\\bin\\icudt58.dll&#39;
()

然后我尝试使用 anaconda navigator 安装 spyder，但 spyder 也未安装。
帮我解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/63871492/error-conda-core-link-execute698-an-error-occurred-while-installing-package</guid>
      <pubDate>Sun, 13 Sep 2020 13:42:24 GMT</pubDate>
    </item>
    <item>
      <title>神经网络模型准确率超低</title>
      <link>https://stackoverflow.com/questions/59278771/super-low-accuracy-for-neural-network-model</link>
      <description><![CDATA[我按照教程使用交叉验证对神经网络模型进行评估，代码如下：
# 使用鸢尾花数据集进行多类分类 
import numpy 
import pandas 
from keras.models import Sequential 
from keras.layers import Dense 
from keras.wrappers.scikit_learn import KerasClassifier 
from keras.utils import np_utils 
from sklearn.model_selection import cross_val_score 
from sklearn.model_selection import KFold 
from sklearn.preprocessing import LabelEncoder 
from sklearn.pipeline import Pipeline 
# 修复随机种子以实现可重复性 
seed = 7 
numpy.random.seed(seed) 
# 加载数据集 
dataframe = pandas.read_csv(&quot;/content/drive/My Drive/iris.data&quot;, header=None) 
dataset = dataframe.values 
X = dataset[:,0:4].astype(float) 
Y = dataset[:,4] 

# 将类值编码为整数 
encoder = LabelEncoder() 
encoder.fit(Y) 
encoded_Y =coder.transform(Y) 

# 将整数转换为虚拟变量（即独热编码） 
dummy_y = np_utils.to_categorical(encoded_Y) 

# 定义基线模型 
def baseline_model():

# 创建模型
model = Sequential()
model.add(Dense(4, input_dim=4,activation=&quot;relu&quot;, kernel_initializer=&quot;normal&quot;))
model.add(Dense(3,activation=&quot;sigmoid&quot;, kernel_initializer=&quot;normal&quot;))

# 编译模型
model.compile(loss= &#39;categorical_crossentropy&#39; , optimizer= &#39;adam&#39; , metrics=[ &#39;accuracy&#39; ])

返回模型 
estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, batch_size=5, verbose=0) 
kfold = KFold(n_splits=10, shuffle=True, random_state=seed) 
results = cross_val_score(estimator, X, dummy_y, cv=kfold) 
print(&quot;Accuracy: %.2f%% (%.2f%%)&quot; % (results.mean()*100, results.std()*100))

准确率应该在 95.33% (4.27%) 左右，但我尝试了几次，得到的结果是 ~Accuracy: 34.00% (13.15%)。模型代码似乎完全相同。我按照说明从此处下载了数据。可能出现什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/59278771/super-low-accuracy-for-neural-network-model</guid>
      <pubDate>Wed, 11 Dec 2019 04:08:32 GMT</pubDate>
    </item>
    <item>
      <title>Python 神经网络准确性-正确实现？</title>
      <link>https://stackoverflow.com/questions/31738520/python-neural-network-accuracy-correct-implementation</link>
      <description><![CDATA[我编写了一个简单的神经网络/MLP，但我得到了一些奇怪的准确度值，我想再检查一下。
这是我想要的设置：具有 913 个样本和 192 个特征（913,192）的特征矩阵。我正在对 2 个结果进行分类，所以我的标签是二进制的，形状为 (913,1)。1 个隐藏层，有 100 个单元（目前）。所有激活都将使用 tanh，所有损失都使用 l2 正则化，并使用 SGD 进行优化
代码如下。它是用 Keras 框架 (http://keras.io/) 用 Python 编写的，但我的问题与 Keras 无关
input_size = 192
hidden_​​size = 100
output_size = 1
lambda_reg = 0.01
learning_rate = 0.01
num_epochs = 100
batch_size = 10

model = Sequential()
model.add(Dense(input_size, hidden_​​size, W_regularizer=l2(lambda_reg), init=&#39;uniform&#39;))
model.add(Activation(&#39;tanh&#39;))
model.add(Dropout(0.5))

model.add(Dense(hidden_​​size, output_size, W_regularizer=l2(lambda_reg), init=&#39;uniform&#39;))
model.add(Activation(&#39;tanh&#39;))

sgd = SGD(lr=learning_rate, decay=1e-6, motivation=0.9, nesterov=True)
model.compile(loss=&#39;mean_squared_error&#39;, optimizer=sgd, class_mode=&quot;binary&quot;)

history = History()

model.fit(features_all, labels_all, batch_size=batch_size, nb_epoch=num_epochs, show_accuracy=True, verbose=2, validation_split=0.2, callbacks=[history])
score = model.evaluate(features_all, labels_all, show_accuracy=True, verbose=1)

我想再检查一下我写的代码是否正确我希望它能够根据我选择的参数及其值等来实现。
使用上面的代码，我得到的训练和测试集准确率在 50-60% 左右徘徊。也许我只是使用了不好的特征，但我想测试一下可能出了什么问题，所以我手动将所有标签和特征设置为可预测的内容：
labels_all[:500] = 1
labels_all[500:] = 0
features_all[:500] = np.ones(192)*500
features_all[500:] = np.ones(192)

因此，我将前 500 个样本的标签设置为 1，其他所有样本的标签都为 0。我手动将前 500 个样本的所有特征设置为 500，所有其他特征（对于其余样本）都设置为 1
当我运行此程序时，我的训练准确率约为 65%，验证准确率约为 0%。我原本期望两种准确率都非常高/几乎完美 - 这不正确吗？我的想法是，具有极高值的特征都具有相同的标签 (1)，而具有低值的特征则获得 0 标签
我主要想知道我的代码/模型是否不正确，或者我的逻辑是否错误。]]></description>
      <guid>https://stackoverflow.com/questions/31738520/python-neural-network-accuracy-correct-implementation</guid>
      <pubDate>Fri, 31 Jul 2015 05:13:58 GMT</pubDate>
    </item>
    <item>
      <title>神经网络的精度</title>
      <link>https://stackoverflow.com/questions/11228407/precision-in-neural-network</link>
      <description><![CDATA[我想知道神经网络是否能够回归非常接近的目标值。例如：
输入 [100 150 200 300]
输出 [0.99903 0.99890 0.99905 0.99895]

或者应该处理输出或目标数据？]]></description>
      <guid>https://stackoverflow.com/questions/11228407/precision-in-neural-network</guid>
      <pubDate>Wed, 27 Jun 2012 14:22:54 GMT</pubDate>
    </item>
    <item>
      <title>如何训练人工神经网络使用视觉输入玩暗黑破坏神 2？</title>
      <link>https://stackoverflow.com/questions/6542274/how-to-train-an-artificial-neural-network-to-play-diablo-2-using-visual-input</link>
      <description><![CDATA[我目前正在尝试让 ANN 玩视频游戏，并希望从这里出色的社区获得一些帮助。
我选择了暗黑破坏神 2。因此，游戏是实时的，并且从等距视角进行，玩家控制一个以相机为中心的化身。
具体来说，任务是让你的角色获得 x 经验值，而不会让其生命值降至 0，其中经验值是通过杀死怪物获得的。以下是游戏玩法示例：

现在，由于我希望网络仅基于从屏幕上的像素获得的信息进行操作，因此它必须学习非常丰富的表示才能有效地玩游戏，因为这可能需要它知道（至少隐式地）如何将游戏世界划分为对象以及如何与它们交互。
所有这些信息都必须以某种方式传授给网络。我无论如何也想不出如何训练这个东西。我唯一的想法是让一个单独的程序从屏幕上直观地提取游戏中天生的好/坏东西（例如健康、金币、经验），然后在强化学习过程中使用该统计数据。我认为这将是答案的一部分，但我认为这还不够；从原始视觉输入到面向目标的行为，抽象层次实在太多，以至于在我有生之年，仅靠如此有限的反馈无法训练出一个网络。
所以，我的问题是：您还能想到什么其他方法来训练网络来完成这项任务的至少一部分？最好不要制作数千个带标签的示例。
仅提供一点指导：我正在寻找其他强化学习来源和/或任何无监督方法来提取此设置中的有用信息。或者，如果您能想到一种无需手动标记即可从游戏世界中获取带标签数据的方法，则可以使用监督算法。
更新（2012 年 4 月 27 日）：
奇怪的是，我仍在研究这个问题，似乎正在取得进展。让 ANN 控制器工作的最大秘诀是使用适合该任务的最先进的 ANN 架构。因此，我一直在使用由分解的条件限制玻尔兹曼机组成的深度信念网络，我以无人监督的方式对其进行了训练（在我玩游戏的视频中），然后使用时间差反向传播（即使用标准前馈 ANN 的强化学习）进行微调。
仍在寻找更有价值的输入，尤其是关于实时动作选择问题以及如何编码颜色的问题图像用于 ANN 处理 :-) 
更新（2015 年 10 月 21 日）：
我刚想起来我以前问过这个问题，我想我应该提一下，这不再是一个疯狂的想法。自从我上次更新以来，DeepMind 发表了他们的自然关于让神经网络通过视觉输入玩 Atari 游戏的论文。事实上，唯一阻止我使用他们的架构来玩《暗黑破坏神 2》的一个有限子集的原因是缺乏对底层游戏引擎的访问。渲染到屏幕然后将其重定向到网络实在是太慢了，无法在合理的时间内进行训练。因此，我们可能不会很快看到这种机器人玩《暗黑破坏神 2》，但这只是因为它将玩一些开源游戏或具有渲染目标 API 访问权限的游戏。（也许是《雷神之锤》？）]]></description>
      <guid>https://stackoverflow.com/questions/6542274/how-to-train-an-artificial-neural-network-to-play-diablo-2-using-visual-input</guid>
      <pubDate>Thu, 30 Jun 2011 23:47:29 GMT</pubDate>
    </item>
    </channel>
</rss>