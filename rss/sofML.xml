<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 08 Feb 2024 12:23:50 GMT</lastBuildDate>
    <item>
      <title>无法为 HeteroData 创建 NeighborLoader：“EdgeStorage”对象没有属性“num_nodes”</title>
      <link>https://stackoverflow.com/questions/77961450/cant-create-a-neighborloader-for-heterodata-edgestorage-object-has-no-attrib</link>
      <description><![CDATA[基本上正如标题所示，当我尝试为我的数据创建 NeighborLoader 时，出现以下错误。
AttributeError：“EdgeStorage”对象没有属性“num_nodes”

如何获取错误：
我加载图表并使用 RandomNodeSplit 将其拆分为训练/测试/验证，然后，我尝试将拆分数据传递到 NeighborLoader 中并得到上面的错误。
使用的代码：
data = torch.load(training_config[&#39;data_file&#39;])
目标 = pd.read_pickle(training_config[&#39;targets_file&#39;])
打印（类型（数据））

# 创建训练、测试和 VAL 掩码
split = T.RandomNodeSplit(num_val=training_config[&#39;validation_split&#39;], num_test=training_config[&#39;test_split&#39;])
data_split = 分割（数据）
data_split.num_nodes = data.num_nodes
打印（数据分割）
打印（类型（数据分割））
采样器 = ImbalancedSampler(data_split[&#39;word&#39;].y, input_nodes=data_split[&#39;word&#39;].train_mask)

train_loader = NeighborLoader(
    数据分割，
    num_neighbors=[10] * 2,
    batch_size=training_config[&#39;batch_size&#39;],
    input_nodes=data_split[&#39;word&#39;].train_mask,
    采样器=采样器
）

这是这些打印语句的结果：
&lt;类&#39;torch_geometric.data.hetero_data.HeteroData&#39;&gt;

异质数据（
  num_classes=2,
  num_nodes=59565,
  字={
    y=[39566],
    x=[39566, 2],
    train_mask=[39566],
    val_mask=[39566],
    test_mask=[39566],
  },
  句子={ x=[19999, 1] },
  (word, depGraph, word)={ edge_index=[2, 934] },
  (词, 头, 词)={ edge_index=[2, 934] },
  (单词, previousWord, 单词)={ edge_index=[2, 842] },
  (单词, fromSentence, 句子)={ edge_index=[2, 574] },
  (单词, 下一个单词, 单词)={ edge_index=[2, 842] },
  (word, pos, pos)={ edge_index=[2, 39566] },
  (字, 边, 边)={ edge_index=[2, 39566] },
  (word, feat_aspect, feat_aspect)={ edge_index=[2, 1318] },
  (word, feat_case, feat_case)={ edge_index=[2, 1251] },
  (word, feat_conjtype, feat_conjtype)={ edge_index=[2, 708] },
  (word, feat_definite, feat_definite)={ edge_index=[2, 5349] },
  (单词, feat_ Degree, feat_ Degree )={ edge_index=[2, 2735] },
  (单词, feat_foreign, feat_foreign)={ edge_index=[2, 19] },
  (单词, feat_gender, feat_gender)={ edge_index=[2, 169] },
  (单词, feat_mood, feat_mood)={ edge_index=[2, 307] },
  (单词, feat_number, feat_number)={ edge_index=[2, 14435] },
  (word, feat_numtype, feat_numtype)={ edge_index=[2, 1588] },
  (word, feat_person, feat_person)={ edge_index=[2, 1713] },
  (单词, feat_polity, feat_polarity)={ edge_index=[2, 35] },
  (word, feat_poss, feat_poss)={ edge_index=[2, 142] },
  (单词, feat_prontype, feat_prontype)={ edge_index=[2, 7914] },
  (word, feat_punctside, feat_punctside)={ edge_index=[2, 1344] },
  (单词, feat_puncttype, feat_puncttype)={ edge_index=[2, 3603] },
  (word, feat_tense, feat_tense)={ edge_index=[2, 1895] },
  (单词, feat_verbform, feat_verbform)={ edge_index=[2, 2457] },
  (句子, 下一个句子, 句子)={ edge_index=[2, 39996] }
）

&lt;类“torch_geometric.data.hetero_data.HeteroData”&gt;

所以类型是 HeteroData，但是，NeighborLoader 在某处获取 EdgeStorage，而我无法使用它来批处理我的数据？我可以尝试解决此问题吗？
我可以在不进行批处理的情况下训练 SageConv，但是，我想尝试一下，看看它如何影响我的结果。]]></description>
      <guid>https://stackoverflow.com/questions/77961450/cant-create-a-neighborloader-for-heterodata-edgestorage-object-has-no-attrib</guid>
      <pubDate>Thu, 08 Feb 2024 11:53:39 GMT</pubDate>
    </item>
    <item>
      <title>我在使用 OpenCV Python 库在 python 中运行代码时遇到问题</title>
      <link>https://stackoverflow.com/questions/77961054/i-have-a-problem-running-a-code-in-python-using-opencv-python-library</link>
      <description><![CDATA[我正在使用 OpenCV Python 库使用 jubyter 创建图像分类器模型，问题是代码是正确的，但库中的函数仍然存在问题，我不知道问题是否出在 OpenCV 版本中或别的东西
错误群发
我正在编写图像分类器代码，并且此单元格中发生了错误
这是错误消息中的函数]]></description>
      <guid>https://stackoverflow.com/questions/77961054/i-have-a-problem-running-a-code-in-python-using-opencv-python-library</guid>
      <pubDate>Thu, 08 Feb 2024 10:53:03 GMT</pubDate>
    </item>
    <item>
      <title>机器学习训练缓慢</title>
      <link>https://stackoverflow.com/questions/77960959/machine-learning-training-slowly</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77960959/machine-learning-training-slowly</guid>
      <pubDate>Thu, 08 Feb 2024 10:38:26 GMT</pubDate>
    </item>
    <item>
      <title>使用重叠窗口构建时间序列</title>
      <link>https://stackoverflow.com/questions/77959721/build-timeseries-with-overlapping-window</link>
      <description><![CDATA[我正在尝试使用 tf.data 构建一个数据管道，该管道将构建过去 5 行的时间序列。由于我有超过 6000 个不同的 csv 文件，这些文件无法放入内存，并且由于大小而无法进行预处理并保存在磁盘中。
要读取 6K csv，我尝试使用 tf.data.experimental.make_csv_dataset 并使用窗口函数应用重叠窗口。到目前为止我得到的最接近的是：
数据集 = tf.data.experimental.make_csv_dataset(
    file_pattern=“/path/stock/*1min*.csv”,
    批量大小=1，
    num_epochs=1,
    随机播放=假，
    标题=假，
    column_names=[&#39;时间戳&#39;,&#39;开盘价&#39;,&#39;最高价&#39;, &#39;最低价&#39;, &#39;收盘价&#39;, &#39;交易量&#39;],
    column_defaults=[tf.string、tf.float32、tf.float32、tf.float32、tf.float32、tf.float32]
）。窗户（
    size=5, # 每个窗口的行数
    shift=1, # 重叠窗口的步幅
    步幅=1
）

理想情况下，我最终的形状应该为（M、5 时间步长、4 [开盘价、最低价、最高价、收盘价]） 
如何解决这个问题？
有没有更好的方法来完成这个任务？]]></description>
      <guid>https://stackoverflow.com/questions/77959721/build-timeseries-with-overlapping-window</guid>
      <pubDate>Thu, 08 Feb 2024 06:46:18 GMT</pubDate>
    </item>
    <item>
      <title>是否有一种工具或强大的应用程序技术可以让我们根据数据集自动填充 Excel 工作表中问题的答案？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77959600/is-there-a-tool-or-power-apps-technique-where-we-can-auto-populate-the-responses</link>
      <description><![CDATA[是否有一种工具或强大的应用程序技术可以让我们根据数据集自动填充 Excel 工作表中问题的答案？
示例 - 我们有一个包含 200 个问题和答案的数据集，我们必须根据工作表中提出的 20 个问题自动用答案填充 Excel 工作表。这应该是一个自动化的过程。
请告诉我市场上是否有可用的工具或我们可以使用的技术。
我尝试了 copilot，我们可以单独获得响应，但需要更加自动化和快速的流程。]]></description>
      <guid>https://stackoverflow.com/questions/77959600/is-there-a-tool-or-power-apps-technique-where-we-can-auto-populate-the-responses</guid>
      <pubDate>Thu, 08 Feb 2024 06:14:52 GMT</pubDate>
    </item>
    <item>
      <title>就地修剪 nn.Linear 权重会导致意外错误，需要稍微奇怪的解决方法。需要解释</title>
      <link>https://stackoverflow.com/questions/77959410/pruning-nn-linear-weights-inplace-causes-unexpected-error-requires-slightly-wei</link>
      <description><![CDATA[失败
导入火炬

def 测试1():
  层 = nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试1()

有错误
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
RuntimeError Traceback（最近一次调用最后一次）
&lt;ipython-input-3-bb36a010bd86&gt;在&lt;细胞系：10&gt;()
      8 x = 5 - torch.sum(layer(torch.ones(90)))
      9 x.backward()
---&gt; 10 测试1()
     11 # 这也有效
     12

2帧
/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py 向后（张量，grad_tensors，retain_graph，create_graph，grad_variables，输入）
    249 # 一些 Python 版本打印多行函数的第一行
    [第 250 章]
--&gt; 251 Variable._execution_engine.run_backward( # 调用 C++ 引擎来运行向后传递
    252个张量，
    第253章

RuntimeError: 函数 TBackward0 在索引 0 返回无效渐变 - 得到 [10, 90] 但预期形状与 [10, 100] 兼容

这有效
导入火炬

def test2():
  层 = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  del x #主要变化
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试2()

这也有效
导入火炬
def test3():
  层 = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  layer.weight = torch.nn.Parameter(layer.weight) #主要变化
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试3()

我在尝试实现一篇关于模型修剪的论文时遇到了这个问题。我相信这与 autograd 图有关，但我不确定到底发生了什么。有什么解释可以解释为什么这些几乎相同的代码片段有效或失败吗？]]></description>
      <guid>https://stackoverflow.com/questions/77959410/pruning-nn-linear-weights-inplace-causes-unexpected-error-requires-slightly-wei</guid>
      <pubDate>Thu, 08 Feb 2024 05:17:32 GMT</pubDate>
    </item>
    <item>
      <title>在 esp32-cam 中使用人脸识别时出现错误 cam_hal：EV-VSYNC-OVF</title>
      <link>https://stackoverflow.com/questions/77958199/error-cam-hal-ev-vsync-ovf-when-using-face-recognition-in-esp32-cam</link>
      <description><![CDATA[我正在我的 esp32-cam 板上使用示例“CameraWebServer”。上传设置如下：
开发板：AI Thinker ESP32-CAM；
CPU频率：240MHz；
闪光频率：80 Mhz；
闪光模式：QIO。
Arduino集成开发环境2.0.0
esp32 乐鑫 版本 2.0.14

通过这些设置，我可以上传我的代码，但粪便识别功能不起作用。当我单击“注册面部”时，没有任何反应，并且我的串行监视器显示消息 EV-VSYNC-OVF。如何解决这个问题？
此外，我已经尝试修改上传设置并更改文件“CameraWebServer.ino”中的参数 config.frame_size 和 config.xclk_freq_hz，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77958199/error-cam-hal-ev-vsync-ovf-when-using-face-recognition-in-esp32-cam</guid>
      <pubDate>Wed, 07 Feb 2024 22:12:55 GMT</pubDate>
    </item>
    <item>
      <title>如何让这个专家混合模型在张量流中工作？</title>
      <link>https://stackoverflow.com/questions/77957928/how-can-i-get-this-mixture-of-experts-model-working-in-tensorflow</link>
      <description><![CDATA[我有两个张量。
张量 1 的形状为 (10, None, 16, 16, 64)
张量 2 的形状为 (None, 10)
“无”是c的批量大小
第一个张量表示来自 10 个不同模型的 logits 集合 (10)，其形状是每组 logits，(None) 是批量大小，(16, 16, 64) 是相应模型的输出。
第二个张量表示来自 1 个较小模型的一组 logits（无），即批处理大小，(10) 是 10 个值，表示第一个张量中每组 10 个 logits 的权重应如何。
我想将第一个张量乘以第二个张量，以便输出形状为 (10, None, 16, 16, 64)，并且第一个轴上的每组 logit 由第二个张量的相应 logit 进行加权
然后，我将对第一个轴上的相乘张量求和，以获得 MoE 模型的一个块的输出
以下是所有这些的实施方式（顺便说一下，MOPE 代表预训练专家的混合）：
def CreateMOPEBlock(x, 块, blockNum):
    专家日志 = []

    对于范围内的 i（num_classes）：
        块[i].trainable = False
        ExpertLogits.append(块[i](x))

    门控输入 = x
    GatingConv1 = tf.keras.layers.Conv2D(16, (3, 3), padding=&#39;相同&#39;)(GatingInput)
    GatingLayerNorm1 = tf.keras.layers.LayerNormalization()(GatingConv1)
    GatingLeakyReLU1 = tf.keras.layers.LeakyReLU()(GatingLayerNorm1)
    GatingConv2 = tf.keras.layers.Conv2D(32, (3, 3), padding=&#39;相同&#39;)(GatingLeakyReLU1)
    GatingLayerNorm2 = tf.keras.layers.LayerNormalization()(GatingConv2)
    GatingLeakyReLU2 = tf.keras.layers.LeakyReLU()(GatingLayerNorm2)
    GatingConv3 = tf.keras.layers.Conv2D(64, (3, 3), padding=&#39;相同&#39;)(GatingLeakyReLU2)
    GatingLayerNorm3 = tf.keras.layers.LayerNormalization()(GatingConv3)
    GatingLeakyReLU3 = tf.keras.layers.LeakyReLU()(GatingLayerNorm3)
    GatingFlatten = tf.keras.layers.Flatten()(GatingLeakyReLU3)
    GatingLogits = tf.keras.layers.Dense（num_classes，激活=&#39;softmax&#39;）（GatingFlatten）

    logits1 = ExpertLogits # 形状：(10, 无, 16, 16, 64)
    logits2 = GatingLogits # 形状：（无，10）

    # 在这里做一些奇特的数学计算
    多重逻辑 = ?

    返回 tf.keras.layers.add(multiple_logits)

MOPEInput = tf.keras.layers.Input(形状=(32, 32, 3))

# Block1、2和3只是10个keras顺序模型的数组
MOPEBlock1 = CreateMOPEBlock(MOPEInput, 块1)
MOPEBlock2 = CreateMOPEBlock(MOPEBlock1, 块2)
MOPEBlock3 = CreateMOPEBlock(MOPEBlock2, 块3)

MOPEFlatten = tf.keras.layers.Flatten()(MOPEBlock3)

MOPEX = tf.keras.layers.Dense(1024，激活=&#39;relu&#39;)(MOPEFlatten)
MOPEX = tf.keras.layers.BatchNormalization()(MOPEX)
MOPEX = tf.keras.layers.Dropout(0.33)(MOPEX)

MOPEX = tf.keras.layers.Dense(1024，激活=&#39;relu&#39;)(MOPEX)
MOPEX = tf.keras.layers.BatchNormalization()(MOPEX)
MOPEX = tf.keras.layers.Dropout(0.33)(MOPEX)

MOPEOutput = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;)(MOPEX)

MOPEModel = tf.keras.Model(MOPEInput, MOPEOutput)

我已经尝试自己解决这个问题了！多次！
我还尝试询问多种大型语言模型，从 Mixtral-8x7b（以我的名字命名）到 GPT4。
结果看起来像这样：
将张量流导入为 tf

# 假设这些是你的张量
张量1 = tf.placeholder(tf.float32, shape=(10, 无, 16, 16, 64))
张量2 = tf.placeholder(tf.float32, shape=(无, 10))

# 重塑张量2（无，10）-&gt; （无、10、1、1、1）
tensor2_expanded = tf.expand_dims(tf.expand_dims(tf.expand_dims(tensor2, axis=-1), axis=-1), axis=-1)

# 排列张量1的轴 (10, None, 16, 16, 64) -&gt; （无、10、16、16、64）
tensor1_permuted = tf.transpose(tensor1, perm=[1, 0, 2, 3, 4])

# 张量相乘
结果= tf.multiply（tensor1_permuted，tensor2_expanded）

# 最后，将结果的轴排列回来 (None, 10, 16, 16, 64) -&gt; （10、无、16、16、64）
结果 = tf.transpose(结果, perm=[1, 0, 2, 3, 4])

即使对每个模型进行了广泛的调试，这些模型的解决方案也不起作用。
我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/77957928/how-can-i-get-this-mixture-of-experts-model-working-in-tensorflow</guid>
      <pubDate>Wed, 07 Feb 2024 21:13:53 GMT</pubDate>
    </item>
    <item>
      <title>错误：OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: 错误：(-215:断言失败) !_src.empty() 在函数“cvtColor”中</title>
      <link>https://stackoverflow.com/questions/77957561/error-opencv4-8-0-io-opencv-modules-imgproc-src-color-cpp182-error-215</link>
      <description><![CDATA[augmented_yes = &#39;/content/drive/MyDrive/脑肿瘤检测/augmented-images/yes&#39;
Augmented_no = &#39;/content/drive/MyDrive/脑肿瘤检测/augmented-images/no&#39;

IMG_宽度、IMG_高度 = (240, 240)

X, y = load_data([augmented_yes,augmented_no], (IMG_WIDTH, IMG_HEIGHT))

我正在运行上面的代码，它给了我这个错误：
错误回溯（最近一次调用最后一次）
&lt;ipython-input-32-c5b21f394fc2&gt;在&lt;细胞系：6&gt;()
      4 IMG_宽度、IMG_高度 = (240, 240)
      5
----&gt; 6 X, y = load_data([augmented_yes,augmented_no], (IMG_WIDTH, IMG_HEIGHT))

1 帧
&lt;ipython-input-15-acac6ff41d23&gt;在crop_brain_contour（图像，绘图）中
      6
      7 # 将图像转为灰度图，并稍微模糊一下
----&gt; 8 灰度 = cv2.cvtColor(图像, cv2.COLOR_BGR2GRAY)
      9 灰色 = cv2.GaussianBlur(灰色, (5, 5), 0)
     10

错误：OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: 错误：(-215:断言失败) !_src.empty() 在函数“cvtColor”中

这是load_data()函数：
def load_data(dir_list, image_size):
    ”“”
    读取图像，调整大小并标准化它们。
    论据：
        dir_list：表示文件目录的字符串列表。
    返回：
        X：形状 = (#_examples, image_width, image_height, #_channels) 的 numpy 数组
        y：形状 = (#_examples, 1) 的 numpy 数组
    ”“”

    # 加载目录下的所有图片
    X = []
    y = []
    图像宽度、图像高度 = 图像大小

    对于 dir_list 中的目录：
        对于 listdir（目录）中的文件名：
            # 加载图像
            image = cv2.imread(目录 + &#39;\\&#39; + 文件名)
            # 裁剪大脑并忽略图像中不必要的其余部分
            图像=crop_brain_contour（图像，情节=假）
            # 调整图像大小
            图像 = cv2.resize(图像, dsize=(image_width, image_height), 插值=cv2.INTER_CUBIC)
            # 标准化值
            图像=图像/255。
            # 将图像转换为 numpy 数组并将其附加到 X
            X.append(图像)
            # 如果图像为目标数组，则将值 1 附加到目标数组
            # 位于名为“yes”的文件夹中，否则附加 0。
            如果目录[-3:] == &#39;是&#39;:
                y.追加([1])
            别的：
                y.追加([0])

    X = np.array(X)
    y = np.array(y)

    # 打乱数据
    X, y = 随机播放(X, y)

    print(f&#39;示例数量为：{len(X)}&#39;)
    print(f&#39;X 形状是：{X.shape}&#39;)
    print(f&#39;y 形状是: {y.shape}&#39;)

    返回 X, y

这是crop_brain_contour()函数：
defcrop_brain_contour（图像，plot=False）：

    #导入imutils
    #导入CV2
    #从 matplotlib 导入 pyplot 作为 plt

    # 将图像转为灰度图，并稍微模糊一下
    灰色 = cv2.cvtColor(图像, cv2.COLOR_BGR2GRAY)

    灰色 = cv2.GaussianBlur(灰色, (5, 5), 0)

    # 对图像设置阈值，然后执行一系列腐蚀 +
    # 膨胀以消除任何小噪声区域
    阈值 = cv2.threshold(灰色, 45, 255, cv2.THRESH_BINARY)[1]
    thresh = cv2.erode(thresh, 无, 迭代=2)
    thresh = cv2.dilate(thresh, 无, 迭代=2)

    # 在阈值图像中找到轮廓，然后抓取最大的轮廓
    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = imutils.grab_contours(cnts)
    c = max(cnts, key=cv2.contourArea)


    # 找到极值点
    extLeft = tuple(c[c[:, :, 0].argmin()][0])
    extRight = tuple(c[c[:, :, 0].argmax()][0])
    extTop = 元组(c[c[:, :, 1].argmin()][0])
    extBot = tuple(c[c[:, :, 1].argmax()][0])

    # 使用四个极值点（左、右、上、下）从原始图像中裁剪出新图像
    new_image = 图像[extTop[1]:extBot[1], extLeft[0]:extRight[0]]

    如果情节：
        plt.figure()

        plt. 子图(1, 2, 1)
        plt.imshow(图像)

        plt.tick_params(axis=&#39;两者&#39;,which=&#39;两者&#39;,
                        上=假，下=假，左=假，右=假，
                        labelbottom=False、labeltop=False、labelleft=False、labelright=False)

        plt.title(&#39;原图&#39;)

        plt. 子图(1, 2, 2)
        plt.imshow(new_image)

        plt.tick_params(axis=&#39;两者&#39;,which=&#39;两者&#39;,
                        上=假，下=假，左=假，右=假，
                        labelbottom=False、labeltop=False、labelleft=False、labelright=False)

        plt.title(&#39;裁剪后的图像&#39;)

        plt.show()

    返回新图像

检查我正在进行的脑肿瘤检测项目的错误在哪里。]]></description>
      <guid>https://stackoverflow.com/questions/77957561/error-opencv4-8-0-io-opencv-modules-imgproc-src-color-cpp182-error-215</guid>
      <pubDate>Wed, 07 Feb 2024 19:52:57 GMT</pubDate>
    </item>
    <item>
      <title>了解变量选择和调整后的 randomForestSRC 行为 [关闭]</title>
      <link>https://stackoverflow.com/questions/77954827/understanding-randomforestsrc-behaviour-after-variable-selection-and-tuning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77954827/understanding-randomforestsrc-behaviour-after-variable-selection-and-tuning</guid>
      <pubDate>Wed, 07 Feb 2024 12:43:24 GMT</pubDate>
    </item>
    <item>
      <title>RNN 的训练循环在每个 epoch 后返回相同的损失</title>
      <link>https://stackoverflow.com/questions/77938129/training-loop-of-rnn-returning-the-same-loss-after-each-epoch</link>
      <description><![CDATA[我正在尝试借助此存储库从头开始构建 RNN (https: //github.com/nicklashansen/rnn_lstm_from_scratch/tree/master），但每个时期后的训练损失保持不变。训练循环的代码如下：
# 超参数
纪元数 = 1000

# 初始化一个新网络
参数 = init_rnn(hidden_​​size=hidden_​​size, vocab_size=vocab_size)

# 将隐藏状态初始化为零
隐藏状态 = np.zeros((隐藏大小, 1))

# 轨迹丢失
训练损失、验证损失 = []、[]

def check_if_params_updated(old_params, new_params):
    # 该函数检查两组参数是否不同
    对于 zip 中的 old_param、new_param(old_params, new_params)：
        如果不是 np.array_equal(old_param, new_param):
            return True # 参数已更新
    return False # 参数尚未更新


# 对于每个纪元
对于范围内的 i（num_epochs）：
    
    # 轨迹丢失
    epoch_training_loss = 0
    epoch_validation_loss = 0
    
     # 对于验证集中的每个句子
    对于输入，val_loader 中的目标：
        
        # One-hot 编码输入和目标序列
        input_one_hot = one_hot_encode_sequence（输入，vocab_size）
        target_one_hot = one_hot_encode_sequence（目标，vocab_size）
        
        # 重新初始化隐藏状态
        隐藏状态 = np.zeros_like(隐藏状态)

        # 前向传递
        输出，hidden_​​states =forward_pass（inputs_one_hot，hidden_​​state，params）

        # 向后传递
        损失，_ =向后传递（inputs_one_hot，输出，hidden_​​states，targets_one_hot，参数）
        
        # 更新损失
        epoch_validation_loss += 损失
    
    # 对于训练集中的每个句子
    对于输入，train_loader 中的目标：
        
        # One-hot 编码输入和目标序列
        input_one_hot = one_hot_encode_sequence（输入，vocab_size）
        target_one_hot = one_hot_encode_sequence（目标，vocab_size）
        
        # 重新初始化隐藏状态
        隐藏状态 = np.zeros_like(隐藏状态)

        # 前向传递
        输出，hidden_​​states =forward_pass（inputs_one_hot，hidden_​​state，params）

        # 向后传递
        损失，梯度=backward_pass（inputs_one_hot，输出，hidden_​​states，targets_one_hot，参数）
        打印（inputs_one_hot.shape）
        
        如果 np.isnan(损失):
            raise ValueError(&#39;梯度消失/爆炸！&#39;)
        
        # 更新参数
        params = update_parameters(params, grads, lr=1e-3)
        
        # 更新损失
        epoch_training_loss += 损失
        
    # 保存绘图损失
    Training_loss.append(epoch_training_loss/len(training_set))
    validation_loss.append(epoch_validation_loss/len(validation_set))

    # 每 100 个 epoch 打印损失
    如果我％100==0：
        print(f&#39;Epoch {i}, 训练损失: {training_loss[-1]}, 验证损失: {validation_loss[-1]}&#39;)


# 获取测试集中的第一个句子
输入，目标 = test_set[1]

# One-hot 编码输入和目标序列
input_one_hot = one_hot_encode_sequence（输入，vocab_size）
target_one_hot = one_hot_encode_sequence（目标，vocab_size）

# 将隐藏状态初始化为零
隐藏状态 = np.zeros((隐藏大小, 1))

# 前向传递
输出，hidden_​​states =forward_pass（inputs_one_hot，hidden_​​state，params）
output_sentence = [idx_to_word[np.argmax(output)] 用于输出中的输出]
print(&#39;输入句子：&#39;)
打印（输入）

print(&#39;\n目标序列:&#39;)
打印（目标）

print(&#39;\n预测序列:&#39;)
print([idx_to_word[np.argmax(output)] 用于输出中的输出])

# 绘制训练和验证损失图
纪元 = np.arange(len(training_loss))
plt.figure()
plt.plot(epoch, Training_loss, &#39;r&#39;, label=&#39;训练损失&#39;,)
plt.plot(epoch,validation_loss,&#39;b&#39;,label=&#39;验证损失&#39;)
plt.图例()
plt.xlabel(&#39;Epoch&#39;), plt.ylabel(&#39;NLL&#39;)
plt.show()

我尝试检查我的参数是否正在更新，它们确实更新了，还尝试检查梯度，它们并不是指数小。每次迭代后损失都会减少，但总纪元的损失保持不变。您可以在存储库中找到完整的代码，其中包括前向和后向传递(https://github.com/危险dude237/RNN_From_Scratch）。]]></description>
      <guid>https://stackoverflow.com/questions/77938129/training-loop-of-rnn-returning-the-same-loss-after-each-epoch</guid>
      <pubDate>Mon, 05 Feb 2024 00:28:27 GMT</pubDate>
    </item>
    <item>
      <title>在 Rust-linfa 中加载用于预测的线性回归模型</title>
      <link>https://stackoverflow.com/questions/77932307/loading-a-linear-regression-model-back-up-for-prediction-in-rust-linfa</link>
      <description><![CDATA[我一直在研究 Rust 机器学习的 linfa，特别是线性回归模型。我希望能够保存和加载经过训练的线性回归模型，但我无法找到实现此目的的方法。
方法 1：
到目前为止，我的方法是获取训练中涉及的主要参数，这些参数可以从 linfa 的线性回归实现中获取，并将它们存储在一个可以存储为 JSON 文件的结构中（通过 serde_json 完成）。然而，在此之后我不知道如何将其加载回来进行训练。
以上内容详情如下：
存储训练参数的结构：
struct ModelJson {
    系数：Vec f64 ，
    拦截：f64，
}

存储过程：
let model = lin_reg.fit(&amp;dataset)?;
让 model_json = ModelJson {
    系数： model.params().to_vec(),
    拦截： model.intercept(),
};

存储的数据看起来如何：
{“系数”:[-0.00017907873576254802,-0.00100659702068151,-0.0008275037845519519,0.0004613216043979551,0.00103006349345 99436]，“拦截”：50.525680622870084}

方法 2：
关于序列化和反序列化整个模型，我发现以下信息表明 linfa 中支持相同的操作。
加载和保存模型
这引出了我的第二种方法，其中我使用了 linfa-linear 的 serde 功能（包含 LinearRegression 模型），首先在我的 Cargo.toml 中包含以下内容：
linfa-线性 = {version=&quot;0.7.0&quot;, features=[&quot;serde&quot;]}
根据我对实现的理解，此功能为 LinearRegression 实现了以下功能：
Serde 序列化和反序列化实现 - 派生
上述实现：
&lt;前&gt;&lt;代码&gt;#[cfg_attr(
    特征=“serde”，
    派生（序列化，反序列化），
    serde(crate = “serde_crate”)
)]
/// 可用于进行预测的拟合线性回归模型。
pub struct FittedLinearRegression; {
    截距：F，
    参数：Array1，
}

发现于： linfa-线性导出实现
我的实现如下：
let model = lin_reg.fit(&amp;dataset)?;
让序列化 = serde_json::to_string(&amp;model).unwrap();

但是此方法出现以下错误：
不满足特征边界 `FittedLinearRegression: serde::ser::Serialize`
以下其他类型实现了特征 `serde::ser::Serialize`：
  布尔值
  字符
  大小
  i8
  i16
  i32
  i64
  i128
和其他 133 个rustcClick 以获取完整的编译器诊断
main.rs(82, 22)：此调用引入的绑定所需

是否有其他方法可以做到这一点，或者是否有某种方法可以使这些方法之一发挥作用？]]></description>
      <guid>https://stackoverflow.com/questions/77932307/loading-a-linear-regression-model-back-up-for-prediction-in-rust-linfa</guid>
      <pubDate>Sat, 03 Feb 2024 13:44:24 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“anomalib.engine”的模块</title>
      <link>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</link>
      <description><![CDATA[# 导入需要的模块

从 anomalib.data 导入 MVTec
从 anomalib.models 导入 Patchcore
从 anomalib.engine 导入引擎

错误：
ModuleNotFoundError：没有名为“anomalib.engine”的模块

我正在尝试运行这个......已经遵循了库安装并看到了
https://anomalib.readthedocs.io/en/latest/markdown/ get_started/anomalib.html
我认为要么是因为引擎已被修改，要么是被库删除了......
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</guid>
      <pubDate>Sat, 03 Feb 2024 05:25:02 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Kaggle 中使用 python 版本 3.7.1</title>
      <link>https://stackoverflow.com/questions/77537786/how-do-i-use-python-version-3-7-1-in-kaggle</link>
      <description><![CDATA[我正在尝试训练用 python 版本 3.7.1 编写的 TensorFlow ML 模型。然而kaggle中的python版本是3.10.2。有什么方法可以使用 python 3.7 环境并且可以将其设为默认环境吗？]]></description>
      <guid>https://stackoverflow.com/questions/77537786/how-do-i-use-python-version-3-7-1-in-kaggle</guid>
      <pubDate>Thu, 23 Nov 2023 14:49:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 JamesSteinEncoder 时出现“is_categorical 已弃用”错误</title>
      <link>https://stackoverflow.com/questions/63589556/getting-is-categorical-is-deprecated-error-while-using-jamessteinencoder</link>
      <description><![CDATA[尝试安装 JamesSteinEncoder 时出现以下错误
编码器 = JamesSteinEncoder().fit(X, y)

FutureWarning： is_categorical 已弃用，并将在未来版本中删除。使用 is_categorical_dtype 代替
  elif pd.api.types.is_categorical(cols):

sklearn.__version__ : &#39;0.23.2&#39;

代码：
速度 = [&#39;德国&#39;,&#39;澳大利亚&#39;,&#39;美国&#39;,&#39;法国&#39;,&#39;英国&#39;,&#39;韩国&#39;,&#39;澳大利亚&#39;]
寿命 = [1, 0, 1, 1, 1, 0, 1]
生命 = [1, 1, 0, 1, 1, 0, np.nan]
索引 = [&#39;蜗牛&#39;, &#39;猪&#39;, &#39;大象&#39;,
         ‘兔子’、‘长颈鹿’、‘土狼’、‘马’]
df = pd.DataFrame({&#39;速度&#39;: 速度,
                   “寿命”：寿命，
                  &#39;life&#39;:life}, 索引=索引)

df[&#39;速度&#39;]= df[&#39;速度&#39;].astype(&#39;类别&#39;)

 从category_encoders导入JamesSteinEncoder
    X = df[&#39;速度&#39;]
    y = df[&#39;寿命&#39;]
    enc = JamesSteinEncoder().fit(X, y)

/Users/*/opt/anaconda3/envs/proj/lib/python3.7/site-packages/category_encoders/utils.py:21：FutureWarning：is_categorical 已弃用，并将在未来版本中删除。使用 is_categorical_dtype 代替
  elif pd.api.types.is_categorical(cols):
]]></description>
      <guid>https://stackoverflow.com/questions/63589556/getting-is-categorical-is-deprecated-error-while-using-jamessteinencoder</guid>
      <pubDate>Wed, 26 Aug 2020 02:16:42 GMT</pubDate>
    </item>
    </channel>
</rss>