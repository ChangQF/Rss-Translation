<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Sun, 16 Feb 2025 01:22:24 GMT</lastBuildDate>
    <item>
      <title>使用numpy实现感知</title>
      <link>https://stackoverflow.com/questions/79442381/implementing-a-perceptron-using-numpy</link>
      <description><![CDATA[当使用符号z = xw+b时，我正在尝试使用numpy在python中实现一个感知器。在研究ML时，我确实看到Z = WX+B也很常见，尤其是在谈论神经网络时。问题在于矩阵的尺寸不累加，我尝试按照网络上的一些答案，但输出没有正确的尺寸。我也尝试询问chatgpt，但它仅在z = xw+b符号之后实现了代码。
这是我用于z = xw+b的代码：
 导入numpy作为NP

n_inpts = 10
in_feats = 5
n_hidden = 8
out_feats = 1

x = np.random.randn（n_inpts，in_feats）

w_x = np.random.randn（in_feats，n_hidden）

bias_h = np.random.randn（1，n_hidden）
h = np.dot（x，w_x） + bias_h
#H是NXH

relu = lambda x：max（0，x）
v_relu = np.Dectorize（relu）

h = v_relu（h）

w_h = np.random.randn（n_hidden，out_feats）

bias_o = np.random.randn（1，out_feats）

输出= np.dot（h，w_h） + bias_o
 
任何人都可以在使用z = wx+b时给我一个实现的实现吗？
我发现的每个实现都遵循z = xw+b符号。我想这取决于您如何指定X和W矩阵，但到目前为止，我还没有运气找到解决我的问题的解决方案]]></description>
      <guid>https://stackoverflow.com/questions/79442381/implementing-a-perceptron-using-numpy</guid>
      <pubDate>Sat, 15 Feb 2025 23:25:17 GMT</pubDate>
    </item>
    <item>
      <title>如何使用WebAssembly和TensorFlow.js优化浏览器中的实时ML推断，而不会损害性能？</title>
      <link>https://stackoverflow.com/questions/79442256/how-to-optimize-real-time-ml-inference-in-the-browser-using-webassembly-and-tens</link>
      <description><![CDATA[我正在开发一个实时的Web应用程序，该应用程序直接在浏览器中执行机器学习推断（使用自定义CNN模型）。该模型使用Rust（Wasm-Pack）编译为WebAssembly（WASM），我将其与TensorFlow.js集成在一起，以进行预处理和后处理。但是，当WASM模块和TensorFlow.js之间的数据传递时，我遇到了明显的延迟（每次推理200ms），尤其是使用高分辨率图像输入（1080x1080px）。&gt;
采取的步骤：
将Rust模型汇总到WASM，带有opt级= 3。
使用TF.Browser.Frompixels用于图像张量转换。
尝试了共享内存共享但面临安全限制（CORS和COOP标题）的共享arraybuffer。
基准的独立wasm（无tf.js）：〜50ms推理时间。
关键挑战：
内存转移开销：WASM和JS之间的数据序列化/次要化似乎很昂贵。
线程限制：WASM中的Rust的多线程不稳定，而TF.JS WebGL后端与同步斗争。
浏览器兼容性：Safari的WASM SIMD支持不一致。
在保持实时性能的同时，如何最大程度地减少WebAssembly和TensorFlow.js之间的延迟？是否有用于零拷贝数据传输或利用WebGPU进行并行处理的优化模式？]]></description>
      <guid>https://stackoverflow.com/questions/79442256/how-to-optimize-real-time-ml-inference-in-the-browser-using-webassembly-and-tens</guid>
      <pubDate>Sat, 15 Feb 2025 21:17:52 GMT</pubDate>
    </item>
    <item>
      <title>随着神经网络的训练，权重方差如何变化？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79440406/how-does-the-variance-of-weights-change-as-the-neural-network-is-trained</link>
      <description><![CDATA[变化在不同的ANN体系结构中会有所不同吗？
当我培训MLP网络时，我发现训练后的权重比训练之前大。标准偏差的增加是典型发生的吗？]]></description>
      <guid>https://stackoverflow.com/questions/79440406/how-does-the-variance-of-weights-change-as-the-neural-network-is-trained</guid>
      <pubDate>Fri, 14 Feb 2025 19:31:29 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降的theta值不连贯</title>
      <link>https://stackoverflow.com/questions/79440021/theta-values-for-gradient-descent-not-coherent</link>
      <description><![CDATA[我制作了梯度下降代码，但似乎效果很好
 导入numpy作为NP
从随机导入兰特，随机
导入matplotlib。 pyplot作为plt


def calculh（theta，x）：
    h = 0
    h+= theta [0]*x＃w*x
    h += theta [-1]＃ +b
    返回h


Def Calculy（Sigma，h）：
    返回Sigma（H）＃Sigma Peut-Etre Tanh，Signoide等。


Def Erreurj（Sigma Theta）：
    somme = 0
    somme = 1/4*（sigma（theta [1]）** 2+sigma（theta [0]+theta [1]）** 2）
    返回索姆


def梯度（x，y，ysol，sigmaprime，h）：
    返回（（y-ysol）*sigmaprime（h）*x，（y-ysol）*sigmaprime（h）*1）
def Grad（theta）：
    w，b = theta [0]，theta [1]
    #print（theta）
    返回[2*b ** 3+3*b ** 2*w+3*b*w ** 2-2*b+w ** 3-w，b ** 3+3*b ** 2* W+3*B*W ** 2-B+W ** 3-W]
＃ *X对应A 0 OU 1：NOS 2Entrées; *1对应A de b

Def Pasfixe（Theta，Eta，Epsilon，X，Y，YSOL，Sigma，Sigmaprime，H）：
    n = 0
    而np.linalg.norm（渐变（x，y，ysol，sigmaprime，h））＆gt; Epsilon和N＆lt; 10000：
        对于我的范围（len（theta））：
            theta [i] = theta [i]  -  eta*渐变（x，y，ysol，sigmaprime，h）[i]
            h = calculh（theta，x）
            Y = Calculy（Sigma，h）
            n+= 1
            如果theta [i]＆gt; 100：### cas de Divergence
                返回[100,100]，y
    返回Theta，Y

Sigma = Lambda Z：Z ** 2-1
sigmaprime = lambda z：2*z
ETA = 0.1

x = 1
ysol = 0
Listey = []
ListEtheta = []
lst = [[3*random（）*（ -  1）** randint（0,1），3*random（）*（ -  1）** randint（0,1）]在范围内（5000）]
NB = 0
因为我在LST：
        NB+= 1
        如果NB％50 == 0：
            打印（NB）
        theta = i [：]
        h = calculh（theta，x）
        Y = Calculy（Sigma，h）
        calcultheta = pasfixe（theta，eta，10 ** -4，x，y，ysol，sigma，sigmaprime，h）
        listetheta.append（calcultheta [0]）
        listey.append（calcultheta [1]）


对于我的范围（Len（Listey））：
          Listey [i] = round（Listey [i]，2）
打印（Listey）

对于我的范围（Len（ListEtheta））：
      对于J范围（2）的J：
          listetheta [i] [j] = round（listetheta [i] [j]，2）
打印（ListEtheta）

对于我的范围（LEN（LST））：
    如果[[-2,1]]中的[int（listetheta [i] [0]），int（listetheta [i] [1]）]：
        plt.plot（lst [i] [0]，lst [i] [1]
    elif [int（listEtheta [i] [0]），int（listetheta [i] [1]）在[[[2，-1]]中：
        plt.plot（lst [i] [0]，lst [i] [1]
    elif [int（listEtheta [i] [0]），int（listetheta [i] [1]）在[[[0，-1]]中：
        plt.plot（lst [i] [0]，lst [i] [1]
    elif [int（listEtheta [i] [0]），int（listEtheta [i] [1]）在[[[0,1]]中：
        plt.plot（lst [i] [0]，lst [i] [1]
    elif int（listetheta [i] [0]）** 2 +int（listEtheta [i] [1]）** 2＆gt; = 10：
        plt.plot（lst [i] [0]，lst [i] [1]

plt.show（）
 
最后，i做一个具有偏差和权重值的图形，每个点在循环开始时给出的theta（重量，偏置）值的功能都呈上色。
我应该具有 的图形
我试图自己计算梯度，但也没有起作用。我应该得到这样的图]]></description>
      <guid>https://stackoverflow.com/questions/79440021/theta-values-for-gradient-descent-not-coherent</guid>
      <pubDate>Fri, 14 Feb 2025 16:41:23 GMT</pubDate>
    </item>
    <item>
      <title>将数据分类为多个独立类的最佳策略[封闭]</title>
      <link>https://stackoverflow.com/questions/79439543/optimal-strategy-for-classification-data-into-multiple-independent-set-of-classe</link>
      <description><![CDATA[需要分类的数据属于两个独立的类。为简单起见，让我们想象一个可以具有 Shape&gt; Shape 和 color 的项目。形状可以是 square ， circle  三角形，而颜色可以是 red ，绿色或蓝色。每个项目都完全属于这两组类之一（即一个可以具有蓝色三角形）。
问题是，通过机器学习对此类数据进行分类的最佳策略是什么。到目前为止，我可以看到三种方法：

  两个独立的神经网络 
这很简单。一个网络将对形状进行分类，而另一个网络为颜色。但是，这看起来有些效率。

  多类分类 
在这里，网络的输出将是矩阵3x3（形状次数的颜色数），作为所有可能组合的产物。 （单个）结果将确定特定类别作为对这两个正交碱基的投影。这里的问题是，必须拥有非独立的NXM输出类，因此问题不必要。

  多标签分类 
在这里，输出将是向量3+3，并且将允许网络选择多个输出。虽然问题的维度现在要低得多，但通过允许模型选择多种形状和/或多种颜色，我们使模型可以预测无法发生的情况。


因此，问题仍然存在，在这种情况下的最佳策略是什么？是否有一些对这种问题最佳的混合范式？]]></description>
      <guid>https://stackoverflow.com/questions/79439543/optimal-strategy-for-classification-data-into-multiple-independent-set-of-classe</guid>
      <pubDate>Fri, 14 Feb 2025 13:33:46 GMT</pubDate>
    </item>
    <item>
      <title>如何在不同的数据范围内训练Sklearn模型？</title>
      <link>https://stackoverflow.com/questions/79439462/how-to-train-sklearn-model-in-different-dataframes</link>
      <description><![CDATA[我有一个用“ knn”制作的ML模型在Scikit-Learn中，注意到我的数据越多，我的模型就会越准确地说服它的预测。问题是，我有很多数据框架显示了我想预测的同一系统的不同情况。是否可以在那些不同的数据范围内训练模型？因为如果我致电.fit（），则将重置它是以前的培训。
  x_cleaned = x.dropna（）
y_cleaned = y [x_cleaned.index]
x_training，x_test，y_training，y_test = train_test_split（x_cleaned，y__cleaned，test_size = 0.15，Random_State = 0）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79439462/how-to-train-sklearn-model-in-different-dataframes</guid>
      <pubDate>Fri, 14 Feb 2025 13:01:45 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow的性能问题</title>
      <link>https://stackoverflow.com/questions/79437180/tensorflow-perfomance-issue</link>
      <description><![CDATA[我正在尝试在TensorFlow上进行非常原始的增强学习模型。尽管它相对较小，但单个迭代需要约6-7秒。
  def build_model（）：
    模型= keras。
        层。输入（shape =（400，）），
        layers.dense（128，激活=; relu; quot;），，
        layers.dense（128，激活=; relu; quot;），，
        层。密度（3）
    ）））
    model.compile（优化器= keras.optimizers.adam（Learning_rate = 0.001），Loss =＆quot; quot;
    返回模型

dqnagent类：
    def __init __（自我）：
        self.model = build_model（）
        self.target_model = build_model（）
        self.target_model.set_weights（self.model.get_weights（））

        self.memory = Deque（Maxlen = 1000）
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.gamma = 0.95
        self.batch_size = 32

    def select_action（self，state）：
        如果np.random.rand（）＆lt; self.epsilon：
            返回随机选择（[0，1，2]）
        q_values = self.model.model.predict（np.array（[state]），详细= 0）
        返回np.argmax（q_values [0]）

    def记住（自我，状态，行动，奖励，next_state，完成）：
        self.memory.append（（（状态，行动，奖励，next_state，Done完成）））

    def火车（自我）：
        如果Len（self.memory）＆lt; self.batch_size：
            返回
        batch =随机。样本（self.memory，self.batch_size）
        状态，目标= []，[]

        对于状态，行动，奖励，next_state，在批处理中完成：
            目标=奖励
            如果没有完成：
                target += self.gamma * np.max（self.target_model.predict（np.array（[[next_state]）），verbose = 0））

            q_values = self.model.model.predict（np.array（[state]），详细= 0）
            q_values [0] [action] =目标

            states.append（状态）
            targets.append（q_values [0]）

        self.model.fit（np.Array（states），np.Array（targets），Epochs = 1，冗长= 0）

        如果self.epsilon＆gt; self.epsilon_min：
            self.epsilon *= self.epsilon_decay

    def Update_target_model（self）：
        self.target_model.set_weights（self.model.get_weights（））
 
分析了所有给定代码后，我看到 model.predict（）花费了很多时间来完成：
 profiler result   
最初，我以为我只需要在GPU上计算，但是两天后尝试这样做，没有真正改变。
真的花了很多时间，还是我在代码中搞砸了？
  gpu：geforce 2060，
CPU：Intel Core i7， 
Windows 11，
Python：3.10
TensorFlow：2.10
 ]]></description>
      <guid>https://stackoverflow.com/questions/79437180/tensorflow-perfomance-issue</guid>
      <pubDate>Thu, 13 Feb 2025 17:07:09 GMT</pubDate>
    </item>
    <item>
      <title>Tweedie回归：功率> = 2'“ Y的某些值超出了损失的有效范围”，但Y值不是</title>
      <link>https://stackoverflow.com/questions/79437039/tweedie-regression-power-2-some-values-of-y-are-out-of-the-valid-range-o</link>
      <description><![CDATA[我正在运行Tweedie回归，对于Powers＆gt; = 2，我遇到了一个错误，告诉我我的y值超出了半weedeieloss的范围。我了解Y损失的有效范围为0。  我所有的y值都是＆gt; 0 and＆lt; 1，但我仍然会遇到这个错误。我不知道为什么。
 Sklearn版本1.3.0 
 i消除了所有行，并用y＆lt; = 0的值和double检查了一个描述。我期望回归器合适，并给我一个更好的理由，尤其是因为我的y值都大于0。我知道伽玛不是我的数据的出色分配，但我希望尝试尝试power = 3（逆高斯），这也是不可能的。
 power = 0和1都可以正常工作（正常和泊松）。
这是我培训y数据的描述（ cv_y ）：
 计数|   616420.000000  
平均|        0.955883  
std |        0.021402  
最小|        0.700465  
25％|        0.937018  
50％|        0.954769  
75％|        0.975716  
最大|        0.990000  
 
这是我的代码的重要元素
  glr = tweedieregressor（）＃广义线性回归模型
x_pipeline = pipeline（[（（＆quot; preprocessor&#39;&#39;，x_transformer），（&#39;Model; quode; glr）]）
esteNator = transformedTargetRegressor（recressor = x_pipeline，变压器= y_transformer）
家庭=“ Tweedie”
链接=“自动”
n_splits = 5
tscv = timeseriessplit（gap = 20，n_splits = n_splits）

param_grid = {
        &#39;recressor__preprocessor__x_pca__whiten&#39;：[true，false]，
        “回归器__model__power”：[0,1,2]，，
        &#39;Recressor__model__alpha&#39;：[0.5]，，，
        &#39;recressor__model__fit_intercept&#39;：[true]，
        “回归器__model__link”：[link]，，，
        &#39;Recressor__model__solver&#39;：[&#39;Newton-Cholesky&#39;]，
        &#39;Recressor__model__max_iter&#39;：[5,10]，，
        &#39;Recressor__model__tol&#39;：[1E-5]，，
        “回归器__model__verbose”：[1]
}

GS = GridSearchCV（
            估算器=估算器，
            param_grid = param_grid，
            得分=得分，
            n_jobs = -1，
            refit = reutit_strategy，
            CV = TSCV，
            详细= 3，
            pre_dispatch = 10，
            ERROR_SCORE =&#39;RAIND&#39;
）

型号= gs.fit（cv_x，cv_y）



 ]]></description>
      <guid>https://stackoverflow.com/questions/79437039/tweedie-regression-power-2-some-values-of-y-are-out-of-the-valid-range-o</guid>
      <pubDate>Thu, 13 Feb 2025 16:29:26 GMT</pubDate>
    </item>
    <item>
      <title>保存到磁盘中的磁盘时会遇到Unicode错误</title>
      <link>https://stackoverflow.com/questions/79436672/getting-unicode-error-while-saving-to-disk-in-distiset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79436672/getting-unicode-error-while-saving-to-disk-in-distiset</guid>
      <pubDate>Thu, 13 Feb 2025 15:04:10 GMT</pubDate>
    </item>
    <item>
      <title>Yolov9e-Seg在6 A100-80G上进行培训，并试图尽可能优化，但是在验证阶段之后，CUDA出现了失误错误</title>
      <link>https://stackoverflow.com/questions/79436107/yolov9e-seg-training-on-6-a100-80g-and-tried-to-optimize-as-much-as-i-could-but</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79436107/yolov9e-seg-training-on-6-a100-80g-and-tried-to-optimize-as-much-as-i-could-but</guid>
      <pubDate>Thu, 13 Feb 2025 12:12:17 GMT</pubDate>
    </item>
    <item>
      <title>如何解决“ RuntimeRorr：张量A（64）的大小必须匹配非辛格尔顿尺寸1”的张量B（6）？</title>
      <link>https://stackoverflow.com/questions/79429107/how-can-i-resolve-runtimeerror-the-size-of-tensor-a-64-must-match-the-size-o</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79429107/how-can-i-resolve-runtimeerror-the-size-of-tensor-a-64-must-match-the-size-o</guid>
      <pubDate>Tue, 11 Feb 2025 06:48:18 GMT</pubDate>
    </item>
    <item>
      <title>从GPU内存中清除tf.data.dataset</title>
      <link>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</guid>
      <pubDate>Fri, 07 Feb 2025 12:02:12 GMT</pubDate>
    </item>
    <item>
      <title>从简历中的R平方比XGBoost中的CV高[关闭]</title>
      <link>https://stackoverflow.com/questions/79320673/r-squared-from-cv-is-way-higher-than-without-cv-in-xgboost</link>
      <description><![CDATA[我有XGBoost模型的此代码：
  temp_dataset_pos = dataset_vehicle_pos.copy（）
＃temp_dataset_pos = temp_dataset_pos.drop（[&#39;sum_passenger&#39;]，axis = 1）
＃temp_dataset_pos = temp_dataset_pos [[&#39;湿度&#39;，&#39;温度&#39;，&#39;warter_avg_speed_pos&#39;]]]

x_pos = temp_dataset_pos.drop（&#39;warter_avg_speed_pos&#39;，axis = 1）
y_pos = temp_dataset_pos [&#39;warter_avg_speed_pos&#39;]

＃将数据分为火车和测试集
x_train，x_test，y_train，y_test = train_test_split（x_pos，y_pos，test_size = 0.20，andury_state = 42）

＃创建XGBoost模型
xgboost_model = xgb.xgbregressor（objective =&#39;reg：squaredError&#39;，andury_state = 42）＃您可以调整HyperParameters

＃适合模型
xgboost_model.fit（x_train，y_train）
＃确保预测的数据与测试集对齐
x_test [&#39;warter_count_pos&#39;] = prediction_vehicle_count_pos [&#39;predicted_vehicle_count_pos&#39;]

＃对测试集进行预测
y_pred_xgboost = xgboost_model.predict（x_test）

＃将预测的值舍入整数
y_pred_rounded = y_pred_xgboost.Round（）。astype（int）

＃评估模型
mse_xgboost = mean_squared_error（y_test，y_pred_rounded）
打印（f&#39;mean Squared错误（XGBOOST）：{MSE_XGBOOST}&#39;）
r2_xgboost = r2_score（y_test，y_pred_rounded）
打印（&#39;r平方值：&#39;，r2_xgboost）
重要性= xgboost_model.feature_importances_


＃创建XGBoost模型
xgboost_model = xgb.xgbregressor（objective =&#39;reg：squaredError&#39;，andury_state = 42）＃您可以调整HyperParameters

＃定义k折的交叉验证器
kf = kfold（n_splits = 5，shuffle = true，andury_state = 42）

＃根据平方误差定义得分手
mse_scorer = make_scorer（mean_squared_error，greate_is_better = false）

＃执行交叉验证并计算MSE
mse_scores = cross_val_score（xgboost_model，x_pos，y_pos，评分= mse_scorer，cv = kf）

＃将MSE分数转换为正； cross_val_score返回&#39;greate_is_better = false的负值&#39;
MSE_SCORES = -MSE_SCORES

＃计算MSE的平均值和标准偏差
MANE_MSE = np.Mean（MSE_SCORES）
std_mse = NP.STD（MSE_SCORES）

print（f&#39;mean MSE来自CV：{mean_mse}&#39;）
print（cv：{std_mse} \ n&#39;）

＃如果您还想在结果摘要中包含R平方
r2_scorer =&#39;r2&#39;
r2_scores = cross_val_score（xgboost_model，x_pos，y_pos，评分= r2_scorer，cv = kf）

平均_r2 = np.mean（r2_scores）
std_r2 = np.std（r2_scores）

print（f&#39;mean r平方从CV：{mean_r2}&#39;）
print（f&#39;std r平方从cv：{std_r2}&#39;）
打印（&#39;\ n&#39;）
 
我得到了这些结果：
平均误差（XGBoost）：395.3362869635255 
 R平方值：0.37072522417952525  
 CV的平均MSE：6.902625450153782 
CV的STD MSE：0.8885273860917627 
平均R平方从CV：0.990070278719086 &lt; /strong&gt; 
CV的STD R平方：0.000623638887270274 
 R平方如何在有或没有简历的情况下如此不同？
我是否缺少代码中的东西？]]></description>
      <guid>https://stackoverflow.com/questions/79320673/r-squared-from-cv-is-way-higher-than-without-cv-in-xgboost</guid>
      <pubDate>Tue, 31 Dec 2024 19:21:35 GMT</pubDate>
    </item>
    <item>
      <title>在CIFAR-10上的Keras中实施Alexnet的准确性差</title>
      <link>https://stackoverflow.com/questions/51403712/implementation-of-alexnet-in-keras-on-cifar-10-gives-poor-accuracy</link>
      <description><![CDATA[我尝试实现视频。请原谅我，如果我实施了错误，这是我在keras中实现的代码。
编辑：cifar-10 imagedatagenerator 
  cifar_generator = imagedatagenerator（）

cifar_data = cifar_generator.flow_from_directory（&#39;dataSets/cifar-10/train&#39;， 
                                                 batch_size = 32， 
                                                 target_size = input_size， 
                                                 class_mode =&#39;分类&#39;）
 
 Keras中描述的模型：

在

ADD（卷积2d（过滤器= 96，kernel_size =（11，11），input_shape =（227，227，3），步幅= 4，激活=&#39;relu&#39;））））
model.Add（maxpool2d（pool_size =（3，3），步幅= 2））

ADD（卷积2d（过滤器= 256，kernel_size =（5，5），步幅= 1，padding =&#39;same&#39;，activation =&#39;relu&#39;））
model.Add（maxpool2d（pool_size =（3，3），步幅= 2））

ADD（卷积2d（过滤器= 384，kernel_size =（3，3），步幅= 1，填充=&#39;same&#39;，activation =&#39;relu&#39;））））
ADD（卷积2d（过滤器= 384，kernel_size =（3，3），步幅= 1，填充=&#39;same&#39;，activation =&#39;relu&#39;））））
model.Add（卷积2d（滤波器= 256，kernel_size =（3，3），步幅= 1，padding =&#39;same&#39;，activation =&#39;relu&#39;））

model.Add（maxpool2d（pool_size =（3，3），步幅= 2））

模型add（Flatten（））
model.Add（密集（单位= 4096））
model.Add（密集（单位= 4096））
model.Add（密度（单位= 10，activation =&#39;softmax&#39;））

model.compile（优化器=&#39;adam&#39;，loss =&#39;apcorical_crossentropopy&#39;，量表= [&#39;准确性&#39;]）
 
我已经使用Imagedatagenerator在CIFAR-10数据集上训练该网络。但是，我只能获得大约.20的准确性。我无法弄清楚我做错了什么。]]></description>
      <guid>https://stackoverflow.com/questions/51403712/implementation-of-alexnet-in-keras-on-cifar-10-gives-poor-accuracy</guid>
      <pubDate>Wed, 18 Jul 2018 13:47:55 GMT</pubDate>
    </item>
    <item>
      <title>返回标签及其在Sklearn LabelenCoder中的编码值</title>
      <link>https://stackoverflow.com/questions/48938905/return-the-labels-and-their-encoded-values-in-sklearn-labelencoder</link>
      <description><![CDATA[我正在使用  labelencoder  和  oneHotEncoder       sklearn  在机器学习项目中，用于编码数据集中的标签（国家名称）。一切都很好，我的模型运行得很好。该项目是根据包括客户国家在内的多个功能（数据）来对银行客户的持续或离开银行的分类。 
当我想预测（分类）新客户（仅一个）时，我的问题就会出现。新客户的数据仍未预处理（即，未编码国家名称）。如下：
  new_customer = np.array（[[[&#39;france&#39;，600，&#39;男性&#39;，40，3，60000，2，1,1，50000]]）
 
在我学习机器学习的在线课程中，讲师打开了包括编码数据的预处理数据集， 手动   检查了法国的代码并更新了它在 new_customer 中，如下：
  new_customer = np.Array（[[[0，0，600，&#39;Male&#39;，40，3，60000，2，1,1，50000]]）
 
我相信这是不切实际的，必须有一种方法可以自动编码法国在原始数据集中使用的相同代码，或者至少是返回国家列表及其编码值的方法。手动编码标签似乎乏味且容易出错。那么，如何自动化此过程或生成标签代码？提前致谢。 ]]></description>
      <guid>https://stackoverflow.com/questions/48938905/return-the-labels-and-their-encoded-values-in-sklearn-labelencoder</guid>
      <pubDate>Thu, 22 Feb 2018 23:28:02 GMT</pubDate>
    </item>
    </channel>
</rss>