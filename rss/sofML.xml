<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 14 Jan 2025 18:21:50 GMT</lastBuildDate>
    <item>
      <title>我需要训练一个多类模型，但我的数据集很小</title>
      <link>https://stackoverflow.com/questions/79355992/i-need-to-train-a-multiclass-model-but-i-have-a-small-dataset</link>
      <description><![CDATA[所以...我有一个包含两列的 excel 文件，一列包含短语之类的文本，另一列告诉我从“CS1”到“CS8”的分类。文本类似于“NE PAGTO PROVENTOS APOSENTADORIA ESPECIAL SERVIDORES SAÚDE, NOV/2024. REF. FATURA 033/2024. INCLUI REFORMA DE ESCOLAS。”。我已经对其他文件进行了清理，总共该文件有 72 个文本，df.shape = (72, 2)
我是机器学习的新手，准确率一直低于 50%。有人能帮我吗？
文件 clean_text.py：
导入 re

def clean_text(text)：
text = re.sub(r&#39;\d{1,4}/\d{4}&#39;, &#39;&#39;, text)
text = re.sub(r&#39;\d+&#39;, &#39;&#39;, text)
text = re.sub(r&#39;[^\w\s]&#39;, &#39;&#39;, text)
text = text.lower()
返回文本

文件 main.py：
导入 numpy 作为 np
导入 tensorflow 作为 tf
从 tensorflow.keras.models 导入 Sequential
从 tensorflow.keras.layers 导入 Dense、Dropout、Input
从 tensorflow.keras.optimizers 导入 Adam
从 sklearn.feature_extraction.text 导入 TfidfVectorizer
从sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 LabelEncoder
从 sklearn.metrics 导入 classification_report、accuracy_score
从 transformers 导入 TFAutoModel、AutoTokenizer
导入 joblib
导入 pandas 作为 pd
从 nltk.corpus 导入 stopwords
导入 re
从 clean_text 导入 clean_text

df = pd.read_excel(&quot;DADOS PARA CLASSIFICAÇÃO MULTICLASSE.xlsx&quot;, sheet_name=&quot;TREINAMENTO&quot;)
df[&#39;EMPENHO&#39;] = df[&#39;EMPENHO&#39;].apply(clean_text)
descriptions = df[&#39;EMPENHO&#39;].tolist()
labels = df[&#39;CLASSE SINTETICA&#39;].tolist()

print(f&quot;Amostras: {df.shape}&quot;)

label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels)

vect = TfidfVectorizer()
X = vect.fit_transform(descriptions).toarray()

X_train, X_test, y_train, y_test = train_test_split(X, labels_encoded, test_size=0.2, random_state=42)

model = Sequential([
Input(shape=(X_train.shape[1],)),
Dense(128,activation=&#39;relu&#39;),
Dropout(0.3),
Dense(64,activation=&#39;relu&#39;),
Dropout(0.3),
Dense(len(label_encoder.classes_),activation=&#39;softmax&#39;)
])

model.compile(optimizer=Adam(learning_rate=1e-4), loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

accuracy = 0
而accuracy &lt; 0.90:
print(&quot;训练模型...&quot;)
emp_train = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2, verbose=0)

y_pred = np.argmax(model.predict(X_test), axis=-1)
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;准确率：{accuracy * 100:.2f}%&quot;)

joblib.dump(vect, &quot;vectorizer.pkl&quot;)
joblib.dump(label_encoder, &quot;label_encoder.pkl&quot;)
model.save(&quot;empenho_model.keras&quot;)

print(&quot;训练模型并计算结果成功了！”）


我试过使用 bert 和 pytorch，但这种方式对我来说更好。]]></description>
      <guid>https://stackoverflow.com/questions/79355992/i-need-to-train-a-multiclass-model-but-i-have-a-small-dataset</guid>
      <pubDate>Tue, 14 Jan 2025 18:16:11 GMT</pubDate>
    </item>
    <item>
      <title>理解经验风险与真实风险中的泛化误差</title>
      <link>https://stackoverflow.com/questions/79355658/understanding-generalization-error-in-empirical-vs-true-risk-illustration</link>
      <description><![CDATA[我试图根据附图来理解泛化误差的概念，该图对比了经验风险 (𝑅_hat) 和真实风险 (𝑅)
在图中标记了两个区域：

红色阴影区域：经验风险 (𝑅_hat) 高于真实风险 (R) 的区域；
紫色阴影区域：经验风险 (R_hat) 低于真实风险 (R) 的区域；

根据我的理解，泛化误差随着 𝑤_hat_D（经验风险的最小化器）接近 𝑤_star（真实风险的最小化器）而减小风险）。
考虑到这一点：

将红色阴影区域解释为表示模型过度拟合的趋势（因为 R_hat &gt; R）是否正确？


同样，紫色阴影区域是否反映了模型欠拟合的趋势（因为 𝑅_hat &lt; 𝑅）？

对于此解释的任何见解或更正，我将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79355658/understanding-generalization-error-in-empirical-vs-true-risk-illustration</guid>
      <pubDate>Tue, 14 Jan 2025 16:20:00 GMT</pubDate>
    </item>
    <item>
      <title>加载 MIT-BIH 心律失常数据库（无需 wfdb 包）</title>
      <link>https://stackoverflow.com/questions/79355429/load-mit-bih-arrhythmia-database-without-wfdb-package</link>
      <description><![CDATA[我打算使用这个数据集，但无论我如何努力搜索，我都找不到文件中数据的格式或结构。我发现的最重要的事情是每隔两个字节就有一个 h33。我在哪里可以找到有关此内容的文档？我找不到注释与这些数据文件的关系。]]></description>
      <guid>https://stackoverflow.com/questions/79355429/load-mit-bih-arrhythmia-database-without-wfdb-package</guid>
      <pubDate>Tue, 14 Jan 2025 15:00:07 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 PyTorch 中保存 ViT 模型时不会生成 constants.pkl，在边缘设备上部署时是否需要它？</title>
      <link>https://stackoverflow.com/questions/79355096/why-is-constants-pkl-not-generated-when-saving-a-vit-model-in-pytorch-and-is-it</link>
      <description><![CDATA[我训练了一个 Vision Transformer (ViT) 模型进行分类，并使用以下 PyTorch 代码保存了该模型：
torch.save(model, &quot;vit_model.pth&quot;)
当我尝试将保存的模型集成到 Android 应用程序中时，我在运行时遇到了以下错误：
无法启动活动 ComponentInfo{com.test.package/com.test.package.MainActivity}：java.lang.RuntimeException：com.facebook.jni.CppException：PytorchStreamReader 无法定位文件 constants.pkl：找不到文件
我所做的：

我将 .pth 模型文件转换为 zip 文件以检查其内容，我注意到文件中不存在 constants.pkl。
我搜索了有关constants.pkl，但我找不到关于为什么它没有生成或它在这种情况下的作用的明确解释。

我的问题：

为什么使用 torch.save() 保存 PyTorch 模型时没有生成 constants.pkl？
在边缘设备（例如 Android）上部署模型是否需要 constants.pkl？
如果不需要 constants.pkl，我该如何将我的模型集成到 Android 应用程序中而不会遇到此错误？
如果需要 constants.pkl，我该如何生成它或修改我的模型保存过程以包含它？

其他信息：

模型：在 PyTorch 中训练的 Vision Transformer (ViT)。
Android 集成：使用PyTorch Android 库。
应用程序尝试加载模型文件时似乎出现错误。

我需要什么：

明确解释 constants.pkl 的作用以及它是否是 Android 上 PyTorch 模型部署的必需文件。

正确保存模型并将其集成到 Android 应用程序中以避免此问题的步骤或代码示例。


提前感谢您的任何指导或建议！]]></description>
      <guid>https://stackoverflow.com/questions/79355096/why-is-constants-pkl-not-generated-when-saving-a-vit-model-in-pytorch-and-is-it</guid>
      <pubDate>Tue, 14 Jan 2025 13:07:19 GMT</pubDate>
    </item>
    <item>
      <title>使用视觉变换器进行图像分类的准确率较低</title>
      <link>https://stackoverflow.com/questions/79354623/low-accuracy-using-vision-transformers-for-image-classification</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79354623/low-accuracy-using-vision-transformers-for-image-classification</guid>
      <pubDate>Tue, 14 Jan 2025 10:07:15 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何工具可以将任何其他数据集格式转换为 sam2 格式以进行微调？[关闭]</title>
      <link>https://stackoverflow.com/questions/79354345/are-there-any-tools-to-convert-from-any-other-dataset-format-to-sam2-format-for</link>
      <description><![CDATA[我在 cvat 中有一个标记的数据集，但 cvat 不支持将数据卸载为 SAM2 格式。有没有可用的自动转换工具？
我可以使用 roboflow，但我有无法上传到第三方资源的数据]]></description>
      <guid>https://stackoverflow.com/questions/79354345/are-there-any-tools-to-convert-from-any-other-dataset-format-to-sam2-format-for</guid>
      <pubDate>Tue, 14 Jan 2025 08:14:24 GMT</pubDate>
    </item>
    <item>
      <title>stable_baselines3：为什么比较 ep_info_buffer 与评估时奖励不匹配？</title>
      <link>https://stackoverflow.com/questions/79353843/stable-baselines3-why-the-reward-does-not-match-comparing-ep-info-buffer-vs-eva</link>
      <description><![CDATA[我正在使用 stable_baselines3 库，这时我发现了一些意想不到的东西。
这里有一个简单的代码来重现这个问题：
import gymnasium as gym

from stable_baselines3 import DQN

env = gym.make(&quot;CartPole-v1&quot;)

model = DQN(&quot;MlpPolicy&quot;, env, verbose=0, stats_window_size=100_000)
model.learn(total_timesteps=100_000)

看看最后一集的奖励：
print(model.ep_info_buffer[-1])


{&#39;r&#39;: 409.0, &#39;l&#39;: 409, &#39;t&#39;: 54.87983

但是如果我使用以下代码评估模型：
obs, info = env.reset()
total_reward = 0
while True:
action, _states = model.predict(obs, deterministic=True)
obs, reward, termed, truncated, info = env.step(action)
total_reward = total_reward + reward
if termed or truncated:
obs, info = env.reset()
break

print(&quot;total_reward {}&quot;.format(total_reward))


total_reward 196.0

我得到了不同的奖励，这是我没有预料到的。
我预计会得到与 409 相同的奖励model.ep_info_buffer[-1]。
为什么会有这种差异？.ep_info_buffer 与每集奖励不同吗？]]></description>
      <guid>https://stackoverflow.com/questions/79353843/stable-baselines3-why-the-reward-does-not-match-comparing-ep-info-buffer-vs-eva</guid>
      <pubDate>Tue, 14 Jan 2025 02:14:32 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练我的音频分类模型时出现错误 as_list()</title>
      <link>https://stackoverflow.com/questions/79353105/error-as-list-when-trying-to-train-my-audio-classification-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79353105/error-as-list-when-trying-to-train-my-audio-classification-model</guid>
      <pubDate>Mon, 13 Jan 2025 18:12:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在 pytorch 中并行计算不同权重和输入的神经网络？</title>
      <link>https://stackoverflow.com/questions/79350425/how-to-calculate-neural-net-for-different-weights-and-inputs-in-parallel-in-pyto</link>
      <description><![CDATA[我正在使用 pytorch 进行机器学习。
有一些实现神经网络的类继承自 nn.Module。实现了一些网络结构。
列表中存储了不同的参数（权重、偏差）：
parameters = [[...], ..., [...]] 。另一个列表中还存储了不同的输入：inputs = [[...], ..., [...]]。所有输入集的大小和维度都相同。
我需要计算输入和参数的网络输出，即我需要构建一个矩阵：




参数[0]
参数[1]
...
参数[n]




输入[0]
结果[0][0]
结果[0][1]
...
结果[0][n]


输入[1]
结果[ 1][0]
result[1][1]
...
result[1][n]


...
...
...
...
...


input[m]
result[m][0]
result[m][1]
...
result[m][n]



最明显的方法是使用两个循环。这对 CPU 来说很好。但如何为 GPU 实现它？每个 result[i][j] 都可以并行计算，所以我会使用某种批量计算。
你能给我一些解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/79350425/how-to-calculate-neural-net-for-different-weights-and-inputs-in-parallel-in-pyto</guid>
      <pubDate>Sun, 12 Jan 2025 18:00:35 GMT</pubDate>
    </item>
    <item>
      <title>如何检查随机森林模型是否过度拟合？</title>
      <link>https://stackoverflow.com/questions/79327542/how-to-check-if-the-model-is-overfitting-for-random-forest</link>
      <description><![CDATA[我已经为数据集实现了随机森林，并且平衡了数据，我使用了 80-10-10、70-15-15、60-20-20 和 80-20 方法。我还使用了特征重要性，并在 41 个独立特征中使用了 10 个 imp 特征、15 个 imp 特征、24 个 imp 特征和 34 个 imp 特征。所有上述方法的平均召回率为 95.8%，平均准确率为 96.6%，精确率为 97%。交叉验证召回率（我主要关注召回率）为 95.5%。
我使用训练数据对训练数据本身进行预测，得到了 99.8%
我还使用了热图并删除了 3 个高度相关的特征，但我得到了 80-10-10 的相同分数（热图之后）。
我的模型是否过度拟合？如何检查是否过度拟合？]]></description>
      <guid>https://stackoverflow.com/questions/79327542/how-to-check-if-the-model-is-overfitting-for-random-forest</guid>
      <pubDate>Fri, 03 Jan 2025 19:56:33 GMT</pubDate>
    </item>
    <item>
      <title>DMIR 的 XMorpher 模型 - 数据集问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79180321/xmorpher-model-for-dmir-dataset-problem</link>
      <description><![CDATA[我正在尝试运行 GitHub 项目 XMorpher (https://github.com/Solemoon/XMorpher/tree/main)，但作者没有提供正确的数据集？他在代码中使用了：
 train_labeled_unlabeled_dir = &#39;data/train_labeled_unlabeled&#39;
train_unlabeled_unlabeled_dir = &#39;data/train_unlabeled_unlabeled&#39;
test_labeled_labeled_dir = &#39;data/test&#39;

但实际上数据文件夹仅包含 0_1.mat 文件，在 Matlab 中打开时，该文件包含 2 个变量：
fix_img # 大小：144x144x128 (int16)
mov_img # 大小：144x144x128 (int16)

由于一张图片包含 5308416 个字节，我无法看到变量的值... 还有其他方法可以运行此模型或我可以使用其他数据集吗？]]></description>
      <guid>https://stackoverflow.com/questions/79180321/xmorpher-model-for-dmir-dataset-problem</guid>
      <pubDate>Tue, 12 Nov 2024 08:25:50 GMT</pubDate>
    </item>
    <item>
      <title>Vertex AI：Automl-tabular 模板不断给我一个错误</title>
      <link>https://stackoverflow.com/questions/79177501/vertex-ai-automl-tabular-template-keeps-giving-me-an-error</link>
      <description><![CDATA[我正在尝试使用 Google 的 AutoML 产品 (VertexAI) 构建机器学习模型。
我已成功上传我的数据集 - 见下图。

但是，当我尝试使用 AutoML 模板为表格回归创建管道运行时，管道失败。我将在 VertexAI 上展示步骤，我只是使用默认设置而不进行任何更改：





我运行的第一个管道失败了。


我将调试 json 粘贴到 ChatGPT 中。它告诉我尝试将机器类型从 n1-standard-8 或 n1-highmem-8 更改为 n1-standard-4。我试过了，但管道仍然失败。我还确保计算服务已启用正确的设置。
]]></description>
      <guid>https://stackoverflow.com/questions/79177501/vertex-ai-automl-tabular-template-keeps-giving-me-an-error</guid>
      <pubDate>Mon, 11 Nov 2024 11:41:07 GMT</pubDate>
    </item>
    <item>
      <title>使用带有 max_new_tokens 的 LLM 进行不完整输出</title>
      <link>https://stackoverflow.com/questions/77061898/incomplete-output-with-llm-with-max-new-tokens</link>
      <description><![CDATA[我正在试验 Huggingface LLM 模型。
我注意到的一个问题是模型的输出突然结束，我理想情况下希望它完成它所在的段落/句子/代码。（或者尝试在某个固定数量的标记内完成答案）
虽然我提供了 max_new_tokens = 300，并且在提示中我写道：
“输出最多应为 300 个字。”
响应总是不完整的，并且突然结束。有什么方法可以要求在所需的输出标记数内完成输出？
代码：
checkpoint = “HuggingFaceH4/starchat-alpha”
device = “cuda” if torch.cuda.is_available() else “cpu”
class StarCoderModel:
def __init__(self):
self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)
# 如果需要 gpu，请确保在 docker run 命令中提供 `--gpus all`
self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=&#39;auto&#39;)

def infer(self, input_text, token_count):
输入 = self.tokenizer.encode(input_text, return_tensors=&quot;pt&quot;).to(device)
输出 = self.model.generate(inputs, max_new_tokens=token_count, pad_token_id=self.tokenizer.eos_token_id)
return self.tokenizer.decode(outputs[0])[len(input_text):]

示例输出：
private DataType FuntionName(String someId) {
// TODO：用利用 someId 获取信息的实现替换
return DataType.Value;
}

注释：

- 如果代码中存在 someId，则使用客户端的 getAPI 并以 someId 作为参数来获取一些信息。
- 如果

]]></description>
      <guid>https://stackoverflow.com/questions/77061898/incomplete-output-with-llm-with-max-new-tokens</guid>
      <pubDate>Thu, 07 Sep 2023 18:02:00 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们不应该在同一层使用多个激活函数？[关闭]</title>
      <link>https://stackoverflow.com/questions/63125782/why-shouldnt-we-use-multiple-activation-functions-in-the-same-layer</link>
      <description><![CDATA[我见过的所有应用神经网络的示例或案例都有一个共同点 - 它们在属于特定层的所有节点中使用特定类型的激活函数。
据我所知，每个节点都使用非线性激活函数来了解数据中的特定模式。如果是这样，为什么不使用多种类型的激活函数？
我确实找到了一个链接，它基本上说如果我们每层只使用一个激活函数，管理网络会更容易。还有其他好处吗？]]></description>
      <guid>https://stackoverflow.com/questions/63125782/why-shouldnt-we-use-multiple-activation-functions-in-the-same-layer</guid>
      <pubDate>Tue, 28 Jul 2020 01:28:55 GMT</pubDate>
    </item>
    <item>
      <title>机器学习还是决策树用于工作匹配？</title>
      <link>https://stackoverflow.com/questions/43319120/machinelearning-or-decisiontree-for-job-matching</link>
      <description><![CDATA[我正在开发一个工作匹配应用程序，我想知道在元素之间进行匹配以获得最佳结果的最佳方法是什么？
在我看来，这是通过决策树，因为我们已经知道元素的结构和预期结果。
但是，机器学习会是一种替代解决方案吗？或者这样做毫无价值？
我可能错了，但对我来说，机器学习对于对乍一看没有明显共同点的数据进行排序是有效的，对吗？
谢谢你的建议！]]></description>
      <guid>https://stackoverflow.com/questions/43319120/machinelearning-or-decisiontree-for-job-matching</guid>
      <pubDate>Mon, 10 Apr 2017 09:10:48 GMT</pubDate>
    </item>
    </channel>
</rss>