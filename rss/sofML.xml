<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 11 Sep 2024 21:15:52 GMT</lastBuildDate>
    <item>
      <title>Ray 自定义环境渲染</title>
      <link>https://stackoverflow.com/questions/78975679/ray-custom-environment-render</link>
      <description><![CDATA[我正在创建自己的 gym 环境来测试 freeze-tag 问题。我正在尝试使用 Ray 来做 MAPPO。我有两个问题：
1：我的模拟没有渲染
2：它创建了多个 PyGame 窗口
我已将渲染方法和训练脚本的片段附加到附件中。
# 渲染函数
def render(self):
self.screen.fill((255, 255, 255))

for agent in self.all_agents:
if agent.status == 1:
pygame.draw.circle(self.screen, agent.color, (agent.x, agent.y), agent.size)

elif agent.status == 0:
pygame.draw.circle(self.screen, (0, 255, 255), (agent.x, agent.y),agent.size)

pygame.display.flip()

# Train_MAPPO_FTP.py
import ray
from ray.rllib.algorithms.ppo import PPOConfig
from ray.tune.registry import register_env
import gym_FTP as e
import pygame
import numpy as np

# 环境创建函数
def env_creator(config):
robots = 5
adversaries = 2
time_steps = 500

screen = pygame.display.set_mode([1000, 1000])
gym_ftp = e.gym_FTP(screen, robots, 0, adversaries, time_steps, 15)
return gym_ftp

def train_and_evaluate(time_steps):
# 初始化 Ray
ray.init(ignore_reinit_error=True)

# 注册环境
register_env(&quot;Env_FTP&quot;, env_creator)

# create_env_on_local_worker = True
# 配置算法
config = PPOConfig() \
.environment(&quot;Env_FTP&quot;) \
.rollouts(num_rollout_workers=1,
rollout_fragment_length=1,
create_env_on_local_worker=True) \
.training(
train_batch_size=1, # 每次训练更新前汇总经验
sgd_minibatch_size=1,
model={&quot;fcnet_hiddens&quot;: [64, 64]}
) \
.framework(&quot;torch&quot;) \
.evaluation(evaluation_num_workers=1) \
.resources(num_gpus=0) # 设置 GPU 数量

# 构建算法
algo = config.build()

# 参数
episodes = 5
iterations = time_steps / 10

for episode in range(episodes):
for i in range(int(iterations)):
results = algo.train()
print(f&quot;训练迭代 {i + 1} 已完成。mean_reward {results[&#39;episode_reward_mean&#39;]},&quot;
f&quot; 总损失 {results[&#39;info&#39;][&#39;learner&#39;][&#39;__all__&#39;][&#39;total_loss&#39;]}&quot;)

# 关闭 Ray
ray.shutdown()

def main():
time_steps = 500
train_and_evaluate(time_steps)

main()


我已进行多次检查，以测试我的代理的速度是否根据新操作进行更新，以及位置是否正在更新，因此我确定这不是问题所在。当我使用其他算法进行测试时，此环境也有效。我可以正确使用 gym 环境的其他功能，并让它渲染和做一些有趣的事情。这似乎完全是 RAY 的问题。我的目标是拥有 n 个机器人和 m 个对手。我想根据环境状态为 n 个代理获取新操作。我想每集训练 500 个时间步，收集 10 个批次。例如前 10 个时间步，然后再添加 10 个时间步作为经验，然后再添加 10 个。所以我们每集最多更新 50 次。我们将进行 100 集。]]></description>
      <guid>https://stackoverflow.com/questions/78975679/ray-custom-environment-render</guid>
      <pubDate>Wed, 11 Sep 2024 20:54:21 GMT</pubDate>
    </item>
    <item>
      <title>跳过多步预测框架的训练窗口和测试范围之间的时间点</title>
      <link>https://stackoverflow.com/questions/78975635/skipping-timepoints-between-the-training-window-and-testing-horizon-for-a-multi</link>
      <description><![CDATA[我正在使用 caret 包构建一个随机森林模型，使用多个预测因子来预测时间序列结果。更具体地说，我使用 createTimeSlices 函数创建不同时间顺序的训练和测试集：

但是，此函数似乎不允许跳过训练窗口（该函数的 initialWindow 参数）和测试窗口（该函数的 horizo​​n 参数）之间的时间观察，以便创建一个看起来像多步预测框架的框架Rob Hyndman 在此建议：

我知道同一函数中存在 skip 参数，但这只会操纵您在每对训练和测试时间片之间跳过的观察结果，而不是操纵每个训练和测试时间片本身之间跳过的观察结果。我如何跳过训练集和测试集之间的观察结果以生成多步预测框架？]]></description>
      <guid>https://stackoverflow.com/questions/78975635/skipping-timepoints-between-the-training-window-and-testing-horizon-for-a-multi</guid>
      <pubDate>Wed, 11 Sep 2024 20:40:33 GMT</pubDate>
    </item>
    <item>
      <title>“无监督”二值图像聚类：聚类效果看似不错，但结果却很糟糕？我做错了什么？</title>
      <link>https://stackoverflow.com/questions/78975401/unsupervised-binary-image-clustering-bad-results-despite-seemingly-good-clust</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78975401/unsupervised-binary-image-clustering-bad-results-despite-seemingly-good-clust</guid>
      <pubDate>Wed, 11 Sep 2024 19:16:38 GMT</pubDate>
    </item>
    <item>
      <title>FFN 模型在预测总和方面实现了 100% 的准确率</title>
      <link>https://stackoverflow.com/questions/78975293/ffn-model-achieving-100-accuracy-in-predicting-sums</link>
      <description><![CDATA[我有一个模型，可以对 -10 到正 10 之间的数字进行加法运算，但使用神经网络通过两个数字相加的数据集来预测结果。然而，在获得训练准确度时，它只是打印出很多 100% 的准确度。我不确定模型是否只是快速训练，或者是否存在问题并且没有正确学习。有人能提供一些见解吗？
这是我的代码
import torch
import torch.nn as nn
import torch.nn. functional as F
from torch.utils.data import DataLoader,TensorDataset
from sklearn.model_selection import train_test_split

import numpy as np

import matplotlib.pyplot as plt
import matplotlib_inline.backend_inline
matplotlib_inline.backend_inline.set_matplotlib_formats(&#39;svg&#39;)

data = []
labels = []

datasetAmount = 2000

for i in range(datasetAmount):
x = np.random.randint(-10, 10)
y = np.random.randint(-10,10)
bothNumber = [x,y]
data.append(bothNumber)
labels.append(x+y)

data_np = np.array(data)
labels_np = np.array(labels).reshape(-1,1)

train_data, test_data, train_labels, test_labels = train_test_split(data_np, labels_np, train_size =.9)

train_data = TensorDataset(torch.tensor(train_data),torch.tensor(train_labels))
test_data = TensorDataset(torch.tensor(test_data),torch.tensor(test_labels))

batchsize = 20

train_loader = DataLoader(train_data, batch_size = batchsize, shuffle = True, drop_last = True)
test_loader = DataLoader(test_data, batch_size = test_data.tensors[0].shape[0])

def createModel():
class myModel(nn.Module):
def __init__(self):
super().__init__()

self.input = nn.Linear(2,8)
self.fc1 = nn.Linear(8,8)
self.output = nn.Linear(8,1)

def forward(self,x):
x = F.relu( self.input(x) )
x = F.relu( self.fc1(x) )
return self.output(x)

net = myModel()
lossfun = nn.MSELoss()
optimizer = torch.optim.SGD(net.parameters(),lr=.001)

return net,lossfun,optimizer

def trainModel():

numepochs = 100
net,lossfun,optimizer = createModel()
loss = torch.zeros(numepochs)
trainacc = []
testacc = []

for epochi in range(numepochs):
batchLoss = []

for X,y in train_loader:
X = X.float()
y = y.float()
yHat = net(X)

loss = lossfun(yHat,y)
batchLoss.append(loss.item())

optimizer.zero_grad()
loss.backward()
optimizer.step()

loss[epochi] = np.mean(batchLoss)

with torch.no_grad():
train_predictions = []
train_labels = []
for x_train, y_train in train_loader:
x_train = x_train.float()
y_train = y_train.float()
train_pred = net(x_train)
train_predictions.append(train_pred)
train_labels.append(y_train)

train_predictions = torch.cat(train_predictions)
train_labels = torch.cat(train_labels)

train_acc = 100 * torch.mean((np.abs(train_predictions - train_labels) &lt; 1).float())
trainacc.append(train_acc.item())

X,y = next(iter(test_data))
X = X.float() # 将 X 转换为浮点数用于测试数据
y = y.float() # 将 y 转换为浮点数用于测试数据
with torch.no_grad():
yHat = net(X)

testacc= 100*torch.mean((np.abs(yHat-y)&lt; 1).float())

return trainacc,testacc,losses,net

trainAcc, testAcc, loss , net = trainModel()


模型有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78975293/ffn-model-achieving-100-accuracy-in-predicting-sums</guid>
      <pubDate>Wed, 11 Sep 2024 18:43:48 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的神经网络模型无法学习绝对函数 abs(x1-x2)？</title>
      <link>https://stackoverflow.com/questions/78974546/why-cant-my-neural-network-model-learn-absolute-function-absx1-x2</link>
      <description><![CDATA[我正在尝试训练一个简单的神经网络模型进行多类分类。
我有 x1、x2、x3、x4 列，其中有 4 个类别需要预测。
如果只对 x1、x2、x3、x4 进行训练，那么我的准确率是 88%
凭借一些领域知识，我可以创建三个新特征，我知道这肯定会帮助模型更好地训练。
这三个新特征是：-

df[&#39;x12&#39;] = abs (df[&#39;x1&#39;]-df[&#39;x2&#39;])
df[&#39;x13&#39;] = abs (df[&#39;x1&#39;]-df[&#39;x3&#39;])
df[&#39;x14&#39;] = abs (df[&#39;x1&#39;]-df[&#39;x4&#39;])

如果我在 x1、x2、x3、x4 和 abs(x1-x2)、abs(x1-x3)、abs(x1-x4) 上进行训练，那么我的准确率是 98%
我想在没有 abs(x1-x2)、abs(x1-x3)、abs(x1-x4) 的情况下获得 98% 的准确率
使用这些新的手动创建的特征，我获得了 98% 的验证准确率，这很棒。
但是，当我删除这些特征时，验证准确率会下降到 88%。
我的问题是函数 abs(x1-x2) 应该非常简单，足以让模型自行学习，而无需我手动进行特征工程。
那么为什么当我删除这三个（非常）时准确率会下降简单）特征？
模型是否没有足够的能力自行学习？
我尝试在模型中使用不同的激活函数。
从线性激活函数、relu 激活函数、leaky relu、prelu 开始。
但是它们都没有给我 98% 的准确率（不使用三个手动创建的特征）。
这是我的模型的样子：
def create_dense_model(input_shape, num_outputs, LR):

inputs = Input(shape=input_shape)

x = Dense(units=64)(inputs)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

x = Dense(units=64)(x)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

x = Dense(units=64)(x)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

x = Dense(units=64)(x)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

x = Dense(units=32)(x)
x = PReLU()(x) 
x = BatchNormalization()(x)
x = Dropout(rate=0.3)(x)

multiclass_output = Dense(units=num_outputs,activation=&#39;softmax&#39;)(x)

model = Model(inputs=inputs,outputs=multiclass_output)

model.compile(
loss=&quot;categorical_crossentropy&quot;,
metrics=[&quot;accuracy&quot;],
optimizer=Adam(learning_rate=LR)
)

返回模型
]]></description>
      <guid>https://stackoverflow.com/questions/78974546/why-cant-my-neural-network-model-learn-absolute-function-absx1-x2</guid>
      <pubDate>Wed, 11 Sep 2024 15:12:02 GMT</pubDate>
    </item>
    <item>
      <title>银行客户流失预测模型 - 预测能力建议 [关闭]</title>
      <link>https://stackoverflow.com/questions/78972707/bank-churn-prediction-model-advise-on-predictive-power</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78972707/bank-churn-prediction-model-advise-on-predictive-power</guid>
      <pubDate>Wed, 11 Sep 2024 08:12:37 GMT</pubDate>
    </item>
    <item>
      <title>如何在深度学习中计算真实世界物体的大小[关闭]</title>
      <link>https://stackoverflow.com/questions/78972330/how-calculate-real-world-object-size-in-deeplearning</link>
      <description><![CDATA[我正在做一个项目，遇到了一个麻烦，需要通过图片计算任何物体的真实世界大小，并且没有指定相机和距离的边界。
是否有任何深度学习模型可以为我计算和预测任何物体的真实世界大小？]]></description>
      <guid>https://stackoverflow.com/questions/78972330/how-calculate-real-world-object-size-in-deeplearning</guid>
      <pubDate>Wed, 11 Sep 2024 06:34:11 GMT</pubDate>
    </item>
    <item>
      <title>程序如何改变张量的大小？</title>
      <link>https://stackoverflow.com/questions/78972166/how-does-the-program-change-the-size-of-the-tensor</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78972166/how-does-the-program-change-the-size-of-the-tensor</guid>
      <pubDate>Wed, 11 Sep 2024 05:31:44 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pytorch 进行人体分割会失败，但使用 Tensorflow Keras 不会失败</title>
      <link>https://stackoverflow.com/questions/78969962/human-segmentation-fails-with-pytorch-not-with-tensorflow-keras</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78969962/human-segmentation-fails-with-pytorch-not-with-tensorflow-keras</guid>
      <pubDate>Tue, 10 Sep 2024 14:18:33 GMT</pubDate>
    </item>
    <item>
      <title>高效的 PyTorch 带矩阵到密集矩阵乘法</title>
      <link>https://stackoverflow.com/questions/78959447/efficient-pytorch-band-matrix-to-dense-matrix-multiplication</link>
      <description><![CDATA[问题：在我的某个程序中，我需要计算矩阵乘法 A @ B，其中两个矩阵的大小均为 N x N，但 N 相当大。我推测使用 band_matrix(A, width) @ B 来近似该乘积即可满足需求，其中 band_matrix(A, width) 表示 A 的带状矩阵部分，宽度为 width。例如，width = 0 给出对角矩阵，对角线元素取自 A，而 width = 1 给出以类似方式获取的三对角矩阵。
我的尝试：我尝试提取三对角矩阵，例如，以以下方式：
# 步骤 1：提取主对角线
main_diag = torch.diagonal(A, dim1=-2, dim2=-1) # 形状：[d1, d2, N]

# 步骤 2：提取上对角线（偏移量=1）
upper_diag = torch.diagonal(A, offset=1, dim1=-2, dim2=-1) # 形状：[d1, d2, N-1]

# 步骤 3：提取下对角线(offset=-1)
lower_diag = torch.diagonal(A, offset=-1, dim1=-2, dim2=-1) # 形状：[d1, d2, N-1]

# 步骤 4：重建三对角矩阵
# 主对角线
tridiag = torch.diag_embed(main_diag) # 形状：[d1, d2, N, N]

# 上对角线（移动值以创建第一个上对角线）
tridiag += torch.diag_embed(upper_diag, offset=1)

# 下对角线（移动值以创建第一个下对角线）
tridiag += torch.diag_embed(lower_diag, offset=-1)

但我不确定 tridiag @ B 是否比原始 A 更有效率@ B 或者只是相同的复杂性，因为 Torch 可能不知道 tridiag 的具体结构。理论上，使用三对角矩阵的计算应该快 N 倍。

任何有助于理解 PyTorch 在这种情况下的行为或实施一些替代的 GPU 优化方法的帮助都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78959447/efficient-pytorch-band-matrix-to-dense-matrix-multiplication</guid>
      <pubDate>Sat, 07 Sep 2024 05:46:17 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 `sklearn` 管道中的 `ravel()` 或 `to_numpy()` 转换目标变量？</title>
      <link>https://stackoverflow.com/questions/78958361/is-it-possible-to-transform-a-target-variable-using-ravel-or-to-numpy-in</link>
      <description><![CDATA[我在 R markdown 文档中使用 RStudio 和 tidymodels。我想整合一些来自 scikit-learn 的模型。将数据从 R 代码块传输到 Python 代码块效果很好，但是当我使用以下代码训练和测试模型时：
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

log_reg_pipe = Pipeline([
(&#39;Logistic Regression&#39;, LogisticRegression())
])

log_reg_pipe.fit(X_train, y_train).score(X_val, y_val)

我收到错误
DataConversionWarning：当预期为 1d 数组时，传递了列向量 y。
请将 y 的形状更改为 (n_samples, )，例如使用 ravel()。

我可以通过使用 y_train[&#39;clinical_course&#39;].to_numpy() 训练数据来解决这个问题，但我希望这直接在管道中完成。这可能吗？
请注意，上面的代码只是一个简单的示例来展示我的问题。在这种情况下，X_train 有四列，y_train 有一列。
如上所述，我尝试使用 .to_numpy()，但我想要一个在管道内完成所有转换的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78958361/is-it-possible-to-transform-a-target-variable-using-ravel-or-to-numpy-in</guid>
      <pubDate>Fri, 06 Sep 2024 18:31:45 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 中的 Autograd Trainstep 中的 Lightning</title>
      <link>https://stackoverflow.com/questions/78956646/autograd-in-pytorch-lightning-in-trainstep</link>
      <description><![CDATA[我想实现一个基于 Pytorch Lightning 的 ML 训练，其中我使用 autograd 功能进行训练损失计算：
def training_step(self, batch, batch_idx):
x, y = batch
y_hat = self(x)
loss = self.loss_function(y_hat, y)
return loss

X 的每个样本 x 都是一个二维向量 x = [v, a]。
在训练步骤中，我想计算 y_hat 相对于 的梯度。到 v。
损失进一步通过以下方式计算：
loss = mse(y,y_hat) + mse(gradient,gradient_hat)

其中给出了（真实）梯度。
到目前为止，尝试了 y_hat.backward() 的（典型）方法，但无法使其工作：
def training_step(self, batch, batch_idx):
x, y = batch
x.requires_grad_(True) # 确保我们跟踪 x 的梯度
y_hat = self(x)

# 计算 y_hat 相对于 v 的梯度（x[:, 0]）
v = x[:, 0]
grads = torch.autograd.grad(y_hat, v, grad_outputs=torch.ones_like(y_hat), create_graph=True)[0] # ...
]]></description>
      <guid>https://stackoverflow.com/questions/78956646/autograd-in-pytorch-lightning-in-trainstep</guid>
      <pubDate>Fri, 06 Sep 2024 10:08:53 GMT</pubDate>
    </item>
    <item>
      <title>具有中间层输出的 Keras 多输出自定义损失</title>
      <link>https://stackoverflow.com/questions/66526604/keras-multioutput-custom-loss-with-intermediate-layers-output</link>
      <description><![CDATA[我在 keras 中有一个模型，它接受两个输入并返回 3 个输出，我想计算自定义损失。我的问题是我不知道如何在损失中使用中间层的输出。到目前为止，该模型由两个子模型（图中的子模型 1 和子模型 2）组成，最终损失由损失 1 和损失 2 的总和组成。这很容易，因为 loss1 将 output1 与数据生成器的 label1 进行比较，将 output2 与数据生成器的 label2 进行比较。

当我在模型中包含 submodel3 时，问题就出现了，因为 loss3 将 output1 与 output3 进行比较，而 output1 是模型某一层的输出，而不是数据生成器的 label3。我试过这个方法：
input1 = Input(shape=input1_shape)
input2 = Input(shape=input2_shape)
output1 = submodel1()([input1,input2]) #不要在意代码符号，因为它是用来解释问题的代码。
output2 = submodel2()(output1)
output3 = submodel3()(output1)
@tf.function
def MyLoss(y_true, y_pred):
out1, out2, out3 = y_pred
inp1, inp2 = y_true

loss1 = tf.keras.losses.some_loss1(out1,inp1)
loss2 = tf.keras.losses.some_loss2(out2, inp2)
loss3 = tf.keras.losses.some_loss3(out2,out3)

loss = loss1 + loss2 + loss3
return loss

model = Model([input1,input2],[output1,output2,output3])
model.compile(optimizer=&#39;adam&#39;,loss = MyLoss)

但我收到此错误：
 OperatorNotAllowedInGraphError：不允许对“tf.Tensor”进行迭代：AutoGraph 确实转换了此函数。这可能表明您正在尝试使用不受支持的功能。

我正在使用 TensorFlow 2.3.0-rc0 版本。]]></description>
      <guid>https://stackoverflow.com/questions/66526604/keras-multioutput-custom-loss-with-intermediate-layers-output</guid>
      <pubDate>Mon, 08 Mar 2021 08:39:20 GMT</pubDate>
    </item>
    <item>
      <title>按特定日期（而非观察结果）进行训练和测试</title>
      <link>https://stackoverflow.com/questions/61096540/train-and-test-splits-by-unique-dates-not-observations</link>
      <description><![CDATA[我正在尝试使用 R 中的随机森林训练一个模型。我有一个时间序列，其中包含每个日期的多只股票的信息，并创建了一个非常简化的版本：
日期 &lt;- rep(seq(as.Date(&quot;2009/01/01&quot;), by = &quot;day&quot;, length.out = 100), 10)
名称 &lt;- c(rep(&quot;Stock A&quot;, 100), rep(&quot;Stock B&quot;,100), rep(&quot;Stock C&quot;, 100), rep(&quot;Stock D&quot;, 100), rep(&quot;Stock E&quot;,100), rep(&quot;Stock F&quot;,100), rep(&quot;Stock G&quot;,100), rep(&quot;Stock H&quot;,100), rep(&quot;Stock I&quot;, 100), rep(&quot;Stock J&quot;, 100))
类别 &lt;- sample(1:10, 1000, replace=TRUE)

DF &lt;- data.frame(Date, Name, Class)
DF &lt;- DF %&gt;% 排列(Date, Name)

看起来像这样：
 日期 名称 类
1 2009-01-01 股票 A 5
2 2009-01-01 股票 B 2
3 2009-01-01 股票 C 4
4 2009-01-01 股票 D 10
5 2009-01-01 股票 E 7
6 2009-01-01 股票 F 3
...
11 2009-01-02 股票 A 10
12 2009-01-02 股票 B 8
13 2009-01-02 股票 C 9


使用时trainControl 用于将数据拆分为训练和测试期，拆分是基于每个观察进行的，但我希望基于特定日期进行。到目前为止，我所做的是：
timecontrol &lt;- DF %&gt;% group_by(Date) %&gt;% trainControl(
method = &#39;timeslice&#39;,
initialWindow = 10,
horizo​​n = 5,
skip = 4,
fixedWindow = TRUE,
returnData = TRUE, 
classProbs = TRUE
)

fitRF &lt;- train(Class ~ ., 
data = DF,
method = &quot;ranger&quot;,
tuneGrid = tunegrid,
na.action = na.omit,
trControl = timecontrol)

这给了我一个包含 10 个观察的训练集，后面是 5 个测试观察。
但是，我希望有一个训练集（和测试集......）包含 10 个不同日期的所有观测值，这样，一个训练集将是 10 天乘以每天的观测值数量，并且在各个时间段之间跳跃，以便每个测试时间段都基于全新的数据（因此 skip=4）。
第一个训练/测试拆分应该是训练=10 数据集的第一个不同日期，测试=接下来的 5 个不同日期，然后第二个训练/测试拆分应该是测试集 2 是第一个测试集之后的 5 天。
与我上面显示的数据集不同，我的数据集每天包含不同数量的观测值。我的数据集包含 417497 个观测值，但只有 2482 个不同日期，因此能够根据“分组”日期进行训练/测试拆分会产生很大的不同。 
我能否使用 trainControl 来获得所需的分割，还是必须手动分割所有数据？]]></description>
      <guid>https://stackoverflow.com/questions/61096540/train-and-test-splits-by-unique-dates-not-observations</guid>
      <pubDate>Wed, 08 Apr 2020 08:33:55 GMT</pubDate>
    </item>
    <item>
      <title>如何处理正态分布中的零项</title>
      <link>https://stackoverflow.com/questions/60408826/how-to-handle-entries-of-zero-in-an-otherwise-normal-distribution</link>
      <description><![CDATA[我正在使用 kaggle 房屋数据集。我正尝试使用神经网络进行练习。我正在尝试规范化数据。我的问题是：我有一个变量 BsmtFinSF1，它指的是“1 型成品平方英尺”，它有很多值为 0。值零对应“无地下室”，事实上，在另一个因子变量中，它对应于一个级别。例如，如果“地下室条件”变量对应于“无地下室”，则意味着 BsmtFinSF1 变量将为 0。下面是 BsmtFinSF1 的直方图。如果我没有弄错的话，如果没有零，分布将是正常的。我该如何将其标准化，或者我是否应该将其标准化？
]]></description>
      <guid>https://stackoverflow.com/questions/60408826/how-to-handle-entries-of-zero-in-an-otherwise-normal-distribution</guid>
      <pubDate>Wed, 26 Feb 2020 07:30:27 GMT</pubDate>
    </item>
    </channel>
</rss>