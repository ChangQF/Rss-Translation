<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 08 Dec 2023 18:17:52 GMT</lastBuildDate>
    <item>
      <title>为什么tensorflow的AudioIOTensor的WAV文件的张量输出与decode_wav不同？</title>
      <link>https://stackoverflow.com/questions/77628394/why-does-tensor-output-for-wav-file-from-tensorflows-audioiotensor-differ-that</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77628394/why-does-tensor-output-for-wav-file-from-tensorflows-audioiotensor-differ-that</guid>
      <pubDate>Fri, 08 Dec 2023 17:55:59 GMT</pubDate>
    </item>
    <item>
      <title>Python-找不到模块-transformers.modeling_tf_gpt2</title>
      <link>https://stackoverflow.com/questions/77628324/python-module-not-found-transformers-modeling-tf-gpt2</link>
      <description><![CDATA[我正在尝试从 Transformers.modeling_tf_gpt2 访问 TFConv1D。但是，我不断收到错误：
ModuleNotFoundError：没有名为“transformers.modeling_tf_gpt2”的模块
我使用的是 python 3.10.13
我尝试卸载并重新安装变压器模块。我对机器学习相当陌生，不确定如何导入这些模块

]]></description>
      <guid>https://stackoverflow.com/questions/77628324/python-module-not-found-transformers-modeling-tf-gpt2</guid>
      <pubDate>Fri, 08 Dec 2023 17:43:02 GMT</pubDate>
    </item>
    <item>
      <title>不确定为什么我在 Snowflake 上使用 Python 进行逻辑回归时会遇到类型错误？</title>
      <link>https://stackoverflow.com/questions/77628292/unsure-why-im-running-into-type-errors-in-logistic-regression-using-python-on-s</link>
      <description><![CDATA[我正在使用 Python 在 Snowflake 上创建逻辑回归模型。我在本地 R 中做了相同的逻辑回归，但想将其转换到我的 Snowflake 数据仓库。我取得了一些成功，但我对 Python 的熟悉度还不如对 R 的熟悉度。
我相信回归是拟合并给出了一个模型。我真的不知道预测的概率是什么样的，但这确实是目前的次要问题。
我只想从 pandas DataFrame 返回一个雪花 DataFrame。我无法让它发生。
下面是我的代码片段。
导入snowflake.snowpark作为snowpark
导入 Snowflake.snowpark.functions 作为 F
从 sklearn.linear_model 导入 LogisticRegression
从 Snowflake.snowpark.functions 导入 col
将 pandas 导入为 pd

def main（会话：snowpark.Session）：
#
# 下面的一切都是数据转换，一切都工作得很好
＃ 据我所知

# ind_cols 和 dep_cols 是列名数组
# 定义哪些列是自变量，哪些列是因变量。
# 这里我将样本分为独立列和从属列，
# 并使用 scikit-learn 中的 LogisticRegression。

    X = full_sample[ind_cols].to_pandas()
    y = full_sample[dep_col].to_pandas()

# ret_df 是我有兴趣预测概率的雪花数据帧。
    ret_df_lm = ret_df[ind_cols].to_pandas()

    lm = 逻辑回归()

    lm.fit(X, y)

    y_pred = lm.predict_proba(ret_df_lm)

    y_final = session.table(y_pred)

    #retention_pred = lm.predict(ret_df)

    返回 y_final

当我尝试返回y_final时，我收到错误TypeError：序列项0：预期的str实例，找到numpy.ndarray。我一定错过了一些东西。我尝试过其他东西，比如雪花的 session.write_pandas() 但我不确定这是否是我需要的。
如何使 y_final 成为雪花 DataFrame？预先感谢您的任何提示或帮助！]]></description>
      <guid>https://stackoverflow.com/questions/77628292/unsure-why-im-running-into-type-errors-in-logistic-regression-using-python-on-s</guid>
      <pubDate>Fri, 08 Dec 2023 17:36:04 GMT</pubDate>
    </item>
    <item>
      <title>用于 Siamese 网络的变压器编码器</title>
      <link>https://stackoverflow.com/questions/77628283/transformer-encoder-for-siamese-networks</link>
      <description><![CDATA[我是本科生，正在考虑制作人脸编码器的方法。我对 Transformer 编码器的工作原理有了直观的了解。据我所知，编码器是双向的，因此可以获取上下文信息，我们能否非常擅长编码......
因此，对于面部编码的图像数据或面部深度数据执行相同的操作是否会更好。
我最初尝试使用 Resnet 和 inceptionnet（均作为暹罗网络）实现 2D 图像的人脸编码器......但想知道我是否可以使用 Transformer 编码器对 3D 数据执行相同的操作3D数据从而获得高精度]]></description>
      <guid>https://stackoverflow.com/questions/77628283/transformer-encoder-for-siamese-networks</guid>
      <pubDate>Fri, 08 Dec 2023 17:34:37 GMT</pubDate>
    </item>
    <item>
      <title>在硬边距 SVM 的背景下，当一个点违反边距时会发生什么？</title>
      <link>https://stackoverflow.com/questions/77628142/in-the-context-of-hard-margin-svms-what-happens-when-a-point-violates-the-margi</link>
      <description><![CDATA[我目前正在自学机器学习理论，并正在阅读有关硬边距和软边距支持向量机的信息。我知道，当我们拥有非线性可分离数据并且我们希望允许一些噪声，这样我们可能会有“违反”标准的点时，软边缘 SVM 会很有用。边距。然而，我想知道与这个想法相关的两个案例，但在硬裕度 SVM 的背景下：(a) 如果向数据集中添加一个“违反”规则的新点硬边际SVM，这一定会导致边际缩小吗？ (b)类似地，如果不是添加新点而是扰乱数据集中已有的点，使得它现在“违反”了数据集。边距，这会导致边距缩小吗？
直观上，我认为边距不一定会缩小，而是硬边距 SVM 不再可行，您会想使用软边距 SVM。例如，如果您正在考虑二元分类问题，如果违规点非常严重，以至于它跨越了决策边界并进入了具有不同分类的点簇，我不知道边距会如何缩小 - 它看来你只能使用软边缘 SVM。]]></description>
      <guid>https://stackoverflow.com/questions/77628142/in-the-context-of-hard-margin-svms-what-happens-when-a-point-violates-the-margi</guid>
      <pubDate>Fri, 08 Dec 2023 17:02:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow 解决机器学习分类中类不平衡的优化技术 [关闭]</title>
      <link>https://stackoverflow.com/questions/77627877/optimization-techniques-for-addressing-class-imbalance-in-machine-learning-class</link>
      <description><![CDATA[我正在使用 TensorFlow 进行机器学习分类项目，并且在我的数据集中遇到了严重的类别不平衡问题。正类实例的数量远远多于负类实例，导致模型性能不佳。我正在寻求有关有效技术的指导，以减轻类别不平衡的影响并提高模型的整体性能。
我已经探索了类权重调整和过采样等方法，但我很想了解 TensorFlow 框架内的其他高级方法或最新进展。此外，我想知道是否有任何预处理步骤或模型架构已被证明可以有效处理此类不平衡场景。
您能给我指出相关的研究论文、教程或代码示例吗？我的目标是增强模型正确预测正类实例的能力，同时保持较高的整体准确性。
作为 Open AI 开发的基于文本的 AI 模型，我没有个人经验、意图或期望。我的目的是根据我收到的意见提供帮助和信息。如果您有特定问题或主题需要帮助，请随时询问，我将尽力提供有用的信息！]]></description>
      <guid>https://stackoverflow.com/questions/77627877/optimization-techniques-for-addressing-class-imbalance-in-machine-learning-class</guid>
      <pubDate>Fri, 08 Dec 2023 16:12:19 GMT</pubDate>
    </item>
    <item>
      <title>“机器学习的最佳预处理：解决数据集中的缺失数据和异常值。”</title>
      <link>https://stackoverflow.com/questions/77627148/optimal-preprocessing-for-machine-learning-addressing-missing-data-and-outlier</link>
      <description><![CDATA[在训练机器学习模型之前，如何有效地预处理和清理数据集，尤其是在处理丢失数据和异常值时？
预处理和清理数据集是为机器学习模型准备数据的关键步骤。处理缺失数据和异常值需要仔细考虑]]></description>
      <guid>https://stackoverflow.com/questions/77627148/optimal-preprocessing-for-machine-learning-addressing-missing-data-and-outlier</guid>
      <pubDate>Fri, 08 Dec 2023 14:11:44 GMT</pubDate>
    </item>
    <item>
      <title>为什么 tfidf 对象抛出它没有属性预测错误[关闭]</title>
      <link>https://stackoverflow.com/questions/77626940/why-is-tfidf-object-throwing-it-has-no-attribute-predict-error</link>
      <description><![CDATA[在此处输入图像描述
这是我的代码
在此处输入图片描述
在此处输入图片描述
我正在尝试加载两个预定义模型，一个是 tfidf，另一个是 LR_model，但如果电影评论是正面还是负面，它将进行分类。]]></description>
      <guid>https://stackoverflow.com/questions/77626940/why-is-tfidf-object-throwing-it-has-no-attribute-predict-error</guid>
      <pubDate>Fri, 08 Dec 2023 13:32:59 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：“H1bInfo”对象没有属性“K_meanAnalyze”</title>
      <link>https://stackoverflow.com/questions/77626832/attributeerror-h1binfo-object-has-no-attribute-k-meananalyze</link>
      <description><![CDATA[def K_meanAnalyze(self,H1bInfo):
        #获取状态值
        工作地点 = H1bInfo[&#39;WORKSITE&#39;]
        H1City, H1State =worksites.str.split(&#39;, &#39;, 1)
        
        #读取经纬度数据（取出null）
        H1LatLong = H1bInfo.loc[H1State！= &#39;NA&#39;]
        H1LatLong = H1LatLong[[&#39;lon&#39;,&#39;lat&#39;]]
        H1LatLong = H1LatLong.dropna()
        H1Long = H1LatLong[&#39;lon&#39;].values
        H1Lat = H1LatLong[&#39;lat&#39;].values

        #K-Means 聚类以查看申请人的位置
        n簇 = 10
        kmeans = KMeans(n_clusters=nClusters, random_state=0).fit(H1LatLong.values)
        #获取每个案例的集群
        kl = kmeans.labels_
        H1LatLong[&#39;簇&#39;] = kl
        H1LatLong[&#39;状态&#39;] = H1状态
        H1LatLong = H1LatLong.dropna()
        返回H1经纬度


如果 __name__ == “__main__”：

    h1bSystem = H1bInfo()
    尝试：
        H1Info = pd.read_csv(&#39;h1b_history.csv&#39;)
    除了 IO 错误：
        print(&quot;错误：找不到路径，请将数据和程序放在同一文件夹中&quot;)
    别的：
        print(&quot;数据文件读取成功&quot;)

    函数 = 1
    k_meanResult = h1bSystem.K_meanAnalyze(H1Info)

我在 google colab 工作，我编写了这段代码，虽然我定义了该函数，但出现以下错误。我查看了该网站，但找不到类似的错误。怎么解决？
&lt;前&gt;&lt;代码&gt;
    AttributeError Traceback（最近一次调用最后一次）
    &lt;ipython-input-114-f029a4ae230e&gt;在&lt;细胞系：1&gt;()
         10
         11 功能 = 1
    ---&gt; 12 k_meanResult = h1bSystem.K_meanAnalyze(H1Info)
         13
         14 工资_Result = h1bSystem.salaryAnalyze(H1Info, k_meanResult)
    
    AttributeError：“H1bInfo”对象没有属性“K_meanAnalyze”

]]></description>
      <guid>https://stackoverflow.com/questions/77626832/attributeerror-h1binfo-object-has-no-attribute-k-meananalyze</guid>
      <pubDate>Fri, 08 Dec 2023 13:18:14 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 进行时间序列中的特征/模式识别</title>
      <link>https://stackoverflow.com/questions/77626747/feature-pattern-recognition-in-time-series-with-python</link>
      <description><![CDATA[

自回归时间序列（下一个值取决于之前的值）
必须对某些模式进行分类
一个序列可以包含多个模式
每个模式的长度可能不同（例如“待机模式”的长度可能为 10 或 1000）

有没有办法解决这样的问题？]]></description>
      <guid>https://stackoverflow.com/questions/77626747/feature-pattern-recognition-in-time-series-with-python</guid>
      <pubDate>Fri, 08 Dec 2023 13:03:23 GMT</pubDate>
    </item>
    <item>
      <title>从 SAM 分割图像进行 3D 重建以进行 Bin-Picking [关闭]</title>
      <link>https://stackoverflow.com/questions/77625864/3d-reconstruction-from-sam-segmented-images-for-bin-picking</link>
      <description><![CDATA[当 Bin-Picking 出现时，我正在考虑 SAM（分段任意模型）的应用。
您会发现一个或多个装有散装货物的盒子，如下所示：
分割前的图像
多个相同种类的物体被扔进盒子里。 Bin-Picking 需要用于引导机器人从容器中拾取零件，而不会发生崩溃等情况。我们不讨论该应用程序的机器人端。
最重要的是可拾取部分的 6 自由度姿态估计。
我的想法有两个：

准备：让我们使用 SAM 分割图像并找到不同的部分，如下所示：
SAM 分割图像
使用图像片段重建对象模型。在最好的情况下，这可以通过部分遮挡的部件且无需校准来实现。例如。 Zero 1-to-3 能够做到这一点，但仅使用单个图像。

串行生产/推理：对 (1) 中的零件进行 3D 重建，现在需要估计图像中找到的零件的姿势。


我并不期望结果与 3D 扫描系统的结果一样好，例如Photoneo Phoxi 扫描。但我想知道这种方法的局限性，特别是如果它无需为每个单独的生产部件重新训练模型即可工作。
我的两个问题：

针对少数/多个图像进行 3D 重建的最先进方法是什么？
是否有可行的方法来根据图像进行姿态估计？

开始我的研究 GitHub 3D 重建概述，我在其中发现了许多有趣的存储库和论文。
例如。我尝试过的 零 1-3 @Github 。
但我缺乏以不同方法使用多个片段/图像的机会。]]></description>
      <guid>https://stackoverflow.com/questions/77625864/3d-reconstruction-from-sam-segmented-images-for-bin-picking</guid>
      <pubDate>Fri, 08 Dec 2023 10:18:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的逻辑回归模型重复预测相同的事情？</title>
      <link>https://stackoverflow.com/questions/77625369/why-is-my-logistic-regression-model-predicting-the-same-thing-repetitively</link>
      <description><![CDATA[https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset 
我使用这个 Kaggle 数据集作为我的糖尿病数据集，并尝试创建一个 LogisticRegression 模型来预测结果。
我创建了以下类：
导入 pandas 作为 pd
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.preprocessing 导入 StandardScaler、MinMaxScaler
从 sklearn.model_selection 导入 KFold
将seaborn导入为sns

从 sklearn.model_selection 导入 train_test_split
从 sklearn.metrics 导入 precision_score

糖尿病日志类别：
    df = pd.read_csv(“/Users/aahan_bagga/Desktop/diabetes_data.csv”)
    X=df.drop([“结果”], axis=1)
    Y=df[“结果”]
    预浸料 = 0
    葡萄糖 = 0
    血压=0
    皮肤厚度 = 0
    胰岛素 = 0
    体重指数 = 0
    糖尿病谱系函数 = 0
    年龄 = 0
    def __init__(self, p, g, BP, ST, I, BMI, DPF, 年龄):
        self.preg = p
        自身葡萄糖 = g
        自身.BP = BP
        self.skinThickness = ST
        自身胰岛素 = I
        自身体重指数 = BMI
        self.diabetesPedigreeFunction = DPF
        自我年龄 = 年龄

    def 预处理（自身）：
        全局 Y_train
        全局Y_测试
        #K 折交叉验证
        kf = KFold(n_splits = 9, shuffle = True, random_state = 19)

        全局 X_train、X_test、Y_train、Y_test
        对于 kf.split(self.X) 中的训练索引、测试索引：
            X_train, X_test = self.X.iloc[训练索引], self.X.iloc[测试索引]
            Y_train, Y_test = self.Y.iloc[训练索引], self.Y.iloc[测试索引]


        #标准化比标准化稍微好一些
        缩放器 = MinMaxScaler()
        全局 x_train_s、x_test_s
        x_train_s = 缩放器.fit_transform(X_train)
        x_test_s = 缩放器.transform(X_test)

    def 火车（自己）：
        全球模式
        模型 = 逻辑回归（max_iter = 2000）
        model.fit(x_train_s,Y_train)
        y_pred = model.predict(x_test_s)
        返回f“{accuracy_score(Y_test, y_pred) * 100}%”
    
        # 在此调整超参数
    
    defdiabetes_pred（自我）：
        prob = model.predict_proba([[self.preg, self.glucose, self.BP, self.skinThickness, self.insulin, self.bmi, self.diabetesPedigreeFunction, self.age]])
        打印（问题）
        如果概率[0,1]&gt; 0.5：
            返回“糖尿病”
        别的：
            返回“没有糖尿病”
    
    #def Decision_boundary_graph():
        #
    


d = 糖尿病LogReg(2,126,45,23,340,30,0.12,29)

d.预处理()
打印（d.train（））
打印（d.diabetes_pred（））

正在重复输出：
&lt;预&gt;&lt;代码&gt;80.0%
[[0。 1.]]
糖尿病

已输出“糖尿病”信息它所做的所有预测的结果。我是机器学习的新手，但我知道我还没有调整我的超参数。这与数据集的长度有关吗，是不是太短了？或者也许与我的 k 折交叉验证有关？]]></description>
      <guid>https://stackoverflow.com/questions/77625369/why-is-my-logistic-regression-model-predicting-the-same-thing-repetitively</guid>
      <pubDate>Fri, 08 Dec 2023 08:47:22 GMT</pubDate>
    </item>
    <item>
      <title>使用 RNN 确定可接受的情感分析基线</title>
      <link>https://stackoverflow.com/questions/77623881/determining-an-acceptable-baseline-for-sentiment-analysis-using-rnns</link>
      <description><![CDATA[按照我发现的教程，我一直在尝试使用循环神经网络 (RNN) 进行情感分析 此处。虽然我的模型可以正常运行，但我在准确性方面遇到了障碍，始终达到 85-90% 之间。我尝试了各种优化，但我不确定什么构成了结束我的努力的合理基线，特别是考虑到基于 Transformer 的 LSTM 模型的潜在效率。
我的主要查询并不是以进一步提高准确性为中心；相反，我寻求指导来确定何时考虑情感分析模型对于实际使用足够有效，特别是在使用 RNN 来完成此任务时。由于我对机器学习相对陌生，因此我不清楚何时确定我的目标已实现。]]></description>
      <guid>https://stackoverflow.com/questions/77623881/determining-an-acceptable-baseline-for-sentiment-analysis-using-rnns</guid>
      <pubDate>Fri, 08 Dec 2023 01:35:34 GMT</pubDate>
    </item>
    <item>
      <title>从图像中检测 ui 播放器元素[关闭]</title>
      <link>https://stackoverflow.com/questions/77623368/detecting-ui-player-elements-from-a-image</link>
      <description><![CDATA[我有一个用例，需要从播放器的屏幕截图中找到视频播放器元素的坐标，例如暂停、全屏、恢复、下一个视频按钮。
困难在于，元素后面实际上有视频帧，因此背景可能会发生变化。我可以通过哪种方式跟进？我使用了opencv2模板匹配，但它无法检测到元素。
]]></description>
      <guid>https://stackoverflow.com/questions/77623368/detecting-ui-player-elements-from-a-image</guid>
      <pubDate>Thu, 07 Dec 2023 22:16:35 GMT</pubDate>
    </item>
    <item>
      <title>TF2 - GradientTape 与 Model.fit() - 为什么 GradientTape 不起作用？</title>
      <link>https://stackoverflow.com/questions/63550752/tf2-gradienttape-vs-model-fit-why-does-gradienttape-doesnt-work</link>
      <description><![CDATA[晚上好，
我想使用 tf2 和 Gradient Tape 函数实现一个简单回归问题的玩具示例。使用 Model.fit 它可以正确学习，但使用 GradientTape 也可以做一些事情，但与 model.fit() 相比，损失不会移动。这是我的示例代码和结果。我找不到问题。
model_opt = tf.keras.optimizers.Adam()
loss_fn = tf.keras.losses.MeanSquaredError()
使用 tf.GradientTape() 作为磁带：
    y = 模型(X, 训练=True)
    loss_value = loss_fn(y_true, y)
grads = Tape.gradient(loss_value, model.trainable_variables)
model_opt.apply_gradients(zip(grads, model.trainable_variables))

＃结果：
42.47433806265809
42.63973672226078
36.687397360178586
38.744844324717526
36.59080452300609
...

这里是 model.fit() 的常规情况
model.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.MSE,metrics=“mse”)
...
model.fit(X,y_true,详细=0)
＃结果
[40.97759069299212]
[28.04145720307729]
[17.643483147375473]
[7.575242056454791]
[5.83682193867299]

准确率应该大致相同，但看起来它根本没有学习。输入 X 是张量，y_true 也是。
编辑以进行测试
导入pathlib
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np
将 pandas 导入为 pd
将张量流导入为 tf
从张量流导入keras
从tensorflow.keras导入层

dataset_path = keras.utils.get_file(“auto-mpg.data”, “http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data”)

column_names = [&#39;MPG&#39;,&#39;气缸&#39;,&#39;排量&#39;,&#39;马力&#39;,&#39;重量&#39;,
                ‘加速’、‘车型年份’、‘起源’]
数据集= pd.read_csv（数据集路径，名称=列名称，
                      na_values = &quot;?&quot;, comment=&#39;\t&#39;,
                      九月=” ＆quot;，skipinitialspace = True）

数据集 = dataset.dropna()
dataset[&#39;Origin&#39;] = dataset[&#39;Origin&#39;].map({1: &#39;美国&#39;, 2: &#39;欧洲&#39;, 3: &#39;日本&#39;})
数据集 = pd.get_dummies(数据集, prefix=&#39;&#39;, prefix_sep=&#39;&#39;)

train_dataset = dataset.sample(frac=0.8,random_state=0)
test_dataset = dataset.drop(train_dataset.index)

train_stats = train_dataset.describe()
train_stats.pop(“MPG”)
train_stats = train_stats.transpose()

train_labels = train_dataset.pop(&#39;MPG&#39;)
test_labels = test_dataset.pop(&#39;MPG&#39;)

定义范数(x)：
  返回 (x - train_stats[&#39;mean&#39;]) / train_stats[&#39;std&#39;]

normed_train_data = 规范（train_dataset）
normed_test_data = 规范（测试数据集）

def build_model_fit():
  模型 = keras.Sequential([
    层.Dense(64, 激活=&#39;relu&#39;, input_shape=[len(train_dataset.keys())]),
    层.Dense(64, 激活=&#39;relu&#39;),
    层.密集(1)])
  优化器 = tf.keras.optimizers.RMSprop(0.001)
  model.compile(loss=&#39;mse&#39;,optimizer=optimizer)
  返回模型

def build_model_tape():
  模型 = keras.Sequential([
    层.Dense(64, 激活=&#39;relu&#39;, input_shape=[len(train_dataset.keys())]),
    层.Dense(64, 激活=&#39;relu&#39;),
    层.密集(1)])
  opt = tf.keras.optimizers.RMSprop(0.001)
  返回模型，选择

model_f = build_model_fit()
model_g, opt_g = build_model_tape()

纪元 = 20

#Model.fit() - 测试
历史= model_f.fit(normed_train_data, train_labels, epochs=EPOCHS, verbose=2)

X = tf.convert_to_tensor(normed_train_data.to_numpy())
y_true = tf.convert_to_tensor(train_labels.to_numpy())

#GradientTape - 测试
loss_fn = tf.keras.losses.MeanSquaredError()
对于范围 (0,EPOCHS) 内的 i：
    使用 tf.GradientTape() 作为磁带：
        y = model_g(X, 训练=True)
        loss_value = loss_fn(y_true, y)
    grads = Tape.gradient(loss_value, model_g.trainable_variables)
    opt_g.apply_gradients(zip(grads, model_g.trainable_variables))
    打印（损失值）
]]></description>
      <guid>https://stackoverflow.com/questions/63550752/tf2-gradienttape-vs-model-fit-why-does-gradienttape-doesnt-work</guid>
      <pubDate>Sun, 23 Aug 2020 18:39:36 GMT</pubDate>
    </item>
    </channel>
</rss>