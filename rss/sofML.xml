<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 12 Jul 2024 03:18:12 GMT</lastBuildDate>
    <item>
      <title>MLP 回归器工程数据 SKLearn</title>
      <link>https://stackoverflow.com/questions/78738380/mlp-regressor-engineering-data-sklearn</link>
      <description><![CDATA[我的飞机分析模型上分布有 10 个加速度计。从我的分析模型中，我有一组传感器加速度，包括 10 个加速度计 X 6 个自由度 X 6000（60 秒）数据，即 60 x 6000 阵列和 49 X 6000 阵列中飞机上 49 个位置的应力。我使用不同频率的不同时间历史力生成了一组 100 个 60 X 6000 传感器数据和相应的 21 X 6000 传感器数据。我正在尝试使用 MLPRegressor 构建一个 ML 模型，使用交叉验证/提前停止，这样如果我给模型一个 60 x 6000 传感器数据，我就可以预测相应的应力矩阵。由于我想计算应力时间历史的循环次数，因此必须很好地预测带有噪声的应力预测。
我在识别模型的超参数时遇到了麻烦。我大概应该使用多少层？如果我得到的预测不能很好地预测循环计数，是否意味着无法为这种工程数据构建回归模型？
当达到 0.73 交叉验证分数时，我尝试过的大多数参数都无法收敛。此外，scikit learn MLP Regressor 不支持 GPU 使用，运行时间很长。]]></description>
      <guid>https://stackoverflow.com/questions/78738380/mlp-regressor-engineering-data-sklearn</guid>
      <pubDate>Fri, 12 Jul 2024 02:52:04 GMT</pubDate>
    </item>
    <item>
      <title>解决具有不同数据类型分类特征的 Keras 函数模型的类型转换错误</title>
      <link>https://stackoverflow.com/questions/78738365/resolving-type-conversion-error-for-keras-functional-model-with-categorical-feat</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78738365/resolving-type-conversion-error-for-keras-functional-model-with-categorical-feat</guid>
      <pubDate>Fri, 12 Jul 2024 02:44:28 GMT</pubDate>
    </item>
    <item>
      <title>使用神经网络预测值[关闭]</title>
      <link>https://stackoverflow.com/questions/78737493/forecasting-values-with-a-neural-network</link>
      <description><![CDATA[我正在使用神经网络进行氧气时间序列回归（TensorFlow 和 Keras），从 2023-08-25 00:02:12 到 2024-06-18 15:38:36，每 5 分钟获取一次氧气、饱和度、温度和盐度数据。
我的问题是，一旦我的神经网络经过训练和验证....我该如何使用它来预测我拥有的数据集之外的值？如果我希望我的网络预测接下来的 24、48 或 72 小时，如果它们不在我的验证数据集中，我该怎么做？
我已经将我的数据分为训练集和测试集，并且我已经对测试集进行了预测，但我总是需要一个“过去的数据集”。现在我想尝试预测步骤 t+24、t+48，但我不知道如何继续。]]></description>
      <guid>https://stackoverflow.com/questions/78737493/forecasting-values-with-a-neural-network</guid>
      <pubDate>Thu, 11 Jul 2024 20:04:45 GMT</pubDate>
    </item>
    <item>
      <title>USE 等上下文编码器与 OpenAI 的 text-embedding-ada-002 之间有什么区别？[关闭]</title>
      <link>https://stackoverflow.com/questions/78737222/whats-the-difference-between-contextual-encoders-like-use-and-openais-text-emb</link>
      <description><![CDATA[刚刚遇到了 Universal Sentence Encoder，它也可以保持上下文在语义搜索和其他操作中。OpenAI 的 text-embedding-ada-002 是否更先进，或者或多或少以相同的方式工作但可以创建高维向量？
我尝试使用 Universal Text Encoders，但当我遇到 ada-002 时，我发现它与 USE 类似，唯一的区别是它具有更高的维度。]]></description>
      <guid>https://stackoverflow.com/questions/78737222/whats-the-difference-between-contextual-encoders-like-use-and-openais-text-emb</guid>
      <pubDate>Thu, 11 Jul 2024 18:37:01 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中创建支持微调的未训练 AI 模型？</title>
      <link>https://stackoverflow.com/questions/78737208/how-to-create-an-untrained-ai-model-in-python-with-support-for-fine-tuning</link>
      <description><![CDATA[我需要从头开始用 Python 创建一个 AI 模型，未经训练，但希望以后使用我的数据集进行微调。我对 TensorFlow 和 PyTorch 等机器学习框架有一些经验，但不知道如何开始使用可能需要微调的空白模型。
具体来说，我需要以下方面的指导：

如何用 Python 创建一个非常基本的、不可训练的模型。
如何构建模型以支持微调。
加载和预处理我的数据集以进行训练的最佳实践。在我的数据集上训练模型的步骤。

以下是我迄今为止尝试过的概要：
安装了 TensorFlow 和 PyTorch。此外，还创建了简单的神经网络模型，但它们是经过预先训练的。
如果能提供任何可以帮助我实现此目标的示例、代码片段或相关文档的参考资料，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78737208/how-to-create-an-untrained-ai-model-in-python-with-support-for-fine-tuning</guid>
      <pubDate>Thu, 11 Jul 2024 18:33:05 GMT</pubDate>
    </item>
    <item>
      <title>均方值与 RELU 激活反向传播问题</title>
      <link>https://stackoverflow.com/questions/78737193/mean-square-value-and-relu-activation-backpropagation-issue</link>
      <description><![CDATA[我正在尝试从头开始编写神经网络代码。我尝试制作一个像这样的神经网络结构：
我的神经网络结构
对我来说很难的问题是反向传播；在我的神经网络中，我使用 RELU 作为激活函数，并使用 MSE 来计算损失，因此我尝试对其进行导数。
以下是我所有的导数：
https://i.sstatic.net/GPuO3JNQ.png
我不知道如何计算这些函数，因为它们的维度可能不同
我是否必须计算这个
https://i.sstatic.net/gMtBWRIz.png 在我的所有导数中]]></description>
      <guid>https://stackoverflow.com/questions/78737193/mean-square-value-and-relu-activation-backpropagation-issue</guid>
      <pubDate>Thu, 11 Jul 2024 18:28:12 GMT</pubDate>
    </item>
    <item>
      <title>如何针对简单的 ML 模型对来自 EE 的卫星数据进行标准化/预处理？</title>
      <link>https://stackoverflow.com/questions/78736772/how-do-i-standardize-preprocess-this-satellite-data-from-ee-for-simple-ml-models</link>
      <description><![CDATA[我对 Earth Engine/QGIS 还不太熟悉（没有 ArcGIS 许可证），我想使用一个简单的 ML 模型，利用卫星 VCD、NDVI 和气象数据估算地面 O3。
我对 GIS/地理空间数据处理的世界感到迷茫，所以我尽我所能，疯狂地谷歌搜索并阅读了一些文章，以解释我的理由。
我想使用的数据：
EE 数据集：

Daymet V4 每日气候变量（https://developers.google.com/earth-engine/datasets/catalog/NASA_ORNL_DAYMET_V4#bands)
MOD13A2 NDVI 产品 (https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD13A2)
Sentinel-5P (TROPOMI) O3 VCD 数据 (https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_NRTI_L3_O3#bands)

标签：- 来自 EPA 的地面 O3 数据（https://epa.maps.arcgis.com/apps/webappviewer/index.html?id=5f239fd3e72f424f98ef3d5def547eb5&amp;extent=-146.2334,13.1913,-46.3896,56.5319），转到右上角的“选择图层”图标并选择 O3 活动/非活动，然后将鼠标悬停在任意点上

似乎这个 EPA 数据可以直接导出为 CSV，包括经度、纬度和臭氧测量值。

在这里，我假设我需要从不同来源提取的数据具有相同的空间/时间分辨率，以便使用一些简单的机器学习算法（RF/线性回归）。如果有其他方法，请告诉我！
在我设想的数据集中，每“行”数据将是给定像素在给定日期的气象变量值、NDVI 和 VCD 值，我可以对其执行基本的 RF/回归（使用 EE 或 Python）。在我看来，要使它发挥作用，所有数据集都需要就“像素”是什么达成一致，并成为/成为每日时间分辨率（Daymet 和 TROPOMI 已经是每日的，我假设我可以取最接近的 16 天 NDVI 值）。
基于这个假设，我想让所有数据都具有相同的空间分辨率，所以我正在尝试弄清楚如何“重新投影”将 TROPOMI 数据（当前分辨率为 1111.3km）转换为 1km 分辨率（据我所知，这是所有 Daymet 数据和 MODIS 数据的分辨率）。 *我不知道如何使来自 EPA 数据的“最近像素”（来自点数据而非栅格数据）匹配，以便将其用作数据标签，但这似乎是一个更常见的问题，因此在整理完其余部分后，我将四处寻找如何修复该问题。
因此，我想在这里完成的主要操作是标准化空间分辨率：将 TROPOMI 数据转换为 1km 像素或将 Daymet/MODIS 数据转换为 1.113km 像素。
我尝试在 Earth Engine 中可视化所有三个输入数据集（为 Daymet 选择最高温度），像素似乎根本没有对齐。我已在此处附上每个图层的屏幕截图：ndvi 像素、daymet 像素和 tropomi 像素
TROPOMI 数据似乎给出了某种奇怪的模糊像素，Daymet 数据是规则的方形像素但倾斜，而 NDVI 数据是平行四边形。我隐约觉得这与不同的“投影”/“CRS”有关设置，但我对这两者都不太了解，并且不确定如何继续我认为我需要做的重新缩放。
脚本链接：https://code.earthengine.google.com/6fafaccf040e206e97a32e795611d7e4]]></description>
      <guid>https://stackoverflow.com/questions/78736772/how-do-i-standardize-preprocess-this-satellite-data-from-ee-for-simple-ml-models</guid>
      <pubDate>Thu, 11 Jul 2024 16:43:51 GMT</pubDate>
    </item>
    <item>
      <title>未识别的 TensorFlow 回溯导致 ResourceExhaustedError</title>
      <link>https://stackoverflow.com/questions/78736732/unidentified-tensorflow-retracing-leading-to-resourceexhaustederror</link>
      <description><![CDATA[我正在使用 TF Keras 来构建和训练两个 ML 模型，对此我还是个新手。下面我的 Encoder 是一个 U-Net CNN，旨在将静态水印隐形嵌入到封面图像上。
class Encoder(Layer):
def __init__(self):
super(Encoder, self).__init__()

self.conv1_1 = Conv2D(32, (3, 3), padding=&#39;same&#39;,activation=&#39;relu&#39;)
self.conv1_2 = Conv2D(32, (3, 3), padding=&#39;same&#39;,activation=&#39;relu&#39;)
self.pool1 = MaxPooling2D(pool_size=(2, 2))

self.conv2_1 = Conv2D(64, (3, 3), padding=&#39;same&#39;,activation=&#39;relu&#39;)
self.conv2_2 = Conv2D(64, (3, 3), padding=&#39;same&#39;,激活=&#39;relu&#39;)
self.pool2 = MaxPooling2D(pool_size=(2, 2))

# 等等...

接下来，Classifier() 是一个模型，它具有一个空间变换网络 (STN)，用于纠正透视扭曲 + 一个 CNN，用于检测水印的存在（使用 sigmoid 函数得出概率）。 NoiseAndDistortion() 是一个使用 OpenCV 的 CV2 应用噪声（如透视扭曲和模糊）的层。
encoder = Encoder()
classifier = Classifier()
noise_and_distortion = NoiseAndDistortion()
optimizer = Adam(learning_rate=LEARNING_RATE)

这是我从 ChatGPT 改编的训练代码的摘录：
X_batch = load_batch(current_batch_image_paths, batch_size)
X_batch = tf.convert_to_tensor(X_batch)

使用 tf.GradientTape() 作为胶带：
watermarked_images =coder(X_batch)

noisy_watermarked_images = apply_distortions(水印图像)
noisy_unmarked_images = apply_distortions(X_batch)

combined_images = tf.concat([noisy_watermarked_images, noisy_unmarked_images], axis=0)
combined_labels = tf.concat([tf.ones((BATCH_SIZE, 1)), tf.zeros((BATCH_SIZE, 1))], axis=0)

classifier_outputs = classifier(combined_images)

loss = combined_loss(X_batch, watermarked_images, combined_labels, classifier_outputs)

gradients = tape.gradient(loss,coder.trainable_variables + classifier.trainable_variables)

optimizer.apply_gradients(zip(gradients,coder.trainable_variables + classifier.trainable_variables))

apply_distortions() 函数先前使用 NoiseAndDistortion() 在当前批次张量上的每幅图像上应用模拟噪声。而 combined_loss() 则使用 TF 的 SSIM 和 BinaryCrossEntropy 分别评估 Encoder 和 Classifier 的性能。
我的图像大小为 (400, 560)，批次大小为 4。一切都在 Google Colab T4 GPU 实例上顺利运行，从开始到梯度计算，使用了大约 11.2/15.0GB 的 GPU。当执行到最后一行：optimizer.apply_gradients(......)时，会出现以下警告和错误：

警告：tensorflow：最后 5 次调用 &lt;function _BaseOptimizer._update_step_xla at 0x79d5f0f07370&gt; 中的 5 次触发了 tf.function 回溯。跟踪成本高昂，过多的跟踪次数可能是由于 (1) 在循环中重复创建 @tf.function、(2) 传递具有不同形状的张量、(3) 传递 Python 对象而不是张量。对于 (1)，请在循环外定义您的 @tf.function。对于 (2)，@tf.function 具有 reduce_retracing=True 选项，可以避免不必要的回溯。对于 (3)，请参阅 https://www.tensorflow.org/guide/function#controlling_retracing 和 https://www.tensorflow.org/api_docs/python/tf/function 了解更多详情。

屏幕截图：ResourceExhaustedError：尝试分配 3670016132 字节时出现内存不足。
我尝试使用 ChatGPT 解决该问题，但没有成功。按照警告消息中的提示操作：

我没有在循环中创建任何 @tf.function；
我不知道它是什么，但我确信形状是一致的。
我已经仔细检查过，tf.GradientTape() 上下文中使用的所有内容都转换为张量而不是 Python 对象。

我相信其中存在问题，导致执行在内部陷入无限循环（根据它试图分配 3670016132 字节 的事实判断）。如何排除故障？]]></description>
      <guid>https://stackoverflow.com/questions/78736732/unidentified-tensorflow-retracing-leading-to-resourceexhaustederror</guid>
      <pubDate>Thu, 11 Jul 2024 16:34:38 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习 Python 预测价格</title>
      <link>https://stackoverflow.com/questions/78736432/predict-a-price-with-machine-learning-python</link>
      <description><![CDATA[我想估算某件物品在某一日期的价格。例如，如果我知道 07/01/24 和 07/11/24 之间的价格，那么 07/12/24 的价格是多少？
我尝试使用线性或多项式回归、RandomForestRegresor 和 GradientBoostingRegresor。这些模型对提供的数据返回非常准确的结果，但当我尝试在此范围之外使用它时，结果要么与最后一个值相同，要么是不可能的结果（有时我最终得到的价格为负数）。
有人知道怎么做吗？我应该使用其他方法吗？
例如，我尝试对 1992 年 1 月至 2024 年 5 月之间的汽油价格进行 GradientBoosting：
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor

data = pd.read_csv(&quot;essence2.csv&quot;, sep=&quot;;&quot;).iloc[::-1]

x = data[&quot;date&quot;].values.reshape(-1, 1)
y = data[&quot;price&quot;].values

x_train, x_test, y_train, y_test = train_test_split(
x,
y,
test_size=0.01,
random_state=2,
)

model = GradientBoostingRegressor(
n_estimators=200,
learning_rate=1,
random_state=2,
)
model.fit(x_train, y_train)

y_predictions = np.round(model.predict(x), 4)
residues = y_predictions - y

fig, ax = plt.subplots(2)

ax[0].plot(x, y, color=&quot;blue&quot;, label=&quot;Prix réel&quot;)
ax[0].plot(x, y_predictions, color=&quot;orange&quot;, linestyle=&quot;--&quot;, label=&quot;Gradient Boosting Regressor&quot;)
ax[0].xaxis.set_major_locator(MaxNLocator(8))
ax[0].tick_params(axis=&quot;x&quot;, rotation=45)
ax[0].legend()

ax[1].plot(x, residuals, color=&quot;orange&quot;, label=&quot;Résidues&quot;)
ax[1].xaxis.set_major_locator(MaxNLocator(8))
ax[1].tick_params(axis=&quot;x&quot;, rotation=45)
ax[1].legend()

plt.tight_layout()
plt.show()


精华预测
如何预测价格2024 年 6 月？]]></description>
      <guid>https://stackoverflow.com/questions/78736432/predict-a-price-with-machine-learning-python</guid>
      <pubDate>Thu, 11 Jul 2024 15:25:21 GMT</pubDate>
    </item>
    <item>
      <title>分割任何模型（SAM）如何使用多个框及其对应点来预测_torch？</title>
      <link>https://stackoverflow.com/questions/78736247/segment-anything-model-sam-how-do-i-predict-torch-with-multiple-boxes-with-the</link>
      <description><![CDATA[我正在尝试 Segment Anything Model (SAM)，我的问题需要多个框及其对应的点，以便在框内具有特异性。例如 box1 = [#, #, # ,#]，其点为 [x,y]，类为 [0 or 1]，然后在单个图像中包含多个此类点。
我仅使用多个边界框就可以做到这一点，但我想在每个框上包含点。
这是我当前的代码，但它给出了一个错误：
RuntimeError：张量的大小必须匹配，但维度 1 除外。
预期大小为 1，但列表中的张量编号 1 的大小为 3。

将 numpy 导入为 np
导入 torch
将 matplotlib.pyplot 导入为 plt

point = np.array([[330, 370]])
label = np.array([1])

input_point = torch.tensor(point, device=predictor.device)
input_point = input_point.unsqueeze(0)
transformed_point = predictor.transform.apply_coords_torch(input_point, image.shape[:2])

input_label = torch.tensor(label, device=predictor.device)
input_label = input_label.unsqueeze(0)

#yxyx-xyxy
filtered_rois_xyxy = transform_yxyx_to_xyxy(filtered_rois)
input_boxes = torch.tensor(filtered_rois_xyxy, device=predictor.device)
transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2]) 

masks,_,_ = predictor.predict_torch(
boxes=transformed_boxes,
point_coords=transformed_point,
point_labels=input_label,
multimask_output=False
)

masks.shape

plt.figure(figsize=(10, 10))
plt.imshow(image)
for mask in mask:
show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)
for box in input_boxes:
show_box(box.cpu().numpy(), plt.gca())
plt.axis(&#39;off&#39;)
plt.show()

为了调试目的，这是每个输入的打印：

print(input_boxes)
print(input_point)
print(input_label)

tensor([[330, 370, 495, 634],
[401, 168, 586, 425],
[ 1, 0, 157, 210]], dtype=torch.int32)
tensor([[[330, 370]]])
tensor([[1]])
]]></description>
      <guid>https://stackoverflow.com/questions/78736247/segment-anything-model-sam-how-do-i-predict-torch-with-multiple-boxes-with-the</guid>
      <pubDate>Thu, 11 Jul 2024 14:49:27 GMT</pubDate>
    </item>
    <item>
      <title>在图像去噪方面，验证中的 SSIM 高于训练中的 SSIM [关闭]</title>
      <link>https://stackoverflow.com/questions/78735787/ssim-in-validation-higher-then-ssim-in-training-for-image-denoising</link>
      <description><![CDATA[我正在使用 2D U-Net 对显微镜图像进行去噪。我正在使用在不同 z 级别拍摄的图像训练我的网络，这些图像具有基本事实，即 z 中图像的平均值。因此，一些图像具有相同的基本事实。我将图像分成 z 组，以将它们放在训练、验证或测试集中。我正在计算每个批次的 SSIM，并在每个时期平均结果，然后绘制它们。
我面临的问题是验证中的 SSIM 总是高于训练中的 SSIM。
我没有在我的网络中使用 dropout 或批量标准化，这会导致验证具有更高的结果。我确保训练和验证中的图像没有重复。
训练和验证中的 SSIM
为什么会发生这种情况以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78735787/ssim-in-validation-higher-then-ssim-in-training-for-image-denoising</guid>
      <pubDate>Thu, 11 Jul 2024 13:18:41 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 中的空间数据管理机器学习模型中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</guid>
      <pubDate>Thu, 11 Jul 2024 05:01:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 Catboost Golang</title>
      <link>https://stackoverflow.com/questions/78732455/use-catboost-golang</link>
      <description><![CDATA[如何在 golang 上使用 CatBoost？也许有解决方案？
我尝试使用 https://github.com/bourbaki/catboost-go 但我一直收到 CGO 错误。
当我尝试使用 go run 运行它时，它会在编译时出错
usr/local/go/pkg/tool/linux_amd64/link：运行 gcc 失败：退出状态 1
/usr/bin/ld：/tmp/go-link-23536551/000001.o：在函数“_cgo_e59e54336bq_Cfunc”中：
/tmp/go-build/cgo-gcc-prolog:69：对“my_function”的未定义引用
collect2：错误：ld 返回 1 退出状态
]]></description>
      <guid>https://stackoverflow.com/questions/78732455/use-catboost-golang</guid>
      <pubDate>Wed, 10 Jul 2024 19:40:07 GMT</pubDate>
    </item>
    <item>
      <title>训练 PINN 来反演未知参数</title>
      <link>https://stackoverflow.com/questions/78730829/train-a-pinn-to-invert-for-unknown-parameters</link>
      <description><![CDATA[我使用 PINN 求解阻尼振荡器微分方程，同时以阻尼振荡器的噪声观测作为输入，找到后者的摩擦参数。我使用自定义训练程序在 Tensorflow 中编写了代码。问题是我定义的可训练参数没有接近我从噪声观测中知道的正确值。最终，PINN 的解决方案完全不正确。但是，我的代码运行得很好，不需要寻找可训练参数，也就是这里的摩擦参数。
# NN 振荡器系统的实现
def rocks_system_data_loss(t, net, func, params, mu, bc, t_data, u_data, lambda1):
t = t.reshape(-1,1)
t = tf.constant(t, dtype = tf.float32)
t_0 = tf.zeros((1,1))

# 2nd 导数的嵌套循环
with tf.GradientTape() as outer_tape:
outer_tape.watch(t)

with tf.GradientTape() as inner_tape:
inner_tape.watch(t)
x = net(t)

dx_dt = inner_tape.gradient(x, t) # 1st导数

d2x_dt2 = outer_tape.gradient(dx_dt, t) # 二阶导数

# 边界损失
bc_loss_1 = tf.square(net(t_0) - bc[0])
bc_loss_2 = tf.square(dx_dt[0] - bc[1])

# 可学习参数 mu 传递给 ODE
ode_loss = d2x_dt2 - func(x, dx_dt, params[0], mu, params[2])

# 超参数 lambda1 的数据损失
data_loss = u_data - net(t_data)

square_loss = tf.square(ode_loss) + lambda1*tf.square(data_loss) + bc_loss_1 + bc_loss_2
total_loss = tf.reduce_mean(square_loss)

return total_loss, mu

# 带有数据丢失的训练程序
def train_NN_data_loss(epochs, optm, NN, func, bc, lambda1, train_t, train_u, data_t, data_u,
data_u_noised, test_t_plot, true_u_plot, testing_t):
train_loss_record = []
loss_tracker = plotting_points(epochs)

mu = tf.Variable(initial_value=tf.ones((1,1)), trainable=True, dtype=tf.float32)
mu_list = []

waiting = 200
best = float(&#39;inf&#39;)

early_stop = 0

for itr in range(epochs):
with tf.GradientTape() as tape:
#tape.watch(mu)
train_loss, mu = rocksor_system_data_loss(train_t, NN，func，params，mu，bc，data_t，data_u_noised，lambda1)
train_loss_record.append(train_loss)

grad_w = tape.gradient(train_loss，NN.trainable_variables + [mu])
optm.apply_gradients(zip(grad_w，NN.trainable_variables + [mu]))

if itr in loss_tracker:
print(train_loss.numpy())
print(mu.numpy())
plot_epochs_with_noise(train_t，train_u，data_t，data_u_noised，test_t_plot，true_u_plot，testing_t，itr，NN)

mu_list.append(mu.numpy()) 

# 提前停止
# wait += 1
# if train_loss.numpy() &lt; best:
# best = train_loss.numpy()
# wait = 0
# if wait &gt;= waiting:
# print(f&quot;在迭代 {itr} 时停止，损失为 {train_loss_record[itr]}。&quot;)
# early_stop = itr
# break

return train_loss_record, mu_list, early_stop

# 用于 ODE 损失计算/最小化
NN_osc_func = lambda x, dx_dt, k, d, m: -k/m*x - d/m*dx_dt

您可以在此处看到 6000 个 epoch 后的结果。神经网络正在收敛到一条水平线，误差为 5.76，参数估计为 0.84，尽管正确值为 4。这是我的阻尼振荡器设置：
k = 400
d = 4
m = 1
y0 = np.array([1.0, 0.0])

错误结果。
相应损失。
不幸的是，此时我不知道问题可能是什么。我尝试更改 NN_osc_func，并在两个函数中使用了 tape.gradient()。有什么帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/78730829/train-a-pinn-to-invert-for-unknown-parameters</guid>
      <pubDate>Wed, 10 Jul 2024 13:22:04 GMT</pubDate>
    </item>
    <item>
      <title>CNN 预测随机图像</title>
      <link>https://stackoverflow.com/questions/78723722/cnn-predicting-random-images</link>
      <description><![CDATA[我已经用 CNN 训练了模型。我已经用胸部 X 光片图像对模型进行了二元分类训练。即使在训练模型之后。模型正在预测汽车、动物等随机图像，并给出更高的置信度。
需要深度学习模型中随机图像预测的解决方案]]></description>
      <guid>https://stackoverflow.com/questions/78723722/cnn-predicting-random-images</guid>
      <pubDate>Tue, 09 Jul 2024 04:49:05 GMT</pubDate>
    </item>
    </channel>
</rss>