<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 05 Sep 2024 12:31:28 GMT</lastBuildDate>
    <item>
      <title>使用自己的多视图图像绕过 zero123 来增强 InstantMesh 3d 重建输出的纹理</title>
      <link>https://stackoverflow.com/questions/78952999/bypass-zero123-with-own-multiview-images-to-enhance-texture-of-instantmesh-3d-re</link>
      <description><![CDATA[我正在使用 InstantMesh，这是一个 3D 重建管道，它使用多视图模型 (zero123)，可以从一个输入图像生成多视图图像。
我和我的团队正在尝试增强 instantmesh 3D 重建纹理输出，经过多次尝试，我们发现最大的问题之一是 zero123 输出（输入重建管道）的分辨率太低。
我们现在的目标是使用我们自己的分辨率更高的多视图图像。
我现在的问题是：InstantMesh 重建管道是否接受更大的分辨率？如果是，代码中需要更改什么？如果没有，我们是否必须重新训练整个模型以考虑更大的分辨率？
这是运行管道的 InstantMesh 代码：https://github.com/TencentARC/InstantMesh/blob/main/run.py]]></description>
      <guid>https://stackoverflow.com/questions/78952999/bypass-zero123-with-own-multiview-images-to-enhance-texture-of-instantmesh-3d-re</guid>
      <pubDate>Thu, 05 Sep 2024 12:19:51 GMT</pubDate>
    </item>
    <item>
      <title>具有高基数分类列的聚类数据</title>
      <link>https://stackoverflow.com/questions/78952979/clustering-data-which-has-high-cardinal-categorical-columns</link>
      <description><![CDATA[我有一个数据 - 例如：电器商店中的商品数据集。
让我们考虑以下是我的列：
ITEM_NO（唯一标识商品的编号）
、品牌（制造品牌）
、类别
、材料
、工艺
、重量
、价格
、类别特定属性（假设您有某个类别的特定属性，而其他商品可能不存在这些属性）
让我们假设材料、工艺的基数非常高（大约 5k）
我的最终目标：我想将类似类型的商品分组，基本上就是聚类
注意：每个材料值和工艺值都具有独特的意义，因此不能组合在一起
我尝试通过采用每个类别对数据进行分组，然后进行聚类（在缩放、编码和 PCA 之后），但效果并不好。但由于材料和工艺的基数非常高，所以这没有奏效。 [我旋转了材料和工艺的列]。
下一个方法：我根据材料对项目进行分组，并选取构成初始研发项目数量最多的材料，并将这些项目创建为数据集。接下来，我选取上述项目数据集的工艺列（以逗号分隔的字符串，例如 P1、P2、P3 作为值）。因此，我使用 Jaccard 指数根据工艺成对查找每个项目之间的相似性，然后选取 Jaccard 指数值大于 0.7 的项目对，但这花了很长时间。此外，由于材料的基数非常高，我不得不对每一种材料重复此练习。
我想要一个可扩展的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78952979/clustering-data-which-has-high-cardinal-categorical-columns</guid>
      <pubDate>Thu, 05 Sep 2024 12:15:35 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习分析日志文件</title>
      <link>https://stackoverflow.com/questions/78952683/analyse-log-file-with-ml</link>
      <description><![CDATA[我有一些带时间戳的日志文件，这些日志文件来自设备的控制器、驱动程序等。我在日志文件中看到了一些错误代码，但原因不明。我想分析这些日志文件并确定模式，例如错误是否连续发生，或者一个错误是否依赖于另一个错误。错误也可能由其他错误触发，甚至可能是一天前或一秒钟前的错误。最好的和最简单的方法是什么？或者如果有类似问题的解决方案，请告诉我 :)
1)我读过关于 LSTM 和 RNN 的文章，但据我所知，它们用于错误预测，而我的情况是，连续错误的发生窗口可能会有所不同（1 天 -1 秒），提前 1 秒预测它不会那么有帮助。
2)我考虑绘制图表并将每个错误作为图表的一个节点 - 但我不太确定如何处理这个问题。
3)我读过一些关于聚类的文章 -&gt; 但它不会丢弃发生的顺序]]></description>
      <guid>https://stackoverflow.com/questions/78952683/analyse-log-file-with-ml</guid>
      <pubDate>Thu, 05 Sep 2024 11:00:41 GMT</pubDate>
    </item>
    <item>
      <title>无法让 XGBRegressor 输出 0 到 1 之间的值</title>
      <link>https://stackoverflow.com/questions/78952638/unable-to-get-xgbregressor-to-output-values-between-0-and-1</link>
      <description><![CDATA[我们创建了一个应用程序，在该应用程序中，我们为客户提供贷款优惠，客户可以根据其用例更改金额。现在，我正在尝试制作一个 XBGRegressor 模型，该模型可以预测客户的接受率金额，这将在下一个过程中进一步使用。
我使用的特征在某些列中具有空值，因此我制作了 XGBoost Regressor，因为它可以轻松处理空值。将平均值代入这些列是不可能的。我的训练数据的接受率在 0 到 1 的范围内，但我的模型预测的值仍然大于 1 甚至为负数。
我正在使用 Baysian Optimiser 来改进模型。 R 平方值不错（约为 0.75），有什么方法可以进一步改进吗？
这是我目前正在使用的代码。
def get_forecast(just_train_df, metric, data_df):
print(&#39;\nMetrics are:&#39;,metric)
data = just_train_df.copy()
X, y, X_train, y_train, X_test, y_test, X_forecast, y_original = data_preprocessing(data, metric, [])

pbounds = {
&#39;max_depth&#39;: (3, 10),
&#39;learning_rate&#39;: (0.01, 0.3),
&#39;gamma&#39;: (0, 0.5),
&#39;min_child_weight&#39;: (1, 10),
&#39;subsample&#39;: (0.5, 1.0),
&#39;colsample_bytree&#39;: (0.5, 1.0),
&#39;reg_alpha&#39;: (0, 1.0),
&#39;reg_lambda&#39;: (0, 1.0)
}

# 贝叶斯优化
def xgb_cv(max_depth, learning_rate, gamma, min_child_weight, subsample, colsample_bytree, reg_alpha, reg_lambda):

xgb_model = XGBRegressor(
objective=&#39;reg:squarederror&#39;,
eval_metric=&#39;rmse&#39;,
max_depth=int(max_depth),
learning_rate=learning_rate,
gamma=gamma,
min_child_weight=int(min_child_weight),
subsample=subsample,
colsample_bytree=colsample_bytree,
reg_alpha=reg_alpha,
reg_lambda=reg_lambda,
n_jobs=-1
)
r2_scorer = make_scorer(r2_score)
scores = cross_val_score(xgb_model, X_train, y_train,scoring=r2_scorer, cv=5, n_jobs=-1)
return scores.mean() # 返回负均方误差，因为 BayesianOptimization 最小化了目标函数

optimizer = BayesianOptimization(
f=xgb_cv,
pbounds=pbounds,
random_state=36,
verbose=2
)

print(&#39;Initiated Bayesian Optimizer...\n&#39;)
optimizer.maximize(init_points=5, n_iter=50)
print(&#39;\nCompleted Bayesian Optimizer\n&#39;)

print(&quot;Best hyperparameters: &quot;, optimizer.max[&#39;params&#39;], &#39;\n&#39;)

# 使用 Bayesian Optimizer 中的最佳参数训练模型
best_params = optimizer.max[&#39;params&#39;]
final_model = XGBRegressor(
objective=&#39;reg:squarederror&#39;,
eval_metric=&#39;rmse&#39;,
max_depth=int(best_params[&#39;max_depth&#39;]),
learning_rate=best_params[&#39;learning_rate&#39;],
gamma=best_params[&#39;gamma&#39;],
min_child_weight=int(best_params[&#39;min_child_weight&#39;]),
subsample=best_params[&#39;subsample&#39;],
colsample_bytree=best_params[&#39;colsample_bytree&#39;],
reg_alpha=best_params[&#39;reg_alpha&#39;],
reg_lambda=best_params[&#39;reg_lambda&#39;],
n_jobs=-1,
random_state=0
)

final_model.fit(X_train, y_train)

# 数据集上的预测
y_pred_test = final_model.predict(X_test)
y_pred_train = final_model.predict(X_train)
y_pred_val = final_model.predict(X_forecast)

# 在训练数据上评估 best_model
mae = mean_absolute_error(y_train, y_pred_train)
rmse = mean_squared_error(y_train, y_pred_train, squared=False)
r2_train = r2_score(y_train, y_pred_train)
print(f&quot;训练平均绝对误差：{mae}&quot;)
print(f&quot;训练 R 平方：{r2_train}\n&quot;)

# 在测试数据上评估模型
mae = mean_absolute_error(y_test, y_pred_test)
rmse = mean_squared_error(y_test, y_pred_test, squared=False)
r2_test = r2_score(y_test, y_pred_test)
print(f&quot;测试平均绝对误差：{mae}&quot;)
print(f&quot;测试 R 平方：{r2_test}\n&quot;)

目前我面临的一个主要问题是，大约 65-70% 的数据采用 1 的比例。我应该更改模型还是对现有模型进行更改。我是否应该在 XGBoost 中将输入特征标准化为 0 和 1。我在这个领域还很新。]]></description>
      <guid>https://stackoverflow.com/questions/78952638/unable-to-get-xgbregressor-to-output-values-between-0-and-1</guid>
      <pubDate>Thu, 05 Sep 2024 10:48:03 GMT</pubDate>
    </item>
    <item>
      <title>分类模型仅预测单个类别</title>
      <link>https://stackoverflow.com/questions/78952023/classification-model-only-predicts-single-class</link>
      <description><![CDATA[我有一个逻辑回归模型，我正在尝试在我的 NLP 项目中做出健康的预测。我做了一些事情，下面是我的分类报告：
 精确率 召回率 f1 分数 支持率

0 0.68 0.83 0.75 23
1 0.78 0.78 0.78 23
2 0.87 0.87 0.87 30
3 0.76 0.62 0.68 26

准确率 0.77 102
宏平均值 0.77 0.77 0.77 102
加权平均值 0.78 0.77 0.77 102

我认为它实际上没有看起来那么糟糕，但即使我以不同的方式设置参数，大多数时候它也会返回“3”，这是我的一个类。我的意思是，它看起来总是专注于一个单一的类别，即使我的分类报告不支持这种行为。
总之，我的模型看起来很平衡，但它的表现却不是那样。我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78952023/classification-model-only-predicts-single-class</guid>
      <pubDate>Thu, 05 Sep 2024 08:30:57 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用 Google Teachable 机器模型作为对象检测模型吗？</title>
      <link>https://stackoverflow.com/questions/78951811/can-i-use-google-teachable-machine-model-as-object-detection-model</link>
      <description><![CDATA[我正在为我的论文项目工作，这是一个带有移动对象检测功能的自动收银机应用程序。我面临的问题是，自定义项目的变体太多了（大约 250 个类别）。如果我要微调以前的模型（如 MobileNet 或 YOLO），这将花费太多时间，因为我仍然需要创建 POS、数据库和集成热敏打印机。
那么，除了微调以前的模型外，我是否可以只使用 Google Teachable Machine 来处理我的对象检测数据集（假设背景相同）？
应用程序的工作方式是，收银员会在白色背景上拍摄买家想要购买的商品的照片，然后应用程序会自动检测出哪些商品在框架内（使用 Google Teachable Machine .tflite 模型）。
Teachable Machine .tflite 模型是否可以替换下面这段代码中的 modelPath？提前致谢。
private fun runObjectDetection(bitmap: Bitmap) {
// 步骤 1：创建 TFLite 的 TensorImage 对象
val image = TensorImage.fromBitmap(bitmap)

// 步骤 2：初始化检测器对象
val options = ObjectDetector.ObjectDetectorOptions.builder()
.setMaxResults(5)
.setScoreThreshold(0.5f)
.build()
val detector = ObjectDetector.createFromFileAndOptions(
this, // 应用程序上下文
**&quot;model.tflite&quot;, **
options
)
// 步骤 3：将给定的图像提供给模型并打印检测结果
val results = detector.detect(image)

// 步骤 4：解析检测结果并显示
debugPrint(results)

val resultToDisplay = results.map {
// 获取 top-1 类别并制作显示文本
val category = it.categories.first()
val text = &quot;${category.label}, ${category.score.times(100).toInt()}%&quot;

// 创建数据对象，用于显示检测结果
DetectionResult(it.boundingBox, text)
}

// 将检测结果绘制到位图上并显示。
val imgWithResult = drawDetectionResult(bitmap, resultToDisplay)
runOnUiThread {
inputImageView.setImageBitmap(imgWithResult)
}
}
]]></description>
      <guid>https://stackoverflow.com/questions/78951811/can-i-use-google-teachable-machine-model-as-object-detection-model</guid>
      <pubDate>Thu, 05 Sep 2024 07:38:34 GMT</pubDate>
    </item>
    <item>
      <title>级联分段-通道设置是否正确？</title>
      <link>https://stackoverflow.com/questions/78951423/cascade-segmentation-are-the-channels-set-up-correctly</link>
      <description><![CDATA[我想训练一个机器学习模型，用于处理 2D DICOM 图像中的精细蒙版细节。我有 500 张图像准备进行标记/注释。我可以使用这种技术吗？还是我理解错了？
鱼 + 脊椎的注释

我用 1 个类别注释了 500 张图像：鱼。然后我训练一个 model1.pth，将鱼与背景区分开来。该模型有 2 个 out_channels：鱼和背景。

我再次注释了相同的 500 张图像，但现在有 2 个类别：鱼 + 脊椎。我加载 model1.pth，并创建一个具有 2 个输入通道 和 3 个输出通道 的模型：脊柱、鱼和背景，并将模型保存为 model2.pth

最后，我再次注释了 500 张图像，但现在我包括了变形。如果我有 3 种类型的变形，则每种变形都有自己的类别。我加载 model2.pth，创建一个具有 3 个输入通道 和 6 个输出通道 的模型：背景、鱼、脊柱、变形 1、变形 2、变形 3，并将模型保存为 model3.pth。

现在模型可以直接在新图像上使用。是这样吗？


背景和细节。我尝试过什么
目标是找到鱼脊椎的变形。到目前为止，我已尝试通过使用 MONAI 的 UNet 模型 来分割 3 个类别 + 背景。图像是转换为 NifTi 格式 (.dcm.nii.gz) 的 2D DICOM 图像，典型尺寸为 2000x900 像素。我使用 3Dslicer 进行注释。到目前为止的类别：

背景

鱼

脊椎

变形


到目前为止，我已经在（仅）12 张训练图像上进行了测试，只是为了让它运行，我得到了所有 3 个类别的结果，但我猜模型训练过度了。此外，我猜这种技术使得在训练结束后进行微小更改变得更加困难。例如，我想要多种不同类型的变形。 
我的结果：红线左侧：来自 tensorboard，红线右侧：在新图像上测试模型
在开始注释 500 张图像之前，我想验证我是否走在正确的道路上。我希望通过使用级联技术，我可以获得一个可以轻松分割鱼和脊椎的模型，并且我可以随后尝试不同的变形注释。]]></description>
      <guid>https://stackoverflow.com/questions/78951423/cascade-segmentation-are-the-channels-set-up-correctly</guid>
      <pubDate>Thu, 05 Sep 2024 05:34:37 GMT</pubDate>
    </item>
    <item>
      <title>用于简单图回归任务的图神经网络——增加节点数时表现不佳</title>
      <link>https://stackoverflow.com/questions/78950808/graph-neural-network-for-simple-graph-regression-task-not-performing-good-when</link>
      <description><![CDATA[我正在编写一个 gnn 来处理一个简单的任务，即为 GNN 提供一个 v 形图（在 2D 空间中），并要求 gnn 预测 v 形的顶点位置（V 的底部）。当我尝试增加节点数时，性能变差，当我增加到 200 时，它变得不可接受。（1000 * 1000 厘米图中的 2 厘米误差）
我尝试使用 ASAPooling，但效果不佳。
我目前使用的模型是
GNN_DUNE(
(convs): ModuleList(
(0): SAGEConv(3, 28, aggr=mean)
(1-4): 4 x SAGEConv(28, 28, aggr=mean)
)
(dropouts): ModuleList(
(0-5): 6 x Dropout(p=0.058092624169980844, inplace=False)
)
(jump): JumpingKnowledge(cat)
(linears): ModuleList(
(0): Linear(in_features=140, out_features=38, bias=True)
(1): Linear(in_features=38, out_features=38, bias=True)
(2): Linear(in_features=38, out_features=2, bias=True)
)
)
]]></description>
      <guid>https://stackoverflow.com/questions/78950808/graph-neural-network-for-simple-graph-regression-task-not-performing-good-when</guid>
      <pubDate>Wed, 04 Sep 2024 22:51:06 GMT</pubDate>
    </item>
    <item>
      <title>如何计算 Conv2D 层的尺寸？</title>
      <link>https://stackoverflow.com/questions/78949615/how-can-the-dimensions-of-a-conv2d-layer-be-calculated</link>
      <description><![CDATA[我试图了解我的 GAN 生成器输出的尺寸。每层之后的结果尺寸如下：
开始：torch.Size([128, 74, 1, 1]) 
block1 之后：torch.Size([128, 256, 3, 3]) 
block2 之后：torch.Size([128, 128, 6, 6]) 
block3 之后：torch.Size([128, 64, 13, 13]) 
block4 之后：torch.Size([128, 1, 28, 28])

生成器代码如下。此处 z_dim 为 74，但最初为 64。它附加了 10 个类标签，如下所示。
fake_noise = get_noise(cur_batch_size, z_dim, device=device) 
noise_and_labels = Combine_vectors(fake_noise, one_hot_labels)
fake = gen(noise_and_labels)

class Generator(nn.Module):
&#39;&#39;&#39;
Generator Class
值：
z_dim：噪声向量的维度，标量
im_chan：输出图像的通道数，标量
（MNIST 是黑白的，因此 1 个通道是您的默认值）
hidden_​​dim：内部维度，标量
&#39;&#39;&#39;
def __init__(self, z_dim=10, im_chan=1, hidden_​​dim=64):
super(Generator, self).__init__()
self.z_dim = z_dim
# 构建神经网络
self.block1 = self.make_gen_block(z_dim, hidden_​​dim * 4)
self.block2 = self.make_gen_block(hidden_​​dim * 4, hidden_​​dim * 2, kernel_size=4, stride=1)
self.block3 = self.make_gen_block(hidden_​​dim * 2, hidden_​​dim)
self.block4 = self.make_gen_block(hidden_​​dim, im_chan, kernel_size=4, final_layer=True)

def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, padding=1, final_layer=False):
&#39;&#39;&#39;
函数返回对应于生成器块的操作序列DCGAN；
转置卷积、批处理规范（最后一层除外）和激活。
参数：
input_channels：输入特征表示有多少个通道
output_channels：输出特征表示应该有多少个通道
kernel_size：每个卷积滤波器的大小，相当于 (kernel_size, kernel_size)
stride：卷积的步幅
final_layer：布尔值，如果是最后一层则为 true，否则为 false
（影响激活和 batchnorm）
&#39;&#39;&#39;
如果不是 final_layer：
return nn.Sequential(
nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),
nn.BatchNorm2d(output_channels),
nn.LeakyReLU(0.2, inplace=True),
)
else：
return nn.Sequential(
nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),
nn.Tanh(),
)

def forward(self, noise):
&#39;&#39;&#39;
完成生成器前向传递的函数：给定一个噪声张量，
返回生成的图像。
参数：
noise：具有维度 (n_samples, input_dim) 的噪声张量
&#39;&#39;&#39;
x = noise.view(len(noise), self.z_dim, 1, 1)
print(f&#39;Gen: {x.shape}&#39;)
x = self.block1(x)
print(f&#39;After block1: {x.shape}&#39;)
x = self.block2(x)
print(f&#39;After block2: {x.shape}&#39;)
x = self.block3(x)
print(f&#39;After block3: {x.shape}&#39;)
x = self.block4(x)
print(f&#39;After block4: {x.shape}&#39;)
return x

def get_noise(n_samples, z_dim, device=&#39;cpu&#39;):
&#39;&#39;&#39;
用于创建噪声向量的函数：给定维度 (n_samples, z_dim)
创建该形状的张量，其中填充了来自正态分布的随机数。
参数：
n_samples：要生成的样本数，标量
z_dim：噪声向量的维度，标量
device：设备类型
&#39;&#39;&#39;
return torch.randn(n_samples, z_dim, device=device)

根据此处的公式，第一个块之后的结果将是(1 + 2x0 -1x(3-1) -1)/2 +1 = 0，但它显示 3x3。我在这里做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78949615/how-can-the-dimensions-of-a-conv2d-layer-be-calculated</guid>
      <pubDate>Wed, 04 Sep 2024 16:09:58 GMT</pubDate>
    </item>
    <item>
      <title>通过索引为张量赋值后，值不匹配</title>
      <link>https://stackoverflow.com/questions/78949501/mismatch-of-values-after-assigning-values-to-a-tensor-by-index</link>
      <description><![CDATA[我正在编写一个 PyTorch 训练代码，它构建了一个算法类。其中有一个步骤需要为内部张量分配一些值。但是，即使代码只有两行，也有一个错误。我发现分配的张量的值与分配的值不同。
这是该类的代码：
class PRODEN(Algorithm):
&quot;&quot;&quot;
PRODEN
参考：部分标签学习的真实标签的渐进式识别，ICML 2020。
&quot;&quot;&quot;

def __init__(self, input_shape, train_givenY, hparams):
super(PRODEN, self).__init__(input_shape, train_givenY, hparams)
self.featurizer = networks.Featurizer(input_shape, self.hparams)
self.classifier = networks.Classifier(
self.featurizer.n_outputs,
self.num_classes)

self.network = nn.Sequential(self.featurizer, self.classifier)
self.optimizer = torch.optim.Adam(
self.network.parameters(),
lr=self.hparams[&quot;lr&quot;],
weight_decay=self.hparams[&#39;weight_decay&#39;]
)
train_givenY = torch.from_numpy(train_givenY)
tempY = train_givenY.sum(dim=1).unsqueeze(1).repeat(1, train_givenY.shape[1])
label_confidence = train_givenY.float()/tempY
self.label_confidence = label_confidence

def update(self, minibatches):
_, x, strong_x, partial_y, _, index = minibatches
loss = self.rc_loss(self.predict(x), index)
self.optimizer.zero_grad()
loss.backward()
self.optimizer.step()
self.confidence_update(x, partial_y, index)
return {&#39;loss&#39;: loss.item()}

def rc_loss(self, output, index):
device = &quot;cuda&quot; if index.is_cuda else &quot;cpu&quot;
self.label_confidence = self.label_confidence.to(device)
logsm_outputs = F.log_softmax(outputs, dim=1)
#print(self.label_confidence.is_cuda)
final_outputs = logsm_outputs * self.label_confidence[index, :]
average_loss = - ((final_outputs).sum(dim=1)).mean()
return average_loss

def predict(self, x):
return self.network(x)

def confidence_update(self, batchX, batchY, batch_index):
with torch.no_grad():
batch_outputs = self.predict(batchX)
temp_un_conf = F.softmax(batch_outputs, dim=1)
&#39;&#39;&#39;有问题的代码开始了&#39;&#39;&#39;
self.label_confidence[batch_index, :] = temp_un_conf * batchY # un_confidence 存储每个示例的权重
&#39;&#39;&#39;问题代码结束&#39;&#39;&#39;
base_value = self.label_confidence.sum(dim=1).unsqueeze(1).repeat(1, self.label_confidence.shape[1])
self.label_confidence = self.label_confidence / base_value

问题出在 confidence_update 上。我发现
self.label_confidence[batch_index, :]

的值与
temp_un_conf * batchY

在此分配之后
self.label_confidence[batch_index, :] = temp_un_conf * batchY

仅适用于少数示例，但适用于大多数示例。例如，对于 1024 的批次大小，第一次迭代时大约有 4 个示例，之后会变得更大。我对这个问题非常沮丧，尝试了很多方法：

这个问题只存在于 CIFAR10 中，但其他数据集不存在这个问题。

所有张量的数据类型都是 Float32。

所有张量都在 gpu 上。


我的代码有什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/78949501/mismatch-of-values-after-assigning-values-to-a-tensor-by-index</guid>
      <pubDate>Wed, 04 Sep 2024 15:41:44 GMT</pubDate>
    </item>
    <item>
      <title>从一个巨大的文本文件和一行提示生成文章[关闭]</title>
      <link>https://stackoverflow.com/questions/78949493/generate-article-from-a-huge-text-file-and-a-one-line-prompt</link>
      <description><![CDATA[我想从一个大文本文件和一个一行提示中生成一篇大约 100 行的文章
第 1 行：敏捷的棕色狐狸跳过了懒狗。

第 2 行：困难中蕴藏着机遇。

第 3 行：生活就是当你忙于制定其他计划时发生的事情。

第 4 行：在这个不断试图让你成为其他东西的世界中做你自己是最大的成就。

第 5 行：成功不是终点，失败也不是致命的：继续前进的勇气才是最重要的。

第 6 行：敏捷的棕色狐狸吃肉。

提示：狐狸做了什么？

输出文章：敏捷的棕色狐狸跳过了懒狗。敏捷的棕色狐狸吃肉。

这只是给出的一个小例子。
你能改进我的代码或给我一个更好的解决方案吗？
我所说的改进是指我想对我的文本文件进行过度拟合。
# prepare_dataset.py

from transformers import GPT2Tokenizer
from datasets import Dataset
import pandas as pd

def load_and_prepare_dataset(file_path):
&quot;&quot;&quot;从文本文件加载并准备数据集。&quot;&quot;&quot;
使用 open(file_path, &#39;r&#39;) 作为文件：
text = file.read()

lines = text.split(&#39;.&#39;)

data = {&#39;text&#39;: lines} 
df = pd.DataFrame(data)

tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)

tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
return tokenizer(examples[&#39;text&#39;], padding=&#39;max_length&#39;, truncation=True, max_length=512)

dataset = Dataset.from_pandas(df)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.rename_column(&quot;input_ids&quot;, &quot;labels&quot;)

return tokenized_datasets

如果__name__ == &quot;__main__&quot;:
dataset = load_and_prepare_dataset(&#39;data.txt&#39;) 
dataset.save_to_disk(&#39;./dataset&#39;)

# train_model.py

from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
from datasets import load_from_disk

def fine_tune_model(train_dataset):
&quot;&quot;&quot;在给定的数据集上微调 GPT-2 模型。&quot;&quot;&quot;
tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)
model = GPT2LMHeadModel.from_pretrained(&#39;gpt2&#39;)

training_args = TrainingArguments(
output_dir=&#39;./results&#39;, # 输出目录
evaluation_strategy=&#39;epoch&#39;, # 要使用的评估策略
learning_rate=2e-5, # 学习率
per_device_train_batch_size=2, # 用于训练的批次大小
per_device_eval_batch_size=2, # 用于评估的批次大小
num_train_epochs=1, # 训练周期数
weight_decay=0.01, # 权重衰减强度
logs_dir=&#39;./logs&#39;, # 用于存储日志的目录
logs_steps=10, # 日志记录之间的步骤数
)

trainer = Trainer(
model=model, # 要训练的模型
args=training_args, # 参数用于训练
train_dataset=train_dataset, # 训练数据集
)

trainer.train()

def main():
dataset = load_from_disk(&#39;./dataset&#39;)
print(dataset.column_names)
print(dataset[0]) 

fine_tune_model(dataset)

if __name__ == &quot;__main__&quot;:
main()
]]></description>
      <guid>https://stackoverflow.com/questions/78949493/generate-article-from-a-huge-text-file-and-a-one-line-prompt</guid>
      <pubDate>Wed, 04 Sep 2024 15:40:27 GMT</pubDate>
    </item>
    <item>
      <title>如何设置 Kubernetes GPU 集群以与本地网络中不同 PC 上的 GPU 协同工作？[关闭]</title>
      <link>https://stackoverflow.com/questions/78947983/how-to-set-up-kubernetes-gpu-cluster-to-work-with-gpus-located-on-different-pcs</link>
      <description><![CDATA[我想在 Jupyter Notebook 中设置一个用于机器学习的 Kubernetes GPU 集群。我在一个本地网络中有 3 台配备 GPU（RTX 3060ti）的计算机，我想结合这些 GPU 的资源来运行神经网络训练和其他机器学习方法。
我的 RTX 3060ti 在解决某些神经网络训练任务时会执行太长的计算，我想用它来加速神经网络的训练。
我尝试搜索有关此主题的信息，但大多数文章都涉及将一个 GPU 的资源分配给不同的机器学习任务。]]></description>
      <guid>https://stackoverflow.com/questions/78947983/how-to-set-up-kubernetes-gpu-cluster-to-work-with-gpus-located-on-different-pcs</guid>
      <pubDate>Wed, 04 Sep 2024 09:48:45 GMT</pubDate>
    </item>
    <item>
      <title>基于上下文的向量搜索获取唯一性分数</title>
      <link>https://stackoverflow.com/questions/78946820/vector-search-to-get-a-uniqueness-score-based-on-context</link>
      <description><![CDATA[我有一篇带有标题和说明的博客文章，我想将其唯一性与 CSV 文件中的多个博客条目进行比较。CSV 包含多个博客，每个博客都有标题和元描述。
我目前使用 TF-IDF 矢量化和余弦相似度将单个博客与 CSV 文件中的所有条目进行比较。但是，这种方法仅基于确切的单词而不是上下文进行匹配。]]></description>
      <guid>https://stackoverflow.com/questions/78946820/vector-search-to-get-a-uniqueness-score-based-on-context</guid>
      <pubDate>Wed, 04 Sep 2024 04:27:53 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow fit 不会采用自定义数据集，但会引发错误：ValueError：无法获取具有未知等级的形状的长度</title>
      <link>https://stackoverflow.com/questions/78902994/tensorflow-fit-wont-take-custom-dataset-but-throws-error-valueerror-cannot-t</link>
      <description><![CDATA[我正在尝试训练一个直到最近才正常工作的模型。 fit 函数抛出了以下错误：
 &lt;ipython-input-20-01755a6ded38&gt; in &lt;cell line: 1&gt;()
----&gt; 1 model.fit(
2 dataset,
3 epochs=100,
4 verbose=1,
5 batch_size=8)

1 frames /usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 中引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py in squeeze_or_expand_to_same_rank(x1, x2, expand_rank_1)
105 def squeeze_or_expand_to_same_rank(x1, x2, expand_rank_1=True):
106 &quot;&quot;&quot;如果等级与预期相差恰好 1，则挤压/扩展最后一个 dim。&quot;&quot;&quot;
--&gt; 107 x1_rank = len(x1.shape)
108 x2_rank = len(x2.shape)
109 if x1_rank == x2_rank:

ValueError: 无法获取具有未知等级的形状的长度。

我尝试将生成器生成的 x 和 y 传递给 fit，并且运行良好，因此这不是形状问题。
以下是错误的重现。该模型只是一个简单的 Sequential 模型：
model = keras.models.Sequential(
[keras.layers.Dense(10,activation=&#39;relu&#39;),
keras.layers.Dense(1,activation=&#39;sigmoid&#39;)]
)
model.compile(loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

数据集由 tf.data.Dataset.from_generator() 生成，如下所示：
#随机数据集
a = tf.convert_to_tensor(np.random.randint(0,100,size=[10,10]))

#数据生成器类
class DataGenerator:
def __init__(self,data,ratio=3):
self._ratio = ratio
self._data = data

def __call__(self):
shape = tf.shape(self._data).numpy()
x = tf.convert_to_tensor(np.random.randint(1000,100000, size=[shape[0] * self._ratio, shape[1]]))
x = tf.concat([self._data, x], axis = 0)
y = tf.convert_to_tensor(np.random.random(shape[0]*(1 + self._ratio)))

产生 x, y

data_gen = DataGenerator(a, 3)
dataset = tf.data.Dataset.from_generator(
data_gen,
output_signature=(
tf.TensorSpec(shape=(None,10), dtype=tf.int32),
tf.TensorSpec(shape=(None), dtype=tf.float32)))

Model.fit() 产生了上述错误：
model.fit(
dataset,
epochs=100,
verbose=1,
batch_size=8)

以下是 Colab 中错误的重现：https://colab.research.google.com/drive/1f7I2St2U3LxaWZZSTxT2xWN14gCZMBSE?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/78902994/tensorflow-fit-wont-take-custom-dataset-but-throws-error-valueerror-cannot-t</guid>
      <pubDate>Thu, 22 Aug 2024 18:25:54 GMT</pubDate>
    </item>
    <item>
      <title>代码错误：模块“torch.nn”没有属性“ConvTranspose2D”</title>
      <link>https://stackoverflow.com/questions/77366473/errror-in-code-module-torch-nn-has-no-attribute-convtranspose2d</link>
      <description><![CDATA[def make_gen_block(self,input_channels,output_channels,kernel_size=3,stride=2,final_layer = False):
10 if not final_layer :
---&gt; 11 return nn.Sequential(nn.ConvTranspose2D(input_layer,output_layer,kernel_size,stride),
12 nn.BatchNorm2d(output_channels),
13 nn.ReLU(inplace = True),)

AttributeError: module &#39;torch.nn&#39; has no attribute &#39;ConvTranspose2D&#39;

解决方案是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77366473/errror-in-code-module-torch-nn-has-no-attribute-convtranspose2d</guid>
      <pubDate>Thu, 26 Oct 2023 11:11:39 GMT</pubDate>
    </item>
    </channel>
</rss>