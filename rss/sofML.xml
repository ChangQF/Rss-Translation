<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 23 Dec 2024 06:25:41 GMT</lastBuildDate>
    <item>
      <title>二进制交叉熵的实现给出不正常的结果</title>
      <link>https://stackoverflow.com/questions/79302179/implementation-of-binary-cross-entropy-gives-not-normal-result</link>
      <description><![CDATA[我正在尝试构建一个 NN，但在训练阶段，我得到的损失函数值异常。这是为什么？
哦，我只能使用 NumPy。
这就是数据看起来相似的方式：
 print(X_train.shape) #(784,800)
print(X_test.shape) #(784,200)
print(Y_train.shape) #(800,1)
print(Y_test.shape) #(200,1)

损失函数 - BCE
 def log_loss(y_hat, y):
m = y.shape[0]
epsilon = 1e-15 
y_hat = np.clip(y_hat, epsilon, 1 - epsilon) 

# loss = -1/m * (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))
损失 = -1/m * (np.dot(y.T,np.log(y_hat)) + np.dot((1-y).T, np.log(1-y_hat)))
回报损失

训练阶段 - 前向传播 + 反向传播
 input_layer = X_train.shape[0]
hidden_​​layer = 128
learning_rate = 0.01
epochs = 10

W1 = np.random.randn(hidden_​​layer, input_layer)
b1 = np.zeros((hidden_​​layer, 1))
W2 = np.random.randn(1, hidden_​​layer)
b2 = np.zeros((1, 1))

X = X_train
Y = Y_train
loss_list = []
epoch_list = []
num_of_examples = X.shape[1]

for i in range(epochs):
avg_epoch_loss = 0
for j in range(num_of_examples):

Z1 = np.matmul(W1,X[:,j].reshape(-1,1)) + b1 # 不要忘记添加偏差
A1 = sigmoid(Z1)
Z2 = np.matmul(W2,A1) + b2
A2 = sigmoid(Z2)
Yout = Y[j]

loss = log_loss( A2, Yout)
avg_epoch_loss = avg_epoch_loss + loss

dZ2 = (A2-Yout)
dW2 = np.matmul(dZ2,A1.T)
db2 = dZ2

dA1 = np.matmul(W2.T,dZ2)
dZ1 = dA1 * A1 * (1 - A1)
dW1 = np.matmul(dZ1,X[:,j].reshape(-1,1).T)
db1 = dZ1

W2 = W2 - 学习率 * dW2
b2 = b2 - 学习率 * db2
W1 = W1 - 学习率 * dW1
b1 = b1 - 学习率 * db1

avg_epoch_loss = avg_epoch_loss/num_of_examples
loss_list.append(avg_epoch_loss)
epoch_list.append(i)
print(&quot;Epoch&quot;, i,&quot;损失：&quot;，avg_epoch_loss)

这些是不正常的值 - 当然是寻找 0-1 之间的值：
Epoch 0 损失：[-18.37821485]
Epoch 1 损失：[-18.82406892]
Epoch 2 损失：[-18.82406892]
Epoch 3 损失：[-19.99316345]
Epoch 4 损失：[-29.91647793]
Epoch 5 损失：[-32.32075724]
Epoch 6 损失：[-32.89639034]
Epoch 7 损失：[-32.34691639]
Epoch 8 损失： [-32.749394]
第 9 阶段损失：[-33.61631871]
]]></description>
      <guid>https://stackoverflow.com/questions/79302179/implementation-of-binary-cross-entropy-gives-not-normal-result</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 ONNXRuntime C++ 优化 Florence-2 模型推理 - 生成循环的性能优化</title>
      <link>https://stackoverflow.com/questions/79302122/optimizing-florence-2-model-inference-with-onnxruntime-c-performance-optimiz</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79302122/optimizing-florence-2-model-inference-with-onnxruntime-c-performance-optimiz</guid>
      <pubDate>Mon, 23 Dec 2024 04:15:57 GMT</pubDate>
    </item>
    <item>
      <title>模型无法正常学习[关闭]</title>
      <link>https://stackoverflow.com/questions/79301299/model-cant-learn-normally</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79301299/model-cant-learn-normally</guid>
      <pubDate>Sun, 22 Dec 2024 16:24:38 GMT</pubDate>
    </item>
    <item>
      <title>单个卷积滤波器是否可以组合来自输入多个通道的值[关闭]</title>
      <link>https://stackoverflow.com/questions/79301125/can-single-convolutional-filter-combine-values-from-input-multiple-channels</link>
      <description><![CDATA[关于卷积神经网络的问题。
第 1 部分：假设输入是 RGB 图像，我们将其放入卷积层。人们是否曾经使用同时对输入的多个通道（例如 R 和 G）进行操作的过滤器，并在计算中结合两个通道的值？换句话说，过滤器矩阵是 3D（或更多）还是 2D？
第 2 部分：如果我错了，请纠正我，但有时在同一层中可以有多个过滤器。我的意思是，也许输入图像有 1 个通道（灰度），但层的输出有 2 个通道。这相当于有 2 个独立的过滤器，即 2 个不同的 3x3 矩阵，每个矩阵产生一个通道。
我理解过滤器是一个单一（例如 3x3）矩阵，我们在输入图像周围移动它并计算它“覆盖”的区域的某个加权平均值。]]></description>
      <guid>https://stackoverflow.com/questions/79301125/can-single-convolutional-filter-combine-values-from-input-multiple-channels</guid>
      <pubDate>Sun, 22 Dec 2024 14:04:26 GMT</pubDate>
    </item>
    <item>
      <title>我可以在 MacbookM4 上使用 CUDA 吗？[重复]</title>
      <link>https://stackoverflow.com/questions/79300848/can-i-use-cuda-on-macbookm4</link>
      <description><![CDATA[我正在尝试为我的本科论文创建一个动作检测系统。
我现在正尝试将 MMSkeleton 集成到我的项目管道中。https://github.com/open-mmlab/mmskeleton
要使用 MMSkeleton，我需要安装 PyTorch 和 torchvision（需要 CUDA），但据我所知，Mac 无法做到这一点。我收到此错误 OSError：编译 MMSkeleton 需要 CUDA！
有人知道我该如何克服这个障碍吗？]]></description>
      <guid>https://stackoverflow.com/questions/79300848/can-i-use-cuda-on-macbookm4</guid>
      <pubDate>Sun, 22 Dec 2024 10:49:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么当 refit=True 时，在 RandomizedSearchCV 之后会进行额外的拟合？</title>
      <link>https://stackoverflow.com/questions/79300159/why-is-an-additional-fitting-performed-after-randomizedsearchcv-when-refit-true</link>
      <description><![CDATA[我正在使用 scikit-learn 中的 RandomizedSearchCV 进行超参数调整，并注意到即使 refit 参数设置为 True，在找到最佳参数后也会执行额外的拟合步骤。
这是一个最小的可重现示例：
来自 sklearn.datasets 导入 make_classification
来自 sklearn.model_selection 导入 RandomizedSearchCV
来自 sklearn.ensemble 导入 RandomForestClassifier
来自 scipy.stats 导入 randint

# 生成合成数据集
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# 定义模型和参数分布
model = RandomForestClassifier(random_state=42)
param_dist = {
&#39;n_estimators&#39;: randint(10, 100),
&#39;max_depth&#39;: randint(3, 20),
}

# 执行 RandomizedSearchCV
search = RandomizedSearchCV(
model, param_dist, n_iter=10, cv=3, random_state=42, refit=True
)
search.fit(X, y)

# 访问最佳估计器
best_model = search.best_estimator_

print(best_model)

我理解 refit=True 标志表示在超参数调整后，应在整个数据集上重新拟合最佳模型。但是，为什么交叉验证中的“最佳模型”不直接作为 search.best_estimator_ 返回？对整个数据集执行另一个拟合步骤的原因是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79300159/why-is-an-additional-fitting-performed-after-randomizedsearchcv-when-refit-true</guid>
      <pubDate>Sat, 21 Dec 2024 21:49:41 GMT</pubDate>
    </item>
    <item>
      <title>使用 MaxViT 进行迁移学习时我的分类器应该是什么？</title>
      <link>https://stackoverflow.com/questions/79300055/what-should-be-my-classifier-in-transfer-learning-using-maxvit</link>
      <description><![CDATA[我正在尝试使用自定义数据集在 Pytorch 预训练模型上进行迁移学习。我已经能够使用 SqueezeNet 成功执行迁移学习。
对于 Squeezenet，我的分类器是，layers source
model.classifier = nn.Sequential(
nn.Dropout(p=0.2),
nn.Conv2d(512, len(class_names), kernel_size=1),
nn.ReLU(inplace=True),
nn.AdaptiveAvgPool2d((1, 1)))

对于 Efficientnet，我的分类器是，layers source
model.classifier = torch.nn.Sequential(
torch.nn.Dropout(p=0.2, inplace=True),
torch.nn.Linear(in_features=1280,
out_features=output_shape,
bias=True))

我也一直在尝试为 MaxViT 做类似的事情，我查看了源代码，发现参数中有 block_channels[-1]。我最近开始用这个，不知道它们是什么，layers source
self.classifier = nn.Sequential(
nn.AdaptiveAvgPool2d(1),
nn.Flatten(),
nn.LayerNorm(block_channels[-1]),
nn.Linear(block_channels[-1], block_channels[-1]),
nn.Tanh(),
nn.Linear(block_channels[-1], num_classes, bias=False),
)

如果需要，以下是我使用 squeezenet 执行迁移学习的完整代码，仅供参考。
weights = torchvision.models.SqueezeNet1_0_Weights.DEFAULT
model = torchvision.models.squeezenet1_0(weights=weights).to(device)
auto_transforms = weights.transforms()
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=d1,
test_dir=d2,
transform=auto_transforms,
batch_size=32)
for param in model.features.parameters():
param.requires_grad = False

torch.manual_seed(42)
torch.cuda.manual_seed(42)
output_shape = len(class_names)

model.classifier = nn.Sequential(
nn.Dropout(p=0.2),
nn.Conv2d(512, len(class_names), kernel_size=1),
nn.ReLU(inplace=True),
nn.AdaptiveAvgPool2d((1, 1))).to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
torch.manual_seed(42)
torch.cuda.manual_seed(42)
results = engine.train(model=model,
train_dataloader=train_dataloader,
test_dataloader=test_dataloader,
optimizer=optimizer,
loss_fn=loss_fn,
epochs=15,
device=device)

我的 MaxViT 分类器应该是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79300055/what-should-be-my-classifier-in-transfer-learning-using-maxvit</guid>
      <pubDate>Sat, 21 Dec 2024 20:24:49 GMT</pubDate>
    </item>
    <item>
      <title>ST-GCN 是否过时了？[关闭]</title>
      <link>https://stackoverflow.com/questions/79297114/is-st-gcn-outdated</link>
      <description><![CDATA[我正在尝试构建一个管道来跟踪视频监控录像中的异常情况。
为了对检测到的人的行为进行分类，我想使用 ST-GCN，但是我能找到的唯一文档是 5 年前更新的，但阅读最新研究 ST-GCN 仍在使用中。
有谁知道更新的文档或能给我一些提示，告诉我如何找到一些关于如何将其实现到我的管道中的信息？]]></description>
      <guid>https://stackoverflow.com/questions/79297114/is-st-gcn-outdated</guid>
      <pubDate>Fri, 20 Dec 2024 11:42:14 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：
&#39;super&#39; 对象没有属性 &#39;\_\_sklearn_tags__&#39;。

当我在 RandomizedSearchCV 对象上调用 fit 方法时会发生这种情况。我怀疑它可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我希望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch GRU 错误 RuntimeError：大小不匹配，m1：[1600 x 3]，m2：[50 x 20]</title>
      <link>https://stackoverflow.com/questions/66131870/pytorch-gru-error-runtimeerror-size-mismatch-m1-1600-x-3-m2-50-x-20</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/66131870/pytorch-gru-error-runtimeerror-size-mismatch-m1-1600-x-3-m2-50-x-20</guid>
      <pubDate>Wed, 10 Feb 2021 06:23:22 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的神经网络</title>
      <link>https://stackoverflow.com/questions/61845701/neural-network-in-python</link>
      <description><![CDATA[我最近开始尝试在不使用任何 NW 模块（如 Tensor Flow）的情况下创建自己的神经网络，但我无法将已定义的变量放入函数中，因此我将数据放入文本文件中，然后对其进行读写。虽然它不允许我将权重重新转换为 int，以便我可以将它们乘以它们的学习率。我收到一条错误消息，提示 int 不适用于基数。

你对此有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/61845701/neural-network-in-python</guid>
      <pubDate>Sun, 17 May 2020 01:07:59 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 对缺失标签的支持是什么</title>
      <link>https://stackoverflow.com/questions/58224649/what-is-lightgbms-support-for-missing-labels</link>
      <description><![CDATA[我们有一个数据集，其中某些标签缺失。我们最近才知道这一点，并且删除了这些行。这让我开始思考这到底是怎么回事？给 GBM 举一个没有标签的例子似乎没有意义。
有人能解释一下（双关语）LightGBM 如何处理缺少标签的行吗？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/58224649/what-is-lightgbms-support-for-missing-labels</guid>
      <pubDate>Thu, 03 Oct 2019 18:07:18 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中初始化权重？</title>
      <link>https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch</link>
      <description><![CDATA[如何初始化网络的权重和偏差（例如通过 He 或 Xavier 初始化）？]]></description>
      <guid>https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch</guid>
      <pubDate>Thu, 22 Mar 2018 16:34:42 GMT</pubDate>
    </item>
    <item>
      <title>神经网络（简单）</title>
      <link>https://stackoverflow.com/questions/46079541/neural-network-simple</link>
      <description><![CDATA[我很好奇为什么我没有打印任何输出，因为代码没有错误。
import numpy as np

class NN():
def _init_(self):
# 种子随机数生成器，因此每次程序运行时都会生成相同的数字
# np.random.seed(1)

# 模型单个神经元，具有 3 个输入连接和 1 个输出连接
# 将随机权重分配给 3x1 矩阵，值范围为 -1 到 1
# 平均值为 0
self.synaptic_weights = 2 * np.random.random((3, 1)) - 1

# 描述 s 形曲线我们传递输入的加权和
# 通过此函数将它们标准化为 0 和 1 之间
def __sigmoid(self, x):
return 1 / (1 + np.exp(-x))

# 梯度sigmoid 曲线
def __sigmoid_derivative(self, x):
return x * (1 - x)

def train(self, training_set_input, training_set_output, number_of_training_iterations):
for iteration in np.xrange(number_of_training_iterations):
# 将训练集通过神经网络
output = self.predict(training_set_input)

error = training_set_output - output

# 将误差乘以输入，再乘以 sigmoid 曲线的梯度
adjustment = np.dot(training_set_input.T, error * self.__sigmoid_derivative(output))

# 调整权重
self.synaptic_weights += adjustment

def predict(self, input):
# 将输入通过神经网络（单个神经元）
return self.__sigmoid(np.dot(inputs, self.synaptic_weights))

if __name__ == &quot;__NN__&quot;:
# 初始化单神经元神经网络
nn = NN()
weightz = nn.synaptic_weights
new_predict = nn.predict(np.array[1, 0, 0])

print(&quot;随机起始突触权重&quot;)
print(weightz)

# T 垂直翻转矩阵
training_set_input = np.array([0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1])
training_set_output = np.array([0, 1, 0, 0]).T

# 使用训练集训练网络
# 执行 10,000 次，每次进行小幅调整
nn.train(training_set_input, training_set_output, 10000)

print(&quot;新的起始突触权重&quot;)
print(weightz)

# 测试
print(&quot;预测&quot;)
print(new_predict)

将文件保存为 NN.py]]></description>
      <guid>https://stackoverflow.com/questions/46079541/neural-network-simple</guid>
      <pubDate>Wed, 06 Sep 2017 15:51:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 MLP 的神经网络分类器</title>
      <link>https://stackoverflow.com/questions/43238285/neural-network-classifier-using-mlp</link>
      <description><![CDATA[我正在开发一个 Python 应用程序，它使用一个数据集对扑克牌进行分类，我将发布一些片段。它似乎效果不佳。它无法正确地对牌进行分类。我得到了以下错误
第 298 行，在 fit 中
raise ValueError(&quot;Multioutput target data is not supports with &quot;
ValueError: Multioutput target data is not supports with label binarization

以下是我的代码：
import pandas as pnd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classes_report
training = pnd.read_csv(&quot;.idea/train.csv&quot;)
training.keys()
training.shape
X = np.array(training)
y = np.array(training)
X_train, X_test, y_train, y_test = train_test_split(X, y)
scaler = StandardScaler()
# 仅适合训练数据
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
mlp = MLPClassifier(hidden_​​layer_sizes=(30, 30, 30, 30, 30, 30, 30, 30, 30))
mlp.fit(X_train, y_train)
predictions = mlp.predict(X_test)
print(classification_report(y_test, predictions))
len(mlp.coefs_)
len(mlp.coefs_[0])
len(mlp.intercepts_[0])

以下是我使用的数据集示例：
图片在这里
这里是数据集的描述：
https://archive.ics.uci.edu/ml/datasets/Poker+Hand
有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/43238285/neural-network-classifier-using-mlp</guid>
      <pubDate>Wed, 05 Apr 2017 17:50:50 GMT</pubDate>
    </item>
    </channel>
</rss>