<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 05 Sep 2024 18:20:31 GMT</lastBuildDate>
    <item>
      <title>基于视觉的力估计</title>
      <link>https://stackoverflow.com/questions/78954291/vision-based-force-estimation</link>
      <description><![CDATA[我想通过图像处理来预测机械臂抓取物体的质量和力，我使用图片来提取物体的特征，然后预测抓取物体的力。
我需要这样的数据集，如果它包含物体的尺寸会更好。

此外，如果您对方法和技术有任何建议，我将非常乐意与我分享。
我尝试测量物体的宽度和高度，然后检测物体（使用神经网络），然后我给出了物体及其密度的列表。在检测到物体及其尺寸后，我使用质量 = 密度 * 体积来计算质量，然后使用 F = m * g 来计算其力
]]></description>
      <guid>https://stackoverflow.com/questions/78954291/vision-based-force-estimation</guid>
      <pubDate>Thu, 05 Sep 2024 17:53:24 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 模型准确率和损失不一致</title>
      <link>https://stackoverflow.com/questions/78954247/tensorflow-model-accuracy-and-loss-not-consistant</link>
      <description><![CDATA[我正在尝试训练一个具有 104 个特征的模型，这些特征全部具有二进制 1 或 0 值，并且输出也将是二进制值。我遇到了一些问题，准确率没有超过 64%，止损也没有像它应该的那样收敛。我有多个文件中超过 200 万行的数据。我只是保存模型并循环到下一个文件并继续训练。
我进行了大约 20 小时的训练，但仍然没有任何改善。我在 MacBook Pro M3 Max 上的 45 个 GPU 上运行这个。
这是我应该运行的最佳模型吗？
model = keras.Sequential()
### 模型
model.add(layers.LSTM(208, input_shape=(features, 1), return_sequences=True))
model.add(layers.Dropout(0.3))
model.add(layers.LSTM(104,activation=&#39;relu&#39;))
model.add(layers.Dropout(0.3))
model.add(layers.Dense(1,activation=&quot;sigmoid&quot;))
model.compile(loss=&quot;binary_crossentropy&quot;,optimizer=&quot;adam&quot;,
metrics[metrics.Accuracy()])
model.summary()

## 提前停止函数
early_stop = EarlyStopping(patience=5, restore_best_weights=True)

trainer = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=150, validation_split=0.20, validation_data=(x_test, y_test), callbacks=[early_stop])
]]></description>
      <guid>https://stackoverflow.com/questions/78954247/tensorflow-model-accuracy-and-loss-not-consistant</guid>
      <pubDate>Thu, 05 Sep 2024 17:38:32 GMT</pubDate>
    </item>
    <item>
      <title>如何从中断的地方继续训练模型？</title>
      <link>https://stackoverflow.com/questions/78954139/how-to-continue-training-a-model-from-where-it-left-off</link>
      <description><![CDATA[我想知道在训练文本分类模型时如何保存检查点，以便我可以从中断的地方继续训练。
我遇到了麻烦，不知道如何配置我的代码以使用适当的文件保存检查点，以便我可以从之前结束的位置继续训练，例如“trainer_state.json”。
这是我的训练代码：
def executar_treinamento(self, base_treinada):
training_args = TrainingArguments(
output_dir=self.output_dir,
learning_rate=2e-5,
per_device_train_batch_size=8,
per_device_eval_batch_size=8,
num_train_epochs=8,
weight_decay=0.01,
evaluation_strategy=&quot;epoch&quot;,
        save_strategy=“epoch”, save_only_model=False, load_best_model_at_end=False ) self.trainer = Trainer( model=self.model, args=training_args,compute_metrics=self.calcular_metricas, train_dataset=base_treinada[&#39;train&#39;], eval_dataset=base_treinada[&#39;validation&#39;], tokenizer=self.tokenizar_textos ) self.trainer.train() def avaliar_modelo(self, base_treinada): self.trainer.evaluate(base_treinada[&#39;test&#39;]) def salvar_modelo(self, caminho): self.model.save_pretrained(caminho)
self.tokenizer.save_pretrained(caminho)

我尝试使用以下参数：
 save_strategy=&quot;steps&quot;, # 每 X 步保存检查点
save_steps=80, # 自定义保存频率（以步数为单位）

但是，即便如此，带有“trainer_state.json”等文件的检查点仍未保存。]]></description>
      <guid>https://stackoverflow.com/questions/78954139/how-to-continue-training-a-model-from-where-it-left-off</guid>
      <pubDate>Thu, 05 Sep 2024 17:01:57 GMT</pubDate>
    </item>
    <item>
      <title>PINN 中的物理损失是如何计算的？</title>
      <link>https://stackoverflow.com/questions/78953649/how-is-physics-loss-calculated-in-pinns</link>
      <description><![CDATA[我正在研究物理信息神经网络 (PINN)，对物理损失的计算方式感到困惑。具体来说，在 1D 热方程示例中：
在此处输入图片说明
我知道该方程被平方以给出正输出，但不确定实际应用原始 1D 热方程如何计算成本。
假设输入是预测值，PINN 究竟如何知道“偏离多远”或预测是否在物理约束范围内？]]></description>
      <guid>https://stackoverflow.com/questions/78953649/how-is-physics-loss-calculated-in-pinns</guid>
      <pubDate>Thu, 05 Sep 2024 14:49:12 GMT</pubDate>
    </item>
    <item>
      <title>如果数据集较小，是否建议拟合线性回归模型并在不拆分数据集的情况下计算分数？[关闭]</title>
      <link>https://stackoverflow.com/questions/78953584/is-fitting-the-linear-regression-model-and-calculating-the-score-without-splitti</link>
      <description><![CDATA[我一直在阅读有关随机梯度下降的教程，网址为 https://towardsdatascience.com/stochastic-gradient-descent-explained-in-real-life-predicting-your-pizzas-cooking-time-b7639d5e6a32

在代码示例中，作者使用“feature_array”和“target array”拟合模型，而没有使用 train_test_split() 方法将其拆分为训练和测试数据集。

我知道这里的数据集很小，但真的推荐这样做吗？
Q1。最重要的是，在用于训练模型的同一数据集上计算 r2_score 即“r_squared”的原因是什么？
Q2。这个分数值与“预测（预测烹饪时间）”有何关系？
Q3。如果我想使用 &#39;sklearn.metrics&#39; 中定义的 r2_score() 方法计算 r2_score，那么我该怎么做？
以下是本教程中的代码：
import numpy as np
import time
from sklearn.linear_model import SGDRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

def stochastic_gd(feature_array,
target_array, 
to_predict, 
learn_rate_type=&quot;invscaling&quot;):

start_time = time.time()
lin_reg_pipeline = make_pipeline(StandardScaler(), 
SGDRegressor(learning_rate=learn_rate_type))
lin_reg_pipeline.fit(feature_array, target_array)
stop_time = time.time()
print(f&quot;总运行时间：{start_time - stop_time:.6f}s&quot;)
print(f&quot;学习率算法：{learn_rate_type}&quot;)

print(f&quot;模型系数：{lin_reg_pipeline[1].coef_}&quot;)
print(f&quot;迭代次数：{lin_reg_pipeline[1].n_iter_}&quot;)
return lin_reg_pipeline

feature_array = [[500, 80, 30, 10],
[550, 75, 25, 0],
[475, 90, 35, 20],
[450, 80, 20,25],
[465, 75, 30, 0],
[525, 65, 40, 15],
[400, 85, 33, 0],
[500, 60, 30, 30],
[435, 45, 25, 0]]

target_array = [17, 11, 21, 23, 22, 15, 25, 18, 16]
to_predict = [[510, 50, 35, 10]]
lin_reg = stochastic_gd(feature_array, target_array, to_predict)
prediction = lin_reg.predict(to_predict)
print(f&quot;预测烹饪时间：{np.round(prediction, 0)[0].astype(&#39;int&#39;)} min&quot;)
r_squared = lin_reg.score(feature_array, target_array).reshape(-1, 1)[0][0]

print(f&quot;R-squared: {np.round(r_squared, 2)}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78953584/is-fitting-the-linear-regression-model-and-calculating-the-score-without-splitti</guid>
      <pubDate>Thu, 05 Sep 2024 14:34:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 Scikit Learn 在 Vertex 上导入模型</title>
      <link>https://stackoverflow.com/questions/78953273/importing-a-model-with-scikit-learn-on-vertex</link>
      <description><![CDATA[我尝试从本地导入模型，但每次我都会从 gcp 日志中收到相同的错误。框架是 scikit-learn
AttributeError: 无法从 &#39;/usr/app/model_server.py&#39;&gt; 获取 &lt;module &#39;model_server&#39; 上的属性 &#39;preprocess_text&#39; 
存在此问题的代码片段是
complaints_clf_pipeline = Pipeline(
[
(&quot;preprocess&quot;, text.TfidfVectorizer(preprocessor=utils.preprocess_text, ngram_range=(1, 2))),
(&quot;clf&quot;, naive_bayes.MultinomialNB(alpha=0.3)),
]
)

这个
preprocess_text 

来自上面的单元格，但我一直收到此问题，model_server 不存在于我的代码中。
有人可以帮忙吗？
我尝试重构代码但得到了同样的错误，尝试撤消此管道结构，但在尝试通过 API 查阅模型时又收到另一个错误。]]></description>
      <guid>https://stackoverflow.com/questions/78953273/importing-a-model-with-scikit-learn-on-vertex</guid>
      <pubDate>Thu, 05 Sep 2024 13:24:34 GMT</pubDate>
    </item>
    <item>
      <title>使用自己的多视图图像绕过 zero123 来增强 InstantMesh 3d 重建输出的纹理</title>
      <link>https://stackoverflow.com/questions/78952999/bypass-zero123-with-own-multiview-images-to-enhance-texture-of-instantmesh-3d-re</link>
      <description><![CDATA[我正在使用 InstantMesh，这是一个 3D 重建管道，它使用多视图模型 (zero123)，可以从一个输入图像生成多视图图像。
我和我的团队正在尝试增强 instantmesh 3D 重建纹理输出，经过多次尝试，我们发现最大的问题之一是 zero123 输出（输入重建管道）的分辨率太低。
我们现在的目标是使用我们自己的分辨率更高的多视图图像。
InstantMesh 重建管道是否接受更大的分辨率？如果是，代码中需要更改什么？如果没有，我们是否必须重新训练整个模型以考虑更大的分辨率？
这是运行管道的 InstantMesh 代码：https://github.com/TencentARC/InstantMesh/blob/main/run.py]]></description>
      <guid>https://stackoverflow.com/questions/78952999/bypass-zero123-with-own-multiview-images-to-enhance-texture-of-instantmesh-3d-re</guid>
      <pubDate>Thu, 05 Sep 2024 12:19:51 GMT</pubDate>
    </item>
    <item>
      <title>在LSTM神经网络中可以使用类似于SHAP值的东西吗？</title>
      <link>https://stackoverflow.com/questions/78952839/what-can-be-used-in-lstm-neural-network-similar-to-shap-values</link>
      <description><![CDATA[在使用长短期记忆 (LSTM) 神经网络执行顺序数据任务的情况下，了解不同输入特征的贡献或重要性对于模型的可解释性至关重要。虽然已经采用了特征重要性技术，但当前的挑战是探索其他指标或方法，以深入了解各个特征如何影响模型的预测。
解决这一挑战的一种潜在方法是应用 SHAP (SHapley Additive exPlanations) 值，这些值通常用于传统机器学习模型中的特征归因。SHAP 值通过将每个特征的贡献归因于预测来提供一种解释模型输出的一致方法。但是，由于 LSTM 网络涉及跨时间步骤的顺序依赖关系和复杂交互，因此实施 SHAP 或类似方法需要适应性或替代可解释性技术，这些技术适用于深度学习模型，尤其是处理时间序列的模型。
目标是识别和实施超越传统特征重要性的指标，重点关注可以解释输入数据的时间结构的方法，并提供更细致的见解，了解每个特征如何影响 LSTM 的预测。
尝试的内容：我们应用传统的特征重要性指标来了解不同的输入特征如何影响 LSTM 神经网络的预测。此外，我们考虑使用机器学习模型中常用的 SHAP 值，以更详细的方式归因特征重要性。
预期结果：我们预计 SHAP 值或类似的可解释性方法将有助于解释每个特征对 LSTM 预测的影响，从而清楚地了解时间特征如何随时间影响模型的输出。
实际结果：虽然特征重要性让我们大致了解哪些特征有影响，但由于 LSTM 网络的顺序性和时间依赖性，SHAP 值更难实现。我们发现为更简单的模型设计的 SHAP 方法没有完全捕捉到 LSTM 固有的时间依赖性的复杂性。因此，模型预测的可解释性仍然有限，需要替代方法。]]></description>
      <guid>https://stackoverflow.com/questions/78952839/what-can-be-used-in-lstm-neural-network-similar-to-shap-values</guid>
      <pubDate>Thu, 05 Sep 2024 11:41:07 GMT</pubDate>
    </item>
    <item>
      <title>无法让 XGBRegressor 输出 0 到 1 之间的值</title>
      <link>https://stackoverflow.com/questions/78952638/unable-to-get-xgbregressor-to-output-values-between-0-and-1</link>
      <description><![CDATA[我们创建了一个应用程序，在该应用程序中，我们为客户提供贷款优惠，客户可以根据其用例更改金额。现在，我正在尝试制作一个 XBGRegressor 模型，该模型可以预测客户的接受率金额，这将在下一个过程中进一步使用。
我使用的特征在某些列中具有空值，因此我制作了 XGBoost Regressor，因为它可以轻松处理空值。将平均值代入这些列是不可能的。我的训练数据的接受率在 0 到 1 的范围内，但我的模型预测的值仍然大于 1 甚至为负数。
我正在使用 Baysian Optimiser 来改进模型。 R 平方值不错（约为 0.75），有什么方法可以进一步改进吗？
这是我目前正在使用的代码。
def get_forecast(just_train_df, metric, data_df):
print(&#39;\nMetrics are:&#39;,metric)
data = just_train_df.copy()
X, y, X_train, y_train, X_test, y_test, X_forecast, y_original = data_preprocessing(data, metric, [])

pbounds = {
&#39;max_depth&#39;: (3, 10),
&#39;learning_rate&#39;: (0.01, 0.3),
&#39;gamma&#39;: (0, 0.5),
&#39;min_child_weight&#39;: (1, 10),
&#39;subsample&#39;: (0.5, 1.0),
&#39;colsample_bytree&#39;: (0.5, 1.0),
&#39;reg_alpha&#39;: (0, 1.0),
&#39;reg_lambda&#39;: (0, 1.0)
}

# 贝叶斯优化
def xgb_cv(max_depth, learning_rate, gamma, min_child_weight, subsample, colsample_bytree, reg_alpha, reg_lambda):

xgb_model = XGBRegressor(
objective=&#39;reg:squarederror&#39;,
eval_metric=&#39;rmse&#39;,
max_depth=int(max_depth),
learning_rate=learning_rate,
gamma=gamma,
min_child_weight=int(min_child_weight),
subsample=subsample,
colsample_bytree=colsample_bytree,
reg_alpha=reg_alpha,
reg_lambda=reg_lambda,
n_jobs=-1
)
r2_scorer = make_scorer(r2_score)
scores = cross_val_score(xgb_model, X_train, y_train,scoring=r2_scorer, cv=5, n_jobs=-1)
return scores.mean() # 返回负均方误差，因为 BayesianOptimization 最小化了目标函数

optimizer = BayesianOptimization(
f=xgb_cv,
pbounds=pbounds,
random_state=36,
verbose=2
)

print(&#39;Initiated Bayesian Optimizer...\n&#39;)
optimizer.maximize(init_points=5, n_iter=50)
print(&#39;\nCompleted Bayesian Optimizer\n&#39;)

print(&quot;Best hyperparameters: &quot;, optimizer.max[&#39;params&#39;], &#39;\n&#39;)

# 使用 Bayesian Optimizer 中的最佳参数训练模型
best_params = optimizer.max[&#39;params&#39;]
final_model = XGBRegressor(
objective=&#39;reg:squarederror&#39;,
eval_metric=&#39;rmse&#39;,
max_depth=int(best_params[&#39;max_depth&#39;]),
learning_rate=best_params[&#39;learning_rate&#39;],
gamma=best_params[&#39;gamma&#39;],
min_child_weight=int(best_params[&#39;min_child_weight&#39;]),
subsample=best_params[&#39;subsample&#39;],
colsample_bytree=best_params[&#39;colsample_bytree&#39;],
reg_alpha=best_params[&#39;reg_alpha&#39;],
reg_lambda=best_params[&#39;reg_lambda&#39;],
n_jobs=-1,
random_state=0
)

final_model.fit(X_train, y_train)

# 数据集上的预测
y_pred_test = final_model.predict(X_test)
y_pred_train = final_model.predict(X_train)
y_pred_val = final_model.predict(X_forecast)

# 在训练数据上评估 best_model
mae = mean_absolute_error(y_train, y_pred_train)
rmse = mean_squared_error(y_train, y_pred_train, squared=False)
r2_train = r2_score(y_train, y_pred_train)
print(f&quot;训练平均绝对误差：{mae}&quot;)
print(f&quot;训练 R 平方：{r2_train}\n&quot;)

# 在测试数据上评估模型
mae = mean_absolute_error(y_test, y_pred_test)
rmse = mean_squared_error(y_test, y_pred_test, squared=False)
r2_test = r2_score(y_test, y_pred_test)
print(f&quot;测试平均绝对误差：{mae}&quot;)
print(f&quot;测试 R 平方：{r2_test}\n&quot;)

目前我面临的一个主要问题是大约 65-70% 的数据采用比率为 1。我应该更改模型还是对现有模型进行更改？我应该在 XGBoost 中将输入特征标准化为 0 和 1 吗？]]></description>
      <guid>https://stackoverflow.com/questions/78952638/unable-to-get-xgbregressor-to-output-values-between-0-and-1</guid>
      <pubDate>Thu, 05 Sep 2024 10:48:03 GMT</pubDate>
    </item>
    <item>
      <title>分类模型仅预测单个类别[关闭]</title>
      <link>https://stackoverflow.com/questions/78952023/classification-model-only-predicts-single-class</link>
      <description><![CDATA[我有一个逻辑回归模型，我正在尝试在我的 NLP 项目中做出健康的预测。我做了一些事情，下面是我的分类报告：
 精确率 召回率 f1 分数 支持率

0 0.68 0.83 0.75 23
1 0.78 0.78 0.78 23
2 0.87 0.87 0.87 30
3 0.76 0.62 0.68 26

准确率 0.77 102
宏平均值 0.77 0.77 0.77 102
加权平均值 0.78 0.77 0.77 102

我认为它实际上没有看起来那么糟糕，但即使我以不同的方式设置参数，大多数时候它也会返回“3”，这是我的一个类。我的意思是，它看起来总是专注于一个单一的类别，即使我的分类报告不支持这种行为。
总之，我的模型看起来很平衡，但它的表现却不是那样。我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78952023/classification-model-only-predicts-single-class</guid>
      <pubDate>Thu, 05 Sep 2024 08:30:57 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Keras 中将 flow_from_directory 与多个目录结合使用，实现多输出神经网络</title>
      <link>https://stackoverflow.com/questions/78951880/how-to-use-flow-from-directory-with-multiple-directories-for-multi-output-neural</link>
      <description><![CDATA[这是我在这里的第一个问题，所以如果有任何不清楚的地方，我深表歉意。
我正在做一个项目，需要使用 Keras 中的 flow_from_directory 从多个目录加载图像。我的目录结构如下：
Images_folder/
═── Carpet_1/
│ ═── training/
│ │ ═── class_1/
│ │ ═── class_2/
│ ═── validation/
═── Carpet_2/
│ ═── training/
│ │ ═── class_1/
│ │ ═── class_2/
│ ═── validation/
...
每个“Carpet”目录（例如 Carpet_1、Carpet_2）包含相同的类集（class_1、class_2 等）。我想使用来自所有这些目录的图像来训练 CNN。我的目标是构建一个多输出神经网络，其中一个输出预测“地毯”编号（1、2、3、...），另一个输出预测该地毯内的类别。
鉴于这种结构，我如何使用 ImageDataGenerator 或 Keras 中的任何其他方法来加载和预处理这些图像？有没有办法将来自所有这些目录的图像组合成一个生成器，同时仍然允许我区分不同的地毯？
任何关于如何解决这个问题的指导都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78951880/how-to-use-flow-from-directory-with-multiple-directories-for-multi-output-neural</guid>
      <pubDate>Thu, 05 Sep 2024 07:56:05 GMT</pubDate>
    </item>
    <item>
      <title>级联分段-通道设置是否正确？</title>
      <link>https://stackoverflow.com/questions/78951423/cascade-segmentation-are-the-channels-set-up-correctly</link>
      <description><![CDATA[我想训练一个机器学习模型，用于处理 2D DICOM 图像中的精细蒙版细节。我有 500 张图像准备进行标记/注释。我可以使用这种技术吗？还是我理解错了？
鱼 + 脊椎的注释

我用 1 个类别注释了 500 张图像：鱼。然后我训练一个 model1.pth，将鱼与背景区分开来。该模型有 2 个 out_channels：鱼和背景。

我再次注释了相同的 500 张图像，但现在有 2 个类别：鱼 + 脊椎。我加载 model1.pth，并创建一个具有 2 个输入通道 和 3 个输出通道 的模型：脊柱、鱼和背景，并将模型保存为 model2.pth

最后，我再次注释了 500 张图像，但现在我包括了变形。如果我有 3 种类型的变形，则每种变形都有自己的类别。我加载 model2.pth，创建一个具有 3 个输入通道 和 6 个输出通道 的模型：背景、鱼、脊柱、变形 1、变形 2、变形 3，并将模型保存为 model3.pth。

现在模型可以直接在新图像上使用。是这样吗？


背景和细节。我尝试过什么
目标是找到鱼脊椎的变形。到目前为止，我已尝试通过使用 MONAI 的 UNet 模型 来分割 3 个类别 + 背景。图像是转换为 NifTi 格式 (.dcm.nii.gz) 的 2D DICOM 图像，典型尺寸为 2000x900 像素。我使用 3Dslicer 进行注释。到目前为止的类别：

背景

鱼

脊椎

变形


到目前为止，我已经在（仅）12 张训练图像上进行了测试，只是为了让它运行，我得到了所有 3 个类别的结果，但我猜模型训练过度了。此外，我猜这种技术使得在训练结束后进行微小更改变得更加困难。例如，我想要多种不同类型的变形。 
我的结果：红线左侧：来自 tensorboard，红线右侧：在新图像上测试模型
在开始注释 500 张图像之前，我想验证我是否走在正确的道路上。我希望通过使用级联技术，我可以获得一个可以轻松分割鱼和脊椎的模型，并且我可以随后尝试不同的变形注释。]]></description>
      <guid>https://stackoverflow.com/questions/78951423/cascade-segmentation-are-the-channels-set-up-correctly</guid>
      <pubDate>Thu, 05 Sep 2024 05:34:37 GMT</pubDate>
    </item>
    <item>
      <title>具有共享权重的嵌套模块是否应为 nn.Module 对象参数？</title>
      <link>https://stackoverflow.com/questions/78950394/should-nested-modules-with-shared-weights-be-an-nn-module-object-parameter-or-no</link>
      <description><![CDATA[我希望两个 torch.nn.Module 类共享其部分架构和权重，如下例所示：
from torch import nn

class SharedBlock(nn.Module):
def __init__(self, *args, **kwargs):
super().__init__()

self.block = nn.Sequential(
# 在此处定义一些块架构...
)

def forward(self, x):
return self.block(x)

class MyNestedModule(nn.Module):
def __init__(self, shared_block: nn.Module, *args, **kwargs):
super().__init__()

self.linear = nn.Linear(...)
self.shared_block = shared_block

def forward(self, x):
return self.shared_block(self.linear(x))

class MyModule(nn.Module):
def __init__(self, *args, **kwargs):
super().__init__()

# 应该是：
shared_block = SharedBlock(*args, **kwargs)
# 或者：
self.shared_block = SharedBlock(*args, **kwargs) # 注意：self。
# ...如果有区别，区别是什么？

self.nested1 = MyNestedModule(shared_block, *args, **kwargs)
self.nested2 = MyNestedModule(shared_block, *args, **kwargs)

def forward(self, x):
x_1, x_2 = torch.split(x, x.shape[0] // 2, dim=0)
y_1 = self.nested1(x_1)
y_2 = self.nested2(y_2)
return y_1, y_2

我想知道 shared_block 是否应该是 MyModule 的对象参数。我认为不是，因为它在 MyNestedModule 类对象中都被设置为对象参数，所以它应该在 torch grad 中注册，但如果我确实在 MyModule 中将它创建为对象参数，会发生什么？]]></description>
      <guid>https://stackoverflow.com/questions/78950394/should-nested-modules-with-shared-weights-be-an-nn-module-object-parameter-or-no</guid>
      <pubDate>Wed, 04 Sep 2024 20:06:25 GMT</pubDate>
    </item>
    <item>
      <title>如何计算伯努利朴素贝叶斯的联合对数似然</title>
      <link>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</link>
      <description><![CDATA[对于使用 BernoulliNB 的分类问题，如何计算联合对数似然。联合似然由以下公式计算，其中 y(d) 是实际输出（不是预测值）的数组，x(d) 是特征的数据集。
我阅读了这个答案并阅读了文档，但它并没有完全满足我的目的。有人可以帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</guid>
      <pubDate>Wed, 17 Oct 2018 18:08:50 GMT</pubDate>
    </item>
    <item>
      <title>将索引数组转换为 NumPy 中的独热编码数组</title>
      <link>https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-one-hot-encoded-array-in-numpy</link>
      <description><![CDATA[给定一个 1D 索引数组：
a = array([1, 0, 3])

我想将其独热编码为 2D 数组：
b = array([[0,1,0,0], [1,0,0,0], [0,0,0,1]])
]]></description>
      <guid>https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-one-hot-encoded-array-in-numpy</guid>
      <pubDate>Thu, 23 Apr 2015 18:24:54 GMT</pubDate>
    </item>
    </channel>
</rss>