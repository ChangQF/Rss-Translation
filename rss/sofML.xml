<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 21 Jun 2024 03:18:01 GMT</lastBuildDate>
    <item>
      <title>通过几个步骤优化一个过程：如果我们使用一个模型几次才能够计算出损失，那么如何训练它？</title>
      <link>https://stackoverflow.com/questions/78650011/optimizing-a-process-in-several-steps-how-to-train-a-model-if-we-use-it-severa</link>
      <description><![CDATA[我正在做一个项目，其中有一个包含一定步骤的流程；假设是 3 个步骤。

我们从一个初始化的零矩阵 M0 开始，该矩阵描述了系统的状态，并且必须采取一个行动，其后果是随机的，但受到该行动的强烈影响
我们更新矩阵 (M1)，然后再次采取行动
我们再次更新矩阵 (M2)，采取最后一个行动
现在我们才可以从最后一个矩阵 M3 计算损失，因此我们可以评估我们的策略

我已经建立了一个神经网络，其中状态矩阵是输入，输出是决定采取哪种行动的权重列表。从我（初学者）的理解来看，整个过程有点像循环神经网络，但有额外的步骤。
我用 Python 实现了这个过程，但当我尝试训练模型时，GradientTape() 看起来不太好，因为我的变量从未计算过梯度；我不确定，但我认为损失的随机性使得梯度不可计算（计算权重的损失远非易事，因为权重放在图的边缘，我们对其执行算法）。
我曾想过在没有权重列表的情况下进行强化学习以采取行动，但我不知道奖励的随机性会有多好。此外，我从来没有做过这样的事情，我只有一周的时间来实现一切。动作空间也相当大，因此这种方法存在很多不确定性。
有没有什么常见的做法来解决这类问题？
谢谢！
最好]]></description>
      <guid>https://stackoverflow.com/questions/78650011/optimizing-a-process-in-several-steps-how-to-train-a-model-if-we-use-it-severa</guid>
      <pubDate>Fri, 21 Jun 2024 00:18:44 GMT</pubDate>
    </item>
    <item>
      <title>如何去除车窗玻璃 bg？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78649936/how-can-remove-car-window-glass-bg</link>
      <description><![CDATA[我正在使用 rembg 去除背景，但问题是车窗玻璃的背景没有被去除。在这种情况下，如何去除车窗玻璃的背景。有人能帮帮我吗？
--&gt;当前结果图像
--&gt;预期结果图像
output_data = rembg.remove(input_data)
]]></description>
      <guid>https://stackoverflow.com/questions/78649936/how-can-remove-car-window-glass-bg</guid>
      <pubDate>Thu, 20 Jun 2024 23:37:59 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在 keras 中加载自定义模型时会出现 TypeError</title>
      <link>https://stackoverflow.com/questions/78649700/why-am-i-getting-a-typeerror-while-loading-a-custom-model-in-keras</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78649700/why-am-i-getting-a-typeerror-while-loading-a-custom-model-in-keras</guid>
      <pubDate>Thu, 20 Jun 2024 21:45:02 GMT</pubDate>
    </item>
    <item>
      <title>决策树回归器输出</title>
      <link>https://stackoverflow.com/questions/78649554/decision-tree-regressor-output</link>
      <description><![CDATA[我有一个非常简单的数据集，其中员工年龄和工作年限作为特征，收入作为标签。要求使用各种回归器预测收入水平，我使用了 4 个：决策树 (DT)、随机森林 (RF)、K-最近邻 (KNN) 和线性回归 (LR)。下表给出了 4 个回归器预测的收入水平。
我的问题是：

为什么 DT 预测的收入水平如此严格（几乎等于数据集的实际收入），而其他回归器似乎运行良好？是因为数据太小，DT 无法训练 / 特征数量太少（只有 2 个特征）还是其他原因？
为什么 DT 给出的结果很荒谬，而 RF 却没有？
DT 的 R^2 和 MSE 分别小于和大于 LR，但 DT 似乎仍然产生了更紧密的范围。怎么做？



这里不包括任何代码，因为这更像是一个上下文问题。如果需要，可以提供代码。]]></description>
      <guid>https://stackoverflow.com/questions/78649554/decision-tree-regressor-output</guid>
      <pubDate>Thu, 20 Jun 2024 20:59:14 GMT</pubDate>
    </item>
    <item>
      <title>具有二元结果预测的单一时间序列</title>
      <link>https://stackoverflow.com/questions/78648701/single-time-series-with-binary-outcome-prediction</link>
      <description><![CDATA[我有全天股票价格的时间序列和与这些价格相关的二元结果变量（行动），即买入/不行动。
目标是预测股票价格和行动变量之间是否存在相关性。
不确定逻辑回归或决策树是否是可行的方法。你认为哪一个更有意义？
考虑到没有太多节点可以使用，决策树是否可行？
我认为随机森林会有点矫枉过正。
仅供参考，我不熟悉 NN。
请就对此类数据建模的最佳方法提供建议。]]></description>
      <guid>https://stackoverflow.com/questions/78648701/single-time-series-with-binary-outcome-prediction</guid>
      <pubDate>Thu, 20 Jun 2024 16:33:58 GMT</pubDate>
    </item>
    <item>
      <title>可从头定制的神经网络</title>
      <link>https://stackoverflow.com/questions/78648642/neural-network-customisable-from-scratch</link>
      <description><![CDATA[我从头开始创建了一个神经网络。仅使用 numpy 库。但损失函数没有收敛，并且准确率不稳定。请帮助我找出问题所在。如果您能给我解决方案的话
这是我的代码
# 计算分类器准确率的函数
def accuracy(output, y):
a = np.argmax(output, axis=0)

return np.mean(a == y)

# 返回初始权重和基准的函数
def initialisation(t):
parameters = {}
np.random.seed(1)
for i in range(len(t) - 1):
parameters[&#39;w&#39; + str(i + 1)] = np.random.randn(t[i + 1], t[i])
parameters[&#39;b&#39; + str(i + 1)] = np.zeros((t[i + 1], 1))
返回参数

def relu_activation(x):
返回 np.maximum(0, x)

def relu_derivative(x):
返回 np.where(x &gt; 0, 1, 0)

def softmax_activation(z):
z = z - np.max(z, axis=0, keepdims=True)
z = np.exp(z)
softmax = z / np.sum(z, axis=0, keepdims=True)
返回 softmax

def sigmoid_activation(x):
返回 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
返回 np.exp(-x) / (1 + np.exp(-x)) ** 2

def derive(x, num):
if num == 1:
return relu_derivative(x)
elif num == 3:
return sigmoid_derivative(x)

#根据数字 n 选择激活函数的函数
def activate(z, n):
if n == 1:
return relu_activation(z)
elif n == 2:
return softmax_activation(z)
elif n == 3:
return sigmoid_activation(z)

#计算损失函数的函数
def loss_function(a_last, y):
output = np.clip(a_last, 1e-7, 1 - 1e-7)
num = y.shape[1]
loss = -np.mean(np.log(output[np.argmax(y, axis=0), np.arange(num)]))
return loss

def Z(x, w, b):
return np.dot(w, x) + b

#实现前向传播的函数
def forward(x, parameters, configuration, t):
cache = {&#39;a0&#39;: x}
for i in range(len(t) - 1):
cache[&#39;z&#39; + str(i + 1)] = Z(cache[&#39;a&#39; + str(i)], parameters[&#39;w&#39; + str(i + 1)], parameters[&#39;b&#39; + str(i + 1)])
cache[&#39;a&#39; + str(i + 1)] = activate(cache[&#39;z&#39; + str(i + 1)], configuration[i])
return cache

#函数实现反向传播
def behind(cache, t, configuration, parameters, y):
m = cache[&#39;a&#39; + str(len(t) - 1)].shape[1]
grad = {&#39;dw&#39; + str(len(t) - 1): np.dot((cache[&#39;a&#39; + str(len(t) - 1)] - y), cache[&#39;a&#39; + str(len(t) - 2)].T) / m,
&#39;db&#39; + str(len(t) - 1): np.sum(cache[&#39;a&#39; + str(len(t) - 1)] - y, axis=1, keepdims=True) / m,
&#39;dz&#39; + str(len(t) - 1): cache[&#39;a&#39; + str(len(t) - 1)] - y}

for i in range(len(t) - 2, 0, -1):
grad[&#39;dz&#39; + str(i)] = np.dot(parameters[&#39;w&#39; + str(i + 1)].T, grad[&#39;dz&#39; + str(i + 1)] * derive(cache[&#39;z&#39; + str(i + 1)], configuration[i - 1]))
grad[&#39;dw&#39; + str(i)] = np.dot(grad[&#39;dz&#39; + str(i)], cache[&#39;a&#39; + str(i - 1)].T) / m
grad[&#39;db&#39; + str(i)] = np.sum(grad[&#39;dz&#39; + str(i)], axis=1, keepdims=True) / m

return grad

# 更新权重和基准的函数
def update(learning_rate, parameters, grad, t):
for i in range(len(t) - 1):
参数[&#39;w&#39; + str(i + 1)] = np.clip(参数[&#39;w&#39; + str(i + 1)] - learning_rate * grad[&#39;dw&#39; + str(i + 1)], -10, 10)
参数[&#39;b&#39; + str(i + 1)] = np.clip(参数[&#39;b&#39; + str(i + 1)] - learning_rate * grad[&#39;db&#39; + str(i + 1)], -10, 10)
返回参数

#学习函数
def learning(x, y, t, configuration, nb, learning_rate):
param = initialisation(t)
los = []
for i in tqdm.tqdm(range(nb)):
cache = forward(x, param, configuration, t)
grad = behind(cache, t,配置，参数，y)
损失 = loss_function(cache[&#39;a&#39; + str(len(t) - 1)], y)
参数 = update(learning_rate, 参数，grad，t)
los = np.append(los, loss)
如果 i % (nb // 20) == 0:
打印(f&quot;loss={loss}, accuracy={100 * accuracy(cache[&#39;a&#39; + str(len(t) - 1)], y)}%&quot;)
plt.plot(np.arange(nb), los)
打印(np.argmax(cache[&#39;a&#39; + str(len(t) - 1)], axis=0), np.argmax(y, axis=0))
返回参数

def predict(x, 参数，配置，t):
cache = forward(x, 参数，配置，t)
返回cache[&#39;a&#39; + str(len(t) - 1)]
import tensorflow as tf
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
#了解训练数据的形状
x_train.shape
x=x_train.reshape(60000,784).T/255.0
y=y_train.reshape(-1,60000)
t=np.array([x.shape[0],128,64,10])
configuration=np.array([1,1,2])
arr_1d=y

arr_2d = np.zeros((10,60000))

#将相应的列索引设置为 1
for i in范围（len（arr_1d））：
arr_2d[y[0, i], i] = 1

para=learning(x,arr_2d,t,configuration,2000,0.0001)
]]></description>
      <guid>https://stackoverflow.com/questions/78648642/neural-network-customisable-from-scratch</guid>
      <pubDate>Thu, 20 Jun 2024 16:22:18 GMT</pubDate>
    </item>
    <item>
      <title>如何将 200 万份简历高精度地与 200 个职位匹配？[关闭]</title>
      <link>https://stackoverflow.com/questions/78647918/how-to-match-2-million-resumes-to-200-jobs-with-high-accuracy</link>
      <description><![CDATA[我面临的挑战是将 200 万份简历与 200 个活跃职位空缺进行匹配，目标准确率为 80%。我的目标是简化我们的招聘流程，确保候选人与职位的准确匹配。以下是详细信息：
问题陈述
目标：有效地将大量简历与较少数量的职位描述进行匹配。
目标：
高精度：目标准确率至少为 80%。
可扩展性：处理大量数据。
自动化：最大限度地减少人工干预。
可能的技术和方法
自然语言处理 (NLP)：
文本预处理：清理、标记化和规范化。
特征提取：TF-IDF、Word2Vec、GloVe、BERT 嵌入。
机器学习模型：
监督学习：对标记数据进行训练。
无监督学习：聚类技术。
深度学习模型：
基于 Transformer 的模型：BERT、RoBERTa，用于更好地理解上下文。
相似度测量：
余弦相似度。
高级指标：针对数据量身定制的自定义指标。
使用的工具
矢量数据库
OpenAI 的 Ada-002 嵌入模型
专业知识请求
有人处理过类似的问题或使用过上述技术吗？对最佳方法有什么见解或建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78647918/how-to-match-2-million-resumes-to-200-jobs-with-high-accuracy</guid>
      <pubDate>Thu, 20 Jun 2024 13:56:09 GMT</pubDate>
    </item>
    <item>
      <title>超分辨率 GAN 训练中生成器和鉴别器损失之间的不平衡</title>
      <link>https://stackoverflow.com/questions/78647617/imbalance-between-generator-and-discriminator-losses-in-gan-training-for-super-r</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78647617/imbalance-between-generator-and-discriminator-losses-in-gan-training-for-super-r</guid>
      <pubDate>Thu, 20 Jun 2024 12:57:24 GMT</pubDate>
    </item>
    <item>
      <title>时间序列预测，其中历史值也会由于滞后而更新</title>
      <link>https://stackoverflow.com/questions/78646921/time-series-forecasting-where-historical-values-also-gets-updated-due-to-lag</link>
      <description><![CDATA[我正在对未来 4 周的 covid 病例进行时间序列预测。

传入数据频率：每周
要预测的周数：4

数据的主要问题是，数据滞后约 8 周，这意味着特定周的值将在接下来的 8 周内更新 8 周，同时添加下周的值数据。
特定周（Epiweek 1）的值如下所示：
Load_Date 值
1-Jan-24 为 10 ，
8-Jan-24 为 11 ，
15-Jan-24 为 14 ，
22-Jan-24 为 15 ，
29-Jan-24 为 16 ，
6-Feb-24 为 18 ,
13-Feb-24 是 23 ,
20-Feb-24 是 26 ,
27-Feb-24 是 26 ,
6-Mar-24 是 26 ,

在这种情况下：
未修订值为 10
修订值为 26

如您在以上数据中看到的那样 - 数据在第 9 周稳定下来。同样，我们有其他一周的值。
当我应用 Auto_Arima、Garch 模型时，当我将我的预测与未修订的数据进行比较时，我可以看到良好的结果，但是当我将它们与修订值（在建模时不可用）进行比较时，我看到更多的 MAPE。
考虑到历史值也会更新 7-8 周，我该如何改善结果？]]></description>
      <guid>https://stackoverflow.com/questions/78646921/time-series-forecasting-where-historical-values-also-gets-updated-due-to-lag</guid>
      <pubDate>Thu, 20 Jun 2024 10:31:39 GMT</pubDate>
    </item>
    <item>
      <title>所有时期的损失和准确率相同</title>
      <link>https://stackoverflow.com/questions/78645720/same-loss-and-accuracy-for-all-epochs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78645720/same-loss-and-accuracy-for-all-epochs</guid>
      <pubDate>Thu, 20 Jun 2024 06:01:17 GMT</pubDate>
    </item>
    <item>
      <title>CUDAError：使用 Gymnasium 的 RL 环境内存不足</title>
      <link>https://stackoverflow.com/questions/78645476/cudaerror-not-enough-memory-for-an-rl-environment-using-gymnasium</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78645476/cudaerror-not-enough-memory-for-an-rl-environment-using-gymnasium</guid>
      <pubDate>Thu, 20 Jun 2024 04:14:28 GMT</pubDate>
    </item>
    <item>
      <title>对训练数据集使用决策树模型后仅生成一个节点</title>
      <link>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</link>
      <description><![CDATA[1
我正在尝试构建一个决策树模型，该模型基于预测变量预测结果变量（名为：结果）。实际上，我已经对一些&quot;&gt;2 级&quot; 变量应用了独热编码，以便稍微扩展预测变量的 n [我的数据]。
我首先探索了数据，然后将其拆分为 80/20 拆分并运行模型，但在训练数据集上运行的模型最终只有一个节点，没有分支。查看类似的帖子，我发现我的数据不平衡，因为通过检查类分配的 prop.table（结果变量），大多数是负面的，而不是正面的。关于在此数据上创建正确树的任何建议
这是我的代码：
将数据拆分为测试和训练数据（80％训练和20％测试数据）
set.seed(1234)
pd &lt;- sample(2, nrow(data_hum_mod), replace = TRUE, prob = c(0.8,0.2))
data_hum_train &lt;- data_hum_mod[pd==1,]
data_hum_test&lt;- data_hum_mod[pd==2,]

拆分后的数据探索
检查数据维度
dim(data_hum_train); dim(data_hum_test)
#确保分离后的数据在每个结果类别（即阳性/阴性 toxo）中的 n 值是平衡的
prop.table(table(data_hum_train$Results)) * 100
prop.table(table(data_hum_test$Results)) *100

这给出了以下结果：
(训练)
阴性 阳性 
75.75758 24.24242

和
(测试)
阴性 阳性 
54.54545 45.45455

检查缺失值
anyNA(data_hum_mod)
#确保所有变量都不为零或接近零方差。
nzv(data_hum_mod)
构建模型（使用 party 包）
install.packages(&#39;party&#39;)
library(party)

data_human_train_tree&lt;- ctree(Results ~., data = data_hum_train,
controls = ctree_control(mincriterion = 0.1))
data_human_train_tree
plot(data_human_train_tree)

使用此代码，我获得了此图
使用其他包（如 C50 和 rpart）也得到了相同的结果
您能对此提出建议吗？我读到了关于多数类别的子采样（这里是负面结果），如何在 R 中实现这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</guid>
      <pubDate>Thu, 20 Jun 2024 01:04:59 GMT</pubDate>
    </item>
    <item>
      <title>确定哪些变量对 FDA 贡献最大 [关闭]</title>
      <link>https://stackoverflow.com/questions/78644579/determine-which-variables-contribute-most-to-fda</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78644579/determine-which-variables-contribute-most-to-fda</guid>
      <pubDate>Wed, 19 Jun 2024 20:46:16 GMT</pubDate>
    </item>
    <item>
      <title>是否可以训练神经网络来输入随机森林分类器或任何其他类型的分类器（如 XGBoost 或决策树）？</title>
      <link>https://stackoverflow.com/questions/78461828/is-it-possible-to-train-a-neural-network-to-feed-into-a-random-forest-classifier</link>
      <description><![CDATA[我想创建一个模型架构来预测未来的股价走势，如下所示：

该模型的目标是预测未来 3 个月内价格是上涨还是下跌。
我尝试过一些模型，例如 Logistic 回归、神经网络、XGBoost 等。我得到了一些不错的结果。通过使用随机森林分类器，我得到了迄今为​​止最好的结果。我如何使用神经网络对数据进行编码，然后将这些值传递给随机森林分类器进行分类，而不是使用如图所示的使用 S 型函数的最终输出层（使用 Python、Keras 和 SKlearn）。
我对 Keras 不是很熟悉，所以我想知道是否有可能训练一个输入到单独分类器的神经网络，如果可以，该怎么做。]]></description>
      <guid>https://stackoverflow.com/questions/78461828/is-it-possible-to-train-a-neural-network-to-feed-into-a-random-forest-classifier</guid>
      <pubDate>Fri, 10 May 2024 17:53:36 GMT</pubDate>
    </item>
    <item>
      <title>如何在 pyspark 上创建分层分割训练、验证和测试集？</title>
      <link>https://stackoverflow.com/questions/58014693/how-to-create-stratified-split-training-validation-and-test-set-on-pyspark</link>
      <description><![CDATA[我有一个小数据集（140K），我想将其分成验证集、验证集测试集，使用目标变量和另一个字段来限制这些分割。]]></description>
      <guid>https://stackoverflow.com/questions/58014693/how-to-create-stratified-split-training-validation-and-test-set-on-pyspark</guid>
      <pubDate>Thu, 19 Sep 2019 15:45:28 GMT</pubDate>
    </item>
    </channel>
</rss>