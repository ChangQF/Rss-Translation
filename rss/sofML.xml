<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 26 Jan 2025 06:21:08 GMT</lastBuildDate>
    <item>
      <title>如何将grain.IterDataset链接在一起？</title>
      <link>https://stackoverflow.com/questions/79387801/how-to-chain-grain-iterdataset-together</link>
      <description><![CDATA[这是一个关于grain的问题，grain是用于数据提取的python库。https://google-grain.readthedocs.io/en/latest/index.html
在强化学习的背景下，我有一个数据预处理步骤，其中我将比赛重播作为单独的文件，预处理为JAX数组，然后进行处理。
如果有一个库可以理解转换JAX数组列表的逻辑，并将它们捆绑在一起形成序列以训练循环网络，并捆绑成批次以进行梯度下降，那就太好了。
我想我几乎拥有所有组件。我已经实现了一个ReplayDataSource，它是grain.RandomAccessDataSource的子类，它可以打开并将比赛文件预处理到内存中。我已经实现了一个 Sampler，它返回连续的索引来构建序列。使用一个数据源，我就可以得到我需要的一切（示意图）
source = ReplayDataSource( ... )
transformations = [grain.Batch(batch_size)]
sampler = MySampler( ... )

data_loader = Grain.DataLoader(
data_source = source,
sampler = sampler,
operations = transformations,
shard_options=grain.NoSharding(),
)

这将为我的数据集添加 2 个外部维度，一个用于序列，另一个用于批次。太棒了！
缺少一个元素。如果我有 1000 个重播文件，但无法全部加载到内存中，那么我该如何告诉 DataLoader 将 ReplayDataSources 实例化加载到内存中，并在需要时删除它们，因为我会迭代 DataLoader 的元素？
我尝试在 DataLoader 参数上使用 source = [source1, source2]，但没有成功。我尝试使用数据源制作 MapDataset，但我也不知道如何编写它们。IterDataset 也是一样。
我真正关心的唯一粒度功能是序列和批次。如何创建由制作重播文件组成的数据集，并仅在需要时打开它们？]]></description>
      <guid>https://stackoverflow.com/questions/79387801/how-to-chain-grain-iterdataset-together</guid>
      <pubDate>Sun, 26 Jan 2025 00:51:32 GMT</pubDate>
    </item>
    <item>
      <title>函数调用 Google GEMINI AI- TypeError：输入类型无效。预期为“genai.FunctionDeclarationType”的实例</title>
      <link>https://stackoverflow.com/questions/79387529/function-calling-google-gemini-ai-typeerror-invalid-input-type-expected-an-in</link>
      <description><![CDATA[我正在尝试使用 iris 数据集运行一个简单的随机森林分类模型并将其集成到 Gemini AI 中
这是我的代码：
import google.generativeai as genai
from vertexai.preview.generative_models import (

FunctionDeclaration,
GenerativeModel,
Part,
Tool,
)

genai.configure(api_key=&quot;API KEY&quot;)

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# 加载并训练模型
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

def predict_iris_species(sepal_length, sepal_width, petal_length, petal_width):
&quot;&quot;&quot;
根据萼片和花瓣测量值预测鸢尾花种类。

参数：
sepal_length (float)：萼片长度（厘米）。
sepal_width (float)：萼片宽度（厘米）。
petal_length (float)：花瓣长度（厘米）。
petal_width (float)：花瓣宽度（厘米）。

返回：
str：预测的鸢尾花种类。
&quot;&quot;&quot;
input_data = [[sepal_length, sepal_width, petal_length, petal_width]]
prediction = model.predict(input_data)
return str(iris.target_names[prediction[0]])

tools = Tool(
function_declarations=[
FunctionDeclaration(
name=&quot;predict_iris_species&quot;,
description=&quot;根据萼片和花瓣测量值预测鸢尾花种类&quot;,
parameters={
&quot;type&quot;: &quot;object&quot;,
&quot;properties&quot;: {
&quot;sepal_length&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;长度（厘米）&quot;},
&quot;sepal_width&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;萼片宽度（厘米）&quot;},
&quot;petal_length&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;花瓣长度（厘米）&quot;},
&quot;petal_width&quot;: {&quot;type&quot;: &quot;number&quot;, &quot;description&quot;: &quot;花瓣宽度（厘米）&quot;}
},
&quot;required&quot;: [&quot;sepal_length&quot;, &quot;sepal_width&quot;, &quot;petal_length&quot;, &quot;petal_width&quot;]
}
)
]
)

llm = genai.GenerativeModel(model_name=&#39;gemini-1.5-flash&#39;,
tools=[tools])

chat = llm.start_chat()
response = chat.send_message(&quot;萼片长度 5.1、萼片宽度 3.5、花瓣长度 1.4、花瓣宽度 0.2 的鸢尾花属于什么品种？&quot;)
response.text

我收到一条错误消息：
TypeError：输入类型无效。应为 `genai.FunctionDeclarationType` 的实例。
但是，收到一个类型为：&lt;class &#39;vertexai.generative_models._generative_models.Tool&#39;&gt; 的对象。
对象值：function_declarations {
...
property_ordering：&quot;petal_length&quot;
property_ordering：&quot;petal_width&quot;
}
}

这是什么意思？我以为这就是你格式化 JSON 数据的方式？是不是我的 predict_iris_species 函数需要返回其他内容而不是 字符串？
是不是因为它需要输出 JSON 字典？]]></description>
      <guid>https://stackoverflow.com/questions/79387529/function-calling-google-gemini-ai-typeerror-invalid-input-type-expected-an-in</guid>
      <pubDate>Sat, 25 Jan 2025 21:07:11 GMT</pubDate>
    </item>
    <item>
      <title>如何让我的 ML 模型只优化尾部</title>
      <link>https://stackoverflow.com/questions/79387317/how-can-i-get-my-ml-model-to-only-optimize-the-tails</link>
      <description><![CDATA[我正在研究一个不平衡的分类模型（5% 少数类），并且只关心输出分布的尾部。我想知道数据中最好的 10% 和最差的 10%。我尝试在 GBM 训练中使用贝叶斯搜索，该搜索使用 ROC AUC 的尾部作为搜索优化器，但还有更好的办法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79387317/how-can-i-get-my-ml-model-to-only-optimize-the-tails</guid>
      <pubDate>Sat, 25 Jan 2025 18:50:34 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 上的 GPU 利用率较低，且无明显瓶颈</title>
      <link>https://stackoverflow.com/questions/79387312/low-gpu-utilization-on-pytorch-without-obvious-bottlenecks</link>
      <description><![CDATA[我正在一个相当简单的深度学习网络（~800kb）上进行训练。
具有 2 层和 128 个神经元的 LSTM，
具有 2 个线性层和 64 个神经元的前馈，
具有 2 个线性层和 64 个神经元的最终前馈，
但是，当我在 RTX 6000 上查看 nvidia-smi 时，我的 GPU 利用率较低，约为 45% 和 140W
我不确定为什么会出现这种情况，因为不应该存在瓶颈，因为数据全部加载到内存中，并且我将张量直接发送到 GPU。因此，GPU 正在对批处理大小进行改组和切片（之前我在 CPU（64 核 Ryzen Threadripper）上执行此操作，但 CPU 已达到最大容量，因此它是瓶颈）。
对于 200 万个输入（每个输入约 1000 个特征），它在我的 RTX 6000 上使用 24GB 中的约 18GB GPU 内存
我使用的批处理大小为 1024，每个时期大约需要 60 秒。训练速度似乎与批处理大小成线性关系，因为 512 大约需要 120 秒，而 2048 需要 33 秒。
我的前向/后向传递是标准的 pytorch 实现。
当我添加 torch.cuda.amp.autocast/GradScaler 和 torch.backends.cudnn.benchmark = True 时，我的利用率和瓦数下降了，但每个时期的时间保持不变。
我想知道您是否可以帮助我排除故障，因为我需要尽快训练这个模型。我还会遇到数据太多而无法放入 GPU（但可以放入 RAM）的问题，因此任何有关有效解决方案的帮助也将不胜感激。我以前使用过 DataLoaders/Dataset，但它真的很慢，当 pinned_memory 和 num_workers &gt; 0 时甚至更慢。
谢谢！如果需要任何进一步的信息，请告诉我]]></description>
      <guid>https://stackoverflow.com/questions/79387312/low-gpu-utilization-on-pytorch-without-obvious-bottlenecks</guid>
      <pubDate>Sat, 25 Jan 2025 18:48:43 GMT</pubDate>
    </item>
    <item>
      <title>NNUE 国际象棋教练</title>
      <link>https://stackoverflow.com/questions/79386852/nnue-chess-trainer</link>
      <description><![CDATA[我需要帮助完成我的 NNUE 项目，我想将其集成到我的国际象棋引擎 Jomfish 中。我已经开始了这个项目，但由于我对 nnue 还比较陌生，所以无法进一步推进。我用我的 repo 训练了我的第一个模型：https://github.com/github-jimjim/NNUE-Trainer，但结果非常糟糕，出现了完全错误的情况。我的问题是，我该如何做得更好，以便 NNUE 至少可以大致给出评分。
我在一个 csv 文件中收集了 10 000 个 FENS 及其评估。由于我的硬件不够强大，我无法处理更多的 FENS。之后，我使用 NNUE.py 脚本用 1000 个 epoch 训练我的模型。然后我运行了evaluate.py，结果很糟糕，输出应该是-5.9，但NNUE给了我+0.8。]]></description>
      <guid>https://stackoverflow.com/questions/79386852/nnue-chess-trainer</guid>
      <pubDate>Sat, 25 Jan 2025 14:01:27 GMT</pubDate>
    </item>
    <item>
      <title>训练NER模型时出现KeyError错误</title>
      <link>https://stackoverflow.com/questions/79386736/keyerror-message-when-trainineg-ner-model</link>
      <description><![CDATA[
我参加了一个 NLP 黑客马拉松来预测 Ner_tags，但我似乎无法使用数据框来训练模型，它说：KeyError 和
弹出 train_df 的索引 这是我目前在 Kaggle Notebook 上的所有代码 [[导入所有库][1]]-&gt; [[下载数据集（eval 和
测试数据集相同）][2]] -&gt; [ [转换为数据框][3]] -&gt; [ [添加
句子_id 并将列拆分为单词和 NER_tags][4]] -&gt; [
[使用 simplestransformer 配置参数][5]] -&gt; [ [DF 检查
eval_df 是否超出][6]] -&gt; [ [DF 检查 2][7]] -&gt; [ [模型
训练和 KeyError 消息][8]] -&gt; [1]:
https://i.sstatic.net/v81T9mro.png [2]:
https://i.sstatic.net/oV7wOSA4.png [3]:
https://i.sstatic.net/mdJdOJVD.png [4]:
https://i.sstatic.net/oFx6afA4.png [5]:
https://i.sstatic.net/7IyVQieK.png [6]:
https://i.sstatic.net/xFVfvTCi.png [7]:
https://i.sstatic.net/Ol6eeWo1.png [8]:
https://i.sstatic.net/gw6bzXhI.png 请帮忙，这次黑客马拉松将在 12 小时后结束
]]></description>
      <guid>https://stackoverflow.com/questions/79386736/keyerror-message-when-trainineg-ner-model</guid>
      <pubDate>Sat, 25 Jan 2025 12:44:39 GMT</pubDate>
    </item>
    <item>
      <title>y 只有 4 个或更少的值，而我的预处理 DataFrame 有超过 4,000 行。回归可能吗？</title>
      <link>https://stackoverflow.com/questions/79386587/y-has-only-four-values-or-less-while-my-preprocessed-dataframe-has-more-than-4</link>
      <description><![CDATA[我的数据集是根据首尔某个特定地区的单人家庭数量来预测该地区的美食数量。
问题：原始数据的行数超过 100,000。但是，我必须分析 2 个限制区域，每个区域有 2 个真实值。
我使用了无监督学习（KMeans Clustering）、引导程序，并故意给出噪音以适应模型。但似乎不起作用。有人能告诉我如何解决这个问题吗？
我的导师希望我建立一个回归模型。
但看起来这是不可能的，就像 chatGPT 说了 20 次一样。
代码（带有任意噪声的 y，每 2 个群集的 r2_score 为 97.8%，但被拒绝）：
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings(&quot;ignore&quot;)
\# 加载数据 rest_data_path = &#39;서울시 휴게음식점 인허가 정보.csv&#39;
one_person_data_path = &#39;1인가구(연령별).csv&#39;
rest_data = pd.read_csv(rest_data_path, encoding=&#39;utf-8&#39;)
one_person_data = pd.read_csv(one_person_data_path, encoding=&#39;utf-8&#39;)

# 预处理数据
one_person_data = one_person_data.rename(columns={&#39;자치구별(2)&#39;: &#39;자치구&#39;})
one_person_data_cleaned = one_person_data[one_person_data[&#39;자치구&#39;] != &#39;자치구별(2)&#39;]
one_person_data_cleaned[&#39;2023_합계&#39;] = one_person_data_cleaned.loc[:, &#39;2023&#39;:&#39;2023.15&#39;].apply(
pd.to_numeric, errors=&#39;coerce&#39;).sum(axis=1)
one_person_summary = one_person_data_cleaned[[&#39;자치구&#39;, &#39;2023_합계&#39;]]

rest_data_cleaned = rest_data.rename(columns={&#39;지번주소&#39;: &#39;주소&#39;})
rest_data_cleaned[&#39;자치구&#39;] = rest_data_cleaned[&#39;주소&#39;].str.split(&#39; &#39;).str[1]

merged_data = pd.merge(rest_data_cleaned, one_person_summary, on=&#39;자치구&#39;, how=&#39;left&#39;)

# 添加噪音
np.随机.种子(42) 
噪声 = np.random.normal(0, 0.05 * merged_data[&#39;2023_합계&#39;].std(),大小=merged_data.shape[0])
merged_data[&#39;y&#39;] = merged_data[&#39;2023_합계&#39;] + 噪声

# 保存数据
result_continuous = merged_data[[&#39;자치구&#39;, &#39;주소&#39;, &#39;2023_합계&#39;, &#39;y&#39;]].dropna()

＃ 输出
结果_连续&#39;

输出
&lt;块引用&gt;
 자치구 주소 2023_합계 y


&lt;块引用&gt;
\5 관악구 서울특별시 관악구 봉천동 1562-17 159036.0 158422.996610
\6 관악구 서울특별시 관악구 봉천동 1562-17 142454.0 146588.600626
\18 관악구 서울특별시 관악구 신림동 1538-14 159036.0 156658.665625
\19 관악구 서울특별시 관악구 신림동 1538-14 142454.0 138756.390843
\46 관악구 서울특별시 관악구 신림동 1519-22 159036.0 157829.983096
\... ... ... ... 
\142131 도봉구 서울특별시 도봉구 창동 808 동아청솔아파트 상і동 209호 46250.0 43593.821116
\142138 관악구 서울특별시 관악구 신림동 1458-4 센트레빌 13차 159036.0 157758.991278
\142139 관악구 서울특별시 관악구 신림동 1458-4 센트레빌 13차 142454.0 140637.617424
\142142  관악구 서울특별시 관악구 봉천동 928-1 우형빌딩 159036.0 158889.888720
\142143 관악구 서울특별시 관악구 봉천동 928-1 우형빌딩 142454.0 142926.767535
\14162行×4列\

from sklearn.preprocessing import OneHotEncoder
从 sklearn.preprocessing 导入StandardScaler

columns_to_encode = [&#39;자치구&#39;]

columns_to_exclude = [&#39;2023_합계&#39;]

encoder = OneHotEncoder(sparse_output=False) # drop=&#39;first&#39; 取消排序
encoded_columns =coder.fit_transform(result_continuous[columns_to_encode])
encoded_column_names =coder.get_feature_names_out(columns_to_encode)

encoded_df = pd.DataFrame(encoded_columns, columns=encoded_column_names, index=result_continuous.index)

processed_data = result_continuous.drop(columns=columns_to_encode + columns_to_exclude)
final_data = pd.concat（[processed_data，encoded_df]，轴= 1）
Final_data[&#39;주소_hash&#39;] = result_continuous[&#39;주소&#39;].apply(hash)
Final_data = Final_data.drop(columns=[&#39;주소&#39;]) # 원래 주소 열 제거
y = np.log1p(final_data[[&#39;y&#39;]])
Final_data.drop([&#39;y&#39;], axis=1, inplace=True)
定标器=标准定标器()
Final_data_scaled = 缩放器.fit_transform(final_data)
最终数据缩放，y

输出
&lt;块引用&gt;
（数组（[[ 0.72281968,-0.72281968,1.14388039],
[0.72281968，-0.72281968，1.14388039]，
[0.72281968，-0.72281968，-0.74799056]，
...,
[0.72281968，-0.72281968，0.60622321]，
[0.72281968，-0.72281968，0.14385426]，
[0.72281968，-0.72281968，0.14385426]]），`
y
5 11.973030
6 11.895392
18 11.961831
19 11.840482
46 11.969280
... ...
142131 10.682694
142138 11.968830
142139 11.853949
142142 11.975973
142143 11.870095

[14162 行 x 1 列]]]></description>
      <guid>https://stackoverflow.com/questions/79386587/y-has-only-four-values-or-less-while-my-preprocessed-dataframe-has-more-than-4</guid>
      <pubDate>Sat, 25 Jan 2025 11:08:40 GMT</pubDate>
    </item>
    <item>
      <title>不平衡的二元分类-修复我对此解决方案的实现[关闭]</title>
      <link>https://stackoverflow.com/questions/79385651/imbalanced-binary-classification-fix-my-implementation-of-this-solution</link>
      <description><![CDATA[我查看了以下帖子：
处理高度不平衡数据的正确方法 - 二元分类
发帖者 @skillsmuggler 说：
上述问题在处理医疗数据集和其他类型的故障检测时非常常见，其中一个类别（不良影响）总是代表性不足。
解决这个问题的最佳方法是生成折叠并应用交叉验证。折叠应以平衡每个折叠中的类别的方式生成。在您的案例中，这将创建 20 个折叠，每个折叠都具有相同的代表性不足的类别和不同比例的代表性过高的类别。
生成平衡折叠
生成平衡折叠并使用交叉验证也会产生更好的通用和稳健模型。在您的案例中，20 个折叠可能看起来太苛刻，因此您可以创建 10 个折叠，每个折叠的类别比率为 2:1。
/结束。
这对我来说没有意义。
所以他的意思是：创建 10-20 个折叠，每个折叠都有 100% 的少数类和多数类的不同随机子集。好的，完成了。我现在该做什么？如果我以传统的 CV 方式对模型进行评分（使用所有其他折叠进行训练，对一个折叠进行评分），我们将得到疯狂的目标泄漏，因为模型看到的确实是与训练时相同的观察结果。
我在这里遗漏了什么？他是否建议在这些 1:1 分割上训练 20 个弱分类器，然后使用集成评分对测试集进行评估？
作为参考，我的正类频率为 1%。我还没有成功使用任何嵌入式方法来处理不平衡（加权）。我用于评估的指标是：召回率、马修相关系数和平均精度得分。]]></description>
      <guid>https://stackoverflow.com/questions/79385651/imbalanced-binary-classification-fix-my-implementation-of-this-solution</guid>
      <pubDate>Fri, 24 Jan 2025 21:15:58 GMT</pubDate>
    </item>
    <item>
      <title>为什么 PyTorch RetinaNet ResNet50 FPN V2 在配备 T4 GPU 的 Google Colab 上训练速度如此之慢？</title>
      <link>https://stackoverflow.com/questions/79385141/why-is-pytorch-retinanet-resnet50-fpn-v2-training-so-slow-on-google-colab-with-a</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79385141/why-is-pytorch-retinanet-resnet50-fpn-v2-training-so-slow-on-google-colab-with-a</guid>
      <pubDate>Fri, 24 Jan 2025 17:19:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 Yolov3 进行光学音乐识别</title>
      <link>https://stackoverflow.com/questions/79382146/optical-music-recognition-using-yolov3</link>
      <description><![CDATA[我正在尝试编写一个模型（Yolov3）来检测乐谱上的各种音乐符号。但所有适合此目的的数据集都只建立在印刷乐谱上。有没有办法以某种方式将模型适应手写字符？预训练 darknet-53 会对此有所帮助吗？如果我训练 darknet-53 识别手写和印刷字符，这会产生什么影响？
Yolov3 架构：Yolov3]]></description>
      <guid>https://stackoverflow.com/questions/79382146/optical-music-recognition-using-yolov3</guid>
      <pubDate>Thu, 23 Jan 2025 18:12:00 GMT</pubDate>
    </item>
    <item>
      <title>如何在固定的BBOX中将YOLOv8model与Deepsort连接起来？</title>
      <link>https://stackoverflow.com/questions/79378146/how-can-i-connect-yolov8model-with-deepsort-in-a-fixed-bbox</link>
      <description><![CDATA[我正在生成一个可以检测摩托车和汽车的模型来提取各自的信息。
但在将 YOLOv8 模型（这是我自定义的模型）与 Deepsort 算法连接的过程中，我发现了几个问题。

起初，自定义模型（YOLOv8）可以检测到每辆车，提取的视频显示完美的边界框
与 Deepsort 连接后，它漏掉了几辆车，提取的视频有错误的边界框（它们太大，不适合每辆车）
我在 YOLOv8 2 Deepsort 之间找不到错误的结果。

请帮帮我
import cv2
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort

# 初始化 YOLO 模型
model_path = &quot;/content/drive/MyDrive/Capstone/best_motorcycle_detector_NIGHT8.pt&quot;
model = YOLO(model_path)
model.to(&#39;cuda&#39;) # 使用 GPU

# 初始化 DeepSORT
tracker = DeepSort(max_age=200, n_init=1, nn_budget=200)

# 帮助程序将 YOLO 结果转换为 DeepSORT 格式
def yolo_to_deepsort(yolo_results, target_classes):
detections = []
for det in yolo_results[0].boxes:
x1, y1, x2, y2 = map(float, det.xyxy[0].cpu().numpy())
confidence = float(det.conf.cpu().numpy().item())
class_id = int(det.cls.cpu().numpy())
if class_id in target_classes:
detections.append([(x1, y1, x2, y2),置信度])
返回检测

# 主处理循环
video_path = &quot;/content/drive/MyDrive/Capstone/11.15 1200-1400/1320-1400.mp4&quot;
cap = cv2.VideoCapture(video_path)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

output_path = &quot;/content/drive/MyDrive/Capstone/Results/processed_video.avi&quot;
video_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*&#39;MJPG&#39;), fps, (frame_width, frame_height))

target_classes = [2, 3] # 汽车 (2)、摩托车 (3)

while cap.isOpened():
ret, frame = cap.read()
if not ret:
break

# 运行 YOLO 模型
results = model(frame, conf=0.3)

# 将 YOLO 结果转换为 DeepSORT 格式
detections = yolo_to_deepsort(results, target_classes)

# 更新跟踪器
tracks = tracker.update_tracks(detections, frame=frame)

# 绘制边界框
for track in tracks:
if not track.is_confirmed():
continue
x1, y1, x2, y2 = map(int, track.to_tlbr())
track_id = track.track_id
标签 = f&quot;ID {track_id}&quot;
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# 保存帧
video_writer.write(frame)

cap.release()
video_writer.release()


找出 YOLO 中的协调性
Deepsort 的输入和输出
与其他算法连接，但 Deepsort 更适合我的视频

这是视频和模型权重（YOLOv8）:)
Deepsort 是一种跟踪检测到的对象的跟踪算法。
-File : extracted_weights =&gt; 定制的 YOLOv8 模型的权重
-File : 11.15 1320-1400_detected_video.avi =&gt; 通过定制的 YOLOv8 模型检测摩托车和汽车的视频
-File : 11.15 1320-1400_deepsort_processed_video.avi =&gt;跟踪（使用深度排序算法）检测到的物体（通过定制的 YOLOv8 模型）的视频
https://drive.google.com/file/d/1-03M2L42RtP6hauVP7fKSSqETEm8LtR6/view?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/79378146/how-can-i-connect-yolov8model-with-deepsort-in-a-fixed-bbox</guid>
      <pubDate>Wed, 22 Jan 2025 14:30:52 GMT</pubDate>
    </item>
    <item>
      <title>在 Colab 中微调 Llama 3.1 时的上下文长度限制</title>
      <link>https://stackoverflow.com/questions/79104305/context-length-limitation-when-fine-tuning-llama-3-1-in-colab</link>
      <description><![CDATA[我正在通过 Unsloth 库，使用带有自定义数据集（使用 LoRA 技术）的 A100 GPU 在 Google Colab Pro 中对 Llama 3.1 模型进行微调。下面是我正在使用的 LoRA 代码：
max_seq_length = 2048
model = FastLanguageModel.get_peft_model(
model,
r=16, # 选择任意数字 &gt; 0 ！建议 8、16、32、64、128
target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
&quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down​​_proj&quot;],
lora_alpha=16,
lora_dropout=0, # 支持任意，但 = 0 是经过优化的
bias=&quot;none&quot;, # 支持任意，但 = &quot;none&quot; 是经过优化的

use_gradient_checkpointing=&quot;unsloth&quot;, # 对于非常长的上下文，为 True 或 &quot;unsloth&quot;
random_state=3407,
use_rslora=False, # 我们支持等级稳定的 LoRA
loftq_config=None, # 和 LoftQ
)
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
train_dataset=dataset,
dataset_text_field=&quot;text&quot;,
max_seq_length=max_seq_length,
dataset_num_proc=2,
packing=False, # 可以使短序列的训练速度提高 5 倍。
args=TrainingArguments(
per_device_train_batch_size=2,
gradient_accumulation_steps=4,
warmup_steps=5,
# num_train_epochs = 1, # 将其设置为 1 次完整的训练运行。
max_steps=60,
learning_rate=2e-4,
fp16=not is_bfloat16_supported(),
bf16=is_bfloat16_supported(),
logs_steps=1,
optim=&quot;adamw_8bit&quot;,
weight_decay=0.01,
lr_scheduler_type=&quot;linear&quot;,
seed=3407,
output_dir=&quot;outputs&quot;,
),
)

加载模型时，我们必须指定最大序列长度，这会限制其上下文窗口。 Llama 3.1 支持最多 128k 的上下文长度，但在本例中我将其设置为 2048，因为它消耗更多的计算和 VRAM。此外，dtype 参数会自动检测您的 GPU 是否支持 BF16 格式，以便在训练期间获得更高的稳定性（此功能仅限于 Ampere 和较新的 GPU）。
我的问题：

如果我在训练时将 max_seq_length 设置为 2048，那么训练后我的模型的上下文长度是多少，128k 还是 2048？
训练模型后，我们可以使用 128k 的上下文长度吗，还是仍然限制为 2048？
]]></description>
      <guid>https://stackoverflow.com/questions/79104305/context-length-limitation-when-fine-tuning-llama-3-1-in-colab</guid>
      <pubDate>Sat, 19 Oct 2024 05:59:26 GMT</pubDate>
    </item>
    <item>
      <title>pyspark 实现的 ALS 是如何处理每个用户-项目组合的多个评级的？</title>
      <link>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</link>
      <description><![CDATA[我观察到 ALS 的输入数据不需要每个用户-项目组合都有唯一的评分。
这是一个可重现的示例。
# 示例数据框
df = spark.createDataFrame([(0, 0, 4.0),(0, 1, 2.0), 
(1, 1, 3.0), (1, 2, 4.0), 
(2, 1, 1.0), (2, 2, 5.0)],[&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;])

df.show(50,0)
+----+----+------+
|user|item|rating|
+----+----+------+
|0 |0 |4.0 |
|0 |1 |2.0 |
|1 |1 |3.0 |
|1 |2 |4.0 |
|2 |1 |1.0 |
|2 |2 |5.0 |
+----+----+------+

可以看到，每个用户-商品组合只有一个评分（理想情况）。
如果我们将这个数据框传递到 ALS，它将为您提供如下预测：
# 拟合 ALS
from pyspark.ml.recommendation import ALS
als = ALS(rank=5, 
maxIter=5, 
seed=0,
regParam = 0.1,
userCol=&#39;user&#39;,
itemCol=&#39;item&#39;,
ratingsCol=&#39;rating&#39;,
nonnegative=True)
model = als.fit(df)

# 来自 als 的预测
all_comb = df.select(&#39;user&#39;).distinct().join(broadcast(df.select(&#39;item&#39;).distinct()))
predictions = model.transform(all_comb)

predictions.show(20,0)
+----+----+----------+
|user|item|prediction|
+----+----+----------+
|0 |0 |3.9169915 |
|0 |1 |2.031506 |
|0 |2 |2.3546133 |
|1 |0 |4.9588947 |
|1 |1 |2.8347554 |
|1 |2 |4.003007 |
|2 |0 |0.9958025 |
|2 |1 |1.0896711 |
|2 |2 |4.895194 |
+----+----+----------+

到目前为止，一切对我来说都是有意义的。但是如果我们有一个包含多个用户-项目评分组合的数据框，如下所示 -
# 示例数据框
df = spark.createDataFrame([(0, 0, 4.0), (0, 0, 3.5),
(0, 0, 4.1),(0, 1, 2.0),
(0, 1, 1.9),(0, 1, 2.1),
(1, 1, 3.0), (1, 1, 2.8),
(1, 2, 4.0),(1, 2, 3.6),
(2, 1, 1.0), (2, 1, 0.9),
(2, 2, 5.0),(2, 2, 4.9)],
[&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;])
df.show(100,0)
+----+----+------+
|user|item|rating|
+----+----+------+
|0 |0 |4.0 |
|0 |0 |3.5 |
|0 |0 |4.1 |
|0 |1 |2.0 |
|0 |1 |1.9 |
|0 |1 |2.1 |
|1 |1 |3.0 |
|1 |1 |2.8 |
|1 |2 |4.0 |
|1 |2 |3.6 |
|2 |1 |1.0 |
|2 |1 |0.9 |
|2 |2 |5.0 |
|2 |2 |4.9 |
+----+----+------+

如您在上面的数据框中看到的那样，一个用户-项目组合有多条记录。例如 - 用户“0”多次对项目“0”进行评分，即分别为 4.0、3.5 和 4.1。
如果我将此输入数据框传递给 ALS 会怎样？这会起作用吗？
我最初认为它不应该起作用，因为 ALS 应该根据用户-项目组合获得唯一评级，但当我运行它时，它起作用了，让我感到惊讶！
# 拟合 ALS
als = ALS(rank=5, 
maxIter=5, 
seed=0,
regParam = 0.1,
userCol=&#39;user&#39;,
itemCol=&#39;item&#39;,
ratingsCol=&#39;rating&#39;,
nonnegative=True)
model = als.fit(df)

# 来自 als 的预测
all_comb = df.select(&#39;user&#39;).distinct().join(broadcast(df.select(&#39;item&#39;).distinct()))
predictions = model.transform(all_comb)

predictions.show(20,0)
+----+----+----------+
|user|item|prediction|
+----+----+----------+
|0 |0 |3.7877638 |
|0 |1 |2.020348 |
|0 |2 |2.4364853 |
|1 |0 |4.9624424 |
|1 |1 |2.7311888 |
|1 |2 |3.8018093 |
|2 |0 |1.2490809 |
|2 |1 |1.0351425 |
|2 |2 |4.8451777 |
+----+----+----------+

为什么它会起作用？我以为它会失败，但它没有，而且还给了我预测。
我尝试查看研究论文、ALS 的有限源代码和互联网上可用的信息，但找不到任何有用的东西。
是取这些不同评分的平均值然后将其传递给 ALS 还是其他什么？
有人遇到过类似的事情吗？或者知道 ALS 内部如何处理此类数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</guid>
      <pubDate>Tue, 26 Apr 2022 10:37:44 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pytorch Geometric 的超图卷积：RuntimeError：索引 2268264 超出了大小为 2268264 的维度 0 的界限</title>
      <link>https://stackoverflow.com/questions/69184856/hypergraph-convolution-using-pytorch-geometric-runtimeerror-index-2268264-is-o</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/69184856/hypergraph-convolution-using-pytorch-geometric-runtimeerror-index-2268264-is-o</guid>
      <pubDate>Tue, 14 Sep 2021 21:48:03 GMT</pubDate>
    </item>
    <item>
      <title>处理高度不平衡数据的正确方法 - 二元分类[关闭]</title>
      <link>https://stackoverflow.com/questions/59409967/proper-way-to-handle-highly-imbalanced-data-binary-classification</link>
      <description><![CDATA[我有一个非常大的数据集，有 6000 万行和 11 个特征。
这是高度不平衡的数据集，20:1（信号：背景）。
正如我所看到的，有两种方法可以解决这个问题：
第一：欠采样/过采样。
我有两个问题/疑问。
如果我在训练测试分割之前进行欠采样，我会丢失大量数据。
但更重要的是，如果我在平衡的数据集上训练模型，我会丢失有关信号数据频率的信息（比如说良性肿瘤的频率高于恶性肿瘤的频率），并且由于模型经过训练和评估，因此模型将表现良好。但如果将来某个时候我要在新数据上尝试我的模型，它的表现会很差，因为真实数据是不平衡的。
如果我在训练测试拆分后进行了欠采样，我的模型将欠拟合，因为它将在平衡数据上进行训练，但在不平衡数据上进行验证/测试。
第二 - 类别权重惩罚
我可以将类别权重惩罚用于 XBG、随机森林、逻辑回归吗？
所以，我正在寻找解决此类问题的解释和想法。]]></description>
      <guid>https://stackoverflow.com/questions/59409967/proper-way-to-handle-highly-imbalanced-data-binary-classification</guid>
      <pubDate>Thu, 19 Dec 2019 12:37:16 GMT</pubDate>
    </item>
    </channel>
</rss>