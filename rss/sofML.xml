<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 16 May 2024 18:18:08 GMT</lastBuildDate>
    <item>
      <title>基于剧情简介的多标签电影类型分类器</title>
      <link>https://stackoverflow.com/questions/78491754/multi-label-movie-genre-classifier-based-on-synopsis</link>
      <description><![CDATA[我必须创建一个机器学习模型，可以根据电影的概要将电影分类为类型。该数据集包含大量示例和标记数据。流派输出变量很复杂，因为有 18 种不同的流派，但所有这些流派都可以组合起来以获得电影的“官方”流派。所以实际上总共有 1000 种不同的流派。我尝试用多标签分类来解决它。然而，最后，我需要根据每部电影的概率对类型进行排名，这很奇怪，因为它是多标签的，对吧？
因此，性能也很差（12% 准确率），我需要达到 77%。我很好地处理了数据（删除了停用词、矢量化文本……），并且使用了逻辑回归和一对一分类器。您对如何改进这一点有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78491754/multi-label-movie-genre-classifier-based-on-synopsis</guid>
      <pubDate>Thu, 16 May 2024 18:10:01 GMT</pubDate>
    </item>
    <item>
      <title>使用 BERT 对连续数据流进行分类</title>
      <link>https://stackoverflow.com/questions/78491661/categorising-continuous-data-streams-with-bert</link>
      <description><![CDATA[我正在研究构建分类系统，该系统基本上将通过各种管道实时摄取连续的数据流，例如 Twitter 帖子和这些帖子中的评论，以文章形式来自网站源的数据。
我希望系统将数据组织在主题和子主题中，例如：
主题——笔记本电脑
副主题——宏碁推出全新 XYZ 笔记本电脑
我无意使用生成式人工智能来编写子主题，我更期待从数据源中形成句子，例如，如果宏碁推出 XYZ 笔记本电脑，那么所有 Twitter 帖子和文章都会包含类似的内容“宏碁透露/推出了等等”。
我只需要一个从哪里开始的方向，因为我有点迷失如何实现它
我什至从来没有超越过构建数据流。更不用说设置模型来进行分类了]]></description>
      <guid>https://stackoverflow.com/questions/78491661/categorising-continuous-data-streams-with-bert</guid>
      <pubDate>Thu, 16 May 2024 17:44:49 GMT</pubDate>
    </item>
    <item>
      <title>具有 n 维卫星图像的多类 UNet</title>
      <link>https://stackoverflow.com/questions/78491587/multiclass-unet-with-n-dimensional-satellite-images</link>
      <description><![CDATA[我正在尝试使用 Pytorch 中的 UNet 从多维（8 波段）卫星图像中提取预测掩模。我无法让预测蒙版看起来有些预期/连贯。我不确定问题是否在于我的训练数据的格式化方式、我的训练代码或我用于进行预测的代码。我怀疑这是我的训练数据输入模型的方式。我有 8 个波段卫星图像和单波段掩码，其值范围为 0-n 个类别，其中 0 是背景，1-n 是目标标签，如下所示：

在单通道示例的情况下，图像形状为 (8, 512, 512)，掩模形状为 (512, 512)，在 OHE 情况下为 (512, 512, 8)，而 (512, 512, 3) 在堆叠的情况下。
有些蒙版可能包含所有类标签，有些可能只有几个或仅是背景标签。我尝试过使用这些单通道掩码，我还将它们转换为 3 通道掩码，第一个通道是给定图像的所有标签，并且我还尝试了对它们进行热编码，以便每个掩码都是 0- n 个维度，每个通道都有一个不同的标签，其中背景/目标为二进制 0-1。
在每种情况下，无论我采用哪种方式格式化训练数据，训练结果最终要么是全黑、全白，要么是像这样的网格效果：

是否有一种理想的方式来格式化这些数据以进行训练/预测，或者我是否做错了什么导致了这些错误的预测掩码？
为了不发布数百行代码，以下是我用来尝试堆叠/OHE 掩码、训练和预测的一些通用片段：
蒙版操作：
将 numpy 导入为 np
从 PIL 导入图像
mask = np.array(Image.open(mask_path))
如果堆栈：
    Zeros = np.zeros((mask.shape[0], mask.shape[1]))
    掩码 = np.transpose(np.array([掩码, 零, 零]), (1, 2, 0)).astype(np.uint8)
如果一个热：
    one_hot_mask = np.zeros((mask.shape[0], mask.shape[1], self.num_classes))
    label_values = 列表(np.unique(mask))
    对于范围内的 i(0, self.num_classes):
        如果我不在 label_values 中：
            one_hot_mask[:, :, i] = 0
        别的：
            如果堆栈：
                one_hot_mask[:, :, i][掩码[:, :, 0] == i] = 1
            别的：
                one_hot_mask[:, :, i][掩码[:, :] == i] = 1

火车
导入火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim

频段数 = 8
num_classes = 8 # 碰巧该数据集具有与输入频段/通道相同数量的类，但情况并非总是如此
纪元=5
学习率 = 0.001
权重衰减 = 0

模型 = UNet(n_channels=num_bands, n_classes=num_classes).to(device)
优化器= optim.Adam(model.parameters(),lr=learning_rate,weight_decay=weight_decay)
loss_fn = nn.CrossEntropyLoss() 如果 num_classes &gt; 1 否则 nn.BCEWithLogitsLoss()

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

对于范围内的纪元（纪元）：
    循环= tqdm（枚举（train_loader），总计= len（train_loader））
    对于batch_idx，（数据，目标）循环：
        数据 = data.float().to(设备)
        目标 = Targets.long().to(设备)
        预测=模型（数据）
        损失= loss_fn（预测，目标）
        优化器.zero_grad()
        循环.set_postfix(loss=loss.item())
    checkpoint_name = os.path.join(model_dir, f&quot;model_{epoch}.pt&quot;)
    torch.save(model.state_dict(), checkpoint_name)

预测
将 numpy 导入为 np
从 skimage 导入 io
图像 = np.array(io.imread(image_path))

张量 = ToTensor()(图像)
batch_t = torch.unsqueeze(张量, 0).to(设备)
preds = 模型(batch_t)
softmax = torch.nn.Softmax(dim=1)
preds = torch.argmax(softmax(model(preds)),axis=1).cpu()
preds = np.array(preds[0,:,:])
plt.imshow(preds, cmap=&#39;tab20&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78491587/multiclass-unet-with-n-dimensional-satellite-images</guid>
      <pubDate>Thu, 16 May 2024 17:28:46 GMT</pubDate>
    </item>
    <item>
      <title>TypeError“NoneType”对象不可订阅错误烧瓶</title>
      <link>https://stackoverflow.com/questions/78491414/typeerror-nonetype-object-is-not-subscriptable-error-flask</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78491414/typeerror-nonetype-object-is-not-subscriptable-error-flask</guid>
      <pubDate>Thu, 16 May 2024 16:44:52 GMT</pubDate>
    </item>
    <item>
      <title>有 0 张图像属于 0 个类别，我被卡住了</title>
      <link>https://stackoverflow.com/questions/78490995/there-are-0-images-belonging-to-0-classes-and-im-stuck</link>
      <description><![CDATA[您好，我是机器学习新手，我正在尝试使用 google Colab 让程序运行我的 google 驱动器中的图像，但它一直给出“model.fit(train_generator, epochs=10, validation_data=test_generator)”
我尝试将批次大小调整为 10，并将所有图像大小调整为 100，我认为它会起作用，但它仍然给了我
找到属于 0 个类别的 0 张图片。
找到属于 0 个类别的 0 张图片。
我将训练和测试图像放入两个名为“training_data”的单独文件夹中和“测试数据”
!pip 安装tensorflow
导入CV2
从tensorflow.keras.preprocessing.image导入img_to_array
将 numpy 导入为 np
将张量流导入为 tf
从tensorflow.keras.preprocessing.image导入ImageDataGenerator
从tensorflow.keras.applications导入EfficientNetB0
从tensorflow.keras.layers导入密集，GlobalAveragePooling2D
从tensorflow.keras.models导入模型
从sklearn.metrics导入confusion_matrix、accuracy_score、 precision_score、recall_score

#上传图片
def load_image(图像位置):
    # 使用OpenCV加载图像
    图像 = cv2.imread(图像位置)

   #检查图片是否上传
    如果图像为无：
        print(“错误：找不到图像”，image_location)
        返回无

    resized_image= cv2.resize(图像, (100, 100))
    image_array = img_to_array(调整大小的图像)

    图像数组 /= 255.0

    返回图像数组


example_image_location = &#39;/content/drive/MyDrive/机器学习文件夹/training_data/LightGreen_Crayon_Testing.jpg&#39;

# 加载并预处理示例图像
示例图像 = 加载图像（示例图像位置）


如果 example_image 不是 None：

    # 收集/处理数据
  train_dir = &#39;/content/drive/MyDrive/机器学习文件夹/training_data&#39;
  test_dir = &#39;/content/drive/MyDrive/机器学习文件夹/testing_data&#39;

  train_datagen = ImageDataGenerator（重新缩放=1。/ 255）
  test_datagen = ImageDataGenerator（重新缩放=1。/ 255）

  train_generator = train_datagen.flow_from_directory(train_dir, target_size=(100, 100), batch_size=10, class_mode=&#39;binary&#39;)

  test_generator = test_datagen.flow_from_directory(test_dir, target_size=(100, 100), batch_size=10, class_mode=&#39;binary&#39;)

    ＃ 建筑模型
  base_model = EfficientNetB0(weights=&#39;imagenet&#39;, include_top=False)
  x = 基础模型.输出
  x = GlobalAveragePooling2D()(x)
  x = 密集（1024，激活=&#39;relu&#39;）（x）
  预测=密集（1，激活=&#39;sigmoid&#39;）（x）
  模型 = 模型（输入=base_model.输入，输出=预测）

  对于 base_model.layers 中的图层：
    可训练层 = False
    model.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
    model.fit（train_generator，epochs = 10，validation_data = test_generator）

    # 模型评估
    预测 = model.predict(test_generator)
    Predicted_classes = np.argmax(预测，轴=1)
    true_classes = test_generator.classes
    conf_matrix = 混淆矩阵（真实类，预测类）
    准确度 = 准确度分数（真实类别，预测类别）
    精度 = precision_score(真实类, 预测类)
    召回率=召回率（真实类别，预测类别）

    num_test_samples = len(test_generator.文件名)
    print(f&quot;测试样本数：{num_test_samples}&quot;)
    print(&quot;混淆矩阵：&quot;)
    打印（conf_matrix）
    print(“准确度：”, 准确度)
    print(&quot;精度：&quot;, 精度)
    print(“回忆：”, 回忆)

]]></description>
      <guid>https://stackoverflow.com/questions/78490995/there-are-0-images-belonging-to-0-classes-and-im-stuck</guid>
      <pubDate>Thu, 16 May 2024 15:27:25 GMT</pubDate>
    </item>
    <item>
      <title>我如何使变压器输出相对于特定上下文的翻译</title>
      <link>https://stackoverflow.com/questions/78490608/how-can-i-make-a-transformer-output-a-translation-relative-to-a-specific-context</link>
      <description><![CDATA[所以我目前正在做一个类似机器翻译的项目，其中我有一个具有编码器-解码器结构的转换器，它应该从自然语言命令生成 SQL 查询，例如：
输入：计算 XYZ 公司生产的飞机数量
输出：从飞机中选择 COUNT(*) 个，其中制造商=&#39;XYZ&#39;；
现在我已经实现了我的目标，我的模型在大多数情况下都可以生成看起来不错的查询，但仍然有一个问题需要解决，那就是它可以生成的查询在语法上都是正确的，但它并没有解决问题属性/表名称，例如：
输入：计算 XYZ 公司生产的飞机数量
输出： SELECT COUNT(*) FROM 飞机，其中生产=&#39;XYZ&#39;;
或者：
输入：显示所有 40 岁以上的员工
输出： SELECT * FROM员工，其中国家/地区&gt; 40；
就像我说的，从语法上讲，查询没有错误，但它肯定无法在数据库本身上正确执行。
我的模型是在 json 数据集上进行训练的，该数据集包含遵循以下形式的样本列表：
&lt;前&gt;&lt;代码&gt;[
  [
    “计算 XYZ 公司生产的飞机数量”，
    “从制造商 = &#39;XYZ&#39; 的飞机中选择 COUNT(*)；”
  ],
  [
    “大西洋发现了多少海洋物种？”,
    “SELECT COUNT(*) FROM Marine_species WHERE location = &#39;Atlantic Ocean&#39;;”
  ]
]

尽管我发现一些数据集包含数据库架构作为一组 CREATE 查询，如下所示：
{“指令”：“CREATE TABLE table_72445 (
    “县”文本，
    “人口”真实的，
    “人均收入”文本，
    “家庭收入中位数”文本，
    “家庭收入中位数”文本
）
-- 列出河滨家庭收入中位数”，
“输出”：“选择\“家庭收入中位数” FROM table_72445 WHERE \“县\” =“河滨”}

那么我是否可以利用这种数据集，以便为我的转换器提供它应该工作的数据库上下文，或者是否有其他方法来实现此任务目标？
注意：我无法将之前的语料库作为一个整体作为我的输入，因为这会导致巨大的性能开销。想象一下，每次我想要执行时，都会定义包含数十个表的相同集合，因此这是别无选择。
提前谢谢大家。]]></description>
      <guid>https://stackoverflow.com/questions/78490608/how-can-i-make-a-transformer-output-a-translation-relative-to-a-specific-context</guid>
      <pubDate>Thu, 16 May 2024 14:25:29 GMT</pubDate>
    </item>
    <item>
      <title>具有数值列和数组列的机器学习输入，处理机器学习中的混合类型数据</title>
      <link>https://stackoverflow.com/questions/78490240/machine-learning-input-with-numerical-columns-and-array-columns-handling-mixed</link>
      <description><![CDATA[我正在开发一个机器学习项目，其中有一个数据集，其中包含数字列和包含数组的列的组合。数字列（例如平均值）包含单个值，而带有数组的列（例如梯度）每行可以包含可变数量的元素。
处理此类输入的最佳实践是什么？我可以在机器学习模型中同时使用数字列和带有数组的列吗？如果是这样，在模型的预处理和训练阶段处理这种数据异构性的最常见策略是什么？
如果有任何建议或资源可以帮助我更好地了解如何应对这一挑战，我将不胜感激。
示例：
模型输入：

&lt;标题&gt;

平均值
渐变


&lt;正文&gt;

0.5
[1,2,3,45,0.2]


1
[2,5,1.2,5,0]



预先感谢您的帮助！
我尝试在网上搜索，但找不到真正的答案。]]></description>
      <guid>https://stackoverflow.com/questions/78490240/machine-learning-input-with-numerical-columns-and-array-columns-handling-mixed</guid>
      <pubDate>Thu, 16 May 2024 13:28:46 GMT</pubDate>
    </item>
    <item>
      <title>如何评估 SVM 模型的变量重要性？</title>
      <link>https://stackoverflow.com/questions/78488619/how-do-i-asses-variable-importance-of-a-svm-model</link>
      <description><![CDATA[我正在开发一个机器学习项目。我的目标变量是二进制的，我用插入符构建了一个 SVM 径向模型：
ctrl_SVM &lt;- trainControl(method=“cv”，number=10，search=“grid”，summaryFunction = TwoClassSummary，classProbs = TRUE)
param_grid_SVM_radial &lt;- Expand.grid(C = c(0.01, 0.1, 1, 10, 100), sigma = c(0.01, 0.1, 1, 10))
SVM_radial &lt;- train(Target~., data=under_svm, method = &quot;svmRadial&quot;, trControl = ctrl_SVM,tuneGrid = param_grid_SVM_radial,
                    指标=“ROC”，预处理=NULL）

如何评估变量的重要性？正确的代码是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78488619/how-do-i-asses-variable-importance-of-a-svm-model</guid>
      <pubDate>Thu, 16 May 2024 08:44:20 GMT</pubDate>
    </item>
    <item>
      <title>有谁可以询问有关使用 NER 和 XLM-RoBERTa 进行信息提取的问题吗？</title>
      <link>https://stackoverflow.com/questions/78488394/is-there-anyone-can-i-ask-about-information-extraction-using-ner-with-xlm-robert</link>
      <description><![CDATA[大家好我想问一下关于从PDF文档中提取信息的问题。因此，我将使用 XLM-RoBERTa 和 NER 来执行信息提取，并使用来自以下来源的代码进行微调：https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a。从这个来源来看，它确实使用了 BERT，并且在我尝试之后，结果发现来自单词的预测实体与真实值不同。我很困惑是否需要从代码中更改某些内容，或者数据集是否有问题，因为我使用的数据集是自制的数据集。有什么可以问的吗？谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78488394/is-there-anyone-can-i-ask-about-information-extraction-using-ner-with-xlm-robert</guid>
      <pubDate>Thu, 16 May 2024 08:03:25 GMT</pubDate>
    </item>
    <item>
      <title>为什么 JAX 编译时间随着 vmap 批处理大小的增加而增长？</title>
      <link>https://stackoverflow.com/questions/78486071/why-does-jax-compilation-time-grow-with-vmap-batch-size</link>
      <description><![CDATA[我正在使用 JAX 来评估批量损失梯度，其中涉及一些复杂的线性代数（包括 Cholesky 分解和解决方案等）。我的梯度损失的示意图形式是
jax.jit( jax.value_and_grad( jax.vmap(loss)(...).mean() ) )

我发现编译/首次评估时间在给定 vmap 的特定批量大小之前是恒定的（正如我通常所期望的那样），然后开始超线性增长。在 A100 上，nbatch &lt;= 64 需要 6 分钟，nbatch=128 需要 13 分钟，nbatch=256 需要 1 小时，这变得很笨拙。
@jakevdp 非常合理地要求提供一个可重现的示例。经过大量二分之后，我想我看到了简单的批量 cholesky 的相同行为：
# 这里没什么可看的，只是设置我们将要解决的线性系统
诺布斯, ngp = 256, 64
t = np.linspace(0, 1, 诺布)
f = np.arange(1, ngp + 1, dtype=np.float64)

fmat = np.zeros((nobs, 2*ngp), dtype=np.float64)
fmat[:, ::2] = np.sin(2.0 * jnp.pi * f * t[:,np.newaxis])
fmat[:, 1::2] = np.cos(2.0 * jnp.pi * f * t[:,np.newaxis])

一、ones = jax.numpy.identity(nobs, dtype=np.float64), jax.numpy.ones(nobs, dtype=np.float64)

def 函数（pars）：
  ftf = fmat @ jax.numpy.diag(pars**2) @ fmat.T + 1
  cf = jax.scipy.linalg.cho_factor(ftf)
  b = jax.scipy.linalg.cho_solve(cf, 个)
  返回 b.mean()

然后如果我获得转换并编译它
jvg = jax.value_and_grad(lambda pars: jax.vmap(func)(pars).mean())
pars = jax.random.normal(jax.random.PRNGKey(0), (nbatch,2*ngp,))
jjvg = jax.jit(jvg).lower(pars).compile()

我发现编译时间（以秒为单位）随着 nbatch 的增加而增长：
&lt;预&gt;&lt;代码&gt;[16,0.532]、[32,0.507]、[64,0.516]、[128,0.580]、[256,0.652]、[512,0.822]、[1024,1.7]、[2048 ,2.75]

如果我尝试 jax.make_jaxpr 而不是 jax.jit，我会看到每个批量大小的相同代码。
这里可能发生了什么？我使用的是 JAX 0.4.26 和带有 CUDA 12.2 和驱动程序 535.104.05 的 V100。]]></description>
      <guid>https://stackoverflow.com/questions/78486071/why-does-jax-compilation-time-grow-with-vmap-batch-size</guid>
      <pubDate>Wed, 15 May 2024 19:17:27 GMT</pubDate>
    </item>
    <item>
      <title>调查 TensorFlow 和 PyTorch 性能的差异</title>
      <link>https://stackoverflow.com/questions/78478574/investigating-discrepancies-in-tensorflow-and-pytorch-performance</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78478574/investigating-discrepancies-in-tensorflow-and-pytorch-performance</guid>
      <pubDate>Tue, 14 May 2024 13:54:26 GMT</pubDate>
    </item>
    <item>
      <title>在深度训练/验证循环期间使用分层 k 折叠时出现越界错误</title>
      <link>https://stackoverflow.com/questions/78473057/out-of-bounds-error-when-using-stratified-k-fold-during-deep-train-validation-lo</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78473057/out-of-bounds-error-when-using-stratified-k-fold-during-deep-train-validation-lo</guid>
      <pubDate>Mon, 13 May 2024 14:43:39 GMT</pubDate>
    </item>
    <item>
      <title>将 PyG 数据对象列表转换为 PyG 数据集？</title>
      <link>https://stackoverflow.com/questions/78433332/turning-a-list-of-pyg-data-objects-into-a-pyg-dataset</link>
      <description><![CDATA[我有一个 torch_geometric.data.Data 对象的 python 列表（每个对象代表一个图形）。我没有简单的方法来访问这些数据的原始文件：我只有列表。我需要将此数据对象列表转换为 torch_geometric.data.InMemoryDataset 或 torch_geometric.data.Dataset 对象，以便将其与我没有编写的更大的代码库集成。我该怎么做？
需要明确的是，我知道可以使用一系列数据对象来创建 torch_geometric.data.DataLoader 对象。但是，我特别需要一个 Dataset 对象，而不是 DataLoader 对象，因为较大的代码库在将 Dataset 对象转换为加载器之前会对它们执行一些额外的处理步骤。
我不明白为什么 PyG 让这变得如此困难。难道没有一种非常简单的方法可以做到这一点吗？
我尝试使用一个简单的 CustomDataset 类
类 CustomDataset(InMemoryDataset):
    def __init__(自身，数据)：
        超级().__init__()
        self.data = 数据
    
    def __len__(自身):
        返回 len(self.data)
    
    def __getitem__(self, idx):
        样本 = self.data[idx]
        返回样品

当尝试获取索引 0 处的 Data 对象时，它给了我一个 KeyIndex 错误。我还尝试了上述代码的一个版本，其中超类是 Dataset 而不是 InMemoryDataset，但我不知道如何制作整理方法有效。]]></description>
      <guid>https://stackoverflow.com/questions/78433332/turning-a-list-of-pyg-data-objects-into-a-pyg-dataset</guid>
      <pubDate>Sun, 05 May 2024 18:30:03 GMT</pubDate>
    </item>
    <item>
      <title>如何绘制多类分类中所有类的 SHAP 摘要图</title>
      <link>https://stackoverflow.com/questions/78396068/how-to-plot-shap-summary-plots-for-all-classes-in-multiclass-classification</link>
      <description><![CDATA[我正在使用 XGBoost 和 SHAP 来分析多类分类问题中的特征重要性，并且需要帮助一次性绘制所有类的 SHAP 摘要图。目前，我一次只能生成一个类的绘图。
SHAP 版本：0.45.0
Python版本：3.10.12

这是我的代码：
将 xgboost 导入为 xgb
导入形状
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.datasets 导入 make_classification
从 sklearn.metrics 导入 precision_score

# 生成合成数据
X，y = make_classification（n_samples = 500，n_features = 20，n_informative = 4，n_classes = 6，random_state = 42）
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 训练用于多类分类的 XGBoost 模型
模型 = xgb.XGBClassifier(objective=“multi:softprob”, random_state=42)
model.fit(X_train, y_train)

然后我尝试绘制形状值：
# 创建一个 SHAP TreeExplainer
解释器 = shap.TreeExplainer(模型)

# 计算测试集的SHAP值
shap_values = 解释器.shap_values(X_test)

# 尝试绘制所有类的摘要
shap.summary_plot（shap_values，X_test，plot_type =“酒吧”）

我得到了这个交互图：

我在此帖子：
shap.summary_plot(shap_values[:,:,0], X_test,plot_type=&quot;bar&quot;)

它给出了 0 类的正常条形图：

然后我可以对类 1、2、3 等执行相同的操作。
问题是，如何为所有类别制作汇总图？即，显示某个特征对每个类的贡献的单个图？]]></description>
      <guid>https://stackoverflow.com/questions/78396068/how-to-plot-shap-summary-plots-for-all-classes-in-multiclass-classification</guid>
      <pubDate>Sat, 27 Apr 2024 19:02:16 GMT</pubDate>
    </item>
    <item>
      <title>错误消息 R - PrettyNum(.Internal(format(x,rim,digits,nsmall,width,3L,：‘digits’参数的值 0 无效”</title>
      <link>https://stackoverflow.com/questions/75961903/error-message-r-prettynum-internalformatx-trim-digits-nsmall-width-3l</link>
      <description><![CDATA[我想知道是否有人可以帮助我识别和解决使用 LCAvarsel 时遇到的错误术语。
就上下文而言，该库旨在用于潜在类分析中的变量选择。
install.packages(“LCAvarsel”)
install.packages(“poLCA”)
库（LCAvarsel）
图书馆（poLCA）

数据（癌）
sel1 &lt;- LCAvarsel（癌）

返回以下错误：
prettyNum(.Internal(format(x,rim,digits,nsmall,width,3L,:
“数字”参数的值 0 无效
https://www.rdocumentation.org/packages/LCAvarsel /versions/1.1/topics/LCAvarsel]]></description>
      <guid>https://stackoverflow.com/questions/75961903/error-message-r-prettynum-internalformatx-trim-digits-nsmall-width-3l</guid>
      <pubDate>Fri, 07 Apr 2023 20:59:28 GMT</pubDate>
    </item>
    </channel>
</rss>