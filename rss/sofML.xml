<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 13 Mar 2024 09:14:20 GMT</lastBuildDate>
    <item>
      <title>使用 AWS Sagemaker Jumpstart Python SDK 微调 Llama 2</title>
      <link>https://stackoverflow.com/questions/78152358/fine-tune-llama-2-with-aws-sagemaker-jumpstart-python-sdk</link>
      <description><![CDATA[我正在尝试使用自定义数据微调 Llama 2 7B 模型，并且正在按照多个 AWS 博客文章的指导，使用带有 Sagemaker Python SDK 的笔记本在 AWS Sagemaker 中开始训练作业
在我准备数据并对数据进行建模并提示进行指令微调并尝试使用估计器启动训练作业后，我在初始化期间收到错误
具体是这样的：
运行时错误：命令 &#39;[&#39;torchrun&#39;、&#39;--nnodes&#39;、&#39;1&#39;、&#39;--nproc_per_node&#39;、&#39;4&#39;、&#39;llama_finetuning.py&#39;、&#39;--model_name&#39;、&#39;/opt /ml/additonals3data&#39;, &#39;--num_gpus&#39;, &#39;4&#39;, &#39;--pure_bf16&#39;, &#39;--dist_checkpoint_root_folder&#39;, &#39;model_checkpoints&#39;, &#39;--dist_checkpoint_folder&#39;, &#39;微调&#39;, &#39;--batch_size_training&#39;, &#39;4&#39;, &#39;--micro_batch_size&#39;, &#39;4&#39;, &#39;--train_file&#39;, &#39;无&#39;, &#39;--lr&#39;, &#39;0.0001&#39;, &#39;--do_train&#39;, &#39;--output_dir&#39;, &#39;saved_peft_model&#39;, &#39;--num_epochs&#39;、&#39;5&#39;、&#39;--use_peft&#39;、&#39;--peft_method&#39;、&#39;lora&#39;、&#39;--max_train_samples&#39;、&#39;-1&#39;、&#39;--max_val_samples&#39;、&#39;-1&#39;、&#39;- -seed&#39;, &#39;10&#39;, &#39;--per_device_eval_batch_size&#39;, &#39;1&#39;, &#39;--max_input_length&#39;, &#39;4096&#39;, &#39;--preprocessing_num_workers&#39;, &#39;--None&#39;, &#39;--validation_split_ratio&#39;, &#39;0.2&#39;, &#39;--train_data_split_seed&#39;, &#39;0&#39;, &#39;--num_workers_dataloader&#39;, &#39;0&#39;, &#39;--weight_decay&#39;, &#39;0.1&#39;, &#39;--lora_r&#39;, &#39;8&#39;, &#39;--lora_alpha&#39;, &#39;32&#39;, &#39;--lora_dropout&#39;、&#39;0.05&#39;、&#39;--enable_fsdp&#39;、&#39;--add_input_output_demarcation_key&#39;、&#39;--validation_file&#39;、&#39;/opt/ml/input/data/validation&#39;、&#39;--instruction_tuned&#39;]&#39;返回非- 零退出状态 1。

ErrorMessage“TypeError：需要 str、bytes 或 os.PathLike 对象，而不是 NoneType

为了提供有关此错误的更多上下文，我注意到训练作业使用的 torchrun 命令中的 --train_file 标志被设置为 None（？），但我找不到方法修改它
这是我用来启动训练作业的命令：
output_bucket = sess.default_bucket()

local_train_data_file =“train/train.jsonl”；
local_validation_data_file =“train/val.jsonl”；

train_data_location = f“s3://{output_bucket}/llama2_7b_get_one_ZD/train”
validation_data_location = f“s3://{output_bucket}/llama2_7b_get_one_ZD/val”

s3_output_location = f“s3://{output_bucket}/llama2_7b_get_one_ZD/output”


S3Uploader.upload(local_train_data_file,train_data_location)
S3Uploader.upload(local_validation_data_file,validation_data_location)
S3Uploader.upload(“train/template.json”, train_data_location)

print(f&quot;训练数据：{train_data_location}&quot;)

estimator.fit({“训练”：train_data_location，“验证”：validation_data_location}，job_name =&#39;meta-text Generation-llama-2-7b-one-zd-2&#39;，logs = True)


这是我的模型定义
model_id = “meta-text Generation-llama-2-7b”
model_version =“3.1.0”
模型 = JumpStartModel(model_id=model_id, model_version=model_version, 角色=
                       角色，sagemaker_session=sess)

估计器 = JumpStartEstimator(
    model_id=model_id,
    模型版本=模型版本，
    角色=角色，
    sagemaker_session=sess,
    环境={“accept_eula”：“true”}，
    instance_type=“ml.g5.12xlarge”，
    disable_output_compression=真，
    实例计数=1，
）
estimator.set_hyperparameters(
    max_input_length=“4096”，
    instructions_tuned=“真”，
    lora_r=“8”，
    时代=“1”，
）

我完全遵循博客文章中的文档建议我做的事情，即使使用数据集库中的训练数据，任务也会失败并出现相同的错误。这是 链接到 aws_sample 存储库
我注意到，在此示例中使用的模型版本是 2.*，但我无法使用它，因为它存在安全漏洞，并且 SDK 不允许使用该版本（默认为 2.1.8）
由于3.1.0是一个主要版本升级，我想知道是否训练数据或初始化失败了
我想找到错误的根源，并获得有关如何进一步排除故障的反馈]]></description>
      <guid>https://stackoverflow.com/questions/78152358/fine-tune-llama-2-with-aws-sagemaker-jumpstart-python-sdk</guid>
      <pubDate>Wed, 13 Mar 2024 08:52:29 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试通过训练迭代计算余弦相似度时，如何选择模型权重</title>
      <link>https://stackoverflow.com/questions/78152246/how-do-i-select-model-weights-when-i-try-to-calculate-cosine-similarity-though-t</link>
      <description><![CDATA[我正在尝试计算“t”步骤的权重值和“t-1”步骤的权重值之间的余弦相似度。
我正在使用 LSTM 网络 &amp; 2FC层。
我想检查训练期间的体重差异。
但是如果我想检查这些事情，我想知道我是只选择 LSTM 层的权重还是全部（LSTM，2FC 权重）或最终层的权重（FC_2 权重）
谢谢。
请帮助我。我不知道如何细化余弦相似条件下选择图层的最佳解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78152246/how-do-i-select-model-weights-when-i-try-to-calculate-cosine-similarity-though-t</guid>
      <pubDate>Wed, 13 Mar 2024 08:34:22 GMT</pubDate>
    </item>
    <item>
      <title>提高特殊神经网络的准确性</title>
      <link>https://stackoverflow.com/questions/78151448/increase-accuracy-in-a-special-neural-network</link>
      <description><![CDATA[事实上，我正在训练一个神经网络来玩 2048 游戏，并且大部分时间都达到 2048。
（如果您不了解该游戏，可以此处玩它来了解我在说什么）
首先，我收集了近 30,000 个游戏的数据集，在 csv 文件中达到 2048 个。每个游戏包含大约 1000 个棋盘游戏状态，可以说我的数据集中有 3000 万个棋盘状态。实际上有 3000 万行，如下图所示。

正如你所见，我每行有 17 列。前 16 列显示每个图块值的 2 的幂（预计 0 表示该图块中没有任何内容）。
最后一列是它根据这个棋盘游戏移动的方向
方向帮助-&gt; （0：上，1：右，2：下，3：左）
例如上图显示了董事会的这种状态：
&lt;前&gt;&lt;代码&gt; 32 64 128 32
 8 32 8 2
 8 0 0 0
 0 2 0 0

此状态的方向为 0，等于向上
所以我创建了一个具有 16 个输入的神经网络（当然我将其更改为 16*11 输入并将输入作为 one-hot 编码传递）、一些隐藏层和 4 个输出。
我在超参数和改变层结构方面进行了很多尝试和错误。
最后我得到了这样的结果。
导入 pandas 作为 pd
从 keras.models 导入顺序
从 keras.layers 导入密集、批量标准化、激活
从 keras.optimizers 导入 Adam
从 keras.callbacks 导入 EarlyStopping
从 sklearn.preprocessing 导入 OneHotEncoder
将 numpy 导入为 np

# 加载你的数据集
数据 = pd.read_csv(&#39;mainDataSet.csv&#39;)

# 分离输入 (X) 和输出 (y)
X = data.iloc[:, 0:16] # 输入特征
y = pd.get_dummies(data.iloc[:, -1]) # 将输出转换为 one-hot 编码

# 定义 one-hot 编码的类别（标签从 0 到 10）
类别 = [i for i in range(11)]

# 对每个输入列应用 one-hot 编码
X_encoded = pd.concat([pd.get_dummies(pd.Categorical(X[col],categories=categories), prefix=col,
prefix_sep=&#39;_&#39;) 对于 X] 中的列，轴=1)



# 定义模型
模型=顺序（）
model.add(Dense(16*11, input_dim=16*11,activation=&#39;relu&#39;)) # 默认包含偏差
model.add(BatchNormalization()) # 添加批量归一化层
model.add(Dense(256,activation=&#39;relu&#39;)) # 默认情况下包含偏差
model.add(BatchNormalization()) # 添加批量归一化层
model.add(Dense(256,activation=&#39;relu&#39;)) # 默认情况下包含偏差
model.add(BatchNormalization()) # 添加批量归一化层
model.add(Dense(256,activation=&#39;relu&#39;)) # 默认情况下包含偏差
model.add(BatchNormalization()) # 添加批量归一化层
model.add(Dense(256,activation=&#39;relu&#39;)) # 默认情况下包含偏差
model.add(BatchNormalization()) # 添加批量归一化层d
model.add(Dense(4,activation=&#39;softmax&#39;)) # 具有 4 个方向节点的输出层

# 定义提前停止回调，以在验证损失停止改善时停止训练
Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 5，restore_best_weights = True）

# 使用 Adam 优化器和默认学习率编译模型
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=&#39;adam&#39;, 指标=[&#39;accuracy&#39;])

# 使用小批量梯度下降和附加回调来训练模型
model.fit（X_encoded，y，epochs = 100，batch_size = 64，validation_split = 0.2，callbacks =
[早停]）

model.save(&#39;2048_model.h5&#39;)

最终它给了我 88% 的准确度和 90% 的验证准确度，但这对于我正在做的事情来说还不够。
那么您建议采取哪些方法来使我的模型在预测方面表现更好？
或者甚至您可以对我的神经网络进行哪些更改以使其更加高效？
你知道我是人工智能世界的新手，我可能不知道很多事情。因此，请随意告知您所知道的任何信息来改进此模型。]]></description>
      <guid>https://stackoverflow.com/questions/78151448/increase-accuracy-in-a-special-neural-network</guid>
      <pubDate>Wed, 13 Mar 2024 05:36:20 GMT</pubDate>
    </item>
    <item>
      <title>简单的 Pytorch LSTM 模型无法学习反事实创建</title>
      <link>https://stackoverflow.com/questions/78151232/simple-pytorch-lstm-model-not-learning-for-counterfactual-creation</link>
      <description><![CDATA[我正在 Pytorch 中构建 LSTM，以创建水流时间序列的反事实预测。为了预测第 i 天的水流 ($\hat{y}_i$)，我的特征集 $X_i$ 包含当天以及过去几天的天气数据。其他模型已经很好地学习了信号（即 LGBM），即使使用惩罚回归，特征集也可以解释水流的大部分变化。
尽管如此，LSTM 似乎并没有学到太多东西，而是学到了平均值。这与我在学习过程中传递数据的方式有关吗？
（丢弃超参数，这些超参数是随机初始化的。稍后我将使用 optuna 对其进行优化）
非常感谢您的帮助！
代码：
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

类 LSTMModel(nn.Module):
    def __init__(自身、输入大小、隐藏大小、层数、输出大小):
        super(LSTMModel, self).__init__()
        self.hidden_​​size = 隐藏大小
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size,hidden_​​size,num_layers,batch_first=True)
        self.fc = nn.Linear(隐藏大小, 输出大小)

    def 前向（自身，x）：
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_​​size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_​​size).to(device)
        
        # print(f“lstm 调用开始处的 x 大小：{x.size()}”)

        out, _ = self.lstm(x, (h0, c0))

        # print(f&quot;前向调用开始时的 x 大小：{x.size()}&quot;)
        out = self.fc(out[:, -1, :]) # 获取最后一个时间步的输出
        返回

dta =“../ml_dataset_Jan24.dta”

数据 = pd.read_stata(dta)

data[&#39;datetime&#39;] = data.apply(lambda x: pd.to_datetime(f&quot;{int(x[&#39;年&#39;])}-{int(x[&#39;月&#39;])}-{int(x[&#39;天&#39;])}&quot;，格式=&#39;%Y-%m-%d&#39;)，轴=1)
data.set_index(&#39;datetime&#39;, drop=True, inplace=True)
data.drop(columns=[&#39;年&#39;,&#39;月&#39;,&#39;日&#39;], inplace=True)

# 创建特征和目标集
X_train = data.loc[(data.index&gt;&#39;1990-12-31&#39;) &amp; (data.index&lt;&#39;2006-01-01&#39;), data.columns != &#39;level_cs&#39;].values
y_train = data.loc[(data.index&gt;&#39;1990-12-31&#39;) &amp; (data.index&lt;&#39;2006-01-01&#39;), &#39;level_cs&#39;].values


X_train = np.array(X_train)
X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
y_train = np.array(y_train)
y_train = y_train.reshape(y_train.shape[0], 1, 1)


X = torch.from_numpy(X_train).float()
y = torch.from_numpy(y_train).float()


# 定义超参数
input_size = 1332 # 特征数量
隐藏大小 = 128
层数 = 5
输出大小 = 1

# 定义训练设备（CPU 或 GPU）
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

# 实例化模型
模型 = LSTMModel(input_size,hidden_​​size,num_layers,output_size).to(device)

# 定义损失函数和优化器
标准 = nn.MSELoss()
优化器 = torch.optim.Adam(model.parameters(), lr=0.001)

# 训练模型
纪元数 = 500
对于范围内的纪元（num_epochs）：
    模型.train()
    优化器.zero_grad()

    # print(f&#39;X 尺寸: &#39;, X.size())
    输出 = 模型(X.to(设备))
    损失 = 标准（输出，y.to（设备））
    loss.backward()
    优化器.step()
    
    print(f&#39;Epoch [{epoch+1}/{num_epochs}], 损失: {loss.item():.4f}&#39;)

# 将模型设置为评估模式
模型.eval()

# 推论
# 使用 torch.no_grad():
# 预测 = 模型(X_test.to(设备))

# print(“样本预测：”, 预测)`

Y_test 预测基本上只是平均值
我批量传递了数据，Y_train 的时间戳与 X_train 中的时间戳相同，因为我想使用日期“i”中的数据来预测水流“i”。该功能集还包括过去的天气数据，我不确定是否应该如此，或者该数据的重要性是否会被记忆机制保留]]></description>
      <guid>https://stackoverflow.com/questions/78151232/simple-pytorch-lstm-model-not-learning-for-counterfactual-creation</guid>
      <pubDate>Wed, 13 Mar 2024 04:21:51 GMT</pubDate>
    </item>
    <item>
      <title>如何在我的 Streamlit 应用程序中实现模型</title>
      <link>https://stackoverflow.com/questions/78151205/how-do-i-implement-a-model-in-my-streamlit-application</link>
      <description><![CDATA[我已经克隆了 GitHub 代表并用它来生成实时语音克隆。它正在工作..我正在 cmd 中执行命令..以获得我的声音的实时语音克隆。现在我希望模型能够训练我的声音，并且我想将它用于我的项目，该项目应该位于流式应用程序中。首先，我不知道模型在用我的声音提供 tts 时是否会自行训练。但它没有使用我的语音数据集进行训练过程。相反，我给出了一个录音和来自 if 的录音。它为输入文本生成语音。其次，如果它经过训练，我如何使用该模型。
我尝试检查模型是否会保存合成语音，并且尝试实现该模型，但我不断收到错误消息，提示未找到模块。但我将它添加到我的路径中。它仍然显示错误。我很困惑]]></description>
      <guid>https://stackoverflow.com/questions/78151205/how-do-i-implement-a-model-in-my-streamlit-application</guid>
      <pubDate>Wed, 13 Mar 2024 04:12:33 GMT</pubDate>
    </item>
    <item>
      <title>多元回归任务，其中有些变量很容易预测，有些变量很难预测，有些则无法预测</title>
      <link>https://stackoverflow.com/questions/78151203/multivariate-regression-task-where-some-variables-are-easy-some-hard-and-some-i</link>
      <description><![CDATA[我正在尝试在 keras 中创建一个多元回归模型，但无论输入是否正确，模型最终都会预测单个值。
我尝试修复学习率、批量大小和模型架构，但到目前为止我无法修复它。
经过进一步检查，我知道输出向量中的某些值对于模型来说很难或不可能预测（例如从他喜欢的汽车品牌推断出一个人的身高），但由于问题的性质，我不知道这些难以或不可能预测的变量是哪些。
我目前正在使用 MSE 损失。我怀疑我必须使用或创建一个自定义损失函数来处理此类问题。
Chatgpt提出了异方差损失，但我不明白它的意思。]]></description>
      <guid>https://stackoverflow.com/questions/78151203/multivariate-regression-task-where-some-variables-are-easy-some-hard-and-some-i</guid>
      <pubDate>Wed, 13 Mar 2024 04:12:10 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试使用数据集包创建数据集时，出现“无法转换，因为列名称不匹配”错误</title>
      <link>https://stackoverflow.com/questions/78151170/im-getting-couldnt-cast-because-column-names-dont-match-error-while-i-was-t</link>
      <description><![CDATA[DataFrame 结构
上图显示了我的数据结构。
from sklearn.model_selection import train_test_split
从数据集中导入特征、ClassLabel、值、数据集、DatasetDict

df_train, df_tmp = train_test_split(
        movie_df,stratify=movie_df[“标签”], test_size=0.2)

df_val, df_test = train_test_split(
        df_tmp,stratify=df_tmp[“标签”], test_size=0.5)

ds_features = Features({“text”: Value(“string”), “label”: ClassLabel(names=labels)})

数据集 = DatasetDict({
    “火车”：Dataset.from_pandas(df_train.reset_index(drop=True),features=ds_features),
    “有效”：Dataset.from_pandas(df_val.reset_index(drop=True),features=ds_features),
    “测试”：Dataset.from_pandas(df_test.reset_index(drop=True),features=ds_features)})

数据集

这段代码给了我一个值错误，如下所示：
错误
错误
我期待类似的东西，但不具有相同的值：
DatasetDict({
    火车：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：13267
    })
    有效：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：1658
    })
    测试：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：1659
    })
})

谁能告诉我我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78151170/im-getting-couldnt-cast-because-column-names-dont-match-error-while-i-was-t</guid>
      <pubDate>Wed, 13 Mar 2024 04:00:13 GMT</pubDate>
    </item>
    <item>
      <title>有人有可行的 Atari 突破性深度 Q 学习实现吗？</title>
      <link>https://stackoverflow.com/questions/78151115/does-anyone-have-an-atari-breakout-deep-q-learning-implementation-that-works</link>
      <description><![CDATA[我一直在尝试使用 Keras 提供的实现，但收到此错误：
19 # 由于 Deepmind 辅助函数而使用基线 Atari 环境
---&gt; 20 env = make_atari(“BreakoutNoFrameskip-v4”)
21 # 扭曲帧，灰度，放四帧并缩放到更小的比例
22 env=wrap_deepmind(env,frame_stack=True,scale=True)
3帧
/usr/local/lib/python3.10/dist-packages/gym/envs/atari/environment.py 种子（自身，种子）
185
186 如果不是 hasattr(roms, self._game):
--&gt; 187 引发错误。错误（
188 f&#39;我们无法找到游戏“{self._game}”。注意：Gym 不再分发 ROM。 &#39;
189 f“如果您拥有将必要的 ROM 用于研究目的的许可证，您可以下载它们”
错误：我们无法找到“Breakout”游戏。注意：Gym 不再分发 ROM。如果您拥有将必要的 ROM 用于研究目的的许可证，您可以通过 pip installgym[accept-rom-license] 下载它们。否则，您应该尝试导入“Breakout”通过命令ale-import-roms。如果您认为这是一个错误，也许您的“Breakout”副本是错误的。不受支持。要检查是否属于这种情况，请尝试提供环境变量 PYTHONWARNINGS=default::ImportWarning:ale_py.roms。有关更多信息，请参阅：https://github.com/mgbellemare/Arcade-Learning -环境#rom-管理
任何帮助将不胜感激，谢谢。
我尝试过许多版本的 Atari 环境（这里括号里的东西
就这一行 --&gt; env = make_atari(“BreakoutNoFrameskip-v4”).我尝试过不同的版本等。一个有趣的注意事项是，当我使用“ALE/Breakout-v5”时，我遇到了不同的错误。
NamespaceNotFound Traceback（最近一次调用最后一次）
 在&lt;细胞系：81&gt;()
79
80 # 由于 Deepmind 辅助函数而使用 Baseline Atari 环境
---&gt; 81 env = make_atari(“ALE/Breakout-v5”)
82 # 扭曲帧，灰度，放四帧并缩放到更小的比例
83 env = wrap_deepmind(env,frame_stack=True,scale=True)
4帧
_check_namespace_exists(ns) 中的/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py
178）
179
--&gt; 180 raise error.NamespaceNotFound(f“未找到命名空间 {ns}。{suggestion_msg}”)
181
182
NamespaceNotFound：未找到命名空间 ALE。您是否为 ALE 安装了正确的软件包？
最终，我非常迷失，如果有人可以链接我可以运行的 Atari Breakout 的功能版本，我将非常感激。]]></description>
      <guid>https://stackoverflow.com/questions/78151115/does-anyone-have-an-atari-breakout-deep-q-learning-implementation-that-works</guid>
      <pubDate>Wed, 13 Mar 2024 03:36:22 GMT</pubDate>
    </item>
    <item>
      <title>使用时间序列数据和离散质量测量进行制造质量预测</title>
      <link>https://stackoverflow.com/questions/78150896/manufacturing-qaulity-prediction-with-timeseries-data-and-discrete-quality-measu</link>
      <description><![CDATA[我有 18 个批次的制造过程的时间序列数据，每个批次有 6 个变量的 2000 行时间序列数据。我还每批次进行 5 次输入质量测量，并尝试预测每批次测量一次的最终产品质量。以下是我拥有的所有数据的示例格式。我能否知道可以使用这些数据构建什么样的模型，以及如何处理时间序列数据以得出结合了时间序列数据和输入条件信息的预测模型。我已经尝试过一些模型，将时间序列数据转换为宽格式，使每个时间戳成为一个变量，并合并输入条件以创建 PLS 预测模型，其准确度为 50%。我想探索机器学习方法来对这些数据进行建模，但我是机器学习的新手。任何想法表示赞赏。谢谢
]]></description>
      <guid>https://stackoverflow.com/questions/78150896/manufacturing-qaulity-prediction-with-timeseries-data-and-discrete-quality-measu</guid>
      <pubDate>Wed, 13 Mar 2024 02:08:13 GMT</pubDate>
    </item>
    <item>
      <title>使用 sklearn，其中标签是多个输入的组合</title>
      <link>https://stackoverflow.com/questions/78150382/using-sklearn-where-the-label-a-combination-of-multiple-inputs</link>
      <description><![CDATA[我正在对分类标签相互关联的数据集进行数据分析。
我的标签跟踪实验条件。
就我而言，标签跟踪两种化学物质的组合浓度，这些化学物质产生由 n 个特征测量的输出。
使用分类标签代替化学物质组合的浓度是最佳做法，还是有更好的方法？
以下是分类标签与其代表的现实生活条件之间的转换示例。

&lt;标题&gt;

条件
化学1
化学2


&lt;正文&gt;

1
1
0


2
2
0


3
0
1


4
0
2


5
1
1


6
1
2


]]></description>
      <guid>https://stackoverflow.com/questions/78150382/using-sklearn-where-the-label-a-combination-of-multiple-inputs</guid>
      <pubDate>Tue, 12 Mar 2024 22:39:35 GMT</pubDate>
    </item>
    <item>
      <title>python darts 中的 RNN 训练指标</title>
      <link>https://stackoverflow.com/questions/78144820/rnn-training-metrics-in-python-darts</link>
      <description><![CDATA[我目前正在使用 python darts 训练 RNNModel。为了比较不同的训练模型，我想从 fit 方法中提取 train_loss 和 val_loss 。我该怎么做？我读过一些有关指标集合的内容，但不知道如何使用它。
这是我当前的代码
from darts.models import RNNModel
从 darts 导入 TimeSeries

train = # 训练数据为 TimeSeries
模型 = RNNModel(模型 =“LSTM”, input_chunk_length=self.past_samples)
模型.fit(火车)

训练过程中损失会显示在控制台中，但我不知道如何访问它。
到目前为止，我已尝试在线查找任何文档，并向 bing chat 和 ChatGPT 寻求帮助。然而他们告诉我使用不存在的model.history.history[“loss”]]]></description>
      <guid>https://stackoverflow.com/questions/78144820/rnn-training-metrics-in-python-darts</guid>
      <pubDate>Tue, 12 Mar 2024 05:29:49 GMT</pubDate>
    </item>
    <item>
      <title>在 SageMaker 上的 TensorFlow Recommenders 中初始化 FactorizedTopK 时出错：“无法将‘计数器’转换为形状”</title>
      <link>https://stackoverflow.com/questions/78144515/error-initializing-factorizedtopk-in-tensorflow-recommenders-on-sagemaker-cann</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78144515/error-initializing-factorizedtopk-in-tensorflow-recommenders-on-sagemaker-cann</guid>
      <pubDate>Tue, 12 Mar 2024 03:28:18 GMT</pubDate>
    </item>
    <item>
      <title>预测分位数与梯度增强回归相交</title>
      <link>https://stackoverflow.com/questions/78140825/prediction-quantiles-intersect-from-gradient-boosted-regression</link>
      <description><![CDATA[我正在尝试构建一个回归模型，该模型接收各种类型的产品和市场信息，对数据进行转换，并在一天中定期预测产品的价格。我希望能够预测分位数置信带，以帮助我的团队根据模型的预测做出决策。我遵循了 scikit-learn 文档中的示例&lt; /code&gt;，但我发现这些回归器产生的预测有时会相交，例如有时P50预测大于P75预测或小于P25预测。
查看此处突出显示的图表中的错误。
即使使用更宽的频段（包括 P95 和 P05），这个问题仍然存在。尽管数据集比我正在使用的数据集简单得多，但我已经能够毫无问题地重现链接的示例。
下面的代码代表了该问题的可重现示例，并带有生成的数据集：
# 导入和定义
将 numpy 导入为 np
从 sklearn.ensemble 导入 HistGradientBoostingRegressor
从 sklearn.model_selection 导入 train_test_split
从 sklearn.datasets 导入 make_regression

X,y = make_regression(
    n_样本 = 14000,
    n_特征 = 39,
    n_目标 = 1
）

# 进行测试/训练分割和模型字典
X_train, X_test, y_train, y_test = train_test_split(
    X、Y、测试大小=0.1
）
型号={}

for alpha in [0.25, 0.5, 0.75]: # HistGradientBoostingRegressor model - Early_stopping = False 有助于解决问题，但不能解决问题
    hgbr = HistGradientBoostingRegressor(
        max_iter=100, # 通常在1000左右，例如减少
        损失=“分位数”，
        分位数 = 阿尔法，
    ）

    # 将最佳模型添加到字典中
    models[f&quot;P{alpha*100:02.0f}&quot;] = hgbr.fit(X_train, y_train.ravel())


任何帮助或建议将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78140825/prediction-quantiles-intersect-from-gradient-boosted-regression</guid>
      <pubDate>Mon, 11 Mar 2024 13:05:06 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 Huggingface MT5 模型中执行批量编码时会得到不同的嵌入？</title>
      <link>https://stackoverflow.com/questions/78139855/why-do-i-get-different-embeddings-when-i-perform-batch-encoding-in-huggingface-m</link>
      <description><![CDATA[我正在尝试使用 HuggingFace 的 mt5-base 模型对一些文本进行编码。我使用的模型如下所示
从转换器导入 MT5EncoderModel、AutoTokenizer

模型 = MT5EncoderModel.from_pretrained(“google/mt5-base”)
tokenizer = AutoTokenizer.from_pretrained(“google/mt5-base”)

def get_t5_embeddings(文本):
    last_hidden_​​state = model(input_ids=tokenizer(texts, return_tensors=“pt”, padding=True).input_ids).last_hidden_​​state
    pooled_sentence = torch.max(last_hidden_​​state, 暗淡=1)
    返回 pooled_sentence[0].detach().numpy()

当我注意到相同的文本与其自身的余弦相似度分数较低时，我正在做一些实验。我做了一些挖掘，意识到如果我批量进行编码，模型会返回非常不同的嵌入。为了验证这一点，我运行了一个小实验，逐步生成 Hello 的嵌入和 10 个 Hello 的列表。并检查列表中 Hello 和第一个 Hello 的嵌入（两者应该相同）。
对于范围 (1, 10) 内的 i：
    print(i, (get_t5_embeddings([“你好”])[0] == get_t5_embeddings([“你好”]*i)[0]).sum())

这将返回嵌入中相互匹配的值的数量。
结果是这样的：
&lt;前&gt;&lt;代码&gt;1 768
2 768
3 768
4 768
5 768
6 768
7 768
8 27
9 27

每次运行它时，如果批量大小超过 768，就会出现不匹配情况。
为什么我会得到不同的嵌入以及如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78139855/why-do-i-get-different-embeddings-when-i-perform-batch-encoding-in-huggingface-m</guid>
      <pubDate>Mon, 11 Mar 2024 10:14:53 GMT</pubDate>
    </item>
    <item>
      <title>无监督自动编码器产生输出维度 - 不同大小数据集的批次数</title>
      <link>https://stackoverflow.com/questions/78107646/unsupervised-autoencoder-produce-output-dimensions-number-of-batches-for-diffe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78107646/unsupervised-autoencoder-produce-output-dimensions-number-of-batches-for-diffe</guid>
      <pubDate>Tue, 05 Mar 2024 12:12:37 GMT</pubDate>
    </item>
    </channel>
</rss>