<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 31 Mar 2025 15:19:16 GMT</lastBuildDate>
    <item>
      <title>如何提高机器学习模型的准确性[关闭]</title>
      <link>https://stackoverflow.com/questions/79546591/how-to-increase-the-accuracy-of-a-machine-learning-model</link>
      <description><![CDATA[我正在尝试训练一个模型，以通过我从Kaggle发现的情感文本数据库进行文本进行情感检测。目前，我正在使用验证的模型：“ distilbert-base-base uncord” 。但是，准确性太低为0.5。我正在尝试学习一种增加此方法以进行更好估计的方法。
任何人可以帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/79546591/how-to-increase-the-accuracy-of-a-machine-learning-model</guid>
      <pubDate>Mon, 31 Mar 2025 15:08:39 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：数据加载器对象不可订阅</title>
      <link>https://stackoverflow.com/questions/79546578/typeerror-dataloader-object-is-not-subscriptable</link>
      <description><![CDATA[我正在创建一个AI模型来生成人群的密度图。将数据集分为两个，一个用于培训，一个用于验证，我创建了两个数据集，然后尝试使用 torch.utils.data.dataloader（test_set，batch_size = batch_size = batch_size，shuffle = false））。之后，要测试数据，我迭代并使用下一个函数获取数据集的下一个元素，然后获得TypeError。
这是完整的代码：
  batch_size = 8 
设备=&#39;cuda：0&#39;如果torch.cuda.is_available（）else&#39;cpu&#39;

train_root_dir =＆quot; data/part_a/train_data/＆quot
init_training_set = dataloader（train_root_dir，shuffle = true）

＃将培训集的一部分分为验证集
train_size = int（0.9 * len（init_training_set））
val_size = len（init_training_set）-train_size

train_indices = list（range（train_size））
val_indices = list（range（train_size，len（init_training_set））））））
train_dataset = torch.utils.data.dataset.subset（init_training_set，train_indices）
val_dataset = torch.utils.data.dataset.subset（init_training_set，val_indices）

train_loader = torch.utils.data.dataloader（train_dataset，batch_size = batch_size，shuffle = true）
val_loader = torch.utils.data.dataloader（val_dataset，batch_size = batch_size，shuffle = false）

test_root_dir =＆quot; data/part_a/test_data/＆quot
test_set = dataloader（test_root_dir，shuffle = false）
test_loader = torch.utils.data.dataloader（test_set，batch_size = batch_size，shuffle = false）

dataiter = iter（train_loader）
ex_images，ex_dmaps，ex_n_people = next（dataiter）


＃显示图像和密度图
plot_corresponding_pairs（ex_images，ex_dmaps）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79546578/typeerror-dataloader-object-is-not-subscriptable</guid>
      <pubDate>Mon, 31 Mar 2025 15:02:15 GMT</pubDate>
    </item>
    <item>
      <title>与Skywise -Slate仪表板相关的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79545514/issue-related-to-skywise-slate-dashboard</link>
      <description><![CDATA[我正在努力修改空客Skywise中现有的仪表板。我的任务涉及：
 提取数据： 
用户单击“获取”时按钮，系统应从用户指定的列中获取数据并将其存储以供以后使用。
 手动描述输入： 
在“手动描述”中单元格，用户将手动粘贴来自空中客车数据库的文本，对应于特定的FUID ID。
 生成TDD评估： 
用户点击“生成评估”时，系统应：

使用先前获取的数据和手册说明
参考。
利用ML/AI模型来自动生成TDD评估
针对不同错误类型的预定义参考脚本。

我正在寻找有关如何实施此功能的指导。具体来说，我需要以下帮助：

提取和存储用户指定的列数据。
处理手动说明并将其链接到Fuid ID。
使用使用ML/AI基于ML/AI的自动生成TDD评估
参考脚本。
]]></description>
      <guid>https://stackoverflow.com/questions/79545514/issue-related-to-skywise-slate-dashboard</guid>
      <pubDate>Mon, 31 Mar 2025 04:58:02 GMT</pubDate>
    </item>
    <item>
      <title>如何使用R中的自定义内核进行SVM？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79545276/how-to-do-svm-with-a-custom-kernel-in-r</link>
      <description><![CDATA[ 内核方程  
我试图使用上面的自定义内核进行SVM，但我不知道该如何编码，有人知道我应该在哪里看吗？哪些软件包允许包括自定义内核？该内核应允许更高阶的相互作用，而不仅仅是常规多项式内核。]]></description>
      <guid>https://stackoverflow.com/questions/79545276/how-to-do-svm-with-a-custom-kernel-in-r</guid>
      <pubDate>Sun, 30 Mar 2025 23:33:57 GMT</pubDate>
    </item>
    <item>
      <title>当给出额外的较大V12权重（Yolov12x）时，Yolo为什么要下载Nano V11型号（Yolov11n）？</title>
      <link>https://stackoverflow.com/questions/79545081/why-does-yolo-download-a-nano-v11-model-yolov11n-when-given-the-extra-larger-v</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79545081/why-does-yolo-download-a-nano-v11-model-yolov11n-when-given-the-extra-larger-v</guid>
      <pubDate>Sun, 30 Mar 2025 19:43:01 GMT</pubDate>
    </item>
    <item>
      <title>将型号的重量从旧的Keras版本转换为Pytorch</title>
      <link>https://stackoverflow.com/questions/79544819/conversion-of-model-weights-from-old-keras-version-to-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79544819/conversion-of-model-weights-from-old-keras-version-to-pytorch</guid>
      <pubDate>Sun, 30 Mar 2025 15:41:21 GMT</pubDate>
    </item>
    <item>
      <title>如何保存Dynamax Gussianhmm型号？</title>
      <link>https://stackoverflow.com/questions/79544766/how-do-i-save-a-dynamax-gussianhmm-model</link>
      <description><![CDATA[我一直在基于本教程开发模型：
 https://probml.github.github.io/dynamax/namax/neamax/notebooks/notebooks/notebooks/gaussian/gaussian_phmmm.phmm.hmm.hmm.hmm.hmm.hmm.hmm.hmm.hmmm.hmm.hmm.hmm.hmt 但是，由于现在我的代码可能需要几天的运行，所以我希望将模型保存在中间，以便以后加载。
我尝试了JSON，Pickle和其他几种GPT建议，但无济于事。
因此，在我尝试保存模型的所有参数作为字符串之前，并在加载模型时将其转换回它们，我想知道是否有更轻松的选择。
这是我的代码的示例：
 来自dynamax.hidden_​​markov_model import import gaussianhmm
导入jax.random作为JR

嗯= Gaussianhmm（5，3）
param，properties = hmm.initialize（jr.prngkey（10））
 
为了保存我尝试的模型，例如：
 导入jax.numpy作为jnp
导入JAX
进口泡菜

def backup_hmm（params，props，filename =; hmm_backup_jax.pkl＆quort;）：
    ＃提取阵列安全
    params_flat，params_tree = jax.tree_util.tree_flatten（params）
    props_flat，props_tree = jax.tree_util.tree_flatten（props）

    ＃转换为普通列表
    params_flat = [jnp.array（p）for params_flat中的p]
    props_flat = [p props_flat中的p的jnp.array（p）]

    ＃保存到泡菜
    用开放式（文件名，“ wb”）为f：
        pickle.dump（{
            ＆quot&#39;params;：params_flat，
            ＆quot&#39;params_tree＆quort;：params_tree，
            ＆quot“ props”：props_flat，
            ＆quot“ props_tree”：props_tree
        }，f）

    打印（“备份完成。”

Def Restore_hmm（filename =＆quot; hmm_backup_jax.pkl＆quort;）：
    用开放式（文件名，rb＆quot）为f：
        data = pickle.load（f）

    ＃ 恢复
    params_flat = [jnp.array（p）for p in data [&#39;params; quot;]]
    props_flat = [jnp.Array（p）for p in Data [＆quots&#39;props; quot;]]

    ＃重建原始结构
    params = jax.tree_util.tree_unflatten（data [; params_tree; quord; quart&#39;; params_flat）
    props = jax.tree_util.tree_unflatten（data [＆quot; props_tree＆quot; quor＆quot; props_flat）

    打印（“恢复已完成。”
    返回参数，道具

 ]]></description>
      <guid>https://stackoverflow.com/questions/79544766/how-do-i-save-a-dynamax-gussianhmm-model</guid>
      <pubDate>Sun, 30 Mar 2025 14:57:55 GMT</pubDate>
    </item>
    <item>
      <title>l如何解决VCC16的输入数据大小问题</title>
      <link>https://stackoverflow.com/questions/79544595/how-can-l-solve-the-input-data-size-problem-of-vcc16</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79544595/how-can-l-solve-the-input-data-size-problem-of-vcc16</guid>
      <pubDate>Sun, 30 Mar 2025 12:34:18 GMT</pubDate>
    </item>
    <item>
      <title>将TensorFlow模型伪像到SageMaker</title>
      <link>https://stackoverflow.com/questions/79363695/deploying-a-tensorflow-model-artifact-to-sagemaker</link>
      <description><![CDATA[我正在尝试将张量流模型部署到萨吉式端点。我在generic_graph.pb上有模型伪像，并在labels.txt。
我首先创建一个带有以下内容的焦油文件：
 ＃＃MODEL目录结构 
＃＃Model.tar.gz
＃└ - ＆lt; model_name＆gt;
＃└ - ＆lt; version_number＆gt;
＃├├─pa
＃└ - 变量
＃├├─标签.txt
 
我将文件上传到S3中的存储桶中。然后，我尝试使用以下代码部署模型：
  sagemaker_session = sagemaker.session（）
角色=&#39;my-lole&#39;

model = tensorflowmodel（model_data =&#39;s3：//my-bucket/model.tar.gz&#39;，
                    角色=角色，
                    framework_version =&#39;2.3.0&#39;）


preditionor = model.deploy（prinity_instance_count = 1，instance_type =&#39;ml.m5.large&#39;）
 
我在我的CloudWatch日志中不断遇到以下错误：
  valueerror：找不到SavedModel捆绑包！
 
不确定还有什么尝试。]]></description>
      <guid>https://stackoverflow.com/questions/79363695/deploying-a-tensorflow-model-artifact-to-sagemaker</guid>
      <pubDate>Fri, 17 Jan 2025 05:19:16 GMT</pubDate>
    </item>
    <item>
      <title>Importerror：使用“ BitsandBytes” 8位量化需要加速</title>
      <link>https://stackoverflow.com/questions/78595127/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate</link>
      <description><![CDATA[从HuggingFace下载模型时，我遇到了错误。它在Google Colab上工作，但不在我的Windows机器上工作。我正在使用python 3.10.0。
错误代码如下：
  e：\ internships \ consciusai \ .venv \ lib \ lib \ site-packages \ huggingface_hub \ file _download.py：1132：1132：futureWarning：futureWarning：`remume_download&#39;s depected and depected and depected and将在版本1.0.0.0.0.0.0.0中删除。下载总是在可能的情况下恢复。如果要强制新下载，请使用`force_download = true`。
  警告。
未使用的Kwargs：[&#39;_load_in_4bit&#39;，&#39;_load_in_8bit&#39;，&#39;QUAT_METHOD&#39;]。这些Kwargs在＆lt; class&#39;Transformers.utils.quantization_config.bitsandbytesconfig&#39;＆gt;中不使用这些夸大。
e：\ internships \ consciusai \ .venv \ lib \ lib \ lib \ site-packages \ transformers \ ventrizers \ venterizers \ auto.py：159：userWarning：您通过`jentization_config`或等效参数to`from_pretrenained&#39;&#39;&#39;但您正在加载的模型已经加载了量化量化量化量化量化。将使用来自模型的`Quantization_Config`。
  WARNINGS.WARN（WARNNING_MSG）
Trackback（最近的最新电话）：
  file＆quort e：\ induthships \ consciusai \ emaim_2.py”，第77行，in＆lt; module＆gt;
    主要的（）
  file＆quot e：\ internships \ consciusai \ emaim_2.py; ,，第71行，主要
    摘要= summarize_email（内容）
  file＆quot e：\ internships \ consciusai \ emaim_2.py”，第22行，在summarize_email中
    管道=变形金刚。Pipeline（
  file＆quot&#39;e：\ internships \ consciusai \ .venv \ lib \ lib \ site-packages \ transformers \ pipelines \ pipelines \ __ init __ init __ in Int __.py＆quort&#39;＆quort&#39;＆quort&#39;＆quort&#39;＆quote＆quote＆quote＆quort in 906
    框架，型号= peash_framework_load_model（
  file＆quot; e：\实习\ consciusai \ .venv \ lib \ lib \ site-packages \ Transformers \ pipelines \ pipelines \ base.py＆quid＆quot＆quot＆quot＆quid＆quort＆quot＆quot＆quot＆quot＆quid＆quot＆quid＆quot＆quot＆quot＆quot＆quid＆quid＆quot＆quot＆quid＆quot＆quid＆quot＆quid＆quort of 283
    model = model_class.from_pretrataining（模型，** kwargs）
  file＆quot; e：\ internships \ consciusai \ .venv \ lib \ lib \ site-packages \ transformers \ transformers \ models \ auto \ auto \ auto_factory.py;
    返回model_class.from_pretaining（
  file＆quort;
    hf_quantizer.validate_environment（
  file＆quot;
    提高侵居者（
Importerror：使用`bitsandBytes` 8位量化都需要加速：`pip安装加速&#39;和最新版本的bitsandbytes：`pip install -i https：//pypi.org/simple/ bitsandble/ bitsandbytes&#39;
 
这是我使用的代码：
  def summarize_email（content）：
    model_id =＆quot&#39;unsploth/llama-3-8b-instruct-bnb-4bit;

    管道=变形金刚。Pipeline（
        ＆quot“文字生成”
        模型= model_id，
        model_kwargs = {
            ＆quot“ torch_dtype”：torch.float16，
            ＆quot&#39;ventalization_config;：{&#39;load_in_4bit＆quort;：true}，
            ＆quot“ low_cpu_mem_usage＆quot”：true，
        }，，
    ）

    消息= [
        {&#39;&#39;：＆quot; quot“ system; quot” content; quot; quot; quot;
        {&#39;&#39;：&#39;用户“ content” contents“：&#39;; quot; quot汇总给我的电子邮件+内容}，
    这是给出的

    提示= pipeline.tokenizer.apply_chat_template（
            消息，
            tokenize = false，
            add_generation_prompt = true
    ）

    终结者= [
        pipeline.tokenizer.eos_token_id，
        pipeline.tokenizer.convert_tokens_to_ids（“”）
    这是给出的

    输出=管道（
        迅速的，
        max_new_tokens = 256，
        eos_token_id =终止者，
        do_sample = true，
        温度= 0.6，
        top_p = 0.9，
    ）
 
我正在尝试使用“不舒服/Llama-3-8B-Instruct-Bnb-4bit”总结文本。来自拥抱面。
它确实在Google Colab和Kaggle上总结了文本，但没有在本地机器上进行。]]></description>
      <guid>https://stackoverflow.com/questions/78595127/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate</guid>
      <pubDate>Sat, 08 Jun 2024 08:37:18 GMT</pubDate>
    </item>
    <item>
      <title>无法下载MMCV 1.3.0并构建车轮</title>
      <link>https://stackoverflow.com/questions/77479005/not-able-to-download-mmcv-1-3-0-and-build-wheels</link>
      <description><![CDATA[当我尝试安装 mmcv-full == 1.3.0 时，它无法下载并构建轮子（我已经更新了车轮）
  错误无法为MMCV-Full构建车轮，这是安装pyproject.toml项目所需的
 但是当我尝试使用时
 MIM安装mmcv-full  
错误消息：
  RROR：MMCV-Full的建筑轮失败
  运行设置。
无法构建mmcv-full
错误：无法为MMCV-Full构建车轮，这是安装pyproject.toml的项目所需的
 
可以下载 mmcv-full 的最新版本，但是由于我试图克隆的存储库需要使用 MMCV版本1.3.0 。
我正在使用 Windows 11 ，想知道我应该如何下载版本。]]></description>
      <guid>https://stackoverflow.com/questions/77479005/not-able-to-download-mmcv-1-3-0-and-build-wheels</guid>
      <pubDate>Tue, 14 Nov 2023 08:00:38 GMT</pubDate>
    </item>
    <item>
      <title>我正在创建一个扑朔迷离的应用程序来检测手势并在文本中转换为聋人和愚蠢的人[封闭]</title>
      <link>https://stackoverflow.com/questions/75481225/i-am-creating-a-flutter-app-to-detect-hand-gestures-and-convert-them-in-text-for</link>
      <description><![CDATA[是否有人可以为我提供有关如何启动项目，从我可以获取数据集以及如何将手势转换为文本的路线图？还是有任何SDK或API，如果您能为我提供git链接，那就太好了]]></description>
      <guid>https://stackoverflow.com/questions/75481225/i-am-creating-a-flutter-app-to-detect-hand-gestures-and-convert-them-in-text-for</guid>
      <pubDate>Fri, 17 Feb 2023 07:21:32 GMT</pubDate>
    </item>
    <item>
      <title>pytorch` thorch.no_grad` vs`torch.inference_mode`</title>
      <link>https://stackoverflow.com/questions/69543907/pytorch-torch-no-grad-vs-torch-inference-mode</link>
      <description><![CDATA[ pytorch具有新的功能 torch.inference_mode ，如V1.9，它是&#39;https：///pytorch.org/docs/stable/generate/torgh..autograd.autograd.grad_mode.grad_mode.Inference一下 torch.no_grad  ...在此模式下运行的代码通过禁用视图跟踪和版本计数器颠簸而获得更好的性能。
如果我只是在测试时间评估我的模型（即未培训），是否有任何情况， torch.no_grad 比 torch.inferey_mode 更可取？我计划用后者替换前者的每个实例，我希望将运行时错误用作护栏（即，我相信任何问题都会表明自己是运行时错误，如果它不会表现为运行时错误，我假设它确实比使用 torch.inference_mode_mode ）。
在 pytorch开发者podcast 。]]></description>
      <guid>https://stackoverflow.com/questions/69543907/pytorch-torch-no-grad-vs-torch-inference-mode</guid>
      <pubDate>Tue, 12 Oct 2021 16:21:23 GMT</pubDate>
    </item>
    <item>
      <title>如何计算Bernoulli幼稚贝叶斯的联合日志样本</title>
      <link>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</link>
      <description><![CDATA[使用 bernoullinb ，如何对关节进行定量。联合的可能性由下方公式计算，其中y（d）是实际输出的数组（不是预测值），而x（d）是特征的数据集。
我阅读 href =“ https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/naive_bayes.py#l839“ rel =“ nofollow noreferrer”&gt; Documentation&gt; Documentation 但并没有完全满足我的目的。请有人可以 
帮助。 &lt;img alt = &lt;img alt =“ Enter Image Description在此处”]]></description>
      <guid>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</guid>
      <pubDate>Wed, 17 Oct 2018 18:08:50 GMT</pubDate>
    </item>
    <item>
      <title>训练后如何用分布的时间来替换嵌入层？</title>
      <link>https://stackoverflow.com/questions/39532572/how-to-replace-an-embedding-layer-with-a-time-distributed-dense-after-training</link>
      <description><![CDATA[我有以下问题：

 我想使用LSTM网络进行文本分类。为了加快训练的速度并使代码更加清楚，我想沿沿 keras.tokenizer 嵌入层以训练我的模型。 
 一旦我训练了我的模型 - 我想计算输出W.R.T.的显着性图。输入。为此，我决定将嵌入层替换为 timeDistributedDense 。 

您知道什么是最好的方法。对于一个简单的模型，我可以简单地使用已知权重的模型来重建模型 - 但我想使其尽可能通用 - 例如替换模型结构的未来并使我的框架尽可能不可知。]]></description>
      <guid>https://stackoverflow.com/questions/39532572/how-to-replace-an-embedding-layer-with-a-time-distributed-dense-after-training</guid>
      <pubDate>Fri, 16 Sep 2016 13:21:25 GMT</pubDate>
    </item>
    </channel>
</rss>