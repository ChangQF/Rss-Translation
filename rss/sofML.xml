<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 12 Aug 2024 21:16:03 GMT</lastBuildDate>
    <item>
      <title>为一系列点的图形创建邻接矩阵</title>
      <link>https://stackoverflow.com/questions/78863456/creating-an-adjacency-matrix-for-a-graph-of-points-in-a-series</link>
      <description><![CDATA[我想创建一种机制，使用邻接矩阵在一条指令中乘以多个 PyTorch 张量。这些点以串联方式相互连接，但有一个小问题。假设这是一个表示 T 点串联连接的邻接矩阵：
adj_mat = torch.tensor([
[0, 1, 0, 0, 0], # T1 连接到 T2
[1, 0, 1, 0, 0], # T2 连接到 T1 和 T3
[0, 1, 0, 1, 0], # T3 连接到 T2 和 T4
[0, 0, 1, 0, 1], # T4 连接到 T3 和 T5
[0, 0, 0, 1, 0], # T5 连接到 T4
], dtype = torch.float32)

我有多个数量需要对输入张量进行算术运算：
K = torch.tensor([
[1.0, 2.0, 3.0, 4.0, 0.0],
[1.0, 2.0, 3.0, 4.0, 0.0]
], dtype=torch.float32)
C = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], dtype = torch.float32)
S = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], dtype = torch.float32)

val = torch.tensor([[1.0, 2.0, 4.0, 8.0, 16.0],
[5.0, 10.0, 15.0, 20.0, 5.0]], dtype = torch.float32)

dev = torch.tensor([11, 30], dtype = torch.float32)

计算如下，遍历 Tensor 的每一行（其中每行将并行计算，希望如此）：
T1 = T1 + (C(T1) * S(T1) * (dev + (T2-T1) * K(T1))
T2 = T2 + (C(T2) * S(T2) * ((T1-T2) * K(T1) + (T3-T2) * K(T2))
T3 = T3 + (C(T3) * S(T3) * ((T2-T3) * K(T2) + (T4-T3) * K(T3))
T4 = T4 + (C(T4) * S(T4) * ((T3-T4) * K(T3)+ (T5-T4) * K(T4))

其中 T1-T5 的值将来自 val 数组。我尝试了类似的方法，首先检查加权差异，即 ((T2-T3) * K(T2) + (T4-T3) * K(T3)) 是否计算正确
T_diffs = val.unsqueeze(1) - val.unsqueeze(2)
T_diffs_weighted = T_diffs * adj_mat.unsqueeze(0) * K.unsqueeze(1)
T_diffs_summed = torch.sum(T_diffs_weighted , dim = 2)
print(&quot;weighted Differences:&quot;, T_diffs_weighted )
print(&quot;Summed Differences:&quot;, T_diffs_summed )

我得到的输出是：
加权差异：张量（[[[ 0., 2., 0., 0., 0.],
[ -1., 0., 6., 0., 0.],
[ -0., -4., 0., 16., 0.],
[ -0., -0., -12., 0., 0.],
[ -0., -0., -0., -32., 0.]],

[[ 0., 10., 0., 0., 0.],
[ -5., 0., 15., 0., -0.],
[ -0., -10., 0., 20., -0.],
[ -0., -0., -15., 0., -0.],
[ 0., 0., 0., 60., 0.]]])

而所需输出为1:
加权差异：张量（[[[ 0., 1., 0., 0., 0.],
[ -1., 0., 6., 0., 0.],
[ 0., -4., 0., 16., 0.],
[ 0., 0., -12., 0., 0.],
[ 0., 0., 0., -32., 0.]],

[[ 0., 5., 0., 0., 0.],
[ -5., 0., 15., 0., -0.],
[ 0., -10., 0., 20., -0.],
[ 0., 0., -15., 0., -0.],
[ 0., 0., 0., 60., 0.]]])

求和差值：张量（[[ 1., 1., 10., -16., 0.],
[ 0., 5., 0., 65., 0.]])

提取的值将是一列，例如，这意味着 T1 将对应于第 0 列。我似乎无法正确地乘以张量 K。我能够正确地计算方程的 K 值，但不能同时计算两者。
最后，如果还有其他方法可以执行此乘法或方程，我们将不胜感激。顺便说一下，这些方程是运行在 Nvidia GPU 上的机器学习模型的一部分。我对 PyTorch 的 [un]squeeze() 方法还比较陌生，这就是为什么这有点难以理解。
1与方程的预期数学输出相比，加权差分矩阵中值的符号可能被反转。但我关注的是与张量 K 的乘法。]]></description>
      <guid>https://stackoverflow.com/questions/78863456/creating-an-adjacency-matrix-for-a-graph-of-points-in-a-series</guid>
      <pubDate>Mon, 12 Aug 2024 20:18:41 GMT</pubDate>
    </item>
    <item>
      <title>ML-Agents：当我的游戏对象的任何部分发生碰撞时，代理为空</title>
      <link>https://stackoverflow.com/questions/78863425/ml-agents-agent-is-null-when-any-part-of-my-game-object-collides</link>
      <description><![CDATA[我确信这是一个非常简单的解决方案，但我无论如何也想不通。我正在根据 Unity 的 WalkerAgent 训练来训练布娃娃走路，我做了一些改动以更好地满足我的目的。现在，每当布娃娃的任何身体部位与地面碰撞时，代理都是空的，从而阻止任何奖励被应用。
我多次检查了我的代码，看起来没有任何问题。该项目似乎也设置正确。导致我陷入这种困境的原始错误是
&quot;nullreferenceexception: 未将对象引用设置为对象实例Unity.MLAgentsExamples.ObjectContact.OnCollisionStay (UnityEngine.Collision col)
(at Assets/Scripts/ObjectContact.cs:55)UnityEngine.Physics:OnSceneContact(PhysicsScene, IntPtr, Int32) (at /Users/bokken/build/output/unity/unity/Modules/Physics/ScriptBindings/PhysicsContact.bindings.cs:49)&quot;。
我很确定所有事情都按正确的顺序发生，并且代理应该在调用之前进行初始化。感谢大家的帮助。
据我所知，这些是代码的相关部分。
来自 WalkerAgent（主脚本）：
 public override void Initialize()
{
m_OrientationCube = GetComponentInChildren&lt;OrientationCubeController&gt;();

//设置每个身体部位
m_JdController = GetComponent&lt;JointDriveController&gt;();

m_JdController.SetupBodyPart(hips);
m_JdController.SetupBodyPart(spine);
m_JdController.SetupBodyPart(head);
m_JdController.SetupBodyPart(legrotateL);
m_JdController.SetupBodyPart(thighL);
m_JdController.SetupBodyPart(kneerotateL);
m_JdController.SetupBodyPart(shinL);
m_JdController.SetupBodyPart(左脚);
m_JdController.SetupBodyPart(右腿旋转);
m_JdController.SetupBodyPart(右大腿);
m_JdController.SetupBodyPart(右膝盖旋转);
m_JdController.SetupBodyPart(右胫骨);
m_JdController.SetupBodyPart(左脚);
m_JdController.SetupBodyPart(左手臂旋转);
m_JdController.SetupBodyPart(左手臂旋转);
m_JdController.SetupBodyPart(左手臂旋转);
m_JdController.SetupBodyPart(左前臂旋转);
m_JdController.SetupBodyPart(左前臂旋转);
m_JdController.SetupBodyPart(右手臂旋转);
m_JdController.SetupBodyPart(右手臂旋转);
m_JdController.SetupBodyPart(右手臂旋转);
m_JdController.SetupBodyPart(右手臂旋转);
m_JdController.SetupBodyPart(左手臂旋转);
m_JdController.SetupBodyPart(forearmR);
m_JdController.SetupBodyPart(handR);

}


来自 JointDriveController：
 public void SetupBodyPart(Transform t)
{

var bp = new BodyPart
{
rb = t.GetComponent&lt;Rigidbody&gt;(),
joint = t.GetComponent&lt;CharacterJoint&gt;(),
StartingPos = t.position,
StartingRot = t.rotation
};
bp.rb.maxAngularVelocity = k_MaxAngularVelocity;

// 添加并设置地面接触脚本
bp.objectContact = t.GetComponent&lt;ObjectContact&gt;();

var agent = gameObject.GetComponent&lt;Agent&gt;();
如果 (agent == null)
{
agent = gameObject.AddComponent&lt;Agent&gt;();
}
bp.objectContact.agent = agent;

如果 (!bp.objectContact)
{
bp.objectContact = t.gameObject.AddComponent&lt;ObjectContact&gt;();
bp.objectContact.agent = gameObject.GetComponent&lt;Agent&gt;();
}
else
{
bp.objectContact.agent = gameObject.GetComponent&lt;Agent&gt;();
}

如果 (bp.objectContact.agent == null)
{
Debug.LogError($&quot;未找到 {t.name} 的代理&quot;);
}

bp.thisJdController = this;
bodyPartsDict.Add(t, bp);
bodyPartsList.Add(bp);
}

来自 ObjectContact：
void OnCollisionEnter(Collision col)
{
if (agent == null)
{
Debug.LogError($&quot;Agent is null on {gameObject.name} during OnCollisionEnter with {col.gameObject.name}&quot;);
return;
}

if (col.transform.CompareTag(k_Ground))
{
touchingGround = true;
}

if (col.transform.CompareTag(k_Wall))
{
touchingWall = true;
}

if (col.transform.CompareTag(k_Target))
{
touchingTarget = true;
agent.AddReward(targetReward);
}
]]></description>
      <guid>https://stackoverflow.com/questions/78863425/ml-agents-agent-is-null-when-any-part-of-my-game-object-collides</guid>
      <pubDate>Mon, 12 Aug 2024 20:07:45 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 Torch 使用 4 个 GPU 进行训练：torch.distributed.elastic.multiprocessing.api</title>
      <link>https://stackoverflow.com/questions/78863216/unable-to-train-with-4-gpus-using-torch-torch-distributed-elastic-multiprocessi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78863216/unable-to-train-with-4-gpus-using-torch-torch-distributed-elastic-multiprocessi</guid>
      <pubDate>Mon, 12 Aug 2024 19:01:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Keras/Tensorflow 中实现“生成器历史” GAN</title>
      <link>https://stackoverflow.com/questions/78863055/how-to-implement-generator-history-gan-in-keras-tensorflow</link>
      <description><![CDATA[生成对抗网络 (GAN) 是一种非常有趣的生成模型，但训练起来却很困难。 本文提供了一种提高稳定性的方法：在第 2.3 节中，他们指出

对抗训练的一个问题是鉴别器网络仅关注最新的精炼图像。

并且

[…] 我们引入了一种通过使用精炼图像的历史记录而不是仅使用当前小批量中的图像来更新鉴别器来提高稳定性的方法。

这只是意味着我们必须保留生成器生成的图像的历史记录（例如最后 50 张图像）并随机将它们显示给鉴别器。
这种方法也用于CycleGAN。
我的问题是
如何通过覆盖 Keras 模型类来实现使用图像历史更新鉴别器的方法？事实上，我想通过这种方法扩展 keras CycleGAN 实现（在此处找到）。
我在本教程中找到了一个使用
train_on_batch

的实现。但是，我无法使用 Keras 教程中描述的
model.train

方法成功完成相同的操作。]]></description>
      <guid>https://stackoverflow.com/questions/78863055/how-to-implement-generator-history-gan-in-keras-tensorflow</guid>
      <pubDate>Mon, 12 Aug 2024 18:06:25 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的 Sums 包无法进行标记</title>
      <link>https://stackoverflow.com/questions/78862511/sums-package-failing-to-tokenize-in-python</link>
      <description><![CDATA[我使用以下代码在 Python 中总结我的文本。代码正在 Jupyter Notebook 中运行。我已经使用 pip 命令安装了 sumy。
pip install sumy nltk
python -m nltk.downloader punkt

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from io import StringIO

# 定义要总结的文本
text = &quot;&quot;&quot;
自然语言处理 (NLP) 是人工智能的一个领域，专注于通过自然语言实现计算机与人类之间的互动。NLP 的最终目标是使计算机能够以有价值和有意义的方式理解、解释和响应人类语言。
NLP 用于应用算法来识别和提取自然语言规则，从而将非结构化语言数据转换为计算机可以理解的形式。当提供文本时，计算机可以采用多种不同的方法来处理它。算法可以是基于规则的方法，也可以是基于机器学习的方法。
“”“”

# 使用 StringIO 模拟文件类对象
text_io = StringIO(text)

# 解析文本
parser = PlaintextParser.from_file(text_io, Tokenizer(&quot;english&quot;))

# 初始化 LSA 摘要器
summarizer = LsaSummarizer()

# 生成摘要（您可以调整句子数量）
summary = summaryr(parser.document, sentences_count=2)

# 打印摘要
for sentence in summary:
print(sentence) 

当我运行程序时，我收到以下错误：
ame)
662 def find_class(self, module, name):
663 # 禁止每个函数
--&gt; 664 引发 pickle.UnpicklingError(f&quot;全局 &#39;{module}.{name}&#39; 被禁止&quot;)

UnpicklingError: 全局 &#39;copy_reg._reconstructor&#39; 被禁止 

有什么想法！]]></description>
      <guid>https://stackoverflow.com/questions/78862511/sums-package-failing-to-tokenize-in-python</guid>
      <pubDate>Mon, 12 Aug 2024 15:37:15 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 nltk 函数</title>
      <link>https://stackoverflow.com/questions/78862426/unable-to-use-nltk-functions</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78862426/unable-to-use-nltk-functions</guid>
      <pubDate>Mon, 12 Aug 2024 15:17:29 GMT</pubDate>
    </item>
    <item>
      <title>访问 Azure 机器学习中计划的创建日期</title>
      <link>https://stackoverflow.com/questions/78862179/acessing-the-creation-date-of-a-schedule-in-azure-machine-learning</link>
      <description><![CDATA[我试图访问 AML 中计划的创建/定义日期。我尝试了文档和一些解决方法，但到目前为止还没有成功。如果可以访问端点或已发布管道的创建日期，那么它也会解决我的问题。有人知道我该怎么做吗？提前谢谢。
我已经尝试访问上述每个类的属性，但无法获取有关创建时间的信息。我唯一访问过的日期是与计划对象关联的运行日期。]]></description>
      <guid>https://stackoverflow.com/questions/78862179/acessing-the-creation-date-of-a-schedule-in-azure-machine-learning</guid>
      <pubDate>Mon, 12 Aug 2024 14:23:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 ImageDataGenerator 训练 CNN，第二次训练后失败</title>
      <link>https://stackoverflow.com/questions/78861705/training-a-cnn-using-imagedatagenerator-and-training-fails-after-the-2nd-epoch</link>
      <description><![CDATA[我使用 ImageDataGenerator 训练 CNN 时遇到了这个问题，在第二个 Epoch 之后出现了属性错误。
模型如下
模型
import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop

def create_model():
&#39;&#39;&#39;创建一个具有 4 个卷积层的 CNN&#39;&#39;&#39;
model = tf.keras.models.Sequential([
tf.keras.layers.Conv2D(32, (3,3),activation=&#39;relu&#39;, input_shape=(150, 150, 3)),
tf.keras.layers.MaxPooling2D(2, 2),
tf.keras.layers.Conv2D(64, (3,3),activation=&#39;relu&#39;),
tf.keras.layers.MaxPooling2D(2,2),
tf.keras.layers.Conv2D(128, (3,3), 激活=&#39;relu&#39;),
tf.keras.layers.MaxPooling2D(2,2),
tf.keras.layers.Conv2D(128, (3,3), 激活=&#39;relu&#39;),
tf.keras.layers.MaxPooling2D(2,2),
tf.keras.layers.Flatten(),
tf.keras.layers.Dense(512, 激活=&#39;relu&#39;),
tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
])

model.compile(loss=&#39;binary_crossentropy&#39;,
optimizer=RMSprop(learning_rate=1e-4),
metrics=[&#39;accuracy&#39;])

返回模型

来自 tensorflow.keras.preprocessing.image 导入 ImageDataGenerator

train_datagen = ImageDataGenerator(rescale=1./255)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
train_dir, # 这是训练图像的源目录
target_size=(150, 150), # 所有图像将调整为 150x150
batch_size=20,
# 由于我们使用 binary_crossentropy 损失，我们需要二进制标签
class_mode=&#39;binary&#39;)

validation_generator = test_datagen.flow_from_directory(
validation_dir,
target_size=(150, 150),
batch_size=20,
class_mode=&#39;binary&#39;,
shuffle= False)

EPOCHS = 20

model = create_model()

history = model.fit(
train_generator,
steps_per_epoch=100, # 2000 images = batch_size * steps
epochs=EPOCHS,
validation_data=validation_generator,
validation_steps=50, # 1000 images = batch_size * steps
verbose=2)

输出
AttributeError Traceback (最近一次调用最后一次)
Cell In[15]，第 8 行
5 model = create_model()
7 # 训练模型
----&gt; 8 history = model.fit(
9 train_generator,
10 steps_per_epoch=100, # 2000 图像 = batch_size * 步骤
11 epochs=EPOCHS,
12 validation_data=validation_generator,
13 validation_steps=50, # 1000 图像 = batch_size * 步骤
14 verbose=2)

文件 ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
-&gt; 122 从 None 中引发 e.with_traceback(filtered_tb)
123 最后：
124 delfiltered_tb

文件 ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py:354，位于 TensorFlowTrainer.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)
333 self._eval_epoch_iterator = TFEpochIterator(
334 x=val_x,
335 y=val_y,
...
355 }
356 epoch_logs.update(val_logs)
358 callbacks.on_epoch_end(epoch, epoch_logs)

AttributeError: &#39;NoneType&#39; 对象没有属性 &#39;items&#39;

输出被截断。以可滚动元素形式查看或在文本编辑器中打开。调整单元格输出设置...

我尝试了以下调试步骤：

升级 Tensorflow 和 Keras
尝试更简单的神经网络，看看是否有相同的问题，但它运行良好。
没有将 validation_generator 传递给 model.fit()，而是使用 numpy 手动执行，但这也没有奏效，因为对于它来说，训练数据的准确性和错误在偶数个时期都为 0仅。

还检查了验证数据是否已正确加载。
Python 版本：3.11.9
Tensorflow 版本：2.17.0
Keras 版本：3.4.1]]></description>
      <guid>https://stackoverflow.com/questions/78861705/training-a-cnn-using-imagedatagenerator-and-training-fails-after-the-2nd-epoch</guid>
      <pubDate>Mon, 12 Aug 2024 12:33:51 GMT</pubDate>
    </item>
    <item>
      <title>使用相关的 Sagemaker HP 调整作业作为热启动父作业</title>
      <link>https://stackoverflow.com/questions/78861542/use-relevant-sagemaker-hp-tuning-jobs-as-warm-start-parent-jobs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78861542/use-relevant-sagemaker-hp-tuning-jobs-as-warm-start-parent-jobs</guid>
      <pubDate>Mon, 12 Aug 2024 11:58:24 GMT</pubDate>
    </item>
    <item>
      <title>我如何知道镜头中的某人是否对齐了轮廓？</title>
      <link>https://stackoverflow.com/questions/78860543/how-can-i-know-if-someone-in-camera-aligns-an-outline</link>
      <description><![CDATA[我正在做一个项目，这个项目与实时摄像头的身体测量有很大关系，但为了校准，我需要人站在屏幕中间，直接适应框架尺寸。为此，我使用了一个透明的人形，除了人形的边界外，其他地方都是透明的。我如何检查人是否符合轮廓？
我尝试在屏幕中间使用 png 并从中绘制轮廓并检查地标，但它总是最终检查一个绘制原始图像边界的正方形。而且，即使一个人站得很近，只用一半的身体来适应它，它也能正常工作。
效果很差。我该如何改进？
现在我的代码片段如下所示：
KadirCoordinates = {
&#39;NOSE&#39;: (320, 36),
&#39;LEFT_SHOULDER&#39;: (274, 100),
&#39;RIGHT_SHOULDER&#39;: (372, 104),
&#39;LEFT_HEEL&#39;: (296, 459),
&#39;RIGHT_HEEL&#39;: (343, 459)
}

points_to_collect = [&#39;NOSE&#39;, &#39;LEFT_SHOULDER&#39;, &#39;RIGHT_SHOULDER&#39;, &#39;LEFT_HEEL&#39;, &#39;RIGHT_HEEL&#39;] #臀部和脚踝处有血迹
current_point_index = 0

if result.pose_landmarks:
landmarks = result.pose_landmarks.landmark

detected_points = {}

for landmark in mp_pose.PoseLandmark:
x = int(landmarks[landmark].x * frame_width)
y = int(landmarks[landmark].y * frame_height)
if landmark.name in points_to_collect:
detected_points[landmark.name] = (x, y)
cv2.circle(blended_frame, (x, y), 5, (0, 255, 0), -1)
alignment_score = 0
threshold = 85 # 降低对齐公差阈值

for key in reference_coordinates:
outline_point = reference_coordinates[key]
detected_point =detected_points.get(key, None)
ifdetected_point:
distance = np.linalg.norm(np.array(outline_point) - np.array(detected_point))
#print(f&quot;Distance for {key}: {distance}&quot;) # 调试信息
if distance &lt; Threshold:
alignment_score += 1 # 根据需要调整阈值

# 计算对齐百分比
alignment_percentage = (alignment_score / len(reference_coordinates)) * 100
cv2.putText(blended_frame, &quot;Alignment score is : {}&quot;.format(alignment_percentage), (50, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)

# 检查对齐，如果对齐则启动计时器
if alignment_percentage &gt;95:
]]></description>
      <guid>https://stackoverflow.com/questions/78860543/how-can-i-know-if-someone-in-camera-aligns-an-outline</guid>
      <pubDate>Mon, 12 Aug 2024 07:55:19 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试为我的深度学习模型创建一个 FastAPI，但它似乎无法复制我在 Colab 中测试时得到的结果</title>
      <link>https://stackoverflow.com/questions/78860470/i-am-trying-to-make-a-fastapi-for-my-deep-learning-model-but-it-does-not-seem-t</link>
      <description><![CDATA[我的模型完全是在 Google Colab 上制作的，我使用 Tensorflow 和 Keras 制作了它。我尝试复制我在 YouTube 上看到的一个教程模型，后来我想为它制作一个网站。因此，我选择了 FastAPI 路线。但它似乎无法复制结果，在 Colab 上，它在验证数据集和测试数据集上的表现都很完美，准确率约为 96%。但是当我为其创建 FastAPI 时，无论我输入什么图像，它始终只显示 1 个特定类作为输出，并且该类的置信度为 96-98%。我以 .keras 格式下载了我的模型，因为 Colab 不支持 .h5 文件下载。这是在 Postman 中，API 似乎运行良好，模型本身在 Google Colab 上运行良好，但当我将其连接到 API 时不起作用。
有趣的是，当我在再次训练后以不同的名称第二次下载模型时，置信度似乎再次发生变化，并且现在始终保持这种分布。但是，是的，结果仍然相同，预测类 0 作为输出，只是这次置信度发生了变化（与之前的版本不同，在提供不同的图像后通常不会改变）。我还尝试在单独的文件中进行调试，在本地仅使用 1 张图像进行预测，但仍然失败。它出现了与以前相同的模式。我再次上​​网（Google）Colab 训练和测试模型，看起来效果不错。
下面我提供了我的 API 代码：
from fastapi import FastAPI, UploadFile, File
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import img_to_array
from matplotlib import pyplot as plt
from PIL import Image
import numpy as np

app = FastAPI()

# 加载模型
try:
model = load_model(&#39;PotatoDisease.keras&#39;)
except ValueError as e:
print(f&quot;Error loading model: {e}&quot;)

class_names = [&#39;Potato___Early_blight&#39;, &#39;Potato___Late_blight&#39;, &#39;Potato___healthy&#39;]

@app.post(&quot;/predict/&quot;)
async def predict(file: UploadFile = File(...)):
image = Image.open(file.file)
image = image.resize((256, 256))
plt.imshow(image)
image = img_to_array(image)
print(&quot;调整大小和数组转换后的图像形状：&quot;, image.shape)
image = np.expand_dims(image, axis=0)
print(&quot;扩展尺寸后的图像形状：&quot;, image.shape)
image = image / 255.0

#prediction
predictions = model.predict(image)
print(&quot;原始预测输出：&quot;, predictions)
predict_class = class_names[np.argmax(predictions[0])]
confidence = round(100 * np.max(predictions[0]), 2)

return {&quot;predicted_class&quot;: predict_class, &quot;confidence&quot;: confidence}

@app.get(&quot;/&quot;)
def read_root():
return {&quot;message&quot;: &quot;欢迎使用图像分类 API&quot;}
`
]]></description>
      <guid>https://stackoverflow.com/questions/78860470/i-am-trying-to-make-a-fastapi-for-my-deep-learning-model-but-it-does-not-seem-t</guid>
      <pubDate>Mon, 12 Aug 2024 07:38:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 ResNet50 创建 [w, h, f] 的特征张量</title>
      <link>https://stackoverflow.com/questions/78860233/using-resnet50-to-create-a-feature-tensor-of-w-h-f</link>
      <description><![CDATA[我正在尝试实现这篇论文，但我没有理解其中的内容。
它希望我使用 ResNet50 从图像中提取特征，但告诉我提取的特征将具有 [w, h, f] 的维度。但是，我用 ResNet50 看到的一切都给我返回了一个 [f] 的张量（即，它将我的整个图像变成特征，而不是将我的像素变成特征）
我读错了吗，还是我只是不明白我应该用 ResNet50 做什么？
论文中的相关引述：
“我们获得了一个大小为 f 的中间视觉特征表示 Fc。我们使用 ResNet50 [26] 作为我们的主干卷积架构。&quot;
&quot;第一步，将三维特征 Fc 通过保持其宽度重塑为二维特征，即获得特征形状 (f × h, w)。&quot;]]></description>
      <guid>https://stackoverflow.com/questions/78860233/using-resnet50-to-create-a-feature-tensor-of-w-h-f</guid>
      <pubDate>Mon, 12 Aug 2024 06:27:03 GMT</pubDate>
    </item>
    <item>
      <title>如何在 HuggingFace 中从头开始重新初始化 GPT XL？</title>
      <link>https://stackoverflow.com/questions/78859343/how-to-reinitialize-from-scratch-gpt-xl-in-huggingface</link>
      <description><![CDATA[我试图确认我的 GPT-2 模型是从头开始训练的，而不是使用任何预先存在的预训练权重。这是我的方法：

加载预训练的 GPT-2 XL 模型：我使用 AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;) 加载预训练的 GPT-2 XL 模型，并计算此模型权重的总 L2 范数。
从头开始初始化新的 GPT-2 模型：然后我使用 GPT2Config 从头开始​​使用自定义配置初始化新的 GPT-2 模型。
比较 L2 范数：我计算预训练模型和新初始化模型的权重的 L2 范数。我的假设是，如果临时模型确实是从随机权重初始化的，那么临时模型的 L2 范数应该比预训练模型小得多。

这是代码片段：
import torch
from transformers import GPT2LMHeadModel, GPT2Config, AutoModelForCausalLM

# 步骤 1：加载预训练的 GPT-2 XL 模型
pretrained_model = AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;)

# 步骤 2：计算预训练模型权重的 L2 范数
pretrained_weight_norm = 0.0
for param in pretrained_model.parameters():
pretrained_weight_norm += torch.norm(param, p=2).item()

print(f&quot;Total L2预训练模型权重的范数：{pretrained_weight_norm:.2f}&quot;)

# 步骤 3：使用自定义配置从头开始初始化新的 GPT-2 模型
config = GPT2Config(
vocab_size=52000, # 确保这与 tokenizer 的词汇量相匹配
n_ctx=1024, # 上下文窗口大小（模型一次可以看到的 token 数量）
bos_token_id=0, # 序列开始 token
eos_token_id=1, # 序列结束 token
)
model = GPT2LMHeadModel(config)

# 步骤 4：计算刚初始化的模型权重的 L2 范数
scratch_weight_norm = 0.0
for param in model.parameters():
scratch_weight_norm += torch.norm(param, p=2).item()

print(f&quot;从头开始初始化的模型的总 L2 范数：{scratch_weight_norm:.2f}&quot;)

这种方法是否是确认模型是从头开始训练的有效方法？是否存在任何潜在问题或更好的方法来验证模型没有预先存在的学习权重？
看起来正确
~/beyond-scale-language-data-diversity$ /opt/conda/envs/beyond_scale_div_coeff/bin/python /home/ubuntu/beyond-scale-language-data-diversity/playground/test_gpt2_pt_vs_reinit_scratch.py​​
config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 689/689 [00:00&lt;00:00，8.05MB/s]
model.safetensors：100%|██████████████████████████████████████████████████████████████████████████| 6.43G/6.43G [00:29&lt;00:00，221MB/s]
generation_config.json：100%|██████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00&lt;00:00，1.03MB/s]
预训练模型权重的总 L2 范数：24542.74
从头初始化模型的总 L2 范数：1637.31
（beyond_scale_div_coeff）

cross: https://discuss.huggingface.co/t/how-to-reinitialize-from-scratch-gpt-xl-in-hugging-face-hf/101905
ref: https://github.com/alycialee/beyond-scale-language-data-diversity/issues/18]]></description>
      <guid>https://stackoverflow.com/questions/78859343/how-to-reinitialize-from-scratch-gpt-xl-in-huggingface</guid>
      <pubDate>Sun, 11 Aug 2024 20:27:07 GMT</pubDate>
    </item>
    <item>
      <title>获取 ValueError：所有数组的长度必须相同</title>
      <link>https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length</link>
      <description><![CDATA[我一直试图将字典转换为数据框，但每次我都收到 ValueError：所有数组的长度必须相同。我已经检查了每个数组的长度并确认它们相同，但我仍然收到相同的错误
def metrics_from_pipes(pipes_dict):
for name, pipeline in pipes_dict.items():

pipeline.fit(X_train, y_train)
y_pred_val = pipeline.predict(X_val)
y_pred_train = pipeline.predict(X_train)

train_metrics = {
&#39;model&#39;:list(pipes_dict.keys()),
&#39;MAE&#39;:train_mae,
&#39;MAPE&#39;:train_mape,
&#39;RMSE&#39;:train_rmse,
&#39;RSquared&#39;:train_rsquared
}

train_metrics_data = pd.DataFrame(train_metrics)
val_metrics = {
&#39;model&#39;:list(pipes_dict.keys()),
&#39;MAE&#39;:val_mae,
&#39;MAPE&#39;:val_mape,
&#39;RMSE&#39;:val_rmse,
&#39;RSquared&#39;:val_rsquared 
}

val_metrics_data = pd.DataFrame(val_metrics,)

# 合并来自训练集和测试集的指标
train_val_metrics = train_metrics_data.merge(val_metrics_data,
on = &#39;Model&#39;,
how = &#39;left&#39;,
suffixes = (&#39;_train&#39;, &#39;_val&#39;))

# 排序列 
train_val_metrics = train_val_metrics.reindex(columns = [&#39;Model&#39;,
&#39;MAE_train&#39;,
&#39;MAPE_train&#39;,
&#39;RMSE_train&#39;,
&#39;RSquared_train&#39;,
&#39;MAE_val&#39;,
&#39;MAPE_val&#39;,
&#39;RMSE_val&#39;,
&#39;RSquared_val&#39;])

return train_val_metrics.set_index(&#39;Model&#39;).transpose()

# 获取指标表
metrics_table = metrics_from_pipes(pipelines)

运行此代码会出现此错误
ValueError Traceback (most recent call last)
Cell In[45]，第 82 行
80 return train_val_metrics.set_index(&#39;Model&#39;).transpose()
81 # 获取指标表
---&gt; 82 metrics_table = metrics_from_pipes(pipelines)
83 #print(&#39;表 1：基本模型指标&#39;)
84 #metrics_table.style.background_gradient(cmap = Blues)
85 metrics_table

单元格 In[45]，第 50 行，位于 metrics_from_pipes(pipes_dict)
41 # 将性能指标列表聚合到单独的数据框中
42 train_metrics = {
43 &#39;model&#39;:list(pipes_dict.keys()),
44 &#39;MAE&#39;:train_mae,
(...)
47 &#39;RSquared&#39;:train_rsquared
48 }
---&gt; 50 train_metrics_data = pd.DataFrame(train_metrics)
51 val_metrics = {
52 &#39;model&#39;:list(pipes_dict.keys()),
53 &#39;MAE&#39;:val_mae,
(...)
56 &#39;RSquared&#39;:val_rsquared 
57 }
59 val_metrics_data = pd.DataFrame(val_metrics,)

ValueError: 所有数组的长度必须相同

当我检查 train_metrics 和 val 指标的字典结果时，我得到了这个
({&#39;model&#39;: [&#39;Linear Regression&#39;,
&#39;Random Forest Regressor&#39;,
&#39;Gradient Boost Regression&#39;,
&#39;Extra Tree Regressor&#39;],
&#39;MAE&#39;: [829.1023412412194,
288.33455697065233,
712.9637267872279,
0.0010629575741748962],
&#39;MAPE&#39;: [1.0302372135902111,
0.20937541440883897,
0.538244903316323,
6.306697580961048e-07],
&#39;RMSE&#39;: [1120.5542708017374,
416.48933196590013,
1012.399201767692,
0.05804079289490426],
&#39;RSquared&#39;: [0.5598288286601083,
0.9391916010838417,
0.6406981997919169,
0.9999999988190745]},
{&#39;model&#39;: [&#39;线性回归&#39;,
&#39;随机森林回归器&#39;,
&#39;梯度提升回归&#39;,
&#39;额外树回归器&#39;],
&#39;MAE&#39;: [855.9254413559535,
802.5902302175274,
772.3140648475379,
839.9018341377154],
&#39;MAPE&#39;: [1.0395487579496652,
0.5607987708065988,
0.5438627253681279,
0.5852285872937784],
&#39;RMSE&#39;: [1148.6549900167981,
1158.8411708570625,
1109.6145558003204,
1223.23337689915],
&#39;RSquared&#39;: [0.5876710102285392,
0.5803255834810521,
0.6152231339508221,
0.5323905190373128]})
]]></description>
      <guid>https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length</guid>
      <pubDate>Sun, 11 Aug 2024 12:27:40 GMT</pubDate>
    </item>
    <item>
      <title>神经网络训练过程中出现 NAN 的常见原因</title>
      <link>https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training-of-neural-networks</link>
      <description><![CDATA[我注意到在训练过程中经常出现 NAN 的情况。
很多时候，它似乎是由内积/全连接或卷积层中的权重爆炸引起的。
这是因为梯度计算爆炸而发生的吗？还是因为权重初始化（如果是这样，为什么权重初始化会产生这种影响）？还是很可能是由输入数据的性质引起的？
这里的首要问题很简单：训练期间出现 NAN 的最常见原因是什么？其次，有哪些方法可以解决这个问题（以及它们为什么有效）？]]></description>
      <guid>https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training-of-neural-networks</guid>
      <pubDate>Fri, 27 Nov 2015 17:23:30 GMT</pubDate>
    </item>
    </channel>
</rss>