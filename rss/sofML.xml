<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 28 Oct 2024 15:19:00 GMT</lastBuildDate>
    <item>
      <title>当没有部分拟合选项时，如何训练分区数据集？</title>
      <link>https://stackoverflow.com/questions/79133844/how-do-i-train-a-partitioned-dataset-when-there-is-not-an-option-for-partial-fit</link>
      <description><![CDATA[我正在从包含 10 个分区的数据集训练 ML 模型，这样我就不会耗尽可用内存。我目前正在每个分区上训练 3 个不同的模型，然后将它们放入 VotingRegressor 中，然后再次进行拟合，但是由于它占用了大量内存，我无法将其拟合到整个训练集。这里有一个小片段
all_feature_cols = [f&quot;feature_{i:02d}&quot; for i in range(79)]

if TRAINING:
# 初始化列表以存储模型
lgbm_models = []
xgb_models = []
cat_models = []

# 逐步训练每个模型
for partion in range(10):
start_time = time.time()

# 为每个分区创建新的模型实例
lgbm = LGBMRegressor(num_leaves=127, n_estimators=200, max_depth=3, 
learning_rate=0.05, device_type=&#39;gpu&#39;, verbose=-1,
reg_alpha = 0.1, reg_lambda = 0.1)
xgb = XGBRegressor(n_estimators=200, min_child_weight=5, max_depth=7, 
learning_rate=0.01, device=&#39;cpu&#39;,
reg_alpha = 0.1, reg_lambda = 0.1)
cat = CatBoostRegressor(n_estimators=200, max_depth=7, learning_rate=0.05, 
reg_lambda = 0.1, task_type=&#39;GPU&#39;, verbose=False)

# 过滤当前分区并分别收集目标
partition_df = df.filter(pl.col(&quot;partition_id&quot;) ==partition)

# 在预处理之前提取并收集目标列
y =partition_df.select(&quot;responder_6&quot;).collect().to_numpy().ravel()

# 预处理特征（不包括目标）
X =partition_df.select(all_feature_cols).collect().to_numpy()

# 分成训练/验证并保持时间顺序
train_idx = int(len(X) * 0.8)
X_train, X_test = X[:train_idx], X[train_idx:]
y_train, y_test = y[:train_idx], y[train_idx:]

# 在此分区上训练模型
lgbm.fit(X_train, y_train)
xgb.fit(X_train, y_train)
cat.fit(X_train, y_train)

# 使用分区标识符保存训练好的模型
lgbm_models.append((f&#39;lgbm_{partition}&#39;, deepcopy(lgbm)))
xgb_models.append((f&#39;xgb_{partition}&#39;, deepcopy(xgb)))
cat_models.append((f&#39;cat_{partition}&#39;, deepcopy(cat)))

# 计算已用时间
end_time = time.time()
elapsed_time = end_time - start_time
elapsed_str = str(timedelta(seconds=int(elapsed_time)))
print(f&quot;分区 {partition} 在 {elapsed_str} 内完成&quot;)

# 清理内存
if partition &lt; 9:
del X, y, X_train, X_test, y_train, y_test, partition_df
else:
del X, y, X_train, y_train, partition_df
gc.collect()

# 创建并拟合 VotingRegressor 和所有经过训练的模型
model = VotingRegressor(lgbm_models + xgb_models + cat_models)

# 拟合模型
model.fit(X_test, y_test)

y_pred = model.predict(X_test)
print(r2_score(y_test, y_pred))

# 保存最终模型 
dump(model, &quot;/kaggle/working/JS_model.joblib&quot;)
```
]]></description>
      <guid>https://stackoverflow.com/questions/79133844/how-do-i-train-a-partitioned-dataset-when-there-is-not-an-option-for-partial-fit</guid>
      <pubDate>Mon, 28 Oct 2024 14:30:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在 IBM Watson Assistant 响应中动态包含来自 Watson Discovery 的完整且正确的 URL？[关闭]</title>
      <link>https://stackoverflow.com/questions/79133652/how-to-dynamically-include-full-and-correct-urls-from-watson-discovery-in-ibm-wa</link>
      <description><![CDATA[我正在使用 IBM Watson Assistant，使用 Llama 3.8 作为语言模型，我面临的一个问题是，模型始终无法在其响应中检索正确的 URL。我从 Watson Discovery 中的文档中动态提取这些 URL，并且每个响应都需要根据提出的问题包含不同的特定链接。尽管我的提示明确指示模型包含一个特定的完整链接，但模型的响应始终包含不正确或不完整的链接。
是否有人遇到过与 IBM Watson Assistant 或其他 LLM 类似的问题，即模型无法检索确切的指定 URL，尤其是在不同响应中需要不同的链接时？是否有任何已知的解决方法、配置或提示调整可以确保模型可靠地从 Watson Discovery 中检索并显示正确的链接？
以下是我尝试过的概述：
提示调整：我在提示中包含了明确的指示“包含整个链接而不缩短它”，我甚至尝试将链接直接放在提示中作为示例。但是，模型要么生成不完整的链接，要么生成完全错误的链接。
提示示例：
下面是我的提示的简化版本，它指示模型将 {DOCUMENTATION_LINK} 替换为与每个问题相关的文档的实际链接：
&quot;您是客户关系经理。您的目的是提供 CRM 流程的简要摘要。如果用户需要详细步骤，请回复：&#39;有关完整流程和所有详细步骤，请点击此链接：{DOCUMENTATION_LINK}&#39;。始终将“{DOCUMENTATION_LINK}”替换为问题中文档的实际链接。&quot;
配置详细信息：
温度：0.5
最大新令牌：900
停止序列：[&quot; &quot;]
重复惩罚：1]]></description>
      <guid>https://stackoverflow.com/questions/79133652/how-to-dynamically-include-full-and-correct-urls-from-watson-discovery-in-ibm-wa</guid>
      <pubDate>Mon, 28 Oct 2024 13:41:29 GMT</pubDate>
    </item>
    <item>
      <title>将注意力机制增强至 O(log N)：一种基于树的 Transformer 模型优化方法</title>
      <link>https://stackoverflow.com/questions/79133220/enhancing-attention-mechanism-to-olog-n-a-tree-based-approach-for-optimizing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79133220/enhancing-attention-mechanism-to-olog-n-a-tree-based-approach-for-optimizing</guid>
      <pubDate>Mon, 28 Oct 2024 11:42:20 GMT</pubDate>
    </item>
    <item>
      <title>如何提高 CNN 模型的准确率并减少损失？</title>
      <link>https://stackoverflow.com/questions/79133215/how-to-increase-accuracy-and-decrease-loss-in-cnn-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79133215/how-to-increase-accuracy-and-decrease-loss-in-cnn-model</guid>
      <pubDate>Mon, 28 Oct 2024 11:41:21 GMT</pubDate>
    </item>
    <item>
      <title>使用没有跳跃连接的 U-Net 进行图像到图像处理。这是真的吗？</title>
      <link>https://stackoverflow.com/questions/79132335/image-to-image-with-u-net-with-no-skip-connections-is-it-real</link>
      <description><![CDATA[如果我想从其他图像（图像到图像）获取一些图像，我可以使用没有跳过连接的 U-Net 吗？因为我不需要保留结构。例如，为了改变某些对象的相机视角。
例如，此模型为 256x256px。它适用于 3 对训练对（输入图像-输出图像），并带有角度增强（+/- 5 度、+/-10 度、+/-15 度），但不适用于 1.000 对。
# 编码器
c = layer.Conv2D(32, kernel_size=4, strides=2, padding=&quot;same&quot;)(inputs)
c = layer.LeakyReLU(negative_slope=0.2)(c) 

c = layer.Conv2D(64, kernel_size=4, strides=2, padding=&quot;same&quot;)(c)
c = layer.LeakyReLU(negative_slope=0.2)(c)

c = layer.Conv2D(128, kernel_size=4, strides=2, padding=&quot;same&quot;)(c)
c =层。LeakyReLU（负斜率=0.2）（c）

c = 层。Conv2D（256，kernel_size=4，strides=2，padding=“相同”（c）

c = 层。LeakyReLU（负斜率=0.2）（c）

c = 层。Conv2D（512，kernel_size=4，strides=2，padding=“相同”（c）

c = 层。LeakyReLU（负斜率=0.2）（c）

c = 层。Conv2D（1024，kernel_size=4，strides=2，padding=“相同”（c）

c = 层。LeakyReLU（负斜率=0.2）（c）

# 瓶颈
b = InstanceNormalization（）（c）

b =图层。重塑((-1,))(b)
b = 图层。密集(512*4*4, kernel_regularizer=l2_reg,)(b)
b = 图层。LeakyReLU(negative_slope=0.2)(b)

b = 图层。Dropout(0.2)(b)

b = 图层。密集(512*4*4*2, kernel_regularizer=l2_reg,)(b)
b = 图层。LeakyReLU(negative_slope=0.2)(b)
b = 图层。重塑((4,4,1024))(b)

# 解码器
d = 图层。UpSampling2D(size=(2, 2))(b)
d = 图层。Conv2D(1024, kernel_size=4, padding=&quot;same&quot;)(d)
d =层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2, 2））（d）

d = 层。Conv2D（512，内核大小=4，填充=“相同”）（d）

d = 层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2, 2））（d）

d = 层。Conv2D（256，内核大小=4，填充=“相同”）（d）

d = 层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2, 2））（d）

d = 层。Conv2D（128，内核大小=4，填充=“相同”）（d）

d =层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2，2））（d）

d = 层。Conv2D（64，kernel_size=4，padding=“相同”（d）

d = 层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2，2））（d）

d = 层。Conv2D（32，kernel_size=4，padding=“相同”（d）

d = 层。LeakyReLU（负斜率=0.2）（d）`

# 输出
输出 = 层。Conv2D（3，kernel_size=3，padding=“相同”，激活=“tanh”（d）

模型 = models.Model(inputs=inputs, output=outputs, name=&quot;build_unet&quot;)
return model`

无论输入图像是否旋转或扭曲，我都需要获取输出。]]></description>
      <guid>https://stackoverflow.com/questions/79132335/image-to-image-with-u-net-with-no-skip-connections-is-it-real</guid>
      <pubDate>Mon, 28 Oct 2024 07:00:00 GMT</pubDate>
    </item>
    <item>
      <title>删除所有人口后，NEAT 给出错误</title>
      <link>https://stackoverflow.com/questions/79130999/neat-giving-error-after-deleting-all-the-population</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79130999/neat-giving-error-after-deleting-all-the-population</guid>
      <pubDate>Sun, 27 Oct 2024 16:27:23 GMT</pubDate>
    </item>
    <item>
      <title>输入图像与 TensorFlow 模型输入形状不兼容</title>
      <link>https://stackoverflow.com/questions/79130521/input-image-is-not-compatible-with-tensorflow-model-input-shape</link>
      <description><![CDATA[我正在构建一个模型，我想测试它的性能，因此我导入了一个本地文件并加载它，并尝试使用以下代码预测它的标签：
from tensorflow.preprocessing import image
# tensorlfow 等的其他导入。

#...

# 示例图像
img_path = &quot;./Model/data/brain/train/Glioma/images/gg (2).jpg&quot;
img = image.load_img(img_path,target_size=(256,256))
arr = image.img_to_array(img)
t_img = tf.convert_to_tensor(arr)
print(t_img.shape) # 返回 (256,256,3)
# 客户端测试
client = Client(&quot;brain&quot;) # 自定义类。包含模型：顺序（已编译和训练）
client.predict(img=t_img) # 调用 self.model.predict(t_img)

但是我收到以下错误：
输入 Tensor(&quot;data:0&quot;, shape=(32, 256, 3), dtype=float32) 的输入形状无效。预期形状 (None, 256, 256, 3)，但输入具有不兼容的形状 (32, 256, 3)

我在训练模型中有一个输入层，其 input_shape=[256,256,3]（来自图像宽度、高度和 rgb 值）
您能帮助我理解问题并解决它吗？]]></description>
      <guid>https://stackoverflow.com/questions/79130521/input-image-is-not-compatible-with-tensorflow-model-input-shape</guid>
      <pubDate>Sun, 27 Oct 2024 11:57:01 GMT</pubDate>
    </item>
    <item>
      <title>如何将 AWS Bedrock 与我的数据库集成以实现基于向量的 LLM 响应上下文检索？</title>
      <link>https://stackoverflow.com/questions/79130070/how-can-i-integrate-aws-bedrock-with-my-database-to-enable-vector-based-context</link>
      <description><![CDATA[我正在构建一个 AI 驱动的忠诚度应用程序，并希望利用大型语言模型 (LLM) 根据我的数据库内容提供响应。我目前的计划是：

将数据库数据转换为向量嵌入：我想将我的结构化数据转换为向量嵌入，以便 LLM 可以更轻松地使用它。
将嵌入存储在向量数据库中：这个想法是存储嵌入以实现基于相似性的高效检索。
使用 AWS Bedrock LLM：我想根据用户的查询从我的数据库中检索上下文，并使用 AWS Bedrock 将此上下文输入到 LLM 中以生成响应。

我将不胜感激任何有关以下方面的指导：

嵌入转换：是否有任何推荐的工具或与 AWS Bedrock 兼容的模型用于将关系数据库中的结构化数据转换为有用的嵌入？ Amazon Titan 是否适合这种情况，还是其他模型更好？

向量数据库选项：对于存储和查询嵌入，Amazon OpenSearch 是否合适，还是我应该考虑 FAISS 之类的东西？我的目标是实现高容量、实时的检索效率。

LLM 集成最佳实践：检索类似嵌入后，格式化并将此上下文传递给 AWS Bedrock 上的 LLM 的最佳方法是什么？有任何示例、文章或模板吗？

有关特定工具、模型或文章的建议，可以提供关于在 AWS Bedrock 上设置此工作流程的进一步见解。

]]></description>
      <guid>https://stackoverflow.com/questions/79130070/how-can-i-integrate-aws-bedrock-with-my-database-to-enable-vector-based-context</guid>
      <pubDate>Sun, 27 Oct 2024 07:10:23 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的 Tensorflow predict() 时间序列对齐</title>
      <link>https://stackoverflow.com/questions/79106002/tensorflow-predict-timeseries-alignment-in-python</link>
      <description><![CDATA[假设我在 Tensorflow 中创建一个顺序输入 LSTM，如下所示：
def Sequential_Input_LSTM(df, input_sequence):
df_np = df.to_numpy()
X = []
y = []

for i in range(len(df_np) - input_sequence):
row = [a for a in df_np[i:i + input_sequence]]
X.append(row)
label = df_np[i + input_sequence]
y.append(label)

return np.array(X), np.array(y)

X, y = Sequential_Input_LSTM(df_data , 10) # pandas DataFrame df_data 包含我们的数据

在此示例中，我将数据切片X（输入向量）和 y（标签），例如前 10 个值（序列长度）用作 X，第 11 个值用作第一个 y。然后，将 10 个值的窗口向右移动一步（再移动一个时间步），我们再次为 X 取 10 个值，并将第二行之后的值作为下一个 y，依此类推。
然后假设我将 X 的一部分作为我的 X_test，并使用 LSTM model 进行时间序列预测，例如 predictions = model.predict(X_test)。
当我实际尝试此操作并绘制 predict(X_test) 的结果时，它看起来像 y 数组，并且预测结果是同步的，无需进一步调整。我预计在将预测数组与标签一起绘制时，我必须手动将预测数组向右移动 10 个时间步，因为我无法解释预测的前 10 个时间戳来自哪里。
由于模型尚未收到 10 个输入序列值，X_test 的前 10 个时间步的预测来自哪里？Tensorflow 是否使用 X_test 中的最后几个时间步来创建前 10 个值的预测，还是一开始的预测只是纯粹的猜测？]]></description>
      <guid>https://stackoverflow.com/questions/79106002/tensorflow-predict-timeseries-alignment-in-python</guid>
      <pubDate>Sat, 19 Oct 2024 21:37:05 GMT</pubDate>
    </item>
    <item>
      <title>迁移学习预训练模型</title>
      <link>https://stackoverflow.com/questions/78875648/transfer-learning-pretrained-model</link>
      <description><![CDATA[我在 Google Colab 上拟合迁移学习模型。但是，我在代码中遇到了一条警告消息
Epoch 1/30
/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: 
UserWarning：您的 `PyDataset` 类应在其构造函数中调用 `super().__init__(**kwargs)`。`**kwargs` 可以包括 `workers`、`use_multiprocessing`、`max_queue_size`。
请勿将这些参数传递给 `fit()`，因为它们将被忽略。
self._warn_if_super_not_called()

在第一个 epoch 之后，我收到以下错误：
----------------------------------------------------------------------------------------
KeyboardInterrupt Traceback（最近一次调用最后一次）
&lt;ipython-input-23-962a870d4412&gt; in &lt;cell line: 16&gt;()
14 # 拟合模型
15 # 运行单元。执行需要一些时间
---&gt; 16 training_history = model_efficientnet.fit(
17 training_set,
18 validation_data=validate_set,

我已经成功地拟合了其他六个迁移学习模型，没有任何问题，它们的准确率令人满意。
如何解决这个问题？
我想获得训练准确率和验证准确率]]></description>
      <guid>https://stackoverflow.com/questions/78875648/transfer-learning-pretrained-model</guid>
      <pubDate>Thu, 15 Aug 2024 14:49:45 GMT</pubDate>
    </item>
    <item>
      <title>Tensor Flow TFX 管道中的图像处理</title>
      <link>https://stackoverflow.com/questions/72166920/image-processing-in-tensor-flow-tfx-pipelines</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/72166920/image-processing-in-tensor-flow-tfx-pipelines</guid>
      <pubDate>Mon, 09 May 2022 04:10:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 scikit 的 Surprise 进行预测？</title>
      <link>https://stackoverflow.com/questions/65282827/how-to-make-predictions-with-scikits-surprise</link>
      <description><![CDATA[我在理解 Surprise 工作流程时遇到了一些困难。我有一个用于训练的文件（我想将其分为训练和验证）和一个用于测试数据的文件。我很难理解 Surprise Dataset 和 Trainset 之间的区别
# 导入数据
data_dir = &#39;DIRECTORY_NAME&#39;
reader = Reader(rating_scale=(1, 5))

# 创建 pandas 数据框
train_valid_df = pd.read_csv(os.path.join(data_dir, &#39;TRAINING_FILENAME.csv&#39;))
train_df, valid_df = train_test_split(train_valid_df, test_size=0.2)
test_df = pd.read_csv(os.path.join(data_dir, &#39;TEST_FILENAME.csv&#39;))

# 创建 Surprise Dataset 对象
train_valid_Dataset = Dataset.load_from_df(train_valid_df[[&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;]], reader)
train_Dataset = Dataset.load_from_df(train_df[[&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;]], reader)
valid_Dataset = Dataset.load_from_df(valid_df[[&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;]], reader)
test_Dataset = Dataset.load_from_df(test_df[[&#39;user_id&#39;, &#39;item_id&#39;, &#39;rating&#39;]], reader)

# 创建惊喜训练集对象（和测试集对象？）
train_Trainset = train_data.build_full_trainset()
valid_Testset = trainset.build_anti_testset()

然后，我创建我的预测器：
algo = KNNBaseline(k=60, min_k=2, sim_options={&#39;name&#39;: &#39;msd&#39;, &#39;user_based&#39;: True})

现在，如果我想进行交叉验证，我会这样做
cross_v = cross_validate(algo, all_data, measures=[&#39;mae&#39;], cv=10, verbose=True)

哪个训练模型（？），但如果我想使用我的固定验证集，我该怎么做？这个：？
algo.fit(train_Trainset)

完成此操作后，我尝试获得一些预测：
predictions = algo.test(valid_Testset)
print(predictions[0])

结果如下

但是当我尝试使用商品和用户 ID 号进行预测时，它说这样的预测是不可能的：
print(algo.predict(&#39;13&#39;, &#39;194&#39;))
print(algo.predict(&#39;260&#39;, &#39;338&#39;))
print(algo.predict(&#39;924&#39;, &#39;559&#39;))

结果：

第一个用户/项目对来自训练反集，第二个来自验证集，第三个来自训练集。我不知道为什么会出现这种情况，而且我发现文档有时令人困惑。同样，网上的许多教程似乎都在对 pandas 数据框进行训练，而我却因此而遇到错误。有人能解释一下 surprise 的工作流程到底是什么样的吗？我如何训练然后在测试集上进行预测？]]></description>
      <guid>https://stackoverflow.com/questions/65282827/how-to-make-predictions-with-scikits-surprise</guid>
      <pubDate>Mon, 14 Dec 2020 02:19:44 GMT</pubDate>
    </item>
    <item>
      <title>Bert 句子嵌入</title>
      <link>https://stackoverflow.com/questions/58168936/bert-sentence-embeddings</link>
      <description><![CDATA[我正在尝试获取 Bert 的句子嵌入，但我不太确定我是否做得正确……是的，我知道已经存在这样的工具，例如 bert-as-service，但我想自己做并了解它的工作原理。
假设我想从以下句子“I am.”的单词嵌入中提取一个句子嵌入。据我了解，Bert 以 (12, seq_lenght, 768) 的形式输出。我从最后一个编码器层以 (1, 768) 的形式提取每个单词嵌入。我现在的疑问在于从这两个词向量中提取句子。如果我有 (2,768)，我应该将 dim=1 相加并获得 (1,768) 的向量吗？或者也许将两个单词 (1, 1536) 连接起来并应用 (均值) 池化并获得形状为 (1, 768) 的句子向量。我不确定针对这个给定的例子获取句子向量的正确方法是什么。 ]]></description>
      <guid>https://stackoverflow.com/questions/58168936/bert-sentence-embeddings</guid>
      <pubDate>Mon, 30 Sep 2019 13:29:18 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习 - 如何处理动作空间中的边界[关闭]</title>
      <link>https://stackoverflow.com/questions/51127979/deep-reinforcement-learning-how-to-deal-with-boundaries-in-action-space</link>
      <description><![CDATA[我构建了一个自定义强化学习环境和代理，它类似于迷宫游戏。
在迷宫中有 5 种可能的动作：上、下、左、右和停留。如果被阻挡，例如代理无法上去，那么人们如何设计环境和代理来模拟这种情况？
具体来说，代理处于当前状态s0，根据定义，采取下、左、右动作将使状态更改为其他值并立即获得奖励（如果在出口则为&gt;0）。一种可能的方法是，当采取上动作时，状态将保持在s0，奖励将是一个很大的负数。理想情况下，代理将学习这一点，并且永远不会再在这个状态下上。 
但是，我的代理似乎没有学到这一点。相反，它仍然向上。另一种方法是对代理和环境进行硬编码，使代理在s0时无法执行操作向上，我能想到的是：

当某些状态下不允许向上时，我们查看不同操作的Q值
选择除向上之外具有最大Q值的操作
因此，代理永远不会执行无效操作

我想问的是上述方法是否可行？会不会有什么与此相关的问题？或者有没有更好的设计来处理边界和无效操作？]]></description>
      <guid>https://stackoverflow.com/questions/51127979/deep-reinforcement-learning-how-to-deal-with-boundaries-in-action-space</guid>
      <pubDate>Mon, 02 Jul 2018 00:35:49 GMT</pubDate>
    </item>
    <item>
      <title>sklearn：获取点到最近聚类的距离</title>
      <link>https://stackoverflow.com/questions/44041347/sklearn-get-distance-from-point-to-nearest-cluster</link>
      <description><![CDATA[我正在使用 DBSCAN 之类的聚类算法。
它返回一个名为 -1 的“聚类”，这些点不属于任何聚类。对于这些点，我想确定它与最近聚类之间的距离，以获得类似于该点异常程度的指标。这可能吗？或者这种指标还有其他选择吗？]]></description>
      <guid>https://stackoverflow.com/questions/44041347/sklearn-get-distance-from-point-to-nearest-cluster</guid>
      <pubDate>Thu, 18 May 2017 07:31:36 GMT</pubDate>
    </item>
    </channel>
</rss>