<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 11 Jul 2024 15:16:02 GMT</lastBuildDate>
    <item>
      <title>分割任何模型（SAM）如何使用多个框及其对应点来预测_torch？</title>
      <link>https://stackoverflow.com/questions/78736247/segment-anything-model-sam-how-do-i-predict-torch-with-multiple-boxes-with-the</link>
      <description><![CDATA[我目前正在尝试 Segment Anything 模型 (SAM)，我的问题需要多个框及其对应的点，以便在框内明确。例如，box1 = [#, #, # ,#]，其点为 [x,y]，类为 [0 或 1]，然后在单个图像中包含多个这样的点。
我仅使用多个边界框就可以做到这一点，但我想在每个框中包含点。
这是我当前的代码，我不再确定它为什么会给我一个错误：
RuntimeError：除了维度 1 之外，张量的大小必须匹配。预期大小为 1，但列表中的张量编号 1 的大小为 3。

import numpy as np
import torch
import matplotlib.pyplot as plt

point = np.array([[330, 370]])
label = np.array([1])

input_point = torch.tensor(point, device=predictor.device)
input_point = input_point.unsqueeze(0)
transformed_point = predictor.transform.apply_coords_torch(input_point, image.shape[:2])

input_label = torch.tensor(label, device=predictor.device)
input_label = input_label.unsqueeze(0)

#yxyx-xyxy
filtered_rois_xyxy = transform_yxyx_to_xyxy(filtered_rois)
input_boxes = torch.tensor(filtered_rois_xyxy, device=predictor.device)
transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, image.shape[:2]) 

masks,_,_ = predictor.predict_torch(
boxes=transformed_boxes,
point_coords=transformed_point,
point_labels=input_label,
multimask_output=False
)

masks.shape

plt.figure(figsize=(10, 10))
plt.imshow(image)
for mask in mask:
show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)
for box in input_boxes:
show_box(box.cpu().numpy(), plt.gca())
plt.axis(&#39;off&#39;)
plt.show()

为了调试目的，这是每个输入的打印：

print(input_boxes)
print(input_point)
print(input_label)

tensor([[330, 370, 495, 634],
[401, 168, 586, 425],
[ 1, 0, 157, 210]], dtype=torch.int32)
tensor([[[330, 370]]])
张量([[1]])
]]></description>
      <guid>https://stackoverflow.com/questions/78736247/segment-anything-model-sam-how-do-i-predict-torch-with-multiple-boxes-with-the</guid>
      <pubDate>Thu, 11 Jul 2024 14:49:27 GMT</pubDate>
    </item>
    <item>
      <title>Flask 后端的部署</title>
      <link>https://stackoverflow.com/questions/78735527/deployment-of-flask-backend</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78735527/deployment-of-flask-backend</guid>
      <pubDate>Thu, 11 Jul 2024 12:29:34 GMT</pubDate>
    </item>
    <item>
      <title>x86 mulss 结果随时间而不同[关闭]</title>
      <link>https://stackoverflow.com/questions/78735265/x86-mulss-result-is-diffrent-over-time</link>
      <description><![CDATA[我正在尝试制作 C++ 神经网络库，我注意到，有时具有相同输入和相同参数值的模型会毫无原因地大幅改变输出。我不知道为什么会发生这种情况，我确信我没有犯任何错误。看起来我的 fpu 已经耗尽并损失了一点精度，这导致较大网络（例如 10m 个参数）中的输出非常不同。
编辑：
经过测试，我发现使用 -O3 标志会导致错误的二进制文件，而 -Oz 会导致有效的二进制文件。有什么想法吗？它仍然使用 mulss...
编辑 2：经过进一步测试，-Oz 只会延迟一段时间的缺陷。
MRE？我无法提供 MRE，因为简单的 10 ^ 7 乘法 100 次返回结果 0 的总差异增量...
我只能提供整个库的“MRE”。如果有错误，我会查看
network.cpp -&gt;

void nn::network::eval::forward(evaluator *evaluator)
void forward_layer(int l1_size, int l2_size, float* l1_values, float* l2_values, float* weights, float* biases, nn::network::modules::activation::activation_func func)
void nn::network::connect(network *network, float dist_max, float dist_min)

我创建为 MRE 的 Repo（我知道它不像想要的那么小）：https://github.com/ZDibLO/libnn
示例：
输入：-0.729, 0.670, 0.937, -0.557, -0.383, 0.094, -0.623, 0.985, 0.992, 0.935
通过：0
输出：-0.180, 0.278, 0.781, 0.994, -0.702, -0.644, -0.849, 0.784, -0.901, -0.981
...
通过：4
输出：-0.180, 0.278, 0.781, 0.994, -0.702, -0.644, -0.849, 0.784, -0.901, -0.981
通过：5
出局：-0.494, -0.852, 0.961, 0.747, 0.826, 0.685, -0.495, 0.662, 0.810, -0.453
...
通过：8
出局：-0.494, -0.852, 0.961, 0.747, 0.826, 0.685, -0.495, 0.662, 0.810, -0.453
通过：9
出局：-0.521, -0.964, 0.973, 0.012, 0.991, 0.818, 0.752, 0.896, 0.995, 0.779
...
通过：16
出局：-0.521, -0.964, 0.973, 0.012, 0.991, 0.818, 0.752, 0.896, 0.995, 0.779
通过：17
出局：-0.180, 0.278, 0.781, 0.994, -0.702, -0.644, -0.849, 0.784, -0.901, -0.981
...
通过：99
出局：-0.180, 0.278, 0.781, 0.994, -0.702, -0.644, -0.849, 0.784, -0.901，-0.981
delta：22.8341
总计：3786ms
平均值：37ms
网络：params：10091120（10.0m）权重：9991100，偏差：100020

Delta：与上次输出相比，总输出变化
请帮忙，因为我无法继续处理未定义的行为。
CPU：i7 8550u / 操作系统：linux | Arch Linux
我将很快在两台使用相同型号的不同 Windows 机器上进行测试。
我尝试了不同的模型，调整了 sse 标志（之前它返回 NaN），并且我在寻找其他错误，但找不到任何错误。]]></description>
      <guid>https://stackoverflow.com/questions/78735265/x86-mulss-result-is-diffrent-over-time</guid>
      <pubDate>Thu, 11 Jul 2024 11:31:13 GMT</pubDate>
    </item>
    <item>
      <title>Skglm：线性结合 SCAD 惩罚和 Ridge 惩罚</title>
      <link>https://stackoverflow.com/questions/78734493/skglm-linearily-combining-scad-penalty-with-ridge-penalty</link>
      <description><![CDATA[我正在使用 skglm 库拟合以下惩罚 SCAD 回归模型。但是，在回归量之间存在高相关性的情况下，纯 SCAD 惩罚实际上并不稳定，而我使用的数据集中就存在这种情况。有没有办法使用由超参数控制的线性组合将 SCAD 惩罚与 RIDGE 惩罚结合起来，就像使用以下代码进行弹性网络回归一样？
import skglm
from skglm.penalties import SCAD
from skglm import GeneralizedLinearEstimator
import numpy as np

# 设置随机种子以实现可重复性
np.random.seed(42)

# 定义维度
num_samples = 300
num_features = 10000

# 使用随机值生成 X
X = np.random.randn(num_samples, num_features)

# 使用随机值生成 y
y = np.random.randn(num_samples, 1)

# SCAD 惩罚
scad_penalty = SCAD(alpha=1,gamma=3.7)

# 使用 SCAD 创建广义线性回归器惩罚
模型 = GeneralizedLinearEstimator(惩罚=scad_penalty)
test_fit=model.fit(X,y)
]]></description>
      <guid>https://stackoverflow.com/questions/78734493/skglm-linearily-combining-scad-penalty-with-ridge-penalty</guid>
      <pubDate>Thu, 11 Jul 2024 08:47:00 GMT</pubDate>
    </item>
    <item>
      <title>迁移学习的输入数据预处理[关闭]</title>
      <link>https://stackoverflow.com/questions/78733982/preprocessing-input-data-for-transfer-learning</link>
      <description><![CDATA[我一直在广泛使用 TensorFlow 的预训练模型，并注意到许多教程中都存在一个反复出现的问题。通常，这些教程会对模型进行微调或冻结，并将其用作特征提取器，根据手头的任务附加不同的输出层。然而，我注意到许多教程似乎跳过了预处理数据的步骤，即使每个模型都需要不同的输入，或者数据仅在 0-1 之间缩放。
对于每个模型，都有一个 preprocess_input 函数，但它似乎在各个教程中的应用不一致。有时它只在预测期间使用，而不是在训练期间使用，有时根本不使用。这让我想到了我的问题：

数据预处理的方式是否应该与训练和预测相同？
假设我所有模型的输入数据始终是 0-255 之间的 RGB 值，是否可以通过将带有 preprocess_input 的 lambda 函数附加到输入层来解决这种不一致问题？例如：

x = tf.keras.layers.Lambda(preprocess_input)(input_layer)
]]></description>
      <guid>https://stackoverflow.com/questions/78733982/preprocessing-input-data-for-transfer-learning</guid>
      <pubDate>Thu, 11 Jul 2024 06:57:27 GMT</pubDate>
    </item>
    <item>
      <title>如何准确计算Doctr ocr中检测到的文本的绝对bbox坐标？</title>
      <link>https://stackoverflow.com/questions/78733724/how-to-accurately-calculate-absolute-bbox-coordinates-of-detected-text-in-doctr</link>
      <description><![CDATA[我一直试图在文档的图片上绘制 bbox，并尝试使用 mindee-doctr 进行 ocr 以查看检测到的文本行。我面临的问题是，我通过乘以相对坐标和页面尺寸计算出的 bbox 的绝对坐标，在原始图像上绘制时都向右上角偏移。有没有办法纠正这个问题？
这是我计算 bbox 的代码：
# 使用 docTR 分析图像并获取结果
line_boundaries = []
model = ocr_predictor(pretrained=True) #设置preserve_aspect_ratio=False 或symmetric_pad=False 没有区别。
doc = DocumentFile.from_images(img_path)
result = model(doc)

# 提取每行的边界框坐标
for page in result.pages:
for block in page.blocks:
for line in block.lines:
# 将相对坐标与页面尺寸相乘，得到绝对坐标
x_min, y_min, x_max, y_max = round(line.geometry[0][0] * page.dimensions[0]), round(line.geometry[0][1] * page.dimensions[1]), round(line.geometry[1][0] * page.dimensions[0]), round(line.geometry[1][1] * page.dimensions[1])
line_boundaries.append((x_min, y_min, x_max, y_max))

这是 line_boundaries 的值：
[(531, 148, 1321, 184), (2725, 148, 3061, 177), (526, 254, 3071, 295), (526, 288, 3071, 332), (535, 324, 3071, 363), ... ]
这是我用来绘制方框的函数：
def draw_rectangles(image_path, line_boundaries):
&quot;&quot;&quot;
使用提供的线边界在图像上绘制矩形。

参数：
image_path：图像文件的路径。
line_boundaries：线边界列表，其中每个边界都是四个点的列表。

返回：
无
&quot;&quot;&quot;

# 加载图像
image = cv2.imread(img_path)
#image = DocumentFile.from_images(img_path)[0] #本质上等同于 cv2.imread()

# 遍历线边界并绘制矩形
for bounding in line_boundaries:
#x1, y1, x2, y2 = int(boundary[0][0]), int(boundary[0][1]), int(boundary[2][0]), int(boundary[2][1])
x1, y1, x2, y2 = map(int, bounding) # 将坐标转换为整数
cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

# 用矩形显示图像
cv2_imshow(image) # 仅在协作时使用 cv2_imshow 代替 cv2.imshow
cv2.waitKey(0)
cv2.destroyAllWindows()

这是绘制了 bbox 的图像。

我尝试过不使用舍入，但没有任何区别，也尝试过只使用预测器，但无济于事。在 ocr_predictor 中设置preserve_aspect_ratio=False 或symmetric_pad=False 也没有区别。]]></description>
      <guid>https://stackoverflow.com/questions/78733724/how-to-accurately-calculate-absolute-bbox-coordinates-of-detected-text-in-doctr</guid>
      <pubDate>Thu, 11 Jul 2024 05:38:22 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 中的空间数据管理机器学习模型中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</guid>
      <pubDate>Thu, 11 Jul 2024 05:01:17 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 benchmark 进行 scikit-learn 运行时性能测试分析</title>
      <link>https://stackoverflow.com/questions/78733634/how-to-use-benchmark-for-scikit-learn-runtime-performance-test-analysis</link>
      <description><![CDATA[现在我已经构建了 scikit-learn，但我不知道如何使用基准测试来测试 DBSCAN、KMeans 等案例，以获取性能数据，例如“每批 CPU 挂机时间、CPU 挂机时间、第一批时间、CPU 峰值内存”。
我完全不知道如何测试这个。我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/78733634/how-to-use-benchmark-for-scikit-learn-runtime-performance-test-analysis</guid>
      <pubDate>Thu, 11 Jul 2024 04:59:40 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Linux 远程服务器中运行计算机视觉 Github 存储库 [关闭]</title>
      <link>https://stackoverflow.com/questions/78733479/how-to-run-computer-vision-github-repositories-in-linux-remote-server</link>
      <description><![CDATA[我是 ml/dl 的新学习者。我想在 Linux 远程服务器上运行计算机视觉 github 存储库。请帮帮我。我不知道如何运行任何计算机视觉 github 存储库。在我 gitclone 它之后，我该如何运行它。此外，我必须在大多数 github 存储库中手动下载数据集。如果数据集太大（例如 80 GB），我该如何在远程服务器中下载它。请帮帮我。
大家好，我是 ml/dl 的新学习者。我想在 Linux 远程服务器上运行计算机视觉 github 存储库。请帮帮我。我不知道如何运行任何计算机视觉 github 存储库。在我 gitclone 它之后，我该如何运行它。此外，我必须在大多数 github 存储库中手动下载数据集。如果数据集太大（例如 80 GB），我该如何在远程服务器中下载它。请帮帮我。如果有人能给我一个大纲，那将非常有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78733479/how-to-run-computer-vision-github-repositories-in-linux-remote-server</guid>
      <pubDate>Thu, 11 Jul 2024 03:52:26 GMT</pubDate>
    </item>
    <item>
      <title>时间序列数据的 LSTM 模型的差异进化训练时间过长 [关闭]</title>
      <link>https://stackoverflow.com/questions/78733225/differential-evolution-on-lstm-model-for-time-series-data-taking-too-long-to-tra</link>
      <description><![CDATA[我正在使用 TensorFlow/Keras 中的 LSTM 进行时间序列预测项目，并尝试使用差分进化来优化我的模型的超参数。但是，即使只有 100 个观察值，训练过程也需要很长时间。
这是我的代码片段：
# 导入库
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from sklearn.model_selection import GridSearchCV
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from scipy.optimize import different_evolution

# 导入数据集
df = pd.read_csv(&quot;1 Yr Yields Series.CSV&quot;)
df.head()

# 删除非数字列和目标变量
features = df.drop(columns=[&#39;Unnamed: 0&#39;, &#39;Date&#39;, &#39;Nation&#39;, &#39;Yield&#39;])

# 存储收益率值
yields = df[&#39;Yield&#39;].values.reshape(-1, 1)

# 将 % 列转换为字符串
features[&#39;DivYield&#39;] = features[&#39;DivYield&#39;].astype(str)
features[&#39;DivYieldFTSE&#39;] = features[&#39;DivYieldFTSE&#39;].astype(str)

# 将 % 列转换为浮点数
features[&#39;DivYield&#39;] = features[&#39;DivYield&#39;].str.rstrip(&#39;%&#39;).astype(float) / 100
features[&#39;DivYieldFTSE&#39;] = features[&#39;DivYieldFTSE&#39;].str.rstrip(&#39;%&#39;).astype(float) / 100

# 缩放数据
scaler = StandardScaler()
features_standardized = scaler.fit_transform(features)

# 创建 pca 实例以拟合标准化数据
pca = PCA()
X_pca = pca.fit_transform(features_standardized)

# 使用肘部方法找到要减少到的 k 个组件
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel(&#39;组件数量&#39;)
plt.ylabel(&#39;累积解释方差&#39;)
plt.show()

# 找到我们的主要原则成分
per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)
labels = [&#39;PC&#39; + str(x) for x in range(1, len(per_var)+1)]

plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)
plt.ylabel(&#39;% of explained variance&#39;)
plt.xlabel(&#39;Principal component&#39;)
plt.title(&#39;Scree Plot&#39;)
plt.show()

# 根据图使用最佳的主成分数量
k = 7
pca = PCA(n_components=k)
X_pca = pca.fit_transform(features_standardized) # 转换我们的数据

# 将收益与 PCA 转换的特征连接起来
pca_array = np.concatenate((yields, X_pca), axis=1)
pca_array.shape # 收益位于第 0 个索引

# 查找我们的初始权重
principal_components = pca.components_
weights = np.abs(principal_components) / np.sum(np.abs(principal_components), axis=1, keepdims=True)
W = tf.convert_to_tensor(weights, dtype=tf.float32) # 确保权重是 float32 张量

print(f&quot;Initial Weights:\n{weights}&quot;)

# 将数据拆分为训练集和验证集
X_train, X_val, y_train, y_val = train_test_split(pca_array[:100, 1:8], pca_array[:100, 0], test_size=0.2)

def create_lstm_model(units, dropout, learning_rate):
model = tf.keras.Sequential([
tf.keras.layers.LSTM(units, input_shape=(X_train.shape[1], X_train.shape[2])),
tf.keras.layers.Dropout(dropout),
tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)
])
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
loss=&#39;binary_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])
return model

def objective(params):
units, dropout, learning_rate = params
model = create_lstm_model(int(units), dropout, learning_rate)

history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=0)

val_loss = min(history.history[&#39;val_loss&#39;])

return val_loss

bounds = [(10, 200), # 单位
(0.1, 0.5), # dropout
(1e-4, 1e-2)] # learning_rate

result = different_evolution(objective, bounds, strategies=&#39;best1bin&#39;, maxiter=100, popsize=15, tol=0.01)

# 打印最优参数和相应的分数
print(&quot;Optimal Parameters:&quot;, result.x)
print(&quot;Best Score:&quot;, result.fun)


我下载了 miniconda 并尝试运行我在 GPU 上进行了差分进化，看看它是否能提高速度，但没有成功。
问题：

有没有更有效的方法来执行超参数优化？

我的 LSTM 模型或我分割数据的方式是否会导致延迟？

是否有任何参数或方法可以调整以加快优化过程？

有没有什么方法可以可视化每一步的时间？

]]></description>
      <guid>https://stackoverflow.com/questions/78733225/differential-evolution-on-lstm-model-for-time-series-data-taking-too-long-to-tra</guid>
      <pubDate>Thu, 11 Jul 2024 01:34:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 Catboost Golang</title>
      <link>https://stackoverflow.com/questions/78732455/use-catboost-golang</link>
      <description><![CDATA[如何在 golang 上使用 CatBoost？也许有解决方案？
我尝试使用 https://github.com/bourbaki/catboost-go 但我一直收到 CGO 错误。
当我尝试使用 go run 运行它时，它会在编译时出错
usr/local/go/pkg/tool/linux_amd64/link：运行 gcc 失败：退出状态 1
/usr/bin/ld：/tmp/go-link-23536551/000001.o：在函数“_cgo_e59e54336bq_Cfunc”中：
/tmp/go-build/cgo-gcc-prolog:69：对“my_function”的未定义引用
collect2：错误：ld 返回 1 退出状态
]]></description>
      <guid>https://stackoverflow.com/questions/78732455/use-catboost-golang</guid>
      <pubDate>Wed, 10 Jul 2024 19:40:07 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft ML：预测代码始终预测 0，但在“评估”选项卡中尝试时，它始终有效（非 0 的值）</title>
      <link>https://stackoverflow.com/questions/78731177/microsoft-ml-the-prediction-code-always-predicts-0-but-when-tried-in-the-evalua</link>
      <description><![CDATA[系统信息：

模型构建器版本 17.15.0.2337001
Visual Studio 版本 2022

我创建了一个机器学习模型（值预测），一切都很好

通过这张图片，你可以看到模型正常工作
但是当我使用模型构建器的代码时：
使用 ConsoleApp2;

//加载样本数据
var sampleData = new MLModel1.ModelInput()
{
Device = 1F,
Temperature = 20F,
Weather = 1F,
Time = 2F,
};

//加载模型并预测输出
var result = MLModel1.Predict(sampleData);
Console.WriteLine(result.Predict);

输出始终为 0 或预测始终为 0：
]]></description>
      <guid>https://stackoverflow.com/questions/78731177/microsoft-ml-the-prediction-code-always-predicts-0-but-when-tried-in-the-evalua</guid>
      <pubDate>Wed, 10 Jul 2024 14:28:45 GMT</pubDate>
    </item>
    <item>
      <title>无法部署基于 Flask 的深度学习应用程序[关闭]</title>
      <link>https://stackoverflow.com/questions/78731060/unable-to-deploy-my-deep-learning-app-based-on-flask</link>
      <description><![CDATA[我的 ml 应用程序基于 flask，它以 pdf 作为输入，让用户提问并利用 BERT 模型来回答，
到目前为止，它在本地运行良好，但在部署中没有运气，我尝试了 vercel、pythonanywhere，但没有任何效果，欢迎您为部署做出贡献
github repo https://github.com/MohdSiddiq12/Natural-Language-Question-Answering-System]]></description>
      <guid>https://stackoverflow.com/questions/78731060/unable-to-deploy-my-deep-learning-app-based-on-flask</guid>
      <pubDate>Wed, 10 Jul 2024 14:04:16 GMT</pubDate>
    </item>
    <item>
      <title>CNN 模型中的损失函数没有减少吗？</title>
      <link>https://stackoverflow.com/questions/78712668/loss-function-not-decreasing-on-a-cnn-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78712668/loss-function-not-decreasing-on-a-cnn-model</guid>
      <pubDate>Fri, 05 Jul 2024 17:20:29 GMT</pubDate>
    </item>
    <item>
      <title>yolo8 面具检测给了我多个轮廓</title>
      <link>https://stackoverflow.com/questions/77409777/yolo8-masks-detection-gives-me-multiple-contours</link>
      <description><![CDATA[我做了一些实验后修改了这个问题！
我正在用 yolo8 做实验。
我制作了一个测试脚本来展示这个问题：
#
# 测试脚本用于探索 1 个找到的对象的多个轮廓。
导入 numpy 作为 np
导入 cv2

img = cv2.imread(&#39;demo.jpg&#39;)
imgcopy = img.copy()

从 ultralytics 导入 YOLO

# 加载模型
model = YOLO(&#39;yolov8n-seg.pt&#39;) # 加载官方模型
#model = YOLO(&#39;path/to/best.pt&#39;) # 加载自定义模型

# 使用模型进行预测
results = model(img, retina_masks=True, save=True, imgsz = 640, conf=0.5, boxes=False, show_labels=False, show_conf=False, save_crop=True, save_txt=True, save_conf=True) # 根据图像进行预测

for result in results:
mask = result.masks
for mask in mask.data:
mask_np= mask.cpu().numpy().astype(np.uint8)
#contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
contours, _ = cv2.findContours(mask_np, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)

print (&#39;我们找到了 &#39; + str(len(contours)) + &#39; 个对象&#39;)

# 我知道我得到了 3 个轮廓，所以这有效！
imgcopy = cv2.drawContours(imgcopy, contours=contours[0], contourIdx=-1, color=(0, 255, 0), thicken=2, lineType=cv2.LINE_AA)
imgcopy = cv2.drawContours(imgcopy, contours=contours[1], contourIdx=-1, color=(255, 255, 0), thicken=2, lineType=cv2.LINE_AA)
imgcopy = cv2.drawContours(imgcopy, contours=contours[2], contourIdx=-1, color=(255,0, 255), thicken=2, lineType=cv2.LINE_AA)

cv2.imwrite(&#39;result.jpg&#39;, imgcopy)


我预期找到 1 个轮廓，但我找到了 3 个轮廓这个例子。
输入图像：

输出图像：

所以 1 个掩码属于 1 个找到的对象，但它有 3 个轮廓。
为什么会找到多个轮廓？
我应该如何过滤出正确的轮廓？
我发现第一个找到的轮廓并不总是正确的。]]></description>
      <guid>https://stackoverflow.com/questions/77409777/yolo8-masks-detection-gives-me-multiple-contours</guid>
      <pubDate>Thu, 02 Nov 2023 12:59:13 GMT</pubDate>
    </item>
    </channel>
</rss>