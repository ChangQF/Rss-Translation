<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 19 Jun 2024 09:19:05 GMT</lastBuildDate>
    <item>
      <title>GPU 张量上的内向旋转环面</title>
      <link>https://stackoverflow.com/questions/78641329/inwards-spinning-torus-on-gpu-tensors</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78641329/inwards-spinning-torus-on-gpu-tensors</guid>
      <pubDate>Wed, 19 Jun 2024 08:28:38 GMT</pubDate>
    </item>
    <item>
      <title>如何使用树提升模型处理多输出回归的 3d 输入数据？</title>
      <link>https://stackoverflow.com/questions/78641199/how-to-handle-3d-input-data-with-tree-boosting-models-for-multi-output-regressio</link>
      <description><![CDATA[我有 32 种商品 x 27 种特征 x 3000 个时间步长的数据用于股票预测。出于某种原因，我必须使用各种 树提升模型（lightgbm、xgboost、adaboost）来比较回归任务的性能：预测单个时间步长上每只股票的回报率，并对之前时间步长的样本进行训练。预期的数据流将是：在 n 个时间步长上训练的模型，输出单个时间步长（例如 t+2）上每种商品的回报率向量（32 个元素向量）。但是，我不确定如何构造数据以进行有效的训练。
现在，我沿着商品和特征维度展平数据以创建 3000x864 数据框，但我认为这不是可行的方法，因为我怀疑我丢失了一些信息。
我使用 sklearn 的 MultiOutputRegressor 来适应 lgbm，但 mse 约为 0.5，对于预测在 -1 和 1 之间标准化的值非常不利，因为数据包含许多易于预测的 0。
这种展平方法是处理 3d 数据的正确方法吗？还是我需要使用其他方法？]]></description>
      <guid>https://stackoverflow.com/questions/78641199/how-to-handle-3d-input-data-with-tree-boosting-models-for-multi-output-regressio</guid>
      <pubDate>Wed, 19 Jun 2024 08:00:34 GMT</pubDate>
    </item>
    <item>
      <title>机器学习——对数据进行反复训练会导致过度拟合吗？</title>
      <link>https://stackoverflow.com/questions/78640262/machine-learning-does-repeatedly-training-on-data-cause-overfit</link>
      <description><![CDATA[所以我有一个已经训练好的机器学习模型，没有过拟合或欠拟合，这个模型是完美的，可以正确检测物体。但是，之后我尝试如果我再次反复训练这个模型，结果发生了过拟合，在一个已经完美的模型上反复训练模型会影响模型吗？所以原本正常的变成了过拟合。
我希望得到关于我的情况的解释]]></description>
      <guid>https://stackoverflow.com/questions/78640262/machine-learning-does-repeatedly-training-on-data-cause-overfit</guid>
      <pubDate>Wed, 19 Jun 2024 02:24:34 GMT</pubDate>
    </item>
    <item>
      <title>神经网络近似代码中的值错误</title>
      <link>https://stackoverflow.com/questions/78640203/value-error-in-code-for-neural-network-approximation</link>
      <description><![CDATA[import numpy as np
import matplotlib.pyplot as plt

# 定义目标函数 f(x) = sin(x)^2
def f(x):
return np.sin(x)**2

# 生成训练数据集
np.random.seed(0) # 为了可重复性
num_samples = 100
x_train = np.linspace(0, 2*np.pi, num_samples)
y_train = f(x_train)

# 初始化系数
n = len(x_train)
a = np.random.randn(n) # 线性组合系数
b = np.random.randn() # 常数项
m = 10 # 神经元数量
alphas = np.random.randn(m, n) # ReLU 神经元的权重
ts = np.random.randn(m) # ReLU 阈值神经元
cs = np.random.randn(m) # ReLU 神经元的系数

# 实现神经网络近似
def relu(x):
return np.maximum(0, x)

def f_N(x):
return np.dot(a, x) + b + np.sum(cs[:, np.newaxis] * relu(np.dot(alphas, x) - ts[:, np.newaxis]), axis=0)

# 定义损失函数
def loss_function(a, b, cs):
predictions = f_N(x_train)
return np.mean((predictions - y_train)**2)

# 梯度下降优化
def gradient_descent(learning_rate=0.01, num_iterations=1000):
loss = []
for i in range(num_iterations):
# 计算梯度
predictions = f_N(x_train)
误差 = 预测 - y_train

grad_a = np.outer(误差, x_train) / num_samples
grad_b = np.mean(误差)

relu_grad = np.dot(cs[:, np.newaxis] * (np.dot(alphas, x_train) - ts[:, np.newaxis] &gt; 0), alphas)
grad_cs = np.sum(error[:, np.newaxis] * relu_grad, axis=0) / num_samples

# 更新系数
a -= learning_rate * grad_a
b -= learning_rate * grad_b
cs -= learning_rate * grad_cs

# 裁剪或投影 cs 以确保它们保持在界限内
cs = np.clip(cs, -1, 1) # 在 -1 和 1 之间裁剪的示例

# 计算当前损失
current_loss = loss_function(a, b, cs)
loss.append(current_loss)

# 打印损失以进行监控
if i % 100 == 0:
print(f&quot;Iteration {i}, Loss: {current_loss}&quot;)

return loss

# 执行梯度下降
losses = gradient_descent()

# 绘制结果
plt.figure(figsize=(12, 6))

# 绘制目标函数
plt.plot(x_train, y_train, label=&#39;目标函数：$f(x) = \sin(x)^2$&#39;, color=&#39;blue&#39;)

# 绘制神经网络近似
y_pred = f_N(x_train)
plt.plot(x_train, y_pred, label=&#39;神经网络近似&#39;, linestyle=&#39;--&#39;, color=&#39;red&#39;)

plt.title(&#39;神经网络近似 vs 目标函数&#39;)
plt.xlabel(&#39;x&#39;)
plt.ylabel(&#39;y&#39;)
plt.legend()
plt.grid(True)
plt.show()

# 绘制损失曲线
plt.figure(figsize=(10, 5))
plt.plot(losses, label=&#39;训练Loss&#39;)
plt.title(&#39;Training Loss over Iterations&#39;)
plt.xlabel(&#39;Iteration&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()
plt.grid(True)
plt.show()


代码的目标如下：
定义目标函数 f(x) = sin(x)^2，并生成训练数据集 {(x_i, y_i)}，其中 y_i = f(x_i)，针对一系列 x_i 值。
用随机值初始化神经网络系数 a、b 和 c_i，并从标准高斯分布生成样本 (alpha_1,t_1),...,(alpha_m,t_m)。
实现神经网络近似 fN(x) = a^Tx+b+sum{i=1}^{m} ci\sigma(alpha{i}^T-t_i) 如图所示，使用初始化系数。
定义损失函数 L(a, b, c_1, ..., c_m)，用于测量神经网络近似值 f_N(x_i) 与训练数据点 (x_i, y_i) 的目标函数 f(x_i) 之间的差异。例如，您可以使用均方误差：L(a, b, c_1, ..., c_m) = Σ_i (f_N(x_i) - y_i)^2
使用线性回归最小化关于系数 a、b 和 c_i 的损失函数 L。这可以使用梯度下降或其他优化算法来完成。
在优化过程中，通过裁剪或投影系数到边界定义的可行区域，确保系数保持在提供的边界内。
我收到以下错误，无法调试它：

-----------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[30]，第 68 行
65 returnloss
67 # 执行梯度下降
---&gt; 68loss = gradient_descent()
70 # 绘制结果
71 plt.figure(figsize=(12, 6))

Cell In[30]，第 41 行
38 for i in range(num_iterations):
39 # 计算梯度
40 predictions = f_N(x_train)
---&gt; 41 error = predictions - y_train
43 grad_a = np.outer(error, x_train) / num_samples
44 grad_b = np.mean(error)

ValueError: 操作数不能与形状 (10,) (100,) 一起广播

如果有人能帮助我，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78640203/value-error-in-code-for-neural-network-approximation</guid>
      <pubDate>Wed, 19 Jun 2024 01:52:20 GMT</pubDate>
    </item>
    <item>
      <title>顺序模型拒绝构建</title>
      <link>https://stackoverflow.com/questions/78640126/sequential-model-refusing-to-build</link>
      <description><![CDATA[我正在尝试使用 TensorFlow 在 Python 中构建一个用于训练数据的模型，但构建失败。
到目前为止，我已经尝试过这个：
def create_model(num_words, embedding_dim, lstm1_dim, lstm2_dim, num_categories):
tf.random.set_seed(200)
model = Sequential([layers.Dense(num_categories,activation=&#39;softmax&#39;),layers.Embedding(num_words, embedding_dim),
layers.Bidirectional(layers.LSTM(lstm1_dim, return_sequences=True)),layers.Bidirectional(layers.LSTM(lstm2_dim))])

model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) 

返回模型

model = create_model(NUM_WORDS, EMBEDDING_DIM, 32, 16, 5)

print(model)

---


每当我 print(model) 时，它都会显示 &lt;Sequential name=sequation,built=False&gt;。]]></description>
      <guid>https://stackoverflow.com/questions/78640126/sequential-model-refusing-to-build</guid>
      <pubDate>Wed, 19 Jun 2024 00:59:40 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络 (CNN) 在决策中使用黑色背景 - LIME</title>
      <link>https://stackoverflow.com/questions/78640044/convolutional-neural-network-cnn-using-black-background-in-decision-making-l</link>
      <description><![CDATA[我是一名正在做学校项目的学生，需要帮助。
我的二元分类卷积神经网络在验证数据上具有非常高的准确率 (&gt;96%)，在测试数据集上的表现也同样出色。然而，当我使用 LIME 可视化图像中对其决策很重要的部分时，它往往会突出显示背景。所以我的问题是：
为什么会这样，以前有人见过吗？
当它在做决定时实际上是看着黑色面具时，它是如何达到 96% 的准确率的？
我在图像上应用黑色面具的原因是，我得到的整个数据集具有完全相同的背景，即白色滚轮，并且正如您从我上传的其中一张图片中看到的那样，该模型在决策过程中严重依赖滚轮，因此我将背景预处理为完全黑色 (0, 0, 0)RGB 像素，但现在模型似乎以某种方式使用了它。
我只是被难住了，非常感谢任何帮助！
模型架构
滚轮问题示例
我尝试了各种架构，其中一些使用 keras 层构建，甚至尝试了预训练的 ResNet50。我还改变了大多数重要的超参数，但行为仍然存在。如果有帮助，我可以提供任何细节。
我很感激任何提前提供的帮助！:)
]]></description>
      <guid>https://stackoverflow.com/questions/78640044/convolutional-neural-network-cnn-using-black-background-in-decision-making-l</guid>
      <pubDate>Wed, 19 Jun 2024 00:05:44 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：'float' 和 'list' 实例之间不支持 '<'</title>
      <link>https://stackoverflow.com/questions/78640019/typeerror-not-supported-between-instances-of-float-and-list</link>
      <description><![CDATA[我正在做家庭作业，但一直遇到这个错误。这些是我正在使用的函数，问题似乎出在预测函数中。预测和 TPR FPR 分数函数按预期工作，直到我在 roc_curve_computer 函数中调用它们。然后我收到 TypeError：&#39;&lt;&#39; not supports between &#39;float&#39; and &#39;list&#39; 消息。我尝试了多种不同的方法，但我遗漏了一些东西，但我仍然无法确定。问题出现在阈值变量从分配早期的单个浮点数变为分配结束时的浮点数组时。
def predict(probs, Threshold):
#创建数组来存储数据
preds = []

#for 循环遍历阈值上方和下方的数据排序。
for probs in probs:
if probs &lt;阈值：
preds.append(0)
else：
preds.append(1)
return preds

# 阈值
thresh = 0.5

# 预测值
preds = predict(probs, thresh)

def TPR_FPR_score(true_labels, preds):

# 将数组分类为 pos 和 neg 类别
actual_pos = np.array([True if x == 1 else False for x in true_labels])
actual_neg = np.array([True if x == 0 else False for x in true_labels])
pred_pos = np.array([True if x == 1 else False for x in preds])
pred_neg = np.array([True if x == 0 else False for x in preds])

# 设置准确度公式的变量
TP = (pred_pos) &amp; actual_pos).sum()
TN = (pred_neg &amp; actual_neg).sum()
FP = (pred_pos &amp; actual_neg).sum()
FN = (pred_neg &amp; actual_pos).sum()

#使用精度分数公式
TPR = TP / (TP + FN)

FPR = FP / (FP + TN)

return TPR, FPR

def roc_curve_computer(true_labels_check, pred_probs_check, Threshold):

TPR = []
FPR = []
preds_new = []
labels = 5

#拉动函数以获取预测值
for labels in range(labels):
preds_new = predict(pred_probs_check, Threshold)

#拉动函数以获取 TPR 和 FPR 值
for labels in range(labels):
TPR, FPR = TPR_FPR_score(labels, preds_new)

返回 TPR、FPR

true_labels_check = [1, 0, 1, 0, 0]
pred_probs_check = [0.875, 0.325, 0.6, 0.09, 0.4]
thresholds_check = [0.00, 0.25, 0.50, 0.75, 1.00]

print(roc_curve_computer(true_labels_check, pred_probs_check, Thresholds_check))

我尝试了另一种方式来执行预测函数，但遇到了类似的问题。任何帮助都值得感激。]]></description>
      <guid>https://stackoverflow.com/questions/78640019/typeerror-not-supported-between-instances-of-float-and-list</guid>
      <pubDate>Tue, 18 Jun 2024 23:47:33 GMT</pubDate>
    </item>
    <item>
      <title>在图层中添加 CategoryEncoding 会导致形状不匹配</title>
      <link>https://stackoverflow.com/questions/78639963/adding-categoryencoding-to-the-layer-causes-mismatch-in-shape</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78639963/adding-categoryencoding-to-the-layer-causes-mismatch-in-shape</guid>
      <pubDate>Tue, 18 Jun 2024 23:15:47 GMT</pubDate>
    </item>
    <item>
      <title>我是否需要在每个 Spark 节点上安装 PyTorch，还是只需要主节点来运行示例？</title>
      <link>https://stackoverflow.com/questions/78639778/do-i-need-pytorch-on-each-spark-node-or-just-the-master-for-running-examples</link>
      <description><![CDATA[我是否需要在 Apache Spark 集群的每个节点上安装 PyTorch，还是仅在主节点上安装它就足以顺利执行示例？
我尝试在 Apache Spark 集群上执行 PyTorch 示例，而无需在所有节点上安装 PyTorch，期望仅在主节点上安装就足以实现无缝执行。]]></description>
      <guid>https://stackoverflow.com/questions/78639778/do-i-need-pytorch-on-each-spark-node-or-just-the-master-for-running-examples</guid>
      <pubDate>Tue, 18 Jun 2024 21:47:13 GMT</pubDate>
    </item>
    <item>
      <title>是否应将多个分类嵌入组合成条件 GAN（cGAN）？</title>
      <link>https://stackoverflow.com/questions/78639650/should-multiple-categorical-embeddings-be-combined-for-a-conditional-gan-cgan</link>
      <description><![CDATA[我正在尝试制作一个条件 GAN (cGAN)，它可以根据标题和视频类别/流派生成 YouTube 缩略图。
它根本不起作用，甚至差得很远，所以我试图回到有关我的架构的基本问题。现在，我所做的是制作两个嵌入向量，一个用于标题，一个用于类别，然后我将它们组合起来并将它们都发送到生成器和鉴别器。
我想知道这样做可以吗？还是我应该分别传递它们？我尝试对其进行一些研究，但这是一个相当小众的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78639650/should-multiple-categorical-embeddings-be-combined-for-a-conditional-gan-cgan</guid>
      <pubDate>Tue, 18 Jun 2024 21:04:12 GMT</pubDate>
    </item>
    <item>
      <title>顺序模型未建立</title>
      <link>https://stackoverflow.com/questions/78639634/sequential-model-not-building</link>
      <description><![CDATA[我正在尝试使用 TensorFlow 在 Python 中构建一个用于训练数据的模型，但构建失败。有人发现问题了吗？
我到目前为止已经尝试过了：
def create_model(num_words, embedding_dim, lstm1_dim, lstm2_dim, num_categories):
tf.random.set_seed(200)
model = Sequential([layers.Dense(num_categories,activation=&#39;softmax&#39;),layers.Embedding(num_words, embedding_dim),
layers.Bidirectional(layers.LSTM(lstm1_dim, return_sequences=True)),layers.Bidirectional(layers.LSTM(lstm2_dim))])

model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;])

return model

model = create_model(NUM_WORDS, EMBEDDING_DIM，32，16，5）

print（model）

每当我print（model）时，它都会显示&lt;Sequential name=sequation,built=False&gt;。]]></description>
      <guid>https://stackoverflow.com/questions/78639634/sequential-model-not-building</guid>
      <pubDate>Tue, 18 Jun 2024 20:58:02 GMT</pubDate>
    </item>
    <item>
      <title>如何处理 ADB ML Workload 中的内存不足错误</title>
      <link>https://stackoverflow.com/questions/78637717/how-to-handle-out-of-memory-error-in-adb-ml-workload</link>
      <description><![CDATA[我的预测数据框有近 200 个特征，因此我得到了 OOM（内存不足错误）。我正在使用 Azure databricks，我该如何处理这个错误？
我已经进行了特征选择以减少特征数量，但我仍然必须使用 100 多个特征。我尝试过广播，但没有解决问题]]></description>
      <guid>https://stackoverflow.com/questions/78637717/how-to-handle-out-of-memory-error-in-adb-ml-workload</guid>
      <pubDate>Tue, 18 Jun 2024 13:19:29 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 Elixir Nx/Schorar 中进行 ELISA 分析？</title>
      <link>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</link>
      <description><![CDATA[我已阅读 Medium 上的文章 ELISA Analysis in Python。
上述文章使用 SciPy 的 curve_fit 函数根据 4 参数逻辑回归 (4PL) 模型找到近似曲线，如下所示：
from scipy.optimize import curve_fit

x = [1.95, 3.91, 7.381, 15.63, 31.25, 62.5, 125,250, 500, 1000]
y = [0.274, 0.347, 0.392, 0.420, 0.586, 1.115, 1.637, 2.227, 2.335, 2.372]

def log4pl(x, A, B, C, D):
return(((A - D) / (1.0 + ((x / C) ** B))) + D)

params, _ = curve_fit(log4pl, x, y)
A, B, C, D = params[0], params[1], params[2], params[3]

我想使用 Nx/Scholar 库。
可能吗？如果您能给我任何提示，我将不胜感激。

[更新]
快速浏览一下 Python scipy.optimize 源代码，似乎 curve_fit 在内部使用了 Fortran 的 MINPACK 库。
据我所知，没有简单的方法可以从 Elixir 使用 MINPACK。
因此，我得出结论，目前在 Elixir 中进行 ELISA 分析很困难。
欢迎提供任何其他信息。]]></description>
      <guid>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</guid>
      <pubDate>Sun, 02 Jun 2024 04:29:18 GMT</pubDate>
    </item>
    <item>
      <title>如何计算空白标记预测的 transformers 损失？</title>
      <link>https://stackoverflow.com/questions/66518375/how-is-transformers-loss-calculated-for-blank-token-predictions</link>
      <description><![CDATA[我目前正在尝试实现一个转换器，但无法理解其损失计算。
我的编码器输入查找 batch_size=1 和 max_sentence_length=8，如下所示：
[[Das, Wetter, ist, gut, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]

我的解码器输入如下所示（德语到英语）：
[[&lt;start&gt;, The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;]]

假设我的转换器预测了这些类概率（仅显示类概率最高的类的单词）：
[[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]

现在我使用以下方法计算损失：
loss = categorical_crossentropy(
[[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]],
[[The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
)

这是计算损失的正确方法吗？我的转换器总是预测下一个单词的空白标记，我认为这是因为我的损失计算有误，在计算损失之前必须对空白标记进行一些处理。]]></description>
      <guid>https://stackoverflow.com/questions/66518375/how-is-transformers-loss-calculated-for-blank-token-predictions</guid>
      <pubDate>Sun, 07 Mar 2021 15:51:16 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络中的旋转等方差？</title>
      <link>https://stackoverflow.com/questions/28201617/rotational-equivariance-in-convolutional-neural-network</link>
      <description><![CDATA[我想知道 CNN 的基本架构是否具有旋转等方差特性？我只知道平移等方差，但不确定旋转等方差。
从我的搜索来看，旋转等方差可以通过旋转输入图像进行训练来实现。我真的需要这样做吗？旋转度数是多少？更具体一点，例如，我有一个可以在横向模式下检测/读取文本的 CNN。如果我将图像旋转 90 度/使其变为纵向，它会给出与原始图像相同的结果/执行相同的操作吗？]]></description>
      <guid>https://stackoverflow.com/questions/28201617/rotational-equivariance-in-convolutional-neural-network</guid>
      <pubDate>Wed, 28 Jan 2015 20:18:54 GMT</pubDate>
    </item>
    </channel>
</rss>