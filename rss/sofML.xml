<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 20 Apr 2024 06:16:45 GMT</lastBuildDate>
    <item>
      <title>对质谱数据库搜索结果进行分类的统计方法</title>
      <link>https://stackoverflow.com/questions/78357067/statistical-methods-for-classifying-mass-spectrometry-database-search-results</link>
      <description><![CDATA[作为一名新手，我想尝试生物信息学方面的一些东西，即用于对质谱数据库搜索结果进行分类的机器学习统计方法。然而，我未能获得用于此目的的公开数据。
我尝试在 NCBI 上搜索某种要使用的数据，但未能在本地计算机上下载和使用任何数据。]]></description>
      <guid>https://stackoverflow.com/questions/78357067/statistical-methods-for-classifying-mass-spectrometry-database-search-results</guid>
      <pubDate>Sat, 20 Apr 2024 05:17:15 GMT</pubDate>
    </item>
    <item>
      <title>使用自动调谐后，如何改进训练/验证图的准确性和损失？ （美国有线电视新闻网）</title>
      <link>https://stackoverflow.com/questions/78357031/how-can-i-improve-the-results-of-this-training-validation-graph-of-accuracy-and</link>
      <description><![CDATA[训练和验证准确率和损失的图表
我运行了我的 CNN 模型（7 个类别，约 33k 张图像和 22 层）100 个 epoch，得到了这个图表。它不是平坦的，而是非常尖锐。下面是我添加到模型中的层。
model = Sequential([
data_augmentation,
layer.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
layer.Conv2D(16, 3, padding=&#39;same&#39;,activation=&#39;relu&#39;),
layer.MaxPooling2D(),
layer.Conv2D(32, 3, padding=&#39;same&#39;,activation=&#39;relu&#39;),
layer.MaxPooling2D(),
layer.Conv2D(64, 3, padding=&#39;same&#39;,activation=&#39;relu&#39;),
layer.MaxPooling2D(),
layer.Dropout(0.05),
layer.Conv2D(128, 3, padding=&#39;same&#39;,activation=&#39;relu&#39;),
layer.MaxPooling2D(),
层。Conv2D（256，3，padding =&#39;same&#39;，activation =&#39;relu&#39;），
层。MaxPooling2D（），
层。Dropout（0.05），
层。Conv2D（512，3，padding =&#39;same&#39;，activation =&#39;relu&#39;），
层。MaxPooling2D（），
层。Flatten（），
层。Dense（512，activation =&#39;relu&#39;），
层。Dropout（0.05），
层。Dense（256，activation =&#39;relu&#39;），
层。Dropout（0.05），
层。Dense（128，activation =&#39;relu&#39;），
层。Dense（num_classes，name =“outputs”）
])

这是我运行的自动调整数据
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

这是使用自动调整之前的图表
添加自动调整之前的图表
虽然这个图表也不是平坦的，但它仍然比第一个好得多。我怎样才能使我的图表平坦化？
我不太确定如何正确地使图表平坦化。我是否只需要运行更多时期？]]></description>
      <guid>https://stackoverflow.com/questions/78357031/how-can-i-improve-the-results-of-this-training-validation-graph-of-accuracy-and</guid>
      <pubDate>Sat, 20 Apr 2024 04:55:34 GMT</pubDate>
    </item>
    <item>
      <title>在weka中重新采样过滤器</title>
      <link>https://stackoverflow.com/questions/78356992/resample-filter-in-weka</link>
      <description><![CDATA[我的数据集中的数据实例数量很少。所以，我尝试了“重新采样” Weka中的过滤器可以增加数据量，从而提高模型性能。样本量百分比设置为200可以吗？因为那时我在交叉验证测试中获得了良好的相关系数。
我想知道将样本大小百分比设置为 200 时，重新采样过滤器是否工作正常。
使用此过滤器后，我的模型会准确预测吗？
由于数据量较少，是否有其他增强方法可以增强模型的性能？]]></description>
      <guid>https://stackoverflow.com/questions/78356992/resample-filter-in-weka</guid>
      <pubDate>Sat, 20 Apr 2024 04:29:50 GMT</pubDate>
    </item>
    <item>
      <title>无法安装gensim模块</title>
      <link>https://stackoverflow.com/questions/78356738/unable-to-install-gensim-module</link>
      <description><![CDATA[我尝试在终端中运行“pip install gensim”命令，但发生了以下情况：
*ld_w2v_模型
运行 build_ext
构建“gensim.models.word2vec_inner”扩展
错误：需要 Microsoft Visual C++ 14.0 或更高版本。使用“Microsoft C++ 构建工具”获取它：https://visualstudio.microsoft。 com/visual-cpp-build-tools/
[输出结束]
注意：此错误源自子进程，并且可能不是 pip 的问题。
错误：gensim 构建轮子失败
构建gen​​sim失败
错误：无法为 gensim 构建轮子，这是安装基于 pyproject.toml 的项目所必需的
*
我该如何解决此错误？
我尝试使用“pip install --use-pep517 gensim==3.8.0”命令并安装了它，但是，在运行我的实际 python 脚本时，模块的错误仍然发生：
回溯（最近一次调用最后一次）：
文件“C:\Users\Anuja Alice Thomas\Documents\CHRIST UNIVERSITY\Trimester 3\Java Planning\Assignments\Sample1\python.py”，第 4 行，位于
导入gensim
文件“C:\Users\Anuja Alice Thomas\AppData\Local\Programs\Python\Python312\Lib\site-packages\gensim_init_.py”，第 5 行，位于
from gensim 导入解析、语料库、matutils、接口、模型、相似性、摘要、utils # noqa:F401
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^
文件“C:\Users\Anuja Alice Thomas\AppData\Local\Programs\Python\Python312\Lib\site-packages\gensim\corpora_init_.py”，第 6 行，位于
from .indexedcorpus import IndexedCorpus # noqa:F401 必须出现在其他类之前
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\Anuja Alice Thomas\AppData\Local\Programs\Python\Python312\Lib\site-packages\gensim\corpora\indexedcorpus.py”，第 15 行，位于
从 gensim 导入接口、utils
文件“C:\Users\Anuja Alice Thomas\AppData\Local\Programs\Python\Python312\Lib\site-packages\gensim\interfaces.py”，第 21 行，位于
从 gensim 导入 utils、matutils
文件“C:\Users\Anuja Alice Thomas\AppData\Local\Programs\Python\Python312\Lib\site-packages\gensim\matutils.py”，第 24 行，位于
从 scipy.linalg.special_matrices 导入 triu
ImportError: 无法从 &#39;scipy.linalg.special_matrices&#39; 导入名称 &#39;triu&#39; (C:\Users\Anuja Alice Thomas\AppData\Local\Programs\Python\Python312\Lib\site-packages\scipy\linalg\special_matrices.py) ]]></description>
      <guid>https://stackoverflow.com/questions/78356738/unable-to-install-gensim-module</guid>
      <pubDate>Sat, 20 Apr 2024 01:37:22 GMT</pubDate>
    </item>
    <item>
      <title>DQN 模型中从未实现的目标状态</title>
      <link>https://stackoverflow.com/questions/78356381/goal-state-never-achieved-in-dqn-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78356381/goal-state-never-achieved-in-dqn-model</guid>
      <pubDate>Fri, 19 Apr 2024 22:22:39 GMT</pubDate>
    </item>
    <item>
      <title>在SVM中，当超参数C增大时，margin会变大吗？</title>
      <link>https://stackoverflow.com/questions/78356095/in-svm-when-hyper-parameter-c-increases-the-margin-will-be-larger</link>
      <description><![CDATA[我目前正在研究SVM，发现关于超参数C和margin之间的关系有两种相反的解释。我知道在支持向量分类器中，C 限制 epsilon 的总和不能大于 C。问题是，我发布的第一张图片说当 C 增加时，边距会更大，但我发布的第二张图片和我发现的其他一些信息说边距会变窄。我很困惑。
在此处输入图像描述
在此处输入图像描述]]></description>
      <guid>https://stackoverflow.com/questions/78356095/in-svm-when-hyper-parameter-c-increases-the-margin-will-be-larger</guid>
      <pubDate>Fri, 19 Apr 2024 20:43:04 GMT</pubDate>
    </item>
    <item>
      <title>使用邮递员使用临时凭证调用 Sagemaker 端点</title>
      <link>https://stackoverflow.com/questions/78356002/calling-sagemaker-endpoint-using-postman-using-temporary-credentials</link>
      <description><![CDATA[我已将 AWS CLI 配置为通过 SSO 使用配置文件。我已引用此文档 AWS 文档
我想使用邮递员调用 Sagemaker 端点。有没有办法做到。我尝试在邮递员中提供我的凭据，但它返回 404 错误。
一些额外的细节 -
Sagemaker 端点使用 VPC。]]></description>
      <guid>https://stackoverflow.com/questions/78356002/calling-sagemaker-endpoint-using-postman-using-temporary-credentials</guid>
      <pubDate>Fri, 19 Apr 2024 20:13:01 GMT</pubDate>
    </item>
    <item>
      <title>Llama2-7b 根据提示生成文本的运行时间较长</title>
      <link>https://stackoverflow.com/questions/78355789/llama2-7b-long-runtime-to-generate-text-from-a-prompt</link>
      <description><![CDATA[我已将 HuggingFace 的 Llama2-7b 模型加载到我的计算机上，以便根据简单提示生成文本。该模型加载速度非常快（大约 2 分钟），但当我想从简单提示生成非常简单的响应时，例如“什么是 LLM？”该模型需要两个多小时才能生成响应。我需要让它运行得更快，但不知道哪里出了问题，无法让它运行得更快。有什么建议吗？
这是我的代码：
class LlamaInference():
“”“”使用 Llama 70b 参数 LLM 的文本生成类。生成数据集。“”“”
def __init__(self, output_file_path):
“”“”初始化 LlamaInference 类和所有变量
关键字参数：
output_file_path -- 数据集生成后输出文件的位置
“”“”“”
self.output_file_path = output_file_path
self.pipeline = None
self.output_list = []
self.output_list_condensed = []

def __set_up(self):
&quot;&quot;&quot; 使用特定模型和修订版本设置 huggingface 管道
&quot;&quot;&quot;
#model = &quot;meta-llama/Llama-2-70b-chat-hf&quot;
#revision = &quot;e6152b720bd3cd67afc66e36d06893a0e1f84b48&quot;
model = &quot;meta-llama/Llama-2-7b-chat-hf&quot;
revision = &quot;08751db2aca9bf2f7f80d2e516117a53d7450235&quot;

self.tokenizer = AutoTokenizer.from_pretrained(model, padding_side=&quot;left&quot;)

self.pipeline = transformers.pipeline(
&quot;text-generation&quot;,
model=model,
tokenizer=self.tokenizer,
torch_dtype=torch.float16,
device_map=&quot;auto&quot;,
revision=revision,
do_sample=True,
return_full_text=False
)
self.pipeline.tokenizer.pad_token_id = self.tokenizer.eos_token_id

def gen_text(self, prompt, **kwargs):
&quot;&quot;&quot; 根据提示生成文本

关键字参数：
prompt -- 我们向 llm 提出的问题
**kwargs -- 要传递给 self.pipeline 的参数
&quot;&quot;&quot;
尝试：
如果 self.pipeline 为 None：
self.__set_up()
如果 &quot;batch_size&quot; 不在 kwargs 中：
kwargs[&quot;batch_size&quot;] = 1

如果 &quot;max_new_tokens&quot;不在 kwargs 中：
kwargs[&quot;max_new_tokens&quot;] = 2048

kwargs.update(
{
&quot;pad_token_id&quot;: self.pipeline.tokenizer.eos_token_id, 
&quot;eos_token_id&quot;: self.pipeline.tokenizer.eos_token_id,
}
)
display(&#39;started pipeline&#39;)
token_outputs = self.tokenizer(prompt)
display(token_outputs)
output = self.pipeline(prompt, **kwargs)
display(&#39;end pipeline&#39;)
return output
except Exception as error:
display(f&#39;__gen_text error: {error}&#39;)

inference = LlamaInference(&#39;test.json&#39;)
result = inference.gen_text(&quot;Hello&quot;, max_new_tokens=2048，batch_size=1，temperature=0.5)
]]></description>
      <guid>https://stackoverflow.com/questions/78355789/llama2-7b-long-runtime-to-generate-text-from-a-prompt</guid>
      <pubDate>Fri, 19 Apr 2024 19:18:13 GMT</pubDate>
    </item>
    <item>
      <title>从 torchensemble 的基本模型中获取嵌入</title>
      <link>https://stackoverflow.com/questions/78355585/getting-embeddings-from-the-base-model-in-torchensemble</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78355585/getting-embeddings-from-the-base-model-in-torchensemble</guid>
      <pubDate>Fri, 19 Apr 2024 18:25:20 GMT</pubDate>
    </item>
    <item>
      <title>带有产品推荐系统的电子商务网络应用程序</title>
      <link>https://stackoverflow.com/questions/78355434/e-commerce-webapp-with-product-recommandation-system</link>
      <description><![CDATA[您好，我有一个带有推荐系统的电子商务 Web 应用程序的 fyp，您能告诉我如何集成制作一个可以在 React JS 中工作的推荐系统，或者如何将其与 React JS 集成
我已经完成了我的 Mern Stack Web 电子商务应用程序，现在我想创建推荐系统并将其与我的 Web 应用程序连接]]></description>
      <guid>https://stackoverflow.com/questions/78355434/e-commerce-webapp-with-product-recommandation-system</guid>
      <pubDate>Fri, 19 Apr 2024 17:50:13 GMT</pubDate>
    </item>
    <item>
      <title>请为我的毕业设计解决机器学习中牙齿分割模型的Valueerror</title>
      <link>https://stackoverflow.com/questions/78350657/solving-valueerror-of-tooth-segmentation-model-in-machine-learning-for-my-gradua</link>
      <description><![CDATA[大家好，我从 此处。
该程序应为用户提供两种选择：

从图像中读取并提取特征：此选项使用 FeatureExtraction 模块从图像中提取 9 个特征（包括图像名称）。
读取预先存在的数据集：此选项读取包含 labels.csv、features.csv 和图像文件的数据集。然后它会询问用户：

执行程序的次数（假设为 5）。
使用 K 折交叉验证分割数据所需的折叠数（假设为 5 折叠，即 k=5）。
测试数据集的大小（假设为 20%）。



模型然后将这些参数传递给classification模块中的分类函数。这就是问题出现的地方：

代码将整个数据集传递给 onlyfiles，其中包含 973 个条目。
然后，它会从 labels.csv（有 778 个条目）中识别 images_name 和 label_color。这代表训练数据集，因为我们之前指定了 20% 的测试集（778 = 973 的 80%）。
以下 for 循环迭代由 k_folds.split(images_name) 生成的分割。此时，我们仍在处理训练数据集，并且当 k=5 时，应该有：

train_index 中有 662 个索引（用于训练数据）。
test_index 中有 156 个索引（用于在训练集中进行验证）。



这是下一个 for 循环中发生错误的位置：
对于 train_index 中的 i：
    current_filename = onlyfiles[i].split(&#39;.&#39;)[0].strip()
    如果 current_filename 在训练数据集中：
        # ...（其余代码）
    别的：
        print(f“警告：在 images_name 中找不到‘{current_filename}’，因为它的索引是 {i}.train。”)


第一行根据 train_index 中的索引 i 检索文件名 (current_filename)。假设 i 为 324，train_index 包含从 156 到 777 的索引（而 test_index 范围从 0 到 155）。
出现此错误的原因是，有时循环会尝试在 images_name 中查找 current_filename，但该文件并不存在。这是因为 images_name 只有 778 个条目（训练数据），其余 195 个条目（测试数据）不包括在内。因此，current_filename 实际上可能属于测试数据集，从而导致错误“101_0032.JPG 不在列表中”。

我尝试对列表进行排序并删除随机播放（在 k_folds.split 中设置 shuffle=False），但错误仍然存​​在。我非常感谢您为解决此问题提供一些帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78350657/solving-valueerror-of-tooth-segmentation-model-in-machine-learning-for-my-gradua</guid>
      <pubDate>Thu, 18 Apr 2024 23:03:02 GMT</pubDate>
    </item>
    <item>
      <title>LMST模型敏感性——初学者抗运气</title>
      <link>https://stackoverflow.com/questions/78349854/lmst-model-sensitivity-beginners-anti-luck</link>
      <description><![CDATA[我一直在尝试使用艾伯塔省电力市场的一些非常基本的数据，并尝试使用时间序列数据的 LMST 模型来尝试预测价格。我确实得到“可能”这是我的模型的结果，而且它似乎确实出现了我们可以预期的一些波动（仅根据我自己的市场经验）。
但是，我正在寻求更好地理解我遇到的一些陷阱。
从 keras.models 导入顺序
从 keras.layers 导入 LSTM
从 keras.layers 导入 Dropout
从 keras.layers 导入密集
将 pandas 导入为 pd
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.model_selection 导入 train_test_split
从 sklearn.metrics 导入mean_absolute_error,mean_squared_error
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns
导入作业库

# 加载数据
# 加载数据

# 加载数据
csv_file_path = &#39;Frankenstein.csv&#39; # 使用您的实际文件路径更新
df = pd.read_csv(csv_file_path)

# 将“日期/时间”转换为日期时间并提取数据集中存在的组件
如果 df.columns 中的“日期/时间”：
    df[&#39;日期&#39;] = pd.to_datetime(df[&#39;日期/时间&#39;])
    df[&#39;年份&#39;] = df[&#39;日期&#39;].dt.year
    df[&#39;月份&#39;] = df[&#39;日期&#39;].dt.月份
    df[&#39;日期&#39;] = df[&#39;日期&#39;].dt.day
    df[&#39;小时&#39;] = df[&#39;日期&#39;].dt.小时
    df.drop([&#39;日期/时间&#39;, &#39;日期&#39;], axis=1, inplace=True)

# 假设“价格”是目标变量
features = df.drop([&#39;价格&#39;], axis=1)
目标 = df[&#39;价格&#39;]

# 标准化特征和目标
缩放器特征 = MinMaxScaler()
features_scaled = scaler_features.fit_transform(features)
缩放器目标 = MinMaxScaler()
target_scaled = scaler_target.fit_transform(target.values.reshape(-1, 1))

# 创建序列函数
def create_sequences（特征，目标，time_steps = 100）：
    X、y = []、[]
    对于范围内的 i(len(features) - time_steps)：
        X.append(特征[i:(i + time_steps)])
        y.append(目标[i + time_steps])
    返回 np.array(X), np.array(y)

# 使用整个数据集创建序列
X, y = create_sequences(features_scaled, target_scaled.flatten())

# 模型配置
input_shape = (X.shape[1], X.shape[2]) # (time_steps, num_features)

# 定义LSTM模型
模型=顺序（[
    LSTM（单位= 100，return_sequences = True，input_shape = input_shape），
    辍学（0.1），
    LSTM（单位=100），
    辍学（0.1），
    密集（单位=100，激活=&#39;elu&#39;），
    Dense(1) # 预测单个值
]）

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)

# 在整个数据集上训练模型
历史= model.fit（X，y，纪元= 150，batch_size = 20，validation_split = 0.1）

# 情节训练&amp;验证损失值
plt.figure(figsize=(10, 6))
plt.plot(history.history[&#39;loss&#39;], label=&#39;火车&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;验证&#39;)
plt.title(&#39;模型损失&#39;)
plt.ylabel(&#39;损失&#39;)
plt.xlabel(&#39;纪元&#39;)
plt.legend(loc=&#39;右上&#39;)
plt.show()

# 保存LSTM模型
model_save_path = &#39;trained_lstm_model.h5&#39;
model.save(model_save_path)
print(f&quot;模型已保存到 {model_save_path}&quot;)
joblib.dump(scaler_features, &#39;scaler_features.pkl&#39;)
joblib.dump(scaler_target, &#39;scaler_target.pkl&#39;)

有人可以给绝对的初学者一些建议吗？主要是为了更好地理解我应该如何设置它。我有一个每小时的数据集，是过去三年的历史生成和交换。我正在寻找方法让我的模型对供应与价格的变化更具反应性。]]></description>
      <guid>https://stackoverflow.com/questions/78349854/lmst-model-sensitivity-beginners-anti-luck</guid>
      <pubDate>Thu, 18 Apr 2024 19:18:18 GMT</pubDate>
    </item>
    <item>
      <title>流式 LightGBM 数据集构建在训练中冻结</title>
      <link>https://stackoverflow.com/questions/78344537/streaming-lightgbm-dataset-construction-freezes-on-training</link>
      <description><![CDATA[我一直在尝试使用参考数据集（称为 ref_dataset）以流方式在 Python 中构建 LightGBM 数据集。我不确定它是如何完成的，它涉及调用 Dataset 类中看似非公共的方法。
我已经尝试过：
label_column = “标签”
权重列=“权重”
ref_dataset = lightgbm.Dataset(
   Sample_df.drop(列=[标签列，权重列])
   标签=sample_df[标签_列],
   权重=sample_df[权重列],
   参数=配置，
   **（ref_dataset_kwargs 或 {}），
）
ref_dataset.construct()
temp_dataset = lightgbm.Dataset（无，参考= ref_dataset，params = ref_dataset.get_params（））
# train_filenames_and_part_infos 只是一个元组列表[filename,part_info_dict]
估计行数=总和（
    part_info[“num_rows”] for _，train_filenames_and_part_infos 中的part_info
）
temp_dataset._init_from_ref_dataset(estimated_num_rows, ref_dataset._handle)

权重列表 = []
标签列表=[]
# 这个循环实际上不是我的代码，它更复杂，但基本上是它的作用
对于文件名，train_filenames_and_part_infos 中的 _：
    tbl: pyarrow.Table = load_from_file(文件名)
    标签 = tbl[label_column].to_pandas().to_numpy()
    权重 = tbl[weight_column].to_pandas().to_numpy()

    labels_list.append(标签)
    weights_list.append(权重)
    tbl = tbl.drop_columns([label_column, Weight_column])
    np_array: np.ndarray = tbl.to_pandas().to_numpy()
    如果 temp_dataset._start_row + np_array.shape[0] &gt; temp_dataset.num_data():
        raise RuntimeError(“数据集太小，无法容纳数据”)
    temp_dataset._push_rows(np_array)

all_weights = np.concatenate(weights_list)
all_labels = np.concatenate(labels_list)
实际长度 = all_weights.shape[0]
# 不幸的是，由于各种原因，这个估计并不准确
extra_zeros_features = np.zeros(
     （估计行数 - 实际长度，temp_dataset.num_feature()），dtype=np.float32
）
temp_dataset._push_rows(extra_zeros_features)
_LIB.LGBM_DatasetMarkFinished(temp_dataset._handle)
extra_zeros = np.zeros(估计行数 - 实际长度, dtype=np.float32)
temp_dataset.set_weight(np.concatenate([all_weights, extra_zeros]))
temp_dataset.set_label(np.concatenate([all_labels, extra_zeros]))

lightgbm.train(
    params=config, # 包含分布式投票并行训练的网络参数
    train_set=temp_dataset，
    num_boost_round=100,
    valid_sets=valid_sets, # 在其他地方初始化
    valid_names=valid_names, # 在其他地方初始化
    init_model=starting_model, # 不是很有必要
    **lightgbm_train_kwargs, # 空
）

不幸的是，当我运行这段代码时，我得到了这个控制台输出（有些行可能是无序的，因为我实际上是在分布式上运行它，并且日志是聚合的；我已经做了一些简单的编辑删除干扰线）：
[LightGBM] [Info] 总 bin 137618
[LightGBM] [Info] 尝试绑定端口 50627...
[LightGBM] [Info] 绑定端口50627成功
[LightGBM] [信息] 聆听...
[LightGBM] [Info] 训练集中的数据点数量：3934363，使用的特征数量：1382
[LightGBM] [信息] 连接到等级 0
[LightGBM] [信息] 连接到排名 1
[LightGBM] [信息] 连接到等级 2
[LightGBM] [信息] 连接到等级 3
[LightGBM] [信息] 已连接至等级 4
[LightGBM] [信息] 已连接至排名 5
[LightGBM] [信息] 已连接至排名 6
[LightGBM] [信息] 已连接至排名 8
[LightGBM] [Info] 本地排名：7，机器总数：9
[LightGBM] [Info] 自动选择col-wise多线程，测试开销为5.318313秒。
[LightGBM] [Info] 从分数-0.000000开始训练

然后它就坐在那里，CPU 和网络都处于空闲状态。我没有看到它在几个小时内取得任何进展。我已经检查了所有的排名，是不是我做错了什么？我还如何使用给定的样本进行构建？
更多信息：
检查空闲 Python 进程的堆栈跟踪显示代码卡在：
更新（lightgbm/basic.py:3891）
火车（lightgbm/engine.py:276）
...我的代码...

对于我正在使用的 LightGBM 版本 (4.3.0)，这对应于代码：
_safe_call(_LIB.LGBM_BoosterUpdateOneIter(
                self._handle,
                ctypes.byref(is_finished)))

另一个更新：
不同工人的垃圾箱数量似乎不同；有些有 137608、137612、137616。这是一个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78344537/streaming-lightgbm-dataset-construction-freezes-on-training</guid>
      <pubDate>Thu, 18 Apr 2024 02:25:39 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Vertex AI 中部署自定义容器作为端点</title>
      <link>https://stackoverflow.com/questions/77266349/how-to-deploy-custom-container-in-vertex-ai-as-endpoint</link>
      <description><![CDATA[我正在尝试在 vertex ai 中部署自定义容器作为端点（REST URL 或 API），我能够成功构建 docker 映像，但无法将模型部署为端点，从日志中我也无法了解错误是什么。
下面是我的 Predict.py 、 dockerfile 和部署脚本
将 numpy 导入为 np

def 预测（数据）：
  # 预测函数示例
  打印（数据）
  #result = np.sum(data, axis=1) # 示例：沿轴 1 求和
  结果 = [3,4,5,6]
  results_array = np.array(结果)
  print ({“预测”: results_array.tolist()})
  return ({“预测”: results_array.tolist()})
  预测([3,4,6])

docker 文件
&lt;前&gt;&lt;代码&gt;来自 python:3.10
工作目录/代码

复制 要求.txt 要求.txt
复制模型.pkl 模型.pkl
运行 pip install --升级 pip
运行 pip --版本
运行 pip install -rrequirements.txt
复制 。 。
CMD [“python3”，“predict.py”]

部署脚本
导入操作系统
将 google.cloud.aiplatform 导入为 aiplatform

# 设置您的 GCP 项目 ID、位置和模型名称
项目=“项目ID”
位置=“us-central1”
model_name =“testing_3”；

# 初始化API客户端
aiplatform.init（项目=项目，位置=位置）

# 定义容器镜像URI
container_image_uri = “图像 URI”

# 创建自定义容器预测模型
模型 = aiplatform.Model.upload(
显示名称=模型名称，
serving_container_image_uri=container_image_uri，
 ）

print (“现在部署模型”)
尝试：
  # 部署模型
  端点 = model.deploy(machine_type=&quot;n1-standard-4&quot;)
  打印（端点）
除了异常 e：
  print(f“部署模型时出错：{e}”)

我还尝试运行已部署的 docker 映像，并且其运行没有任何错误，只是我无法部署端点
有人可以帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77266349/how-to-deploy-custom-container-in-vertex-ai-as-endpoint</guid>
      <pubDate>Tue, 10 Oct 2023 13:43:48 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenCV 和 Mediapipe 实现逼真的唇色变化？</title>
      <link>https://stackoverflow.com/questions/75793658/how-to-achieve-realistic-lip-color-change-using-opencv-and-mediapipe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/75793658/how-to-achieve-realistic-lip-color-change-using-opencv-and-mediapipe</guid>
      <pubDate>Mon, 20 Mar 2023 17:50:46 GMT</pubDate>
    </item>
    </channel>
</rss>