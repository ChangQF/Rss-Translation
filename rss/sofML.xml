<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 14 May 2024 12:27:04 GMT</lastBuildDate>
    <item>
      <title>Azure 助理 API 文件搜索和响应时间问题</title>
      <link>https://stackoverflow.com/questions/78477560/issues-with-azure-assistant-api-file-search-and-response-times</link>
      <description><![CDATA[我目前正在使用 Azure Assistant API，并面临一些挑战。具体来说，当我尝试添加文件并使用函数调用时，文件搜索功能无法按预期工作。尽管提供了正确的说明，API 经常会回复“我没有该信息”。即使文件中提供了必要的数据。
我已使用 GPT-3.5 Turbo、GPT-4 和 GPT-4 Turbo 对此进行了测试，但该问题在所有型号上仍然存在。此外，与标准 OpenAI Assistant API 相比，响应时间明显更长，在文件搜索、函数调用和响应时间方面表现良好。
我需要有关此问题的进一步支持。]]></description>
      <guid>https://stackoverflow.com/questions/78477560/issues-with-azure-assistant-api-file-search-and-response-times</guid>
      <pubDate>Tue, 14 May 2024 10:52:10 GMT</pubDate>
    </item>
    <item>
      <title>如何编写具有多个输入的神经网络[关闭]</title>
      <link>https://stackoverflow.com/questions/78477416/how-to-write-a-neural-network-with-multiple-inputs</link>
      <description><![CDATA[这里我想问一下如何编写具有多个输入的神经网络。您的方法、现代解决方案和建议。
我认为制作一个只能获得一个输入并且可以多次使用它的模型是一个很好的决定。
&lt;前&gt;&lt;代码&gt;# 输入1，输入2，输入3 = ...，...，...
模型=顺序（）

输出1 = 模型(输入1)
输出2 = 模型(输入2)
输出3 = 模型(输入3)`

您认为一致性如何？这样做真的很重要吗？或者你可以这样做：
输出1 = 模型（输入3） 输出2 = 模型（输入1） 输出3 = 模型（输入2）
也许你的做法不同，所以你可以在这里告诉...
我只是尝试这样做，所以听到更多信息很有趣]]></description>
      <guid>https://stackoverflow.com/questions/78477416/how-to-write-a-neural-network-with-multiple-inputs</guid>
      <pubDate>Tue, 14 May 2024 10:24:18 GMT</pubDate>
    </item>
    <item>
      <title>亮度增强破坏了图像</title>
      <link>https://stackoverflow.com/questions/78477344/albumentations-intensity-augmentations-disrupt-the-image</link>
      <description><![CDATA[我使用经过预处理的 z 分数标准化列表作为数据集的来源。
这是一张由专辑增强的图像拼贴画：
在此处输入图片描述
这是我的撰写：
增强 = A.Compose([
    A.Horizo​​ntalFlip(),
    A.RandomBrightnessContrast（brightness_limit=（-0.0001，0.0001），contrast_limit=（-0.01，0.01）），
    A.CoarseDropout(8, 0.1, 0.1),
    A.旋转(限制=15),
    A.Affine(剪切=(-2, 2), 尺度=(0.95, 1.05)),
&gt;！ ToTensorV2()
]）

在 50% 的图像上，即使参数非常小，也应用了 RandBrightnessContrast，图像的整个分布被压缩为 [0, 1]（如 z 分数归一化图像所预期的那样，从 ~-2,~2 ）。
有办法解决这个问题吗？
也许我应该在这些之后执行 z 分数标准化，但我最初的意图是将所有确定性步骤（调整大小、标准化等）与增强步骤分开以提高效率。]]></description>
      <guid>https://stackoverflow.com/questions/78477344/albumentations-intensity-augmentations-disrupt-the-image</guid>
      <pubDate>Tue, 14 May 2024 10:11:54 GMT</pubDate>
    </item>
    <item>
      <title>为什么应用 Standard Scaler 时我的行数会减少？</title>
      <link>https://stackoverflow.com/questions/78477264/why-is-my-number-of-rows-getting-reduced-when-applying-standard-scaler</link>
      <description><![CDATA[我正在应用 StandardScaler() 来标准化我的数据集。但生成的标准化数据集的行数比原始数据集少。为什么会发生这种情况？我想在标准化后保留原始行数。
应用 StandardScaler() 函数后，我希望获得与标准化值相同的行数。]]></description>
      <guid>https://stackoverflow.com/questions/78477264/why-is-my-number-of-rows-getting-reduced-when-applying-standard-scaler</guid>
      <pubDate>Tue, 14 May 2024 09:57:32 GMT</pubDate>
    </item>
    <item>
      <title>掩模注释的质量保证</title>
      <link>https://stackoverflow.com/questions/78477050/quality-assurance-for-annotation-of-masks</link>
      <description><![CDATA[我有来自多人的注释作为相同图像的掩模。
如何组合这些蒙版以获得更“真实”的效果？注解？有某种标准吗？
我还考虑过使用 Cohen&#39;s kappa、Krippendorf&#39;s alpha、F1 等，但我刚刚意识到我不能使用它们来保证质量，因为它们更多的是用于标记而不是分割中的注释。
你知道我也可以用什么来保证质量吗？]]></description>
      <guid>https://stackoverflow.com/questions/78477050/quality-assurance-for-annotation-of-masks</guid>
      <pubDate>Tue, 14 May 2024 09:22:21 GMT</pubDate>
    </item>
    <item>
      <title>VGG19比VGG16好吗</title>
      <link>https://stackoverflow.com/questions/78477046/is-vgg19-better-than-vgg16</link>
      <description><![CDATA[根据我的研究，这就是我得到的
VGG16：

由 16 层组成（13 个卷积层和 3 个全连接层）。
使用 3x3 卷积层，层层堆叠，深度不断增加。
通过使用最大池减小卷大小。
连接到三个全连接层，然后连接到 softmax 分类器1。

VGG19：

与 VGG16 类似，但有 19 层（16 个卷积层和 3 个全连接层）。

VGG19有更多层，这是否意味着它更好？]]></description>
      <guid>https://stackoverflow.com/questions/78477046/is-vgg19-better-than-vgg16</guid>
      <pubDate>Tue, 14 May 2024 09:21:31 GMT</pubDate>
    </item>
    <item>
      <title>如何修改 Pytorch ImageFolder 分配标签的方式</title>
      <link>https://stackoverflow.com/questions/78476619/how-to-modify-how-pytorch-imagefolder-assigns-labels</link>
      <description><![CDATA[我有一个数据集存储在这样的目录中：

我希望属于同一图像目录中同一特征目录的所有图像都获得相同的标签。分配给图像的标签本质上应该基于“图像#”和“图像#”的组合。和“feature_#”目录。如果不同图像目录中的图像共享相同的功能目录名称，我不希望为它们分配相同的标签。有没有一种简单的方法来修改 ImageFolder 分配标签的方式？]]></description>
      <guid>https://stackoverflow.com/questions/78476619/how-to-modify-how-pytorch-imagefolder-assigns-labels</guid>
      <pubDate>Tue, 14 May 2024 08:04:53 GMT</pubDate>
    </item>
    <item>
      <title>通过系数和截距进行线性回归---</title>
      <link>https://stackoverflow.com/questions/78476584/linear-regression-by-coefficieents-and-intercept</link>
      <description><![CDATA[我想通过系数和截距绘制线性回归。我在 Jupiter 笔记本上看到了它的代码。但我无法理解其中的特殊部分。这是代码：
plt.scatter(train.ENGINESIZE, train.CO2EMISSION, color=&#39;blue&#39;)
plt.plot (train_x, regr.coef_[0][0]*train_x +regr.intercept_[0], &#39;-r&#39;)

为什么它不像 regr.coef_*train_x +regr.intercept_]]></description>
      <guid>https://stackoverflow.com/questions/78476584/linear-regression-by-coefficieents-and-intercept</guid>
      <pubDate>Tue, 14 May 2024 07:57:40 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：CUDA 错误：设备端断言已触发</title>
      <link>https://stackoverflow.com/questions/78475975/runtimeerror-cuda-error-device-side-assert-triggered</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78475975/runtimeerror-cuda-error-device-side-assert-triggered</guid>
      <pubDate>Tue, 14 May 2024 05:51:57 GMT</pubDate>
    </item>
    <item>
      <title>OSError：[model] 似乎没有名为 config.json 的文件</title>
      <link>https://stackoverflow.com/questions/78474448/oserror-model-does-not-appear-to-have-a-file-named-config-json</link>
      <description><![CDATA[我想加载一个拥抱模型。 我要加载的模型大约有 15 万次下载所以我不认为模型本身有什么问题。
使用下面的两个加载代码我得到相同的错误：
从变压器导入 AutoModel
AutoModel.from_pretrained(“laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup”)

还有
从 Transformers 导入 CLIPProcessor、CLIPModel
model_id =“laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup”
处理器 = CLIPProcessor.from_pretrained(model_id)
模型 = CLIPModel.from_pretrained(model_id)

两者我都得到：
OSError：laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup 似乎没有名为 preprocessor_config.json 的文件。查看“https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup/main”以获取可用文件。

任何加载模型的帮助将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78474448/oserror-model-does-not-appear-to-have-a-file-named-config-json</guid>
      <pubDate>Mon, 13 May 2024 19:38:47 GMT</pubDate>
    </item>
    <item>
      <title>在深度训练/验证循环期间使用分层 k 折叠时出现越界错误</title>
      <link>https://stackoverflow.com/questions/78473057/out-of-bounds-error-when-using-stratified-k-fold-during-deep-train-validation-lo</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78473057/out-of-bounds-error-when-using-stratified-k-fold-during-deep-train-validation-lo</guid>
      <pubDate>Mon, 13 May 2024 14:43:39 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv5 标记对象的正确方法</title>
      <link>https://stackoverflow.com/questions/78471070/correct-way-to-tag-objects-with-yolov5</link>
      <description><![CDATA[我需要标记一系列图像以用于织物上的缝纫检测。我使用 YOLOv5 算法。
我遇到的问题是，我不清楚标记这些缝纫的最佳方式应该是什么。
下图显示了织物中的缝线。

正如您在图片中看到的，缝线总是会占据布料的整个宽度。最初，我曾想过对缝纫的几个部分/部分进行标记（检测到的缝纫数量并不重要，对我来说真正重要的是它检测到至少有一个缝纫）。下图展示了这个想法：

但是，我不清楚这是否是正确的方法（而是最佳方法），或者应该创建一个完全包围缝纫的单个标签。
另一方面，根据文档中给出的标签提示，标签应该恰好包围要检测的对象，在对象和标签的边界框之间留出尽可能小的空间。
&lt;块引用&gt;
标签准确性。标签必须紧密包围每个对象。没有空间
应该存在于对象与其边界框之间。没有物体
应该缺少标签。

获得最佳训练结果的提示
在这种特殊情况下，考虑到缝线总是以非常相似的方式出现（它们总是具有水平方向），这些标签将非常薄（高度很小），因此不清楚我认为该算法将能够检测到它们。以我的拙见，我认为稍微增加标签的高度将使算法更有效地检测缝纫，因为通过这些缝纫连接的织物可能具有相同的颜色。 （第二张图片显示了我正在谈论的想法）。
如果您能帮助我并告诉我进行此标记的最佳方法，我将不胜感激。
提前非常感谢您。]]></description>
      <guid>https://stackoverflow.com/questions/78471070/correct-way-to-tag-objects-with-yolov5</guid>
      <pubDate>Mon, 13 May 2024 08:48:35 GMT</pubDate>
    </item>
    <item>
      <title>不平衡学习管道的哪些部分应用于测试集？</title>
      <link>https://stackoverflow.com/questions/78462616/which-parts-of-the-imbalanced-learn-pipeline-are-applied-to-the-test-set</link>
      <description><![CDATA[我创建了一个由 RobustScaler、SMOTE-NC、 组成的 imbalanced-learn Pipeline随机欠采样和随机森林分类器。
RandomSearchCV 用于选择最佳的超参数。
我想在我的测试集上测试最佳估计器。
cv = RepeatedStratifiedKFold(n_splits=5,
                             n_重复=10，
                             随机状态=42
）

缩放器 = RobustScaler(quantile_range=(25.0, 75.0))

smote = SMOTENC（
    分类特征=分类特征，
    采样策略=0.35，
    随机状态=42
）
rus = RandomUnderSampler(sampling_strategy=0.35, random_state=42)

分类器 = RandomForestClassifier(random_state=42)

管道=不平衡_make_pipeline（缩放器，smote，rus，分类器）

random_search = RandomizedSearchCV(
    管道，
    param_distributions=参数，
    评分=scoring_metric，
    简历=简历，
    n_iter=10,
    随机状态=42，
    n_工作=-1，
）

best_model = random_search.fit(X_train, y_train).best_estimator_

y_pred = best_model.predict(X_test)

据我了解，只有缩放（通过X_train获得的设置）和分类器应该应用于测试集。 SMOTE 和 RandomUndersampling 不应应用于 X_test。
这是由 imbalanced-learn 管道保证的还是我必须考虑其他事情？]]></description>
      <guid>https://stackoverflow.com/questions/78462616/which-parts-of-the-imbalanced-learn-pipeline-are-applied-to-the-test-set</guid>
      <pubDate>Fri, 10 May 2024 21:28:24 GMT</pubDate>
    </item>
    <item>
      <title>Kohonen 神经网络用于确定点平面的象限</title>
      <link>https://stackoverflow.com/questions/78456978/kohonen-neural-network-for-determining-the-quadrant-of-a-point-plane</link>
      <description><![CDATA[我需要实现 Kohonen 神经网络（没有老师）来确定点的平面（从 1 到 8）的象限。坐标 [x, y, z] 的向量被馈送到神经网络的输入。在输出中，我们得到一个向量，通过该向量可以确定该点属于哪个象限。
这是我训练神经网络的代码：
将 numpy 导入为 np
将 numpy.typing 导入为 npt
将 matplotlib.pyplot 导入为 plt


def 生成数据集（
        大小：整数，
        高：浮动= 10.0
）-&gt;列表[元组[npt.NDArray[npt.NDArray[浮点]], npt.NDArray[npt.NDArray[浮点]]]]:
    数据集 = []

    象限乘数 = {
        1: (1, 1, 1),
        2: (-1, 1, 1),
        3: (-1, -1, 1),
        4: (1, -1, 1),
        5: (1, 1, -1),
        6: (-1, 1, -1),
        7：（-1，-1，-1），
        8：（1，-1，-1）
    }

    对于范围 (1, 8 + 1) 中的象限：
        数据集 += [
            （
                np.array([[x, y, z]]) *quadrants_multipliers[象限],
                np.eye(8)[象限 - 1].reshape(1, 8)
            ) 对于 zip 中的 x、y、z(
                np.random.uniform（低= 0.0，高=高，大小=大小），
                np.random.uniform（低= 0.0，高=高，大小=大小），
                np.random.uniform(低=0.0，高=高，大小=大小)
            ）
        ]

    返回数据集

定义火车（
        数据集：list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]],
        学习率：浮动，
        纪元：int
）-&gt; npt.NDArray[npt.NDArray[浮点]]：

    W = np.random.randn(3, 8)

    对于范围（1，纪元 + 1）中的纪元：
        random_indexes = np.random.choice(len(数据集)，size=len(数据集)，replace=False)
        对于 random_indexes 中的索引：
            x = 数据集[索引][0] # [[x, y, z]]
            y = 数据集[索引][1]

            x_normalized = x / np.sqrt(np.sum(x ** 2))

            z = np.dot(x_归一化，W)

            W[0][z.argmax()] += 学习率 * (x[0][0] - W[0][z.argmax()])
            W[1][z.argmax()] += 学习率 * (x[0][1] - W[1][z.argmax()])
            W[2][z.argmax()] += 学习率 * (x[0][2] - W[2][z.argmax()])

        学习率 = 学习率 / 时期

    返回W

def calc_accuracy(
        数据集：list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]],
        W: npt.NDArray[npt.NDArray[float]]) -&gt;漂浮：
    正确 = 0
    对于数据集中的 x、y：
        z = np.dot(x, W)
        如果 z.argmax() == y.argmax():
            正确+=1
    返回（正确/len（数据集））* 100


def draw_accuracy_epoch_plots(
        数据集：list[list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]]],
        epoch_number：int
）-&gt;没有任何：
    对于索引，枚举中的数据集（数据集）：
        准确度列表 = []
        准确度列表_2 = []
        对于范围（1，epoch_number + 1）中的纪元：
            W = 训练（数据集，learning_rate=0.7，epochs=epochs）
            precision_list.append(calc_accuracy(数据集，W))
            precision_list_2.append(calc_accuracy(generate_dataset(15), W))

        plt.subplot(3, 2, (2 * 索引) + 1)
        plt.plot(accuracy_list)

        plt.subplot(3, 2, (2 * 索引) + 2)
        plt.plot(accuracy_list_2)


def main() -&gt;;没有任何：
    draw_accuracy_epoch_plots([generate_dataset(250),generate_dataset(500),generate_dataset(1000)],200)

    plt.tight_layout()
    plt.show()


如果 __name__ == “__main__”：
    主要的（）


在代码中，我计算了不同时期神经网络预测的准确性。结果，我得到的准确度为 0 到 45%，这不适合我。同时，神经网络经常产生 0% 的匹配。
如果更改学习率和历元没有帮助，我该如何解决这个问题？我可以添加图层吗（我还没有找到这个问题的答案）？或者也许我错误地构建了学习算法？
注意：
我昨天创建了准确度图表 - 花了大约半小时，也许更多。 Y 轴上可以看到精度，X 轴上可以看到纪元。左边是不同训练选择的图表（此类样本中的点数在图表上方签名），在右边的图表中，我们使用从左边图表中的样本获得的权重进行新的测试样本200点。在我看来，情况不应该如此，日程安排本质上应该增加，但事实并非如此。
]]></description>
      <guid>https://stackoverflow.com/questions/78456978/kohonen-neural-network-for-determining-the-quadrant-of-a-point-plane</guid>
      <pubDate>Thu, 09 May 2024 21:05:03 GMT</pubDate>
    </item>
    <item>
      <title>深度学习Nan丢失原因</title>
      <link>https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons</link>
      <description><![CDATA[什么会导致卷积神经网络发散？
具体：
我正在使用 Tensorflow 的 iris_training 模型和我自己的一些数据，并不断得到
&lt;块引用&gt;
错误：tensorflow：模型发散，损失 = NaN。
回溯...
tensorflow.contrib.learn.python.learn.monitors.NanLossDuringTrainingError：训练期间的 NaN 损失。

回溯源自行：
 tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                        隐藏单位=[300, 300, 300],
                                        #optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.001, l1_regularization_strength=0.00001),
                                        n_classes=11,
                                        model_dir=&quot;/tmp/iris_model&quot;)

我尝试过调整优化器，使用零学习率，并且不使用优化器。]]></description>
      <guid>https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons</guid>
      <pubDate>Fri, 14 Oct 2016 19:07:18 GMT</pubDate>
    </item>
    </channel>
</rss>