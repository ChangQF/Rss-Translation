<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 27 Jun 2024 01:04:40 GMT</lastBuildDate>
    <item>
      <title>针对不同物种的分类训练各种模型的建议</title>
      <link>https://stackoverflow.com/questions/78674961/suggestion-for-training-various-models-for-classification-of-different-species</link>
      <description><![CDATA[致力于开发各种有害藻类与海水中发现的藻类/其他物体的分类网络。我们已经为一些有害藻类与海洋开发了二元网络。这些有害藻类有 5000 多个类别。我们的一些客户需要 5 种有害藻类分类，而其他客户可能需要更多。为这个问题开发机器学习解决方案的最佳方法是什么？任何建议都值得赞赏。
方法 1：我们可以训练多类模型以满足客户需求。假设客户对 A、B 和 C 有害藻类与其他（海洋 + 剩余有害藻类）感兴趣。但这需要大量数据和针对这些物种的每种组合进行训练。有没有更好的方法？]]></description>
      <guid>https://stackoverflow.com/questions/78674961/suggestion-for-training-various-models-for-classification-of-different-species</guid>
      <pubDate>Wed, 26 Jun 2024 22:55:16 GMT</pubDate>
    </item>
    <item>
      <title>腌制机器学习模型无法做出预测</title>
      <link>https://stackoverflow.com/questions/78674912/pickled-machine-learning-model-doesnt-make-predictions</link>
      <description><![CDATA[我创建了一个 MLP 模型并用 Pickle 保存了它，然后使用以下代码加载该模型并将其应用于新数据：
mlp_probs = pickle_model.predict_proba(X_test)

这很完美。但是当我尝试获取类别预测时：
mlp_predictions = pickle_model.predict(X_test)

所有预测均为 0。换句话说，没有正类。
在我将其腌制之前，该模型运行良好，因此我猜测在腌制过程中未保存阈值。
我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78674912/pickled-machine-learning-model-doesnt-make-predictions</guid>
      <pubDate>Wed, 26 Jun 2024 22:36:57 GMT</pubDate>
    </item>
    <item>
      <title>我无法使用 FastApi 运行使用 Tensorflow 保存的模型</title>
      <link>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</guid>
      <pubDate>Wed, 26 Jun 2024 19:25:44 GMT</pubDate>
    </item>
    <item>
      <title>如何避免在超过 300 万个实例的数据库中商标排名算法获得最高分？</title>
      <link>https://stackoverflow.com/questions/78673987/how-do-i-avoid-maximum-scores-from-a-trademark-ranking-algorithm-on-a-database-o</link>
      <description><![CDATA[我有一个包含三百万个商标的数据库，最终目标是对数据库进行排序并得到 N 个最相似的商标。
我目前解决这个问题的方法是，我有一个包含大约 14,000 个实例的数据集，这些实例是成对的商标 - 7,000 个保证相似并标记为 1，而其余的则是从我的数据库中随机挑选的，因此不相似并标记为 0。然后我所做的是通过在语法、语音和语义上比较这两个商标来生成特征。
然后目标是建立一个模型来生成两个商标之间相似度的分数（介于 0 和 1 之间），然后将其应用于数据库中的每个商标并选出最高分数。
然后我建立了几个模型来预测两个商标是否相似（我认为这个问题很简单，所以我使用了逻辑回归、决策树、随机森林、knn）。我最终生成了一个运行良好且 MSE 较低的模型。
但是，当我尝试将该模型应用于数据库中的每个商标时，问题出现了。由于商标数量太多（300 万），我最终得到了一个很大的商标子集，这些商标的得分为 1（这意味着它们与我的目标商标极为相似）。因此，不可能对这么多被评为最相似的商标进行排名。
如何构建一个不太可能给出最高分的模型？我应该使用什么评估指标来确定这种模型的有效性？
我附上了只有 10,000 个商标的试验截图。如您所见，有许多商标获得了最高的“最终分数”，因此无法进行排名。当将其应用于整个数据库时，将会有更多这样的商标。
在此处输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/78673987/how-do-i-avoid-maximum-scores-from-a-trademark-ranking-algorithm-on-a-database-o</guid>
      <pubDate>Wed, 26 Jun 2024 17:55:47 GMT</pubDate>
    </item>
    <item>
      <title>再生基因数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/78673886/regenerative-genes-datasets</link>
      <description><![CDATA[我是计算机专业网络安全专业的学生。我正在做的最后一年项目是：
DNA（脱氧核酸）由基因组成。基因有助于通过转录和翻译过程产生氨基酸，进而产生蛋白质。蛋白质进行各种活动以保持我们的健康并使每个细胞独一无二。某些疾病也是由某些基因引起的，例如镰状细胞性贫血。该项目将使用机器学习算法来研究哪些特定基因与再生有关。将研究共表达基因的概念，以了解哪种蛋白质触发了再生基因。某些蛋白质的合成并将其注射到某些患者体内可能有助于加速再生。然而，该项目的进一步应用可能是抑制产生癌细胞的基因。
我并没有真正开始这个项目，我可以随时更改范围
我可以在哪里找到此研究的此特定数据集的数据集？
我的讲师告诉我要做特征提取。
我尝试在 GitHub 和其他网站中查找数据集，但我没有找到可以处理的特征]]></description>
      <guid>https://stackoverflow.com/questions/78673886/regenerative-genes-datasets</guid>
      <pubDate>Wed, 26 Jun 2024 17:33:19 GMT</pubDate>
    </item>
    <item>
      <title>Android 图像分割应用程序在不插入 Android Studio 时运行速度更快</title>
      <link>https://stackoverflow.com/questions/78673441/android-image-segmentation-app-runs-faster-when-not-plugged-into-android-studio</link>
      <description><![CDATA[因此，我创建了一个简单的应用程序，它运行一个 torchscript 模型并对大约 100 张图像进行推理。当我将手机插入我的机器并通过 android studio 运行它时，每张图像需要 300 毫秒。当拔下电源并运行应用程序时，每张图像需要 180 毫秒。这有意义吗？或者有人知道为什么会发生这种情况吗？
经过多次测试以确认。
图像作为资产保存在应用程序中。]]></description>
      <guid>https://stackoverflow.com/questions/78673441/android-image-segmentation-app-runs-faster-when-not-plugged-into-android-studio</guid>
      <pubDate>Wed, 26 Jun 2024 15:49:38 GMT</pubDate>
    </item>
    <item>
      <title>如何为 RNN 编码多个文本列</title>
      <link>https://stackoverflow.com/questions/78673211/how-to-encode-multiple-text-column-for-rnn</link>
      <description><![CDATA[我有多个文本特征，需要对其进行编码才能在 RNN 模型中使用。建议通过 TextVectorization 来处理训练文本
 A B C D Target
0 一些文本 A 一些文本 B 一些文本 C 一些文本 D 0
1 一些文本 A 一些文本 B 一些文本 C 一些文本 D 1
2 一些文本 A 一些文本 B 一些文本 C 一些文本 D 0
3 一些文本 A 一些文本 B 一些文本 C 一些文本 D 1
4 一些文本 A 一些文本 B 一些文本 C 一些文本 D 1

目前 train_dataset 包含一列和一个 Target 标签
VOCAB_SIZE = 1000
encoder = tf.keras.layers.TextVectorization(
max_tokens=VOCAB_SIZE)
encoder.adapt(train_dataset.map(lambda text, label: text))
]]></description>
      <guid>https://stackoverflow.com/questions/78673211/how-to-encode-multiple-text-column-for-rnn</guid>
      <pubDate>Wed, 26 Jun 2024 14:58:57 GMT</pubDate>
    </item>
    <item>
      <title>选择 Hyperopt 管道参数的问题</title>
      <link>https://stackoverflow.com/questions/78672059/problems-with-selecting-hyperopt-pipeline-parameters</link>
      <description><![CDATA[我有一个代码，用于迭代模型本身和整个管道的超参数
preprocessor = ColumnTransformer(
[
(&#39;OneHotEncoder&#39;, OneHotEncoder(drop=&#39;if_binary&#39;, sparse_output=False), binary_cols),
(&#39;CatBoostEncoder&#39;, CatBoostEncoder(random_state=RANDOM_STATE), non_binary_cat_cols),
(&#39;StandardScaler&#39;, StandardScaler(), num_cols)
],
verbose_feature_names_out=False,
remainder=&#39;drop&#39;
)

pipe_final = ImbPipeline([
(&#39;preprocessor&#39;, preprocessor),
(&#39;target_imbalance&#39;, ADASYN()),
(&#39;selection&#39;, PCA()),
(&#39;models&#39;, CatBoostClassifier(random_state=RANDOM_STATE))
])

# CatBoostClassifier 的参数集
param_grid = {
&#39;models__iterations&#39;: [1000, 2000, 3000],
&#39;models__class_weights&#39;: [&#39;Balanced&#39;, None],
&#39;target_imbalance&#39;: [ADASYN(random_state=RANDOM_STATE), SMOTETomek(random_state=RANDOM_STATE),
SMOTE(random_state=RANDOM_STATE, k_neighbors=10), &#39;passthrough&#39;],
&#39;preprocessor__StandardScaler&#39;: [StandardScaler(), RobustScaler(), MinMaxScaler(),
PowerTransformer(), QuantileTransformer(),
Normalizer()，PolynomialFeatures（degree=2，include_bias=False），&#39;passthrough&#39;]，
&#39;selection&#39;：[PCA（random_state=RANDOM_STATE，n_components=“mle”，svd_solver=“full”），
SelectKBest（mutual_info_classif，k=40），
SelectKBest（f_classif，k=40），
SelectKBest（chi2，k=40），
SelectPercentile（mutual_info_classif，百分位数=10），
SelectPercentile（f_classif，百分位数=10），
SelectFromModel（CatBoostClassifier（random_state=RANDOM_STATE）），
SelectFromModel（LogisticRegression（random_state=RANDOM_STATE）），
SelectFromModel（RandomForestClassifier（random_state=RANDOM_STATE）），
&#39;passthrough&#39;],
}

gs = GridSearchCV(
pipe_final, 
param_grid, 
cv=5, 
scoring=&#39;roc_auc&#39;, 
n_jobs=-1
)

# Запускаем поиск гиперпараметров
gs.fit(X, y_enc)

这需要很长时间才能完成，我想加快速度。为此，我想使用 OptunaSearchCV。我是否理解正确，使用 OptunaSearchCV 我可以枚举模型本身的超参数，但不能枚举整个管道，因为没有可以设置 SelectKBest(f_classif, k=40)、RobustScaler() 等的分布？
抱歉，如果我的措辞在某些地方不准确，我使用谷歌翻译，因为......我不是母语人士]]></description>
      <guid>https://stackoverflow.com/questions/78672059/problems-with-selecting-hyperopt-pipeline-parameters</guid>
      <pubDate>Wed, 26 Jun 2024 11:14:26 GMT</pubDate>
    </item>
    <item>
      <title>Copilot for Microsoft 365 中的搜索结果优化 [关闭]</title>
      <link>https://stackoverflow.com/questions/78671906/search-results-optimization-in-copilot-for-microsoft-365</link>
      <description><![CDATA[语义索引如何在组织内的 Copilot for Microsoft 365 搜索研究中对文档进行排名？
我们如何操纵 Copilot M365 对我们组织的搜索结果，以便只有具有特定元数据或格式或标记的文档显示在搜索结果的顶部？
问候
我研究了很多文章，发现以下工具之一可以完成这项工作，但我不知道怎么做？

语义索引
RAG（Retravel-Augmented Generation）
Copilot 插件
机器学习技术
或者其他东西……
]]></description>
      <guid>https://stackoverflow.com/questions/78671906/search-results-optimization-in-copilot-for-microsoft-365</guid>
      <pubDate>Wed, 26 Jun 2024 10:44:35 GMT</pubDate>
    </item>
    <item>
      <title>Polars - 性能问题 - 尝试为每行创建一个新的数据框</title>
      <link>https://stackoverflow.com/questions/78668432/polars-issues-with-performance-attempting-to-create-a-new-dataframe-per-row</link>
      <description><![CDATA[我需要在大型 df 的每一行上运行另一个库的算法，但在将我的代码转换为极坐标表达式以获得更好的性能时遇到了麻烦。以下是几个示例 DF：
df_products = pl.DataFrame({
&#39;SKU&#39;:[&#39;apple&#39;,&#39;banana&#39;,&#39;carrot&#39;,&#39;date&#39;],
&#39;DESCRIPTION&#39;: [
&quot;Wire Rope&quot;,
&quot;Connector&quot;,
&quot;Tap&quot;,
&quot;Zebra&quot;
],
&#39;CATL3&#39;: [
&quot;Fittings&quot;,
&quot;Tube&quot;,
&quot;Tools&quot;,
&quot;Animal&quot;
],
&#39;YELLOW_CAT&#39;: [
&quot;Rope Accessories&quot;,
&quot;Tube Fittings&quot;,
&quot;Forming Taps&quot;,
&quot;Striped&quot;
],
&#39;INDEX&#39;: [0, 5, 25, 90],
&#39;嵌入&#39;: [
[1, 2, 3],
[4, 5, 6],
[7, 8, 9],
[10,11,12]
],
})

df_items_sm_ex = pl.DataFrame({
&#39;PRODUCT_INFO&#39;:[&#39;苹果&#39;,&#39;香蕉&#39;,&#39;胡萝卜&#39;],
&#39;搜索相似性分数&#39;: [
[1., 0.87, 0.54, 0.33],
[1., 0.83, 0.77, 0.55],
[1., 0.92, 0.84, 0.65]
],
&#39;搜索位置&#39;: [
[0, 5, 25, 90],
[1, 2, 151, 373],
[3, 5, 95, 1500]
],
&#39;SKU&#39;:[&#39;apple&#39;,&#39;banana&#39;,&#39;carrot&#39;],
&#39;YELLOW_CAT&#39;: [
&quot;绳索配件&quot;,
&quot;管接头&quot;,
&quot;成型丝锥&quot;
],
&#39;CATL3&#39;: [
&quot;接头&quot;,
&quot;管&quot;,
&quot;工具&quot;
],
&#39;EMBEDDINGS&#39;: [
[1, 2, 3],
[4, 5, 6],
[7, 8, 9]
],
})

现在是代码
对于每一行，我有 3 个主要操作：生成基本新数据框、对数据框进行预处理/清理/运行预测、将数据框写入几个 SQL 表。我注意到步骤 1 和 2 很容易花费最长的时间来执行：
df_items_sm_ex.select(
pl.struct(df_items_sm_ex.columns)
.map_elements(lambda row: build_complements(
row, df_items, rfc, rfc_comp, engine, current_datetime
)))

def build_complements(row, df_products, ml, ml_comp, engine, current_datetime):
try:
#步骤 1 - 生成基本新数据框
output_df = build_candidate_dataframe(row, df_products)
#步骤 2 - 对数据框进行预处理/清理/运行预测
output_df = process_candidate_output(df_products, output_df, ml, ml_comp)
#步骤 3 将数据框写入 SQL
write_validate_complements(output_df, row, current_datetime, engine)
except Exception as e:
print(f&#39;exception: {repr(e)}&#39;)

def build_candidate_dataframe(row, df_products):
df_len = len(row[&#39;SEARCH_SIMILARITY_SCORE&#39;])
schema = {&#39;QUERY&#39;: str,
&#39;SIMILARITY_SCORE&#39;: pl.Float32, 
&#39;POSITION&#39;: pl.Int64,
&#39;QUERY_SKU&#39;: str, 
&#39;QUERY_LEAF&#39;: str,
&#39;QUERY_CAT&#39;: str,
&#39;QUERY_EMBEDDINGS&#39;: pl.List(pl.Float32)
}
output_df = pl.DataFrame({&#39;QUERY&#39;: [row[&#39;PRODUCT_INFO&#39;]] * df_len,
&#39;SIMILARITY_SCORE&#39;: row[&#39;SEARCH_SIMILARITY_SCORE&#39;], 
&#39;POSITION&#39;: row[&#39;SEARCH_POSITION&#39;],
&#39;QUERY_SKU&#39;: [row[&#39;SKU&#39;]] * df_len, 
&#39;QUERY_LEAF&#39;: [row[&#39;YELLOW_CAT&#39;]] * df_len,
&#39;QUERY_CAT&#39;: [row[&#39;CATL3&#39;]] * df_len,
&#39;QUERY_EMBEDDINGS&#39;: [row[&#39;EMBEDDINGS&#39;]] * df_len
}, schema=schema).sort(&quot;SIMILARITY_SCORE&quot;, descending=True)

output_df = output_df.join(df_products[[&#39;SKU&#39;, &#39;EMBEDDINGS&#39;, &#39;INDEX&#39;, &#39;DESCRIPTION&#39;, &#39;CATL3&#39;, &#39;YELLOW_CAT&#39;]], left_on=[&#39;POSITION&#39;], right_on=[&#39;INDEX&#39;], how=&#39;left&#39;)
output_df = output_df.rename({&quot;DESCRIPTION&quot;: &quot;SIMILAR_PRODUCT_INFO&quot;, &quot;CATL3&quot;: &quot;SIMILAR_PRODUCT_CAT&quot;, &quot;YELLOW_CAT&quot;: &quot;SIMILAR_PRODUCT_LEAF&quot;})
return output_df

def process_candidate_output(df_products, output_df, ml, ml_comp):
combined_embeddings = (output_df.to_pandas()[&#39;QUERY_EMBEDDINGS&#39;] + output_df.to_pandas()[&#39;EMBEDDINGS&#39;]) / 2
output_df = output_df.with_columns(pl.Series(name=&#39;COMBINED_EMBEDDINGS&#39;, values=combined_embeddings))
output_df = output_df[[&#39;QUERY&#39;, &#39;QUERY_SKU&#39;, &#39;QUERY_CAT&#39;, &#39;QUERY_LEAF&#39;, &#39;SIMILAR_PRODUCT_INFO&#39;, &#39;SIMILAR_PRODUCT_CAT&#39;, &#39;SIMILAR_PRODUCT_LEAF&#39;, &#39;SIMILARITY_SCORE&#39;, &#39;COMBINED_EMBEDDINGS&#39;, &#39;SKU&#39;, &#39;POSITION&#39;]]
output_df = output_df.filter(
pl.col(&#39;SKU&#39;) != output_df[&#39;QUERY_SKU&#39;][0]
)
#ML 预测
output_df = predict_complements(output_df, ml)
output_df = output_df.filter(
pl.col(&#39;COMPLEMENTARY_PREDICTIONS&#39;) == 1
)
#其他 ML 预测
output_df = predict_required_accessories(output_df, ml_comp)
output_df = output_df.sort(by=&#39;LABEL_PROBABILITY&#39;, descending=True)
return output_df
]]></description>
      <guid>https://stackoverflow.com/questions/78668432/polars-issues-with-performance-attempting-to-create-a-new-dataframe-per-row</guid>
      <pubDate>Tue, 25 Jun 2024 16:01:45 GMT</pubDate>
    </item>
    <item>
      <title>两个单一模型还是一个多类模型？</title>
      <link>https://stackoverflow.com/questions/78638050/two-mono-models-or-one-multi-class-model</link>
      <description><![CDATA[我需要预测下一个目标事件 - 购买两个价格类别的汽车（例如，高档和中档）。训练的目标数量大致相同（假设每个价格类别有 10,000 次购买）。我在这里看到两种训练机器学习模型的方法。第一种是多类模型。第二种是两个单独的模型来预测每个细分市场。您认为应该采用哪种方法，为什么？我想以多类模型为基础，您能列出哪些优点和缺点吗？]]></description>
      <guid>https://stackoverflow.com/questions/78638050/two-mono-models-or-one-multi-class-model</guid>
      <pubDate>Tue, 18 Jun 2024 14:23:34 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 阅读阿拉伯语 pdf 书</title>
      <link>https://stackoverflow.com/questions/78631415/read-arabic-pdf-book-using-python</link>
      <description><![CDATA[我正在使用 python 阅读一本阿拉伯语书籍（pdf 是可选的，它不需要任何 OCR（光学字符识别从图像中提取文本）），所以我使用了多个库 pdfplumber、pdfminer.six 和 flitz（PyMuPdf））这是我使用的代码之一：
import pdfplumber
from bidi.algorithm import get_display
import arabic_reshaper
import re

def clean_text(text):
# 删除 NULL 字节和控制字符
cleaned_text = re.sub(r&#39;[\x00-\x1F\x7F]&#39;, &#39;&#39;, text)
return cleaned_text

def reshape_and_bidi_text(text):
# 重塑阿拉伯语文本并应用 bidi 算法
reshaped_text = arabic_reshaper.reshape(text)
bidi_text = get_display(reshaped_text)
return bidi_text

def extract_text_from_pdf(pdf_path):
text = &quot;&quot;
with pdfplumber.open(pdf_path) as pdf:
for page in pdf.pages:
page_text = page.extract_text()
if page_text:
text += page_text + &quot;\n&quot;
返回文本

def save_text_to_file(text, output_path):
with open(output_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as text_file:
text_file.write(text)

def convert_pdf_to_text(pdf_path, output_path):
# 使用 pdfplumber 从 PDF 中提取文本
extracted_text = extract_text_from_pdf(pdf_path)

# 清理提取的文本
cleaned_text = clean_text(extracted_text)

# 重塑文本并将 bidi 算法应用于文本
reshaped_bidi_text = reshape_and_bidi_text(cleaned_text)

# 将清理和重塑的文本保存到文本文件
save_text_to_file(reshaped_bidi_text, output_path)
print(f&quot;来自 {pdf_path} 的文本已保存到 {output_path}&quot;)

# 示例用法
pdf_path = r&#39;C:\Users\DELL\Desktop\Book Printed\البوليميرات العالية الأداء.pdf&#39;
text_output_path = r&quot;C:\Users\DELL\Desktop\output.txt&quot;

convert_pdf_to_text(pdf_path, text_output_path)

所以当使用这些库时，我总是得到以下带有错误编码的输出，我不知道该用什么来修复？
注意：附在上面https://www.noor-book.com/%D9%83%D8%AA%D8%A7%D8%A8-%D8%A7%D9%84%D8%A8%D9%88%D9%84%D9%8A%D9%85%D9%8A%D8%B1%D8%A7%D8%AA-%D8%A7%D9%84%D8%B9%D8%A7%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A3%D8%AF%D8%A7%D8%A1-pdf?next=72c6f38a363b368a7bd978a8449ea530 是我尝试阅读的阿拉伯语书]]></description>
      <guid>https://stackoverflow.com/questions/78631415/read-arabic-pdf-book-using-python</guid>
      <pubDate>Mon, 17 Jun 2024 07:46:38 GMT</pubDate>
    </item>
    <item>
      <title>在 AWS Sagemaker 中训练线性模型时出现 UnexpectedStatusException？</title>
      <link>https://stackoverflow.com/questions/78628328/getting-unexpectedstatusexception-while-training-linear-model-in-aws-sagemaker</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78628328/getting-unexpectedstatusexception-while-training-linear-model-in-aws-sagemaker</guid>
      <pubDate>Sun, 16 Jun 2024 05:27:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 Cord-V2 数据集微调 LayoutLmv3</title>
      <link>https://stackoverflow.com/questions/78606543/fine-tuning-layoutlmv3-using-cord-v2-dataset</link>
      <description><![CDATA[我正在使用 CORD-v2 数据集对 LayoutLMv3 进行微调。我在数据预处理部分遇到了困难，特别是如何从图像中正确提取总量 (TTC)。我在网上找到的示例似乎使用了较旧的 CORD 数据集，该数据集的格式不同。新的 CORD-v2 数据集仅包含图像和地面实况标签。
如何解决这个问题？
我尝试过 YouTube 和 Hugging Face 中的示例，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78606543/fine-tuning-layoutlmv3-using-cord-v2-dataset</guid>
      <pubDate>Tue, 11 Jun 2024 09:22:25 GMT</pubDate>
    </item>
    <item>
      <title>为什么训练时我的 Keras 模型的准确率始终为 0？</title>
      <link>https://stackoverflow.com/questions/45632549/why-is-the-accuracy-for-my-keras-model-always-0-when-training</link>
      <description><![CDATA[我构建了一个简单的 Keras 网络：
import numpy as np;

from keras.models import Sequential;
from keras.layers import Dense,Activation;

data= np.genfromtxt(&quot;./kerastests/mydata.csv&quot;, delimiter=&#39;;&#39;)
x_target=data[:,29]
x_training=np.delete(data,6,axis=1)
x_training=np.delete(x_training,28,axis=1)

model=Sequential()
model.add(Dense(20,activation=&#39;relu&#39;, input_dim=x_training.shape[1]))
model.add(Dense(10,activation=&#39;relu&#39;))
model.add(Dense(1));

model.compile(optimizer=&#39;adam&#39;,loss=&#39;mean_squared_error&#39;,metrics=[&#39;accuracy&#39;])
model.fit(x_training, x_target)

如您所见，从我的源数据中，我删除了 2 列。一列是带有字符串格式日期的列（在数据集中，除此之外，我还有表示日期的列、表示月份的列和表示年份的列，因此我不需要该列），另一列是我用作模型目标的列）。
当我训练这个模型时，我得到了这个输出：
32/816 [&gt;.............................] - ETA：23s - loss：13541942.0000 - acc：0.0000e+00
800/816 [===========================&gt;.] - ETA：0s - loss：11575466.0400 - acc：0.0000e+00 
816/816 [================================] - 1s - 损失：11536905.2353 - 精度：0.0000e+00 
纪元 2/10
32/816 [&gt;.............................] - ETA：0s - 损失：6794785.0000 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5381360.4314 - 精度：0.0000e+00 
纪元 3/10
32/816 [&gt;.............................] - ETA：0s - 损失： 6235184.0000 - 精度：0.0000e+00
800/816 [============================&gt;.] - ETA：0s - 损失：5199512.8700 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5192977.4216 - 精度：0.0000e+00 
纪元 4/10
32/816 [&gt;.............................] - ETA：0s - 损失：4680165.5000 - 精度： 0.0000e+00
736/816 [===========================&gt;...] - ETA：0s - 损失：5050110.3043 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5168771.5490 - 精度：0.0000e+00 
纪元 5/10
32/816 [&gt;.............................] - ETA：0s - 损失：5932391.0000 - 精度：0.0000e+00
768/816 [============================&gt;..] - ETA：0 秒 - 损失：5198882.9167 - 精度：0.0000e+00
816/816 [==============================] - 0 秒 - 损失：5159585.9020 - 精度：0.0000e+00 
纪元 6/10
32/816 [&gt;.............................] - ETA：0 秒 - 损失：4488318.0000 - 精度：0.0000e+00
768/816 [============================&gt;..] - ETA：0s - 损失：5144843.8333 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5151492.1765 - 精度：0.0000e+00 
纪元 7/10
32/816 [&gt;.............................] - ETA：0s - 损失：6920405.0000 - 精度：0.0000e+00
800/816 [=============================&gt;.] - ETA：0s - 损失：5139358.5000 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5169839.2941 - 精度：0.0000e+00 
Epoch 8/10
32/816 [&gt;.............................] - ETA：0s - 损失：3973038.7500 - 精度：0.0000e+00
672/816 [==========================&gt;......] - ETA：0s - 损失：5183285.3690 - 精度：0.0000e+00
816/816 [===============================] - 0s - 损失：5141417.0000 - 精度：0.0000e+00 
Epoch 9/10
32/816 [&gt;.............................] - ETA：0s - 损失：4969548.5000 - 精度：0.0000e+00
768/816 [===========================&gt;..] - ETA：0s - 损失：5126550.1667 - 精度： 0.0000e+00
816/816 [===============================] - 0s - 损失：5136524.5098 - 精度：0.0000e+00 
纪元 10/10
32/816 [&gt;.............................] - ETA：0s - 损失：6334703.5000 - 精度：0.0000e+00
768/816 [===========================&gt;..] - ETA：0s - 损失：5197778.8229 - 精度：0.0000e+00
816/816 [===============================] - 0s - 损失：5141391.2059 - 准确率：0.0000e+00 

为什么会发生这种情况？我的数据是时间序列。我知道对于时间序列，人们通常不使用 Dense 神经元，但这只是一个测试。真正让我困惑的是准确率始终为 0。而且，在其他测试中，我甚至输了：得到一个“NAN”值。
有人能帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/45632549/why-is-the-accuracy-for-my-keras-model-always-0-when-training</guid>
      <pubDate>Fri, 11 Aug 2017 10:08:03 GMT</pubDate>
    </item>
    </channel>
</rss>