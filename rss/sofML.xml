<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 10 Jan 2024 06:19:20 GMT</lastBuildDate>
    <item>
      <title>Handritten 数字识别算法前向传播中的矩阵乘法效率低下</title>
      <link>https://stackoverflow.com/questions/77790906/matrix-multiplication-in-forward-propogation-for-handritten-digit-recog-algo-ine</link>
      <description><![CDATA[我正在用 python 编写手写数字识别神经网络算法，而不使用预先编写的 ML 库。我目前正在尝试实现一个 DenseLayer 类，并在其中实现一个前向传播函数。我当前的功能如下所示。


DenseLayer 类：
  ...
  
  ...
  def for_prop(自身, input_data):
    self.input = input_data

    transpose_weights = self.weights.T
    # matMulComponent = np.matmul(input_data, transpose_weights)
    print(f&quot;转置形状：{transpose_weights.shape} 和输入形状 {input_data.shape}&quot;)
    matMulComponent = input_data.T @ transpose_weights
    打印（len（matMulComponent））

    z = matMulComponent + self.biases.T
    f_wb = self.act_fun(z)
    
    

    self.output = f_wb.reshape(-1, 1)
    print(f&quot;形状结果：{self.output.shape}&quot;)
    返回 self.output



问题是我正在进行大量的重塑和转置以获得结果。这看起来效率不高。我只是像这样实现它，因为这是我让它工作的唯一方法。
所以我的问题是：

这个实施起来好吗（因此会导致效率低下）
有没有更好的方法来实现这个前向传播函数

这就是我的输入数据数组的样子（我刚刚打印它并采取了 ss）。我供参考的输入数据是一个扁平的 28*28 数组，每个单元格代表一种颜色。我首先对数据进行标准化（z 分数标准化）
输入数据图像
如果有帮助的话，我还截取了第一层的权重格式的屏幕截图。 （请记住，它在 for_prop 函数中使用之前已被转置）。
第一个隐藏层的权重矩阵图片
前向传播似乎确实有效，但这很好：前向传播进度 
任何有关我做错的事情的帮助将不胜感激:)]]></description>
      <guid>https://stackoverflow.com/questions/77790906/matrix-multiplication-in-forward-propogation-for-handritten-digit-recog-algo-ine</guid>
      <pubDate>Wed, 10 Jan 2024 04:20:06 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 DCGAN 模型在训练中表现如此糟糕？</title>
      <link>https://stackoverflow.com/questions/77790800/why-is-my-dcgan-model-performing-so-bad-in-training</link>
      <description><![CDATA[我一直在尝试创建一个 DCGAN 作为一个项目来尝试理解它是如何工作的，但是；我遇到的主要问题之一是判别器似乎一直过度拟合。我不确定这是否是由于模型本身或可能是错误的超参数造成的，也许我可能遗漏了一些东西，我想帮助理解原因；我尝试了不同的学习率组合，但在 15 个 epoch 左右之后，我不断得到鉴别器准确率 100% 的相同结果。这是 Colab 的链接：
https://drive.google.com/file/d/1CrZYEV6RNaklKqXYgrbF4SqggxHvabh0/查看？usp=drive_link
我尝试过让整体架构更简单，在超参数中使用不同的值，但我迷失了方向，我认为现在是时候让真正了解这些东西的人来让我了解我不了解的地方了做正确的事。]]></description>
      <guid>https://stackoverflow.com/questions/77790800/why-is-my-dcgan-model-performing-so-bad-in-training</guid>
      <pubDate>Wed, 10 Jan 2024 03:34:03 GMT</pubDate>
    </item>
    <item>
      <title>NLTK 词形还原器收到错误 BadZipFile：文件不是 zip 文件</title>
      <link>https://stackoverflow.com/questions/77790772/nltk-lemmatizer-received-error-badzipfile-file-is-not-a-zip-file</link>
      <description><![CDATA[我正在尝试使用 NLTK 包中的词形还原器，但出现此错误
_RealGetContents 中的文件 /opt/anaconda3/lib/python3.8/zipfile.py:1336
raise BadZipFile(“文件不是 zip 文件”)
BadZipFile：文件不是 zip 文件
我的代码如下
导入nltk
从 nltk.corpus 导入停用词
导入字符串
进口重新
将 pandas 导入为 pd
从 nltk.stem 导入 WordNetLemmatizer
wn = WordNetLemmatizer()
print(wn.lemmatize(&#39;均值&#39;))
print(wn.lemmatize(&#39;含义&#39;))
到目前为止已采取措施但仍无法解决问题：

我尝试使用 conda uninstall nltk 卸载 nltk 软件包并重新下载
我还尝试删除 nltk_data 文件并使用 nltk.download() 再次下载该文件。我注意到随后弹出了下载文件的窗​​口，文件的状态为“已过时”，并且我还收到了错误“下载的 zip 文件出错”

有人可以帮助我吗？我很想学习 NLP，但我目前被困在这里。预先感谢您]]></description>
      <guid>https://stackoverflow.com/questions/77790772/nltk-lemmatizer-received-error-badzipfile-file-is-not-a-zip-file</guid>
      <pubDate>Wed, 10 Jan 2024 03:19:26 GMT</pubDate>
    </item>
    <item>
      <title>如何将变量的每个组合纳入机器学习建模？</title>
      <link>https://stackoverflow.com/questions/77790711/how-to-get-every-combi-of-vars-into-ml-modeling</link>
      <description><![CDATA[假设我有 var a 和 b。我正在使用 var a 进行聚类，另一个使用 b 进行聚类，另一个使用 a 和 b 进行聚类。我不知道如何用 python 实现这个。
感谢您的建议
在下面的代码中，我添加了“#HAS TO BE AUTOMATED”我认为需要自动化的地方
示例数据：
    id 年龄 bp sg al 苏 rbc
0 0 48 80 1.020 1 0 1
1 1 7 50 1.020 4 0 1


id：建模中不需要
年龄 bp sg al su ：数字
rbc ：分类

将 numpy 导入为 np
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns
从 kmodes 导入 kprototypes

数据集 = pd.read_csv(..)
df=数据集.copy()


#删除不必要的列
df.drop(列=[“id”],inplace=True)

#标准化
columns_to_normalize = [&#39;age&#39;,&#39;bp&#39;,&#39;sg&#39;,&#39;al&#39;,&#39;su&#39;] #必须自动化
df[columns_to_normalize] = df[columns_to_normalize].apply(lambda x: (x - x.mean()) / np.std(x))


#获取值数组
data_array=df.值


#指定数据类型
data_array[:, 0:4] = data_array[:, 0:4].astype(float) #必须自动化
data_array[:, 5] = data_array[:, 5].astype(str) #必须自动化


#创建未经训练的模型
untrained_model = kprototypes.KPrototypes(n_clusters=2,max_iter=20)


#预测集群
集群 = untrained_model.fit_predict(data_array, categorical=[5])

数据集[“聚类标签”]=聚类
print(&quot;聚类后的数据是：&quot;)

]]></description>
      <guid>https://stackoverflow.com/questions/77790711/how-to-get-every-combi-of-vars-into-ml-modeling</guid>
      <pubDate>Wed, 10 Jan 2024 03:01:06 GMT</pubDate>
    </item>
    <item>
      <title>将 Pytorch 模型 .pth 转换为 onnx 模型时遇到问题</title>
      <link>https://stackoverflow.com/questions/77790645/having-problem-on-converting-pytorch-model-pth-into-onnx-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77790645/having-problem-on-converting-pytorch-model-pth-into-onnx-model</guid>
      <pubDate>Wed, 10 Jan 2024 02:28:32 GMT</pubDate>
    </item>
    <item>
      <title>如何补全机器学习中的缺失值[关闭]</title>
      <link>https://stackoverflow.com/questions/77790058/how-to-complete-missing-values-in-machine-learning</link>
      <description><![CDATA[如何使用模式完成多个分类数据
我尝试使用模式来获取最常出现的单词，即城市名称，然后将其替换缺失的值，但它不起作用，它给出了错误，请大家帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/77790058/how-to-complete-missing-values-in-machine-learning</guid>
      <pubDate>Tue, 09 Jan 2024 22:30:02 GMT</pubDate>
    </item>
    <item>
      <title>多个时间序列的离散化</title>
      <link>https://stackoverflow.com/questions/77789902/discretization-of-multiple-time-series</link>
      <description><![CDATA[我正在为一个项目离散化多个时间序列。这是我到目前为止所做的：

我将列车信号连接起来，如下所示：[1,2,3,5] 和 [7,3,6,7] 为 [1,2,3,5,7,3,6,7]。&lt; /里&gt;
然后，我使用单个组合信号训练了 K-means ML 模型。
最后，我使用经过训练的模型对所有信号进行了聚类。

我不确定这是否是正确的方法。我选择连接信号是因为 k-mean（来自 scikit-learn）只允许插入一个数组，所以我不知道如何为其提供多个时间序列。
有没有人做过类似的事情，或者有没有人对更好的离散化方法有建议？]]></description>
      <guid>https://stackoverflow.com/questions/77789902/discretization-of-multiple-time-series</guid>
      <pubDate>Tue, 09 Jan 2024 21:48:07 GMT</pubDate>
    </item>
    <item>
      <title>我在视觉变压器中有矩形图像数据集。我设置 image_size= (128, 256) 但补丁大小可能是多少？</title>
      <link>https://stackoverflow.com/questions/77788451/i-have-rectangular-image-dataset-in-vision-transformers-i-set-image-size-128</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77788451/i-have-rectangular-image-dataset-in-vision-transformers-i-set-image-size-128</guid>
      <pubDate>Tue, 09 Jan 2024 16:55:52 GMT</pubDate>
    </item>
    <item>
      <title>scikit 的 RFECV 类如何计算 cv_results_？</title>
      <link>https://stackoverflow.com/questions/77788410/how-does-scikits-rfecv-class-compute-cv-results</link>
      <description><![CDATA[根据我对 sklearn.feature_selection.RFECV（递归特征消除交叉验证）的理解，您提供了一种在整个数据集上进行训练的算法，并使用属性 coef_ 创建特征重要性排名 或 feature_importances_。现在包含了所有功能，该算法通过交叉验证进行评估。然后，删除排名底部的特征，并在数据集上重新训练模型并创建新的排名，再次通过交叉验证进行评估。这一过程将持续下去，直到只剩下一个特征（或由 min_features_to_select 指定），并且最终选择的特征数量取决于产生最高 CV 分数的特征。 （来源)
每个特征数量的 CV 分数存储在 rfecv.cv_results_[“mean_test_score”] 中，并且在不使用 scikit 内置方法的情况下尝试复制这些分数时遇到了麻烦。 
这是我试图获得 n-1 个特征的分数，其中 n 是特征总数。
从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.model_selection 导入 StratifiedKFold
从 sklearn.model_selection 导入 cross_validate
从 sklearn.feature_selection 导入 RFECV

alg = DecisionTreeClassifier(random_state = 0)
cv_split = 分层KFold(5)
# train 是 pandas 数据框，x_var 和 y_var 都是包含变量字符串的列表
X = 火车[x_var]
y = np.ravel(train[y_var])

alg.fit(X, y)
最低排名特征 = np.argmin(alg.feature_importances_)
x_var.pop(最低排名特征)

one_removed_feature = 火车[x_var]
alg.fit(one_removed_feature, y)
cv_score = cross_validate(alg, one_removed_feature, y, cv=cv_split, 评分=“准确度”)
np.mean(cv_score[“test_score”])

这是提供不同分数的内置方法：
&lt;前&gt;&lt;代码&gt;rfecv = RFECV(
    估计量=alg,
    步骤=1，
    CV=CV_分裂，
    评分=“准确度”，
）

rfecv.fit(X, y)
rfecv.cv_results_[“mean_test_score”][-2]

如何获得内置方法中计算出的准确分数？
我还想提一下，我确实首先尝试了所有 n 个功能，并且我的方法与
rfecv.cv_results_[“mean_test_score”][-1]。]]></description>
      <guid>https://stackoverflow.com/questions/77788410/how-does-scikits-rfecv-class-compute-cv-results</guid>
      <pubDate>Tue, 09 Jan 2024 16:47:46 GMT</pubDate>
    </item>
    <item>
      <title>过多的 padding 导致 NN 模型精度下降</title>
      <link>https://stackoverflow.com/questions/77785503/excessive-padding-causes-accuracy-decrease-to-nn-model</link>
      <description><![CDATA[我训练了一个简单的神经网络模型来进行二元分类，并能够区分真假新闻
#创建模型的类
类 FakeNewsDetectionModelV0(nn.Module):
     def __init__(自身, input_size):
        超级().__init__()
        
        self.layer_1=nn.Linear(in_features=input_size, out_features=8)
        self.layer_2=nn.Linear(in_features=8, out_features=1) #从前一层获取5个特征并输出单个特征

     #定义一个forward()用于前向传播
     def 前向（自身，x，掩码）：
        
        # 应用掩码来忽略某些值
        如果掩码不是 None：
            x = x * 掩码

        x = self.layer_1(x)
        x = self.layer_2(x)
        返回x




我使用 CountVectorizer 将文本转换为列表，然后转换为张量
从 sklearn.feature_extraction.text 导入 CountVectorizer

矢量化器 = CountVectorizer(min_df=0, 小写=False)
矢量化器.fit(df[&#39;文本&#39;])

X=vectorizer.fit_transform(df[&#39;text&#39;]).toarray()

问题在于，由于数据集有超过 9000 个条目，因此训练模型的输入大小非常大（大约 120000 个）。因此，当我尝试对单个句子进行预测时，由于大小明显较小，我需要过度填充句子以使其适合模型的输入，这极大地影响了模型的准确性。
from io 导入 StringIO
来自 torch.nn.function 导入垫
导入字符串
进口重新
从 nltk.tokenize 导入 word_tokenize
从 nltk.corpus 导入停用词
导入nltk
从 keras.preprocessing.text 导入 Tokenizer
从 keras.preprocessing.sequence 导入 pad_sequences


尝试：
    #nltk.download(&#39;停用词&#39;)
    nltk.download(&#39;punkt&#39;)
除了：
    print(&quot;下载停用词时出错&quot;)

def normalise_text (文本):

  text = text.lower() # 小写
  text = text.replace(r&quot;\#&quot;,&quot;&quot;) # 替换主题标签
  text = text.replace(r&quot;http\S+&quot;,&quot;URL&quot;) # 删除 URL 地址
  text = text.replace(r&quot;@&quot;,&quot;&quot;)
  text = text.replace(r&quot;[^A-Za-z0-9()!?\&#39;\`\&quot;]&quot;, &quot; &quot;)
  text = text.replace(&quot;\s{2,}&quot;, &quot;&quot;)
  文本 = re.sub(r&#39;[^\w\s]&#39;, &#39;&#39;, 文本)
  返回文本

def fake_news_detection(df, model, model_input_size):
    预测=[]
    最大字数 = 10000
    最大长度 = 模型输入大小

    模型.eval()

    对于 df[&#39;text&#39;][:4000] 中的预测数据：
        预测数据=标准化文本（预测数据）

        #print([预测数据])



        # 使用CountVectorizer将文本数据转换为数组
        矢量化器 = CountVectorizer(min_df=0, 小写=False)
        Prediction_data_array = Vectorizer.fit_transform([prediction_data]).toarray()

        #tokenizer = Tokenizer(num_words=max_words)
        #tokenizer.fit_on_texts([预测数据])
        #sequences = tokenizer.texts_to_sequences([预测数据])


        #prediction_data_array = pad_sequences(序列, maxlen=max_length,value=-1.0)

        #print(预测数据数组.形状)

        # 检查转换后数据的形状
        当前输入大小=预测数据数组.形状[1]


        Prediction_data_tensor = torch.tensor(prediction_data_array, dtype=torch.float32)


        # 如果形状不匹配，则调整其大小
        如果当前输入大小！=模型输入大小：

            打印（当前输入大小）
            填充 = 模型输入大小 - 当前输入大小
            Prediction_data_tensor = pad(prediction_data_tensor, (0, 填充), &#39;常量&#39;, 值 = 0)
            mask_tensor = torch.ones_like(prediction_data_tensor)
            mask_tensor[:, -padding:] = 0 # 将填充区域中的值设置为 0
            #print(torch.unique(mask_tensor, return_counts=True))

            # 应用掩码来忽略某些值
            #预测数据张量 = 预测数据张量 * 掩码张量



        # 假设你的模型将 input_data 作为输入
        使用 torch.inference_mode()：
            预测 = torch.round(torch.sigmoid(model(prediction_data_tensor, mask_tensor))).squeeze()

        预测.append(round(预测.item()))

    print(f“我们的数据张量形状是 {prediction_data_tensor.shape}”)

    Predictions_tensor = torch.FloatTensor(预测)

    返回预测张量

有谁知道有什么解决方法可以让我将数据适合我的模型而不降低其准确性分数吗？
尝试：在对小尺寸数据进行预测时填充向量
预期：准确的预测类似于我在训练/评估过程中得到的结果
得到：预测不准确，准确度非常低（大约 43%）]]></description>
      <guid>https://stackoverflow.com/questions/77785503/excessive-padding-causes-accuracy-decrease-to-nn-model</guid>
      <pubDate>Tue, 09 Jan 2024 09:03:52 GMT</pubDate>
    </item>
    <item>
      <title>SHAP 解释器获取错误的数据类型[关闭]</title>
      <link>https://stackoverflow.com/questions/77785286/shap-explainer-getting-wrong-datatype</link>
      <description><![CDATA[这是我的代码。我正在尝试获取 X 射线图像的 SHAP 值。
model.eval()

＃ 转型
def preprocess_image(image_path):
    图像 = Image.open(image_path).convert(&#39;RGB&#39;)
    变换 = 变换.Compose([
        变换.调整大小((224, 224)),
        变换.ToTensor(),
        变换.Normalize(平均值=[0.485,0.456,0.406],std=[0.229,0.224,0.225]),])
    input_image = 变换（图像）.unsqueeze（0）
    返回输入图像

image_path = &#39;C.jpg&#39;
输入图像 = 预处理图像（图像路径）

masker = shap.maskers.Image(“inpaint_telea”, input_image.size())

解释器= shap.Explainer（模型，掩码器，output_names = [“A”，“B”，“C”]）

shap_values = 解释器(input_image)

shap.image_plot(shap_values, input_image.numpy())

当我运行此命令时，解释器获取错误的数据类型，并且出现此错误：
 *（张量输入、张量权重、张量偏差、整数步幅元组、整数填充元组、整数膨胀元组、整数组）
      不匹配，因为某些参数具有无效类型： (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple (int, int)!, int)
 *（张量输入、张量权重、张量偏差、整数步幅元组、str 填充、整数膨胀元组、整数组）
      不匹配，因为某些参数具有无效类型： (!numpy.ndarray!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple (int, int)!, int)

上线：
shap_values = 解释器(input_image)

我想获取图像 C.jpg 的 SHAP 值]]></description>
      <guid>https://stackoverflow.com/questions/77785286/shap-explainer-getting-wrong-datatype</guid>
      <pubDate>Tue, 09 Jan 2024 08:26:58 GMT</pubDate>
    </item>
    <item>
      <title>跨多个模型的交叉验证的一致性</title>
      <link>https://stackoverflow.com/questions/77778800/consistency-in-cross-validation-folds-across-multiple-models</link>
      <description><![CDATA[我目前正在做一个机器学习项目，其中使用三种不同的模型：随机森林、AdaBoost 和梯度提升。对于每个模型，我将它们应用于一组训练和测试数据。此外，我计划将五重交叉验证纳入我的实验中。
我的问题涉及这些模型之间交叉验证的实施。具体来说，我是否应该对所有三个模型（RF、ADA 和 GB）使用相同的五倍，以确保每个模型训练和测试的数据的一致性？或者，为每个模型生成不同的折叠集，从而独立地对 RF、ADA 和 GB 进行交叉验证过程是否更合适？
我有兴趣了解哪种方法更有利于实验的完整性，以及在这种情况下是否有任何标准实践或建议。
如果您能分享任何见解或经验，我们将不胜感激。谢谢！
我在网上发现了相互矛盾的信息。]]></description>
      <guid>https://stackoverflow.com/questions/77778800/consistency-in-cross-validation-folds-across-multiple-models</guid>
      <pubDate>Mon, 08 Jan 2024 10:17:44 GMT</pubDate>
    </item>
    <item>
      <title>训练时在 pytorch 中没有出现 grad set 错误</title>
      <link>https://stackoverflow.com/questions/77777706/getting-no-grad-set-error-in-pytorch-while-traning</link>
      <description><![CDATA[运行时错误：张量的元素 0 不需要 grad 并且没有 grad_fn 
我在以下训练循环中遇到此错误，梯度必须由顺序本身设置，但它说没有梯度。
“”“培训”“”
纪元 = 100


对于范围内的历元（Epochs）：
    模型.train()

    train_logits = 模型(X_train)
    train_preds_probs = torch.softmax(train_logits,dim=1).argmax(dim=1).type(torch.float32)
    损失 = loss_fn(train_preds_probs,y_train)
    train_accu = 准确率(y_train,train_preds_probs)
    打印（train_preds_probs）
    优化器.zero_grad()

    loss.backward()

    优化器.step()

    ＃训练
    模型.eval()
    使用 torch.inference_mode()：
        test_logits = 模型(X_test)
        test_preds = torch.softmax(test_logits.type(torch.float32),dim=1).argmax(dim=1)
        test_loss = loss_fn(test_preds,y_train)
        test_acc = 准确度(y_test,test_preds)

    
    如果纪元％10 == 0：
        print(f&#39;Epoch:{epoch} | 训练损失: {loss} |泰宁 acc:{train_accu} | 测试损失: {test_loss} | 测试acc: {test_acc}&#39;)






我尝试上网冲浪，但没有找到解决方案。
感谢任何帮助！]]></description>
      <guid>https://stackoverflow.com/questions/77777706/getting-no-grad-set-error-in-pytorch-while-traning</guid>
      <pubDate>Mon, 08 Jan 2024 07:51:44 GMT</pubDate>
    </item>
    <item>
      <title>将大型语料库中的 n 元模型加载到集合中时如何避免内存问题</title>
      <link>https://stackoverflow.com/questions/77758125/how-to-circumvent-memory-issues-when-loading-n-grams-from-large-corpus-into-set</link>
      <description><![CDATA[我一直在尝试实现一种无监督学习算法，该算法根据从语料库中提取的特定特征来匹配相似性。一个用例是作者识别。该算法的工作方式是从训练语料库中提取不同类型的 n-gram，然后每个作者都会获得一个“指纹”。基于文章中出现的 n 元语法。
为此，我首先需要收集训练语料库中存在的所有 n 元语法。这就是我遇到内存问题的地方，我一直在使用 Yelp 评论数据，并且在某些时候我的程序由于内存限制而崩溃。我尝试过存储中间结果，然后将 n-gram 加载到最终集合中，以避免我的稀疏计算中出现任何潜在的内存泄漏问题，但这也失败了，看来该集合太大了。
如何解决这个问题？
根据反馈，我希望这能提供一些见解，不确定如果我通过 800 多行代码会有多大帮助，我希望我能够提取要点：
假设我有以下文本片段：
&#39;Zahlreiche Konzertabsagen aufgrund von Impf-Nebenwirkungen。 Teil XI – 11 Direkt zum 视频：Politiker und hochrangige Beamte leiden unter Nebenwirkungen und sterben nach der Impfung。 Teil XIV Direkt zum 视频：Frontstadt Charkow – Putins verlorene Schlacht 乌克兰 – Der Kontext，der in den Medien fehlt |历史学家 Prashad Freitod 教授 nach „Impfschaden“ – Ein Weckruf Corona [...]&#39;
在本文中，我仅使用如下内容保留功能词：
 def filter_function_words(doc, lta_table,phrase_matcher):
    匹配的单词= []
    masked_matched_words = [&#39;#&#39;] * len(doc)
    短语匹配 = 短语匹配器（文档）
    对于在 doc.sents 中发送：
        # 过滤属于当前句子的匹配项
        短语匹配_发送 = [
            匹配phrase_matches中的匹配项
            如果已发送.start &lt;= match[1] &lt;已发送结束]

        如果没有phrase_matches_sent：
            继续
            
        对于 match_id、开始、结束于phrase_matches_sent：
            跨度 = doc[开始:结束]
            masked_matched_words[开始:结束] = span.text.split()
        matched_words = [x for x in masked_matched_words if x != &#39;#&#39;]
    返回 masked_matched_words、matched_words

[&#39;aufgrund&#39;, &#39;von&#39;, &#39;direkt&#39;, &#39;zum&#39;, &#39;und&#39;, &#39;unter&#39;, &#39;und&#39;, &#39;nach&#39;, &#39;der&#39;, &#39;direkt&#39;, &#39;zum&#39;, &#39;der &#39;, &#39;der&#39;, &#39;in&#39;, &#39;den&#39;, &#39;nach&#39;, &#39;ein&#39;]
现在我构造多个 n-gram，即 (1,2,3,4)，这里是 3 的示例：
[(&#39;aufgrund&#39;, &#39;von&#39;, &#39;direkt&#39;),
(&#39;von&#39;, &#39;direkt&#39;, &#39;zum&#39;),
(&#39;direct&#39;, &#39;zum&#39;, &#39;und&#39;),
(&#39;zum&#39;, &#39;und&#39;, &#39;unter&#39;),
(&#39;和&#39;, &#39;下&#39;, &#39;和&#39;),
(&#39;unter&#39;, &#39;und&#39;, &#39;nach&#39;),
(&#39;und&#39;, &#39;nach&#39;, &#39;der&#39;),
(&#39;nach&#39;, &#39;der&#39;, &#39;direkt&#39;),
(&#39;der&#39;, &#39;direkt&#39;, &#39;zum&#39;),
(&#39;direct&#39;, &#39;zum&#39;, &#39;der&#39;),
(&#39;zum&#39;, &#39;der&#39;, &#39;der&#39;),
(&#39;der&#39;, &#39;der&#39;, &#39;in&#39;),
(&#39;der&#39;, &#39;in&#39;, &#39;den&#39;),
(&#39;在&#39;, &#39;书房&#39;, &#39;nach&#39;),
(&#39;den&#39;, &#39;nach&#39;, &#39;ein&#39;)]
我对整个语料库执行此操作，收集一组 n 元语法。
对于 self.n_grams_token 中的 n：
        self.token_n_grams_set[n].update(ngrams(matched_words, n))

现在我有了整个语料库中出现的所有 n 元语法，然后我用这样的东西构建特征向量
 def _get_feature_vector(self,
                            元素：列表[str]，
                            壮举名称，
                            n=无）-&gt;火炬.张量：
        计数器 = 计数器（元素）
        counter_set = OrderedSet(计数器)
        # 更改为 torch.Tensor
        如果 n 不是 None：
            feat_set = self.feature_name_set_dict[feat_name][n]
        别的：
            feat_set = self.feature_name_set_dict[feat_name]
        available_terms = feat_set.intersection(counter_set)
        索引 = feat_set.index(available_terms)
        值 = np.fromiter(counter.values(),
                             dtype=int)[counter_set.index(available_terms)]
        #feature_vector = torch.zeros(len(feat_set), dtype=torch.int)
        #feature_vector[indexes] = torch.tensor(values, dtype=torch.int)
        idxs = torch.tensor([索引], dtype=torch.long)
        vals = torch.tensor(values, dtype=torch.float)
        大小 = torch.Size([len(feat_set)])
        feature_vector = torch.sparse_coo_tensor(idxs, vals, 大小)
        返回特征向量

我的数据存储在数据框中，它的大小为 5854272 行，我得到以下内存使用情况输出：
df.memory_usage()
索引 128
作者 46834176
文章 46834176
数据类型：int64

基本上，我正在计算每篇文章的 n 元语法，将它们添加到一个集合中，然后将它们存储到字典中。]]></description>
      <guid>https://stackoverflow.com/questions/77758125/how-to-circumvent-memory-issues-when-loading-n-grams-from-large-corpus-into-set</guid>
      <pubDate>Thu, 04 Jan 2024 12:00:51 GMT</pubDate>
    </item>
    <item>
      <title>分割模型推理延迟问题</title>
      <link>https://stackoverflow.com/questions/77678168/segmentation-model-inference-latency-issue</link>
      <description><![CDATA[我使用了 pyannote 的开源分割模型和 Diart diarization 的说话者二值化存储库，使用 diart==0.5.1，
在 diart/blocks/segmentation.py 中，我进行了以下更改::
 与 torch.no_grad()：
        wave=rearrange(self.formatter.cast(waveform),“批量采样通道-&gt;批量通道采样”)
        # 波火炬.Tensor (1, 1, 80000)
        打印（wave.get_device（））
        开始 = 时间.time()
        输出 = self.model(wave.to(self.device)).cpu()
        停止=时间.time()
        print(&#39;分段时间:&#39;)
        打印（停止-开始）
        # 输出：torch.Tensor (1, 293, 3)
    返回 self.formatter.restore_type(输出)

在输出中，seg时间：0.4s
但是如果我尝试在 diart 存储库之外进行推断（在独立的存储库中）：
defegmentation_model(self,batch:np.ndarray) -&gt;; np.ndarray：
    块 = torch.tensor(batch)
    print(chunks.get_device()) # -1

    使用 torch.no_grad()：
        尝试：
            打印（块.形状）
            输出 = self.model(chunks.to(self.device)).cpu()
        除了 RuntimeError 作为例外：
            如果 is_oom_error（异常）：
                引发内存错误（
                    f&quot;batch_size ({self.batch_size: d}) 可能太大。 ”
                    f“尝试使用较小的值，直到内存错误消失。”
                ）
            别的：
                引发异常

    返回输出.numpy()

此处分段时间：10s
资源、输入格式、形状、类型一切都是相同的
为什么延迟不同？
期望延迟相同]]></description>
      <guid>https://stackoverflow.com/questions/77678168/segmentation-model-inference-latency-issue</guid>
      <pubDate>Mon, 18 Dec 2023 09:38:17 GMT</pubDate>
    </item>
    </channel>
</rss>