<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 12 Apr 2024 15:13:36 GMT</lastBuildDate>
    <item>
      <title>人工智能和机器学习 (AI-ML) 医学科学职业</title>
      <link>https://stackoverflow.com/questions/78317024/career-in-medical-science-with-artificial-intelligence-and-machine-learning-ai</link>
      <description><![CDATA[所以，我想问一个关于使用 AI-ML 专业化的医学科学职业的真实问题。
我是一名三年级学生，攻读计算机科学工程学士学位，专业为 AI-ML。我想在医学科学领域从事计算机工程方面的职业，特别是 AI-ML。
我很困惑我应该采取什么途径来实现这个目标。您能否指导我在实习期间可以申请哪些公司？另外，如果您能给我一个路线图，告诉我除了我的专业之外我还应该具备哪些技能，那就太好了。
我尝试在互联网上搜索，但没有找到我的问题的任何明确答案。]]></description>
      <guid>https://stackoverflow.com/questions/78317024/career-in-medical-science-with-artificial-intelligence-and-machine-learning-ai</guid>
      <pubDate>Fri, 12 Apr 2024 14:42:36 GMT</pubDate>
    </item>
    <item>
      <title>一些训练记录，以及训练后的一份记录 [关闭]</title>
      <link>https://stackoverflow.com/questions/78316879/a-few-records-for-training-and-one-record-after-training</link>
      <description><![CDATA[我尝试做一些信用评分任务。我陷入了概念问题。
有：
train_data（62 列、10339239 行、1250000 个唯一 ID 值 [0 - 1249999]（[最小-最大] ID 值） ),
test_data（62 列、4724601 行、500000 个唯一 ID 值 [3000000 - 3499999]（[最小-最大] ID 值） ),
train_target.csv（2列：ID和flag（flag是目标变量，必须预测)，有 361870 行，全部具有唯一的 ID，[0 - 361869]([min-max] ID 值)) ,
test_target.csv（1 列：ID，500000 行，所有 ID 都是唯一的，[3000000 - 3499999]([min- max] ID 值)) 。
需要为 test_target.csv 获取 [0,1] 范围内的分数。
train_data 和 test_data 有 62 列，ID、RN，...。两者都是对应的时间（如果日期时间较大，则 RN 的 ID 值会更大）。 ID表示信用/贷款请求，RN表示信用记录中的信用/贷款数量。 train_target.csv 中的 FLAG 表示：1-默认/破产。
我不知道如何针对这些数据训练模型。我尝试使用XGBoost。训练后的模型必须采用具有相同 ID 的 RN 排序的几条记录，并给出 [0, 1] 范围内的一个 FLAG 答案。如何才能做到这一点？ XGB分类器？或回归器？通过train_test_split还是TimeSeriesSplit？你能给我建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78316879/a-few-records-for-training-and-one-record-after-training</guid>
      <pubDate>Fri, 12 Apr 2024 14:19:39 GMT</pubDate>
    </item>
    <item>
      <title>重新称重预处理技术与 AdaBoost 的结合</title>
      <link>https://stackoverflow.com/questions/78316838/combination-of-reweighing-as-pre-processing-technique-and-adaboost</link>
      <description><![CDATA[我使用了预处理技术“重新称重”来消除数据集的偏差。该技术为我提供了一个列，其中包含每个实例的权重。对于重新称重，我使用了 AIF 360。
对于模型，我想使用 sklearn 的 AdaBoost 分类器。
将 .fit() 方法中的预处理技术的权重移交给 AdaBoost 分类器是否有意义？该参数为sample_weight。
有点不清楚我是否应该使用 class_weight 还是sample_weight。
我已经研究了应该使用什么参数，或者将权重交给 AdaBoost 是否有意义。然而，我觉得这项研究让我更加不清楚了。我知道 sklearn 的 AdaBoost 分类器没有 class_weight 参数。但如果将权重交给 AdaBoost 有意义的话，也许专家可以帮助我。]]></description>
      <guid>https://stackoverflow.com/questions/78316838/combination-of-reweighing-as-pre-processing-technique-and-adaboost</guid>
      <pubDate>Fri, 12 Apr 2024 14:11:12 GMT</pubDate>
    </item>
    <item>
      <title>这是对不平衡数据集进行过采样交叉验证的正确方法吗？</title>
      <link>https://stackoverflow.com/questions/78316022/is-this-the-right-way-to-do-cross-validation-with-oversampling-on-imbalance-data</link>
      <description><![CDATA[def stratified_cross_validation_metrics(模型, X, y, method=&#39;&#39;):
    指标={
        &#39;准确性&#39;： []，
        &#39;精确&#39;： []，
        &#39;记起&#39;： []，
        &#39;f1_score&#39;: []
    }
    
    kf = StratifiedKFold(n_splits=10, shuffle=False)
    
    对于 kf.split(X, y) 中的 train_index、test_index：
        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]
        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]
        
        if method.lower() == &#39;adasyn&#39;:
            X_train_cv, y_train_cv = adasyn.fit_resample(X_train_cv, y_train_cv)
            X_test_cv, y_test_cv = adasyn.fit_resample(X_test_cv, y_test_cv)
        elif method.lower() == &#39;smote&#39;:
            X_train_cv, y_train_cv = smote.fit_resample(X_train_cv, y_train_cv)
            X_test_cv, y_test_cv = smote.fit_resample(X_test_cv, y_test_cv)
        
        model.fit(X_train_cv, y_train_cv)

        y_pred_cv = model.predict(X_test_cv)

        准确度=准确度_分数（y_test_cv，y_pred_cv）
        精度 = precision_score(y_test_cv, y_pred_cv, 平均值=&#39;加权&#39;)
        召回率=召回率（y_test_cv，y_pred_cv，平均值=&#39;加权&#39;）
        f1 = f1_score(y_test_cv, y_pred_cv, 平均值=&#39;加权&#39;)

        指标[&#39;准确度&#39;].append(准确度)
        指标[&#39;精度&#39;].append(精度)
        指标[&#39;recall&#39;].append(recall)
        指标[&#39;f1_score&#39;].append(f1)

    print(&quot;10 倍分层交叉验证的平均指标：&quot;)
    print(&quot;准确率：&quot;, np.mean(metrics[&#39;accuracy&#39;]))
    print(&quot;精度：&quot;, np.mean(metrics[&#39;精度&#39;]))
    print(&quot;召回率：&quot;, np.mean(metrics[&#39;recall&#39;]))
    print(&quot;F1 分数：&quot;, np.mean(metrics[&#39;f1_score&#39;]))

    df = pd.DataFrame(指标)
    
    返回df

我读到，当你使用过采样方法进行交叉验证时，你不应该首先过采样，这样它就不会泄漏到测试数据，所以我按照所示的方式执行函数，这是正确的方法还是我做错了什么？
对于不平衡数据集使用什么指标平均值比较好？]]></description>
      <guid>https://stackoverflow.com/questions/78316022/is-this-the-right-way-to-do-cross-validation-with-oversampling-on-imbalance-data</guid>
      <pubDate>Fri, 12 Apr 2024 11:37:37 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：操作数无法与形状一起广播 (10,1024) (1024,1) [关闭]</title>
      <link>https://stackoverflow.com/questions/78316002/valueerror-operands-could-not-be-broadcast-together-with-shapes-10-1024-1024</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78316002/valueerror-operands-could-not-be-broadcast-together-with-shapes-10-1024-1024</guid>
      <pubDate>Fri, 12 Apr 2024 11:33:12 GMT</pubDate>
    </item>
    <item>
      <title>根据之前的元素，甚至不基于之前的元素，使用 LSTM 预测下一个元素</title>
      <link>https://stackoverflow.com/questions/78315830/predicting-next-elements-with-an-lstm-based-on-previous-ones-or-even-based-on</link>
      <description><![CDATA[我有一个时间序列问题，其中包括使用 LSTM 预测价格。该数据集是从 Python 库 yfinance 导入的。我在内置的极客教程中使用了示例代码Pytorch，并设法理解一切。我认为我不明白的唯一具体部分可以在这段代码片段中看到：
# 定义要预测的未来时间步数
预测步数 = 30

# 转换为 NumPy 并删除单一维度
sequence_to_plot = X_test.squeeze().cpu().numpy()

# 使用最后 30 个数据点作为起点
历史数据=序列到图[-1]
打印（历史数据.形状）

# 初始化一个列表来存储预测值
预测值 = []

# 使用训练好的模型来预测未来值
使用 torch.no_grad()：
    对于 _ 在范围内（num_forecast_steps*2）：
        # 准备历史数据张量
        历史数据张量 = torch.as_tensor(历史数据).view(1, -1, 1).float().to(设备)
        # 使用模型预测下一个值
        预测值 = 模型(历史数据张量).cpu().numpy()[0, 0]

        # 将预测值添加到forecasted_values列表中
        Forecasted_values.append(predicted_value[0])

        # 通过删除最旧的值并添加预测值来更新历史数据序列
        历史数据 = np.roll(历史数据, 移位=-1)
        历史数据[-1] = 预测值

        
# 生成未来日期
最后日期 = test_data.index[-1]

# 生成接下来的 30 个日期
future_dates = pd.date_range(start=last_date + pd.DateOffset(1), period=30)

# 将原始索引与未来日期连接起来
组合索引 = test_data.index.append(future_dates)

根据教程内容，使用滚动预测。这意味着，如果我是对的，带有输入元素的数组将用作预测下一个元素的窗口，该窗口将附加到窗口，以预测下一个元素，依此类推。我的问题可能与概念或代码的其他部分有关，但我不太理解这行代码：
# 将预测值追加到 Forecasted_values 列表中
Forecasted_values.append(predicted_value[0])

如果我想根据之前的 30 个元素来预测下一个元素，为什么我应该采用预测数组的第一个预测元素？ （对我来说，这意味着前 30 个输入序列的第二个元素）。如果我需要基于动态生成的值构建一个序列窗口，并且之前没有先前的元素，我该如何修改此示例？]]></description>
      <guid>https://stackoverflow.com/questions/78315830/predicting-next-elements-with-an-lstm-based-on-previous-ones-or-even-based-on</guid>
      <pubDate>Fri, 12 Apr 2024 11:01:41 GMT</pubDate>
    </item>
    <item>
      <title>读取包含字符串数据的 csv 文件时出现错误</title>
      <link>https://stackoverflow.com/questions/78315561/getting-error-while-reading-csv-file-with-string-data</link>
      <description><![CDATA[我正在编写 DeepLearning4j 代码来读取下面的 csv 文件：
键，值
员工 ID、同事 ID
员工证、工人证
员工证,员工证

下面是我尝试过的Java代码
 数据集 allData1;
    尝试 (RecordReader recordReader1 = new CSVRecordReader(1, &#39;,&#39;)) {
        recordReader1.initialize(new FileSplit(new ClassPathResource(“EmployeeData.txt”).getFile()));

        DataSetIterator 迭代器 = new RecordReaderDataSetIterator(recordReader1, 3, 1,1, true);
        allData1 = 迭代器.next();
    }

但是，导致以下错误..
线程“main”中出现异常java.lang.NumberFormatException：对于输入字符串：“员工 ID”
    在 java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)
    在 java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
    在 java.base/java.lang.Double.parseDouble(Double.java:651)
    在 org.datavec.api.writable.Text.toDouble(Text.java:590)
    在 org.datavec.api.util.ndarray.RecordConverter.toMinibatchArray(RecordConverter.java:207)
    在 org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.next（RecordReaderMultiDataSetIterator.java:153）
    在 org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next（RecordReaderDataSetIterator.java:346）
    在 org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next（RecordReaderDataSetIterator.java:421）
    在 org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next（RecordReaderDataSetIterator.java:53）

我哪里出错了？
此代码可以正常处理 CSV 文件中的 int 和 float 值...]]></description>
      <guid>https://stackoverflow.com/questions/78315561/getting-error-while-reading-csv-file-with-string-data</guid>
      <pubDate>Fri, 12 Apr 2024 10:10:50 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python 检测 PDF 中选定的文本？</title>
      <link>https://stackoverflow.com/questions/78314645/how-to-detect-selected-text-from-a-pdf-using-python</link>
      <description><![CDATA[我有一个 Python 程序，可以使用 PDF 查看器打开 PDF 文件。查看 PDF 时，我用鼠标光标选择一些文本。有没有办法让我的 Python 程序检测我选择的文本？
我知道 PyMuPDF、PyPDF2 和 pdfplumber 等库可用于从 PDF 中提取文本。不过，我正在专门寻找一种方法来检测我在查看 PDF 时以交互方式选择的文本。
如果无法从鼠标光标直接检测，是否有任何替代方法或解决方法可以实现类似的结果？
有什么见解或建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78314645/how-to-detect-selected-text-from-a-pdf-using-python</guid>
      <pubDate>Fri, 12 Apr 2024 06:57:54 GMT</pubDate>
    </item>
    <item>
      <title>不同职业的人发布的文本数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/78314226/the-texts-dataset-posted-by-people-of-different-occupations</link>
      <description><![CDATA[最近，我一直在从事一些 NLP 任务：根据不同职业群体的帖子预测职业。我搜索了 Kaggle 等多个平台，但找不到这样的数据集。我怎样才能找到这个数据集？
我尝试了kaggle和google数据集，但不起作用]]></description>
      <guid>https://stackoverflow.com/questions/78314226/the-texts-dataset-posted-by-people-of-different-occupations</guid>
      <pubDate>Fri, 12 Apr 2024 04:54:13 GMT</pubDate>
    </item>
    <item>
      <title>手语项目（Ai）[已关闭]</title>
      <link>https://stackoverflow.com/questions/78313876/sign-language-project-ai</link>
      <description><![CDATA[目前，我们是一个团队在做毕业设计，做一个手语应用，当我们完成2个模型的训练和评估后，我们想把这2个模型放在main函数中，以便交给后端团队将模型链接到应用程序。这里的问题是
我们如何制作主函数以及完成它需要哪些步骤和库主函数？]]></description>
      <guid>https://stackoverflow.com/questions/78313876/sign-language-project-ai</guid>
      <pubDate>Fri, 12 Apr 2024 02:31:40 GMT</pubDate>
    </item>
    <item>
      <title>训练时间融合网络 - 多少数据？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78313682/training-temporal-fusion-network-how-much-data</link>
      <description><![CDATA[关于多少数据足以训练时间融合变压器，是否有任何经验法则？更具体地说，我有大约 20 个特征，数据集大约有 50 万。这足够吗？对于多大的模型来说？
或者，我有大约 20 个产品，每个产品大约有 500k 行，而不是为每个“产品”训练不同的模型。也许我应该为所有产品训练一个模型以获得一个模型，然后定制它？
我知道这是非常高水平和模糊的，我只是在寻找一些经验法则和指导 - 我是否处于正确的范围，或者我应该去哪里？]]></description>
      <guid>https://stackoverflow.com/questions/78313682/training-temporal-fusion-network-how-much-data</guid>
      <pubDate>Fri, 12 Apr 2024 01:13:06 GMT</pubDate>
    </item>
    <item>
      <title>在运行 ML 项目时如何使用存储在 google Drive 中的数据集？</title>
      <link>https://stackoverflow.com/questions/78312337/how-can-i-use-the-dataset-that-is-stored-in-google-drive-while-running-ml-projec</link>
      <description><![CDATA[我正在运行 SoccerNet 项目，该项目为足球视频生成字幕。
我正在尝试将路径传递到存储数据集的 Google 云端硬盘，即 https://drive.google.com/drive/folders/{folder_id}
我正在运行的命令如下：
python main.py --SoccerNet_path=“https://drive.google.com/drive/folders/{folder_id}” --model_name=new_model --features=baidu_soccer_embeddings.npy --framerate=1 --pool=NetVLAD --window_size_caption=45 --window_size_spotting=15 --NMS_window=30 --num_layers=4 --first_stage=caption --pretrain --GPU=0

我收到操作系统错误：如下
OSError: [Errno 22] 无效参数: &#39;https:https://drive.google.com/drive/folders/{folder_id}?usp=drive_link\\england_epl\\2014-2015\\2015 -02-21 - 18-00 切尔西 1 - 1 伯恩利\\1_baidu_soccer_embeddings.npy&#39;

我尝试使用 google colab，但我没有得到 colab 中预期的输出，因为它没有生成应包含预期字幕的 json 文件。
我正在使用 VS 代码。]]></description>
      <guid>https://stackoverflow.com/questions/78312337/how-can-i-use-the-dataset-that-is-stored-in-google-drive-while-running-ml-projec</guid>
      <pubDate>Thu, 11 Apr 2024 18:15:53 GMT</pubDate>
    </item>
    <item>
      <title>TensorBoard HParams 未显示超参数调整的准确性指标</title>
      <link>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</link>
      <description><![CDATA[我正在 TensorFlow 中进行超参数调整，并使用 TensorBoard 中的 HParams 插件设置了一个实验来记录不同的配置。我的模型正在使用 dropout 和学习率的变化进行训练，并且我正在记录这些参数以及模型的准确性。但是，当我打开 TensorBoard 并导航到 HParams 仪表板时，不会显示与每个试验相关的准确性指标。该表正确显示了超参数，但“准确性”列为空，即使我的代码使用“准确性”作为指标来编译模型并使用 hp.KerasCallback 进行日志记录。我已经验证了模型训练正确，并且标量仪表板等其他 TensorBoard 功能显示了各个时期的准确性趋势。我正在寻求帮助来理解为什么 HParams 表中没有显示准​​确性以及如何解决此问题。
图片：准确度列中缺少值
我使用 TensorBoard 的 HParams 进行超参数调整的代码：
从tensorboard.plugins.hparams导入api作为hp
将张量流导入为 tf
从tensorflow.keras.layers导入Conv2D、MaxPooling2D、Dense、Flatten、Dropout

# 定义超参数
HP_DROPOUT = hp.HParam(&#39;dropout&#39;, hp.Discrete([0.2, 0.3, 0.4]))
HP_LEARNING_RATE = hp.HParam(&#39;learning_rate&#39;, hp.Discrete([1e-2, 1e-3]))

# 设置日志记录
log_dir = &#39;./tensorboard/nn_1&#39;
使用 tf.summary.create_file_writer(log_dir).as_default()：
    hp.hparams_config(
        hparams=[HP_DROPOUT, HP_LEARNING_RATE],
        指标=[hp.Metric(&#39;准确度&#39;, display_name=&#39;准确度&#39;)]
    ）

# 训练函数
def train_test_model(hparams, session_num):
    model_name = f“model_1_session_{session_num}”
    print(f&quot;使用超参数 {hparams} 训练 {model_name}...&quot;)
    模型 = tf.keras.Sequential([
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        MaxPooling2D(pool_size=(2, 2)),
        展平（），
        密集（10，激活=&#39;softmax&#39;）
    ]）
    模型.编译(
        损失=&#39;分类交叉熵&#39;，
        优化器=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),
        指标=[&#39;准确性&#39;]
    ）

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f&#39;{log_dir}/{model_name}&#39;)
    hparams_callback = hp.KerasCallback(writer=f&#39;{log_dir}/{model_name}&#39;, hparams=hparams)

    模型.拟合(
        x_train_reshape, y_train_,
        纪元=3，
        验证数据=（x_val_reshape，y_val），
        回调=[hparams_callback，tensorboard_callback]
    ）

# 对每组超参数进行训练
会话编号 = 0
对于 HP_DROPOUT.domain.values 中的 dropout_rate：
    对于 HP_LEARNING_RATE.domain.values 中的learning_rate：
        hparams = {
            HP_DROPOUT：辍学率，
            HP_LEARNING_RATE：学习率，
        }
        train_test_model(hparams, session_num)
        会话编号 += 1

]]></description>
      <guid>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</guid>
      <pubDate>Tue, 09 Apr 2024 12:14:56 GMT</pubDate>
    </item>
    <item>
      <title>使用 Docker 和 Flask 进行机器学习的性能问题</title>
      <link>https://stackoverflow.com/questions/50464643/performance-issues-with-machine-learning-using-docker-and-flask</link>
      <description><![CDATA[我有一些应用于 json 文件的 python3 代码，代码中有一些神经网络和随机森林。我将代码放入 Docker 容器中，但注意到这些 ML 任务在不使用 Docker 的情况下比使用 Docker 运行得更快。在 Docker 中，我使用 Flask 加载 json 文件并运行代码。当然，我在本地和 Docker 内部使用了相同版本的 python 模块，这些是：

theano 0.8.2
keras 2.0.5
scikit-learn 0.19.0

另外，Flask 是

0.12

起初，我认为 theano 在有 Docker 的情况下可能会使用不同的资源，但它同时运行单 CPU 和单线程。它也没有使用我的 GPU。当我意识到我的随机森林在 Docker 中运行速度也变慢时，我意识到这可能不是 theano。以下是我执行的一系列测试（我对每个测试进行了多次测试，我报告了平均时间，因为这些测试是稳定的）
没有 Docker，没有 Flask：

任务 1（theano + keras 代码）：1.0s 
任务 2（theano + keras 代码）：0.7s
任务 3（scikit-learn 代码）：0.25 秒

Docker (cpus=1) + Flask (调试模式 = True):

T1：6.5秒
T2：2.2秒
T3：0.58s

Docker (cpus=2) + Flask (调试模式 = True):

T1：5.5秒
T2：1.4秒
T3：0.55s

Docker (cpus=2) + Flask (调试模式 = False)：

T1：4.5秒
T2：1.2秒
T3：0.5秒

Docker (cpus=2)（无 Flask，仅调用本地完成的 json 文件）：

T1：2.8s
T2：1.1秒
T3：0.5秒

Flask（调试模式 = True）（无 Docker 容器）：

T1：2.8s
T2：1.5秒
T3：0.2秒

我猜 cpu=1 与 cpu=2 只是将一个 cpu 分配给代码，而第二个 cpu 只是接管一些其他工作。显然，当不使用 Flask 或 Docker 时，时间会有所减少，但仍然无法达到没有 Docker 和 Flask 的速度。有谁猜到为什么会发生这种情况吗？
这是我们如何使用 Flask 运行应用程序的最小代码块
api = Flask(__name__)
pipeline = Pipeline() # 调用多个任务的私有类

@api.route(&quot;/&quot;,methods=[&#39;POST&#39;])
def 条目():
    数据 = request.get_json(force=True)
    数据 = pipeline.process(数据)
    # 这会调用不同的定时任务

如果 __name__ == &quot;__main__&quot;:
    api.run（调试= True，主机=&#39;0.0.0.0&#39;，线程= False）


PS。如果问题缺少任何内容，请原谅我，这是我的第一个 StackOverflow 问题]]></description>
      <guid>https://stackoverflow.com/questions/50464643/performance-issues-with-machine-learning-using-docker-and-flask</guid>
      <pubDate>Tue, 22 May 2018 09:48:58 GMT</pubDate>
    </item>
    <item>
      <title>使用 word2vec 对类别中的单词进行分类</title>
      <link>https://stackoverflow.com/questions/47666699/using-word2vec-to-classify-words-in-categories</link>
      <description><![CDATA[背景
我有带有一些示例数据的向量，每个向量都有一个类别名称（地点、颜色、名称）。
[&#39;约翰&#39;,&#39;杰伊&#39;,&#39;丹&#39;,&#39;内森&#39;,&#39;鲍勃&#39;] -&gt; “名字”
[&#39;黄色&#39;, &#39;红色&#39;, &#39;绿色&#39;] -&gt; &#39;颜色&#39;
[&#39;东京&#39;,&#39;北京&#39;,&#39;华盛顿&#39;,&#39;孟买&#39;] -&gt; “地方”

我的目标是训练一个模型，该模型采用新的输入字符串并预测它属于哪个类别。例如，如果新输入是“紫色”，那么我应该能够将“颜色”预测为正确的类别。如果新输入是“卡尔加里”，它应该将“地点”预测为正确的类别。
方法
我做了一些研究并发现了Word2vec。这个库有一个我可以使用的“相似性”和“最相似性”函数。所以我想到的一种强力方法如下：

接受新的输入。
计算它与每个向量中每个单词的相似度并取平均值。

例如，对于输入“粉红色”，我可以计算其与向量“名称”中单词的相似度，取平均值，然后对其他 2 个向量也执行此操作。给我最高相似度平均值的向量将是输入所属的正确向量。
问题
鉴于我在 NLP 和机器学习方面的知识有限，我不确定这是否是最好的方法，因此我正在寻求有关更好方法的帮助和建议来解决我的问题。我愿意接受所有建议，也请指出我可能犯的任何错误，因为我是机器学习和 NLP 世界的新手。]]></description>
      <guid>https://stackoverflow.com/questions/47666699/using-word2vec-to-classify-words-in-categories</guid>
      <pubDate>Wed, 06 Dec 2017 04:16:35 GMT</pubDate>
    </item>
    </channel>
</rss>