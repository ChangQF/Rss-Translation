<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 19 Mar 2024 09:13:35 GMT</lastBuildDate>
    <item>
      <title>如何解决在Python中导入TensorFlow时出现“unhashable type”错误？</title>
      <link>https://stackoverflow.com/questions/78185218/how-can-i-solve-the-unhashable-type-error-when-importing-tensorflow-in-python</link>
      <description><![CDATA[我在 Virtual Studio Code 中导入 TensorFlow 时遇到问题。我尝试执行我的代码，该代码从导入不同的模块开始。其中一行正在导入 Tensorflow：
将tensorflow导入为tf
这给了我错误：
不可哈希类型：“列表”
文件“链接”，第 10 行，位于
导入tensorflow as tf TypeError：不可散列类型：&#39;list&#39;
首先，我在另一台计算机上使用 Jupyter Notebook 编写了此代码，我可以毫无问题地执行它。在我想要执行它的计算机上遇到此错误后，我尝试重现该错误。我在第一台计算机上安装了 Virtual Studio Code，安装了所有模块并成功执行。好像是设置什么的问题。
为了测试我只执行了这个，这给了我错误：
将tensorflow导入为tf
我在两台计算机上都安装了 Python 3.9.0 和 Tensorflow 2.16.1。经过多次尝试（例如卸载并安装tensorflow，或重置Virtual Studio Code），我决定在这里询问。也许这里有人更了解这个问题:)]]></description>
      <guid>https://stackoverflow.com/questions/78185218/how-can-i-solve-the-unhashable-type-error-when-importing-tensorflow-in-python</guid>
      <pubDate>Tue, 19 Mar 2024 08:44:21 GMT</pubDate>
    </item>
    <item>
      <title>从朱莉娅的玻尔兹曼机采样[关闭]</title>
      <link>https://stackoverflow.com/questions/78184695/sampling-from-boltzmann-machine-in-julia</link>
      <description><![CDATA[对于给定的玻尔兹曼机，如何从玻尔兹曼机获取样本？
玻尔兹曼机是一种亵渎的概率分布

对于给定参数b和w。我需要来自 p(x) 的样本

我怎样才能在 Julia 中做到这一点？我假设系统N的大小是20或30。]]></description>
      <guid>https://stackoverflow.com/questions/78184695/sampling-from-boltzmann-machine-in-julia</guid>
      <pubDate>Tue, 19 Mar 2024 06:57:53 GMT</pubDate>
    </item>
    <item>
      <title>尝试从我的网络摄像头访问实时检测时，y python 代码中出现 ocr 错误 [关闭]</title>
      <link>https://stackoverflow.com/questions/78184460/ocr-error-in-y-python-code-while-trying-to-acces-the-realtime-detection-from-my</link>
      <description><![CDATA[这是我收到的错误：
&lt;小时/&gt;
错误回溯（最近一次调用最后一次）
单元格 In[79]，第 40 行
     37 除外：
     38 通
---&gt; 40 cv2.imshow(&#39;物体检测&#39;, cv2.resize(image_np_with_detections, (800, 600)))
     42 if cv2.waitKey(10) &amp; 0xFF == ord(&#39;q&#39;):
     43 cap.release()

错误：OpenCV(4.9.0) D:\a\opencv-python\opencv-python\opencv\modules\highgui\src\window.cpp:1272：错误：（-2：未指定错误）该功能未实现。使用 Windows、GTK+ 2.x 或 Cocoa 支持重建库。如果您使用的是 Ubuntu 或 Debian，请安装 libgtk2.0-dev 和 pkg-config，然后在函数“cvShowImage”中重新运行 cmake 或配置脚本

我尝试更换网络摄像头，但遇到同样的问题]]></description>
      <guid>https://stackoverflow.com/questions/78184460/ocr-error-in-y-python-code-while-trying-to-acces-the-realtime-detection-from-my</guid>
      <pubDate>Tue, 19 Mar 2024 05:44:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 MinMaxScaler() 进行特征缩放</title>
      <link>https://stackoverflow.com/questions/78184444/feature-scaling-with-minmaxscaler</link>
      <description><![CDATA[我有 31 个特征要输入到 ML 算法中。这 22 个特征值已经在 0 到 1 的范围内。其余 9 个特征在 0 到 750 之间变化。我的疑问是，如果我选择应用 MinMaxScaler() 并将范围设置为 (0,1)，是应该对所有特征进行缩放还是仅对所需范围之外的 9 个特征进行缩放？什么比较合适？]]></description>
      <guid>https://stackoverflow.com/questions/78184444/feature-scaling-with-minmaxscaler</guid>
      <pubDate>Tue, 19 Mar 2024 05:40:04 GMT</pubDate>
    </item>
    <item>
      <title>pytorch CUDA版本和CUDA实际如何工作？</title>
      <link>https://stackoverflow.com/questions/78184358/how-does-the-pytorch-cuda-version-and-cuda-actually-works</link>
      <description><![CDATA[这是我的 conda 环境的软件包列表：

如你所见，我还没有安装cudatoolkit，并且nvcc命令无法使用。但我确实安装了 CUDA 版本的 pytorch。
但是，当我在 python 中导入 torch 并检查 torch.cuda.is_available() 时，它返回 Ture。
我什至运行这个测试脚本：
导入火炬
从火炬导入 nn
从 torch.nn 导入模块
从 torch.optim.lr_scheduler 导入 LambdaLR


测试网类（模块）：
    def __init__(self) -&gt;; __init__(self) -&gt;没有任何：
        超级().__init__()
        self.线性 = nn.Linear(10,10)

    def 前向（自身，x）：
        返回自线性(x)
    

如果 __name__==“__main__”：
    如果 torch.cuda.is_available():
        设备=“cuda”；
    别的：
        设备=“CPU”
    print(f“使用设备 {device}”)
    test_samples = torch.rand([32,10]).to(设备)
    gt_matrix = torch.eye(10).to(设备)
    目标 = torch.matmul(test_samples, gt_matrix)

    模型 = TestNet().to(设备)

    优化器 = torch.optim.SGD(model.parameters(), lr=1)
    标准 = nn.MSELoss()
    调度程序 = LambdaLR(优化器, lr_lambda=lambda x: min(x, 24)/24)

    对于范围（128）内的纪元：
        logits = 模型（测试样本）
        损失=标准（logits，目标）
        Learning_rate = Optimizer.param_groups[0][“lr”]

        优化器.zero_grad()
        loss.backward()
        优化器.step()
        调度程序.step()

        print(f&quot;历元 {epoch+1}/{24}, 损失 {loss.item()}, lr {learning_rate}&quot;)
    
    print(&quot;学习矩阵：&quot;)
    打印（model.state_dict（）[“线性.权重”]）

并且运行成功。
所以我很好奇 pytorch CUDA 版本实际上是如何工作的？是否需要预装CUDA工具包？另外，通过 conda install cudatoolkit 和 conda install cuda 安装 CUDA 甚至通过图形安装程序安装有什么区别？]]></description>
      <guid>https://stackoverflow.com/questions/78184358/how-does-the-pytorch-cuda-version-and-cuda-actually-works</guid>
      <pubDate>Tue, 19 Mar 2024 05:18:07 GMT</pubDate>
    </item>
    <item>
      <title>选择模型训练参数（和层）</title>
      <link>https://stackoverflow.com/questions/78184146/selecting-model-training-params-and-layers</link>
      <description><![CDATA[在代码中（下面从tensorflow.org“针对初学者的 Tensorflow 2 快速入门”复制/粘贴了所有许多变量，例如：

纪元数
批量大小
学习率
层（或层的组合）
每层的激活函数
损失函数
单位（密集层的参数）
比率（针对 Dropout 层）
等等...

除了“猜测和猜测”之外，是否还有一些受过教育的程序？测试”，以便找到“最佳”方案。参数组合 --- 或者甚至知道从哪个参数组合开始？
我是否缺乏“专家”所需要的一些知识（或信息）？或博士学位将为我提供，这将使我有能力选择“最好的”。参数（第一次）？
我当然明白最后一层 Dense(10) 的一些参数需要是 10 个单位来表示 10 个类...而且我知道 10,000 层可能不太好...所以我不是在谈论极端值或最后一个密集层中的 10 之类的东西，而是其他参数。
含义 - 作为示例 - 在下面的代码中，假设我更改了行中的单位
tf.keras.layers.Dense（32，激活=&#39;relu&#39;）

到其他 64（而不是 32）。
对于下面的例子，
64 肯定比 32 更好（或更差）有什么理由吗？
是否有理由添加另一个密集层会更好（或更差）？
是否有明确的理由说明为什么 6 个 epoch 的训练会比 5 个 epoch 更好（或更差）？
我知道，如果我理解模型最终所做的所有计算，我就会得到我的答案......这意味着，因为“某人”写了那个代码，有能力知道这一点并不是不可能的......
所有教程和内容都令人非常沮丧。那里的网站关注的是如何，而不是为什么。
（任何建议/链接/资源将不胜感激）
提前非常感谢各位专家！
导入argparse
将张量流导入为 tf


def 过程（参数）：
    ( X_train, y_train ), ( X_test, y_test ) = tf.keras.datasets.mnist.load_data()
    X_train = X_train / 255.0
    X_测试 = X_测试 / 255.0

    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    模型 = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(10)
    ]）

    model.compile(optimizer=&#39;adam&#39;,loss=loss_fn,metrics=[&#39;accuracy&#39;])

    hist = model.fit(X_train,
                     y_火车，
                     批量大小=32，
                     纪元=5，
                     验证分割=0.2，
                     随机播放=真）


如果 __name__ == &#39;__main__&#39;:
    解析器 = argparse.ArgumentParser()
    parser.add_argument(&#39;--batch_size&#39;, type=int, default=32)
    args = parser.parse_args()
    过程（参数）

``
]]></description>
      <guid>https://stackoverflow.com/questions/78184146/selecting-model-training-params-and-layers</guid>
      <pubDate>Tue, 19 Mar 2024 03:47:31 GMT</pubDate>
    </item>
    <item>
      <title>TF2 和 python 中的 BERT 预处理器模型存在问题</title>
      <link>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</link>
      <description><![CDATA[我正在尝试使用 BERT 来做一个文本分类项目。但是我一直遇到这个错误
`
ValueError Traceback（最近一次调用最后一次）
单元格 In[37]，第 4 行
      2 text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
      3 bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
----&gt; 4 preprocessed_text = bert_preprocess(text_input)
      5 bert_encoder = hub.KerasLayer(encoder_url,
      6 可训练=真，
      7 名称=&#39;BERT_编码器&#39;)
      8 个输出 = bert_encoder(preprocessed_text)
ValueError：调用层“预处理”时遇到异常（类型 KerasLayer）。
KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。

调用层“预处理”接收的参数（类型 KerasLayer）：
  输入=
  • 培训=无

KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。
调用层“预处理”接收的参数（类型 KerasLayer）：
• 输入=
• 训练=无`
构建此模型时：
&lt;前&gt;&lt;代码&gt;
preprocess_url = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3&#39;
编码器网址 = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/bert-en-uncased-l-12-h-768-a-12/versions/2&#39;

# Bert 层
text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
预处理文本 = bert_preprocess(text_input)
bert_encoder = hub.KerasLayer(encoder_url,
                              可训练=真，
                              名称=&#39;BERT_编码器&#39;)
输出= bert_encoder（预处理文本）

# 神经网络层
l = tf.keras.layers.Dropout(0.1)(输出[&#39;pooled_output&#39;])
l = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;, name=&#39;输出&#39;)(l)

# 构建最终模型
模型 = tf.keras.Model(输入=[text_input], 输出=[l])

我看过无数的教程，甚至使用了张量流文档上的教程，即使我复制和粘贴，它们仍然不起作用。我尝试过不同版本的 tf、tf-text 和 tf-hub。我在这个项目中使用了tensorflow-gpu-jupyter docker 容器。
这是我安装库的方法：
!pip install “tensorflow-text”
!pip install “tf-models-official”
!pip install “tensorflow-hub”

版本是：
张量流：2.16.1
张量流文本：2.16.1
张量流中心：0.16.1
我看到的有关此问题的所有其他论坛都说要执行 tf.config.run_functions_eagerly(True) 但这不起作用。
任何事情都会有所帮助。如果您知道如何解决请回答。]]></description>
      <guid>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</guid>
      <pubDate>Tue, 19 Mar 2024 01:42:01 GMT</pubDate>
    </item>
    <item>
      <title>分割接触谷物的最快方法：ML 模型还是传统图像处理？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78183577/fastest-approach-for-segmenting-touching-cereal-grains-ml-model-or-traditional</link>
      <description><![CDATA[对于分割接触谷物的图像，最快的方法是什么：训练 ML 模型还是使用传统图像处理工具开发算法？
我想分别获取每个颗粒的图像。
示例图像如下：
米粒]]></description>
      <guid>https://stackoverflow.com/questions/78183577/fastest-approach-for-segmenting-touching-cereal-grains-ml-model-or-traditional</guid>
      <pubDate>Mon, 18 Mar 2024 23:50:14 GMT</pubDate>
    </item>
    <item>
      <title>java中异常检测的最佳算法或库[关闭]</title>
      <link>https://stackoverflow.com/questions/78183122/best-algorithm-or-library-for-anomaly-detection-in-java</link>
      <description><![CDATA[我们正在做一个项目，尝试测量数据集中的异常情况，其中有我们公司的员工（大约 1000 名），并且每个人都拥有进入许多门的徽章。我们拥有完整的用户数据，例如职位、部门、职位代码、经理等。
每个人可能可以进入 10-100 扇门，我们正在尝试编写一个 java 程序来找到奇怪的人。可能是这样的情况，比如你是唯一一个拥有 5 号门的头衔的人，或者可能很多为 A 经理工作的人都有 10 号门，但为 B 经理工作的这个人也有它。
我们看到的大多数非结构化学习的例子都有数值。我们不这样做。事实上，像系统管理员和高级系统管理员这样的类似头衔是“接近”的。所以可能应该以某种方式考虑到它们是相似的。因此，如果一位 SR 还拥有 7 号门和 5 号门系统管理员，那可能并不是什么奇怪的事情。
理想的结果是列出似乎不合适的人-门组合。也许是一个数字置信值。门本身是一个固定值。但标准是诸如头衔、职位代码或经理层级（向上一级或四级）之类的词。
看来我们需要首先评估用户参数（如标题、部门）中的语言相似性，然后使用这些多值来运行门统计数据。
这似乎是机器学习应该能够做到的事情，但我们在这方面有点新手。有人有开源 java 库、想法或教程的链接吗？尤其是如何链接“相似”的内容。头衔或经理层次结构放在一起，以免将一个人的团队标记为完全异常。]]></description>
      <guid>https://stackoverflow.com/questions/78183122/best-algorithm-or-library-for-anomaly-detection-in-java</guid>
      <pubDate>Mon, 18 Mar 2024 21:35:09 GMT</pubDate>
    </item>
    <item>
      <title>用于西班牙语文本校正的人工智能模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78182982/ai-models-for-text-correction-in-spanish</link>
      <description><![CDATA[我们需要实现不同的人工智能模型来将音频转录为文本，然后将该文本转换为特定的 Word 格式。目前，这是使用从 Python 中的 Hugging Face 获得的模型来完成的，但我正在尝试找到一个可以让我纠正音频转录中的不准确之处的模型，因为这些模型往往非常字面意思以及说话者犯下的任何发音错误或错误保留在正文中。
所以，我需要一个带有人工智能的校正功能，可以进行西班牙语文本校正。我一直在寻找一些，但大部分都是英文的。您是否知道有任何免费或付费模型可以实现此目的？
我寻找了一些人工智能训练模型，但修正是英文的，我需要一个西班牙文的。我还在考虑为这项任务训练我自己的 T5 模型。]]></description>
      <guid>https://stackoverflow.com/questions/78182982/ai-models-for-text-correction-in-spanish</guid>
      <pubDate>Mon, 18 Mar 2024 20:58:11 GMT</pubDate>
    </item>
    <item>
      <title>从哪里可以找到 ML 神经网络的代码？</title>
      <link>https://stackoverflow.com/questions/78181767/from-where-i-can-find-the-codes-for-ml-neural-network</link>
      <description><![CDATA[我想在 Python 上研究机器学习神经网络，以预测遗传相互作用。我正在寻找有关编码和从公共数据生成结果的实践学习经验。
我发现的大多数都是商业或理论信息书籍，没有实用代码。如果您有任何消息来源可以指导我完成此操作，我真的很感激。]]></description>
      <guid>https://stackoverflow.com/questions/78181767/from-where-i-can-find-the-codes-for-ml-neural-network</guid>
      <pubDate>Mon, 18 Mar 2024 16:50:29 GMT</pubDate>
    </item>
    <item>
      <title>为用户创建推荐系统 API 时出现问题</title>
      <link>https://stackoverflow.com/questions/78177747/issue-while-creating-an-api-for-recommendation-system-for-users</link>
      <description><![CDATA[fromflask导入Flask，jsonify，request
将 pandas 导入为 pd
将 numpy 导入为 np
从 sklearn.metrics.pairwise 导入 cosine_similarity
导入作业库

应用程序=烧瓶（__名称__）

# 加载预训练的用户-用户相似度矩阵
user_user_similarity_matrix = joblib.load(&#39;预测模型/user_user_similarity_matrix.pkl&#39;)

# 从 Excel 文件加载数据集（API 功能不需要此行）
data = pd.read_excel(&#39;预测模型/DDDD.xlsx&#39;)

# 将占位符列“PurchasedYes”添加到数据 DataFrame
数据[&#39;已购买&#39;] = 1

@app.route(&#39;/推荐&#39;, 方法=[&#39;POST&#39;])
def 推荐():
    user_id = request.json[&#39;user_id&#39;] # 从请求中获取用户ID
    推荐=generate_recommendations（user_user_similarity_matrix，数据，user_id，n_recommendations = 10）
    return jsonify({&#39;user_id&#39;: user_id, &#39;推荐&#39;: 推荐})


defgenerate_recommendations(user_similarity_matrix, data, user_id, n_recommendations=10):
    # 透视表以获得矩阵，其中行代表客户，列代表项目
    customer_item_matrix = data.pivot_table(index=&#39;客户&#39;, columns=&#39;SalesItem&#39;, value=&#39;PurchasedYes&#39;, fill_value=0)

    # 获取用户购买的商品
    user_purchases = customer_item_matrix.loc[user_id].values.reshape(1, -1) # 重塑为行向量

    # 计算用户-用户相似度分数（确保 user_similarity_matrix 与客户具有相同的行数）
    user_similarity_scores = user_similarity_matrix[user_id].reshape(1, -1) # 转置矩阵

    # 选项 1（确保 user_similarity_matrix 具有兼容的维度）
    # 检查如何加载“user_user_similarity_matrix”以使其具有与客户相同的行数

    # 选项 2（重塑 user_similarity_scores 以匹配 customer_item_matrix.columns）
    # user_similarity_scores = user_similarity_scores.reshape(-1, customer_item_matrix.shape[1])

    # 相似用户购买的加权总和
    加权购买 = customer_item_matrix.values.dot(user_similarity_scores.T)

    # 过滤掉用户已经购买过的商品
    加权购买[customer_item_matrix.loc[user_id] != 0] = -np.inf

    # 获取前n个推荐的索引
    top_indices = np.argsort(weighted_purchases.flatten())[::-1][:n_recommendations]

    # 获取推荐商品
    推荐项目 = [customer_item_matrix.columns[i] for i in top_indices]

    返回推荐的_项目


如果 __name__ == &#39;__main__&#39;:
    应用程序运行（调试=真）

对于此代码，我收到错误，我无法解决此错误。
错误：weighted_purchases = customer_item_matrix.values.dot(user_similarity_scores.T)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^
ValueError：形状 (35,3725) 和 (32,1) 未对齐：3725 (dim 1) != 32 (dim 0)

我确实尝试通过使两个矩阵的尺寸相同来修改代码。也用过chatgpt但是没用。如果您可以为我修改这段代码，请给我好的解决方案，这将是巨大的帮助。
这是我的数据集概述。

这也是 pkl 文件和数据集的驱动器链接。
文字]]></description>
      <guid>https://stackoverflow.com/questions/78177747/issue-while-creating-an-api-for-recommendation-system-for-users</guid>
      <pubDate>Mon, 18 Mar 2024 02:58:03 GMT</pubDate>
    </item>
    <item>
      <title>关于机器学习和数值训练数据的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78172418/question-about-machine-learning-and-numerical-training-data</link>
      <description><![CDATA[如果您使用 is_away 之类的内容作为数据中的数字字段，人工智能将对其进行训练以预测团队获胜的可能性。所以 1 代表 true，0 代表 false，那么是否应该将其更改为 is_home 之类的内容并翻转值，或者人工智能最终会在预测诸如获胜机会之类的内容时了解到 0 更有可能获胜？
我认为另一个很好的例子是海拔高度，而你的目标值是点。在大多数情况下，海拔越高，得分越少或赛道时间越慢。我假设人工智能理解海拔越高意味着它更有可能预测较低的目标值。在看到一些奇怪的预测后，我将玩家的高度提高了 5k，并在一场游戏中复制了所有其他字段，并且它总是预测该游戏的目标值更高。所以我对人工智能如何处理更高的数值感到困惑。
另请注意，我正在使用 relu 激活和 500k 行数据。我将随机更改单行的高度并使用相同的参数重新训练。与之前的训练数据相比，该行的预测值将从 20 变为 25，其他任何事情都不会发生变化...所以总结一下我的问题，应该反转对预测目标值产生负面影响的数值数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/78172418/question-about-machine-learning-and-numerical-training-data</guid>
      <pubDate>Sat, 16 Mar 2024 15:57:58 GMT</pubDate>
    </item>
    <item>
      <title>Keras 中的形状不兼容，但 model.summary() 似乎没问题</title>
      <link>https://stackoverflow.com/questions/78066771/incompatible-shapes-in-keras-but-model-summary-seems-ok</link>
      <description><![CDATA[下面是我的模型
# 由于我们要预测每个时间步的值，因此我们设置 return_sequences=True
输入=输入(batch_shape=ip_shape)
mLSTM = LSTM（单位=32，return_sequences=True，stateful=True）（输入）
mDense = Dense(单位=32，激活=&#39;线性&#39;)(输入)
mSkip = Add()([mLSTM, mDense])

mSkip = 密集（单位=1，激活=&#39;线性&#39;）（mSkip）
模型=模型（输入，mSkip）

亚当 = 亚当(learning_rate=0.01)
model.compile(优化器=adam, 损失=total_loss)
模型.summary()

模型摘要
型号：“model_3”
_______________________________________________________________________________________________
 层（类型）输出形状参数#连接到
=================================================== ===============================================
 input_5 (输入层) [(104, 22050, 1)] 0 []
                                                                                                  
 lstm_4 (LSTM) (104, 22050, 32) 4352 [&#39;input_5[0][0]&#39;]
                                                                                                  
 稠密_5（密集）(104, 22050, 32) 64 [&#39;input_5[0][0]&#39;]
                                                                                                  
 add_3 (添加) (104, 22050, 32) 0 [&#39;lstm_4[0][0]&#39;,
                                                                     &#39;dense_5[0][0]&#39;]
                                                                                                  
 稠密_6（密集）(104, 22050, 1) 33 [&#39;add_3[0][0]&#39;]
                                                                                                  
=================================================== ===============================================
总参数：4449 (17.38 KB)
可训练参数：4449 (17.38 KB)
不可训练参数：0（0.00 字节）
_______________________________________________________________________________________________

我正在使用自定义损失函数。我不确定反向传播时是否会弄乱形状
def Total_loss(y_true, y_pred):
    比率 = 0.5
    dc_loss = math_ops.pow(math_ops.subtract(math_ops.mean(y_true, 0), math_ops.mean(y_pred, 0)), 2)
    dc_loss = math_ops.mean(dc_loss, axis=-1)
    dc_energy = math_ops.mean(math_ops.pow(y_true, 2), axis=-1) + 0.00001
    dc_loss = math_ops.div(dc_loss, dc_energy)

    esr_loss = math_ops.squared_difference(y_pred, y_true)
    esr_loss = math_ops.mean(esr_loss, axis=-1)
    esr_energy = math_ops.mean(math_ops.pow(y_true, 2), axis=-1) + 0.00001
    esr_loss = math_ops.div(esr_loss, esr_energy)

    返回（比率）*dc_loss +（1-比率）*esr_loss

最后的错误：让我知道是否需要整个回溯
InvalidArgumentError：图形执行错误：

...

不兼容的形状：[104,22050,32] 与 [32,22050,1]
     [[{{节点gradient_tape/total_loss/BroadcastGradientArgs}}]] [Op：__inference_train_function_9604]

设置 stateful=False 似乎有效，但我不明白为什么]]></description>
      <guid>https://stackoverflow.com/questions/78066771/incompatible-shapes-in-keras-but-model-summary-seems-ok</guid>
      <pubDate>Tue, 27 Feb 2024 10:22:55 GMT</pubDate>
    </item>
    <item>
      <title>如何利用 GPU 减少 xgboost 的处理时间？</title>
      <link>https://stackoverflow.com/questions/77643788/how-can-i-reduce-processing-time-with-xgboost-by-utilizing-my-gpu</link>
      <description><![CDATA[我正在关注数据营的本教程，他们有一件事提到的是利用 GPU 来加快处理时间。他们甚至说它“速度极快”。
然而，我看到了相反的结果。对于下面的代码块，在 10k 提升的情况下，我看到在我的 params 中传递 “hist” 大约需要 30 秒，而在 ” 中传递则只需一分多钟。 gpu_hist&quot; 与我的 params 一起传递。
使用 “gpu_hist” 时，我的 GPU 的使用率上限为 40%，使用 “hist” 时，所有 24 个逻辑核心的使用率上限为 100%
params = {“objective”: “reg:squarederror”, “tree_method”: “gpu_hist”, “subsample”: 0.8,
    “colsample_bytree”：0.8}

evals = [(dtrain_reg, “训练”),(dtest_reg, “验证”)]

n = 10000


模型 = xgb.train(
   参数=参数，
   dtrain=dtrain_reg,
   num_boost_round=n,
   评估=评估，
   详细评估=50，
）

我正在尝试在 jupyter 笔记本的 VSCode 中运行它。

我已安装 CUDA 工具包和 cuDNN
我已检查它们是否已添加到路径中
我已确保安装了正确版本的 xgboost 来使用 GPU。
数据集有 53k 行 10 列，所以我不认为数据集太小
我已确认兼容性（使用 RTX 2060）

我问过 chatGPT，在网上搜索过，甚至问过我正在学习的课程中的导师，但无法诊断为什么“gpu_hist”花费了这么长时间。 vs 只是“历史”。
4 个月前，Stack Overflow 上还有另一个类似问题其响应为零。]]></description>
      <guid>https://stackoverflow.com/questions/77643788/how-can-i-reduce-processing-time-with-xgboost-by-utilizing-my-gpu</guid>
      <pubDate>Tue, 12 Dec 2023 05:02:17 GMT</pubDate>
    </item>
    </channel>
</rss>