<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 23 Dec 2023 06:16:49 GMT</lastBuildDate>
    <item>
      <title>运行时错误：形状“[8192]”对于大小 8256 的输入无效</title>
      <link>https://stackoverflow.com/questions/77706527/runtimeerror-shape-8192-is-invalid-for-input-of-size-8256</link>
      <description><![CDATA[导入火炬
将 torch.nn 导入为 nn
从 torch.nn 导入功能为 F

批量大小 = 64
块大小 = 128
最大迭代数 = 5000
评估间隔 = 200
学习率 = 3e-4
设备 = &#39;cuda&#39; 如果 torch.cuda.is_available() else &#39;cpu&#39;
评估次数 = 100

火炬.manual_seed(5452)

打开（&#39;DiscordGPT_data.txt&#39;，&#39;r&#39;，encoding=&#39;utf-8&#39;）作为f：
    文本 = f.read()

字符=排序（列表（集合（文本）））
vocab_size = len(字符)

stoi = { ch:i for i,ch in enumerate(characters) }
itos = { i:ch for i,ch in enumerate(characters) }
编码 = lambda s: [stoi[c] for c in s]
解码 = lambda l: &#39;&#39;.join(itos[i] for i in l)

数据 = torch.tensor(编码(文本), dtype=torch.long)
n = int(0.9*len(数据))
训练数据 = 数据[:n]
评估数据 = 数据[n:]

def get_batch(分割):
    数据 = train_data 如果 split == &#39;train&#39; 否则评估_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i:i+block_size+1] for i in ix])
    x, y = x.to(设备), y.to(设备)

    返回 x、y

@torch.no_grad()
defestimate_loss():
    输出 = {}
    模型.eval()
    对于 [&#39;train&#39;, &#39;val&#39;] 中的分割：
        损失 = torch.zeros(eval_iters)
        对于范围内的 k(eval_iters)：
            X, Y = get_batch(分割)
            logits，损失 = 模型（X，Y）
            损失[k] = loss.item()
        out[split] = 损失.mean()
    模型.train()
    返回

类 BigramLangModel(nn.Module):

    def __init__(自身, vocab_size):
        超级().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

    def 前进（自我，idx，目标=无）：

        logits = self.token_embedding_table(idx)

        如果目标为无：
            损失=无
        别的：
            B、T、C = logits.shape
            logits = logits.view(B*T, C)
            目标 = 目标.view(B*T)
            损失 = F.cross_entropy(logits, 目标)

        返回 logits，损失
    
    def 生成（自我，idx，max_new_tokens）：

        对于 _ 在范围内（max_new_tokens）：
            logits，损失 = self(idx)
            logits = logits[:, -1, :]
            probs = F.softmax(logits, 暗淡=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), 暗淡=1)

        返回idx

模型 = BigramLangModel(vocab_size)
m = model.to(设备)

优化器 = torch.optim.AdamW(m.parameters(), lr=learning_rate)

对于范围内的迭代器（max_iters）：

    如果 iter % eval_interval == 0:
        损失=estimate_loss()
        print(f&quot;step {iter}: train loss {losses[&#39;train&#39;]:.4f}, val loss {losses[&#39;val&#39;]:.4f}&quot;)
        
    xb, yb = get_batch(&#39;火车&#39;)

    logits，损失 = 模型（xb，yb）
    优化器.zero_grad(set_to_none = True)
    loss.backward()
    优化器.step()

上下文 = torch.zeros((1,1), dtype = torch.long, 设备 = 设备)
print(解码(m.generate(上下文, max_new_tokens=500)[0].tolist()))

很好奇哪里出了问题，即使我减少批量和块大小，仍然会出现运行时错误，调整数字并没有真正的帮助。我对 ML/AI 领域还很陌生，因此任何有关如何前进的指示都会有所帮助。这应该是 Nanogpt 的复制品，并试图自己弄清楚。]]></description>
      <guid>https://stackoverflow.com/questions/77706527/runtimeerror-shape-8192-is-invalid-for-input-of-size-8256</guid>
      <pubDate>Sat, 23 Dec 2023 03:21:19 GMT</pubDate>
    </item>
    <item>
      <title>在机器学习模型中对 NDVI 和 LST 数据使用相同的空间分辨率</title>
      <link>https://stackoverflow.com/questions/77705553/using-same-spatial-resolution-for-ndvi-and-lst-data-in-machine-learning-model</link>
      <description><![CDATA[我目前正在开发干旱预报机器学习模型，这是我第一次处理卫星数据。我的两个数据源是来自 Google Earth Engine 的用于获取 NDVI 值的 MOD13A1 数据集和用于获取 LST 值的 MOD11A1 数据集。感兴趣的区域是南加州。根据 Google Earth Engine，MOD13A1 的默认分辨率为 500m，而 MOD11A1 的默认分辨率为 1000m。
我是否需要对这两种产品使用相同的空间分辨率才能最大限度地提高模型的性能？如果是这样，那会是什么决议？
我尝试更改 NDVI 的 getRegion() 方法中的“scale”参数，当我这样做时，我得到了不同的值。这就是我提出问题的原因。]]></description>
      <guid>https://stackoverflow.com/questions/77705553/using-same-spatial-resolution-for-ndvi-and-lst-data-in-machine-learning-model</guid>
      <pubDate>Fri, 22 Dec 2023 19:46:46 GMT</pubDate>
    </item>
    <item>
      <title>我无法将数组传递给 svm 模型，它显示使用序列设置数组元素的错误</title>
      <link>https://stackoverflow.com/questions/77705514/i-am-not-able-to-pass-an-array-to-svm-model-it-is-showing-an-error-of-setting-an</link>
      <description><![CDATA[由于我正在处理音频数据集，所以首先我提取了 MFCC 特征并将其存储到列表中，然后我进行了填充和展平，我收到了此警告，但我忽略了它一段时间
我不能在这里将数据类型声明为对象，因为我必须在 Svm 模型中传递它
mfcc 功能及其数组的代码
SVM 代码：
svm 代码和错误
那么任何人都可以更正代码，以便我可以将其传递到我的 Svm 模型中，并且我可以进行音频分类吗？]]></description>
      <guid>https://stackoverflow.com/questions/77705514/i-am-not-able-to-pass-an-array-to-svm-model-it-is-showing-an-error-of-setting-an</guid>
      <pubDate>Fri, 22 Dec 2023 19:34:37 GMT</pubDate>
    </item>
    <item>
      <title>如何解决机器学习中的值错误？</title>
      <link>https://stackoverflow.com/questions/77705470/how-do-i-solve-a-value-error-in-machine-learning</link>
      <description><![CDATA[我正在尝试使用朴素贝叶斯选择和训练我的模型，并且正在处理糖尿病数据集，但我不断收到如下值错误：
ValueError Traceback（最近一次调用最后一次）
〜\ AppData \ Local \ Temp \ ipykernel_15552 \ 3536100043.py 在？（）
      1 模型 = GaussianNB()
----&gt; 2 model.fit(X_train, y_train)

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ sklearn \ naive_bayes.py 在？（自我，X，y，sample_weight）
    263 返回实例本身。
    第264章
    第265章
    266 y = self._validate_data(y=y)
--&gt; [第 267 章]
    第268章
    第269章）

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ sklearn \ naive_bayes.py 在？（自我，X，y，类，_refit，sample_weight）
    第424章
    第425章
    第426章
    第427章
--&gt;第428章
    第429章
    第430章
    第431章

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ sklearn \ base.py 在？（自我，X，y，重置，validate_separately，** check_params）
    [580] 第580章不在 check_y_params 中：
    第581章
    第582章
    第583章：
--&gt;第584章
    第585章
    第586章
    第587章

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ sklearn \ utils \ validation.py 中？（X，y，accept_sparse，accept_large_sparse，dtype，order，copy，force_all_finite，ensure_2d，allow_nd，multi_output， Ensure_min_samples、ensure_min_features、y_numeric、估计器）
   第1102章
   第1103章 1103
   第1104章）
   1105
-&gt;第1106章
   第1107章
   第1108章
   第1109章

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ sklearn \ utils \ validation.py 在？（数组，accept_sparse，accept_large_sparse，dtype，order，copy，force_all_finite，ensure_2d，allow_nd，ensure_min_samples，ensure_min_features，估计器，输入名称）
    第876章）
    第877章
    第878章：
    第879章
--&gt;第880章
    第881章
    第882章
    第883章）

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ sklearn \ utils \ _array_api.py 在？（数组，dtype，顺序，复制，xp）
    第181章
    182 xp，_ = get_namespace（数组）
    183 if xp.__name__ in {“numpy”,“numpy.array_api”}：
    [第 184 章] 第 184 章
--&gt; array[185] = numpy.asarray(数组, order=order, dtype=dtype)
    186 return xp.asarray(数组，复制=复制)
    187 其他：
    [第 188 章]

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ pandas \ core \ generic.py？（自我，dtype）
   1996 def __array__(self, dtype: npt.DTypeLike | None = None) -&gt; np.ndarray：
   1997 值 = self._values
-&gt; 1998 arr = np.asarray（值，dtype = dtype）
   1999 如果 (
   2000 astype_is_view（值.dtype，arr.dtype）
   2001 和 using_copy_on_write()

ValueError：无法将字符串转换为浮点数：&#39;F&#39;

我尝试再次运行所有代码单元，因为我使用的是 Jupyter Notebook，但结果与我不断得到的结果相同。]]></description>
      <guid>https://stackoverflow.com/questions/77705470/how-do-i-solve-a-value-error-in-machine-learning</guid>
      <pubDate>Fri, 22 Dec 2023 19:21:17 GMT</pubDate>
    </item>
    <item>
      <title>Keras 变分自动编码器的输入 (X) 和输出层 (Y) 不匹配，无法运行模型</title>
      <link>https://stackoverflow.com/questions/77705287/input-x-and-output-layers-y-of-keras-variational-autoencoder-dont-match-ca</link>
      <description><![CDATA[我正在尝试在 Keras 中构建一个变分自动编码器，输入形状为 X= (1,50) 和 Y= (1,20) .
我做了 2 个输入，一个用于 Y 的预测，第二个用于重建，我将用它来生成新的简单。重建训练得很好，但是模型无法学习。
0.77 中的 val_predictor_loss 堆栈
有什么问题吗？
这是我的代码：
&lt;前&gt;&lt;代码&gt;# %% [代码]
从 keras.layers 导入 Lambda、输入、密集、重塑、重复向量、丢弃
从 keras.models 导入模型
从 keras.datasets 导入 mnist
从 keras.losses 导入 mse，binary_crossentropy
从 keras.utils 导入plot_model
从 keras 导入后端为 K
从 keras.constraints 导入 unit_norm, max_norm
将张量流导入为 tf

从 scipy 导入统计数据
将 pandas 导入为 pd
将 numpy 导入为 np
导入 matplotlib
将 matplotlib.pyplot 导入为 plt
导入argparse
导入操作系统
从 sklearn.manifold 导入 MDS
从 sklearn.model_selection 导入 StratifiedKFold
从 sklearn.metrics 导入mean_squared_error, r2_score
从 keras.layers 导入输入、密集、扁平化、Lambda、Conv1D、BatchNormalization、MaxPooling1D、激活
从 keras.models 导入模型
将 keras.backend 导入为 K
将 numpy 导入为 np

从 mpl_toolkits.mplot3d 导入 Axes3D



# 重新参数化技巧
# 采样 eps = N(0,I)，而不是从 Q(z|X) 采样
# z = z_mean + sqrt(var)*eps
def 采样（参数）：
    “”“通过对各向同性单位高斯进行采样来重新参数化技巧。
    # 参数：
        args（张量）：Q(z|X) 的方差均值和对数
    # 返回：
        z（张量）：采样的潜在向量
    ”“”

    z_mean、z_log_var = args
    批次 = K.shape(z_mean)[0]
    暗淡 = K.int_shape(z_mean)[1]
    # 默认情况下，random_normal 的mean=0 和std=1.0
    epsilon = K.random_normal(shape=(batch, dim))
    thre = K.random_uniform(shape=(batch,1))
    返回 z_mean + K.exp(0.5 * z_log_var) * epsilon


# 加载我的数据
训练特征 = X
ground_truth_r = Y

np.random.seed(种子=0)
原始尺寸 = 32

# 定义VAE模型组件
输入形状_x = (32, )
输入形状_r = (16, )
中间暗度 = 32
潜伏暗度 = 32

# 编码器网络
input_x = 输入（形状=input_shape_x，名称=&#39;encoder_input&#39;）
input_x_dropout = Dropout(0.25)(inputs_x)
inter_x1 = 密集（128，激活=&#39;tanh&#39;）（inputs_x_dropout）
inter_x2 = 密集（intermediate_dim，激活=&#39;tanh&#39;）（inter_x1）
z_mean = 密集（latent_dim，名称=&#39;z_mean&#39;）（inter_x2）
z_log_var = 密集（latent_dim，名称=&#39;z_log_var&#39;）（inter_x2）
z = Lambda(采样, output_shape=(latent_dim,), name=&#39;z&#39;)([z_mean, z_log_var])
编码器 = 模型(inputs_x, [z_mean, z_log_var, z], name=&#39;编码器&#39;)

# 用于重建的解码器网络
Latent_inputs = 输入（形状=（latent_dim，），名称=&#39;z_sampling&#39;）
inter_y1 = 密集（intermediate_dim，激活=&#39;tanh&#39;）（latent_inputs）
inter_y2 = 密集（128，激活=&#39;tanh&#39;）（inter_y1）
输出重建 = 密集（original_dim）（inter_y2）
解码器=模型（latent_inputs，outputs_reconstruction，name=&#39;解码器&#39;）

# 将预测网络与潜在空间分开
outputs_prediction = Dense(Y.shape[1])(inter_y2) # 根据您的数据调整 Y.shape[1]
预测器=模型（潜在输入，输出预测，名称=&#39;预测器&#39;）

# 实例化具有两个输出的 VAE 模型
outputs_vae = [解码器（编码器（inputs_x）[2]），预测器（编码器（inputs_x）[2]）]
vae = 模型(inputs_x,outputs_vae,name=&#39;vae_mlp&#39;)
vae.compile(optimizer=&#39;adam&#39;, loss=[&#39;mean_squared_error&#39;, &#39;mean_squared_error&#39;])

# 训练模型
历史记录 = vae.fit(X, [X, Y], epochs=200, batch_size=64, shuffle=True,validation_data=(XX,[XX, YY]))
]]></description>
      <guid>https://stackoverflow.com/questions/77705287/input-x-and-output-layers-y-of-keras-variational-autoencoder-dont-match-ca</guid>
      <pubDate>Fri, 22 Dec 2023 18:31:14 GMT</pubDate>
    </item>
    <item>
      <title>输入数据耗尽，中断张量流训练</title>
      <link>https://stackoverflow.com/questions/77705086/input-ran-out-of-data-interrupting-training-in-tensorflow</link>
      <description><![CDATA[我正在从事肿瘤分割工作，图像是 nifti 格式的 3D（MRI 图像）。我创建了一个数据生成器，因为如果我将完整数据集上传到 RAM，它会因 3D 图像而崩溃。该数据集由611张图像组成，尺寸为（240,240,160），计算时的块数为61000和11375
这是我的数据管道：
def load_nifti_image(文件路径, patch_size=(48, 48, 32), step_size=(48, 48, 32)):
    nifti = nib.load(文件路径)
    体积 = nifti.get_fdata()

    # 从卷创建补丁
    补丁 = patchify(体积, patch_size, 步骤=step_size)

    # 重塑 patch 相乘 (5, 5, 5) 并添加通道维度（1 表示灰度）
    补丁 = patchs.reshape(-1, *patches.shape[-3:])
    补丁= np.expand_dims（补丁，轴=-1）

    返回补丁

＃  -  -  -  -  -  -  -  -  -  -  - -火车 -  -  -  -  -  -  -  -  -  -  - -
nifti_files = [os.path.join(“/content/drive/MyDrive/Interpolated/train/images”, f) for f in os.listdir(“/content/drive/MyDrive/Interpolated/train/images”) if f.endswith(&#39;.nii.gz&#39;)]
mask_files = [os.path.join(“/content/drive/MyDrive/Interpolated/train/masks”, f) for f in os.listdir(“/content/drive/MyDrive/Interpolated/train/masks”) if f.endswith(&#39;.nii.gz&#39;)]

 ＃  -  -  -  -  -  -  -  -  -  -  - -验证 -  -  -  -  -  -  -  -  -  -  - -
nifti_files_val = [os.path.join(&quot;/content/drive/MyDrive/Interpolated/validation/images&quot;, f) for f in os.listdir(&quot;/content/drive/MyDrive/Interpolated/validation/images&quot;) if f.endswith(&#39;.nii.gz&#39;)]
mask_files_val = [os.path.join(&quot;/content/drive/MyDrive/Interpolated/validation/masks&quot;, f) for f in os.listdir(&quot;/content/drive/MyDrive/Interpolated/validation/masks&quot;) if f.endswith(&#39;.nii.gz&#39;)]

defcalculate_patches(文件路径, patch_size=(48, 48, 32), step_size=(48, 48, 32)):
    nifti = nib.load(文件路径)
    体积 = nifti.get_fdata()

    # 计算补丁数量
    patch_shape = [((i - p) // s) + 1 for i, p, s in zip(volume.shape, patch_size, step_size)]
    num_patches = np.prod(patches_shape)

    返回 num_patches

num_train_patches = sum(calculate_patches(f) for i, f in enumerate(nifti_files) if print(f“处理文件 {i}...”) 为 None)
num_val_patches = sum(calculate_patches(f) for i, f in enumerate(nifti_files_val) if print(f“处理文件 {i}...”) 为 None)

def data_generator(image_files, mask_files):
    对于 zip(image_files, mask_files) 中的 img_file、mask_file：
        image_patches = load_nifti_image(img_file)
        mask_patches = load_nifti_image(mask_file)

        对于 zip(image_patches, mask_patches) 中的 img_patch、mask_patch：
            产量 img_patch, mask_patch

train_generator = data_generator(nifti_files, mask_files)
val_generator = data_generator(nifti_files_val, mask_files_val)

输出签名 = (
    tf.TensorSpec(形状=(48, 48, 32, 1), dtype=tf.float64),
    tf.TensorSpec(形状=(48, 48, 32, 1), dtype=tf.float64)
）

数据集= tf.data.Dataset.from_generator（lambda：train_generator，output_signature=output_signature）.repeat（）
dataset_val = tf.data.Dataset.from_generator(lambda: val_generator, output_signature=output_signature).repeat()

数据集=数据集.batch(32)
dataset_val = dataset_val.batch(32)

test_model.fit（数据集，validation_data=dataset_val，epochs=100，steps_per_epoch=num_train_patches//32，validation_steps=num_val_patches//32）

我添加了 .repeat() 希望它能有所帮助，但事实并非如此。
我还尝试计算补丁的数量并手动插入它们，但它也不起作用。
这是完整的回溯：
纪元 1/100
1906/1906 [================================] - 1963s 1s/步 - 损失：0.6447 - dice_coefficient：0.3553 - val_loss ：0.9113 - val_dice_系数：0.0887
纪元 2/100
   1/1906 [................................] - 预计到达时间：10:17 - 损失：1.0000 - dice_coefficient：1.7961e -05

警告：tensorflow：您的输入数据不足；中断训练。确保您的数据集或生成器可以生成至少“steps_per_epoch * epochs”批次（在本例中为 190600 个批次）。构建数据集时，您可能需要使用 Repeat() 函数。
警告：tensorflow：您的输入数据不足；中断训练。确保您的数据集或生成器可以生成至少“steps_per_epoch * epochs”批次（在本例中为 355 个批次）。构建数据集时，您可能需要使用 Repeat() 函数。

1906/1906 [================================] - 0s 31us/步 - 损失：1.0000 - dice_coefficient：1.7961e- 05


]]></description>
      <guid>https://stackoverflow.com/questions/77705086/input-ran-out-of-data-interrupting-training-in-tensorflow</guid>
      <pubDate>Fri, 22 Dec 2023 17:43:32 GMT</pubDate>
    </item>
    <item>
      <title>在本地计算机上使用 jupyter lab 时卡在 train_data_loader 中</title>
      <link>https://stackoverflow.com/questions/77704959/stuck-in-the-train-data-loader-while-using-jupyter-lab-on-local-machine</link>
      <description><![CDATA[已经过去几个小时了，但我的代码根本没有运行，在任务管理器的 CPU 资源中，它显示 python 使用的资源为零。
在kaggle笔记本中使用相同的代码时，它工作得很好。
这是什么问题。
&lt;前&gt;&lt;代码&gt;
# 训练数据集样本
对于图像，目标在 (train_data_loader) 中：
    休息
images = list(image.to(device) 用于图像中的图像)
目标 = [{k: v.to(device) for k, v in t.items()} for t in Targets]

对于 random.sample([1,2,3],3) 中的数字：
  框 = 目标[数字][&#39;框&#39;].cpu().numpy().astype(np.int32)
  img = 图片[数字].permute(1,2,0).cpu().numpy()
  标签=目标[数字][&#39;标签&#39;].cpu().numpy().astype(np.int32)
  图, ax = plt.subplots(1, 1, Figsize=(16, 8))

  对于范围内的 i(len(boxes))：
      img = cv2.rectangle(img,(boxes[i][0]+paddingSize,boxes[i][1]+paddingSize),(boxes[i][2]+paddingSize,boxes[i][3]+paddingSize ),(255,0,0),2)
      #print(le.inverse_transform([labels[i]-1])[0])
      #print(label_to_name(标签[i]), (int(boxes[i][0]), int(boxes[i][1])))
      img = cv2.putText(img, label_to_name(labels[i]), (int(boxes[i][0]), int(boxes[i][1])), cv2.FONT_HERSHEY_TRIPLEX,1, (255,0 ,0), 2, cv2.LINE_AA)

  ax.set_axis_off()
  斧头.imshow(img)

训练数据加载器代码：
# 将数据集拆分为训练集和测试集
索引 = torch.randperm(len(train_dataset)).tolist()
# 创建训练并验证数据加载器
train_data_loader = 数据加载器(
    训练数据集，
    批量大小=8，
    随机播放=真，
    工人数=12，
    collat​​e_fn=整理_fn
）

valid_data_loader = 数据加载器(
    有效数据集，
    批量大小=8，
    随机播放=假，
    工人数=12，
    collat​​e_fn=整理_fn
）

创建数据集的代码
class MalariaDataset(torch.utils.data.Dataset)：
    def __init__(自身、数据、变换=无):
        超级().__init__()
        ”“”
        输入
            数据：数据框
            转换：撰写或列表
                Torchvision 图像转换。
        ”“”
        self.data = 数据
        self.transforms = 变换
        self.label_dict = label_dict
    def __getitem__(self, idx):
        img_path = self.data.pathname.unique()[idx]
        
        ann_df = self.data[self.data.pathname == img_path]
        
        图像 = plt.imread(img_path)*255
        
            
        盒子=[]
        标签=[]
        对于 _，ann_df.iterrows() 中的行：
            x1 = 行[&#39;x1&#39;]
            y1 = 行[&#39;y1&#39;]
            x2 = 行[&#39;x2&#39;]
            y2 = 行[&#39;y2&#39;]
            标签=行[&#39;类名&#39;]
            
            box.append([x1, y1, x2, y2])
            labels.append(标签)
            
        盒子 = np.array(盒子)
        面积 = (盒子[:, 3] - 盒子[:, 1]) * (盒子[:, 2] - 盒子[:, 0])
        面积 = torch.as_tensor(面积, dtype=torch.float32)
        
        标签 = torch.as_tensor(标签, dtype=torch.int64)
            
        
        目标={}
        目标[&#39;盒子&#39;] = 盒子
        目标[&#39;标签&#39;] = 标签
        目标[&#39;区域&#39;] = 区域
        如果自我变换：
            样本={
                    “图像”：图像，
                    &#39;bboxes&#39;：目标[&#39;boxes&#39;]，
                    ‘标签’：标签
            }
            样本 = self.transforms(**样本)
            图像 = 样本[&#39;图像&#39;]
            
        
            target[&#39;boxes&#39;] = torch.tensor(sample[&#39;bboxes&#39;])
        返回（图像，目标）
    def __len__(自身):
        返回 self.data.pathname.unique().shape[0]


我尝试使用 Kaggle 笔记本，代码运行良好]]></description>
      <guid>https://stackoverflow.com/questions/77704959/stuck-in-the-train-data-loader-while-using-jupyter-lab-on-local-machine</guid>
      <pubDate>Fri, 22 Dec 2023 17:09:35 GMT</pubDate>
    </item>
    <item>
      <title>Resnet34第一层7x7或3x3</title>
      <link>https://stackoverflow.com/questions/77704426/resnet34-first-layer-7x7-or-3x3</link>
      <description><![CDATA[我一直在尝试使用 pytorch 实现 Resnet34，但在查看其他实现时，我发现其中一些具有 3x3 卷积层 + bn + relu 作为第一层。然而，架构图上却写着7x7/2的卷积层。我真的很困惑哪一个是正确的。顺便说一下，我正在 CIFAR10 上进行训练，目前使用 7x7 卷积层经过 100 个周期后获得了 0.9 的准确率。
谢谢！
架构图
self.input_layer = nn.Sequential(
nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,bias=False),
nn.BatchNorm2d(64),
ReLU(),
nn.MaxPool2d(3, 步长=2, 填充=1)
）

这是我的第一个卷积层的代码。]]></description>
      <guid>https://stackoverflow.com/questions/77704426/resnet34-first-layer-7x7-or-3x3</guid>
      <pubDate>Fri, 22 Dec 2023 15:14:50 GMT</pubDate>
    </item>
    <item>
      <title>对新数据集进行预测[关闭]</title>
      <link>https://stackoverflow.com/questions/77704141/make-prediction-on-new-data-set</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77704141/make-prediction-on-new-data-set</guid>
      <pubDate>Fri, 22 Dec 2023 14:13:16 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络不学习</title>
      <link>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</link>
      <description><![CDATA[我正在尝试在包含 1500 张图像（15 个类别）的训练集上训练用于图像识别的卷积神经网络。有人告诉我，采用这种架构和从均值为 0、标准差为 0.01 的高斯分布得出的初始权重以及初始偏差值为 0 的情况，在适当的学习率下，它应该达到 30 左右的准确度%。
但是，它根本没有学到任何东西：准确率与随机分类器相似，并且训练后的权重仍然遵循正态分布。我做错了什么？
这是神经网络
class simpleCNN(nn.Module)：
  def __init__(自身):
    super(simpleCNN,self).__init__() #初始化模型

    self.conv1=nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,stride=1) #输出图像大小为(size+2*padding-kernel)/stride --&gt;62*62
    self.relu1=nn.ReLU()
    self.maxpool1=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像62/2--&gt;31*31

    self.conv2=nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,stride=1) #输出图像为29*29
    self.relu2=nn.ReLU()
    self.maxpool2=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像为29/2--&gt;14*14（MaxPool2d近似大小与floor）

    self.conv3=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1) #输出图像为12*12
    self.relu3=nn.ReLU()

    self.fc1=nn.Linear(32*12*12,15) #16 个通道 * 16*16 图像（64*64，步幅为 2 的 2 个 maxpooling），15 个输出特征=15 个类
    self.softmax = nn.Softmax(dim=1)

  def 前向（自身，x）：
    x=self.conv1(x)
    x=self.relu1(x)
    x=self.maxpool1(x)

    x=self.conv2(x)
    x=self.relu2(x)
    x=self.maxpool2(x)

    x=self.conv3(x)
    x=self.relu3(x)

    x=x.view(-1,32*12*12)

    x=self.fc1(x)
    x=self.softmax(x)

    返回x

初始化：
def init_weights(m):
  如果 isinstance(m,nn.Conv2d) 或 isinstance(m,nn.Linear)：
    nn.init.normal_(m.weight,0,0.01)
    nn.init.zeros_(m.bias)

模型 = simpleCNN()
模型.应用（init_weights）

训练函数：
loss_function=nn.CrossEntropyLoss()
优化器=optim.SGD(model.parameters(),lr=0.1,动量=0.9)

def train_one_epoch(epoch_index,loader):
  运行损失=0

  对于 i，枚举（加载器）中的数据：

    input,labels=data #获取小批量
    输出=模型（输入）#前向传递

    loss=loss_function(outputs,labels) #计算损失
    running_loss+=loss.item() #总结到目前为止处理的小批量的损失

    Optimizer.zero_grad() #重置梯度
    loss.backward() #计算梯度
    optimizer.step() #更新权重

  return running_loss/(i+1) # 每个小批量的平均损失


培训：
&lt;前&gt;&lt;代码&gt;纪元=20

best_validation_loss=np.inf

对于范围内的纪元（EPOCHS）：
  print(&#39;纪元{}:&#39;.format(纪元+1))

  模型.train(True)
  train_loss=train_one_epoch(epoch,train_loader)

  运行验证损失=0.0

  模型.eval()

  with torch.no_grad(): # 禁用梯度计算并减少内存消耗
    对于 i，枚举中的 vdata（validation_loader）：
      vinputs,vlabels=vdata
      v输出=模型（v输入）
      vloss=loss_function(v输出,v标签)
      running_validation_loss+=vloss.item()
  验证损失=运行验证损失/(i+1)
  print(&#39;LOSS 训练：{} 验证：{}&#39;.format(train_loss,validation_loss))

  if validation_loss
使用默认初始化，效果会好一些，但使用高斯应该可以达到 30%。
您能发现一些可能导致它无法学习的问题吗？我已经尝试过不同的学习率和动力。]]></description>
      <guid>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</guid>
      <pubDate>Fri, 22 Dec 2023 14:06:23 GMT</pubDate>
    </item>
    <item>
      <title>用于实时流处理的 Sagemaker 端点</title>
      <link>https://stackoverflow.com/questions/77702505/sagemaker-endpoint-for-processing-on-live-stream</link>
      <description><![CDATA[我正在 aws 上对实时视频流进行实时机器学习处理。
对于直播，正在使用 kinesis 视频流。
我正在从模型工件（存储我们的推理脚本和模型文件的位置）创建 sagemaker 端点
当我们从实时流中获取它们时，每个帧都会独立调用此端点。
挑战在于维护变量的缓存/会话状态。当每个帧到达端点时，它没有有关先前运行的结果的信息。为了解决这个问题，我为每次调用下载缓存并将其上传到数据库（dynamo db），这似乎是一种低效的方法。
我想探索是否有某种方式 - 实例在整个直播流中处于活动状态，每当实例接收到帧时，都会对其进行处理，缓存将一直存在，直到实例死亡？
而不是为每个帧单独流调用端点。
供参考 - 我在推理脚本中使用 pytorch 框架中外部训练的 YOLO 对象检测模型。]]></description>
      <guid>https://stackoverflow.com/questions/77702505/sagemaker-endpoint-for-processing-on-live-stream</guid>
      <pubDate>Fri, 22 Dec 2023 08:28:38 GMT</pubDate>
    </item>
    <item>
      <title>如何使用自定义数据集格式训练自定义 Transformer 模型</title>
      <link>https://stackoverflow.com/questions/77680618/how-to-train-a-customized-transformer-model-with-custom-dataset-formatting</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77680618/how-to-train-a-customized-transformer-model-with-custom-dataset-formatting</guid>
      <pubDate>Mon, 18 Dec 2023 16:56:58 GMT</pubDate>
    </item>
    <item>
      <title>神经先知根本不预测？</title>
      <link>https://stackoverflow.com/questions/77231544/neural-prophet-not-predicting-at-all</link>
      <description><![CDATA[我正在尝试预测进入某个海滩的顾客数量。因此，数据中的数字往往会波动，并且希望使用 Neural Prophet 来预测未来的客人。然而，根据我的神经先知模型的当前设置，该模型根本无法预测原始数据中的最终日期，即使对于其他参数它确实预测，只是不准确。
以下是预测结果（虚线）与原始结果（实线）的对比：
在此处输入图片描述
我特意要求预测未来 100 天，但根本没有显示。
这是我的模型设置：
 模型 = NeuralProphet(
        #growth=“off”, # 确定趋势类型：&#39;线性&#39;、&#39;不连续&#39;、&#39;关闭&#39;
        #changepoints=None, #可能包含变更点的日期列表（None -&gt;automatic ）
        n_changepoints=0,
        #changepoints_range=0,
        #trend_reg=0,
        # trend_reg_threshold=False,
        # #seasonality_reg=1,
        # # d_hidden = 0,
        n_lags=10,
        # # num_hidden_​​layers=0, # AR-Net 隐藏层的维度
        # # ar_reg=None, # AR 系数的稀疏性
        学习率=0.01，
        纪元=100，
        Normalize=“auto”, # 标准化类型 (&#39;minmax&#39;, &#39;standardize&#39;, &#39;soft&#39;, &#39;off&#39;)
        impute_missing=真，
        yearly_seasonality=真，
        week_seasonality=假，
        daily_seasonality=假，
        季节性_模式=“乘法”，
        loss_func=“均方误差”,
    ）

    # 将模型与训练数据进行拟合
    model.fit(数据,频率=“D”)
    未来= model.make_future_dataframe（数据，周期= 1000，n_historic_predictions = len（数据））
    预测 = model.predict(future)
]]></description>
      <guid>https://stackoverflow.com/questions/77231544/neural-prophet-not-predicting-at-all</guid>
      <pubDate>Wed, 04 Oct 2023 16:52:25 GMT</pubDate>
    </item>
    <item>
      <title>尝试通过回归来预测算法的运行时间</title>
      <link>https://stackoverflow.com/questions/65862139/trying-to-predict-running-time-of-algorithms-through-regression</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/65862139/trying-to-predict-running-time-of-algorithms-through-regression</guid>
      <pubDate>Sat, 23 Jan 2021 17:19:51 GMT</pubDate>
    </item>
    <item>
      <title>如何动态地将curl变量发送给管道工函数？</title>
      <link>https://stackoverflow.com/questions/61824959/how-to-send-curl-variables-to-plumber-function-dynamically</link>
      <description><![CDATA[我想根据任意数量的输入变量动态调用管道工 API。我需要将curl 输入映射到函数名称的输入。例如，如果函数有一个输入 hi，则 curl -s --data &#39;hi=2&#39; 意味着 hi=2 应该是作为输入参数传递给函数。这可以直接在 R 中使用 match.call() 完成，但在通过管道工 API 调用它时失败。
获取函数

&lt;前&gt;&lt;代码&gt;#&#39; @post /API
#&#39; @serializer unboxedJSON
tmp &lt;- 函数(hi) {

  输出 &lt;- 列表(hi=hi)

  out &lt;- toJSON(out, Pretty = TRUE, auto_unbox = TRUE)

  返回（出）

}

tmp(嗨=2)
输出：{嗨：2}

然后
curl -s --data &#39;hi=10&#39; http://127.0.0.1/8081/API
输出：{\n \&quot;hi\&quot;: \&quot;2\&quot;\n}

一切看起来都不错。但是，取函数

&lt;前&gt;&lt;代码&gt;#&#39; @post /API
#&#39; @serializer unboxedJSON
tmp &lt;- 函数(...) {

  out &lt;- match.call() %&gt;%
         as.list() %&gt;%
         .[2:长度(.)] # %&gt;%

  out &lt;- toJSON(out, Pretty = TRUE, auto_unbox = TRUE)

  返回（出）

}
tmp(嗨=2)
输出：{嗨：2}

然后
curl -s --data &#39;hi=10&#39; http://127.0.0.1/8081/API
out: {&quot;error&quot;:&quot;500 - 内部服务器错误&quot;,&quot;message&quot;:&quot;错误: 没有方法 asJSON S3 类: R6\n&quot;}

实际上，我真正想做的是加载我的 ML 模型以使用管道工 API 预测分数。例如
model &lt;- readRDS(&#39;model.rds&#39;) # 将模型加载为全局变量

预测得分 &lt;- 函数(...) {

    df_in &lt;- match.call() %&gt;%
        as.list() %&gt;%
        .[2:长度(.)] %&gt;%
        as.data.frame()

    json_out &lt;- 列表(
        Score_out = 预测(模型, df_in) %&gt;%
        toJSON(., 漂亮 = T, auto_unbox = T)

    返回（json_out）
}


此函数在本地运行时按预期工作，但通过 curl -s --data &#39;var1=1&amp;var2=2...etc&#39; http://listen_address 通过 API 运行&lt; /p&gt;

我收到以下错误：{&quot;error&quot;:&quot;500 - 内部服务器错误&quot;,&quot;message&quot;:&quot;as.data.frame.default(x[[i]], 可选 = TRUE 中的错误): 无法将类 \&quot;c(\&quot;PlumberResponse\&quot;, \&quot;R6\&quot;)\&quot; 强制转换为 data.frame\n&quot;}]]></description>
      <guid>https://stackoverflow.com/questions/61824959/how-to-send-curl-variables-to-plumber-function-dynamically</guid>
      <pubDate>Fri, 15 May 2020 17:22:08 GMT</pubDate>
    </item>
    </channel>
</rss>