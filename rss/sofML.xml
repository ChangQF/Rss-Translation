<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 06 Dec 2023 06:18:10 GMT</lastBuildDate>
    <item>
      <title>SARSA模型减少交通拥堵的奖励计算</title>
      <link>https://stackoverflow.com/questions/77610911/reward-calculation-for-a-sarsa-model-to-reduce-traffic-congestion</link>
      <description><![CDATA[我正在尝试实施奖励系统，SARSA 模型可以使用该系统来做出更好的决策，以缓解十字路口所有车道的交通流量。这就是我的奖励函数的样子：
 defcalculate_reward(self, old_dti, new_dti):

        阿尔法 = 0.5
        贝塔 = 0.3
        伽马= 0.2

        duction_in_total_congestion = sum(old_dti.values()) - sum(new_dti.values())
        extra_vehicles = [max(0, count - self.vehicle_threshold) for count in self.vehicle_parameters[“vehicle_count”].values()]
        avg_congestion_above_threshold = sum(excess_vehicles) / 4

        如果 self.action_changed，则 action_cost = 1 否则 0

        奖励 = alpha * 总拥塞减少量 - beta * avg_congestion_above_threshold - gamma * action_cost

        返回奖励

dti（Delay Time Indicator）：是车道上所有车辆等待时间的总和
示例：old_dti = {“北”：4334，“南”：83，“东”：2332，“西”：432}
vehicle_threshold：一条车道可以拥有的最大车辆数量。我已将其设置为 12
self.vehicle_parameters[“vehicle_count”]：是每个车道上等待红灯的车辆数量。
示例：{“北”：12，“南”：0，“东”：2，“西”：2}
action_cost：如果SARSA模型做了一个决策，如果和之前不是同一个决策，则成本为1。如果做出了相同的决策，则成本为0
我为上述参数添加了权重以表示它们的重要性。 DTI 的重要性最高，因为一条 DTI 较低的车道上可以有 10 辆车，而另一条 DTI 较高的车道上可以有 5 辆车。在这种情况下，DTI 的优先级高于vehicle_count。
我之前的奖励计算函数：
&lt;前&gt;&lt;代码&gt; @staticmethod
    defcalculate_reward(old_dti,new_dti,vehicle_count):
        最大奖励 = 10
        最大惩罚 = -10

        延迟之前 = sum(old_dti.values())
        延迟后 = sum(new_dti.values())

        如果delay_before == 0：
            如果delay_after&gt; 0:
                # 在没有延迟的情况下引入延迟应该受到惩罚
                返回最大惩罚值
            别的：
                # 保持不拥堵可能是一个中性或稍微积极的结果
                return 1 # 或一些小的正值
        别的：
            改进 = 延迟之前 - 延迟之后
            如果改善&gt; 0:
                # 根据改进百分比调整奖励
                奖励 = (改进 / 之前延迟) * max_reward
            elif改进&lt; 0:
                # 根据恶化的百分比调整惩罚范围
                惩罚率 = 绝对值（改进） / 之前延迟
                奖励 = 惩罚比例 * 最大惩罚
            别的：
                # 延迟没有变化
                奖励=0

        返回奖励

在此实现中，我仅根据 DTI 计算奖励。但经过 20 代后，奖励并没有显着变化，模型还没有正确学习。
我计算奖励的新方法是否能更好地缓解每条车道的拥堵？还有，我的SARSA应该根据什么来做出下一步的决定呢？截至目前，SARSA 每 0.5 秒就会做出一次决定。我正在考虑实施vehicle_threshold，如果车道的车辆阈值超过预设限制，SARSA 应该做出决定]]></description>
      <guid>https://stackoverflow.com/questions/77610911/reward-calculation-for-a-sarsa-model-to-reduce-traffic-congestion</guid>
      <pubDate>Wed, 06 Dec 2023 06:16:13 GMT</pubDate>
    </item>
    <item>
      <title>我不理解分类中的 .pred_class （使用逻辑回归）</title>
      <link>https://stackoverflow.com/questions/77610242/im-not-understanding-pred-class-in-classification-using-logistic-regression</link>
      <description><![CDATA[我有一个非常简单的问题，我的结果是二元的，我正在尝试使用逻辑回归（使用 tidymodels）根据一些预测变量（其中一些是众所周知的良好预测变量）进行分类。
我将因子结果编码为 0 和 1（1=正，这是我最感兴趣的）。
当我使用两种 types=“class” 运行预测函数时且 types=“prob”我得到名为：.pred_class、.pred_0 和 .pred_1 的列。
然后，例如，在绘制 ROC 曲线时，我想知道是否应该使用
roc1 &lt;- roc_curve(data_test_pred, 结果, .pred_1)

或
roc1 &lt;- roc_curve(data_test_pred, 结果, .pred_0)。

第一个（我认为是正确的）在对角线下方给出了一个糟糕的 ROC 曲线，第二个给出了一个不错的 ROC 曲线。
所以，我只是不明白这里发生了什么，也不知道如何继续。]]></description>
      <guid>https://stackoverflow.com/questions/77610242/im-not-understanding-pred-class-in-classification-using-logistic-regression</guid>
      <pubDate>Wed, 06 Dec 2023 02:26:33 GMT</pubDate>
    </item>
    <item>
      <title>如何训练笔迹模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77610232/how-to-train-handwriting-model</link>
      <description><![CDATA[我看到了 sjvasquez 的一个名为手写合成的项目。我真的很想训练自己的模型来匹配我的笔迹，但我不确定如何开始。
基本上，我只是不知道如何输入数据并训练它。]]></description>
      <guid>https://stackoverflow.com/questions/77610232/how-to-train-handwriting-model</guid>
      <pubDate>Wed, 06 Dec 2023 02:20:08 GMT</pubDate>
    </item>
    <item>
      <title>无法让我的逻辑回归算法显示任何图[关闭]</title>
      <link>https://stackoverflow.com/questions/77609888/cant-get-my-logistic-regression-algorithm-to-display-any-of-the-plots</link>
      <description><![CDATA[这是我的逻辑回归项目的代码。我没有错误并且程序运行，但是一旦运行完成，它只显示进程完成退出代码0。它应该显示诸如我的成本函数和 thetas 以及所有常见的机器学习信息之类的内容，但我一生都无法让它显示。
导入 matplotlib.pyplot 作为 plt
将 numpy 导入为 np
将 pandas 导入为 pd
从 numpy 导入日志、点、exp、形状
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.model_selection 导入 train_test_split


def标准化（x_tr）：
    对于范围内的 i(shape(x_tr)[1])：
        x_tr[:, i] = (x_tr[:, i] - np.mean(x_tr[:, i])) / np.std(x_tr[:, i])
    返回x_tr


定义 sigmoid(z):
    sig = 1 / (1 + exp(-z))
    返回信号


定义成本（theta，x，y）：
    z = 点(x, θ)
    cost0 = y.T.dot(log(sigmoid(z)))
    成本1 = (1 - y).T.dot(log(1 - sigmoid(z)))
    成本 = -(成本1 + 成本0) / len(y)
    退货成本


def 初始化（x，y）：
    thetas = np.zeros((shape(x)[1] + 1, len(np.unique(y))))
    x = np.c_[np.ones((形状(x)[0], 1)), x]
    返回 θ，x


def fit(x, y, alpha=0.001, 迭代=400):
    θ，x = 初始化（x，y）
    cost_list = np.zeros(迭代, )

    对于范围内的 i（迭代）：
        对于范围内的 c(len(np.unique(y)))：
            y_temp = np.where(y == c, 1, 0)
            thetas[:, c] = thetas[:, c] - alpha * dot(x.T, (sigmoid(dot(x, thetas[:, c])) - y_temp))
            cost_list[i] += cost(thetas[:, c], x, y_temp)

    返回cost_list，thetas


def 预测（x，thetas）：
    x = np.c_[np.ones((形状(x)[0], 1)), x]
    z = 点(x, θ)
    sig = sigmoid(z)
    返回 np.argmax(sig, axis=1)


def 比较（y_test，y_pred，y_pred1）：
    正确分类 = np.sum(y_test == y_pred)
    正确分类1 = np.sum(y_test == y_pred1)

    print(“我们的模型测试集的准确度：”, (rightly_classified / len(y_test)) * 100)
    print(“sklearn 模型测试集的准确度：”, (rightly_classified1 / len(y_test)) * 100)


def main():
    df = pd.read_csv(&#39;Iris.csv&#39;) # 使用正确的文件名更新文件名
    x = df.iloc[:, :-1].values
    y = df.iloc[:, -1].值

    # 将分类值转换为数值
    df[&#39;物种&#39;].replace(&#39;Iris-setosa&#39;, 0, inplace=True)
    df[&#39;物种&#39;].replace(&#39;杂色鸢尾&#39;, 1, inplace=True)
    df[&#39;物种&#39;].replace(&#39;维吉尼亚鸢尾&#39;, 2, inplace=True)

    x_tr, x_te, y_tr, y_te = train_test_split(x, y, test_size=0.3, random_state=0)
    x_tr = 标准化(x_tr)

    cost_list, thetas = fit(x_tr, y_tr)
    plt.scatter(range(len(cost_list)), cost_list, c=“蓝色”)
    plt.show()

    y_pred = 预测（x_te，thetas）
    模型1 = 逻辑回归()
    model1.fit(x_tr, y_tr)
    y_pred1 = model1.预测(x_te)

    比较（y_te，y_pred，y_pred1）

    主要的（）
]]></description>
      <guid>https://stackoverflow.com/questions/77609888/cant-get-my-logistic-regression-algorithm-to-display-any-of-the-plots</guid>
      <pubDate>Wed, 06 Dec 2023 00:03:09 GMT</pubDate>
    </item>
    <item>
      <title>数百个时间序列的需求预测方法</title>
      <link>https://stackoverflow.com/questions/77609637/demand-forecasting-methods-for-hundreds-of-time-series</link>
      <description><![CDATA[我有 TFL 骑行数据集时间段从 2023 年 1 月到 2023 年 6 月。我想预测需求或预期数量。一天中每个小时每个车站的班次。
发布一些数据处理，下面是我的预测变量 -
起始站号
自行车型号
开始时间
一周开始日
开始月份
trip_duration_bins
我基本上会预测每个车站每天每小时的可能行程数量，这些行程属于 &lt;30 分钟、30-60 分钟和超过 1 小时的行程持续时间类别。

我不清楚是否可以只使用线性回归、基于树的回归方法等，或者是否必须单独使用时间序列方法。

如果我可以使用任何回归方法，我可以将数据拆分为训练和验证吗？还是必须按时间序列拆分？

如果我必须使用时间序列方法（我对此了解不多），我该怎么做？我很困惑，因为感觉就像没有。随着时间的推移，每个车站的旅程将成为一个时间序列，由于车站有很多，我们有数百个时间序列。

如果每个车站每小时的出行是一个时间序列，那么如何对数百个时间序列去除季节性并找到它们的模型顺序？


可能是愚蠢的问题，因为我对这些东西很陌生。但是，提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/77609637/demand-forecasting-methods-for-hundreds-of-time-series</guid>
      <pubDate>Tue, 05 Dec 2023 22:45:16 GMT</pubDate>
    </item>
    <item>
      <title>训练 CNN 时如何绘制学习率和 Epoch？</title>
      <link>https://stackoverflow.com/questions/77609614/how-to-plot-learning-rate-and-epoch-when-training-a-cnn</link>
      <description><![CDATA[我正在尝试使用基于余弦的衰减时间表来提高 CNN 的准确性。我希望直观地看到整个训练过程中此计划对模型学习率造成的变化。
如何在 R 中绘制学习率曲线，其中 y 轴为学习率，x 轴为步长？这里提供了来自其他来源的示例曲线：https://i.stack.imgur.com/ q7d1q.png
参见下面的代码：
# 1.学习率设置
Learning_rate_schedule_cosine_decay（initial_learning_rate = 0，decay_steps = 1000，alpha = 0，name = &#39;余弦衰减&#39;）
lr_decayed_fn &lt;-learning_rate_schedule_cosine_decay（initial_learning_rate = 0.0001，decay_steps = 1000）

# 2.模型编译
模型 &lt;- keras_model_sequential()
型号%&gt;%
  layer_conv_2d(滤波器 = 32, kernel_size = c(3,3), 激活 = &#39;relu&#39;, input_shape = c(256, 256, 3))%&gt;%
  layer_conv_2d（过滤器= 32，kernel_size = c（3,3），激活=&#39;relu&#39;）％&gt;％
  Layer_max_pooling_2d(pool_size = c(2,2))%&gt;%
  layer_dropout(rate = 0.25)%&gt;%
  layer_conv_2d（过滤器= 64，kernel_size = c（3,3），激活=&#39;relu&#39;）％&gt;％
  layer_conv_2d（过滤器= 64，kernel_size = c（3,3），激活=&#39;relu&#39;）％&gt;％
  Layer_dropout(率 = 0.25) %&gt;%
  Layer_flatten()%&gt;%
  Layer_dense(单位 = 256, 激活 = &#39;relu&#39;)%&gt;%
  layer_dropout(rate = 0.25)%&gt;%
  Layer_dense(单位 = 128, 激活 = &#39;relu&#39;)%&gt;%
  layer_dropout(rate = 0.25)%&gt;%
  Layer_dense(单位 = 4, 激活 = &#39;softmax&#39;)%&gt;%
  编译（损失=&#39;分类交叉熵&#39;，
          优化器 = optimizer_adam(lr = lr_decayed_fn, name = &#39;Adam&#39;),
          指标 = c(&#39;准确度&#39;))
摘要（模型）


# 适合模型
历史记录&lt;-模型%&gt;%
  适合（训练x，
      火车标签，
      历元 = 30,
      批量大小=32，
      验证分割= 0.2）
情节（历史）
]]></description>
      <guid>https://stackoverflow.com/questions/77609614/how-to-plot-learning-rate-and-epoch-when-training-a-cnn</guid>
      <pubDate>Tue, 05 Dec 2023 22:38:48 GMT</pubDate>
    </item>
    <item>
      <title>HMM R 包 if (d < delta) { 中的错误：缺少 TRUE/FALSE 需要的值</title>
      <link>https://stackoverflow.com/questions/77609409/hmm-r-package-error-in-if-d-delta-missing-value-where-true-false-needed</link>
      <description><![CDATA[我正在尝试在 R 中使用 HMM 包。
我想要 4 个隐藏状态，我的观察值范围为 2 到 15。
我可以毫无问题地初始化隐藏模型：
if (!require(HMM, 悄悄地 = TRUE)) {
  install.packages(“HMM”)
  图书馆（隐马尔可夫模型）
} 别的 {
  图书馆（隐马尔可夫模型）
}

# 加载数据
url &lt;-“https://raw.githubusercontent.com/luancvieira/HMM/main/ottawa_2010-2012.csv”
df &lt;- read.csv(url)

观察到的数据 &lt;- df$AvgTemperature

# 定义状态的数量和名称
n_states &lt;- 4
state_names &lt;-paste0(“州”, 1:n_states)

# 对符号进行排序
symbol_names &lt;- as.character(sort(unique(observed_data)))
observed_data &lt;- as.character(observed_data)

# 用随机概率初始化HMM模型
start_probs &lt;- runif(n_states)
trans_probs &lt;- 矩阵(runif(n_states * n_states),
               nrow = n_states, ncol = n_states)
emission_probs &lt;- 矩阵(runif(n_states * length(symbol_names)),
                  nrow = n_states, ncol = 长度(symbol_names))

# 标准化行以确保概率总和为 1
start_probs &lt;- start_probs / sum(start_probs)
trans_probs &lt;- trans_probs / rowSums(trans_probs)
Emission_probs &lt;- Emission_probs / rowSums(emission_probs)

# 初始化HMM模型
hmm_model &lt;- initHMM(States = state_names,
                     符号 = 符号名称，
                     起始概率=起始概率，
                     反式概率=反式概率，
                     排放概率 = 排放概率）

# 打印初始化的模型
打印（嗯_模型）


但是，当我尝试运行它时：
bw = baumWelch(hmm = hmm_model,观察=observed_data,
               最大迭代次数 = 100，增量 = 0.001)

我明白了
if (d &lt; delta) { 中的错误：缺少 TRUE/FALSE 需要的值。

如果我将迭代次数减少到 10 次，它运行时不会出现问题，但 10 次迭代不足以收敛。 delta 是算法的停止标准，如下所示： https:// /cran.r-project.org/web/packages/HMM/HMM.pdf。 delta 的默认值为 1e-9，因此即使没有指定 delta，在运行更多次迭代时仍然会返回错误。]]></description>
      <guid>https://stackoverflow.com/questions/77609409/hmm-r-package-error-in-if-d-delta-missing-value-where-true-false-needed</guid>
      <pubDate>Tue, 05 Dec 2023 21:45:12 GMT</pubDate>
    </item>
    <item>
      <title>一个张量流管道，输入 shape=(256, 256, 3), dtype=tf.float32 图像，使用 MTCNN() 提取人脸。我就是无法完成这件事</title>
      <link>https://stackoverflow.com/questions/77608982/a-tensorflow-pipeline-that-inputs-shape-256-256-3-dtype-tf-float32-image</link>
      <description><![CDATA[一个张量流 2.15 管道，需要两个 256X256X3、uint8 图像...&#39;input_image&#39;、“real_image”，来自之前的 tf.data.Dataset 类型管道，使用 MTCNN.detect_faces() 从 input_image 中提取面部，并在图像上绘制面加上 5% 额外的有界框作为相同大小的输入图像，并返回 input_image 和 real_image 而不更改 real_image
。
..
...
....
def extract_faces_from_tensors(input_image, real_image, margin_percent=5, required_size=(256, 256)):
# 将输入图像张量转换为 NumPy 数组
input_image = input_image.numpy()
# 检测人脸
检测器 = MTCNN()
faces = detector. detector_faces(input_image)

脸部图像 = []
真实图像 = 真实图像.numpy()

如果面临：
    对于面孔中的面孔：
        # 从请求的面中提取带有边距的边界框
        x, y, 宽度, 高度 = 面[&#39;box&#39;]
        边距 = int(min(宽度, 高度) * (margin_percent / 100.0))
        x1, y1 = max(x - 边距, 0), max(y - 边距, 0)
        x2, y2 = x + 宽度 + 边距, y + 高度 + 边距

        # 提取有边缘的脸
        面边界 = 输入图像[y1:y2, x1:x2]

        # 在创建 PIL 图像之前转换为 uint8
        面边界 = np.uint8(面边界)

        # 从数组创建一个 PIL 图像
        face_image = Image.fromarray(face_boundary)

        # 将像素大小调整为所需的大小
        面部图像 = 面部图像.调整大小(required_size)
        
        # 转换为 float32 并标准化为范围 [0, 1]
        face_array = tf.cast(np.array(face_image), tf.float32) / 255.0
        真实图像 = tf.cast(真实图像, tf.float32)/255.0
        face_images.append(face_array)
        输入图像=人脸图像

返回输入图像、真实图像

tr_data = tr_data.map(extract_faces_from_tensors, num_parallel_calls=tf.data.AUTOTUNE)
tr_数据
。
..
...
属性错误：在用户代码中：
文件“/tmp/ipykernel_47/4070542639.py”，第 3 行，位于 extract_faces_from_tensors *
    输入图像 = 输入图像.numpy()

AttributeError：“SymbolicTensor”对象没有属性“numpy”

过去两天我一直陷入这个问题，已经尝试了 gpt、co-pilot 一切。
请大家帮忙]]></description>
      <guid>https://stackoverflow.com/questions/77608982/a-tensorflow-pipeline-that-inputs-shape-256-256-3-dtype-tf-float32-image</guid>
      <pubDate>Tue, 05 Dec 2023 20:05:29 GMT</pubDate>
    </item>
    <item>
      <title>缺陷的时间演变：预测剩余缺陷</title>
      <link>https://stackoverflow.com/questions/77607265/temporal-evolution-of-defects-predicting-remaining-defect</link>
      <description><![CDATA[我在一家公司的人工智能论文是预测 10 分钟或 Xtime 后纸张上剩余的缺陷

墨水印刷在金属板上
打印后直接出现一些缺陷（打印步骤后1秒拍照）
但一段时间后，一些缺陷消失了，一些缺陷仍然存在

我的目标是打印一张新纸张并拍摄即时照片后：使用机器/深度学习预测 10 分钟或 Xtime 后剩余的缺陷
说明（涂上墨水后涂抹更多并覆盖疤痕等缺陷）
我有多种资源，如相机、机器等
我愿意接受任何想法]]></description>
      <guid>https://stackoverflow.com/questions/77607265/temporal-evolution-of-defects-predicting-remaining-defect</guid>
      <pubDate>Tue, 05 Dec 2023 15:12:18 GMT</pubDate>
    </item>
    <item>
      <title>拆分苹果ML模型可以获得更好的准确性？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77607078/splitting-the-apple-ml-model-gets-better-accuracy</link>
      <description><![CDATA[我的目标是使用 WLASL 数据集创建一个将手语转换为文本的模型。现在，从一开始就从kaggle下载这个模型，虽然数据集看起来相当全面，但每个类别的视频数量从5到13个不等，这显然需要训练的内容相当少。我决定尝试 Apple Create ML，而不是像 Tensorflow 或更复杂的深度学习框架，因为这样会简单得多。由于数据集在每个类别的视频方面非常有限，因此我在“手部动作分类器”中使用了所有 6 个数据增强。 （水平翻转、旋转、平移、缩放、插帧、丢帧）。虽然我知道这无法保存模型，但它肯定会大大提高准确性。请注意，我没有使用数据集中的所有 2000 个类（单词），而是仅使用了 300 个类的子集。我获得了 16% 的验证准确率，以及 90% 的所有增强训练准确率。所以我对 25 个类进行了同样的尝试，这次我获得了 42% 的验证准确率，以及 100% 的训练准确率。我进入实时预览，几乎我尝试的每个迹象都被预测为错误。
现在，我决定使用“模型源”在侧边栏中。我不太确定它们的用途，但这是我尝试过的：
我将数据子集分成 2 个单独的模型源（16 个类，但数量仍然很高），分别获得了 83% 的验证准确率和 90% 的验证准确率。这两个模型源都使用所有数据增强。两个来源都有 100% 的训练准确度，但将其分成两个模型显然提高了我的准确度，当我在“实时预览”中测试这一点时，我自己做的每个 ASL 标志，它都能够准确地猜测每个单词置信度超过 90%。
所以我的问题是，即使我的数据有限（虽然增强确实增加了很多，但显然性能差异不应该这么大），我的模型如何表现得这么好？此外，将一个模型拆分为单独的模型源是否可行？我不确定“模型来源”有什么用？甚至是，所以我尝试了这个，不知怎的，我得到了更好的结果。如果可行，我如何将它们实现到一个快速应用程序中。我现在有点困惑，所以希望有人能告诉我发生了什么事。如果这不是一个可行的解决方案，有人可以提供另一个解决方案来说明我如何使用这个数据集吗？事先了解它会非常有帮助，但即使你不知道，你能帮助我吗？
Kaggle链接：https://www.kaggle.com/datasets/risangbaskoro/ wlasl 处理
原始论文github页面：https://github.com/dxli94/WLASL]]></description>
      <guid>https://stackoverflow.com/questions/77607078/splitting-the-apple-ml-model-gets-better-accuracy</guid>
      <pubDate>Tue, 05 Dec 2023 14:43:09 GMT</pubDate>
    </item>
    <item>
      <title>spaCy 值错误：[E1041] 需要字符串、文档或字节作为输入，但得到：<class 'float'></title>
      <link>https://stackoverflow.com/questions/77596731/spacy-value-error-e1041-expected-a-string-doc-or-bytes-as-input-but-got</link>
      <description><![CDATA[我正在尝试使用 spaCy 对中文输入进行矢量化。
我的代码如下：


nlp = spacy.load(&#39;zh_core_web_md&#39;)

def tokenize_and_vectorize_textZH(文本):
    clean_tokensZH = []
    对于 nlp(text) 中的标记：
        if (不是 token.is_stop) &amp; (token.lemma_ != &#39;-PRON-&#39;) &amp; （不是 token.is_punct）：
          # -PRON- 是一个特殊的全包“引理” spaCy 用于任何代词，我们要排除这些
            if (len(token.vector) != 300):
              打印（令牌）
            clean_tokensZH.append(token.vector)
    返回 np.array(clean_tokensZH)
    
    
all_summmed_vecsZH = []

def sum_vecsZH(输入):
  tokenized_vectorsZH = input.apply(tokenize_and_vectorize_textZH)
  tokenized_vectorZH = tokenized_vectorsZH.to_numpy()

  打印（len（tokenized_vectorsZH））
  #print(类型(标记化向量))

  对于 tokenized_vectorsZH 中的行：

    #打印（行）

    summed_vecZH = [0]*300 # 从 300 个零的列表开始

    for vec in row: # 循环遍历与行中每个标记对应的每个向量
      #if (len(vec) != 300):
        #打印（向量）
      summed_vecZH += vec

    all_summmed_vecs.append(summed_vecZH)

  #print(tokenized_vectors[0][0].向量)
  
  
#@title 应用矢量化
sum_vecsZH(X_trainZH)
打印（all_summmed_vecs）

sum_vecsZH(y_trainZH)
打印（all_summmed_vecs）

sum_vecsZH(X_testZH)
打印（all_summmed_vecs）

sum_vecsZH(y_testZH)
打印（all_summmed_vecs）



最后 8 行的预期输出应与此类似：
33384
33384
14308
14308
这是我的数据集的照片：https://i.stack。 imgur.com/lJVJo.png
这个错误的原因是什么？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77596731/spacy-value-error-e1041-expected-a-string-doc-or-bytes-as-input-but-got</guid>
      <pubDate>Mon, 04 Dec 2023 00:59:14 GMT</pubDate>
    </item>
    <item>
      <title>如何修复我的感知器来识别数字？</title>
      <link>https://stackoverflow.com/questions/77594625/how-can-i-fix-my-perceptron-to-recognize-numbers</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77594625/how-can-i-fix-my-perceptron-to-recognize-numbers</guid>
      <pubDate>Sun, 03 Dec 2023 14:03:49 GMT</pubDate>
    </item>
    <item>
      <title>执行手动交叉验证时 Sklearn 的 precision_score 与 cross_val_score 的行为不一致</title>
      <link>https://stackoverflow.com/questions/77269168/inconsistent-behavior-of-sklearns-precision-score-when-manual-cross-validation</link>
      <description><![CDATA[我尝试将  precision_score  与  np.nan  一起用于  Zero_division 。它不适用于 cross_val_score，但当我使用相同的对进行手动交叉验证时，它可以工作。
这是要重现的数据文件：
sklearn_data.pkl.zip
# 加载数据
将 open(“sklearn_data.pkl”, “rb”) 作为 f：
    对象 = pickle.load(f)


#&gt;对象.keys()
# dict_keys([&#39;估计器&#39;, &#39;X&#39;, &#39;y&#39;, &#39;评分&#39;, &#39;cv&#39;, &#39;n_jobs&#39;])

估计器=对象[“估计器”]
X = 对象[“X”]
y = 对象[“y”]
评分=对象[“评分”]
简历 = 对象[“简历”]
n_jobs = 对象[“n_jobs”]

#&gt;得分
# make_scorer( precision_score, pos_label=Case_0, Zero_division=nan)

#&gt; y.unique()
# [&#39;控制&#39;, &#39;Case_0&#39;]
# 类别（2，对象）：[&#39;Case_0&#39;, &#39;Control&#39;]

# 首先我检查以确保所有训练和验证对中都存在两个类
pos_label =“案例_0”
control_label = “控制”
对于简历中的index_training、index_validation：
    断言 y.iloc[index_training].nunique() == 2
    断言 y.iloc[index_validation].nunique() == 2
    在 y.values 中断言 pos_label
    在 y.values 中断言 control_label

# 如果我手动运行：
分数 = 列表()
对于简历中的index_training、index_validation：
    estimator.fit(X.iloc[index_training], y.iloc[index_training])
    y_hat = estimator.predict(X.iloc[index_validation])
    分数 = precision_score(y_true = y.iloc[index_validation], y_pred=y_hat, pos_label=pos_label)
    分数.append(分数)
#&gt; print(np.mean(分数))
# 0.501156937317928

# 如果我使用 cross_val_score:
cross_val_score（估计器=估计器，X=X，y=y，cv=cv，评分=评分，n_jobs=n_jobs）
# /Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:839：UserWarning：评分失败。这些参数的训练测试分区的分数将设置为 nan。细节：
# 回溯（最近一次调用最后一次）：
# 文件“/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/python3.9/site-packages/sklearn/metrics/_scorer.py”，第 136 行，在 __call__ 中
# 分数 = Scorer._score(
# 文件“/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/python3.9/site-packages/sklearn/metrics/_scorer.py”，第 355 行，在 _score 中
# 返回 self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
# 文件“/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/python3.9/site-packages/sklearn/utils/_param_validation.py”，第 201 行，包装器中
# 验证参数约束（
# 文件“/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/python3.9/site-packages/sklearn/utils/_param_validation.py”，第 95 行，位于 validate_parameter_constraints
# 引发无效参数错误(
# sklearn.utils._param_validation.InvalidParameterError: precision_score 的 &#39;zero_division&#39; 参数必须是 {0.0, 1.0, nan} 中的 float 或 {&#39;warn&#39;} 中的 str。取而代之的是南。

这是我的版本：
&lt;前&gt;&lt;代码&gt;系统：
    蟒蛇: 3.9.16 |由 conda-forge 打包 | （主要，2023 年 2 月 1 日，21:42:20）[Clang 14.0.6 ]
可执行文件：/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/bin/python
   机器：macOS-13.4.1-x86_64-i386-64位

Python 依赖项：
      sklearn：1.3.1
          点：22.0.3
   安装工具：60.7.1
        numpy：1.24.4
        scipy：1.8.0
       赛通：0.29.27
       熊猫：1.4.0
   matplotlib：3.7.1
       作业库：1.3.2
线程池控制：3.1.0

使用 OpenMP 构建：正确

线程池控制信息：
       user_api: 布拉斯
   内部API：openblas
         前缀：libopenblas
       文件路径：/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/libopenblasp-r0.3.18.dylib
        版本：0.3.18
线程层：openmp
   架构：哈斯韦尔
    线程数：16

       user_api：openmp
   内部API：openmp
         前缀：libomp
       文件路径：/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/libomp.dylib
        版本：无
    线程数：16

]]></description>
      <guid>https://stackoverflow.com/questions/77269168/inconsistent-behavior-of-sklearns-precision-score-when-manual-cross-validation</guid>
      <pubDate>Tue, 10 Oct 2023 21:56:09 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow JS学习数据太大，内存一次装不下，如何学习？</title>
      <link>https://stackoverflow.com/questions/76615356/tensorflow-js-learning-data-too-big-to-fit-in-memory-at-once-how-to-learn</link>
      <description><![CDATA[我遇到的问题是，我的数据集变得太大，无法立即装入内存。从所有数据条目中学习什么好的解决方案？我的数据来自mongodb实例，需要异步加载。
我尝试使用生成器函数，但还无法让异步生成器工作。我也在想也许可以批量地将模型拟合到数据上？
如果有人能为我提供一个关于如何适应通过批处理或数据库游标异步加载的数据的最小示例，那就太好了。
例如，当尝试从生成器返回承诺时，我收到打字稿错误。
 constgenerate = function* () {
        产生新的 Promise(() =&gt; {});
    };

    tf.data.generator(生成);

&#39;() =&gt; 类型的参数生成器，无效，未知&gt;&#39;不可分配给 &#39;() =&gt; 类型的参数迭代器 | Promise&gt;&#39;。
&lt;小时/&gt;
使用异步生成器也不起作用：
异步生成器导致类型错误
tf.data.generator(异步函数* () {})

抛出
&#39;() =&gt; 类型的参数AsyncGenerator&lt;任何、无效、未知&gt;&#39;不可分配给 &#39;() =&gt; 类型的参数迭代器 | Promise&gt;&#39;。]]></description>
      <guid>https://stackoverflow.com/questions/76615356/tensorflow-js-learning-data-too-big-to-fit-in-memory-at-once-how-to-learn</guid>
      <pubDate>Tue, 04 Jul 2023 19:15:23 GMT</pubDate>
    </item>
    <item>
      <title>SHAP中的Explainer和Kernelexplainer有什么区别？</title>
      <link>https://stackoverflow.com/questions/74251331/what-is-difference-between-explainer-and-kernelexplainer-in-shap</link>
      <description><![CDATA[我对可解释的人工智能很陌生。我开始研究 SHAP。当我查看代码时，我很困惑。该网站是 https:// /towardsdatascience.com/using-shap-v​​alues-to-explain-how-your-machine-learning-model-works-732b3f40e137
您可以看到下面的代码：
# 适合解释器
解释器 = shap.Explainer(model.predict, X_test)
# 计算 SHAP 值 - 需要一些时间
shap_values = 解释器(X_test)

另一个网站是 https://snyk.io/advisor/python /shap/functions/shap.KernelExplainer
您可以看到下面的代码：
 # 使用 Kernel SHAP 解释测试集预测
    解释器 = shap.KernelExplainer(svm.predict_proba, X_train, nsamples=100, link=“logit”)
    shap_values = 解释器.shap_values(X_test)

有什么区别？哪一个是真的？在第一个代码中，X_test 用于解释器。在第二个代码中，X_train 用于 kernelexplainer。为什么？]]></description>
      <guid>https://stackoverflow.com/questions/74251331/what-is-difference-between-explainer-and-kernelexplainer-in-shap</guid>
      <pubDate>Sun, 30 Oct 2022 07:48:03 GMT</pubDate>
    </item>
    </channel>
</rss>