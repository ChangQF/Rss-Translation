<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 02 Jul 2024 01:05:21 GMT</lastBuildDate>
    <item>
      <title>HTML 文件按钮未生成</title>
      <link>https://stackoverflow.com/questions/78694578/html-file-button-not-generating</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78694578/html-file-button-not-generating</guid>
      <pubDate>Mon, 01 Jul 2024 23:43:32 GMT</pubDate>
    </item>
    <item>
      <title>我尝试使用 hugginsface 中的 convert_graph_to_onnx.py，但出现此错误：“转换模型时出错：未安装模块 onnx！”</title>
      <link>https://stackoverflow.com/questions/78694557/i-tried-to-use-convert-graph-to-onnx-py-from-hugginsface-but-i-got-the-this-erro</link>
      <description><![CDATA[我尝试使用此 [page][1] 中的示例使用导出模型功能
python convert_graph_to_onnx.py --framework pt --model bert-base-cased bert-base-cased.onnx

我收到一个错误，我需要使用 ONNX opset 版本 14，因此我使用了这个：
python convert_graph_to_onnx.py --framework pt --opset 14 --model bert-base-cased bert-base-cased.onnx

我收到此错误：

====== 将模型转换为 ONNX ====== convert_graph_to_onnx.py:361: FutureWarning: &#39;transformers.convert_graph_to_onnx` 包已弃用并将在 Transformers 版本 5 中删除
warnings.warn( ONNX opset 版本设置为：14 加载管道（模型：bert-base-cased，tokenizer：bert-base-cased）使用框架 PyTorch：2.3.1+cpu 发现输入 input_ids 形状：{0：&#39;batch&#39;，1：&#39;sequence&#39;} 发现输入 token_type_ids 形状：{0：&#39;batch&#39;，1：
&#39;sequence&#39;} 发现输入tention_mask 形状：{0：&#39;batch&#39;，1：
&#39;sequence&#39;} 发现输出 output_0 形状：{0：&#39;batch&#39;，1：
&#39;sequence&#39;} 发现输出 output_1 形状：{0：&#39;batch&#39;} 确保输入的顺序正确 position_ids 不存在于生成的输入列表中。生成的输入顺序：[&#39;input_ids&#39;,
&#39;attention_mask&#39;, &#39;token_type_ids&#39;] 转换模型时出错：
未安装模块 onnx！

我安装了 onnx==1.16.1
有人能帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/78694557/i-tried-to-use-convert-graph-to-onnx-py-from-hugginsface-but-i-got-the-this-erro</guid>
      <pubDate>Mon, 01 Jul 2024 23:34:48 GMT</pubDate>
    </item>
    <item>
      <title>在 cross_val_score 中获取 mac-avg f1-score</title>
      <link>https://stackoverflow.com/questions/78694297/get-mac-avg-f1-score-in-cross-val-score</link>
      <description><![CDATA[我正在尝试执行一个简单的二元分类实验。这是代码
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.model_selection import cross_val_score
import xgboost as xgb
models = [
RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0),
LinearSVC(),
MultinomialNB(),
LogisticRegression(random_state=0),
xgb.XGBClassifier(max_depth=3, 
objective=&#39;binary:logistic&#39;, 
n_estimators=100, 
num_classes=2, 
n_jobs = -1)
]
CV = 5
cv_df = pd.DataFrame(index=range(CV * len(models)))
entries = []
for模型中的模型：
model_name = model.__class__.__name__
f1_score = cross_val_score(model, X_prep, labels,scoring = make_scorer(f1_score, average=&#39;weighted&#39;, labels=[2]), cv=CV)
for fold_idx, f1score in enumerate(f1_score):
entrys.append((model_name, fold_idx, f1score))

我收到以下错误消息：
-------------------------------------------------------------------------------
InvalidParameterError Traceback (最近一次调用最后一次)
Cell In[34]，第 22 行
20 模型中的模型：
21 model_name = model.__class__.__name__
---&gt; 22 f1_score = cross_val_score(model, X_prep, labels,scoring = make_scorer(f1_score, average=&#39;weighted&#39;, labels=[2]), cv=CV)
23 for fold_idx, f1score in enumerate(f1_score):
24 entities.append((model_name, fold_idx, f1score))

文件 ~/anaconda3/envs/gpt-ds/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:203，在validate_params.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)
200 to_ignore += [&quot;self&quot;, &quot;cls&quot;]
201 params = {k: v for k, v 在 params.arguments.items() 中，如果 k 不在 to_ignore 中}
--&gt; 203 验证参数约束 (
204 参数约束，参数，调用者名称 = 函数。__qualname__
205 )
207 尝试：
208 使用 config_context (
209 跳过参数验证 = (
210 首选跳过嵌套验证或全局跳过验证
211 )
212 ):

文件 ~/anaconda3/envs/gpt-ds/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:95，在验证参数约束 (参数约束，参数，调用者名称) 中
89 其他：
90 约束字符串 = (
91 f&quot;{&#39;, &#39;.join([str(c) for c in约束[:-1]])} 或&quot;
92 f&quot; {约束[-1]}&quot;
93 )
---&gt; 95 引发 InvalidParameterError(
96 f&quot;{caller_name} 的 {param_name!r} 参数必须是&quot;
97 f&quot; {约束_str}。得到的是 {param_val!r}。&quot;
98 )

InvalidParameterError: make_scorer 的 &#39;score_func&#39; 参数必须是可调用的。得到的是数组([0., 0., 0., 0., 0.])。

如何获取 cross_val_score 的 mac_avg f1 分数？]]></description>
      <guid>https://stackoverflow.com/questions/78694297/get-mac-avg-f1-score-in-cross-val-score</guid>
      <pubDate>Mon, 01 Jul 2024 21:32:42 GMT</pubDate>
    </item>
    <item>
      <title>如何在 macOS 10.12 上运行 Core ML 模型？</title>
      <link>https://stackoverflow.com/questions/78694076/how-can-one-run-a-core-ml-model-on-macos-10-12</link>
      <description><![CDATA[https://developer.apple.com/documentation/coreml 提到 macOS 10.13+：

如何在 macOS 10.12 上运行 Core ML 模型？

在 Ubuntu 20.04 上创建的 Core ML 模型示例（使用 Python 3.10 和 torch 2.3.1 测试）：
git clone https://github.com/huggingface/exporters.git
cd exporters
pip install -e .
python -m exporters.coreml --model=distilbert-base-uncasederated/ --quantize=float32 
]]></description>
      <guid>https://stackoverflow.com/questions/78694076/how-can-one-run-a-core-ml-model-on-macos-10-12</guid>
      <pubDate>Mon, 01 Jul 2024 20:13:24 GMT</pubDate>
    </item>
    <item>
      <title>本地机器上的实时语音分割 [无 GPU/外部 API] [关闭]</title>
      <link>https://stackoverflow.com/questions/78693474/realtime-speech-diarization-on-local-machine-no-gpu-external-apis</link>
      <description><![CDATA[我正在寻找一些在本地机器上不使用任何 GPU 的实时语音分割解决方案。目前有类似的东西吗？
我知道分割对于 CPU 来说是一项复杂的任务，老实说，我的任务甚至不需要分割。我想要实现的任务是麦克风的音频将继续流式传输，任何随机的人都可以对着麦克风说话，但当第一个人已经在说话时，每当另一个人要说话时，代码就会指出检测到了第二个人。就是这样！分割其实是不需要的，但除了分割之外，我想不出更好的解决方案来实现我想要的。
目前有没有这样的解决方案可以用于我的任务？
我能找到的只有 pyannote 解决方案、Nvidia 的 NeMo 和一些其他解决方案，但它们都必须加载需要高 GPU RAM 的重型模型。我想要一些可以在本地 CPU 上运行的简单方法。而且我绝对不允许使用付费的外部 API，例如 Assembly AI/Deepgram。]]></description>
      <guid>https://stackoverflow.com/questions/78693474/realtime-speech-diarization-on-local-machine-no-gpu-external-apis</guid>
      <pubDate>Mon, 01 Jul 2024 17:09:07 GMT</pubDate>
    </item>
    <item>
      <title>如何解决VAE训练中的梯度爆炸问题？</title>
      <link>https://stackoverflow.com/questions/78693456/how-to-solve-exploding-gradient-problem-in-vae-training</link>
      <description><![CDATA[我尝试在 CelebA 数据集上实现 VAE，灵感来自 MNIST 的 Tensorflow 实现。我尝试过改变批处理大小，但似乎没有效果。形成的图像大部分都是灰色的。理想情况下，我们希望 KL 散度和重建损失都接近于零，但在我的例子中，两者都呈指数增长。
这是我得到的损失曲线。
这是损失函数定义块：
optimizer = tf.keras.optimizers.Adam(1e-4)
def log_normal_pdf(sample, mean, logvar, raxis=1):
log2pi = tf.math.log(2. * np.pi)
return tf.reduce_sum(
-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),
axis=raxis)

def compute_loss(model, x):
mean, logvar = model.encode(x)
z = model.reparameterize(mean, logvar)
x_logit = model.decode(z)
cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)
#logpx_z = tf.reduce_mean(tf.square(x - x_logit), axis=[1, 2, 3])
logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])
logpz = log_normal_pdf(z, 0., 0.)
logqz_x = log_normal_pdf(z, mean, logvar)
return -tf.reduce_mean(logpx_z + logpz - logqz_x), logpx_z, logqz_x-logpz

我的潜在维度是 16，批量大小为 500。另外，我的输入只有 500 张图片。
我已经尝试更改输入的大小，但似乎没有影响。
以下是模型定义：
class CVAE(tf.keras.Model):
def __init__(self, latent_dim):
super(CVAE, self).__init__()
self.latent_dim = latent_dim
self.encoder = tf.keras.Sequential(
[
tf.keras.layers.InputLayer(input_shape=(64, 64, 3)),
tf.keras.layers.Conv2D(
filters=32, kernel_size=3, strides=(2, 2),activation=&#39;relu&#39;),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Conv2D(
filters=64, kernel_size=3, strides=(2, 2),activation=&#39;relu&#39;),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Conv2D(
filters=128, kernel_size=3, strides=(2, 2), activity=&#39;relu&#39;),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Conv2D(
filters=256, kernel_size=3, strides=(2, 2), activity=&#39;relu&#39;),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Conv2D(
filters=512, kernel_size=3, strides=(2, 2), activity=&#39;relu&#39;),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Flatten(),
# 无激活
tf.keras.layers.Dense(latent_dim + latent_dim),
]
)

self.decoder = tf.keras.Sequential(
[
tf.keras.layers.InputLayer(input_shape=(latent_dim,)),
tf.keras.layers.Dense(units=4*4*256,activation=tf.nn.relu),
tf.keras.layers.Reshape(target_shape=(4, 4, 256)),
tf.keras.layers.Conv2DTranspose(
filters=128, kernel_size=3, strides=2, padding=&#39;same&#39;,
activation=&#39;relu&#39;),
tf.keras.layers.Conv2DTranspose(
filters=64, kernel_size=3, strides=2, padding=&#39;same&#39;,
激活=&#39;relu&#39;),
tf.keras.layers.Conv2DTranspose(
过滤器=32，kernel_size=3，strides=2，padding=&#39;same&#39;,
激活=&#39;relu&#39;),
tf.keras.layers.Conv2DTranspose(
过滤器=3，kernel_size=3，strides=2，padding=&#39;same&#39;),
]
)
@tf.function
def sample(self，eps=None):
如果 eps 为 None:
eps = tf.random.normal(shape=(100，self.latent_dim))
返回 self.decode(eps，apply_sigmoid=True)

def encode(self，x):
平均值，logvar = tf.split(self.encoder(x)，num_or_size_splits=2，axis=1)
返回平均值，logvar

def reparameterize(self，平均值， logvar):
eps = tf.random.normal(shape=mean.shape)
return eps * tf.exp(logvar * .5) + mean

def decrypt(self, z, apply_sigmoid=False):
logits = self.decoder(z)
if apply_sigmoid:
probs = tf.sigmoid(logits)
return probs
return logits

这是 colab 笔记本的链接]]></description>
      <guid>https://stackoverflow.com/questions/78693456/how-to-solve-exploding-gradient-problem-in-vae-training</guid>
      <pubDate>Mon, 01 Jul 2024 17:03:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 Detectron2 进行多任务问题分割和关键点检测</title>
      <link>https://stackoverflow.com/questions/78692534/multitask-issue-segmentation-keypoint-detection-with-detectron2</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78692534/multitask-issue-segmentation-keypoint-detection-with-detectron2</guid>
      <pubDate>Mon, 01 Jul 2024 13:32:45 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 pytorch 提取视觉变换器的倒数第二层输出</title>
      <link>https://stackoverflow.com/questions/78691616/cannot-extract-penultimate-layer-output-of-a-vision-transformer-with-pytorch</link>
      <description><![CDATA[我有以下模型，该模型使用我自己的 DataParallel 训练的数据集进行了调整：
model = timm.create_model(&#39;vit_base_patch16_224&#39;, pretrained=False)
model.head = nn.Sequential(nn.Linear(768, 512),nn.ReLU(),nn.BatchNorm1d(512),nn.Dropout(p=0.2),nn.Linear(512, 141))
checkpoint = torch.load(&#39;vit_b_16v3.pth&#39;)
checkpoint = {k.partition(&#39;module.&#39;)[2]: v for k, v in checkpoint.items()}
# 加载参数
model.load_state_dict(checkpoint)

但是，我不知道如何获取这种视觉转换器的倒数第二层输出。我尝试了本教程，但不起作用。我只想输入一张图片，并有一个 512 维向量来描述它。使用 Tensorflow 做这件事很容易，但在 Pytorch 中我却很挣扎。
我最后的几层如下：
(norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(fc_norm): Identity()
(head_drop): Dropout(p=0.0, inplace=False)
(head): Sequential(
(0): Linear(in_features=768, out_features=512, bias=True)
(1): ReLU()
(2): BatchNorm1d(512, eps=1e-05, motivation=0.1, affine=True, track_running_stats=True)
(3): Dropout(p=0.2, inplace=False)
(4): Linear(in_features=512, out_features=141, bias=True)
)
)
]]></description>
      <guid>https://stackoverflow.com/questions/78691616/cannot-extract-penultimate-layer-output-of-a-vision-transformer-with-pytorch</guid>
      <pubDate>Mon, 01 Jul 2024 10:16:39 GMT</pubDate>
    </item>
    <item>
      <title>Roboflow Vs. Darknet 用于生成权重文件和创建模型</title>
      <link>https://stackoverflow.com/questions/78691574/roboflow-vs-darknet-for-generating-weight-file-and-creating-the-model</link>
      <description><![CDATA[我有一个 YoloV8 数据文件格式，它是手动完成的数据（图像）注释。
生成模型并因此产生权重文件的最有效和最直接的方法是什么？是通过以下命令使用 darknet 吗：
darknet.exe detector train data/obj.data yolo-obj.cfg backup\yolo-obj_2000.weights

然后使用类似下面的命令生成关联模型：
python tools/model_converter/convert.py cfg/yolov3.cfg weights/yolov3.weights weights/yolov3.h5

或者通过以下命令使用 Roboflow：
version.deploy(model_type=&quot;yolov8&quot;, model_path=f”{HOME}/runs/detect/train/&quot;)

在我看来，darknet 更难安装。]]></description>
      <guid>https://stackoverflow.com/questions/78691574/roboflow-vs-darknet-for-generating-weight-file-and-creating-the-model</guid>
      <pubDate>Mon, 01 Jul 2024 10:07:36 GMT</pubDate>
    </item>
    <item>
      <title>学习 Python 的最佳书籍 [关闭]</title>
      <link>https://stackoverflow.com/questions/78690958/best-book-to-learn-python</link>
      <description><![CDATA[寻求具有最新更新的最佳 Python 书籍推荐
我渴望从头开始学习 Python，并及时了解该语言的最新发展。随着 Python 的快速发展，我想确保自己学习的是最新的功能、最佳实践和行业标准。
您能否推荐一本全面且适合初学者的书，涵盖 Python 3.x（最好是最新版本 Python 3.10 或 3.11），并包含以下主题：

核心 Python 概念：变量、数据类型、控制结构、函数、面向对象编程等
数据分析和可视化：NumPy、Pandas、Matplotlib 和 Seaborn
Web 开发：Flask 或 Django、HTML、CSS 和 JavaScript 基础知识
机器学习和人工智能：scikit-learn、TensorFlow 和 Keras
最佳实践和编码标准：代码组织、调试和测试
]]></description>
      <guid>https://stackoverflow.com/questions/78690958/best-book-to-learn-python</guid>
      <pubDate>Mon, 01 Jul 2024 07:35:55 GMT</pubDate>
    </item>
    <item>
      <title>在 shap.Explainer() 中，我应该输入 classifier 还是 classifier.predict？那么，如何获取 shap 值？[关闭]</title>
      <link>https://stackoverflow.com/questions/78690391/in-shap-explainer-should-i-input-classifier-or-classifier-predict-then-how</link>
      <description><![CDATA[我正在使用 SHAP（Shapley Additive Explanations）。我理解工作流程必须是：将我的数据拆分为训练和测试，训练我的模型，运行 SHAP 解释器，获取 SHAP 值。
当然，我见过使用不同方法做同样事情的代码（有点像 Perl 哲学，但很好）。我搞不懂它们之间的区别。我已阅读 SHAP 文档，但未能理解正确的方法。
我已看到两者：

explainer = shap.Explainer(clf)
explainer = shap.Explainer (clf.predict, X train)

后来，为了获取 shap 值，我看到了：

explainer(X)
explainer(X_test)
explainer.shap_values(X_test) - 这是我唯一理解差异的。它返回一个 numpy 数组，而不是 shap 解释对象。

下面，我复制了一些我见过的例子。
在Towards Data Science中：
X_train, X_test, y_train, y_test = train_test_split(X, y)
clf.fit(X_train, y_train)

explainer = shap.Explainer(clf.predict, X_test)
shap_values = explainer(X_test)

在 SHAP 官方GitHub 页面（不是他们的文档）
explainer = shap.Explainer(clf)
shap_values = explainer(X)

在geeks for geeks和datacamp
X_train, X_test, y_train, y_test = train_test_split(X, y)
clf.fit(X_train, y_train)

解释器 = shap.Explainer(clf)
shap_values = explainer(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/78690391/in-shap-explainer-should-i-input-classifier-or-classifier-predict-then-how</guid>
      <pubDate>Mon, 01 Jul 2024 03:49:45 GMT</pubDate>
    </item>
    <item>
      <title>wandb 的超频算法在哪些时期检查改进？</title>
      <link>https://stackoverflow.com/questions/78530549/at-which-epochs-does-the-hyperband-algorithm-of-wandb-checks-for-improvement</link>
      <description><![CDATA[我正在尝试使用 wandb 库的超参数调整（又名扫描）功能（链接到其官方页面）。我正在尝试应用贝叶斯超带算法。
现在，正如这些页面中提到的那样（如何定义扫描配置），（与提前终止选项相关的参数是什么），在提前终止下，我们必须提到 4 个参数（一般），它们是 min_iter、s、eta 和 max_iter，它看起来如下所示。
我的疑问总结：
总之，我想知道的是，
给定所有 4 个：- min_iter、s、eta 和max_iter

超频带算法将在哪些时期检查改进？

考虑到我正在尝试做贝叶斯超频带，第一个括号中将评估多少次运行，连续的括号中将评估多少次运行？

是否有任何方法或经验法则来决定这 4 个参数（min_iter、s、eta 和 max_iter）的取值？

请更详细地解释一下参数 s 和 eta（特别是 eta），即使用一些基础数学知识（如果可能，请保持简单）。


我对什么有疑问？ （更详细/上下文解释）：
（这里），他们有点解释了在哪些时期（他们的）超频带算法实现检查改进并决定是否终止运行。
当我们只关注每次运行的最小迭代次数时
当我们只关注每次运行的最小迭代次数时
但是对于我们关注的是每次运行的最小和最大迭代次数的情况？
就像下面这个...
#在 yaml 文件中，由 wanbd 在 python 中使用
early_terminate:
type: hyperband
min_iter: 10
s: 3
eta: 4
max_iter: 50

我已经尝试过的：
我甚至尽我所能阅读原始论文并试图了解发生了什么（或可能发生什么）（链接到 hyperband 算法的原始论文），但无法得到满意的答案。
我甚至尝试访问他们的 github 页面，那里有示例，但他们只展示了如何编写配置，并没有深入解释它的作用。]]></description>
      <guid>https://stackoverflow.com/questions/78530549/at-which-epochs-does-the-hyperband-algorithm-of-wandb-checks-for-improvement</guid>
      <pubDate>Fri, 24 May 2024 20:25:42 GMT</pubDate>
    </item>
    <item>
      <title>将 ONNX 模型转换为 Tensorflow Lite - 不支持 pytorch_half_pixel</title>
      <link>https://stackoverflow.com/questions/78218890/converting-onnx-model-to-tensorflow-lite-pytorch-half-pixel-not-supported</link>
      <description><![CDATA[我正在尝试将 ONNX 模型转换为 Tensorflow Lite 格式。代码简单，但出现此错误。我更新了我的 onnx 版本，但没有成功
import onnx
import tensorflow as tf
import onnx_tf
#
#
# README：此文件将 onnx 模型转换为 tflite
#
#
#

onnx_model_path = &#39;/home/sfrye/segmentation/segmentation_checkpoints/efficientnet/modified-new.onnx&#39;

onnx_model = onnx.load(onnx_model_path)

tf_model = onnx_tf.backend.prepare(onnx_model)
tf_model.export_graph(&quot;tflite_model.tf&quot;)

错误如下

RuntimeError：在用户代码中：

文件&quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/backend_tf_module.py&quot;，第 99 行，在 __call__ *
output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,
File &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/backend.py&quot;，第 347 行，在 _onnx_node_to_tensorflow_op *
return handler.handle(node, tensor_dict=tensor_dict, strict=strict)
File &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/handlers/handler.py&quot;，第 58 行，在 handle *
cls.args_check(node, **kwargs)
文件 &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/handlers/backend/resize.py&quot;，第 125 行，在 args_check *
exception.OP_UNSUPPORTED_EXCEPT(
文件 &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/common/exception.py&quot;，第 50 行，在 __call__ *
raise self._func(self.get_message(op, framework))

RuntimeError: Tensorflow 不支持调整 coordinate_transformation_mode=pytorch_half_pixel 的大小。


我尝试更新我的 onnx，因为这解决了此错误代码的某些问题]]></description>
      <guid>https://stackoverflow.com/questions/78218890/converting-onnx-model-to-tensorflow-lite-pytorch-half-pixel-not-supported</guid>
      <pubDate>Mon, 25 Mar 2024 11:55:44 GMT</pubDate>
    </item>
    <item>
      <title>反向传播和反向模式自动微分之间有什么区别？</title>
      <link>https://stackoverflow.com/questions/49926192/what-is-the-difference-between-backpropagation-and-reverse-mode-autodiff</link>
      <description><![CDATA[通过阅读这本书，我熟悉了以下内容：

对于每个训练实例，反向传播算法首先进行
预测（正向传递），测量误差，然后反向遍历每个
层以测量每个
连接的误差贡献（反向传递），最后稍微调整连接
权重以减少误差。

但是我不确定这与 TensorFlow 的反向模式自动微分实现有何不同。
据我所知，反向模式自动微分首先沿正向遍历图形，然后在第二遍中计算输出相对于输入的所有偏导数。这与传播算法非常相似。
反向传播与反向模式自动微分有何不同？]]></description>
      <guid>https://stackoverflow.com/questions/49926192/what-is-the-difference-between-backpropagation-and-reverse-mode-autodiff</guid>
      <pubDate>Thu, 19 Apr 2018 16:43:07 GMT</pubDate>
    </item>
    <item>
      <title>当setCar设置为true时，如何显示前提和后果？</title>
      <link>https://stackoverflow.com/questions/39066421/how-to-display-the-premise-and-consequence-when-the-setcar-is-set-to-true</link>
      <description><![CDATA[我想在 Weka 3.8.0 中运行 apriori 算法后，获取生成规则的每一行的前提和后果。
 apriori.setNumRules(NUMBER_OF_RULES);
apriori.setMinMetric(MINIMUM_CONFIDENCE);
apriori.setLowerBoundMinSupport(MINIMUM_SUPPORT);

apriori.setCar(true);

apriori.buildAssociations(instances);

我尝试使用下面的代码来获取规则，但它给出了一个异常
（weka.associations.ItemSet 无法转换为 weka.associations.AprioriItemSet）：
 AssociationRules arules = apriori.getAssociationRules();

此外，我尝试使用 getAllTheRules() 方法，但它给出了不同的结果。
 ArrayList&lt;Object&gt;[] arules = apriori.getAllTheRules();
System.out.println(((ItemSet)arules[0].get(1)).getRevision()); //12014
System.out.println(((ItemSet)arules[0].get(2)).getRevision()); //12014
System.out.println(((ItemSet)arules[0].get(5)).getRevision()); //12014
]]></description>
      <guid>https://stackoverflow.com/questions/39066421/how-to-display-the-premise-and-consequence-when-the-setcar-is-set-to-true</guid>
      <pubDate>Sun, 21 Aug 2016 16:30:51 GMT</pubDate>
    </item>
    </channel>
</rss>