<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 22 Oct 2024 21:16:23 GMT</lastBuildDate>
    <item>
      <title>如何对数据框中的单个列进行单列编码？</title>
      <link>https://stackoverflow.com/questions/79114762/how-do-i-onehotencode-a-single-column-in-a-dataframe</link>
      <description><![CDATA[我有一个名为“vehicles”的数据框，它有 8 列。其中 7 列是数字，但名为“Car_name”的列在数据框中是索引 1，是分类的。我需要对其进行编码
我试过这个代码，但不起作用
ohe = OneHotEncoding(categorical_features = [1])

vehicles_enc = ohe.fit_transform(vehicles).toarray()

TypeError: OneHotEncoder.__init__() 获得了一个意外的关键字参数“categorical_features”

然而，这在我使用的 YouTube 视频中运行良好。]]></description>
      <guid>https://stackoverflow.com/questions/79114762/how-do-i-onehotencode-a-single-column-in-a-dataframe</guid>
      <pubDate>Tue, 22 Oct 2024 15:12:04 GMT</pubDate>
    </item>
    <item>
      <title>如何将 *.mil 加载到 coremltools 中？</title>
      <link>https://stackoverflow.com/questions/79114613/how-to-load-a-mil-into-coremltools</link>
      <description><![CDATA[在 Apple 的文档中，模型中间语言 (MIL) 被描述为一种中间语言。我在 Apple 的系统中发现了许多 .mil 文件。您可以使用以下命令轻松找到它们：
find /System/Library -name &quot;*.mil&quot;

我正在尝试研究这些 .mil 文件并将它们转换为 Core ML 文件以运行它们。
apple 的示例向我们展示了如何生成 mil 程序。但是我找不到从 *.mil 文件加载 mil 程序的方法。
以下是 Apple 的示例。
# import builder
from coremltools.converters.mil import Builder as mb

# MIL 程序的输入是张量列表。这里我们有一个输入，其
# 形状为 (1, 100, 100, 3)，隐式 dtype == fp32
@mb.program(input_specs=[mb.TensorSpec(shape=(1, 100, 100, 3)),])
def prog(x):
# MIL 操作采用命名输入（而不是位置输入）。
# 此处 `name` 参数是可选的。
x = mb.relu(x=x, name=&#39;relu&#39;)
x = mb.transpose(x=x, perm=[0, 3, 1, 2], name=&#39;transpose&#39;)
x = mb.reduce_mean(x=x, axis=[2, 3], keep_dims=False, name=&#39;reduce&#39;)
x = mb.log(x=x, name=&#39;log&#39;)
return x

print(prog)

main(%x: (1, 100, 100, 3, fp32)(Tensor)) {
block0() {
%relu: (1, 100, 100, 3, fp32)(Tensor) = relu(x=%x, name=&quot;relu&quot;)
%transpose_perm_0: (4,i32)*(Tensor) = const(val=[0, 3, 1, 2], name=&quot;transpose_perm_0&quot;)
%transpose: (1, 3, 100, 100, fp32)(Tensor) = transpose(x=%relu, perm=%transpose_perm_0, name=&quot;transpose&quot;)
%reduce_axes_0: (2,i32)*(Tensor) = const(val=[2, 3], name=&quot;reduce_axes_0&quot;)
%reduce_keep_dims_0: (bool)*(Scalar) = const(val=False, name=&quot;reduce_keep_dims_0&quot;)
%reduce: (1, 3, fp32)(Tensor) = reduce_mean(x=%transpose, axis=%reduce_axes_0, keep_dims=%reduce_keep_dims_0，name=&quot;reduce&quot;)
%log_epsilon_0: (fp32)*(Scalar) = const(val=1e-45，name=&quot;log_epsilon_0&quot;)
%log: (1, 3, fp32)(Tensor) = log(x=%reduce，epsilon=%log_epsilon_0，name=&quot;log&quot;)
} -&gt; (%log)
}

以下是来自 macOS 的 *.mil 文件
/System/Library/AssetsV2/com_apple_MobileAsset_UAF_Siri_Understanding/purpose_auto/86aa08fa2cf25cbea1f9f8dee8c0bdad3a3802b9.asset/AssetData/sr/Hermes/int8_conformer_matrix_split.mlmodelc/model.bnns.mil
program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{&quot;coremlc-component-MIL&quot;, &quot;5.33.5&quot;}, {&quot;coremlc-version&quot;, &quot;1877.0.10.0.2&quot;}, {&quot;coremltools-component-torch&quot;, &quot;2.0.1&quot;}, {&quot;coremltools-source-dialect&quot;, &quot;TorchScript&quot;}, {&quot;coremltools-version&quot;, &quot;7.1.2&quot;}})]
{
func main&lt;ios17&gt;(tensor&lt;fp16, [256, 14]&gt; conformer_cnn_cache_0_shared_in, tensor&lt;fp16, [256, 14]&gt; conformer_cnn_cache_1_shared_in, tensor&lt;fp16, [256, 14]&gt; constrainer_cnn_cache_2_shared_in，张量
所以我的最后一个问题是

有没有办法将 .mil 文件加载到 coremltools.converters.mil.mil.program.Program？

上面 Apple 网站上的 mil 和 /System/Library 中的 mil 一样吗？

]]></description>
      <guid>https://stackoverflow.com/questions/79114613/how-to-load-a-mil-into-coremltools</guid>
      <pubDate>Tue, 22 Oct 2024 14:38:22 GMT</pubDate>
    </item>
    <item>
      <title>决策树的修剪函数</title>
      <link>https://stackoverflow.com/questions/79113940/prune-function-for-decision-tree</link>
      <description><![CDATA[我正在从头开始创建决策树并实施修剪。目前，我认为我的代码中的问题是，当我修剪一棵树时，我创建的新叶节点不会放入原始树中，因此当我计算新的准确度时，它是有效的，并且我的所有分支都会被修剪。附件是我的树类、叶类和修剪函数的代码。
我的问题是，当我尝试计算新的准确度时，我为尝试修剪而创建的新叶节点似乎没有反映在原始树中。
我如何修复/重构我的代码，以便修剪更新当前树，这样当我计算新的准确度时，就可以确定修剪是否成功？
class TreeNode:
def __init__(self, feature, split,depth, left = None, right = None):
&quot;&quot;&quot;
self.feature = 节点分裂的特征
self.split = 节点分裂的特征的值
self.left = 左子节点
self.right = 右子节点
self.depth = 此时树的深度
&quot;&quot;&quot;
self.feature = 特征
self.split = 分裂
self.left = 左
self.right = 右
self.depth = 深度

def getLeft(self):
return self.left

def getRight(self):
return self.right

def getFeature(self):
return self.feature

def getSplit(self):
return self.split

def getDepth(self):
return self.depth

def eval(self, sample):
if sample[self.feature] &lt; self.split:
return self.left.eval(sample)
else:
return self.right.eval(sample)

class LeafNode:
def __init__(self, roomNumber, users,depth):
&quot;&quot;&quot;
self.roomNumer = 分配给叶子的房间号
self.depth = 树中叶子的深度
&quot;&quot;&quot;
self.roomNumber = roomNumber
self.depth =depth
self.users = users

def getRoomNumber(self):
返回 self.roomNumber

def getDepth(self):
返回 self.depth

def getUsers(self):
返回 self.users

def eval(self, sample):
self.users += 1
返回 self.getRoomNumber()

def pruneTree(original_tree, validation, node):
如果节点为 None:
返回 None
如果 isinstance(node, LeafNode):
返回节点
node.left = pruneTree(original_tree, validation, node.left)
node.right = pruneTree(original_tree, validation, node.right)
如果 isinstance(node.left, LeafNode) 和 isinstance(node.right, LeafNode):

current_accuracy = assess(validation, original_tree)

leftRoom, leftPopulation = node.left.getRoomNumber(), node.left.getUsers()

rightRoom, rightPopulation = node.right.getRoomNumber(), node.right.getUsers()

previous_feature, previous_split, previous_depth, previous_left, previous_right = node.getFeature(), node.getSplit(), node.getDepth(), node.getLeft(), node.getRight()

newRoom = -1

newPopulation = leftPopulation + rightPopulation

如果 rightPopulation &gt;= leftPopulation:
newRoom = rightRoom
否则:
newRoom = leftRoom

node = LeafNode(roomNumber = newRoom, users=newPopulation,depth = previous_depth)

new_accuracy = assess(validation, original_tree)

如果 new_accuracy &lt; current_accuracy:
node = TreeNode(split = previous_split, feature=previous_feature,depth=previous_depth)
node.left = previous_left
node.right = previous_right
返回节点

def assess(test_db, trained_tree):
num_correct = 0
对于 test_db 中的数据：
sample = data[:-1]
prediction = trained_tree.eval(sample)

如果 prediction == data[-1]:
num_correct += 1
返回 num_correct/len(test_db)

pruned_tree = pruneTree(tree, validation, tree)
]]></description>
      <guid>https://stackoverflow.com/questions/79113940/prune-function-for-decision-tree</guid>
      <pubDate>Tue, 22 Oct 2024 11:59:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 macOS 中加载 *.mil 文件？[关闭]</title>
      <link>https://stackoverflow.com/questions/79113430/how-to-load-a-mil-file-in-macos</link>
      <description><![CDATA[在 Apple 的文档中，模型中间语言 (MIL) 被描述为一种中间语言。我在 Apple 的系统中发现了许多 .mil 文件。您可以使用以下命令轻松找到它们：
find /System/Library -name &quot;*.mil&quot;

我正在尝试研究这些 .mil 文件并将它们转换为 Core ML 文件以运行它们。但是，我在 coremltools 中找不到允许我这样做的任何功能。您知道有什么方法可以实现这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/79113430/how-to-load-a-mil-file-in-macos</guid>
      <pubDate>Tue, 22 Oct 2024 09:45:03 GMT</pubDate>
    </item>
    <item>
      <title>过滤相关图像的自动化方法[关闭]</title>
      <link>https://stackoverflow.com/questions/79113214/automated-method-to-filter-relevant-images</link>
      <description><![CDATA[我有一组遥感图像，如下图所示。总共有大约 100,000 张图像，但只有少数与我的任务相关。这些图像中有很多都是我不需要的海洋之类的东西，或者只是损坏的照片。我感兴趣的是下图中第 2 和第 4 幅图像（它们包括森林区域、道路、村庄等）。我想知道是否有某种自动方法来选择这些图像。绘制颜色直方图时肯定存在模式，但我很难想出一种方法来从不相关的图像中过滤出相关的图像。这只是为了数据清理步骤。
我刚刚研究了深度聚类技术，但想知道是否有更简单的解决方案，因为它只是为了数据清理。
任何有关此任务的信息都非常感谢！
]]></description>
      <guid>https://stackoverflow.com/questions/79113214/automated-method-to-filter-relevant-images</guid>
      <pubDate>Tue, 22 Oct 2024 08:57:14 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法将 MS COCO 2017 测试数据集中的图像分成包含小、中、大物体的图像？</title>
      <link>https://stackoverflow.com/questions/79113120/is-there-a-way-to-segregate-images-in-ms-coco-2017-test-dataset-into-images-cont</link>
      <description><![CDATA[我是计算机视觉领域的新手，正在从事一项小任务，即在 MS COCO 测试数据集中分离包含小物体的图像。由于测试集没有注释，有什么方法可以完成此任务吗？如果您能提供任何帮助，我将不胜感激。
我尝试使用图像的宽度和高度参数，但在小图像部分只得到了 360 张图像。如何获取测试集的注释信息？]]></description>
      <guid>https://stackoverflow.com/questions/79113120/is-there-a-way-to-segregate-images-in-ms-coco-2017-test-dataset-into-images-cont</guid>
      <pubDate>Tue, 22 Oct 2024 08:33:45 GMT</pubDate>
    </item>
    <item>
      <title>SRGAN Android Tensorflow Lite 推理输出与 Python 版本不匹配</title>
      <link>https://stackoverflow.com/questions/79111493/srgan-android-tensorflow-lite-inference-output-doesnt-match-to-python-version</link>
      <description><![CDATA[我尝试使用此模型的 tflite 版本实现 Android Kotlin 应用，https://github.com/krasserm/super-resolution/blob/master
我有一个 gan_generator.tflite 模型。我可以推断它并在 Python 中使用它获得正确的输出。但是即使我进行了相同的后处理操作，我也无法在 Android Kotlin 中获得与位图相同的结果。我遗漏了什么？
from model.srgan import generator
from utils import load_image, plot_sample
from model import resolve_single
import tensorflow as tf

model = generator()
model.load_weights(&#39;weights/srgan/gan_generator.h5&#39;)

# 从 Keras 模型创建 TFLite 转换器
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# 设置转换参数（可选）
converter.optimizations = [tf.lite.Optimize.DEFAULT] 
converter.target_spec.supported_types = [tf.float32]

# 转换模型
tflite_model = converter.convert()

# 将 TFLite 模型保存到文件
with open(&#39;gan_generator.tflite&#39;, &#39;wb&#39;) as f:
f.write(tflite_model)

interpreter = tf.lite.Interpreter(model_path=&#39;gan_generator.tflite&#39;)
interpreter.allocate_tensors()

input_details = interpretationer.get_input_details()

output_details = interpretationer.get_output_details()

input_shape = input_details[0][&#39;shape&#39;]

input_data = np.asarray(load_image(&#39;demo/0869x4-crop.png&#39;), dtype=np.float32)
input_data = np.expand_dims(input_data, axis=0)
input_data = np.reshape(input_data, (1, input_data.shape[1], input_data.shape[2], 3))

# 设置输入张量
interpreter.resize_tensor_input(input_details[0][&#39;index&#39;], (1, input_data.shape[1], input_data.shape[2], 3),strict=True)
interpreter.allocate_tensors()

interpreter.set_tensor(input_details[0][&#39;index&#39;], input_data)

interpreter.invoke()

# 获取输出
output_data = interpretation.get_tensor(output_details[0][&#39;index&#39;])

output_data = np.squeeze(output_data)
output_data = np.clip(output_data, 0, 255).astype(np.uint8)

# 从 NumPy 数组创建 PIL 图像
image = Image.fromarray(output_data)
image #此处创建的输出图像成功

###----------Android 端-------------------------
 val tensorImage = TensorImage(INPUT_IMAGE_TYPE).also { it.load(inputBitmap) }
val processingImage = imageProcessor.process(tensorImage)

//[1,4X,4X,3] 输出
val outputBuffer = TensorBuffer.createFixedSize(
intArrayOf(
1,
processingImage.width * outputShapeForScaling,
processingImage.height * outputShapeForScaling,
3
),
OUTPUT_IMAGE_TYPE
)

// [1, None, None, 3] 动态输入
explainer!!.resizeInput(
explainer!!.getInputTensor(0).index(),
intArrayOf(1, processingImage.width, processingImage.height, 3)
)

解释器！！.allocateTensors()

解释器！！.run(processedImage.buffer, outputBuffer.buffer)

val processingOutput = processOutput(
outputBuffer,
width = processingImage.width * 4,
height = processingImage.height * 4
)


##---------后期处理---------------------------
private fun processOutput(outputBuffer: TensorBuffer, width: Int, height: Int): Bitmap {
val data = outputBuffer.floatArray

// 检查浮点数组是否具有正确数量的元素
if (data.size != width * height * 3) {
throw IllegalArgumentException(&quot;数据大小与预期图像大小不匹配。&quot;)
}

// 创建一个空的 Bitmap
val bitmap = Bitmap.createBitmap(width, height, Bitmap.Config.ARGB_8888)

//遍历浮点数组并设置位图中的像素
var index: Int
for (y in 0 till height) {
for (x in 0 till width) {
index = (y * width + x) * 3
// 从浮点数组中提取 RGB 值
val r = data[index].coerceIn(0f, 255f).toInt()
val g = data[index + 1].coerceIn(0f, 255f).toInt()
val b = data[index + 2].coerceIn(0f, 255f).toInt()

// 设置像素颜色（我们将 alpha 设置为 255 以实现完全不透明度）
bitmap.setPixel(x, y, Color.rgb(r, g, b))
}
}

return bitmap
}

结果：
]]></description>
      <guid>https://stackoverflow.com/questions/79111493/srgan-android-tensorflow-lite-inference-output-doesnt-match-to-python-version</guid>
      <pubDate>Mon, 21 Oct 2024 19:24:29 GMT</pubDate>
    </item>
    <item>
      <title>如何通过flask API调用逻辑回归模型？</title>
      <link>https://stackoverflow.com/questions/79111190/how-to-call-logistic-regression-model-through-flask-api</link>
      <description><![CDATA[创建了一个逻辑回归模型，该模型运行良好，在本地运行时（通过 PyCharm）预测的值到目前为止都是正确的。
尝试通过 flask API 调用模型，从而创建一个 pickle 文件。
不确定如何在 API 中调用 classifier.predict 方法（即下面代码中的第 7 行）？因为我没有可用的实例（我遗漏了什么？或者做错了什么？）
API 代码

从 flask 导入 Flask、request、render_template、jsonify
导入 pickle

app = Flask(__name__)

-- 加载 pickle 文件
使用 open(&#39;sc.pkl&#39;, &#39;rb&#39;) 作为文件：
model = pickle.load(file) 

@app.route(&quot;/test&quot;, methods=[&quot;GET&quot;])
def test():
trans = model.transform([[1, 0, 12, 30]])

-- 想法是先通过硬编码值进行测试，然后使其参数化。
这是我遇到困难的地方，因为我没有分类器的实例来调用预测方法（下面代码中的第 7 行）
这是实际的代码我已成功从 pycharm(.py 文件) 运行 -
1. 读取 CSV，
为 x、y 赋值，
运行 train_test_split 函数
2. sc = StandardScaler()
3. x_train = sc.fit_transform(x_train)
4. x_test = sc.transform(x_test)

5. classifier = LogisticRegression(random_state=0)
6. classifier.fit(x_train,y_train)

7. print(classifier.predict(sc.transform([[1, 0, 12,30]]))) -- 预测
8. pickle.dump(sc, open(&#39;sc.pkl&#39;,&#39;wb&#39;)) -- 此处正在创建 Pickle 文件
]]></description>
      <guid>https://stackoverflow.com/questions/79111190/how-to-call-logistic-regression-model-through-flask-api</guid>
      <pubDate>Mon, 21 Oct 2024 17:46:17 GMT</pubDate>
    </item>
    <item>
      <title>基于 CNN 的音频分析模型中的过度拟合问题：以 100% 的准确率将每个声音归类为救护车</title>
      <link>https://stackoverflow.com/questions/79109759/overfitting-issue-in-my-cnn-based-audio-analysis-model-classifying-every-sound</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79109759/overfitting-issue-in-my-cnn-based-audio-analysis-model-classifying-every-sound</guid>
      <pubDate>Mon, 21 Oct 2024 11:10:19 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的 Tensorflow predict() 时间序列对齐</title>
      <link>https://stackoverflow.com/questions/79106002/tensorflow-predict-timeseries-alignment-in-python</link>
      <description><![CDATA[假设我在 Tensorflow 中创建一个顺序输入 LSTM，如下所示：
def Sequential_Input_LSTM(df, input_sequence):
df_np = df.to_numpy()
X = []
y = []

for i in range(len(df_np) - input_sequence):
row = [a for a in df_np[i:i + input_sequence]]
X.append(row)
label = df_np[i + input_sequence]
y.append(label)

return np.array(X), np.array(y)

X, y = Sequential_Input_LSTM(df_data , 10) # pandas DataFrame df_data 包含我们的数据

在此示例中，我将数据切片X（输入向量）和 y（标签），例如前 10 个值（序列长度）用作 X，第 11 个值用作第一个 y。然后，将 10 个值的窗口向右移动一步（再移动一个时间步），我们再次为 X 取 10 个值，并将第二行之后的值作为下一个 y，依此类推。
然后假设我将 X 的一部分作为我的 X_test，并使用 LSTM model 进行时间序列预测，例如 predictions = model.predict(X_test)。
当我实际尝试此操作并绘制 predict(X_test) 的结果时，它看起来像 y 数组，并且预测结果是同步的，无需进一步调整。我预计在将预测数组与标签一起绘制时，我必须手动将预测数组向右移动 10 个时间步，因为我无法解释预测的前 10 个时间戳来自哪里。
由于模型尚未收到 10 个输入序列值，X_test 的前 10 个时间步的预测来自哪里？Tensorflow 是否使用 X_test 中的最后几个时间步来创建前 10 个值的预测，还是一开始的预测只是纯粹的猜测？]]></description>
      <guid>https://stackoverflow.com/questions/79106002/tensorflow-predict-timeseries-alignment-in-python</guid>
      <pubDate>Sat, 19 Oct 2024 21:37:05 GMT</pubDate>
    </item>
    <item>
      <title>Val_accuracy 正在改变，有时它在补码之间交替（100％-val_acc）</title>
      <link>https://stackoverflow.com/questions/78885395/val-accuracy-inst-changing-and-sometimes-it-alternates-between-it-complement-10</link>
      <description><![CDATA[我被分配根据我读过的一篇论文来实现一个机器学习模型。
这篇论文实现了一个用于属性分类的多任务学习模型（带标签的图像是模型输入，带标签的意思是属性注释，每幅图像有 40 个）。
它是一个多任务学习模型，因为在模型输入层和 40 个属性分支之后有一个共享的密集层，每个分支都有自己的损失函数（所有分支的二元交叉熵）和自己的 S 型激活函数（在最后一层，用于预测 40 个属性中的每一个是否在图像中）。
经过大量艰苦的努力，它终于开始在所有分支上返回所有 S 型函数的概率，但只有 val_accuracy 的概率是错误的：val_loss 和损失（训练损失）越来越小，acc（训练准确率）也在正常的概率值范围内，除了 val_accuracy 总是相同的值或它的补码。
例如（仅举 5 个时期为例）：
40 个分支之一的一个属性预测的准确率：
5_o_Clock_Shadow_Accuracy
0 0.823665
1 0.891178
2 0.891178
3 0.891178

同一属性的损失：
 5_o_Clock_Shadow_loss
0 0.921046
1 0.701494
2 0.913597
3 0.765397
4 0.894950

val_loss：
val_5_o_Clock_Shadow_loss
0 730232.750000
1 300412.500000
2 376215.843750
3 0.747685
4 1.607191

最后是 val_Accuracy：
val_5_o_Clock_Shadow_Accuracy
0 0.882382
1 0.117618
2 0.882382
3   0.882382 4 0.882382  我的模型： def subnet(shared_layers_output, i): att_branch = Dense(512, name=&#39;dense_&#39;+str(i)+&#39;_1&#39;)(shared_layers_output) att_branch = ReLU()(att_branch) att_branch = BatchNormal ization()(att_branch) att_branch = Dropout(0.5)(att_branch) att_branch = Dense(512, name=&#39;dense_&#39;+str(i)+&#39;_2&#39;)(att_branch) att_branch = ReLU()(att_branch) att_branch = BatchNormalization()(att_branch) att_branch = Dropout(0.5)(att_branch)

branch_output = Dense(1, name=att_list[i],activation=&#39;sigmoid&#39;)(att_branch)

return branch_output

def multi_task_model():

#输入
input_layer = Input(shape=(512,), name=&#39;input_layer&#39;)

#共享网络（1 个网络）
shared_x = Dense(512, name=&#39;shared_dense_layer&#39;)(input_layer)
shared_x = ReLU()(shared_x)
shared_x = BatchNormalization()(shared_x)
shared_x = Dropout(0.5)(shared_x)

branch_outputs = list()
for i in range(40):
branch_outputs.append(subnet(shared_x, i))

model = Model(input_layer, branch_outputs, name=&#39;model&#39;)

返回模型


训练和测试输入形状：(n_samples, 512)
训练和测试标签输入形状：(40, n_samples)
学习率：1e-03

5_o_Clock_Shadow 损失、val_loss、acc 和 val_acc 超过 5 个时期
 损失 val_loss acc val_acc
0 0.422385 1.949578 0.864272 0.8873
1 0.354094 151.987991 0.888797 0.1127
2 0.354356 58.867992 0.888797 0.1127
3 0.352891 94.257980 0.888797 0.1127
4 0.353390 10.997763 0.888797 0.1127
]]></description>
      <guid>https://stackoverflow.com/questions/78885395/val-accuracy-inst-changing-and-sometimes-it-alternates-between-it-complement-10</guid>
      <pubDate>Sun, 18 Aug 2024 18:38:14 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow/Keras CNN 图像分类中，预测准确度低于训练/测试准确度，为什么？</title>
      <link>https://stackoverflow.com/questions/73714379/in-tensorflow-keras-cnn-image-classification-predictions-are-less-accurate-tha</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/73714379/in-tensorflow-keras-cnn-image-classification-predictions-are-less-accurate-tha</guid>
      <pubDate>Wed, 14 Sep 2022 09:15:40 GMT</pubDate>
    </item>
    <item>
      <title>用于检测猫和狗的 Keras 神经网络预测不正确</title>
      <link>https://stackoverflow.com/questions/72221474/keras-neural-network-for-detecting-cats-vs-dogs-dont-predicting-correctly</link>
      <description><![CDATA[我一直在关注 YouTube 教程，让神经网络预测猫与狗的图像。教程结束时并未展示如何使用任何图像进行预测，我一直在努力理清它。
这是网络的代码：
import numpy as np
import cv2
import os
import random
import matplotlib.pyplot as plt
import pickle

DIRECTORY = r&#39;/content/drive/MyDrive/neural&#39;
CATEGORIES = [&#39;cats&#39;, &#39;dogs&#39;]

IMG_SIZE = 100
data = []

for category in CATEGORIES:
folder = os.path.join(DIRECTORY, category)
label = CATEGORIES.index(category)
for img in os.listdir(folder):
img_path = os.path.join(folder, img)
img_arr = cv2.imread(img_path)
img_arr = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE))
data.append([img_arr, label])

random.shuffle(data)

X = []
y=[]
对于数据中的特征、标签：
X.append(features)
y.append(labels)

X = np.array(X)
y = np.array(y)

X = X/255

从 keras.models 导入 Sequential
从 keras.layers 导入 Conv2D、MaxPooling2D、Flatten、Dense

model = Sequential()
model.add(Conv2D(64, (3,3), 激活 = &#39;relu&#39;))
model.add(MaxPooling2D((2,2)))

model.add(Conv2D(64, (3,3), 激活 = &#39;relu&#39;))
model.add(MaxPooling2D((2,2)))

model.add(Flatten())

model.add(Dense(128, input_shape = X.shape[1:],activation = &#39;relu&#39;))

model.add(Dense(2,activation = &#39;softmax&#39;))

model.compile(optimizer = &#39;adam&#39;,loss=&#39;sparse_categorical_crossentropy&#39;,metrics = [&#39;accuracy&#39;])

model.fit(X, y, epochs = 10,validation_split = 0.1)


以下是训练结果：
Epoch 10/10
647/647 [=================================] - 58s 90ms/step - 损失： 0.0262 - 准确率：0.9917 - val_loss：1.4013 - val_accuracy：0.7630
现在我尝试用模型进行预测。
import numpy as np
import cv2
import keras
CATEGORIES = [&#39;Cat&#39;, &#39;Dog&#39;]

def image(path):
img = cv2.imread(path)
new_arr = cv2.resize(img, (100, 100))
new_arr = np.array(new_arr)
new_arr = new_arr.reshape(-1, 100, 100, 3)
new_arr = new_arr/255
return new_arr

prediction = model.predict([image(&#39;/content/drive/MyDrive/neural/test/photo-1609779361684-8196b3a0abf1.jpg&#39;)])
print(CATEGORIES[prediction.argmax()])

我得到的结果完全是随机的。我认为问题在于调整我想要预测的图像的大小，但我尝试了不同的东西，却无法解决。]]></description>
      <guid>https://stackoverflow.com/questions/72221474/keras-neural-network-for-detecting-cats-vs-dogs-dont-predicting-correctly</guid>
      <pubDate>Thu, 12 May 2022 19:59:40 GMT</pubDate>
    </item>
    <item>
      <title>如何提高 CNN 模型的准确性</title>
      <link>https://stackoverflow.com/questions/71083471/how-can-i-increase-my-cnn-models-accuracy</link>
      <description><![CDATA[我建立了一个 CNN 模型，将面部情绪分为快乐、悲伤、精力充沛和中性面部。我使用了 Vgg16 预训练模型并冻结了所有层。经过 50 个训练周期后，我的模型的测试准确率为 0.65，验证损失约为 0.8。
我的训练数据文件夹有 16000（4x4000），验证数据文件夹有 2000（4x500），测试数据文件夹有 4000（4x1000）个 RGB 图像。

你有什么建议可以提高模型准确率？

我尝试用我的模型做一些预测，预测的类别总是相同的。什么会导致问题？


到目前为止我尝试过的方法：

添加 dropout 层 (0.5)
在最后一层之前添加 Dense (256, relu)
对训练和验证数据进行混洗。
将学习率降低到 1e-5

但我无法提高验证和测试准确率。
我的代码
train_src = &quot;/content/drive/MyDrive/Affectnet/train_class/&quot;
val_src = &quot;/content/drive/MyDrive/Affectnet/val_class/&quot;
test_src=&quot;/content/drive/MyDrive/Affectnet/test_classs/&quot;

train_datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(
rescale=1./255, 
sher_range=0.2,
zoom_range=0.2,
Horizo​​ntal_flip=True,

)

train_generator = train_datagen.flow_from_directory(
train_src,
target_size=(224,224 ),
batch_size=32,
class_mode=&#39;categorical&#39;,
shuffle=True
)

validation_datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(
rescale=1./255
)

validation_generator = validation_datagen.flow_from_directory(
val_src,
target_size=(224, 224),
batch_size=32,
class_mode=&#39;categorical&#39;,
shuffle=True
)
conv_base = tensorflow.keras.applications.VGG16(weights=&#39;imagenet&#39;,
include_top=False,
input_shape=(224, 224, 3)
)
for layer in conv_base.layers:
layer.trainable = False

model = tensorflow.keras.models.Sequential()

# VGG16 添加为卷积层。
model.add(conv_base)

# 层从矩阵转换为向量。
model.add(tensorflow.keras.layers.Flatten())

# 我们的神经层已添加。
model.add(tensorflow.keras.layers.Dropout(0.5))
model.add(tensorflow.keras.layers.Dense(256, 激活=&#39;relu&#39;))

model.add(tensorflow.keras.layers.Dense(4, 激活=&#39;softmax&#39;))

model.compile(loss=&#39;categorical_crossentropy&#39;,
optimizer=tensorflow.keras.optimizers.Adam(lr=1e-5),
metrics=[&#39;acc&#39;])
history = model.fit_generator(
train_generator,
epochs=50,
steps_per_epoch=100,
validation_data=validation_generator,
validation_steps=5,
worker=8
)

损失和准确性]]></description>
      <guid>https://stackoverflow.com/questions/71083471/how-can-i-increase-my-cnn-models-accuracy</guid>
      <pubDate>Fri, 11 Feb 2022 16:33:02 GMT</pubDate>
    </item>
    <item>
      <title>K 均值聚类超参数调整</title>
      <link>https://stackoverflow.com/questions/61998081/k-means-clustering-hyperparameter-tuning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/61998081/k-means-clustering-hyperparameter-tuning</guid>
      <pubDate>Mon, 25 May 2020 07:54:46 GMT</pubDate>
    </item>
    </channel>
</rss>