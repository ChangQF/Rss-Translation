<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 29 Jul 2024 21:16:50 GMT</lastBuildDate>
    <item>
      <title>TFLM“Interpreter->Invoke()”问题导致硬故障</title>
      <link>https://stackoverflow.com/questions/78808999/issues-with-tflm-interpreter-invoke-causing-hard-fault</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78808999/issues-with-tflm-interpreter-invoke-causing-hard-fault</guid>
      <pubDate>Mon, 29 Jul 2024 20:48:04 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 数据集中某些类别的训练-验证分割结果为零样本</title>
      <link>https://stackoverflow.com/questions/78808028/train-validation-split-results-in-zero-samples-for-some-classes-in-pytorch-datas</link>
      <description><![CDATA[我正在使用 PyTorch 进行图像分类。我的数据集是目录格式。我已经设置了数据管道和模型。但是，我在训练-验证拆分方面遇到了一个问题，即某些类别在训练或验证数据集中没有样本。这是我的代码和设置的相关部分：
class CustomDataset(Dataset):
def __init__(self, root_dir, transform=None):
self.root_dir = root_dir
self.transform = transform
self.classes = os.listdir(root_dir)
self.image_paths = []
self.labels = []

for label, class_name in enumerate(self.classes):
class_dir = os.path.join(root_dir, class_name)
for img_path in glob.glob(os.path.join(class_dir, &#39;*.png&#39;)) + \
glob.glob(os.path.join(class_dir, &#39;*.jpg&#39;)) + \
glob.glob(os.path.join(class_dir, &#39;*.jpeg&#39;)):
self.image_paths.append(img_path)
self.labels.append(label)

def __len__(self):
return len(self.image_paths)

def __getitem__(self, idx):
img_path = self.image_paths[idx]
image = Image.open(img_path).convert(&#39;RGB&#39;)
label = self.labels[idx]

if self.transform:
image = self.transform(image)
else:
image = transforms.ToTensor()(image) # 如果未提供变换，则将 PIL 图像转换为张量

return image, label

class AugmentedDataset(Dataset):
def __init__(self, base_dataset, transforms_list):
self.base_dataset = base_dataset
self.transforms_list = transforms_list if isinstance(transforms_list, list) else [transforms_list]

def __len__(self):
return len(self.base_dataset) * len(self.transforms_list)

def __getitem__(self, idx):
base_idx = idx // len(self.transforms_list)
transform_idx = idx % len(self.transforms_list)

image, label = self.base_dataset[base_idx]
transform = self.transforms_list[transform_idx]

if transform:
image = transform(image)

return image, label

训练测试分割
base_dataset = CustomDataset(train_dir, v2.Compose(basic_transformations))

train_size = int(0.8 * len(base_dataset))
val_size = len(base_dataset) - train_size

train_base_dataset, val_dataset = random_split(base_dataset, [train_size, val_size])
train_dataset = AugmentedDataset(train_base_dataset, augmentations)
val_dataset = AugmentedDataset(train_base_dataset, v2.Compose(final_transformation))

trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
valloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

执行拆分后，我注意到有些类在训练或验证数据集中没有样本。这会导致模型性能不佳，并且无法正确评估某些类别。

为什么 pytorch random_split 方法会出现这种情况？
有哪些最佳实践可以确保在训练集和验证集中的所有类别之间实现平衡分割？
在分割过程中，我可以使用哪些特定技术或库来保持类别平衡？

这是我的笔记本：EfficientNet with Augmentation]]></description>
      <guid>https://stackoverflow.com/questions/78808028/train-validation-split-results-in-zero-samples-for-some-classes-in-pytorch-datas</guid>
      <pubDate>Mon, 29 Jul 2024 15:51:51 GMT</pubDate>
    </item>
    <item>
      <title>Catboost 特征重要性计算</title>
      <link>https://stackoverflow.com/questions/78807931/catboost-feature-importance-calculation</link>
      <description><![CDATA[我仅用 3 棵树拟合了一个简单二分类模型，并想检查特征重要性结果是否与 Catboost 文档 (PredictionValuesChange) 中的公式相似。
训练模型后，我按照CatBoost JSON 模型教程中的步骤操作，并得到了以下树结构：
{
&quot;leaf_values&quot;: [
-0.13915912880676032,
0.1097787155963716
],
&quot;leaf_weights&quot;: [
2143.0251545906067,
2252.974784851074
],
&quot;splits&quot;: [
{
&quot;border&quot;: 3.5,
&quot;float_feature_index&quot;: 13,
&quot;split_index&quot;: 0,
&quot;split_type&quot;: &quot;FloatFeature&quot;
}
]
} 

模型中的每棵树只有深度 = 1，并且只有一棵树（索引 = 1）具有感兴趣的特征。我决定根据上述公式手动计算特征重要性，并将结果与​​ .get_feature_importance 方法进行比较。结果大不相同：

特征重要性：28.2947825
手动计算：68.06248029261762

以下是用于特征重要性计算的代码：
tree_indx = 1
v_1 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_values&#39;][0]
v_2 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_values&#39;][1]

c_1 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_weights&#39;][0]
c_2 = model[&#39;oblivious_trees&#39;][tree_indx][&#39;leaf_weights&#39;][1]

avr = (v_1*c_1 + v_2*c_2)/(c_1+c_2)

fi = ((v_1 - avr)**2)*c_1 + ((v_2 - avr)**2)*c_2
print(fi)

我犯了错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78807931/catboost-feature-importance-calculation</guid>
      <pubDate>Mon, 29 Jul 2024 15:29:02 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python 和计算机视觉检测图像中的人是否赤裸上身？[关闭]</title>
      <link>https://stackoverflow.com/questions/78806780/how-to-detect-if-a-person-is-shirtless-in-an-image-using-python-and-computer-vis</link>
      <description><![CDATA[我有一个数据集，其中包含一个人的多张自拍照，每张都是从胸部以上拍摄的，格式为自拍。我的目标是识别每张照片中的人是否赤裸上身。这些照片并不露骨，而是随意的自拍照，照片中的人可能穿着也可能没穿着衬衫。
由于我没有太多数据，我认为从头开始训练神经网络模型不是最好的选择——我的数据集不是那么大。
有什么指导或建议吗？
我曾尝试使用 Python 中的 NSFW-Detector 库（https://pypi.org/project/nsfw-detector/），但它没有太大帮助，因为它专注于检测露骨内容，而不是识别一个人是否赤裸上身。]]></description>
      <guid>https://stackoverflow.com/questions/78806780/how-to-detect-if-a-person-is-shirtless-in-an-image-using-python-and-computer-vis</guid>
      <pubDate>Mon, 29 Jul 2024 11:19:21 GMT</pubDate>
    </item>
    <item>
      <title>在 javafx 应用程序中集成 pickle 模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78805706/integrating-pickle-model-in-javafx-application</link>
      <description><![CDATA[我试图将 python pickle 模型集成到 javafx 中，我尝试使用 py4j 进行连接，但没有成功，一直出现连接错误
我尝试使用 py4j 将 javafx 代码与读取模型的 python 代码连接起来，但没有成功，还尝试使用 onnx 运行时，但由于某种原因，使用 onnx 运行时生成的模型已损坏。我想要的是让模型预测结果并将结果发送到 javafx 进行显示]]></description>
      <guid>https://stackoverflow.com/questions/78805706/integrating-pickle-model-in-javafx-application</guid>
      <pubDate>Mon, 29 Jul 2024 06:43:34 GMT</pubDate>
    </item>
    <item>
      <title>Python\Python312\Lib\站点包\torch\lib\fbgemm.dll</title>
      <link>https://stackoverflow.com/questions/78805219/python-python312-lib-site-packages-torch-lib-fbgemm-dll</link>
      <description><![CDATA[在此处输入图片描述
我正尝试从 Hugging Face 导入 GPT-2 Transformer 模型，但当我尝试导入它时，我遇到了错误。即使我尝试只导入 Torch，我也会收到同样的错误。
我尝试重新安装 Torch 并做了所有事情，包括更新 Visual C++ Redistributable 软件包和更新我的驱动程序，但问题仍然存在。]]></description>
      <guid>https://stackoverflow.com/questions/78805219/python-python312-lib-site-packages-torch-lib-fbgemm-dll</guid>
      <pubDate>Mon, 29 Jul 2024 02:23:30 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降应用</title>
      <link>https://stackoverflow.com/questions/78804107/gradient-descent-application</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78804107/gradient-descent-application</guid>
      <pubDate>Sun, 28 Jul 2024 15:15:45 GMT</pubDate>
    </item>
    <item>
      <title>龙格-库塔求解器无法取得进展（RuntimeWarning：迭代没有取得良好进展）</title>
      <link>https://stackoverflow.com/questions/78803278/runge-kutta-solver-cannot-make-progress-runtimewarning-iteration-is-not-making</link>
      <description><![CDATA[我收到此警告：
RuntimeWarning：迭代没有取得良好进展，以过去十次迭代的改进来衡量。 

我正在尝试从工业制冷系统的数字孪生构建自定义 RL 环境。我正在模拟特定时间步骤内的过程，并在一段时间内（即 24 小时）循环执行该步骤。
我尝试更改 RK 求解器的 time_step_methods 数量和 configure 方法中的 time_step_minutes。可能是我的动作空间/观察空间或奖励函数定义不明确？
我的代码片段：
class RefrigerationEnv(gym.Env):
def __init__(self, num_evaporators=7, time_step_seconds=10, time_step_minutes=15):
super(RefrigerationEnv, self).__init__()

self.num_evaporators = num_evaporators
self.time_step_seconds = time_step_seconds
self.time_step_minutes = time_step_minutes

tz = dateutil.tz.gettz(&quot;America/Los_Angeles&quot;)
self.start_datetime = datetime(2024, 5, 12, 10, 0, 0, tzinfo=tz)

self.end_datetime_episode = self.start_datetime + timedelta(hours=12)
self.simulation_time = self.start_datetime # 初始化模拟时间

# 动作空间
self.action_space = space.Box(
low=np.array([100, -1, 20] + [-15] * num_evaporators), # 每个设定点的下限
high=np.array([180, 12, 80] + [10] * num_evaporators), # 每个设定点的上限
dtype=np.float32
)

# 观察空间
self.observation_space = space.Dict(
{
&quot;refrigerator_temp__c&quot;: space.Box(low=-30, high=10, shape=(1,), dtype=np.float64),
&quot;energy_consumption&quot;: space.Box(low=0.0, high=400.0, shape=(1,), dtype=np.float64),
&quot;ambient_temp&quot;: space.Box(low=-10, high=50, shape=(1,), dtype=np.float64),
&quot;time_of_day&quot;: space.Box(low=0, high=24, shape=(1,), dtype=np.float64),
}
)

# 初始化模拟器
self.simulator = configure_example_simulation_from_path(
process_config_path=Path(&quot;simulation_payload/simple_flowsheet_config.json&quot;),
control_config_path=Path(&quot;simulation_payload/simple_control_config.yaml&quot;),
disorders=self.create_disturbances(),
settings=self.create_settings(),
start_datetime=self.start_datetime,
end_datetime=self.start_datetime + timedelta(minutes=self.time_step_minutes), # 模拟 1 小时
solver=RK12(max_step__s=self.time_step_seconds,relative_tolerance=0.00001),
)
]]></description>
      <guid>https://stackoverflow.com/questions/78803278/runge-kutta-solver-cannot-make-progress-runtimewarning-iteration-is-not-making</guid>
      <pubDate>Sun, 28 Jul 2024 08:21:22 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 文档解释[关闭]</title>
      <link>https://stackoverflow.com/questions/78803273/xgboost-docs-explanation</link>
      <description><![CDATA[我正在 https://xgboost.readthedocs.io/en/stable/tutorials/model.html 上阅读有关 XGBoost 的文章，我无法弄清楚下面屏幕截图中的方程式如何进入下一步
在此处输入图片描述
我试图扩展方程式来思考为什么事情会这样发展，但我仍然不明白。]]></description>
      <guid>https://stackoverflow.com/questions/78803273/xgboost-docs-explanation</guid>
      <pubDate>Sun, 28 Jul 2024 08:17:04 GMT</pubDate>
    </item>
    <item>
      <title>MoviePy 文件数量限制</title>
      <link>https://stackoverflow.com/questions/78802929/moviepy-limit-on-number-of-files</link>
      <description><![CDATA[我正在尝试将大约 300 个 mp4 视频（来自 Ekman-6 数据集）转换为 mp3 音频文件。我目前正在使用 MoviePy 的 VideoFileClip（在 Google Colab 中）进行此转换：
id = 0

for i in range(6):
path = directory + folders[i]
cnt = 0
for file in os.listdir(path):
filename = os.fsdecode(file)
clip = VideoFileClip(path + filename)
audio = clip.audio
audio.write_audiofile(to_directory + folders[i] + str(id) + &quot;.mp3&quot;)
audio.close()
clip.close()
id += 1
cnt += 1
if cnt == 50:
break

但是，在转换第 47 个文件时，我收到以下错误：
-------------------------------------------------------------------------------
AttributeError Traceback（最近一次调用last)
&lt;ipython-input-4-c6ad7f378072&gt; in &lt;cell line: 7&gt;()
12 clip = VideoFileClip(directory + folders[i] + file)
13 audio = clip.audio
---&gt; 14 audio.write_audiofile(to_directory + folders[i] + str(id) + &quot;.mp3&quot;)
15 audio.close()
16 clip.close()

AttributeError: &#39;NoneType&#39; 对象没有属性 &#39;write_audiofile&#39;

我不确定是什么导致了这个错误。所以，我在网上搜索，发现另一个 stackoverflow 帖子说这个问题是由于 mp4 文件没有音频而引起的。所以，我检查了导致错误的文件，但它确实有音频。为了 100% 确保 MoviePy 正确地将该文件转换为音频，我还单独对该文件进行了转换。这次，MoviePy 成功地将文件转换为音频。
为什么 MoviePy 单独处理该文件时可以正常工作，而在转换之前的 46 个文件后却不行？使用 MoviePy 转换的文件数量/视频长度是否有限制？我在网上找不到有关此问题的任何信息。]]></description>
      <guid>https://stackoverflow.com/questions/78802929/moviepy-limit-on-number-of-files</guid>
      <pubDate>Sun, 28 Jul 2024 04:07:19 GMT</pubDate>
    </item>
    <item>
      <title>在 YOLO 推理中，GPU 性能不如 CPU 性能</title>
      <link>https://stackoverflow.com/questions/78802177/gpu-performance-worse-than-cpu-performance-on-yolo-inferences</link>
      <description><![CDATA[我正在使用 YoloDotNet NuGet 包来测试 YOLO 模型的性能。我正在为我的学位论文做这个测试。但是，我遇到了一个问题，GPU 性能明显比 CPU 性能差。

问题是前 50/60 次推理的性能非常好（比如 20 毫秒），然后它们开始变差，直到时间稳定在每张图像 70/75 毫秒左右。我不明白为什么性能会以这种方式变差。

环境：

YoloDotNet 版本：v2.0
CPU：AMD ryzen 7 7800X3D
GPU：4070 super
CUDA/cuDNN 版本：cuda 11.8 和 cudnn 8.9.7
.NET 版本：8

重现步骤：
var sw = new Stopwatch();
for (var i = 0; i &lt; 500; i++)
{
var file = $@&quot;C:\Users\Utente\Documents\assets\images\input\frame_{i}.jpg&quot;;

使用 var image = SKImage.FromEncodedData(file);
sw.Restart();
var results = yolo.RunObjectDetection(image, confidence: 0.25, iou: 0.7);
sw.Stop();
image.Draw(results);

image.Save(file.Replace(&quot;input&quot;, $&quot;output_{yolo_version}{version}_{target}&quot;).Replace(&quot;.jpg&quot;, $&quot;_detect_{yolo_version}{version}_{target}.jpg&quot;),
SKEncodedImageFormat.Jpeg);
times.Add(sw.Elapsed.TotalMilliseconds);
Console.WriteLine($&quot;图像 {i} 所用时间：{sw.Elapsed.TotalMilliseconds:F2} 毫秒&quot;);

这是我对检测进行时间测量的方式。
要加载模型，我在 GPU 情况下使用此设置
yolo = new Yolo(new YoloOptions
{
OnnxModel = @$&quot;C:\Users\Utente\Documents\assets\model\yolov{yolo_version}{version}_{target}.onnx&quot;,
ModelType = ModelType.ObjectDetection, // 模型类型
Cuda = true, // 使用 CPU 或 CUDA 进行 GPU 加速推理。默认值 = true
GpuId = 0, // 根据 id 选择 Gpu。默认值 = 0
PrimeGpu = true, // 先预分配 GPU。默认值 = false
});
Console.WriteLine(yolo.OnnxModel.ModelType);
Console.WriteLine($&quot;使用 GPU 版本 {yolo_version}{version}&quot;);

使用 yolov8 的性能指标：
CPU 推理时间：
版本 m 的总时间：25693 毫秒

版本 m 每幅图像的平均时间：51.25 毫秒

GPU 推理时间：
版本 m 的总时间：34459.73 毫秒

版本 m 每幅图像的平均时间：69.74 毫秒

我想发布有关时间的图表，但我没有足够的声誉
该问题针对不同大小的模型自行呈现。我仅打印了 m 大小以方便可视化。
预期行为是使用 GPU 的推理应该比使用 CPU 的推理更快。
但使用 GPU 后性能并没有提高。]]></description>
      <guid>https://stackoverflow.com/questions/78802177/gpu-performance-worse-than-cpu-performance-on-yolo-inferences</guid>
      <pubDate>Sat, 27 Jul 2024 18:33:48 GMT</pubDate>
    </item>
    <item>
      <title>在搜索系统结果上计算 NDCG 时，处理假阳性和假阴性有哪些不同的方法？</title>
      <link>https://stackoverflow.com/questions/78798774/what-are-the-different-ways-to-handle-false-positives-and-false-negatives-when-c</link>
      <description><![CDATA[上下文：
我正在使用 NDCG（归一化折扣累积增益）来评估包含相关性分数的地面实况数据集上的语义搜索系统。我想为此使用 sklearn 的 ndcg_score()。
问题：有哪些处理方法：

假阳性文档：对于给定的查询，那些出现在搜索系统的响应中但不出现在地面实况数据中的文档
假阴性文档：对于给定的查询，那些出现在地面实况数据中但不出现在搜索系统的响应中的文档

一种可能性是插入预测分数 = 0 来表示假阴性并忽略假阳性。但我并不完全确定这是否是正确的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78798774/what-are-the-different-ways-to-handle-false-positives-and-false-negatives-when-c</guid>
      <pubDate>Fri, 26 Jul 2024 15:02:50 GMT</pubDate>
    </item>
    <item>
      <title>只有输入张量可以作为位置参数传递</title>
      <link>https://stackoverflow.com/questions/78360982/only-input-tensors-may-be-passed-as-positional-arguments</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78360982/only-input-tensors-may-be-passed-as-positional-arguments</guid>
      <pubDate>Sun, 21 Apr 2024 09:15:40 GMT</pubDate>
    </item>
    <item>
      <title>Intel MacBook 上的 VNGeneratePersonSegmentationRequest 速度缓慢</title>
      <link>https://stackoverflow.com/questions/72810091/vngeneratepersonsegmentationrequest-slow-on-intel-macbooks</link>
      <description><![CDATA[我有一个非常简单的功能，它尝试使用 VNGeneratePersonSegmentationRequest 和 VNImageRequestHandler 从网络摄像头视频流中删除背景。
在配备 M1 处理器的 Mac 电脑上，效果很好（60-120 fps 没有任何问题）。在 MacBook Pro Intel i7 2.7GHz 和 Intel Iris Plus Graphics 655 上使用相同的代码/应用程序，性能只有 10 fps，低得多。
这是使用的代码：
let personSegmentationRequest = VNGeneratePersonSegmentationRequest()
personSegmentationRequest.qualityLevel = .balanced
let imageRequestHandler = VNImageRequestHandler(ciImage: ciImage) // 来自摄像头的一帧

尝试？ imageRequestHandler.perform([personSegmentationRequest])
guard let result = personSegmentationRequest.results?.first else {
return nil
}

有什么想法吗？谢谢]]></description>
      <guid>https://stackoverflow.com/questions/72810091/vngeneratepersonsegmentationrequest-slow-on-intel-macbooks</guid>
      <pubDate>Thu, 30 Jun 2022 03:44:54 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Keras 对 CNN 模型中的多个输入数据进行交叉验证</title>
      <link>https://stackoverflow.com/questions/59277549/how-to-do-cross-validation-with-multiple-input-data-in-cnn-model-with-keras</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/59277549/how-to-do-cross-validation-with-multiple-input-data-in-cnn-model-with-keras</guid>
      <pubDate>Wed, 11 Dec 2019 01:07:35 GMT</pubDate>
    </item>
    </channel>
</rss>