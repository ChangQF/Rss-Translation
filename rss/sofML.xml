<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 18 Oct 2024 09:18:43 GMT</lastBuildDate>
    <item>
      <title>如何从图表中提取数据用于机器学习？</title>
      <link>https://stackoverflow.com/questions/79100419/how-do-i-extract-data-from-a-graph-for-machine-learning</link>
      <description><![CDATA[问题就是标题：如何从图表中提取数据用于机器学习？作为背景，我想根据光变曲线图对变星的类型进行分类。但唯一的方法是将每个点设为单独的特征。例如，t1 将具有该时间戳的光度值。如何从图表中提取它，还有其他更简单的方法吗？
我曾尝试在线查找此表格数据，但无济于事。]]></description>
      <guid>https://stackoverflow.com/questions/79100419/how-do-i-extract-data-from-a-graph-for-machine-learning</guid>
      <pubDate>Fri, 18 Oct 2024 03:09:57 GMT</pubDate>
    </item>
    <item>
      <title>如何提高大型不平衡数据集的随机森林模型性能？</title>
      <link>https://stackoverflow.com/questions/79100257/how-to-improve-random-forest-model-performance-for-large-imbalanced-datasets</link>
      <description><![CDATA[我一直在研究随机森林模型来预测员工流失。我的数据集非常不平衡，大约有 80% 的非流失案例和 20% 的流失案例。虽然我尝试使用 SMOTE 来平衡类别，但我的模型的准确率已经提高，但精确度和召回率仍未达到我想要的水平。
我目前所做的工作：

使用 SMOTE 对少数类进行过采样
使用 GridSearchCV 调整超参数，如 n_estimators、max_depth 和 min_samples_split
使用 class_weight=‘balanced’ 进行测试
使用混淆矩阵、ROC 曲线和精确度-召回率曲线进行评估

该模型目前的准确率达到 86%，但流失类的精确度和召回率仍然很低。我想在不牺牲整体性能的情况下提高模型正确预测减员情况的能力。]]></description>
      <guid>https://stackoverflow.com/questions/79100257/how-to-improve-random-forest-model-performance-for-large-imbalanced-datasets</guid>
      <pubDate>Fri, 18 Oct 2024 01:19:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在创建模型时考虑不同的列</title>
      <link>https://stackoverflow.com/questions/79100145/how-to-consider-varying-columns-while-creating-a-model</link>
      <description><![CDATA[我有一个发送警报的监控服务。我正在创建一个模型，如果警报在过去 1 个月内发生超过 3 次，该模型将标记警报。
我可以使用 IsolationForest 实现这一点，并指定模型中每个警报要考虑的字段。
但是，我面临的问题是警报的字段可能会有所不同。
考虑以下 2 个警报
AlertName Date FQDN DBName
磁盘使用率 90% 10/17/2024 00:00:000 test.com 
DB 已重新启动。10/17/2024 01:00:000 db1

在上面的例子中，如果它是磁盘使用率 90% 警报，那么我应该使用 FQDN 字段
如果是 DB 已重新启动，我应该使用 DBName 字段。
对于每个警报，用于确定其是否为重复警报的字段会有所不同，而我无法控制这些字段。
是否可以开发一个模型，该模型会动态考虑不同警报的不同列，而我无需指定要为每种警报类型考虑哪一列？]]></description>
      <guid>https://stackoverflow.com/questions/79100145/how-to-consider-varying-columns-while-creating-a-model</guid>
      <pubDate>Thu, 17 Oct 2024 23:57:47 GMT</pubDate>
    </item>
    <item>
      <title>重塑自定义策略网络中的错误</title>
      <link>https://stackoverflow.com/questions/79100066/reshaping-error-in-my-custom-policy-network</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79100066/reshaping-error-in-my-custom-policy-network</guid>
      <pubDate>Thu, 17 Oct 2024 22:55:39 GMT</pubDate>
    </item>
    <item>
      <title>使用 FastAPI 和 GPT 的 RAG 系统</title>
      <link>https://stackoverflow.com/questions/79099986/rag-system-using-fastapi-and-gpt</link>
      <description><![CDATA[我有一项任务，使用 RAG 和 FastAPI 在特定数据集上建立 Web 应用程序。
信息位于链接 https://github.com/drrahulsuresh/bounce/tree/dev
但是我的查询显示未获得相关数据。
有人能帮我解决这个问题吗？
我尝试了 sql，甚至将 excel 转换为 json 或 csv。我认为问题出在复杂的嵌套数据集上。我不知道如何通过 rag 将数据处理成可读格式。]]></description>
      <guid>https://stackoverflow.com/questions/79099986/rag-system-using-fastapi-and-gpt</guid>
      <pubDate>Thu, 17 Oct 2024 22:12:24 GMT</pubDate>
    </item>
    <item>
      <title>是否有用于机器学习训练代码的自动回归测试库？[关闭]</title>
      <link>https://stackoverflow.com/questions/79099561/is-there-a-library-for-automated-regression-testing-for-machine-learning-trainin</link>
      <description><![CDATA[我想以自动化的方式测试我的深度学习训练代码，以确保重构不会改变我训练的更新步骤。我想我还需要某种度量来覆盖我的计算图。
是否有一个库可以为 pytorch 或 jax 执行此操作？我不想测试经过训练的模型（有库可以做到这一点），而是测试训练代码。]]></description>
      <guid>https://stackoverflow.com/questions/79099561/is-there-a-library-for-automated-regression-testing-for-machine-learning-trainin</guid>
      <pubDate>Thu, 17 Oct 2024 19:16:16 GMT</pubDate>
    </item>
    <item>
      <title>像素映射到网络输入</title>
      <link>https://stackoverflow.com/questions/79096878/pixel-mapping-to-network-inputs</link>
      <description><![CDATA[我正在嵌入式设备中使用对象检测模型，需要运行测试来比较嵌入式平台和 PC 上的性能。为了测试的完整性，我需要确保在两种情况下像素的映射方式相同。我了解 Pytorch 如何将图像转换为形状为 CxHxW 的张量，但我要问的是这些像素究竟是如何映射到输入的。那么，例如，图像左上角的像素（所有 3 个通道中的值）是分配给前向传递中的第一个输入，还是最后一个输入？]]></description>
      <guid>https://stackoverflow.com/questions/79096878/pixel-mapping-to-network-inputs</guid>
      <pubDate>Thu, 17 Oct 2024 06:59:11 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程二元分类：为什么 GPy 的方差比 scikit-learn 小得多？</title>
      <link>https://stackoverflow.com/questions/79086293/gaussian-process-binary-classification-why-is-the-variance-with-gpy-much-smalle</link>
      <description><![CDATA[我正在学习使用高斯过程进行二元分类，并且正在将 GPy 与 scikit-learn 进行比较，该问题涉及一个玩具一维问题，灵感来自 Martin Krasser 的博客文章。两种实现（GPy 和 scikit-learn）似乎都使用带有 RBF 内核的类似设置。优化内核超参数后，长度尺度相似，但方差相差很大。GPy 内核方差似乎太小了。
我如何修改我的 GPy 实现并获得与 scikit-learn 类似的结果？我怀疑这与每个算法的内部实现有关，但我不知道是什么导致了这种巨大的差异。下面我将进一步解释为什么我认为我的 GPy 实现需要修复。
实现细节：Python 3.9 搭配 GPy 1.13.2 和 scikit-learn 1.5.1。可重现的示例：
import numpy as np
from scipy.stats import bernoulli
from scipy.special import expit as sigmoid

##############################
# 第 1 部分：玩具数据集创建
#################################

np.random.seed(0)
X = np.arange(0, 5, 0.05).reshape(-1, 1)
X_test = np.arange(-2, 7, 0.1).reshape(-1, 1)

a = np.sin(X * np.pi * 0.5) * 2 # 潜在函数
t = bernoulli.rvs(sigmoid(a)) # Bernoulli 训练数据（0 和1s)

#####################################
# 第 2 部分：scikit-learn 实现
#####################################

从 sklearn.gaussian_process 导入 GaussianProcessClassifier
从 sklearn.gaussian_process.kernels 导入 ConstantKernel，RBF

rbf = ConstantKernel(1.0, constant_value_bounds=(1e-3, 10)) \
* RBF(length_scale=1.0, length_scale_bounds=(1e-3, 10))
gpc = GaussianProcessClassifier(
kernel=rbf,
optimizer=&#39;fmin_l_bfgs_b&#39;,
n_restarts_optimizer=10)

gpc.fit(X_scaled, t.ravel())

print(gpc.kernel_)
# 1.5**2 * RBF(length_scale=0.858)

############################
# 第 3 部分：GPy 实现
############################

导入 GPy

kern = GPy.kern.RBF(
input_dim=1,
variance=1.,
lengthscale=1.)
kern.lengthscale.unconstrain()
kern.variance.unconstrain()
kern.lengthscale.constrain_bounded(1e-3, 10)
kern.variance.constrain_bounded(1e-3, 10)

m = GPy.core.GP(
X=X,Y=t, kernel=kern, 
inference_method=GPy.inference.latent_function_inference.laplace.Laplace(), 
可能性=GPy.likelihoods.Bernoulli())

m.optimize_restarts(
num_restarts=10, optimizer=&#39;lbfgs&#39;,
verbose=True, robust=True)

print(m.kern)
# rbf。| 值 | 约束 | 先验
# 方差 | 0.8067562453940487 | 0.001,10.0 | 
# lengthscale | 0.8365668826459536 | 0.001,10.0 |

lenghtscale 值大致相似 (0.858 vs 0.836)，但方差值非常不同 (scikit-learn 为 1.5**2 = 2.25，GPy 仅为 0.806)。
我认为我的 GPy 实现需要调整的原因是，即使有 +/- 2 个标准偏差界限，真正的潜在函数 (参见上面代码第 1 部分中的“a”) 也与预测函数不紧密匹配。另一方面，scikit-learn 实现与之相当匹配（可以使用 scikit-learn 检索潜在函数平均值和标准差如此处所示）。

 左：两个模型的预测概率相似（这是有道理的，因为它们共享相似的长度尺度值）。右：GPy 的预测潜在函数与真实潜在函数的拟合度不如 scikit-learn 模型。 
到目前为止，我尝试过的方法，结果没有显著变化：

输入特征 (X) 归一化
使用 GPy.inference.latent_function_inference.expectation_propagation.EP() 作为 GPy 推理方法，而不是拉普拉斯方法
按照此处的建议，将 WhiteKernel 组件添加到 scikit-learn 实现中&gt;
]]></description>
      <guid>https://stackoverflow.com/questions/79086293/gaussian-process-binary-classification-why-is-the-variance-with-gpy-much-smalle</guid>
      <pubDate>Mon, 14 Oct 2024 13:10:16 GMT</pubDate>
    </item>
    <item>
      <title>Gymnasium 自定义环境“太多值无法解压”错误</title>
      <link>https://stackoverflow.com/questions/79084313/gymnasium-custom-environment-too-many-values-to-unpack-error</link>
      <description><![CDATA[我正在尝试使用具有体育馆和稳定基线的自定义群体聚集环境。我有一个自定义策略和训练循环。
我的行动和观察空间如下：
min_action = np.array([-5, -5] * len(self.agents), dtype=np.float32)
max_action = np.array([5, 5] * len(self.agents), dtype=np.float32)

min_obs = np.array([-np.inf, -np.inf, -2.5, -2.5] * len(self.agents), dtype=np.float32)
max_obs = np.array([np.inf, np.inf, 2.5, 2.5] * len(self.agents), dtype=np.float32)

训练代码：
import numpy as np
import torch as th
from Parameters import *
from stable_baselines3 import PPO
from main import FlockingEnv, CustomMultiAgentPolicy
from Callbacks import TQDMProgressCallback, LossCallback
import os
from stable_baselines3.common.vec_env import DummyVecEnv

if os.path.exists(Results[&quot;Rewards&quot;]):
os.remove(Results[&quot;Rewards&quot;])
print(f&quot;File {Results[&#39;Rewards&#39;]} has been removed.&quot;)

if os.path.exists(&quot;training_rewards.json&quot;):
os.remove(&quot;training_rewards.json&quot;)
print(f&quot;文件 training_rewards 已被删除。&quot;) 

def seed_everything(seed):
np.random.seed(seed)
os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
th.manual_seed(seed)
th.cuda.manual_seed(seed)
th.backends.cudnn.deterministic = True
env.seed(seed)
env.action_space.seed(seed)

loss_callback = LossCallback()
env = DummyVecEnv([lambda: FlockingEnv()])

seed_everything(SimulationVariables[&quot;Seed&quot;])

# # 模型训练
model = PPO(CustomMultiAgentPolicy, env, tensorboard_log=&quot;./ppo_Agents_tensorboard/&quot;, verbose=1)
model.set_random_seed(SimulationVariables[&quot;ModelSeed&quot;])
progress_callback = TQDMProgressCallback(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;])
# 训练模型
model.learn(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;], callback=[progress_callback, loss_callback])

错误：
使用 cuda 设备
回溯（最近一次调用最后一次）：
文件 &quot;D:\Thesis_\FlockingFinal\MultiAgentFlocking\Training.py&quot;，行45，在&lt;module&gt;中
model.learn(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;], callback=[progress_callback, loss_callback]) 
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\ppo\ppo.py&quot;，第 315 行，在 learn 中
return super().learn(
^^^^^^^^^^^^^^^
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py&quot;，第 287 行，在 learn 中
total_timesteps, callback = self._setup_learn(
^^^^^^^^^^^^^^^^^^^
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\common\base_class.py&quot;, 第 423 行, 在 _setup_learn
self._last_obs = self.env.reset() # 类型: ignore[assignment]
^^^^^^^^^^^^^^^^^
文件 &quot;C:\Python312\Lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py&quot;, 第 77 行, 在 reset
obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: 太需要解包的值很多（预计为 2 个）

我也在 gym 中使用了类似的种子函数，但没有出现错误，我以为是它导致了错误，但即使我不使用它，错误也不会消失。]]></description>
      <guid>https://stackoverflow.com/questions/79084313/gymnasium-custom-environment-too-many-values-to-unpack-error</guid>
      <pubDate>Sun, 13 Oct 2024 22:45:48 GMT</pubDate>
    </item>
    <item>
      <title>GNU Octave 是多线程的吗？</title>
      <link>https://stackoverflow.com/questions/79050512/is-gnu-octave-multi-threaded</link>
      <description><![CDATA[根据这个老问题的答案，GNU Octave 似乎是一个单线程应用程序。
但是，我正在试验一个名为nnet的旧 Octave 神经网络包，并惊讶地发现我的 Octave 程序使用了笔记本电脑的所有 4 个核心。自从我链接的问题提出以来，情况有变化吗？GNU Octave 现在是多线程的吗？据我所知，我没有看到 nnet 内部有任何并行实现。
有关我的安装的一些信息：

我的操作系统是 Linux Mint 20
我的机器有 4 个处理单元（这是 nproc 在我的终端中显示的内容）
我的 Octave 版本是 5.2.0（如果这有区别的话，我正在使用 GUI）

我的代码相当简单，只导入了 nnet 包，没有其他内容。当我查看运行程序时的资源时，我看到所有核心都已使用（下面是 htop 屏幕截图）

这是我正在做的事情：
pkg load nnet

starttime = clock();

# 取自 http://matlab.izmiran.ru/help/toolbox/nnet/newff.html
Pr = -1:0.00005:1;
Tr = 3*sin(pi*Pr)-cos(pi*Pr);
Prmin = min(Pr);
Prmax = max(Pr);
net = newff([Prmin Prmax],[3 2 1],{&#39;tansig&#39;,&#39;logsig&#39;,&#39;purelin&#39;},&#39;trainlm&#39;);
[net] = train(net,Pr,Tr,[],[],[]);
[netoutput] = sim(net,Pr);

etime(clock(),starttime)

% 测试结果 
plot(Pr,Tr,&#39;b+&#39;);
hold on; 
plot(Pr,netoutput,&#39;r-&#39;);
hold off;

编辑
根据评论中的 @JérômeRichard 提示和 @NickJ 建议，我通过在终端中执行 export OMP_NUM_THREADS=1 来启动 Octave，只为 BLAS 分配 1 个线程。该脚本的速度是原始设置的两倍（根据上面发布的 htop 屏幕截图，默认设置是 4）。我确保我的程序只使用一个核心和 htop。]]></description>
      <guid>https://stackoverflow.com/questions/79050512/is-gnu-octave-multi-threaded</guid>
      <pubDate>Thu, 03 Oct 2024 12:19:13 GMT</pubDate>
    </item>
    <item>
      <title>LLM Studio 无法下载模型并出现错误：无法获取本地颁发者证书</title>
      <link>https://stackoverflow.com/questions/78379820/llm-studio-fail-to-download-model-with-error-unable-to-get-local-issuer-certif</link>
      <description><![CDATA[在 LLM 工作室中，当我尝试下载任何模型时，我遇到以下错误：
下载失败：无法获取本地颁发者证书
]]></description>
      <guid>https://stackoverflow.com/questions/78379820/llm-studio-fail-to-download-model-with-error-unable-to-get-local-issuer-certif</guid>
      <pubDate>Wed, 24 Apr 2024 16:03:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中有效地实现非全连接线性层？</title>
      <link>https://stackoverflow.com/questions/70269663/how-to-efficiently-implement-a-non-fully-connected-linear-layer-in-pytorch</link>
      <description><![CDATA[我制作了一个示例图，展示了我试图实现的缩小版本：

因此，顶部两个输入节点仅完全连接到顶部三个输出节点，并且相同的设计适用于底部两个节点。到目前为止，我已经想出了两种在 PyTorch 中实现此目的的方法，但都不是最佳方法。
第一种方法是创建一个包含许多较小线性层的 nn.ModuleList，并在前向传递期间，通过它们迭代输入。对于图表的示例，它看起来像这样：
class Module(nn.Module):
def __init__(self):
self.layers = nn.Module([nn.Linear(2, 3) for i in range(2)])

def forward(self, input):
output = torch.zeros(2, 3)
for i in range(2):
output[i, :] = self.layers[i](input.view(2, 2)[i, :])
return output.flatten()

因此，这完成了图中的网络，主要问题是它非常慢。我认为这是因为 PyTorch 必须按顺序处理 for 循环，而不能并行处理输入张量。
要“矢量化”模块以便 PyTorch 可以更快地运行它，我有这个实现：
class Module(nn.Module):
def __init__(self):
self.layer = nn.Linear(4, 6)
self.mask = # 创建 1 和 0 的掩码来“阻止”某些层连接

def forward(self, input):
prune.custom_from_mask(self.layer, name=&#39;weight&#39;, mask=self.mask)
return self.layer(input)

这也完成了图表的网络，通过使用权重修剪来确保完全连接层中的某些权重始终为零（例如，连接顶部输入节点和底部输出节点的权重将始终为零，因此它实际上是“断开连接的”）。这个模块比上一个模块快得多，因为没有 for 循环。现在的问题是这个模块占用了更多的内存。这可能是因为，即使大多数层的权重为零，PyTorch 仍会将网络视为它们存在。此实现本质上保留了比需要更多的权重。
有人遇到过这个问题并想出了有效的解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/70269663/how-to-efficiently-implement-a-non-fully-connected-linear-layer-in-pytorch</guid>
      <pubDate>Wed, 08 Dec 2021 03:41:06 GMT</pubDate>
    </item>
    <item>
      <title>在 scikit learn 中实现自定义损失函数</title>
      <link>https://stackoverflow.com/questions/54267745/implementing-custom-loss-function-in-scikit-learn</link>
      <description><![CDATA[我想在 scikit learn 中实现自定义损失函数。我使用以下代码片段：
def my_custom_loss_func(y_true,y_pred):
diff3=max((abs(y_true-y_pred))*y_true)
return diff3

score=make_scorer(my_custom_loss_func,greater_ is_better=False)
clf=RandomForestRegressor()
mnn= GridSearchCV(clf,score)
knn = mnn.fit(feam,labm) 

传递给 my_custom_loss_func 的参数应该是什么？我的标签矩阵称为 labm。我想计算实际输出与预测输出（由模型）乘以真实输出之间的差值。如果我使用 labm 代替 y_true，那么我应该使用什么代替 y_pred？]]></description>
      <guid>https://stackoverflow.com/questions/54267745/implementing-custom-loss-function-in-scikit-learn</guid>
      <pubDate>Sat, 19 Jan 2019 13:47:47 GMT</pubDate>
    </item>
    <item>
      <title>Python 中图像增强的图像增强管道中的错误</title>
      <link>https://stackoverflow.com/questions/50887274/error-in-image-augmentation-pipeline-for-image-augmentation-in-python</link>
      <description><![CDATA[据我所知，路径是正确的，我也遵循了 Augmentor 文档。
代码：
import Augmentor
import os
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
import keras
import glob

for img in glob.glob(&quot;C:\\Users\\Diganta\\Desktop\\Courses and Projects\\Projects\\Bennet\\irregular*.jpg&quot;):
p = Augmentor.Pipeline(img)
p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)
p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)
p.sample(100)

这确实运行了，但没有创建包含增强图像的输出文件夹按照 Augmentor 文档中指定的目录进行操作]]></description>
      <guid>https://stackoverflow.com/questions/50887274/error-in-image-augmentation-pipeline-for-image-augmentation-in-python</guid>
      <pubDate>Sat, 16 Jun 2018 10:54:47 GMT</pubDate>
    </item>
    <item>
      <title>DNNCLassifier Tensorflow 上的 label_keys 类型错误</title>
      <link>https://stackoverflow.com/questions/44219077/label-keys-type-error-on-dnnclassifier-tensorflow</link>
      <description><![CDATA[我想将标签嵌入到 Tensorflow 中的 DNNClassifier 模型中。
与文档示例此处不同，我收到以下错误消息：
label_keys_values = [&quot;satan&quot;, &quot;ipsweep&quot;, &quot;nmap&quot;, &quot;portsweep&quot;] 
m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,
feature_columns=deep_columns,
n_classes=4,
hidden_​​units=[12, 4],
label_keys=label_keys_values)
m.fit(input_fn=train_input_fn, steps=200)

文件 &quot;embedding_model_probe.py&quot;，第 118 行，位于 &lt;module&gt;
m.fit(input_fn=train_input_fn, steps=200)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py&quot;，第 281 行，位于 new_func
return func(*args, **kwargs)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py&quot;，第 430 行，位于 fit
loss = self._train_model(input_fn=input_fn, hooks=hooks)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py&quot;，第 927 行，位于 _train_model
model_fn_ops = self._get_train_ops(features, labels)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py&quot;，第 1132 行，在 _get_train_ops 中
return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py&quot;，第 1103 行，在 _call_model_fn 中
model_fn_results = self._model_fn(features, labels, **kwargs)
文件&quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py&quot;，第 180 行，在 _dnn_model_fn
logits=logits)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py&quot;，第 1004 行，在 create_model_fn_ops
labels = self._transform_labels(mode=mode, labels=labels)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py&quot;，第 1033 行，在 _transform_labels
&quot;label_ids&quot;: table.lookup(labels_tensor),
文件 &lt;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lookup/lookup_ops.py&gt;，第 179 行，在查找中
(self._key_dtype, keys.dtype))
TypeError: 签名不匹配。密钥必须是 dtype 

&lt; dtype: &#39;string&#39;&gt;，得到 &lt; dtype: &#39;int64&#39;&gt;

另一方面，如果我将 label_key_values 列设为 numpy.array，则会出现以下错误：
label_keys_values = np.array([&quot;satan&quot;, &quot;ipsweep&quot;, &quot;nmap&quot;, &quot;portsweep&quot;], dtype=&#39;string&#39;)

回溯（最近一次调用最后一次）：
文件 &quot;embedding_model_probe.py&quot;，第 116 行，位于 &lt;module&gt;
label_keys=label_keys_values)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py&quot;，第 337 行，位于 __init__
label_keys=label_keys)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py&quot;，第 331 行，位于 multi_class_head
label_keys=label_keys)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py&quot;，第 986 行，位于 __init__
如果 label_keys 和 len(label_keys) != n_classes:
ValueError:具有多个元素的数组的真值不明确。请使用 a.any() 或 a.all()
]]></description>
      <guid>https://stackoverflow.com/questions/44219077/label-keys-type-error-on-dnnclassifier-tensorflow</guid>
      <pubDate>Sat, 27 May 2017 16:16:58 GMT</pubDate>
    </item>
    </channel>
</rss>