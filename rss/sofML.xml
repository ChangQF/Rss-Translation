<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 24 Jan 2025 06:23:32 GMT</lastBuildDate>
    <item>
      <title>有人有使用 kluster.ai 进行 DeepSeek-R1 托管的经验吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79382626/does-anyone-have-experience-using-kluster-ai-for-deepseek-r1-hosting</link>
      <description><![CDATA[寻找一个使用 DeepSeek-R1 且价格不贵的地方。我正在为一家初创公司做一个项目，他们的预算有限。
我只想得到推理结果而不是推理过程，因为我不需要这些信息。我该怎么做？https://kluster.ai]]></description>
      <guid>https://stackoverflow.com/questions/79382626/does-anyone-have-experience-using-kluster-ai-for-deepseek-r1-hosting</guid>
      <pubDate>Thu, 23 Jan 2025 21:28:38 GMT</pubDate>
    </item>
    <item>
      <title>使用Yolov3进行光学音乐字符识别</title>
      <link>https://stackoverflow.com/questions/79382146/optical-music-characters-recognition-using-yolov3</link>
      <description><![CDATA[我正在尝试编写一个模型（Yolov3）来检测乐谱上的各种音乐符号。但所有适合此目的的数据集都只建立在印刷乐谱上。有没有办法以某种方式将模型适应手写字符？预训练 darknet-53 会对此有所帮助吗？如果我训练 darknet-53 识别手写和印刷字符，这会产生什么影响？
Yolov3 架构：Yolov3]]></description>
      <guid>https://stackoverflow.com/questions/79382146/optical-music-characters-recognition-using-yolov3</guid>
      <pubDate>Thu, 23 Jan 2025 18:12:00 GMT</pubDate>
    </item>
    <item>
      <title>java.lang.AssertionError：Android Studio 中“不支持数据类型 INT32”</title>
      <link>https://stackoverflow.com/questions/79380176/java-lang-assertionerror-does-not-support-data-type-int32-in-android-studio</link>
      <description><![CDATA[我正在使用 TensorFlow Lite 模型在 Android Studio 中开发应用程序。运行应用程序时，我遇到以下错误：
java.lang.AssertionError：TensorFlow Lite 不支持数据类型 INT32

以下是我的代码的相关部分：
// 准备输入张量
val inputFeature0 = TensorBuffer.createFixedSize(inputShape, DataType.FLOAT32)
inputFeature0.loadArray(flatArray)

// 运行推理
val output = model?.process(inputFeature0)
val rawOutputBuffer = output?.outputFeature0AsTensorBuffer

// 根据数据类型将原始数据提取为 IntArray 或 FloatArray
val outputArray = when (rawOutputBuffer?.dataType) {
DataType.INT32 -&gt; rawOutputBuffer.intArray // 直接访问 INT32 数据
DataType.FLOAT32 -&gt; rawOutputBuffer.floatArray.map { it.toInt() }.toIntArray() // 将 FloatArray 转换为 IntArray
else -&gt;抛出 IllegalArgumentException(&quot;不支持的输出张量数据类型：${rawOutputBuffer?.dataType}&quot;)
}


输入张量的类型为 FLOAT32，并且使用 TensorBuffer.createFixedSize() 和 loadArray() 正确加载输入数据。

在处理模型的输出张量 (outputFeature0AsTensorBuffer) 时，我添加了检查以处理 FLOAT32 和 INT32 输出。

尽管如此，应用程序崩溃并显示错误，表明 TensorFlow Lite 不支持 INT32。


我有什么已尝试：

确保输入张量使用 FLOAT32。
验证 TensorFlow Lite 模型是否与 FLOAT32 数据类型兼容。
检查输出张量数据类型并添加对 FLOAT32 和 INT32 的处理。

我预计模型推理可以顺利运行，因为我已经处理了 FLOAT32 和 INT32 输出情况。
问题：

为什么即使输入张量使用 FLOAT32，TensorFlow Lite 也会抛出此错误？
该错误是否与 TensorFlow Lite 内部管理张量维度或元数据等数据类型的方式有关？
如何解决此错误并确保 TensorFlow Lite 成功运行推理？
]]></description>
      <guid>https://stackoverflow.com/questions/79380176/java-lang-assertionerror-does-not-support-data-type-int32-in-android-studio</guid>
      <pubDate>Thu, 23 Jan 2025 07:30:52 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试使用 GNN-LSTM 预测尼日利亚某州未来确诊的脑膜炎病例数</title>
      <link>https://stackoverflow.com/questions/79377500/i-am-trying-to-predict-the-future-number-of-confirmed-cases-of-meningitis-for-a</link>
      <description><![CDATA[以下是代码：
 # 初始化模型
# 初始化模型
model = GNN_LSTM(input_dim=window_size, gcn_hidden_​​dim=16, lstm_hidden_​​dim=32, predict_steps=20)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.MSELoss()
print(f&quot;Shape of time_series_features: {time_series_features.shape}&quot;)

# 训练循环
for epoch in range(200):
model.train()
optimizer.zero_grad()

# 前向传递
out = model(data, time_series_features) # 无需解压，应符合模型输入预期
zamfara_predictions = out[target_idx] # 提取Zamfara 的预测

# 使用 train_mask 计算损失
adapted_train_mask = train_mask[-predict_steps:]

# 打印形状以供调试
print(f&quot;Shape of zamfara_predictions: {zamfara_predictions.shape}&quot;)
print(f&quot;Shape of zamfara_features[-predict_steps:]: {zamfara_features[-predict_steps:].shape}&quot;)
print(f&quot;Shape of adapted_train_mask: {adjusted_train_mask.shape}&quot;)

# 使用调整后的掩码计算损失
loss = criterion(
zamfara_predictions.squeeze()[adjusted_train_mask],
zamfara_features[-predict_steps:][adjusted_train_mask]
)

loss.backward()
optimizer.step()

# 验证
if (epoch + 1) % 20 == 0:
model.eval()
with torch.no_grad():
print(f&quot;zamfara_predictions (squeezed) 的形状：{zamfara_predictions.squeeze().shape}&quot;)
print(f&quot;zamfara_features 的形状：{zamfara_features.shape}&quot;)
print(f&quot;val_mask 的形状：{val_mask.shape}，类型：{val_mask.dtype}&quot;)
print(f&quot;test_mask 的形状：{test_mask.shape}，类型：{test_mask.dtype}&quot;)

# 验证损失计算
val_loss = criterion(
zamfara_predictions.squeeze()[val_mask],
zamfara_features[val_mask]
)
print(f&quot;Epoch {epoch+1}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}&quot;)

# 测试
model.eval()
with torch.no_grad():
test_loss = criterion(zamfara_predictions.squeeze()[test_mask], zamfara_features[test_mask])
print(f&quot;Test Loss: {test_loss.item()}&quot;)

# 指标计算
true_values = zamfara_features[test_mask].numpy()
predictions = zamfara_predictions.squeeze()[test_mask].numpy()

mse = mean_squared_error(true_values, predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(true_values, predictions)
print(f&quot;Test MSE: {mse:.4f}&quot;)
print(f&quot;Test RMSE: {rmse:.4f}&quot;)
print(f&quot;Test MAE: {mae:.4f}&quot;)

# Zamfara 的预测案例

我正在使用 Google Colab 编译该程序。编译后，代码中突出显示了此行：loss = criterion( zamfara_predict，并出现以下错误：
TypeError：&#39;int&#39; 对象不可调用。

代码由 meningitis_​​cases_graph.graphml 组成，它是尼日利亚 37 个州确诊脑膜炎病例的图表表示，edges.csv 是尼日利亚这些州的形状文件。我们想要预测尼日利亚某个州“赞法拉”未来 20 天的确诊病例数。如何解决此错误？]]></description>
      <guid>https://stackoverflow.com/questions/79377500/i-am-trying-to-predict-the-future-number-of-confirmed-cases-of-meningitis-for-a</guid>
      <pubDate>Wed, 22 Jan 2025 11:15:34 GMT</pubDate>
    </item>
    <item>
      <title>当我使用 knn 时收到错误：-215：断言失败）test_samples.type() == CV_32F && test_samples.cols == samples.cols 在函数“findNearest”中</title>
      <link>https://stackoverflow.com/questions/79375305/receiving-error-when-i-use-knn-215assertion-failed-test-samples-type-cv</link>
      <description><![CDATA[我正在开展一个项目，通过 OCR 从图像中读取手写数字。我使用这个 OpenCV 源代码来尝试它是否有效：https://docs.opencv.org/4.x/d8/d4b/tutorial_py_knn_opencv.html。
这是到目前为止有效的代码：
import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt

img = cv.imread(&#39;digits.png&#39;)
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)

# 现在我们将图像拆分为 5000 个单元格，每个单元格大小为 20x20
cells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]

# 将其转换为 Numpy 数组：其大小为 (50,100,20,20)
x = np.array(cells)

# 现在我们准备训练数据和测试数据
train = x[:,:50].reshape(-1,400).astype(np.float32) # Size = (2500,400)
test = x[:,50:100].reshape(-1,400).astype(np.float32) # Size = (2500,400)

# 为训练和测试数据创建标签
k = np.arange(10)
train_labels = np.repeat(k,250)[:,np.newaxis]
test_labels = train_labels.copy()

#启动 kNN，在训练数据上进行训练，然后使用 k=1 的测试数据进行测试
knn = cv.ml.KNearest_create()
knn.train(train, cv.ml.ROW_SAMPLE, train_labels)
ret,result,neighbours,dist = knn.findNearest(test,k=5)

# 现在我们检查分类的准确性
# 为此，将结果与 test_labels 进行比较，并检查哪些是错误的
matches = result==test_labels
correct = np.count_nonzero(matches)
accuracy = correct*100.0/result.size
print( accuracy )

# 保存数据
np.savez(&#39;knn_data.npz&#39;,train=train, train_labels=train_labels)

# 现在保存 kin_data.npz 后加载数据
使用np.load(&#39;knn_data.npz&#39;) 作为数据：
print( data.files )
train = data[&#39;train&#39;]
train_labels = data[&#39;train_labels&#39;]

现在让我们继续尝试手写数字：
在此处输入图像描述
但是我收到一条错误消息：错误：OpenCV(4.10.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/ml/src/knearest.cpp:313: 错误：(-215：断言失败) test_samples.type() == CV_32F &amp;&amp; test_samples.cols == samples.cols 在函数“findNearest”中
此消息的原因是什么？我该如何修复它？]]></description>
      <guid>https://stackoverflow.com/questions/79375305/receiving-error-when-i-use-knn-215assertion-failed-test-samples-type-cv</guid>
      <pubDate>Tue, 21 Jan 2025 17:12:30 GMT</pubDate>
    </item>
    <item>
      <title>训练 Hugging Face Transformer 期间 GPU 利用率几乎始终为 0</title>
      <link>https://stackoverflow.com/questions/79375287/gpu-utilization-almost-always-0-during-training-hugging-face-transformer</link>
      <description><![CDATA[我正在使用我的发票数据对 Donut Cord-v2 模型进行微调，该发票数据在预处理并作为数据集保存在磁盘上时大小约为 360 GB。我几乎完全按照这个笔记本进行操作，只是我有 6 个训练周期而不是 3 个。
我在单个 Nvidia H100 SXM GPU / Intel Xeon® Gold 6448Y / 128 GB RAM 上进行训练。
每当我开始训练并使用 htop 和 nvidia-smi 检查 CPU 和 GPU 利用率时，我都会看到 CPU 的利用率为 10-12%，由 python 使用，GPU 内存几乎一直被占用 90%，但 GPU 利用率几乎始终为 0。如果我不断刷新 nvidia-smi 的输出，则每 10-12 秒一次，利用率将跳转到100% 然后立即回到 0。我不禁感觉到我的 CPU 和 GPU 之间存在瓶颈，CPU 尝试不断处理数据并将其发送到 GPU，GPU 处理速度非常快，并且只是闲置，等待来自 CPU 的下一批。我从磁盘加载已经预处理的数据集，如下所示：
from datasets import load_from_disk
processed_dataset = load_from_disk(r&quot;/dataset/dataset_final&quot;)

我的处理器配置如下：
from transformers import DonutProcessor

new_special_tokens = [] # 将添加到 tokenizer 的新 token
task_start_token = &quot;&lt;s&gt;&quot; # 任务 token 的启动
eos_token = &quot;&lt;/s&gt;&quot; # tokenizer 的 eos token

processor = DonutProcessor.from_pretrained(&quot;naver-clova-ix/donut-base-finetuned-cord-v2&quot;)

# 向 tokenizer 添加新的特殊 token
processor.tokenizer.add_special_tokens({&quot;additional_special_tokens&quot;: new_special_tokens + [task_start_token] + [eos_token]})

# 我们更新了一些与预训练不同的设置；即图像的大小 + 无需旋转
processor.feature_extractor.size = [1200,1553] # 应为 (宽度, 高度)
processor.feature_extractor.do_align_long_axis = False

我的模型配置是：
import torch
from transformers import VisionEncoderDecoderModel, VisionEncoderDecoderConfig

#print(torch.cuda.is_available())

# 从 huggingface.co 加载模型
model = VisionEncoderDecoderModel.from_pretrained(&quot;naver-clova-ix/donut-base-finetuned-cord-v2&quot;)

# 调整嵌入层的大小以匹配词汇表大小
new_emb = model.decoder.resize_token_embeddings(len(processor.tokenizer))
print(f&quot;新嵌入大小： {new_emb}&quot;)
# 调整我们的图像大小和输出序列长度
model.config.encoder.image_size = process.feature_extractor.size[::-1] # (height, width)
model.config.decoder.max_length = len(max(processed_dataset[&quot;train&quot;][&quot;labels&quot;], key=len))

# 添加解码器启动的任务令牌
model.config.pad_token_id = process.tokenizer.pad_token_id
model.config.decoder_start_token_id = process.tokenizer.convert_tokens_to_ids([&#39;&lt;s&gt;&#39;])[0]

我的训练代码是：
import gc
gc.collect()

torch.cuda.empty_cache()

from transformers import Seq2SeqTrainingArguments，Seq2SeqTrainer

导入日志记录
logging.basicConfig(level=logging.INFO)

# 训练参数
training_args = Seq2SeqTrainingArguments(
output_dir=r&quot;/trained&quot;, # 指定本地目录保存模型
num_train_epochs=6,
learning_rate=2e-5,
per_device_train_batch_size=8,
weight_decay=0.01,
fp16=True,
logs_steps=50,
save_total_limit=2,
evaluation_strategy=&quot;no&quot;,
save_strategy=&quot;epoch&quot;,
predict_with_generate=True,
report_to=&quot;none&quot;,
# 禁用推送到集线器
push_to_hub=False

)

# 创建训练器
trainer = Seq2SeqTrainer(
model=model,
args=training_args,
train_dataset=processed_dataset[&quot;train&quot;],
)

# 开始训练
trainer.train()

使用 360 GB 数据集完成 6 个 epoch 的训练预计需要 54 小时。当我在装有 Intel i9 11900KF / RTX 3050 的 PC 上运行完全相同的代码时，我发现 GPU 利用率一直保持在 100%。我的代码中是否存在瓶颈？为什么 CPU 会继续处理已经预处理的数据集？ Cuda 12.6
编辑：
由于我的 RAM 和 CPU 核心数量允许，将 Seq2SeqTrainer 的 dataloader_num_workers 参数更改为 &gt;0 值是否有意义？（并且 CPU 利用率最高为 10-12%）]]></description>
      <guid>https://stackoverflow.com/questions/79375287/gpu-utilization-almost-always-0-during-training-hugging-face-transformer</guid>
      <pubDate>Tue, 21 Jan 2025 17:09:03 GMT</pubDate>
    </item>
    <item>
      <title>使用 nb 方法进行交叉验证</title>
      <link>https://stackoverflow.com/questions/79338629/cross-validation-with-nb-method</link>
      <description><![CDATA[我尝试在 WESBROOK 数据集上使用 k 倍交叉验证。它使用 caret 包中的 train 函数来执行此操作。到目前为止，此函数已与 svm、knn 和 rpart 等方法配合使用，但使用 nb（朴素贝叶斯）方法时，我收到以下错误：
错误 { : 
任务 1 失败 - “未在 newdata 中找到对象中使用的所有变量名称”

我的 train 函数如下所示：
k_folds &lt;- 5
train_control &lt;- trainControl(method = &quot;cv&quot;, number = k_folds, classProbs = TRUE, summaryFunction = twoClassSummary)

nb_model &lt;- train(
TOTLGIVE ~ ., data = train_data,
method = &quot;nb&quot;,
trControl = train_control
)

我检查了一下，没有缺失数据，训练集和测试集中的列名及其类型相同。]]></description>
      <guid>https://stackoverflow.com/questions/79338629/cross-validation-with-nb-method</guid>
      <pubDate>Wed, 08 Jan 2025 09:50:09 GMT</pubDate>
    </item>
    <item>
      <title>如何在处理 EOS 代币时计算拥抱人脸模型的教师强制准确度 (TFA)？</title>
      <link>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</guid>
      <pubDate>Thu, 21 Nov 2024 00:25:48 GMT</pubDate>
    </item>
    <item>
      <title>我正在进行 Yolov8 模型训练，但准确率只有 70% [关闭]</title>
      <link>https://stackoverflow.com/questions/78919106/im-doing-yolov8-model-training-but-the-accuracy-rate-is-70</link>
      <description><![CDATA[我目前正在为我的项目训练 YOLOv8 模型。目标是训练该模型从发送给聊天机器人的照片中识别产品的股票代码。
我有 1,522 个股票代码，每个股票代码大约有 10-15 张照片。虽然照片数量有点少，但我们的客户通常会向我们发送训练中使用的相同照片。例如，他们可能会从我们的 Instagram 个人资料中截取屏幕截图并将其发送给我们。
我正在训练模型，但它的准确率只有 70% 左右。我相信问题可能与超参数有关，但我对它们不太熟悉。我非常感谢您提供的任何建议或意见。
hyp.yaml
# 学习率和动量 Ayarları
lr0: 0.01 # 关闭状态
lrf: 0.01 # Final öğrenme oranı (lr0 ile çarpılır)
动量：0.9#SGD动量
Weight_decay: 0.0005 # L2 正则化（权重衰减）
Warmup_epochs: 2.0 # Isınma epoch sayısı
Warmup_momentum: 0.8 # Isınma süresince başlangıç Momentumu
Warmup_bias_lr: 0.1 # 是否存在偏差
#Kayip Fonksiyonu （损失函数）Ayarları
box: 0.05 # Box kaybı kazancı (GIoU/DIoU/CIoU)
cls: 0.5 # Sınıf kaybı kzancı
iou: 0.2 # IoU eşiği (标签 için)
kobj: 1.0 # 不可以
# 增强 Ayarları (Veri artırma)
hsv_h: 0.005 # Görüntü HSV-Hue artırma（分数）- Çok küçük değişiklikler
hsv_s: 0.1 # Görüntü HSV-Saturation artırma（分数） - Çok küçük değişiklikler
hsv_v: 0.1 # Görüntü HSV-Value artırma（分数） - Çok küçük değişiklikler
度数：0.0 # Görüntü döndürme (+/- derece)
翻译：0.1 # Görüntü kaydırma (+/- 分数)a
比例：0.5 # Görüntü ölçekleme (+/- kazanç)
剪切力：0.0 # Görüntü kaydırma (+/- derece)
透视图：0.0 # Görüntü perspektifi（+/- 分数），0-0.001 arası
Flipud: 0.0 # Görüntüyü yukarıdan aşağıya çevirme (olasılık)
Fliplr: 0.5 # Görüntüyü sağdan sola çevirme （奥拉斯利克）
马赛克: 0.0 # 马赛克艺术 (olasılık) - Bu durumda kapalı
mixup: 0.0 # Mixup artırma (olasılık) - Bu durumda kapalı
copy_paste: 0.0 # 复制粘贴艺术 (olasılık) - Bu durumda kapalı

火车.py =
从 ultralytics 导入 YOLO
导入万数据库
从 wandb.integration.ultralytics 导入 add_wandb_callback
# WandB oturumunu başlatın
如果 __name__ == “__main__”：
    万db.login()
    wandb.init(project=&quot;ultralytics&quot;, job_type=&quot;training&quot;)
    # 模型尤克莱因
    模型 = YOLO(&#39;yolov8n.pt&#39;)
    # WandB 回调&#39;ini ekleyin
    add_wandb_callback（模型，enable_model_checkpointing = True）
    # 模型 eğitin
    模型.火车(
        data=&#39;y.yaml&#39;, # Veri kümesi yapılandırma dosyası
        epochs=100, # Eğitim epoch sayısı
        batch=16, # 批量博语图
        project=&#39;my_project&#39;, # Proje adı (varsayılan: 运行/训练)
        name=&#39;exp&#39;, # 名称 (varsayılan: exp)
        cfg=&#39;hyp.yaml&#39; # 超参数 ayarları
    ）
    #WandB oturumunu sonlandırın
    wandb.finish()
]]></description>
      <guid>https://stackoverflow.com/questions/78919106/im-doing-yolov8-model-training-but-the-accuracy-rate-is-70</guid>
      <pubDate>Tue, 27 Aug 2024 13:16:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在微调期间正确设置 pad token（而不是 eos）以避免模型无法预测 EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>pyspark 实现的 ALS 是如何处理每个用户-项目组合的多个评级的？</title>
      <link>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</link>
      <description><![CDATA[我观察到 ALS 的输入数据不需要每个用户-项目组合都有唯一的评分。
这是一个可重现的示例。
# 示例数据框
df = spark.createDataFrame([(0, 0, 4.0),(0, 1, 2.0), 
(1, 1, 3.0), (1, 2, 4.0), 
(2, 1, 1.0), (2, 2, 5.0)],[&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;])

df.show(50,0)
+----+----+------+
|user|item|rating|
+----+----+------+
|0 |0 |4.0 |
|0 |1 |2.0 |
|1 |1 |3.0 |
|1 |2 |4.0 |
|2 |1 |1.0 |
|2 |2 |5.0 |
+----+----+------+

可以看到，每个用户-商品组合只有一个评分（理想情况）。
如果我们将这个数据框传递到 ALS，它将为您提供如下预测：
# 拟合 ALS
from pyspark.ml.recommendation import ALS
als = ALS(rank=5, 
maxIter=5, 
seed=0,
regParam = 0.1,
userCol=&#39;user&#39;,
itemCol=&#39;item&#39;,
ratingsCol=&#39;rating&#39;,
nonnegative=True)
model = als.fit(df)

# 来自 als 的预测
all_comb = df.select(&#39;user&#39;).distinct().join(broadcast(df.select(&#39;item&#39;).distinct()))
predictions = model.transform(all_comb)

predictions.show(20,0)
+----+----+----------+
|user|item|prediction|
+----+----+----------+
|0 |0 |3.9169915 |
|0 |1 |2.031506 |
|0 |2 |2.3546133 |
|1 |0 |4.9588947 |
|1 |1 |2.8347554 |
|1 |2 |4.003007 |
|2 |0 |0.9958025 |
|2 |1 |1.0896711 |
|2 |2 |4.895194 |
+----+----+----------+

到目前为止，一切对我来说都是有意义的。但是如果我们有一个包含多个用户-项目评分组合的数据框，如下所示 -
# 示例数据框
df = spark.createDataFrame([(0, 0, 4.0), (0, 0, 3.5),
(0, 0, 4.1),(0, 1, 2.0),
(0, 1, 1.9),(0, 1, 2.1),
(1, 1, 3.0), (1, 1, 2.8),
(1, 2, 4.0),(1, 2, 3.6),
(2, 1, 1.0), (2, 1, 0.9),
(2, 2, 5.0),(2, 2, 4.9)],
[&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;])
df.show(100,0)
+----+----+------+
|user|item|rating|
+----+----+------+
|0 |0 |4.0 |
|0 |0 |3.5 |
|0 |0 |4.1 |
|0 |1 |2.0 |
|0 |1 |1.9 |
|0 |1 |2.1 |
|1 |1 |3.0 |
|1 |1 |2.8 |
|1 |2 |4.0 |
|1 |2 |3.6 |
|2 |1 |1.0 |
|2 |1 |0.9 |
|2 |2 |5.0 |
|2 |2 |4.9 |
+----+----+------+

如您在上面的数据框中看到的那样，一个用户-项目组合有多条记录。例如 - 用户“0”多次对项目“0”进行评分，即分别为 4.0、3.5 和 4.1。
如果我将此输入数据框传递给 ALS 会怎样？这会起作用吗？
我最初认为它不应该起作用，因为 ALS 应该根据用户-项目组合获得唯一评级，但当我运行它时，它起作用了，让我感到惊讶！
# 拟合 ALS
als = ALS(rank=5, 
maxIter=5, 
seed=0,
regParam = 0.1,
userCol=&#39;user&#39;,
itemCol=&#39;item&#39;,
ratingsCol=&#39;rating&#39;,
nonnegative=True)
model = als.fit(df)

# 来自 als 的预测
all_comb = df.select(&#39;user&#39;).distinct().join(broadcast(df.select(&#39;item&#39;).distinct()))
predictions = model.transform(all_comb)

predictions.show(20,0)
+----+----+----------+
|user|item|prediction|
+----+----+----------+
|0 |0 |3.7877638 |
|0 |1 |2.020348 |
|0 |2 |2.4364853 |
|1 |0 |4.9624424 |
|1 |1 |2.7311888 |
|1 |2 |3.8018093 |
|2 |0 |1.2490809 |
|2 |1 |1.0351425 |
|2 |2 |4.8451777 |
+----+----+----------+

为什么它会起作用？我以为它会失败，但它没有，而且还给了我预测。
我尝试查看研究论文、ALS 的有限源代码和互联网上可用的信息，但找不到任何有用的东西。
是取这些不同评分的平均值然后将其传递给 ALS 还是其他什么？
有人遇到过类似的事情吗？或者知道 ALS 内部如何处理此类数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</guid>
      <pubDate>Tue, 26 Apr 2022 10:37:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中创建混淆矩阵的图像</title>
      <link>https://stackoverflow.com/questions/65317685/how-to-create-image-of-confusion-matrix-in-python</link>
      <description><![CDATA[我是 Python 和机器学习的新手。我正在研究多类分类（3 个类）。我想将混淆矩阵保存为图像。现在，sklearn.metrics.confusion_matrix() 帮助我找到混淆矩阵，例如：
array([[35, 0, 6],
[0, 0, 3],
[5, 50, 1]])

接下来，我想知道如何将这个混淆矩阵转换为图像并保存为 png。]]></description>
      <guid>https://stackoverflow.com/questions/65317685/how-to-create-image-of-confusion-matrix-in-python</guid>
      <pubDate>Wed, 16 Dec 2020 05:11:00 GMT</pubDate>
    </item>
    <item>
      <title>加载 pickle NotFittedError：TfidfVectorizer - 词汇表不适合</title>
      <link>https://stackoverflow.com/questions/57213165/loading-pickle-notfittederror-tfidfvectorizer-vocabulary-wasnt-fitted</link>
      <description><![CDATA[多标签分类
我正在尝试使用 scikit-learn/pandas/OneVsRestClassifier/logistic 回归预测多标签分类。构建和评估模型有效，但尝试对新样本文本进行分类无效。
场景 1：
一旦我构建了一个模型，就使用名称 (sample.pkl) 保存该模型并重新启动我的内核，但是当我在对样本文本进行预测期间加载已保存的模型 (sample.pkl) 时，它给出了错误：
 NotFittedError：TfidfVectorizer - 词汇表不适合。

我构建了模型并评估了模型，然后我使用名称 sample.pkl 保存了该模型。我重启了内核，然后加载模型对样本文本进行预测 NotFittedError: TfidfVectorizer - 词汇表不适合
推理
import pickle,os
import collections
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import Counter
from nltk.corpus import stopwords
import json, nltk, re, csv, pickle
from sklearn.metrics import f1_score # 性能矩阵
from sklearn.multiclass import OneVsRestClassifier # 二元相关性
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split 
from sklearn.feature_extraction.text import TfidfVectorizer 
来自 sklearn.feature_extraction.text 导入 CountVectorizer
来自 sklearn.preprocessing 导入 MultiLabelBinarizer
来自 sklearn.model_selection 导入 train_test_split
来自 sklearn.linear_model 导入 LogisticRegression
stop_words = set(stopwords.words(&#39;english&#39;))

def cleanHtml(sentence):
&#39;&#39;&#39;&#39; 删除标签 &#39;&#39;&#39;
cleanr = re.compile(&#39;&lt;.*?&gt;&#39;)
cleantext = re.sub(cleanr, &#39; &#39;, str(sentence))
return cleantext

def cleanPunc(sentence): 
&#39;&#39;&#39; 函数用于清除单词中的任何标点符号或特殊字符 &#39;&#39;&#39;
cleaned = re.sub(r&#39;[?|!|\&#39;|&quot;|#]&#39;,r&#39;&#39;,sentence)
cleaned = re.sub(r&#39;[.|,|)|(|\|/]&#39;,r&#39; &#39;,cleaned)
cleaned = cleaned.strip()
cleaned = cleaned.replace(&quot;\n&quot;,&quot; &quot;)
return cleaned

def keepAlpha(sentence):
&quot;&quot;&quot; 保留 alpha 句子 &quot;&quot;&quot;
alpha_sent = &quot;&quot;
for word in sentence.split():
alpha_word = re.sub(&#39;[^a-z A-Z]+&#39;, &#39; &#39;, word)
alpha_sent += alpha_word
alpha_sent += &quot; &quot;
alpha_sent = alpha_sent.strip()
return alpha_sent

def remove_stopwords(text):
&quot;&quot;&quot; 删除停用词 &quot;&quot;&quot;
no_stopword_text = [w for w in text.split() if not w in stop_words]
return &#39; &#39;.join(no_stopword_text)

test1 = pd.read_csv(&quot;C:\\Users\\abc\\Downloads\\test1.csv&quot;)
test1.columns

test1.head()
siNo plot movie_name gender_new
1 故事以 Hannah 开始... 唱歌 [戏剧,teen]
2 Debbie 最喜欢的乐队是 Dream... 最忠实的粉丝 [戏剧]
3 这个祖鲁家庭的故事是... 回来，非洲 [戏剧,纪录片]

获取错误
当我对示例文本进行推断时，我在这里获取错误
def infer_tags(q):
q = cleanHtml(q)
q = cleanPunc(q)
q = keepAlpha(q)
q = remove_stopwords(q)
multilabel_binarizer = MultiLabelBinarizer()
tfidf_vectorizer = TfidfVectorizer()
q_vec = tfidf_vectorizer.transform([q])
q_pred = clf.predict(q_vec)
return multilabel_binarizer.inverse_transform(q_pred)

for i in range(5):
print(i)
k = test1.sample(1).index[0] 
print(&quot;电影：&quot;, test1[&#39;movie_name&#39;][k], &quot;\n预测类型：&quot;, infer_tags(test1[&#39;plot&#39;][k])), print(&quot;实际类型：&quot;,test1[&#39;genre_new&#39;][k], &quot;\n&quot;)


已解决
我解决了将 tfidf 和 multibiniraze 保存到 pickle 模型中的问题
从 sklearn.externals 导入 joblib
pickle.dump(tfidf_vectorizer, open(&quot;tfidf_vectorizer.pickle&quot;, &quot;wb&quot;))
pickle.dump(multilabel_binarizer, open(&quot;multibinirizer_vectorizer.pickle&quot;, &quot;wb&quot;))
vectorizer = joblib.load(&#39;/abc/downloads/tfidf_vectorizer.pickle&#39;)
multilabel_binarizer = joblib.load(&#39;/abc/downloads/multibinirizer_vectorizer.pickle&#39;)

def infer_tags(q):
q = cleanHtml(q)
q = cleanPunc(q)
q = keepAlpha(q) 
q = remove_stopwords(q)
q_vec = vectorizer .transform([q])
q_pred = rf_model.predict(q_vec)
return multilabel_binarizer.inverse_transform(q_pred)

我通过以下链接找到了解决方案
,https://stackoverflow.com/questions/32764991/how-do-i-store-a-tfidfvectorizer-for-future-use-in-scikit-learn&gt;]]></description>
      <guid>https://stackoverflow.com/questions/57213165/loading-pickle-notfittederror-tfidfvectorizer-vocabulary-wasnt-fitted</guid>
      <pubDate>Fri, 26 Jul 2019 04:21:05 GMT</pubDate>
    </item>
    <item>
      <title>Keras 进度条中的准确度是什么意思？</title>
      <link>https://stackoverflow.com/questions/52559086/what-does-the-accuracy-mean-in-the-keras-progress-bar</link>
      <description><![CDATA[在 Keras 中，您将获得类似以下内容：
Epoch 1/1
60000/60000 [==============================] - 297s 5ms/step - 损失：0.7048 - acc：0.7669

60000/60000 [==============================] - 179s 3ms/step
训练集：
acc：94.60%

10000/10000 [================================] - 30s 3ms/step
测试集：
acc： 95.10%

但我是这样拟合的：

model.fit(X_train, oh_y_train,
batch_size=512,
epochs=1,
verbose=1)

.fit() 方法中没有验证数据，它从第 1 个时期测量准确率的是什么？
最终准确率差别很大。]]></description>
      <guid>https://stackoverflow.com/questions/52559086/what-does-the-accuracy-mean-in-the-keras-progress-bar</guid>
      <pubDate>Fri, 28 Sep 2018 16:07:45 GMT</pubDate>
    </item>
    <item>
      <title>ALS（交替最小二乘）算法对用户的多个排名</title>
      <link>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</link>
      <description><![CDATA[嗨，经过大量研究，我们决定使用 Google Cloud 基础架构，并在我们的产品推荐系统中使用 ALS 算法（一种协同过滤方法 - https://cloud.google.com/solutions/recommendations-using-machine-learning-on-compute-engine#Training-the-models ），详细说明如下：
我们有两种类型的客户。第一类是附近销售产品的公司，第二类是打算从这些公司购买产品的消费者

每个消费者都可以搜索附近的公司或按行业搜索公司（例如杂货店、干洗店、肉店等）
当消费者找到一家公司时，他/她可以执行以下操作（他可以一次执行多项操作）
2.1. 仅查看公司简介
2.2. 将公司添加到收藏夹
2.3. 开始与公司聊天
2.4. 从公司下订单
2.5.给公司评分和评论

所以我不明白的是：上面描述的每件商品都被确定为我们数据库中的某些评分列，例如：
查看公司简介：10 分
从公司下订单：20 分
给公司打星或评论：20 分
因此，对于同一用户，每件商品都是单独的评分。
在我们的数据库中，对于用户-公司对，可能会有超过 1 行
例如：
第 1 行：user18-company18-10pts（查看过一次个人资料）
第 2 行：user18-company18-20pts（从公司下订单）
第 3 行：user18-company19-10pts
我不确定这个算法，它是计算该用户对同一家公司的所有评分的总和（我到底想要什么）还是只是寻找单个用户对单个公司的评分的单行？（我想要的是这个 ALS 算法来总结该用户-公司对的第 1 行和第 2 行）
有人知道吗？这对我们的推荐系统非常重要。因为我正在寻找的算法需要计算用户所有评分的总和，以便推荐另一家公司。因为我们的商业模式与电影评分系统不同
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</guid>
      <pubDate>Tue, 01 May 2018 09:53:59 GMT</pubDate>
    </item>
    </channel>
</rss>