<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 27 Sep 2024 21:16:08 GMT</lastBuildDate>
    <item>
      <title>尝试保存自定义 Keras 模型时出现“TypeError：不支持的整数大小 (0)”</title>
      <link>https://stackoverflow.com/questions/79032646/typeerror-unsupported-integer-size-0-when-attempted-to-save-custom-keras-mo</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79032646/typeerror-unsupported-integer-size-0-when-attempted-to-save-custom-keras-mo</guid>
      <pubDate>Fri, 27 Sep 2024 19:14:51 GMT</pubDate>
    </item>
    <item>
      <title>加载变压器时出现问题；ModuleNotFoundError：没有名为“transformers”的模块</title>
      <link>https://stackoverflow.com/questions/79031959/problem-loading-transformers-modulenotfounderror-no-module-named-transformers</link>
      <description><![CDATA[我想使用 huggingface 提供的一些模型。我甚至在开始的时候都遇到了最大的困难。有人能帮我识别和解决这个问题吗？
我正在使用 Kubuntu 24.04。

首先，我创建并激活一个虚拟环境，在其中安装变压器。
python3 -m venv .env
source .env/bin/activate

这是成功的，因为现在我在 Visual Code Studio 中的终端有前缀“(.env)”。
接下来，我从 github 安装最新的变压器：
pip install git+https://github.com/huggingface/transformers

输出成功。然后，我使用 hugginface.co 上推荐的方法测试其成功率：
python3 -c &quot;from transformers import pipeline; print(pipeline(&#39;sentiment-analysis&#39;)(&#39;I love you&#39;))&quot;

输出对我来说看起来正确：
未提供模型，默认为 distilbert/distilbert-base-uncased-finetuned-sst-2-english 和修订版本 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)。
不建议在生产中使用未指定模型名称和修订版本的管道。
硬件加速器（例如 GPU）在环境中可用，但没有将“设备”参数传递给“管道”对象。模型将在 CPU 上。
[{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998656511306763}]

从那里，我尝试运行以下代码：
from transformers import pipeline

但每次我都会得到以下输出：
/bin/python3 /path-to/main.py
回溯（最近一次调用最后一次）：
文件&quot;/path-to/main.py&quot;，第 5 行，在&lt;module&gt;
from transformers import pipeline
ModuleNotFoundError：没有名为“transformers”的模块
]]></description>
      <guid>https://stackoverflow.com/questions/79031959/problem-loading-transformers-modulenotfounderror-no-module-named-transformers</guid>
      <pubDate>Fri, 27 Sep 2024 15:09:24 GMT</pubDate>
    </item>
    <item>
      <title>无法从 coqui-tts 的 tts 库生成语音，并且此错误在单人和多人说话时都会发生</title>
      <link>https://stackoverflow.com/questions/79031258/cant-generate-the-speech-from-library-tts-of-coqui-tts-and-this-error-happens-i</link>
      <description><![CDATA[从 TTS.utils.manage 导入 ModelManager
从 TTS.utils.synthesizer 导入 Synthesizer
从 google.colab 导入文件
初始化模型管理器并加载模型
model_name = &quot;tts_models/en/ljspeech/tacotron2-DDC&quot;
vocoder_name = &quot;vocoder_models/en/ljspeech/hifigan_v2&quot;
model_manager = ModelManager()

model_path, config_path, _ = model_manager.download_model(model_name)

vocoder_path, vocoder_config_path, _ = model_manager.download_model(vocoder_name)

创建合成器对象

synthesizer = Synthesizer(model_path, config_path, vocoder_path, vocoder_config_path, use_cuda=False)

根据手动选择的情绪生成动态 SSML

def generate_dynamic_ssml(chunk):

ssml = f&quot;&quot;&quot;&lt;speak version=&#39;1.0&#39; xmlns=&#39;http://www.w3.org/2001/10/synthesis&#39; xml:lang=&#39;en-US&#39;&gt;&quot;&quot;&quot; 

# 取消注释您想要应用的情感

ssml += f&quot;&lt;prosody pitch=&#39;+10%&#39; rate=&#39;fast&#39;&gt;&lt;emphasis level=&#39;moderate&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # happy

# ssml += f&quot;&lt;prosody pitch=&#39;+5%&#39; rate=&#39;medium&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # romantic

# ssml += f&quot;&lt;prosody pitch=&#39;+7%&#39; rate=&#39;medium&#39;&gt;&lt;emphasis level=&#39;moderate&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 充满希望

# ssml += f&quot;&lt;prosody pitch=&#39;default&#39; rate=&#39;medium&#39;&gt;&lt;emphasis level=&#39;none&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 中立

# ssml += f&quot;&lt;prosody pitch=&#39;-5%&#39; rate=&#39;slow&#39;&gt;&lt;emphasis level=&#39;moderate&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 失望

# ssml += f&quot;&lt;prosody pitch=&#39;-10%&#39; rate=&#39;fast&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 生气

# ssml += f&quot;&lt;prosody pitch=&#39;-10%&#39; rate=&#39;slow&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 害怕

# ssml += f&quot;&lt;prosody pitch=&#39;-15%&#39; rate=&#39;slow&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # 悲伤

# ssml += f&quot;&lt;prosody pitch=&#39;-20%&#39; rate=&#39;very slow&#39;&gt;&lt;emphasis level=&#39;strong&#39;&gt;{chunk}&lt;/emphasis&gt;&lt;/prosody&gt;&quot; # devastated

ssml += &quot;&lt;/speak&gt;&quot; 

return ssml 

为每个块合成语音的函数

def synthesize_speech(text):

# 使用手动选择的情绪为整个文本生成 SSML 

ssml = generate_dynamic_ssml(text) 

# 合成文本 

wav = synthesizer.tts(ssml) 

# 保存输出文件 

output_file = &quot;output_with_emotion.wav&quot; 

synthesizer.save_wav(wav, output_file) 

# 将文件下载到本地机器 

files.download(output_file) 

示例用法

if name == &quot;main&quot;:

sample_text = &quot;&quot;&quot;我很高兴见到你！你让我的心因喜悦和爱而跳动。&quot;&quot;&quot; 

# 使用所选情绪将文本转换为语音

synthesize_speech(sample_text)

使用 tacotron2 模型和 vocoder hifigen 编写代码，之后我使用合成器库 ssml 来改变声音的音调，使其像情绪一样
AttributeError Traceback (most recent call last)

&lt;ipython-input-8-2698293d91f3&gt; in &lt;cell line: 51&gt;()

53 

54 # 使用所选情绪将文本转换为语音

---&gt; 55 synthesize_speech(sample_text)

1 帧

&lt;ipython-input-8-2698293d91f3&gt;在 synthesize_speech(text) 中

39 

40 # 合成文本 

---&gt; 41 wav = synthesizer.tts(ssml)

42 

43 # 保存输出文件 

/usr/local/lib/python3.10/dist-packages/TTS/utils/synthesizer.py 在 tts(self, text, Speaker_name, Language_name, Speaker_wav, Style_wav, Style_text, Reference_wav, Reference_speaker_name, Split_sentences, **kwargs) 中

320 Speaker_id = self.tts_model.Speaker_manager.name_to_id[Speaker_name] 

321 # 处理单扬声器的 Neon 模型。

--&gt; 322 elif len(self.tts_model.speaker_manager.name_to_id) == 1:

323 Speaker_id = list(self.tts_model.speaker_manager.name_to_id.values())[0] 

324 elif not Speaker_name and not Speaker_wav: 

AttributeError: &#39;NoneType&#39; 对象没有属性 &#39;name_to_id&#39;，这是错误


该 tts 代码使用 coqui-tts 制作情感 tts，因此总是出现以下错误，我试图使代码成为单个和多个扬声器
制作单个和多个扬声器，也许有人知道解决方案或建议我使用另一个模型，我想要一个免费的模型]]></description>
      <guid>https://stackoverflow.com/questions/79031258/cant-generate-the-speech-from-library-tts-of-coqui-tts-and-this-error-happens-i</guid>
      <pubDate>Fri, 27 Sep 2024 12:00:55 GMT</pubDate>
    </item>
    <item>
      <title>如何才能准确填补数据集中的缺失值？</title>
      <link>https://stackoverflow.com/questions/79030256/how-can-i-achieve-accurate-imputation-of-missing-values-in-a-dataset</link>
      <description><![CDATA[我正在处理一个包含二手车详细信息的数据集，我发现 Fuel_Type 列中缺少几个值。可能的值包括“汽油”、“E85 混合燃料”、“混合动力”、“柴油”等。目前，我的数据中有超过 4,000 辆电动汽车、不到 50 辆汽油车和一些缺少 Fuel_Type 条目的混合动力汽车。此外，一些条目包含非标准值，如“–”和“不支持”。准确填充这些缺失值对我的分析至关重要，因为它们会显著影响结果。
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# 示例 DataFrame
data = {
&#39;Car&#39;: [&#39;Toyota&#39;, &#39;Honda&#39;, &#39;Tesla&#39;, None, &#39;Ford&#39;],
&#39;Fuel_Type&#39;: [&#39;Gasoline&#39;, &#39;E85 Flex Fuel&#39;, np.nan, &#39;Hybrid&#39;, None],
&#39;Transmission&#39;: [&#39;Automatic&#39;, None, &#39;Automatic&#39;, &#39;Manual&#39;, &#39;Manual&#39;]
}

df = pd.DataFrame(data)

# 初始插补尝试
imputer = SimpleImputer(strategy=&#39;most_frequent&#39;)
df[&#39;Fuel_Type&#39;] = imputer.fit_transform(df[[&#39;Fuel_Type&#39;]])
print(df)
]]></description>
      <guid>https://stackoverflow.com/questions/79030256/how-can-i-achieve-accurate-imputation-of-missing-values-in-a-dataset</guid>
      <pubDate>Fri, 27 Sep 2024 07:15:40 GMT</pubDate>
    </item>
    <item>
      <title>当有多个场景切换时，有没有办法让 SAM2 跨场景跟踪同一个人？</title>
      <link>https://stackoverflow.com/questions/79029852/is-there-a-way-to-have-sam2-track-the-same-person-across-scenes-when-there-are-m</link>
      <description><![CDATA[使用 Meta 的 SAM2 演示，当场景切换时，面具通常会切换到不同的玩家身上。
我知道手动重新标记每个场景中的玩家是一种选择，但我想探索是否有可用的自动化解决方案。

我尝试使用 Meta 的 SAM2 演示，网址为 https://sam2.metademolab.com/
我希望它能在整个视频中跟踪勒布朗
我发现它只在第一个场景中这样做，偶尔当勒布朗是镜头中唯一的人或主要人物时
]]></description>
      <guid>https://stackoverflow.com/questions/79029852/is-there-a-way-to-have-sam2-track-the-same-person-across-scenes-when-there-are-m</guid>
      <pubDate>Fri, 27 Sep 2024 04:47:00 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在 rust 中导入用 Python 制作的 ML 模型 (.pkl) 吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79029841/can-we-import-a-python-made-ml-model-pkl-in-rust</link>
      <description><![CDATA[我之前用 Python 构建了一个项目，但由于它占用了太多资源并且缺乏并发性，所以我改用了 rust。现在我不知道如何正确迁移它，大多数代码已经迁移，但我无法导入导出为 .pkl 文件的 ml 模型。]]></description>
      <guid>https://stackoverflow.com/questions/79029841/can-we-import-a-python-made-ml-model-pkl-in-rust</guid>
      <pubDate>Fri, 27 Sep 2024 04:43:46 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练推荐系统算法[关闭]</title>
      <link>https://stackoverflow.com/questions/79029441/trying-to-train-an-algorithm-for-a-recommender-system</link>
      <description><![CDATA[我一直收到以下错误：

ratings_matrix[which_train, ] 中的错误：维度数不正确

&gt; which_train &lt;- sample(x = c(TRUE, FALSE),
+ size = (ratings_matrix),
+ replace = TRUE,
+ prob = c(0.8, 0.2))
&gt; recc_data_train &lt;- ratings_matrix[which_train, ]
]]></description>
      <guid>https://stackoverflow.com/questions/79029441/trying-to-train-an-algorithm-for-a-recommender-system</guid>
      <pubDate>Thu, 26 Sep 2024 23:51:08 GMT</pubDate>
    </item>
    <item>
      <title>如何保存/查看树状图中的信息？</title>
      <link>https://stackoverflow.com/questions/79028907/how-can-you-save-look-at-the-information-in-a-dendogram</link>
      <description><![CDATA[我正在尝试分析数据以根据树状图确定结果。问题是我主要有 2 组数据“H”和“U”，两者一起进行分析。在树状图的末尾，我需要知道哪个区域的“H”更多和“U”。
我尝试使用树状图的字典信息，但当我打开函数时，我注意到它根本没有包含我通过链接输入的信息。
以下是我正在使用的代码：
import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, set_link_color_palette
from scipy.cluster import Hierarchy
import pandas as pd
from sklearn.decomposition import PCA

df=pd.read_csv(name+&quot;.csv&quot;, index_col=None, header=0)

normalized_df=(df-df.mean())/df.std()

pca=PCA(n_components=6, n_oversamples=6)
principalComponents = pca.fit_transform(df)
principalDF = pd.DataFrame(data = principalComponents, columns = [&#39;principal component 1&#39;, &#39;principal component 2&#39;, &#39;principal component 3&#39;, &#39;principal component 4&#39;, &#39;principal component 5&#39;, &#39;principal component 6&#39;])

#将类型 (U/H) 添加到数据框
finalDF = pd.concat([principalDF, full_df[[&#39;type&#39;]]], axis = 1)

#从这里开始，我开始遇到问题
data = list(zip(finalDF[&#39;principal component 1&#39;], finalDF[&#39;principal component 2&#39;]))

linkage_data = linkage(data, method=&#39;ward&#39;, metric=&#39;euclidean&#39;)
hierarchy.set_link_color_palette([&#39;r&#39;,&#39;g&#39;,&#39;b&#39;,&#39;w&#39;])
den=dendrogram(linkage_data)
plt.title(&quot;Attempt #1&quot;)
plt.show()

树状图看起来与预期一致。问题是我无法区分每个点的类型（H 或 U）。
如前所述，我尝试查看树状图的属性，例如 icoord 和 dcoord，但我无法弄清楚它的含义。
我使用的数据本身是 6 个不同的列，具有不同的数字，这是我标准化的数据((df-df.mean())/df.std)，然后取主成分。
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79028907/how-can-you-save-look-at-the-information-in-a-dendogram</guid>
      <pubDate>Thu, 26 Sep 2024 19:43:38 GMT</pubDate>
    </item>
    <item>
      <title>如何在多个 gpu 上运行 Qwen2-VL 模型？</title>
      <link>https://stackoverflow.com/questions/79027046/how-to-run-qwen2-vl-models-on-multiple-gpus</link>
      <description><![CDATA[我有 4 个 gpu，我想运行 Qwen2 VL 模型，但我收到“设备端断言已触发。使用 TORCH_USE_CUDA_DSA 进行编译以启用设备端断言”错误。
model_name=&quot;Qwen/Qwen2-VL-2B-Instruct&quot;
model = Qwen2VLForConditionalGeneration.from_pretrained(
model_name, torch_dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)
model = nn.DataParallel(model)
processor = AutoProcessor.from_pretrained(model_name)

messages = [
{
&quot;role&quot;: &quot;user&quot;,
&quot;content&quot;: [
{
&quot;type&quot;: &quot;image&quot;,
&quot;image&quot;: file
},
{
&quot;type&quot;: &quot;text&quot;,
&quot;text&quot;: &quot;&quot;&quot;描述图像&quot;&quot;&quot;
}
]
}
]
text = processing.apply_chat_template(
messages, tokenize=False, add_generation_prompt=True)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processing(
text=[text],
images=image_inputs,
videos=video_inputs,
padding=True,
return_tensors=&quot;pt&quot;,
)
使用 torch.no_grad():
generated_ids = model.module.generate(**inputs, max_new_tokens=128)

但我总是得到：
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [35,0,0], thread: [31,0,0] 断言 `-sizes[i] &lt;= index &amp;&amp; index &lt; sizes[i] &amp;&amp; &quot;index out of bounds&quot;` 失败。
错误：CUDA 错误：设备端断言已触发
使用 `TORCH_USE_CUDA_DSA` 进行编译以启用设备端断言。

回溯（最近一次调用）：
文件“/home/ubuntu/projects/mistral-qaC/services/VisionService.py”，第 104 行，位于 ask_vision
generated_ids = self.model.module.generate(
^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/utils/_contextlib.py”，第 116 行，位于 decorate_context
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^
文件“/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/generation/utils.py”，第2015，在 generate 中
result = self._sample(
^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/generation/utils.py&quot;，第 2965 行，在 _sample 中
output = self(**model_inputs, return_dict=True)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;，第 1553 行，在 _wrapped_call_impl 中
return self._call_impl(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/torch/nn/modules/module.py&quot;，第 1562 行，在 _call_impl 中
return forward_call(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/accelerate/hooks.py&quot;，第 169 行，在 new_forward 中
output = module._old_forward(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/home/ubuntu/projects/upper/lib/python3.12/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py&quot;，第 1598 行，正向
输入_embeds[image_mask] = image_embeds
~~~~~~~~~~~~~^^^^^^^^^^^^
RuntimeError：CUDA 错误：设备端断言已触发
使用 `TORCH_USE_CUDA_DSA` 进行编译以启用设备端断言。

我尝试了什么？

使用 CUDA_LAUNCH_BLOCKING=1 python script.py 运行我的 python 脚本，但它也不起作用。
打印输入和模型设备：模型设备：cuda:0
输入设备：cuda:0
torch.cuda.synchronize() 和 torch.cuda.empty_cache() 在生成之前。

输入的形状：

input_ids 的形状：torch.Size([1, 759])
attention_mask 的形状：torch.Size([1, 759])
 pixel_values: torch.Size([2940, 1176])
image_grid_thw 的形状：torch.Size([1, 3])

我的 transformers 和 pytorch 版本是：
transformers==4.45.0.dev0
torch==2.4.1+cu124

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79027046/how-to-run-qwen2-vl-models-on-multiple-gpus</guid>
      <pubDate>Thu, 26 Sep 2024 11:21:59 GMT</pubDate>
    </item>
    <item>
      <title>机器学习与分类神经网络中的数据重叠[关闭]</title>
      <link>https://stackoverflow.com/questions/79026738/dataoverlap-in-machine-learning-with-classification-neurnal-network</link>
      <description><![CDATA[嗨，我正在做一个关于体积脑图像的机器学习项目
但我没有 ML 背景（有一点编码……但不多），所以我一直在用 pytorch lightning 学习 ML。这很有趣，但有时很难……
所以我现在的问题是我的模型在很大程度上对数据集过度拟合，这使得我在训练中的三个类准确度达到每类 0.85 - ~1.0 之间的准确度，并且损失以良好的曲线下降。遗憾的是验证准确度不足。对于第一类，准确度稳定在 0.05 左右，而其他类稳定在 0.3 左右徘徊。此外，损失从 1.2 略微下降到略高于 1。
（一些医学内容，让您了解数据重叠问题所在）
这些类别的图像是接受 CT 灌注的患者的灰度图像。它显示了患者动脉闭塞或阻塞时大脑灌注的变化。
对于非医学方面的人来说，你可以把它看作是一棵树的树枝，上面有叶子。如果树枝有阻塞，那么该树枝上的叶子就会枯萎。
我的类别是 ICA-T、M1 和最后一类 M2
ICA-T 分支在 M1 中。这意味着体积图像可能包含这些片段的信号相似性。所以也许模型认为由于重叠，ICA-T 病例被猜测为 M1??
这就是类别背景。原始数据集总体如下：
类别 0 (ICA-T)：94，
类别 1 (M1)：366，
类别 2 (M2)：119
总计 579
这显然在类别 1 中占比过高。我对此有疑问，因此我将类别 1 减少到 157，没有考虑任何策略。
0 类 (ICA-T)：94，
1 类 (M1)：157，
2 类 (M2)：119
总计 370
我有一个训练数据集、验证数据集和测试数据集，这些数据集是我通过分层随机分割获得的，这给了我
训练：259 - 0 类：66，1 类：110，2 类：83
验证：37 - 0 类：9，1 类：16，2 类：12
测试：74 - 0 类：19，1 类：31，2 类：24
进一步的信息是，图像是体积图像，空间大小为 256,256,32。
我认为问题在于类相似性。但我不确定是否如此。
有人遇到过类似的问题吗？他们解决了吗？或者有人可以指导我找到答案吗？
遗憾的是我的项目很快就要完成了，所以我没有时间。如果有人需要代码，请询问，但我认为这对我的问题来说没有必要？..
这是我使用的结构：
class CNN5_Mod(L.LightningModule): # 模型定义
def __init__(self, num_classes):
super(CNN5_Mod, self).__init__()
from Project1.MyFile.ConfigMain import Config
config = Config()
# 特征 - x y z
# 1 - 256 256 32
&quot;&quot;&quot; Block 1 &quot;&quot;&quot;
self.conv1 = nn.Conv3d(1, config.c1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=1) # 16 - 256 256 32
self.relu1 = nn.ReLU()
self.bt_nm1 = nn.BatchNorm3d(config.c1)

self.conv2 = nn.Conv3d(config.c1, config.c2, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=1) # 32 - 128 128 16
self.relu2 = nn.ReLU()
self.bt_nm2 = nn.BatchNorm3d(config.c2)

self.dropout1 = nn.Dropout3d(config.dropout1)
self.pooling1 = nn.MaxPool3d(kernel_size=2, stride=2)
# 输出 (64, 64,64,8)

&quot;&quot;&quot; 块 2 &quot;&quot;&quot;
self.conv3 = nn.Conv3d(config.c2, config.c3, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=1) # 32 32 4
self.relu3 = nn.ReLU()
self.bt_nm3 = nn.BatchNorm3d(config.c3)

self.conv4 = nn.Conv3d(config.c3, config.c4, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=1) # 16 16 2
self.relu4 = nn.ReLU()
self.bt_nm4 = nn.BatchNorm3d(config.c4)

self.dropout2 = nn.Dropout3d(config.dropout2)
self.pooling2 = nn.MaxPool3d(kernel_size=2, stride=2)
# 输出 (256, 8,8,1)

&quot;&quot;&quot; 扁平化 &quot;&quot;&quot;
self.flatten = nn.Flatten(1)

&quot;&quot;&quot; 隐藏层 &quot;&quot;&quot;
self.fc1 = nn.Linear(config.in_fc, 64)
self.relu5 = nn.ReLU()
self.fc_dropout = nn.Dropout3d(config.fc_dropout)

&quot;&quot;&quot; 输出 &quot;&quot;&quot;
self.fc2 = nn.Linear(64, num_classes)

&quot;&quot;&quot; 预测 &quot;&quot;&quot;
self.softmax = nn.Softmax(dim=1)

我改变了 dropout rate 和其他一些值。但我不知所措。。]]></description>
      <guid>https://stackoverflow.com/questions/79026738/dataoverlap-in-machine-learning-with-classification-neurnal-network</guid>
      <pubDate>Thu, 26 Sep 2024 10:13:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中训练眼睛验证（而非识别）模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</link>
      <description><![CDATA[我想知道我们如何训练一对一图像验证模型。模型拍摄两张图像并验证它们是否相同。我想要训练眼睛认证模型的软件算法。
我在网上搜索过，但只能找到关于识别软件算法（一对多）的答案。
如何在 PyTorch 代码中训练验证模型？
需要澄清的是，相同是指眼睛相同，意味着它们属于同一个人。这是一个验证模型。]]></description>
      <guid>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</guid>
      <pubDate>Tue, 24 Sep 2024 18:10:07 GMT</pubDate>
    </item>
    <item>
      <title>使用斑点检测来计数黑色圆形种子[关闭]</title>
      <link>https://stackoverflow.com/questions/79016356/counting-black-round-seeds-with-blob-detection</link>
      <description><![CDATA[我想玩一下 OpenCV 或类似技术，以便能够计算简单的黑色球体（种子，在本例中为拟花椒）。
其他时候，种子中间的白色反射较少，但主要特征是它是“圆形”的、黑色的，几乎总是具有相同的尺寸，并且可以（或有时没有）一个白色的小斑点。





我应该从哪里开始才能让 5 张照片的种子数量大致相同（或者最好是完全相同）？（种子数量相同，我只是在拍摄照片之间摇晃了一下容器）
CV 还是 ML？从哪里开始？
附言：如果有帮助，我也可以尝试物理去除较小的黑色棍子和不好的种子……但理论上，如果可以有可靠的方法可以忽略这些小的“非种子”暗元素，那就太好了……
附言：如果这也能有帮助，我还可以修改拍照的方式……]]></description>
      <guid>https://stackoverflow.com/questions/79016356/counting-black-round-seeds-with-blob-detection</guid>
      <pubDate>Mon, 23 Sep 2024 21:30:43 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenCV 进行 OCR 和文本检测和识别</title>
      <link>https://stackoverflow.com/questions/69647125/how-to-use-opencv-to-do-ocr-and-text-detect-and-recognition</link>
      <description><![CDATA[我正在开发一个测试应用程序，使用 Google Collab 在 Python 中开发一个小型文本检测和识别应用程序。您能提供一些代码示例来实现这一点吗？我的要求是我应该能够使用 OpenCV 检测和识别图像中的文本。
请提供建议。]]></description>
      <guid>https://stackoverflow.com/questions/69647125/how-to-use-opencv-to-do-ocr-and-text-detect-and-recognition</guid>
      <pubDate>Wed, 20 Oct 2021 13:40:59 GMT</pubDate>
    </item>
    <item>
      <title>xgboost.plot_tree：二元特征解释</title>
      <link>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</link>
      <description><![CDATA[我构建了一个 XGBoost 模型，并试图检查各个估计量。作为参考，这是一个二元分类任务，具有离散和连续输入特征。输入特征矩阵是 scipy.sparse.csr_matrix。
然而，当我去检查一个单独的估计量时，我发现很难解释二元输入特征，例如下面的 f60150。最底部图表中的实值 f60150 很容易解释 - 其标准在该特征的预期范围内。但是，对二元特征 &lt;X&gt; &lt; -9.53674e-07 进行的比较没有意义。这些特征中的每一个要么是 1，要么是 0。-9.53674e-07 是一个非常小的负数，我想这只是 XGBoost 或其底层绘图库中的一些浮点特性，但当特征始终为正时使用这种比较是没有意义的。有人能帮我理解哪个方向（即 是、缺失 与 否 对应这些二进制特征节点的哪一侧为真/假吗？
这是一个可重现的示例：
import numpy as np
import scipy.sparse
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from xgboost import plot_tree, XGBClassifier
import matplotlib.pyplot as plt

def booleanize_csr_matrix(mat):
&#39;&#39;&#39; 将具有正整数元素的稀疏矩阵转换为 1 &#39;&#39;&#39;
nnz_inds = mat.nonzero()
keep = np.where(mat.data &gt; 0)[0]
n_keep = len(keep)
result = scipy.sparse.csr_matrix(
(np.ones(n_keep), (nnz_inds[0][keep], nnz_inds[1][keep])),
shape=mat.shape
)
返回结果

### 设置数据集
res = fetch_20newsgroups()

text = res.data
outcome = res.target

### 使用 CountVectorizer 的默认参数创建初始计数矩阵
vec = CountVectorizer()
X = vec.fit_transform(text)

# 是否“布尔化”输入矩阵
booleanize = True

# 是否在“布尔化”之后将数据类型转换为与 `vec.fit_transform(text)` 返回的内容相匹配
to_int = True

如果 booleanize 和 to_int:
X = booleanize_csr_matrix(X)
X = X.astype(np.int64)

# 使其成为二元分类问题
y = np.where(outcome == 1, 1, 0)

# 随机状态确保我们能够一致地比较树及其特征
model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

将 booleanize 和 to_int 设置为 True 并运行上述程序，将生成以下图表：

将 booleanize 和 to_int 设置为 False 并运行上述程序，将生成以下图表：

哎呀，即使我做了一个非常简单的例子，我也会得到“正确”的结果，无论 X 或 y 是整数还是浮点类型。
X = np.matrix(
[
[1,0],
[1,0],
[0,1],
[0,1],
[1,1],
[1,0],
[0,0],
[0,0],
[1,1],
[0,1]
]
)

y = np.array([1,0,0,0,1,1,1,0,1,1])

model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</guid>
      <pubDate>Thu, 13 Sep 2018 13:06:06 GMT</pubDate>
    </item>
    <item>
      <title>真实世界参数优化</title>
      <link>https://stackoverflow.com/questions/14013266/realworld-parameter-optimization</link>
      <description><![CDATA[我需要对我的最新研究项目进行参数优化。我有一个算法，目前有 5 个参数（四个双精度 [0,1] 和一个具有 3 个值的名义参数）。该算法使用这些参数来计算一些东西，然后我计算准确率、召回率和 FMeasure。一次运行大约需要 1.8 秒。目前，我正在以 0.1 的步长遍历每个参数，这向我展示了全局最大值的大致位置。但我想找到精确的全局最大值。我研究过梯度下降，但我真的不知道如何将其应用于我的算法（如果可能的话）。有人可以指导我如何实现这样的算法吗，因为我对这类工作很陌生。
干杯，
丹尼尔]]></description>
      <guid>https://stackoverflow.com/questions/14013266/realworld-parameter-optimization</guid>
      <pubDate>Sun, 23 Dec 2012 17:55:54 GMT</pubDate>
    </item>
    </channel>
</rss>