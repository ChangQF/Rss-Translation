<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 01 Sep 2024 06:22:34 GMT</lastBuildDate>
    <item>
      <title>数据分类不适用于 BERT 模型</title>
      <link>https://stackoverflow.com/questions/78936387/data-classification-doesnt-work-with-bert-model</link>
      <description><![CDATA[我需要使用输入 CSV 来训练模型，其中包含错误消息和错误分类。然后，当仅使用错误消息进行测试时，它应该会自动分类。
我使用了 BERT 模型，这是代码：
import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
# 加载训练数据并打印列
training_data = pd.read_excel(&quot;C:\\Users\\foobar\\Documents\\Training Data.xlsx&quot;)
print(training_data.columns) # 打印以验证列名
# 确保列名中没有空格
training_data.columns = training_data.columns.str.strip()
# 根据输出使用正确的列名
error_messages = training_data[&quot;Error Message&quot;].dropna()
error_classification = training_data[&quot;Error Classification&quot;].dropna()
# 确保长度一致
min_length = min(len(error_messages), len(error_classification))
error_messages = error_messages.iloc[:min_length]
error_classification = error_classification.iloc[:min_length]
# 使用 LabelEncoder 将标签转换为数字
label_encoder = LabelEncoder()
error_classification_encoded = label_encoder.fit_transform(error_classification)
# 将数据拆分为训练集和验证集
train_messages, val_messages, train_labels, val_labels = train_test_split(
error_messages, error_classification_encoded, test_size=0.2, random_state=42
)
# 从 Hugging Face 加载 BERT 模型和 tokenizer
tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
bert_model = TFBertModel.from_pretrained(&#39;bert-base-uncased&#39;)
# 对数据进行标记和预处理
def preprocess_text(texts):
return tokenizer(
texts.tolist(),
max_length=128,
truncation=True,
padding=&#39;max_length&#39;,
return_tensors=&#39;tf&#39;
)
train_tokens = preprocess_text(train_messages)
val_tokens = preprocess_text(val_messages)
# 定义模型架构
input_ids = tf.keras.Input(shape=(128,), dtype=tf.int32, name=&#39;input_ids&#39;)
attention_mask = tf.keras.Input(shape=(128,), dtype=tf.int32, name=&#39;attention_mask&#39;)
bert_output = bert_model([input_ids,tention_mask])[1]
x = tf.keras.layers.Dense(64,activation=&quot;relu&quot;)(bert_output)
output = tf.keras.layers.Dense(len(label_encoder.classes_),activation=&quot;softmax&quot;)(x)
model = tf.keras.Model(inputs=[input_ids,attention_mask],outputs=output)
# 编译模型
model.compile(optimizer=&quot;adam&quot;,loss=&quot;sparse_categorical_crossentropy&quot;,metrics=[&quot;accuracy&quot;])
# 将标签转换为 NumPy 数组并确保形状正确
train_labels = np.array(train_labels).astype(int).reshape(-1)
val_labels = np.array(val_labels).astype(int).reshape(-1)
# 打印形状和类型以供调试
print(f&quot;train_input shape: {train_tokens[&#39;input_ids&#39;].shape}&quot;)
print(f&quot;val_input shape: {val_tokens[&#39;input_ids&#39;].shape}&quot;)
print(f&quot;train_labels shape: {train_labels.shape}&quot;)
# 训练模型
model.fit(
[train_tokens[&#39;input_ids&#39;], train_tokens[&#39;attention_mask&#39;]],
train_labels,
validation_data=([val_tokens[&#39;input_ids&#39;], val_tokens[&#39;attention_mask&#39;]], val_labels),
epochs=10
)
# 加载测试数据
testing_data = pd.read_excel(&quot;C:\\Users\\foobar\\Documents\\Testing Data.xlsx&quot;)
test_messages = testing_data[&quot;Error Message&quot;]
# 预处理测试数据
test_tokens = preprocess_text(test_messages)
# 对测试数据执行预测
predictions = model.predict([test_tokens[&#39;input_ids&#39;], test_tokens[&#39;attention_mask&#39;]])
predicted_labels = np.argmax(predictions, axis=1)
# 将标签解码回原始格式
decoded_predictions = label_encoder.inverse_transform(predicted_labels)
# 使用预测的错误分类更新输出 CSV
output_data = pd.DataFrame({&quot;Error Message&quot;: test_messages, &quot;Error Classification&quot;:coded_predictions})
output_data.to_csv(&quot;C:\\Users\\foobar\\Documents\\Output.csv&quot;, index=False)

训练输入包含 HTTP 错误消息和分类：（样本）

测试输入如下：

但是 Output.csv 输出结果不正确，出现 5** 错误，错误地显示“客户端错误”。
]]></description>
      <guid>https://stackoverflow.com/questions/78936387/data-classification-doesnt-work-with-bert-model</guid>
      <pubDate>Sun, 01 Sep 2024 00:59:26 GMT</pubDate>
    </item>
    <item>
      <title>使用flutter计算图像中的对象数[关闭]</title>
      <link>https://stackoverflow.com/questions/78935944/count-objects-in-image-by-using-flutter</link>
      <description><![CDATA[我有一张包含很多物体的图像，所以我想检测每个物体，然后计算该图像中重复的物体并返回每个重复物体的数量，但我不知道是否可以使用flutter来实现这一点？
我搜索了更多关于这个主题的内容，但大多数都与使用Tflite有关，并且该包与最新版本的flutter不匹配，而且我根本找不到任何关于使用flutter计算图像中物体的示例。
图像将如下所示：
]]></description>
      <guid>https://stackoverflow.com/questions/78935944/count-objects-in-image-by-using-flutter</guid>
      <pubDate>Sat, 31 Aug 2024 19:26:04 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 验证拆分导致 75% 的类别过度拟合，25% 的类别为随机噪声</title>
      <link>https://stackoverflow.com/questions/78934378/tensorflow-validation-split-leading-to-75-of-classes-overfitted-25-random-noi</link>
      <description><![CDATA[我正在执行机器学习任务，其中验证分割似乎存在数据泄漏。如果我调整分割，它会移动过度拟合的类的数量。
X_train = resize_images(X_train, (512, 512))

y_train = label_enc.transform(y_train)
n_classes = len(list(label_enc.classes_))

X_train = np.asarray(X_train)

X_train, y_train = add_extra_dim(X_train, y_train, n_classes)

model = build_model()

model.fit(X_train, y_train, epochs=50, validation_split = 0.25, callbacks=[callback])

def build_model():
model = ATCNet(shape=(100, 100, 1), n_classes=n_classes)

top3 = tf.keras.metrics.TopKCategoricalAccuracy(k=3, name=&quot;top3&quot;)
top5 = tf.keras.metrics.TopKCategoricalAccuracy(k=5, name=&quot;top5&quot;)
model.compile(optimizer=Adam(), 
loss=&#39;categorical_crossentropy&#39;, 
metrics=[&#39;accuracy&#39;, top3, top5])
返回模型

[[输入图片描述在此输入图片描述在此描述](https://i.sstatic.net/nS2vvCJP.png)](https://i.sstatic.net/oCSLgoA4.png)
我已测试，不存在任何数据泄露。
我已将问题隔离到验证拆分，但我找不到任何有关如何修复它或我做错了什么的信息]]></description>
      <guid>https://stackoverflow.com/questions/78934378/tensorflow-validation-split-leading-to-75-of-classes-overfitted-25-random-noi</guid>
      <pubDate>Sat, 31 Aug 2024 05:41:32 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn)</title>
      <link>https://stackoverflow.com/questions/78934120/runtimeerror-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-gra</link>
      <description><![CDATA[我正在训练一个自动编码器，针对我的具体用例，我在 PyTorch 中实现了基于 Wasserstein 度量（地球移动度量）的自定义损失。我使用这个的一般想法是，下面代码片段中的 pred 和 target 是一批向量，比如 batch_size x vector_length
def wasserstein_distance(pred, target, num_bins=200):
# 获取列数（特征）
num_columns = pred.size(1)

# 计算每列的 Wasserstein 距离
distances = []
for i in range(num_columns):
# 获取列的预测值和目标值
pred_col = pred[:, i]
target_col = target[:, i]

# 确定列的直方图范围
min_val = min(pred_col.min().item(), target_col.min().item())
max_val = max(pred_col.max().item(), target_col.max().item())

# 计算预测值和目标值的直方图
pred_hist = torch.histc(pred_col, bins=num_bins, min=float(min_val), max=float(max_val))
target_hist = torch.histc(target_col, bins=num_bins, min=float(min_val), max=float(max_val))

# 将直方图标准化以形成概率分布
pred_hist /= pred_hist.sum()
target_hist /= target_hist.sum()

# 计算累积分布函数 (CDF)
pred_cdf = torch.cumsum(pred_hist, dim=0)
target_cdf = torch.cumsum(target_hist, dim=0)

# 计算 Wasserstein 距离（地球移动距离）
wasserstein_dist = torch.sum(torch.abs(pred_cdf - target_cdf))

distances.append(wasserstein_dist)

# 计算所有列的 Wasserstein 距离的平均值
mean_wasserstein_distance = torch.mean(torch.stack(distances))

return mean_wasserstein_distance

我在以下训练函数中使用它：
def train_model(model: nn.Module, data_loader: torch.utils.data.DataLoader, epoch_count: int, learning_rate: float) -&gt; np.ndarray:
print(&quot;##### 开始训练模型 #####&quot;)
model.train()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model.to(device)
loss = []

for epoch in range(epoch_count):
total_loss = 0.0
total_samples = 0

for batch in data_loader:
x = batch[0]
x = x.to(device)
optimizer.zero_grad()
pred_ae = model(x)
loss_ae = wasserstein_distance(pred_ae, x[:, 6:])

loss_ae.backward()
optimizer.step()

total_loss += loss_ae.item()
total_samples += x.size(0)

avg_loss = total_loss / total_samples
loss.append(avg_loss)

print(f&#39;Epoch: {epoch} Loss per unit: {avg_loss}&#39;)

print(&quot;##### FINISHED TRAINING OF MODEL #####&quot;)
return model, np.array(losses)


但是，我收到以下错误：
File &quot;&quot;, line 432, in &lt;module&gt;
model,losses = train_model(model,data,50,0.0001)
文件&quot;&quot;，第 237 行，在 train_model 中
loss_ae.backward()
文件&quot;C:\Users\aksha\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_tensor.py&quot;，第 522 行，在 Backward 中
torch.autograd.backward(
文件&quot;C:\Users\aksha\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\autograd\__init__.py&quot;，第 266 行，在 Backward 中
Variable._execution_engine.run_backward( # 调用 C++ 引擎运行反向传递
RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn

我确信每个requires_grad 默认为 true（我没有手动更改任何内容）。我怀疑问题是我为损失返回了一个值，而 Backward 需要每个张量都有一个值？但这不是我真正想要的训练方案，我希望每个更新都是特定于批次的（具体来说，学习每个批次中每列的分布）]]></description>
      <guid>https://stackoverflow.com/questions/78934120/runtimeerror-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-gra</guid>
      <pubDate>Sat, 31 Aug 2024 01:05:47 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 FAISS 减少大型人脸数据库的人脸识别中的误报？</title>
      <link>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</link>
      <description><![CDATA[我正在开发一个使用人脸识别的考勤跟踪系统。
该系统的工作原理如下：

1. 人脸检测：使用 Ultra Face 检测人脸。
2. 人脸编码：使用 FaceNet 对检测到的人脸进行编码。
3. 人脸比较：将编码的人脸与现有数据库进行比较以标记出勤率
4.使用的库：OpenCV 和 FAISS。
5.来源：CCTV摄像机镜头。

考勤系统说明：
当一个人走到摄像机前时，系统使用Ultra Face检测人脸，并使用FaceNet进行编码。然后将编码的人脸与现有数据库进行比较。如果相似度（余弦相似度）小于0.25，则标记出勤。
问题：
最初，数据库中的人数少于100人，比较时间是可以接受的。随着人数的增加，比较时间明显变长。每个人在数据库中都有5张图片。为了加快比较速度，我改用FAISS库。虽然FAISS显著缩短了比较时间，但也增加了误报（错误地标记出勤）。
人脸比较的旧方法：
for db_name, db_encode in encoding_dict.items():
尝试：
dist = cosine(db_encode, f_e[1])
除 ValueError 为 e 外：
print(&quot;&gt;&gt;&gt;&gt;&gt;&gt; : &quot;,f_e[1],&quot;\n&quot;,type(f_e[1]))
继续
if dist &lt;识别_t：
name = db_name
distance = dist

cv2.rectangle(img, (f_e[0][0], f_e[0][1]), (f_e[0][2], f_e[0][3]), (0, 255, 0), 1)
cv2.putText(img, f&#39;{name}:{distance - 1:.2f}&#39;, (f_e[0][0], f_e[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

使用 FAISS 的新方法：
class StaffCustManagement：
def __init__(self, staff_n_neighbours=4, identification_t=0.80):
self.staff_db：Custom_DB = Custom_DB（db_name =“mydatabase”，col_name =“staff”）
self.staff_names，self.staff_encodings = self.staff_load_encodings（）
self.staff_n_neighbours：int = staff_n_neighbours
self.staff_ini_faiss（）
self.recognition_t：float = identification_t

def staff_load_encodings（self） -&gt; Tuple[List[str], List[np.ndarray]]:
staff_names, staff_encodings = [], []
for document in self.staff_db.find_all_data():
staff_names.append(document[&#39;_id&#39;])
staff_encodings.append(ArrayEncDec.decode_from_base64(b64_str=document[&#39;encoding&#39;]))
return staff_names, staff_encodings

def staff_ini_faiss(self):
if self.staff_names and self.staff_encodings:
Dimensions = 128
self.staff_index_faiss = faiss.IndexFlatL2(dimensions)
faiss_embeddings = np.array(self.staff_encodings, dtype=&#39;float32&#39;)
faiss.normalize_L2(faiss_embeddings)
self.staff_index_faiss.add(faiss_embeddings)

def find_staff_cust(self, current_encode: np.ndarray) -&gt; Tuple[str, float]:
name = &quot;Unknown&quot;
distance = float(&quot;inf&quot;)
if len(self.staff_names) == 0:
return name, distance
target_rep = np.expand_dims(current_encode, axis=0)
# faiss.normalize_L2(target_rep)
distances, neighbours = self.staff_index_faiss.search(target_rep, self.staff_n_neighbours)
print(&quot;Distances&quot;, distances)
print(&quot;neighbors&quot;, neighbours)
if distances[0][0] &gt;= self.recognition_t:
return self.staff_names[neighbors[0][0]].split(&#39;-&#39;)[0], distances[0][0]
return name, distance

问题：
如何在使用 FAISS 进行人脸比较时减少误报我的出勤跟踪系统如何做到这一点？虽然 FAISS 大大缩短了比较时间，但准确性却受到影响，导致出勤标记不正确。是否有任何最佳实践或替代方法可以在大型数据库中保持高精度？]]></description>
      <guid>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</guid>
      <pubDate>Fri, 12 Jul 2024 10:33:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 pytorch 计算特征中心时 GPU 内存不足</title>
      <link>https://stackoverflow.com/questions/63734506/the-gpu-memory-is-not-enough-when-computing-feature-centers-using-pytorch</link>
      <description><![CDATA[使用 pytorch 时，我想在损失函数中使用类特征中心。但是当我计算中心时，GPU 内存不够。我该如何解决？代码如下：
for epoch in range((args.start_epoch+1), args.epochs):
Center= computer_Center(model,dataloader, classnum)
for input, target in train_loader:
target = target.cuda()
input = input.cuda()
input_var = torch.autograd.Variable(input)
target_var = torch.autograd.Variable(target)
outputs, feature = model(input_var)
l = criterion(feature,target_var, Center) .forward()
l.backward(retain_graph=True)

def computer_Center(model,dataloader, classnum):
model.train()
for i in range(classnum):
j=0
for input,target in dataloader:
target=target.cuda()
input = input.cuda()
input_var = torch.autograd.Variable(input)
target=torch.autograd.Variable(target)
_, feature_ext = model(input_var)
ind=torch.where(target==i)[0]
if ind.shape[0]&gt;0:
if j==0:
feature_mid = feature_ext[ind, :]
feature_sum_mid=feature_mid.sum(0)
else:
feature_mid = feature_ext[ind, :]
feature_sum_mid = feature_sum_mid+feature_mid.sum(0)
j=j+1

feature_sum_mid=feature_sum_mid.unsqueeze(0)
if i==0:
feature_sum=feature_sum_mid
else:
feature_sum=torch.cat([feature_sum,feature_sum_mid],dim=0)

Center=feature_sum
for i in range(7):
Center[i,:]=feature_sum[i,:]/ClaSamNum[i]

返回中心
]]></description>
      <guid>https://stackoverflow.com/questions/63734506/the-gpu-memory-is-not-enough-when-computing-feature-centers-using-pytorch</guid>
      <pubDate>Fri, 04 Sep 2020 03:37:49 GMT</pubDate>
    </item>
    <item>
      <title>如何在基于 TensorFlow Lite 对象检测 Android 的应用程序中添加文本转语音功能？</title>
      <link>https://stackoverflow.com/questions/62623547/how-can-i-add-text-to-speech-in-tensorflow-lite-object-detection-android-based-a</link>
      <description><![CDATA[我正在尝试构建一款应用，帮助视障人士检测路上的物体​​/障碍物。因此，一旦检测到物体，使用 TensorFlow 库和 Android 文本转语音，应用程序就会让用户知道该物体是什么。我目前正在尝试构建 TensorFlow 提供的 Android 对象检测示例，但我很难找到边界框标签字符串的存储位置，以便在运行文本转语音时调用它]]></description>
      <guid>https://stackoverflow.com/questions/62623547/how-can-i-add-text-to-speech-in-tensorflow-lite-object-detection-android-based-a</guid>
      <pubDate>Sun, 28 Jun 2020 13:59:07 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：（'无法识别的关键字参数：'，dict_keys（['ragged']））</title>
      <link>https://stackoverflow.com/questions/61934272/valueerror-unrecognized-keyword-arguments-dict-keysragged</link>
      <description><![CDATA[我在 google colab 上训练了一个细胞分割模型，一个使用 tensorflow 版本 1.x，另一个使用 2.x。我下载了这个模型，现在正尝试加载它并预测我下载的预选图像。训练和建立模型，我用的是 tf.keras.layers.layername 代码如下：
import numpy as np
import cv2
import tensorflow as tf
import keras
from tensorflow.python.keras.models import load_model
from tensorflow.python.keras.utils import CustomObjectScope
from tensorflow.python.keras.initializers import glorot_uniform

def prepare(filepath):
IMG_SIZE = 100
img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)
new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 1)

#def load_model():
with CustomObjectScope({&#39;GlorotUniform&#39;:glorot_uniform()}):
model = tf.keras.models.load_model(&quot;./models/nuclei_1-15.h5&quot;, compile=False)

def predict():
prediction = model.predict([prepare(&quot;./images/cell.jpeg&quot;)])
print(prediction)

predict()

出现以下错误：
回溯（最近一次调用）：
文件“model_werk.py”，第 20 行，位于 &lt;module&gt;
model = tf.keras.models.load_model(&quot;./models/nuclei_1-15.h5&quot;, compile=False)
文件“/home/hmrbcnt/Documents/thesis/try_2.1.5/trial/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/models.py”，第 246 行，位于 load_model
model = model_from_config(model_config, custom_objects=custom_objects)
文件“/home/hmrbcnt/Documents/thesis/try_2.1.5/trial/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/models.py”，第 324 行，位于 model_from_config
返回 layer_module.deserialize(config, custom_objects=custom_objects)
文件“/home/hmrbcnt/Documents/thesis/try_2.1.5/trial/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/serialization.py”，第 63 行，位于 deserialize
printable_module_name=&#39;layer&#39;)
文件“/home/hmrbcnt/Documents/thesis/try_2.1.5/trial/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/utils/generic_utils.py”，第 164 行，位于 deserialize_keras_object
list(custom_objects.items())))
文件“/home/hmrbcnt/Documents/thesis/try_2.1.5/trial/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py”，第 975 行，位于 from_config
process_layer(layer_data)
文件“/home/hmrbcnt/Documents/thesis/try_2.1.5/trial/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py”，第 961 行，位于 process_layer
layer = deserialize_layer(layer_data, custom_objects=custom_objects)
文件“/home/hmrbcnt/Documents/thesis/try_2.1.5/trial/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/layers/serialization.py”，第63，在 deserialize 中
printable_module_name=&#39;layer&#39;)
文件“/home/hmrbcnt/Documents/thesis/try_2.1.5/trial/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/utils/generic_utils.py”，第 166 行，在 deserialize_keras_object 中
return cls.from_config(config[&#39;config&#39;])
文件“/home/hmrbcnt/Documents/thesis/try_2.1.5/trial/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py”，第 483 行，在 from_config 中
return cls(**config)
文件“/home/hmrbcnt/Documents/thesis/try_2.1.5/trial/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py”，第 524 行，在 __init__ 中
raise ValueError(&#39;无法识别的关键字参数：&#39;, kwargs.keys())
ValueError: (&#39;无法识别的关键字参数：&#39;, dict_keys([&#39;ragged&#39;]))


我使用 tensorflow.python.keras 而不是 tensorflow.keras 的原因是使用 tensorflow.keras 会返回 ModuleNotFound 错误。
tensorflow 的版本是 1.5，keras 是 2.2.4。我不能使用高于 1.5 的 tensorflow 版本，因为我的 CPU 很旧，它不支持 avx 指令。
任何帮助都将不胜感激。谢谢！
编辑：更改了错误和一行代码，因为我忽略了一些东西，现在得到了类似但不同的错误]]></description>
      <guid>https://stackoverflow.com/questions/61934272/valueerror-unrecognized-keyword-arguments-dict-keysragged</guid>
      <pubDate>Thu, 21 May 2020 12:12:38 GMT</pubDate>
    </item>
    <item>
      <title>选择权重进行加权损失计算的逻辑是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/59433485/logic-behind-choosing-weight-for-weighted-loss-calculation</link>
      <description><![CDATA[在计算加权 S 型交叉熵损失时，或者在数据集不平衡的情况下，选择权重的一般逻辑是什么？问题领域基于视觉/图像分类。]]></description>
      <guid>https://stackoverflow.com/questions/59433485/logic-behind-choosing-weight-for-weighted-loss-calculation</guid>
      <pubDate>Sat, 21 Dec 2019 04:26:57 GMT</pubDate>
    </item>
    <item>
      <title>如何从 vgg19 中删除自适应平均池层？</title>
      <link>https://stackoverflow.com/questions/59269140/how-to-remove-the-adaptive-average-pool-layer-from-vgg19</link>
      <description><![CDATA[我已加载 vgg19 预训练模型。如何删除分类器之前的自适应平均池化层？]]></description>
      <guid>https://stackoverflow.com/questions/59269140/how-to-remove-the-adaptive-average-pool-layer-from-vgg19</guid>
      <pubDate>Tue, 10 Dec 2019 14:10:12 GMT</pubDate>
    </item>
    <item>
      <title>对坐标数据进行非规范化 Tensorflow、Tflite - Python</title>
      <link>https://stackoverflow.com/questions/58539579/denormalize-coordinate-data-tensorflow-tflite-python</link>
      <description><![CDATA[我目前正在开发一款物体检测应用，该应用能够检测轮胎是否损坏。为此，我使用了 Google 的 AutoML edge，它可以导出 TFlite 模型。现在我想在我的代码中实现这个模型，但显然它预测的坐标是标准化的，而我却无法对其进行非标准化处理
在这里查看我的代码：
import tensorflow as tf
import numpy as np
import cv2

MODEL_PATH = &#39;Resources/model_v1_OD.tflite&#39;
LABEL_PATH = &#39;Resources/model_v1_OD.txt&#39;

class TFTireModel():
labels = []
intepreter = None
input_details = []
output_details = []
height = 0
width = 0

def __init__(self):
with open(LABEL_PATH, &#39;r&#39;) as f:
self.labels = [line.strip() for line in f.readlines()]

# 初始化 TFlite 解释器
self.interpreter = tf.lite.Interpreter(model_path=MODEL_PATH)
self.interpreter.allocate_tensors()

# 获取输入和输出张量。
self.input_details = self.interpreter.get_input_details()
self.output_details = self.interpreter.get_output_details()
# 获取输入尺寸
self.height = self.input_details[0][&#39;shape&#39;][1]
self.width = self.input_details[0][&#39;shape&#39;][2]

def predict(self, img, Threshold=0.3):
# 将图像调整为输入尺寸
img = cv2.resize(img, (self.width, self.height))
img = np.expand_dims(img, axis=0)
img = (2.0 / 255.0) * img - 1.0
img = img.astype(&#39;uint8&#39;)

# 预测图像
self.interpreter.set_tensor(self.input_details[0][&#39;index&#39;], img)
self.interpreter.invoke()

# 获取结果
boxes = self.interpreter.get_tensor(
self.output_details[0][&#39;index&#39;])
print(f&quot;boxes: {boxes}&quot;)

classes = self.interpreter.get_tensor(
self.output_details[1][&#39;index&#39;])

scores = self.interpreter.get_tensor(
self.output_details[2][&#39;index&#39;])

num = self.interpreter.get_tensor(
self.output_details[3][&#39;index&#39;])

# 获取输出
output =self._boxes_coordinates(boxes=np.squeeze(boxes[0]),
classes=np.squeeze(classes[0]+1).astype(np.int32),
scores=np.squeeze(scores[0]),
im_width=self.width,
im_height=self.height,
min_score_thresh=threshold)

print(f&quot;output: {output}&quot;)

# 格式化输出
返回输出

def _boxes_coordinates(self,
boxes,
classes,
scores,
im_width,
im_height,
max_boxes_to_draw=4,
min_score_thresh=0.4):

print(f&quot;width: {im_width}, height {im_height}&quot; )
if not max_boxes_to_draw:
max_boxes_to_draw = boxes.shape[0]
number_boxes = min(max_boxes_to_draw, boxes.shape[0])
tire_boxes = []
# person_labels = []
for i in range(number_boxes):
if scores is None or scores[i] &gt; min_score_thresh:
box = tuple(boxes[i].tolist())
ymin, xmin, ymax, xmax = box
xmin, ymin, xmax, ymax = (int(xmin * im_width), int(xmax * im_width), int(ymin * im_height), int(ymax * im_height)) #TODO：做一个循环

#tire_boxes.append([(ymin, xmin, ymax, xmax), scores[i], self.labels[classes[i]]]) #更完整
tire_boxes.append((xmin, ymin, xmax, ymax))
return tire_boxes

出错的地方：
 boxes = self.interpreter.get_tensor(
self.output_details[0][&#39;index&#39;])
print(f&quot;盒子：{盒子}&quot;

盒子：[[[ 0.00263482 0.50020593 0.3734043 0.83953816]
[ 0.12580797 0.14952084 0.65327024 0.61710536]
[ 0.13584864 0.38896233 0.6485662 0.85324436]
[ 0.31914377 0.3945622 0.87147605 0.8458656 ]
[ 0.01334581 0.03666234 0.46443292 0.55461186]
[ 0.1018104 -0.08279537 0.6541427 0.37984413]

由于此处的输出已标准化，我不知道如何对其进行非标准化。所需的输出是宽度和高度的百分比，如 _boxes_coordinates 函数中所示。]]></description>
      <guid>https://stackoverflow.com/questions/58539579/denormalize-coordinate-data-tensorflow-tflite-python</guid>
      <pubDate>Thu, 24 Oct 2019 10:42:04 GMT</pubDate>
    </item>
    <item>
      <title>获取 5 个随机裁剪 - TypeError：pic 应为 PIL Image 或 ndarray。获取 <type ‘tuple’></title>
      <link>https://stackoverflow.com/questions/55323821/getting-5-random-crops-typeerror-pic-should-be-pil-image-or-ndarray-got-typ</link>
      <description><![CDATA[我对图像进行如下转换（与 RandCrop 配合使用）：（来自此数据加载器脚本：https://github.com/jeffreyhuang1/two-stream-action-recognition/blob/master/dataloader/motion_dataloader.py）
def train(self):
training_set = motion_dataset(dic=self.dic_video_train, in_channel=self.in_channel, root_dir=self.data_path,
mode=‘train’,
transform = transforms.Compose([
transforms.Resize([256,256]),
transforms.FiveCrop([224, 224]),
#transforms.RandomCrop([224, 224]),
transforms.ToTensor(),
#transforms.Normalize([0.5], [0.5])
]))
打印‘==&gt;训练数据：’，len(training_set)，’ 视频’，training_set[1][0].size()

train_loader = DataLoader(
dataset=training_set, 
batch_size=self.BATCH_SIZE,
shuffle=True,
num_workers=self.num_workers,
pin_memory=True
)

return train_loader

但是当我尝试获取 Five Crops 时，我收到此错误：
回溯（最近一次调用最后一次）：
文件“motion_cnn.py”，第 267 行，在 
main()
文件“motion_cnn.py”，第 51 行，在 main
train_loader,test_loader, test_video = data_loader.run()
文件“/media/d/DATA_2/two-stream-action-recognition-master/dataloader/motion_dataloader.py”，第 120 行，正在运行
train_loader = self.train()
文件“/media/d/DATA_2/two-stream-action-recognition-master/dataloader/motion_dataloader.py”，第 156 行，正在训练
print ‘==&gt;训练数据：’,len(training_set),’ videos’,training_set[1][0].size()
文件“/media/d/DATA_2/two-stream-action-recognition-master/dataloader/motion_dataloader.py”，第 77 行，在 getitem 中
data = self.stackopf()
文件“/media/d/DATA_2/two-stream-action-recognition-master/dataloader/motion_dataloader.py”，第 51 行，在 stackopf 中
H = self.transform(imgH)
文件“/media/d/DATA_2/two-stream-action-recognition-master/venv/local/lib/python2.7/site-packages/torchvision/transforms/transforms.py”，第 60 行，在 call 中
img = t(img)
文件“/media/d/DATA_2/two-stream-action-recognition-master/venv/local/lib/python2.7/site-packages/torchvision/transforms/transforms.py”，第 91 行，在调用中
return F.to_tensor(pic)
文件“/media/d/DATA_2/two-stream-action-recognition-master/venv/local/lib/python2.7/site-packages/torchvision/transforms/ functional.py”，第 50 行，在 to_tensor 中
raise TypeError(‘pic 应为 PIL 图像或 ndarray。得到 {}’.format(type(pic)))
TypeError：pic 应为 PIL 图像或 ndarray。得到 &lt;type ‘tuple’&gt;

获取 5 个随机裁剪，我应该处理一组图像而不是 PIL 图像 - 所以我使用 Lambda，但随后我得到了错误，在
 第 55 行，在 stackopf
flow[2*(j),:,:] = H
RuntimeError: expand(torch.FloatTensor{[5, 1, 224, 224]}, size=[224,224]): 提供的尺寸数量 (2) 必须大于或等于张量中的维数 (4)

当我尝试设置 flow = torch.FloatTensor(5, 2*self.in_channel,self.img_rows,self.img_cols)
我得到了
 motion_dataloader.py&quot;，第 55 行，在 stackopf 中
flow[:,2*(j),:,:] = H
RuntimeError: expand(torch.FloatTensor{[5, 1, 224, 224]}, size=[5, 224, 224]): 提供的尺寸数量 (3) 必须大于或等于张量中的维数 (4)

当我将返回的训练批次大小乘以 5 时，我也得到了同样的错误。]]></description>
      <guid>https://stackoverflow.com/questions/55323821/getting-5-random-crops-typeerror-pic-should-be-pil-image-or-ndarray-got-typ</guid>
      <pubDate>Sun, 24 Mar 2019 12:31:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 Android 相机进行人脸检测。？</title>
      <link>https://stackoverflow.com/questions/52826977/face-detection-using-android-camera</link>
      <description><![CDATA[是否可以使用手机摄像头扫描护照图像并保存其详细信息，然后使用 Android 手机摄像头检测该人并获取其详细信息？（人脸检测）]]></description>
      <guid>https://stackoverflow.com/questions/52826977/face-detection-using-android-camera</guid>
      <pubDate>Tue, 16 Oct 2018 02:08:22 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络总是输出一个类</title>
      <link>https://stackoverflow.com/questions/37483671/convolutional-neural-net-always-ouputs-one-class</link>
      <description><![CDATA[我尝试使用 matconvnet 包在 matlab 中微调 imagenet-vgg-f 网络。它只能对两个类进行分类，而不是 1000 个类。
如果我只将最后一层的输出神经元从 1000 个改为 2 个，那么一切都会顺利进行。但是，如果我在最后一层前面添加一个（或多个）额外的（完全连接的）层，网络就不会收敛。它以 0.369 的评估误差开始，并在所有训练阶段保持在该值。事实证明，0.369 恰好是我数据集中 2 类 图像的比例。所以网络总是显示 1 类。
net= load(&#39;imagenet-vgg-f.mat&#39;) ;
f=1/100; 

net.layers{end-1} = struct(&#39;type&#39;, &#39;conv&#39;, ....
&#39;weights&#39;, {{f*randn(1,1,4096,24, &#39;single&#39;), zeros(1, 24, &#39;single&#39;)}}, ...
&#39;stride&#39;, 1, ...
&#39;pad&#39;, 0, ...
&#39;name&#39;, &#39;fc8&#39;) ;
net.layers{end}=struct(&#39;type&#39;, &#39;relu&#39;) ;

net.layers{end+1}=struct(&#39;type&#39;, &#39;conv&#39;, ... 
&#39;weights&#39;, {{f*randn(1,1,24,2, &#39;single&#39;), zeros(1, 2,&#39;single&#39;)}}, ...
&#39;stride&#39;, 1, ...
&#39;pad&#39;, 0, ...
&#39;name&#39;, &#39;fc8&#39;) ;

net.layers{end+1} = struct(&#39;type&#39;, &#39;softmaxloss&#39;) ;

我正在使用部署的 cnn_train.m 进行训练，使用动量 0.9 和学习率为 .001。更改这些参数无助于启动学习过程。我还尝试了 sigmoid 激活 fcts 而不是 relu，但这也无济于事。
知道我做错了什么吗？特别是考虑到当我不从 4096 到 24 再到 2 而是直接从 4096 到 2 个神经元时，一切都运行正常。
提前谢谢
乔纳斯]]></description>
      <guid>https://stackoverflow.com/questions/37483671/convolutional-neural-net-always-ouputs-one-class</guid>
      <pubDate>Fri, 27 May 2016 12:31:21 GMT</pubDate>
    </item>
    <item>
      <title>检测纸上的符号</title>
      <link>https://stackoverflow.com/questions/11727485/detect-symbol-on-paper</link>
      <description><![CDATA[我想检测用户在纸上绘制的符号。文档将指定此符号，因此每个用户都会绘制相同的符号，但由于每个用户的笔迹不同，因此当然会存在差异。

我应该选择哪种符号？哪种符号易于识别，但也能使检测容忍微小修改（每个用户的手写）？
我应该使用哪种模式匹配方法/算法来检测文档图像中的符号？
]]></description>
      <guid>https://stackoverflow.com/questions/11727485/detect-symbol-on-paper</guid>
      <pubDate>Mon, 30 Jul 2012 18:34:44 GMT</pubDate>
    </item>
    </channel>
</rss>