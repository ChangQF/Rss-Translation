<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 07 Apr 2024 18:15:54 GMT</lastBuildDate>
    <item>
      <title>关闭选项卡后如何保持 Paperspace 的渐变笔记本运行</title>
      <link>https://stackoverflow.com/questions/78288786/how-do-i-keep-paperspaces-gradients-notebooks-running-after-i-close-my-tabs</link>
      <description><![CDATA[我正在纸空间的渐变上运行笔记本。当我预订一台机器 4 小时并开始在其中运行 jupyter 笔记本时，如果我关闭浏览器，执行就会停止。我怎样才能改变这种行为？我在 google collab pro 上没有遇到这个问题
我尝试了免费和付费 GPU 机器，并尝试升级到 Pro 帐户]]></description>
      <guid>https://stackoverflow.com/questions/78288786/how-do-i-keep-paperspaces-gradients-notebooks-running-after-i-close-my-tabs</guid>
      <pubDate>Sun, 07 Apr 2024 17:58:26 GMT</pubDate>
    </item>
    <item>
      <title>澄清：模型评估 - Tran 和 Val 损失</title>
      <link>https://stackoverflow.com/questions/78288712/clarification-model-evaluation-tran-and-val-loss</link>
      <description><![CDATA[我不确定我的模型是否表现良好。我的理解是，如果我的模型在训练和验证损失之间进行调整，也表明没有欠拟合或过拟合。然而，我的教授不同意，并说我的模型没有经过充分的训练，这表明图表本身是不正确的。
我不确定问题出在哪里或者到底出了什么问题。有人可以提供一些指导或帮助吗？
我的模型的训练和 val 损失图
我通过在嵌入层中添加一些噪声来训练模型。
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/78288712/clarification-model-evaluation-tran-and-val-loss</guid>
      <pubDate>Sun, 07 Apr 2024 17:35:17 GMT</pubDate>
    </item>
    <item>
      <title>如何创建 CNN-LSTM 架构？</title>
      <link>https://stackoverflow.com/questions/78288542/how-to-create-cnn-lstm-architecture</link>
      <description><![CDATA[我尝试创建混合 CNN 和 LSTM 模型。我遇到了与架构形状相关的问题。这导致epoch无法跑完数据200次。
我的数据大小是（96,2）
错误：
纪元 1/200
    178/未知 9s 34ms/步 - 损耗：1.2366 - mse：5.4560
-------------------------------------------------- ------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
第 4 行 [40] 中的单元格
      2 is_train = True
      3 如果是_train：
----&gt; 4 model_create.fit（train_dataset，epochs = 200，batch_size = 128）

无法将张量添加到批次中：元素数量不匹配。形状为：[张量]：[78,2]，[批次]：[96,2]
     [[{{node IteratorGetNext}}]] [操作：__inference_one_step_on_iterator_23678]

CNN-LSTM模型：
def create_model_architecture():
    model_cnn = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;,
                               输入形状=输入数据形状），
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;),
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.LSTM(32, return_sequences=True),
        tf.keras.layers.LSTM(16),
        tf.keras.layers.Reshape((-1,16)),
        #tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）
    返回 model_cnn


编译模型
def create_model():
    tf.random.set_seed(51)

    model_create = create_model_architecture()
    #model_create = create_LSTM_model()
    model_create.compile(loss=tf.keras.losses.Huber(),
                  优化器=tf.keras.optimizers.Adam(learning_rate=0.001),
                  指标=[“mse”])
    返回模型_创建

模型创建 = 创建模型()

model_create.summary()

model_create.fit（train_dataset，epochs = 200，batch_size = 128）


我曾尝试在 flatten() 函数之前添加 reshape 来改变形状。我还减小了批量大小和纪元大小。这些都不起作用。如何将我的模型与 train_data 相匹配？]]></description>
      <guid>https://stackoverflow.com/questions/78288542/how-to-create-cnn-lstm-architecture</guid>
      <pubDate>Sun, 07 Apr 2024 16:34:57 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降：缩减特征集的运行时间比原始特征集更长</title>
      <link>https://stackoverflow.com/questions/78288109/gradient-descent-reduced-feature-set-has-a-longer-runtime-than-the-original-fea</link>
      <description><![CDATA[我尝试用 Python 实现梯度下降算法来解决机器学习问题。我正在使用的数据集已经过预处理，并且在比较两个数据集（一个具有原始特征，另一个数据集使用奇异值分解（SVD）减少了前一组的维度）时，我在运行时观察到了意外的行为。我始终如一观察到，与减少的数据集相比，较大的原始数据集的梯度下降算法的运行时间较低，这与我的预期相反。鉴于数据集较小，缩减后的数据集的运行时间是否应该更短？我试图理解为什么会发生这种情况。
以下是相关代码片段：
导入时间
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

def h_theta(X1, theta1):
    # 假设函数的实现
    返回 np.dot(X1, theta1)

def j_theta(X1, y1, theta1):
    # 成本函数的实现
    返回 np.sum((h_theta(X1, theta1) - y1) ** 2) / (2 * X1.size)

def grad(X1, y1, θ):
    # 梯度计算
    梯度 = np.dot(X1.T, h_theta(X1, theta) - y1) / len(y1)
    返回梯度

def 梯度下降(X1, y1):
    theta_initial = np.zeros(X1.shape[1]) # 用零初始化 theta
    迭代次数 = 1000
    学习率 = [0.1, 0.01, 0.001]
    成本迭代 = []
    θ值 = []
    开始 = 时间.time()
    对于 Learning_rates 中的 alpha：
        theta = theta_initial.copy()
        成本历史 = []
        对于范围内的 i(num_iterations)：
            梯度 = grad(X1, y1, θ)
            theta = theta - np.dot(alpha, 梯度)
            成本 = j_theta(X1, y1, θ)
            cost_history.append(成本)
        cost_iterations.append(cost_history)
        theta_values.append(theta)
    结束 = 时间.time()
    print(f&quot;所用时间：{end - start} 秒&quot;)
    图, axs = plt.subplots(len(learning_rates), Figsize=(8, 15))
    对于 i，枚举中的 alpha（学习率）：
        axs[i].plot(范围(num_iterations), cost_iterations[i], label=f&#39;alpha = {alpha}&#39;)
        axs[i].set_title(f&#39;学习率：{alpha}&#39;)
        axs[i].set_ylabel(&#39;成本 J&#39;)
        axs[i].set_xlabel(&#39;迭代次数&#39;)
        axs[i].legend()
    plt.tight_layout()
    plt.show()

# 使用 SVD 将 X 减少到 3 个特征（列）的代码：
# 对 X 进行奇异值分解并将其减少到 3 列
U、S、Vt = np.linalg.svd(X_归一化)
# 将 X 减少到 3 列
X_reduced = np.dot(X_normalized, Vt[:3].T)

# 打印 X_reduced 的前 5 行
print(&quot;X_reduced 的前 5 行：&quot;)
# 标准化 X_reduced
X_reduced = (X_reduced - np.mean(X_reduced, axis=0)) / np.std(X_reduced, axis=0)

print(&quot;减少和归一化后 X 的均值和标准差：\n&quot; ,X_reduced.mean(axis=0), X_reduced.std(axis=0))
# 打印缩小后的 X 的形状以确认它只有 3 个特征
print(&quot;X_reduced 的形状：&quot;, X_reduced.shape)

# 将截距列添加到 X_reduced
X_reduced_with_intercept = np.hstack((intercept_column, X_reduced))


# 用法示例
# X_normalized_with_intercept 和 y_normalized 表示原始数据集
# X_reduced_with_intercept 和 y_normalized 表示缩减后的数据集

# 对原始数据集进行梯度下降
梯度下降（X_normalized_with_intercept，y_normalized）

# 对缩减后的数据集执行梯度下降
梯度下降（X_reduced_with_intercept，y_归一化）

在我的梯度下降实现中，与完整数据集相比，什么可能导致缩减数据集始终具有更长的运行时间？任何有关故障排除的见解或建议将不胜感激。
我尝试重写和审查我的实现，但似乎对于大多数学习率和迭代次数的增加，较大功能集的运行时间低于其 SVD 子集。]]></description>
      <guid>https://stackoverflow.com/questions/78288109/gradient-descent-reduced-feature-set-has-a-longer-runtime-than-the-original-fea</guid>
      <pubDate>Sun, 07 Apr 2024 14:13:17 GMT</pubDate>
    </item>
    <item>
      <title>内核形状必须与输入具有相同的长度，但接收形状为 (3, 3, (None, 7, 7, 512), 64) 的内核和形状为 [(None, 7, 7, 512)] 的输入</title>
      <link>https://stackoverflow.com/questions/78287794/kernel-shape-must-have-the-same-length-as-input-but-received-kernel-of-shape-3</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78287794/kernel-shape-must-have-the-same-length-as-input-but-received-kernel-of-shape-3</guid>
      <pubDate>Sun, 07 Apr 2024 12:27:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pytorch 进行高效的成对采样</title>
      <link>https://stackoverflow.com/questions/78287754/efficient-pair-sampling-with-pytorch</link>
      <description><![CDATA[我有一个包含大约 150k 图像和 150k 音频的数据集。每个图像，都有相应的音频。我希望我的网络能够学习使用图像数据将一个音频映射到另一个音频。
当我创建这些源目标对时，我的目标是从训练数据的子集中采样的，并且还必须满足特定的阈值标准。然后创建源目标对进行训练。
我现在这样做的方式是导出目标采样的子集，并在自定义数据集的 getitem() 中应用阈值。这花费了太长的时间并在训练过程中造成了瓶颈。
我不确定如何解决这个问题并加快训练速度。我该如何解决这个问题。
def _getitem_(idx):
   sourceimg、sourceaudio = self.data[idx]
   Target_dataset = create_subset(sourceimg, sourceaudio)
   迭代次数 = 0
   虽然正确：
     targetimg, targetaudio = np.random(Target_dataset)
     如果 cal_value(target_img) &gt;= 阈值：
       返回sourceimg、sourceaudio、targetimg、targetaudio
     迭代+=1
     如果迭代次数 &gt;= 10：
       _getitem_(random.randint(0, len(self.dataset))

我在训练期间尝试过这样做。但就我拥有的数据量而言，这根本没有效率。如何在不增加数据加载器中的批处理大小或 num_worker 的情况下加快训练速度]]></description>
      <guid>https://stackoverflow.com/questions/78287754/efficient-pair-sampling-with-pytorch</guid>
      <pubDate>Sun, 07 Apr 2024 12:16:21 GMT</pubDate>
    </item>
    <item>
      <title>如何结合CNN-LSTM架构？</title>
      <link>https://stackoverflow.com/questions/78287568/how-to-combine-cnn-lstm-architecture</link>
      <description><![CDATA[CNN和LSTM的架构代码
我想创建 CNN-LSTM 。我在 keras 中遇到了 Flatten() 问题，形状不匹配导致模型无法运行所有数据。
数据形状 = (96,2)
def create_model_architecture():
    model_cnn = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;,
                               输入形状=输入数据形状），
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;),
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.LSTM(32, return_sequences=True),
        tf.keras.layers.LSTM(16),
        tf.keras.layers.Reshape((-1,16)),
        #tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）
    返回 model_cnn

def create_model():
    tf.random.set_seed(51)

    model_create = create_model_architecture()
    #model_create = create_LSTM_model()
    model_create.compile(loss=tf.keras.losses.Huber(),
                  优化器=tf.keras.optimizers.Adam(learning_rate=0.001),
                  指标=[“mse”])
    返回模型_创建

模型创建 = 创建模型()

model_create.summary()


model_create.fit（train_dataset，epochs=5，batch_size=128）

错误：
&lt;前&gt;&lt;代码&gt;
纪元 1/5
    178/未知 9s 40ms/步 - 损耗：3.5391 - mse：18.2760
-------------------------------------------------- ------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
[213] 中的单元格，第 4 行
      2 is_train = True
      3 如果是_train：
----&gt; 4 model_create.fit（train_dataset，epochs=5，batch_size=128）
      5 #其他：
      6 #model.fit(train_dataset,epochs =200,batch_size = 512)
无法将张量添加到批次中：元素数量不匹配。形状为：[张量]：[85,2]，[批次]：[96,2]
     [[{{node IteratorGetNext}}]] [操作：__inference_one_step_on_iterator_64945]


为了修复 Flatten()，我尝试使用 Reshape()，但是模型拟合无法运行 5 个时期。如何修复形状以使模型拟合运行 5 个时期？]]></description>
      <guid>https://stackoverflow.com/questions/78287568/how-to-combine-cnn-lstm-architecture</guid>
      <pubDate>Sun, 07 Apr 2024 11:06:09 GMT</pubDate>
    </item>
    <item>
      <title>如何在不始终拥有用户 ID 的情况下根据多个标准推荐增强功能？</title>
      <link>https://stackoverflow.com/questions/78287221/how-to-recommend-enhancements-based-on-multiple-criteria-without-always-having-u</link>
      <description><![CDATA[我正在开发一个推荐系统，该系统根据按摩名称、服务长度、中心名称以及偶尔的用户详细信息向用户建议增强功能（附加服务）。但是，我遇到了一些需要帮助的挑战。
涉及的数据集结构如下：
用户数据集：包含 USER_ID、AGE、GENDER、ZIPCODE 和 BASE_CENTER。
项目数据集：包含ITEM_ID和ITEM_NAME（代表增强）。
交互数据集：包括 USER_ID、ITEM_ID、TIMESTAMP、SERVICE_LENGTH、MASSAGE_NAME、CENTER_NAME 和 EVENT_TYPE。
我的系统的一个值得注意的方面是，每个增强功能都可以与多个按摩名称和中心名称相关联。以下是我面临的具体问题：
用户 ID 缺失：在很多情况下，我没有 USER_ID。在推荐过程中处理此类情况的最佳实践是什么？我应该默认使用通用用户配置文件，还是有更复杂的方法来保持个性化而无需用户识别？
不同中心的建议不一致：我发现不同的中心有时会产生相同的增强建议，尽管我预计中心名称会影响建议的多样性。此外，用户年龄、性别和中心名称的变化似乎并没有像预期那样改变结果。
不同输入的相同结果：无论年龄、性别和中心名称输入如何变化，系统都倾向于推荐相同的增强功能。我正在寻找策略来使结果多样化并使建议对这些输入变量更加敏感。
问题：
如何改进我的推荐系统以有效处理没有用户 ID 的实例，确保一定程度的个性化？
我可以采用哪些策略或模型来使我的系统对用户详细信息（年龄、性别）和中心名称的变化更加敏感，从而提供更加多样化和相关的增强建议？
任何有关如何应对这些挑战的见解或建议将不胜感激。
尝试更改架构，但输出没有太大差异]]></description>
      <guid>https://stackoverflow.com/questions/78287221/how-to-recommend-enhancements-based-on-multiple-criteria-without-always-having-u</guid>
      <pubDate>Sun, 07 Apr 2024 08:59:41 GMT</pubDate>
    </item>
    <item>
      <title>OpenCV 新手，我如何安装/构建 opencv_traincascade</title>
      <link>https://stackoverflow.com/questions/78286577/new-to-opencv-how-do-i-install-build-opencv-traincascade</link>
      <description><![CDATA[所以我一直在从事机器学习项目，并且需要使用 opencv_traincascade 训练自定义数据集。但每当我尝试安装它时，它就永远无法工作。我还有其他东西，比如 opencv_annotation 和其他东西可以工作，但是 traincascade 或 event createsamples 不起作用。我必须手动构建这些吗？
我下载了mingw-gcc、cmake，在网上找不到可行的解决方案。顺便说一句，我有 opencv 4.9.0，手动安装在 anaconda 和我的 C: 驱动器中。我也尝试过寻找一些第三方，他们安装了整个 opencv 并且可以复制，但没有运气。任何帮助将不胜感激，谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78286577/new-to-opencv-how-do-i-install-build-opencv-traincascade</guid>
      <pubDate>Sun, 07 Apr 2024 03:46:34 GMT</pubDate>
    </item>
    <item>
      <title>所有模型的训练、验证集和测试集的 F1 分数、精确度和召回率均较高</title>
      <link>https://stackoverflow.com/questions/78286020/high-f1-score-precision-and-recall-on-training-validation-set-and-test-set-on</link>
      <description><![CDATA[我正在研究一个关于 Kaggle。有一些特征，例如燃料消耗、燃料等。我尝试根据该数据集进行分类任务，将排放量高于 255 定义为不可接受 (1)，低于可接受 (0)。数据不平衡，可接受类有13269个数据，不可接受类有9287个数据。我尝试使用不同的分类模型，例如随机森林分类器、决策树分类器和逻辑回归，所有这些模型在训练集、验证集甚至测试集上都实现了接近 1 的 f1 分数，这看起来很奇怪。数据没有缺失值或空值，并且在将其输入模型之前由标准定标器进行标准化。
在此处输入图像说明在此处输入图像描述
我的第一个假设是当所有模型都这样执行时数据泄漏。我多次检查了代码，甚至用函数检查了数据集，训练集和测试集之间没有重复的行。我尝试使用所有特征，然后使用随机森林发现最相关的一些特征，例如“COMB（L/100 km）”、“COMB（mpg）”、“燃油消耗”、“HWY（L/100 km）” 100 公里）”、“气缸”、“发动机尺寸”和他们玩了一下，但在所有精确度、召回率和 f1 上仍然获得了高分。我通过偶然从较大的类中删除一些数据来平衡数据集。我使用网格搜索制作模型，因此我尝试通过定义不同的参数网格以及手动定义来使模型更加复杂和简单。我还通过阈值检查了精度和召回率。
在此处输入图片说明
在此处输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/78286020/high-f1-score-precision-and-recall-on-training-validation-set-and-test-set-on</guid>
      <pubDate>Sat, 06 Apr 2024 21:57:10 GMT</pubDate>
    </item>
    <item>
      <title>神经网络对不同输入的相同预测</title>
      <link>https://stackoverflow.com/questions/78284988/neural-network-same-prediction-for-different-inputs</link>
      <description><![CDATA[我正在尝试在 Matlab 中构建一个神经网络，而不使用深度学习工具箱，其中一个隐藏层可以预测图像显示的是脑肿瘤还是健康的大脑。我使用的数据库包含 4000 张图像（2000 张脑肿瘤图像和 2000 张健康大脑图像）。
我面临的问题是准确率为 50%，并且每张图像的预测都是相同的。结果，混淆矩阵的一列始终为 0。我尝试更改学习率，尝试更改隐藏层上的神经元数量，但没有任何改变输出。我使用的隐藏层和输出层的激活函数都是 sigmoid，并且使用的优化算法是梯度下降。]]></description>
      <guid>https://stackoverflow.com/questions/78284988/neural-network-same-prediction-for-different-inputs</guid>
      <pubDate>Sat, 06 Apr 2024 15:52:17 GMT</pubDate>
    </item>
    <item>
      <title>在 Kaggle Notebook 中降级 Tensorflow 版本时遇到问题；我应该怎么办</title>
      <link>https://stackoverflow.com/questions/78279273/facing-problem-while-downgrading-tensorflow-version-in-kaggle-notebook-what-sho</link>
      <description><![CDATA[我在tensorflow versino 2.11.0中编写了一个代码，但是最近我的代码无法运行，发现当前的tensorflow版本2.15.0是主要问题，所以我使用代码降级了我的版本！pip install tensorflow- GPU==2.11.0
但是我的笔记本确实找到了任何 GPU，尽管我像以前一样在我的 Kaggle 笔记本中启用了 GPU P100 加速器。我还在代码中检查 GPU。
导入tensorflow为tf

如果 tf.test.gpu_device_name():

print(&#39;默认 GPU 设备：{}&#39;.format(tf.test.gpu_device_name()))

别的：

print(&quot;请安装GPU版本的TF&quot;)

得到了
&lt;前&gt;&lt;代码&gt;
    请安装GPU版本的TF


请在这方面帮助我。我的项目截止日期非常接近
在 Kaggle 笔记本中降级 Tensorflow 版本时遇到问题。]]></description>
      <guid>https://stackoverflow.com/questions/78279273/facing-problem-while-downgrading-tensorflow-version-in-kaggle-notebook-what-sho</guid>
      <pubDate>Fri, 05 Apr 2024 10:28:53 GMT</pubDate>
    </item>
    <item>
      <title>多元线性回归房价r2得分问题</title>
      <link>https://stackoverflow.com/questions/78275121/multiple-linear-regression-house-price-r2-score-problem</link>
      <description><![CDATA[我有样本房价数据和简单代码：
导入 pandas 作为 pd
从 sklearn.preprocessing 导入 LabelEncoder、StandardScaler
从 sklearn.model_selection 导入 train_test_split
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.metrics 导入 r2_score

数据 = pd.read_csv(&#39;house_price_4.csv&#39;)
df = pd.DataFrame(数据)
df[&#39;区域&#39;] = df[&#39;区域&#39;].str.replace(&#39;,&#39;, &#39;&#39;)
df = df.dropna()

# 对分类特征“地址”进行编码
df[&#39;地址&#39;] = df[&#39;地址&#39;].astype(&#39;类别&#39;).cat.codes
df[&#39;停车&#39;] = df[&#39;停车&#39;].replace({True: 1, False: 0})
df[&#39;仓库&#39;] = df[&#39;仓库&#39;].replace({True: 1, False: 0})
df[&#39;电梯&#39;] = df[&#39;电梯&#39;].replace({True: 1, False: 0})

X = df.drop(columns=[&#39;价格(美元)&#39;,&#39;价格&#39;])
y = df[&#39;价格&#39;]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

模型=线性回归()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

r_squared = r2_score(y_test, y_pred)
print(f&#39;R^2 得分: {r_squared:.4f}&#39;)

                                                                  

我的 R2 分数非常低：0.34
如何获得更高的 R2 分数？
这是我的示例数据：https://drive.google .com/file/d/14Se90XbGJivftq3_VrtgRSalkCplduVX/view?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/78275121/multiple-linear-regression-house-price-r2-score-problem</guid>
      <pubDate>Thu, 04 Apr 2024 16:01:38 GMT</pubDate>
    </item>
    <item>
      <title>ML ColumnTransformer OneHotEncoder</title>
      <link>https://stackoverflow.com/questions/78274904/ml-columntransformer-onehotencoder</link>
      <description><![CDATA[当在数据帧的第一列中转换分类数据时，我发现 ColumnTransformer 与 OneHotEncoder 出现奇怪的行为。当我向 csv 文件添加一行时，就会发生此行为。
初始数据为：
标题、每日总收入、影院、DayInYear
AC汀巴黎,307,5,257
给莫莫的一封信，307,5,257
生命的另一天,307,5,257
批准收养，307,5,257
四月与非凡的世界, 307,5,257
美女,307,5,257
鸟男孩被遗忘的孩子，307,5,257
奇科丽塔,307,5,257

运行代码时
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd

数据集 = pd.read_csv(&#39;../data/GKIDS_DayNum_test_names.csv&#39;)
数据集[&#39;标题&#39;].str.strip()
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

从 sklearn.compose 导入 ColumnTransformer
从 sklearn.preprocessing 导入 OneHotEncoder

title_column_index = dataset.columns.get_loc(&#39;标题&#39;)
print(&#39;标题索引：&#39;, title_column_index)
ct = ColumnTransformer(transformers=[(&#39;编码器&#39;, OneHotEncoder(), [title_column_index])], 剩余=&#39;passthrough&#39;)
X_Encoded = np.array(ct.fit_transform(X))
打印（X_编码）

结果是正确的：
&lt;前&gt;&lt;代码&gt;[[1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 307 5]]

但是，当我添加附加行时：BlueGiant,307,5,257
到文件并重新运行代码我得到奇怪的输出：
&lt;前&gt;&lt;代码&gt; (0, 0) 1.0
  (0, 9) 307.0
  (0, 10) 5.0
  (1, 1) 1.0
  (1, 9) 307.0
  (1, 10) 5.0
  (2, 2) 1.0
  (2, 9) 307.0
  (2, 10) 5.0
  (3, 3) 1.0
  (3, 9) 307.0
  (3, 10) 5.0
  (4, 4) 1.0
  (4, 9) 307.0
  (4, 10) 5.0
  (5, 5) 1.0
  (5, 9) 307.0
  (5, 10) 5.0
  (6, 6) 1.0
  (6, 9) 307.0
  (6, 10) 5.0
  (7, 8) 1.0
  (7, 9) 307.0
  (7, 10) 5.0
  (8, 7) 1.0
  (8, 9) 307.0
  (8, 10) 5.0

我不明白为什么会这样。
请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78274904/ml-columntransformer-onehotencoder</guid>
      <pubDate>Thu, 04 Apr 2024 15:25:57 GMT</pubDate>
    </item>
    <item>
      <title>我在 Gradio 部署后收到输出错误</title>
      <link>https://stackoverflow.com/questions/78058612/i-am-getting-output-error-after-gradio-deployment</link>
      <description><![CDATA[我正在使用 Gradio 部署机器学习模型，在 Gradio 上部署后，我在输入和输出显示错误后收到错误
此输出的代码是
导入gradio为gr
将 numpy 导入为 np
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.preprocessing 导入 StandardScaler、MinMaxScaler

# 示例数据（将其替换为您的实际数据）
X_train = np.array([[230.1, 37.8, 69.2],
                    [44.5、39.3、45.1]、
                    [17.2、45.9、69.3]、
                    [151.5、41.3、58.5]、
                    [180.8, 10.8, 58.4]])
y_train = np.array([22.1, 10.4, 9.3, 18.5, 12.9])

# 初始化并训练您的线性回归模型
缩放器 = MinMaxScaler()
缩放器.fit(X_train)
X_train_scale = 缩放器.transform(X_train)

lm = 线性回归()
lm.fit(X_train_scale, y_train)

# 定义预测函数
def Predict_sales(电视、广播、报纸):
    # 缩放输入特征
    input_features = scaler.transform([[电视、广播、报纸]])
    # 预测销量
    预测 = lm.predict(input_features)
    返回预测[0]

# 创建渐变界面
tv_input = gr.Number(标签=“电视”)
radio_input = gr.Number(label=&quot;Radio&quot;)
newspaper_input = gr.Number(label=“报纸”)
output_text = gr.Textbox(label=&quot;预测销售额&quot;)

gr.Interface(fn=predict_sales,
             输入=[电视输入、广播输入、报纸输入]、
             输出=输出文本，
             title=&quot;销售预测&quot;,
             description=&quot;输入广告费用以预测销售额&quot;,
            debug=True,enable_queue=True).launch()

]

如何解决这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/78058612/i-am-getting-output-error-after-gradio-deployment</guid>
      <pubDate>Mon, 26 Feb 2024 04:17:40 GMT</pubDate>
    </item>
    </channel>
</rss>