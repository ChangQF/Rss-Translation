<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 20 Mar 2024 00:57:01 GMT</lastBuildDate>
    <item>
      <title>如何优化这个二元分类 ML 模型以给出更小的损失？</title>
      <link>https://stackoverflow.com/questions/78190265/how-can-i-optimize-this-binary-classification-ml-model-to-give-smaller-loss</link>
      <description><![CDATA[我已经尝试优化这个模型有一段时间了，但我无法弄清楚如何减少损失和训练所需的时间。损失需要很长时间才能减少，并且不会低于 0.27（大约该值）。
这是我使用 Kaggle 上提供的泰坦尼克号数据集训练自己的第一个分类模型。
请帮忙！
#定义导入
进口火炬
从火炬导入 nn
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd
将 numpy 导入为 np
导入io

X= df2[[ &#39;年龄&#39;, &#39;性别&#39;, &#39;Sib_And_Parch&#39;, &#39;Pclass&#39;]].to_numpy()
X_train= torch.from_numpy(X).type(torch.float32).to(device)


def precision_fn(y_true,y_pred):
  正确 = torch.eq(y_true,y_pred).sum().item()
  acc = (正确/len(y_true))*100
  返回帐户

# 编写模型
类 SurvivalPredictor(nn.Module):
  def __init__(自身):
    超级().__init__()
    self.layer_1 = nn.Linear(in_features=4, out_features=16)
    self.layer_2 = nn.Linear(in_features=16, out_features=32)
    self.layer_3 = nn.Linear(in_features=32, out_features=8)
    self.layer_4 = nn.Linear(in_features=8, out_features=1)
    self.relu = nn.ReLU()

  def 前向（自身，x）：
    返回 self.layer_4(self.relu(self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x))))))))

model_0 = SurvivalPredictor().to(设备)


# 设置损失函数
loss_fn = nn.BCEWithLogitsLoss()

# 设置优化器
优化器= torch.optim.SGD(params= model_0.parameters(),
                          lr=0.001)

纪元=100001

对于范围内的纪元（纪元）：

  优化器.zero_grad()


  ＃训练
  model_0.train()

  #前向传递
  y_logits = model_0(X_train).squeeze()
  y_preds =torch.argmax(torch.sigmoid(y_logits)) # logits-&gt;预测概率 -&gt;预测标签

  # 计算损失
  损失 = loss_fn(y_logits,y_train)

  loss.backward()

  优化器.step()


  如果纪元％10000 == 0：
    print(f&quot;历元: {epoch} | 损失: {loss:.5f} &quot;)


]]></description>
      <guid>https://stackoverflow.com/questions/78190265/how-can-i-optimize-this-binary-classification-ml-model-to-give-smaller-loss</guid>
      <pubDate>Wed, 20 Mar 2024 00:25:23 GMT</pubDate>
    </item>
    <item>
      <title>在 Pandas 中逐行填充单元格子集的最佳方法</title>
      <link>https://stackoverflow.com/questions/78189781/best-way-to-fill-a-subset-of-cells-row-by-row-in-pandas</link>
      <description><![CDATA[我正在运行半监督学习的超参数优化问题。每行都是用于标记未标记数据的模型置信度截止值。每一列都是一个迭代。每个单元格都是模型测试数据的准确性。 pandas 执行此操作的最佳实践是什么？
我应该将截止保留为列还是索引吗？
如果是列，我应该如何向截止列以外的单元格添加一行精度？例如类似于： df_test_results.loc[[“Iter_1_acc”, “Iter_2_acc”...]] = [ac] * len(iteration_names)
如果有索引，如何附加具有有效索引的新行？比如说，之前的所有索引都是数字 - range(0.3, 1, 0.05) 但我想添加最后一行“no_relabeling”。
concat、insert 需要重置索引或组成自己的索引，因此我无法再通过截止值进行引用。使用 loc tp 创建带有索引的行似乎一度有效，但不再有效：
df_test_results.loc[[&#39;no_relabel&#39;]] = [acc_test_ignore] * 迭代次数

示例数据框：
Iter_1_acc Iter_2_acc Iter_3_acc Iter_4_acc Iter_5_acc
0.45 0.96296 0.96296 0.96296 0.96296 0.96296
0.50 0.96296 0.96296 0.96296 0.96296 0.96296
0.55 0.96296 0.96296 0.96296 0.96296 0.96296
]]></description>
      <guid>https://stackoverflow.com/questions/78189781/best-way-to-fill-a-subset-of-cells-row-by-row-in-pandas</guid>
      <pubDate>Tue, 19 Mar 2024 21:42:01 GMT</pubDate>
    </item>
    <item>
      <title>通过标准定标器实现前瞻偏差？</title>
      <link>https://stackoverflow.com/questions/78189717/look-ahead-bias-via-standardscaler</link>
      <description><![CDATA[我非常确定存在前瞻偏差，因为标准缩放器之前看到过测试数据。但事情是这样的，我用大量不同的配置和参数设置运行了这个，它们的表现都很负面，这让我确信，如果存在前瞻偏差，一切都会看起来令人难以置信，并且参数负载会很高积极，但事实并非如此。
这是代码，我尝试对其进行一些标记：
scaler = StandardScaler()
X_scaled = 缩放器.fit_transform(X)

# 用于交叉验证的 TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)

# GridSearch 的 LightGBM 参数
参数网格 = {
    &#39;num_leaves&#39;: [--, --],
    &#39;学习率&#39;: [--, --],
    &#39;n_estimators&#39;: [--, --]
}

# 模型初始化
lgbm = lgb.LGBMRegressor(force_col_wise=True)

# 带有 TimeSeriesSplit 的网格搜索
grid_search = GridSearchCV(估计器=lgbm，param_grid=param_grid，cv=tscv，评分=&#39;neg_mean_absolute_error&#39;，详细=1)
grid_search.fit(X_scaled, y)

最佳模型 = grid_search.best_estimator_

# 使用最佳模型和早期停止进行训练和评估


feature_importances = best_model.feature_importances_
sorted_idx = np.argsort（feature_importances）
plt.figure(figsize=(10, 8))
plt.barh(范围(len(sorted_idx))，feature_importances[sorted_idx]，align=&#39;center&#39;)
plt.yticks(范围(len(sorted_idx)), np.array(features)[sorted_idx])
plt.title(&#39;功能重要性&#39;)
plt.show()

# 预测方向变化精度
方向准确度分数 = []

mae_scores = []

对于 tscv.split(X_scaled) 中的 train_index、test_index：
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    y_pred = best_model.predict(X_test)
    Direction_pred = np.sign(y_pred[1:] - y_pred[:-1])
    Direction_actual = np.sign(y_test.values[1:] - y_test.values[:-1])
    方向精度 = np.mean(方向预测 == 方向实际)
    Direction_accuracy_scores.append(direction_accuracy)
    mae_scores.append(mean_absolute_error(y_test, y_pred))

Average_mae = np.mean(mae_scores)
print(f&#39;平均绝对误差 (MAE): {average_mae}&#39;)
    
# 绘制方向变化精度
plt.figure(figsize=(10, 6))
plt.plot(direction_accuracy_scores, 标记=&#39;o&#39;)
plt.title(&#39;方向变化预测精度&#39;)
plt.xlabel(&#39;分割&#39;)
plt.ylabel(&#39;准确率&#39;)
plt.show()

print(f&#39;平均方向变化预测精度: {np.mean(direction_accuracy_scores)}&#39;)

# 预测下一个值
next_value_prediction = best_model.predict(X_scaled[-1].reshape(1, -1))
print(f&#39;预测下一个值：{next_value_prediction[0]}&#39;)

现在事情是这样的。模型获得了我最好的 MAE，但它并没有高得令人怀疑，并且需要进行大量的调整和其他工作才能达到这个目标。当我稍微改变叶子或分割或 n_estimators 的数量时，模型会变得明显更差，这样性能大约有一半的时间是正确的（与随机一样好）。
我是否疯狂地认为，即使缩放器提供了前瞻偏差，它也不会是一个巨大的偏差？ 70% 的时间预测都是朝着正确的方向发展的（并非不切实际），当我更改配置或拆分时，预测值会下降到 48 左右。但我很担心，因为当我实现此功能时：
对于 tscv.split(X) 中的 train_index、test_index：
    # 将数据分成每次折叠的训练集和测试集
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # 在没有前瞻偏差的情况下缩放数据
    定标器=标准定标器()
    X_train_scaled = 缩放器.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test) # 根据训练数据缩放测试数据
    
    # 根据缩放的训练数据拟合模型
    grid_search.fit(X_train_scaled, y_train)
    
    # 根据缩放测试数据评估模型
    y_pred = grid_search.best_estimator_.predict(X_test_scaled)
    Direction_pred = np.sign(y_pred[1:] - y_pred[:-1])
    Direction_actual = np.sign(y_test.values[1:] - y_test.values[:-1])
    方向精度 = np.mean(方向预测 == 方向实际)
    Direction_accuracy_scores.append(direction_accuracy)
    mae_scores.append(mean_absolute_error(y_test, y_pred))


模型 MAE 急剧恶化。我只是一个搞砸了定标器并引入了巨大的前瞻偏差的傻瓜吗？还是有更好的方法来测试定标器是否引起了巨大的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78189717/look-ahead-bias-via-standardscaler</guid>
      <pubDate>Tue, 19 Mar 2024 21:23:05 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow IO：如何按 ROW 而不是数组元素读取包含数组的镶木地板文件？</title>
      <link>https://stackoverflow.com/questions/78189710/tensorflow-io-how-to-read-a-parquet-file-with-arrays-by-row-instead-of-array-el</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78189710/tensorflow-io-how-to-read-a-parquet-file-with-arrays-by-row-instead-of-array-el</guid>
      <pubDate>Tue, 19 Mar 2024 21:21:26 GMT</pubDate>
    </item>
    <item>
      <title>我应该选择哪种机器学习方法？</title>
      <link>https://stackoverflow.com/questions/78189637/which-machine-learning-method-should-i-choose</link>
      <description><![CDATA[我请求帮助=!!表中有数据 |时间|-|罐压|.传感器以 0.05 秒的增量发送压力读数信号。该表适用于常规情况和紧急情况（当压力超出允许标准时）。我想训练该模型，使其能够识别紧急情况和相关信号。我应该选择哪种方法，该怎么做？该模型将是一种传入数据流的过滤器
我通过pandas编译了一个数据集，并使用googlcolab进行训练]]></description>
      <guid>https://stackoverflow.com/questions/78189637/which-machine-learning-method-should-i-choose</guid>
      <pubDate>Tue, 19 Mar 2024 21:04:58 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit 学习回归</title>
      <link>https://stackoverflow.com/questions/78189577/using-scikit-learn-regression</link>
      <description><![CDATA[我正在尝试使用如下所示的简单函数来学习scikit-learn回归：
导入 matplotlib.pyplot 作为 plt
将 numpy 导入为 np
从 sklearn.ensemble 导入 GradientBoostingRegressor、RandomForestRegressor
从 sklearn.gaussian_process 导入 GaussianProcessRegressor
从 sklearn.gaussian_process.kernels 导入 RBF
从 sklearn.metrics 导入mean_squared_error
从 sklearn.model_selection 导入 train_test_split
从 sklearn.multioutput 导入 MultiOutputRegressor
从 sklearn.neural_network 导入 MLPRegressor
从 sklearn.svm 导入 SVR


def func(x: np.ndarray):
    # x (N, 24), y (N, 2)
    x_mean = np.mean(x, 轴=1)
    a = np.sin(x_mean)
    b = np.cos(x_mean)
    y = np.array([a, b]).T
    返回y


np.随机.种子(0)
x = np.random.rand(1000, 24) * np.pi * 10
y = 函数(x)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

型号=[
    梯度提升回归器（），
    随机森林回归器(),
    高斯过程回归器(RBF()),
    MLPRegressor(max_iter=1000),
    支持向量机（），
]

plt.figure(figsize=(5, len(模型) * 3))

对于 i，枚举（模型）中的 base_model：
    名称 = 基础模型.__class__.__name__
    模型 = MultiOutputRegressor(base_model)
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    mse = 均方误差(y_test, y_pred)

    x_mean = np.mean(x_test, 轴=1)
    plt.subplot(len(模型), 1, i + 1)
    标题 = f&quot;{名称}. MSE: {mse:.2e}”
    打印（标题）
    plt.标题（标题）
    plt.plot(x_mean, y_test[:, 0], “.”, label=“y_test[:, 0]”)
    plt.plot(x_mean, y_test[:, 1], “.”, label=“y_test[:, 1]”)
    plt.plot(x_mean, y_pred[:, 0], “.”, label=“y_pred[:, 0]”)
    plt.plot(x_mean, y_pred[:, 1], “.”, label=“y_pred[:, 1]”)
    plt.图例()

plt.tight_layout()
plt.savefig(“regression_test.png”)


但是结果并不好：

我认为我没有正确使用这些回归器。
我应该如何修改我的代码？
更新
通过减少 x 的暗度，我得到了一些好的结果：
我应该如何更改 GP 的代码以支持高亮度输入？
]]></description>
      <guid>https://stackoverflow.com/questions/78189577/using-scikit-learn-regression</guid>
      <pubDate>Tue, 19 Mar 2024 20:49:54 GMT</pubDate>
    </item>
    <item>
      <title>序列分类中的通道重要性</title>
      <link>https://stackoverflow.com/questions/78189215/channel-importance-in-sequence-classification</link>
      <description><![CDATA[我有一个 ONNX 模型，它接受输入 [1, 35, 4]，即 [batch_size, num_channels, seq_len]，并输出 [1 , 3]，即[batch_size, num_classes]。我需要的是为每个通道分配一个数字，告诉我它对于模型做出的预测有多重要。我想我会使用 shap 的 PermutationExplainer 来达到此目的，但我不确定如何让它知道我不关心 seq_len&lt; /代码&gt;.
我尝试这样做：
导入 onnxruntime 作为 ort
导入形状
将 numpy 导入为 np

# 加载 ONNX 模型
model_path = &#39;解释示例.onnx&#39;
sess = ort.InferenceSession(model_path)

# 定义模型函数来处理批处理
定义模型(x):
    x = x.reshape(-1, 35, 4).astype(np.float32)
    # 模型期望输入 [1, 35, 4] 并返回 [1, 3]
    输出 = [sess.run(None, {&#39;input&#39;: x[i:i+1]})[0] for i in range(x.shape[0])]
    返回 np.concatenate(输出，轴=0)

# 创建样本输入
X = np.random.rand(1000, 35, 4).astype(np.float32)

输出名称 = [f&quot;输出_{i}&quot;;对于范围 (3) 内的 i]
feature_names = [f“频道 {i}”对于我在范围（35）]

def masker_fn(掩码, x):
    # 问题，掩码形状为 (140,)，x 形状为 (35, 4)
    masked_x = x.copy()
    对于范围内的 i(x.shape[1])：
        如果掩码[i] == 0：
            masked_x[:, i, :] = 0
    返回 masked_x

# 使用通道的自定义掩码声明解释器
解释器= shap.PermutationExplainer（模型，masker_fn，feature_names=feature_names，output_names=output_names）

# 计算形状值
shap_values = 解释器(X)

但问题是发送到掩码器的掩码具有形状 (140,) 而不是 (35,)。我当然可以以某种方式合并跨 seq_len 维度的掩码，但我认为 1. 解释器完成了不必要的工作，2. 也许这会以某种方式破坏其内部算法。
第一个问题：我怎样才能正确地告诉它，不，没有 140 个功能，而是 35 个？
第二个问题：你会如何解决这个问题？您是否有推荐的不同/更好/实际关心的图书馆？我正在考虑编写一个自定义排列解释器，但如果存在更好的可用解决方案，为什么还要麻烦呢。]]></description>
      <guid>https://stackoverflow.com/questions/78189215/channel-importance-in-sequence-classification</guid>
      <pubDate>Tue, 19 Mar 2024 19:34:59 GMT</pubDate>
    </item>
    <item>
      <title>R 中插入符号的训练函数中的中心和比例因子预测器</title>
      <link>https://stackoverflow.com/questions/78189166/center-and-scale-factor-predictors-within-train-function-from-caret-in-r</link>
      <description><![CDATA[我在使用 R caret 包的 train 函数的 preProc 参数方面遇到问题。我想居中并缩放我的预测变量，但忽略因子列。当我在火车之外进行预处理时，它工作正常，但我希望在火车功能内进行预处理。我错过了什么吗？
下面是一个示例，其中在训练之外使用 preProcess 时忽略因子预测器。
df &lt;- data.frame(
    分数 = runif(1000, 80, 110),
    var1 = as.factor(样本(0:1, 1000, 替换 = TRUE)),
    var2 = runif(1000, 5, 25)
）
预处理(df[-1], method=c(&#39;center&#39;,&#39;scale&#39;))

由 1000 个样本和 2 个变量创建

预处理：
  - 居中 (1)
  - 被忽略 (1)
  - 缩放 (1)

这是我在火车内部使用 preProc 时发生的情况
df &lt;- data.frame(
    分数 = runif(1000, 80, 110),
    var1 = as.factor(样本(0:1, 1000, 替换 = TRUE)),
    var2 = runif(1000, 5, 25)
）
mod &lt;- 训练（分数〜.，数据= df，
             方法=“lm”，
             preProc = c(“中心”,“比例”))
mod$预处理

由 1000 个样本和 2 个变量创建

预处理：
  - 居中 (2)
  - 被忽略 (0)
  - 缩放 (2)

提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78189166/center-and-scale-factor-predictors-within-train-function-from-caret-in-r</guid>
      <pubDate>Tue, 19 Mar 2024 19:25:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么具有 Cartpole 健身房环境的 stable_baselines3 模型通过 sutton_barto_reward 提高了平均剧集奖励？</title>
      <link>https://stackoverflow.com/questions/78188062/why-is-my-stable-baselines3-model-with-cartpole-gym-environment-improving-mean-e</link>
      <description><![CDATA[当我运行此代码时，我看到剧集长度平均值不断增加（就像一个好的模型应该的那样），而剧集平均奖励保持在 -1 不变，这就是 sutton_barto_reward 系统的工作原理。
导入体育馆
从 cartpole 导入 CartPoleEnv
从 stable_baselines3 导入 PPO
从 stable_baselines3.ppo.policies 导入 MlpPolicy

env = CartPoleEnv(sutton_barto_reward=True)

模型 = PPO(“MlpPolicy”, env, gamma=1, verbose=1)
model.learn(total_timesteps=30000)


但是，我不明白为什么会这样，因为在gymnasium的GitHub文档中的Cartpole代码中似乎没有使用任何折扣率。既然剧集的累积奖励始终相同，那么剧集长度平均值难道不应该没有任何改善吗？]]></description>
      <guid>https://stackoverflow.com/questions/78188062/why-is-my-stable-baselines3-model-with-cartpole-gym-environment-improving-mean-e</guid>
      <pubDate>Tue, 19 Mar 2024 16:02:07 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch LSTM 不使用隐藏层</title>
      <link>https://stackoverflow.com/questions/78187211/pytorch-lstm-not-using-hidden-layer</link>
      <description><![CDATA[我正在使用 PyTorch 的 LSTM api，但遇到了一些问题。我使用 LSTM 作为虚拟 AI 模型。该模型的任务是如果前一个数字小于当前数字，则返回 1。
因此，对于像 [0.7, 0.3, 0.9, 0.99] 这样的数组，预期输出是 [1.0, 0.0, 1.0, 1.0]。无论如何，第一个输出应该是 1.0。
我设计了以下网络来尝试这个问题：
# network.py

进口火炬

N_输入 = 1
N_STACKS = 1
N_隐藏 = 3

LR = 0.001


网络类（torch.nn.Module）：

    # 参数：自身
    def __init__(自身):
        超级（网络，自我）.__init__()

        self.lstm = torch.nn.LSTM(
            输入大小=N_INPUT，
            隐藏大小=N_HIDDEN,
            num_layers=N_STACKS,
        ）

        self.线性 = torch.nn.Linear(N_HIDDEN, 1)
        self.relu = torch.nn.ReLU()

        self.optim = torch.optim.Adam(self.parameters(), lr=LR)
        self.loss = torch.nn.MSELoss()

    # 参数：自身、预测、预期
    def backprop(self, xs, es):

        # 执行反向传播
        self.optim.zero_grad()
        l = self.loss(xs, torch.tensor(es))
        l.backward()
        自我优化.step()

        返回l

    # params: self, data (作为Python数组)
    def 前向（自身，数据）：

        out, _ = self.lstm(torch.tensor(dat))

        输出 = self.relu(输出)
        输出 = 自.线性(输出)

        返回

我从这个文件中调用它：
# main.py

进口网络

将 numpy 导入为 np

# 创建一个新网络
n: 网络.网络 = 网络.网络()


# 创建一些数据
def rand_array():

    # 一堆随机数
    a = [[np.random.uniform(0, 1)] for i in range(1000)]

    # 现在，如果前一个数字更大，我们的预期值为 0，否则为 1
    预期 = [0.0 如果 a[i - 1][0] &gt; a[i][0] 否则 1.0 for i in range(len(a))]
    Expected[0] = 1.0 # 使第一个元素始终为 1.0

    返回[a，预期]


# 一堆随机数组
数据 = [rand_array() for i in range(1000)]

# 100 个纪元
对于范围（100）内的 i：
    对于数据中的 i：

        预测 = n(i[0])
        损失 = n.backprop(pred, i[1])
        print(&quot;损失: {:.5f}&quot;.format(loss))

现在，当我运行这个程序时，我只是得到了大约 0.25 的损失，而且一旦到达那里，它就不会真正改变。我认为该模型只是为每个输入选取 0 和 1 (0.5) 的平均值。
这让我相信模型无法看到以前的数据；数据只是随机数（尽管预期输出基于这些随机数），并且模型不记得之前发生了什么。
我的问题是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78187211/pytorch-lstm-not-using-hidden-layer</guid>
      <pubDate>Tue, 19 Mar 2024 13:59:08 GMT</pubDate>
    </item>
    <item>
      <title>这样分割数据会不会造成数据泄露？</title>
      <link>https://stackoverflow.com/questions/78187069/is-there-data-leakage-while-splitting-the-data-this-way</link>
      <description><![CDATA[以这种方式分割数据集时是否存在数据泄漏的可能性：
def split_dataset(ds, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=True):
    # 获取数据集大小
    数据集大小 = len(ds)

    # 计算分割大小
    train_size = int(train_ratio * dataset_size)
    val_size = int(val_ratio * 数据集大小)
    测试大小 = 数据集大小 - 训练大小 - val_size

    # 如果需要的话，打乱数据集
    如果随机播放：
        ds = ds.shuffle(dataset_size)

    # 分割数据集
    train_dataset = ds.take(train_size)
    val_dataset = ds.skip(train_size).take(val_size)
    test_dataset = ds.skip(train_size + val_size).take(test_size)

    返回train_dataset、val_dataset、test_dataset

训练深度学习模型
只是对模型的准确率感到惊讶，它超过了 98%。]]></description>
      <guid>https://stackoverflow.com/questions/78187069/is-there-data-leakage-while-splitting-the-data-this-way</guid>
      <pubDate>Tue, 19 Mar 2024 13:37:37 GMT</pubDate>
    </item>
    <item>
      <title>选择模型训练参数（和层）</title>
      <link>https://stackoverflow.com/questions/78184146/selecting-model-training-params-and-layers</link>
      <description><![CDATA[在代码中（下面从tensorflow.org“针对初学者的 Tensorflow 2 快速入门”复制/粘贴了所有许多变量，例如：

纪元数
批量大小
学习率
层（或层的组合）
每层的激活函数
损失函数
单位（密集层的参数）
比率（针对 Dropout 层）
等等...

除了“猜测和猜测”之外，是否还有一些受过教育的程序？测试”，以便找到“最佳”方案。参数组合 --- 或者甚至知道从哪个参数组合开始？
我是否缺乏“专家”所需要的一些知识（或信息）？或博士学位将为我提供，这将使我有能力选择“最好的”。参数（第一次）？
我当然明白最后一层 Dense(10) 的一些参数需要是 10 个单位来表示 10 个类...而且我知道 10,000 层可能不太好...所以我不是在谈论极端值或最后一个密集层中的 10 之类的东西，而是其他参数。
含义 - 作为示例 - 在下面的代码中，假设我更改了行中的单位
tf.keras.layers.Dense（32，激活=&#39;relu&#39;）

到其他 64（而不是 32）。
对于下面的例子，
64 肯定比 32 更好（或更差）有什么理由吗？
是否有理由添加另一个密集层会更好（或更差）？
是否有明确的理由说明为什么 6 个 epoch 的训练会比 5 个 epoch 更好（或更差）？
我知道，如果我理解模型最终所做的所有计算，我就会得到我的答案......这意味着，因为“某人”写了那个代码，有能力知道这一点并不是不可能的......
所有教程和内容都令人非常沮丧。那里的网站关注的是如何，而不是为什么。
（任何建议/链接/资源将不胜感激）
提前非常感谢各位专家！
导入argparse
将张量流导入为 tf


def 过程（参数）：
    ( X_train, y_train ), ( X_test, y_test ) = tf.keras.datasets.mnist.load_data()
    X_train = X_train / 255.0
    X_测试 = X_测试 / 255.0

    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    模型 = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(10)
    ]）

    model.compile(optimizer=&#39;adam&#39;,loss=loss_fn,metrics=[&#39;accuracy&#39;])

    hist = model.fit(X_train,
                     y_火车，
                     批量大小=32，
                     纪元=5，
                     验证分割=0.2，
                     随机播放=真）


如果 __name__ == &#39;__main__&#39;:
    解析器 = argparse.ArgumentParser()
    parser.add_argument(&#39;--batch_size&#39;, type=int, default=32)
    args = parser.parse_args()
    过程（参数）

``
]]></description>
      <guid>https://stackoverflow.com/questions/78184146/selecting-model-training-params-and-layers</guid>
      <pubDate>Tue, 19 Mar 2024 03:47:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 tf.compat.v1.summary.merge_all() 合并摘要时出错</title>
      <link>https://stackoverflow.com/questions/78131106/error-while-merging-summaries-using-tf-compat-v1-summary-merge-all</link>
      <description><![CDATA[由于某种原因，self._summaries 对象为 None。
这会导致以下错误：
 文件“main.py”，第 216 行，在  中
    tf.compat.v1.app.run()
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/platform/app.py”，第 40 行，运行中
    _run（主=主，argv=argv，flags_parser=_parse_flags_tolerate_undef）
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/absl/app.py”，第 312 行，运行中
    _run_main（主要，参数）
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/absl/app.py”，第 258 行，位于 _run_main
    sys.exit(主(argv))
  文件“main.py”，第 203 行，在 main 中
    setup_training（hps.mode，生成器，鉴别器，generator_batcher，discriminator_batcher，generator_val_batcher，discriminator_val_batcher）
  文件“main.py”，第 160 行，位于 setup_training
    trainer.adversarial_train（生成器、鉴别器、generator_batcher、discriminator_batcher、generator_val_batcher、discriminator_val_batcher、summary_writer、sess_context_manager）
  文件“/home/20bce120/tf2/trainer.py”，第 106 行，adversarial_train
    result_train = 生成器.run_train_step(sess, 批处理)
  文件“/home/20bce120/tf2/generator.py”，第 636 行，在 run_train_step 中
    返回 sess.run(to_return, feed_dict)
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 968 行，运行中
    运行元数据指针）
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 1176 行，位于 _run
    self._graph、获取、feed_dict_tensor、feed_handles=feed_handles）
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 487 行，在 __init__ 中
    self._fetch_mapper = _FetchMapper.for_fetch(获取)
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 270 行，在 for_fetch 中
    返回_DictFetchMapper(获取)
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 419 行，位于 __init__ 中
    _FetchMapper.for_fetch(fetch) 用于在 fetches.values() 中获取
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 419 行，在  中
    _FetchMapper.for_fetch(fetch) 用于在 fetches.values() 中获取
  文件“/apps/codes/anaconda3/envs/myenv2/lib/python3.6/site-packages/tensorflow/python/client/session.py”，第 265 行，在 for_fetch 中
    （获取，类型（获取）））
类型错误：获取参数 None 具有无效类型 

代码：
 def build_graph(self):
        
        tf.logging.info(&#39;构建图表...&#39;)
        t0 = 时间.time()
        self._add_placeholders()
        self._add_seq2seq()
        self.global_step = tf.Variable(0, name=&#39;global_step&#39;, trainable=False)
        如果 self._hps.mode == &#39;train&#39; 或 self._hps.mode == &#39;pretrain&#39;:
            self._add_train_op()
            self._rollout()
            
        self._summaries = tf.compat.v1.summary.merge_all()
        tf.logging.info(&#39;构建图表的时间：%i秒&#39;, time.time() - t0)

我尝试在调用 tf.compat.v1.summary.merge_all() 之前添加一个 tf.scalar_summary()，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78131106/error-while-merging-summaries-using-tf-compat-v1-summary-merge-all</guid>
      <pubDate>Sat, 09 Mar 2024 03:00:37 GMT</pubDate>
    </item>
    <item>
      <title>Keras 中的形状不兼容，但 model.summary() 似乎没问题</title>
      <link>https://stackoverflow.com/questions/78066771/incompatible-shapes-in-keras-but-model-summary-seems-ok</link>
      <description><![CDATA[下面是我的模型
# 由于我们要预测每个时间步的值，因此我们设置 return_sequences=True
输入=输入(batch_shape=ip_shape)
mLSTM = LSTM（单位=32，return_sequences=True，stateful=True）（输入）
mDense = Dense(单位=32，激活=&#39;线性&#39;)(输入)
mSkip = Add()([mLSTM, mDense])

mSkip = 密集（单位=1，激活=&#39;线性&#39;）（mSkip）
模型=模型（输入，mSkip）

亚当 = 亚当(learning_rate=0.01)
model.compile(优化器=adam, 损失=total_loss)
模型.summary()

模型摘要
型号：“model_3”
_______________________________________________________________________________________________
 层（类型）输出形状参数#连接到
=================================================== ===============================================
 input_5 (输入层) [(104, 22050, 1)] 0 []
                                                                                                  
 lstm_4 (LSTM) (104, 22050, 32) 4352 [&#39;input_5[0][0]&#39;]
                                                                                                  
 稠密_5（密集）(104, 22050, 32) 64 [&#39;input_5[0][0]&#39;]
                                                                                                  
 add_3 (添加) (104, 22050, 32) 0 [&#39;lstm_4[0][0]&#39;,
                                                                     &#39;dense_5[0][0]&#39;]
                                                                                                  
 稠密_6（密集）(104, 22050, 1) 33 [&#39;add_3[0][0]&#39;]
                                                                                                  
=================================================== ===============================================
总参数：4449 (17.38 KB)
可训练参数：4449 (17.38 KB)
不可训练参数：0（0.00 字节）
_______________________________________________________________________________________________

我正在使用自定义损失函数。我不确定反向传播时是否会弄乱形状
def Total_loss(y_true, y_pred):
    比率 = 0.5
    dc_loss = math_ops.pow(math_ops.subtract(math_ops.mean(y_true, 0), math_ops.mean(y_pred, 0)), 2)
    dc_loss = math_ops.mean(dc_loss, axis=-1)
    dc_energy = math_ops.mean(math_ops.pow(y_true, 2), axis=-1) + 0.00001
    dc_loss = math_ops.div(dc_loss, dc_energy)

    esr_loss = math_ops.squared_difference(y_pred, y_true)
    esr_loss = math_ops.mean(esr_loss, axis=-1)
    esr_energy = math_ops.mean(math_ops.pow(y_true, 2), axis=-1) + 0.00001
    esr_loss = math_ops.div(esr_loss, esr_energy)

    返回（比率）*dc_loss +（1-比率）*esr_loss

最后的错误：让我知道是否需要整个回溯
InvalidArgumentError：图形执行错误：

...

不兼容的形状：[104,22050,32] 与 [32,22050,1]
     [[{{节点gradient_tape/total_loss/BroadcastGradientArgs}}]] [Op：__inference_train_function_9604]

设置 stateful=False 似乎有效，但我不明白为什么]]></description>
      <guid>https://stackoverflow.com/questions/78066771/incompatible-shapes-in-keras-but-model-summary-seems-ok</guid>
      <pubDate>Tue, 27 Feb 2024 10:22:55 GMT</pubDate>
    </item>
    <item>
      <title>Python 中具有正系数的线性回归</title>
      <link>https://stackoverflow.com/questions/35986627/linear-regression-with-positive-coefficients-in-python</link>
      <description><![CDATA[我正在尝试找到一种方法来拟合具有正系数的线性回归模型。
我发现的唯一方法是sklearn的Lasso模型，它有一个 positive=True 参数，但不建议与 alpha=0 一起使用（意味着对权重没有其他限制）。
您知道另一种模型/方法/方式吗？]]></description>
      <guid>https://stackoverflow.com/questions/35986627/linear-regression-with-positive-coefficients-in-python</guid>
      <pubDate>Mon, 14 Mar 2016 11:43:38 GMT</pubDate>
    </item>
    </channel>
</rss>