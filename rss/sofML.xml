<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 06 Jan 2025 18:23:06 GMT</lastBuildDate>
    <item>
      <title>如何改进我的线性回归模型？</title>
      <link>https://stackoverflow.com/questions/79333949/how-to-improve-my-linear-regression-model</link>
      <description><![CDATA[今天终于完成了线性回归的 Scratch 实现
filepath = f&quot;{path}/Food_Delivery_Times.csv&quot;
df = pd.read_csv(filepath)
df.head()

print(df.columns)
df.isnull().sum()

df.dropna(inplace = True , thresh=4)
df.drop_duplicates(inplace= True)
df[&#39;Courier_Experience_yrs&#39;] = df[&#39;Courier_Experience_yrs&#39;].interpolate()
columns_to_fill = [&#39;Weather&#39;, &#39;Traffic_Level&#39;, &#39;Time_of_Day&#39;]
for col in columns_to_fill:
mode_value = df[col].mode()[0]
df[col]= df[col].fillna(mode_value)
df.info()

print(df[&#39;Weather&#39;].unique())
print(df[&#39;Traffic_Level&#39;].unique())
print(df[&#39;Time_of_Day&#39;].unique())
print(df[&#39;Vehicle_Type&#39;].unique())

def onehot(df,column):
values = df[column].unique()
for val in values:
df[val] = (df[column] == val).astype(int)
return df

df = onehot(df,&#39;Weather&#39;)
df = onehot(df,&#39;Vehicle_Type&#39;)
df = onehot(df,&#39;Time_of_Day&#39;)
traffic_mapping = {&#39;低&#39;:0,&#39;中&#39;:1,&#39;高&#39;:2}
df[&#39;Traffic_encoded&#39;] = df[&#39;Traffic_Level&#39;].map(traffic_mapping)
df

categorical_col = [&#39;天气&#39;, &#39;Traffic_Level&#39;, &#39;时间&#39;,&#39;车辆类型&#39;]
col_new_df = [col for col in df.columns if col not in categorical_col ]
col_new_df

new_df = df[col_new_df]
new_df.info()

# 特征缩放
for col in new_df.columns:
if col not in [&#39;订单ID&#39;,&#39;交货时间分钟&#39;]:
new_df[col] = new_df[col].astype(&#39;float64&#39;)
mean = new_df[col].mean()
std = new_df[col].std()
new_df.loc[:,col] = (new_df[col]-mean)/std #转换为浮点数，因为在缩放浮点数值后将其分配给 int 数据类型，这会引发警告

new_df.describe()

train_df = new_df.sample(frac = 0.8,random_state =200)
test_df = new_df.drop(train_df.index)

train_df.reset_index(inplace=True)
test_df.reset_index(inplace=True)

train_df.drop(columns = [&#39;index&#39;],inplace=True)
test_df.drop(columns = [&#39;index&#39;],inplace=True)

columns_needed =列表（new_df.columns）
columns_needed.remove（&#39;订单 ID&#39;）
columns_needed.remove（&#39;送货时间分钟&#39;）
X = train_df[columns_needed].to_numpy()
Y = train_df[&#39;送货时间分钟&#39;].to_numpy()
Y_mean = train_df[&#39;送货时间分钟&#39;].mean()
Y_std = train_df[&#39;送货时间分钟&#39;].std()
Y = (Y-Y_mean)/Y_std

m = len(X)
np.random.seed(42)
W = np.random.randn(len(X[0]))
b = 0
alpha = 0.5
Lambda = 0.5
迭代 = 0
dW = np.zeros(len(X[0]))
当迭代 &lt; 100000：
f = np.dot(X,W) + b
损失 = (np.sum((f - Y)**2) + Lambda*np.sum(W*W))/(2*m)
dW = (np.dot(X.T,(f-Y)) + Lambda*W)/m
db = np.sum(f-Y)/m
W -= alpha*dW
b -= alpha*db
迭代 += 1
如果迭代 % 10000 == 0：
打印（迭代，“，“，“，损失）
如果（损失 &lt; 10**(-3)）：
中断

打印（“完成”）

#测试
sum = 0
sum2 =0
ymean = test_df[&#39;Delivery_Time_min&#39;].mean()
Y_original = test_df[&#39;Delivery_Time_min&#39;].to_numpy()
X_test = test_df[columns_needed].to_numpy()
Y_predicted = np.dot(X_test,W) + b
Y_predicted_ori = Y_predicted*Y_std + Y_mean
print(&quot;原始 ---- 预测 ---- 错误&quot;)
for i in range(len(Y_original)):
print(Y_original[i],&quot;---&quot;,Y_predicted_ori[i],&quot;---&quot;,Y_original[i]-Y_predicted_ori[i])
sum += (Y_original[i]-Y_predicted_ori[i])**2
sum2 += (Y_original[i] - ymean)**2

rscore = 1 - (sum/sum2)
msme = sum/len(Y_original)
print(rscore)
print(msme)

以上是它的实现

数据集 - 食品配送时间预测（Kaggle）
R2_score 为 0.754
MSME 为 133.17

那么它对初学者来说好吗？
可以做些什么来改进它？]]></description>
      <guid>https://stackoverflow.com/questions/79333949/how-to-improve-my-linear-regression-model</guid>
      <pubDate>Mon, 06 Jan 2025 18:12:55 GMT</pubDate>
    </item>
    <item>
      <title>通过 streamlit 执行时出现“短信垃圾邮件分类器”错误“实例不适合”</title>
      <link>https://stackoverflow.com/questions/79333200/sms-spam-classifier-error-while-executing-it-via-streamlit-instance-not-fitte</link>
      <description><![CDATA[错误图片
这是我的第一个 ML 项目：“短信垃圾邮件分类器”，当我通过 streamlit 运行它时，我遇到了这个错误。请帮我调试一下
后端代码运行正常，但这是我部署后显示的内容
输出应该是垃圾邮件/非垃圾邮件]]></description>
      <guid>https://stackoverflow.com/questions/79333200/sms-spam-classifier-error-while-executing-it-via-streamlit-instance-not-fitte</guid>
      <pubDate>Mon, 06 Jan 2025 13:34:05 GMT</pubDate>
    </item>
    <item>
      <title>在 Rust 中的 Burn 张量中使用 f64</title>
      <link>https://stackoverflow.com/questions/79333072/using-f64-in-burn-tensor-in-rust</link>
      <description><![CDATA[我是 Rust 新手，我正在研究使用 Burn 移植一些 Python/Torch 代码，用于新的统计参数方法。
第一步：我想生成一个 (10, 1) 张量，其中包含从具有已知参数的柯西分布生成的随机值。Burn 中的分布非常有限，因此我使用 statrs。通过使用 statrs，我可以获得一个 Vec&lt;f64&gt;，然后我可以将其包装到 Burn 中的 TensorData 中，从而生成一个 Tensor。
我添加了一些类型签名，但 Burn 有 Float 而不是特定的 f64，我对此有点困惑。事实上，只是为了调试目的，我想从 Burn 张量中提取数据作为 Vec&lt;f64&gt; 来查看它（我应该从 vec: Vec&lt;f64&gt; 中看到相同的值）但我得到了运行时类型不兼容。
使用 rand::prelude::Distribution;
使用 statrs::distribution::Cauchy;
使用 rand_chacha::ChaCha8Rng;
使用 rand_core::SeedableRng;
使用 burn::tensor::{Tensor, TensorData, Float};
使用 burn::backend::Wgpu;

类型 Backend = Wgpu;

fn main() {
// 一些全局引用
let device = Default::default();
let mut rng: ChaCha8Rng = ChaCha8Rng::seed_from_u64(2);

// 使用 statrs 创建随机 vec，存储在 Vec&lt;f64&gt; 中
let dist: Cauchy = Cauchy::new(5.0, 2.0).unwrap();
let vec: Vec&lt;f64&gt; = dist.sample_iter(&amp;mut rng).take(10).collect();

// 将其包装到 Burn 张量中
let td: TensorData = TensorData::new(vec, [10, 1]);
let tensor: Tensor&lt;Backend, 2, Float&gt; = Tensor::&lt;Backend, 2, Float&gt;::from_data(td, &amp;device);

print!(&quot;{:?}\n&quot;, tensor.to_data().to_vec::&lt;f64&gt;().unwrap());
}


在上面运行时，我得到
thread &#39;main&#39; panicked at src/main.rs:23:55:
called `Result::unwrap()` on an `Err` value: TypeMismatch(&quot;Invalid target element type 
(expected F32, got F64)&quot;)

使用 to_vec::&lt;f32&gt; 有效，但我希望 Burn 张量具有 f64 值（torch 有这个），因为错误似乎意味着我在某个时候丢失了精度 - 不太好。
是否可以将 f64 存储在 Burn 张量中？]]></description>
      <guid>https://stackoverflow.com/questions/79333072/using-f64-in-burn-tensor-in-rust</guid>
      <pubDate>Mon, 06 Jan 2025 12:43:18 GMT</pubDate>
    </item>
    <item>
      <title>如何获取在线视频游戏的类型？我的数据集包含大约 3500 个用户 ID、大约 5000 个游戏及其由这些用户给出的评分 [关闭]</title>
      <link>https://stackoverflow.com/questions/79332726/how-do-i-get-the-genre-of-video-games-online-my-dataset-contains-around-3500-us</link>
      <description><![CDATA[我正在尝试构建一个推荐引擎作为家庭作业的一部分。数据集包含大约 3500 个用户 ID、大约 5000 个游戏及其由这些用户给出的评分。我的当务之急是确定这些游戏的类型，根据一些人的说法，最好的方法是编写一个 Python 程序来获取包含此类信息的列表。但是，我不知道如何找到这样的列表。如果您有推荐系统经验，请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/79332726/how-do-i-get-the-genre-of-video-games-online-my-dataset-contains-around-3500-us</guid>
      <pubDate>Mon, 06 Jan 2025 10:16:01 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法将手势识别集成到 .NET MAUI 中？</title>
      <link>https://stackoverflow.com/questions/79332674/is-there-any-way-to-integrate-hand-gesture-recognition-in-net-maui</link>
      <description><![CDATA[我有一个名为“手语应用”的项目，我需要在其中集成手势识别功能来识别手语。我不知道从哪里开始，因为我只具备 C# 基础知识。
我需要指南或建议，看看是否可行。我是大学二年级计算机科学专业学生]]></description>
      <guid>https://stackoverflow.com/questions/79332674/is-there-any-way-to-integrate-hand-gesture-recognition-in-net-maui</guid>
      <pubDate>Mon, 06 Jan 2025 09:55:54 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Flutter 应用中自动裁剪收据以仅显示购买的商品和价格？</title>
      <link>https://stackoverflow.com/questions/79332546/how-to-automatically-crop-a-receipt-to-show-only-purchased-items-and-prices-in-a</link>
      <description><![CDATA[我正在构建一个 Flutter 应用，用户可以在其中上传收据，我想处理这些收据，以便只显示购买的商品、金额和价格。我的最终目标是提取这些信息以供进一步处理。
以下是我迄今为止尝试过的方法：

OCR：我使用 google_ml_kit 进行 OCR，但它会捕获收据上的所有文本，包括页眉、页脚和其他不相关的部分。
正则表达式：我尝试使用正则表达式过滤 OCR 输出以匹配价格和商品的模式，但这不起作用，因为收据的格式和结构差异很大。

为了提高准确性，我考虑使用图像标记或训练自定义 TensorFlow Lite 模型来识别和裁剪收据中包含购买商品和价格的部分，然后再应用 OCR。但是，我不确定如何有效地解决这个问题，特别是在 Flutter 的背景下。
挑战：

收据具有不同的布局、字体和大小，因此没有固定的裁剪区域。
以编程方式准确识别购买商品和价格的区域。
将经过训练的 TensorFlow Lite 模型整合到 Flutter 应用中以进行图像分割或标记。

问题：

如何使用图像处理或机器学习技术自动裁剪收据的相关部分（包含购买的商品和价格）？
训练自定义 TensorFlow Lite 模型是一种可行的方法吗？如果是，我该如何训练它来检测收据的相关部分？
是否有现有的工具、框架或软件包（与 Flutter/Dart 兼容）可以简化此过程？

示例图像（所以我只想要框中的内容）：
]]></description>
      <guid>https://stackoverflow.com/questions/79332546/how-to-automatically-crop-a-receipt-to-show-only-purchased-items-and-prices-in-a</guid>
      <pubDate>Mon, 06 Jan 2025 08:50:54 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow 中加载模型时出现图形断开错误</title>
      <link>https://stackoverflow.com/questions/79331871/graph-disconnected-error-when-loading-model-in-tensorflow</link>
      <description><![CDATA[我正在运行代码存储库，在重新创建模型时收到一条错误消息。

ValueError：图形断开连接：无法获取层“resnet50”处张量 KerasTensor(type_spec=TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=&#39;resnet50_input&#39;), name=&#39;resnet50_input&#39;, description=&quot;created by layer &#39;resnet50_input&#39;&quot;) 的值。以下先前的层可以毫无问题地访问：......

我使用的是 Tensorflow 2.14.0 和 Python 3.10
这是测试代码，我发现这是来自 repo 的错误代码：
num_classes = 10
input_shape = (32, 32, 3)
model = tf.keras.Sequential()
base_model = tf.keras.applications.ResNet50(weights=&#39;imagenet&#39;, include_top=False, input_shape=input_shape)
for layer in base_model.layers:
layer.trainable = False
model.add(base_model)
model.add(tf.keras.layers.GlobalAveragePooling2D())
model.add(tf.keras.layers.Dense(num_classes,激活=&#39;softmax&#39;))

extractor = tf.keras.Model(inputs=model.layers[0].input,
outputs=[layer.output for layer in model.layers])

我搜索了谷歌，尝试了所有方法。有人说我应该先创建一个输入，但下面的代码仍然出错。有人能给我一个解决方案吗？我很感激。
num_classes = 10
input_shape = (32, 32, 3)
input_tensor = tf.keras.Input(shape=input_shape)
model = tf.keras.Sequential()
base_model = tf.keras.applications.ResNet50(weights=&#39;imagenet&#39;, include_top=False, input_shape=input_shape, input_tensor=input_tensor)
for layer in base_model.layers:
layer.trainable = False
model.add(base_model)
model.add(tf.keras.layers.GlobalAveragePooling2D())
model.add(tf.keras.layers.Dense(num_classes,activation=&#39;softmax&#39;))

extractor = tf.keras.Model(inputs=model.layers[0].input,
输出=[模型层中的层层输出])
]]></description>
      <guid>https://stackoverflow.com/questions/79331871/graph-disconnected-error-when-loading-model-in-tensorflow</guid>
      <pubDate>Mon, 06 Jan 2025 01:35:22 GMT</pubDate>
    </item>
    <item>
      <title>如何提高我的多类分类模型的准确性[关闭]</title>
      <link>https://stackoverflow.com/questions/79331544/how-to-increase-the-accuracy-of-my-multiclass-classification-model</link>
      <description><![CDATA[在此处输入图片说明
我正在研究驾驶员得分预测问题。我的数据集包括 RPM、角度、速度和油门等特征。然而，挑战在于我没有基本事实标签。为了解决这个问题，我进行了彻底的分析，并咨询了业内专业人士。基于这些见解，我创建了一个方程，其中驾驶员得分计算为特征的加权总和（权重 * 特征）。
具体来说，对于刹车痕迹特征，我单独包含了它的权重，因为它是一个二元变量。数据集显示大多数特征之间具有高度相关性，但角度除外，它与其他特征的相关性似乎最小。此外，数据不平衡，应用缩放后，RPM 和速度都呈正态分布。然而，角度表现得更像一个多维特征。
我读到几篇研究论文，它们表明特征相关性对于分类任务来说并不是一个关键问题，尤其是在使用随机森林等模型时。为了进一步增强数据集，我引入了加速度，使用公式 (Δv / Δt) 来确定制动事件是突然的还是正常的，如上式所示。
尽管付出了这些努力，但模型的准确性仍然很低。我最初避免平衡数据，因为应用 SMOTE（合成少数过采样技术）会导致严重的数据失真。 SMOTE 生成的合成数据严重依赖于特征空间的边缘，这使得分布偏离了正态性，使结果变得更糟。
关于如何提高模型性能，有什么建议吗？
我尝试了 pca 分析，结果表明角度贡献不大，但根据专家的说法，角度非常重要，在这种情况下使用 iforest 是否比 xgboost 或 randomforest 分类器更好？我自己计算了驾驶员得分，然后将其分为 [0,30,50,70,100]，差，平均，好，优秀，不平衡在于我们的道路类型，高速公路比另一条道路少得多，优秀的道路也比平均水平低得多]]></description>
      <guid>https://stackoverflow.com/questions/79331544/how-to-increase-the-accuracy-of-my-multiclass-classification-model</guid>
      <pubDate>Sun, 05 Jan 2025 20:53:17 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Flutter 应用程序中实现 Roboflow 模型 API？</title>
      <link>https://stackoverflow.com/questions/79330782/how-to-implement-roboflow-model-api-in-flutter-app</link>
      <description><![CDATA[我正在尝试在我的 Flutter 应用中实现 RoboFlow 模型 API 来分析图像。我想从用户那里获取图像并使用 Roboflow 模型 API 对其进行分析。
URL、API 密钥、模型 API、版本一切都正确，但输入数据有些错误。
下面是我的函数：
Future&lt;void&gt; testRoboflowAPI(File imagePath) async {
try {
List&lt;int&gt; imageBytes = imagePath.readAsBytesSync();
String base64Image = base64Encode(imageBytes);

final url =
&#39;https://classify.roboflow.com/**********/1?api_key=*********&#39;;
final headers = {&#39;Content-Type&#39;: &#39;application/x-www-form-urlencoded&#39;};

final body = {
&quot;data&quot;: base64Image,
};

finalcodedBody = Uri(queryParameters: body).query;
final response = await http.post(
Uri.parse(url),
headers: headers,
body:codedBody,
);

if (response.statusCode == 200) {
print(&#39;成功：${response.body}&#39;);
} else {
print(
&#39;错误：状态代码 ${response.statusCode}，响应：${response.body}&#39;);
}
} catch (e) {
print(&#39;异常：${e.toString()}&#39;);
}
}

我收到给定的错误：
错误：状态代码 400，响应：{&quot;message&quot;:&quot;无法加载输入图像。原因：base64 输入图像格式错误。&quot;}

您能帮我得到正确的响应并解决错误吗？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79330782/how-to-implement-roboflow-model-api-in-flutter-app</guid>
      <pubDate>Sun, 05 Jan 2025 13:15:11 GMT</pubDate>
    </item>
    <item>
      <title>如何将 pixelClassificationLayer() 更新为自定义损失函数？</title>
      <link>https://stackoverflow.com/questions/79328556/how-do-i-update-pixelclassificationlayer-to-a-custom-loss-function</link>
      <description><![CDATA[我在 Mathworks 官方网站上看到 pixelClassificationLayer() 函数，我应该使用以下代码将其更新为自定义损失函数：
function loss = modelLoss(Y,T) 
mask = ~isnan(T);
target(isnan(T)) = 0;
loss = crossentropy(Y,T,Mask=mask,NormalizationFactor=&quot;mask-included&quot;); 
end

netTrained = trainnet(images,net,@modelLoss,options); 

但是，我看不到任何关于输入“Classes”或“ClassWeights”的提及，我目前正使用它们来定义自定义 pixelClassificationLayer：
pixelClassificationLayer(&#39;Classes&#39;,classNames,&#39;ClassWeights&#39;,classWeights)，其中 classNames 是一个向量，以字符串形式包含每个类的名称，classWeights 是一个向量，包含每个类的权重，用于在训练数据中存在代表性不足的类时平衡类。
如何在自定义损失函数中包含这些参数？]]></description>
      <guid>https://stackoverflow.com/questions/79328556/how-do-i-update-pixelclassificationlayer-to-a-custom-loss-function</guid>
      <pubDate>Sat, 04 Jan 2025 09:05:32 GMT</pubDate>
    </item>
    <item>
      <title>验证损失增加且准确率停滞 [关闭]</title>
      <link>https://stackoverflow.com/questions/79326823/validation-loss-increasing-and-accuracy-is-stuck</link>
      <description><![CDATA[在此处输入图片说明
在此处输入图片说明

# 导入必要的库

导入 numpy 作为 np
导入 pandas 作为 pd
导入 matplotlib.pyplot 作为 plt
导入 os
导入 keras
导入 seaborn 作为 sns
导入 tensorflow 作为 tf
从 sklearn.model_selection 导入 train_test_split
从 tensorflow.keras.preprocessing.image 导入 ImageDataGenerator
从 tensorflow.keras 导入 Input
从 tensorflow.keras.models 导入 Sequential
从 tensorflow.keras.layers 导入 Conv2D， MaxPooling2D、Flatten、Dense、Dropout
来自 sklearn.metrics 导入分类报告、confusion_matrix
来自 tensorflow.keras.utils 导入 load_img、img_to_array
来自 tensorflow.keras.applications 导入 VGG16
来自 tensorflow.keras.models 导入模型
来自 tensorflow.keras.preprocessing 导入图像
来自 tensorflow.keras.layers 导入 BatchNormalization
来自 tensorflow.keras.callbacks 导入 ReduceLROnPlateau
来自 tensorflow.keras.callbacks 导入 EarlyStopping
来自 tensorflow.keras.optimizers 导入 Adam
来自 google.colab 导入 drive

# 数据集加载

drive.mount(&#39;/content/drive&#39;)
train_dir = &#39;/content/drive/MyDrive/CSE427/Project/Dataset/New Train&#39;
test_dir = &#39;/content/drive/MyDrive/CSE427/Project/Dataset/New Test&#39;

# 预处理数据 &amp;数据增强

train_datagen = ImageDataGenerator(
rescale=1./255,
sher_range=0.2,
zoom_range=0.2,
Horizo​​ntal_flip=True,
rotation_range=30,
bright_range=(0.8, 1.2),
width_shift_range=0.2,
height_shift_range=0.2,
channel_shift_range=30)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
train_dir,
target_size=(128, 128),
batch_size=32,
class_mode=&#39;binary&#39;)

validation_generator = test_datagen.flow_from_directory(
test_dir,
target_size=(128, 128),
batch_size=32,
class_mode=&#39;binary&#39;)

print(&#39;\n&#39;,train_generator.class_indices)

# 构建自定义 CNN 模型
from tensorflow.keras.regularizers import l2
model = Sequential([
Input(shape=(128, 128, 3)), 
Conv2D(32, (3, 3),activation=&#39;relu&#39;),
BatchNormalization(),
MaxPooling2D(pool_size=(2, 2)),
Dropout(0.3),
Conv2D(64, (3, 3),activation=&#39;relu&#39;),
BatchNormalization(),
MaxPooling2D(pool_size=(2, 2)),
Dropout(0.3),
Conv2D(128, (3, 3),activation=&#39;relu&#39;),
BatchNormalization(),
MaxPooling2D(pool_size=(2, 2)),
Flatten(),
Dense(128, 激活=&#39;relu&#39;, kernel_regularizer=l2(0.01)),
Dropout(0.5),
Dense(1, 激活=&#39;sigmoid&#39;)
])

model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

# 训练模型
lr_scheduler = ReduceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.5, waiting=5, min_lr=1e-6, verbose=1)
early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, waiting=5, restore_best_weights=True, verbose=1)

history = model.fit(
train_generator,
steps_per_epoch=len(train_generator),
epochs=30,
validation_data=validation_generator,
validation_steps=len(validation_generator),
callbacks=[early_stopping, lr_scheduler])

# 在测试数据上评估模型
test_loss, test_acc = model.evaluate(validation_generator)
print(f&quot;测试准确率：{test_acc*100}%&quot;)

请有人帮我确定模型实现是否正确。我是这个领域的新手。我尝试了很多次，但无法解决这个问题。但在一次迭代中，它给出了 91.3% 的准确率。我正在根据图像对适合公交车和不适合公交车进行分类。训练图像为 482 张，验证图像为 46 张]]></description>
      <guid>https://stackoverflow.com/questions/79326823/validation-loss-increasing-and-accuracy-is-stuck</guid>
      <pubDate>Fri, 03 Jan 2025 14:46:02 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：
&#39;super&#39; 对象没有属性 &#39;__sklearn_tags__&#39;。

当我在 RandomizedSearchCV 对象上调用 fit 方法时会发生这种情况。我怀疑它可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我希望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“sklearn.neighbors._base”导入名称“_check_weights”</title>
      <link>https://stackoverflow.com/questions/75633185/importerror-cannot-import-name-check-weights-from-sklearn-neighbors-base</link>
      <description><![CDATA[我正在尝试使用 Missforest 来处理表数据中的缺失值。
import sklearn
print(sklearn.__version__)
-&gt;1.2.1

import sklearn.neighbors._base
import sys
sys.modules[&#39;sklearn.neighbors.base&#39;] = sklearn.neighbors._base

!pip install missingpy
from missingpy import MissForest

到目前为止，它运行良好，但从昨天开始，出现了以下错误消息。
ImportError：无法从“sklearn.neighbors._base”导入名称“_check_weights”

我想知道如何处理这个错误。]]></description>
      <guid>https://stackoverflow.com/questions/75633185/importerror-cannot-import-name-check-weights-from-sklearn-neighbors-base</guid>
      <pubDate>Sat, 04 Mar 2023 01:48:43 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在同一层使用多个损失函数吗？</title>
      <link>https://stackoverflow.com/questions/62861773/can-we-use-multiple-loss-functions-in-same-layer</link>
      <description><![CDATA[我们可以在这个架构中使用多个损失函数吗：
我有两种不同类型的损失函数，并希望在最后一层使用它 [输出]
损失函数：

binary_crossentropy
自定义损失函数

我们可以这样做吗？
]]></description>
      <guid>https://stackoverflow.com/questions/62861773/can-we-use-multiple-loss-functions-in-same-layer</guid>
      <pubDate>Sun, 12 Jul 2020 13:37:51 GMT</pubDate>
    </item>
    <item>
      <title>CNN 中的反向传播（通过卷积层）和梯度</title>
      <link>https://stackoverflow.com/questions/42588047/backpropagation-through-convolutional-layer-and-gradients-in-cnn</link>
      <description><![CDATA[我正在学习使用卷积神经网络，并继续为这些神经网络编写自己的框架。
我被困在必须通过网络反向传播误差（增量）并计算梯度的部分。我知道 CNN 中的过滤器是 3D 的，所以我们有一些过滤器的宽度、高度和深度。
前馈很好。让我们看一下前馈步骤中某些层的输出计算公式：

为了进行卷积，l 层中过滤器的深度应与前一层 l-1 的输出 z 的输出通道数（深度）相同。因此，在这里，在这个公式中，我们将前一层的输出与当前层的权重进行卷积，这是有效的，因为这两个中的第三个坐标（深度）是相等的。现在，让我们检查一下误差反向传播的公式：

在这个公式中，我们有来自 l+1 层的 delta 和权重数组 w 的卷积。现在这就是让我困惑的地方，因为一般来说它们的第三个坐标（深度）并不总是相等的。考虑一下 VGGNet 架构，让我们看一下过滤器数量变化的三个连续层：
...
CONV3-128：[112x112x128] 内存：112*112*128=1.6M 权重：(3*3*128)*128 = 147,456
POOL2：[56x56x128] 内存：56*56*128=400K 权重：0
CONV3-256：[56x56x256] 内存：56*56*256=800K 权重：(3*3*128)*256 = 294,912
...
在过滤器数量从128 到 256（在 CONV3-256 层中），它具有上述激活（和误差增量）和权重的维度。但是，由于过滤器的深度（在本例中为 128）与其增量的第 3 维（在本例中为 256）不同，我该如何对这两个数组进行卷积？非常感谢任何能提供帮助的人。我觉得这很令人困惑，我在网上找不到太多这方面的帮助。它大多解释得不太清楚或被认为是“已知的”。]]></description>
      <guid>https://stackoverflow.com/questions/42588047/backpropagation-through-convolutional-layer-and-gradients-in-cnn</guid>
      <pubDate>Fri, 03 Mar 2017 20:38:26 GMT</pubDate>
    </item>
    </channel>
</rss>