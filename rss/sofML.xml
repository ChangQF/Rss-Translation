<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 07 Feb 2024 00:56:12 GMT</lastBuildDate>
    <item>
      <title>在非线性回归模型中测量 r2_score</title>
      <link>https://stackoverflow.com/questions/77951236/measuring-the-r2-score-in-a-non-linear-regression-model</link>
      <description><![CDATA[我想测量 kaggle 中可用的 kc_house_data 数据中的 r2_score，但我不明白为什么它给我一个负面结果。你能解释一下吗？
df[“组合”] = 卧室_norm + 浴室_norm + sqftliving_norm + sqftabove_norm + long_norm + lat_norm + sqftliving15_norm +yrbuilt_norm + Grade_norm + Floors_norm + sqftbasement_norm+condition_norm
cdf = df[[“组合”,“价格”]]

def log(a, Beta_1, Beta_2, Beta_3):
    y = (Beta_1 * (np.power(Beta_2, a)) + Beta_3)
    返回y
beta_3 = 0.10
贝塔_2 = 1.5
贝塔_1 = 1
x_data, y_data = (df[“组合”].值, df[“价格”].值)
x_data_norm = x_data / 最大值(x_data)
y_data_norm = y_data / max(y_data)
y_pred = log(x_data_norm, beta_1, beta_2, beta_3)
从 scipy.optimize 导入 curve_fit
popt, pcov = curve_fit(log, x_data_norm, y_data_norm)
x = np.linspace(4, 12, 21613)
x = x / 最大值(x)
plt.figure(figsize=(8,5))
y = log(x, *popt)
plt.plot(x_data_norm, y_data_norm, &#39;ro&#39;, label=&#39;data&#39;)
plt.plot(x, y, 线宽=3.0, 标签=&#39;适合&#39;)
plt.legend(loc=&#39;最佳&#39;)
plt.ylabel(&#39;价格&#39;)
plt.xlabel(&#39;组合&#39;)
plt.show()

从 sklearn.metrics 导入 r2_score
print(&quot;R2-score: %.2f&quot; % r2_score(y_data_norm , y))


我尝试在 sklearn.metrics 中使用 r2_score，我期望得到一个 0 到 1 之间的数字，但我不明白为什么它计算出负数
from sklearn.metrics import r2_score
print(&quot;R2-score: %.2f&quot; % r2_score(y_data_norm , y))
R2 分数：-59.51
]]></description>
      <guid>https://stackoverflow.com/questions/77951236/measuring-the-r2-score-in-a-non-linear-regression-model</guid>
      <pubDate>Tue, 06 Feb 2024 22:40:53 GMT</pubDate>
    </item>
    <item>
      <title>在 Rust 的“ort”包中使用 ONNX CLIP ViT-B-32，收到有关无效输入维度的错误</title>
      <link>https://stackoverflow.com/questions/77950750/using-onnx-clip-vit-b-32-in-rusts-ort-crate-getting-errors-about-invalid-inp</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77950750/using-onnx-clip-vit-b-32-in-rusts-ort-crate-getting-errors-about-invalid-inp</guid>
      <pubDate>Tue, 06 Feb 2024 20:43:57 GMT</pubDate>
    </item>
    <item>
      <title>图像的梯度批处理</title>
      <link>https://stackoverflow.com/questions/77950584/gradio-batch-processing-of-images</link>
      <description><![CDATA[我正在尝试使用 gradio.File() 处理 gradio 中的图像，但它需要一个 Zip 文件，然后我必须解压缩它并复制路径，然后打开图像。我的算法适用于 gr.image() 打开的图像，但是当我尝试对多个图像执行相同操作时，它无法正常工作。我试图找出答案，但没有成功。我想批量打开图像，它应该与 gr.Image() 相同，但多个图像。
我尝试了选择 zip 文件的方法，但它无法正确处理图像。但对于像 gr.Image() 这样的单个图像，它工作得很好。]]></description>
      <guid>https://stackoverflow.com/questions/77950584/gradio-batch-processing-of-images</guid>
      <pubDate>Tue, 06 Feb 2024 20:09:38 GMT</pubDate>
    </item>
    <item>
      <title>从头开始反向传播方法出错</title>
      <link>https://stackoverflow.com/questions/77949922/error-in-backpropagation-method-from-scratch</link>
      <description><![CDATA[我正在尝试制作一个预测加密货币价格的人工智能，现在我在反向传播方法中遇到了这个持续性错误（特别是关于 np.dot 语句中用于计算新权重的数组规模） ），我认为这可能是由于某个功能造成的，但我不知道如何纠正它。
类 Layer_Dense：
    def __init__(self, n_neurons, 权重, 偏差):
        self.n_neurons = np.array(n_neurons)
        self.weights = np.array(权重)
        self.biases = np.array(偏差)
    def 前向（自身，输入）：
        self.inputs = np.array(输入)
        self.output = np.dot(self.weights, 输入) + self.biases
    def 反向传播（自身，梯度，学习率 = 0.01）：
        梯度 = np.array(梯度)
        # 计算相对于权重和偏差的梯度
        weights_gradient = np.dot(梯度, self.inputs.T) / len(self.inputs)
        biases_gradient = np.sum(梯度) / len(self.inputs)

        # 使用一些优化算法（例如梯度下降）更新权重和偏差
        self.weights -= 学习率 * 权重梯度
        self.biases -= 学习率 *biases_gradient
        
        self.weights = np.array(self.weights)

        # 返回相对于下一层输入的梯度
        返回 np.dot(self.weights.T, 梯度)



[...]



神经网络层 = [
    Layer_Dense（neural_network_layers_and_its_neurons[1]，weights_list_of_matrises_for_continuation[0]，biases_matrix_continuation[0]），
    Layer_Dense（neural_network_layers_and_its_neurons[2]，weights_list_of_matrises_for_continuation[1]，biases_matrix_continuation[1]），
    Layer_Dense（neural_network_layers_and_its_neurons[3]，weights_list_of_matrises_for_continuation[2]，biases_matrix_continuation[2]），
    Layer_Dense（neural_network_layers_and_its_neurons[4]，weights_list_of_matrises_for_continuation[3]，biases_matrix_continuation[3]），
    Layer_Dense(neural_network_layers_and_its_neurons[5]、weights_list_of_matrises_for_continuation[4]、biases_matrix_continuation[4])
    ]




[...]




对于范围内的 i（0，Batch_size）：

        真实输出 = []

        对于 X 中的 i：
            
            当前输入=我

            # 前向传递各层
            对于 enumerate(neural_network_layers) 中的 a 层：
                层.forward（当前输入）
                如果 a != (len(neural_network_layers)-1):
                    当前输入 = 向前（层.输出）
                如果 a == (len(neural_network_layers)-1):
                    当前输入 = 层.输出
                    当前输入 = 当前输入[0]

            real_outputs.append（当前输入）

        loss_a、accuracy_a = F.accuracy_and_or_loss_in_one_output_NN(expected_outputs, real_outputs, 0)

        准确度.append(accuracy_a)
        loss.append(loss_a)
        
#这是我之前讨论过的函数

        loss_gradient = F.gradient_of_loss(expected_outputs, real_outputs)
        
        如果 a12 == 1：
            损失_b = 损失_a + 1e10
        
        当前梯度 = 损失梯度
        对于反向层（neural_network_layers）：
            current_gradient = layer.backpropagation(current_gradient,learning_rate)


def梯度损失（真实值，预测值）：
    true_values = np.array(true_values_)
    预测值 = np.array(预测值_)


    # 计算真实值和预测值之间的差异
    梯度 = 预测值 - 真实值

    返回梯度

我尝试过重塑数组，我尝试过转置它们，我已经改变了损失函数的梯度大约10次，我已经观看了3个关于偏导数的数学youtube视频，但没有任何效果]]></description>
      <guid>https://stackoverflow.com/questions/77949922/error-in-backpropagation-method-from-scratch</guid>
      <pubDate>Tue, 06 Feb 2024 18:02:18 GMT</pubDate>
    </item>
    <item>
      <title>Faiss GPU索引传递给拥抱面部训练器时无法序列化</title>
      <link>https://stackoverflow.com/questions/77949462/faiss-gpu-index-cannot-be-serialised-when-passed-to-hugging-face-trainer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77949462/faiss-gpu-index-cannot-be-serialised-when-passed-to-hugging-face-trainer</guid>
      <pubDate>Tue, 06 Feb 2024 16:45:32 GMT</pubDate>
    </item>
    <item>
      <title>Prophet 1.1.5 模型在通过 model_to_json 和 model_from_json 保存和加载后损坏</title>
      <link>https://stackoverflow.com/questions/77949186/prophet-1-1-5-model-is-corrupted-after-being-saved-and-loaded-via-model-to-json</link>
      <description><![CDATA[当模型拟合然后通过 model_to_json 和 model_from_json 保存和加载时，它不会提供准确的预测。如果跳过保存和加载过程并在拟合后立即使用模型进行预测，则不会出现问题。
加载 Prophet 模型后，显示 1970 年的所有数据相隔 1 秒，而不是 2023/24 相隔 15 分钟/1 小时。预测数据最终也以 1970 年代相隔 1 秒 (make_future_dataframe)。此外，预测数据往往不太准确。
当拟合和预测结合在一起时，绕过保存和加载阶段，Prophet 的行为符合预期。仅当我将模型保存到云存储桶或文件系统中时，才会出现此问题。
我比较了plot命令的输出。在保存和加载之前，其输出是准确的并且符合预期。
我怎样才能保存和保存？加载模型而不损坏模型？建议采取哪些故障排除步骤？]]></description>
      <guid>https://stackoverflow.com/questions/77949186/prophet-1-1-5-model-is-corrupted-after-being-saved-and-loaded-via-model-to-json</guid>
      <pubDate>Tue, 06 Feb 2024 16:06:33 GMT</pubDate>
    </item>
    <item>
      <title>对来自简单多元高斯的数据进行 VAE 训练会导致崩溃的重构分布</title>
      <link>https://stackoverflow.com/questions/77949137/training-vae-on-data-from-simple-multivariate-gaussian-leads-to-collapsed-recons</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77949137/training-vae-on-data-from-simple-multivariate-gaussian-leads-to-collapsed-recons</guid>
      <pubDate>Tue, 06 Feb 2024 16:00:22 GMT</pubDate>
    </item>
    <item>
      <title>梯度与损失不匹配</title>
      <link>https://stackoverflow.com/questions/77949060/gradient-does-not-match-loss</link>
      <description><![CDATA[我有一个综合实验，我在数据上拟合了一个模型，分布如下：

类别 0 - ~N(-1, 1)
1 级 - ~N(+1, 1)

该模型基本上将每个正数分类为 1，将负数分类为 0。到目前为止一切顺利。
在测试集中，我通过添加 b=3 更改了这两个类，所以现在：

类别 0 - ~ N(2, 1)
1 级 - ~ N(4, 1)

这是片段：
pbar = tqdm(cs)
对于 pbar 中的 c：
    偏差=变量(torch.tensor([c]).type(dtype),requires_grad=True)
    样本 = np.array(test_df.sample(frac=0.05).x)
    张量_x = torch.张量（样本，requires_grad=False）

    probs = calc_p(tensor_x, 偏差)
    ents = calc_ents(概率)
    
    使用 torch.no_grad()：
        protected_ents, 保护信息 = 保护器.protect(ents)

    损失 = mse_loss(ents, protected_ents.cpu())
    loss.backward()
    bias.grad.data.zero_()

现在我绘制了 X 数据分布的梯度和损失：

由于某种原因，损失看起来是正确的，但梯度却不然，当损失最大化或最小化时，它们应该交叉 0。
如果我改变线路：
loss = mse_loss(ents, protected_ents.cpu())

至：
loss = mse_loss(ents, torch.zeros_like(ents).type(torch.float64).cpu())

然后它按预期工作：

有谁知道是什么原因吗？]]></description>
      <guid>https://stackoverflow.com/questions/77949060/gradient-does-not-match-loss</guid>
      <pubDate>Tue, 06 Feb 2024 15:50:28 GMT</pubDate>
    </item>
    <item>
      <title>如何获得更好的 AUC 分数？ （和累积提升）</title>
      <link>https://stackoverflow.com/questions/77948795/how-to-yield-a-better-auc-score-and-cumulative-lift</link>
      <description><![CDATA[我有一个包含 60 万条记录和 173 个专注于二元分类的特征的数据集。班级比例约为 98.7:1.3（1.3% 目标=1）。
目前，我正在尝试提高模型的性能，该模型的 AUC 为 73%。此外，我对前 2% 的累积提升是 10.41，对前 5% 的累积提升是 5.92。由于我只会针对正面预测分数的前 2-5%，因此我并不特别关心混淆矩阵阈值或改进矩阵值（FP、FN）。
我通过转换（交互，^2）和手动数学计算执行了特征工程。
尽管如此，在没有工程化特征的情况下训练模型后，AUC 分数大致相同，在没有工程化特征的模型中，累积提升略高。我使用了一个自动功能选择工具，该工具使用 RFE 和 XGBoost 来指示所选功能。
我应该注意到，我训练了模型，该模型具有 3 个周期的下采样数据集（3 个周期中每个周期 40k），分类比为 93.5:6.5（6.5% 目标=1），并使用常规的第 4 个周期验证数据集上的数据（原始 1.3% tareget=1 率）。我使用 H20 来训练我的模型（选择 XGBoost）。
如何提高模型得分和模型质量？我知道模型训练涉及插补，但我应该在预处理/清理阶段尝试使用 SimpleImputer、IterativeImputer 或/和 KNNImputer 吗？这会改善我的模型吗？
我尝试使用或不使用我的工程特征重新训练多个模型，并返回到第 1 步并创建更多变量（工程）以尝试帮助我的 AUC 和提升分数。]]></description>
      <guid>https://stackoverflow.com/questions/77948795/how-to-yield-a-better-auc-score-and-cumulative-lift</guid>
      <pubDate>Tue, 06 Feb 2024 15:11:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 对大多数字符串数据集进行分类或聚类的解决方案[关闭]</title>
      <link>https://stackoverflow.com/questions/77948741/a-solution-for-categorizing-or-clustering-mostly-string-dataset-using-tensorflow</link>
      <description><![CDATA[我想使用机器学习对主要字符串数据集进行分类/聚类。我不确定实现此目的的最佳方法是什么。我想使用 Tensorflow 来完成这个任务。也许有人有开发这样的解决方案的经验并且可以推荐一些东西。
至于我的数据集主要包含字符串值的问题，据我所知，我可以使用 pandas 类别将此数据转换为算法可以理解的内容。如果有更好的解决方案请务必推荐。
到目前为止，我一直在学习 Tensorflow 并分析我拥有的数据集（清理数据、理解结构）。]]></description>
      <guid>https://stackoverflow.com/questions/77948741/a-solution-for-categorizing-or-clustering-mostly-string-dataset-using-tensorflow</guid>
      <pubDate>Tue, 06 Feb 2024 15:03:49 GMT</pubDate>
    </item>
    <item>
      <title>keras.LSTM 如何将 3D 输入转换为 2D 输出？</title>
      <link>https://stackoverflow.com/questions/77946209/how-keras-lstm-converts-3d-input-to-2d-output</link>
      <description><![CDATA[根据 keras 的 LSTM 文档，输入应该是具有形状（批量、时间步长、特征）的 3D 张量
输出将为（批次，单位），其中单位是我们想要从 LSTM 单元获得的数字特征。
据我所知，lstm 的单个单元格将隐藏状态、单元格状态和单个数字作为时间戳 t 的输入，并将其输出以 c(t+1) 和 h(t+1) 的形式传递到下一个单元格。但从文档代码来看，它正在生成 2D 形式的输出？
输入 = np.random.random((32, 10, 8))
lstm = keras.layers.LSTM(4)
输出 = lstm(输入)
输出形状
(32, 4)

问题 1：向量表示如何传递给 LSTM？ （在每个时间戳处，它传递 8 个特征。如果有 8 个 lstm 单元并行运行，则输出大小也应为 8）
问题2：最终输出的大小如何为4。（如果我们忽略批量大小）]]></description>
      <guid>https://stackoverflow.com/questions/77946209/how-keras-lstm-converts-3d-input-to-2d-output</guid>
      <pubDate>Tue, 06 Feb 2024 08:25:41 GMT</pubDate>
    </item>
    <item>
      <title>预处理新数据以从 PyCaret 中的现有模型进行预测[关闭]</title>
      <link>https://stackoverflow.com/questions/77938501/preprocessing-new-data-for-predictions-from-an-existing-model-in-pycaret</link>
      <description><![CDATA[我试图使用 Python 的 PyCaret 库开发一个 ML 模型来分析具有 34 个特征（列）的数据集。我运行 setup() 函数并注意到，由于编码，它将原始数据集扩展至 58 个特征。它还进行了插补和转换。经过这些步骤后，该库将基础数据分为训练集和测试集。
最终根据训练数据集从compare_models()中选择合适的模型。由此，我在测试集上使用 Predict_model() 进行了预测，并对结果感到满意。
我的提供商现在如何向我提供新数据，我想针对新数据运行 Predict_model() ，以便我可以将这些预测返回给我的提供商。为此，我使用了 Predict_model() 函数的“data”参数，但是，我得到的错误低于以下最小可行代码示例。
如何确保我收到的新数据经过适当的预处理，以便在 predict_model() 的“data”参数中使用？
带注释的最小可行代码示例：
&lt;前&gt;&lt;代码&gt;# 库
导入 pycaret
从 pycaret.regression 导入 *
将 pandas 导入为 pd

# 通过从 CSV 导入创建一个新的 DF
# 这个 DF 有 34 列
baseDf = pd.read_csv(“wave1data.csv”)

# 在 baseDf 上进行设置
# 转换/编码的 DF 结果有 58 列，并在训练/测试数据集之间进行分割
s = setup(baseDf, target = “我的目标功能”, session_id = 64)

# 基本模型对比
最好=比较模型（）

# 预测测试分割
wave1_pred = 预测模型（最佳）

# 此时，我有一个理想的模型，但它基于具有 58 列和多个转换/插补的 DF
# 我尝试使用以下方法对全新的未见过的数据进行预测：
wave2Df = pd.read_csv(“wave2data.csv”)
wave2_pred = Predict_model（最佳，数据= wave2Df）

错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
KeyError Traceback（最近一次调用）在&lt;细胞系：2&gt;()
      1 # 预测第 2 波
----&gt; 2 wave2_pred = Predict_model（最佳，数据= wave2Df）

/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py 中的 5 帧
_raise_if_missing(self, key, 索引器, axis_name) 6131 6132 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())
-&gt;第6133章 6134 第6135章

KeyError：“[&#39;Endorsed By&#39;] 不在索引中”

（如果我理解正确的话，wave2Df 数据与“最佳”数据的结构不同，这是有道理的。）
如何克服此错误并使用我根据新传入数据（在本例中为 wave2Df 的内容）生成的“最佳”模型运行 predict_model()？]]></description>
      <guid>https://stackoverflow.com/questions/77938501/preprocessing-new-data-for-predictions-from-an-existing-model-in-pycaret</guid>
      <pubDate>Mon, 05 Feb 2024 03:32:23 GMT</pubDate>
    </item>
    <item>
      <title>比较不同模型的 F1 分数的概率阈值图 [关闭]</title>
      <link>https://stackoverflow.com/questions/77935679/comparing-probability-threshold-graphs-for-f1-score-for-different-models</link>
      <description><![CDATA[下面是两个并排的图，针对不平衡的数据集。

我们有一个非常大的不平衡数据集，我们正在以不同的方式处理/转换。每次转换后，我们都会对其运行 xgboost 估计器。
左侧是三个 xgboost 模型在三个不同转换数据集上的 PR 曲线。从左图可以看出，3条PR曲线全部重叠；事实上，其中两个（红色和绿色）曲线下的面积是相同的。
右侧是来自相同三个模型但在不同概率阈值下的 F1 分数（根据测试数据计算）的图。左右图中模型的颜色匹配。红色和绿色模型在不同概率阈值下的峰值 F1 分数大致相同。蓝色模型的峰值 F1 分数略低于其他两个模型的峰值 F1 分数。我的问题是：
A。我可以说，绿色模型比红色模型“远”好，因为它的 F1 分数在很大的概率阈值范围内相当稳定，而红色模型的 F1 分数随着概率阈值的微小变化而迅速下降。概率阈值。
b.红色和蓝色这两种型号中，哪一种更好，为什么？
如果您能给出合理的答复，我将不胜感激，因为它可能对我的工作有所帮助。顺便说一句，我已经进行了大量关于 F1 分数、AUC 和 PR 曲线的讨论，包括这个。
简单地说，这个问题涉及如何解释不同模型的 F1 分数阈值图，因为 PR 曲线没有得出结论。]]></description>
      <guid>https://stackoverflow.com/questions/77935679/comparing-probability-threshold-graphs-for-f1-score-for-different-models</guid>
      <pubDate>Sun, 04 Feb 2024 11:59:48 GMT</pubDate>
    </item>
    <item>
      <title>导入bertopic时导入UMAP的问题</title>
      <link>https://stackoverflow.com/questions/75158273/problem-in-importing-umap-while-importing-bertopic</link>
      <description><![CDATA[所以我的代码一切正常，然后突然 hdbscan 不再工作了，然后我重新安装了所有软件包，现在我遇到了 umap 问题。
我按照此处和其他论坛中的建议进行了操作，卸载并重新安装了 umap-learn 和 bertopic 。我可以将 umap 导入为 import umap 或 import umap.umap_ as UMAP ，问题是当我导入 bertopic 时。我尝试过：
导入bertopic

和
导入 umap.umap_ 作为 UMAP
导入bertopic

和
导入umap
导入bertopic

和
导入umap
从 bertopic 导入 BERTopic

最后：
导入 umap.umap_ 作为 UMAP
从 bertopic 导入 BERTopic

在所有情况下，当我导入 bertopic 时都会出现问题： ImportError: Cannot import name &#39;UMAP&#39; from &#39;umap&#39; (unknown location) 。我也重新启动机器几次。我不认为这个问题与环境有关，因为我之前在相同的代码运行时一直使用相同的环境： Python 3.10.7 和 Visual Code Studio 1.74.3 。 bertopic版本为0.13.0，umap-learn版本为0.5.3]]></description>
      <guid>https://stackoverflow.com/questions/75158273/problem-in-importing-umap-while-importing-bertopic</guid>
      <pubDate>Wed, 18 Jan 2023 11:06:19 GMT</pubDate>
    </item>
    <item>
      <title>BUFFER_SIZE 在 Tensorflow 数据集改组中起什么作用？</title>
      <link>https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling</link>
      <description><![CDATA[所以我一直在玩这个代码：https://www.tensorflow。 org/tutorials/generative/dcgan 并几乎对其功能有了一个很好的了解。但是，我不太清楚 BUFFER_SIZE 变量的用途是什么。我怀疑它可能用于创建大小为 BUFFER_SIZE 的数据库子集，然后从该子集中获取批次，但我没有看到这一点，也找不到人解释它。
所以，如果有人能解释一下 BUFFER_SIZE 的作用，我将不胜感激❤]]></description>
      <guid>https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling</guid>
      <pubDate>Thu, 15 Oct 2020 13:14:54 GMT</pubDate>
    </item>
    </channel>
</rss>