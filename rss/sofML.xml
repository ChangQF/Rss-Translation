<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 09 Sep 2024 03:19:34 GMT</lastBuildDate>
    <item>
      <title>为什么 nn.Linear(in_features, out_features) 在 PyTorch 中使用形状为 (out_features, in_features) 的权重矩阵？</title>
      <link>https://stackoverflow.com/questions/78963755/why-does-nn-linearin-features-out-features-use-a-weight-matrix-of-shape-out</link>
      <description><![CDATA[我试图理解为什么 PyTorch 的 nn.Linear(in_features, out_features) 层的权重矩阵具有形状 (out_features, in_features) 而不是 (in_features, out_features)。
从基本矩阵乘法的角度来看，具有形状 (in_features, out_features) 似乎可以消除在乘法过程中转置权重矩阵的需要。例如，对于形状为 (batch_size, in_features) 的输入张量 x，与形状为 (in_features, out_features) 的权重矩阵相乘将直接产生形状为 (batch_size, out_features) 的输出，而无需转置操作。
但是，PyTorch 将权重矩阵定义为 (out_features, in_features)，这意味着它在前向传递过程中会被转置。这种设计有什么好处？它如何与线性代数和神经网络实现的更广泛原则保持一致？这种选择背后是否有任何效率或一致性考虑使其更可取？
我很感激任何关于这一设计决策背后理由的见解。]]></description>
      <guid>https://stackoverflow.com/questions/78963755/why-does-nn-linearin-features-out-features-use-a-weight-matrix-of-shape-out</guid>
      <pubDate>Mon, 09 Sep 2024 03:08:49 GMT</pubDate>
    </item>
    <item>
      <title>无法让 MMCV 构建来检测我的 Conda Env 的 Cuda</title>
      <link>https://stackoverflow.com/questions/78963624/cant-get-mmcv-build-to-detect-my-conda-envs-cuda</link>
      <description><![CDATA[我通过 conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia 安装了 pytorch，然后安装了 conda install cuda=11.8 -c nvidia 以获取构建工具，但是在运行 pip install -U openmim 然后运行 ​​mim install mmcv 之后，我得到了一个 Thedetected CUDA version (12.5) mismatches the version that was used to compile PyTorch (11.8). Please make sure to use the same CUDA editions. 错误，即使运行 nvcc -V 显示 cuda 11.8。我的主机正在运行 cuda 12.5。我该如何解决这个问题？（以及 cuda、cudatoolkit、cudatoolkit-dev 和 cuda-toolkit 等所有 cuda conda 包之间有什么区别？）]]></description>
      <guid>https://stackoverflow.com/questions/78963624/cant-get-mmcv-build-to-detect-my-conda-envs-cuda</guid>
      <pubDate>Mon, 09 Sep 2024 01:22:33 GMT</pubDate>
    </item>
    <item>
      <title>结合语音、面部表情和文本数据进行实时心理健康监测的挑战</title>
      <link>https://stackoverflow.com/questions/78963600/challenges-in-combining-speech-facial-expression-and-text-data-for-real-time-m</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78963600/challenges-in-combining-speech-facial-expression-and-text-data-for-real-time-m</guid>
      <pubDate>Mon, 09 Sep 2024 01:01:00 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 的机器学习模型中处理具有数值的对象数据类型变量？[关闭]</title>
      <link>https://stackoverflow.com/questions/78963448/how-to-deal-with-object-data-type-variables-with-numeric-values-in-machine-learn</link>
      <description><![CDATA[我尝试在 Python 中构建机器学习模型，我有几个变量的数据类型为“object”，值如下：411,71 等。我的问题是：

我是否必须将这种具有 411,71 等值的对象变量更改为数字类型才能使用 xgboost 或随机森林等算法？

如何在我的数据集中找到适合将数据类型更改为数字的“object”类型的变量，即具有 411,71 等值的变量，以及在这种情况下是否也需要将“，”更改为“。”？你能给我展示一下 Python Pandas 中的示例代码来做到这一点吗？

]]></description>
      <guid>https://stackoverflow.com/questions/78963448/how-to-deal-with-object-data-type-variables-with-numeric-values-in-machine-learn</guid>
      <pubDate>Sun, 08 Sep 2024 22:50:51 GMT</pubDate>
    </item>
    <item>
      <title>如何在增加训练数据时调整 LightGBM 参数（如“min_child_samples”）？</title>
      <link>https://stackoverflow.com/questions/78963446/how-to-adjust-lightgbm-parameters-like-min-child-samples-when-increasing-train</link>
      <description><![CDATA[我正在训练 LightGBM 模型，目前面临着增加训练数据量时参数调整的困境。
最初，我将数据集分成 90% 用于训练，10% 用于测试。使用网格搜索，我找到了模型的最佳参数。现在，我想利用 100% 的数据来训练模型，使其尽可能强大。
我的问题是关于 min_child_samples 等参数，它与数据量有关。当我将数据从 90% 增加到 100% 时，我应该将 min_child_samples 保持为与 90% 数据训练期间找到的值相同吗？还是应该因为数据量增加了而进行调整？
有人可以提供如何处理此问题的指导，或者分享任何最佳实践吗？]]></description>
      <guid>https://stackoverflow.com/questions/78963446/how-to-adjust-lightgbm-parameters-like-min-child-samples-when-increasing-train</guid>
      <pubDate>Sun, 08 Sep 2024 22:50:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 Raspberry Pi 5 在 Edge TPU 上运行 YOLOv8 分割模型时出现 KeyError</title>
      <link>https://stackoverflow.com/questions/78963308/keyerror-when-running-yolov8-segmentation-model-on-edge-tpu-with-raspberry-pi-5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78963308/keyerror-when-running-yolov8-segmentation-model-on-edge-tpu-with-raspberry-pi-5</guid>
      <pubDate>Sun, 08 Sep 2024 21:01:01 GMT</pubDate>
    </item>
    <item>
      <title>优化心理健康应用程序中的实时多模式数据集成和机器学习[关闭]</title>
      <link>https://stackoverflow.com/questions/78962995/optimizing-real-time-multimodal-data-integration-and-machine-learning-in-a-menta</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78962995/optimizing-real-time-multimodal-data-integration-and-machine-learning-in-a-menta</guid>
      <pubDate>Sun, 08 Sep 2024 17:57:14 GMT</pubDate>
    </item>
    <item>
      <title>如何根据多项评估对推荐路线与实际路线的相似度进行分类？[关闭]</title>
      <link>https://stackoverflow.com/questions/78962013/how-to-classify-the-similarity-between-a-recommended-and-an-actual-route-based-o</link>
      <description><![CDATA[我需要对应用程序推荐的路线和司机所走的路线是否相似进行分类。
我有一个包含以下变量的数据集：

id_viaje（行程 ID）
evaluador（评估者）——有 8 个不同的评估者
evaluacion（评估）——评估者的判断：相似、不相似或不确定
ruta_real（实际路线的点集）
ruta_estimada（估计路线的点集）

我已经做了一些探索性数据分析，以下是一些观察结果：

同一个评估者有时会对同一次行程进行两次评估，并给出相互矛盾的结果（一次说路线相似，另一次说不相似）。
在某些情况下在某些情况下，他们说他们不确定，但当我可视化路线时，它们实际上是相同的。
对于同一次旅行，不同的评估者会有相互矛盾的意见。

我对如何处理这个问题有些疑问：
对于评估者说“不确定”的情况，我计划为“相似”和“不相似”分配 0.5 的权重。 （或者，我可以忽略这些，或者先建立一个初始模型对这些情况进行分类，然后使用包含所有数据的最终模型。）
由于该任务似乎涉及对实际路线和拟议路线之间的相似性进行分类，而不是建立传统模型，因此似乎基于距离进行分类是最好的方法。
关于如何处理这些情况或计算路线相似性有什么建议吗？
我有这种数据
{&#39;journey_id&#39;: &#39;9cd6cd52-54c5-11ec-ae0a-0d030544d074&#39;,
&#39;annotator&#39;: 3,
&#39;annotation&#39;: &#39;两者相同&#39;,
&#39;estimated_route&#39;: [[-21.11149, -60.21455],
[-12.11145, -77.04671],
[-12.11139, -77.04664],
[-12.11134, -77.04659]
...],
&#39;real_route&#39;: 
[[-21.77149, -60.17455],
[-12.11139, -77.04659],
[-12.11139, -77.04671]]}

实际路线和估算路线并不总是具有相同长度的点]]></description>
      <guid>https://stackoverflow.com/questions/78962013/how-to-classify-the-similarity-between-a-recommended-and-an-actual-route-based-o</guid>
      <pubDate>Sun, 08 Sep 2024 09:35:49 GMT</pubDate>
    </item>
    <item>
      <title>什么是局部梯度错位？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78961861/what-is-local-gradient-misalignment</link>
      <description><![CDATA[我在阅读FedPRoto相关论文时，文章中提到了这个概念，但是我却无法理解它的具体定义。
但是我似乎无法从如此少的信息中找到明确的答案。]]></description>
      <guid>https://stackoverflow.com/questions/78961861/what-is-local-gradient-misalignment</guid>
      <pubDate>Sun, 08 Sep 2024 07:57:37 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 中的 Autograd Trainstep 中的 Lightning</title>
      <link>https://stackoverflow.com/questions/78956646/autograd-in-pytorch-lightning-in-trainstep</link>
      <description><![CDATA[我想实现一个基于 Pytorch Lightning 的 ML 训练，其中我使用 autograd 功能进行训练损失计算：
def training_step(self, batch, batch_idx):
x, y = batch
y_hat = self(x)
loss = self.loss_function(y_hat, y)
return loss

X 的每个样本 x 都是一个二维向量 x = [v, a]。
在训练步骤中，我想计算 y_hat 相对于 的梯度。到 v。
损失进一步通过以下方式计算：
loss = mse(y,y_hat) + mse(gradient,gradient_hat)

其中给出了（真实）梯度。
到目前为止，尝试了 y_hat.backward() 的（典型）方法，但无法使其工作：
def training_step(self, batch, batch_idx):
x, y = batch
x.requires_grad_(True) # 确保我们跟踪 x 的梯度
y_hat = self(x)

# 计算 y_hat 相对于 v 的梯度（x[:, 0]）
v = x[:, 0]
grads = torch.autograd.grad(y_hat, v, grad_outputs=torch.ones_like(y_hat), create_graph=True)[0] # ...
]]></description>
      <guid>https://stackoverflow.com/questions/78956646/autograd-in-pytorch-lightning-in-trainstep</guid>
      <pubDate>Fri, 06 Sep 2024 10:08:53 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Keras 中使用 flow_from_directory 和多个目录实现多输出神经网络</title>
      <link>https://stackoverflow.com/questions/78951880/how-to-use-flow-from-directory-with-multiple-directories-for-multi-output-neural</link>
      <description><![CDATA[我需要使用 Keras 中的 flow_from_directory 从多个目录加载图像。我的目录结构如下：
Images_folder/ 
═── Carpet_1/ 
│ ═── training/ 
│ │ ═── class_1/ 
│ │ ═── class_2/ 
│ ═── validation/ 
═── Carpet_2/ 
│ ═── training/ 
│ │ ═── class_1/ 
│ │ ═── class_2/ 
│ ═── validation/ 
...

每个“Carpet”目录（例如 Carpet_1、Carpet_2）包含相同的一组类（class_1、class_2 等）。我想使用来自所有这些目录的图像来训练 CNN。我的目标是构建一个多输出神经网络，其中一个输出预测“Carpet”数字（1、2、3、...），另一个输出预测该地毯内的类别。
鉴于这种结构，我如何使用 ImageDataGenerator 或 Keras 中的任何其他方法来加载和预处理这些图像？有没有办法将所有这些目录中的图像组合成一个生成器，同时仍然允许我区分不同的地毯？]]></description>
      <guid>https://stackoverflow.com/questions/78951880/how-to-use-flow-from-directory-with-multiple-directories-for-multi-output-neural</guid>
      <pubDate>Thu, 05 Sep 2024 07:56:05 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用 Google Teachable 机器模型作为对象检测模型吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78951811/can-i-use-google-teachable-machine-model-as-object-detection-model</link>
      <description><![CDATA[我正在开发一款带有对象检测功能的移动自动收银应用程序。我面临的问题是，自定义项目的变体太多了（大约 250 个类别）。如果我要对以前的模型（如 MobileNet 或 YOLO）进行微调，这将花费太多时间，因为我仍然需要创建 POS、数据库和集成热敏打印机。
那么，我是否可以只使用 Google Teachable Machine 来处理我的对象检测数据集（假设背景相同），而不是对以前的模型进行微调？
应用程序的工作方式是，收银员将在白色背景上拍摄买家想要购买的商品的照片，然后应用程序将自动检测出哪些商品在画面中（使用 Google Teachable Machine .tflite 模型）。
Teachable Machine .tflite 模型是否可以替换下面代码中的 modelPath？
private fun runObjectDetection(bitmap: Bitmap) {
// 步骤 1：创建 TFLite 的 TensorImage 对象
val image = TensorImage.fromBitmap(bitmap)

// 步骤 2：初始化检测器对象
val options = ObjectDetector.ObjectDetectorOptions.builder()
.setMaxResults(5)
.setScoreThreshold(0.5f)
.build()
val detector = ObjectDetector.createFromFileAndOptions(
this, // 应用程序上下文
**&quot;model.tflite&quot;, **
options
)
// 步骤 3：将给定的图像输入模型并打印检测结果
val results = detector.detect(image)

// 步骤 4：解析检测结果并显示
debugPrint(results)

val resultToDisplay = results.map {
// 获取 top-1 类别并制作显示文本
val category = it.categories.first()
val text = &quot;${category.label}, ${category.score.times(100).toInt()}%&quot;

// 创建数据对象，用于显示检测结果
DetectionResult(it.boundingBox, text)
}

// 将检测结果绘制到位图上并显示。
val imgWithResult = drawDetectionResult(bitmap, resultToDisplay)
runOnUiThread {
inputImageView.setImageBitmap(imgWithResult)
}
}
]]></description>
      <guid>https://stackoverflow.com/questions/78951811/can-i-use-google-teachable-machine-model-as-object-detection-model</guid>
      <pubDate>Thu, 05 Sep 2024 07:38:34 GMT</pubDate>
    </item>
    <item>
      <title>使用神经网络将输入分成几组[关闭]</title>
      <link>https://stackoverflow.com/questions/78950639/using-a-neural-network-to-separate-inputs-into-groups</link>
      <description><![CDATA[原始问题如下：
许多粒子撞击某些探测器，我们从这些事件中获得的信息是每次激活的坐标。探测器层层相继，我们可以为每个粒子绘制一些轨迹，只知道它经过的几个有探测器的坐标。
现在神经网络的问题是向它提供一段时间内发生的所有撞击坐标，并让它返回哪些撞击属于一个粒子，哪些属于另一个粒子
我对输出的唯一想法是给遵循相同轨迹的每个撞击系列编号，编号相同，最后得到一个向量“命名”每个输入坐标。我怀疑神经网络是否可以“即兴”这样的标签，因为理论上它应该迭代的每个数字不一定与任何可能的输入有联系，唯一的条件是当它们不属于同一个粒子时，它们彼此不同。
有没有更好的方法可以做到这一点？这是否可以作为解决问题的合理方法？]]></description>
      <guid>https://stackoverflow.com/questions/78950639/using-a-neural-network-to-separate-inputs-into-groups</guid>
      <pubDate>Wed, 04 Sep 2024 21:41:04 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 尽管进行了子采样并且没有设置种子，仍然表现确定性吗？</title>
      <link>https://stackoverflow.com/questions/76691875/xgboost-behaving-deterministically-despite-subsampling-and-not-setting-seed</link>
      <description><![CDATA[据我所知，XGBoost（版本 1.7.4）中的子采样是随机进行的，因此应该将随机行为引入 xgboost - 随机梯度下降也应如此。但是，当我在相同的数据分割上训练/测试 xgboost 时，尽管没有在任何地方设置随机状态，但我总是得到相同的结果，这是为什么？
在定义 XGBoostRegressor 时，我尝试在有和没有 seed=42 的情况下运行代码。我为精心挑选的数据执行了此操作，以便我知道随机行为只能源自 XGBoost 模型本身，而不是来自变量数据分割。]]></description>
      <guid>https://stackoverflow.com/questions/76691875/xgboost-behaving-deterministically-despite-subsampling-and-not-setting-seed</guid>
      <pubDate>Sat, 15 Jul 2023 01:24:32 GMT</pubDate>
    </item>
    <item>
      <title>机器学习：使用卷积神经网络将图像分为 3 类（狗、猫或非猫）</title>
      <link>https://stackoverflow.com/questions/40853349/machine-learning-image-classification-into-3-classes-dog-or-cat-or-neither-us</link>
      <description><![CDATA[如果您能帮我仔细思考一下，我将不胜感激。我有一个分类器，可以成功地将图像分类为狗或猫，并且准确度很高。我有一个很好的数据集来训练分类器。到目前为止没有问题。
我有大约 20,000 张狗和 20,000 张猫的图像。
但是，当我尝试呈现其他图像（如汽车、建筑物或老虎）时，这些图像中既没有狗也没有猫，我希望分类器的输出为“既不是”。现在，分类器显然试图将所有东西都分类为狗或猫，这是不正确的。
问题 1：
我该如何实现这一点？我是否需要有第三组不包含狗或猫的图像，并在这些额外的图像上训练分类器以将其他所有图像识别为“既不”？
大致来说，我需要多少张非狗/猫类别的图像才能获得良好的准确率？由于非狗/猫图像域非常大，大约 50,000 张图像就可以了？或者我是否需要更多图像？
问题 2：
我可以使用 Imagenet 训练的 VGG16 Keras 模型作为初始层，并在顶部添加 DOG/CAT/Neither 分类器作为全连接层，而不是使用自己的图像数据训练自己的分类器？
查看此示例以加载预先训练的 imagenet 模型
非常感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/40853349/machine-learning-image-classification-into-3-classes-dog-or-cat-or-neither-us</guid>
      <pubDate>Mon, 28 Nov 2016 20:58:33 GMT</pubDate>
    </item>
    </channel>
</rss>