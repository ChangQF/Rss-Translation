<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 17 Jan 2025 12:31:21 GMT</lastBuildDate>
    <item>
      <title>将 Tensorflow 模型工件部署到 Sagemaker</title>
      <link>https://stackoverflow.com/questions/79363695/deploying-a-tensorflow-model-artifact-to-sagemaker</link>
      <description><![CDATA[我正在尝试将 TensorFlow 模型部署到 Sagemaker 端点。我在 generic_graph.pb 处有模型工件，在 labels.txt 处有模型标签。
我首先创建了一个包含以下内容的 tar 文件：
# #model 目录结构 
# #model.tar.gz
# └── &lt;model_name&gt;
# └── &lt;version_number&gt;
# ═── saved_model.pb
# └── 变量
# ═── labels.txt

我将文件上传到 S3 中的存储桶。然后，我尝试使用以下代码部署模型：
sagemaker_session = sagemaker.Session()
role = &#39;my-role&#39;

model = TensorFlowModel(model_data=&#39;s3://my-bucket/model.tar.gz&#39;,
role=role,
framework_version=&#39;2.3.0&#39;)

predictor = model.deploy(initial_instance_count=1, instance_type=&#39;ml.m5.large&#39;)

我的 cloudwatch 日志中不断出现以下错误：
ValueError: 未找到 SavedModel 包！

不确定还能尝试什么。]]></description>
      <guid>https://stackoverflow.com/questions/79363695/deploying-a-tensorflow-model-artifact-to-sagemaker</guid>
      <pubDate>Fri, 17 Jan 2025 05:19:16 GMT</pubDate>
    </item>
    <item>
      <title>如何处理中间阶段的 NaN 损失</title>
      <link>https://stackoverflow.com/questions/79362342/how-to-handle-nan-losses-at-intermediate-epochs</link>
      <description><![CDATA[import torch
import torch.nn as nn

class PINN(nn.Module):
def __init__(self, input_dim, output_dim, hidden_​​layers, neurons_per_layer):
super(PINN, self).__init__()
layer = []
layer.append(nn.Linear(input_dim, neurons_per_layer))
for _ in range(hidden_​​layers):
layer.append(nn.Linear(neurons_per_layer, neurons_per_layer))
layer.append(nn.Linear(neurons_per_layer, output_dim))
self.network = nn.Sequential(*layers)

def forward(self, x):
return self.network(x)

# 示例：生成随机输入数据
inputs = torch.rand((1000, 3)) # 3D 输入坐标

model = PINN(input_dim=3, output_dim=3, hidden_​​layers=4, neurons_per_layer=64)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 10000
for epoch in range(epochs):
optimizer.zero_grad() 

nn_output = model(inputs) # 计算 NN 预测
# 根据 nn_output 计算损失
loss = loss_func(nn_output) # --&gt; loss = NaN，如何继续？

loss.backward() 
optimizer.step()

将输入传递到网络后，基于物理的损失函数（NN 输出函数）可能会产生 NaN，因为 NN 输出不合适。
在中间阶段，我可以接受违反物理定律，但优化器（例如 Adam）是否能够从返回的 NaN 损失中恢复？
（MATLAB 中的许多优化器可以正确处理 NaN，并希望目标函数在无法进行前向评估的点返回 NaN。这是一个尝试不同点的指标。）]]></description>
      <guid>https://stackoverflow.com/questions/79362342/how-to-handle-nan-losses-at-intermediate-epochs</guid>
      <pubDate>Thu, 16 Jan 2025 16:17:08 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 AI 在 GPU 上的训练比 CPU 慢很多</title>
      <link>https://stackoverflow.com/questions/79362113/why-is-my-ai-training-on-gpu-is-a-lot-slower-than-cpu</link>
      <description><![CDATA[我目前正在训练我的简单预测 AI，但我的 GPU 训练速度为每 epoch 40S，而我的 CPU 训练速度为每 epoch 9S
我的 CPU 是 i7-4720HQ，我的 GPU 是 Nvidia 950m
这是我的代码
`import tensorflow as tf
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

df = pd.read_csv(&#39;Updated_Train.csv&#39;)
df = df.drop(columns=&#39;date&#39;)
df = df.drop(columns=&#39;id&#39;)
label_encoder = LabelEncoder()

df[&#39;Country&#39;] = label_encoder.fit_transform(df[&#39;Country&#39;])
df[&#39;Store&#39;] = label_encoder.fit_transform(df[&#39;Store&#39;])
df[&#39;Product&#39;] = label_encoder.fit_transform(df[&#39;Product&#39;])

x = df.iloc[:, :-1]
y = df.iloc[:, -1]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = (
train_dataset
.shuffle(buffer_size=1000) # 打乱数据
.batch(32) # 批次大小为 32
.prefetch(tf.data.AUTOTUNE) # 预取以提高管道性能
)

test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)
with tf.device(&#39;/CPU:0&#39;):
输入 = tf.keras.layers.Input(shape=(3,))
m = tf.keras.layers.Dense(32,activation=&#39;relu&#39;)(输入)
m = tf.keras.layers.Dense(16,activation=&#39;relu&#39;)(m)
m = tf.keras.layers.Dense(8,activation=&#39;relu&#39;)(m)
m = tf.keras.layers.Dense(8,activation=&#39;relu&#39;)(m)
输出 = tf.keras.layers.Dense(1)(m)

model = tf.keras.models.Model(inputs=Input, output=output)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
loss=&#39;mse&#39;)

history = model.fit(train_dataset,
epochs=100,
validation_data=test_dataset)

model.save(&#39;Sticker.keras&#39;)

plt.plot(history.history[&#39;loss&#39;], label=&#39;Training Loss&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;Validation Loss&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.title(&#39;Loss Over时间&#39;)
plt.legend()
plt.show()

`

我的数据有 3 个输入列和 1 个输出列，共 230129 行
我询问了 gpt，他们让我改进数据管道，我照做了，但我的 GPU 每个时期仍然需要 40 秒]]></description>
      <guid>https://stackoverflow.com/questions/79362113/why-is-my-ai-training-on-gpu-is-a-lot-slower-than-cpu</guid>
      <pubDate>Thu, 16 Jan 2025 15:11:47 GMT</pubDate>
    </item>
    <item>
      <title>所有样本的前向传递</title>
      <link>https://stackoverflow.com/questions/79361940/forward-pass-with-all-samples</link>
      <description><![CDATA[import torch
import torch.nn as nn

class PINN(nn.Module):
def __init__(self, input_dim, output_dim, hidden_​​layers, neurons_per_layer):
super(PINN, self).__init__()
layer = []
layer.append(nn.Linear(input_dim, neurons_per_layer))
for _ in range(hidden_​​layers):
layer.append(nn.Linear(neurons_per_layer, neurons_per_layer))
layer.append(nn.Linear(neurons_per_layer, output_dim))
self.network = nn.Sequential(*layers)

def forward(self, x):
return self.network(x)

# 示例：生成随机输入数据
inputs = torch.rand((1000, 3)) # 3D 输入坐标

model = PINN(input_dim=3, output_dim=3, hidden_​​layers=4, neurons_per_layer=64)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 10000
for epoch in range(epochs):
optimizer.zero_grad() 
nn_output = model(inputs) # 计算 NN 预测
# 计算 nn_output 的梯度
loss.backward() 
optimizer.step() 

我想实现一个物理信息 NN，其中输入是 N 个 3d 点 (x,y,z)，NN 输出是此时的矢量值量，即输入维度和输出维度都相同。
要计算每个时期的损失，我需要有该量的值在所有点。示例：对于 N=1000 点，我需要所有 1000 个 NN 预测，然后才能继续进行损失计算。
在我的代码中，我基本上将一个 1000x3 对象提供给输入层，假设 pytorch 将每一行（1x3）分别传递给网络，最后将其再次组织为 1000x3 对象。
pytorch 是否像那样工作，还是我必须重新考虑这种方法？]]></description>
      <guid>https://stackoverflow.com/questions/79361940/forward-pass-with-all-samples</guid>
      <pubDate>Thu, 16 Jan 2025 14:23:21 GMT</pubDate>
    </item>
    <item>
      <title>代码无法在 AWS sagemaker 上成功运行</title>
      <link>https://stackoverflow.com/questions/79361126/code-not-running-sucessfully-on-aws-sagemaker</link>
      <description><![CDATA[当我运行应该创建训练作业的单元时，我没有在 sagemaker 笔记本上收到任何更新？我在 AWS Sagemaker 上运行以下代码，当我运行此代码时，它显示训练作业已创建并且没有进一步的更新。当我检查训练作业时，我可以看到状态已完成，但代码仍在笔记本上运行并且没有显示任何更新。有人可以帮我为什么会这样吗？
kmeans.fit(kmeans.record_set(scaled_data)) 
]]></description>
      <guid>https://stackoverflow.com/questions/79361126/code-not-running-sucessfully-on-aws-sagemaker</guid>
      <pubDate>Thu, 16 Jan 2025 10:15:35 GMT</pubDate>
    </item>
    <item>
      <title>有效覆盖 pytorch 数据集</title>
      <link>https://stackoverflow.com/questions/79360229/override-pytorch-dataset-efficiently</link>
      <description><![CDATA[我想继承 torch.utils.data.Dataset 类来加载我的自定义图像数据集，比如说用于分类任务。这是官方 pytorch 网站的示例，位于此 链接:
import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
self.img_labels = pd.read_csv(annotations_file)
self.img_dir = img_dir
self.transform = transform
self.target_transform = target_transform

def __len__(self):
return len(self.img_labels)

def __getitem__(self, idx):
img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
image = read_image(img_path)
label = self.img_labels.iloc[idx, 1]
if self.transform:
image = self.transform(image)
if self.target_transform:
label = self.target_transform(label)
return image, label

我注意到：

在 __getitem__ 中，我们将图像从磁盘读取到内存中。这意味着如果我们训练模型几个时期，我们会多次将同一幅图像重新读入内存。据我所知，这是一个代价高昂的操作
每次从磁盘读取图像时都会应用一次变换，在我看来，这几乎是一个多余的操作。

我理解，在非常大的数据集中，我们无法将数据完全放入内存中，因此我们别无选择，只能以这种方式读取数据（因为我们必须在一个时期内迭代所有数据），我想知道，如果我的所有数据都可以放入内存中，那么在 __init__ 函数中从磁盘读取所有数据不是更好的方法吗？
通过我在计算机视觉方面的一点经验，我注意到在 变换 中将图像裁剪成固定大小的图像非常常见。那么为什么我们不应该裁剪图像一次并将其存储在磁盘上的其他地方，而在整个训练过程中只读取裁剪后的图像呢？在我看来，这似乎是一种更有效的方法。
我理解，一些用于增强而不是规范化的转换最好应用于 __getitem__ 中，以便获得随机生成的数据而不是固定的数据。
你能为我澄清一下这个主题吗？
如果我缺少的是常识，请用正确的方法指导我找到代码库。]]></description>
      <guid>https://stackoverflow.com/questions/79360229/override-pytorch-dataset-efficiently</guid>
      <pubDate>Thu, 16 Jan 2025 02:38:21 GMT</pubDate>
    </item>
    <item>
      <title>F1-score、IOU 和 Dice Score 的实现</title>
      <link>https://stackoverflow.com/questions/79359767/implementation-of-f1-score-iou-and-dice-score</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79359767/implementation-of-f1-score-iou-and-dice-score</guid>
      <pubDate>Wed, 15 Jan 2025 21:33:49 GMT</pubDate>
    </item>
    <item>
      <title>stable_baselines3：为什么比较 ep_info_buffer 与评估时奖励不匹配？</title>
      <link>https://stackoverflow.com/questions/79353843/stable-baselines3-why-the-reward-does-not-match-comparing-ep-info-buffer-vs-eva</link>
      <description><![CDATA[我正在使用 stable_baselines3 库，这时我发现了一些意想不到的东西。
这里有一个简单的代码来重现这个问题：
import gymnasium as gym

from stable_baselines3 import DQN

env = gym.make(&quot;CartPole-v1&quot;)

model = DQN(&quot;MlpPolicy&quot;, env, verbose=0, stats_window_size=100_000)
model.learn(total_timesteps=100_000)

看看最后一集的奖励：
print(model.ep_info_buffer[-1])


{&#39;r&#39;: 409.0, &#39;l&#39;: 409, &#39;t&#39;: 54.87983

但是如果我使用以下代码评估模型：
obs, info = env.reset()
total_reward = 0
while True:
action, _states = model.predict(obs, deterministic=True)
obs, reward, termed, truncated, info = env.step(action)
total_reward = total_reward + reward
if termed or truncated:
obs, info = env.reset()
break

print(&quot;total_reward {}&quot;.format(total_reward))


total_reward 196.0

我得到了不同的奖励，这是我没有预料到的。
我预计会得到与 409 相同的奖励model.ep_info_buffer[-1]。
为什么会有这种差异？.ep_info_buffer 与每集奖励不同吗？]]></description>
      <guid>https://stackoverflow.com/questions/79353843/stable-baselines3-why-the-reward-does-not-match-comparing-ep-info-buffer-vs-eva</guid>
      <pubDate>Tue, 14 Jan 2025 02:14:32 GMT</pubDate>
    </item>
    <item>
      <title>使用 MAPIE 进行共形预测时，当 alpha 较大时，我会得到空的预测集</title>
      <link>https://stackoverflow.com/questions/78240714/using-mapie-for-conformal-predictions-i-get-empty-predictions-sets-when-alpha-is</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78240714/using-mapie-for-conformal-predictions-i-get-empty-predictions-sets-when-alpha-is</guid>
      <pubDate>Thu, 28 Mar 2024 20:28:12 GMT</pubDate>
    </item>
    <item>
      <title>如何为 Transformer 实现位置方向前馈神经网络？</title>
      <link>https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers</link>
      <description><![CDATA[我很难理解 transformer 架构中的位置式前馈神经网络。

让我们以机器翻译任务为例，其中输入是句子。从图中我了解到，对于每个单词，不同的前馈神经网络用于自注意子层的输出。前馈层应用了类似的线性变换，但每个变换的实际权重和偏差是不同的，因为它们是两个不同的前馈神经网络。
参考链接，这是PositionWiseFeedForward神经网络的类
class PositionwiseFeedForward(nn.Module):
“实现 FFN 方程。”
def __init__(self, d_model, d_ff, dropout=0.1):
super(PositionwiseFeedForward, self).__init__()
self.w_1 = nn.Linear(d_model, d_ff)
self.w_2 = nn.Linear(d_ff, d_model)
self.dropout = nn.Dropout(dropout)

def forward(self, x):
return self.w_2(self.dropout(F.relu(self.w_1(x))))

我的问题是：
我没有看到任何与位置相关的信息。这是一个具有两层的简单全连接神经网络。假设 x 是句子中每个单词的嵌入列表，句子中的每个单词都由上面的层使用相同的权重和偏差进行转换。（如果我错了，请纠正我）
我期望找到类似将每个单词嵌入传递到单独的 Linear 层的方法，该层将具有不同的权重和偏差，以实现与图片中所示的类似效果。]]></description>
      <guid>https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers</guid>
      <pubDate>Mon, 02 Jan 2023 05:59:25 GMT</pubDate>
    </item>
    <item>
      <title>(使用 cpu)Pytorch：IndexError：索引超出自身范围。 (使用 cuda)断言“srcIndex < srcSelectDimSize”失败。 如何解决？</title>
      <link>https://stackoverflow.com/questions/69596496/with-cpupytorch-indexerror-index-out-of-range-in-self-with-cudaassertion</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/69596496/with-cpupytorch-indexerror-index-out-of-range-in-self-with-cudaassertion</guid>
      <pubDate>Sat, 16 Oct 2021 14:28:36 GMT</pubDate>
    </item>
    <item>
      <title>在 lightgbm 或 XGBoost 中校准概率</title>
      <link>https://stackoverflow.com/questions/60772387/calibrating-probabilities-in-lightgbm-or-xgboost</link>
      <description><![CDATA[我需要帮助校准 lightgbm 中的概率
下面是我的代码
cv_results = lgb.cv(params, 
lgtrain, 
nfold=10,
stratified=False ,
num_boost_round = num_rounds,
verbose_eval=10,
early_stopping_rounds = 50, 
seed = 50)

best_nrounds = cv_results.shape[0] - 1

lgb_clf = lgb.train(params, 
lgtrain, 
num_boost_round=10000 ,
valid_sets=[lgtrain,lgvalid],
early_stopping_rounds=50,
verbose_eval=10)

ypred = lgb_clf.predict(test, num_iteration=lgb_clf.best_iteration)
]]></description>
      <guid>https://stackoverflow.com/questions/60772387/calibrating-probabilities-in-lightgbm-or-xgboost</guid>
      <pubDate>Fri, 20 Mar 2020 10:29:00 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习进行形状检测[关闭]</title>
      <link>https://stackoverflow.com/questions/44649290/shape-detection-using-machine-learning</link>
      <description><![CDATA[我想使用机器学习技术检测形状，即圆形、正方形、矩形、三角形等。
以下是形状检测的规范，

使用卷积神经网络 (CNN)。
对于训练，数据集包含 10 种形状的每个类别中的 1000 张图像。
对于测试，数据集包含 10 种形状的每个类别中的 100 张图像。
所有图像均为 28x28 大小，具有一个通道（灰色通道）。
数据集中的所有图像都是边缘检测图像。

问题

机器学习算法是否可以区分正方形和矩形……？，正方形和菱形……？
如何改进形状检测数据集？
]]></description>
      <guid>https://stackoverflow.com/questions/44649290/shape-detection-using-machine-learning</guid>
      <pubDate>Tue, 20 Jun 2017 09:36:35 GMT</pubDate>
    </item>
    <item>
      <title>sklearn 中的 fit 方法</title>
      <link>https://stackoverflow.com/questions/34727919/fit-method-in-sklearn</link>
      <description><![CDATA[我问了自己关于 sklearn 中的 fit 方法的各种问题。
问题 1：当我这样做时：
from sklearn.decomposition import TruncatedSVD
model = TruncatedSVD()
svd_1 = model.fit(X1)
svd_2 = model.fit(X2)

变量 model 的内容在此过程中是否发生了任何变化？
问题 2：当我这样做时：
from sklearn.decomposition import TruncatedSVD
model = TruncatedSVD()
svd_1 = model.fit(X1)
svd_2 = svd_1.fit(X2)

svd_1 发生了什么？换句话说，svd_1 已经被拟合了，我再次拟合它，那么它的组件发生了什么？]]></description>
      <guid>https://stackoverflow.com/questions/34727919/fit-method-in-sklearn</guid>
      <pubDate>Mon, 11 Jan 2016 17:49:15 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习创建语音识别系统[关闭]</title>
      <link>https://stackoverflow.com/questions/15127461/creating-a-voice-identification-system-using-machine-learning</link>
      <description><![CDATA[作为机器学习的一个教育项目，我考虑从头开始创建一个语音识别系统。它应该能够在之前对说话者的声音进行训练后，根据说话者的声音识别出说话者。
我应该采取什么方法来应对这一挑战？具体来说，这样的系统在高水平上将如何工作？]]></description>
      <guid>https://stackoverflow.com/questions/15127461/creating-a-voice-identification-system-using-machine-learning</guid>
      <pubDate>Thu, 28 Feb 2013 04:22:56 GMT</pubDate>
    </item>
    </channel>
</rss>