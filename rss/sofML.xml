<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Wed, 05 Mar 2025 12:36:11 GMT</lastBuildDate>
    <item>
      <title>'unsploth/llava-v1.6-Mistral-7b-hf'模型推理runtimeerror：unsploth：未能使输入需要梯度</title>
      <link>https://stackoverflow.com/questions/79486458/unsloth-llava-v1-6-mistral-7b-hf-model-inference-runtimeerror-unsloth-failed</link>
      <description><![CDATA[ I正在使用Llavanext Mismtral 7b Unsploth的模型测试模型推断，但会收到以下错误（图像）。已经向GitHub回购开了一个问题，但尚未收到他们的答复。我目前对此受到阻止。如果有人可以帮助理解起源会很棒。
有关更多详细信息，这是COLAB笔记本（ https://colab.research.google.com/drive/1i1kiwswsn2mgx6ucy8xd2x3o5ufk4jdw?usp = sharing ）具有代码生成错误。在此先感谢您的帮助。
  &lt;img alt =“ image” src =“ https://i.sstatic.net/lvpgcpgcpdr.pdr.pdr.png”]]></description>
      <guid>https://stackoverflow.com/questions/79486458/unsloth-llava-v1-6-mistral-7b-hf-model-inference-runtimeerror-unsloth-failed</guid>
      <pubDate>Wed, 05 Mar 2025 12:11:22 GMT</pubDate>
    </item>
    <item>
      <title>我在拥抱脸上有错误，什么都没有起作用</title>
      <link>https://stackoverflow.com/questions/79486433/im-having-an-error-on-hugging-face-and-nothing-is-working</link>
      <description><![CDATA[我试图在拥抱脸上部署一个项目，但有一个我似乎无法解决的错误。
 [我遇到此错误]
（ https://i.sstatic.net/iyu7obnw.png ）
 无法在9001-9001范围内的任何端口上启动节点服务器。
请安装节点20或更高版本，并将环境变量gradio_node_path设置为节点可执行文件的路径。
您可以通过设置环境变量gradio_node_port。 明确指定端口
即使我拥有节点版本20。
有什么建议吗？
我尝试了所有内容，检查了二手端口，我检查了环境变量是否正确，我检查了Gradio版本，我没有做什么]]></description>
      <guid>https://stackoverflow.com/questions/79486433/im-having-an-error-on-hugging-face-and-nothing-is-working</guid>
      <pubDate>Wed, 05 Mar 2025 12:02:21 GMT</pubDate>
    </item>
    <item>
      <title>当使用不同GPU训练模型时，结果不一致</title>
      <link>https://stackoverflow.com/questions/79486105/inconsistent-results-when-training-models-using-different-gpus</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79486105/inconsistent-results-when-training-models-using-different-gpus</guid>
      <pubDate>Wed, 05 Mar 2025 10:10:04 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost在等于传输数据的输入上无法正确预测</title>
      <link>https://stackoverflow.com/questions/79485691/xgboost-does-not-predict-properly-on-input-thats-equal-to-traning-data</link>
      <description><![CDATA[为什么这个非常简单的XGBoost ML示例即使在输入中也会产生全弹药，这相当于训练数据？这看起来像是一种琐碎的输入情况，不需要对ML进行任何微调，但是即使我对ML（MAX_DEPTH，ETA等）调整了HyperParams，也没有任何改变。
 将大熊猫作为pd导入
导入XGBoost为XGB

x = pd.dataframe（（[[[0]，[1]，[2]，[3]，[4]，[5]]），列= [&#39;x&#39;]）
y = pd.dataframe（[0，1，0，1，0，1]，列= [&#39;y&#39;]）

型号= xgb.xgbClassifier（）
型号（x，y）
打印（模型。

[0 0 0 0 0 0]
 ]]></description>
      <guid>https://stackoverflow.com/questions/79485691/xgboost-does-not-predict-properly-on-input-thats-equal-to-traning-data</guid>
      <pubDate>Wed, 05 Mar 2025 06:59:19 GMT</pubDate>
    </item>
    <item>
      <title>无法生成图像嵌入：张量A的大小（1246）必须与非辛格尔顿维度1处的张量B（77）匹配</title>
      <link>https://stackoverflow.com/questions/79485638/failed-to-generate-image-embeddings-the-size-of-tensor-a-1246-must-match-the</link>
      <description><![CDATA[我正在使用句子转换器模型来嵌入图像文件（pil ImageFile）。但是，它在标题中给出了错误。我尝试了很多事情来解决它，但无济于事。
我知道这与张量的大小有关，因此我尝试将其截断，但我做了一些研究，但找不到截断的方法，而无需更改代码。我认为可能有一个简单的解决方案，但找不到。
代码分析一个文件夹，（应该）返回其中图像的嵌入。
 将大熊猫作为pd导入
从stone_transformers导入句子词术语
导入操作系统
导入numpy作为NP
从pil导入图像，imageFile

imageFile.load_truncated_images = true

image_files = [&#39;.jpg&#39;，&#39;.jpeg&#39;，&#39;.png&#39;]

班级分析仪：

    def __init __（自我）：
        self.image_model = sencencetransformer（&#39;clip-vit-b-32＆quot）
        
    def Analyze_directory（自我，路径）：

        files_data = []
        
        使用os.scandir（路径）作为dir_iter：
            要进入dir_iter：
                尝试：
                    如果entry.is_file（）：
                        _，ext = os.path.splitext（entry.name）
                        如果在image_files中进行ext：
                            尝试：
                                使用image.open（os.path.join（path，entry.name））作为img：
                                    img.convert（“ RGB”）
                                    file_data = {
                                        ＆quot“ path＆quot”：entry.name，
                                        ＆quot“ content＆quot”：img，
                                        “类型”：“图像”
                                    }
                            除例外为E：
                                file_data = {
                                    ＆quot“ path＆quot”：entry.name，
                                    ＆quot“ content＆quot”：“”
                                    “类型”：“图像”
                                }

                        别的：
                            file_data = {
                                ＆quot“ path＆quot”：entry.name，
                                ＆quot“ content＆quot”：“”
                                “类型”：“未知”
                            }
                        
                    files_data.append（file_data）
                
                除例外为E：
                    继续
        
        df = pd.dataframe（files_data）

        嵌入= []
        对于_，在df.iterrows（）中行列：
            如果行[type; quot&#39;] ==;
                尝试：
                    img = img.resize（（224，224））
                    ＃将pil图像转换为张量
                    img_tensor = np.array（img）
                    ＃将像素值标准化为[-1，1]范围通过剪辑期望的范围
                    img_normalized =（img_tensor / 255.0 * 2.0） -  1.0
                    img_batch = np.expand_dims（img_normalized，axis = 0）
                    嵌入= self.image_model.encode（str（img_batch））。numpy（）[0]
                除例外为E：
                    提高RuntimeError（f＆quot“无法生成图像嵌入：{str（e）};）
            别的：
                ＃处理未知类型
                嵌入= np .eros（384）

            embeddings.Append（嵌入）
        
        嵌入= np.array（嵌入）
        
        返回嵌入
 
我尝试截断张量，但找不到方法。
我认为仅预处理图像可以解决它，但它没有
错误消息：
  trackback（最近的最新通话）：
  file＆quort＆lt; frozen runpy＆gt;＆quot，line 198，in _run_module_as_main
  file＆quort＆lt; frozen runpy＆gt;＆quot，line 88，in _run_code in _run_code
  file＆quot＆quot c：\ users \ ... \ src \ document_analyzer \ main.py ,, 15，in＆lt; module＆gt;
    主要的（）
    ~~~~ ^^
  file＆quot c：\ users \ ... \ src \ document_analyzer \ main.py，&#39;第6行，在main中
    folder_structure = Analyzer.Analyze_directory（路径）
  file＆quort c：\ users \ ... \ src \ document_analyzer \ andaryzer.py ,， 69，在Analyze_directory中
    提高RuntimeError（f＆quot“无法生成图像嵌入：{str（e）};）
RuntimeError：无法生成图像嵌入：张量A（1203）的大小必须匹配张量B（77）在非辛格尔顿尺寸1
 ]]></description>
      <guid>https://stackoverflow.com/questions/79485638/failed-to-generate-image-embeddings-the-size-of-tensor-a-1246-must-match-the</guid>
      <pubDate>Wed, 05 Mar 2025 06:33:01 GMT</pubDate>
    </item>
    <item>
      <title>使用layoutlmv3获取文本和令牌</title>
      <link>https://stackoverflow.com/questions/79485430/getting-the-text-and-tokens-using-layoutlmv3</link>
      <description><![CDATA[我培训了使用标签工作室创建标签数据集的Layoutlmv3模型。我能够使用以下代码测试模型的输出
  encoding =处理器（图像，单词，boxes = box，word_labels = word_labels，return_tensors =＆quort; pt;
对于k，v in encoding.items（）：
  打印（K，V.Shape）
 
并使用以下代码将盒子绘制在PDF上
 导入火炬

使用Torch.no_grad（）：
  输出=模型（**编码）
logits = outputs.logits
logits.shape
打印（输出）


预测= logits.argmax（-1）.squeeze（）。tolist（）
打印（预测）
标签= encoding.labels.squeeze（）。tolist（）
打印（标签）
def unormalize_box（bbox，宽度，高度）：
     返回 [
         宽度 *（bbox [0] / 1000），
         高度 *（bbox [1] / 1000），
         宽度 *（bbox [2] / 1000），
         高度 *（bbox [3] / 1000），
     这是给出的

token_boxes = encoding.bbox.squeeze（）.tolist（）
宽度，高度=图像尺寸

true_predictions = [model.config.id2label [pred] for pred，zip中的标签（预测，标签），如果标签！=  -  100]
true_labels = [model.config.id2label [label]用于预测，在zip中标记（预测，标签），如果标签！= -100]
true_boxes = [unnormalize_box（盒子，宽度，高度），标签在zip中（token_boxes，labels），如果标签！= -100]

打印（编码。值）
 
我试图理解的是，该模型放弃的预测是否只是我必须在框上再次运行teserract以提取文本的框架或使用相关标签吐出单词。。]]></description>
      <guid>https://stackoverflow.com/questions/79485430/getting-the-text-and-tokens-using-layoutlmv3</guid>
      <pubDate>Wed, 05 Mar 2025 04:04:14 GMT</pubDate>
    </item>
    <item>
      <title>小型数据集的微调骆驼[关闭]</title>
      <link>https://stackoverflow.com/questions/79485426/fine-tuning-llama-for-small-datasets</link>
      <description><![CDATA[我正在寻求微调美洲驼模型来回答某些问题。当前，必须回答问题的知识库很小，因此我可以作为上下文提供，并要求美洲驼基于上下文回答问题。但是，这个知识基础将继续增长，并且可能会有时间太大。
因此，我一直在寻找通过使用知识库作为数据集来微调Llama指示模型来回答问题。但是，当前数据集仅为25行。想知道目前仅针对25行进行微调是否还可以，还是我应该提供知识库作为上下文。如果是这样，如何适应将来知识基础的增长。]]></description>
      <guid>https://stackoverflow.com/questions/79485426/fine-tuning-llama-for-small-datasets</guid>
      <pubDate>Wed, 05 Mar 2025 04:00:48 GMT</pubDate>
    </item>
    <item>
      <title>将Yolo分割模型导出到Coreml</title>
      <link>https://stackoverflow.com/questions/79484862/exporting-yolo-segmentation-model-to-coreml</link>
      <description><![CDATA[我要导出这样的模型：
 模型= Yolo（&#39;yolo11m-seg.pt&#39;）
Model.export（格式=; Coreml;）
 
，然后加载到Xcode中。效果很好。这是我进行推理和检查结果的方式：
 守卫让结果：yoloptoutput =尝试？ Model.prediction（图像：InputPixelBuffer）else {return}

/// var_1648为1×116×8400 3维浮标
让classPredictions：mlmultiarray = result.var_1648
让classpredictions形状：mlshapearray＆lt; float＆gt; =结果

让NumanchorBoxes = classPredictions.shape [2] .intvalue // 8400
令numValuesperbox = classpredictions.shape [1] .intvalue // 116
令classCount = 80

//假设前5个值是bbox（4） +对象（1），而接下来的80个是类概率
令class -probabilitiesstartindex = 5

var maxboxprob = -float.infinity
var maxboxindex：int = 0
var maxboxobjectness：float = 0
var BestClassIndex：int = 0

对于boxIndex in 0 ..＆lt; numanchorboxes {
    让ObjectnessLogit = classPredictions形状[0，4，BoxIndex] .scalar ?? 0
    让客观性= sigmoid（objectneslogit）
    
    Guard Absocnessprobability＆gt; 0.51 else {继续}
    
    var classLogits：[float] = []
    对于classIndex in 0 ..＆lt; classCount {
        LET valueIndex = classProbabiLitiessTartexex + classIndex
        令logit = classpredictions形状[0，valueindex，boxIndex] .scalar ?? 0
        classLogits.Append（logit）
    }
    
    Guard！classLogits.isempty else {继续}
    
    //计算SoftMax并获得最佳概率和类索引
    让（BestProb，BestClassix）= SoftMaxWithBestClass（classLogits）
    
    //检查此框是否到目前为止的概率最高
    如果Bestprob＆gt; maxboxprob {
        MaxBoxProb = BestProb
        MaxBoxIndex = BoxIndex
        maxBoxObjectness = objectnessprobability
        BestClassIndex = BestClassix
    }
}

print（$$ -MaxBoxIndex：\（MaxBoxIndex）-MaxBoxProb：\（MaxBoxProb）-BestClassIndex：\（BestClassIndex）-MaxBoxOjectness：\（MaxBoxObject）＆quot;）
 
这是我计算SoftMax和Sigmoid的方式：
  func softmaxwithBestClass（_ logits：[float]） - ＆gt; （Bestobability：float，bestClassIndex：int）{
    让eufferogits = logits.map {exp（$ 0）}
    令Expsum = Expeogits.Reduce（0， +）
    LET概率= Explogits.map {$ 0 / expsum}
    
    var Bestrobyability：float = -float.infinity
    var BestClassIndex：int = 0
    
    for（索引，概率）概率。
        如果概率＆gt;最好的探针{
            BestRobibility =概率
            BestClassIndex =索引
        }
    }
    
    返回（Bestrobyability，BestClassIndex）
}

func sigmoid（_ x：float） - ＆gt;漂浮 {
    返回1 /（1 + EXP（-X））
}
 
我看到的是非常低的物质得分，主要是零，但最多〜0.53。而且非常低的概率，通常非常接近零。这是一个示例：
  $$ -MaxBoxIndex：7754 -MaxBoxProb：0.0128950095 -BestClassIndex：63 -MaxBoxOjectness：0.51033634
 
 63的类索引是正确的，或者合理地接近，但是为什么对象这么低呢？为什么班级概率如此之低？我担心我无法正确访问这些值。]]></description>
      <guid>https://stackoverflow.com/questions/79484862/exporting-yolo-segmentation-model-to-coreml</guid>
      <pubDate>Tue, 04 Mar 2025 21:03:23 GMT</pubDate>
    </item>
    <item>
      <title>如何将语义图作为输入？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79484796/how-to-formulate-semantic-map-as-input</link>
      <description><![CDATA[我有一个问题，需要通过UNET编码传递场景的语义图。我想知道如何塑造此地图的输入。
现在，我有一个单个通道映射，该图具有为场景中对象的每个像素的整数对象ID。我应该为场景中的每个对象使用二进制通道吗？]]></description>
      <guid>https://stackoverflow.com/questions/79484796/how-to-formulate-semantic-map-as-input</guid>
      <pubDate>Tue, 04 Mar 2025 20:32:18 GMT</pubDate>
    </item>
    <item>
      <title>Android MediaPipe第二推理实例无法初始化</title>
      <link>https://stackoverflow.com/questions/79484601/android-mediapipe-second-inference-instance-cant-be-initialized</link>
      <description><![CDATA[我对 google.mediapipe进行了一些实验框架并观察下一期……如果初始化了某些实例/任务，则无法在推理任务的另一个实例。之后。
例如，我以姿势检测模型运行MediaPipe任务，当我尝试初始化TexteMbedder实例时，我会收到以下例外：

 com.google.mediapipe.framework.mediapipeexception：找不到：验证的GraphConfig初始化失败。
没有名称的注册对象：MediaPipe :: tasks :: text :: text_embedder :: textembeddergraph;无法找到计算器“ MediaPipe.tasks.text.text_embedder.textembeddergraph”
在com.google.mediapipe.framework.graph.nativestartrunninggraph（本机方法）
在com.google.mediapipe.framework.graph.startrunninggraph（graph.java:336）
在com.google.mediapipe.tasks.core.taskrunner.create（taskrunner.java:72）
在com.google.mediapipe.tasks.text.textembedder.textembedder.createfromoptions（textembedder.java:159）

 i从他们的示例（姿势检测）中运行代码库，并将文本任务依赖添加到项目中。
实例化：
  baseOptions.builder baseOptionsBuilder = baseOptions.builder（）;
             baseOptionsBuilder.SetModelassetPath（&#39;unision_sentence_encoder.tflite＆quot;）;
textembedder.textembedderoptions选项=
                            textembedder.textembedpertions.builder（）
                                    .setBaseOptions（baseOptionsbuilder.build（））
                                                                            。建造（）;
mtextembedder = textembedder.createfromoptions（上下文，选项）;
 
我尝试了不同的版本/组合（视觉＆amp; text）到目前为止，但到目前为止还没有运气... 
我想知道是否可以同时运行2个实例？]]></description>
      <guid>https://stackoverflow.com/questions/79484601/android-mediapipe-second-inference-instance-cant-be-initialized</guid>
      <pubDate>Tue, 04 Mar 2025 18:46:25 GMT</pubDate>
    </item>
    <item>
      <title>过度拟合模型：训练准确性和验证精度之间的差异很大[封闭]</title>
      <link>https://stackoverflow.com/questions/79484336/overfitting-model-difference-between-training-accuracy-and-validation-accuracy</link>
      <description><![CDATA[我正在建立一个地理空间模型，该模型可以预测未来几年的犯罪模式。我正在使用1999年至2023年的内核密度栅格数据训练模型。我从一个国家内的犯罪活动的点数据集创建了栅格。我的模型基于卷积神经网络和LSTM。运行该模型，由于训练准确性和测试精度之间的价值差异，我的模型往往会过高。我以以下方式将数据集分开：

培训数据：1999-2017 
验证数据：2018-2020 
测试数据：2021-2023 

我不明白为什么该模型是ove拟合的。有人可以帮助如何优化模型以正常工作吗？
创建标签
  加载栅格
  构建模型
    def load_kde_rasters（年，base_path =＆quot; quot;）：
    ““负载和重新示例” kde栅格文件，以确保“均匀形状”。
    kde_rasters = []
    ref_rows =无
    max_cols = 0
    丢失_years = []
    几年中的一年：
        raster_path = os.path.join（base_path，f＆quot; {年}/kd.tif&quot;）
        如果不是OS.PATH.EXISTS（RASTER_PATH）：
            打印（f＆quot;找不到文件：{raster_path}＆quot;）
            丢失_years.append（年）
            继续
        使用rasterio.open（raster_path）作为src：
            kde_raster = src.read（1）
            行，cols = kde_raster.shape
            print（for {Year}，形状：{kde_raster.shape}＆quort;）
            如果ref_rows是无：
                ref_rows =行
            max_cols = max（max_cols，cols）
            kde_rasters.append（（Kde_raster，src.transform，src.crs）））））
    如果丢失了：
        打印（f＆quot“多年缺少kde栅格文件：{novsed_years}＆quot”）
    如果不是kde_rasters：
        打印（“没有有效的kde rasters加载。退出功能。”）
        没有返回，没有
    标准化_rasters = []
    对于kde_raster，转换，kde_rasters中的CRS：
        standardized_raster = np.zeros（（ref_rows，max_cols），dtype = np.float32）
        rasterio.warp.reproject（
            source = kde_raster，
            目的地=标准化_raster，
            src_transform =变换，
            src_crs = crs，
            dst_transform =变换，
            dst_crs = crs，
            重采样=重新采样。
        ）
        standardized_rasters.append（标准化_raster）
    kde_stack = np.stack（standardized_rasters，axis = -1）
    打印（最终kde_stack形状：{kde_stack.shape}＆quot”）
    返回kde_stack
 
训练过程中终端上的详细消息
  我遇到的问题是，该模型不是在概括，而是在记住数据，以便当我尝试预测和开发使用最新数据的未来几年的光栅图像时，它将仅产生相同的图像而不是不同的图像。。
  def build_spatiotemporal_model（input_shape）：
    模型=模型。
        layers.unput（shape = input_shape），
        ＃通过空间辍学减少了卷积层
        层。
            16，（3，3），激活=&#39;relu&#39;，padding =&#39;same&#39;，kernel_regularizer =正元器.l2（0.002）），），），），），
        layers.timedistribated（layers.batchnormization（）），
        layers.timedistribated（layers.spatiallopout2d（0.2）），
        层。
            32，（3，3），激活=&#39;relu&#39;，padding =&#39;same&#39;，kernel_regularizer =正元器.l2（0.002）），），），），），
        layers.timedistribated（layers.batchnormization（）），
        layers.timedistribated（layers.spatiallopout2d（0.2）），
        ＃通过复发掉落减少Convlstm（并删除一层）
        layers.convlstm2d（64，（3，3），activation =&#39;relu&#39;，padding =&#39;same&#39;，return_sepences = true，true，
                          kernel_regularizer = rodorizers.l2（0.003），recurrent_dropout = 0.2），
        layers.dropout（0.4），
        ＃最终卷积层
        layers.timedistribed（layers.conv2d（1，（1，1），activation =&#39;sigmoid&#39;））
    ）））
    返回模型
 
如何改进模型以使我能够获得一些准确性？]]></description>
      <guid>https://stackoverflow.com/questions/79484336/overfitting-model-difference-between-training-accuracy-and-validation-accuracy</guid>
      <pubDate>Tue, 04 Mar 2025 16:45:53 GMT</pubDate>
    </item>
    <item>
      <title>如何在机器学习模型中使用目标寻求功能[关闭]</title>
      <link>https://stackoverflow.com/questions/79480354/how-to-use-goal-seek-functionality-in-machine-learning-model</link>
      <description><![CDATA[我正在尝试在我的回归模型中实施目标寻求功能。我的模型已经构建，并获得了1月25日，2月25日，3月25日的预测值，如果我知道1月25日，2月25日，3月25日，我想对此进行逆转，那么本月的输入功能值可能是什么。我要更改的输入功能是SB和LEAD。但是使用此代码，我的SB值和LEAD在1月25日，2月25日，3月25日，尽管我的目标值不同。
  target_values = {
    &#39;2025-01-01&#39;：12000，
    &#39;2025-02-01&#39;：15000，
    &#39;2025-03-01&#39;：16000
}

base_data = pd.dataframe（index = pd.to_dateTime（[[&#39;2025-01-01&#39;，&#39;2025-02-01&#39;，&#39;2025-03-01&#39;]））））））））））
base_data [&#39;ds&#39;] = base_data.index

base_data [&#39;montry&#39;] = base_data [&#39;ds&#39;]。dt.month
base_data [&#39;day_of_year&#39;] = base_data [&#39;ds&#39;]。dt.dayofyear
base_data [&#39;sin_month&#39;] = np.sin（2 * np.pi * base_data [&#39;montry&#39;] / 12）
base_data [&#39;cos_month&#39;] = np.cos（2 * np.pi * base_data [&#39;montry&#39;] / 12）

如果test_data.index.duplicated（）。任何（）：
    打印（警告：test_data具有重复的索引值。重置索引。”
    test_data = test_data.reset_index（drop = true）

last_row = test_data.iloc [-1] .copy（）

static_cols = [&#39;a&#39;，&#39;b&#39;，&#39;c&#39;，&#39;d&#39;，&#39;e&#39;， 
               &#39;f&#39;，&#39;g&#39;，&#39;h&#39;]


对于static_cols中的col：
    base_data [col] = last_row [col] 

base_data [&#39;sb&#39;] = last_row [&#39;sb&#39;]
base_data [&#39;leads&#39;] = last_row [&#39;leads&#39;]

def predition_y（inputs，base_row，scaleer，型号）：
    行= base_row.copy（）
    行[&#39;sb&#39;] =输入[0]
    行[&#39;leds&#39;] =输入[1]
    x =行[[&#39;sb&#39;，&#39;a&#39;，&#39;b&#39;，&#39;sin_month&#39;，&#39;cos_month&#39;，&#39;c&#39;，&#39;d&#39;，&#39;e&#39;，e&#39;， 
             &#39;f&#39;，&#39;g&#39;，&#39;h&#39;， 
             &#39;leads&#39;]]。values.reshape（1，-1）
    X_SCALED = Scaleer.Transform（x）
    预测= model.predict（x_scaled）[0]
    print（for {base_row.name}的输入：sb = {inputs [0]：。2f}，leds = {inputs [1]：。2f}，预测y = {prediction y = {.2f}＆quort;）
    返回预测

def Objective_function（输入，目标，base_row，scaer，Model）：
    predicted_y = preditive_y（输入，base_row，scaleer，模型）
    print（{base_row.name}的目标：{target}，预测y：{prediction_y：.2f}，差异：{（prediction_y -target）** 2：.2f}
    返回（预测_y-目标）** 2

结果= {}
对于日期，target_values.items（）中的target：
    base_row = base_data.loc [date]
    onitire_guess = [base_row [&#39;sb&#39;]，base_row [&#39;leads&#39;]]
    
    打印（用目标{target};）
    print（f＆quot;初始猜测：sb = {prinity_guess [0]：。2f}，leds = {initial_guess [1]：。2f}＆quort;）
    
    结果=最小化（
        objective_function，
        initial_guess，
        args =（target，base_row，sualer，model13），
        方法=&#39;Powell&#39;， 
        bounds = [（0，无），（0，none）]，
        选项= {&#39;disp&#39;：true} 
    ）
    
    如果结果。
        结果[date] = {&#39;sb&#39;：result.x [0]，&#39;leads&#39;：result.x [1]}
    别的：
        结果[date] = {&#39;错误&#39;：&#39;优化失败&#39;}

对于日期，results.items（）中的vals：
    打印（f＆quot; \ nfor {date}：＆quot;）
    如果瓦尔中的“错误”：
        打印（val [&#39;错误&#39;]）
    别的：
        print（f＆quot; sb = {vals [&#39;sb&#39;]：。2f}，leds = {vals [&#39;leads&#39;] :. 2f}＆quot;）

print（f＆quot {date}}：＆quot;）
print（f＆quot;初始猜测：sb = {prinity_guess [0]：。2f}，leds = {initial_guess [1]：。2f}＆quort;）
打印（f＆quot;成功：{result.success}，最终值：sb = {result.x [0]：。2f}，leds = {result.x [1]：。2f}＆quort;）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79480354/how-to-use-goal-seek-functionality-in-machine-learning-model</guid>
      <pubDate>Mon, 03 Mar 2025 07:02:20 GMT</pubDate>
    </item>
    <item>
      <title>MATLAB中支持向量机的输出中的框约束是什么？</title>
      <link>https://stackoverflow.com/questions/79478755/what-is-the-box-constraint-in-the-output-of-a-support-vector-machine-in-matlab</link>
      <description><![CDATA[在MATLAB中，函数 FITCSVM 训练支持向量机。
在输出中，有一个组件 boxconstraints 。我已经阅读了帖子此帖子
并了解框约束的含义。
但是在帖子的回答中，框约束C是标量，而在MATLAB的输出中，它是一个矢量，示例数量相同，而所有词则是1，我在输入中使用了默认的框约束1。   。
我不明白输出是什么。
（我已经在上阅读了svm的部分，统计学习的要素    ，因此我认为这可能是不同的术语选择。如果您想回答，您可以跳过解释SVM的基本概念。）））]]></description>
      <guid>https://stackoverflow.com/questions/79478755/what-is-the-box-constraint-in-the-output-of-a-support-vector-machine-in-matlab</guid>
      <pubDate>Sun, 02 Mar 2025 07:40:03 GMT</pubDate>
    </item>
    <item>
      <title>哪些词嵌入维度值代表什么？</title>
      <link>https://stackoverflow.com/questions/49732976/what-actually-word-embedding-dimensions-values-represent</link>
      <description><![CDATA[我对Word2Vec和Word Embedding有疑问，我下载了手套预训练的单词嵌入（形状40,000 x 50），并使用此功能从中提取信息：
 导入numpy作为np
DEF LOADGLOVEMODEL（手套）：
    打印（“加载手套型”）
    f = open（手套，&#39;r&#39;）
    模型= {}
    对于F中的行
        splitline = line.split（）
        word = splitline [0]
        嵌入= np.Array（[[float（val）for val for val in Splitline [1：]]）
        模型[Word] =嵌入
    打印（完成。“
    返回模型
 
现在，如果我称此函数为Word &#39;Python&#39;，则类似：
  print（loadglovemodel（&#39;glove.6b.100d.txt&#39;）[&#39;python&#39;]）
 
它给了我1x50形状向量，例如：
  [0.24934 0.68318 -0.044711 -1.3842 -0.0073079 0.651
 -0.33958 -0.19785 -0.33925 0.26691 -0.033062 0.15915
  0.89547 0.53999 -0.55817 0.46245 0.36722 0.1889
  0.83189 0.81421 -0.11835 -0.53463 0.24158 -0.038864
  1.1907 0.79353 -0.12308 0.6642 -0.77619 -0.45713
 -1.054 -0.20557 -0.13296 0.12239 0.88458 1.024
  0.32288 0.82105 -0.069367 0.024211 -0.51418 0.8727
  0.25759 0.91526 -0.64221 0.041159 -0.60208 0.54631
  0.66076 0.19796 -1.1393 0.79514 0.45966 -0.18463
 -0.64131 -0.24929 -0.40194 -0.50786 0.80579 0.53365
  0.52732 0.39247 -0.29884 0.009585 0.99953 -0.061279
  0.71936 0.32901 -0.052772 0.67135 -0.80251 -0.25789
  0.49615 0.48081 -0.68403 -0.012239 0.048201 0.29461
  0.20614 0.33556 -0.64167 -0.64708 0.13377 -0.12574
 -0.46382 1.3878 0.95636 -0.067869 -0.0017411 0.52965
  0.45668 0.61041 -0.11514 0.42627 0.17342 -0.7995
 -0.24502 -0.6086 -0.38469 -0.4797]
 
我需要帮助了解输出矩阵。这些价值代表什么，生成新单词具有重要意义？]]></description>
      <guid>https://stackoverflow.com/questions/49732976/what-actually-word-embedding-dimensions-values-represent</guid>
      <pubDate>Mon, 09 Apr 2018 12:27:22 GMT</pubDate>
    </item>
    <item>
      <title>为什么Word2Vec为每个单词使用2个表示？</title>
      <link>https://stackoverflow.com/questions/29381505/why-does-word2vec-use-2-representations-for-each-word</link>
      <description><![CDATA[我试图理解为什么Word2Vec的Skipgram模型为每个单词具有2个表示（隐藏的表示嵌入单词）和输出表示形式（也称为上下文单词嵌入）。这仅仅是为了普遍性，上下文可以是任何东西（不仅是单词），还是有更根本的原因]]></description>
      <guid>https://stackoverflow.com/questions/29381505/why-does-word2vec-use-2-representations-for-each-word</guid>
      <pubDate>Wed, 01 Apr 2015 01:44:41 GMT</pubDate>
    </item>
    </channel>
</rss>