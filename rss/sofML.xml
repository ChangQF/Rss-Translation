<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 25 Jun 2024 12:28:25 GMT</lastBuildDate>
    <item>
      <title>用于放射性核素识别的 CNN：如何使用 CategoricalCrossentropy 表示损失并使用指标进行性能测试来规范化数据？</title>
      <link>https://stackoverflow.com/questions/78667148/cnn-for-radionuclide-identification-how-to-normalize-data-using-categoricalcro</link>
      <description><![CDATA[目前，在阅读了一些与此类工作相关的论文后，我已经使用伽马光谱制作了一个用于放射性核素识别的 CNN 模型。该模型应该用于多标签分类。
第一层是 CNN 层（Conv1D），它将整个伽马光谱作为输入。
下面显示了一个伽马光谱示例...

注意 - 该模型不将图像作为输入。它将每个通道的计数作为输入。
我应该使用什么方法来规范化这种类型的数据？从我读过的论文来看，似乎不对伽马光谱进行归一化会产生最佳结果，但对于之前做过 ML 的人来说，对此类数据进行归一化似乎很重要。
由于我正在制作一个进行多标签分类的模型来检测伽马光谱中存在的放射性核素，我应该使用什么损失函数？看了论文 1 后，似乎我应该使用 CategoricalCrossentropy，但在网上搜索后，似乎每个人都使用 BinaryCrossentropy。任何建议，因为目前我倾向于 CategoricalCrossentropy。
我倾向于使用 CategoricalCrossentropy 作为损失函数的原因在于论文和这样一个事实：如果模型选择了一种存在于伽马光谱中的放射性核素，那么它应该会影响它所预测的其他放射性核素。
最后，由于我正在做多标签分类，我正在考虑使用 F1 分数作为我的指标来确定我的模型在放射性核素识别方面是否表现良好，但这似乎不是最好的指标，因为我希望我的模型能够预测伽马光谱中存在的所有放射性核素。而且准确率在多标签分类中并没有真正用到。
我正在用 TensorFlow 编写代码，因为我一般遵循论文 1。
我读过的一些论文 -
(1) 使用深度学习和通道注意模块进行多放射性核素识别和视觉解释
(2) NaI γ 光谱的放射性同位素识别算法
(3) 使用深度学习进行同位素识别：解释
(4) 伽马射线光谱中放射性核素的自动实时识别：一种基于卷积神经网络的新方法，使用合成数据集训练
等等]]></description>
      <guid>https://stackoverflow.com/questions/78667148/cnn-for-radionuclide-identification-how-to-normalize-data-using-categoricalcro</guid>
      <pubDate>Tue, 25 Jun 2024 11:46:49 GMT</pubDate>
    </item>
    <item>
      <title>为给定的文本生成标签</title>
      <link>https://stackoverflow.com/questions/78667039/generate-tags-for-a-text-given</link>
      <description><![CDATA[我正在尝试使用 flask 构建一个 API，它将从给定的 URL 中提取文本并为该文本生成有效标签。例如，文本是咖喱鸡的食谱，有效标签可以是食谱、印度菜、食物等。
我试过 nltk 库、TF-IDF 矢量化器等，但它们都在分析单词的最大频率，而不是生成新单词。
有没有人能解决这个问题。
我也尝试过使用 gtp2 模块，但输出不是我期望的
import request
from bs4 import BeautifulSoup
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from flask import Flask, request, jsonify

app = Flask(__name__)

# 加载预先训练的 GPT-2 模型和 tokenizer
model_name = &#39;gpt2-medium&#39;
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

def fetch_text_from_url(url):
try:
response = request.get(url)
response.raise_for_status() # 对错误响应引发 HTTPError
soup = BeautifulSoup(response.content, &#39;html.parser&#39;)

# 查找所有段落并连接它们的文本
passages = soup.find_all(&#39;p&#39;)
text = &#39; &#39;.join([para.get_text() for para in passages])

return text
except request.exceptions.RequestException as e:
print(f&quot;Error fetching content from {url}: {str(e)}&quot;)
return None
except Exception as e:
print(f&quot;Error parsing content from {url}: {str(e)}&quot;)
return None

def generate_tags(text, max_length=20, num_return_sequences=3):
try:
# 创建提示以生成标签
prompt = &quot;Tags for this text: &quot;

# 对输入文本和提示进行标记
input_ids = tokenizer.encode(prompt + text, return_tensors=&#39;pt&#39;)

# 使用 GPT-2 模型生成标签
output = model.generate(
input_ids,
max_length=max_length + len(input_ids[0]),
num_return_sequences=num_return_sequences,
num_beams=5,
no_repeat_ngram_size=2,
early_stopping=True
)

# 解码生成的标签
tags = []
for seq in output:
coded_seq = tokenizer.decode(seq, skip_special_tokens=True).strip()
# 从生成的文本中提取标签
tags.extend([t.strip() for t incoded_seq.split() if t.startswith(&#39;#&#39;)])

return tags

except Exception as e:
print(f&quot;生成标签时出错：{str(e)}&quot;)
return None

@app.route(&#39;/generate_tags&#39;, methods=[&#39;POST&#39;])
def generate_tags_api():
data = request.get_json()
url = data.get(&#39;url&#39;)
if not url:
return jsonify({&#39;error&#39;: &#39;URL 是必需的&#39;}), 400

try:
text = fetch_text_from_url(url)
if not text:
return jsonify({&#39;error&#39;: &#39;无法从 URL 获取内容&#39;}), 500

tags = generate_tags(text)
if tags:
return jsonify({&#39;tags&#39;: tags})
else:
return jsonify({&#39;error&#39;: &#39;无法从 URL 生成标签&#39;}), 500
except Exception as e:
return jsonify({&#39;error&#39;: str(e)}), 500

如果 __name__ == &quot;__main__&quot;:
app.run(port=8000, debug=True)
]]></description>
      <guid>https://stackoverflow.com/questions/78667039/generate-tags-for-a-text-given</guid>
      <pubDate>Tue, 25 Jun 2024 11:22:18 GMT</pubDate>
    </item>
    <item>
      <title>Megatron-LM 用于学习目的</title>
      <link>https://stackoverflow.com/questions/78666975/megatron-lm-for-learning-purpose</link>
      <description><![CDATA[我想在本地机器上使用 GatorTron LM 进行学习。我从 https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og 下载了文件（config.json、hparam.yml、MegtronBERT.nemo、MegatronBERT.pt、vocab.txt）。我还有 NVIDIA GPU。我应该如何向模型发送提示消息。
嗨，我想在本地机器上使用 GatorTron LM 进行学习。我从 https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og 下载了文件（config.json、hparam.yml、MegtronBERT.nemo、MegatronBERT.pt、vocab.txt）。我还有 NVIDIA GPU。我应该如何向模型发送提示消息。]]></description>
      <guid>https://stackoverflow.com/questions/78666975/megatron-lm-for-learning-purpose</guid>
      <pubDate>Tue, 25 Jun 2024 11:08:50 GMT</pubDate>
    </item>
    <item>
      <title>元特征分析：根据可用内存拆分数据进行计算</title>
      <link>https://stackoverflow.com/questions/78666961/meta-feature-analysis-split-data-for-computation-on-available-memory</link>
      <description><![CDATA[我正在使用元特征提取器包：pymfe进行复杂性分析。
例如，在小型数据集上，这不是问题。
pip install -U pymfe

from sklearn.datasets import make_classification
from sklearn.datasets import load_iris
from pymfe.mfe import MFE

data = load_iris()
X= data.data
y = data.target

extractor = MFE(features=[ &quot;t1&quot;], groups=[&quot;complexity&quot;],
summary=[&quot;min&quot;, &quot;max&quot;, &quot;mean&quot;, &quot;sd&quot;])
extractor.fit(X,y)
extractor.extract()
([&#39;t1&#39;], [0.12])

我的数据集很大 (32690, 80)，由于内存过大，此计算被终止 用法。 我在 Ubuntu 24.04 上工作，有 32GB RAM。
要重现场景：
# 生成数据集
X, y = make_classification(n_samples=20_000,n_features=80,
n_informative=60, n_classes=5, random_state=42)

extractor = MFE(features=[ &quot;t1&quot;], groups=[&quot;complexity&quot;],
summary=[&quot;min&quot;, &quot;max&quot;, &quot;mean&quot;, &quot;sd&quot;])
extractor.fit(X,y)
extractor.extract()
Killed

问题：
如何我可以将此任务拆分为数据集的小分区进行计算，并合并最终结果（平均）？]]></description>
      <guid>https://stackoverflow.com/questions/78666961/meta-feature-analysis-split-data-for-computation-on-available-memory</guid>
      <pubDate>Tue, 25 Jun 2024 11:05:09 GMT</pubDate>
    </item>
    <item>
      <title>有没有更好更有效的方法来从图像中识别聚类？</title>
      <link>https://stackoverflow.com/questions/78666791/is-there-a-better-and-more-efficient-way-to-identify-clusters-from-an-image</link>
      <description><![CDATA[所以我有一个移动细菌的视频，我已将其转换为图像。现在我必须在每个图像中找到细菌的簇。如果两个相邻细菌的质心相距小于距离 R，并且它们的运动方向相差小于角度 alpha，我们将它们定义为同一簇的成员。动态簇被定义为包括所有与至少一个满足局部距离和运动方向标准的其他细菌相连的细菌。
这是一张示例图片：

我首先将每个轮廓的 COM 坐标和角度存储在字典中。我识别第一个轮廓并将其作为标准。然后我检查另一个轮廓，它最接近第一个轮廓，并且它的角度相差最小。如果这在给定的标准范围内，那么它们都属于第 1 组。然后第二个轮廓成为新的标准，并重复此过程，直到没有其他轮廓满足标准。然后所有这些轮廓都被认为属于第 1 组。再次重复此过程以形成其他组。但我的问题是，有没有更简单的方法可以做到这一点？我的方法花费大量时间并且不准确。有没有更好的算法或函数可以做同样的事情？]]></description>
      <guid>https://stackoverflow.com/questions/78666791/is-there-a-better-and-more-efficient-way-to-identify-clusters-from-an-image</guid>
      <pubDate>Tue, 25 Jun 2024 10:32:17 GMT</pubDate>
    </item>
    <item>
      <title>关于如何使用 Python 创建文本到 3D AI 模型，有什么建议吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78666584/any-advice-on-how-i-would-create-a-text-to-3d-ai-model-using-python</link>
      <description><![CDATA[在学习的最后一年，我必须做一个项目。我发现图像和模型生成非常有趣，所以我想在这方面做点什么。我在机器学习方面有一些基本经验，但我甚至不知道从哪里开始这个项目。有人能给我一些建议吗？
我看过现有的库，但我真的无法使用它们，因为它们显然会让项目变得太简单。我需要自己建立一个模型。]]></description>
      <guid>https://stackoverflow.com/questions/78666584/any-advice-on-how-i-would-create-a-text-to-3d-ai-model-using-python</guid>
      <pubDate>Tue, 25 Jun 2024 09:48:24 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 CV/ML 识别重叠图片中的不同符号？</title>
      <link>https://stackoverflow.com/questions/78666370/how-to-recognize-different-symbols-in-a-picture-with-overlapping-using-cv-ml</link>
      <description><![CDATA[我有一张包含每个项目里程碑信息的图片。每个里程碑都用独特的符号在图表中标记。在图片中，我们有不同的三角形或正方形，颜色也不同。
我需要从图例中的符号中提取有关日期的信息。这意味着我需要知道每个符号及其图例的位置。然后在 x 轴和 y 轴上搜索临近日期。
里程碑图片
我尝试过使用 cv2 进行颜色识别。首先，我提取了 x-y 轴上的日期并保存了它们的位置。我对黄色菱形使用 cv2，效果很好。但是当涉及到红色或灰色时，因为有不同的符号，但都是红色。我无法仅使用 cv2 来区分它们。
cv2 中的传统计算机视觉方法无法处理重叠。例如有两种类型的红色三角形，一种指向左侧。面对所有这些问题，我无法找到一种好的方法来识别图例中的每个符号。你有什么建议吗？
也许训练 ML 是一个潜在的解决方案。我有大约 1000 张这样的图片。我不知道训练模型是否足够。但工作量也会很大。]]></description>
      <guid>https://stackoverflow.com/questions/78666370/how-to-recognize-different-symbols-in-a-picture-with-overlapping-using-cv-ml</guid>
      <pubDate>Tue, 25 Jun 2024 09:04:21 GMT</pubDate>
    </item>
    <item>
      <title>Pyspark MLlib 自定义 Transformer 类 -AttributeError:'DummyMod' 对象没有属性 'MyTransformer'</title>
      <link>https://stackoverflow.com/questions/78665992/pyspark-mllib-custom-transformer-class-attributeerror-dummymod-object-has-no</link>
      <description><![CDATA[我正在尝试创建一个自定义转换器作为管道中的一个阶段。一些转换我通过 SparkNLP 进行，接下来的几个转换使用 MLlib。要将某个阶段的 SparkNLP 转换结果传递给下一个 MLlib 转换，我需要提取 spark_nlp_col.result 列并传递它，为此我使用了自定义转换阶段。
在我安装好管道后，我可以将其持久化，但当我再次加载它时，我收到错误：
AttributeError：&#39;DummyMod&#39; 对象没有属性 &#39;MyTransformer&#39;
这是我的课程：
从 pyspark.ml 导入 Transformer
从 pyspark.ml.param.shared 导入 Param、Params、TypeConverters

class MyTransformer(Transformer、DefaultParamsWritable、DefaultParamsReadable):
inputCol = Param(Params._dummy(), &quot;inputCol&quot;, &quot;&quot;,TypeConverters.toString)
outputCol = Param(Params._dummy(), &quot;outputCol&quot;, &quot;&quot;,TypeConverters.toString)

def __init__(self,inputCol=None,outputCol=None):
super(MyTransformer, self).__init__()
self._setDefault(inputCol=None)
self._set(inputCol = inputCol)
self._setDefault(outputCol=None)
self._set(outputCol = outputCol)

def getInputCol(self):
return self.getOrDefault(self.inputCol)

def setInputCol(self, inputCol):
self._set(inputCol=inputCol)

def getOutputCol(self):
return self.getOrDefault(self.outputCol)

def setOutputCol(self, outputCol):
self._set(outputCol=outputCol)

def _transform(self, dataset):
in_col = self.getInputCol()
out_col = self.getOutputCol()

final_in_col = in_col+&quot;.result&quot;
result = dataset.withColumn(out_col, dataset[final_in_col])
返回结果

我已在其上创建了一个简单的包装函数以进行标准化，然后使用它来创建管道、拟合并保存它：
def extract_col(cols, in_suffix, out_suffix):
return [MyTransformer(inputCol=col+in_suff, outputCol=col+out_suffix) for col in cols]

自定义转换器之前的阶段数

extractors = extract_col(cols, &quot;_in&quot;, &quot;_out&quot;)

自定义转换器之后的阶段数

stages = s1 + s2 + .. + extractors + .. + snlast + sn
pipeline = Pipeline(stages = periods)
fit_pipeline = pipeline.fit(data)
fit_pipeline.write().overwrite().save(&quot;path_to_store_at&quot;)

我如何读回它：
saved_pipeline = PipelineModel.load(&quot;path_where_stored&quot;)

然后我遇到了错误。
我尝试了多种编写自定义类的方法，使用 HasInputCol、HasOutputCol 等，但到目前为止都没有效果。
有什么想法可以解决它吗？]]></description>
      <guid>https://stackoverflow.com/questions/78665992/pyspark-mllib-custom-transformer-class-attributeerror-dummymod-object-has-no</guid>
      <pubDate>Tue, 25 Jun 2024 07:42:56 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试读取 .wav 文件，但只能转换 20 秒的对话。请提出建议</title>
      <link>https://stackoverflow.com/questions/78665946/i-am-trying-to-read-a-wav-file-but-was-able-to-convert-only-20s-conversation-p</link>
      <description><![CDATA[我尝试读取 .wav 文件，但只能将 20 秒的对话转换为文本。请提出建议。
 # 读取 .wav 文件的代码 
import Speech_recognition as sr
r = sr.Recognizer()
audio = r&quot;D:\Fraud_Call_Detection\audio1.wav&quot;
with sr.AudioFile(audio) as source:
audio = r.record(source)
print (&#39;Done!&#39;)

try:

text = (r.recognize_google(audio, language=&quot;en-HK&quot;))
print (text)

except Exception as e:
print (e)

输出：]]></description>
      <guid>https://stackoverflow.com/questions/78665946/i-am-trying-to-read-a-wav-file-but-was-able-to-convert-only-20s-conversation-p</guid>
      <pubDate>Tue, 25 Jun 2024 07:33:04 GMT</pubDate>
    </item>
    <item>
      <title>在 C# 应用程序中使用我的模型时出现形状不兼容错误</title>
      <link>https://stackoverflow.com/questions/78665873/incompatible-shapes-error-while-using-my-model-in-a-c-sharp-app</link>
      <description><![CDATA[我在使用 Python 开发的图像分类器模型时遇到了问题，我想将其用于 C# 工业应用中。
这是模型结构：
model = Sequential([
Conv2D(32, (3, 3),activation=&#39;relu&#39;, input_shape=(img_height, img_width, 3)),
MaxPooling2D((2, 2)),
Conv2D(64, (3, 3),activation=&#39;relu&#39;),
MaxPooling2D((2, 2)),
Conv2D(128, (3, 3),activation=&#39;relu&#39;),
MaxPooling2D((2, 2)),
Flatten(),
Dense(512,activation=&#39;relu&#39;),
Dropout(0.5),
Dense(len(class_names),激活=&#39;softmax&#39;)
])

这是我在 C# 中使用的代码：
var modelfile = &quot;D:\\Soft\\Soft\\C#\\LastVersion\\frozen_model.pb&quot;;
var labels = new[] { &quot;ORANGE&quot;, &quot;BANANA&quot;, &quot;APPLE&quot;, &quot;MIX&quot; };

var graph = new Graph().as_default();
graph.Import(File.ReadAllBytes(modelfile));

using (var session = tf.Session(graph))
{
var tensor = ImageToTensor(RutaImagen);

var input_op = graph.OperationByName(&quot;sequence_1/conv2d_1/convolution&quot;); 
var output_op = graph.OperationByName(&quot;sequence_1/dense_1_2/Softmax&quot;); 

// 下一行给我一个运行时错误
var result = session.run(output_op.outputs[0], (input_op.outputs[0], tensor));

var results = result.ToArray&lt;float&gt;();
float totalConfidence = results.Sum();

int[] quantized = Quantized(results);

label1.Text = &quot;Resultados cuantizados:\n&quot;;
for (int i = 0; i &lt; results.Length; i++)
{
label1.Text += $&quot;{labels[i]}:\n&quot; +
$&quot; Valor original: {results[i]:0.000}\n&quot; +
$&quot;概率：{results[i] * 100:0.00}%\n&quot; +
$&quot; 归一化：{(results[i] / totalConfidence) * 100:0.00}%\n&quot; +
$&quot; 概率：{(results[i] &gt;= 0.5 ? &quot;可能&quot; : &quot;不可能&quot;)}\n&quot;;
}

我调用 ImageToTensor 函数的代码来预处理图像：
 private static NDArray ImageToTensor(string file)
{
Bitmap bitmap = new Bitmap(file);
Bitmap resizedBitmap = new Bitmap(bitmap, new Size(imgWidth, imgHeight));

// 将图像转换为张量
float[,,,] input = new float[1, imgHeight, imgWidth, 3];
for (int y = 0; y &lt; imgHeight; y++)
{
for (int x = 0; x &lt; imgWidth; x++)
{
Color pixel = resizedBitmap.GetPixel(x, y);
input[0, y, x, 0] = pixel.R / 255.0f;
input[0, y, x, 1] = pixel.G / 255.0f;
input[0, y, x, 2] = pixel.B / 255.0f;
}
}
//return new DenseTensor&lt;float&gt;(input);
return np.array(input);
}

最后是运行时错误：
执行时
var result = session.run(output_op.outputs[0], (input_op.outputs[0], tensor));
Tensorflow.InvalidArgumentError
HResult=0x80131500
Message=不兼容的形状：[1,1,1,32] vs. [1,200,150,3]
[[{{node chronological_1/conv2d_1/add}}]]
Source=Tensorflow.Binding
StackTrace:
at Tensorflow.Status.Check(Boolean throwException)
at Tensorflow.BaseSession._call_tf_sessionrun(KeyValuePair`2[] feed_dict, TF_Output[] fetch_list, List`1 target_list)
at Tensorflow.BaseSession._do_run(List`1 target_list, List`1 fetch_list, Dictionary`2 feed_dict)
在 Tensorflow.BaseSession._run(Object fetches, FeedItem[] feed_dict)
在 Tensorflow.BaseSession.run(Tensor fetche, FeedItem[] feed_dict)
在 Test2.Form1.EvaluarImagen(String RutaImagen) 在 D:\Soft\Soft\C#\LastVersion\Test2\Form1.cs:line 103
在 Test2.Form1.button1_Click(Object sender, EventArgs e) 在 D:\Soft\Soft\C#\LastVersion\Test2\Form1.cs:line 26
在 System.Windows.Forms.Button.OnClick(EventArgs e)
在 System.Windows.Forms.Button.OnMouseUp(MouseEventArgs mevent)
在 System.Windows.Forms.Control.WmMouseUp(Message&amp; m，MouseButtons 按钮，Int32 点击）
在 System.Windows.Forms.Control.WndProc(Message&amp; m)
在 System.Windows.Forms.ButtonBase.WndProc(Message&amp; m)
在 System.Windows.Forms.Control.ControlNativeWindow.WndProc(Message&amp; m)
在 System.Windows.Forms.NativeWindow.Callback(HWND hWnd, MessageId msg, WPARAM wparam, LPARAM lparam)


有人能帮我一下吗？我很绝望
谢谢大家]]></description>
      <guid>https://stackoverflow.com/questions/78665873/incompatible-shapes-error-while-using-my-model-in-a-c-sharp-app</guid>
      <pubDate>Tue, 25 Jun 2024 07:17:05 GMT</pubDate>
    </item>
    <item>
      <title>如何自动从工程图中提取外部尺寸？[关闭]</title>
      <link>https://stackoverflow.com/questions/78665748/how-do-i-extract-the-outer-dimensions-from-an-engineering-drawing-automatically</link>
      <description><![CDATA[我最近一直在研究从工程图中提取信息，因为公司经常会收到很多图纸，而编制清单需要标题、说明、重量、图纸编号和外部尺寸等特征。
我看到了 Werk24 的这个工具 - https://werk24.io/blog/why-extracting-external-dimensions-from-technical-drawings-is-compliated。
他们声称他们将图纸转换为 3D 图纸，然后从 3D 模型中可靠地提取外部尺寸。但我不明白如何自动将 PDF 工程图转换为 3D 格式
我能够准确地提取标题、说明、重量和图纸编号，但无法提取外部尺寸。我尝试使用 ChatGPT 4o，但它只是在编造一些数字。尽管修改了提示并解释了它们的工作原理，但输出没有任何改进。]]></description>
      <guid>https://stackoverflow.com/questions/78665748/how-do-i-extract-the-outer-dimensions-from-an-engineering-drawing-automatically</guid>
      <pubDate>Tue, 25 Jun 2024 06:45:03 GMT</pubDate>
    </item>
    <item>
      <title>在语义分割模型上对脑肿瘤患者进行分割时，keras 中的 nan 损失 2021 年任务 1</title>
      <link>https://stackoverflow.com/questions/78665443/nan-loss-in-keras-when-working-on-a-semantic-segmentation-model-to-segment-brain</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78665443/nan-loss-in-keras-when-working-on-a-semantic-segmentation-model-to-segment-brain</guid>
      <pubDate>Tue, 25 Jun 2024 04:49:26 GMT</pubDate>
    </item>
    <item>
      <title>U-Net 无法过度拟合单个训练示例 - 损失平台</title>
      <link>https://stackoverflow.com/questions/78642145/u-net-unable-to-overfit-single-training-example-loss-plateaus</link>
      <description><![CDATA[我正在通过在由单个训练示例（图像到图像）组成的“数据集”上训练 U-Net 架构来测试它。输入图像是输出图像的噪声版本。最初，输出图像开始看起来更像所需的输出，但损失曲线开始趋于稳定，模型停止改进。
我的问题是：

U-Net（或任何没有完全连接层的 CNN）是否应该能够在给定单个示例的情况下在恒定图像上过度拟合？
如果它不能完成这个简单的任务，会犯什么常见错误或需要注意什么？

我通过将深度降低到几乎双卷积层（没有任何编码器/解码器层）来简化架构，并调整了学习率，但它仍然不会过度拟合。我期望模型在单个训练示例上完美地过度拟合，但事实并非如此。我已经调整了学习率，但模型仍然无法在这个简单的任务上实现完美的过度拟合。
输入图像：

地面实况：

达到平台期后的模型输出：


以下是我的实验的具体内容：
架构（深度 1）：
UNet(
（编码器）： ModuleList(
(0): Sequential(
(0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(1): ReLU(inplace=True)
(2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(3): ReLU(inplace=True)
)
)
(解码器): ModuleList(
(0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
(1): Sequential(
(0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(1): ReLU（就位=True）
（2）：Conv2d（64，64，kernel_size=（3，3），stride=（1，1），padding=（1，1））
（3）：ReLU（就位=True）
）
）
（池）：MaxPool2d（kernel_size=2，stride=2，padding=0，dilation=1，ceil_mode=False）
（瓶颈）：Sequential（
（0）：Conv2d（64，128，kernel_size=（3，3），stride=（1，1），padding=（1，1））
（1）：ReLU（就位=True）
（2）：Conv2d（128，128，kernel_size=（3，3），stride=（1，1），padding=（1，1））
（3）：ReLU（就位=True）
)
(out_conv): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
)

初始化：凯明法线
 def _initialize_weights(self) -&gt; None:
for m in self.modules():
if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
nn.init.kaiming_normal_(m.weight, mode=&#39;fan_in&#39;, nonlinearity=&#39;relu&#39;)
if m.bias is not None:
nn.init.constant_(m.bias, 0)

上采样方法：裁剪和连接
 def crop_and_concat(self, upsampled: torch.Tensor,
passive: torch.Tensor) -&gt; torch.Tensor:
diffY =pass.size()[2] - upsampled.size()[2]
diffX =pass.size()[3] - upsampled.size()[3]
upsampled = F.pad(upsampled, (diffX // 2, diffX - diffX // 2,
diffY // 2, diffY - diffY // 2))
return torch.cat((upsampled,bypass), dim=1)

损失：MSELoss
优化器：Adam，学习率从 1e-3 到 1e-4（上面的一切都在震荡，下面的也达到了稳定状态并且收敛速度较慢）]]></description>
      <guid>https://stackoverflow.com/questions/78642145/u-net-unable-to-overfit-single-training-example-loss-plateaus</guid>
      <pubDate>Wed, 19 Jun 2024 11:09:33 GMT</pubDate>
    </item>
    <item>
      <title>如何将媒体管道 3D 关键点转换为骨架的欧拉角？</title>
      <link>https://stackoverflow.com/questions/78424171/how-can-i-convert-mediapipe-3d-keypoints-to-eular-angles-of-a-skeleton</link>
      <description><![CDATA[我有一个代码片段，用于将 openpose 3D 关键点转换为骨架的欧拉角。从 openpose 检测到的关键点可以在下图中看到：

将这些 3D 关键点转换为骨架的欧拉角的代码是：
def pose2euler(self, pose, header):
channel = []
quats = {}
eulers = {}
stack = [header.root]
while stack:
node = stack.pop()
joint = node.name
joint_idx = self.keypoint2index[joint]

if node.is_root:
channel.extend(pose[joint_idx])

index = self.keypoint2index
order = None
if joint == &#39;臀部&#39;：
x_dir = pose[index[&#39;左臀部&#39;]] - pose[index[&#39;右臀部&#39;]]
y_dir = 无
z_dir = pose[index[&#39;脊柱&#39;]] - pose[joint_idx]
order = &#39;zyx&#39;
elif 关节在 [&#39;右臀部&#39;，&#39;右膝&#39;]：
child_idx = self.keypoint2index[node.children[0].name]
x_dir = pose[index[&#39;臀部&#39;]] - pose[index[&#39;右臀部&#39;]]
y_dir = 无
z_dir = pose[joint_idx] - pose[child_idx]
order = &#39;zyx&#39;
elif 关节在 [&#39;左臀部&#39;，&#39;左膝&#39;]：
child_idx = self.keypoint2index[node.children[0].name]
x_dir = pose[index[&#39;左臀部&#39;]] - pose[index[&#39;臀部&#39;]]
y_dir = 无
z_dir = pose[joint_idx] - pose[child_idx]
order = &#39;zyx&#39;
elif 关节 == &#39;脊椎&#39;:
x_dir = pose[index[&#39;左髋&#39;]] - pose[index[&#39;右髋&#39;]]
y_dir = 无
z_dir = pose[index[&#39;胸部&#39;]] - pose[joint_idx]
order = &#39;zyx&#39;
elif 关节 == &#39;胸部&#39;:
x_dir = pose[index[&#39;左肩&#39;]] - \
pose[index[&#39;右肩&#39;]]
y_dir = 无
z_dir = pose[joint_idx] - pose[index[&#39;脊椎&#39;]]
order = &#39;zyx&#39;
elif 关节 == &#39;颈部&#39;:
x_dir = 无
y_dir = pose[index[&#39;胸廓&#39;]] - pose[joint_idx]
z_dir = pose[index[&#39;头端站点&#39;]] - pose[index[&#39;胸廓&#39;]]
order = &#39;zxy&#39;
elif 关节 == &#39;左肩&#39;:
x_dir = pose[index[&#39;左肘&#39;]] - pose[joint_idx]
y_dir = pose[index[&#39;左肘&#39;]] - pose[index[&#39;左腕&#39;]]
z_dir = None
order = &#39;xzy&#39;
elif 关节 == &#39;左肘&#39;:
x_dir = pose[index[&#39;左腕&#39;]] - pose[joint_idx]
y_dir = pose[joint_idx] - pose[index[&#39;左肩&#39;]]
z_dir = None
order = &#39;xzy&#39;
elif 关节 == &#39;右肩&#39;:
x_dir = pose[joint_idx] - pose[index[&#39;右肘&#39;]]
y_dir = pose[index[&#39;右肘&#39;]] - pose[index[&#39;右腕&#39;]]
z_dir = 无
order = &#39;xzy&#39;
elif joint == &#39;右肘&#39;:
x_dir = pose[joint_idx] - pose[index[&#39;右腕&#39;]]
y_dir = pose[joint_idx] - pose[index[&#39;右肩&#39;]]
z_dir = 无
order = &#39;xzy&#39;
if order:
dcm = math3d.dcm_from_axis(x_dir, y_dir, z_dir, order)
quats[joint] = math3d.dcm2quat(dcm)
else:
quats[joint] = quats[self.parent[joint]].copy()

local_quat = quats[joint].copy()
if node.parent:
local_quat = math3d.quat_divide(
q=quats[joint], r=quats[node.parent.name]
)

euler = math3d.quat2euler(
q=local_quat, order=node.rotation_order
)
euler = np.rad2deg(euler)
eulers[joint] = euler
channel.extend(euler)

for child in node.children[::-1]:
if not child.is_end_site:
stack.append(child)

return channel

现在我想实现 mediapipe 关键点的欧拉角，但我看不懂上面的代码。 x_dir、y_dir 和 z_dir 是如何计算的？为什么？有人能给我解释一下吗？我正在做一个项目，已经在这里呆了 4 周了。Mediapipe 计算出的关键点如下图所示：

仅适用于姿势关键点，不适用于面部。]]></description>
      <guid>https://stackoverflow.com/questions/78424171/how-can-i-convert-mediapipe-3d-keypoints-to-eular-angles-of-a-skeleton</guid>
      <pubDate>Fri, 03 May 2024 10:38:56 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“anomalib.engine”的模块</title>
      <link>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</link>
      <description><![CDATA[# 导入所需模块

从 anomalib.data 导入 MVTec
从 anomalib.models 导入 Patchcore
从 anomalib.engine 导入 Engine

错误：
ModuleNotFoundError：没有名为“anomalib.engine”的模块

我正在尝试运行它......已按照库安装并看到
https://anomalib.readthedocs.io/en/latest/markdown/get_started/anomalib.html
我认为这要么是因为引擎已被修改，要么已被库删除......
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</guid>
      <pubDate>Sat, 03 Feb 2024 05:25:02 GMT</pubDate>
    </item>
    </channel>
</rss>