<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 07 Jan 2024 03:14:46 GMT</lastBuildDate>
    <item>
      <title>如何将 JSON 数据对象引入 LLM 模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77770292/how-to-ingest-json-data-object-into-an-llm-model</link>
      <description><![CDATA[我正在寻求开发一个自定义（langchain、llama 或 openAI）LLM 模型，该模型可以摄取 JSON 数据对象格式的股票数据，如下所示：
示例数据
&lt;前&gt;&lt;代码&gt;[
    {
        “符号”：“AAPL”，
        “价格”：178.72，
        “贝塔”：1.286802，
        “平均成交量”：58405568，
        “mktCap”：2794144143933，
        “最后一个Div”：0.96，
        “范围”：“124.17-198.23”，
        “变化”：-0.13，
        “公司名称”：“苹果公司”，
        “货币”：“美元”，
        “cik”：“0000320193”，
        “isin”：“US0378331005”，
        “尖头”：“037833100”，
        “交易所”：“纳斯达克全球精选”，
        “exchangeShortName”：“纳斯达克”，
        “行业”：“消费电子产品”，
        “网站”：“https://www.apple.com”，
        “首席执行官”：“先生”蒂莫西·D·库克”，
        “部门”：“技术”，
        “国家”：“美国”，
        “全职员工”：“164000”，
        “电话”：“408 996 1010”，
        “地址”：“苹果公园路一号”，
        “城市”：“库比蒂诺”，
        “州”：“CA”，
        “邮编”：“95014”，
        “dcfDiff”：4.15176，
        “DCF”：150.082，
        “图像”：“https://financialmodelingprep.com/image-stock/AAPL.png”，
        “ipoDate”：“1980-12-12”，
        “默认图像”：假，
        “isEtf”：假，
        “isActivelyTrading”：true，
        “isAdr”：假，
        “isFund”：假
    }
]

我目前正在处理一个数据集，其中包括纽约证券交易所和纳斯达克的所有股票代码。本质上，我很好奇将这些数据输入语言模型的最佳方法。我应该使用 .txt 文件来实现此目的，还是需要与 API 交互以动态检索数据？此外，是否可以直接向 LLM 提供表格数据对象？
摘要
我的目标是建立一个本地托管模型，可以解释自然语言查询，例如“提供价格低于 40 美元且属于技术领域的所有股票行情？”这将导致根据指定的条件检索相关库存数据。]]></description>
      <guid>https://stackoverflow.com/questions/77770292/how-to-ingest-json-data-object-into-an-llm-model</guid>
      <pubDate>Sat, 06 Jan 2024 17:12:22 GMT</pubDate>
    </item>
    <item>
      <title>为自己的家庭助理编程有多困难？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77770135/how-difficult-would-it-be-to-program-your-own-home-assistant</link>
      <description><![CDATA[作为一名程序员，我最大的梦想之一就是为我自己的家庭助理编程。就像托尼·斯塔克的星期五一样。我也喜欢作为开发人员与人工智能和法学硕士合作，我很想知道是否有人知道从哪里开始这样的项目的资源。]]></description>
      <guid>https://stackoverflow.com/questions/77770135/how-difficult-would-it-be-to-program-your-own-home-assistant</guid>
      <pubDate>Sat, 06 Jan 2024 16:24:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在Siamese网络中实现Triplet损失？</title>
      <link>https://stackoverflow.com/questions/77769407/how-to-implement-triplet-loss-in-siamese-network</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77769407/how-to-implement-triplet-loss-in-siamese-network</guid>
      <pubDate>Sat, 06 Jan 2024 12:38:05 GMT</pubDate>
    </item>
    <item>
      <title>RandomizedSearchCV 独立于集成中的模型</title>
      <link>https://stackoverflow.com/questions/77769033/randomizedsearchcv-independently-on-models-in-an-ensemble</link>
      <description><![CDATA[假设我构建了两个估计器的集合，其中每个估计器运行自己的参数搜索：
导入和回归数据集：
从 sklearn.ensemble 导入 VotingRegressor、StackingRegressor、RandomForestRegressor
从 sklearn.tree 导入 DecisionTreeRegressor
从 sklearn.datasets 导入 make_regression

从 sklearn.model_selection 导入 RandomizedSearchCV

X, y = make_regression()

定义两个自调整估计器，并将它们组合起来：
rf_param_dist = dict(n_estimators=[1, 2, 3, 4, 5])
rf_searcher = RandomizedSearchCV(RandomForestRegressor(), rf_param_dist, n_iter=5, cv=3)

dt_param_dist = dict(max_深度=[4, 5, 6, 7, 8])
dt_searcher = RandomizedSearchCV(DecisionTreeRegressor(), dt_param_dist, n_iter=5, cv=3)

合奏 = StackingRegressor(
    [（&#39;rf&#39;，rf_searcher），（&#39;dt&#39;，dt_searcher）]
).fit(X, y)

我的问题是关于sklearn如何处理ensemble的拟合。
Q1）我们有两个并行的未拟合估计器，并且都需要在 ensemble.predict(...) 工作之前进行拟合。但是，如果没有首先从整体中获得预测，我们就无法拟合任何估计器。 sklearn 如何处理这种循环依赖？
Q2）由于我们有两个运行独立调整的估计器，每个估计器是否会错误地假设另一个估计器的参数是固定的？因此，我们最终遇到了一个定义不明确的优化问题。
&lt;小时/&gt;
作为参考，我认为联合优化集成模型的正确方法是定义一个联合搜索所有参数的 CV，如下所示。但我的问题是关于 sklearn 如何处理前面描述的特殊情况。
#联合优化
合奏 = VotingRegressor(
    [ (&#39;rf&#39;, RandomForestRegressor()), (&#39;dt&#39;, DecisionTreeRegressor()) ]
）

jointsearch_param_dist = 字典(
    rf__n_estimators=[1, 2, 3, 4, 5],
    dt__max_深度=[4,5,6,7,8]
）

ensemble_jointsearch = RandomizedSearchCV(ensemble, jointsearch_param_dist)
]]></description>
      <guid>https://stackoverflow.com/questions/77769033/randomizedsearchcv-independently-on-models-in-an-ensemble</guid>
      <pubDate>Sat, 06 Jan 2024 10:35:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么 cartpole 奖励不收敛？</title>
      <link>https://stackoverflow.com/questions/77766359/why-is-cartpole-reward-not-converging</link>
      <description><![CDATA[通过此图像，训练损失和期望值随着时间的推移而收敛，但每个情节的回报没有收敛，即使是伟大的情节。
这是我的训练循环代码：（抱歉，如果有一些混乱的事情）
def selectAction(状态):
    全局步骤_完成
    样本 = random.random()
    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)
    步骤_完成 += 1
    如果样品&gt; eps_阈值：
        使用 torch.no_grad()：
            返回policy_net(state).max(1).indices.view(1, 1)
    别的：
        返回 torch.tensor([env.action_space.sample()], device=device, dtype=torch.long).unsqueeze(0)
            
def 学习():
    如果steps_done &lt; BATCH_SIZE：
        返回

    转换=内存.样本(BATCH_SIZE)
    批处理 = 过渡(*zip(*过渡))
    
    state_batch = torch.cat(batch.state)
    action_batch = torch.cat(batch.action)
    奖励_batch = torch.cat(batch.reward)
    
    next_state_batch = torch.cat(batch.next_state)
    state_action_values = policy_net(state_batch).gather(1, action_batch)
    
    使用 torch.no_grad()：
        argmax_action = target_net(next_state_batch).max(1)[1].view(1,BATCH_SIZE)
    预期状态动作值 = 奖励批次 + GAMMA * target_net(next_state_batch).gather(1, argmax_action)

    损失 = loss_func(state_action_values,expected_state_action_values)
    
    损失.追加（损失.项目（））
    Expected_values.extend(expected_state_action_values.detach().numpy())

    优化器.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)
    优化器.step()
    
对于范围内的剧集（500）：
    状态，信息 = env.reset()
    状态 = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
    总奖励 = 0

    对于 count() 中的 t：
        env.render()
        动作=选择动作（状态）
        观察、奖励、完成、_ = env.step(action.item())[:4]
        总奖励+=奖励
        奖励 = torch.tensor([奖励], 设备=设备)

        next_state = torch.tensor(观察, dtype=torch.float32, device=device).unsqueeze(0)
        memory.push（状态，操作，next_state，奖励）
        状态 = 下一个状态
        
        学习（）
        
        target_net_state_dict = target_net.state_dict()
        policy_net_state_dict=policy_net.state_dict()
        对于policy_net_state_dict中的参数：
            target_net_state_dict[参数]=policy_net_state_dict[参数]*TAU+policy_net_state_dict[参数]*(1-TAU)

        target_net.load_state_dict(target_net_state_dict)
        
        如果完成：
            Episode_durations.append（总奖励）
            绘图持续时间（）
            休息
    print(&#39;i_episode:&#39;, 剧集)
    打印（&#39;学习步骤：&#39;，steps_done）

all_total_reward = Episode_durations
打印（“完成”）

在此处输入图像描述
有什么方法可以帮助看到收敛吗？
我尝试了很多次，有时候奖励增加了，但还是不收敛。]]></description>
      <guid>https://stackoverflow.com/questions/77766359/why-is-cartpole-reward-not-converging</guid>
      <pubDate>Fri, 05 Jan 2024 17:51:33 GMT</pubDate>
    </item>
    <item>
      <title>AWS ElasticBean CodePipeline 部署一次又一次失败。我缺少什么？</title>
      <link>https://stackoverflow.com/questions/77766259/aws-elasticbean-codepipeline-deployment-failed-again-and-again-what-am-i-missi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77766259/aws-elasticbean-codepipeline-deployment-failed-again-and-again-what-am-i-missi</guid>
      <pubDate>Fri, 05 Jan 2024 17:28:46 GMT</pubDate>
    </item>
    <item>
      <title>ML 模型与 ReactNative 集成</title>
      <link>https://stackoverflow.com/questions/77764870/ml-model-integration-with-reactnative</link>
      <description><![CDATA[我正在尝试集成以检查 React Native 中的 ML 模型。我在 google colab 上制作了一个模型，并将训练后的模型保存为 Model.pkl 和 Model.joblib 现在我想将此模型集成到我的 React Native 应用程序中，但我不知道方法，请帮助我！
我一直在尝试在chatgpt的帮助下运行，但它对我不起作用]]></description>
      <guid>https://stackoverflow.com/questions/77764870/ml-model-integration-with-reactnative</guid>
      <pubDate>Fri, 05 Jan 2024 13:23:18 GMT</pubDate>
    </item>
    <item>
      <title>TF Transformer 模型永远不会过拟合，只会停滞不前：训练曲线的解读和改进建议</title>
      <link>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</link>
      <description><![CDATA[此训练曲线适用于 Transformer 模型，该模型处理 2D（不包括批次）顺序信号并使用 Adam 优化器、32 批次大小和学习率：一个自定义 LR 调度程序，它复制在“Attention is”中使用的预热调度程序所有你需要的&#39;纸。训练曲线如下所示，最终训练损失略低于验证损失，但训练损失永远不会开始回升，我将其解释为模型永远不会开始过度拟合，只是在 90 纪元后停止重新调整权重。
更好的解释和解决方案来改进这个模型？

下面是我的简短的可重现代码：
x_train = np.random.normal(size=(32, 512, 512))
批量大小 = 32
H, W = x_train.shape
行，列= np.indices（（H，W），稀疏= True）
padding_mask_init = np.zeros((H, W, W), dtype=np.bool_)
padding_mask_init[行，1：，列] = 1
padding_mask = padding_mask_init[:batch_size]
嵌入尺寸 = 512
密集_暗 = 2048
头数 = 2
形状 = (batch_size, embed_dim, 512) #(32, 512, 512)
解码器_输入=层.输入（batch_input_shape=形状，dtype=tensorflow.float16）
mha_1 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
mha_2 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
Layernorm_1 = 层.LayerNormalization()

Z = 解码器输入
Z = mha_1(查询=Z、值=Z、键=Z、use_causal_mask=True、attention_mask=padding_mask)
Z = layernorm_1(Z + 解码器输入)
Z = mha_2(查询=Z，值=解码器输入，键=解码器输入，attention_mask=padding_mask)
输出=layers.TimeDistributed（keras.layers.Dense（embed_dim，激活=“softmax”））（Z）

模型 = keras.Model(decoder_inputs, 输出)
model.compile（损失=“mean_squared_error”，optimizer=tf.keras.optimizers.Adam（learning_rate=lr_schedule（embed_dim，3000），beta_1=0.9，beta_2=0.98，epsilon=1.0e-9），metrics=[&quot; “准确度”]）

历史= model.fit（数据集，epochs = 200，validation_data = val_dataset）
]]></description>
      <guid>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</guid>
      <pubDate>Fri, 05 Jan 2024 02:47:25 GMT</pubDate>
    </item>
    <item>
      <title>DataFrame'对象没有属性'符号</title>
      <link>https://stackoverflow.com/questions/77755413/dataframe-object-has-no-attribute-symbol</link>
      <description><![CDATA[我想使用机器学习创建股票价格预测，但出现“‘DataFrame’对象没有属性‘符号’”我的错误是什么以及如何修复它
将 numpy 导入为 np
将 pandas 导入为 pd
从sklearn导入预处理
从 sklearn.model_selection 导入 train_test_split
从 sklearn. Linear_model 导入 LinearRegression

def prepare_data(df,forecast_col,forecast_out,test_size) :
    label = df[forecast_col].shift(-forecast_out) #创建名为 label 的新列，最后 5 行为 nan
    X = np.array(df [[forecast_col]]) #创建特征数组
    X = preprocessing.scale(X) #处理特征数组
    X_lately = X[-forecast_out:] #创建我想稍后在预测方法中使用的列
    X = X[:-forecast_out] # X 将包含训练和测试
    label.dropna(inplace=True) #删除na值
    y = np.array(label) # 分配 Y
    X_train,X_test,Y_train,Y_test = train_test_split(X, y, test_size=test_size, random_state=0) #交叉验证

    响应 = [X_train,X_test,Y_train,Y_test,X_lately]
    返回响应

df = pd.read_csv(“GOOG.csv”)
df = df[df.symbol == &quot;GOOG&quot;]- &quot;错误信息出现的位置&quot;
Forecast_col = “关闭”
预测输出 = 5
   

测试大小 = 0,2

X_train、X_test、Y_train、Y_test、X_lately = 准备数据（df、forecast_col、forecast_out、test_size）
学习者 = 线性回归()
learner.fit (X_train,Y_train )
Score=learner.score(X_test,Y_test)#测试线性回归模型
Forecast= learner.predict(X_lately) #将包含预测数据的集合
响应={}#creting json 对象
响应[&#39;test_score&#39;]=分数
响应[&#39;forecast_set&#39;]=预测

打印（响应）
]]></description>
      <guid>https://stackoverflow.com/questions/77755413/dataframe-object-has-no-attribute-symbol</guid>
      <pubDate>Thu, 04 Jan 2024 01:24:44 GMT</pubDate>
    </item>
    <item>
      <title>创建多语言聊天机器人</title>
      <link>https://stackoverflow.com/questions/77737679/create-a-multilingual-chatbot</link>
      <description><![CDATA[我使用 PyTorch 创建了一个聊天机器人，我想让它支持法语。请注意，我想训练聊天机器人，以便它能够回答技术问题。
我想到的一件事是使用翻译 API，但由于聊天机器人需要回答技术问题，翻译 API 可能会提供不准确的信息]]></description>
      <guid>https://stackoverflow.com/questions/77737679/create-a-multilingual-chatbot</guid>
      <pubDate>Sat, 30 Dec 2023 23:05:27 GMT</pubDate>
    </item>
    <item>
      <title>如何用视频数据训练一些模型</title>
      <link>https://stackoverflow.com/questions/77733070/how-to-train-some-model-with-video-data</link>
      <description><![CDATA[我用这样的图像训练了我的模型：
类 ASDataset(数据集):
    def __init__(self, client_file: str, imposter_file: str, 转换=无):
        将 open(client_file, &quot;r&quot;) 作为 f：
            client_files = f.read().splitlines()
        将 open(imposter_file, &quot;r&quot;) 作为 f：
            imposter_files = f.read().splitlines()
        self.labels = torch.cat((torch.ones(len(client_files)), torch.zeros(len(imposter_files))))
        self.imgs = client_files + imposter_files
        self.transforms = 变换

    def __len__(自身):
        返回 len(self.imgs)

    def __getitem__(self, idx):
        img_name = self.imgs[idx]
        img = Image.open(img_name)
        标签 = self.labels[idx]
        如果自我变换：
            img = self.transforms(img)
        返回图片、标签

train_dataset = ASDataset(client_file=“/kaggle/input/nuaaaa/raw/client_train_raw.txt”，imposter_file=“/kaggle/input/nuaaaa/raw/imposter_train_raw.txt”，transforms=预处理)
val_dataset = ASDataset(client_file=“/kaggle/input/nuaaaa/raw/client_test_raw.txt”，imposter_file=“/kaggle/input/nuaaaa/raw/imposter_test_raw.txt”，transforms=预处理)

# 创建数据加载器
train_loader = DataLoader(train_dataset,batch_size=8,shuffle=True)
val_loader = DataLoader(val_dataset,batch_size=8,shuffle=False)


但现在我有了视频数据，据我了解，我需要更改 class ASDataset。我尝试了不同的变体。例如：
类VideoDataset（数据集）：
    def __init__(self, video_file: str, 标签: int, 转换=无):
        self.video = cv2.VideoCapture(video_file)
        self.label = 标签
        self.transforms = 变换

    def __len__(自身):
        return int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))

    def __getitem__(self, idx):
        self.video.set(cv2.CAP_PROP_POS_FRAMES, idx)
        成功，frame = self.video.read()
        如果没有成功：
            raise ValueError(“读取帧失败”)
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # 转换为RGB
        如果自我变换：
            框架 = self.transforms(框架)
        返回框架，自我标签

但是没有人给我结果。
请帮助我，如何使用视频数据进行训练？
数据示例client_train_raw.txt：
&lt;前&gt;&lt;代码&gt;/kaggle/input/dfdcdfdc/DFDCDFDC/dbnygxtwek.mp4
/kaggle/input/dfdcdfdc/DFDCDFDC/dbtbbhakdv.mp4
/kaggle/输入/dfdcdfdc/DFDCDFDC/ddepeddixj.mp4
]]></description>
      <guid>https://stackoverflow.com/questions/77733070/how-to-train-some-model-with-video-data</guid>
      <pubDate>Fri, 29 Dec 2023 16:49:09 GMT</pubDate>
    </item>
    <item>
      <title>Llama 2 上的 PEFT QLoRA 培训</title>
      <link>https://stackoverflow.com/questions/77725437/peft-qlora-training-on-llama-2</link>
      <description><![CDATA[这是一个更具概念性的问题。我正在尝试在 Llama 2 上执行 PEFT QLoRA，特别是在 imdb 电影评论数据集上。我仅使用 650 个样本进行训练，使用 650 个样本进行测试。我使用了“meta-llama/Llama-2-7b-chat-hf”模型作为我的基本 llama 2 模型。使用 SFTTrainer 训练后，我将模型保存到目录中。如果我没有记错的话，只有适配器权重会保存到目录中，而不是整个模型权重。完成此操作后，我知道这些适配器权重可以与原始模型权重一起加载。
模型 = PeftModel.from_pretrained(
    模型，
    “./my_dir”，
）

完成此操作后，我们应该将这些适配器权重合并到原始模型
merged_model = model.merge_and_unload()

但是，当我使用此 merged_model 进行推理时，我注意到性能非常差，因为仅对 PEFT 加载的模型进行推理，即来自
模型 = PeftModel.from_pretrained(
    模型，
    “./my_dir”，
）

是理想的。这种行为是预期的吗？我正在这样进行推理
tokenizer = AutoTokenizer.from_pretrained(“meta-llama/Llama-2-7b-chat-hf”)
管道=变压器.管道(
    “文本生成”，
    型号=型号，
    分词器=分词器，
    torch_dtype=torch.float16,
    device_map=“自动”，
）

序列=管道（
    迅速的，
    do_sample=真，
    顶部_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    最大长度=500，
）
对于序列中的 seq：
    print(f&quot;结果: {seq[&#39;generate_text&#39;]}&quot;)

还有什么我可以做得更好的吗？]]></description>
      <guid>https://stackoverflow.com/questions/77725437/peft-qlora-training-on-llama-2</guid>
      <pubDate>Thu, 28 Dec 2023 07:01:41 GMT</pubDate>
    </item>
    <item>
      <title>llama.cpp llama_cublas 已启用，但运行 ./main 时仅使用 75mb/6gb 的 vram</title>
      <link>https://stackoverflow.com/questions/77354443/llama-cpp-llama-cublas-enabled-but-only-75mb-6gb-of-vram-used-when-running-ma</link>
      <description><![CDATA[我启用了 llama_cublas 来与 nvidia cuda 工具包一起使用
使 LLAMA_CUBLAS=1

编译得很好
但是当我运行模型并监控 nvidia-smi 内存消耗时，只使用了 75mb。见下文。
llm_load_tensors：使用CUDA进行GPU加速
llm_load_tensors：所需内存 = 13189.99 MB
llm_load_tensors：将 0 个重复层卸载到 GPU
llm_load_tensors：将 0/43 层卸载到 GPU
llm_load_tensors：使用的 VRAM：0.00 MB
...................................................... ......................................................
llama_new_context_with_model：n_ctx = 512
llama_new_context_with_model：freq_base = 10000.0
llama_new_context_with_model：freq_scale = 1
llama_new_context_with_model：kv 自身大小 = 400.00 MB
llama_new_context_with_model：计算缓冲区总大小 = 81.13 MB
llama_new_context_with_model：VRAM 暂存缓冲区：75.00 MB
llama_new_context_with_model：使用的总 VRAM：75.00 MB（模型：0.00 MB，上下文：75.00 MB）

nvidia smi 输出
2023 年 10 月 24 日星期二 10:53:17
+------------------------------------------------ --------------------------------------+
| NVIDIA-SMI 535.113.01 驱动程序版本：535.113.01 CUDA 版本：12.2 |
|------------------------------------------+----- ---------------+--------------------+
| GPU 名称持久性-M |总线 ID Disp.A |挥发性未校正。 ECC |
|风扇温度性能功率：使用/上限 |内存使用情况 | GPU-Util 计算 M。
| | |米格·M。
|============================================+======== ==============+======================|
| 0 NVIDIA GeForce RTX 4050 ...关闭| 00000000:01:00.0 关闭 |不适用 |
|不适用 42C P8 5W / 30W | 89MiB / 6141MiB | 0% 默认 |
| | |不适用 |
+----------------------------------------------------+----- ---------------+--------------------+
                                                                                         
+------------------------------------------------ --------------------------------------+
|流程：|
| GPU GI CI PID 类型 进程名称 GPU 内存 |
| ID ID 用途 |
|=================================================== ========================================|
| 0 不适用 不适用 1991 G /usr/lib/xorg/Xorg 4MiB |
+------------------------------------------------ --------------------------------------+
]]></description>
      <guid>https://stackoverflow.com/questions/77354443/llama-cpp-llama-cublas-enabled-but-only-75mb-6gb-of-vram-used-when-running-ma</guid>
      <pubDate>Tue, 24 Oct 2023 18:03:49 GMT</pubDate>
    </item>
    <item>
      <title>如何为 Transformer 实现位置明智的前馈神经网络？</title>
      <link>https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers</link>
      <description><![CDATA[我很难理解 Transformer 架构中的位置明智前馈神经网络。

让我们以机器翻译任务为例，其中输入是句子。从图中我了解到，对于每个单词，不同的前馈神经网络用于自注意力子层的输出。前馈层应用类似的线性变换，但每个变换的实际权重和偏差不同，因为它们是两个不同的前馈神经网络。
参考链接，这里是类PositionWiseFeedForward神经网络
class PositionwiseFeedForward(nn.Module)：
    “实现 FFN 方程。”
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def 前向（自身，x）：
        返回 self.w_2(self.dropout(F.relu(self.w_1(x))))

我的问题是：
我没有看到任何关于此的立场。这是一个简单的两层全连接神经网络。假设 x 是句子中每个单词的嵌入列表，句子中的每个单词都由上层使用相同的权重和偏差集进行转换。（如果我错了，请纠正我）
我期望找到类似将每个单词嵌入传递到单独的Linear层之类的东西，该层将具有不同的权重和偏差，以实现与图片中所示类似的效果。]]></description>
      <guid>https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers</guid>
      <pubDate>Mon, 02 Jan 2023 05:59:25 GMT</pubDate>
    </item>
    <item>
      <title>用于分类特征的 LabelEncoder？</title>
      <link>https://stackoverflow.com/questions/61217713/labelencoder-for-categorical-features</link>
      <description><![CDATA[这可能是一个初学者问题，但我见过很多人使用 LabelEncoder() 用序数替换分类变量。很多人通过一次传递多个列来使用此功能，但是我对我的某些功能中的序数错误以及它将如何影响我的模型有些怀疑。这是一个例子：
输入
导入 pandas 作为 pd
将 numpy 导入为 np
从 sklearn.preprocessing 导入 LabelEncoder

a = pd.DataFrame([&#39;高&#39;,&#39;低&#39;,&#39;低&#39;,&#39;中&#39;])
le = 标签编码器()
le.fit_transform(a)

输出
数组([0, 1, 1, 2], dtype=int64)

如您所见，序数值未正确映射，因为我的 LabelEncoder 只关心列/数组中的顺序（应该是 High=1、Med=2、Low=3，反之亦然）。错误的映射会对模型产生多大的影响？除了 OrdinalEncoder() 之外，是否有一种简单的方法可以正确映射这些值？]]></description>
      <guid>https://stackoverflow.com/questions/61217713/labelencoder-for-categorical-features</guid>
      <pubDate>Tue, 14 Apr 2020 21:40:52 GMT</pubDate>
    </item>
    </channel>
</rss>