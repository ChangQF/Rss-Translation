<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 30 Jun 2024 15:15:43 GMT</lastBuildDate>
    <item>
      <title>使用嵌入技术从数据库中进行人脸识别</title>
      <link>https://stackoverflow.com/questions/78688976/face-recognize-from-the-database-using-embedding-technique</link>
      <description><![CDATA[我目前正在开展一个项目，旨在识别大学记录中是否存在任何个人的照片。所提出的方法涉及将每个学生照片的嵌入及其详细信息存储在矢量数据库中。当需要比较照片时，系统将生成该照片的嵌入值，然后将该值与数据库进行比较。如果该值在特定阈值内，则表明该个人存在于记录中。
我正在寻求专家建议，以确定这种方法是否可行。如果对此方法有任何疑虑，我将不胜感激最佳解决方案的建议。]]></description>
      <guid>https://stackoverflow.com/questions/78688976/face-recognize-from-the-database-using-embedding-technique</guid>
      <pubDate>Sun, 30 Jun 2024 15:09:55 GMT</pubDate>
    </item>
    <item>
      <title>大数据的 n 分割计算存在问题</title>
      <link>https://stackoverflow.com/questions/78688933/having-problem-with-n-split-computation-for-large-data</link>
      <description><![CDATA[我正在执行元特征提取的任务，但计算在某些时候似乎消耗了所有可用内存，因此 Ubuntu 不断终止进程（Killed）以过度使用内存。
我决定将计算拆分为较小的任务，然后汇总最终结果。这似乎没问题，但我确实注意到一些指标计算失败了。我在两种情况下对小数据集进行了测试：1）使用整个数据集（因为它适合内存），2）使用我实现的 n_splits 计算。
我预计场景 2 的最终结果将大致接近场景 1。但是，它也无法计算这些度量。
为了给出 MWE，我使用 iris 数据集 进行了说明，如下所示：
!pip -q install pymfe # 用于元特征计算的库。

导入 numpy 作为 np
从 sklearn.datasets 导入 load_iris
从 pymfe.mfe 导入 MFE

data = load_iris()
X, y= data.data, data.target


场景 1 计算整个数据
features_to_compute = [&#39;f1&#39;, &#39;f2&#39;, &#39;f3&#39;, &#39;t1&#39;] # 元特征
summaries = [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;, &#39;sd&#39;] # 摘要

extractor = MFE(features=features_to_compute, groups=[&quot;complexity&quot;], summary=summaries)
extractor.fit(X,y)
res = extractor.extract()

# 结果
for i in range(len(res[0])):
var_sp = res[0][i].split(&#39;.&#39;)
g_name = var_sp[0]
print(f&quot;{res[0][i]} = {res[1][i]}\n&quot;)

f1.max = 0.599217152923665

f1.mean = 0.2775641932566493

f1.min = 0.05862828094263208

f1.sd = 0.2612622587707819

f2.max = 0.01914529914529914

f2.mean = 0.0063817663817663794

f2.min = 0.0

f2.sd = 0.011053543615254369

f3.max = 0.37

f3.mean = 0.12333333333333334

f3.min = 0.0

f3.sd = 0.21361959960016152

t1 = 0.12

这看起来不错。
场景 2 将超出可用内存的大型数据集拆分为较小的任务。
# 辅助函数
def split_dataset(X, y, n_splits):
# 将数据拆分为 n_splits 个较小的数据集。
split_X = np.array_split(X, n_splits)
split_y = np.array_split(y, n_splits)
return split_X, split_y

def compute_meta_features(X, y, features, summary):
# 计算给定数据集分割的元特征。
extractor = MFE(features=features, groups=[&quot;complexity&quot;], summary=summary)
extractor.fit(X,y)
return extractor.extract()

def average_results(results):
# 对多个分割的结果取平均值。
features = results[0][0]
summary_values = np.mean([result[1] for result in results], axis=0)
return features, summary_values

n_splits = 10
split_X, split_y = split_dataset(X, y, n_splits)

results = [compute_meta_features(X_part, y_part, features=features_to_compute,
summary=summaries) for X_part, y_part in zip(split_X, split_y)]
# 此处堆栈跟踪发出了几个警告，例如
0/dist-packages/pymfe/_internal.py:731: RuntimeWarning: 无法使用摘要“mean”总结特征“f2”。将设置为“np.nan”。
warnings.warn(
/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:731: RuntimeWarning: 无法使用摘要“max”总结功能“f2”。将设置为“np.nan”。
warnings.warn(
/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:731: RuntimeWarning: 无法使用摘要“min”总结功能“f2”。将设置为“np.nan”。
warnings.warn(
/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:731: RuntimeWarning: 无法使用摘要“sd”总结功能“f2”。将设置为“np.nan”。
warnings.warn(
/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:731: RuntimeWarning: 无法使用摘要“mean”总结特征“f3”。将设置为“np.nan”。


场景 2 的结果：
final_features, final_summary = average_results(results)
for i in range(len(final_features)):
var_sp = final_features[i].split(&#39;.&#39;)
g_name = var_sp[0]
print(f&quot;{final_features[i]} = {final_summary[i]}\n&quot;)

f1.max = 0.9378374974227318

f1.mean = 0.8736795575459622

f1.min = 0.8198589466408711

f1.sd = 0.058203211724503635

f2.max = nan

f2.mean = nan

f2.min = nan

f2.sd = nan

f3.max = nan

f3.mean = nan

f3.min = nan

f3.sd = nan

t1 = nan

f1、f2、f3、... 均为 nan。使用 openml volcanoesa1 数据集获得了类似的结果。
我不明白是什么原因造成的，问题出在哪里。如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78688933/having-problem-with-n-split-computation-for-large-data</guid>
      <pubDate>Sun, 30 Jun 2024 14:57:46 GMT</pubDate>
    </item>
    <item>
      <title>如何进入 ML？[关闭]</title>
      <link>https://stackoverflow.com/questions/78688663/how-to-get-into-ml</link>
      <description><![CDATA[您好，我使用 Python 编程大约 2 个月了。可以说我处于初学者和中级水平之间。我做过一些项目、一些游戏等等，但现在我不知道该做什么或学什么。我对 ML 非常感兴趣，但我不知道如何进入这个领域，甚至不知道我是否应该尝试它，或者更确切地说，做一些更简单的事情并获得更多技能。
感谢您的所有回答。]]></description>
      <guid>https://stackoverflow.com/questions/78688663/how-to-get-into-ml</guid>
      <pubDate>Sun, 30 Jun 2024 13:05:19 GMT</pubDate>
    </item>
    <item>
      <title>编码图像缩放后的质量下降</title>
      <link>https://stackoverflow.com/questions/78688658/degradation-after-scaling-of-coded-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78688658/degradation-after-scaling-of-coded-images</guid>
      <pubDate>Sun, 30 Jun 2024 13:01:40 GMT</pubDate>
    </item>
    <item>
      <title>数据集中每个类别的类别不平衡计算</title>
      <link>https://stackoverflow.com/questions/78687893/class-imbalance-calculation-for-each-class-in-a-dataset</link>
      <description><![CDATA[我正在尝试计算每个数据集中的类别不平衡，我的方法是检查计数的平均值和标准差。平均值是类别 1 中的样本总数/数据集中的样本总数。
 # 统计每个类别中的实例数
class_counts = df[class_column].value_counts()
total_samples = len(df)

# 计算不平衡率
imbalance_ratios = {class_name: count / total_samples for class_name, count in class_counts.items()}

# 计算不平衡率的平均值和标准差
ratios = list(imbalance_ratios.values())
avg = 1 / len(ratios) # 平均值
stddev = statistics.stdev(ratios)

# 确定识别不平衡的下限和上限
lower_limit = avg - stddev
upper_limit = avg + stddev

# 识别不平衡的类别
imbalance_classes = [class_name for class_name, ratio in不平衡率.项目（） 
如果比例&lt;下限或比例&gt; upper_limit]

print(imbalance_classes)

但在 keel 网站上，他们正在计算相对于多数类的类不平衡。
#计算每个类中的实例数
class_counts = df[class_column].value_counts()

#计算相对于多数类的不平衡率
majority_class_count = (class_counts).max()
imbalance_ratios = {}

for cls, count in class_counts.items():
balance_ratio = major_class_count / count
balance_ratios[cls] = floating_ratio

print(&quot;Imbalance ratios:&quot;)
for cls, ratio in floating_ratios.items():
print(f&quot;Class {cls}: {ratio:.2f}&quot;)

我不知道该使用哪一个；按照我的方法，我会平衡一些数据集，但在龙骨网站上，这些数据集被提及为不平衡的。我的目标是确定有多少类别是不平衡的，并列出它们。]]></description>
      <guid>https://stackoverflow.com/questions/78687893/class-imbalance-calculation-for-each-class-in-a-dataset</guid>
      <pubDate>Sun, 30 Jun 2024 07:13:21 GMT</pubDate>
    </item>
    <item>
      <title>如何在 FastAPI 中加载深度学习模型</title>
      <link>https://stackoverflow.com/questions/78687792/how-to-load-deep-learning-model-in-fastapi</link>
      <description><![CDATA[“我在使用 FastAPI 时遇到问题，无法使用 Keras 成功加载以 .keras 格式保存的机器学习模型。尽管按照标准程序使用 tensorflow.keras.models.load_model() 加载模型，但应用程序仍会抛出错误，指示文件未找到、FastAPI 和 TensorFlow 版本的兼容性问题或依赖项冲突。我已验证文件路径并确保安装了所有必要的依赖项，但问题仍然存在。详细的调试尝试包括检查 FastAPI、Keras 和 TensorFlow 之间的版本兼容性，以及确认模型文件可访问且格式正确。尽管做出了这些努力，但模型加载过程始终失败，阻碍了机器学习功能进一步集成到我的 FastAPI 应用程序中。寻求解决此问题的指导，以便将 Keras 模型成功集成到 FastAPI 中。”]]></description>
      <guid>https://stackoverflow.com/questions/78687792/how-to-load-deep-learning-model-in-fastapi</guid>
      <pubDate>Sun, 30 Jun 2024 06:14:54 GMT</pubDate>
    </item>
    <item>
      <title>调整超参数的步骤是什么？</title>
      <link>https://stackoverflow.com/questions/78687774/what-are-the-steps-to-adjusting-my-hyperparameters</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78687774/what-are-the-steps-to-adjusting-my-hyperparameters</guid>
      <pubDate>Sun, 30 Jun 2024 06:03:01 GMT</pubDate>
    </item>
    <item>
      <title>对 GAN 输出大小的困惑</title>
      <link>https://stackoverflow.com/questions/78687394/confusion-about-output-sizes-of-gan</link>
      <description><![CDATA[我正在尝试理解代码，我对测试单元感到困惑。当我打印输出的形状时，它是 hidden_​​output.shape =(num_test, 20, 4, 4), test_hidden_​​block_stride(hidden_​​output).shape) == (num_test, 20, 10, 10) 和 Gen_output.shape=(num_test, 1,28,28)（对于 Mnist 数据集）。我试图理解这里的大小是如何计算的。任何帮助都将不胜感激！
class Generator(nn.Module):
def __init__(self, z_dim=10, im_chan=1, hidden_​​dim=64):
super(Generator, self).__init__()
self.z_dim = z_dim
# 构建神经网络
self.gen = nn.Sequential(
self.make_gen_block(z_dim, hidden_​​dim * 4),
self.make_gen_block(hidden_​​dim * 4, hidden_​​dim * 2, kernel_size=4, stride=1),
self.make_gen_block(hidden_​​dim * 2, hidden_​​dim),
self.make_gen_block(hidden_​​dim, im_chan, kernel_size=4, final_layer=True),
)
def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, padding=0 ,final_layer=False):
# 构建神经块
layer = []
layer.append(nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding, output_padding=padding))
if not final_layer:
layer.append(nn.BatchNorm2d(output_channels))
layer.append(nn.ReLU(True))
else:
layer.append(nn.Tanh())

return nn.Sequential(*layers)
# 测试
gen = Generator()
num_test = 100
# 测试隐藏块
test_hidden_​​noise = get_noise(num_test, gen.z_dim)
test_hidden_​​block = gen.make_gen_block(10, 20, kernel_size=4, stride=1)
test_uns_noise = gen.unsqueeze_noise(test_hidden_​​noise)
hidden_​​output = test_hidden_​​block(test_uns_noise)
# 检查它是否与其他 strides 兼容
test_hidden_​​block_stride = gen.make_gen_block(20, 20, kernel_size=4, stride=2)
test_final_noise = get_noise(num_test, gen.z_dim) * 20
test_final_block = gen.make_gen_block(10, 20, final_layer=True)
test_final_uns_noise = gen.unsqueeze_noise(test_final_noise)
final_output = test_final_block(test_final_uns_noise)
# 测试整个过程：
test_gen_noise = get_noise(num_test, gen.z_dim)
test_uns_gen_noise = gen.unsqueeze_noise(test_gen_noise)
gen_output = gen(test_uns_gen_noise)

我正在尝试手动计算公式中的大小。我只是看到不同的内核大小、步幅和填充。不确定要使用哪些值。]]></description>
      <guid>https://stackoverflow.com/questions/78687394/confusion-about-output-sizes-of-gan</guid>
      <pubDate>Sun, 30 Jun 2024 00:41:37 GMT</pubDate>
    </item>
    <item>
      <title>使用类似数据集 (slack) 的消息回复对 llama3 进行微调</title>
      <link>https://stackoverflow.com/questions/78687005/fine-tune-llama3-with-message-replies-like-dataset-slack</link>
      <description><![CDATA[我想在一个数据集上微调 llama3，其中数据结构是考虑以下规则的消息列表：

有频道。
每个频道都有来自各种用户的消息。
每条消息可能都有与其上下文相对应的回复。

我已经有了抓取所有数据的逻辑，但我对数据集结构应该是什么样子有点困惑。
我读过 llama3 文档，看起来应该应用下面的模板（示例取自：https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/):
&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;

您是一位乐于助人的 AI 助手，可提供旅行提示和建议&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;

您能帮我什么忙？&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;

假设每个单独的消息/重放如下所示：
&lt;timestamp&gt; - &lt;user&gt;: &lt;message&gt;

数据的最终结果应该是什么样的？
它是一个字典列表吗？如果是，那么回复应该如何放置？
我敢于问 GPT4o，它给了我以下示例：
prompt_example_1 = [
{&quot;role&quot;: &quot;system&quot;, &quot;message&quot;: &quot;Channel: general&quot;},
{&quot;role&quot;: &quot;user&quot;, &quot;message&quot;: &quot;U12345678 [2023-06-01 12:00:00]: 频道中的主要消息&quot;},
{&quot;role&quot;: &quot;assistant&quot;, &quot;message&quot;: &quot;U87654321 [2023-06-01 12:05:00]: 回复主要消息message&quot;},
{&quot;role&quot;: &quot;assistant&quot;, &quot;message&quot;: &quot;U23456789 [2023-06-01 12:10:00]: 对主消息的另一条回复&quot;},
{&quot;role&quot;: &quot;user&quot;, &quot;message&quot;: &quot;U23456789 [2023-06-01 12:15:00]: 另一条主消息&quot;},
{&quot;role&quot;: &quot;assistant&quot;, &quot;message&quot;: &quot;U34567890 [2023-06-01 12:20:00]: 对第二条主消息的回复&quot;}
]

感觉不对对我来说。
如果我打乱数据集，会发生什么？所有回复都将失去与其父消息的关联。
我从 huggingface 中看到了这个文档：https://huggingface.co/docs/transformers/main/en/chat_templating
这让我想到我实际上可以按如下方式使用 apply_chat_template：
dataset = [
[
{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Channel: general&quot;},
{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;U12345678 [2023-06-01 12:00:00]: Main频道中的消息”},
{&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;U87654321 [2023-06-01 12:05:00]: 回复主消息”},
{&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;U23456789 [2023-06-01 12:10:00]: 再次回复主消息”}
],
[
{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;频道：随机&quot;},
{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;U23456789 [2023-06-01 12:15:00]: 另一条主消息&quot;},
{&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;U34567890 [2023-06-01 12:20:00]: 回复第二条主消息&quot;}
]
]

hf_dataset = Dataset.from_dict({&quot;chat&quot;: dataset})

hf_dataset = hf_dataset.map(lambda x: {
&quot;formatted_chat&quot;: tokenizer.apply_chat_template(
x[&quot;chat&quot;], 
tokenize=False, 
add_generation_prompt=False
)
})
]]></description>
      <guid>https://stackoverflow.com/questions/78687005/fine-tune-llama3-with-message-replies-like-dataset-slack</guid>
      <pubDate>Sat, 29 Jun 2024 20:35:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么循环中classification_report和precision_recall_fscore_support之间的敏感度（召回率）值不同？</title>
      <link>https://stackoverflow.com/questions/78686328/why-do-the-sensitivity-recall-values-differ-between-classification-report-and</link>
      <description><![CDATA[我正在使用 sklearn.datasets 中的 make_classification 生成的合成数据集，其中包含 5 个类别。我已经在此数据上训练了一个 RandomForestClassifier，并使用两种不同的方法评估其性能。但是，我观察到这两种方法的敏感度（召回率）值存在差异。
这是我使用的代码：
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classes_report, precision_recall_fscore_support
import numpy as np
import pandas as pd

# 生成包含 5 个类别的合成数据集
X, y = make_classification(n_samples=1000, n_classes=5, n_informative=10, n_clusters_per_class=1, random_state=42)

# 分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练分类器
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = clf.predict(X_test)

# 方法 1：classification_report
print(&quot;分类报告&quot;)
print(classification_report(y_test, y_pred))

# 方法 2：使用 precision_recall_fscore_support 循环
res = []

for l in range(5):
prec, recall, _, _ = precision_recall_fscore_support(np.array(y_test) == l,
np.array(y_pred) == l,
pos_label=True, average=None)
res.append([l, recall[0], recall[1]])

df = pd.DataFrame(res, columns=[&#39;class&#39;, &#39;sensitivity&#39;, &#39;specificity&#39;])
print(&quot;\nSensitivity and Specificity&quot;)
print(df)

输出：
分类报告
准确率 召回率 f1 分数 支持率

0 0.76 0.71 0.74 35
1 0.72 0.93 0.81 30
2 0.72 0.81 0.76 32
3 0.85 0.86 0.86 59
4 0.88 0.64 0.74 44

准确率 0.79 200
宏平均值 0.78 0.79 0.78 200
加权平均值 0.80 0.79 0.79 200

敏感度和特异性
类别敏感度特异性
0 0 0.951515 0.714286
1 1 0.935294 0.933333
2 2 0.940476 0.812500
3 3 0.936170 0.864407
4 4 0.974359 0.636364

问题：
为什么 classification_report 和使用 precision_recall_fscore_support 的循环之间的敏感度（召回率）值不同？具体来说，为什么 classification_report 报告的召回率值与循环方法中计算出的敏感度值之间存在差异？如果可能，您能否用一个简单的示例（手动解决）来展示它
您尝试了什么，您期望什么？
我使用了两种方法来评估我的 RandomForestClassifier 的性能。首先，我使用 classification_report 来获取每个类别的精确度、召回率和 F1 分数。然后，我使用带有 precision_recall_fscore_support 的循环计算每个类别的敏感度和特异性。
我期望循环方法中计算出的敏感度值与 classification_report 中的召回率值相匹配，因为敏感度和召回率在分类任务中通常被视为同义词。但是，我发现两组值之间存在差异。
实际上结果如何？
classification_report 中的召回率值与循环方法中计算的敏感度值不同。classification_report 在多类上下文中为每个类提供召回率值，而循环方法将每个类视为二元分类问题，从而导致不同的敏感度和特异性值。]]></description>
      <guid>https://stackoverflow.com/questions/78686328/why-do-the-sensitivity-recall-values-differ-between-classification-report-and</guid>
      <pubDate>Sat, 29 Jun 2024 14:57:35 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Android Studio 上使用可教机器分析机器学习模型（使用 Kotlin）</title>
      <link>https://stackoverflow.com/questions/78685944/failed-to-analyze-machine-learning-models-with-teachable-machine-on-android-stud</link>
      <description><![CDATA[我在分析照片时遇到了问题。我有 3 种模型，分别是癌症、非癌症和未知。当我尝试分析时，结果始终是“未知” 有人知道如何解决吗？我尝试更改我的代码并询问人工智能，但仍然卡住了。
这是我的代码
ImageClassifierHelper

class ImageClassifierHelper(
private var Threshold: Float = 0.5f, // Tingkatkan Threshold
private var maxResult: Int = 3,
private val modelName: String = &quot;model_unquant.tflite&quot;,
val context: Context,
val classifierListener: ClassifierListener?
) {
interface ClassifierListener {
fun onError(error: String)
fun onResults(result: MutableList&lt;Category&gt;?)
}

private var imageClassifier: ModelUnquant? = null

init {
setupImageClassifier()
}

private fun setupImageClassifier() {
try {
imageClassifier = ModelUnquant.newInstance(context)
} catch (e: IOException) {
classifierListener?.onError(context.getString(R.string.failed))
Log.e(TAG, e.message.toString())
}
}

伴随对象 {
private const val TAG = &quot;ImageClassifierHelper&quot;
}

// 按照您的模型添加标签列表
private val labels = listOf(&quot;Non-Cancer&quot;, &quot;Cancer&quot;, &quot;Unknown&quot;)

fun classifyStaticImage(imageUri: Uri) {
if (imageClassifier == null) {
setupImageClassifier()
}

// 将 Uri 转换为 Bitmap
val bitmap = uriToBitmap(context.contentResolver, imageUri)
?: run {
classifierListener?.onError(&quot;Failed to decrypt image&quot;)
return
}
if (bitmap == null) {
classifierListener?.onError(&quot;Failed to decrypt image&quot;)
return
}

val resizedBitmap = Bitmap.createScaledBitmap(bitmap, 224, 224, true)

//将 Bitmap 转换为 TensorImage
val tensorImage = TensorImage(DataType.FLOAT32)
tensorImage.load(resizedBitmap)

// 为输入创建 TensorBuffer
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 224, 224, 3), DataType.FLOAT32)
inputFeature0.loadBuffer(tensorImage.buffer)

// 调用推理模型并计算结果
val output = imageClassifier?.process(inputFeature0)
val outputFeature0 = output?.outputFeature0AsTensorBuffer

// 将 Bitmap 转换为类别列表
val probability = outputFeature0?.let { tensorBuffer -&gt;
tensorBuffer.floatArray.mapIndexed { 索引，值 -&gt;
Category(labels.getOrElse(index) { &quot;Unknown&quot; }, value)
}.filter { it.score &gt;= Threshold } // 根据阈值过滤
.toMutableList()
}

classifierListener?.onResults(probability)
}

private fun uriToBitmap(contentResolver: ContentResolver, uri: Uri): Bitmap? {
return try {
val inputStream = contentResolver.openInputStream(uri)
BitmapFactory.decodeStream(inputStream)
} catch (e: IOException) {
e.printStackTrace()
null
}
}
}

]]></description>
      <guid>https://stackoverflow.com/questions/78685944/failed-to-analyze-machine-learning-models-with-teachable-machine-on-android-stud</guid>
      <pubDate>Sat, 29 Jun 2024 12:13:59 GMT</pubDate>
    </item>
    <item>
      <title>q/kdb+ 中的机器学习</title>
      <link>https://stackoverflow.com/questions/78684235/machine-learning-in-q-kdb</link>
      <description><![CDATA[如果我在端口 5012 的 hdb 进程中存储了一个表。
我已经安装了 PyKX 并将其成功导入到终端中的 python 提示符中。
然后我连接到我的 host=‘localhost’, port=5012  并运行一个简单的查询以从 hdb 返回我的数据 q(‘{select name,price,volume,vwap from tab where date&gt;2024.01.01}’)
然后如何在 python 机器学习算法之一中使用这些数据。您如何将表数据转换为可用的 python 数据点，然后输入到您选择的模型中？您是否必须提取每列数据并保存为某种类型的变量，例如在 q 进程中运行 exec  语句？]]></description>
      <guid>https://stackoverflow.com/questions/78684235/machine-learning-in-q-kdb</guid>
      <pubDate>Fri, 28 Jun 2024 19:47:48 GMT</pubDate>
    </item>
    <item>
      <title>在深度学习中对同一数据集中的两个分类数据进行编码</title>
      <link>https://stackoverflow.com/questions/76149232/encoding-two-categorial-data-present-in-same-dataset-in-deep-learning</link>
      <description><![CDATA[我有一个数据集，其中包含列 reason 和 issue。
我想将其编码为：
enc = OneHotEncoder()
reason_no_enc = enc.fit_transform(temp[&#39;REASON NO&#39;].values.reshape(-1, 1)).toarray()
issue_enc = enc.fit_transform(temp[&#39;Issue&#39;].values.reshape(-1, 1)).toarray()

但我意识到它正在产生问题，后者 issue_enc 被认为是编码的，当我尝试反转 reason_no_enc 时，它会生成错误。
如何处理？]]></description>
      <guid>https://stackoverflow.com/questions/76149232/encoding-two-categorial-data-present-in-same-dataset-in-deep-learning</guid>
      <pubDate>Mon, 01 May 2023 18:22:08 GMT</pubDate>
    </item>
    <item>
      <title>Detectron2 在 Docker 容器中使用 layoutparser 预训练模型错误：未找到检查点</title>
      <link>https://stackoverflow.com/questions/76098441/detectron2-pre-trained-model-using-layoutparser-in-docker-container-error-check</link>
      <description><![CDATA[以下是我的 Dockerfile。
来自 python:3.9
运行 apt-get clean &amp;&amp; apt-get update
pip install --upgrade pip

运行 pip install layoutparser 

运行 pip install &quot;layoutparser[ocr]&quot; 

运行 pip install pytesseract 

运行 pip install pdf2image 

运行 pip install torch 

运行 pip install torchvision

运行 apt-get install -y poppler-utils #(pdf-image) 

运行 apt-get install -y tesseract-ocr 

运行 apt-get install git #(安装 detectron2) 

运行 pip install &quot;git+https://github.com/facebookresearch/detectron2.git&quot; #(detectron2 模型) 

运行 apt-get update &amp;&amp; apt-get install ffmpeg libsm6 libxext6 -y #(运行软件包所需)

workdir /home/jovyan/work/layout_parser

volume [&quot;/home/jovyan/work/layout_parser&quot;]

CMD [&quot;python&quot;, &quot;test_code.py&quot;]

python 代码 test_code.py:
import pdf2image
import layoutparser as lp
import pytesseract
import numpy as np
import cv2
import matplotlib.pyplot as plt

pdf_file= r&quot;/home/jovyan/work/layout_parser/test_pdf.pdf&quot;
image = np.asarray(pdf2image.convert_from_path(pdf_file)[0])

model = lp.Detectron2LayoutModel(&#39;lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config&#39;)


我收到以下错误：

我尝试了以下方法解决问题，但均未成功：

使用的 torch 和 torchvision 版本的变化
使用的 python 3.7、3.8、3.9 基础版本
预建模型的 config_path 的变化使用
手动下载配置文件中的模型 -&gt; layout_parser_modelzoo
尝试在扩展 pth 和 pkl 中手动下载模型。收到以下错误：


如何使用预先训练的模型？]]></description>
      <guid>https://stackoverflow.com/questions/76098441/detectron2-pre-trained-model-using-layoutparser-in-docker-container-error-check</guid>
      <pubDate>Tue, 25 Apr 2023 07:07:40 GMT</pubDate>
    </item>
    <item>
      <title>python中两个大数据样本的二次拟合函数进行预测</title>
      <link>https://stackoverflow.com/questions/74160767/quadratic-fit-function-of-two-large-data-samples-in-python-to-predict</link>
      <description><![CDATA[使用此 csv：
https://docs.google.com/spreadsheets/d/1QbFIUE1AcFCOgDZH5K2vPColxXeBVDV-sJhS9On2BYY/edit?usp=sharing
我正在尝试弄清楚如何编写程序来拟合二次函数，以预测全球温度的变化
（y）作为年份（x）的函数。
所讨论的函数是：
函数
该程序应创建一个图表，显示训练示例和数据的二次拟合。
这是我迄今为止为弄清楚基础知识所做的尝试。只需转换一下即可：
import numpy as np
from numpy.linalg import inv
import matplotlib.pyplot as plt

#生成200个训练样本

m = 200
x = np.random.randn(m)
y = np.random.randn(1) * x ** 2 + np.random.randn(1) * x + 
np.random.randn(1)
y = y + 0.4 * np.random.randn(m)

#二次拟合

X = np.transpose([np.ones(m), x, x ** 2])
print(np.shape(X))
print(np.shape(y))

theta = inv(np.transpose(X) @X) @ np.transpose(X) @ y

plt.plot(x, y, &#39;bo&#39;)

xp = np.arange(-5, 5, 0.1)
yp = theta[0] + theta[1] * xp + theta[2] * xp ** 2

plt.plot(xp, yp, &#39;r-&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/74160767/quadratic-fit-function-of-two-large-data-samples-in-python-to-predict</guid>
      <pubDate>Sat, 22 Oct 2022 02:53:21 GMT</pubDate>
    </item>
    </channel>
</rss>