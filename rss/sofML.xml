<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Tue, 04 Mar 2025 06:27:19 GMT</lastBuildDate>
    <item>
      <title>如何在Docker容器中使用C ++运行此TFLITE模型？</title>
      <link>https://stackoverflow.com/questions/79482801/how-to-get-this-tflite-model-running-with-c-inside-a-docker-container</link>
      <description><![CDATA[背景
我培训并导出了一个TensorFlow模型到 tflite 格式的嵌入式部署。该模型分别采用两个大小x 120 x 3（张量尺寸）和批处理x 2（）的输入，以生成1*1输出。这是Python中的模型调用。
  batch_size：int = 1
context_window：int = 120
num_dyn_features：int = 3
num_static_features：int = 2
dyn_inputs = tf.random.normal（shape =（batch_size，context_window，num_dyn_features））
static_inputs = tf.random.normal（shape =（batch_size，num_static_features）））
模型（输入= [dyn_inputs，static_inputs]） 
 
我将模型导出到Tflite，并编写了以下C ++代码以随机输入张量调用。
  #include＆lt; iostream＆gt;
#include＆lt; vector＆gt;
#include＆lt;随机＆gt;
//编译器说，以下标头文件不可用。 
#include&#39;tensorflow/lite/intermeter.h＆quot 
#include&#39;tensorflow/lite/model.h&#39;＆quot。
#include&#39;tensorflow/lite/bernels/register.h&#39;＆quot。
#include&#39;tensorflow/lite/optional_debug_tools.h＆quot

int main（）{
    //加载模型
    const std :: string model_path {＆quot; transformer.tflite＆quot;};
    自动模型= Tflite :: TF :: LoadModel（model_path）;
    如果（！型号）{
        std :: cerr＆lt;＆lt; “无法加载模型。” ＆lt;＆lt; std :: endl;
        返回exit_failure;
    }

    //创建解释器
    tflite :: ops :: hindin :: hindinopresolver架；
    std :: simel_ptr＆lt; tflite :: internemer＆gt;解释器；
    if（tflite :: interneterbuilder（*模型，分辨率）（＆amp; interner）！= ktfliteok）{
        std :: cerr＆lt;＆lt; “未能创建解释器。” ＆lt;＆lt; std :: endl;
        返回exit_failure;
    }

    //分配张量
    如果（解释器 - ＆gt; salocateTensors（）！= ktfliteok）{
        std :: cerr＆lt;＆lt; “未能分配张量。” ＆lt;＆lt; std :: endl;
        返回exit_failure;
    }

    //准备随机输入
    float Input1 [1 * 120 * 3];
    float Input2 [1 * 2];

    STD :: MT19937 GEN（42）; //随机号码生成器
    std :: sumiform_real_distribution＆lt; float＆gt; dis（0.0，1.0）;

    for（int i = 0; i＆lt; 1 * 120 * 3; ++ i）{
        input1 [i] = dis（gen）; //填写输入1随机数据
    }
    for（int i = 0; i＆lt; 1 * 2; ++ i）{
        input2 [i] = dis（gen）; //填充输入2随机数据
    }

    //设置输入张量
    float* input_tensor1 = interner--＆gt; typed_input_tensor＆lt; float＆gt;（0）;
    float* input_tensor2 = interner--＆gt; typed_input_tensor＆lt; float＆gt;（1）;
    std :: copy（input1，input1 +（1 * 120 * 3），input_tensor1）;
    std :: copy（input2，input2 +（1 * 2），input_tensor2）;

    //运行推断
    如果（解释器 - ＆gt; Invoke（）！= ktfliteok）{
        std :: cerr＆lt;＆lt; “未能调用”。 ＆lt;＆lt; std :: endl;
        返回exit_failure;
    }

    //获取输出
    float* output = interner--＆gt; typed_output_tensor＆lt; float＆gt;（0）;
    std :: cout＆lt;＆lt; “输出：” ＆lt;＆lt;输出[0]＆lt;＆lt; std :: endl; //打印输出的第一个要素

    返回exit_success;
}
 
，但我无法让G ++编译器实际运行它，并且无法设置必要的路径和依赖项。因此，两个问题

我的C ++代码是正确调用模型吗？
我需要哪种环境设置来编译C ++代码并运行它。无需使用GPU。

最好的话，如果您可以以dockerfile的形式提供环境，我可以复制上面的model.cpp和transormer.tflite运行。]]></description>
      <guid>https://stackoverflow.com/questions/79482801/how-to-get-this-tflite-model-running-with-c-inside-a-docker-container</guid>
      <pubDate>Tue, 04 Mar 2025 06:20:48 GMT</pubDate>
    </item>
    <item>
      <title>我们可以使用任何好的数据集来衡量个性化学习代理的性能 - 接受主题材料的培训吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79482687/any-good-datasets-we-can-use-to-measure-the-performance-of-a-personalized-learni</link>
      <description><![CDATA[我计划在开源LLM上培训的个性化学习代理，可以在特定领域（例如医学考试准备或财务教育）上定制课程结构，解释，内容形式等方面表现出色。如何衡量该代理的表现？有哪些好数据集可以使用？]]></description>
      <guid>https://stackoverflow.com/questions/79482687/any-good-datasets-we-can-use-to-measure-the-performance-of-a-personalized-learni</guid>
      <pubDate>Tue, 04 Mar 2025 04:45:00 GMT</pubDate>
    </item>
    <item>
      <title>如何在不使用大型变压器模型的情况下改善Python聊天机器人的上下文保留？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79482631/how-to-improve-context-retention-in-a-python-chatbot-without-using-large-transfo</link>
      <description><![CDATA[我正在构建一个python聊天机器人，需要在多个用户交互中记住上下文。但是，我面临的问题是，聊天机器人在响应时无法保留以前的用户输入。由于资源限制，我无法使用GPT-4或BERT等大型变压器模型，因此我需要轻量级的替代方案。
 当前方法＆amp;问题
我已经尝试了几种方法，但是似乎没有任何方法可以有效地提供短期记忆：
 令牌化＆amp;基本NLP处理（NLTK，Spacy） 
问题：用于单词细分工作，但不能保留含义或对话历史记录。
tf-idf＆amp;字袋（弓）
问题：将每个消息视为独立，因此它不会链接过去和现在的查询。
单词嵌入（Word2Vec，Glove）
问题：有意义地编码单词，但不能很好地处理多转交谈。
对话流的有限状态机（FSM）
问题：过于僵化 - 用户期望对话流的灵活性。
我想实现的目标
有效地存储和检索对话历史。
使聊天机器人能够了解过去消息的引用。
保持解决方案轻巧且有效地进行本地执行（无云依赖性）。
到目前为止我尝试的是（使用代码示例）
我目前正在尝试LSTMS以获取序列内存，但是聊天机器人仍然忘记了对话的早期部分。这是我实施的简化版本：
 来自keras.models导入顺序
来自keras.layers导入LSTM，密集，嵌入
导入numpy作为NP

＃示例聊天机器人培训数据
x_train = np.Array（[[[1，2，3]，[4，5，6]]）＃示例tokenized句子
y_train = np.Array（[[[3，2，1]，[6，5，4]]）＃预期响应

＃LSTM模型
型号=顺序（）
ADD（嵌入（input_dim = 10，output_dim = 8，input_length = 3）））
model.ADD（LSTM（50，return_sequences = false））
型号add（密集（3，激活=; softmax; quot;））

model.compile（优化器=; adam＆quot; loss =; cancorical_crossentropy＆quort; quot; 
指标= [＆quciency;]）
model.fit（x_train，y_train，epochs = 10）

＃问题：模型忘记了几次交互后的早期用户输入。
 
 问题
在不使用大型变压器模型的情况下，如何改善聊天机器人中的短期上下文保留？
知识图或矢量数据库（例如Faiss或Chromadb）是否是一个不错的选择？
商业聊天机器人（例如RASA或DialogFlow）如何在没有大型AI模型的情况下处理多转交谈？
有没有轻巧的基于注意的模型可以帮助维护上下文？
💡注意：我在这里发现了一些有关实现简单的Python聊天机器人的有用见解： https://emitechlogic.com/how-to-build-a-simple-python-chatbot/  
任何建议或替代方法都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79482631/how-to-improve-context-retention-in-a-python-chatbot-without-using-large-transfo</guid>
      <pubDate>Tue, 04 Mar 2025 03:48:58 GMT</pubDate>
    </item>
    <item>
      <title>高测试准确性值，但预测不良[闭合]</title>
      <link>https://stackoverflow.com/questions/79482112/high-test-accuracy-value-but-bad-at-prediction</link>
      <description><![CDATA[ 将TensorFlow导入为TF
来自tensorflow.keras.preprocessing.image导入成像的Atagenerator
来自Tensorflow.keras.applications导入Mobilenet

batch_size = 8
num_epochs = 20
num_classes = 4
train_dir =&#39;/users/borakarakus/桌面/数据集/培训&#39;
test_dir =&#39;/users/borakarakus/desktop/dataset/testing&#39;

train_datagen = imagedatagenerator（）
test_datagen = imagedatagenerator（）

train_data = train_datagen.flow_from_directory（
    train_dir，
    target_size =（300，300），
    batch_size = batch_size，
    class_mode =&#39;分类&#39;，
    洗牌= false
）

test_data = test_datagen.flow_from_directory（
    test_dir，
    target_size =（300，300），
    batch_size = batch_size，
    class_mode =&#39;分类&#39;，
    洗牌= false
）

＃创建Mobilenet模型
model = mobilenet（input_shape =（300，300，3），
                  include_top = false，
                  weights =&#39;/users/borakarakus/.keras/models/mobilenet_1_0_224_tf_no_top.h5&#39;）

＃冻结层
对于模型中的图层。层：
    layer.trainable = false

＃添加新的顶层
x = model.Output
x = tf.keras.layers.globalaveratepooling2d（）（x）
x = tf.keras.layers.dense（128，activation =&#39;relu&#39;）（x）
x = tf.keras.layers.dropout（0.2）（x）
预测= tf.keras.layers.dense（num_classes，activation =&#39;softmax&#39;）（x）

模型= tf.keras.models.model（inputs = model.input，outputs =预测）

＃编译模型
model.compile（优化器=&#39;adam&#39;，
              损失=&#39;apcorical_crossentropy&#39;，
              指标= [&#39;准确性&#39;]）

＃火车模型
历史= model.fit（train_data，
                    时代= 20）

＃评估模型
分数= model.Evaluate（test_data）

＃打印精度
打印（&#39;测试准确性：&#39;，得分[1]）

model.save（&#39;SA_HUS_MOBILENETMODEL_F1.H5&#39;）
 
我的代码为我提供了0.90的测试准确性值，但其他指标很糟糕，请帮助
 测试精度：0.909229576587677

分类报告：
               精确召回F1得分支持

           0 0.24 0.27 0.25 300
           1 0.25 0.19 0.22 306
           2 0.34 0.36 0.35 405
           3 0.22 0.24 0.23 300

    准确性0.27 1311
   宏公平0.26 0.26 0.26 1311
加权公平0.27 0.27 0.27 1311

精度：0.2707856598016781
精度（加权）：0.27067005240444436``
召回（加权）：0.2707856598016781
F1分数（加权）：0.269302534531395
 ]]></description>
      <guid>https://stackoverflow.com/questions/79482112/high-test-accuracy-value-but-bad-at-prediction</guid>
      <pubDate>Mon, 03 Mar 2025 20:47:43 GMT</pubDate>
    </item>
    <item>
      <title>当批次尺寸= 1且批次尺寸=训练批量尺寸[关闭]时，不同的测试结果不同</title>
      <link>https://stackoverflow.com/questions/79481464/different-test-results-when-batch-size-1-and-batch-size-training-batch-size</link>
      <description><![CDATA[我已经训练了一个UNET，该UNET在所有下样本和Uplample层中都使用BatchNorm2D。训练后，我加载了最好的检查站，并在测试数据上运行，并注意到在测试期间，如果将批次大小设置为1，则性能会严重降低。但是，如果我将其设置为与训练批量大小相同的大小（64），则性能更好。
为了进一步检查，我只需将1s的张量传递给我的型号，一次批次大小为1，另一个时间为64，两种情况下的输出都不同。这里发生了一些奇怪的事情，我不知道什么。

我有track_running_stats =在训练期间true 
我在这里尝试了解决方案： https://discuss.pytorch.org/t/performance-highly-degraded-when-when-when-eval-is-activated-in-the-test-phase/3323/67?page=3  

我没有运气尝试使用各种批次尺寸在经过训练的型号上获得相同的结果。对于上下文，两种情况之间的误差很大。]]></description>
      <guid>https://stackoverflow.com/questions/79481464/different-test-results-when-batch-size-1-and-batch-size-training-batch-size</guid>
      <pubDate>Mon, 03 Mar 2025 15:58:04 GMT</pubDate>
    </item>
    <item>
      <title>自动编码器学习问题 - 验证比培训更好吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79480958/auto-encoder-learning-issue-validation-better-than-training</link>
      <description><![CDATA[我正在开发一种自动编码器，该自动编码器将基于树的表达式编码为潜在向量并将其解码回原始形式。
我的数据集由像这样格式化的基因编程树组成：
  prog2（whileLoop（swapad），prog2（clri，inci））
prog2（prog2（prog2（clri，movbmaxiter）），而勒洛普（clri）），而勒洛普（nileLoop（swapad）））
而LileLoop（INCI）
 
我对我的自动编码器的学习结果有疑问，并想验证我的解释是否正确或我的培训是否有问题。
查看所附屏幕截图，我们可以看到两个图：

一个代表培训和验证集的损失曲线。
另一个显示了训练和验证集的精度曲线。

 问题： 
验证损失低于训练损失，这似乎是不寻常的。我的假设是：

由于我在模型的体系结构中使用辍学，这可以解释为什么验证似乎表现更好。在训练过程中，辍学会停用某些神经元，引入噪声和略有降解的性能。但是，在验证期间，辍学是禁用的，使模型的预测更加稳定，并给人以更高精度的印象。

 问题： 
这种解释是合理的，还是在我的培训过程中表明问题？
 训练/varrion/varion/val损失和准确性  
为了诊断问题，我尝试了以下内容：

降低了辍学率，以查看它是否影响了验证性能。
测试了不同的激活功能和批准化以稳定训练。
]]></description>
      <guid>https://stackoverflow.com/questions/79480958/auto-encoder-learning-issue-validation-better-than-training</guid>
      <pubDate>Mon, 03 Mar 2025 12:20:05 GMT</pubDate>
    </item>
    <item>
      <title>使用此条带明智的OCR（或其他分类模型）技术，是否可以使用草书写作角色分割？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79480579/is-cursive-writing-character-segmentation-even-possible-using-this-strip-wise-oc</link>
      <description><![CDATA[我需要从用户手写文本示例中提取字符实例。实际上，我试图创建一个数据库，说明用户如何从提交的文本样本中写出特定的字母。这是我对草书角色分割问题的尝试。
我在问题上的方法：

我摄取图像的小条，这些图像从图像的左端开始，并逐渐增加，直到整个图像被覆盖 
我在每个条带上运行OCR模型。
我应该能够看到“信心”中的尖峰。每当字符完全位于窗口内时模型。
因此，我应该具有新字符开始的位置的像素值。

这个想法在理论上似乎是可能的。
我发现A 类似的方法 之前在Stack Exchange中介绍了。
我运行了我的应该通过更大的改进的OCR模型或实际上专门针对数字识别训练的另一个模型来实质上改善。。
我认为它也可以在手写字符上工作，因为Trocr不像Tesseract不同，也应该用于手写。
当我运行它时
诸如Quic（Quick“ Quick”）之类的单词正在快速阅读。这意味着Trocr的语义引擎不会轻易通过中途结束的单词来说服。
如下所示，总体趋势表明句子中置信度得分的趋势增加。
 

可以通过分割每个单词，然后运行TROCR来解决第二个问题。这会失败。事实证明，特罗克（Trocr）在识别没有上下文的单词方面很糟糕。当我发送一片“狗”一片时，它开始返回“ diores”之类的单词。
我也对单个角色进行了trocr，它也失败了。
我的问题：

使用CNN或基于学习的角色分类模型可以更好地工作吗？如果是，我在哪里可以找到一个细分草书的数据集？
无论如何，是否有继续使用trocr，以至于它不在乎单词含义？
这种方法能否希望超越最近的分割技术，例如 this ？？
]]></description>
      <guid>https://stackoverflow.com/questions/79480579/is-cursive-writing-character-segmentation-even-possible-using-this-strip-wise-oc</guid>
      <pubDate>Mon, 03 Mar 2025 09:08:56 GMT</pubDate>
    </item>
    <item>
      <title>如何在机器学习模型中使用目标寻求功能[关闭]</title>
      <link>https://stackoverflow.com/questions/79480354/how-to-use-goal-seek-functionality-in-machine-learning-model</link>
      <description><![CDATA[我正在使用XGBoost算法进行销售预测，并具有一些输入功能。我的预测月是1月25日，2月25日，3月25日。我想检查如果我有固定的目标值1月，2月，3月，输入值的任何两个特定列输入值更改。我正在获得输出，但输出为所有3个月的值相同。以下是我的代码。
  target_values = {
    &#39;2025-01-01&#39;：12000，
    &#39;2025-02-01&#39;：15000，
    &#39;2025-03-01&#39;：16000
}

base_data = pd.dataframe（index = pd.to_dateTime（[[&#39;2025-01-01&#39;，&#39;2025-02-01&#39;，&#39;2025-03-01&#39;]））））））））））
base_data [&#39;ds&#39;] = base_data.index

base_data [&#39;montry&#39;] = base_data [&#39;ds&#39;]。dt.month
base_data [&#39;day_of_year&#39;] = base_data [&#39;ds&#39;]。dt.dayofyear
base_data [&#39;sin_month&#39;] = np.sin（2 * np.pi * base_data [&#39;montry&#39;] / 12）
base_data [&#39;cos_month&#39;] = np.cos（2 * np.pi * base_data [&#39;montry&#39;] / 12）

如果test_data.index.duplicated（）。任何（）：
    打印（警告：test_data具有重复的索引值。重置索引。”
    test_data = test_data.reset_index（drop = true）

last_row = test_data.iloc [-1] .copy（）

static_cols = [&#39;a&#39;，&#39;b&#39;，&#39;c&#39;，&#39;d&#39;，&#39;e&#39;， 
               &#39;f&#39;，&#39;g&#39;，&#39;h&#39;]


对于static_cols中的col：
    base_data [col] = last_row [col] 

base_data [&#39;sb&#39;] = last_row [&#39;sb&#39;]
base_data [&#39;leads&#39;] = last_row [&#39;leads&#39;]

def predition_y（inputs，base_row，scaleer，型号）：
    行= base_row.copy（）
    行[&#39;sb&#39;] =输入[0]
    行[&#39;leds&#39;] =输入[1]
    x =行[[&#39;sb&#39;，&#39;a&#39;，&#39;b&#39;，&#39;sin_month&#39;，&#39;cos_month&#39;，&#39;c&#39;，&#39;d&#39;，&#39;e&#39;，e&#39;， 
             &#39;f&#39;，&#39;g&#39;，&#39;h&#39;， 
             &#39;leads&#39;]]。values.reshape（1，-1）
    X_SCALED = Scaleer.Transform（x）
    预测= model.predict（x_scaled）[0]
    print（for {base_row.name}的输入：sb = {inputs [0]：。2f}，leds = {inputs [1]：。2f}，预测y = {prediction y = {.2f}＆quort;）
    返回预测

def Objective_function（输入，目标，base_row，scaer，Model）：
    predicted_y = predive_y（输入，base_row，safleer，模型）
    print（{base_row.name}的目标：{target}，预测y：{prediction_y：.2f}，差异：{（prediction_y -target）** 2：.2f}
    返回（预测_y-目标）** 2

结果= {}
对于日期，target_values.items（）中的target：
    base_row = base_data.loc [date]
    onitire_guess = [base_row [&#39;sb&#39;]，base_row [&#39;leads&#39;]]
    
    打印（用目标{target};）
    print（f＆quot;初始猜测：sb = {initial_guess [0]：。2f}，leds = {initial_guess [1] :. 2f}＆quort;）
    
    结果=最小化（
        objective_function，
        initial_guess，
        args =（target，base_row，sualer，model13），
        方法=&#39;Powell&#39;， 
        bounds = [（0，无），（0，none）]，
        选项= {&#39;disp&#39;：true} 
    ）
    
    如果结果。
        结果[date] = {&#39;sb&#39;：result.x [0]，&#39;leads&#39;：result.x [1]}
    别的：
        结果[date] = {&#39;错误&#39;：&#39;优化失败&#39;}

对于日期，results.items（）中的vals：
    打印（f＆quot; \ nfor {date}：＆quot;）
    如果瓦尔中的“错误”：
        打印（val [&#39;错误&#39;]）
    别的：
        print（f＆quot; sb = {vals [&#39;sb&#39;]：。2f}，leds = {vals [&#39;leads&#39;] :. 2f}＆quot;）

print（f＆quot {date}}：＆quot;）
print（f＆quot;初始猜测：sb = {initial_guess [0]：。2f}，leds = {initial_guess [1] :. 2f}＆quort;）
打印（f＆quot;成功：{result.success}，最终值：sb = {result.x [0]：。2f}，leds = {result.x [1]：。2f}＆quort;）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79480354/how-to-use-goal-seek-functionality-in-machine-learning-model</guid>
      <pubDate>Mon, 03 Mar 2025 07:02:20 GMT</pubDate>
    </item>
    <item>
      <title>如何在保留OCR的手写数字/字符的同时删除虚线边界框</title>
      <link>https://stackoverflow.com/questions/79479925/how-to-remove-dotted-boundary-boxes-while-preserving-handwritten-digits-characte</link>
      <description><![CDATA[我正在使用包含手写数字和字母&#39;x&#39;的图像（表示应该将其视为“空”的图像），其中每个字符都写在一个虚线框中。我正在尝试删除角色周围的那些虚线盒，以供以后的OCR。另一个问题是，这些图像的质量不同，并且这些虚线经常合并为坚固的线条。我尝试仅隔离字符，这些字符经常与这些盒子重叠，但无处不在。
以下是IM与以下图像的示例：
        有点成功的管道是（使用Python和OpenCV）：

灰度
 otsu 
 rode＆amp;扩张
理想情况下，只有4个字符

但是，在某些情况下，笔迹非常微弱，在侵蚀期间也被侵蚀。我还尝试通过检测水平和垂直线来扩展图像和绘图盒，然后将其删除，但是由于很多角色要么与盒子重叠或超越它，因此这种方法也产生了可疑的结果。。
我不确定如何进行或如何处理；有任何建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79479925/how-to-remove-dotted-boundary-boxes-while-preserving-handwritten-digits-characte</guid>
      <pubDate>Sun, 02 Mar 2025 23:44:25 GMT</pubDate>
    </item>
    <item>
      <title>Autokeras需要在Python中重写Keras代码</title>
      <link>https://stackoverflow.com/questions/79479570/autokeras-need-to-rewrite-keras-code-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79479570/autokeras-need-to-rewrite-keras-code-in-python</guid>
      <pubDate>Sun, 02 Mar 2025 18:57:13 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用变压器训练RTDETRV2时的运行时错误</title>
      <link>https://stackoverflow.com/questions/79474519/runtime-error-while-trying-to-train-rtdetrv2-with-transformer</link>
      <description><![CDATA[我正在尝试训练RTDETRV2在水表数字上检测。 I use an ipynb file form here &lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb#scrollTo=a7kZr8OtGkIO&quot; rel=&quot;nofollow noreferrer“&gt; https://colab.research.google.com/github/roboflow-ai/notebookss/blob/main/main/notebooks/notebooks/train-rain-rain-rain-rain-custom-custom-dataset-dataset-with-with-with-with-with-with-transformers.ipynb#scrollto = a7kzr8otgkio 
  model = automodelforobjectDetection.from_pretrated（
检查点，
ID2LABEL = ID2LABEL，
label2id = label2ID，
anchor_image_size = none，
ignore_mismatched_sizes = true，）
 
此警告出现
  runtimeError：rtdetrv2forobjectDetection的加载state_dict中的错误（s）：
模型的尺寸不匹配。denoising_class_embed.Wate.Wewert：用Shape Torch.Size（[81，256]）从检查点复制一个参数，当前模型中的形状为TORCH.SIZE.SIZE（[12，256]）。
模型的尺寸不匹配。ENC_SCORE_HEAD。
模型的尺寸不匹配。ENC_SCORE_HEAD.BIAS：用Shape Torch.Size（[80]）从检查点复制一个参数，当前模型中的形状为Torch.Size（[11]）。
模型的尺寸不匹配。decoder.class_embed.0.0.Weight：用Shape Torch.Size（[80，256]）从检查点复制一个参数，当前模型中的形状为Torch.Size。Size（[11，256]）。
模型的尺寸不匹配。decoder.class_embed.0.bias：用Shape torch.size（[80]）从检查点复制一个参数，当前模型中的形状为torch.size.size（[11]）。
模型的尺寸不匹配。decoder.class_embed.1.1.关键：用shape torch.size（[80，256]）从检查点复制一个参数，当前模型中的形状为torch.size。size（[11，256]）。
模型的尺寸不匹配。decoder.class_embed.1.bias：用shape torch.size（[80]）从检查点复制一个参数，当前模型中的形状为torch.size.size（[11]）。
模型的尺寸不匹配。decoder.class_embed.2.2.Ceight：用Shape Torch.Size（[80，256]）从检查点复制一个参数，当前模型中的形状为Torch.Size.Size（[11，256]）。
模型的尺寸不匹配。decoder.class_embed.2.bias：用Shape torch.size（[80]）从检查点复制一个参数，当前模型中的形状为torch.size（[11]）。
型号的尺寸不匹配。decoder.class_embed.3.3。从：用shape torch.size（[80，256]）从检查点复制一个参数，当前模型中的形状为torch.size.size（[11，256]）。
模型的尺寸不匹配。decoder.class_embed.3.bias：用Shape Torch.Size（[80]）从检查点复制一个参数，当前模型中的形状为Torch.Size（[11]）。
模型的尺寸不匹配。decoder.class_embed.4.4.关键：用Shape torch.Size（[80，256]）从检查点复制一个参数，当前模型中的形状为torch.size.size（[11，256]）。
模型的尺寸不匹配。decoder.class_embed.4.bias：用shape torch.size（[80]）从检查点复制一个参数，当前模型中的形状为torch.size.size（[11]）。
模型的尺寸不匹配。decoder.class_embed.5.5.关键：用shape torch.size（[80，256]）从检查点复制一个参数，当前模型中的形状为torch.size.size（[11，256]）。
模型的尺寸不匹配。decoder.class_embed.5.bias：用Shape torch.size（[80]）从检查点复制一个参数，当前模型中的形状为torch.size.size（[11]）。
 
我在pycharm上运行这个ipynb。
我尝试删除ID2LABEL和LABEL2ID。它没有显示出错误，但是当我尝试仅训练一个时代时，它仍在训练。 idk如何检查它是否有效。]]></description>
      <guid>https://stackoverflow.com/questions/79474519/runtime-error-while-trying-to-train-rtdetrv2-with-transformer</guid>
      <pubDate>Fri, 28 Feb 2025 04:35:29 GMT</pubDate>
    </item>
    <item>
      <title>在序列编码中，whand_unknown = use_encoded_values做什么？</title>
      <link>https://stackoverflow.com/questions/79471646/in-ordinal-encoder-what-does-handle-unknown-use-encoded-values-do</link>
      <description><![CDATA[我已经完成了研究，但我对文档和双子座的答案不满意。  use_encoded_value 它是什么意思？我必须通过一个论点作为编码值吗？如果是这样，您可以举例说明它的用法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79471646/in-ordinal-encoder-what-does-handle-unknown-use-encoded-values-do</guid>
      <pubDate>Thu, 27 Feb 2025 05:09:19 GMT</pubDate>
    </item>
    <item>
      <title>BigQuery ML时间序列模型评估保持返回零</title>
      <link>https://stackoverflow.com/questions/79471261/bigquery-ml-time-series-model-evaluate-keeps-returning-null</link>
      <description><![CDATA[我正在使用BigQuery ML来训练ARIMA_PLUS模型，以预测CPU使用情况。该模型成功训练，但是当我运行ml。评估时，所有结果值均为null。
 模型训练查询 
 创建或替换模型`project.dataset.arima_model`
选项（
  model_type =&#39;arima_plus&#39;，
  time_series_timestamp_col =&#39;timestamp_column&#39;，
  time_series_id_col = [&#39;id_column_1&#39;，&#39;id_column_2&#39;]，
  time_series_data_col =&#39;data_column&#39;，
  forecast_limit_lower_bound = 0，
  forecast_limit_upper_bound = 100
） 作为
选择data_column，id_column_1，id_column_2，timestamp_column
来自`project.dataset.source_table`
在“ 2025-02-5”和&#39;2025-02-12&#39;之间的日期（timestamp_column）;
 
 评估查询 
 选择 * 
来自ml.evaluate（
  模型`project.dataset.arima_model`，
  （（
    选择data_column，id_column_1，id_column_2，timestamp_column
    来自`project.dataset.source_table`
    在“ 2025-02-13&#39;和&#39;2025-02-20&#39;之间的日期（timestamp_column）
  ），
  结构（
    true作为persim_gregation， 
    10作为地平线， 
    0.9作为信心_level
  ）
）；
 
 ml.Evaluate成功运行，但返回所有ID的null
查询结果 ]]></description>
      <guid>https://stackoverflow.com/questions/79471261/bigquery-ml-time-series-model-evaluate-keeps-returning-null</guid>
      <pubDate>Wed, 26 Feb 2025 23:29:23 GMT</pubDate>
    </item>
    <item>
      <title>预测促进问题：newmfinal必须为1 <newmfinal <mfinal</title>
      <link>https://stackoverflow.com/questions/78604530/predict-boosting-problem-newmfinal-must-be-1newmfinalmfinal</link>
      <description><![CDATA[我正在尝试使用 precadion.boosting（）  adabag  package使用adaboost算法进行预测，但是发生错误：

&#39;precade.boosting中的错误（adaboost，train01_new，newmfinal = 9）：
newmfinal必须为1＆lt; newmfinal＆lt; mfinal; 

这是脚本：
  install.packages（“ adabag; quot”）
图书馆（“ Adabag”）
adaboost＆lt;  -  boosting.cv（factor_new〜rfs+li+sdi+sdi+ldi+dr+dbt+dbt+dbt+fct+fii+ditp+adcg+adcg+addg+roa+roa+roi+roi+roe+roe，data = triar = triar = train = triar = train = triar = trie，boos = true，mfinal = 10，mfinal = 10，v = 5，par = 5，par = true = rp = rp = rp = rp = rp。
preditive_adaboost_cv_train＆lt;  -  predict.boosting（adaboost，train01_new，newmfinal = 9）
 
我使用newmfinal = 9，因为错误建议，但错误仍在这里。]]></description>
      <guid>https://stackoverflow.com/questions/78604530/predict-boosting-problem-newmfinal-must-be-1newmfinalmfinal</guid>
      <pubDate>Mon, 10 Jun 2024 21:09:02 GMT</pubDate>
    </item>
    <item>
      <title>使用开发设备或火车组</title>
      <link>https://stackoverflow.com/questions/45909024/using-the-dev-set-or-the-train-set</link>
      <description><![CDATA[有人可以清除我的疑问。
在评估模型时，我们应该尝试一个较小的集合。开发设置是一个小集。因此，我们在开发设置上尝试一些东西，然后得出结论，然后去火车训练并检查它。 
或
我们训练训练集并在开发设置上评估模型。将开发设置为基准。]]></description>
      <guid>https://stackoverflow.com/questions/45909024/using-the-dev-set-or-the-train-set</guid>
      <pubDate>Sun, 27 Aug 2017 20:24:55 GMT</pubDate>
    </item>
    </channel>
</rss>