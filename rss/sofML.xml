<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 25 Dec 2023 21:12:01 GMT</lastBuildDate>
    <item>
      <title>根据游戏数据计算偏差和偏差锐度</title>
      <link>https://stackoverflow.com/questions/77714958/calculating-deviation-and-sharpness-of-deviation-from-game-data</link>
      <description><![CDATA[我发现这个网站可以计算足球比赛的赔率/上盘/下盘。
我想知道如何实现类似的功能。
我可以通过抓取bet365来获取赛季数据，这非常简单。
但是，我对如何计算“偏差”感到困惑。和“偏差锐度” （表中的列名称）基于任何给定比赛中的实时事件。]]></description>
      <guid>https://stackoverflow.com/questions/77714958/calculating-deviation-and-sharpness-of-deviation-from-game-data</guid>
      <pubDate>Mon, 25 Dec 2023 20:58:12 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 raytune 同时优化多个目标（最小化 MSE 和最大化 R^2）？</title>
      <link>https://stackoverflow.com/questions/77714892/how-do-i-optimize-multiple-objectives-minimizing-mse-and-maximizing-r2-simult</link>
      <description><![CDATA[我正在尝试将 MSE_test 优化为最小值，将 R^2 优化为最高，但在尝试弄清楚如何同时执行这两个操作时遇到困难。
我现在拥有的这段代码仅优化 MSE_test
def main():
    # 定义超参数搜索空间
    配置={
        “lr”：tune.loguniform(0.0001, 0.1),
        “时代”：tune.randint(70, 100)
    }

    分析=调.运行(
        调整.with_parameters（train_model，X_train = X_train_normalized，Y_train = Y_train，X_test = X_test_normalized，Y_test = Y_test），
        配置=配置，
        num_samples=15, # 根据您的资源调整此值
        metric=“mse_test”, # 针对测试集上较低的 MSE 进行优化
        模式＝“分钟”，
        Progress_reporter=CLIReporter(metric_columns=[“mse_train”、“r2_train”、“mse_test”、“r2_test”])
    ）

    best_config = Analysis.get_best_config(metric=“mse_test”, mode=“min”)
    print(&quot;最佳超参数：&quot;, best_config)

如果 __name__ == “__main__”：
    主要的（）


我这样做了，但它单独完成了这项工作，并给了我两个我正在尝试优化的不同值。
# 针对 mse_test 进行优化
mse_analysis=tune.run（
    调整.with_parameters（train_model，X_train = X_train_normalized，Y_train = Y_train，X_test = X_test_normalized，Y_test = Y_test），
    配置=配置，
    样本数=10，
    指标=“mse_test”，
    模式＝“分钟”，
    Progress_reporter=CLIReporter(metric_columns=[“mse_train”、“r2_train”、“mse_test”、“r2_test”])
）

# 针对 r2_test 进行优化
r2_analysis=tune.run（
    调整.with_parameters（train_model，X_train = X_train_normalized，Y_train = Y_train，X_test = X_test_normalized，Y_test = Y_test），
    配置=配置，
    样本数=10，
    度量=“r2_test”，
    模式=“最大”，
    Progress_reporter=CLIReporter(metric_columns=[“mse_train”、“r2_train”、“mse_test”、“r2_test”])
）

# 检索最佳配置
best_config_mse = mse_analysis.get_best_config(metric=“mse_test”, mode=“min”)
best_config_r2 = r2_analysis.get_best_config(metric=“r2_test”, mode=“max”)

print(“mse_test 的最佳超参数：”, best_config_mse)
print(“r2_test 的最佳超参数：”, best_config_r2)

]]></description>
      <guid>https://stackoverflow.com/questions/77714892/how-do-i-optimize-multiple-objectives-minimizing-mse-and-maximizing-r2-simult</guid>
      <pubDate>Mon, 25 Dec 2023 20:21:58 GMT</pubDate>
    </item>
    <item>
      <title>用于机器学习的 python REST API aiohttp 加载时间错误</title>
      <link>https://stackoverflow.com/questions/77714772/python-rest-api-for-machine-learning-aiohttp-bad-load-times</link>
      <description><![CDATA[我在 python 中使用 aiohttp。我有一个机器学习项目，我想通过restapi 提供该项目。我对这些函数进行了计时，它们执行起来并不需要很长时间。他们不会等待结果，而是检查进度并开始任务。问题是等待时间相当长。我试图找到问题所在，它不是函数本身，而是 aiohttp 来运行函数或发送结果。函数本身不是问题。（等待时间10s，函数执行2e-05）。我只是创建一个包含该项目的类。有人对如何确保快速响应时间有建议吗？有没有办法为 api 预分配资源或在单独的线程中生成机器学习项目。]]></description>
      <guid>https://stackoverflow.com/questions/77714772/python-rest-api-for-machine-learning-aiohttp-bad-load-times</guid>
      <pubDate>Mon, 25 Dec 2023 19:30:24 GMT</pubDate>
    </item>
    <item>
      <title>Tesseract 在简单的手写测试中找不到文本。有没有什么办法解决这一问题？</title>
      <link>https://stackoverflow.com/questions/77714612/tesseract-is-not-finding-text-in-simple-handwriting-test-is-there-any-way-to-fi</link>
      <description><![CDATA[我正在尝试为纸质测试的自动评分提供更好的解决方案。
问题是从测试中提取矩形区域并对手写输入进行 OCR。
虽然手写显然具有挑战性，但这个问题比一般阅读手写要简单得多：

文本方向已知
我可以准确指定我期望的答案和/或合法的字符集。
我愿意从引擎获得概率，如果概率太低，请叫人来裁决（最好不要）。

Tesseract 声称可以手写，可以使用 mingw 在 Linux 和 Windows 上运行，所以看起来不错。
我从表单中提取了手写数据样本。这是示例：

在这种情况下，矩形的边界没有被裁剪掉，但我期望它能够找到我的 64。它失败了。
当我裁剪边界框时，它起作用了。
虽然在这种情况下，我可以解决问题，但我想知道是否可以采取任何措施来提高识别率，因为边界框似乎无害，而且我担心任何微不足道的噪音都会破坏检测。

我可以使用更好的开源包吗？

有没有办法改进我的应用程序的培训？
我想我可以创造一种“语言”对于单个字母，对于整数使用不同的语言，并加载多个超正方引擎，每个引擎专门针对一种问题类型。

内部 API 是否有办法为其提供潜在字符串/字符集的列表，即暗示提高准确性？

]]></description>
      <guid>https://stackoverflow.com/questions/77714612/tesseract-is-not-finding-text-in-simple-handwriting-test-is-there-any-way-to-fi</guid>
      <pubDate>Mon, 25 Dec 2023 18:24:54 GMT</pubDate>
    </item>
    <item>
      <title>为什么支持向量机的 varImp() 出现错误？</title>
      <link>https://stackoverflow.com/questions/77714417/why-am-i-getting-an-error-with-varimp-for-support-vector-machine</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77714417/why-am-i-getting-an-error-with-varimp-for-support-vector-machine</guid>
      <pubDate>Mon, 25 Dec 2023 17:08:22 GMT</pubDate>
    </item>
    <item>
      <title>无法在 F# 中将内存数据传递到 AutoML。不明白为什么它不能编译</title>
      <link>https://stackoverflow.com/questions/77714343/cant-pass-in-memory-data-to-automl-in-f-not-understanding-why-it-doesnt-com</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77714343/cant-pass-in-memory-data-to-automl-in-f-not-understanding-why-it-doesnt-com</guid>
      <pubDate>Mon, 25 Dec 2023 16:43:24 GMT</pubDate>
    </item>
    <item>
      <title>Mediapipe-model-maker安装问题</title>
      <link>https://stackoverflow.com/questions/77713844/mediapipe-model-maker-installation-issue</link>
      <description><![CDATA[希望你们一切都好。我正在研究图像分类模型，为此，我正在安装 mediapipe-model-marker，但遇到以下错误。
这是我的 python 和 pip 版本
在此处输入图像描述
这是我用来安装此软件包的 pip 命令。
pip install mediapipe-model-maker 
这是错误
在此处输入图像描述
我尝试在我的 venv 中下载 medipipe-model-marker 包来使用图像分类模型。
我希望下载这个包。]]></description>
      <guid>https://stackoverflow.com/questions/77713844/mediapipe-model-maker-installation-issue</guid>
      <pubDate>Mon, 25 Dec 2023 13:36:29 GMT</pubDate>
    </item>
    <item>
      <title>SARIMAX 错误：ValueWarning：已提供日期索引，但它没有关联的频率信息，因此在例如预测</title>
      <link>https://stackoverflow.com/questions/77713776/sarimax-errorvaluewarninga-date-index-has-been-provided-but-it-has-no-associat</link>
      <description><![CDATA[我有这样的数据

&lt;表类=“s-表”&gt;
&lt;标题&gt;

时间
湿度_(%)


&lt;正文&gt;

15:21:02
31.03


15:23:05
30.98




当我运行代码时出现此错误：
ValueWarning：已提供日期索引，但它没有关联的频率信息，因此在例如预测。
有熟悉这方面的人可以帮助我吗？
不知道是什么原因
我写了这段代码：
导入 pandas 作为 pd
将 statsmodels.api 导入为 sm


data.set_index(&#39;时间&#39;, inplace=True)


# 将数据分为训练集和测试集
train_data = data.iloc[:-17000] # 使用除最后 8 个样本之外的所有样本进行训练
test_data = data.iloc[-17000:] # 使用最后8个样本进行测试

# 将 SARIMAX 模型拟合到训练数据
模型 = sm.tsa.SARIMAX(train_data[&#39;湿度_(%)&#39;], order=(1, 0, 0))
model_fit = model.fit()
]]></description>
      <guid>https://stackoverflow.com/questions/77713776/sarimax-errorvaluewarninga-date-index-has-been-provided-but-it-has-no-associat</guid>
      <pubDate>Mon, 25 Dec 2023 13:13:17 GMT</pubDate>
    </item>
    <item>
      <title>尝试从 inceptionv3 架构中提取特征时出现图形断开连接错误</title>
      <link>https://stackoverflow.com/questions/77713548/graph-disconnected-error-when-trying-to-extract-features-from-inceptionv3-archit</link>
      <description><![CDATA[我正在尝试从架构中间提取一些特征并将其用于另一个模型。
base_model = InceptionV3(weights=&#39;imagenet&#39;, include_top=False)
input_tensor = Input(shape=(299, 299, 3)) # InceptionV3 的输入形状
InceptionA_feature_extractor = models.Model(inputs=input_tensor,outputs=base_model.get_layer(&#39;mixed2&#39;).output)

在尝试执行此操作时，我收到以下错误，可能的原因是什么？
ValueError：图形已断开连接：无法获取张量 KerasTensor 的值（type_spec=TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name=&#39;input_6&#39;), name=&#39; input_6&#39;，描述=“由层&#39;input_6&#39;创建”）在层“conv2d_376”。访问以下先前层没有问题：[]
]]></description>
      <guid>https://stackoverflow.com/questions/77713548/graph-disconnected-error-when-trying-to-extract-features-from-inceptionv3-archit</guid>
      <pubDate>Mon, 25 Dec 2023 11:49:34 GMT</pubDate>
    </item>
    <item>
      <title>从 Adaboost 的错误中学习</title>
      <link>https://stackoverflow.com/questions/77712597/learning-from-errors-in-adaboost</link>
      <description><![CDATA[我对 adaboost 算法的工作原理有深入的了解：
1.初始化：每个样本初始分配相等的权重，使得所有样本在第一轮中同等重要。

训练弱学习器：在数据集上训练弱学习器（例如，深度有限的决策树）。弱学习器关注整个数据集，但更重视前几轮错误分类的样本。

计算误差：计算弱学习器的训练误差。该误差本质上是错误分类的加权和，其中权重是样本权重。

更新样本权重：增加误分类样本的权重，减少正确分类样本的权重。这个想法是让下一个弱学习者更多地关注之前错误分类的样本。

计算弱学习器权重：弱学习器本身的权重是根据其在训练过程中的准确性来计算的。

合并弱学习器：将弱学习器添加到集成中，并使用反映其准确性的权重。弱学习器的准确率越高，对最终预测的影响就越大。


重复：重复步骤2-6指定的轮次（迭代），将一系列弱学习器组合成一个强模型。
通过在每次迭代中为错误分类的样本赋予更高的权重，AdaBoost 确保后续的弱学习器更加关注之前难以正确分类的样本。这种自适应过程使 AdaBoost 能够专注于“困难”问题。示例，提高整体模型的性能。最终模型是所有弱学习器的加权组合，权重由它们在训练过程中的个体准确率决定。
这是我从互联网上得到的理解。但我仍然不清楚“后续的弱学习器更多地关注以前难以正确分类的样本”是什么意思。这是否意味着：

在后续的学习器中，我们用分类错误的样本填充训练数据集？
我试图了解幕后发生的事情。
]]></description>
      <guid>https://stackoverflow.com/questions/77712597/learning-from-errors-in-adaboost</guid>
      <pubDate>Mon, 25 Dec 2023 05:47:38 GMT</pubDate>
    </item>
    <item>
      <title>NLP TFBertForSequenceClassification 文本分类非常慢</title>
      <link>https://stackoverflow.com/questions/77712257/nlp-tfbertforsequenceclassification-text-classification-being-very-slow</link>
      <description><![CDATA[我一直在尝试训练TFBERtForSequenceClassification来解决文本分类问题；但是，当我尝试执行模型时，它非常慢，有时甚至会崩溃。
我的数据集分为训练、验证、测试。它也相当大。
训练样本大小：315057。
验证样本大小：39382。
测试样本大小：39383
from Transformers import TFBertForSequenceClassification, BertConfig

伯特 = TFBertForSequenceClassification.from_pretrained(
    &#39;bert-base-uncased&#39;,
    num_labels = 6 # 标签数量
）

模型=顺序（[
        输入（形状=（无，），dtype=tf.int32），
        伯特,
        密集（64，激活=&#39;relu&#39;），
        辍学率（0.3），
        密集（6，激活=&#39;softmax&#39;）
    ]）

模型.编译(
        损失=&#39;sparse_categorical_crossentropy&#39;,
        优化器=keras.optimizers.Adam(learning_rate = 0.001)
        指标=[&#39;准确性&#39;]
）

模型.summary()

h_bert = bert_model.fit(
    训练输入_tf，
    训练标签_tf，
    验证数据=（val_inputs_tf，val_labels_tf），
    纪元=2，
    批量大小=128，
    回调=[
        EarlyStopping（监视器=&#39;val_accuracy&#39;，耐心=2）
    ]
）


有什么办法可以提高性能吗？我的模型配置有问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77712257/nlp-tfbertforsequenceclassification-text-classification-being-very-slow</guid>
      <pubDate>Mon, 25 Dec 2023 01:48:31 GMT</pubDate>
    </item>
    <item>
      <title>假新闻检测数据集。 （CSV 格式）[关闭]</title>
      <link>https://stackoverflow.com/questions/77712037/dataset-for-fake-news-detections-csv-format</link>
      <description><![CDATA[我想实现一个专门针对某个地区的假新闻检测的机器学习模型，我需要一个针对该主题的大型数据集，我的主要想法是在一个网站中实现它，其中将给出 URL 链接用户可以选择输入主题的 URL 或标题。但是，我在为此寻找数据集时遇到困难。
有什么见解、建议或资源可以帮助我推进这个项目吗？]]></description>
      <guid>https://stackoverflow.com/questions/77712037/dataset-for-fake-news-detections-csv-format</guid>
      <pubDate>Sun, 24 Dec 2023 22:37:35 GMT</pubDate>
    </item>
    <item>
      <title>自定义梯度提升分类器实现。训练无进展</title>
      <link>https://stackoverflow.com/questions/77710582/custom-gradient-boosting-classifier-implementation-no-training-progress</link>
      <description><![CDATA[我正在尝试实现一个GradientBoostingClassifier
我从 StatQuest 视频中获取了该算法（梯度提升第 4 部分（共 4 部分）：分类详细信息）并尝试使用 numpy + sklearn.DecisionTreeRegressor 作为基本模型来实现它。
这是我的代码：
从 sklearn.tree 导入 DecisionTreeRegressor

定义 sigmoid(x):
  如果x&gt; 0:
    z = np.exp(-x)
    返回 1/(1+z)
  别的：
    z = np.exp(x)
    返回 z/(1+z)

类 GradientBoostingClassifier()：
  def __init__(self, n_estimators=20, lr=0.1):
    self.n_estimators = n_estimators
    self.lr = lr
    self.training_history = {
        “log_loss”：[]，“roc_auc”：[]，“pr_auc”：[]
    }
    self.base_learners = []

  def fit(自身, X, y):
    数据 = X.copy()
    特征=数据.列
    # 1. 使用常量值初始化模型：
    p = y.sum() / len(y)
    赔率 = p / (1-p)
    log_odds = np.log(赔率)
    数据[&#39;cur_log_odds&#39;] = log_odds
    数据[&#39;预测&#39;] = 数据[&#39;cur_log_odds&#39;].apply(sigmoid)

    self.training_history[&#39;log_loss&#39;].append( log_loss(y, data[&#39;prediction&#39;]) )
    self.training_history[&#39;roc_auc&#39;].append( roc_auc_score(y, data[&#39;预测&#39;]) )
    self.training_history[&#39;pr_auc&#39;].append(average_ precision_score(y, data[&#39;prediction&#39;]) )

    # 2. 对于 m = 1 到 M：
    for _ in tqdm(range(self.n_estimators)):
      # 2.1 计算所谓的伪残差：
      数据[&#39;残差&#39;] = (数据[&#39;预测&#39;] - y)

      # 2.2 拟合基学习器回归器来预测伪残差：
      base_learner = DecisionTreeRegressor（max_深度 = 3，min_samples_split = 2，random_state = 42）
      base_learner.fit(数据[特征],数据[&#39;残差&#39;])
      self.base_learners.append(base_learner)

      # 2.3 对于每个叶子计算其输出对数几率
      # 从回归树中获取叶子数
      数据[&#39;叶子&#39;] = base_learner.apply(数据[特征])

      # 将输出对数奇数计算为 sum(residuals) / sum(old_prediction * (1 - old_prediction))
      leafs_output = data.groupby(&#39;leaf&#39;, as_index=False).apply(
          lambda d: d[&#39;残差&#39;].sum() / (0.00001+(d[&#39;预测&#39;] * (1-d[&#39;预测&#39;]) ).sum())
      ).rename(columns={无: &#39;lambda_odds&#39;})

      数据 = data.merge(leafs_output, on=&#39;leaf&#39;)
      # 2.4 更新 current_log_odds = current_log_odds + lr*predicted_log_odds
      数据[&#39;cur_log_odds&#39;] += self.lr*data[&#39;lambda_odds&#39;]
      数据 = data.drop(&#39;lambda_odds&#39;, axis=1)

      数据[&#39;预测&#39;] = 数据[&#39;cur_log_odds&#39;].apply(sigmoid)

      self.training_history[&#39;log_loss&#39;].append( log_loss(y, data[&#39;prediction&#39;]) )
      self.training_history[&#39;roc_auc&#39;].append( roc_auc_score(y, data[&#39;预测&#39;]) )
      self.training_history[&#39;pr_auc&#39;].append(average_ precision_score(y, data[&#39;prediction&#39;]) )

    返回数据

  def Predict_proba(自身, X):
    经过

问题是，即使经过 1000 次迭代 (n_estimators = 1000)，我的 roc_auc 和 pr_auc 分数也接近随机模型给出的分数（roc_auc=0.5，pr_auc=0.29，这是正类的比例）。
我做错了什么？
即使在 n_estimators = 10 的情况下，sklearn GradientBoostingClassifier 实现在同一数据集上也能给出更高的分数]]></description>
      <guid>https://stackoverflow.com/questions/77710582/custom-gradient-boosting-classifier-implementation-no-training-progress</guid>
      <pubDate>Sun, 24 Dec 2023 12:11:08 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习将地址文本拆分为多个组件</title>
      <link>https://stackoverflow.com/questions/77710080/split-address-text-into-components-using-machine-learning</link>
      <description><![CDATA[我有一个 CSV 文件，每一行代表地址的不同组成部分，例如城市、街道、门牌号等，然后一列在一行中包含组合地址，具有预定义的格式，例如街道房屋号码、邮政编码、城市。
我想要的是判断用户输入的地址文本的不同组成部分，例如我想知道用户是否输入了所有组件，或者只是输入了街道名称和城市等，以及这些组件的值是什么。
我可以通过机器学习技术来实现这一目标，以便我使用 CSV 文件来教导模型，这就是地址文本被分割成不同组件的方式，然后期望它根据该训练为我提供不同的组件？
添加更多信息，因为我正在 .NET 中实现这一点，因此基于 ML.NET 的解决方案或可以轻松与 .NET 集成的解决方案将是更好的选择。
此外，我们可以不管地址解析上下文如何来看待这个问题。难道我们不应该能够教导一个模型，让其知道文本句子在任何给定上下文中是如何由不同部分组成的吗？然后期望模型建议给定的新文本句子中的各个部分？]]></description>
      <guid>https://stackoverflow.com/questions/77710080/split-address-text-into-components-using-machine-learning</guid>
      <pubDate>Sun, 24 Dec 2023 08:47:00 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络不学习</title>
      <link>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</link>
      <description><![CDATA[我正在尝试在包含 1500 张图像（15 个类别）的训练集上训练用于图像识别的卷积神经网络。有人告诉我，采用这种架构和从均值为 0、标准差为 0.01 的高斯分布得出的初始权重以及初始偏差值为 0 的情况，在适当的学习率下，它的准确度应该达到 30 左右%。
但是，它根本没有学到任何东西：准确度与随机分类器相似，并且训练后的权重仍然遵循正态分布。我做错了什么？
这是神经网络
class simpleCNN(nn.Module)：
  def __init__(自身):
    super(simpleCNN,self).__init__() #初始化模型

    self.conv1=nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,stride=1) #输出图像大小为(size+2*padding-kernel)/stride --&gt;62*62
    self.relu1=nn.ReLU()
    self.maxpool1=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像62/2--&gt;31*31

    self.conv2=nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,stride=1) #输出图像为29*29
    self.relu2=nn.ReLU()
    self.maxpool2=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像为29/2--&gt;14*14（MaxPool2d近似大小与floor）

    self.conv3=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1) #输出图像为12*12
    self.relu3=nn.ReLU()

    self.fc1=nn.Linear(32*12*12,15) #16 个通道 * 16*16 图像（64*64，步幅为 2 的 2 个 maxpooling），15 个输出特征=15 个类
    self.softmax = nn.Softmax(dim=1)

  def 前向（自身，x）：
    x=self.conv1(x)
    x=self.relu1(x)
    x=self.maxpool1(x)

    x=self.conv2(x)
    x=self.relu2(x)
    x=self.maxpool2(x)

    x=self.conv3(x)
    x=self.relu3(x)

    x=x.view(-1,32*12*12)

    x=self.fc1(x)
    x=self.softmax(x)

    返回x

初始化：
def init_weights(m):
  如果 isinstance(m,nn.Conv2d) 或 isinstance(m,nn.Linear)：
    nn.init.normal_(m.weight,0,0.01)
    nn.init.zeros_(m.bias)

模型 = simpleCNN()
模型.应用（init_weights）

训练函数：
loss_function=nn.CrossEntropyLoss()
优化器=optim.SGD(model.parameters(),lr=0.1,动量=0.9)

def train_one_epoch(epoch_index,loader):
  运行损失=0

  对于 i，枚举（加载器）中的数据：

    input,labels=data #获取小批量
    输出=模型（输入）#前向传递

    loss=loss_function(outputs,labels) #计算损失
    running_loss+=loss.item() #总结到目前为止处理的小批量的损失

    Optimizer.zero_grad() #重置梯度
    loss.backward() #计算梯度
    optimizer.step() #更新权重

  return running_loss/(i+1) # 每个小批量的平均损失


培训：
&lt;前&gt;&lt;代码&gt;纪元=20

best_validation_loss=np.inf

对于范围内的纪元（EPOCHS）：
  print(&#39;纪元{}:&#39;.format(纪元+1))

  模型.train(True)
  train_loss=train_one_epoch(epoch,train_loader)

  运行验证损失=0.0

  模型.eval()

  with torch.no_grad(): # 禁用梯度计算并减少内存消耗
    对于 i，枚举中的 vdata（validation_loader）：
      vinputs,vlabels=vdata
      v输出=模型（v输入）
      vloss=loss_function(v输出,v标签)
      running_validation_loss+=vloss.item()
  验证损失=运行验证损失/(i+1)
  print(&#39;LOSS 训练：{} 验证：{}&#39;.format(train_loss,validation_loss))

  if validation_loss
使用默认初始化，它的效果会好一点，但我应该使用高斯达到 30%。
您能发现一些可能导致它无法学习的问题吗？我已经尝试过不同的学习率和动力。]]></description>
      <guid>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</guid>
      <pubDate>Fri, 22 Dec 2023 14:06:23 GMT</pubDate>
    </item>
    </channel>
</rss>