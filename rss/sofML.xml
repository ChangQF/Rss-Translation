<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 22 Aug 2024 06:21:51 GMT</lastBuildDate>
    <item>
      <title>用于手语的 LSTM</title>
      <link>https://stackoverflow.com/questions/78899773/lstm-for-sign-language</link>
      <description><![CDATA[大家好，我是机器学习和人工智能的新手，我可以问一些问题吗？
实时模型是否可以转换为预先录制的视频上传？
例如，我将训练一个用于手语的 LSTM 模型，以实现实时识别。现在我想将其集成到移动应用程序中，这样我就可以随身携带手机，因此预先录制的视频会更好。
提前感谢您的回答。
我想学习 LSTM/RNN 中的新技术]]></description>
      <guid>https://stackoverflow.com/questions/78899773/lstm-for-sign-language</guid>
      <pubDate>Thu, 22 Aug 2024 04:16:56 GMT</pubDate>
    </item>
    <item>
      <title>适合专家阅读的机器学习书籍有哪些？[关闭]</title>
      <link>https://stackoverflow.com/questions/78899696/what-are-books-on-machine-learning-for-experts</link>
      <description><![CDATA[我是一名自学成才的程序员，我已经掌握了所有基础知识，例如 ml 模型、cnn、rnn、lstm 等

我想知道如何才能成为机器学习专家。您可以固定任何书籍、代码链接、
博客等。请告诉我机器学习中有哪些主题在计算机视觉、nlp、时间序列、文本分类等所有​​领域的流行互联网书籍中都找不到。
并告诉我一些只有高级机器学习工程师才能做的项目。
提前感谢您的回答]]></description>
      <guid>https://stackoverflow.com/questions/78899696/what-are-books-on-machine-learning-for-experts</guid>
      <pubDate>Thu, 22 Aug 2024 03:32:40 GMT</pubDate>
    </item>
    <item>
      <title>KerasTensors 上的二元交叉熵不起作用</title>
      <link>https://stackoverflow.com/questions/78898962/binary-crossentropy-on-kerastensors-not-working</link>
      <description><![CDATA[我正尝试在 tf/keras 中实现此 VAE，但 binary_crossentropy 似乎出了点问题。
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Lambda, Input, Dense
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.models import Model

...

def build_vae():
...

rebuild_loss = binary_crossentropy(inputs, output) * image_size
kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
kl_loss = K.sum(kl_loss, axis=-1)
kl_loss *= -0.5
vae_loss = K.mean(reconstruction_loss + kl_loss)

我在 binary_crossentropy 行上收到错误。确认 inputs 和 outputs 属于 KerasTensor 类型后，回溯状态为：

ValueError：KerasTensor 不能用作 TensorFlow 函数的输入。KerasTensor 是形状和 dtype 的符号占位符，用于构建 Keras Functional 模型或 Keras Functions。您只能将其用作 Keras 层或 Keras 操作的输入（来自命名空间 keras.layers 和 keras.operations）。

如何修复此问题？此外，我很好奇为什么 tensorflow.keras.losses.binary_crossentropy 不接受 KerasTensors，或者为什么它不被视为 Keras 函数，即使它在 keras 库中。
我尝试将 binary_crossentropy 包装在 keras Layer 子类中，该行有效，但随后所有 K 函数都会出错。如果我必须对所有使用的 K 函数重复该过程，我会感到惊讶]]></description>
      <guid>https://stackoverflow.com/questions/78898962/binary-crossentropy-on-kerastensors-not-working</guid>
      <pubDate>Wed, 21 Aug 2024 20:53:11 GMT</pubDate>
    </item>
    <item>
      <title>即使训练=False，Tensorflow 模型仍可进行训练</title>
      <link>https://stackoverflow.com/questions/78898892/tensorflow-model-still-trains-even-with-training-false</link>
      <description><![CDATA[以下是简单的重现代码
import tensorflow as tf
import tensorflow.keras as keras

inp = keras.Input((3, 3))
layer = keras.layers.Dense(1)
tar = layer(inp)

tar2 = layer(inp, training=False)
model2 = keras.Model(inp, tar2)
model2.compile(loss=&#39;mse&#39;, optimizer=keras.optimizers.Adam(0.01))

# fit
a = tf.random.normal((1, 3, 3))
b = tf.random.normal((1, 3, 1))

model2.fit(a, b)

如果您在之前/之后检查 model2.trainable_variables训练过程中，您可以轻松检查 model2 的参数是否已更改，这意味着它已完成训练。
我该怎么做才能在训练期间不在特定时间更新特定层？
我需要这样做的原因是，我的方案是重用我之前制作的一些层，并且我不希望在仍然更新模型中的其他层时再次训练这些层。如下所示：
inp1 = keras.Input((2, 3))
inp2 = keras.Input((4, 3))

layer1 = keras.layers.Dense(1)
intermediate_output1 = layer1(inp1)
intermediate_output2 = layer1(inp2, training=False) # 我不想为 inp2 再次训练该层。

已添加 = tf.concat([intermediate_output1, middle_output2], axis=1)
layer2 = keras.layers.Dense(2)
final_output = layer2(已添加)
final_output.shape # [无, 6, 4]
]]></description>
      <guid>https://stackoverflow.com/questions/78898892/tensorflow-model-still-trains-even-with-training-false</guid>
      <pubDate>Wed, 21 Aug 2024 20:28:32 GMT</pubDate>
    </item>
    <item>
      <title>尝试加载 hydra 的配置时出现问题</title>
      <link>https://stackoverflow.com/questions/78896800/problem-when-trying-to-load-the-config-of-hydra</link>
      <description><![CDATA[我正在 google collab 中运行 python 脚本。当我执行此操作时，我收到配置错误，但我不确定如何修复它
代码片段：
import hydra
from pathlib import Path # 导入文件路径处理的路径
import sys # 导入 sys 模块

@hydra.main(config_path=&quot;cfgs&quot;, config_name=&quot;config.yaml&quot;)
def main(cfg):
print(cfg) # 打印解析的配置
from train import Workspace as W
root_dir = Path.cwd()

working = W(cfg)

snap = root_dir / &#39;snapshot.pt&#39;

if snap.exists():
print(f&#39;resuming: {snapshot}&#39;)
working.load_snapshot()

working.train()

if __name__ == &#39;__main__&#39;:
main() 

输出我得到的是：
usage: colab_kernel_launcher.py [--help] [--hydra-help] [-- 
version] [--cfg {job,hydra,all}]
[--resolve] [--package PACKAGE] [-- 
run] [--multirun]
[--shell-completion] [--config-path 
CONFIG_PATH]
[--config-name CONFIG_NAME] [--config- 
dir CONFIG_DIR]
[--info 
[{all,config,defaults,defaults-tree,plugins,searchpath}]]
[overrides ...]
colab_kernel_launcher.py：错误：无法识别的参数：-f
发生异常，使用 %tb 查看完整回溯。
SystemExit：2
]]></description>
      <guid>https://stackoverflow.com/questions/78896800/problem-when-trying-to-load-the-config-of-hydra</guid>
      <pubDate>Wed, 21 Aug 2024 11:54:28 GMT</pubDate>
    </item>
    <item>
      <title>回归模型一遍又一遍地得到相同的输出</title>
      <link>https://stackoverflow.com/questions/78894555/getting-the-same-output-over-and-over-again-for-regression-model</link>
      <description><![CDATA[我的回归模型与自动驾驶汽车有关，数据由 5 个传感器的幅度和一个转弯角度组成。
DataParser.Parse(out double[][] input, out double[] output,&quot;data.csv&quot;, (0, 4), (4, 5));
var model = new FanChenLinSupportVectorRegression&lt;Gaussian&gt;();
var svm = model.Learn(input, output);

Console.WriteLine(svm.Score(new double[] { 95.0, 0.0, 67.8, 0.0, 0.0 }));

我决定进行实验，看看 FanChenLin 回归算法是否有效。
现在，我面临的问题是，对于输出，我得到的是 217.88 的常数值。我已经生成了一个模型训练数据，它可能有误，但我希望它在输入不同时给出不同的输出，但在这里，尽管改变了输出，我仍然得到了 217.88 的值。
我编写的数据解析器似乎得到了正确的输入和输出。
我搞不清楚哪里出了问题。
我尝试更改复杂度等值，但尽管它们给出了不同的输出，但该输出在不同情况下仍然是恒定的输入。
37.454011884736246,3.142918568673425,64.20316461542878,5.16817211686077,10.31238688359326,21.365509618766623
95.07143064099162,63.641041 12637804,8.413996499504883,53.1354631568148,90.25529066795667,61.22639465322385 73.1993941811405,31.435598107632668,16.162 871409461378,54.06351216101065,50.5252 37244785714,0.16392599623276283 59.86584841970366,50.85706911647028,89.85541885270793,63.742990149820656,82.64574661077417 ,338.3506582186952 15.601864044243651 ,90.7566473926093,60.642905965958995,72.60913337226616,32.00496010306117,1.6476821446117356 15.599452033620265,24.92922291 4887493,0.9197051616629648,97.58520794 625346,89.55232284962005,270.35942257808824 5.8083612168199465,41.038292303562976,10.147154286603211,51.630034830119534,38 .92016787341631,283.0714170955975 86. 61761457749351,75.55511385430486,66.35017691080559,32.2956472941246,1.083765148029836,62.743931568541655 60.11150117432088 ,22.879816549162246,0.5061583846218687 ,79.51861947687037,90.53819764192636,270.3330873043567 70.80725777960456,7.697990982879299,16.080805141749867,27.083225126 207424,9.128667678613356,69.182165231 95599 2.0584494295802447,28.9751452913768,54.87337893665861,43.89714207056361,31.93136375904149,320.77412786375123 96.9909 8521619943,16.122128725400444,69.18951 976926932,7.845638134226595,95.0061967050805,8.435865969658153 83.24426408004217,92.96976523425731,65.19612595026005,2.535 074341545751,95.06071469375561,50.331 93776194861 21.233911067827616,80.8120379564417,22.42693094605598,96.26484146779251,57.34378881232861,293.50639083221773 1 8.182496720710063,63.34037565104234,71 .2179221347536,83.59801205122058,63.183721216979926,317.50014925812815 18.34045098534338,87.14605901877177,23.724908749680 008,69.59742060936979,44.844552197831 98,339.3199373713304 30.42422429595377,80.36720768991145,32.539969815926774,40.89529444142699,29.321077169806454,51.271442 802352 52.475643163223786,18.657005888 603585,74.64914051180241,17.329432007084577,32.8664545369916,15.667156378272637 43.194501864211574,89.25589984899777,64.96 328990472146,15.643704267108605,67.25 184560770384,37.33686449574435 29.122914019804192,53.93422419156507,84.9223410494178,25.024289816459532,75.237452943768,34 8.5473288270272 61.18528947223795,80.7 4401551640625,65.76128923003434,54.92266647061205,79.15790437258485,6.806163770302646 13.949386065204184,89.60912999234932 ,56.830860333547164,71.45959227000624, 78.96181427945538,320.4909103298581
29.214464853521815,31.800347497186387,9.367476782809248,66.01973767177313,9.120610304869036,303.55070797479505
]]></description>
      <guid>https://stackoverflow.com/questions/78894555/getting-the-same-output-over-and-over-again-for-regression-model</guid>
      <pubDate>Tue, 20 Aug 2024 22:18:47 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow load_model()'charmap'编解码器无法对位置 18-37 的字符进行编码：字符映射到 <undefined> 错误 [重复]</title>
      <link>https://stackoverflow.com/questions/78894108/tensorflow-load-model-charmap-codec-cant-encode-characters-in-position-18-3</link>
      <description><![CDATA[我正在尝试使用 django 连接一个 ml 模型。在这里我已加载模型和必要的编码器。在这里我已使用 tensorflow 加载模型。但是当尝试预测输出时，它会抛出此错误
import joblib
import os
#from keras.model import load_model
from keras.src.saving.saving_api import load_model
from django.conf import settings
import numpy as np

def load_keras_model():
# 定义模型文件的路径
model_path = os.path.join(settings.BASE_DIR, &#39;Ml_Models&#39;, &#39;football_prediction_model.h5&#39;)
print(&quot;Keras model path:&quot;, model_path)

try:
# 加载模型
model1 = load_model(model_path)
# 通过打印其摘要来验证模型加载
print(&quot;模型已成功加载。&quot;)
print(&quot;模型摘要：&quot;)
model1.summary()
return model1

except Exception as e:
# 处理异常并打印错误消息
print(f&quot;加载模型时出错：{str(e)}&quot;)
return None

def load_encoder(filename):
coder_path = os.path.join(settings.BASE_DIR, &#39;Ml_Models&#39;, filename)
print(f&quot;{filename} path:&quot;,coder_path) # 调试路径
return joblib.load(encoder_path)

# 加载所有必要的模型和编码器
model = load_keras_model()
team_label_encoder = load_encoder(&#39;team_label_encoder.pkl&#39;)
outcome_label_encoder = load_encoder(&#39;outcome_label_encoder.pkl&#39;)
scaler = load_encoder(&#39;scaler.pkl&#39;)

def predict_outcome(home_team,away_team,year,month,day,temperature):
try:
print(f&quot;主队： {home_team}&quot;)
print(f&quot;客队：{away_team}&quot;)
print(f&quot;年份：{year}, 月份：{month}, 日：{day}, 温度：{temperature}&quot;)
# 对输入数据进行编码和缩放
home_team_encoded = team_label_encoder.transform([home_team])[0]
away_team_encoded = team_label_encoder.transform([away_team])[0]
temperature_scaled = scaler.transform([[temperature]])[0][0]

print(f&quot;编码的主队：{home_team_encoded}&quot;)
print(f&quot;编码的客队：{away_team_encoded}&quot;)
print(f&quot;缩放的温度：{temperature_scaled}&quot;)

# 为模型准备输入
input_data = np.array([[home_team_encoded, away_team_encoded, year, month, day,temperature_scaled]])
print(f&quot;输入日期：{input_data}&quot;)
input_data = input_data.reshape((1, 1, 6))
print(f&quot;输入更新日期：{input_data}&quot;)

# 进行预测
prediction = model.predict(input_data)
print(f&quot;预测：{prediction}&quot;)
consequence_index = np.argmax(prediction)
print(f&quot;结果索引：{outcome_index}&quot;)

# 将预测映射回原始结果标签
consequence_label = consequence_label_encoder.inverse_transform([outcome_index])
print(f&quot;输出标签：{outcome_label}&quot;)

return consequence_label[0]

except ValueError as e:
return f&quot;Error: {str(e)}&quot;

home_team = &#39;Scotland&#39;
away_team = &#39;England&#39;
year = 2024
month = 8
day = 20
temperature = 25

predicted_outcome = predict_outcome(home_team, away_team, year, month, day,temperature)
print(f&quot;Predicted Outcome: {predicted_outcome}&quot;)

对于上述代码，以下是输出。请注意，我在控制台中包含了部分输出。
主队：苏格兰
客队：英格兰
年份：2024，月份：8，日期：20，温度：25
D:\My Projects\FootBall-Match-Win-Prediction\BackEnd\venv\Lib\site-packages\sklearn\base.py:465：UserWarning：X 没有有效的特征名称，但 MinMaxScaler 配备了特征名称
warnings.warn(
编码的主队：3
编码的客队：1
缩放温度：0.75
输入日期：[[3.000e+00 1.000e+00 2.024e+03 8.000e+00 2.000e+01 7.500e-01]]
输入更新日期：[[[3.000e+00 1.000e+00 2.024e+03 8.000e+00 2.000e+01 7.500e-01]]]

预测结果：错误：“charmap”编解码器无法对位置 18-37 的字符进行编码：
字符映射到 &lt;undefined&gt;

系统检查未发现任何问题（0 静音）。
2024 年 8 月 21 日 - 00:30:52
Django 版本 5.1，使用设置“BackEnd.settings”
在 http://localhost:8000/ 启动开发服务器
使用 CTRL-BREAK 退出服务器。

对于它打印的 predicted_outcome 变量
错误：“charmap”编解码器无法对位置的字符进行编码18-37：
字符映射到 &lt;undefined&gt;。

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78894108/tensorflow-load-model-charmap-codec-cant-encode-characters-in-position-18-3</guid>
      <pubDate>Tue, 20 Aug 2024 19:16:51 GMT</pubDate>
    </item>
    <item>
      <title>利用分割模型对直肠内超声图像中的肿瘤分期进行分类</title>
      <link>https://stackoverflow.com/questions/78893937/leveraging-segmentation-model-for-tumor-stage-classification-in-endorectal-ultra</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78893937/leveraging-segmentation-model-for-tumor-stage-classification-in-endorectal-ultra</guid>
      <pubDate>Tue, 20 Aug 2024 18:28:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 sklearn python 获取预测</title>
      <link>https://stackoverflow.com/questions/78890391/how-use-sklearn-python-get-predicion</link>
      <description><![CDATA[我有一张表，我想传递 features = &quot;train_1, train_2, train_3, train_4&quot; 和 target_result = result_cor。
我想知道什么时候值是 = &quot;1 或 2&quot;在我的预测中：
关注我的数据
关注我的代码：
从 enum 导入 auto
从 sklearn.svm 导入 LinearSVC
从 sklearn.model_selection 导入 train_test_split
从 sklearn.metrics 导入 accuracy_score
从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.metrics 导入 classes_report
从 sklearn 导入 svm
从 sklearn.linear_model 导入 LogisticRegression
导入 pandas 作为 pd
导入 numpy 作为 np
导入 matplotlib.pyplot 作为 plt
导入 math
导入 seaborn 作为 sns

sheet_id = &#39;1CfnVwuqysTYNPKLVhgjJ44Af8VDcdN1l&#39; dados = pd.read_excel(f&#39;https://docs.google.com/spreadsheets/export?id={sheet_id}&amp;format=xlsx&#39;) bads.head() # 实现更多数量的数据 x = bados[[&#39;train_1&#39;,&#39;train _2&#39;,&#39;train_3&#39;,&#39;train_4&#39;]] # Gabarito 或 corretos y = bados[[&#39;result_cor&#39;]] # 将 x e y e testes de x e y 分开 treino_x, teste_x, treino_y, teste_y = train_test_split(x,y,test_size=0.33) # 模型类型 modelo = DecisionTreeClassifier() # 训练效果 modelo.fit(x,np.ravel(y,order=&quot;c&quot;)) # 预测新值 model_predict = [0,1,0,1] treino_x[:1] = model_predict model_predict = treino_x[:1] result_cor = [1] treino_y[:1] = result_cor result_cor = treino_y[:1] # 预测新模型 previsoes = modelo.predict(model_predict) # 检查准确率 precision = precision_score(result_cor,previsoes) * 100 print(f&#39;A acuracia é: {round(accuracy,2)}&#39;)


但结果始终为 100.0 % 或 0.0。我需要知道我的 result_cor 出现在模型训练模型的 model_predict 中的次数百分比
请帮忙]]></description>
      <guid>https://stackoverflow.com/questions/78890391/how-use-sklearn-python-get-predicion</guid>
      <pubDate>Tue, 20 Aug 2024 02:18:19 GMT</pubDate>
    </item>
    <item>
      <title>Val_accuracy 正在改变，有时它在补码之间交替（100％-val_acc）</title>
      <link>https://stackoverflow.com/questions/78885395/val-accuracy-inst-changing-and-sometimes-it-alternates-between-it-complement-10</link>
      <description><![CDATA[我被分配根据我读过的一篇论文来实现一个机器学习模型。
这篇论文实现了一个用于属性分类的多任务学习模型（带标签的图像是模型输入，带标签的意思是属性注释，每幅图像有 40 个）。
它是一个多任务学习模型，因为在模型输入层和 40 个属性分支之后有一个共享的密集层，每个分支都有自己的损失函数（所有分支的二元交叉熵）和自己的 S 型激活函数（在最后一层，用于预测 40 个属性中的每一个是否存在于图像中）。
经过大量艰苦的努力，它终于开始在所有分支上返回所有 S 型函数的概率，但只有 val_accuracy 的概率是错误的：val_loss 和损失（训练损失）越来越小，acc（训练准确度）也在正常的概率值范围内，除了 val_accuracy 总是相同的值或它的补码。
例如（仅举 5 个时期为例）：
40 个分支之一的一个属性预测的准确度：
5_o_Clock_Shadow_Accuracy
0 0.823665
1 0.891178
2 0.891178
3 0.891178

同一属性的损失：
 5_o_Clock_Shadow_loss
0 0.921046
1 0.701494
2 0.913597
3 0.765397
4 0.894950

val_loss：
val_5_o_Clock_Shadow_loss
0 730232.750000
1 300412.500000
2 376215.843750
3 0.747685
4 1.607191

最后是 val_Accuracy：
val_5_o_Clock_Shadow_Accuracy
0 0.882382
1 0.117618
2 0.882382
3   0.882382 4 0.882382  我的模型： def subnet(shared_layers_output, i): att_branch = Dense(512, name=&#39;dense_&#39;+str(i)+&#39;_1&#39;)(shared_layers_output) att_branch = ReLU()(att_branch) att_branch = BatchNormal ization()(att_branch) att_branch = Dropout(0.5)(att_branch) att_branch = Dense(512, name=&#39;dense_&#39;+str(i)+&#39;_2&#39;)(att_branch) att_branch = ReLU()(att_branch) att_branch = BatchNormalization()(att_branch) att_branch = Dropout(0.5)(att_branch)

branch_output = Dense(1, name=att_list[i],activation=&#39;sigmoid&#39;)(att_branch)

return branch_output

def multi_task_model():

#输入
input_layer = Input(shape=(512,), name=&#39;input_layer&#39;)

#共享网络（1 个网络）
shared_x = Dense(512, name=&#39;shared_dense_layer&#39;)(input_layer)
shared_x = ReLU()(shared_x)
shared_x = BatchNormalization()(shared_x)
shared_x = Dropout(0.5)(shared_x)

branch_outputs = list()
for i in range(40):
branch_outputs.append(subnet(shared_x, i))

model = Model(input_layer, branch_outputs, name=&#39;model&#39;)

返回模型


训练和测试输入形状：(n_samples, 512)
训练和测试标签输入形状：(40, n_samples)
学习率：1e-03

5_o_Clock_Shadow 损失、val_loss、acc 和 val_acc 超过 5 个时期
 损失 val_loss acc val_acc
0 0.422385 1.949578 0.864272 0.8873
1 0.354094 151.987991 0.888797 0.1127
2 0.354356 58.867992 0.888797 0.1127
3 0.352891 94.257980 0.888797 0.1127
4 0.353390 10.997763 0.888797 0.1127
]]></description>
      <guid>https://stackoverflow.com/questions/78885395/val-accuracy-inst-changing-and-sometimes-it-alternates-between-it-complement-10</guid>
      <pubDate>Sun, 18 Aug 2024 18:38:14 GMT</pubDate>
    </item>
    <item>
      <title>确保数据标记、数据注释的质量</title>
      <link>https://stackoverflow.com/questions/78882012/ensure-quality-of-data-labeling-data-annotation</link>
      <description><![CDATA[我有一个包含两百万数据图像、视频和文本的数据集。这些都未标记。我想雇佣来自世界各地的工人来标记它们。这是一个庞大的人数。我如何确保我的员工标记的数据的质量？我担心他们只是为了赚钱而大量浪费工作。
P/S：我不能使用 Scale&#39;AI 等其他公司为我标记。
对于简单的分类。我可以使用像 CAPCHA 这样的方法。它效果很好，但对于其他情况，如绘制边界框 = 或分割，我不知道如何检查标签数据的质量。]]></description>
      <guid>https://stackoverflow.com/questions/78882012/ensure-quality-of-data-labeling-data-annotation</guid>
      <pubDate>Sat, 17 Aug 2024 10:47:26 GMT</pubDate>
    </item>
    <item>
      <title>使用人工智能进行文本提取和文本识别</title>
      <link>https://stackoverflow.com/questions/72297600/text-extraction-and-text-recognition-with-ai</link>
      <description><![CDATA[从文本开始，我希望能够识别特定信息。
示例：
输入文本：“发票号为 18”、“发票号：75”、“发票号：84”
已识别的发票号：“18”、“75”、“84”
具体问题是我有很多包含大量此类信息的文档，我想使用算法来识别和提取各种类型的字段。
我认为理论上我会使用某种框架/算法，输入我的所有文档并通过批准或不批准结果来训练算法，但我不知道从哪里开始。
我研究了非结构化文本的深度学习、机器学习、斯坦福 NER、命名实体认可为一般概念等。
我希望得到一些关于从哪里开始实施此类解决方案的指导。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/72297600/text-extraction-and-text-recognition-with-ai</guid>
      <pubDate>Thu, 19 May 2022 01:15:54 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的 `squeeze()` 与 `unsqueeze()`</title>
      <link>https://stackoverflow.com/questions/61598771/squeeze-vs-unsqueeze-in-pytorch</link>
      <description><![CDATA[我不明白 squeeze() 和 unsqueeze() 对张量做了什么，即使查看了文档和相关问题。
我试图通过在 Python 中自己探索来理解它。我首先用创建了一个随机张量
x = torch.rand(3,2,dtype=torch.float)
&gt;&gt;&gt; x
tensor([[0.3703, 0.9588],
[0.8064, 0.9716],
[0.9585, 0.7860]])

但无论我如何挤压它，我都会得到相同的结果：
torch.equal(x.squeeze(0), x.squeeze(1))
&gt;&gt;&gt; True

如果我现在尝试解压，我会得到以下结果，
&gt;&gt;&gt; x.unsqueeze(1)
张量([[[0.3703, 0.9588]],
[[0.8064, 0.9716]],
[[0.9585, 0.7860]]])
&gt;&gt;&gt; x.unsqueeze(0)
张量([[[0.3703, 0.9588],
[0.8064, 0.9716],
[0.9585, 0.7860]]])
&gt;&gt;&gt; x.unsqueeze(-1)
tensor([[[0.3703],
[0.9588]],
[[0.8064],
[0.9716]],
[[0.9585],
[0.7860]]])

但是，如果我现在创建一个张量  x = torch.tensor([1,2,3,4])，然后我尝试解压它，那么看起来 1 和 -1 使它成为一列，而 0 保持不变。
x.unsqueeze(0)
tensor([[1, 2, 3, 4]])
&gt;&gt;&gt; x.unsqueeze(1)
tensor([[1],
[2],
[3],
[4]])
&gt;&gt;&gt; x.unsqueeze(-1)
tensor([[1],
[2],
[3],
[4]])

有人能解释一下 squeeze() 和 unsqueeze() 对张量做了什么吗？提供参数 0、1 和 -1 之间有什么区别？]]></description>
      <guid>https://stackoverflow.com/questions/61598771/squeeze-vs-unsqueeze-in-pytorch</guid>
      <pubDate>Mon, 04 May 2020 18:06:38 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的 `stack()` 与 `cat()`</title>
      <link>https://stackoverflow.com/questions/54307225/stack-vs-cat-in-pytorch</link>
      <description><![CDATA[OpenAI 的强化学习 REINFORCE 和 actor-critic 示例有以下代码：
REINFORCE：
policy_loss = torch.cat(policy_loss).sum()

actor-critic：
loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()

一个使用torch.cat，另一个使用torch.stack，用于类似的用例。
据我所知，文档没有明确区分它们。
我很高兴知道这些函数之间的区别。]]></description>
      <guid>https://stackoverflow.com/questions/54307225/stack-vs-cat-in-pytorch</guid>
      <pubDate>Tue, 22 Jan 2019 11:24:47 GMT</pubDate>
    </item>
    <item>
      <title>我怎么知道训练数据足以进行机器学习</title>
      <link>https://stackoverflow.com/questions/24752941/how-can-i-know-training-data-is-enough-for-machine-learning</link>
      <description><![CDATA[例如：如果我想训练一个分类器（可能是 SVM），我需要收集多少样本？有没有测量方法？]]></description>
      <guid>https://stackoverflow.com/questions/24752941/how-can-i-know-training-data-is-enough-for-machine-learning</guid>
      <pubDate>Tue, 15 Jul 2014 08:03:32 GMT</pubDate>
    </item>
    </channel>
</rss>