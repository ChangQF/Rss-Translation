<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 29 May 2024 06:21:41 GMT</lastBuildDate>
    <item>
      <title>KITTI 数据集中的标签文件</title>
      <link>https://stackoverflow.com/questions/78547630/label-file-in-kitti-dataset</link>
      <description><![CDATA[我正在研究 3d 对象检测，并遇到了 PointPillars。PointPillars 使用 KITTI 数据集。
现在我想使用自己的数据集，但首先我应该像 KITTI 一样格式化我的数据集，因为我想知道 KITTI 数据集的标签文件中的那些值是什么。有人可以解释一下吗？]]></description>
      <guid>https://stackoverflow.com/questions/78547630/label-file-in-kitti-dataset</guid>
      <pubDate>Wed, 29 May 2024 06:15:31 GMT</pubDate>
    </item>
    <item>
      <title>如何将坐标系转换为普通笛卡尔坐标系？我正在用 Python 进行图像分析</title>
      <link>https://stackoverflow.com/questions/78547444/how-do-i-translate-the-coordinate-system-into-the-normal-cartesian-i-am-working</link>
      <description><![CDATA[因此，本质上我的问题是找到椭圆的半长轴的角度。为此，我使用了以下方法：
我首先加载图像并将其转换为灰度。然后，我使用阈值对灰度图像进行二值化（阈值以上的像素变为白色，阈值以下的像素变为黑色）。然后遍历二值化图像中的每个像素。如果像素为黑色（值 0），我将计算该像素与椭圆的质心（质心）之间的欧几里得距离。最大距离将是长轴，像素和中心的坐标将是两个坐标，我可以使用这两个坐标来获得长轴和 theta 的方程。
这是我得到的结果，这是有道理的。 
不合理的是最大点和 COM 的坐标。COM 坐标约为 500,500，而最大点坐标约为 270,1000。但最大点的坐标不应该大于 COM 坐标吗？如果我们想象一条连接两个十字的线，那么这两个十字的梯度应该是正的，但使用这些我得到的答案是负的。我觉得问题是因为 y 轴从左上角开始，x 轴从左下角开始。我该如何解决这个问题？
import cv2
import numpy as np
from scipy import ndimage
import matplotlib.pyplot as plt
import math

def euclidean_distance(a, row, col):

return ((a[1] - col) ** 2 + (a[0] - row) ** 2)

def angle(a, row, col):

slope = (a[0] - row) / (a[1] - col)
return math.atan(slope)

# 加载图像
image_path = &quot;/Users/yahya2/Desktop/Scr​​eenshot 2024-05-29 at 1.17.34AM.png&quot;
img = cv2.imread(image_path)
a = ndimage.center_of_mass(img)

# 将图像转换为灰度
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# 将灰度图像二值化
_, img_bin = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)

# 获取图像的尺寸
rows, cols = img_bin.shape
max_length = 0
max_row, max_col = 0, 0

# 循环遍历图像中的每个像素
for row in range(rows):
for col in range(cols):
# 检查像素是否为黑色 (0)
if img_bin[row, col] == 0:
length = euclidean_distance(a, row, col)
if length &gt; max_length:
max_length = length
max_row, max_col = row, col
print(a[1],a[0])
print(max_col,max_row)
# 计算角度
major_axis_angle = angle(a, max_row, max_col)

# 可视化结果
fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(img, cmap=&#39;gray&#39;)
plt.scatter(max_col, max_row, color=&#39;red&#39;, marker=&#39;x&#39;, s=100)
plt.scatter(a[1], a[0], color=&#39;red&#39;, marker=&#39;x&#39;, s=100)
plt.show()

print(major_axis_angle)
]]></description>
      <guid>https://stackoverflow.com/questions/78547444/how-do-i-translate-the-coordinate-system-into-the-normal-cartesian-i-am-working</guid>
      <pubDate>Wed, 29 May 2024 05:21:02 GMT</pubDate>
    </item>
    <item>
      <title>YoloV8 结果中没有 'box'、'max' 属性</title>
      <link>https://stackoverflow.com/questions/78547320/yolov8-results-have-no-box-max-properties-in-it</link>
      <description><![CDATA[我已经训练了一个 YOLOV8 模型来识别十字路口的物体（即汽车、道路等）。
它工作正常，我可以将输出作为图像，其中包含感兴趣的分割对象。
但是，我需要做的是捕获原始几何图形（多边形），以便稍后将它们保存在 txt 文件中。
我尝试了在文档中找到的内容（https://docs.ultralytics.com/modes/predict/#key-features-of-predict-mode）但返回的对象与文档所述不同。
例如，这是 Ultralytics 推荐的：
from ultralytics import YOLO

# 加载模型
model = YOLO(&quot;yolov8n.pt&quot;) # 预训练的 YOLOv8n 模型

#对图像列表运行批量推理
results = model([&quot;im1.jpg&quot;, &quot;im2.jpg&quot;]) # 返回 Results 对象列表

# 处理结果列表
for result in results:
boxes = result.boxes # 用于边界框输出的 Boxes 对象
mask = result.masks # 用于分割掩码输出的 Masks 对象
keypoints = result.keypoints # 用于姿势输出的关键点对象
probs = result.probs # 用于分类输出的 Probs 对象
obb = result.obb # 用于 OBB 输出的定向框对象
result.show() # 显示到屏幕
result.save(filename=&quot;result.jpg&quot;) # 保存到磁盘

但是，当我尝试时，预测的返回对象与上面的代码完全不同。没有框、掩码、关键点等。
实际上，结果是 TensorFlow 数字列表：

这是我的代码：
import argparse
import cv2
import numpy as np
from pathlib import Path
from ultralytics.yolo.engine.model import YOLO

# 解析命令行参数
parser = argparse.ArgumentParser()
parser.add_argument(&#39;--source&#39;, type=str, required=True, help=&#39;源图像目录或file&#39;)
parser.add_argument(&#39;--output&#39;, type=str, default=&#39;output&#39;, help=&#39;Output directory&#39;)
args = parser.parse_args()

# 如果不存在，则创建输出目录
Path(args.output).mkdir(parents=True, exist_ok=True)

# 模型路径
model_path = r&#39;C:\\_Projects\\best_100img.pt&#39;

# 直接加载模型
model = YOLO(model_path)
model.fuse()

# 加载图像
if Path(args.source).is_dir():
image_paths = list(Path(args.source).rglob(&#39;*.tiff&#39;))
else:
image_paths = [args.source]

# 处理每个图像
for image_path in image_paths:
img = cv2.imread(str(image_path))
if img is None:
continue

# 执行推理
predictions = model.predict(image_path, save=True, save_txt=True)

print(&quot;处理完成。&quot;)

我想我的问题是：

为什么结果与文档如此不同？
是否有转换步骤？
]]></description>
      <guid>https://stackoverflow.com/questions/78547320/yolov8-results-have-no-box-max-properties-in-it</guid>
      <pubDate>Wed, 29 May 2024 04:32:01 GMT</pubDate>
    </item>
    <item>
      <title>机器学习项目[关闭]</title>
      <link>https://stackoverflow.com/questions/78547175/machine-learning-project</link>
      <description><![CDATA[我有一个项目要创建一个机器学习模型，其中有一个印刷电路板 (PCB) 被分成 16 个部分的图像，我们以正确的印刷电路板组件为基础，然后在相机的帮助下放大印刷电路板 (PCB) 组件以检查它是否与正确的印刷电路板 (PCB) 组件匹配。
我完全不知道该如何处理这个问题，因为我对图像处理的机器学习知识很少，所以请有人帮助我。]]></description>
      <guid>https://stackoverflow.com/questions/78547175/machine-learning-project</guid>
      <pubDate>Wed, 29 May 2024 03:18:48 GMT</pubDate>
    </item>
    <item>
      <title>当使用大规模数据进行训练时，数据是如何处理？</title>
      <link>https://stackoverflow.com/questions/78547001/when-training-with-large-scale-data-how-is-the-data-processed</link>
      <description><![CDATA[我面临着使用大规模数据进行训练的挑战，具体来说是大约 19 TB 的视频数据。创建模型并不困难，但我不知道在哪里存储如此大量的数据以及如何使用它。由于我们没有高性能计算机，似乎我们可能需要租用一些。我很好奇处理大规模数据的 AI 开发人员通常如何处理这种情况。
此外，我发现可以使用 AWS，但我想知道是否真的采用了这种方法，或者是否有更好的替代方案。]]></description>
      <guid>https://stackoverflow.com/questions/78547001/when-training-with-large-scale-data-how-is-the-data-processed</guid>
      <pubDate>Wed, 29 May 2024 01:50:16 GMT</pubDate>
    </item>
    <item>
      <title>为什么加载 AutoTokenizer 会占用这么多的 RAM？</title>
      <link>https://stackoverflow.com/questions/78546693/why-loading-autotokenizer-takes-so-much-ram</link>
      <description><![CDATA[我测量了脚本使用的 RAM，惊讶地发现它占用了大约 300Mb 的 RAM，而 tokenizer 文件本身大约只有 9MB。这是为什么？
我试过：
from transformers import AutoTokenizer
from memory_profiler import profile

@profile
def load_tokenizer():
path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
tokenizer = AutoTokenizer.from_pretrained(path)

return tokenizer

load_tokenizer()

输出：
行 # 内存使用量 增量 发生次数 行内容
==================================================================
4 377.4 MiB 377.4 MiB 1 @profile
5 def load_tokenizer():
6 377.4 MiB 0.0 MiB 1 path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
7 676.6 MiB 299.2 MiB 1 tokenizer = AutoTokenizer.from_pretrained(path)
8 
9 
10 676.6 MiB 0.0 MiB 1 返回 tokenizer
]]></description>
      <guid>https://stackoverflow.com/questions/78546693/why-loading-autotokenizer-takes-so-much-ram</guid>
      <pubDate>Tue, 28 May 2024 22:44:10 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么机器学习方法来找到最佳卷积？</title>
      <link>https://stackoverflow.com/questions/78546274/what-machine-learning-approach-should-i-use-to-find-optimal-convolutions</link>
      <description><![CDATA[因此，我想寻找最适合某个物理实验的熵编码波形的最佳卷积运算。您可以想象，搜索空间非常大，我需要一种机器学习方法来找到一个好的最小值。我有大量数据和一台超级计算机可供使用。
我曾考虑使用与 AlphaTensor 类似的方法（有没有开源替代方案？），但我对 ML 算法不太熟悉，因此任何建议都将不胜感激。
我搜索过非 ML 方法，但认为其中一种方法效果最好。]]></description>
      <guid>https://stackoverflow.com/questions/78546274/what-machine-learning-approach-should-i-use-to-find-optimal-convolutions</guid>
      <pubDate>Tue, 28 May 2024 20:16:15 GMT</pubDate>
    </item>
    <item>
      <title>使用并行神经网络实现动态权重分配，提升 CNN 性能</title>
      <link>https://stackoverflow.com/questions/78545298/improving-cnn-performance-with-a-parallel-neural-network-for-dynamic-weight-assi</link>
      <description><![CDATA[我试图通过为所有数据点分配 0-1 之间的权重来改进我的回归模型，即卷积神经网络，这反过来又告诉我特定数据点在进行预测时有多好。关键是有些数据点很嘈杂并且会做出更糟糕的预测，我希望这些数据点具有较低的权重，以便在最后和训练期间忽略它们。
我所做的：尝试实现一个并行神经网络 (nn)，输出 0 到 1 之间的权重，并将此权重分配给相关的数据点/图像。
两个网络都接收相同的数据点作为输入，nn 输出权重，而回归 cnn 输出值 Y_i。然后将结果连接起来并传递给两个网络的相互自定义损失函数，该函数将预测值和目标值之间的平方差与并行 nn 中的给定权重相乘：sum((Y(:,i)-T(:,i))^2)*W(i)。
问题是模型似乎向较小的权重 W(i) 收敛，因为这将最大限度地减少损失，我该如何解决这个问题？
我尝试添加与权重大小成比例的正则化项来惩罚较小的权重。它有所帮助，但结果仍然不如没有并行 nn 时那么好。
# functional api
inputs = Input(shape=(16,150,1) )

x2 = Conv2D(64, 3, 1,activation=&#39;relu&#39;,data_format=&quot;channels_last&quot;,name=&#39;Conv1&#39;)(inputs)

x2 = MaxPooling2D()(x2)
x2 = BatchNormalization()(x2)

x2 = Conv2D(74, 2,activation=&#39;relu&#39;)(x2)
x2 = MaxPooling2D()(x2)
x2 = BatchNormalization()(x2)

x2 = Conv2D(128, 2,activation=&#39;relu&#39;)(x2)
x2 = Flatten()(x2)

x2 = Dense(256,activation=&#39;relu&#39;, name=&quot;FC1&quot;)(x2)
x2 = Dense(256,activation=&#39;relu&#39;,name=&#39;FC2&#39;)(x2)
regression_output = Dense(1,name=&#39;Output&#39;)(x2)

# 权重 NN 
x1 = Flatten()(inputs) # 与上面的 CNN 相同的输入
x1 = Dense(64,activation=&#39;relu&#39;,kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)
x1 = Dense(64,activation=&#39;relu&#39;,kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)
x1 = Dense(64,activation=&#39;relu&#39;)(x1)

weight_output = Dense(1,activation=&#39;sigmoid&#39;,name=&#39;weight_output&#39;)(x1) # 权重作为输出
weight_output = tf.maximum(weight_output, 0.1) # 确保权重至少为 0.1
combined_output = Concatenate()([regression_output, weight_output])
model = Model(inputs=inputs, output=combined_output)

def custom_loss(y_true, y_pred):
regression = y_pred[:, 0]
weight = y_pred[:, 1]
# 与之前一样对权重进行 L2 惩罚
#weight_penalty = tf.reduce_mean(tf.square(weight))
#l1_penalty = tf.reduce_sum(tf.abs(weight)) # L1 惩罚
#lambda_l1 = 0.10 # L1 的正则化强度
return tf.reduce_mean(weight * tf.square(y_true - return))
model.compile(optimizer=&#39;adam&#39;, loss=custom_loss, metrics = [&#39;mae&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/78545298/improving-cnn-performance-with-a-parallel-neural-network-for-dynamic-weight-assi</guid>
      <pubDate>Tue, 28 May 2024 16:06:38 GMT</pubDate>
    </item>
    <item>
      <title>将 Spark 中的大数据导入到 Feast Feature Store</title>
      <link>https://stackoverflow.com/questions/78544969/ingesting-big-data-from-spark-into-feast-feature-store</link>
      <description><![CDATA[我目前正在为 MLOps 项目构建大数据管道，该管道用于批处理。
这是当前设置：

我将原始结构化数据存储在 Hive 中。
Spark 作业提取原始数据并对其进行处理。
我打算使用 feast 和 Apache Cassandra 作为离线存储，用于存储由我的 Spark 作业产生的计算和策划特征。

我想高效地将数据从 spark 作业传递到 feast 和 Cassandra，我不确定在将处理后的数据传递给 feast 以存储在离线存储中之前，是否需要中间数据持久性解决方案来保存处理后的数据，在我的情况下有必要吗？]]></description>
      <guid>https://stackoverflow.com/questions/78544969/ingesting-big-data-from-spark-into-feast-feature-store</guid>
      <pubDate>Tue, 28 May 2024 15:00:31 GMT</pubDate>
    </item>
    <item>
      <title>提高人脸识别性能并扩大检测范围 [关闭]</title>
      <link>https://stackoverflow.com/questions/78543297/improving-face-recognition-performance-and-extending-detection-range</link>
      <description><![CDATA[我正在使用 Python、dlib 和 OpenCV 开发人脸识别系统，但是在检测到多张人脸时会遇到性能问题，并且我需要扩大检测范围以检测 3 米以外距离的人脸。以下是我当前设置和挑战的摘要：
当前设置：

使用 Logitech Brio 4K Ultra HD 网络摄像头捕获视频流。
使用 dlib 的正面人脸检测器进行人脸检测。
利用 Dlib ResNet 模型进行人脸识别。
使用 OpenCV 处理视频流。
将已知的人脸特征存储在 CSV 文件中以供比较。

挑战：

当帧中有多个人脸时，性能会显著下降，
导致丢帧和 FPS 降低。
需要从 3 米以上的距离检测人脸，同时
保持准确性。

是否有任何硬件加速技术或优化我应该考虑以更快的速度计算？]]></description>
      <guid>https://stackoverflow.com/questions/78543297/improving-face-recognition-performance-and-extending-detection-range</guid>
      <pubDate>Tue, 28 May 2024 09:51:28 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 保存模型有效，但加载模型无效</title>
      <link>https://stackoverflow.com/questions/78535919/tensorflow-saving-model-works-but-loading-it-doesnt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78535919/tensorflow-saving-model-works-but-loading-it-doesnt</guid>
      <pubDate>Sun, 26 May 2024 16:58:03 GMT</pubDate>
    </item>
    <item>
      <title>基于 Python 的模型学习，使用 TF、Keras 和 NLTK 进行标记</title>
      <link>https://stackoverflow.com/questions/78531788/python-based-model-learning-through-intents-using-tf-keras-and-nltk-for-tokeniz</link>
      <description><![CDATA[我已经使用 tensorflow、keras 和 nltk 在 Python 中开发了一个聊天机器人模型，用于标记化。当我在 vs 终端中运行它时，它会显示时间戳和模型提供答案所需的时间，但我试图在使用 React 设计的网站中显示它。如何从输出中删除日志。我尝试了所有方法，包括隐藏日志（除非它们至关重要），但我仍然无法删除它们。
我试过用这个，但没有用，它仍然显示它们。我知道日志不是警告，所以它们可能不会被删除。
import os os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78531788/python-based-model-learning-through-intents-using-tf-keras-and-nltk-for-tokeniz</guid>
      <pubDate>Sat, 25 May 2024 08:01:27 GMT</pubDate>
    </item>
    <item>
      <title>在 PyCharm 虚拟环境中训练后模型未保存</title>
      <link>https://stackoverflow.com/questions/78531747/model-not-saving-after-training-in-pycharm-virtual-environment</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78531747/model-not-saving-after-training-in-pycharm-virtual-environment</guid>
      <pubDate>Sat, 25 May 2024 07:44:18 GMT</pubDate>
    </item>
    <item>
      <title>librosa、MFCC 中的 TypeError</title>
      <link>https://stackoverflow.com/questions/75775979/typeerror-in-librosa-mfcc</link>
      <description><![CDATA[我有以下代码，它获取一个数据集（GTZAN）并将其转换为字典中的 MFCC：
DATASET_PATH = &#39;/content/drive/MyDrive/ColabNotebooksNew/PROJECT/ProjectMusic/Data/genres_original&#39;
JSON_PATH = &quot;data_10.json&quot;
SAMPLE_RATE = 22050 #每首歌曲时长 30 秒，采样率为 22,050 Hz
TRACK_DURATION = 30 # 以秒为单位
SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION #=661,500

def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):

# 用于存储映射、标签和 MFCC 的字典
data = {
&quot;mapping&quot;: [], #标签名称。size - (10,)
&quot;labels&quot;: [], #存储“真实”歌曲类型（值从 0-9）。 size - (5992,)
&quot;mfcc&quot;: [] #存储 mfccs.size - (5992, 216, 13)
}

samples_per_segment = int(SAMPLES_PER_TRACK / num_segments) #=110250
num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length) #=216(math.ceil of 215.332)
# 循环遍历所有流派子文件夹
for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):

# 确保我们正在处理流派子文件夹级别
if dirpath is not dataset_path:

# 在映射中保存流派标签（即子文件夹名称）
semantic_label = dirpath.split(&quot;/&quot;)[-1]
data[&quot;mapping&quot;].append(semantic_label)
print(&quot;\nProcessing: {}&quot;.format(semantic_label))
# 处理类型子目录中的所有音频文件
for f in filenames:

# 加载音频文件

file_path = os.path.join(dirpath, f)

if file_path != &#39;/content/drive/MyDrive/ColabNotebooksNew/PROJECT/ProjectMusic/Data/genres_original/jazz/jazz.00054.wav&#39;: 
&quot;&quot;&quot;fileError: 错误打开 &#39;/content/drive/MyDrive/ColabNotebooksNew/PROJECT/ProjectMusic/Data/genres_original/jazz/jazz.00054.wav&#39;: 文件包含未知数据格式。&quot;&quot;&quot;

signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE) #signal=音频文件中有多少样本, sample rate =音频文件的采样率, sample=22050

#处理音频文件的所有片段
for d in range(num_segments):

#计算当前片段的开始和结束样本
start = samples_per_segment * d
finish = start + samples_per_segment

#提取mfcc
mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length) #mfcc - 时间和 Coef(13 因为 num_mfcc=13), 
mfcc = mfcc.T #[216,13]
#仅存储具有预期向量数量的 mfcc 特征
if len(mfcc) == num_mfcc_vectors_per_segment: #==216
data[&quot;mfcc&quot;].append(mfcc.tolist())
data[&quot;labels&quot;].append(i-1)
print(&quot;{},segment:{}&quot;.format(file_path, d+1))
# 将 MFCC 保存到 json 文件
with open(json_path, &quot;w&quot;) as fp:
json.dump(data, fp, indent=4) # 将所有内容放入 Json 文件中

# 运行数据处理 
save_mfcc(DATASET_PATH, JSON_PATH, num_segments=6)

我已经使用这个代码很长一段时间了，它一直运行良好，直到今天我收到以下错误：
TypeError Traceback (most最近调用最后一次)
&lt;ipython-input-10-4a9371926618&gt; 在 &lt;module&gt;
1 # 运行数据处理
----&gt; 2 save_mfcc(DATASET_PATH, JSON_PATH, num_segments=6)

&lt;ipython-input-9-8ba1c6e78747&gt; 在 save_mfcc(dataset_path, json_path, num_mfcc, n_fft, hop_length, num_segments)
56 
57 # 提取 mfcc
---&gt; 58 mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length) #mfcc - 时间和 Coef（13 因为 num_mfcc=13），
59 mfcc = mfcc.T #[216,13]
60 # 仅存储具有预期向量数量的 mfcc 特征

TypeError：mfcc() 接受 0 个位置参数，但给出了 2 个位置参数（和 1 个仅关键字参数）

关于 save_mfcc 函数：
从音乐数据集中提取 MFCC 并将它们与流派标签一起保存到 json 文件中。
 :param dataset_path (str)：数据集路径
:param json_path (str)：用于保存的 json 文件的路径MFCCs
:param num_mfcc (int)：要提取的系数数量
:param n_fft (int)：我们考虑应用 FFT 的间隔。以样本数量为单位测量
:param hop_length (int)：FFT 的滑动窗口。以样本数量为单位测量
:param: num_segments (int)：我们要将样本轨迹划分成的段数
:return:

我不明白为什么今天才出现这个问题，以及如何解决它。
我该如何解决这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/75775979/typeerror-in-librosa-mfcc</guid>
      <pubDate>Sat, 18 Mar 2023 12:46:34 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 StandardScaler 正确扩展训练、验证和测试集？</title>
      <link>https://stackoverflow.com/questions/58823264/how-to-scale-train-validation-and-test-sets-properly-using-standardscaler</link>
      <description><![CDATA[有些文章说，在只有训练集和测试集的情况下，首先，我们需要使用 fit_transform() 来缩放训练集，然后只对测试集使用 transform()，以防止数据泄漏。
就我而言，我还有验证集。
我认为下面这些代码之一可以使用，但我不能完全依赖它们。任何形式的帮助都将不胜感激，谢谢！
1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 2/7)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

2)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 2/7)
X_test = scaler.transform(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/58823264/how-to-scale-train-validation-and-test-sets-properly-using-standardscaler</guid>
      <pubDate>Tue, 12 Nov 2019 16:54:52 GMT</pubDate>
    </item>
    </channel>
</rss>