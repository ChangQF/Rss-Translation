<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 29 Jun 2024 12:27:10 GMT</lastBuildDate>
    <item>
      <title>无法在 Android Studio 上使用可教机器分析机器学习模型（使用 Kotlin）</title>
      <link>https://stackoverflow.com/questions/78685944/failed-to-analyze-machine-learning-models-with-teachable-machine-on-android-stud</link>
      <description><![CDATA[我在分析照片时遇到了问题，到现在还是卡住了。我有 3 个模型，分别是癌症、非癌症和未知。当我尝试分析时，结果总是“未知” 有人知道该如何解决吗？我尝试更改代码并询问人工智能，但仍然卡住了。
您好，我在分析照片时遇到了问题，到现在还是卡住了。我有 3 个模型，分别是癌症、非癌症和未知。当我尝试分析时，结果总是“未知” 有人知道该如何解决吗？我尝试更改我的代码并询问人工智能，但仍然卡在这里是我的代码
ImageClassifierHelper

class ImageClassifierHelper(
private var Threshold: Float = 0.5f, // Tingkatkan Threshold
private var maxResult: Int = 3,
private val modelName: String = &quot;model_unquant.tflite&quot;,
val context: Context,
val classifierListener: ClassifierListener?
) {
interface ClassifierListener {
fun onError(error: String)
fun onResults(result: MutableList&lt;Category&gt;?)
}

private var imageClassifier: ModelUnquant? = null

init {
setupImageClassifier()
}

private fun setupImageClassifier() {
try {
imageClassifier = ModelUnquant.newInstance(context)
} catch (e: IOException) {
classifierListener?.onError(context.getString(R.string.failed))
Log.e(TAG, e.message.toString())
}
}

伴随对象 {
private const val TAG = &quot;ImageClassifierHelper&quot;
}

// 按照您的模型添加标签列表
private val labels = listOf(&quot;Non-Cancer&quot;, &quot;Cancer&quot;, &quot;Unknown&quot;)

fun classifyStaticImage(imageUri: Uri) {
if (imageClassifier == null) {
setupImageClassifier()
}

// 将 Uri 转换为 Bitmap
val bitmap = uriToBitmap(context.contentResolver, imageUri)
?: run {
classifierListener?.onError(&quot;Failed to decrypt image&quot;)
return
}
if (bitmap == null) {
classifierListener?.onError(&quot;Failed to decrypt image&quot;)
return
}

val resizedBitmap = Bitmap.createScaledBitmap(bitmap, 224, 224, true)

//将 Bitmap 转换为 TensorImage
val tensorImage = TensorImage(DataType.FLOAT32)
tensorImage.load(resizedBitmap)

// 为输入创建 TensorBuffer
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 224, 224, 3), DataType.FLOAT32)
inputFeature0.loadBuffer(tensorImage.buffer)

// 调用推理模型并计算结果
val output = imageClassifier?.process(inputFeature0)
val outputFeature0 = output?.outputFeature0AsTensorBuffer

// 将 Bitmap 转换为类别列表
val probability = outputFeature0?.let { tensorBuffer -&gt;
tensorBuffer.floatArray.mapIndexed { 索引，值 -&gt;
Category(labels.getOrElse(index) { &quot;Unknown&quot; }, value)
}.filter { it.score &gt;= Threshold } // 根据阈值过滤
.toMutableList()
}

classifierListener?.onResults(probability)
}

private fun uriToBitmap(contentResolver: ContentResolver, uri: Uri): Bitmap? {
return try {
val inputStream = contentResolver.openInputStream(uri)
BitmapFactory.decodeStream(inputStream)
} catch (e: IOException) {
e.printStackTrace()
null
}
}
}

]]></description>
      <guid>https://stackoverflow.com/questions/78685944/failed-to-analyze-machine-learning-models-with-teachable-machine-on-android-stud</guid>
      <pubDate>Sat, 29 Jun 2024 12:13:59 GMT</pubDate>
    </item>
    <item>
      <title>为什么两个 Spurce 代码显示出两种类型的精度？</title>
      <link>https://stackoverflow.com/questions/78685570/why-are-two-spurce-codes-showing-two-types-of-accuracies</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78685570/why-are-two-spurce-codes-showing-two-types-of-accuracies</guid>
      <pubDate>Sat, 29 Jun 2024 09:20:34 GMT</pubDate>
    </item>
    <item>
      <title>RK3588 上支持的 ML 框架和命令 NPU 执行</title>
      <link>https://stackoverflow.com/questions/78685276/supported-ml-frameworks-and-commanding-npu-execution-on-rk3588</link>
      <description><![CDATA[我正在探索在 NPU RK3588 上运行机器学习模型。有人可以分享哪些 ML 框架支持此 NPU 吗？此外，我想知道如何命令 NPU 专门运行使用这些框架的模型。任何见解或资源都将不胜感激！
我是新手，到目前为止我还没有尝试过任何事情]]></description>
      <guid>https://stackoverflow.com/questions/78685276/supported-ml-frameworks-and-commanding-npu-execution-on-rk3588</guid>
      <pubDate>Sat, 29 Jun 2024 06:59:25 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 GNN 不能完成这么简单的任务？（使用其特征及其邻居的特征来预测其标签）</title>
      <link>https://stackoverflow.com/questions/78685248/why-cant-my-gnn-work-on-such-a-simple-taskusing-its-features-and-its-neighbou</link>
      <description><![CDATA[我正在使用图神经网络 (GNN) 进行节点回归预测。目前，我正在为每个节点生成具有 10 维特征且没有边特征的随机图。每个节点的标签是通过将其自身特征相加并将其邻居特征的总和相加而创建的。因此，我认为 GNN 预测标签应该是一个简单的任务，损失应该会收敛到零。然而，实际损失在 45 左右波动。我尝试了很多方法将损失降至零，但都没有奏效。该模型在某种程度上可以起到这样的作用：
节点 0：预测：9.7522，真实：12.9197
节点 1：预测：-6.3811，真实：-9.2859
节点 2：预测：-19.5746，真实：-7.5249
节点 3：预测：-77.1716，真实：-17.7568
节点 4：预测：-2.2538，真实：-5.5253
节点 5：预测：-9.8838，真实：-4.1401
节点 6：预测：-10.5909，真实：-8.0105
节点 7：预测：-5.9015，真实：-9.8000
节点 8：预测： -20.8053，真实：-30.8081
节点 9：预测：5.1975，真实：2.2300
节点 10：预测：-14.8946，真实：-8.9688
节点 11：预测：-4.1185，真实：-4.4405
节点 12：预测：-21.0528，真实：-25.8891
节点 13：预测：-3.3378，真实：-8.9060
节点 14：预测：-6.5355，真实：-7.4367
节点 15：预测：29.2404，真实：30.6309
节点 16：预测：-37.1685，真实：-64.5965
节点17：预测：-6.4728，真实：-4.6115
节点 18：预测：8.0359，真实：9.0135
节点 19：预测：25.7627，真实：29.4366
节点 20：预测：26.4058，真实：20.6240
节点 21：预测：13.3435，真实：12.7873
节点 22：预测：-47.8510，真实：-53.5470
节点 23：预测：-12.2702，真实：-21.0920
节点 24：预测：-33.2615，真实：-29.9373
节点 25：预测：19.7694，真实值：26.7916
节点 26：预测值：3.0540，真实值：7.8801
节点 27：预测值：22.9412，真实值：23.2001
节点 28：预测值：10.8517，真实值：11.9646
节点 29：预测值：-9.5051，真实值：-14.9755

但我认为预测值应该更符合真实值。我想知道为什么我的 GNN 不能完成这么简单的任务？
我的完整代码位于：
https://github.com/zyg18/GNN/blob/main/GNN.py
我尝试过许多 GNN 模型，例如：
class SimpleGNN(torch.nn.Module):
def __init__(self, num_node_features):
super(SimpleGNN, self).__init__()
self.conv = GCNConv(num_node_features, 1, bias=False)

def forward(self, x, edge_index):
return self.conv(x, edge_index).squeeze(-1)

class GNNWithTwoConvLayers(torch.nn.Module):
def init(self, num_node_features):
super(GNNWithTwoConvLayers, self).init()
self.conv1 = GCNConv(num_node_features, 64)
self.conv2 = GCNConv(64, 64)
self.fc1 = torch.nn.Linear(64, 16)
self.fc2 = torch.nn.Linear(16, 1)
self.dropout = torch.nn.Dropout(0.5)

def forward(self, x, edge_index):
x = self.conv1(x, edge_index)
x = F.relu(x)
x = self.dropout(x)
x = self.conv2(x, edge_index)
x = F.relu(x)
x = self.dropout(x)
x = self.fc1(x)
x = F.relu(x)
x = self.fc2(x)
返回 x.squeeze(-1)

class GNN(torch.nn.Module):
def init(self, num_node_features):
super(GNNWithTwoConvLayers, self).init()
self.conv1 = GCNConv(num_node_features, 64)
self.fc1 = torch.nn.Linear(64, 16)
self.fc2 = torch.nn.Linear(16, 1)
self.dropout = torch.nn.Dropout(0.5)

def forward(self, x, edge_index):
x = self.conv1(x, edge_index)
x = F.relu(x)
x = self.dropout(x)
x = self.fc1(x)
x = F.relu(x)
x = self.fc2(x)
返回 x.squeeze(-1)
]]></description>
      <guid>https://stackoverflow.com/questions/78685248/why-cant-my-gnn-work-on-such-a-simple-taskusing-its-features-and-its-neighbou</guid>
      <pubDate>Sat, 29 Jun 2024 06:43:38 GMT</pubDate>
    </item>
    <item>
      <title>我想了解更多有关代码中用于迁移学习的技术[关闭]</title>
      <link>https://stackoverflow.com/questions/78684984/i-want-to-know-more-about-the-technique-used-for-transfer-learning-in-the-code</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78684984/i-want-to-know-more-about-the-technique-used-for-transfer-learning-in-the-code</guid>
      <pubDate>Sat, 29 Jun 2024 03:21:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 X = tester.drop('label', axis=1) 和 y = tester['label'] 拆分数据时出现 KeyError: “['label'] not found in axis”</title>
      <link>https://stackoverflow.com/questions/78684880/keyerror-label-not-found-in-axis-when-splitting-data-with-x-tester-drop</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78684880/keyerror-label-not-found-in-axis-when-splitting-data-with-x-tester-drop</guid>
      <pubDate>Sat, 29 Jun 2024 01:40:35 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归梯度下降中的不同成本函数算法与成本函数本身[关闭]</title>
      <link>https://stackoverflow.com/questions/78684727/logistic-regression-different-cost-function-algorithm-in-gradient-descent-vs-cos</link>
      <description><![CDATA[为什么在逻辑回归中成本函数看起来像这样：
在此处输入图像描述
但是，在实现梯度下降时，成本函数基本上被简化为不包含对数的线性回归成本函数：
在此处输入图像描述
我完全理解为什么在逻辑回归中对数是必要的，因为存在多个局部最小值并且可能无法达到全局最小值。那么为什么它们不存在于梯度下降算法中呢？
我原本以为梯度下降中的成本函数是相同的。]]></description>
      <guid>https://stackoverflow.com/questions/78684727/logistic-regression-different-cost-function-algorithm-in-gradient-descent-vs-cos</guid>
      <pubDate>Fri, 28 Jun 2024 23:30:17 GMT</pubDate>
    </item>
    <item>
      <title>尝试理解 GAN 不同层的输出大小 [关闭]</title>
      <link>https://stackoverflow.com/questions/78684659/trying-to-understand-output-sizes-of-different-layers-if-gan</link>
      <description><![CDATA[我正在上 Coursera 上的 GAN 课程。但我很难理解不同层的大小。有很多步幅和内核大小。我只是不明白如何计算大小。我只能添加一张图片，但这样做不行。我需要一个熟悉 GAN 的人。可以帮我吗？
我尝试了很多方法，但还是很困惑。如果不知道，我将根本无法设计我的 GAN。]]></description>
      <guid>https://stackoverflow.com/questions/78684659/trying-to-understand-output-sizes-of-different-layers-if-gan</guid>
      <pubDate>Fri, 28 Jun 2024 22:49:57 GMT</pubDate>
    </item>
    <item>
      <title>kdb+/q q/kdb+ 中的机器学习</title>
      <link>https://stackoverflow.com/questions/78684235/kdb-q-machine-learning-in-q-kdb</link>
      <description><![CDATA[如果我在端口 5012 的 hdb 进程中存储了一个表。
我已经安装了 PyKX 并将其成功导入到终端中的 python 提示符中。
然后我连接到我的 host=‘localhost’, port=5012  并运行一个简单的查询以从 hdb 返回我的数据 q(‘{select name,price,volume,vwap from tab where date&gt;2024.01.01}’)
然后如何在 python 机器学习算法之一中使用这些数据。您如何将表数据转换为可用的 python 数据点，然后输入到您选择的模型中？您是否必须提取每列数据并保存为某种类型的变量，例如在 q 进程中运行 exec  语句？]]></description>
      <guid>https://stackoverflow.com/questions/78684235/kdb-q-machine-learning-in-q-kdb</guid>
      <pubDate>Fri, 28 Jun 2024 19:47:48 GMT</pubDate>
    </item>
    <item>
      <title>用于安装机器的优化模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/78684160/optimization-model-for-fitting-a-machine</link>
      <description><![CDATA[我正在从事这个项目，目标是让机器自动对准一个非常具体的点。无需过多细节，我能够轻松确定 0 到 1 范围内的对准程度，并且能够在 x 轴和 y 轴上移动我的机器，并立即看到这个“对准率”如何增加或减少。这在理论上似乎很简单，但有一些问题让我在尝试让机器对准时不知所措。
我的第一个想法是尝试强行使用它。机器永远不会从 0 对准开始，所以我在想我可以让它在给定轴上任意向一个方向移动，看看对准度量是增加还是减少。如果从非零值开始，它总是会看到一些增加或减少。如果它增加，则再次朝该方向移动，如果它减少，则返回并尝试另一个方向，如果它在两个方向上都减少，则停止。我会在另一个轴上重复此操作，效果会很好。
但是，由于对齐的特殊性，我被告知这极易陷入局部最大值。所以我的下一个想法是梯度下降。我将对齐参数设置为负数，并尝试找到最小值。但我以前从未使用过梯度下降，从我在网上看到的 Python 示例中，梯度下降的每个用例都需要了解函数的梯度才能使用。我不知道这一点，所以我不知道梯度下降是否是我想要的。
我想我只是问是否有人对这种情况有任何优化建议，可以推荐仅在不断构建的数据库上工作，而不是预先建立的公式或信息数据库。一开始我只知道当前 (x, y) 方向和当前对齐比率（从 0 到 1），我必须通过进行细微调整并观察这对系统的影响，使其尽可能接近 1，但我还必须避免陷入局部最大值而无法达到最佳状态。也许最好的选择是实现类似蛮力的东西，但强制它跳过多个步骤并重复，如果“最佳”比率小于给定数字（如 0.9 或类似数字）？我不知道。任何帮助都将不胜感激，即使它只是一份其他优化策略的列表，让我可以在自己的时间查找。我甚至不知道其中大多数的正式术语，所以研究很痛苦。]]></description>
      <guid>https://stackoverflow.com/questions/78684160/optimization-model-for-fitting-a-machine</guid>
      <pubDate>Fri, 28 Jun 2024 19:23:42 GMT</pubDate>
    </item>
    <item>
      <title>使用 Yamnet 检测乐器</title>
      <link>https://stackoverflow.com/questions/78684112/detection-of-musical-instruments-using-yamnet</link>
      <description><![CDATA[我的目标是用人工智能（机器学习）检测乐器。
我目前正在使用 Yamnet 模型进行推理，但它的类别范围非常广泛，例如“咆哮”、“打印机”和“钢琴”。我想知道这是否会导致它在检测乐器时不太精确，因为乐器类别只是总类别的一小部分。
Kaggle 上对 Yamnet 模型的描述指出：

您应该进行一定程度的微调和校准，以使 YAMNet 可用于您构建的任何系统。

还有另一个名为 NSynth 的模型，它拥有大量乐器样本数据集，但它用于合成新声音，而不是对乐器进行分类/检测。
在这种情况下，使用 NSynth 对 Yamnet 模块进行微调是否有意义？]]></description>
      <guid>https://stackoverflow.com/questions/78684112/detection-of-musical-instruments-using-yamnet</guid>
      <pubDate>Fri, 28 Jun 2024 19:08:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在使用 Haarcascades 时提高我的结果 [关闭]</title>
      <link>https://stackoverflow.com/questions/78682011/how-to-improve-my-results-while-using-haarcascades</link>
      <description><![CDATA[来自https://github.com/opencv/opencv/tree/master/data/haarcascades
一直在使用 smile.xml 文件，但效果不太准确。
我应该做哪些更改才能提高准确性
我的代码：
import cv2 as cv
import numpy as np
smile_cascade = cv.CascadeClassifier(&#39;haarcascade_smile.xml&#39;)
face_cascade = cv.CascadeClassifier(&#39;haarcascade_frontalface_default.xml&#39;)
cap = cv.VideoCapture(0)
while True:
ret,img = cap.read()
gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)
faces = face_cascade.detectMultiScale(gray,1.3,5)
for (x,y,w,h) in faces:
cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
roi_gray = gray[y:y+h,x:x+w]
roi_color = img[y:y+h,x:x+w]
smiles = smile_cascade.detectMultiScale(gray, 
scaleFactor=1.3, 
minNeighbors=40, 
minSize=(30, 30),
flags=cv.CASCADE_SCALE_IMAGE)
for (ex,ey,ew,eh) in smiles:
cv.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)
cv.imshow(&#39;img&#39;,img)
k = cv.waitKey(30) &amp; 0xFF
if k ==27:
break
cap.release()
cv.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/78682011/how-to-improve-my-results-while-using-haarcascades</guid>
      <pubDate>Fri, 28 Jun 2024 10:47:20 GMT</pubDate>
    </item>
    <item>
      <title>我无法使用 FastAPI 运行使用 Tensorflow 保存的模型</title>
      <link>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</link>
      <description><![CDATA[从 fastapi 导入 FastAPI、File、UploadFile、HTTPException
从 fastapi.middleware.cors 导入 CORSMiddleware
导入 uvicorn
导入 numpy 作为 np
从 io 导入 BytesIO
从 PIL 导入 Image
导入 tensorflow 作为 tf

app = FastAPI()

# 允许向此 API 发出请求的来源列表
origins = [
&quot;http://localhost&quot;,
&quot;http://localhost:3000&quot;, # 在此处添加您的前端 URL
]

app.add_middleware(
CORSMiddleware,
allow_origins=origins,
allow_credentials=True,
allow_methods=[&quot;*&quot;], # 允许所有方法（GET、POST 等）
allow_headers=[&quot;*&quot;], # 允许所有标头
)

# 加载模型
MODEL = tf.saved_model.load(r&quot;D:\Derin_Ogrenme\doma\saved_models\models\model2&quot;)
print(&quot;模型已成功加载&quot;)

CLASS_NAMES = [&quot;早疫病&quot;, &quot;晚疫病&quot;, &quot;健康&quot;]

@app.get(&quot;/&quot;)
async def ping():
return &quot;你好，我还活着&quot;

def read_file_as_image(data) -&gt; np.ndarray:
image = Image.open(BytesIO(data))
image = image.resize((224, 224)) # 将图像大小调整为预期的输入形状
image = np.array(image)
return image

@app.post(&quot;/predict&quot;)
async def predict(file: UploadFile = File(...)):
image = read_file_as_image(await file.read())
img_batch = np.expand_dims(image, 0)

# 执行预测
try:
predictions = MODEL.predict(img_batch)
predict_class = CLASS_NAMES[np.argmax(predictions[0])]
confidence = float(np.max(predictions[0]))
return {
&#39;class&#39;: predict_class,
&#39;confidence&#39;: confidence
}
except Exception as e:
raise HTTPException(status_code=500, detail=&quot;内部服务器错误&quot;)

if __name__ == &quot;__main__&quot;:
uvicorn.run(app, host=&quot;localhost&quot;, port=8000)


编辑：我应该提到我对此很陌生，所以如果您希望我修复某些问题，请简单解释一下，我将不胜感激。当我运行上面看到的代码时，服务器启动，当我转到 localhost:8000 &quot;detail&quot;: &quot;Not Found&quot; 时，我收到此警告。
这是尝试使用我的模型进行预测后出现的终端错误
模型加载成功
INFO：已启动服务器进程 [1140]
INFO：正在等待应用程序启动。
INFO：应用程序启动完成。
信息：Uvicorn 正在 http://localhost:8000 上运行（按 CTRL+C 退出）
信息：::1:55388 - “GET / HTTP/1.1” 200 OK
信息：::1:55393 - “GET /docs HTTP/1.1” 200 OK
信息：::1:55393 - “GET /openapi.json HTTP/1.1” 200 OK
信息：::1:55395 - “POST /predict HTTP/1.1” 500 内部服务器错误

我正在尝试运行此代码，但这行不起作用
#MODEL = tf.keras.models.load_model(r&quot;D:\\Derin_Ogrenme\\doma\\saved_models\\models\\2&quot;)

我向 chatgpt 询问了这个问题，他给了我这行代码。
MODEL = TFSMLayer(r&quot;D:\\Derin_Ogrenme\\doma\\saved_models\\models\\2&quot;, call_endpoint=&#39;serving_default&#39;)

当我使用 FastApi 加载植物生长时，它应该做出预测（它必须对叶子图片进行分类。但正如您在照片中看到的那样，它导致了错误。
]]></description>
      <guid>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</guid>
      <pubDate>Wed, 26 Jun 2024 19:25:44 GMT</pubDate>
    </item>
    <item>
      <title>绘制预测掩码的问题</title>
      <link>https://stackoverflow.com/questions/78669554/issue-with-plotting-predicted-masks</link>
      <description><![CDATA[我目前正在进行一个深度学习项目“叶病分割”。我已经训练了一个模型超过 50 个时期，并获得了以下准确度和损失指标：
训练损失：19.4736，训练准确度：0.9395
验证损失：19.6197，验证准确度：0.9100
测试损失：19.6148，测试准确度：0.9123
但是，当我绘制预测的蒙版时，它们看起来不准确。我的绘图代码有问题吗？
def plot_predictions(model, images, mask, num_samples=5):
predictions = model.predict(images[:num_samples])
for i in range(num_samples):
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.title(&#39;真实图像&#39;)
plt.imshow(images[i])
plt.subplot(1, 3, 2)
plt.title(&#39;地面真相面具&#39;)
plt.imshow(masks[i], cmap=&#39;gray&#39;) # 假设面具已经是二进制的
plt.subplot(1, 3, 3)
plt.title(&#39;预测面具&#39;)
plt.imshow(predictions[i][:, :, 0], cmap=&#39;gray&#39;) # 转换预测面具转换为二进制
plt.show()

plot_predictions(model, test_images.numpy(), test_masks_L, num_samples=5)

原始图像-蒙版-预测蒙版
请检查我的代码并帮助找出可能导致此问题的任何错误？]]></description>
      <guid>https://stackoverflow.com/questions/78669554/issue-with-plotting-predicted-masks</guid>
      <pubDate>Tue, 25 Jun 2024 21:18:52 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：未知层：“CustomScaleLayer”。请确保您使用的是`keras.utils.custom_object_scope`</title>
      <link>https://stackoverflow.com/questions/76488688/valueerror-unknown-layer-customscalelayer-please-ensure-you-are-using-aker</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76488688/valueerror-unknown-layer-customscalelayer-please-ensure-you-are-using-aker</guid>
      <pubDate>Fri, 16 Jun 2023 09:13:29 GMT</pubDate>
    </item>
    </channel>
</rss>