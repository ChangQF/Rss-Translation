<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 22 Jul 2024 18:20:00 GMT</lastBuildDate>
    <item>
      <title>带偏移的增强回归树模型中的错误</title>
      <link>https://stackoverflow.com/questions/78780133/error-in-boosted-regression-tree-model-with-offset</link>
      <description><![CDATA[我正在使用增强回归树模型 (BRT) 来预测海豚物种出现的概率。为此，我有存在/不存在数据和几个环境变量。发生数据被分成几段，我想使用段的长度作为模型中的偏移量，以说明数据收集的工作量。
我使用 dismo 包中的 gbm.step() 函数来拟合模型。
brt&lt;-gbm.step(data=df, gbm.x=c(6,7,12,42,43,15,45,53,93,41,81,87,97), gbm.y = 4, offset = offset, family=&quot;bernoulli&quot;, tree.complexity=3, learning.rate = 0.01, bag.fraction = 0.6)

我将偏移量定义为 log(length of段）
offset&lt;-log(df$eff_length)

&gt; head(offset)
[1] 9.017928 9.171184 9.239406 9.367430 9.264165 9.233178
&gt; summary(offset)
最小值 第 1 区 中位数 平均值 第 3 区 最大值。
8.987 9.065 9.146 9.181 9.265 9.615 

当我运行模型时，它会反复出现此警告：
gbm::predict.gbm(model.list[[i]], x.data[pred.mask, , drop = FALSE], 中出现警告：
predict.gbm 不会将偏移量添加到预测值中。

随后出现此错误：
if (cv.loss.values[j] &gt; cv.loss.values[j - 1]) { 中的错误：
需要 TRUE/FALSE 的值缺失

我尝试包含 fold.vector=offset，因为看起来错误与交叉验证过程，但它给了我相同的警告和错误。
我在这里遗漏了什么？它与伯努利分布类型有关吗？]]></description>
      <guid>https://stackoverflow.com/questions/78780133/error-in-boosted-regression-tree-model-with-offset</guid>
      <pubDate>Mon, 22 Jul 2024 17:51:21 GMT</pubDate>
    </item>
    <item>
      <title>如何在图像中围绕定义的椭圆绘制同心椭圆？</title>
      <link>https://stackoverflow.com/questions/78780078/how-to-draw-concentric-ellipses-around-a-defined-ellipse-in-an-image</link>
      <description><![CDATA[我尝试识别图像中的椭圆，然后绘制两个同心椭圆 - 一个在识别的椭圆外部，一个在识别的椭圆内部。我需要这些椭圆与已识别椭圆的轮廓对齐。
输入图像：原始图像
目标图像：目标图像
这是我目前拥有的代码：
import cv2
import matplotlib.pyplot as plt

# 加载图像
image = cv2.imread(&quot;image_dy.png&quot;)

# 应用高斯模糊
image_gauss = cv2.GaussianBlur(src=image, ksize=(5, 5), sigmaX=10)

# 将模糊图像转换为灰度
image_gray = cv2.cvtColor(image_gauss, cv2.COLOR_BGR2GRAY)

# 查找轮廓
contours, _ = cv2.findContours(image_gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

# 绘制椭圆
for contour in contours:
if len(contour) &gt;= 5:
ellipse = cv2.fitEllipse(contour)
cv2.ellipse(image, ellipse, (0, 255, 0), 2) # 原始椭圆

# 绘制内椭圆和外椭圆
ellipse_inner = (ellipse[0], (ellipse[1][0]*0.8, ellipse[1][1]*0.8), ellipse[2])
ellipse_outer = (ellipse[0], (ellipse[1][0]*1.2, ellipse[1][1]*1.2), ellipse[2])

cv2.ellipse(image, ellipse_inner, (255, 0, 0), 2) # 内椭圆
cv2.ellipse(image, ellipse_outer, (0, 0, 255), 2) # 外椭圆

# 显示结果
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.axis(&#39;off&#39;)
plt.show()

问题：

正确识别图像。
绘制与所识别椭圆正确对齐的内椭圆和外椭圆。

如何改进代码以正确识别椭圆并绘制与所识别椭圆正确对齐的内椭圆和外椭圆？]]></description>
      <guid>https://stackoverflow.com/questions/78780078/how-to-draw-concentric-ellipses-around-a-defined-ellipse-in-an-image</guid>
      <pubDate>Mon, 22 Jul 2024 17:38:05 GMT</pubDate>
    </item>
    <item>
      <title>keras BackupAndRestore 恢复模型但准确率较低且损失较高</title>
      <link>https://stackoverflow.com/questions/78779866/keras-backupandrestore-resuming-a-model-but-shows-lower-accuracy-and-higher-loss</link>
      <description><![CDATA[你好朋友，我正在使用 google colab gpu，我需要尽可能多地保存模型，然后在新的 google colab 中重新启动训练模型，下面是我的代码，但准确率较低，损失较高，例如在 epoch 89 中我有
acc：0.9990301728 loss：0.002603143221
acc_val：0.9557291865 loss_val：0.2962754667
在 epoch 90 中，第一次在新的 google colab 中运行我有：
acc：0.9803879261 loss：01103143221
acc_val：0.939127624 loss_val：0.1836656481
# 定义 5 倍交叉验证
kf = KFold(n_splits=5, shuffle=True, random_state=42)

conf_matrix_all_CNN_10s = []
fpr_matrix_all_CNN_10s = []
tpr_matrix_all_CNN_10s = []

for i, (train_val_index, test_index) in enumerate(kf.split(all_file_paths)):
print(f&quot;Fold {i+1}:&quot;)

# 将数据拆分为该折叠的训练和验证集
train_val_files = [all_file_paths[j] for j in train_val_index]
test_files = [all_file_paths[j] for j in test_index]

train_files, val_files = train_test_split(train_val_files, test_size=0.25, random_state=42)

# 创建训练和测试数据生成器
train_generator = data_generator_train(train_files, batch_size)
val_generator = data_generator_val(val_files, batch_size)
test_generator = data_generator_test(test_files, batch_size)

# 定义模型架构
model = Sequential()
input_shape = np.load(epilepsy_file_paths[0]).T.shape

model.add(Conv1D(64, 3, strides=1, input_shape=input_shape, padding=&#39;same&#39;, activity=&#39;relu&#39;))
model.add(BatchNormalization())
model.add(MaxPool1D(pool_size=(2)))
model.add(Dropout(0.5))

model.add(Conv1D(48, 3, strides=1, padding=&#39;same&#39;, activity=&#39;relu&#39;))
model.add(BatchNormalization())
model.add(MaxPool1D(pool_size=(2)))
model.add(Dropout(0.5))

model.add(Conv1D(32, 3, strides=1, padding=&#39;same&#39;, activated=&#39;relu&#39;))
model.add(BatchNormalization())
model.add(MaxPool1D(pool_size=(2)))
model.add(Dropout(0.5))

model.add(Flatten())

model.add(Dense(256, activated=&#39;relu&#39;))
model.add(Dropout(0.2))
model.add(Dense(128, activated=&#39;relu&#39;))
model.add(Dropout(0.2))
model.add(Dense(1, activated=&#39;sigmoid&#39;))

modelOptimizer = Adam(learning_rate=0.001)
model.compile(optimizer=modelOptimizer, loss=BinaryCrossentropy(), metrics=[&#39;accuracy&#39;])

reduceLR_callback = ReduceLROnPlateau(
monitor=&quot;val_loss&quot;,
factor=0.5,
waiting=7,
mode=&quot;min&quot;,
min_lr=1e-5,
)

checkpoint_dir = &#39;/content/drive/MyDrive/Project1/Weights/10s&#39;

# 定义 CSV 记录器回调
csv_logger = CSVLogger(f&#39;{checkpoint_dir}/training_log_fold_{i + 1}_CNN_10s.csv&#39;, append=True)

backup_restore_callback = BackupAndRestore(backup_dir=f&#39;{checkpoint_dir}/backup_fold_{i + 1}&#39;)

# 训练带有训练数据的模型
history = model.fit(
train_generator,
epochs=epochs,
steps_per_epoch=len(train_files) // batch_size,
validation_data=val_generator,
validation_steps=len(val_files) // batch_size,
callbacks=[reduceLR_callback, csv_logger, backup_restore_callback],
shuffle=False
)

我预计重新加载模型后准确率和损失率会接近]]></description>
      <guid>https://stackoverflow.com/questions/78779866/keras-backupandrestore-resuming-a-model-but-shows-lower-accuracy-and-higher-loss</guid>
      <pubDate>Mon, 22 Jul 2024 16:38:44 GMT</pubDate>
    </item>
    <item>
      <title>如何针对 NYU 数据集训练 Yolo 3D</title>
      <link>https://stackoverflow.com/questions/78779752/how-to-train-yolo-3d-for-nyu-dataset</link>
      <description><![CDATA[我已经在 KITTI 数据集中训练了我的 Yolo 3D 模型，现在我想在 NYU 数据集上训练它。我必须在 NYU 数据集中做哪些更改才能在 YOLO 3D 模型中训练它？
我想知道 YOLO 3D 接受的数据集格式。]]></description>
      <guid>https://stackoverflow.com/questions/78779752/how-to-train-yolo-3d-for-nyu-dataset</guid>
      <pubDate>Mon, 22 Jul 2024 16:09:49 GMT</pubDate>
    </item>
    <item>
      <title>从 huggingface 下载数据集（仅包含图像 id 的 json 文件）</title>
      <link>https://stackoverflow.com/questions/78779713/downloading-dataset-from-huggingface-with-only-json-file-with-image-id</link>
      <description><![CDATA[我正在尝试重现LLaVA-RLHF论文。
我正在使用他们的数据集访问他们的官方huggingface repo，
https://huggingface.co/datasets/zhiqings/LLaVA-Human-Preference-10K
但我有点困惑。
提供的文件只是一个json文件，没有实际的图像，只有图像id和文件名。
（例如000000013249.jpg）
我想下载数据集的图像，但我不知道如何仅使用json文件进行下载。我很确定一定有一种方法可以下载数据集的图像，但我不知道如何下载。
有没有办法将图像与 json 文件一起下载？
我觉得我遗漏了一个非常基本的要点，但我现在有点迷茫。
我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78779713/downloading-dataset-from-huggingface-with-only-json-file-with-image-id</guid>
      <pubDate>Mon, 22 Jul 2024 16:03:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在没有 keras 的情况下根据训练过的 keras 模型做出预测？</title>
      <link>https://stackoverflow.com/questions/78779669/how-to-make-predictions-from-trained-keras-model-without-keras</link>
      <description><![CDATA[我有一个使用 keras 创建和训练的 1D CNN，并且我将权重保存在 h5 文件中，将架构保存在 json 文件中。现在，我希望能够读取架构和权重，并使用它们进行新的预测，独立于 keras 或 TensorFlow。我只想使用 numpy、scipy 等基本软件包，但如果有任何其他软件包可以以最少的依赖性完成此操作，那么我想使用它。我找到了 Konverter，但它不适用于 CNN。]]></description>
      <guid>https://stackoverflow.com/questions/78779669/how-to-make-predictions-from-trained-keras-model-without-keras</guid>
      <pubDate>Mon, 22 Jul 2024 15:53:20 GMT</pubDate>
    </item>
    <item>
      <title>哪些模型是图像形状检测最有效的模型？</title>
      <link>https://stackoverflow.com/questions/78779365/which-are-the-most-efficient-models-for-shape-detection-in-an-image</link>
      <description><![CDATA[我想创建一个 Python 算法来检测两张照片之间出现和消失的物体。这些照片不是从同一个地方拍摄的。我的想法是使用一个预训练模型，它可以检测两张照片中所有“足够大”的物体，并检测一个或多个物体何时出现或消失。
我尝试使用 YOLO，但它似乎只能检测到他可以分类的物体。由于我的数据不是关于常见物体的，所以效果不太好。
是否有一些预训练模型可以检测几何形状/物体而无需对它们进行分类？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78779365/which-are-the-most-efficient-models-for-shape-detection-in-an-image</guid>
      <pubDate>Mon, 22 Jul 2024 14:45:32 GMT</pubDate>
    </item>
    <item>
      <title>PyBullet 的 Walker2D 的关节角度</title>
      <link>https://stackoverflow.com/questions/78778576/joint-angles-of-pybullets-walker2d</link>
      <description><![CDATA[我正在使用 pybullet_envs 中的 Walker2D 环境，并尝试获取 6 个关节角度，以便将它们用于由 Arduino 控制的实际双足机器人。
我尝试过但未能找到文档，例如 gym 中的文档，其中指定了 6 个关节角度的索引，并且它们的值由 rad 测量。我发现有人指出，在 pybullet_envs GitHub 文档中，有评论说关节位置是观察空间的偶数元素。我认为这从 Walker2D 环境的每个列的数组索引 8 开始是正确的。我的问题是我不知道这些值的单位，我不确定它们是否正确。有没有更简单的方法可以通过物理客户端 ID 和机器人 ID 通过环境访问实际值（rad 或 deg）？]]></description>
      <guid>https://stackoverflow.com/questions/78778576/joint-angles-of-pybullets-walker2d</guid>
      <pubDate>Mon, 22 Jul 2024 12:02:18 GMT</pubDate>
    </item>
    <item>
      <title>多类别分类选择机器学习模型</title>
      <link>https://stackoverflow.com/questions/78777697/multi-class-classification-selecting-machine-learning-model</link>
      <description><![CDATA[在多类分类问题中，使用神经网络通常比使用 svm 模型更好吗？
这是我用数据制作的图。
计算 PCA 并使用 SVM 进行线性分类对我来说会更好吗？scatterplot3d
pca visuslization
那么当我得到这个准确度指标时。我保留代码可以吗？准确度]]></description>
      <guid>https://stackoverflow.com/questions/78777697/multi-class-classification-selecting-machine-learning-model</guid>
      <pubDate>Mon, 22 Jul 2024 08:40:05 GMT</pubDate>
    </item>
    <item>
      <title>GPy 回归中的输出 Gram 矩阵</title>
      <link>https://stackoverflow.com/questions/78776963/output-gram-matrix-in-gpy-regression</link>
      <description><![CDATA[因为我需要在大量点上训练我的高斯过程 (GP)，所以我不仅想保存优化的超参数
ker = GPy.kern.Matern32(nc, ARD=True) 
m = GPy.models.GPRegression(xTrain, np.reshape(yTrain, (-1, 1)), ker, noise_var=1e-8)
m.Mat32.lengthscale.constrain_bounded(0.01, 5e0)
m.Mat32.variance.constrain_bounded(1e-4, 1e+10)
m.Gaussian_noise.variance.constrain_fixed(1e-8)

m.optimize_restarts(messages=True, num_restarts=1, max_f_eval=10000)
np.save(f, m.param_array)

yPrd, yVar = m.predict_noiseless(xTest)
yStd = np.sqrt(yVar)
yPred = np.squeeze(yPrd)

但也要保存 Gram 矩阵与观察到的 值 K(s,s)^{-1} f 的乘积
后验的平均值 = K(s^*,s)K(s,s)^{-1} f
因此加载 块 K(s,s)^{-1} f 并仅与相应的测试 点 K(s^*,s) 执行一次乘法。我如何访问/输出这些矩阵/保存它们？]]></description>
      <guid>https://stackoverflow.com/questions/78776963/output-gram-matrix-in-gpy-regression</guid>
      <pubDate>Mon, 22 Jul 2024 04:33:20 GMT</pubDate>
    </item>
    <item>
      <title>无法重现 ViTImageProcessor 变压器库的预处理</title>
      <link>https://stackoverflow.com/questions/78776752/cannot-reproduction-vitimageprocessor-preprocessing-of-transformers-library</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78776752/cannot-reproduction-vitimageprocessor-preprocessing-of-transformers-library</guid>
      <pubDate>Mon, 22 Jul 2024 02:25:58 GMT</pubDate>
    </item>
    <item>
      <title>使用python图像去噪没有得到想要的重建图像</title>
      <link>https://stackoverflow.com/questions/78776540/not-getting-desired-reconstructed-image-with-python-image-denoising</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78776540/not-getting-desired-reconstructed-image-with-python-image-denoising</guid>
      <pubDate>Sun, 21 Jul 2024 23:19:13 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Yolo 3D 中获取边界框标签及其格式</title>
      <link>https://stackoverflow.com/questions/78776364/how-to-get-bounding-box-labels-and-its-format-in-yolo-3d</link>
      <description><![CDATA[我已经在工作区中实现了 Yolo 3D 模型，并且运行良好。我得到了一个图像作为输出。所以我想知道在哪里可以得到我的 3D 边界框的标签。我甚至想知道标签的存储格式是什么。
x,y,x,y... 格式或 x,y,z,x,y,z... 格式。
(编辑)
我已经实现了 Yolo 3D 模型：
https://github.com/ruhyadi/YOLO3D
我下载了预训练的权重，并使用以下命令运行推理：
python inference.py \
--weights yolov5s.pt \
--source eval/image_2 \
--reg_weights weights/resnet18.pkl \
--model_select resnet18 \
--output_path runs/ \
--show_result --save_result

现在在运行文件夹中，我仅获得 PNG 图像作为输出。我的输出是：

现在我甚至想获取此图像中 3D 边界框的点，我甚至想知道这些点的存储格式是 x,y,x,y.. 格式还是 x,y,z,x,y,z.. 格式？]]></description>
      <guid>https://stackoverflow.com/questions/78776364/how-to-get-bounding-box-labels-and-its-format-in-yolo-3d</guid>
      <pubDate>Sun, 21 Jul 2024 21:16:51 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Flux.jl 中的 RNN 层？</title>
      <link>https://stackoverflow.com/questions/78776287/how-to-use-the-rnn-layer-in-flux-jl</link>
      <description><![CDATA[我对这个 Flux.jl NN 做错了什么？
我的 batch_size=5 和 num_features = 1200
如果我想输入 (num_timesteps, num_features)，RNN 层如何工作？
Flux.Chain(
Flux.RNN(num_features, 32, tanh, init=Flux.glorot_normal), 
Flux.Dropout(0.2),
Flux.flatten,
Flux.Dense(32*batch_size, output_size,identity;bias = false)
)

错误：
DimensionMismatch：层 Dense(160 =&gt; 1;bias=false) 期望 size(input, 1) == 160，但得到的是 32×5矩阵{Float32}
]]></description>
      <guid>https://stackoverflow.com/questions/78776287/how-to-use-the-rnn-layer-in-flux-jl</guid>
      <pubDate>Sun, 21 Jul 2024 20:22:24 GMT</pubDate>
    </item>
    <item>
      <title>将safetensors模型格式（LLaVA模型）转换为gguf格式</title>
      <link>https://stackoverflow.com/questions/78763327/convert-safetensors-model-formatllava-model-into-gguf-format</link>
      <description><![CDATA[我想在 ollama 中进行 LLaVA 推理，因此我需要将其转换为 gguf 文件格式。
我的模型具有文件格式 safetensors。（使用 lora 训练）
似乎 ollama 仅支持 llama，但不支持 llava，如下所示，
https://github.com/ollama/ollama/blob/main/docs/import.md
我遵循了 llama.cpp 的说明，并在此处使用了代码 convert_lora_to_gguf.py，
https://github.com/ggerganov/llama.cpp/blob/master/convert_lora_to_gguf.py
但是我收到如下错误：
ERROR:lora-to-gguf:不支持 Model LlavaLlamaForCausalLM

如果我在模型文件的 config.json 中写入 llama 模型并运行以下代码，则会收到另一个错误。
model_instance.gguf_writer.add_string(gguf.Keys.General.TYPE, gguf.GGUFType.ADAPTER)
model_instance.gguf_writer.add_string(gguf.Keys.Adapter.TYPE, &quot;lora&quot;)
model_instance.gguf_writer.add_float32(gguf.Keys.Adapter.LORA_ALPHA, float(alpha))
model_instance.gguf_writer.add_quantization_version(gguf.GGML_QUANT_VERSION)
logger.info(&quot;Exporting model...&quot;)
model_instance.write()
logger.info(f&quot;模型已成功导出至 {model_instance.fname_out}&quot;)

Traceback (most recent call last):
File &quot;C:\Users\jjjy2\OneDrive\Desktop\VLM_FastAPI\ollama\convert_lora_to_gguf.py&quot;, line 373, in &lt;module&gt;
model_instance.gguf_writer.add_string(gguf.Keys.General.FILE_TYPE, gguf.GGUFType.ADAPTER)
AttributeError: module &#39;gguf&#39; has no attribute &#39;GGUFType&#39;

似乎所有代码和 gguf 包都不支持 llava，只支持 llama。我必须将我自己训练的模型转换为 gguf。我无法使用 hugging face 的 gguf llava 模型进行推理。
有没有办法转换它？]]></description>
      <guid>https://stackoverflow.com/questions/78763327/convert-safetensors-model-formatllava-model-into-gguf-format</guid>
      <pubDate>Thu, 18 Jul 2024 08:47:53 GMT</pubDate>
    </item>
    </channel>
</rss>