<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 18 Dec 2024 18:25:04 GMT</lastBuildDate>
    <item>
      <title>为什么我的 OLS 和梯度下降结果对于线性回归有显著差异？</title>
      <link>https://stackoverflow.com/questions/79291892/why-do-my-ols-and-gradient-descent-results-differ-significantly-for-linear-regre</link>
      <description><![CDATA[我正在写一篇论文，比较普通最小二乘法 (OLS) 与传统梯度下降法 (CGD) 在线性回归中的性能。虽然我使用 SciPy 的 OLS 实现符合预期结果（例如，statsmodels），但我的 CGD 实现产生了截然不同的参数估计，即使在标准化数据、使用低学习率并运行许多个时期之后也是如此。
以下是我所做的：
A. 使用 SciPy 的 OLS：
我使用闭式正则方程和伪逆计算系数。我使用的正态方程与往常一样：

结果与 statsmodels 相符：
beta_encoding_scipy = pinv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y

OLS 结果：

截距 -22.507045
学习时间 2.852729
之前的分数 1.018319
睡眠时间 0.480321
练习的样题 0.193910
课外活动_否-11.561869
课外活动_是 -10.945176
dtype: float64

B.梯度下降实现：
我标准化了训练数据并使用以下设置实现了梯度下降：


学习率：0.001
迭代次数：5000
初始值：所有系数均设置为 0

我的实现：
def gradient_descent(features, label, learning_rate, epochs, precision):
# 初始化
X_augmented = np.hstack((np.ones((features.shape[0], 1)), features))
beta = np.zeros(X_augmented.shape[1]) # 初始化系数

for epoch in range(epochs):
predictions = X_augmented @ beta
residuals = predictions - label
gradient = (2 / len(label)) * X_augmented.T @ residuals
beta = beta - learning_rate * gradient
return beta

CGD 结果:

截距 55.142748
学习时长 7.392875
之前的分数 17.722890
睡眠时间 0.819071
练习过的样题 0.531543
课外活动_否 -0.149439
课外活动_是 0.149439
dtype: float64

观察到的差异：
CGD 的结果与 OLS 的结果相差甚远，我不确定原因。我怀疑这可能是由于数据分割过程（应用 StandardScaler 进行训练/测试分割）造成的。
关键问题：

如果经过适当训练，CGD 不应该产生与 OLS 类似的结果吗？
训练测试分割或缩放过程是否会影响 CGD 性能？
我的 CGD 实现或参数初始化有什么问题吗？

我很感激任何关于这些方法为何分歧以及如何提高我的梯度下降实现的准确性的见解！]]></description>
      <guid>https://stackoverflow.com/questions/79291892/why-do-my-ols-and-gradient-descent-results-differ-significantly-for-linear-regre</guid>
      <pubDate>Wed, 18 Dec 2024 16:52:02 GMT</pubDate>
    </item>
    <item>
      <title>训练期间更高的验证准确率</title>
      <link>https://stackoverflow.com/questions/79291675/greater-validation-accuracy-during-training</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79291675/greater-validation-accuracy-during-training</guid>
      <pubDate>Wed, 18 Dec 2024 15:37:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么 compare_models 和 plot_model 的性能结果对于具有 10 倍 cv 的设置不同</title>
      <link>https://stackoverflow.com/questions/79291336/why-the-performance-results-of-compare-models-and-plot-model-for-a-setup-with-10</link>
      <description><![CDATA[在 10 倍 cv 会话中，setup 没有额外的“test_data”，因此数据被拆分为每倍的训练数据和测试数据。
compare_models 的结果是否来自 10 倍训练 + 测试数据的平均性能？
plot_model 的结果来自手册中所示的“hold-out”数据集的平均性能。这里的“hold-out”数据集是每倍的测试数据，而 plot_model 的结果则是 10 倍测试数据的平均性能？
因为 compare_models 和 plot_model 的性能结果不同。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/79291336/why-the-performance-results-of-compare-models-and-plot-model-for-a-setup-with-10</guid>
      <pubDate>Wed, 18 Dec 2024 13:51:33 GMT</pubDate>
    </item>
    <item>
      <title>随机森林分类器的适应性</title>
      <link>https://stackoverflow.com/questions/79290974/adaptation-of-random-forest-classifier</link>
      <description><![CDATA[我正在尝试更改随机森林分类器的功能。虽然通常每次分割都会随机选择特征，但我希望每次分割时都评估一个特定特征。我知道这会影响性能，但我想尝试一下这在非常具体的用例中是否是个好主意。因此，调整的结果应为：用于分割的特征是随机选择的（与往常一样），但始终会考虑一个特定特征（例如索引 15）（不一定使用）。据我所知，没有函数允许我指定这一点（如果有，请告诉我），所以我正在尝试更改分类器函数的源代码。
我目前的状态是我选择了 R 中的 randomForest 包。我创建了一个本地版本（通过从 cran 下载 .tar.gz）并更改了 c 脚本中的某些内容（特别是“findbestsplit”函数中的以下部分：
for (i = 0, i &lt; mtry; ++i) {
/* 样本 mtry 变量，不替换。 */
j = (int) (unif_rand() * (last + 1));
mvar = mIndex[j];
swapInt(mIndex[j], mIndex[last]);
la​​st--;

if (i == mtry - 1) { /* 在最后一次迭代中更改某些内容 */
mvar = additionalVarIndex; 
/* 专门设置mvars 最后一个值到所需特征 */
}

此处，additionalVarIndex 是我通过 c 脚本中的函数传递给 findbestsplit 函数（似乎有效）的索引。我的 c 知识非常有限，所以我认为这还不够，但我也无法测试它，因为我的更改似乎不会影响 r 应用程序。当我加载调整后的包（通过以下方式将其转回 .tar.gz）时
&amp; &quot;C:\Program Files\R\R-4.4.1\bin\x64\R.exe&quot; CMD build .

通过
install.packages(&quot;C:/Users/niklas.jacobs/AppData/Local/R/win- 
library/randomForest/randomForest_4.7-1.2.tar.gz&quot;, repos = NULL, type = &quot;source&quot;)

randomForest r 函数的行为没有发生任何变化（即使我删除了 c 代码的随机部分，我这样做只是为了了解我是否可以更改内容）。我仍然确信我的更改会产生一些影响，因为如果我删除大部分 c 代码，该函数将停止工作。我目前的怀疑是c 脚本的 fortran 调用以某种方式绕过了我的更改（据我所知，使用“findbestsplit”的“buildtree”函数不是在 c 脚本中定义的，而是在 fortran 中定义的），但我不知道如何访问 fortran。
简而言之：有人对我接下来的行动有什么建议吗（或者你有更好的想法来实现我的目标吗？）。提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/79290974/adaptation-of-random-forest-classifier</guid>
      <pubDate>Wed, 18 Dec 2024 11:48:27 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：

&#39;super&#39; 对象没有属性 &#39;__sklearn_tags__&#39;。

当我在 RandomizedSearchCV 对象上调用 fit 方法时会发生这种情况。我怀疑它可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我希望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>如何从信息流中删除“助手”一词？[关闭]</title>
      <link>https://stackoverflow.com/questions/79290138/how-do-i-remove-the-word-assistant-from-a-stream</link>
      <description><![CDATA[我一直试图从流中删除单词“assistant”，因为它总是以这个词开头。无论我在后端使用和实现什么逻辑，它都不起作用，我想从后端处理它。
def generate_streamed_response(prompt: str, behavior: str):
try:
text_pipeline = load_pipeline()

# 准备具有系统和用户角色的输入消息
messages = [
{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: behavior},
{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}
]

# 准备用于文本生成的流式传输器
streamer = TextIteratorStreamer(text_pipeline.tokenizer, skip_prompt=True, skip_special_tokens=True)

# 在单独的线程中启动文本生成
thread = threading.Thread(
target=text_pipeline.model.generate,
kwargs={
&quot;inputs&quot;: text_pipeline.tokenizer.apply_chat_template(
messages, return_tensors=&quot;pt&quot;
).to(device),
&quot;streamer&quot;: streamer,
&quot;max_new_tokens&quot;: settings.max_tokens
}
)
thread.start()

# 第一个块标志
first_chunk = True

# 生成令牌时产生令牌
for chunk in streamer:
if first_chunk:
# 处理 &quot;assistant&quot;在第一个块中
chunk = chunk.strip()
if chunk.lower().startswith(&quot;assistant&quot;):
chunk = chunk[len(&quot;assistant&quot;):].strip()
first_chunk = False
yield chunk

except Exception as e:
logger.error(f&quot;流式响应生成期间出错：{e}&quot;)
raise HTTPException(status_code=500,detail=&quot;生成流式响应时出错&quot;)

]]></description>
      <guid>https://stackoverflow.com/questions/79290138/how-do-i-remove-the-word-assistant-from-a-stream</guid>
      <pubDate>Wed, 18 Dec 2024 06:14:21 GMT</pubDate>
    </item>
    <item>
      <title>ADTK 模型消耗高 CPU [Python]</title>
      <link>https://stackoverflow.com/questions/79290008/adtk-model-consuming-high-cpu-python</link>
      <description><![CDATA[我们从去年开始在 Python 应用程序中使用 ADTK 模型来检测异常。
自上周以来，我们观察到 CPU 利用率大幅上升，原因已确定为 Python 进程。
经过故障排除，我们发现 ADTK 模型是导致该问题的原因。我们正在使用其他 ML 模型，如 IForest、ECOD、CBLOF 等。但只有 ADTK 负责此峰值。
请查找当前模型代码以供参考：
data=df.copy() 
minutes = 300

df[&#39;predict_dt&#39;]=pd.to_datetime(df[&#39;predict_dt&#39;])
df = df.set_index([&#39;predict_dt&#39;]).sort_index() 

seasonal_vol = SeasonalAD(c=1.6,side=&#39;negative&#39;,trend=True) 
seasonal_vol.fit(df[&#39;predict_count&#39;])
df[&#39;anomalies&#39;]=seasonal_vol.predict(df[&#39;predict_count&#39;]) 

df2 = pd.DataFrame(columns=[&#39;predict_dt&#39;, &#39;hour&#39;, &#39;min&#39;,&#39;predict_count&#39;,&#39;label&#39;]) 
final = datetime.datetime.now(pytz.timezone(&#39;America/Los_Angeles&#39;)).replace(tzinfo=None)-timedelta(minutes=minutes)

for i in range(len(df)): 
if(data[&#39;predict_dt&#39;][i] &gt;= final):
if str(df[&#39;anomalies&#39;][i]).lower() == &quot;true&quot;:
#调用 SP 进行进一步操作

我们不确定哪个部分导致了问题。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79290008/adtk-model-consuming-high-cpu-python</guid>
      <pubDate>Wed, 18 Dec 2024 04:50:01 GMT</pubDate>
    </item>
    <item>
      <title>如何在 LM 中使用标记化</title>
      <link>https://stackoverflow.com/questions/79289981/how-to-use-tokenizeation-in-lm</link>
      <description><![CDATA[所以我一直在用这些超参数训练这个 LM..
blocksiz = 128
batchsiz = 32
nemb = 256
nhead = 4
nlayers = 4
evalIters = 100
lr = 3e-4
epochs = 30

我的数据集也是 93kb..，
我使用 GPT-2 作为 Tokenizer..
然而，当我使用 GPT-2 Tokenizer 时，训练时间太长了，
但是当我将模型训练为二元组时。它不需要那么多。
所以使用具有 50257 个 Vocabsiz 的 GPT2 作为 tokenizer，会影响模型训练吗？
训练是否需要更多 GPU 资源？
所以对于小数据集。我应该使用什么作为标记器...？
我减少了超参数，上面我使用的参数是减少的结果
但模型训练时间太长了]]></description>
      <guid>https://stackoverflow.com/questions/79289981/how-to-use-tokenizeation-in-lm</guid>
      <pubDate>Wed, 18 Dec 2024 04:30:58 GMT</pubDate>
    </item>
    <item>
      <title>如果原始训练数据是离散的，是否可以将数据添加到连续的 keras 模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79289159/is-it-possible-to-add-data-to-a-keras-model-that-is-continous-if-the-original-tr</link>
      <description><![CDATA[我有一个模型 keras 模型，该模型是在离散数据（基于生物体中基因的存在而得出的是/否数据）上进行训练的，这可以预测离散响应（生物体是否执行某种功能而得出的是/否）。
我们想要添加连续的新数据（生物体中蛋白质的丰度），现在我们想要预测连续响应（某种功能的活动水平）。
这有可能做到吗？我已按照此链接中的步骤进行操作，但似乎假设您使用的是相同类型的数据。我该如何添加不同类型的数据？]]></description>
      <guid>https://stackoverflow.com/questions/79289159/is-it-possible-to-add-data-to-a-keras-model-that-is-continous-if-the-original-tr</guid>
      <pubDate>Tue, 17 Dec 2024 19:32:07 GMT</pubDate>
    </item>
    <item>
      <title>Databricks MLFlow 和 MetaFlow 集成</title>
      <link>https://stackoverflow.com/questions/79287981/databricks-mlflow-and-metaflow-integration</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79287981/databricks-mlflow-and-metaflow-integration</guid>
      <pubDate>Tue, 17 Dec 2024 13:02:01 GMT</pubDate>
    </item>
    <item>
      <title>LSTM RNN Tensorflow 语言预测模型卡住了吗？</title>
      <link>https://stackoverflow.com/questions/79273048/lstm-rnn-tensorflow-language-prediction-model-stuck</link>
      <description><![CDATA[我正在尝试开发一个模型，从不同艺术家的歌词中学习，以确定是谁写的。我的 Jupyter 笔记本和 tsv 文件保存在此文件夹中：文件夹链接。
我尝试过不同的策略，但结果很差。要么是低损失/低准确度，要么是高精度/高损失。想知道是否有人能发现我意识之外的明显问题。
Jupyter 笔记本代码在这里：
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras import regularizers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks 导入 EarlyStopping
来自 tensorflow.keras.layers 导入 BatchNormalization
导入 torch
导入 torch.nn 作为 nn

# 用于操作目录路径
导入 os

# 用于 python 的科学和矢量计算
导入 numpy 作为 np

# 绘图库
来自 matplotlib 导入 pyplot 作为 plt

# scipy 中的优化模块
来自 scipy 导入 o​​ptimize

# 将用于加载 MATLAB mat 数据文件格式
来自 scipy.io 导入 loadmat

# 告诉 matplotlib 在笔记本中嵌入绘图
%matplotlib inline

导入 pandas 作为 pd

来自 sklearn.metrics 导入 confused_matrix、ConfusionMatrixDisplay
来自 sklearn.utils.class_weight 导入 compute_class_weight
来自 sklearn.utils 导入 class_weight

导入 seaborn 作为 sns

df = pd.read_csv(&#39;shuffled_verses.tsv&#39;, sep=&#39;\t&#39;)
X = np.asarray(df.values[:3975, 6]).astype(&#39;str&#39;)
X_cv = np.asarray(df.values[3975:5300, 6]).astype(&#39;str&#39;)
X_test = np.asarray(df.values[5300:, 6]).astype(&#39;str&#39;)

# Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
X_sequences = tokenizer.texts_to_sequences(X)
X_cv_sequences = tokenizer.texts_to_sequences(X_cv)
X_test_sequences = tokenizer.texts_to_sequences(X_test)

# 添加填充
X_padded = pad_sequences(X_sequences, padding=&#39;post&#39;)
X_cv_padded = pad_sequences(X_cv_sequences, padding=&#39;post&#39;)
X_test_padded = pad_sequences(X_test_sequences, padding=&#39;post&#39;)

y = np.asarray(df.values[:3975, 1]).astype(&#39;float32&#39;)
y_cv = np.asarray(df.values[3975:5300, 1]).astype(&#39;float32&#39;)
y_test = np.asarray(df.values[5300:, 1]).astype(&#39;float32&#39;)

df = pd.read_csv(&#39;my_artists.tsv&#39;, sep=&#39;\t&#39;)
X_encoding = np.asarray(df.values[:, 0]).astype(&#39;str&#39;)
y_encoding = np.asarray(df.values[:, 1]).astype(&#39;float32&#39;)

# 超参数
embedding_dim = 100 # 从 100 更改为 200
rnn_units = 128 # 从 128 更改为 256
max_sequence_length = X_padded.shape[1] # 填充序列的长度
vocab_size = len(tokenizer.word_index) + 1 # 词汇表大小（为填充标记添加 1）
class_weights = class_weight.compute_class_weight(&#39;balanced&#39;, classes=np.unique(y_encoding), y=y)
class_weights = dict(enumerate(class_weights))
total_weight = sum(class_weights.values())
normalized_class_weights = {k: v / total_weight for k, v in class_weights.items()}

# 构建模型
model = Sequential()

# 嵌入层
model.add(Embedding(input_dim=vocab_size, # 词汇表的大小
output_dim=embedding_dim,
input_length=max_sequence_length,)) # 输入序列的长度

# RNN
model.add(Bidirectional(LSTM(rnn_units, return_sequences=True, kernel_regularizer=l2(0.006))))
model.add(Dense(70,activation=&#39;relu&#39;, kernel_regularizer=l2(0.006)))
model.add(Dropout(0.2))
model.add(Dense(45,activation=&#39;relu&#39;, kernel_regularizer=l2(0.006)))
model.add(Dropout(0.2))
model.add(Dense(num_classes,activation=&#39;softmax&#39;, kernel_regularizer=l2(0.001)))

# 编译
model.compile(optimizer=Adam(learning_rate=0.0001), 
loss=&#39;sparse_categorical_crossentropy&#39;, # 使用 &#39;categorical_crossentropy&#39; 进行多分类
metrics=[&#39;accuracy&#39;])

# EarlyStopping
early_stopping = EarlyStopping(
monitor=&#39;val_loss&#39;, # 监控验证损失
patience=3, # 如果连续 3 个时期没有改善，则停止
restore_best_weights=True # 恢复最佳模型权重
)

model.summary()

# 训练
history = model.fit(X_padded, y,
epochs=300,
batch_size=32,
validation_data=(X_cv_padded, y_cv),
class_weight=normalized_class_weights)
]]></description>
      <guid>https://stackoverflow.com/questions/79273048/lstm-rnn-tensorflow-language-prediction-model-stuck</guid>
      <pubDate>Wed, 11 Dec 2024 20:02:40 GMT</pubDate>
    </item>
    <item>
      <title>处理 Llama 3.2：3b-Instruct 模型中的令牌限制问题（最多 2048 个令牌）[关闭]</title>
      <link>https://stackoverflow.com/questions/79267003/handling-token-limit-issues-in-llama-3-23b-instruct-model-2048-tokens-max</link>
      <description><![CDATA[我正在使用 Llama 3.2:3b-instruct 模型并遇到以下错误：
此模型的最大上下文长度为 2048 个令牌。但是，您请求了 
2049 个令牌（消息中 1681 个，完成中 368 个）。

我理解这是由于超出令牌限制造成的，但我想知道：

是否有任何最佳实践或技术可以减少令牌使用量，而不会丢失消息或完成中的关键上下文？
]]></description>
      <guid>https://stackoverflow.com/questions/79267003/handling-token-limit-issues-in-llama-3-23b-instruct-model-2048-tokens-max</guid>
      <pubDate>Tue, 10 Dec 2024 04:19:17 GMT</pubDate>
    </item>
    <item>
      <title>将请求上下文从 FastAPI 传递到用于 OpenAI 集成的 Microsoft Semantic Kernel 插件</title>
      <link>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</link>
      <description><![CDATA[我正在 FastAPI 应用程序中将 Microsoft Semantic Kernel 与 OpenAI 集成。我有一个聊天/端点，我从请求中收到一个 session_id，我需要将此 session_id 与 openai_client 一起传递给插件。但是，我不确定如何在内核的执行过程中将 FastAPI 请求中的 session_id 正确传递给插件。
以下是设置内核和插件的相关代码：
# 内核和服务设置
kernel = Kernel()

execution_settings = AzureChatPromptExecutionSettings(tool_choice=&quot;auto&quot;)
execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={})

openai_client = OpenAI(api_key=api_key)
chat_completion_service = OpenAIChatCompletion(
ai_model_id=model_id, 
api_key=api_key, 
service_id=service_id 
)

# 添加服务和插件
kernel.add_service(chat_completion_service)
kernel.add_plugin(MovesPlugin(openai_client), plugin_name=&#39;MovesPlugin&#39;)

在我的 FastAPI 端点内，我想在调用内核进行聊天响应时将 session_id 传递给插件：
# 在 FastAPI 端点内
@app.post(&quot;/chat/&quot;)
async def chat(request: Request):
session_id = await request.json().get(&#39;session_id&#39;)

# 获取聊天完成服务
_chat_completion_service = kernel.get_service(type=ChatCompletionClientBase)

# 获取聊天完成响应
response = await _chat_completion_service.get_chat_message_content(
chat_history=chat_history,
kernel=kernel,
settings=execution_settings
)

return响应

如何将请求上下文 (session_id) 从 FastAPI 请求传递到 MovesPlugin，并确保它与语义内核执行中的 openai_client 一起正确使用？
如能得到任何指导或建议，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</guid>
      <pubDate>Tue, 26 Nov 2024 12:51:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在 nltk 中下载 punkt tokenizer？</title>
      <link>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</link>
      <description><![CDATA[我使用 pip install nltk 安装了 NLTK 库
pip install nltk

在使用库时
from nltk.tokenize import sent_tokenize 
sent_tokenize(text)

我收到此错误
LookupError: 
**************************************************************************
未找到资源 punkt。
请使用 NLTK 下载器获取资源：

&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.download(&#39;punkt&#39;)

有关更多信息，请参阅：https://www.nltk.org/data.html

尝试加载 tokenizers/punkt/english.pickle

搜索位置：
- &#39;C:\\Users\\adars/nltk_data&#39;
- &#39;C:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python310\\nltk_data&#39;
- &#39;C:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python310\\share\\nltk_data&#39;
- &#39;C:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python310\\lib\\nltk_data&#39;
- &#39;C:\\Users\\adars\\AppData\\Roaming\\nltk_data&#39;
- &#39;C:\\nltk_data&#39;
- &#39;D:\\nltk_data&#39;
- &#39;E:\\nltk_data&#39;
- &#39;&#39;

因此，为了解决此错误，我尝试了
import nltk
nltk.download(&#39;punkt&#39;)

但是我无法下载此包，因为每次运行此包时都会出现错误，提示
[nltk_data] 加载 punkt 时出错：&lt;urlopen 错误 [WinError 10060] A
[nltk_data] 连接尝试失败，因为连接方
[nltk_data] 在一段时间后未正确响应，或者
[nltk_data] 建立连接失败，因为连接的主机
[nltk_data] 未响应&gt;

请帮帮我]]></description>
      <guid>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</guid>
      <pubDate>Tue, 19 Sep 2023 04:36:59 GMT</pubDate>
    </item>
    <item>
      <title>收到错误“未创建保护程序，因为图中没有要恢复的变量”</title>
      <link>https://stackoverflow.com/questions/64182533/getting-the-error-saver-not-created-because-there-are-no-variables-in-the-graph</link>
      <description><![CDATA[我正在为我的项目使用此 tf OpenPose 模型实现。
从 models/graph/mobilenet_thin 目录，我尝试使用 graph.pb 文件，方法是使用
tf.saved_model.load() 函数将其加载到 tensorflow 中，但在加载模型时，我收到此警告：
未创建保存器，因为图中没有要恢复的变量 

因此，我无法在代码中稍后使用 model.predict() 函数。
如何解决此问题？
PS：我只有 .pb文件，而不是 vaiables 文件夹]]></description>
      <guid>https://stackoverflow.com/questions/64182533/getting-the-error-saver-not-created-because-there-are-no-variables-in-the-graph</guid>
      <pubDate>Sat, 03 Oct 2020 09:08:25 GMT</pubDate>
    </item>
    </channel>
</rss>