<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 20 Jan 2024 12:23:53 GMT</lastBuildDate>
    <item>
      <title>当机器学习回归从许多预测中得出几乎相同的数字时该怎么办？</title>
      <link>https://stackoverflow.com/questions/77850915/what-to-do-when-machine-learning-regression-resulting-almost-the-same-number-fro</link>
      <description><![CDATA[我是机器学习新手
我有 200 行数据集 14 个每日传感器读数（第 1 天到第 14 天），因为这是一种罕见的现象，所以我只得到 200 个数据集行。
目标范围（来自数据集列）为 100 至 2500 和 600 标准差，呈正态分布。
回归结果总是接近 1000（这是目标数据的平均值/平均值和中位数）。
我已经尝试过：

（再次）检查并清理数据
使用 gridsearch 寻找最佳模型参数
尝试 3 种不同的模型（AdaBoost、梯度提升、随机森林）
回归结果仍然接近平均值/平均值&amp;目标数据集的中值。
]]></description>
      <guid>https://stackoverflow.com/questions/77850915/what-to-do-when-machine-learning-regression-resulting-almost-the-same-number-fro</guid>
      <pubDate>Sat, 20 Jan 2024 11:59:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow 训练文本生成模型时出现 MemoryError</title>
      <link>https://stackoverflow.com/questions/77850543/memoryerror-when-training-text-generation-model-with-tensorflow</link>
      <description><![CDATA[我在尝试使用 TensorFlow 训练文本生成模型时遇到内存错误。该错误发生在 model.fit 阶段，特别是在尝试创建形状 (401233, 12512) 和数据类型 int32 的数组时。系统无法分配所需的 18.7 GiB 内存。
file_path = &#39;story.txt&#39;
最大读取字节数 = 2 * 1024 * 1024 # 5 MB
​
打开（file_path，&#39;r&#39;，encoding=&#39;utf-8&#39;）作为文件：
    数据 = file.read(max_bytes_to_read)
分词器 = 分词器()
​
语料库 = data.lower().split(“\n”)
​
tokenizer.fit_on_texts（语料库）
​
总单词数 = len(tokenizer.word_index) + 1
​
输入序列 = []
​
对于语料库中的行：
  token_list = tokenizer.texts_to_sequences([行])[0]
  对于范围内的 i(1, len (token_list))：
    n_gram_sequence = token_list[:i+1]
    input_sequences.append(n_gram_sequence)
​
max_seq_len = max([len(i) for i in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences,maxlen= max_seq_len,padding=“pre”))
​
xs = 输入序列[:,:-1]
标签 = input_sequences[:,-1]
fromtensorflow.keras.utils import to_categorical # 专门针对 to_categorical 函数
ys = tf.keras.utils.to_categorical(标签, num_classes=total_words, dtype=&#39;int32&#39;)



模型=顺序（）
model.add(嵌入(total_words, 240, input_length=max_seq_len-1))
model.add(双向(LSTM(150)))
model.add（密集（total_words，激活=&#39;softmax&#39;））
​
​
model.compile(optimizer=“adam”,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
model.fit(xs, ys, epochs=10)
​

这是在 jupyter 笔记本中运行 model.fit 代码单元时出现的错误
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
MemoryError Traceback（最近一次调用）
[32] 中的单元格，第 8 行
      4 model.add（密集（total_words，激活=&#39;softmax&#39;））
      7 model.compile(optimizer=“adam”,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
----&gt; 8 model.fit(xs, ys, epochs=10)

文件 ~\anaconda3\lib\site-packages\keras\src\utils\traceback_utils.py:70，位于filter_traceback..error_handler(*args, **kwargs)
     67、filtered_tb = _process_traceback_frames（e.__traceback__）
     68 # 要获取完整的堆栈跟踪，请调用：
     69 # `tf.debugging.disable_traceback_filtering()`
---&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
     71 最后：
     72 删除filtered_tb

文件~\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py:86，在convert_to_eager_tensor(value, ctx, dtype)中
     66 &quot;&quot;&quot;将给定的“value”转换为“EagerTensor”。
     67
     68 请注意，此函数可以返回已创建常量的缓存副本
   （...）
     80 类型错误：如果 `dtype` 与 t 的类型不兼容。
     第81章
     82 if isinstance(值, np.ndarray):
     83 # 显式创建一个副本，因为 EagerTensor 可能共享底层
     84 # 内存与输入数组。如果没有此副本，用户将能够
     85 # 在创建后通过更改输入数组来修改 EagerTensor。
---&gt; 86 值 = value.copy()
     87 if isinstance(value, ops.EagerTensor):
     88 如果 dtype 不是 None 并且 value.dtype != dtype:

MemoryError：无法为形状为 (401233, 12512) 和数据类型为 int32 的数组分配 18.7 GiB
]]></description>
      <guid>https://stackoverflow.com/questions/77850543/memoryerror-when-training-text-generation-model-with-tensorflow</guid>
      <pubDate>Sat, 20 Jan 2024 10:00:47 GMT</pubDate>
    </item>
    <item>
      <title>a决策树回归和决策树分类器之间的区别</title>
      <link>https://stackoverflow.com/questions/77850372/adifference-between-decision-tree-regression-and-decision-tree-classifier-proper</link>
      <description><![CDATA[当输出变量是分类变量时使用分类树，而当输出变量是连续变量时使用回归树。我知道这是正确的，但我对这个主题还没有清楚的了解，如果您帮助我举例，请帮助我更好地获得最佳答案。]]></description>
      <guid>https://stackoverflow.com/questions/77850372/adifference-between-decision-tree-regression-and-decision-tree-classifier-proper</guid>
      <pubDate>Sat, 20 Jan 2024 09:00:05 GMT</pubDate>
    </item>
    <item>
      <title>在拥抱脸上部署机器学习模型</title>
      <link>https://stackoverflow.com/questions/77850134/deploy-machine-learning-model-on-hugging-face</link>
      <description><![CDATA[任何人都可以帮助我部署在拥抱脸上使用 Streamlit 构建的机器学习模型吗？
我上传了app.py文件、.hdf5模型和requirements.txt文件。我收到构建错误。总是显示找不到 TensorFlow 模块。我包含了我在项目中使用的所有包。
所以我希望有人指导我解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77850134/deploy-machine-learning-model-on-hugging-face</guid>
      <pubDate>Sat, 20 Jan 2024 07:20:44 GMT</pubDate>
    </item>
    <item>
      <title>对于特征工程师来说，有哪些好主意可以为分类模型创建与目标特征的更多相关性？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77848799/what-are-good-ideas-to-feature-engineer-features-to-create-more-correlation-with</link>
      <description><![CDATA[我正在开发一个分类模型，可以对客户进行分类，判断他们是否会成功付款或未能付款。
我尝试了 Scikit Learn 分类模型，但它的准确率略高于 50%。然后，我研究了 TensorFlow 分类模型，并获得了 67% 的准确率分数。
我正在努力提高准确率，希望模型的准确率能够达到 90% 以上。
我注意到的主要问题是没有一个特征与目标特征具有高相关性得分。
这是我尝试过的：
&lt;前&gt;&lt;代码&gt;df5.corr()

结果是

我尝试了多个 TensorFlow 模型，最高准确度得分为 67%
tf.random.set_seed(42)

log_model_8 = tf.keras.Sequential([
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(1)
]）

log_model_8.compile(loss = tf.keras.losses.BinaryCrossentropy(),
                   优化器= tf.keras.optimizers.Adam(),
                   指标 = [&#39;准确性&#39;])

log_model_8.fit(X,y, epochs = 100, verbose = 1)

log_model_8.evaluate(X,y)


在这种情况下有什么好主意，如何才能提高模型准确率达到 90% 以上？]]></description>
      <guid>https://stackoverflow.com/questions/77848799/what-are-good-ideas-to-feature-engineer-features-to-create-more-correlation-with</guid>
      <pubDate>Fri, 19 Jan 2024 21:03:11 GMT</pubDate>
    </item>
    <item>
      <title>如何将字符串转换为浮点数，dtype='numeric' 与字节/字符串数组不兼容。将数据显式转换为数值</title>
      <link>https://stackoverflow.com/questions/77848723/how-to-convert-string-to-float-dtype-numeric-is-not-compatible-with-arrays-of</link>
      <description><![CDATA[将 pandas 导入为 pd
从 sklearn.tree 导入 DecisionTreeClassifier
从sklearn导入预处理

cols = [&#39;国家&#39;, &#39;人口&#39;, &#39;中位数年龄&#39;]
col_types = {&#39;国家&#39;：str，&#39;人口&#39;：int，&#39;median_age&#39;：int}
数据库 = pd.read_csv(&#39;dataset.csv&#39;, dtype=col_types)

le = 预处理.LabelEncoder()
数据库[&#39;国家&#39;] = le.fit_transform(数据库[&#39;国家&#39;])

X = (数据库[[&#39;国家&#39;]])
y = database.drop(列=[&#39;国家&#39;])

模型 = DecisionTreeClassifier()
model.fit(X.值, y.值)
prevision = model.predict([[&#39;意大利&#39;]])
打印（预览）

我正在尝试使用机器学习编写一个程序，该程序以一个国家的名称作为唯一输入，返回中位年龄以及该国家有多少居民。问题是拟合和预测函数需要浮点数，但我的输入是字符串，因此我尝试使用标签编码器对其进行转换，但收到此错误：
dtype=&#39;numeric&#39; 与字节/字符串数组不兼容。
明确地将数据转换为数值

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77848723/how-to-convert-string-to-float-dtype-numeric-is-not-compatible-with-arrays-of</guid>
      <pubDate>Fri, 19 Jan 2024 20:44:26 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 中具有多个层的简单 RNN，用于顺序预测</title>
      <link>https://stackoverflow.com/questions/77848436/simple-rnn-with-more-than-one-layer-in-pytorch-for-squential-prediction</link>
      <description><![CDATA[我得到了连续的时间序列数据。在每个时间戳，只有一个变量可供观察（如果我的理解是正确的，这意味着特征数量 = 1）。我想训练一个具有多个层的简单 RNN 来预测下一个观察结果。
我使用滑动窗口创建了训练数据，窗口大小设置为8。为了给出具体的想法，下面是我的原始数据、训练数据和目标。
示例数据
0.40 0.82 0.14 0.01 0.98 0.53 2.5 0.49 0.53 3.37 0.49
训练数据
&lt;前&gt;&lt;代码&gt;X =
    0.40 0.82 0.14 0.01 0.98 0.53 2.5 0.49
    0.82 0.14 0.01 0.98 0.53 2.5 0.49 0.53
    0.14 0.01 0.98 0.53 2.5 0.49 0.53 3.37


对应的目标是
&lt;前&gt;&lt;代码&gt;Y =
     0.53
     3.37
     0.49

我将批量大小设置为 3。但它给了我一个错误
运行时错误：input.size(-1) 必须等于 input_size。期望 8，得到 1
导入火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
导入 torch.utils.data 作为数据
将 numpy 导入为 np

X = np.array( [ [0.40, 0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49], [0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53], [0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53, 3.37] ], dtype=np.float32)

Y = np.array([[0.53], [3.37], [0.49]], dtype=np.float32)

类 RNNModel(nn.Module):
    def __init__(self, input_sz, n_layers):
        超级（RNNModel，自我）.__init__()
        self.hidden_​​dim = 3*input_sz
        self.n_layers = n_layers
        输出大小 = 1
        self.rnn = nn.RNN（input_sz，self.hidden_​​dim，num_layers = n_layers，batch_first = True）
        self.线性 = nn.Linear(self.hidden_​​dim, output_sz)

    def 前向（自身，x）：
        batch_sz = x.size(0)
        hide = torch.zeros(self.n_layers, batch_sz, self.hidden_​​dim) #初始化n_layer*batch_sz维度的隐藏状态数hidden_​​dim)
        out, 隐藏 = self.rnn(x, 隐藏)
        out = out.contigious().view(-1, self.hidden_​​dim)
        返回，隐藏

device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
模型 = RNNModel(8,2)
X = torch.tensor(X[:,:,np.newaxis])
Y = torch.tensor(Y[:,:,np.newaxis])
X = X.to(设备)
Y = Y.to(设备)
模型 = model.to(设备)
优化器 = optim.Adam(model.parameters())
loss_fn = nn.MSELoss()

加载器= data.DataLoader（data.TensorDataset（X，Y），shuffle=False，batch_size=3）

n_epoch = 10
对于范围内的历元（n_epoch）：
    模型.train()
    对于加载器中的 X_batch、Y_batch：
        Y_pred = 模型(X_batch)
        损失 = loss_fn(Y_pred,Y_batch)
        优化器.zero_grad()
        loss.backward()
        优化器.step()

    如果纪元 % 10 != 0:
        继续
        模型.eval()
        使用 torch.no_grad()：
            Y_pred = 模型(X)
            train_rmse = np.sqrt(loss_fn(Y_pred,Y))
        print(“纪元 %d: 训练 RMSE %.4f” % (纪元, train_rmse))


我做错了什么？谁能帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77848436/simple-rnn-with-more-than-one-layer-in-pytorch-for-squential-prediction</guid>
      <pubDate>Fri, 19 Jan 2024 19:36:58 GMT</pubDate>
    </item>
    <item>
      <title>如果新的交叉验证迭代出现，是否需要重新创建神经网络层？</title>
      <link>https://stackoverflow.com/questions/77847336/whether-should-be-the-neuron-network-layers-recreated-if-the-new-iteration-of-cr</link>
      <description><![CDATA[我有一个关于交叉验证的问题。
k=5，
将有 4/5 训练数据集和 1/5 验证数据集
 data = np.concatenate([self.training_data1, self.training_data2], axis=1)
        
        kf = KFold(n_splits=k, shuffle=True) # k 折叠交叉验证
        kf.get_n_splits(data) # 返回交叉验证器中的分割迭代次数
     
        损失CV = 0
        val_loss_cv = 0
    
        对于 kf.split(data) 中的 train_index、val_index：
            
            logging.info(f“train_index:{train_index.shape}”)
            logging.info(f&quot;val_index:{val_index.shape}&quot;)

            self.is_train = tf.Variable(initial_value=True, trainable=False, dtype=tf.bool, name=“is_train”)
   
            loss_cv, val_loss_cv = self.train(train_index, val_index)
            loss_cv += loss_cv
            val_loss_cv += val_loss_cv

loss_cv = loss_cv / k
val_loss_cv = val_loss_cv / k

每次迭代都有新的train_index、val_index。（例如在train_index0、val_index0之后，下一次迭代将从train_index1、val_index1开始）
这些数据集将被加载到函数 self.train(train_index, val_index) 中。
在 train() 函数中，有一个使用神经元网络层创建的自动编码器层。
当新的交叉验证索引（train_index1，val_index1）在新的迭代中出现时，是否应该使用新的初始权重和偏差重新创建新的神经元网络层？
如果我在神经元网络层中使用继承的权重和偏差，而不是创建新的神经元网络层，结果是否会导致过拟合？]]></description>
      <guid>https://stackoverflow.com/questions/77847336/whether-should-be-the-neuron-network-layers-recreated-if-the-new-iteration-of-cr</guid>
      <pubDate>Fri, 19 Jan 2024 15:56:07 GMT</pubDate>
    </item>
    <item>
      <title>Xgboost算法问题文件为空</title>
      <link>https://stackoverflow.com/questions/77843515/xgboost-algorithm-issue-file-empty</link>
      <description><![CDATA[我尝试使用 1.7-1 版本的 Xgboost 算法训练数据集。调用 Xgboost 函数时，它会抛出如下错误。
2024-01-19:02:57:27:INFO] 导入框架 sagemaker_xgboost_container.training
[2024-01-19:02:57:27:INFO] 未检测到 GPU（如果未安装 GPU，则正常）
[2024-01-19:02:57:27:INFO] 调用用户培训脚本。
[2024-01-19:02:57:27:错误] 报告培训失败
[2024-01-19:02:57:27:ERROR] 框架错误：
回溯（最近一次调用最后一次）：
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 2318 行，下一个
    tarinfo = self.tarinfo.fromtarfile(self)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1105 行，fromtarfile
    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1041 行，frombuf 中
    raise EmptyHeaderError(“空标题”)
tarfile.EmptyHeaderError：空标头
在处理上述异常的过程中，又出现了一个异常：
回溯（最近一次调用最后一次）：
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_trainer.py”，第 84 行，列车中
    入口点（）
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py”，第 102 行，在 main 中
    火车（框架.training_env（））
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py”，第 87 行，训练中
    框架.模块.run_module(
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_modules.py”，第 290 行，在 run_module 中
    _files.download_and_extract(uri, _env.code_dir)
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_files.py”，第 131 行，位于 download_and_extract 中
    使用 tarfile.open(name=dst, mode=“r:gz”) 作为 t：
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1621 行，打开
    返回 func(名称、文件模式、fileobj、**kwargs)
  gzopen 中的文件“/miniconda3/lib/python3.8/tarfile.py”，第 1674 行
    t = cls.taropen(名称、模式、fileobj、**kwargs)
  taropen 中的文件“/miniconda3/lib/python3.8/tarfile.py”，第 1651 行
    返回 cls(名称、模式、fileobj、**kwargs)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1514 行，位于 __init__ 中
    self.firstmember = self.next()
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 2333 行，在下一个
    引发 ReadError(“空文件”)
tarfile.ReadError：空文件
空的文件

我有两个具有相同结构且扩展名为 .csv 的源文件。
我不知道为什么它抱怨 tar 文件为空]]></description>
      <guid>https://stackoverflow.com/questions/77843515/xgboost-algorithm-issue-file-empty</guid>
      <pubDate>Fri, 19 Jan 2024 03:10:14 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：给定 groups=1，权重大小为 [128, 64, 4, 4]，预期输入 [1, 128, 65, 65] 有 64 个通道，但得到了 128 个通道</title>
      <link>https://stackoverflow.com/questions/77843263/runtimeerror-given-groups-1-weight-of-size-128-64-4-4-expected-input1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77843263/runtimeerror-given-groups-1-weight-of-size-128-64-4-4-expected-input1</guid>
      <pubDate>Fri, 19 Jan 2024 01:31:36 GMT</pubDate>
    </item>
    <item>
      <title>不平衡数据的隔离森林和SHAP过程</title>
      <link>https://stackoverflow.com/questions/77837871/isolation-forest-and-shap-process-for-imbalanced-data</link>
      <description><![CDATA[我想对类别为正常 99.93% 异常 0.07% 的数据使用隔离森林，并使用 SHAP 检查异常数据特征之间的相关性。
于是，我参考了下面kaggle网站上的方法继续学习。
kaggle
在这个 Kaggle 站点上，Class = 0 的数据和 Class = 1 的数据划分如下：
inliers = df[df.Class==0]
ins = inliers.drop([&#39;Class&#39;], axis=1)

离群值 = df[df.Class==1]
outs = outliers.drop([&#39;Class&#39;], axis=1)

为了查看学习中使用的特征与异常值（“Class == 1”的数据）之间的相关性，我按如下方式使用了 SHAP，并通过蜂群图检查了相关性。
&lt;前&gt;&lt;代码&gt;状态= 42
ISF = 隔离森林（random_state=状态）
ISF.fit(ins)

normal_isf = ISF.predict(ins)
欺诈_isf = ISF.predict(outs)

导入形状
解释器 = shap.TreeExplainer(ISF)
shap_values = 解释器(outs)
shap.plots.beeswarm(shap_values)

代码工作正常，但是 beeswarn 的结果与我使用 shap_values ​​=explainer(ins) 时类似，即正常数据。我犯错了吗？如果您能告诉我是否有任何需要改进的地方，我将非常感激。]]></description>
      <guid>https://stackoverflow.com/questions/77837871/isolation-forest-and-shap-process-for-imbalanced-data</guid>
      <pubDate>Thu, 18 Jan 2024 08:20:12 GMT</pubDate>
    </item>
    <item>
      <title>在小型训练数据集上训练的文本转语音模型</title>
      <link>https://stackoverflow.com/questions/77406851/text-to-speech-model-that-trains-on-small-training-dataset</link>
      <description><![CDATA[我需要一个模型，可以使用包含转录本和最多 20 个句子的 wav 文件的数据集进行训练。
我尝试在这样的情况下训练 https://github.com/coqui-ai/TTS数据集，它根本没有训练得很好。这个推论只是噪音而不是文字。
我正在研究 https ://github.com/microsoft/SpeechT5/tree/main/SpeechLM#pre-trained-and-fine-tuned-models 但他们使用的微调数据集似乎也有超过 100 小时的音频内容。
解决这个问题的最佳研究模型是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77406851/text-to-speech-model-that-trains-on-small-training-dataset</guid>
      <pubDate>Thu, 02 Nov 2023 04:00:19 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost：如何使用带有 scikit-learn 接口 .fit 的 DMatrix</title>
      <link>https://stackoverflow.com/questions/76502318/xgboost-how-to-use-a-dmatrix-with-scikit-learn-interface-fit</link>
      <description><![CDATA[我目前在我的项目中使用 XGBoost 的 scikit-learn 接口。但是，我有一个非常大的数据集，每次调用 .fit 时，数据都会转换为 DMatrix，这非常耗时，尤其是在使用训练相对较快的 GPU 时。我使用本机接口对每次拟合使用单个 DMatrix 进行基准测试，结果显示出显着差异（每次拟合 14 秒与每次拟合 0.9 秒）。问题是，我需要一个 scikit-learn 模型，以便它可以与我的程序的其余部分配合使用。
有没有办法在 XGBoost 中将 DMatrix 与 scikit-learn 接口结合使用，或者有任何解决方法来避免重复转换为 DMatrix，同时仍保持与 scikit-learn 的兼容性？
请参阅下面的代码以获取导致此问题的可重现方法。
from sklearn.datasets import make_classification
从 xgboost 导入 XGBClassifier
将 xgboost 导入为 xgb

# 大型综合数据集
X, y = make_classification(n_samples=500_0000, n_features=20,
                           n_informative=10，n_redundant=10，random_state=42）

# scikit 学习
t = 时间.time()
模型 = XGBClassifier(tree_method=“gpu_hist”, gpu_id=0,
                      预测器=“gpu_predictor”，max_bin=256）
模型.fit(X, y)
print(&quot;scikit-learn 接口：&quot;, time.time() - t)

# 再次进行 scikit-learn
t = 时间.time()
模型.fit(X, y)
print(&quot;scikit-learn (2nd) 接口：&quot;, time.time() - t)

打印（）

#DMatrix
dtrain = xgb.DMatrix(数据=X,标签=y)
t = 时间.time()
model = xgb.train({“tree_method”: “gpu_hist”, “gpu_id”: 0,
                  “预测器”：“gpu_predictor”}，dtrain）
print(&quot;本机接口：&quot;, time.time() - t)

#再次DMatrix
t = 时间.time()
model = xgb.train({“tree_method”: “gpu_hist”, “gpu_id”: 0,
                  “预测器”：“gpu_predictor”}，dtrain）
print(“本机（第二）接口::”, time.time() - t)

输出：
scikit-learn 接口：14.393212795257568
scikit-learn（第二）接口：14.048950433731079

本机接口：3.9494242668151855
本机（第二）接口:: 0.9888997077941895

如您所见，scikit-learn 和 Native 之间存在很大的时间差异。]]></description>
      <guid>https://stackoverflow.com/questions/76502318/xgboost-how-to-use-a-dmatrix-with-scikit-learn-interface-fit</guid>
      <pubDate>Sun, 18 Jun 2023 19:56:25 GMT</pubDate>
    </item>
    <item>
      <title>卡方检验的计算</title>
      <link>https://stackoverflow.com/questions/64271948/computation-of-chi-square-test</link>
      <description><![CDATA[我试图了解如何针对以下输入计算 chi2 函数。
sklearn.feature_selection.chi2([[1, 2, 0, 0, 1],
                                [0, 0, 1, 0, 0],
                                [0, 0, 0, 2, 1]], [真, 假, 假])

对于 chi2，我得到以下结果 [2, 4, 0.5, 1, 0.25]。
我已经在维基百科上找到了以下计算公式（x_i 也称为观察值，m_i 称为预期值），但我不知道如何应用它。

我的理解是，我有三个类别的输入（行）和四个特征（列），chi2函数返回特征和类别之间是否存在相关性。第一列表示的特征在第一个类别中出现两次，并且 chi2 值为 4。
我想我已经弄清楚了

各列彼此独立，这是有道理的
如果我省略第三行，预期值将是列的总和，观察值只是相应单元格中的值，但这不适用于最后一列
带有 False 的两列似乎以某种方式组合在一起，但我还没有弄清楚如何组合。

如果有人可以帮助我，我将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/64271948/computation-of-chi-square-test</guid>
      <pubDate>Thu, 08 Oct 2020 23:27:51 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中处理 nan/null 的分类器</title>
      <link>https://stackoverflow.com/questions/30317119/classifiers-in-scikit-learn-that-handle-nan-null</link>
      <description><![CDATA[我想知道 scikit-learn 中是否有处理 nan/null 值的分类器。我认为随机森林回归器可以处理这个问题，但当我调用 predict 时出现错误。
X_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])
y_train = np.array([1, 2])
clf = RandomForestRegressor(X_train, y_train)
X_test = np.array([7, 8, np.nan])
y_pred = clf.predict(X_test) # 失败！

我不能使用任何带有缺失值的 scikit-learn 算法来调用预测吗？
编辑。
现在我想起来，这是有道理的。这在训练期间不是问题，但是当您预测变量为空时如何分支时？也许你可以将两种方式分开并平均结果？看来只要距离函数忽略空值，k-NN 就应该可以正常工作。
编辑 2（年长且聪明的我）
一些 gbm 库（例如 xgboost）使用三叉树而不是二叉树正是为了这个目的：2 个子节点用于是/否决策，1 个子节点用于缺失决策。 sklearn 使用二叉树&lt; /a&gt;]]></description>
      <guid>https://stackoverflow.com/questions/30317119/classifiers-in-scikit-learn-that-handle-nan-null</guid>
      <pubDate>Tue, 19 May 2015 05:02:35 GMT</pubDate>
    </item>
    </channel>
</rss>