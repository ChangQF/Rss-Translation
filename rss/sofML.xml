<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 14 Jan 2025 15:17:09 GMT</lastBuildDate>
    <item>
      <title>加载 MIT-BIH 心律失常数据库（无需 wfdb 包）</title>
      <link>https://stackoverflow.com/questions/79355429/load-mit-bih-arrhythmia-database-without-wfdb-package</link>
      <description><![CDATA[我打算使用这个数据集，但无论我如何努力搜索，我都找不到文件中数据的格式或结构。我发现的最重要的事情是每隔两个字节就有一个 h33。我在哪里可以找到有关此内容的文档？我找不到注释与这些数据文件的关系。]]></description>
      <guid>https://stackoverflow.com/questions/79355429/load-mit-bih-arrhythmia-database-without-wfdb-package</guid>
      <pubDate>Tue, 14 Jan 2025 15:00:07 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何工具可以将任何其他数据集格式转换为 sam2 格式以进行微调？[关闭]</title>
      <link>https://stackoverflow.com/questions/79354345/are-there-any-tools-to-convert-from-any-other-dataset-format-to-sam2-format-for</link>
      <description><![CDATA[我在 cvat 中有一个标记的数据集，但 cvat 不支持将数据卸载为 SAM2 格式。有没有可用的自动转换工具？
我可以使用 roboflow，但我有无法上传到第三方资源的数据]]></description>
      <guid>https://stackoverflow.com/questions/79354345/are-there-any-tools-to-convert-from-any-other-dataset-format-to-sam2-format-for</guid>
      <pubDate>Tue, 14 Jan 2025 08:14:24 GMT</pubDate>
    </item>
    <item>
      <title>stable_baselines3：为什么比较 ep_info_buffer 与评估时奖励不匹配？</title>
      <link>https://stackoverflow.com/questions/79353843/stable-baselines3-why-the-reward-does-not-match-comparing-ep-info-buffer-vs-eva</link>
      <description><![CDATA[我正在使用 stable_baselines3 库，这时我发现了一些意想不到的东西。
这里有一个简单的代码来重现这个问题：
import gymnasium as gym

from stable_baselines3 import DQN

env = gym.make(&quot;CartPole-v1&quot;)

model = DQN(&quot;MlpPolicy&quot;, env, verbose=0, stats_window_size=100_000)
model.learn(total_timesteps=100_000)

看看最后一集的奖励：
print(model.ep_info_buffer[-1])


{&#39;r&#39;: 409.0, &#39;l&#39;: 409, &#39;t&#39;: 54.87983

但是如果我使用以下代码评估模型：
obs, info = env.reset()
total_reward = 0
while True:
action, _states = model.predict(obs, deterministic=True)
obs, reward, termed, truncated, info = env.step(action)
total_reward = total_reward + reward
if termed or truncated:
obs, info = env.reset()
break

print(&quot;total_reward {}&quot;.format(total_reward))


total_reward 196.0

我得到了不同的奖励，这是我没有预料到的。
我预计会得到与 409 相同的奖励model.ep_info_buffer[-1]。
为什么会有这种差异？.ep_info_buffer 与每集奖励不同吗？]]></description>
      <guid>https://stackoverflow.com/questions/79353843/stable-baselines3-why-the-reward-does-not-match-comparing-ep-info-buffer-vs-eva</guid>
      <pubDate>Tue, 14 Jan 2025 02:14:32 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练我的音频分类模型时出现错误 as_list()</title>
      <link>https://stackoverflow.com/questions/79353105/error-as-list-when-trying-to-train-my-audio-classification-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79353105/error-as-list-when-trying-to-train-my-audio-classification-model</guid>
      <pubDate>Mon, 13 Jan 2025 18:12:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python 为 UNO 纸牌游戏创建 AI 玩家？[关闭]</title>
      <link>https://stackoverflow.com/questions/79352641/how-do-i-create-an-ai-player-for-uno-card-game-in-python</link>
      <description><![CDATA[我熟悉 Python 的基础知识，我想创建一个 UNO 纸牌游戏，让 AI 与用户竞争。我唯一的经验是使用 Python 中的极小极大算法制作井字游戏机器人。
我读过，极小极大算法是不可行的，因为 UNO 有更大的移动范围。我读过一些关于蒙特卡洛树搜索、深度强化学习、神经网络等的文章。对于 1v1（AI 与玩家）的 UNO 游戏，哪种方法更容易实现？
我读过的一点资料建议在纸牌游戏中使用蒙特卡洛算法。我想知道这里的情况是否如此，这是否确实是最佳方法。]]></description>
      <guid>https://stackoverflow.com/questions/79352641/how-do-i-create-an-ai-player-for-uno-card-game-in-python</guid>
      <pubDate>Mon, 13 Jan 2025 15:15:46 GMT</pubDate>
    </item>
    <item>
      <title>如何在 pytorch 中并行计算不同权重和输入的神经网络？</title>
      <link>https://stackoverflow.com/questions/79350425/how-to-calculate-neural-net-for-different-weights-and-inputs-in-parallel-in-pyto</link>
      <description><![CDATA[我正在使用 pytorch 进行机器学习。
有一些实现神经网络的类继承自 nn.Module。实现了一些网络结构。
列表中存储了不同的参数（权重、偏差）：
parameters = [[...], ..., [...]] 。另一个列表中还存储了不同的输入：inputs = [[...], ..., [...]]。所有输入集的大小和维度都相同。
我需要计算输入和参数的网络输出，即我需要构建一个矩阵：




参数[0]
参数[1]
...
参数[n]




输入[0]
结果[0][0]
结果[0][1]
...
结果[0][n]


输入[1]
结果[ 1][0]
result[1][1]
...
result[1][n]


...
...
...
...
...


input[m]
result[m][0]
result[m][1]
...
result[m][n]



最明显的方法是使用两个循环。这对 CPU 来说很好。但如何为 GPU 实现它？每个 result[i][j] 都可以并行计算，所以我会使用某种批量计算。
你能给我一些解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/79350425/how-to-calculate-neural-net-for-different-weights-and-inputs-in-parallel-in-pyto</guid>
      <pubDate>Sun, 12 Jan 2025 18:00:35 GMT</pubDate>
    </item>
    <item>
      <title>使用矩阵在 PyTorch 上计算公式</title>
      <link>https://stackoverflow.com/questions/79350403/calculate-formulas-on-pytorch-using-matrix</link>
      <description><![CDATA[我有方程式：
$e_{ij} = \frac{X_i W^Q (X_j W^K + A^K_{ij}) }{\sqrt{D_z}}$
$\alpha_{ij} = softmax(e_{ij})$
$z_{i} = \sum_j \alpha_{ij} (X_j W^V + A^V_{ij})$

其中大小：
X：[B，S，H，D]
每个 W：[H，D，D]
每个 A：[S，S，H，D]

我如何通过矩阵运算计算它？
我有一个部分解决方案
import torch
import torch.nn. functional as F

B, S, H, D = X.shape
d_z = D # 为简单起见，假设 d_z 等于 D

W_Q = torch.randn(H, D, D)
W_K = torch.randn(H, D, D)
W_V = torch.randn(H, D, D)

a_K = torch.randn(S, S, H, D)
a_V = torch.randn(S, S, H, D)
}
XW_Q = torch.einsum(&#39;bshd,hde-&gt;bshe&#39;, X, W_Q) # [B, S, H, D] @ [H, D, D] -&gt; [B，S，H，D]
XW_K = torch.einsum(&#39;bshd,hde-&gt;bshe&#39;, X, W_K) # [B，S，H，D] @ [H，D，D] -&gt; [B，S，H，D]

e_ij_numerator = XW_Q.unsqueeze(2) @ (XW_K.unsqueeze(1) + a_K).transpose(-1, -2) # [B，S，1，H，D] @ [B，1，S，H，D] -&gt; [B，S，S，H，D]
e_ij = e_ij_numerator / torch.sqrt(torch.tensor(d_z, dtype=torch.float32)) # [B，S，S，H，D]

XW_V = torch.einsum(&#39;bshd,hde-&gt;bshe&#39;, X, W_V) # [B，S，H，D] @ [H，D，D] -&gt; [B，S，H，D]
alpha = F.softmax(e_ij, dim=2) # [B，S，S，H，D]

z_i = torch.einsum(&#39;bshij,bshjd-&gt;bshid&#39;, alpha, XW_V.unsqueeze(1) + a_V) # [B，S，S，H，D] @ [B，1，S，H，D] -&gt; [B, S, S, H, D]

但 z 应该是 [B, S, H,D]]]></description>
      <guid>https://stackoverflow.com/questions/79350403/calculate-formulas-on-pytorch-using-matrix</guid>
      <pubDate>Sun, 12 Jan 2025 17:48:23 GMT</pubDate>
    </item>
    <item>
      <title>如何在 GPU 中实现 KNNImputer？</title>
      <link>https://stackoverflow.com/questions/79350213/how-to-implement-knnimputer-in-gpu</link>
      <description><![CDATA[我正在处理 Kaggle 上的一个大型数据集，并希望通过使用 GPU 加速进行 KNN 插补来加快插补过程。我目前的方法使用 sklearn 的基于 CPU 的 KNNImputer，但它的速度太慢了，无法满足我的需求。
我听说 RAPIDS cuML 提供 GPU 加速的 KNN 插补。这是我到目前为止尝试的代码
import pandas as pd
import cudf
from cuml.experimental.preprocessing import KNNImputer

# 将 Pandas DataFrame 转换为 cuDF DataFrame
df_bad_cleaned_gpu = cudf.DataFrame.from_pandas(df_bad_cleaned)

# 使用邻居初始化 KNN imputer
knn_imputer_gpu = KNNImputer(n_neighbors=36)

# 拟合和转换
df_bad_knn_filled_gpu = knn_imputer_gpu.fit_transform(df_bad_cleaned_gpu)

# 转换回 Pandas DataFrame（如果需要）
df_bad_knn_filled = df_bad_knn_filled_gpu.to_pandas()

这是使用 RAPIDS 在 GPU 上实现 KNN 插补的正确方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79350213/how-to-implement-knnimputer-in-gpu</guid>
      <pubDate>Sun, 12 Jan 2025 16:00:11 GMT</pubDate>
    </item>
    <item>
      <title>DMIR 的 XMorpher 模型 - 数据集问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/79180321/xmorpher-model-for-dmir-dataset-problem</link>
      <description><![CDATA[我正在尝试运行 GitHub 项目 XMorpher (https://github.com/Solemoon/XMorpher/tree/main)，但作者没有提供正确的数据集？他在代码中使用了：
 train_labeled_unlabeled_dir = &#39;data/train_labeled_unlabeled&#39;
train_unlabeled_unlabeled_dir = &#39;data/train_unlabeled_unlabeled&#39;
test_labeled_labeled_dir = &#39;data/test&#39;

但实际上数据文件夹仅包含 0_1.mat 文件，在 Matlab 中打开时，该文件包含 2 个变量：
fix_img # 大小：144x144x128 (int16)
mov_img # 大小：144x144x128 (int16)

由于一张图片包含 5308416 个字节，我无法看到变量的值... 还有其他方法可以运行此模型或我可以使用其他数据集吗？]]></description>
      <guid>https://stackoverflow.com/questions/79180321/xmorpher-model-for-dmir-dataset-problem</guid>
      <pubDate>Tue, 12 Nov 2024 08:25:50 GMT</pubDate>
    </item>
    <item>
      <title>Vertex AI：Automl-tabular 模板不断给我一个错误</title>
      <link>https://stackoverflow.com/questions/79177501/vertex-ai-automl-tabular-template-keeps-giving-me-an-error</link>
      <description><![CDATA[我正在尝试使用 Google 的 AutoML 产品 (VertexAI) 构建机器学习模型。
我已成功上传我的数据集 - 见下图。

但是，当我尝试使用 AutoML 模板为表格回归创建管道运行时，管道失败。我将在 VertexAI 上展示步骤，我只是使用默认设置而不进行任何更改：





我运行的第一个管道失败了。


我将调试 json 粘贴到 ChatGPT 中。它告诉我尝试将机器类型从 n1-standard-8 或 n1-highmem-8 更改为 n1-standard-4。我试过了，但管道仍然失败。我还确保计算服务已启用正确的设置。
]]></description>
      <guid>https://stackoverflow.com/questions/79177501/vertex-ai-automl-tabular-template-keeps-giving-me-an-error</guid>
      <pubDate>Mon, 11 Nov 2024 11:41:07 GMT</pubDate>
    </item>
    <item>
      <title>删除边界框detectron2之外的所有内容</title>
      <link>https://stackoverflow.com/questions/78626157/delete-everything-outside-of-bounding-boxes-detectron2</link>
      <description><![CDATA[我已经训练了一个包含一个类的 detectron2 模型。现在我想将不在 bbox 内的所有内容设置为白色，其余部分保持原样。一张图片上可以有多个 bbox，它们可以叠加。
我阅读了 detectron2 的文档以及 cv2，但我找不到解决问题的方法。
这是我的预测代码：
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
import cv2

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(&#39;COCO-Detection/tmp&#39;))
cfg.MODEL.WEIGHTS = &#39;tmp&#39;

预测器 = DefaultPredictor(cfg)

img = cv2.imread(&#39;tmp&#39;)

out = 预测器(img)
]]></description>
      <guid>https://stackoverflow.com/questions/78626157/delete-everything-outside-of-bounding-boxes-detectron2</guid>
      <pubDate>Sat, 15 Jun 2024 09:09:50 GMT</pubDate>
    </item>
    <item>
      <title>使用带有 max_new_tokens 的 LLM 进行不完整输出</title>
      <link>https://stackoverflow.com/questions/77061898/incomplete-output-with-llm-with-max-new-tokens</link>
      <description><![CDATA[我正在试验 Huggingface LLM 模型。
我注意到的一个问题是模型的输出突然结束，我理想情况下希望它完成它所在的段落/句子/代码。（或者尝试在某个固定数量的标记内完成答案）
虽然我提供了 max_new_tokens = 300，并且在提示中我写道：
“输出最多应为 300 个字。”
响应总是不完整的，并且突然结束。有什么方法可以要求在所需的输出标记数内完成输出？
代码：
checkpoint = “HuggingFaceH4/starchat-alpha”
device = “cuda” if torch.cuda.is_available() else “cpu”
class StarCoderModel:
def __init__(self):
self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)
# 如果需要 gpu，请确保在 docker run 命令中提供 `--gpus all`
self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=&#39;auto&#39;)

def infer(self, input_text, token_count):
输入 = self.tokenizer.encode(input_text, return_tensors=&quot;pt&quot;).to(device)
输出 = self.model.generate(inputs, max_new_tokens=token_count, pad_token_id=self.tokenizer.eos_token_id)
return self.tokenizer.decode(outputs[0])[len(input_text):]

示例输出：
private DataType FuntionName(String someId) {
// TODO：用利用 someId 获取信息的实现替换
return DataType.Value;
}

注释：

- 如果代码中存在 someId，则使用客户端的 getAPI 并以 someId 作为参数来获取一些信息。
- 如果

]]></description>
      <guid>https://stackoverflow.com/questions/77061898/incomplete-output-with-llm-with-max-new-tokens</guid>
      <pubDate>Thu, 07 Sep 2023 18:02:00 GMT</pubDate>
    </item>
    <item>
      <title>如何实现巴特沃斯滤波器</title>
      <link>https://stackoverflow.com/questions/74003337/how-to-implement-a-butterworth-filter</link>
      <description><![CDATA[我正在尝试使用 python 实现 butterworthfilter
数据来自 CSV 文件，名为 Samples.csv，如下所示
998,4778415
1009,209592
1006,619094
1001,785406
993,9426543
990,1408991
992,736118
995,8127334
...

该列调用欧几里得范数。数据范围从 0 到 1679.286158，共有 1838 行。
这是我使用的代码：
from scipy.signal import filtfilt
from scipy import stats

import csv
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import scipy
def plot():
data=pd.read_csv(&#39;Samples.csv&#39;,sep=&quot;;&quot;, decimal=&quot;,&quot;)
sensor_data=data[[&#39;Euclidian Norm&#39;]]
sensor_data=np.array(sensor_data)

time=np.linspace(0,1679.286158,1838)
plt.plot(time,sensor_data)
plt.show()

filtered_signal=bandPassFilter(sensor_data)
plt.plot(time,sensor_data)
plt.show()

def bandPassFilter(signal):
fs = 4000.0
lowcut=20.0
highcut=50.0

nyq=0.5*fs
low=lowcut/nyq
high=highcut/nyq

order =2

b,a=scipy.signal.butter(order,[low,high],&#39;bandpass&#39;,analog=False)

y=scipy.signal.filtfilt(b,a,signal,axis=0)

return(y)

plot()


我的问题是我的数据没有任何变化。它没有过滤我的数据。过滤数据的图表与源数据相同。有人知道哪里出了问题吗？
第一个图表是源数据，第二个图表是过滤后的图表，在我看来，它们看起来像是同一张图表。
]]></description>
      <guid>https://stackoverflow.com/questions/74003337/how-to-implement-a-butterworth-filter</guid>
      <pubDate>Sun, 09 Oct 2022 08:49:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们不应该在同一层使用多个激活函数？[关闭]</title>
      <link>https://stackoverflow.com/questions/63125782/why-shouldnt-we-use-multiple-activation-functions-in-the-same-layer</link>
      <description><![CDATA[我见过的所有应用神经网络的示例或案例都有一个共同点 - 它们在属于特定层的所有节点中使用特定类型的激活函数。
据我所知，每个节点都使用非线性激活函数来了解数据中的特定模式。如果是这样，为什么不使用多种类型的激活函数？
我确实找到了一个链接，它基本上说如果我们每层只使用一个激活函数，管理网络会更容易。还有其他好处吗？]]></description>
      <guid>https://stackoverflow.com/questions/63125782/why-shouldnt-we-use-multiple-activation-functions-in-the-same-layer</guid>
      <pubDate>Tue, 28 Jul 2020 01:28:55 GMT</pubDate>
    </item>
    <item>
      <title>机器学习还是决策树用于工作匹配？</title>
      <link>https://stackoverflow.com/questions/43319120/machinelearning-or-decisiontree-for-job-matching</link>
      <description><![CDATA[我正在开发一个工作匹配应用程序，我想知道在元素之间进行匹配以获得最佳结果的最佳方法是什么？
在我看来，这是通过决策树，因为我们已经知道元素的结构和预期结果。
但是，机器学习会是一种替代解决方案吗？或者这样做毫无价值？
我可能错了，但对我来说，机器学习对于对乍一看没有明显共同点的数据进行排序是有效的，对吗？
谢谢你的建议！]]></description>
      <guid>https://stackoverflow.com/questions/43319120/machinelearning-or-decisiontree-for-job-matching</guid>
      <pubDate>Mon, 10 Apr 2017 09:10:48 GMT</pubDate>
    </item>
    </channel>
</rss>