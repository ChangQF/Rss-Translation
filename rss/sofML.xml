<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 24 Dec 2023 03:14:38 GMT</lastBuildDate>
    <item>
      <title>如何将 model.safetensor 转换为 pytorch_model.bin？</title>
      <link>https://stackoverflow.com/questions/77708996/how-to-convert-model-safetensor-to-pytorch-model-bin</link>
      <description><![CDATA[我正在微调预训练的 bert 模型，但遇到了一个奇怪的问题：
当我使用 CPU 进行微调时，代码会像这样保存模型：

使用“pytorch_model.bin”。但是当我使用 CUDA（我必须这样做）时，模型会像这样保存：

当我尝试加载这个“model.safetensors”时将来，它会引发错误“pytorch_model.bin”未找到。我使用两个不同的 venv 来测试 CPU 和 CUDA。
如何解决这个问题？是版本问题吗？
我正在使用sentence_transformers框架来微调模型。
这是我的训练代码：
检查点 = &#39;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&#39;

word_embedding_model = models.Transformer(checkpoint,cache_dir=f&#39;model/{checkpoint}&#39;)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=&#39;mean&#39;)
模型 = SentenceTransformer(模块=[word_embedding_model, pooling_model], device=&#39;cuda&#39;)


train_loss = 损失.CosineSimilarityLoss(模型)

evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(val_examples, name=&#39;sbert&#39;)

model.fit(train_objectives=[(train_dataloader, train_loss)]，epochs=5，evaluator=evaluator，show_progress_bar=True，output_path=f&#39;model_FT/{checkpoint}&#39;，save_best_model=True)

我确实在两个不同的环境中尝试了测试，我希望代码能够保存一个“pytorch_model.bin”文件。不是“model.safetensors”。
编辑：我真的还不知道，但似乎是新版本的 Transformer 库导致了这个问题。我发现使用拥抱脸可以加载安全张量，但使用句子转换器（我需要使用）则不能。]]></description>
      <guid>https://stackoverflow.com/questions/77708996/how-to-convert-model-safetensor-to-pytorch-model-bin</guid>
      <pubDate>Sat, 23 Dec 2023 20:43:20 GMT</pubDate>
    </item>
    <item>
      <title>什么类型的 ML 模型可以检测过渡对中图像的哪一部分发生了变化？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77708727/what-type-of-ml-model-can-detect-which-part-of-the-image-has-changed-in-a-transi</link>
      <description><![CDATA[假设我有一个图像对的数据集，其中每一对代表给定问题中的一个序列
例如，下面的一对图像代表 8-MNIST 拼图问题中的一个步骤（其中代表空白的“0”图块与“3”图块一起移动）。

对于每一对，我还有一个表示给定动作的 one-hot 向量，例如对于上面的对，one-hot 将是 [0 0 0 ... 0 1 0] 表示“” 0”瓷砖被推倒了
是否有一个模型可以从给定的图像和动作的标签中学习，以预测图像的哪一部分将被修改？ （给定示例中图像的“0 3”部分）。
非常感谢]]></description>
      <guid>https://stackoverflow.com/questions/77708727/what-type-of-ml-model-can-detect-which-part-of-the-image-has-changed-in-a-transi</guid>
      <pubDate>Sat, 23 Dec 2023 19:04:39 GMT</pubDate>
    </item>
    <item>
      <title>从口腔内扫描中检测点[关闭]</title>
      <link>https://stackoverflow.com/questions/77708692/detect-points-from-an-intraoral-scan</link>
      <description><![CDATA[我们需要从口腔内扫描中检测点，但我们不知道如何开始或如何注释图像以训练模型来检测看不见的图像上的点。有人可以指导我们吗？
我们尝试了姿势检测和不同的代码进行训练，但数据不准确。]]></description>
      <guid>https://stackoverflow.com/questions/77708692/detect-points-from-an-intraoral-scan</guid>
      <pubDate>Sat, 23 Dec 2023 18:49:39 GMT</pubDate>
    </item>
    <item>
      <title>无论输入图像如何，具有 TensorFlow Lite 模型的 Flask API 始终预测相同的类别</title>
      <link>https://stackoverflow.com/questions/77708650/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</link>
      <description><![CDATA[我正在开发 Flask API，以使用 TensorFlow Lite 模型执行推理，该模型是在阿尔茨海默病 5 类图像数据集上训练的，这些图像是 [“AD - 阿尔茨海默病”、“CN - 认知正常”、“EMCI - 早期轻度”认知障碍”、“LMCI - 晚期轻度认知障碍”、“MCI - 轻度认知障碍”]。该模型在我的训练环境中运行良好，但当我将其部署到 Flask API 中时，出现了问题。 API 一致地为每个图像预测相同的类别（“CN - 认知正常”），而在我的 Colab 笔记本中训练的模型则准确地预测各种类别。该 API 稍后将与 React Native App 集成。
TFLite 模型的代码：https://colab.research.google。 com/drive/1xxW8v5ZBKLvlGrofL2fBy9WYk_Fn5Dj_?usp=sharing
FlaskAPI 代码：
fromflask导入Flask，request，jsonify
将张量流导入为 tf
导入CV2
将 numpy 导入为 np
从 PIL 导入图像
导入io

应用程序=烧瓶（__名称__）


解释器 = tf.lite.Interpreter(model_path=&quot;updated_final_model.tflite&quot;)
解释器.allocate_tensors()


class_names = [“CN-认知正常”、“AD-阿尔茨海默病”、“EMCI-早期轻度认知障碍”、“MCI-轻度认知障碍”、“LMCI-晚期轻度认知障碍”]

def preprocess_image(图像):
图像 = image.astype(&#39;float32&#39;) / 255.0
图像 = cv2.resize(图像, (150, 150))
图像 = np.expand_dims(图像, 轴=0)
返回图像

@app.route(&#39;/predict&#39;,methods=[&#39;POST&#39;])
def 预测（）：
尝试：
    文件 = request.files[&#39;文件&#39;]
    image_file = Image.open(io.BytesIO(file.read()))
    图像 = cv2.cvtColor(np.array(image_file), cv2.COLOR_RGB2BGR)

    
    print(&quot;输入图像形状：&quot;, image.shape)

    预处理图像 = 预处理图像（图像）

    
    terpreter.set_tensor(interpreter.get_input_details()[0][&#39;index&#39;], preprocessed_image)

    
    解释器.invoke()

    
    output_tensor =terpreter.get_tensor(interpreter.get_output_details()[0][&#39;index&#39;])

    Predicted_class = np.argmax(output_tensor, axis=1)[0]

  
    print(“预测类别索引：”,predicted_class)

    结果 = {“预测”：class_names[预测类]}
    返回 jsonify(结果)
除了异常 e：
    返回 jsonify({“错误”: str(e)})

如果 __name__ == &#39;__main__&#39;:
应用程序运行（调试=真）

我已尝试以下步骤来解决该问题：

我在 Flask API 代码的不同位置添加了打印语句，以了解执行流程。我预计会看到针对不同输入图像的不同预测，但 API 始终预测相同的类别（“CN - 认知正常”）。

我查看了加载的 TensorFlow Lite 模型并检查了其输入和输出详细信息。该模型似乎加载正确，并且输入图像按预期进行了预处理。我希望预测与模型的训练性能保持一致。

考虑到我的训练代码和 Flask API 之间颜色空间处理的差异，我尝试了不同的颜色空间转换。然而，这并没有解决问题。

我仔细检查了训练和 API 代码中的图像预处理步骤，以确保一致性。 Flask API 中的预处理与训练过程保持一致。


我希望 Flask API 能够根据输入图像提供多种预测，与模型在训练期间的性能保持一致。每个输入图像都应该产生与其实际类别相对应的预测。
无论图像的实际内容如何，​​Flask API 都会为每个输入图像一致地预测相同的类别（“CN - 认知正常”）。这种行为与训练期间模型的准确性不一致。
TFLite 转换或模型准确性是否存在我可能忽略的潜在问题？]]></description>
      <guid>https://stackoverflow.com/questions/77708650/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</guid>
      <pubDate>Sat, 23 Dec 2023 18:35:50 GMT</pubDate>
    </item>
    <item>
      <title>Python Tensorflow.keras LSTM：类型错误：`generator` 产生了形状为 (36, 36, 147) 的元素，而预期形状为 (36, 147) 的元素</title>
      <link>https://stackoverflow.com/questions/77708557/python-tensorflow-keras-lstm-typeerror-generator-yielded-an-element-of-shape</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77708557/python-tensorflow-keras-lstm-typeerror-generator-yielded-an-element-of-shape</guid>
      <pubDate>Sat, 23 Dec 2023 18:05:26 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 tf.GradientTape 训练训练/有效/测试集？</title>
      <link>https://stackoverflow.com/questions/77708229/how-to-train-train-valid-test-set-using-tf-gradienttape</link>
      <description><![CDATA[我想问一下如何使用train/valid/test模型来训练模型。
我主要模仿这里的代码（https://www.tensorflow.org/guide/ keras/writing_a_training_loop_from_scratch）。
这是生成数据集和基本内容的代码。
导入keras
从 keras 导入层
将 numpy 导入为 np

输入= keras.Input（形状=（784，），名称=“数字”）
x1 = 层.Dense(64, 激活=“relu”)(输入)
x2 = 层.Dense(64, 激活=“relu”)(x1)
输出=layers.Dense(10,name=“预测”)(x2)
模型= keras.Model（输入=输入，输出=输出）

# 实例化优化器。
优化器 = tf.keras.optimizers.SGD(learning_rate=1e-3)
# 实例化损失函数。
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# 准备训练数据集。
批量大小 = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

# 保留 10,000 个样本用于验证。
x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

# 准备训练数据集。
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)

# 准备验证数据集。
val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
val_dataset = val_dataset.batch(batch_size)

# 获取模型
输入= keras.Input（形状=（784，），名称=“数字”）
x = 层.Dense(64, 激活=“relu”, 名称=“dense_1”)(输入)
x = 层.Dense(64, 激活=“relu”, 名称=“dense_2”)(x)
输出=layers.Dense(10,name=“预测”)(x)
模型= keras.Model（输入=输入，输出=输出）

# 实例化优化器来训练模型。
优化器 = tf.keras.optimizers.SGD(learning_rate=1e-3)
# 实例化损失函数。
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# 准备指标。
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

@tf.函数
def train_step(x, y):
    使用 tf.GradientTape() 作为磁带：
        logits = 模型(x, 训练=True)
        loss_value = loss_fn(y, logits)
    grads = Tape.gradient(loss_value, model.trainable_weights)
    优化器.apply_gradients(zip(grads, model.trainable_weights))
    train_acc_metric.update_state(y, logits)
    回波损耗值

@tf.函数
def test_step(x, y):
    val_logits = 模型(x, 训练=False)
    val_acc_metric.update_state(y, val_logits)

这是训练部分。
导入时间

历元 = 2
对于范围内的纪元（纪元）：
    print(&quot;\n纪元%d开始&quot; % (纪元,))
    开始时间 = 时间.time()

    # 迭代数据集的批次。
    对于枚举（train_dataset）中的步骤（x_batch_train，y_batch_train）：
        loss_value = train_step(x_batch_train, y_batch_train)

        # 每 200 个批次记录一次。
        如果步骤 % 200 == 0:
            打印（
                “第 %d 步的训练损失（一批）：%.4f”
                %（步长，浮点数（损失值））
            ）
            print(“到目前为止已看到：%d 个样本” % ((step + 1) * batch_size))

    # 显示每个时期结束时的指标。
    train_acc = train_acc_metric.result()
    print(&quot;历元训练 acc: %.4f&quot; % (float(train_acc),))

    # 在每个时期结束时重置训练指标
    train_acc_metric.reset_states()

    # 在每个时期结束时运行验证循环。
    对于 val_dataset 中的 x_batch_val、y_batch_val：
        test_step(x_batch_val, y_batch_val)

    val_acc = val_acc_metric.result()
    val_acc_metric.reset_states()
    print(&quot;验证acc: %.4f&quot; % (float(val_acc),))
    print(&quot;所用时间: %.2fs&quot; % (time.time() - start_time))


我的问题是，如果数据集分为 3 个类别：训练、测试、有效，那么每个类别的代码会有什么不同，尤其是测试和有效。
例如，
&lt;前&gt;&lt;代码&gt;x_val = x_train[-20000:]
y_val = y_train[-20000:]

x_test = x_train[-20000:-10000]
y_test = y_train[-20000:-10000]

x_train = x_train[:-10000]
y_train = y_train[:-10000]


valid 也应该使用 tf.gradienttape 吗？或训练=真？这让我很困惑。
感谢您的回复。
谢谢，
分钟]]></description>
      <guid>https://stackoverflow.com/questions/77708229/how-to-train-train-valid-test-set-using-tf-gradienttape</guid>
      <pubDate>Sat, 23 Dec 2023 16:01:22 GMT</pubDate>
    </item>
    <item>
      <title>我正在研究一个示例：简单线性回归薪资数据解释了为什么为 x 列创建新轴的步骤以及什么是 np.newaxis [关闭]</title>
      <link>https://stackoverflow.com/questions/77708064/i-was-working-on-a-examples-simple-linear-regression-salary-data-from-that-expl</link>
      <description><![CDATA[# 分割训练数据和测试数据

X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state=100)

# 为 x 列创建新轴

X_train = X_train[:,np.newaxis]
X_test = X_test[:,np.newaxis]

使用 np.newaxis 添加新轴有助于重塑数据以实现兼容性]]></description>
      <guid>https://stackoverflow.com/questions/77708064/i-was-working-on-a-examples-simple-linear-regression-salary-data-from-that-expl</guid>
      <pubDate>Sat, 23 Dec 2023 15:05:36 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Label Studio 没有找到云源 - 本地存储（不存在）</title>
      <link>https://stackoverflow.com/questions/77707858/why-my-label-studio-didnt-find-the-cloud-source-local-storage-doesnt-exist</link>
      <description><![CDATA[我已经从 git 安装了 Label-Studio git 安装，
并且想要将存储更改为云存储 - 本地存储，路径为 C:\data\media\dataset 数据集图像路径
所以我从这一步开始 https://labelstud.io/guide/storage#Local -storage 和我的命令是：
 设置 LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED=true

 设置 LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT=C:\\data\\media\\dataset

我添加了set，因为如果没有它，它会出错。
当我更改本地存储时，它不存在。它警告使用“必须以 LOCAL_FILES_DOCUMENT_ROOT 开头”
[ErrorDetail(string=&#39;路径 C:\\\\data\\\\media 必须以 LOCAL_FILES_DOCUMENT_ROOT=D:\\ 开头，并且必须是子项，例如：D:\\abc&#39;, code =&#39;无效&#39;）]

[ErrorDetail(string=&#39;路径 LOCAL_FILES_DOCUMENT_ROOT=C:\\\\data\\\\media 不存在&#39;, code=&#39;无效&#39;)]

屏幕截图
屏幕截图]]></description>
      <guid>https://stackoverflow.com/questions/77707858/why-my-label-studio-didnt-find-the-cloud-source-local-storage-doesnt-exist</guid>
      <pubDate>Sat, 23 Dec 2023 13:55:59 GMT</pubDate>
    </item>
    <item>
      <title>如何为 XGboost 性能分配月度数据的权重？</title>
      <link>https://stackoverflow.com/questions/77707285/how-to-assign-weights-to-monthly-data-to-xgboost-performance</link>
      <description><![CDATA[我们收到了用于训练的逐月数据，使用每个月末的快照。 XGboost 模型（二元分类）在训练测试中一个月表现良好，但在实时测试中表现不佳，我们添加了额外的月份数据，但召回率下降了。有没有办法影响模型，使最近的数据更加重要/权重，同时使用上个月的数据来支持学习和数量。
我尝试单独使用不同月份的数据进行训练。由于数据高度不平衡，尝试了SMOTE ENN、tomek，但是使用SMOTE降低了召回率很多。]]></description>
      <guid>https://stackoverflow.com/questions/77707285/how-to-assign-weights-to-monthly-data-to-xgboost-performance</guid>
      <pubDate>Sat, 23 Dec 2023 10:10:43 GMT</pubDate>
    </item>
    <item>
      <title>Vertex AI 表格数据预测错误</title>
      <link>https://stackoverflow.com/questions/77707074/vertex-ai-tabular-data-incorrect-prediction</link>
      <description><![CDATA[我使用表格数据集在 Google Vertex AI 中创建了模型，该数据集有两个感兴趣的列，称为 ActivityName 和 WorkType。我希望模型根据 ActivityName 预测工作类型。以下是我遵循的步骤：-

上传了包含用于训练、验证和测试的 SPLIT 列的 CSV 文件，确保每个工作类型至少拥有 80% 的训练数据。
创建了一个具有 3 个节点小时的 AutoML 模型，用于使用对数损失进行训练
部署模型
尝试了不同的活动名称，但预测错误

源示例位于此处]]></description>
      <guid>https://stackoverflow.com/questions/77707074/vertex-ai-tabular-data-incorrect-prediction</guid>
      <pubDate>Sat, 23 Dec 2023 08:37:44 GMT</pubDate>
    </item>
    <item>
      <title>如何拆分其中的列和数据？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77707030/how-split-column-and-data-in-it</link>
      <description><![CDATA[如何划分列及其中的数据。我已附上供参考。
示例：水泥_水列，其值为 3002； 203.0 必须分成名为水泥和水的两列，其值分别为 302.0 和 203.0。列值具有不同的分隔符（;，_），必须进行处理，并且这些值具有字符串数据，必须使用单词到数字将其转换为数值。
train.csv -https://drive.google.com/file/d/1rXC_cgJDHgvEsly9mVXphM_rl0ZI4sO2/view?usp=drive_link
test.csv https://drive.google。 com/file/d/1jUqlt5NVLvf9Y-vNLehCZShBqBkK0_NV/view?usp=drive_link
供您参考
&lt;前&gt;&lt;代码&gt;#代码
将 pandas 导入为 pd
从 word2number 导入 w2n

df = pd.read_csv(&#39;test.csv - Sheet1.csv&#39;)
def Convert_words_to_numbers(文本):
    单词 = text.replace(&#39;_&#39;, &#39; &#39;).replace(&#39;;&#39;, &#39; &#39;).replace(&#39;,&#39;, &#39; &#39;).split()
    Converted_words = [str(w2n.word_to_num(word)) if word.isalpha() else 逐字逐字]
    返回 &#39;​​ &#39;.join(converted_words)
df[&#39;水泥水&#39;] = df[&#39;水泥水&#39;].apply(lambda x:convert_words_to_numbers(x))
df[[&#39;水泥&#39;, &#39;水&#39;]] = df[&#39;水泥水&#39;].str.split(&#39; &#39;, Expand=True)
df[[&#39;coarse_aggregate&#39;, &#39;fine_aggregate&#39;]] = df[&#39;coarse_fine_aggregate&#39;].str.split(&#39;;&#39;, Expand=True)
df = df.drop([&#39;水泥水&#39;, &#39;coarse_fine_aggregate&#39;], axis=1)
df = df.apply(pd.to_numeric, 错误=&#39;忽略&#39;)
打印（df）
]]></description>
      <guid>https://stackoverflow.com/questions/77707030/how-split-column-and-data-in-it</guid>
      <pubDate>Sat, 23 Dec 2023 08:18:33 GMT</pubDate>
    </item>
    <item>
      <title>Resnet34第一层7x7或3x3</title>
      <link>https://stackoverflow.com/questions/77704426/resnet34-first-layer-7x7-or-3x3</link>
      <description><![CDATA[我一直在尝试使用 pytorch 实现 Resnet34，但在查看其他实现时，我发现其中一些具有 3x3 卷积层 + bn + relu 作为第一层。然而，架构图上却写着7x7/2的卷积层。我真的很困惑哪一个是正确的。顺便说一下，我正在 CIFAR10 上进行训练，目前使用 7x7 卷积层经过 100 个周期后获得了 0.9 的准确率。
谢谢！
架构图
self.input_layer = nn.Sequential(
nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,bias=False),
nn.BatchNorm2d(64),
ReLU(),
nn.MaxPool2d(3, 步长=2, 填充=1)
）

这是我的第一个卷积层的代码。]]></description>
      <guid>https://stackoverflow.com/questions/77704426/resnet34-first-layer-7x7-or-3x3</guid>
      <pubDate>Fri, 22 Dec 2023 15:14:50 GMT</pubDate>
    </item>
    <item>
      <title>如何绘制交叉验证的 AUROC 并找到最佳阈值？</title>
      <link>https://stackoverflow.com/questions/77702305/how-to-plot-cross-validated-auroc-and-find-the-optimal-threshold</link>
      <description><![CDATA[在通过交叉验证评估我的机器学习模型时，我遇到了一个问题。我知道如何在交叉验证中绘制 AUROC 和每个折叠的相应阈值，但我不确定是否绘制所有折叠的平均 AUROC 及其相应阈值。
为此，我在Stack Overflow上探索了相关问题，并找到了相应的解决方案。您可以通过以下链接找到原始问题：[https://stackoverflow.com/questions/57708023/plotting-the-roc-curve-of-k-fold-cross-validation%5C]。尽管我成功生成了平均 ROC，但在准确绘制相应阈值方面遇到了挑战。为了解决这个问题，我根据自己的理解合并了额外的代码，但我不确定这种方法的正确性。
此外，我观察到使用 np.mean() 计算的平均 AUC 与使用 sklearn.metrics 计算的 AUC 值之间存在差异。因此，我正在寻求关于哪个值更准确以获得精确的 AUC 结果的指导。下面是我调整后的修改代码。
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

cv = 分层KFold(n_splits=10)
分类器= SVC（内核=&#39;sigmoid&#39;，概率= True，random_state = 0）

tprs = []
曲线面积=[]
最佳阈值 = []
Mean_fpr = np.linspace(0, 1, 100)
plt.figure(figsize=(10,10))
我=0
对于火车，在 cv.split(X, y) 中测试：
    probas_ = classifier.fit(X[训练], y[训练]).predict_proba(X[测试])
    # 计算 ROC 曲线并计算曲线面积
    fpr, tpr, 阈值 = roc_curve(y[测试], probas_[:, 1])

    # 我添加的代码：
    最优阈值索引 = np.argmax(tpr-fpr)
    最优阈值 = 阈值[最优阈值索引]
    最佳阈值.append(最佳阈值)
    #

    tprs.append(np.interp(mean_fpr, fpr, tpr))
    tprs[-1][0] = 0.0
    roc_auc = auc(fpr, tpr)
    aucs.append(roc_auc)
    plt.plot(fpr, tpr, lw=1, alpha=0.3,
             label=&#39;ROC 折叠 %d (AUC = %0.4f)&#39; % (i, roc_auc))

    我 += 1



plt.plot([0, 1], [0, 1], 线型=&#39;--&#39;, lw=2, 颜色=&#39;r&#39;,
         标签=&#39;机会&#39;，alpha=.8)

mean_tpr = np.mean(tprs, 轴=0)
平均值_tpr[-1] = 1.0


mean_auc = auc(mean_fpr,mean_tpr)

# 我添加的代码：
np_mean_AUC = np.mean(aucs)
# print(f&quot;np_mean_AUC={np_mean_AUC},mean_auc={mean_auc}&quot;)
#

std_auc = np.std(aucs)

plt.plot(mean_fpr,mean_tpr,颜色=&#39;b&#39;,
         标签=r&#39;平均ROC (AUC = %0.4f $\pm$ %0.4f)&#39; % (np_mean_AUC, std_auc),
         lw=2，阿尔法=.8)

# 我添加的代码：
mean_optimal_threshold_index = np.argmax(mean_tpr-mean_fpr)
plt.annotate(f&#39;平均最佳阈值({np.mean(optimal_thresholds):.2f})&#39;,
                xy=(mean_fpr[平均最佳阈值索引],mean_tpr[平均最佳阈值索引]),
                xy 文本=(5, -5),
                textcoords=&#39;偏移点&#39;,
                arrowprops = dict（facecolor =&#39;红色&#39;，arrowstyle =&#39;楔形，tail_width = 0.7&#39;，shrinkA = 0，shrinkB = 10），
                颜色=&#39;红色&#39;）
#

std_tpr = np.std(tprs, 轴=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
plt.fill_ Between(mean_fpr, tprs_lower, tprs_upper, color=&#39;grey&#39;, alpha=.2,
                 标签=r&#39;$\pm$ 1 标准。开发。”）

plt.xlim([-0.01, 1.01])
plt.ylim([-0.01, 1.01])
plt.xlabel(&#39;误报率&#39;,fontsize=18)
plt.ylabel(&#39;真阳性率&#39;,fontsize=18)
plt.title(&#39;SVM的交叉验证ROC&#39;,fontsize=18)
plt.legend(loc=“右下”, prop={&#39;size&#39;: 15})
plt.show()

以下是输出：
在此处输入图像描述
请告诉我我在代码中所做的更改是否可以准确绘制用于交叉验证的 ROC 曲线以及相应的阈值，以及标记的 AUC 值是否有意义。]]></description>
      <guid>https://stackoverflow.com/questions/77702305/how-to-plot-cross-validated-auroc-and-find-the-optimal-threshold</guid>
      <pubDate>Fri, 22 Dec 2023 07:42:20 GMT</pubDate>
    </item>
    <item>
      <title>TypeError: JoypadSpace.reset() 有一个意外的关键字参数“seed”，当我运行以下代码时，我应该做什么来解决这个问题？</title>
      <link>https://stackoverflow.com/questions/76509663/typeerror-joypadspace-reset-got-an-unexpected-keyword-argument-seed-when-i</link>
      <description><![CDATA[当我运行此代码时：
from nes_py.wrappers import JoypadSpace
进口健身房
导入gym_super_mario_bros
从gym_super_mario_bros.actions导入SIMPLE_MOVMENT
从gym.wrappers导入GrayScaleObservation
从 stable_baselines3.common.vec_env 导入 VecFrameStack,DummyVecEnv
从 matplotlib 导入 pyplot 作为 plt

env =gym_super_mario_bros.make(&#39;SuperMarioBros-v0&#39;,apply_api_compatibility=True,render_mode=“人类”)
env = JoypadSpace(env, SIMPLE_MOVMENT)
env = GrayScaleObservation(env,keep_dim=True)
env = DummyVecEnv([lambda:env])
env = VecFrameStack(env,4,channels_order=&#39;最后&#39;)
状态 = env.reset()

我收到以下错误：

我应该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/76509663/typeerror-joypadspace-reset-got-an-unexpected-keyword-argument-seed-when-i</guid>
      <pubDate>Mon, 19 Jun 2023 19:47:30 GMT</pubDate>
    </item>
    <item>
      <title>如何动态地将curl变量发送给管道工函数？</title>
      <link>https://stackoverflow.com/questions/61824959/how-to-send-curl-variables-to-plumber-function-dynamically</link>
      <description><![CDATA[我想根据任意数量的输入变量动态调用管道工 API。我需要将curl 输入映射到函数名称的输入。例如，如果函数有一个输入 hi，则 curl -s --data &#39;hi=2&#39; 意味着 hi=2 应该是作为输入参数传递给函数。这可以直接在 R 中使用 match.call() 完成，但在通过管道工 API 调用它时失败。
获取函数
&lt;前&gt;&lt;代码&gt;#&#39; @post /API
#&#39; @serializer unboxedJSON
tmp &lt;- 函数(hi) {

  输出 &lt;- 列表(hi=hi)

  out &lt;- toJSON(out, Pretty = TRUE, auto_unbox = TRUE)

  返回（出）

}

tmp(嗨=2)
输出：{嗨：2}

然后
curl -s --data &#39;hi=10&#39; http://127.0.0.1/8081/API
输出：{\n \&quot;hi\&quot;: \&quot;2\&quot;\n}

一切看起来都不错。但是，取函数
&lt;前&gt;&lt;代码&gt;#&#39; @post /API
#&#39; @serializer unboxedJSON
tmp &lt;- 函数(...) {

  out &lt;- match.call() %&gt;%
         as.list() %&gt;%
         .[2:长度(.)] # %&gt;%

  out &lt;- toJSON(out, Pretty = TRUE, auto_unbox = TRUE)

  返回（出）

}
tmp(嗨=2)
输出：{嗨：2}

然后
curl -s --data &#39;hi=10&#39; http://127.0.0.1/8081/API
out: {“错误”:“500 - 内部服务器错误”,“消息”:“错误: 没有方法 asJSON S3 类: R6\n”}

实际上，我真正想做的是加载我的 ML 模型以使用管道工 API 预测分数。例如
model &lt;- readRDS(&#39;model.rds&#39;) # 将模型加载为全局变量

预测得分 &lt;- 函数(...) {
    
    df_in &lt;- match.call() %&gt;%
        as.list() %&gt;%
        .[2:长度(.)] %&gt;%
        as.data.frame()

    json_out &lt;- 列表(
        Score_out = 预测(模型, df_in) %&gt;%
        toJSON(., 漂亮 = T, auto_unbox = T)

    返回（json_out）
}


此函数在本地运行时按预期工作，但通过 curl -s --data &#39;var1=1&amp;var2=2...etc&#39; http://listen_address 通过 API 运行&lt; /p&gt;
我收到以下错误：
&lt;块引用&gt;
{“错误”：“500 - 内部服务器错误”，“消息”：“as.data.frame.default(x[[i]]，可选 = TRUE) 中的错误：无法强制类“c(“PlumberResponse”，“R6”)”到 data.frame\n&quot;}
]]></description>
      <guid>https://stackoverflow.com/questions/61824959/how-to-send-curl-variables-to-plumber-function-dynamically</guid>
      <pubDate>Fri, 15 May 2020 17:22:08 GMT</pubDate>
    </item>
    </channel>
</rss>