<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 28 Nov 2024 03:32:47 GMT</lastBuildDate>
    <item>
      <title>为什么有些模型架构使用加法运算符而不是减法，反之亦然？</title>
      <link>https://stackoverflow.com/questions/79232424/why-do-some-model-architectures-use-the-addition-operator-instead-of-subtraction</link>
      <description><![CDATA[为什么有些模型架构使用加法运算符而不是减法运算符，反之亦然？例如，在 ResNet 中，更改运算符是否会影响模型（F(x) + x -&gt; F(x) - x）？模型是否只需通过翻转符号就可以轻松学习？]]></description>
      <guid>https://stackoverflow.com/questions/79232424/why-do-some-model-architectures-use-the-addition-operator-instead-of-subtraction</guid>
      <pubDate>Thu, 28 Nov 2024 02:39:18 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Hugging Face Trainer 或 SFT Trainer 中记录第零步的训练损失？</title>
      <link>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</link>
      <description><![CDATA[我正在使用 Hugging Face Trainer（或 SFTTrainer）进行微调，我想在步骤 0（在执行任何训练步骤之前）记录训练损失。我知道有一个用于评估的 eval_on_start 选项，但我找不到在训练开始时记录训练损失的直接等效方法。
是否有办法使用 Trainer 或 SFTTrainer 在步骤 0（在任何更新之前）记录初始训练损失？理想情况下，我希望使用类似于 eval_on_start 的方法。
以下是我迄今为止尝试过的方法：
解决方案 1：自定义回调
我实现了自定义回调，以在训练开始时记录训练损失：
from transformers import TrainerCallback

class TrainOnStartCallback(TrainerCallback):
def on_train_begin(self, args, state, control, logs=None, **kwargs):
# 在步骤 0 记录训练损失
logs = logs or {}
logs[&quot;train/loss&quot;] = None # 如果可用，用初始值替换 None
logs[&quot;train/global_step&quot;] = 0
self.log(logs)

def log(self, logs):
print(f&quot;Logging at start: {logs}&quot;)
wandb.log(logs)

# 将回调添加到 Trainer
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
train_dataset=train_dataset,
eval_dataset=eval_dataset,
args=training_args,
optimizers=(optimizer, scheduler),
callbacks=[TrainOnStartCallback()],
)

这有效，但感觉有点过头了。它会在训练开始时记录任何步骤之前的指标。
解决方案 2：手动记录
或者，我在开始训练之前手动记录训练损失：
wandb.log({&quot;train/loss&quot;: None, &quot;train/global_step&quot;: 0})
trainer.train()

问题：
Trainer 或 SFTTrainer 中是否有任何内置功能可以在第 0 步记录训练损失？或者自定义回调或手动记录是这里的最佳解决方案吗？如果是这样，是否有更好的方法来实现此功能？与 eval_on_start 类似，但 train_on_start？
交叉：https://discuss.huggingface.co/t/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer/128188]]></description>
      <guid>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</guid>
      <pubDate>Thu, 28 Nov 2024 00:23:35 GMT</pubDate>
    </item>
    <item>
      <title>如何摆脱 Unity ML 中找不到类型或命名空间名称“Keypoint”的错误</title>
      <link>https://stackoverflow.com/questions/79231502/how-to-get-rid-of-the-type-or-namespace-name-keypoint-could-not-be-found-error</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79231502/how-to-get-rid-of-the-type-or-namespace-name-keypoint-could-not-be-found-error</guid>
      <pubDate>Wed, 27 Nov 2024 18:36:11 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么 ML/优化算法来找到最大输出的最佳输入值？[关闭]</title>
      <link>https://stackoverflow.com/questions/79229880/what-ml-optimization-algorithm-should-i-use-to-find-the-optimum-input-values-for</link>
      <description><![CDATA[我有一些数据，需要找到 X*Y 的最佳组合，以便因变量 A、B、C 达到最大值。
所有这些都是简单的数值，就像科学实验的观察结果一样。因此，A、B、C 是在同一范围内具有不同值的性能变量，而 X 和 Y 是两个测量变量。
基本上，我想制作一个程序来不断处理未来实验中涌入的任何数据。
我遇到过有人使用 ANN 回归来解决类似的问题，还有人建议使用梯度下降。由于我必须从头开始完成这项任务，如果我能建议我应该从哪种算法开始，我将非常高兴。]]></description>
      <guid>https://stackoverflow.com/questions/79229880/what-ml-optimization-algorithm-should-i-use-to-find-the-optimum-input-values-for</guid>
      <pubDate>Wed, 27 Nov 2024 10:35:42 GMT</pubDate>
    </item>
    <item>
      <title>Python 和 Maple SVR 结果中的差异 [关闭]</title>
      <link>https://stackoverflow.com/questions/79229459/discrepancy-in-python-and-maple-svr-results</link>
      <description><![CDATA[我正在使用最小二乘 SVR 方法求解积分方程。我应该将下面链接中提供的 Maple 代码转换为 Python。我已编写如下所示的 Python 代码，但无论我做什么，都无法获得准确的结果。您能告诉我如何让我的代码产生与 Maple 相同的结果吗？
https://github.com/alirezaafzalaghaei/LSSVR-FIE/blob/master/paper-examples/example-4/CLS-SVR-dual.mw
import numpy as np
from scipy.integrate import quad
from scipy.special import legendre
from scipy.optimize import minimal
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from scipy.stats import qmc
from scipy.integrate import quad
from scipy.integrate import dblquad
import time
from tensorflow.keras import regularizers
from tensorflow.keras.losses import MeanSquaredError

# 设置精度
np.set_printoptions(precision=15)

# 定义积分函数
def Quad(f, a, b):
# 注意：这是一个简化版本，您可能需要根据要使用的具体求积方法进行调整。
return dblquad(f, a, b,a,b)[0]

# 定义区间
a, b = 0, 1

# 定义精确函数
def exact(x,y):
return 1/((x+y+1)**2)

# 定义函数 f
def f(x,y):
return 1/((x+y+1)**2)-(x/(6*(8+y)))

# 定义函数 k
def k(x, y,t,s):
return (x/((8+y)*(1+t+s)))

# 示例用法（替换为您的具体用例）
result_quad = Quad(f, a, b)
print(result_quad)
import numpy as np
from scipy.special import legendre
from scipy.optimize import fsolve

def shift(x):
return (2 * x - a - b) / (b - a)

gamma = 10 ** 8
M = 4
N = M + 1

a, b = 0, 1 # 定义 a 和 b

# 计算勒让德多项式的根
train1 = fsolve(lambda x: legendre(N)(shift(x)), np.linspace(0, 1, N))
train2 = fsolve(lambda y: legendre(N)(shift(y)), np.linspace(0, 1, N))

# 使用 NumPy 的 meshgrid 和 stacking 创建 N x 2 矩阵
X, Y = np.meshgrid(train1, train2)
train = np.stack((X.flatten(), Y.flatten()), axis=1)

print(train)
print(train.shape) # 输出形状以确认
phi = []
L_phi = []
for kindx in range((M + 1) * (M + 1)):
i = kindx // (M + 1) + 1
j = kindx % (M + 1) + 1
#print(i,j)
# 创建 i 次勒让德多项式
p = legendre(i)
# 使用嵌套函数定义 phi[i] 以创建新范围
def make_phi(p=p): # 在嵌套函数的参数中捕获 p
return lambda x: p(shift(x))
phi.append(make_phi())
L_phi.append(lambda x,y: phi[i](x)*phi[j](y) - dblquad(lambda t,s: k(x,y, t,s) 
* phi[i](t)*phi[j](s), a, b,a,b)[0])

train = np.array([train1, train2]).T #假设 train1 和 train2 是列表或 
数组
A = np.zeros((M+1, M+1))
for m in range(M+1):
for n in range(M+1):
A[m, n] = L_phi[m](train[n, 0], train[n, 1])
print(A)
Omega = np.dot(A.T, A) + np.identity(M+1) / gamma
GAMMA = np.array([f(train1[i],train2[i]) for i in range(M+1)]).reshape(M+1, 1)
alpha = resolve(Omega, GAMMA)
import numpy as np
l=[]
import numpy as np
from scipy.linalg import resolve

# ... (其他导入和函数) ...

def u_tilde(x, y):

# 仅对 x 和 y 处 phi 中的前 N ​​个勒让德多项式进行求值
# 这与 A 的维度一致
P_x = np.array([phi[i](x) for i in range(M+1)]).reshape(-1, 1) # 此处更改
P_y = np.array([phi[i](y) for i in range(M+1)]).reshape(-1, 1) # 此处更改

# 使用逐元素乘法和求和计算 u˜(x)
u_tilde_x = alpha.T @ A.T @ (P_x * P_y) # 此处更改

return u_tilde_x[0, 0] # 从结果中提取标量值

# ...（其余代码）...# 从结果中提取标量值

return result
# 示例用法：
x_value = train1
y_value = train2
for k in range(M+1):
u_tilde_at_x = u_tilde(x_value[k],y_value[k])
l.append(u_tilde_at_x)
print(f&quot;u˜({x_value[k]}) = {u_tilde_at_x}&quot;)
#u_tilde_at_x = u_tilde(x_value)
#print(f&quot;u˜({x_value}) = {u_tilde_at_x}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79229459/discrepancy-in-python-and-maple-svr-results</guid>
      <pubDate>Wed, 27 Nov 2024 08:30:38 GMT</pubDate>
    </item>
    <item>
      <title>寻找洞察聚类机器学习项目[关闭]</title>
      <link>https://stackoverflow.com/questions/79228750/looking-for-insights-clustering-machine-leaning-prohect</link>
      <description><![CDATA[我正在为高中开展一个涉及聚类（k 均值和 DBSCAN）的机器学习项目
我抓取了一个电子商务网站 (StockX)，并对某些商品（包括设计师品牌等）的零售价值 (x) 和转售价值 (y) 进行聚类。然后对它们进行聚类。
最终的聚类结果非常基础，有 3 个聚类 - 围绕低零售/转售、中等零售/转售和高零售/转售价格。
我想知道你们是否对我可以用数据和我的项目做的更细微的事情有什么建议。如果有更多有趣的发现，我可以尝试挖掘出来。]]></description>
      <guid>https://stackoverflow.com/questions/79228750/looking-for-insights-clustering-machine-leaning-prohect</guid>
      <pubDate>Wed, 27 Nov 2024 01:58:21 GMT</pubDate>
    </item>
    <item>
      <title>大型多 GPU ML 训练作业的 GPU 间流量 [关闭]</title>
      <link>https://stackoverflow.com/questions/79228728/inter-gpu-traffic-for-large-multi-gpu-ml-training-jobs</link>
      <description><![CDATA[对于具有不同并行类型（如数据、张量、管道等）的分布式多 GPU 大型机器学习作业，我正在寻找点对点、全对全、全归约等 GPU 间流量的模式和百分比。是否有关于此的研究/数据？
大多数研究都讨论数据并行，其中全归约类型的流量占分配梯度的大多数。]]></description>
      <guid>https://stackoverflow.com/questions/79228728/inter-gpu-traffic-for-large-multi-gpu-ml-training-jobs</guid>
      <pubDate>Wed, 27 Nov 2024 01:44:11 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试创建多尺度 CNN，但遇到此错误：RuntimeError：mat1 和 mat2 形状无法相乘（32x4095 和 4096x4096）</title>
      <link>https://stackoverflow.com/questions/79228528/i-am-trying-to-create-multiscale-cnn-but-facing-this-error-runtimeerror-mat1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79228528/i-am-trying-to-create-multiscale-cnn-but-facing-this-error-runtimeerror-mat1</guid>
      <pubDate>Tue, 26 Nov 2024 23:06:22 GMT</pubDate>
    </item>
    <item>
      <title>Keras 神经网络回归模型优先考虑 2 个输出值，如何才能让它更好地概括？[关闭]</title>
      <link>https://stackoverflow.com/questions/79228286/keras-neural-network-regression-model-prioritizes-2-output-values-over-the-rest</link>
      <description><![CDATA[我正在尝试使用其他特征预测附加数据集中的“温度”值。执行代码时，模型对两个值有明显的偏差。我正在使用 plotly 图显示预测的准确性，其中包含真实值和预测值以及表示最佳预测的线。我的预测准确性
因此，如您所见，有两个主要的信息集群，表明我的模型主要选择这两个值作为输出。我不知道为什么会发生这种情况，也不知道我可以做些什么来补救。我将链接我正在使用的 .csv 文件和代码（google Drive / Colab）
此外，当删除预处理步骤时，它会产生类似的效果，但有三个主要集群而不是两个。 https://drive.google.com/drive/folders/1uSHTVAmW-UXhutZa5-Tf1QcB_e9cl_DR?usp=sharing
我尝试过：

删除预处理步骤。
添加特征工程和通过相关性进行数据选择。
排除分类值。
我已经试验了神经网络的大小和密度，增加或减少以查看它是否是欠拟合问题。
我已经引入了最多 100 次试验的自动超参数选择。
我在神经网络中添加了批量和特征规范化。
我已经手动调整了超参数的值，例如时期、激活函数等。
我使用了 KMeans 来降低密度。
我尝试了不同的数据分割。

我预计分布会略有变化，但它总是水平聚集（与预测值一起）
总之，这是我的神经网络还是预处理的问题？]]></description>
      <guid>https://stackoverflow.com/questions/79228286/keras-neural-network-regression-model-prioritizes-2-output-values-over-the-rest</guid>
      <pubDate>Tue, 26 Nov 2024 21:09:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么（远程） Jupyter 在 ML 训练期间很忙，但实际上却没有做任何事情？</title>
      <link>https://stackoverflow.com/questions/79226995/why-is-remote-jupyter-busy-during-ml-training-but-not-actually-doing-anything</link>
      <description><![CDATA[我正在使用 PyTorch 在自己的专用远程服务器上训练 ML 模型，使用 Jupyter 作为我的 IDE。
大约 120 个 epoch（训练大约 2 小时），Jupyter 单元停止更新输出，但状态栏仍显示内核状态为 busy，SSH 连接仍处于活动状态。
我认为训练可能仍在继续，但输出单元停止更新，因为它包含太多输出。为了验证这个假设，我昨晚让 Jupyter 运行了大约 7 个小时。当我醒来时，它已经在 123 个 epoch 时停止更新输出单元，当我终止执行并打印出当前 epoch 数时，它只达到了 126 个 epoch。
知道是什么原因造成的吗？]]></description>
      <guid>https://stackoverflow.com/questions/79226995/why-is-remote-jupyter-busy-during-ml-training-but-not-actually-doing-anything</guid>
      <pubDate>Tue, 26 Nov 2024 13:55:11 GMT</pubDate>
    </item>
    <item>
      <title>将请求上下文从 FastAPI 传递到用于 OpenAI 集成的 Microsoft Semantic Kernel 插件</title>
      <link>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</link>
      <description><![CDATA[我正在 FastAPI 应用程序中将 Microsoft Semantic Kernel 与 OpenAI 集成。我有一个聊天/端点，我从请求中收到一个 session_id，我需要将此 session_id 与 openai_client 一起传递给插件。但是，我不确定如何在内核的执行过程中将 FastAPI 请求中的 session_id 正确传递给插件。
以下是设置内核和插件的相关代码：
# 内核和服务设置
kernel = Kernel()

execution_settings = AzureChatPromptExecutionSettings(tool_choice=&quot;auto&quot;)
execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={})

openai_client = OpenAI(api_key=api_key)
chat_completion_service = OpenAIChatCompletion(
ai_model_id=model_id, 
api_key=api_key, 
service_id=service_id 
)

# 添加服务和插件
kernel.add_service(chat_completion_service)
kernel.add_plugin(MovesPlugin(openai_client), plugin_name=&#39;MovesPlugin&#39;)

在我的 FastAPI 端点内，我想在调用内核进行聊天响应时将 session_id 传递给插件：
# 在 FastAPI 端点内
@app.post(&quot;/chat/&quot;)
async def chat(request: Request):
session_id = await request.json().get(&#39;session_id&#39;)

# 获取聊天完成服务
_chat_completion_service = kernel.get_service(type=ChatCompletionClientBase)

# 获取聊天完成响应
response = await _chat_completion_service.get_chat_message_content(
chat_history=chat_history,
kernel=kernel,
settings=execution_settings
)

return响应

如何将请求上下文 (session_id) 从 FastAPI 请求传递到 MovesPlugin，并确保它与语义内核执行中的 openai_client 一起正确使用？
如能得到任何指导或建议，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</guid>
      <pubDate>Tue, 26 Nov 2024 12:51:34 GMT</pubDate>
    </item>
    <item>
      <title>在使用 dataloader 测试数据集时，我们应该设置 shuffle=true 吗？或者这无所谓？</title>
      <link>https://stackoverflow.com/questions/79212687/in-testing-dataset-using-dataloader-should-we-set-shuffle-true-or-it-doesnt-m</link>
      <description><![CDATA[我有一个自定义数据集（披萨、寿司和牛排的图片）。
我正在使用 torch DataLoader 来处理它，现在在编写测试数据加载器自定义时，我们应该设置 shuffle=true 还是这无关紧要？？
我还没有看到区别，只是问一般情况。]]></description>
      <guid>https://stackoverflow.com/questions/79212687/in-testing-dataset-using-dataloader-should-we-set-shuffle-true-or-it-doesnt-m</guid>
      <pubDate>Thu, 21 Nov 2024 19:33:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在 google colab 中使用从 kaggle 加载的数据（实际使用它）</title>
      <link>https://stackoverflow.com/questions/79195592/how-to-use-loaded-data-from-kaggle-in-google-colab-to-actually-work-with-it</link>
      <description><![CDATA[因此，我最近从此 https://www.kaggle.com/datasets/mostafaabla/garbage-classification 网站导入了数据集。尽管我在 google colab 中的文件中有它（已解压和所有这些东西），但我不知道如何在代码本身中实现它。就像来自 tensorflow 的 Fashion mnist 教程 https://www.tensorflow.org/tutorials/keras/classification?hl 它加载为
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
如何将数据导入/加载到代码单元并通过分成类来处理它（因为在该教程数据集中有多个类，而在我的自定义数据集中有 12 个）
请问如何操作？
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# 定义训练和验证目录的路径
train_dir = &#39;garbage-classification/train&#39;
val_dir = &#39;garbage-classification/validation&#39;

# 创建 ImageDataGenerator 进行数据增强
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

# 从目录加载图像
train_generator = train_datagen.flow_from_directory(
train_dir,
target_size=(150, 150), # 根据需要调整图像大小
batch_size=32,
class_mode=&#39;categorical&#39; # 如果有多个类，请使用 &#39;categorical&#39;
)

validation_generator = val_datagen.flow_from_directory(
val_dir,
target_size=(150, 150),
batch_size=32,
class_mode=&#39;categorical&#39;
)

我使用 perplexity 尝试解决，结果得到了这个。显然它没有起作用，所以..]]></description>
      <guid>https://stackoverflow.com/questions/79195592/how-to-use-loaded-data-from-kaggle-in-google-colab-to-actually-work-with-it</guid>
      <pubDate>Sat, 16 Nov 2024 16:34:39 GMT</pubDate>
    </item>
    <item>
      <title>为什么基于 Tensorflow.js 的天气预测模型无法预测正确的天气</title>
      <link>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</guid>
      <pubDate>Sun, 26 May 2024 18:36:09 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Windows 机器上安装 Rasa</title>
      <link>https://stackoverflow.com/questions/78483192/unable-to-install-rasa-in-windows-machine</link>
      <description><![CDATA[我尝试在我的 Windows 10 笔记本电脑上使用命令 pip install rasa 安装 Rasa。
我安装了 Python 3.11 版。
但是我收到以下错误：
获取构建 wheel 的要求未成功运行。
│ 退出代码：1
╰─&gt; [20 行输出]
回溯（最近一次调用最后一次）：
文件“C:\python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，行 
353，在&lt;module&gt; 中
main()
文件“C:\python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，第 335 行，
在 main 中 
json_out[&#39;return_val&#39;] = hook(**hook_input[&#39;kwargs&#39;])
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

我遗漏了什么？之前，我尝试安装 chatterbot 模块，也遇到了类似的错误。
此外，我也尝试使用 pip install chatterbot，但仍然出现错误。]]></description>
      <guid>https://stackoverflow.com/questions/78483192/unable-to-install-rasa-in-windows-machine</guid>
      <pubDate>Wed, 15 May 2024 10:10:42 GMT</pubDate>
    </item>
    </channel>
</rss>