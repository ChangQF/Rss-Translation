<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 31 Jul 2024 12:29:51 GMT</lastBuildDate>
    <item>
      <title>在 mlflow 中使用 lightgbm 进行数据集记录（自动记录）时速度严重减慢</title>
      <link>https://stackoverflow.com/questions/78816040/severe-slowdown-when-using-dataset-logging-autologging-with-lightgbm-in-mlflow</link>
      <description><![CDATA[在使用 mlflow 进行训练期间跟踪数据集时，我遇到了严重的减速（通常需要 10-50 倍的时间）。
代码如下所示：
mlflow.set_tracking_uri(&quot;http://localhost:5000&quot;)
mlflow.set_experiment(&quot;My Experiment&quot;)

mlflow.lightgbm.autolog()
train, val = get_datasets() # 返回 lightgbm.Dataset 对象
train_model(train, val) # 调用 lightgbm.train

通过设置 mlflow.lightgbm.autolog(log_datasets=False) 禁用数据集跟踪时，我没有遇到明显的减速。
有人知道为什么会这样吗？情况如何？
我预计不会出现明显的减速，因为 mlflow 仅跟踪数据集的一些基本统计数据（或至少看起来是这样。用户界面和文档并未表明其他情况）]]></description>
      <guid>https://stackoverflow.com/questions/78816040/severe-slowdown-when-using-dataset-logging-autologging-with-lightgbm-in-mlflow</guid>
      <pubDate>Wed, 31 Jul 2024 11:30:39 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow：您正在将 complex64 类型的输入转换为不兼容的 dtype float32。这将丢弃虚部</title>
      <link>https://stackoverflow.com/questions/78816038/tensorflowyou-are-casting-an-input-of-type-complex64-to-an-incompatible-dtype-f</link>
      <description><![CDATA[我正在尝试为我的模型定义 Adam 优化器。我使用的是 Tensorflow 2.16.1 和 Cuda 12.3。我有以下代码来定义 adam 优化器。
optimizer = tf.compat.v1.train.AdamOptimizer(ph_learning_rate, beta1 = 0.85, beta2 = 0.98)
tvs = tf.compat.v1.trainable_variables()
gradients, variable = zip(*optimizer.compute_gradients(net_cost, tvs))
train_step = optimizer.apply_gradients(zip(gradients, variable))

当我为 compute_gradients 运行它时，它会给我一个警告。如下所示
警告：tensorflow：您正在将 complex64 类型的输入转换为不兼容的 dtype float32。这将丢弃虚部，可能不是您想要的。
警告：tensorflow：您正在将 complex64 类型的输入转换为不兼容的 dtype float32。这将丢弃虚部，可能不是您想要的。
警告：tensorflow：您正在将 complex64 类型的输入转换为不兼容的 dtype float32。这将丢弃虚部，可能不是您想要的。
警告：tensorflow：您正在将 complex64 类型的输入转换为不兼容的 dtype float32。这将丢弃虚部，可能不是您想要的。

同样的例子在 tensorflow 2.10 中运行良好，但在 tensorflow 2.16.1 中却不行。任何帮助都将不胜感激。
我尝试使用不同版本的 tensorflow，但在 tensorflow 2.12 版本之后，它不起作用。我还尝试让网络只有 float32 输入，但使用 float32 输入的问题是，我们必须同时拥有两个部分（实部、虚部），因为我们必须在模型中处理复杂数据。]]></description>
      <guid>https://stackoverflow.com/questions/78816038/tensorflowyou-are-casting-an-input-of-type-complex64-to-an-incompatible-dtype-f</guid>
      <pubDate>Wed, 31 Jul 2024 11:30:11 GMT</pubDate>
    </item>
    <item>
      <title>如何创建边界框以在对象检测中检测不到任何内容？[关闭]</title>
      <link>https://stackoverflow.com/questions/78815620/how-do-i-create-my-bounding-boxes-to-detect-nothing-in-object-detection</link>
      <description><![CDATA[我正在开发一个计算机视觉项目，其中有猫和狗的类别，但我的数据集中也有一些既不包含猫也不包含狗的图像。我想教我的模型在这些情况下检测“无”，但我不确定如何为这些输出创建边界框。
我尝试使用带有 torch.tensor([]) 的空张量，但它与其他边界框的大小不同。我还尝试使用 [None, None, None, None] 创建一个边界框，但我不确定它是否会起作用。
我正在考虑创建一个整个图像大小的边界框，例如 [0, 0, 224, 224]，但我想知道这是否是唯一的方法。
我在网上搜索过，但没有找到明确的解决方案。有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78815620/how-do-i-create-my-bounding-boxes-to-detect-nothing-in-object-detection</guid>
      <pubDate>Wed, 31 Jul 2024 09:48:09 GMT</pubDate>
    </item>
    <item>
      <title>如何使用苹果 MLX 框架正确保存微调模型</title>
      <link>https://stackoverflow.com/questions/78815544/how-to-correctly-save-a-fine-tuned-model-using-apple-mlx-framework</link>
      <description><![CDATA[我们使用 MLX 来微调从 hugging face 获取的模型。
from transformers import AutoModel
model = AutoModel.from_pretrained(&#39;deepseek-ai/deepseek-coder-6.7b-instruct&#39;)

我们使用 python -m mlx_lm.lora --config lora_config.yaml 等命令对模型进行了微调，配置文件如下所示：
# 本地模型目录或 Hugging Face repo 的路径。
model: &quot;deepseek-ai/deepseek-coder-6.7b-instruct&quot;
# 训练过的适配器权重的保存/加载路径。
adapter_path: &quot;adapters&quot;

当微调后生成适配器文件时，我们通过类似脚本对模型进行评估
from mlx_lm.utils import *
model,tokenizer = load(path_or_hf_repo =&quot;deepseek-ai/deepseek-coder-6.7b-instruct&quot;,
adapter_path = &quot;adapters&quot; # path to new training adaptor
)
text = &quot;Tell sth about New York&quot;
response = generate(model, tokenizer, prompt=text, verbose=True, temp=0.01, max_tokens=100)

并且它按预期工作。
但是，在我们保存模型并使用 mlx_lm.generate 进行评估后，模型运行不佳。 （该行为与使用 generate(model, tokenizer, prompt=text, verbose=True, temp=0.01, max_tokens=100) 调用模型完全不同。
mlx_lm.fuse --model &quot;deepseek-ai/deepseek-coder-6.7b-instruct&quot; --adapter-path &quot;adapters&quot; --save-path new_model
mlx_lm.generate --model new_model --prompt &quot;Tell sth about New York&quot; --adapter-path &quot;adapters&quot; --temp 0.01

欢迎任何评论，谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78815544/how-to-correctly-save-a-fine-tuned-model-using-apple-mlx-framework</guid>
      <pubDate>Wed, 31 Jul 2024 09:35:34 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 中的矩阵分解从 3 个不同的矩阵（2 个交互矩阵和 1 个相似矩阵）创建一个矩阵？</title>
      <link>https://stackoverflow.com/questions/78814865/how-to-use-matrix-factorization-in-python-to-create-a-matrix-from-3-different-ma</link>
      <description><![CDATA[我有 3 个不同大小的矩阵 A、B、C，它们有一些共同的元素。A（相似性）与 B（交互）有一些共同的元素，但没有 C（第二个交互矩阵）。B 有一些与 C 共同的元素。我想创建一个基于 B 和 C 的矩阵 M，但提供基于 A、B、C 的输出，就像一个推荐系统类型，其中根据评级、观看次数和偏好推荐电影。例如：
A= [1,0,0.85,0,0,0,0.56
0,1,0,0.87,0.6,0.7.0,0
0,0,1,0,56,0,0,0.64,0,0.34]
B= [0,0,1,1,0,0
1,0,0,1,0,1
0,1,0,0,1,1]
C= [0,0,1,0,1
1,1,1,0,0
0,0,1,0,1]

我希望 M 类似于
M= [0,0,0.85,1,0,0,
1,1,1,.....]

尽管搜索了互联网甚至谷歌学术，但我还是无法找到为了找到实现相同目的的方法，我遇到的最多的方法就是对两个矩阵进行矩阵分解。]]></description>
      <guid>https://stackoverflow.com/questions/78814865/how-to-use-matrix-factorization-in-python-to-create-a-matrix-from-3-different-ma</guid>
      <pubDate>Wed, 31 Jul 2024 07:10:20 GMT</pubDate>
    </item>
    <item>
      <title>模型选择</title>
      <link>https://stackoverflow.com/questions/78814603/model-selection</link>
      <description><![CDATA[您好，我是 AI/ML 新手，我需要一些帮助。
我有一个数据框：-

我想训练一个模型，我可以提供数据列 1 和列 2，它们是系统 A 中的设备名称和警报。该模型应该预测输出并给出列 3 和列 4。显然，这只是一个示例数据。如果我的模型可以预测输出，并且如果有多个系统受到影响，它也可以给出输出，那就更好了。
更多解释：-
系统A 以某种方式连接到系统B，因此如果 A 中出现任何问题，系统 B 可能会发出警报。以下数据相同。
我的问题是我应该为这种类型的数据选择哪种模型，以便获得最大的准确性。]]></description>
      <guid>https://stackoverflow.com/questions/78814603/model-selection</guid>
      <pubDate>Wed, 31 Jul 2024 05:52:58 GMT</pubDate>
    </item>
    <item>
      <title>导入 pywrap_saved_model 时 DLL 加载失败：找不到指定的过程</title>
      <link>https://stackoverflow.com/questions/78814370/dll-load-failed-while-importing-pywrap-saved-model-the-specified-procedure-coul</link>
      <description><![CDATA[我在导入 tflite-model-maker 时遇到问题，
我已经使用 cmd 管理员安装了它，它完全完成了
问题是当我将其导入到我的代码中时
我尝试使用 pip install，完全没有问题，
我不知道，顺便说一下，我使用的是 py 版本 3.8
在此处输入图片说明
在此处输入图片说明
我尝试使用 tensorflow 的 tflite-model-maker 制作音频分类模型
并将其用于我的语音识别项目]]></description>
      <guid>https://stackoverflow.com/questions/78814370/dll-load-failed-while-importing-pywrap-saved-model-the-specified-procedure-coul</guid>
      <pubDate>Wed, 31 Jul 2024 03:52:53 GMT</pubDate>
    </item>
    <item>
      <title>优化序列以最小化涉及求和与序列长度的自定义评分函数</title>
      <link>https://stackoverflow.com/questions/78814217/optimizing-a-sequence-to-minimize-a-custom-score-function-involving-summation-an</link>
      <description><![CDATA[背景：
这是我随机想到的一个问题，我觉得它很有趣，我暂时想不出可行的解决方案。
正文：
问题定义
我试图找到一个序列 S，使得对于从 1 到 n 的每个数字 i，使用以下公式计算并最小化分数 f(i)：
f(i) = a * |S| + (1-a) * N(S, i)
其中：

|S| 是序列 S 的长度。
N(S, i) 是 S 中加起来达到目标​​ i 所需的最小元素数。序列 S 中的元素可以重复使用。
a 是一个介于 0 和 1 之间的参数，用于平衡 |S| 的贡献并将 N(S, i) 添加到分数中。

目标是找到这样一个序列 S，使得从 1 到 n 的每个 i 产生的最大分数 f(i) 尽可能低。
我尝试过的方法
我最初尝试使用蛮力方法来生成可能的序列并对其进行评估，但这种方法计算成本高，并且对于较大的数字（例如 n = 20 及以上）不可行。
问题

是否有更有效的算法或方法来解决这个问题，可能使用动态规划或其他优化技术？是否有类似于此问题结构的已知问题或数学框架，可以指导解决方案的开发？你们有办法解决这个问题吗？

想法
我考虑过使用随机近似来近似解决方案，但我无法执行这个想法。
观察：

我不知道如何证明，但 {1,2,3,…,N} 序列可以形成 1 到 (1+N)N/2 之间的任何数字。因此，如果 a=1，则最佳序列应为 {1,2,3,…,x}，其中 argminx (1+x)x/2 &gt;= target

如果 a=0，(1-a)=1，则 N(S,i) 始终为 1，因为我们可以形成一个序列 {1,2,3,…,T}，这样我们就可以仅使用单个数字本身来表示低于目标的任何数字

根据观察 1 和观察 2，我猜测最佳序列应介于 {1,2,…x} 和 {1,2,…,T} 的组合之间，并且总共会有 T-x+1 次这样的迭代。


]]></description>
      <guid>https://stackoverflow.com/questions/78814217/optimizing-a-sequence-to-minimize-a-custom-score-function-involving-summation-an</guid>
      <pubDate>Wed, 31 Jul 2024 02:23:03 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 PyTorch 在嘈杂的回归数据上生成过度拟合？</title>
      <link>https://stackoverflow.com/questions/78814212/how-can-i-generate-overfitting-on-noisy-regression-data-using-pytorch</link>
      <description><![CDATA[如何使用 PyTorch 在嘈杂的回归数据上生成过度拟合？尽管进行了各种尝试，但 PyTorch 倾向于泛化数据，这使得记住（过度拟合）数据变得具有挑战性。我修改了权重初始化，使用了 batch_size=1，使学习率变得灵活，并增加了层数和参数数量，但它仍然具有泛化能力。
是否有任何参数需要更改？
]]></description>
      <guid>https://stackoverflow.com/questions/78814212/how-can-i-generate-overfitting-on-noisy-regression-data-using-pytorch</guid>
      <pubDate>Wed, 31 Jul 2024 02:16:21 GMT</pubDate>
    </item>
    <item>
      <title>Pandas-Profiling 与 scikit-learn 发生冲突</title>
      <link>https://stackoverflow.com/questions/78814203/pandas-profiling-being-conflicted-with-scikit-learn</link>
      <description><![CDATA[当我尝试在 jupyter notebook 中安装 pandas profiling 时，我遇到一个错误，如下所示：“错误：pip 的依赖解析器目前没有考虑到所有已安装的软件包。此行为是以下依赖冲突的根源。
scikit-learn 1.5.1 需要 joblib&gt;=1.2.0，但您的 joblib 1.1.1 不兼容。”
现在，当我尝试升级 joblib 的版本时，我会遇到新的错误，如下所示：“错误：pip 的依赖解析器目前没有考虑到所有已安装的软件包。此行为是以下依赖冲突的根源。
pandas-profiling 3.2.0 需要 joblib~=1.1.0，但您的 joblib 1.4.2 不兼容。”
再次升级 irreparabled-learn &amp; scikit-learn 会给我同样的不兼容错误。
我该如何缓解这个错误
我希望 pandas profiling 能够正确安装，没有任何冲突和错误]]></description>
      <guid>https://stackoverflow.com/questions/78814203/pandas-profiling-being-conflicted-with-scikit-learn</guid>
      <pubDate>Wed, 31 Jul 2024 02:12:55 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn 预测需要很长时间</title>
      <link>https://stackoverflow.com/questions/78814028/sklearn-prediction-takes-forever</link>
      <description><![CDATA[我已经被这个问题困扰好几天了，真的需要一些帮助。我在 sklearn 中遇到了几种常见的机器学习方法的严重性能问题。我正在研究一个概率预测（二元分类）问题，数据集包含 500 万个观测值和 100 个特征，使用 sklearn 中的 LogisticRegression()、MLPClassifier()、RandomForestClassifier() 和 LinearSVC() 等模型。
例如，这是我用于 L2 逻辑回归的设置，使用交叉验证从网格 c_grid = [1e-15, 1e-10, 1e-5, 1e-1, 10] 中找到最佳正则化项 C：
lr = LogisticRegression(class_weight=class_weight,
solver=&#39;sag&#39;, # 我也尝试了 &#39;liblinear&#39;
max_iter=10000,
tol=0.1,
random_state=seed,
penalty=&#39;l2&#39;)

C = [1e-15, 1e-10, 1e-5, 1e-1, 10]
c_grid = {&quot;C&quot;: C}
c_grid = {k: v for k, v in c_grid.items() if v is not None}

...

cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True) 
clf = GridSearchCV(estimator=lr, 
param_grid=c_grid, 
scoring=&#39;roc_auc&#39;,
cv=cv, 
return_train_score=True).fit(X_train, Y_train) 
best_model = clf.best_estimator_
prob = clf.predict_proba(X_train)[:, 1]
pred = clf.predict(X_train)

但是整个训练过程耗时将近20个小时。对于这种大小的数据集来说，这是否正常，或者可能是由于参数或设置不正确造成的？例如，我调整了 LogisticRegression 中的各种参数，但似乎都没有改善这种情况。
此外，当我尝试使用 best_model 来计算测试结果时
prob = clf.predict_proba(X_test)[:, 1]
pred = clf.predict(X_test)

这似乎需要很长时间才能完成。我尝试使用类似这样的方法并行化该过程
X_test_batches = np.array_split(X_test, N)
args = [(best_model, batch) for batch in X_test_batches]

with Pool(N) as pool:
prob_batches = pool.map(predict_batch, args)
prob = np.concatenate(prob_batches)
pred = (prob &gt;= 0.5)

但它也没有太大帮助，所以最终我不得不手动实现我自己的预测函数（显然它只适用于物流，但不适用于我想要测试的其他模型）
z = np.dot(X_test, best_model.coef_.T) + best_model.intercept_
prob = 1 / (1 + np.exp(-z))

鉴于训练和测试都花费了不合理的长时间，我猜测问题可能出在 clf.predict_proba() 和 clf.predict() 上。但是，我希望 sklearn 能够有效处理包含数百万个观测值的数据集？如能得到任何帮助，我将不胜感激，谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78814028/sklearn-prediction-takes-forever</guid>
      <pubDate>Wed, 31 Jul 2024 00:04:22 GMT</pubDate>
    </item>
    <item>
      <title>如何让 Python 脚本在后台运行同时仍与前端交互？[关闭]</title>
      <link>https://stackoverflow.com/questions/78813752/how-to-have-a-python-script-run-in-the-background-while-still-interacting-with-t</link>
      <description><![CDATA[嘿，我有这个网站，它允许人们添加新的预测。首先，他们点击一个新的预测和广告数据（csvs），然后选择他们想要使用的机器学习模型，然后它应该运行python代码并将csv输出到具有相关预测参数的数据库。
我的问题是我应该如何运行python代码，因为这个代码可能需要几个小时才能运行，然后我必须考虑许多试图做出新预测的用户。我研究了任务调度和redis。我也听说我可以使用一些AWS服务，但我不确定这里最好的选择是什么，因为我想尽可能地防止内存错误和超时，同时确保代码在稳定的环境中运行。
顺便说一下，我使用flask作为后端并在前端做出反应]]></description>
      <guid>https://stackoverflow.com/questions/78813752/how-to-have-a-python-script-run-in-the-background-while-still-interacting-with-t</guid>
      <pubDate>Tue, 30 Jul 2024 21:37:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 opencv 检测该焊接带中的缺陷（例如孔洞）[关闭]</title>
      <link>https://stackoverflow.com/questions/78813507/how-can-i-detect-defectssuch-as-holes-in-this-welding-strip-using-opencv</link>
      <description><![CDATA[参考图：带孔的焊条：https://i.sstatic.net/VaVQX3th.jpg
正常焊条：https://i.sstatic.net/MBcyyIyp.jpg
我在使用 opencv 检测缺陷（如孔、不均匀性）时遇到问题。我是 opencv 新手，尝试过轮廓检测、边缘检测，但没有得到想要的结果。我想使用 opencv 构建一个算法，检测这些孔并标记它们，而不标记任何其他不必要的东西，这些东西不是缺陷。
这是我在代码中使用的方法
import cv2
import numpy as np
from matplotlib import pyplot as plt

# 加载图像
image_path = &quot;sample weld strip.jpg&quot;
image = cv2.imread(image_path)

# 将图像转换为 HSV 颜色空间
hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

# 定义焊缝条的 HSV 范围
lower_hsv = (1, 1, 1)
upper_hsv = (177, 255, 255)

# 应用 HSV 掩码
mask = cv2.inRange(hsv_image, lower_hsv, upper_hsv)
masked_image = cv2.bitwise_and(image, image, mask=mask)

# 转换为灰度
gray_image = cv2.cvtColor(masked_image, cv2.COLOR_BGR2GRAY)

# 应用高斯模糊
blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)

# 使用 Canny 进行边缘检测
edges = cv2.Canny(blurred_image, 50, 150)

# 查找轮廓
contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 在重要轮廓（孔洞）周围绘制边界框
holes_image = image.copy()
for contour in contours:
area = cv2.contourArea(contour)
if 10 &lt;area &lt; 25：# 根据您的需要调整此阈值
x, y, w, h = cv2.boundingRect(contour)
cv2.rectangle(holes_image, (x, y), (x+w, y+h), (0, 0, 255), 2)

我正在寻求有关此问题的帮助或指导。任何形式的意见或帮助都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78813507/how-can-i-detect-defectssuch-as-holes-in-this-welding-strip-using-opencv</guid>
      <pubDate>Tue, 30 Jul 2024 20:06:30 GMT</pubDate>
    </item>
    <item>
      <title>使用 chatgpt 从采访脚本中提取问题和答案 [关闭]</title>
      <link>https://stackoverflow.com/questions/78812091/extracting-question-and-answer-from-the-interview-script-using-chatgpt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78812091/extracting-question-and-answer-from-the-interview-script-using-chatgpt</guid>
      <pubDate>Tue, 30 Jul 2024 13:46:06 GMT</pubDate>
    </item>
    <item>
      <title>上传多张图片时 React 前端和 Flask 后端出现 CORS 策略错误</title>
      <link>https://stackoverflow.com/questions/78809866/cors-policy-error-with-react-frontend-and-flask-backend-when-uploading-multiple</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78809866/cors-policy-error-with-react-frontend-and-flask-backend-when-uploading-multiple</guid>
      <pubDate>Tue, 30 Jul 2024 04:57:46 GMT</pubDate>
    </item>
    </channel>
</rss>