<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 23 Jul 2024 12:29:13 GMT</lastBuildDate>
    <item>
      <title>多输出回归可根据 ROAS 和其他功能预测成本和收入</title>
      <link>https://stackoverflow.com/questions/78783100/multi-output-regression-to-predict-cost-and-revenue-from-roas-and-other-features</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78783100/multi-output-regression-to-predict-cost-and-revenue-from-roas-and-other-features</guid>
      <pubDate>Tue, 23 Jul 2024 11:10:59 GMT</pubDate>
    </item>
    <item>
      <title>找到正确的数据初始化形状，无需大量填充或减少数据</title>
      <link>https://stackoverflow.com/questions/78783065/finding-the-right-shape-for-data-initialization-without-padding-a-lot-or-without</link>
      <description><![CDATA[我正在构建一个 RNN 模型，用于分析第二天的降雨概率，以及每天的降雨量。我有一个张量 (1, 542375)，我想将其重塑为 (状态数，46, 366)（46 年，每年 366 天）。问题是，当我想要训练时，将状态数转换为批大小是一项艰巨的任务，因为 batch_size 需要为 2^n，而状态数远不及这个数字......我相信我有两个选择：或者我将批大小设为 64，我必须填充 500000 个输入，这在计算上效率较低；或者我将批大小设为 32，如果我的数据为 3623 个输入，我必须删除 3623 个输入...还有其他选择吗？在哪里可以了解有关组织数据的更多信息？
32 * 46 * 366 = 548 752
542375 - 538752 = 3623
64 * 46 * 366 = 1077504
10777594 - 542375 = 535129
填充 535129 个输入
或
删除 3623 个参数]]></description>
      <guid>https://stackoverflow.com/questions/78783065/finding-the-right-shape-for-data-initialization-without-padding-a-lot-or-without</guid>
      <pubDate>Tue, 23 Jul 2024 11:02:21 GMT</pubDate>
    </item>
    <item>
      <title>在 vscode 上导入 tensorflow 时出现 NotFoundError（IMAC M1）</title>
      <link>https://stackoverflow.com/questions/78782546/notfounderror-while-importing-tensorflow-on-vscode-imac-m1</link>
      <description><![CDATA[有人能解释一下这个错误或者帮我解决这个问题吗？
代码：
# 导入库
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf

错误：
------------------------------------------------------------------------------
NotFoundError Traceback (最近一次调用最后一次)
Cell In[17]，第 4 行
2 import cv2
3 import matplotlib.pyplot as plt
----&gt; 4 将 tensorflow 导入为 tf
5 将 tensorflow_hub 导入为 hub
6 将 numpy 导入为 np

文件 /opt/homebrew/lib/python3.9/site-packages/tensorflow/__init__.py:434
432 _plugin_dir = _os.path.join(_s, &quot;tensorflow-plugins&quot;)
433 如果 _os.path.exists(_plugin_dir):
--&gt; 434 _ll.load_library(_plugin_dir)
435 # 加载可插拔设备库
436 _ll.load_pluggable_device_library(_plugin_dir)

文件 /opt/homebrew/lib/python3.9/site-packages/tensorflow/python/framework/load_library.py:151，在 load_library(library_location) 中
148 kernel_libraries = [library_location]
150 for lib in kernel_libraries:
--&gt; 151 py_tf.TF_LoadLibrary(lib)
153 else:
154 raise OSError(
155 errno.ENOENT,
156 &#39;用于加载内核库的文件或文件夹不存在。&#39;,
157 library_location)

NotFoundError: dlopen(/opt/homebrew/lib/python3.9/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): 未在平面命名空间中找到符号 (__ZN10tensorflow8internal10LogMessage16VmoduleActivatedEPKci)

大多数解决方案包括创建一个新的虚拟环境或/和执行

pip3 install tensorflow-macos

pip3 install tensorflow-metal


没有帮助或更改错误
多次刷新 VSCode]]></description>
      <guid>https://stackoverflow.com/questions/78782546/notfounderror-while-importing-tensorflow-on-vscode-imac-m1</guid>
      <pubDate>Tue, 23 Jul 2024 09:04:05 GMT</pubDate>
    </item>
    <item>
      <title>如何利用深度信息改进距离估计</title>
      <link>https://stackoverflow.com/questions/78782389/how-to-improve-distance-estimation-with-depth-information</link>
      <description><![CDATA[我一直在进行距离估计项目，但我遇到了 OpenCV 等库所采用的方法的问题。这些库基于像素测量来估计距离，这会导致不准确的结果。例如，靠近相机的两个物体与远离相机的两个物体具有相同的基于像素的距离，这是不正确的。
为了解决这个问题，我想结合深度估计来区分近处和远处的物体，从而提高距离估计的准确性。
但是，我找不到任何论文或代码示例来演示如何实现这一点。有人能给我指出相关资源或提供有关如何实现基于深度的距离估计的指导吗？]]></description>
      <guid>https://stackoverflow.com/questions/78782389/how-to-improve-distance-estimation-with-depth-information</guid>
      <pubDate>Tue, 23 Jul 2024 08:31:39 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们使用正确的参数网格来微调预测精度？</title>
      <link>https://stackoverflow.com/questions/78781865/why-we-use-correct-parameter-grid-for-fine-tune-a-prediction-accuracy</link>
      <description><![CDATA[parameters = {
&#39;n_estimators&#39;: [100, 200],
&#39;learning_rate&#39;: [0.01, 0.1],
&#39;max_depth&#39;: [3, 5],
&#39;subsample&#39;: [0.8, 1.0],
&#39;colsample_bytree&#39;: [0.8, 1.0],
&#39;gamma&#39;: [0, 0.1],
&#39;min_child_weight&#39;: [1, 2]
}

并且需要更多时间来拟合
对此的解释和用例。
grid_search.fit(X_train, y_train) 当我执行此行时，它需要超过 45 分钟，但仍然没有完成
什么是 GridSearchCV？它的用例？什么是 cv=3，为什么我们给它以及什么是评分？为什么我们使用 GridSearchCV？]]></description>
      <guid>https://stackoverflow.com/questions/78781865/why-we-use-correct-parameter-grid-for-fine-tune-a-prediction-accuracy</guid>
      <pubDate>Tue, 23 Jul 2024 06:28:25 GMT</pubDate>
    </item>
    <item>
      <title>使用数组作为特征/独立变量？[关闭]</title>
      <link>https://stackoverflow.com/questions/78781575/using-an-array-as-a-feature-independent-variable</link>
      <description><![CDATA[假设我有一个数据框：

主题 ID (int)
年龄 (int)
性别 (int 1- 代表男性，2- 代表女性)
Pearson CC 表示功能网络 (矩阵)

这意味着我有一个 PearsonCC 数组数组。例如：



p_id
性别
PearsonCC




128_S_0200
M
[0.5052435694128596, 0.3375816208945487, 0.206...


003_S_0908
F
[-0.18955977794142087, 0.01652734870786999, -0...


141_S_1052
F
[0.0562331642358682, 0.5698911953687733, -0.17... -0...



所以我明白我们需要展平矩阵/矢量化上三角
然后将其转换为 numpy 以便进行特征缩放。我尝试过类似这样的方法：
X_mat_train = np.vstack(X_matrix_train[&#39;Z_new&#39;].values)
X_mat_test = np.vstack(X_matrix_train[&#39;Z_new&#39;].values)
我的问题是：
在使用它来拟合模型时，我是否应该将向量转换回原始形式？
因为我在网上搜索时得到了混合的结果。我很困惑，不知道对于这个特定的列，什么格式最好？
任何帮助都非常感谢！
如果我使用 sklearn 的 Logistic 回归：
model = LogisticRegression(max_iter=1000)
model.fit(X_train_final, y_train)
我收到一个错误，提示 PearsonCC 列可能只能是 int/float：
TypeError Traceback（最近一次调用最后一次）
TypeError：只有 size-1 数组可以转换为 Python 标量

对于扩展该功能，这是我的方法：
scaler = StandardScaler()

X_mat_train = np.vstack(X_matrix_train[&#39;Z_new&#39;].values)
X_mat_test = np.vstack(X_matrix_train[&#39;Z_new&#39;].values)

X_mat_train_scaled = scaler.fit_transform(X_mat_train)
X_mat_test_scaled = scaler.transform(X_mat_test)

我只是不确定如何将这个 X_mat_train 转换回 ML/Regression 可接受的格式]]></description>
      <guid>https://stackoverflow.com/questions/78781575/using-an-array-as-a-feature-independent-variable</guid>
      <pubDate>Tue, 23 Jul 2024 04:35:17 GMT</pubDate>
    </item>
    <item>
      <title>人工智能是出路吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78781284/is-artificial-intelligence-the-way-to-go</link>
      <description><![CDATA[大家好吗？我是一名电影行业专业人士，每个人都知道，目前从事这个行业很糟糕。有一件事让我着迷，那就是人工智能。我认为它远没有现在被描绘成的可怕流行语那么可怕。就我个人而言，我相信对于适应的电影制作人来说，它将是一个很好的工具。问题是，大多数人不会因为恐惧而适应。
话虽如此，我想更多地了解人工智能。我不知道从哪里开始，在这方面我完全是个新手。我希望真正精通它，这样我在未来几年就会有优势。有人有什么建议吗？我应该学习什么关于人工智能的知识？我应该学习如何使用生成式人工智能吗？如何“制造”生成式人工智能？目标应该是放弃电影制作事业，转而进入一家更依赖人工智能的公司吗？
请记住，我是一个完全的新手。目标是获得一套有用的新技能。
我还没有尝试任何东西，因为我不确定如何开始。我应该进入机器学习领域吗？我应该学习 Python 吗？有什么期刊我应该关注吗？]]></description>
      <guid>https://stackoverflow.com/questions/78781284/is-artificial-intelligence-the-way-to-go</guid>
      <pubDate>Tue, 23 Jul 2024 01:55:46 GMT</pubDate>
    </item>
    <item>
      <title>使用按位或自定义损失函数进行多标签分类</title>
      <link>https://stackoverflow.com/questions/78771566/multi-label-classification-using-bit-wise-or-custom-loss-function</link>
      <description><![CDATA[我对这个公式和多标签分类的按位或有疑问。我应该在这里对所有 R 使用连续输出值，还是只需要它们来计算 N，并在 A = ... 中对 R 使用二进制阈值？因为描述令人困惑，首先它指出“L 和 R 是 N × c 二进制矩阵，由独热编码的地面实况标签和阈值模型输出分数组成”，然后在 N 的等式中“二进制矩阵 R 现在可以使用原始输出分数 pc 替换为其连续等效值，如下所示”。 （Vicar 等人的论文“使用具有全局跳过连接和自定义损失函数的卷积网络进行心电图异常识别”）
]]></description>
      <guid>https://stackoverflow.com/questions/78771566/multi-label-classification-using-bit-wise-or-custom-loss-function</guid>
      <pubDate>Sat, 20 Jul 2024 00:39:15 GMT</pubDate>
    </item>
    <item>
      <title>如何纠正 Reshape 函数中的错误</title>
      <link>https://stackoverflow.com/questions/78749448/how-do-you-correct-the-error-in-reshape-function</link>
      <description><![CDATA[当我拟合各种人工神经网络时，TLNN 的代码显示不正确，尤其是包含重塑函数的行。这有什么问题吗？更改数据集是否意​​味着重塑函数不起作用并显示此错误
def Forecast_TLNN(model, time_lagged_points, last_sequence, Future_steps):
Forecasted_values = []
max_lag = max(time_lagged_points)
for i in range(future_steps):
input_sequence = [last_sequence[max_lag - p] for p in time_lagged_points]
Forecasted_value = model.predict(np.reshape(input_sequence, (1, len(input_sequence))))
Forecasted_values.append(forecasted_value[0][0])
last_sequence = last_sequence[1:] + [forecasted_value[0][0]]
return Forecasted_values

错误显示在以下行中：forecasted_value = model.predict(np.reshape(input_sequence, (1, len(input_sequence))))
我似乎无法在互联网上找到有关此代码的任何更正。
 ---------------------------------------------------------------------------------------
ValueError Traceback (most recent call last)
Cell In[83], line 13
10 # look_back, hidden_​​nodes, output_nodes, epochs, batch_size, future_steps
11 parameters_LSTM = [[1,2,3,4,5,6,7,8,9,10,11,12,13], [3,4,5,6], [1], [300], [20], [future_steps]]
---&gt; 13 RMSE_info = compare_ANN_methods(rainfall_data, test_rainfall_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps)

单元格 In[79]，第 6 行，在 compare_ANN_methods(rainfall_data, test_rainfall_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps) 中
3 information_FNN_df = get_accuracies_FNN(rainfall_data, test_rainfall_data, parameters_FNN, scaler)
4 optimal_params_FNN = analyze_results(information_FNN_df, test_rainfall_data, &#39;FNN&#39;)
----&gt; 6 information_TLNN_df = get_accuracies_TLNN(rainfall_data, test_rainfall_data, parameters_TLNN, scaler)
7 optimal_params_TLNN = analyze_results(information_TLNN_df, test_rainfall_data, &#39;TLNN&#39;)
9 information_SANN_df = get_accuracies_SANN(rainfall_data, test_rainfall_data, parameters_SANN, scaler)

单元格 In[55]，第 21 行，在 get_accuracies_TLNN(rainfall_data, test_rainfall_data, parameters, scaler)
18 batch_size = param[4]
19 future_steps = param[5]
---&gt; 21 model_TLNN, Forecasted_values_TLNN = TLNN(rainfall_data, time_lagged_points, hidden_​​nodes, output_nodes, epochs, batch_size, Future_steps, scaler)
23 y_true = test_rainfall_data.iloc[:future_steps].Precipitation
24 mse, mae, mape, rmse = calculate_performance(y_true, Forecasted_values_TLNN)

单元格 In[53]，第 9 行，在 TLNN(data, time_lagged_points, hidden_​​nodes, output_nodes, epochs, batch_size, Future_steps, scaler)
6 model_TLNN = train_model(model_TLNN, X_train, y_train, epochs, batch_size)
8 max_lag = max(time_lagged_points)
----&gt; 9 预测值_TLNN = 预测值_TLNN(model_TLNN, time_lagged_points, 
10 列表(数据[-max_lag:]), 未来步骤=未来步骤)
11 预测值_TLNN = 列表(scaler.inverse_transform([预测值_TLNN])[0])
13 返回 model_TLNN, 预测值_TLNN

单元格 In[51]，第 6 行，在预测值_TLNN(模型, time_lagged_points, last_sequence, 未来步骤)
4 for i in range(future_steps):
5 输入序列 = [last_sequence[max_lag - p] for p in time_lagged_points]
----&gt; 6 Forecasted_value = model.predict((np.reshape(input_sequence, (1, len(input_sequence)))))
7 Forecasted_values.append(forecasted_value[0][0])
8 last_sequence = last_sequence[1:] + [forecasted_value[0][0]]

文件 ~\anaconda3\Lib\site-packages\numpy\core\fromnumeric.py:285，在 reshape(a, newshape, order) 中
200 @array_function_dispatch(_reshape_dispatcher)
201 def reshape(a, newshape, order=&#39;C&#39;):
202 &quot;&quot;&quot;
203 为数组赋予新形状而不更改其数据。
204 
(...)
283 [5, 6]])
284 &quot;&quot;&quot;
--&gt; 285 返回 _wrapfunc(a, &#39;reshape&#39;, newshape, order=order)

文件 ~\anaconda3\Lib\site-packages\numpy\core\fromnumeric.py:56，位于 _wrapfunc(obj, method, *args, **kwds)
54 bound = getattr(obj, method, None)
55 如果 bound 为 None:
---&gt; 56 return _wrapit(obj, method, *args, **kwds)
58 try:
59 return bound(*args, **kwds)

File ~\anaconda3\Lib\site-packages\numpy\core\fromnumeric.py:45, in _wrapit(obj, method, *args, **kwds)
43 except AttributeError:
44 wrap = None
---&gt; 45 result = getattr(asarray(obj), method)(*args, **kwds)
46 if wrap:
47 if not isinstance(result, mu.ndarray):

ValueError: 设置带有序列的数组元素。请求的数组在 1 维之后具有非均匀形状。检测到的形状为 (5,) + 非均匀部分。
]]></description>
      <guid>https://stackoverflow.com/questions/78749448/how-do-you-correct-the-error-in-reshape-function</guid>
      <pubDate>Mon, 15 Jul 2024 10:54:58 GMT</pubDate>
    </item>
    <item>
      <title>如何在全连接物理信息神经网络中选择超参数？[关闭]</title>
      <link>https://stackoverflow.com/questions/78739152/how-to-select-hyperparameters-in-fully-connected-physics-informed-neural-network</link>
      <description><![CDATA[我创建了具有物理约束的全连接 NN（即物理信息神经网络或 PINN）。但我完全不明白每层应该使用多少层和多少个神经元？
我尝试了不同数量的层（5-9），每层有不同数量的神经元（100-200）。据我所知，这也取决于输出大小。您在 PINN 方面有经验吗？可以建议最佳超参数吗？]]></description>
      <guid>https://stackoverflow.com/questions/78739152/how-to-select-hyperparameters-in-fully-connected-physics-informed-neural-network</guid>
      <pubDate>Fri, 12 Jul 2024 07:47:06 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Tensorflow/keras 上添加 InstanceNormalization</title>
      <link>https://stackoverflow.com/questions/68088889/how-to-add-instancenormalization-on-tensorflow-keras</link>
      <description><![CDATA[我是 TensorFlow 和 Keras 的新手，我一直在制作扩张的 resnet，并想在层上添加实例规范化，但我做不到，因为它一直抛出错误。
我正在使用 tensorflow 1.15 和 keras 2.1。我注释掉了可以工作的 BatchNormalization 部分，并尝试添加实例规范化，但找不到模块。
非常感谢您的建议


来自 keras.layers 导入 Conv2D
来自 keras.layers.normalization 导入 BatchNormalization
来自 keras.optimizers 导入 Nadam、Adam
来自 keras.layers 导入 Input、Dense、Reshape、Activation、Flatten、Embedding、Dropout、Lambda、add、concatenate、Concatenate、ConvLSTM2D、LSTM、average、MaxPooling2D、multiply、MaxPooling3D
来自 keras.layers 导入 GlobalAveragePooling2D、Permute
来自 keras.layers.advanced_activations 导入 LeakyReLU、PReLU
来自 keras.layers.convolutional 导入 UpSampling2D、Conv2D、 Conv1D
从 keras.models 导入 Sequential、Model
从 keras.utils 导入 multi_gpu_model
从 keras.utils.generic_utils 导入 Progbar
从 keras.constraints 导入 maxnorm
从 keras.activations 导入 tanh、softmax
从 keras 导入 metrics、initializers、utils、regularizers
将 tensorflow 导入为 tf
将 numpy 导入为 np
导入 math
导入 os
导入 sys
导入 random
将 keras.backend 导入为 K
epsilon = K.epsilon()

def basic_block_conv2D_norm_elu(filters、kernel_size、kernel_regularizer=regularizers.l2(1e-4)、act_func=&quot;elu&quot;、normalize=&quot;Instance&quot;、dropout=&#39;0.15&#39;,
strides=1、use_bias = True,kernel_initializer = &quot;he_normal&quot;,_dilation_rate=0):
def f(input):
if kernel_regularizer == None:
if _dilation_rate == 0:
conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,
padding=&quot;same&quot;, use_bias=use_bias)(input)
else:
conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,
padding=&quot;same&quot;, use_bias=use_bias,dilation_rate=_dilation_rate)(input)
else:
if _dilation_rate == 0:
conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,
kernel_initializer=kernel_initializer, padding=&quot;same&quot;, use_bias=use_bias,
kernel_regularizer=kernel_regularizer)(输入)
else:
conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,
kernel_initializer=kernel_initializer, padding=&quot;same&quot;, use_bias=use_bias,
kernel_regularizer=kernel_regularizer, dilation_rate=_dilation_rate)(输入)
if dropout != None:
dropout_layer = Dropout(0.15)(conv)

if normalize == None 且 dropout != None:
norm_layer = conv(dropout_layer)
else:
norm_layer = InstanceNormalization()(dropout_layer)
# norm_layer = BatchNormalization()(dropout_layer)
return激活（act_func）（norm_layer）
返回 f
]]></description>
      <guid>https://stackoverflow.com/questions/68088889/how-to-add-instancenormalization-on-tensorflow-keras</guid>
      <pubDate>Tue, 22 Jun 2021 18:14:05 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中使用具有焦点损失的类权重来处理多类分类的不平衡数据集</title>
      <link>https://stackoverflow.com/questions/64751157/how-to-use-class-weights-with-focal-loss-in-pytorch-for-imbalanced-dataset-for-m</link>
      <description><![CDATA[我正在研究语言任务的多类分类（4 个类别），并且我正在使用 BERT 模型进行分类任务。我正在关注这篇博文NLP 迁移学习：针对文本分类的微调 BERT。 我的 BERT Fine Tuned 模型返回 nn.LogSoftmax(dim=1)。
我的数据非常不平衡，因此我使用 sklearn.utils.class_weight.compute_class_weight 来计算类别的权重，并使用 Loss 中的权重。
class_weights = compute_class_weight(&#39;balanced&#39;, np.unique(train_labels), train_labels)
weights= torch.tensor(class_weights,dtype=torch.float)
cross_entropy = nn.NLLLoss(weight=weights) 


我的结果不太好，因此我想用 Focal Loss 进行实验，并为 Focal Loss 编写了一个代码。
class FocalLoss(nn.Module):
def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):
super(FocalLoss, self).__init__()
self.alpha = alpha
self.gamma = gamma
self.logits = logits
self.reduce = reduce

def forward(self, input, target):
BCE_loss = nn.CrossEntropyLoss()(inputs, target)

pt = torch.exp(-BCE_loss)
F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss

if self.reduce:
return torch.mean(F_loss)
else:
return F_loss

我现在有 3 个问题。首先也是最重要的是

我应该将类权重与 Focal Loss 结合使用吗？
如果我必须在这个 Focal Loss 中实现权重，我可以在 nn.CrossEntropyLoss() 中使用 weights 参数吗？
如果这个实现不正确，那么包括权重在内的正确代码应该是什么（如果可能）
]]></description>
      <guid>https://stackoverflow.com/questions/64751157/how-to-use-class-weights-with-focal-loss-in-pytorch-for-imbalanced-dataset-for-m</guid>
      <pubDate>Mon, 09 Nov 2020 11:53:49 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以获得包含世界上几乎所有国家护照的护照图像数据集？</title>
      <link>https://stackoverflow.com/questions/60039938/where-can-i-get-passport-images-dataset-that-contain-passport-of-almost-all-coun</link>
      <description><![CDATA[我正在训练一个 OCR 模型，用于从护照中识别 MRZ。为了训练我的模型以获得更高的准确性，我需要用尽可能多的图片来训练它。我试图在 KAGGLE 上找到护照的数据集，但找不到。
有人能告诉我从哪里可以获得包含几乎所有国家或北美和南美护照的护照图像数据集吗？
非常感谢您的帮助。
祝好，
Asma]]></description>
      <guid>https://stackoverflow.com/questions/60039938/where-can-i-get-passport-images-dataset-that-contain-passport-of-almost-all-coun</guid>
      <pubDate>Mon, 03 Feb 2020 13:11:08 GMT</pubDate>
    </item>
    <item>
      <title>在 MultiLabelBinarizer 中获取计数</title>
      <link>https://stackoverflow.com/questions/56372324/getting-counts-in-multilabelbinarizer</link>
      <description><![CDATA[如何获取 MultiLabelBinarizer 中的项目数？
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()

pd.DataFrame(mlb.fit_transform([(1,1,2), (3,3,2,5)]),columns=mlb.classes_)

Out[0]: 
1 2 3 5
0 1 1 0 0
1 0 1 1 1

与此相反，我想要获取
Out[0]: 
1 2 3 5
0 2 1 0 0
1 0 1 2 1

因为 1 在第 1 行重复 2 次，而 3 在第 1 行重复 2 次]]></description>
      <guid>https://stackoverflow.com/questions/56372324/getting-counts-in-multilabelbinarizer</guid>
      <pubDate>Thu, 30 May 2019 05:47:14 GMT</pubDate>
    </item>
    <item>
      <title>序列长度在 LSTM 中的作用</title>
      <link>https://stackoverflow.com/questions/46980058/the-role-of-sequence-length-in-lstms</link>
      <description><![CDATA[我有时间序列数据，一个很长的序列，比如说有 30,000 个数据点。
对于 Tensorflow 中的 LSTM，输入形状为 [batch_size、time_steps、features]。我的批处理大小等于 1，因为我只有一个时间序列。
现在我的问题是关于序列长度的作用。
我可以使用 timestep=1 将整个时间序列传递到 tensorflow LSTM 中，在这种情况下，整个时间序列将逐一插入到 LSTM 中。或者我可以使用一些 &gt;1 的时间步长，并使其成为序列到序列模型。
这样，我可以一次插入 7 个数据点（= 一周的数据）并预测 7 个输出（= 未来一周）。
问题是，我们知道 LSTM 的隐藏状态最终会记住整个数据集（如果我们进行足够多的训练）。那么两者之间有什么区别呢？
a) timesteps=1，我预测一个时期，并将预测重新插入神经网络 7 次，
b) timesteps=7，我预测一个 7 的序列（= 一周）。]]></description>
      <guid>https://stackoverflow.com/questions/46980058/the-role-of-sequence-length-in-lstms</guid>
      <pubDate>Fri, 27 Oct 2017 16:55:09 GMT</pubDate>
    </item>
    </channel>
</rss>