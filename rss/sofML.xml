<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 17 Mar 2025 18:24:21 GMT</lastBuildDate>
    <item>
      <title>无法使KERAS CV对象检测器（Yolov8）和超级分析</title>
      <link>https://stackoverflow.com/questions/79515278/unable-to-get-keras-cv-object-detector-yolov8-to-perform-as-well-as-ultralytic</link>
      <description><![CDATA[我遵循已发表的Keras CV计算机视觉教程（ https://keras.io/examples/examples/vision/vision/yolov8/yolov8/yolov8/ yolo v8我希望得到一些帮助的问题。
问题是，使用KERAS CV中的Yolo V8检测器，地图得分非常低（1-10％）。  在训练和推断期间，我看到非最大抑制作用似乎无法正常工作。  我看到，具有相同置信度得分的同一对象有几个重叠的边界框。  更改NMS设置并不能改善这一点。
为了验证我的数据集，我尝试使用Roboflow和Ultrytics库使用我的数据集微调模型，并能够获得约71％-91.4％的地图。数据集大小为586，每个图像的每个类别为4个类别（故意避免类不平衡问题）。
这是我在训练和调整NMS后如何运行推理的一个示例：
在
    bounding_box_format = bounding_box_format，
    from_logits = true，＃或false
    iou_threshold = 0.5，＃尝试在此处调整所有内容 
    信任_threshold = 0.5，
    max_detections = 10
）
 
我在训练期间（以及其他）使用pycococalback：
  callbacks = [
        keras_cv.callbacks.pycococallback（＃可可指标（ap/ar @ iou）
            val_ds，
            bounding_box_format = bounding_box_format，
        ），
        keras.callbacks.tensorboard（＃培训进度可视化
            log_dir =＆quot; triending_logs＆quot
        ），
        keras.callbacks.modelcheckpoint（
            filepath =＆quot ;/ model/best_model.keras＆quot;
            Monitor =; val_ap＆quot;
            mode =“最大＃”，＃保存地图最大化时
            save_best_only = true，＃仅保留最佳模型
            详细= 1，
        ），
        keras.callbacks.earlystopping（
            耐心= 20，＃在25个时代不改进后停止
            Monitor =; val_ap＆quot;
            mode =“最大监视器最大地图”
            详细= 1，
            Restore_best_weights = true，＃恢复最佳时期的重量
        ），
        ＃visualizedEtections（），＃在每个时期之后的视觉验证
    ]，，
 
我如何训练模型的示例：
  backbone = keras_cv.models.yolov8backbone.from_preset（
    ＆quot&#39;yolo_v8_xs_backbone_coco＆quot＆quort load_weights = true
）

prediction_decoder = keras_cv.layers.nonmaxsuppression（
    bounding_box_format = bounding_box_format，
    from_logits = false，
    iou_threshold = 0.5，
    信心_threshold = 0.6，
    max_detections = 200
）

模型= keras_cv.models.yolov8detector（
    num_classes = len（class_ids），
    bounding_box_format = bounding_box_format，
    骨干=骨干，
    fpn_depth = 2，
    prediction_decoder = prediction_decoder，
）

initial_learning_rate = 0.001
lr_schedule = keras.optimizers.schedules.cosinedecay（
    initial_learning_rate，
    decay_steps = 1000，
    alpha = 0.0
）
优化器= keras.optimizers.adam（Learning_rate = lr_schedule，global_clipnorm = 10.0）

model.compile（
    classification_loss = keras.losses.categoricalcrossentropy（），
    box_loss = keras_cv.losses.ciouloss（bounding_box_format = bounding_box_format），
    优化器=优化器，
）
 
我尝试过：

检查以确保边界框和标签正确。
使用不同的优化器，类/损失功能等。
有和没有增强。
通过Roboflow，Keras/Tensorflow或两者使用增强。
带有和没有预训练重量的不同骨干。
不同的NMS设置（置信度，来自_logits和iou）。

我期望KERAS CV中的Yolov8检测器的性能能够在某种程度上执行超级实现。  由于我的增强管道可能不像超级词那样稳健，但这似乎是一个巨大的差距。
我无法弄清楚我似乎被忽略了！缩小这一巨大差距的任何帮助将不胜感激！让我知道您是否需要更多信息！]]></description>
      <guid>https://stackoverflow.com/questions/79515278/unable-to-get-keras-cv-object-detector-yolov8-to-perform-as-well-as-ultralytic</guid>
      <pubDate>Mon, 17 Mar 2025 17:04:10 GMT</pubDate>
    </item>
    <item>
      <title>在R数据集中创建人工差距[重复]</title>
      <link>https://stackoverflow.com/questions/79515036/creating-artificial-gaps-in-r-dataset</link>
      <description><![CDATA[我正在使用随机森林处理数据，我正在尝试在数据集中创建随机人造间隙，以便我可以测试随机森林预测的准确性。
  timestamp＆lt;  -  C（2001：2020）
Ch4_flux＆lt;  -  C（67.36，66.39，65.39，64.41，63.52，62.76，62.16,61.76，61.54,61.54,61.53,61.7,7,62.05,62.05,62.05,62.52
Ch4_flux_gaps＆lt;  -  c（67.36，66.39，65.39，64.41，63.52，62.76，62.16,61.76，61.54,61.54,61.53,61.7,7,7,62.055,62.52 66.32）
距离＆lt;  -  c（1000,1000,1000,125.35,1000,1000,1000,1000,5.50,1000,1000,1000,1000，1000,1000,179.65,1000,1000,1000,1000,1000,1000）
CONNUM＆LT;  -  C（0，0，0，30，0，0，0，81，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0）
dd＆lt;  -  data.Frame（Timestamp，ch4_flux，ch4_flux_gaps，距离，cownum）
 
在上面的示例数据中，ch4_flux和ch4_flux_gap是相同的列，因为我将在唯一的 中差距 ch4_flux_gaps列，然后比较它们。我希望将差距添加到5-10％的行。我已经看到了有关如何添加整个差距的信息，但没有仅针对一个列并使差距是随机的信息。
我希望ch4_flux_gaps列以后看起来像这样：
  ch4_flux_gaps＆lt;  -  C（67.36，66.39，65.39，65.39，Na，63.52，Na，62.16,61.76，61.54,61.53,61.53,61.7,7,62.05
 ]]></description>
      <guid>https://stackoverflow.com/questions/79515036/creating-artificial-gaps-in-r-dataset</guid>
      <pubDate>Mon, 17 Mar 2025 15:36:21 GMT</pubDate>
    </item>
    <item>
      <title>连接到Zenml仪表板</title>
      <link>https://stackoverflow.com/questions/79514826/connecting-to-zenml-dashboard</link>
      <description><![CDATA[通过运行 zenml login -local 。
我的Zenml版本是0.75.0。我在Windows机器上。

错误：作为背景过程不是本地运行zenml服务器
在Windows上支持。请使用“阻止”。标志以运行
处于阻塞模式的服务器，或通过Docker容器运行服务器
设置“  -  docker”而是。

然后，我进行了 zenml登录-local -blocking ，但获得了以下错误：

 ModulenotFoundError：处理过程中没有名为“ JWT”的模块
最重要的是，发生了另一个例外：RuntimeError：本地
守护程序Zenml Server提供商不可用，因为Zenml Server
在您的机器上似乎不可用。这可能是
因为ZenML是在没有可选Zenml Server的情况下安装的
依赖性。要安装丢失的依赖项运行 pip install; zenml [server] == 0.75.0; 。

安装 pip install; zenml [server] == 0.75.0;  之后保留错误。
我是Zenml的新手，谢谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/79514826/connecting-to-zenml-dashboard</guid>
      <pubDate>Mon, 17 Mar 2025 14:09:13 GMT</pubDate>
    </item>
    <item>
      <title>尝试从补丁重建原始图像</title>
      <link>https://stackoverflow.com/questions/79514560/trying-to-reconstruct-original-image-from-patches</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79514560/trying-to-reconstruct-original-image-from-patches</guid>
      <pubDate>Mon, 17 Mar 2025 12:19:35 GMT</pubDate>
    </item>
    <item>
      <title>如何在Pytorch中合并Pinn（物理中非机器学习）中的ODE系统的多个变化的初始条件？</title>
      <link>https://stackoverflow.com/questions/79513461/how-to-incorporate-multiple-changing-initial-conditions-for-a-system-of-odes-in</link>
      <description><![CDATA[我有两种。第一个ODE的初始条件等于第二个ode的最终值。第二个ODE的初始条件是第一个ODE的最终值。这些初始条件也会改变。我将如何将其整合到我的典型Pinn编码脚本中？]]></description>
      <guid>https://stackoverflow.com/questions/79513461/how-to-incorporate-multiple-changing-initial-conditions-for-a-system-of-odes-in</guid>
      <pubDate>Mon, 17 Mar 2025 00:59:55 GMT</pubDate>
    </item>
    <item>
      <title>如何使用Penzai和Optax进行每个参数的学习率</title>
      <link>https://stackoverflow.com/questions/79512816/how-to-scale-learning-rates-per-parameter-with-penzai-and-optax</link>
      <description><![CDATA[我想训练我在 nofollow noreferrer”&gt; penzai 元数据，例如。像这样：
 参数（
    label =&#39;mlp/affine_0/linear.weights&#39;，
    value =＆lt;名为float32（|功能：784，features_out：128）（包装jax.array）＆gt; ,,
    元数据= {&#39;Learning_rate&#39;：0.0012755102040816326}，
）
 
我使用penzai的 statefultrainer 进行培训，并声明这样的优化器：
  optax.chain（链）（
    optax.scale_by_adam（），
    scale_by_metadata_value（&#39;Learning_rate＆quot;），
    optax.scale_by_learning_rate（0.01），
）
 
我在哪里定义 scale_by_metadata_value 像这样：
  def scale_by_metadata_value（metadata_field_name：str）：
    def init_fn（params）：
        Learning_rates = jax.tree.map（
            lambda param：param.metadata [metadata_field_name]，
            参数，
            is_leaf =（lambda节点：isInstance（node，pz.parametervalue））
        ）
        返回{＆quot; learning_rates＆quot;：learning_rates}

    def Update_fn（更新，状态，参数）：
        del params
        更新= jax.tree.map（
            ＃这是TypeError抛出的地方：
            Lambda LR，G：lr * g，state [＆quot; learne_rates; quot;]，更新
        ）
        返回更新，状态

    返回optax.gradienttransformation（init_fn，update_fn）
 
但是，当我执行训练步骤时，我会得到
  typeError： *：&#39;jaxlib.xla_extension.arrayimpl&#39;and&#39;parametervalue&#39;
 
我特别令人困惑，因为当我删除 scale_by_metadata_value（&#39;Learning_rate＆quot&#39;&#39;） line时，一切都起作用，尽管optax.scale_by_learning_rate（0.01）与我使用 scal_by_metadataa_value 
 实现 scale_by_metadata_value ？ 的正确/最佳方法是什么
这是一个最小的失败示例（Penzai版本0.2.4）：
 导入penzai.toolshed.basic_training
进口Penzai
导入Penzai.pz作为PZ
导入JAX
导入jax.numpy作为jnp
导入Optax
导入numpy作为NP


型号= pz.nn.linear（
    权重= Pz.参数（参数
        value = pz.nx.wrap（np.ones（（（8，4））），“功能”，“功能”;
        标签=“线性”
        metadata = {＆quot; learning_rate＆quot;：0.5}，
    ），
    in_axis_names =（“功能”，），
    out_axis_names =（; quot; features_out＆quort;），，
）


DEF SOFTMAX_CROSS_ENTROPY_LOSS（
    型号，rng，state，current_input，current_target：pz.nx.NamedArray
）：
    Del RNG，州
    logits：pz.nx.NamedArray =模型（current_input）
    损失= jnp.sum（
        optax.losses.softmax_cross_entropy（
            logits.unwrap（“ features_out＆quot”），
            current_target.unwrap（“ features_out; quot”），，
        ）
    ）
    返回（损失，无，{＆quot; softmax_cross_entropy_loss＆quot;：lose}）


def scale_by_metadata_value（metadata_field_name：str）：
    def init_fn（params）：
        Learning_rates = jax.tree.map（
            lambda param：param.metadata [metadata_field_name]，
            参数，
            is_leaf =（lambda节点：isInstance（node，pz.parametervalue）），），
        ）
        返回{＆quot; learning_rates＆quot;：learning_rates}

    def Update_fn（更新，状态，参数）：
        del params
        更新= jax.tree.map（lambda lr，g：lr * g，state [＆quot; learning_rates＆quort＆quort＆quort＆quort&#39;&#39;，更新）
        返回更新，状态

    返回optax.gradienttransformation（init_fn，update_fn）


培训师= penzai.toolshed.basic_training.statefultrainer.build（
    root_rng = jax.random.key（2025），
    模型=模型，
    Optimizer_Def = optax.chain（
        optax.scale_by_adam（），
        scale_by_metadata_value（&#39;Learning_rate＆quot;），
        optax.scale_by_learning_rate（0.01），
    ），
    lose_fn = softmax_cross_entropy_loss，
    jit = false，
）

trainer.step（
    current_input = pz.nx.wrap（np.zeros（8），“功能”，“），
    current_target = pz.nx.wrap（np.ones（4），;
）
＃typeError： *：&#39;jaxlib.xla_extension.arrayimpl&#39;和&#39;parametervalue&#39;
 ]]></description>
      <guid>https://stackoverflow.com/questions/79512816/how-to-scale-learning-rates-per-parameter-with-penzai-and-optax</guid>
      <pubDate>Sun, 16 Mar 2025 15:55:53 GMT</pubDate>
    </item>
    <item>
      <title>标题：为什么我的shap值尺寸[sample_num，feature，class]而不是[class，sample_num，功能]？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79512564/title-why-are-my-shap-values-dimensions-sample-num-feature-class-instead-of</link>
      <description><![CDATA[为什么所有AI都认为塑形值的计算如下？
  x_sample = pd.dataframe（x_train_resampled，columns = x.columns）[：500]  
Invelimer_rf = shap.treeexplainer（rf_model）  
shap_values_rf = rumenter_rf.shap_values（x_sample）  
 
生成的 shap_values_rf 尺寸应为[class，sample_num，feature]，但实际上，我的输出是[sample_num，feature，class，class]。
 null，我尝试过许多AI来生成Shap摘要图，并且尺寸似乎存在问题。即使使用默认的IRIS数据集，描述的问题仍然存在。 AI始终假定类维度处于第一个位置。]]></description>
      <guid>https://stackoverflow.com/questions/79512564/title-why-are-my-shap-values-dimensions-sample-num-feature-class-instead-of</guid>
      <pubDate>Sun, 16 Mar 2025 12:42:47 GMT</pubDate>
    </item>
    <item>
      <title>TPU上的RESNET50计算NAN的准确性和损失，在Google Colab中的CPU上正常工作</title>
      <link>https://stackoverflow.com/questions/79495542/resnet50-on-tpu-calculates-nan-for-accuracy-and-loss-works-fine-on-cpu-in-googl</link>
      <description><![CDATA[我使用Google Colab在V2-8 TPU加速器上培训了RESNET50，我已经用5000张形状（224、224、3）喂了它。我已经对它们进行了规范，没有NAN，没有INF，没有阶级失衡，一切都还好：
  input_shape =（224，224，3）

使用Strategy.scope（）：
    base_model = resnet50（weights =&#39;imagenet&#39;，include_top = false，input_shape = input_shape）
    base_model.trainable = false
    型号= tf.keras.models.sequeential（[[
        base_model，
        tf.keras.layers.globalaveragepooling2d（），
        tf.keras.layers.dense（1024，activation =&#39;relu&#39;），
        tf.keras.layers.dense（6，activation =&#39;sigmoid&#39;） 
    ）））
    model.compile（优化器=&#39;adam&#39;， 
                  损失=&#39;binary_crossentropy&#39;，
                  指标= [&#39;准确性&#39;]）
    
型号
    x_train， 
    y_train， 
    时代= 10， 
    验证_data =（x_val，y_val），
    batch_size = 32
  ）
 
当我对TPU进行训练时，训练期间的准确性和损失成为NAN。当我切换到CPU时，一切正常。
为什么会发生这种情况以及如何解决？
我尝试在Google Colab中对TPU和CPU进行培训。我希望该模型在没有任何问题的情况下进行培训，包括NAN值损失或准确性，尤其是因为培训在CPU上效果很好。但是，当使用TPU时，我遇到了NAN值的准确性和损失。我还验证了数据很干净，没有NAN，无限或失衡问题，并确保模型编译和培训设置是正确的。]]></description>
      <guid>https://stackoverflow.com/questions/79495542/resnet50-on-tpu-calculates-nan-for-accuracy-and-loss-works-fine-on-cpu-in-googl</guid>
      <pubDate>Sun, 09 Mar 2025 07:07:10 GMT</pubDate>
    </item>
    <item>
      <title>模型的雪花ML注册表解释性：`valueerror：模型类型<类'nontype'>在记录管道时不支持``不支持</title>
      <link>https://stackoverflow.com/questions/79490841/snowflake-ml-registry-for-model-explainability-valueerror-model-type-class</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79490841/snowflake-ml-registry-for-model-explainability-valueerror-model-type-class</guid>
      <pubDate>Thu, 06 Mar 2025 22:28:30 GMT</pubDate>
    </item>
    <item>
      <title>在React Native App中访问.bin文件的问题</title>
      <link>https://stackoverflow.com/questions/78161067/issue-accessing-bin-files-in-react-native-app</link>
      <description><![CDATA[我正在遇到一个问题，即在我的React Native应用程序中访问.bin文件。我遵循了将TensorFlow模型保存在.H5格式中，将其转换为JSON和二进制文件，然后将这些文件转换为应用程序目录的过程。但是，尽管我可以访问.json文件，但我无法访问.bin文件。
  const {getDefaultConfig} = require（＆quot; metro-config; quot;）;

模块。Exports= async（）=＆gt; {
  const defaultConfig =等待GetDefaultConfig（）;
  const {asseTexts} = defaultConfig.resolver;

  返回 {
    解析器：{
      Assetexts：[
        ... assetexts，
        ＆quot“ json”，// json文件
        ＆quot“ bin”，//二进制文件
      ]，，
    }，，
  };
};

 
其他注释：
我确保了.bin文件正确放置在我项目中的资产目录中。
我还验证了Metro.config.js文件中没有语法错误或错别字。
尽管做了这些努力，但我仍然无法访问我的应用中的.bin文件。
如何解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/78161067/issue-accessing-bin-files-in-react-native-app</guid>
      <pubDate>Thu, 14 Mar 2024 13:59:11 GMT</pubDate>
    </item>
    <item>
      <title>什么是x_train.reshape（），它有什么作用？</title>
      <link>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</link>
      <description><![CDATA[使用MNIST数据集
 导入numpy作为NP
导入TensorFlow作为TF
来自TensorFlow.keras.datasets导入mnist

＃MNIST数据集参数
num_classes = 10＃总类（0-9位数字）
num_features = 784＃数据功能（IMG形状：28*28）

（x_train，y_train），（x_test，y_test）= mnist.load_data（）

＃转换为float32
x_train，x_test = np.array（x_train，np.float32），np.array（x_test，np.float32）

＃平坦图像到784个功能的1-D矢量（28*28）
x_train，x_test = x_train.Reshape（[ -  1，num_features]），x_test.reshape（[ -  1，num_features]）

＃将图像从[0，255]到[0，1]的标准化值
x_train，x_test = x_train /255。，x_test / 255。
 
在第15行是
  x_train，x_test = x_train.Reshape（[ -  1，num_features]），x_test.reshape（[ -  1，num_features]））。我不明白这些重塑在我们的数据集中的真正作用。请向我解释。]]></description>
      <guid>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</guid>
      <pubDate>Sat, 02 May 2020 06:44:34 GMT</pubDate>
    </item>
    <item>
      <title>预测预测类的概率</title>
      <link>https://stackoverflow.com/questions/55880598/predict-probability-of-predicted-class</link>
      <description><![CDATA[我有一个数据集，其中包含GPA，GRE，TOEFL，SOP＆amp; lor排名（5个）等。 （所有数值）和最后一栏，该列说明它们是否被录取到大学（0或1），这是我们将其用作y__train的方法。
我不仅应该对预测的标签进行分类，还应该计算每个人被录取的概率。
编辑：因此，从第一个评论中，我构建了一个逻辑回归模型，并通过一些谷歌搜索了Sklearn中的“ Prection_proba”并尝试实施它。有任何句法错误，但是prective_proba给出的代码值非常错误。
链接： https://github.com/tarunn2799/gre-pred/blob/master/master/gre%20Admission%20Probibaliety-%20Extraaeedge.ipynb   
请帮助我找到自己出了问题的地方，也可以减少损失的技巧]]></description>
      <guid>https://stackoverflow.com/questions/55880598/predict-probability-of-predicted-class</guid>
      <pubDate>Sat, 27 Apr 2019 12:33:37 GMT</pubDate>
    </item>
    <item>
      <title>如何解释几乎完美的准确性和AUC-ROC但F1得分为零，精确和回忆</title>
      <link>https://stackoverflow.com/questions/34698161/how-to-interpret-almost-perfect-accuracy-and-auc-roc-but-zero-f1-score-precisio</link>
      <description><![CDATA[我正在培训ML Logistic分类器，使用Python Scikit-Learn对两个类进行分类。它们处于极度不平衡的数据中（大约14300：1）。我的准确性几乎为100％和ROC-AUC，但精确度为0％，召回和F1得分。我了解准确性通常在非常不平衡的数据中没有用，但是为什么ROC-AUC措施也接近完美？
 来自Sklearn.metrics导入roc_curve，auc

＃获取ROC 
y_score = classifierused2.decision_function（x_test）
false_posisitive_rate，true_posive_rate，阈值= roc_curve（y__test，y_score）
roc_auc = auc（false_posistion_rate，true_posistion_rate）
打印&#39;auc  - &#39;+&#39;=&#39;，roc_auc

1 = class1
0 = class2
班级计数：
0 199979
1 21

精度：0.99992
分类报告：
             精确召回F1得分支持

          0 1.00 1.00 1.00 99993
          1 0.00 0.00 0.00 7

AVG /总计1.00 1.00 1.00 100000

混乱矩阵：
[[99992 1]
 [7 0]]
AUC = 0.977116255281
 
以上是使用逻辑回归，以下是使用决策树，决策矩阵看起来几乎相同，但是AUC却大不相同。
  1 = class1
0 = class2
班级计数：
0 199979
1 21
精度：0.99987
分类报告：
             精确召回F1得分支持

          0 1.00 1.00 1.00 99989
          1 0.00 0.00 0.00 11

AVG /总计1.00 1.00 1.00 100000

混乱矩阵：
[[99987 2]
 [11 0]]
AUC = 0.4999899989
 ]]></description>
      <guid>https://stackoverflow.com/questions/34698161/how-to-interpret-almost-perfect-accuracy-and-auc-roc-but-zero-f1-score-precisio</guid>
      <pubDate>Sat, 09 Jan 2016 19:50:50 GMT</pubDate>
    </item>
    <item>
      <title>如何找到功能对逻辑回归模型的重要性？</title>
      <link>https://stackoverflow.com/questions/34052115/how-to-find-the-importance-of-the-features-for-a-logistic-regression-model</link>
      <description><![CDATA[我有一个由逻辑回归算法训练的二进制预测模型。我想知道哪些功能（预测指标）对于正面或负面类别的决策更为重要。我知道有来自Scikit-Learn软件包的 COEF _ 参数，但我不知道它是否足以满足重要性。另一件事是我如何根据否定和正类别的重要性来评估 coef _ 值。我还阅读了有关标准化回归系数的信息，但我不知道它是什么。
可以说，有肿瘤大小，肿瘤重量等特征，可以决定恶性肿瘤或不恶性等测试案例。我想知道哪些功能对于恶性和不是恶性预测更为重要。]]></description>
      <guid>https://stackoverflow.com/questions/34052115/how-to-find-the-importance-of-the-features-for-a-logistic-regression-model</guid>
      <pubDate>Wed, 02 Dec 2015 20:11:21 GMT</pubDate>
    </item>
    <item>
      <title>当尝试使用SVC进行概率预测时，为什么我会得到一个noctemplementederror？</title>
      <link>https://stackoverflow.com/questions/19878285/why-do-i-get-a-notimplementederror-when-trying-to-use-svc-for-probability-predic</link>
      <description><![CDATA[我正在尝试使用Sklearn的SVC来解决分类问题。给出了一堆数据，并告诉我某些学科是否在某个类中，我希望能够给出一个新的，未知的主题是在类中。
我只有2个类，所以问题是二进制的。这是我的代码和我的一些错误
 来自Sklearn.svm导入SVC
clf = svc（）

clf = clf.fit（x，y）


SVC（概率= true）
打印clf.predict_proba（w）#Error在这里
 
，但它返回以下错误：

 notimplemplementError：必须启用概率估计来使用此方法

我该如何修复？]]></description>
      <guid>https://stackoverflow.com/questions/19878285/why-do-i-get-a-notimplementederror-when-trying-to-use-svc-for-probability-predic</guid>
      <pubDate>Sat, 09 Nov 2013 16:30:58 GMT</pubDate>
    </item>
    </channel>
</rss>