<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 28 Apr 2024 09:13:26 GMT</lastBuildDate>
    <item>
      <title>与支持向量回归模型相关的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78397399/question-related-to-support-vector-regression-model</link>
      <description><![CDATA[我正在构建一个支持向量回归模型，以根据以下一些特征来预测公交车的运行时间：GPS 经度、纬度和公交车所在的路段：
from sklearn.model_selection import train_test_split, GridSearchCV
从 sklearn.svm 导入 SVR
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.pipeline 导入管道
从 sklearn.metrics 导入mean_squared_error

# 分离特征和目标变量
X = df[[&#39;段纬度&#39;, &#39;段经度&#39;, &#39;段&#39;]]
y = df[&#39;segment_run_time&#39;]

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建具有缩放和 SVR 的管道
管道=管道（[
    (&#39;缩放器&#39;, StandardScaler()),
    (&#39;svr&#39;, SVR(内核=&#39;rbf&#39;, gamma=&#39;scale&#39;))
]）

# 定义参数网格
参数网格 = {
    &#39;svr__C&#39;: [0.1, 1, 10], # C 的不同值
    &#39;svr__epsilon&#39;: [0.1, 0.2, 0.5] # 不同的 epsilon 值
}

# 执行网格搜索
grid_search = GridSearchCV(管道, param_grid, cv=5, 评分=&#39;neg_mean_squared_error&#39;)
grid_search.fit(X_train, y_train)

# 获取最佳参数
best_params = grid_search.best_params_
print(&quot;最佳参数：&quot;, best_params)

# 使用最佳估计器预测测试集
最佳估计器= grid_search.最佳估计器_
y_pred = best_estimator.predict(X_test)

# 评估模型
mse = 均方误差(y_test, y_pred)
print(&quot;均方误差：&quot;, mse)

# 计算均方根误差
rmse =mean_squared_error(y_test, y_pred, squared=False)
print(&quot;均方根误差：&quot;, rmse)

我有 2 个问题：

我正在尝试调整超参数，以便使用 gridSearchCV 获得最佳结果。你们能检查一下我做得是否正确，是否有更好的方法可以实现我的模型以获得最佳的超参数？
我尝试在 Kaggle 和 Google Collab 中运行代码，但它运行了几个小时，最终无法执行。这是因为我的数据集太大了吗？我的数据集中有 130464 条记录。
下面是我的模型的代码。如果你们能看一下我真的很感激。
]]></description>
      <guid>https://stackoverflow.com/questions/78397399/question-related-to-support-vector-regression-model</guid>
      <pubDate>Sun, 28 Apr 2024 07:28:54 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：形状 (64,100) 和 (10,100) 未对齐：100 (dim 1) != 10 (dim 0)</title>
      <link>https://stackoverflow.com/questions/78397046/valueerror-shapes-64-100-and-10-100-not-aligned-100-dim-1-10-dim-0</link>
      <description><![CDATA[我在训练 2 层神经网络时遇到这个错误
我尝试了这段代码，但它给出了上述错误。
将 numpy 导入为 np
从 numpy.random 导入 randn
&lt;前&gt;&lt;代码&gt;N、D_输入、H、D_输出 = 64、1000、100、10
x, y = randn(N, D_in), randn(N, D_out)
w1, w2 = randn(D_in, H), randn(D_out, H)

对于范围（2000）内的 t：
    h = 1 / (1 + np.exp(-x.dot(w1)))
    y_pred = h.dot(w2)
    损失 = np.square(y_pred - y).sum()
    打印（t，损失）

    grad_y_pred = 2.0 * (y_pred -y)
    grad_w2 = h.T.dot(grad_y_pred)
    grad_h = grad_y_pred.dot(w2.T)
    grad_w1 = x.T.dot(grad_h * h * (1 - h))

    w1 -= 1e-4 * grad_w1
    w2 -= 1e-4 * grad_w2
]]></description>
      <guid>https://stackoverflow.com/questions/78397046/valueerror-shapes-64-100-and-10-100-not-aligned-100-dim-1-10-dim-0</guid>
      <pubDate>Sun, 28 Apr 2024 04:12:08 GMT</pubDate>
    </item>
    <item>
      <title>机器学习：numpy，处理 NaN 值的问题</title>
      <link>https://stackoverflow.com/questions/78396645/machine-learning-numpy-issue-dealing-with-nan-values</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78396645/machine-learning-numpy-issue-dealing-with-nan-values</guid>
      <pubDate>Sat, 27 Apr 2024 23:19:52 GMT</pubDate>
    </item>
    <item>
      <title>多级数据的分类模型</title>
      <link>https://stackoverflow.com/questions/78396178/classification-models-for-multilevel-data</link>
      <description><![CDATA[我正在研究一个机器学习项目，准确地说是分类。我的数据集包含 217 个国家的社会、人口和经济指数，每个国家 60 年。目标变量是二进制的。我想训练随机森林和 xgboost 模型，我想知道：我可以用 Caret 训练这些模型吗？他们能够理解这种结构并处理多级数据吗？
如果是的话，我想用这种方式训练模型：
&lt;前&gt;&lt;代码&gt;#tree
ctrl_tree &lt;- trainControl(方法 = “cv”，数字 = 10，classProbs = TRUE，summaryFunction=twoClassSummary)
树 &lt;- train(Target~.，data=under，method =“rpart”，tuneLength = 10，metric=“ROC”，trControl = ctrl_tree)


#XGBoost
设置.种子(76)
ctrl_xgb &lt;- trainControl(方法=“cv”，数字=10，搜索=“网格”，summaryFunction = TwoClassSummary，classProbs = TRUE)
param_grid_xgb &lt;- Expand.grid(nrounds=500, max_depth = c(3, 6, 9),eta = c(0.01, 0.1, 0.3),
  伽马 = c(0, 0.2, 0.4)，子样本 = c(0.8, 0.9, 1)，colsample_bytree = c(0.8, 0.9, 1)，
  min_child_weight=c(1, 5, 10))
xgb&lt;-train(Target~.,data=under,method=“xgbTree”,metric=“ROC”,tuneGrid=param_grid_xgb,
           trControl=ctrl_xgb,详细程度=0)
]]></description>
      <guid>https://stackoverflow.com/questions/78396178/classification-models-for-multilevel-data</guid>
      <pubDate>Sat, 27 Apr 2024 19:37:36 GMT</pubDate>
    </item>
    <item>
      <title>如何绘制多类分类中所有类的 SHAP 摘要图</title>
      <link>https://stackoverflow.com/questions/78396068/how-to-plot-shap-summary-plots-for-all-classes-in-multiclass-classification</link>
      <description><![CDATA[我正在使用 XGBoost 和 SHAP 来分析多类分类问题中的特征重要性，并且需要帮助一次性绘制所有类的 SHAP 摘要图。目前，我一次只能生成一个类的绘图。
SHAP 版本：0.45.0
Python版本：3.10.12

这是我的代码：
将 xgboost 导入为 xgb
导入形状
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.datasets 导入 make_classification
从 sklearn.metrics 导入 precision_score

# 生成合成数据
X，y = make_classification（n_samples = 500，n_features = 20，n_informative = 4，n_classes = 6，random_state = 42）
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 训练用于多类分类的 XGBoost 模型
模型 = xgb.XGBClassifier(objective=“multi:softprob”, random_state=42)
model.fit(X_train, y_train)

然后我尝试绘制形状值：
# 创建一个 SHAP TreeExplainer
解释器 = shap.TreeExplainer(模型)

# 计算测试集的SHAP值
shap_values = 解释器.shap_values(X_test)

# 尝试绘制所有类的摘要
shap.summary_plot（shap_values，X_test，plot_type =“酒吧”）

我得到了这个交互图：

我在 此帖子：
shap.summary_plot(shap_values[:,:,0], X_test,plot_type=&quot;bar&quot;)

它给出了 0 类的正常条形图：

然后我可以对类 1、2、3 等执行相同的操作。
问题是，如何为所有类别制作汇总图？即，显示某个特征对每个类的贡献的单个图？]]></description>
      <guid>https://stackoverflow.com/questions/78396068/how-to-plot-shap-summary-plots-for-all-classes-in-multiclass-classification</guid>
      <pubDate>Sat, 27 Apr 2024 19:02:16 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ML 检测实时 Windows 网络异常？</title>
      <link>https://stackoverflow.com/questions/78396046/how-to-detect-real-time-windows-network-anomaly-using-ml</link>
      <description><![CDATA[我正在开发一个项目来检测网络中的异常检测，并且我的目标是其中的 Windows 网络异常检测。但我遇到了一些问题。

如何获取异常检测的 ML 模型的数据集，即我已经搜索过，大多数数据集都是 HDFS、服务器等。Windows 网络异常数据集没有特定的数据集

如何实时获取数据以进行异常检测。就像我希望将数据从用户电脑发送到管理员电脑，然后每 20 分钟发送到 api，看看是否有任何异常。


我对这两方面感到困惑。谁能引导我走上正确的道路？]]></description>
      <guid>https://stackoverflow.com/questions/78396046/how-to-detect-real-time-windows-network-anomaly-using-ml</guid>
      <pubDate>Sat, 27 Apr 2024 18:51:52 GMT</pubDate>
    </item>
    <item>
      <title>将灰狼优化器应用于我的支持向量回归模型</title>
      <link>https://stackoverflow.com/questions/78395857/applying-a-gray-wolf-optimiser-to-my-support-vector-regression-model</link>
      <description><![CDATA[这是我正在使用的 SVR 模型。我只需要稍微提高模型的准确性，并想尝试 GWO。我需要包含什么代码？
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler
进口火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
从 torch.optim.lr_scheduler 导入 ReduceLROnPlateau
从 scipy 导入统计数据

# 使用 PyTorch 定义 SVR 模型
类 SVRModel(nn.Module):
    def __init__(自身):
        超级（SVRModel，自我）.__init__()
        self.fc1 = nn.Linear(in_features=7, out_features=64) # 如果需要调整输入和输出特征
        self.fc2 = nn.Linear(in_features=64, out_features=32)
        self.fc3 = nn.Linear(in_features=32, out_features=1) # 输出层

    def 前向（自身，x）：
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        返回x


# 将数据转换为 PyTorch 张量
X_train_tensor = torch.tensor(scaled_X_train, dtype=torch.float32)
Y_train_tensor = torch.tensor(Y_train.reshape(-1, 1), dtype=torch.float32)
X_test_tensor = torch.tensor(scaled_X_test, dtype=torch.float32)

# 定义超参数
C = 1
ε = 0.1
lr = 0.01
历元 = 1000

# 实例化SVR模型
模型 = SVRModel()

# 定义损失函数和优化器
标准 = nn.MSELoss()
优化器 = optim.Adam(model.parameters(), lr=lr)

# 定义学习率调度器
调度程序=ReduceLROnPlateau（优化器，模式=&#39;min&#39;，因子=0.5，耐心=5，详细=True）

# 训练循环
对于范围内的纪元（纪元）：
    模型.train()
    优化器.zero_grad()
    输出=模型（X_train_tensor）
    损失 = 标准（输出，Y_train_tensor）
    loss.backward()
    优化器.step()
    
    # 步骤调度器
    调度程序.step(损失)
    
    print(f&#39;Epoch [{epoch+1}/{epochs}], 损失: {loss.item():.4f}&#39;)

＃ 评估
模型.eval()
使用 torch.no_grad()：
    Y_pred_tensor = 模型(X_test_tensor)
    Y_pred = Y_pred_tensor.numpy().flatten()

# 计算相关性
corr = stats.pearsonr(Y_test, Y_pred)[0]
print(f&#39;相关性：{corr}&#39;)

# 创建结果数据框
result_df = pd.DataFrame({&#39;svr_predicted&#39;: Y_pred, &#39;true&#39;: Y_test})
result_df[&#39;svr_error&#39;] = abs(result_df[&#39;true&#39;] - result_df[&#39;svr_predicted&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/78395857/applying-a-gray-wolf-optimiser-to-my-support-vector-regression-model</guid>
      <pubDate>Sat, 27 Apr 2024 17:43:56 GMT</pubDate>
    </item>
    <item>
      <title>模型训练时间过长</title>
      <link>https://stackoverflow.com/questions/78395684/model-training-taking-too-long-time</link>
      <description><![CDATA[我正在尝试训练 XGBRegressor，但代码执行时间太长。我做错了什么吗？代码如下：
&lt;前&gt;&lt;代码&gt;%%时间
!pip 安装 xgboost
将 xgboost 导入为 xgb
测试 = dd.read_csv(&#39;/kaggle/input/leap-atmospheric-physicals-ai-climsim/test.csv&#39;)
#dd 指 dask.dataframe
样本 ID = 测试[&#39;样本 ID&#39;]

测试 = test.drop(&#39;sample_id&#39;, axis = 1)
X = df_train.drop(目标 + [&#39;sample_id&#39;], axis = 1)
预测 = {}
提交={}
提交[&#39;样本id&#39;] = 样本id
对于 i，枚举中的目标（目标）：
    y = df_train[目标]
    X_train，X_test，y_train，y_test = train_test_split（X，y，random_state = 42，test_size = 0.33）
    dtr = xgb.XGBRegressor(详细 = False)
    dtr.fit(X_train, y_train)
    y_hat = dtr.predict(X_test)
    提交[目标] = dtr.predict(测试)
    预测[目标] = y_hat
    print(f&#39;r2_score for {target} : {r2_score(y_hat, y_test)}&#39;)

训练数据很大，但即使记录应该有效，但我在输出中看不到任何内容。]]></description>
      <guid>https://stackoverflow.com/questions/78395684/model-training-taking-too-long-time</guid>
      <pubDate>Sat, 27 Apr 2024 16:34:34 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 获取 One vs Rest SVC() 的模型参数？</title>
      <link>https://stackoverflow.com/questions/78395647/how-to-get-the-model-parameter-for-one-vs-rest-svc-using-python</link>
      <description><![CDATA[我尝试使用decision_function_shape= ovr制作onvsrest分类模型，但是当我将其更改为decision_function_shape= ovo时，它给了我与ovr相同的结果。结果我读到 svc() 正在使用 ovo 作为基础，无论它是作为 ovr 还是 ovo 启动的。那么我怎样才能改变我的代码，以便它给我一个 ovr 结果呢？
model3 = SVC(kernel = &#39;rbf&#39;, Decision_function_shape=&#39;ovr&#39;)
model3.fit(X_train, Y_train)
model3_predictions = model3.predict(X_test)

我尝试过使用 OneVsRestClassifier() 但不知道如何给出所有这些命令的输出，它总是出错并说 OneVsRestClassifier 没有这些命令。有没有办法用 OneVsRestClassifier 获取 cm、sm、sv、beta 和截距？
cm3 = fusion_matrix(Y_test, model3_predictions, labels=[-1,0,1])
sm3 = 分类报告（Y_测试，model3_预测）
support_vector3 = model3.support_
n_sv_model3 = model3.n_support_
alpha_model3 = pd.DataFrame(model3.dual_coef_)
b_model3 = pd.DataFrame(model3.intercept_)

希望有人能帮助我，先谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78395647/how-to-get-the-model-parameter-for-one-vs-rest-svc-using-python</guid>
      <pubDate>Sat, 27 Apr 2024 16:20:07 GMT</pubDate>
    </item>
    <item>
      <title>检查给定图像是否是另一个更大图像的裁剪[关闭]</title>
      <link>https://stackoverflow.com/questions/78389839/check-if-the-given-image-is-a-crop-of-another-bigger-image</link>
      <description><![CDATA[我有一些图片。其中一些图像是裁剪版本。
就像这里是原始图片大图
和裁剪后的图像小图像。
请注意，图像的形状（分辨率）不相同。
我有几双这样的。原始图像保存在一个文件夹中，裁剪后的图像保存在另一个文件夹中。
最终我想从这些图像中找到原始图像和裁剪图像对。
所以我想迭代这两个文件夹中的图像，并检查裁剪后的图像是否是更大图像的一部分。
但是我找不到任何算法可以用不同形状（分辨率）的图像给出这样的结果。
我已经尝试过cv2.matchTemplate和skimage.metrics.structural_similarity
但它们仅适用于形状（分辨率）相似的图像。]]></description>
      <guid>https://stackoverflow.com/questions/78389839/check-if-the-given-image-is-a-crop-of-another-bigger-image</guid>
      <pubDate>Fri, 26 Apr 2024 10:32:29 GMT</pubDate>
    </item>
    <item>
      <title>使用带注释的数据进行图像分类</title>
      <link>https://stackoverflow.com/questions/78386605/image-classification-using-annotated-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78386605/image-classification-using-annotated-data</guid>
      <pubDate>Thu, 25 Apr 2024 18:31:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 的神经网络 NarX 模型</title>
      <link>https://stackoverflow.com/questions/78374854/neural-network-narx-model-with-python</link>
      <description><![CDATA[我正在尝试使用 Python 编写 NarX 模型代码。当我运行以下代码时，出现以下错误。我该如何解决这个错误？这个错误是什么意思？我相信问题出在我正在使用的数据集格式上，因为它适用于另一个数据集。我添加了来自 kaagle 数据集的示例。

这也是我的代码和错误消息：
inpt = all_data[[“category”,“cuisine”,“checkout_price”,“center_type”,“num_orders”,“city_code”]]
inpt = pd.get_dummies(inpt, columns=categorical_columns)
输出 = inpt[[“订单数量”]]
inpt = inpt.drop(columns=[“num_orders”])
sc = 标准缩放器()
in1 = sc.fit_transform(inpt[0:int(len_df * .8)]) #训练并拟合训练输入数据
in2 = sc.transform(inpt[int(len_df * .8)+1:len_df-1]) #测试变换
inp = np.concatenate([in1,in2]) #添加到末尾 df1 arr df2
inpt = pd.DataFrame(inp, columns=inpt.columns)
out1 = sc.fit_transform(output[0:int(len_df * .8)]) #训练并拟合训练出数据
out2 = sc.transform(output[int(len_df * .8)+1:len_df-1]) #测试变换
out = np.concatenate([out1,out2]) #添加到末尾 df1 arr df2
输出 = pd.DataFrame(out, columns=output.columns)
all_inputs = inpt.values
all_targets = 输出.值
对于 all_targets 中的 val：
    值=[值]
类型（所有目标）

&lt;前&gt;&lt;代码&gt;input_nodes = 6
隐藏节点 = 3
输出节点 = 1

输出顺序 = 9
来自输出的传入权重 = .6
输入顺序 = 2
来自输入的传入权重 = .4
网络 = 神经网络()

net.init_layers（输入节点，[隐藏节点]，输出节点，
    NARX循环（
        输出顺序，
        来自输出的传入权重，
        输入顺序，
        来自输入的传入权重））

net.randomize_network()

net.set_halt_on_extremes(True)
net.set_random_constraint(.5)
net.set_learnrate(.1)

net.set_all_inputs(all_inputs)
net.set_all_targets(all_targets)

长度 = len(所有输入)
学习结束点 = int(长度 * .8)

net.set_learn_range(0, learn_end_point)
net.set_test_range(learn_end_point + 1, 长度 - 1)

net.layers[1].set_activation_type(&#39;sigmoid&#39;)#sigmoid,线性 TF

net.learn(epochs= 30, show_epoch_results=True, random_testing=False)

ValueError：尝试将输入值加载到非输入节点
]]></description>
      <guid>https://stackoverflow.com/questions/78374854/neural-network-narx-model-with-python</guid>
      <pubDate>Tue, 23 Apr 2024 20:31:49 GMT</pubDate>
    </item>
    <item>
      <title>我应该把reuse_actors=True放在哪里？</title>
      <link>https://stackoverflow.com/questions/76354078/where-should-i-put-reuse-actors-true</link>
      <description><![CDATA[运行以下代码后，它会显示
&lt;块引用&gt;
INFO trainable.py:172 – Trainable.setup 花费了 2940.989 秒。如果您的可训练初始化速度很慢，请考虑设置reuse_actors=True以减少actor创建开销

导入光线
ray.init(地址=“自动”, _temp_dir=&#39;/home/ray_dir&#39;)

rnd = 随机.种子(8)
grid_cv = StratifiedKFold(n_splits=3,random_state=rnd, shuffle=True)

从 xgboost.callback 导入 EarlyStopping
Early_stopping = EarlyStopping(轮数 = 50, 最大化 = True, save_best = True)
clf = xgb.XGBClassifier(
                tree_method=&#39;gpu_hist&#39;,
                最大bin=512，
                学习率 = 0.0001,
                n_估计器=1000，
                目标=&#39;二进制：逻辑&#39;，reg_alpha=0.01，
                scale_pos_weight = pos_weight, eval_metric= &#39;aucpr&#39;,
                回调=[early_stopping],
                详细程度 = 0,
                线程数 = 96
                ）


参数 = {
    &#39;eta&#39;: [0.01, 0.1, 0.3],
    &#39;min_child_weight&#39;: [1,3,8,16],
    &#39;最大深度&#39;:[25,50,100,500],
    &#39;colsample_bytree&#39;: [0.4,0.6,0.8],
    &#39;子样本&#39;: [0.4,0.6,0.8],
    &#39;伽玛&#39;：[0,0.5,2,10],
}

gs = TuneGridSearchCV（估计器 = clf、param_grid = param、cv = grid_cv、n_jobs = -1、refit = True、return_train_score = True、verbose = 3、评分 = &#39;average_ precision&#39;、use_gpu = True ）

gs.fit(X_train, y_train, eval_set= eval_set_xgboost, verbose=True)

我应该在代码中的何处添加 reuse_actors=True ？]]></description>
      <guid>https://stackoverflow.com/questions/76354078/where-should-i-put-reuse-actors-true</guid>
      <pubDate>Mon, 29 May 2023 00:25:33 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 中的 softmax 输出进行神经网络和机器学习来解释多项 Logit 模型？ [复制]</title>
      <link>https://stackoverflow.com/questions/60482320/how-to-use-softmax-output-in-python-for-neural-network-and-machine-learning-to-i</link>
      <description><![CDATA[它涉及使用机器学习和神经网络的 softmax 函数输出来理解和解释多项 Logit 模型。]]></description>
      <guid>https://stackoverflow.com/questions/60482320/how-to-use-softmax-output-in-python-for-neural-network-and-machine-learning-to-i</guid>
      <pubDate>Mon, 02 Mar 2020 03:49:09 GMT</pubDate>
    </item>
    <item>
      <title>平滑后的GPS数据对比</title>
      <link>https://stackoverflow.com/questions/27709732/gps-data-comparison-after-smoothing</link>
      <description><![CDATA[我正在尝试比较用于平滑 GPS 数据的多种算法。我想知道比较结果以查看哪一个提供更好的平滑效果的标准方法应该是什么。
我正在考虑一种机器学习方法。基于分类器创建汽车模型并检查哪些轨道提供更好的行为。
对于在这方面有更多经验的人来说，这是一个好方法吗？还有其他方法可以做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/27709732/gps-data-comparison-after-smoothing</guid>
      <pubDate>Tue, 30 Dec 2014 17:21:04 GMT</pubDate>
    </item>
    </channel>
</rss>