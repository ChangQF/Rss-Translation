<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 01 Oct 2024 21:17:09 GMT</lastBuildDate>
    <item>
      <title>Jupyter Notebook 和 Python 脚本返回不同的 YOLOV8 实例分割掩码</title>
      <link>https://stackoverflow.com/questions/79043979/jupyter-notebook-and-python-script-return-different-yolov8-instance-segmentation</link>
      <description><![CDATA[我一直在运行预先训练的 YOLOV8 模型来进行实例分割。为了进行测试和开发，我一直在使用 Anaconda Jupyter Notebook，在部署之前我将其转换为 Python 脚本。
以下是最小可重现示例的代码：
from ultralytics import YOLO
import torch
import matplotlib.pyplot as plt
import numpy as np
import PIL

print(torch.cuda.is_available())

model = YOLO(&quot;YOLO/yolov8x-seg.pt&quot;)

image = PIL.Image.open(&quot;YOLO/test.png&quot;)

results = model(image, verbose=False)
plt.imshow(np.logical_or.reduce(results[0].masks.data.cpu().numpy()).astype(np.uint8)*255)
plt.show()

这是我使用的测试图像
这是来自 Jupyter Notebook 的（正确）蒙版：

这是来自 Python 脚本的结果：

您可以看到 Python 结果在边界框的边缘有这些奇怪的块。

两个版本的代码完全相同
Python 虚拟环境直接从 Jupyter 环境导出创建（我仔细检查了：Python 版本相同，所有库的版本也相同）
相同的 CUDA 版本 (12.4)
相同的硬件（两个示例都在同一台机器上执行 - GPU 是 RTX 3070 Ti 笔记本电脑）
两个网络使用相同的预训练权重（可从此处下载）

我的第一个猜测是一些库在Jupyter Notebook 和 Python 环境，但我手动检查了它们，发现它们是相同的。
我还尝试手动设置 Python 脚本的浮点精度，以防这是由网络权重中的一些聚合精度错误引起的，但这也不起作用。
我读到 Anaconda 使用 IPython，这可能会导致不同的行为，但我不确定如何处理这些信息。
我也不能简单地减小边界框的大小，因为这些伪影的位置和大小在运动图像中是不可预测的。减小边界框大小会导致部分蒙版偶尔被切断。
为什么会发生这种情况，我该如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/79043979/jupyter-notebook-and-python-script-return-different-yolov8-instance-segmentation</guid>
      <pubDate>Tue, 01 Oct 2024 16:24:34 GMT</pubDate>
    </item>
    <item>
      <title>项目帮助：视频输入的脚步计数器 - 寻找 SOTA 模型和启发式方法 [关闭]</title>
      <link>https://stackoverflow.com/questions/79043873/project-help-footsteps-counter-for-video-input-looking-for-sota-models-and-he</link>
      <description><![CDATA[我正在开展一个项目，用于计算输入视频中的脚步数，并一直在尝试使用 YOLOv8 和 MediaPipe 等姿势估计方法。我的目标是涵盖以下测试用例：

只有人的上半身在画面中，但他们正在行走。

只有人的下半身在画面中。

解决方案应该是防遮挡的。


这是我目前用来通过计算左右脚踝之间的距离来计算步数的逻辑：
def distanceCalculate(p1, p2):
&quot;&quot;&quot;p1 和 p2 格式为 (x1, y1) 和 (x2, y2) 元组&quot;&quot;&quot;
dis = ((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2) ** 0.5
return dis

# 计算脚踝之间的距离（粗略估计迈出一步）
if distanceCalculate(leftAnkle, rightAnkle) &gt; 100: # 步数检测阈值
if not stepStart:
stepStart = 1
stepCount += 1

# 附加到输出 JSON
output_data[&quot;footsteps&quot;].append({
&quot;step&quot;: stepCount,
&quot;timestamp&quot;: round(current_time, 2)
})

elif stepStart and distanceCalculate(leftAnkle, rightAnkle) &lt; 50:
stepStart = 0 # 完成一步后重置

但是，这种逻辑并不适用于所有视频。我正在寻找有关最先进 (SOTA) 模型和启发式逻辑的建议，这些建议可以帮助改进步数检测，特别是针对上述场景。
有什么建议或意见吗？]]></description>
      <guid>https://stackoverflow.com/questions/79043873/project-help-footsteps-counter-for-video-input-looking-for-sota-models-and-he</guid>
      <pubDate>Tue, 01 Oct 2024 15:50:15 GMT</pubDate>
    </item>
    <item>
      <title>如何防止大型 sklearn 随机森林模型使 CPU 核心过载？</title>
      <link>https://stackoverflow.com/questions/79043871/how-to-prevent-big-sklearn-random-forest-model-from-overloading-the-cpu-core</link>
      <description><![CDATA[我使用 RandomizedSearchCV 通过 sklearn 训练了一个随机森林模型。
我将模型存储为 joblib 文件，现在我每天都使用它在 docker 容器中的服务器上进行预测。
基本上，该过程如下：
watcher.py:
def watch():

def run_script():
scripts = [&quot;forecaster.py&quot;]
for script in scripts:
subprocess.run([&#39;python&#39;, script], check=True)
time.sleep(63)

while True:
now = datetime.datetime.now(pytz.utc).astimezone(berlin_tz)
if now.hour == 23 and now.minute == 0:
logs.info(f&quot;Running scripts at {now}.&quot;)
run_script()
time.sleep(3) 

if __name__ == &#39;__main__&#39;:
logs.info(&quot;Watcher gestartet&quot;)
watch()

forecaster.py:
data = get_data_from_API(...)
features = [&quot;list&quot;, &quot;of&quot;, &quot;variables&quot;, &quot;contained&quot;, &quot;in&quot;, &quot;data&quot;]

rf = load(model_path.joblib)
rf_pred = rf.predict(data[features])

table = pd.DataFrame({
&#39;UTC&#39;: data[&#39;UTC&#39;],
&#39;Forecast&#39;: rf_pred
})
table.to_csv(&#39;somepath.csv&#39;)

但是，我最近训练了一些更好的模型，这些模型更大（&gt;100.000KB，所以不是那么大想想），现在我的脚本不断被杀死（&lt;Signals.SIGKILL: 9&gt;），我可以在 docker 桌面上看到 CPU 使用率达到 100%（服务器的核心功能比我用来训练模型的笔记本电脑要少）。
我尝试了不同大小的模型。较小的模型可以正常工作，但较大的模型会崩溃。
目前，我坚持使用准确度较低的较小模型，这种方法有效，但我正在寻找适用于任何我想使用的模型的解决方案。
我认为并行处理可能是一种解决方案，但我只找到仅适用于其他库（如 PyTorch）或关于在训练模型时并行化的教程，而不是在应用模型时并行化的教程。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79043871/how-to-prevent-big-sklearn-random-forest-model-from-overloading-the-cpu-core</guid>
      <pubDate>Tue, 01 Oct 2024 15:49:44 GMT</pubDate>
    </item>
    <item>
      <title>AWS SageMaker 上的 Flask 应用程序无法通过异步端点调用处理 S3 视频处理</title>
      <link>https://stackoverflow.com/questions/79043743/flask-app-on-aws-sagemaker-fails-to-handle-s3-video-processing-via-async-endpoin</link>
      <description><![CDATA[我正在 AWS SageMaker 中部署一个 Flask 应用程序，该应用程序旨在处理异步调用。该应用程序应从 S3 获取视频文件，使用面部情绪预测模型对其进行处理，然后返回结果。但是，我的设置没有按预期工作。
以下是我的流程中步骤的摘要：
异步 SageMaker 调用：我使用 Python 脚本中的invoke_endpoint_async 方法触发 SageMaker 端点的调用，为视频文件提供 S3 URI。
response = Runtime.invoke_endpoint_async(
EndpointName=endpoint_name,
InputLocation=&quot;&lt;s3 URI&gt;&quot;, 
ContentType=&quot;application/json&quot;
)

Flask 应用程序（通过 ECR 部署）：Flask 应用程序在 /invocations 端点监听 POST 请求，从请求正文中提取 S3 URI，下载视频文件，处理它，然后返回预测。
import os
from flask import Flask, request, jsonify
import source.face_emotion_utils.predict as face_predict
import boto3
import logs

app = Flask(__name__)

# 设置 S3 客户端
s3 = boto3.client(&#39;s3&#39;)

@app.route(&#39;/ping&#39;, methods=[&#39;GET&#39;])
def ping():
return jsonify({&#39;status&#39;: 200})

@app.route(&#39;/invocations&#39;, methods=[&#39;POST&#39;])
def faceInvocations():
try:
# 提取 JSON 负载
payload = request.get_json()
s3_uri = payload.get(&#39;InputLocation&#39;)

if not s3_uri:
return jsonify({&#39;error&#39;: &#39;s3_uri is required&#39;}), 400

# 从 S3 URI 解析存储桶名称和密钥
bucket_name, key = s3_uri.replace(&quot;s3://&quot;, &quot;&quot;).split(&quot;/&quot;, 1)
filename = key.split(&quot;/&quot;)[-1]
file_path = os.path.join(&#39;/app/input_files&#39;, filename)

# 从 S3 下载文件
s3.download_file(bucket_name, key, file_path)

# 处理文件（预测情绪）
prediction = face_predict.predict(file_path, video_mode=True)

# 清理临时文件
if os.path.exists(file_path):
os.remove(file_path)

return jsonify({&#39;prediction&#39;: prediction})
except Exception as e:
logs.error(f&quot;Error: {str(e)}&quot;)
return jsonify({&#39;error&#39;: str(e)}), 500

if __name__ == &#39;__main__&#39;:
app.run(host=&#39;0.0.0.0&#39;, port=8080)

我的期望：

SageMaker 端点应该从异步调用接收 S3 URI。Flask 应用应该下载视频文件、运行情绪预测并返回结果。

问题：
调用失败，我在 Flask 日志中收到错误。似乎 S3 文件没有被正确下载或处理。以下是一些日志：
INFO:root:Entered faceInvocations function
INFO:root:S3 URI: &lt;s3 URI&gt;
ERROR:root:下载文件时出错：调用 HeadObject 操作时发生错误 (404)：未找到


我还可以看到到目前为止的日志 INFO:root:Created directory
我尝试过的方法：

确保 S3 URI 有效且文件存在。
验证附加到 SageMaker 端点的 IAM 角色是否具有 S3 存储桶的 s3:GetObject 权限。
检查 Flask 应用代码是否存在处理 S3 URI 的错误，并能够使用 docker 获取结果
在日志中打印 S3 存储桶名称和密钥，以确保它们被正确解析。

问题：

为什么我的 Flask 应用无法从中下载视频文件S3，即使 S3 URI 看起来正确？
我在 Flask 应用程序中处理 S3 URI 的方式是否存在问题？
传递 S3 URI 时，我是否遗漏了 SageMaker 异步调用的特定内容？
]]></description>
      <guid>https://stackoverflow.com/questions/79043743/flask-app-on-aws-sagemaker-fails-to-handle-s3-video-processing-via-async-endpoin</guid>
      <pubDate>Tue, 01 Oct 2024 15:09:40 GMT</pubDate>
    </item>
    <item>
      <title>在 TensorFlow/Keras 中使用 tf.keras.utils.image_dataset_from_directory 时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/79042251/valueerror-when-using-tf-keras-utils-image-dataset-from-directory-in-tensorflow</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79042251/valueerror-when-using-tf-keras-utils-image-dataset-from-directory-in-tensorflow</guid>
      <pubDate>Tue, 01 Oct 2024 08:17:16 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 mmdet 推断自定义预训练模型？</title>
      <link>https://stackoverflow.com/questions/79041799/how-to-infer-a-custom-pretrained-model-using-mmdet</link>
      <description><![CDATA[我正尝试使用一些自定义模型（调整后的 yolov5xu fwiw）运行 masa，这些模型已在某些外部数据集上训练。我很难理解如何使用 mdmdet 框架运行它。文档对我来说似乎有点不清楚，因为它指出我需要某种配置文件，但这令人惊讶，因为

我不会训练一个，我只需要 model().predict()
配置文件包含很多我不太熟悉的细节。为简单运行制作这个巨大的配置文件看起来有点麻烦。

虽然使用本机 api 推断 yolo 非常简单
model = YOLO(&#39;model.pt&#39;)
results = model.predict(&#39;video.mp4&#39;,save=True)

到目前为止，使用 mmdet 执行此操作似乎过于复杂。
有人知道我是否真的可以轻松地将我的模型插入 mmdet 吗？
我尝试过制作类似的配置
model = dict(
type=&#39;YOLOX&#39;,
backbone=dict(
init_cfg=dict(type=&#39;Pretrained&#39;, checkpoint=&#39;/path/to/yolo.pt&#39;)
)
)
load_from=&#39;/path/to/yolo.pt&#39;

但似乎还不足以运行。它会在配置中询问一些额外的字段]]></description>
      <guid>https://stackoverflow.com/questions/79041799/how-to-infer-a-custom-pretrained-model-using-mmdet</guid>
      <pubDate>Tue, 01 Oct 2024 05:50:15 GMT</pubDate>
    </item>
    <item>
      <title>神经网络输入数据中的特征应该是行还是列？</title>
      <link>https://stackoverflow.com/questions/79041608/should-features-be-rows-or-columns-in-input-data-for-neural-networks</link>
      <description><![CDATA[我已经实现了一个神经网络，并且对处理输入矩阵的数据形状的正确方法有疑问。具体来说，我想知道输入数据 X 是否应该在行上有示例而在列上有特征，或者反过来（行上有特征而在列上有示例）。
目前，我已经实现了它以采用形状为 (num_features, num_examples) 的 X。但是，我发现相互矛盾的来源表明，许多库和框架中的常态恰恰相反。例如，当我加载 MNIST 数据集时，它会以 (num_examples, num_features) 的形式出现，这要求我在将数据作为输入提供给神经网络之前对其进行转置
鉴于此，如果常见的做法确实是使用 (num_examples, num_features)：

我是否应该接受原始形状的 X 并在内部对其进行转置？
我是否应该更改实现以直接使用其他维度的 X？
我是否应该简单地在网络文档中记录预期形状？
]]></description>
      <guid>https://stackoverflow.com/questions/79041608/should-features-be-rows-or-columns-in-input-data-for-neural-networks</guid>
      <pubDate>Tue, 01 Oct 2024 04:07:49 GMT</pubDate>
    </item>
    <item>
      <title>了解 K 折交叉验证、模型训练和 R² 分数</title>
      <link>https://stackoverflow.com/questions/79039465/understanding-k-fold-cross-validation-model-training-and-r%c2%b2-scores</link>
      <description><![CDATA[我正在使用网格搜索设置中的 K 折交叉验证来调整超参数。我对模型的训练和评估方式有几个问题：

当我使用 GridSearchCV 时，模型会在多个折（假设为 10 个）上进行评估。对于每个超参数组合，模型会在 (K-1) 个折上进行训练，并在剩余的折上进行验证。当我在网格搜索后获得 best_grid 模型时，该模型是在哪些特定训练数据（即哪些折）上进行训练的？

当我调用 best_grid.predict(X_test) 时，该模型在哪个数据集上进行预测？它是否在网格搜索后在整个数据集上进行了训练，还是仍然基于交叉验证期间使用的折叠？

如果 best_grid 模型尚未在整个数据集上进行训练，我是否需要在进行预测之前再次明确地将其拟合到完整数据集？

我想获得 R² 训练分数，但我对使用以下代码时收到的分数感到困惑：
param_grid = {f&#39;regressor__regressor__{param}&#39;: values for param, values in model_info[&#39;params&#39;].items()}
grid_search = GridSearchCV(full_pipeline, param_grid, cv=stratified_kf.split(X, y_binned),scoring=&quot;r2&quot;, n_jobs=4, return_train_score=True)

grid_search.fit(X,y)

if grid_search.best_score_ &gt; best_score:
best_score = grid_search.best_score_
best_model = model_name
best_grid = grid_search

mean_train_score = best_grid.cv_results_[&#39;mean_train_score&#39;][best_grid.best_index_] #
print(mean_train_score) # 这里就是这个东西


]]></description>
      <guid>https://stackoverflow.com/questions/79039465/understanding-k-fold-cross-validation-model-training-and-r%c2%b2-scores</guid>
      <pubDate>Mon, 30 Sep 2024 12:59:27 GMT</pubDate>
    </item>
    <item>
      <title>使用 NumPy 从头开始​​实现 AdaGrad 优化器的问题</title>
      <link>https://stackoverflow.com/questions/79037845/problem-implementing-adagrad-optimizer-from-scratch-with-numpy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79037845/problem-implementing-adagrad-optimizer-from-scratch-with-numpy</guid>
      <pubDate>Mon, 30 Sep 2024 03:42:07 GMT</pubDate>
    </item>
    <item>
      <title>BigQuery Arima Plus 预测高于预期</title>
      <link>https://stackoverflow.com/questions/79032297/bigquery-arima-plus-forecasts-are-higher-than-expected</link>
      <description><![CDATA[我试图以 15 分钟的分辨率预测来自许多不同设备（1000 个，出于性能原因在几百个设备上进行测试）的值。所有设备都记录：名称和值。我尝试过不同的时间范围进行训练，但到目前为止还没有成功。
问题的一些示例：
设备始终记录 40 -&gt; Arima 从第一次预测（30 天训练）开始预测 20-80 的范围
设备记录 60-64 -&gt; Arima 预测 65-120 个峰值，然后在 102 时趋于平稳（1 天训练）。我正在努力理解 Arima plus 如何得出比训练数据集中的所有值都高出数量级的值。
这是 arima plus 模型创建和预测的伪代码：
我使用摘要表作为数据源，数据分辨率为 15 分钟
训练：
CREATE OR REPLACE MODEL predictioning.model
OPTIONS(
MODEL_TYPE=&#39;ARIMA_PLUS&#39;
,TIME_SERIES_TIMESTAMP_COL=&#39;timestamp&#39;
,TIME_SERIES_DATA_COL=&#39;value&#39;
,TIME_SERIES_ID_COL=&#39;id&#39;
-- ,auto_arima = TRUE
,clean_spikes_and_dips = FALSE
,adjust_step_changes = FALSE
-- ,data_frequency = &#39;AUTO_FREQUENCY&#39;
-- ,auto_arima_max_order = 2 -- 默认值为 5
-- ,max_time_series_length = 96 -- 1 d?
) AS
(
SELECT timestamp
,id
,SUM(value) as value

FROM `forecasting.summary`
WHERE CAST(TIMESTAMP_TRUNC(timestamp, DAY) AS DATE) &lt; _CUTOFF_DATE
AND CAST(TIMESTAMP_TRUNC(timestamp, DAY) AS DATE) &gt;= DATE_ADD(CURRENT_DATE(), INTERVAL _PERIOD DAY)
GROUP BY 1,2

);

虽然我确实使用了 SUM(value)，但很确定它只有 1 行的总和。从其他尝试中得出。
SELECT 
id
,forecast_timestamp as timestamp
,forecast_value as value
,standard_error 
,confidence_level 
,prediction_interval_lower_bound 
,prediction_interval_upper_bound
FROM 
ML.FORECAST(MODEL `forecasting.forecast`
,STRUCT(672 as horizo​​n -- 192 is 2d; 672 is 7d 
,0.95 as confidence_level
)
)
;

过去对我有用的一种方法是将每 15m 预测为其自己的时间序列。问题是，在这种情况下，它会导致数万或数十万个时间序列。虽然 ArimaPlus 声称能够处理数百万个数据，但如果我进行 30 天训练并且不限制最大 pdq，它的训练性能就已经很慢了。
我在预测表现出每日和每周季节性的数据时也遇到了类似的问题。在当前设备数据的情况下，可能存在与气候和天气条件相关的每日和年度季节性。
我怎样才能让 Arima 发挥作用？如果不行，你会推荐什么方法？]]></description>
      <guid>https://stackoverflow.com/questions/79032297/bigquery-arima-plus-forecasts-are-higher-than-expected</guid>
      <pubDate>Fri, 27 Sep 2024 16:57:25 GMT</pubDate>
    </item>
    <item>
      <title>在函数中创建 VLLM 对象时会导致内存错误，即使明确清除 GPU 缓存也是如此，只有共享引用才能使代码不会崩溃</title>
      <link>https://stackoverflow.com/questions/78959131/vllm-objects-cause-memory-errors-when-created-in-a-function-even-when-explicitly</link>
      <description><![CDATA[我在 Python 中使用 VLLM 库时遇到了问题。具体来说，当我在函数内部创建 VLLM 模型对象时，我遇到了内存问题，并且无法有效清除 GPU 内存，即使在删除对象并使用 torch.cuda.empty_cache() 之后也是如此。
当我尝试在函数内部实例化 LLM 对象时会出现问题，但如果我在父进程或全局范围内实例化该对象，则不会发生这种情况。这表明 VLLM 在函数中创建和管理对象时存在问题，从而导致内存保留和 GPU 耗尽。
以下是代码的简化版本：
import torch
import gc
from vllm import LLM

def run_vllm_eval(model_name, samples_params, path_2_eval_dataset):
# 在函数中实例化 LLM
llm = LLM(model=model_name, dtype=torch.float16, trust_remote_code=True)

# 在此处运行一些 VLLM 推理或评估（简化）
result = llm.generate([path_2_eval_dataset], samples_params)

# 推理后清理
del llm
gc.collect()
torch.cuda.empty_cache()

# 在此之后，GPU 内存不会被清除正确并导致 OOM 错误
run_vllm_eval()
run_vllm_eval()
run_vllm_eval()

但是
llm = run_vllm_eval2()
llm = run_vllm_eval2(llm)
llm = run_vllm_eval2(llm)

有效。
即使明确删除 LLM 对象并清除缓存后，GPU 内存仍未正确释放，导致在尝试加载或运行同一脚本中的另一个模型时出现内存不足 (OOM) 错误。
我尝试过的方法：

使用 del 删除 LLM 对象。
运行 gc.collect() 以触发 Python 的垃圾集合。
使用 torch.cuda.empty_cache() 清除 CUDA 内存。
确保父进程中没有实例化 VLLM 对象。

当在函数内创建 LLM 对象时，这些似乎都无法解决问题。
问题：

在函数内创建 VLLM 对象时，有人遇到过类似的内存问题吗？
是否有推荐的方法来管理或清除函数中的 VLLM 对象以防止 GPU 内存保留？
在这种情况下，是否存在与标准 Hugging Face 或 PyTorch 模型不同的特定 VLLM 处理技术？
]]></description>
      <guid>https://stackoverflow.com/questions/78959131/vllm-objects-cause-memory-errors-when-created-in-a-function-even-when-explicitly</guid>
      <pubDate>Sat, 07 Sep 2024 00:58:59 GMT</pubDate>
    </item>
    <item>
      <title>无法从‘typing_extensions’（/usr/local/lib/python3.10/dist-packages/typing_extensions.py）导入名称‘TypeAliasType’</title>
      <link>https://stackoverflow.com/questions/77490008/cannot-import-name-typealiastype-from-typing-extensions-usr-local-lib-pyth</link>
      <description><![CDATA[我使用 elevenlab api 在 google colab 中进行语音克隆。这是我的代码
import elevenlabs

from elevenlabs import set_api_key

set_api_key(&quot;*****************&quot;)

它给了我这个错误：

ImportError：无法从“typing_extensions”导入名称“TypeAliasType”（/usr/local/lib/python3.10/dist-packages/typing_extensions.py

我更改了 typing_extensions 的版本，但它对我来说不起作用。
我更改了 typing_extension 的版本，因为 gpt 建议我更改它的版本。但它仍然不起作用。当我安装新版本的 typing_extention 时。然后它给了我这个错误：

错误：pip 的依赖项解析器目前不考虑已安装的所有软件包。此行为是以下依赖项冲突的根源。
tensorflow-probability 0.22.0 需要 typing-extensions&lt;4.6.0，但您有不兼容的 typing-extensions 4.8.0。

以及安装]]></description>
      <guid>https://stackoverflow.com/questions/77490008/cannot-import-name-typealiastype-from-typing-extensions-usr-local-lib-pyth</guid>
      <pubDate>Wed, 15 Nov 2023 18:08:34 GMT</pubDate>
    </item>
    <item>
      <title>我无法从“typing_extensions”导入名称“TypeAliasType”</title>
      <link>https://stackoverflow.com/questions/77450322/i-cannot-import-name-typealiastype-from-typing-extensions</link>
      <description><![CDATA[我是 Python 新手，发现了以下这样的错误。非常感谢您的评论。谢谢
我尝试将 Gradio 库导入为 gr
我尝试了几个现有的建议，但结果都是徒劳的。我不知道该怎么办]]></description>
      <guid>https://stackoverflow.com/questions/77450322/i-cannot-import-name-typealiastype-from-typing-extensions</guid>
      <pubDate>Thu, 09 Nov 2023 03:38:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Android 应用程序中使用 ML 模型[关闭]</title>
      <link>https://stackoverflow.com/questions/66057364/how-to-use-ml-models-in-android-application</link>
      <description><![CDATA[我的毕业设计是关于使用数据挖掘技术或 ML 模型从数据集（而非 API）预测 Covid-19 的 Android 应用程序，其中一部分供用户按地区搜索，以了解该地区是否安全或是否挤满了 covid 病例，并提供统计数据以避免前往那里，另一部分是一个聊天机器人，它会记录用户症状并预测这些症状是 Covid-19 还是流感。
问题是我是一名 Web 开发人员，我以前没有使用过 Android Studio，也从未使用过 ML，我不知道如何开始这个项目，有人可以指导我如何开始并给我一些指导吗？这是用于毕业设计目的。
提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/66057364/how-to-use-ml-models-in-android-application</guid>
      <pubDate>Fri, 05 Feb 2021 03:55:46 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 UNet 训练图像分割模型</title>
      <link>https://stackoverflow.com/questions/55671269/cannot-train-image-segmentation-model-with-unet</link>
      <description><![CDATA[我的环境

Ubuntu 18.04
Python 3.6.8
Tensorflow 1.12.0

问题
我用 Tensorflow 做了 UNet 来分割图像。我参考了原论文，实现了相同的结构，只是输出通道数不同（用了 3 而不是 2）。我的模型看起来运行良好，但是损失不收敛，就像心电图一样……
我使用了 MSE 作为损失函数，但有些网站说 MSE 在 UNet 中无效，所以我改用 dice loss。但还是不行。我怀疑是网络结构不好。
代码
import tensorflow as tf

tf.reset_default_graph()

with tf.name_scope(&#39;input&#39;):
X = tf.placeholder(tf.float32, shape=[None, 572, 572, 3], name=&#39;X&#39;)
y = tf.placeholder(tf.float32, shape=[None, 388, 388, 3], name=&#39;y&#39;)

# 编码
with tf.name_scope(&#39;layer1&#39;):
conv1 = tf.layers.conv2d(X, filters=64, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv1&#39;)
conv2 = tf.layers.conv2d(conv1, filters=64, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv2&#39;)

使用 tf.name_scope(&#39;layer2&#39;):
pool1 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&#39;VALID&#39;, name=&#39;pool1&#39;)
conv3 = tf.layers.conv2d(pool1, filters=128, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv3&#39;)
conv4 = tf.layers.conv2d(conv3, filters=128, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv4&#39;)

使用tf.name_scope(&#39;layer3&#39;):
pool2 = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&#39;VALID&#39;, name=&#39;pool2&#39;)
conv5 = tf.layers.conv2d(pool2, filters=256, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv5&#39;)
conv6 = tf.layers.conv2d(conv5, filters=256, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv6&#39;)

使用 tf.name_scope(&#39;layer4&#39;):
pool3 = tf.nn.max_pool(conv6, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&#39;VALID&#39;, name=&#39;pool3&#39;)
conv7 = tf.layers.conv2d(pool3, filters=512, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv7&#39;)
conv8 = tf.layers.conv2d(conv7, filters=512, kernel_size=3, strides=1,activation=tf.nn.relu, name=&#39;conv8&#39;)

使用 tf.name_scope(&#39;layer5&#39;):
pool4 = tf.nn.max_pool(conv8, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&#39;VALID&#39;, name=&#39;pool4&#39;)
conv9 = tf.layers.conv2d(pool4, filters=1024, kernel_size=3, strides=1,激活=tf.nn.relu，名称=&#39;conv9&#39;)
conv10 = tf.layers.conv2d(conv9，filters=1024，kernel_size=3，strides=1，激活=tf.nn.relu，名称=&#39;conv10&#39;)

#解码
使用 tf.name_scope(&#39;layer6&#39;)：
up_conv1 = tf.layers.conv2d_transpose(conv10，filters=512，kernel_size=2，strides=2)
croped_conv8 = tf.image.central_crop(conv8，7/8)
concat1 = tf.concat([croped_conv8，up_conv1]，axis=-1)
conv11 = tf.layers.conv2d(concat1，filters=512，kernel_size=3，激活=tf.nn.relu， name=&#39;conv11&#39;)
conv12 = tf.layers.conv2d(conv11, filters=512, kernel_size=3,activation=tf.nn.relu, name=&#39;conv12&#39;)

使用 tf.name_scope(&#39;layer7&#39;)：
up_conv2 = tf.layers.conv2d_transpose(conv12, filters=256, kernel_size=2, strides=2)
croped_conv6 = tf.image.central_crop(conv6, 13/17)
concat2 = tf.concat([croped_conv6, up_conv2], axis=-1)
conv13 = tf.layers.conv2d(concat2, filters=256, kernel_size=3,activation=tf.nn.relu, name=&#39;conv13&#39;)
conv14 = tf.layers.conv2d(conv13, filters=256, kernel_size=3,activation=tf.nn.relu, name=&#39;conv14&#39;)

使用 tf.name_scope(&#39;layer8&#39;)：
up_conv3 = tf.layers.conv2d_transpose(conv14, filters=128, kernel_size=2, strides=2)
croped_conv4 = tf.image.central_crop(conv4, 5/7)
concat3 = tf.concat([croped_conv4, up_conv3], axis=-1)
conv15 = tf.layers.conv2d(concat3, filters=128, kernel_size=3,activation=tf.nn.relu, name=&#39;conv15&#39;)
conv16 = tf.layers.conv2d(conv15, filters=128，kernel_size=3，activation=tf.nn.relu，name=&#39;conv16&#39;)

使用 tf.name_scope(&#39;layer8&#39;)：
up_conv4 = tf.layers.conv2d_transpose(conv16，filters=64，kernel_size=2，strides=2)
croped_conv2 = tf.image.central_crop(conv2，49/71)
concat4 = tf.concat([croped_conv2，up_conv4]，axis=-1)
conv17 = tf.layers.conv2d(concat4，filters=64，kernel_size=3，activation=tf.nn.relu，name=&#39;conv17&#39;)
conv18 = tf.layers.conv2d(conv17，filters=64，kernel_size=3，激活=tf.nn.relu，名称=&#39;conv18&#39;)

输出=tf.layers.conv2d(conv18，过滤器=3，kernel_size=1，名称=&#39;output&#39;)

使用tf.name_scope(&#39;train&#39;)：
骰子=2 * tf.math.reduce_sum(output*y) / (tf.math.reduce_sum(output) + tf.math.reduce_sum(y) + 1)
损失=1 - 骰子
优化器=tf.train.AdamOptimizer()
训练操作=优化器.最小化(损失)

使用tf.name_scope(&#39;save&#39;)：
保存器=tf.train.Saver()
损失_sumary=tf.summary.scalar(&#39;ls&#39;，损失)
文件写入器=tf.summary.FileWriter(&#39;./&#39;， tf.get_default_graph())
]]></description>
      <guid>https://stackoverflow.com/questions/55671269/cannot-train-image-segmentation-model-with-unet</guid>
      <pubDate>Sun, 14 Apr 2019 01:32:07 GMT</pubDate>
    </item>
    </channel>
</rss>