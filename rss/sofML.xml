<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 14 Apr 2025 12:38:05 GMT</lastBuildDate>
    <item>
      <title>删除“日期”列时丢失的数据，使用模型创建预测，然后读取“日期”列</title>
      <link>https://stackoverflow.com/questions/79573034/data-lost-when-removing-date-column-creating-predictions-using-a-model-and-t</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79573034/data-lost-when-removing-date-column-creating-predictions-using-a-model-and-t</guid>
      <pubDate>Mon, 14 Apr 2025 11:28:06 GMT</pubDate>
    </item>
    <item>
      <title>对名义分类变量培训的决策树</title>
      <link>https://stackoverflow.com/questions/79572704/decision-tree-trained-on-nominal-categorical-variables</link>
      <description><![CDATA[是否有任何python实现可以通过名义（而不是序数）分类数据传递给决策树，而无需使用一个热编码？我目前正在使用Sklearn实现。这是我的论文所必需的。]]></description>
      <guid>https://stackoverflow.com/questions/79572704/decision-tree-trained-on-nominal-categorical-variables</guid>
      <pubDate>Mon, 14 Apr 2025 08:35:10 GMT</pubDate>
    </item>
    <item>
      <title>如何根据历史任务序列使用机器学习 / AI链接任务？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79572620/how-can-i-link-tasks-using-machine-learning-ai-based-on-historical-task-sequen</link>
      <description><![CDATA[我正在制定AI模型，以根据历史项目数据来预测工业计划任务之间的依赖关系。我有两个桌子：
任务表：
TaskID，TaskName，EquigmentType
（我正在考虑添加StartDate和端代码）
依赖项表：
predexpsortAskID，ScunstAskID，linkType 
 目标：
给出了一个新的任务列表（通常由设备类型过滤），我希望该模型暗示它们之间可能的依赖性（最终是链接类型） - 从现有数据中的历史模式中学到了。中的历史模式。
 我尝试的是： 
决策树
基本神经网络
 遇到的问题： 
随机或无关的链接
模型预测所有任务之间的依赖性
缺乏从历史数据中学到的逻辑流
我很确定我不会正确处理数据，因为我没有如何处理任务名称以识别“ ”的任务名称。
 我的问题：
将其作为图形问题并使用图形神经网络（GNN）是否有意义？还是在这种情况下进行建模和预测任务之间的依赖性的ML或统计方法更好？
我愿意就可能提高性能的模型架构或数据预处理策略的建议。预先感谢！]]></description>
      <guid>https://stackoverflow.com/questions/79572620/how-can-i-link-tasks-using-machine-learning-ai-based-on-historical-task-sequen</guid>
      <pubDate>Mon, 14 Apr 2025 07:51:11 GMT</pubDate>
    </item>
    <item>
      <title>我的神经网络试图找到懒惰的解决方案，而不是找到卑鄙的最佳解决方案，我不知道为什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79572183/my-neural-network-tries-to-find-a-lazy-solution-instead-of-the-optimal-solution</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79572183/my-neural-network-tries-to-find-a-lazy-solution-instead-of-the-optimal-solution</guid>
      <pubDate>Mon, 14 Apr 2025 00:00:34 GMT</pubDate>
    </item>
    <item>
      <title>参数“目标”和“输出”必须具有相同的等级（NDIM）。接收：target.shape =（none，），output.shape =（无，9）</title>
      <link>https://stackoverflow.com/questions/79572055/arguments-target-and-output-must-have-the-same-rank-ndim-received-target</link>
      <description><![CDATA[我一直在尝试创建一个可以识别图像的神经网络，但是当我尝试适合我的模型时，我会收到以下错误：
  value error trackback（最近的最新通话最后）
＆lt; ipython-Input-100-98095d26e04d＆gt;在＆lt;单元线：0＆gt;（）
      1＃训练模型
----＆gt; 2历史= model.fit（训练，batch_size = batch_size，epochs = epoch，verialation_data =测试）

1帧
/USR/local/lib/python3.11/dist-packages/keras/src/backend/backend/tensorflow/nn.py in cancorical_crossentropy（target，output，output，from_logits，axis）
    651）
    652如果Len（target.hape）！= len（output.hape）：
 - ＆gt; 653提高价值Error（
    654“参数”和“输出”必须具有相同的等级”。
    655“（NDIM）。收到：＆quot;

ValueError：参数“目标”和“输出”必须具有相同的等级（NDIM）。接收：target.shape =（none，），output.shape =（无，9）
 
我正在通过合作构建它，我正在使用的代码如下：
 导入numpy作为NP
导入matplotlib.pyplot作为PLT
导入TensorFlow作为TF

从Google.Colab Import Drive
从google.colab.patches导入cv2_imshow
drive.mount（&#39;/content/drive&#39;）


img_height = 180
img_width = 180
batch_size = 32
时期= 20
 
  normalization_layer = tf.keras.layers.Rescaling（1./255）
normalized_ds = triending.map（lambda x，y ：（ a rustaranization_layer（x），y​​））
image_batch，labels_batch = next（iter（normalized_ds））
first_image = image_batch [0]
打印（np.min（first_image），np.max（first_image））
 
  autotune = tf.data.autotune

训练=训练。
testing = testing.cache（）。prefetch（buffer_size = autotune）
 
 来自Tensorflow.keras.models导入顺序
来自tensorflow.keras.layers导入conv2d，maxpooling2d，扁平，密集
来自Tensorflow.keras.utils导入到_categorical

num_classes = 9

模型=顺序（[
  conv2d（32，（3，3），激活=&#39;relu&#39;），，
  maxpooling2d（2，2），
  conv2d（64，（3，3），激活=&#39;relu&#39;），，
  maxpooling2d（2，2），
  conv2d（128，（3，3），激活=&#39;relu&#39;），，
  maxpooling2d（2，2），
  flatten（），
  密集（256，激活=&#39;relu&#39;），
 密集（num_classes，激活=&#39;softmax&#39;）
）））
model.build（input_shape =（batch_size，img_height，img_width，3））
 
 ＃compilaçãodomodero
model.compile（优化器=&#39;adam&#39;， 
              损失=&#39;apcorical_crossentropy&#39;， 
              指标= [&#39;准确性&#39;]）

 
  history = model.fit（训练，batch_size = batch_size，epochs = epochs，verialation_data = testing）
 
训练和测试变量是使用创建的预取介台
  tf.keras.utils.image_dataset_from_directory
 
我如何解决此错误。]]></description>
      <guid>https://stackoverflow.com/questions/79572055/arguments-target-and-output-must-have-the-same-rank-ndim-received-target</guid>
      <pubDate>Sun, 13 Apr 2025 21:09:26 GMT</pubDate>
    </item>
    <item>
      <title>ML中的损失函数之间的差异[关闭]</title>
      <link>https://stackoverflow.com/questions/79571518/difference-between-loss-function-in-ml</link>
      <description><![CDATA[ Sparse_Categorical_CrossentRopy和centorical_crossentropy有什么区别？什么时候应该使用一个损失而不是另一个损失？例如，这些损失适合线性回归吗？]]></description>
      <guid>https://stackoverflow.com/questions/79571518/difference-between-loss-function-in-ml</guid>
      <pubDate>Sun, 13 Apr 2025 12:06:58 GMT</pubDate>
    </item>
    <item>
      <title>从本地模型[封闭]创建一个全局模型</title>
      <link>https://stackoverflow.com/questions/79571154/create-a-global-model-from-local-models</link>
      <description><![CDATA[ 当前场景： 
所以我有一项任务。我有一个具有时间戳，org_id，no_of_calls_on_premise，no_of_of_calls_cloud，bw_savings的数据。这是每天汇总的数据
（我也将时间戳分开，而不是直接将其用于诸如Day，Day_of_week，is_weekend，Quarter等各个领域））
现在，我已经训练了XGBoost，Random Forest和Sarimax模型等不同模型，它们具有不错的测试准确性。这项培训是对2024年的数据进行的。
训练后，我使用每个模型使用上述变量从所有3个模型中的所有模型中预测BW_SAVINGS，用于1月，2月和3月。现在，由于我已经是BW_SAVINGS，所以我可以从所有3个中获得最佳预测值，但是我做了很多组织的R2分数小于0。
现在，我也必须部署此模型，这里的数据将用于没有现有bw_savings的新组织，因此无法确认我的模型是否在几个月内表现出色。目前，我从上述模型中获取3个预测值的中位数。
 问题： 
因此，这里缺少的真正的是，这些模型正在同时培训组织的所有数据，这导致缺失趋势和不同的组织具有不同的趋势。即使我们离开趋势，每个组织的呼叫规模也会有所不同。这似乎是一个问题，因为R2得分为负。提高我的算法的任何建议。类似它为每个组织训练一个一个org，并将单个培训结合到全球模型中，以便它可以掌握每个组织中不同尺度的概念 ]]></description>
      <guid>https://stackoverflow.com/questions/79571154/create-a-global-model-from-local-models</guid>
      <pubDate>Sun, 13 Apr 2025 02:45:29 GMT</pubDate>
    </item>
    <item>
      <title>简单线性回归模型的剩余分析</title>
      <link>https://stackoverflow.com/questions/79571067/residual-analysis-for-simple-linear-regression-model</link>
      <description><![CDATA[我正在尝试进行简单线性回归的残差分析。我需要证明残差遵循近似正态分布。
我正在使用的CSV文件具有10年级分数百分比的值，而学生的薪水是
运行以下代码后，我的情节看起来像这样：
  
书中的情节看起来像这样：
 我期望我的情节像书一样出现，因为数据是相同的。我已经进行了仔细检查以确保我不会丢失任何数据等。
 数据如下：

薪水百分比
62 270000
76.33 200000
72 240000
60 250000
61 180000
55 300000
70 260000
68 235000
82.8 425000
59 240000
58 250000
60 180000
66 428000
83 450000
68 300000
37.33 240000
79 252000
68.4 280000
70 231000
59 224000
63 120000
50 260000
69 300000
52 120000
49 120000
64.6 250000
50 180000
74 218000
58 360000
67 150000
75 250000
60 200000
55 300000
78 330000
50.08 265000
56 340000
68 177600
52 236000
54 265000
52 200000
76 393000
64.8 360000
74.4 300000
74.5 250000
73.5 360000
57.58 180000
68 180000
69 270000
66 240000
60.8 300000
 
代码如下：
 ＃导入所有必需的库以构建回归模型
导入熊猫作为pd导入numpy作为np
导入statsmodels.api作为sm
来自sklearn.model_selection导入train_test_split
导入matplotlib.pyplot作为PLT

＃将数据集加载到数据框中
mba_salary_df = pd.read_csv（&#39;MBA Salary.csv&#39;）

＃将1的常数项添加到数据集
x = sm.add_constant（MBA_SALARY_DF [&#39;10年级的百分比]）
Y = MBA_SALARY_DF [&#39;SALERARY&#39;]

＃分别将数据集分别为火车和测试设置分别为80:20
train_x，test_x，train_y，test_y = train_test_split（x，y，train_size = 0.8，andural_state = 100）

＃适合回归模型
mba_salary_lm = sm.ols（train_y，train_x）.fit（）

mba_salary_resid = mba_salary_lm.resid 

probplot = sm.probplot（mba_salary_resid） 

plt.figure（无花果=（8，6）） 

probplot.ppplot（line = &#39;45&#39;） 

plt.title（回归标准化残差的正常p-p图；） 

plt.show（）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79571067/residual-analysis-for-simple-linear-regression-model</guid>
      <pubDate>Sun, 13 Apr 2025 00:11:50 GMT</pubDate>
    </item>
    <item>
      <title>如何从传感器数据中正确实现滑动窗口以实时活动识别？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79570885/how-to-properly-implement-sliding-windows-for-real-time-activity-recognition-fro</link>
      <description><![CDATA[问题摘要
我们使用可穿戴设备（加速度计 +陀螺仪）的实时数据部署了训练有素的ML模型，以供秋季检测。该模型在20秒的数据窗口上进行了训练，该数据以80 Hz采样（每个窗口1600个样本）。在部署中，我们的传感器将80个样本/秒发送到firebase，而ML代码：

每秒民意调查火箱以获取最新的80样品批次。
将样品附加到1600尺寸的Deque（滑动缓冲区）。
触发缓冲区已满时的预测，而step_interval（5秒）已通过。

尽管如此，我们的预测通常是不准确的。我们怀疑我们处理缓冲和窗户的方式可能与模型的培训期望不符。
主要问题
当传感器数据以固定的间隔流传输时，实现实时滑动窗口的正确方法是什么？

我们应该使用带有固定步幅的固定尺寸窗口（例如一次滑动80个样本）？
在此中使用step_interval基于时间的逻辑是一种不良练习
上下文？
使用20秒窗口（1600个样本）实时时间太长
检测，一个较短的窗口（例如5或10秒）可以提供更快或更实际的预测而不牺牲准确性？

这是一个简化的片段，说明了我们实施的相关部分：
  buffer_size = 1600＃20 sec窗口80 Hz
step_interval = 5＃每5秒预测一次
data_buffer = deque（maxlen = buffer_size）
last_prediction_time = time.time（）

而真：
    样本= fetch_from_firebase（）＃获取新数据批次（每秒80个样本）

    对于样本中的样本：
        ＃归一化并将每个传感器样品附加到缓冲区
        读取= np.array（[
            示例[&#39;ax&#39;]，样本[&#39;ay&#39;]，样本[&#39;az&#39;]，
            样本[&#39;gx&#39;]，样本[&#39;gy&#39;]，样品[&#39;gz&#39;]
        ]） / 10000.0
        data_buffer.append（读取）

    ＃预测逻辑（缓冲区填充后每个step_interval秒）
    current_time = time.time（）
    如果len（data_buffer）== buffer_size和（current_time -last_prediction_time）＆gt; = step_interval：
        窗口= np.array（data_buffer）.RESHAPE（1，-1）
        window_scaled = sualer.transform（窗口）
        预测= model.predict（window_scaled）[0]

        last_prediction_time = current_time
        handing_prediction（预测）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79570885/how-to-properly-implement-sliding-windows-for-real-time-activity-recognition-fro</guid>
      <pubDate>Sat, 12 Apr 2025 19:59:33 GMT</pubDate>
    </item>
    <item>
      <title>如何根据命名级别设计卷积神经网络[封闭]</title>
      <link>https://stackoverflow.com/questions/79569309/how-to-design-a-convolutional-neural-network-based-on-the-nomenclatural-rank</link>
      <description><![CDATA[我正在尝试弄清楚如何设计CNN结构，该体系结构可以使用其命名级别的等级对超过2,000种动物进行分类。这意味着我的模型将在分类层次结构的每个级别上预测图像的类别，从而逐渐缩小分类。例如，如果我有一张猫的图片，我的模型将首先将其分类为属于门的门，然后将其缩小到哺乳动物类，然后是食肉动物的顺序，依此类推，直到达到最终水平，即物种。我最初的想法是为每个级别创建多个CNN模型，但事实证明这很耗时。有人对我如何创建这种体系结构有任何想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79569309/how-to-design-a-convolutional-neural-network-based-on-the-nomenclatural-rank</guid>
      <pubDate>Fri, 11 Apr 2025 16:20:56 GMT</pubDate>
    </item>
    <item>
      <title>如何使用随机森林来预测数据集中的空白？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79567319/how-do-i-use-a-random-forest-to-predict-gaps-in-a-dataset</link>
      <description><![CDATA[我有一个用来制作一个随机森林的数据集（分为测试和培训数据）。我已经做了随机的森林并产生了预测（下面的代码），但是我不知道如何进行这些预测并使用它们来生成一个完整的数据表，其中包含缝隙填充值。
  #DATA表头
时间戳＆lt;  -  c（2019-05-31 17：00：00：00：00 2019-05-31 17：30：00：00 2019-05-31 18：00：00：00：00：00：00：00：00：00：00：00 2019-05-31 18：00：30：00：00：00：00：00＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot 2019-05-31 19：30：00； 2019-05-31 20：00：00：00：00 2019-05-31 20：30：30：00 2019-05-05-31 21：00：00：00：00：00：00：00：00：00;
RH＆LT; -c（38、40、41、42、44、49、65、72、74、77）
FCH4＆lt;  -  C（0.045，-0.002，0.001，0.004，0，-0.013，0.004，-0.003，-0.001，-0.002）
距离＆lt;  -  c（1000,1000,180,125.35,1000,180,1000,5.50,180,1000）
ta＆lt; -c（29.52，29.01，29.04，28.39，27.87，26.68，23.28，21.16，19.95，19.01）
fe＆lt;  -  c（95.16，68.95，68.62，39.24，35.04，27.26，-2.60，5.09,7.28，2.08）

dd＆lt;  -  data.Frame（Timestamp，RH，FCH4，距离，TA，FE）

＃制造RF和预测
set.seed（1）
Intraining＆lt;  -  CreateTataPartition（DD $ FCH4，P = 0.65，List = false）
训练＆lt;  -  dd [intraining，]
测试＆lt;  -  dd [ -  intraining，]

set.seed（1）
pfpfit＆lt;  -  Randomforest（FCH4〜。，训练，NTREE = 500，type =＆quot&#39;回归＆quot;）
预测＆lt;  - 预测（pfpfit，newdata =测试）
 
因此，对于上述代码，我有预测模型，但是我不知道如何将其应用于我已经拥有差距的数据集（下面）。我也不知道在变量中差距不是我要差距填充的变量（我想差距填充FCH4）是否有问题，但是我也有FE和TA的空白）。
我想差距填充数据集的一个示例如下：
  #DATA表头
    时间戳＆lt;  -  c（2019-05-31 17：00：00：00：00 2019-05-31 17：30：00：00 2019-05-31 18：00：00：00：00：00：00：00：00：00：00：00 2019-05-31 18：00：30：00：00：00：00：00＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot 2019-05-31 19：30：00； 2019-05-31 20：00：00：00：00 2019-05-31 20：30：30：00 2019-05-05-31 21：00：00：00：00：00：00：00：00：00;
    RH＆LT; -c（38、40、41、42、44、49、65、72、74、77）
    FCH4＆lt; -c（Na，-0.002，0.001，0.004，Na，-0.013，0.004，Na，-0.001，-0.002）
    距离＆lt;  -  c（1000,1000,180,125.35,1000,180,1000,5.50,180,1000）
    ta＆lt; -c（29.52，29.01，NA，28.39，27.87，26.68，23.28，NA，19.95，19.01）
    fe＆lt; -c（Na，Na，68.62，39.24，35.04，27.26，-2.60，Na，7.28，2.08）
dd＆lt;  -  data.Frame（Timestamp，RH，FCH4，距离，TA，FE）
 
我希望填充的数据集看起来像这样：
  #DATA表头
    时间戳＆lt;  -  c（2019-05-31 17：00：00：00：00 2019-05-31 17：30：00：00 2019-05-31 18：00：00：00：00：00：00：00：00：00：00：00 2019-05-31 18：00：30：00：00：00：00：00＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot 2019-05-31 19：30：00； 2019-05-31 20：00：00：00：00 2019-05-31 20：30：30：00 2019-05-05-31 21：00：00：00：00：00：00：00：00：00;
    RH＆LT; -c（38、40、41、42、44、49、65、72、74、77）
    FCH4＆lt;  -  C（0.045，-0.002，0.001，0.004，0，-0.013，0.004，-0.003，-0.001，-0.002）
    距离＆lt;  -  c（1000,1000,180,125.35,1000,180,1000,5.50,180,1000）
    ta＆lt; -c（29.52，29.01，29.04，28.39，27.87，26.68，23.28，21.16，19.95，19.01）
    fe＆lt;  -  c（95.16，68.95，68.62，39.24，35.04，27.26，-2.60，5.09,7.28，2.08）
dd＆lt;  -  data.Frame（Timestamp，RH，FCH4，距离，TA，FE）
 
（我意识到这三个数据集大多是相同的，并且您不应该在与培训数据相同的数据上测试。这仅仅是为了使某些内容运行。我可以自己完善它。））]]></description>
      <guid>https://stackoverflow.com/questions/79567319/how-do-i-use-a-random-forest-to-predict-gaps-in-a-dataset</guid>
      <pubDate>Thu, 10 Apr 2025 18:12:26 GMT</pubDate>
    </item>
    <item>
      <title>模块“ keras.layers”没有属性“实验”</title>
      <link>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</link>
      <description><![CDATA[您好，我试图调整大小和重新列出我的数据集，如下所示，但我遇到了此错误：
 attributeError：模块&#39;keras.layers&#39;没有属性&#39;实验&#39;
 
resize_and_rescale = tf.keras.Sequeential（[
    layers.experiment.preprocessing.resizing（image_size，image_size），
    layers.expermentim.preprocessing.Rescaling（1.0/255）
）））

 ]]></description>
      <guid>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</guid>
      <pubDate>Wed, 14 Dec 2022 00:43:49 GMT</pubDate>
    </item>
    <item>
      <title>如何改善Levenberg-Marquardt的多项式曲线拟合方法？</title>
      <link>https://stackoverflow.com/questions/62231658/how-to-improve-levenberg-marquardts-method-for-polynomial-curve-fitting</link>
      <description><![CDATA[几周前，我开始从Matlab的Scratch中编码Levenberg-Marquardt算法。我对数据的多项式拟合感兴趣，但是我无法达到我想要的准确性水平。尝试其他多项式后，我正在使用第五阶多项式，这似乎是最好的选择。无论我尝试实现哪些改进，该算法始终会收敛到相同的功能最小化。到目前为止，我未能添加以下功能：

测量加速度作为二阶校正
延迟满足阻尼参数
增益因素以靠近高斯 - 纽顿方向或
最陡峭的下降方向取决于迭代。
有限差异方法的主要差异和正向差异

我没有非线性最小二乘的经验，所以我不知道是否有一种方法可以最大程度地减少剩余的方法，或者使用这种方法没有更多的改进空间。我将在最后一次迭代的多项式行为的图像下附加。如果我运行代码以进行更多迭代，则曲线最终不会从迭代变为迭代。正如观察到的那样，从时间= 0到时间= 12。
  ]]></description>
      <guid>https://stackoverflow.com/questions/62231658/how-to-improve-levenberg-marquardts-method-for-polynomial-curve-fitting</guid>
      <pubDate>Sat, 06 Jun 2020 12:22:40 GMT</pubDate>
    </item>
    <item>
      <title>批归一化，是还是否？ [关闭]</title>
      <link>https://stackoverflow.com/questions/58612783/batch-normalization-yes-or-no</link>
      <description><![CDATA[我使用TensorFlow 1.14.0和Keras 2.2.4。以下代码实现了一个简单的神经网络：
 导入numpy作为NP
np.random.seed（1）
导入随机
随机。种子（2）
导入TensorFlow作为TF
tf.set_random_seed（3）

来自Tensorflow.keras.models导入模型，顺序
来自tensorflow.keras.layers导入输入，密集，激活


x_train = np.random.normal（0,1，（100,12））

型号=顺序（）
ADD（密集（8，Input_Shape =（12，））））
＃model.add（tf.keras.layers.batchnormalization（））
ADD（激活（&#39;Linear&#39;））
型号（密集（12））
ADD（激活（&#39;Linear&#39;））
model.compile（优化器=&#39;adam&#39;，loss =&#39;mean_squared_error&#39;）
model.fit（x_train，x_train，epochs = 20，验证_split = 0.1，shuffle = false，derbose = 2）
 
 20个时期后的最终val_loss为0.7751。当我删除添加批处理标准化层的唯一评论行时，val_loss更改为1.1230。
我的主要问题更为复杂，但是发生同样的事情。由于我的激活是线性的，因此是否在激活之后或之前将批处理归一化都无关紧要。 
 问题：为什么批处理归一化无济于事？我有什么可以更改的，以便批处理归一化可以改善结果而不更改激活功能？
 发表评论后更新： 
一个带有一个隐藏层和线性激活的NN有点像PCA。有很多论文。对我来说，此设置在隐藏层和输出的激活函数的所有组合中给出了最小的MSE。
一些陈述线性激活的资源是指PCA：
  https://arxiv.org/pdf/pdf/1702.07800.pdf    
   https：////www.quora.com/www.quora.com/www.quora.com/www.quora/]]></description>
      <guid>https://stackoverflow.com/questions/58612783/batch-normalization-yes-or-no</guid>
      <pubDate>Tue, 29 Oct 2019 17:38:38 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络中的分批归一化[封闭]</title>
      <link>https://stackoverflow.com/questions/38553927/batch-normalization-in-convolutional-neural-network</link>
      <description><![CDATA[我想知道有关在CNN中应用批处理标准化的一些细节。
I read this paper https://arxiv.org/pdf/1502.03167v3.pdf and could understand the BN algorithm applied on a data but in the end they mentioned that a将应用于CNN时需要轻微的修改：

对于卷积层，我们还希望归一化遵守卷积特性 - 以便以相同的方式将同一特征图的不同元素（在不同位置）进行标准化。为了实现这一目标，我们将所有位置的小批次中的所有激活共同标准化。在alg。 1，我们让b是小批量和空间位置的两个元素的特征映射中的所有值集 - 因此，对于MINI尺寸m和大小p×Q的特征映射，我们使用尺寸m&#39;= | b |的有效的小批量的小批量。 = M·Pq。我们每个特征图学习一对参数γ（k）和β（k），而不是每个激活。 alg。 2进行了类似的修改，因此在推断中，BN转换在给定特征映射中适用于每个激活的相同线性变换。

当他们说时，我完全感到困惑

使得在不同位置的同一特征图的不同元素以相同的方式归一化

我知道特征映射的含义，不同的元素是每个功能映射中的权重。但是我不明白位置或空间位置的含义。
我根本无法理解以下句子：

在ALG中。 1，我们让b为小批次和空间位置的两个元素的元素中的特征图中的所有值集

有人可以在更简单的任期内详细说明我吗？]]></description>
      <guid>https://stackoverflow.com/questions/38553927/batch-normalization-in-convolutional-neural-network</guid>
      <pubDate>Sun, 24 Jul 2016 15:54:58 GMT</pubDate>
    </item>
    </channel>
</rss>