<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 09 Aug 2024 18:20:28 GMT</lastBuildDate>
    <item>
      <title>如何下载机器学习的数据集但仅限于我的 python 文件中？</title>
      <link>https://stackoverflow.com/questions/78854017/how-do-i-download-datasets-for-machine-learning-but-only-in-my-python-file</link>
      <description><![CDATA[正在做一个与机器学习和音频文件相关的项目，需要下载音频数据集。我正在按照本教程 https://www.tensorflow.org/tutorials/audio/simple_audio 进行操作，但我不知道如何获取像本教程用于设置原点的链接。我正在使用 Google Collab。
DATASET_PATH = &#39;data/mini_speech_commands&#39;

data_dir = pathlib.Path(DATASET_PATH)
if not data_dir.exists():
tf.keras.utils.get_file(
&#39;mini_speech_commands.zip&#39;,
origin=&quot;http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip&quot;,
extract=True,
cache_dir=&#39;.&#39;, cache_subdir=&#39;data&#39;)

commands = np.array(tf.io.gfile.listdir(str(data_dir)))
commands = commands[(commands != &#39;README.md&#39;) &amp; (commands != &#39;.DS_Store&#39;)]
print(&#39;Commands:&#39;, command)

按照教程中的所有内容，它可以正常工作，但我不明白将音频数据集放入我的 python 文件中的过程以及如何浏览每个音频]]></description>
      <guid>https://stackoverflow.com/questions/78854017/how-do-i-download-datasets-for-machine-learning-but-only-in-my-python-file</guid>
      <pubDate>Fri, 09 Aug 2024 17:32:08 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用 Movenet 来检测正确和不正确的姿势？</title>
      <link>https://stackoverflow.com/questions/78853603/how-would-i-use-movenet-to-detect-correct-and-incorrect-poses</link>
      <description><![CDATA[我对机器学习还很陌生，目前正在尝试编写一个程序，使用 Tensorflow、Keras 和 Movenet 检测正确和不正确的坐姿。目前，我在训练、有效和测试文件夹中有好姿势和坏姿势文件夹，其中包含相应的姿势。我对编写模型的方法有一些想法，但我不确定哪种方法真正有效/最合适：

保留好姿势/坏姿势文件夹，使用数据构建和训练 CNN，然后构建/训练微调神经网络以对好姿势和坏姿势进行分类

仅保留好姿势图像，使用斜率计算某些身体部位的正确位置，不构建 CNN。我能想到的唯一问题是，如果某个身体部位阻碍了另一个身体部位怎么办？例如，我有一张正确姿势的照片，我的手臂放在桌子上，准备打字，但我还有另一张正确姿势的照片，我的手臂放在身体两侧，前臂向下是看不见的。或者姿势不正确，如果有人向一侧倾斜，脸靠在手上，身体的某些部位就看不见了，该怎么办？我或 Movenet 会如何处理这种情况？

完全不同的方法


如果有人能帮我指明正确的方向，我将不胜感激！提前谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78853603/how-would-i-use-movenet-to-detect-correct-and-incorrect-poses</guid>
      <pubDate>Fri, 09 Aug 2024 15:33:58 GMT</pubDate>
    </item>
    <item>
      <title>无法将标准 torchvision ResNet50 模型导出到 ONNX 文件</title>
      <link>https://stackoverflow.com/questions/78853571/cant-export-standard-torchvision-resnet50-model-into-onnx-file</link>
      <description><![CDATA[我制作了一个非常简单的 Python 脚本，它加载 torchvision ResNet50 模型并尝试以两种方式导出到 onnx 文件（torch.onnx.export 和 torch.onnx.dynamo_export）
import torch
import torch.onnx

import torchvision

torch_model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2( weights=&#39;DEFAULT&#39;)
torch_model.eval()
torch_input = torch.randn(1, 3, 32, 32)

is_dynamo_export = False

if (is_dynamo_export):
onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)
onnx_program.save(&quot;onnx_dynamo_export_ResNET50.onnx&quot;) 
else:
torch.onnx.export(torch_model, # 正在运行的模型
torch_input, # 模型输入（或多个输入的元组）
&quot;onnx_export_ResNET50.onnx&quot;, # 模型保存位置（可以是文件或类似文件的对象）
export_params=True, # 将训练后的参数权重存储在模型文件中
opset_version=10, # 将模型导出到的 ONNX 版本
do_constant_folding=True, # 是否执行常量折叠以进行优化
input_names = [&#39;input&#39;], # 模型的输入名称
output_names = [&#39;output&#39;], # 模型的输出名称
dynamic_axes={&#39;input&#39; : {0 : &#39;batch_size&#39;}, # 可变长度轴
&#39;output&#39; : {0 : &#39;batch_size&#39;}}) 

出现错误：
文件“C:\tools\Python311\Lib\site-packages\torch\onnx\_internal\exporter.py”，第 1439 行，在 dynamo_export
raise OnnxExporterError(

torch.onnx.OnnxExporterError：无法将模型导出到 ONNX。在“report_dynamo_export.sarif”处生成 SARIF 报告。SARIF 是静态分析工具输出的标准格式。SARIF 日志可以在 VS Code SARIF 查看器扩展或 SARIF Web 查看器（https://microsoft.github.io/sarif-web-component/）中加载。请在 PyTorch Github 上报告错误：https://github.com/pytorch/pytorch/issues

torch.onnx.errors.SymbolicValueError：不支持：ONNX 导出 opset 9 中的 Pad。填充的大小必须是恒定的。请尝试 opset 版本 11。[由 (%535 : int[] = prim::ListConstruct(%405, %534, %405, %533, %405, %532) 中定义的值 &#39;535 引起，范围：torchvision.models.detection.faster_rcnn.FasterRCNN::

这两种方法都适用于极其简单的模型，例如
class MyModel(nn.Module):

def __init__(self):
super(MyModel, self).__init__()
self.conv1 = nn.Conv2d(1, 6, 5)
self.conv2 = nn.Conv2d(6, 16, 5)
self.fc1 = nn.Linear(16 * 5 * 5, 120)
self.fc2 = nn.Linear(120, 84)
self.fc3 = nn.Linear(84, 10)

def forward(self, x):
x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
x = F.max_pool2d(F.relu(self.conv2(x)), 2)
x = torch.flatten(x, 1)
x = F.relu(self.fc1(x))
x = F.relu(self.fc2(x))
x = self.fc3(x)
return x
]]></description>
      <guid>https://stackoverflow.com/questions/78853571/cant-export-standard-torchvision-resnet50-model-into-onnx-file</guid>
      <pubDate>Fri, 09 Aug 2024 15:25:36 GMT</pubDate>
    </item>
    <item>
      <title>我正在做一个项目，需要完成遮挡形状</title>
      <link>https://stackoverflow.com/questions/78853333/i-am-working-on-a-project-in-which-i-have-to-complete-occluded-shapes</link>
      <description><![CDATA[我正在做一个项目来补全遮挡形状。
我浏览了互联网，发现我们可以使用插值方法来完成不完整的曲线，但唯一的问题是，在我使用的数据中（在包含每条曲线的点的 csv 文件中），曲线是封闭的……有什么想法可以做到这一点吗？

就像下面给出的这张图片一样，输入是一个 csv 文件，其中包含 3 条曲线的数据点，这些曲线都是封闭的。如果我单独绘制被遮挡的椭圆的输入点，它们将像第二张照片一样出现，而不仅仅是不完整的椭圆边界曲线。

输出应如下所示：
]]></description>
      <guid>https://stackoverflow.com/questions/78853333/i-am-working-on-a-project-in-which-i-have-to-complete-occluded-shapes</guid>
      <pubDate>Fri, 09 Aug 2024 14:32:17 GMT</pubDate>
    </item>
    <item>
      <title>无法设置张量：获取 STRING 类型的值，但输入 0 应为 FLOAT32 类型，名称：serving_default_args_0:0</title>
      <link>https://stackoverflow.com/questions/78853060/cannot-set-tensor-got-value-of-type-string-but-expected-type-float32-for-input</link>
      <description><![CDATA[当我尝试加载模型时，我收到一条错误消息
这是我的代码

interpreter = tf.lite.Interpreter(model_path=&quot;/content/colab_model.tflite&quot;)
interpreter.allocate_tensors()

# 获取输入和输出张量。
input_details = explainer.get_input_details()
output_details = explainer.get_output_details()
# 打印输入详细信息以了解其预期类型和形状
for input_detail in input_details:
print(f&quot;Name: {input_detail[&#39;name&#39;]}, Shape: {input_detail[&#39;shape&#39;]}, Dtype: {input_detail[&#39;dtype&#39;]}&quot;)

# 将用户 ID 转换为字符串
user_id_value = np.array([&#39;42&#39;], dtype=np.str_) 

# 将配方 ID 转换为字符串
recipe_id_values = np.array([&#39;49&#39;, &#39;66&#39;, &#39;62&#39;], dtype=np.str_)

# 使用字符串设置张量值
interpreter.set_tensor(input_details[0][&#39;index&#39;], np.array([recipe_id_values[0]])) 
interpreter.set_tensor(input_details[1][&#39;index&#39;], user_id_value)

# 调用模型
interpreter.invoke()

# 获取输出
output_data = interpretationer.get_tensor(output_details[0][&#39;index&#39;])
print(output_data)


错误
ValueError：无法设置张量：获得 STRING 类型的值，但输入 0 应为 FLOAT32 类型，名称：serving_default_args_0:0
当我尝试将其首先转换为浮点 32 时，出现错误
RuntimeError：不支持将浮点转换为字符串委托内核不支持已初始化节点号 19（TfLiteFlexDelegate）准备失败。委托内核未初始化节点号 19（TfLiteFlexDelegate）准备失败。委托内核未初始化节点号 19（TfLiteFlexDelegate）准备失败。]]></description>
      <guid>https://stackoverflow.com/questions/78853060/cannot-set-tensor-got-value-of-type-string-but-expected-type-float32-for-input</guid>
      <pubDate>Fri, 09 Aug 2024 13:30:18 GMT</pubDate>
    </item>
    <item>
      <title>集成识别手绘形状并实时重绘的功能</title>
      <link>https://stackoverflow.com/questions/78852946/integrate-a-feature-to-recognize-hand-drawn-shapes-and-redraw-it-in-real-time</link>
      <description><![CDATA[我正在构建一个项目，它可以跟踪我的手指运动并根据我的运动在屏幕上绘图。为此，我使用了 mediaPipe Hands 解决方案。我想集成一个功能来识别圆形、矩形等形状，并以理想的形式重新绘制它。
（当然是视频流）
我实现了一个功能来识别使用 OpenCV 在屏幕上绘制的形状。我尝试在绘图模式下捕获手指跟踪的点，并使用这些点来检测所绘制的形状是圆形还是矩形。我期望它在完成后准确识别形状。然而，实际发生的是，一旦我进入绘图模式，系统就开始将我的手指检测为圆形，并在手指移动到的每个点周围绘制一个圆圈。
#############
def understand_shape(points):
if len(points) &lt; 5：
return None

# 计算点的边界框
x_coords, y_coords = zip(*points)
min_x, max_x = min(x_coords), max(x_coords)
min_y, max_y = min(y_coords), max(y_coords)
width, height = max_x - min_x, max_y - min_y

# 使用半径方差检查圆
center_x, center_y = np.mean(x_coords), np.mean(y_coords)
radii = [distance.euclidean((x, y), (center_x, center_y)) for x, y in points]
mean_radius = np.mean(radii)
radius_variance = np.var(radii)

if radius_variance &lt; 1000：# 调整阈值以提高准确度
return (&quot;circle&quot;, (int(center_x), int(center_y), int(mean_radius)))

# 使用纵横比检查矩形
if 0.9 &lt; width / height &lt; 1.1：# 允许正方形略有偏差
if all(min_x &lt;= x &lt;= max_x and min_y &lt;= y &lt;= max_y for x, y in points):
return (&quot;rectangle&quot;, (min_x, min_y, max_x, max_y))

return None

def draw_shape(shape, imgCanvas):
if shape[0] == &quot;circle&quot;:
_, (center_x, center_y, radius) = shape
cv2.circle(imgCanvas, (center_x, center_y), radius, (0, 0, 255), 2)
elif shape[0] == &quot;rectangle&quot;:
_, (min_x, min_y, max_x, max_y) = shape
cv2.rectangle(imgCanvas, (min_x, min_y), (max_x, max_y), (0, 0, 255), 2)
#############
]]></description>
      <guid>https://stackoverflow.com/questions/78852946/integrate-a-feature-to-recognize-hand-drawn-shapes-and-redraw-it-in-real-time</guid>
      <pubDate>Fri, 09 Aug 2024 13:06:37 GMT</pubDate>
    </item>
    <item>
      <title>机器学习/统计学：如何找到参数之间的因果关系？[关闭]</title>
      <link>https://stackoverflow.com/questions/78852790/machine-learning-statistics-how-to-find-causal-relationship-between-parameter</link>
      <description><![CDATA[我的公司生产嵌入式设备，我们从这些设备中收集了一堆参数（几百个）并将它们保存在 CSV 文件中。现在我想找出一个重要的错误参数和所有其他参数之间的因果关系。到目前为止，我所做的就是训练一个 SVM 模型，将重要的错误参数作为目标，将其余参数作为特征，然后找到对目标参数贡献最大的特征。我对结果非常满意，因为贡献最大的标签实际上是有意义的，并且该模型在测试数据上的准确率超过了 90%。
但我不知道如何从这些结果继续找到特定事件中的贡献参数。例如，如果触发了这个错误参数 - 我如何使用我训练过的模型知道哪个参数是贡献最大的参数（假设所有其他参数的上下文）？目前，该模型只能告诉我触发错误参数的可能性 - 但不能告诉我相反的可能性。
我希望我在这里描述的是有意义的。抱歉，我的英语不是我的母语。]]></description>
      <guid>https://stackoverflow.com/questions/78852790/machine-learning-statistics-how-to-find-causal-relationship-between-parameter</guid>
      <pubDate>Fri, 09 Aug 2024 12:31:24 GMT</pubDate>
    </item>
    <item>
      <title>在浏览器/API 中通过 RTSP 流实时处理低延迟视频的方法</title>
      <link>https://stackoverflow.com/questions/78852698/ways-for-real-time-processing-of-video-via-rtsp-stream-in-browser-api-with-low-l</link>
      <description><![CDATA[对于 RTSP 流，我想使用基于 Python 的模型来处理它，问题是延迟，对于实时处理，没有太多经济上可行的选择，这些选择由于延迟问题而受到限制。
独特而有趣的方式是使用 roboflow.js 进行浏览器推理，但将 RTSP 转换为另一种格式，以便可以使用该流，这给我带来了另一个问题。
处理结果的整个流需要在 UI 上显示
鉴于需要使用 Python 实用程序实时处理 RTSP 流，最佳选择是什么？
我已经通过命令行工具获取 RTSP 流然后转换为 HLS 格式，但流是在 10 秒内创建的，除了处理（鉴于我已经有一个模型），它可能需要更长的时间，那么有没有更好的方法可以处理 RTSP 并实时处理？]]></description>
      <guid>https://stackoverflow.com/questions/78852698/ways-for-real-time-processing-of-video-via-rtsp-stream-in-browser-api-with-low-l</guid>
      <pubDate>Fri, 09 Aug 2024 12:12:35 GMT</pubDate>
    </item>
    <item>
      <title>云管理矢量数据库选项或图像匹配选项</title>
      <link>https://stackoverflow.com/questions/78852684/cloud-managed-vector-database-options-or-image-matching-options</link>
      <description><![CDATA[我们有一个项目，将图像转换为矢量并存储在 Milvus 数据库中。之后，我们检查图像之间的相似性以找到矢量中距离较小的图像。
目前，我们自己管理 Milvus 数据库集群。我正在寻找矢量数据库的云托管解决方案。是否有任何云提供商提供此类解决方案，例如用于 SQL 数据库的 AWS RDS。
我还检查了 AWS Rekognize，它可以识别图像中的脸部。市场上还有其他解决方案可以提供矢量图像中的距离吗？]]></description>
      <guid>https://stackoverflow.com/questions/78852684/cloud-managed-vector-database-options-or-image-matching-options</guid>
      <pubDate>Fri, 09 Aug 2024 12:10:49 GMT</pubDate>
    </item>
    <item>
      <title>搜索重复图片</title>
      <link>https://stackoverflow.com/questions/78852645/search-duplicate-images</link>
      <description><![CDATA[我有许多图片，大约有 20 万张，而且每天都在增加。这些图片中有一些是重复的。有些图片完全相同，有些则被裁剪、旋转、沿 x 或 y 轴移动等。我想检测重复的图片。重复是指两张图片的一部分完全相同，我并不是指具有不同相机视角、图像与物体之间距离不同的不同图片。如果图片是重复的，则无论它们被旋转、裁剪或进行其他处理，它们都是完全相同的或它们的部分匹配。我在下面添加了四个示例图像。







请给我一些建议来解决这个问题。如果我使用的算法有问题，你可以纠正我。
这是我尝试检测的：
哈希算法
如果图像完全相同，哈希算法会很好地工作。但是，如果图像被裁剪，则哈希算法无法将它们视为重复。 这是我使用的哈希算法
深度学习算法
深度学习算法可能是文献中最常用的技术。但是，它们不足以解决我的问题，因为深度学习算法也考虑了上下文信息。例如，深度学习算法发现两个不同的电力分配变压器图像具有很高的相似性。例如，当我提取第一幅和第四幅图像的嵌入并计算它们之间的余弦相似度时，我得到了 0.63 的相似度得分。
模板匹配
模板匹配工作正常，但我认为它很昂贵，而且我不确定如何选择哪幅图像或图像的一部分作为模板。
直方图匹配
我将图像分成多个块并计算每个块的直方图。然后计算这些块之间的直方图相似度以匹配图像部分。但是，它没有给出好的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78852645/search-duplicate-images</guid>
      <pubDate>Fri, 09 Aug 2024 12:00:57 GMT</pubDate>
    </item>
    <item>
      <title>每秒 2500 帧视频处理的硬件要求</title>
      <link>https://stackoverflow.com/questions/78852250/hardware-requirement-for-video-processing-of-2500-frames-per-second</link>
      <description><![CDATA[我有 50 个摄像头，每个摄像头都提供 1080p 分辨率的馈送，我正在使用 YOLOv8n 模型在这些摄像头上应用视频分析和计算机视觉技术。为了满足我的要求，我需要每秒处理大约 2,500 帧。
我在配备 32GB RAM 的 Rtx3090 GPU 上进行了测试，我能够每秒处理 200 帧。
您能否建议实现此目标所需的适​​当硬件和 GPU 设置？此外，是否有经济高效的替代方案，例如使用多个 Jetson AGX Orin 设备或选择基于云的解决方案？]]></description>
      <guid>https://stackoverflow.com/questions/78852250/hardware-requirement-for-video-processing-of-2500-frames-per-second</guid>
      <pubDate>Fri, 09 Aug 2024 10:24:21 GMT</pubDate>
    </item>
    <item>
      <title>如何在多视图中进行相机校准</title>
      <link>https://stackoverflow.com/questions/78852205/how-to-make-camera-calibration-in-multi-view</link>
      <description><![CDATA[我使用 Panoptic 数据集训练 VoxelPose。
因此，我尝试使用我们的自定义数据集实现训练好的模型。
但是，我们的五个摄像机没有摄像机校准信息（K、distCoef、R 和 t）。
我们只有同时录制的 5 个视频。
我们在每个视频的每个帧中提取了 2D 姿势。
我们从每个帧中提取了 3D 姿势。
现在，
在这种情况下如何进行摄像机校准？

可以进行多视图校准，也可以单独校准每个摄像机。

我正在尝试在 Python 中打开 CV2，例如 cv2.calibrateCamera
但我无法获得正确的校准。]]></description>
      <guid>https://stackoverflow.com/questions/78852205/how-to-make-camera-calibration-in-multi-view</guid>
      <pubDate>Fri, 09 Aug 2024 10:07:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 device_map 选择可用的 GPU 设备</title>
      <link>https://stackoverflow.com/questions/78852192/choose-available-gpu-devices-with-device-map</link>
      <description><![CDATA[from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
model_id,
torch_dtype=torch.bfloat16,
device_map=&quot;cuda:3&quot;,
)

服务器上有很多 GPU，但我只能使用其中两个。我应该如何配置 device_map（或其他参数）才能让模型在两个 GPU 上运行？]]></description>
      <guid>https://stackoverflow.com/questions/78852192/choose-available-gpu-devices-with-device-map</guid>
      <pubDate>Fri, 09 Aug 2024 10:04:34 GMT</pubDate>
    </item>
    <item>
      <title>将图像数据地理配准到 Google staelite 地图的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/78852106/what-is-the-best-way-to-georeference-image-data-to-google-staelite-map</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78852106/what-is-the-best-way-to-georeference-image-data-to-google-staelite-map</guid>
      <pubDate>Fri, 09 Aug 2024 09:42:51 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Tensorflow / Python 中修剪这个神经网络？</title>
      <link>https://stackoverflow.com/questions/78851708/how-can-i-prune-this-neural-network-in-tensorflow-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78851708/how-can-i-prune-this-neural-network-in-tensorflow-python</guid>
      <pubDate>Fri, 09 Aug 2024 07:50:59 GMT</pubDate>
    </item>
    </channel>
</rss>