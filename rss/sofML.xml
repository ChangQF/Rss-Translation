<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 17 Nov 2024 15:16:23 GMT</lastBuildDate>
    <item>
      <title>创建检测区块链异常任务的代表性子集</title>
      <link>https://stackoverflow.com/questions/79197028/creating-representative-subset-for-detecting-blockchain-anomalies-task</link>
      <description><![CDATA[我目前正在从事大学小组项目，我们必须创建云解决方案，在该解决方案中，我们从三个网络（solana、比特币、以太坊）收集和转换区块链交易数据，然后使用机器学习方法进行异常检测。为了首先降低成本，我们希望获取大约 30GB-50GB 的数据（而不是 TB），并在本地进行训练，以确定哪种 ML 方法最适合此任务。
问题是我们真的不知道应该采取什么方法来为我们的子集选择数据。我们曾考虑过从选定的时间段（例如 3 个月）获取数据，但问题是 Solana 数据集在数据量方面要大几倍（300 TB 对比比特币和以太坊的约 &lt;10TB - 这实际上也会在云端出现问题）。此外，减少选定时间段内的 solana 量可能会是一个问题，因为我们可能会通过这种方式摆脱一些数据模式（选定钱包地址的交易频率是一个重要因素）。缩短 solana 的窗口期是否正确？（例如，从比特币和以太坊中抽取 3 个月，而 solana 只抽取 1 周，导致每个网络的数据大小和交易数量相似）还是太短而无法反映模式？如何实际处理？
此外，我们知道数据集在类别方面是不平衡的（少数交易是异常的），但我们希望在选择子集后执行平衡方法（以反映我们将在云上处理的环境，并使用整个数据集进行平衡）
你有什么建议？]]></description>
      <guid>https://stackoverflow.com/questions/79197028/creating-representative-subset-for-detecting-blockchain-anomalies-task</guid>
      <pubDate>Sun, 17 Nov 2024 10:56:17 GMT</pubDate>
    </item>
    <item>
      <title>有条件地更改 RGB 图像中的像素值，然后删除通道</title>
      <link>https://stackoverflow.com/questions/79196999/conditionally-changing-the-pixel-values-in-an-rgb-image-then-removing-the-chann</link>
      <description><![CDATA[我只想比较图像的像素，如果这个像素是粉红色（R 值 = 0.502、G 值 = 0.0、B 值 = 0.502）则将其更改为黑色，否则将其更改为白色。在此之后，我想删除通道，只得到一个 (512,512,) 形状的张量。
此 getitem 位于我的数据集类中。
代码：
 def __getitem__(self, index):
img = Image.open(self.images[index]).convert(&quot;RGB&quot;) # 这是一张图像
mask = Image.open(self.masks[index]).convert(&quot;RGB&quot;) # 这是相应的掩码
mask = self.transform_tensor(mask) # 张量为 (3,512,512) 形状
mask = torch.where((mask[0, :, :] == 0.502) &amp; (mask[1, :, :] == 0.0) &amp; (mask[2, :, :] == 0.502), torch.tensor([0.0, 0.0, 0.0]), torch.tensor([1.0, 1.0, 1.0])) # 错误
mask = self.transform_to_image(mask)
mask.show()
return self.transform_image(img), self.transform_mask(mask)


RuntimeError: 张量 a (512) 的大小必须与非单例维度 1 上的张量 b (3) 的大小匹配
错误：意外类型：(bool, Tensor, Tensor)]]></description>
      <guid>https://stackoverflow.com/questions/79196999/conditionally-changing-the-pixel-values-in-an-rgb-image-then-removing-the-chann</guid>
      <pubDate>Sun, 17 Nov 2024 10:36:45 GMT</pubDate>
    </item>
    <item>
      <title>ResNet50 模型在测试集上具有较高的准确率，但在手动测试时在同一组上表现不佳</title>
      <link>https://stackoverflow.com/questions/79196985/resnet50-model-has-high-accuracy-on-test-set-but-performs-poorly-on-the-same-set</link>
      <description><![CDATA[我是机器学习的新手，我正在尝试在约 100 个类别的数据集上训练 ResNet50 模型。
为此，我首先使用 splitfolders 将数据拆分为训练集、验证集和测试集，然后将这些集保存在单独的文件夹中。然后我预处理数据，定义我的特定模型并在训练集上对其进行训练。随后，我保存模型并在测试集上对其进行评估。到目前为止，一切似乎都运行良好，我在测试集上的准确率约为 0.89。
但是，然后我尝试在同一个测试集上手动测试模型。我从最大的类开始，它运行良好。然而，对于较小的类，所有预测都是错误的。奇怪的是，大多数这些错误的预测都是相同的，例如。 63 类中的大多数图像被预测为 62 类，15 类中的大多数图像被预测为 7 类等等……所以在我看来，该模型实际上可能做出了正确的预测，但将它们与错误的类别相关联……总而言之，有太多错误的预测无法达到 0.89 的准确率。
我想我可能犯了一些相当愚蠢的错误，因为我的手动测试导致的结果与模型的先前评估如此不同，这毫无道理？有人知道这里可能是什么问题吗？
我用来手动测试测试集中文件夹的预测的代码如下：
# 遍历文件夹中的所有图像
for img_file in os.listdir(folder_path):
img_path = os.path.join(folder_path, img_file)

# 加载并预处理图像
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = tf.keras.applications.resnet50.preprocess_input(img_array)

# 进行预测
predictions = model.predict(img_array)
predict_class = np.argmax(predictions)

print(f&quot;图像：{img_file} - 预测类别：{predicted_class}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79196985/resnet50-model-has-high-accuracy-on-test-set-but-performs-poorly-on-the-same-set</guid>
      <pubDate>Sun, 17 Nov 2024 10:29:27 GMT</pubDate>
    </item>
    <item>
      <title>在 Juter 笔记本上导入文件</title>
      <link>https://stackoverflow.com/questions/79196840/importing-file-on-jupter-notebook</link>
      <description><![CDATA[我尝试在 Google Cloud AI 上的 Jupter Notebook 上运行 Kaggle 竞赛的数据，但每次尝试访问 Kaggle 竞赛文件 jane-street-real-time-market-data-forecasting 时，都会出现错误
import sys
sys.path.append(&#39;imported/kaggle/input/jane-street-real-time-market-data-forecasting/kag​​gle_evaluation/jane_street_inference_server.py&#39;)
from kaggle_evaluation.jane_street_inference_server import JSInferenceServer
ModuleNotFoundError: 没有名为“kaggle_evaluation”的模块

一切都定义正确，
请在此处找到链接
https://3ca4a2895c6aced0-dot-us-central1.notebooks.googleusercontent.com/lab/tree/imported/jane-street-rmf-demo-submission-985fd555-1309-45b9-9db2-def394cb5c21.ipynb
即使尝试查找根文件存在
import os 
# 定义数据集路径dataset_path = &quot;/&quot; 
os.path.exists(&#39;dataset_path&#39;) 
&gt;&gt;&gt; 输出 False

我希望让 Kaggle 数据集和 aggle_evaluation 模块正常工作？]]></description>
      <guid>https://stackoverflow.com/questions/79196840/importing-file-on-jupter-notebook</guid>
      <pubDate>Sun, 17 Nov 2024 09:02:15 GMT</pubDate>
    </item>
    <item>
      <title>如何防止目标编码泄漏？[关闭]</title>
      <link>https://stackoverflow.com/questions/79196742/how-can-i-prevent-leaks-in-my-target-encoding</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79196742/how-can-i-prevent-leaks-in-my-target-encoding</guid>
      <pubDate>Sun, 17 Nov 2024 08:04:31 GMT</pubDate>
    </item>
    <item>
      <title>XGBRegressor 模型在时间序列模型中欠拟合</title>
      <link>https://stackoverflow.com/questions/79196179/xgbregressor-model-underfitting-in-time-series-model</link>
      <description><![CDATA[我正在尝试拟合 XGBRegressor 来预测传感器的未来行为。数据具有 6 个周期的季节性。当我拟合数据时，训练的 RMSE 会降低，而测试的 RMSE 不会降低太多并开始增加。如果我将学习率更改为 1 并将最大深度更改为 3，它会在训练数据上过度拟合，但在测试上是一条直线。以下是模型的代码训练数据预测
from xgboost import XGBRegressor
model = XGBRegressor(n_estimators = 1000, early_stopping_rounds = 50,
learning_rate = 0.01, max_depth = 8)
model.fit(X_train, Y_train, 
eval_set = [(X_train, Y_train), (X_test, Y_test)],
verbose = 10)

# 对训练数据进行预测
Y_train_pred = model.predict(X_train)

供参考：X 有 3 个特征是时间（每个点为 3 秒），其他 2 个是
sin_time = 0.5 * np.sin(time) * 2 * np.sin(time) * time 
sin_2pi_time = np.sin(2 * np.pi * time)

Y 是阻力
尝试更改参数，但没有成功，即使我过度拟合模型，训练数据预测也是一条直线测试数据预测]]></description>
      <guid>https://stackoverflow.com/questions/79196179/xgbregressor-model-underfitting-in-time-series-model</guid>
      <pubDate>Sat, 16 Nov 2024 22:36:52 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的暹罗模型不适用于验证数据？</title>
      <link>https://stackoverflow.com/questions/79196054/why-does-my-siamese-model-not-work-on-verification-data</link>
      <description><![CDATA[我的模型之前运行良好，并做出了良好的预测。然而，现在我尝试使用它，它无法识别图像之间的相似性。请帮助解决此问题。
# 保存权重
siamese_model.save(&#39;siamesemodel.h5&#39;)

# 加载模型
model = tf.keras.models.load_model(
&#39;siamesemodel.h5&#39;, 
custom_objects={&#39;L1Dist&#39;: L1Dist, &#39;BinaryCrossentropy&#39;: tf.losses.BinaryCrossentropy}
)

# 验证函数
def verify(model, detection_threshold, validation_threshold):
# 构建结果数组
results = []
for image in os.listdir(os.path.join(&#39;application_data&#39;, &#39;verification_images&#39;)):
input_img = preprocess(os.path.join(&#39;application_data&#39;, &#39;input_image&#39;, &#39;input_image.jpg&#39;))
validation_img = preprocess(os.path.join(&#39;application_data&#39;, &#39;verification_images&#39;, image))

result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))
results.append(result)

# 检测阈值：高于该指标的预测被视为正值
detection = np.sum(np.array(results) &gt; detection_threshold)

 # 验证阈值：正预测的比例/总正样本
validation = detection / len(os.listdir(os.path.join(&#39;application_data&#39;, &#39;verification_images&#39;)))
verified = validation &gt; verify_threshold

返回结果，已验证

cap = cv2.VideoCapture(0)
while cap.isOpened():
ret, frame = cap.read()
frame = frame[120:120+250, 200:200+250, :]

cv2.imshow(&#39;Verification&#39;, frame)

# 验证触发器
if cv2.waitKey(10) &amp; 0xFF == ord(&#39;v&#39;):
# 将输入图像保存到 input_image 文件夹
cv2.imwrite(os.path.join(&#39;application_data&#39;, &#39;input_image&#39;, &#39;input_image.jpg&#39;), frame)
# 运行验证
results, verified = verify(model, 0.5, 0.5)
print(verified)

if cv2.waitKey(10) &amp; 0xFF == ord(&#39;q&#39;):
break
cap.release()
cv2.destroyAllWindows()

打印结果如下：
[array([[9.938484e-09]], dtype=float32),
array([[0.00011181]], dtype=float32),
array([[4.0544733e-06]], dtype=float32),
array([[3.6490118e-07]], dtype=float32),
array([[1.779369e-07]], dtype=float32),
array([[0.15224604]], dtype=float32),
array([[2.0296879e-05]], dtype=float32),
数组([[7.9831276e-05]], dtype=float32),
数组([[2.3284203e-05]], dtype=float32),
数组([[8.0619594e-07]], dtype=float32),
数组([[1.0691416e-06]], dtype=float32),
数组([[1.9231505e-08]], dtype=float32),
数组([[2.243531e-05]], dtype=float32),
数组([[6.483703e-07]], dtype=float32),
数组([[6.656185e-07]], dtype=float32),
array([[4.8954314e-07]], dtype=float32),
array([[9.550116e-08]], dtype=float32),
array([[1.305056e-07]], dtype=float32),
array([[4.187218e-09]], dtype=float32),
array([[3.8443446e-08]], dtype=float32),
array([[5.9630083e-09]], dtype=float32),
array([[1.1699244e-06]], dtype=float32),

我知道保存模型没有问题，因为当我在原始输入上测试重新加载的模型与原始模型时，它们具有相同的输出。
对于给定的输入（两次都是我的一帧），大多数结果应该远高于 0.5。我不明白到底出了什么问题。顺便说一句，这段代码主要来自 YT 教程：https://www.youtube.com/watch?v=FNHLVRJ1HU4&amp;list=PLgNJO2hghbmhHuhURAGbe6KWpiYZt0AMH&amp;index=8
如果能就此事提供任何帮助，我将不胜感激，因为我不明白哪里出了问题。谢谢]]></description>
      <guid>https://stackoverflow.com/questions/79196054/why-does-my-siamese-model-not-work-on-verification-data</guid>
      <pubDate>Sat, 16 Nov 2024 21:07:31 GMT</pubDate>
    </item>
    <item>
      <title>如何在 google colab 中使用从 kaggle 加载的数据（实际使用它）</title>
      <link>https://stackoverflow.com/questions/79195592/how-to-use-loaded-data-from-kaggle-in-google-colab-to-actually-work-with-it</link>
      <description><![CDATA[因此，我最近从此 https://www.kaggle.com/datasets/mostafaabla/garbage-classification 网站导入了数据集。尽管我在 google colab 中的文件中有它（已解压和所有这些东西），但我不知道如何在代码本身中实现它。就像来自 tensorflow 的 Fashion mnist 教程 https://www.tensorflow.org/tutorials/keras/classification?hl 它加载为
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
如何将数据导入/加载到代码单元并通过分成类来处理它（因为在该教程数据集中有多个类，而在我的自定义数据集中有 12 个）
请问如何操作？
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# 定义训练和验证目录的路径
train_dir = &#39;garbage-classification/train&#39;
val_dir = &#39;garbage-classification/validation&#39;

# 创建 ImageDataGenerator 进行数据增强
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

# 从目录加载图像
train_generator = train_datagen.flow_from_directory(
train_dir,
target_size=(150, 150), # 根据需要调整图像大小
batch_size=32,
class_mode=&#39;categorical&#39; # 如果有多个类，请使用 &#39;categorical&#39;
)

validation_generator = val_datagen.flow_from_directory(
val_dir,
target_size=(150, 150),
batch_size=32,
class_mode=&#39;categorical&#39;
)

我使用 perplexity 尝试解决，结果得到了这个。显然它没有起作用，所以..]]></description>
      <guid>https://stackoverflow.com/questions/79195592/how-to-use-loaded-data-from-kaggle-in-google-colab-to-actually-work-with-it</guid>
      <pubDate>Sat, 16 Nov 2024 16:34:39 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow/Keras 模型的 SHAP force_plot 中出现 InvalidArgumentError：切片索引超出范围</title>
      <link>https://stackoverflow.com/questions/79195478/invalidargumenterror-in-shap-force-plot-for-tensorflow-keras-model-slice-index</link>
      <description><![CDATA[我正在使用 TensorFlow/Keras 二元分类模型并使用 SHAP 来解释单个预测。但是，当我尝试生成力图时，我遇到了以下错误：
# 导入 SHAP
import shap

# 确保 data_for_prediction 具有正确的形状
data_for_prediction_reshaped = data_for_prediction.reshape(1, -1)

# 为 DeepExplainer 提供背景数据
background = X_train[:100] # 使用来自训练数据的 100 个样本作为背景

# 初始化 DeepExplainer
explainer = shap.DeepExplainer(model, background)

# 计算 SHAP 值
shap_values = explainer.shap_values(data_for_prediction_reshaped)

# 生成力图
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction_reshaped)


错误：
InvalidArgumentError：{{function_node _wrapped__StridedSlice_device/job:localhost/replica:0/task:0/device:CPU:0}} 切片索引 1 的维度 0 超出范围。[Op:StridedSlice] 名称：strided_slice/
其他详细信息：
1. 该模型是具有以下架构的 Keras Sequential 模型：
• 具有 ReLU 激活的多个密集层。
• 每个密集层后都有一个 Dropout 层。
• 具有用于二元分类的 S 形激活的输出层。
2. 背景数据：
• X_train[:100] 是我预处理的训练数据（NumPy 数组）的一部分。
3. 预测输入：
• data_for_prediction_reshaped 是重塑为 (1, n_features) 的单个样本。
4. 形状：
• shap_values[1].shape：SHAP 值的输出形状（针对第 1 类）。
• data_for_prediction_reshaped.shape：重塑为 (1, n_features) 的输入特征。
问题：
1. 在此上下文中，“维度 0 的切片索引 1 超出范围”错误是什么意思？
2. 我应该如何调整代码以确保 shap.force_plot 能够与 SHAP 和 TensorFlow/Keras 模型一起正常工作？
3. 对于此用例，我应该注意 SHAP 和 TensorFlow/Keras 之间是否存在特定的兼容性问题？
如能提供任何指导，我们将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79195478/invalidargumenterror-in-shap-force-plot-for-tensorflow-keras-model-slice-index</guid>
      <pubDate>Sat, 16 Nov 2024 15:38:25 GMT</pubDate>
    </item>
    <item>
      <title>视觉变换器的计算复杂度[关闭]</title>
      <link>https://stackoverflow.com/questions/79195302/computation-complexity-of-vision-transformer</link>
      <description><![CDATA[我正在寻求有关 Vision Transformers (ViTs) 计算复杂度的澄清，特别是关于多头自注意力 (MSA)、MLP 块、LayerNorm (LN) 和每个块后应用的残差连接等组件。尽管查阅了大量研究论文和资源，但我发现很难掌握这些组件的计算复杂度是如何得出和计算的清晰而简单的解释。有人能提供更详细、更易于理解的解释吗？提前感谢您的帮助。
我试图理解如何计算 ViT 的计算复杂度，但我对计算它感到困惑。我期待解释 ViT 计算复杂度部分从补丁嵌入到 MLP 块。]]></description>
      <guid>https://stackoverflow.com/questions/79195302/computation-complexity-of-vision-transformer</guid>
      <pubDate>Sat, 16 Nov 2024 14:07:32 GMT</pubDate>
    </item>
    <item>
      <title>BERTopic partial_fit 占位符集群表示</title>
      <link>https://stackoverflow.com/questions/79195169/bertopic-partial-fit-placeholder-cluster-representation</link>
      <description><![CDATA[我有大约 2M 个文本文档，每个文档都很小，我想对它们进行聚类。（最终将增长到大约 500M。）虽然我愿意接受建议，但我目前正在使用 Python3 包 BERTopic 的在线技术。在对 140 个 15k 个文档块使用 partial_fit 后，我只剩下对 topic_model.get_topic_info() 的调用，它返回未完成的聚类表示。也就是说，我看到聚类名称 0____ 的表示为 [,,,,,,,,]。我看到大多数聚类都是这样的。我从 Google Gemini 得到的建议是对我的所有文档调用 topic_model.fit(all_documents)，这些文档目前在磁盘上压缩后只有 86 GB，对于 RAM 来说太多了。我该如何填写这些聚类的表示？]]></description>
      <guid>https://stackoverflow.com/questions/79195169/bertopic-partial-fit-placeholder-cluster-representation</guid>
      <pubDate>Sat, 16 Nov 2024 12:50:38 GMT</pubDate>
    </item>
    <item>
      <title>无法训练我的 UNET 多类别细分模型</title>
      <link>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</link>
      <description><![CDATA[我尝试使用 pytorch 从头开始​​制作 UNET。我的模型输出只有黑色蒙版。我需要分割汽车上的损坏，所以我实现了一个彩色图。我确信 70% 的数据集有问题，而这个彩色图恰恰就是其中的原因。任务是多类预测，所以我使用交叉熵损失函数。我将提供我的数据集和训练文件的代码。
# dataset.py
import os
from PIL import Image
from torch.utils.data import Dataset
import numpy as np
import torch

class Segm_Dataset(Dataset):
def __init__(self, image_dir, mask_dir, color_map):
self.image_dir = image_dir
self.mask_dir = mask_dir
self.image_files = os.listdir(self.image_dir)
self.mask_files = os.listdir(self.mask_dir)
self.color_map = color_map

def __len__(self):
return len(self.image_files)

def __getitem__(self, idx):
image_path = os.path.join(self.image_dir, self.image_files[idx])
mask_path = os.path.join(self.mask_dir, self.mask_files[idx])
image = np.array(Image.open(image_path).convert(&#39;RGB&#39;))
mask = np.array(Image.open(mask_path).convert(&#39;RGB&#39;), dtype=np.float32)
label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int64)

for color, label in self.color_map.items():
color_array = np.array(color, dtype=np.float32)
mask_area = np.all(mask == color_array, axis=-1)
label_mask[mask_area] = label

image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)
label_mask = torch.tensor(label_mask, dtype=torch.long)

返回图像，label_mask

# train.py
从模型导入 UNET
从 tqdm 导入 tqdm
从数据集导入 Segm_Dataset
导入 torch
从 torch.utils.data 导入 DataLoader
导入 torch.nn 作为 nn
导入 torch.optim 作为 optim
导入 os

LEARNING_RATE = 1e-4
BATCH_SIZE = 5
NUM_EPOCHS = 10
NUM_WORKERS = 2
IMAGE_HEIGHT = 180
IMAGE_WIDTH = 180
PIN_MEMORY = True
LOAD_MODEL =错误
TRAIN_IMG_DIR = r&#39;data\train\images&#39;
TRAIN_MASK_DIR = r&#39;data\train\masks&#39;
VAL_IMG_DIR = r&#39;data\val\images&#39;
VAL_MASK_DIR = r&#39;data\val\masks&#39;
SAVED_MODELS_PATH = r&#39;saved_models&#39;

color_map = {
(19, 164, 201): 0, # 缺失部分：#13A4C9
(166, 255, 71): 1, # 破损部分：#A6FF47
(180, 45, 56): 2, # 划痕：#B42D38
(225, 150, 96): 3, # 破裂：#E19660
(144, 60, 89): 4, # 凹痕： #903C59
(167, 116, 27): 5, # 剥落: #A7741B
(180, 14, 19): 6, # 油漆剥落: #B40E13
(115, 194, 206): 7, # 腐蚀: #73C2CE
}

train_dataset = Segm_Dataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, color_map)
train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)

val_dataset = Segm_Dataset(VAL_IMG_DIR, VAL_MASK_DIR, color_map)
val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)

model = UNET(in_channels=3, out_channels=len(color_map))
model = model.cuda() if torch.cuda.is_available() else model

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

for epoch in range(NUM_EPOCHS):
train_loop = tqdm(enumerate(train_loader), total=len(train_loader))

for batch_index, (data, target) in train_loop: 
#前向传递
scores = model(data)
train_loss = criterion(scores, target)

#后向传递
optimizer.zero_grad()
train_loss.backward()

#梯度下降或优化器步骤
optimizer.step()

if batch_index % 10 == 0:
current_batch = batch_index
val_loss = 0
with torch.no_grad():
for val_data, val_targets in val_loader:
val_scores = model(val_data)
val_loss = criterion(val_scores, val_targets)

#更新进度条
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

else:
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

checkpoint = {
&#39;epoch&#39;: epoch + 1,
&#39;model_state_dict&#39;: model.state_dict(),
&#39;optimizer_state_dict&#39;: optimizer.state_dict(),
&#39;train_loss&#39;: train_loss.item(),
&#39;val_loss&#39;: val_loss.item()
}

torch.save(checkpoint, os.path.join(SAVED_MODELS_PATH, f&#39;unet_epoch_{epoch}.pth&#39;))

一些训练 epoches:
Epoch: [9/10]: 100%|████████████████| 888/888 [34:24&lt;00:00, 2.32s/it, train_loss=0.000271, val_batch=880, val_loss=0.000278]

Epoch：[10/10]：100%|███████████████| 888/888 [34:29&lt;00:00, 2.33s/it, train_loss=0.000163, val_batch=880, val_loss=0.000167]
]]></description>
      <guid>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</guid>
      <pubDate>Thu, 14 Nov 2024 09:17:27 GMT</pubDate>
    </item>
    <item>
      <title>如何在二维图上可视化鸢尾花数据集的不同特征组合</title>
      <link>https://stackoverflow.com/questions/64068419/how-to-visualize-the-iris-dataset-on-2d-plots-for-different-combinations-of-feat</link>
      <description><![CDATA[我想用所有六种组合（萼片宽度-萼片长度）、（花瓣宽度-萼片长度）、（萼片长度-花瓣宽度）、（花瓣长度-花瓣宽度）（花瓣长度-萼片宽度）（萼片宽度-花瓣长度）在二维中可视化鸢尾花数据集，基本上这就是我目前得到的结果：
import matplotlib
matplotlib.rcParams[&#39;figure.figsize&#39;] = (9.0, 7.0)

data = load_iris()

pairs = [(i, j) for i in range(4) for j in range(i+1, 4)]

fig, subfigs = pyplot.subplots(2, 3, tight_layout=True)
t1 = time.time()

for (f1, f2), subfig in zip(pairs, subfigs.reshape(-1)):

根据说明，我们必须根据此对生成二维图，每次列出两个度量，以 f1 和 f2 作为度量，并创建类指标和 legend() 以更好地可视化图形，我尝试了不同的散点图，但似乎都不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/64068419/how-to-visualize-the-iris-dataset-on-2d-plots-for-different-combinations-of-feat</guid>
      <pubDate>Fri, 25 Sep 2020 17:00:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么训练时我的 Keras 模型的准确率始终为 0？</title>
      <link>https://stackoverflow.com/questions/45632549/why-is-the-accuracy-for-my-keras-model-always-0-when-training</link>
      <description><![CDATA[我构建了一个简单的 Keras 网络：
import numpy as np;

from keras.models import Sequential;
from keras.layers import Dense,Activation;

data= np.genfromtxt(&quot;./kerastests/mydata.csv&quot;, delimiter=&#39;;&#39;)
x_target=data[:,29]
x_training=np.delete(data,6,axis=1)
x_training=np.delete(x_training,28,axis=1)

model=Sequential()
model.add(Dense(20,activation=&#39;relu&#39;, input_dim=x_training.shape[1]))
model.add(Dense(10,activation=&#39;relu&#39;))
model.add(Dense(1));

model.compile(optimizer=&#39;adam&#39;,loss=&#39;mean_squared_error&#39;,metrics=[&#39;accuracy&#39;])
model.fit(x_training, x_target)

如您所见，从我的源数据中，我删除了 2 列。一列是带有字符串格式日期的列（在数据集中，除此之外，我还有表示日期的列、表示月份的列和表示年份的列，因此我不需要该列），另一列是我用作模型目标的列）。
当我训练这个模型时，我得到了这个输出：
32/816 [&gt;.............................] - ETA：23s - loss：13541942.0000 - acc：0.0000e+00
800/816 [===========================&gt;.] - ETA：0s - loss：11575466.0400 - acc：0.0000e+00 
816/816 [================================] - 1s - 损失：11536905.2353 - 精度：0.0000e+00 
纪元 2/10
32/816 [&gt;.............................] - ETA：0s - 损失：6794785.0000 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5381360.4314 - 精度：0.0000e+00 
纪元 3/10
32/816 [&gt;.............................] - ETA：0s - 损失： 6235184.0000 - 精度：0.0000e+00
800/816 [============================&gt;.] - ETA：0s - 损失：5199512.8700 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5192977.4216 - 精度：0.0000e+00 
纪元 4/10
32/816 [&gt;.............................] - ETA：0s - 损失：4680165.5000 - 精度： 0.0000e+00
736/816 [===========================&gt;...] - ETA：0s - 损失：5050110.3043 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5168771.5490 - 精度：0.0000e+00 
纪元 5/10
32/816 [&gt;.............................] - ETA：0s - 损失：5932391.0000 - 精度：0.0000e+00
768/816 [============================&gt;..] - ETA：0 秒 - 损失：5198882.9167 - 精度：0.0000e+00
816/816 [==============================] - 0 秒 - 损失：5159585.9020 - 精度：0.0000e+00 
纪元 6/10
32/816 [&gt;.............................] - ETA：0 秒 - 损失：4488318.0000 - 精度：0.0000e+00
768/816 [============================&gt;..] - ETA：0s - 损失：5144843.8333 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5151492.1765 - 精度：0.0000e+00 
纪元 7/10
32/816 [&gt;.............................] - ETA：0s - 损失：6920405.0000 - 精度：0.0000e+00
800/816 [=============================&gt;.] - ETA：0s - 损失：5139358.5000 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5169839.2941 - 精度：0.0000e+00 
纪元 8/10
32/816 [&gt;.............................] - ETA：0s - 损失：3973038.7500 - 精度：0.0000e+00
672/816 [==========================&gt;......] - ETA：0s - 损失：5183285.3690 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5141417.0000 - 精度：0.0000e+00 
Epoch 9/10
32/816 [&gt;.............................] - ETA：0s - 损失：4969548.5000 - 精度：0.0000e+00
768/816 [===========================&gt;..] - ETA：0s - 损失：5126550.1667 - 精度： 0.0000e+00
816/816 [===============================] - 0s - 损失：5136524.5098 - 精度：0.0000e+00 
纪元 10/10
32/816 [&gt;.............................] - ETA：0s - 损失：6334703.5000 - 精度：0.0000e+00
768/816 [===========================&gt;..] - ETA：0s - 损失：5197778.8229 - 精度：0.0000e+00
816/816 [===============================] - 0s - 损失：5141391.2059 - 准确率：0.0000e+00 

为什么会发生这种情况？我的数据是时间序列。我知道对于时间序列，人们通常不使用 Dense 神经元，但这只是一个测试。真正让我困惑的是准确率始终为 0。而且，在其他测试中，我甚至输了：得到一个“NAN”值。
有人能帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/45632549/why-is-the-accuracy-for-my-keras-model-always-0-when-training</guid>
      <pubDate>Fri, 11 Aug 2017 10:08:03 GMT</pubDate>
    </item>
    <item>
      <title>bnlearn 节点的尺寸错误</title>
      <link>https://stackoverflow.com/questions/34127885/bnlearn-wrong-dimensions-for-node</link>
      <description><![CDATA[我正在用 R 解决一个简单的问题。这是代码：
library(bnlearn)

dag &lt;- model2network(&quot;[Location][Quality][Cost|Location:Quality][NoPeople|Location:Cost]&quot;)
plot(dag)

quality.values &lt;- factor(c(&quot;Good&quot;, &quot;Normal&quot;, &quot;Bad&quot;))
location.values &lt;- factor(c(&quot;Good&quot;, &quot;Bad&quot;))
cost.values &lt;- factor(c(&quot;High&quot;, &quot;Low&quot;))
nopeople.values &lt;- factor(c(&quot;High&quot;, &quot;Low&quot;))

quality.prob &lt;- array(c(0.3, 0.5, 0.2), dim = 3, dimnames = list(quality = quality.values))
location.prob &lt;- array(c(0.6, 0.4), dim = 2, dimnames = 列表（位置 = 位置.值））
cost.prob &lt;- 数组（c（0.8, 0.2, 0.6, 0.4, 0.1, 0.9, 0.6, 0.4, 0.6, 0.4, 0.05, 0.95）dim = c（2, 3, 2）dimnames = 
列表（成本 = 成本.值，质量 = 质量.值，位置 = 位置.值））
nopeople.prob &lt;- 数组（c（0.6, 0.4, 0.8, 0.2, 0.1, 0.9, 0.6, 0.4）dim = c（2, 2, 2）dimnames = 
列表（NoPeople = nopeople.值，成本 = 成本.值，位置 = 位置.值））

condProbTable &lt;- 列表（位置 = location.prob, Quality = quality.prob, Cost = cost.prob, NoPeople = nopeople.prob)
bn &lt;- custom.fit(dag, condProbTable)

除了当我尝试这个时，我得到了一个 Error in check.dnode.vs.spec(dist[[cpd]], old = fitted[[cpd]]$parents, : 节点 Cost 的尺寸错误 错误。我不确定我做错了什么。我对 R 还很陌生，所以任何帮助都很好。
作为参考，我正在尝试构建这个：

谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/34127885/bnlearn-wrong-dimensions-for-node</guid>
      <pubDate>Mon, 07 Dec 2015 06:45:41 GMT</pubDate>
    </item>
    </channel>
</rss>