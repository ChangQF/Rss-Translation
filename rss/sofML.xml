<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 25 Apr 2024 21:16:07 GMT</lastBuildDate>
    <item>
      <title>我正在尝试对具有 2729 个特征和 1006 个样本的数据进行二进制分类</title>
      <link>https://stackoverflow.com/questions/78387140/i-am-trying-to-binary-classify-my-data-which-has-2729-feature-and-1006-sample</link>
      <description><![CDATA[我使用 BorutaPy 来选择最佳特征。结果是这样的：
BorutaPy 完成运行。
迭代：100 / 100
已确认：79
暂定：4
拒绝：2645

[这是缩放之前的数据分布][1]
[这是使用 StandardScaler() 进行缩放之后的结果][2]
[这是每个班级的人数][3]

执行交叉验证并保存每个分类器的准确性结果
{&#39;随机森林&#39;: array([0.78873239, 0.69014085, 0.6056338 , 0.67605634, 0.64285714,
       0.77142857, 0.67142857, 0.74285714, 0.71428571, 0.71428571]), &#39;XGBoost&#39;: 数组([0.81690141, 0.71830986, 0.70422535, 0.70422535, 0.6 ,
       0.78571429, 0.72857143, 0.75714286, 0.71428571, 0.72857143]), &#39;SVM&#39;: 数组([0.69014085, 0.64788732, 0.63380282, 0.63380282, 0.61 428571,
       0.68571429, 0.8 , 0.8 , 0.58571429, 0.61428571]), &#39;逻辑回归&#39;: array([0.64788732, 0.63380282, 0.64788732, 0.70422535, 0.57142857,
       0.7, 0.68571429, 0.74285714, 0.58571429, 0.57142857])}

  [1]：https://i.sstatic.net/65z0944B.png
  [2]：https://i.sstatic.net/Lf0UESdr.png
  [3]：https://i.sstatic.net/zOXxeFt5.png
1.随机森林测试集准确率：0.6897689768976898，AUC-ROC得分：0.7690234340194244
2. XGBoost测试集准确率：0.6831683168316832，AUC-ROC分数：0.7467254744720664
3. SVM测试集准确率：0.6303630363036303，AUC-ROC得分：0.7130669161543258
4.Logistic回归测试集准确率：0.6567656765676567，AUC-ROC分数：0.7165196471531675
5.ExtraTreesClassifier测试集准确率：0.7029702970297029，AUC-ROC得分：0.7524948765927114
6.LGBMClassifier测试集准确率：0.6963696369636964，AUC-ROC得分：0.7774659182036889

我需要将准确率提高到 80%
我愿意吗？]]></description>
      <guid>https://stackoverflow.com/questions/78387140/i-am-trying-to-binary-classify-my-data-which-has-2729-feature-and-1006-sample</guid>
      <pubDate>Thu, 25 Apr 2024 20:35:40 GMT</pubDate>
    </item>
    <item>
      <title>识别图像中的重叠粒子</title>
      <link>https://stackoverflow.com/questions/78387055/identify-overlapping-particles-in-image</link>
      <description><![CDATA[我有一些塑料颗粒和水波实验的图像。目标是自动识别塑料颗粒。它们有时会重叠，我不需要找到单个粒子，找到那些包含塑料的像素就足够了。
由于粒子是红色的，并且背景大多是白色或黑色，我想我可以进行简单的阈值处理，如果R &gt; &gt;，则说像素是塑料。 5*B 和 R&gt; 0.25，其中 R 和 B 是红色和蓝色通道。然而，不同实验之间的曝光差异很大，有时在实验中，当部分表面被水覆盖时，所以我的方法不能非常一致地工作，有时会错误地识别侧面的黑色裂缝。
我想知道还有什么其他选择。我对神经网络的经验有限，所以我不确定这是否可行（需要付出合理的努力）。特别是，我认为形状不会有太大帮助，因为粒子靠近在一起并且部分重叠，它们之间的对比度很差，但也许颜色就足够了？
示例图像：


]]></description>
      <guid>https://stackoverflow.com/questions/78387055/identify-overlapping-particles-in-image</guid>
      <pubDate>Thu, 25 Apr 2024 20:14:01 GMT</pubDate>
    </item>
    <item>
      <title>我有一个问题，如何使用机器视觉技术检测按顺序排列的物体错位位置</title>
      <link>https://stackoverflow.com/questions/78386980/i-have-a-questions-that-how-can-i-detect-the-order-wise-arranged-object-misplace</link>
      <description><![CDATA[我有一个像螺栓、螺母和螺钉这样的物体，按垂直顺序排列，这是实际的正确顺序。我的项目目标是找到这些图像检测的非顺序位置，就像螺母先出现然后螺栓和螺钉，那么这种排列是错误的，类似地，如果螺钉先出现，然后是螺母，螺栓出现，这也是错误的。我想要螺栓，NU，螺钉位置模式中的顺序，只有其他，那么这个顺序将错过放置的顺序。现在我该如何构建这些图像数据集以及当这种无序模式与任何其他对象一起使用时可以使用哪种机器视觉技术，它应该检测那些带有带有标签的边界框的无序对象我该怎么做？
如何构建这些图像数据集，以及当这种无序模式与任何其他对象一起使用时，可以使用哪种机器视觉技术，它应该检测那些带有带有标签的边界框的无序对象，我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78386980/i-have-a-questions-that-how-can-i-detect-the-order-wise-arranged-object-misplace</guid>
      <pubDate>Thu, 25 Apr 2024 19:55:08 GMT</pubDate>
    </item>
    <item>
      <title>Xgboost 自定义目标函数。如何修改权重？</title>
      <link>https://stackoverflow.com/questions/78386885/xgboost-custom-objective-function-how-to-modify-the-weights</link>
      <description><![CDATA[我有一个 xgboost 的自定义目标函数：
` def custom_objective(y_true, y_pred, x = 3000):
    
        pred_probs = 1.0 / (1.0 + np.exp(-y_pred))
    
        top_x_indices = np.argsort(pred_probs)[-x:]
    
        #权重数组：前 x 个实例的权重较高
        权重 = np.ones_line(y_true)
        权重[top_x_indices] = 10
    
        梯度 = (pred_probs - y_true) / (pred_probs*(1 - pred_probs))
        
        hess = (2 * pred_probs**2 - 3 * pred_probs + 1 + y_true - 2 * y_true * pred_probs) / (pred_probs * (1 - pred_probs))**2
    
        返回 grad * 权重，hess * 权重`

这个想法是将前 3000 个样本的权重设置得更高，因为只有最高分数才会用于生产。
问题是它每次分配相同的权重，并且权重在构建下一棵树时不会更新。如何在训练过程中将给定简单的实际权重放入自定义目标函数中？如果我能做到这一点，那么我可以将 top3000 的权重设置得更高，例如将这些权重乘以 10。
谢谢！
我试图解决这个问题，但找不到权重。]]></description>
      <guid>https://stackoverflow.com/questions/78386885/xgboost-custom-objective-function-how-to-modify-the-weights</guid>
      <pubDate>Thu, 25 Apr 2024 19:32:23 GMT</pubDate>
    </item>
    <item>
      <title>与偏移量 pandas 的累积总和</title>
      <link>https://stackoverflow.com/questions/78386774/cumulative-sum-with-offset-pandas</link>
      <description><![CDATA[我正在开发一个机器学习模型来预测给定团队的得分。我想创建一个列，跟踪每个球队主场比赛的累积得分，但不包括当前比赛（行）。我可以轻松计算累计总数，但我想抵消累计总数以显示累计但不包括当前游戏的数据，下面是数据集的示例。我理想地希望创建累积列

&lt;标题&gt;

game_id
比赛日期
home_id
home_score
累计


&lt;正文&gt;

718730
2023-04-03
145
3
0


718695
2023-04-05
145
7
3


718687
2023-04-06
145
6
10


718683
2023-04-06
109
2
0


718671
2023-04-07
109
6
2


718656
2023-04-08
109
12
8



下面的代码是我迄今为止为创建累计总数所做的事情
导入 pandas 作为 pd

数据 = pd.read_csv(&#39;game_data.csv&#39;)
data[&#39;home_cumulative&#39;] = data.groupby(&#39;home_id&#39;)[&#39;home_score&#39;].cumsum()

我尝试了以下操作，但结果与我的预期不符
data[&#39;home_offset&#39;] = data.groupby(&#39;home_id&#39;)[&#39;home_score&#39;].shift(periods = 1).cumsum().fillna(0)
]]></description>
      <guid>https://stackoverflow.com/questions/78386774/cumulative-sum-with-offset-pandas</guid>
      <pubDate>Thu, 25 Apr 2024 19:09:26 GMT</pubDate>
    </item>
    <item>
      <title>呼吸信号的对数功率谱</title>
      <link>https://stackoverflow.com/questions/78386374/log-power-spectrum-for-breath-signal</link>
      <description><![CDATA[在我的模型中，我的数据集是噪声频谱的LPS，我的标签是干净频谱的LPS，每个图片大小是（1025*1292），我使用unet作为我的模型。
型号：
#pytorch
进口火炬
将 torch.nn 导入为 nn

解码器类（nn.Module）：
    def __init__(self, in_channels,out_features, kernel_size, maxpoolindex, apply_dropout,stride):
        super(解码器, self).__init__()
        self.conv = nn.Conv2d(in_channels=in_channels、out_channels=out_features、kernel_size=kernel_size、stride=stride、padding=0、bias=True)
        self.batch_norm = nn.BatchNorm2d(out_features)
        self.relu = nn.LeakyReLU(负斜率=0.2)
        self.dropout = nn.Dropout(p=0.1)
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2) 如果 maxpoolindex == 1 否则无

    def 前向（自身，x）：
        x = self.conv(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        如果 self.dropout 不是 None：
            x = self.dropout(x)
        如果 self.maxpool 不是 None：
            x = self.maxpool(x)
        返回x

编码器类（nn.Module）：
    def __init__(self,in_channels, out_features, kernel_size, apply_dropout):
        超级（编码器，自我）.__init__()
        self.conv_transpose = nn.ConvTranspose2d（in_channels = in_channels，out_channels = out_features，kernel_size = kernel_size，stride = 2，padding = 0，bias = True）
        self.batch_norm = nn.BatchNorm2d(out_features)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.1)

    def 前向（自身，x）：
        x = self.conv_transpose(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        如果 self.dropout 不是 None：
            x = self.dropout(x)
        返回x

class DenoiseUnet(nn.Module):#除到第一个奇数停止的设计
    def __init__(自身):
        超级（DenoiseUnet，自我）.__init__()
        self.down_procedure = nn.ModuleList([
            解码器(1,8,2,0,0,2),
            解码器(8,16,2,0,0,2),
            解码器(16,32,2,0,0,2),
            解码器(32,128,2,0,0,2),
            解码器(128,128,1,0,0,1)
        ]）
        self.up_procedure = nn.ModuleList([
            编码器(2​​56,32,2,0),
            编码器(64,16,2,0),
            编码器(32,8,2,0),
            编码器(16,4,2,0),
        ]）
        self.convert = nn.ConvTranspose2d(4, 1, kernel_size=1, stride=1, padding=0)


    def 前向（自身，x）：
        连接=[]
        对于 self.down_procedure 中的 down：
            x = 向下(x)
            连接.append(x)

        连接=列表（反转（连接[：-1]））
        对于 up，在 zip(self.up_procedure, connection) 中连接：


            如果 x.shape[2] &lt;连接.形状[2]：
              连接 = 连接[:, :, :x.shape[2], :]
            别的：
              x = x[:, :, :connect.shape[2], :]

            如果 x.shape[3] &lt;连接.形状[3]：
              连接 = 连接[:, :, :, :x.shape[3]]
            别的：
              x = x[:, :, :, :connect.shape[3]]



            x = torch.cat([x, 连接], 暗淡=1)
            x = 上(x)

        y = self.convert(x)
        返回y


模型 = DenoiseUnet()
打印（模型）

但是经过 10 轮训练后，我得到这样的结果：[在此处输入图像描述]输入图像描述这里（https://i.sstatic.net/UDhZNKHE.png）
一种是预测结果，一种是测试，谁能帮我找出问题所在，谢谢！
数据集数量不同，看起来是一样的。]]></description>
      <guid>https://stackoverflow.com/questions/78386374/log-power-spectrum-for-breath-signal</guid>
      <pubDate>Thu, 25 Apr 2024 17:43:28 GMT</pubDate>
    </item>
    <item>
      <title>无法将 NumPy 数组转换为 LSTM 张量</title>
      <link>https://stackoverflow.com/questions/78385645/failed-to-convert-a-numpy-array-to-a-tensor-for-lstm</link>
      <description><![CDATA[尝试运行 LSTM 模型，其中数据被分成 csv 中的几列，并且我正在尝试从此类 csv 中准备日期。
获取错误
&lt;块引用&gt;
ValueError：无法将 NumPy 数组转换为张量（不支持的对象类型 numpy.ndarray）。

# 加载 CSV 数据 # 3 个特征 (1, 2, 3)，1 个标签 (5) 列
# 所有数据均为浮点数，1,0 位于标签中
data = pd.read_csv(“data_1.csv”, usecols=[“Column_1”,“Column_2”,“Column_3”,“Column_5”])
data2 = pd.read_csv(“data_2.csv”, usecols=[“Column_1”,“Column_2”,“Column_3”,“Column_5”])


window_size = 1 # 对于时间步长

# 创建序列的函数
def create_sequences(数据, window_size, label_col):
  序列=[]
  标签=[]
  对于范围内的 i(len(data) - window_size + 1)：
    window = data.loc[i:i+window_size,[&quot;Column_1&quot;,&quot;Column_2&quot;,&quot;Column_3&quot;]].astype(&#39;float64&#39;)
    label = data.loc[i + window_size - 1, label_col] # 窗口末尾的标签
    序列.append(window.to_numpy())
    labels.append(标签)
  返回 np.array(序列), np.array(标签)

# 创建序列和标签
序列2，标签2 = create_sequences（数据，window_size，“Column_5”）
序列2 =序列2.reshape(-1, 1) #（样本，时间步长，特征）
print(&quot;数据1准备完成&quot;)

序列3、标签3 = create_sequences(data2、window_size、“Column_5”)
序列3 = 序列3.reshape(-1, 1) # （样本、时间步长、特征）

print(&quot;数据2准备完成&quot;)

序列 = np.concatenate((序列2, 序列3))
标签 = np.concatenate((labels2, labels3), axis=0)

# 定义LSTM模型
模型 = keras.Sequential([
  keras.layers.LSTM（64，return_sequences = True，input_shape =（window_size，sequences.shape [2]）），
  keras.layers.LSTM(32),
  keras.layers.Dense(len(np.unique(labels)),activation=&#39;softmax&#39;) # 多类输出
]）

# 编译模型
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=&#39;adam&#39;, 指标=[&#39;accuracy&#39;])
model.fit(序列, keras.utils.to_categorical(标签), epochs=1)

我也可以通过运行代码来复制它，
sequences=tf.convert_to_tensor(sequences)

期望在 LSTM 层中正确获取数据集。]]></description>
      <guid>https://stackoverflow.com/questions/78385645/failed-to-convert-a-numpy-array-to-a-tensor-for-lstm</guid>
      <pubDate>Thu, 25 Apr 2024 15:22:49 GMT</pubDate>
    </item>
    <item>
      <title>我的预测结果中有缺失值（NaN），但数据集中没有 NaN 值</title>
      <link>https://stackoverflow.com/questions/78385503/i-have-missing-valuesnan-in-my-prediction-result-but-theres-no-nan-value-in-t</link>
      <description><![CDATA[我正在处理两个数据集（train.csv 和 test.csv）。
我已对这两个文件执行了 EDA。
我已经训练了我的模型并且它运行良好。
但是我对 test.csv 的一些预测结果显示 NaN，我无法弄清楚问题出在哪里。


test_df_copy = pd.get_dummies(test_df_copy, columns = categorical_data, drop_first = True)
test_df_copy = test_df_copy.replace({True: 1, False: 0})
test_df_copy.head()
test_df_copy[新数值数据] =Robust_scaler.fit_transform(test_df_copy[新数值数据])



test_df_copy2 = test_df_copy.drop([“Id”], axis = 1)

#逻辑回归
logPred = log_reg.predict(test_df_copy2)
＃决策树
dec_tree_predict = dec_tree.predict(test_df_copy2)
#SVC 预测
svc_predict = svc_model.predict(test_df_copy2)
#随机森林
rf_predict = random_forest.predict(test_df_copy2)

#将我们的预测结果转换为数据帧
Log_predict_df =pd.DataFrame(logPred)
dec_tree_predict_df =pd.DataFrame(dec_tree_predict)
svc_predict_df =pd.DataFrame(svc_predict)
rf_predict_df =pd.DataFrame(rf_predict)

#将我们的预测添加到我们的数据集中
测试结果=测试df_copy
test_result[&#39;log_pred&#39;] = Log_predict_df
test_result[&#39;tree_pred&#39;] = dec_tree_predict_df
test_result[&#39;svc_pred&#39;] = svc_predict_df
test_result[&#39;rf_pred&#39;] = rf_predict_df
]]></description>
      <guid>https://stackoverflow.com/questions/78385503/i-have-missing-valuesnan-in-my-prediction-result-but-theres-no-nan-value-in-t</guid>
      <pubDate>Thu, 25 Apr 2024 14:56:30 GMT</pubDate>
    </item>
    <item>
      <title>改进长尾数据的回归模型</title>
      <link>https://stackoverflow.com/questions/78384887/improving-a-regression-model-for-long-tailed-data</link>
      <description><![CDATA[我正在对来自信贷持有的预测收入数据进行建模。正如预期的那样，该数据是长尾数据，收入较高的数据要小得多。在下图中，它是这些收入在申报收入和预测收入之间的分布。 申报收入与预测收入
正如您所看到的，该模型将低收入向上移动，但没有捕捉到长尾。我完成了一些特征工程步骤，例如对任何货币值进行对数转换、查找不同风险因素（例如信用评分）的收入中位数、查找区域收入中位数等。
模型参数如下：调优的catboost模型
{&#39;迭代&#39;：300，&#39;学习率&#39;：0.1，&#39;深度&#39;：8，&#39;损失函数&#39;：&#39;RMSE&#39;}
我选择了 catboost，因为就 RMSE 而言，它是使用 Pycaret 时最好的模型。
任何具有长尾数据建模经验并愿意提供指导的人将不胜感激]]></description>
      <guid>https://stackoverflow.com/questions/78384887/improving-a-regression-model-for-long-tailed-data</guid>
      <pubDate>Thu, 25 Apr 2024 13:09:19 GMT</pubDate>
    </item>
    <item>
      <title>当我创建 venv 时，我遇到了这个问题：此时是意外的</title>
      <link>https://stackoverflow.com/questions/78384811/when-i-create-venv-i-got-this-issue-was-unexpected-at-this-time</link>
      <description><![CDATA[
conda create -p venv python==3.10 -y
或y
conda 激活 venv/

然后我得到了错误：此时是意外的
请提供我应该遵循的步骤。]]></description>
      <guid>https://stackoverflow.com/questions/78384811/when-i-create-venv-i-got-this-issue-was-unexpected-at-this-time</guid>
      <pubDate>Thu, 25 Apr 2024 12:54:19 GMT</pubDate>
    </item>
    <item>
      <title>数据分割时保留数据的空间分布</title>
      <link>https://stackoverflow.com/questions/78383883/preserving-spatial-distribution-of-data-during-data-splitting</link>
      <description><![CDATA[我正在尝试使用随机森林模型来模拟德国巴伐利亚河流中的硝酸盐浓度。我使用 Python，主要使用 sklearn。我有 490 个水质站的数据。我遵循 LongzhuQ.Shen 等人论文中的方法，该论文可以在这里找到：https://www.nature.com/articles/s41597-020-0478-7
我想将数据集分成训练集和测试集，以便两个集中数据的空间分布相同。这个想法是，如果数据分割忽略空间分布，则训练集可能最终会集中来自人口稠密区域的点，而忽略稀疏区域。这可能会扭曲模型的学习过程，使其在整个感兴趣领域的准确性或概括性降低。 sklearn train_test_split只是将数据随机划分为训练集和测试集，并且不考虑数据中的空间模式。
我上面提到的论文遵循了这种方法：“我们将完整的数据集分为两个子数据集，分别是训练和测试。为了考虑监测站空间分布的异质性，我们在数据分割步骤中采用了空间密度估计技术，通过使用带宽为 50 km 的高斯核（使用 GRASS GIS33 中可用的 v.kernel）构建密度表面来计算每个物种和季节。所得密度表面的像素值用作权重因子，将数据分成具有相同空间分布的训练和测试子集。”
我想遵循相同的方法，但我不使用草地 GIS，而是自己用 Python 构建密度表面。我还提取了概率密度值和站点的权重。 （附图）
现在我面临的唯一问题是如何使用这些权重将数据分成训练集和测试集？我检查了sklearn train_test_split函数中没有可以考虑权重的关键字。我也与GPT 4聊天来回，但它也无法给我一个明确的答案。我在互联网上也没有找到任何关于此的具体信息。也许我错过了一些东西。
还有其他函数可以用来执行此操作吗？或者我必须编写自己的算法来进行分割？如果是后者，您能否建议我一种方法，以便我自己编写代码？
在附图中您可以看到站点的位置以及使用核密度估计方法（使用高斯核）生成的概率密度面。
还附上我的数据框的屏幕截图，让您了解数据结构。 （经度（‘lon’）列之后的所有列都用作特征。NO3 列用作目标变量。）
如果有任何答案，我将不胜感激。
请查找附件图片以供参考。
使用高斯核的核密度估计方法生成的概率密度曲面。&lt; /p&gt;
我用来模拟硝酸盐浓度的数据集]]></description>
      <guid>https://stackoverflow.com/questions/78383883/preserving-spatial-distribution-of-data-during-data-splitting</guid>
      <pubDate>Thu, 25 Apr 2024 10:10:26 GMT</pubDate>
    </item>
    <item>
      <title>为什么 bigQueryML 的转换子句不支持 ML.NGRAM？</title>
      <link>https://stackoverflow.com/questions/78379552/why-isnt-ml-ngram-not-supported-in-transform-clause-in-bigqueryml</link>
      <description><![CDATA[我正在使用以下查询来创建模型，但编辑器抱怨转换子句中不支持 ML.NGRAM。
创建或替换模型
`singular-hub-291814.movi​​e_sentiment.mymodel3`
TRANSFORM(ML.NGRAM(string_field_0,[1,2])OVER() 作为 ngram )
选项
  ( model_type=&#39;LOGISTIC_REG&#39;,
    auto_class_weights = TRUE，
    data_split_method=&#39;随机&#39;,
    DATA_SPLIT_EVAL_FRACTION = .10，
    input_label_cols=[&#39;评论&#39;]
  ） 作为

选择
  string_field_0 ，从表中查看；

尽管可以在 SELECT 查询中使用相同的转换。
&lt;前&gt;&lt;代码&gt;选择
  ML.NGRAMS(words_array, [1,2]) 作为 ngrams，
  审查
从表；

而其他转换函数（如 bag_of_words、min_abs_scalar）可以在转换中使用。为什么这种行为会如此不同？是否有不能在 TRANSFORM 子句中使用的转换器的明确列表？]]></description>
      <guid>https://stackoverflow.com/questions/78379552/why-isnt-ml-ngram-not-supported-in-transform-clause-in-bigqueryml</guid>
      <pubDate>Wed, 24 Apr 2024 15:18:24 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 绘制糖尿病视网膜病变检测图</title>
      <link>https://stackoverflow.com/questions/78365786/mapping-in-a-diabetic-retinopathy-detection-using-python</link>
      <description><![CDATA[我目前正在开发一个使用 Inception V3 预训练模型进行糖尿病视网膜病变分类的 Python 程序，然后将其传递到 SVM 和随机森林分类器。快速分解该项目：

对 5 种类型（0、1、2、3、4）的眼睛图像进行分类，其中 0 表示没有患病，4 表示严重。
在这个特定的程序中，我有意将 0,1 图像映射到值 10，将 2,3,4 映射到值 11，将其变成二元分类器。
然后构建模型，然后正确训练和执行。

为了提供有关我的代码工作的更多详细信息，我想显示图像的特定映射。
我建了两个表：
表 1： 它有三列，第一列是从 xero 开始的所有图像的编号，第二列是与图像对应的唯一标识符，第三列是原始映射类 0,1,2,3,4。
表 2：前两列相同，但第三列现在显示映射的 ID（10 或 11）
我想从第二个表中随机获取行及其映射的 id，然后比较第一个表中的值以获得原始 id，这样我就可以看到实际和预测的混淆矩阵。
一些有帮助的图像是：
图片 1
图片_2 图片 3
图片 4
我尝试实现上述问题，但 2,3,4 的映射没有出现，只有 0,1 的映射出现。
这是我迄今为止编写的代码：
# 从训练数据中随机选择 916 行及其对应的 ID 代码
selected_indices = np.random.choice(len(x_train), 916, 替换=False)
x_train2 = x_train[选定的索引]
y_train2_ids = [labels_df.iloc[i][&#39;id_code&#39;] for i in selected_indices]

# 查找测试数据中与x_train2中的ID代码匹配的索引
匹配索引 = []
对于 y_train2_ids 中的 id_code：
    索引 = labels_df[labels_df[&#39;id_code&#39;] == id_code].index
    如果 len(索引) &gt; 0:
        matching_indices.append(索引[0])
    别的：
        print(f&quot;未找到 ID 代码的匹配索引：{id_code}&quot;)

# 将对应的标签存储在y_a中
y_a = y_train_encoded[匹配索引]

# 根据唯一类动态初始化每个组合的计数
组合计数 = {}
对于 unique_classes 中的 true_class：
    对于 unique_classes 中的 Predicted_class：
        key = f&#39;True Positive ({true_class} -&gt; {predicted_class})&#39;
        组合计数[键] = 0

# 迭代每对真实标签（y_a）和预测标签（predictions）
对于 zip(y_a, Predictions.round()) 中的 true_label、predicted_label：

    # 将预测标签转换为整数
    预测标签 = int(预测标签[0])
    key = f&#39;True Positive ({true_label} -&gt; {predicted_label})&#39;
    组合计数[键] += 1

# 打印结果的表格结构
print(“真实标签和预测标签的比较”)
print(&quot;{:&lt;25} {:&lt;20}&quot;.format(&quot;组合&quot;, &quot;计数&quot;))
打印（“-”* 45）

对于组合，在combination_counts.items()中计数：
    print(&quot;{:&lt;25} {:&lt;20}&quot;.format(组合, 计数))

当前输出是：
真实标签和预测标签的比较
组合数
--------------------------------------------------------
真阳性 (0 -&gt; 0) 325
真阳性 (0 -&gt; 1) 229
真阳性 (1 -&gt; 0) 250
真阳性 (1 -&gt; 1) 112

我也想获取 0,1,2,3,4 的值]]></description>
      <guid>https://stackoverflow.com/questions/78365786/mapping-in-a-diabetic-retinopathy-detection-using-python</guid>
      <pubDate>Mon, 22 Apr 2024 11:05:03 GMT</pubDate>
    </item>
    <item>
      <title>根据单个树获取 XGBoost 预测</title>
      <link>https://stackoverflow.com/questions/78334661/get-xgboost-prediction-based-on-individual-trees</link>
      <description><![CDATA[它可能与如何获取每棵树的重复xgboost 中的预测？ 但解决方案不再有效（可能是 XGBoost 库上的更改）。我的想法是以原始格式 model.get_booster().get_dump() 转储模型并在不同的平台中实现它（仅预测）。不过，我首先尝试用 python 来实现它。运行以下代码，使用所有单独的增强器进行预测并将它们组合起来，不会返回与 model.predict() 函数相同的结果。
有什么方法可以将 model.predict() 与助推器的组合相匹配吗？我错过了什么？
将 numpy 导入为 np
将 xgboost 导入为 xgb
从sklearn导入数据集
from scipy.special import expit as sigmoid, logit as inverse_sigmoid

# 加载数据
iris = datasets.load_iris()
X, y = iris.data, (iris.target == 1).astype(int)

# 拟合模型
模型 = xgb.XGBClassifier(
    n_估计器=10，
    最大深度=10，
    use_label_encoder=False,
    目标=&#39;二进制：逻辑&#39;
）
模型.fit(X, y)
booster_ = model.get_booster()

# 提取个人预测
individual_preds = []
对于 booster_ 中的 tree_：
    individual_preds.append(
        tree_.predict(xgb.DMatrix(X))
    ）
individual_preds = np.vstack(individual_preds)

# 将个体预测汇总为最终预测
individual_logits = inverse_sigmoid(individual_preds)
Final_logits = individual_logits.sum(axis=0)
Final_preds = sigmoid(final_logits)

# 验证正确性
xgb_preds = booster_.predict(xgb.DMatrix(X))
np.testing.assert_almost_equal(final_preds, xgb_preds)

&lt;块引用&gt;
断言错误：数组不几乎等于小数点后 7 位
不匹配的元素：150 / 150 (100%)
最大绝对差：0.90511334
最大相对差值：0.99744916
x: 数组([7.4847587e-05, 7.4847587e-05, 7.4847587e-05, 7.4847587e-05,
7.4847587e-05、7.4847587e-05、7.4847587e-05、7.4847587e-05、
7.4847587e-05, 7.4847587e-05, 7.4847587e-05, 7.4847587e-05,...
y: 数组([0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,
0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,
0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127, 0.0293127,...
]]></description>
      <guid>https://stackoverflow.com/questions/78334661/get-xgboost-prediction-based-on-individual-trees</guid>
      <pubDate>Tue, 16 Apr 2024 12:49:27 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 生存模型</title>
      <link>https://stackoverflow.com/questions/75010397/xgboost-survival-model</link>
      <description><![CDATA[我正在尝试开发 XGBoost 生存模型。这是我的代码的快速快照：
X = df_High_School[[&#39;Gender&#39;, &#39;Lived_both_Parents&#39;, &#39;Moth_Born_in_Canada&#39;, &#39;Father_Born_in_Canada&#39;,&#39;Born_in_Canada&#39;,&#39;Aboriginal&#39;,&#39;Visible_Minority&#39;]] # 协变量
y = df_High_School[[&#39;time_to_event&#39;, &#39;event&#39;]] # 事件发生时间和事件指示器

#将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#开发模型
model = xgb.XGBRegressor(objective=&#39;survival:cox&#39;)

它给了我以下错误：
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-9-1c5a15fa4b2b&gt;在&lt;模块&gt;中
     18
     19 # 将模型拟合到训练数据
---&gt; 20 model.fit(X_train, y_train)
     21
     22 # 对测试集进行预测

2帧
_maybe_pandas_label（标签）中的/usr/local/lib/python3.8/dist-packages/xgboost/core.py
    261 if isinstance（标签，DataFrame）：
    [262] 第 262 章1：
--&gt; 263 raise ValueError（&#39;标签的数据帧不能有多列&#39;）
    264
    第265章

ValueError：标签的数据帧不能有多列

由于这是一个生存模型，我需要两列 t 来指示事件和 time_to_event。我还尝试将 Dataframes 转换为 Numpy，但它也不起作用。
有什么线索吗？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/75010397/xgboost-survival-model</guid>
      <pubDate>Wed, 04 Jan 2023 19:32:04 GMT</pubDate>
    </item>
    </channel>
</rss>