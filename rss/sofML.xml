<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 13 Aug 2024 21:15:02 GMT</lastBuildDate>
    <item>
      <title>如何将保存的模型从 Kaggle 导入和下载到本地模型</title>
      <link>https://stackoverflow.com/questions/78867971/how-to-import-and-download-saved-models-from-kaggle-to-local-model</link>
      <description><![CDATA[第一次在这里发帖，所以对任何不规范的格式深表歉意。我正在开展一个使用模型组合进行集成训练的项目，但在处理某些数据格式时遇到了问题。我能够成功下载并使用 Kaggle 上的 gemma 和 llama 语言模型，但很难从 Bert 模型下载并转换为有用的模型进行预处理。文件格式为 .pb 保存的模型格式。到目前为止，我已经成功导入了模型数据，构建了编码器，并从下载的文件中保存了一个模型（至少我认为是这样）。这是我目前所拥有的：
import tensorflow as tf
from transformers import BertTokenizer
import kagglehub
import keras

# 下载模型（假设已设置 api 密钥和访问权限）
path = kagglehub.model_download(&quot;tensorflow/bert/tensorFlow2/en-wwm-uncased-l-24-h-1024-a-16&quot;)

print(&quot;模型文件路径：&quot;, path)

model_path=path
#使用 keras 和 bert tokenizer 构建模型和编码器
model = keras.layers.TFSMLayer(model_path, call_endpoint=&#39;serving_default&#39;)
encoder = BertTokenizer.from_pretrained(model_path+r&#39;\assets\vocab.txt&#39;)

# 概念证明
print(&quot;用户：&quot;)
input_text = tf.keras.layers.Input(shape=(), dtype=tf.string)
# 对输入进行标记（此处出错）
tokenize=[encoder(segment) for fragment in input_text]
seq_length = 128 
bert_pack_inputs = keras.Layer(
coder,
arguments=dict(seq_length=seq_length)) # 可选参数。


我遇到的主要问题是标记器。当我对文本进行标记时，它会抛出错误：
回溯（最近一次调用最后一次）：
文件“C:\Users\cwaid\example4.py”，第 20 行，位于
tokenize=[encoder(segment) for section in input_text]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\cwaid\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\backend\common\keras_tensor.py”，第 120 行，位于iter
raise NotImplementedError(
NotImplementedError:不支持对符号 KerasTensor 进行迭代。
我不确定我的模型和编码器是否根据此错误正确初始化，但我不知道如何修复它，因为这是 Kaggle 文档中演示的内容：
https://www.kaggle.com/models/tensorflow/bert/tensorFlow2/en-wwm-uncased-l-24-h-1024-a-16
就我的项目而言，我没有使用 kaggle journal 或 jupyter notebook，因为我正在尝试构建一个独立的预训练模型系统，用于低级语言的集成学习，而是构建python 中的概念证明。
我曾尝试将 pb 文件转换为纯 keras 和 meta 文件，但这不是一个有据可查的解决方案，所以我试图避免这样做，以免使我正在做的事情复杂化（尽管如果它更合适，我愿意接受它）。此外，我尝试将其转换为基于 pytorch 的系统，但似乎数据不适合直接翻译，除非我的模型和编码器正确，但同样，我不知道是否是这种情况]]></description>
      <guid>https://stackoverflow.com/questions/78867971/how-to-import-and-download-saved-models-from-kaggle-to-local-model</guid>
      <pubDate>Tue, 13 Aug 2024 19:13:56 GMT</pubDate>
    </item>
    <item>
      <title>COCO 格式转为 YOLO 格式（分割掩码）</title>
      <link>https://stackoverflow.com/questions/78867841/coco-format-to-yolo-format-segmentation-masks</link>
      <description><![CDATA[我基本上需要分割我拥有的图像，以便创建一个数据集来训练 YOLOv8-seg。
我正在尝试使用 CVAT 创建分割蒙版，但是我无法使用 YOLO 导出格式导出分割注释，因此我使用 COCO 导出格式，然后考虑将其转换为 YOLO 格式。
如何使用 Python 代码将 COCO 格式（带分割）转换为 YOLO 格式（带分割）？
有谁知道任何（免费/便宜）工具可以自动将分割导出为 YOLO 格式，而不必转换 COCO 格式？
我很感谢你的帮助 :)
我已经问过 Chatgpt，但它没有给我任何好的答案。]]></description>
      <guid>https://stackoverflow.com/questions/78867841/coco-format-to-yolo-format-segmentation-masks</guid>
      <pubDate>Tue, 13 Aug 2024 18:23:45 GMT</pubDate>
    </item>
    <item>
      <title>如何使用声学信号处理和机器学习将蓝莓分类为“脆”、“多汁”或“软”？[关闭]</title>
      <link>https://stackoverflow.com/questions/78867785/how-to-classify-blueberries-as-crunchy-juicy-or-soft-using-acoustic-signa</link>
      <description><![CDATA[我正在开展一个项目，根据蓝莓的质地对蓝莓进行分类，具体来说，是软的、多汁的还是脆的，使用蓝莓压碎时发出的声音。我有大约 1100 个音频样本，并且为每个样本生成了声谱图。
不幸的是，我没有标记数据，所以我不能直接应用监督机器学习技术。相反，我正在寻找基于声谱图区分这三个类别的有效方法。
我附上了我认为可能是软的、多汁的和脆的蓝莓的声谱图示例。但是，由于数据没有标记，我不确定这些假设是否正确。
这是我的假设：
脆浆果：压碎时，它们会在音频信号中产生单独的、不同的峰值。这些峰值随时间而分散，表明浆果正在以清晰、分段的方式分裂。
脆浆果
多汁浆果：压碎时，它们会在音频信号中产生连续的峰值。这些峰值更紧密地堆积在一起并持续存在，表明果汁和果肉会爆发，阻力较小，从而产生更平滑的声音。
多汁浆果
软浆果：这些浆果产生的峰值很少且很小。声音微弱且不太清晰，表明浆果很容易被压碎，阻力很小，对音频信号的干扰最小。
软浆果
我尝试的方法：
我尝试通过检测音频信号特定时间范围内的峰值来对蓝莓进行分类。这种方法让我能够有效地区分软浆果和脆浆果，因为软浆果产生的峰值更少、更小，而脆浆果的峰值则明显、分离。
我的预期：
我预计这种峰值检测方法也有助于对多汁浆果进行分类，因为我预计连续、幅度更高的峰值将与其他类别不同。
实际发生的情况：
虽然该方法对软浆果和脆浆果很有效，但未能成功区分多汁浆果。多汁浆果峰的连续性并不像我预期的那样突出，因此很难对它们进行准确的分类。]]></description>
      <guid>https://stackoverflow.com/questions/78867785/how-to-classify-blueberries-as-crunchy-juicy-or-soft-using-acoustic-signa</guid>
      <pubDate>Tue, 13 Aug 2024 18:10:48 GMT</pubDate>
    </item>
    <item>
      <title>如何开始开发将印度手语 (ISL) 翻译成文本和语音的应用程序？[关闭]</title>
      <link>https://stackoverflow.com/questions/78867530/how-to-begin-developing-an-app-for-translating-indian-sign-language-isl-into-t</link>
      <description><![CDATA[我们的团队正在参加一个项目竞赛，我们计划创建一个应用程序，将印度手语 (ISL) 实时翻译成文本和语音。目标是促进聋人和听力障碍者与听力世界的交流。该应用程序应该能够：

识别和解释各种 ISL 手势。
以多种印度语言提供准确的文本和语音输出。

当前状态：

我们只讨论了问题陈述，尚未开始任何开发。
我们的团队成员在应用程序开发方面经验很少或根本没有。

需要指导：

我们应该如何开始这个项目？我们应该探索哪些技术或工具？
是否有任何特定的资源或教程可以帮助我们学习与该项目相关的应用程序开发基础知识？
在刚接触此项目的团队成员之间分配任务的好方法是什么？
]]></description>
      <guid>https://stackoverflow.com/questions/78867530/how-to-begin-developing-an-app-for-translating-indian-sign-language-isl-into-t</guid>
      <pubDate>Tue, 13 Aug 2024 17:06:32 GMT</pubDate>
    </item>
    <item>
      <title>YOLOV8 创建了太多边界框[重复]</title>
      <link>https://stackoverflow.com/questions/78867501/yolov8-creating-too-many-bounding-boxes</link>
      <description><![CDATA[我使用 yoloV8 训练了一个模型，该模型有助于检测摩托车骑手的头盔，并尝试通过创建虚拟环境 venv 并安装以下软件包 numpy opencv-python tensorflow ultralytics 来运行
起初，它向我显示了错误

OSError：[WinError 126] 找不到指定的模块。加载“D:\ACADEMICS\projects\helmet\venv\Lib\site-packages\torch\lib\fbgemm.dll”时出错或其依赖项之一。

通过在安装过程中下载 Visual Studio 2022 社区版 并安装 C++ 桌面环境 解决了缺少文件的问题。
但是现在在随机位置创建的边界框太多了
以下是示例
太多框
它实际上应该是什么样的
预期结果

已编辑 == 在下面添加了代码

import cv2
from ultralytics import YOLO

模型 = YOLO(&quot;runs/detect/train2/weights/best.pt&quot;)

结果 = 模型.预测(source=&#39;download.jpeg&#39;, show = True, save=True)
]]></description>
      <guid>https://stackoverflow.com/questions/78867501/yolov8-creating-too-many-bounding-boxes</guid>
      <pubDate>Tue, 13 Aug 2024 16:57:54 GMT</pubDate>
    </item>
    <item>
      <title>算法无法找到图像</title>
      <link>https://stackoverflow.com/questions/78866202/the-algorithm-cannot-find-the-images</link>
      <description><![CDATA[我正在使用 Yolo-nas 开发一种算法，我用 labelImg 准备了数据集。我使用 Python 3.10.11 和超梯度监督一起执行此算法。问题如下：算法加载数据，但在绘制图像时，它显示无法在目录中找到图像，我用其他算法进行了一些测试，它可以找到目录的路径。我怀疑是超梯度版本（3.7.1）
当我必须绘制训练数据时，错误开始出现
FileNotFoundError：未找到dataset\\images\\train\\img1.png。
请确保已下载数据集并且路径正确

注意：数据集中的图像是 pdf，我将它们转换为 png，以便能够在 labelImg 中使用它们并识别对象类

我尝试更改目录
重新制作数据集
我检查了其他算法是否可以搜索图像，结果确实如此。

import torch
torch.__version__

from tqdm.notebook import tqdm
from super_gradients.training import dataloaders
from super_gradients.training.dataloaders.dataloaders import coco_detection_yolo_format_train, coco_detection_yolo_format_val
from super_gradients.training import models
from super_gradients.training.losses import PPYoloELoss
from super_gradients.training.metrics 导入 DetectionMetrics_050
从 super_gradients.training.models.detection_models.pp_yolo_e 导入 PPYoloEPostPredictionCallback

dataset_params = {
&#39;data_dir&#39;: &quot;nf/dataset&quot;, 
&#39;train_images_dir&#39;: &quot;dataset/images/train&quot;,
&#39;train_labels_dir&#39;: &quot;dataset/labels/train&quot;,
&#39;val_images_dir&#39;: &quot;dataset/images/val&quot;,
&#39;val_labels_dir&#39;: &quot;dataset/labels/val&quot;,
&#39;classes&#39;: [&#39;cabecalho&#39;, &#39;assinatura&#39;, &#39;rodape&#39;]
}

MODEL_ARCH = &#39;yolo_nas_l&#39;
DEVICE = &#39;cuda&#39;如果 torch.cuda.is_available() 否则 &quot;cpu&quot;
BATCH_SIZE = 10 
MAX_EPOCHS = 12
CHECKPOINT_DIR = &#39;\checkpoint&#39;
EXPERIMENT_NAME = &quot;nf&quot;

dados_treino = coco_detection_yolo_format_train(
dataset_params={
&#39;data_dir&#39;: dataset_params[&#39;data_dir&#39;],
&#39;images_dir&#39;: dataset_params[&#39;train_images_dir&#39;],
&#39;labels_dir&#39;: dataset_params[&#39;train_labels_dir&#39;],
&#39;classes&#39;: dataset_params[&#39;classes&#39;]
},
dataloader_params={
&#39;batch_size&#39;: BATCH_SIZE,
&#39;num_workers&#39;: 1
}
)

val_dados = coco_detection_yolo_format_val(
dataset_params={
&#39;data_dir&#39;: dataset_params[&#39;data_dir&#39;],
&#39;images_dir&#39;: dataset_params[&#39;val_images_dir&#39;],
&#39;labels_dir&#39;: dataset_params[&#39;val_labels_dir&#39;], 
&#39;classes&#39;: dataset_params[&#39;classes&#39;]
},
dataloader_params={
&#39;batch_size&#39;: BATCH_SIZE,
&#39;num_workers&#39;: 1
}
)

dados_treino.dataset.transforms

dados_treino.dataset.plot()
]]></description>
      <guid>https://stackoverflow.com/questions/78866202/the-algorithm-cannot-find-the-images</guid>
      <pubDate>Tue, 13 Aug 2024 12:07:15 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试使用 Hampel 进行 ECG 信号分析时，出现了 IndexError</title>
      <link>https://stackoverflow.com/questions/78865609/i-am-getting-indexerror-when-i-am-trying-to-use-hampel-for-ecg-signal-analysis</link>
      <description><![CDATA[代码：
signal = signal - np.mean(signal)
# 从结果对象中提取过滤后的数据
outlier_indices = hampel(pd.Series(signal), window_size=100).filtered_data 
peaks = [list(map(itemgetter(1), g)) for k, g in groupby(enumerate(outlier_indices), lambda x: x[0] - x[1])]

# 在使用索引之前，先将其转换为整数
peaks_max_vals = [peaks[i][np.argmax(abs(signal[np.round(peaks[i]).astype(int)]))] for i in range(len(peaks))] 

peaks_sign = np.sign(signal[peaks_max_vals])

错误：
IndexError Traceback（最近一次调用最后一次）&lt;ipython-input-13-c66286decef9&gt; in &lt;cell line: 9&gt;()
7 peaks_max_vals = [peaks[i][np.argmax(abs(signal[np.round(peaks[i]).astype(int)]))] for i in range(len(peaks))]
8 
----&gt; 9 peaks_sign = np.sign(signal[peaks_max_vals])
10 
11 diffs_max = np.where(np.diff(peaks_max_vals) &lt; 40)[0]

IndexError：只有整数、切片（`:`）、省略号（`...`）、numpy.newaxis（`None`）和整数或布尔数组才是有效索引
]]></description>
      <guid>https://stackoverflow.com/questions/78865609/i-am-getting-indexerror-when-i-am-trying-to-use-hampel-for-ecg-signal-analysis</guid>
      <pubDate>Tue, 13 Aug 2024 10:01:02 GMT</pubDate>
    </item>
    <item>
      <title>我的 ML 二元分类模型的最终预测准确率非常糟糕[关闭]</title>
      <link>https://stackoverflow.com/questions/78863903/final-predictions-accuracy-of-my-ml-binary-classification-model-is-horrible</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78863903/final-predictions-accuracy-of-my-ml-binary-classification-model-is-horrible</guid>
      <pubDate>Mon, 12 Aug 2024 23:32:18 GMT</pubDate>
    </item>
    <item>
      <title>我们如何运行 Apple 雪貂模型？</title>
      <link>https://stackoverflow.com/questions/78863794/how-do-we-run-the-apple-ferret-model</link>
      <description><![CDATA[我已经安装了苹果雪貂模型所需的所有权重和检查点，除了运行 3 个终端并测试演示之外，我如何在本地运行模型来为图像文件夹添加标题？
我尝试了 https://vivekupadhyay1.medium.com/how-to-use-ferret-apples-open-source-multimodal-llm-for-your-next-project-c561f0087a5d，但无法找到 github repo 中提到的 eval.py 或脚本。]]></description>
      <guid>https://stackoverflow.com/questions/78863794/how-do-we-run-the-apple-ferret-model</guid>
      <pubDate>Mon, 12 Aug 2024 22:36:25 GMT</pubDate>
    </item>
    <item>
      <title>如何使用多个掩模进行医学图像分割，然后进行逐像素分类？</title>
      <link>https://stackoverflow.com/questions/78863682/how-to-go-for-medical-image-segmentation-with-multiple-masks-and-then-doing-pixe</link>
      <description><![CDATA[我获得了训练图像，每幅图像都有两个掩码，分别代表两个不同的类别。我应该如何分割图像，然后进行像素分类，以获得输出图像，其中 0 代表背景，1 和 2 代表两个类别？
我尝试组合掩码，然后将掩码和相应的图像输入分割模型，但不知何故我失败了。我是否应该组合掩码，因为我们还需要在输出中对像素进行分类。]]></description>
      <guid>https://stackoverflow.com/questions/78863682/how-to-go-for-medical-image-segmentation-with-multiple-masks-and-then-doing-pixe</guid>
      <pubDate>Mon, 12 Aug 2024 21:37:57 GMT</pubDate>
    </item>
    <item>
      <title>当我的游戏对象的任何部分发生碰撞时，代理为空</title>
      <link>https://stackoverflow.com/questions/78863425/agent-is-null-when-any-part-of-my-game-object-collides</link>
      <description><![CDATA[我正在根据 Unity 的 WalkerAgent 训练布娃娃走路，我做了一些改动以更好地满足我的目的。现在，每当布娃娃的任何身体部位与地面碰撞时，代理都是空的，从而阻止任何奖励被应用。
我多次检查了我的代码，看起来没有任何问题。该项目似乎也设置正确。导致我陷入这种困境的原始错误是

nullreferenceexception：对象引用未设置为对象实例Unity.MLAgentsExamples.ObjectContact.OnCollisionStay
(UnityEngine.Collision col) (at
Assets/Scripts/ObjectContact.cs:55)UnityEngine.Physics:OnSceneContact(PhysicsScene,
IntPtr, Int32) (at
/Users/bokken/build/output/unity/unity/Modules/Physics/ScriptBindings/PhysicsContact.bindings.cs:49)

我很确定所有事情都按正确的顺序发生，并且代理应该在调用之前初始化。
据我所知，这些是代码。
来自 WalkerAgent（主脚本）：
public override void Initialize()
{
m_OrientationCube = GetComponentInChildren&lt;OrientationCubeController&gt;();

//设置每个身体部位
m_JdController = GetComponent&lt;JointDriveController&gt;();

m_JdController.SetupBodyPart(hips);
m_JdController.SetupBodyPart(spine);
m_JdController.SetupBodyPart(head);
m_JdController.SetupBodyPart(legrotateL);
m_JdController.SetupBodyPart(thighL);
m_JdController.SetupBodyPart(kneerotateL);
m_JdController.SetupBodyPart(shinL);
m_JdController.SetupBodyPart(footL);
m_JdController.SetupBodyPart(腿部旋转R);
m_JdController.SetupBodyPart(大腿R);
m_JdController.SetupBodyPart(膝盖旋转R);
m_JdController.SetupBodyPart(小腿R);
m_JdController.SetupBodyPart(脚R);
m_JdController.SetupBodyPart(手臂旋转L);
m_JdController.SetupBodyPart(手臂L);
m_JdController.SetupBodyPart(前臂旋转L);
m_JdController.SetupBodyPart(前臂L);
m_JdController.SetupBodyPart(手L);
m_JdController.SetupBodyPart(手臂旋转R);
m_JdController.SetupBodyPart(手臂R);
m_JdController.SetupBodyPart(前臂旋转R);
m_JdController.SetupBodyPart(前臂R);
m_JdController.SetupBodyPart(handR);
}

来自 JointDriveController：
public void SetupBodyPart(Transform t)
{
var bp = new BodyPart
{
rb = t.GetComponent&lt;Rigidbody&gt;(),
joint = t.GetComponent&lt;CharacterJoint&gt;(),
StartingPos = t.position,
StartingRot = t.rotation
};
bp.rb.maxAngularVelocity = k_MaxAngularVelocity;

// 添加并设置地面接触脚本
bp.objectContact = t.GetComponent&lt;ObjectContact&gt;();

var agent = gameObject.GetComponent&lt;Agent&gt;();
如果 (agent == null)
{
agent = gameObject.AddComponent&lt;Agent&gt;();
}
bp.objectContact.agent = agent;

如果 (!bp.objectContact)
{
bp.objectContact = t.gameObject.AddComponent&lt;ObjectContact&gt;();
bp.objectContact.agent = gameObject.GetComponent&lt;Agent&gt;();
}
else
bp.objectContact.agent = gameObject.GetComponent&lt;Agent&gt;();

如果 (bp.objectContact.agent == null)
Debug.LogError($&quot;未找到 {t.name} 的代理&quot;);

bp.thisJdController = this;
bodyPartsDict.Add(t, bp);
bodyPartsList.Add(bp);
}

来自 ObjectContact：
void OnCollisionEnter(Collision col)
{
if (agent == null)
{
Debug.LogError($&quot;Agent is null on {gameObject.name} during OnCollisionEnter with {col.gameObject.name}&quot;);
return;
}

if (col.transform.CompareTag(k_Ground))
touchingGround = true;

if (col.transform.CompareTag(k_Wall))
touchingWall = true;

if (col.transform.CompareTag(k_Target))
{
touchingTarget = true;
agent.AddReward(targetReward);
}
}

void OnCollisionStay(Collision col)
{
if (col.transform.CompareTag(k_Ground))
agent.AddReward(groundContactPenalty);

if (col.transform.CompareTag(k_Wall))
agent.AddReward(wallContactPenalty);

Debug.Log($&quot;OnCollisionStay called for {gameObject.name} with {col.gameObject.name}&quot;);

if (agent == null)
{
Debug.LogError(&quot;Agent is null during OnCollisionStay&quot;);
return;
}
}
]]></description>
      <guid>https://stackoverflow.com/questions/78863425/agent-is-null-when-any-part-of-my-game-object-collides</guid>
      <pubDate>Mon, 12 Aug 2024 20:07:45 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 Torch 使用 4 个 GPU 进行训练：torch.distributed.elastic.multiprocessing.api</title>
      <link>https://stackoverflow.com/questions/78863216/unable-to-train-with-4-gpus-using-torch-torch-distributed-elastic-multiprocessi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78863216/unable-to-train-with-4-gpus-using-torch-torch-distributed-elastic-multiprocessi</guid>
      <pubDate>Mon, 12 Aug 2024 19:01:10 GMT</pubDate>
    </item>
    <item>
      <title>为什么 YOLOv8 可以检测到一堆随机物体</title>
      <link>https://stackoverflow.com/questions/78860154/why-yolov8-detects-bunch-of-random-objects</link>
      <description><![CDATA[我正在使用 YOLOv8 命令行界面 (CLI) 对图像运行对象检测，但得到了意想不到的结果。该模型似乎检测到了图像中不存在的一堆随机对象。
这是我使用的命令：
yolo predict model=yolov8n.pt source=&#39;https://ultralytics.com/images/bus.jpg&#39;
当我运行此命令时，它会产生类似于下图所示的结果：
]]></description>
      <guid>https://stackoverflow.com/questions/78860154/why-yolov8-detects-bunch-of-random-objects</guid>
      <pubDate>Mon, 12 Aug 2024 05:58:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv8 进行大量错误检测</title>
      <link>https://stackoverflow.com/questions/78820748/alot-of-incorrect-detection-using-yolov8</link>
      <description><![CDATA[我尝试使用 Visual Code Studio 运行 YOLOv8。安装了 ultralytics 并在 vs code 终端上运行了 yolo predict model=yolov8n.pt source=&#39;https://ultralytics.com/images/bus.jpg&#39;。
但是我收到的输出是
2 个人、1 辆自行车、5 辆汽车、10 辆摩托车、73 艘船、3 个停车标志、1 只狗、10 匹马、10 头牛、32 只熊、1 只长颈鹿、63 把雨伞、6 个手提包、9 个飞盘、15 块滑雪板、5 块冲浪板、12 把刀、5 张床、37 张餐桌

这些显然不是这张图片的一部分。

当我第一次安装 ultralytics 并尝试运行 torch 时，出现了缺少依赖项的错误。fbgemm.ddl 丢失。后来，当我安装 vs_BuildTools 时，这个问题得到了解决。然后我继续在虚拟环境中运行代码，其中使用 torch 的程序运行没有任何错误。然后我继续输入此代码片段并遇到此问题。我也尝试使用命令提示符和 jupyter 笔记本运行，但同样的问题仍然存在。
我也检查了版本是否兼容，结果是兼容的。我还没有安装 cuda，是因为这个原因还是还有其他我不知道的问题？请有人帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78820748/alot-of-incorrect-detection-using-yolov8</guid>
      <pubDate>Thu, 01 Aug 2024 11:33:58 GMT</pubDate>
    </item>
    <item>
      <title>从 R 中的空间数据框中识别横断面上的点</title>
      <link>https://stackoverflow.com/questions/78787801/identify-points-lying-on-transects-from-a-spatial-dataframe-in-r</link>
      <description><![CDATA[我有调查数据，沿着与海岸垂直的平行横断面每隔 1 海里记录一次。对于每条记录，我都有纬度和经度、速度、方位等信息。沿着横断面，速度约为 10 节。我还在样条间（速度可能不同，方位肯定不同）处有一些点，如果进行了拖网，我还在样条外有一些点。
我想要做的是将属于同一样条的所有点分组（例如，参见图）：

这只是使用 1 NM 点间距离完成的，正如您在图中看到的那样，这实际上不起作用，因为只要有样条间（如样条 5），它就会与样条本身分组在一起。此外，在横断面 13 中，由于某种原因，2 个后续记录之间的距离略大于 1 海里，因此这些点被分成 2 个横断面（您可以看到颜色略有不同）。
此处显示的数据框示例：
 |year |datetime |xkm |ykm |logdiff |time_diff |distance |bearing |speed |
|&lt;dbl&gt; |&lt;dttm&gt; |&lt;dbl&gt; |&lt;dbl&gt; |&lt;dbl&gt;| &lt;dbl&gt;| &lt;drtn&gt; | &lt;dbl&gt;| &lt;dbl&gt;| &lt;dbl&gt;| &lt;dbl&gt;|
|------|---------- |------|-----|--------|------ --|---------|--------|------|
|2023 |2023-09-26 15:03:00 |221. |1606.| 1| 360 秒 | 1| -1.58| 10|
|2023 |2023-09-26 15:08:00 |223. |1606.| 1| 300 秒 | 1| -1.58| 12|
|2023 |2023-09-26 15:14:00 |225. |1606.| 1| 360 秒 | 1| -1.58| 10|
|2023 |2023-09-26 15:19:00 |227. |1606.| 1| 300 秒 | 1| -1.58| 12|
|2023 |2023-09-26 15:25:00 |229. |1606.| 1| 360 秒 | 1| -1.84| 10|
|2023 |2023-09-26 15:30:00 |231. |1606.| 1| 300 秒 | 1| -1.85| 12|
|2023 |2023-09-26 15:36:00 |233. |1606.| 1| 360 秒 | 1| -1.58| 10|
|2023 |2023-09-26 15:41:00 |234. |1605.| 1| 300 秒 | 1| -1.85| 12|
|2023 |2023-09-26 15:47:00 |236. |1605.| 1| 360 秒 | 1| -1.58| 10|
|2023 |2023-09-26 15:52:00 |238. |1605.| 1| 300 秒 | 1| -1.58| 12|

在 R 中解决这个问题的最佳方法是什么？我考虑过一些无监督的机器学习算法，比如使用 dbscan 进行聚类，但我不确定我是否正确使用了它。除了点之间的距离，我还想使用其他参数来分类一个点是否属于横断面（例如方位和速度）。
我的尝试：
# 准备聚类数据
clustering_data &lt;- df %&gt;% select(year, speed, bearing, xkm, ykm)

dput(clustering_data) 

# dput 输出
structure(list(year = c(2023, 2023, 2023, 2023, 2023, 2023, 2023, 
2023, 2023, 2023), datetime = c(45195.6270833333, 45195.6305555556, 
45195.6347222222, 45195.6381944444, 45195.6423611111, 45195.6458333333, 
45195.65, 45195.6534722222, 45195.6576388889, 45195.6611111111
), xkm = c(221, 223, 225, 227, 229, 231, 233, 234, 236, 238), 
ykm = c(1606, 1606, 1606, 1606, 1606, 1606, 1606, 1605, 1605, 
1605), logdiff = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1), time_diff = c(&quot; 360 秒 &quot;, 
&quot; 300 秒 &quot;, &quot; 360 秒 &quot;, &quot; 300 秒 &quot;, &quot; 360 秒 &quot;, &quot; 300 秒 &quot;, 
&quot; 360 秒 &quot;, &quot; 300 秒 &quot;, &quot; 360 秒 &quot;, &quot; 300 秒 &quot;), 
distance = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1), bearing = c(-1.58, 
-1.58, -1.58, -1.58, -1.84, -1.85, -1.58, -1.85, -1.58, -1.58
), speed = c(10, 12, 10, 12, 10, 12, 10, 12, 10, 12)), row.names = c(NA, 
10L), class = &quot;data.frame&quot;)

# 应用 DBSCAN 聚类
set.seed(123)
db &lt;- dbscan(clu​​stering_data, eps = 1.8, minPts = 5)


有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78787801/identify-points-lying-on-transects-from-a-spatial-dataframe-in-r</guid>
      <pubDate>Wed, 24 Jul 2024 10:33:52 GMT</pubDate>
    </item>
    </channel>
</rss>