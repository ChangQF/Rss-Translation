<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 28 Oct 2024 18:23:20 GMT</lastBuildDate>
    <item>
      <title>性能权衡 - 降低速度以更好地利用 TTS 中的内存</title>
      <link>https://stackoverflow.com/questions/79134305/performance-trade-off-reduce-speed-for-better-memory-usage-in-tts</link>
      <description><![CDATA[我正在使用 https://github.com/coqui-ai/tts/ 运行多个 TTS 模型。目前，我正在试验 your_tts 模型，这是一个较小的模型。我有 4 GB 内存 GPU（GTX 1050），支持 Cuda。一段时间内，它运行没有问题，而且速度非常快。过了一会儿，它抛出了内存错误。我想知道是否有办法降低速度并减少内存使用？通过 config.json 或类似的东西，也许在代码中通过一些参数？]]></description>
      <guid>https://stackoverflow.com/questions/79134305/performance-trade-off-reduce-speed-for-better-memory-usage-in-tts</guid>
      <pubDate>Mon, 28 Oct 2024 16:30:39 GMT</pubDate>
    </item>
    <item>
      <title>模型（输入，训练=True）和模型（输入，训练=False）之间的巨大差异</title>
      <link>https://stackoverflow.com/questions/79134261/huge-difference-between-modelinput-training-true-and-modelinput-training-fa</link>
      <description><![CDATA[我被要求根据我读过的一篇文章实现一个机器学习模型。为此，该论文推荐了一种特定类型的预测可靠性度量：该模型运行 M 次随机前向传递，其中有 M 种不同的 dropout 模式（其中 alpha=0.5%）。但是，我注意到，模型（输入，训练=True）的 M 次随机运行的平均输出与模型（输入，训练=False）的 M 次随机运行的平均输出有很大不同。这是由于什么原因？如果不使用 dropout 模式，运行不应该收敛到相似的值吗？]]></description>
      <guid>https://stackoverflow.com/questions/79134261/huge-difference-between-modelinput-training-true-and-modelinput-training-fa</guid>
      <pubDate>Mon, 28 Oct 2024 16:16:55 GMT</pubDate>
    </item>
    <item>
      <title>当没有部分拟合选项时，如何训练分区数据集？</title>
      <link>https://stackoverflow.com/questions/79133844/how-do-i-train-a-partitioned-dataset-when-there-is-not-an-option-for-partial-fit</link>
      <description><![CDATA[我正在从包含 10 个分区的数据集训练 ML 模型，这样我就不会耗尽可用内存。我目前正在每个分区上训练 3 个不同的模型，然后将它们放入 VotingRegressor 中，然后再次进行拟合，但是由于它占用了大量内存，我无法将其拟合到整个训练集。这里有一个小片段
all_feature_cols = [f&quot;feature_{i:02d}&quot; for i in range(79)]

if TRAINING:
# 初始化列表以存储模型
lgbm_models = []
xgb_models = []
cat_models = []

# 逐步训练每个模型
for partion in range(10):
start_time = time.time()

# 为每个分区创建新的模型实例
lgbm = LGBMRegressor(num_leaves=127, n_estimators=200, max_depth=3, 
learning_rate=0.05, device_type=&#39;gpu&#39;, verbose=-1,
reg_alpha = 0.1, reg_lambda = 0.1)
xgb = XGBRegressor(n_estimators=200, min_child_weight=5, max_depth=7, 
learning_rate=0.01, device=&#39;cpu&#39;,
reg_alpha = 0.1, reg_lambda = 0.1)
cat = CatBoostRegressor(n_estimators=200, max_depth=7, learning_rate=0.05, 
reg_lambda = 0.1, task_type=&#39;GPU&#39;, verbose=False)

# 过滤当前分区并分别收集目标
partition_df = df.filter(pl.col(&quot;partition_id&quot;) ==partition)

# 在预处理之前提取并收集目标列
y =partition_df.select(&quot;responder_6&quot;).collect().to_numpy().ravel()

# 预处理特征（不包括目标）
X =partition_df.select(all_feature_cols).collect().to_numpy()

# 分成训练/验证并保持时间顺序
train_idx = int(len(X) * 0.8)
X_train, X_test = X[:train_idx], X[train_idx:]
y_train, y_test = y[:train_idx], y[train_idx:]

# 在此分区上训练模型
lgbm.fit(X_train, y_train)
xgb.fit(X_train, y_train)
cat.fit(X_train, y_train)

# 使用分区标识符保存训练好的模型
lgbm_models.append((f&#39;lgbm_{partition}&#39;, deepcopy(lgbm)))
xgb_models.append((f&#39;xgb_{partition}&#39;, deepcopy(xgb)))
cat_models.append((f&#39;cat_{partition}&#39;, deepcopy(cat)))

# 计算已用时间
end_time = time.time()
elapsed_time = end_time - start_time
elapsed_str = str(timedelta(seconds=int(elapsed_time)))
print(f&quot;分区 {partition} 在 {elapsed_str} 内完成&quot;)

# 清理内存
if partion &lt; 9:
del X, y, X_train, X_test, y_train, y_test, partition_df
else:
del X, y, X_train, y_train, partition_df
gc.collect()

# 创建并拟合 VotingRegressor 和所有经过训练的模型
model = VotingRegressor(lgbm_models + xgb_models + cat_models)

# 拟合模型
model.fit(X_test, y_test)

y_pred = model.predict(X_test)
print(r2_score(y_test, y_pred))

# 保存最终模型 
dump(model, &quot;/kaggle/working/JS_model.joblib&quot;)
```
]]></description>
      <guid>https://stackoverflow.com/questions/79133844/how-do-i-train-a-partitioned-dataset-when-there-is-not-an-option-for-partial-fit</guid>
      <pubDate>Mon, 28 Oct 2024 14:30:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在 IBM Watson Assistant 响应中动态包含来自 Watson Discovery 的完整且正确的 URL？[关闭]</title>
      <link>https://stackoverflow.com/questions/79133652/how-to-dynamically-include-full-and-correct-urls-from-watson-discovery-in-ibm-wa</link>
      <description><![CDATA[我正在使用 IBM Watson Assistant，使用 Llama 3.8 作为语言模型，我面临的一个问题是，模型始终无法在其响应中检索正确的 URL。我从 Watson Discovery 中的文档中动态提取这些 URL，并且每个响应都需要根据提出的问题包含不同的特定链接。尽管我的提示明确指示模型包含一个特定的完整链接，但模型的响应始终包含不正确或不完整的链接。
是否有人遇到过与 IBM Watson Assistant 或其他 LLM 类似的问题，即模型无法检索确切的指定 URL，尤其是在不同响应中需要不同的链接时？是否有任何已知的解决方法、配置或提示调整可以确保模型可靠地从 Watson Discovery 中检索并显示正确的链接？
以下是我尝试过的概述：
提示调整：我在提示中包含了明确的指示“包含整个链接而不缩短它”，我甚至尝试将链接直接放在提示中作为示例。但是，模型要么生成不完整的链接，要么生成完全错误的链接。
提示示例：
下面是我的提示的简化版本，它指示模型将 {DOCUMENTATION_LINK} 替换为与每个问题相关的文档的实际链接：
&quot;您是客户关系经理。您的目的是提供 CRM 流程的简要摘要。如果用户需要详细步骤，请回复：&#39;有关完整流程和所有详细步骤，请点击此链接：{DOCUMENTATION_LINK}&#39;。始终将“{DOCUMENTATION_LINK}”替换为问题中文档的实际链接。&quot;
配置详细信息：
温度：0.5
最大新令牌：900
停止序列：[&quot; &quot;]
重复惩罚：1]]></description>
      <guid>https://stackoverflow.com/questions/79133652/how-to-dynamically-include-full-and-correct-urls-from-watson-discovery-in-ibm-wa</guid>
      <pubDate>Mon, 28 Oct 2024 13:41:29 GMT</pubDate>
    </item>
    <item>
      <title>将注意力机制增强至 O(log N)：一种基于树的 Transformer 模型优化方法</title>
      <link>https://stackoverflow.com/questions/79133220/enhancing-attention-mechanism-to-olog-n-a-tree-based-approach-for-optimizing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79133220/enhancing-attention-mechanism-to-olog-n-a-tree-based-approach-for-optimizing</guid>
      <pubDate>Mon, 28 Oct 2024 11:42:20 GMT</pubDate>
    </item>
    <item>
      <title>如何提高 CNN 模型的准确率并降低损失？</title>
      <link>https://stackoverflow.com/questions/79133215/how-to-increase-accuracy-and-decrease-loss-in-cnn-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79133215/how-to-increase-accuracy-and-decrease-loss-in-cnn-model</guid>
      <pubDate>Mon, 28 Oct 2024 11:41:21 GMT</pubDate>
    </item>
    <item>
      <title>使用没有跳跃连接的 U-Net 进行图像到图像处理。这是真的吗？</title>
      <link>https://stackoverflow.com/questions/79132335/image-to-image-with-u-net-with-no-skip-connections-is-it-real</link>
      <description><![CDATA[如果我想从其他图像（图像到图像）获取一些图像，我可以使用没有跳过连接的 U-Net 吗？因为我不需要保留结构。例如，为了改变某些对象的相机视角。
例如，此模型为 256x256px。它适用于 3 对训练对（输入图像-输出图像），并带有角度增强（+/- 5 度、+/-10 度、+/-15 度），但不适用于 1.000 对。
# 编码器
c = layer.Conv2D(32, kernel_size=4, strides=2, padding=&quot;same&quot;)(inputs)
c = layer.LeakyReLU(negative_slope=0.2)(c) 

c = layer.Conv2D(64, kernel_size=4, strides=2, padding=&quot;same&quot;)(c)
c = layer.LeakyReLU(negative_slope=0.2)(c)

c = layer.Conv2D(128, kernel_size=4, strides=2, padding=&quot;same&quot;)(c)
c =层。LeakyReLU（负斜率=0.2）（c）

c = 层。Conv2D（256，kernel_size=4，strides=2，padding=“相同”（c）

c = 层。LeakyReLU（负斜率=0.2）（c）

c = 层。Conv2D（512，kernel_size=4，strides=2，padding=“相同”（c）

c = 层。LeakyReLU（负斜率=0.2）（c）

c = 层。Conv2D（1024，kernel_size=4，strides=2，padding=“相同”（c）

c = 层。LeakyReLU（负斜率=0.2）（c）

# 瓶颈
b = InstanceNormalization（）（c）

b =图层。重塑((-1,))(b)
b = 图层。密集(512*4*4, kernel_regularizer=l2_reg,)(b)
b = 图层。LeakyReLU(negative_slope=0.2)(b)

b = 图层。Dropout(0.2)(b)

b = 图层。密集(512*4*4*2, kernel_regularizer=l2_reg,)(b)
b = 图层。LeakyReLU(negative_slope=0.2)(b)
b = 图层。重塑((4,4,1024))(b)

# 解码器
d = 图层。UpSampling2D(size=(2, 2))(b)
d = 图层。Conv2D(1024, kernel_size=4, padding=&quot;same&quot;)(d)
d =层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2, 2））（d）

d = 层。Conv2D（512，内核大小=4，填充=“相同”）（d）

d = 层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2, 2））（d）

d = 层。Conv2D（256，内核大小=4，填充=“相同”）（d）

d = 层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2, 2））（d）

d = 层。Conv2D（128，内核大小=4，填充=“相同”）（d）

d =层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2，2））（d）

d = 层。Conv2D（64，kernel_size=4，padding=“相同”（d）

d = 层。LeakyReLU（负斜率=0.2）（d）

d = 层。UpSampling2D（大小=（2，2））（d）

d = 层。Conv2D（32，kernel_size=4，padding=“相同”（d）

d = 层。LeakyReLU（负斜率=0.2）（d）`

# 输出
输出 = 层。Conv2D（3，kernel_size=3，padding=“相同”，激活=“tanh”（d）

模型 = models.Model(inputs=inputs, output=outputs, name=&quot;build_unet&quot;)
return model`

无论输入图像是否旋转或扭曲，我都需要获取输出。]]></description>
      <guid>https://stackoverflow.com/questions/79132335/image-to-image-with-u-net-with-no-skip-connections-is-it-real</guid>
      <pubDate>Mon, 28 Oct 2024 07:00:00 GMT</pubDate>
    </item>
    <item>
      <title>删除所有人口后，NEAT 给出错误</title>
      <link>https://stackoverflow.com/questions/79130999/neat-giving-error-after-deleting-all-the-population</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79130999/neat-giving-error-after-deleting-all-the-population</guid>
      <pubDate>Sun, 27 Oct 2024 16:27:23 GMT</pubDate>
    </item>
    <item>
      <title>输入图像与 TensorFlow 模型输入形状不兼容</title>
      <link>https://stackoverflow.com/questions/79130521/input-image-is-not-compatible-with-tensorflow-model-input-shape</link>
      <description><![CDATA[我正在构建一个模型，我想测试它的性能，因此我导入了一个本地文件并加载它，并尝试使用以下代码预测它的标签：
from tensorflow.preprocessing import image
# tensorlfow 等的其他导入。

#...

# 示例图像
img_path = &quot;./Model/data/brain/train/Glioma/images/gg (2).jpg&quot;
img = image.load_img(img_path,target_size=(256,256))
arr = image.img_to_array(img)
t_img = tf.convert_to_tensor(arr)
print(t_img.shape) # 返回 (256,256,3)
# 客户端测试
client = Client(&quot;brain&quot;) # 自定义类。包含模型：顺序（已编译和训练）
client.predict(img=t_img) # 调用 self.model.predict(t_img)

但是我收到以下错误：
输入 Tensor(&quot;data:0&quot;, shape=(32, 256, 3), dtype=float32) 的输入形状无效。预期形状 (None, 256, 256, 3)，但输入具有不兼容的形状 (32, 256, 3)

我在训练模型中有一个输入层，其 input_shape=[256,256,3]（来自图像宽度、高度和 rgb 值）
您能帮助我理解问题并解决它吗？]]></description>
      <guid>https://stackoverflow.com/questions/79130521/input-image-is-not-compatible-with-tensorflow-model-input-shape</guid>
      <pubDate>Sun, 27 Oct 2024 11:57:01 GMT</pubDate>
    </item>
    <item>
      <title>如何将 AWS Bedrock 与我的数据库集成以实现基于向量的 LLM 响应上下文检索？</title>
      <link>https://stackoverflow.com/questions/79130070/how-can-i-integrate-aws-bedrock-with-my-database-to-enable-vector-based-context</link>
      <description><![CDATA[我正在构建一个 AI 驱动的忠诚度应用程序，并希望利用大型语言模型 (LLM) 根据我的数据库内容提供响应。我目前的计划是：

将数据库数据转换为向量嵌入：我想将我的结构化数据转换为向量嵌入，以便 LLM 可以更轻松地使用它。
将嵌入存储在向量数据库中：这个想法是存储嵌入以实现基于相似性的高效检索。
使用 AWS Bedrock LLM：我想根据用户的查询从我的数据库中检索上下文，并使用 AWS Bedrock 将此上下文输入到 LLM 中以生成响应。

我将不胜感激任何有关以下方面的指导：

嵌入转换：是否有任何推荐的工具或与 AWS Bedrock 兼容的模型用于将关系数据库中的结构化数据转换为有用的嵌入？ Amazon Titan 是否适合这种情况，还是其他模型更好？

向量数据库选项：对于存储和查询嵌入，Amazon OpenSearch 是否合适，还是我应该考虑 FAISS 之类的东西？我的目标是实现高容量、实时的检索效率。

LLM 集成最佳实践：检索类似嵌入后，格式化并将此上下文传递给 AWS Bedrock 上的 LLM 的最佳方法是什么？有任何示例、文章或模板吗？

有关特定工具、模型或文章的建议，可以提供关于在 AWS Bedrock 上设置此工作流程的进一步见解。

]]></description>
      <guid>https://stackoverflow.com/questions/79130070/how-can-i-integrate-aws-bedrock-with-my-database-to-enable-vector-based-context</guid>
      <pubDate>Sun, 27 Oct 2024 07:10:23 GMT</pubDate>
    </item>
    <item>
      <title>迁移学习预训练模型</title>
      <link>https://stackoverflow.com/questions/78875648/transfer-learning-pretrained-model</link>
      <description><![CDATA[我在 Google Colab 上拟合迁移学习模型。但是，我在代码中遇到了一条警告消息
Epoch 1/30
/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: 
UserWarning：您的 `PyDataset` 类应在其构造函数中调用 `super().__init__(**kwargs)`。`**kwargs` 可以包括 `workers`、`use_multiprocessing`、`max_queue_size`。
请勿将这些参数传递给 `fit()`，因为它们将被忽略。
self._warn_if_super_not_called()

在第一个 epoch 之后，我收到以下错误：
----------------------------------------------------------------------------------------
KeyboardInterrupt Traceback（最近一次调用最后一次）
&lt;ipython-input-23-962a870d4412&gt; in &lt;cell line: 16&gt;()
14 # 拟合模型
15 # 运行单元。执行需要一些时间
---&gt; 16 training_history = model_efficientnet.fit(
17 training_set,
18 validation_data=validate_set,

我已经成功地拟合了其他六个迁移学习模型，没有任何问题，它们的准确率令人满意。
如何解决这个问题？
我想获得训练准确率和验证准确率]]></description>
      <guid>https://stackoverflow.com/questions/78875648/transfer-learning-pretrained-model</guid>
      <pubDate>Thu, 15 Aug 2024 14:49:45 GMT</pubDate>
    </item>
    <item>
      <title>Bert 句子嵌入</title>
      <link>https://stackoverflow.com/questions/58168936/bert-sentence-embeddings</link>
      <description><![CDATA[我正在尝试获取 Bert 的句子嵌入，但我不太确定我是否做得正确……是的，我知道已经存在这样的工具，例如 bert-as-service，但我想自己做并了解它的工作原理。
假设我想从以下句子“I am.”的单词嵌入中提取一个句子嵌入。据我了解，Bert 的输出形式为 (12, seq_lenght, 768)。我从最后一个编码器层提取了每个单词嵌入，形式为 (1, 768)。我现在的疑问在于从这两个词向量中提取句子。如果我有 (2,768)，我应该将 dim=1 相加并获得 (1,768) 的向量吗？或者也许将两个单词 (1, 1536) 连接起来并应用 (均值) 池化并获得形状为 (1, 768) 的句子向量。我不确定针对这个给定的例子获取句子向量的正确方法是什么。 ]]></description>
      <guid>https://stackoverflow.com/questions/58168936/bert-sentence-embeddings</guid>
      <pubDate>Mon, 30 Sep 2019 13:29:18 GMT</pubDate>
    </item>
    <item>
      <title>sklearn：获取点到最近聚类的距离</title>
      <link>https://stackoverflow.com/questions/44041347/sklearn-get-distance-from-point-to-nearest-cluster</link>
      <description><![CDATA[我正在使用 DBSCAN 之类的聚类算法。
它返回一个名为 -1 的“聚类”，这些点不属于任何聚类。对于这些点，我想确定它与最近聚类之间的距离，以获得类似于该点异常程度的指标。这可能吗？或者这种指标还有其他选择吗？]]></description>
      <guid>https://stackoverflow.com/questions/44041347/sklearn-get-distance-from-point-to-nearest-cluster</guid>
      <pubDate>Thu, 18 May 2017 07:31:36 GMT</pubDate>
    </item>
    <item>
      <title>如何在spark集群环境下高效训练word2vec模型？</title>
      <link>https://stackoverflow.com/questions/34377742/how-to-train-word2vec-model-efficiently-in-the-spark-cluster-environment</link>
      <description><![CDATA[我想在我的 Spark 集群上训练 10G 新闻语料的 word2vec 模型。
以下是我的 spark 集群的配置：

一个 Master 和 4 个 Worker
每个都有 80G 内存和 24 个核心

但是我发现使用 Spark Mllib 训练 Word2vec 并没有充分利用集群的资源。
例如：
ubuntu 中的 top 命令图片
如上图所示，只有一个 worker 使用了 100% 的 cpu，其他三个 worker 未使用（因此不粘贴它们的图片）并且我刚刚如何训练一个关于 2G 新闻语料的 word2vec 模型，大约需要 6 小时，所以我想知道如何更有效地训练模型？提前谢谢大家:)

UPDATE1：以下命令是我在 spark-shell 中使用的

如何启动 spark-shell
spark-shell \
--master spark://ip:7077 \
--executor-memory 70G \
--driver-memory 70G \
--conf spark.akka.frameSize=2000 \
--conf spark.driver.maxResultSize=0 \
--conf spark.default.parallelism=180
以下命令是我在spark-shell中训练word2vec模型时使用的：
//导入相关包
import org.apache.spark._
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
//读取约10G newsdata 语料库
val newsdata = sc.textFile(&quot;hdfs://ip:9000/user/bd/newsdata/*&quot;,600).map(line =&gt; line.split(&quot; &quot;).toSeq)
//配置word2vec参数
val word2vec = new Word2Vec()
word2vec.setMinCount(10)
word2vec.setNumIterations(10)
word2vec.setVectorSize(200)
//训练模型
val model = word2vec.fit(newsdata)


更新2：
我已经训练模型大约24小时了，但还没有完成。集群运行如下：
只有一个worker使用了100%的cpu，其他三个worker没有像以前一样被使用。]]></description>
      <guid>https://stackoverflow.com/questions/34377742/how-to-train-word2vec-model-efficiently-in-the-spark-cluster-environment</guid>
      <pubDate>Sun, 20 Dec 2015 03:40:58 GMT</pubDate>
    </item>
    <item>
      <title>免费提供的真实公共数据</title>
      <link>https://stackoverflow.com/questions/24962111/freely-available-real-public-data</link>
      <description><![CDATA[注意：我不是在寻找样本数据。
不同域中向公众免费公开的真实数据集：
例如：

FCM 的财务报告。
http://www.cftc.gov/MarketReports/FinancialDataforFCMs/HistoricalFCMReports/index.htm
YouTube 数据：（频道的受欢迎程度指标和统计数据）
https://developers.google.com/youtube/analytics/

如果有更多此类数据，请分享。 
可能与以下内容或其他任何可能有用的内容相关。
可能涉及医疗领域、药房、药品消费。
不同城市、道路等的交通、事故、伤亡。
不同地区的妇女安全指标。
食品/饮料消费、价格。
根据地区/公寓的垃圾收集量、洗手间。
有多少孤儿院以及他们获得了多少资助。
一个城市有多少个残疾人停车位等。
如果您认为这个问题不适合这种类型的平台，我将非常感激您为我推荐一个更好的论坛。]]></description>
      <guid>https://stackoverflow.com/questions/24962111/freely-available-real-public-data</guid>
      <pubDate>Fri, 25 Jul 2014 18:18:32 GMT</pubDate>
    </item>
    </channel>
</rss>