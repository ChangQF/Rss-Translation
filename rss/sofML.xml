<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 05 Dec 2024 12:35:58 GMT</lastBuildDate>
    <item>
      <title>根据二元变量制定信用评级（最好使用 R 语言）</title>
      <link>https://stackoverflow.com/questions/79254610/develop-credit-rating-based-on-binary-variable-in-r-preferably</link>
      <description><![CDATA[我在 R 中处理信用数据。我有一个贷款数据集，其中包含借款人和信用特定变量以及二元指标违约/不违约。第一步，我进行 logit 并获取违约概率 (PD)。接下来我想做什么：

根据这些 PD 建立类似信用评级模型的东西。例如，类别 1：PD&lt;0.2，类别 2：0.2&lt;PD&lt;0.4 等。
此类别数应在计量经济学/统计上达到最优。
找到类别之间的这些阈值。

我该如何实现？
我认为基于 PD 和其他一些特征，我可以进行聚类分析，然后将这些聚类用作有序 logit 中的因变量或类似的东西。但我害怕相关性和其他东西。我也对证明最佳聚类数感到困惑。我该如何选择？
如果您能提供 R 代码思路就更好了，但不一定
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79254610/develop-credit-rating-based-on-binary-variable-in-r-preferably</guid>
      <pubDate>Thu, 05 Dec 2024 12:05:53 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ML/DL 模型找到两个矩阵之间的映射，其中矩阵一的形状为 [B, n, 特征]，另一个矩阵的形状为 [B, m, 特征]</title>
      <link>https://stackoverflow.com/questions/79254384/how-to-find-mapping-between-two-matrices-where-matrix-one-is-of-shape-b-n-fea</link>
      <description><![CDATA[我正在研究一个问题，需要将一个矩阵映射到另一个矩阵。考虑 X 和 Y，如下所示
X 的形状为 [batch_size, seq_len_1, feature_dim]
Y 的形状为 [batch_size, seq_len_2, feature_dim]
这里，seq_len_1 和 seq_len_2 可以在不同的样本中变化，我需要在这些矩阵之间执行序列到序列的映射。我想训练一个 ml/dl 模型，该模型将矩阵 X 作为输入，并将矩阵 Y 作为输出。
这些矩阵包含数值，准确地说，将第一个矩阵视为一种语言的梅尔频谱图表示，将第二个矩阵视为另一种语言的梅尔频谱图表示。
到目前为止，我已经训练了一个简单的神经网络，它将在给定输入中的矩阵 X 的情况下在输出中给出一个矩阵，我只是在 output_matrix 和 Y 之间找到了 mse 损失。]]></description>
      <guid>https://stackoverflow.com/questions/79254384/how-to-find-mapping-between-two-matrices-where-matrix-one-is-of-shape-b-n-fea</guid>
      <pubDate>Thu, 05 Dec 2024 11:02:10 GMT</pubDate>
    </item>
    <item>
      <title>Azure ML 实验指标限制：无法在实验比较中查看所有记录的指标</title>
      <link>https://stackoverflow.com/questions/79254263/azure-ml-experiment-metric-limit-unable-to-see-all-logged-metrics-in-experiment</link>
      <description><![CDATA[我正在开展一项实验，在 Azure 机器学习 (Azure ML) 中记录 12 种不同机器学习模型的指标。每个模型都有 680 个指标。这是因为我有一个包含 150 多个类别的多分类模型，并且每个类别都有准确率、召回率、f1 分数和召回率值。
以下是我记录指标的方式：
# 使用 mlflow 记录指标
mlflow.log_metric(f&#39;{metric_name}&#39;, metric_value)

这些指标显示在“运行详细信息”选项卡中，当我查看单个运行时，我可以查看所有指标。但是，当我切换到实验级别来比较模型时，我只能在所有运行中看到最多 250 个指标。
运行指标

实验指标

这似乎是 Azure ML 的一个限制，但我不确定它是否是服务问题、我的配置问题，或者 Azure ML 或 MLflow 中是否存在关于可在实验级别显示的指标数量的已知限制。
我尝试过的方法：

验证所有指标是否在运行详细信息中正确记录。
确保指标在 mlflow.log_metric() 调用中可用。
检查了 Azure ML 文档和配置设置，但找不到任何有关指标限制的引用。

问题：

在 Azure ML 的实验比较视图中可显示的指标数量是否存在已知限制？
是否有任何配置更改或解决方法来克服此限制？
我是否应该考虑对大量指标使用不同的日志记录方法？
]]></description>
      <guid>https://stackoverflow.com/questions/79254263/azure-ml-experiment-metric-limit-unable-to-see-all-logged-metrics-in-experiment</guid>
      <pubDate>Thu, 05 Dec 2024 10:26:57 GMT</pubDate>
    </item>
    <item>
      <title>为什么通用 Windows 应用程序中的调试器显示 modelgen 为空</title>
      <link>https://stackoverflow.com/questions/79254069/why-is-the-debugger-in-universal-windows-app-showing-modelgen-is-null</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79254069/why-is-the-debugger-in-universal-windows-app-showing-modelgen-is-null</guid>
      <pubDate>Thu, 05 Dec 2024 09:33:13 GMT</pubDate>
    </item>
    <item>
      <title>torch.utils.data.random_split 中的随机性与基于 numpy 的拆分</title>
      <link>https://stackoverflow.com/questions/79254043/randomness-in-torch-utils-data-random-split-vs-numpy-based-splitting</link>
      <description><![CDATA[我正在使用 PyTorch Lightning 基础架构在 PyTorch 中实现卷积神经网络。
我的合作者要求我使用 kfold 交叉验证来评估跨数据集性能（是的，我知道数据泄漏和过度拟合）。
我的问题是基于 numpy 的随机分割产生的性能明显比 torch.utils.data.random_split 差。我使用 train/val/test 方法的测试性能约为 AUC 0.95，使用 8:1:1 分割。

使用手动 numpy 分割的 5 倍 CV 产生 0.88 的 AUC
使用自动 pytorch random_split 的 5 倍 CV 产生 0.95 的 AUC

用于计算 AUC 的样本量约为 10,000，因此两种方法之间的 AUC 差异很大。
我的问题是为什么 pytorch random_split 函数返回更好的结果？在我的 numpy 实现中，属于给定折叠的概率在观察之间是独立的。 random_split 函数是否也是如此，或者它是否使用随机样本间隔？
Kfold 实验
当我使用 numpy 对数据集进行子集化时，我获得的性能约为 AUC=0.88。这是我的代码：
fold = 1
indices = np.arange(X.shape[0])
sampler = np.random.permutation(indices) % 5 # 5 倍 CV
X_train, X_test = X[(sampler!=fold)], X[(sampler==fold)]
y_train, y_test = y[(sampler!=fold], y[(sampler==fold)]

dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1])
test_dataset = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))

当我对数据集进行子集化时在 pytorch 中使用 random_split 我获得了大约 AUC=0.95 的性能。这是我的代码：
fold = 1
dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))
five_folds = torch.utils.data.random_split(dataset, [1/5]*5) # 5 倍 CV
test_dataset = five_folds[fold]
train_dataset = torch.utils.data.ConcatDataset(
[five_folds[i] for i in range(5) if i != fold]
)
train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [0.9, 0.1])
]]></description>
      <guid>https://stackoverflow.com/questions/79254043/randomness-in-torch-utils-data-random-split-vs-numpy-based-splitting</guid>
      <pubDate>Thu, 05 Dec 2024 09:28:01 GMT</pubDate>
    </item>
    <item>
      <title>文本类别预测 [关闭]</title>
      <link>https://stackoverflow.com/questions/79252250/text-category-prediction</link>
      <description><![CDATA[我需要创建某种方法，以便预测文本的类别。
现在我正在使用这样的 PHP-ML：
 $texts = $queriesService-&gt;getTexts();

foreach($texts as $key =&gt; $text){
$string = preg_replace(&#39;/\s+/&#39;, &#39; &#39;, strip_tags($text[&#39;text&#39;]));
$string = str_replace(&#39;&quot;&#39;, &#39;&#39;, $string);
$samples[] = $string;
$labels[] = $text[&#39;category&#39;]; 
}

$tokenize = new WordTokenizer();
$vectorizer = new TokenCountVectorizer($tokenize);

$vectorizer-&gt;fit($samples);
$vectorizer-&gt;transform($samples);

$transformer = new TfIdfTransformer($samples);
$transformer-&gt;transform($samples);

$classifier = new NaiveBayes();
$classifier-&gt;train($samples, $labels);

$testSamples = [
&#39;关于某些产品的示例文本&#39;,
&#39;这是关于糟糕的服务&#39;,
&#39;由于某种原因电子设备无法正常工作&#39;,
];

$vectorizer-&gt;transform($testSamples);
$transformer-&gt;transform($testSamples);

$predictions = $classifier-&gt;predict($testSamples);

这样运行良好，但问题是 - 它占用了大量内存。所讨论的文本各不相同，从 5 个单词到 200 个单词，大约有 100k 个。
即使训练分类器并将其保存到文件 - 文件大小超过 10GB，因此读取它需要大量内存。有没有更好的方法来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/79252250/text-category-prediction</guid>
      <pubDate>Wed, 04 Dec 2024 17:55:51 GMT</pubDate>
    </item>
    <item>
      <title>Python 错误：rv_generic.interval() 缺少 1 个必需的位置参数：“confidence”</title>
      <link>https://stackoverflow.com/questions/79250549/python-error-rv-generic-interval-missing-1-required-positional-argument-con</link>
      <description><![CDATA[我一直在尝试运行下面的代码来使用 t 分布计算上限和下限置信区间，但它一直在主题中抛出错误。代码片段如下：
def trans_threshold(Day):
Tran_Cnt=Tran_Cnt_DF[[&#39;Sample&#39;,Day]].dropna()
Tran_Cnt=Tran_Cnt.astype({&#39;Sample&#39;:&#39;str&#39;})
Tran_Cnt.dtypes
#通过 IQR 查找 Materiality 中的异常值
X_Tran = Tran_Cnt.drop(&#39;Sample&#39;, axis=1)
Tran_arr1 = X_Tran.values
#查找第一个四分位数
Tran_q1= np.quantile(Tran_arr1, 0.25)
# 查找第三个四分位数
Tran_q3 = np.quantile(Tran_arr1, 0.75)
# 查找 iqr 区域
Tran_iqr = Tran_q3-Tran_q1
# 查找上限和下限异常值
Tran_upper_bound = Tran_q3+(1.5*Tran_iqr)
Tran_lower_bound = Tran_q1-(1.5*Tran_iqr)
# 删除异常值
Tran_arr2 = Tran_arr1[(Tran_arr1 &gt;= Tran_lower_bound) &amp; (Tran_arr1 &lt;= Tran_upper_bound)]
# 使用 t 分布确定实质性限值
Tran_Threshold_mat=st.t.interval(alpha=0.99999999999, df=len(Tran_arr2-1),
loc=np.mean(Tran_arr2),
scale=st.sem(Tran_arr2))
return Tran_Threshold_mat

trn_lim_FullFeed_Mon = trans_threshold(Day) 

-------------------------------------------------------------------------------------------
TypeError Traceback (最近一次调用最后一次)
Cell In[106]，第 19 行
17 Tran_arr2 = Tran_arr1[(Tran_arr1 &gt;= Tran_lower_bound) &amp; (Tran_arr1 &lt;= Tran_upper_bound)]
18 #使用 t 分布计算实质性限值
---&gt; 19 Tran_Threshold_mat=st.t.interval(alpha=0.99999999999, df=len(Tran_arr2-1),
20 loc=np.mean(Tran_arr2),
21 scale=st.sem(Tran_arr2))

TypeError: rv_generic.interval() 缺少 1 个必需的位置参数：&#39;confidence&#39;

问题似乎出在下面的代码片段上。但是，我提供了计算置信区间所需的所有参数，包括自由度，但仍然出现此错误。我哪里做错了，需要做什么？
Tran_Threshold_mat=st.t.interval(alpha=0.99999999999, df=len(Tran_arr2-1),
loc=np.mean(Tran_arr2),
scale=st.sem(Tran_arr2))

此外，Tran_arr2 列表如下所示：
array([12617., 12000., 1123., 537., 8605., 4365., 11292., 12231.,
7640., 9583., 9257., 13864., 14682., 11744., 10501., 8694.,
5327., 10066., 13022., 11092., 7444., 11658., 14920., 12849.,
14681., 5719., 11029., 3814., 14703., 5593., 9772., 8851.,
9551., 15975., 6532., 13827., 8547.])

因此，直到代码块的最后一个使用 t 分布估计置信区间的代码之前，都没有问题。
我使用了以下包：
import pandas as pd
import numpy 作为 np
导入 scipy.stats 作为 st
导入 matplotlib.pyplot 作为 plt
导入 matplotlib.ticker 作为 tkr
导入 matplotlib.scale 作为 mscale
从 matplotlib.ticker 导入 FixedLocator、NullFormatter
pd.options.display.float_format = &#39;{:.0f}&#39;.format
pd.options.mode.chained_assignment = None
]]></description>
      <guid>https://stackoverflow.com/questions/79250549/python-error-rv-generic-interval-missing-1-required-positional-argument-con</guid>
      <pubDate>Wed, 04 Dec 2024 09:32:13 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“anomalib.engine”的模块</title>
      <link>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</link>
      <description><![CDATA[# 导入所需模块

从 anomalib.data 导入 MVTec
从 anomalib.models 导入 Patchcore
从 anomalib.engine 导入 Engine

错误：
ModuleNotFoundError：没有名为“anomalib.engine”的模块

我正在尝试运行它......已按照库安装并看到
https://anomalib.readthedocs.io/en/latest/markdown/get_started/anomalib.html
我认为这要么是因为引擎已被修改，要么已被库删除......
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</guid>
      <pubDate>Sat, 03 Feb 2024 05:25:02 GMT</pubDate>
    </item>
    <item>
      <title>处理不平衡问题后，数据严重倾斜，准确率会下降</title>
      <link>https://stackoverflow.com/questions/72715345/accuracy-degrade-with-highly-skewed-data-after-handling-imbalance-problem</link>
      <description><![CDATA[在对数据进行预处理（如缺失值替换和异常值检测）后，我使用 WEKA 随机化和移除百分比过滤器对数据进行分区。我的数据集是一个高度倾斜的数据集，不平衡比率为 6:1，分别对应负类和正类。如果我使用朴素贝叶斯分类器对数据进行分类而不处理类别不平衡问题，我的准确率将达到 83%，召回率为 0.623。但是，如果我使用监督实例 - 重新采样或监督实例 - 扩展子采样过滤器处理类别不平衡（平衡 1:1 后），然后应用朴素贝叶斯分类准确率将下降 77%，召回率为 0.456。
为什么处理类别不平衡率时准确率会下降？]]></description>
      <guid>https://stackoverflow.com/questions/72715345/accuracy-degrade-with-highly-skewed-data-after-handling-imbalance-problem</guid>
      <pubDate>Wed, 22 Jun 2022 12:18:22 GMT</pubDate>
    </item>
    <item>
      <title>sklearn中RepeatedStratifiedKFold和StratifiedKFold有什么区别？</title>
      <link>https://stackoverflow.com/questions/71181291/what-is-the-difference-between-repeatedstratifiedkfold-and-stratifiedkfold-in-sk</link>
      <description><![CDATA[我尝试阅读 RepeatedStratifiedKFold 和 StratifiedKFold 的文档，但无法分辨这两种方法之间的区别，除了 RepeatedStratifiedKFold 重复 StratifiedKFold n 次，每次重复的随机化方式不同。
我的问题是：这两种方法是否返回相同的结果？在执行 GridSearchCV 时，我应该使用哪一个方法来分割不平衡的数据集，选择该方法的理由是什么？]]></description>
      <guid>https://stackoverflow.com/questions/71181291/what-is-the-difference-between-repeatedstratifiedkfold-and-stratifiedkfold-in-sk</guid>
      <pubDate>Sat, 19 Feb 2022 00:22:41 GMT</pubDate>
    </item>
    <item>
      <title>Cartpole-v0 的 PyTorch PPO 实现陷入局部最优</title>
      <link>https://stackoverflow.com/questions/70191012/pytorch-ppo-implementation-for-cartpole-v0-getting-stuck-in-local-optima</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/70191012/pytorch-ppo-implementation-for-cartpole-v0-getting-stuck-in-local-optima</guid>
      <pubDate>Wed, 01 Dec 2021 20:48:16 GMT</pubDate>
    </item>
    <item>
      <title>如何纠正训练过程中不稳定的损失和准确率？</title>
      <link>https://stackoverflow.com/questions/55894132/how-to-correct-unstable-loss-and-accuracy-during-training</link>
      <description><![CDATA[我正在使用 TensorFlow 中的新 keras API 开发一个小型二元分类项目。该问题是几年前在 Kaggle.com 上发布的希格斯玻色子挑战赛的简化版本。数据集形状为 2000x14，其中每行的前 13 个元素构成输入向量，第 14 个元素是相应的标签。以下是所述数据集的示例：
86.043,52.881,61.231,95.475,0.273,77.169,-0.015,1.856,32.636,202.068, 2.432,-0.419,0.0,0
138.149,69.197,58.607,129.848,0.941,120.276,3.811,1.886,71.435,384.916,2.447,1.408,0.0,1
137.457,3.018,74.670,81.705,5.954,775.772,-8.854,2.625,1.942,157.231,1.193,0.873,0.824,1

我曾尝试根据在线找到的二元分类问题示例来构建各种模型，但在训练模型时遇到了困难。在训练过程中，损失有时会在同一时期内增加，导致学习不稳定。准确率在 70% 左右达到稳定状态。我尝试过改变学习率和其他超参数，但无济于事。相比之下，我已经硬编码了一个完全连接的前馈神经网络，在同样的问题上，它的准确率达到了 80-85% 左右。
这是我当前的模型：
import tensorflow as tf
from tensorflow.python.keras.layers.core import Dense
import numpy as np
import pandas as pd

def normalize(array):
return array/np.linalg.norm(array, ord=2, axis=1, keepdims=True)

x_train = pd.read_csv(&#39;data/labeled.csv&#39;, sep=&#39;\s+&#39;).iloc[:1800, :-1].values
y_train = pd.read_csv(&#39;data/labeled.csv&#39;, sep=&#39;\s+&#39;).iloc[:1800, -1:].values

x_test = pd.read_csv(&#39;data/labeled.csv&#39;, sep=&#39;\s+&#39;).iloc[1800:, :-1].values
y_test = pd.read_csv(&#39;data/labeled.csv&#39;, sep=&#39;\s+&#39;).iloc[1800:, -1:].values

x_train = normalize(x_train)
x_test = normalize(x_test)

model = tf.keras.Sequential()
model.add(Dense(9, input_dim=13,activation=tf.nn.sigmoid)
model.add(Dense(6,activation=tf.nn.sigmoid))
model.add(Dense(1,activation=tf.nn.sigmoid))

model.compile(optimizer=&#39;adam&#39;,
loss=&#39;binary_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

model.fit(x_train, y_train, epochs=50)
model.evaluate(x_test, y_test)

如前所述，一些 epoch 的起始准确度比结束准确度高，导致学习不稳定。
 32/1800 [................................] - ETA：0s - 损失：0.6830 - 精度：0.5938
1152/1800 [==================&gt;...........] - ETA：0s - 损失：0.6175 - 精度：0.6727
1800/1800 [==============================] - 0s 52us/step - 损失：0.6098 - 精度：0.6861
Epoch 54/250

32/1800 [..............................] - ETA：0s - 损失：0.5195 - 精度：0.8125
1376/1800 [======================&gt;........] - ETA：0s - 损失：0.6224 - 精度：0.6672
1800/1800 [==============================] - 0s 43us/step - 损失：0.6091 - 精度：0.6850
Epoch 55/250

在如此简单的学习中，这些振荡的原因可能是什么模型？

编辑：
我遵循了评论中的一些建议，并相应地修改了模型。现在看起来更像这样：
model = tf.keras.Sequential()
model.add(Dense(250, input_dim=13,activation=tf.nn.relu))
model.add(Dropout(0.4))
model.add(Dense(200,activation=tf.nn.relu))
model.add(Dropout(0.4))
model.add(Dense(100,activation=tf.nn.relu))
model.add(Dropout(0.3))
model.add(Dense(50,activation=tf.nn.relu))
model.add(Dense(1,activation=tf.nn.sigmoid))

model.compile(optimizer=&#39;adadelta&#39;,
loss=&#39;binary_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/55894132/how-to-correct-unstable-loss-and-accuracy-during-training</guid>
      <pubDate>Sun, 28 Apr 2019 20:02:19 GMT</pubDate>
    </item>
    <item>
      <title>Weka：不兼容的训练和测试集</title>
      <link>https://stackoverflow.com/questions/50246992/weka-incompatible-training-and-test-sets</link>
      <description><![CDATA[我正在 Weka 上研究一个 分类 问题。我将 arff 文件作为我的训练数据，并从 数据库 中获取我的测试数据。但它们不兼容。在 Weka 工具中，我可以使用 InputMappedClassifier 并解决问题。但我无法在 Java 代码中实现它。请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/50246992/weka-incompatible-training-and-test-sets</guid>
      <pubDate>Wed, 09 May 2018 06:40:00 GMT</pubDate>
    </item>
    <item>
      <title>无法通过命令行使用 Weka 进行分类</title>
      <link>https://stackoverflow.com/questions/35697652/unable-to-classify-using-weka-from-command-line</link>
      <description><![CDATA[我正在使用 weka 3.6.13 并尝试使用模型对数据进行分类：
java -cp weka-stable-3.6.13.jar weka.classifiers.Evaluation weka.classifiers.trees.RandomForest -l Parking.model -t Data_features_class_ques-2.arff 

java.lang.Exception：训练和测试集不兼容

虽然当我们使用 GUI 时模型可以工作，通过 Explorer-&gt;Claasify-&gt;Supplied 测试集并加载 arff 文件-&gt;右键单击结果列表并加载模型-&gt;再次右键单击-&gt;在当前数据集上重新评估模型...
任何指示请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/35697652/unable-to-classify-using-weka-from-command-line</guid>
      <pubDate>Mon, 29 Feb 2016 10:50:09 GMT</pubDate>
    </item>
    <item>
      <title>weka 中的数据集不平衡？不起作用</title>
      <link>https://stackoverflow.com/questions/23426582/imbalanced-dataset-in-weka-does-not-work</link>
      <description><![CDATA[我有一个 239 个正数据集和一个 32 个负数据集，因为它是与癌症相关的数据，所以我们只有很少的负数据集。现在，当应用分类时，不平衡的数据集肯定会因为数量巨大而过于偏向正数据集。所以我尝试在 weka 中应用 SMOTE。我也尝试了各种百分比和最近邻居。令我惊讶的是，不是负类增加了几个实例，而是正类进一步增加，使得不平衡的数据集过于偏向。可以做些什么来克服这个问题。如果可用，请给我一些其他方法？
对于初步研究，我们使用了 LIBSVM 和 RBF 作为分类器 ]]></description>
      <guid>https://stackoverflow.com/questions/23426582/imbalanced-dataset-in-weka-does-not-work</guid>
      <pubDate>Fri, 02 May 2014 11:03:34 GMT</pubDate>
    </item>
    </channel>
</rss>