<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 12 Dec 2024 12:36:46 GMT</lastBuildDate>
    <item>
      <title>无监督递归特征消除 - R</title>
      <link>https://stackoverflow.com/questions/79275101/unsupervised-recursive-feature-elimination-r</link>
      <description><![CDATA[我想知道是否有人能帮忙提供一些无监督递归特征消除 (uRFE) 的 R 代码？
简而言之，我运行了一个无监督的 SOM，用 k-medoids 进行分区，但也想使用轮廓分数来识别 uRFE。
所有帮助都将不胜感激。下面附上了一些虚拟代码。
library(aweSOM)

#IRIS 数据

full.data &lt;- iris
train.data &lt;- full.data[, c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;)]
train.data &lt;- scale(train.data)

#SOM 初始化 + MAP

set.seed(1465)
init &lt;- somInit(train.data, 4, 4)
iris.som &lt;- kohonen::som(train.data, grid = kohonen::somgrid(4, 4, &quot;hexagonal&quot;), 
rlen = 100, alpha = c(0.05, 0.01), radius = c(2.65,-2.65), 
dist.fcts = &quot;sumofsquares&quot;, init = init)

#SOM CLUSTERS

superclust_pam &lt;- cluster::pam(iris.som$codes[[1]], 3)
superclasses_pam &lt;- superclust_pam$clustering


再次感谢您对轮廓的 uRFE 的任何帮助。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/79275101/unsupervised-recursive-feature-elimination-r</guid>
      <pubDate>Thu, 12 Dec 2024 12:27:01 GMT</pubDate>
    </item>
    <item>
      <title>将机器学习模型部署在一台服务器上（而不是两台）可增加其响应时间</title>
      <link>https://stackoverflow.com/questions/79275068/increased-response-time-of-a-machine-learning-model-when-deploying-it-on-a-singl</link>
      <description><![CDATA[我最近在两台并行服务器上部署了一个利用 GPU 的机器学习模型。使用负载平衡器在它们之间平衡请求负载。为了减少资源使用量，我决定切换到单服务器设置，将所有请求直接路由到其中一台服务器。该服务器有一个 Nvidia tesla t4 GPU。该模型仅使用了 15GB 容量中的 1.2GB 左右。
更改后，我观察到以下情况：
CPU 利用率：几乎没有变化。
GPU 利用率：如预期一样翻倍，但未超过 GPU 的容量。
平均 GPU 利用率百分比从 7.5% 上升到 16.5%
最大 GPU 利用率百分比从 38% 上升到 48%
第 90 个百分位的 GPU 利用率百分比从 19% 上升到 34%
模型响应时间：平均增加了约 10% 到 15%。
尽管有这些观察结果，但我无法确定模型响应时间增加的确切原因。如果有人有见解或建议，我将非常感谢您的帮助。提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/79275068/increased-response-time-of-a-machine-learning-model-when-deploying-it-on-a-singl</guid>
      <pubDate>Thu, 12 Dec 2024 12:15:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 peft 进行微调时出错</title>
      <link>https://stackoverflow.com/questions/79274171/getting-error-while-fine-tuning-using-peft</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79274171/getting-error-while-fine-tuning-using-peft</guid>
      <pubDate>Thu, 12 Dec 2024 07:31:09 GMT</pubDate>
    </item>
    <item>
      <title>使用预先训练的 CNN 进行螃蟹性别分类 [关闭]</title>
      <link>https://stackoverflow.com/questions/79273773/using-pre-trained-cnn-for-crab-gender-classification</link>
      <description><![CDATA[我可以使用预先训练好的、已针对对象分类进行微调的 cnn 模型，然后添加我自己的螃蟹数据集吗？
示例：一个经过训练可以对花朵等对象进行分类的 cnn 模型，该模型是在 5 个类别上进行训练的，对吗？现在我想添加螃蟹图像，这样它就会知道它是螃蟹。我需要重新训练整个模型吗，还是可以只添加我的数据集并训练模型？
我现在正在进行螃蟹性别分类。但我的顾问想在我的系统演示中添加，如果用户在螃蟹旁边添加图像，它将识别出不是螃蟹。我解释说这是不可能的，因为模型是针对性别分类而不是对象分类进行训练的（我使用 efficientnetb7 来训练我的螃蟹图像）。
我怎样才能实现他们的要求？
我还没有尝试任何东西，因为 google colab 很贵，而且我还在读学士学位]]></description>
      <guid>https://stackoverflow.com/questions/79273773/using-pre-trained-cnn-for-crab-gender-classification</guid>
      <pubDate>Thu, 12 Dec 2024 03:46:31 GMT</pubDate>
    </item>
    <item>
      <title>根据线条粗细、文本大小和背景颜色对图像进行分类</title>
      <link>https://stackoverflow.com/questions/79273650/classify-images-based-on-line-thickness-text-size-and-background-color</link>
      <description><![CDATA[我正在尝试找到一种方法，根据图像是否适合印在衣服上对图像进行分类，最好使用 Python 库。我主要想做的是找到文本大小和颜色以及一般的线条粗细和背景颜色 - 当背景颜色较暗时，需要大量墨水。如果图像的线条非常细，颜色较浅或为白色，墨水就会“侵入”这些线条，图像将无法识别。检测文本及其大小/线条粗细将是一个不错的开始。我有成千上万张“好”和“坏”图像，因此训练 AI/ML 模型是一种选择，但在我看来，图像分析库可能已经足够好了。欢迎所有选项。]]></description>
      <guid>https://stackoverflow.com/questions/79273650/classify-images-based-on-line-thickness-text-size-and-background-color</guid>
      <pubDate>Thu, 12 Dec 2024 02:14:00 GMT</pubDate>
    </item>
    <item>
      <title>Softmax 函数的哪种表示是正确的？[关闭]</title>
      <link>https://stackoverflow.com/questions/79272621/which-representation-of-the-softmax-function-is-correct</link>
      <description><![CDATA[我偶然发现了 Softmax 函数的公式：
Softmax 公式
但是，我发现了 Softmax 函数的两个非常不同的视觉表示，我不知道哪一个是正确的：

类似 S 形的曲线：随着输入的增长，一条曲线从 0 增加到 1。它看起来类似于 Sigmoid 函数。
类似 Sigmoid 的曲线

多类归一化输出：显示多个类的概率如何随不同输入而变化的图表，其中概率总和为 1。
多类归一化输出



第一张图（类似 Sigmoid 的曲线）是否正确表示了 Softmax 函数，还是仅描述特定条件下的一个组件？
Softmax 是否应始终表示为跨多个类的概率分布（如第二张图所示）图表）？

我正在寻找有关如何正确解释和表示 Softmax 函数的清晰度。]]></description>
      <guid>https://stackoverflow.com/questions/79272621/which-representation-of-the-softmax-function-is-correct</guid>
      <pubDate>Wed, 11 Dec 2024 17:19:50 GMT</pubDate>
    </item>
    <item>
      <title>图像分类的核心 ML 预测创建 ml 模型对不同的图像预测相同的结果</title>
      <link>https://stackoverflow.com/questions/79271499/core-ml-prediction-for-image-classification-create-ml-model-predict-same-result</link>
      <description><![CDATA[我正在探索 Apple Core ML 框架。
我使用 Create ML 应用创建了一个训练模型。图像分类用于识别图像是猫还是狗。我使用的数据集来自
https://www.kaggle.com/datasets/samuelcortinhas/cats-and-dogs-image-classification?resource=download
我通过提供训练数据从 create ml 创建了我的 .mlmodel 文件。
现在我在应用程序中进行预测，它给出了相同的目标和概率结果以及不同的图像（猫或狗，给出相同的结果 - [&quot;cats&quot;: 0.6281524444894766, &quot;dogs&quot;: 0.3718475555105234]
应用程序代码：
override func viewDidLoad() {
if let img = UIImage(named: &quot;dog_29.jpg&quot;) {
predictImage(image: img)
} else {
print(&quot;image not caught&quot;)
}
}

private func predictImage(image: UIImage) {

let inputImageSize: CGFloat = 299.0
let minLen = min(image.size.width, image.size.height)
let resizedImage = image.resize(to: CGSize(width: inputImageSize * image.size.width / minLen, height: inputImageSize * image.size.height / minLen))

guard let pixelBuffer = resizedImage.pixelBuffer() else {
fatalError()
}

do {
let config = MLModelConfiguration()
let model = try CatsAndDogs(configuration:config)
let result = try model.prediction(image: pixelBuffer)
print(result.target)
print(result.targetProbability)

print(result)
} catch {
print(&quot;image category error&quot;)
}

}

resize 和 getcvbuffer 的代码：
 func resize(to newSize: CGSize) -&gt; UIImage {
UIGraphicsBeginImageContextWithOptions(CGSize(width: newSize.width, height: newSize.height), true, 1.0)
self.draw(in: CGRect(x: 0, y: 0, width: newSize.width, height: newSize.height))
let resizedImage = UIGraphicsGetImageFromCurrentImageContext()!
UIGraphicsEndImageContext()

return resizedImage
}

func pixelBuffer() -&gt; CVPixelBuffer? {
let width = self.size.width
let height = self.size.height
let attrs = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue,
kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue] as CFDictionary
var pixelBuffer: CVPixelBuffer?
让 status = CVPixelBufferCreate(kCFAllocatorDefault,
Int(width),
Int(height),
kCVPixelFormatType_32ARGB,
attrs,
&amp;pixelBuffer)

保护让 resultPixelBuffer = pixelBuffer, status == kCVReturnSuccess else {
return nil
}

CVPixelBufferLockBaseAddress(resultPixelBuffer, CVPixelBufferLockFlags(rawValue: 0))
让 pixelData = CVPixelBufferGetBaseAddress(resultPixelBuffer)

让 rgbColorSpace = CGColorSpaceCreateDeviceRGB()
保护让 context = CGContext(data: pixelData,
width: Int(width),
height: Int(height),
bitsPerComponent: 8,
bytesPerRow: CVPixelBufferGetBytesPerRow(resultPixelBuffer),
space: rgbColorSpace,
bitmapInfo: CGImageAlphaInfo.no​​neSkipFirst.rawValue) else {
return nil
}

context.translateBy(x: 0, y: height)
context.scaleBy(x: 1.0, y: -1.0)

UIGraphicsPushContext(context)
self.draw(in: CGRect(x: 0, y: 0, width: width, height: height))
UIGraphicsPopContext()
CVPixelBufferUnlockBaseAddress(resultPixelBuffer, CVPixelBufferLockFlags(rawValue: 0))

return resultPixelBuffer
}

我也尝试了网络上可用的各种调整大小和 getcvbuffer 的方法，但结果都一样。
我尝试了数据集链接中测试文件夹中的猫和狗的不同图像。结果仍然相同。
为什么预测不正确？]]></description>
      <guid>https://stackoverflow.com/questions/79271499/core-ml-prediction-for-image-classification-create-ml-model-predict-same-result</guid>
      <pubDate>Wed, 11 Dec 2024 11:16:12 GMT</pubDate>
    </item>
    <item>
      <title>我实现的 KMeans 的结果并不一致</title>
      <link>https://stackoverflow.com/questions/79271387/the-result-of-my-implementation-of-kmeans-is-not-consistent</link>
      <description><![CDATA[我正在尝试实现一个简化版的 KMeans 聚类，但不知何故，结果有时不一致
import numpy as np
import random

import matplotlib.pyplot as plt

class KMeans:
def __init__(
self,
n_clusters,
max_iter
):
self.n_clusters = n_clusters
self.max_iter = max_iter

def _get_distance(self, x, cluster_location):
&quot;&quot;&quot;计算每个点到聚类中心的欧几里得距离
&quot;&quot;&quot;
return np.linalg.norm(x[:,np.newaxis,:]-cluster_location, axis = 2)

def fit(self, x:np.ndarray) -&gt;无：
“”k 均值聚类步骤
1. 启动聚类位置
2. 计算每个点到聚类点的距离
3. 将每个点分配给一个聚类
4. 使用相关点的平均值更新聚类位置
5. 重复 2-4 直到收敛或达到 max_iter

参数：
x (_type_)：_description_

返回：
_type_：_description_
“”
self.x = x
data_dim = x.shape[1]

# 1. 初始化簇位置
cluster_locations = np.random.uniform(x.min(), x.max(), size=(self.n_clusters,data_dim))
# print(&quot;initial:\n&quot;,cluster_locations)

for _ in range(self.max_iter):
# 2. 计算每个点到簇点的距离
distances = self._get_distance(x, cluster_locations)

# 3. 将每个点分配给一个簇
clusters = np.argmin(distances, axis=1)

# 4. 使用关联点的平均值更新簇位置
for cluster in range(self.n_clusters):
# 检查簇中是否有任何点
cluster_mask = clusters == cluster
if np.any(cluster_mask):
cluster_locations[cluster] = np.mean(x[cluster_mask], axis=0)
else:
# 如果簇为空，则用随机点重新初始化
cluster_locations[cluster] = x[np.random.randint(x.shape[0])] 

self.cluster_locations = cluster_locations
self.clusters = clusters
return None

def visualize(self, data, clusters):
_, ax = plt.subplots(1,1,figsize=(5,5))

cluster_color = [(random.random(),random.random(),random.random()) for _ in range(self.n_clusters)]

for cluster in range(self.n_clusters):
to_plot = data[np.where(clusters == cluster)[0]]
ax.scatter(to_plot[:,0], to_plot[:,1], color=cluster_color[cluster])
ax.scatter(self.cluster_locations[:,0], self.cluster_locations[:,1], marker=&quot;x&quot;, color=&#39;r&#39;, s=30)

plt.show()

这是我使用它的方式
from sklearn.datasets import make_blobs

random_state = 42
n_samples = 100

x, _ = make_blobs(n_samples=n_samples, random_state=random_state)
my_kmeans = KMeans(3, 50)
my_kmeans.fit(x)
my_kmeans.visualize(x, my_kmeans.clusters)

大多数时候，它都会给我一个合理的输出，像这样

但每运行几次，它就会给我类似这样的结果

我是否遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79271387/the-result-of-my-implementation-of-kmeans-is-not-consistent</guid>
      <pubDate>Wed, 11 Dec 2024 10:41:28 GMT</pubDate>
    </item>
    <item>
      <title>无法访问自由变量“fig”，因为它与封闭范围内的值没有关联</title>
      <link>https://stackoverflow.com/questions/79270292/cannot-access-free-variable-fig-where-it-is-not-associated-with-a-value-in-enc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79270292/cannot-access-free-variable-fig-where-it-is-not-associated-with-a-value-in-enc</guid>
      <pubDate>Wed, 11 Dec 2024 02:04:10 GMT</pubDate>
    </item>
    <item>
      <title>为什么预先训练的 Swin Transformer 编码器在 TPU 上失败但在 Colab 中的 CPU 上可以运行？</title>
      <link>https://stackoverflow.com/questions/79244294/why-does-pre-trained-swin-transformer-encoder-fail-on-tpu-but-works-on-cpu-in-co</link>
      <description><![CDATA[我正在处理图像分割任务，并尝试使用预先训练的 Swin Transformer Large (Swin-L) 编码器作为特征提取主干。代码在 Colab 中的 CPU 上完美运行。但是，当切换到 TPU 时，它会抛出如下所示的错误。
代码：
from tensorflow.keras import layer, Model, Input
from tfswin import SwinTransformerLarge224

def load_swin_encoder(input_shape=(512, 512, 3)):
# 加载预训练的 Swin-L 模型
swin_encoder = SwinTransformerLarge224(include_top=False, weights=&#39;imagenet&#39;,
input_shape=input_shape)

# 冻结预训练层
for layer in swin_encoder.layers:
layer.trainable = False

# 从四个阶段提取输出
stage_outputs = [
swin_encoder.get_layer(&#39;normalize&#39;).output, # 从 0 阶段输出
swin_encoder.get_layer(&#39;layers.0&#39;).output, # 第一阶段的输出
swin_encoder.get_layer(&#39;layers.1&#39;).output, # 第二阶段的输出
swin_encoder.get_layer(&#39;layers.2&#39;).output, # 第三阶段的输出
swin_encoder.get_layer(&#39;layers.3&#39;).output, # 第四阶段的输出
]
return Model(swin_encoder.input, stage_outputs, name=&quot;SwinTransformerEncoder&quot;)

# 测试代码
encoder = load_swin_encoder(input_shape=(512, 512, 3))
dummy_input = tf.random.uniform((1, 512, 512, 3))
encoder_outputs =coder(dummy_input)

for i, output in enumerate(encoder_outputs):
print(f&quot;阶段 {i + 1} 输出形状：{output.shape}&quot;)


错误：
代码在 TPU 上抛出以下错误：
------------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-28-3cb122d32678&gt; 在 &lt;cell line: 2&gt;()
1 # 加载健全性检查
----&gt; 2 编码器 = load_swin_encoder(input_shape=(512, 512, 3))
3 dummy_input = tf.random.uniform((1, 512, 512, 3))
4 编码器输出 = 编码器(dummy_input)
5 

2 帧
/usr/local/lib/python3.10/dist-packages/keras/src/models/ functional.py in __init__(self, 输入, 输出, 名称, **kwargs)
117 for x in flat_inputs:
118 if not isinstance(x, backend.KerasTensor):
-&gt; 119 引发 ValueError(
120 “所有 `inputs` 值都必须是 KerasTensors。已收到：”
121 f“inputs={inputs} 包括无效值 {x}”

ValueError：所有 `inputs` 值都必须是 KerasTensors。已收到：inputs=KerasTensor(type_spec=TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=&#39;input_4&#39;), name=&#39;input_4&#39;, description=“由层 &#39;input_4&#39; 创建”) 包括无效值 KerasTensor(type_spec=TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=&#39;input_4&#39;), name=&#39;input_4&#39;, description=“由层创建” &#39;input_4&#39;&quot;) 类型为 &lt;class &#39;tf_keras.src.engine.keras_tensor.KerasTensor&#39;&gt;


问题：
为什么此代码在 Colab 中的 CPU 上有效，但在 TPU 上失败？我该如何修复此问题以使其与 TPU 执行兼容？
任何见解或指导都将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79244294/why-does-pre-trained-swin-transformer-encoder-fail-on-tpu-but-works-on-cpu-in-co</guid>
      <pubDate>Mon, 02 Dec 2024 13:35:57 GMT</pubDate>
    </item>
    <item>
      <title>pytorch torchvision.datasets.ImageFolder FileNotFoundError：未找到类 .ipynb_checkpoints 的有效文件</title>
      <link>https://stackoverflow.com/questions/68229246/pytorch-torchvision-datasets-imagefolder-filenotfounderror-found-no-valid-file</link>
      <description><![CDATA[尝试在 Colab 中使用 pytorch torch.datasets.ImageFolder 加载训练数据。
transform = transforms.Compose([transforms.Resize(400),
transforms.ToTensor()])
dataset_path = &#39;ss/&#39;
dataset = datasets.ImageFolder(root=dataset_path, transform=transform)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=20)

我遇到了以下错误：
-------------------------------------------------------------------------------
FileNotFoundError Traceback (most recent call last)
&lt;ipython-input-27-7abcc1f434b1&gt; in &lt;module&gt;()
2 transforms.ToTensor()])
3 dataset_path = &#39;ss/&#39;
----&gt; 4 dataset = datasets.ImageFolder(root=dataset_path, transform=transform)
5 dataloader = torch.utils.data.DataLoader(dataset, batch_size=20)

3 帧
/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py in make_dataset(directory, class_to_idx, extensions, is_valid_file)
100 if extensions 不为 None:
101 msg += f&quot;支持的扩展名是：{&#39;, &#39;.join(extensions)}&quot;
--&gt; 102 raise FileNotFoundError(msg)
103 
104 return entities

FileNotFoundError: 未找到类 .ipynb_checkpoints 的有效文件。支持的扩展名为：.jpg、.jpeg、.png、.ppm、.bmp、.pgm、.tif、.tiff、.webp

我的数据集文件夹包含一个子文件夹，其中包含许多 png 格式的训练图像，但 ImageFolder 仍然无法访问它们。]]></description>
      <guid>https://stackoverflow.com/questions/68229246/pytorch-torchvision-datasets-imagefolder-filenotfounderror-found-no-valid-file</guid>
      <pubDate>Fri, 02 Jul 2021 17:31:32 GMT</pubDate>
    </item>
    <item>
      <title>对大量分类特征进行编码的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/67197522/what-is-the-best-way-to-encode-a-large-number-of-categorical-features</link>
      <description><![CDATA[我正在尝试制作一个小型数据科学工具（有点像 WEKA 的迷你版）。现在，我有这些具有大量特征（70-100+）的数据集，它们大多是分类的。我正在使用 Python sklearn 进行机器学习逻辑，我需要根据我得到的 sklearn 错误将这些类别转换为数值。
鉴于此，One Hot Encoding 不是一种选择，因为它会过度扩大维度。
我研究过其他可能有效的方法，如频率编码、标签编码等。但我真的不确定在我的情况下该选择什么。
此外，WEKA 实际上如何处理这些问题？我在 WEKA 中输入了我的数据集，它们运行良好，给了我很好的结果！]]></description>
      <guid>https://stackoverflow.com/questions/67197522/what-is-the-best-way-to-encode-a-large-number-of-categorical-features</guid>
      <pubDate>Wed, 21 Apr 2021 14:07:19 GMT</pubDate>
    </item>
    <item>
      <title>数据集特征编码和缩放[关闭]</title>
      <link>https://stackoverflow.com/questions/65028379/dataset-features-encoding-and-scaling</link>
      <description><![CDATA[我有一个包含非序数分类特征的数据集。在训练机器学习模型（线性 SVC）之前，对它们进行转换（编码 + 缩放）的最佳方法是什么？
我尝试过的方法：

标签编码 - 这种方法有效。但缩放毫无意义，因为特征中的不同类别没有任何特定顺序。

独热编码 - 特征中有数千个唯一类别，这会通过创建数千列使 ML 模型变得复杂。

计数编码 - 我的训练测试拆分没有训练集中特征的所有唯一类别，当我对这些特征进行计数编码时，测试集中会引入 NaN。

]]></description>
      <guid>https://stackoverflow.com/questions/65028379/dataset-features-encoding-and-scaling</guid>
      <pubDate>Thu, 26 Nov 2020 19:45:23 GMT</pubDate>
    </item>
    <item>
      <title>整合数值/物理数据进行 CNN 图像分类</title>
      <link>https://stackoverflow.com/questions/63206214/integrating-numerical-physical-data-for-cnn-image-classification</link>
      <description><![CDATA[我正在尝试使用 CNN 通过 keras 在 Python 中对医学图像进行分类。这些医学图像还包括年龄和性别等文本信息，这些信息会影响模型的决策。我如何训练一个可以同时使用图像和现实世界信息进行训练的 CNN，以便它能够根据两者进行分类？]]></description>
      <guid>https://stackoverflow.com/questions/63206214/integrating-numerical-physical-data-for-cnn-image-classification</guid>
      <pubDate>Sat, 01 Aug 2020 14:17:57 GMT</pubDate>
    </item>
    <item>
      <title>如何在具有分类和数字特征的 Pandas 数据框上应用独热编码？</title>
      <link>https://stackoverflow.com/questions/39258158/how-do-i-apply-one-hot-encoding-on-a-pandas-dataframe-with-both-categorical-and</link>
      <description><![CDATA[某些特征是数值的，例如“学校毕业率”，而其他特征则是分类的，例如学校名称。我对分类特征使用了标签编码器，将它们转换为整数。
我现在有一个包含浮点数和整数的数据框，分别表示数值特征和分类特征（使用标签编码器转换）。
我不确定如何使用学习器，我需要使用独热编码吗？如果需要，我该怎么做？根据我目前的理解，由于数据框中有浮点数，因此我无法简单地将数据框传递给 sklearn OneHotEncoder。我是否只需将标签编码器应用于所有特征即可解决问题？
来自我的数据框的示例数据。 OPEID 和 opeid6 使用标签编码器进行转换]]></description>
      <guid>https://stackoverflow.com/questions/39258158/how-do-i-apply-one-hot-encoding-on-a-pandas-dataframe-with-both-categorical-and</guid>
      <pubDate>Wed, 31 Aug 2016 20:06:49 GMT</pubDate>
    </item>
    </channel>
</rss>