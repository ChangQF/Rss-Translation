<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 18 Sep 2024 09:18:31 GMT</lastBuildDate>
    <item>
      <title>请问，亏损偏差巨大，到底是什么原因造成的？</title>
      <link>https://stackoverflow.com/questions/78997392/i-would-like-to-ask-what-are-the-reasons-for-the-huge-deviation-of-loss</link>
      <description><![CDATA[我的loss
我的loss函数
这是我的loss值，我觉得它看起来不稳定。
请问loss偏差这么大的原因是什么？
可能是loss函数的设计问题？还是数据集的质量问题？
非常感谢您的回答。]]></description>
      <guid>https://stackoverflow.com/questions/78997392/i-would-like-to-ask-what-are-the-reasons-for-the-huge-deviation-of-loss</guid>
      <pubDate>Wed, 18 Sep 2024 09:05:41 GMT</pubDate>
    </item>
    <item>
      <title>无法从移动设备访问 Flask 服务器：图像分类问题</title>
      <link>https://stackoverflow.com/questions/78997111/flask-server-not-accessible-from-mobile-device-image-classification-issue</link>
      <description><![CDATA[我的本​​地机器上运行着一个 Flask 服务器，它被设置为使用预先训练的 TensorFlow 模型对图像进行分类。Flask 服务器在我的计算机上成功运行，并且可以在从同一台机器上的模拟器访问时处理请求。但是，当我尝试从移动设备访问 Flask 服务器以对图像进行分类时，我遇到了问题。
Flask 服务器代码：
if __name__ == &#39;__main__&#39;:
app.run(debug=True, host=&#39;0.0.0.0&#39;, port=5000)

在 Flutter 中，我将发送图像的 URL 设置为“http://&lt;my_computer_ip&gt;:5000/predict”。
当我尝试从移动应用程序向 Flask 服务器发送图像时，我遇到了以下问题：
连接超时：请求超时而未到达 Flask 服务器

但是，我没有在 Flask 服务器控制台上收到相关日志。Flask 服务器在 0.0.0.0 上运行，应该可以从同一网络上的其他设备访问。当我尝试将图像从移动设备发送到本地计算机上运行的 Flask 服务器时，如何解决连接超时问题？]]></description>
      <guid>https://stackoverflow.com/questions/78997111/flask-server-not-accessible-from-mobile-device-image-classification-issue</guid>
      <pubDate>Wed, 18 Sep 2024 07:55:14 GMT</pubDate>
    </item>
    <item>
      <title>MLC-LLM Android 应用程序中遇到构建问题</title>
      <link>https://stackoverflow.com/questions/78996973/facing-buid-issue-in-the-mlc-llm-android-app</link>
      <description><![CDATA[我尝试使用以下链接运行 MLC-LLM Android 应用程序，但由于以下屏幕截图中显示的错误，我无法运行该应用程序：
Git 存储库

我已克隆并运行 MLC-AI/MLC-LLC，但它没有运行，我得到了它，帮我解决这个问题。
有人在 android 上运行过这个吗，请帮我解决这个问题。
在 android 目录中，有 3 个文件夹：

mlc4j
MLCChat
MLCEngineExample

我们需要使用哪一个？我该如何使用它？]]></description>
      <guid>https://stackoverflow.com/questions/78996973/facing-buid-issue-in-the-mlc-llm-android-app</guid>
      <pubDate>Wed, 18 Sep 2024 07:17:43 GMT</pubDate>
    </item>
    <item>
      <title>ImportError: 导入 o​​nnx_cpp2py_export 时 DLL 加载失败：动态链接库 (DLL) 初始化例程失败</title>
      <link>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</guid>
      <pubDate>Wed, 18 Sep 2024 07:08:40 GMT</pubDate>
    </item>
    <item>
      <title>如何利用 writer.add_image 将图像添加到 tensorboard 记录器？</title>
      <link>https://stackoverflow.com/questions/78996316/how-to-add-images-to-tensorboard-logger-leveraging-writer-add-image</link>
      <description><![CDATA[我有一个通道图像，想使用 writer.add_image 将它们添加到 tensorboard 记录器中，如下所示：
 generated_images_to_show = torch.cat(gtimage_cpu, predimage_cpu), dim=0)
gen_image_grid = make_grid(generated_images_to_show .unsqueeze(1), nrow=inputs.size(0), padding=6, normalize=False)
writer.add_image(&#39;Validation Gen Comparison&#39;, gen_image_grid, epoch)

我生成的图像有一个通道，我需要使用 cmap= &#39;hsv&#39; 绘制它们，但 add_image 中没有可用的 cmap 功能。在这种情况下，我该怎么办？我只想在 tensorboard 中查看结果。
一种解决方案是将图像转换为 RGB 通道，然后使用 cmap=hsv 并将转换后的图像记录到 tensorboard，还有其他（也许更有效的）解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78996316/how-to-add-images-to-tensorboard-logger-leveraging-writer-add-image</guid>
      <pubDate>Wed, 18 Sep 2024 01:48:03 GMT</pubDate>
    </item>
    <item>
      <title>如何重新训练 ML 模型</title>
      <link>https://stackoverflow.com/questions/78996028/how-to-re-train-ml-model</link>
      <description><![CDATA[我正在使用 AWS Sagemaker，用一些带标签的数据训练了一个模型，将其部署到终端并设置 Lambda 以提供预测。
一切都很好，但我想定期重新训练我的模型，例如使用 1 周的历史数据。
但我的历史数据没有标签，这意味着它不能用于训练。我该如何标记它？
我最初认为我可以使用我的模型的预测来标记新的（未标记的）数据，但我读到这不是一个好主意，因为它只会确保我的模型的准确性，即使它可能远非准确。
那么我在哪里可以获得我的历史数据的标签？
如果历史数据不能通过模型​​标记，那么是否意味着它应该手动标记？那么，训练和提供模型的意义何在？
举个例子，让我们进行欺诈交易检测。好的，有一些初始数据，由确切知道交易是否为欺诈的人手动标记，因此具有 100% 的准确性。
那么是否应该定期手动更新额外的 100% 准确的事件？]]></description>
      <guid>https://stackoverflow.com/questions/78996028/how-to-re-train-ml-model</guid>
      <pubDate>Tue, 17 Sep 2024 22:44:46 GMT</pubDate>
    </item>
    <item>
      <title>生存分析的校准图</title>
      <link>https://stackoverflow.com/questions/78995759/calibration-plots-for-survival-analysis</link>
      <description><![CDATA[我无法为我的生存分析项目创建校准图。（食管癌数据集）
经过调整后，我已经完成了我的模型（AORSF）：
aorsf_fit &lt;- last_fit(
final_aorsf_wf,
split = initial_split(final_main, prop = 0.75),
metrics = survivor_metrics,
eval_time = time_points_complete,
)

然后我使用以下代码收集我的预测集：
&gt; predictions &lt;- collect_predictions(aorsf_fit)
&gt; predictions 
# A tibble: 687 × 6
.pred .pred_time id .row surv .config 
&lt;list&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;Surv&gt; &lt;chr&gt; 
1 &lt;tibble [8 × 3]&gt; 107. 训练/测试分割 3 57.8+ Preprocessor1_Model1
2 &lt;tibble [8 × 3]&gt; 96.1 训练/测试分割 7 96.8+ Preprocessor1_Model1
3 &lt;tibble [8 × 3]&gt; 94.2 训练/测试分割 11 130.0 Preprocessor1_Model1
4 &lt;tibble [8 × 3]&gt; 11.4 训练/测试分割 15 9.0 Preprocessor1_Model1
5 &lt;tibble [8 × 3]&gt; 102. 训练/测试分割 16 69.1+ Preprocessor1_Model1
6 &lt;tibble [8 × 3]&gt; 37.8 训练/测试分割 17 33.0 Preprocessor1_Model1
7 &lt;tibble [8 × 3]&gt; 103. 训练/测试分割 18 142.8 Preprocessor1_Model1
8 &lt;tibble [8 × 3]&gt; 23.4 训练/测试分割 20 13.5 Preprocessor1_Model1
9 &lt;tibble [8 × 3]&gt; 89.7 训练/测试分割 21 146.5 Preprocessor1_Model1
10 &lt;tibble [8 × 3]&gt; 107. 训练/测试分割 23 60.1+ Preprocessor1_Model1
# ℹ 677 更多行

我甚至能够获得时间相关的 ROC 曲线（感谢 roc_curve_survival() 函数）：
predictions |&gt; 
roc_curve_survival(truth = surv, .pred)|&gt; 
filter(.eval_time == 60) |&gt; 
ggplot(aes(1 - specificity,sensitive)) +
geom_line() +
theme_minimal()

但是，我无法使用“.pred”列表创建校准图。我尝试使用链接中建议的代码：使用 tidymodels 进行校准的简介
我希望能够使用“predictions”对象中的预测生存数据来构建校准图。predictions 对象中的“.pred”变量包含以下 3 行：
.eval_time
.pr​​ed_survival
.weight_censored
我确信，如果以某种方式提取，它可以用于创建校准图（如链接中所示），然后可以用来显示模型相对于观察到的生存率的运行情况！（我也观察到了我的数据集的生存率）
我只尝试了以下方法，但没有结果：
predictions |&gt; 
+ ggplot(aes(.pred_survival))
]]></description>
      <guid>https://stackoverflow.com/questions/78995759/calibration-plots-for-survival-analysis</guid>
      <pubDate>Tue, 17 Sep 2024 20:49:44 GMT</pubDate>
    </item>
    <item>
      <title>如何从 R 中的最小生成树模型中找到连通实例</title>
      <link>https://stackoverflow.com/questions/78995310/how-to-find-the-connected-instances-from-a-minimum-spanning-trees-model-in-r</link>
      <description><![CDATA[我正在构建最小生成树模型，并且成功了。我生成了一个图，并想确定每个数据点连接了哪些替代数据点。有办法吗？
建模代码如下。
data(iris)
mst.mod &lt;- ape::mst(dist(iris))
plot(mst.mod)

树已可视化。它看起来有点乱，但我想确定哪些实例与实例 1 等相连。从视觉上看，可以看出实例与实例 28 和 40 有一条边。但是是否有 R 代码可以为每个数据点找到它们？
]]></description>
      <guid>https://stackoverflow.com/questions/78995310/how-to-find-the-connected-instances-from-a-minimum-spanning-trees-model-in-r</guid>
      <pubDate>Tue, 17 Sep 2024 18:11:43 GMT</pubDate>
    </item>
    <item>
      <title>自动清除 AppOptics 中的停止报告警报 [关闭]</title>
      <link>https://stackoverflow.com/questions/78994587/auto-clearing-a-stop-reporting-alert-in-appoptics</link>
      <description><![CDATA[我在 AppOptics 中有一个警报，在一台机器 10 分钟没有报告后触发。有时，机器会因为不再使用而停止运转，因此 10 分钟后，我会收到它们不再报告的警报。AppOptics 是否会在某个时候“了解”不再报告的机器也不应该报告（机器最早的报告现在超出了指标的回溯窗口）？如果是，回溯窗口的长度是多少？如果没有，有没有办法防止正在关闭的机器触发警报？
从 Appoptics 网站上检查了警报的设置后，我找不到回溯窗口的设置。]]></description>
      <guid>https://stackoverflow.com/questions/78994587/auto-clearing-a-stop-reporting-alert-in-appoptics</guid>
      <pubDate>Tue, 17 Sep 2024 14:42:34 GMT</pubDate>
    </item>
    <item>
      <title>混淆矩阵中的空列</title>
      <link>https://stackoverflow.com/questions/78993181/empty-columns-in-confusion-matrix</link>
      <description><![CDATA[我正在对预处理的 APTOS 2019 数据集进行疾病分级训练，我的混淆矩阵的最后两列每次都恒定为零。数据分布如下：

类别 0：1805 幅图像
类别 1：370 幅图像
类别 3：999 幅图像
类别 4：193 幅图像
类别 5：295 幅图像

以下是 70 个时期的结果：
测试准确率=77%

混淆矩阵：
[[258 3 1 0 0]
[ 6 38 16 0 0]
[ 13 13 130 0 0]
[ 0 7 22 0 0]
[ 4 9 30 0 0]]

测试集的类别准确率模型：
[98.47328244 63.33333333 83.33333333 0. 0.]

每个类别的敏感度（召回率）：
[0.98473282 0.63333333 0.83333333 0. 0.]

每个类别的特异性：
[0.98513011 0.95416667 0.92592593 0.94727273 0.92181818]

我尝试应用类别权重、分层 k 倍交叉验证，但没有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78993181/empty-columns-in-confusion-matrix</guid>
      <pubDate>Tue, 17 Sep 2024 08:31:41 GMT</pubDate>
    </item>
    <item>
      <title>Nougat OCR 对页面进行部分检测时出现问题</title>
      <link>https://stackoverflow.com/questions/78992893/issue-with-partial-detection-of-pages-by-nougat-ocr</link>
      <description><![CDATA[我正在使用 Nougat OCR 模型将数学方程式转换为 latex 格式。我遇到的问题是，有些页面无法被 Nougat OCR 模型完全检测到。在许多情况下，页面上只有一半的内容被检测到，而其余部分则被跳过。但是，对于其他页面，检测工作完全正常。
重现步骤：

将 PDF 转换为图像（每页一张图像）。
使用 Nougat OCR 模型单独处理每张图像。
观察到某些页面被部分检测到，而其他页面则被正确处理。

（这是我用于推理的笔记本）
示例结果：
第一页
## 答案 (LC2020 HL, P2):

1.\(0\); \(A\)、\(B\) 和 \(C\) 共线 [0, 4, 7, 11, 15]
2. \(33\cdot 435^{\circ}\)[0, 4, 7, 11, 15]
3. \(9\)[0, 4, 7, 11, 15]
4. \(x^{2}+y^{2}+4x-21=0\)、\(x^{2}+y^{2}-8x-9=0\)[0, 4, 7, 11, 15]
5. \(6\cdot 44\) m [0, 4, 7, 11, 15]
6. \(k=9\)[0, 4, 7, 11, 15]
7. \(\frac{5\pi}{3}\), \(

第二页

## 答案 (LC 2019 HL, P2):

1. (i) \(\frac{48}{95}\) [**0, 4, 7, 10**], (ii) \(\frac{88}{969}\) [**0, 4, 5, 8, 10**]
2. 1400 [**0, 4, 7, 10**]
3. 显示 [**0, 4, 7, 10**]
4. (i) \(mx-y-6m=0\) [**0, 2, 5**], (ii) \(P\bigg{(}\frac{18m+25}{3m+4}\), \(\frac{m}{3m+4}\bigg{)}\) [**0, 4, 7, 11, 15**]

1. \(k=-4\), 10 [**0, 4, 7, 10**]
2. \(s\): \(x^{2}+y^{2}-2x-2y+1=0\) [**0, 5, 10, 15, 20**]
3. 显示 [**0, 2, 3, 5**]
4. \(\frac{1}{3}\) [**0, 5, 10, 15, 20**]
5. 构造 [**0, 4, 7, 11, 15**]
6. \(30^{\circ

预期行为：OCR 模型应一致地检测每个页面的所有部分，而不是仅检测部分内容。
问题：是否需要进行任何预处理以确保完整的页面检测？或者在 Nougat OCR 中是否有特定参数需要调整以改善结果？]]></description>
      <guid>https://stackoverflow.com/questions/78992893/issue-with-partial-detection-of-pages-by-nougat-ocr</guid>
      <pubDate>Tue, 17 Sep 2024 07:12:30 GMT</pubDate>
    </item>
    <item>
      <title>对多个相互交织的目标使用反向预测[关闭]</title>
      <link>https://stackoverflow.com/questions/78992262/using-reverse-prediction-for-multiple-intertwined-targets</link>
      <description><![CDATA[我有一个人生阶段预测模型，它根据一组规则为每个潜在的人生阶段分配点数，总点数最高的人生阶段被视为赢家。这种无监督模型在识别样本客户是青少年、单身、夫妻、有小孩等方面效果很好。
在每次模型运行结束时，我都会总结结果，以获得每个人生阶段的客户比例。该模型每三个月运行一次，如果不进行补救，每次的最终比例都会与目标比例相差太大。目前，我使用反复试验来重新平衡比例，通过在每个人生阶段增加或减去少量点数，将边缘客户转移到另一个人生阶段类别。
我想要的是一种自动化方法，可以指定重新校准最终比例所需的点数，以比我的手动方法更轻松（更少的迭代次数）地匹配目标比例。本质上，协调点 (recon_pts) 是模型的输入，预测每个客户的生命阶段是关键输出，同时还有相应的比例 (model_pn)。我希望模型比例最终与目标比例 (target_pn) 相匹配。
一个例子肯定有助于说明我所追求的。请注意，以下是实际观察到的结果，但我不想要使用或输出这些精确数字的工具，而是使用机器学习来更有效地重现我的反复试验方法。
假设起始位置为：

提供者：
life_stage=c(&#39;CHD&#39;, &#39;TNG&#39;, &#39;TWS&#39;, &#39;SGL&#39;, &#39;CPL&#39;, &#39;YGF&#39;, &#39;PTF&#39;, &#39;OLF&#39;, &#39;ENR&#39;),
recon_pts=c(0.215, -0.143, -0.086, 0.024, -0.049, -0.079, -0.14, -0.162, 0.083),
model_pn=c(0.012, 0.087, 0.091, 0.065, 0.113, 0.115, 0.123, 0.122, 0.273),
target_pn=c(0.014, 0.091, 0.095, 0.065, 0.114, 0.114, 0.107, 0.122, 0.277))

CHD 生命阶段比例过低 (1.2%)，低于目标 (1.4%)。在另一个极端，PTF 太高 (12.3%)，因为它高于目标 (10.7%)。
在我的手动方法下，我选择将 CHD 的 recon_pts 从 0.215 增加到 0.22，这样我就能在这个生命阶段获得更多客户。同时，我还将 PTF 的分数从 -0.14 减少到 -0.17，这样分配到这里的客户就会减少。
然后我得到了这些结果，所有比例都经过了调整。 CHD 比例更接近目标但仍然太低，PTF 现在太低，因为比例已低于目标，其他所有方面也都需要注意：

因此，在下一次迭代中，我可能会选择增加 CHD 的 recon_pts，增加 PTF 的点数并减少 YFG 的点数。然后它就以一种极其缓慢的方式继续下去了！
对我来说，这看起来像是机器学习的理想候选者，其中的过程可以学习如何调整 recon_pts，以便 model_pn 最终匹配每个生命阶段的 target_pn。
如何在 R 中有效地编码？]]></description>
      <guid>https://stackoverflow.com/questions/78992262/using-reverse-prediction-for-multiple-intertwined-targets</guid>
      <pubDate>Tue, 17 Sep 2024 01:29:30 GMT</pubDate>
    </item>
    <item>
      <title>我的扩散训练出了什么问题，导致模型无法学习？</title>
      <link>https://stackoverflow.com/questions/78974693/what-is-wrong-with-my-diffusion-training-that-is-not-letting-the-model-to-learn</link>
      <description><![CDATA[我是扩散模型的新手，这是我第一次训练用于图像到图像转换（超分辨率）的扩散。我想训练一个扩散模型，我使用扩散器库中的 UNet2DModel，这是模型：
model = UNet2DModel(
sample_size= (576, 768), # 目标图像分辨率
in_channels=6, # 输入通道数，RGB 图像为 3
out_channels=3, # 输出通道数
layer_per_block=2, # 每个 UNet 块使用多少个 ResNet 层
# block_out_channels=(128, 128, 256, 256, 512, 512), # 每个 UNet 块的输出通道数
block_out_channels=(16, 16, 32, 32, 64, 64), # 每个 UNet 块的输出通道数
norm_num_groups=4,
down_block_types=(
&quot;DownBlock2D&quot;, # 常规 ResNet 下采样块
&quot;DownBlock2D&quot;,
&quot;DownBlock2D&quot;,
&quot;DownBlock2D&quot;,
&quot;AttnDownBlock2D&quot;, # 具有空间自注意的 ResNet 下采样块
&quot;DownBlock2D&quot;,
),
up_block_types=(
&quot;UpBlock2D&quot;, # 常规 ResNet 上采样块
&quot;AttnUpBlock2D&quot;, # 具有空间自注意的 ResNet 上采样块
&quot;UpBlock2D&quot;,
&quot;UpBlock2D&quot;,
&quot;UpBlock2D&quot;,
&quot;UpBlock2D&quot;,
),
)

这是我的训练循环：
def train_network(model, train_loader, val_loader, epochs, device, writer):
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-6)
criterion = torch.nn.MSELoss()
transmission_scheduler = DDPMScheduler(num_train_timesteps=1000) # Diffusion Scheduler
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)
best_loss = float(&#39;inf&#39;)
train_losses = []
val_losses = []
global_step = 0 # 跟踪 TensorBoard 日志的全局步骤

for epoch in range(epochs):
model.train()
running_loss = 0.0
for i, (inputs, target) in enumerate(tqdm(train_loader)):
input, target = input.to(device), target.to(device)
input.requires_grad_(True)
target.requires_grad_(True)
optimizer.zero_grad()
# 添加噪声
noise = torch.randn(targets.shape).to(device)
timesteps = torch.randint(0, transmission_scheduler.config.num_train_timesteps, (inputs.size(0),), device=device)
noisy_targets = transmission_scheduler.add_noise(targets, noise, timesteps).to(device)

# 将输入与噪声目标相结合作为模型输入
combined_input = torch.cat((inputs, noisy_targets), dim=1).to(device)

# 预测在所选时间步添加的噪声
predicted_noise = model(combined_input, timesteps)
predicted_noise = predict_noise.sample

# 计算预测噪声和实际噪声之间的损失
loss = criterion(predicted_noise.float(), noise.float())
loss.backward()

optimizer.step()

running_loss += loss.item()
global_step += 1#

epoch_loss = running_loss / len(train_loader.dataset)
train_losses.append(epoch_loss)
writer.add_scalar(&#39;Loss/train_epoch&#39;, epoch_loss, epoch)
writer.add_scalar(&#39;Learning Rate&#39;, optimizer.param_groups[0][&#39;lr&#39;], epoch)
# 验证
val_loss, avg_ssim, avg_psnr, avg_mae = verify(model, val_loader, criterion, device, writer, epoch, transmission_scheduler)
val_losses.append(val_loss)

if val_loss &lt; best_loss:
best_loss = val_loss
save_model(model, f&#39;diffusion_{epoch}_{best_loss: .3f}.pth&#39;, best_loss)

lr_scheduler.step()

writer.close()
return train_losses, val_losses

我的训练出了点问题，但我搞不清楚哪个部分出了问题。模型根本没学到什么，指标很低，一点进展都没有，尽管我给了它一个很大的数据集，并训练了它足够长的时间，但它仍然只会产生噪音（我绘制的 pplot 和看到的是目标模型的预测；因为模型应该预测噪音）。
我的训练出了什么问题，导致模型无法学习？
我所做的就是记录超参数，看看它们是否有问题，但实际上没有。我更改了超参数，但没有效果。我更改了模型，问题仍然存在，这就是为什么我决定我的训练可能有问题，但无法确定是哪个部分。]]></description>
      <guid>https://stackoverflow.com/questions/78974693/what-is-wrong-with-my-diffusion-training-that-is-not-letting-the-model-to-learn</guid>
      <pubDate>Wed, 11 Sep 2024 15:50:01 GMT</pubDate>
    </item>
    <item>
      <title>Keras-rl2 错误与 Tensorflow 的兼容性</title>
      <link>https://stackoverflow.com/questions/78340927/keras-rl2-error-compability-with-tensorflow</link>
      <description><![CDATA[我目前在使用 keras-rl2 和 tensorflow 时遇到了一个问题，我不知道为什么，我只是在互联网上搜索了 keras-rl2、tensorflow 和 keras 文档，但没有找到解决方案。
目前，我想将 keras-rl2 与最新版本的 tensorflow 2.16.1 和 keras 3 一起使用，使用时出现了这样的错误
from rl.agents import DKQAgent

ModuleNotFoundError Traceback（最近一次调用最后一次）
Cell In[37]，第 1 行
----&gt; 1 导入 rl.agents
3 print(&quot;RL 代理库版本：&quot;, rl.agents.__version__)

文件 D:\Anaconda\Lib\site-packages\rl\agents\__init__.py:2
1 来自 .dqn 导入 DQNAgent、NAFAgent、ContinuousDQNAgent
----&gt; 2 来自 .ddpg 导入 DDPGAgent
3 来自 .cem 导入 CEMAgent
4 来自 .sarsa 导入 SarsaAgent、SARSAAgent

文件 D:\Anaconda\Lib\site-packages\rl\agents\dqn.py:8
5 来自 tensorflow.keras.layers 导入 Lambda、Input、Layer、Dense
7 来自 rl.core 导入 Agent
----&gt; 8 从 rl.policy 导入 EpsGreedyQPolicy、GreedyQPolicy
9 从 rl.util 导入 *
12 def mean_q(y_true, y_pred):

文件 D:\Anaconda\Lib\site-packages\rl\core.py:8
4 导入 numpy 作为 np
5 从 tensorflow.keras.callbacks 导入 History
7 从 rl.callbacks 导入 (
----&gt; 8 CallbackList,
9 TestLogger,
10 TrainEpisodeLogger,
11 TrainIntervalLogger,
12 Visualizer
13 )
16 类 Agent:
17 &quot;&quot;&quot;所有已实现代理的抽象基类。
18 
19 每个代理通过首先观察
(...)
37 处理器（“Processor”实例）与环境进行交互：有关详细信息，请参阅 [Processor](#processor)。
38 &quot;&quot;&quot;

文件 D:\Anaconda\Lib\site-packages\rl\callbacks.py:12
9 from tensorflow.python.keras.callbacks import Callback as KerasCallback, CallbackList as KerasCallbackList
10 from tensorflow.python.keras.utils.generic_utils import Progbar
---&gt; 12 class Callback(KerasCallback):
13 def _set_env(self, env):
14 self.env = env

ModuleNotFoundError: 没有名为“keras.utils.generic_utils”的模块

当我以为我只需要将其降级到某个版本（如 2.13.0 和 keras 2.13.0）时，它仍然会出错
-------------------------------------------------------------------------------------------
ImportError Traceback（最近一次调用最后一次）
Cell In[18]，第 1 行
----&gt; 1 from rl.agents.dqn import DQNAgent

File D:\Anaconda\envs\AI\Lib\site-packages\rl\agents\__init__.py:1
----&gt; 1 从 .dqn 导入 DQNAgent、NAFAgent、ContinuousDQNAgent
2 从 .ddpg 导入 DDPGAgent
3 从 .cem 导入 CEMAgent

文件 D:\Anaconda\envs\AI\Lib\site-packages\rl\agents\dqn.py:7
4 从 tensorflow.keras.models 导入 Model
5 从 tensorflow.keras.layers 导入 Lambda、Input、Layer、Dense
----&gt; 7 从 rl.core 导入 Agent
8 从 rl.policy 导入 EpsGreedyQPolicy、GreedyQPolicy
9 从 rl.util 导入 *

文件 D:\Anaconda\envs\AI\Lib\site-packages\rl\core.py:7
4 将 numpy 导入为 np
5 从 tensorflow.keras.callbacks 导入 History
----&gt; 7 from rl.callbacks import (
8 CallbackList,
9 TestLogger,
10 TrainEpisodeLogger,
11 TrainIntervalLogger,
12 Visualizer
13 )
16 class Agent:
17 &quot;&quot;&quot;所有已实现代理的抽象基类。
18 
19 每个代理通过首先观察
(...) 来与环境（由 `Env` 类定义）交互
37 处理器（`Processor` 实例）：有关详细信息，请参阅 [Processor](#processor)。
38 &quot;&quot;&quot;

文件 D:\Anaconda\envs\AI\Lib\site-packages\rl\callbacks.py:8
6 import numpy as np
7 import tensorflow as tf
----&gt; 8 从 tensorflow.keras 导入 __version__ 作为 KERAS_VERSION
9 从 tensorflow.python.keras.callbacks 导入 Callback 作为 KerasCallback，CallbackList 作为 KerasCallbackList
10 从 tensorflow.python.keras.utils.generic_utils 导入 Progbar

ImportError：无法从“tensorflow.keras”导入名称“__version__”（D:\Anaconda\envs\AI\Lib\site-packages\keras\api\_v2\keras\__init__.py）

有人能给我解释或解决方案为什么它总是错误吗？
感谢您的关心]]></description>
      <guid>https://stackoverflow.com/questions/78340927/keras-rl2-error-compability-with-tensorflow</guid>
      <pubDate>Wed, 17 Apr 2024 12:20:33 GMT</pubDate>
    </item>
    <item>
      <title>神经网络在大小变化的同时还能进行训练吗？</title>
      <link>https://stackoverflow.com/questions/38426117/can-a-neural-network-be-trained-while-it-changes-in-size</link>
      <description><![CDATA[是否有已知的方法可以在神经网络缩小或增大（节点数、连接数等）时对其进行持续训练和优雅降级？
据我所知，我读过的有关神经网络的所有内容都是从静态角度进行的。您先定义网络，然后对其进行训练。
如果存在具有 N 个节点（神经元等）的某个神经网络 X，是否可以训练网络 (X)，以便在 N 增加或减少时，网络仍然有用且能够执行？]]></description>
      <guid>https://stackoverflow.com/questions/38426117/can-a-neural-network-be-trained-while-it-changes-in-size</guid>
      <pubDate>Sun, 17 Jul 2016 21:30:19 GMT</pubDate>
    </item>
    </channel>
</rss>