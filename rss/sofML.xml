<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 16 Mar 2024 00:56:02 GMT</lastBuildDate>
    <item>
      <title>当调用 sklearn Pipeline 对象上的方法时，Python 会引发 AttributeError</title>
      <link>https://stackoverflow.com/questions/78170066/python-raises-an-attributeerror-when-methods-on-the-sklearn-pipeline-object-are</link>
      <description><![CDATA[问题
我正在对 Pipeline 对象调用 fit_transform() 和 transform() 方法，但每当我尝试时，Python 都会引发 AttributeError这样做。这是我正在尝试运行的导入内容。 （注意：训练/测试分割已经完成）
from sklearn.impute import SimpleImputer
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.pipeline 导入管道

管道 = 管道([(&#39;mean_impute&#39;, SimpleImputer()),
                 (&#39;标准&#39;, StandardScaler()),
                 (&#39;sklearn_lm&#39;, LinearRegression())])

pipeline.fit_transform(x_train, y_train) #&lt;-- 此处错误

x_transform = pipeline.transform(x_test) #&lt;-- 如果上一行不存在

错误内容如下：
属性错误：此“管道”没有属性“fit_transform”
出了什么问题？我确信这很简单。
我尝试过的事情：

查看了 sci-kit learn 的文档，确认 sklearn 中的 Pipeline 对象存在这些方法
检查了 x_train 和 y_train 的大小，确保它们相同，并且都有标题
重新安装sci-kit learn
]]></description>
      <guid>https://stackoverflow.com/questions/78170066/python-raises-an-attributeerror-when-methods-on-the-sklearn-pipeline-object-are</guid>
      <pubDate>Fri, 15 Mar 2024 23:15:11 GMT</pubDate>
    </item>
    <item>
      <title>XGBRegressor 树中叶子值的求和与预测不匹配</title>
      <link>https://stackoverflow.com/questions/78169666/summing-the-values-of-leafs-in-xgbregressor-trees-do-not-match-prediction</link>
      <description><![CDATA[据我了解，XGBoost 模型（在本例中为 XGBRegressor）的最终预测是通过对预测叶子的值求和来获得的 [1] [2]。然而，我未能匹配对值求和的预测。这是 MRE：
导入json
从集合导入双端队列

将 numpy 导入为 np
从 sklearn.datasets 导入 load_diabetes
从 sklearn.model_selection 导入 train_test_split
将 xgboost 导入为 xgb


def leafs_vector(树):
    “”“”返回每棵树的节点向量，只有叶子有 0 个不同“”“”

    堆栈 = 双端队列([树])

    而堆栈：
        节点 = stack.popleft()
        如果“叶”是在节点中：
            产量节点[“叶子”]
        别的：
            产量 0
            对于节点 [“children”] 中的子节点：
                堆栈.追加（子）


# 加载糖尿病数据集
糖尿病 = load_diabetes()
X, y = 糖尿病.数据, 糖尿病.目标

# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义 XGBoost 回归模型
xg_reg = xgb.XGBRegressor(目标=&#39;reg:squarederror&#39;,
                          最大深度=5，
                          n_估计器=10)

# 训练模型
xg_reg.fit(X_train, y_train)

# 计算原始预测
y_pred = xg_reg.predict(X_test)

# 获取每个预测叶子的索引
Predicted_leafs_indices = xg_reg.get_booster().predict(xgb.DMatrix(X_test), pred_leaf=True).astype(np.int32)

# 获取树木
树 = xg_reg.get_booster().get_dump(dump_format=“json”)
trees = [json.loads(tree) 用于树中的树]

# 获取节点向量（按节点 ID 排序）
leafs = [树中树的列表(leafs_vector(tree))]

l_pred = []
对于 Predicted_leafs_indices 中的 pli：
    l_pred.append(sum(li[p] for li, p in zip(leafs, pli)))

断言 np.allclose(np.array(l_pred), y_pred, atol=0.5) # 失败

我还尝试添加 base_score 的默认值 (0.5)（如 这里）到总和，但它也不起作用。
&lt;前&gt;&lt;代码&gt;l_pred = []
对于 Predicted_leafs_indices 中的 pli：
    l_pred.append(sum(li[p] for li, p in zip(leafs, pli)) + 0.5)
]]></description>
      <guid>https://stackoverflow.com/questions/78169666/summing-the-values-of-leafs-in-xgbregressor-trees-do-not-match-prediction</guid>
      <pubDate>Fri, 15 Mar 2024 21:07:25 GMT</pubDate>
    </item>
    <item>
      <title>Pycaret 3.3.0 Compare_models() 显示所有模型 AUC 为零</title>
      <link>https://stackoverflow.com/questions/78169647/pycaret-3-3-0-compare-models-show-zeros-for-all-models-auc</link>
      <description><![CDATA[在使用compare_model()评估模型期间。所有 AUC 均为零。
Pycaret 3.3.0 的这个输出很奇怪。这是什么原因？
[1]: https://i.stack.imgur.com/qm2ZT.png]]></description>
      <guid>https://stackoverflow.com/questions/78169647/pycaret-3-3-0-compare-models-show-zeros-for-all-models-auc</guid>
      <pubDate>Fri, 15 Mar 2024 21:02:29 GMT</pubDate>
    </item>
    <item>
      <title>从 dvc 中提取当前运行阶段</title>
      <link>https://stackoverflow.com/questions/78169479/extract-current-running-stage-from-dvc</link>
      <description><![CDATA[我正在使用“dvc repro -f”进行实验，其中根据 dvc.yaml 配置执行多个阶段。例如：
&lt;前&gt;&lt;代码&gt;阶段：
 训练：
   foreach：
    -周期：0
    -周期：1
    -周期：2
   做：
    命令：
     蟒蛇火车.py
 选择：
   foreach：
    -周期：0
    -周期：1
    -周期：2
   做：
    命令：
     蟒蛇火车.py

在每个阶段的执行过程中，例如第0个周期的训练阶段，我的目标是在Python程序中提取其阶段名称，例如“Training_0”，而在选择阶段，它应该是Selection_0。我正在寻找一种方法来在阶段正在执行时或在其执行开始之前提取此信息。我尝试使用 dvc.api，但 api 不返回当前正在运行的阶段。我怎样才能实现这个目标？]]></description>
      <guid>https://stackoverflow.com/questions/78169479/extract-current-running-stage-from-dvc</guid>
      <pubDate>Fri, 15 Mar 2024 20:20:24 GMT</pubDate>
    </item>
    <item>
      <title>想要建立一个作物推荐系统[关闭]</title>
      <link>https://stackoverflow.com/questions/78169349/want-to-build-a-crop-recommendation-system</link>
      <description><![CDATA[我有一个数据集，其中有 4 个柱状土壤类型、州、季节、作物。这里土壤类型、州、季节是字符串数据类型的特征，标签是农作物，也是字符串数据类型。我想建立一个推荐系统，以土壤类型、季节、州为输入，并根据输入推荐作物。有人可以帮助我吗我面临困难，因为它们都是字符串数据类型
这是我的数据集的样本
、州、土壤类型、季节、农作物
0,Andaman_and_Nicobar_Island,Latterite,rabi,“黑豆，小麦”
1、Andaman_and_Nicobar_Island、冲积土、Kharif、“小豆蔻、黑胡椒、槟榔、咖啡”
2、Andaman_and_Nicobar_Island、Red、Kharif、“小豆蔻、黑胡椒、槟榔、咖啡”
3、Andaman_and_Nicobar_Island、Black、Kharif、“Bajra、黑克、Moong、豇豆、高粱”
4、Andaman_and_Nicobar_Island、Latterite、Kharif、“乌拉德、玉米”
5、Andaman_and_Nicobar_Island，Black，Kharif，“乌拉德，玉米”
6、Andaman_and_Nicobar_Island、Latterite、Kharif、“Bajra、黑豆、Moong、豇豆、高粱”
7、Andaman_and_Nicobar_Island、Latterite、Kharif、“丁香、咖啡、椰子、槟榔”
8,Andaman_and_Nicobar_Island,Alluvial,Kharif,“丁香、咖啡、椰子、槟榔”
9、Andaman_and_Nicobar_Island，Red，Kharif，“丁香、咖啡、椰子、槟榔”
10、Andaman_and_Nicobar_Island、Latterite、Kharif、“小豆蔻、黑胡椒、槟榔、咖啡”
11、Andaman_and_Nicobar_Island，粘土，拉比，“香蕉，姜黄”
12、Andaman_and_Nicobar_Island，冲积物，拉比，“香蕉，姜黄”
13、Andaman_and_Nicobar_Island，粘土，冬天，“油籽、花生、蓖麻、芝麻、鹰嘴豆”
14、Andaman_and_Nicobar_Island，红土，冬天，“油籽、花生、蓖麻、芝麻、鹰嘴豆”
15、Andaman_and_Nicobar_Island，冲积土，冬季，“油籽、花生、蓖麻、芝麻、鹰嘴豆”
16、Andaman_and_Nicobar_Island，Black，rabi，“黑革，小麦”
]]></description>
      <guid>https://stackoverflow.com/questions/78169349/want-to-build-a-crop-recommendation-system</guid>
      <pubDate>Fri, 15 Mar 2024 19:46:02 GMT</pubDate>
    </item>
    <item>
      <title>制作基于fpga的au加速器[关闭]</title>
      <link>https://stackoverflow.com/questions/78169187/making-a-fpga-based-au-accelerator</link>
      <description><![CDATA[我有一个新项目，是基于 FPGA 重新创建人工智能加速器。
我找到了此类事物的实现示例，但它们仅支持定点量化类型。不同的是我想支持bf16格式，这样ai加速器也可以用来训练。
问题不在于这是否可能，我知道它是可能的，但我认为我可能对此抱有不切实际的期望。
根据我在网上可以找到的信息，单个 bf16 乘法器最多可以占用 75 个逻辑元件（不使用 dsp 片进行 7 位乘法），如果我选择脉动阵列类型的架构，每个处理元件可能需要200-500 个逻辑元件。我能买得起的 FPGA 类型有 25k 到 100k 逻辑元件，因此这种方法的性能会非常差。
理想情况下，我希望达到 100GFLOPs（每秒 1000 亿 bf16 mult+add）。这意味着我至少需要 1000 个 500MHz 的处理元件。
有人对如何执行此操作有一些指示或建议吗？
对于任何想知道的人，我知道 FPGA 设计和软件开发是非常乏味的任务，但我并不害怕承担它们。
我还没有开始这个项目，只是研究了它的可行性。]]></description>
      <guid>https://stackoverflow.com/questions/78169187/making-a-fpga-based-au-accelerator</guid>
      <pubDate>Fri, 15 Mar 2024 19:11:09 GMT</pubDate>
    </item>
    <item>
      <title>WSL 2 对于机器学习训练 (PyTorch) 的性能有多好？</title>
      <link>https://stackoverflow.com/questions/78169143/how-good-are-wsl-2-performances-for-machine-learning-training-pytorch</link>
      <description><![CDATA[由于 Linux 的硬件兼容性问题，我目前无法在 Windows 上工作。由于我使用的大多数软件都是本机 Linux CLI 工具，因此我正在考虑使用 WSL 2 构建一个开发环境。我的问题是：如果我选择在 WSL 2 中工作，预计性能会下降多少？
更具体地说：我主要需要为机器学习应用程序编写和运行 python 代码，因此我预计 CPU 和 Nvidia GPU 都会承受繁重的本地工作负载。因为我需要进行认真的神经网络训练，所以性能对我来说是一件大事。最初，由于 WSL 2 是虚拟化的一种形式，我放弃了它作为一种选择，就像我放弃了使用 GPU 直通来启动 VM 的想法一样；但后来我了解到 WSL 2 应该是“类型 1 管理程序”1，这应该意味着更好的性能。
所以我想我的完整问题是：神经网络训练 (PyTorch) 的 WSL 2 性能与裸机上的 Windows 训练相比如何？
&lt;小时/&gt;
[1]：我认为我完全理解什么是“类型 1 虚拟机管理程序”。意味着...]]></description>
      <guid>https://stackoverflow.com/questions/78169143/how-good-are-wsl-2-performances-for-machine-learning-training-pytorch</guid>
      <pubDate>Fri, 15 Mar 2024 19:02:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 LSTM 进行异常检测的重构误差计算结果不一致</title>
      <link>https://stackoverflow.com/questions/78168724/inconsistent-results-in-reconstruction-error-calculation-for-anomaly-detection-w</link>
      <description><![CDATA[我在使用 LSTM 模型进行异常检测时遇到问题。尽管对具有不同分布的不同数据集进行了训练和测试，但我注意到攻击（测试）数据的重建误差存在变化。有人可以帮助我理解为什么会出现这种差异并提出潜在的解决方案吗？
我的异常检测系统遇到问题，特别是与重建误差的计算相关的问题。我使用 TensorFlow/Keras 实现了一个基于自动编码器的异常检测模型。该模型在包含正常和异常数据样本的数据集上进行训练。
我面临的问题是，每次运行模型并计算异常数据样本的重建误差（reconstruction_error_attack）时，我都会得到不同的结果。但是，我在运行之间没有对代码或数据集进行任何更改。
以下是我正在采取的步骤的摘要：

预处理数据：使用 StandardScaler 缩放数据并将其重塑为序列。
构建自动编码器模型：我使用的是基于 LSTM 的简单自动编码器架构。
训练模型：我在普通数据样本 (df_normal) 上训练自动编码器。
测试模型：我使用经过训练的自动编码器计算异常数据样本 (df_attack) 的重建误差。

每次计算异常数据样本的重建误差时，都会得到不同的结果。但是，在计算正常数据样本的重建误差（reconstruction_error_normal）时，我没有遇到这个问题。异常数据样本的重建误差是否有可能受到正常数据样本重建误差的影响？
我找不到静态的顶部和底部：dynamic_threshold_bottom、dynamic_threshold_top
我尝试根据攻击重建误差的百分位数定义动态阈值，然后识别高于和低于这些阈值的异常点。我希望将重建误差与动态阈值一起可视化，并相应地突出显示异常点。然而，得到的异常点似乎与我的预期不一致。
scaler = StandardScaler()
df_normal_scaled = 缩放器.fit_transform(df_normal)
df_attack_scaled = 缩放器.transform(df_attack)

# 将数据重塑为序列
def reshape_data(数据):
    返回 data.reshape(data.shape[0], 1, data.shape[1])
X_train = reshape_data(df_normal_scaled)
X_test = reshape_data(df_attack_scaled)
# 测试
defcalculate_reconstruction_error（模型，数据）：
    重建 = model.predict(data)
    误差 = np.mean(np.square(数据 - 重建), axis=(1,2))
    返回错误

Attack_reconstruction_error = 计算_reconstruction_error（自动编码器，X_test）

# 定义动态阈值
Dynamic_threshold_top = np.percentile(attack_reconstruction_error, 99)
Dynamic_threshold_bottom = np.percentile(attack_reconstruction_error, 30)
#dynamic_threshold_top = np.percentile(attack_reconstruction_error, 70)
#dynamic_threshold_bottom = np.percentile(attack_reconstruction_error, 1)
# 寻找异常点
anomaly_points_top = np.where（attack_reconstruction_error＆gt;dynamic_threshold_top）[0]
anomaly_points_bottom = np.where(attack_reconstruction_error 
哪个顶部和底部是正确的？]]></description>
      <guid>https://stackoverflow.com/questions/78168724/inconsistent-results-in-reconstruction-error-calculation-for-anomaly-detection-w</guid>
      <pubDate>Fri, 15 Mar 2024 17:32:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在 ML 中为基于文本的数据创建管道？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78168474/how-to-create-a-pipeline-for-text-based-data-in-ml</link>
      <description><![CDATA[我想为机器学习中基于文本的数据集编写一个管道。我不明白如何写以及用什么来获得它？
有人可以帮我解决这个问题吗？
我尝试使用普通模型
我想要这样，但它来自数字数据]]></description>
      <guid>https://stackoverflow.com/questions/78168474/how-to-create-a-pipeline-for-text-based-data-in-ml</guid>
      <pubDate>Fri, 15 Mar 2024 16:43:47 GMT</pubDate>
    </item>
    <item>
      <title>Fabric Notebook 在使用 TSfresh 进行特征提取时遇到错误 [关闭]</title>
      <link>https://stackoverflow.com/questions/78167696/fabric-notebook-running-into-error-while-using-tsfresh-for-feature-extraction</link>
      <description><![CDATA[当我尝试调用 tsfresh 的函数 extract_features 时，我不断遇到相同的错误/警告：
请确保您通过以下方式将环境 EnvConfigs 传递给工作人员
调用“set_mlflow_env_config”以正确触发工作人员的 mlflow。

这特别奇怪，因为此时我什至还没有导入 mlflow
我尝试过在导入和不导入 MLflow 的情况下运行它，两者都会产生相同的结果。在文档中，我似乎找不到名为“set_mlflow_env_config”的函数我很迷失。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78167696/fabric-notebook-running-into-error-while-using-tsfresh-for-feature-extraction</guid>
      <pubDate>Fri, 15 Mar 2024 14:35:49 GMT</pubDate>
    </item>
    <item>
      <title>Generative Ai API 获取响应时出错</title>
      <link>https://stackoverflow.com/questions/78167229/generative-ai-apis-error-in-getting-response</link>
      <description><![CDATA[我在 Generatve Ai 、HuggingFace api 密钥 google/flan-base 模型中生成响应时遇到错误
这是我的代码和最后一个单元格错误
导入操作系统
导入 json
将 pandas 导入为 pd
导入回溯
从 langchain 导入 PromptTemplate、HuggingFaceHub、LLMChain
从 langchain.chains 导入 SequentialChain
从 dotenv 导入 load_dotenv

加载_dotenv()
key = os.getenv(“hugging_face_key”)
os.environ[&#39;HUGGINGFACEHUB_API_TOKEN&#39;] = key
llm = HuggingFaceHub(repo_id=&#39;google/flan-t5-base&#39;, model_kwargs={&#39;温度&#39;: 0.5})

RESPONSE_JSON = {
    “1”：{
        &quot;mcq&quot;: &quot;多项选择题&quot;,
        “选项”：{
            “a”：“选择此处”，
            “b”：“选择此处”，
            “c”：“选择此处”，
            “d”：“选择此处”，
        },
        “正确”：“正确答案”，
    },
    “2”：{
        &quot;mcq&quot;: &quot;多项选择题&quot;,
        “选项”：{
            “a”：“选择此处”，
            “b”：“选择此处”，
            “c”：“选择此处”，
            “d”：“选择此处”，
        },
        “正确”：“正确答案”，
    },
    “3”：{
        &quot;mcq&quot;: &quot;多项选择题&quot;,
        “选项”：{
            “a”：“选择此处”，
            “b”：“选择此处”，
            “c”：“选择此处”，
            “d”：“选择此处”，
        },
        “正确”：“正确答案”，
    },
}

模板=“”“
文本：{文本}
您是 MCQ 专家。鉴于上述文字，您的工作是\
以 {tone} 语气为 {subject} 学生创建一个包含 {number} 个多项选择题的测验，
确保问题不重复，并检查所有问题是否与文本相符。
确保按照下面的 RESPONSE_JSON 格式设置您的响应并将其用作指南。 \
确保进行 {number} 个 MCQ
### RESPONSE_JSON
{响应_json}
”“”

quiz_ Generation_prompt = 提示模板(
    input_variables=[&quot;文本&quot;,&quot;数字&quot;,&quot;主题&quot;,&quot;语气&quot;,&quot;re​​sponse_json&quot;],
    模板=模板
）
quiz_chain = LLMChain(llm=llm，prompt=quiz_ Generation_prompt，output_key=“quiz”，verbose=True)

模板2 =“”“
您是一位专业的英语语法学家和作家。为 {subject} 学生提供多项选择测验。\
您需要评估问题的复杂性并对测验进行完整的分析。出于复杂性考虑，最多只能使用 50 个单词
如果测验不符合学生的认知和分析能力，\
更新需要更改的测验问题并更改语气，使其完全适合学生的能力。
测验_MCQ：
{测验}

上述测验的专家英语作家检查：
”“”
    
quiz_evaluation_prompt = PromptTemplate(input_variables=[“主题”,“测验”], template=TEMPLATE2)
review_chain = LLMChain(llm=llm，prompt=quiz_evaluation_prompt，output_key=“审阅”，verbose=True)

generate_evaluate_chain = SequentialChain(chains=[quiz_chain, review_chain], input_variables=[&quot;text&quot;,&quot;number&quot;,&quot;subject&quot;,&quot;tone&quot;,&quot;re​​sponse_json&quot;],
                                          output_variables=[“测验”,“评论”], verbose=True)

# 假设您之前已经定义了“file_path”
以 open(file_path, &#39;r&#39;) 作为文件：
    文本 = 文件.read()

json.dumps(RESPONSE_JSON)

数量 = 5
主题=“机器学习”
TONE =“简单”

# 执行generate_evaluate_chain并将结果存储在&#39;response&#39;中
响应=generate_evaluate_chain.run（文本=文本，数字=数字，主题=主题，音调=音调，response_json=json.dumps（RESPONSE_JSON））



我最后一个单元格中的错误
当不存在一个输出键时，不支持`run`。
得到\[&#39;测验&#39;、&#39;评论&#39;\]。

尝试了不同的功能并做了一些研究，但没有用]]></description>
      <guid>https://stackoverflow.com/questions/78167229/generative-ai-apis-error-in-getting-response</guid>
      <pubDate>Fri, 15 Mar 2024 13:21:49 GMT</pubDate>
    </item>
    <item>
      <title>如何在Android Studio中实现实时tflite模型？</title>
      <link>https://stackoverflow.com/questions/78165517/how-to-implement-realtime-tflite-model-in-android-studio</link>
      <description><![CDATA[我正在尝试在移动应用程序上进行实时模型实现。我在 Teachable Machine 中训练了模型并将其导出为 model_unquanted.tflite。当我将其导入 Android Studio 时，它会提供以下 Kotlin 代码来实现它：
val model = ModelUnquant.newInstance(context)

// 创建输入以供参考。
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 224, 224, 3), DataType.FLOAT32)
inputFeature0.loadBuffer(byteBuffer)

// 运行模型推理并获取结果。
val 输出 = model.process(inputFeature0)
valoutputFeature0=outputs.outputFeature0AsTensorBuffer

// 如果不再使用则释放模型资源。
模型.close()

以下是我对实时更新的实现：
 覆盖 fun onSurfaceTextureUpdated(p0: SurfaceTexture) {
            位图=textureView.bitmap！！
            val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 224, 224, 3), DataType.FLOAT32)

            val byteBuffer: ByteBuffer = ByteBuffer.allocate(224* 224* 3)
            byteBuffer.rewind()

            inputFeature0.loadBuffer(byteBuffer)

            val 输出 = model.process(inputFeature0)
            valoutputFeature0=outputs.outputFeature0AsTensorBuffer

            var 可变 = bitmap.copy(Bitmap.Config.ARGB_8888, true)
            val canvas = android.graphics.Canvas(可变)

            val h = mutable.height
            val w = 可变的.width

            val xPosition = 10 // 根据需要调整该值
            val yPosition = 30 // 根据需要调整该值

            canvas.drawText（outputFeature0.toString（），xPosition.toFloat（），yPosition.toFloat（），绘画）

            imageView.setImageBitmap(可变)

        }

logcat 错误：
致命异常：main
进程：com.example.tflite_realtime，PID：14671
java.lang.IllegalArgumentException：字节缓冲区的大小和形状不匹配。
]]></description>
      <guid>https://stackoverflow.com/questions/78165517/how-to-implement-realtime-tflite-model-in-android-studio</guid>
      <pubDate>Fri, 15 Mar 2024 08:22:05 GMT</pubDate>
    </item>
    <item>
      <title>加载共享库时出错：libonnxruntime.so.1.7.0：无法打开共享对象文件：没有这样的文件或目录</title>
      <link>https://stackoverflow.com/questions/78165433/error-while-loading-shared-libraries-libonnxruntime-so-1-7-0-cannot-open-share</link>
      <description><![CDATA[当我在终端中运行它时，如下所示，出现此错误
nizhar@nizhar-desktop:~/Documents$ g++ -std=c++11 tespredict.cpp -o Predictx -I/usr/local/include/onnxruntime/include -L/usr/local/lib -lonnx运行时
nizhar@nizhar-desktop:~/文档$ ./predictx
./predictx：加载共享库时出错：libonnxruntime.so.1.7.0：无法打开共享对象文件：没有这样的文件或目录


我的 onnxrune-time 库路径位于
&lt;前&gt;&lt;代码&gt;/usr/local/lib
/usr/local/include/onnxruntime

我还在.bashrc中添加了
export ONNXRUNTIME_DIR=“/usr/local/include/onnxruntime/include”
导出 LD_LIBRARY_PATH=“/usr/local/lib:$LD_LIBRARY_PATH”

并且已经这样做了
源 ~/.bashrc

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78165433/error-while-loading-shared-libraries-libonnxruntime-so-1-7-0-cannot-open-share</guid>
      <pubDate>Fri, 15 Mar 2024 08:04:05 GMT</pubDate>
    </item>
    <item>
      <title>如何重塑sklearn IncrementalPCA的结果（ipca.transform）</title>
      <link>https://stackoverflow.com/questions/78164669/how-to-reshape-the-result-of-sklearns-incrementalpca-ipca-transform</link>
      <description><![CDATA[我有一个巨大的 numpy 数组，我需要使用 sklearn 的 IncrementalPCA 对其进行转换。
该数组是 (22, 258186260)。所以我将它分为两​​个轴，如下面的代码。
首先）我不确定它是否允许使用第一轴和第二轴划分输入。或者它应该仅使用第一个轴。
第二）如果正确，如何调整“ipca.transform”的输出
 dset = h5f[&#39;测试&#39;]
               
               ds0, ds1 = dset.shape[0], dset.shape[1]
             
               n = dset .shape[0] # 数据集中有多少行
               chunk_size = 2#1000 # 我们一次向 IPCA 提供多少行，n 的除数
                  

               for i in range(0, n//chunk_size):# forpartial_fit
               
                   对于 np.split( dset[i*chunk_size : (i+1)*chunk_size] , 10,axis=1) 中的批次：
                         pca1.partial_fit（批量）

               my_out_all=[]
               for i in range(0, n//chunk_size): # 用于变换
                  
                   对于 np.split( dset[i*chunk_size : (i+1)*chunk_size], 10,axis=1) 中的批次：
                         
                         my_out=pca1.transform(批处理)
                         
                         my_out_all.append (my_out) # 如何调整输出
]]></description>
      <guid>https://stackoverflow.com/questions/78164669/how-to-reshape-the-result-of-sklearns-incrementalpca-ipca-transform</guid>
      <pubDate>Fri, 15 Mar 2024 04:26:23 GMT</pubDate>
    </item>
    <item>
      <title>比较两个名字的相似度并使用神经网络识别重复项</title>
      <link>https://stackoverflow.com/questions/72914328/compare-similarity-of-two-names-and-identify-duplicates-with-neural-network</link>
      <description><![CDATA[我有一个包含成对名称的数据集，它看起来像这样：
&lt;前&gt;&lt;代码&gt;ID；姓名1；姓名2
1;迈克·米勒；迈克·米勒
2；约翰·多伊；皮特·麦吉伦
3；萨拉·约翰逊；伊迪塔·约翰逊
4；约翰·莱蒙德-李·彼得；约翰·LL.彼得
5；玛塔·桑兹；玛莎桑德
6；约翰·彼得；约翰娜·彼得拉
7；乔安娜·内姆齐克；乔安娜·尼姆齐克

我有一些案例，已贴上标签。所以我手动检查它们并确定它们是否重复。这些情况下的手动判断将是：
1：是重复的
2：不重复
3：不重复
4：是重复的
5：不重复
6：不重复
7：是重复的

（第七个案例是一个具体案例，因为这里语音也参与了游戏。但这不是主要问题，我可以忽略语音。）
第一种方法是计算每对的编辑距离并将其标记为重复项，其中编辑距离例如小于或等于 2。这将导致以下输出：
1：编辑距离：2 =&gt;复制
2：编辑距离：11 =&gt;不是重复的
3：编辑距离：4 =&gt;不是重复的
4：编辑距离：8 =&gt;不是重复的
5：编辑距离：2 =&gt;复制
6：编辑距离：4 =&gt;不是重复的
7：编辑距离：2 =&gt;复制

这将是一种使用“固定”的方法。基于Levinshtein距离的算法。
现在，我想使用神经网络/机器学习来完成此任务：
我不需要神经网络来检测语义相似性，例如“医院”和“临床”。然而，我想避免 Levenshtein 距离，因为我希望 ML 算法能够检测“John Lemond-Lee Peter”。和“约翰·LL。彼得”作为潜在的重复，也不是 100% 确定。在这种情况下，编辑距离会导致相对较高的数字 (8)，因为需要添加相当多的字符。在像“约翰·彼得”这样的情况下，和“约翰娜彼得拉”编辑距离会导致较小的数字 (4)，但这实际上不是重复的，对于这种情况，我希望 ML 算法能够检测到这可能不是重复的。所以我需要 ML 算法来“学习我需要检查重复项的方式”。通过我的标签，我将作为输入给出 ML 算法我想要的方向。
我实际上认为这对于机器学习算法/神经网络来说应该是一个简单的任务，但我不确定。
如何实现神经网络来比较名称对并识别重复项，而不使用显式距离度量（例如编辑距离、欧几里德距离等）？
我认为可以将字符串转换为数字，并且神经网络可以处理它并学习根据我的标签风格检测重复项。因此不必指定距离度量。 我想到了一个人：我会把这个任务交给一个人，这个人会判断并做出决定。此人对编辑距离或任何其他数学概念一无所知。所以我只是想训练神经网络学会做人类正在做的事情。当然，每个人都是不同的，这也取决于我的标签。
（编辑：到目前为止我见过的机器学习/神经网络解决方案（例如this) 使用像 levenshtein 这样的度量作为特征输入。但正如我所说，我认为应该可以教会神经网络“学习”。 “人类判断”而不使用这样的距离度量？关于我的具有名称对的具体情况：使用编辑距离作为特征的 ML 方法有什么好处？因为它只会将这些名称对检测为具有低编辑距离的重复。因此，如果两个名称之间的编辑距离小于 x，我可以使用一个简单的算法将一对标记为重复。为什么要使用 ML，额外的好处是什么？）]]></description>
      <guid>https://stackoverflow.com/questions/72914328/compare-similarity-of-two-names-and-identify-duplicates-with-neural-network</guid>
      <pubDate>Fri, 08 Jul 2022 16:25:04 GMT</pubDate>
    </item>
    </channel>
</rss>