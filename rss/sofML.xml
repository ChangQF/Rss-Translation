<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 08 Jan 2025 09:18:18 GMT</lastBuildDate>
    <item>
      <title>如何从 fit_resamples 和超参数调整中获取训练误差？</title>
      <link>https://stackoverflow.com/questions/79338394/how-to-get-the-training-error-from-fit-resamples-and-hyperparameter-tuning</link>
      <description><![CDATA[在交叉验证期间，fit_resamples 返回验证集中度量的平均值。
lr_model &lt;-
linear_reg() |&gt;
set_engine(&#39;lm&#39;)

lr_wf &lt;-
working() |&gt;
add_recipe(basic_recipe) |&gt;
add_model(lr_model)

lr_cv &lt;-
lr_wf |&gt;
fit_resamples(
folds,
metrics = metric_set(rmse),
control = control
)

# 让我们从 CV 中提取结果。这将有助于我们将其与其他模型进行比较
lr_cv |&gt;
collect_metrics()
# 这是 RMSE 验证错误
# .metric .estimator mean n std_err .config
# &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; 
# rmse standard 0.161 10 0.000370 Preprocessor1_Model1

我遇到的问题是如何获取训练误差。
在调整超参数后也会出现同样的问题。
例如，在调整 KNN 以找到最佳邻居数时，collect_metrics 和 show_best 会返回来自交叉验证的验​​证集指标的平均值，而我们都知道，最佳邻居数是当训练误差减少而验证误差开始增加时。
不幸的是，autoplot 函数不显示训练误差，只显示验证误差。
在这种情况下，例如
tree_grid &lt;-
grid_regular(
cost_complexity(),
tree_depth(),
min_n(),
levels = c(3, 5, 10)
)

tree_wf &lt;-
working() %&gt;%
add_model(tree_model) %&gt;%
add_recipe(basic_recipe)

tree_res &lt;- 
tree_wf %&gt;%
tune_grid(
resamples = folds,
grid = tree_grid,
metrics = metric_set(rmse),
control = control
)

如何提取每对超参数/折叠的训练误差？]]></description>
      <guid>https://stackoverflow.com/questions/79338394/how-to-get-the-training-error-from-fit-resamples-and-hyperparameter-tuning</guid>
      <pubDate>Wed, 08 Jan 2025 08:23:55 GMT</pubDate>
    </item>
    <item>
      <title>构建开源聊天机器人的工具和技术[关闭]</title>
      <link>https://stackoverflow.com/questions/79338115/tools-and-technologies-for-building-open-source-chatbots</link>
      <description><![CDATA[目前在一家公司实习，该项目是构建聊天机器人以与客户网站集成。请推荐完全开源的工具和技术。
到目前为止，我已经尝试过 rasa、Botpress 和 dialogflow。而且我也怀疑这些是否是开源的。]]></description>
      <guid>https://stackoverflow.com/questions/79338115/tools-and-technologies-for-building-open-source-chatbots</guid>
      <pubDate>Wed, 08 Jan 2025 06:15:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用带有噪声传感器数据的 ML.NET 峰值检测来检测燃料盗窃和加注？</title>
      <link>https://stackoverflow.com/questions/79338109/how-to-detect-fuel-theft-and-filling-using-ml-net-spike-detection-with-noisy-sen</link>
      <description><![CDATA[我正在开展一个项目，使用 ML.NET 从嘈杂的传感器数据中检测燃油盗窃和加油事件。传感器数据包括带时间戳的燃油液位和速度读数，但通常包含噪音，这使得检测过程具有挑战性。
我正在使用 ML.NET 提供的 DetectSpikeBySsa 方法实现峰值检测算法。下面是我的代码的简化版本：
public static void DetectSpikeBySsa(IReadOnlyList&lt;SensorData&gt; data
, double confidence
, int pvalueHistoryLength
, int trainingWindowSize
, int seasonalityWindowSize)
{
var context = new MLContext();
var dataView = context.Data.LoadFromEnumerable(data);
var pipeline = context.Transforms.DetectSpikeBySsa(
outputColumnName: nameof(AnomalyPrediction.Prediction)
, inputColumnName: nameof(SensorData.Value)
, confidence: confidence // 99.0
, pvalueHistoryLength: pvalueHistoryLength // data.Count / 2
, trainingWindowSize: trainingWindowSize // data.Count
, seasonalityWindowSize: seasonalityWindowSize // data.Count / 4);

var model = pipeline.Fit(dataView);
var perceivedData = model.Transform(dataView);

List&lt;AnomalyDetectionResult&gt; results = [];
var predictions = context.Data.CreateEnumerable&lt;AnomalyPrediction&gt;(transformedData, repeatRowObject: false).ToList();
for (int i = 0; i &lt; predictions.Count; i++)
{
var sensorData = data[i];
var prediction = predictions[i];
var isAnomaly = prediction.Prediction[0] is 1;
Console.WriteLine($&quot;{prediction.Prediction[0]}\t {prediction.Prediction[1]:f2} \t {prediction.Prediction[2]:f2}&quot;);
results.Add(new AnomalyDetectionResult
{
IsAnomaly = isAnomaly,
Value = sensorData.Value,
Speed = sensorData.Speed,
Timestamp = sensorData.Timestamp
});

if (!isAnomaly)
{
// Todo
}
}
}

目标是：

当燃油液位突然下降时检测燃油盗窃。
当燃油液位突然升高时检测燃油填充。
有效处理传感器数据中的噪声以减少误报。

以下是我面临的一些挑战：

针对噪声数据（可能是高值或低值）微调置信度、pvalueHistoryLength、trainingWindowSize 和 seasonalityWindowSize 参数。
过滤掉噪声，同时确保检测到真正的峰值（盗窃/填充事件）。
区分有效异常（燃油盗窃或填充）和噪声引起的峰值。

问题：

如何优化 DetectSpikeBySsa 的参数以有效处理嘈杂的传感器数据？
在应用峰值检测之前，是否有任何预处理步骤或替代方法来更好地处理噪声？
是否有更合适的机器学习方法来检测燃油液位数据中的这些特定类型的异常？

我每 10 分钟检查一次燃油液位，确保每次检测的数据集大小始终大于或等于 40 条记录。但是，数据通常包含噪声，使检测过程具有挑战性。
任何建议、见解或示例都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79338109/how-to-detect-fuel-theft-and-filling-using-ml-net-spike-detection-with-noisy-sen</guid>
      <pubDate>Wed, 08 Jan 2025 06:11:12 GMT</pubDate>
    </item>
    <item>
      <title>在网格搜索中使用 sklearn 特征选择器来评估所有特征的有用性的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/79337434/whats-the-best-way-to-use-a-sklearn-feature-selector-in-a-grid-search-to-evalu</link>
      <description><![CDATA[我正在训练一个 sklearn 分类器，并在管道中插入一个特征选择步骤。通过网格搜索，我想确定允许我最大化性能的特征数量。不过，我想在网格搜索中探索没有特征选择，只有“直通”的可能性步骤，是最大化性能的最佳选择。
这是一个可重现的示例：
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

# 加载泰坦尼克号数据集
titanic = sns.load_dataset(&#39;titanic&#39;)

# 选择特征和目标
features = [&#39;age&#39;, &#39;fare&#39;, &#39;sex&#39;]
X = titanic[features]
y = titanic[&#39;survived&#39;]

# 预处理管道数字和分类特征
numeric_features = [&#39;age&#39;, &#39;fare&#39;]
numeric_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;)),
(&#39;scaler&#39;, StandardScaler())
])

categorical_features = [&#39;sex&#39;]
categorical_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;)),
(&#39;onehot&#39;, OneHotEncoder(drop=&#39;first&#39;))
])

# 合并预处理步骤
preprocessor = ColumnTransformer(transformers=[
(&#39;num&#39;, numeric_transformer, numeric_features),
(&#39;cat&#39;, categorical_transformer, categorical_features)
])

# 初始化分类器和特征选择器
clf = LogisticRegression(max_iter=1000,solver=&#39;liblinear&#39;)
sfs = SequentialFeatureSelector(clf, direction=&#39;forward&#39;)

# 创建一个包含预处理、特征选择和分类的管道
pipeline = Pipeline(steps=[
(&#39;preprocessor&#39;, preprocessor),
(&#39;feature_selection&#39;, sfs),
(&#39;classifier&#39;, clf)
])

# 定义要搜索的参数网格
param_grid = {
&#39;feature_selection__n_features_to_select&#39;: [2],
&#39;classifier__C&#39;: [0.1, 1.0, 10.0], # 正则化强度
}

# 创建并运行网格搜索
grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X, y)

# 输出最佳参数和分数
print(&quot;找到最佳参数：&quot;, grid_search.best_params_)
print(&quot;最佳交叉验证分数：&quot;, grid_search.best_score_)

X 此处有三个特征（即使在 预处理器 步骤之后），但上面的网格搜索代码不允许探索使用所有 3 个特征的模型，因为设置
feature_selection__n_features_to_select: [2,3]

将给出 ValueError: n_features_to_select 必须是 &lt; n_features。
这里的障碍是 SequentialFeatureSelector 不将所有特征的选择（又名直通选择器）视为有效的特征选择。
换句话说，我想运行网格搜索，同时考虑
(&#39;feature_selection&#39;, &#39;passthrough&#39;)

在可能的管道配置空间中的设置。有没有一种惯用的/好的方法可以做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/79337434/whats-the-best-way-to-use-a-sklearn-feature-selector-in-a-grid-search-to-evalu</guid>
      <pubDate>Tue, 07 Jan 2025 21:39:17 GMT</pubDate>
    </item>
    <item>
      <title>AdaBoostClassifier：test_size=0.25 的指标完美，但其他值的样本误差不一致</title>
      <link>https://stackoverflow.com/questions/79337075/adaboostclassifier-perfect-metrics-with-test-size-0-25-but-inconsistent-sample</link>
      <description><![CDATA[我正在使用 AdaBoostClassifier 和弱学习器 (DecisionTreeClassifier) 对数据集进行分类。数据集有 7857 个样本：
X.shape
# 输出：(7857, 5)

y.shape
# 输出：(7857,)

以下是拆分数据集和训练模型的代码：

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.25, random_state=28
)

weak_learner = DecisionTreeClassifier(max_depth=1)

adb = AdaBoostClassifier(estimator=weak_learner, n_estimators=50, random_state=42)
adb_model = adb.fit(X_train, y_train)

y_pred = adb_model.predict(X_test)
print(classification_report(y_test, y_pred))

当我使用 test_size=0.25 运行此代码时，所有类别的分类指标输出均为 100%：
 precision recall f1-score support

Cheap 1.00 1.00 1.00 496
Expensive 1.00 1.00 1.00 506
Reasonable 1.00 1.00 1.00 963

accuracy 1.00 1965
macro avg 1.00 1.00 1.00 1965
weighted avg 1.00 1.00 1.00 1965

这不可能是真的，因为我的数据点并不是完全可分离的。 （我用图表检查过）
但是，当我将 test_size 更改为任何其他值（例如 0.3、0.2）时，我收到以下错误：
ValueError：发现输入变量的样本数不一致


我已检查的内容：

确保 X 和 y 具有相同数量的样本。
确认 X 或 y 中没有缺失值。


问题：

为什么 test_size=0.25产生完美的指标，但其他 test_size 值会导致错误？
如何解决此问题以使用不同的 test_size 值？
]]></description>
      <guid>https://stackoverflow.com/questions/79337075/adaboostclassifier-perfect-metrics-with-test-size-0-25-but-inconsistent-sample</guid>
      <pubDate>Tue, 07 Jan 2025 19:00:22 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Turing.jl Julia 中使用通过核密度估计 (KDE) 估计的分布作为先验</title>
      <link>https://stackoverflow.com/questions/79336637/how-to-use-a-distribution-estimated-via-kernel-density-estimation-kde-as-a-pri</link>
      <description><![CDATA[背景和目标
我有一组离散样本（例如，来自一些现有数据集或后验）。我使用核密度估计 (KDE) 方法（例如通过 KernelDensity.jl）从这些样本中估计出连续分布。现在，我想将基于 KDE 的分布用作 Turing.jl 中实现的贝叶斯模型中某些参数的先验。
我已查看了 Turing 的高级用法官方文档 (https://turing.ml/dev/docs/using-turing/advanced)，但我并不完全清楚如何将自定义的基于 KDE 的分布作为先验，尤其是关于 Bijectors.jl 可能需要某些转换才能与 Turing.jl 的采样算法（如 NUTS）集成。
问题
• 如何获取经验或离散数据集，通过 KDE 估计分布，然后直接在 Turing.jl 中使用该基于 KDE 的分布作为先验？
• 需要哪些具体的实施步骤（即 pdf、logpdf、rand 以及可能的自定义双射器的定义）来确保 Turing.jl 的 MCMC（特别是 NUTS）能够无错误地处理这个基于 KDE 的自定义先验？
任何实用的代码示例或最佳实践指导都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/79336637/how-to-use-a-distribution-estimated-via-kernel-density-estimation-kde-as-a-pri</guid>
      <pubDate>Tue, 07 Jan 2025 16:14:31 GMT</pubDate>
    </item>
    <item>
      <title>对呼叫中心呼叫的原始音频文件（而非记录）进行情感分析[关闭]</title>
      <link>https://stackoverflow.com/questions/79335686/performing-sentiment-analysis-on-raw-audio-files-not-transcripts-for-call-cent</link>
      <description><![CDATA[我正在使用原始音频文件对客户呼叫中心对话进行情绪分析。我使用 Ravdess、TESS 和 SAVEE 等数据集对 Wav2VecForSequenceClassification（来自 Hugging Face）进行了微调，这些数据集包含简短片段（约 3-4 秒）的专业录音。但是，该模型在处理更长、更真实的呼叫中心音频文件时会遇到困难。
我怀疑训练数据（简短、干净的音频）与真实世界测试数据（较长、嘈杂且有重叠语音的音频）之间的不匹配是主要问题。
以下是我目前所做的工作：

对预先训练的 Wav2VecForSequenceClassification 模型进行了微调。
使用 Ravdess、TESS 和 SAVEE 等数据集进行训练。

当前挑战：

该模型不能很好地推广到较长的呼叫中心音频文件。
所使用的数据集在口音、语言和真实世界噪音方面缺乏多样性。

我的问题：

我如何更好地预处理或微调模型来处理更长、更嘈杂的呼叫中心对话？
是否有任何技术或最佳实践可用于对更长的音频序列进行训练，例如拆分、填充或段聚合？
是否有一种有效的方法可以将声学（原始波形）和语言（转录文本）特征结合起来进行情绪分析？
是否有任何公开可用的数据集，其中包含带有情绪标签的长、多样化且逼真的呼叫中心音频？

我希望得到有关提高模型性能的具体、可操作的建议。]]></description>
      <guid>https://stackoverflow.com/questions/79335686/performing-sentiment-analysis-on-raw-audio-files-not-transcripts-for-call-cent</guid>
      <pubDate>Tue, 07 Jan 2025 10:41:02 GMT</pubDate>
    </item>
    <item>
      <title>将机器学习模型集成到 Windows 窗体中的问题</title>
      <link>https://stackoverflow.com/questions/79335677/problem-with-integrating-the-machine-learning-model-into-windows-forms</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79335677/problem-with-integrating-the-machine-learning-model-into-windows-forms</guid>
      <pubDate>Tue, 07 Jan 2025 10:38:45 GMT</pubDate>
    </item>
    <item>
      <title>检测器模型中框的正确损失函数</title>
      <link>https://stackoverflow.com/questions/79331211/correct-loss-function-for-bboxes-in-a-detector-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79331211/correct-loss-function-for-bboxes-in-a-detector-model</guid>
      <pubDate>Sun, 05 Jan 2025 17:31:42 GMT</pubDate>
    </item>
    <item>
      <title>如何清除 doctr 模型使用的 gpu 内存</title>
      <link>https://stackoverflow.com/questions/78768994/how-to-clear-gpu-memory-used-by-doctr-model</link>
      <description><![CDATA[我正在使用 doctr 进行 ocr 操作。doctr 模型在 gpu 中运行，因此在我完成 ocr 工作后，它仍然加载在 gpu 内存中。
我正在尝试使用
with torch.no_grad(): torch.cuda.empty_cache()
清除 gpu 内存，但没有任何效果。
有没有其他方法可以在不使用 PID 终止进程的情况下清除 gpu 内存？
我想使用 doctr 对图像执行 ocr]]></description>
      <guid>https://stackoverflow.com/questions/78768994/how-to-clear-gpu-memory-used-by-doctr-model</guid>
      <pubDate>Fri, 19 Jul 2024 10:59:26 GMT</pubDate>
    </item>
    <item>
      <title>安装 pycoral 会导致“未满足的依赖项”错误，尽管依赖项已得到满足</title>
      <link>https://stackoverflow.com/questions/78593719/installing-pycoral-results-in-unmet-dependencies-error-despite-dependencies-be</link>
      <description><![CDATA[我在 Chromebook 上通过 sudo apt-get install python-pycoral 安装 pycoral 时出现以下错误
以下软件包具有未满足的依赖项：
python3-pycoral：依赖项：python3-tflite-runtime（= 2.5.0.post1），但不会安装
依赖项：python3（&lt; 3.10），但要安装 3.11.2-1+b1
E：无法更正问题，您持有损坏的软件包。

我确认我的设置符合列出的要求，但 pycoral 安装程序似乎忽略了 tflite（出现在 pip list 中）以及当前的 python 版本。
如果您有这方面的经验，我将不胜感激任何解决此问题的帮助。从“未满足的依赖项”错误的怪异性来看，我感觉 tflite 运行时和 python 版本之外的某些东西存在问题]]></description>
      <guid>https://stackoverflow.com/questions/78593719/installing-pycoral-results-in-unmet-dependencies-error-despite-dependencies-be</guid>
      <pubDate>Fri, 07 Jun 2024 19:28:31 GMT</pubDate>
    </item>
    <item>
      <title>在 docker 中运行 doctr ml 模型永远挂起</title>
      <link>https://stackoverflow.com/questions/77552150/running-doctr-ml-model-in-docker-hangs-forever</link>
      <description><![CDATA[我有一个 fastapi 应用程序，它在代码中加载 doctr 模型，它只需要一个图像路径并将其转换为文本
这是代码
从 doctr.io 导入 DocumentFile
从 doctr.models 导入 o​​cr_predictor
__version__ = &quot;0.1.1&quot;
model = ocr_predictor(pretrained=True)

def process_image(image_path):
document = DocumentFile.from_images(image_path)
result = model(document)
json_response = result.export()
return json_response

我只想通过 API 公开模型。
main.py 文件，其中设置了我的 api 代码


from app.model.model import __version__ as model_version
from app.model.model import process_image
from fastapi import FastAPI, HTTPException, UploadFile

app = FastAPI()

@app.get(&quot;/&quot;)
def home():
return {&quot;health_check&quot;: &quot;OK&quot;, &quot;model_version&quot;: &#39;model_version&#39;}

当我使用此代码运行 main.py 时代码
uvicorn app.main:app --reload
一切正常
但当我尝试将其 dockerize 时，它​​就永远挂起了，这是我的 Dockerfile
来自 tiangolo/uvicorn-gunicorn-fastapi:python3.9

运行 apt-get update

运行 apt install -y libgl1-mesa-glx

复制 ./requirements.txt /app/requirements.txt

运行 pip install --no-cache-dir --upgrade -r /app/requirements.txt

复制 ./app /app/app

注意：我有一台 m1 mac
我尝试了一切方法使其正常工作，包括将导入移动到函数内部
__version__ = &quot;0.1.1&quot;

def process_image(image_path):
from doctr.io import DocumentFile
from doctr.models import ocr_predictor
model = ocr_predictor(pretrained=True)
document = DocumentFile.from_images(image_path)
result = model(document)
json_response = result.export()
return json_response

但是，当 api 到达导入时，什么都不起作用，它只是永远挂起，而且只有在使用 docker 时才会出现这种情况]]></description>
      <guid>https://stackoverflow.com/questions/77552150/running-doctr-ml-model-in-docker-hangs-forever</guid>
      <pubDate>Sun, 26 Nov 2023 14:46:54 GMT</pubDate>
    </item>
    <item>
      <title>如何在图像的特定区域运行 mindee doctr？</title>
      <link>https://stackoverflow.com/questions/75916342/how-to-run-mindee-doctr-on-specific-area-of-image</link>
      <description><![CDATA[我使用下面的代码通过 mindee/doctr 包从图像中提取文本。
from doctr.models import  ocr_predictor
from doctr.io import DocumentFile

import json
model = ocr_predictor( reco_arch=&#39;crnn_vgg16_bn&#39;, pretrained=True,export_as_straight_boxes=True)
image = DocumentFile.from_images(&quot;docs/temp.jpg&quot;)
result = model(image)
result_json = result.export()
json_object =  json.dumps(result_json, indent=4)

with open(&quot;temp.json&quot;, &quot;w&quot;) as outfile:
   outfile.write(json_object)
result.show(image)

通过使用 mindee/doctr，我从 temp.json 文件中的图像中获取了整个文本。
OCR 后的输出：

我想从图像中获取特定区域的文本。
有什么好方法可以实现这一点？]]></description>
      <guid>https://stackoverflow.com/questions/75916342/how-to-run-mindee-doctr-on-specific-area-of-image</guid>
      <pubDate>Mon, 03 Apr 2023 05:47:13 GMT</pubDate>
    </item>
    <item>
      <title>批量数据在线方差更新的有效算法</title>
      <link>https://stackoverflow.com/questions/75545944/efficient-algorithm-for-online-variance-update-over-batched-data</link>
      <description><![CDATA[我有大量多维数据，想计算所有数据中某个轴的方差。从内存角度来看，我无法创建一个大型数组来一步计算方差。因此，我需要分批加载数据，并需要在每个批次之后以在线方式更新当前方差。
示例
最后，按批次更新的 online_var 应该与 correct_var 匹配。
但是，我很难找到一种有效的算法。
import numpy as np
np.random.seed(0)
# 正确计算方差
all_data = np.random.randint(0, 9, (9, 3)) # &lt;-- 不适合内存
correct_var = all_data.var(axis=0)
# 创建批次
batches = all_data.reshape(-1, 3, 3)

online_var = 0
for batch in batches:
batch_var = batch.var(axis=0)
online_var = ? # 如何正确更新
assert np.allclose(correct_var, online_var)


我找到了Welford 在线算法，但是它非常慢，因为它只更新单个新值的方差，即它不能一次处理整个批次。当我处理图像时，每个像素和每个通道都需要更新。

如何以有效的方式更新多个新观测值的方差，同时考虑整个批次？]]></description>
      <guid>https://stackoverflow.com/questions/75545944/efficient-algorithm-for-online-variance-update-over-batched-data</guid>
      <pubDate>Thu, 23 Feb 2023 14:10:26 GMT</pubDate>
    </item>
    <item>
      <title>增强周期性数据的机器学习模型</title>
      <link>https://stackoverflow.com/questions/60620596/enhance-a-machin-learning-model-for-periodic-data</link>
      <description><![CDATA[我正在通过 TensorFlow (tfjs) 学习 ML。
我的第一个测试是训练我的模型预测 cos(x) 作为 x 的函数（从 0 到 2*Math.PI*4，即 4 个周期）
特征：2000 个 x 值（随机）
标签：2000 个 cos(x) 值 
模型：
const model = tf.sequence({
layer: [
tf.layers.dense({ inputShape: [1], unit: 22, activated: &#39;tanh&#39; }),
tf.layers.dense({ unit: 1 }),
]
});

model.compile({
优化器：tf.train.adam(0.01)，
损失：&#39;meanSquaredError&#39;，
指标：[&#39;mae&#39;]
});

...

await model.fit(feature, label, {
epochs: 500,
validationSplit: 0.2,
})

结果相当“有趣”：

现在我想知道如何增强我的模型以适应 cos(x) 的周期性（不使用 cos(x) 的数学周期性，如 y = cos(x modulo 2PI) ）。
我的模型有可能“理解”存在周期性吗？]]></description>
      <guid>https://stackoverflow.com/questions/60620596/enhance-a-machin-learning-model-for-periodic-data</guid>
      <pubDate>Tue, 10 Mar 2020 15:17:18 GMT</pubDate>
    </item>
    </channel>
</rss>