<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 07 Dec 2023 01:01:15 GMT</lastBuildDate>
    <item>
      <title>将模型部署到 GCP 的 Vertex 时如何利用 L4 GPU</title>
      <link>https://stackoverflow.com/questions/77617088/how-to-utilize-an-l4-gpu-when-deploying-a-model-to-gcps-vertex</link>
      <description><![CDATA[我将一个模型部署到 GCP 上的 Vertex，用于部署它的配置代码如下所示：
dedicated_resources=dict(
    机器规格=字典（
        machine_type=“g2-standard-8”，
        Accelerator_type=“NVIDIA_L4”，
        Accelerator_count=1,
    ),
    min_replica_count = 2，
    最大副本数=10,
    autoscaling_metric_specs=[
        字典（
            metric_name=“aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle”，
            目标=20，
        ),
    ],
）


该模型配置为利用 GPU 资源，并且此配置已在 n1 计算机上使用 P4 和 P100 运行。当在具有 L4 加速器的 g2 机器上运行模型时，请求会导致极高的延迟和 CPU 利用率，而 GPU 利用率则稳定在 0%。
我不知道接下来该去哪里，也不知道最好的故障排除选项是什么。]]></description>
      <guid>https://stackoverflow.com/questions/77617088/how-to-utilize-an-l4-gpu-when-deploying-a-model-to-gcps-vertex</guid>
      <pubDate>Thu, 07 Dec 2023 00:51:07 GMT</pubDate>
    </item>
    <item>
      <title>从包含单列的每一行中的子列表的 DataFrame 中获取特征</title>
      <link>https://stackoverflow.com/questions/77616611/getting-features-from-a-dataframe-containing-sublists-in-every-row-of-a-single-c</link>
      <description><![CDATA[我正在完成一项机器学习的学校作业，但遇到了麻烦。我这里有我的数据框：
6列DataFrame，subject_id，task_code，data_lw，t_start，t_end&lt; /p&gt;
问题始于 data_lw 列。此列的每一行都有一个包含 1000 到 1600 行数据的三列子列表。
列中单个元素的示例
我遇到的问题是提取特征。我之前通过这样做提取了特征：
导入 pandas 作为 pd
导入 tsfel
 
cfg=tsfel.get_features_by_domain(&#39;统计&#39;)
特征列表=[]
对于 df[&#39;data_lw&#39;] 中的索引：
  测试= tsfel.time_series_features_extractor（cfg，索引）
  feature_list.append（测试）

打印（测试）

但是，我总共得到了 120 个特征。这 120 个被分成三列，每列 40 个。我想它的作用是将它们逐行平均在一起，直到完成。当我告诉我的教授这一点时，他们说我应该每行获得 300 个特征，所以我想知道如何解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77616611/getting-features-from-a-dataframe-containing-sublists-in-every-row-of-a-single-c</guid>
      <pubDate>Wed, 06 Dec 2023 22:14:21 GMT</pubDate>
    </item>
    <item>
      <title>我应该在 Seq2seq 模型中的解码器输入上使用嵌入吗？</title>
      <link>https://stackoverflow.com/questions/77616375/should-i-use-embeddings-on-input-of-decoder-in-seq2seq-model</link>
      <description><![CDATA[设置：
我在 Pytorch 中使用序列到序列模型来预测下一个标记，并使用先前标记的序列。它对编码器和解码器使用相同的词汇。该模型是通过一批相同长度的（src，trg）张量对（对于teacher_forcing）进行训练的，尽管trg张量由一个标记组成，其余的是“”。索引，损失函数会忽略它们。
问题：
我的朋友是一位经验丰富的机器学习工程师，他审查了我的作业，但不太确定我是否应该嵌入解码器输入序列。我尝试深入研究一下这个问题，但似乎我能找到的几乎所有 seq2seq 模型的示例都是为翻译而构建的，因此解码器和编码器中的每种语言分别有两种不同的嵌入。
下面是模型代码：
类编码器（nn.Module）：
    def __init__(self, input_dim, emb_dim = 50, hid_dim = 128, n_layers = 1, dropout = 0.05):
        超级().__init__()
        self.input_dim = input_dim
        self.embedding_dim = emb_dim
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)

    def 转发（自身，src）：
        嵌入 = self.embedding(src)
        输出，（隐藏，单元）= self.rnn（嵌入）
        返回隐藏单元格


解码器类（nn.Module）：

    def __init__(self, 嵌入, vocab_dim, emb_dim = 50 , hid_dim = 128, n_layers = 1, dropout = 0.05):
        超级().__init__()

        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.embeddings = 嵌入
        self.emb_dim = emb_dim
        self.fc_out = nn.Linear(hid_dim, vocab_dim)
        self.out = nn.Sequential(
            nn.Linear(hid_dim, vocab_dim),
            nn.Sigmoid()
        ）

    defforward（自身，输入，隐藏，单元格）：
        输入 = 输入.unsqueeze(0)
        嵌入 = self.embeddings(输入)
        输出，（隐藏，单元）= self.rnn（嵌入，（隐藏，单元））
        预测 = self.out(output.squeeze(0))
        返回预测、隐藏、单元格


类 Seq2seq(nn.Module):
    def __init__(自身、编码器、解码器、设备):
        超级().__init__()

        self.encoder = 编码器
        self.decoder = 解码器
        self.device = 设备

    def 前向（自身、src、trg、teacher_force_ratio=0.05）：
        批量大小 = trg.size(1)
        max_len = trg.size(0)
        trg_vocab_size = self.encoder.input_dim

        输出= torch.zeros（max_len，batch_size，trg_vocab_size）.to（self.device）
        隐藏，单元格 = self.encoder(src)
        输入 = trg[0, :]

        对于 t in range(1, max_len)：
            输出、隐藏、单元 = self.decoder(输入、隐藏、单元)
            输出[t] = 输出
            Teacher_force = random.random() &lt;教师力量比
            top1 = 输出.max(1)[1]
            输入 = (trg[t] if Teacher_force else top1)

        返回输出

和训练脚本：
model.train()
对于范围内的纪元（纪元）：
    对于batch_idx，tqdm中的批处理（枚举（tensorized_train_pairs））：
        input_data = 批次[0].to(设备)
        目标 = 批次[1].to(设备)

        输出=模型（输入数据，目标）

        输出 = 输出[1:].reshape(-1, 输出.shape[2])
        目标 = 目标[1:].reshape(-1)

        优化器.zero_grad()
        损失 = loss_func(输出, 目标)
        train_losses.append(损失)
        
        如果（batch_idx％1000==0）：
            print(f&quot;批次 idx:{batch_idx}&quot;, f&quot;loss: {loss}&quot;, f&quot;epoch:{epoch}&quot;)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.,norm_type=2)

        优化器.step()

此外，模型训练没有产生任何结果：损失函数（交叉熵）只是波动。我尝试了几种超参数组合，包括删除梯度裁剪，但没有效果。
这是损失图：
]]></description>
      <guid>https://stackoverflow.com/questions/77616375/should-i-use-embeddings-on-input-of-decoder-in-seq2seq-model</guid>
      <pubDate>Wed, 06 Dec 2023 21:20:41 GMT</pubDate>
    </item>
    <item>
      <title>我想使用具有多个输入的tensorflow js获得接下来的25个值预测</title>
      <link>https://stackoverflow.com/questions/77616362/i-want-to-get-next-25-value-predictions-using-tensorflow-js-with-multiple-input</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77616362/i-want-to-get-next-25-value-predictions-using-tensorflow-js-with-multiple-input</guid>
      <pubDate>Wed, 06 Dec 2023 21:19:20 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中使用分布式数据并行处理具有昂贵块读取时间的大型数据集？</title>
      <link>https://stackoverflow.com/questions/77615926/how-to-use-distributed-data-parallelism-in-pytorch-with-large-datasets-with-expe</link>
      <description><![CDATA[我有一个由 100 个 .npz 文件组成的数据集，其中包含形状为 (285341, 60664) 的 98% 稀疏 scipy CSR 矩阵，其中行是样本数，列是特征大小。我想在数据集上运行变分自动编码器。我可以访问大内存节点（每个 1.5TB RAM）、计算节点（总共 190GB RAM）和 4 个 GPU（总共 190GB RAM）。目前主要的瓶颈是加载数据，因为整个数据集无法立即装入内存。有哪些仍然允许随机洗牌并减少读取时间的建议？
已尝试过的内容：

使用 MapDataset 来分离不断填充队列并在加载时随机洗牌的加载线程。似乎只能在一个 GPU 上工作，但无法在工作人员之间分配。
IterableDataset 的功能与 MapDataset 相同，但使用了worker_init_fn 来分割负载，但不断出现未实现的错误。 （注意：类太大而且吓人，但如果需要的话可以附加，以及 DataLoader 初始化调用）
]]></description>
      <guid>https://stackoverflow.com/questions/77615926/how-to-use-distributed-data-parallelism-in-pytorch-with-large-datasets-with-expe</guid>
      <pubDate>Wed, 06 Dec 2023 19:51:51 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：“Flags”对象没有属性“c_contigious”</title>
      <link>https://stackoverflow.com/questions/77615883/attributeerror-flags-object-has-no-attribute-c-contiguous</link>
      <description><![CDATA[我正在阅读 Aurélien Géron 编写的《机器学习实践》一书，但遇到了以下错误。
代码：
y_train_large = (y_train.astype(&quot;int&quot;) &gt;= 7)
y_train_odd = (y_train.astype(“int”) % 2 == 1)
y_multilabel = np.c_[y_train_large, y_train_odd]

＃模型
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_multilabel)

y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)

最后一行产生以下错误：
&lt;前&gt;&lt;代码&gt;{
AttributeError: &#39;Flags&#39; 对象没有属性 &#39;c_contigious&#39;”
}

由于我正在关注这本书，所以我希望这段代码能够工作。我尝试过 Google Bard 和 Claude AI 聊天机器人的解决方案，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77615883/attributeerror-flags-object-has-no-attribute-c-contiguous</guid>
      <pubDate>Wed, 06 Dec 2023 19:42:47 GMT</pubDate>
    </item>
    <item>
      <title>自动执行 Sagemaker Autopilot 生成的笔记本</title>
      <link>https://stackoverflow.com/questions/77615516/automatic-execution-of-sagemaker-autopilot-generated-notebook</link>
      <description><![CDATA[我能够将数据从本地计算机上传到我的 s3 存储桶，根据该数据创建自动驾驶仪作业，将自动驾驶仪作业的文件输出放回我的 s3 存储桶，并将输出下载到我的本地计算机。我是在 Python 代码中完成这一切的。唯一的问题是，我想运行自动驾驶作业在 Sagemaker 中生成的笔记本，然后从我的存储桶中提取这些执行的笔记本。我想坚持以编程方式执行此操作。有没有人遇到过类似的任务并能够完成它？另外，我是否使用了可行或现实的方法？
我能够将自动驾驶仪生成的笔记本从我的 s3 存储桶下载到本地计算机，并使用此生命周期配置使用该笔记本创建一个笔记本实例：
lifecycle_config_script = &quot;&quot;&quot;&quot;
#!/bin/bash

设置-e

BUCKET_NAME=“{bucket_name}”
NOTEBOOK_PATHS=({notebook_paths})

pip 安装造纸厂

对于“${{NOTEBOOK_PATHS[@]}}”中的笔记本
做
    aws s3 cp“s3://${{BUCKET_NAME}}/${{NOTEBOOK}}” “/home/ec2-user/SageMaker/${{NOTEBOOK}}”
    papermill“/home/ec2-user/SageMaker/${{NOTEBOOK}}” “/home/ec2-user/SageMaker/executed-${{NOTEBOOK}}”
    aws s3 cp“/home/ec2-user/SageMaker/executed-${{NOTEBOOK}}” “s3://${{BUCKET_NAME}}/executed-${{NOTEBOOK}}”
完毕
&quot;&quot;&quot;.format(bucket_name=bucketName, notebook_paths=&#39; &#39;.join([&#39;&quot;&#39; + notebook + &#39;&quot;&#39; for notebook_files]))

在配置实例花费超过 5 分钟后，我查看了 cloudwatch 日志中的生命周期配置，并收到此导入错误：
ImportError：urllib3 v2.0 仅支持 OpenSSL 1.1.1+，当前“ssl”模块使用“OpenSSL 1.0.2k-fips 26 Jan 2017”编译。请参阅：https://github.com/urllib3/urllib3/issues/2168
]]></description>
      <guid>https://stackoverflow.com/questions/77615516/automatic-execution-of-sagemaker-autopilot-generated-notebook</guid>
      <pubDate>Wed, 06 Dec 2023 18:28:43 GMT</pubDate>
    </item>
    <item>
      <title>基于相同输入数据的并行或共享回归网络会更好吗？为什么？</title>
      <link>https://stackoverflow.com/questions/77615153/would-it-be-better-to-have-parallel-or-a-shared-regression-network-based-on-the</link>
      <description><![CDATA[我将多个并行回归网络组合成一个模型，其中组合输出以创建单个损失函数。这些网络正在寻找相同数据的不同方面，并同时进行训练。这可以被认为是一个基于物理的神经网络。
本能地，我想说，分割网络允许每个网络拥有自己的权重，而不受其他方面的干扰，这将加快训练速度和/或提高性能。 ChatGPT 似乎证实了我的怀疑，但无法给我任何来源。
有人有任何论文/证明或知道这两种方法的更具体术语吗？我只是真的不知道如何提出这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77615153/would-it-be-better-to-have-parallel-or-a-shared-regression-network-based-on-the</guid>
      <pubDate>Wed, 06 Dec 2023 17:29:30 GMT</pubDate>
    </item>
    <item>
      <title>使用（非图像）细胞计数数据创建 pytorch CNN</title>
      <link>https://stackoverflow.com/questions/77615149/creating-a-pytorch-cnn-using-non-image-cytometric-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77615149/creating-a-pytorch-cnn-using-non-image-cytometric-data</guid>
      <pubDate>Wed, 06 Dec 2023 17:28:44 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 故障排除：常见错误和解决方案[关闭]</title>
      <link>https://stackoverflow.com/questions/77614844/troubleshooting-tensorflow-navigating-common-errors-and-solutions</link>
      <description><![CDATA[从 matplotlib 导入 gridspec
将 matplotlib.pylab 导入为 plt
将 numpy 导入为 np
将张量流导入为 tf
导入tensorflow_hub作为集线器#

安装后无法修复tensorflow的错误
一次又一次安装后我没有弄清楚错误，他们给出的错误为
错误：找不到满足张量流要求的版本（来自版本：无）
错误：找不到张量流的匹配分布
]]></description>
      <guid>https://stackoverflow.com/questions/77614844/troubleshooting-tensorflow-navigating-common-errors-and-solutions</guid>
      <pubDate>Wed, 06 Dec 2023 16:37:59 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 tidymodels 框架中的 Youden 索引获取 ML 性能指标？</title>
      <link>https://stackoverflow.com/questions/77614694/how-do-you-get-ml-performance-metrics-using-the-youden-index-in-the-tidymodels-f</link>
      <description><![CDATA[如何使用 tidymodels 框架中的 Youden 索引获取机器学习性能指标？
我在网上搜索过示例，但没有找到。]]></description>
      <guid>https://stackoverflow.com/questions/77614694/how-do-you-get-ml-performance-metrics-using-the-youden-index-in-the-tidymodels-f</guid>
      <pubDate>Wed, 06 Dec 2023 16:12:03 GMT</pubDate>
    </item>
    <item>
      <title>训练 llm 以在图数据库中生成查询</title>
      <link>https://stackoverflow.com/questions/77613507/training-llm-for-query-generation-in-a-graph-database</link>
      <description><![CDATA[如果我开发了一个有自己的查询语言的图数据库。我必须找到一种方法向 llm 提供图表，然后 llm 应该能够生成我们数据库的查询。
我在 langchain 中发现了类似的东西，我们可以向它提供 rdf 文件，然后它会生成 sparql 查询。
所以我对此有很多疑问，因为我对此很陌生：
是否可以使用全新的技术来培训法学硕士，就像我们的数据库一样。如果可以的话怎么办。
我知道我们必须向法学硕士提供训练数据。那么在这种情况下，它会是我们数据库查询的数据集吗？如果是，那么我们必须在数据集中提供多少个查询。
抱歉，问题不详细，这只是我第二次在这里问。]]></description>
      <guid>https://stackoverflow.com/questions/77613507/training-llm-for-query-generation-in-a-graph-database</guid>
      <pubDate>Wed, 06 Dec 2023 13:34:06 GMT</pubDate>
    </item>
    <item>
      <title>spaCy 值错误：[E1041] 需要字符串、文档或字节作为输入，但得到：<class 'float'></title>
      <link>https://stackoverflow.com/questions/77596731/spacy-value-error-e1041-expected-a-string-doc-or-bytes-as-input-but-got</link>
      <description><![CDATA[我正在尝试使用 spaCy 对中文输入进行矢量化。
我的代码如下：


nlp = spacy.load(&#39;zh_core_web_md&#39;)

def tokenize_and_vectorize_textZH(文本):
    clean_tokensZH = []
    对于 nlp(text) 中的标记：
        if (不是 token.is_stop) &amp; (token.lemma_ != &#39;-PRON-&#39;) &amp; （不是 token.is_punct）：
          # -PRON- 是 spaCy 用于任何代词的特殊全包引理，我们要排除这些
            if (len(token.vector) != 300):
              打印（令牌）
            clean_tokensZH.append(token.vector)
    返回 np.array(clean_tokensZH)
    
    
all_summmed_vecsZH = []

def sum_vecsZH(输入):
  tokenized_vectorsZH = input.apply(tokenize_and_vectorize_textZH)
  tokenized_vectorZH = tokenized_vectorsZH.to_numpy()

  打印（len（tokenized_vectorsZH））
  #print(类型(标记化向量))

  对于 tokenized_vectorsZH 中的行：

    #打印（行）

    summed_vecZH = [0]*300 # 从 300 个零的列表开始

    for vec in row: # 循环遍历与行中每个标记对应的每个向量
      #if (len(vec) != 300):
        #打印（向量）
      summed_vecZH += vec

    all_summmed_vecs.append(summed_vecZH)

  #print(tokenized_vectors[0][0].向量)
  
  
#@title 应用矢量化
sum_vecsZH(X_trainZH)
打印（all_summmed_vecs）

sum_vecsZH(y_trainZH)
打印（all_summmed_vecs）

sum_vecsZH(X_testZH)
打印（all_summmed_vecs）

sum_vecsZH(y_testZH)
打印（all_summmed_vecs）



最后 8 行的预期输出应与此类似：
33384
33384
14308
14308
这是我的数据示例：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

not_cyberbullying
你看起来像个吉普赛人抱歉，我不遗憾


&lt;正文&gt;

不是网络欺凌
RT @Kurdsnews：土耳其国家在过去11年中杀害了241名儿童 http/t.co/JlvkE1epws #news ##GoogleÇeviriciTopluluğuKürtçeyideE...


性别
如果我叫你婊子你就生气了，那就别叫自己咕咕女。




这个错误的原因是什么？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77596731/spacy-value-error-e1041-expected-a-string-doc-or-bytes-as-input-but-got</guid>
      <pubDate>Mon, 04 Dec 2023 00:59:14 GMT</pubDate>
    </item>
    <item>
      <title>在 scikit-learn 中使用 skopt.BayesSearchCV 时如何修复“numpy.int”属性错误？</title>
      <link>https://stackoverflow.com/questions/76321820/how-to-fix-the-numpy-int-attribute-error-when-using-skopt-bayessearchcv-in-sci</link>
      <description><![CDATA[当我在官方文档上运行以下代码时，出现错误。
最小示例
from skopt import BayesSearchCV
从 sklearn.datasets 导入 load_digits
从 sklearn.svm 导入 SVC
从 sklearn.model_selection 导入 train_test_split

X, y = load_digits(n_class=10, return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=.25, random_state=0)

# log-uniform：理解为通过改变 x 对 p = exp(x) 进行搜索
选择 = BayesSearchCV(
    SVC(),
    {
        &#39;C&#39;: (1e-6, 1e+6, &#39;对数均匀&#39;),
        &#39;伽玛&#39;: (1e-6, 1e+1, &#39;对数均匀&#39;),
        &#39;level&#39;: (1, 8), # 整数值参数
        &#39;kernel&#39;: [&#39;线性&#39;, &#39;poly&#39;, &#39;rbf&#39;], # 分类参数
    },
    n_iter=32,
    简历=3
）

opt.fit(X_train, y_train)

最后一行产生错误：
&lt;块引用&gt;
属性错误
模块“numpy”没有属性“int”。
np.int 是内置 int 的已弃用别名。要避免现有代码中出现此错误，请单独使用 int。这样做不会改变任何行为并且是安全的。替换 np.int 时，您可能希望使用例如np.int64 或 np.int32 指定精度。如果您想查看当前的使用情况，请查看发行说明链接以获取更多信息。
别名最初在 NumPy 1.20 中已弃用；有关更多详细信息和指导，请参阅原始发行说明：
https://numpy.org/devdocs/release/1.20.0-notes .html#deprecations

如何解决这个问题？还有其他方法来实现贝叶斯搜索吗？
也许skopt的版本太旧了。还有其他方法来实现贝叶斯搜索吗？除了网格搜索、随机搜索和贝叶斯搜索之外，还有其他方法可以帮助我选择机器学习模型的超参数吗？]]></description>
      <guid>https://stackoverflow.com/questions/76321820/how-to-fix-the-numpy-int-attribute-error-when-using-skopt-bayessearchcv-in-sci</guid>
      <pubDate>Wed, 24 May 2023 09:06:15 GMT</pubDate>
    </item>
    <item>
      <title>Pyro Paramstore 的不同访问方法会产生不同的结果</title>
      <link>https://stackoverflow.com/questions/61684499/different-access-methods-to-pyro-paramstore-give-different-results</link>
      <description><![CDATA[我正在学习预测中的 Pyro 介绍性教程，并尝试访问学习到的参数训练模型后，我对其中一些模型使用不同的访问方法得到了不同的结果（而其他模型得到了相同的结果）。
以下是本教程中可重现的精简代码：
导入火炬
进口火爆
导入 Pyro.distributions 作为 dist
从pyro.contrib.examples.bart导入load_bart_od
从pyro.contrib.forecast导入ForecastingModel，Forecaster

pyro.enable_validation（真）
Pyro.clear_param_store()

火爆.__版本__
#&#39;1.3.1&#39;
火炬.__版本__
#&#39;1.5.0+cu101&#39;

# 导入&amp;准备数据
数据集 = load_bart_od()
T, O, D = 数据集[&quot;计数&quot;].shape
data = dataset[&quot;counts&quot;][:T // (24 * 7) * 24 * 7].reshape(T // (24 * 7), -1).sum(-1).log()
数据 = data.unsqueeze(-1)
T0 = 0 # 开始
T2 = data.size(-2) # 结束
T1 = T2 - 52 # 训练/测试分割

# 定义模型类
类 Model1（预测模型）：

    def 模型（自身、零数据、协变量）：
        data_dim = Zero_data.size(-1)
        feature_dim = covariates.size(-1)

        偏差 =pyro.sample(&quot;偏差&quot;, dist.Normal(0, 10).expand([data_dim]).to_event(1))
        重量 =pyro.sample(&quot;重量&quot;, dist.Normal(0, 0.1).expand([feature_dim]).to_event(1))
        预测 = 偏差 + (权重 * 协变量).sum(-1, keepdim=True)
        断言 Prediction.shape[-2:] == Zero_data.shape

        noise_scale =pyro.sample(&quot;noise_scale&quot;, dist.LogNormal(-5, 5).expand([1]).to_event(1))
        噪声距离 = dist.Normal(0, 噪声尺度)

        self.predict(noise_dist, 预测)

# 拟合模型
Pyro.set_rng_seed(1)
Pyro.clear_param_store()
时间 = torch.arange(float(T2)) / 365
协变量 = torch.stack([时间], 暗淡=-1)
预测器=预测器（模型1（），数据[：T1]，协变量[：T1]，学习率= 0.1）

到目前为止一切顺利；现在，我想检查存储在 Paramstore 中的学习到的潜在参数。似乎有不止一种方法可以做到这一点；使用 get_all_param_names() 方法：
pyro.get_param_store().get_all_param_names() 中的名称：
    打印（名称，pyro.param（名称）.data.numpy（））

我明白了
AutoNormal.locs.bias [14.585433]
AutoNormal.scales.bias [0.00631594]
AutoNormal.locs.weight [0.11947815]
AutoNormal.scales.weight [0.00922901]
AutoNormal.locs.noise_scale [-2.0719821]
AutoNormal.scales.noise_scale [0.03469057]

但是使用named_pa​​rameters()方法：
pyro.get_param_store().named_pa​​rameters()

为位置 (locs) 参数提供相同的值，但为所有比例参数提供不同的值：
dict_items([
(&#39;AutoNormal.locs.bias&#39;,参数包含：tensor([14.5854],requires_grad=True)),
(&#39;AutoNormal.scales.bias&#39;,参数包含：tensor([-5.0647],requires_grad=True)),
(&#39;AutoNormal.locs.weight&#39;,参数包含:tensor([0.1195],requires_grad=True)),
(&#39;AutoNormal.scales.weight&#39;,参数包含：tensor([-4.6854],requires_grad=True)),
(&#39;AutoNormal.locs.noise_scale&#39;,参数包含：tensor([-2.0720],requires_grad=True)),
(&#39;AutoNormal.scales.noise_scale&#39;, 参数包含：tensor([-3.3613], require_grad=True))
]）

这怎么可能？根据文档，Paramstore是一个简单的键值存储；里面只有这六个键：
pyro.get_param_store().get_all_param_names() # .keys() 方法给出相同的结果
＃ 结果
字典键（[
&#39;AutoNormal.locs.bias&#39;,
&#39;AutoNormal.scales.bias&#39;,
&#39;AutoNormal.locs.weight&#39;,
&#39;AutoNormal.scales.weight&#39;,
&#39;AutoNormal.locs.noise_scale&#39;,
&#39;AutoNormal.scales.noise_scale&#39;])

因此，不可能一种方法访问一组项目，而另一种方法访问不同的项目。
我在这里遗漏了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/61684499/different-access-methods-to-pyro-paramstore-give-different-results</guid>
      <pubDate>Fri, 08 May 2020 17:20:33 GMT</pubDate>
    </item>
    </channel>
</rss>