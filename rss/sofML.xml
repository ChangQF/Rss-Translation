<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 20 Jun 2024 01:03:12 GMT</lastBuildDate>
    <item>
      <title>我在创建与NLP相关的项目时遇到了问题</title>
      <link>https://stackoverflow.com/questions/78644918/i-encountered-a-problem-when-creating-a-project-related-to-nlp</link>
      <description><![CDATA[需要帮助编写和训练神经网络来设置文本样式。输入两个文本：一个样式化示例和一个信息文本。输出是与第一个文本样式化的文本，但其信息含义与第二个文本（摘要）相似，但不包括重复。神经网络必须考虑各种风格特征（表情符号、粗体、斜体、链接、删除线和下划线文本、编号/列表等）。

是否有可能实现这一点？
我尝试使用文本摘要方法制作一个神经元，但后来我卡住了，不知道该去哪里，我尝试了 T5 模型，但没有得到任何有意义的结果
import re
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer

model = T5ForConditionalGeneration.from_pretrained(&#39;t5-base&#39;)
tokenizer = T5Tokenizer.from_pretrained(&#39;t5-base&#39;)

def style_transfer(content_text, style_text):
def preprocess_text(text):
text = re.sub(r&#39;\s+&#39;, &#39; &#39;, text)
return text.strip()

def postprocess_text(text):

text = re.sub(r&#39;\n\s*\n&#39;, &#39;\n&#39;, text)

text = text.strip()
返回文本

content = preprocess_text(content_text)
style = preprocess_text(style_text)

input_text = f&quot;transfer style: {style} content: {content}&quot; 
输入 = tokenizer（输入文本，
return_tensors=&#39;pt&#39;，
max_length=512，
truncation=True，
padding=&#39;max_length&#39;）

输出 = model.generate（输入[&#39;输入ids&#39;]，
注意力掩码=输入[&#39;注意力掩码&#39;]，
max_length=512，
num_beams=5，
early_stopping=True，
top_k=50，
top_p=0​​.95，
)

生成文本 = tokenizer.decode（输出[0]，skip_special_tokens=True）

生成文本 = postprocess_text（生成文本）

返回生成文本

内容文本 = ()
样式文本 = ()

结果 = style_transfer（内容文本，样式文本）
打印（结果）
]]></description>
      <guid>https://stackoverflow.com/questions/78644918/i-encountered-a-problem-when-creating-a-project-related-to-nlp</guid>
      <pubDate>Wed, 19 Jun 2024 22:51:06 GMT</pubDate>
    </item>
    <item>
      <title>神经网络在反向传播后归零</title>
      <link>https://stackoverflow.com/questions/78644685/neural-network-zeroing-out-after-back-propogation</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78644685/neural-network-zeroing-out-after-back-propogation</guid>
      <pubDate>Wed, 19 Jun 2024 21:19:59 GMT</pubDate>
    </item>
    <item>
      <title>Excel 中的逻辑回归</title>
      <link>https://stackoverflow.com/questions/78644668/logistic-regression-in-excel</link>
      <description><![CDATA[我有两个优化模型：
LR-P1：
LR-P1 模型
LR-P2：
LR-P2 模型
我期望两个模型都获得相同的最优值，但我无法计算模型 LR-P1。我进行了所有计算，但 Excel 求解器无法找到最优值。当我将所有系数设为 0.1 时，求解器只会说找到了最优值，但不会更改决策变量。
我的问题是，我进行了所有计算，但 Excel 给出了 NUM 错误，而模型 LR-P2 没有。这是因为 LR-P1 的目标函数太小，以致 Excel 求解器无法对其进行交换，从而导致数值问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78644668/logistic-regression-in-excel</guid>
      <pubDate>Wed, 19 Jun 2024 21:13:58 GMT</pubDate>
    </item>
    <item>
      <title>确定哪些变量对 FDA 贡献最大</title>
      <link>https://stackoverflow.com/questions/78644579/determine-which-variables-contribute-most-to-fda</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78644579/determine-which-variables-contribute-most-to-fda</guid>
      <pubDate>Wed, 19 Jun 2024 20:46:16 GMT</pubDate>
    </item>
    <item>
      <title>无法重现 PyTorch 模型训练性能</title>
      <link>https://stackoverflow.com/questions/78644377/unable-to-reproduce-pytorch-model-training-performance</link>
      <description><![CDATA[我已经在自定义数据集上为图像分类任务训练了一个 RegNet 模型。那是在 2023 年 8 月。现在我想使用相同的数据集再次训练完全相同的模型。我希望这个新模型能够实现与 2023 年 8 月之前的模型大致相同的性能，因为没有任何变化：

我使用完全相同的 PyTorch 和 Torchvision 版本（1.13 和 0.14）
我使用完全相同的图像数据集进行训练/验证/测试
我使用完全相同的脚本通过 torch 训练模型
我使用与以前完全相同的训练超参数

但是，即使没有任何变化，新训练的模型的表现也明显差于去年的原始模型。 2023 年 8 月的第一个模型的测试准确率达到 0.97，而现在新模型在同一个测试数据集上仅达到 0.94。在训练期间，训练和验证准确率与以前大致相同。
我知道两个模型不会达到完全相同的性能，但 3% 的差异似乎太多了。无论我做什么，我都无法接近去年的 0.97 测试准确率，我得到的只有 0.94。即使一切都完全相同，如所述。即使是带有四个 GPU 的机器和在该机器上运行的 Ubuntu 版本也与 2023 年之前完全相同。
我知道其中涉及一个随机种子，但我怀疑这会导致如此大的 3% 的测试准确率差异。我还知道也许 Nvidia / CUDA 驱动程序已在该机器上更新，当然还有一些依赖项和软件包（例如 numpy）。但这会导致如此巨大的差异吗？]]></description>
      <guid>https://stackoverflow.com/questions/78644377/unable-to-reproduce-pytorch-model-training-performance</guid>
      <pubDate>Wed, 19 Jun 2024 19:45:53 GMT</pubDate>
    </item>
    <item>
      <title>为什么这些简单的线性回归权重梯度 numpy 计算会给出不同的结果？</title>
      <link>https://stackoverflow.com/questions/78644274/why-are-these-simple-linear-regression-weights-gradient-numpy-calculations-givin</link>
      <description><![CDATA[对于权重梯度计算，它们对相同参数给出了不同的结果。
# 定义训练集
X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])
y_train = np.array([460, 232, 178])
b_init = 785.1811367994083
w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])

方法 (1)
def dj_dw(w,x,b,y):
# 示例数量
m = x.shape[0]
dj_dw = (1/m)*(np.dot(np.transpose(np.dot(x,w)+b-y),x))
return dj_dw

方法 (2)
def dj_dw_2(w, x, b, y):
m, n = x.shape
dj_dw = np.zeros(n)
for j in range(n):
for i in range(m):
dj_dw[j] += (1/m) * ((w[j]*x[i][j] + b - y[i]) * (x[i][j]))
return dj_dw

以及结果分别
[-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05]
[ 1.59529824e+06 1.73748484e+03 5.72854200e+02 -2.33772157e+04]
]]></description>
      <guid>https://stackoverflow.com/questions/78644274/why-are-these-simple-linear-regression-weights-gradient-numpy-calculations-givin</guid>
      <pubDate>Wed, 19 Jun 2024 19:14:12 GMT</pubDate>
    </item>
    <item>
      <title>如何提取不同图像中相同的 ROI 区域？[关闭]</title>
      <link>https://stackoverflow.com/questions/78644034/how-can-i-extract-roi-region-same-in-different-images</link>
      <description><![CDATA[我是 ROI 提取方面的新手。我有许多像这样的图像 ROI 区域，这些图像是由相机从人们的眼睛中捕捉到的。我想确定某人是否患有贫血症。
想使用此 ROI 区域通过 Pearson 相关指数进行数学运算。
我使用开放 CV 和 python。想在所有不同的图像中准确提取这个区域我的 ROI 目标。也不知道如何找到这个 ROI 区域的坐标。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78644034/how-can-i-extract-roi-region-same-in-different-images</guid>
      <pubDate>Wed, 19 Jun 2024 18:05:07 GMT</pubDate>
    </item>
    <item>
      <title>控制 Azure ML 命令源代码的上传位置</title>
      <link>https://stackoverflow.com/questions/78643575/control-where-source-code-for-azure-ml-command-gets-uploaded</link>
      <description><![CDATA[我正在 Azure 机器学习工作室的笔记本中工作，并使用以下代码块通过 命令函数 实例化作业。
来自 azure.ai.ml 导入命令、输入、输出
来自 azure.ai.ml.entities 导入数据
来自 azure.ai.ml.constants 导入 AssetTypes

subscription_id = &quot;&lt;subscription_id&gt;&quot;
resource_group = &quot;&lt;resource_group&gt;&quot;
working = &quot;&lt;workspace&gt;&quot;
storage_account = &quot;&lt;storage_account&gt;&quot;
输入路径 = &quot;&lt;输入路径&gt;&quot;
输出路径 = &quot;&lt;输出路径&gt;&quot;

input_dict = {
&quot;input_data_object&quot;: 输入(
type=AssetTypes.URI_FILE, 
path=f&quot;azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{storage_account}/paths/{input_path}&quot;
)
}

output_dict = {
&quot;output_folder_object&quot;: 输出(
type=AssetTypes.URI_FOLDER,
path=f&quot;azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{storage_account}/paths/{output_path}&quot;,
)
}

job = command(
code=&quot;./src&quot;, 
command=&quot;python 01_read_write_data.py -v --input_data=${{inputs.input_data_object}} --output_folder=${{outputs.output_folder_object}}&quot;,
inputs=input_dict,
outputs=output_dict,
environment=&quot;&lt;asset_env&gt;&quot;,
compute=&quot;&lt;compute_cluster&gt;&quot;,
)

returned_job = ml_client.create_or_update(job)

此操作成功运行，但每次运行时，如果存储在 ./src 目录中的代码发生变化，则会将新副本上传到默认的 blob 存储帐户。我不介意这一点，但每次运行时，代码都会上传到我的 blob 存储帐户根目录下的新容器中。因此，我的默认存储帐户会因容器而变得杂乱无章。我已阅读使用 command() 函数实例化 command 对象的文档，但我没有看到可用于控制 ./src 代码上传位置的参数。有什么方法可以控制吗？]]></description>
      <guid>https://stackoverflow.com/questions/78643575/control-where-source-code-for-azure-ml-command-gets-uploaded</guid>
      <pubDate>Wed, 19 Jun 2024 16:06:28 GMT</pubDate>
    </item>
    <item>
      <title>如果动作依赖于状态并且动作空间巨大，那么如何定义动作？</title>
      <link>https://stackoverflow.com/questions/78643467/how-to-define-action-if-the-actions-are-state-dependent-and-the-action-space-are</link>
      <description><![CDATA[我正在尝试使用 RL 来解决我的问题。这个问题具有巨大的状态空间（离散）和巨大的动作空间。此外，每个状态可用的动作各不相同（取决于状态）。因此，我不能使用通用 DQN，其中输出状态等于类的数量。此外，我在将动作编码到 NN 中时遇到了问题（因为大小和不同的类别）。但是，这个问题中的转换是确定性的，其中我知道采取行动后从当前状态到下一个状态。
有什么方法可以表示动作？或者有没有比 DQN 更好的 RL 算法可以使用？
谢谢
目前的想法是训练一个 DQN，其中输入既是当前状态，也是采取行动后的下一个状态。]]></description>
      <guid>https://stackoverflow.com/questions/78643467/how-to-define-action-if-the-actions-are-state-dependent-and-the-action-space-are</guid>
      <pubDate>Wed, 19 Jun 2024 15:40:09 GMT</pubDate>
    </item>
    <item>
      <title>cached_download 是从 HF 中心下载文件的传统方式，请考虑升级到“hf_hub_download</title>
      <link>https://stackoverflow.com/questions/78642556/cached-download-is-the-legacy-way-to-download-files-from-the-hf-hub-please-cons</link>
      <description><![CDATA[我在使用嵌入模块进行人脸编码时遇到了这个问题。我该如何解决它们？
我尝试下载 Hugging Face 库并对其进行了探索，但找不到方法，并且还出现了类似 Future Warning: feature_extractor 已弃用并将在 v5 中删除的警告。请改用 image_processor。或已弃用并将在 v5 中删除。请改用image_processor\。它在笔记本电脑上运行良好，并会生成警告，但在 Raspbery Pi 中，它会终止程序。]]></description>
      <guid>https://stackoverflow.com/questions/78642556/cached-download-is-the-legacy-way-to-download-files-from-the-hf-hub-please-cons</guid>
      <pubDate>Wed, 19 Jun 2024 12:36:27 GMT</pubDate>
    </item>
    <item>
      <title>U-Net 无法过度拟合单个训练示例 - 损失平台</title>
      <link>https://stackoverflow.com/questions/78642145/u-net-unable-to-overfit-single-training-example-loss-plateaus</link>
      <description><![CDATA[我正在通过在由单个训练示例（图像到图像）组成的“数据集”上训练 U-Net 架构来测试它。输入图像是输出图像的噪声版本。最初，输出图像开始看起来更像所需的输出，但损失曲线开始趋于稳定，模型停止改进。
我的问题是：

U-Net（或任何没有完全连接层的 CNN）是否应该能够在给定单个示例的情况下在恒定图像上过度拟合？
如果它不能完成这个简单的任务，会犯什么常见错误或需要注意什么？

我通过将深度降低到几乎双卷积层（没有任何编码器/解码器层）来简化架构，并调整了学习率，但它仍然不会过度拟合。我期望模型在单个训练示例上完美地过度拟合，但事实并非如此。我已经调整了学习率，但模型仍然无法在这个简单的任务上实现完美的过度拟合。
缩放输入图像
缩放地面实况
达到平台期后的缩放模型输出

以下是我的实验的具体内容：
架构（深度 1）：
UNet(
(encoders): ModuleList(
(0): Sequential(
(0): Conv2d(4, 64，kernel_size=(3, 3)，stride=(1, 1)，padding=(1, 1))
(1): ReLU(inplace=True)
(2): Conv2d(64, 64，kernel_size=(3, 3)，stride=(1, 1)，padding=(1, 1))
(3): ReLU(inplace=True)
)
)
(解码器): ModuleList(
(0): ConvTranspose2d(128, 64，kernel_size=(2, 2)，stride=(2, 2))
(1): Sequential(
(0): Conv2d(128, 64，kernel_size=(3, 3)，stride=(1, 1)，padding=(1, 1))
(1): ReLU(inplace=True)
(2): Conv2d(64, 64，kernel_size=(3, 3)，步幅=(1, 1), 填充=(1, 1))
(3): ReLU(inplace=True)
)
)
(pool): MaxPool2d(kernel_size=2, 步幅=2, 填充=0, 扩张=1, ceil_mode=False)
(bottleneck): Sequential(
(0): Conv2d(64, 128, kernel_size=(3, 3), 步幅=(1, 1), 填充=(1, 1))
(1): ReLU(inplace=True)
(2): Conv2d(128, 128, kernel_size=(3, 3), 步幅=(1, 1), 填充=(1, 1))
(3): ReLU(inplace=True)
)
(out_conv): Conv2d(64, 4, kernel_size=(1, 1), 步幅=(1, 1))
)

初始化：Kaiming Normal
 def _initialize_weights(self) -&gt; None:
for m in self.modules():
if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
nn.init.kaiming_normal_(m.weight, mode=&#39;fan_in&#39;, nonlinearity=&#39;relu&#39;)
if m.bias is not None:
nn.init.constant_(m.bias, 0)

上采样方法：Crop 和 Concat
 def crop_and_concat(self, upsampled: torch.Tensor,
passive: torch.Tensor) -&gt; torch.Tensor:
diffY =pass.size()[2] - upsampled.size()[2]
diffX =pass.size()[3] - upsampled.size()[3]
upsampled = F.pad(upsampled, (diffX // 2, diffX - diffX // 2,
diffY // 2, diffY - diffY // 2))
return torch.cat((upsampled,bypass), dim=1)

损失：MSELoss
优化器：Adam，学习率从 1e-3 到 1e-4（上面的一切都在震荡，下面的也达到了稳定状态并且收敛速度较慢）]]></description>
      <guid>https://stackoverflow.com/questions/78642145/u-net-unable-to-overfit-single-training-example-loss-plateaus</guid>
      <pubDate>Wed, 19 Jun 2024 11:09:33 GMT</pubDate>
    </item>
    <item>
      <title>是否应将多个分类嵌入组合成条件 GAN（cGAN）？[关闭]</title>
      <link>https://stackoverflow.com/questions/78639650/should-multiple-categorical-embeddings-be-combined-for-a-conditional-gan-cgan</link>
      <description><![CDATA[我正在尝试制作一个条件 GAN (cGAN)，它可以根据标题和视频类别/流派生成 YouTube 缩略图。
它根本不起作用，甚至没有接近，所以我试图回到有关我的架构的基本问题。现在，我所做的是制作两个嵌入向量，一个用于标题，一个用于类别，然后我将它们组合起来并将它们都发送到生成器和鉴别器。
以下是我的代码：
class CategoryTitleEmbeddingNet(nn.Module):
def __init__(self, num_categories:int, category_embedding_dim:int, vocab_size:int, title_embedding_dim:int, title_max_length:int):
&quot;&quot;&quot;
初始化标题/类别嵌入神经网络

参数：
num_categories：唯一类别的总数。
category_embedding_dim：类别嵌入向量的维度。
vocab_size：唯一标记的总数。
title_embedding_dim：标题嵌入向量的维度。
title_max_length：标题中允许的最大标记数。

返回：
无
&quot;&quot;&quot;
super(CategoryTitleEmbeddingNet，self).__init__()
self.category_embedding = nn.Embedding(num_categories，category_embedding_dim)
self.title_embedding = nn.Embedding(vocab_size，title_embedding_dim)

self.fully_connected1 = nn.Linear(category_embedding_dim + title_embedding_dim * title_max_length，256)
self.fully_connected2 = nn.Linear(256，128)
self.fully_connected3 = nn.Linear(128，64)
self.fully_connected4 = nn.Linear(64，1)

def forward(self，category_indices：torch.Tensor，title_indices：torch.Tensor) -&gt; torch.Tensor:

category_embedded = self.category_embedding(category_indices)
title_embedded = self.title_embedding(title_indices)
title_embedded = title_embedded.view(title_embedded.size(0), -1)
category_embedded = category_embedded.view(category_embedded.size(0), -1)

combined_embeddings = torch.cat((category_embedded, title_embedded), dim=1)
combined_output = torch.relu(self.fully_connected1(combined_embeddings))
combined_output = torch.relu(self.fully_connected2(combined_output))
combined_output = torch.relu(self.fully_connected3(combined_output))
final_output = self.fully_connected4(combined_output)
返回final_output

我只是将两个嵌入向量连接成一个。我想知道这样做可以吗？还是我应该分别传递它们？我尝试对此进行一些研究，但这是一个相当小众的问题]]></description>
      <guid>https://stackoverflow.com/questions/78639650/should-multiple-categorical-embeddings-be-combined-for-a-conditional-gan-cgan</guid>
      <pubDate>Tue, 18 Jun 2024 21:04:12 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：模块“keras.src.backend”没有属性“convert_to_numpy”</title>
      <link>https://stackoverflow.com/questions/78638871/attributeerror-module-keras-src-backend-has-no-attribute-convert-to-numpy</link>
      <description><![CDATA[我尝试使用 utoencoder 和 rus 相应的代码，并在使用 tensorflow 和 keras 时遇到问题，在下面的代码中我展示了代码和相应的错误。当我拟合自动编码器模型时，它显示 AttributeError: module &#39;keras.src.backend&#39; 没有属性 &#39;convert_to_numpy&#39;。我无法理解这个错误和相应的解决方案。对于这种情况我该如何解决我的问题？我使用 anaconda3 运行此代码。我使用 tensorflow 版本 2.16.1 和 keras 版本 3.3.3，错误显示在模型、拟合线中。我尝试使用自动编码器消除噪音，在这种情况下我编写了代码。我尝试运行多次但没有成功。我在 genimi 中写入错误。它向我展示了两种方法 1. 升级 tensorflow 和 keras 2. 不要使用 backend.convert_to_numpy，而是使用推荐的方法在较新版本中将张量转换为 NumPy 数组。]]></description>
      <guid>https://stackoverflow.com/questions/78638871/attributeerror-module-keras-src-backend-has-no-attribute-convert-to-numpy</guid>
      <pubDate>Tue, 18 Jun 2024 17:28:14 GMT</pubDate>
    </item>
    <item>
      <title>使用 BARTDecoder 和 cached_property 的 Nougat OCR 中的 ImportError 和 TypeError 问题</title>
      <link>https://stackoverflow.com/questions/78594832/importerror-and-typeerror-issues-in-nougat-ocr-with-bartdecoder-and-cached-prope</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78594832/importerror-and-typeerror-issues-in-nougat-ocr-with-bartdecoder-and-cached-prope</guid>
      <pubDate>Sat, 08 Jun 2024 05:43:48 GMT</pubDate>
    </item>
    <item>
      <title>sklearn ImportError：无法导入名称 plot_roc_curve</title>
      <link>https://stackoverflow.com/questions/60321389/sklearn-importerror-cannot-import-name-plot-roc-curve</link>
      <description><![CDATA[我尝试按照 sklearn 文档中提供的 示例，绘制带有交叉验证的接收者操作特性 (ROC) 曲线。但是，以下导入在 python2 和 python3 中都给出了 ImportError。
from sklearn.metrics import plot_roc_curve

错误：
回溯（最近一次调用最后一次）：
文件“&lt;stdin&gt;”，第 1 行，在 &lt;module&gt;
ImportError：无法导入名称 plot_roc_curve

python-2.7 sklearn 版本：0.20.2.
python-3.6 sklearn 版本：0.21.3.
我发现以下导入工作正常，但它与 plot_roc_curve 不太一样。
from sklearn.metrics import roc_curve

plot_roc_curve 是否已弃用？有人可以尝试代码并告诉我 sklearn 版本是否有效吗？]]></description>
      <guid>https://stackoverflow.com/questions/60321389/sklearn-importerror-cannot-import-name-plot-roc-curve</guid>
      <pubDate>Thu, 20 Feb 2020 13:44:41 GMT</pubDate>
    </item>
    </channel>
</rss>