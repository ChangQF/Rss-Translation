<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 24 Mar 2024 15:12:36 GMT</lastBuildDate>
    <item>
      <title>机器学习和创建简单的人工智能[关闭]</title>
      <link>https://stackoverflow.com/questions/78214831/machine-learning-and-creating-a-simple-ai</link>
      <description><![CDATA[我想创建一个可以和我一起玩简单游戏的人工智能。这个游戏有一些思维层次，所以我应该教人工智能如何在不同的独特情况下思考等。但我不知道从哪里开始。该游戏将在 iOS(swiftui) 上运行。我在考虑 createML，但我不想把精力花在错误的事情上，所以我需要你的帮助。基本上需要一个路线图。
我没有尝试太多。我知道 Apple 发布了 CoreML 和 CreateML，但不知道这些是否适合我。]]></description>
      <guid>https://stackoverflow.com/questions/78214831/machine-learning-and-creating-a-simple-ai</guid>
      <pubDate>Sun, 24 Mar 2024 14:12:48 GMT</pubDate>
    </item>
    <item>
      <title>多标签分类的堆叠</title>
      <link>https://stackoverflow.com/questions/78214688/stacking-for-multilabelclassification</link>
      <description><![CDATA[我有两个 BERT 模型来实现代码中漏洞检测的多标签分类。一名接受过源代码培训，另一名接受过编译代码培训。他们实现的任务是多标签分类，因此两个模型的单个输出都是一个包含 6 个元素的数组，每个元素可以是 0 或 1，指示漏洞是否存在。
我想在这两个模型之上构建一个经典的 ML 分类器（如 RandomForest、SVM 等），实现称为 Stacking 的集成技术。知道我正在处理多标签分类，我该如何实现这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78214688/stacking-for-multilabelclassification</guid>
      <pubDate>Sun, 24 Mar 2024 13:28:23 GMT</pubDate>
    </item>
    <item>
      <title>如果文件夹已存在 ChromaDB，则防止创建嵌入</title>
      <link>https://stackoverflow.com/questions/78214495/prevent-create-embeddings-if-folder-already-present-chromadb</link>
      <description><![CDATA[这是我第一次尝试RAG应用。我正在尝试使用法学硕士进行问答。我将在下面粘贴运行良好的代码。我的问题是每次运行 python 代码时都会运行生成嵌入的代码。有没有办法只运行一次或检查嵌入文件夹是否为空，然后运行该代码。
from langchain_community.document_loaders import WebBaseLoader
从 langchain_community.document_loaders 导入 TextLoader
从 langchain_community.vectorstores 导入 Chroma
从 langchain_community 导入嵌入
从 langchain_community.chat_models 导入 ChatOllama
从 langchain_core.runnables 导入 RunnablePassthrough
从 langchain_core.output_parsers 导入 StrOutputParser
从 langchain_core.prompts 导入 ChatPromptTemplate
从 langchain.output_parsers 导入 PydanticOutputParser
从 langchain.text_splitter 导入 CharacterTextSplitter
从 langchain_community.embeddings 导入 OllamaEmbeddings

model_local = ChatOllama(model=&quot;codellama:7b&quot;)

loader = TextLoader(“remedy.txt”)
raw_doc = loader.load()

# 将文本文件内容分割成块
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
splitted_docs = text_splitter.split_documents(raw_doc)

# 使用嵌入函数将它们存储在向量数据库中
ollamaEmbeddings = embeddings.ollama.OllamaEmbeddings(model=“nomic-embed-text”)


# 使用色度向量数据库来存储数据
矢量存储 = Chroma.from_documents(
    文档=splitted_docs，
    嵌入=ollamaEmbeddings，
    persist_directory=&quot;./vector/my_data&quot;,
）

# 这会将数据写入本地
检索器 = vectorstore.as_retriever()

# 4. RAG 之后
print(&quot;RAG 之后\n&quot;)
after_rag_template = “””
    仅根据以下上下文回答问题：
    {语境}
    问题{问题}？
”“”
after_rag_prompt = ChatPromptTemplate.from_template(after_rag_template)
after_rag_chain = (
    {“上下文”：检索器，“问题”：RunnablePassthrough()}
    | after_rag_prompt
    |模型本地
    | StrOutputParser()
）
print(after_rag_chain.invoke(“普通感冒的家庭疗法是什么？”))
]]></description>
      <guid>https://stackoverflow.com/questions/78214495/prevent-create-embeddings-if-folder-already-present-chromadb</guid>
      <pubDate>Sun, 24 Mar 2024 12:31:15 GMT</pubDate>
    </item>
    <item>
      <title>模型精度低-Kaggle Titanic数据集</title>
      <link>https://stackoverflow.com/questions/78214016/low-model-accuracy-kaggle-titanic-dataset</link>
      <description><![CDATA[对于 Kaggle 上的泰坦尼克号挑战，我应用了几个预处理步骤和特征工程技术：
1.将工单列拆分为数字和字母，将它们视为分类特征：
X[“Ticket”] = X[“Ticket”].apply(lambda x: 1 if str(x).isdigit() else 0)

2.删除名称列：
X = X.drop([“名称”,“票”], axis=1)

3.将 Cabin 列中的缺失值视为单独的类别：
X[“Cabin”] = X[“Cabin”].apply(lambda x: 1 if pd.isna(x) else 0)

4.使用SimpleImputer使用均值策略填充缺失值，并删除Embarked列中包含缺失值的行。
`X_reshape = X[:, 2].reshape(-1, 1) # 输入应该是二维数组，但 X[:, 3] 是一维数组。
imputer = SimpleImputer（missing_values=np.nan，strategy=“mean”）
imputer.fit(X_reshape)

X[:, 2] = imputer.transform(X_reshape).flatten()`

5.使用ColumnTransformer进行标准缩放和数据分割。
from sklearn.preprocessing import StandardScaler
sc = 标准缩放器()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

尽管做出了这些努力，模型的准确度仍保持在 75%，而期望的准确度为 81%，标准差为 4%。我尝试过不同的模型，但尚未达到预期的性能。任何改进建议将不胜感激。
整个预处理步骤：
`# 分离票。如果有字母表则为 0，如果没有则为 1。

X[“票”] = X[“票”].apply(lambda x: 1 if str(x).isdigit() else 0)

# 删除名称列
X = X.drop([“名称”], axis=1)

从 sklearn.impute 导入 SimpleImputer

# 在 Cabin 中引入 NaN 值作为新类别。
X[“Cabin”] = X[“Cabin”].apply(lambda x: 1 if pd.isna(x) else 0)
X = X.值

＃ 年龄
X_reshape = X[:, 2].reshape(-1, 1) # 输入应该是二维数组，但 X[:, 3] 是一维数组。

imputer = SimpleImputer(missing_values=np.nan,策略=“平均值”)
imputer.fit(X_reshape)

X[:, 2] = imputer.transform(X_reshape).flatten()

# 上船了。因为只有两个样本的 Embarked 列为 NaN，所以我们可以删除这两行。
# train_df[train_df[&#39;Embarked&#39;].isnull()]
train_df.drop(索引=61，轴=0，就地=True)
train_df.drop(索引=829，轴=0，就地=True)

从 sklearn.compose 导入 ColumnTransformer
从 sklearn.preprocessing 导入 OneHotEncoder

ct = ColumnTransformer([(“编码”, OneHotEncoder(), [1, 6, 7])], 余数=“直通”)
X = np.array(ct.fit_transform(X))

从 sklearn.preprocessing 导入 StandardScaler
sc = 标准缩放器()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)`
]]></description>
      <guid>https://stackoverflow.com/questions/78214016/low-model-accuracy-kaggle-titanic-dataset</guid>
      <pubDate>Sun, 24 Mar 2024 09:36:05 GMT</pubDate>
    </item>
    <item>
      <title>错误消息['OneHotEncoder'对象没有属性'_drop_idx_after_grouping']</title>
      <link>https://stackoverflow.com/questions/78212580/error-messageonehotencoder-object-has-no-attribute-drop-idx-after-grouping</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78212580/error-messageonehotencoder-object-has-no-attribute-drop-idx-after-grouping</guid>
      <pubDate>Sat, 23 Mar 2024 21:35:23 GMT</pubDate>
    </item>
    <item>
      <title>统一构建 APK [关闭]</title>
      <link>https://stackoverflow.com/questions/78212208/building-apk-in-unity</link>
      <description><![CDATA[我做了一个统一项目，它采用机器学习模型来预测图像的类别。它在统一中完美运行。但是当我构建 apk 文件并在我的手机中运行它时。没有什么是可见的。即预测是不可见的。为什么 ？我直接在unity中集成了keras模型，是不是因为我的手机没有安装python？
Android 手机上的结果Unity 上的结果
当我提取 apk 时，我没有在 apk 中看到我的模型或其他详细信息。怎么解决这个问题。如何在我的 apk 中添加 Unity 中使用的所有文件夹和文件]]></description>
      <guid>https://stackoverflow.com/questions/78212208/building-apk-in-unity</guid>
      <pubDate>Sat, 23 Mar 2024 19:38:12 GMT</pubDate>
    </item>
    <item>
      <title>无法从“jax”导入名称“linear_util”</title>
      <link>https://stackoverflow.com/questions/78210393/cannot-import-name-linear-util-from-jax</link>
      <description><![CDATA[我正在尝试重现S5模型的实验，https://github.com/lindermanlab/ S5，但是在解决环境的时候遇到了一些问题。当我运行 shell 脚本./run_lra_cifar.sh时，出现以下错误
回溯（最近一次调用最后一次）：
  文件“/Path/S5/run_train.py”，第3行，在&lt;module&gt;中。
    从 s5.train 导入火车
  文件“/Path/S5/s5/train.py”，第7行，在&lt;module&gt;中。
    从.train_helpers导入create_train_state，reduce_lr_on_plateau，\
  文件“/Path/train_helpers.py”，第 6 行，在  中。
    从 flax.training 导入 train_state
  文件“/Path/miniconda3/lib/python3.12/site-packages/flax/__init__.py”，第 19 行，在  中
    从 。导入核心
  文件“/Path/miniconda3/lib/python3.12/site-packages/flax/core/__init__.py”，第 15 行，在  中
    从 .axes_scan 导入广播
  文件“/Path/miniconda3/lib/python3.12/site-packages/flax/core/axes_scan.py”，第 22 行，在  中
    从 jax 导入 Linear_util 作为 lu
ImportError：无法从“jax”导入名称“linear_util”（/Path/miniconda3/lib/python3.12/site-packages/jax/__init__.py）

我在 RTX4090 上运行它，我的 CUDA 版本是 11.8。我的jax版本是0.4.25，jaxlib版本是0.4.25+cuda11.cudnn86
我首先尝试使用作者的安装依赖项
pip install -rrequirements_gpu.txt

但是，这似乎不适用于我的情况，因为我什至无法导入 jax。所以我根据 https://jax.readthedocs.io/en 上的说明安装了 jax /latest/installation.html
通过输入
pip install --upgrade pip
pip install --upgrade “jax[cuda11_pip]” -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html

到目前为止我已经尝试过：

使用较旧的 GPU（3060 和 2070）
将 python 降级到 3.9

有谁知道可能出了什么问题吗？感谢任何帮助]]></description>
      <guid>https://stackoverflow.com/questions/78210393/cannot-import-name-linear-util-from-jax</guid>
      <pubDate>Sat, 23 Mar 2024 09:57:12 GMT</pubDate>
    </item>
    <item>
      <title>Python：运行保存的 SVM 模型时出错：ValueError：X 有 2943 个功能，但 SVC 期望 330320 个功能作为输入</title>
      <link>https://stackoverflow.com/questions/78207432/python-error-while-running-saved-svm-model-valueerror-x-has-2943-features-bu</link>
      <description><![CDATA[我使用 Sickit-learn 创建了一个 SVM 模型：
导入 pandas 作为 pd
df = pd.read_csv(r&quot;C:\Users\aaa\Documents\bbb\svm_.csv&quot;, 编码=&#39;latin1&#39;, sep=&#39;;&#39;)

从 imblearn.over_sampling 导入 RandomOverSampler
过采样器 = RandomOverSampler(sampling_strategy=&#39;auto&#39;, random_state=42)

X_resampled, y_resampled = oversampler.fit_resample(df.drop(columns=[&#39;alvo&#39;]), df[&#39;alvo&#39;])

df_resampled = pd.concat([X_resampled, pd.DataFrame({&#39;alvo&#39;: y_resampled})], axis=1)

打印（df_重采样）

将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.svm 导入 SVC
从sklearn.metrics导入accuracy_score，classification_report
从 sklearn.preprocessing 导入 OneHotEncoder

X = df_resampled.drop(columns=[&#39;alvo&#39;])
y = df_resampled[&#39;alvo&#39;]

编码器 = OneHotEncoder()
X_encoded = 编码器.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=2)

svm_classifier = SVC(kernel=&#39;poly&#39;, random_state=42)

svm_classifier.fit(X_train, y_train)

y_pred = svm_classifier.predict(X_test)

准确度=准确度_得分(y_test, y_pred)
分类报告结果 = 分类报告(y_test, y_pred)

print(f&quot;准确度: {accuracy}&quot;)
print(&quot;分类报告：\n&quot;,classification_report_result)

进口泡菜

model_file_path = “C:\\Users\\aaa\\Documents\\bbb\\svm_modelo_sent_simnao2.pkl”

打开（model_file_path，&#39;wb&#39;）作为f：
    pickle.dump(svm_classifier, f)

print(&quot;模型保存成功！&quot;)


支持向量机的简单实现来预测分类变量是正确的，工作得很好。
但是，当加载模型并运行以下代码时：
导入 pandas 作为 pd
从 sklearn.preprocessing 导入 OneHotEncoder
导入作业库

model_file_path = &quot;C:\\Users\\AAA\\Documents\\BBB\\svm_modelo_sent_simnao.pkl&quot;
svm_classifier = joblib.load(model_file_path)

new_df = pd.read_csv(r&quot;C:\Users\AAA\Documents\BBB\svm_simnao_rodar.csv&quot;, 编码=&#39;latin1&#39;, sep=&#39;;&#39;)

X_new = new_df.drop(columns=[&#39;alvo&#39;, &#39;ID_ASSUNTO&#39;], axis=1) # 删除 &#39;alvo&#39; 和 &#39;ID_ASSUNTO&#39; 列

categorical_columns = [&#39;UF&#39;, &#39;TIPO_ACAO&#39;, &#39;AREA_JURIDICA&#39;, &#39;VARA_CAMARA&#39;, &#39;CLIENTE_NOME&#39;] # 分类列列表
编码器= OneHotEncoder（类别=&#39;自动&#39;，稀疏=假）
X_new_encoded = 编码器.fit_transform(X_new[categorical_columns])

X_new_processed = pd.concat([pd.DataFrame(X_new_encoded), X_new.drop(columns=categorical_columns)], axis=1)

y_pred_new = svm_classifier.predict(X_new_processed)

new_df[&#39;alvo&#39;] = y_pred_new

预测 = new_df[[&#39;ID_ASSUNTO&#39;, &#39;alvo&#39;]]

打印（预测）

什么会导致以下错误：
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-3-522717c33138&gt;在&lt;模块&gt;中
     22
     23 # 进行预测
---&gt; 24 y_pred_new = svm_classifier.predict(X_new_processed)
     25
     26 # 将预测（&#39;alvo&#39;）添加到新数据中

〜\ AppData \ Roaming \ Python \ Python39 \ site-packages \ sklearn \ svm \ _base.py 在预测（自我，X）
    第818章
    第819章：
--&gt;第820章
    第821章
    第822章

〜\ AppData \ Roaming \ Python \ Python39 \ site-packages \ sklearn \ svm \ _base.py 在预测（自我，X）
    第431章 预测值。
    第432章
--&gt;第433章
    第434章
    第435章 回归预测（X）

_validate_for_predict(self, X) 中的 ~\AppData\Roaming\Python\Python39\site-packages\sklearn\svm\_base.py
    611
...
--&gt;第389章
    [第 390 章]
    [391] f“期待{self.n_features_in_}特征作为输入。”

ValueError: X 有 2943 个特征，但 SVC 预计有 330320 个特征作为输入。

&lt;小时/&gt;
我在部署代码中用于预测 alvo 的数据与我训练模型的数据具有完全相同的结构，并且我在训练和部署代码中都使用 OneHotEncoding...所以我有点迷失在这个之中。
知道如何解决这个问题吗？
提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78207432/python-error-while-running-saved-svm-model-valueerror-x-has-2943-features-bu</guid>
      <pubDate>Fri, 22 Mar 2024 15:57:18 GMT</pubDate>
    </item>
    <item>
      <title>测试精度大于 1，并且一开始就非常高</title>
      <link>https://stackoverflow.com/questions/78197173/testing-accuracy-is-greater-than-1-and-starts-off-very-high</link>
      <description><![CDATA[问题在于，在测试循环中打印时，正确的样本多于样本总数。训练函数正常计算精度，但测试函数始终以 1.1-1.3 的精度开始。此外，这两种准确性一开始都非常高，然后就会下降。
我使用的是 sst2 数据集，批量大小 = 16，总批量为 55。
这是我的数据准备
def preprocess_function（示例）：
        返回分词器（示例[“句子”]，
                         填充=“最大长度”，
                         截断=真，
                         最大长度=MODEL_MAX_LENGTH）
    
    tokenized_datasets = dataset.map(preprocess_function,batched=True)
    tokenized_datasets = tokenized_datasets.remove_columns([“sentence”, “idx”, “attention_mask”])
    tokenized_datasets = tokenized_datasets.rename_column(“标签”, “标签”)
    tokenized_datasets.set_format(“火炬”)

    train_dataset = tokenized_datasets[“train”].shuffle(seed=SEED)
    valid_dataset = tokenized_datasets[“验证”].shuffle(seed=SEED)

    data_collat​​or = DataCollat​​orForLanguageModeling（
        分词器=分词器，
        传销=真实，
        MLM_概率=0.15
    ）

    class_count = [sum(train_dataset[&#39;labels&#39;] == label) 范围内的标签 (NUM_LABELS)]
    class_weights = 1. / torch.tensor(class_count, dtype=torch.float)

    class_weights_all = class_weights[train_dataset[&#39;labels&#39;]]

    加权采样器 = 加权随机采样器（
        权重=class_weights_all，
        num_samples= len(class_weights_all),
        替换=假
    ）

    train_dataloader = 数据加载器(
        训练数据集，
        collat​​e_fn=data_collat​​or,
        批量大小=批量大小，
        采样器=加权采样器，
        pin_memory=真
    ）

    valid_dataloader = 数据加载器(
        有效数据集，
        collat​​e_fn=data_collat​​or,
        批量大小=批量大小，
        pin_memory=真
    ）

这是我的训练和测试循环：
def train_loop(模型,
               训练数据加载器，
               优化器，
               lr_调度程序，
               设备）：
    模型.train()
    总损失= 0
    总正确率 = 0
    样本总数 = 0
    计数器 = 0
    对于步骤，批量枚举（tqdm（train_dataloader））：
        如果计数器 &gt;= 100：
            休息

        batch = {k: v.to(DEVICE) for k, v in batch.items()}

        输出=模型（**批次）
        logits = 输出.logits
        预测 = torch.argmax(logits[:, 8:], -1)
        # print(预测, 预测.size(), len(预测))

        标签=批次[&#39;标签&#39;]
        # print(标签, labels.size(), len(标签))
        正确 = (预测 == 标签).sum().item()
        总正确率 += 正确率
        样本总数 += labels.size(0)

        损失 = 输出.损失
        总损失 += loss.detach()

        # loss.requires_grad = True
        loss.backward()
        优化器.step()
        lr_scheduler.step()
        优化器.zero_grad()

        计数器 += 1

    如果total_samples &gt; 则准确度=total_ Correct /total_samples 0 否则 0
    返回总损失、准确率

def test_loop(模型,
              有效数据加载器，
              设备）：
    模型.eval()
    评估损失 = 0
    总正确率 = 0
    样本总数 = 0
    对于步骤，批量枚举（tqdm（valid_dataloader））：
        batch = {k: v.to(DEVICE) for k, v in batch.items()}

        使用 torch.no_grad()：
            输出=模型（**批次）

        logits = 输出.logits
        预测 = torch.argmax(logits[:, 8:], -1)

        标签=批次[&#39;标签&#39;]
        正确 = (预测 == 标签).sum().item() # 错误行
        总正确率 += 正确率
        样本总数 += labels.size(0)
        print(&quot;正确总数：&quot; + str(total_ Correct) + &quot; ||| 样本总数：&quot; + str(total_samples))

        损失 = 输出.损失
        eval_loss += loss.detach()

    如果total_samples &gt; eval_accuracy =total_ Correct /total_samples 0 否则 0
    返回 eval_loss、eval_accuracy

我想我现在只是没有看到一些东西。我是否使用了相同的变量？]]></description>
      <guid>https://stackoverflow.com/questions/78197173/testing-accuracy-is-greater-than-1-and-starts-off-very-high</guid>
      <pubDate>Thu, 21 Mar 2024 02:12:15 GMT</pubDate>
    </item>
    <item>
      <title>libmagic 不可用，但有助于对类文件对象进行文件类型检测</title>
      <link>https://stackoverflow.com/questions/78186569/libmagic-is-unavailable-but-assists-in-filetype-detection-on-file-like-objects</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78186569/libmagic-is-unavailable-but-assists-in-filetype-detection-on-file-like-objects</guid>
      <pubDate>Tue, 19 Mar 2024 12:21:50 GMT</pubDate>
    </item>
    <item>
      <title>TF2 和 python 中的 BERT 预处理器模型存在问题</title>
      <link>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</link>
      <description><![CDATA[我正在尝试使用 BERT 来做一个文本分类项目。但是我一直遇到这个错误
`
ValueError Traceback（最近一次调用最后一次）
单元格 In[37]，第 4 行
      2 text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
      3 bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
----&gt; 4 preprocessed_text = bert_preprocess(text_input)
      5 bert_encoder = hub.KerasLayer(encoder_url,
      6 可训练=真，
      7 名称=&#39;BERT_编码器&#39;)
      8 个输出 = bert_encoder(preprocessed_text)
ValueError：调用层“预处理”时遇到异常（类型 KerasLayer）。
KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。

调用层“预处理”接收的参数（类型 KerasLayer）：
  输入=
  • 培训=无

KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。



构建此模型时：
&lt;前&gt;&lt;代码&gt;
preprocess_url = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3&#39;
编码器网址 = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/bert-en-uncased-l-12-h-768-a-12/versions/2&#39;

# Bert 层
text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
预处理文本 = bert_preprocess(text_input)
bert_encoder = hub.KerasLayer(encoder_url,
                              可训练=真，
                              名称=&#39;BERT_编码器&#39;)
输出= bert_encoder（预处理文本）

# 神经网络层
l = tf.keras.layers.Dropout(0.1)(输出[&#39;pooled_output&#39;])
l = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;, name=&#39;输出&#39;)(l)

# 构建最终模型
模型 = tf.keras.Model(输入=[text_input], 输出=[l])

我看过无数的教程，甚至使用了张量流文档上的教程，即使我复制和粘贴，它们仍然不起作用。我尝试过不同版本的 tf、tf-text 和 tf-hub。我在这个项目中使用了tensorflow-gpu-jupyter docker 容器。
这是我安装库的方法：
!pip install “tensorflow-text”
!pip install “tf-models-official”
!pip install “tensorflow-hub”

版本是：
张量流：2.16.1
张量流文本：2.16.1
张量流中心：0.16.1
我看到的有关此问题的所有其他论坛都说要执行 tf.config.run_functions_eagerly(True) 但这不起作用。
任何事情都会有所帮助。如果您知道如何解决请回答。]]></description>
      <guid>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</guid>
      <pubDate>Tue, 19 Mar 2024 01:42:01 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 的 Google Colab Bert 实例化错误</title>
      <link>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</link>
      <description><![CDATA[我正在尝试在 Colab 上使用 Tensorflow 构建 Bert 模型。这段代码几周前就可以完美运行。现在，如果我尝试实例化模型，则会收到以下错误：
初始化 TF 2.0 模型 TFBertModel 时未使用 PyTorch 模型的某些权重：[&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls .predictions.transform.LayerNorm.weight&#39;、&#39;cls.predictions.bias&#39;、&#39;cls.seq_relationship.bias&#39;、&#39;cls.predictions.transform.dense.bias&#39;、&#39;cls.seq_relationship.weight&#39;]
- 如果您从在其他任务或其他架构上训练的 PyTorch 模型初始化 TFBertModel（例如，从 BertForPreTraining 模型初始化 TFBertForSequenceClassification 模型），这是预期的。
- 如果您从希望完全相同的 PyTorch 模型初始化 TFBertModel（例如，从 BertForSequenceClassification 模型初始化 TFBertForSequenceClassification 模型），则不会出现这种情况。
TFBertModel 的所有权重都是从 PyTorch 模型初始化的。
如果您的任务与检查点模型训练的任务类似，您就可以使用 TFBertModel 进行预测，而无需进一步训练。
-------------------------------------------------- ------------------------
TypeError Traceback（最近一次调用最后一次）
&lt;ipython-input-14-b0e769ef7​​890&gt;在&lt;细胞系：7&gt;()
      5 SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
      6 SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
----&gt; 7 SC_pooler_output = SC_bert_model(SC_input_layer, Attention_mask=SC_mask_layer)[1] # 第二个输出，che è il pooler_output
      8
      9 # 辍学层的Aggiungi

36帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py 在 type_spec_from_value(value) 中
   1002 3，“无法将 %r 转换为张量：%s” % (类型(值).__name__, e))
   1003
-&gt;第1004章
   第1005章 1005
   1006

TypeError：调用层“嵌入”时遇到异常（类型 TFBertEmbeddings）。

无法为名称构建 TypeSpec：“tf.debugging.assert_less_5/assert_less/Assert/Assert”
op：“断言”
输入：“tf.debugging.assert_less_5/assert_less/All”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_0”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_1”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_2”
输入：“占位符”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_4”
输入：“tf.debugging.assert_less_5/assert_less/y”
属性{
  键：“总结”
  价值 {
    我：3
  }
}
属性{
  键：“T”
  价值 {
    列表 {
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_INT32
      类型：DT_STRING
      类型：DT_INT32
    }
  }
}
 不支持的类型。

调用层“embeddings”接收的参数（类型 TFBertEmbeddings）：
  • input_ids=
  •position_ids=无
  • token_type_ids=
  • input_embeds=无
  •过去的键值长度=0
  • 训练=False

模型的代码是：
SC_input_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“input_ids”)
SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
SC_pooler_output = SC_bert_model（SC_input_layer，attention_mask = SC_mask_layer）[1]

# Dropout 层的Aggiungi
SC_dropout_layer = Dropout(dropout_rate)(SC_pooler_output)
SC_output_layer = 密集（6，激活=&#39;sigmoid&#39;）（SC_dropout_layer）
SC_model = 模型(输入=[SC_input_layer, SC_mask_layer], 输出=SC_output_layer)

我发现安装tensorflow 2.10.0可以工作，但是使用Google Colab时我的CUDA版本有问题，并且使用tensorflow 2.10它无法识别GPU。
该代码几周前就可以工作，有人有解决方案吗？
编辑：同样的错误出现在 Kaggle 上。]]></description>
      <guid>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</guid>
      <pubDate>Sun, 17 Mar 2024 17:03:42 GMT</pubDate>
    </item>
    <item>
      <title>从二维输入预测多个输出的回归问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</link>
      <description><![CDATA[我有几个二维图表，每个图表都有七个独特的数字特征，可用于生成这些图表。我以大量 CSV 文件的形式获得了所有这些图表的 x 和 y 坐标及其数值特征。我想通过使用机器学习或深度学习模型来预测每个图的数值特征（通过使用图的图像或使用每个图的点的坐标）
例如，这是我的一张图表：

该图的独特数字特征是 [8.76e15, 8e-1, 5e-2, 5e-3, 5e-2, 9.65e-1, 2.1e-9] （我有该图所有点的坐标对 (x, y) 以两列 CSV 文件的形式存在，我也可以使用它们）。
到目前为止，我已经寻找了很多预训练的模型，并在 HuggingFace 等网站上搜索了此类模型，还在 GitHub 代码中搜索了很多。我还在 Papers with Code 网站上搜索了做过同样事情的文章，但不幸的是，我仍然没有找到任何东西！我曾多次尝试自己编写一个网络，但由于这样做的复杂性以及对于如何设置网络的超参数以达到预期结果的知识不够，我遇到了很多错误并且无法做到这一点！
例如，我编写了以下代码：
X = []
y = []
目录=“数据”；
对于 os.listdir（目录）中的 csv_file：
    data = pd.read_csv(f&quot;{目录}/{csv_file}&quot;)
    X.append(data.iloc[1:, :2].astype(float).values)
    y.append(data.iloc[0, 2:].astype(float).values)
X = np.array(X, dtype=np.float64) # X.shape: (50000, 253, 2)
y = np.array(y, dtype=np.float64) # y.shape: (50000, 7)

X_train = X[:40000,:,:]
X_val = X[40000:, :, :]
y_train = y[:40000,:]
y_val = y[40000:, :]

定标器=标准定标器()
X_train_scaled = 缩放器.fit_transform(X_train)
X_val_scaled = 缩放器.fit_transform(X_val)

输入 = keras.layers.Input(shape=(X.shape[1], X.shape[2]))
lstm_out = keras.layers.LSTM(32)(输入)
输出 = keras.layers.Dense(7)(lstm_out)

模型= keras.Model（输入=输入，输出=输出）
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=“mse”)
模型.summary()

历史=模型.fit(
    x=X_train,
    y = y_train，
    纪元=10，
）

损失非常高，而且一点也不好。
我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</guid>
      <pubDate>Sat, 16 Mar 2024 07:03:13 GMT</pubDate>
    </item>
    <item>
      <title>sklearn.multiclass.OneVsRestClassifier 中的回调</title>
      <link>https://stackoverflow.com/questions/78119978/callbacks-in-sklearn-multiclass-onevsrestclassifier</link>
      <description><![CDATA[我想使用回调和 eval_set 等。
但我有一个问题：
from sklearn.multiclass import OneVsRestClassifier
导入lightgbm

&lt;前&gt;&lt;代码&gt;详细 = 100
参数 = {
    “目标”：“二元”，
    “n_估计器”：500，
    “详细”：0
}
适合参数= {
    “eval_set”：eval_数据集，
    “回调”：[CustomCallback（详细）]
}

clf = OneVsRestClassifier(lightgbm.LGBMClassifier(**params))
clf.fit(X_train, y_train, **fit_params)

我如何将 fit_params 交给我的估算器？我明白
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- --------------------------
---&gt; 13 clf.fit(X_train, y_train, **fit_params)

TypeError：OneVsRestClassifier.fit() 得到意外的关键字参数“eval_set”
]]></description>
      <guid>https://stackoverflow.com/questions/78119978/callbacks-in-sklearn-multiclass-onevsrestclassifier</guid>
      <pubDate>Thu, 07 Mar 2024 08:59:29 GMT</pubDate>
    </item>
    <item>
      <title>将自定义 ML 模型与 Service Now 预测智能结合使用</title>
      <link>https://stackoverflow.com/questions/75694868/using-custom-ml-model-with-service-now-predictive-intelligence</link>
      <description><![CDATA[我有一个自定义 ML 模型，希望将其与 ServiceNow 的预测智能集成。
我一直在阅读他们的文档，但只能找到一些 ML API 和类的用法，但没有看到是否有办法将其与自定义 ML 模型集成。
SNOW 提供了一个 ClassificationSolution 类来使用内置模型，但我无法继续了解如何在 Predictive Intelligence 中创建我们自己的自定义模型。
文档
ServiceNow 版本 - 圣地亚哥]]></description>
      <guid>https://stackoverflow.com/questions/75694868/using-custom-ml-model-with-service-now-predictive-intelligence</guid>
      <pubDate>Fri, 10 Mar 2023 10:20:26 GMT</pubDate>
    </item>
    </channel>
</rss>