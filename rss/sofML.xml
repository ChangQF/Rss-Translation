<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 06 Aug 2024 01:07:20 GMT</lastBuildDate>
    <item>
      <title>ValueError：在层“conv2d_7”上调用“set_weights(weights)”，权重列表长度为 2，但该层需要 1 个权重</title>
      <link>https://stackoverflow.com/questions/78836514/valueerror-called-set-weightsweights-on-layer-conv2d-7-with-a-weight-list</link>
      <description><![CDATA[我尝试在模型中的某一层上设置权重，但无济于事。
我在网上查找了类似问题的解决方案，但似乎都不起作用。变量“w”（如下面代码所示）的结构为 [numpy array, numpy array]。第一个的大小为 (3, 3, 3, 64)，第二个的形状为 (64,)。我想实现与 tf 2.X 中的“weights”kwarg 类似的功能，但似乎无法让它工作。这是我的代码：
encoder = Sequential()
encoder.add(layers.Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;,use_bias=False,input_shape=(SIZE,SIZE,3)))
w = model.layers[0].get_weights()
encoder.layers[0].set_weights([w])
encoder.add(layers.MaxPooling2D((2, 2),padding=&#39;same&#39;))
encoder.add(layers.Conv2D(32, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;,weights=model.layers[2].get_weights()))
encoder.add(layers.MaxPooling2D((2, 2), padding=&#39;same&#39;))
encoder.add(layers.Conv2D(16, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;,weights=model.layers[4].get_weights()))
encoder.add(layers.MaxPooling2D((2, 2), padding=&#39;same&#39;))
encoder.summary()

错误：
错误：ValueError：您在层“conv2d_7”上调用了`set_weights(weights)`，权重列表长度为 2，但该层需要 1 个权重。
]]></description>
      <guid>https://stackoverflow.com/questions/78836514/valueerror-called-set-weightsweights-on-layer-conv2d-7-with-a-weight-list</guid>
      <pubDate>Mon, 05 Aug 2024 21:33:29 GMT</pubDate>
    </item>
    <item>
      <title>如何通过从 aws s3 存储桶访问视频来获取视频的时间戳，而无需使用 ai 下载视频[关闭]</title>
      <link>https://stackoverflow.com/questions/78835658/how-to-get-timestamps-for-a-video-by-acessing-it-from-aws-s3-bucket-without-down</link>
      <description><![CDATA[这里我使用了 client.invoke_model、boto3.client、&#39;bedrock-runtime&#39; 等，但这里我获得的时间戳超过了视频时长，比如我的视频时长是 5 分钟，但我获得的时间戳长达 9 分钟。这里我不想下载视频，我只想生成时间戳
我尝试使用 openai 密钥，但我没有获得正确的视频时间戳，我该怎么办，实际上我的视频时长是 9 分 40 秒，但这里我得到的是“00:00 Python 会话简介 \n00:15 今天会话的议程 \n00:30 什么是 Python？ \n01:15 为什么 Python 如此受欢迎？ \n02:30 Python 的特性 \n03:45 Python 的简单性和开源性 \n04:15 Python 的可移植性和嵌入性 \n05:00 Python 的解释性和庞大的库支持 \n05:45 Python 中的面向对象概念 \n06:15 使用 Python 的公司 \n07:30 开始学习 Python \n09:00 Python 基础知识：变量、数据类型和运算符 \n10:15 了解 Python 中的列表、流控制和函数 \n11:30 Python 中的文件处理和面向对象编程 \n12:45 掌握 Python 和职业机会 \n14:15 Web 应用开发、游戏开发和大数据分析 \n15:30 数据科学、机器学习和人工智能 \n16:45 智能物联网设备和其他职业机会\n17:30 总结与告别“喜欢这个]]></description>
      <guid>https://stackoverflow.com/questions/78835658/how-to-get-timestamps-for-a-video-by-acessing-it-from-aws-s3-bucket-without-down</guid>
      <pubDate>Mon, 05 Aug 2024 17:17:55 GMT</pubDate>
    </item>
    <item>
      <title>如何将 adaboost 与 xgboost 结合起来</title>
      <link>https://stackoverflow.com/questions/78835596/how-to-combine-adaboost-with-xgboost</link>
      <description><![CDATA[这是一个基于 5 个特征的 3 类分类问题。y 的 3 个类别（标签）分别为 0、1 和 2。我尝试使用 xgboost 模型作为基础估计器并将其放入 adabboost 中。在训练集上拟合模型后，我使用这个拟合模型来预测测试集。但是测试集上的预测值全都是0。
如何解决这个问题？
我的代码在这里
x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.3, random_state=10)
ss_x = StandardScaler()
x_train_transformed = ss_x.fit_transform(x_train)
x_test_transformed = ss_x.transform(x_test)
le_y = LabelEncoder()
y_train_transformed = le_y.fit_transform(y_train)
y_test_transformed = le_y.transform(y_test)

base_model = xgb.XGBClassifier()
model = AdaBoostClassifier(estimator=base_model)
model.fit(x_train_transformed, y_train_transformed)
model_test_sc = accuracy_score(y_test_transformed, model.predict(x_test_transformed))
conf_matrix = confused_matrix(y_test_transformed, model.predict(x_test_transformed),labels=[0,1,2])

我不确定这样将两个助推器结合起来是否合理。]]></description>
      <guid>https://stackoverflow.com/questions/78835596/how-to-combine-adaboost-with-xgboost</guid>
      <pubDate>Mon, 05 Aug 2024 17:00:45 GMT</pubDate>
    </item>
    <item>
      <title>如何让孤立森林在差异的峰值处检测异常，而不是在看到的第一个值处检测异常</title>
      <link>https://stackoverflow.com/questions/78835539/how-to-make-isolation-forest-detect-anomaly-at-the-peak-of-the-difference-inste</link>
      <description><![CDATA[我正在使用孤立森林来识别非常大的数据框中的异常。数据很嘈杂，因此我进行了许多过滤操作来消除噪音，以便数据中存在的真实异常脱颖而出。然后，我在此数据集上使用 .diff() 创建一条直线，当异常发生时，该直线会突然出现峰值。然后使用孤立森林来识别这些异常。
我的问题是，孤立森林在它可以检测到异常发生的最早时间点识别异常，但我需要它在峰值差异处检测到它。
df[&quot;Ref Wt. Denoised&quot;] = denoise(df[&quot;Ref Wt.&quot;].values, level=2)
df[&quot;Ref Wt. Savgol&quot;] = apply_savgol_filter(df[&quot;Ref Wt. Denoised&quot;], window_length=101, polyorder=3)
df[&quot;Ref Wt. Smoothed&quot;] = df[&quot;Ref Wt. Savgol&quot;].rolling(window=indexer).mean()
df[&quot;Ref Wt. Diff&quot;] = df[&quot;Ref Wt. Smoothed&quot;].diff(periods=300).fillna(0)

df[&quot;WOB Anomaly&quot;] = detect_wob.predict(df[&quot;Ref Wt. Diff&quot;].values.reshape(-1, 1))

df[&quot;WOB Zero Event&quot;] = df[&quot;WOB Anomaly&quot;] == -1

我尝试使用 .shift() 来修复它，但这种手动更改对某些值有效，但不是全部。我真的想避免更改用于平滑数据的窗口大小，因为这会严重影响准确性。
我正在寻找的问题和修复的图片]]></description>
      <guid>https://stackoverflow.com/questions/78835539/how-to-make-isolation-forest-detect-anomaly-at-the-peak-of-the-difference-inste</guid>
      <pubDate>Mon, 05 Aug 2024 16:43:41 GMT</pubDate>
    </item>
    <item>
      <title>深度学习中使用拓扑数据分析的示例</title>
      <link>https://stackoverflow.com/questions/78834875/example-of-using-topological-data-analysis-in-deep-learning</link>
      <description><![CDATA[以下是论文：Daniel Leykam 和 Dimitris G. Angelakis，拓扑数据分析和机器学习。
我想要一些在深度学习模型中使用和实现拓扑数据分析的示例、教程和源代码。]]></description>
      <guid>https://stackoverflow.com/questions/78834875/example-of-using-topological-data-analysis-in-deep-learning</guid>
      <pubDate>Mon, 05 Aug 2024 14:13:42 GMT</pubDate>
    </item>
    <item>
      <title>Deepface - 仅显示置信度超过阈值的面部表情</title>
      <link>https://stackoverflow.com/questions/78834690/deepface-only-display-the-emotion-for-faces-with-confidence-over-threshold</link>
      <description><![CDATA[我有一段可以检测面部表情的代码，但是它会在没有表情的地方找到面部表情。所以我想知道是否可以放心地做到这一点。
我尝试在谷歌上搜索如何做到这一点，但没有找到任何有用的东西。
import cv2
from deepface import DeepFace

# 加载人脸级联分类器
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + &#39;haarcascade_frontalface_default.xml&#39;)

# 开始捕获视频
cap = cv2.VideoCapture(&#39;ap.mp4&#39;)

while True:
# 逐帧捕获
ret, frame = cap.read()

# 将帧转换为灰度
gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

# 将灰度帧转换为 RGB 格式
rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)

# 检测帧中的脸部
faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
for (x, y, w, h) in faces:
# 提取脸部 ROI（感兴趣区域）
face_roi = rgb_frame[y:y + h, x:x + w]

# 对脸部 ROI 进行情绪分析
result = DeepFace.analyze(face_roi, action=[&#39;emotion&#39;], force_detection=False)

# 确定主要情绪
print(result[0][&#39;emotion&#39;])
emotion = result[0][&#39;dominant_emotion&#39;]

# 在脸部周围绘制矩形并标记预测情绪
cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)
cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)

# 显示结果帧
cv2.imshow(&#39;Real-time Emotion Detection&#39;, frame)

# 按“q”退出
if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
break

# 释放捕获并关闭所有窗口
cap.release()
cv2.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/78834690/deepface-only-display-the-emotion-for-faces-with-confidence-over-threshold</guid>
      <pubDate>Mon, 05 Aug 2024 13:30:34 GMT</pubDate>
    </item>
    <item>
      <title>一维变分自动编码器重构不佳</title>
      <link>https://stackoverflow.com/questions/78834575/bad-reconstruction-of-1-d-variational-autoencoder</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78834575/bad-reconstruction-of-1-d-variational-autoencoder</guid>
      <pubDate>Mon, 05 Aug 2024 13:03:53 GMT</pubDate>
    </item>
    <item>
      <title>yolov9 在自定义数据上进行训练</title>
      <link>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</link>
      <description><![CDATA[我正尝试在 PyCharm 而不是 google colab 上用一些自定义数据训练 yolov9。我该怎么做？
将存储库克隆到我的计算机后，我在虚拟环境中安装了所有要求。然后我创建了训练脚本，但我觉得有些短。
这是我的训练脚本：
import os
import subprocess

dataset_path = &#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject&#39;

def train_yolov5(train_images_path, val_images_path, yaml_file_path, weights_path=&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main/yolov9-c.pt&#39;, epochs=50):

# 获取 yolov5 目录的绝对路径
yolov9_dir = os.path.abspath(&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main&#39;)

# 将当前工作目录更改为 yolov9 目录
os.chdir(yolov9_dir)
# 训练 yolov9 模型
command = f&#39;python train.py --workers 8 --device cpu --batch 16 --data {dataset_path}/sfdV2_musa.yaml --img 640 --cfg models/detect/yolov9-c.yaml --weights yolov9-c --hyp hyp.scratch-high.yaml --min-items 0 --epochs 5 --close-mosaic 15&#39;

# 执行命令
process = subprocess.Popen(command, shell=True)
process.wait()

if __name__ == &quot;__main__&quot;:
TRAIN_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/train&#39;)
VAL_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/val&#39;)
YAML_FILE_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/sfdV2_musa.yaml&#39;)

# 训练 YOLOv9 模型
train_yolov5(TRAIN_IMAGES_PATH, VAL_IMAGES_PATH、YAML_FILE_PATH）`
]]></description>
      <guid>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</guid>
      <pubDate>Mon, 05 Aug 2024 12:28:07 GMT</pubDate>
    </item>
    <item>
      <title>深度学习训练策略：避免对单个训练图像进行打乱，而是对批次进行打乱？[关闭]</title>
      <link>https://stackoverflow.com/questions/78834079/deep-learning-training-strategy-avoid-shuffling-individual-training-images-ins</link>
      <description><![CDATA[我正在为工业环境中的应用训练 YOLO（你只看一次）物体检测器。由于使用固定的相机设置，图像的背景与相机有关，但不会随时间而变化（除了测量部件的位置不准确性）。
从数据科学的角度来看，这意味着：我的训练数据可以逻辑地分组为 N 个相机视图，每个视图包含一个特定的背景。
我的想法如下：
我不会在一个大数据集内混洗所有图像，而是只混洗批次，这样相机视图就不会在单个训练批次中混淆。使用这种方法，我希望能够更具体地学习背景视图的特殊属性。
权衡将是“牺牲”神经网络的一些泛化能力（在这个特殊应用中不是必需的）以便在这个专门的设置中获得更好的性能。
使用这种方法更新权重会有什么不同？一些文献也很酷，但我在研究期间没有发现类似的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78834079/deep-learning-training-strategy-avoid-shuffling-individual-training-images-ins</guid>
      <pubDate>Mon, 05 Aug 2024 10:53:19 GMT</pubDate>
    </item>
    <item>
      <title>用于聚类的机器学习模型（与 K-means 类似，但功能不同）[关闭]</title>
      <link>https://stackoverflow.com/questions/78833002/machine-learning-model-for-clustering-similar-with-k-means-but-different-funct</link>
      <description><![CDATA[当我研究几个机器学习模型时，我看到了几个聚类算法，包括K-Means。
据我所知K-Means使用欧几里德距离作为他们自己的计算方法，
我想要的不是使用欧几里德距离，而是数据的值。
例如，样本分布很广（如坐标），坐标有自己的值。
我想找到平均值较高的N个簇。
有没有其他适合这个图的算法，或者我是否只处理K-Means算法中的几个参数来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78833002/machine-learning-model-for-clustering-similar-with-k-means-but-different-funct</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>createDataPartition 给出异常不均匀的测试和训练集</title>
      <link>https://stackoverflow.com/questions/78832537/createdatapartition-gives-abnormally-uneven-test-and-train-sets</link>
      <description><![CDATA[我正在尝试使用 caret 包将我的数据拆分为测试集和训练集。我有 77 行，每列都有完整的数据。函数“createDataPartition”导致训练数据为 4 行，测试数据为 73 行，这似乎不对。任何帮助都将不胜感激。这是我的代码：
&gt; library(caret)
&gt; # 将数据拆分为训练和测试
&gt; set.seed(123)
&gt; data.full &lt;- data.full %&gt;% select(fasting_status, a1c, glu, uc_ratio)
&gt; training.samples &lt;- data.full %&gt;% 
+ createDataPartition(p = 0.8, list = FALSE)
警告消息：
1：在 createDataPartition(., p = 0.8, list = FALSE) 中：
某些类没有记录 ( )，这些将被忽略
2：在 createDataPartition(., p = 0.8, list = FALSE) 中：
某些类只有一条记录 ( )，这些将被选为样本
&gt; train.data &lt;- data.full[training.samples, ]
&gt; test.data &lt;- data.full[-training.samples, ] 

以下是我的可重现数据：
&gt; dput(data.full) 结构(列表(fasting_status = 结构(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L), 级别 = c(&quot;1&quot;, &quot;2&quot;), 类别 = &quot;因素&quot;), 
a1c = c(4.3, 4.5, 4.4, 2.9, 4.3, 4.4, 4.2, 4.5, 4.2, 4.2, 
4.5, 4.5, 4.8, 4.5, 5.2, 4.9, 4.6, 4.2, 4.4, 4.9, 4.6, 4.5, 
    4.4、4.8、4.5、4.1、3.8、3.1、4.3、4.6、4.7、4.9、4.6、4.4、3.1、4.6、4.4、4.2、4.4、5.2、4.4、5.1、4.6、4.7、5.2、4.7、4。 7、4.6、4.4、4.4、4.2、4.5、4.6、4.4、3.2、4.8、5.2、5.2、4.6、4.9、5.6、4.6、4.9、4.5、5.1、4.6、4.9、4.6、4.3、4.6、
4.6, 4.3, 4.6, 4.3, 4.6, 6.5, 4.8), glu = c(88.5, 98, 117.5, 
53, 108.5, 106, 105, 101, 91, 99.5, 128.5, 113, 114, 121.5, 
121, 131.5, 160.5, 96, 110, 140, 119.5, 115.3, 112, 143.5, 
116.5, 116.5, 111, 139.5, 123.5, 131, 113, 137, 114, 98.5, 
    124.5、123.5、111.5、111、127、123、137.5、119、107、130.5、142.5、115、133.5、119、148.3、125.5、138.5、106.5、153.5、 .5、179、145、143、124.5、134、146.5、127.5、124.5、123、129、145.3、125.5、146.5、153.5、115.5、128、110.5、131、 
139.5, 124, 154, 94, 76.3), uc_ratio = c(30.65603924, 15.32801962, 
60.59075991, 7.39973361, 57.84661317, 27.46781116, 16.0944206, 
6.131207848, 94.61568474, 19.50838861, 7.803355443, 19.41549152, 
7.464079119, 19.67095851, 29.50643777, 62.94706724, 80.472103、25.75107296、73.57449418、39.01677721、41.13018598、10.62933697、7.803355443、30.04291845、32.75355771、 9416、5.969860273、22.72153497、7.153075823、75.61823012、23.50296342、53.64806867、11.19611891、38.25340549、 88.36152487、51.50214592、9.196811772、41.98544505、6.35828962、9.196811772、94.87237407、12.87553648、6.035407725、7.3997 3361、10.72961373、11.70503316、9.035464197、16.34988759、11.68917269、35.11509949、61.85306741、11.36076748、 
    12.2624157、7.153075823、14.30615165、10.40447392、3.901677721、52.11526671、21.45922747、30.49469166、81.06819266、 38861、34.33476395、8.0472103、24.94635193、9.754194304、64.3776824、9.196811772、11.92179304、34.87124464、 74.39198856, 124.4635193, 
13.79521766, 5.722460658, 66.76204101, 69.9757432, 19.50838861
)), row.names = c(NA, -77L), class = &quot;data.frame&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78832537/createdatapartition-gives-abnormally-uneven-test-and-train-sets</guid>
      <pubDate>Mon, 05 Aug 2024 02:29:00 GMT</pubDate>
    </item>
    <item>
      <title>检测并定位图像中的多个物体[关闭]</title>
      <link>https://stackoverflow.com/questions/78830813/detect-and-localize-multiple-objects-in-the-image</link>
      <description><![CDATA[尝试对以下给定图像及其模板应用特征匹配。我想匹配给定图像中的模板，并在匹配的对象周围绘制框。这些只是示例图像，我还有很多不同分辨率的图像，所以不能使用模板匹配。以下是代码。
import cv2
import numpy as np
# 加载图像
img1 = cv2.imread(&#39;C:/Users/Autobobcat/Desktop/Candy- crush.23.jpg&#39;, cv2.IMREAD_COLOR) # 查询图像
img2 = cv2.imread(&#39;C:/Users/Autobobcat/Desktop/blueVertical.png&#39;, cv2.IMREAD_COLOR) # 训练图像
# 将图像转换为灰度
gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
# 创建 SIFT 检测器
sift = cv2.SIFT_create()
# 使用 SIFT 查找关键点和描述符
kp1, des1 = sift.detectAndCompute(gray1, None)
kp2, des2 = sift.detectAndCompute(gray2, None)
# 使用默认参数创建 BFMatcher 对象
bf = cv2.BFMatcher()
# 使用 KNN 匹配描述符
matches = bf.knnMatch(des1, des2, k=2)
# 应用具有更宽松阈值的比率测试
good_matches = []
for m, n in matches:
if m.distance &lt; 0.3 * n.distance: # 调整比例测试
good_matches.append(m)
# 按距离对匹配项进行排序
good_matches = sorted(good_matches, key=lambda x: x.distance)
# 绘制匹配项
img_matches = cv2.drawMatches(img1, kp1, img2, kp2, 
good_matches[:50], None, 
flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
# 定义目标屏幕尺寸
target_width = 1280
target_height = 720
# 计算图像的纵横比
(h, w) = img_matches.shape[:2]
aspect_ratio = w / h
# 确定新的尺寸并保持纵横比
如果 w &gt; target_width 或 h &gt;目标高度：
如果纵横比 &gt; 1：# 图像宽度大于高度
new_width = target_width
new_height = int(target_width /aspect_ratio)
else：# 图像高度大于宽度
new_height = target_height
new_width = int(target_height *aspect_ratio)
else：
new_width, new_height = w, h
dim = (new_width, new_height)
# 调整图像大小
resized_img_matches = cv2.resize(img_matches, dim, 
interpolation=cv2.INTER_AREA)
# 显示结果
cv2.imshow(&#39;Feature Matching&#39;, resized_img_matches)
cv2.waitKey(0)
cv2.destroyAllWindows()



]]></description>
      <guid>https://stackoverflow.com/questions/78830813/detect-and-localize-multiple-objects-in-the-image</guid>
      <pubDate>Sun, 04 Aug 2024 10:33:33 GMT</pubDate>
    </item>
    <item>
      <title>DSPy 无法检索 ChromaDB 中带有文本嵌入的段落</title>
      <link>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</link>
      <description><![CDATA[我正在使用 DSPy 和 ChromaDB 为 pdf 文件开发 RAG 应用程序。
首先，我从 pdf 中获取文本并将其作为块添加到 Chromadb。还添加了块的嵌入。并尝试使用 DSPy 检索与查询相关的块。但是它出现了错误
存储数据和嵌入
def store_document_in_chromadb(text):
chunks = chunk_document(text)
ids = [f&#39;chunk_{i}&#39; for i in range(len(chunks))]
embeddings = [get_embedding(chunk).tolist() for chunk in chunks]

collection.add(ids=ids, documents=chunks, embeddings=embeddings)

我尝试像这样检索相关块，
retriever_model = ChromadbRM(&quot;contracts_collection&quot;, &#39;db/&#39;, k=2)
dspy.settings.configure(lm=llama2_model, rm=retriever_model)

class GenerateAnswer(dspy.Signature): 
“”“”根据给出的上下文回答问题。“”“”
context = dspy.InputField(desc=&quot;可能包含相关上下文&quot;)
question = dspy.InputField()
answer = dspy.OutputField(desc=&quot;通常为 5 到 10 个单词&quot;)

class RAG(dspy.Module): 
def __init__(self, num_passages=2):
super().__init__()
self.retrieve = dspy.Retrieve(k=num_passages)
self.generate_answer = dspy.ChainOfThought(GenerateAnswer)

def forward(self, question):
context = self.retrieve(question).passages
prediction = self.generate_answer(context=context, question=question)
return dspy.Prediction(context=context, answer=prediction.answer)

with dspy.context(lm=llama2_model, rm=retriever_model):
module = RAG()
response = module(&quot;总支出是多少&quot;)
print(response)

当我运行此程序时，出现此错误
InvalidDimensionException：嵌入维度 384 与集合维度 768 不匹配
但是当我从 ChromaDB 中删除嵌入时，它会正确检索相关块。
为什么使用嵌入时没有出现此错误？]]></description>
      <guid>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</guid>
      <pubDate>Wed, 17 Jul 2024 08:03:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在微调过程中正确设置 pad token（而不是 eos）以避免模型无法预测 EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么 xgboost.QuantileDMatrix 使用自定义数据迭代器对数据进行四次传递？</title>
      <link>https://stackoverflow.com/questions/76569411/why-does-xgboost-quantiledmatrix-do-four-passes-of-the-data-with-custom-data-ite</link>
      <description><![CDATA[由于我的数据集太大，我尝试使用此处所示的自定义数据迭代器。为了测试其工作原理，我使用示例的一个子集并运行以下代码。 X 是我的数据的 numpy 数组。
我的迭代器如下所示
class IterForQDMatrix(xgb.core.DataIter):
def __init__(self, df, batch_size):
self.df = df
self.batch_size = batch_size
self.batches = np.ceil(len(df) // self.batch_size)
self.it = 0
super().__init__()

def reset(self):
self.it = 0

def next(self, input_data):
if self.it == self.batches:
print(&quot;done&quot;)
return 0
a = self.it * self.batch_size
b = min((self.it + 1) * self.batch_size, len(self.df))
input_data(data=self.df[a:b, : -1], label=self.df[a:b, -1])
self.it += 1
return 1

iterator = IterForQDMatrix(X, 30)
xgb_data = xgb.QuantileDMatrix(iterator)

当我运行上述代码时，我注意到 &quot;done&quot; 被打印了四次，这意味着当我将迭代器传递给 xgb.QuantileDMatrix 时，它会传递整个数据集四次。我试图理解它为什么要传递数据四次。有没有办法只传递一次数据就能实现它所做的事情？]]></description>
      <guid>https://stackoverflow.com/questions/76569411/why-does-xgboost-quantiledmatrix-do-four-passes-of-the-data-with-custom-data-ite</guid>
      <pubDate>Wed, 28 Jun 2023 00:55:19 GMT</pubDate>
    </item>
    </channel>
</rss>