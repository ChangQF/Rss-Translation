<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 04 Jun 2024 06:20:59 GMT</lastBuildDate>
    <item>
      <title>人类与机器意识 [关闭]</title>
      <link>https://stackoverflow.com/questions/78572931/human-versus-machine-consciousness</link>
      <description><![CDATA[在评估人工智能系统时，我试图确定不同的系统在使用算法来确认处理人类提示时对真理的认识，而不是它们自己的内部迭代处理周期时，是否会区分不同形式的意识。人工智能系统是否区分人类意识和形而上学意识？
这是我第一次尝试理解人类意识与机器意识。]]></description>
      <guid>https://stackoverflow.com/questions/78572931/human-versus-machine-consciousness</guid>
      <pubDate>Tue, 04 Jun 2024 00:27:10 GMT</pubDate>
    </item>
    <item>
      <title>运行 fit 函数时，如何解决输入和输出之间的值错误？</title>
      <link>https://stackoverflow.com/questions/78572692/how-could-i-resolve-value-error-between-my-input-and-output-while-running-the-fi</link>
      <description><![CDATA[这是我的数据形状
print(X_train1_padded.shape, y_train1_padded.shape)
print(X_val1_padded.shape, y_val1_padded.shape)
print(X_test1_padded.shape, y_test1_padded.shape)

(203161, 496, 1) (203161, 496, 1)
(118096, 496, 1) (118096, 496, 1)
(85491, 496, 1) (85491, 496, 1)

这是我的模型代码
# 初始化 Sequential 模型
model = Sequential()

# 1D 卷积层1
model.add(Conv1D(8, 4, 激活=&quot;linear&quot;, input_shape=(sequence_len, 1), padding=&quot;same&quot;, strides=1))
model.add(MaxPooling1D(pool_size=2))

# 1D 卷积层 2
model.add(Conv1D(8, 4, 激活=&quot;linear&quot;, padding=&quot;same&quot;, strides=1))
model.add(MaxPooling1D(pool_size=2))

model.add(Flatten())

# 完全连接层
model.add(Dropout(0.2))
model.add(Dense((sequence_len - 3) * 8, 激活=&#39;relu&#39;)) # 调整后的输入大小

model.add(Dropout(0.2))
model.add(Dense(128,激活=&#39;relu&#39;))

model.add(Dropout(0.2))
model.add(Dense((sequence_len - 3) * 8, 激活=&#39;relu&#39;)) # 调整输入大小

model.add(Dropout(0.2))

# UpSampling1D 第 1 层
model.add(Reshape(((sequence_len-3), 8)))
model.add(UpSampling1D(size=2))
model.add(Conv1D(1, 4, 激活=&quot;linear&quot;, 填充=&quot;same&quot;, strides=1))

# UpSampling1D 第 2 层
model.add(UpSampling1D(size=2))
# 调整过滤器数量和内核大小以匹配所需的输出形状
model.add(Conv1D(1, 4, 激活=&quot;linear&quot;, 填充=&quot;same&quot;, strides=1))

model.summary()
optimizer = SGD(lr=0.01, motivation=0.9, decay=1e-6)
model.compile(loss=&#39;mse&#39;, optimizer=optimizer)

我正在尝试运行它，但主要问题是输入和输出形状不匹配。这是错误
model.fit(X_train1_padded, y_train1_padded, epochs=50, batch_size=16, validation_data=(X_val1_padded, y_val1_padded),shuffle=True)

ValueError：维度必须相等，但对于“{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_11/conv1d_37/BiasAdd, IteratorGetNext:1)”，维度分别为 1972 和 496，输入形状为：[?,1972,1], [?,496,1]。

我无法更改模型，因为这是要求，所以有人可以建议一些与数据形状有关的事情吗？]]></description>
      <guid>https://stackoverflow.com/questions/78572692/how-could-i-resolve-value-error-between-my-input-and-output-while-running-the-fi</guid>
      <pubDate>Mon, 03 Jun 2024 22:23:29 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：new（）：创建 PyTorch 几何数据对象时数据类型“str”无效</title>
      <link>https://stackoverflow.com/questions/78572603/typeerror-new-invalid-data-type-str-when-creating-pytorch-geometric-data-o</link>
      <description><![CDATA[labeled_X 和 unlabeled_X 分别是带注释的参考数据集和目标未注释数据集的节点特征（即基因/蛋白质表达）矩阵。它们应该是形状为 [num_nodes, num_node_features]（即 [num_cells, num_genes]）的 numpy 数组。
labeled_y 定义带注释的参考数据集的注释。它是一个形状为 [num_nodes,]（即 [num_cells]）的 numpy 数组。注释应为数值类类别。
问题：
我正在尝试使用 PyTorch Geometric 的 InMemoryDataset 为图形神经网络创建自定义数据集。但是，在创建 Data 对象时，我遇到了错误 TypeError：new()：无效的数据类型“str”。下面是我的数据集类和回溯：
代码：
class GraphDataset(InMemoryDataset):
def __init__(self, labeled_X, labeled_y, unlabeled_X, labeled_edges, unlabeled_edges, transform=None,):
self.root = &#39;.&#39;
super(GraphDataset, self).__init__(self.root, transform)
self.labeled_data = Data(x=torch.FloatTensor(labeled_X), edge_index=torch.LongTensor(labeled_edges).T, y=torch.LongTensor(labeled_y))
self.unlabeled_data = Data(x=torch.FloatTensor(unlabeled_X), edge_index=torch.LongTensor(unlabeled_edges).T)
def __len__(self):
return 2
def __getitem__(self, idx):
return self.labeled_data, self.unlabeled_data

回溯：
回溯（最近一次调用）：
文件“/scg/apps/software/stellar/13c81ad/stellar/STELLAR_run_yan.py”，第 51 行，位于&lt;module&gt;
main()
文件 &quot;/scg/apps/software/stellar/13c81ad/stellar/STELLAR_run_yan.py&quot;，第 44 行，在 main 中
dataset = GraphDataset(labeled_X, labeled_y, unlabeled_X, labeled_edges, unlabeled_edges)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/scg/apps/software/stellar/13c81ad/stellar/datasets.py&quot;，第 84 行，在 __init__ 中
self.labeled_data = Data(x=torch.FloatTensor(labeled_X), edge_index=torch.LongTensor(labeled_edges).T, y=torch.LongTensor(labeled_y))
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: new(): 无效数据类型 &#39;str&#39;

输入：
labeled_X
array([[0.05318533, 0.10899889, 0.06755578],
[0.06478137, 0.08745035, 0.09479886],
[0.11840215, 0.06308291, 0.07728422],
[0.09797833, 0.09564686, 0.05342986],
[0.11716319, 0.13197362, 0.07772677]])

labeled_X.dtype
dtype(&#39;float64&#39;)

labeled_y
array([0, 0, 2, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 2, 1])

labeled_y.dtype
dtype(&#39;int64&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78572603/typeerror-new-invalid-data-type-str-when-creating-pytorch-geometric-data-o</guid>
      <pubDate>Mon, 03 Jun 2024 21:52:21 GMT</pubDate>
    </item>
    <item>
      <title>多类别多标签分类后如何实现回归？</title>
      <link>https://stackoverflow.com/questions/78572569/how-can-i-implement-regression-after-multi-class-multi-label-classification</link>
      <description><![CDATA[我有一个数据集，其中有些对象（15%）属于不同的类别，并且每个类别都有一个属性值。我如何制作一个预测多标签或多类别的模型，然后根据分类器的输出进行回归预测？我还需要输出每个类别的概率。不幸的是，我无法删除这 15%。
在此处输入图片描述
我不知道如何将它们组合在一起。我只找到了如何单独实现它的方法。有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78572569/how-can-i-implement-regression-after-multi-class-multi-label-classification</guid>
      <pubDate>Mon, 03 Jun 2024 21:32:38 GMT</pubDate>
    </item>
    <item>
      <title>如何使用随机森林计算预测的置信区间？</title>
      <link>https://stackoverflow.com/questions/78572538/how-to-calculate-confidence-interval-for-forecast-using-random-forest</link>
      <description><![CDATA[我正在计算名为“spot”的变量的预测（数据的未来结果）。我使用随机森林和名为“DTCI”的独立变量来协助预测“spot”。预测以每月频率进行，与数据频率相同。我想根据每个月的上限和下限获得每个预测月份的置信区间。它与附图中所做的类似，带有绿色限制。
limit
我尝试使用 GradientBoostingRegressor 构建间隔，如下所示：
# 设置下限和上限分位数
inf = 0.1
sup = 0.9

# 每个模型必须分开
lower_model = GradientBoostingRegressor(loss=&quot;quantile&quot;, alpha=inf)
upper_model = GradientBoostingRegressor(loss=&quot;quantile&quot;, alpha=sup)

lower_model.fit(X_train, y_train)
upper_model.fit(X_train, y_train)

predictions = pd.DataFrame(y_hat_forecast_spot)

predictions[&quot;inf&quot;] = lower_model.predict(X_fore)
predictions[&quot;sup&quot;] = upper_model.predict(X_fore)

然而，结果并没有我预期的趋势。由于它是一个时间序列，我想象（如上图所示）限值应该以置信区域变大的方式增长。换句话说，日期越远，预测就越困难，因此与之相关的误差或间隔就越大。
我使用 GradientBoostingRegressor 发现的结果（如下所示）的间隔会随时间变化而不是增长。
结果
GradientBoostingRegressor 适合时间序列吗？或者还有其他函数可以更好地理解时间序列吗？]]></description>
      <guid>https://stackoverflow.com/questions/78572538/how-to-calculate-confidence-interval-for-forecast-using-random-forest</guid>
      <pubDate>Mon, 03 Jun 2024 21:17:18 GMT</pubDate>
    </item>
    <item>
      <title>在执行无监督域自适应时，属性移位数据的性能下降</title>
      <link>https://stackoverflow.com/questions/78572439/decrease-in-performance-on-attribute-shift-data-on-performing-unsupervised-domai</link>
      <description><![CDATA[我正在对 cifar10 和 cifar10.1 数据集进行实验，其中我使用 Resnet18 模型在 cifar10 训练数据上进行训练，并在 cifar10 测试数据和 cifar10.1（属性移位数据）上进行评估。我使用监督对比学习来训练特征编码器（当前特征向量 dim id 为 64），然后训练投影头（单层 MLP），在评估此模型时，cifar10 测试数据上的 ACC 为 91%，cifar10.1 数据集上的 ACC 为 81%。
为了将特征编码器模型改进为看不见的属性移位数据，我正在使用自监督对比损失对 cifar10.1 数据集上的特征编码器进行微调，但在对微调的特征编码器以及在 cifar10 训练数据上预训练的投影头进行微调时，其性能比未微调的模型更差，微调模型的准确率为 56%（之前为 81%），请问为什么微调模型效果不佳。
为什么在 cifar10.1 上对自动编码器进行微调时，其在 cifar10.1 上的表现更差，在 cifar10 测试集上的表现也很糟糕？]]></description>
      <guid>https://stackoverflow.com/questions/78572439/decrease-in-performance-on-attribute-shift-data-on-performing-unsupervised-domai</guid>
      <pubDate>Mon, 03 Jun 2024 20:39:21 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中用于回归任务的批量归一化</title>
      <link>https://stackoverflow.com/questions/78572327/batch-normalization-in-neural-network-for-regression-task</link>
      <description><![CDATA[我有一个想要适合回归的神经网络。当我在层之间不使用批量归一化时，它表现良好，但是当我添加批量归一化时，它表现非常糟糕。
例如，没有批量归一化的 MAE 是 17000，而使用批量归一化的 MAE 则高达 180000。这是为什么？
X = train_data.drop([&#39;SalePrice&#39;], axis=1)
y = train_data[&#39;SalePrice&#39;]

# 获取每种类型的列
numeric_cols = [col for col in X.columns if X[col].dtype in [&#39;float64&#39;, &#39;int64&#39;]]
categoric_cols = [col for col in X.columns if X[col].dtype == &#39;object&#39;]

X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)

# 转换管道
numeric_transformer = Pipeline(steps=[
(&#39;impute_numeric&#39;, SimpleImputer(strategy=&#39;median&#39;)),
(&#39;scale&#39;, StandardScaler())
])

categoric_transformer = Pipeline(steps=[
(&#39;impute_categoric&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encode&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;)),
])

preprocessing = ColumnTransformer(transformers=[
(&#39;num&#39;, numeric_transformer, numeric_cols),
(&#39;cat&#39;, categoric_transformer, categoric_cols)
])

# 拟合变换器
transformed_X_train = preprocessing.fit_transform(X_train)
transformed_X_val = preprocessing.transform(X_val)

# 模型 14392
input_shape = perceived_X_train.shape[1]
model = keras.Sequential([
layer.Dense(units=256,activation=&#39;swish&#39;,input_shape=[input_shape]),
layers.BatchNormalization(),
layers.Dense(units=256,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=128,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=128,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=64,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=64,activation=&#39;swish&#39;),
layers.BatchNormalization(),
layers.Dense(units=1),
])

# 设置优化器和损失函数
model.compile(
optimizer=&#39;adam&#39;,
loss=&#39;mae&#39;
)

early_stopping = callups.EarlyStopping(
min_delta=0.01, # 计为改进的最小变化量
patient=20, # 需要失败的时期数 min_delta 才能提前停止
restore_best_weights=True,
)

# 拟合神经网络
history = model.fit(
transformed_X_train, y_train,
epochs=200,
batch_size=128,
callbacks=[early_stopping],
validation_data=(transformed_X_val, y_val)
#validation_split=0.2,# 自动创建测试和验证集
)

history_df = pd.DataFrame(history.history)
history_df.loc[0:, [&#39;loss&#39;, &#39;val_loss&#39;]].plot()

# 最佳 13962.4795
print(&quot;Evaluation模型的”，model.evaluate(transformed_X_val, y_val, batch_size=128))
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78572327/batch-normalization-in-neural-network-for-regression-task</guid>
      <pubDate>Mon, 03 Jun 2024 20:03:21 GMT</pubDate>
    </item>
    <item>
      <title>加载预先训练的 json 模型时出错</title>
      <link>https://stackoverflow.com/questions/78570246/error-when-loading-a-pre-trained-json-model</link>
      <description><![CDATA[这是我得到的错误：
回溯（最近一次调用）：
文件“C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\anti.py”，第 14 行，位于 
model = tf.keras.models.model_from_json(loaded_model_json)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\models\model.py”，第 575 行，位于 model_from_json
返回serialization_lib.deserialize_keras_object(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\saving\serialization_lib.py&quot;，第 694 行，位于 deserialize_keras_object
cls = _retrieve_class_or_fn(
^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\saving\serialization_lib.py&quot;， _retrieve_class_or_fn 中的第 812 行
raise TypeError(
TypeError：无法找到类“Functional”。确保自定义类已用 @keras.saving.register_keras_serializable() 修饰。完整对象配置：{&#39;class_name&#39;: &#39;Functional&#39;, &#39;config&#39;:.....(json 文件的内容........&#39;keras_version&#39;: &#39;2.15.0&#39;, &#39;backend&#39;: &#39;tensorflow&#39;)
以下是库版本：
keras 3.3.3
opencv-python 4.9.0.80
tensorflow 2.16.1
python 3.12.3

以下是我的代码：
import cv2
import tensorflow as tf
from tensorflow.keras.preprocessing.image import img_to_array 
import os
import numpy as np

root_dir = os.getcwd()
# 加载人脸检测模型
trained_face_data = cv2.CascadeClassifier(cv2.data.haarcascades + &#39;haarcascade_frontalface_default.xml&#39;)
# 加载反欺骗模型图
json_file = open(&#39;C:/Users/Richard.Joy/Desktop/Final-antispoofing_models/Antispoofing_model_mobilenet.json&#39;,&#39;r&#39;)
loaded_model_json = json_file.read()
json_file.close()
model = tf.keras.models.model_from_json(loaded_model_json)
# 加载反欺骗模型权重 
model.load_weights(&#39;C:/Users/Richard.Joy/Desktop/Final-antispoofing_models/project_antispoofing_model_97-0.957895.h5&#39;)
print(&quot;模型从磁盘加载&quot;)

video = cv2.VideoCapture(0)
while True:
try:
ret,frame = video.read()
gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
faces = training_face_data.detectMultiScale(gray,1.3,5)
for (x,y,w,h) in faces: 
face = frame[y-5:y+h+5,x-5:x+w+5]
resized_face = cv2.resize(face,(160,160))
resized_face = resized_face.astype(&quot;float&quot;) / 255.0
resized_face = np.expand_dims(resized_face, axis=0)
# 将人脸 ROI 传递给训练过的活体检测器
# 模型以确定人脸是“真”还是“假”
preds = model.predict(resized_face)[0]
print(preds)
if preds&gt; 0.5:
标签 = &#39;poof&#39;
cv2.putText(frame, 标签, (x,y - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)
cv2.rectangle(frame, (x, y), (x+w,y+h),
(0, 0, 255), 2)
else:
标签 = &#39;eal&#39;
cv2.putText(frame, 标签, (x,y - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)
cv2.rectangle(frame, (x, y), (x+w,y+h),
(0, 255, 0), 2)
cv2.imshow(&#39;frame&#39;, frame)
key = cv2.waitKey(1)
if key == ord(&#39;q&#39;):
break
except Exception as e: 
pass
video.release() 
cv2.destroyAllWindows()

我该怎么办？我尝试降级库，但也遇到了错误。]]></description>
      <guid>https://stackoverflow.com/questions/78570246/error-when-loading-a-pre-trained-json-model</guid>
      <pubDate>Mon, 03 Jun 2024 12:21:33 GMT</pubDate>
    </item>
    <item>
      <title>梯度累积损失计算</title>
      <link>https://stackoverflow.com/questions/78569958/gradient-accumulation-loss-compute</link>
      <description><![CDATA[假设我们有数据 [b,s,dim]，我最近注意到 CrossEntropyLoss 是 (1) 计算一批中所有 token (b * s) 的平均值，而不是 (2) 计算每个句子然后计算平均值。
以下是计算 hugging_face 转换器 BertLMHeadMode 损失的代码
sequence_output = output[0]
prediction_scores = self.cls(sequence_output)

lm_loss = None
if labels is not None:
# 我们正在进行下一个 token 预测；将预测分数和输入 ID 移位一格
shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()
labels = labels[:, 1:].contiguous()
loss_fct = CrossEntropyLoss()
lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))

我知道 (1) 和 (2) 在这种情况下没有区别。但是当我们应用梯度累积时，我认为情况就不同了。
假设我的batch_size为4，4个句子的长度分别为100,200,300,400。
使用batch_size 4时，损失是根据总共1000个token的平均值计算的。
但是当batch_size = 1且梯度累积= 4时，我认为损失是不同的。我们首先分别计算每个句子的损失，然后计算平均值，这意味着对于100个token的句子，我们计算100个token的损失平均值，然后除以4并将其添加到总损失中，对于其他3个句子也是如此，我认为以这种方式计算的损失与使用batch_size = 4计算的损失不同。
我误解了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78569958/gradient-accumulation-loss-compute</guid>
      <pubDate>Mon, 03 Jun 2024 11:19:53 GMT</pubDate>
    </item>
    <item>
      <title>用于聊天中问答识别的印地语 NLP 模型</title>
      <link>https://stackoverflow.com/questions/78568815/hindi-nlp-model-for-question-and-answer-identification-in-chats</link>
      <description><![CDATA[我正在开展一个自然语言处理项目，涉及分析印地语聊天数据。具体来说，我需要在聊天记录中识别问题及其对应的答案。
是否有任何预先训练过的印地语 NLP 模型或库可以有效地处理此任务？理想情况下，我正在寻找一种解决方案，可以：
检测句子并将其分类为问题或答案。
识别上下文并将问题与各自的答案配对。
我尝试了各种模型，但没有一个合适。]]></description>
      <guid>https://stackoverflow.com/questions/78568815/hindi-nlp-model-for-question-and-answer-identification-in-chats</guid>
      <pubDate>Mon, 03 Jun 2024 06:56:50 GMT</pubDate>
    </item>
    <item>
      <title>矩阵的线性回归</title>
      <link>https://stackoverflow.com/questions/78568690/linear-reggresion-of-matrix</link>
      <description><![CDATA[令 ( \mathbf{X} \in \mathbb{R}^{n \times d} ) 为回归量矩阵，并令 ( \mathbf{Y} \in \mathbb{R}^n ) 为响应向量。考虑具有参数向量 ( \mathbf{b} \in \mathbb{R}^d ) 的线性回归模型。求最小二乘 (LS) 函数相对于参数向量 ( \mathbf{b} ) 的梯度和 Hessian 矩阵。
线性回归模型：
[
\mathbf{Y} = \mathbf{Xb} + \mathbf{\epsilon}
]
其中 ( \mathbf{Y} \in \mathbb{R}^n ) 是响应向量， ( \mathbf{X} \in \mathbb{R}^{n \times d} ) 是回归量矩阵，( \mathbf{b} \in \mathbb{R}^d ) 是参数向量，( \mathbf{\epsilon} ) 是误差向量。
我被困住了。也许
相对于 \( \mathbf{b}\) 的梯度为： \[\mathbf{b}} J(\mathbf{b}) = \mathbf{b}} \left ( \frac{1} {2}(\mathbf{Y} - \mathbf{Xb})^\top(\mathbf{Y} - \mathbf{Xb})\right) \] 简化 \( J (\mathbf{b}) 的表达式) \): \[ J(\mathbf{b}) = \frac{1}{2} (\mathbf{Y}^\top \mathbf{Y} - 2 \mathbf{Y} ^\top \mathbf{ Xb} + \mathbf{b}^\top \mathbf{X}^\top \mathbf{Xb}) \] 则梯度为： \[ \nabla_{\mathbf{b}} J (\mathbf{b} ) = -\mathbf{X}^\top \mathbf{Y} + \mathbf{X}^\top \mathbf{Xb}\]]]></description>
      <guid>https://stackoverflow.com/questions/78568690/linear-reggresion-of-matrix</guid>
      <pubDate>Mon, 03 Jun 2024 06:25:46 GMT</pubDate>
    </item>
    <item>
      <title>我的训练被随机终止，没有错误日志</title>
      <link>https://stackoverflow.com/questions/78568551/my-training-gets-killed-randomly-without-an-error-log</link>
      <description><![CDATA[我一直在尝试在集群计算机上训练 trackformer 模型。它显示了一条 Killed 消息，并且没有任何日志。

将权重传递到模型中时发生错误
尝试 dmesg 时，我得到了以下输出
[Mon Jun 3 07:24:26 2024] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=task_0,mems_allowed=0-1,oom_memcg=/system.slice/slurmstepd.scope/job_19894856,task_memcg=/system
.slice/slurmstepd.scope/job_19894856/step_batch/user/task_0,task=python,pid=702977,uid=1380211
[2024 年 6 月 3 日星期一 07:24:26] 内存 cgroup 内存不足：已终止进程 702977 (python) total-vm:16068556kB, anon-rss:2144556kB, file-rss:123156kB, shmem-rss:28256kB, UID:1380211 pgtab
les:10612kB oom_score_adj:0

我试图在自定义数据集上训练 trackformer 模型。但训练过程随机停止。]]></description>
      <guid>https://stackoverflow.com/questions/78568551/my-training-gets-killed-randomly-without-an-error-log</guid>
      <pubDate>Mon, 03 Jun 2024 05:38:39 GMT</pubDate>
    </item>
    <item>
      <title>如何在可变形 DETR 中可视化注意力以及基于 DETR 的一系列后续工作？</title>
      <link>https://stackoverflow.com/questions/78568416/how-to-visualize-attentions-in-deformable-detr-and-a-series-of-follow-up-works-i</link>
      <description><![CDATA[许多论文，即使代码是开源的，也没有提供生成这些可视化的具体代码。是否有任何脚本或参考资料可用于指导这些可视化的重现？
具体来说，Conditional-DETR 中的框定位可视化或其他一些作品中的 Cross Attention 可视化。很多时候，即使我能理解作者想要通过这些图传达的意思，但理解用于可视化的实际视觉特征和后处理方法仍然完全难以捉摸。]]></description>
      <guid>https://stackoverflow.com/questions/78568416/how-to-visualize-attentions-in-deformable-detr-and-a-series-of-follow-up-works-i</guid>
      <pubDate>Mon, 03 Jun 2024 04:41:57 GMT</pubDate>
    </item>
    <item>
      <title>在 sagemaker 中部署 llama-3 8B 时出错：标记器不匹配？</title>
      <link>https://stackoverflow.com/questions/78563364/error-deploying-llama-3-8b-in-sagemaker-tokenizer-mismatch</link>
      <description><![CDATA[尝试部署 meta/llama-3-8B-Instruct 时，我在 sagemaker 代码编辑器中收到以下错误：
“您从此检查点加载的 tokenizer 类与调用此函数的类的类型不同。这可能会导致意外的标记化。
您从此检查点加载的 tokenizer 类是“PreTrainedTokenizerFast”。
调用此函数的类是“LlamaTokenizer”。&quot;
我正在使用 Huggingface API 将模型从 HF Hub 直接加载到 sagemaker 代码编辑器，使用 AWS 中的 ml.g5xlarge 实例类型。使用 HF API 不需要或公开 tokenizer。]]></description>
      <guid>https://stackoverflow.com/questions/78563364/error-deploying-llama-3-8b-in-sagemaker-tokenizer-mismatch</guid>
      <pubDate>Sat, 01 Jun 2024 09:35:46 GMT</pubDate>
    </item>
    <item>
      <title>随机森林/决策树输出概率设计：使用正输出叶样本/总输出叶样本</title>
      <link>https://stackoverflow.com/questions/78561885/random-forest-decision-tree-output-probability-design-using-positive-output-l</link>
      <description><![CDATA[我正在使用 python 和 scikitlearn 设计一个二元分类器随机森林模型，我想在其中检索我的测试集是两个标签之一的概率。据我了解，predict_proba(xtest) 将给我以下结果：
投票给分类器的树数/树数

我发现这太不精确了，因为某些树节点可能将我的（非确定性）样本分成相当精确的叶子（100 个 a 类，0 个 b 类）和不精确的叶子（5 个 a 类，3 个 b 类）。我想要一个“概率”的实现，将我的 n 个分类器输出叶子中的样本总数作为主导，将输出叶子中总体选择的分类器的总数作为分子（即使对于选择大多数树没有选择的类的树及其输出叶子也是如此）。
例如（简单）：
2 棵树：
树 1： 
--- 5, 0 类 A（已选择） 
10 
--- 2, 3 类 B（未选择） 

树 2： 
--- 3, 2 类 A（已选择） 
10 
--- 5, 0 类 B（未选择）

predict_proba 结果：
选择类 A 的树数 (2) / 树数 (2) = 1.0

期望结果：
输出叶子中的 A 类样本数 (8) / 输出叶子中的样本总数 (10) = 0.8

有人知道如何做到这一点，或者他们正在使用什么实现？
我有一个想法，就是遍历每棵树，检索它们的概率，然后取平均值。但是，这会给样本较少的输出叶子带来更高的偏差（选举团风格）。
如何直接访问特定样本的决策树输出叶子的样本数量及其类别（或者甚至只是叶子索引，然后从那里开始）？在随机森林的情况下，对它们求和并取平均值？
如果不行，就完全切换平台/库？或者可能只是增加分类器的数量（不是最佳的）？
一些可能有用的文档？：
dtc.tree_.n_node_samples
dtc.tree_[node_index].n_node_samples ?
]]></description>
      <guid>https://stackoverflow.com/questions/78561885/random-forest-decision-tree-output-probability-design-using-positive-output-l</guid>
      <pubDate>Fri, 31 May 2024 19:44:58 GMT</pubDate>
    </item>
    </channel>
</rss>