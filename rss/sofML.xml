<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 28 Jun 2024 15:15:48 GMT</lastBuildDate>
    <item>
      <title>如何应用 mlflow 来处理 scipy 模型？</title>
      <link>https://stackoverflow.com/questions/78682858/how-can-i-apply-mlflow-to-handle-scipy-models</link>
      <description><![CDATA[我已经使用 Python 中的 scipy 实现了 ML 模型。该模型解决了线性回归估计问题，该问题将回归权重限制在给定区间内。
一旦模型校准完毕，我就会存储 scipy.optimize 返回的权重，并像这样使用它们来预测新样本：
import numpy as np
def predict(scipy_model, x_test):
w = scipy_model.x
y_pred = np.sum(w * x_test, axis=1)
return y_pred

我想使用 mlflow 在生产环境中部署此模型。但是，我在文档中没有看到如何将 scipy 与 mlflow 集成。
如果不可能，我可以创建一个具有自定义 train 和 predict 函数的自定义“模型”类并将其与 mlflow 集成吗？]]></description>
      <guid>https://stackoverflow.com/questions/78682858/how-can-i-apply-mlflow-to-handle-scipy-models</guid>
      <pubDate>Fri, 28 Jun 2024 13:49:01 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习确定车辆类别</title>
      <link>https://stackoverflow.com/questions/78682086/vehicle-class-determination-using-machine-learning</link>
      <description><![CDATA[如果我的数据集仅包含以下信息：路段容量、路段行程时间、起点-终点行程时间 (ODTT)、平均行程长度和持续时间以及拥堵程度。
并且没有道路图像。我还能使用 ML 将车辆类型分为重型货车和轻型车辆吗？
我还没有尝试过这种方法]]></description>
      <guid>https://stackoverflow.com/questions/78682086/vehicle-class-determination-using-machine-learning</guid>
      <pubDate>Fri, 28 Jun 2024 11:02:43 GMT</pubDate>
    </item>
    <item>
      <title>如何在使用 Haarcascades 时提高我的结果</title>
      <link>https://stackoverflow.com/questions/78682011/how-to-improve-my-results-while-using-haarcascades</link>
      <description><![CDATA[所以基本上，
来自https://github.com/opencv/opencv/tree/master/data/haarcascades
我一直在使用 smile.xml 文件，但它不能准确地工作。
我想知道我可以做些什么来改进它，以便它可以在现实生活中准确地实现。
另外，我如何为图像添加标签？
我的代码：
import cv2 as cv
import numpy as np
smile_cascade = cv.CascadeClassifier(&#39;haarcascade_smile.xml&#39;)
face_cascade = cv.CascadeClassifier(&#39;haarcascade_frontalface_default.xml&#39;)
cap = cv.VideoCapture(0)
while True:
ret,img = cap.read()
gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)
faces = face_cascade.detectMultiScale(gray,1.3,5)
for (x,y,w,h) in faces:
cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
roi_gray =灰色[y:y+h,x:x+w]
roi_color = img[y:y+h,x:x+w]
smiles = smile_cascade.detectMultiScale(gray, 
scaleFactor=1.3, 
minNeighbors=40, 
minSize=(30, 30),
flags=cv.CASCADE_SCALE_IMAGE)
for (ex,ey,ew,eh) in smiles:
cv.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)
cv.imshow(&#39;img&#39;,img)
k = cv.waitKey(30) &amp; 0xFF
if k ==27:
break
cap.release()
cv.destroyAllWindows()

一直在我的脸上而不是嘴巴上画矩形。]]></description>
      <guid>https://stackoverflow.com/questions/78682011/how-to-improve-my-results-while-using-haarcascades</guid>
      <pubDate>Fri, 28 Jun 2024 10:47:20 GMT</pubDate>
    </item>
    <item>
      <title>MATLAB 中“trainbr”训练函数的 Python 等效项是什么？</title>
      <link>https://stackoverflow.com/questions/78680950/what-is-the-python-equivalent-of-trainbr-training-function-in-matlab</link>
      <description><![CDATA[我正在尝试将以下用 MATLAB 编写的 ANN 模型代码转换为 Python。我只是想知道如何将 trainbr（贝叶斯正则化反向传播）算法转换为 Python。
net=newff(INPTRN,TARTRN,hidden,{&#39;logsig&#39;,&#39;purelin&#39;},&#39;trainbr&#39;);
net.divideFcn=&#39;&#39;;
net.performFcn=&#39;msereg&#39;;
net.trainParam.show=10;
net.trainParam.epochs=50000;
net.trainParam.goal=0.0001;
rand(&#39;state&#39;,0);
net=init(net);
]]></description>
      <guid>https://stackoverflow.com/questions/78680950/what-is-the-python-equivalent-of-trainbr-training-function-in-matlab</guid>
      <pubDate>Fri, 28 Jun 2024 06:34:07 GMT</pubDate>
    </item>
    <item>
      <title>逆问题：使用 LightGBM 模型推荐 X（特征）范围以实现特定的 y（目标）范围</title>
      <link>https://stackoverflow.com/questions/78680915/inverse-problem-using-lightgbm-model-to-recommend-x-feature-ranges-to-achieve</link>
      <description><![CDATA[我正在尝试构建一个 LightGBM 回归模型，其中我有大约 15-20 个输入特征，而我的目标变量在 20-40 的范围内。
我使用了 SHAP 蜂群图来了解每个特征的重要性（由于数据的敏感性，特征名称被隐藏）

现在，用户希望我基于此 LightGBM 模型创建一个优化模型，其中将输入 Y 变量的预期范围，并且模型将返回每个输入变量的理想范围（X，15-20 个特征）以实现相同的目标。问题更像是输入一个 Y 变量，然后返回每个 X 变量的范围。
然而，这里的挑战是，需要 15-20 个特征的组合才能达到 Y 变量的预期范围，因此模型推荐也需要注意这一点。
是否有任何 scikit-learn 库可用于解决这个问题？或者在这种情况下我如何实现解决方案？
我遇到过许多类似的问题，但没有一个是我想要的。]]></description>
      <guid>https://stackoverflow.com/questions/78680915/inverse-problem-using-lightgbm-model-to-recommend-x-feature-ranges-to-achieve</guid>
      <pubDate>Fri, 28 Jun 2024 06:21:25 GMT</pubDate>
    </item>
    <item>
      <title>如何查看 YOLOv6 中的评估指标？</title>
      <link>https://stackoverflow.com/questions/78680846/how-to-see-evaluation-metrics-in-yolov6</link>
      <description><![CDATA[我有以下输出，但无法弄清楚如何评估，因为没有 F1 分数 或 混淆矩阵。
平均召回率 (AR) @[ IoU=0.50:0.95 | area= small |maxDets=100] = -1.000

平均召回率 (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.250

平均召回率 (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410

20/499 0.001595 0.6697 0 1.393: 100%|██████████| 12/12 [00:

21/499 0.001594 0.6417 0 1.353: 100%|██████████| 12/12 [00:

22/499 0.001594 0.6727 0 1.431: 100%|██████████| 12/12 [00:

我训练了 400 个 epoch，这只是输出的一小部分。我也看不到 mAP。
我有这行代码要评估
!python tools/eval.py --data Fabric-Defect-2/data.yaml --weights runs/train/exp/weights/best_ckpt.pt --device 0

有没有办法获得详细的评估指标，例如 F1 分数、混淆矩阵 和 mAP？]]></description>
      <guid>https://stackoverflow.com/questions/78680846/how-to-see-evaluation-metrics-in-yolov6</guid>
      <pubDate>Fri, 28 Jun 2024 05:55:12 GMT</pubDate>
    </item>
    <item>
      <title>期望频率的计算和卡方实现</title>
      <link>https://stackoverflow.com/questions/78680802/calculation-of-expected-frequency-and-chi-square-implementation</link>
      <description><![CDATA[我的数据在一个名为“Data.csv”的 CSV 文件中，具体关注“空气温度”列。我尝试使用两个分布来拟合数据：“gamma”和“对数正态”。现在，我不确定我是否正确计算了预期频率，以及我对卡方检验的实施是否准确。
以下是我面临的具体问题：
预期频率计算：在将“gamma”和“对数正态”分布拟合到我的数据时，我需要澄清计算卡方检验预期频率的正确方法。
卡方检验实施：我不确定我对卡方检验的实施是否正确。我将非常感激有关正确步骤和任何必要更正的指导。
如果可能的话，有人可以提供计算预期频率的正确方法以及针对我的情况的卡方检验的正确实现吗？
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from scipy.stats import gamma, lognorm, kstest, chisquare, chi2_contingency
import warnings

warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning)

df = pd.read_csv(f&#39;Data.csv&#39;)

# 提取数据
temperature = df[&#39;Air_Temperature(AT)&#39;]

def cs(n, y):
return chisquare(n, np.sum(n) / np.sum(y) * y)

def explain_goodness_of_fit(chi2_p, ks_p, alpha=0.05):
if chi2_p &gt; alpha or ks_p &gt; alpha:
return &quot;良好拟合&quot;
else:
return &quot;不合适&quot;

# 计算箱大小
number_of_bins = int(1 + np.log2(len(temperature)))
print(&quot;箱数：&quot;, number_of_bins)

# 拟合伽马分布
gamma_params = gamma.fit(temperature)
print(&quot;伽马分布参数：&quot;, gamma_params)

# 拟合对数正态分布
lognorm_params = lognorm.fit(temperature)
print(&quot;对数正态分布参数：&quot;, lognorm_params)
print(&quot;---------------&quot;)

# 生成拟合的伽马分布
x = np.linspace(min(temperature), max(temperature), 100)
gamma_pdf_fitted = gamma.pdf(x, *gamma_params)

# 生成拟合的对数正态分布
lognorm_pdf_fitted = lognorm.pdf(x, *lognorm_params)

# 使用 KDE 和拟合分布绘制直方图
plt.figure(figsize=(10, 6))
sns.histplot(df, x=&quot;Air_Temperature(AT)&quot;, bins=number_of_bins, kde=True, stat=&#39;density&#39;, label=&#39;Data with KDE&#39;)
plt.plot(x, gamma_pdf_fitted, &#39;r-&#39;, label=&#39;Fitted Gamma Distribution&#39;)
plt.plot(x, lognorm_pdf_fitted, &#39;g-&#39;, label=&#39;Fitted Log-normal Distribution&#39;)
plt.xlabel(&#39;Air Temperature&#39;)
plt.ylabel(&#39;Density&#39;)
plt.title(f&quot;Density Plot&quot;)
plt.legend()
plt.show()

# 卡方检验
observed_freq, bins = np.histogram(temperature, bins=number_of_bins, density=False)

expected_freq_gamma = len(temperature) * gamma.cdf(bins[1:], *gamma_params) - len(temperature) * gamma.cdf(bins[:-1], *gamma_params)

expected_freq_lognorm = len(temperature) * lognorm.cdf(bins[1:], *lognorm_params) - len(temperature) * lognorm.cdf(bins[:-1], *lognorm_params)

result_goodness_of_fit_gamma = cs(observed_freq, expected_freq_gamma)
result_goodness_of_fit_log_normal = cs(observed_freq, expected_freq_lognorm)
print(f&quot;结果优度Gamma 拟合优度结果为 {result_goodness_of_fit_gamma}&quot;)
print(f&quot;对数正态拟合优度结果为 {result_goodness_of_fit_log_normal}&quot;)
print(&quot;---------------&quot;)

# KS-Test
ks_stat_gamma, ks_p_gamma = kstest(temperature, &#39;gamma&#39;, args=gamma_params)
ks_stat_lognorm, ks_p_lognorm = kstest(temperature, &#39;lognorm&#39;, args=lognorm_params)

print(f&quot;Gamma KS 统计量：{ks_stat_gamma}, p 值：{ks_p_gamma}&quot;)
print(f&quot;对数正态 KS 统计量：{ks_stat_lognorm}, p 值： {ks_p_lognorm}&quot;)
print(&quot;------------&quot;)

chi2_stat_gamma, chi2_p_gamma = cs(observed_freq, expected_freq_gamma)
chi2_stat_lognorm, chi2_p_lognorm = cs(observed_freq, expected_freq_lognorm)

gamma_fit = explain_goodness_of_fit(chi2_p_gamma, ks_p_gamma)
lognorm_fit = explain_goodness_of_fit(chi2_p_lognorm, ks_p_lognorm)

print(f&quot;Gamma 分布拟合优度：{gamma_fit}&quot;)
print(f&quot;对数正态分布拟合优度：{lognorm_fit}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78680802/calculation-of-expected-frequency-and-chi-square-implementation</guid>
      <pubDate>Fri, 28 Jun 2024 05:36:04 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft Fabric 数据科学 - 如何使用应用模型向导在 Delta 表中保存概率和预测？</title>
      <link>https://stackoverflow.com/questions/78680642/microsoft-fabric-data-science-how-to-save-probabilities-along-with-predictions</link>
      <description><![CDATA[我对 Fabric 和 ML 还不是很熟悉。我曾为二元分类任务创建了一个逻辑回归模型。该模型在预测方面表现良好，我得到了我想要的结果。但随着业务需求的变化，我也想获得每个类别的概率。我使用 predict_proba 方法来获取 2 个类别的概率。这适用于测试数据。我得到了预测及其概率。我将实验保存为 ML 模型（使用 ML 向导中的应用此模型）并按照以下步骤操作：

选择用于评分的源数据
将数据正确映射到我的 ML 模型的输入
指定我的模型输出的目标
创建一个使用 PREDICT 生成预测结果并将其作为增量表存储到 Lakehouse 的笔记本

但是，我只在我的增量表中获得了预测，而没有概率。有没有办法我也可以获得概率？
PS：我按照此链接上的说明进行操作：链接
我注意到的另一件事是在实验和模型的输出模式上，数据类型是 int 32，这对于预测来说是正确的，但对于概率来说应该是大小为 [-1,2] 的数组。
为了以防万一，我还将链接附加到我的笔记本上：笔记本。

谢谢，]]></description>
      <guid>https://stackoverflow.com/questions/78680642/microsoft-fabric-data-science-how-to-save-probabilities-along-with-predictions</guid>
      <pubDate>Fri, 28 Jun 2024 04:22:59 GMT</pubDate>
    </item>
    <item>
      <title>Oracle 机器学习（OML）df_datetime 给出“未选择任何列”错误</title>
      <link>https://stackoverflow.com/questions/78678402/oracle-machine-learning-oml-df-datetime-gives-no-columns-are-selected-error</link>
      <description><![CDATA[如果有人能帮忙，我遇到了一些编码问题！我试图从 oml.Dataframe df 中获取 Datetime 类型。我试过这个代码：
 df = oml.sync(query=QUERY)
df_datetime = df.select_types(include=[&#39;oml.Datetime&#39;])

但我收到一个错误，提示没有选择任何列。我是否错误地使用了此功能？
我找到了一种解决方法
 df = oml.sync(query=QUERY)
df_datetime = []
for col, dtype in df.dtypes.items():
if dtype.__name__ == &#39;Datetime&#39;:
df_datetime.append(col)

这确实返回了 Datetime 对象，所以我知道它们存在。如果可以的话，我更愿意使用 select_types 方法，如果有人能向我解释我做错了什么。]]></description>
      <guid>https://stackoverflow.com/questions/78678402/oracle-machine-learning-oml-df-datetime-gives-no-columns-are-selected-error</guid>
      <pubDate>Thu, 27 Jun 2024 14:52:10 GMT</pubDate>
    </item>
    <item>
      <title>无法运行 xgboost 导入 XGBRegressor [关闭]</title>
      <link>https://stackoverflow.com/questions/78672076/unable-to-run-xgboost-import-xgbregressor</link>
      <description><![CDATA[当我运行以下代码时，
from xgboost import XGBRegressor

我收到错误：
&gt; --------------------------------------------------------------------------- XGBoostError Traceback (most recent call
&gt; last) Cell In[64], line 19
&gt; 17 from sklearn.metrics import mean_squared_error
&gt; 18 from sklearn import metrics
&gt; ---&gt; 19 from xgboost import XGBRegressor File /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/__init__.py:6
&gt; 1 &quot;&quot;&quot;XGBoost: eXtreme Gradient Boosting library.
&gt; 2 
&gt; 3 贡献者：https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md
&gt; 4 &quot;&quot;&quot;
&gt; ----&gt; 6 来自 . import tracker # noqa
&gt; 7 来自 . import collective, dask
&gt; 8 来自 .core import (
&gt; 9 Booster,

我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78672076/unable-to-run-xgboost-import-xgbregressor</guid>
      <pubDate>Wed, 26 Jun 2024 11:17:02 GMT</pubDate>
    </item>
    <item>
      <title>如何修复 Huggingface 训练器的学习率？</title>
      <link>https://stackoverflow.com/questions/77792137/how-to-fix-the-learning-rate-for-huggingface%c2%b4s-trainer</link>
      <description><![CDATA[我正在使用以下参数训练模型：
Seq2SeqTrainingArguments(
output_dir = &quot;./out&quot;, 
overwrite_output_dir = True,
do_train = True,
do_eval = True,

per_device_train_batch_size = 2, 
gradient_accumulation_steps = 4,
per_device_eval_batch_size = 8, 

learning_rate = 1.25e-5,
warmup_steps = 1,

save_total_limit = 1,

evaluation_strategy = &quot;epoch&quot;,
save_strategy = &quot;epoch&quot;,
logs_strategy = &quot;epoch&quot;, 
num_train_epochs = 5, 

gradient_checkpointing = True,
fp16 = True, 

predict_with_generate = True,
generation_max_length = 225,

report_to = [&quot;tensorboard&quot;],
load_best_model_at_end = True,
metric_for_best_model = &quot;wer&quot;,
greater_is_better = False,
push_to_hub = False,
)

我假设 warmup_steps=1 固定了学习率。
但是，训练结束后，我查看文件 trainer_state.json，发现学习率似乎没有固定。
以下是 learning_rate 和 step 的值：
learning_rate，steps
1.0006 e-05 1033
7.5062 e-06 2066
5.0058 e-06 3099
2.5053 e-06 4132
7.2618 e-09 5165

学习率似乎没有固定在 1.25e-5（步骤 1 之后）。我遗漏了什么？如何修复学习率。]]></description>
      <guid>https://stackoverflow.com/questions/77792137/how-to-fix-the-learning-rate-for-huggingface%c2%b4s-trainer</guid>
      <pubDate>Wed, 10 Jan 2024 09:14:26 GMT</pubDate>
    </item>
    <item>
      <title>计算“torch.tensor”中条目之间的成对距离</title>
      <link>https://stackoverflow.com/questions/75309052/calculating-pairwise-distances-between-entries-in-a-torch-tensor</link>
      <description><![CDATA[我正在尝试实现流形对齐类型的损失，如此处所示。
给定一个表示一批形状为 (L,N) 的嵌入的张量，例如 L=256：
tensor([[ 0.0178, 0.0004, -0.0217, ..., -0.0724, 0.0698, -0.0180],
[ 0.0160, 0.0002, -0.0217, ..., -0.0725, 0.0655, -0.0207],
[ 0.0155, -0.0010, -0.0153, ..., -0.0750, 0.0688, -0.0253],
...,
[ 0.0130, -0.0113, -0.0078, ..., -0.0805, 0.0634, -0.0241],
[ 0.0120, -0.0047, -0.0135, ..., -0.0846, 0.0722, -0.0230],
[ 0.0120, -0.0048, -0.0142, ..., -0.0843, 0.0734, -0.0246]],
grad_fn=&lt;AddmmBackward0&gt;)

我想计算所有成对距离行条目。产生 (L, L) 形状的输出。
我尝试使用 torch.nn.PairwiseDistance，但我不清楚它是否有用，是否符合我的要求。]]></description>
      <guid>https://stackoverflow.com/questions/75309052/calculating-pairwise-distances-between-entries-in-a-torch-tensor</guid>
      <pubDate>Wed, 01 Feb 2023 10:47:38 GMT</pubDate>
    </item>
    <item>
      <title>在 scikit-learn 中使用 GridSearchCV 选择前 k 个模型</title>
      <link>https://stackoverflow.com/questions/47793569/choosing-top-k-models-using-gridsearchcv-in-scikit-learn</link>
      <description><![CDATA[是否有一种简单/预先存在的方法可以在 scikit-learn 中执行网格搜索，然后自动返回前 k 个最佳表现模型或自动平均它们的输出？我打算尝试通过这种方式减少过度拟合。我还没有找到与此相关的任何内容。
编辑：澄清一下，我知道 sklearn 的 GridSearch，我正在寻找一种选项来执行网格搜索，然后返回前 k 个最佳表现模型或对它们进行平均，而不仅仅是返回最佳单个模型。]]></description>
      <guid>https://stackoverflow.com/questions/47793569/choosing-top-k-models-using-gridsearchcv-in-scikit-learn</guid>
      <pubDate>Wed, 13 Dec 2017 12:55:18 GMT</pubDate>
    </item>
    <item>
      <title>SelectKBest 与 GaussianNB 结果不精确/不一致</title>
      <link>https://stackoverflow.com/questions/42193893/selectkbest-with-gaussiannb-not-precise-consistent-results</link>
      <description><![CDATA[我想使用 SelectKBest 选择 前 K 个特征 并运行 GaussianNB：
selection = SelectKBest(mutual_info_classif, k=300)

data_transformed = choice.fit_transform(data, labels)
new_data_transformed = choice.transform(new_data)

classifier = GaussianNB()
classifier.fit(data_transformed, labels)
y_predicted = classifier.predict(new_data)
acc = accuracy_score(new_data_labels, y_predicted)

但是，对于相同的数据，我没有得到一致的准确度结果。
准确度为：
0.61063743402354853
0.60678034916768164 
0.61733658140479086 
0.61652456354039786 
0.64778725131952908 
0.58384084449857898

对于相同的数据。我不进行拆分等。我只使用两组静态的 data 和 new_data。
为什么结果会有所不同？如何确保对相同的数据获得相同的准确度？ ]]></description>
      <guid>https://stackoverflow.com/questions/42193893/selectkbest-with-gaussiannb-not-precise-consistent-results</guid>
      <pubDate>Sun, 12 Feb 2017 22:11:29 GMT</pubDate>
    </item>
    <item>
      <title>初始化权重后，scikit 学习分类器的准确率降低</title>
      <link>https://stackoverflow.com/questions/41804937/decreasing-accuracy-of-scikit-learn-classifier-after-initializing-weight</link>
      <description><![CDATA[我想基于 sklearn 分类器实现 adaboost 分类器，在算法分类器的第一步中我应该将权重初始化为&quot; 1 / # 训练数据&quot;
但这会降低分类器的准确率，我不知道为什么？ （我为所有数据点设置了相同的权重）
我的代码：
svm_weight = SVC()
svm_non_weight = SVC()

w = np.ones(len(target_train))
w.fill(float(1)/float(len(target_train)))
svm_weight.fit(data_train_feature_scaled_pca,
target_train,
sample_weight= w)

svm_non_weight.fit(data_train_feature_scaled_pca,
target_train)

print &quot;score weight : &quot;,svm_weight.score(data_test_feature_scaled_pca,target_train)

print &quot;score non weight : &quot;,svm_non_weight.score(data_test_feature_scaled_pca,target_train)

输出：
得分权重：0.503592561285
得分非权重：0.729289940828

实现的 adaboost：
类 adaboost_classifier：
def __init__(self,train,target,classifier,n_estimator)：
#准备数据集
self.N_classes = np.unique(target)
self.n_estimator = n_estimator
self.N_data = len(train)
self.trained_classifier = [[classifier,float(0),float(0), True ] for i in range(n_estimator)]
indice = []
train = np.array(train)
target = np.array(target)
dataset = np.concatenate((train,target),axis=1)
#连接训练和目标以进行提升

for i in range(len(dataset[0])-1):
indice.append(i)

self.weights = np.zeros([n_estimator,self.N_data])

#初始化权重的 1/n 值
self.weights.fill(1/float(self.N_data))
#进行采样
new_dataset = dataset
self.N_data = len(new_dataset)
#开始训练子分类器
for i in range(n_estimator):
self.loss = np.zeros(self.N_data)
#分离训练和目标数据
new_train = new_dataset[:,indice]
new_target = new_dataset[:,(len(dataset[0])-1)]
#训练分类器：使用数据权重学习 f(X)
self.trained_classifier[i][0].fit(new_train,new_target,sample_weight=self.weights[i])
#计算加权误差，存储在 trained_classifier[i][1] 中
for point in range(self.N_data) :
if(self.trained_classifier[i][0].predict([new_train[point]]) != new_target[point]):
self.loss[point] = 1
self.trained_classifier[i][1] += self.weights[i][point]

#计算分类器 i 的系数，存储在 trained_classifier[i][2] 中
self.trained_classifier[i][2] = 0.5 * np.log((1-self.trained_classifier[i][1])/self.trained_classifier[i][1])
#重新计算权重
for j in range(self.N_data):
if(self.loss[j] == 1):
self.weights[i][j] *= np.exp(self.trained_classifier[i][2])
else:
self.weights[i][j] *= np.exp(-self.trained_classifier[i][2])

#规范化权重
self.trained_classifier[i][1] = self.trained_classifier[i][1] / self.weights[i].sum()
]]></description>
      <guid>https://stackoverflow.com/questions/41804937/decreasing-accuracy-of-scikit-learn-classifier-after-initializing-weight</guid>
      <pubDate>Mon, 23 Jan 2017 11:09:21 GMT</pubDate>
    </item>
    </channel>
</rss>