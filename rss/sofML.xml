<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 06 Aug 2024 06:23:25 GMT</lastBuildDate>
    <item>
      <title>是否有可以执行 DLSS/FSR 功能的 CPU 超级采样库？[关闭]</title>
      <link>https://stackoverflow.com/questions/78837143/is-there-cpu-super-sampling-library-that-does-what-dlss-fsr-do</link>
      <description><![CDATA[我想将基于机器学习的升级器添加到我的 CPU 光线追踪器中。是否有可以这样做的 CPU 框架？
谢谢 :)]]></description>
      <guid>https://stackoverflow.com/questions/78837143/is-there-cpu-super-sampling-library-that-does-what-dlss-fsr-do</guid>
      <pubDate>Tue, 06 Aug 2024 03:39:35 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：具有多个输出的模型，当以列表形式提供“metrics”参数时，它应该具有与模型输出一样多的条目</title>
      <link>https://stackoverflow.com/questions/78837133/valueerror-a-model-with-multiple-outputs-when-providing-the-metrics-argument</link>
      <description><![CDATA[当我的朋友使用卷积神经网络开发年龄和性别检测模型时，他遇到了这个 ValueError。他在定义模型时将“metrics”参数作为列表给出，但它表示它应该具有与模型一样多的条目。我的模型有两个输出“年龄”和“性别”。
他正在定义他的 CNN 模型，当他尝试将模型拟合到训练数据上时，它显示错误提示

ValueError：对于具有多个输出的模型，当以列表形式提供 metrics 参数时，它应该具有与模型输出一样多的条目。收到：
metrics=[&#39;accuracy&#39;]
长度为 1，而模型有 2 个输出。

这是我的完整错误：
ValueError Traceback（最近一次调用最后一次）
Cell In[47]，第 1 行
----&gt; 1 History=Model.fit(X_train,Y_train_2,batch_size=64,validation_data=(X_test,Y_test_2),epochs=10,callbacks=callback_list)

文件 c:\Users\mebub_9a7jdi8\Desktop\Age_Gender_Detection_Model\.env\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 中引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 c:\Users\mebub_9a7jdi8\Desktop\Age_Gender_Detection_Model\.env\Lib\site-packages\keras\src\trainers\compile_utils.py:250，位于 CompileMetrics._build_metrics_set(self, metrics, num_outputs, output_names, y_true, y_pred,argument_name)
248 if isinstance(metrics, (list, tuple)):
249 if len(metrics) != len(y_pred):
--&gt; 250 raise ValueError(
251 &quot;对于具有多个输出的模型，&quot;
252 f&quot;当以 &quot;
253 &quot;列表形式提供 `{argument_name}` 参数时，它应该具有与模型具有的 &quot;
254 f&quot;输出一样多的条目。收到:\n{argument_name}={metrics}\nof &quot;
255 f&quot;长度 {len(metrics)}，而模型具有 &quot;
256 f&quot;{len(y_pred)} 个输出。&quot;
257 )
258 for idx, (mls, yt, yp) in enumerate(
...
261 if not isinstance(mls, list):

ValueError: 对于具有多个输出的模型，当以列表形式提供 `metrics` 参数时，它应该具有与模型具有的一样多的条目输出。收到：
metrics=[&#39;accuracy&#39;]
长度为 1，而模型有 2 个输出。

这是我定义模型的代码：
def model(input_shape):
inputs=Input((input_shape))
conv_1=Convolution(inputs,32)
maxp_1=MaxPooling2D(pool_size=(2,2))(conv_1)
conv_2=Convolution(maxp_1,64)
maxp_2=MaxPooling2D(pool_size=(2,2))(conv_2)
conv_3=Convolution(maxp_2,128)
maxp_3=MaxPooling2D(pool_size=(2,2))(conv_3)
conv_4=Convolution(maxp_3,256)
maxp_4=MaxPooling2D(pool_size=(2,2))(conv_4)
flatten= Flatten()(maxp_4)
density_1=Dense(64,activation=&#39;relu&#39;)(flatten)
density_2=Dense(64,activation=&#39;relu&#39;)(flatten)
drop_1=Dropout(0.2)(dense_1)
drop_2=Dropout(0.2)(dense_2)
output_1=Dense(1,activation=&#39;sigmoid&#39;,name=&#39;sex_out&#39;)(drop_1)
output_2=Dense(1,activation=&#39;relu&#39;,name=&#39;age_out&#39;)(drop_2)
model=Model(inputs=[inputs],outputs=[output_1,output_2])
model.compile(loss=[&quot;binary_crossentropy&quot;,&quot;mae&quot;],optimizer=&quot;Adam&quot;,metrics=[&quot;accuracy&quot;])
返回模型
]]></description>
      <guid>https://stackoverflow.com/questions/78837133/valueerror-a-model-with-multiple-outputs-when-providing-the-metrics-argument</guid>
      <pubDate>Tue, 06 Aug 2024 03:33:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML 算法的神经网络将产品列表映射到类别</title>
      <link>https://stackoverflow.com/questions/78837098/mapping-product-listings-to-categories-using-neural-networks-of-ml-algorithm</link>
      <description><![CDATA[我正在尝试实现 NN ML 算法来将产品映射到各自的类别，但该算法没有给出一致的结果。我使用了 keras、tensorflow 的顺序模型。请提出是否有更好的方法来解决此问题。
代码片段：
 # 分成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
print(y_train)
# 定义模型
model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_len))
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(units=128,activation=&#39;relu&#39;))
model.add(Dropout(0.5))
model.add(Dense(units=y_train.shape[1],activation=&#39;softmax&#39;))

# 编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;]

)
]]></description>
      <guid>https://stackoverflow.com/questions/78837098/mapping-product-listings-to-categories-using-neural-networks-of-ml-algorithm</guid>
      <pubDate>Tue, 06 Aug 2024 03:13:17 GMT</pubDate>
    </item>
    <item>
      <title>何时使用复合损失深度学习（图像分割）？</title>
      <link>https://stackoverflow.com/questions/78837046/when-to-use-composite-losses-deep-learning-image-segmentation</link>
      <description><![CDATA[我想知道在训练图像分割算法时，复合损失（例如 Dice + Focal Loss、Dice + Cross-Entropy Loss 或 Generalized Dice + Focal Loss）何时优于使用常规 Dice/CE Loss。
在什么情况下它们可以帮助算法更好地收敛？这是一个比较普遍的问题，但是否存在一个粗略的策略来确定某些算法的良好复合损失？]]></description>
      <guid>https://stackoverflow.com/questions/78837046/when-to-use-composite-losses-deep-learning-image-segmentation</guid>
      <pubDate>Tue, 06 Aug 2024 02:43:38 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow Privacy 中的差分隐私错误</title>
      <link>https://stackoverflow.com/questions/78836989/error-in-tensorflow-privacy-for-differential-privacy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78836989/error-in-tensorflow-privacy-for-differential-privacy</guid>
      <pubDate>Tue, 06 Aug 2024 02:18:31 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：在层“conv2d_7”上调用“set_weights(weights)”，权重列表长度为 2，但该层需要 1 个权重</title>
      <link>https://stackoverflow.com/questions/78836514/valueerror-called-set-weightsweights-on-layer-conv2d-7-with-a-weight-list</link>
      <description><![CDATA[我尝试在模型中的某一层上设置权重，但无济于事。
我在网上查找了类似问题的解决方案，但似乎都不起作用。变量“w”（如下面代码所示）的结构为 [numpy array, numpy array]。第一个的大小为 (3, 3, 3, 64)，第二个的形状为 (64,)。我想实现与 tf 2.X 中的“weights”kwarg 类似的功能，但似乎无法让它工作。这是我的代码：
encoder = Sequential()
encoder.add(layers.Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;,use_bias=False,input_shape=(SIZE,SIZE,3)))
w = model.layers[0].get_weights()
encoder.layers[0].set_weights([w])
encoder.add(layers.MaxPooling2D((2, 2),padding=&#39;same&#39;))
encoder.add(layers.Conv2D(32, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;,weights=model.layers[2].get_weights()))
encoder.add(layers.MaxPooling2D((2, 2), padding=&#39;same&#39;))
encoder.add(layers.Conv2D(16, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;,weights=model.layers[4].get_weights()))
encoder.add(layers.MaxPooling2D((2, 2), padding=&#39;same&#39;))
encoder.summary()

错误：
错误：ValueError：您在层“conv2d_7”上调用了`set_weights(weights)`，权重列表长度为 2，但该层需要 1 个权重。
]]></description>
      <guid>https://stackoverflow.com/questions/78836514/valueerror-called-set-weightsweights-on-layer-conv2d-7-with-a-weight-list</guid>
      <pubDate>Mon, 05 Aug 2024 21:33:29 GMT</pubDate>
    </item>
    <item>
      <title>如何将 adaboost 与 xgboost 结合起来</title>
      <link>https://stackoverflow.com/questions/78835596/how-to-combine-adaboost-with-xgboost</link>
      <description><![CDATA[这是一个基于 5 个特征的 3 类分类问题。y 的 3 个类别（标签）分别为 0、1 和 2。我尝试使用 xgboost 模型作为基础估计器并将其放入 adabboost 中。在训练集上拟合模型后，我使用这个拟合模型来预测测试集。但是测试集上的预测值全都是0。
如何解决这个问题？
我的代码在这里
x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.3, random_state=10)
ss_x = StandardScaler()
x_train_transformed = ss_x.fit_transform(x_train)
x_test_transformed = ss_x.transform(x_test)
le_y = LabelEncoder()
y_train_transformed = le_y.fit_transform(y_train)
y_test_transformed = le_y.transform(y_test)

base_model = xgb.XGBClassifier()
model = AdaBoostClassifier(estimator=base_model)
model.fit(x_train_transformed, y_train_transformed)
model_test_sc = accuracy_score(y_test_transformed, model.predict(x_test_transformed))
conf_matrix = confused_matrix(y_test_transformed, model.predict(x_test_transformed),labels=[0,1,2])

我不确定这样将两个助推器结合起来是否合理。]]></description>
      <guid>https://stackoverflow.com/questions/78835596/how-to-combine-adaboost-with-xgboost</guid>
      <pubDate>Mon, 05 Aug 2024 17:00:45 GMT</pubDate>
    </item>
    <item>
      <title>一维变分自动编码器重构不佳</title>
      <link>https://stackoverflow.com/questions/78834575/bad-reconstruction-of-1-d-variational-autoencoder</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78834575/bad-reconstruction-of-1-d-variational-autoencoder</guid>
      <pubDate>Mon, 05 Aug 2024 13:03:53 GMT</pubDate>
    </item>
    <item>
      <title>yolov9 在自定义数据上进行训练</title>
      <link>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</link>
      <description><![CDATA[我正尝试在 PyCharm 而不是 google colab 上用一些自定义数据训练 yolov9。我该怎么做？
将存储库克隆到我的计算机后，我在虚拟环境中安装了所有要求。然后我创建了训练脚本，但我觉得有些短。
这是我的训练脚本：
import os
import subprocess

dataset_path = &#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject&#39;

def train_yolov5(train_images_path, val_images_path, yaml_file_path, weights_path=&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main/yolov9-c.pt&#39;, epochs=50):

# 获取 yolov5 目录的绝对路径
yolov9_dir = os.path.abspath(&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main&#39;)

# 将当前工作目录更改为 yolov9 目录
os.chdir(yolov9_dir)
# 训练 yolov9 模型
command = f&#39;python train.py --workers 8 --device cpu --batch 16 --data {dataset_path}/sfdV2_musa.yaml --img 640 --cfg models/detect/yolov9-c.yaml --weights yolov9-c --hyp hyp.scratch-high.yaml --min-items 0 --epochs 5 --close-mosaic 15&#39;

# 执行命令
process = subprocess.Popen(command, shell=True)
process.wait()

if __name__ == &quot;__main__&quot;:
TRAIN_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/train&#39;)
VAL_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/val&#39;)
YAML_FILE_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/sfdV2_musa.yaml&#39;)

# 训练 YOLOv9 模型
train_yolov5(TRAIN_IMAGES_PATH, VAL_IMAGES_PATH、YAML_FILE_PATH）`
]]></description>
      <guid>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</guid>
      <pubDate>Mon, 05 Aug 2024 12:28:07 GMT</pubDate>
    </item>
    <item>
      <title>深度学习训练策略：避免对单个训练图像进行打乱，而是对批次进行打乱？[关闭]</title>
      <link>https://stackoverflow.com/questions/78834079/deep-learning-training-strategy-avoid-shuffling-individual-training-images-ins</link>
      <description><![CDATA[我正在为工业环境中的应用训练 YOLO（你只看一次）物体检测器。由于使用固定的相机设置，图像的背景与相机有关，但不会随时间而变化（除了测量部件的位置不准确性）。
从数据科学的角度来看，这意味着：我的训练数据可以逻辑地分组为 N 个相机视图，每个视图包含一个特定的背景。
我的想法如下：
我不会在一个大数据集内混洗所有图像，而是只混洗批次，这样相机视图就不会在单个训练批次中混淆。使用这种方法，我希望能够更具体地学习背景视图的特殊属性。
权衡将是“牺牲”神经网络的一些泛化能力（在这个特殊应用中不是必需的）以便在这个专门的设置中获得更好的性能。
使用这种方法更新权重会有什么不同？一些文献也很酷，但我在研究期间没有发现类似的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78834079/deep-learning-training-strategy-avoid-shuffling-individual-training-images-ins</guid>
      <pubDate>Mon, 05 Aug 2024 10:53:19 GMT</pubDate>
    </item>
    <item>
      <title>用于聚类的机器学习模型（与 K-means 类似，但功能不同）[关闭]</title>
      <link>https://stackoverflow.com/questions/78833002/machine-learning-model-for-clustering-similar-with-k-means-but-different-funct</link>
      <description><![CDATA[当我研究几个机器学习模型时，我看到了几个聚类算法，包括K-Means。
据我所知K-Means使用欧几里德距离作为他们自己的计算方法，
我想要的不是使用欧几里德距离，而是数据的值。
例如，样本分布很广（如坐标），坐标有自己的值。
我想找到平均值较高的N个簇。
有没有其他适合这个图的算法，或者我是否只处理K-Means算法中的几个参数来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78833002/machine-learning-model-for-clustering-similar-with-k-means-but-different-funct</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>createDataPartition 给出异常不均匀的测试和训练集</title>
      <link>https://stackoverflow.com/questions/78832537/createdatapartition-gives-abnormally-uneven-test-and-train-sets</link>
      <description><![CDATA[我正在尝试使用 caret 包将我的数据拆分为测试集和训练集。我有 77 行，每列都有完整的数据。函数“createDataPartition”导致训练数据为 4 行，测试数据为 73 行，这似乎不对。任何帮助都将不胜感激。这是我的代码：
&gt; library(caret)
&gt; # 将数据拆分为训练和测试
&gt; set.seed(123)
&gt; data.full &lt;- data.full %&gt;% select(fasting_status, a1c, glu, uc_ratio)
&gt; training.samples &lt;- data.full %&gt;% 
+ createDataPartition(p = 0.8, list = FALSE)
警告消息：
1：在 createDataPartition(., p = 0.8, list = FALSE) 中：
某些类没有记录 ( )，这些将被忽略
2：在 createDataPartition(., p = 0.8, list = FALSE) 中：
某些类只有一条记录 ( )，这些将被选为样本
&gt; train.data &lt;- data.full[training.samples, ]
&gt; test.data &lt;- data.full[-training.samples, ] 

以下是我的可重现数据：
&gt; dput(data.full) 结构(列表(fasting_status = 结构(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、1L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、2L、 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L), 级别 = c(&quot;1&quot;, &quot;2&quot;), 类别 = &quot;因素&quot;), 
a1c = c(4.3, 4.5, 4.4, 2.9, 4.3, 4.4, 4.2, 4.5, 4.2, 4.2, 
4.5, 4.5, 4.8, 4.5, 5.2, 4.9, 4.6, 4.2, 4.4, 4.9, 4.6, 4.5, 
    4.4、4.8、4.5、4.1、3.8、3.1、4.3、4.6、4.7、4.9、4.6、4.4、3.1、4.6、4.4、4.2、4.4、5.2、4.4、5.1、4.6、4.7、5.2、4.7、4。 7、4.6、4.4、4.4、4.2、4.5、4.6、4.4、3.2、4.8、5.2、5.2、4.6、4.9、5.6、4.6、4.9、4.5、5.1、4.6、4.9、4.6、4.3、4.6、
4.6, 4.3, 4.6, 4.3, 4.6, 6.5, 4.8), glu = c(88.5, 98, 117.5, 
53, 108.5, 106, 105, 101, 91, 99.5, 128.5, 113, 114, 121.5, 
121, 131.5, 160.5, 96, 110, 140, 119.5, 115.3, 112, 143.5, 
116.5, 116.5, 111, 139.5, 123.5, 131, 113, 137, 114, 98.5, 
    124.5、123.5、111.5、111、127、123、137.5、119、107、130.5、142.5、115、133.5、119、148.3、125.5、138.5、106.5、153.5、 .5、179、145、143、124.5、134、146.5、127.5、124.5、123、129、145.3、125.5、146.5、153.5、115.5、128、110.5、131、 
139.5, 124, 154, 94, 76.3), uc_ratio = c(30.65603924, 15.32801962, 
60.59075991, 7.39973361, 57.84661317, 27.46781116, 16.0944206, 
6.131207848, 94.61568474, 19.50838861, 7.803355443, 19.41549152, 
7.464079119, 19.67095851, 29.50643777, 62.94706724, 80.472103、25.75107296、73.57449418、39.01677721、41.13018598、10.62933697、7.803355443、30.04291845、32.75355771、 9416、5.969860273、22.72153497、7.153075823、75.61823012、23.50296342、53.64806867、11.19611891、38.25340549、 88.36152487、51.50214592、9.196811772、41.98544505、6.35828962、9.196811772、94.87237407、12.87553648、6.035407725、7.3997 3361、10.72961373、11.70503316、9.035464197、16.34988759、11.68917269、35.11509949、61.85306741、11.36076748、 
    12.2624157、7.153075823、14.30615165、10.40447392、3.901677721、52.11526671、21.45922747、30.49469166、81.06819266、 38861、34.33476395、8.0472103、24.94635193、9.754194304、64.3776824、9.196811772、11.92179304、34.87124464、 74.39198856, 124.4635193, 
13.79521766, 5.722460658, 66.76204101, 69.9757432, 19.50838861
)), row.names = c(NA, -77L), class = &quot;data.frame&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78832537/createdatapartition-gives-abnormally-uneven-test-and-train-sets</guid>
      <pubDate>Mon, 05 Aug 2024 02:29:00 GMT</pubDate>
    </item>
    <item>
      <title>DSPy 无法检索 ChromaDB 中带有文本嵌入的段落</title>
      <link>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</link>
      <description><![CDATA[我正在使用 DSPy 和 ChromaDB 为 pdf 文件开发 RAG 应用程序。
首先，我从 pdf 中获取文本并将其作为块添加到 Chromadb。还添加了块的嵌入。并尝试使用 DSPy 检索与查询相关的块。但是它出现了错误
存储数据和嵌入
def store_document_in_chromadb(text):
chunks = chunk_document(text)
ids = [f&#39;chunk_{i}&#39; for i in range(len(chunks))]
embeddings = [get_embedding(chunk).tolist() for chunk in chunks]

collection.add(ids=ids, documents=chunks, embeddings=embeddings)

我尝试像这样检索相关块，
retriever_model = ChromadbRM(&quot;contracts_collection&quot;, &#39;db/&#39;, k=2)
dspy.settings.configure(lm=llama2_model, rm=retriever_model)

class GenerateAnswer(dspy.Signature): 
“”“”根据给出的上下文回答问题。“”“”
context = dspy.InputField(desc=&quot;可能包含相关上下文&quot;)
question = dspy.InputField()
answer = dspy.OutputField(desc=&quot;通常为 5 到 10 个单词&quot;)

class RAG(dspy.Module): 
def __init__(self, num_passages=2):
super().__init__()
self.retrieve = dspy.Retrieve(k=num_passages)
self.generate_answer = dspy.ChainOfThought(GenerateAnswer)

def forward(self, question):
context = self.retrieve(question).passages
prediction = self.generate_answer(context=context, question=question)
return dspy.Prediction(context=context, answer=prediction.answer)

with dspy.context(lm=llama2_model, rm=retriever_model):
module = RAG()
response = module(&quot;总支出是多少&quot;)
print(response)

当我运行此程序时，出现此错误
InvalidDimensionException：嵌入维度 384 与集合维度 768 不匹配
但是当我从 ChromaDB 中删除嵌入时，它会正确检索相关块。
为什么使用嵌入时没有出现此错误？]]></description>
      <guid>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</guid>
      <pubDate>Wed, 17 Jul 2024 08:03:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在微调过程中正确设置 pad token（而不是 eos）以避免模型无法预测 EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么 xgboost.QuantileDMatrix 使用自定义数据迭代器对数据进行四次传递？</title>
      <link>https://stackoverflow.com/questions/76569411/why-does-xgboost-quantiledmatrix-do-four-passes-of-the-data-with-custom-data-ite</link>
      <description><![CDATA[由于我的数据集太大，我尝试使用此处所示的自定义数据迭代器。为了测试其工作原理，我使用示例的子集并运行以下代码。 X 是我的数据的 numpy 数组。
我的迭代器如下所示
class IterForQDMatrix(xgb.core.DataIter):
def __init__(self, df, batch_size):
self.df = df
self.batch_size = batch_size
self.batches = np.ceil(len(df) // self.batch_size)
self.it = 0
super().__init__()

def reset(self):
self.it = 0

def next(self, input_data):
if self.it == self.batches:
print(&quot;done&quot;)
return 0
a = self.it * self.batch_size
b = min((self.it + 1) * self.batch_size, len(self.df))
input_data(data=self.df[a:b, : -1], label=self.df[a:b, -1])
self.it += 1
return 1

iterator = IterForQDMatrix(X, 30)
xgb_data = xgb.QuantileDMatrix(iterator)

当我运行上述代码时，我注意到 &quot;done&quot; 被打印了四次，这意味着当我将迭代器传递给 xgb.QuantileDMatrix 时，它会传递整个数据集四次。我试图理解它为什么要传递数据四次。有没有办法只传递一次数据就能实现它所做的事情？]]></description>
      <guid>https://stackoverflow.com/questions/76569411/why-does-xgboost-quantiledmatrix-do-four-passes-of-the-data-with-custom-data-ite</guid>
      <pubDate>Wed, 28 Jun 2023 00:55:19 GMT</pubDate>
    </item>
    </channel>
</rss>