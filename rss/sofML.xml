<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 05 Aug 2024 18:21:09 GMT</lastBuildDate>
    <item>
      <title>如何通过从 aws s3 bucket 访问视频来获取视频的时间戳，而无需使用 ai 下载视频</title>
      <link>https://stackoverflow.com/questions/78835658/how-to-get-timestamps-for-a-video-by-acessing-it-from-aws-s3-bucket-without-down</link>
      <description><![CDATA[这里我使用了 client.invoke_model、boto3.client、&#39;bedrock-runtime&#39; 等，但这里我获得的时间戳超过了视频时长，比如我的视频时长是 5 分钟，但我获得的时间戳长达 9 分钟。这里我不想下载视频，我只想生成时间戳
我尝试使用 openai 密钥，但我没有获得正确的视频时间戳，我该怎么办，实际上我的视频时长是 9 分 40 秒，但这里我得到的是“00:00 Python 会话简介 \n00:15 今天会话的议程 \n00:30 什么是 Python？ \n01:15 为什么 Python 如此受欢迎？ \n02:30 Python 的特性 \n03:45 Python 的简单性和开源性 \n04:15 Python 的可移植性和嵌入性 \n05:00 Python 的解释性和庞大的库支持 \n05:45 Python 中的面向对象概念 \n06:15 使用 Python 的公司 \n07:30 开始学习 Python \n09:00 Python 基础知识：变量、数据类型和运算符 \n10:15 了解 Python 中的列表、流控制和函数 \n11:30 Python 中的文件处理和面向对象编程 \n12:45 掌握 Python 和职业机会 \n14:15 Web 应用开发、游戏开发和大数据分析 \n15:30 数据科学、机器学习和人工智能 \n16:45 智能物联网设备和其他职业机会\n17:30 总结与告别“喜欢这个]]></description>
      <guid>https://stackoverflow.com/questions/78835658/how-to-get-timestamps-for-a-video-by-acessing-it-from-aws-s3-bucket-without-down</guid>
      <pubDate>Mon, 05 Aug 2024 17:17:55 GMT</pubDate>
    </item>
    <item>
      <title>如何让孤立森林在差异的峰值处检测异常，而不是在看到的第一个值处检测异常</title>
      <link>https://stackoverflow.com/questions/78835539/how-to-make-isolation-forest-detect-anomaly-at-the-peak-of-the-difference-inste</link>
      <description><![CDATA[我正在使用孤立森林来识别非常大的数据框中的异常。数据很嘈杂，因此我进行了许多过滤操作来消除噪音，以便数据中存在的真实异常脱颖而出。然后，我在此数据集上使用 .diff() 创建一条直线，当异常发生时，该直线会激增。然后使用孤立森林来识别这些异常。
我的问题是，孤立森林在它可以检测到异常发生的最早时间点识别异常，但我需要它在峰值差异处检测到它。
df[&quot;Ref Wt. Denoised&quot;] = denoise(df[&quot;Ref Wt.&quot;].values, level=2)
df[&quot;Ref Wt. Savgol&quot;] = apply_savgol_filter(df[&quot;Ref Wt. Denoised&quot;], window_length=101, polyorder=3)
df[&quot;Ref Wt. Smoothed&quot;] = df[&quot;Ref Wt. Savgol&quot;].rolling(window=indexer).mean()
df[&quot;Ref Wt. Diff&quot;] = df[&quot;Ref Wt. Smoothed&quot;].diff(periods=300).fillna(0)

df[&quot;WOB Anomaly&quot;] = detect_wob.predict(df[&quot;Ref Wt. Diff&quot;].values.reshape(-1, 1))

df[&quot;WOB Zero Event&quot;] = df[&quot;WOB Anomaly&quot;] == -1

我尝试使用 .shift() 来修复它，但这种手动更改对某些值有效，但不是全部。我真的想避免更改用于平滑数据的窗口大小，因为这会严重影响准确性。
我正在寻找的问题和修复的图片]]></description>
      <guid>https://stackoverflow.com/questions/78835539/how-to-make-isolation-forest-detect-anomaly-at-the-peak-of-the-difference-inste</guid>
      <pubDate>Mon, 05 Aug 2024 16:43:41 GMT</pubDate>
    </item>
    <item>
      <title>深度学习中使用拓扑数据分析的示例</title>
      <link>https://stackoverflow.com/questions/78834875/example-of-using-topological-data-analysis-in-deep-learning</link>
      <description><![CDATA[以下是论文：Daniel Leykam 和 Dimitris G. Angelakis，拓扑数据分析和机器学习。
我想要一些在深度学习模型中使用和实现拓扑数据分析的示例、教程和源代码。
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78834875/example-of-using-topological-data-analysis-in-deep-learning</guid>
      <pubDate>Mon, 05 Aug 2024 14:13:42 GMT</pubDate>
    </item>
    <item>
      <title>Deepface - 仅显示置信度超过阈值的面部表情</title>
      <link>https://stackoverflow.com/questions/78834690/deepface-only-display-the-emotion-for-faces-with-confidence-over-threshold</link>
      <description><![CDATA[所以我有一段代码可以检测面部表情，但它会在没有表情的地方找到表情。所以我想知道是否可以自信地做到这一点。
我尝试在谷歌上搜索如何做到这一点，但没有找到任何有用的东西。希望有人能帮忙。
import cv2
from deepface import DeepFace

# 加载人脸级联分类器
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + &#39;haarcascade_frontalface_default.xml&#39;)

# 开始捕捉视频
cap = cv2.VideoCapture(&#39;ap.mp4&#39;)

while True:
# 逐帧捕捉
ret, frame = cap.read()

# 将帧转换为灰度
gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

# 将灰度帧转换为 RGB 格式
rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)

# 检测帧中的人脸
faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
for (x, y, w, h) in faces:
# 提取面部 ROI（感兴趣区域）
face_roi = rgb_frame[y:y + h, x:x + w]

# 对面部 ROI 进行情绪分析
result = DeepFace.analyze(face_roi, action=[&#39;emotion&#39;], force_detection=False)

# 确定主要情绪
print(result[0][&#39;emotion&#39;])
emotion = result[0][&#39;dominant_emotion&#39;]

# 在面部周围绘制矩形并标注预测情绪
cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)
cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)

# 显示结果帧
cv2.imshow(&#39;Real-time Emotion Detection&#39;, frame)

# 按“q”退出
if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
break

# 释放捕获并关闭所有窗口
cap.release()
cv2.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/78834690/deepface-only-display-the-emotion-for-faces-with-confidence-over-threshold</guid>
      <pubDate>Mon, 05 Aug 2024 13:30:34 GMT</pubDate>
    </item>
    <item>
      <title>一维变分自动编码器重构不佳</title>
      <link>https://stackoverflow.com/questions/78834575/bad-reconstruction-of-1-d-variational-autoencoder</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78834575/bad-reconstruction-of-1-d-variational-autoencoder</guid>
      <pubDate>Mon, 05 Aug 2024 13:03:53 GMT</pubDate>
    </item>
    <item>
      <title>yolov9 在自定义数据上进行训练</title>
      <link>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</link>
      <description><![CDATA[各位。我正在尝试在 PyCharm 上用一些自定义数据训练 yolov9，而不是像我看过的许多教程所建议的那样使用 google colab。我该怎么做？
将存储库克隆到我的计算机后，我在虚拟环境中安装了所有必需的软件。然后我创建了训练脚本，但我觉得有些短。
这是我的训练脚本：
import os
import subprocess

dataset_path = &#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject&#39;

def train_yolov5(train_images_path, val_images_path, yaml_file_path, weights_path=&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main/yolov9-c.pt&#39;, epochs=50):

# 获取 yolov5 目录的绝对路径
yolov9_dir = os.path.abspath(&#39;C:/Users/rsingh/Desktop/Musa_PDC/yolov9-main&#39;)

# 更改当前工作目录到 yolov9 目录
os.chdir(yolov9_dir)
# 训练 yolov9 模型
command = f&#39;python train.py --workers 8 --device cpu --batch 16 --data {dataset_path}/sfdV2_musa.yaml --img 640 --cfg models/detect/yolov9-c.yaml --weights yolov9-c --hyp hyp.scratch-high.yaml --min-items 0 --epochs 5 --close-mosaic 15&#39;

# 执行命令
process = subprocess.Popen(command, shell=True)
process.wait()

if __name__ == &quot;__main__&quot;:
TRAIN_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/train&#39;)
VAL_IMAGES_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/captured_images/images/val&#39;)
YAML_FILE_PATH = (
&#39;C:/Users/rsingh/Desktop/Rahul_PDC/Repositories/Smart_Factory/YoloV5_Training/sfd_colorobject/sfdV2_musa.yaml&#39;)

# 训练 YOLOv9 模型
train_yolov5(TRAIN_IMAGES_PATH, VAL_IMAGES_PATH、YAML_FILE_PATH）`
]]></description>
      <guid>https://stackoverflow.com/questions/78834445/yolov9-training-on-custom-data</guid>
      <pubDate>Mon, 05 Aug 2024 12:28:07 GMT</pubDate>
    </item>
    <item>
      <title>使用 GAN 生成医学图像</title>
      <link>https://stackoverflow.com/questions/78834126/medical-image-generation-using-gans</link>
      <description><![CDATA[我正在使用 gans 生成合成 ct 扫描图像以用于我的分割任务。我有一个包含 75 张 PNG 图像的数据集，我想生成合成图像。
这是我使用的代码，但我没有得到正确的输出。[在此处输入图像描述](https://i.sstatic.net/82rfdq8T.png)
是否有可用的在线代码或资源可以做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/78834126/medical-image-generation-using-gans</guid>
      <pubDate>Mon, 05 Aug 2024 11:07:41 GMT</pubDate>
    </item>
    <item>
      <title>深度学习训练策略：避免打乱单个训练图像，而是打乱批次？</title>
      <link>https://stackoverflow.com/questions/78834079/deep-learning-training-strategy-avoid-shuffling-individual-training-images-ins</link>
      <description><![CDATA[我目前正在为工业环境中的应用训练 YOLO（你只看一次）物体检测器。由于使用固定的相机设置，图像的背景与相机有关，但不会随时间而变化（除了测量部件的位置不准确性）。
从数据科学的角度来看，这意味着：我的训练数据可以逻辑地分组为 N 个相机视图，每个视图包含一个特定的背景。
我的想法如下：
我不会在一个大数据集内混洗所有图像，而是只混洗批次，这样相机视图就不会在单个训练批次中混淆。使用这种方法，我希望能够更具体地学习背景视图的特殊属性。
权衡将是“牺牲”神经网络的一些泛化能力（在这个特殊应用中不是必需的）以便在这个专门的设置中获得更好的性能。
这里有人对使用这种方法更新权重的不同方式有深入的了解吗？一些文献也很酷，但我在研究期间没有发现类似的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78834079/deep-learning-training-strategy-avoid-shuffling-individual-training-images-ins</guid>
      <pubDate>Mon, 05 Aug 2024 10:53:19 GMT</pubDate>
    </item>
    <item>
      <title>如何在一个短语中组合两个函数机器学习sklearn</title>
      <link>https://stackoverflow.com/questions/78833739/how-to-combine-2-functions-in-one-phrase-machine-learning-sklearn</link>
      <description><![CDATA[我有一个 data_set，用于存储助手对我的问题/任务的回答
data_set = { &#39;whats new&#39;:&#39;passive 没什么特别的...&#39;, &#39;&#39;:&#39;&#39;, }

passive - 与机器人的简单对话中的存根。
助手可以同时执行 2 个功能吗？例如，打开浏览器和游戏的功能。
也就是说，我希望它看起来像这样：
data_set = { &#39;whats new&#39;:&#39;game browser 没什么特别的...&#39;, &#39;&#39;:&#39;&#39;, }

def game():
pass
def browser():
pass

我希望我的助手在一个短语中同时执行 2 个功能]]></description>
      <guid>https://stackoverflow.com/questions/78833739/how-to-combine-2-functions-in-one-phrase-machine-learning-sklearn</guid>
      <pubDate>Mon, 05 Aug 2024 09:28:05 GMT</pubDate>
    </item>
    <item>
      <title>如何保存 TensorFlow 模型并在不同的文件中使用它？</title>
      <link>https://stackoverflow.com/questions/78833727/how-to-save-a-tensorflow-model-and-use-it-in-a-different-file</link>
      <description><![CDATA[我想保存我训练过的模型并将其加载到另一个文件中。
我尝试使用
model.save(my_model.keras)
但当我将其加载到另一个文件中或在运行时断开连接后加载它时（Colab 笔记本），它不起作用。有人可以建议不同的方法或解释我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78833727/how-to-save-a-tensorflow-model-and-use-it-in-a-different-file</guid>
      <pubDate>Mon, 05 Aug 2024 09:24:25 GMT</pubDate>
    </item>
    <item>
      <title>检测并定位图像中的多个物体[关闭]</title>
      <link>https://stackoverflow.com/questions/78830813/detect-and-localize-multiple-objects-in-the-image</link>
      <description><![CDATA[尝试对以下给定图像及其模板应用特征匹配。我想匹配给定图像中的模板，并在匹配的对象周围绘制框。这些只是示例图像，我还有很多不同分辨率的图像，所以不能使用模板匹配。以下是代码。
import cv2
import numpy as np
# 加载图像
img1 = cv2.imread(&#39;C:/Users/Autobobcat/Desktop/Candy- crush.23.jpg&#39;, cv2.IMREAD_COLOR) # 查询图像
img2 = cv2.imread(&#39;C:/Users/Autobobcat/Desktop/blueVertical.png&#39;, cv2.IMREAD_COLOR) # 训练图像
# 将图像转换为灰度
gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
# 创建 SIFT 检测器
sift = cv2.SIFT_create()
# 使用 SIFT 查找关键点和描述符
kp1, des1 = sift.detectAndCompute(gray1, None)
kp2, des2 = sift.detectAndCompute(gray2, None)
# 使用默认参数创建 BFMatcher 对象
bf = cv2.BFMatcher()
# 使用 KNN 匹配描述符
matches = bf.knnMatch(des1, des2, k=2)
# 应用具有更宽松阈值的比率测试
good_matches = []
for m, n in matches:
if m.distance &lt; 0.3 * n.distance: # 调整比例测试
good_matches.append(m)
# 按距离对匹配项进行排序
good_matches = sorted(good_matches, key=lambda x: x.distance)
# 绘制匹配项
img_matches = cv2.drawMatches(img1, kp1, img2, kp2, 
good_matches[:50], None, 
flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
# 定义目标屏幕尺寸
target_width = 1280
target_height = 720
# 计算图像的纵横比
(h, w) = img_matches.shape[:2]
aspect_ratio = w / h
# 确定新的尺寸并保持纵横比
如果 w &gt; target_width 或 h &gt;目标高度：
如果纵横比 &gt; 1：# 图像宽度大于高度
new_width = target_width
new_height = int(target_width /aspect_ratio)
else：# 图像高度大于宽度
new_height = target_height
new_width = int(target_height *aspect_ratio)
else：
new_width, new_height = w, h
dim = (new_width, new_height)
# 调整图像大小
resized_img_matches = cv2.resize(img_matches, dim, 
interpolation=cv2.INTER_AREA)
# 显示结果
cv2.imshow(&#39;Feature Matching&#39;, resized_img_matches)
cv2.waitKey(0)
cv2.destroyAllWindows()



]]></description>
      <guid>https://stackoverflow.com/questions/78830813/detect-and-localize-multiple-objects-in-the-image</guid>
      <pubDate>Sun, 04 Aug 2024 10:33:33 GMT</pubDate>
    </item>
    <item>
      <title>无法解决 QiskitMachineLearningError：'输入数据的形状不正确，最后一个维度不等于输入的数量：0，但得到：2'</title>
      <link>https://stackoverflow.com/questions/78828998/not-able-to-resolve-qiskitmachinelearningerror-input-data-has-incorrect-shape</link>
      <description><![CDATA[我收到错误：
32 vqc.fit(X_train, X_test)
33 
34 # 评估分类器

15 帧
/usr/local/lib/python3.10/dist-packages/qiskit_machine_learning/neural_networks/neural_network.py in _validate_input(self, input_data)
132 
133 if shape[-1] != self._num_inputs:
-&gt; 134 引发 QiskitMachineLearningError(
135 f&quot;输入数据的形状不正确，最后一个维度 &quot;
136 f&quot;不等于输入数量： &quot;

QiskitMachineLearningError：&#39;输入数据的形状不正确，最后一个维度不等于输入数量：0，但得到：2。&#39;

来自 qiskit 导入 QuantumCircuit、transpile、assemble
来自 qiskit_aer 导入 Aer
来自 qiskit_machine_learning.algorithms 导入 VQC
来自 qiskit_algorithms.optimizers 导入 COBYLA
来自 qiskit.circuit.library 导入 TwoLocal
来自 sklearn.preprocessing 导入 StandardScaler
来自 sklearn.model_selection 导入 train_test_split
导入 numpy 作为 np

# 用于演示目的的样本数据
# 将其替换为您的实际数据
scaled_data = np.random.rand(150, 2) # 用您的缩放数据替换
y = np.random.randint(0, 3, size=150) # 用您的标签替换

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(scaled_data, y, test_size=0.3, random_state=42)

# 根据特征维度定义量子比特的数量
num_qubits = X_train.shape[1]

# 使用正确的参数定义量子特征图和 ansatz
feature_map = TwoLocal(num_qubits=num_qubits, entanglement=&#39;linear&#39;) #, rotation_blocks=[&#39;ry&#39;, &#39;rz&#39;], entanglement_gate=&#39;cz&#39;)
ansatz = TwoLocal(num_qubits=num_qubits, entanglement=&#39;linear&#39;)#, rotation_blocks=[&#39;ry&#39;, &#39;rz&#39;], entanglement_gate=&#39;cz&#39;)

# 定义优化器
optimizer = COBYLA()

# 使用唯一参数名称初始化 VQC 分类器
vqc = VQC(feature_map=feature_map, ansatz=ansatz, optimizer=optimizer)

# 训练量子分类器
vqc.fit(X_train, X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/78828998/not-able-to-resolve-qiskitmachinelearningerror-input-data-has-incorrect-shape</guid>
      <pubDate>Sat, 03 Aug 2024 14:52:35 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 YOLO-v8 TFLite 模型比我的 Pytorch 模型慢？</title>
      <link>https://stackoverflow.com/questions/78510654/why-is-my-yolo-v8-tflite-model-slower-than-my-pytorch-model</link>
      <description><![CDATA[我最初有一个经过训练的 Pytorch YOLO-v8 纳米模型，用于视频中的多物体检测（10 个类别 - “自行车”、“椅子”、“盒子”、“桌子”、“塑料袋”、“花盆”、“行李箱​​和袋子”、“雨伞”、“购物车”、“人”）。
我使用 ultralytics 库的导出功能将其转换为 TFLite 模型。但是，当我在视频流上运行这两个模型时，我的 TFLite 模型的运行速度（FPS 约为 8）比我的 Pytorch 模型（FPS 约为 20）慢得多。为什么会这样？
TFLite 和 Pytorch 模型都在这里：https://drive.google.com/drive/folders/1A2XUD5sV332nXv-Z756Di_QUIZV3ObFv?usp=sharing
从训练好的 Pytorch 模型转换为 tflite 模型。
from ultralytics import YOLO

model = YOLO(&quot;yolov8n_trained.pt&quot;)
path = model.export(format=&quot;tflite&quot;) 

在 上运行视频流模型：
从 ultralytics 导入 YOLO
导入 cv2
从 time 导入 time

# 启动网络摄像头
stream_url = &quot;video_stream_path&quot; 

cap = cv2.VideoCapture(stream_url) # 使用 0 表示网络摄像头
cap.set(3, 640)
cap.set(4, 640)

# 加载重新训练的 YOLOv8 模型
model = YOLO(&quot;yolov8n_trained.tflite&quot;) # 测试 tflite 模型时使用此模型
# model = YOLO(&quot;yolov8n_trained.pt&quot;) # 测试 pytorch 模型时使用此模型

# 自定义类名
classNames = [&quot;Bicycle&quot;, &quot;Chair&quot;, &quot;Box&quot;, &quot;Table&quot;, &quot;Plastic bag&quot;, &quot;Flowerpot&quot;, 
&quot;Luggage and bags&quot;, &quot;Umbrella&quot;, &quot;Shopping trolley&quot;, &quot;Person&quot;]

#初始化变量以计算帧速率
prev_time = 0
fps = 0

while True:
success, img = cap.read()
if not success:
break

#计算处理帧所需的时间
curr_time = time()
fps = 1 / (curr_time - prev_time)
prev_time = curr_time

#在图像上显示帧速率
cv2.putText(img, f&quot;FPS: {fps:.2f}&quot;, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

# 在图像上运行 YOLO 模型
results = model(img, stream=True)

# 处理检测结果
for r in results:
for box in r.boxes:
# 提取边界框坐标和置信度
x1, y1, x2, y2 = map(int, box.xyxy[0]) # 转换为整数
confidence = box.conf[0] # 置信度得分
class_id = box.cls[0] # 类别 ID

# 绘制边界框
color = (0, 255, 0) # 边界框的绿色
cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)

# 绘制带有类别名称和置信度的标签
label = f&quot;{classNames[int(class_id)]}: {confidence:.2f}&quot;
label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
y1 = max(y1, label_size[1])
cv2.rectangle(img, (x1, y1 - label_size[1]), (x1 + label_size[0], y1 + base_line), (0, 255, 0), cv2.FILLED)
cv2.putText(img, label, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)

# 显示网络摄像头信息
cv2.imshow(&#39;Webcam&#39;, img)
if cv2.waitKey(1) == ord(&#39;q&#39;):
break

# 释放资源
cap.release()
cv2.destroyAllWindows()

]]></description>
      <guid>https://stackoverflow.com/questions/78510654/why-is-my-yolo-v8-tflite-model-slower-than-my-pytorch-model</guid>
      <pubDate>Tue, 21 May 2024 08:49:32 GMT</pubDate>
    </item>
    <item>
      <title>无法在 python 中安装 lap==0.4.0 库</title>
      <link>https://stackoverflow.com/questions/76463707/unable-to-install-lap-0-4-0-library-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76463707/unable-to-install-lap-0-4-0-library-in-python</guid>
      <pubDate>Tue, 13 Jun 2023 09:55:26 GMT</pubDate>
    </item>
    <item>
      <title>python 在对数逻辑回归中遇到除以零</title>
      <link>https://stackoverflow.com/questions/38125319/python-divide-by-zero-encountered-in-log-logistic-regression</link>
      <description><![CDATA[我正在尝试实现一个多类逻辑回归分类器，以区分 k 个不同的类。
这是我的代码。
import numpy as np
from scipy.special import expit

def cost(X,y,theta,regTerm):
(m,n) = X.shape
J = (np.dot(-(y.T),np.log(expit(np.dot(X,theta))))-np.dot((np.ones((m,1))-y).T,np.log(np.ones((m,1)) - (expit(np.dot(X,theta))).reshape((m,1))))) / m + (regTerm / (2 * m)) * np.linalg.norm(theta[1:])
return J

def梯度（X，y，theta，regTerm）：
（m，n）= X.shape
grad = np.dot（（（expit（np.dot（X，theta）））。reshape（m，1）- y）。T，X）/m +（np.concatenate（（[0]，theta[1：]。T），axis = 0））。reshape（1，n）
返回np.asarray（grad）

def train（X，y，regTerm，learnRate，epsilon，k）：
（m，n）= X.shape
theta = np.zeros（（k，n））
对于i在范围（0，k）中：
previousCost = 0;
currentCost = cost(X,y,theta[i,:],regTerm)
while(np.abs(currentCost-previousCost) &gt; epsilon):
print(theta[i,:])
theta[i,:] = theta[i,:] - learnRate*gradient(X,y,theta[i,:],regTerm)
print(theta[i,:])
previousCost = currentCost
currentCost = cost(X,y,theta[i,:],regTerm)
return theta

trX = np.load(&#39;trX.npy&#39;)
trY = np.load(&#39;trY.npy&#39;)
theta = train(trX,trY,2,0.1,0.1,4)

我可以验证 cost 和 gradient 是否返回正确维度的值（cost 返回标量，gradient 返回 1 乘以n 行向量），但我得到了错误
RuntimeWarning：在 log 中遇到除以零的情况
J = (np.dot(-(y.T),np.log(expit(np.dot(X,theta))))-np.dot((np.ones((m,1))-y).T,np.log(np.ones((m,1)) - (expit(np.dot(X,theta))).reshape((m,1))))) / m + (regTerm / (2 * m)) * np.linalg.norm(theta[1:])

为什么会发生这种情况，我该如何避免？]]></description>
      <guid>https://stackoverflow.com/questions/38125319/python-divide-by-zero-encountered-in-log-logistic-regression</guid>
      <pubDate>Thu, 30 Jun 2016 13:57:13 GMT</pubDate>
    </item>
    </channel>
</rss>