<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 06 Aug 2024 12:29:23 GMT</lastBuildDate>
    <item>
      <title>神经网络，信号处理，窗口大小</title>
      <link>https://stackoverflow.com/questions/78838750/neural-network-signal-processing-window-size</link>
      <description><![CDATA[我想找到合适的窗口大小来划分包含信号的 csvm 数据，以训练我的神经网络。您建议使用哪些方法来找到合适的窗口大小？
最主要的傅立叶频率
多窗口查找器
摘要统计子序列]]></description>
      <guid>https://stackoverflow.com/questions/78838750/neural-network-signal-processing-window-size</guid>
      <pubDate>Tue, 06 Aug 2024 11:24:57 GMT</pubDate>
    </item>
    <item>
      <title>如何优化机器学习模型，以最少的误报检测网络钓鱼推文？</title>
      <link>https://stackoverflow.com/questions/78838741/how-to-optimize-a-machine-learning-model-for-detecting-phishing-tweets-with-mini</link>
      <description><![CDATA[我目前正在开展一个旨在检测网络钓鱼推文的机器学习项目。虽然我的模型表现相当不错，但我面临的挑战是误报数量相对较多，这降低了系统的整体准确性和可用性。以下是我当前设置的详细信息：
数据集：平衡的网络钓鱼和非网络钓鱼推文数据集。
特征：推文文本、用户元数据、URL 特征。
尝试的算法：随机森林、SVM 和基本 LSTM 模型。
当前性能：准确率高，但由于误报，召回率低。
特征工程：添加了更多特征，如情绪分数和词嵌入。
模型调整：执行网格搜索以优化超参数。
集成方法：结合不同的算法来提高鲁棒性。]]></description>
      <guid>https://stackoverflow.com/questions/78838741/how-to-optimize-a-machine-learning-model-for-detecting-phishing-tweets-with-mini</guid>
      <pubDate>Tue, 06 Aug 2024 11:23:31 GMT</pubDate>
    </item>
    <item>
      <title>relative_attention_max_distance and relative_attention_num_buckets?</title>
      <link>https://stackoverflow.com/questions/78838655/relative-attention-max-distance-and-relative-attention-num-buckets</link>
      <description><![CDATA[在 T5 中，relative_attention_max_distance 和relative_attention_num_buckets 这两个参数代表什么？
在我的用例中，我想给出 512 个条件标记（编码器）并生成 384 个标记（解码器）。
这个用例的最佳参数应该是什么。我也想要长距离依赖。在理想情况下，任务需要绝对位置嵌入。]]></description>
      <guid>https://stackoverflow.com/questions/78838655/relative-attention-max-distance-and-relative-attention-num-buckets</guid>
      <pubDate>Tue, 06 Aug 2024 11:02:17 GMT</pubDate>
    </item>
    <item>
      <title>调用 cublasLtMatmul 时出现 RuntimeError：CUDA 错误：CUBLAS_STATUS_EXECUTION_FAILED</title>
      <link>https://stackoverflow.com/questions/78838155/runtimeerror-cuda-error-cublas-status-execution-failed-when-calling-cublasltma</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78838155/runtimeerror-cuda-error-cublas-status-execution-failed-when-calling-cublasltma</guid>
      <pubDate>Tue, 06 Aug 2024 09:00:40 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试使用 TensorFlow 运行一些面部识别代码，但我一直收到此 FLAG 错误</title>
      <link>https://stackoverflow.com/questions/78838145/im-trying-to-run-some-facial-recognition-code-using-tensorflow-but-i-keep-on-ge</link>
      <description><![CDATA[2024-08-06 14:18:52.654763:
I tensorflow/core/platform/cpu_feature_guard.cc:210]。此 TensorFlow 二进制文件经过优化，可在性能关键型操作中使用可用的 CPU 指令。
要启用以下指令：AVX2 AVX_VNNI FMA，在其他操作中，使用适当的编译器标志重建 TensorFlow。
每当我尝试运行任何类型的面部识别代码时，我都会收到这种错误。即使它与面部识别无关，而只是常规的张量流，我也会收到此错误。有人可以帮忙吗？但由于这个错误，我被困在了起跑线上。
 from deepface import DeepFace
import os

os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39;
img1 = &#39;reference.jpg&#39;
img2 = &#39;reference1.jpg&#39;

model_name = &#39;Facenet&#39;

result = DeepFace.verify(
img1_path=img1,
img2_path=img2,
model_name=model_name
)

result
]]></description>
      <guid>https://stackoverflow.com/questions/78838145/im-trying-to-run-some-facial-recognition-code-using-tensorflow-but-i-keep-on-ge</guid>
      <pubDate>Tue, 06 Aug 2024 08:58:22 GMT</pubDate>
    </item>
    <item>
      <title>什么是 tf.data.Dataset 以及为什么我的 Epoch 没有运行？</title>
      <link>https://stackoverflow.com/questions/78837808/what-tf-data-dataset-and-why-is-my-epoch-not-running</link>
      <description><![CDATA[history = Model_Enhancer.fit(x=[X_,X_wb,X_gc,X_ce],y=X_gt,batch_size=16,epochs=400,validation_split=0.3,shuffle=True)

我在使用这段代码时遇到了问题。
当我运行这段代码时，出现了以下错误：
RuntimeError Traceback (most recent call last)
&lt;ipython-input-19-46332b46168a&gt; in &lt;cell line: 1&gt;()
----&gt; 1 history = Model_Enhancer.fit(x=[X_,X_wb,X_gc,X_ce],y=X_gt,batch_size=16,epochs=400,validation_split=0.3,shuffle=True)

1 帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __iter__(self)
501 return iterator_ops.OwnedIterator(self)
502 else:
--&gt; 503 引发 RuntimeError(“`tf.data.Dataset` 仅支持 Python 样式”
504 “在 Eager 模式或 tf.function 内迭代。”)
505 

RuntimeError：`tf.data.Dataset` 仅支持在 Eager 模式或 tf.function 内迭代 Python 样式。

我期待 Epoch 运行。为什么 Epoch 没有运行。]]></description>
      <guid>https://stackoverflow.com/questions/78837808/what-tf-data-dataset-and-why-is-my-epoch-not-running</guid>
      <pubDate>Tue, 06 Aug 2024 07:42:40 GMT</pubDate>
    </item>
    <item>
      <title>NLTK bleu 分数明显高于 Sacrebleu bleu 分数</title>
      <link>https://stackoverflow.com/questions/78837792/nltk-bleu-score-significantly-higher-than-sacrebleu-bleu-score</link>
      <description><![CDATA[我试图将 bleu-4 分数的结果与 sacrebleu 和 NLTK corpus bleu 包进行比较，但结果之间的差异非常显著。
对于 NLTK corpus bleu，我获得了非常高的 bleu 分数（0.47、0.39、0.33、0.28）
但对于 sacrebleu，我获得了较低的分数（19.57、10.78、7.07、5.15），sacrebleu 分数已经将它们乘以 100，而 NLTK 没有
这是我计算这些分数的实现：
def compute_and_save_metrics(all_references, all_hypotheses, dataset_type, folder_name):
# 确保长度匹配
print(f&quot;Number of references: {len(all_references)}&quot;)
print(f&quot;假设数量：{len(all_hypotheses)}&quot;)

if len(all_references) != len(all_hypotheses):
raise ValueError(&quot;参考文献和假设的数量必须匹配。&quot;)

sacrebleu_scores = corpus_bleu(all_hypotheses, [all_references]).scores

bleu_score1 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(1.0, 0.0, 0.0, 0.0))
bleu_score2 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.5, 0.5))
bleu_score3 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.33, 0.33, 0.33))
bleu_score4 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.25, 0.25, 0.25, 0.25))


这是我生成预测的函数：
def assess_and_save(loader, dataset_type, folder_name):
model.eval()
all_references = []
all_hypotheses = []

sample_file_path = os.path.join(folder_name, &#39;samples.txt&#39;)
with open(sample_file_path, &#39;a&#39;, encoding=&#39;utf-8&#39;) as sample_file:
with torch.no_grad():
for Skeletons, Labels in loader:
Skeletons, Labels = Skeletons.to(device), Labels.to(device)

Outputs = model(skeletons, Labels[:, :-1])
Predictions = torch.argmax(outputs, dim=-1)

for i in range(predictions.size(0)):
Reference = tokenizer.decode(labels[i], skip_special_tokens=True)
Hypothesis = tokenizer.decode(predictions[i], skip_special_tokens=True)

# 调试：打印一些样本
if i &lt; 25：# 仅打印前 5 个样本
sample_text = f&quot;样本 {i+1}:\n参考：{reference}\n假设：{hypothesis}\nNew\n&quot;
print(sample_text)
sample_file.write(sample_text)

all_references.append(reference)
all_hypotheses.append(hypothesis)

# 检查是否有空引用或假设
empty_references = [ref for ref in all_references if not ref.strip()]
empty_hypotheses = [hyp for hyp in all_hypotheses if not hyp.strip()]

print(f&quot;空引用数：{len(empty_references)}&quot;)
print(f&quot;空假设数：{len(empty_hypotheses)}&quot;)

# 过滤掉空假设和相应的引用
non_empty_indices = [i for i, hyp in enumerate(all_hypotheses) if hyp.strip()]
all_references = [all_references[i] for i in non_empty_indices]
all_hypotheses = [all_hypotheses[i] for i in non_empty_indices]

compute_and_save_metrics(all_references, all_hypotheses, dataset_type, folder_name)


有什么建议我哪里错了吗？提前谢谢您]]></description>
      <guid>https://stackoverflow.com/questions/78837792/nltk-bleu-score-significantly-higher-than-sacrebleu-bleu-score</guid>
      <pubDate>Tue, 06 Aug 2024 07:40:08 GMT</pubDate>
    </item>
    <item>
      <title>在 Jupyter Notebook 中导入自定义模块的问题</title>
      <link>https://stackoverflow.com/questions/78837738/issues-with-importing-custom-modules-in-jupyter-notebook</link>
      <description><![CDATA[假设我们有一个如下的文件结构，
#注意：不是实际的目录结构，而是类似的结构
root../
tests../
src../
__init__.py
utils../
__init__.py
data.py
pipeline.py
sqlal../
__init__.py
sql_alchm.py
test.py
process.ipynb

以下是 data.py 的示例
from abc import ABC

class DataStrategy(ABC):
pass

以下是 test.py 的示例
from utils.data import DataStrategy

class TestPipeline(DataStrategy):
pass

现在我尝试在我的jupyter notebook process.ipynb，我得到如下错误
import os
from src.test import Test Pipeline

#单元格中的其余代码
.....
.....

ModuleNotFoundError：没有名为“utils”的模块

但是如果我运行文件 test.py，我没有收到错误，这个错误是否与笔记本的范围有关...还是我遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78837738/issues-with-importing-custom-modules-in-jupyter-notebook</guid>
      <pubDate>Tue, 06 Aug 2024 07:28:05 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML 算法的神经网络将产品列表映射到类别</title>
      <link>https://stackoverflow.com/questions/78837098/mapping-product-listings-to-categories-using-neural-networks-of-ml-algorithm</link>
      <description><![CDATA[我正在尝试实现 NN ML 算法来将产品映射到各自的类别，但该算法没有给出一致的结果。我使用了 keras、tensorflow 的顺序模型。请提出是否有更好的方法来解决此问题。
代码片段：
 # 分成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
print(y_train)
# 定义模型
model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_len))
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(units=128,activation=&#39;relu&#39;))
model.add(Dropout(0.5))
model.add(Dense(units=y_train.shape[1],activation=&#39;softmax&#39;))

# 编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;]

)
]]></description>
      <guid>https://stackoverflow.com/questions/78837098/mapping-product-listings-to-categories-using-neural-networks-of-ml-algorithm</guid>
      <pubDate>Tue, 06 Aug 2024 03:13:17 GMT</pubDate>
    </item>
    <item>
      <title>何时使用复合损失深度学习（图像分割）？</title>
      <link>https://stackoverflow.com/questions/78837046/when-to-use-composite-losses-deep-learning-image-segmentation</link>
      <description><![CDATA[我想知道在训练图像分割算法时，复合损失（例如 Dice + Focal Loss、Dice + Cross-Entropy Loss 或 Generalized Dice + Focal Loss）何时优于使用常规 Dice/CE Loss。
在什么情况下它们可以帮助算法更好地收敛？这是一个比较普遍的问题，但是否存在一个粗略的策略来确定某些算法的良好复合损失？]]></description>
      <guid>https://stackoverflow.com/questions/78837046/when-to-use-composite-losses-deep-learning-image-segmentation</guid>
      <pubDate>Tue, 06 Aug 2024 02:43:38 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 中的差分隐私错误</title>
      <link>https://stackoverflow.com/questions/78836989/error-in-tensorflow-for-differential-privacy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78836989/error-in-tensorflow-for-differential-privacy</guid>
      <pubDate>Tue, 06 Aug 2024 02:18:31 GMT</pubDate>
    </item>
    <item>
      <title>点数据（x，y 坐标）数组在 python 中转换为图像？</title>
      <link>https://stackoverflow.com/questions/78836964/point-data-x-y-coordinate-array-convert-to-image-in-python</link>
      <description><![CDATA[我从用户处收到点数据（x，y）。用户在屏幕上绘制一些数字。
我收到点数据...我需要将这些点数据转换为图像数据。
（用于识别数字）
请帮帮我...]]></description>
      <guid>https://stackoverflow.com/questions/78836964/point-data-x-y-coordinate-array-convert-to-image-in-python</guid>
      <pubDate>Tue, 06 Aug 2024 02:01:38 GMT</pubDate>
    </item>
    <item>
      <title>如何训练 haarcascades 分类器进行车牌号检测？[关闭]</title>
      <link>https://stackoverflow.com/questions/78836768/how-to-train-haarcascades-classifier-for-plate-number-detection</link>
      <description><![CDATA[我正在菲律宾使用 python、OpenCV 和 haarcascade 分类器进行车牌号检测。我正在使用 haarcascade_russian_plate_number.xml，但它给出了更多的误报。检测的示例文件为 mp4 格式。
我尝试使用 indian_license_plate.xml，但它给出了更多的误报，并且是在图像的同一位置进行检测。]]></description>
      <guid>https://stackoverflow.com/questions/78836768/how-to-train-haarcascades-classifier-for-plate-number-detection</guid>
      <pubDate>Mon, 05 Aug 2024 23:47:54 GMT</pubDate>
    </item>
    <item>
      <title>自定义模型聚合器 TensorFlow Federated</title>
      <link>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</link>
      <description><![CDATA[我正在尝试使用 TensorFlow Federated，我想为我的训练程序编写一个自定义模型聚合器，这样客户端大小与某个预定义值的距离就用于加权每个客户端的更新（优先考虑某些客户端的更新而不是其他客户端的更新）。
我想继续使用我在之前的模拟中使用的简单 FedAvg 算法，以便能够比较结果：
trainer = tff.learning.algorithms.build_weighted_fed_avg(
model_fn= tff_model,
client_optimizer_fn=client_optimizer,
server_optimizer_fn=server_optimizer
)

我知道 tff.learning.algorithms.build_weighted_fed_avg() 接受 model_aggregator 作为参数，但我不知道如何创建一个计算此类权重的聚合器。
有没有简单的方法来定义它？]]></description>
      <guid>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</guid>
      <pubDate>Mon, 05 Aug 2024 16:06:48 GMT</pubDate>
    </item>
    <item>
      <title>超参数调优与分类算法对比</title>
      <link>https://stackoverflow.com/questions/65516888/hyper-prparameter-tuning-and-classification-algorithm-comparation</link>
      <description><![CDATA[我对分类算法比较有疑问。
我正在做一个关于数据集的超参数调整和分类模型比较的项目。
目标是找出最适合我的数据集的具有最佳超参数的最佳拟合模型。
例如：我有 2 个分类模型（SVM 和随机森林），我的数据集有 1000 行和 10 列（9 列是特征），最后一列是标签。
首先，我将数据集分成 2 个部分（80-10），分别用于训练（800 行）和测试（200 行）。之后，我使用 CV = 10 的网格搜索来调整这两个模型（SVM 和随机森林）在训练集上的超参数。当确定了每个模型的超参数后，我会使用这两个模型的超参数再次测试训练集和测试集上的 Accuracy_score，以找出哪个模型最适合我的数据（条件：训练集上的 Accuracy_score &lt; 测试集上的 Accuracy_score（未过度拟合）并且哪个模型在测试集上的 Accuracy_score 更高，则该模型为最佳模型）。
但是，SVM 显示训练集的 Accuracy_score 为 100，测试集的 Accuracy_score 为 83.56，这意味着调整超参数的 SVM 过度拟合。另一方面，随机森林显示训练集的 Accuracy_score 为 72.36，测试集的 Accuracy_score 为 81.23。很明显，SVM 的测试集准确度得分高于随机森林的测试集准确度得分，但 SVM 过度拟合。
我有一些问题：
_ 当我像上面一样对训练和测试集的准确度得分进行比较而不是使用交叉验证时，我的方法是否正确？（如果使用交叉验证，该怎么做？
_ 很明显，上面的 SVM 过度拟合，但其测试集准确度得分高于随机森林的测试集准确度得分，在这种情况下我能得出 SVM 是最佳模型的结论吗？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/65516888/hyper-prparameter-tuning-and-classification-algorithm-comparation</guid>
      <pubDate>Thu, 31 Dec 2020 05:11:06 GMT</pubDate>
    </item>
    </channel>
</rss>