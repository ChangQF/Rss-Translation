<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 26 Jan 2024 12:23:11 GMT</lastBuildDate>
    <item>
      <title>从头开始训练神经网络时如何处理成本值波动？</title>
      <link>https://stackoverflow.com/questions/77886091/how-to-handle-cost-value-fluctuations-when-training-a-neural-network-from-scratc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77886091/how-to-handle-cost-value-fluctuations-when-training-a-neural-network-from-scratc</guid>
      <pubDate>Fri, 26 Jan 2024 12:01:16 GMT</pubDate>
    </item>
    <item>
      <title>为什么在小数据集上微调 MLP 模型，仍然保持与预训练权重相同的测试精度？</title>
      <link>https://stackoverflow.com/questions/77885918/why-finetuning-mlp-model-on-a-small-dataset-still-keeps-the-test-accuracy-same</link>
      <description><![CDATA[我设计了一个简单的 MLP 模型，在 6k 数据样本上进行训练。
类 MLP(nn.Module)：
    def __init__(自身,input_dim=92,hidden_​​dim=150,num_classes=2):
        超级().__init__()
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.hidden_​​dim = 隐藏_dim
        #self.softmax = nn.Softmax(dim=1)

        self.layers = nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_​​dim),
            ReLU(),
            nn.Linear(self.hidden_​​dim, self.hidden_​​dim),
            ReLU(),
            nn.Linear(self.hidden_​​dim, self.hidden_​​dim),
            ReLU(),
            nn.Linear(self.hidden_​​dim, self.num_classes),

        ）

    def 前向（自身，x）：
        x = self.layers(x)
        返回x

并且模型已实例化
model = MLP(input_dim=input_dim,hidden_​​dim=hidden_​​dim,num_classes=num_classes).to(设备)

优化器= Optimizer.Adam(model.parameters(),lr=learning_rate,weight_decay=1e-4)
标准 = nn.CrossEntropyLoss()

和超参数：
num_epoch = 300 # 200e3//len(train_loader)
学习率 = 1e-3
批量大小 = 64
设备 = torch.device(“cuda”)
种子 = 42
火炬.manual_seed(42)

我的实现主要遵循这个问题。我将模型保存为预训练权重 model_weights.pth。
测试数据集上模型的准确率为96.80%。
然后，我还有另外 50 个样本（在 finetune_loader 中），我正在尝试在这 50 个样本上微调模型：
model_finetune = MLP()
model_finetune.load_state_dict(torch.load(&#39;model_weights.pth&#39;))
model_finetune.to（设备）
model_finetune.train()
# 训练网络
对于 t in tqdm(范围(num_epoch))：
  对于 i，enumerate(finetune_loader, 0) 中的数据：
    #def 闭包():
      # 获取并准备输入
      输入、目标 = 数据
      输入，目标=输入.float(), 目标.long()
      输入，目标 = 输入.to(设备), 目标.to(设备)
      
      # 将梯度归零
      优化器.zero_grad()
      # 执行前向传递
      输出 = model_finetune(输入)
      # 计算损失
      损失=标准（输出，目标）
      # 执行向后传递
      loss.backward()
      #回波损耗
      优化器.step() # a

model_finetune.eval()
使用 torch.no_grad()：
    输出2 = model_finetune(测试数据)
    #predicted_labels =outputs.squeeze().tolist()

    _, preds = torch.max(输出2, 1)
    Prediction_test = np.array(preds.cpu())
    准确度测试微调 = 准确度得分（y_测试，预测测试）
    精度测试微调
    
    输出：0.9680851063829787

我检查过，精度与将模型微调到 50 个样本之前保持不变，并且输出概率也相同。
可能是什么原因？我在微调代码中是否犯了一些错误？]]></description>
      <guid>https://stackoverflow.com/questions/77885918/why-finetuning-mlp-model-on-a-small-dataset-still-keeps-the-test-accuracy-same</guid>
      <pubDate>Fri, 26 Jan 2024 11:32:05 GMT</pubDate>
    </item>
    <item>
      <title>Mwaa 气流任务跳过</title>
      <link>https://stackoverflow.com/questions/77885701/mwaa-airflow-task-skipping</link>
      <description><![CDATA[我正在尝试创建一个机器学习训练 mwaa 气流工作流程
我有以下步骤

数据处理
培训
批量推理

我的要求是每周进行一次数据处理和推理，每月进行一次训练。
如何跳过训练步骤并使其每月仅运行一次]]></description>
      <guid>https://stackoverflow.com/questions/77885701/mwaa-airflow-task-skipping</guid>
      <pubDate>Fri, 26 Jan 2024 10:55:24 GMT</pubDate>
    </item>
    <item>
      <title>让谢尔曼-莫里森更新更加高效</title>
      <link>https://stackoverflow.com/questions/77884077/make-sherman-morrison-update-more-efficient</link>
      <description><![CDATA[我需要计算 CIFAR10 数据集子集的点上的参数梯度的协方差矩阵。为此，我有以下代码：
from torch.func import function_call, vmap, grad

model1 = LogisticModel().to(设备)

def loss_fn（预测，目标）：
  损失 = nn.CrossEntropyLoss()
  回波损耗（预测、目标）

defcompute_loss（参数，缓冲区，样本，目标）：
  批次=样本.unsqueeze(0)
  目标 = target.unsqueeze(0)

  预测 = function_call(model1, (params, buffers), (batch,))
  损失= loss_fn（预测，目标）
  回波损耗

ft_compute_grad = grad(compute_loss)
ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(无, 无, 0, 0))

def sherman_morrison_update(A, u, v):
  vT = v.T
  金=A@u

  α = 1/(1 + vT@Au)
  A = A - alpha*torch.outer(Au, vT@A)
  返回A

testloader1 = DataLoader（test_dataset，batch_size = 512）
params = {k: v.detach() for k, v in model1.named_pa​​rameters()}
buffers = {k: v.detach() for k, v in model1.named_buffers()}
w = 0

p_covs = {p:torch.eye(q.flatten().shape[0]).to(device) for p,q in param_grads.items()}
param_grad_mean = {p:torch.zeros(q​​.flatten().shape[0]).to(device) for p,q in param_grads.items()}

对于 tqdm(testloader1) 中的 x,y：
  param_grads = ft_compute_sample_grad(参数、缓冲区、x.to(设备)、y.to(设备))
  对于 p，q，zip 中的平均值（param_grads.values（），p_covs，param_grad_mean）：
    对于 p 中的 p_grad：
      w += 1
      diff = p_grad.flatten() - param_grad_mean[平均值]
      param_grad_mean[平均值] += diff / w
      p_covs[q] = sherman_morrison_update(A=p_covs[q], u=diff, v= diff)

现在，这是非常低效的，并且在每次迭代中执行此操作都非常耗时。
此外，我们无法真正同时获取所有点的参数梯度，因为这会导致内存问题（因此我转向 Sherman-Morrison）。
有没有办法提高效率？谢尔曼-莫里森的更好实施？还有什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/77884077/make-sherman-morrison-update-more-efficient</guid>
      <pubDate>Fri, 26 Jan 2024 03:37:37 GMT</pubDate>
    </item>
    <item>
      <title>如何跟踪无人机图像中的单个点（例如角落）？</title>
      <link>https://stackoverflow.com/questions/77883796/how-to-track-a-single-point-e-g-corner-in-uav-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77883796/how-to-track-a-single-point-e-g-corner-in-uav-images</guid>
      <pubDate>Fri, 26 Jan 2024 01:40:21 GMT</pubDate>
    </item>
    <item>
      <title>“java.lang.IllegalArgumentException：字节缓冲区的大小和形状不匹配。” - 但他们确实</title>
      <link>https://stackoverflow.com/questions/77883608/java-lang-illegalargumentexception-the-size-of-byte-buffer-and-the-shape-do-no</link>
      <description><![CDATA[我正在尝试使用 Android Studio 中的以下代码将 ML 模型集成到 Android 应用中：
predictBtn.setOnClickListener(new View.OnClickListener() {
            @覆盖
            公共无效onClick（查看视图）{
                尝试 {
                    ModelUnquant 模型 = ModelUnquant.newInstance(MainActivity.this);


                    // 检查位图是否不为空
                    if (位图！= null) {
                        // 创建输入以供参考。
                        TensorBuffer inputFeature0 = TensorBuffer.createFixedSize(new int[]{1, 244, 244, 3}, DataType.FLOAT32);

                        位图 = Bitmap.createScaledBitmap(位图, 244, 244, true);
                        inputFeature0.loadBuffer(TensorImage.fromBitmap(bitmap).getBuffer());

                        // 运行模型推理并获取结果。
                        ModelUnquant.Outputs 输出 = model.process(inputFeature0);
                        TensorBufferoutputFeature0=outputs.getOutputFeature0AsTensorBuffer();

                        // 在这里更改类号
                        result.setText(labels[getMax(outputFeature0.getFloatArray())]+“”);
                    } 别的 {
                        // 处理bitmap为null的情况
                        result.setText(“错误：无图像”);
                    }

                    // 如果不再使用则释放模型资源。
                    模型.close();
                } catch (IOException e) {
                    // TODO 处理异常
                }
            }
        });

返回以下错误：
致命异常：main
进程：com.example.archeyewitness，PID：15698

java.lang.IllegalArgumentException：字节缓冲区的大小和形状不匹配。

在 org.tensorflow.lite.support.common.SupportPreconditions.checkArgument（SupportPreconditions.java:104）

在 org.tensorflow.lite.support.tensorbuffer.TensorBuffer.loadBuffer(TensorBuffer.java:309)

在 org.tensorflow.lite.support.tensorbuffer.TensorBuffer.loadBuffer(TensorBuffer.java:328)

在 com.example.archeyewitness.MainActivity$3.onClick(MainActivity.java:96)

现在，我尝试通过打印字节缓冲区的大小和形状来调试它：
Log.d(&quot;DebugInfo&quot;, &quot;张量缓冲区大小：&quot; + tensorBufferSize);
Log.d(&quot;DebugInfo&quot;, &quot;字节缓冲区大小:&quot; + byteBufferSize);
Log.d(“DebugInfo”, “张量缓冲区形状：” + Arrays.toString(tensorBufferShape));
Log.d(“DebugInfo”, “预期形状：” + Arrays.toString(byteBufferShape));


返回以下内容：
张量缓冲区大小：150528
2024-01-25 18:29:33.965 14367-14367 DebugInfo com.example.archeyewitness D 字节缓冲区大小：150528
2024-01-25 18:29:33.965 14367-14367 DebugInfo com.example.archeyewitness D 张量缓冲区形状：[1, 224, 224, 3]
2024-01-25 18:29:33.965 14367-14367 DebugInfo com.example.archeyewitness D 预期形状：[1, 224, 224, 3]
2024-01-25 18:29:33.971 14367-14367 AndroidRuntime com.example.archeyewitness D 关闭虚拟机
2024-01-25 18:29:33.992 14367-14367 AndroidRuntime com.example.archeyewitness

如您所见，尺寸确实匹配，但我仍然遇到相同的错误。有人可以解释一下发生了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/77883608/java-lang-illegalargumentexception-the-size-of-byte-buffer-and-the-shape-do-no</guid>
      <pubDate>Fri, 26 Jan 2024 00:16:55 GMT</pubDate>
    </item>
    <item>
      <title>如何采用在 Python 中训练并使用 Pickle 保存的机器学习模型，并在 C++ 中将其作为预测模型运行？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77883553/how-do-i-take-a-machine-learning-model-that-i-trained-in-python-and-saved-with</link>
      <description><![CDATA[我有一个用Python训练的机器学习模型，我需要最好将该模型转换成可以在C++中运行的dll。我希望也许有一个可以读取 pickle 的 C++ 包，或者其他一些可以共享机器学习模型进行预测的 Python/C++ 库。
我发现了这个名为 Pickling tools 的库，它应该用于 Python 和 C++ 之间的交叉通信，其口号是“How can I pickle in C++?”但我很难找到将我的pickle 放入C++ 中的部分。
http://www.picklingtools.com/html/usersguide.html ]]></description>
      <guid>https://stackoverflow.com/questions/77883553/how-do-i-take-a-machine-learning-model-that-i-trained-in-python-and-saved-with</guid>
      <pubDate>Thu, 25 Jan 2024 23:54:37 GMT</pubDate>
    </item>
    <item>
      <title>管道并行神经网络[关闭]</title>
      <link>https://stackoverflow.com/questions/77883493/pipeline-parallel-neural-network</link>
      <description><![CDATA[图中的Cn、Cn-x、Cn+1、Cn-x+1是什么意思？我正在阅读有关 Pipedream 管道并行的内容，但无法理解该图像。
论文链接（图4）：
https://arxiv.org/pdf/1806.03377.pdf
pipedream 图像
论文中提到C是计算，但没有提到n,x, n+1,n-x是什么。]]></description>
      <guid>https://stackoverflow.com/questions/77883493/pipeline-parallel-neural-network</guid>
      <pubDate>Thu, 25 Jan 2024 23:32:35 GMT</pubDate>
    </item>
    <item>
      <title>如何保留 Oracle Machine Learning (OML) k-means 模型？</title>
      <link>https://stackoverflow.com/questions/77883357/how-can-i-persist-an-oracle-machine-learning-oml-k-means-model</link>
      <description><![CDATA[此示例不展示如何将 k-means 聚类模型保存到数据库中。有没有办法做到这一点并且有示例代码吗？
适用于 Python 的 Oracle 机器学习
这是我创建模型的方法，
# 创建KM模型对象并拟合。
km_mod = oml.km(n_clusters = 3, **设置).fit(数据)
]]></description>
      <guid>https://stackoverflow.com/questions/77883357/how-can-i-persist-an-oracle-machine-learning-oml-k-means-model</guid>
      <pubDate>Thu, 25 Jan 2024 22:46:42 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch LSTM 多目标维度错误</title>
      <link>https://stackoverflow.com/questions/77883294/pytorch-lstm-multi-target-dimension-error</link>
      <description><![CDATA[我几天来一直在尝试执行 LSTM 多目标，但没有成功，因为数据集的前 8 列是目标，其他列是特征，从而产生维度错误。挑战包括根据特征值预测 8 个整数值可以是 0、1 或 2 的目标。之前我成功创建了一个 LSTM 来预测单个目标列，该目标列是所有 8 列的总和。但这个总和在置信度得分中产生了不良结果。有什么错误吗？
导入 pandas 作为 pd
将 numpy 导入为 np

# 设置种子以实现可重复性
np.随机.种子(42)

# 创建数据框
data = {&#39;col_&#39; + str(i+1): np.random.choice([0, 1, 2], 100) 如果 i &lt; 8 else np.random.uniform(-0.99, 0.99, 100) for i in range(100)}
df = pd.DataFrame(数据)
df.head()

# 显示数据框
打印（df）

进口火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
从 torch.utils.data 导入 DataLoader，TensorDataset
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler

# 提取目标和特征
目标 = df.iloc[:, :8].values
特征 = df.iloc[:, 8:].values

# 缩放功能
定标器=标准定标器()
特征=scaler.fit_transform（特征）

# 将目标转换为 LongTensor
目标 = torch.tensor(targets, dtype=torch.long)

# 将特征转换为FloatTensor
特征 = torch.tensor(特征, dtype=torch.float32)

# 将数据分为训练集和测试集
features_train、features_test、targets_train、targets_test = train_test_split(
    特征、目标、test_size=0.2、random_state=None
）

# 创建数据加载器
train_dataset = TensorDataset（features_train，targets_train）
train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True)

# 定义LSTM模型
类 LSTMModel(nn.Module):
    def __init__(自身、输入大小、隐藏大小、层数、输出大小):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size,hidden_​​size,num_layers,batch_first=True)
        self.fc = nn.Linear(隐藏大小, 输出大小)

    def 前向（自身，x）：
        输出，_ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        返回

# 设置超参数
输入大小 = features.shape[1]
隐藏大小 = 64
层数 = 2
output_size = 8 # 目标类的数量
纪元数 = 10
学习率 = 0.001

# 实例化模型、损失函数和优化器
模型 = LSTMModel(输入大小、隐藏大小、层数、输出大小)
标准 = nn.CrossEntropyLoss()
优化器 = optim.Adam(model.parameters(), lr=learning_rate)

# 训练循环
对于范围内的纪元（num_epochs）：
    对于train_loader中的batch_features、batch_targets：
        优化器.zero_grad()
        输出=模型（batch_features）
        损失 = 标准（输出，batch_targets）
        loss.backward()
        优化器.step()

    print(f&#39;Epoch [{epoch+1}/{num_epochs}], 损失: {loss.item():.4f}&#39;)

# 在测试集上评估模型
模型.eval()
使用 torch.no_grad()：
    test_outputs = 模型（features_test）
    _, 预测 = torch.max(test_outputs, 1)

# 计算准确率
正确=(预测==targets_test).sum().item()
总计=targets_test.size(0)
准确率=正确率/总分
print(f&#39;测试准确度: {accuracy:.4f}&#39;)

运行时错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- ------------------------------- IndexError Traceback（最近一次调用最后一次）Cell In[4]，第 60 58 行，针对batch_features , train_loader 中的batch_targets: 59 Optimizer.zero_grad() ---&gt; 60 输出 = 模型（batch_features） 61 损失 = criteria（输出，batch_targets） 62 loss.backward（）

文件 c:\Users\Admin\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py:1501，在 Module._call_impl(self, *args, **kwargs)第1496章 第1497章第1498章 1499、第1500章第1501章 return front_call(*args, **kwargs) 第1502章 使用jit时不要调用函数 第1503章 full_backward_hooks, non_full_backward_hooks = [], []

Cell In[4]，第 40 行 38 def forward(self, x): 39 out, _ = self.lstm(x) ---&gt; 40 out = self.fc(out[:, -1, :]) 41 返回 out

IndexError：2 维张量的索引过多

我尝试多次修改 LSTModel 类，但没有得到安全结果]]></description>
      <guid>https://stackoverflow.com/questions/77883294/pytorch-lstm-multi-target-dimension-error</guid>
      <pubDate>Thu, 25 Jan 2024 22:32:05 GMT</pubDate>
    </item>
    <item>
      <title>在 Cloud Functions 中运行 MobileNetV2</title>
      <link>https://stackoverflow.com/questions/77882843/running-mobilenetv2-in-cloud-functions</link>
      <description><![CDATA[我正在尝试使用 Cloud Functions v2、Python 3.11 和 Tensorflow 2.15 获取给定图像的特征。
由于某种原因，模型在获取预测时没有返回，因此我的函数超时。
我一直在添加更多的 CPU，确切地说是 8 个以及 32GB 内存，但我仍然找不到返回任何结果的方法。
这是我正在执行的代码的一部分：
从 keras.applications 导入 MobileNetV2
从 keras.preprocessing 导入图像
从 keras.applications.mobilenet_v2 导入 preprocess_input

# include_top=False - 排除最终的分类层，专注于提取特征。
# pooling=&#39;avg&#39; - 添加全局平均池化层以将特征图压缩为单个向量（嵌入）。
#weights=&#39;imagenet&#39; - 使用在 ImageNet 上预先训练的权重以实现更好的特征提取。
模型 = MobileNetV2(权重=&#39;imagenet&#39;, include_top=False, pooling=&#39;avg&#39;)

def extract_embeddings(image_url):
  
    响应 = requests.get(image_url)
    响应.raise_for_status()

    img = image.load_img(BytesIO(响应.内容), target_size=(224, 224))

    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, 轴=0)
    img_array = 预处理_输入(img_array)

    嵌入 = model.predict(img_array)
    print(“预测的嵌入”)
    返回 embeddings.flatten()

我在这里有点迷失，所以希望有人能帮忙
我尝试扩展我的资源，但没有成功]]></description>
      <guid>https://stackoverflow.com/questions/77882843/running-mobilenetv2-in-cloud-functions</guid>
      <pubDate>Thu, 25 Jan 2024 20:45:38 GMT</pubDate>
    </item>
    <item>
      <title>“PyTorch Conv2d 错误：预期有 3 个通道，但 [1, 128, 128, 3] 为 128。需要帮助！”</title>
      <link>https://stackoverflow.com/questions/77881247/pytorch-conv2d-error-expected-3-channels-got-128-for-1-128-128-3-help-n</link>
      <description><![CDATA[我在使用 PyTorch 卷积神经网络时遇到问题。我收到的错误消息是：
给定 groups=1，权重大小为 [8, 3, 5, 5]，预期输入 [1, 128, 128, 3] 有 3 个通道，但实际有 128 个通道
上下文：
模型架构：
导入 torch.nn 作为 nn
导入 torch.nn.function 作为 F

CNN 类（nn.Module）：
    def __init__(自身):
        超级（CNN，自我）.__init__()
        self.cnn_model = nn.Sequential(
        nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 5),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size = 3, stride = 5),
        nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 5),
        nn.Tanh(),
        nn.AvgPool2d(kernel_size = 2, stride = 5))
        
        self.fc_model = nn.Sequential(
        nn.Linear（输入特征= 256，输出特征= 120），
        nn.Tanh(),
        nn.Linear(in_features = 120, out_features = 84),
        nn.Tanh(),
        nn.Linear(in_features = 84, out_features = 1))
        
    def 前向（自身，x）：
        x = self.cnn_model(x)
        x = x.view(x.size(0), -1)
        x = self.fc_model(x)
        x = F.sigmoid(x)
        
        返回x

数据加载：
类 MRI（数据集）：
    def __init__(自身):
        # 加载图像和标签
        肿瘤=[]
        path_tumor = &#39;/kaggle/input/brain-tumor/Dataset/Yes_Data/*.jpg&#39;
        对于 glob.iglob(path_tumor) 中的 f：
            img = cv2.imread(f)
            img = cv2.resize(img, (128, 128), 插值=cv2.INTER_AREA)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            肿瘤.append(img)

        健康=[]
        path_healthy = &#39;/kaggle/input/brain-tumor/Dataset/No_data/*.jpg&#39;
        对于 glob.iglob(path_healthy) 中的 f：
            img = cv2.imread(f)
            img = cv2.resize(img, (128, 128), 插值=cv2.INTER_AREA)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            健康的.append(img)

        # 将列表转换为 numpy 数组
        健康 = np.array(健康)
        肿瘤 = np.array(肿瘤)
        
        # 创建标签
        tumor_label = np.ones(tumor.shape[0], dtype=np.float32)
        health_label = np.zeros(healthy.shape[0], dtype=np.float32)

        # 连接图像和标签
        images = np.concatenate((肿瘤，健康)，轴=0)
        标签 = np.concatenate((tumor_label,healthy_label), axis=0)
        self.images = 图像
        self.labels = 标签

    def __getitem__(自身，索引)：
        样本 = {&#39;image&#39;: self.images[index], &#39;label&#39;: self.labels[index]}
        返回样品

    def __len__(自身):
        返回 self.images.shape[0]

    def 标准化（自身）：
        self.images = (self.images / 255.0).astype(np.float32)

错误上下文：
model.eval()
输出 = []
y_true = []

使用 torch.no_grad()：
    对于数据加载器中的示例：
        对于范围内的 i(sample[&#39;image&#39;].size(0))：
            图像 = 样本[&#39;图像&#39;][i].squeeze().to(device).float()
            标签 = 样本[&#39;标签&#39;][i].to(设备)

            y_hat = 模型（图像）
            输出.append(y_hat.cpu().detach().numpy())
            y_true.append(label.cpu().detach().numpy())


我已经尝试了所有调试方法，打印并检查形状]]></description>
      <guid>https://stackoverflow.com/questions/77881247/pytorch-conv2d-error-expected-3-channels-got-128-for-1-128-128-3-help-n</guid>
      <pubDate>Thu, 25 Jan 2024 15:46:44 GMT</pubDate>
    </item>
    <item>
      <title>Python 3.12 中的 TensorFlow</title>
      <link>https://stackoverflow.com/questions/77471744/tensorflow-in-python-3-12</link>
      <description><![CDATA[我正在尝试通过 Python 将数据导入 SQL，并且在 Python 3.12 中使用 pyodbc 和 TensorFlow，但 TensorFlow 不起作用。我无法使用 python 3.11，因为 pyodbc 与它不兼容。
我尝试使用pip install tensorflow，但收到此错误：
&lt;块引用&gt;
错误：找不到满足张量流要求的版本（来自版本：无）
错误：找不到张量流的匹配分布

如何在使用 Python 3.12 时解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/77471744/tensorflow-in-python-3-12</guid>
      <pubDate>Mon, 13 Nov 2023 04:34:57 GMT</pubDate>
    </item>
    <item>
      <title>如何删除 JupyterLab 打印输出上的 <pad> 和 </s></title>
      <link>https://stackoverflow.com/questions/71671769/how-to-remove-pad-and-s-on-jupyterlab-print-output</link>
      <description><![CDATA[我正在使用通过 Anaconda 安装的 JupyterLab 笔记本来运行机器学习应用程序。如果我运行该应用程序，JupyterLab 会自动插入 和&lt;/s&gt;每个生成的句子的开头和结尾都有标签。
这是一个例子：
导入重新
从变压器导入 T5Tokenizer、T5ForConditionalGeneration
推文数据 = [
    “尤其是在接下来的几天和几周内，......平台将其标准应用于人”，
    “只剩下 2 天了，我的时间表对 #USEelections2020 有何看法”，
    “...更多数据在这里”
]
模型 = T5ForConditionalGeneration.from_pretrained(&#39;t5-base&#39;)
tokenizer = T5Tokenizer.from_pretrained(&#39;t5-base&#39;)
文本=“ ”.join(tweet_data)
TEXT_CLEANING_RE = &quot;@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+&quot;
文本 = re.sub(TEXT_CLEANING_RE, &#39; &#39;, str(text).lower()).strip()
Preprocessed_text = “总结：”+文本
tokens_input = tokenizer.encode(
    预处理文本，
    return_tensors =“pt”，
    最大长度=512，
    截断=真）
摘要_ids = model.generate(
    令牌_输入，
    最小长度=60，
    最大长度=180，
    长度惩罚=4.0)
摘要 = tokenizer.decode(summary_ids[0])
打印（摘要）

这是输出：
&lt;前&gt;&lt;代码&gt;&lt;垫&gt;; srpoll：乔拜登选举2020：乔拜登平等自由。&lt;/s&gt;

如何确保和&lt;/s&gt;不在打印输出上吗？该应用程序是面向用户的，因此标签如果出现可能会损害他们的体验。
我尝试将它们作为字符串删除，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/71671769/how-to-remove-pad-and-s-on-jupyterlab-print-output</guid>
      <pubDate>Wed, 30 Mar 2022 05:03:19 GMT</pubDate>
    </item>
    <item>
      <title>无法导入名称“ops”python</title>
      <link>https://stackoverflow.com/questions/51076277/cannot-import-name-ops-python</link>
      <description><![CDATA[我正在尝试运行一个应用程序。但是我收到错误：
from createDB import load_dataset
将 numpy 导入为 np
导入keras
从 keras.utils 导入到_categorical
将 matplotlib.pyplot 导入为 plt
从 sklearn.model_selection 导入 train_test_split
从 keras.models 导入顺序、输入、模型
从 keras.layers 导入密集、Dropout、Flatten
从 keras.layers 导入 Conv2D、MaxPooling2D
从 keras.layers.normalization 导入 BatchNormalization
从 keras.layers.advanced_activations 导入 LeakyReLU
################################################33
#显示数据集
X_train,y_train,X_test,y_test = load_dataset()
print(&#39;训练数据形状：&#39;, X_train.shape, y_train.shape)
print(&#39;测试数据形状：&#39;, X_test.shape, y_test.shape)
#################################################### ##########
# 从火车标签中找到唯一的数字
类 = np.unique(y_train)
nClasses = len(类)
print(&#39;输出总数：&#39;, nClasses)
print(&#39;输出类：&#39;, 类)
#################################################### #
#plt.figure(figsize=[5,5])
#
## 显示训练数据中的第一张图像
#plt.子图(121)
#plt.imshow(X_train[0,:,:], cmap=&#39;灰色&#39;)
#plt.title(&quot;基本事实：{}&quot;.format(y_train[0]))
#
## 显示测试数据中的第一张图像
#plt.子图(122)
#################################################### ######
#X_train.max()
#X_train.shape()
####################################
# 标准化和 float32
X_train = X_train.astype(&#39;float32&#39;)
X_test = X_test.astype(&#39;float32&#39;)
X_train = X_train / 255。
X_测试 = X_测试 / 255。
###############################3
#将标签从分类编码更改为one-hot编码
y_train_one_hot = to_categorical(y_train)
y_test_one_hot = to_categorical(y_test)

# 使用one-hot编码显示类别标签的变化
print(&#39;原标签：&#39;, y_train[25])
print(&#39;转换为one-hot后：&#39;, y_train_one_hot[25])
##############################################
# 训练分为训练和验证
X_train,X_valid,train_label,valid_label = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=13)
X_train.shape,
X_valid.shape,
火车标签.形状，
valid_label.shape
##########################
批量大小 = 64
历元 = 20
类数 = 3
####################
Fashion_model = 顺序()
Fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation=&#39;线性&#39;,input_shape=(28,28,3),padding=&#39;相同&#39;))
Fashion_model.add(LeakyReLU(alpha=0.1))
Fashion_model.add(MaxPooling2D((2, 2),padding=&#39;相同&#39;))
Fashion_model.add(Conv2D(64, (3, 3), 激活=&#39;线性&#39;,填充=&#39;相同&#39;))
Fashion_model.add(LeakyReLU(alpha=0.1))
Fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding=&#39;相同&#39;))
Fashion_model.add(Conv2D(128, (3, 3), 激活=&#39;线性&#39;,填充=&#39;相同&#39;))
Fashion_model.add(LeakyReLU(alpha=0.1))
Fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding=&#39;相同&#39;))
Fashion_model.add(压平())
Fashion_model.add（密集（128，激活=&#39;线性&#39;））
Fashion_model.add(LeakyReLU(alpha=0.1))
Fashion_model.add（密集（num_classes，激活=&#39;softmax&#39;））


&lt;块引用&gt;
  文件“F:\anaconda\install\envs\anaconda35\lib\site-packages\keras\backend\tensorflow_backend.py”，第 6 行，位于
      从tensorflow.python.framework导入ops作为tf_ops
导入错误：无法导入名称“ops”

如何解决此错误？]]></description>
      <guid>https://stackoverflow.com/questions/51076277/cannot-import-name-ops-python</guid>
      <pubDate>Thu, 28 Jun 2018 06:42:49 GMT</pubDate>
    </item>
    </channel>
</rss>