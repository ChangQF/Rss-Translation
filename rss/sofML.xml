<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 08 Jun 2024 06:19:10 GMT</lastBuildDate>
    <item>
      <title>使用 BARTDecoder 和 cached_property 的 Nougat OCR 中的 ImportError 和 TypeError 问题</title>
      <link>https://stackoverflow.com/questions/78594832/importerror-and-typeerror-issues-in-nougat-ocr-with-bartdecoder-and-cached-prope</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78594832/importerror-and-typeerror-issues-in-nougat-ocr-with-bartdecoder-and-cached-prope</guid>
      <pubDate>Sat, 08 Jun 2024 05:43:48 GMT</pubDate>
    </item>
    <item>
      <title>尽管 Resnet50 的测试和训练准确率很高，但 F1 分数却很低</title>
      <link>https://stackoverflow.com/questions/78594365/resnet50-low-f1-score-despite-high-testing-and-training-accuracy</link>
      <description><![CDATA[我目前正在使用 Resnet50 在 Amazon Berkley Objects 数据集上进行图像分类，我一直面临 F1 分数低的问题，我确保训练和测试样本中的类别相等（总共约 50k 张图像），尽管它不会超过 10%（我知道图像显示的是第 7 个 epoche，但直到运行结束我都不会移动），有什么建议吗？
预处理步骤主要是过滤、数据增强、重新缩放、基本数据准备，数据分为 80 20。]]></description>
      <guid>https://stackoverflow.com/questions/78594365/resnet50-low-f1-score-despite-high-testing-and-training-accuracy</guid>
      <pubDate>Sat, 08 Jun 2024 00:03:13 GMT</pubDate>
    </item>
    <item>
      <title>成本函数增加，然后停止增长</title>
      <link>https://stackoverflow.com/questions/78594171/cost-function-increases-then-stops-growing</link>
      <description><![CDATA[我理解应用梯度下降时成本函数的曲折性质，但让我困扰的是成本一开始只有 300，最后却增加到 1600。
成本函数会在 300 和 4000 之间波动，最终达到 1600。我想我应该得到一个 300 或更低的数字。我尝试过改变学习率，但它仍然将我带到了 1600。我应该得到一个大约 300 的成本，而不是一个增长的成本。
数据：
square_feet = [1661.0, 871.0, 1108.0, 1453.0, 1506.0, 1100.0, 1266.0, 1514.0, 948.0, 1878.0, 1522.0, 931.0, 1475.0, 1177.0, 1844.0, 1469.0, 2155.0, 967.0, 1092.0]
prices = [1350.0, 489.0, 589.0, 539.0, 775.0, 575.0, 749.0, 795.0, 644.9, 590.0, 575.0, 699.0, 999.0, 775.0, 599.0, 599.0, 895.0, 550.0, 849.0]

这两个列表在原始代码中都是 Pandas 系列，但为了清晰起见，已在此处将其转换为列表。
主要：
# 添加起始权重和偏差
w_init = 5e-1 # 每 1 平方英尺的价格增加
b_init = 200 # 最便宜房屋的起始价格

# 梯度下降算法的迭代和学习率
iterations = 10000
alpha = 1.0e-6 # 很微妙，如果设置得太高会导致发散大

w_final，b_final，J_hist，p_hist = gradient_descent（
square_feet，prices，w_init，b_init，alpha，iterations）

print（f&#39;w：{w_final}，b：{b_final}，成本：{J_hist}，权重和偏差：{p_hist}&#39;）

函数：
# 成本函数用于确定实际值和预测值之间的累积误差
def cost_function（x，y，w，b）：

# 1）训练示例的数量
m = x.size
cost = 0

# 2）索引训练示例并考虑每个实例的成本
for i in range（m）：
y_hat = w * x[i] + b
cost += (y_hat-y[i])**2
cost /= 2 * m

# 3）返回总成本
return cost

# 计算梯度，即提高准确率的标量
def gradient_function(x, y, w, b):

m = x.size

# 成本函数关于权重和偏差的偏导数
dj_dw = 0
dj_db = 0

for i in range(m):
y_hat = w * x[i] + b
dj_dw_i = (y_hat - y[i]) * x[i]
dj_db_i = (y_hat - y[i])
dj_db += dj_db_i
dj_dw += dj_dw_i

dj_dw /= m
dj_db /= m

return dj_dw, dj_db

def gradient_descent(x, y, w_init, b_init, learning_rate, 
num_iters):

# 用于绘图
J_history = []
p_history = []
b = b_init
w = w_init

for i in range(num_iters):
dj_dw, dj_db = gradient_function(x, y, w, b) # 梯度

# 更新权重、偏差
w -= learning_rate * dj_dw
b -= learning_rate * dj_db

# 防止资源耗尽；无需存储类似成本
# 过去 100000 次迭代
if i &lt; 100000:
J_history.append(cost_function(x, y, w, b))
p_history.append([w,b])

return w, b, J_history, p_history

我对这个问题感到困惑。]]></description>
      <guid>https://stackoverflow.com/questions/78594171/cost-function-increases-then-stops-growing</guid>
      <pubDate>Fri, 07 Jun 2024 22:10:23 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 损失差异很大</title>
      <link>https://stackoverflow.com/questions/78594078/lstm-losses-diverge-quite-drastically</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78594078/lstm-losses-diverge-quite-drastically</guid>
      <pubDate>Fri, 07 Jun 2024 21:32:37 GMT</pubDate>
    </item>
    <item>
      <title>优化评分指标的多变量线性回归系数</title>
      <link>https://stackoverflow.com/questions/78593935/optimize-coefficients-for-multi-variable-linear-regression-of-scoring-metric</link>
      <description><![CDATA[我有一个电子商务网站，我尝试优化搜索结果，为用户提供最相关的结果。
为了提供最相关的搜索结果，我制定了一个评级指标。评级指标基于产品参数和每个参数的权重构建。
评级分数从 0-1 标准化
fn 函数返回 0-1 的分数
rating = b0*f0(X0) + b0*f1(X1) + bn*fn(Xn)

例如：rating = 0.4*f(seller_score) + 0.25*f(customers_stars) + 0.35*f(return_rate) = 0.683
当有人搜索“厨房椅子”时我根据评分对返回的结果进行排序。
SELECT 
* 
FROM 
db 
WHERE 
category = &quot;kitchen chair&quot; 
ORDER BY ratings DESC 
LIMIT 50

用户点击我记录的一些结果。在最佳情况下，用户将选择第一个结果之一，因为评分效果良好，并且用户可能得到了他想要的东西。但如果用户从列表中选择了第 24 个产品，那么我的评分可能没有得到优化。
我的目标是优化我的评分指标的系数，使其尽可能匹配用户实际点击的内容，这样他们就能首先获得最佳结果。
我有一系列 n 个搜索词，以及它们的结果和用户点击的内容。
我可以使用什么算法来找到 b0、b1、..bn 的最佳系数，这样点击的结果实际上就会位于顶部。现在，我希望找到适合所有搜索词的结果，而不是优化每个搜索词。]]></description>
      <guid>https://stackoverflow.com/questions/78593935/optimize-coefficients-for-multi-variable-linear-regression-of-scoring-metric</guid>
      <pubDate>Fri, 07 Jun 2024 20:43:38 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow Lite 尺寸错误？同样的模型，如果不使用 tensorflow lite，模型可以完美运行。为什么？</title>
      <link>https://stackoverflow.com/questions/78592790/tensorflow-lite-dimmension-error-with-the-same-model-if-not-using-tensorflow</link>
      <description><![CDATA[我将模型运行到 tflite 中。
然后收到错误：
ValueError：无法设置张量：维度不匹配。得到 56 但输入 0 的维度 0 应为 1
我将使用的输入是一个名为 user_place_array 的数组
这是我的代码：
model = tf.lite.Interpreter(model_path=&quot;/content/recommender_model.tflite&quot;)

input_details = model.get_input_details()
output_details = model.get_output_details()
input_shape = input_details[0][&#39;shape&#39;]

print(f&quot;user_place_array shape = &quot;,user_place_array.shape)
print(f&quot;model input = &quot;,input_shape)


返回值为：
user_place_array shape = (56, 2)
模型输入= [1 2]

然后，我检查 user_place_array，输出为：
array([[253, 0],
[253, 1],
[253, 2],
[253, 3],
# 直到 56


然后我使用 tflite 运行模型：
model.set_tensor(input_details[0][&#39;index&#39;], user_place_array)
这就是问题所在。我在这里得到了 tflite 错误
ValueError: 无法设置张量：维度不匹配。得到 56 但输入 0 的维度 0 应为 1。
当我不使用 tflite 时，模型运行完美。我使用：
model.predict(user_place_array).flatten()
该模型按预期提供建议。我不知道为什么它在 Tflite 上不起作用。
有人能帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/78592790/tensorflow-lite-dimmension-error-with-the-same-model-if-not-using-tensorflow</guid>
      <pubDate>Fri, 07 Jun 2024 15:22:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 提取多个实体 [关闭]</title>
      <link>https://stackoverflow.com/questions/78592545/extract-multiple-entities-with-llms</link>
      <description><![CDATA[我正在开展一个项目，该项目涉及从相同类型的 PDF 文档中提取实体（40-100 个），每个文档包含 8-20 页。这些文档包含表格、键值对和文本。我正在尝试寻找一种高效且经济的方法，以使用大型语言模型 (LLM) 实现实体提取的高精度和高速度。
我尝试过 RAG（检索增强生成），但它似乎很昂贵，因为它需要获取相关块并为每个实体生成 JSON 输出。因此，我正在寻找完成此任务的替代方法。
我考虑的一种方法是使用滑动窗口技术，其中我将连续的文档部分提供给 LLM 并采用提示工程来提取该特定部分中定义的实体。但是，这种方法引入了复杂性，例如处理某些实体的重复条目。
如果您能提供任何建议或最佳实践，以便以更高效、更经济的方式使用 LLM 从多页 PDF 中提取实体，我将不胜感激。
提前感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78592545/extract-multiple-entities-with-llms</guid>
      <pubDate>Fri, 07 Jun 2024 14:33:37 GMT</pubDate>
    </item>
    <item>
      <title>scikit 中的 gbrt_minimize 如何决定尝试多少个参数分割</title>
      <link>https://stackoverflow.com/questions/78592454/how-does-gbrt-minimize-from-scikit-decide-how-many-parameter-splits-to-try</link>
      <description><![CDATA[根据我对https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html和梯度提升决策树的理解，我假设对于 N 个参数，回归器会沿着每个参数选择一组分割，计算出应用此分割如何对数据进行分区，然后决定选择哪个分割以最大程度地减少损失（对于特定分位数）。
我的问题是，如果您的参数是实数，您如何决定在哪些参数值处进行分割？我原本希望找到某种参数来确定要进行多少次“等距”分割，但我只看到一个参数可以确定分割两侧所需的数据值数量以使其有效。这是否意味着它以某种方式反向运作？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78592454/how-does-gbrt-minimize-from-scikit-decide-how-many-parameter-splits-to-try</guid>
      <pubDate>Fri, 07 Jun 2024 14:14:02 GMT</pubDate>
    </item>
    <item>
      <title>如果训练数据集中的正样本多于负样本，XGBoost 的 scale_pos_weight 是否可以正确平衡正样本？</title>
      <link>https://stackoverflow.com/questions/78587301/does-xgboosts-scale-pos-weight-correctly-balance-the-positive-samples-if-the-tr</link>
      <description><![CDATA[经过研究，我意识到 scale_pos_weight 通常计算为训练数据中负样本数量与正样本数量的比率。我的数据集有 840 个负样本和 2650 个正样本，因此比率为 0.32。如果我的样本反过来，我相信 scale_pos_weight 会是一种更好的方法。
是否可以安全地假设，由于比率小于 1，它仍将正确平衡类别？特异性在我的研究中很重要，但我们的主要目标集中在召回率、精确度和 F1 分数上。此设置是否会通过最大程度地影响特异性而导致更多的假阳性？]]></description>
      <guid>https://stackoverflow.com/questions/78587301/does-xgboosts-scale-pos-weight-correctly-balance-the-positive-samples-if-the-tr</guid>
      <pubDate>Thu, 06 Jun 2024 14:27:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 logistf 包时，Firth 的模型在 R 中卡住了（出现未收敛警告，CPU 使用率高达 99%）</title>
      <link>https://stackoverflow.com/questions/78579401/firths-model-stuck-with-non-converge-warning-and-cpu-usage-99-in-r-using-the</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78579401/firths-model-stuck-with-non-converge-warning-and-cpu-usage-99-in-r-using-the</guid>
      <pubDate>Wed, 05 Jun 2024 07:34:40 GMT</pubDate>
    </item>
    <item>
      <title>大数据去重</title>
      <link>https://stackoverflow.com/questions/78565978/duplicate-removal-for-large-data</link>
      <description><![CDATA[我正在处理一个零售数据集，其中发票有 250000 个重复值。如何处理数据中的重复项我仍在尝试找出解决查询的方法。我考虑用均值中位数或众数来解决它]]></description>
      <guid>https://stackoverflow.com/questions/78565978/duplicate-removal-for-large-data</guid>
      <pubDate>Sun, 02 Jun 2024 09:23:42 GMT</pubDate>
    </item>
    <item>
      <title>通过单一数字指标训练 XGBoost</title>
      <link>https://stackoverflow.com/questions/78558863/training-xgboost-over-a-single-number-metric</link>
      <description><![CDATA[假设我正在用 Python（xgboost 版本 2.0.3）构建一个 XGBoost 模型（这里的回归或分类完全不重要）来预测股票市场时间序列分析中的目标变量。
例如，目标可能是：时间序列中的下一个值或二进制变量，如果下一个值高于前一个值，则设置为 1，否则设置为 0。
为了训练模型，是否可以使用回归问题中的 MSE 或分类问题中的“二元逻辑”。
训练后，可以根据测试集中模型的输出对策略进行回测并计算总体回报。
我的问题是：使用 xgboost scikit-learn 接口，是否可以根据用于回测策略的性能指标来训练模型？
例如：按照策略最大化训练集中的总体回报规则。
在xgboost库网站上，展示了如何使用自定义损失函数来训练模型：
def softprob_obj(labels: np.ndarray, predt: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:
rows = labels.shape[0]
classes = predt.shape[1]
grad = np.zeros((rows, classes), dtype=float)
hess = np.zeros((rows, classes), dtype=float)
eps = 1e-6
for r in range(predt.shape[0]):
target = labels[r]
p = softmax(predt[r, :])
for c in range(predt.shape[1]):
g = p[c] - 1.0 if c == target else p[c]
h = max((2.0 * p[c] * (1.0 - p[c])).item(), eps)
grad[r, c] = g
hess[r, c] = h

grad = grad.reshape((rows * classes, 1))
hess = hess.reshape((rows * classes, 1))
return grad, hess

clf = xgb.XGBClassifier(tree_method=&quot;hist&quot;, objective=softprob_obj)

目标函数需要计算梯度和 hessian。
假设函数定义如下：
def maximum_performance_metric(y_true: np.ndarray, y_pred: np.ndarray):
# 指标计算（例如：使用 y_pred 计算总体回报
overall_return = get_overall_return(y_pred, real_prices, ...) #overall_return 是浮点数
return grad, hess


是否可以根据总体回报计算梯度和 hessian，然后使用此自定义损失训练模型函数？
函数 maximize_performance_metric() 如何访问包含 real_prices 的变量（需要整体回报计算）？]]></description>
      <guid>https://stackoverflow.com/questions/78558863/training-xgboost-over-a-single-number-metric</guid>
      <pubDate>Fri, 31 May 2024 08:14:07 GMT</pubDate>
    </item>
    <item>
      <title>如何绘制多类别分类中所有类别的 SHAP 摘要图</title>
      <link>https://stackoverflow.com/questions/78396068/how-to-plot-shap-summary-plots-for-all-classes-in-multiclass-classification</link>
      <description><![CDATA[我正在使用 XGBoost 和 SHAP 来分析多类分类问题中的特征重要性，并需要帮助同时绘制所有类的 SHAP 摘要图。目前，我一次只能生成一个类的图。
SHAP 版本：0.45.0
Python 版本：3.10.12

这是我的代码：
import xgboost as xgb
import shap
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score

# 生成合成数据
X, y = make_classification(n_samples=500, n_features=20, n_informative=4, n_classes=6, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 训练 XGBoost 模型进行多类分类
model = xgb.XGBClassifier(objective=&quot;multi:softprob&quot;, random_state=42)
model.fit(X_train, y_train)

然后我尝试绘制形状值：
# 创建 SHAP TreeExplainer
explainer = shap.TreeExplainer(model)

# 计算测试集的 SHAP 值
shap_values = explainer.shap_values(X_test)

# 尝试绘制所有类的摘要
shap.summary_plot(shap_values, X_test, plot_type=&quot;bar&quot;)

我得到了这个交互图而是：

我在这篇文章的帮助下解决了这个问题：
shap.summary_plot(shap_values[:,:,0], X_test, plot_type=&quot;bar&quot;)

哪个给出类别 0 的正常条形图：

然后我可以对类别 1、2、3 等执行相同操作。
问题是，如何为所有类别制作摘要图？即，单个图显示特征对每个类别的贡献？]]></description>
      <guid>https://stackoverflow.com/questions/78396068/how-to-plot-shap-summary-plots-for-all-classes-in-multiclass-classification</guid>
      <pubDate>Sat, 27 Apr 2024 19:02:16 GMT</pubDate>
    </item>
    <item>
      <title>由于“tokenizer”不为“none”，因此不会使用参数“token_pattern”</title>
      <link>https://stackoverflow.com/questions/77149319/the-parameter-token-pattern-will-not-be-used-since-tokenizer-is-not-none</link>
      <description><![CDATA[我试图删除标点符号和空格（包括换行符）并过滤仅由字母组成的标记，然后返回标记文本。
我首先定义函数
 return [t.text for t in nlp(doc) if \
not t.is_punct and \
not t.is_space and \
t.is_alpha]

然后我进行矢量化
vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)
train_feature_vects = vectorizer.fit_transform(train_data)

终端卡住了，并说参数“token_pattern”不会被使用，因为“tokenizer”不是“none”。
我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/77149319/the-parameter-token-pattern-will-not-be-used-since-tokenizer-is-not-none</guid>
      <pubDate>Thu, 21 Sep 2023 10:17:24 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 模型拟合与 train_on_batch 之间的区别</title>
      <link>https://stackoverflow.com/questions/62629997/difference-between-tensorflow-model-fit-and-train-on-batch</link>
      <description><![CDATA[我正在构建一个 vanilla DQN 模型来玩 OpenAI gym Cartpole 游戏。
但是，在训练步骤中，我将状态作为输入，将目标 Q 值作为标签，如果我使用 model.fit(x=states, y=target_q)，它会正常工作，并且代理最终可以很好地玩游戏，但如果我使用 model.train_on_batch(x=states, y=target_q)，损失不会减少，并且模型在游戏中的表现不会比随机策略更好。
我想知道 fit 和 train_on_batch 之间有什么区别？据我了解，fit 在后台调用批处理大小为 32 的 train_on_batch，这应该没有什么区别，因为将批处理大小指定为等于我输入的实际数据大小并没有区别。
如果需要更多上下文信息来回答这个问题，完整的代码在这里：https://github.com/ultronify/cartpole-tf]]></description>
      <guid>https://stackoverflow.com/questions/62629997/difference-between-tensorflow-model-fit-and-train-on-batch</guid>
      <pubDate>Mon, 29 Jun 2020 01:30:09 GMT</pubDate>
    </item>
    </channel>
</rss>