<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 14 Oct 2024 21:15:33 GMT</lastBuildDate>
    <item>
      <title>高斯过程二元分类：为什么 GPy 的方差比 scikit-learn 小得多？</title>
      <link>https://stackoverflow.com/questions/79086293/gaussian-process-binary-classification-why-is-the-variance-with-gpy-much-smalle</link>
      <description><![CDATA[我正在学习使用高斯过程进行二元分类，并且正在将 GPy 与 scikit-learn 进行比较，解决一个受 Martin Krasser 的博客文章启发的玩具一维问题。两种实现（GPy 和 scikit-learn）似乎都使用带有 RBF 内核的类似设置。优化内核超参数后，长度尺度相似，但方差相差很大。GPy 内核方差似乎太小了。
我如何修改我的 GPy 实现并获得与 scikit-learn 类似的结果？我怀疑这与每个算法的内部实现有关，但我不知道是什么导致了这种巨大的差异。下面我将进一步解释为什么我认为我的 GPy 实现需要修复。
实现细节：Python 3.9 搭配 GPy 1.13.2 和 scikit-learn 1.5.1。可重现的示例：
import numpy as np
from scipy.stats import bernoulli
from scipy.special import expit as sigmoid

##############################
# 第 1 部分：玩具数据集创建
#################################

np.random.seed(0)
X = np.arange(0, 5, 0.05).reshape(-1, 1)
X_test = np.arange(-2, 7, 0.1).reshape(-1, 1)

a = np.sin(X * np.pi * 0.5) * 2 # 潜在函数
t = bernoulli.rvs(sigmoid(a)) # Bernoulli 训练数据（0 和1s)

#####################################
# 第 2 部分：scikit-learn 实现
#####################################

从 sklearn.gaussian_process 导入 GaussianProcessClassifier
从 sklearn.gaussian_process.kernels 导入 ConstantKernel，RBF

rbf = ConstantKernel(1.0, constant_value_bounds=(1e-3, 10)) \
* RBF(length_scale=1.0, length_scale_bounds=(1e-3, 10))
gpc = GaussianProcessClassifier(
kernel=rbf,
optimizer=&#39;fmin_l_bfgs_b&#39;,
n_restarts_optimizer=10)

gpc.fit(X_scaled, t.ravel())

print(gpc.kernel_)
# 1.5**2 * RBF(length_scale=0.858)

############################
# 第 3 部分：GPy 实现
############################

导入 GPy

kern = GPy.kern.RBF(
input_dim=1,
variance=1.,
lengthscale=1.)
kern.lengthscale.unconstrain()
kern.variance.unconstrain()
kern.lengthscale.constrain_bounded(1e-3, 10)
kern.variance.constrain_bounded(1e-3, 10)

m = GPy.core.GP(
X=X,Y=t, kernel=kern, 
inference_method=GPy.inference.latent_function_inference.laplace.Laplace(), 
可能性=GPy.likelihoods.Bernoulli())

m.optimize_restarts(
num_restarts=10, optimizer=&#39;lbfgs&#39;,
verbose=True, robust=True)

print(m.kern)
# rbf。| 值 | 约束 | 先验
# 方差 | 0.8067562453940487 | 0.001,10.0 | 
# lengthscale | 0.8365668826459536 | 0.001,10.0 |

lenghtscale 值大致相似 (0.858 vs 0.836)，但方差值非常不同 (scikit-learn 为 1.5**2 = 2.25，而 GPy 仅为 0.806)。
我认为我的 GPy 实现需要调整的原因是，即使有 +/- 2 个标准偏差界限，真正的潜在函数 (参见上面代码第 1 部分中的“a”) 也与预测函数不紧密匹配。另一方面，scikit-learn 实现与之相当匹配（可以使用 scikit-learn 检索潜在函数平均值和标准差如此处所示）。

 左：两个模型的预测概率相似（这是有道理的，因为它们共享相似的长度尺度值）。右：GPy 的预测潜在函数与真实潜在函数的拟合度不如 scikit-learn 模型。 
到目前为止，我尝试过的方法，结果没有显著变化：

输入特征 (X) 归一化
使用 GPy.inference.latent_function_inference.expectation_propagation.EP() 作为 GPy 推理方法，而不是拉普拉斯方法
按照此处的建议，将 WhiteKernel 组件添加到 scikit-learn 实现中&gt;
]]></description>
      <guid>https://stackoverflow.com/questions/79086293/gaussian-process-binary-classification-why-is-the-variance-with-gpy-much-smalle</guid>
      <pubDate>Mon, 14 Oct 2024 13:10:16 GMT</pubDate>
    </item>
    <item>
      <title>我在 google colab 中工作，我的单元格中有访问地球数据的代码，它永远运行[关闭]</title>
      <link>https://stackoverflow.com/questions/79085474/im-working-in-google-colab-and-my-cell-that-has-code-to-access-data-from-earth</link>
      <description><![CDATA[我是一个相当新的程序员，并试图从地球数据中访问数据。这是文档https://disc.gsfc.nasa.gov/information/howto?keywords=python&amp;title=How%20to%20Access%20GES%20DISC%20Data%20Using%20Python。这是我的代码。
我尝试关闭 ds，创建一个预处理函数。我也尝试使用 colab ai 和 chatgpt 来帮助修复它，但没有任何效果。在此处输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/79085474/im-working-in-google-colab-and-my-cell-that-has-code-to-access-data-from-earth</guid>
      <pubDate>Mon, 14 Oct 2024 09:04:35 GMT</pubDate>
    </item>
    <item>
      <title>summary() 函数在 cnn 中不起作用（ValueError：不支持未定义的形状。）</title>
      <link>https://stackoverflow.com/questions/79084869/summary-function-not-working-in-cnn-valueerror-undefined-shapes-are-not-supp</link>
      <description><![CDATA[我正在尝试创建一个分类网络，用于识别来自 cifar10 数据集的图片。
当我尝试使用 summary() 函数时，我总是收到此错误。
ValueError Traceback (most recent call last)
Cell In[267], line 4
1 #base_model.summary()
2 #top_model.summary()
3 #print(base_model.output_shape)
----&gt; 4 model2.summary()

文件 c:\Users\noahc\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 c:\Users\noahc\anaconda3\Lib\site-packages\optree\ops.py:747，在 tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests) 中
745 leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
746 flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--&gt; 747 return treespec.unflatten(map(func, *flat_args))

ValueError：不支持未定义的形状。

代码如下...
import tensorflow as tf
from keras.applications import VGG16

base_model = VGG16(weights=&#39;imagenet&#39;, include_top=False, input_shape=(32, 32, 3))

top_model = tf.keras.Sequential([
layer.Flatten(input_shape=base_model.output_shape[1:]),
layer.Dense(10,activation=&#39;softmax&#39;)
])

for layer in base_model.layers[:10]:
layer.trainable = False

model2 = tf.keras.models.Sequential([
base_model,
top_model
])

model2.summary() # 出现错误这里

我已经为基础模型和顶层模型做了总结，效果很好。但是当我测试 model2 时，出现了错误。不知道为什么。不确定“未定义”形状是什么意思。不知道还能尝试什么。当我只取 vgg16 的前 11 或 15 层时，总结就起作用了。我听说这可能是 python 版本本身的问题，但我不知道……]]></description>
      <guid>https://stackoverflow.com/questions/79084869/summary-function-not-working-in-cnn-valueerror-undefined-shapes-are-not-supp</guid>
      <pubDate>Mon, 14 Oct 2024 05:50:22 GMT</pubDate>
    </item>
    <item>
      <title>来自 Google Drive 的 Langchain Dir 加载器抛出错误</title>
      <link>https://stackoverflow.com/questions/79084452/langchain-dir-loader-from-google-drive-throwing-errors</link>
      <description><![CDATA[我正在尝试从 google myDrive 加载 html 文件。我正在 colab 上尝试此操作。我能够加载驱动器，当我使用 DirectoryLoader 时，它会看到文件名，但它会引发以下错误。顺便说一句，我授予了任何人评论的权限（即使 colab 通过 google 帐户请求加载 myDrive 的权限）。错误很神秘，我找不到其他遇到过这种情况的人（当我在 google 上搜索时）。如有任何帮助，敬请谅解
os.chdir(&#39;/content/drive/MyDrive&#39;)

loader = DirectoryLoader(path = os.getcwd(), glob=&quot;datasets/opp/*.html&quot;, show_progress=True, use_multithreading=True, loader_cls = UnstructuredHTMLLoader)
documents = loader.load()

错误：
错误：langchain_community.document_loaders.directory：加载文件 /content/drive/MyDrive/datasets/opp/745_eatchicken.com.html 时出错
63%|██████▎ | 72/115 [00:00&lt;00:00, 336.85it/s]错误：langchain_community.document_loaders.directory：加载文件 /content/drive/MyDrive/datasets/opp/503_buffalowildwings.com.html 时出错
错误：langchain_community.document_loaders.directory：加载文件 /content/drive/MyDrive/datasets/opp/807_lodgemfg.com.html 时出错
------------------------------------------------------------------------------------------
ModuleNotFoundError 回溯（最近一次调用最后一次）
/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/unstructured.py in __init__(self, mode, post_processors, **unstructured_kwargs)
58 尝试：
---&gt; 59 import unstructured # noqa:F401
60 except ImportError:

ModuleNotFoundError: 没有名为“unstructured”的模块

在处理上述异常期间，发生了另一个异常：

ImportError Traceback（最近一次调用最后一次）
12 帧
/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/unstructured.py in __init__(self, mode, post_processors, **unstructured_kwargs)
59 import unstructured # noqa:F401
60 except ImportError:
---&gt; 61 raise ImportError(
62 &quot;未找到非结构化包，请使用&quot;
63 &quot;`pip install unstructured`&quot;

ImportError：未找到非结构化包，请使用`pip install unstructured`安装

-----------------------------------------------------------------------
注意：如果由于缺少包而导致导入失败，您可以
使用 !pip 或 !apt 手动安装依赖项。

要查看安装一些常见依赖项的示例，请单击下面的
&quot;打开示例&quot; 按钮。
----------------------------------------------------------------------------------------
]]></description>
      <guid>https://stackoverflow.com/questions/79084452/langchain-dir-loader-from-google-drive-throwing-errors</guid>
      <pubDate>Mon, 14 Oct 2024 00:41:50 GMT</pubDate>
    </item>
    <item>
      <title>Gymnasium 自定义环境“太多值无法解压”错误</title>
      <link>https://stackoverflow.com/questions/79084313/gymnasium-custom-environment-too-many-values-to-unpack-error</link>
      <description><![CDATA[我正在尝试使用具有体育馆和稳定基线的自定义群体聚集环境。我有一个自定义策略和训练循环。
我的行动和观察空间如下：
min_action = np.array([-5, -5] * len(self.agents), dtype=np.float32)
max_action = np.array([5, 5] * len(self.agents), dtype=np.float32)

min_obs = np.array([-np.inf, -np.inf, -2.5, -2.5] * len(self.agents), dtype=np.float32)
max_obs = np.array([np.inf, np.inf, 2.5, 2.5] * len(self.agents), dtype=np.float32)

训练代码：
import numpy as np
import torch as th
from Parameters import *
from stable_baselines3 import PPO
from main import FlockingEnv, CustomMultiAgentPolicy
from Callbacks import TQDMProgressCallback, LossCallback
import os
from stable_baselines3.common.vec_env import DummyVecEnv

if os.path.exists(Results[&quot;Rewards&quot;]):
os.remove(Results[&quot;Rewards&quot;])
print(f&quot;File {Results[&#39;Rewards&#39;]} has been removed.&quot;)

if os.path.exists(&quot;training_rewards.json&quot;):
os.remove(&quot;training_rewards.json&quot;)
print(f&quot;文件 training_rewards 已被删除。&quot;) 

def seed_everything(seed):
np.random.seed(seed)
os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
th.manual_seed(seed)
th.cuda.manual_seed(seed)
th.backends.cudnn.deterministic = True
env.seed(seed)
env.action_space.seed(seed)

loss_callback = LossCallback()
env = DummyVecEnv([lambda: FlockingEnv()])

seed_everything(SimulationVariables[&quot;Seed&quot;])

# # 模型训练
model = PPO(CustomMultiAgentPolicy, env, tensorboard_log=&quot;./ppo_Agents_tensorboard/&quot;, verbose=1)
model.set_random_seed(SimulationVariables[&quot;ModelSeed&quot;])
progress_callback = TQDMProgressCallback(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;])
# 训练模型
model.learn(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;], callback=[progress_callback, loss_callback])

错误：
使用 cuda 设备
回溯（最近一次调用最后一次）：
文件 &quot;D:\Thesis_\FlockingFinal\MultiAgentFlocking\Training.py&quot;，行45，在&lt;module&gt;中
model.learn(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;], callback=[progress_callback, loss_callback]) 
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\ppo\ppo.py&quot;，第 315 行，在 learn 中
return super().learn(
^^^^^^^^^^^^^^^
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py&quot;，第 287 行，在 learn 中
total_timesteps, callback = self._setup_learn(
^^^^^^^^^^^^^^^^^^^
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\common\base_class.py&quot;，第 423 行，在 _setup_learn
self._last_obs = self.env.reset() # 类型：ignore[assignment]
^^^^^^^^^^^^^^^^^
文件 &quot;C:\Python312\Lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py&quot;，第 77 行，在 reset
obs 中，self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError：太需要解压的值很多（预计为 2 个）

我也在 gym 中使用了类似的种子函数，但没有出现错误，我以为是它导致了错误，但即使我不使用它，错误也不会消失。]]></description>
      <guid>https://stackoverflow.com/questions/79084313/gymnasium-custom-environment-too-many-values-to-unpack-error</guid>
      <pubDate>Sun, 13 Oct 2024 22:45:48 GMT</pubDate>
    </item>
    <item>
      <title>DETR（检测变压器）模型不适用于我的数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/79084295/detr-detection-transformer-model-is-not-working-for-my-dataset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79084295/detr-detection-transformer-model-is-not-working-for-my-dataset</guid>
      <pubDate>Sun, 13 Oct 2024 22:27:50 GMT</pubDate>
    </item>
    <item>
      <title>我是否应该将遗忘集拆分为训练/验证/测试，以使用 CNN 和 Celeba 数据集进行反学习实验？[关闭]</title>
      <link>https://stackoverflow.com/questions/79084014/should-i-split-the-forget-set-into-train-validate-test-for-an-unlearning-experim</link>
      <description><![CDATA[我正在 CelebA 数据集上训练一个简单的 CNN，并尝试通过创建一个遗忘集（来自 CelebA 的随机部分图像）来进行反学习实验。
CNN 已在完整训练集上进行训练，并进行了单独的验证和测试拆分以进行评估。
我是否应该将遗忘集拆分为训练/测试/验证子集，如果是，为什么？（遗忘集约占训练集的 10% - 其余 90% 为保留集）
在我当前的反学习实验中，我只对遗忘集进行反学习，没有进行任何重新训练。因此，我在此过程中仅使用完整模型和遗忘集。
我尝试使用我相应拆分的遗忘集，但测试变得棘手，并且可能只有一个遗忘集感觉合乎逻辑（因为遗忘集中不是同一个人，它是一个混合包）
因此，拆分为测试和验证意味着它是不同面孔的不同包。]]></description>
      <guid>https://stackoverflow.com/questions/79084014/should-i-split-the-forget-set-into-train-validate-test-for-an-unlearning-experim</guid>
      <pubDate>Sun, 13 Oct 2024 19:44:40 GMT</pubDate>
    </item>
    <item>
      <title>为什么尽管使用了 RLZoo3 的最佳超参数，我的 SB3 DQN 代理仍无法学习 CartPole-v1？</title>
      <link>https://stackoverflow.com/questions/79083972/why-is-my-sb3-dqn-agent-unable-to-learn-cartpole-v1-despite-using-optimal-hyperp</link>
      <description><![CDATA[我从 RLZoo3 获得了用于训练 CartPole-v1 的最佳超参数。我创建了一个最小示例来展示我的 CartPole 代理的性能。根据官方文档，代理应获得 500 分，才能成功完成一集。不幸的是，分数没有超过 300。
这是我的代码 -
import gymnasium as gym
import numpy as np
import torch
from stable_baselines3 import DQN
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback
from torch.utils.tensorboard import SummaryWriter
import os

def set_seed(seed):
np.random.seed(seed)
torch.manual_seed(seed)
torch.backends.cudnn.deterministic = True

class TensorBoardCallback(BaseCallback):
def __init__(self, log_dir):
super().__init__()
self.writer = SummaryWriter(log_dir=log_dir)
self.episode_rewards = []
self.current_episode_reward = 0

def _on_step(self):
self.current_episode_reward += self.locals[&#39;rewards&#39;][0]

如果 self.locals[&#39;dones&#39;][0]:
self.episode_rewards.append(self.current_episode_reward)
self.writer.add_scalar(&#39;train/episode_reward&#39;, self.current_episode_reward, self.num_timesteps)
self.current_episode_reward = 0

如果 len(self.episode_rewards) &gt;= 100:
avg_reward = sum(self.episode_rewards[-100:]) / 100
self.writer.add_scalar(&#39;train/average_reward&#39;, avg_reward, self.num_timesteps)

return True

def on_training_end(self):
self.writer.close()

# 设置日志目录
log_dir = &quot;tensorboard_logs&quot;
os.makedirs(log_dir, exist_ok=True)

# 设置可重复性的种子
seed = 42
set_seed(seed)

# 创建环境
env = gym.make(&quot;CartPole-v1&quot;)
env = DummyVecEnv([lambda: env])

# 使用来自 rlzoo3 的超参数创建模型
model = DQN(
policy=&quot;MlpPolicy&quot;,
env=env,
learning_rate=2.3e-3,
batch_size=64,
buffer_size=100000,
learning_starts=1000,
gamma=0.99,
target_update_interval=10,
train_freq=256,
gradient_steps=128,
exploration_fraction=0.16,
exploration_final_eps=0.04,
policy_kwargs=dict(net_arch=[256, 256]),
verbose=1,
tensorboard_log=log_dir,
seed=seed
)

# 创建回调
tb_callback = TensorBoardCallback(log_dir)

# 训练模型
total_timesteps = 50000
model.learn(total_timesteps=total_timesteps, callback=tb_callback)

print(&quot;训练完成。您可以使用 TensorBoard 查看结果。&quot;)
print(f&quot;在您的终端中运行以下命令：tensorboard --logdir {log_dir}&quot;)

env.close()

这是最终结果 -
]]></description>
      <guid>https://stackoverflow.com/questions/79083972/why-is-my-sb3-dqn-agent-unable-to-learn-cartpole-v1-despite-using-optimal-hyperp</guid>
      <pubDate>Sun, 13 Oct 2024 19:23:17 GMT</pubDate>
    </item>
    <item>
      <title>Python 在分配 numpy 数组时抛出 MemoryError</title>
      <link>https://stackoverflow.com/questions/79082341/python-throws-memoryerror-while-allocating-a-numpy-array</link>
      <description><![CDATA[我正在以类似 sklearn 的方式拟合 QML 算法：
num_features = X_train.shape[1]

feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)
ansatz = RealAmplitudes(num_qubits=num_features, reps=3)
optimizer = COBYLA(maxiter=100)

vqc = VQC(
feature_map=feature_map,
ansatz=ansatz,
optimizer=optimizer
)

vqc.fit(X_train, y_train.to_numpy())

在执行 vqc.fit(X_train, y_train.to_numpy()) 行时，解释器抛出异常：
MemoryError：无法为形状为 (1048576,) 且数据类型为 &lt;U420  的数组分配 1.64 GiB
问题是，我正在使用的机器有 120 GB 的 RAM，我不明白它为什么不能为数组分配 1.64 GB。你能帮我解决这个问题吗？有什么方法可以突破这个 RAM 限制吗？
我不确定，但我想试试这个 https://stackoverflow.com/a/58686879。然而，我认为这行不通，也许你有更多的想法。]]></description>
      <guid>https://stackoverflow.com/questions/79082341/python-throws-memoryerror-while-allocating-a-numpy-array</guid>
      <pubDate>Sun, 13 Oct 2024 03:26:07 GMT</pubDate>
    </item>
    <item>
      <title>为什么飞机没有显示在 matplotlib 图中</title>
      <link>https://stackoverflow.com/questions/79081747/why-the-plane-doesnt-show-in-matplotlib-plot</link>
      <description><![CDATA[我正在对具有 13 个特征的波士顿房屋数据集实施 SLP。我为 X 选择“rm”和“zn”，为目标 Y 选择“medv”。我还从头实施了一个感知器类。在这个类中，我有一个名为 plot_losses 的函数，它在一个窗口中绘制预测线（2d）和损失，还绘制 3d 图的预测平面，这就是问题所在，即 3d 部分。
平面未显示在 3d 散点图上。
感知器类实现：
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter

感知器类：
def __init__(self, input_size, lr, epochs):
self.w = np.zeros(input_size)
self.b = 0
self.lr = lr
self.epochs = epochs
self.losses = []

def fit(self, X_train, Y_train):
for _ in range(self.epochs):
for x_i in range(X_train.shape[0]):
x = X_train[x_i]
y = Y_train[x_i]
y_pred = np.dot(x, self.w) + self.b
error = y - y_pred

self.w = self.w + (error * x * self.lr)
self.b = self.b + (error * self.lr)

loss = np.mean(np.abs(error))
self.losses.append(loss)

def predict(self, X_test):
return np.dot(X_test, self.w) + self.b

def plot_losses(self, X_train, Y_train, ax1_title, ax2_title, plot_3d=False, plot_3d_title=&#39;3D Plot&#39;):
for _ in range(self.epochs):
for x_i in range(X_train.shape[0]):
x = X_train[x_i]
y = Y_train[x_i]
Y_pred = np.dot(x, self.w) + self.b

if plot_3d:
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111,projection=&#39;3d&#39;)

X_feature1 = X_train[:, 0]
X_feature2 = X_train[:, 1]

ax.scatter(X_feature1, X_feature2, Y_train, color=&#39;blue&#39;, label=&#39;True Values&#39;)

X1_grid, X2_grid = np.meshgrid(
np.linspace(X_feature1.min(), X_feature1.max(), 20),
np.linspace(X_feature2.min(), X_feature2.max(), 20)
)

Z_pred = self.w[0] * X1_grid + self.w[1] * X2_grid + self.b

ax.plot_surface(X1_grid, X2_grid, Z_pred, color=&#39;red&#39;, alpha=0.5)
ax.set_xlabel(&quot;特征 &#39;rm&#39;&quot;)
ax.set_ylabel(&quot;特征 &#39;zn&#39;&quot;)
ax.set_zlabel(&quot;目标 &#39;medv​​&#39;&quot;)
ax.set_title(plot_3d_title)
ax.legend()
plt.show()
else:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 5))

ax1.scatter(X_train[:, 0], Y_train, color=&#39;blue&#39;, label=&#39;真值&#39;)
ax1.plot(X_train[:, 0], Y_pred, color=&#39;red&#39;, label=&#39;预测值Line&#39;)
ax1.set_title(ax1_title)
ax1.legend()

ax2.plot(self.losses)
ax2.set_title(ax2_title)
ax2.set_xlabel(&quot;Epochs&quot;)
ax2.set_ylabel(&quot;均方误差 (MSE)&quot;)

plt.tight_layout()
plt.show()

波士顿房屋数据集的线性回归：
%matplotlib qt
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
from perceptron import Perceptron
df_boston = pd.read_csv(&#39;input/BostonHousing.csv&#39;)
X = df_boston[[&#39;rm&#39;,&#39;zn&#39;]].values
Y = df_boston[&#39;medv​​&#39;].values
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=.2)

slp = Perceptron(2, .01, 100)
slp.fit(X_train, Y_train) 
slp.plot_losses(X_train,Y_train, &#39;员工工资和经验感知器&#39;, &#39;损失值&#39;, plot_3d=True, plot_3d_title=&#39;波士顿住房感知器&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79081747/why-the-plane-doesnt-show-in-matplotlib-plot</guid>
      <pubDate>Sat, 12 Oct 2024 19:31:21 GMT</pubDate>
    </item>
    <item>
      <title>只有输入张量可以作为位置参数传递</title>
      <link>https://stackoverflow.com/questions/78360982/only-input-tensors-may-be-passed-as-positional-arguments</link>
      <description><![CDATA[从 PIL 导入图像
导入 matplotlib.pyplot 作为 plt
导入 argparse
导入 pickle
导入 numpy
作为 np
从 tensorflow 导入 keras
从 keras.applications.xception 导入 Xception
从 keras.preprocessing.sequence 导入 pad_sequences
从 tensorflow.keras.preprocessing.text 导入 Tokenizer
导入 tensorflow 作为 tf

# 使用 Lambda 定义自定义层（不带 name 参数）
class NotEqual(tf.keras.layers.Layer):
def __init__(self, name=None):
super(NotEqual, self).__init__(name=name)

def call(self, x, y): # 使用关键字参数“x”和“y”
return tf.math.not_equal(x, y)

# 定义用于提取特征、生成描述的函数，和其他必要的实用程序
def extract_features(filename, model):
try:
image = Image.open(filename)
except:
print(&quot;ERROR: 无法打开图像！请确保图像路径和扩展名正确&quot;)
image = image.resize((299, 299))
image = np.array(image)
if image.shape[2] == 4:
image = image[..., :3]
image = np.expand_dims(image, axis=0)
image = image / 127.5
image = image - 1.0
feature = model.predict(image)
return feature

def word_for_id(integer, tokenizer):
for word, index in tokenizer.word_index.items():
if index == integer:
return word
return None

def generate_desc(model, tokenizer, photo, max_length):
in_text = &#39;start&#39;
for i in range(max_length):
sequence = tokenizer.texts_to_sequences([in_text])[0]
sequence = pad_sequences([sequence], maxlen=max_length)
pred = model.predict({&#39;image_input&#39;: photo, &#39;text_input&#39;:sequence}) # 将输入作为字典传递
pred = np.argmax(pred)
word = word_for_id(pred, tokenizer)
if word is None:
break
in_text += &#39; &#39; + word
if word == &#39;end&#39;:
break
return in_text

# 解析参数
ap = argparse.ArgumentParser()
ap.add_argument(&#39;-i&#39;, &#39;--image&#39;, required=True, help=&quot;Image Path&quot;)
args = vars(ap.parse_args())
img_path = args[&#39;image&#39;]

# 加载tokenizer
tokenizer = pickle.load(open(&quot;tokenizer.p&quot;, &quot;rb&quot;))

# 定义模型的路径
model_path = &#39;models/model_9.h5&#39;
# 使用自定义对象（包括 NotEqual 层）加载模型
使用 keras.utils.custom_object_scope({&#39;NotEqual&#39;: NotEqual}):
model = tf.keras.models.load_model(model_path)

我尝试以各种方式运行此代码，但出现错误：
 model = tf.keras.models.load_model(model_path)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\saving\saving_api.py”，第 183 行，位于 load_model
return legacy_h5_format.load_model_from_hdf5(filepath)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 “C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\legacy\saving\legacy_h5_format.py”，第 133 行，位于 load_model_from_hdf5
model = saving_utils.model_from_config(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\legacy\saving\saving_utils.py”，第 85 行，位于 model_from_config
return serialization.deserialize_keras_object(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\legacy\saving\serialization.py”，第 495 行，位于 deserialize_keras_object
deserialized_obj = cls.from_config(
^^^^^^^^^^^^^^^^^
文件“C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\model.py”，第 528 行，位于 from_config 中
return functional_from_config(
^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\function.py”，第 528 行，位于 functional_from_config 中
process_node(layer, node_data)
文件&quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\function.py&quot;，第 475 行，位于 process_node
layer(*args, **kwargs)
文件 &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;，第 122 行，位于 error_handler
raise e.with_traceback(filtered_tb) from None
文件 &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\layers\layer.py&quot;，第 721 行，位于 __call__
raise ValueError(
ValueError: 只能将输入张量作为位置参数传递。以下参数值应作为关键字参数传递：0（类型为 &lt;class &#39;int&#39;&gt;）
]]></description>
      <guid>https://stackoverflow.com/questions/78360982/only-input-tensors-may-be-passed-as-positional-arguments</guid>
      <pubDate>Sun, 21 Apr 2024 09:15:40 GMT</pubDate>
    </item>
    <item>
      <title>错误：具有 ML Compute 加速功能的 TensorFlow 仅适用于 macOS 11.0 及更高版本</title>
      <link>https://stackoverflow.com/questions/74563984/error-tensorflow-with-ml-compute-acceleration-is-only-available-on-macos-11-0-a</link>
      <description><![CDATA[下载 Tensor Flow mac 发布包 (https://github.com/apple/tensorflow_macos/releases)，打开 tensorflow_macos-0.1alpha3.tar.gz 包，运行“install_venv.sh”脚本，出现以下错误 ERROR: 带有 ML Compute 加速的 TensorFlow 仅在 macOS 11.0 及更高版本上可用。
我在 conda 环境中使用 python 3.8 运行它（我尝试过 3.7-3.11，结果相同）。
我会在 apple github repo 上发布，但它已被公开存档，所以假设它不再可用？
我尝试过在 conda 环境中使用 python 3.8 运行它（我尝试过 3.7-3.11，结果相同）。
我预计安装会成功。我尝试安装 pip install tensorflow（即没有特定于 mac），pyCharm 看到了 lib tensorflow，但我得到
Apple M1：进程完成，退出代码为 132（被信号 4 中断：sigill，解决方案是从上面安装包：https://github.com/apple/tensorflow_macos/issues/270]]></description>
      <guid>https://stackoverflow.com/questions/74563984/error-tensorflow-with-ml-compute-acceleration-is-only-available-on-macos-11-0-a</guid>
      <pubDate>Thu, 24 Nov 2022 16:46:17 GMT</pubDate>
    </item>
    <item>
      <title>nn.Parameter 上的 Torch 操作</title>
      <link>https://stackoverflow.com/questions/71490306/torch-operation-on-nn-parameter</link>
      <description><![CDATA[我有一个参数列表，我想对列表中的所有元素求和
import torch
from torch import nn

a = nn.Parameter(torch.rand(1))
b = nn.Parameter(torch.rand(1))

my_list = [a, b]
torch.sum(*my_list)

我收到错误

回溯（最近一次调用最后一次）：
文件“&lt;input&gt;&gt;”，第 8 行，在&lt;module&gt;
TypeError: sum() 收到的参数组合无效 - 得到 (Parameter, Parameter)，但预期为以下之一：
* (Tensor 输入，*，torch.dtype dtype)
* (Tensor 输入，整数元组 dim，bool keepdim，*，torch.dtype dtype，Tensor out)
* (Tensor 输入，名称元组 dim，bool keepdim，*，torch.dtype dtype，Tensor out)


我想知道是否有办法对 Parameter 执行 torch.sum 之类的操作？]]></description>
      <guid>https://stackoverflow.com/questions/71490306/torch-operation-on-nn-parameter</guid>
      <pubDate>Tue, 15 Mar 2022 23:52:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么自然语言处理中的 Transformer 需要一堆编码器？</title>
      <link>https://stackoverflow.com/questions/59384146/why-do-transformers-in-natural-language-processing-need-a-stack-of-encoders</link>
      <description><![CDATA[我正在关注这篇关于 transformers 的博客
http://jalammar.github.io/illustrated-transformer/
我唯一不明白的是为什么需要一堆编码器或解码器。我知道多头注意力层捕获了问题的不同表示空间。我不明白为什么需要一堆编码器和解码器。一个编码器/解码器层不行吗？]]></description>
      <guid>https://stackoverflow.com/questions/59384146/why-do-transformers-in-natural-language-processing-need-a-stack-of-encoders</guid>
      <pubDate>Wed, 18 Dec 2019 00:57:26 GMT</pubDate>
    </item>
    <item>
      <title>线性回归爆炸的梯度下降</title>
      <link>https://stackoverflow.com/questions/50219054/gradient-descent-for-linear-regression-exploding</link>
      <description><![CDATA[我正在尝试使用此资源实现线性回归的梯度下降：https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/
我的问题是，我的权重正在爆炸式增长（呈指数增长），并且本质上与预期相反。
首先，我创建了一个数据集：
def y(x, a):
return 2*x + a*np.random.random_sample(len(x)) - a/2

x = np.arange(20)
y_true = y(x,10)

看起来像这样：

要优化的线性函数：
def y_predict(x, m, b):
return m*x + b

因此，对于一些随机选择的参数，结果如下：
m0 = 1
b0 = 1

a = y_predict(x, m0, b0)

plt.scatter(x, y_true)
plt.plot(x, a)
plt.show()


现在成本看起来是这样的：
cost = (1/2)* np.sum((y_true - a) ** 2)

成本相对于预测 (dc_da) 的偏导数：
dc_da = (a - y_true) # 仍然是一个向量

成本相对于斜率参数 (dc_dm) 的偏导数：
dc_dm = dc_da.dot(x) # 现在是一个常数

成本相对于 y 截距参数 (dc_db) 的偏导数：
dc_db = np.sum(dc_da) # 也是一个常数

最后是梯度下降的实现：
iterations = 10

m0 = 1

b0 = 1

learning_rate = 0.1

N = len(x)

for i in range(iterations):

a = y_predict(x, m0, b0)

cost = (1/2) * np.sum((y_true - a) ** 2)

dc_da = (a - y_true)

mgrad = dc_da.dot(x)
bgrad = np.sum(dc_da)

m0 -= learning_rate * (2 / N) * mgrad
b0 -= learning_rate * (2 / N) * bgrad

if (i % 2 == 0):
print(&quot;Iteration {}&quot;.format(i))
print(&quot;Cost: {}, m: {}, b: {}\n&quot;.format(cost, m0, b0))

结果为：
迭代 0
Cost: 1341.5241150881411, m: 26.02473879743261, b: 2.8683883457327797

迭代 2
Cost: 409781757.38124645, m: 13657.166910552878, b: 1053.5831308528543

迭代 4
Cost: 132510115599264.75，m：7765058.4350503925，b：598610.1166795876

迭代 6
成本：4.284947676217907e+19，m：4415631880.089208，b：340401694.5610262

迭代 8
成本：1.3856132043127762e+25，m：2510967578365.3584，b：193570850213.62192

我的实现有什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/50219054/gradient-descent-for-linear-regression-exploding</guid>
      <pubDate>Mon, 07 May 2018 16:54:52 GMT</pubDate>
    </item>
    </channel>
</rss>