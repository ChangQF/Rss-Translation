<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 02 Jan 2025 03:20:34 GMT</lastBuildDate>
    <item>
      <title>在机器学习中，混淆矩阵不会在热图的每个部分显示数字，可能是数字颜色和热图颜色重叠</title>
      <link>https://stackoverflow.com/questions/79321937/in-machine-learning-confusion-matrix-not-showing-numbers-in-every-portion-of-he</link>
      <description><![CDATA[[该图像显示混淆矩阵，其中文本为数字，图像颜色与结果混淆矩阵中相同，但混淆矩阵第一行不显示数字](https://i.sstatic.net/GsVNbJRQ.png)
# 计算准确率
from sklearn import metrics

result_N = metrics.confusion_matrix(y_test, y_pred_N)
print(&quot;Confusion Matrix:&quot;)
print(result_N)

def plt1():
import seaborn as sns; sns.set()
plt.figure(figsize=(4,4))
c_mtrx_N = pd.crosstab(y_test, y_pred_N, rownames=[&#39;Actual&#39;], colnames=[&#39;Predicted&#39;])
sns.heatmap(c_mtrx_N, annot=True, fmt = &#39;.3g&#39;)

plt1()

我向 chat gpt 寻求帮助，但问题仍然存在。请指导我如何在混淆矩阵中用颜色显示数字。我也尝试过更改混淆矩阵的颜色，但同样的问题仍然存在。]]></description>
      <guid>https://stackoverflow.com/questions/79321937/in-machine-learning-confusion-matrix-not-showing-numbers-in-every-portion-of-he</guid>
      <pubDate>Wed, 01 Jan 2025 16:22:09 GMT</pubDate>
    </item>
    <item>
      <title>Roboflow 中的“fine-tune-sam-2.1”nb 在我的数据集中出现错误。训练期间出现错误。我的数据集中的重叠掩码可能是问题所在吗？</title>
      <link>https://stackoverflow.com/questions/79321905/error-in-fine-tune-sam-2-1-nb-from-roboflow-with-my-dataset-an-error-occurred</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79321905/error-in-fine-tune-sam-2-1-nb-from-roboflow-with-my-dataset-an-error-occurred</guid>
      <pubDate>Wed, 01 Jan 2025 16:15:28 GMT</pubDate>
    </item>
    <item>
      <title>机器学习任务[关闭]</title>
      <link>https://stackoverflow.com/questions/79321506/machine-learning-task</link>
      <description><![CDATA[我在制作可以推荐药物进行诊断的模型时遇到了问题
请帮我制作模型
有人能告诉我构建模型的分步过程吗？我已经多次训练模型，但准确率 f1 分数不超过 21%，我该如何处理这个问题。
在下面的图片中，我还提供了我的数据集，它是什么样子的
在此处输入图片说明
在此处输入图片说明]]></description>
      <guid>https://stackoverflow.com/questions/79321506/machine-learning-task</guid>
      <pubDate>Wed, 01 Jan 2025 11:11:37 GMT</pubDate>
    </item>
    <item>
      <title>训练视频异常检测模型时出现 torch.OutOfMemoryError: CUDA out of memory 错误</title>
      <link>https://stackoverflow.com/questions/79321152/error-of-torch-outofmemoryerror-cuda-out-of-memory-when-training-video-anomaly</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79321152/error-of-torch-outofmemoryerror-cuda-out-of-memory-when-training-video-anomaly</guid>
      <pubDate>Wed, 01 Jan 2025 04:31:05 GMT</pubDate>
    </item>
    <item>
      <title>使用 ResNet50 + BiLSTM 的识别模型的损失函数太高且准确率为 0.0000e+00</title>
      <link>https://stackoverflow.com/questions/79320980/loss-function-too-high-and-accuracy-0-0000e00-for-recognition-model-using-resne</link>
      <description><![CDATA[我正在使用 ResNet50 和 BiLSTM 的组合进行手写句子识别任务。我的数据集包括手写句子的图像，每个句子都表示为一个图像序列。但是，我在训练期间遇到了模型性能问题：
损失函数从 9.1130 开始，并且保持较高水平。
准确率始终为 0.0000e+00，即使在 20 个 epoch 之后也是如此。
图像存储在文件夹中，每个句子一个文件夹，每个句子有 10 个样本。
标签存储在带有句子 ID 及其对应文本标签的 CSV 文件中。
我使用 tensorflow。
# 数据集路径

dataset_path = &quot;/content/drive/My Drive/Images&quot;
csv_path = &quot;/content/drive/My Drive/Labels/lines-labels.csv&quot;

# 数据生成器：
# 我实现了一个自定义数据生成器来加载图像序列及其标签：

class DataGenerator(Sequence):
def __init__(self, dataset_path, labels_df, batch_size, seq_length, img_width, img_height):
# 初始化参数...

def __len__(self):
# 批次数...

def __getitem__(self, index):
# 加载一批序列...

该模型使用 ResNet50 基础进行特征提取，使用双向 LSTM 进行序列建模：
def build_resnet_bilstm_model(input_shape, num_classes):
resnet_base = ResNet50(weights=&#39;imagenet&#39;, include_top=False, input_shape=(224, 224, 3))

for layer in resnet_base.layers:
layer.trainable = False

input_layer = 输入（shape=input_shape）
time_distributed = TimeDistributed（resnet_base）（input_layer）
time_distributed = TimeDistributed（Flatten（））（time_distributed）

bilstm = Bidirectional（LSTM（256，return_sequences=False））（time_distributed）
dropout = Dropout（0.5）（bilstm）

output_layer = Dense（num_classes，activation=&#39;softmax&#39;）（dropout）

model = Model（inputs=input_layer，outputs=output_layer）
model.compile（optimizer=Adam（learning_rate=0.001），loss=&#39;categorical_crossentropy&#39;，metrics=[&#39;accuracy&#39;]）

返回模型

训练：
epochs：20，patience：5， min_lr:1e-6
什么原因可能导致此问题？是否与以下因素有关：

图像或标签的预处理不正确？
数据生成器或序列长度 (SEQ_LENGTH = 10) 存在问题？
模型架构，特别是 ResNet50 和 BiLSTM 的集成？

如果您能提供任何建议或见解以改进模型的训练行为，我将不胜感激。
模型架构]]></description>
      <guid>https://stackoverflow.com/questions/79320980/loss-function-too-high-and-accuracy-0-0000e00-for-recognition-model-using-resne</guid>
      <pubDate>Wed, 01 Jan 2025 00:37:11 GMT</pubDate>
    </item>
    <item>
      <title>自定义神经网络中的回归为所有输入提供相同的输出</title>
      <link>https://stackoverflow.com/questions/79320551/regression-in-custom-neural-network-giving-same-output-for-all-inputs</link>
      <description><![CDATA[我最近开始研究神经网络，在使用 tensorflow 或 pytorch 等库之前，我考虑编写自己的神经网络，以便深入了解网络内部发生的事情。我根据神经网络的数学知识开发了代码。现在网络出现了一些奇怪的行为。首先，网络有时表现非常好，例如使用 scikit learn 的 iris 数据集和一个小配置，我能够将网络的准确率提高到 99.3%。但有时它的表现太差了。此外，在一个问题中，我注意到使用 ReLU 激活函数作为第一层，然后所有其余的 sigmoid 给出了近 63% 的准确率，但是当我将第一层设为 sigmoid 和第二层设为 ReLU 并其余所有 sigmoid 时，准确率上升到 93%。现在的主要问题是回归，它似乎根本不起作用。该模型似乎对所有输入都输出相同的东西。
以下是网络的工作原理，它是单层的简单代码。当我们声明一个层时，它需要 4 个参数 —— 输入数量、神经元数量、激活函数、激活函数的微分。现在我们可以使用 .forward(inputs) 方法转发该层。对于反向传播，我们有 Backward() 方法，它采用 d（损失函数）/d（激活输出）和学习率。它还会自动返回 d（损失函数）/d（前一层的激活输出），可以将其放在前一层进行反向传播。此外，由于对于 softmax 激活，微分有点复杂，我作弊了一点，假设 softmax 激活函数只会在最后一层使用，因此对于具有 softmax 函数的神经层，我们不需要传递任何东西，只需学习率，其余的它会计算（因此你可以忽略 softmax_diff 函数，它没用）。有了所有这些上下文，下面是代码
import numpy as np

def ReLU(z):
return np.maximum(0,z)
def ReLU_diff(a):
return (a&gt;0).astype(int)
def sigmoid(z):
z = np.clip(z, -500, 500)
return 1 / (1 + np.exp(-z))
def sigmoid_diff(a):
return a * (1 - a)
def tanh(z):
return np.tanh(z)
def tanh_diff(a):
return 1 - a ** 2
def softmax(z):
z = z - np.max(z, axis=1, keepdims=True)
return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)
def softmax_diff(a):
return a * (1 - a)
def linear(z):
return z
def linear_diff(a):
return np.ones(a.shape)

class Layer():
def __init__(self, numberOfInputs,numberOfNeurons,activationFunction,activationFunctionDiff):
self.numberOfInputs = numberOfInputs
self.numberOfNeurons = numberOfNeurons
self.activationFunction =activationFunction
self.activationFunctionDiff =activationFunctionDiff
self.weights = np.random.randn(numberOfNeurons, numberOfInputs) * np.sqrt(2.0/numberOfInputs) # He 初始化
self.biases = np.zeros((numberOfNeurons,1)) # 将偏差初始化为零
def forward(self,inputs):
self.inputs = 输入
self.z = np.dot(输入，self.weights.T) + self.biases.T
self.outputs = self.activationFunction(self.z)
返回 self.outputs
def behind(self, dl_da, learning_rate,outPuts=None):

如果 self.activationFunction == softmax:
dz = dz = self.outputs - outPuts
否则：
da_dz = self.activationFunctionDiff(self.outputs)
dz = dl_da * da_dz
dl_da_prev = np.dot(dz,self.weights)
self.backParam = dl_da_prev
dw = np.zeros((self.numberOfNeurons, self.numberOfInputs))
for i in range(dz.shape[0]):
dws = np.dot(dz[i].reshape(self.numberOfNeurons,1), self.inputs[i].reshape(1,self.numberOfInputs))
dw += dws
dw = dw / dz.shape[0]
db = np.sum(dz, axis=0, keepdims=True).T / dz.shape[0]
self.weights = self.weights - learning_rate * dw
self.biases = self.biases - learning_rate * db
return dl_da_prev


现在我来介绍一下我如何使用这段代码
from sklearn.datasets import fetch_california_housing
data = fetch_california_housing()
X, y = data.data, data.target
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
layer1 = Layer(8,8,ReLU,ReLU_diff)
layer2 = Layer(8,1,linear,linear_diff)
y_t = np.array(y_train).reshape(len(y_train),1)
for i in range(1000):
outputs = layer1.forward(X_train)
outputs = layer2.forward(outputs)
dl = -2*(y_t-outputs)
backParam = layer2.backward(dl,0.001)
backParam = layer1.backward(backParam,0.001)

因此，最终输出位于 layer2.outputs 中，与输出相同。但首先输出是完全错误的，其次所有输入都一样。为什么会发生这种情况？
我已经考虑了很久，并尝试了 ChatGPT 和其他模型的帮助。我不明白错误在哪里？
谢谢帮助！]]></description>
      <guid>https://stackoverflow.com/questions/79320551/regression-in-custom-neural-network-giving-same-output-for-all-inputs</guid>
      <pubDate>Tue, 31 Dec 2024 17:57:05 GMT</pubDate>
    </item>
    <item>
      <title>为何我无法包装 LGBM？</title>
      <link>https://stackoverflow.com/questions/79320289/why-cant-i-wrap-lgbm</link>
      <description><![CDATA[我使用 LGBM 预测数值量的相对变化。我使用 MSLE（均方对数误差）损失函数来优化我的模型并获得正确的误差缩放比例。由于 MSLE 不是 LGBM 的原生功能，因此我必须自己实现它。但幸运的是，数学可以大大简化。这是我的实现；
class MSLELGBM(LGBMRegressor):
def __init__(self, **kwargs): 
super().__init__(**kwargs)

def predict(self, X):
return np.exp(super().predict(X))

def fit(self, X, y, eval_set=None, callbacks=None):
y_log = np.log(y.copy())
print(super().get_params()) # 这不会打印任何 kwargs
if eval_set:
eval_set = [(X_eval, np.log(y_eval.copy())) for X_eval, y_eval in eval_set]
super().fit(X, y_log, eval_set=eval_set, callbacks=callbacks)

如您所见，它非常简单。我基本上只需要对模型目标应用对数变换，并对预测取指数以返回我们自己的非对数世界。
但是，我的包装器不起作用。我使用以下命令调用该类；
model = MSLELGBM(**lgbm_params)
model.fit(data[X_cols_all], data[y_col_train]) 

我收到以下异常；

-----------------------------------------------------------------------------------------
KeyError Traceback (most recent call last)
Cell In[31], line 38
32 callbacks = [
33 lgbm.early_stopping(10, verbose=0), 
34 lgbm.log_evaluation(period=0),
35 ]
37 model = MSLELGBM(**lgbm_params)
---&gt; 38 model.fit(data[X_cols_all], data[y_col_train]) 
40 feature_importances_df = pd.DataFrame([model.booster_.feature_importance(importance_type=&#39;gain&#39;)], columns=X_cols_all).T.sort_values(by=0, accending=False)
41 feature_importances_df.iloc[:30]

单元格 In[31]，第 17 行
15 if eval_set:
16 eval_set = [(X_eval, np.log(y_eval.copy())) for X_eval, y_eval in eval_set]
---&gt; 17 super().fit(X, y_log, eval_set=eval_set, callbacks=callbacks)

文件 c:\X\.venv\lib\site-packages\lightgbm\sklearn.py:1189，在 LGBMRegressor.fit(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)
1172 def fit( # type: ignore[override]
1173 self,
1174 X: _LGBM_ScikitMatrixLike,
(...)
1186 init_model: Optional[Union[str, Path, Booster, LGBMModel]] = None,
1187 ) -&gt; &quot;LGBMRegressor&quot;:
1188 &quot;&quot;&quot;Docstring 继承自 LGBMModel。&quot;&quot;&quot;
...
--&gt; 765 if isinstance(params[&quot;random_state&quot;], np.random.RandomState):
766 params[&quot;random_state&quot;] = params[&quot;random_state&quot;].randint(np.iinfo(np.int32).max)
767 elif isinstance(params[&quot;random_state&quot;], np.random.Generator):

KeyError: &#39;random_state&#39;

我不知道 random_state 为何从 fit 方法中缺失，因为该函数甚至不需要它。我感觉这是一个复杂的软件工程问题，超出了我的理解范围。有人知道发生了什么吗？
如果有帮助的话，我尝试使用更简单的非 lgbm 结构来说明我想要的内容；

我只想将我提供给 MSLELGBM 的任何参数传递给原始 LGBM，但这样做时我遇到了很多问题。]]></description>
      <guid>https://stackoverflow.com/questions/79320289/why-cant-i-wrap-lgbm</guid>
      <pubDate>Tue, 31 Dec 2024 15:25:17 GMT</pubDate>
    </item>
    <item>
      <title>为 RAG 应用程序设计一个可以有效处理 MongoDB 中多个集合的检索系统的最佳方法是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79319727/what-is-the-best-approach-to-design-a-retrieval-system-for-a-rag-application-tha</link>
      <description><![CDATA[数据库集合还包括关系集合。根据用户查询，我希望应用程序能够有效地从不同的集合中检索所需的数据。
我之前尝试将整个数据库转换为向量嵌入，但这种方法太慢了。同样，为此目的使用聚合管道也很慢，并且会消耗大量处理资源。
我还尝试使用 openai 平台，即使用提示将数据提供给 chatgpt，然后从中检索数据，但输入大小太大，无法处理]]></description>
      <guid>https://stackoverflow.com/questions/79319727/what-is-the-best-approach-to-design-a-retrieval-system-for-a-rag-application-tha</guid>
      <pubDate>Tue, 31 Dec 2024 10:23:26 GMT</pubDate>
    </item>
    <item>
      <title>加载的 Keras 模型在预测时出现错误（可能与掩蔽有关）</title>
      <link>https://stackoverflow.com/questions/79318939/loaded-keras-model-throws-error-while-predicting-likely-issues-with-masking</link>
      <description><![CDATA[我目前正在开发和测试一个依赖大量数据进行训练的 RNN，因此尝试将训练文件和测试文件分开。我有一个文件，用于创建、训练和保存 tensorflow.keras 模型到文件 &#39;model.keras&#39;。然后，我将这个模型加载到另一个文件中并预测一些值，但出现以下错误：
无法将元素 {&#39;class_name&#39;: &#39;__tensor__&#39;, &#39;config&#39;: {&#39;dtype&#39;: &#39;float64&#39;, &#39;value&#39;: [0.0, 0.0, 0.0, 0.0]}} 转换为 Tensor。请考虑将元素转换为受支持的类型。请参阅 https://www.tensorflow.org/api_docs/python/tf/dtypes 了解受支持的 TF dtypes
顺便说一句，我曾尝试使用训练模型的文件中完全相同的数据运行 model.predict，并且运行顺利。模型加载肯定是问题所在，而不是用于预测的数据。
这个神秘的 float64 张量是我传递到掩码层的值。我不明白为什么 keras 无法将这个 JSON 对象识别为张量并以此方式应用掩码操作。我在下面附上了我的代码片段，为清晰和简洁起见进行了编辑：
model_generation.py：
# 创建模型

model = tf.keras.Sequential([
tf.keras.layers.Input((352, 4)),
tf.keras.layers.Masking(mask_value=tf.convert_to_tensor(np.array([0.0, 0.0, 0.0, 0.0]))),
tf.keras.layers.GRU(50, return_sequences=True,activation=&#39;tanh&#39;),
tf.keras.layers.Dropout(0.2),
tf.keras.layers.GRU(50,activation=&#39;tanh&#39;),
tf.keras.layers.Dropout(0.2),
tf.keras.layers.Dense(units=1,activation=&#39;sigmoid&#39;)])

# 编译模型...
# 训练模型...
model.save(&#39;model.keras&#39;)

model.predict(data) # 此行在此起作用

model_testing.py
model = tf.keras.models.load_model(&#39;model.keras&#39;)

model.predict(data) # 此行生成错误

编辑：
将加载命令移至与训练相同的文件中，仍然收到完全相同的错误消息。]]></description>
      <guid>https://stackoverflow.com/questions/79318939/loaded-keras-model-throws-error-while-predicting-likely-issues-with-masking</guid>
      <pubDate>Tue, 31 Dec 2024 00:53:11 GMT</pubDate>
    </item>
    <item>
      <title>mlagents-learn --help 出现错误（python=3.11、3.10、3.9、3.8）</title>
      <link>https://stackoverflow.com/questions/79316958/mlagents-learn-help-is-giving-errors-python-3-11-3-10-3-9-3-8</link>
      <description><![CDATA[我正在尝试安装 mlagents。我进入了 python 部分，但在使用 pyenv 创建虚拟环境并将本地版本设置为 3.10、3.9 和 3.8 后，它们都不起作用。我升级了 pip，安装了 mlagents，然后安装了 torch、torchvision 和 torchaudio。然后我测试了 mlagents-learn --help，然后因为错误安装了 protobuf 3.20.3。然后我再次测试，得到以下错误
(venv) D:\Unity\AI Ecosystem&gt;mlagents-learn --help
回溯（最近一次调用）：
文件“&lt;frozen runpy&gt;”，第 198 行，在 _run_module_as_main
文件“&lt;frozen runpy&gt;”，第 88 行，在 _run_code
文件“D:\Unity\AI Ecosystem\venv\Scripts\mlagents-learn.exe\__main__.py”，第 4 行，在 &lt;module&gt;
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\trainers\learn.py”，第 2 行，在 &lt;module&gt;
从 mlagents 导入 torch_utils
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\torch_utils\__init__.py”，第 1 行，位于 &lt;module&gt;
从 mlagents.torch_utils.torch 导入 torch 作为 torch # noqa
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\torch_utils\torch.py​​”，第 6 行，位于 &lt;module&gt;
从 mlagents.trainers.settings 导入 TorchSettings
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\trainers\settings.py”，第 644 行，位于 &lt;module&gt;
class TrainerSettings(ExportableSettings):
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\mlagents\trainers\settings.py”，第 667 行，位于 TrainerSettings
cattr.register_structure_hook(
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\cattr\converters.py”，第 207 行，位于 register_structure_hook
self._structure_func.register_cls_list([(cl, func)])
文件“D:\Unity\AI Ecosystem\venv\Lib\site-packages\cattr\dispatch.py​​”，第 55 行，位于 register_cls_list
self._single_dispatch.register(cls, handler)
文件&quot;C:\Users\Ebrah\AppData\Local\Programs\Python\Python311\Lib\functools.py&quot;，第 864 行，在寄存器中
raise TypeError(
TypeError: `register()` 的第一个参数无效。 typing.Dict[mlagents.trainers.settings.RewardSignalType, mlagents.trainers.settings.RewardSignalSettings] 不是类或联合类型。

我尝试安装 cattrs 1.5.0，但错误仍然存​​在。正如我之前所说，我也尝试了 3.11、3.10、3.9 和 3.8，但在所有这些版本中都出现了相同的错误。我的 unity 版本是 2022.3.5f1，但我看不出这会有什么不同。我的 pyenv 版本是 3.1.1。我在 Windows 11 上并且正在使用 pyenv-win。]]></description>
      <guid>https://stackoverflow.com/questions/79316958/mlagents-learn-help-is-giving-errors-python-3-11-3-10-3-9-3-8</guid>
      <pubDate>Mon, 30 Dec 2024 06:36:09 GMT</pubDate>
    </item>
    <item>
      <title>如何在目录中找到给定图像的相似图像[关闭]</title>
      <link>https://stackoverflow.com/questions/79315320/how-to-find-similar-images-for-given-image-in-a-directory</link>
      <description><![CDATA[我是机器学习和 Python 的新手，我正在尝试在目录中找到与给定图像最匹配（相似）的图像。
例如，我的文件夹中有 50,000 张硬币图像，并尝试找到与用户指定图像最匹配的前 10 张图像。
这是我的示例文件 https://drive.google.com/drive/folders/1pxz4nuyWfSLcrzs0QZpvqWO77ZI3Vn6M?usp=sharing
这是示例搜索图像 https://drive.google.com/file/d/1cWTxLIjG2pKW4-1g-LYfdAbccG18sPnX/view?usp=sharing
我也用 OpenCV 修改了给定的图像，但没有任何变化，这里是裁剪版本 https://drive.google.com/file/d/1AvZbPgZJvPwSBqNxVZ9N7t-DxTI_Xzsl/view?usp=sharing
预期 18206-f.jpg 应为根据指定示例文件匹配度最高的文件。
我尝试了许多算法并提供了类似下面的解决方案，但找不到合适的解决方案。

我使用了自定义训练模型进行图像分类，如这里所述 https://www.tensorflow.org/tutorials/images/classification 但我失败了，我猜我没有针对特定示例的不同文件，我还尝试了自定义数据增强机制，但再次失败（失败 = 可能找不到正确的相关文件）

我使用了https://github.com/TechyNilesh/DeepImageSearch但失败了我想这种解决方案最适合语义搜索，我需要图像到图像的搜索，例如图像相似性，而不是关键字/内容相似性等。

我使用了 Ahash、Dhash 和 Phash 等哈希算法，但在图像处于不同条件等时失败了。

我尝试了这里提到的 KNN 搜索https://stackoverflow.com/a/56881718/6799182但失败了再次。


如何从指定的图像中找到最佳匹配图像。在我的示例数据集中。
输入：1-tl.jpg
输出：
1853-f.jpg
...
...
...
import cv2
import os
import numpy as np

from typing import Union

def read_img_from_dir(image_dir: str, query_shape: Union[list, tuple]) -&gt; tuple[list, np.ndarray]:
name_image = []
img_array = np.empty((0, np.prod(query_shape)), dtype=np.float32)
for image_name in os.listdir(image_dir):
name_image.append(image_name)
image = cv2.imread(os.path.join(image_dir, image_name))
if not isinstance(image, np.ndarray):
# 如果路径不是图像
continue
image = cv2.resize(image, query_shape[:2][::-1])
image = image.reshape(1, -1) / 255.
img_array = np.concatenate((img_array, image))
return name_image, img_array 

def find_by_knn(query_img: np.ndarray，list_name：list [str]，数据库：np.ndarray）-&gt;; str:
query_img = query_img.reshape(1, -1) / 255.
dists = np.sqrt(np.sum((database-query_img) ** 2,axis = 1))
idx = dists.argmin()
return list_name[idx]

if __name__==&#39;__main__&#39;:
image_query = &#39;1tl-cropped.png&#39;
image_dir = &#39;sample_flat&#39;
img = cv2.imread(image_query)

# 可选：由于查询图像大小可能很大，调整大小 
# 所有图像为较小的所需形状可以避免 OOM 问题
# 并提高计算速度

# global_shape = (320, 320)
# img = cv2.resize(img, global_shape)

shape = img.shape
name_image, img_array = read_img_from_dir(image_dir, shape)
result = find_by_knn(img, name_image, img_array)
print(result)
]]></description>
      <guid>https://stackoverflow.com/questions/79315320/how-to-find-similar-images-for-given-image-in-a-directory</guid>
      <pubDate>Sun, 29 Dec 2024 09:30:09 GMT</pubDate>
    </item>
    <item>
      <title>计算模型的最小二乘误差[关闭]</title>
      <link>https://stackoverflow.com/questions/79315069/calculating-the-least-squares-error-for-a-model</link>
      <description><![CDATA[我做了一些自学，遇到了这个问题。我研究过 LSE 的问题，但我真的不明白这是怎么回事。有人能给我一个关于如何解决这个问题的简要想法吗？我一直在想办法解决这个问题，但没有成功。
]]></description>
      <guid>https://stackoverflow.com/questions/79315069/calculating-the-least-squares-error-for-a-model</guid>
      <pubDate>Sun, 29 Dec 2024 06:25:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLO 将无界输入导出到 mlpackage/mlmodel 文件</title>
      <link>https://stackoverflow.com/questions/79305588/use-yolo-with-unbounded-input-exported-to-an-mlpackage-mlmodel-file</link>
      <description><![CDATA[我想创建一个 .mlpackage 或 .mlmodel 文件，可以将其导入 Xcode 进行图像分割。为此，我想使用 YOLO 中的分割包来检查它是否符合我的需求。
现在的问题是，此脚本创建的 .mlpackage 文件仅接受固定大小（640x640）的图像：
from ultralytics import YOLO

model = YOLO(&quot;yolo11n-seg.pt&quot;)

model.export(format=&quot;coreml&quot;)

我想在这里进行一些更改，可能使用 coremltools，以处理无界范围（我想处理任意大小的图像）。这里有一些描述：https://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html#enable-unbounded-ranges，但我不明白如何用我的脚本实现它。]]></description>
      <guid>https://stackoverflow.com/questions/79305588/use-yolo-with-unbounded-input-exported-to-an-mlpackage-mlmodel-file</guid>
      <pubDate>Tue, 24 Dec 2024 12:23:06 GMT</pubDate>
    </item>
    <item>
      <title>如何将两个不同的训练过的 ML 模型合二为一？</title>
      <link>https://stackoverflow.com/questions/64801479/how-to-combine-two-different-trained-ml-models-as-one</link>
      <description><![CDATA[我根据两个不同的数据集训练了两个 ml 模型。然后我将它们保存为 model1.pkl 和 model2.pkl 。有两个用户输入（不是模型的输入数据），如 x=0 和 x=1，如果 x=0，我必须使用 model1.pkl 进行预测，否则我必须使用 model2.pkl 进行预测。我可以使用 if 条件来执行它们，但我的问题是我必须知道是否有可能将其保存回 model.pkl 包括此条件语句。如果我将它们组合并保存为模型，它将很容易在其他 IDE 中加载。]]></description>
      <guid>https://stackoverflow.com/questions/64801479/how-to-combine-two-different-trained-ml-models-as-one</guid>
      <pubDate>Thu, 12 Nov 2020 09:44:27 GMT</pubDate>
    </item>
    <item>
      <title>如何使用带有灰度图像的预训练神经网络？</title>
      <link>https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images</link>
      <description><![CDATA[我有一个包含灰度图像的数据集，我想在这些图像上训练最先进的 CNN。我非常想微调一个预先训练好的模型（比如这里的模型）。
问题是，我能找到权重的几乎所有模型都是在包含 RGB 图像的 ImageNet 数据集上训练的。
我无法使用其中一个模型，因为它们的输入层需要一批形状为 (batch_size, height, width, 3) 或 (64, 224, 224, 3) 的模型，但我的图像批次是 (64, 224, 224)。
我有什么办法可以使用其中一个模型吗？我曾考虑在加载权重后删除输入层并添加自己的输入层（就像我们对顶层所做的那样）。这种方法正确吗？]]></description>
      <guid>https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images</guid>
      <pubDate>Fri, 24 Aug 2018 00:33:04 GMT</pubDate>
    </item>
    </channel>
</rss>