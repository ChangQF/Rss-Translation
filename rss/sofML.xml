<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 06 Feb 2025 21:16:14 GMT</lastBuildDate>
    <item>
      <title>MATLAB RL 代理学习陷入困境</title>
      <link>https://stackoverflow.com/questions/79419073/matlab-rl-agent-learning-stucks</link>
      <description><![CDATA[我正在尝试熟悉 MATLAB 的强化学习库。我正在努力创建一个可以学习正弦函数作为简单热身的代理，但我已经陷入困境。问题是，经过几次迭代后，代理达到一定水平，然后达到上限，无论我让学习过程运行多长时间，它都不会进一步学习。这是代码：
obs_info = rlNumericSpec([1 1], LowerLimit=-pi, UpperLimit=pi);
obs_info.Name = &quot;Sinus Value&quot;;

act_info = rlNumericSpec([1 1], LowerLimit=-1, UpperLimit=1);
act_info.Name = &quot;Predicted Value&quot;;

reset_fcn_handle = @()reset_train();
step_fcn_handle = @(action, portfolio)step_train( ...
action, portfolio);

sinus_train_env = rlFunctionEnv( ...
obs_info, act_info, step_fcn_handle, reset_fcn_handle);

function [initial_observation, portfolio] = reset_train()
initial_observation = 2*pi*rand(1)-pi;
portfolio = struct;
portfolio.LastValue = initial_observation;
end

function [next_observation, reward, is_done, portfolioOut] = step_train( ...
action, portfolio)
expected_prediction = sin(portfolio.LastValue);
reward = 1 / 100 / (0.01 + abs(action - expected_prediction));
next_observation = 2*pi*rand(1)-pi;
portfolioOut = portfolio;
portfolioOut.LastValue = next_observation;
is_done = false;
end

我使用强化学习设计器来构建代理。&quot;兼容算法&quot;设置为 TD3（默认选项），隐藏单元数为 32。超参数和探索模型设置：

训练时，最大片段长度 = 1000，平均窗口长度 = 5，停止标准 = AverageReward，停止值 = 900。
30 分钟后的结果：

30 分钟后的结果1 小时：

我尝试修改奖励函数并让它运行两个小时：
reward = 1 / (0.01 + abs(action - expected_prediction));

结果：

第二次尝试修改奖励函数：
if (abs(action-expected_prediction) &gt; 0.05)
reward = -1;
else 
reward = 1;
结束

结果：

我遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79419073/matlab-rl-agent-learning-stucks</guid>
      <pubDate>Thu, 06 Feb 2025 19:03:32 GMT</pubDate>
    </item>
    <item>
      <title>哪种 AI 检测工具可以提供最准确的结果来识别 AI 生成的内容？[关闭]</title>
      <link>https://stackoverflow.com/questions/79418854/which-ai-detection-tool-provides-the-most-accurate-results-for-identifying-ai-ge</link>
      <description><![CDATA[我想确保我收到或编写的内容是人工生成的。有各种可用的 AI 检测工具，但我不确定哪一种最准确、最可靠。有些文章可能部分由 AI 生成，这使得检测变得困难。为此目的，有哪些最好的工具可用？&quot;
我尝试过的方法和预期结果：
&quot;我尝试过 GPTZero 和 Originality.ai 等工具，但我正在寻找更准确或被广泛接受的解决方案。理想情况下，我需要一种能够高精度检测 AI 编写内容并提供可靠见解的工具。
我测试过 GPTZero、Originality.ai 和 Copyleaks AI Detector 等工具。虽然它们提供了一些见解，但我注意到它们的结果不一致。一些 AI 生成的内容被正确标记，而在其他情况下，人工编写的文本被错误地识别为 AI 生成的。我期望一种工具能够提供更高的准确性、清晰地解释其检测过程并能够有效地分析短篇和长篇内容。]]></description>
      <guid>https://stackoverflow.com/questions/79418854/which-ai-detection-tool-provides-the-most-accurate-results-for-identifying-ai-ge</guid>
      <pubDate>Thu, 06 Feb 2025 17:34:58 GMT</pubDate>
    </item>
    <item>
      <title>CNN 二元分类任务中 f-1 分数的不同结果</title>
      <link>https://stackoverflow.com/questions/79418471/different-results-on-f-1-score-in-binary-classification-task-in-cnn</link>
      <description><![CDATA[我正在为二元分类任务制作一个 CNN 模型。

当我使用 binary_crossentropy 作为损失函数并在最后一层保留 1 个神经元时，我的准确率约为 94%，val_accuracy 约为 85%，但我的 f-1 分数停留在 69% 左右。
当我使用 categorical_crossentropy 作为损失函数时，结果有些相似，但这次 f-1 分数约为 85%。

model = Sequential([
Input(shape=(*input_shape, 1)),

Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&quot;same&quot;, kernel_regularizer=l2(0.001)),
CBAMLayer(),
Conv2D(64, (3, 3),activation=&#39;relu&#39;, padding=“相同”，kernel_regularizer=l2(0.001)),
CBAMLayer(),
MaxPooling2D((2, 2)),
BatchNormalization(),

Conv2D(64, (3, 3), 激活=&#39;relu&#39;, padding=“相同”，kernel_regularizer=l2(0.001)),
CBAMLayer(),
Conv2D(64, (3, 3), 激活=&#39;relu&#39;, padding=“相同”，kernel_regularizer=l2(0.001)),
CBAMLayer(),
MaxPooling2D((2, 2)),
BatchNormalization(),

Conv2D(128, (3, 3), 激活=&#39;relu&#39;, padding=“相同”， kernel_regularizer=l2(0.001)),
Conv2D(128, (3, 3), 激活=&#39;relu&#39;, 填充=&quot;same&quot;, kernel_regularizer=l2(0.001)),
CBAMLayer(),
MaxPooling2D((2, 2)),
BatchNormalization(),

Conv2D(128, (3, 3), 激活=&#39;relu&#39;, 填充=&quot;same&quot;, kernel_regularizer=l2(0.001)),
Conv2D(128, (3, 3), 激活=&#39;relu&#39;, 填充=&quot;same&quot;, kernel_regularizer=l2(0.001)),
CBAMLayer(),
MaxPooling2D((2, 2)),
BatchNormalization(),

Conv2D(256, (3, 3),activation=&#39;relu&#39;, padding=&quot;same&quot;, kernel_regularizer=l2(0.001)),
Conv2D(256, (3, 3),activation=&#39;relu&#39;, padding=&quot;same&quot;, kernel_regularizer=l2(0.001)),
CBAMLayer(),
MaxPooling2D((2, 2)),
BatchNormalization(),

Flatten(),
Dense(512,activation=&#39;relu&#39;),
Dropout(0.5),
Dense(256,activation=&#39;relu&#39;),
Dropout(0.2),
Dense(2,activation=&#39;softmax&#39;)
])

谁能告诉我为什么会发生这种情况，以及解决办法是什么。
此外，我想知道准确率和 val_accuracy 差距的原因，即使类别是平衡的。
我尝试过改变模型结构和损失函数，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/79418471/different-results-on-f-1-score-in-binary-classification-task-in-cnn</guid>
      <pubDate>Thu, 06 Feb 2025 15:24:42 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 图像分类过度拟合问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79418275/tensorflow-image-classification-overfitting-issue</link>
      <description><![CDATA[我正在尝试使用 tensorflow keras 最新 api 和函数创建一个图像分类模型。我的模型通过查看复杂的特征和设计（例如微缩印刷、全息图、透明图案等）将纸币分为真币和假币。我有一个包含大约 300-400 张高质量图像的小型数据集。无论我做什么，我的模型都会过度拟合。它的训练准确率高达 1.000，训练损失高达 0.012。但验证准确率保持在 0.60-0.75 之间，验证损失保持在 0.40-0.53 之间。
我尝试了以下方法：

增加数据集。 （但我知道这不会有太大帮助，因为钞票差别不大。它们都非常相似。所以它不会有助于推广模型）
使用 drop-out、l1/l2 正则化
使用迁移学习。我使用了 ResNet50 模型。我首先通过冻结基础模型训练了几个时期，然后解冻模型并重新训练了更多时期。
使用类权重来平衡权重。
使用计划学习率在训练过程中进行修改。
使用提前停止和回调等。
尝试使用预处理

此外，如果我在其中使用规范化层，我的模型性能会更差，而没有它，它的性能会更好。所以我排除了该层。
但是，没有什么能帮助我提高泛化能力。我不知道我错过了什么。
我的模型：

data_augmentation = tf.keras.Sequential([
tf.keras.layers.RandomRotation(0.1),
tf.keras.layers.RandomZoom(0.1),
tf.keras.layers.RandomBrightness(0.1),
tf.keras.layers.RandomContrast(0.1),
])

train_ds = tf.keras.utils.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;training&quot;,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;validation&quot;,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

train_ds = (
train_ds
.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)
.cache()
.shuffle(1000)
.prefetch(buffer_size=AUTOTUNE)
)

base_model = tf.keras.applications.ResNet50(

input_shape=(img_height, img_width, 3),
include_top=False,
weights=&#39;imagenet&#39;
)

# 取消冻结特定层微调

base_model.trainable = True
for layer in base_model.layers[:-10]: # 保持第一层冻结
layer.trainable = False

l2_lambda=0.0001

#model
model = tf.keras.Sequential([
base_model,
tf.keras.layers.GlobalAveragePooling2D(),
tf.keras.layers.Dense
(512,activation=&#39;relu&#39;,kernel_regularizer=regularizers.l2(l2_lambda)),
tf.keras.layers.Dropout(0.5),
tf.keras.layers.Dense(256,activation=&#39;relu&#39;),
tf.keras.layers.Dropout(0.4),
tf.keras.layers.Dense(1,activation = &quot;sigmoid&quot;)
])
]]></description>
      <guid>https://stackoverflow.com/questions/79418275/tensorflow-image-classification-overfitting-issue</guid>
      <pubDate>Thu, 06 Feb 2025 14:19:24 GMT</pubDate>
    </item>
    <item>
      <title>NameError: 尝试运行 def __init__(self, width, height, inter=cv2.INTER_AREA) 时未定义名称“cv2”：[关闭]</title>
      <link>https://stackoverflow.com/questions/79417276/nameerror-name-cv2-is-not-define-while-trying-to-run-def-init-self-width</link>
      <description><![CDATA[我尝试使用 cv2 编写一些神经网络代码，但出现错误
def __init__(self, width, height, inter=cv2.INTER_AREA):
^^^
NameError：名称“cv2”未定义

knn.py --dataset ./datasets/animals
回溯（最近一次调用）：
文件“/Users/test/Desktop/CODE/knn.py”，第 6 行，位于&lt;module&gt;
来自 pyimagesearch.preprocessing 导入 SimplePreprocessor
文件“/Users/test/Desktop/CODE/pyimagesearch/preprocessing/SimplePreprocessor.py”，第 4 行，位于&lt;module&gt;
类 SimplePreprocessor:
文件 &quot;/Users/test/Desktop/CODE/pyimagesearch/preprocessing/SimplePreprocessor.py&quot;，第 5 行，在 SimplePreprocessor 中
def __init__(self, width, height, inter=cv2.INTER_AREA):
^^^
NameError: 名称 &#39;cv2&#39; 未定义
]]></description>
      <guid>https://stackoverflow.com/questions/79417276/nameerror-name-cv2-is-not-define-while-trying-to-run-def-init-self-width</guid>
      <pubDate>Thu, 06 Feb 2025 08:53:50 GMT</pubDate>
    </item>
    <item>
      <title>XGboost 在不同的标记器上具有不同的准确性[关闭]</title>
      <link>https://stackoverflow.com/questions/79416096/xgboost-has-different-accuracy-on-different-tokenizers</link>
      <description><![CDATA[我有一个transformer bodomerka/Mil_class_exp_sber_balanssedclass，我在sberbank-ai/ruBert-base的基础上对其进行了训练。额外训练的本质是，该模型可以对用俄语写的文本进行分类，并分类是否是军事经验（0或1）。
而且我还想训练Xgboost模型。我用transformer中的tokenizer对文本进行了token化。
最初，当我使用sberbank-ai/ruBert-base tokenizer时，准确率为0.86。但是，当我将其更改为bodomerka/Mil_class_exp_sber_balanssedclass时，准确率上升到了0.96。这是为什么呢？]]></description>
      <guid>https://stackoverflow.com/questions/79416096/xgboost-has-different-accuracy-on-different-tokenizers</guid>
      <pubDate>Wed, 05 Feb 2025 20:12:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow Lite 将 ML 模型实现到 Android 应用中</title>
      <link>https://stackoverflow.com/questions/79415465/implementing-an-ml-model-into-an-android-app-with-tensorflow-lite</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79415465/implementing-an-ml-model-into-an-android-app-with-tensorflow-lite</guid>
      <pubDate>Wed, 05 Feb 2025 16:29:25 GMT</pubDate>
    </item>
    <item>
      <title>用 Java 编写的对偶数和奇数进行分类的人工智能无法工作</title>
      <link>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</guid>
      <pubDate>Wed, 05 Feb 2025 02:09:06 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用哪些方法来找出行人轨迹的部分轨迹[关闭]</title>
      <link>https://stackoverflow.com/questions/79409749/what-are-methods-i-can-use-to-find-out-parts-of-trajectory-that-is-pedestrian-tr</link>
      <description><![CDATA[我有一个表示手机移动轨迹的数据集，该轨迹由步行和驾车行驶的路段组成。数据包括经度、纬度、时间戳和 3 轴加速度计数据。我需要提取所有步行行驶的子轨迹。有没有现成的解决方案可以解决这个问题？如果没有，我该如何处理这个任务？
我试图在互联网上寻找现成的解决方案，但没有找到任何有价值的东西。]]></description>
      <guid>https://stackoverflow.com/questions/79409749/what-are-methods-i-can-use-to-find-out-parts-of-trajectory-that-is-pedestrian-tr</guid>
      <pubDate>Mon, 03 Feb 2025 18:56:03 GMT</pubDate>
    </item>
    <item>
      <title>ML-Agents 代理无法在 Unity 中完成简单的“射弹到目标”任务</title>
      <link>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</link>
      <description><![CDATA[代理在重力作用下向目标发射弹丸。代理只有一个动作 - 射击角度。发射力是恒定的。我还没有改变目标的位置。因此这应该是微不足道的，因为模型只需要学习正确的射击角度。但经过 300000 个训练步骤后，模型仍然射击不稳定。
代理：
使用 Unity.MLAgents;
使用 Unity.MLAgents.Actuators;
使用 Unity.MLAgents.Sensors;
使用 UnityEngine;

公共类 ProjectileAgent：代理
{
公共 Transform 目标; //带有 2D 碰撞器和“目标”标签的固定目标
公共 Transform launchPoint; //生成弹丸的位置
公共 GameObject projectilePrefab; //带有 Rigidbody2D 和 ProjectileCollision 脚本的预制件
公共 float fixedForce = 500f; // 对射弹施加恒定的力

private bool hasLaunched = false;

public override void OnEpisodeBegin()
{
hasLaunched = false;
RequestDecision(); // 在每个情节开始时请求一个决定
}

public override void CollectObservations(VectorSensor sensor)
{
// 观察从发射点到目标的相对位置 (x,y)
Vector2 diff = target.position - launchPoint.position;
sensor.AddObservation(diff.x);
sensor.AddObservation(diff.y);
}

public override void OnActionReceived(ActionBuffers action)
{
if (!hasLaunched)
{
// 一个连续动作 (0..1) 映射到 [0..180] 度
float angle01 = Mathf.Clamp01(actions.ContinuousActions[0]);
float angleDegrees = Mathf.Lerp(0f, 180f, angle01);

LaunchProjectile(angleDegrees);
hasLaunched = true;
}
}

private void LaunchProjectile(float angleDegrees)
{
GameObject projObj = Instantiate(projectilePrefab, launchPoint.position, Quaternion.identity);
ProjectileCollision projScript = projObj.GetComponent&lt;ProjectileCollision&gt;();
projScript.agent = this;

Rigidbody2D rb = projObj.GetComponent&lt;Rigidbody2D&gt;();
float rad = angleDegrees * Mathf.Deg2Rad;
Vector2 direction = new Vector2(Mathf.Cos(rad), Mathf.Sin(rad));
rb.AddForce(direction * fixedForce);
}

// 射弹击中目标时调用
public void OnHitTarget()
{
AddReward(1.0f);
EndEpisode();
}

// 射弹未击中目标时调用
public void OnMiss(Vector2 projectilePosition)
{
float distance = Vector2.Distance(projectilePosition, target.position);
float maxDistance = 10f; // 根据需要调整
float vicinity = 1f - (distance / maxDistance);
vicinity = Mathf.Clamp01(proximity);

// 接近目标时获得部分奖励
AddReward(proximity * 0.5f);

// 未击中时获得小额惩罚
AddReward(-0.1f);
EndEpisode();
}

// Unity 编辑器中测试的启发式方法（随机角度）
public override void Heuristic(in ActionBuffers actionOut)
{
actionOut.ContinuousActions[0] = Random.value;
}
}

Projectile:
using UnityEngine;

public class ProjectileCollision : MonoBehaviour
{
public ProjectileAgent agent;

private void Start()
{
// 短暂时间后销毁，以便我们可以记录未击中
Destroy(gameObject, lifetime);
}

private void OnCollisionEnter2D(Collision2D collision)
{
if (collision.gameObject.CompareTag(&quot;Target&quot;))
{
agent.OnHitTarget();
}
else
{
agent.OnMiss(transform.position);
}
销毁（游戏对象）；
}
}


我尝试过的方法

奖励塑造：
击中目标可获得 +1 奖励，近距离击中可获得部分基于距离的奖励，未击中可获得少量负奖励。
我将击中奖励提高到 +3，降低了未击中惩罚，等等。
训练步骤：
我使用 PPO 运行了 300k+ 步。
碰撞检查：
日志确认 OnHitTarget() 和 OnMiss() 在预期时间触发。
固定力和重力：
通过硬编码角度，验证箭可以手动到达目标。
重力已设置，因此物理上可以击中。
无随机目标：
目标目前固定在一个位置以保持简单。
]]></description>
      <guid>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</guid>
      <pubDate>Mon, 27 Jan 2025 02:47:35 GMT</pubDate>
    </item>
    <item>
      <title>在 TensorFlow 中开发用于图像分类的预训练模型</title>
      <link>https://stackoverflow.com/questions/78540179/pretrain-model-developing-in-tensorflow-for-image-classification</link>
      <description><![CDATA[我有一个问题，关于如何修改预训练模型以对 3 个类而不是 1000 个类进行分类。这是我目前想到的 2 种方法。我不确定哪种方法最好。
NASNetMobile_model = tf.keras.applications.NASNetMobile (
input_shape=(224,224,3),
include_top=False,
pooling=&#39;avg&#39;,
classes=3,
weights=&#39;imagenet&#39;
)
NASNetMobile_model.trainable=False
NASNetMobile_model.summary()type here

在方法 1 中，NASNetMobile 模型使用预训练的 ImageNet 权重初始化，排除顶层并使用平均池化。该模型设置为不可训练，以防止其权重在训练期间更新。然后构建一个新的 Sequential 模型，其中包括预先训练的 NASNetMobile 模型，后面跟着两个密集层：一个有 128 个单元和 ReLU 激活，另一个有 3 个单元和 softmax 激活，用于最终分类。Sequential 模型使用 Adam 优化器和稀疏分类交叉熵损失进行编译。最后，在数据集上对模型进行 20 个 epoch 的训练，批处理大小为 4，验证分割为 20%。
方法 1
new_pretrained_model = tf.keras.Sequential()

new_pretrained_model.add(NASNetMobile_model)
new_pretrained_model.add(tf.keras.layers.Dense(128,activation=&#39;relu&#39;))
new_pretrained_model.add(tf.keras.layers.Dense(3,activation=&#39;softmax&#39;))

new_pretrained_model.layers[0].trainable = False
new_pretrained_model.summary() 此处

new_pretrained_model.compile(
optimizer=&#39;adam&#39;,
loss=&#39;sparse_categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;]
)

new_pretrained_model.fit(
Xtrain,
Ytrain,
epochs=20,
batch_size=4,
validation_split=0.2
)

方法 2
在方法 2 中，使用功能 API 创建新模型。预训练的 NASNetMobile 模型的输出被用作具有 128 个单元和 ReLU 激活的新密集层的输入，然后是具有 3 个单元和 softmax 激活的最终密集层。此方法明确将 NASNetMobile 模型的输入连接到新的输出层，形成一个新模型，其输入与原始 NASNetMobile 模型相同，但具有用于分类的附加密集层。然后使用 Adam 优化器和稀疏分类交叉熵损失编译新模型，并在数据集上训练 20 个时期，批处理大小为 4，验证分割为 20%。
NASNetMobile_model_out = NASNetMobile_model.output
x = tf.keras.layers.Dense(128,activation=&#39;relu&#39;)(NASNetMobile_model_out)
output = tf.keras.layers.Dense(3,activation=&#39;softmax&#39;)(x)
model_2 = tf.keras.Model(inputs = NASNetMobile_model.input,outputs=output)

model_2.summary()

model_2.compile(
optimizer=&#39;adam&#39;,
loss=&#39;sparse_categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;]
)

model_2.fit(
Xtrain,
Ytrain,
epochs=20,
batch_size=4,
validation_split=0.2
)
]]></description>
      <guid>https://stackoverflow.com/questions/78540179/pretrain-model-developing-in-tensorflow-for-image-classification</guid>
      <pubDate>Mon, 27 May 2024 16:22:12 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Swift、UIkit 和 CoreML 在 iOS 应用中访问图像分类器 ML 模型的预测结果</title>
      <link>https://stackoverflow.com/questions/69899044/how-to-access-prediction-results-of-an-image-classifier-ml-model-in-an-ios-app-u</link>
      <description><![CDATA[我正在尝试开发一款应用，使用经过 Apple CoreML 训练的模型对从相机拍摄的图像或从图像库中选择的图像进行分类。该模型经过了适当的训练和测试。在将其添加到 xcode 项目后，我使用 Preview 对其进行测试时，它没有显示任何问题。但是当我尝试使用 Swift 获取预测时，结果是错误的，与 Preview 显示的完全不同。感觉就像模型未经训练一样。
这是我访问模型所做预测的代码：
let pixelImage = buffer(from: (image ?? UIImage(named: &quot;imagePlaceholder&quot;))!)
self.imageView.image = image

guard let result = try? imageClassifier!.prediction(image: pixelImage!) else {
fatalError(&quot;发生意外错误&quot;)
}

let className: String = result.classLabel
let confidence: Double = result.classLabelProbs[result.classLabel] ?? 1.0
classifier.text = &quot;\(className)\nWith Confidence:\n\(confidence)&quot;

print(&quot;分类结果为：\(className)\n置信度为：\(confidence)&quot;)

imageClassifier 是我在代码段之前使用此行代码创建的模型：
let imageClassifier = try? myImageClassifier(configuration: MLModelConfiguration())

myImageClassifier 是我使用 CoreML 创建的 ML 模型的名称。
图像是正确的，即使我输入相同的图像，它也会显示与预览不同的结果。但必须将其转换为 UIImage 到 CVPixelBuffer 类型，因为预测只允许输入 CVPixelBuffer 类型。上面代码段中的 pixelImage 是更改为 CVPixelBuffer 类型后的图像。我使用这个 stackoverflow 问题中的解决方案进行转换。代码在这里以防出现问题：
func buffer(from image: UIImage) -&gt; CVPixelBuffer? {
let attrs = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue, kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue] 作为 CFDictionary
var pixelBuffer : CVPixelBuffer?
让 status = CVPixelBufferCreate(kCFAllocatorDefault, Int(image.size.width), Int(image.size.height), kCVPixelFormatType_32ARGB, attrs, &amp;pixelBuffer)
guard (status == kCVReturnSuccess) else {
return nil
}

CVPixelBufferLockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0))
让 pixelData = CVPixelBufferGetBaseAddress(pixelBuffer!)

让 rgbColorSpace = CGColorSpaceCreateDeviceRGB()
让 context = CGContext(data: pixelData, width: Int(image.size.width), height: Int(image.size.height), bitsPerComponent: 8, bytesPerRow: CVPixelBufferGetBytesPerRow(pixelBuffer!), space: rgbColorSpace, bitmapInfo: CGImageAlphaInfo.no​​neSkipFirst.rawValue)

context?.translateBy(x: 0, y: image.size.height)
context?.scaleBy(x: 1.0, y: -1.0)

UIGraphicsPushContext(context!)
image.draw(in: CGRect(x: 0, y: 0, width: image.size.width, height: image.size.height))
UIGraphicsPopContext()
CVPixelBufferUnlockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0))

return pixelBuffer
}

我认为模型本身没有任何问题，只是我将其实现到应用程序中的方式有​​问题。
编辑：
我已经从 Apple 的教程中下载了一个示例项目，并将其模型 MobileNet 实现到我的项目中。代码执行没有错误，结果是正确的。我创建的模型可能出了问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/69899044/how-to-access-prediction-results-of-an-image-classifier-ml-model-in-an-ios-app-u</guid>
      <pubDate>Tue, 09 Nov 2021 13:30:48 GMT</pubDate>
    </item>
    <item>
      <title>Keras，内存错误 - data = data.astype("float") / 255.0。无法为形状为 (13165, 32, 32, 3) 的数组分配 309.MiB</title>
      <link>https://stackoverflow.com/questions/61046588/keras-memoryerror-data-data-astypefloat-255-0-unable-to-allocate-309</link>
      <description><![CDATA[我目前正在研究 Smiles 数据集，然后应用深度学习来检测微笑是正面的还是负面的。我使用的机器是 Raspberry Pi 3，用于执行此程序的 Python 版本是 3.7（不是 2.7）
我的训练集中总共有 13165 张图像。我想将其存储到一个数组中。但是，我遇到了一个问题，就是分配一个形状为（13165, 32, 32, 3）的数组。
下面是源代码（shallownet_smile.py）：
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classes_report
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import SimplePreprocessor
from pyimagesearch.datasets import SimpleDatasetLoader
from pyimagesearch.nn.conv.shallownet import ShallowNet
from keras.optimizers import SGD
from imutils import routes
import matplotlib.pyplot as plt
import numpy as np
import argparse

ap = argparse.ArgumentParser()
ap.add_argument(&quot;-d&quot;, &quot;--dataset&quot;, required=True, help=&quot;path to input dataset&quot;)
args = vars(ap.parse_args())

# 获取我们将要描述的图像列表
print(&quot;[INFO] loading images...&quot;)

imagePaths = list(paths.list_images(args[&quot;dataset&quot;]))

sp = SimplePreprocessor(32, 32)
iap = ImageToArrayPreprocessor()

sdl = SimpleDatasetLoader(preprocessors=[sp, iap])
(data, labels) = sdl.load(imagePaths, verbose=1)
# 将值转换为 0-1 之间的值
data = data.astype(&quot;float&quot;) / 255.0

# 将数据划分为训练集和测试集
(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25,
random_state=42)

# 将标签从整数转换为向量
trainY = LabelBinarizer().fit_transform(trainY)
testY = LabelBinarizer().fit_transform(testY)

# 初始化优化器和模型
print(“INFO] 编译模型...”)

# 初始化随机梯度下降，学习率为 0.005
opt = SGD(lr=0.005)

model = ShallowNet.build(width=32, height=32,depth=3,classes=2)
model.compile(loss=&quot;categorical_crossentropy&quot;,optimizer=opt,
metrics=[&quot;accuracy&quot;])

# 训练网络
print(“INFO] 训练网络...”)

H = model.fit(trainX, trainY,validation_data=(testX, testY),batch_size=32,
epochs=100, verbose=1)

print(“[INFO] 评估网络...”)

predictions = model.predict(testX, batch_size=32)

print(classification_report(
testY.argmax(axis=1),
predictions.argmax(axis=1),
target_names=[&quot;positive&quot;, &quot;negative&quot;]
))

plt.style.use(&quot;ggplot&quot;)
plt.figure()
plt.plot(np.arange(0, 100), H.history[&quot;loss&quot;], label=&quot;train_loss&quot;)
plt.plot(np.arange(0, 100), H.history[&quot;val_loss&quot;], label=&quot;val_loss&quot;)
plt.plot(np.arange(0, 100), H.history[&quot;acc&quot;], label=&quot;train_acc&quot;)
plt.plot(np.arange(0, 100), H.history[&quot;val_acc&quot;], label=&quot;val_acc&quot;)
plt.title(&quot;训练损失和准确率&quot;)
plt.xlabel(&quot;Epoch #&quot;)
plt.ylabel(&quot;损失/准确率&quot;)
plt.legend()
plt.show()

假设数据集位于我当前的目录中。以下是我得到的错误：

python3 shallownet_smile.py -d=datasets/Smiles

错误消息
我仍然感到困惑，不知道哪里出了问题。我将非常感谢任何专家或有深度学习/机器学习经验的人向我解释和澄清我做错了什么。
感谢您的帮助和关注。]]></description>
      <guid>https://stackoverflow.com/questions/61046588/keras-memoryerror-data-data-astypefloat-255-0-unable-to-allocate-309</guid>
      <pubDate>Sun, 05 Apr 2020 17:25:26 GMT</pubDate>
    </item>
    <item>
      <title>如何计算神经网络预测的置信度分数</title>
      <link>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</link>
      <description><![CDATA[我正在使用深度神经网络模型（在 keras 中实现）进行预测。类似这样的内容：
def make_model():
model = Sequential() 
model.add(Conv2D(20,(5,5),activation = &quot;relu&quot;))
model.add(MaxPooling2D(pool_size=(2,2))) 
model.add(Flatten())
model.add(Dense(20,activation = &quot;relu&quot;))
model.add(Lambda(lambda x: tf.expand_dims(x, axis=1)))
model.add(SimpleRNN(50,activation=&quot;relu&quot;))
model.add(Dense(1,activation=&quot;softmax&quot;)) 
model.compile(loss = &quot;categorical_crossentropy&quot;,optimizer = adagrad,metrics = [&quot;accuracy&quot;])

返回模型

model = make_model()
model.fit(x_train, y_train, validation_data = (x_validation,y_validation), epochs = 25, batch_size = 25, verbose = 1)

##预测：
prediction = model.predict_classes(x)
probabilities = model.predict_proba(x) #我假设这些是被预测的类的概率

我的问题是分类（二元）问题。我希望计算每个预测的置信度分数，即我想知道 - 我的模型是否 99% 确定它是“0”或 58% 是“0”。
我找到了一些关于如何做到这一点的观点，但无法实现它们。我希望遵循的方法表示：“使用分类器，当您对输出进行 softmax 时，您可以将值解释为属于每个特定类别的概率。您可以使用它们的分布作为粗略衡量您对观察结果属于该类别的信心的指标。”
我应该如何使用类似上述模型的东西进行预测，以便获得对每个预测的信心？我希望有一些实际的例子（最好是在 Keras 中）。]]></description>
      <guid>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</guid>
      <pubDate>Wed, 22 Jan 2020 02:52:32 GMT</pubDate>
    </item>
    <item>
      <title>ResNet50 模型未通过 keras 中的迁移学习进行学习</title>
      <link>https://stackoverflow.com/questions/58390209/resnet50-model-is-not-learning-with-transfer-learning-in-keras</link>
      <description><![CDATA[我正在尝试对 ResNet50 模型执行迁移学习，该模型已针对 PASCAL VOC 2012 数据集的 Imagenet 权重进行了预训练。由于它是一个多标签数据集，因此我在最后一层使用 sigmoid 激活函数和 binary_crossentropy 损失。指标包括 precision、recall 和 accuracy。下面是我用来为 20 个类（PASCAL VOC 有 20 个类）构建模型的代码。
img_height,img_width = 128,128
num_classes = 20
#如果正在加载 imagenet 权重，
#输入必须具有静态正方形（(128, 128)、(160, 160)、(192, 192) 或 (224, 224) 之一）
base_model = applications.resnet50.ResNet50(weights= &#39;imagenet&#39;, include_top=False, input_shape= (img_height,img_width,3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
#x = Dropout(0.7)(x)
predictions = Dense(num_classes,activation= &#39;sigmoid&#39;)(x)
model = Model(inputs = base_model.input, output = predictions)
for layer in model.layers[-2:]:
layer.trainable=True
for layer in model.layers[:-3]:
layer.trainable=False

adam = Adam(lr=0.0001)
model.compile(optimizer= adam, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;,precision_m,recall_m])
#print(model.summary())

X_train, X_test, Y_train, Y_test = train_test_split(x_train, y, random_state=42, test_size=0.2)
savingcheckpoint = ModelCheckpoint(&#39;ResnetTL.h5&#39;,monitor=&#39;val_loss&#39;,verbose=1,save_best_only=True,mode=&#39;min&#39;)
earlystopcheckpoint = EarlyStopping(monitor=&#39;val_loss&#39;,patience=10,verbose=1,mode=&#39;min&#39;,restore_best_weights=True)
model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test,Y_test), batch_size=batch_size,callbacks=[savingcheckpoint,earlystopcheckpoint],shuffle=True)
model.save_weights(&#39;ResnetTLweights.h5&#39;)

它运行了 35 个 epoch 直到 earlystopping，指标如下（没有 Dropout 层）：
loss: 0.1195 - accuracy: 0.9551 - precision_m: 0.8200 - recall_m: 0.5420 - val_loss: 0.3535 - val_accuracy: 0.8358 - val_precision_m: 0.0583 - val_recall_m: 0.0757

即使使用 Dropout 层，我也看不出有什么区别。
loss: 0.1584 - accuracy: 0.9428 - precision_m: 0.7212 - recall_m: 0.4333 - val_loss: 0.3508 - val_accuracy: 0.8783 - val_precision_m: 0.0595 - val_recall_m: 0.0403

使用 dropout，对于经过几个时期，模型的验证精度和准确率达到了 0.2，但并未超过该值。
我发现，与有和没有 dropout 层的训练集相比，验证集的精度和召回率相当低。我应该如何解释这一点？这是否意味着模型过度拟合。如果是这样，我该怎么办？截至目前，模型预测相当随机（完全不正确）。数据集大小为 11000 张图像。]]></description>
      <guid>https://stackoverflow.com/questions/58390209/resnet50-model-is-not-learning-with-transfer-learning-in-keras</guid>
      <pubDate>Tue, 15 Oct 2019 08:20:12 GMT</pubDate>
    </item>
    </channel>
</rss>