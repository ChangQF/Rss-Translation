<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 11 Mar 2024 18:16:56 GMT</lastBuildDate>
    <item>
      <title>我的 LSTM 模型可以自行学习特征工程吗？</title>
      <link>https://stackoverflow.com/questions/78142401/can-my-lstm-model-learn-feature-engineering-on-its-own</link>
      <description><![CDATA[我有一个时间序列数据集，我正在其上训练 LSTM 模型以执行多类分类。
我的数据集有 7 列 =&gt; x1,x2,x3....x7
并且有 4 个标签 =&gt; f1,f2,f3,f4
由于我拥有数据集的领域知识，因此我确切地知道需要完成哪些特征工程。
例如，我需要通过在每一行应用一些规则来从当前功能创建 4 个新功能：-

newx1 是由 =&gt; 创建的if (x2==x3) then 1, else 0
newx2 是由 =&gt; 创建的if (x1==x4 and x1&gt;x5) then 1, else 0
newx3 是由 =&gt; 创建的if ((x1-x6)/x1&gt;x7) 则 1，否则 0
newx4 是由 =&gt; 创建的if ((x6-x1)x1/&gt;x7) then 1. else 0

如果我在 newx1、newx2、newx3、newx4 上训练 LSTM 模型，测试数据的准确率将达到 100%。
但是，在原始特征 (x1,x2...x7) 上对其进行训练时，测试数据的准确度降低了 85-90%。
我试图解决的问题要求我的准确率高于 99%，因此仅仅 90% 的准确率是不够的。
我想知道我的 LSTM 模型是否可以自行学习特征工程的规则，或者我是否必须更改我的模型？
注意：我无法手动应用特征工程规则，因为我正在多个数据集上训练 LSTM 模型，并且每个数据集都需要自己的特征工程规则。我想让它尽可能通用。
LSTM模型：
def create_lstm_model(MaxTimeslice, H, LR, num_classes, dropout_rate=0.1, l2_reg=0.001):

    ip = 输入(形状=(MaxTimeslice, H))
    x = LSTM(32, return_sequences=True, dropout=dropout_rate, kernel_regularizer=l2(l2_reg))(ip)
    x = LSTM(16, dropout=dropout_rate, kernel_regularizer=l2(l2_reg))(x)
    
   
    x = 密集（单位=16，激活=&#39;relu&#39;）（x）
    multiclass_output = Dense（单位=num_classes，激活=&#39;softmax&#39;）（x）

    模型=模型（输入=ip，输出=multiclass_output）

    model.compile(loss=“categorical_crossentropy”,metrics=[“accuracy”],
    优化器=RMSprop(learning_rate=LR))

    返回模型
]]></description>
      <guid>https://stackoverflow.com/questions/78142401/can-my-lstm-model-learn-feature-engineering-on-its-own</guid>
      <pubDate>Mon, 11 Mar 2024 17:19:56 GMT</pubDate>
    </item>
    <item>
      <title>用于图像分类的早期融合模型，存在评估准确性的问题</title>
      <link>https://stackoverflow.com/questions/78141978/early-fusion-model-for-image-classification-with-problems-evaluating-accuracy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78141978/early-fusion-model-for-image-classification-with-problems-evaluating-accuracy</guid>
      <pubDate>Mon, 11 Mar 2024 15:58:59 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 中的 Brian2 模拟器实现液体状态机用于分类任务？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78141953/how-to-implement-liquid-state-machine-for-a-classification-task-using-brian2-sim</link>
      <description><![CDATA[我是一名正在攻读本科生的学生，目前正在研究用于分类相关任务的液态机。但是，即使花了几天时间研究 LSM，我也无法找到如何实现它们进行分类的良好开端。
如果没有适当的资源，学习概念也非常困难。那么，有人可以帮助我进行研究吗？]]></description>
      <guid>https://stackoverflow.com/questions/78141953/how-to-implement-liquid-state-machine-for-a-classification-task-using-brian2-sim</guid>
      <pubDate>Mon, 11 Mar 2024 15:53:53 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch Lightning 中实施 SSD</title>
      <link>https://stackoverflow.com/questions/78141575/implement-ssd-into-pytorch-lightning</link>
      <description><![CDATA[我不断收到此错误 AssertionError: Expected target boxs to be a tensor of shape [N, 4], got torch.Size([10, 100, 4]). ，这很奇怪因为我制作了形状 N,4 但由于某种原因我的训练批次 10 变成了形状？
我正在尝试训练图像和注释/地面实况框
下面是我对数据集的准备。
from pathlib 导入路径
从输入导入 Any、Callable、Dict、List、Optional、Tuple

将闪电导入为 L # noqa: N812
将 numpy 导入为 np
将 pandas 导入为 pd
进口火炬
导入 torch.nn.function 作为 F # noqa: N812
将 torchvision 导入为电视
导入 torchvision.transforms 作为 tv
从 Lightning.pytorch.utilities.types 导入 STEP_OUTPUT，OptimizerLRScheduler
从 PIL 导入图像
从 torch.utils.data 导入 DataLoader
从 torchvision.datasets 导入 VisionDataset

一个=[]

data_folder = Path().parent / “数据”

图像集 = 设置()
对于 data_folder.glob(“*.png”) 中的图像：
    image_set.add(images.stem)

盒子集 = 设置（）
对于data_folder.glob(“*txt”)中的boxes_list：
    box_set.add(boxes_list.stem)

交集= image_set &amp;盒子集

对于交叉点的茎：
    image_path = data_folder / (stem + “.png”)
    box_path = data_folder / (stem + “.txt”)

    a.append((image_path,boxes_path))

图像张量列表 = []
盒子列表 = []
自定义数据 = []

对于 a[0:10] 中的 image_path、boxes_path：
    img = Image.open(图像路径)
    变换 = tv.Compose([
        电视.调整大小((300,300)),
        tv.ToTensor()
        ]
    ）

    img_tensor = 变换(img)
    image_tensor_list.append（img_tensor）

    # 张量转换需要 dtype=float 来处理空 csv
    框= pd.read_csv(boxes_path,skiprows=[0],sep=&quot;&quot;,names=[&quot;xmin&quot;,&quot;ymin&quot;,&quot;xmax&quot;,&quot;ymax&quot;], dtype=np.float64 ）
    盒子 = torch.tensor(boxes.values)
    盒子=盒子.resize_(100,4)
    标签 = torch.zeros(size=(boxes.shape[0],), dtype=torch.int64)
    box_list.append({
        “盒子”：盒子，
        “标签”：标签
    })

数据集=列表（zip（image_tensor_list，boxs_list））

类数据集（视觉数据集）：
    def __init__(
        自己，
        根：str，
        变换：可选[可调用] = 无，
        变换：可选[可调用] = 无，
        target_transform：可选[可调用] = 无，
    ）-&gt;没有任何：
        super().__init__(根、变换、变换、target_transform,)
        self.dataset = 数据集

    def __getitem__(self, 索引: int) -&gt;元组[torch.Tensor, dict] |没有任何：
        返回 self.dataset[索引]

    def __len__(self) -&gt;; __len__(self) -&gt;整数：
        返回 len(self.dataset)

类分类器（L.LightningModule）：
    def __init__(自身):
        超级().__init__()

        self.model = tv.models.detection.ssd300_vgg16(权重=“SSD300_VGG16_Weights.DEFAULT”)
        对于 self.model.parameters() 中的参数：
            param.requires_grad = False

    定义前进（
        self，图像：List [torch.Tensor]，目标：可选[List [Dict [str，torch.Tensor]]] = None
    ）-&gt; Tuple[Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]:
        返回 self.model(图像、目标)

    def Training_step（自身，批次，batch_idx）：
        图像、目标 = 批次
        y_hat = self(图像, [目标])
        损失 = F.l1_loss(y_hat, 目标)
        self.log(“train_loss”, loss, on_epoch=True)
        回波损耗

    def configure_optimizers(self) -&gt;;优化器LRScheduler：
        返回 torch.optim.Adam(self.parameters())

类数据模块（L.LightningDataModule）：
    def __init__(自身,batch_size=32):
        超级().__init__()
        self.batch_size = 批量大小
        self.train_dataset = 无
        self.val_dataset = 无

    #（数据 - 平均值）/std
    def 设置（自身，阶段：str）：
        data_root = Path().parent / “数据”
        变换 = tv.transforms.Compose([
            电视. 变换. 调整大小((300, 300)),
            tv.transforms.ToTensor()
    ]）

        # TODO(ehnjcio): 进行适当的训练、测试、val 分割
        self.train_dataset = VarroaDataset(根=data_root, 变换=变换)
        # self.val_dataset = VarroaDataset(根=根, 变换=变换)

    def train_dataloader(自身):
        返回DataLoader（self.train_dataset，batch_size = self.batch_size，
                          工人数=0)

模型 = 分类器()
数据 = 数据模块()
训练员 = L.Trainer()
trainer.fit(模型，数据)
]]></description>
      <guid>https://stackoverflow.com/questions/78141575/implement-ssd-into-pytorch-lightning</guid>
      <pubDate>Mon, 11 Mar 2024 14:58:08 GMT</pubDate>
    </item>
    <item>
      <title>使用稳定基线3进行作业调度[关闭]</title>
      <link>https://stackoverflow.com/questions/78141506/job-scheduling-using-stable-baselines3</link>
      <description><![CDATA[我正在解决一个调度问题，其中有机器、作业和食谱。我在使用稳定基线实现 DQN 时遇到问题。在自定义环境中，我发现 PPO 表现良好，但 DQN 很糟糕。我已经测试过更改超参数。可能是什么问题呢？有什么我错过的吗？
我有一个遵循稳定基线3和健身房（体育馆）界面的自定义环境，但从稳定基线导入的 DQN 模型与 PPO 相比表现非常糟糕。通过将 DQN 和 PPO 与 FIFO（先进先出）和 EDD（最早到期日）等模型进行比较来完成评估。 DQN 的表现甚至比这些传统模型更差。]]></description>
      <guid>https://stackoverflow.com/questions/78141506/job-scheduling-using-stable-baselines3</guid>
      <pubDate>Mon, 11 Mar 2024 14:48:21 GMT</pubDate>
    </item>
    <item>
      <title>结合 Savee 和 Crema-d 音频数据集常见情感的 Python 代码 [关闭]</title>
      <link>https://stackoverflow.com/questions/78141428/python-code-combining-common-emotions-from-savee-and-crema-d-audio-dataset</link>
      <description><![CDATA[https://www.kaggle.com/datasets/ejlok1/cremad
https://www.kaggle.com/datasets/ejlok1/萨里视听表达情感保存
将这些由不同感官组成的数据集中的共同情感结合起来。
您能分享带有组合声音集的 rar 文件吗？
如何将这些数据集中的常见声音与机器学习结合起来？
由于我不懂机器学习，所以尝试使用Python，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78141428/python-code-combining-common-emotions-from-savee-and-crema-d-audio-dataset</guid>
      <pubDate>Mon, 11 Mar 2024 14:34:37 GMT</pubDate>
    </item>
    <item>
      <title>在张量流中重命名模型输出名称</title>
      <link>https://stackoverflow.com/questions/78141102/rename-model-output-names-in-tensorflow</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78141102/rename-model-output-names-in-tensorflow</guid>
      <pubDate>Mon, 11 Mar 2024 13:47:17 GMT</pubDate>
    </item>
    <item>
      <title>如何在 M1 MacBook Pro (Max) 上优化 Tensorflow/Keras 训练？</title>
      <link>https://stackoverflow.com/questions/78141065/how-to-optimize-tensorflow-keras-trainings-on-a-m1-macbook-pro-max</link>
      <description><![CDATA[我想知道如何优化配备 32GB RAM 的 M2 MB Pro Max 上的训练时间。有没有办法参数化TF框架？或者我可以插入 NVIDIA CUDA GPU 来加速该过程吗？
您有什么建议以及如何做？或者您只是在本地开发并在其他地方进行训练？
我想加快机器上的训练时间。我正在使用 TF 2.15.0 &amp; conda 环境中的 Python 3.11.5。]]></description>
      <guid>https://stackoverflow.com/questions/78141065/how-to-optimize-tensorflow-keras-trainings-on-a-m1-macbook-pro-max</guid>
      <pubDate>Mon, 11 Mar 2024 13:42:12 GMT</pubDate>
    </item>
    <item>
      <title>预测分位数与梯度增强回归相交</title>
      <link>https://stackoverflow.com/questions/78140825/prediction-quantiles-intersect-from-gradient-boosted-regression</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78140825/prediction-quantiles-intersect-from-gradient-boosted-regression</guid>
      <pubDate>Mon, 11 Mar 2024 13:05:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 Power Automate 将我的 Excel 输入连接到 Azure ML Studio API 端点并进行预测</title>
      <link>https://stackoverflow.com/questions/78140665/using-power-automate-to-connect-my-excel-inputs-to-azure-ml-studio-api-endpoint</link>
      <description><![CDATA[我正在研究如何使用 Power Automate 配置一个流程，该流程可以将我的 Excel 文件输入与我的 Azure ML Studio API 端点连接起来。理想情况下，当添加或修改 Excel 中的某一列值时，将触发此流程。
Excel 输入
基于 iris 数据集，我在 Azure ML Studio 中使用 API 端点创建了一个简单的机器学习模型。该模型需要 4 个输入，并根据这些输入返回预测标签。
将数据输入到测试端点
我创建了一个 Excel 文件，其中有 4 列（sepal_length、sepal_width、petal_length、petal_width），每列都包含一个值，如 Excel 输入屏幕截图中所示。
在 Power Automate 中，我能够使用“列出表中存在的行”操作连接到我的 Excel 文件。在此操作中，我可以找到我的表“Table1”。
我应该在 Power Automate 中采取哪些后续步骤来连接到 API 并能够接收预测结果？
这就是我的流程当前的样子。 当前流程
因此，我在将输入数据转换为可以作为 API 输入提供的格式时遇到了一些困难。下面的屏幕截图显示了我如何使用 Bearer 令牌和特定 URI 连接到我的 API。
API 连接]]></description>
      <guid>https://stackoverflow.com/questions/78140665/using-power-automate-to-connect-my-excel-inputs-to-azure-ml-studio-api-endpoint</guid>
      <pubDate>Mon, 11 Mar 2024 12:36:57 GMT</pubDate>
    </item>
    <item>
      <title>对模型进行 GAN 训练，根据 4 个人体测量输入（身高、腰部、胸部、臀部）生成 3D 模型</title>
      <link>https://stackoverflow.com/questions/78140368/gan-training-of-a-model-to-generate-3d-models-from-4-anthropometric-inputheight</link>
      <description><![CDATA[# 训练循环
纪元数 = 100
对于范围内的纪元（num_epochs）：
    对于 train_dataloader 中的批次：
        真实数据 = 批次
        fake_data = Generator(anthropometric_tensor) # 替换为实际输入

        # 训练鉴别器
        optim_d.zero_grad()
        real_labels = torch.ones(real_data.size(0), 1)
        fake_labels = torch.zeros(fake_data.size(0), 1)
        loss_real = 标准(鉴别器(real_data), real_labels)
        loss_fake = 标准(鉴别器(fake_data.detach()), fake_labels)
        loss_D = loss_real + loss_fake
        loss_D.backward()
        优化器_D.step()

        # 训练生成器
        Optimizer_G.zero_grad()
        loss_G = 标准（鉴别器（假数据），真实标签）
        loss_G.backward()
        优化器_G.step()

    print(f&quot;Epoch [{epoch}/{num_epochs}] Loss_D: {loss_D.item()} Loss_G: {loss_G.item()}&quot;)

我明白了
运行时错误：形状“[-1, 37500]”对于大小 64 的输入无效
]]></description>
      <guid>https://stackoverflow.com/questions/78140368/gan-training-of-a-model-to-generate-3d-models-from-4-anthropometric-inputheight</guid>
      <pubDate>Mon, 11 Mar 2024 11:43:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 Huggingface MT5 模型中执行批量编码时会得到不同的嵌入？</title>
      <link>https://stackoverflow.com/questions/78139855/why-do-i-get-different-embeddings-when-i-perform-batch-encoding-in-huggingface-m</link>
      <description><![CDATA[我正在尝试使用 HuggingFace 的 mt5-base 模型对一些文本进行编码。我使用的模型如下所示
从转换器导入 MT5EncoderModel、AutoTokenizer

模型 = MT5EncoderModel.from_pretrained(“google/mt5-base”)
tokenizer = AutoTokenizer.from_pretrained(“google/mt5-base”)

def get_t5_embeddings(文本):
    last_hidden_​​state = model(input_ids=tokenizer(texts, return_tensors=“pt”, padding=True).input_ids).last_hidden_​​state
    pooled_sentence = torch.max(last_hidden_​​state, 暗淡=1)
    返回 pooled_sentence[0].detach().numpy()

当我注意到相同的文本与其自身的余弦相似度分数较低时，我正在做一些实验。我做了一些挖掘，意识到如果我批量进行编码，模型会返回非常不同的嵌入。为了验证这一点，我运行了一个小实验，逐步生成 Hello 的嵌入和 10 个 Hello 的列表。并检查列表中 Hello 和第一个 Hello 的嵌入（两者应该相同）。
对于范围 (1, 10) 内的 i：
    print(i, (get_t5_embeddings([“你好”])[0] == get_t5_embeddings([“你好”]*i)[0]).sum())

这将返回嵌入中相互匹配的值的数量。
结果是这样的：
&lt;前&gt;&lt;代码&gt;1 768
2 768
3 768
4 768
5 768
6 768
7 768
8 27
9 27

每次运行它时，如果批量大小超过 768，就会出现不匹配情况。
为什么我会得到不同的嵌入以及如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78139855/why-do-i-get-different-embeddings-when-i-perform-batch-encoding-in-huggingface-m</guid>
      <pubDate>Mon, 11 Mar 2024 10:14:53 GMT</pubDate>
    </item>
    <item>
      <title>Predict_proba() 给出的概率为 0 和 1，但中间值很少</title>
      <link>https://stackoverflow.com/questions/78139504/predict-proba-giving-probabilities-as-0s-and-1s-but-few-intermediate-values</link>
      <description><![CDATA[我正在研究乳腺癌检测分类问题。我已经从 Kaggle 下载了数据集： https://www.kaggle.com/数据集/yasserh/breast-cancer-dataset
我想预测：
a) 肿瘤是良性还是恶性
和
b) 肿瘤恶性的概率（0-1）是多少。
我正在实现随机森林分类器。
我面临的问题是，当我使用 rf_classifier.predict_proba() 方法时，我获得的概率包含大量 1 和 0，但中间值很少。理想情况下，我希望概率列中的所有值都是 0 到 1 之间的小数。
这种方法是实现目标的正确方法吗？如果是，如何解决这个问题？
分类器表现非常好。
这是我的代码的相关部分：
X_train、X_test、y_train、y_test = train_test_split(X、y、test_size=0.2)
定标器=标准定标器()

X_train = 缩放器.fit_transform(X_train)
X_test = 缩放器.transform(X_test)

rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_train, y_train)

y_pred = rf_classifier.predict(X_test)

y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]

结果 = np.column_stack((y_test[:200], y_pred[:200], y_pred_proba[:200]))
np.set_printoptions(精度=2, 抑制=True)
print(&quot;实际|预测|概率&quot;)
打印（结果）

输出：

分类报告：
]]></description>
      <guid>https://stackoverflow.com/questions/78139504/predict-proba-giving-probabilities-as-0s-and-1s-but-few-intermediate-values</guid>
      <pubDate>Mon, 11 Mar 2024 09:13:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 绘制梯度下降曲线</title>
      <link>https://stackoverflow.com/questions/78137739/plotting-of-gradient-descent-curves-by-using-keras</link>
      <description><![CDATA[我在 Keras 中实现了以下代码，该代码使用加州住房数据集，试图绘制 theta 1 和 theta 2 的值，以及随机梯度下降、批量梯度或小批量的选择如何影响结果： 
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
从 keras.models 导入顺序
从 keras.layers 导入密集
从 keras.optimizers 导入 SGD
从 sklearn.datasets 导入 fetch_california_housing
从 sklearn.preprocessing 导入 StandardScaler

# 加载加州住房数据集
加州住房 = fetch_加州住房()
X, y = california_housing.data, california_housing.target

# 标准化特征
定标器=标准定标器()
X_归一化 = 缩放器.fit_transform(X)

def build_model():
    模型=顺序（[
    密集（64，激活=“relu”，input_shape=（8，）），
    密集(64，激活=“relu”)，
    密集(1)
    ]）
    #model.compile（优化器=“rmsprop”，损失=“mse”，指标=[“mae”]）
    返回模型


sgd_optimizer = SGD(lr=0.001) # 随机梯度下降
minibatch_sgd_optimizer = SGD(lr=0.001) # 小批量梯度下降
batch_sgd_optimizer = SGD(lr=0.001) # 批量梯度下降


# 编译模型
模型=build_model()
model.compile(loss=&#39;mse&#39;, 优化器=sgd_optimizer)

theta1_sgd、theta2_sgd = []、[]
theta1_minibatch_sgd、theta2_minibatch_sgd = []、[]
theta1_batch_sgd、theta2_batch_sgd = []、[]


# 执行梯度下降并存储 theta 值的函数
def Perform_gradient_descent（优化器，batch_size=None）：
    theta1_列表、theta2_列表 = []、[]
    损失历史记录 = []
    for _ in range(5): # 纪元数
        历史记录=model.fit(X_normalized, y, epochs=1,batch_size=batch_size, verbose=0)
        loss_history.append(history.history[&#39;loss&#39;][0])
        weights = model.layers[0].get_weights()[0].flatten() # 获取当前 theta 值
        theta1_list.append(权重[0])
        theta2_list.append(权重[1])
    打印（theta1_列表，“”，theta2_列表）
    返回loss_history，theta1_list，theta2_list

# 使用不同的优化器执行梯度下降
loss_sgd, theta1_sgd, theta2_sgd = Perform_gradient_descent(sgd_optimizer, batch_size=1) # 随机梯度下降
loss_minibatch_sgd, theta1_minibatch_sgd, theta2_minibatch_sgd = Perform_gradient_descent(minibatch_sgd_optimizer, batch_size=32) # 小批量梯度下降
loss_batch_sgd, theta1_batch_sgd, theta2_batch_sgd = Perform_gradient_descent(batch_sgd_optimizer, batch_size=len(X_normalized)) # 批量梯度下降

# 绘制损失与纪元数的关系图
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(loss_sgd) + 1), loss_sgd, label=&#39;随机梯度下降&#39;)
plt.plot(range(1, len(loss_minibatch_sgd) + 1), loss_minibatch_sgd, label=&#39;小批量梯度下降&#39;)
plt.plot(range(1, len(loss_batch_sgd) + 1), loss_batch_sgd, label=&#39;批量梯度下降&#39;)
plt.xlabel(&#39;历元数&#39;)
plt.ylabel(&#39;损失&#39;)
plt.title(&#39;损失与历元数&#39;)
plt.图例()
plt.网格（真）
plt.show()

# 绘制梯度下降轨迹
plt.figure(figsize=(10, 6))
plt.plot(theta1_sgd,theta2_sgd,label=&#39;随机梯度下降&#39;,marker=&#39;o&#39;)
plt.plot(theta1_minibatch_sgd, theta2_minibatch_sgd, label=&#39;小批量梯度下降&#39;,marker=&#39;s&#39;)
plt.plot(theta1_batch_sgd, theta2_batch_sgd, label=&#39;批量梯度下降&#39;,marker=&#39;x&#39;)
plt.xlabel(&#39;Theta 1&#39;)
plt.ylabel(&#39;Theta 2&#39;)
plt.title(&#39;梯度下降轨迹&#39;)
#plt.xlim(-0.08, -0.05) # 设置 Theta 1 的限制
#plt.ylim(0.02, 0.03) # 设置 Theta 2 的限制
plt.图例()
plt.网格（真）
plt.show()

但是，我发现的问题是，有时保存 theta 值的列表的值是 Nan，而在其他情况下是正常值。当 epoch 数量增加到 10 以上时，我注意到了这一点，这是为什么？]]></description>
      <guid>https://stackoverflow.com/questions/78137739/plotting-of-gradient-descent-curves-by-using-keras</guid>
      <pubDate>Sun, 10 Mar 2024 22:46:54 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 模型未进行训练</title>
      <link>https://stackoverflow.com/questions/45359111/pytorch-model-is-not-training</link>
      <description><![CDATA[我有一个问题，一周内都无法解决。我正在尝试构建 CIFAR-10 分类器，但是每批之后的损失值都是随机跳跃的，即使在同一批次上，准确性也没有提高（我什至不能用一批来过度拟合模型），所以我猜唯一可能的原因is - 权重没有更新。 
我的模块类
类 Net(nn.Module):
def __init__(自身):
    超级（网络，自我）.__init__()
    self.conv_pool = nn.Sequential(
        nn.Conv2d(3, 64, 3, 填充=1),
        ReLU(),
        nn.MaxPool2d(2, 2),
        nn.Conv2d(64, 128, 3, 填充=1),
        ReLU(),
        nn.MaxPool2d(2, 2),
        nn.Conv2d(128, 256, 3, 填充=1),
        ReLU(),
        nn.MaxPool2d(2, 2),
        nn.Conv2d(256, 512, 3, 填充=1),
        ReLU(),
        nn.MaxPool2d(2, 2),
        nn.Conv2d(512, 512, 1),
        ReLU(),
        nn.MaxPool2d(2, 2))

    self.fcnn = nn.Sequential(
        nn.线性(512, 2048),
        ReLU(),
        nn.线性(2048, 2048),
        ReLU(),
        nn.线性(2048, 10)
    ）

def 前向（自身，x）：
    x = self.conv_pool(x)
    x = x.view(-1, 512)
    x = self.fcnn(x)
    返回x

我正在使用的优化器：

&lt;前&gt;&lt;代码&gt;net = Net()
标准 = nn.CrossEntropyLoss()
优化器 = optim.SGD(net.parameters(), lr=0.001, 动量=0.9)

我的火车功能：
def train():
for epoch in range(5): # 多次循环数据集
    对于范围内的 i（0，df_size）：
        # 获取数据

        尝试：
            图像、标签 = loadBatch(ds, i)
        除了基本异常：
            继续

        ＃ 裹
        输入=变量（图像）

        优化器.zero_grad()

        输出 = 净值（输入）

        损失=标准（输出，变量（标签））

        loss.backward()
        优化器.step()
        acc = 测试（图像，标签）
        print(&quot;损失：&quot; + str(loss.data[0]) + &quot; 准确率 %：&quot; + str(acc) + &quot; 迭代：&quot; + str(i))

        如果我%40==39：
            torch.save(net.state_dict(), &quot;model_save_cifar&quot;)

    print(&quot;完成纪元&quot; + str(纪元))

我使用batch_size = 20，image_size = 32 (CIFAR-10)
loadBatch 函数返回图像的 LongTensor 20x3x32x32 和标签的 LongTensor 20x1 的元组
如果您能帮助我，或者提出可能的解决方案，我会非常高兴（我猜测这是因为 NN 中的顺序模块，但我传递给优化器的参数似乎是正确的）]]></description>
      <guid>https://stackoverflow.com/questions/45359111/pytorch-model-is-not-training</guid>
      <pubDate>Thu, 27 Jul 2017 19:07:27 GMT</pubDate>
    </item>
    </channel>
</rss>