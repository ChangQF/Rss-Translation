<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 23 Mar 2024 12:24:53 GMT</lastBuildDate>
    <item>
      <title>动态设置SelectKBest的K值</title>
      <link>https://stackoverflow.com/questions/78210795/dynamically-set-k-value-of-selectkbest</link>
      <description><![CDATA[我在管道中使用 SelectKBest，并且希望能够使用 config.ini 文件配置要选择的功能数量。所以基本上在 .ini 文件中我有这个：
# FeatureSelection：设置要选择的特征数量 [FeatureSelection] nb_features = 10000 # 使用 chi2 进行 Kbest 特征选择时要选择的特征数量（整数或全部，all 将保留所有特征，因此不执行特征选择）
所以，问题是，如果我使用的数据输入不足以提取 10 000 个特征，selectKBest 将遇到问题：
ValueError：k 应该 &lt;= n_features = 4873；得到 10000。使用 k=&#39;all&#39; 返回所有特征。
这是正常的，因为它找不到足够的特征来返回 10 000 个特征。现在我正在考虑两种方法，但我不知道哪一种更容易实现并且最准确？

动态设置 k_value，以便如果它高于可用特征的数量，则将其设置为“全部”以便它选择最大数量的特征。否则，将 k_value 保留为 config.ini 文件中设置的值。

根据训练期间提取的比率设置 k_value。想象一下，您在训练 (.fit) 过程中提取 12 000 个特征并保留 10 000 个，则比率为 5/6。因此，这意味着在对验证数据进行预测时，您将保留 5/6 * 可用的功能。假设是 6000，您可以将 k_value 设置为 5000，以最终保持所选特征的相同比例。然而，应该有一个保证，以确定该比率是否大于1。 1 应该将 k_value 设置为“all”另外，因为您无法选择比提取的特征更多的特征。


你会推荐什么？如果有人能帮我解决这个问题，我将非常感激：D
感谢您的宝贵时间！]]></description>
      <guid>https://stackoverflow.com/questions/78210795/dynamically-set-k-value-of-selectkbest</guid>
      <pubDate>Sat, 23 Mar 2024 12:08:23 GMT</pubDate>
    </item>
    <item>
      <title>无法从“jax”导入名称“linear_util”</title>
      <link>https://stackoverflow.com/questions/78210393/cannot-import-name-linear-util-from-jax</link>
      <description><![CDATA[我正在尝试重现S5模型的实验，https://github.com/lindermanlab/ S5，但是在解决环境的时候遇到了一些问题。当我运行 shell 脚本./run_lra_cifar.sh时，出现以下错误
回溯（最近一次调用最后一次）：
  文件“/Path/S5/run_train.py”，第3行，在&lt;module&gt;中。
    从 s5.train 导入火车
  文件“/Path/S5/s5/train.py”，第7行，在&lt;module&gt;中。
    从.train_helpers导入create_train_state，reduce_lr_on_plateau，\
  文件“/Path/train_helpers.py”，第 6 行，在  中。
    从 flax.training 导入 train_state
  文件“/Path/miniconda3/lib/python3.12/site-packages/flax/__init__.py”，第 19 行，在  中
    从 。导入核心
  文件“/Path/miniconda3/lib/python3.12/site-packages/flax/core/__init__.py”，第 15 行，在  中
    从 .axes_scan 导入广播
  文件“/Path/miniconda3/lib/python3.12/site-packages/flax/core/axes_scan.py”，第 22 行，在  中
    从 jax 导入 Linear_util 作为 lu
ImportError：无法从“jax”导入名称“linear_util”（/Path/miniconda3/lib/python3.12/site-packages/jax/__init__.py）

我在 RTX4090 上运行它，我的 CUDA 版本是 11.8。我的jax版本是0.4.25，jaxlib版本是0.4.25+cuda11.cudnn86
我首先尝试使用作者的安装依赖项
pip install -rrequirements_gpu.txt

但是，这似乎不适用于我的情况，因为我什至无法导入 jax。所以我根据 https://jax.readthedocs.io/en 上的说明安装了 jax /latest/installation.html
通过输入
pip install --upgrade pip
pip install --upgrade “jax[cuda11_pip]” -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html

有谁知道可能出了什么问题吗？感谢任何帮助]]></description>
      <guid>https://stackoverflow.com/questions/78210393/cannot-import-name-linear-util-from-jax</guid>
      <pubDate>Sat, 23 Mar 2024 09:57:12 GMT</pubDate>
    </item>
    <item>
      <title>如何将预训练的拥抱脸模型转换为.pt并在本地完全运行？</title>
      <link>https://stackoverflow.com/questions/78210297/how-to-convert-pretrained-hugging-face-model-to-pt-and-run-it-fully-locally</link>
      <description><![CDATA[我正在尝试将此模型转换为.pt格式。它对我来说工作得很好，所以我不想对其进行微调。如何将其导出为.pt并运行界面？
我尝试使用它转换为 .pt：
从变压器导入 AutoConfig、AutoProcessor、AutoModelForCTC、AutoTokenizer、Wav2Vec2Processor
导入库
进口火炬



# 定义模型名称
model_name = “UrukHan/wav2vec2-俄罗斯”

# 加载模型和分词器
config = AutoConfig.from_pretrained(model_name)
模型 = AutoModelForCTC.from_pretrained(model_name, config=config)
处理器 = Wav2Vec2Processor.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 将模型保存为.pt 文件
torch.save(model.state_dict(), &quot;model.pt&quot;)

# 如果需要的话也保存分词器
tokenizer.save_pretrained(“模型标记器”)

但不幸的是它没有运行界面：
model = AutoModelForCTC.from_pretrained(“model.pt”)
处理器 = AutoProcessor.from_pretrained(“model.pt”)


# 使用模型进行推理
FILE = &#39;这里是 wav.wav&#39;
音频，_ = librosa.load（文件，sr = 16000）
音频=列表（音频）
def map_to_result(batch):
  使用 torch.no_grad()：
    input_values = torch.tensor(batch, device=“cpu”).unsqueeze(0) #, device=“cuda”
    logits = 模型(input_values).logits
  pred_ids = torch.argmax(logits, dim=-1)
  批处理=处理器.batch_decode(pred_ids)[0]
  退货批次
映射到结果（音频）
打印（映射到结果（音频））


模型.eval()

并遇到错误：
`model.pt 不是本地文件夹，也不是“https://huggingface.co/models”上列出的有效模型标识符
`]]></description>
      <guid>https://stackoverflow.com/questions/78210297/how-to-convert-pretrained-hugging-face-model-to-pt-and-run-it-fully-locally</guid>
      <pubDate>Sat, 23 Mar 2024 09:18:49 GMT</pubDate>
    </item>
    <item>
      <title>我做了什么来纠正属性错误。请帮助我[关闭]</title>
      <link>https://stackoverflow.com/questions/78210052/what-did-i-do-to-correct-the-attribute-error-please-help-me</link>
      <description><![CDATA[&lt;前&gt;&lt;代码&gt;batch_size = 100
对于范围 (25) 内的 i：
    num_batches = int(mnist.train.num_examples/batch_size)
    总成本 = 0
    对于范围内的 j（num_batches）：
        batch_x, batch_y = mnist.train.next_batch(batch_size)
        c, _ = sess.run([成本,优化], feed_dict={x:batch_x, y:batch_y, keep_prob:0.8})
        总成本 += c
    打印（总成本）

属性错误
                            回溯（最近一次调用最后一次）
单元格 In[65]，第 3 行
      1 批量大小 = 100
      2 对于范围 (25) 内的 i：
----&gt; 3 num_batches = int(mnist.train.num_examples/batch_size)
      4 总成本 = 0
      5 对于 j 在范围内（num_batches）：

AttributeError：模块“keras.datasets.mnist”没有属性“train”
]]></description>
      <guid>https://stackoverflow.com/questions/78210052/what-did-i-do-to-correct-the-attribute-error-please-help-me</guid>
      <pubDate>Sat, 23 Mar 2024 07:25:50 GMT</pubDate>
    </item>
    <item>
      <title>用户警告：RNN 模块权重不是单个连续内存块的一部分</title>
      <link>https://stackoverflow.com/questions/78209777/userwarning-rnn-module-weights-are-not-part-of-single-contiguous-chunk-of-memor</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78209777/userwarning-rnn-module-weights-are-not-part-of-single-contiguous-chunk-of-memor</guid>
      <pubDate>Sat, 23 Mar 2024 04:48:10 GMT</pubDate>
    </item>
    <item>
      <title>形状值与模型预测之间的差异</title>
      <link>https://stackoverflow.com/questions/78209765/discrepency-between-shap-value-versus-model-prediction</link>
      <description><![CDATA[我使用 shap.Explainer 使用 XgboostRegressor 模型，
我在 pyspark 中应用了 udf，以便在数据很大时可以进行扩展。
我正在使用最新的 shap 库和 Xgboost 以及 Spark 版本 3.2。
下面的示例代码
feature_list = model.feature_names_in_
对于迭代器中的 X：
    解释器 = shap.TreeExplainer(模型, X.sample(20), feature_perturbation = &#39;干预&#39;)
    shap_values = 解释器(X)
    yield pd.concat(pd.Dataframe( shap_values.values, column = [feature + &#39;CONTRIBUTION&#39; for col in feature_list]),
                    pd.Dataframe( shap_values.base_values, 列 = [&#39;BASE_VALUE&#39;]))

model_prediction和shap值之间的关系是
model_prediction = feature1_contribution + feature2_contribution +... + featuren_contribution + base_value
假设对于一种组合，我的 model_prediction = 20
但我的一项功能贡献约为 50，但其他功能贡献在 -1 &lt;= feature_contribution &lt;= 1 之间。
这是什么意思？我已经检查过我使用的是与形状和模型预测相同的模型。
我做错了什么？任何建议将不胜感激。
谢谢。
我尝试过的方法
而不是解释器(X)
我尝试过 explainer.shap_values(X) 但这不起作用。
我尝试过 Explainer，而不是 TreeExplainer]]></description>
      <guid>https://stackoverflow.com/questions/78209765/discrepency-between-shap-value-versus-model-prediction</guid>
      <pubDate>Sat, 23 Mar 2024 04:41:50 GMT</pubDate>
    </item>
    <item>
      <title>本地机器上的图像分类，但偶数纪元明显快于奇数纪元</title>
      <link>https://stackoverflow.com/questions/78209594/image-classification-on-local-machine-but-even-numbered-epoch-are-significantly</link>
      <description><![CDATA[我正在使用 Visual Studio 代码在本地计算机上进行图像分类训练，但每个偶数纪元都比奇数纪元快得多，以前有人遇到过这个问题吗？我在谷歌上到处找，但似乎没有人提到这种不规则现象。
因此，我无法正确集成早期停止回调，因为偶数纪元的 val_loss 被搞乱了。我尝试在 google collab 中运行，效果很好，但免费版本有限，有什么想法可以解决这个问题吗？ 我的基线模型的训练结果图像
对于我更复杂的模型，这个问题仍然存在。
# 定义一个 EarlyStopping 回调
Early_stopping = 回调.EarlyStopping(
    Monitor=&#39;val_loss&#39;, # 监控提前停止的指标（验证损失）
    Patient=5, # 停止前没有改善的 epoch 数
    min_delta=1e-7, # 被视为改进的监控指标的最小变化
    verbose=1, # 详细级别（1 表示更新）
    Restore_best_weights=True, # 停止时将模型权重恢复到最佳状态
）

# 定义一个ReduceLROnPlateau回调
高原 = 回调.ReduceLROnPlateau(
    Monitor=&#39;val_loss&#39;, # 监控学习率降低的指标（验证损失）
    Factor=0.5, # 学习率降低的因子（例如，0.2 表示 lr *= 0.2）
    Patient=2, # 降低学习率之前没有改善的 epoch 数
    min_delta=1e-6, # 触发减少的监控指标的最小变化
    Cooldown=0, # 减少后恢复正常操作之前等待的纪元数
    verbose=1 # 详细级别（1 表示更新）
）

def 基线CNN模型():
  # 输入层
  输入=输入（形状=（图像H，图像W，3））

  # 卷积层
  x = Conv2D(32, (3, 3), 激活=&#39;relu&#39;)(输入)
  x = MaxPooling2D((2, 2))(x)
  x = Conv2D(64, (3, 3), 激活=&#39;relu&#39;)(x)
  x = MaxPooling2D((2, 2))(x)

  # 压平层
  x = 展平()(x)

  # 全连接层
  x = 密集(64, 激活=&#39;relu&#39;)(x)
  输出=密集（1，激活=&#39;sigmoid&#39;）（x）

  # 创建模型
  模型=模型（输入=[输入]，输出=输出）

  返回模型

# 清除Keras会话以释放资源
keras.backend.clear_session()

# 使用定义的 &#39;baseline_model&#39; 函数创建 CNN 模型
基线CNN = 基线CNN模型()

# 使用指定的损失、优化器和指标编译 CNN 模型
基线CNN.编译（
    loss=&#39;binary_crossentropy&#39;, # 二元分类的二元交叉熵损失
    Optimizer=keras.optimizers.Adam(), # 具有自定义学习率的 Adam 优化器
    metrics=[&#39;binary_accuracy&#39;] # 训练期间监控的指标（二进制精度）
）

# 显示模型架构的摘要
基线CNN.summary()

# 训练模型
bCNNHist = 基线CNN.fit(trainDS,
          验证数据=valDS，
          纪元=10，
          批量大小=批量大小，
          callbacks=[plateau], #early_stopping, 提前停止和降低学习率的回调
          steps_per_epoch=int(len(trainDF)/batchSize), # 每个训练周期的步数
          validation_steps=int(len(valDF) / batchSize) # 每个验证时期的步骤数)
）
]]></description>
      <guid>https://stackoverflow.com/questions/78209594/image-classification-on-local-machine-but-even-numbered-epoch-are-significantly</guid>
      <pubDate>Sat, 23 Mar 2024 02:41:21 GMT</pubDate>
    </item>
    <item>
      <title>TPU 连接问题 训练 TF 模型 Google Colab</title>
      <link>https://stackoverflow.com/questions/78209293/tpu-connectivity-issue-training-tf-model-google-colab</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78209293/tpu-connectivity-issue-training-tf-model-google-colab</guid>
      <pubDate>Fri, 22 Mar 2024 23:57:10 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用训练集还是验证集来进行参数优化？</title>
      <link>https://stackoverflow.com/questions/78209231/should-i-use-training-or-validation-set-for-parameter-otimization</link>
      <description><![CDATA[我正在使用决策树和参数优化来训练模型。
我了解到验证集的目标是评估训练期间的模型性能并帮助调整参数。
考虑到这一点，我不应该使用 grid_search.fit 上的验证集而不是训练集吗？
param_grid = {
    &#39;最大深度&#39;: [3, 5, 7, 10],
    &#39;min_samples_split&#39;: [2, 5, 10],
    &#39;min_samples_leaf&#39;: [1, 2, 4]
}

clf = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(clf, param_grid, cv=5, 评分=&#39;准确度&#39;)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print(&quot;最佳参数：&quot;, best_params)
打印(“\n”)

＃验证
best_clf = grid_search.best_estimator_
val_accuracy = best_clf.score(X_val, y_val)
print(“最佳模型的验证准确度：”, val_accuracy)
打印(“\n”)

＃测试
y_test_pred = best_clf.predict(X_test)
test_accuracy = precision_score(y_test, y_test_pred)
test_ precision = precision_score(y_test, y_test_pred)
test_recall=recall_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred)
print(“具有最佳模型的测试集上的决策树测量：”)
print(&quot;准确度：&quot;, test_accuracy)
print(&quot;精度：&quot;, test_ precision)
print(&quot;召回：&quot;, test_recall)
print(&quot;F1 分数：&quot;, test_f1)
打印(“---------------------------------------------- ---------”）
]]></description>
      <guid>https://stackoverflow.com/questions/78209231/should-i-use-training-or-validation-set-for-parameter-otimization</guid>
      <pubDate>Fri, 22 Mar 2024 23:27:32 GMT</pubDate>
    </item>
    <item>
      <title>协议错误 - 连接中止</title>
      <link>https://stackoverflow.com/questions/78208623/protocolerror-connection-aborted</link>
      <description><![CDATA[所以我正在制作一个 NLP 项目，其中我有效地进行了数据验证和数据转换，但是当涉及到模型训练时，它抛出了这个错误。我正在尝试从 Hugging-face 获取 cnn pegasus 模型，但它抛出此错误我使用相同的 api 进行数据转换并且它有效，但它抛出此错误，而模型火车请帮助我被困在这里。
我尝试了从禁用防火墙到良好的互联网连接的所有方法。还尝试从 Huggingface 下载模型，然后获取它，但我收到了有关不同权重的警告。]]></description>
      <guid>https://stackoverflow.com/questions/78208623/protocolerror-connection-aborted</guid>
      <pubDate>Fri, 22 Mar 2024 20:07:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么 torch.nn.function.linear 中重量的维度是 (out,in) 而不是 (in,out)</title>
      <link>https://stackoverflow.com/questions/78208603/why-are-the-dimension-of-the-weight-in-torch-nn-functional-linear-out-in-inste</link>
      <description><![CDATA[在 torch.nn.function.linear 的文档中（https://pytorch.org/docs/stable/ generated/torch.nn.function.linear.html），权重输入的维度为（out_features，in_features），然后计算时对权重矩阵进行转置输出：y=xA^T+b。为什么他们这样做而不是采用维度矩阵 W（in_features、out_features）并执行 y=xW+b？
通过执行 y=xW+b 尺寸将匹配，因此我找不到上述的明确原因。]]></description>
      <guid>https://stackoverflow.com/questions/78208603/why-are-the-dimension-of-the-weight-in-torch-nn-functional-linear-out-in-inste</guid>
      <pubDate>Fri, 22 Mar 2024 20:01:42 GMT</pubDate>
    </item>
    <item>
      <title>Azure ML Studio Web 服务始终返回相同的预测</title>
      <link>https://stackoverflow.com/questions/78207131/azure-ml-studio-web-service-always-returns-the-same-prediction</link>
      <description><![CDATA[我目前正在为我的课程开发一个小型项目，但遇到了障碍，希望能得到一些帮助。在 Azure 机器学习工作室中训练 SVM 模型并将其部署为 Web 服务后，我遇到了一个特殊问题 - 无论输入数据如何，该服务都会返回相同的预测。
可能出了什么问题？
https://gallery.cortanaintelligence.com/Experiment/Binary-Classifier -SVM-Web
https://gallery.cortanaintelligence.com/Experiment/Binary-Classifiers-SVM
这是我到目前为止所做的事情：

确保所有预处理步骤与模型训练阶段使用的步骤相同（包括数据标准化和缺失值处理）。
仔细检查模型是否使用输入数据进行评分。
验证了网络服务的配置，特别是在构建响应方面，以确保不返回任何静态值。
确保输入数据的格式与预期架构匹配。该模型在 ML Studio 环境中表现良好，并根据测试数据进行准确预测。但是，部署的 Web 服务似乎并未反映此行为并输出恒定值。
]]></description>
      <guid>https://stackoverflow.com/questions/78207131/azure-ml-studio-web-service-always-returns-the-same-prediction</guid>
      <pubDate>Fri, 22 Mar 2024 15:06:25 GMT</pubDate>
    </item>
    <item>
      <title>R 混淆矩阵 - 错误：“数据”和“参考”应该是具有相同级别的因素[关闭]</title>
      <link>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</link>
      <description><![CDATA[尽管还有其他针对相同错误消息的报告，但没有一个对我的情况有帮助。
我已经准备了自己的数据，分割如下，但无法获得混淆矩阵。
test_index &lt;- createDataPartition(y =workingData$PM10, times = 1, p = 0.5, list = FALSE)
train_set &lt;-工作数据[-test_index,]
test_set &lt;-工作数据[test_index,]

train_knn &lt;- train(PM10 ~. , method= &quot;knn&quot; , data = train_set)

y_hatknn &lt;- 预测(train_knn, train_set, type = “raw”)

fusionMatrix(y_hatknn, test_set$PM10)

上面最后一行给出
错误：“data”和“reference”应该是具有相同级别的因素。

我想上传数据进行复制，但可以提供基本的：
&lt;前&gt;&lt;代码&gt;str（工作数据）
“数据帧”：3653 obs。 3 个变量：
&#39; $ 日期 : 数字 2e+07 2e+07 2e+07 2e+07 2e+07 ...
&#39; $ Rain_mm: 数字 0.1 6.7 0 1.4 0.8 1.8 15.3 0 2.6 3.8 ...
&#39; $ PM10 : 数字 -1 -1 -1 -1 -1 ...

PM10 是污染 PM10 水平。
如何解决？
添加更多信息：
在原始错误之后：
&lt;块引用&gt;
confusionMatrix(y_hatknn, test_set$PM10)
错误：data 和 reference 应该是具有相同水平的因子。

我尝试将其设置为因素...
&lt;块引用&gt;
confusionMatrix(y_hatknn, as.factor(test_set$PM10))
错误：data 和 reference 应该是具有相同水平的因子。

以预测为因素...
&lt;块引用&gt;
confusionMatrix(as.factor(y_hatknn), test_set$PM10)
错误：data 和 reference 应该是具有相同水平的因子。

以两个参数为因素...
&lt;块引用&gt;
confusionMatrix(as.factor(y_hatknn), as.factor(test_set$PM10))
fusionMatrix.default(as.factor(y_hatknn), as.factor(test_set$PM10)) 中的错误：
数据的级别不能多于参考

确实需要得到整理，Stack坚持关闭我的帖子，写下gmail中navarrodan007的解决方案]]></description>
      <guid>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</guid>
      <pubDate>Fri, 22 Mar 2024 09:39:08 GMT</pubDate>
    </item>
    <item>
      <title>scikeras.wrappers.KerasClassifier 返回 ValueError：无法解释指标标识符：loss</title>
      <link>https://stackoverflow.com/questions/78089332/scikeras-wrappers-kerasclassifier-returning-valueerror-could-not-interpret-metr</link>
      <description><![CDATA[我正在研究 KerasClassifier，因为我想将其插入 scikit-learn 管道中，但我收到了前面提到的 ValueError。
以下代码应该能够重现我遇到的错误：
从 sklearn.model_selection 导入 KFold，cross_val_score
从 sklearn.preprocessing 导入 StandardScaler
从 scikeras.wrappers 导入 KerasClassifier
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Dense
从 sklearn.datasets 导入 load_iris
将 numpy 导入为 np

数据 = load_iris()
X = 数据.数据
y = 数据.目标

def create_model():
    模型=顺序（）
    model.add（密集（8，input_dim = 4，激活=&#39;relu&#39;））
    model.add（密集（3，激活=&#39;softmax&#39;））
    model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,
                  优化器=&#39;亚当&#39;,
                  指标=[&#39;准确性&#39;])
    返回模型

clf = KerasClassifier(build_fn=create_model,
                      纪元=100，
                      批量大小=10，
                      详细=1)

管道=管道([
    (&#39;缩放器&#39;, StandardScaler()),
    （&#39;clf&#39;，clf）
]）

kf = KFold(n_splits=5, shuffle=True, random_state=42)
结果= cross_val_score（管道，X，y，cv = kf）
print(&quot;交叉验证准确度：&quot;, np.mean(结果))

似乎我的模型正在随着纪元的运行而被编译。但是，之后我收到错误：
ValueError：无法解释指标标识符：丢失

tensorflow 和 scikeras 库的版本是：
scikeras==0.12.0
张量流==2.15.0

编辑：
最终我尝试了不同的库版本，以下内容让我成功运行了代码，看来问题是由 scikit-learn 的版本引起的：
scikeras==0.12.0
张量流==2.15.0
scikit学习==1.4.1
]]></description>
      <guid>https://stackoverflow.com/questions/78089332/scikeras-wrappers-kerasclassifier-returning-valueerror-could-not-interpret-metr</guid>
      <pubDate>Fri, 01 Mar 2024 17:03:39 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch中的forward函数到底输出什么？</title>
      <link>https://stackoverflow.com/questions/64987430/what-exactly-does-the-forward-function-output-in-pytorch</link>
      <description><![CDATA[此示例逐字取自 PyTorch 文档。现在我确实对深度学习有了一些总体背景，并且知道很明显，forward 调用表示前向传递，穿过不同的层并最终到达终点，在本例中有 10 个输出，然后获取前向传递的输出并使用定义的损失函数计算损失。现在，我忘记了在这种情况下 forward() 传递的输出到底给我带来了什么。
我认为神经网络的最后一层应该是某种激活函数，例如 sigmoid() 或 softmax() ，但我没有看到这些是定义在任何地方，而且，当我现在做项目时，我发现后来调用了 softmax() 。所以我只是想澄清 outputs = net(inputs) 给我的到底是什么 link，在我看来，默认情况下 PyTorch 模型前向传递的输出是 logits？
transform = Transforms.Compose(
    [transforms.ToTensor(),
     变换.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True,
                                        下载=真，变换=变换）
trainloader = torch.utils.data.DataLoader(trainset,batch_size=4,
                                          洗牌=真，num_workers=2）

将 torch.nn 导入为 nn
导入 torch.nn.function 作为 F


类 Net(nn.Module):
    def __init__(自身):
        超级（网络，自我）.__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def 前向（自身，x）：
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        返回x


净=净()

导入 torch.optim 作为 optim

标准 = nn.CrossEntropyLoss()
优化器 = optim.SGD(net.parameters(), lr=0.001, 动量=0.9)

for epoch in range(2): # 多次循环数据集

    运行损失 = 0.0
    对于 i，enumerate(trainloader, 0) 中的数据：
        # 获取输入；数据是[输入，标签]的列表
        输入，标签=数据

        # 将参数梯度归零
        优化器.zero_grad()

        # 前向+后向+优化
        输出 = 净值（输入）
        打印（输出）
        休息
        损失=标准（输出，标签）
        loss.backward()
        优化器.step()

        # 打印统计数据
        running_loss += loss.item()
        if i % 2000 == 1999: # 每 2000 个小批量打印一次
            print(&#39;[%d, %5d] 损失: %.3f&#39; %
                  (epoch + 1, i + 1, running_loss / 2000))
            运行损失 = 0.0

print(&#39;训练完成&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/64987430/what-exactly-does-the-forward-function-output-in-pytorch</guid>
      <pubDate>Tue, 24 Nov 2020 13:21:18 GMT</pubDate>
    </item>
    </channel>
</rss>