<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 15 Jan 2025 15:17:11 GMT</lastBuildDate>
    <item>
      <title>我想以 AI 工程师的身份开始我的职业生涯吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79358614/what-i-want-is-to-start-my-career-in-ai-engineer</link>
      <description><![CDATA[你能帮我吗，我该如何开始我的旅程？
我需要一个路线图来跟进。
我是计算机应用学士（BCA）学生。我计划学习人工智能并使用人工智能开发出色的产品。我真的很喜欢这个领域，我知道它在我们国家尼泊尔正在兴起。
我开始解决数学问题，并学习了一门编程语言 Python。
所以我想要一个完整的详细路线图，这样我就可以遵循它并成为一名人工智能工程师。]]></description>
      <guid>https://stackoverflow.com/questions/79358614/what-i-want-is-to-start-my-career-in-ai-engineer</guid>
      <pubDate>Wed, 15 Jan 2025 14:35:34 GMT</pubDate>
    </item>
    <item>
      <title>该模型无法提前几步预测数据</title>
      <link>https://stackoverflow.com/questions/79358267/the-model-does-not-predict-the-data-several-steps-ahead</link>
      <description><![CDATA[我正在尝试训练一个模型来提前 2 步预测关闭列。准备好数据集：



datetime
close
close_forward
close_shift_1
close_shift_2
close_shift_3
close_shift_4
close_shift_5




2024-10-01 10:05:00
4009.0
4020.0
4002.5
3994.5
3993.0
3991.0
4007.5


2024-10-01 10:06:00
4020.5
4018.0
4009.0
4002.5
3994.5
3993.0
3991.0


2024-10-01 10:07:00
4020.0
4018.5
4020.5
4009.0
4002.5
3994.5
3993.0


2024-10-01 10:08:00
4018.0
4010.5
4020.0
4020.5
4009.0
4002.5
3994.5


2024-10-01 10:09:00
4018.5
4017.0
4018.0
4020.0
4020.5
4009.0
4002.5


2024-10-01 10:10:00
4010.5
4010.0
4018.5
4018.0
4020.0
4020.5
4009.0



其中 close_forward 是 2 步后关闭的值，close_shift 是后退几步的值。
训练后的线性回归
column_names = column_names =[&#39;close&#39;, &#39;close_shift_1&#39;, &#39;close_shift_2&#39;&#39;close_shift_3&#39;&#39;close_shift_4&#39;, &#39;close_shift_5&#39;]
X_train = df[column_names].values
y_train = df[&#39;сlose_forward&#39;].values

scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))

model = Ridge(alpha=0.0001)
# model.fit(X_train, y_train)
model.fit(X_train_scaled, y_train_scaled)

X_test_scaled = scaler_X.transform(X_test)

# y_pred = model.predict(X_test)
y_pred_scaled = model.predict(X_test_scaled)

y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()

构建了一个图表
在此处输入图片说明
如果放大其中一个部分
在此处输入图片说明
我们可以看到，模型按要求预测了 close 而不是 test 的值。错误可能是什么，如何修复？
需要训练模型，以便它能够提前 2 步进行预测。我尝试使用其他步骤、其他模型（梯度提升和 lstm）进行训练，但效果是一样的]]></description>
      <guid>https://stackoverflow.com/questions/79358267/the-model-does-not-predict-the-data-several-steps-ahead</guid>
      <pubDate>Wed, 15 Jan 2025 12:44:48 GMT</pubDate>
    </item>
    <item>
      <title>使用历史数据训练模型后预测日前每小时电价</title>
      <link>https://stackoverflow.com/questions/79357740/predict-day-ahead-hourly-electricity-prices-after-having-trained-a-model-using-h</link>
      <description><![CDATA[我使用 2015 年至 2024 年的历史数据训练了一个 XGboost 模型。我添加了一些功能，例如天气数据、电力消耗、来自不同来源（如核能和其他可再生能源）的发电量。我已将电价设定为目标。所有数据均为每小时格式。我已经成功训练了模型，但现在我很难预测第二天的电费每小时价格。
我不知道如何使用训练好的模型，使用包含截至 2024 年 12 月 7 日的数据的数据集来预测价格。例如：在模型训练到 2024 年 12 月 7 日后，我如何预测 2025 年 1 月 15 日的每小时价格？
现在，预测代码只取所有特征的最后一个值，并将相同的值放在它试图预测和运行模型的所有行中。
到目前为止，我已经使用了数据集中最近 24 小时的数据（所有特征的最后 24 行），并使用模型来预测价格，但它甚至离实际价格还很远。
# 现在添加预测代码
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# 获取最后已知数据
last_known_data = X_test.copy()
# print(last_known_data.columns)

# 设置明天的目标日期
target_date = datetime.now().replace(hour=0,minute=0,second=0,microsecond=0) + timedelta(days=1)

# 创建 24 小时的 DataFrame
prediction_hours = pd.date_range(target_date, target_date + timedelta(hours=23), freq=&#39;H&#39;)
future_df = pd.DataFrame(index=prediction_hours)
future_df[&#39;Start_date&#39;] = future_df.index

# 将日期时间转换为 unix 时间戳（用于训练）
future_df[&#39;Start_date&#39;] = future_df[&#39;Start_date&#39;].astype(np.int64) // 10**9

# 获取所有特征的最后已知值
last_values = last_known_data.iloc[-1].copy()

# 创建用于预测的特征矩阵
for col in X_train.columns:
if col in [&#39;Start_date&#39;, &#39;End_date_x&#39;, &#39;End_date_y&#39;, &#39;End_date&#39;, &#39;dt_iso&#39;]:
future_df[col] = future_df[&#39;Start_date&#39;]
else:
future_df[col] = last_values[col]

# 添加基于时间的特征
future_df[&#39;hour&#39;] = future_df.index.hour
future_df[&#39;day_of_week&#39;] = future_df.index.dayofweek
future_df[&#39;month&#39;] = future_df.index.month

# 使用最后已知价格处理价格滞后特征
last_known_prices = merged_data[&#39;德国/卢森堡 [€/MWh]&#39;].iloc[-72:]

future_df[&#39;price_lag_24&#39;] = last_known_prices.iloc[-24:].values
future_df[&#39;price_lag_48&#39;] = last_known_prices.iloc[-48:-24].values
future_df[&#39;price_lag_72&#39;] = last_known_prices.iloc[-72:-48].values

# 计算滚动温度特征
for window in [24, 48, 72]:
future_df[f&#39;temp_roll_{window}&#39;] = future_df[&#39;temp&#39;].rolling(window=window, min_periods=1).mean()

# 确保列与训练数据匹配
prediction_features = future_df[X_train.columns]

#进行预测
predictions = xgb_model.predict(prediction_features)

# 创建结果 DataFrame
results_df = pd.DataFrame({
&#39;Datetime&#39;: prediction_hours,
&#39;Hour&#39;: prediction_hours.hour,
&#39;Predicted_Price_EUR_MWh&#39;: np.round(predictions, 2)
})

# 打印预测
print(&quot;\n未来 24 小时的预测价格：&quot;)
print(results_df.to_string(index=False))
]]></description>
      <guid>https://stackoverflow.com/questions/79357740/predict-day-ahead-hourly-electricity-prices-after-having-trained-a-model-using-h</guid>
      <pubDate>Wed, 15 Jan 2025 09:50:48 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 GCP 服务进行图像分类[关闭]</title>
      <link>https://stackoverflow.com/questions/79356912/how-to-use-gcp-service-for-image-classification</link>
      <description><![CDATA[我从物联网设备获取图像作为输入，发送到云端，进行图像分类，并将结果发送回某个 URL。
我尝试使用本地模型为 TF 提供 docker 镜像，并在我的设备上进行分类。
（云端 Ml 部署新手）]]></description>
      <guid>https://stackoverflow.com/questions/79356912/how-to-use-gcp-service-for-image-classification</guid>
      <pubDate>Wed, 15 Jan 2025 02:26:56 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 自动编码器效果很差</title>
      <link>https://stackoverflow.com/questions/79356691/lstm-autoencoder-very-poor-results</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79356691/lstm-autoencoder-very-poor-results</guid>
      <pubDate>Tue, 14 Jan 2025 23:45:14 GMT</pubDate>
    </item>
    <item>
      <title>我应该将目标变量编码为二进制还是仅将其用作逻辑回归的随机梯度上升中的实际值[关闭]</title>
      <link>https://stackoverflow.com/questions/79356129/should-i-encode-my-target-variable-into-binary-or-just-use-it-as-real-value-in-s</link>
      <description><![CDATA[逻辑回归中有 Yi，其公式为公式。
Yi 是目标的实际值还是 Yi 应该是二进制的。如果我的目标变量 / Yi 在问题集中编码为 y 元素 {1 和 -1}，会怎么样？请记住，我使用的是随机梯度上升。所以我的问题集包括来自不同特征 (x) 的数据以及目标变量 (y)。问题还指出，y 已被编码为 1 作为正值，-1 作为负值。但是，当我使用逻辑回归时，我通常将目标变量设置为 0 或 1。因此，在继续使用上图的公式之前，我应该先将其编码为 0/1 吗？还是应该按原样使用 {1,-1}。]]></description>
      <guid>https://stackoverflow.com/questions/79356129/should-i-encode-my-target-variable-into-binary-or-just-use-it-as-real-value-in-s</guid>
      <pubDate>Tue, 14 Jan 2025 19:10:31 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习算法的逐步方法[关闭]</title>
      <link>https://stackoverflow.com/questions/79356061/stepwise-way-of-using-machine-learning-algorithms</link>
      <description><![CDATA[
如何使用数据集
如何找到它们，即使网站已经使用过。
-设计测验算法
将数据集映射到测验算法以预测职业。
快速学习的有效方法。

我和我的朋友决定使用霍兰德代码进行技术职业预测。为此，我们需要使用像 knn 这样的机器学习算法，但我们需要合并数据集？但我们不知道如何做到这一点。您能提供指导吗？您是否有其他方法可以解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79356061/stepwise-way-of-using-machine-learning-algorithms</guid>
      <pubDate>Tue, 14 Jan 2025 18:42:46 GMT</pubDate>
    </item>
    <item>
      <title>我需要训练一个多类模型，但我有一个小数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/79355992/i-need-to-train-a-multiclass-model-but-i-have-a-small-dataset</link>
      <description><![CDATA[我有一个包含两列的 Excel 文件，一列包含短语之类的文本，另一列告诉我从“CS1”到“CS8”的分类。文本如下
&quot;NE PAGTO PROVENTOS APOSENTADORIA ESPECIAL SERVIDORES SAÚDE, NOV/2024. REF. FATURA 033/2024. INCLUI REFORMA DE ESCOLAS.&quot;

我已经清理了其他文件，文件总共有 72 个文本，其中 df.shape = (72, 2)。
准确率保持在 50% 以下。但我需要更高。
文件 clean_text.py：
import re

def clean_text(text):
text = re.sub(r&#39;\d{1,4}/\d{4}&#39;, &#39;&#39;, text)
text = re.sub(r&#39;\d+&#39;, &#39;&#39;, text)
text = re.sub(r&#39;[^\w\s]&#39;, &#39;&#39;, text)
text = text.lower()
return text

文件 main.py：
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import顺序
从 tensorflow.keras.layers 导入 Dense、Dropout、Input
从 tensorflow.keras.optimizers 导入 Adam
从 sklearn.feature_extraction.text 导入 TfidfVectorizer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 LabelEncoder
从 sklearn.metrics 导入 Classification_report、Accuracy_score
从 transformers 导入 TFAutoModel、AutoTokenizer
导入 joblib
将 pandas 导入为 pd
从 nltk.corpus 导入停用词
导入 re
从 clean_text 导入 clean_text

df = pd.read_excel(&quot;DADOS PARA CLASSIFICAÇÃO MULTICLASSE.xlsx&quot;, sheet_name=&quot;TREINAMENTO&quot;)
df[&#39;EMPENHO&#39;] = df[&#39;EMPENHO&#39;].apply(clean_text)
descriptions = df[&#39;EMPENHO&#39;].tolist()
labels = df[&#39;CLASSE SINTETICA&#39;].tolist()

print(f&quot;Amostras: {df.shape}&quot;)

label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels)

vect = TfidfVectorizer()
X = vect.fit_transform(descriptions).toarray()

X_train, X_test, y_train, y_test = train_test_split(X, labels_encoded, test_size=0.2, random_state=42)

model = Sequential([
输入(shape=(X_train.shape[1],)),
Dense(128, 激活=&#39;relu&#39;),
Dropout(0.3),
Dense(64, 激活=&#39;relu&#39;),
Dropout(0.3),
Dense(len(label_encoder.classes_), 激活=&#39;softmax&#39;)
])

model.compile(optimizer=Adam(learning_rate=1e-4), loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

accuracy = 0
while accuracy &lt; 0.90:
print(&quot;训练模型...&quot;)
emp_train = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2, verbose=0)

y_pred = np.argmax(model.predict(X_test), axis=-1)
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;准确率：{accuracy * 100:.2f}%&quot;)

joblib.dump(vect, &quot;vectorizer.pkl&quot;)
joblib.dump(label_encoder, &quot;label_encoder.pkl&quot;)
model.save(&quot;empenho_model.keras&quot;)

print(&quot;训练模型并计算结果成功了！”）

我尝试使用 BERT 和 PyTorch，但这种方式对我来说更好。]]></description>
      <guid>https://stackoverflow.com/questions/79355992/i-need-to-train-a-multiclass-model-but-i-have-a-small-dataset</guid>
      <pubDate>Tue, 14 Jan 2025 18:16:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 PyTorch 中保存 ViT 模型时不会生成 constants.pkl，在边缘设备上部署时是否需要它？</title>
      <link>https://stackoverflow.com/questions/79355096/why-is-constants-pkl-not-generated-when-saving-a-vit-model-in-pytorch-and-is-it</link>
      <description><![CDATA[我训练了一个 Vision Transformer (ViT) 模型进行分类，并使用以下 PyTorch 代码保存了该模型：
torch.save(model, &quot;vit_model.pth&quot;)
当我尝试将保存的模型集成到 Android 应用程序中时，我在运行时遇到了以下错误：
无法启动活动 ComponentInfo{com.test.package/com.test.package.MainActivity}：java.lang.RuntimeException：com.facebook.jni.CppException：PytorchStreamReader 无法定位文件 constants.pkl：找不到文件
我所做的：

我将 .pth 模型文件转换为 zip 文件以检查其内容，我注意到文件中不存在 constants.pkl。
我搜索了有关constants.pkl，但我找不到关于为什么它没有生成或它在这种情况下的作用的明确解释。

我的问题：

为什么使用 torch.save() 保存 PyTorch 模型时没有生成 constants.pkl？
在边缘设备（例如 Android）上部署模型是否需要 constants.pkl？
如果不需要 constants.pkl，我该如何将我的模型集成到 Android 应用程序中而不会遇到此错误？
如果需要 constants.pkl，我该如何生成它或修改我的模型保存过程以包含它？

其他信息：

模型：在 PyTorch 中训练的 Vision Transformer (ViT)。
Android 集成：使用PyTorch Android 库。
应用程序尝试加载模型文件时似乎出现错误。

我需要什么：

明确解释 constants.pkl 的作用以及它是否是 Android 上 PyTorch 模型部署的必需文件。

正确保存模型并将其集成到 Android 应用程序中以避免此问题的步骤或代码示例。

]]></description>
      <guid>https://stackoverflow.com/questions/79355096/why-is-constants-pkl-not-generated-when-saving-a-vit-model-in-pytorch-and-is-it</guid>
      <pubDate>Tue, 14 Jan 2025 13:07:19 GMT</pubDate>
    </item>
    <item>
      <title>如何计算 detector2 中语义分割的每个类的像素总数</title>
      <link>https://stackoverflow.com/questions/78583802/how-to-count-total-number-of-pixels-of-each-class-for-semantic-segmentation-in-d</link>
      <description><![CDATA[我想计算每个分割类的像素总数，我只需要每个一般对象的计数，例如每辆车一个类，每个人一个类等等。出于这个原因，我使用语义分割而不是实例分割（实例分割会分别考虑每个车辆或人员实例）。但detectron2中语义分割的输出没有二进制掩码。
我知道实例分割的输出是二进制掩码，可以使用以下代码获取像素数：
masks = output[&#39;instances&#39;].pred_masks 
results = torch.sum(torch.flatten(masks, start_dim=1),dim=1)

这给出了像素数，但分别考虑了每个车辆实例，这是我不想要的。
但是语义分割的输出是字段“sem_seg”，其中包含每个一般类的预测类概率而不是二元掩码，我怎样才能继续获取语义分割中每个类的像素数？]]></description>
      <guid>https://stackoverflow.com/questions/78583802/how-to-count-total-number-of-pixels-of-each-class-for-semantic-segmentation-in-d</guid>
      <pubDate>Wed, 05 Jun 2024 22:40:27 GMT</pubDate>
    </item>
    <item>
      <title>如何理解二元分类问题的Shapley值？</title>
      <link>https://stackoverflow.com/questions/66018154/how-to-understand-shapley-value-for-binary-classification-problem</link>
      <description><![CDATA[我对 shap python 包非常陌生。我想知道我应该如何解释二元分类问题的 shapley 值？这是我到目前为止所做的。
首先，我使用 lightGBM 模型来拟合我的数据。类似于
import shap
import lightgbm as lgb

params = {&#39;object&#39;:&#39;binary, 
...}
gbm = lgb.train(params, lgb_train, num_boost_round=300)
e = shap.TreeExplainer(gbm)
shap_values = e.shap_values(X)
shap.summary_plot(shap_values[0][:, interested_feature], X[interested_feature])

由于这是一个二元分类问题。shap_values 包含两个部分。我假设一个是针对类别 0，另一个是类别 1。如果我想知道一个特征的贡献。我必须绘制两个如下图所示的图形。
针对类别 0

针对类别 1

但是我应该如何进行更好的可视化呢？结果无法帮助我理解“cold_days 会增加输出成为 1 类还是 0 类的概率？”
使用相同的数据集，如果我使用 ANN，输出就是那样的。我认为 shapley 结果清楚地告诉我“cold_days”将积极增加结果成为 1 类的概率。
我感觉 LightGBM 输出有问题，但我不知道如何修复它。我怎样才能获得更清晰的类似于 ANN 模型的可视化效果？
#编辑
我怀疑我错误地使用了 lightGBM 来获得奇怪的结果。以下是原始代码
import lightgbm as lgb
import shap

lgb_train = lgb.Dataset(x_train, y_train, free_raw_data=False)
lgb_eval = lgb.Dataset(x_val, y_val, free_raw_data=False)
params = {
&#39;boosting_type&#39;: &#39;gbdt&#39;,
&#39;objective&#39;: &#39;binary&#39;,
&#39;metric&#39;: &#39;binary_logloss&#39;,
&#39;num_leaves&#39;: 70,
&#39;learning_rate&#39;: 0.005,
&#39;feature_fraction&#39;: 0.7,
&#39;bagging_fraction&#39;: 0.7,
&#39;bagging_freq&#39;: 10,
&#39;verbose&#39;: 0,
&#39;min_data_in_leaf&#39;: 30,
&#39;max_bin&#39;: 128,
&#39;max_depth&#39;: 12,
&#39;early_stopping_round&#39;: 20,
&#39;min_split_gain&#39;: 0.096,
&#39;min_child_weight&#39;: 6,
}

gbm = lgb.train(params,
lgb_train,
num_boost_round=300,
valid_sets=lgb_eval,
)
e = shap.TreeExplainer(gbm)
shap_values = e.shap_values(X)
shap.summary_plot(shap_values[0][:, interested_feature], X[interested_feature])
]]></description>
      <guid>https://stackoverflow.com/questions/66018154/how-to-understand-shapley-value-for-binary-classification-problem</guid>
      <pubDate>Tue, 02 Feb 2021 21:52:39 GMT</pubDate>
    </item>
    <item>
      <title>训练历史与验证历史非常相似可以吗？</title>
      <link>https://stackoverflow.com/questions/65628074/is-it-ok-to-have-the-training-history-very-similar-to-the-validation-history</link>
      <description><![CDATA[我训练了一个模型 50 个 epoch，按以下比例分割数据集：

X_train, Y_train = 70%
X_validation, Y_validation = 20%
X_test, Y_test = 10%

所有分割均使用 train_test_split(shuffle=True) keras 函数完成：
X = np.load(....)
Y = np.load(....)

# 在训练和验证上进行分割
N_validation = int(len(X) * 0.2)
X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=N_validation)

# 再次分割训练数据以获取测试数据
N_test = int(len(X_train) * 0.1)
X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=N_test)

这是历史图。
从历史中可以看出，验证准确率/损失与训练准确率/损失非常相似。有时验证损失甚至低于训练损失。
至于最后这句话，我在这里读到，这可能导致较高的 dropout 值。可能是这种情况，因为我有一个 rate=0.3 的 dropout 层。
我不明白这是否是个问题。
在测试集上测试模型，我的准确率为91%。]]></description>
      <guid>https://stackoverflow.com/questions/65628074/is-it-ok-to-have-the-training-history-very-similar-to-the-validation-history</guid>
      <pubDate>Fri, 08 Jan 2021 11:23:50 GMT</pubDate>
    </item>
    <item>
      <title>SHAP值能解释一下吗？</title>
      <link>https://stackoverflow.com/questions/59035008/shap-value-can-explain-right</link>
      <description><![CDATA[我在使用 SHAP 值解释基于树的模型时遇到问题（https://github.com/slundberg/shap）。
首先，我输入了大约 30 个特征，其中 2 个特征之间存在高度正相关性。
之后，我训练 XGBoost 模型（python）并查看 2 个特征的 SHAP 值，发现 SHAP 值具有负相关性。
您能向我解释一下，为什么 2 个特征之间的输出 SHAP 值的相关性与输入相关性不一样吗？我可以相信 SHAP 的输出吗？
===========================
输入之间的相关性：0.91788
SHAP 值之间的相关性：-0.661088
2 个特征是

省份的人口
省份的家庭数量

模型性能
训练 AUC：0.73
测试 AUC：0.71
输入散点图（x：省份的家庭数量，y：省份的人口）：

SHAP 值输出散点图（x：省内家庭数量，y：省内人口）：
]]></description>
      <guid>https://stackoverflow.com/questions/59035008/shap-value-can-explain-right</guid>
      <pubDate>Mon, 25 Nov 2019 15:21:26 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归的随机梯度下降总是返回 Inf 的成本，并且权重向量永远不会接近</title>
      <link>https://stackoverflow.com/questions/26418178/stochastic-gradient-descent-for-logistic-regression-always-returns-a-cost-of-inf</link>
      <description><![CDATA[我正在尝试在 MATLAB 中实现逻辑回归求解器，并通过随机梯度下降找到权重。我遇到了一个问题，我的数据似乎产生了无限的成本，无论发生什么，它都不会下降……
这是我的梯度下降函数：
function weightVector = logisticWeightsByGradientDescentStochastic(trueClass,features)
%% 此函数尝试收敛到逻辑回归阶数为 1 的最佳权重集
%% 输入：
% trueClass - 训练数据的真实类值向量
% features
%% 输出：
% weightVector - 大小为 n+1 的向量（n 是特征数）
% 对应于收敛权重

%% 获取数据大小
dataSize = size(features);

%% 初始选择权重向量
weightVector = zeros(dataSize(2)+1, 1) %创建一个等于特征数量加 1 的零向量

%% 选择学习率
learningRate = 0.0001;

%% 初始成本
cost = logisticCost(weightVector, features, trueClass)

%% 随机梯度下降
costThresh = 0.05 %定义成本阈值

iterCount = 0;
while(cost &gt; costThresh)
for m=1:dataSize(1) %for all samples

%% test 语句
curFeatures = transpose([1.0 features(m,:)])

%% 计算 Sigmoid 预测值 
predictClass = assessSigmoid(weightVector , [1.0 features(m,:)] )

%% test 语句
truth = trueClass(m)

%% 计算所有特征的梯度
gradient = learningRate .* (trueClass(m) - predictClass) .* transpose([1.0 features(m,:)])

%% 通过从旧权重向量中减去梯度来更新权重向量
weightVector = weightVector - gradient 

%% 使用新权重向量重新评估 Cost
cost = logisticCost(weightVector, features, trueClass)

if(cost &lt; costThresh)
break
end
iterCount = iterCount + 1

结束 %for m
结束 %while cost &gt; 0.05

weightVector
iterCount
end

这是我的成本函数：
function cost = logisticCost(weightVector, features, trueClass)
%% 计算将 weightVector 应用于所有样本的总成本
%% 对于线性回归模型，根据
%% J(theta) = -(1/m) sum[ trueClass(log(predictedClass) + (1-trueClass)log(predictedClass)]
%% 输入：
% weightVector - n+1 个权重向量，其中 n 是特征数
% 加 1
% features - 特征矩阵
% trueClass - 训练数据的真实类别
%% 输出：
% cost - 总成本

dataSize = size(features); %获取数据大小

errorSum = 0.0; %存储错误总和
for m = 1:dataSize(1) %每行
predictedClass = assessEvaluateSigmoid(weightVector, [1.0 features(m,:)]); %评估 Sigmoid 来预测样本 m 的类别
if trueClass(m) == 1
errorSum = errorSum + log(predictedClass);
else
errorSum = errorSum + log(1 - predictClass);
end
end

cost = errorSum / (-1 .* dataSize(1)); % 乘以 -(1/m) 以获得成本
结束

这两者看起来都很好，我无法想象为什么我的成本函数总是会返回无穷大。
这是我的训练数据，其中第一列是类（1 或 0），接下来的七列是我要回归的特征。]]></description>
      <guid>https://stackoverflow.com/questions/26418178/stochastic-gradient-descent-for-logistic-regression-always-returns-a-cost-of-inf</guid>
      <pubDate>Fri, 17 Oct 2014 05:00:30 GMT</pubDate>
    </item>
    <item>
      <title>Matlab 中 Libsvm SVR 训练的数据格式</title>
      <link>https://stackoverflow.com/questions/19163090/data-format-for-libsvm-svr-training-in-matlab</link>
      <description><![CDATA[我有两个与 LIBSVM 中的数据输入相关的问题。

我是否需要将数据格式化为稀疏格式才能在 matlab 中输入 svr libsvm？
在将数据输入训练器之前，我是否需要对数据进行规范化？

我正在训练 svr，但没有做任何这些，即使格式化，我也得到了相同的结果。正如 libsvm 文档中提到的，当我们为 OCTAVE 执行数据格式化时，会使用数据格式化，因为只需运行 train.py 和 test.py 即可自动运行所有内容。但在 matlab 中我不确定。
有人可以澄清一下吗？]]></description>
      <guid>https://stackoverflow.com/questions/19163090/data-format-for-libsvm-svr-training-in-matlab</guid>
      <pubDate>Thu, 03 Oct 2013 15:25:13 GMT</pubDate>
    </item>
    </channel>
</rss>