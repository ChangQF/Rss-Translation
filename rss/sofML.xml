<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 19 Nov 2024 01:19:40 GMT</lastBuildDate>
    <item>
      <title>使用 LSTM 模型进行股票价格分析/预测，时间戳问题</title>
      <link>https://stackoverflow.com/questions/79201451/stock-price-analysis-prediction-with-lstm-model-timestamp-problem</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79201451/stock-price-analysis-prediction-with-lstm-model-timestamp-problem</guid>
      <pubDate>Mon, 18 Nov 2024 21:09:06 GMT</pubDate>
    </item>
    <item>
      <title>如何修改我的代码来处理 RGBX（4 通道）图像以进行语义分割？</title>
      <link>https://stackoverflow.com/questions/79201296/how-to-modify-my-code-to-handle-rgbx-4-channel-images-for-semantic-segmentatio</link>
      <description><![CDATA[我是该领域的新手，一直在关注 U-Net 教程，使用 3 通道 RGB 图像进行语义分割https://www.youtube.com/watch?v=68HR_eyzk00&amp;list=PLZsOBAyNTZwbR08R959iCvYT3qzhxvGOE&amp;index=2&amp;ab_channel=DigitalSreeni，对我来说效果很好。但是，我现在需要扩展管道以支持 4 通道 RGBX 图像（即 RGB + 一个额外通道），但我不确定如何修改代码以适应额外的通道，尤其是对于预处理和 ImageDataGenerator 部分（我认为 ImageDataGenerator 不支持 4 通道图像）。
这是代码（将图像修补为（256 * 256 * 4）并将蒙版修补为（256*256）后）：
import os
import cv2
import numpy as np
import glob
from matplotlib import pyplot as plt
from patchify import patchify
import tensorflow as tf
import splitfolders
import fragmentation_models as sm
from tensorflow.keras.metrics import MeanIoU
from sklearn.preprocessing import MinMaxScaler
from keras.utils import to_categorical

input_folder=&#39;我的图像和掩码的文件夹路径&#39;

output_folder=&#39;输出文件夹的路径&#39;
#按比例分割
splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.75,.25),group_prefix=None) 

#重新排列用于 keras 增强的文件夹结构

seed=24
batch_size=16 
n_classes=2 

scaler=MinMaxScaler()

BACKBONE=&#39;resnet34&#39; 
preprocess_input=sm.get_preprocessing(BACKBONE)

def preprocess_data(img, mask, num_class):
#缩放图像
img=scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)
img=preprocess_input(img) #基于预训练的主干进行预处理
mask=to_categorical(mask, num_class)
return (img,mask)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
def trainGenerator(train_img_path, train_mask_path, num_class):
img_data_gen_args=dict(horizo​​ntal_flip=True, vertical_flip=True, fill_mode=&#39;reflect&#39;) #数据增强

image_datagen=ImageDataGenerator(**img_data_gen_args)
mask_datagen=ImageDataGenerator(**img_data_gen_args)

image_generator=image_datagen.flow_from_directory(train_img_path, class_mode=None, batch_size=batch_size, seed=seed)
mask_generator=image_datagen.flow_from_directory(train_mask_path, class_mode=None, color_mode=&#39;grayscale&#39;, batch_size=batch_size, seed=seed)

train_generator=zip(image_generator, mask_generator)

for (img, mask) in train_generator:
img, mask= preprocess_data(img, mask, num_class)
Yield (img, mask)

train_img_path=&#39;训练图像路径&#39;
train_mask_path=&#39;训练掩码路径&#39;
train_img_gen=trainGenerator(train_img_path, train_mask_path, num_class=2)

val_img_path=&#39;验证图像路径&#39;
val_mask_path=&#39;验证掩码路径&#39;
val_img_gen=trainGenerator(val_img_path, val_mask_path, num_class=2)

x, y=train_img_gen.__next__()

for i in range(0,3):
image=x[i]
mask=np.argmax(y[i], axis=2)
plt.subplot(1,2,1)
plt.imshow(image)
plt.subplot(1,2,2)
plt.imshow(mask, cmap=&#39;gray&#39;)
plt.show()

num_train_imgs=len(os.listdir(&#39;训练图像路径&#39;))
num_val_images=len(os.listdir(&#39;验证路径图像&#39;))
steps_per_epochs=num_train_imgs//batch_size
val_steps_per_epoch=num_val_images//batch_size

IMG_HEIGHT=x.shape[1]
IMG_WIDTH=x.shape[2]
IMG_CHANNELS=x.shape[3]

n_classes=2

model=sm.Unet(&#39;resnet34&#39;,coder_weights=&#39;None&#39;,input_shape=(IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS),classes=n_classes,activation=&#39;softmax&#39;)
model.compile(&#39;Adam&#39;,loss=sm.losses.binary_crossentropy,metrics=[sm.metrics.iou_score,sm.metrics.FScore()])

history=model.fit(train_img_gen, steps_per_epoch=steps_per_epochs, epochs=100, verbose=1, validation_data=val_img_gen, validation_steps=val_steps_per_epoch)

]]></description>
      <guid>https://stackoverflow.com/questions/79201296/how-to-modify-my-code-to-handle-rgbx-4-channel-images-for-semantic-segmentatio</guid>
      <pubDate>Mon, 18 Nov 2024 20:06:05 GMT</pubDate>
    </item>
    <item>
      <title>标记 3D 网格数据 [关闭]</title>
      <link>https://stackoverflow.com/questions/79200777/labeling-3d-mesh-data</link>
      <description><![CDATA[背景：我有关于人体的 3D 网格数据，以未标记的 .obj 和 .stl 文件的形式存在。我想给它贴上标签（如躯干、手等），这样我就可以用它来训练我正在研究的 AI 模型。该模型的目标是通过监督深度学习来分割（实现不同的部分）。
问题：我有什么选择？人们使用一些软件、python 脚本/程序或其他巧妙的方法吗？您也可以为我提供有关该过程的详细信息吗？我看到一个数据集将 .off 视为未标记，而标记的数据集是 .seg 文件，但我不知道它们是如何从 .off 到达 .seg 的。]]></description>
      <guid>https://stackoverflow.com/questions/79200777/labeling-3d-mesh-data</guid>
      <pubDate>Mon, 18 Nov 2024 16:59:34 GMT</pubDate>
    </item>
    <item>
      <title>如何使用用户定义的类别在 iOS 上动态地重新训练 YOLO 模型，同时保留以前的知识？</title>
      <link>https://stackoverflow.com/questions/79200388/how-to-dynamically-retrain-a-yolo-model-on-ios-with-user-defined-classes-while-p</link>
      <description><![CDATA[我正在开发一款 iOS 应用，该应用使用 Vision Kit 和 YOLO（例如 YOLOv4-Tiny）模型来检测卡片上的数据和图像。有时，应用会遇到模型无法识别的新图标。我想要：

允许用户为未知图标提供标签。
使用这些新数据重新训练现有的 YOLO 模型。
确保保留以前训练过的类别（避免灾难性遗忘）。
将更新后的模型无缝部署回 iOS 应用。

要求：
应用应支持在设备上或通过使用后端进行轻量级再训练。
再训练过程应将新类别与现有模型的知识合并。
该解决方案必须在 iPhone 等移动硬件上有效运行。
我目前的方法：

我正在使用转换为 iOS 版 Core ML 的预训练 YOLOv4-Tiny 模型。
Vision Kit 负责处理图像预处理和检测。

挑战：
如何将新用户提供的数据集成到训练管道中？
如何在 iOS 或后端服务器上高效地重新训练 YOLO，同时保留现有类别？
将更新的模型部署到应用程序的最佳实践。
我正在寻找以下方面的指导：

用于设备端或服务器端 YOLO 再训练的工具或库。

用于 YOLO 模型的增量学习或知识提炼的方法。

如何处理将再训练的模型部署到 iOS 设备。

]]></description>
      <guid>https://stackoverflow.com/questions/79200388/how-to-dynamically-retrain-a-yolo-model-on-ios-with-user-defined-classes-while-p</guid>
      <pubDate>Mon, 18 Nov 2024 14:49:08 GMT</pubDate>
    </item>
    <item>
      <title>SAM 2.1 是什么导致 hydra.errors.MissingConfigException：未找到主配置模块“sam2”？</title>
      <link>https://stackoverflow.com/questions/79199682/sam-2-1-what-is-causing-hydra-errors-missingconfigexception-primary-config-modu</link>
      <description><![CDATA[我正在尝试使用此处给出的 roboflow 指南微调新的 SAM 2.1 分割模型：Sam 2.1 roboflow 指南
使用 google collab 时，此代码运行正常，没有遇到任何错误。当我在本地机器上运行完全相同的代码时，运行训练代码命令时会出现以下错误：
!python training/train.py -c &#39;configs/train.yaml&#39; --use-cluster 0 --num-gpus 1
在 Windows 10 上使用 vscode 运行时出现以下错误：
hydra.errors.MissingConfigException：未找到主配置模块“sam2”。
检查它是否正确并包含 __init__.py 文件

我的工作目录：
C:\..\SAM_2_1\sam2

]]></description>
      <guid>https://stackoverflow.com/questions/79199682/sam-2-1-what-is-causing-hydra-errors-missingconfigexception-primary-config-modu</guid>
      <pubDate>Mon, 18 Nov 2024 11:00:31 GMT</pubDate>
    </item>
    <item>
      <title>句子转换器中的降维</title>
      <link>https://stackoverflow.com/questions/79199199/dimensionality-reduction-in-sentence-transformers</link>
      <description><![CDATA[我需要在预处理中计算大量句子（比如 10K）的嵌入，并且在运行时我必须一次计算一个句子的嵌入向量（用户查询），然后根据嵌入向量找到最相似的句子（使用余弦相似度）。
我目前正在使用句子转换器，它们的输出大小为 768，这对于我的情况来说太大了。所以我想尝试更小的尺寸，比如 256 甚至 128。
我熟悉 PCA 和量化。然而，ChatGPT 和 Gemini 都建议我在池化层之后添加一个密集层。示例：
dense = models.Dense(in_features=base_model.get_sentence_embedding_dimension(), out_features=256)
model = SentenceTransformer(modules=[base_model, density])

我的问题是，我认为我必须重新训练/微调我的模型，但由于我没有标记数据，我无法做到这一点。但 ChatGPT 和 Gemini 声称我可以不用重新训练或微调就完成这个实现，尽管“这样会更好”。
我很困惑这怎么可能行得通，因为如果不进行训练，密集层中的初始权重将是随机的。
我是否遗漏了什么，或者添加密集层而不进行重新训练/微调是否真的可行？]]></description>
      <guid>https://stackoverflow.com/questions/79199199/dimensionality-reduction-in-sentence-transformers</guid>
      <pubDate>Mon, 18 Nov 2024 08:31:21 GMT</pubDate>
    </item>
    <item>
      <title>惩罚逻辑回归的 F 分数、精度和召回率指标</title>
      <link>https://stackoverflow.com/questions/79198299/f-score-precision-and-recall-metrics-for-a-penalized-logistic-regression</link>
      <description><![CDATA[我正在尝试对我制作的惩罚逻辑回归模型进行交叉验证。
我还注意到，互联网上的一些示例假设我只是手动输入完整数据集中的实际数字和预测数字数组（谁有时间做这个哈哈？）。
我之前使用 JuliusAI 是因为它可以开箱即用地报告这些内容，但我又回到了 R，因为我现在不相信 Julius 的可重复性（为了全面披露），并且停止使用它，尽管按钮分析很有诱惑力。我希望能够自己忠实地计算这些指标，并且是使用 R 中的交叉验证的新手（通常已经完成了 AIC 和重要性统计）。但关于 JuliusAI 就说这么多。
问题就在这里。我有一个数据集（n=32），我使用 80/20 分割来训练和测试。那部分没问题。我的预测变量是二进制的。训练数据集称为 train（25 行），测试称为 test（有 7 行）。惩罚逻辑回归模型有 3 个预测变量，结果为失败或不失败。回归过程运行良好，因此我添加了混淆矩阵来获得所需的结果。
我一直在尝试使用 ConfusionMatrix 命令，但一直出现错误
混淆矩阵中的错误：数据和参考因素必须具有相同数量的级别

这是我使用的：
library(rsample)
library(caret)
library(logistf)
split &lt;- initial_split(newdata1, prop = .8)
train &lt;- training(split)
test &lt;- testing(split)
fit &lt;- logistf(data=train, newdata1$`Fail (1=yes)`~ 
newdata1$`rev`+newdata1$`totassets`+newdata1$`totliab`)
prediction &lt;- predict(fit, data = test, type = &#39;response&#39;)
pred &lt;- factor(ifelse(prediction &lt;= 0.5,0,1))
result &lt;- caret::confusionMatrix(pred,test$`Fail (1=yes)`)

于是，我尝试：
cm &lt;- chaosMatrix(pred, as.factor(test$`Fail (1=yes)`)

结果为
error in chaosMatrix.default(pred, as.factor(test$`Fail (1=yes)`)) : 
数据不能比参考值有更多级别&quot;
chaosMatrix(pred,as.factor(test$`Fail (1=yes)`))
confusionMatrix.default(pred,as.factor(test$`Fail (1=yes)`)) 中的错误：
数据不能比参考具有更多级别

有没有更直接的方法来执行此操作并获取指标？我以为有一个 F1 分数命令，但似乎这需要一个 y_pred，我不知道如何生成或访问它（除非通过 predict()）。
可能是因为我的数据集有 10 个变量，而我使用了其中的 3 个，还是因为拆分？我该如何解决这个问题？
你的建议？]]></description>
      <guid>https://stackoverflow.com/questions/79198299/f-score-precision-and-recall-metrics-for-a-penalized-logistic-regression</guid>
      <pubDate>Sun, 17 Nov 2024 22:38:11 GMT</pubDate>
    </item>
    <item>
      <title>创建检测区块链异常任务的代表性子集</title>
      <link>https://stackoverflow.com/questions/79197028/creating-representative-subset-for-detecting-blockchain-anomalies-task</link>
      <description><![CDATA[我们必须创建云解决方案，在该解决方案中，我们从三个网络（solana、比特币、以太坊）收集和转换区块链交易数据，然后使用机器学习方法进行异常检测。为了首先降低成本，我们希望获取大约 30GB-50GB 的数据（而不是 TB）并在本地进行训练，以确定哪种 ML 方法最适合此任务。
问题是我们真的不知道应该采取什么方法来为我们的子集选择数据。我们曾考虑过从选定的时间段（例如 3 个月）获取数据，但问题是 Solana 数据集在数据量方面要大几倍（300 TB 对比比特币和以太坊的约 &lt;10TB - 这实际上也会在云端出现问题）。此外，在选定的时间段内减少 solana 的数量可能会是一个问题，因为我们可能会通过这种方式摆脱一些数据模式（选定钱包地址的交易频率是一个重要因素）。缩短 solana 的窗口期是正确的方法吗？ （例如，从比特币和以太坊中抽取 3 个月的数据，从 solana 中抽取仅 1 周的数据，导致每个网络的数据大小和交易数量相似）或者它太短而无法反映模式？如何实际处理这个问题？
我们还知道数据集在类别方面是不平衡的（少数交易是异常的），但我们希望在选择子集后执行平衡方法（以反映我们将在云上处理的环境，并使用整个数据集进行平衡）
你有什么建议？]]></description>
      <guid>https://stackoverflow.com/questions/79197028/creating-representative-subset-for-detecting-blockchain-anomalies-task</guid>
      <pubDate>Sun, 17 Nov 2024 10:56:17 GMT</pubDate>
    </item>
    <item>
      <title>有条件地更改 RGB 图像中的像素值，然后删除通道</title>
      <link>https://stackoverflow.com/questions/79196999/conditionally-changing-the-pixel-values-in-an-rgb-image-then-removing-the-chann</link>
      <description><![CDATA[我只想比较图像的像素，如果这个像素是粉红色（R 值 = 0.502、G 值 = 0.0、B 值 = 0.502）则将其更改为黑色，否则将其更改为白色。在此之后，我想删除通道，只得到一个 (512,512,) 形状的张量。
此 getitem 位于我的数据集类中。
代码：
def __getitem__(self, index):
img = Image.open(self.images[index]).convert(&quot;RGB&quot;) # 这是一张图像
mask = Image.open(self.masks[index]).convert(&quot;RGB&quot;) # 这是相应的掩码
mask = self.transform_tensor(mask) # 张量为 (3,512,512) 形状
mask = torch.where((mask[0, :, :] == 0.502) &amp; (mask[1, :, :] == 0.0) &amp; (mask[2, :, :] == 0.502), torch.tensor([0.0, 0.0, 0.0]), torch.tensor([1.0, 1.0, 1.0])) # 错误
mask = self.transform_to_image(mask)
mask.show()
return self.transform_image(img), self.transform_mask(mask)

我明白了
RuntimeError：张量 a (512) 的大小必须与非单例维度 1 上的张量 b (3) 的大小匹配

错误：意外类型：(bool, Tensor, Tensor)
]]></description>
      <guid>https://stackoverflow.com/questions/79196999/conditionally-changing-the-pixel-values-in-an-rgb-image-then-removing-the-chann</guid>
      <pubDate>Sun, 17 Nov 2024 10:36:45 GMT</pubDate>
    </item>
    <item>
      <title>ResNet50 模型在测试集上具有较高的准确率，但在手动测试时在同一组上表现不佳</title>
      <link>https://stackoverflow.com/questions/79196985/resnet50-model-has-high-accuracy-on-test-set-but-performs-poorly-on-the-same-set</link>
      <description><![CDATA[我正在尝试在约 100 个类别的数据集上训练 ResNet50 模型。
为此，我首先使用 splitfolders 将数据拆分为训练集、验证集和测试集，然后将这些集保存在单独的文件夹中。然后我预处理数据，定义我的特定模型并在训练集上对其进行训练。随后，我保存模型并在测试集上对其进行评估。到目前为止，一切似乎都运行良好，我在测试集上的准确率约为 0.89。
但是，然后我尝试在同一个测试集上手动测试模型。我从最大的类开始，它运行良好。然而，对于较小的类，所有预测都是错误的。奇怪的是，大多数这些错误的预测都是相同的，例如。 63 类中的大多数图像被预测为 62 类，15 类中的大多数图像被预测为 7 类等等……所以在我看来，该模型实际上可能做出了正确的预测，但将它们与错误的类别相关联……总而言之，有太多错误的预测无法达到 0.89 的准确率。
我想我可能犯了一些相当愚蠢的错误，因为我的手动测试导致的结果与模型的先前评估如此不同，这毫无道理？有人知道这里可能是什么问题吗？
我用来手动测试测试集中文件夹的预测的代码如下：
# 遍历文件夹中的所有图像
for img_file in os.listdir(folder_path):
img_path = os.path.join(folder_path, img_file)

# 加载并预处理图像
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = tf.keras.applications.resnet50.preprocess_input(img_array)

# 进行预测
predictions = model.predict(img_array)
prediction_class = np.argmax(predictions)

print(f&quot;图像：{img_file} - 预测的类：{predicted_class}&quot;)

将上述代码循环遍历所有类文件夹，并在每次预测后更新准确率：
total_samples = 0
correct_predictions = 0

# 遍历代表类的每个子文件夹
for class_folder in os.listdir(parent_folder_path):
class_folder_path = os.path.join(parent_folder_path, class_folder)

# 遍历类文件夹中的所有图像
for img_file in os.listdir(class_folder_path):
img_path = os.path.join(class_folder_path, img_file)

# 加载并预处理图像
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = tf.keras.applications.resnet50.preprocess_input(img_array)

# 进行预测
predictions = model.predict(img_array)
predict_class = np.argmax(predictions)

total_samples += 1
if predict_class == int(class_folder): #文件夹名称由代表类别的数字组成
correct_predictions += 1

current_accuracy = correct_predictions/ total_samples * 100

print(f&quot;图像：{img_file} - 预测类别：{predicted_class} - 实际类别：{int(class_folder)} -正确预测：{correct_predictions} - 总样本数：{total_samples} - 当前准确率：{current_accuracy:.2f}%&quot;)

我用来评估模型的代码如下：
test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=tf.keras.applications.resnet50.preprocess_input)

test_generator = test_datagen.flow_from_directory(
test_data_dir,
target_size=(img_height, img_width),
batch_size=1,
class_mode=&quot;categorical&quot;
)

test_loss, test_acc = model.evaluate(test_generator, verbose=2)
print(&quot;\nTest accuracy:&quot;, test_acc)
]]></description>
      <guid>https://stackoverflow.com/questions/79196985/resnet50-model-has-high-accuracy-on-test-set-but-performs-poorly-on-the-same-set</guid>
      <pubDate>Sun, 17 Nov 2024 10:29:27 GMT</pubDate>
    </item>
    <item>
      <title>如何防止目标编码泄漏？[关闭]</title>
      <link>https://stackoverflow.com/questions/79196742/how-can-i-prevent-leaks-in-my-target-encoding</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79196742/how-can-i-prevent-leaks-in-my-target-encoding</guid>
      <pubDate>Sun, 17 Nov 2024 08:04:31 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的暹罗模型不适用于验证数据？</title>
      <link>https://stackoverflow.com/questions/79196054/why-does-my-siamese-model-not-work-on-verification-data</link>
      <description><![CDATA[我的模型之前运行良好，并做出了良好的预测。然而，现在我尝试使用它，它无法识别图像之间的相似性。请帮助解决此问题。
# 保存权重
siamese_model.save(&#39;siamesemodel.h5&#39;)

# 加载模型
model = tf.keras.models.load_model(
&#39;siamesemodel.h5&#39;, 
custom_objects={&#39;L1Dist&#39;: L1Dist, &#39;BinaryCrossentropy&#39;: tf.losses.BinaryCrossentropy}
)

# 验证函数
def verify(model, detection_threshold, validation_threshold):
# 构建结果数组
results = []
for image in os.listdir(os.path.join(&#39;application_data&#39;, &#39;verification_images&#39;)):
input_img = preprocess(os.path.join(&#39;application_data&#39;, &#39;input_image&#39;, &#39;input_image.jpg&#39;))
validation_img = preprocess(os.path.join(&#39;application_data&#39;, &#39;verification_images&#39;, image))

result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))
results.append(result)

# 检测阈值：高于该指标的预测被视为正值
detection = np.sum(np.array(results) &gt; detection_threshold)

 # 验证阈值：正预测的比例/总正样本
validation = detection / len(os.listdir(os.path.join(&#39;application_data&#39;, &#39;verification_images&#39;)))
verified = validation &gt; verify_threshold

返回结果，已验证

cap = cv2.VideoCapture(0)
while cap.isOpened():
ret, frame = cap.read()
frame = frame[120:120+250, 200:200+250, :]

cv2.imshow(&#39;Verification&#39;, frame)

# 验证触发器
if cv2.waitKey(10) &amp; 0xFF == ord(&#39;v&#39;):
# 将输入图像保存到 input_image 文件夹
cv2.imwrite(os.path.join(&#39;application_data&#39;, &#39;input_image&#39;, &#39;input_image.jpg&#39;), frame)
# 运行验证
results, verified = verify(model, 0.5, 0.5)
print(verified)

if cv2.waitKey(10) &amp; 0xFF == ord(&#39;q&#39;):
break
cap.release()
cv2.destroyAllWindows()

打印结果如下：
[array([[9.938484e-09]], dtype=float32),
array([[0.00011181]], dtype=float32),
array([[4.0544733e-06]], dtype=float32),
array([[3.6490118e-07]], dtype=float32),
array([[1.779369e-07]], dtype=float32),
array([[0.15224604]], dtype=float32),
array([[2.0296879e-05]], dtype=float32),
数组([[7.9831276e-05]], dtype=float32),
数组([[2.3284203e-05]], dtype=float32),
数组([[8.0619594e-07]], dtype=float32),
数组([[1.0691416e-06]], dtype=float32),
数组([[1.9231505e-08]], dtype=float32),
数组([[2.243531e-05]], dtype=float32),
数组([[6.483703e-07]], dtype=float32),
数组([[6.656185e-07]], dtype=float32),
array([[4.8954314e-07]], dtype=float32),
array([[9.550116e-08]], dtype=float32),
array([[1.305056e-07]], dtype=float32),
array([[4.187218e-09]], dtype=float32),
array([[3.8443446e-08]], dtype=float32),
array([[5.9630083e-09]], dtype=float32),
array([[1.1699244e-06]], dtype=float32),

我知道保存模型没有问题，因为当我在原始输入上测试重新加载的模型与原始模型时，它们具有相同的输出。
对于给定的输入（两次都是我的一帧），大多数结果应该远高于 0.5。我不明白到底出了什么问题。顺便说一句，这段代码主要来自 YT 教程：https://www.youtube.com/watch?v=FNHLVRJ1HU4&amp;list=PLgNJO2hghbmhHuhURAGbe6KWpiYZt0AMH&amp;index=8]]></description>
      <guid>https://stackoverflow.com/questions/79196054/why-does-my-siamese-model-not-work-on-verification-data</guid>
      <pubDate>Sat, 16 Nov 2024 21:07:31 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow/Keras 模型的 SHAP force_plot 中出现 InvalidArgumentError：切片索引超出范围</title>
      <link>https://stackoverflow.com/questions/79195478/invalidargumenterror-in-shap-force-plot-for-tensorflow-keras-model-slice-index</link>
      <description><![CDATA[我正在使用 TensorFlow/Keras 二元分类模型并使用 SHAP 来解释单个预测。但是，当我尝试生成力图时，我遇到了以下错误：
# 导入 SHAP
import shap

# 确保 data_for_prediction 具有正确的形状
data_for_prediction_reshaped = data_for_prediction.reshape(1, -1)

# 为 DeepExplainer 提供背景数据
background = X_train[:100] # 使用来自训练数据的 100 个样本作为背景

# 初始化 DeepExplainer
explainer = shap.DeepExplainer(model, background)

# 计算 SHAP 值
shap_values = explainer.shap_values(data_for_prediction_reshaped)

# 生成力图
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction_reshaped)

错误：
InvalidArgumentError：{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} 切片索引 1 的维度 0 超出范围。 [Op:StridedSlice] 名称：strided_slice/

其他详细信息：

该模型是具有以下架构的 Keras Sequential 模型：
• 具有 ReLU 激活的多个密集层。
• 每个密集层后都有一个 Dropout 层。
• 用于二元分类的具有 S 形激活的输出层。

背景数据：
• X_train[:100] 是我预处理的训练数据（NumPy 数组）的一部分。

预测输入：

• data_for_prediction_reshaped 是重塑为 (1, n_features) 的单个样本。

形状：

• shap_values[1].shape：SHAP 值的输出形状（针对第 1 类）。
• data_for_prediction_reshaped.shape：将输入特征重塑为 (1, n_features)。


问题：

在此上下文中，“切片索引 1 的维度 0 超出范围”错误是什么意思？
我应该如何调整代码以确保 shap.force_plot 能够与 SHAP 和 TensorFlow/Keras 模型正确配合使用？
对于此用例，我应该注意 SHAP 和 TensorFlow/Keras 之间是否存在特定的兼容性问题？
]]></description>
      <guid>https://stackoverflow.com/questions/79195478/invalidargumenterror-in-shap-force-plot-for-tensorflow-keras-model-slice-index</guid>
      <pubDate>Sat, 16 Nov 2024 15:38:25 GMT</pubDate>
    </item>
    <item>
      <title>检测物体异常</title>
      <link>https://stackoverflow.com/questions/47829065/detecting-anomalies-in-objects</link>
      <description><![CDATA[假设我有一个 json 对象数组。
{ firstName: John, Children: [&quot;Maria&quot;,&quot;Alfred&quot;], marriage: false }
{ firstName: George, Children: [&#39;**zoekerbergen alfonso the second**&#39;,&#39;Harvey&#39;], marriage: false }

{ firstName: Hary, Children: [&quot;Sam&quot;,&quot;Obama&quot;], marriage: false }

通常情况下，子项都是一个由单词组成的小数组。
Zoekerbergen alfonso the second 却是一个异常。
有没有办法学习对象的模式，然后检测出诸如此类的异常，比如一个人有 1000 个孩子？]]></description>
      <guid>https://stackoverflow.com/questions/47829065/detecting-anomalies-in-objects</guid>
      <pubDate>Fri, 15 Dec 2017 09:11:55 GMT</pubDate>
    </item>
    <item>
      <title>神经网络是一种懒惰的学习方法还是积极学习的方法？</title>
      <link>https://stackoverflow.com/questions/5749867/is-a-neural-network-a-lazy-or-eager-learning-method</link>
      <description><![CDATA[神经网络是一种懒惰的还是积极学习的方法？不同的网页有不同的说法，所以我想得到一个可靠的答案，并有好的文献来支持它。最明显的书是米切尔著名的《机器学习》一书，但浏览整本书我找不到答案。谢谢 :)。]]></description>
      <guid>https://stackoverflow.com/questions/5749867/is-a-neural-network-a-lazy-or-eager-learning-method</guid>
      <pubDate>Thu, 21 Apr 2011 21:16:25 GMT</pubDate>
    </item>
    </channel>
</rss>