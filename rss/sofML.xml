<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 16 Oct 2024 03:25:59 GMT</lastBuildDate>
    <item>
      <title>如何使用base_margin参数并获得更好的结果？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79091342/how-to-use-base-margin-parameters-and-have-better-results</link>
      <description><![CDATA[我想使用 base_margin 变量来重用先前训练的模型的预测。但我得到的结果更糟。
代码：
import xgboost as xgb
import numpy as np

# X_train/Y_train 是数据/预测

n_train = len(X_train)
ind = np.arange(n_train)
np.random.shuffle(ind)
X_train = X_train[ind]
Y_train = Y_train[ind]

# xgboost 模型训练

# 第一个模型
print(&quot;训练第一个模型&quot;)
first_model = xgb.XGBRegressor(
nthread=1,
seed=55
)
first_model.fit(X_train, Y_train)
initial_predictions = first_model.predict(X_train)

print(first_model.score(X_train, Y_train))

# 第二个model
print(&quot;训练第二个模型&quot;)
verbosity = 2
nthread = 3
seed = 12
xgb_model = xgb.XGBRegressor(
nthread=nthread,
seed=seed
)
model = xgb_model.fit(X_train, Y_train, base_margin = initial_predictions)model.score(X_train, Y_train)
print(model.score(X_train, Y_train))
]]></description>
      <guid>https://stackoverflow.com/questions/79091342/how-to-use-base-margin-parameters-and-have-better-results</guid>
      <pubDate>Tue, 15 Oct 2024 18:43:41 GMT</pubDate>
    </item>
    <item>
      <title>最近的 Windows 更新后，WSL 上的 H2O 出现连接被拒绝错误</title>
      <link>https://stackoverflow.com/questions/79091245/connection-refused-error-with-h2o-on-wsl-after-recent-windows-update</link>
      <description><![CDATA[在最近的 Windows 更新后，我在 Windows Subsystem for Linux (WSL) 上运行 H2O 时遇到了连接问题。以下是问题的详细分析以及我迄今为止采取的故障排除步骤：
系统信息：

操作系统：带有 WSL 的 Windows（Linux 内核版本 5.15.153.1-microsoft-standard-WSL2）
WSL 发行版：Ubuntu 24.04
Java 版本：Java 11.0.24
Python 版本：使用 Python 3.7、3.9、3.10、3.11 和 3.12 测试
Anaconda 版本：Anaconda3-2024.06-1-Linux-x86_64
H2O 版本：3.42.0.2 至 3.46.0.5

问题描述：
当我使用 Python 中的 h2o.init() 在 WSL 上启动 H2O 时，它会尝试形成一个大小为 2 的云，包括 10.255.255.254:54321 处的节点，但失败并出现“连接被拒绝”错误。此问题不会发生在装有 Ubuntu 操作系统的专用 PC 上。
采取的步骤：

强制本地连接：使用 h2o.init(ip=&quot;127.0.0.1&quot;, port=54321,
force_connect=True) 确保 H2O 仅在本地运行。没有成功。

网络配置：已验证没有防火墙设置阻止必要的端口。暂时禁用防火墙，但问题仍然存在。

WSL 配置：使用 wsl --update 确保 WSL 是最新的。
重新启动 WSL 和机器，但问题仍然存在。

H2O 配置：检查可能指示 H2O 连接到其他节点的配置。将 H2O 设置为仅使用本地 IP
127.0.0.1。

日志分析：查看了各种 H2O 日志，所有日志都表明与节点 10.255.255.254:54321 存在相同的连接错误。

连接测试：使用 python3 -m http.server
54321 测试了连接，并且能够从 WSL 外部访问服务器而不会出现问题。

与 Ubuntu 笔记本电脑的比较：在运行原生 Ubuntu 的笔记本电脑上，
H2O 运行正常，表明问题特定于 WSL。


附加信息：
直到 2024 年 5 月，H2O 在我的系统上运行良好。自上次运行以来所做的唯一更改是保持 Windows 和 WSL 更新。该问题于 2024 年 9 月 20 日左右被发现。
什么原因可能导致此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79091245/connection-refused-error-with-h2o-on-wsl-after-recent-windows-update</guid>
      <pubDate>Tue, 15 Oct 2024 18:14:42 GMT</pubDate>
    </item>
    <item>
      <title>RVC 无法转换外部音频[关闭]</title>
      <link>https://stackoverflow.com/questions/79090806/rvc-cant-convert-external-audios</link>
      <description><![CDATA[我不知道为什么，但是对于我训练的模型，我只能从 audacity 转换音频记录。
当我尝试使用下载的音频（如 youtube mp3 或 obs）然后通过 audacity 转换时，它就是不起作用，每次都在 5.6 秒后出现错误。我使用多个 python 版本（3.10 和 3.9）进行了多次全新安装。
问题是控制台中没有错误，所以我很绝望。这里有一些屏幕截图，以便您可以更好地理解。如果它有帮助，我在一台 MSI katana 笔记本电脑上，配备 rtx 4060 和 i5 12th 以及 Windows 11。
我多次按照 README.MD 中的安装程序进行操作，但我无法让它工作。这是屏幕截图（https://i.sstatic.net/TqwriGJj.png）（https://i.sstatic.net/kEPoi1cb.png）（https://i.sstatic.net/1972o7s3.png）]]></description>
      <guid>https://stackoverflow.com/questions/79090806/rvc-cant-convert-external-audios</guid>
      <pubDate>Tue, 15 Oct 2024 16:03:47 GMT</pubDate>
    </item>
    <item>
      <title>从音频中提取购物清单的快速方法[关闭]</title>
      <link>https://stackoverflow.com/questions/79090665/quick-way-to-extract-a-shopping-list-from-audio</link>
      <description><![CDATA[我想编写代码来从音频文件中提取零件列表。我不确定如何进行，我想听听大家是否有任何建议。
音频文件包含一个声音，例如说“我们应该得到四个 16 英寸的轮子，每个轮子都有一个轮胎，然后是 2 个 5 英尺长和 2 英寸宽的铜管......”，列表应该有类似的东西
4 XWB16
4 XWB16T
2 PCPR52

我要使用的第一步是获取音频文件的转录 - 我会遵循 hugging 的 ASR 教程。然后，我可以编写一个简单的脚本，逐个读取单词列表，并且每次找到与配置文件中存储的项目之一匹配的内容时，它都会将一个元素添加到列表中。
但是，这种方法需要手动编码每个项目的所有潜在变化，例如“铜管”和“铜管”，还需要处理我在上面的示例中使用的情况（“4 个轮子，每个轮子一个轮胎”）。
有没有办法将零件代码和每个项目的描述一起提供给机器学习模型，并接收零件清单作为输出？]]></description>
      <guid>https://stackoverflow.com/questions/79090665/quick-way-to-extract-a-shopping-list-from-audio</guid>
      <pubDate>Tue, 15 Oct 2024 15:24:11 GMT</pubDate>
    </item>
    <item>
      <title>Numpy 中的感知器简单实现与其列表实现截然不同</title>
      <link>https://stackoverflow.com/questions/79090615/perceptron-naive-implementation-in-numpy-drastically-diverges-from-its-list-impl</link>
      <description><![CDATA[在制作一个简单的机器学习项目时，我决定使用 numpy 数组重新设计一段列表逻辑。只需将循环中的数值增量更改为 numpy 算法。但是，numpy 实现给出的决策边界与循环实现的决策边界相差甚远，行为混乱，并且不会收敛。
完整的工作示例可以在 Google Colab 中找到：https://colab.research.google.com/drive/1zLy0oTidhm2lrwkASgT1lpsI9yfrBPvx?usp=sharing
这真的让我很困扰。如果是 numpy 精度错误，在较大的项目中可能不那么明显，并会阻碍结果。如果是概念错误，我看不到。这个问题很可能看起来很幼稚，但我真的想知道出了什么问题。
考虑以下感知器训练的列表实现。其背后的逻辑将在下文中进一步解释，但最重要的是，看看 W 和 b 的循环 num 值增量如何没有什么特别的。
def perceptronStep(X, y, W, b, learn_rate = 0.01):
diff = []
for i in range(len(X)):
y_hat = prediction(X[i],W,b)[0]
# diff=0 表示预测正确
# diff=1 或 diff=-1 表示 W 和 b 变化的方向
diff.append(y[i] - y_hat)
dif = diff[i]
W[0] += dif * X[i][0]*learn_rate
W[1] += dif * X[i][1]*learn_rate
b += dif * learn_rate
return W, b, diff

在我看来，以下numpy 实现应该可以完美重现 perceptronStep 的行为方式。
def np_perceptronStep(X, y, W, b, learn_rate = 0.01):
Y_hat = np.squeeze(prediction(X, W, b))
diff = y - Y_hat
sumX = np.sum(X * diff[..., np.newaxis], axis=0, keepdims=True)
W += learn_rate * sumX.T # W_np
b += learn_rate * np.sum(diff) # b_np
return W, b, diff

但事实并非如此。 W_np 和 b_np 与 W 和 b 有所不同，从第 0 纪元开始相差 1.0e-16 秒，在训练结束时相差 10 秒。b_np 到处跳跃，而 b 表明它应该很快稳定下来。混乱。

通过阶跃函数获得预测。
def prediction(X, W, b):
Y_hat = np.matmul(X,W)+b
# 阶跃函数
Y_hat[Y_hat &gt;= 0] = 1
Y_hat[Y_hat &lt; 0] = 0
返回 Y_hat

上下文
我学习了Udacity 上的 PyTorch 机器学习入门。在教授梯度下降之前，介绍了一个粗略的技巧。考虑一个感知器，它有一个线性分类器 Wx+b，在本例中为 w1x1 + w2x2 + b，其中两个可能的类是 0 和 1。将点 x&#39; 的真实类别与预测类别之间的差异设为 c，感知器将按如下方式更新：W + acx&#39; 和 b + ac，其中 a 是学习率。非常简单。
我尝试在 np_perceptronStep 中手动从循环过渡到 np，希望一件事会导致问题。我尝试了确定 diff、W 和 b 的不同组合 - 无论是在循环中还是使用 np。我跟踪了两个实现的 W 和 b 之间的差异如何变化。它们发生了变化，但 np_perceptronStep 从未接近 perceptronStep 结果。]]></description>
      <guid>https://stackoverflow.com/questions/79090615/perceptron-naive-implementation-in-numpy-drastically-diverges-from-its-list-impl</guid>
      <pubDate>Tue, 15 Oct 2024 15:14:59 GMT</pubDate>
    </item>
    <item>
      <title>多类 SVM 中的一个类别几乎所有数据点都作为支持向量是正常的吗？</title>
      <link>https://stackoverflow.com/questions/79090407/is-it-normal-for-a-class-in-multiclass-svm-to-have-nearly-all-data-points-as-sup</link>
      <description><![CDATA[我正在使用 scikit-learn 的 SVC 对鸢尾花数据集进行多类分类，其中一个类几乎所有数据点都作为支持向量。这是预期的结果吗，还是我的实现或参数设置存在问题？
import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier

# 读取 Excel 文件
df = pd.read_excel(&quot;Classification iris.xlsx&quot;, index_col=&quot;instance_id&quot;)

# 将数据拆分为训练数据和测试数据
train_data = pd.concat([
df.loc[1:35],
df.loc[51:85],
df.loc[101:135]
])

test_data = pd.concat([
df.loc[36:50],
df.loc[86:100],
df.loc[136:150]
])

# 分离特征和目标变量
train_features = train_data.drop(columns=[&#39;class&#39;])
train_target = train_data[&#39;class&#39;]
test_features = test_data.drop(columns=[&#39;class&#39;])
test_target = test_data[&#39;class&#39;]

# 使用 One-vs-Rest 策略构建支持向量机
svm = OneVsRestClassifier(SVC(kernel=&#39;linear&#39;, C=1e5))
svm.fit(train_features, train_target)

# 进行预测
targets_train_pred = svm.predict(train_features)
targets_test_pred = svm.predict(test_features)

# 计算总误差
total_training_error = sum(targets_train_pred != train_target) / len(train_data)
total_testing_error = sum(targets_test_pred != test_target) / len(test_data)

# 打印总训练和测试错误
print(f&quot;Q2.2.2 使用标准 SVM 模型计算:\ntotal training error: {total_training_error:.10f}, total testing error: {total_testing_error:.10f}&quot;)

# 初始化列表以存储结果
classes = [&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;]
linear_separable = []

for i, class_name in enumerate(classes):
# 获取当前类的二分类器
classifier = svm.estimators_[i]

# 计算每个类的错误
train_class_error = sum((targets_train_pred == class_name) != (train_target == class_name)) / len(train_data)
test_class_error = sum((targets_test_pred == class_name) != (test_target == class_name)) / len(test_data)

# 获取权重向量 w 和偏差 b
w = classifier.coef_[0]
b = classifier.intercept_[0]
support_vector_indices = classifier.support_

# 以正确的格式打印每个类的结果
print(f&quot;\nclass {class_name}:&quot;)
print(f&quot;training error: {train_class_error:.10f}, testing error: {test_class_error:.10f}&quot;)
print(f&quot;w: [{&#39;, &#39;.join([f&#39;{x:.10f}&#39; for x in w])}]&quot;)
print(f&quot;b: {b:.10f}&quot;)
print(f&quot;support vector indices: {support_vector_indices.tolist()}&quot;)

# 检查类是否线性可分
if train_class_error == 0:
linear_separable.append(class_name)

print(f&quot;\nLinear separable classes: {&#39;, &#39;.join(linear_separable)}&quot;)

这是输出：
Q2.2.2 使用标准 SVM 模型进行计算：
总训练误差：0.0571428571，总测试误差：0.0888888889

class setosa：
训练误差：0.0000000000，测试误差：0.0000000000
w：[0.0097327108， 0.5377790363, -0.8273513712, -0.3820427629]
b: 0.7734548984
支持向量索引：[42, 23, 24]

类 versicolor:
训练错误：0.0000000000，测试错误：0.0000000000
w：[1.8485997715, -4.5023738999, -1.1043393026, 0.3212849949]
b：5.6773042297
支持向量索引：[1, 2, 8, 9, 13, 25, 27, 34, 71, 72, 73, 77, 78, 80, 81, 86, 88, 89, 90, 91, 92, 93, 96, 99, 100, 102, 103, 104, 35, 36, 37, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 52, 55, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69]

class virginica:
训练错误：0.0000000000，测试错误：0.0000000000
w：[-3.6465034341， -5.1763639697, 7.4285254512, 11.0024158268]
b：-17.5703922240
支持向量索引：[55, 57, 62, 68, 96, 97, 99, 103]

线性可分类别：setosa、versicolor、virginica

由于 Versicolor 只有 35 个训练数据点，因此根据我对 SVM 的理论理解，我并不认为它们都是支持向量。此外，现在各个类别的训练误差对我来说似乎有点奇怪。]]></description>
      <guid>https://stackoverflow.com/questions/79090407/is-it-normal-for-a-class-in-multiclass-svm-to-have-nearly-all-data-points-as-sup</guid>
      <pubDate>Tue, 15 Oct 2024 14:28:31 GMT</pubDate>
    </item>
    <item>
      <title>Unet 或如何创建幽灵模特效果？[关闭]</title>
      <link>https://stackoverflow.com/questions/79090398/unet-or-how-do-i-create-a-ghost-mannequin-effect</link>
      <description><![CDATA[我怎样才能只提取衣服 - 不提取手或其他部位，只提取衣服的全貌？
我们可以使用 U-net 等。它就像幽灵模特效果或半身像
import sys
sys.path.append(&#39;/content/clothes-extractor&#39;)

from dataset import ClothesDataset, ClothesDataLoader
from config import Config

cfg = Config()
cfg.dataset_dir = &quot;/content/clothes-extractor/data/train&quot; 
cfg.load_height = 224
cfg.load_width = 224
cfg.batch_size = 1

clothes_dataset = ClothesDataset(cfg, dataset_mode=&quot;train&quot;, device=&quot;cpu&quot;)
clothes_loader = ClothesDataLoader(dataset=clothes_dataset, batch_size=cfg.batch_size)

for idx in range(len(clothes_loader)):
result = clothes_loader[idx]
img_name = result[&quot;img_name&quot;]

print(f&quot;Extracted clothing from {img_name}&quot;)

result = clothes_dataset[2] # 大小：已验证 2 个字符

image_keys = [&quot;centered_mask_body&quot;, &quot;cloth_mask&quot;, &quot;target&quot;]
fig,axes=plt.subplots(1,len(image_keys),figsize=(20,20))
]]></description>
      <guid>https://stackoverflow.com/questions/79090398/unet-or-how-do-i-create-a-ghost-mannequin-effect</guid>
      <pubDate>Tue, 15 Oct 2024 14:27:04 GMT</pubDate>
    </item>
    <item>
      <title>没有传感器数据，预测模型还能有效吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79089664/can-predictive-models-be-effective-without-sensor-data</link>
      <description><![CDATA[我有纠正和预防性维护数据。对于纠正性维护，我有两台机器及其问题（只有一句话，没有数值）、日期、解决方案和停机时间。对于预防性维护，我有对同一两台机器进行季度、年度还是每月维护的数据。
如果我依靠这些值来创建机器学习模型来预测下一次故障，我的目标是回答这个问题——预防性任务完成后停机频率是增加还是减少，我的模型是否可靠并提供良好的预测，或者由于缺乏传感器数据，结果是否不准确？
我正在尝试清理数据并获取停机原因等特征，但我迷路了。我考虑过使用随机森林，但我不确定我的数据是否足够。]]></description>
      <guid>https://stackoverflow.com/questions/79089664/can-predictive-models-be-effective-without-sensor-data</guid>
      <pubDate>Tue, 15 Oct 2024 11:09:03 GMT</pubDate>
    </item>
    <item>
      <title>使用 AI 进行图像重叠检测</title>
      <link>https://stackoverflow.com/questions/79089453/image-overlap-detection-with-ai</link>
      <description><![CDATA[我正在研究一个项目，需要检测图像或图形是否重叠。我一直在研究 AWS Rekognition，因为它们可以进行一些图像分析并提供边界框，我可以通过这些边界框计算它们是否重叠。但是，我正在寻找其他想法的指针，这些想法可能更强大、更可扩展。
我遇到过 openCV，但从未听说过，也没有尝试过。这可能是一个负责任的解决方案，有人可以提供一些见解吗？
我有很多可以通过监督学习提供的图像，因此我可以使用我已经拥有的数据来训练模型也可能是一个有效的选择。
有任何意见、想法或指向我可以进行更多研究的地方的指针吗？
]]></description>
      <guid>https://stackoverflow.com/questions/79089453/image-overlap-detection-with-ai</guid>
      <pubDate>Tue, 15 Oct 2024 10:07:25 GMT</pubDate>
    </item>
    <item>
      <title>summary() 函数在 cnn 中不起作用（ValueError：不支持未定义的形状。）</title>
      <link>https://stackoverflow.com/questions/79084869/summary-function-not-working-in-cnn-valueerror-undefined-shapes-are-not-supp</link>
      <description><![CDATA[我正在尝试创建一个分类网络，用于识别来自 cifar10 数据集的图片。
当我尝试使用 summary() 函数时，我总是收到此错误。
ValueError Traceback (most recent call last)
Cell In[267], line 4
1 #base_model.summary()
2 #top_model.summary()
3 #print(base_model.output_shape)
----&gt; 4 model2.summary()

文件 c:\Users\noahc\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 c:\Users\noahc\anaconda3\Lib\site-packages\optree\ops.py:747，在 tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests) 中
745 leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
746 flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--&gt; 747 return treespec.unflatten(map(func, *flat_args))

ValueError：不支持未定义的形状。

代码如下...
import tensorflow as tf
from keras.applications import VGG16

base_model = VGG16(weights=&#39;imagenet&#39;, include_top=False, input_shape=(32, 32, 3))

top_model = tf.keras.Sequential([
layer.Flatten(input_shape=base_model.output_shape[1:]),
layer.Dense(10,activation=&#39;softmax&#39;)
])

for layer in base_model.layers[:10]:
layer.trainable = False

model2 = tf.keras.models.Sequential([
base_model,
top_model
])

model2.summary() # 出现错误这里

我已经为基础模型和顶层模型做了总结，效果很好。但是当我测试 model2 时，出现了错误。不知道为什么。不确定“未定义”形状是什么意思。不知道还能尝试什么。当我只取 vgg16 的前 11 或 15 层时，总结就起作用了。我听说这可能是 python 版本本身的问题，但我不知道……]]></description>
      <guid>https://stackoverflow.com/questions/79084869/summary-function-not-working-in-cnn-valueerror-undefined-shapes-are-not-supp</guid>
      <pubDate>Mon, 14 Oct 2024 05:50:22 GMT</pubDate>
    </item>
    <item>
      <title>RNN 中多对一预测的损失计算</title>
      <link>https://stackoverflow.com/questions/79074702/loss-calculation-for-many-to-one-prediction-in-rnn</link>
      <description><![CDATA[我正在尝试在 PyTorch 中实现一个简单的 RNN 模型，用于多对一时间序列预测。
假设我记录了 1-7 的观察结果，设计矩阵的结构如下：
[1,2,3,4]
[2,3,4,5]
[3,4,5,6]
[4,5,6,7]
这个想法是基于之前的四个观察结果做出一个预测，即多对一预测。
[1,2,3,4] - [5]
[2,3,4,5] - [6]
[3,4,5,6] - [7]
[4,5,6,7] - [8]
并且 [5,6,7,8] 构成了我的预测序列值。
我理解，每个数据点通过 RNN 模型时，都会生成一个预测。我的问题涉及每个时间步骤的损失计算。
具体来说，在第一个数据点 1 传入模型并产生预测后，会产生 1&#39;。

然后模型是否会查看预测 1&#39; 和目标 5，然后计算 1&#39; 和 5 之间的损失，并将预测 2&#39; 与 5 进行比较，依此类推。

所以基本上第一个序列是 [1,2,3,4]，相应的目标是 [5,5,5,5]。该模型总共计算损失 4 次，每对一次。

或者它是否让所有四个时间步骤（对应于数据点 1-4）通过并识别出这 4 个数据点的序列应该产生一个预测。因此，模型处理所有前三个数据点 1、2、3，并在不考虑目标的情况下产生预测 1&#39;、2&#39;、3&#39;。它计算损失直到第四次预测 4&#39; 之后，这个损失 L(4&#39;, 5) 以某种方式代表了该序列的总损失，然后向后传播。

在阅读了大量教程和我的课程文本后，我倾向于相信，对于一系列数据点 [1,2,3,4]，我们仍然必须为该模型提供一个目标 [2,3,4,5]，这样模型就会学习到 1&#39; 是 2 的预测（向前迈出一步），2&#39; 是 3 的预测，并相应地适应模式。
在这种情况下，多对一预测与一对一预测的不同之处仅在于我们只是提取最后一个预测，并且 RNN 模型在参数更新方面的内部工作原理与代码中具体呈现的内容相同（输入形状等）。]]></description>
      <guid>https://stackoverflow.com/questions/79074702/loss-calculation-for-many-to-one-prediction-in-rnn</guid>
      <pubDate>Thu, 10 Oct 2024 13:44:29 GMT</pubDate>
    </item>
    <item>
      <title>GNU Octave 是多线程的吗？</title>
      <link>https://stackoverflow.com/questions/79050512/is-gnu-octave-multi-threaded</link>
      <description><![CDATA[根据这个老问题的答案，GNU Octave 似乎是一个单线程应用程序。
但是，我正在试验一个名为nnet的旧 Octave 神经网络包，并惊讶地发现我的 Octave 程序使用了笔记本电脑的所有 4 个核心。自从我链接的问题提出以来，情况有变化吗？GNU Octave 现在是多线程的吗？据我所知，我没有看到 nnet 内部有任何并行实现。
有关我的安装的一些信息：

我的操作系统是 Linux Mint 20
我的机器有 4 个处理单元（这是 nproc 在我的终端中显示的内容）
我的 Octave 版本是 5.2.0（如果这有区别的话，我正在使用 GUI）

我的代码相当简单，只导入了 nnet 包，没有其他内容。当我查看运行程序时的资源时，我看到所有核心都已使用（下面是 htop 屏幕截图）

这是我正在做的事情：
pkg load nnet

starttime = clock();

# 取自 http://matlab.izmiran.ru/help/toolbox/nnet/newff.html
Pr = -1:0.00005:1;
Tr = 3*sin(pi*Pr)-cos(pi*Pr);
Prmin = min(Pr);
Prmax = max(Pr);
net = newff([Prmin Prmax],[3 2 1],{&#39;tansig&#39;,&#39;logsig&#39;,&#39;purelin&#39;},&#39;trainlm&#39;);
[net] = train(net,Pr,Tr,[],[],[]);
[netoutput] = sim(net,Pr);

etime(clock(),starttime)

% 测试结果 
plot(Pr,Tr,&#39;b+&#39;);
hold on; 
plot(Pr,netoutput,&#39;r-&#39;);
hold off;

编辑
按照评论中的 @JérômeRichard 提示和 @NickJ 建议，我通过在终端中执行 export OMP_NUM_THREADS=1 并启动 Octave 来启动 Octave，只为 BLAS 分配 1 个线程。该脚本的速度是原始设置的两倍（根据上面发布的 htop 屏幕截图，默认设置是 4）。我确保我的程序只使用一个核心和 htop。]]></description>
      <guid>https://stackoverflow.com/questions/79050512/is-gnu-octave-multi-threaded</guid>
      <pubDate>Thu, 03 Oct 2024 12:19:13 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenCV 改进这种图像自然背景扩展方法？</title>
      <link>https://stackoverflow.com/questions/78969286/how-can-i-improve-this-approach-for-natural-background-extension-in-an-image-usi</link>
      <description><![CDATA[我正在使用 Python 中的 OpenCV 扩展图像的背景。我目前的方法是复制边框并对扩展区域应用高斯模糊以将它们混合到原始图像中。目标是使背景扩展看起来更自然，尤其是对于具有一致纹理的图像。
这是我当前使用的代码：
import cv2
import numpy as np

def expand_image_with_smart_blend(image_path, top=50, bottom=50, left=50, right=50):
img = cv2.imread(image_path)
original_h, original_w = img.shape[:2]

expanded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_REPLICATE)

blured_img = expand_img.copy()

if top &gt; 0:
blured_img[0:top, :] = cv2.GaussianBlur(expanded_img[0:top, :], (51, 51), 0)

如果底部 &gt; 0:
blured_img[original_h + top:original_h + top + bottom, :] = cv2.GaussianBlur(expanded_img[original_h + top:original_h + top + bottom, :], (51, 51), 0)

如果左侧 &gt; 0:
blured_img[:, 0:left] = cv2.GaussianBlur(expanded_img[:, 0:left], (51, 51), 0)

如果右侧 &gt; 0:
blured_img[:, original_w + left:original_w + left + right] = cv2.GaussianBlur(expanded_img[:, original_w + left:original_w + left + right], (51, 51), 0)

cv2.namedWindow(&quot;智能混合扩展图像&quot;, cv2.WINDOW_NORMAL)
cv2.namedWindow(&quot;原始图像&quot;, cv2.WINDOW_NORMAL)
cv2.imwrite(&#39;expanded_smart_blended_image.jpg&#39;, blured_img)
cv2.imshow(&#39;智能混合扩展图像&#39;, blured_img)
cv2.imshow(&quot;原始图像&quot;, img)
cv2.waitKey(0)
cv2.destroyAllWindows()

expand_image_with_smart_blend(&#39;test_img.jpg&#39;, top=100, bottom=100, left=100, right=100)

我尝试过的方法：
cv2.BORDER_REPLICATE：我使用它将原始图像的边缘复制到新扩展的区域中。
高斯模糊：应用于扩展区域以柔化原始图像和新区域之间的过渡。
问题：
结果在某种程度上是可以接受的，但过渡仍然看起来不像我想要的那样自然。特别是：
某些区域的过度模糊使背景看起来不真实。
对于纹理更复杂的图像，边缘复制并不总是有效。
原始图像 结果图像
问题：
在 OpenCV 或其他库中，是否有更复杂的方法来扩展图像的背景，从而产生更自然、无缝的结果？我愿意接受涉及高级图像处理技术或机器学习的方法。任何使用扩散模型的方法都可以。]]></description>
      <guid>https://stackoverflow.com/questions/78969286/how-can-i-improve-this-approach-for-natural-background-extension-in-an-image-usi</guid>
      <pubDate>Tue, 10 Sep 2024 11:32:09 GMT</pubDate>
    </item>
    <item>
      <title>如何将泊松 CDF 写成 Python 极坐标表达式</title>
      <link>https://stackoverflow.com/questions/75303038/how-to-write-poisson-cdf-as-python-polars-expression</link>
      <description><![CDATA[我有一组用于生成 ML 模型特征的极坐标表达式。我想向该集合添加泊松 cdf 特征，同时保持惰性执行（具有速度、缓存等优势...）。到目前为止，我还没有找到一种简单的方法来实现这一点。
我已经能够使用以下方法在所需的惰性表达式框架之外获得我想要的结果：
import polars as pl
from scipy.stats import poisson

df = pl.DataFrame({&quot;count&quot;: [9,2,3,4,5], &quot;expected_count&quot;: [7.7, 0.2, 0.7, 1.1, 7.5]})
result = poisson.cdf(df[&quot;count&quot;].to_numpy(), df[&quot;expected_count&quot;].to_numpy())
df = df.with_columns(pl.Series(result).alias(&quot;poisson_cdf&quot;))

然而，实际上我希望它看起来像这样：
df = pl.DataFrame({&quot;count&quot;: [9,2,3,4,5], &quot;expected_count&quot;: [7.7, 0.2, 0.7, 1.1, 7.5]})
df = df.select(
[
... # 这里还有一堆其他表达式
poisson_cdf()
]
)

其中 poisson_cdf 是一些极坐标表达式，例如：
def poisson_cdf():
# 这只是为了说明，显然不起作用
return scipy.stats.poisson.cdf(pl.col(&quot;count&quot;), pl.col(&quot;expected_count&quot;)).alias(&quot;poisson_cdf&quot;)

我还尝试使用由 &quot;count&quot; 和 &quot;expected_count&quot; 组成的结构，并在应用自定义函数时按照文档中的建议进行应用。但是，我的数据集实际上有几百万行 - 导致执行时间荒谬。
任何建议或指导都将不胜感激。理想情况下，在某个地方存在这样的表达式？提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/75303038/how-to-write-poisson-cdf-as-python-polars-expression</guid>
      <pubDate>Tue, 31 Jan 2023 20:55:29 GMT</pubDate>
    </item>
    <item>
      <title>keras（或任何其他 ML 框架）如何计算反向传播的 lambda 函数层的梯度？</title>
      <link>https://stackoverflow.com/questions/41331604/how-does-kerasor-any-other-ml-framework-calculate-the-gradient-of-a-lambda-fun</link>
      <description><![CDATA[Keras 允许添加一个层来计算用户定义的 lambda 函数。
我不明白的是 Keras 如何知道为反向传播计算这个用户定义函数的梯度。]]></description>
      <guid>https://stackoverflow.com/questions/41331604/how-does-kerasor-any-other-ml-framework-calculate-the-gradient-of-a-lambda-fun</guid>
      <pubDate>Mon, 26 Dec 2016 12:52:33 GMT</pubDate>
    </item>
    </channel>
</rss>