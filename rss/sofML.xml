<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 22 Jan 2025 09:17:50 GMT</lastBuildDate>
    <item>
      <title>Matlab 神经网络预测结果趋近于 1</title>
      <link>https://stackoverflow.com/questions/79376474/matlab-nueral-network-predictions-convering-to-1</link>
      <description><![CDATA[我对 matlab 有一定的了解，对神经网络也有一些基本的了解。我正在尝试训练神经网络，但遇到了障碍。我有一个简化版的我面临的问题：
设置：
NN = trainnet(X_data, Y_data, NN, &#39;crossentropy&#39;, options);

X_data 大小为 25455x3，其中 3 列中的每一列都经过了归一化（平均值 = 0 和标准差 = 1）
Y_data 大小为 25455x1，值为 1 或 0（二元分类）（sum(Y_data)= 11541(~45%))
NN 是从以下代码中新生成的 dlnetwork：

 featureInputLayer(3, &quot;Name&quot;, &quot;InputLayer&quot;)
fullyConnectedLayer(32, &quot;Name&quot;, &quot;HiddenLayer1&quot;, &quot;WeightsInitializer&quot;, &quot;he&quot;) 
reluLayer(&quot;Name&quot;, &quot;ReLU&quot;)
dropoutLayer(0.2, &quot;Name&quot;, &quot;Dropout&quot;) 
fullyConnectedLayer(1, &quot;Name&quot;, &quot;OutputLayer&quot;, &quot;WeightsInitializer&quot;, &quot;glorot&quot;)
sigmoidLayer(&quot;Name&quot;, &quot;SigmoidOutput&quot;) 
];
NN = dlnetwork(NN);


选项：

选项 = trainingOptions(&quot;adam&quot;, ...
LearnRateSchedule = &quot;piecewise&quot;, ...
LearnRateDropFactor = 0.2, ...
LearnRateDropPeriod = 5, ...
MaxEpochs = 1, ... 
MiniBatchSize = 128, ...
ExecutionEnvironment = &quot;cpu&quot;, ...
Plots = &quot;none&quot;);

问题：
当我运行单行 NN = trainnet(X_data, Y_data, NN, &#39;crossentropy&#39;, options); 时，模型运行 194 次迭代，最终 trainingloss 为 0.044545，然而，在类似的测试数据上，模型的测试准确率仅为 ~45%，但更令人担忧的是，使用类似数据生成的预测非常偏向 1。事实上，最低预测是 0.6876，平均值是 0.9030。这是一个巨大的飞跃，对我来说毫无意义。我希望模型的预测保持大约 0.5 的正常值（目前不考虑任何学习）或者可能稍微少一点以匹配略低的 1 个标签数量。有人能帮助解释为什么会发生这种情况以及我该如何解决它吗？谢谢。
--额外说明：我真正做的是运行一个循环，该循环运行我上面提到的代码行。我描述的问题只是循环的第一次迭代。随着它的继续，偏差变得更加极端，直到所有预测都只有 1.00000。每次循环中的数据都不同，但它非常相似，并且其中存在应该可以学习的趋势（从其他类型的回归建模中发现）。]]></description>
      <guid>https://stackoverflow.com/questions/79376474/matlab-nueral-network-predictions-convering-to-1</guid>
      <pubDate>Wed, 22 Jan 2025 03:36:04 GMT</pubDate>
    </item>
    <item>
      <title>解释 TensorFlow 决策森林 Python 中的变量重要性方法</title>
      <link>https://stackoverflow.com/questions/79376427/explaining-variable-importance-methods-in-tensorflow-decision-forests-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79376427/explaining-variable-importance-methods-in-tensorflow-decision-forests-python</guid>
      <pubDate>Wed, 22 Jan 2025 03:01:46 GMT</pubDate>
    </item>
    <item>
      <title>机器学习检查过度拟合[关闭]</title>
      <link>https://stackoverflow.com/questions/79376198/machine-learning-checking-for-overfitting</link>
      <description><![CDATA[我训练了一个机器学习模型，但不确定它是否过度拟合。使用训练集进行预测时的准确率、精确率、召回率和 f1 分数均为 1.0，而对于测试集，所有分数均为 ~0.9。我知道当它不能很好地概括测试集时就会发生过度拟合，但我的测试集结果相当高。我感到困惑的是，训练集是完美的。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/79376198/machine-learning-checking-for-overfitting</guid>
      <pubDate>Tue, 21 Jan 2025 23:37:48 GMT</pubDate>
    </item>
    <item>
      <title>训练 Hugging Face Transformer 期间 GPU 利用率几乎始终为 0</title>
      <link>https://stackoverflow.com/questions/79375287/gpu-utilization-almost-always-0-during-training-hugging-face-transformer</link>
      <description><![CDATA[我正在使用我的发票数据对 Donut Cord-v2 模型进行微调，该发票数据在预处理并作为数据集保存在磁盘上时大小约为 360 GB。我几乎完全按照这个笔记本进行操作，只是我有 6 个训练周期而不是 3 个。
我在单个 Nvidia H100 SXM GPU / Intel Xeon® Gold 6448Y / 128 GB RAM 上进行训练。
每当我开始训练并使用 htop 和 nvidia-smi 检查 CPU 和 GPU 利用率时，我都会看到 CPU 的利用率为 10-12%，由 python 使用，GPU 内存几乎一直被占用 90%，但 GPU 利用率几乎始终为 0。如果我不断刷新 nvidia-smi 的输出，则每 10-12 秒一次，利用率将跳转到100% 然后立即回到 0。我不禁感觉到我的 CPU 和 GPU 之间存在瓶颈，CPU 尝试不断处理数据并将其发送到 GPU，GPU 处理速度非常快，并且只是闲置，等待来自 CPU 的下一批。我从磁盘加载已经预处理的数据集，如下所示：
from datasets import load_from_disk
processed_dataset = load_from_disk(r&quot;/dataset/dataset_final&quot;)

我的处理器配置如下：
from transformers import DonutProcessor

new_special_tokens = [] # 将添加到 tokenizer 的新 token
task_start_token = &quot;&lt;s&gt;&quot; # 任务 token 的启动
eos_token = &quot;&lt;/s&gt;&quot; # tokenizer 的 eos token

processor = DonutProcessor.from_pretrained(&quot;naver-clova-ix/donut-base-finetuned-cord-v2&quot;)

# 向 tokenizer 添加新的特殊 token
processor.tokenizer.add_special_tokens({&quot;additional_special_tokens&quot;: new_special_tokens + [task_start_token] + [eos_token]})

# 我们更新了一些与预训练不同的设置；即图像的大小 + 无需旋转
processor.feature_extractor.size = [1200,1553] # 应为 (宽度, 高度)
processor.feature_extractor.do_align_long_axis = False

我的模型配置是：
import torch
from transformers import VisionEncoderDecoderModel, VisionEncoderDecoderConfig

#print(torch.cuda.is_available())

# 从 huggingface.co 加载模型
model = VisionEncoderDecoderModel.from_pretrained(&quot;naver-clova-ix/donut-base-finetuned-cord-v2&quot;)

# 调整嵌入层的大小以匹配词汇表大小
new_emb = model.decoder.resize_token_embeddings(len(processor.tokenizer))
print(f&quot;新嵌入大小： {new_emb}&quot;)
# 调整我们的图像大小和输出序列长度
model.config.encoder.image_size = process.feature_extractor.size[::-1] # (height, width)
model.config.decoder.max_length = len(max(processed_dataset[&quot;train&quot;][&quot;labels&quot;], key=len))

# 添加解码器启动的任务令牌
model.config.pad_token_id = process.tokenizer.pad_token_id
model.config.decoder_start_token_id = process.tokenizer.convert_tokens_to_ids([&#39;&lt;s&gt;&#39;])[0]

我的训练代码是：
import gc
gc.collect()

torch.cuda.empty_cache()

from transformers import Seq2SeqTrainingArguments，Seq2SeqTrainer

导入日志记录
logging.basicConfig(level=logging.INFO)

# 训练参数
training_args = Seq2SeqTrainingArguments(
output_dir=r&quot;/trained&quot;, # 指定本地目录保存模型
num_train_epochs=6,
learning_rate=2e-5,
per_device_train_batch_size=8,
weight_decay=0.01,
fp16=True,
logs_steps=50,
save_total_limit=2,
evaluation_strategy=&quot;no&quot;,
save_strategy=&quot;epoch&quot;,
predict_with_generate=True,
report_to=&quot;none&quot;,
# 禁用推送到集线器
push_to_hub=False

)

# 创建训练器
trainer = Seq2SeqTrainer(
model=model,
args=training_args,
train_dataset=processed_dataset[&quot;train&quot;],
)

# 开始训练
trainer.train()

使用 360 GB 数据集完成 6 个 epoch 的训练预计需要 54 小时。当我在装有 Intel i9 11900KF / RTX 3050 的 PC 上运行完全相同的代码时，我发现 GPU 利用率一直保持在 100%。我的代码中是否存在瓶颈？为什么 CPU 会继续处理已经预处理的数据集？ Cuda 12.6
编辑：
由于我的 RAM 和 CPU 核心数量允许，将 Seq2SeqTrainer 的 dataloader_num_workers 参数更改为 &gt;0 值是否有意义？（并且 CPU 利用率最高为 10-12%）]]></description>
      <guid>https://stackoverflow.com/questions/79375287/gpu-utilization-almost-always-0-during-training-hugging-face-transformer</guid>
      <pubDate>Tue, 21 Jan 2025 17:09:03 GMT</pubDate>
    </item>
    <item>
      <title>我们可以通过编程找出 Android 设备使用的 NPU 吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79374045/can-we-find-out-the-npu-used-by-the-android-device-programatically</link>
      <description><![CDATA[是否有任何基于 C/C++ 或 Java 的 API 来查明设备是否具有 NPU？]]></description>
      <guid>https://stackoverflow.com/questions/79374045/can-we-find-out-the-npu-used-by-the-android-device-programatically</guid>
      <pubDate>Tue, 21 Jan 2025 10:31:12 GMT</pubDate>
    </item>
    <item>
      <title>在 Keras 分类器中获取属性错误</title>
      <link>https://stackoverflow.com/questions/79374019/getting-attribute-error-in-keras-classifier</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79374019/getting-attribute-error-in-keras-classifier</guid>
      <pubDate>Tue, 21 Jan 2025 10:21:29 GMT</pubDate>
    </item>
    <item>
      <title>在保持批量大小、损失函数、架构、度量、数据、优化器的同时，TensorFlow 和 PyTorch 之间的差异</title>
      <link>https://stackoverflow.com/questions/79373655/discrepancies-between-tensorflow-and-pytorch-while-maintaining-batch-size-loss</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79373655/discrepancies-between-tensorflow-and-pytorch-while-maintaining-batch-size-loss</guid>
      <pubDate>Tue, 21 Jan 2025 08:27:06 GMT</pubDate>
    </item>
    <item>
      <title>Kubernetes MPIJob 在 3-GPU 集群上进行分布式推理时出现静默故障</title>
      <link>https://stackoverflow.com/questions/79373629/silent-failure-in-kubernetes-mpijob-for-distributed-inference-on-3-gpu-cluster</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79373629/silent-failure-in-kubernetes-mpijob-for-distributed-inference-on-3-gpu-cluster</guid>
      <pubDate>Tue, 21 Jan 2025 08:18:37 GMT</pubDate>
    </item>
    <item>
      <title>微调后的IP-Adapter模型未能取得有效效果</title>
      <link>https://stackoverflow.com/questions/79373102/the-finetuned-ip-adapter-model-fails-to-achieve-effective-results</link>
      <description><![CDATA[微调环境基于开源及其说明：
https://github.com/tencent-ailab/IP-Adapter
我使用图像+提示对作为训练数据，训练了一个用于微调的IP-Adapter模型。但是，经过微调的模型并没有反映出根据提示得出的预期结果。
训练图像和 .json 提示文件的 2 个失败案例：

白天图像的相同提示（17 张图像）：
例如，&quot;image_file&quot;: &quot;day_1.jpg&quot;, &quot;text&quot;: &quot;白天有阳光的城市。&quot;
夜间图像的相同提示（17 张图像）：
例如，&quot;image_file&quot;: &quot;night_1.jpg&quot;, &quot;text&quot;: &quot;夜晚有光的城市。没有阳光。&quot;
推理结果：训练后，输入白天图像，提示如 &quot;白天有阳光的城市。&quot;为了进行推理，生成的输出图像仍然是白天图像，而不是夜间图像。

针对数千张图像（1500 张中性面部图像和 2500 张悲伤面部图像）进行训练：
4 种不同的提示，用于表示中性面部表情的中性图像。
例如，&quot;image_file&quot;: &quot;neu_1.jpg&quot;, &quot;text&quot;: &quot;此人的面部表情为中性。&quot;
4 种不同的提示，用于表示悲伤面部表情的悲伤图像。
例如，&quot;image_file&quot;: &quot;sad_1.jpg&quot;, &quot;text&quot;: &quot;这个人正露出悲伤的表情。&quot;


推理结果：训练后，应用带有提示的中性图像，例如 &quot;一个人很伤心。&quot;，或 &quot;这个人正露出悲伤的表情。&quot; ，生成的输出人脸图像仍然很中性，一点也不悲伤。
微调步骤：

修改tutorial_train_plus.py中的代码：
将accelerator.save_state(save_path)
替换为accelerator.save_state(save_path, safe_serialization=False)

运行以下脚本进行微调：
accelerate launch --num_processes 2 --multi_gpu --mixed_precision &quot;fp16&quot; 
tutorial_train_plus.py 
--pretrained_model_name_or_path=&quot;stable-diffusion-v1-5/&quot; 
--pretrained_ip_adapter_path=“models/ip-adapter-plus_sd15.bin”
--image_encoder_path=“models/image_encoder/”
--data_json_file=“assets/prompt_image.json”
--data_root_path=“assets/images/train/”
--mixed_precision=“fp16”
--resolution=512
--train_batch_size=8
--dataloader_num_workers=4
--learning_rate=1e-04
--weight_decay=0.01
--output_dir=“out_model” 
--num_train_epochs=300
--save_steps=200

参考readme中的说明，将pytorch.bin转换为ip-adapter.bin。
在推理文件
ip_adapter-plus_demo.py中，
修改原始模型：ip_ckpt = &quot;models/ip-adapter-plus_sd15.bin&quot;


对训练好的模型ip_ckpt = &quot;models/ip-adapter.bin&quot;

运行python3 ip_adapter-plus_demo.py进行推理

上面的过程有什么问题吗，或者问题出在输入图像或提示上？
在我的例子中，通常至少需要多少个数据集对（一个图像和一个提示）才能获得有效的结果？]]></description>
      <guid>https://stackoverflow.com/questions/79373102/the-finetuned-ip-adapter-model-fails-to-achieve-effective-results</guid>
      <pubDate>Tue, 21 Jan 2025 03:08:09 GMT</pubDate>
    </item>
    <item>
      <title>合并两个图像数据集</title>
      <link>https://stackoverflow.com/questions/79372667/combining-two-image-dataset</link>
      <description><![CDATA[我有 6 个图像数据集，一个来自 Kaggle，其他来自 Roboflow。我想使用所有 6 个数据集在 Google Colab 中训练模型。我可以在 Google Colab 中正常使用它们吗？处理和组合这些数据集进行训练的最佳实践是什么？
我还没有尝试任何方法，但我已经搜索过了，仍然没有得到问题的答案]]></description>
      <guid>https://stackoverflow.com/questions/79372667/combining-two-image-dataset</guid>
      <pubDate>Mon, 20 Jan 2025 21:17:58 GMT</pubDate>
    </item>
    <item>
      <title>React Native CLI 项目中的 YOLO 对象检测模型</title>
      <link>https://stackoverflow.com/questions/79372324/yolo-object-detection-model-in-react-native-cli-project</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79372324/yolo-object-detection-model-in-react-native-cli-project</guid>
      <pubDate>Mon, 20 Jan 2025 18:24:39 GMT</pubDate>
    </item>
    <item>
      <title>VSCode 安装 hugginface relik 库时出错</title>
      <link>https://stackoverflow.com/questions/79182549/vscode-install-error-for-the-hugginface-relik-library</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79182549/vscode-install-error-for-the-hugginface-relik-library</guid>
      <pubDate>Tue, 12 Nov 2024 19:53:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么 OvO 和 OvR 返回相同的结果？</title>
      <link>https://stackoverflow.com/questions/70981876/why-does-ovo-and-ovr-return-the-same-result</link>
      <description><![CDATA[我正在使用 scikit-learn 的 roc_auc_score() 函数来解决多类分类问题。对于具有三个标签（和另一个数据集）的鸢尾花，当我使用一对一和一对其余时，我得到完全相同的输出。有人知道为什么会这样吗？这是我的代码：
从 sklearn 导入数据集
从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.metrics 导入 roc_auc_score
从 sklearn.model_selection 导入 train_test_split
从 sklearn.tree 导入 DecisionTreeClassifier

iris = datasets.load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0)
clf = DecisionTreeClassifier(random_state=0)
clf = clf.fit(X_train, y_train)
y_pred = clf.predict_proba(X_test)
auc_ovr = roc_auc_score(y_test, y_pred, average=&#39;macro&#39;, multi_class=&#39;ovr&#39;)
auc_ovo = roc_auc_score(y_test, y_pred, average=&#39;macro&#39;, multi_class=&#39;ovo&#39;)
print(f&#39;OVR: {auc_ovr}, OVO: {auc_ovo}&#39;)


最后一行的输出是：
OVR: 0.9833333333333334, OVO: 0.9833333333333334
]]></description>
      <guid>https://stackoverflow.com/questions/70981876/why-does-ovo-and-ovr-return-the-same-result</guid>
      <pubDate>Fri, 04 Feb 2022 05:35:38 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow keras 序列模型 - 如何仅预测最后一步的输出</title>
      <link>https://stackoverflow.com/questions/53825156/tensorflow-keras-sequence-models-how-to-only-predict-output-of-last-step</link>
      <description><![CDATA[我正在使用 tf.keras 库开发一个序列模型。
假设我有 5 个时间步骤，每个时间步骤本质上都有一个输出。
每个时间步骤中的特征数量为 - 比如 - 4。我正在研究分类问题，输出可以是 0、1 或 2 之一。
例如，以下是一个训练示例。
步骤 1 输入：`[0, 5, 4, 5]` 和输出 = `0`
步骤 2 输入：`[1, 2, 2, 7]` 和输出 = `1`
步骤 3 输入：`[7, 5, 3, 4]` 和输出 = `0`
步骤 4 输入：`[4, 5, 1, 2]` 和输出 = `1`
步骤 5 输入：`[8, 5, 4, 5]` 和输出 = `2`

在训练我的模型时，我希望以这样的方式训练它：
在步骤 1 中，如果您的输入是 [0, 5, 4, 5]，则此时间步的输出为 0。

在步骤 2 中，如果您的输入是 [1, 2, 2, 7]，则此时间步的输出为 1。
以此类推....
但在后期制作中，我只希望我的模型能够估计最后一个时间步的输出。例如：
步骤 1 输入：`[0, 5, 4, 5]` 且输出 = `0`
步骤 2 输入：`[1, 2, 2, 7]` 且输出 = `1`
步骤 3 输入：`[7, 5, 3, 4]` 且输出 = `0`
步骤 4 输入：`[4, 5, 1, 2]` 且输出 = `1`
步骤 5 输入：`[8, 5, 4, 5]` 且输出 = **`?`**

基于此，在训练我的模型时，我对应该如何构建和训练我的模型有点困惑？由于我只对最后一步的输出感兴趣，但仍希望在训练阶段通过提供最后一步之前的先前步骤的输出来帮助我的模型，我想知道我应该如何构建模型？
如果我提供所有时间步的输出作为预期输入，据我所知，损失/成本是基于此计算的。例如，如果第 3 个时间步的输出计算错误，成本将增加。这可能是预料之中的，但对我来说重要的是最后一步的输出，我主要感兴趣的是在最后一步做出正确的预测。在这种情况下，如何使用 tf.keras 构建我的模型？
（或者，如果我仍然需要以某种方式训练我的模型，以便它尝试分别估计每个时间步的输出。最后，我仍然希望仅基于最后一步的输出来计算准确度。）]]></description>
      <guid>https://stackoverflow.com/questions/53825156/tensorflow-keras-sequence-models-how-to-only-predict-output-of-last-step</guid>
      <pubDate>Tue, 18 Dec 2018 01:24:53 GMT</pubDate>
    </item>
    <item>
      <title>KD树最近邻搜索如何工作？</title>
      <link>https://stackoverflow.com/questions/4418450/how-does-the-kd-tree-nearest-neighbor-search-work</link>
      <description><![CDATA[我正在查看 KD 树的 Wikipedia 页面。例如，我用 Python 实现了列出的构建 kd 树的算法。
但是，使用 KD 树进行 KNN 搜索的算法会切换语言，并且并不完全清楚。英语解释开始有意义，但其中的部分（例如他们“展开递归”以检查其他叶节点的区域）对我来说真的没有任何意义。
这是如何工作的，以及如何在 Python 中使用 KD 树进行 KNN 搜索？这并不是一个“给我发代码！”类型的问题，我也不指望这样。请简单解释一下 :)]]></description>
      <guid>https://stackoverflow.com/questions/4418450/how-does-the-kd-tree-nearest-neighbor-search-work</guid>
      <pubDate>Sat, 11 Dec 2010 19:14:25 GMT</pubDate>
    </item>
    </channel>
</rss>