<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 01 Feb 2025 18:20:07 GMT</lastBuildDate>
    <item>
      <title>我如何判断模型是否过度拟合？</title>
      <link>https://stackoverflow.com/questions/79405195/how-can-i-tell-when-the-model-is-overfitting</link>
      <description><![CDATA[我最近开始研究机器学习，我有一个代码用于评估 SNN 和 ANN 对合成数据中的运动事件进行分类的适用性，重点关注它们的速度（推理时间）和准确性。当我绘制损失和准确性曲线时，我确实觉得模型过度拟合了，尽管我检查了聊天 GPT 和 DeepSeek，两者都提到它不是过度拟合，这些结果是由于任务太简单，我可以依靠它们进行这种类型的分析吗？我每次如何才能知道模型是否正确学习？
以下是函数的图表
损失和准确率函数
图形表示
我尝试更改 epoch 的数量，但结果非常相似]]></description>
      <guid>https://stackoverflow.com/questions/79405195/how-can-i-tell-when-the-model-is-overfitting</guid>
      <pubDate>Sat, 01 Feb 2025 15:02:04 GMT</pubDate>
    </item>
    <item>
      <title>1. 分类变量如何影响回归模型？</title>
      <link>https://stackoverflow.com/questions/79404865/1-how-do-categorical-variables-impact-a-regression-model</link>
      <description><![CDATA[在机器学习中，有两种类型的变量，一种是分类变量，另一种是数值变量。当我们对分类变量进行编码时，只有 0 和 1，0 和 1 如何影响模型或对回归模型的影响。它会降低模型性能或提高模型性能等。]]></description>
      <guid>https://stackoverflow.com/questions/79404865/1-how-do-categorical-variables-impact-a-regression-model</guid>
      <pubDate>Sat, 01 Feb 2025 11:09:48 GMT</pubDate>
    </item>
    <item>
      <title>文件的 CRC-32 错误</title>
      <link>https://stackoverflow.com/questions/79404810/bad-crc-32-for-file</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79404810/bad-crc-32-for-file</guid>
      <pubDate>Sat, 01 Feb 2025 10:39:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用图像上传作为 Deepseek-R1 等文本生成模型的输入？</title>
      <link>https://stackoverflow.com/questions/79404729/how-can-i-use-image-uploads-as-input-for-text-generation-models-like-deepseek-r1</link>
      <description><![CDATA[我正在使用文本生成模型 (Deepseek-R1)，并注意到某些平台允许在文本提示的同时上传图片。例如，在之前的一次互动中，我引用了一张外卖容器中烤五花肉的图片，上面有塑料包装，以询问该模型的功能。但是，由于 Deepseek-R1 是基于文本的，这种集成如何工作？
像 Deepseek-R1 这样的文本生成模型可以直接处理图像输入吗？还是有一个中间步骤（例如，图像到文本的转换）？
如果不支持直接图像处理，那么在文本提示中描述视觉内容以指导模型输出的最佳实践是什么？
我希望有处理此类用例的示例或文档参考。
我使用的是 Deepseek-R1，而不是 Deepseek VL]]></description>
      <guid>https://stackoverflow.com/questions/79404729/how-can-i-use-image-uploads-as-input-for-text-generation-models-like-deepseek-r1</guid>
      <pubDate>Sat, 01 Feb 2025 09:33:40 GMT</pubDate>
    </item>
    <item>
      <title>辍学率 AI - ML 课程 [关闭]</title>
      <link>https://stackoverflow.com/questions/79404303/dropout-rate-ai-ml-courses</link>
      <description><![CDATA[有谁知道我可以在哪里获得与人工智能机器学习相关的在线课程的辍学率？
我正在尝试发表一篇关于机器学习领域专门专业人员数量少的原因的文章，我想通过我提出的一个假设来获取有关人工智能课程辍学率及其可能原因的信息。]]></description>
      <guid>https://stackoverflow.com/questions/79404303/dropout-rate-ai-ml-courses</guid>
      <pubDate>Sat, 01 Feb 2025 01:52:01 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：从 Google Drive 加载经过微调的 LLaVA 模型时，“CLIPImageProcessor”对象没有属性“patch_size”</title>
      <link>https://stackoverflow.com/questions/79403742/attributeerror-clipimageprocessor-object-has-no-attribute-patch-size-when-l</link>
      <description><![CDATA[我在 Google Colab 上微调了一个 LLaVA（大型语言和视觉助手）模型，并将其保存到我的 Google Drive 中。以下是我保存模型的方法：
from google.colab import drive 
drive.mount(&#39;/content/drive&#39;, force_remount=True) 
import os 

save_path = &quot;/content/drive/MyDrive/fineTune model1/LLaVA-med-MAKAUT_v1&quot; 
os.makedirs(save_path, exist_ok=True) 

trainer.model.save_pretrained(save_path) 
trainer.tokenizer.save_pretrained(save_path) 
processor.image_processor.save_pretrained(save_path) 

保存后，我的 Google Drive 文件夹包含以下内容文件：

README.md

adapter_model.safetensors

adapter_config.json

tokenizer_config.json

special_tokens_map.json

added_tokens.json

tokenizer.model

tokenizer.json

preprocessor_config.json

config.json


但是，当我尝试加载模型进行测试时，我收到与以下相关的 AttributeError： patch_size:
导入 torch 
从 PIL 导入图像 
从 transformers 导入 LlavaProcessor、LlavaForConditionalGeneration、CLIPImageProcessor 

model_path = &quot;/content/drive/MyDrive/fineTune model/LLaVA-med-MAKAUT_v1&quot; 
process1 = LlavaProcessor.from_pretrained(model_path)

从模型的 vision_config 检查补丁大小
patch_size = new_model_v1.config.vision_config.patch_size 
print(&quot;Patch size:&quot;, patch_size) 

输出：
Patch size: 14 

错误发生在这里：
print(processor1.image_processor.patch_size) 

错误消息：
AttributeError: &#39;CLIPImageProcessor&#39; 对象没有属性 &#39;patch_size&#39;

我尝试过的方法：

确保模型正确保存和加载。

确认模型的视觉配置中存在补丁大小（patch_size：14）。

尝试手动设置 patch_size：
processor1.image_processor.patch_size = 14 



但是，这似乎不是正确的方法，因为 CLIPImageProcessor 没有此属性。
问题：

为什么 CLIPImageProcessor 缺少 patch_size 属性，即使它在模型的 vision_config 中定义？
确保 LLaVA 处理器与微调模型的配置保持一致的正确方法是什么，尤其是关于 patch_size？
是否有推荐的方法来正确加载和利用微调 LLaVA 模型及其处理器以在 Colab 中进行推理？
]]></description>
      <guid>https://stackoverflow.com/questions/79403742/attributeerror-clipimageprocessor-object-has-no-attribute-patch-size-when-l</guid>
      <pubDate>Fri, 31 Jan 2025 18:45:34 GMT</pubDate>
    </item>
    <item>
      <title>从 pix2pix 实现矢量化生成的线条</title>
      <link>https://stackoverflow.com/questions/79402632/vectorize-generated-lines-from-a-pix2pix-implementation</link>
      <description><![CDATA[我有以下使用 pix2pix 模型生成的图像。我希望生成的线条是直的，并且具有相同的高度（如果是水平的）和相同的宽度（如果是垂直的）。
基本上这些线是墙壁，稍后应该被渲染为房子里的墙壁。
有没有任何 python /ml 逻辑可以实现这一点？
]]></description>
      <guid>https://stackoverflow.com/questions/79402632/vectorize-generated-lines-from-a-pix2pix-implementation</guid>
      <pubDate>Fri, 31 Jan 2025 11:47:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyTorch 微调 LLM 时出现运行时错误：“张量的元素 0 不需要梯度”</title>
      <link>https://stackoverflow.com/questions/79402407/runtimeerror-with-pytorch-when-fine-tuning-llm-element-0-of-tensors-does-not-r</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79402407/runtimeerror-with-pytorch-when-fine-tuning-llm-element-0-of-tensors-does-not-r</guid>
      <pubDate>Fri, 31 Jan 2025 10:11:53 GMT</pubDate>
    </item>
    <item>
      <title>训练 Keras 模型来识别闰年</title>
      <link>https://stackoverflow.com/questions/79401755/training-a-keras-model-to-identify-leap-years</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79401755/training-a-keras-model-to-identify-leap-years</guid>
      <pubDate>Fri, 31 Jan 2025 04:14:59 GMT</pubDate>
    </item>
    <item>
      <title>从蒙版风力涡轮机图像中提取叶片尖端坐标的想法[关闭]</title>
      <link>https://stackoverflow.com/questions/79400379/ideas-for-extracting-blade-tip-coordinates-from-masked-wind-turbine-image</link>
      <description><![CDATA[我正在寻找 Python 中的图像处理工具来获取风力涡轮机叶片尖端的坐标，在本例中是小型模型。叶片已经由 yoloV8 分割模型分割，现在我想使用该图像获取尖端的 xy 坐标。示例图像：
风能涡轮机的蒙版机翼。
有人可以推荐一些关于如何做到这一点的想法吗？转子可以旋转，因此三个尖端可以位于椭圆上的任何位置。
我已经尝试训练 yolo-pose 模型进行关键点检测，但它没有给出足够精确的结果。我将使用这些坐标来计算转子盘的偏心率，因此这些点需要相当精确。]]></description>
      <guid>https://stackoverflow.com/questions/79400379/ideas-for-extracting-blade-tip-coordinates-from-masked-wind-turbine-image</guid>
      <pubDate>Thu, 30 Jan 2025 15:27:02 GMT</pubDate>
    </item>
    <item>
      <title>EfficientNetB3模型识别脑肿瘤准确率极低及学习停滞问题</title>
      <link>https://stackoverflow.com/questions/79390644/very-low-accuracy-of-efficientnetb3-model-and-learning-plateau-on-identifying-br</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79390644/very-low-accuracy-of-efficientnetb3-model-and-learning-plateau-on-identifying-br</guid>
      <pubDate>Mon, 27 Jan 2025 11:58:17 GMT</pubDate>
    </item>
    <item>
      <title>ML-Agents 代理无法在 Unity 中完成简单的“射弹到目标”任务</title>
      <link>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</link>
      <description><![CDATA[代理在重力作用下向目标发射弹丸。代理只有一个动作 - 射击角度。发射力是恒定的。我还没有改变目标的位置。因此这应该是微不足道的，因为模型只需要学习正确的射击角度。但经过 300000 个训练步骤后，模型仍然射击不稳定。
代理：
使用 Unity.MLAgents;
使用 Unity.MLAgents.Actuators;
使用 Unity.MLAgents.Sensors;
使用 UnityEngine;

公共类 ProjectileAgent：代理
{
公共 Transform 目标; //带有 2D 碰撞器和“目标”标签的固定目标
公共 Transform launchPoint; //生成弹丸的位置
公共 GameObject projectilePrefab; //带有 Rigidbody2D 和 ProjectileCollision 脚本的预制件
公共 float fixedForce = 500f; // 对射弹施加恒定的力

private bool hasLaunched = false;

public override void OnEpisodeBegin()
{
hasLaunched = false;
RequestDecision(); // 在每个情节开始时请求一个决定
}

public override void CollectObservations(VectorSensor sensor)
{
// 观察从发射点到目标的相对位置 (x,y)
Vector2 diff = target.position - launchPoint.position;
sensor.AddObservation(diff.x);
sensor.AddObservation(diff.y);
}

public override void OnActionReceived(ActionBuffers action)
{
if (!hasLaunched)
{
// 一个连续动作 (0..1) 映射到 [0..180] 度
float angle01 = Mathf.Clamp01(actions.ContinuousActions[0]);
float angleDegrees = Mathf.Lerp(0f, 180f, angle01);

LaunchProjectile(angleDegrees);
hasLaunched = true;
}
}

private void LaunchProjectile(float angleDegrees)
{
GameObject projObj = Instantiate(projectilePrefab, launchPoint.position, Quaternion.identity);
ProjectileCollision projScript = projObj.GetComponent&lt;ProjectileCollision&gt;();
projScript.agent = this;

Rigidbody2D rb = projObj.GetComponent&lt;Rigidbody2D&gt;();
float rad = angleDegrees * Mathf.Deg2Rad;
Vector2 direction = new Vector2(Mathf.Cos(rad), Mathf.Sin(rad));
rb.AddForce(direction * fixedForce);
}

// 射弹击中目标时调用
public void OnHitTarget()
{
AddReward(1.0f);
EndEpisode();
}

// 射弹未击中目标时调用
public void OnMiss(Vector2 projectilePosition)
{
float distance = Vector2.Distance(projectilePosition, target.position);
float maxDistance = 10f; // 根据需要调整
float vicinity = 1f - (distance / maxDistance);
vicinity = Mathf.Clamp01(proximity);

// 接近目标时获得部分奖励
AddReward(proximity * 0.5f);

// 未击中时获得小额惩罚
AddReward(-0.1f);
EndEpisode();
}

// Unity 编辑器中测试的启发式方法（随机角度）
public override void Heuristic(in ActionBuffers actionOut)
{
actionOut.ContinuousActions[0] = Random.value;
}
}

Projectile:
using UnityEngine;

public class ProjectileCollision : MonoBehaviour
{
public ProjectileAgent agent;

private void Start()
{
// 短暂时间后销毁，以便我们可以记录未击中
Destroy(gameObject, lifetime);
}

private void OnCollisionEnter2D(Collision2D collision)
{
if (collision.gameObject.CompareTag(&quot;Target&quot;))
{
agent.OnHitTarget();
}
else
{
agent.OnMiss(transform.position);
}
Destroy(gameObject);
}
}


我尝试过的方法

奖励塑造：
击中目标可获得 +1 奖励，近距离击中可获得部分基于距离的奖励，未击中可获得少量负奖励。
我将击中奖励提高到 +3，降低了未击中惩罚，等等。
训练步骤：
我使用 PPO 运行了 300k+ 步。
碰撞检查：
日志确认 OnHitTarget() 和 OnMiss() 在预期时间触发。
固定力和重力：
通过硬编码角度，验证箭可以手动到达目标。
重力已设置，因此物理上可以击中。
无随机目标：
目标目前固定在一个位置以保持简单。
]]></description>
      <guid>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</guid>
      <pubDate>Mon, 27 Jan 2025 02:47:35 GMT</pubDate>
    </item>
    <item>
      <title>安装 pycoral 会导致“未满足的依赖项”错误，尽管依赖项已得到满足</title>
      <link>https://stackoverflow.com/questions/78593719/installing-pycoral-results-in-unmet-dependencies-error-despite-dependencies-be</link>
      <description><![CDATA[我在 Chromebook 上通过 sudo apt-get install python-pycoral 安装 pycoral 时出现以下错误
以下软件包具有未满足的依赖项：
python3-pycoral：依赖项：python3-tflite-runtime（= 2.5.0.post1），但不会安装
依赖项：python3（&lt; 3.10），但要安装 3.11.2-1+b1
E：无法更正问题，您持有损坏的软件包。

我确认我的设置符合列出的要求，但 pycoral 安装程序似乎忽略了 tflite（出现在 pip list 中）以及当前的 python 版本。
如果您有这方面的经验，我将不胜感激任何解决此问题的帮助。从“未满足的依赖项”错误的怪异性来看，我感觉 tflite 运行时和 python 版本之外的某些东西存在问题]]></description>
      <guid>https://stackoverflow.com/questions/78593719/installing-pycoral-results-in-unmet-dependencies-error-despite-dependencies-be</guid>
      <pubDate>Fri, 07 Jun 2024 19:28:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 AI 和机器学习库编写 Python 程序来预测游戏的下一个结果（从两种颜色中挑选一种）</title>
      <link>https://stackoverflow.com/questions/76086477/how-to-make-a-python-program-to-predict-the-next-outcome-of-a-game-of-picking-a</link>
      <description><![CDATA[我想用 Python 编写一个程序，使用 LSTM 技术，可以预测下一个结果或从两种颜色中挑选一种颜色的概率。该程序应使用 AI 和机器学习库，读取最后 40 个结果的模式，从而预测下一个结果。
我为此编写了以下程序。
from keras.models import Sequential
from keras.layers import LSTM, Dense
import numpy as np

def predict_next_color_lstm(outcomes):
if len(outcomes) &lt; 40:
return &quot;Error: 提供的结果数量少于 40。&quot;

# 将字符串输入转换为整数序列
seq = [0 if x == &#39;r&#39; else 1 for x in results]

# 创建 40 个结果的滚动窗口
X = []
y = []
for i in range(len(seq) - 40):
X.append(seq[i:i + 40])
y.append(seq[i + 40])
X = np.array(X)
y = np.array(y)

# 重塑 X 以适应 LSTM 输入形状
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# 创建 LSTM 模型
model = Sequential()
model.add(LSTM(50, input_shape=(40, 1)))
model.add(Dense(1,activation=&#39;sigmoid&#39;))

# 编译模型
model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;)

# 训练模型
model.fit(X, y, epochs=50, batch_size=32)

# 预测下一个结果
last_40 = seq[-40:]
pred = model.predict(np.array([last_40]))
如果 pred &lt;，则返回“r” 0.5 else &#39;g&#39;

def get_input():
# 要求用户输入长度为 40 的球颜色序列
ball_seq = input(&quot;输入长度为 40 的球颜色序列（例如 rrggrrgrrgggrgrgrgrgrgrgrgrgggrrggggrgggg）：&quot;)
return ball_seq

# _main_
ball_seq = get_input()
print(&quot;预测：&quot;, predict_next_color_lstm(ball_seq))

但是我在执行时不断收到以下错误：
C:\Users\Ashish\miniconda3\python.exe C:\Users\Ashish\Desktop\pyt_pract\test_prob1.py
输入长度为 40 的球颜色序列（例如 rrggrrgrrgggrgrgrrgggggrgrgrgrgrgggrrgggggg）：rgggrrgrggrrgrgrgrgrgrggggrrrrggrrgrgggrgrg
回溯（最近一次调用）：
文件“C:\Users\Ashish\Desktop\pyt_pract\test_prob1.py”，第 50 行，位于 &lt;module&gt;
print(&quot;预测：&quot;，predict_next_color_lstm(ball_seq))
文件“C:\Users\Ashish\Desktop\pyt_pract\test_prob1.py”，第 23 行，位于 predict_next_color_lstm
X = np.reshape(X, (X.shape[0], X.shape[1], 1))
IndexError：元组索引超出范围
]]></description>
      <guid>https://stackoverflow.com/questions/76086477/how-to-make-a-python-program-to-predict-the-next-outcome-of-a-game-of-picking-a</guid>
      <pubDate>Sun, 23 Apr 2023 18:08:18 GMT</pubDate>
    </item>
    <item>
      <title>宏观 VS 微观 VS 加权 VS 样本 F1 分数</title>
      <link>https://stackoverflow.com/questions/55740220/macro-vs-micro-vs-weighted-vs-samples-f1-score</link>
      <description><![CDATA[在 sklearn.metrics.f1_score 中，f1 分数有一个名为“平均值”的参数。宏、微、加权和样本是什么意思？请详细说明，因为在文档中，没有正确解释。或者简单地回答以下问题：

为什么“样本”是多标签分类的最佳参数？
为什么微最适合不平衡的数据集？
加权和宏有什么区别？
]]></description>
      <guid>https://stackoverflow.com/questions/55740220/macro-vs-micro-vs-weighted-vs-samples-f1-score</guid>
      <pubDate>Thu, 18 Apr 2019 06:26:25 GMT</pubDate>
    </item>
    </channel>
</rss>