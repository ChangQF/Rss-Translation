<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 16 Jan 2025 15:17:02 GMT</lastBuildDate>
    <item>
      <title>pytorch：所有样本的前向传递</title>
      <link>https://stackoverflow.com/questions/79361940/pytorch-forward-pass-with-all-samples</link>
      <description><![CDATA[import torch
import torch.nn as nn

class PINN(nn.Module):
def __init__(self, input_dim, output_dim, hidden_​​layers, neurons_per_layer):
super(PINN, self).__init__()
layer = []
layer.append(nn.Linear(input_dim, neurons_per_layer))
for _ in range(hidden_​​layers):
layer.append(nn.Linear(neurons_per_layer, neurons_per_layer))
layer.append(nn.Linear(neurons_per_layer, output_dim))
self.network = nn.Sequential(*layers)

def forward(self, x):
return self.network(x)

# 示例：生成随机输入数据
inputs = torch.rand((1000, 3)) # 3D 输入坐标

model = PINN(input_dim=3, output_dim=3, hidden_​​layers=4, neurons_per_layer=64)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 10000
for epoch in range(epochs):
optimizer.zero_grad() 
nn_output = model(inputs) # 计算 NN 预测
# 计算 nn_output 的梯度
loss.backward() 
optimizer.step() 

我想实现一个物理信息 NN，其中输入是 N 个 3d 点 (x,y,z)，NN 输出是此时的矢量值量，即输入维度和输出维度都相同。
要计算每个时期的损失，我需要有该量的值在所有点。示例：对于 N=1000 点，我需要所有 1000 个 NN 预测，然后才能继续进行损失计算。
在我的代码中，我基本上将一个 1000x3 对象提供给输入层，假设 pytorch 将每一行（1x3）分别传递给网络，最后将其再次组织为 1000x3 对象。
pytorch 是否像那样工作，还是我必须重新考虑这种方法？]]></description>
      <guid>https://stackoverflow.com/questions/79361940/pytorch-forward-pass-with-all-samples</guid>
      <pubDate>Thu, 16 Jan 2025 14:23:21 GMT</pubDate>
    </item>
    <item>
      <title>如何测试这些在 CICIDS2017 数据上训练的模型是否能够检测到来自 suricata 日志的攻击？[关闭]</title>
      <link>https://stackoverflow.com/questions/79361293/how-to-test-these-models-trained-on-cicids2017-data-will-they-be-able-to-detect</link>
      <description><![CDATA[我如何开发一个有效的自适应入侵检测系统，利用机器学习技术根据动态网络流量分析自动修改现有的 Suricata 规则，同时最大限度地减少误报并确保这些自动生成的规则的安全实施？
具体来说，我正在寻找以下方面的指导：
鉴于我当前的功能集导致模型即使在模拟攻击期间也将所有流量归类为良性，应该从网络流量数据和 Suricata 日志中选择或设计哪些功能来训练能够实时准确检测已知和新威胁的机器学习模型？
考虑到需要从 CICIDS2017 等初始数据集中学习，同时不断适应我自己网络不断变化的流量模式和新出现的威胁，哪些机器学习架构和训练方法最适合这种自适应 IDS 用例？
我如何设计一个集成管道，自动从 Suricata 中提取训练数据，验证机器学习模型输出，从高置信度检测中生成优化的 Suricata 规则，并结合分析师反馈，随着时间的推移迭代提高系统的准确性和稳健性？
最终目标是入侵检测系统可以动态适应不断变化的网络条件并提供可靠、可操作的安全警报，而不会让分析师承受过多的误报或引入可能破坏合法流量的错误配置规则。我很感激任何关于成功实施这种自适应、ML 驱动的 IDS 所需的特征工程、建模技术、系统架构和自动化机制的见解或建议。目前我使用此功能，但即使我创建 DDoS 攻击或 suricate 创建警报，模型也会将所有日志视为 BENIGN。模型在 CICIDS2017 数据上进行训练。
selected_features = [
# 基于流的特征
&#39;流持续时间&#39;, &#39;流字节/秒&#39;, &#39;流数据包/秒&#39;,
&#39;转发数据包总长度&#39;, &#39;转发数据包总长度&#39;,

# 时间特征
&#39;流 IAT 平均值&#39;, &#39;流 IAT 标准&#39;, &#39;流 IAT 最大值&#39;, &#39;流 IAT 最小值&#39;,
&#39;转发 IAT 总数&#39;, &#39;转发 IAT 总数&#39;,

# 数据包特征
&#39;转发数据包长度最大值&#39;, &#39;转发数据包长度最小值&#39;,
&#39;转发数据包长度最大值&#39;, &#39;转发数据包长度最小值&#39;,
&#39;数据包长度平均值&#39;, &#39;数据包长度标准&#39;, &#39;数据包长度方差&#39;,

# TCP标志
&#39;SYN 标志计数&#39;, &#39;FIN 标志计数&#39;, &#39;RST 标志计数&#39;,
&#39;PSH 标志计数&#39;, &#39;ACK 标志计数&#39;, &#39;URG 标志计数&#39;,

# 附加功能
&#39;总转发数据包&#39;, &#39;总反向数据包&#39;,
&#39;转发报头长度&#39;, &#39;反向报头长度&#39;,
&#39;活动平均值&#39;, &#39;活动标准&#39;, &#39;空闲平均值&#39;,
&#39;Init_Win_bytes_forward&#39;, &#39;Init_Win_bytes_backward&#39;
]
]]></description>
      <guid>https://stackoverflow.com/questions/79361293/how-to-test-these-models-trained-on-cicids2017-data-will-they-be-able-to-detect</guid>
      <pubDate>Thu, 16 Jan 2025 10:57:37 GMT</pubDate>
    </item>
    <item>
      <title>通过手动汇总值来重现 LGBMRegressor 预测</title>
      <link>https://stackoverflow.com/questions/79361226/reproduce-lgbmregressor-predictions-by-manually-aggregate-the-values</link>
      <description><![CDATA[我正在尝试自己重现 LGBMRegressor 预测，因此当我成功时，我会将平均值转换为中位数。但目前看来我做不到。
这是我为检查是否可以重现结果而创建的简单脚本。
我需要 reg_y_hat 与 self_y_hat 相同。
我遗漏了什么？如果我知道训练中的哪些样本落到每个叶子上，我就可以自己汇总预测...
 import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split

# 生成一些随机回归数据
np.random.seed(42)
X = np.random.rand(100, 5)
y = 4 * X[:, 0] - 2 * X[:, 1] + np.random.rand(100) * 0.1

# 分割数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 LGBMRegressor
model = lgb.LGBMRegressor(objective=&#39;regression&#39;, n_estimators=2, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)

# 常规预测：
reg_y_hat = model.predict(X_test)

# 获取初始预测（y_train 的平均值）
init_pred = np.mean(y_train)

# 获取训练叶子值
train_leaf_indices = model.predict(X_train, pred_leaf=True)
leaf_samples = {(i, leaf_id): [] for i in range(model.n_estimators) for leaf_id in np.unique(train_leaf_indices[:, i])}

# 存储每个叶子的相应目标值
for i, row in enumerate(train_leaf_indices):
for j, leaf_id in enumerate(row):
leaf_samples[(j, leaf_id)].append(y_train[i])

#计算每个叶子的平均值：
leaf_agg = {}
for key, values in leaf_samples.items():
leaf_agg[key] = np.mean(values)

# 通过聚合平均值并添加初始预测进行预测：
preds = []
test_leaf_indices = model.predict(X_test, pred_leaf=True)
for row_indices in test_leaf_indices:
row_pred = init_pred
for i, leaf_index in enumerate(row_indices):
row_pred += model.learning_rate * (leaf_agg[(i, leaf_index)] - init_pred) # 仅初始预测后叶子的残差贡献
preds.append(row_pred)
self_y_hat = np.array(preds)

# 验证结果
print(&#39;reg_y_hat 之间的差异和 self_y_hat:&#39;, np.abs(reg_y_hat - self_y_hat).sum())
]]></description>
      <guid>https://stackoverflow.com/questions/79361226/reproduce-lgbmregressor-predictions-by-manually-aggregate-the-values</guid>
      <pubDate>Thu, 16 Jan 2025 10:39:14 GMT</pubDate>
    </item>
    <item>
      <title>代码无法在 AWS sagemaker 上成功运行</title>
      <link>https://stackoverflow.com/questions/79361126/code-not-running-sucessfully-on-aws-sagemaker</link>
      <description><![CDATA[当我运行应该创建训练作业的单元时，我没有在 sagemaker 笔记本上收到任何更新？我在 AWS Sagemaker 上运行以下代码，当我运行此代码时，它显示训练作业已创建并且没有进一步的更新。当我检查训练作业时，我可以看到状态已完成，但代码仍在笔记本上运行并且没有显示任何更新。有人可以帮我为什么会这样吗？
kmeans.fit(kmeans.record_set(scaled_data)) 
]]></description>
      <guid>https://stackoverflow.com/questions/79361126/code-not-running-sucessfully-on-aws-sagemaker</guid>
      <pubDate>Thu, 16 Jan 2025 10:15:35 GMT</pubDate>
    </item>
    <item>
      <title>有效覆盖 pytorch 数据集</title>
      <link>https://stackoverflow.com/questions/79360229/override-pytorch-dataset-efficiently</link>
      <description><![CDATA[我想继承 torch.utils.data.Dataset 类来加载我的自定义图像数据集，比如说用于分类任务。这是官方 pytorch 网站的示例，位于此 链接:
import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
self.img_labels = pd.read_csv(annotations_file)
self.img_dir = img_dir
self.transform = transform
self.target_transform = target_transform

def __len__(self):
return len(self.img_labels)

def __getitem__(self, idx):
img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
image = read_image(img_path)
label = self.img_labels.iloc[idx, 1]
if self.transform:
image = self.transform(image)
if self.target_transform:
label = self.target_transform(label)
return image, label

我注意到：

在 __getitem__ 中，我们将图像从磁盘读取到内存中。这意味着如果我们训练模型几个时期，我们会多次将同一幅图像重新读入内存。据我所知，这是一个代价高昂的操作
每次从磁盘读取图像时都会应用一次变换，在我看来，这几乎是一个多余的操作。

我理解，在非常大的数据集中，我们无法将数据完全放入内存中，因此我们别无选择，只能以这种方式读取数据（因为我们必须在一个时期内迭代所有数据），我想知道，如果我的所有数据都可以放入内存中，那么在 __init__ 函数中从磁盘读取所有数据不是更好的方法吗？
通过我在计算机视觉方面的一点经验，我注意到在 变换 中将图像裁剪成固定大小的图像非常常见。那么为什么我们不应该裁剪图像一次并将其存储在磁盘上的其他地方，而在整个训练过程中只读取裁剪后的图像呢？在我看来，这似乎是一种更有效的方法。
我理解，一些用于增强而不是规范化的转换最好应用于 __getitem__ 中，以便获得随机生成的数据而不是固定的数据。
你能为我澄清一下这个主题吗？
如果我缺少的是常识，请用正确的方法指导我找到代码库。]]></description>
      <guid>https://stackoverflow.com/questions/79360229/override-pytorch-dataset-efficiently</guid>
      <pubDate>Thu, 16 Jan 2025 02:38:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 yolov5 创建的模型执行脚本时出现图像大小调整问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/79360063/image-resizing-issue-while-executing-script-with-a-model-created-using-yolov5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79360063/image-resizing-issue-while-executing-script-with-a-model-created-using-yolov5</guid>
      <pubDate>Thu, 16 Jan 2025 00:27:41 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-learn 中 .fit() 方法的用例？</title>
      <link>https://stackoverflow.com/questions/79360038/use-cases-for-the-fit-method-in-scikit-learn</link>
      <description><![CDATA[是否存在 .fit() 比使用 .fit_transform() 更实用的情况/用例？例如，当标签编码时：
encoder = LabelEncoder()
title = data[&#39;title&#39;]
encoder.fit(title)
title_encoded =coder.transform(title)

vs
encoder = LabelEncoder()
title = data[&#39;title&#39;]
title_encoded =coder.fit_transform(title)
]]></description>
      <guid>https://stackoverflow.com/questions/79360038/use-cases-for-the-fit-method-in-scikit-learn</guid>
      <pubDate>Thu, 16 Jan 2025 00:04:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 OpenCV 库训练 ML 模型时出现问题</title>
      <link>https://stackoverflow.com/questions/79359797/problems-training-the-ml-model-using-the-opencv-library</link>
      <description><![CDATA[我想创建一个 ML 模型，用于识别 MNIST 数据集中的手写数字。ML 模型是使用 OpenCV 库用 C++ 编写的。在构建模型并训练数据后，我在输出层中得到的数据总是 Nan 或 Inf。
我尝试更改训练参数，但没有任何效果。我尝试处理一个较小的 100 个单位的数据集，在该数据集中，我设法在输出层中获取一些值，但其中大多数仍然是 Nan。我就是找不到原因。以下是 ML 模型的代码：
cv::Ptr&lt;cv::ml::ANN_MLP&gt; mlp = cv::ml::ANN_MLP::create();
mlp-&gt;setActivationFunction(cv::ml::ANN_MLP::SIGMOID_SYM, 1, 1);

int inputLayerSize = imagesData[0].total();
if (inputLayerSize &gt; std::numeric_limits&lt;int&gt;::max()) {
throw std::overflow_error(&quot;inputLayerSize 超出 int 的最大值&quot;);
}
size_t hiddenLayerSize = 100;
size_t outputLayerSize = 10;

cv::Mat layer = (cv::Mat_&lt;int&gt;(3, 1)&lt;&lt;inputLayerSize, hiddenLayerSize, outputLayerSize);

mlp-&gt;setLayerSizes(layers);

int numSamples = imagesData.size();

cv::Mat trainingData(numSamples, inputLayerSize, CV_32F);
cv::Mat labelData(numSamples, outputLayerSize, CV_32F);

for (int i = 0; i &lt; numSamples; i++) {

cv::Mat image = imagesData[i].reshape(1, 1);
image.convertTo(trainingData.row(i), CV_32F);

cv::Mat label = cv::Mat::zeros(1, outputLayerSize, CV_32F);
la​​bel.at&lt;float&gt;(0, labelsData[i]) = 1.0;
label.copyTo(labelData.row(i));

}

cv::TermCriteria termCrit(cv::TermCriteria::MAX_ITER + cv::TermCriteria::EPS, 10, 0.001);
mlp-&gt;setTermCriteria(termCrit);

mlp-&gt;setTrainMethod(cv::ml::ANN_MLP::BACKPROP, 0.001, 0.1);

mlp-&gt;train(trainingData, cv::ml::ROW_SAMPLE, labelData);

我还对像素数据进行了标准化。]]></description>
      <guid>https://stackoverflow.com/questions/79359797/problems-training-the-ml-model-using-the-opencv-library</guid>
      <pubDate>Wed, 15 Jan 2025 21:50:41 GMT</pubDate>
    </item>
    <item>
      <title>F1-score、IOU 和 Dice Score 的实现</title>
      <link>https://stackoverflow.com/questions/79359767/implementation-of-f1-score-iou-and-dice-score</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79359767/implementation-of-f1-score-iou-and-dice-score</guid>
      <pubDate>Wed, 15 Jan 2025 21:33:49 GMT</pubDate>
    </item>
    <item>
      <title>使用 GCP 服务进行图像分类[关闭]</title>
      <link>https://stackoverflow.com/questions/79356912/use-gcp-service-for-image-classification</link>
      <description><![CDATA[我的任务：我将图像作为来自物联网设备的输入，将其发送到云端，进行图像分类，并将结果发送回某个 URL。
我做了什么：我尝试了 TF 服务，docker 镜像与我的本地模型，并在我的设备上进行分类。
有没有更好的方法来实现它。]]></description>
      <guid>https://stackoverflow.com/questions/79356912/use-gcp-service-for-image-classification</guid>
      <pubDate>Wed, 15 Jan 2025 02:26:56 GMT</pubDate>
    </item>
    <item>
      <title>损失函数是否返回正确的值？</title>
      <link>https://stackoverflow.com/questions/79336024/is-the-loss-function-returns-the-correct-value</link>
      <description><![CDATA[我正在研究医学数据集的语义分割问题。我使用专注于正类像素的软骰子损失，具体实现如下：https://arxiv.org/abs/2209.06078 。这是我的代码：
def soft_dice_loss(y_true, y_pred):
epsilon = 1e-8 # 添加小 epsilon 以避免除以零

# 计算分子和分母 
numerator_dice_coef= 2 * tf.reduce_sum(y_true * y_pred, axis=(2, 3)) + epsilon

den_dice_coef = (tf.reduce_sum(y_true * y_true, axis=(2, 3))) + (tf.reduce_sum(y_pred * y_pred, axis=(2, 3))) + epsilon

# 批次中每幅图像的骰子系数
dice_coef = numerator_dice_coef / den_dice_coef

# 批次中的平均骰子系数
#mean_dice_coef = tf.reduce_mean(dice_coef)

# 每个样本所有通道的平均 Dice 系数
mean_dice_coef_per_sample = tf.reduce_mean(dice_coef, axis=1) # 形状：[B]

return 1 - mean_dice_coef_per_sample

我的问题是，我返回的值是否正确？我之所以问这个问题，是因为我很困惑，我是否应该将整个批次的平均损失值返回给 Keras，还是只返回一个大小为 batch_size 的数组，其中每个元素都是每个样本的损失值？我很困惑，因为我看到一些实现只返回一个值，而另一些实现则返回一个数组。]]></description>
      <guid>https://stackoverflow.com/questions/79336024/is-the-loss-function-returns-the-correct-value</guid>
      <pubDate>Tue, 07 Jan 2025 12:37:35 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 nltk 函数</title>
      <link>https://stackoverflow.com/questions/78862426/unable-to-use-nltk-functions</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78862426/unable-to-use-nltk-functions</guid>
      <pubDate>Mon, 12 Aug 2024 15:17:29 GMT</pubDate>
    </item>
    <item>
      <title>在 lightgbm 或 XGBoost 中校准概率</title>
      <link>https://stackoverflow.com/questions/60772387/calibrating-probabilities-in-lightgbm-or-xgboost</link>
      <description><![CDATA[我需要帮助校准 lightgbm 中的概率
下面是我的代码
cv_results = lgb.cv(params, 
lgtrain, 
nfold=10,
stratified=False ,
num_boost_round = num_rounds,
verbose_eval=10,
early_stopping_rounds = 50, 
seed = 50)

best_nrounds = cv_results.shape[0] - 1

lgb_clf = lgb.train(params, 
lgtrain, 
num_boost_round=10000 ,
valid_sets=[lgtrain,lgvalid],
early_stopping_rounds=50,
verbose_eval=10)

ypred = lgb_clf.predict(test, num_iteration=lgb_clf.best_iteration)
]]></description>
      <guid>https://stackoverflow.com/questions/60772387/calibrating-probabilities-in-lightgbm-or-xgboost</guid>
      <pubDate>Fri, 20 Mar 2020 10:29:00 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：分类指标无法处理多类别和多标签指标目标的混合</title>
      <link>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</link>
      <description><![CDATA[我有一个多类标记文本分类问题，有 2000 个不同的标签。使用带有 Glove Embedding 的 LSTM 进行分类。

目标变量的标签编码器
带有 Embedd 层的 LSTM 层
错误度量是 F2 分数

LabelEncoded 目标变量：
le = LabelEncoder() 
le.fit(y)
train_y = le.transform(y_train)
test_y = le.transform(y_test)

带有 Glove Embeddings 的 LSTM 网络如下所示
np.random.seed(seed)
K.clear_session()
model = Sequential()
model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1],
weights=[embedding_matrix]))#,trainable=False
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(num_classes,activation=&#39;softmax&#39;))
model.compile(optimizer=&#39;rmsprop&#39;,loss=&#39;sparse_categorical_crossentropy&#39;)
print(model.summary())

我的错误指标是 F1 分数。我为错误度量构建了以下函数
class Metrics(Callback):
def on_train_begin(self, logs={}):
self.val_f1s = []
self.val_recalls = []
self.val_precisions = []

def on_epoch_end(self, epoch, logs={}):
val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()
val_targ = self.validation_data[1]
_val_f1 = f1_score(val_targ, val_predict)
_val_recall = recall_score(val_targ, val_predict)
_val_precision = precision_score(val_targ, val_predict)
self.val_f1s.append(_val_f1)
self.val_recalls.append(_val_recall)
self.val_precisions.append(_val_precision)
print(&quot;— val_f1: %f — val_precision: %f — val_recall %f&quot; % (_val_f1, _val_precision, _val_recall))
return

metrics = Metrics()

##模型拟合是
model.fit(X_train, train_y, validation_data=(X_test, test_y),epochs=10, batch_size=64, callbacks=[metrics])

第一个 epoch 后出现以下错误：
ValueError：分类指标无法处理多类和连续多输出目标的混合

在哪里我的代码有错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</guid>
      <pubDate>Fri, 07 Jun 2019 14:55:37 GMT</pubDate>
    </item>
    <item>
      <title>sklearn 中的 fit 方法</title>
      <link>https://stackoverflow.com/questions/34727919/fit-method-in-sklearn</link>
      <description><![CDATA[我问了自己关于 sklearn 中的 fit 方法的各种问题。
问题 1：当我这样做时：
from sklearn.decomposition import TruncatedSVD
model = TruncatedSVD()
svd_1 = model.fit(X1)
svd_2 = model.fit(X2)

变量 model 的内容在此过程中是否发生了任何变化？
问题 2：当我这样做时：
from sklearn.decomposition import TruncatedSVD
model = TruncatedSVD()
svd_1 = model.fit(X1)
svd_2 = svd_1.fit(X2)

svd_1 发生了什么？换句话说，svd_1 已经被拟合了，我再次拟合它，那么它的组件发生了什么？]]></description>
      <guid>https://stackoverflow.com/questions/34727919/fit-method-in-sklearn</guid>
      <pubDate>Mon, 11 Jan 2016 17:49:15 GMT</pubDate>
    </item>
    </channel>
</rss>