<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 20 Nov 2024 21:16:10 GMT</lastBuildDate>
    <item>
      <title>如何构建更高效的 DataLoader 来加载大型图像数据集？</title>
      <link>https://stackoverflow.com/questions/79208825/how-to-build-a-more-efficient-dataloader-to-load-large-image-datasets</link>
      <description><![CDATA[亲爱的有经验的朋友，我正在尝试在一个非常大的图像数据集上训练深度学习模型。模型输入需要一对图像（A 和 B）。由于我的图像尺寸很大，我已将它们中的每一个调整为形状为 (3x224x224) 的 torch.Tensor，并将每对作为单独的文件存储在我的磁盘上。相同的对共享相同的索引。
但是，当使用数据集和 DataLoader 将这些文件加载​​到内存中时，我遇到了以下问题：

CPU 内存问题：将工作器数量设置为 12 时，200GB 内存很快就会耗尽。我尝试设置 prefetch_factor=1，但没有帮助。
初始化缓慢：在训练开始之前，每个 epoch 之前都需要很长时间进行初始化。我在之前的帖子中看到，这可能是由于初始化的开销造成的。我设置了 persistent_workers=True，但也没有帮助。
GPU 和批次大小：我使用 4 个 GPU 进行 DDP 训练，当前批次大小为 1024。

请问您对如何提高数据集或 DataLoader 的效率有何建议？如有任何建议，我将不胜感激。非常感谢！

normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], 
std=[0.229, 0.224, 0.225])

augmentation = transforms.Compose([
transforms.RandomApply([transforms.ColorJitter(0.2, 0.2, 0.2, 0.05)], p=0.8),
transforms.RandomApply([transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))], p=0.5),
transforms.RandomGrayscale(p=0.1),
transforms.RandomVerticalFlip(p=0.5),
normalize,
])

class ImagePairDataset(Dataset):
def __init__(self, data_save_folder, dataset_name, num_samples, transform=None):
&quot;&quot;&quot;
Args:
data_save_folder (str)：包含数据文件的文件夹路径。
dataset_name (str)：表示数据集拆分的“train”、“val”或“test”之一。
num_samples (int)：数据集拆分中的样本数。（train：3000000，val：10000，test：10000）
transform (可调用，可选)：应用于图像张量的可选转换。
&quot;&quot;&quot;
self.data_save_folder = data_save_folder
self.dataset_name = dataset_name
self.num_samples = num_samples
self.transform = transform

def __len__(self):
return self.num_samples

def __getitem__(self, idx):

# 根据 idx 构建文件路径
A_image_path = f&quot;{self.data_save_folder}/{self.dataset_name}_A_images_{idx}.pt&quot;
B_image_path = f&quot;{self.data_save_folder}/{self.dataset_name}_B_images_{idx}.pt&quot;
label_path = f&quot;{self.data_save_folder}/{self.dataset_name}_labels_{idx}.pt&quot;

# 从文件路径加载张量
A_image = torch.load(A_image_path)
B_image = torch.load(B_image_path)
label = torch.load(label_path)

# 如果可用，则应用转换
if self.transform:
A_image = self.transform(A_image)
B_image = self.transform(B_image)

return A_image, B_image, label

class ImagePairDataModule(pl.LightningDataModule):

def __init__(self, data_save_folder, train_samples, val_samples, test_samples, batch_size=32, num_workers=4):
super().__init__()
self.data_save_folder = data_save_folder
self.train_samples = train_samples
self.val_samples = val_samples
self.test_samples = test_samples
self.batch_size = batch_size
self.num_workers = num_workers
self.train_transform = augmentation
self.eval_transform = normalize # 仅对验证和测试进行标准化

def setup(self, stage=None):

self.train_dataset = ImagePairDataset(self.data_save_folder, &#39;train&#39;, self.train_samples, transform=self.train_transform)
self.val_dataset = ImagePairDataset(self.data_save_folder, &#39;val&#39;, self.val_samples, transform=self.eval_transform)
self.test_dataset = ImagePairDataset(self.data_save_folder, &#39;test&#39;, self.test_samples, transform=self.eval_transform)

def train_dataloader(self): #prefetch_factor=1, , persistent_workers=True
return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers) 

def val_dataloader(self):
return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)

# 初始化 DataModule
data_module = ImagePairDataModule(
data_save_folder=args.data_save_folder,
train_samples=train_samples,
val_samples=val_samples,
test_samples=test_samples,
batch_size=args.batch_size,
num_workers=12,
)
]]></description>
      <guid>https://stackoverflow.com/questions/79208825/how-to-build-a-more-efficient-dataloader-to-load-large-image-datasets</guid>
      <pubDate>Wed, 20 Nov 2024 20:12:35 GMT</pubDate>
    </item>
    <item>
      <title>为什么 gamma=0 的二元焦点交叉熵总是会产生 nan 损失？</title>
      <link>https://stackoverflow.com/questions/79207979/why-does-binary-focal-cross-entropy-with-gamma-0-always-make-nan-loss</link>
      <description><![CDATA[我正在训练一个 U-Net 来对我们的实验图像进行二值化。但前景通常没有得到很好的体现，换句话说，我有类别不平衡。我一直在使用 BinaryCrossEntropy 作为损失函数。因此，我理解一种简单的方法是定义一个自定义的损失函数，为每个类别赋予权重。但我在这样做时遇到了一些问题，所以放弃了这个尝试（见最后）。我认为使用 BinaryFocalCrossEntropy 更简单，它的表达式为（如果我理解得好的话）

因此，我的计划是将其与 gamma=0 一起使用，这样我就可以通过调整 alpha 值来赋予类别权重。但是，我不断得到 nan 损失。它发生在几个批次之后的第一个时期内：（这里我使用 \alpha = 0.75）

这里我使用了 adam 优化器和 Learning_rate 1e-3。我注意到，如果我改用 1e-4，即使 nan 仍然出现，它也会再完成几个批次后出现。您能帮我找出这是怎么回事吗？我该如何解决这个问题？
附加问题（自定义函数的问题）
对于加权交叉熵，我编写了一个函数，如下所示：
def weighted_binary_crossentropy(y_true, y_pred):
# 计算标准二元交叉熵
bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)

# 创建一个权重图，其中 y_true 用于将正确的权重应用于每个像素
weights = y_true * weight_for_white + (1 - y_true) * weight_for_black

# 将二元交叉熵乘以权重
weighted_bce = weights * bce

# 取平均值得到最终的加权损失
return tf.reduce_mean(weighted_bce)

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=weighted_binary_crossentropy, metrics=[&#39;accuracy&#39;]) 

# 进行训练
results = model.fit(X_train, Y_train, validation_data=(X_validation,Y_validation), batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks)

但我在实现它时遇到了问题，所以半途而废了。如果您对此有任何评论，也会很有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/79207979/why-does-binary-focal-cross-entropy-with-gamma-0-always-make-nan-loss</guid>
      <pubDate>Wed, 20 Nov 2024 15:49:20 GMT</pubDate>
    </item>
    <item>
      <title>真实情况和预测标签不匹配[关闭]</title>
      <link>https://stackoverflow.com/questions/79207893/ground-truh-and-prediected-label-not-match</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79207893/ground-truh-and-prediected-label-not-match</guid>
      <pubDate>Wed, 20 Nov 2024 15:29:38 GMT</pubDate>
    </item>
    <item>
      <title>使用不同的损失来训练不同阶段的模型</title>
      <link>https://stackoverflow.com/questions/79205991/training-different-stage-of-model-with-different-loss</link>
      <description><![CDATA[我正在尝试以端到端的方式训练一个两阶段模型。但是，我想用不同的损失更新模型的不同阶段。例如，假设端到端模型由两个模型组成：model1 和 model2。输出是通过运行计算的
features = model1(inputs)
output = model2(features)

我想用 loss1 更新 model1 的参数，同时保持 model2 的参数不变。接下来，我想用 loss2 更新 model2 的参数，同时保持 model1 的参数不变。我的完整实现如下：
import torch
import torch.nn as nn

# 定义第一个模型
class Net(nn.Module):
def __init__(self):
super(Net, self).__init__()
self.conv1 = nn.Linear(20, 10)
self.conv2 = nn.Linear(10, 5)

def forward(self, x):
x = self.conv1(x)
x = self.conv2(x)
return x

# 定义第二个模型
class Net1(nn.Module):
def __init__(self):
super(Net1, self).__init__()
self.conv1 = nn.Linear(5, 1)

def forward(self, x):
x = self.conv1(x)
return x

# 初始化模型
model1 = Net()
model2 = Net1()

# 初始化单独的每个模型的优化器
optimizer = torch.optim.SGD(model1.parameters(), lr=0.1)
optimizer1 = torch.optim.SGD(model2.parameters(), lr=0.1)

optimizer.zero_grad() 
optimizer1.zero_grad()

criterion = nn.CrossEntropyLoss()

# 样本输入和标签
inputs = torch.randn(2, 20)
labels = torch.randn(2,1)

features = model1(inputs) 
outputs_model = model2(features) 

loss1 = criterion(outputs_model[0], labels[0]) 
loss2 = criterion(outputs_model, labels) 

loss1.backward(retain_graph=True) 
optimizer.step() 
optimizer.zero_grad()
optimizer1.zero_grad() 

loss2.backward() 

但是，这将返回
回溯（最近一次调用最后一次）：
文件，第 55 行，在 &lt;module&gt;
loss2.backward() 
^^^^^^^^^^^^^^^^^
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_tensor.py&quot;, 第 521 行, 在反向传播中
torch.autograd.backward(
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py&quot;, 第 289 行, 在反向传播中
_engine_run_backward(
文件 &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py&quot;, 第 769 行, 在 _engine_run_backward 中
return Variable._execution_engine.run_backward( # 调用 C++ 引擎运行反向传播
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError：梯度计算所需的变量之一已被就地操作修改：[torch.FloatTensor [10, 5]]（AsStridedBackward0 的输出 0）处于版本 2；预期为版本 1。提示：启用异常检测以查找无法计算梯度的操作，使用 torch.autograd.set_detect_anomaly(True)。

我有点明白为什么会发生这种情况，但有办法解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79205991/training-different-stage-of-model-with-different-loss</guid>
      <pubDate>Wed, 20 Nov 2024 06:10:33 GMT</pubDate>
    </item>
    <item>
      <title>识别基于树的方法中的过度拟合特征[关闭]</title>
      <link>https://stackoverflow.com/questions/79205796/identifying-overfitting-features-in-tree-based-methods</link>
      <description><![CDATA[我有一份表格数据，我尝试过的所有基于树的方法（随机森林、XGBoost、Catboost、LightGBM 等）都过度拟合了。我尝试过更改 `ccp_alpha` 等参数，这会降低我的训练和验证指标。我将忽略过度拟合的程度，因为它与我的实际问题无关。
我正在尝试检查我的某些特征是否会导致过度拟合。以下是我对如何进行此操作的想法。
计算训练和验证数据上的特征重要性，现在如果我看到某些高重要性特征的特征重要性急剧下降，我会得出结论，是该特征导致了我的痛苦。我的方法有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79205796/identifying-overfitting-features-in-tree-based-methods</guid>
      <pubDate>Wed, 20 Nov 2024 04:21:51 GMT</pubDate>
    </item>
    <item>
      <title>对来自不同研究的汇总数据进行建模，并确定每项研究的变量重要性</title>
      <link>https://stackoverflow.com/questions/79205749/modelling-aggregated-data-from-different-studies-and-determining-variable-import</link>
      <description><![CDATA[我有来自四项独立研究的数据。每项研究都考察了不同药物对治疗反应的影响。所有四项研究都有相同的基线变量和结果变量（治疗成功）。除了使用基线变量对治疗成功进行分类之外，我还想确定哪些基线变量对于对这四种药物的成功进行分类最为重要。我计划使用 tidymodels 来做到这一点。即使用一组由不同算法（xgboost、随机森林、svm）组成的工作流，确定最佳工作流并检查变量重要性。
我不确定如何最好地比较这四种药物的变量重要性。最好是整理所有数据并运行一个具有每种药物和每个基线变量之间相互作用效应（step_interact）的模型，还是运行四个单独的模型（每种药物一个）并比较四个模型中变量的重要性，还是做其他事情？我不确定第一种方法是否能让我分离出每种药物的特定变量的重要性。
如何最好地解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79205749/modelling-aggregated-data-from-different-studies-and-determining-variable-import</guid>
      <pubDate>Wed, 20 Nov 2024 03:53:42 GMT</pubDate>
    </item>
    <item>
      <title>在 sklearn 中，模型训练完成后，可以将数据集上的“transform”替换为“predict”吗？</title>
      <link>https://stackoverflow.com/questions/79205630/in-sklearn-can-you-replace-transform-with-predict-on-the-dataset-after-the</link>
      <description><![CDATA[我理解 sklearn 中的 transform 和 predict 之间存在差异，但是由于两者都用于新数据集，如果我的目标只是通过 umap 和 clustering 等模型获得预测结果，我可以在模型训练后用 predict 替换使用 transform 来获得相同的结果集吗？

一些参考：
sklearn 的 transform() 和 predict() 方法有什么区别？
transform、fit_transform、predict 和 fit 之间的区别]]></description>
      <guid>https://stackoverflow.com/questions/79205630/in-sklearn-can-you-replace-transform-with-predict-on-the-dataset-after-the</guid>
      <pubDate>Wed, 20 Nov 2024 02:31:28 GMT</pubDate>
    </item>
    <item>
      <title>独热编码特征不匹配问题</title>
      <link>https://stackoverflow.com/questions/79205343/one-hot-encoding-feature-mismatch-issue</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79205343/one-hot-encoding-feature-mismatch-issue</guid>
      <pubDate>Tue, 19 Nov 2024 23:21:56 GMT</pubDate>
    </item>
    <item>
      <title>何时使用深度学习进行机器人和欺诈检测，何时不使用它？[关闭]</title>
      <link>https://stackoverflow.com/questions/79205257/when-to-use-deep-learning-for-bot-and-fraud-detection-and-when-not-to-use-it</link>
      <description><![CDATA[我的数据集大约有 10 万行，是表格数据。
我的问题是：这里有使用深度学习的案例吗？
我目前的方法如下：1) 对历史数据进行监督训练，然后 2) 运行无监督机器学习模型，如聚类​​或异常检测。
然后，对于任何新用户，他们都会收到来自监督模型的预测分数，如果他们是异常，就会被标记。这样，我既可以平衡历史模式，又可以解释新的欺诈模式。]]></description>
      <guid>https://stackoverflow.com/questions/79205257/when-to-use-deep-learning-for-bot-and-fraud-detection-and-when-not-to-use-it</guid>
      <pubDate>Tue, 19 Nov 2024 22:38:13 GMT</pubDate>
    </item>
    <item>
      <title>Ollama 中使用 GGUF 格式微调 LLaMA 3.2 1B 模型时出现问题</title>
      <link>https://stackoverflow.com/questions/79205128/issue-with-fine-tuned-llama-3-2-1b-model-in-ollama-using-gguf-format</link>
      <description><![CDATA[我最近使用 Unsloth 在自定义数据集上对 LLaMA 3.21B 基础模型进行了微调。微调后，我将模型导出为 GGUF 格式并尝试将其加载到 Ollama 中。虽然微调后的模型在 Google Colab 中经过训练的数据集上运行良好，但在我将 GGUF 文件上传到 Ollama 后，它开始产生乱码响应。
随后进行微调
数据集准备：我使用了 CSV 格式的自定义数据集。以下是我用于将数据集格式化为 Alpaca 样式提示的代码片段：
import pandas as pd
from datasets import Dataset

# 使用不同编码加载自定义 CSV 数据集
df = pd.read_csv(&quot;/content/llama3.2trainedDataset.csv&quot;, encoding=&#39;ISO-8859-1&#39;)

# 将 Pandas DataFrame 转换为 Hugging Face 数据集
dataset = Dataset.from_pandas(df)

# 定义 Alpaca 提示格式
alpaca_prompt = &quot;&quot;&quot;以下是描述临床试验的研究目标。根据给定的研究目标编写纳入和排除标准。

### 研究目标：
{}

### 响应（纳入和排除标准）：
{}&quot;&quot;&quot;

EOS_TOKEN = tokenizer.eos_token # 确保生成正确停止。

# 定义函数以格式化提示
def formatting_prompts_func(examples):
goals = examples[&quot;eligibilityCriteria&quot;] # 从“eligibilityCriteria”列中研究目标
criteria = examples[&quot;combined_criteria&quot;] # 从“combined_criteria”列中获取标准

texts = []
for objective, criterion in zip(objectives,criteria):
# 添加 EOS_TOKEN 以确保生成正确停止。
text = alpaca_prompt.format(objective, criterion) + EOS_TOKEN
texts.append(text)
return {&quot;text&quot;: texts}

# 将格式化函数应用于您的数据集
dataset = dataset.map(formatting_prompts_func, batched=True)

微调配置：我使用 Unsloth 的以下参数微调了模型：
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
import wandb # 导入 WandB

trainer = SFTTrainer(
model = model,
tokenizer = tokenizer,
train_dataset = dataset,
dataset_text_field = &quot;text&quot;,
max_seq_length = max_seq_length,
dataset_num_proc = 2,
packing = False, # 可以使短序列的训练速度提高 5 倍。
args = TrainingArguments(
per_device_train_batch_size = 2,
gradient_accumulation_steps = 4,
warmup_steps = 5,
num_train_epochs = 1, # 将其设置为 1 次完整的训练运行。
learning_rate = 2e-4,
fp16 = not is_bfloat16_supported(),
bf16 = is_bfloat16_supported(),
logs_steps = 1,
optim = &quot;adamw_8bit&quot;,
weight_decay = 0.01,
lr_scheduler_type = &quot;linear&quot;,
seed = 3407,
output_dir = &quot;outputs&quot;,
report_to = &quot;wandb&quot;, # 用于 WandB 等。
),
)

# 训练模型
trainer_stats = trainer.train()


保存模型：训练后，我使用以下代码以 GGUF 格式保存了模型：
model.save_pretrained(&quot;lora_model&quot;) # 本地保存
tokenizer.save_pretrained(&quot;lora_model&quot;)

# 保存为 GGUF 格式
model.save_pretrained_gguf(&quot;model&quot;, tokenizer)

上传到 Ollama：我下载了 unsloth.Q8_0.gguf 文件并将其加载到 Ollama：
来自 unsloth.Q8_0.gguf

问题

在 Google Colab 中测试微调模型时（在将其导出到 GGUF 之前），它与训练过的数据集完美配合。
但是，将其导出为 GGUF 格式并上传到 Ollama 后，该模型开始产生与训练数据集完全无关的乱码响应提示。

我的问题

为什么经过微调的 LLaMA 3.21B 模型在 Google Colab 中运行良好，但在使用 GGUF 文件在 Ollama 本地加载时却会产生乱码响应？
我在 Ollama 中使用的模板是否适合经过微调的模型？这个问题是否与 GGUF 文件的生成方式或 Ollama 如何解释模型有关？
]]></description>
      <guid>https://stackoverflow.com/questions/79205128/issue-with-fine-tuned-llama-3-2-1b-model-in-ollama-using-gguf-format</guid>
      <pubDate>Tue, 19 Nov 2024 21:36:18 GMT</pubDate>
    </item>
    <item>
      <title>Sk-learn GridSearchCV 适用于完整数据</title>
      <link>https://stackoverflow.com/questions/61882244/sk-learn-gridsearchcv-fits-on-full-data</link>
      <description><![CDATA[我使用 sklearn GridSearchCV 通过 lda 模型搜索了 # 个主题。拟合模型后，拟合模型保存在 CV_model.best_estimator_ 中。根据 sklearn 文档，GridSearchCV 具有默认选项 refit, default=True，即“使用在整个数据集上找到的最佳参数重新拟合估计器”。 Sklearn GridSearchCV
由于文档中说它已经适合完整数据，因此我相信 CV_model.best_estimator_.fit_transform(full_train_data) 将带来与 CV_model.best_estimator_.transform(full_train_data) 相同的结果。但是，使用 fit_transform 和 transform 的输出不同。我错过了什么？在 GridsearchCV 之后我应该使用 fit_transform 还是 transform？]]></description>
      <guid>https://stackoverflow.com/questions/61882244/sk-learn-gridsearchcv-fits-on-full-data</guid>
      <pubDate>Tue, 19 May 2020 02:29:07 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的 `squeeze()` 与 `unsqueeze()`</title>
      <link>https://stackoverflow.com/questions/61598771/squeeze-vs-unsqueeze-in-pytorch</link>
      <description><![CDATA[我不明白 squeeze() 和 unsqueeze() 对张量做了什么，即使查看了文档和相关问题。
我试图通过在 Python 中自己探索来理解它。我首先用创建了一个随机张量
x = torch.rand(3,2,dtype=torch.float)
&gt;&gt;&gt; x
tensor([[0.3703, 0.9588],
[0.8064, 0.9716],
[0.9585, 0.7860]])

但无论我如何挤压它，我都会得到相同的结果：
torch.equal(x.squeeze(0), x.squeeze(1))
&gt;&gt;&gt; True

如果我现在尝试解压，我会得到以下结果，
&gt;&gt;&gt; x.unsqueeze(1)
张量([[[0.3703, 0.9588]],
[[0.8064, 0.9716]],
[[0.9585, 0.7860]]])
&gt;&gt;&gt; x.unsqueeze(0)
张量([[[0.3703, 0.9588],
[0.8064, 0.9716],
[0.9585, 0.7860]]])
&gt;&gt;&gt; x.unsqueeze(-1)
tensor([[[0.3703],
[0.9588]],
[[0.8064],
[0.9716]],
[[0.9585],
[0.7860]]])

但是，如果我现在创建一个张量  x = torch.tensor([1,2,3,4])，然后我尝试解压它，那么看起来 1 和 -1 使它成为一列，而 0 保持不变。
x.unsqueeze(0)
tensor([[1, 2, 3, 4]])
&gt;&gt;&gt; x.unsqueeze(1)
tensor([[1],
[2],
[3],
[4]])
&gt;&gt;&gt; x.unsqueeze(-1)
tensor([[1],
[2],
[3],
[4]])

有人能解释一下 squeeze() 和 unsqueeze() 对张量做了什么吗？提供参数 0、1 和 -1 之间有什么区别？]]></description>
      <guid>https://stackoverflow.com/questions/61598771/squeeze-vs-unsqueeze-in-pytorch</guid>
      <pubDate>Mon, 04 May 2020 18:06:38 GMT</pubDate>
    </item>
    <item>
      <title>如何正确地将不平衡的数据集拆分为训练集和测试集？</title>
      <link>https://stackoverflow.com/questions/57229775/how-can-i-properly-split-imbalanced-dataset-to-train-and-test-set</link>
      <description><![CDATA[我有一个航班延误数据集，在采样之前尝试将该数据集拆分为训练集和测试集。准时情况约占总数据的 80%，延误情况约占 20%。
通常，机器学习中训练集和测试集的大小比例为 8:2。但数据太不平衡了。因此，考虑到极端情况，大多数训练数据都是准时情况，而大多数测试数据都是延误情况，准确率会很差。
所以我的问题是如何正确将不平衡的数据集拆分为训练集和测试集？]]></description>
      <guid>https://stackoverflow.com/questions/57229775/how-can-i-properly-split-imbalanced-dataset-to-train-and-test-set</guid>
      <pubDate>Sat, 27 Jul 2019 06:34:52 GMT</pubDate>
    </item>
    <item>
      <title>仅使用 2 张图像即可自动进行人脸验证[关闭]</title>
      <link>https://stackoverflow.com/questions/45351886/automatic-face-verification-with-only-2-images</link>
      <description><![CDATA[问题陈述：
给定两张图片，例如下面两张布拉德·皮特的图片，判断图片中是否包含同一个人。困难在于我们每个人只有一张参考图像，如何确定其他传入图像是否包含同一个人。

一些研究：
有几种不同的方法可以解决此任务，这些是

使用颜色直方图
面向关键点的方法
使用深度卷积神经网络或其他 ML 技术

直方图方法涉及根据颜色计算直方图并在它们之间定义某种度量，然后确定阈值。我尝试过的一个方法是地球移动距离。但是这种方法缺乏准确性。
因此，最好的方法应该是第二种和第三种方法的某种混合，以及一些预处理。
对于预处理，明显的步骤是：

运行人脸检测（例如 Viola-Jones）并分离包含人脸的区域
将所述人脸转换为灰度
运行眼睛、嘴巴、鼻子检测算法（可能使用 opencv 的 haar_cascades）
根据找到的标志对齐人脸图像

所有这些都是使用 opencv 完成的。
提取诸如 SIFT 和 MSER 之类的特征可产生介于73-76%。经过一些额外的研究，我偶然发现了这篇使用fisherfaces的论文。事实上，opencv 现在有能力创建 fisherface 检测器并对其进行训练，这很棒，而且效果非常好，达到了论文在耶鲁数据集上承诺的准确率。
问题的复杂之处在于，在我的情况下，我没有一个包含同一个人的多张图像的数据库来训练检测器。我只有一张对应于一个人的图像，而给出另一张图像，我想了解这是否是同一个人。
所以我感兴趣的是`
有人尝试过这样的事情吗？我应该研究哪些论文/方法/库？
您对如何解决问题有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/45351886/automatic-face-verification-with-only-2-images</guid>
      <pubDate>Thu, 27 Jul 2017 13:14:18 GMT</pubDate>
    </item>
    <item>
      <title>深度学习序列推理</title>
      <link>https://stackoverflow.com/questions/43880116/deep-learning-for-inferences-in-sequences</link>
      <description><![CDATA[我想使用深度学习技术来执行比隐马尔可夫模型（浅层模型）更好的推理任务？我想知道什么是最先进的深度学习模型来取代隐马尔可夫模型 (HMM)？该设置是半监督的。训练数据 X(t)、Y(t) 是一个时间序列，具有显著的时间相关性。此外，还有大量未标记的数据，即只有 X(t) 而没有 Y(t)。在阅读了许多论文后，我缩小了以下模型 -&gt; 条件限制玻尔兹曼机（Ilya Sustkever 硕士论文）并使用深度信念网络进行无监督预训练（或使用变分自动编码器进行预训练）。我对这个领域很陌生，想知道这些技术是否已经过时了。]]></description>
      <guid>https://stackoverflow.com/questions/43880116/deep-learning-for-inferences-in-sequences</guid>
      <pubDate>Tue, 09 May 2017 21:25:33 GMT</pubDate>
    </item>
    </channel>
</rss>