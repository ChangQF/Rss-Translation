<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Tue, 11 Feb 2025 03:21:28 GMT</lastBuildDate>
    <item>
      <title>无法使用TensorFlow C ++ API执行预测</title>
      <link>https://stackoverflow.com/questions/79428265/failed-to-execute-a-prediction-with-the-tensorflow-c-api</link>
      <description><![CDATA[我创建并训练了一个模型来识别一些卡通的角色。
我正在使用Tensorflow 2.18，并且用TF Python API训练的模型足够准确。
准确性非常好。
训练后，该模型通过 tensorflow.saved_model.save（）。保存。
 i测试模型，使用Python API，一切正常。
现在，我正在尝试使用C ++ API测试模型。

加载模型：OK 
将图像转换为张量：OK 
执行推理：错误如下

  2025-02-10 21：07：16.084677：i tensorflow/core/core/framework/local_rendezvous.cc：405] local Rendezvous正在与状态中流产：failed_precontition：无法找到可变的顺序/conv2d_3/kernel。这可能意味着已删除该变量。在TF1中，这也可能意味着该变量是非初始化的。调试信息：container = localhost，状态错误消息=资源localhost/sequential/conv2d_3/kernel/n10tensorflow3vare不存在。
 
请注意，找不到变量可能会因执行而异。
我知道C ++ API DOC的记录不是很好。
用于加载模型，我正在使用以下代码段：
  ///尝试加载模型 

m_status = loadsavedmodel（m_sessionoptions， 
                          m_runoptions，filepath，tensorflow :: ksavedmodeltagserve，＆amp; m_bundle）; 
 
执行推理：
  //执行推理
TensorFlow ::状态= 
m_bundle.getSession（） - ＆gt; run（
{{{＆quort; serving_default_inputs：0＆quort; inputtensor}}，
{＆quot; statefulpartitionedcall：0＆quort;}，
{}，
＆amp;输出）;
 
如何解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79428265/failed-to-execute-a-prediction-with-the-tensorflow-c-api</guid>
      <pubDate>Mon, 10 Feb 2025 20:44:06 GMT</pubDate>
    </item>
    <item>
      <title>FLAML中是否有相当于Optuna get_param_importances？</title>
      <link>https://stackoverflow.com/questions/79428150/is-there-an-equivalent-of-optuna-get-param-importances-in-flaml</link>
      <description><![CDATA[我正在尝试使用FLAML进行模型的高参数调整，我想看看每个高参数如何促进目标值。类似于Optuna的 get_param_importances 或 plot_param_importances 。有没有办法使用flaml？]]></description>
      <guid>https://stackoverflow.com/questions/79428150/is-there-an-equivalent-of-optuna-get-param-importances-in-flaml</guid>
      <pubDate>Mon, 10 Feb 2025 19:48:01 GMT</pubDate>
    </item>
    <item>
      <title>机器学习模型建议使用一个传感器的输出来从其他传感器生成输出[封闭]</title>
      <link>https://stackoverflow.com/questions/79427928/machine-learning-model-suggestions-for-using-an-output-from-one-sensor-to-genera</link>
      <description><![CDATA[您能否建议使用其他传感器的输出来从传感器产生输出？
传感器应该显示同一件事，但只使用不同的方法来获取连续的时间信号数据。我有来自两个传感器的时间同步数据记录，并希望根据这些数据训练模型。然后，我想提供一个传感器记录作为模型的输入，并从另一个传感器中获取谓语信号记录。我是否有关于我可以用来实现这一目标的机器学习方法的建议？
注意：数据是两个传感器的连续时间记录的不同集，每个传感器的记录大约为一分钟。]]></description>
      <guid>https://stackoverflow.com/questions/79427928/machine-learning-model-suggestions-for-using-an-output-from-one-sensor-to-genera</guid>
      <pubDate>Mon, 10 Feb 2025 18:02:32 GMT</pubDate>
    </item>
    <item>
      <title>但是，使用合规列名称上传数据。但是，它仍然通知列名中有无效的数据</title>
      <link>https://stackoverflow.com/questions/79427914/upload-data-on-neuton-ai-using-compliant-column-name-however-it-still-notify-t</link>
      <description><![CDATA[我正在使用neton.ai训练我的小型AI模型，我有一个由Excel编辑的CSV文件数据集，格式为：
 


 ax1 
 ay1 
 az1 
 gx1 
 gy1 
 gz1 
 ... 
 ax50 
 ay50 
 az50 
 gx50 
 gy50 
 gz50 




数据
数据
数据
数据
数据
数据
 ... 
数据
数据
数据
数据
数据
数据


数据
数据
数据
数据
数据
数据
 ... 
数据
数据
数据
数据
数据
数据


 ... 
 
 
 
 
 
 
 
 
 
 
 
 


 
问题是，即使我尝试多次以不同的方式更改列名，它也会给我相同的错误消息：
 所有列名（CSV文件标头中的值）必须仅包含字母（A-Z，A-Z）， 
数字（0-9），连字符（ - ）或下划线（_）。更改名称并下载 
再次数据集。
 
 错误消息屏幕屏幕示例 
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79427914/upload-data-on-neuton-ai-using-compliant-column-name-however-it-still-notify-t</guid>
      <pubDate>Mon, 10 Feb 2025 17:56:26 GMT</pubDate>
    </item>
    <item>
      <title>使用OpenCV和MediaPipe进行姿势估算时面对此错误[封闭]</title>
      <link>https://stackoverflow.com/questions/79427900/facing-this-error-when-doing-pose-estimation-with-opencv-and-mediapipe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79427900/facing-this-error-when-doing-pose-estimation-with-opencv-and-mediapipe</guid>
      <pubDate>Mon, 10 Feb 2025 17:49:19 GMT</pubDate>
    </item>
    <item>
      <title>降尺度操作不捕获峰[关闭]</title>
      <link>https://stackoverflow.com/questions/79427067/downscaling-operation-not-capturing-the-peaks</link>
      <description><![CDATA[我正在使用随机的森林回归模型将数据集降低从更粗的分辨率到更高分辨率。我尝试了交叉验证并调整参数。在检查验证分数时，我的MSE和R2值都相当好。但是，当我使用相同的模型预测新数据集的目标变量时，该模型将无法捕获数据初始数年的峰值。可能是其中一个功能具有主要的重要性。在绘制主要功能的数据时，全年数据稳定。
这使趋势偏离了
具有最重要性的数据之一  ]]></description>
      <guid>https://stackoverflow.com/questions/79427067/downscaling-operation-not-capturing-the-peaks</guid>
      <pubDate>Mon, 10 Feb 2025 12:29:38 GMT</pubDate>
    </item>
    <item>
      <title>如何在大型数据集上训练XGBoost并改善欺诈检测？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79426998/how-to-train-xgboost-on-a-large-dataset-and-improve-fraud-detection</link>
      <description><![CDATA[我只是从ML开始，所以我感谢任何建议。
我正在尝试培训一个模型以进行交易中的欺诈检测。数据高度不平衡（〜96％的正常人比〜4％欺诈）。
第一期 - 记忆消耗
培训文件为32 GB，但是即使仅阅读100万行，我也会收到一个内存分配错误：
  XGBOOST.CORE.XGBOOSTERROR：BAD_MALLOC：无法分配255479999900字节。
 
第二期 - 预测质量差
我正在对100k行进行训练，但是无论我如何调整XGBoost，该模型都几乎都无法检测到欺诈案件。
在这种情况下，XGBoost的最佳类平衡技术是什么？我应该如何处理这个数据集？您建议更改什么？
在此处输入图像描述 
  df = pd.read_csv（&#39;train.csv&#39;，nrows = 100000） 

df.drop（[&#39;transaction_id&#39;，&#39;card_holder_first_name&#39;，&#39;card_holder_last_name&#39;，&#39;is_verified&#39;，&#39;browser&#39;，&#39;browser_version&#39;，&#39;browser_version&#39;，&#39;
    &#39;operation_system&#39;，&#39;operation_system_version&#39;，&#39;card_id&#39;，&#39;ip_address&#39;，&#39;merchant_customer_id&#39;，&#39;merchant_id&#39;，&#39;user_agent&#39;，&#39;user_agent&#39;，
    &#39;merchant_customer_last_name&#39;，&#39;merchant_customer_first_name&#39;，&#39;merchant_customer_phone&#39;，&#39;merchant_customer_email&#39;，&#39;bin&#39;，&#39;bin&#39;，&#39;&#39;
    &#39;clockal_source&#39;，&#39;transaction_source&#39;，&#39;merchant_city&#39;，&#39;merchant_shop_id&#39;，&#39;merchant_shop_name&#39;，&#39;order_number&#39;]， 
    轴= 1，Inplace = true）

df [&#39;bank&#39;]。替换（&#39;&#39;，&#39;_&#39;，regex = true，inplace = true）

df [&#39;create_at&#39;] = pd.to_datetime（df [&#39;create_at&#39;]）
df [&#39;seconds_since_midnight&#39;] = df [&#39;create_at&#39;]。dt.hour * 3600 + df [&#39;create_at&#39;]。dt.minute * 60 + df [&#39;create_at&#39;]。dt.dt.second
df [&#39;day_of_week&#39;] = df [&#39;create_at&#39;]。dt.weekday

df.drop（&#39;create_at&#39;，axis = 1，Inplace = true）

df.loc [pd.isna（df [&#39;merchant_language&#39;]），&#39;merchant_language&#39;] =&#39;unknown&#39;

df.loc [pd.isna（df [&#39;payment_type&#39;]），&#39;payment_type&#39;] = 0

x = df.drop（&#39;is_fraud&#39;，axis = 1）.copy（）

y = df [&#39;is_fraud&#39;]。copy（）

x_encoded = pd.get_dummies（x，columns = [&#39;merchant_country&#39;，      
                                        &#39;transaction_type&#39;，
                                        “商人_language”，
                                        &#39;平台&#39;，
                                        &#39;ip_country&#39;，
                                        &#39;银行&#39;，
                                        “ cardbrand&#39;，
                                        &#39;cardcountry&#39;，
                                        &#39;cardtype&#39;， 
                                        &#39;payment_type&#39;]））


x_train，x_test，y_train，y_test = train_test_split（x_encoded，y，andury_state = 42，strate = y）


clf_xgb = xgb.xgbClassifier（
    objective =;二进制：logistic; quot;
    种子= 42，
    eval_metric =＆quot; aucpr; quot;
    早期_stopping_rounds = 10，
    max_depth = 6，  
    子样本= 0.8，  
    colsample_bytree = 0.8 
）

clf_xgb.fit（
    x_train，
    y_train，
    eval_set = [（x_test，y_test）]，
    冗长= true
）

disp =混淆matrixdisplay.from_estimator（
    clf_xgb，  
    x_test，  
    y_test，  
    display_labels = [不是欺诈者“  
    cmap =&#39;blues;
）
disp.plot（values_format =&#39;d&#39;）

plt.show（）
 
我是ML的新手，所以我还没有尝试过很多技术。我尝试了不同的XGBoost参数，并减少了数据集大小以适合内存，但是该模型仍在努力检测欺诈情况。我不确定最好的方法是处理如此大的不平衡数据集，因此我感谢任何建议。]]></description>
      <guid>https://stackoverflow.com/questions/79426998/how-to-train-xgboost-on-a-large-dataset-and-improve-fraud-detection</guid>
      <pubDate>Mon, 10 Feb 2025 11:56:48 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习和NER自动化网络刮擦，以进行产品数据提取</title>
      <link>https://stackoverflow.com/questions/79426509/automating-web-scraping-with-machine-learning-and-ner-for-product-data-extractio</link>
      <description><![CDATA[我目前正在执行数据＆amp; AI实习。我的工作是通过从制造商的网站中检索信息（产品名称，图像，描述，零件号/SKU，技术规格，数据表等）来构建产品数据库。
挑战是，有300多个不同的制造商，每个制造商都有自己的网站和结构，使传统的网络刮擦不切实际且难以维护。为了克服这一点，我正在考虑使用AI和机器学习，以使我的刮擦剂适应每个页面的HTML结构的更改。
我已经下载并手动标记了50页。这是我的数据集的样子：
 ＃列非零计数dtype 
------------------------------------------- 
 0文本52非挂钩对象
 1个product_name 49非无效对象
 2 HTML_PRODUCT_NAME 51非无效对象
 3 image_url 50非无效对象
 4 html_image_url 50非挂钩对象
 5说明32非无效对象
 6 HTML_DESCRIPTION 51非无效对象
 7 part_number 35非无效对象
 8 HTML_PART_NUMBER 36非无效对象
 9 html_specification 44非无效对象
 10 dataSheet_url 40非无效对象
 11 HTML_DATASHEET_URL 41非无效对象
 12规格2非无效对象 
 
文本列包含产品页面的清洁html，而其他列则代表目标字段 - 需要识别和提取的HTML的特定部分。
这个问题似乎与命名实体识别（NER）非常相似。如何训练机器学习模型以成功地从RAW HTML中提取这些字段？最好的方法是什么（例如，微调变压器模型，序列标签或其他方法）？
预先感谢！]]></description>
      <guid>https://stackoverflow.com/questions/79426509/automating-web-scraping-with-machine-learning-and-ner-for-product-data-extractio</guid>
      <pubDate>Mon, 10 Feb 2025 08:46:11 GMT</pubDate>
    </item>
    <item>
      <title>ML模型保存</title>
      <link>https://stackoverflow.com/questions/79426066/ml-models-saving</link>
      <description><![CDATA[我正在训练一种机器学习模型，将阿尔茨海默氏病分为四类。运行培训时期后，我使用代码将模型保存在.pth文件中。但是，在下载了保存的模型并将其与我的接口连接起来后，它没有产生任何结果。经过进一步检查，似乎模型是空的。
 导入火炬
导入Torch.nn作为nn
导入Torch.optim作为最佳
来自TQDM.Auto Import TQDM
来自Torchvision导入模型

＃设置设备（GPU如果可用）
设备= torch.device（&#39;cuda&#39;如果torch.cuda.is_available（）else&#39;cpu&#39;）

＃定义Resnet18模型
类Resnet18（nn.Module）：
    def __init __（self，num_classes）：
        super（resnet18，self）.__ init __（）
        self.model = model.Resnet18（预审计= false）＃设置为true以使用预训练的权重
        self.model.fc = nn.linear（self.model.fc.in_features，num_classes）＃修改最终层

    def向前（self，x）：
        返回self.model（x）

＃用数据集中的类数初始化模型
num_classes = len（train_data_simple.classes）＃根据您的增强数据集进行调整
model_resnet = resnet18（num_classes = num_classes）。

＃损失功能和优化器（SGD和Crossentropy）
loss_fn = nn.Crossentropyloss（）
优化器= optim.sgd（model_resnet.parameters（），lr = 0.1，动量= 0.9）

＃训练循环（使用TQDM进行进度栏）
def train（型号，train_dataloader，test_dataloader，优化器，loss_fn，epochs）：
    对于TQDM中的时期（范围（时代））：
        model.train（）＃将模型设置为训练模式
        Running_loss = 0.0
        recript_train，total_train = 0，0

        对于枚举（train_dataloader）的批次（x，y）：
            x，y = x.to（设备），y.to（设备）

            优化器.zero_grad（）＃零梯度

            输出=模型（x）＃前向通行证
            损失= loss_fn（输出，y）＃计算损失

            lose.backward（）＃backpropagation
            Optimizer.step（）＃更新权重

            running_loss += loss.item（）
            _，预测= torch.max（outputs.data，1）
            total_train += y.size（0）
            recripe_train +=（预测== y）.sum（）。item（）

        ＃打印培训损失和准确性
        打印（f＆quot; epoch [{epoch+1}/{epochs}]，损失：{runn_loss/len（train_dataloader）：。4f}＆quort;）
        打印（f＆quot“训练精度：{100 * recript_train / total_train：.2f}％＆quot”）

        ＃ 验证
        model.eval（）＃开关模型到评估模式进行验证
        recript_val，total_val = 0，0
        使用Torch.no_grad（）：
            对于x_val，y_val在test_dataloader中：
                x_val，y_val = x_val.to（设备），y_val.to（设备）
                val_outputs =模型（x_val）
                _，预测= torch.max（val_outputs.data，1）
                total_val += y_val.size（0）
                prection_val +=（预测== y_val）.sum（）。item（）。

        ＃打印验证精度
        打印（f＆quot“验证精度：{100 * recript_val / total_val：.2f}％＆quort”）

    返回模型

＃示例培训
num_epochs = 30＃根据您的要求调整时期
＃使用增强培训数据加载器
trained_model_resnet = train（model_resnet，train_dataloader_simple，test_dataloader_simple，optimizer，optimizer，lose_fn，num_epochs）

 
这是我在不同单元格中运行的代码来保存模型
 ＃保存训练有素的模型
model_path =＆quot trained_resnet18_model.pth＆quot;
TORCH.SAVE（model_resnet.state_dict（），model_path）
打印（f＆quot“模型保存到{model_path}”
 
我正在训练一种机器学习模型，将阿尔茨海默氏病分为四类，使用pytorch分为四类。经过几个时期的训练后，我想使用诸如model.save（&#39;my_model.h5&#39;）或torch.save（model.state_dict（），&#39;model.pth&#39;）的代码保存训练的模型。
我应该添加代码以将训练的模型保存在定义训练时期并运行训练环的同一单元格中，还是将其放在其他单元格中更好？放置是否会影响模型的保存方式，或者以后如何加载？]]></description>
      <guid>https://stackoverflow.com/questions/79426066/ml-models-saving</guid>
      <pubDate>Mon, 10 Feb 2025 04:15:24 GMT</pubDate>
    </item>
    <item>
      <title>在扩散模型中U形网络的上下抽样阶段中不同的缩小参数</title>
      <link>https://stackoverflow.com/questions/79422978/different-concate-dimension-parameters-in-the-up-and-down-sampling-phase-of-u-sh</link>
      <description><![CDATA[我遇到的错误是
 文件＆quot＆quot＆quot＆quot&gt;
    x = torch.cat（（x，residual_x），dim = 1） 
RuntimeError：张量的尺寸必须匹配，除了尺寸1。预期尺寸24，但在列表中获得张量1的尺寸25。
 
该模型的训练数据集输入是Paviau。
调试后要找到问题的输出向量如下：
  down = torch.size（[4，64，25，64，64]）
UP = Torch.Size（[4，64，24，64，64]）
 
整个代码如下：
 
    def向前（self，x，timeStep，feature = false）：
        ＃嵌入式时间
        t = self.time_mlp（timeStep）
        ＃初始转换
        x = self.conv0（x）
        ＃UNET
        ristual_inputs = []
        在self.downs中进行下降：
            x = down（x，t）
            ristual_inputs.append（x）
        为了自我。
            ristual_x = ristual_inputs.pop（）
            ＃print（down =; quord =; residual_x.shape，＆quot up =; quord =; x.Shape）
            ＃添加残留X作为其他频道
            x = torch.cat（（x，residual_x），dim = 1）* 
            如果特征：
                self.features.append（x.detach（）。cpu（）。numpy（））
            x =向上（x，t）
        返回self.output（x）
         
 ]]></description>
      <guid>https://stackoverflow.com/questions/79422978/different-concate-dimension-parameters-in-the-up-and-down-sampling-phase-of-u-sh</guid>
      <pubDate>Sat, 08 Feb 2025 10:14:34 GMT</pubDate>
    </item>
    <item>
      <title>从GPU内存中清除tf.data.dataset</title>
      <link>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</guid>
      <pubDate>Fri, 07 Feb 2025 12:02:12 GMT</pubDate>
    </item>
    <item>
      <title>随机森林回归中的树木数[关闭]</title>
      <link>https://stackoverflow.com/questions/56505551/number-of-trees-in-random-forest-regression</link>
      <description><![CDATA[我正在学习随机的森林回归模型。我知道它形成了许多树（模型），然后我们可以通过平均所有树的结果来预测目标变量。我也对决策树回归算法有一个下降的理解。我们如何形成最佳树木数量？
例如，我有一个数据集，我可以在其中预测人的薪水，而我只有两个输入变量是“年度经验”，“绩效得分”，那么我可以使用这样的数据集形成多少个随机树？随机林木是否取决于输入变量的数量？任何好例子都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/56505551/number-of-trees-in-random-forest-regression</guid>
      <pubDate>Sat, 08 Jun 2019 10:33:36 GMT</pubDate>
    </item>
    <item>
      <title>r的回归森林中的特征选择和预测准确性</title>
      <link>https://stackoverflow.com/questions/45935917/feature-selection-and-prediction-accuracy-in-regression-forest-in-r</link>
      <description><![CDATA[我正在尝试解决一个回归问题，其中输入功能集的大小〜54。
使用单个预测指标“ x1”使用OLS线性回归，我无法解释y-的变化 - 因此，我试图使用回归森林（即随机森林回归）找到其他重要特征。后来发现选定的“ X1”是最重要的功能。
我的数据集有〜14500个条目。我将其分为训练和测试集，比率为9：1。
我有以下问题：

 试图找到重要功能时，我应该在整个数据集上或仅在培训数据上运行回归森林吗？

 找到重要的功能后，是否应该使用顶级功能重新构建模型，以查看功能选择是否以较小的预测能力成本加速计算？

 目前，我已经使用训练集和所有功能构建了模型，并且我将其用于预测测试集。我正在计算训练集中的MSE和R平方。我在培训数据上获得了高MSE和低R2，并反向测试数据（如下所示）。这是不寻常的吗？


 ＆gt;森林＆lt;  -  rancomalforest（fmla，dtraining，ntree = 501，重要性= t）
    
     ＆gt;平均值（（dtraining $ y-预测（森林，data = dtraining））^2）
    
     ＆gt; ＆gt; 0.9371891
    
     ＆gt; rsquared（dtraining $ y，dtraining $ y-预测（森林，data = dtraining））

     ＆gt; ＆gt; 0.7431078
    
     ＆gt;平均值（（dtest $ y-预测（森林，newdata = dtest））^2）

     ＆gt; ＆gt; 0.009771256

     ＆gt; rsquared（dtest $ y，dtest $ y-预测（森林，newdata = dtest））
    
     ＆gt; ＆gt; 0.9950448
 
是否有R-Squared和MSE是此问题的好指标，或者我需要查看其他一些指标来评估该模型是否良好？]]></description>
      <guid>https://stackoverflow.com/questions/45935917/feature-selection-and-prediction-accuracy-in-regression-forest-in-r</guid>
      <pubDate>Tue, 29 Aug 2017 09:51:00 GMT</pubDate>
    </item>
    <item>
      <title>Python中的聚类时间序列数据</title>
      <link>https://stackoverflow.com/questions/45604143/clustering-time-series-data-in-python</link>
      <description><![CDATA[我正在尝试使用不同的聚类技术将Python聚集时间序列数据。 K-均值没有很好的效果。以下图像是使用聚集聚类聚类后的群集后的图像。我还尝试了动态的时间扭曲。这两个似乎给出了类似的结果。 
我理想地拥有第二张图像中时间序列的两个不同的簇。第一个图像是用于快速增加的群集。第二种不像稳定的稳定，第三个是降低趋势的集群。我想知道哪个时间序列既稳定又流行（在这里流行，我的意思是高度计数）。我尝试了分层聚类，但结果显示了太多的层次结构，我不确定如何选择层次结构。有人可以阐明如何将第二张图像中的时间序列分为两个不同的群集，一个群集计数低，另一个群体计数很高？有可能吗？或者我应该在视觉上选择一个阈值将它们切成两个？
快速增加的群集：
    
稳定计数的集群：
    
趋势下降的集群：
    
这是非常非常模糊的，但这是我分层聚类的结果。 
   
我知道这个特殊的图像根本没有用，但这对我来说也是一个死胡同。 
一般而言，如果您想区分趋势，例如为YouTube视频说，如何只为“流行趋势”部分挑选一些，而其他一些则是“本周趋势”部分？我了解“趋势”部分视频是显示与第一张图像相似的特征的视频。 “本周趋势”部分有一系列视频，这些视频的观点计数很高，但在计数方面保持稳定（即没有显示快速增加）。我知道，在YouTube的情况下，除了观看计数外，还有许多其他因素。有了第二张图像，我要做的事情类似于“本周趋势”部分。我想挑选那些很高的计数。在这种情况下，如何拆分时间序列？ 
我知道DTW捕获了趋势。 DTW给出了与上图相同的结果。它已经确定了第二张图像中的趋势，即“稳定”。但这并未在此处捕获“计数”元素。我希望趋势和计数被捕获，在这种情况下，稳定且计数很高。
上面的图像是根据计数聚类的时间序列。我是否错过了其他可以实现这一目标的聚类技术？即使仅计数，我如何根据我的需求不同？
任何想法都将不胜感激。预先感谢！]]></description>
      <guid>https://stackoverflow.com/questions/45604143/clustering-time-series-data-in-python</guid>
      <pubDate>Thu, 10 Aug 2017 03:51:00 GMT</pubDate>
    </item>
    <item>
      <title>Python监督机器学习</title>
      <link>https://stackoverflow.com/questions/35438540/python-supervised-machine-learning</link>
      <description><![CDATA[我试图了解如何使用 scikit 进行监督机器学习，因此我已经编造了一些属于两个不同集的数据：集合A和集合B。我有18个元素集合中的18个元素。每个元素都有三个变量。请参阅下文：
  #seta
变量1a = [3,4,4,5,4,5,5,6,7,7,5,4,5,6,4,9,3,4]
变量2A = [5,4,4,4,3,4,5,4,5,4,3,4,5,3,4,3,4,4,3]
变量3A = [7,8,4,5,6,7,3,3,3,4,4,9,7,6,8,6,7,8]


#setB
变量1b = [7,8,11,12,7,9,8,7,8,11,15,9,7,6,9,7,11]]
变量2b = [1,2,3,3,4,4,2,4,1,0,1,2,1,3,4,3,1,2,3]
变量3b = [12,18,14,15,16,17,13,13,13,14,14,14,19,17,16,18,16,17,18]
 
我将如何使用
对数据集很小的道歉，并且“构成”。我只想在其他数据集上使用Scikit应用相同的方法。]]></description>
      <guid>https://stackoverflow.com/questions/35438540/python-supervised-machine-learning</guid>
      <pubDate>Tue, 16 Feb 2016 16:59:48 GMT</pubDate>
    </item>
    </channel>
</rss>