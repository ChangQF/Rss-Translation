<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 18 May 2024 18:18:29 GMT</lastBuildDate>
    <item>
      <title>感知器的图像识别不起作用</title>
      <link>https://stackoverflow.com/questions/78500607/image-recognition-with-perceptrons-not-working</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78500607/image-recognition-with-perceptrons-not-working</guid>
      <pubDate>Sat, 18 May 2024 18:12:53 GMT</pubDate>
    </item>
    <item>
      <title>计算损失函数的梯度</title>
      <link>https://stackoverflow.com/questions/78500380/calculate-graident-of-loss-function</link>
      <description><![CDATA[考虑如下所示的神经网络。
考虑我们有一个用于二元分类的交叉熵损失函数：
参考图

L=−[𝑦 ln(𝑎)+(1−𝑦) ln(1−𝑎)]，其中𝑎是输出层激活函数的概率。我们构建了网络的计算图，如下所示。下面的蓝色字母是中间变量标签，可以帮助你理解上面的网络架构图和计算图之间的联系。

当 𝑦=1 时，损失函数相对于梯度的梯度是多少。 𝑊11？将你的答案写到小数点后三位。
注：请使用计算图法。可以直接使用链式规则计算梯度，但如果根本不使用计算图，则无法正确评分。尝试填写上面的红色框。本题不需要编码，分析即可轻松得出答案。

我在解决这个问题时遇到问题]]></description>
      <guid>https://stackoverflow.com/questions/78500380/calculate-graident-of-loss-function</guid>
      <pubDate>Sat, 18 May 2024 16:44:46 GMT</pubDate>
    </item>
    <item>
      <title>TPU v3-8 TensorFlow CrossReplicaSum 错误</title>
      <link>https://stackoverflow.com/questions/78500291/tpu-v3-8-tensorflow-crossreplicasum-error</link>
      <description><![CDATA[当我适合我的模型时，我收到此错误。
tensorflow/core/tpu/kernels/tpu_compilation_cache_external.cc:112] 要求将动态维度从 hlo transpose.3750@{}@0 传播到 hlo %all-reduce.3755 = f32[&lt;= 70,256]{1,0}全归约(f32[&lt;=70,256]{1,0}%transpose.3750),replica_groups={{0,1,2,3,4,5,6,7}} ，to_apply=%sum.3751，metadata={op_type=“CrossReplicaSum” op_name=“CrossReplicaSum_33” source_file=“虚拟文件名” source_line=10}，未实现。

1013 tpu_program_group.cc:90] 检查失败：xla_tpu_programs.size() &gt; 0（0 对 0）

但是我传递显式输入形状，如下所示：
Config.COMPUTED_BATCH_SIZE = 128

使用strategy.scope()：
    模型 = my_model()
    输入形状 = [
        [配置.COMPUTED_BATCH_SIZE, 192],
        [配置.COMPUTED_BATCH_SIZE, 192],
        [计算通道、105、129、100]、
        [计算通道、105、129、100]、
        [计算通道、105、129、100]、
        [配置.COMPUTED_BATCH_SIZE, 70],
        [配置.COMPUTED_BATCH_SIZE, 320]
    ]
    model.build(input_shape=input_shapes)

编辑：
我已经追踪到这段代码：
hidden_​​size = 128
self.descriptor_embedding = 层.Dense(
    隐藏大小 * 2, #256
    激活=&#39;relu&#39;,
    input_shape=(Config.COMPUTED_BATCH_SIZE, 70)
）


学习描述符 = tf.expand_dims(
    self.descriptor_embedding（描述符），
    1
) # [BS, 1, HS * 2]

有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78500291/tpu-v3-8-tensorflow-crossreplicasum-error</guid>
      <pubDate>Sat, 18 May 2024 16:07:20 GMT</pubDate>
    </item>
    <item>
      <title>无法为 tflite 模型创建地图</title>
      <link>https://stackoverflow.com/questions/78500067/unable-to-create-a-map-for-tflite-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78500067/unable-to-create-a-map-for-tflite-model</guid>
      <pubDate>Sat, 18 May 2024 14:41:10 GMT</pubDate>
    </item>
    <item>
      <title>我该如何解决python中的属性错误？ [复制]</title>
      <link>https://stackoverflow.com/questions/78499207/how-can-i-solve-attributeerror-in-python</link>
      <description><![CDATA[我正在从事一个数据科学项目，但遇到了困难
我使用了附加功能，但它不起作用。
这里给出了一个错误：
AttributeError：“DataFrame”对象没有属性“concat”
def推荐（名称，cosine_similarities = cosine_similarities）：
    # 创建一个列表来放置顶级餐厅
    推荐餐厅=[]
    
    # 查找输入的酒店索引
    idx = 索引[索引 == 名称].index[0]
    
    # 找到具有相似 cosine-sim 值的餐厅并从大数中排序
    Score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending=False)
    
    # 提取 cosine-sim 值相似的前 30 家餐厅索引
    top30_indexes = 列表(score_series.iloc[0:31].index)
    
    # 排名前 30 的餐厅名称
    对于 top30_indexes 中的每个：
        推荐_餐厅.append（列表（df_percent.index）[每个]）
    
    # 创建新数据集来显示类似的餐馆
    df_new = pd.DataFrame(columns=[&#39;美食&#39;, &#39;平均评分&#39;, &#39;成本&#39;])
    
    # 创建前 30 名类似餐厅及其一些列
    对于推荐餐厅中的每个：
        df_new = df_new.concat(pd.DataFrame(df_percent[[&#39;cuisines&#39;,&#39;平均评分&#39;, &#39;cost&#39;]][df_percent.index == every].sample()))
    
    # 删除同名餐厅，仅按最高评分对前 10 家餐厅进行排序
    df_new = df_new.drop_duplicates(subset=[&#39;cuisines&#39;,&#39;平均评分&#39;, &#39;cost&#39;], keep=False)
    df_new = df_new.sort_values(by=&#39;平均评分&#39;, 升序=False).head(10)
    
    print(&#39;排名前 %s 的餐厅，如 %s 且具有相似的评论：&#39; % (str(len(df_new)), name))
    
    返回 df_new
]]></description>
      <guid>https://stackoverflow.com/questions/78499207/how-can-i-solve-attributeerror-in-python</guid>
      <pubDate>Sat, 18 May 2024 09:14:08 GMT</pubDate>
    </item>
    <item>
      <title>计算错误的神经网络比正确计算的神经网络更好</title>
      <link>https://stackoverflow.com/questions/78497893/neural-network-with-incorrect-calculation-better-than-correct-one</link>
      <description><![CDATA[我设计了自己的神经网络并发现了一个错误。在反向传播期间，我没有将 Z 值插入到激活函数的导数中，而是插入了 A 值。结果是，当我使用 A 值时，神经网络比使用 Z 值进行计算学习得更快、更稳定。计算应该是不正确的。那么为什么它效果更好，产生更好的结果和更稳定的结果呢？
Z=x×w+b A=激活函数(Z)
错误的计算却有更好的结果：
dA/dZ=activationfunction_derivative(A)
delta = deltas[-1].dot(self.weights[i].T) * self.leaky_relu_derivative(self.activations[i])
正确的计算，但结果更差：
dA/dZ=activationfunction_derivative(Z)
错误的代码具有更好的结果：
defleaky_relu(self, x, alpha=0.01):
    返回 np.where(x &gt; 0, x, alpha * x)

defleaky_relu_derivative(self, x, alpha=0.01):
    返回 np.where(x &gt; 0, 1, alpha)
def 线性（自身，x）：
    返回x

def 线性导数（自身）：
    返回1

defforward_propagation(self, X):
    自我激活 = []
    激活=X
    self.activations.append(激活)
    对于权重，zip中的偏差(self.weights[:-1], self.biases[:-1])：
        激活 = self.leaky_relu(np.dot(激活, 权重) + 偏差)
        self.activations.append(激活)
    激活 = self.线性(np.dot(激活, self.weights[-1]) + self.biases[-1])
    self.activations.append(激活)
    返回激活

def msnq(self, y_true, y_pred):
    返回 np.mean(np.square(y_true - y_pred))

def 反向传播(self, y_true, t):
    增量 = []
    错误= y_true - self.activations[-1]
    delta = 误差 * self.linear_derivative()
    deltas.append(delta)
    对于范围内的 i(len(self.activations) - 2, 0, -1)：
        delta = deltas[-1].dot(self.weights[i].T) * self.leaky_relu_derivative(self.activations[i])
        deltas.append(delta)
]]></description>
      <guid>https://stackoverflow.com/questions/78497893/neural-network-with-incorrect-calculation-better-than-correct-one</guid>
      <pubDate>Fri, 17 May 2024 20:48:33 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 - 如何将数据清理纳入训练模型中</title>
      <link>https://stackoverflow.com/questions/78497891/machine-learning-how-to-incorporate-data-cleansing-into-trained-model</link>
      <description><![CDATA[我一直在尝试寻找这个问题的答案，但还没有找到任何与之相关的东西。
问题是，如果我清理数据并将中值归结为 NaN 值，我是否应该以某种方式将其合并到将用于测试数据的模型中。换句话说，我的测试数据是否也需要清理和归结，或者训练会处理这个问题。我想说它需要被纳入，因为否则 NaN 值会破坏模型，而且任何偏斜都不会得到解决。
特别是：
用中位数替换 NaN：
data = data.fillna(data.median())

使用分位数变换处理偏斜，以遵循每个特征的正态分布（以下仅针对其中一个）。
qualtile_transformer = QuantileTransformer(output_distribution=&#39;normal&#39;, random_state=0&#39;)
data[&#39;feat_0&#39;] = quantile_transformer.fit_transform(data[&#39;feat_0&#39;].values.reshape(-1,1)).flatten()

模型：
来自 sklearn.linear_model import LinearRegression
linear_regr = LinearRegression()
linear_regr.fit(Xtrain,Ytrain)

预测：
# 使用测试集进行预测
Ypred = linear_regr.predict(Xtest)

谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78497891/machine-learning-how-to-incorporate-data-cleansing-into-trained-model</guid>
      <pubDate>Fri, 17 May 2024 20:47:39 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 为我的标签应用程序处理数据库锁</title>
      <link>https://stackoverflow.com/questions/78497797/handle-database-locks-in-python-for-my-labeling-app</link>
      <description><![CDATA[我希望我的应用程序的用户检索一些要标记的数据。在第一个版本中，我没有实现锁定，因此多个用户可以同时访问相同的数据，因此第二个版本会覆盖第一个版本的标签。
我正在使用 python fastAPI sqlite 后端。
我最初想出了为标签添加“is-being-labelized”值的想法，以便下一个提议的数据不一样。我不喜欢它，因为我不知道如何处理用户在没有标记数据的情况下退出应用程序（或其他应用程序）的情况。目前，我最好的方法是添加一个包含检索时间时间戳的列，并实现一个逻辑，假设检索后 30 秒，我们检查标签是否不再是 None。如果它仍然是 None （意味着该人没有标记），我们删除时间戳的值。我也不完全高兴，因为它不能处理人们冥想然后回来标记数据的情况。
您有更好的建议吗？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78497797/handle-database-locks-in-python-for-my-labeling-app</guid>
      <pubDate>Fri, 17 May 2024 20:19:31 GMT</pubDate>
    </item>
    <item>
      <title>我应该在 GitHub 上分享我的自我项目吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78484980/should-i-share-my-self-projects-on-github</link>
      <description><![CDATA[我一直在考虑是否应该在 GitHub 上分享我自己的项目。这些项目主要涉及机器学习，重点关注回归和分类任务。虽然我已经付出了努力，但我不确定是否值得清理代码并上传它。展示这些小项目实际上是否有益，或者最终只是浪费时间？]]></description>
      <guid>https://stackoverflow.com/questions/78484980/should-i-share-my-self-projects-on-github</guid>
      <pubDate>Wed, 15 May 2024 15:26:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 VAE 减少和重建 CNN 模型参数</title>
      <link>https://stackoverflow.com/questions/78457309/reducing-and-reconstruction-cnn-model-parameters-using-a-vae</link>
      <description><![CDATA[假设我有一个带有 2 个 Conv2D 层的简单 CNN 模型，我在图像数据集上训练了这个模型，我将把这个 CNN 模型的参数输入到 VAE（作为编码器的输入）中，首先将其参数减少为嵌入空间（Z 或 VAE 的潜在空间）。然后，我想使用 VAE 解码器的输出重建 CNN 参数（及其原始尺寸）。
我不知道如何在 PyTorch 中实现这一点，并将经过训练的 CNN 参数输入到 VAE 模型的编码器输入中，最后将参数向量重建为 CNN 模型参数。
提前致谢！
这是 CNN 模型：
类 Net(nn.Module):
    def __init__(自身):
        超级（网络，自我）.__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def 前向（自身，x）：
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, 训练=self.training)
        x = self.fc2(x)
        返回 F.log_softmax(x)

下面的代码用于VAE：
类 VAE(nn.Module):
    def __init__(self, image_channels=1, h_dim=1024, z_dim=32):
        超级（VAE，自我）.__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),
            ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, 步幅=2),
            ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, 步长=2),
            ReLU(),
            nn.Conv2d(128, 256, kernel_size=4, 步长=2),
            ReLU(),
            展平()
        ）
        
        self.fc1 = nn.Linear(h_dim, z_dim)
        self.fc2 = nn.Linear(h_dim, z_dim)
        self.fc3 = nn.Linear(z_dim, h_dim)
        
        self.decoder = nn.Sequential(
            展开（），
            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),
            ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=5, 步长=2),
            ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size=6, 步长=2),
            ReLU(),
            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),
            nn.Sigmoid(),
        ）
        
    def 重新参数化(self, mu, logvar):
        std = logvar.mul(0.5).exp_()
        # 返回 torch.normal(mu, std)
        esp = torch.randn(*mu.size())
        z = mu + std * esp
        返回 z
    
    def 瓶颈（自身，h）：
        mu, logvar = self.fc1(h), self.fc2(h)
        z = self.reparameterize(mu, logvar)
        返回 z、mu、logvar

    def 编码（自身，x）：
        h = self.encoder(x)
        z, mu, logvar = self.bottleneck(h)
        返回 z、mu、logvar

    def 解码（自身，z）：
        z = self.fc3(z)
        z = self.decoder(z)
        返回 z

    def 前向（自身，x）：
        z、mu、logvar = self.encode(x)
        z = self.decode(z)
        返回 z、mu、logvar
]]></description>
      <guid>https://stackoverflow.com/questions/78457309/reducing-and-reconstruction-cnn-model-parameters-using-a-vae</guid>
      <pubDate>Thu, 09 May 2024 22:59:50 GMT</pubDate>
    </item>
    <item>
      <title>在 CUDA GPU 上运行 Pytorch 量化模型</title>
      <link>https://stackoverflow.com/questions/69718379/running-pytorch-quantized-model-on-cuda-gpu</link>
      <description><![CDATA[我很困惑是否可以在 CUDA 上运行 int8 量化模型，或者只能使用 fakequantise 在 CUDA 上训练量化模型以部署在另一个后端（例如 CPU）上。
我想使用实际的 int8 指令而不是 FakeQuantized float32 指令在 CUDA 上运行模型，并享受效率提升。奇怪的是，Pytorch 文档对此没有具体说明。如果可以使用不同的框架（例如 TensorFlow）在 CUDA 上运行量化模型，我很想知道。
这是准备量化模型的代码（使用训练后量化）。该模型是带有 nn.Conv2d 和 nn.LeakyRelu 以及 nn.MaxPool 模块的普通 CNN：
model_fp = torch.load(models_dir+net_file)

model_to_quant = copy.deepcopy(model_fp)
model_to_quant.eval()
model_to_quant = quantize_fx.fuse_fx(model_to_quant)

qconfig_dict = {“”: torch.quantization.get_default_qconfig(&#39;qnnpack&#39;)}

model_prepped = quantize_fx.prepare_fx(model_to_quant, qconfig_dict)
model_prepped.eval()
model_prepped.to(device=&#39;cuda:0&#39;)

train_data = ImageDataset(img_dir, train_data_csv, &#39;cuda:0&#39;)
train_loader = DataLoader(train_data,batch_size=32,shuffle=True,pin_memory=True)

对于 i，枚举（train_loader）中的（输入，_）：
    如果我&gt; 1：中断
    print(&#39;batch&#39;, i+1, end=&#39;\r&#39;)
    输入 = input.to(&#39;cuda:0&#39;)
    model_prepped（输入）

这实际上量化了模型：
model_quantized = quantize_fx.convert_fx(model_prepped)
model_quantized.eval()

这是在 CUDA 上运行量化模型的尝试，并引发 NotImplementedError，当我在 CPU 上运行它时，它工作正常：
model_quantized = model_quantized.to(&#39;cuda:0&#39;)
对于 train_loader 中的 i、_：
    输入 = input.to(&#39;cuda:0&#39;)
    输出= model_quantized（输入）
    打印（输出，输出形状）
    休息

这是错误：
回溯（最近一次调用最后一次）：
  文件“/home/adam/Desktop/thesis/Ship Detector/quantization.py”，第54行，在&lt;模块&gt;中。
    输出= model_quantized（输入）
  文件“/home/adam/.local/lib/python3.9/site-packages/torch/fx/graph_module.py”，第 513 行，位于wrapped_call 中
    引发 e.with_traceback(无)
NotImplementedError：无法使用来自“QuantizedCUDA”后端的参数运行“quantized::conv2d.new”。
这可能是因为该后端不存在该运算符，或者在选择性/自定义构建过程中省略了该运算符（如果使用自定义构建）。
如果您是在移动设备上使用 PyTorch 的 Facebook 员工，请访问 https://fburl.com/ptmfixes 了解可能的解决方案。
“quantized::conv2d.new”仅适用于以下后端：[QuantizedCPU、BackendSelect、Named、ADInplaceOrView、AutogradOther、AutogradCPU、AutogradCUDA、AutogradXLA、UNKNOWN_TENSOR_TYPE_ID、AutogradMLC、Tracer、Autocast、Batched、VmapMode]。
]]></description>
      <guid>https://stackoverflow.com/questions/69718379/running-pytorch-quantized-model-on-cuda-gpu</guid>
      <pubDate>Tue, 26 Oct 2021 06:30:58 GMT</pubDate>
    </item>
    <item>
      <title>ARFaceAnchor如何确定顶点索引？</title>
      <link>https://stackoverflow.com/questions/63188856/how-arfaceanchor-determine-to-vertices-indices</link>
      <description><![CDATA[我使用过 ARKit 人脸追踪。通过使用它，我可以获得人脸特征点
x=faceAnchor.geometry.vertices[638][0]
  
y=faceAnchor.geometry.vertices[638][1]

z=faceAnchor.geometry.vertices[638][2]

我想到了一些简单的问题。
这就是“ARFaceAnchor 如何确定顶点索引？”
我搜索了许多网址（apple，stackoverflow），但我找不到真相。
我以为dlib之类的，机器学习用的？
我在哪里可以找到有关该信息（如何计算面向量指数）？
ARKit的人脸顶点比ARcore更准确
使用什么不同的方法？]]></description>
      <guid>https://stackoverflow.com/questions/63188856/how-arfaceanchor-determine-to-vertices-indices</guid>
      <pubDate>Fri, 31 Jul 2020 09:04:35 GMT</pubDate>
    </item>
    <item>
      <title>Kmeans 算法的特征缩放</title>
      <link>https://stackoverflow.com/questions/57507584/feature-scaling-for-kmeans-algorithm</link>
      <description><![CDATA[我知道下定义的 KMeans 算法需要特征缩放
sklearn.cluster.KMeans
我的问题是，在使用 KMeans 之前是否需要手动完成，或者 KMeans 会自动执行特征缩放？如果是自动的，请告诉我它在 KMeans 算法中指定的位置，因为我无法在此处的文档中找到它：
https://scikit-learn.org/stable /modules/ generated/sklearn.cluster.KMeans.html
顺便说一句，人们说 Kmeans 本身负责特征缩放。]]></description>
      <guid>https://stackoverflow.com/questions/57507584/feature-scaling-for-kmeans-algorithm</guid>
      <pubDate>Thu, 15 Aug 2019 09:27:46 GMT</pubDate>
    </item>
    <item>
      <title>F1 分数 vs ROC AUC</title>
      <link>https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc</link>
      <description><![CDATA[我有以下 2 个不同案例的 F1 和 AUC 分数

&lt;块引用&gt;
  模型 1：精度：85.11 召回率：99.04 F1：91.55 AUC：69.94
模型 2：精度：85.1 召回率：98.73 F1：91.41 AUC：71.69

我的问题的主要动机是正确预测正例，即减少假负例（FN）。我应该使用 F1 分数并选择模型 1 还是使用 AUC 并选择模型 2。谢谢 ]]></description>
      <guid>https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc</guid>
      <pubDate>Thu, 25 May 2017 04:14:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 NLP 和 python 从文档中提取特定内容，例如姓名或出生日期？ [关闭]</title>
      <link>https://stackoverflow.com/questions/37967282/how-to-extract-specific-content-like-name-or-dob-from-a-document-using-nlp-and</link>
      <description><![CDATA[我想从文档（例如简历）中提取非常具体的内容，如姓名、地址和出生日期。假设我有 1000 份这样的文档，我想使用机器学习和自然语言处理来自动化它。最好是 Python。
我该怎么做？或者我从哪里开始？
更新：我知道 NER，但我希望从文档中提取非常具体的信息，这些信息可以加载到 excel 或其他东西中。
示例：从项目报告中，我想提取项目的主题、团队成员姓名和任期。]]></description>
      <guid>https://stackoverflow.com/questions/37967282/how-to-extract-specific-content-like-name-or-dob-from-a-document-using-nlp-and</guid>
      <pubDate>Wed, 22 Jun 2016 11:52:43 GMT</pubDate>
    </item>
    </channel>
</rss>