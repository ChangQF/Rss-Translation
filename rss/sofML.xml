<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 23 Nov 2024 03:26:17 GMT</lastBuildDate>
    <item>
      <title>InvalidArgumentError：在使用单个图像输入进行预测时，TensorFlow 模型中只有一个输入大小可以为 -1，而不能同时为 0 和 1</title>
      <link>https://stackoverflow.com/questions/79215834/invalidargumenterror-only-one-input-size-may-be-1-not-both-0-and-1-in-tensorf</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79215834/invalidargumenterror-only-one-input-size-may-be-1-not-both-0-and-1-in-tensorf</guid>
      <pubDate>Fri, 22 Nov 2024 16:34:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么以及如何在 ML 预处理中使用数据管道而不是仅仅使用 Pandas [关闭]</title>
      <link>https://stackoverflow.com/questions/79215565/why-and-how-to-use-data-pipelines-compared-to-just-pandas-in-preprocessing-for-m</link>
      <description><![CDATA[我们正在使用 SKlearn Pipelines。老实说，我还没有完全理解它们的意义。当我预处理数据时，我发现使用 pandas 更容易、更灵活。我可以随时在笔记本中直接检查 DataFrame，而使用 Pipelines 似乎并不那么简单。
此外，对于将字符串转换为整数等简单任务，使用自定义转换器和 ColumnTransformers（具有针对不同列的多个管道）通常感觉有点过度。
所以，我很好奇——更有经验的人如何处理这个问题？您是否坚持在大型项目中使用管道，还是将 pandas 混合用于更简单的任务，并仅使用管道进行扩展等操作？
从使用 pandas 进行探索和准备开始，然后在明确需要什么后将其全部形式化为管道是否有意义？]]></description>
      <guid>https://stackoverflow.com/questions/79215565/why-and-how-to-use-data-pipelines-compared-to-just-pandas-in-preprocessing-for-m</guid>
      <pubDate>Fri, 22 Nov 2024 15:15:51 GMT</pubDate>
    </item>
    <item>
      <title>PointPillars 模型调试请求</title>
      <link>https://stackoverflow.com/questions/79214060/pointpillars-model-debug-request</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79214060/pointpillars-model-debug-request</guid>
      <pubDate>Fri, 22 Nov 2024 07:44:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么在我的案例中，逻辑回归在二值图像分割的准确度上优于 CNN？[关闭]</title>
      <link>https://stackoverflow.com/questions/79213671/why-does-logistic-regression-outperform-cnn-in-accuracy-for-binary-image-segment</link>
      <description><![CDATA[我是机器学习的新手，我正在研究一个二值图像分割任务，我需要根据背景对物体（杯子）进行分类。我创建了一个彩色输入图像数据集（JPEG，在 0 和 1 之间归一化）和相应的二值蒙版（灰度图像，归一化为 0 和 1）。
我使用 TensorFlow 实现了一个 CNN 模型，并将其性能与逻辑回归模型 (scikit-learn) 进行了比较。令人惊讶的是，逻辑回归模型实现了比 CNN（83%）更高的准确率（97%）并产生了更好的蒙版。我很困惑，因为 CNN 通常更适合与图像相关的任务。
我已经使用 TensorFlow 构建了我的 CNN 模型，如下所示：
def create_cnn_model(learning_rate = 0.0001, filters=64, kernel_size=(3, 3),depth=3):
model = Sequential()
# 初始 Conv 和 Pool 层
model.add(Conv2D(filters, kernel_size=kernel_size,activation=&#39;relu&#39;,padding=&#39;same&#39;,input_shape=(Resolution, Resolution, 3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
# 根据深度添加 Conv-Pool 层
for i in range(1,depth):
model.add(Conv2D(filters * (2 ** i), kernel_size=kernel_size,activation=&#39;relu&#39;, padding=&#39;same&#39;))
# 仅当尺寸大于 1x1 时才添加 MaxPooling
model.add(MaxPooling2D(pool_size=(2, 2)))
# 检查空间维度是否达到 1x1，如果为真则停止
if (Resolution // (2 ** (i+1))) &lt; 2:
break
# UpSampling 和 Conv 层以恢复图像尺寸
for i in range(depth, 1, -1):
model.add(Conv2D(filters * (2 ** i), kernel_size=kernel_size,activation=&#39;relu&#39;, padding=&#39;same&#39;))
model.add(UpSampling2D(size=(2, 2)))
if (Resolution // (2 ** (i+1))) &lt; 2：
break 
model.add(UpSampling2D(size=(2, 2)))
model.add(Conv2D(1, kernel_size=(1, 1),activation=&#39;sigmoid&#39;,padding=&#39;same&#39;)) # 具有 1 个通道的输出层
# 编译模型
model.compile(optimizer=Adam(learning_rate = learning_rate),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
return model

使用上述模型，我的准确率约为 83%
然后我尝试将我的模型与机器学习模型 Logistic 回归进行比较：
model = LogisticRegression(max_iter=1000, random_state=42).fit(X_train, y_train)

使用此模型，我的准确率达到 97%（并且预测的掩码比 CNN 预测的掩码更好）
我使用的训练数据是定制的：
-输入图像（马克杯）为 jpeg 彩色，以 RGB 格式导入并进行预处理（在 0&amp;1 之间标准化），然后发送到模型
-掩码图像（马克杯）为黑白灰度（白色为物体，黑色为背景），以 RGB 格式导入并进行预处理（标准化为 0&amp; 1)，然后发送到模型。
-有 240 张图像和相同数量的掩码
-使用的图像是马克杯，几乎没有背景
预期：由于 CNN 能够捕捉空间特征，其表现优于逻辑回归。
结果：逻辑回归实现了更好的准确性和掩码质量，这似乎违反直觉。
我怀疑问题可能与数据集、模型架构或预处理步骤的简单性有关。
有人可以帮忙解释为什么逻辑回归在这种情况下可能优于 CNN，以及我如何改进我的 CNN 以完成这项任务？]]></description>
      <guid>https://stackoverflow.com/questions/79213671/why-does-logistic-regression-outperform-cnn-in-accuracy-for-binary-image-segment</guid>
      <pubDate>Fri, 22 Nov 2024 04:50:14 GMT</pubDate>
    </item>
    <item>
      <title>预测管道收购日期的模型</title>
      <link>https://stackoverflow.com/questions/79213087/model-to-predict-acquisition-date-for-pipeline</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79213087/model-to-predict-acquisition-date-for-pipeline</guid>
      <pubDate>Thu, 21 Nov 2024 22:25:24 GMT</pubDate>
    </item>
    <item>
      <title>如何修复尝试通过 Azure Ai Studio 项目访问 VS 代码时出现的访问问题</title>
      <link>https://stackoverflow.com/questions/79212964/how-to-fix-access-issues-when-trying-to-access-vs-code-via-azure-ai-studio-proje</link>
      <description><![CDATA[我们从 Windows 机器访问 Azure AI Studio Compute 时遇到一个问题。当计算准备就绪并尝试通过 VScode Desktop 访问它时，我们会看到以下错误消息：在门户上

这是我从 Vscode 获得的扩展：

此问题仅发生在 Windows 计算机上，从 Mac 计算机访问时不会发生此问题。 Compute 实例已启动并正在运行，但尝试打开 vscode 时出现错误。对此有任何解决方案吗？
]]></description>
      <guid>https://stackoverflow.com/questions/79212964/how-to-fix-access-issues-when-trying-to-access-vs-code-via-azure-ai-studio-proje</guid>
      <pubDate>Thu, 21 Nov 2024 21:17:20 GMT</pubDate>
    </item>
    <item>
      <title>在使用 dataloader 测试数据集时，我们应该设置 shuffle=true 吗？或者这无所谓？</title>
      <link>https://stackoverflow.com/questions/79212687/in-testing-dataset-using-dataloader-should-we-set-shuffle-true-or-it-doesnt-m</link>
      <description><![CDATA[我有一个自定义数据集（披萨、寿司和牛排的图片）。
我正在使用 torch DataLoader 来处理它，现在在编写测试数据加载器自定义时，我们应该设置 shuffle=true 还是这无关紧要？？
我还没有看到区别，只是问一般情况。]]></description>
      <guid>https://stackoverflow.com/questions/79212687/in-testing-dataset-using-dataloader-should-we-set-shuffle-true-or-it-doesnt-m</guid>
      <pubDate>Thu, 21 Nov 2024 19:33:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 ML 模型在不同的运行中获得不同的性能？</title>
      <link>https://stackoverflow.com/questions/79212471/why-do-i-get-different-performance-on-different-runs-on-my-ml-model</link>
      <description><![CDATA[我正在使用 snowpark 训练 ml 模型（Xgboost 和 LightGbm），但每次运行后我都会得到不同的指标值（AUC、平均精度），因此永远不知道哪个是我最好的模型。
我尝试在笔记本的开头设置一个全局变量 random_seed = 42，并将其放在我的欠采样函数和模型的初始化中：
 if model_type == &#39;xgboost&#39;:
model = XGBClassifier(
random_state=random_seed,
input_cols=feature_cols,
label_cols=target_col,
output_cols=[&#39;PREDICTION&#39;],
passthrough_cols=[&#39;INDIVIDUAL_SK&#39;, &#39;DATE_MONTH&#39;],
**hyperparameters
)

elif model_type == &#39;lightgbm&#39;:
model = LGBMClassifier（
random_state=random_seed，
input_cols=feature_cols，
label_cols=target_col，
output_cols=[&#39;PREDICTION&#39;]，
passthrough_cols=[&#39;INDIVIDUAL_SK&#39;，&#39;DATE_MONTH&#39;]，
**超参数

)

def undersample_majority_class（df）：

df_with_seniority = df.with_column（“years_since”，（F.col（&#39;TIME_SINCE_FIRST_LEAD&#39;）/12）。cast（&#39;int&#39;））

df_with_random = df_with_seniority.with_column（&#39;random_order&#39;，F.random（seed=random_seed））
window_spec = Window.partition_by(&quot;INDIVIDUAL_SK&quot;).order_by(F.col(&#39;random_order&#39;).asc())
df_ranked = df_with_random.with_column(&quot;month_rank&quot;, F.row_number().over(window_spec)
)

df_majority = df_ranked.filter(F.col(&quot;CONVERSION_INDICATOR&quot;) == 0)
df_majority_sampled = df_majority.filter(((F.col(&quot;years_since&quot;) &gt; 10) &amp; (F.col(&quot;month_rank&quot;) == 1)) |
((F.col(&quot;years_since&quot;) &lt;= 10) &amp; (F.col(&quot;month_rank&quot;) &lt;= 2))
)

df_majority_sampled = df_majority_sampled.drop(&#39;years_since&#39;,&#39;month_rank&#39;,&#39;random_order&#39; )
df_minority = df.filter(F.col(&quot;CONVERSION_INDICATOR&quot;) == 1)
df_balanced = df_majority_sampled.union_all(df_minority)

return df_balanced

我不知道该怎么做才能解决这个问题。

]]></description>
      <guid>https://stackoverflow.com/questions/79212471/why-do-i-get-different-performance-on-different-runs-on-my-ml-model</guid>
      <pubDate>Thu, 21 Nov 2024 18:16:23 GMT</pubDate>
    </item>
    <item>
      <title>为什么 gamma=0 的二元焦点交叉熵总是会产生 nan 损失？[关闭]</title>
      <link>https://stackoverflow.com/questions/79207979/why-does-binary-focal-cross-entropy-with-gamma-0-always-make-nan-loss</link>
      <description><![CDATA[我正在训练一个 U-Net 来对我们的实验图像进行二值化。但前景通常没有得到很好的体现，换句话说，我有类别不平衡，网络学习得不好。我一直在使用 BinaryCrossEntropy 作为损失函数。所以，我明白解决这个问题的一个简单方法是定义一个自定义的损失函数，为每个类别赋予权重。然而，我尝试使用 BinaryFocalCrossEntropy，它的表达式为（如果我理解得好的话）

所以，我的计划是将它与 gamma = 0 一起使用，这样我就可以通过调整 alpha 值来赋予类别权重。然而，我一直得到 nan 损失。它发生在几个批次之后的第一个时期内：（这里我使用 alpha = 0.75）

在这里我使用了 adam 优化器和 Learning_rate 1e-3。我注意到，如果我改用 1e-4，即使 nan 仍然出现，它也会再出现几个批次。你能帮我找出这是怎么回事，我该如何解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79207979/why-does-binary-focal-cross-entropy-with-gamma-0-always-make-nan-loss</guid>
      <pubDate>Wed, 20 Nov 2024 15:49:20 GMT</pubDate>
    </item>
    <item>
      <title>EOFError：输入不足 - Pickle</title>
      <link>https://stackoverflow.com/questions/79198550/eoferror-ran-out-of-input-pickle</link>
      <description><![CDATA[当我尝试以 pickle 格式加载机器学习模型以便它在 Web 应用程序中运行时，我收到此错误，路径显然是正确的，我的所有文件都位于同一文件夹中。但是，错误始终存在。

df = pd.DataFrame.from_dict(user_values, orient=&quot;index&quot;).T

with open(&quot;classifier.pkl&quot;, &quot;rb&quot;) as file:
loaded_model = pickle.load(file)

prediction = loaded_model.predict(df)[0][0]

else:
prediction = None

return render_template(&quot;index.html&quot;, prediction=prediction)

if __name__ == &quot;__main__&quot;:
app.run(debug=True)

]]></description>
      <guid>https://stackoverflow.com/questions/79198550/eoferror-ran-out-of-input-pickle</guid>
      <pubDate>Mon, 18 Nov 2024 02:30:01 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow InvalidArgumentError：ConcatOp 中的连接维度不匹配 - 形状不匹配</title>
      <link>https://stackoverflow.com/questions/79141216/tensorflow-invalidargumenterror-concatenation-dimension-mismatch-in-concatop</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79141216/tensorflow-invalidargumenterror-concatenation-dimension-mismatch-in-concatop</guid>
      <pubDate>Wed, 30 Oct 2024 13:00:13 GMT</pubDate>
    </item>
    <item>
      <title>sklearn“transform_output”在Flask应用程序上下文和请求上下文中的设置不同</title>
      <link>https://stackoverflow.com/questions/77907033/sklearn-transform-output-setting-different-in-flask-application-context-v-requ</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77907033/sklearn-transform-output-setting-different-in-flask-application-context-v-requ</guid>
      <pubDate>Tue, 30 Jan 2024 14:17:46 GMT</pubDate>
    </item>
    <item>
      <title>如何列出 OpenAI gym 中每个状态的可能后继状态？（严格针对普通 MDP）</title>
      <link>https://stackoverflow.com/questions/53690171/how-to-list-possible-successor-states-for-each-state-in-openai-gym-strictly-fo</link>
      <description><![CDATA[有没有办法遍历每个状态，强制环境进入该状态，然后采取一步，然后使用返回的“信息”字典来查看所有可能的后继状态？
或者有一种更简单的方法来恢复每个状态的所有可能的后继状态，也许隐藏在某个地方？
我在网上看到一个叫做 MuJoKo 或类似的东西有一个 set_state 函数，但我不想创建一个新的环境，我只想设置 openAi gym 已经提供的状态。
上下文：尝试实现拓扑顺序值迭代，这需要制作一个图表，其中每个状态都有一个指向任何动作可以转换到的任何状态的边。
我意识到显然在某些游戏中没有提供，但对于那些有它的游戏，有办法吗？
（除了运行游戏并采取我尚未采取的每一步的蛮力方法之外无论我处于什么状态，直到我到达所有状态并看到所有内容，这取决于游戏，可能需要很长时间）
这是我第一次使用 OpenAi gym，所以请尽可能详细地解释。例如，我不知道什么是 Wrappers。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/53690171/how-to-list-possible-successor-states-for-each-state-in-openai-gym-strictly-fo</guid>
      <pubDate>Sun, 09 Dec 2018 07:08:15 GMT</pubDate>
    </item>
    <item>
      <title>xgboost.plot_tree：二元特征解释</title>
      <link>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</link>
      <description><![CDATA[我构建了一个 XGBoost 模型，并试图检查各个估计量。作为参考，这是一个二元分类任务，具有离散和连续输入特征。输入特征矩阵是 scipy.sparse.csr_matrix。
然而，当我去检查一个单独的估计量时，我发现很难解释二元输入特征，例如下面的 f60150。最底部图表中的实值 f60150 很容易解释 - 其标准在该特征的预期范围内。但是，对二元特征 &lt;X&gt; &lt; -9.53674e-07 进行的比较没有意义。这些特征中的每一个要么是 1，要么是 0。-9.53674e-07 是一个非常小的负数，我想这只是 XGBoost 或其底层绘图库中的一些浮点特性，但当特征始终为正时使用这种比较是没有意义的。有人能帮我理解哪个方向（即 是、缺失 与 否 对应这些二进制特征节点的哪一侧为真/假吗？
这是一个可重现的示例：
import numpy as np
import scipy.sparse
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from xgboost import plot_tree, XGBClassifier
import matplotlib.pyplot as plt

def booleanize_csr_matrix(mat):
&#39;&#39;&#39; 将具有正整数元素的稀疏矩阵转换为 1 &#39;&#39;&#39;
nnz_inds = mat.nonzero()
keep = np.where(mat.data &gt; 0)[0]
n_keep = len(keep)
result = scipy.sparse.csr_matrix(
(np.ones(n_keep), (nnz_inds[0][keep], nnz_inds[1][keep])),
shape=mat.shape
)
返回结果

### 设置数据集
res = fetch_20newsgroups()

text = res.data
outcome = res.target

### 使用 CountVectorizer 的默认参数创建初始计数矩阵
vec = CountVectorizer()
X = vec.fit_transform(text)

# 是否“布尔化”输入矩阵
booleanize = True

# 是否在“布尔化”之后将数据类型转换为与 `vec.fit_transform(text)` 返回的内容相匹配
to_int = True

如果 booleanize 和 to_int:
X = booleanize_csr_matrix(X)
X = X.astype(np.int64)

# 使其成为二元分类问题
y = np.where(outcome == 1, 1, 0)

# 随机状态确保我们能够一致地比较树及其特征
model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

将 booleanize 和 to_int 设置为 True 并运行上述程序，将生成以下图表：

将 booleanize 和 to_int 设置为 False 并运行上述程序，将生成以下图表：

哎呀，即使我做了一个非常简单的例子，我也会得到“正确”的结果，无论 X 或 y 是整数还是浮点类型。
X = np.matrix(
[
[1,0],
[1,0],
[0,1],
[0,1],
[1,1],
[1,0],
[0,0],
[0,0],
[1,1],
[0,1]
]
)

y = np.array([1,0,0,0,1,1,1,0,1,1])

model = XGBClassifier(random_state=100)
model.fit(X, y)

plot_tree(model, rankdir=&#39;LR&#39;); plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/52314401/xgboost-plot-tree-binary-feature-interpretation</guid>
      <pubDate>Thu, 13 Sep 2018 13:06:06 GMT</pubDate>
    </item>
    <item>
      <title>神经网络收敛至零输出</title>
      <link>https://stackoverflow.com/questions/44213659/neural-network-converging-to-zero-output</link>
      <description><![CDATA[我正在尝试训练这个神经网络来对一些数据进行预测。
我在一个小的数据集（大约 100 条记录）上尝试了它，它工作得很好。然后我插入了新的数据集，我发现 NN 收敛到 0 输出，误差大约收敛到正例数与总例数之比。
我的数据集由是/否特征（1.0/0.0）组成，基本事实也是是/否。
我的假设：

1) 存在输出为 0 的局部最小值（但我尝试了许多学习率和初始权重值，它似乎总是收敛到那里）

2) 我的权重更新是错误的（但在我看来很好）

3) 它只是一个输出缩放问题。我尝试缩放输出（即输出/最大值（输出）和输出/平均值（输出）），但结果并不好，如您在下面提供的代码中看到的那样。我应该以不同的方式缩放它吗？Softmax？ 
代码如下：
import pandas as pd
import numpy as np
import pickle
import random
from collections import defaultdict

alpha = 0.1
N_LAYERS = 10
N_ITER = 10
#N_FEATURES = 8
INIT_SCALE = 1.0

train = pd.read_csv(&quot;./data/prediction.csv&quot;)

y = train[&#39;y_true&#39;].as_matrix()
y = np.vstack(y).astype(float)
ytest = y[18000:]
y = y[:18000]

X = train.drop([&#39;y_true&#39;], axis = 1).as_matrix()
Xtest = X[18000:].astype(float)
X = X[:18000]

def tanh(x,deriv=False):
if(deriv==True):
return (1 - np.tanh(x)**2) * alpha
else:
return np.tanh(x)

def sigmoid(x,deriv=False):
if(deriv==True):
return x*(1-x)
else:
return 1/(1+np.exp(-x))

def relu(x,deriv=False):
if(deriv==True):
return 0.01 + 0.99*(x&gt;0)
else:
return 0.01*x + 0.99*x*(x&gt;0)

np.random.seed()

syn = defaultdict(np.array)

for i in range(N_LAYERS-1):
syn[i] = INIT_SCALE * np.random.random((len(X[0]),len(X[0]))) - INIT_SCALE/2
syn[N_LAYERS-1] = INIT_SCALE * np.random.random((len(X[0]),1)) - INIT_SCALE/2

l = defaultdict(np.array)

delta = defaultdict(np.array)

for j in xrange(N_ITER):
l[0] = X
for i in range(1,N_LAYERS+1):
l[i] = relu(np.dot(l[i-1],syn[i-1]))

error = (y - l[N_LAYERS])

e = np.mean(np.abs(error))
if (j% 1) == 0:
print &quot;\nIteration &quot; + str(j) + &quot; of &quot; + str(N_ITER)
print &quot;Error: &quot; + str(e)

delta[N_LAYERS] = error*relu(l[N_LAYERS],deriv=True) * alpha
for i in range(N_LAYERS-1,0,-1):
error = delta[i+1].dot(syn[i].T)
delta[i] = error*relu(l[i],deriv=True) * alpha

for i in range(N_LAYERS):
syn[i] += l[i].T.dot(delta[i+1])

pickle.dump(syn, open(&#39;neural_weights.pkl&#39;, &#39;wb&#39;))

# 使用 f1-measure 进行测试
# 召回率 = 真阳性 / (真阳性 + 假阴性)
# 准确率 = 真阳性 / (真阳性阳性 + 假阳性)

l[0] = Xtest
for i in range(1,N_LAYERS+1):
l[i] = relu(np.dot(l[i-1],syn[i-1]))

out = l[N_LAYERS]/max(l[N_LAYERS])

tp = float(0)
fp = float(0)
fn = float(0)
tn = float(0)

for i in l[N_LAYERS][:50]:
print i

for i in range(len(ytest)):
if out[i] &gt; 0.5 and ytest[i] == 1:
tp += 1
if out[i] &lt;= 0.5 and ytest[i] == 1:
fn += 1
if out[i] &gt; 0.5 且 ytest[i] == 0:
fp += 1
if out[i] &lt;= 0.5 且 ytest[i] == 0:
tn += 1

print &quot;tp: &quot; + str(tp)
print &quot;fp: &quot; + str(fp)
print &quot;tn: &quot; + str(tn)
print &quot;fn: &quot; + str(fn)

print &quot;\nprecision: &quot; + str(tp/(tp + fp))
print &quot;recall: &quot; + str(tp/(tp + fn))

f1 = 2 * tp /(2 * tp + fn + fp)
print &quot;\nf1-measure:&quot; + str(f1)

输出结果如下：
第 0 次迭代（共 10 次）
错误： 0.222500767998

10 次迭代中的第 1 次
错误：0.222500771157

10 次迭代中的第 2 次
错误：0.222500774321

10 次迭代中的第 3 次
错误：0.22250077749

10 次迭代中的第 4 次
错误：0.222500780663

10 次迭代中的第 5 次
错误：0.222500783841

10 次迭代中的第 6 次
错误：0.222500787024

10 次迭代中的第 7 次
错误：0.222500790212

10 次迭代中的第 8 次
错误：0.222500793405

10 次迭代中的第 9 次10
错误：0.222500796602

[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 4.62182626e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 4.62182626e-06]
[ 0.]
[ 0.]
[ 5.04501079e-10]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 5.04501079e-10]
[ 0.]
[ 0.]
[ 4.62182626e-06]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 1.31432294e-05]

tp：28.0
fp：119.0
tn： 5537.0
fn：1550.0

精度：0.190476190476
召回率：0.0177439797212

f1-measure：0.0324637681159
]]></description>
      <guid>https://stackoverflow.com/questions/44213659/neural-network-converging-to-zero-output</guid>
      <pubDate>Sat, 27 May 2017 06:21:36 GMT</pubDate>
    </item>
    </channel>
</rss>