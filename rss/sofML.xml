<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 13 Mar 2024 12:24:18 GMT</lastBuildDate>
    <item>
      <title>Unity ML Agent 无法学习基本 Agent</title>
      <link>https://stackoverflow.com/questions/78153122/unity-ml-agents-not-learning-with-basic-agent</link>
      <description><![CDATA[我有一个使用 MLAgents 的简单代理，我希望能够学习它。如果猜对了 1 则得 1 分，否则得 -1 分。它有 1 个离散分支，分支 0 的大小等于 2（因此它选择 0 或 1）。决策请求者的决策周期为 5，并且我已关闭“在决策之间采取操作”。然而，代理似乎永远不会进步（总是在大约 0 的摘要之间获得平均奖励）。即使走了 50,000 步。
我想知道我是否配置错误，这意味着代理无法学习。
这是我的代码：
使用Unity.MLAgents；
使用 Unity.MLAgents.Actuators；
使用 Unity.MLAgents.Sensors；

公共类 TestAgent ：代理
{
    公共覆盖无效 CollectObservations（VectorSensor 传感器）
    {
        传感器.AddObservation(0);
    }

    公共覆盖无效OnActionReceived（ActionBuffers操作）
    {
        if (actions.DiscreteActions[0] == 0)
            添加奖励(-1);
        别的
            添加奖励(1);
        结束情节（）；
    }
}


这是我的点冻结：
&lt;前&gt;&lt;代码&gt;absl-py==2.1.0
属性==23.2.0
cattrs==1.5.0
证书==2024.2.2
字符集标准化器==3.3.2
云泡菜==3.0.0
色彩==0.4.6
文件锁==3.9.0
fsspec==2024.2.0
grpcio==1.48.2
健身房==0.26.2
健身房通知==0.0.8
h5py==3.10.0
拥抱脸集线器==0.21.4
idna==3.6
Jinja2==3.1.2
降价==3.5.2
标记安全==2.1.3
mlagents @ 文件:///C:/Program%20Files%20%28x86%29/Unity%20Projects/.../ml-agents
mlagents-envs @ file:///C:/Program%20Files%20%28x86%29/Unity%20Projects/.../ml-agents-envs
mpmath==1.3.0
网络x==3.2.1
numpy==1.23.0
onnx==1.12.0
包装==24.0
宠物动物园==1.15.0
枕头==10.2.0
协议缓冲区==3.19.6
pypiwin32==223
pywin32==306
PyYAML==6.0.1
请求==2.31.0
六==1.16.0
sympy==1.12
张量板==2.16.2
张量板数据服务器==0.7.2
火炬==2.2.1+cu121
tqdm==4.66.2
打字扩展==4.8.0
urllib3==2.2.1
武器==3.0.1

这是我的日志：
&lt;前&gt;&lt;代码&gt; ┐ ╖
        ╓╖╬│╡││╬╖╖
    ╓╖╬│││││┘╬│││││╬╖
 ╖╬│││││╬╜╙╬│││││╖╖╗╗╗
 ╬╬╬╬╖││╦╖ ╖╬│╗╣╣╣╬╟╣╣╬╟╣╣╣╜╜╜╟╣╣
 ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬╟╣╣╬╟╣╣╣╒╣╣╖╗╣╣ ╣╗╣╣╣╣╣╣╣ ╣╣╟╣╣╖╣╣╣
 ╬╬╬╬┐╙╬╬╬╬│╓╣╣╣╝╜╫╣╣╣╬╟╣╣╬╟╣╣╣╟╣╣╣╙╙╣╣╣ ╣╣╣╙╟╣╣╜╙╫╣ ╣╟╣╣
 ╬╬╬╬┐╙╬╬╣╣╫╣╣╣╬╟╣╣╬╟╣╣╣╟╣╣╬╣╣╣╣╣╣╟╣╣╣╣ ╣┌╣╣╜
 ╬╬╬╜╬╬╣╣╙╝╣╣╬╙╣╣╣╗╖╓╗╣╣╣╜╟╣╣╬╣╣╣╣╣╣╟╣╣ ╦╓╣╣╣╣╣
 ╙╓╦╖╬╬╣╣╙╝╣╣╣╣╝╜╘╝╝╜╝╝╝╝╝╝╙╣╣╣
   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝╫╣╣╣╣
      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜
          ╙╬╬╬╣╣╣╜
             ╙

 版本信息：
  毫升代理：1.0.0，
  ml-agents-envs：1.0.0，
  通讯器API：1.5.0，
  PyTorch：2.2.1+cu121
C:\Users\\miniconda3\envs\mlagents\lib\site-packages\torch\__init__.py:696: UserWarning: torch.set_default_tensor_type() 从 PyTorch 2.1 开始已弃用，请使用 torch.set_default_dtype( ) 和 torch.set_default_device() 作为替代方案。 （在 ..\torch\csrc\tensor\python_tensor.cpp:453 内部触发。）
  _C._set_default_tensor_type(t)
[信息] 在端口 5004 上监听。按 Unity 编辑器中的“播放”按钮开始训练。
[INFO] 连接到Unity环境，包版本为2.0.1，通信版本为1.5.0
[INFO] 连接新大脑：Test?team=0
[INFO] 行为名称测试的超参数：
        教练类型：ppo
        超参数：
          批量大小：1024
          缓冲区大小：10240
          学习率：0.0003
          贝塔值：0.005
          厄普西隆：0.2
          羊羔肉：0.95
          纪元数：3
          共享评论家：错误
          Learning_rate_schedule：线性
          beta_schedule：线性
          epsilon_schedule：线性
        检查点间隔：500000
        网络设置：
          标准化：假
          隐藏单元：128
          层数：2
          vis_encode_type：简单
          内存：无
          goal_conditioning_type：超级
          确定性：错误
        奖励信号：
          外在：
            伽玛：0.99
            强度：1.0
            网络设置：
              标准化：假
              隐藏单元：128
              层数：2
              vis_encode_type：简单
              内存：无
              goal_conditioning_type：超级
              确定性：错误
        初始化路径：无
        保持检查点：5
        Even_checkpoints：假
        最大步数：500000
        时间范围：64
        摘要频率：1000
        螺纹：假
        自我游戏：无
        行为克隆：无

我尝试过增加奖励、更改决策周期值、打开和关闭“在决策之间采取行动”标志。]]></description>
      <guid>https://stackoverflow.com/questions/78153122/unity-ml-agents-not-learning-with-basic-agent</guid>
      <pubDate>Wed, 13 Mar 2024 10:46:44 GMT</pubDate>
    </item>
    <item>
      <title>在python中创建线性回归模型的问题</title>
      <link>https://stackoverflow.com/questions/78152862/problem-with-creating-a-linear-regression-model-in-python</link>
      <description><![CDATA[我有一个数据库，其中包含一个城市的多个属性：
该数据库中保存了各种属性（约 19,000 个）。每个房产都有一些特征，例如：销售价格、房产面积、浴室数量、建造年份、上市天数......
我是机器学习算法编程的新手，希望首先编写一个简单的线性回归模型，该模型可以根据其他数据预测该房产的上市天数。
数据保存在Excel中。
这就是我所做的：
&lt;前&gt;&lt;代码&gt;
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.metrics 导入mean_squared_error, r2_score
从 sklearn.model_selection 导入 train_test_split
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd
导入请求


模型=线性回归()
data=pd.read_excel(r“C:\Users....”)

X=np.array(data.drop([“daysOnMarket”], axis=1))
Y=np.array(数据[“daysOnMarket”])
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2)
model.fit(x_train, y_train)
y_pred=模型.预测(x_test)
print(model.score(x_test, y_test))
打印（均方误差（y_test，y_pred））


现在让我觉得我做错了的是，我得到的分数是 0.9999852324248868，均方误差是 0.07659752059595726
现在我不明白这是一个过度拟合问题还是我只是在编程中做错了什么。
谁能帮我找出问题出在哪里吗？
这是我的数据示例：
]]></description>
      <guid>https://stackoverflow.com/questions/78152862/problem-with-creating-a-linear-regression-model-in-python</guid>
      <pubDate>Wed, 13 Mar 2024 10:06:17 GMT</pubDate>
    </item>
    <item>
      <title>我们可以使用不同的 yaml 文件来微调新数据集的模型，但与之前的数据集具有相同的类吗？</title>
      <link>https://stackoverflow.com/questions/78152669/can-we-use-different-yaml-file-for-fine-tuning-a-model-with-newdataset-but-has-s</link>
      <description><![CDATA[我使用我的自定义数据集训练了 YOLOv8 模型，其中使用了 ultralytics 和 pytorch。我具有与第一个数据集相同的数据集，但图像中对象的位置在新数据集中发生了更改。现在，如果我在不从第一个数据集 YAML 文件中获取任何引用的情况下进行注释，它会起作用吗？
第一个经过训练的 YAML 文件如下：
训练：/home/user/Code/newdata/YOLODataset/images/train/
val：/home/user/Code/newdata/YOLODataset/images/val/
测试：/home/user/Code/newdata/YOLODataset/images/test/
数控：8
名称：[“货架”、“托盘”、“条形码”、“box-botte”、“mbox”、“cobj”、“糖浆”、“半”]

微调 yaml 文件：
训练：/home/user/Code/newdata/YOLODataset/images/train/
val：/home/user/Code/newdata/YOLODataset/images/val/
测试：/home/user/Code/newdata/YOLODataset/images/test/
数控：7
名称：[“货架”、“托盘”、“条形码”、“盒子瓶”、“mbox”、“糖浆”、“半个”]

第二个 YAML 文件适合微调自定义预训练模型吗？我第一次使用 520 张图像和 25 个 epoch 进行训练。现在我应该微调模型多少张图像和纪元？
提前致谢
我使用 Pytorch 和 Ultralytics。我可以按照上面的YAML文件进行微调吗？]]></description>
      <guid>https://stackoverflow.com/questions/78152669/can-we-use-different-yaml-file-for-fine-tuning-a-model-with-newdataset-but-has-s</guid>
      <pubDate>Wed, 13 Mar 2024 09:41:41 GMT</pubDate>
    </item>
    <item>
      <title>验证错误：无法实例化 GPT4AllEmbeddings 模型</title>
      <link>https://stackoverflow.com/questions/78152636/validation-error-unable-to-instantiate-gpt4allembeddings-model</link>
      <description><![CDATA[我在尝试创建 GPT4AllEmbeddings 实例时遇到问题。但是我不断收到以下错误
[15] 中的单元格，第 1 行
----&gt; 1 vectorstore = Chroma.from_documents(文档 = 分割, 嵌入 = GPT4AllEmbeddings())
2 检索器 = vectorstore.as_retriever(search_type = &#39;相似度&#39;, search_kwargs = {&#39;k&#39;:6})
3retrieve_docs =retrieve.get_relevant_documents(“你是什么？”)
文件 ~\anaconda3\Lib\site-packages\pydantic\main.py:341，位于 pydantic.main.BaseModel.init()
ValidationError：GPT4AllEmbeddings 出现 1 个验证错误
根
无法实例化模型（type=value_error）
这是相关的代码片段
vectorstore = Chroma.from_documents(documents = splits, embeddings = GPT4AllEmbeddings())
检索器 = vectorstore.as_retriever(search_type = &#39;相似度&#39;, search_kwargs = {&#39;k&#39;:6})
retrieved_docs =retrieve.get_relevant_documents(“什么是Young Decade？”)
打印（len（检索文档））
打印（retrieve_docs[0].page_content）

有人可以提供有关如何解决此错误的指导吗？]]></description>
      <guid>https://stackoverflow.com/questions/78152636/validation-error-unable-to-instantiate-gpt4allembeddings-model</guid>
      <pubDate>Wed, 13 Mar 2024 09:36:07 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试通过训练迭代计算余弦相似度时，如何选择模型权重</title>
      <link>https://stackoverflow.com/questions/78152246/how-do-i-select-model-weights-when-i-try-to-calculate-cosine-similarity-though-t</link>
      <description><![CDATA[我正在尝试计算“t”步骤的权重值和“t-1”步骤的权重值之间的余弦相似度。
我正在使用 LSTM 网络 &amp; 2FC层。
我想检查训练期间的体重差异。
但是如果我想检查这些事情，我想知道我是只选择 LSTM 层的权重还是全部（LSTM，2FC 权重）或最终层的权重（FC_2 权重）
谢谢。
请帮助我。我不知道如何细化余弦相似条件下选择图层的最佳解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78152246/how-do-i-select-model-weights-when-i-try-to-calculate-cosine-similarity-though-t</guid>
      <pubDate>Wed, 13 Mar 2024 08:34:22 GMT</pubDate>
    </item>
    <item>
      <title>简单的 Pytorch LSTM 模型无法学习反事实创建</title>
      <link>https://stackoverflow.com/questions/78151232/simple-pytorch-lstm-model-not-learning-for-counterfactual-creation</link>
      <description><![CDATA[我正在 Pytorch 中构建 LSTM，以创建水流时间序列的反事实预测。为了预测第 i 天的水流 ($\hat{y}_i$)，我的特征集 $X_i$ 包含当天以及过去几天的天气数据。其他模型已经很好地学习了信号（即 LGBM），即使使用惩罚回归，特征集也可以解释水流的大部分变化。
尽管如此，LSTM 似乎并没有学到太多东西，而是学到了平均值。这与我在学习过程中传递数据的方式有关吗？
（丢弃超参数，这些超参数是随机初始化的。稍后我将使用 optuna 对其进行优化）
非常感谢您的帮助！
代码：
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

类 LSTMModel(nn.Module):
    def __init__(自身、输入大小、隐藏大小、层数、输出大小):
        super(LSTMModel, self).__init__()
        self.hidden_​​size = 隐藏大小
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size,hidden_​​size,num_layers,batch_first=True)
        self.fc = nn.Linear(隐藏大小, 输出大小)

    def 前向（自身，x）：
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_​​size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_​​size).to(device)
        
        # print(f“lstm 调用开始处的 x 大小：{x.size()}”)

        out, _ = self.lstm(x, (h0, c0))

        # print(f&quot;前向调用开始时的 x 大小：{x.size()}&quot;)
        out = self.fc(out[:, -1, :]) # 获取最后一个时间步的输出
        返回

dta =“../ml_dataset_Jan24.dta”

数据 = pd.read_stata(dta)

data[&#39;datetime&#39;] = data.apply(lambda x: pd.to_datetime(f&quot;{int(x[&#39;年&#39;])}-{int(x[&#39;月&#39;])}-{int(x[&#39;天&#39;])}&quot;，格式=&#39;%Y-%m-%d&#39;)，轴=1)
data.set_index(&#39;datetime&#39;, drop=True, inplace=True)
data.drop(columns=[&#39;年&#39;,&#39;月&#39;,&#39;日&#39;], inplace=True)

# 创建特征和目标集
X_train = data.loc[(data.index&gt;&#39;1990-12-31&#39;) &amp; (data.index&lt;&#39;2006-01-01&#39;), data.columns != &#39;level_cs&#39;].values
y_train = data.loc[(data.index&gt;&#39;1990-12-31&#39;) &amp; (data.index&lt;&#39;2006-01-01&#39;), &#39;level_cs&#39;].values


X_train = np.array(X_train)
X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
y_train = np.array(y_train)
y_train = y_train.reshape(y_train.shape[0], 1, 1)


X = torch.from_numpy(X_train).float()
y = torch.from_numpy(y_train).float()


# 定义超参数
input_size = 1332 # 特征数量
隐藏大小 = 128
层数 = 5
输出大小 = 1

# 定义训练设备（CPU 或 GPU）
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

# 实例化模型
模型 = LSTMModel(input_size,hidden_​​size,num_layers,output_size).to(device)

# 定义损失函数和优化器
标准 = nn.MSELoss()
优化器 = torch.optim.Adam(model.parameters(), lr=0.001)

# 训练模型
纪元数 = 500
对于范围内的纪元（num_epochs）：
    模型.train()
    优化器.zero_grad()

    # print(f&#39;X 尺寸: &#39;, X.size())
    输出 = 模型(X.to(设备))
    损失 = 标准（输出，y.to（设备））
    loss.backward()
    优化器.step()
    
    print(f&#39;Epoch [{epoch+1}/{num_epochs}], 损失: {loss.item():.4f}&#39;)

# 将模型设置为评估模式
模型.eval()

# 推论
# 使用 torch.no_grad():
# 预测 = 模型(X_test.to(设备))

# print(“样本预测：”, 预测)`

Y_test 预测基本上只是平均值
我批量传递了数据，Y_train 的时间戳与 X_train 中的时间戳相同，因为我想使用日期“i”中的数据来预测水流“i”。该功能集还包括过去的天气数据，我不确定是否应该如此，或者该数据的重要性是否会被记忆机制保留]]></description>
      <guid>https://stackoverflow.com/questions/78151232/simple-pytorch-lstm-model-not-learning-for-counterfactual-creation</guid>
      <pubDate>Wed, 13 Mar 2024 04:21:51 GMT</pubDate>
    </item>
    <item>
      <title>如何在我的 Streamlit 应用程序中实现模型</title>
      <link>https://stackoverflow.com/questions/78151205/how-do-i-implement-a-model-in-my-streamlit-application</link>
      <description><![CDATA[我已经克隆了 GitHub 代表并用它来生成实时语音克隆。它正在工作..我正在 cmd 中执行命令..以获得我的声音的实时语音克隆。现在我希望模型能够训练我的声音，并且我想将它用于我的项目，该项目应该位于流式应用程序中。首先，我不知道模型在用我的声音提供 tts 时是否会自行训练。但它没有使用我的语音数据集进行训练过程。相反，我给出了一个录音和来自 if 的录音。它为输入文本生成语音。其次，如果它经过训练，我如何使用该模型。
我尝试检查模型是否会保存合成语音，并且尝试实现该模型，但我不断收到错误消息，提示未找到模块。但我将它添加到我的路径中。它仍然显示错误。我很困惑]]></description>
      <guid>https://stackoverflow.com/questions/78151205/how-do-i-implement-a-model-in-my-streamlit-application</guid>
      <pubDate>Wed, 13 Mar 2024 04:12:33 GMT</pubDate>
    </item>
    <item>
      <title>如何检测图片是否上下颠倒？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78151191/how-to-detect-if-a-picture-is-upside-down</link>
      <description><![CDATA[我的团队正在开发一项在 AWS 上运行并从 S3 存储桶中提取图片的服务。这些图片是来自监控摄像头视频流的帧，导致它们没有 Exif 元数据，而且大多数时候，图片中不会有任何人。
该服务需要“读取”这些图片并区分其中是否有颠倒的情况，以便我们确定这些相机是否正常工作。最初我以为可以使用AWS Rekognition，但是看了文档，似乎无法满足要求。
所以，我想知道是否有任何 AWS 服务或库可以完成此任务并部署在 Lambda 或 ECS 中？]]></description>
      <guid>https://stackoverflow.com/questions/78151191/how-to-detect-if-a-picture-is-upside-down</guid>
      <pubDate>Wed, 13 Mar 2024 04:08:00 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试使用数据集包创建数据集时，出现“无法转换，因为列名称不匹配”错误</title>
      <link>https://stackoverflow.com/questions/78151170/im-getting-couldnt-cast-because-column-names-dont-match-error-while-i-was-t</link>
      <description><![CDATA[DataFrame 结构
上图显示了我的数据结构。
from sklearn.model_selection import train_test_split
从数据集中导入特征、ClassLabel、值、数据集、DatasetDict

df_train, df_tmp = train_test_split(
        movie_df,stratify=movie_df[“标签”], test_size=0.2)

df_val, df_test = train_test_split(
        df_tmp,stratify=df_tmp[“标签”], test_size=0.5)

ds_features = Features({“text”: Value(“string”), “label”: ClassLabel(names=labels)})

数据集 = DatasetDict({
    “火车”：Dataset.from_pandas(df_train.reset_index(drop=True),features=ds_features),
    “有效”：Dataset.from_pandas(df_val.reset_index(drop=True),features=ds_features),
    “测试”：Dataset.from_pandas(df_test.reset_index(drop=True),features=ds_features)})

数据集

这段代码给了我一个值错误，如下所示：
错误
错误
我期待类似的东西，但不具有相同的值：
DatasetDict({
    火车：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：13267
    })
    有效：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：1658
    })
    测试：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：1659
    })
})

谁能告诉我我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78151170/im-getting-couldnt-cast-because-column-names-dont-match-error-while-i-was-t</guid>
      <pubDate>Wed, 13 Mar 2024 04:00:13 GMT</pubDate>
    </item>
    <item>
      <title>python darts 中的 RNN 训练指标</title>
      <link>https://stackoverflow.com/questions/78144820/rnn-training-metrics-in-python-darts</link>
      <description><![CDATA[我目前正在使用 python darts 训练 RNNModel。为了比较不同的训练模型，我想从 fit 方法中提取 train_loss 和 val_loss 。我该怎么做？我读过一些有关指标集合的内容，但不知道如何使用它。
这是我当前的代码
from darts.models import RNNModel
从 darts 导入 TimeSeries

train = # 训练数据为 TimeSeries
模型 = RNNModel(模型 =“LSTM”, input_chunk_length=self.past_samples)
模型.fit(火车)

训练过程中损失会显示在控制台中，但我不知道如何访问它。
到目前为止，我已尝试在线查找任何文档，并向 bing chat 和 ChatGPT 寻求帮助。然而他们告诉我使用不存在的model.history.history[“loss”]]]></description>
      <guid>https://stackoverflow.com/questions/78144820/rnn-training-metrics-in-python-darts</guid>
      <pubDate>Tue, 12 Mar 2024 05:29:49 GMT</pubDate>
    </item>
    <item>
      <title>预测分位数与梯度增强回归相交</title>
      <link>https://stackoverflow.com/questions/78140825/prediction-quantiles-intersect-from-gradient-boosted-regression</link>
      <description><![CDATA[我正在尝试构建一个回归模型，该模型接收各种类型的产品和市场信息，对数据进行转换，并在一天中定期预测产品的价格。我希望能够预测分位数置信带，以帮助我的团队根据模型的预测做出决策。我遵循了 scikit-learn 文档中的示例&lt; /code&gt;，但我发现这些回归器产生的预测有时会相交，例如有时P50预测大于P75预测或小于P25预测。
查看此处突出显示的图表中的错误。
即使使用更宽的频段（包括 P95 和 P05），这个问题仍然存在。尽管数据集比我正在使用的数据集简单得多，但我已经能够毫无问题地重现链接的示例。
下面的代码代表了该问题的可重现示例，并带有生成的数据集：
# 导入和定义
将 numpy 导入为 np
从 sklearn.ensemble 导入 HistGradientBoostingRegressor
从 sklearn.model_selection 导入 train_test_split
从 sklearn.datasets 导入 make_regression

X,y = make_regression(
    n_样本 = 14000,
    n_特征 = 39,
    n_目标 = 1
）

# 进行测试/训练分割和模型字典
X_train, X_test, y_train, y_test = train_test_split(
    X、Y、测试大小=0.1
）
型号={}

for alpha in [0.25, 0.5, 0.75]: # HistGradientBoostingRegressor model - Early_stopping = False 有助于解决问题，但不能解决问题
    hgbr = HistGradientBoostingRegressor(
        max_iter=100, # 通常在1000左右，例如减少
        损失=“分位数”，
        分位数 = 阿尔法，
    ）

    # 将最佳模型添加到字典中
    models[f&quot;P{alpha*100:02.0f}&quot;] = hgbr.fit(X_train, y_train.ravel())


任何帮助或建议将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78140825/prediction-quantiles-intersect-from-gradient-boosted-regression</guid>
      <pubDate>Mon, 11 Mar 2024 13:05:06 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 Huggingface MT5 模型中执行批量编码时会得到不同的嵌入？</title>
      <link>https://stackoverflow.com/questions/78139855/why-do-i-get-different-embeddings-when-i-perform-batch-encoding-in-huggingface-m</link>
      <description><![CDATA[我正在尝试使用 HuggingFace 的 mt5-base 模型对一些文本进行编码。我使用的模型如下所示
从转换器导入 MT5EncoderModel、AutoTokenizer

模型 = MT5EncoderModel.from_pretrained(“google/mt5-base”)
tokenizer = AutoTokenizer.from_pretrained(“google/mt5-base”)

def get_t5_embeddings(文本):
    last_hidden_​​state = model(input_ids=tokenizer(texts, return_tensors=“pt”, padding=True).input_ids).last_hidden_​​state
    pooled_sentence = torch.max(last_hidden_​​state, 暗淡=1)
    返回 pooled_sentence[0].detach().numpy()

当我注意到相同的文本与其自身的余弦相似度分数较低时，我正在做一些实验。我做了一些挖掘，意识到如果我批量进行编码，模型会返回非常不同的嵌入。为了验证这一点，我运行了一个小实验，逐步生成 Hello 的嵌入和 10 个 Hello 的列表。并检查列表中 Hello 和第一个 Hello 的嵌入（两者应该相同）。
对于范围 (1, 10) 内的 i：
    print(i, (get_t5_embeddings([“你好”])[0] == get_t5_embeddings([“你好”]*i)[0]).sum())

这将返回嵌入中相互匹配的值的数量。
结果是这样的：
&lt;前&gt;&lt;代码&gt;1 768
2 768
3 768
4 768
5 768
6 768
7 768
8 27
9 27

每次运行它时，如果批量大小超过 768，就会出现不匹配情况。
为什么我会得到不同的嵌入以及如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78139855/why-do-i-get-different-embeddings-when-i-perform-batch-encoding-in-huggingface-m</guid>
      <pubDate>Mon, 11 Mar 2024 10:14:53 GMT</pubDate>
    </item>
    <item>
      <title>无监督自动编码器产生输出维度 - 不同大小数据集的批次数</title>
      <link>https://stackoverflow.com/questions/78107646/unsupervised-autoencoder-produce-output-dimensions-number-of-batches-for-diffe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78107646/unsupervised-autoencoder-produce-output-dimensions-number-of-batches-for-diffe</guid>
      <pubDate>Tue, 05 Mar 2024 12:12:37 GMT</pubDate>
    </item>
    <item>
      <title>为多标签 ViTForImageClassification 准备数据集</title>
      <link>https://stackoverflow.com/questions/77967230/prepare-a-dataset-for-multilabel-vitforimageclassification</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77967230/prepare-a-dataset-for-multilabel-vitforimageclassification</guid>
      <pubDate>Fri, 09 Feb 2024 09:52:59 GMT</pubDate>
    </item>
    <item>
      <title>xgboost 错误：dtypes 必须是 int、float 或 bool，但它们是</title>
      <link>https://stackoverflow.com/questions/67977854/xgboost-error-dtypes-must-be-int-float-or-bool-but-they-are</link>
      <description><![CDATA[我有一个经过预处理的房地产相关DataFrame，具有以下数据类型：
&lt;前&gt;&lt;代码&gt;&gt;&gt; df.dtype

OHE_Cat__x0_单户住宅 Int64
OHE_Cat__x0_联排别墅 Int64
OHE_Cat__x1_1 Int64
OHE_Cat__x1_2 Int64
邮政编码 Int64
价格 Int64
床位 Int64
浴室 Float64
平方英尺 Int64
批量大小 Int64
建造年份 Int64
纬度 Float64
经度 Float64
年龄 Int64
时间 VAR UNIX Int64
is_Pandemic 布尔值
is_CY 布尔值
数据类型：对象

但是，当尝试拟合我的 XGBRegressor 时，我收到以下错误：
ValueError：数据的 DataFrame.dtypes 必须是 int、float 或 bool。
            没想到字段 OHE_Cat__x0_Single Family Residential、OHE_Cat__x0_Townhouse、OHE_Cat__x1_1、OHE_Cat__x1_2、ZIP OR POSTAL CODE、BEDS、BATHS、SQUARE FEET、LOT SIZE、YEAR BUILT、LATITUDE、LONGITUDE、Age、TIME VAR UNIX、is_Pandemic、is_CY 中的数据类型

奇怪的是，当我使用pd.get_dummmies时，这个错误并不存在，但在切换到sklearn.preprocessing.OneHotEncoder后，我开始收到它，所以我在想我的下面的代码有错误吗？
oneHotE = OneHotEncoder(drop=&#39;first&#39;)
变压器 = ColumnTransformer([(&#39;OHE_Cat&#39;, oneHotE, 分类)], 剩余 = &#39;直通&#39;)
    
df = pd.DataFrame(transformer.fit_transform(df), columns=transformer.get_feature_names()).convert_dtypes()

在train_test_split之前，我已尝试对数据集进行以下更改：
df = df.apply(pd.to_numeric, axis=1)
df = df.apply(pd.to_numeric, axis=0)
df = df.convert_dtypes()
df = df.dropna()
df = df._get_numeric_data()

编辑：

RandomForestRegressor 运行正常，没有错误，但出现以下警告：

&lt;块引用&gt;
futurewarning：字节/字符串数组正在转换为十进制
如果 dtype=&#39;numeric&#39; 则为数字。此行为在 0.24 中已弃用，并且
将在 1.1 中删除（0.26 重命名）。请将您的数据转换为
明确地改为数值。


CatBoostRegressor 出现以下错误：无法将 FloatingArray 转换为 numpy.ndarray
]]></description>
      <guid>https://stackoverflow.com/questions/67977854/xgboost-error-dtypes-must-be-int-float-or-bool-but-they-are</guid>
      <pubDate>Mon, 14 Jun 2021 22:20:42 GMT</pubDate>
    </item>
    </channel>
</rss>