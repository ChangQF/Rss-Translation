<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 02 May 2024 12:26:31 GMT</lastBuildDate>
    <item>
      <title>ConvLSTM 模型的数据预处理过程中遇到问题</title>
      <link>https://stackoverflow.com/questions/78418879/having-trouble-during-preprocessing-of-data-for-a-convlstm-model</link>
      <description><![CDATA[我有 10 年（2014-24）的 GHRSST 数据，我将其分为两部分：训练（2014-2021）和测试（2021-2024）数据集。训练数据集大小为 4.18GB，测试数据集大小为 1.98GB。我正在尝试构建一个 ConvLSTM 模型来预测未来几天的 SST 数据，但是我似乎无法通过预处理阶段。我正在尝试在 google colab 上执行以下代码：
导入火炬
将 numpy 导入为 np
进口达斯克

# NetCDF 文件的路径
dataset_path = &#39;/content/drive/MyDrive/train_dataset_10.nc&#39;
# 使用 dask chunks 打开数据集
ds = xr.open_mfdataset(dataset_path, chunks={&#39;时间&#39;: 1, &#39;纬度&#39;: 50, &#39;经度&#39;: 50})

# 提取SST数据；假设变量名为“analysis_sst”，并使用计算将其转换为 numpy 来加载数据
sst_data = ds[&#39;analysisd_sst&#39;].compute() # 这将确保数据加载到内存中

上面的代码需要永远执行，而且似乎没有完成（1 小时以上）。我对以下代码也有同样的问题。它要么永远不会停止执行，要么使用过多的 CPU（尽管 GPU 处于打开状态）并且内核崩溃：
# 清除 CUDA 中所有未使用的内存
torch.cuda.empty_cache()

# 将数据转换为张量，出于兼容性原因确保它首先在 CPU 上
sst_tensor = torch.tensor(sst_data.values, dtype=torch.float32)

# 如果有可用的 GPU，则将张量传输到 GPU
如果 torch.cuda.is_available():
    sst_tensor = sst_tensor.to(&#39;cuda&#39;)

print(&quot;SST 张量的形状：&quot;, sst_tensor.shape)

defscale_data_gpu(data_tensor,batch_size):
    scaled_data = torch.full_like(data_tensor, float(&#39;nan&#39;)) # 为填充 NaN 的缩放数据初始化张量

    # 批量处理数据
    对于范围内的开始（0，data_tensor.shape [0]，batch_size）：
        结束 = 开始 + 批次大小
        批处理 = data_tensor[开始:结束]

        # 为有效（非 NaN）数据点创建掩码
        valid_mask = ~torch.isnan(batch)

        if valid_mask.any(): # 确保至少有一些有效数据
            data_min = torch.min(batch[valid_mask])
            data_max = torch.max(batch[valid_mask])

            # 缩放批次，但仅在有效的情况下应用
            batch_scaled = (batch - data_min) / (data_max - data_min)
            scaled_data[开始:结束][valid_mask] = batch_scaled[valid_mask]

    返回缩放数据

# 在 GPU 上应用缩放，仅考虑有效（非 NaN）值
使用 torch.no_grad()：
  sst_scaled_tensor = scale_adata_gpu（data_tensor = sst_tensor，batch_size = 64）

在上面的代码中，我试图掩盖标记为“NaN”的值因为它代表了我的数据集中的地形，然后缩放数据并批量处理它以准备训练。
我该如何进行这项工作？这个过程需要多长时间？有更有效的方法吗？
我尝试过使用 Dask，但没有成功，而且 GEE 不支持 CNN。我对机器学习还很陌生，非常感谢您提供的任何帮助。我不知道此后如何继续。]]></description>
      <guid>https://stackoverflow.com/questions/78418879/having-trouble-during-preprocessing-of-data-for-a-convlstm-model</guid>
      <pubDate>Thu, 02 May 2024 11:34:30 GMT</pubDate>
    </item>
    <item>
      <title>如何将我的 svm 模型部署到我的 flutter 应用程序中？</title>
      <link>https://stackoverflow.com/questions/78418707/how-can-i-deploy-my-svm-model-to-my-flutter-app</link>
      <description><![CDATA[将我的 SVM 模型部署到 Flutter 应用的最佳方法是什么？
我无法将我的代码重新实现为 TenserFlow Lite，因为准确率会下降到 15%。我的模型获取音频文件并对其进行分析。我的模型的输出是文本，我希望它按原样显示。我对使用 hugging face 一无所知。对我来说最好的方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78418707/how-can-i-deploy-my-svm-model-to-my-flutter-app</guid>
      <pubDate>Thu, 02 May 2024 11:04:31 GMT</pubDate>
    </item>
    <item>
      <title>如何将关系数据库集成到数据科学项目中？</title>
      <link>https://stackoverflow.com/questions/78418612/how-to-integrate-a-relational-database-into-a-data-science-project</link>
      <description><![CDATA[我是一名数据科学家，主要使用 CSV 文件进行数据分析，但我现在正在探索在我的项目中使用关系数据库。我想了解将关系数据库集成到我的工作流程中的最佳实践。
我应该如何将数据从关系数据库（例如 PostgreSQL、MySQL）导入到我的数据科学环境（例如 Python、R）中？我应该直接在数据库中执行连接和探索性数据分析，还是应该将数据导出到 CSV 文件，然后继续分析？
我过去主要使用 CSV 文件，但现在我正在着手一个现实世界的数据科学项目，我需要在其中使用关系数据库。不过，我对此还比较陌生，正在寻求有关如何有效地将数据库集成到我的工作流程中的指导。]]></description>
      <guid>https://stackoverflow.com/questions/78418612/how-to-integrate-a-relational-database-into-a-data-science-project</guid>
      <pubDate>Thu, 02 May 2024 10:49:01 GMT</pubDate>
    </item>
    <item>
      <title>IndexError：streamlit 中的列表索引超出范围</title>
      <link>https://stackoverflow.com/questions/78418486/indexerror-list-index-out-of-range-in-streamlit</link>
      <description><![CDATA[因此，我正在尝试构建一个 Streamlit RAG 应用程序，该应用程序从 url 中提取信息并从中学习，然后用户可以向模型询问与 url 中的文章相关的问题，模型将提供合适的答案。
我在我的笔记本上执行了此操作，它工作得很好，只是在我的 Streamlit 应用程序中遇到 IndexError: list index out of range 错误，我将 GoogleGenerativeAIEmbeddings 与 FAISS 结合使用。
这是代码块
embeddings = GoogleGenerativeAIEmbeddings(model = &#39;models/embedding-001&#39;)
矢量索引= FAISS.from_documents（文档，嵌入）

这是来自 stresmlit 应用程序的回溯
IndexError：列表索引超出范围
追溯：
文件“C:\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py”，第 584 行，位于 _run_script
    exec（代码，模块.__dict__）
文件“C:\Users\owner\Desktop\Projects\nlp\main.py”，第 84 行，在  中
    vectorstore_openai = FAISS.from_documents（文档，嵌入）
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_core\vectorstores.py”，第 550 行，from_documents
    返回 cls.from_texts(文本、嵌入、元数据=元数据、**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_community\vectorstores\faiss.py”，第 931 行，from_texts
    返回 cls.__from(
           ^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_community\vectorstores\faiss.py”，第 888 行，位于 __from
    索引 = faiss.IndexFlatL2(len(embeddings[0]))
                                  ~~~~~~~~~~^^^

就像我上面说的，这在我的笔记本上完美运行，我很困惑为什么会发生这种情况]]></description>
      <guid>https://stackoverflow.com/questions/78418486/indexerror-list-index-out-of-range-in-streamlit</guid>
      <pubDate>Thu, 02 May 2024 10:25:27 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中累积损失/梯度时如何处理 BatchNorm？</title>
      <link>https://stackoverflow.com/questions/78418346/how-to-deal-with-batchnorm-when-accumulating-loss-gradients-in-pytorch</link>
      <description><![CDATA[由于硬件限制，我试图增加有效批次大小。为了做到这一点，我对几个小批次进行了 N 次前向传递，从而累积了损失。但是，众所周知，BatchNorm 具有跟踪均值和标准差的运行统计数据。对于较小的物理小批次大小（即 1），甚至无法重现较大批次大小的训练。如何处理这个问题？
我想到的是以下内容：“手动更新 BatchNorm 统计数据”、“禁用跟踪批次统计数据”、“减少动量”和“使用幽灵批次规范化”。但是，手动更新成本高昂，禁用批次统计跟踪只会禁用批次规范化的好处，当物理批次大小为 1 时，减少动量并不能解决问题，而幽灵批次规范化与我试图解决的问题相反。]]></description>
      <guid>https://stackoverflow.com/questions/78418346/how-to-deal-with-batchnorm-when-accumulating-loss-gradients-in-pytorch</guid>
      <pubDate>Thu, 02 May 2024 10:00:57 GMT</pubDate>
    </item>
    <item>
      <title>Seq2Seq 模型是否适用于拼写纠正？如果是的话，为什么我会弄错？</title>
      <link>https://stackoverflow.com/questions/78418175/does-seq2seq-model-work-for-spelling-correction-if-yes-why-i-am-getting-it-wro</link>
      <description><![CDATA[我正在使用 seq2seq 模型来预测或纠正产品名称的拼写，我确实有产品名称的数据集及其拼写错误和更正的版本（它们也包含一些特殊字符）。我已经在几个时期训练了这些数据并看到了一些输出。但我给用户输入，它没有按预期进行预测。
然后，在训练模型并使用此代码之后：
对于 range(1, 50) 中的 seq_index：
    input_seq = 编码器_input_data[seq_index : seq_index + 1]
    解码句子 = 解码序列(input_seq)
    打印（“-”）
    print(&quot;输入句子：&quot;, input_texts[seq_index]) #用char打印输入序列！
    print(&quot;解码后的句子：&quot;,decoded_sentence)

我得到了很好的输出，例如：
输入语句：Fluidic WorkCation
解码句子：Fluidic Worksation

输入语句：Li@uid Handler、Biomek FXp DuaO
解码句子：Liquid Handler、Biomek NXp Mult

然后，如果我尝试提供用户输入并让模型进行预测，我确实会得到一些像这样的文本
输入语句：系统
解码句子：“伽玛计数器/转子 - 水机系统，自动并行\n”

这与它所学到的知识相去甚远，但我使用了相同的编码器和解码器模型代码。我想先知道这个 seq2seq 模型是否适用于这些场景。]]></description>
      <guid>https://stackoverflow.com/questions/78418175/does-seq2seq-model-work-for-spelling-correction-if-yes-why-i-am-getting-it-wro</guid>
      <pubDate>Thu, 02 May 2024 09:32:40 GMT</pubDate>
    </item>
    <item>
      <title>如何在短时间内建立准确的数据集？</title>
      <link>https://stackoverflow.com/questions/78418098/how-can-i-build-an-accurate-dataset-in-a-short-span-of-time</link>
      <description><![CDATA[我们正在开发一款 iOS 应用，让用户可以发送可定制的数字卡片。用户可以从各种卡片模板中进行选择，输入自己的文本，并根据自己的喜好对卡片进行编辑。我们还有一项功能，用户可以提供短信，例如“妈妈生日快乐”，并收到文本的扩展版本，例如“祝我特别的母亲生日快乐！”我爱你，希望你度过愉快的一天。”
我正在研究如何实现这一目标，并计划使用自然语言处理 (NLP) 和 CoreML 创建一个模型。然而，我在为这个特定任务寻找合适的数据集时遇到了问题。因此，我有兴趣构建专门为此目的而定制的准确数据集。但是，我不确定从哪里可以获得必要的数据，或者是否有其他数据源可供快速使用。
如果您有任何见解或替代方法来实现此功能，请分享。]]></description>
      <guid>https://stackoverflow.com/questions/78418098/how-can-i-build-an-accurate-dataset-in-a-short-span-of-time</guid>
      <pubDate>Thu, 02 May 2024 09:18:54 GMT</pubDate>
    </item>
    <item>
      <title>如何解决此错误并让我的代码运行？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78417439/how-do-i-resolve-this-error-and-get-my-code-running</link>
      <description><![CDATA[找不到指定的模块。错误加载
“C:\Users\mechg\OneDrive\Desktop\modeltesting\env\Lib\site-packages\torch\lib\fbgemm.dll”
或其依赖项之一

当我运行依赖于 torch 的 Python 脚本时，这是我遇到的错误，到处搜索，但无法解决。
这是代码：
导入火炬
从 PIL 导入图像
从 Transformer 导入 AutoModelForCausalLM、AutoTokenizer

model_id =“qresearch/llama-3-vision-alpha-hf”
模型 = AutoModelForCausalLM.from_pretrained(
    model_id、trust_remote_code=True、torch_dtype=torch.float16
）

tokenizer = AutoTokenizer.from_pretrained(
    型号_id，
    use_fast=真，
）

image = Image.open(“照片.jpg”)

打印（
    分词器.解码(
        model.answer_question(image, “描述图像”, tokenizer),
        Skip_special_tokens=真，
    ）
）
]]></description>
      <guid>https://stackoverflow.com/questions/78417439/how-do-i-resolve-this-error-and-get-my-code-running</guid>
      <pubDate>Thu, 02 May 2024 07:09:45 GMT</pubDate>
    </item>
    <item>
      <title>为什么准确度有如此差异？</title>
      <link>https://stackoverflow.com/questions/78417156/why-such-difference-in-accuracy</link>
      <description><![CDATA[我正在使用 resnet 50 模型进行图像分类。运行模型几个时期后，以下是我得到的结果：
&lt;前&gt;&lt;代码&gt;纪元 8/20
损失：0.5705 - 准确度：0.8785 - val_loss：0.9226 - val_accuracy：0.8135

纪元 9/20
损失：0.5509 - 准确度：0.8780 - val_loss：0.9321 - val_accuracy：0.7979

获得的这些结果可以改进，但我们暂时保留这些结果。
我获得了我期望的值：
# 评估模型
test_loss, test_acc = model.evaluate(val_data_flow)
print(&quot;测试准确度：&quot;, test_acc)

＃ 结果：
测试精度：0.7978515625

但是这里的准确性很糟糕：
从sklearn.metrics导入classification_report


# 对测试数据集生成预测
预测 = model.predict(val_data_flow)

# 将预测转换为类标签
Predicted_labels = np.argmax(预测，轴=1)

# 从测试数据集中提取真实标签
true_labels = val_data_flow.labels

# 生成分类报告
class_names = list(val_data_flow.class_indices.keys())
打印（分类报告（真实标签，预测标签，目标名称=类名称））


 精确召回率 f1-score 支持

       1 0.33 0.38 0.36 330
       2 0.00 0.00 0.00 14
       3 0.17 0.17 0.17 133
       4 0.09 0.11 0.10 102
       5 0.00 0.00 0.00 5
       6 0.00 0.00 0.00 21
       7 0.12 0.12 0.12 133
       8 0.18 0.16 0.17 154
       9 0.18 0.13 0.15 132

精度 0.21 1024
宏观平均 0.12 0.12 0.12 1024
加权平均值 0.20 0.21 0.21 1024

什么可能导致这样的问题？或者是因为功能的工作方式不同？
我的数据集非常不平衡。]]></description>
      <guid>https://stackoverflow.com/questions/78417156/why-such-difference-in-accuracy</guid>
      <pubDate>Thu, 02 May 2024 06:02:21 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 上的推理困难</title>
      <link>https://stackoverflow.com/questions/78416324/difficulty-performing-inference-on-lstm</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78416324/difficulty-performing-inference-on-lstm</guid>
      <pubDate>Wed, 01 May 2024 23:08:48 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 - 神经网络</title>
      <link>https://stackoverflow.com/questions/78416266/machine-learning-neural-network</link>
      <description><![CDATA[我正在尝试创建一个具有一个隐藏层的神经网络，它将图像分为 12 个不同的类别。当我尝试使用梯度下降函数运行代码来开始训练模型时，代码根本不输出任何内容并移至下一个单元格。
definitialize_parameters(hidden_​​units):
    w1 = np.random.randn(hidden_​​units, 640 * 480 * 3) * 0.01
    b1 = np.zeros((hidden_​​units, 1))
    w2 = np.random.randn(12, 隐藏单元) * 0.01
    b2 = np.zeros((12, 1))
    返回 w1、b1、w2、b2

def ReLU(Z):
    返回 np.maximum(0, Z)

def softmax(Z):
    expZ = np.exp(Z)
    返回 expZ / np.sum(expZ, axis=0, keepdims=True)

defforward_propagation(w1, b1, w2, b2, X):
    z1 = np.dot(w1, X) + b1
    a1 = ReLU(z1)
    z2 = np.dot(w2, a1) + b2
    a2 = softmax(z2)
    返回 z1、a1、z2、a2

def onehotencoding(Y):
    one_hot_Y = np.zeros((Y.size, Y.max() + 1))
    one_hot_Y[np.arange(Y.size), Y] = 1
    one_hot_Y = one_hot_Y.T
    返回 one_hot_Y

def导数ReLU(Z):
    返回Z&gt; 0

def back_propagation(w2, a1, z1, a2, X, Y):
    m = Y 尺寸
    one_hot_Y = onehotencoding(Y)
    dz2 = a2 - one_hot_Y
    dw2 = 1 / m * np.dot(dz2, a1.T)
    db2 = 1 / m * np.sum(dz2, axis=1, keepdims=True)
    dz1 = np.dot(w2.T, dz2) *导数ReLU(z1)
    dw1 = 1 / m * np.dot(dz1, X.T)
    db1 = 1 / m * np.sum(dz1, axis=1, keepdims=True)
    返回 dw1、db1、dw2、db2

def update_parameters(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha):
    w1 = w1 - 阿尔法 * dw1
    b1 = b1 - 阿尔法 * db1
    w2 = w2 - 阿尔法 * dw2
    b2 = b2 - 阿尔法 * db2
    返回 w1、b1、w2、b2

此代码单元格在此结束，后面是此代码块。
def get_predictions(a2):
    返回 np.argmax(a2, 轴=0)

def get_accuracy(预测, Y):
    返回 np.sum(预测 == Y) / Y.size

defgradient_descent(X,Y,hidden_​​units,迭代,alpha):
    w1、b1、w2、b2 = 初始化参数（隐藏单元）
    对于范围内的 i（迭代）：
        z1, a1, z2, a2 = 前向传播(w1, b1, w2, b2, X)
        dw1, db1, dw2, db2 = 反向传播(w2, a1, z1, a2, X, Y)
        w1, b1, w2, b2 = update_parameters(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha)
        如果我％10==0：
            预测 = get_predictions(a2)
            准确度 = get_accuracy(预测, Y)
            print(&quot;迭代：&quot;, i)
            print(&quot;准确率：&quot;, 准确率)
    返回 w1、b1、w2、b2


这是开始训练的梯度下降函数。
w1, b1, w2, b2 = 梯度下降(X_train, Y_train, 500, 迭代=1000, alpha=0.1)
]]></description>
      <guid>https://stackoverflow.com/questions/78416266/machine-learning-neural-network</guid>
      <pubDate>Wed, 01 May 2024 22:43:52 GMT</pubDate>
    </item>
    <item>
      <title>我的 LightGBM 模型中实际上有多少棵树？</title>
      <link>https://stackoverflow.com/questions/78416214/how-many-trees-do-i-actually-have-in-my-lightgbm-model</link>
      <description><![CDATA[我的代码看起来像这样
clf = lgb.LGBMClassifier(max_深度=3，详细程度=-1，n_estimators=3)
clf.fit(train_data[特征],train_data[&#39;y&#39;],sample_weight=train_data[&#39;权重&#39;])
print (f“我有 {clf.n_estimators_} 估计器”)
图，ax = plt.subplots（nrows = 4，figsize =（50,36），sharex = True）
lgb.plot_tree(clf, tree_index=7, dpi=600, ax=ax[0]) # 为什么有第七棵树？
lgb.plot_tree(clf, tree_index=8, dpi=600, ax=ax[1]) # 为什么它有第 8 棵树？
#lgb.plot_tree(clf, tree_index=9, dpi=600, ax=ax[2]) # 崩溃
#lgb.plot_tree(clf, tree_index=10, dpi=600, ax=ax[3]) # 崩溃

令我惊讶的是，尽管有 n_estimators=3，但我似乎有 9 棵树？我如何实际设置树的数量，以及与之相关的，n_estimators 是做什么的？我读过文档，我以为是树的数量，但似乎是别的东西。
另外，我如何解释单独的树及其顺序 0、1、2 等。我了解随机森林，以及每棵树如何同等重要。在 boosting 中，第一棵树最重要，下一棵树的重要性要低得多，下一棵树的重要性要低得多。所以在我的脑海中，当我查看树形图时，我该如何“模拟” LightGBM 推理过程？]]></description>
      <guid>https://stackoverflow.com/questions/78416214/how-many-trees-do-i-actually-have-in-my-lightgbm-model</guid>
      <pubDate>Wed, 01 May 2024 22:24:21 GMT</pubDate>
    </item>
    <item>
      <title>当不打乱测试数据时，Torchmetrics 的准确性问题。为什么？</title>
      <link>https://stackoverflow.com/questions/78415660/torchmetrics-accuracy-issue-when-dont-shuffle-test-data-why</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78415660/torchmetrics-accuracy-issue-when-dont-shuffle-test-data-why</guid>
      <pubDate>Wed, 01 May 2024 19:45:02 GMT</pubDate>
    </item>
    <item>
      <title>如何基于掩码相乘矩阵并排除元素？</title>
      <link>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</guid>
      <pubDate>Mon, 29 Apr 2024 19:07:12 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：使用“bitsandbytes”8 位量化需要加速：“pip install accelerate”</title>
      <link>https://stackoverflow.com/questions/78040978/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</link>
      <description><![CDATA[我正在尝试使用开源数据集微调 llama2-13b-chat-hf。
我一直使用此模板，但现在收到此错误：
导入错误：使用bitsandbytes 8位量化需要加速：pip install Accelerate和最新版本的bitsandbytes：pip install -i https://pypi .org/simple/bitsandbytes
我安装了所需的所有软件包，这些是版本：
加速@git+https://github.com/huggingface/accelerate.git@97d2168e5953fe7373a06c69c02c5a00a84d5344
    位和字节==0.42.0
    数据集==2.17.1
    拥抱脸集线器==0.20.3
    佩夫特==0.8.2
    分词器==0.13.3
    火炬==2.1.0+cu118
    火炬音频==2.1.0+cu118
    火炬视觉==0.16.0+cu118
    变形金刚==4.30.0
    trl==0.7.11

有人知道这是不是版本问题吗？
你是如何解决这个问题的？
我尝试安装其他版本，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78040978/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</guid>
      <pubDate>Thu, 22 Feb 2024 12:37:11 GMT</pubDate>
    </item>
    </channel>
</rss>