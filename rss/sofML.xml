<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 22 May 2024 01:02:15 GMT</lastBuildDate>
    <item>
      <title>模型通过后的梯度检查点</title>
      <link>https://stackoverflow.com/questions/78514889/gradient-checkpointing-after-model-passing</link>
      <description><![CDATA[假设在默认设置中，我有两个 3d 张量作为输入，例如 batch_len x seq_len x ebb_size，我将其传递给类似模型，在输出上我再次有 2 个 3d 张量，然后我得到通过 einsum 计算 4d 张量，例如 torch.einsum(&#39;ijk, mnk -&gt; ijmn&#39;,first_tensor, secondary_tensor)，然后用最大值之和将其减少到 2d，获取分数并计算损失。问题是这个 4d 张量需要很多内存，而且我不能接受大批量大小，这就是为什么我想在 einsum 之前检查所有内容，然后计算第一个张量的前 k 批的 einsum，如 torch.einsum(&#39; ijk, mnk → ijmn,first_tensor[:k], secondary_tensor)，其中k ~first_tensor.shape[0] / 10，然后计算这些小张量的最大值之和，计算损失，从 GPU 内存中删除这个小的 4d 张量，对第二批第一个张量、第三批等进行相同的操作。最主要的是我想从 GPU 内存中删除所有 4d 张量。向后，我们将从检查点（模型传递的输出，但在 einsum 计算之前）向前推进并再次计算 4d 张量，但由于批处理，这个 4d 张量是一个小得多的张量，并且在每个批次中我们都会计算这个小的 4d 张量再次使用张量来计算梯度，然后再次将其从GPU内存中删除。在我看来，它运作良好。当然，由于检查点，它的运行速度较慢，但​​它使用的内存应该比具有大张量的默认 einsum 少得多。
但是我很难在 PyTorch 中实现它。我想典型的 torch.checkpoint 在这里是不够的。有人可以帮助我理解如何以正确的方式实现它，而不是破坏渐变]]></description>
      <guid>https://stackoverflow.com/questions/78514889/gradient-checkpointing-after-model-passing</guid>
      <pubDate>Wed, 22 May 2024 00:41:43 GMT</pubDate>
    </item>
    <item>
      <title>对数核和指数激活的非线性关系模型精度无法达到 100%</title>
      <link>https://stackoverflow.com/questions/78514097/model-accuracy-for-non-linear-relationship-with-logarithm-kernel-and-exponential</link>
      <description><![CDATA[我正在做一个项目，需要用神经网络来建立非线性关系模型。这个关系是 ( y = 3x_1^2x_2^3 )。网络设置如下：

预处理：输入的自然对数
网络设计：单层，一个神经元
激活函数：指数
损失函数：MAE（平均绝对误差）
优化器： Adam
迭代次数： 50
批次大小： 32

输入和预期输出：

输入：（[x1, x2]）
正确权重：（[2, 3]）
正确偏差：（\ln 3）

尽管这些设置，我无法达到 100% 的准确度。我尝试过随机初始化权重和偏差以及使用特定值。
代码如下：
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# 生成数据
x1 = np.random.randint(1, 21, size=(1000, 1))
x2 = np.random.randint(1, 21, size=(1000, 1))
y = 3 * (x1 ** 2) * (x2 ** 3)

# 预处理数据
log_x1 = np.log(x1)
log_x2 = np.log(x2)
log_inputs = np.hstack((log_x1, log_x2))

# 定义模型
model = Sequential()
model.add(Dense(1, input_dim=2,activation=&#39;exponential&#39;, kernel_initializer=&#39;ones&#39;,bias_initializer=&#39;zeros&#39;))

# 编译模型
model.compile(optimizer=Adam(learning_rate=0.01),loss=&#39;mae&#39;)

# 训练模型
model.fit(log_inputs, np.log(y),epochs=50,batch_size=32)

# 评估模型
test_x1 = np.array([[2],[4],[5]])
test_x2 = np.array([[3],[7],[19]])
test_inputs = np.hstack((np.log(test_x1), np.log(test_x2)))
predicted = model.predict(test_inputs)
print(np.exp(predicted))

有人对如何提高这个模型的准确性有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78514097/model-accuracy-for-non-linear-relationship-with-logarithm-kernel-and-exponential</guid>
      <pubDate>Tue, 21 May 2024 19:54:48 GMT</pubDate>
    </item>
    <item>
      <title>确定可最小化预测误差的最佳聚合级别</title>
      <link>https://stackoverflow.com/questions/78514089/determining-an-optimal-level-of-aggregation-that-would-minimize-prediction-error</link>
      <description><![CDATA[我正在寻找以最大化类别数量同时最小化分类错误的方式聚合预测结果的想法。
作为一个激励示例，假设我正在执行一项预测任务，以按流派对歌曲进行分类，并且有 6 种流派（来自下面的目标列）：

&lt;标题&gt;

类型（广泛）
类型（目标）


&lt;正文&gt;

流行音乐
独立流行音乐


流行音乐
超级流行


流行音乐
韩国流行音乐


摇滚
另类摇滚


摇滚
经典摇滚


摇滚
硬摇滚



该模型在识别前 4 个类别（独立流行、超级流行、韩国流行、另类摇滚）方面具有 100% 的准确率，但将大约 50% 的硬摇滚歌曲错误地分类为经典摇滚，将大约 20% 的经典摇滚歌曲错误地分类为硬摇滚摇滚。
基于此，我们可以想象通过几种方式聚合目标类型，从而减少分类错误。例如

2 个类别：流行、摇滚
4 个类别：独立流行音乐、超级流行音乐、韩国流行音乐、摇滚
5 个类别：独立流行音乐、超级流行音乐、韩国流行音乐、另类摇滚、其他摇滚

在本例中，我希望将目标类型聚合到这 5 个类别，保留尽可能多的类别，同时保持 100% 的准确性。
为了找到理想的聚合结构，我可以排列所有可能的聚合并计算 MSE。然而，考虑到我正在使用的类的数量，这在计算上是不可行的。所以，我想知道是否有一些相关文献可以阅读，以更好地理解如何解决这个问题，或者是否有人有想法。
谢谢您，如果这个问题太模糊，我们深表歉意。很高兴对其进行编辑以改进它！]]></description>
      <guid>https://stackoverflow.com/questions/78514089/determining-an-optimal-level-of-aggregation-that-would-minimize-prediction-error</guid>
      <pubDate>Tue, 21 May 2024 19:51:46 GMT</pubDate>
    </item>
    <item>
      <title>如何将ML模型部署到omnet ++中</title>
      <link>https://stackoverflow.com/questions/78513810/how-to-deploy-ml-model-into-omnet</link>
      <description><![CDATA[我用tensorflow和keras训练了一个深度学习模型来解决分类问题（ddos攻击），
现在我想将此模型部署到 omnet ++ 。我尝试使用 docker，但在 Windows 上效果不佳，所以现在在 omnet ++ 中部署 ml 模型的最简单方法是什么]]></description>
      <guid>https://stackoverflow.com/questions/78513810/how-to-deploy-ml-model-into-omnet</guid>
      <pubDate>Tue, 21 May 2024 18:43:13 GMT</pubDate>
    </item>
    <item>
      <title>联合概率分布如何帮助生成事物？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78513688/how-does-the-joint-probability-distribution-help-to-generate-things</link>
      <description><![CDATA[我试图理解判别模型和生成模型之间的区别。 Stack Overflow 上有用的答案之一在这里：生成算法和判别算法有什么区别？
在最上面的答案中（请参阅上面的链接），有一个简单的示例，其中只有四个 (x,y) 形式的数据点。答案的作者说了以下内容：分布 p(y|x) 是将给定示例 x 分类为类 y 的自然分布code&gt;，这就是为什么直接建模的算法被称为判别算法。生成算法模型p(x,y)，可以应用贝叶斯规则将其转化为p(y|x)，然后用于分类。然而，分布p(x,y)也可以用于其他目的。例如，您可以使用p(x,y)生成可能的(x,y)对。
我不太明白如何使用p(x,y)来生成可能的(x,y)对。我有兴趣查看使用联合概率分布 p(x,y) 生成的 (x,y) 对的示例？另外，为什么条件概率分布p(y|x)不能用来生成新的对？]]></description>
      <guid>https://stackoverflow.com/questions/78513688/how-does-the-joint-probability-distribution-help-to-generate-things</guid>
      <pubDate>Tue, 21 May 2024 18:13:38 GMT</pubDate>
    </item>
    <item>
      <title>Model.forecast() 并不预测下一个日期的数据，而是预测训练数据中已有的日期的数据</title>
      <link>https://stackoverflow.com/questions/78513216/model-forecast-is-not-showin-forecasting-the-data-for-the-next-date-rather-is</link>
      <description><![CDATA[Python 中的问题：model.forecast() 不显示下一个日期
我在 Python 中遇到 model.forecast() 函数的问题。具体来说，它不显示下一个日期，即我用来训练的 y_train 的最后一天的第二天。详细信息如下：
我正在从事一个时间序列预测项目。
我已将数据拆分为 y_train 和 y_test。
我正在使用 AutoReg 库 AutoReg。
这是我的代码的简化版本：
从 statsmodels.tsa.ar_model 导入 AutoReg

data = pd.DataFrame(df).set_index(“日期”)
data.index = pd.DatetimeIndex(data.index).to_period(&#39;D&#39;)
# 删除 nan 值
data.fillna(method=“ffill”, inplace=True)


series_data=data[“价格”] 


截止 = int(len(series_data)*0.9)
y_train = series_data.iloc[:截止]
y_test = series_data.iloc[截止：]

# 将自回归模型拟合到历史记录中，滞后=13
模型 = AutoReg(y_train, lags=13).fit()
# 预测下一个值
预测=模型.预测()

# 显示预测
打印（预测）

输出：2011-09-08 2.799414 
频率：D，数据类型：float64

y_test.head(1)
输出：日期2018-04-23 2.78
 

问题：
库输出不包含 y_train 中最后一个日期之后的下一个日期。相反，它显示
我尝试过的：

我已检查 y_train 中的日期格式正确且连续。
我尝试在 forecast() 函数中的步骤使用不同的值。

期望的结果：
我希望预测显示 y_train 中最后一天的第二天的预测值。
其他信息：

我正在使用 AutoReg 库。
print(forecast) 应该给我 Date2018-04-23 而不是 2011-09-08
]]></description>
      <guid>https://stackoverflow.com/questions/78513216/model-forecast-is-not-showin-forecasting-the-data-for-the-next-date-rather-is</guid>
      <pubDate>Tue, 21 May 2024 16:34:13 GMT</pubDate>
    </item>
    <item>
      <title>如何优化Python的多线程性能以实现实时机器学习预测？</title>
      <link>https://stackoverflow.com/questions/78512908/how-to-optimize-pythons-multithreaded-performance-for-real-time-machine-learnin</link>
      <description><![CDATA[我目前正在使用 Python 开发实时机器学习应用程序，并且面临着优化多线程性能以减少模型预测延迟的挑战。即使有高效的模型架构，线程方面似乎也是瓶颈。
这是我使用并发.futures.ThreadPoolExecutor 的 Python 代码的相关部分：
导入并发.futures
将 numpy 导入为 np

def 预测（模型，数据）：
    # 模拟预测
    返回模型.预测（数据）

def main():
    model = load_your_model() # 加载预训练模型的假设函数
    data = np.random.rand(100, 10) # 模拟输入的随机数据

    以并发.futures.ThreadPoolExecutor(max_workers=5) 作为执行器：
        futures = [executor.submit(predict, model, data[i]) for i in range(100)]
        results = [f.result() for f in future]

    print(&quot;预测结果：&quot;, results)

如果 __name__ == &#39;__main__&#39;:
    主要的（）

这种方法没有产生预期的吞吐量，我怀疑 GIL（全局解释器锁）可能会导致问题，尤其是 I/O 绑定任务与 CPU 绑定任务混合在一起时。
在这种情况下实现最大并发性和并行性的最佳实践是什么？
过渡到多处理或使用 asyncio 等库会显着提高性能吗？
是否有已知的 Python 机器学习或并发库或框架可以更有效地处理此类实时预测需求？
任何见解或替代方法建议将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78512908/how-to-optimize-pythons-multithreaded-performance-for-real-time-machine-learnin</guid>
      <pubDate>Tue, 21 May 2024 15:30:51 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 RTX 4090 训练 Mask RCNN？</title>
      <link>https://stackoverflow.com/questions/78511541/is-it-possible-to-train-mask-rcnn-with-a-rtx-4090</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78511541/is-it-possible-to-train-mask-rcnn-with-a-rtx-4090</guid>
      <pubDate>Tue, 21 May 2024 11:27:34 GMT</pubDate>
    </item>
    <item>
      <title>DecisionTreeRegressor 算法如何工作？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78510567/how-does-the-decisiontreeregressor-algorithm-work</link>
      <description><![CDATA[我是人工智能新手，老实说，我不明白决策树回归算法为何有效。
我尝试研究其背后的算法，但找不到令我满意的答案。
# 导入数组和东西的 numpy 包 
将 numpy 导入为 np 

# 导入 matplotlib.pyplot 来绘制我们的结果 
将 matplotlib.pyplot 导入为 plt 

# import pandas 用于导入 csv 文件 
将 pandas 导入为 pd 

# 导入数据集 
# 数据集 = pd.read_csv(&#39;Data.csv&#39;) 
# 或者打开.csv文件来读取数据 

数据集 = np.array( 
[[&#39;资产翻转&#39;, 100, 1000], 
[&#39;基于文本&#39;, 500, 3000], 
[&#39;视觉小说&#39;, 1500, 5000], 
[&#39;2D 像素艺术&#39;, 3500, 8000], 
[&#39;2D 矢量艺术&#39;, 5000, 6500], 
[&#39;策略&#39;, 6000, 7000], 
[&#39;第一人称射击游戏&#39;, 8000, 15000], 
[&#39;模拟器&#39;, 9500, 20000], 
[&#39;赛车&#39;, 12000, 21000], 
[&#39;角色扮演&#39;, 14000, 25000], 
[&#39;沙盒&#39;, 15500, 27000], 
[&#39;开放世界&#39;, 16500, 30000], 
[&#39;MMOFPS&#39;, 25000, 52000], 
[&#39;MMORPG&#39;, 30000, 80000] 
]） 

# 打印数据集 
打印（数据集） 

# 按 : 选择所有行和第 1 列 
# 按1:2表示特征 
X = 数据集[:, 1:2].astype(int) 

# 打印X 
打印（X） 

# 按 : 选择所有行和第 2 列 
# 由2到Y代表标签 
y = 数据集[:, 2].astype(int) 

# 打印y 
打印（y） 

# 导入回归器 
从 sklearn.tree 导入 DecisionTreeRegressor 

# 创建一个回归器对象 
回归器 = DecisionTreeRegressor(random_state = 0) 

# 用 X 和 Y 数据拟合回归器 
回归器.fit(X, y) 

# 预测一个新值 

# 通过更改值来测试输出，例如 3750 
y_pred = regressor.predict([[3750]]) #输出：8000

# 打印预测价格 
print(&quot;预测价格：% d\n&quot;% y_pred) 

为什么输出是8000？
我的意思是，我们如何知道这些价格预测函数背后运行的是什么？
它背后的数学公式是什么，我想手动完成，而不使用任何内置函数。
我尝试研究其背后的算法，但找不到令我满意的答案。
我希望得到满意的答复。]]></description>
      <guid>https://stackoverflow.com/questions/78510567/how-does-the-decisiontreeregressor-algorithm-work</guid>
      <pubDate>Tue, 21 May 2024 08:34:32 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow PPO 模型未给出模型输出</title>
      <link>https://stackoverflow.com/questions/78510077/tensorflow-ppo-model-not-giving-model-output</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78510077/tensorflow-ppo-model-not-giving-model-output</guid>
      <pubDate>Tue, 21 May 2024 06:49:17 GMT</pubDate>
    </item>
    <item>
      <title>我用自己的数据集训练yolo模型但没有测试结果</title>
      <link>https://stackoverflow.com/questions/78497575/i-train-yolo-model-with-my-own-data-set-but-there-is-no-test-result</link>
      <description><![CDATA[我正在使用 Yolov3 模型以及从 Kaggle 收到的数据集来训练模型。模型训练已完成，我将新权重添加到备份文件夹中。我运行了我训练过的一种水果进行测试，但没有发生对象检测。同一图像显示为 Prediction.jpg。训练看起来不错，但我不明白为什么它不能检测物体。请帮助我。
火车站代码：
./darknet探测器列车 /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/Desktop/Projects/Bitirmeprojesi/yolov3.weights

测试终端代码：
./darknet探测器测试 /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/darknet/backup/yolov3_final.weights -thresh 0.25 -out预测.jpg

我设置并编辑了 obj.data、obj.names 和 yolov3.cfg 文件。
我有 3 个类别：苹果、香蕉和橙子。我已经根据3个类在cfg文件中正确设置了filter和class值等值。
cfg 文件 
[网]
# 测试
批次=64
细分=1
＃ 训练
细分=16
宽度= 608
高度=608
通道=3
动量=0.9
衰减=0.0005
角度=0
饱和度=1.5
曝光=1.5
色调=0.3

学习率=0.001
烧入=1000
max_batches = 6000 # 类数 * 2000
政策=步骤
步骤=3600,4800 # max_batches num %80, %90 
尺度=.1,.1

数据集中除了.jpg图片外，还有yolo格式的同名.txt文件。
文件图片：
&lt;img alt=&quot;文件图像&quot; src=&quot;https://i.sstatic.net/Ed1BBcZP.png ” /&gt;
包含所有图像路径的 train.txt 和 test.txt 文件也已准备就绪。
当我在终端中运行测试命令时，它可以工作，但图片看起来相同，没有检测对象的边界框。我确定我已经安装了 Opencv。我正在使用 macOS。为什么它没有检测到它？请有人帮忙。我多次通过 make clean 清理暗网，并通过 make opencv = 1 运行它，但结果没有改变。
[yolo]参数：iou损失：mse（2），iou_norm：0.75，obj_norm：1.00，cls_norm：1.00，delta_norm：1.00，scale_x_y：1.00
总 BFLOPS 137.613 
平均输出 = 1052318 
正在从 /Users/melisabagcivan/darknet/backup/yolov3_final.weights 加载权重...
 看过 64 个，训练过：32013 个 K 图像（500 Kilo-batches_64） 
完毕！从权重文件加载 107 层 
输入图像路径：/Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg
 检测层：82-类型=28 
 检测层数：94-型=28 
 检测层：106-类型=28 
/Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg：预测为 6738.129000 毫秒。

我尝试了很多图像，但它没有在任何图像中绘制方框。我不明白是它无法检测到还是我在测试时犯了错误。]]></description>
      <guid>https://stackoverflow.com/questions/78497575/i-train-yolo-model-with-my-own-data-set-but-there-is-no-test-result</guid>
      <pubDate>Fri, 17 May 2024 19:21:32 GMT</pubDate>
    </item>
    <item>
      <title>使用 Devnagri 字体（gargi.ttf）的词云中的印地语重音（“数量”）问题</title>
      <link>https://stackoverflow.com/questions/73877676/hindi-accent%e0%a4%ae%e0%a4%be%e0%a4%a4%e0%a5%8d%e0%a4%b0%e0%a4%be-issue-in-word-cloud-using-devnagri-fontgargi-ttf</link>
      <description><![CDATA[我正在为一个项目处理印地语数据集，并对数据进行了预处理，为此创建了一个词云。我使用“gargi”字体在词云上绘制印地语单词，我遇到了重音问题（“ि मात्रा”）。在词云中，这个重音出现在它应该出现的字母旁边，例如，पुलिस 出现为 पुलसि。（请参考附件，其中单词 किसान 的重音（मात्रा）位于同一字母（वर्ण）上）。

此词云中还有其他几个词反映了类似的问题。我也尝试过使用不同的字体，例如“lohit-devnagri”、“samyak-devnagri”。
font = “gargi.ttf”

图，轴 = plt.subplots(2,2,figsize=(16,10))
图.tight_layout(pad=5.0)

wordcloud_kisaan = WordCloud(width = 1000, height = 700,
background_color =&#39;white&#39;,
min_font_size = 10, font_path= font).generate_from_frequencies(counter_kisaan)

axis[0][0].imshow(wordcloud_kisaan,interpolation=&quot;bilinear&quot;)
axis[0][0].axis(&#39;off&#39;)
axis[0][0].set_title(&#39;Kisaan Andolan&#39;, fontsize=22)

plt.axis(&quot;off&quot;)
plt.tight_layout(pad = 5.0)

plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/73877676/hindi-accent%e0%a4%ae%e0%a4%be%e0%a4%a4%e0%a5%8d%e0%a4%b0%e0%a4%be-issue-in-word-cloud-using-devnagri-fontgargi-ttf</guid>
      <pubDate>Wed, 28 Sep 2022 07:26:12 GMT</pubDate>
    </item>
    <item>
      <title>使用更高版本的 PyTorch 库的一阶 MAML 的官方实现是什么？</title>
      <link>https://stackoverflow.com/questions/70961541/what-is-the-official-implementation-of-first-order-maml-using-the-higher-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/70961541/what-is-the-official-implementation-of-first-order-maml-using-the-higher-pytorch</guid>
      <pubDate>Wed, 02 Feb 2022 19:17:08 GMT</pubDate>
    </item>
    <item>
      <title>来自变形金刚拥抱脸部的 Adafactor 仅适用于 Transfromers - 它不适用于更高版本的 Resnets 和 MAML 吗？</title>
      <link>https://stackoverflow.com/questions/70171427/adafactor-from-transformers-hugging-face-only-works-with-transfromers-does-it</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/70171427/adafactor-from-transformers-hugging-face-only-works-with-transfromers-does-it</guid>
      <pubDate>Tue, 30 Nov 2021 14:57:13 GMT</pubDate>
    </item>
    <item>
      <title>如何使用批量归一化而不忘记刚刚在 Pytorch 中使用的批量统计信息？</title>
      <link>https://stackoverflow.com/questions/64920715/how-to-use-have-batch-norm-not-forget-batch-statistics-it-just-used-in-pytorch</link>
      <description><![CDATA[我处于一个不寻常的环境中，我不应该使用运行统计数据（因为这会被视为作弊，例如元学习）。然而，我经常对一组点（实际上是 5 个点）进行前向传递，然后我只想使用之前的统计数据对 1 个点进行评估，但批归一化会忘记它刚刚使用的批统计数据。我尝试对它应该的值进行硬编码，但出现了奇怪的错误（即使我取消了 pytorch 代码本身的注释，例如检查尺寸大小）。
如何对之前的批次统计数据进行硬编码，以便批次规范适用于新的单个数据点，然后将其重置为全新的下一批？
注意：我不想更改批量标准化图层类型。
我尝试过的示例代码：
def set_tracking_running_stats(模型):
    对于 dir(model) 中的 attr：
        如果属性中有“bn”：
            target_attr = getattr(模型, attr)
            target_attr.track_running_stats = True
            target_attr.running_mean = torch.nn.Parameter(torch.zeros(target_attr.num_features,requires_grad=False))
            target_attr.running_var = torch.nn.Parameter(torch.ones(target_attr.num_features,requires_grad=False))
            target_attr.num_batches_tracked = torch.nn.Parameter(torch.tensor(0, dtype=torch.long), require_grad=False)
            # target_attr.reset_running_stats()
    返回

我最多的评论错误：
&lt;块引用&gt;
引发 ValueError(&#39;预期 2D 或 3D 输入（获得 {}D 输入）&#39;
ValueError：预期 2D 或 3D 输入（获得 1D 输入）

和
&lt;块引用&gt;
IndexError：维度超出范围（预期在 [-1, 0] 范围内，但得到 1）

相关：

https://discuss.pytorch.org/t/how-to-use-have-batch-norm-not-forget-batch-statistics-it-just-used/103437
何时使用 PyTorch 高级库执行 MAML 时应该调用 .eval() 和 .train() 吗？
]]></description>
      <guid>https://stackoverflow.com/questions/64920715/how-to-use-have-batch-norm-not-forget-batch-statistics-it-just-used-in-pytorch</guid>
      <pubDate>Thu, 19 Nov 2020 22:05:16 GMT</pubDate>
    </item>
    </channel>
</rss>