<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Sat, 08 Feb 2025 12:30:02 GMT</lastBuildDate>
    <item>
      <title>在扩散模型中U形网络的上下抽样阶段中不同的缩小参数</title>
      <link>https://stackoverflow.com/questions/79422978/different-concate-dimension-parameters-in-the-up-and-down-sampling-phase-of-u-sh</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79422978/different-concate-dimension-parameters-in-the-up-and-down-sampling-phase-of-u-sh</guid>
      <pubDate>Sat, 08 Feb 2025 10:14:34 GMT</pubDate>
    </item>
    <item>
      <title>在Google子午线中运行地理级别与国家级模型时，R平方值的差异</title>
      <link>https://stackoverflow.com/questions/79421421/discrepancy-in-r-squared-values-when-running-geo-level-vs-national-level-models</link>
      <description><![CDATA[我正在使用Google Meridian分析包含日期，国家，渠道和KPI的数据集。当我运行包括GEO（乡村）数据在内的模型时，我会得到两个R平方值：

 R平方用于地理级别数据：0.89 
国家级别的R平方数据：0.98 

但是，当我在没有地理数据的情况下运行模型（不包括地理列）时，R-squared显着下降至0.51。
我不明白为什么使用GEO数据时国家级别的R平方为0.98，但是当我排除它时，为什么降至0.51。在这两种情况下，国家级别的R平方不应该更加一致吗？
我尝试在有没有GEO（国家）列的情况下运行该模型。我希望在这两种情况下，R-squared都将保持相似之处，因为我认为删除地理信息不会对国家级模型产生巨大影响。
相反，包括地理数据的国家级别的R平方为0.98，但在排除该数据时仅为0.51。我想知道Google Meridian使用影响国家级别结果的地理数据时是否应用某种聚合或加权。
为什么存在这种差异，子午线如何处理GEO与国家数据？]]></description>
      <guid>https://stackoverflow.com/questions/79421421/discrepancy-in-r-squared-values-when-running-geo-level-vs-national-level-models</guid>
      <pubDate>Fri, 07 Feb 2025 15:35:09 GMT</pubDate>
    </item>
    <item>
      <title>从GPU内存中清除tf.data.dataset</title>
      <link>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</link>
      <description><![CDATA[在实施使用 tf.data.dataset 作为KERAS模型的输入的训练环时，我遇到了问题。我的数据集具有以下格式的元素规范：
 （{&#39;data&#39;：tensorSpec（shape =（15000，1），dtype = tf.float32），&#39;index&#39;：tensorspec&#39;：tensorspec（shape =（2，2，2，2， ），dtype = tf.int64）}，tensorspec（shape =（1，），dtype = tf.int32））
 
因此，基本上，每个样本均以元组（x，y）构建形状（15000，1），另一个形状索引（2，）（在培训期间不使用索引）， y 是单个标签。
The tf.data.Dataset is created using dataset = tf.data.Dataset.from_tensor_slices((X, y)), where X是两个密钥的命令：

 数据：形状的NP数组（200k，1500，1）， index  with 
 索引：形状的NP数组（200K，2） 

和 y 是一个形状的单个数组`（200k，1）
我的数据集有大约200k的培训样本（运行底漆后）和200k验证样本。
呼叫 tf.data.dataset.from_tensor_slices 我注意到GPU内存使用情况中有一个尖峰，在创建培训 tf.dataset ，and和创建验证 tf.dataset 。之后，更多16GB
创建 tf.dataSet 后，我运行了一些操作（例如，洗牌，批处理和预拿方），并调用 model.fit.fit 。我的型号大约有500K可训练的参数。
我遇到的问题是  拟合模型。我需要在一些其他数据上运行推断，因此我使用此数据创建了一个新的 tf.dataset ，再次使用 tf.dataset.from_tensor_slices 。但是，我注意到培训和验证 tf.dataset 仍然存在于GPU内存中，这导致我的脚本随着新的 tf.dataset  i而遇到的不含内存问题 i想要推断。
我在两个 tf.dataset 上尝试调用 del ，然后随后调用 gc.collect（）清除RAM，而不是GPU内存。另外，我尝试禁用我采用的一些操作，例如预摘要，也可以使用批量大小，但是这些都没有起作用。
是否有任何方法可以从GPU中清除两个 tf.dataset ，而无需致电 keras.backend.clear_session（） ，因为这也将从gpu？还是我唯一调用 clear_session 并从磁盘中重新加载模型的唯一选择？]]></description>
      <guid>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</guid>
      <pubDate>Fri, 07 Feb 2025 12:02:12 GMT</pubDate>
    </item>
    <item>
      <title>如何向量融合？</title>
      <link>https://stackoverflow.com/questions/79420799/how-to-vector-fusion</link>
      <description><![CDATA[假设有一个文档。在本文档中，有多个主题。通过其他手段，例如神经深度学习，这些主题嵌入了向量中。例如，“＃天气真的很好” ＆quot＃适合外出玩耍。”被转换为矢量A [0.1，0.25，...]和B [0.1，0.34，...]。
该文档还包含内容，该内容还通过嵌入将其转换为向量C [0.1，0.34，...]。它们都是2560-尺寸空间向量，具有相同的维度。
现在，我有一个要求。基于搜索条件，将其嵌入矢量f [0.6，0.78，...]。
使用余弦相似性的原理，我们计算：
cos（_f，_a）
cos（_f，_b）
cos（_f，_C）
我们得到三个分数，然后平均得分以获得全面的分数，这是本文档和搜索条件之间最相似的分数，实现了语义检索。
但是，该方法的计算复杂性太高了，因为主题的数量不是固定的，并且可能超过十。
所以，我们提出了计划2。
在计划2中，我们在主题_a和_b上执行平均合并，即计算向量的算术平均值以获取_ac。然后，我们执行内容向量_C和_ac的加权平均值：0.7 * _C + 0.3 * _ac = _bac。然后，我们计算搜索 - 条件向量_F和_bac之间的余弦相似性，以获得相似性。但是问题在于，是_a和_b的算术平均值还是_ac和_c的加权平均值，某些功能将丢失，从而导致语义查询不令人满意。
有什么方法可以融合_a，_b和_c而不会丢失语义？]]></description>
      <guid>https://stackoverflow.com/questions/79420799/how-to-vector-fusion</guid>
      <pubDate>Fri, 07 Feb 2025 11:55:21 GMT</pubDate>
    </item>
    <item>
      <title>无法理解机器学习算法背后的数学之间的关系[关闭]</title>
      <link>https://stackoverflow.com/questions/79420509/couldnt-understand-relation-between-math-behind-the-machine-learning-algorithm</link>
      <description><![CDATA[我正在研究加强学习算法。我正在观看一些诸如CS285的讲座 - 来自Berkley或Google DeepMind。我能理解讲师教的90-95％。  但是，当我尝试实现自己学到的算法时，我遇到了很多困难。我阅读了有关该主题的文章，并检查了作者或人们编写的示例代码，但我无法将文章的数学公式与文章解释 - 和书面代码相关联。  为了谈话，让我举个例子。
  -log [π_θ（â_θ（s，ξ）| S）]  
 注意：我删除了拆卸零件，以使我更易于理解。 
那是我从或 sac文章这就是代码实现i，我发现不同的git repos  [1]  -line-line 78，70-，70-，&lt; a href =“ https://github.com/pranz24/pytorch-soft-actor-critic/blob/master/master/model.py.py#l64” rel =“ nofollow noreferrer”&gt; [2] 104- 
  log_probs = normals.log_prob（xs）-torch.log（1- actions.pow（2） + self.eps）
 
 完整公式的图像  
我做了一些研究和gpting，我大多了解我们在代码中正在做什么以及为什么要做。  但是根据我的数学知识 - 我可以将数学技能排名为中级 - 我将公式解释为“计算–_θ（s，ξ），然后将结果放在结果之后属于的结果，然后计算π_θ（-_θ（s，ξ）| s）。除了我理解的内容外，本文中的代码和解释以相反的顺序进行。  ＆quot从网络中获取操作，并将其发送到边界转换。 ＆quot 
我无法弄清楚问题是否是我的数学知识，而不是我的英语差，反之亦然。我对我做错了什么的想法或思考。]]></description>
      <guid>https://stackoverflow.com/questions/79420509/couldnt-understand-relation-between-math-behind-the-machine-learning-algorithm</guid>
      <pubDate>Fri, 07 Feb 2025 10:16:12 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch LSTM模型的评估非常缓慢</title>
      <link>https://stackoverflow.com/questions/79420479/very-slow-evaluation-of-a-pytorch-lstm-model</link>
      <description><![CDATA[我正在遇到一些旨在执行“音频事件识别”的LSTM模型的问题。从与犯罪相关的音频样本的数据集中。
整个网络在训练过程中似乎运行良好，获得了90％准确性的良好结果。真正的关键是评估阶段，它非常缓慢且经常超出记忆。
 bellow评估功能的代码：
在

    TORCH.CUDA.EMPTY_CACHE（）

    model.eval（）

    total_loss = 0.0
    total_correct = 0
    total_data = 0

    progress_bar = tqdm（dl，desc =; testing＆quord; uits =; batch; quot; quot;

    使用Torch.no_grad（）：
        对于枚举（DL）中的batch_index（数据，目标）：
            data = data.to（设备）
            target = target.to（设备）

            输出=模型（数据）

            损失=标准（输出，目标）

            _，预测= torch.max（输出，dim = 1）
            _，正确= torch.max（target，dim = 1）

            total_data += data.size（0）

            partial_correct = torch.sum（预测==正确）.Item（）
            partial_accuracy = partial_correct / data.size（0）
            partial_loss = loss.item（）

            total_correct += partial_correct
            total_loss += partial_loss * data.size（0）

            progress_bar.set_postfix（{{＆quert&#39;：partial_accuracy，; quot; less&#39;：partial_loss}）

    test_loss = total_loss / total_data
    test_acc = total_correct / total_data

    返回test_loss，test_acc
 
测试功能与评估或多或少相似，唯一的例外是使用 model.train（）且没有 torch.no_grad（） 代码块。
作为一种典型的ML方法，对于每个时期，我都会在此之后运行一个训练阶段和评估阶段。我应该只是训练整个模型并在最后进行测试吗？
您在我试图清除GPU缓存的代码中看到的，但这对速度也没有任何帮助，也没有“失误”。错误。]]></description>
      <guid>https://stackoverflow.com/questions/79420479/very-slow-evaluation-of-a-pytorch-lstm-model</guid>
      <pubDate>Fri, 07 Feb 2025 10:04:14 GMT</pubDate>
    </item>
    <item>
      <title>在拥有IterabledataSet和DataLoader的同时，如何计算培训和验证的准确性和损失？</title>
      <link>https://stackoverflow.com/questions/79419743/how-can-i-calculate-the-training-and-validation-accuracy-and-losses-while-having</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79419743/how-can-i-calculate-the-training-and-validation-accuracy-and-losses-while-having</guid>
      <pubDate>Fri, 07 Feb 2025 00:48:54 GMT</pubDate>
    </item>
    <item>
      <title>Yolov8最终检测头仍输出（1、7、8400），而不是（1、8、8400）</title>
      <link>https://stackoverflow.com/questions/79419018/yolov8-final-detection-head-still-outputs-1-7-8400-instead-of-1-8-8400-f</link>
      <description><![CDATA[我训练了3个类的Yolov8检测模型，但是原始的正向通行证仍然显示（1、7、8400）而不是（1、8、8400）的最终检测输出。
我做了什么：
检查了我的data.yaml：
  yaml
火车：路径/到/火车/图像
Val：路径/到/Val/图像
NC：3
名称：[&#39;神经胶质瘤&#39;，&#39;脑膜瘤&#39;，&#39;垂体&#39;]
 
确认的NC：3是正确的。
通过命令从头开始训练：
  bash
Yolo检测火车\
    data =路径/到/data.yaml \
    模型= yolov8x \
    epochs = 1000 \
    imgsz = 640 \
    设备= 1 \
    耐心= 100
 
训练没有错误地进行，并成功完成。
安装了最新的Ultrytics版本（v8.3.72），以确保没有版本问题：
  bash
PIP卸载超级分析
PIP安装超级词
 
直接加载了新的best.pt：
  python
从超级物质进口YOLO
导入火炬

型号= yolo（r＆quot; best.pt;）。模型
model.eval（）

dummy_input = torch.randn（1，3，640，640）
使用Torch.no_grad（）：
    输出=模型（Dummy_input）

输出输出：
    ＃一些输出是列表；仔细检查每个元素
    如果Isinstance（Out，Torch.Tensor）：
        打印（OUT.SHAPE）
    别的：
        打印（“列表输出：＆quot” [o。
 
控制台显示检测输出（1、7、8400）。
经过验证的模型元数据说NC = 3和Model.Names有3个类。但是，原始检测层输出仍然是7个通道。
观察：
如果Yolo检测层是真正的3类，则应输出（5 + 3）=每个锚点8个通道，而不是7通道。
不匹配（1、7、8400）通常表明尽管NC = 3。
问题 /请求帮助：
为什么即使我从头开始训练了3堂课，为什么还要原始检测头（1、7、8400）？
如何确保将检测层完全重新定位为（5 + 3）= 8以进行3级检测？
我已经尝试删除旧的.pt文件，重新检查我的数据。yaml，重新安装超级图像和确认模型。model.nc== 3。但是最终检测层继续产生7个频道，而不是8个频道。
关于可能导致这种持续不匹配的什么想法？
事先感谢您的任何帮助或见解！]]></description>
      <guid>https://stackoverflow.com/questions/79419018/yolov8-final-detection-head-still-outputs-1-7-8400-instead-of-1-8-8400-f</guid>
      <pubDate>Thu, 06 Feb 2025 18:39:21 GMT</pubDate>
    </item>
    <item>
      <title>如何将输入传递到张量流骨干模型而不获得属性：'元组'对象没有属性'as_list'</title>
      <link>https://stackoverflow.com/questions/79411501/how-to-pass-input-to-a-tensorflow-backbone-model-without-getting-attributeerror</link>
      <description><![CDATA[我试图从 tensorflow models 库中使用resnet3d，但是在尝试运行块时我会遇到这个奇怪的错误
 ！pip install tf-models-inficial == 2.17.0
 
 TensorFlow版本是 2.18 在Kaggle Notebook上。
安装 tf-models-inficial  
 来自Tensorflow.keras.callbacks导入早期踩踏，REDUCELRONPLATEAU
来自Tensorflow.keras.models导入模型
来自tensorflow.keras.layers导入密集，globalaverate -pooling3d，输入
来自TensorFlow.keras.optimizer导入adamw
导入TensorFlow_Models作为TFM

def create_model（）：
    base_model = tfm.vision.backbones.resnet3d（model_id = 50，
        tuermal_strides = [3,3,3,3]，
        temulal_kernel_sizes = [（5,5,5），（5,5,5,5），（5,5,5,5,5,5），（5,5,5）]，]，
        input_specs = tf.keras.layers.inputspec（shape =（无，无，img_size，img_size，3））
    ）
    
    ＃解冻基本型号层
    base_model.trainable = true
    
    ＃创建模型
    输入=输入（shape = [无，无，img_size，img_size，3]）
    x = base_model（输入）＃b，1,7,7,2048
    x = globalaveragepooling3d（data_format =＆quot; channels_last_last＆quort; keepdims = false）（x）
    x =密集（1024，激活=&#39;relu&#39;）（x）
    x = tf.keras.layers.dropout（0.3）（x）＃添加辍学以防止过度拟合
    输出=密集（num_classes，activation =&#39;softmax&#39;）（x）
    
    模型=模型（输入，输出）
    
    ＃用级别的权重编译模型
    优化器= ADAMW（Learning_rate = 1E-4，weight_decay = 1E-5）
    model.compile（
        优化器=优化器，
        损失=&#39;Sparse_categorical_crossentropy&#39;，
        量表= [&#39;feceracy&#39;，tf.keras.metrics.auc（）]
    ）
    
    返回模型

＃创建和显示模型
模型= create_model（）
model.summary（）
 
运行此操作时，我会在下面收到错误：
  ---------------------------------------------------------------------------------------------------------------------------------------------- ----------------------------------------------
AttributeError Trackback（最近的最新电话）
＆lt; ipython输入-56-363271B4DDA8＆gt; in＆lt;单元线：39＆gt;（）
     37 
     38＃创建和显示模型
---＆gt; 39模型= create_model（）
     40 Model.Summary（）

＆lt; ipython输入-56-363271B4DDA8＆gt;在create_model（）中
     18＃创建模型
     19个输入=输入（shape =（无，无，img_size，img_size，3））
---＆gt; 20 x = base_model（输入）＃b，1,7,7,2048

/USR/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py in __call __（self， *args，** kwargs）
    586 layout_map_lib._map_subclass_model_varible（self，self._layout_map）
    587 
 - ＆gt; 588返回super（）.__调用__（*args，** kwargs）

/USR/local/lib/python3.10/dist-packages/tf_keras/src/engine/base/base_layer.py in __call ____________________________________________________________________________________________________________________________________________________________________________________________________________________
   1101培训=训练_mode，
   1102）：
 - ＆gt; 1103 input_spec.assert_input_compatibility（
   1104 self.input_spec，输入，self.name
   1105）

/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/input_spec.py in assert_input_compatibility（input_spec，inputs，inputs，lays_name）
    300“与该层不兼容：”
    301 f＆quot“预期形状= {spec.shape}，”
 - ＆gt; 302 f＆quot``找到形状= {display_shape（x.shape）}}＆quot;
    303）
    304 

/USR/local/lib/python3.10/dist-packages/tf_keras/src/engine/input_spec.py in Display_shape（shape）
    305 
    306 def display_shape（形状）：
 - ＆gt; 307返回str（元组（shape.as_list（）））
    308 
    309 

attributeError：&#39;tuple&#39;对象没有属性&#39;as_list&#39;
 
我尝试将输入传递到 shape 参数作为列表，但仍会遇到相同的错误。
此错误正在发生
 ！pip install tf-models-inficial == 2.17.0

导入TensorFlow作为TF

inputs = tf.keras.input（shape = [none，none，img_size，img_size，3]）
打印（inputs.shape.as_list（））
 
错误：
  ---------------------------------------------------------------------------------------------------------------------------------------------- ----------------------------------------------
AttributeError Trackback（最近的最新电话）
＆lt; ipython-Input-39-6e88680ff7df＆gt; in＆lt;单元线：2＆gt;（）
      1个输入= tf.keras.input（shape = [none，none，img_size，img_size，3]）
----＆gt; 2 print（inputs.shape.as_list（））

attributeError：&#39;tuple&#39;对象没有属性&#39;as_list&#39;
 ]]></description>
      <guid>https://stackoverflow.com/questions/79411501/how-to-pass-input-to-a-tensorflow-backbone-model-without-getting-attributeerror</guid>
      <pubDate>Tue, 04 Feb 2025 11:22:41 GMT</pubDate>
    </item>
    <item>
      <title>在硬3.5中使用SavedModel格式</title>
      <link>https://stackoverflow.com/questions/79388942/using-savedmodel-format-in-keras-3-5</link>
      <description><![CDATA[尝试使用 tf.keras.models.models.load_model（path_to_to_pb_model）加载SavedFormat模型。
我正在使用Tensorflow 2.17.1与Keras 3.5。
显然KERAS 3仅接受 .keras 或 .h5 格式。
首先，我有点困惑，因为根据此文档：
保存和加载模型，您应该能够保存＆amp;负载.pb模型在Keras 3。
    
文档是错误的吗？因为当我尝试在COLAB中运行此代码时，我会收到一个错误，只说 .keras ＆amp;  .h5 支持文件格式。
另外，我还检查了本指南有关如何加载＆amp;使用tf.saved_model apis保存SAVEDMODEL格式，但这不会返回keras对象，并且没有 .evaluate（）或或 .predict（）.predict（） methods。
那么，是否有任何方法可以将SavedModel格式加载到KERAS3中？然后在我的测试数据上评估该模型？]]></description>
      <guid>https://stackoverflow.com/questions/79388942/using-savedmodel-format-in-keras-3-5</guid>
      <pubDate>Sun, 26 Jan 2025 17:06:14 GMT</pubDate>
    </item>
    <item>
      <title>我如何在Kaggle中使用Python版本3.7.1</title>
      <link>https://stackoverflow.com/questions/77537786/how-do-i-use-python-version-3-7-1-in-kaggle</link>
      <description><![CDATA[我正在尝试训练以Python版本3.7.1脚本脚本的TensorFlow ML模型。但是，Kaggle中的Python版本为3.10.2。是否有任何方法可以使用Python 3.7环境，是否可以使其成为默认的env？]]></description>
      <guid>https://stackoverflow.com/questions/77537786/how-do-i-use-python-version-3-7-1-in-kaggle</guid>
      <pubDate>Thu, 23 Nov 2023 14:49:38 GMT</pubDate>
    </item>
    <item>
      <title>微调和很少的射击学习之间有什么区别？</title>
      <link>https://stackoverflow.com/questions/72611335/what-are-the-differences-between-fine-tuning-and-few-shot-learning</link>
      <description><![CDATA[我试图理解微调和的概念，学习。
我了解进行微调的必要性。它本质上是将预训练的模型调整为特定的下游任务。但是，最近我看到了大量博客文章，说明零拍的学习，一次性学习和很少的学习。

它们与微调有何不同？在我看来，很少有学习是对微调的专业化。我在这里想念什么？

有人可以帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/72611335/what-are-the-differences-between-fine-tuning-and-few-shot-learning</guid>
      <pubDate>Tue, 14 Jun 2022 03:54:23 GMT</pubDate>
    </item>
    <item>
      <title>ALS的Pyspark实施如何处理每个用户项目组合的多个评级？</title>
      <link>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</link>
      <description><![CDATA[我观察到，到ALS的输入数据不需要每个用户项目组合的唯一评分。
这是一个可再现的例子。
 ＃示例数据框架
df = spark.createDataframe（[（0，0，4.0），（0，1，2.0）， 
（1，1，3.0），（1，2，4.0）， 
（2，1，1.0），（2，2，5.0）]，[&#39;用户;

DF.Show（50,0）
+-----+----+-------+
|用户|项目|评级|
+-----+----+-------+
| 0 | 0 | 4.0 |
| 0 | 1 | 2.0 |
| 1 | 1 | 3.0 |
| 1 | 2 | 4.0 |
| 2 | 1 | 1.0 |
| 2 | 2 | 5.0 |
+-----+----+-------+
 
您可以看到，每个用户项目组合只有一个评分（理想的情况）。
如果我们将此数据框传递到ALS，它将为您提供如下的预测：
 ＃拟合ALS
来自pyspark.ml.Recmendation Import ALS
als = als（rank = 5， 
          maxiter = 5， 
          种子= 0，
          regparam = 0.1，
         usercol =&#39;用户&#39;，
         itemcol =&#39;item&#39;，
         评分=&#39;等级&#39;，
         非负= true）
型号= als.fit（DF）

＃来自ALS的预测
all_comb = df.Select（&#39;user&#39;）。distract（）。join（广播（df.Select（&#39;item&#39;）。dimption（）））
预测= model.transform（all_comb）

预测显示（20,0）
+----+----+------------+
|用户|项目|预测|
+----+----+------------+
| 0 | 0 | 3.9169915 |
| 0 | 1 | 2.031506 |
| 0 | 2 | 2.3546133 |
| 1 | 0 | 4.9588947 |
| 1 | 1 | 2.8347554 |
| 1 | 2 | 4.003007 |
| 2 | 0 | 0.9958025 |
| 2 | 1 | 1.0896711 |
| 2 | 2 | 4.895194 |
+----+----+------------+
 
到目前为止，一切都有意义。但是，如果我们有一个包含多个用户项目评级组合的数据框，如以下 -  
 ＃示例daataframe
df = spark.createDataFrame（[（（0，0，4.0），（0，0，3.5），
                            （0，0，4.1），（0，1，2.0），
                            （0，1，1.9），（0，1，2.1），
                            （1，1，3.0），（1，1，2.8），
                            （1，2，4.0），（1，2，3.6），
                            （2，1，1.0），（2，1，0.9），
                            （2，2，5.0），（2，2，4.9）]，
                           [用户“
DF.Show（100,0）
+-----+----+-------+
|用户|项目|评级|
+-----+----+-------+
| 0 | 0 | 4.0 |
| 0 | 0 | 3.5 |
| 0 | 0 | 4.1 |
| 0 | 1 | 2.0 |
| 0 | 1 | 1.9 |
| 0 | 1 | 2.1 |
| 1 | 1 | 3.0 |
| 1 | 1 | 2.8 |
| 1 | 2 | 4.0 |
| 1 | 2 | 3.6 |
| 2 | 1 | 1.0 |
| 2 | 1 | 0.9 |
| 2 | 2 | 5.0 |
| 2 | 2 | 4.9 |
+-----+----+-------+
 
您可以在上述数据框架中看到，有一个用户项目组合的多个记录。例如 - 用户&#39;0&#39;对项目&#39;0&#39;分别额定为4.0,3.5和4.1。。
如果我将此输入数据范围传递给ALS怎么办？这个可以吗？
我最初认为它不应该工作，因为ALS应该每个用户项目组合获得唯一的评分，但是当我运行此功能时，它可以奏效并使我感到惊讶！&gt; 
 ＃拟合ALS
als = als（rank = 5， 
          maxiter = 5， 
          种子= 0，
          regparam = 0.1，
         usercol =&#39;用户&#39;，
         itemcol =&#39;item&#39;，
         评分=&#39;等级&#39;，
         非负= true）
型号= als.fit（DF）

＃来自ALS的预测
all_comb = df.Select（&#39;user&#39;）。distract（）。join（广播（df.Select（&#39;item&#39;）。dimption（）））
预测= model.transform（all_comb）

预测显示（20,0）
+----+----+------------+
|用户|项目|预测|
+----+----+------------+
| 0 | 0 | 3.7877638 |
| 0 | 1 | 2.020348 |
| 0 | 2 | 2.4364853 |
| 1 | 0 | 4.9624424 |
| 1 | 1 | 2.7311888 |
| 1 | 2 | 3.8018093 |
| 2 | 0 | 1.2490809 |
| 2 | 1 | 1.0351425 |
| 2 | 2 | 4.8451777 |
+----+----+------------+
 
为什么起作用？我认为它会失败，但也没有给我预测。
我尝试查看研究论文，有限的ALS源代码以及Internet上的可用信息，但找不到任何有用的东西。
是否平均需要这些不同的评分，然后将其传递给ALS或其他任何东西？
以前有人遇到过类似的事情吗？还是关于ALS如何在内部处理此类数据的任何想法？]]></description>
      <guid>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</guid>
      <pubDate>Tue, 26 Apr 2022 10:37:44 GMT</pubDate>
    </item>
    <item>
      <title>用户的多个排名中的ALS（交替平方）算法</title>
      <link>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</link>
      <description><![CDATA[嗨，经过大量的研究，我们决定使用Google云基础架构并使用ALS算法（一种协作过滤方法 -   https://cloud.google.com/solutions/solutions/recommendations-using-machine-learning-learning-compute-engine-engine-engine-wrinage--模式）在我们的产品推荐系统中，该系统在下面的详细信息中进行了解释：
我们有两种类型的客户。第一类是在附近出售产品的公司，第二类是将要从这些公司购买产品的消费者

每个消费者都有能力搜索附近的公司或通过其行业搜索公司（例如杂货，干洗，屠夫等））
 当消费者找到一家公司时，他/她可以执行以下操作（他可以一次执行多个项目）
 2.1。仅查看公司资料
 2.2。将公司添加到收藏夹
 2.3。开始与公司聊天
 2.4。从公司订购
 2.5。给公司评级和评论 

所以我不理解的是：上面描述的每个项目都被确定为数据库中的一些评分列，例如：
查看公司资料：10 pts 
从公司订购：20 pts 
向公司发表明星或发表评论：20 pts 
因此，每个项目都是同一用户的单独评级。
在我们的数据库中，用户 - 公司对可能有超过1行
例如：
第1行：User18-Company18-10pts（一次查看配置文件）
第2行：User18-Company18-20pts（从公司订购）
第3行：User18-Company19-10pts 
我不确定该算法，它是计算该用户对同一公司的所有评级的总和（我确切想要的），还是只是为用户的评分寻找一行一家公司？ （我想要的是该ALS算法总结该用户 - 公司对的Row1和Row2）
有人知道吗？这对于我们的推荐系统非常重要。因为我要寻找的算法需要计算用户的所有评分总和，以便推荐另一家公司。因为我们的业务模型与电影评级系统有所不同
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</guid>
      <pubDate>Tue, 01 May 2018 09:53:59 GMT</pubDate>
    </item>
    <item>
      <title>顺序分类软件包和算法</title>
      <link>https://stackoverflow.com/questions/3495157/ordinal-classification-packages-and-algorithms</link>
      <description><![CDATA[我正在尝试制作一个分类器，该分类器选择项目 i 的评分（1-5）。  对于每个项目I，我都有一个向量 x ，其中包含大约40个与 i 有关的数量。  我也对每个项目都有一个金标准评分。  根据 X 的某些功能，我想训练分类器，以给我1-5的评分，与金标准非常匹配。  
我在分类器上看到的大多数信息仅处理二进制决策，而我有一个评级决定。  是否有常见的技术或代码库来处理这种问题？ ]]></description>
      <guid>https://stackoverflow.com/questions/3495157/ordinal-classification-packages-and-algorithms</guid>
      <pubDate>Mon, 16 Aug 2010 16:25:53 GMT</pubDate>
    </item>
    </channel>
</rss>