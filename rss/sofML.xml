<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 18 Dec 2023 01:01:37 GMT</lastBuildDate>
    <item>
      <title>TD(1) 如何等同于蒙特卡罗采样？</title>
      <link>https://stackoverflow.com/questions/77676320/how-td1-is-equivalent-to-monte-carlo-sampling</link>
      <description><![CDATA[我需要证明给定 λ=1，TD(λ) = Monte-Carlo。我在几个网站上阅读过，但我不知道如何得出证明。
我们知道以下内容：
TD(lambda) 和蒙特卡洛方程
但是，如果在 λ-&gt;0 时设置限制。 1 使用 TD(λ) 目标方程，您会得到如下结果：
lim(λ-&gt;1) (1-λ) ...
所以设λ = 1，(1-λ) = 0，则所有表达式都为0。
如果有人可以提供有关如何摆脱此 (1-λ) 的信息，以分析证明 TD(1) 相当于蒙特卡洛，我将非常感激。]]></description>
      <guid>https://stackoverflow.com/questions/77676320/how-td1-is-equivalent-to-monte-carlo-sampling</guid>
      <pubDate>Sun, 17 Dec 2023 23:17:25 GMT</pubDate>
    </item>
    <item>
      <title>在电脑上训练我的模型，然后在微控制器上使用它[关闭]</title>
      <link>https://stackoverflow.com/questions/77675723/train-my-model-on-pc-then-use-it-on-microcontroller</link>
      <description><![CDATA[如果我想在我的 PC 上训练一个模型（无论是 ML、NN 还是 CNN），因为我有强大的 GPU，是否可以在 Arduino 或 Raspberry Pi Pico 等微控制器上导出或保存这个训练模型以直接使用它？或者我需要从头开始重新训练这些模型？]]></description>
      <guid>https://stackoverflow.com/questions/77675723/train-my-model-on-pc-then-use-it-on-microcontroller</guid>
      <pubDate>Sun, 17 Dec 2023 19:33:23 GMT</pubDate>
    </item>
    <item>
      <title>从磁盘加载和使用 PyTorch 模型</title>
      <link>https://stackoverflow.com/questions/77675553/loading-and-using-a-pytorch-model-from-disk</link>
      <description><![CDATA[我是 PyTorch 的新手，在第一页上 - https:/ /pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html
该页面希望我在同一个文件中运行整个代码。即创建模型、训练模型、将其保存到磁盘、从磁盘加载并使用模型进行推理。
我发现推理代码依赖于训练期间完成的模型定义。我的问题是，我是否需要知道模型定义才能使用预训练模型？这似乎是错误的。我确信事实并非如此。那么，如何使用保存的模型进行推理呢？
谷歌搜索向我展示了使用 python 的导入函数导入模型的实例。但是，我找不到如何从磁盘加载模型并在不知道模型定义的情况下使用它的示例。请问有什么帮助吗？
我想将下面代码的模型加载和推理部分保存到另一个文件并运行它。我不知道该怎么做
导入火炬
从火炬导入 nn
从 torch.utils.data 导入 DataLoader
从 torchvision 导入数据集
从 torchvision.transforms 导入 ToTensor

# 从开放数据集中下载训练数据。
训练数据 = 数据集.FashionMNIST(
    根=“数据”，
    火车=真，
    下载=真，
    变换=ToTensor(),
）

# 从开放数据集中下载测试数据。
test_data = 数据集.FashionMNIST(
    根=“数据”，
    火车=假，
    下载=真，
    变换=ToTensor(),
）

批量大小 = 64

# 创建数据加载器。
train_dataloader = DataLoader(training_data,batch_size=batch_size)
test_dataloader = DataLoader(test_data,batch_size=batch_size)

对于 test_dataloader 中的 X、y：
    print(f&quot;X 的形状 [N, C, H, W]: {X.shape}&quot;)
    print(f&quot;y 的形状：{y.shape} {y.dtype}&quot;)
    休息


# 获取 cpu、gpu 或 mps 设备进行训练。
设备=（
    “cuda”
    如果 torch.cuda.is_available()
    否则“mps”
    如果 torch.backends.mps.is_available()
    否则“CPU”
）
print(f“使用 {device} 设备”)

# 定义模型
神经网络类（nn.Module）：
    def __init__(自身):
        超级().__init__()
        self.flatten = nn.Flatten()
        self. Linear_relu_stack = nn.Sequential(
            nn.线性(28*28, 512),
            ReLU(),
            nn.线性(512, 512),
            ReLU(),
            nn.线性(512, 10)
        ）

    def 前向（自身，x）：
        x = self.展平(x)
        logits = self. Linear_relu_stack(x)
        返回逻辑值

模型 = NeuralNetwork().to(设备)
print(model)# 获取 cpu、gpu 或 mps 设备进行训练。

loss_fn = nn.CrossEntropyLoss()
优化器 = torch.optim.SGD(model.parameters(), lr=1e-3)

def train（数据加载器，模型，loss_fn，优化器）：
    大小 = len(dataloader.dataset)
    模型.train()
    对于批处理，枚举（dataloader）中的（X，y）：
        X, y = X.to(设备), y.to(设备)

        # 计算预测误差
        预测值=模型(X)
        损失 = loss_fn(pred, y)

        # 反向传播
        loss.backward()
        优化器.step()
        优化器.zero_grad()

        如果批次 % 100 == 0:
            损失，当前 = loss.item(), (batch + 1) * len(X)
            print(f&quot;loss: {loss:&gt;7f} [{current:&gt;5d}/{size:&gt;5d}]&quot;)



def 测试（数据加载器、模型、loss_fn）：
    大小 = len(dataloader.dataset)
    num_batches = len(数据加载器)
    模型.eval()
    test_loss, 正确 = 0, 0
    使用 torch.no_grad()：
        对于数据加载器中的 X、y：
            X, y = X.to(设备), y.to(设备)
            预测值=模型(X)
            test_loss += loss_fn(pred, y).item()
            正确 += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    正确/=尺寸
    print(f&quot;测试误差:\n 准确度:{(100*正确):&gt;0.1f}%, 平均损失:{test_loss:&gt;8f}\n&quot;)



历元 = 5
对于范围内的 t（纪元）：
    print(f&quot;历元 {t+1}\n--------------------------------------------&quot;)
    火车（train_dataloader，模型，loss_fn，优化器）
    测试（test_dataloader，模型，loss_fn）
print(&quot;完成！&quot;)

torch.save(model.state_dict(), &quot;model.pth&quot;)
print(&quot;将 PyTorch 模型状态保存到 model.pth&quot;)


模型 = NeuralNetwork().to(设备)
model.load_state_dict(torch.load(“model.pth”))

类=[
    “T 恤/上衣”，
    “裤子”，
    “套头衫”，
    “礼服”，
    “外套”，
    “凉鞋”，
    “衬衫”，
    “运动鞋”，
    “袋子”，
    “踝靴”，
]

模型.eval()
x, y = 测试数据[0][0], 测试数据[0][1]
使用 torch.no_grad()：
    x = x.to(设备)
    预测值=模型(x)
    预测，实际=类[pred[0].argmax(0)]，类[y]
    print(f&#39;预测：“{预测}”，实际：“{实际}”&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/77675553/loading-and-using-a-pytorch-model-from-disk</guid>
      <pubDate>Sun, 17 Dec 2023 18:40:15 GMT</pubDate>
    </item>
    <item>
      <title>用于音频分类的 CNN 模型</title>
      <link>https://stackoverflow.com/questions/77675519/cnn-model-for-audio-classification</link>
      <description><![CDATA[我想构建一个可以使用 CNN 识别仪器的系统。但是，我对如何进行学习有一些疑问。

现在，其他研究使用的大多数方法都涉及使用频谱图作为 CNN 模型的输入。如果我使用原始值（即音频特征值数组）作为 CNN 的输入，我的系统性能会受到影响吗？这个办法可行吗？
我想为我的项目提取多种音频特征，例如 MFCC、梅尔频谱图、频谱质心、频谱通量等。我是否只需要选择一种特征来输入到我的 CNN 模型中，或者将它们组合起来实际上可行并将其作为我对 CNN 的输入？如果后者可行，我该怎么做？是否需要某种特征选择或降维技术（即 PCA/TSNE）？
]]></description>
      <guid>https://stackoverflow.com/questions/77675519/cnn-model-for-audio-classification</guid>
      <pubDate>Sun, 17 Dec 2023 18:32:22 GMT</pubDate>
    </item>
    <item>
      <title>我不明白如何训练元学习器或它如何预测</title>
      <link>https://stackoverflow.com/questions/77675237/i-dont-understand-how-to-train-a-meta-learner-or-how-it-predicts</link>
      <description><![CDATA[我一直在学习机器学习模型，并且遇到了元学习器的概念。
据我所知，这些方法通常通过将多个分类器的预测作为特征、将真实标签作为标签并在此基础上训练另一个模型来工作。
如果我有两个情感分类器，则会给出以下结果：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

文本
真实标签
CL 1 标签
CL 1 概率
CL 2 标签
CL 2 概率


&lt;正文&gt;

快乐
积极
积极
0.9
积极
0.7


悲伤
否定
积极
0.7
否定
0.9


生气
否定
否定
0.8
积极
0.6


内容
积极
积极
0.5
否定
0.8




我训练了一个新模型，将真实标签作为标签，将概率作为特征。新模型如何知道概率指的是哪个标签？基分类器可以以 0.9 的确定性预测任一标签。那么元学习器也应该包含基分类器标签吗？或者基分类器标签是否是元学习器的特征？
一旦元学习器经过训练，我假设它将在测试数据的文本部分上进行测试。但由于它没有学习文本中包含的任何特征，它到底在预测什么？]]></description>
      <guid>https://stackoverflow.com/questions/77675237/i-dont-understand-how-to-train-a-meta-learner-or-how-it-predicts</guid>
      <pubDate>Sun, 17 Dec 2023 16:54:36 GMT</pubDate>
    </item>
    <item>
      <title>根据新数据训练 NLP 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/77674787/training-nlp-model-on-new-data</link>
      <description><![CDATA[我正在开发一个 NLP 模型（使用 LSTM），该模型接收文本句子（例如谷歌地图评论）并预测星级评分。
我掌握了拥有超过 600 万条评论的 yelp 数据集，我正在用它来训练我的模型。数据集太大，导致我分批训练模型（10K 批审核）。
假设我完成了训练并且取得了合理的表现。随后，Yelp 发布了 100 万条新评论。如何在新数据上训练模型？
1- 我应该仅根据新数据训练模型吗？
或者
2- 将新数据与旧数据结合并重新训练模型？
3-如何避免灾难性遗忘？]]></description>
      <guid>https://stackoverflow.com/questions/77674787/training-nlp-model-on-new-data</guid>
      <pubDate>Sun, 17 Dec 2023 14:36:08 GMT</pubDate>
    </item>
    <item>
      <title>关于PyTorch中matmul的批量效果——什么时候减少？</title>
      <link>https://stackoverflow.com/questions/77673891/about-batched-effect-of-matmul-in-pytorch-when-to-reduce</link>
      <description><![CDATA[bp 在批处理 matmul 场景中如何工作？假设我们有一个线性，x的形状是[b,m,k]，W的形状是[k,n]：
&lt;前&gt;&lt;代码&gt; y = xW

然后：
 \frac{\partial L}{\partial W} = x^T\frac{\partial L}{\partial y}

那么W后面发生了什么？

我们保存完整的 $x^T$ 张量 [b,k,m]，并执行批处理的 matmul [b,k,m] x [b,m,n ]，最后将结果归约到[k,n]得到梯度。
我们直接保存平均$x^T$张量[k,m]，并执行批处理的matmul [k,m] x [m,n]获取梯度。

我已经阅读了 PyTorch 的源代码，但我在理解它时遇到了问题。]]></description>
      <guid>https://stackoverflow.com/questions/77673891/about-batched-effect-of-matmul-in-pytorch-when-to-reduce</guid>
      <pubDate>Sun, 17 Dec 2023 09:27:33 GMT</pubDate>
    </item>
    <item>
      <title>自定义神经网络输出层的梯度爆炸问题</title>
      <link>https://stackoverflow.com/questions/77673873/gradient-explosion-issue-with-custom-neural-network-output-layer</link>
      <description><![CDATA[我目前正在制作拟合函数来训练我的神经网络的自定义输出层。当将我的输出层拟合到数据集时，权重很快就会变得非常大。通过迭代 5-6，我已经获得了 Nan 值。
我尝试过提高学习率，但没有成功。我相当确定问题出在我用来计算调整权重步骤的公式中，因为它纯粹是我自己得出的，没有确认其正确性。
这是我的代码。我不确定是否应该包含 Neuron 类。
将 matplotlib 导入为 plt
从神经元导入*
导入时间
将 matplotlib.pyplot 导入为 plt

类输出层：
    def __init__(自身，input_shape，output_shape，activation_func=none)：
        神经元层 = np.array([])
        对于范围内的 x(output_shape)：
            神经元层 = np.append(神经元层,感知器(n_of_weights=input_shape, 步骤=activation_func, 激活=activation_func))
        self.neurons = 神经元_层
    def get_neuron(self, n):
        返回自身神经元[n]
    
    def get_neurons(self):
        返回自身神经元
    
    def set_neurons(self, adj_neurons):
        self.神经元 = adj_neurons
    
    defforward_pass（自身，X）：
        输出 = np.array([])
        对于 self.neurons 中的神经元：
            输出 = np.append(输出,neuron.step_pass(X))
        返回输出
    
    def relu（自身，输入）：
        如果输入&gt;0：
            返回输入
        别的：
            返回0
        
    def drelu（自身，输入）：
        如果输入&gt;0：
            返回1
        别的：
            返回0

    def sigmoid（自身，输入）：
        返回 1/(1+np.e**(-输入))

    def dsigmoid（自身，输入）：
        返回 self.sigmoid(输入)*(1-self.sigmoid(输入))
    
    def 失活（自身，神经元，输入）：
        激活=神经元.get_activation()
        如果激活==&#39;sigmoid&#39;：
            返回 dsigmoid(neuron.raw_pass(input))
        elif激活==&#39;relu&#39;：
            返回 drelu(neuron.raw_pass(input))
        别的：
            返回神经元.raw_pass(输入)
    
    def fit(自我, X, y, 学习率):
        Weight_change = [[w] for w in self.get_neurons()[0].get_weights()]
        错误 = [0]
        神经元 = self.get_neurons()
        对于范围内的 k(len(X))：
            对于神经元中的 n：
                权重 = n.get_weights()[:-1]
                print(&quot;通过：&quot;,self.forward_pass(X[k]))
                a = [(learning_rate*(self.forward_pass(X[k])-y[k])*self.dactivation(n, X[k])*X[k][x])[0] 对于 x 在范围内(长度(权重))]
                print(&quot;输出：&quot;,y[k])
                print(&quot;输入：&quot;,X[k])
                print(&quot;权重&quot;,n.get_weights())
                adj_weights = [权重[x]-(learning_rate*(self.forward_pass(X[k])-y[k])*self.dactivation(n, X[k])*X[k][x])[0 ] 对于范围内的 x(len(权重))]
                adj_weights.append(n.get_weights()[-1:][0]-(learning_rate*(self.forward_pass(X[k])-y[k])*self.dactivation(n, X[k])) [0]）
                n.change_weights(adj_weights)
                print(&#39;权重:&#39;,adj_weights)
                print(&#39;新通行证：&#39;,self.forward_pass(X[k]))
                对于范围内的 x(len(adj_weights))：
                    weight_change[x].append(adj_weights[x])
            error.append(y[k]-n.step_pass(X[k]))
            self.set_neurons(神经元)
        返回weight_change，错误```
]]></description>
      <guid>https://stackoverflow.com/questions/77673873/gradient-explosion-issue-with-custom-neural-network-output-layer</guid>
      <pubDate>Sun, 17 Dec 2023 09:19:07 GMT</pubDate>
    </item>
    <item>
      <title>我想解决一个机器学习问题，其中多个 Excel 文件中的每个文件代表不同的功能[关闭]</title>
      <link>https://stackoverflow.com/questions/77673612/i-want-to-solve-a-machine-learning-problem-where-each-of-a-number-of-excel-files</link>
      <description><![CDATA[我有一个机器学习问题，给我 6 个 Excel 文件，每个文件都有不同的特征作为输入，而我有一个包含输出的 Excel 工作表。以前我习惯只有一张包含所有输入和输出的 Excel 工作表，我很容易解决这个问题，但现在知道我有多个 Excel 数据文件，我该如何解决这个问题。我实际上不知道如何开始这个问题，因为这对我来说是全新的。]]></description>
      <guid>https://stackoverflow.com/questions/77673612/i-want-to-solve-a-machine-learning-problem-where-each-of-a-number-of-excel-files</guid>
      <pubDate>Sun, 17 Dec 2023 07:29:24 GMT</pubDate>
    </item>
    <item>
      <title>我刚开始使用 Streamlit for ML，在将其安装到桌面时遇到此错误。有什么建议吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77673471/im-new-to-using-streamlit-for-ml-and-getting-this-error-on-installing-it-deskto</link>
      <description><![CDATA[
我关闭了 Windows 防火墙，仍然无法下载 Streamlit 软件包
我使用的是 python 3.12.0 而不是 Anaconda
有什么帮助吗？
在 YouTube 或任何其他网站上几乎看不到任何解决方案
系统找不到指定的文件：&#39;C:\Python312\Scripts\f2py.exe&#39; -&gt; &#39;C:\Python312\Scripts\f2py.exe.deleteme&#39;
总是出现此错误
ㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤ ㅤㅤㅤㅤㅤㅤㅤ ㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤ ㅤㅤㅤㅤ]]></description>
      <guid>https://stackoverflow.com/questions/77673471/im-new-to-using-streamlit-for-ml-and-getting-this-error-on-installing-it-deskto</guid>
      <pubDate>Sun, 17 Dec 2023 06:13:39 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用 dataloader 来存储训练数据会改变模型的训练？</title>
      <link>https://stackoverflow.com/questions/77673297/why-does-the-usage-of-dataloader-for-train-data-change-the-training-of-the-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77673297/why-does-the-usage-of-dataloader-for-train-data-change-the-training-of-the-model</guid>
      <pubDate>Sun, 17 Dec 2023 04:17:54 GMT</pubDate>
    </item>
    <item>
      <title>我应该应用什么预处理来对雕刻文本执行 OCR？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77671624/what-preproccessing-should-i-apply-to-perform-an-ocr-on-an-engraved-text</link>
      <description><![CDATA[我正在尝试对下面有一些其他文本的雕刻数字执行 OCR（参见下图），我正在尝试读取数字并忽略黑色文本。

我尝试过 Google ML 套件文本识别，但它不起作用，我尝试过应用许多过滤器，例如灰度和颜色反转，但它也不起作用。我有一些关于使用 python 和 pytesseract 进行深度学习的线索，但我几乎可以肯定这也行不通。
我想知道，这里的方法是什么？我应该对图像应用任何特定的处理，还是这是一个机器学习问题？这还能解决吗？]]></description>
      <guid>https://stackoverflow.com/questions/77671624/what-preproccessing-should-i-apply-to-perform-an-ocr-on-an-engraved-text</guid>
      <pubDate>Sat, 16 Dec 2023 16:10:46 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试从头开始构建神经网络时出现矩阵乘法错误</title>
      <link>https://stackoverflow.com/questions/77671165/matrix-multiplication-error-when-i-tried-to-build-the-neural-network-from-scratc</link>
      <description><![CDATA[当我了解神经网络的数学工作原理时，我尝试使用 Numpy 从头开始​​构建它。我尝试构建的神经网络结构是具有 3 个节点的 input_layer -&gt; hidden_​​layer_1 有 2 个节点 -&gt; hidden_​​layer_2 有 2 个节点 -&gt;具有 1 个节点的输出层。两个隐藏层都使用ReLu激活函数，输出层使用Sigmoid激活函数。以下是数据集：https://drive.google.com/ file/d/1xP6BvneSdG5SoXL1ADp0veZMSrOcFlQc/view?usp=drive_link代码如下：
将 numpy 导入为 np
将 pandas 导入为 pd

W1 = np.random.rand(2, 3) # 权重矩阵
W2 = np.random.rand(2, 2)
W3 = np.random.rand(1, 2)

B1 = np.random.rand(2, 1) # 偏差向量
B2 = np.random.rand(2, 1)
B3 = np.random.rand(1, 1)

ReLu = lambda x : np.maximum(x, 0)
S 型 = lambda x : 1/ (1+np.exp(-x))

defforward_prop（输入）：

    Z1 = W1@输入 + B1
    A1 = ReLu(Z1)
    
    Z2 = W2@A1 + B2
    A2 = ReLu(Z2)
    
    Z3 = W3@A2 + B3
    A3 = 乙状结肠(Z3)
        
    返回 Z1、A1、Z2、A2、Z3、A3

d_relu = lambda x : x&gt;0
d_sigmoid = lambda x : np.exp(-x) / (1+np.exp(-x))**2

def back_prop(Z1, A1, Z2, A2, Z3, A3, X, Y):
    
    #衍生品
    dC_dA3 = 2*A3 - 2*Y
    
    dA3_dZ3 = d_sigmoid(Z3)
    dZ3_dW3 = A2
    dZ3_dB3 = 1
    dZ3_dA2 = W3
    
    dA2_dZ2 = d_relu(Z2)
    dZ2_dW2 = A1
    dZ2_dB2 = 1
    dZ2_dA1 = W2
    
    dA1_dZ1 = d_relu(Z1)
    dZ1_dW1 = X
    dZ1_dB1 = 1
    
    dC_dW3 = dC_dA3 @ dA3_dZ3 @ dZ3_dW3.T
    dC_dB3 = dC_dA3 @ dA3_dZ3 * dZ3_dB3
    dC_dA2 = dC_dA3 @ dA3_dZ3 @ dZ3_dA2
    dC_dW2 = dC_dA2 @ dA2_dZ2 @ dZ2_dW2.T
    dC_dB2 = dC_dA2 @ dA2_dZ2 * dZ2_dB2
    dC_dA1 = dC_dA2 @ dA2_dZ2 @ dZ2_dA1 # 问题就在这里
    dC_dW1 = dC_dA1 @ dA1_dZ1 @ dZ1_dW1.T
    dC_dB1 = dC_dA1 @ dA1_dZ1 * dZ1_dB1
    
    返回 dC_dW1、dC_dB1、dC_dW2、dC_dB2、dC_dW3、dC_dB3

数据 = pd.read_csv(&#39;light_dark_font_training_set.csv&#39;)
x = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/3)

n = x_train.shape[0]
L = 0.05
历元 = 100_000

对于范围内的 i（纪元）：

    idx = np.random.choice(n, 1, 替换=False)
    x_sample = x_train[idx].transpose()
    y_样本 = y_train[idx]
    
    z1、a1、z2、a2、z3、a3 =forward_prop(x_sample)
    w1、b1、w2、b2、w3、b3 = back_prop(z1、a1、z2、a2、z3、a3、x_sample、y_sample)
    
    W1 -= 长*w1
    W2 -= 长*w2
    W3 -= 长*w3
    
    B1 -= L*b1
    B2 -= L*b2
    B3 -= L*b3
    
    如果 i%10000 == 0:
        打印（一）

当我尝试运行上述程序时，它显示以下值错误，这表明矩阵维度与矩阵乘法不匹配。起初我以为我在反向传播中的偏导数方面遇到了一些问题，但就我的知识而言，它对我来说看起来很好。
为什么会发生这种情况以及如何解决？
&lt;小时/&gt;
ValueError Traceback（最近一次调用最后一次）
 中的 ~\AppData\Local\Temp/ipykernel_11036/3060626554.py
      6
      7 z1, a1, z2, a2, z3, a3 =forward_prop(x_sample)
----&gt; 8 w1, b1, w2, b2, w3, b3 = back_prop(z1, a1, z2, a2, z3, a3, x_sample, y_sample)
      9
     10 W1 -= 长*w1

~\AppData\Local\Temp/ipykernel_11036/566612658.py in back_prop(Z1, A1, Z2, A2, Z3, A3, X, Y)
     22 dC_dW2 = dC_dA2 @ dA2_dZ2 @ dZ2_dW2.T
     23 dC_dB2 = dC_dA2 @ dA2_dZ2 * dZ2_dB2
---&gt; 24 dC_dA1 = dC_dA2 @dA2_dZ2 @dZ2_dA1#问题就在这里
     25 dC_dW1 = dC_dA1 @ dA1_dZ1 @ dZ1_dW1.T
     26 dC_dB1 = dC_dA1 @ dA1_dZ1 * dZ1_dB1

ValueError: matmul: 输入操作数 1 的核心维度 0 不匹配，gufunc 签名为 (n?,k),(k,m?)-&gt;(n?,m?)（大小 2 与 1 不同）
]]></description>
      <guid>https://stackoverflow.com/questions/77671165/matrix-multiplication-error-when-i-tried-to-build-the-neural-network-from-scratc</guid>
      <pubDate>Sat, 16 Dec 2023 13:56:10 GMT</pubDate>
    </item>
    <item>
      <title>从时间和非时间数据进行预测[关闭]</title>
      <link>https://stackoverflow.com/questions/77667561/making-predictions-from-temporal-and-non-temporal-data</link>
      <description><![CDATA[我正在研究一个回归问题来预测由 2 个参数组成的目标。这两个参数将根据时间 (YYYY-MM-DD HH:MM) 和非时间数据组成的特征进行预测。
我开始基于“决策树回归”构建 Python 代码算法。尽管数据趋势看起来令人满意，但我的模型似乎产生了与用于训练/测试的数据范围相同的输出。我想知道我的方法好不好。我偶然发现了一些处理相同问题的论文，并使用 CNN-LSTM 模型等进行了解决。这里是我的代码（Google Colab 文件）的链接： https:// drive.google.com/file/d/1ar5Z8kMXx_9slc1e_MMzaAcQAvx5lmzr/view?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/77667561/making-predictions-from-temporal-and-non-temporal-data</guid>
      <pubDate>Fri, 15 Dec 2023 16:25:51 GMT</pubDate>
    </item>
    <item>
      <title>如何为此笔记本创建 Sagemaker 端点？</title>
      <link>https://stackoverflow.com/questions/77660996/how-can-i-create-a-sagemaker-endpoint-for-this-notebook</link>
      <description><![CDATA[我创建了一个 VectorDB (FAISS) 并将 PDF 输入到其中。然后我使用 AWS Bedrock 的 Langchain 包装器来调用它。我知道现在存在 Kowledge Base，但至少在 SageMaker 笔记本中，我有更多的控制权。该模型在 SageMaker Notebook 中完美运行，当我提出问题时，它会返回答案。
我想做的是创建一个小网页（并通过 HTTP/REST API），只需在文本字段中提交问题并在文本字段中接收答案。我猜如果链中某个地方没有 Lambda 函数，这很难做到，或者也许不是？
当我查看 Sagemaker 控制台的推理选项卡下时，没有模型或没有端点，或者没有&lt; /strong&gt; 端点配置（因为我没有从 Sagemaker 选择模型，所以我只是在 Python 笔记本中使用 langchain LLM 和 Bedrock，如下所示）。
&lt;前&gt;&lt;代码&gt;导入boto3
导入 json

bedrock = boto3.client(service_name=&quot;bedrock&quot;)
bedrock_runtime = boto3.client(service_name=“bedrock-runtime”)



从 langchain.llms.bedrock 导入 Bedrock
从 langchain.chains 导入 RetrievalQA
从 langchain.prompts 导入 PromptTemplate

嵌入 = BedrockEmbeddings(model_id=“amazon.titan-embed-text-v1”,
                               客户端=bedrock_runtime）

最终我将文档嵌入到 FAISS Vector 数据库中，我查询的就是这个数据库
db = FAISS.from_documents（文档，嵌入）


模型泰坦 = {
    “最大令牌计数”：512，
    “停止序列”：[]，
    “温度”：0.0，
    “顶部P”：0.5
}

# 亚马逊泰坦模型
llm = 基岩(
    model_id=&quot;amazon.titan-text-express-v1&quot;,
    客户端=bedrock_runtime，
    model_kwargs=model_titan,
）

然后定义一个提示......
提示 = 提示模板(
    template=prompt_template, input_variables=[“上下文”, “问题”]
）

并查询数据库：
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=“东西”，
    检索器=db.as_retriever(
        search_type=“相似度”，
    ),
    return_source_documents=真，
    chain_type_kwargs={“提示”: 提示},
）



query =“未来的技术是什么样的？”

结果 = qa({“查询”: 查询})

print(f&#39;查询: {结果[“查询”]}\n&#39;)
print(f&#39;结果: {结果[“结果”]}\n&#39;)
print(f&#39;上下文文档：&#39;)
对于结果 [“source_documents”] 中的 srcdoc：
      打印（f&#39;{srcdoc}\n&#39;）

这恰好返回了我在 Sagemaker 中需要的内容，我只需要从外部查询数据库即可。
我不想让 lambda 函数每次都重建链。我考虑的是效率，我需要的只是在 lambda 函数中传递查询并返回结果。]]></description>
      <guid>https://stackoverflow.com/questions/77660996/how-can-i-create-a-sagemaker-endpoint-for-this-notebook</guid>
      <pubDate>Thu, 14 Dec 2023 14:49:20 GMT</pubDate>
    </item>
    </channel>
</rss>