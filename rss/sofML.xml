<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 06 Oct 2024 12:30:30 GMT</lastBuildDate>
    <item>
      <title>NEAT-python：如何找到/改变神经元的潜力？（试图播种/直接修改种群）</title>
      <link>https://stackoverflow.com/questions/79059007/neat-python-how-to-find-change-the-potential-of-a-neuron-trying-to-seed-direc</link>
      <description><![CDATA[我正在使用 neat-python。我试图让 CTRNN 模拟正弦函数，并希望使用 CTRNN 演示神经元作为种子。然而，我尝试修改的神经元完全是非确定性的；除非我在 pop=neat.Population(config) 之前放置种子，否则每次运行代码时它们的行为都会改变。我唯一没有找到的值是“潜在”或神经元的衰减（如本CTRNN 文档所述。我查看了 github 源代码、配置文档，并使用 dir 命令查看了各种 Python 对象的对象描述。
这是具体代码，带有确定性的种子），使用config-ctrnn 来自单极平衡。
import neat
import numpy as np
import matplotlib.pyplot as plt
import random

simulationSteps = 100
timeConst = 1/100
goalFunc = lambda step: np.sin(step*25)

config = neat.Config(
neat.DefaultGenome,
neat.DefaultReproduction,
neat.DefaultSpeciesSet,
neat.DefaultStagnation,
r&quot;path_to\CTRNNconfig&quot;
)

random.seed(5)
pop = neat.Population(config)
pop.add_reporter(neat.StdOutReporter(False))

# 播种
for i in range(100, 200):
gen = pop.population[i+1]
n1, n2 = list(gen.nodes.keys())[1:]
for node, bias in zip((n1,n2), (-2.75/5, -1.75/5)):
# gen.nodes[node].aggregation = sum
# gen.nodes[node].activation = sigmoid_activation
gen.nodes[node].bias = bias
gen.nodes[node].response = 1
gen.add_connection(config.genome_config, n1, n1, 0.9, True)
gen.add_connection(config.genome_config, n1, n2, 0.2, True)
gen.add_connection(config.genome_config, n2, n1,-0.2, True)
gen.add_connection(config.genome_config, n2, n2, 0.9, True)

# 仅绘制最后一个种子基因组
network = neat.ctrnn.CTRNN.create(gen, config, timeConst)
network.set_node_value(n1, 0)
network.set_node_value(n2, 0)
network.set_node_value(0, 0.5)

actions = []
correct = []
for step in range(simulationSteps):
action = network.advance(
[], 
timeConst, 
timeConst,
)
action.append(action[0])
correct.append(goalFunc(step)/3 + 0.5)

plt.plot(actions, label=&quot;Network&quot;)
plt.plot(correct, label=&quot;Goal&quot;)
plt.legend()
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79059007/neat-python-how-to-find-change-the-potential-of-a-neuron-trying-to-seed-direc</guid>
      <pubDate>Sun, 06 Oct 2024 11:52:31 GMT</pubDate>
    </item>
    <item>
      <title>无法运行 Github 存储库</title>
      <link>https://stackoverflow.com/questions/79058738/not-able-to-run-github-repository</link>
      <description><![CDATA[我的项目是“社交媒体中的多模态命名实体识别”（尝试将其扩展为聊天机器人）
我正尝试使用这个 github 存储库作为基础
https://github.com/Multimodal-NER/RpBERT
尽管尝试了几次，我还是无法运行它。
有人可以帮我吗？你能告诉我如何实现它的步骤吗？
我已经将它克隆到我的系统中。我的系统中的 loader.py 文件出现错误，但不知何故修复了它。我运行了 main.py 文件。它似乎正在运行。但我仍然有几个疑问。我想知道如何从头开始实现它。]]></description>
      <guid>https://stackoverflow.com/questions/79058738/not-able-to-run-github-repository</guid>
      <pubDate>Sun, 06 Oct 2024 09:38:24 GMT</pubDate>
    </item>
    <item>
      <title>分类神经网络不收敛[关闭]</title>
      <link>https://stackoverflow.com/questions/79058176/classification-neural-network-not-converging</link>
      <description><![CDATA[我为 MNIST 开发了一个基本的分类网络，但在训练期间，验证准确率在 10% 左右。我尝试过各种优化器（SGD、Adam、Nadam）以及不同的学习率（0.1、1e-3、1e-4、1e-5），但验证准确率在每个时期都保持在 10% 左右。
这是我的代码：
import tensorflow as tf
from tensorflow import keras

(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()
X_train, X_val = X_train[5000:]/255.0, X_train[:5000]/255.0
y_train, y_val = y_train[5000:]/255.0, y_train[:5000]/255.0

class_NN = keras.models.Sequential([
keras.layers.Input(shape = [28, 28]),
keras.layers.Flatten(),
keras.layers.Dense(300, 激活 = &quot;relu&quot;),
keras.layers.Dense(100, 激活 = &quot;relu&quot;),
keras.layers.Dense(10, 激活 = &quot;softmax&quot;)
])

class_NN.compile(loss = &quot;sparse_categorical_crossentropy&quot;,
optimizer = keras.optimizers.SGD(learning_rate = 1e-3), 
metrics = [&quot;accuracy&quot;])
class_NN.fit(X_train, y_train, epochs = 15,
validation_data = (X_val, y_val))

我在写这道题的时候，在开始训练之前从代码中去掉了/255.0。目前，验证的准确率随着训练的进行而提高。我的验证准确率达到了 96% 左右。是什么原因导致在训练过程中去掉/255.0后验证准确率有所提高？
X_train, X_val = X_train[5000:], X_train[:5000]
y_train, y_val = y_train[5000:], y_train[:5000]
]]></description>
      <guid>https://stackoverflow.com/questions/79058176/classification-neural-network-not-converging</guid>
      <pubDate>Sun, 06 Oct 2024 00:55:12 GMT</pubDate>
    </item>
    <item>
      <title>如何创建用于算法交易的人工智能机器人？[关闭]</title>
      <link>https://stackoverflow.com/questions/79058024/how-to-create-an-ai-bot-for-algorithmic-trading</link>
      <description><![CDATA[我们想创建一个基于人工智能的机器人来预测场外市场（二元期权）的下一个一分钟蜡烛图。
在开始之前，我们还有一些疑问，例如：

该机器人是基于人工智能的，但最合适的方法是什么？例如，我们应该使用神经网络吗？如果是，哪种类型？（LSTM 等）。我们应该考虑 SVM 吗？我也看过应用模式识别技术的文档。哪种技术最适合这种类型的问题？

由于我们使用的是一分钟蜡烛图，我们知道训练将非常耗时，并且需要大量的计算能力。我们可以在哪里训练模型？（我在考虑 AWS 或类似的云服务）。


我们不是在寻找一个具体的答案，而是要了解哪些条件和解决方案最适合以最佳方式实现这一目标。]]></description>
      <guid>https://stackoverflow.com/questions/79058024/how-to-create-an-ai-bot-for-algorithmic-trading</guid>
      <pubDate>Sat, 05 Oct 2024 22:13:56 GMT</pubDate>
    </item>
    <item>
      <title>在二元分类数据上拟合神经网络模型的问题</title>
      <link>https://stackoverflow.com/questions/79057502/problem-with-fitting-a-neural-network-model-on-binary-classification-data</link>
      <description><![CDATA[我有一个包含 280 个样本的数据集，其中有 20 个特征和 0.1 个结果。我缩放了它们。
此外，还有三个神经网络模型来训练它们的数据。
但是在循环模型时，我收到了拟合错误。
如何解决？
import numpy as np
import math

import tensorflow as tf

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler,PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.optimizers import Adam

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import密集

model1 = Sequential([
密集(单位=25, 激活=&#39;relu&#39;),
密集(单位=15, 激活=&#39;relu&#39;),
密集(单位=1, 激活=&#39;线性&#39;)
])
model2 = Sequential([
密集(单位=20, 激活=&#39;relu&#39;),
密集(单位=12, 激活=&#39;relu&#39;),
密集(单位=12, 激活=&#39;relu&#39;),
密集(单位=20, 激活=&#39;relu&#39;),
密集(单位=1, 激活=&#39;线性&#39;)
])
model3 = Sequential([
密集(单位=32, 激活=&#39;relu&#39;),
密集(单位=16, 激活=&#39;relu&#39;),
密集(单位=8, 激活=&#39;relu&#39;),
密集(单位=4, 激活=&#39;relu&#39;),
Dense(units=12,activation=&#39;relu&#39;),
Dense(units=1,activation=&#39;linear&#39;)
])

nn_train_error = []
nn_cv_error = []

models_bc = [model1, model2, model3]
for model in models_bc:

# 设置损失和优化器
model.compile(
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
)

print(f&quot;Training {model.name}...&quot;)
# 训练模型
model.fit(x_bc_train_scaled, y_bc_train, epochs=100, verbose=0)
print(&quot;Done!\n&quot;)

# 设置阈值用于分类
threshold = 0.5

# 记录训练集错误分类示例的比例
yhat = model.predict(x_bc_train_scaled)
yhat = tf.math.sigmoid(yhat)
yhat = np.where(yhat &gt;= Threshold, 1, 0)# np 中的 where：条件函数
train_error = np.mean(yhat != y_bc_train)# np 中的 mean：显示错误分类的百分比

nn_train_error.append(train_error)

# 记录交叉验证集错误分类示例的比例
yhat = model.predict(x_bc_cv_scaled)
yhat = tf.math.sigmoid(yhat)
yhat = np.where(yhat &gt;= Threshold, 1, 0)
cv_error = np.mean(yhat != y_bc_cv)

nn_cv_error.append(cv_error)

print(nn_train_error)
print(nn_cv_error)

输出：
 ValueError Traceback（最近一次调用
最后）单元格 In\[109\]，第 15 行 13 print(f&quot;Training
{model.name}...&quot;) 14 # 训练模型 ---\&gt; 15
model.fit(x_bc_train_scaled, y_bc_train, epochs=100, verbose=0) 
16 print(&quot;Done!\\n&quot;)

Sequential.call() 接收的参数：
• input=tf.Tensor(shape=(None, 20), dtype=float32)
• training=True 
• mask=None
]]></description>
      <guid>https://stackoverflow.com/questions/79057502/problem-with-fitting-a-neural-network-model-on-binary-classification-data</guid>
      <pubDate>Sat, 05 Oct 2024 16:52:02 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：无法将 NumPy 数组转换为 Tensor（不支持的对象类型 csr_matrix）</title>
      <link>https://stackoverflow.com/questions/79057484/valueerror-failed-to-convert-a-numpy-array-to-a-tensor-unsupported-object-type</link>
      <description><![CDATA[我尝试运行：
import numpy as np
import pandas as pd
import tensorflow as tf
import numpy as np

从 tensorflow.keras 导入 Sequential
从 tensorflow.keras.layers 导入 Dense、Embedding、GlobalAveragePooling1D
从 tensorflow.keras.layers 导入 TextVectorization
从 sklearn.model_selection 导入 train_test_split
从 tensorflow 导入 keras

从 nltk.tokenize.treebank 导入 TreebankWordTokenizer、TreebankWordDetokenizer
从 sklearn.feature_extraction.text 导入CountVectorizer

dataf=pd.read_csv(&#39;D:/datafile.csv&#39;)
data=pd.read_csv(&quot;D:/dataset1c2f4b7/dataset/train.csv&quot;,encoding=&#39;latin-1&#39;)
l=[]
对于 dataf[&#39;text&#39;] 中的 a:
l.append(a)
m=[]
对于 dataf[&#39;target&#39;] 中的 a:
m.append(a)

X_train, X_test, y_train, y_test = train_test_split(l, m, test_size=0.2, random_state=42)

vectorizer = CountVectorizer()
vectorizer.fit(X_train)
X_train = vectorizer.transform(X_train)
X_test = vectorizer.transform(X_test)
X_train=np.array(X_train)
X_test=np.array(X_test)
y_train=np.array(y_train)
y_test=np.array(y_test)
print(X_train)
model = keras.models.Sequential() 
model.add(keras.layers.Embedding(10000, 128)) 
model.add(keras.layers.SimpleRNN(64, return_sequences=True)) 
model.add(keras.layers.SimpleRNN(64)) 
model.add(keras.layers.Dense(128,activation=&quot;relu&quot;)) 
model.add(keras.layers.Dropout(0.4)) 
model.add(keras.layers.Dense(1,激活=“sigmoid”）） 
model.summary() 

model.compile(“rmsprop”， 
“binary_crossentropy”， 
metrics=[“accuracy”])
model.fit(X_train, y_train,epochs=5,verbose=False,validation_data=(X_test, y_test),batch_size=10)
model.save(&#39;gfgModel.h5&#39;) 
tf.saved_model.save(model, &#39;one_step 05&#39;)

这显示
ValueError：无法将 NumPy 数组转换为 Tensor（不支持的对象类型 csr_matrix）

我正在尝试创建一个文本分类器。
我只是期待要训​​练的模型，因为所有内容都是数组形式。]]></description>
      <guid>https://stackoverflow.com/questions/79057484/valueerror-failed-to-convert-a-numpy-array-to-a-tensor-unsupported-object-type</guid>
      <pubDate>Sat, 05 Oct 2024 16:43:01 GMT</pubDate>
    </item>
    <item>
      <title>Azure AI | 机器学习实时推理</title>
      <link>https://stackoverflow.com/questions/79057387/azure-ai-machine-learning-real-time-inference</link>
      <description><![CDATA[我创建了一个 Azure ML 实验（带有拖放组件），其实际行为就像一个简单的 Web 服务。
基本上，我根据数据集上的输入（而不是使用经过训练的机器学习模型）进行查询，然后将行作为输出返回。
我创建了一个类似于 SQL 查询系统的管道，但它没有按预期工作。我尝试了几种方法来实现这一点（应用 SQL 转换、连接数据、执行 Python 脚本），但都没有成功。输出始终为空。
我不明白我做错了什么。
在所附图片/场景中，我的 t1 数据集包含超过 4500 行，具有以下字段：
PROC_CODE
FIELD_1
FIELD_2
FIELD_3
我的 t2（数据集中选择列的输出）仅包含 PROC_CODE。
错误显示：
ModuleExceptionMessage:InvalidSQLScript: SQL 查询“SELECT * FROM t1 WHERE t1.PROC_CODE = t2.PROC_CODE”不正确。异常消息：（sqlite3.OperationalError）没有这样的列：t2.PROC_CODE

[SQL：SELECT * FROM t1 WHERE t1.PROC_CODE = t2.PROC_CODE]
我想知道这（与类似 SQL 的查询系统类似的管道）是否可行，然后解决问题。]]></description>
      <guid>https://stackoverflow.com/questions/79057387/azure-ai-machine-learning-real-time-inference</guid>
      <pubDate>Sat, 05 Oct 2024 15:48:19 GMT</pubDate>
    </item>
    <item>
      <title>如何将有限列表中的元素作为输入传递？</title>
      <link>https://stackoverflow.com/questions/79057233/how-to-pass-an-element-from-a-limited-list-as-input</link>
      <description><![CDATA[我拥有“石头剪刀布”游戏中手臂不同状态的汇编。我的目的是以类似的方式对这些类别进行编程。
[1, 0, 0] - 石头 
[0, 1, 0] - 布 
[0, 0, 1] - 剪刀

有没有方便的自动方法？
我使用了嵌入层，但我不确定它是否合适。]]></description>
      <guid>https://stackoverflow.com/questions/79057233/how-to-pass-an-element-from-a-limited-list-as-input</guid>
      <pubDate>Sat, 05 Oct 2024 14:23:29 GMT</pubDate>
    </item>
    <item>
      <title>nnUNetv2_plan_and_preprocess：未找到命令</title>
      <link>https://stackoverflow.com/questions/79054923/nnunetv2-plan-and-preprocess-command-not-found</link>
      <description><![CDATA[在程序中，U-Mamba，
当我运行
nnUNetv2_plan_and_preprocess -d 701 --verify_dataset_integrity

它显示
nnUNetv2_plan_and_preprocess：未找到命令

我的运行环境：
Ubuntu 20.04.6 LTS (GNU/Linux 5.15.0-101-generic x86_64)
RTX 2080ti，RTX 3090
这些事情已经完成：

conda activate env-...
torch with cuda
causal-conv1d
mamba-ssm
umamba pip install -e .
nnunetv2 pip install -e .
nnU-Net 数据集格式
导出 nnUNet_raw、nnUNet_preprocessed、nnUNet_results
]]></description>
      <guid>https://stackoverflow.com/questions/79054923/nnunetv2-plan-and-preprocess-command-not-found</guid>
      <pubDate>Fri, 04 Oct 2024 15:22:50 GMT</pubDate>
    </item>
    <item>
      <title>多模态命名实体识别</title>
      <link>https://stackoverflow.com/questions/79054866/multimodal-named-entity-recognition</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79054866/multimodal-named-entity-recognition</guid>
      <pubDate>Fri, 04 Oct 2024 15:08:57 GMT</pubDate>
    </item>
    <item>
      <title>MLFlow 在提供模型时抛出错误：配置的跟踪 uri 方案：“文件”对于代理 mlflow-artifact 方案的使用无效</title>
      <link>https://stackoverflow.com/questions/78266509/mlflow-throws-and-error-when-serving-a-model-the-configured-tracking-uri-scheme</link>
      <description><![CDATA[我尝试使用 MLFlow CLI 在本地将模型作为 REST 端点提供服务。MLflow 版本为 2.11.3。以下是我使用的参数。
mlflow models serve -m &quot;runs:/7782c4a700cc49c1a04f9ce608d90358/knnmodel&quot; --env-manager local --port 5000

以下是例外情况：
回溯（最近一次调用最后一次）：
文件 &quot;/home/vagrant/.local/bin/mlflow&quot;，第 8 行，位于 &lt;module&gt;
sys.exit(cli())
文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第 1157 行，在 __call__ 中
return self.main(*args, **kwargs)
文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第 1078 行，在 main 中
rv = self.invoke(ctx)
文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第 1688 行，在invoke 中
return _process_result(sub_ctx.command.invoke(sub_ctx))
文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第1688，在invoke中
返回_process_result（sub_ctx.command.invoke（sub_ctx））
文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第1434行，在invoke中
返回ctx.invoke（self.callback，**ctx.params）
文件“/home/vagrant/.local/lib/python3.8/site-packages/click/core.py”，第783行，在invoke中
返回__callback（*args，**kwargs）
文件“/home/vagrant/.local/lib/python3.8/site-packages/mlflow/models/cli.py”，第104行，在serve中
返回get_flavor_backend（
文件&quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/models/flavor_backend_registry.py&quot;，第 44 行，在 get_flavor_backend
local_path = _download_artifact_from_uri(
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/tracking/artifact_utils.py&quot;，第 105 行，在 _download_artifact_from_uri
返回 get_artifact_repository(artifact_uri=root_uri).download_artifacts(
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;，第 124 行，在 get_artifact_repository
返回_artifact_repository_registry.get_artifact_repository(artifact_uri)
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;，第 77 行，位于 get_artifact_repository
return storage(artifact_uri)
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/runs_artifact_repo.py&quot;，第 27 行，位于 __init__
self.repo = get_artifact_repository(uri)
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;，第 124 行，在 get_artifact_repository 中
返回 _artifact_repository_registry.get_artifact_repository(artifact_uri)
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/artifact_repository_registry.py&quot;，第 77 行，在 get_artifact_repository 中
返回 storage(artifact_uri)
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py&quot;，第 45 行，在 __init__ 中
super().__init__(self.resolve_uri(artifact_uri, get_tracking_uri()))
文件&quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py&quot;，第 59 行，在 resolve_uri
_validate_uri_scheme(track_parse.scheme)
文件 &quot;/home/vagrant/.local/lib/python3.8/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py&quot;，第 35 行，在 _validate_uri_scheme
raise MlflowException(
mlflow.exceptions.MlflowException: 配置的跟踪 uri 方案：&#39;file&#39; 无法与代理 mlflow-artifact 方案一起使用。允许的跟踪方案为：{&#39;https&#39;, &#39;http&#39;}

似乎 MLflow 无法从 MLflow Web 服务器访问工件。有解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78266509/mlflow-throws-and-error-when-serving-a-model-the-configured-tracking-uri-scheme</guid>
      <pubDate>Wed, 03 Apr 2024 09:02:38 GMT</pubDate>
    </item>
    <item>
      <title>在 FastAPI 中使用机器学习时出现“TypeError：float（）参数必须是字符串或数字，而不是‘PatientAttendance’”[关闭]</title>
      <link>https://stackoverflow.com/questions/71606629/getting-typeerror-float-argument-must-be-a-string-or-a-number-not-patienta</link>
      <description><![CDATA[我目前正在使用 FastAPI 构建 API 来部署我的逻辑回归模型。由于某种原因，我在测试模型时在服务器文档中收到上述错误。
我的代码如下：
app = FastAPI()

class PatientAttendance(BaseModel):
apptslotduration: int
patientage: int
log_distance: float
pct_appts_missed: float
doc_no_show_rate: float
zip_no_show_rate: float
note_no_show_rate: float
type_no_show_rate: float
spec_type_no_show_rate: float
monthly_no_show_rate: float
seasonal_no_show_rate: float
dow_no_show_rate: float
clinic_no_show_rate: float
lead_time_in_days: int
groupedstarttime: int
priminsurance_no_show_rate:浮点数
secondinsurance_no_show_rate：浮点数

@app.post(&#39;/predict/&#39;)
def predict(features：PatientAttendance)：
data = features
prediction = model.predict([[data]])
if prediction[0] == 0:
result = &quot;Patient Show&quot;
else:
result = &quot;No-Show&quot;
probability = model.predict_proba([[data]])

return {
&#39;prediction&#39;: prediction,
&#39;probability&#39;: probability
}

if __name__ == &#39;__main__&#39;:
uvicorn.run(app, host=&quot;127.0.0.1&quot;, port=8000)

错误：
TypeError: float() 参数必须是字符串或数字，而不是 &#39;PatientAttendance&#39;

我正在使用 Pydantic BaseModel，但我不知道为什么会收到此错误。我相信我的应用程序相对于服务器指向了正确的方向。我尝试使用 GET 和 POST。features 是我标准化并转换为字典的数据集中的特征数组。所有功能都已矢量化。每当我在服务器文档中测试 API 时，我似乎总是会遇到某种类型的错误。]]></description>
      <guid>https://stackoverflow.com/questions/71606629/getting-typeerror-float-argument-must-be-a-string-or-a-number-not-patienta</guid>
      <pubDate>Thu, 24 Mar 2022 17:08:12 GMT</pubDate>
    </item>
    <item>
      <title>错误：尝试在自定义 HF 数据集上使用 trainer.train() 时，vars() 参数必须具有 __dict__ 属性？</title>
      <link>https://stackoverflow.com/questions/69539538/error-vars-argument-must-have-dict-attribute-when-trying-to-use-trainer-t</link>
      <description><![CDATA[我有以下正在尝试微调的模型（CLIP_ViT + 分类头）。这是我的模型定义：
class CLIPNN(nn.Module):

def __init__(self, num_labels, pretrained_name=&quot;openai/clip-vit-base-patch32&quot;, dropout=0.1):
super().__init__()
self.num_labels = num_labels
# 加载预训练的转换器 &amp;处理器
self.transformer = CLIPVisionModel.from_pretrained(pretrained_name)
self.processor = CLIPProcessor.from_pretrained(pretrained_name)
# 初始化其他层（transformer 主体之后的头部）
self.classifier = nn.Sequential(
nn.Linear(512, 128, bias=True),
nn.ReLU(inplace=True),
nn.Dropout(p=dropout, inplace=False),
nn.Linear(128, self.num_labels, bias=True))

def forward(self, input, labels=None, **kwargs):
logits = self.classifier(inputs)
loss = None
if labels 不是 None:
loss_fct = nn.CrossEntropyLoss()
loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

return SequenceClassifierOutput(
loss=loss,
logits=logits,
)

我还对数据集进行了以下定义：
class CLIPDataset(nn.utils.data.Dataset):
def __init__(self, embeddings, labels):
self.embeddings = embeddings
self.labels = labels

def __getitem__(self, idx):
item = {&quot;embeddings&quot;: nn.Tensor(self.embeddings[idx])}
item[&#39;labels&#39;] = nn.LongTensor([self.labels[idx]])
return item

def __len__(self):
return len(self.labels)


注意：这里我假设模型输入的是预先计算的嵌入，而不是计算嵌入，我知道这是如果我想微调 CLIP 基础模型，那么这不是正确的逻辑，我只是​​想让我的代码工作。
类似这样的事情会引发错误：
model = CLIPNN(num_labels=2)
train_data = CLIPDataset(train_data, y_train)
test_data = CLIPDataset(test_data, y_test)

trainer = Trainer(
model=model, args=training_args, train_dataset=train_data, eval_dataset=test_data
)
trainer.train()


TypeError Traceback (most recent call last) in
----&gt; 1 trainer.train()
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py
in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval,
**kwargs) 1256 self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)
1257 → 1258 for step, input in enumerate(epoch_iterator): 1259 1260 #
如果恢复训练，则跳过任何已经训练过的步骤
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py in next(self) 515 if self._sampler_iter为 None: 516 self._reset() →
517 data = self._next_data() 518 self._num_yielded += 1 519 if
self._dataset_kind == _DatasetKind.Iterable and \
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self) 555 def _next_data(self): 556 index =
self._next_index() # 可能引发 StopIteration → 557 data =
self._dataset_fetcher.fetch(index) # 可能引发 StopIteration 558 if
self._pin_memory: 559 data = _utils.pin_memory.pin_memory(数据)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py​​
在 fetch(self, perhaps_batched_index) 45 else: 46 data =
self.dataset[possibly_batched_index] —&gt; 47 return
self.collat​​e_fn(数据)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/data/data_collat​​or.py
在 default_data_collat​​or(features, return_tensors) 64 65 if
return_tensors == “pt”: —&gt; 66 返回
torch_default_data_collat​​or(features) 67 elif return_tensors == “tf”:
68 返回 tf_default_data_collat​​or(features)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/data/data_collat​​or.py
在 torch_default_data_collat​​or(features) 中 80 81 如果不是
isinstance(features[0], (dict, BatchEncoding)): —&gt; 82 features =
[vars(f) for f in features] 83 first = features[0] 84 batch = {
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/data/data_collat​​or.py
in (.0) 80 81 if not isinstance(features[0], (dict, BatchEncoding)):
—&gt; 82 features = [vars(f) for f in features] 83 first = features[0] 84
batch = {
TypeError: vars() 参数必须具有 dict 属性

知道我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/69539538/error-vars-argument-must-have-dict-attribute-when-trying-to-use-trainer-t</guid>
      <pubDate>Tue, 12 Oct 2021 11:12:03 GMT</pubDate>
    </item>
    <item>
      <title>错误 conda.core.link:_execute(698): 安装包“defaults::icu-58.2-ha925a31_3”时发生错误</title>
      <link>https://stackoverflow.com/questions/63871492/error-conda-core-link-execute698-an-error-occurred-while-installing-package</link>
      <description><![CDATA[我使用 anaconda prompt conda create -n talkingbot python=3.5 创建了环境，然后安装了 pip install tensorflow==1.0.0（遵循与 udemy 课程中使用的相同命令），但是当我尝试使用 conda install spyder 安装 spyder 时，它给了我这个错误：
准备交易：完成
验证交易：完成
执行交易：完成
错误 conda.core.link:_execute(698)：安装包“defaults::icu-58.2-ha925a31_3”时发生错误。
回滚事务：完成

[Errno 13] 权限被拒绝：&#39;C:\\Users\\Lenovo\\anaconda3\\envs\\talkingbot\\Library\\bin\\icudt58.dll&#39;
()

然后我尝试使用 anaconda navigator 安装 spyder，但 spyder 也未安装。
帮我解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/63871492/error-conda-core-link-execute698-an-error-occurred-while-installing-package</guid>
      <pubDate>Sun, 13 Sep 2020 13:42:24 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中用列的平均值填补缺失值</title>
      <link>https://stackoverflow.com/questions/60363476/impute-missing-values-with-mean-of-column-in-machine-learning</link>
      <description><![CDATA[我知道，估算缺失值就是字面意思，我说的是用列的平均值估算缺失值。我通常在将数据拆分为训练和测试之前估算缺失值，但后来我看到了这个QnA，上面写着 

注意：如果您想将其用于机器学习/数据科学：从数据科学的角度来看，首先替换 NA 然后拆分为训练和测试是错误的……您必须首先拆分为训练和测试，然后用训练中的平均值替换 NA，然后将这个有状态的预处理模型应用于测试，请参阅下面涉及 sklearn 的答案！– Fabian Werner 2019 年 8 月 28 日 9:18

这是什么意思？我们能做到吗？我们怎么做？在分割数据之前或之后做这件事有什么不同吗？如果有，为什么？请帮我理解，因为我对这件事很困惑。]]></description>
      <guid>https://stackoverflow.com/questions/60363476/impute-missing-values-with-mean-of-column-in-machine-learning</guid>
      <pubDate>Sun, 23 Feb 2020 14:58:00 GMT</pubDate>
    </item>
    </channel>
</rss>