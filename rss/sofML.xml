<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 31 Mar 2025 18:25:11 GMT</lastBuildDate>
    <item>
      <title>如何提高机器学习模型的准确性[关闭]</title>
      <link>https://stackoverflow.com/questions/79546591/how-to-increase-the-accuracy-of-a-machine-learning-model</link>
      <description><![CDATA[我正在尝试训练一个模型，以通过我从Kaggle发现的情感文本数据库进行文本进行情感检测。目前，我正在使用验证的模型：“ distilbert-base-base uncord” 。但是，准确性太低为0.5。我正在尝试学习一种增加此方法以进行更好估计的方法。
任何人可以帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/79546591/how-to-increase-the-accuracy-of-a-machine-learning-model</guid>
      <pubDate>Mon, 31 Mar 2025 15:08:39 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：数据加载器对象不可订阅</title>
      <link>https://stackoverflow.com/questions/79546578/typeerror-dataloader-object-is-not-subscriptable</link>
      <description><![CDATA[我正在创建一个AI模型来生成人群的密度图。将数据集分为两个，一个用于培训，一个用于验证，我创建了两个数据集，然后尝试使用 torch.utils.data.dataloader（test_set，batch_size = batch_size = batch_size，shuffle = false））。之后，要测试数据，我迭代并使用下一个函数获取数据集的下一个元素，然后获得TypeError。
我正在使用Kaggle的数据集从Kaggle使用：这是完整的代码：
  batch_size = 8 
设备=&#39;cuda：0&#39;如果torch.cuda.is_available（）else&#39;cpu&#39;

train_root_dir =＆quot; data/part_a/train_data/＆quot
init_training_set = dataloader（train_root_dir，shuffle = true）

＃将培训集的一部分分为验证集
train_size = int（0.9 * len（init_training_set））
val_size = len（init_training_set）-train_size

train_indices = list（range（train_size））
val_indices = list（range（train_size，len（init_training_set））））））
train_dataset = torch.utils.data.dataset.subset（init_training_set，train_indices）
val_dataset = torch.utils.data.dataset.subset（init_training_set，val_indices）

train_loader = torch.utils.data.dataloader（train_dataset，batch_size = batch_size，shuffle = true）
val_loader = torch.utils.data.dataloader（val_dataset，batch_size = batch_size，shuffle = false）

test_root_dir =＆quot; data/part_a/test_data/＆quot
test_set = dataloader（test_root_dir，shuffle = false）
test_loader = torch.utils.data.dataloader（test_set，batch_size = batch_size，shuffle = false）

dataiter = iter（train_loader）
ex_images，ex_dmaps，ex_n_people = next（dataiter）


＃显示图像和密度图
plot_corresponding_pairs（ex_images，ex_dmaps）
 
具体错误是：
  trackback（最近的最新通话）：
 第61行，in＆lt;模块＆gt;
    对于ex_images，ex_dmaps，ex_n_people in train_loader中
typeError：“数据加载程序”对象不可订阅
 ]]></description>
      <guid>https://stackoverflow.com/questions/79546578/typeerror-dataloader-object-is-not-subscriptable</guid>
      <pubDate>Mon, 31 Mar 2025 15:02:15 GMT</pubDate>
    </item>
    <item>
      <title>与Skywise -Slate仪表板相关的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79545514/issue-related-to-skywise-slate-dashboard</link>
      <description><![CDATA[我正在努力修改空客Skywise中现有的仪表板。我的任务涉及：
 提取数据： 
用户单击“获取”时按钮，系统应从用户指定的列中获取数据并将其存储以供以后使用。
 手动描述输入： 
在“手动描述”中单元格，用户将手动粘贴来自空中客车数据库的文本，对应于特定的FUID ID。
 生成TDD评估： 
用户点击“生成评估”时，系统应：

使用先前获取的数据和手册说明
参考。
利用ML/AI模型来自动生成TDD评估
针对不同错误类型的预定义参考脚本。

我正在寻找有关如何实施此功能的指导。具体来说，我需要以下帮助：

提取和存储用户指定的列数据。
处理手动说明并将其链接到Fuid ID。
使用使用ML/AI基于ML/AI的自动生成TDD评估
参考脚本。
]]></description>
      <guid>https://stackoverflow.com/questions/79545514/issue-related-to-skywise-slate-dashboard</guid>
      <pubDate>Mon, 31 Mar 2025 04:58:02 GMT</pubDate>
    </item>
    <item>
      <title>如何使用R中的自定义内核进行SVM？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79545276/how-to-do-svm-with-a-custom-kernel-in-r</link>
      <description><![CDATA[ 内核方程  
我试图使用上面的自定义内核进行SVM，但我不知道该如何编码，有人知道我应该在哪里看吗？哪些软件包允许包括自定义内核？该内核应允许更高阶的相互作用，而不仅仅是常规多项式内核。]]></description>
      <guid>https://stackoverflow.com/questions/79545276/how-to-do-svm-with-a-custom-kernel-in-r</guid>
      <pubDate>Sun, 30 Mar 2025 23:33:57 GMT</pubDate>
    </item>
    <item>
      <title>当给出额外的较大V12权重（Yolov12x）时，Yolo为什么要下载Nano V11型号（Yolov11n）？</title>
      <link>https://stackoverflow.com/questions/79545081/why-does-yolo-download-a-nano-v11-model-yolov11n-when-given-the-extra-larger-v</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79545081/why-does-yolo-download-a-nano-v11-model-yolov11n-when-given-the-extra-larger-v</guid>
      <pubDate>Sun, 30 Mar 2025 19:43:01 GMT</pubDate>
    </item>
    <item>
      <title>将型号的重量从旧的Keras版本转换为Pytorch</title>
      <link>https://stackoverflow.com/questions/79544819/conversion-of-model-weights-from-old-keras-version-to-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79544819/conversion-of-model-weights-from-old-keras-version-to-pytorch</guid>
      <pubDate>Sun, 30 Mar 2025 15:41:21 GMT</pubDate>
    </item>
    <item>
      <title>如何保存Dynamax Gussianhmm型号？</title>
      <link>https://stackoverflow.com/questions/79544766/how-do-i-save-a-dynamax-gussianhmm-model</link>
      <description><![CDATA[我一直在基于本教程开发模型：
 https://probml.github.github.io/dynamax/namax/neamax/notebooks/notebooks/notebooks/gaussian/gaussian_phmmm.phmm.hmm.hmm.hmm.hmm.hmm.hmm.hmm.hmmm.hmm.hmm.hmm.hmt 但是，由于现在我的代码可能需要几天的运行，所以我希望将模型保存在中间，以便以后加载。
我尝试了JSON，Pickle和其他几种GPT建议，但无济于事。
因此，在我尝试保存模型的所有参数作为字符串之前，并在加载模型时将其转换回它们，我想知道是否有更轻松的选择。
这是我的代码的示例：
 来自dynamax.hidden_​​markov_model import import gaussianhmm
导入jax.random作为JR

嗯= Gaussianhmm（5，3）
param，properties = hmm.initialize（jr.prngkey（10））
 
为了保存我尝试的模型，例如：
 导入jax.numpy作为jnp
导入JAX
进口泡菜

def backup_hmm（params，props，filename =; hmm_backup_jax.pkl＆quort;）：
    ＃提取阵列安全
    params_flat，params_tree = jax.tree_util.tree_flatten（params）
    props_flat，props_tree = jax.tree_util.tree_flatten（props）

    ＃转换为普通列表
    params_flat = [jnp.array（p）for params_flat中的p]
    props_flat = [p props_flat中的p的jnp.array（p）]

    ＃保存到泡菜
    用开放式（文件名，“ wb”）为f：
        pickle.dump（{
            ＆quot&#39;params;：params_flat，
            ＆quot&#39;params_tree＆quort;：params_tree，
            ＆quot“ props”：props_flat，
            ＆quot“ props_tree”：props_tree
        }，f）

    打印（“备份完成。”

Def Restore_hmm（filename =＆quot; hmm_backup_jax.pkl＆quort;）：
    用开放式（文件名，rb＆quot）为f：
        data = pickle.load（f）

    ＃ 恢复
    params_flat = [jnp.array（p）for p in data [&#39;params; quot;]]
    props_flat = [jnp.Array（p）for p in Data [＆quots&#39;props; quot;]]

    ＃重建原始结构
    params = jax.tree_util.tree_unflatten（data [; params_tree; quord; quart&#39;; params_flat）
    props = jax.tree_util.tree_unflatten（data [＆quot; props_tree＆quot; quor＆quot; props_flat）

    打印（“恢复已完成。”
    返回参数，道具

 ]]></description>
      <guid>https://stackoverflow.com/questions/79544766/how-do-i-save-a-dynamax-gussianhmm-model</guid>
      <pubDate>Sun, 30 Mar 2025 14:57:55 GMT</pubDate>
    </item>
    <item>
      <title>将TensorFlow模型伪像到SageMaker</title>
      <link>https://stackoverflow.com/questions/79363695/deploying-a-tensorflow-model-artifact-to-sagemaker</link>
      <description><![CDATA[我正在尝试将张量流模型部署到萨吉式端点。我在generic_graph.pb上有模型伪像，并在labels.txt。
我首先创建一个带有以下内容的焦油文件：
 ＃＃MODEL目录结构 
＃＃Model.tar.gz
＃└ - ＆lt; model_name＆gt;
＃└ - ＆lt; version_number＆gt;
＃├├─pa
＃└ - 变量
＃├├─标签.txt
 
我将文件上传到S3中的存储桶中。然后，我尝试使用以下代码部署模型：
  sagemaker_session = sagemaker.session（）
角色=&#39;my-lole&#39;

model = tensorflowmodel（model_data =&#39;s3：//my-bucket/model.tar.gz&#39;，
                    角色=角色，
                    framework_version =&#39;2.3.0&#39;）


preditionor = model.deploy（prinity_instance_count = 1，instance_type =&#39;ml.m5.large&#39;）
 
我在我的CloudWatch日志中不断遇到以下错误：
  valueerror：找不到SavedModel捆绑包！
 
不确定还有什么尝试。]]></description>
      <guid>https://stackoverflow.com/questions/79363695/deploying-a-tensorflow-model-artifact-to-sagemaker</guid>
      <pubDate>Fri, 17 Jan 2025 05:19:16 GMT</pubDate>
    </item>
    <item>
      <title>Importerror：使用“ BitsandBytes” 8位量化需要加速</title>
      <link>https://stackoverflow.com/questions/78595127/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate</link>
      <description><![CDATA[从HuggingFace下载模型时，我遇到了错误。它在Google Colab上工作，但不在我的Windows机器上工作。我正在使用python 3.10.0。
错误代码如下：
  e：\ internships \ consciusai \ .venv \ lib \ lib \ site-packages \ huggingface_hub \ file _download.py：1132：1132：futureWarning：futureWarning：`remume_download&#39;s depected and depected and depected and将在版本1.0.0.0.0.0.0.0中删除。下载总是在可能的情况下恢复。如果要强制新下载，请使用`force_download = true`。
  警告。
未使用的Kwargs：[&#39;_load_in_4bit&#39;，&#39;_load_in_8bit&#39;，&#39;QUAT_METHOD&#39;]。这些Kwargs在＆lt; class&#39;Transformers.utils.quantization_config.bitsandbytesconfig&#39;＆gt;中不使用这些夸大。
e：\ internships \ consciusai \ .venv \ lib \ lib \ lib \ site-packages \ transformers \ ventrizers \ venterizers \ auto.py：159：userWarning：您通过`jentization_config`或等效参数to`from_pretrenained&#39;&#39;&#39;但您正在加载的模型已经加载了量化量化量化量化量化。将使用来自模型的`Quantization_Config`。
  WARNINGS.WARN（WARNNING_MSG）
Trackback（最近的最新电话）：
  file＆quort e：\ induthships \ consciusai \ emaim_2.py”，第77行，in＆lt; module＆gt;
    主要的（）
  file＆quot e：\ internships \ consciusai \ emaim_2.py; ,，第71行，主要
    摘要= summarize_email（内容）
  file＆quot e：\ internships \ consciusai \ emaim_2.py”，第22行，在summarize_email中
    管道=变形金刚。Pipeline（
  file＆quot&#39;e：\ internships \ consciusai \ .venv \ lib \ lib \ site-packages \ transformers \ pipelines \ pipelines \ __ init __ init __ in Int __.py＆quort&#39;＆quort&#39;＆quort&#39;＆quort&#39;＆quote＆quote＆quote＆quort in 906
    框架，型号= peash_framework_load_model（
  file＆quot; e：\实习\ consciusai \ .venv \ lib \ lib \ site-packages \ Transformers \ pipelines \ pipelines \ base.py＆quid＆quot＆quot＆quot＆quid＆quort＆quot＆quot＆quot＆quot＆quid＆quot＆quid＆quot＆quot＆quot＆quot＆quid＆quid＆quot＆quot＆quid＆quot＆quid＆quot＆quid＆quort of 283
    model = model_class.from_pretrataining（模型，** kwargs）
  file＆quot; e：\ internships \ consciusai \ .venv \ lib \ lib \ site-packages \ transformers \ transformers \ models \ auto \ auto \ auto_factory.py;
    返回model_class.from_pretaining（
  file＆quort;
    hf_quantizer.validate_environment（
  file＆quot;
    提高侵居者（
Importerror：使用`bitsandBytes` 8位量化都需要加速：`pip安装加速&#39;和最新版本的bitsandbytes：`pip install -i https：//pypi.org/simple/ bitsandble/ bitsandbytes&#39;
 
这是我使用的代码：
  def summarize_email（content）：
    model_id =＆quot&#39;unsploth/llama-3-8b-instruct-bnb-4bit;

    管道=变形金刚。Pipeline（
        ＆quot“文字生成”
        模型= model_id，
        model_kwargs = {
            ＆quot“ torch_dtype”：torch.float16，
            ＆quot&#39;ventalization_config;：{&#39;load_in_4bit＆quort;：true}，
            ＆quot“ low_cpu_mem_usage＆quot”：true，
        }，，
    ）

    消息= [
        {&#39;&#39;：＆quot; quot“ system; quot” content; quot; quot; quot;
        {&#39;：＆quot“ user quot” contents“ contents”：’汇总给我的电子邮件+内容}，
    这是给出的

    提示= pipeline.tokenizer.apply_chat_template（
            消息，
            tokenize = false，
            add_generation_prompt = true
    ）

    终结者= [
        pipeline.tokenizer.eos_token_id，
        pipeline.tokenizer.convert_tokens_to_ids（“”）
    这是给出的

    输出=管道（
        迅速的，
        max_new_tokens = 256，
        eos_token_id =终止者，
        do_sample = true，
        温度= 0.6，
        top_p = 0.9，
    ）
 
我正在尝试使用“不舒服/Llama-3-8B-Instruct-Bnb-4bit”总结文本。来自拥抱面。
它确实在Google Colab和Kaggle上总结了文本，但没有在本地机器上进行。]]></description>
      <guid>https://stackoverflow.com/questions/78595127/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate</guid>
      <pubDate>Sat, 08 Jun 2024 08:37:18 GMT</pubDate>
    </item>
    <item>
      <title>无法下载MMCV 1.3.0并构建车轮</title>
      <link>https://stackoverflow.com/questions/77479005/not-able-to-download-mmcv-1-3-0-and-build-wheels</link>
      <description><![CDATA[当我尝试安装 mmcv-full == 1.3.0 时，它无法下载并构建轮子（我已经更新了车轮）
  错误无法为MMCV-Full构建车轮，这是安装pyproject.toml项目所需的
 但是当我尝试使用时
 MIM安装mmcv-full  
错误消息：
  RROR：MMCV-Full的建筑轮失败
  运行设置。
无法构建mmcv-full
错误：无法为MMCV-Full构建车轮，这是安装pyproject.toml的项目所需的
 
可以下载 mmcv-full 的最新版本，但是由于我试图克隆的存储库需要使用 MMCV版本1.3.0 。
我正在使用 Windows 11 ，想知道我应该如何下载版本。]]></description>
      <guid>https://stackoverflow.com/questions/77479005/not-able-to-download-mmcv-1-3-0-and-build-wheels</guid>
      <pubDate>Tue, 14 Nov 2023 08:00:38 GMT</pubDate>
    </item>
    <item>
      <title>自定义SVM多项式内核</title>
      <link>https://stackoverflow.com/questions/69573659/custom-svm-polynomial-kernel</link>
      <description><![CDATA[我被要求作为分配，以开发SVM的自定义多项式（学位= 3,4,5）内核，并将其准确性与Sklearn的内置多核进行比较（应该几乎相同）。
我试图遵循多项式内核定义，但我的结果似乎并不相似，这是我的代码：
  def poly_kernel_fn（x，y）：
＃实现多项式内核
＃ -  args：2个形状的numpy阵列[n_samples，n_features]
＃ - 返回：计算的形状内核矩阵[n_samples，n_samples]

k = np.zeros（（x. shape [0]，y.shape [0]））
k =（x.dot（y.t） + 1）** 4
返回k

clfpoly = svm.svc（kernel =&#39;poly&#39;，度= 4）
clfpoly.fit（x_train，y_train）
zpoly = clfpoly.predict（x_test）
打印（“具有内置3D多项式内核的准确性为：”，Quecy_score（Y__Test，Zpoly）*100，“％＆quot”）

clf = svm.svc（kernel = poly_kernel_fn）
clf.fit（x_train，y_train）
z = clf.predict（x_test）
打印（“自定义RBF内核的准确性是：”，“ ecucre_score（y_test，z）*100，; quot”％＆quot;
 
精度结果如下：

内置4D多项式内核的准确性为：56.999999999999999％
内核的准确性为：59.0％

如果我将多项式等级更改为3或5，它会更改更多，因此我不知道我做错了什么还是只是不可能匹配内置的精度。]]></description>
      <guid>https://stackoverflow.com/questions/69573659/custom-svm-polynomial-kernel</guid>
      <pubDate>Thu, 14 Oct 2021 15:46:08 GMT</pubDate>
    </item>
    <item>
      <title>如何计算幼稚贝叶斯分类器中的证据？</title>
      <link>https://stackoverflow.com/questions/60454210/how-to-calculate-evidence-in-naive-bayes-classifier</link>
      <description><![CDATA[我在Python写了一个简单的多项式幼稚贝叶斯分类器。该代码预测
整个过程基于我从 nofollow noreferrer“&gt; wikipedia文章关于幼稚的贝耶斯：
  
因此，第一步是从文章中提取特征。为此，我使用Sklearn的Count Vectorizer。它计算词汇中所有单词的出现数量：

 来自Sklearn.feature_extraction.text Import CountVectorizer
vectorizer = countvectorizer（stop_words =&#39;英语&#39;，min_df = 5，ngram_range =（1,1））
功能= vectorizer.fit_transform（data.news）.toArray（）
打印（功能。形状）
（2225，9138）
 
结果，我在数据集中获得了每个文章的9138个功能。

下一步是为每个标签计算P（x  i  | c  k ）。它由多项式分布公式给出：

   i计算p  ki 如下：
  def count_word_probability（功能）：
  v_size =功能。形状[1]
  alpha = 1
  total_counts_for_each_word = np.sum（功能，轴= 0）
  total_count_of_words = np.sum（total_counts_for_each_word）
  probs =（alpha + total_counts_for_each_word） /（（v_size * alpha） + total_count_of_words）
  返回概率
 
基本上，此函数的作用是计算所有文章中带有特定标签（例如业务）的每个单词的总频率，并除以所有文章中带有该标签的单词总数。它还适用拉普拉斯平滑（alpha = 1）以说明0频率的单词。

接下来，我计算P（C  k ），这是标签的先验概率。我只是将一个类别中的文章总数除以所有类别的文章总数：

  labels_probs = [len（data.index [data [&#39;category_id&#39;] == i]） / len（data）for Range（5）]
 

这些是缩放术语和恒定项的函数（p（x）相应：

 将数学导入数学
来自Scipy.特定进口阶乘

def scaing_term（doc）：
  term = Math.factorial（np.sum（doc）） / np.prod（fortorial（doc））
  返回期限 
 
缩放函数上面的缩放函数将文章中的单词总和划分为段落的产物。
  def nb_constant（文章，labels_probs，word_probs）：
  s_term = scaing_term（文章）
  证据= [np.log（s_term） + np.sum（文章 * np.log（word_probs [i]））） + np.log（labels_probs [i]）for Range（len（word_probs））
  证据= np.sum（证据）
  返回证据
 
 o，上面的最后一个函数计算分母（先验概率p（x）。它总结了所有文章类别的UPS p（x | c  k ）：
  
和最终的天真贝叶斯分类器看起来像这样：

  def naive_bayes（文章，label_probs，words_probs）：
  class_probs = []
  s_term = scaing_term（文章）
  constant_term = nb_constant（文章，label_probs，words_probs）
  对于范围（Len（Label_probs））的Cl）：
    class_prob =（np.log（s_term） + np.sum（文章 * np.log（words_probs [cl]））） + np.log（label_probs [cl]）） / constant_term
    class_probs.append（class_prob）
  class_probs = np.exp（np.array（class_probs））
  返回class_probs
 
没有恒定术语的情况下，此功能为我馈送的任何自定义文本都会输出正确的标签。但是所有类别的分数都是均匀的，接近零。当我除以恒定术语以获取总和最高为零的实际概率值时，我会得到所有类别的奇怪结果，例如所有类别的概率。我绝对缺少理论上的东西，因为我对概率理论和数学不了解。]]></description>
      <guid>https://stackoverflow.com/questions/60454210/how-to-calculate-evidence-in-naive-bayes-classifier</guid>
      <pubDate>Fri, 28 Feb 2020 14:56:20 GMT</pubDate>
    </item>
    <item>
      <title>如何计算Bernoulli幼稚贝叶斯的联合日志样本</title>
      <link>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</link>
      <description><![CDATA[For a classification problem using BernoulliNB , how to calculate the joint log-likelihood?关节可能性应通过下方公式计算，其中y（d）是实际输出的数组（不是预测值），而x（d）是特征的数据集。
  我阅读 href =“ https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/naive_bayes.py#l839“ rel =“ nofollow noreferrer”]]></description>
      <guid>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</guid>
      <pubDate>Wed, 17 Oct 2018 18:08:50 GMT</pubDate>
    </item>
    <item>
      <title>幼稚的贝叶斯分类器从头开始实现</title>
      <link>https://stackoverflow.com/questions/19349567/naive-bayes-classifier-implementation-from-scratch</link>
      <description><![CDATA[我正在尝试自己实施我的第一个天真的贝叶斯分类器，以更好地理解。因此，我的数据集来自 http://archive.ics.uci.uci.uci.uci.edu/ml/datasets/datasets/datasets/Adult  Adadult  American Census Data，Spersus sass seals seals＆lt; &#39;＆gt; 50k&#39;）。
这是我的python代码：
 导入系统
导入CSV

word_stats = {}＃{&#39;word&#39;：{&#39;class1&#39;：cnt，&#39;class2&#39;：cnt&#39;}}}
word_cnt = 0

targets_stats = {}＃{&#39;class1&#39;：3234，&#39;class2&#39;：884}每个类中有多少个单词
class_stats = {}＃{&#39;class1&#39;：7896，&#39;class2&#39;：3034}每个类中有多少行
items_cnt = 0

def train（数据集，目标）：
    global word_stats，words_cnt，targets_stats，items_cnt，class_stats

    num = len（数据集）
    对于Xrange（num）中的项目：
        class_stats [targets [item]] = class_stats.get（targets [item]，0） + 1

        对于i在Xrange（len（dataset [item]）））：
            word = dataset [item] [i]
            如果不是words_stats.has_key（word）：
                word_stats [word] = {}

            TGT =目标[项目]

            cnt = word_stats [word] .get（tgt，0）
            word_stats [word] [tgt] = cnt + 1

            targets_stats [tgt] = targets_stats.get（tgt，0） + 1
            word_cnt += 1

    items_cnt = num

DEF分类（DOC，TGT_SET）：
    global words_stats，words_cnt，targets_stats，items_cnt

    probs = {}＃概率本身p（c | w）= p（w | c） * p（c） / p（w）
    PC = {}＃probability of Clofe in Document set p（c）中
    pwc = {}＃probability在特定类中的单词设置。 P（W | C）
    pw = 1 #1＃documet set中的单词集

    doc中的单词：
        如果在words_stats中没有单词：
            继续#dirty，非常肮脏 
        pw = pw * float（sum（word_stats [word] .values（））） / word_cnt

    对于TGT_SET中的TGT：
        PC [tgt] = class_stats [tgt] / float（items_cnt）
        doc中的单词：
            如果在words_stats中没有单词：
                继续#dirty，非常肮脏
            tgt_wrd_cnt = word_stats [word] .get（tgt，0）
            pwc [tgt] = pwc.get（tgt，1） * float（tgt_wrd_cnt） / targets_stats [tgt]

        probs [tgt] =（pwc [tgt] * pc [tgt]） / pw

    l =排序（probs.items（），key = lambda i：i [1]，反向= true）
    打印概率
    返回L [0] [0]

def check_results（数据集，目标）：
    num = len（数据集）
    tgt_set = set（目标）
    正确= 0
    错误= 0

    对于Xrange（num）中的项目：
        res =分类（dataset [item]，tgt_set）
        如果res ==目标[项目]：
            正确=正确 + 1
        别的：
            错误=不正确 + 1

    打印“正确：”，float（正确） / num，&#39;不正确：&#39;，float（不正确） / num
            
def load_data（fil）：
    数据= []
    tgts = []

    阅读器= csv.reader（fil）
    对于读者中的行：
        d = [X.Strip（）in in in in in in in in in in]
        如果 &#39;？&#39;在D：
            继续

        如果不是Len（D）：
            继续
        
        data.append（d [： -  1]）
        tgts.append（D [-1：] [0]）

    返回数据，TGTS

如果__name__ ==&#39;__ -main __&#39;：
    如果Len（sys.argv）＆lt; 3：
        打印&#39;./program train_data.txt test_data.txt&#39;
        sys.exit（1）

    文件名= sys.argv [1]
    fil = open（文件名，&#39;r&#39;）
    数据，tgt = load_data（fil）
    火车（数据，TGT）

    test_file = open（sys.argv [2]，&#39;r&#39;）
    test_data，test_tgt = load_data（test_file）

    check_results（test_data，tgt）
 
它给出了〜61％的正确结果。当我打印概率时，我会得到以下内容：
  {&#39;＆lt; = 50K&#39;：0.07371606889800396，&#39;＆gt; 50K&#39;：15.325378327213354}
 
但是，在正确的分类器的情况下，我希望看到这两个概率的总和等于1。
起初，我认为问题是在浮动底流中，并试图以对数进行所有计算，但结果是相似的。
我知道省略一些单词会影响准确性，但是概率是错误的。
我做错了什么或不明白？
出于您的说服力，我在这里上传了数据集和Python脚本：
&lt;A href =“ https://dl.dropboxusercontent.com/u/36180992/adult.tar.gz”]]></description>
      <guid>https://stackoverflow.com/questions/19349567/naive-bayes-classifier-implementation-from-scratch</guid>
      <pubDate>Sun, 13 Oct 2013 19:52:37 GMT</pubDate>
    </item>
    <item>
      <title>天真的贝叶斯分类器</title>
      <link>https://stackoverflow.com/questions/9677603/naive-bayes-classifier</link>
      <description><![CDATA[我正在使用Scikit-learn来查找文档的TF-IDF重量，然后使用幼稚
贝叶斯分类器对文本进行分类。但是文档中所有单词的TF-IDF权重除了少数单词。但据我所知，负值意味着不重要的术语。那么，是否有必要将整个TF-IDF值传递给贝叶斯分类器？如果我们只需要通过其中的一些，我们该怎么做？与LinearSVC相比，贝叶斯分类器的好坏是多么好吗？除了使用TF-IDF以外，是否有更好的方法可以在文本中找到标签？]]></description>
      <guid>https://stackoverflow.com/questions/9677603/naive-bayes-classifier</guid>
      <pubDate>Tue, 13 Mar 2012 02:28:44 GMT</pubDate>
    </item>
    </channel>
</rss>