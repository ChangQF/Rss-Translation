<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 13 Oct 2024 21:15:57 GMT</lastBuildDate>
    <item>
      <title>我是否应该将遗忘集分成训练/验证/测试以使用 CNN 和 Celeba 数据集进行反学习实验？</title>
      <link>https://stackoverflow.com/questions/79084014/should-i-split-the-forget-set-into-train-validate-test-for-an-unlearning-experim</link>
      <description><![CDATA[我正在 CelebA 数据集上训练一个简单的 CNN，并尝试通过创建一个遗忘集（来自 CelebA 的随机部分图像）来进行反学习实验。
CNN 已在完整训练集上进行训练，并进行了单独的验证和测试拆分以进行评估。
我是否应该将遗忘集拆分为训练/测试/验证子集，如果是，为什么？（遗忘集约占训练集的 10% - 其余 90% 为保留集）
在我当前的反学习实验中，我只对遗忘集进行反学习，没有进行任何重新训练。因此，我在此过程中仅使用完整模型和遗忘集。
我尝试使用我相应拆分的遗忘集，但测试变得棘手，并且可能只有一个遗忘集感觉合乎逻辑（因为遗忘集中不是同一个人，它是一个混合包）
因此，拆分为测试和验证意味着它是不同面孔的不同包。]]></description>
      <guid>https://stackoverflow.com/questions/79084014/should-i-split-the-forget-set-into-train-validate-test-for-an-unlearning-experim</guid>
      <pubDate>Sun, 13 Oct 2024 19:44:40 GMT</pubDate>
    </item>
    <item>
      <title>为什么尽管使用了 RLZoo3 的最佳超参数，我的 SB3 DQN 代理仍无法学习 CartPole-v1？</title>
      <link>https://stackoverflow.com/questions/79083972/why-is-my-sb3-dqn-agent-unable-to-learn-cartpole-v1-despite-using-optimal-hyperp</link>
      <description><![CDATA[我从 RLZoo3 获得了用于训练 CartPole-v1 的最佳超参数。我创建了一个最小示例来展示我的 CartPole 代理的性能。根据官方文档，代理应获得 500 分，才能成功完成一集。不幸的是，分数没有超过 300。
这是我的代码 -
import gymnasium as gym
import numpy as np
import torch
from stable_baselines3 import DQN
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback
from torch.utils.tensorboard import SummaryWriter
import os

def set_seed(seed):
np.random.seed(seed)
torch.manual_seed(seed)
torch.backends.cudnn.deterministic = True

class TensorBoardCallback(BaseCallback):
def __init__(self, log_dir):
super().__init__()
self.writer = SummaryWriter(log_dir=log_dir)
self.episode_rewards = []
self.current_episode_reward = 0

def _on_step(self):
self.current_episode_reward += self.locals[&#39;rewards&#39;][0]

如果 self.locals[&#39;dones&#39;][0]:
self.episode_rewards.append(self.current_episode_reward)
self.writer.add_scalar(&#39;train/episode_reward&#39;, self.current_episode_reward, self.num_timesteps)
self.current_episode_reward = 0

如果 len(self.episode_rewards) &gt;= 100:
avg_reward = sum(self.episode_rewards[-100:]) / 100
self.writer.add_scalar(&#39;train/average_reward&#39;, avg_reward, self.num_timesteps)

return True

def on_training_end(self):
self.writer.close()

# 设置日志目录
log_dir = &quot;tensorboard_logs&quot;
os.makedirs(log_dir, exist_ok=True)

# 设置可重复性的种子
seed = 42
set_seed(seed)

# 创建环境
env = gym.make(&quot;CartPole-v1&quot;)
env = DummyVecEnv([lambda: env])

# 使用来自 rlzoo3 的超参数创建模型
model = DQN(
policy=&quot;MlpPolicy&quot;,
env=env,
learning_rate=2.3e-3,
batch_size=64,
buffer_size=100000,
learning_starts=1000,
gamma=0.99,
target_update_interval=10,
train_freq=256,
gradient_steps=128,
exploration_fraction=0.16,
exploration_final_eps=0.04,
policy_kwargs=dict(net_arch=[256, 256]),
verbose=1,
tensorboard_log=log_dir,
seed=seed
)

# 创建回调
tb_callback = TensorBoardCallback(log_dir)

# 训练模型
total_timesteps = 50000
model.learn(total_timesteps=total_timesteps, callback=tb_callback)

print(&quot;训练完成。您可以使用 TensorBoard 查看结果。&quot;)
print(f&quot;在您的终端中运行以下命令：tensorboard --logdir {log_dir}&quot;)

env.close()

这是最终结果 -
]]></description>
      <guid>https://stackoverflow.com/questions/79083972/why-is-my-sb3-dqn-agent-unable-to-learn-cartpole-v1-despite-using-optimal-hyperp</guid>
      <pubDate>Sun, 13 Oct 2024 19:23:17 GMT</pubDate>
    </item>
    <item>
      <title>ML 梯度下降 [关闭]</title>
      <link>https://stackoverflow.com/questions/79082800/ml-gradient-descent</link>
      <description><![CDATA[梯度下降
梯度下降是机器学习中最广泛使用的优化算法之一。它通过迭代调整模型参数来帮助最小化成本函数（或损失函数）。
梯度下降中的导数：梯度（偏导数向量）显示损失函数最陡峭的增长方向。为了最小化损失，参数会沿梯度的反方向更新。从数学上来说，参数更新规则是：
𝜃
𝜃
−
𝛼
∇
𝜃
𝐽
(
𝜃
)
θ=θ−α∇
θ
​
J(θ)
其中
𝜃
θ 表示模型参数，
𝛼
α 表示学习率，
∇
𝜃
𝐽
(
𝜃
)
∇
θ
​
J(θ) 是成本函数相对于
𝜃
θ 的梯度。
2. 成本函数（损失函数）
成本函数是训练期间要最小化的目标函数。示例包括回归任务的均方误差 (MSE) 和分类任务的交叉熵。
微分学用于计算成本函数相对于模型参数的导数，以指导如何调整参数。]]></description>
      <guid>https://stackoverflow.com/questions/79082800/ml-gradient-descent</guid>
      <pubDate>Sun, 13 Oct 2024 09:18:27 GMT</pubDate>
    </item>
    <item>
      <title>Python 在分配 numpy 数组时抛出 MemoryError</title>
      <link>https://stackoverflow.com/questions/79082341/python-throws-memoryerror-while-allocating-a-numpy-array</link>
      <description><![CDATA[我正在以类似 sklearn 的方式拟合 QML 算法：
num_features = X_train.shape[1]

feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)
ansatz = RealAmplitudes(num_qubits=num_features, reps=3)
optimizer = COBYLA(maxiter=100)

vqc = VQC(
feature_map=feature_map,
ansatz=ansatz,
optimizer=optimizer
)

vqc.fit(X_train, y_train.to_numpy())

在执行 vqc.fit(X_train, y_train.to_numpy()) 行时，解释器抛出异常：
MemoryError：无法为形状为 (1048576,) 且数据类型为 &lt;U420  的数组分配 1.64 GiB
问题是，我正在使用的机器有 120 GB 的 RAM，我不明白它为什么不能为数组分配 1.64 GB。你能帮我解决这个问题吗？有什么方法可以突破这个 RAM 限制吗？
我不确定，但我想试试这个 https://stackoverflow.com/a/58686879。然而，我认为这行不通，也许你有更多的想法。]]></description>
      <guid>https://stackoverflow.com/questions/79082341/python-throws-memoryerror-while-allocating-a-numpy-array</guid>
      <pubDate>Sun, 13 Oct 2024 03:26:07 GMT</pubDate>
    </item>
    <item>
      <title>结合几种监督学习技术？[关闭]</title>
      <link>https://stackoverflow.com/questions/79081793/combining-several-supervised-learning-techniques</link>
      <description><![CDATA[我的数据集包含大量图像和制表数据（存储在 .csv 文件中）。我打算使用我拥有的所有数据（图像和制表数据）创建一个能够对案例 A 和案例 B 进行分类/识别的机器学习模型。
是否可以结合几种监督学习技术，例如使用 CNN 处理图像，使用随机森林分析制表数据？目标是创建一个对两种类型的数据进行训练的组合模型，该模型可以将案例 A 和案例 B 进行分类。
我的问题是这种方法在机器学习中是否可行。
是否可以在训练过程中整合图像和制表数据？
对于这些类型的数据，通常推荐使用哪种机器学习算法？
我相信 CNN 是图像的不错选择，但我不确定制表数据的最佳方法是什么。
最适合用于此任务的 Python 包是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79081793/combining-several-supervised-learning-techniques</guid>
      <pubDate>Sat, 12 Oct 2024 19:52:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么飞机没有显示在 matplotlib 图中</title>
      <link>https://stackoverflow.com/questions/79081747/why-the-plane-doesnt-show-in-matplotlib-plot</link>
      <description><![CDATA[我正在对具有 13 个特征的波士顿房屋数据集实施 SLP。我为 X 选择“rm”和“zn”，为目标 Y 选择“medv”。我还从头实施了一个感知器类。在这个类中，我有一个名为 plot_losses 的函数，它在一个窗口中绘制预测线（2d）和损失，还绘制 3d 图的预测平面，这就是问题所在，即 3d 部分。
平面未显示在 3d 散点图上。
感知器类实现：
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter

感知器类：
def __init__(self, input_size, lr, epochs):
self.w = np.zeros(input_size)
self.b = 0
self.lr = lr
self.epochs = epochs
self.losses = []

def fit(self, X_train, Y_train):
for _ in range(self.epochs):
for x_i in range(X_train.shape[0]):
x = X_train[x_i]
y = Y_train[x_i]
y_pred = np.dot(x, self.w) + self.b
error = y - y_pred

self.w = self.w + (error * x * self.lr)
self.b = self.b + (error * self.lr)

loss = np.mean(np.abs(error))
self.losses.append(loss)

def predict(self, X_test):
return np.dot(X_test, self.w) + self.b

def plot_losses(self, X_train, Y_train, ax1_title, ax2_title, plot_3d=False, plot_3d_title=&#39;3D Plot&#39;):
for _ in range(self.epochs):
for x_i in range(X_train.shape[0]):
x = X_train[x_i]
y = Y_train[x_i]
Y_pred = np.dot(x, self.w) + self.b

if plot_3d:
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111,projection=&#39;3d&#39;)

X_feature1 = X_train[:, 0]
X_feature2 = X_train[:, 1]

ax.scatter(X_feature1, X_feature2, Y_train, color=&#39;blue&#39;, label=&#39;True Values&#39;)

X1_grid, X2_grid = np.meshgrid(
np.linspace(X_feature1.min(), X_feature1.max(), 20),
np.linspace(X_feature2.min(), X_feature2.max(), 20)
)

Z_pred = self.w[0] * X1_grid + self.w[1] * X2_grid + self.b

ax.plot_surface(X1_grid, X2_grid, Z_pred, color=&#39;red&#39;, alpha=0.5)
ax.set_xlabel(&quot;特征 &#39;rm&#39;&quot;)
ax.set_ylabel(&quot;特征 &#39;zn&#39;&quot;)
ax.set_zlabel(&quot;目标 &#39;medv​​&#39;&quot;)
ax.set_title(plot_3d_title)
ax.legend()
plt.show()
else:
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 5))

ax1.scatter(X_train[:, 0], Y_train, color=&#39;blue&#39;, label=&#39;真值&#39;)
ax1.plot(X_train[:, 0], Y_pred, color=&#39;red&#39;, label=&#39;预测值Line&#39;)
ax1.set_title(ax1_title)
ax1.legend()

ax2.plot(self.losses)
ax2.set_title(ax2_title)
ax2.set_xlabel(&quot;Epochs&quot;)
ax2.set_ylabel(&quot;均方误差 (MSE)&quot;)

plt.tight_layout()
plt.show()

波士顿房屋数据集的线性回归：
%matplotlib qt
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
from perceptron import Perceptron
df_boston = pd.read_csv(&#39;input/BostonHousing.csv&#39;)
X = df_boston[[&#39;rm&#39;,&#39;zn&#39;]].values
Y = df_boston[&#39;medv​​&#39;].values
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=.2)

slp = Perceptron(2, .01, 100)
slp.fit(X_train, Y_train) 
slp.plot_losses(X_train,Y_train, &#39;员工工资和经验感知器&#39;, &#39;损失值&#39;, plot_3d=True, plot_3d_title=&#39;波士顿住房感知器&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79081747/why-the-plane-doesnt-show-in-matplotlib-plot</guid>
      <pubDate>Sat, 12 Oct 2024 19:31:21 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 load_state_dict 加载我的模型：RuntimeError：为 UNetGenerator 加载 state_dict 时出错：state_dict 中出现意外键</title>
      <link>https://stackoverflow.com/questions/79081715/cant-load-my-model-using-load-state-dict-runtimeerror-errors-in-loading-sta</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79081715/cant-load-my-model-using-load-state-dict-runtimeerror-errors-in-loading-sta</guid>
      <pubDate>Sat, 12 Oct 2024 19:09:08 GMT</pubDate>
    </item>
    <item>
      <title>结合 CNN 和随机森林方法？[关闭]</title>
      <link>https://stackoverflow.com/questions/79081396/combining-cnn-and-random-forest-approach</link>
      <description><![CDATA[我的数据集包含大量图像和制表数据（存储在 .csv 文件中）。我打算使用我拥有的所有数据（图像和制表数据）创建一个能够对案例 A 和案例 B 进行分类/识别的机器学习模型。
是否可以结合几种监督学习技术，例如，使用 CNN 处理图像，使用随机森林分析制表数据？目标是创建一个对两种类型的数据进行训练的组合模型，该模型可以将案例 A 和案例 B 进行分类。
我的问题是这种方法在机器学习中是否可行。
是否可以在训练过程中集成图像和制表数据？
对于这些类型的数据，通常推荐使用哪种机器学习算法？
我认为 CNN 是图像的不错选择，但我不确定制表数据的最佳方法是什么。
最适合用于此任务的 Python 包是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79081396/combining-cnn-and-random-forest-approach</guid>
      <pubDate>Sat, 12 Oct 2024 16:08:46 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：X 有 14 个特征，但 LogisticRegression 需要 17128 个特征作为输入 [重复]</title>
      <link>https://stackoverflow.com/questions/79081243/valueerror-x-has-14-features-but-logisticregression-is-expecting-17128-feature</link>
      <description><![CDATA[# 预测系统
# content = input(&quot;输入您的文本：&quot;)

# 访问文本内容而不是 csr_matrix 元素
text_content = X_train[20].toarray()[0]

# 假设 X_train 包含文本数据；如果不包含，则进行相应调整。
# 如果需要，转换为常规 Python 字符串：
text_content = &quot; &quot;.join([str(element) for element in text_content if element != 0])

processed_content = preprocess_and_translate(text_content)
print(&quot;处理后的内容（英文）：&quot;,processed_content)

vectorizer = TfidfVectorizer()

vectorizer.fit_transform([processed_content])

input_data = vectorizer.transform([processed_content])

prediction = model.predict(input_data)
if prediction[0] == 1:
print(&#39;假新闻&#39;)
else:
print(&#39;真实新闻&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79081243/valueerror-x-has-14-features-but-logisticregression-is-expecting-17128-feature</guid>
      <pubDate>Sat, 12 Oct 2024 14:51:02 GMT</pubDate>
    </item>
    <item>
      <title>用于增量学习的 Python 非线性回归器</title>
      <link>https://stackoverflow.com/questions/79063665/python-non-linear-regressor-for-incremental-learning</link>
      <description><![CDATA[我想知道 scikit-learn 中是否有一个非线性回归程序，允许增量学习，即通过 partial_fit 调用。我发现 SGDRegressor 和 PassiveAggressiveRegressor 都允许 partial_fit，但它们是线性的，而我的数据显然是非线性的，因此拟合效果并不理想。]]></description>
      <guid>https://stackoverflow.com/questions/79063665/python-non-linear-regressor-for-incremental-learning</guid>
      <pubDate>Mon, 07 Oct 2024 21:18:59 GMT</pubDate>
    </item>
    <item>
      <title>只有输入张量可以作为位置参数传递</title>
      <link>https://stackoverflow.com/questions/78360982/only-input-tensors-may-be-passed-as-positional-arguments</link>
      <description><![CDATA[从 PIL 导入图像
导入 matplotlib.pyplot 作为 plt
导入 argparse
导入 pickle
导入 numpy
作为 np
从 tensorflow 导入 keras
从 keras.applications.xception 导入 Xception
从 keras.preprocessing.sequence 导入 pad_sequences
从 tensorflow.keras.preprocessing.text 导入 Tokenizer
导入 tensorflow 作为 tf

# 使用 Lambda 定义自定义层（不带 name 参数）
class NotEqual(tf.keras.layers.Layer):
def __init__(self, name=None):
super(NotEqual, self).__init__(name=name)

def call(self, x, y): # 使用关键字参数“x”和“y”
return tf.math.not_equal(x, y)

# 定义用于提取特征、生成描述的函数，和其他必要的实用程序
def extract_features(filename, model):
try:
image = Image.open(filename)
except:
print(&quot;ERROR: 无法打开图像！请确保图像路径和扩展名正确&quot;)
image = image.resize((299, 299))
image = np.array(image)
if image.shape[2] == 4:
image = image[..., :3]
image = np.expand_dims(image, axis=0)
image = image / 127.5
image = image - 1.0
feature = model.predict(image)
return feature

def word_for_id(integer, tokenizer):
for word, index in tokenizer.word_index.items():
if index == integer:
return word
return None

def generate_desc(model, tokenizer, photo, max_length):
in_text = &#39;start&#39;
for i in range(max_length):
sequence = tokenizer.texts_to_sequences([in_text])[0]
sequence = pad_sequences([sequence], maxlen=max_length)
pred = model.predict({&#39;image_input&#39;: photo, &#39;text_input&#39;:sequence}) # 将输入作为字典传递
pred = np.argmax(pred)
word = word_for_id(pred, tokenizer)
if word is None:
break
in_text += &#39; &#39; + word
if word == &#39;end&#39;:
break
return in_text

# 解析参数
ap = argparse.ArgumentParser()
ap.add_argument(&#39;-i&#39;, &#39;--image&#39;, required=True, help=&quot;Image Path&quot;)
args = vars(ap.parse_args())
img_path = args[&#39;image&#39;]

# 加载tokenizer
tokenizer = pickle.load(open(&quot;tokenizer.p&quot;, &quot;rb&quot;))

# 定义模型的路径
model_path = &#39;models/model_9.h5&#39;
# 使用自定义对象（包括 NotEqual 层）加载模型
使用 keras.utils.custom_object_scope({&#39;NotEqual&#39;: NotEqual}):
model = tf.keras.models.load_model(model_path)

我尝试以各种方式运行此代码，但出现错误：
 model = tf.keras.models.load_model(model_path)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\saving\saving_api.py”，第 183 行，位于 load_model
return legacy_h5_format.load_model_from_hdf5(filepath)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 “C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\legacy\saving\legacy_h5_format.py”，第 133 行，位于 load_model_from_hdf5
model = saving_utils.model_from_config(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\legacy\saving\saving_utils.py”，第 85 行，位于 model_from_config
return serialization.deserialize_keras_object(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\legacy\saving\serialization.py”，第 495 行，位于 deserialize_keras_object
deserialized_obj = cls.from_config(
^^^^^^^^^^^^^^^^^
文件“C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\model.py”，第 528 行，位于 from_config 中
return functional_from_config(
^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\function.py”，第 528 行，位于 functional_from_config 中
process_node(layer, node_data)
文件&quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\function.py&quot;，第 475 行，位于 process_node
layer(*args, **kwargs)
文件 &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;，第 122 行，位于 error_handler
raise e.with_traceback(filtered_tb) from None
文件 &quot;C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\layers\layer.py&quot;，第 721 行，位于 __call__
raise ValueError(
ValueError: 只能将输入张量作为位置参数传递。以下参数值应作为关键字参数传递：0（类型为 &lt;class &#39;int&#39;&gt;）
]]></description>
      <guid>https://stackoverflow.com/questions/78360982/only-input-tensors-may-be-passed-as-positional-arguments</guid>
      <pubDate>Sun, 21 Apr 2024 09:15:40 GMT</pubDate>
    </item>
    <item>
      <title>为什么自然语言处理中的 Transformer 需要一堆编码器？</title>
      <link>https://stackoverflow.com/questions/59384146/why-do-transformers-in-natural-language-processing-need-a-stack-of-encoders</link>
      <description><![CDATA[我正在关注这篇关于 transformers 的博客
http://jalammar.github.io/illustrated-transformer/
我唯一不明白的是为什么需要一堆编码器或解码器。我知道多头注意力层捕获了问题的不同表示空间。我不明白为什么需要一堆编码器和解码器。一个编码器/解码器层不行吗？]]></description>
      <guid>https://stackoverflow.com/questions/59384146/why-do-transformers-in-natural-language-processing-need-a-stack-of-encoders</guid>
      <pubDate>Wed, 18 Dec 2019 00:57:26 GMT</pubDate>
    </item>
    <item>
      <title>CNN 架构：对“好”图像和“坏”图像进行分类</title>
      <link>https://stackoverflow.com/questions/57943425/cnn-architecture-classifying-good-and-bad-images</link>
      <description><![CDATA[我正在研究实现 CNN 的可能性，以便将图像分类为“好”或“坏”，但我目前的架构没有成功。
表示“坏”的特征图像：

过度曝光
过度饱和
白平衡不正确
模糊

根据这些特征实现神经网络对图像进行分类是否可行，还是最好使用传统算法，该算法仅查看整个图像的亮度/对比度变化并以此方式进行分类？
我曾尝试使用 VGGNet 架构训练 CNN，但无论 epoch 数或步骤数有多少，我似乎总是得到一个有偏差且不可靠的模型。
示例：

我当前的模型架构非常简单（因为我对整个机器学习世界还很陌生），但似乎可以很好地处理其他分类问题，并且我对其进行了轻微修改，以便更好地处理这个二元分类问题：
 # CONV =&gt; RELU =&gt; POOL 层集
# 定义卷积层，使用&quot;ReLU&quot;激活函数
# 并使用池化层减少空间大小（宽度和高度）
model.add(Conv2D(32, (3, 3), padding=&quot;same&quot;, input_shape=input_shape)) # 32 个 3x3 过滤器（高度、宽度、深度）
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization(axis=channel_dimension))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25)) # 有助于防止过度拟合（25% 的神经元随机断开连接）

# (CONV =&gt; RELU) * 2 =&gt; POOL 层集（随着 CNN 的深入，层数会增加）
model.add(Conv2D(64, (3, 3), padding=&quot;same&quot;, input_shape=input_shape)) # 64 个 3x3 过滤器
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization(axis=channel_dimension))
model.add(Conv2D(64, (3, 3), padding=&quot;same&quot;, input_shape=input_shape)) # 64 个 3x3 过滤器
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization(axis=channel_dimension))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25)) # 有助于防止过度拟合 (25%随机断开的神经元数量)

# (CONV =&gt; RELU) * 3 =&gt; POOL 层集（输入体积大小越来越小）
model.add(Conv2D(128, (3, 3), padding=&quot;same&quot;, input_shape=input_shape)) # 128 个 3x3 滤波器
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization(axis=channel_dimension))
model.add(Conv2D(128, (3, 3), padding=&quot;same&quot;, input_shape=input_shape)) # 128 个 3x3 滤波器
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization(axis=channel_dimension))
model.add(Conv2D(128, (3, 3), padding=&quot;same&quot;, input_shape=input_shape)) # 128 3x3 过滤器
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization(axis=channel_dimension))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25)) # 有助于防止过度拟合（25% 的神经元随机断开连接）

# 仅设置 FC =&gt; RELU 层
model.add(Flatten())
model.add(Dense(512))
model.add(Activation(&quot;relu&quot;))
model.add(BatchNormalization())
model.add(Dropout(0.5))

# sigmoid 分类器（输出层）
model.add(Dense(classes))
model.add(Activation(&quot;sigmoid&quot;))

这个模型是否有任何明显的遗漏或错误，或者我根本无法使用深度学习（使用我当前的 GPU，GTX 970）解决这个问题？
这是我编译/训练模型的代码：
# 初始化模型和优化器
print(&quot;[INFO] Training network...&quot;)
opt = SGD(lr=initial_lr, decay=initial_lr / epochs)
model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=opt, metrics=[&quot;accuracy&quot;])

# 设置检查点
model_name = &quot;output/50_epochs_{epoch:02d}_{val_acc:.2f}.model&quot;
checkpoint = ModelCheckpoint(model_name, monitor=&#39;val_acc&#39;, verbose=1, 
save_best_only=True, mode=&#39;max&#39;)
reduce_lr = ReduceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.2,
waiting=5, min_lr=0.001)
tensorboard = TensorBoard(log_dir=&quot;logs/{}&quot;.format(time()))
callbacks_list = [checkpoint, reduce_lr, tensorboard]

# 训练网络
H = model.fit_generator(training_set, steps_per_epoch=500, epochs=50, validation_data=test_set, validation_steps=150, callbacks=callbacks_list)
]]></description>
      <guid>https://stackoverflow.com/questions/57943425/cnn-architecture-classifying-good-and-bad-images</guid>
      <pubDate>Sun, 15 Sep 2019 10:53:49 GMT</pubDate>
    </item>
    <item>
      <title>线性回归爆炸的梯度下降</title>
      <link>https://stackoverflow.com/questions/50219054/gradient-descent-for-linear-regression-exploding</link>
      <description><![CDATA[我正在尝试使用此资源实现线性回归的梯度下降：https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/
我的问题是，我的权重正在爆炸式增长（呈指数增长），并且本质上与预期相反。
首先，我创建了一个数据集：
def y(x, a):
return 2*x + a*np.random.random_sample(len(x)) - a/2

x = np.arange(20)
y_true = y(x,10)

看起来像这样：

要优化的线性函数：
def y_predict(x, m, b):
return m*x + b

因此，对于一些随机选择的参数，结果如下：
m0 = 1
b0 = 1

a = y_predict(x, m0, b0)

plt.scatter(x, y_true)
plt.plot(x, a)
plt.show()


现在成本看起来是这样的：
cost = (1/2)* np.sum((y_true - a) ** 2)

成本相对于预测 (dc_da) 的偏导数：
dc_da = (a - y_true) # 仍然是一个向量

成本相对于斜率参数 (dc_dm) 的偏导数：
dc_dm = dc_da.dot(x) # 现在是一个常数

成本相对于 y 截距参数 (dc_db) 的偏导数：
dc_db = np.sum(dc_da) # 也是一个常数

最后是梯度下降的实现：
iterations = 10

m0 = 1

b0 = 1

learning_rate = 0.1

N = len(x)

for i in range(iterations):

a = y_predict(x, m0, b0)

cost = (1/2) * np.sum((y_true - a) ** 2)

dc_da = (a - y_true)

mgrad = dc_da.dot(x)
bgrad = np.sum(dc_da)

m0 -= learning_rate * (2 / N) * mgrad
b0 -= learning_rate * (2 / N) * bgrad

if (i % 2 == 0):
print(&quot;Iteration {}&quot;.format(i))
print(&quot;Cost: {}, m: {}, b: {}\n&quot;.format(cost, m0, b0))

结果为：
迭代 0
Cost: 1341.5241150881411, m: 26.02473879743261, b: 2.8683883457327797

迭代 2
Cost: 409781757.38124645, m: 13657.166910552878, b: 1053.5831308528543

迭代 4
Cost: 132510115599264.75，m：7765058.4350503925，b：598610.1166795876

迭代 6
成本：4.284947676217907e+19，m：4415631880.089208，b：340401694.5610262

迭代 8
成本：1.3856132043127762e+25，m：2510967578365.3584，b：193570850213.62192

我的实现有什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/50219054/gradient-descent-for-linear-regression-exploding</guid>
      <pubDate>Mon, 07 May 2018 16:54:52 GMT</pubDate>
    </item>
    <item>
      <title>如何将用PCA和随机森林训练的模型应用于测试数据？</title>
      <link>https://stackoverflow.com/questions/36382572/how-to-apply-model-trained-with-pca-and-random-forest-to-test-data</link>
      <description><![CDATA[在解决一个机器学习问题时，我在训练数据上实施 PCA，然后使用 sklearn 在训练数据上应用 .transform。观察方差后，我只保留方差较大的转换数据中的那些列。然后我使用 RandomForestClassifier 训练模型。现在，我很困惑：如何将训练好的模型应用于测试数据，因为测试数据的列数和保留的转换数据（应用随机森林）不同？]]></description>
      <guid>https://stackoverflow.com/questions/36382572/how-to-apply-model-trained-with-pca-and-random-forest-to-test-data</guid>
      <pubDate>Sun, 03 Apr 2016 07:07:01 GMT</pubDate>
    </item>
    </channel>
</rss>