<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 05 Jun 2024 15:15:44 GMT</lastBuildDate>
    <item>
      <title>我可以使用来自不同实验条件的数据来训练和测试 k-NN 分类器吗？</title>
      <link>https://stackoverflow.com/questions/78581759/can-i-train-and-test-a-k-nn-classifier-on-data-from-different-experimental-condi</link>
      <description><![CDATA[我有一些关于单个神经元群体的数据，我想看看在刺激前基线期间观察到的活动模式是否与刺激出现时观察到的活动模式具有相似的结构。基本上，当预期刺激（在基线）与显示的刺激相同时，我预计解码准确度只会略低，但当预期和实际刺激不匹配时，我预计它们之间的活动模式会有所不同，这将导致解码性能比预期和刺激匹配时更差（即前一种情况）。
我知道 k-最近邻分类器的工作原理，也知道如何实现它。我不知道这是否是一种常见/合理的方法。
我尝试使用这种方法寻找出版物，但我没有找到任何出版物，坦率地说，我不确定我是否使用了正确的搜索词...
任何帮助 - 以解释、链接、文献参考、搜索词的形式 - 都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78581759/can-i-train-and-test-a-k-nn-classifier-on-data-from-different-experimental-condi</guid>
      <pubDate>Wed, 05 Jun 2024 14:36:05 GMT</pubDate>
    </item>
    <item>
      <title>llama-index、uncharted 和 llama2:7b 在本地运行以生成索引</title>
      <link>https://stackoverflow.com/questions/78581041/llama-index-uncharted-and-llama27b-run-locally-to-generate-index</link>
      <description><![CDATA[我想在本地使用 llama-index 和 ollama 以及 llama3:8b 来索引 utf-8 json 文件。我没有 gpu。我使用 uncharted 将文档转换为 json。现在，如果没有 GPU 就无法在本地使用 llama-index，我想使用 hugging face 推理 API。但我不确定它是否免费。有人能建议一种方法吗？
这是我的 python 代码：


from llama_index.core import Document, SimpleDirectoryReader, VectorStoreIndex
from llama_index.llms.ollama import Ollama
import json
from llama_index.core import Settings

# 将 JSON 文档转换为 LlamaIndex Document 对象
with open(&#39;data/UBER_2019.json&#39;, &#39;r&#39;,encoding=&#39;utf-8&#39;) as f:
json_doc = json.load(f)
documents = [Document(text=str(doc)) for doc in json_doc]

# 使用本地 LLM 初始化 Ollama
ollama_llm = Ollama(model=&quot;llama3:8b&quot;)
Settings.llm = ollama_llm

# 使用本地 LLM 创建索引
index = VectorStoreIndex.from_documents(documents)#, llm=ollama_llm)


但我一直收到没有 OPENAI 密钥的错误。我想使用 llama2，这样就不需要 OPENAI 密钥了
有人能指出我做错了什么吗？我还可以免费使用 huggingfaceinference API 对本地 json 文件进行索引吗？]]></description>
      <guid>https://stackoverflow.com/questions/78581041/llama-index-uncharted-and-llama27b-run-locally-to-generate-index</guid>
      <pubDate>Wed, 05 Jun 2024 12:38:14 GMT</pubDate>
    </item>
    <item>
      <title>在没有批处理数据的情况下实施批处理训练</title>
      <link>https://stackoverflow.com/questions/78580702/implementing-batched-training-without-batching-data</link>
      <description><![CDATA[我有一个初学者的问题，似乎在任何地方都找不到明确的答案。
据我所知，进行批量训练有两个好处：

批量操作可以在 GPU 上得到更好的优化，从而提高性能
优化器在执行 .step() 之前查看多个数据样本后运行，这使得模型对单个样本的敏感度降低。

我的问题如下：如果我们忽略性能优势，这两个训练循环是否等效？
正确的批处理：
model = SomeModel()
optimizer = SomeOptimizer(model.parameters(), ...)
dataloader = DataLoader(dataset, batch_size=10)

for batch_data in enumerate(dataloader):
loss = model(batch_data)
loss.backward()
optimizer.step()
optimizer.zero_grad()

手动批处理？：
model = SomeModel()
optimizer = SomeOptimizer(model.parameters(), ...)
dataloader = DataLoader(dataset, batch_size=10)

for batch_data in enumerate(dataloader):
for i in range(10):
loss = model(batch_data[i])
loss.backward()

optimizer.step()
optimizer.zero_grad()

如果它们不等同，那么区别是什么？是否可以在不将批处理数据传入模型的情况下进行补救？
就上下文而言，由于时间压力，我无法使用批处理输入来实现模型，但我仍然想模拟批处理学习的效果（除了性能优势）
提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/78580702/implementing-batched-training-without-batching-data</guid>
      <pubDate>Wed, 05 Jun 2024 11:37:14 GMT</pubDate>
    </item>
    <item>
      <title>无监督学习和类别不平衡</title>
      <link>https://stackoverflow.com/questions/78579967/unsupervised-learning-and-class-imbalance</link>
      <description><![CDATA[我有一个包含大约 7000 个观测值和 18 个变量的数据集。只有年龄变量是连续的。分类变量具有巨大的类别不平衡。我想使用无监督学习来分析数据。有什么建议吗？我发现在没有专家帮助的情况下很难做到这一点。类别不平衡是由于罕见的情况造成的。我尝试了 K 均值聚类，但没有明显的聚类。我也尝试了 t-SNE，但无法产生任何推论]]></description>
      <guid>https://stackoverflow.com/questions/78579967/unsupervised-learning-and-class-imbalance</guid>
      <pubDate>Wed, 05 Jun 2024 09:20:46 GMT</pubDate>
    </item>
    <item>
      <title>对图像应用离散余弦变换 (DCT) 时获取黑色图像</title>
      <link>https://stackoverflow.com/questions/78579740/getting-black-images-while-applying-discrete-cosine-transform-dct-on-image</link>
      <description><![CDATA[在对图像应用离散余弦变换 (DCT) 时获取黑色图像

def perform_dct(image_path):
# 以灰度加载图像
img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
if img is None:
raise ValueError(f&quot;Image at {image_path} could not be loaded.&quot;)
# 将图像大小调整为 224x224（ViT 模型预期的大小）

img = cv2.resize(img, (224, 224))
# 执行 DCT
dct_img = cv2.dct(np.float32(img))

# 将 DCT 图像规范化为范围[0, 1]
dct_img_min = np.min(dct_img)
dct_img_max = np.max(dct_img)
dct_img_normalized = (dct_img - dct_img_min) / (dct_img_max - dct_img_min)

# 转换为 3 通道 RGB 图像
dct_img_normalized = (dct_img_normalized * 255).astype(np.uint8)
dct_img_rgb = cv2.merge([dct_img_normalized, dct_img_normalized, dct_img_normalized])

plt.imshow(dct_img_rgb)
plt.title(&quot;DCT of Imagesss&quot;)
plt.show()

return dct_img_rgb

尝试获取离散余弦变换 (DCT) 图像，但获取的是整个黑色图像，我无法在代码中发现错误。如果可以，请帮助我]]></description>
      <guid>https://stackoverflow.com/questions/78579740/getting-black-images-while-applying-discrete-cosine-transform-dct-on-image</guid>
      <pubDate>Wed, 05 Jun 2024 08:41:59 GMT</pubDate>
    </item>
    <item>
      <title>最新版本的 Mask-RCNN 和 TensorFlow 版本的错误</title>
      <link>https://stackoverflow.com/questions/78579595/newest-version-of-working-mask-rcnn-errors-on-tensorflow-version</link>
      <description><![CDATA[我一直在尝试在生物实验室的细胞图像上实现 Mask-RCNN。
我知道 matterport/Mask_RCNN 无法正常工作，因为它使用的是 TensorFlow 1，所以我尝试使用使用 TensorFlow 2 的 github repos。但我仍然觉得有些已经过时了，或者我的设置不匹配，它没有运行。我一直在使用这个：https://github.com/ahmedfgad/Mask-RCNN-TF2
是否有 2024 年或 2023 年的最新版本可以作为参考？我真的很想尝试在我的系统上实现，但是当我尝试从同一位置修复问题时，不断收到类似 ModuleNotFoundError: 没有名为“keras.engine”的模块 或 ERROR: 找不到满足要求 tensorflow==2.2.0 的版本的错误。]]></description>
      <guid>https://stackoverflow.com/questions/78579595/newest-version-of-working-mask-rcnn-errors-on-tensorflow-version</guid>
      <pubDate>Wed, 05 Jun 2024 08:11:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 logistf 包在 R 中计算 Firth 模型非收敛误差</title>
      <link>https://stackoverflow.com/questions/78579401/firths-model-non-converge-error-in-r-using-the-logistf-package</link>
      <description><![CDATA[这是示例数据的链接（示例数据不大 - 只有 23 kb，但可能是导致错误的原因）：

https://drive.google.com/file/d/1TWkFIKhq9VZkFnhUrt6LxYmab54ouODd/view?usp=sharing

这是我运行 firth 模型的代码。我在不同的运行中遇到了不同的错误（重新启动 r 或 r 会话），有时程序似乎卡住了（但是，活动监视器显示 CPU 使用率为 99%），其他时候我遇到了诸如不收敛之类的错误，并建议我增加迭代次数，但这并没有真正帮助。
library(caret)
library(logistf)
library(data.table)

# 定义训练控制
train_control &lt;- trainControl(method = &quot;repeatedcv&quot;, 
number = 3, repeats = 3,
savePredictions = TRUE,
classProbs = TRUE)

# 定义自定义模型函数
firth_model &lt;- list(
type = &quot;Classification&quot;,
library = &quot;logistf&quot;,
loop = NULL,
parameters = data.frame(parameter = c(&quot;none&quot;), class = c(&quot;character&quot;), label = c(&quot;none&quot;)),
grid = function(x, y, len = NULL, search = &quot;grid&quot;) {
data.frame(none = &quot;none&quot;)
},
fit = function(x, y, wts, param, lev, last, classProbs, ...) {
data &lt;- as.data.frame(x)
data$group &lt;- y
logistf(group ~ ., data = data, control = logistf.control(maxit = 100), ...)
},
predict = function(modelFit, newdata, submodels = NULL) {
as.factor(ifelse(predict(modelFit, newdata, type = &quot;response&quot;) &gt; 0.5, &quot;AD&quot;, &quot;control&quot;))
},
prob = function(modelFit, newdata, submodels = NULL) {
preds &lt;- predict(modelFit, newdata, type = &quot;response&quot;)
data.frame(control = 1 - preds, AD = preds)
}
)

train_proc &lt;- fread(&quot;train_proc.csv&quot;)

# 训练模型
set.seed(123)
firth.logist.model &lt;- train(train_proc[, .SD, .SDcols = !c(&quot;group&quot;)],
train_proc$group,
method = firth_model,
trControl = train_control)

print(firth.logist.model)


这是最新的错误
logistf(group ~ ., data = data, control = logistf.control(maxit = 100), 中出现警告：
未收敛的 PL 置信限度：变量的最大迭代次数：（截距）、x1、x2、x3、x4、x5、x6、x7、x8、x9、x10、x11、x12、x13、x14、x15、x16、x17、x18、x19、x20、x21、x22、x23、x24 超出。尝试通过将“logistpl.control(maxit=...)”传递给参数 plcontrol 来增加迭代次数

相同的代码似乎在某些数据集上运行，但在其他数据集上运行不起来。但这也可能是由于我的函数无法针对特定数据集进行自定义。我遇到了许多不同类型的错误，我开始怀疑 logistf 包本身是否不稳定。
为了提供更多信息，这是我的 r 版本：
R.version
_ 
platform aarch64-apple-darwin20 
arch aarch64 
os darwin20 
system aarch64, darwin20 
status 
major 4 
minor 3.2 
year 2023 
month 10 
day 31 
svn rev 85441 
language R 
version.string R version 4.3.2 (2023-10-31)
nickname Eye Holes 

这是我的包版本：
&gt; packageVersion(&quot;caret&quot;)
[1] ‘6.0.94’
&gt; packageVersion(&quot;logistf&quot;)
[1] ‘1.26.0’
&gt; packageVersion(&quot;data.table&quot;)
[1] ‘1.14.10’
]]></description>
      <guid>https://stackoverflow.com/questions/78579401/firths-model-non-converge-error-in-r-using-the-logistf-package</guid>
      <pubDate>Wed, 05 Jun 2024 07:34:40 GMT</pubDate>
    </item>
    <item>
      <title>R 中的 Firth 模型“重采样性能指标中的缺失值”</title>
      <link>https://stackoverflow.com/questions/78577799/firths-model-in-r-missing-values-in-resampled-performance-measures</link>
      <description><![CDATA[以下是带有样本数据的示例
library(caret)
library(logistf)
library(data.table)
library(ggplot2)

# 创建合成数据集
set.seed(123)
n &lt;- 100
data &lt;- data.table(
predictor1 = rnorm(n),
predictor2 = rnorm(n),
predictor3 = rnorm(n),
group = factor(sample(c(&quot;control&quot;, &quot;AD&quot;), n, replace = TRUE))
)

# 标准化预测变量
data_normalized &lt;- as.data.table(scale(data[, .SD, .SDcols = -&quot;group&quot;]))
data_normalized[, group := data$group]

# 分为训练集和测试集
train_index &lt;- createDataPartition(data_normalized$group, p = .75, list = FALSE)
train &lt;- data_normalized[train_index, ]
test &lt;- data_normalized[-train_index, ]

# 标准化训练数据
train_proc &lt;- predict(preProcess(train, method = c(&quot;center&quot;, &quot;scale&quot;), verbose = TRUE), train)
train_proc[, group := factor(group, levels = c(&quot;control&quot;, &quot;AD&quot;))]

# 定义训练控制
train_control &lt;- trainControl(method = &quot;repeatedcv&quot;, 
number = 3, repeats = 3,
savePredictions = TRUE,
classProbs = TRUE)

# 定义自定义模型函数
firth_model &lt;- list(
type = &quot;Classification&quot;,
library = &quot;logistf&quot;,
loop = NULL,
parameters = data.frame(parameter = c(&quot;none&quot;), class = c(&quot;character&quot;), label = c(&quot;none&quot;)),
grid = function(x, y, len = NULL, search = &quot;grid&quot;) {
data.frame(none = &quot;none&quot;)
},
fit = function(x, y, wts, param, lev, last, classProbs, ...) {
data &lt;- as.data.frame(x)
data$group &lt;- y
logistf(group ~ ., data = data, control = logistf.control(maxit = 100), ...)
},
predict = function(modelFit, newdata, submodels = NULL) {
as.factor(ifelse(predict(modelFit, newdata, type = &quot;response&quot;) &gt; 0.5, &quot;AD&quot;, &quot;control&quot;))
},
prob = function(modelFit, newdata, submodels = NULL) {
preds &lt;- predict(modelFit, newdata, type = &quot;response&quot;)
data.frame(control = 1 - preds, AD = preds)
}
)

# 训练模型
set.seed(123)
firth.logist.model &lt;- train(train_proc[, .SD, .SDcols = !c(&quot;group&quot;)],
train_proc$group,
method = firth_model,
trControl = train_control)

print(firth.logist.model)


收到以下错误:
&gt; firth.logist.model &lt;- train(train_proc[, .SD, .SDcols = !c(&quot;group&quot;)],
+ train_proc$group,
+ method = firth_model,
+ trControl = train_control)
出现问题；所有准确度指标值均缺失：
准确度 Kappa 
最小值：NA 最小值：NA 
第 1 组：NA 第 1 组：NA 
中位数：NA 中位数：NA 
平均值：NaN 平均值：NaN 
第 3 组：NA 第 3 组：NA 
最大值：NA 最大值: NA 
NA :1 NA :1 
错误：正在停止
此外：警告消息：
在 normTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
中，重新采样的性能指标中缺少值。


我试图比较不同模型的性能，当我使用 caret 中的逻辑回归或随机森林时，模型都有效（使用相同的训练、交叉验证和测试数据），所以不确定 Firth 模型在这里失败的原因是什么。]]></description>
      <guid>https://stackoverflow.com/questions/78577799/firths-model-in-r-missing-values-in-resampled-performance-measures</guid>
      <pubDate>Tue, 04 Jun 2024 21:24:40 GMT</pubDate>
    </item>
    <item>
      <title>Julia MLJ Forest Load：错误：MethodError：没有与 BetaML.Bmlj.RandomForestRegressor() 匹配的方法</title>
      <link>https://stackoverflow.com/questions/78577415/julia-mlj-forest-loaderror-methoderror-no-method-matching-betaml-bmlj-randomf</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78577415/julia-mlj-forest-loaderror-methoderror-no-method-matching-betaml-bmlj-randomf</guid>
      <pubDate>Tue, 04 Jun 2024 19:40:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么MobileNetv2微调时Conv中的数组形状不匹配？</title>
      <link>https://stackoverflow.com/questions/78576851/why-the-shape-of-array-in-conv-when-fine-tuning-mobilenetv2-doesnt-match</link>
      <description><![CDATA[我正在尝试从检查点（https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet，请参阅“Mobilenet V2 Imagenet 检查点”下的“float_v2_1.4_224”）对 MobileNetv2 模型进行微调，以完成使用 Deeplabv3 进行图像分割的任务。我按照 deeplab repo 中描述的方法，将 pascal VOC 数据集替换为我自己的数据集，该数据集由大小为 224x224 的图像组成（与用于训练该检查点的图像大小相同），然后使用此脚本作为参考，以确定要设置哪些标志来训练 mobilenetv2。（https://github.com/tensorflow/models/blob/master/research/deeplab/local_test_mobilenetv2.sh）
这是我正在使用的命令：
 python deeplab/train.py
--logtostderr 
--train_split=&quot;train&quot; 
--model_variant=&quot;mobilenet_v2&quot;
--output_stride=16
--train_crop_size=&quot;225,225&quot; #图像为 224x224，因此根据 tensorflow/research/deeplab 常见问题解答将其设置为此尺寸
--train_batch_size=4 
--training_number_of_steps=100
--fine_tune_batch_norm=true
--dataset=&quot;custom&quot; #正确定义为其自己的数据集
--tf_initial_checkpoint=&quot;./deeplab/data_resized/initial_checkpoint/mobilenet_v2_1.4_224.ckpt&quot;
--train_logdir=&quot;./deeplab/data_resized/checkpoint&quot;
--dataset_dir=&quot;./deeplab/data_resized/ltfrecord&quot;

但我一直收到以下错误：
 回溯（最近一次调用）：
文件 &quot;deeplab/train.py&quot;，第 464 行，位于 &lt;module&gt;
tf.app.run()
文件“C:\Users\Luca\.pyenv\pyenv-win\versions\3.7.6\lib\site-packages\tensorflow_core\python\platform\app.py”，第 40 行，运行中
_run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
文件“C:\Users\Luca\.pyenv\pyenv-win\versions\3.7.6\lib\site-packages\absl\app.py”，第 308 行，运行中
_run_main(main, args)
文件“C:\Users\Luca\.pyenv\pyenv-win\versions\3.7.6\lib\site-packages\absl\app.py”，第 254 行，运行中
sys.exit(main(argv))
文件 &quot;deeplab/train.py&quot;，第 444 行，在 main
ignore_missing_vars=True)
文件 &quot;C:\Users\Luca\Documents\marathon\facade_ml\training_gitclone\models-master\research\deeplab\utils\train_utils.py&quot;，第 221 行，在 get_model_init_fn
ignore_missing_vars=ignore_missing_vars)
文件 &quot;C:\Users\Luca\.pyenv\pyenv-win\versions\3.7.6\lib\site-packages\tensorflow_core\contrib\framework\python\ops\variables.py&quot;，第 690 行，在assign_from_checkpoint
(ckpt_name, str(ckpt_value.shape), str(var.get_shape())))
ValueError: Total对于 MobilenetV2/Conv/weights，新数组的大小必须保持不变 lh_shape: [(3, 3, 3, 48)], rh_shape: [(3, 3, 3, 32)]

我尝试过将其他配置值（例如将“initialize_last_layer”更改为 false 并将“last_layers_contain_logits_only”更改为 false）但这没有帮助，而且 Google 和 GitHub 上似乎都不存在有关此问题的信息。一位用户建议将“depth_multiplier”标志更改为 0.5 和 1.4，但除了将 rh 形状更改为 [3,3,3,16] 和 [3,3,3,45] 之外，它没有任何效果。我认为 dm=1.4 是假定值，因为它几乎与形状非常接近，但有一些我无法理解的填充/缺失。
感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78576851/why-the-shape-of-array-in-conv-when-fine-tuning-mobilenetv2-doesnt-match</guid>
      <pubDate>Tue, 04 Jun 2024 17:13:08 GMT</pubDate>
    </item>
    <item>
      <title>为什么会出现此错误？“TransformerMixin.fit_transform() 缺少 1 个必需的位置参数：'X'”-机器学习相关问题 [重复]</title>
      <link>https://stackoverflow.com/questions/78575884/why-is-this-error-coming-up-transformermixin-fit-transform-missing-1-require</link>
      <description><![CDATA[我有这行代码
cols_to_scale = [&#39;tenure&#39;,&#39;MonthlyCharges&#39;,&#39;TotalCharges&#39;]

来自 sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler

df2[cols_to_scale] = scaler.fit_transform(df2[cols_to_scale])

这会引发此错误，为什么？
TypeError Traceback (most recent call last)
Cell In[77], line 6
3 来自 sklearn.preprocessing import MinMaxScaler
4 scaler = MinMaxScaler
----&gt; 6 df2[cols_to_scale] = scaler.fit_transform(df2[cols_to_scale])

TypeError: TransformerMixin.fit_transform() 缺少 1 个必需的位置参数：“X”

不知道为什么会发生此错误。]]></description>
      <guid>https://stackoverflow.com/questions/78575884/why-is-this-error-coming-up-transformermixin-fit-transform-missing-1-require</guid>
      <pubDate>Tue, 04 Jun 2024 13:58:35 GMT</pubDate>
    </item>
    <item>
      <title>无论如何在 tensorflow 2.16 上使用 tensorflow 的 set_session</title>
      <link>https://stackoverflow.com/questions/78575816/anyway-to-use-tensorflows-set-session-on-tensorflow-2-16</link>
      <description><![CDATA[我完全不知道该如何为项目导入这个 set_session。我尝试过使用：
from tensorflow.compat.v1.keras.backend import set_session

and
from tensorflow.keras.backend import set_session`

（以及许多其他变体）
一切都没希望了吗？我使用的是 python 3.11.1、tensorflow 2.16.1（安装 keras 3.3.3）

我希望让这个程序尽可能容易运行，但似乎唯一的方法可能是安装旧版本的 tensorflow（使用 pip 无法做到这一点）
set_session 应该可以工作，我已经让它在旧版本的 tensorflow 上运行，我只是希望它能够友好地下载我的程序，而不需要经历这种挣扎。我也用过这个：
def get_session(gpu_fraction): #为 TensorFlow 设置 GPU 内存使用量
config = tf.compat.v1.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = gpu_fraction
return tf.compat.v1.Session(config=config)
]]></description>
      <guid>https://stackoverflow.com/questions/78575816/anyway-to-use-tensorflows-set-session-on-tensorflow-2-16</guid>
      <pubDate>Tue, 04 Jun 2024 13:45:55 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用相同的 Q 值表会得到不同的测试结果</title>
      <link>https://stackoverflow.com/questions/78564460/why-do-i-get-different-testing-result-using-the-same-q-value-table</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78564460/why-do-i-get-different-testing-result-using-the-same-q-value-table</guid>
      <pubDate>Sat, 01 Jun 2024 17:17:18 GMT</pubDate>
    </item>
    <item>
      <title>使用单类 SVM 计算异常检测的异常分数</title>
      <link>https://stackoverflow.com/questions/53956538/calculating-anomaly-score-for-anomaly-detection-using-one-class-svm</link>
      <description><![CDATA[我对使用单类 SVM 计算异常检测的异常分数有疑问。我的问题是：如何使用 decision_function(X) 计算它，就像我在隔离森林中计算异常分数一样？
非常感谢，]]></description>
      <guid>https://stackoverflow.com/questions/53956538/calculating-anomaly-score-for-anomaly-detection-using-one-class-svm</guid>
      <pubDate>Fri, 28 Dec 2018 09:50:20 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 中的步骤和时期有什么区别？</title>
      <link>https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow</link>
      <description><![CDATA[在大多数模型中，都有一个 steps 参数，表示在数据上运行的步骤数。但我在大多数实际使用中看到，我们还会执行拟合函数 N epochs。
用 1 个 epoch 运行 1000 步和用 10 个 epoch 运行 100 步有什么区别？在实践中哪一个更好？连续 epoch 之间有任何逻辑变化吗？数据混洗？]]></description>
      <guid>https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow</guid>
      <pubDate>Tue, 12 Jul 2016 23:20:22 GMT</pubDate>
    </item>
    </channel>
</rss>