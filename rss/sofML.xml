<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 06 Jul 2024 03:18:17 GMT</lastBuildDate>
    <item>
      <title>此函数能找到 pytorch 张量中最后一个元素的内存地址吗？</title>
      <link>https://stackoverflow.com/questions/78713769/does-this-function-find-the-memory-address-of-the-last-element-in-a-pytorch-tens</link>
      <description><![CDATA[我想知道我编写的这个函数是否能找到 pytorch 张量中最后一个元素的内存地址。我运行它，它看起来不错，但有几次它返回了张量的“最后一个元素”，并且它显示它位于内存中比第一个元素更靠后的部分，我使用 .data_ptr() 函数获得第一个元素。张量是连续的，尺寸为 (3,1000,1000)。
我将张量作为参数，然后获取张量的最后一个索引，然后调用 data_ptr() 函数。以下是我的代码：
def get_last_mem(tensor):
last_element = tensor[-1,-1].data_ptr()
return last_element

这是我的输出
变换前的第一个元素内存地址：136496996679680，连续：True
变换前的最后一个元素内存地址：136497008675680]]></description>
      <guid>https://stackoverflow.com/questions/78713769/does-this-function-find-the-memory-address-of-the-last-element-in-a-pytorch-tens</guid>
      <pubDate>Sat, 06 Jul 2024 03:07:18 GMT</pubDate>
    </item>
    <item>
      <title>我加载一个 float32 Hugging Face 模型，将其转换为 float16，然后保存。我该如何将其加载为 float16？</title>
      <link>https://stackoverflow.com/questions/78713551/i-load-a-float32-hugging-face-model-cast-it-to-float16-and-save-it-how-can-i</link>
      <description><![CDATA[我加载一个 huggingface-transformers float32 模型，将其转换为 float16，然后保存。我如何将其加载为 float16？
示例：
# pip install tr​​ansformers
from transformers import AutoModelForTokenClassification, AutoTokenizer

# 加载模型
model_path = &#39;huawei-noah/TinyBERT_General_4L_312D&#39;
model = AutoModelForTokenClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# 将模型转换为 FP16
model.half()

# 检查模型 dtype
def print_model_layer_dtype(model):
print(&#39;\nModel dtypes:&#39;)
for name, param in model.named_pa​​rameters():
print(f&quot;参数：{name}，数据类型：{param.dtype}&quot;)

print_model_layer_dtype(model)
save_directory = &#39;temp_model_SE&#39;
model.save_pretrained(save_directory)

model2 = AutoModelForTokenClassification.from_pretrained(save_directory, local_files_only=True)
print(&#39;\n\n##################&#39;)
print(model2)
print_model_layer_dtype(model2)

在此示例中，model2 加载为 float32 模型（如 print_model_layer_dtype(model2) 所示），即使 model2 已保存为 float16（如 config.json 中所示）。将其加载为 float16 的正确方法是什么？
在 Windows 10 上使用 transformers==4.36.2 和 Python 3.11.7 进行了测试。]]></description>
      <guid>https://stackoverflow.com/questions/78713551/i-load-a-float32-hugging-face-model-cast-it-to-float16-and-save-it-how-can-i</guid>
      <pubDate>Fri, 05 Jul 2024 23:58:06 GMT</pubDate>
    </item>
    <item>
      <title>AlexNet 的顶层和底层如何通信？</title>
      <link>https://stackoverflow.com/questions/78713337/how-do-the-top-and-bottom-layers-of-alexnet-communicate</link>
      <description><![CDATA[我正在尝试使用 PyTorch 重新实现 Krizhevsky et al. (2012)，并且我对 AlexNet 模型的第二和第三卷积层如何精确通信感到困惑（第五层到第六层以及第六层到第七层的输入也是如此，尽管我在这里的问题中省略了这一点）。
在下图中，有两个&quot;过滤器&quot;，它们将输出从上半部分传递到下一个上半部分，但也传递到下半部分。同样，下半部分也有两个&quot;过滤器&quot;将输出传递到下一个下半部分和上半部分。
我没有足够的声誉点来嵌入图像，所以这里是Krizhevsky et al. (2012) 的图 1 的部分屏幕截图。
第二层的输出如何传递到第三层？
我读了这篇论文，除非我错过了什么，否则作者似乎没有准确概述输出是如何从第二层传递到第三层的。我浏览了大量博客文章和 git 存储库，大多数描述都是高级的，大多数实现似乎没有将模型拆分到两个 GPU 之间。
我能找到的最相关的内容是来自 convnet2 readme 的以下句子：

这里，层 conv2a 和 conv2b 将 conv1a 和 conv1b 都作为输入。执行隐式复制操作，以便将 conv1a 的输出放入 conv2b 的输入中，以及将 conv1b 的输出放入 conv2a 的输入中。

我最好的猜测是第二层中的 out_channels 参数实际上应该是 64 而不是 128，然后顶层和底层的输出应该连接为 torch.cat([output_from_top_half, output_from_bottom_half], dim=1) 并传递给第三层的上半部分和下半部分。但我不确定我的理解是否正确。]]></description>
      <guid>https://stackoverflow.com/questions/78713337/how-do-the-top-and-bottom-layers-of-alexnet-communicate</guid>
      <pubDate>Fri, 05 Jul 2024 21:44:17 GMT</pubDate>
    </item>
    <item>
      <title>XGBClassifier.fit() 得到了一个意外的关键字参数“early_stopping_rounds”</title>
      <link>https://stackoverflow.com/questions/78713048/xgbclassifier-fit-got-an-unexpected-keyword-argument-early-stopping-rounds</link>
      <description><![CDATA[我的代码如下：
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
import pandas as pd
RANDOM_STATE = 55 ## 您将把它传递给每个 sklearn 调用，以便我们确保可重复性
n = int(len(X_train)*0.8) ## 让我们使用 80% 进行训练，20% 进行评估
这将用独热编码的列替换列，并保持“列”参数之外的列不变。
df = pd.read_csv(&quot;doc/heart.csv&quot;)
cat_variables = [&#39;Sex&#39;,
&#39;ChestPainType&#39;,
&#39;RestingECG&#39;,
&#39;ExerciseAngina&#39;,
&#39;ST_Slope&#39;
]
df = pd.get_dummies(data = df,
prefix = cat_variables,
columns = cat_variables)
var = [x for x in df.columns if x not in &#39;HeartDisease&#39;] ## 删除我们的目标变量
X_train, X_test, y_train, y_test = train_test_split(df[var], df[&#39;HeartDisease&#39;], train_size = 0.8, random_state = RANDOM_STATE)
print(X_train.shape)
X_train_fit, X_train_eval, y_train_fit, y_train_eval = X_train[:n], X_train[n:], y_train[:n], y_train[n:]
import xgboost
print(xgboost.__version__) # 2.1.0
xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)
xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)],early_stopping_rounds = 10)

详细错误信息如下：
Traceback (most recent call last):
File &quot;C:\my_document\11_Python\exercise\main.py&quot;, line 153, in &lt;module&gt;
xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)],early_stopping_rounds = 10)
文件 &quot;C:\Users\samc\AppData\Local\Programs\Python\Python312\Lib\site-packages\xgboost\core.py&quot;，第 726 行，在 inner_f
return func(**kwargs)
^^^^^^^^^^^^^^^
TypeError: XGBClassifier.fit() 获得意外的关键字参数“early_stopping_rounds”

我该如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78713048/xgbclassifier-fit-got-an-unexpected-keyword-argument-early-stopping-rounds</guid>
      <pubDate>Fri, 05 Jul 2024 19:33:28 GMT</pubDate>
    </item>
    <item>
      <title>需要提高我的卷积神经网络模型的准确性。我对数据或模型所做的任何更改都不会影响准确性</title>
      <link>https://stackoverflow.com/questions/78712868/need-to-increase-the-accuracy-of-my-convolution-neural-network-model-the-accura</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78712868/need-to-increase-the-accuracy-of-my-convolution-neural-network-model-the-accura</guid>
      <pubDate>Fri, 05 Jul 2024 18:23:27 GMT</pubDate>
    </item>
    <item>
      <title>关于 Hugging Face 的 Tokenizer</title>
      <link>https://stackoverflow.com/questions/78712655/regarding-tokenizer-of-hugging-face</link>
      <description><![CDATA[很难理解 tokenizer 的工作原理
from transformers import AutoModelForSequenceClassification,AutoTokenizer #hugging face libraries
tkz = AutoTokenizer.from_pretrained(model)

函数
def tkz_func(x): return tkz(x[&#39;input&#39;])

当我们将它应用于数据集时，它可以完美运行，返回包含 input_ids、token_type_ids、attention_masks 的更新数据集
当我们将它应用于数据框 df.apply(tkz_func,axis=1) 时，它只会返回所有行值的行名列表
[input_ids,token_type_ids,attention_masks]。
为什么？]]></description>
      <guid>https://stackoverflow.com/questions/78712655/regarding-tokenizer-of-hugging-face</guid>
      <pubDate>Fri, 05 Jul 2024 17:15:32 GMT</pubDate>
    </item>
    <item>
      <title>一旦使用测试集进行测试为什么不在训练加测试集上训练模型？</title>
      <link>https://stackoverflow.com/questions/78712450/once-using-the-test-set-for-testing-why-do-not-training-the-model-on-train-plus</link>
      <description><![CDATA[一旦我使用测试集来评估模型性能是否良好，为什么我不能从头开始在训练和测试集上重新训练模型？
以预测为例：我有一个长度为 L 的时间序列，我训练到 T（T &lt; L），然后从 T 到 L 进行测试。我获得了良好的表现，所以我知道模型很好。然后在推理时我必须预测 L+1 个元素，那么为什么我不能训练模型到 L，然后在 L+1 上进行推理？
我希望模型在更大的数据集（训练+测试）上训练后表现更好，那么我遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78712450/once-using-the-test-set-for-testing-why-do-not-training-the-model-on-train-plus</guid>
      <pubDate>Fri, 05 Jul 2024 16:18:01 GMT</pubDate>
    </item>
    <item>
      <title>寻找包含带有摘要分析注释的电子表格的数据集</title>
      <link>https://stackoverflow.com/questions/78712190/looking-for-dataset-containing-spreadsheets-with-summary-analytical-comments</link>
      <description><![CDATA[我正在开展一个机器学习项目，该项目需要一种独特类型的数据集。具体来说，我需要包含分析注释或描述的 CSV 或 XLSX 格式的电子表格。
我正在寻找这样的数据集：每个电子表格都有清晰的行和列标题，并在文件底部或注释部分包含注释，以像人类一样描述数据。这些注释应该解释数据、提供见解或描述数据所代表的内容，而不仅仅是元数据或文档。
例如，请参阅附件示例电子表格。此表显示了学生的每周时间表，下面有分析注释，根据下面注释中的数据描述了潜在问题和建议。
数据集可以来自任何领域，而不仅仅是学术领域。示例包括：

商业公司：公司的季度资产负债表，附有描述业绩和未来计划的评论。
政府报告：龙卷风发生统计数据，附有突出重要年份和因素的评论。
个人信息：财务数据，附有财富分配和促成因素的分析。

有谁知道这类电子表格的集合或我可以找到这类数据集的来源？或者，任何关于如何有效创建或注释这些数据集的建议都将不胜感激。
我也在考虑购买这类数据集。例如，Statista 提供带注释的数据，但我在付费之前不确定数据量。像这样的平台或其他可以让我与销售代表交谈的来源会很有帮助。
我正在考虑的另一种方法是下载长期上市公司的季度财务报告，并添加来自《华尔街日报》等稳定记者的评论。但是，找到预先存在的数据集会更有益。
我已经使用“带分析的电子表格”和“带注释的电子表格”等关键字在 Google 搜索、Statista、Kaggle、Google 数据集搜索和美国人口普查局等来源上搜索过，但仍然需要进一步的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78712190/looking-for-dataset-containing-spreadsheets-with-summary-analytical-comments</guid>
      <pubDate>Fri, 05 Jul 2024 15:16:23 GMT</pubDate>
    </item>
    <item>
      <title>对失败的测试进行分类，并提出解决方案 [关闭]</title>
      <link>https://stackoverflow.com/questions/78711965/classification-of-failed-tests-and-fix-them-with-solution-proposal</link>
      <description><![CDATA[我现在在一家运行测试台的组织工作，这些测试是用 C 和 C++ 开发的。
当执行结束时，我们通过了测试，并且测试失败了，每个失败的测试都会生成一个日志文件。
现在我们想使用 IA 对这些失败的测试进行分类（因为有些测试有相同的错误）。
我们想预测这些失败测试的解决方案。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78711965/classification-of-failed-tests-and-fix-them-with-solution-proposal</guid>
      <pubDate>Fri, 05 Jul 2024 14:21:48 GMT</pubDate>
    </item>
    <item>
      <title>Darknet Yolov4-tiny（灰度输入）到 Tensorflow 权重，转换</title>
      <link>https://stackoverflow.com/questions/78711847/darknet-yolov4-tiny-grayscale-input-to-tensorflow-weights-conversion</link>
      <description><![CDATA[TL;DR:
1 通道 TF 模型的行为与 3 通道模型不同。两者都成功从 Darknet -&gt; TF 转换，但 1 通道模型的表现不如转换前。
手头的任务和声明：
我有两个经过训练的 yolov4-tiny darknet 权重文件 (.weights)，一个有灰度输入（1 通道），另一个有颜色输入（3 通道）。我正在将两个权重文件转换为 Tensorflow 检查点格式，使用一个通用存储库（用于此任务），该存储库位于：
https://github.com/hunglc007/tensorflow-yolov4-tflite.git
两种模型的性能都已通过 c++ opencv LoadFromDarknet() 和 Python 等效项进行了测试。这两个模型本质上都是用灰度图像进行训练的，并且对灰度图像进行操作。3 通道模型的输入只是缩放到 3 通道的灰度图像。
Python 版本：3.10.11
TF 版本：2.10.1
问题陈述：
使用 tf.keras.Models.load_model(X) 加载时，带有颜色输入的权重文件转换良好，之后运行良好，但是当转换灰度输入权重文件时，使用 Tensorflow 加载时模型的性能急剧下降，我的意思是在最明显的情况下，带有颜色输入的模型运行完美，检测效果很差或不存在。值得注意的是，框不会错位，这意味着当发现检测结果时，它们大约在正确的位置，但例如宽度和高度可能会偏离。
我知道这个存储库的常见问题（硬编码内容等），并相应地更改了每次转换/模型加载的参数，并且在转换或模型加载期间不会发生任何错误。
我已经确认了输入层：

灰度：（无，640,640,1）
颜色：（无，640,640,3）

测试图像（用于性能测试）使用 opencv-python 加载，并且它们的有效性也已审查，即使将错误维度的数据插入到输入层也会出现错误。
除输入层之外的架构相同，已使用 model.summary() 确认。
我注意到，几年前我用不同的 TF 版本转换的 3 通道模型由 model.summary() 生成的架构有些不同。一些图块层似乎缺失了。此外，一些 tf 操作的名称也不同，但这可能只是 TF 版本不同。
旧颜色模型：
 tf_op_layer_Sigmoid (TensorFlo (None, 40, 40, 3, 2 0 [&#39;tf_op_layer_split_3[0][0]&#39;]
wOpLayer) )

tf_op_layer_Tile/multiples (Te (5,) 0 [&#39;tf_op_layer_strided_slice[0][0]
nsorFlowOpLayer) &#39;]

tf_op_layer_Sigmoid_3 (TensorF (None, 20, 20, 3, 2 0 [&#39;tf_op_layer_split_4[0][0]&#39;]
lowOpLayer) ) )

新灰度模型：
 tf.math.sigmoid (TFOpLambda) (无，40，40，3，2 0 [&#39;tf.split_3[0][0]&#39;]
)

---此处缺少图块层---

tf.math.sigmoid_3 (TFOpLambda) (无，20，20，3，2 0 [&#39;tf.split_4[0][0]&#39;]
)

我现在很卡。有什么帮助吗？
一些反复试验：

使用 Yolov4-tiny Head 解码块 -&gt;即使在模型能够加载的情况下也没有变化（解码时错误的尺寸会引发错误）
之前提到的旧 3 通道模型（几年前已转换为 Darknet -&gt; TF），以与新模型相同的方式加载时可以完美运行
]]></description>
      <guid>https://stackoverflow.com/questions/78711847/darknet-yolov4-tiny-grayscale-input-to-tensorflow-weights-conversion</guid>
      <pubDate>Fri, 05 Jul 2024 13:54:37 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的上游与下游是什么？</title>
      <link>https://stackoverflow.com/questions/78711787/what-is-upstream-vs-downstream-in-machine-learning</link>
      <description><![CDATA[目前有很多深度学习模型库，这些模型被上传到那里用于下游任务。但是，我相信有目的地保存用于训练这些模型的代码，例如 GitHub。在这种情况下，我们可以将深度学习库称为下游，将代码库称为上游吗？]]></description>
      <guid>https://stackoverflow.com/questions/78711787/what-is-upstream-vs-downstream-in-machine-learning</guid>
      <pubDate>Fri, 05 Jul 2024 13:40:14 GMT</pubDate>
    </item>
    <item>
      <title>当它们都使用同一个类时，我该如何抓取整个表？到目前为止，我只能得到名称</title>
      <link>https://stackoverflow.com/questions/78710808/how-do-i-scrape-a-whole-table-when-they-all-use-the-same-class-i-can-so-far-ge</link>
      <description><![CDATA[我试图获取标有“关键利率”的表中的数据。但是，我只能提取名称，因为它们具有独特的样式（）。
我想要表中的数据并使用 Pandas 将其排列成表。
请帮忙
代码：
URL = &quot;https://www.centralbank.go.ke/&quot;
page = request.get(url)
soup = BeautifulSoup(page.content, &#39;html.parser&#39;)

job_elems = soup.find_all(&#39;td&#39;, class_=&quot;tg-4eph&quot;) 
for job_elem in job_elems: 
title_elem = job_elem.find(&#39;small&#39;) 
if title_elem: 
print(title_elem.text.strip())
]]></description>
      <guid>https://stackoverflow.com/questions/78710808/how-do-i-scrape-a-whole-table-when-they-all-use-the-same-class-i-can-so-far-ge</guid>
      <pubDate>Fri, 05 Jul 2024 09:55:49 GMT</pubDate>
    </item>
    <item>
      <title>将最优模型应用于测试集</title>
      <link>https://stackoverflow.com/questions/78707916/to-apply-the-optimal-model-to-the-test-set</link>
      <description><![CDATA[我有一个数据集需要训练和测试，还有另一个数据集作为测试集。
我已经使用训练数据集获得了最佳模型，并希望将该模型应用于测试集进行预测，但遇到了此错误消息：
ValueError：特征名称应与拟合期间传递的特征名称匹配。
在拟合时看到但现在缺失的特征名称：
- 教育
- 婚姻

如何解决此错误？
示例数据集

# 加载和预处理训练数据
df = pd.read_csv(&#39;training.csv&#39;)
df.drop([&#39;USAGE(0)&#39;, &#39;CUSTOMERID&#39;], axis=1, inplace=True)

# 初始化编码器并拟合训练数据
encoder = OneHotEncoder(drop=&#39;first&#39;, sparse_output=False)
encoder.fit(df.select_dtypes(include=[&#39;object&#39;]))

# 在训练数据中编码字符串类型变量
for column in df.select_dtypes(include=[&#39;object&#39;]).columns:
coded_result =coder.transform(df[[column]])
coded_df = pd.DataFrame(encoded_result, columns=encoder.get_feature_names_out([column]))
df.drop(column, axis=1, inplace=True)
df = pd.concat([df,coded_df], axis=1)

# 分离特征和标签
label = &#39;PAYMENT(0)&#39;
excluded_columns = [label]
features = [feature for feature in df.columns if feature not in excluded_columns]
X = df[features]
y = df[label]

# 训练-测试分割
test_size = 0.2
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)

# 构建并训练初始决策树模型
model = DecisionTreeClassifier(criterion=&#39;gini&#39;, min_samples_leaf=3000)
model.fit(X_train, y_train)

# 使用网格搜索和交叉验证进行超参数调整
param_grid = {
&#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;],
&#39;min_samples_leaf&#39;: [10, 20, 30, 40, 50, 60, 70, 80]
}
cv = KFold(n_splits=10, shuffle=True)
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv,scoring=&#39;accuracy&#39;)
grid_search.fit(X_train, y_train)

# 训练最优模型
optimal_model = grid_search.best_estimator_

# 可视化最优决策树
plt.figure(figsize=(100, 20))
plot_tree(optimal_model, filled=True, feature_names=features)
plt.show()

# 加载新测试数据
new_test_df = pd.read_csv(&#39;trial.csv&#39;)

# 保留提交文件的“ID”列
submission_ids = new_test_df[&#39;CUSTOMERID&#39;].copy()

new_test_df.drop(&#39;USAGE(0)&#39;, axis=1, inplace=True)

# 确保所有必需的列都存在
for column in df.select_dtypes(include=[&#39;object&#39;]).columns:
if column not in new_test_df.columns:
new_test_df[column] = 0

# 在新测试数据中编码字符串类型变量
for column in new_test_df.select_dtypes(include=[&#39;object&#39;]).columns:
coded_result =coder.transform(new_test_df[[column]]) # 使用拟合的编码器进行转换
coded_df = pd.DataFrame(encoded_result, columns=encoder.get_feature_names_out([column]))
new_test_df.drop(column, axis=1, inplace=True)
new_test_df = pd.concat([new_test_df,coded_df], axis=1)

# 确保新测试数据具有与训练数据相同的特征列
for feature在功能中：
如果功能不在 new_test_df.columns 中：
new_test_df[功能] = 0
X_new_test = new_test_df[功能]

# 将最佳模型应用于新测试数据
y_new_test_pred = optimal_model.predict(X_new_test)

# 将预测保存到名为“mapping.csv”的 CSV 文件中
submission = pd.DataFrame({
&#39;CUSTOMERID&#39;: submission_ids,
&#39;PAYMENT(0)&#39;: y_new_test_pred
})

submission.to_csv(&#39;mapping.csv&#39;, index=False)
]]></description>
      <guid>https://stackoverflow.com/questions/78707916/to-apply-the-optimal-model-to-the-test-set</guid>
      <pubDate>Thu, 04 Jul 2024 15:27:01 GMT</pubDate>
    </item>
    <item>
      <title>鉴于我的数据集不大且大多数模型都无法生成正确的响应，我该如何训练我的聊天机器人</title>
      <link>https://stackoverflow.com/questions/78705520/how-do-i-train-my-chatbot-given-that-my-dataset-is-not-vast-large-and-most-mode</link>
      <description><![CDATA[我从 llama 开始，因为我被告知要使用 meta-llama（hugging face 上的任何一种），但由于我的笔记本电脑规格有限，meta-llama 从未进入训练阶段。甚至尝试了 meta 中的 10 亿参数模型，仍然不好。
我有一个包含原始数据的 .txt 文件（未按问题：答案对排列），所以我编写了一个脚本，首先创建一个 json 文件，在该文件中，数据按问题：答案对排列，现在我的目标是使用特定模型对其进行微调和训练。我的数据不是庞大/大，最多是小或中等。所以，我尝试了很多模型。Gpt2 等给了我糟糕的回应，所以我尝试了一些其他模型，如 Albert、tiny-bert、roberta-large，但问题是，有些问题确实得到了机器人的正确回答，但其中很大一部分都失败了。有些问题的答案只是重复的、不完整的。
我尝试了几种方法来提高效率（SBERT/TF-IDF 等），甚至尝试修改微调文件，但都无济于事。我对这一切都很陌生，所以我想知道如何继续。T
尝试了几种模型来训练我的聊天机器人，但几乎都失败了。
希望我的聊天机器人能够使用包含问题：答案对的 json 文件进行适当的训练，但是数据可能不会太大。
笔记本电脑也有一些限制（规格，例如低内存 -&gt; 8GB）]]></description>
      <guid>https://stackoverflow.com/questions/78705520/how-do-i-train-my-chatbot-given-that-my-dataset-is-not-vast-large-and-most-mode</guid>
      <pubDate>Thu, 04 Jul 2024 07:06:35 GMT</pubDate>
    </item>
    <item>
      <title>将基于 Bert 的 PyTorch 模型导出到 CoreML。如何让 CoreML 模型适用于任何输入？</title>
      <link>https://stackoverflow.com/questions/78704542/exporting-a-bert-based-pytorch-model-to-coreml-how-can-i-make-the-coreml-model</link>
      <description><![CDATA[我使用以下代码将基于 Bert 的 PyTorch 模型导出到 CoreML。
由于我使用
dummy_input = tokenizer(&quot;A French fan&quot;, return_tensors=&quot;pt&quot;)

在 macOS 上测试时，CoreML 模型仅适用于该输入。如何让 CoreML 模型适用于任何输入（即任何文本）？

导出脚本：
# -*- coding: utf-8 -*-
&quot;&quot;&quot;Core ML Export
pip install tr​​ansformers torch coremltools nltk
&quot;&quot;&quot;
导入 os
从 transformers 导入 AutoModelForTokenClassification、AutoTokenizer
导入 torch
导入 torch.nn 作为 nn
导入 nltk
导入 coremltools 作为 ct
nltk.download(&#39;punkt&#39;)
# 加载模型和 tokenizer
model_path = os.path.join(&#39;model&#39;)
model = AutoModelForTokenClassification.from_pretrained(model_path, local_files_only=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
# 修改模型的 forward 方法以返回元组
class ModifiedModel(nn.Module):
def __init__(self, model):
super(ModifiedModel, self).__init__()
self.model = model
self.device = model.device # 添加设备属性

def forward(self, input_ids,tention_mask, token_type_ids=None):
outputs = self.model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)
returnoutputs.logits

modified_model = ModifiedModel(model)

# 导出到 Core ML
def convert_to_coreml(model, tokenizer):
# 定义用于跟踪的虚拟输入
dummy_input = tokenizer(&quot;A French fan&quot;, return_tensors=&quot;pt&quot;)
dummy_input = {k: v.to(model.device) for k, v in dummy_input.items()}

# 使用虚拟输入跟踪模型
traced_model = torch.jit.trace(model,(
dummy_input[&#39;input_ids&#39;],dummy_input[&#39;attention_mask&#39;], dummy_input.get(&#39;token_type_ids&#39;)))

# 转换为 Core ML
输入 = [
ct.TensorType(name=&quot;input_ids&quot;, shape=dummy_input[&#39;input_ids&#39;].shape),
ct.TensorType(name=&quot;attention_mask&quot;, shape=dummy_input[&#39;attention_mask&#39;].shape)
]
if &#39;token_type_ids&#39; in dummy_input:
输入.append(ct.TensorType(name=&quot;token_type_ids&quot;, shape=dummy_input[&#39;token_type_ids&#39;].shape))

mlmodel = ct.convert(traced_model, 输入=inputs)

# 保存 Core ML 模型
mlmodel.save(&quot;model.mlmodel&quot;)
print(&quot;模型导出到 Core ML成功&quot;)

convert_to_coreml(modified_model, tokenizer)

要使用导出的模型：
import os
from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch
import torch.nn as nn
import nltk
import coremltools as ct
from coremltools.models import MLModel
import numpy as np
from transformers import AutoTokenizer
import nltk

nltk.download(&#39;punkt&#39;)

# 加载 Core ML 模型
model = MLModel(&#39;model.mlmodel&#39;)

# 加载 tokenizer
model_path = &#39;model&#39;
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)

def prepare_input(text, tokenizer):
tokens = nltk.tokenize.word_tokenize(text)
tokenized_inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=&quot;np&quot;)
input_ids = tokenized_inputs[&#39;input_ids&#39;].astype(np.int32)
tention_mask = tokenized_inputs[&#39;attention_mask&#39;].astype(np.int32)

input_data = {
&#39;input_ids&#39;: input_ids,
&#39;attention_mask&#39;:tention_mask
}

if &#39;token_type_ids&#39; in tokenized_inputs:
input_data[&#39;token_type_ids&#39;] = tokenized_inputs[&#39;token_type_ids&#39;].astype(np.int32)

return input_data, tokens

def predict(text):
# 准备输入
input_data, tokens = prepare_input(text, tokenizer)

# 进行预测
prediction = model.predict(input_data)

# 提取预测标签
logits = prediction[&#39;output&#39;] # 根据模型的输出调整此键
predicted_label = np.argmax(logits, axis=-1)[0]

# 显示结果
for word, label in zip(tokens, predicted_label):
print(f&quot;{word}: {model.model_description.outputDescriptions[0].dictionaryType.int64KeyType.stringDictionary[label]}&quot;)

# 用一个句子测试模型
predict(&quot;A French fan&quot;)

该脚本仅适用于示例“A French Fan”。当我尝试另一个示例 predict(&quot;A football fan is standing in the stadium.&quot;) 时，它会触发错误：
NSLocalizedDescription = &quot;MultiArray shape (1 x 12) does not match the shape (1 x 5) specified in the model description&quot;;


环境：

导出脚本：在 Ubuntu 20.04 上测试了 Python 3.10 和 torch 2.3.1（在 Windows 10 上不起作用）。
预测脚本：必须在 macOS 10.13+ 上运行，因为 CoreML 模型仅支持在 macOS 10.13+ 上进行预测。
]]></description>
      <guid>https://stackoverflow.com/questions/78704542/exporting-a-bert-based-pytorch-model-to-coreml-how-can-i-make-the-coreml-model</guid>
      <pubDate>Wed, 03 Jul 2024 23:39:36 GMT</pubDate>
    </item>
    </channel>
</rss>