<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 23 Mar 2024 21:11:18 GMT</lastBuildDate>
    <item>
      <title>统一构建APK</title>
      <link>https://stackoverflow.com/questions/78212208/building-apk-in-unity</link>
      <description><![CDATA[我做了一个统一项目，它采用机器学习模型来预测图像的类别。它在统一中完美运行。但是当我构建 apk 文件并在我的手机中运行它时。没有什么是可见的。即预测是不可见的。为什么 ？我直接在unity中集成了keras模型。是不是因为我的手机没有安装python？android手机上的结果Unity 上的结果
团结一致。我已经导出了我的模型、我的标签和所有内容。我直接使用 python 脚本来运行我的 .h5 模型。我还需要在我的 apk 中获取 saame 预测]]></description>
      <guid>https://stackoverflow.com/questions/78212208/building-apk-in-unity</guid>
      <pubDate>Sat, 23 Mar 2024 19:38:12 GMT</pubDate>
    </item>
    <item>
      <title>深度学习模型训练精度高，但在二进制文本分类中的测试数据上表现不佳[关闭]</title>
      <link>https://stackoverflow.com/questions/78212101/deep-learning-models-yielding-high-training-accuracy-but-poor-performance-on-tes</link>
      <description><![CDATA[我在处理二进制文本分类任务时遇到了一个令人困惑的问题。尽管尝试了多种深度学习模型，包括各种架构和超参数，但我始终观察到很高的训练准确度，通常在 97% 到 99% 之间。然而，当我根据看不见的测试数据评估这些模型时，它们的性能显着恶化。
为了解决这个问题，我决定探索机器学习模型作为替代方法。令人惊讶的是，随机森林等模型的性能与深度学习模型相当甚至更好，训练和测试数据的准确率均达到 97% 左右。随后，我尝试了其他几种机器学习算法，逻辑回归成为最适合我的特定用例的选择。
尽管有这些发现，我仍然感到困惑，为什么深度学习模型尽管表现出令人印象深刻的训练准确性，却无法很好地泛化到未见过的数据。有人可以阐明这种差异背后的潜在原因吗？是否存在我可能忽略的深度学习特有的常见陷阱或注意事项？任何见解或建议将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78212101/deep-learning-models-yielding-high-training-accuracy-but-poor-performance-on-tes</guid>
      <pubDate>Sat, 23 Mar 2024 19:05:28 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助启动去中心化联合学习+区块链集成项目的模拟[已关闭]</title>
      <link>https://stackoverflow.com/questions/78212019/need-help-starting-simulation-for-decentralized-federated-learning-blockchain</link>
      <description><![CDATA[我正在开发一个项目，该项目涉及在集成区块链技术之前模拟去中心化联合学习 (DFL) 模型。我正在寻找有关如何开始模拟部分的指导。具体来说，我需要有关如何模拟 DFL 的想法、我可以使用的任何现有代码库或库，以及有关如何进行模拟过程的提示，我计划开始使用 python。我是这个领域的新手，因此任何帮助或建议将不胜感激。提前致谢！
我希望找到有关如何创建模仿 DFL 模型行为的模拟的指导。具体来说，我想模拟边缘设备之间的交互、数据分布、模型训练和聚合过程。理想情况下，我想在将区块链技术集成到模拟中之前控制各种参数来测试不同的场景。]]></description>
      <guid>https://stackoverflow.com/questions/78212019/need-help-starting-simulation-for-decentralized-federated-learning-blockchain</guid>
      <pubDate>Sat, 23 Mar 2024 18:41:24 GMT</pubDate>
    </item>
    <item>
      <title>多元多步时间序列预测问题</title>
      <link>https://stackoverflow.com/questions/78211600/a-multivariate-multi-step-time-series-prediction-problem</link>
      <description><![CDATA[我有一个多元多步时间序列预测问题，其中输入 waterA、waterB、waterC、medicineA、medicineB，并输出浊度。
在此处输入图片说明
其中，药物A和药物B是可控的，而三类水是不可控的。
我使用 LSTM 模型使用前 15 个数据点来预测接下来的 15 个数据点（假设 15 个数据点代表 1 小时）。以下是测试集上的一些结果。
在此处输入图片描述
在此处输入图片描述
如果我根据当前时刻过去的数据预测下一小时的浊度。但此刻我打算改变用药量，并预测未来一个多小时的浊度（主要是改变用药对未来浊度的影响），但我不知道三种水的未来值。我曾经尝试使用三个 LSTM 来预测三种类型的未来水，而不是未来的水输入，但这会导致更糟糕的预测结果。还有其他方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78211600/a-multivariate-multi-step-time-series-prediction-problem</guid>
      <pubDate>Sat, 23 Mar 2024 16:27:56 GMT</pubDate>
    </item>
    <item>
      <title>Google Colab：ImportError：无法从“imblearn.over_sampling”导入名称“MLSMOTE”</title>
      <link>https://stackoverflow.com/questions/78211346/google-colab-importerror-cannot-import-name-mlsmote-from-imblearn-over-samp</link>
      <description><![CDATA[我正在尝试使用 imblearn 库中的 MLSMOTE 包：
从 imblearn.over_sampling 导入 MLSMOTE

收到以下错误消息：
无法从“imblearn.over_sampling”导入名称“MLSMOTE”
不平衡学习包信息：
名称：不平衡学习
版本：0.12.0
摘要：机器学习中不平衡数据集的工具箱。
主页：https://github.com/scikit-learn-contrib /不平衡学习
作者：
作者电子邮件：
许可证：麻省理工学院
位置：/usr/local/lib/python3.10/dist-packages
需要：joblib、numpy、scikit-learn、scipy、threadpoolctl
必需者：imblearn
真的很难过这一点，任何指导将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78211346/google-colab-importerror-cannot-import-name-mlsmote-from-imblearn-over-samp</guid>
      <pubDate>Sat, 23 Mar 2024 15:02:34 GMT</pubDate>
    </item>
    <item>
      <title>Python Phonemizer 库在 ubuntu VM 中找不到 espeak 库</title>
      <link>https://stackoverflow.com/questions/78210991/python-phonemizer-library-cant-find-espeak-library-in-ubuntu-vm</link>
      <description><![CDATA[尽管该模型在 Windows 本地计算机上运行良好，但根据此安装指南将路径传递到 espeak-ng 库时 https://bootphon.github.io/phonemizer/install.html ，我无法使其在 Ubuntu 22.04.4 LTS (x86-64) 下的虚拟机中工作。当运行我的脚本通过 wav2vec2phoneme 转录音素时，我收到以下消息
回溯（最近一次调用最后一次）：
文件“/dialrec/phoneme_transcription/phoneme_recognizers/transcribe.py”，第 50 行，位于
phoneme_recognizer = Wav2Vec2Phoneme()
文件“/dialrec/phoneme_transcription/phoneme_recognizers/wav2vec2phoneme.py”，第 24 行，init 中
self.processor = Wav2Vec2Processor.from_pretrained(&quot;facebook/wav2vec2-xlsr-53-espeak-cv-ft&quot;)
文件“/usr/local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py”，第 52 行，在 from_pretrained 中
返回 super().from_pretrained(pretrained_model_name_or_path, **kwargs)
文件“/usr/local/lib/python3.10/site-packages/transformers/processing_utils.py”，第 465 行，from_pretrained
args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)
文件“/usr/local/lib/python3.10/site-packages/transformers/processing_utils.py”，第 511 行，位于 _get_arguments_from_pretrained
args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))
文件“/usr/local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py”，第 837 行，在 from_pretrained 中
返回 tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
文件“/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py”，第 2086 行，from_pretrained
返回 cls._from_pretrained(
文件“/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py”，第 2325 行，位于 _from_pretrained
分词器 = cls(*init_inputs, **init_kwargs)
文件“/usr/local/lib/python3.10/site-packages/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py”，第 153 行， init
self.init_backend(self.phonemizer_lang)
文件“/usr/local/lib/python3.10/site-packages/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py”，第 202 行， init_backend
self.backend = BACKENDS[self.phonemizer_backend](phonemizer_lang, language_switch=&quot;remove-flags&quot;)
文件“/usr/local/lib/python3.10/site-packages/phonemizer/backend/espeak/espeak.py”，第 45 行，在 init 中
超级().init(
文件“/usr/local/lib/python3.10/site-packages/phonemizer/backend/espeak/base.py”，第 39 行，在 init 中
超级().init(
文件“/usr/local/lib/python3.10/site-packages/phonemizer/backend/base.py”，第 77 行，在 init 中
引发 RuntimeError( # pragma: nocover
运行时错误：您的系统上未安装 espeak
为了安装 espeak，我按照以下步骤操作：

apt-get 安装 espeak-ng
pip3 安装phonemizer
pip3 install espeakng（也尝试过 pip3 install py-espeak-ng）

Espeak 肯定安装在 /usr/lib/x86_64-linux-gnu/libespeak-ng.so.1 和 /usr/bin/espeak-ng 下。
我尝试了以下方法：

无需额外步骤
设置环境变量 PHONEMIZER_ESPEAK_LIBRARY=&#39;/usr/lib/x86_64-linux-gnu/libespeak-ng.so.1&#39; 和 PHONEMIZER_ESPEAK_PATH=&#39;/usr/bin/espeak-ng&#39;。
直接在脚本中设置环境变量
os.environ[&#39;PHONEMIZER_ESPEAK_LIBRARY&#39;] = &#39;/usr/lib/x86_64-linux-gnu/libespeak-ng.so.1&#39;
os.environ[&#39;PHONEMIZER_ESPEAK_PATH&#39;] = &#39;/usr/bin/espeak-ng&#39;

如果有任何帮助，我将不胜感激。提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78210991/python-phonemizer-library-cant-find-espeak-library-in-ubuntu-vm</guid>
      <pubDate>Sat, 23 Mar 2024 13:14:00 GMT</pubDate>
    </item>
    <item>
      <title>人体检测模型 - 使用张量流，无需对象检测 API 或任何预训练模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/78210961/human-detection-model-using-tensorflow-without-object-detection-api-or-any-pre</link>
      <description><![CDATA[我希望使用 TensorFlow 的 CNN（卷积神经网络）构建人体检测模型，而不依赖于任何预先训练的模型。我选择的数据集是 COCO 2017，特别关注人体检测。
任何人都可以提供有关如何有效完成此任务的见解或分步指南吗？我非常感谢任何可以帮助开发此人体检测模型的代码片段、教程或推荐资源。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78210961/human-detection-model-using-tensorflow-without-object-detection-api-or-any-pre</guid>
      <pubDate>Sat, 23 Mar 2024 13:04:12 GMT</pubDate>
    </item>
    <item>
      <title>认识十几个^2简单的鼠标绘制的象形文字/符号？</title>
      <link>https://stackoverflow.com/questions/78210940/recognizing-a-dozen2-simple-mouse-drawn-pictographs-symbols</link>
      <description><![CDATA[问候公平的旅行者！，需要算法？？？确定用户的快速涂鸦是否类似于任何象形文字（在最终确定时可能大小约为 50-200 的组中），如果是，则哪个象形文字最接近。理想情况下相对较轻。
谢谢-AAARRGGGHH！！！啊啊啊！！！谢谢好心的向导...
我尝试将一个完全识字的黑猩猩的思维复制到计算机上，打算强迫它的灰质受到束缚 - 但我缺乏时间、资源和知识来做到这一点。
不幸的是............我自己还没有找到一个自定义iconz的算法，因此我不得不依靠这些无知的巫师来让我最挑剔地找到所有的成分...电子思维。]]></description>
      <guid>https://stackoverflow.com/questions/78210940/recognizing-a-dozen2-simple-mouse-drawn-pictographs-symbols</guid>
      <pubDate>Sat, 23 Mar 2024 12:57:19 GMT</pubDate>
    </item>
    <item>
      <title>在 Kotlin Android 应用程序中运行 TensorFlow Lite 模型遇到困难</title>
      <link>https://stackoverflow.com/questions/78210864/difficulty-running-tensorflow-lite-model-in-kotlin-android-app</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78210864/difficulty-running-tensorflow-lite-model-in-kotlin-android-app</guid>
      <pubDate>Sat, 23 Mar 2024 12:36:06 GMT</pubDate>
    </item>
    <item>
      <title>如何将预训练的拥抱脸模型转换为.pt并在本地完全运行？</title>
      <link>https://stackoverflow.com/questions/78210297/how-to-convert-pretrained-hugging-face-model-to-pt-and-run-it-fully-locally</link>
      <description><![CDATA[我正在尝试将此模型转换为.pt格式。它对我来说工作得很好，所以我不想对其进行微调。如何将其导出为.pt并运行界面？
我尝试使用它转换为 .pt：
从变压器导入 AutoConfig、AutoProcessor、AutoModelForCTC、AutoTokenizer、Wav2Vec2Processor
导入库
进口火炬



# 定义模型名称
model_name = “UrukHan/wav2vec2-俄罗斯”

# 加载模型和分词器
config = AutoConfig.from_pretrained(model_name)
模型 = AutoModelForCTC.from_pretrained(model_name, config=config)
处理器 = Wav2Vec2Processor.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 将模型保存为.pt 文件
torch.save(model.state_dict(), &quot;model.pt&quot;)

# 如果需要的话也保存分词器
tokenizer.save_pretrained(“模型标记器”)

但不幸的是它没有运行界面：
model = AutoModelForCTC.from_pretrained(“model.pt”)
处理器 = AutoProcessor.from_pretrained(“model.pt”)


# 使用模型进行推理
FILE = &#39;这里是 wav.wav&#39;
音频，_ = librosa.load（文件，sr = 16000）
音频=列表（音频）
def map_to_result(batch):
  使用 torch.no_grad()：
    input_values = torch.tensor(batch, device=“cpu”).unsqueeze(0) #, device=“cuda”
    logits = 模型(input_values).logits
  pred_ids = torch.argmax(logits, dim=-1)
  批处理=处理器.batch_decode(pred_ids)[0]
  退货批次
映射到结果（音频）
打印（映射到结果（音频））


模型.eval()

并遇到错误：
`model.pt 不是本地文件夹，也不是“https://huggingface.co/models”上列出的有效模型标识符
`]]></description>
      <guid>https://stackoverflow.com/questions/78210297/how-to-convert-pretrained-hugging-face-model-to-pt-and-run-it-fully-locally</guid>
      <pubDate>Sat, 23 Mar 2024 09:18:49 GMT</pubDate>
    </item>
    <item>
      <title>LLaMA2 工作负载跟踪</title>
      <link>https://stackoverflow.com/questions/78208827/llama2-workload-traces</link>
      <description><![CDATA[是否有任何数据集可以捕获 LLaMA2 模型每一层的执行模式和资源消耗？我的研究需要分析粒度工作负载跟踪，特别关注 TFLOPS、GPU 内存使用情况、内存带宽、存储需求以及 LLaMA2 模型各个组件的运行时需求等指标。我将非常感谢任何关于在哪里找到此类数据的指导或建议。预先感谢您。]]></description>
      <guid>https://stackoverflow.com/questions/78208827/llama2-workload-traces</guid>
      <pubDate>Fri, 22 Mar 2024 21:03:23 GMT</pubDate>
    </item>
    <item>
      <title>MLP a2c 策略抱怨 0 不大于 0，或者无穷大不大于 0？</title>
      <link>https://stackoverflow.com/questions/78208624/mlp-a2c-policy-complaining-that-0-isnt-greater-than-0-or-infinity-isnt-greate</link>
      <description><![CDATA[当我训练一些火炬模型时出现以下错误：
ValueError(&#39;分布Normal(loc: torch.Size([1, 4]))的预期参数尺度（形状为(1, 4)的张量），scale: torch.Size([1, 4] )) 以满足约束 GreaterThan(lower_bound=0.0)，但发现无效值：\ntensor([[inf, inf, 0., 0.]])&#39;)。

我的行为具有形状 (4,) 和观察 (3,)。
它是否认为无穷大不&gt;0，或者0不大于0？我不知道为什么会出现这种情况。它是简单地使用 model.learn 在稳定的基线 3 中训练模型。然而，它学习了一段时间，但在这一步失败了：
~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py 学习中（self、total_timesteps、callback、log_interval、tb_log_name、reset_num_timesteps、progress_bar）
    第257章
    [第 258 章]总时间步数：
--&gt;第 259 章
    260
    261 如果 continue_training 为 False：

〜\ anaconda3 \ envs \ lib \ site-packages \ stable_baselines3 \ common \ on_policy_algorithm.py在collect_rollouts中（self，env，callback，rollout_buffer，n_rollout_steps）
    167 # 转换为pytorch张量或TensorDict
    第168章
--&gt; 169 个动作，值，log_probs = self.policy(obs_tensor)
    170 个动作 = actions.cpu().numpy()
    171

_call_impl 中的 ~\anaconda3\envs\\lib\site-packages\torch\nn\modules\module.py(self, *input, **kwargs)
   第1192章
   第1193章
-&gt;第1194章
   第1195章
   第1196章

〜\anaconda3\envs\\lib\site-packages\stable_baselines3\common\policies.py 向前（自我，obs，确定性）
    第624章
    625 个值 = self.value_net(latent_vf)
--&gt; [第 626 章]
    第627章 行动=distribution.get_actions(确定性=确定性)
    第628章

~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\policies.py 在 _get_action_dist_from_latent(self, Latent_pi)
    第654章
    第655章
--&gt;第656章
    第657章
    第658章

proba_distribution 中的 ~\anaconda3\envs\\lib\site-packages\stable_baselines3\common\distributions.py(self,mean_actions,log_std)
    第162章 162
    第 163 章
--&gt;第164章
    第165章 回归自我
    166

~\anaconda3\envs\\lib\site-packages\torch\distributions\normal.py 在 __init__(self, loc, scale, validate_args)
     54 其他：
     55 batch_shape = self.loc.size（）
---&gt; 56 super(普通，自我).__init__(batch_shape, validate_args=validate_args)
     57
     58 def Expand(self,batch_shape,_instance=None):

__init__ 中的 ~\anaconda3\envs\\lib\site-packages\torch\distributions\distribution.py(self、batch_shape、event_shape、validate_args)
     55 如果无效.all():
     56 引发值错误（
---&gt; 57 f“预期参数{param}”
     58 f&quot;({type(value).__name__}，形状为{tuple(value.shape)})&quot;
     59 f”分布{repr(self)}”

请记住我的操作是 0&lt;=a&lt;=1。我需要将其设置为 0
我很难知道它到底在抱怨什么，因为这段代码深入稳定的基线3。这可能是他们的包中的一个小故障吗？我希望它更新权重并继续运行，但它却抱怨 0 不大于 0.. 我不知道为什么我关心这个，但它应该继续运行，不是吗？
感谢您的浏览。]]></description>
      <guid>https://stackoverflow.com/questions/78208624/mlp-a2c-policy-complaining-that-0-isnt-greater-than-0-or-infinity-isnt-greate</guid>
      <pubDate>Fri, 22 Mar 2024 20:07:29 GMT</pubDate>
    </item>
    <item>
      <title>如何将我的 fastai resnet50/vision_learner 训练模型导出到 torchserve 中？</title>
      <link>https://stackoverflow.com/questions/78203794/how-do-i-export-my-fastai-resnet50-vision-learner-trained-model-into-torchserve</link>
      <description><![CDATA[我的目标是将我用 Fastai 训练的模型部署到 Torchserve 中。我正在关注 本教程，但卡在了他为 pytorch 创建模型类的部分。
他提到要在 Torchserve 中运行我们的模型，我们需要以下内容：

模型类
从 pytorch 导出的权重（pth 文件）
处理程序

其中，我得到两个：重量和处理程序。然而，我陷入困境的是模型类。他创建了一个类文件，但我不知道他从哪里获得DynamicUnet作为该类的基础，也不知道他如何将该类与unet_learner混合以创建自定义PyTorch模型类。你能帮我为在学习器 vision_learner 下训练的模型和 resnet50 的预训练模型建立一个模型类吗？]]></description>
      <guid>https://stackoverflow.com/questions/78203794/how-do-i-export-my-fastai-resnet50-vision-learner-trained-model-into-torchserve</guid>
      <pubDate>Fri, 22 Mar 2024 02:55:25 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 numpy 函数计算以下 hessian 矩阵以加快计算速度？</title>
      <link>https://stackoverflow.com/questions/78199806/how-can-i-compute-the-following-hessian-using-numpy-functions-to-speed-up-the-co</link>
      <description><![CDATA[我必须实现一个等效函数来计算逻辑损失的 hessian，写为指数项对数之和。我在Python中实现了以下功能：
def hessian(self,w,hess_trick=0):
        赫斯 = 0
        对于 zip(self.data, self.labels) 中的 x_i,y_i:
            hess += np.exp(y_i * np.dot(w.T, x_i))/((1 + np.exp(y_i * np.dot(w.T,x_i)))**2) * np.outer(x_i, x_i.T)
        返回hess + lambda_reg * np.identity(w.shape[0]) + hess_trick * 10**(-12) * np.identity(w.shape[0])

我的问题是如何在不使用慢速 python 的情况下编写等效但更快的函数？
由于我对 numpy 不太有信心，我尝试编写以下函数：
 def new_hessian(self, w, hess_trick=0):
        exp_term = np.exp(self.labels * np.dot(self.data, w))
        sigmoid_term = 1 + exp_term
        inv_sigmoid_sq = 1 / sigmoid_term ** 2

        diag_elements = np.sum((exp_term * inv_sigmoid_sq)[:, np.newaxis] * self.data ** 2, axis=0)
        off_diag_elements = np.dot((exp_term * inv_sigmoid_sq) * self.data.T, self.data)
        hess = np.diag(diag_elements) + off_diag_elements
        正则化 = lambda_reg * np.identity(w.shape[0])

        hess += hess_trick * 1e-12 * np.identity(w.shape[0])

        返回 hess + 正则化

通过调试这个函数，我发现存在一个根本性的问题。对于特征数量较小的值（例如小于 200），hessian 的两种实现不相等。当我增加特征数量时，这两个函数似乎是相等的。问题是，当使用牛顿方法来优化对数损失来测试这些实现时，较快的实现会比第一个（但在运行时速度方面较慢）实现更多的迭代收敛。]]></description>
      <guid>https://stackoverflow.com/questions/78199806/how-can-i-compute-the-following-hessian-using-numpy-functions-to-speed-up-the-co</guid>
      <pubDate>Thu, 21 Mar 2024 12:02:04 GMT</pubDate>
    </item>
    <item>
      <title>pytorch 中张量的 Autograd.grad()</title>
      <link>https://stackoverflow.com/questions/54754153/autograd-grad-for-tensor-in-pytorch</link>
      <description><![CDATA[我想计算网络中两个张量之间的梯度。输入 X 张量（批量大小 x m）通过一组卷积层发送，这些卷积层返回并输出 Y 张量（批量大小 x n）。
我正在创建一个新的损失，我想知道 Y 的梯度。 X. 在张量流中会是这样的：
tf.gradients(ys=Y, xs=X)
不幸的是，我一直在使用torch.autograd.grad()进行测试，但我不知道该怎么做。我收到如下错误：“RunTimeerror：只能为标量输出隐式创建 grad”。
如果我想知道 Y 的梯度，torch.autograd.grad() 中的输入应该是什么。 X？]]></description>
      <guid>https://stackoverflow.com/questions/54754153/autograd-grad-for-tensor-in-pytorch</guid>
      <pubDate>Mon, 18 Feb 2019 19:32:26 GMT</pubDate>
    </item>
    </channel>
</rss>