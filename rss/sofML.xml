<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 13 Feb 2025 21:15:41 GMT</lastBuildDate>
    <item>
      <title>Tensorflow 性能问题</title>
      <link>https://stackoverflow.com/questions/79437180/tensorflow-perfomance-issue</link>
      <description><![CDATA[我正在尝试在 TensorFlow 上做一个非常原始的强化学习模型。尽管它相对较小，但单次迭代需要约 6-7 秒。
def build_model():
model = keras.Sequential([
layer.Input(shape=(400,)),
layer.Dense(128,activation=&quot;relu&quot;),
layer.Dense(128,activation=&quot;relu&quot;),
layer.Dense(3)
])
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),loss=&quot;huber&quot;)
return model

class DQNAgent:
def __init__(self):
self.model = build_model()
self.target_model = build_model()
self.target_model.set_weights(self.model.get_weights())

self.memory = deque(maxlen=1000)
self.epsilon = 1.0
self.epsilon_min = 0.01
self.epsilon_decay = 0.995
self.gamma = 0.95
self.batch_size = 32

def choose_action(self, state):
if np.random.rand() &lt; self.epsilon:
return random.choice([0, 1, 2])
q_values = self.model.predict(np.array([state]), verbose=0)
return np.argmax(q_values[0])

def Remember(self, state, action, reward, next_state, done):
self.memory.append((state, action, reward, next_state, done))

def train(self):
if len(self.memory) &lt; self.batch_size:
返回
batch = random.sample(self.memory, self.batch_size)
states, target = [], []

对于 state, action, reward, next_state, done in batch:
target = reward
如果未完成：
target += self.gamma * np.max(self.target_model.predict(np.array([next_state]), verbose=0))

q_values = self.model.predict(np.array([state]), verbose=0)
q_values[0][action] = target

states.append(state)
target.append(q_values[0])

self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)

如果 self.epsilon &gt; self.epsilon_min:
self.epsilon *= self.epsilon_decay

def update_target_model(self):
self.target_model.set_weights(self.model.get_weights())

在对所有给定的代码进行分析后，我发现 model.predict() 需要这么多时间才能完成：
分析器结果
最初我以为我只需要在 GPU 上进行计算，但在浪费了两天时间尝试之后，什么都没有改变。
它真的需要那么多时间吗，还是我在代码中搞砸了什么？
GPU：Geforce 2060，
CPU：Intel Core i7，
Windows 11，
Python：3.10
Tensorflow：2.10
]]></description>
      <guid>https://stackoverflow.com/questions/79437180/tensorflow-perfomance-issue</guid>
      <pubDate>Thu, 13 Feb 2025 17:07:09 GMT</pubDate>
    </item>
    <item>
      <title>Tweedie 回归：功率 >=2 '“y 的某些值超出了损失的有效范围”，但 y 值不是</title>
      <link>https://stackoverflow.com/questions/79437039/tweedie-regression-power-2-some-values-of-y-are-out-of-the-valid-range-o</link>
      <description><![CDATA[我正在运行 Tweedie 回归，对于幂 &gt;= 2，我收到一个错误，告诉我我的 y 值超出了 HalfTweedieLoss 的范围。我知道这个损失的 y 的有效范围是 &gt;0。我所有的 y 值都是 &gt;0 和 &lt;1，但我仍然收到这个错误。我不明白为什么。
sklearn 版本 1.3.0
我删除了所有 y 值 &lt;=0 的行，并使用 describe 进行了仔细检查。我期望回归量能够拟合，并给出一个更好的理由来解释为什么它不拟合，尤其是当我的 y 值都大于 0 的时候。我知道 gamma 对我的数据来说不是一个很好的分布，但我希望尝试 power=3（逆高斯），这也是不可能的。
power = 0 和 1 都可以正常工作（正态和泊松）。
这是我的训练 y 数据 (cv_y) 的描述：
count | 616420.000000 
mean | 0.955883 
std | 0.021402 
min | 0.700465 
25% | 0.937018 
50% | 0.954769 
75% | 0.975716 
max | 0.990000 

以下是我的代码的重要元素
glr = TweedieRegressor() # 广义线性回归模型
X_pipeline = Pipeline([(&quot;preprocessor&quot;,X_transformer),(&quot;model&quot;,glr)])
estimator = TransformedTargetRegressor(regressor=X_pipeline, transformer=y_transformer)
family = &quot;Tweedie&quot;
link = &quot;auto&quot;
n_splits=5
tscv = TimeSeriesSplit(gap=20, n_splits=n_splits)

param_grid = {
&#39;regressor__preprocessor__X_pca__whiten&#39;: [True,False],
&#39;regressor__model__power&#39;:[0,1,2],
&#39;regressor__model__alpha&#39;:[0.5],
&#39;regressor__model__fit_intercept&#39;: [True],
&#39;regressor__model__link&#39;: [link],
&#39;regressor__model__solver&#39;: [&#39;newton-cholesky&#39;],
&#39;regressor__model__max_iter&#39;: [5,10],
&#39;regressor__model__tol&#39;: [1e-5],
&#39;regressor__model__verbose&#39;:[1]
}

gs = GridSearchCV(
estimator=estimator,
param_grid=param_grid,
scoring=scoring,
n_jobs=-1,
refit=refit_strategy,
cv = tscv,
verbose=3,
pre_dispatch=10,
error_score = &#39;raise&#39;
)

model = gs.fit(cv_X,cv_y)

]]></description>
      <guid>https://stackoverflow.com/questions/79437039/tweedie-regression-power-2-some-values-of-y-are-out-of-the-valid-range-o</guid>
      <pubDate>Thu, 13 Feb 2025 16:29:26 GMT</pubDate>
    </item>
    <item>
      <title>在 distiset 中保存到磁盘时出现 Unicode 错误</title>
      <link>https://stackoverflow.com/questions/79436672/getting-unicode-error-while-saving-to-disk-in-distiset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79436672/getting-unicode-error-while-saving-to-disk-in-distiset</guid>
      <pubDate>Thu, 13 Feb 2025 15:04:10 GMT</pubDate>
    </item>
    <item>
      <title>在 6 A100-80G 上进行 YOLOv9e-seg 训练，并尝试进行尽可能的优化，但在验证阶段之后出现 CUDA 内存不足错误</title>
      <link>https://stackoverflow.com/questions/79436107/yolov9e-seg-training-on-6-a100-80g-and-tried-to-optimize-as-much-as-i-could-but</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79436107/yolov9e-seg-training-on-6-a100-80g-and-tried-to-optimize-as-much-as-i-could-but</guid>
      <pubDate>Thu, 13 Feb 2025 12:12:17 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：X 有 7 个特征，但 ColumnTransformer 需要 13 个特征</title>
      <link>https://stackoverflow.com/questions/79434756/valueerror-x-has-7-features-but-columntransformer-expects-13-features</link>
      <description><![CDATA[我有以下代码，我尝试使用泊松回归预测工具的价格。
# --- 加载和准备数据 ---
y = train[&#39;PriceToday&#39;]
X = train.drop(columns=[&#39;PriceToday&#39;])

# 定义非标准类型
non_standard_types = [&quot;Nar&quot;, &quot;Orch&quot;, &quot;Fru&quot;,&quot;Comp&quot;]

# 为非标准创建标志特征
X[&quot;Non_Standard_Flag&quot;] = X[&quot;Type_LS&quot;].isin(non_standard_types).astype(int)

# 识别数字和分类列
num_features = [&quot;AGE&quot;, &quot;POWER&quot;, &quot;Hours&quot;, &quot;Non_Standard_Flag&quot;]
cat_features = [&quot;BRAND&quot;, &quot;Country&quot;, &quot;Final_Trans&quot;]

# 定义预处理管道
preprocessor = ColumnTransformer(
transformers=[
(&#39;num&#39;, StandardScaler(), num_features),
(&#39;cat&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;), cat_features)
], remainder=&quot;drop&quot;
)

# --- 训练/测试拆分 ---
# 创建权重列
train[&quot;sample_weight&quot;] = train[&quot;Type_LS&quot;].apply(lambda x: 1 if x == &quot;Standard&quot; else 5)

train[&quot;stratify_group&quot;] = train[&quot;BRAND&quot;].astype(str)
X_train, X_val, y_train, y_val, train_weights, val_weights = train_test_split(
X, y, train[&quot;sample_weight&quot;], test_size=0.2, random_state=42, stratify=train[&quot;stratify_group&quot;]
)
# 在训练数据上对预处理器进行一次拟合
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_val_preprocessed = preprocessor.transform(X_val)

# 定义模型
models = {
&quot;Poisson&quot;: PoissonRegressor(alpha=0.01)
}

# 训练和评估模型
model_results = {}

for model_name, model in models.items():
model.fit(X_train_preprocessed, y_train, sample_weight=train_weights)

# 预测
predictions = model.predict(X_val_preprocessed)
# 计算指标
r2 = r2_score(y_val, predictions)

model_results[model_name] = {
&quot;model&quot;: model,
&quot;R2&quot;: r2
}

我有一个测试数据，我想将其价格与模型预测的价格进行比较。
我的测试数据是这样的：
# 确保新数据具有正确的格式
new_data = pd.DataFrame({
&quot;AGE&quot;: [12, 24, 36, 48, 60, 72, 84, 12, 24, 36, 48, 60, 72, 84],
&quot;Hours&quot;: [500, 1000, 1500, 2000, 2500, 3000, 3500, 500, 1000, 1500, 2000, 2500, 3000, 3500],
&quot;BRAND&quot;: [&quot;NH&quot;] * 14,
&quot;POWER&quot;: [150] * 7 + [80] * 7,
&quot;Final_Trans&quot;: [&quot;Cv&quot;] * 14,
&quot;Country&quot;: [&quot;DEU&quot;] * 14,
&quot;Type_LS&quot;: [Nar, Nar, Nar, ST, ST, ST, ST, ST, ST, ST, ST, ST, ST, ST, ST] 
&quot;Current_Pred&quot;: [105614, 96681, 88504, 81018, 74165, 67892, 62150, 42608, 39728, 37043, 34540, 32206, 30029, 28000]
})

我的代码是：
new_df = pd.DataFrame(new_data)
# 创建 &#39;Non_Standard_Flag&#39;
new_df[&quot;Non_Standard_Flag&quot;] = new_df[&quot;Type_LS&quot;].isin(non_standard_types).astype(int)

# 选择预处理器所需的列
X_new = new_df[[&#39;AGE&#39;, &#39;POWER&#39;, &#39;Hours&#39;, &#39;Non_Standard_Flag&#39;, &#39;BRAND&#39;, &#39;Country&#39;, &#39;Final_Trans&#39;]]

X_new_preprocessed = preprocessor.transform(X_new) 

# 从训练数据中获取独热编码后的列名
ohe = preprocessor.named_transformers_[&#39;cat&#39;]
encoded_cat_columns = ohe.get_feature_names_out(cat_features)

#为数字特征创建列名
num_columns = num_features

# 合并列名
all_columns = num_columns + list(encoded_cat_columns)

# 从预处理数据创建 DataFrame
X_new_preprocessed_df = pd.DataFrame(X_new_preprocessed, columns=all_columns)

# --- 使用泊松模型进行预测 ---
poisson_model = model_results[&quot;Poisson&quot;][&quot;model&quot;] # 访问经过训练的泊松模型
predicted_prices = poisson_model.predict(X_new_preprocessed_df)

# 比较并存储结果 ---
new_df[&#39;Predicted_Price&#39;] = predict_prices

# 计算预测价格与当前价格之间的差额
new_df[&#39;Price_Difference&#39;] = new_df[&#39;Predicted_Price&#39;] - new_df[&#39;Current_Pred&#39;]

但执行此操作后，我收到错误：
X 有 7 个特征，但 ColumnTransformer 需要 13 个特征

我有相同数量的列，所以我不明白为什么会出现此错误。]]></description>
      <guid>https://stackoverflow.com/questions/79434756/valueerror-x-has-7-features-but-columntransformer-expects-13-features</guid>
      <pubDate>Wed, 12 Feb 2025 23:51:03 GMT</pubDate>
    </item>
    <item>
      <title>如何使用嵌入和余弦相似度改进 Excel 文件中的列标题匹配？[关闭]</title>
      <link>https://stackoverflow.com/questions/79433790/how-to-improve-column-header-matching-in-excel-files-using-embeddings-and-cosine</link>
      <description><![CDATA[我正在构建一个处理用户上传的 Excel 文件的工具。这些文件可以有各种列标题，我的目标是将这些标题映射到一组预定义的输出列。例如：
输出列是固定的：名字、姓氏、年龄、性别、城市、地址等。
输入的 Excel 标题可以有所不同。例如，输出中的名字可能在输入文件中表示为员工名字、F_Name 或名字。
如果工具找不到列的匹配项（例如，不存在与名字等同的项），则输出列应填充为 null。
尝试的方法
我使用了基于嵌入的方法：

我使用模型（例如，来自 OpenAI 或其他 NLP 模型的 text-embedding-ada-002）为输入列标题生成嵌入。

我计算这些嵌入与预定义输出列名称的嵌入之间的余弦相似度。

我根据相似度分数确定匹配项。


面临的问题
虽然这适用于某种程度上，余弦相似度得分往往不可靠：

对于名字（输出列）：

与员工名字的相似度 = 0.90（预期）。
与从属名字的相似度 = 0.92（意外且不正确）。

对于名字和不相关的列：

与年龄的相似度 = 0.70，对于不相关的术语来说太高了。
这个问题使得很难区分相关和不相关的匹配。例如：
年龄和名字不应被视为相似，但相似度仍然很高。
员工名字和受抚养人名字应具有不同的分数，以有利于正确匹配。
要求
我需要一个确保准确映射列的解决方案，考虑到以下几点：

相似的列名（例如，名字和员工名字）应具有较高的相似度分数。

不相关的列名（例如，名字和年龄）应具有较低的相似度分数。

解决方案应处理列名的变化，例如同义词（性别 ↔ 性别）或缩写（出生日期 ↔ 出生日期）。


问题

为什么余弦相似度不相关的列对（例如，名字 ↔ 年龄）的得分如此之高？

在这种情况下，我如何提高列匹配的准确性？


尝试的潜在解决方案

手动创建常见变体的映射字典，但这不可扩展。

尝试使用余弦相似度的阈值，但仍然不一致。


我正在寻找什么

替代方法（例如，微调嵌入模型或使用特定于领域的模型）。

专门为匹配列名而设计的任何预训练模型或库。

将基于规则的方法与嵌入来提高准确性。

]]></description>
      <guid>https://stackoverflow.com/questions/79433790/how-to-improve-column-header-matching-in-excel-files-using-embeddings-and-cosine</guid>
      <pubDate>Wed, 12 Feb 2025 16:28:05 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 dart 提取 MFCC 特征</title>
      <link>https://stackoverflow.com/questions/79433652/how-to-extract-mfcc-features-using-dart</link>
      <description><![CDATA[我正在开发一个 Flutter 应用程序，尝试使用机器学习模型来识别谁是说话者，该应用程序将离线运行，输入是 5 秒记录的 MFCC 特征数组，输出是说话者的姓名，数组的大小是 13，有没有办法使用 dart 提取这些特征。]]></description>
      <guid>https://stackoverflow.com/questions/79433652/how-to-extract-mfcc-features-using-dart</guid>
      <pubDate>Wed, 12 Feb 2025 15:40:32 GMT</pubDate>
    </item>
    <item>
      <title>lightgbm 强制变量拆分</title>
      <link>https://stackoverflow.com/questions/79433458/lightgbm-force-variables-to-be-in-splits</link>
      <description><![CDATA[我正在尝试找到一种方法来训练 lightgbm 模型，强制将某些特征放在分割中，即：“放在特征重要性中”，然后预测会受到这些变量的影响。
这是一个建模代码的示例，其中有一个无用的变量，因为它是常量，但其想法是从业务角度来看可能有一个重要的变量不在特征中
from lightgbm import LGBMRegressor
import pandas as pd
import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 生成随机回归数据集
X, y = make_regression(n_samples=1000, n_features=10, noise=0.9, random_state=42)
feature_names = [f&quot;feature_{i}&quot;对于范围内的 i(X.shape[1])]

# 将 DataFrame 转换为可读取的数据
X = pd.DataFrame(X, columns=feature_names)

# Agregar características inútiles
X[“useless_feature_1”] = 1

# 区分数据和相关内容
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义 LGBMRegressor 模型
模型 = LGBMRegressor(
    目标=“回归”，
    度量=“rmse”，
    随机状态=1，
    n_估计器=100
）

# 恩特雷纳埃尔modelo
model.fit(X_train, y_train, eval_set=[(X_test, y_test)])

# 预测和评估
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f&quot;Test RMSE: {rmse:.4f}&quot;)

# 特征重要性
importance = pd.DataFrame({
&quot;feature&quot;: X.columns,
&quot;importance&quot;: model.feature_importances_
}).sort_values(by=&quot;importance&quot;, accending=False)

print(&quot;\nFeature Importance:&quot;)
print(importance)

预期解决方案：应该有一些解决方法，但最有趣的是在拟合或回归方法中使用某些参数的方法。]]></description>
      <guid>https://stackoverflow.com/questions/79433458/lightgbm-force-variables-to-be-in-splits</guid>
      <pubDate>Wed, 12 Feb 2025 14:36:26 GMT</pubDate>
    </item>
    <item>
      <title>来自视频的实时对象跟踪模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79433270/real-time-object-tracking-model-from-videos</link>
      <description><![CDATA[我需要开发一个可以实时准确跟踪物体的机器学习模型。以下是我的想法：

数据收集：我计划在网上寻找大约 10 个视频，这些视频清楚地显示了物体从一个地方移动到另一个地方。
目标：使用这些视频，我的目标是构建和训练一个模型，可以实时跟踪这些物体并准确确定它们的位置

我正在寻找以下方面的建议：

如何最好地选择和预处理此类视频进行训练。
您发现哪些算法或框架对实时物体跟踪有效？
关于处理遮挡或物体速度变化等潜在挑战的任何提示。
]]></description>
      <guid>https://stackoverflow.com/questions/79433270/real-time-object-tracking-model-from-videos</guid>
      <pubDate>Wed, 12 Feb 2025 13:38:28 GMT</pubDate>
    </item>
    <item>
      <title>在 XGBoost 中，CV 的 R 平方比没有 CV 的 R 平方高得多 [关闭]</title>
      <link>https://stackoverflow.com/questions/79320673/r-squared-from-cv-is-way-higher-than-without-cv-in-xgboost</link>
      <description><![CDATA[我有这个 XGBoost 模型的代码：
temp_dataset_pos = dataset_vehicle_pos.copy()
# temp_dataset_pos = temp_dataset_pos.drop([&#39;sum_passenger&#39;], axis=1)
# temp_dataset_pos = temp_dataset_pos[[&#39;hydrity&#39;,&#39;temperature&#39;, &#39;vehicle_avg_speed_pos&#39;]]

X_pos = temp_dataset_pos.drop(&#39;vehicle_avg_speed_pos&#39;, axis=1)
y_pos = temp_dataset_pos[&#39;vehicle_avg_speed_pos&#39;]

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_pos, y_pos, test_size=0.20, random_state=42)

# 创建一个XGBoost 模型
xgboost_model = xgb.XGBRegressor(objective=&#39;reg:squarederror&#39;, random_state=42) # 您可以调整超参数

# 拟合模型
xgboost_model.fit(X_train, y_train)
# 确保预测数据与测试集一致
X_test[&#39;vehicle_count_pos&#39;] = predicted_vehicle_count_pos[&#39;predicted_vehicle_count_pos&#39;]

# 对测试集进行预测
y_pred_xgboost = xgboost_model.predict(X_test)

# 将预测值四舍五入为整数
y_pred_rounded = y_pred_xgboost.round().astype(int)

# 评估模型
mse_xgboost = mean_squared_error(y_test, y_pred_rounded)
print(f&#39;Mean Squared误差（XGBoost）：{mse_xgboost}&#39;)
r2_xgboost = r2_score(y_test, y_pred_rounded)
print(&#39;R 平方值：&#39;, r2_xgboost)
importance = xgboost_model.feature_importances_

# 创建 XGBoost 模型
xgboost_model = xgb.XGBRegressor(objective=&#39;reg:squarederror&#39;, random_state=42) # 您可以调整超参数

# 定义 K-Fold 交叉验证器
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# 根据均方误差定义评分器
mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)

# 执行交叉验证并计算 MSE
mse_scores = cross_val_score(xgboost_model, X_pos, y_pos,scoring=mse_scorer, cv=kf)

# 将 MSE 分数转换为正数； cross_val_score 对 &#39;greater_is_better=False&#39; 返回负值
mse_scores = -mse_scores

# 计算 MSE 的平均值和标准差
mean_mse = np.mean(mse_scores)
std_mse = np.std(mse_scores)

print(f&#39;Mean MSE from CV: {mean_mse}&#39;)
print(f&#39;Std MSE from CV: {std_mse}\n&#39;)

# 如果您还想在结果摘要中包含 R 平方
r2_scorer = &#39;r2&#39;
r2_scores = cross_val_score(xgboost_model, X_pos, y_pos,scoring=r2_scorer, cv=kf)

mean_r2 = np.mean(r2_scores)
std_r2 = np.std(r2_scores)

print(f&#39;来自 CV 的平均 R 平方：{mean_r2}&#39;)
print(f&#39;来自 CV 的标准 R 平方：{std_r2}&#39;)
print(&#39;\n&#39;)

我得到了这些结果：
均方误差 (XGBoost)：395.3362869635255 
R 平方值：0.37072522417952525
来自 CV 的平均 MSE：6.902625450153782
来自 CV 的标准 MSE：0.8885273860917627
来自 CV 的平均 R 平方： 0.990070278719086
CV 的标准 R 平方：0.000623638887270274
为什么有和没有 CV 时 R 平方如此不同？
我的代码中是否遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79320673/r-squared-from-cv-is-way-higher-than-without-cv-in-xgboost</guid>
      <pubDate>Tue, 31 Dec 2024 19:21:35 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost/LightGBM 中的多类分类中的自定义标签</title>
      <link>https://stackoverflow.com/questions/79309001/custom-labelling-in-multi-class-classification-in-xgboost-lightgbm</link>
      <description><![CDATA[我有以下数据框，记录了不同班级（Class_ID）的学生 1、2、3、4 的 IQ、Hours（学习小时数）和 Score（过去的考试成绩），我想使用这些特征来预测谁将以班级第一名的成绩毕业（Top）
Class_ID IQ_1 IQ_2 IQ_3 IQ_4 Hours_1 Hours_2 Hours_3 Hours_4 Score_1 Score_2 Score_3 Score_4 Top
1 101 99 130 100 10 19 3 12 98 80 95 88 3 
2 93 103 112 200 5 9 12 10 50 88 99 100 1
3 100 102 101 102 12 13 17 9 84 88 89 98 4

为此，我使用了 XGBoost 库中的多类分类（类别为 1,2,3,4）。但是，由于排名第二的人接近榜首，但在训练中他/她无论如何都会得到 0 分，因此更好的训练方案是奖励得分最高的榜首，并给第二名一些分数，而不是像排名垫底的人那样给它 0 分。因此我想在 XGBoost 中询问，是否有办法进行自定义标记，例如：
Class_ID IQ_1 IQ_2 IQ_3 IQ_4 Hours_1 Hours_2 Hours_3 Hours_4 Score_1 Score_2 Score_3 Score_4 Student_1 Student_2 Student_3 Student_4
1 101 99 130 100 10 19 3 12 98 80 95 88 0 0.3 0.7 0
2 93 103 112 200 5 9 12 10 50 88 99 100 0.7 0 0.3 0
3 100 102 101 102 12 13 17 9 84 88 89 98 0 0.3 0 0.7

即，我们奖励类别 0.7 中的首位和类别 0.3 中的第二位（类别 1 的 [0, 0.3, 0.7, 0] 而不是原始标签中的 [0,0,1,0]？）
如果 XGBoost 没有这种自定义标签，我们可以在类似 LightGBM/Catboost 的包中这样做吗？]]></description>
      <guid>https://stackoverflow.com/questions/79309001/custom-labelling-in-multi-class-classification-in-xgboost-lightgbm</guid>
      <pubDate>Thu, 26 Dec 2024 08:18:15 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ML 模型和 FastAPI 处理来自多个用户的请求？</title>
      <link>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</link>
      <description><![CDATA[我正在研究通过FastAPI分发人工智能模块的过程。
我创建了一个FastAPI应用，使用预先学习的机器学习模型来回答问题。
这种情况下，一个用户使用是没有问题的，但是多个用户同时使用时，响应可能会太慢。
那么，当多个用户输入一个问题时，有没有办法一次性复制模型并加载进去？
class sentencebert_ai():
def __init__(self) -&gt;无：
super().__init__()

def ask_query(self,query, topN):
startt = time.time()

ask_result = []
score = []
result_value = [] 
embedder = torch.load(model_path)
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)
query_embedding = embedder.encode(query, convert_to_tensor=True)
cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0] #torch.Size([121])121 表示该数据集为 10 ... cos_scores = cos_scores.cpu()

        top_results = np.argpartition(-cos_scores, range(topN))[0:topN]

        对于 top_results[0:topN] 中的 idx：        
            Ask_result.append(corpusid[idx].item())
            #.item()으로 접근하는 유는 张量(5)에서 해당 숫자에 접근하기 위한 방식다.
            score.append(round(cos_scores[idx].item(),3))

# 生成 json 数组并返回结果集
for i,e in zip(ask_result,score):
result_value.append({&quot;pred_id&quot;:i,&quot;pred_weight&quot;:e})
endd = time.time()
print(&#39;结果集&#39;,endd-startt)
return result_value
# return &#39;,&#39;.join(str(e) for e in ask_result),&#39;,&#39;.join(str(e) for e in score)

class Item_inference(BaseModel):
text : str
topN : Optional[int] = 1

@app.post(&quot;/retrieval&quot;, tags=[&quot;knowledge referral&quot;])
async def Knowledge_recommendation(item: Item_inference):

# db.append(item.dict())
item.dict()
results = _ai.ask_query(item.text, item.topN)

return results

if __name__ == &quot;__main__&quot;:
parser = argparse.ArgumentParser()
parser.add_argument(&quot;--port&quot;, default=&#39;9003&#39;, type=int)
# parser.add_argument(&quot;--mode&quot;, default=&#39;cpu&#39;, type=str, help=&#39;cpu for CPU mode, gpu for GPU mode&#39;)
args = parser.parse_args()

_ai = sentencebert_ai()
uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=args.port,workers=4)

更正版本
@app.post(&quot;/aaa&quot;) def your_endpoint(request: Request, item:Item_inference): start = time.time() model = request.app.state.model item.dict() # 测试结果 _ai = sentencebert_ai() results = _ai.ask_query(item.text, item.topN,model) end = time.time() print(end-start) return results ``` 
]]></description>
      <guid>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</guid>
      <pubDate>Fri, 25 Mar 2022 07:13:32 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch RuntimeError：CUDA 内存不足，有大量可用内存</title>
      <link>https://stackoverflow.com/questions/71498324/pytorch-runtimeerror-cuda-out-of-memory-with-a-huge-amount-of-free-memory</link>
      <description><![CDATA[在训练模型时，我遇到了以下问题：
RuntimeError：CUDA 内存不足。尝试分配 304.00 MiB（GPU 0；总容量 8.00 GiB；已分配 142.76 MiB；6.32 GiB 可用；PyTorch 总共保留 158.00 MiB）如果保留内存是 &gt;&gt; 分配的内存，请尝试设置 max_split_size_mb 以避免碎片。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档
我们可以看到，当尝试分配 304 MiB 内存时发生错误，而 6.32 GiB 是可用的！问题是什么？我可以看到，建议的选项是设置 max_split_size_mb 以避免碎片。这有帮助吗？如何正确做到这一点？
这是我的 PyTorch 版本：
torch==1.10.2+cu113 
torchvision==0.11.3+cu113 
torchaudio===0.10.2+cu113
]]></description>
      <guid>https://stackoverflow.com/questions/71498324/pytorch-runtimeerror-cuda-out-of-memory-with-a-huge-amount-of-free-memory</guid>
      <pubDate>Wed, 16 Mar 2022 13:53:45 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 上带预热功能的 Adam 优化器</title>
      <link>https://stackoverflow.com/questions/65343377/adam-optimizer-with-warmup-on-pytorch</link>
      <description><![CDATA[在论文Attention is all you need中，在第 5.3 节中，作者建议线性增加学习率，然后按步数的平方根倒数比例减少。

我们如何使用Adam 优化器？最好不要使用其他软件包。]]></description>
      <guid>https://stackoverflow.com/questions/65343377/adam-optimizer-with-warmup-on-pytorch</guid>
      <pubDate>Thu, 17 Dec 2020 15:12:56 GMT</pubDate>
    </item>
    <item>
      <title>Xgboost 欠拟合</title>
      <link>https://stackoverflow.com/questions/57978541/xgboost-underfitting</link>
      <description><![CDATA[我训练了以下 Xgboost 分类器：
xgb.XGBClassifier(tree_method=&#39;hist&#39;, grow_policy=&#39;lossguide&#39;,gamma=1.0, max_depth=0, max_leaves=255, min_child_weight=100,n_estimators=500,
n_jobs=-1,
learning_rate=0.1,
subsample=0.7,
colsample_bytree=0.7,
)

我绘制了训练集和验证集的学习曲线（对数损失与时期）。
我得到的学习曲线对于训练集和验证集来说完全相同：对数损失一开始从 0.6 减少到 0.3，然后在 100 个时期后趋于稳定
我认为这是一个欠拟合的情况？因为我有很高的偏差。
在这种情况下，我该如何使模型复杂化？
非常感谢任何帮助
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/57978541/xgboost-underfitting</guid>
      <pubDate>Tue, 17 Sep 2019 16:36:03 GMT</pubDate>
    </item>
    </channel>
</rss>