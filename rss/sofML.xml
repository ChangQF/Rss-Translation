<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 29 Jan 2024 03:15:09 GMT</lastBuildDate>
    <item>
      <title>机器学习、人工智能和数据工程课程材料所需的指导</title>
      <link>https://stackoverflow.com/questions/77896962/guidance-required-with-ml-ai-and-data-engineering-course-material</link>
      <description><![CDATA[我是一名拥有 2 年经验的数据工程师，过去 1 年我一直在读研究生，我觉得我已经失去了数据工程概念，而且我有实践知识，但我不认为我有深厚的或学术知识数据工程（特别是我不认为我对可扩展系统的选择有很好的了解，我已经做了一些 ETL 管道，但我不知道技术术语来帮助人们解决如何扩展系统的问题）并且我没有做一些学术研究使用机器学习模型的项目，但我没有适当的注释或机器学习算法的思维导图，比如在哪种情况下哪种模型会比其他模型更受青睐。一般来说，对于机器学习，我依赖谷歌搜索并获取足以满足该场景的模型。
与 AI 类似，我对 LLM 微调进行了修改，我已经完成了一些小型 YouTube 教程。
我预计在几个月后接受面试，我想知道是否有人可以指导我通过一些资源来让我更清楚地了解概念（数据工程、机器学习、人工智能），我不会说我正在寻找深度在细微差别方面的知识，而是对现有概念的更强把握，这可以帮助我在面试中站稳脚跟。
非常感谢任何课程、路线图、建议。
感谢社区。]]></description>
      <guid>https://stackoverflow.com/questions/77896962/guidance-required-with-ml-ai-and-data-engineering-course-material</guid>
      <pubDate>Mon, 29 Jan 2024 00:59:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 yolov8 进行物体检测时，笔记本电脑的相机窗口未显示</title>
      <link>https://stackoverflow.com/questions/77896458/laptops-camera-window-not-being-displayed-when-using-it-for-object-detection-u</link>
      <description><![CDATA[在下面给出的代码中，我尝试通过笔记本电脑的摄像头使用yolov8进行物体检测，但是摄像头转动了3-5秒，并且摄像头的窗口没有弹出。
我将非常感谢所提供的任何帮助。
代码：
导入火炬
将 numpy 导入为 np
导入CV2
从导入时间开始
从 ultralytics 导入 YOLO
进口监管作为SV

类对象检测：

    def __init__(self, capture_index):
       
        self.capture_index = capture_index
        
        self.device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
        print(&quot;使用设备：&quot;, self.device)
        
        self.model = self.load_model()
        
        self.CLASS_NAMES_DICT = self.model.model.names
    
        self.box_annotator = sv.BoxAnnotator(sv.ColorPalette.default(), 厚度=3, text_thickness=3, text_scale=1.5)
    

    def load_model(自身):
       
        model = YOLO(“yolov8m.pt”) # 加载预训练的 YOLOv8n 模型
        model.fuse()
    
        返回模型


    def 预测（自身，框架）：
       
        结果 = self.model(frame)
        
        返回结果
    

    defplot_bboxes（自身，结果，框架）：
        
        xyxys = []
        信心 = []
        类ID = []
        
         # 提取人员类别的检测
        对于结果中的结果：
            盒子 = result.boxes.cpu().numpy()
            class_id = box.cls[0]
            conf = 盒子.conf[0]
            xyxy = 盒子.xyxy[0]

            如果 class_id == 0.0：
          
              xyxys.append(结果.boxes.xyxy.cpu().numpy())
              confidences.append(result.boxes.conf.cpu().numpy())
              class_ids.append(result.boxes.cls.cpu().numpy().astype(int))
            
        
        # 设置可视化检测
        检测= sv.检测（
                    xyxy=结果[0].boxes.xyxy.cpu().numpy(),
                    置信度=结果[0].boxes.conf.cpu().numpy(),
                    class_id=结果[0].boxes.cls.cpu().numpy().astype(int),
                    ）
        
    
        # 设置自定义标签格式
        self.labels = [f&quot;{self.CLASS_NAMES_DICT[class_id]} {置信度:0.2f}&quot;;
        对于 _、置信度、class_id、tracker_id
        在检测中]
        
        # 注释并显示框架
        框架= self.box_annotator.annotate（场景=框架，检测=检测，标签= self.labels）
        
        返回帧
    
    
    
    def __call__(自我):

        cap = cv2.VideoCapture(self.capture_index)
        断言 cap.isOpened()
        帽.设置（cv2.CAP_PROP_FRAME_WIDTH，1280）
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
      
        而真实：
          
            开始时间 = 时间()
            
            ret, 框架 = cap.read()
            断言 ret
            
            结果 = self.predict(frame)
            框架= self.plot_bboxes（结果，框架）
            
            结束时间 = 时间()
            fps = 1/np.round(结束时间 - 开始时间, 2)
             
            cv2.putText(frame, f&#39;FPS: {int(fps)}&#39;, (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)
            
            cv2.imshow(&#39;YOLOv8检测&#39;,frame)
 
            如果 cv2.waitKey(5) &amp; 0xFF==27：
                
                休息
        
        cap.release()
        
        
    
检测器 = ObjectDetection(capture_index=0)
探测器（）

我尝试通过网络摄像头在 yolo v8 中进行对象检测，但网络摄像头的窗口未打开。]]></description>
      <guid>https://stackoverflow.com/questions/77896458/laptops-camera-window-not-being-displayed-when-using-it-for-object-detection-u</guid>
      <pubDate>Sun, 28 Jan 2024 21:12:30 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 python 上制作姿势动画，也许使用索具和姿势估计？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77896382/is-it-possible-to-do-pose-animation-on-python-perhaps-using-rigging-and-pose-es</link>
      <description><![CDATA[我正在尝试制作一个像“https://pose-animator-demo.firebaseapp.com/camera.html”这样的项目，它使用机器学习来找出身体地标，并且卡在动画的位上身体。我正在考虑使用预先装配的身体，然后将其添加到虚拟骨架上。我正在尝试在 python 上完成这一切。这可能吗？
我已经成功制作了一个虚拟骨架，它悬停在实时视频中的身体上，或者如果需要的话可以悬停在黑框上，但我陷入了动画位]]></description>
      <guid>https://stackoverflow.com/questions/77896382/is-it-possible-to-do-pose-animation-on-python-perhaps-using-rigging-and-pose-es</guid>
      <pubDate>Sun, 28 Jan 2024 20:51:26 GMT</pubDate>
    </item>
    <item>
      <title>生成期间请求失败：服务器错误：CUDA 内存不足</title>
      <link>https://stackoverflow.com/questions/77896274/request-failed-during-generation-server-error-cuda-out-of-memory</link>
      <description><![CDATA[我正在使用 Mixtral 的最新模型：https://huggingface.co/米斯特拉莱/Mixtral-8x7B-v0.1
我在 HuggingFace 的推理端点上创建了一个端点，使用：GPU · Nvidia A100 · 2x GPU · 160 GB。
当我尝试一代时，我收到错误：
&lt;前&gt;&lt;代码&gt;[错误] =&gt;生成期间请求失败：服务器错误：CUDA 内存不足。尝试分配 16.00 MiB。 GPU 0 的总容量为 79.15 GiB，其中 5.25 MiB 是免费的。进程 771596 已使用 79.13 GiB 内存。在分配的内存中，77.80 GiB 由 PyTorch 分配，184.40 MiB 由 PyTorch 保留但未分配。如果保留但未分配的内存很大，请尝试设置 max_split_size_mb 以避免碎片。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档。

将端点升级到 Nvidia A100 4x GPU · 320 GB 是否可以解决问题？或者我是否在配置中遗漏了一些明显的东西，并且它应该在我已有的计算上运行良好？]]></description>
      <guid>https://stackoverflow.com/questions/77896274/request-failed-during-generation-server-error-cuda-out-of-memory</guid>
      <pubDate>Sun, 28 Jan 2024 20:13:21 GMT</pubDate>
    </item>
    <item>
      <title>将图像移动到目录后数字不同</title>
      <link>https://stackoverflow.com/questions/77895090/different-numbers-after-moving-images-into-a-directory</link>
      <description><![CDATA[我试图将一些图像移动到包含 3 个类（石头、布和剪刀）的目录中，以分割基本目录。
我尝试过这段代码：
train_dir = os.path.join(base_dir, &#39;train&#39;)
val_dir = os.path.join(base_dir, &#39;验证&#39;)
test_dir = os.path.join(base_dir, &#39;测试&#39;)

# 创建子目录
对于 [train_dir, val_dir, test_dir] 中的目录：
    os.makedirs（目录，exist_ok=True）

# 类别列表（石头、剪刀、布）
类名 = os.listdir(base_dir)

# 迭代每个类
对于 class_names 中的 class_name：
    class_path = os.path.join(base_dir, class_name)

    如果 os.path.isdir(class_path):
        # 列出类中的所有图像
        images = [img for img in os.listdir(class_path) if img.endswith(&#39;.jpg&#39;) or img.endswith(&#39;.png&#39;) or img.endswith(&#39;.jpeg&#39;) and img != &#39;README_rpc-简历-images.txt&#39;]

        # 将图像分为训练集、验证集和测试集
        训练图像，测试图像，训练标签，测试标签=训练测试分割（
            所有图像、所有标签、test_size=0.2、random_state=42）

        train_images，val_images，train_labels，val_labels = train_test_split（
            训练图像、训练标签、测试大小=0.25、随机状态=42）
        
        对于 [os.path.join(train_dir, class_name), os.path.join(val_dir, class_name), os.path.join(test_dir, class_name)] 中的目录：
            os.makedirs（目录，exist_ok=True）

        # 将图片移动到对应的子目录
        对于 train_images 中的 img：
          Shutil.move(os.path.join(class_path, img), os.path.join(train_dir, class_name, img))
        对于 val_images 中的 img：
          Shutil.move(os.path.join(class_path, img), os.path.join(val_dir, class_name, img))
        对于 test_images 中的 img：
          Shutil.move(os.path.join(class_path, img), os.path.join(test_dir, class_name, img))

但是在我使用此代码仔细检查了 train_images 和 train_dir 中的图像数量后：
print(f“train_images 中的图像数量: {len(train_images)}”)
print(f&quot;train_dir 中的图像数量: {len(train_dir)}&quot;)

结果是：
train_images 中的图像数量：1312 和
train_dir 中的图像数量：28
我一直在寻找导致这种数字差异的代码出了什么问题。
关于如何解决这个问题有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77895090/different-numbers-after-moving-images-into-a-directory</guid>
      <pubDate>Sun, 28 Jan 2024 14:15:59 GMT</pubDate>
    </item>
    <item>
      <title>Java Weka API：获取 ROC 面积值</title>
      <link>https://stackoverflow.com/questions/77895012/java-weka-api-getting-roc-area-values</link>
      <description><![CDATA[我正在尝试在 java 类中使用 Weka API。我执行了 10 倍交叉验证，然后使用不同的阈值对数据进行二值化。
但是我是使用 Weka API 的新手，所以不确定我所做的是否正确。我正在获取 ROC 值，但请注意确保它们是正确的。下面是我的代码：
for(int i = 0; i &lt; 阈值.length; i++)
{
     // learnSet 的深拷贝
     ArrayList&gt; learnSetCopy = new ArrayList&lt;&gt;();
     for (ArrayList insideList : learnSet)
     {
         ArrayList; innerCopy = new ArrayList&lt;&gt;();
         for (int[] 数组：innerList)
         {
             int[] arrayCopy = Arrays.copyOf(array, array.length);
             innerCopy.add(arrayCopy);
         }
         learnSetCopy.add(innerCopy);
     }

     // validSet 的深拷贝
     ArrayList&gt; validSetCopy = new ArrayList&lt;&gt;();
     for (ArrayList insideList : validSet)
     {
         ArrayList; innerCopy = new ArrayList&lt;&gt;();
         for (int[] 数组：innerList)
         {
             int[] arrayCopy = Arrays.copyOf(array, array.length);
             innerCopy.add(arrayCopy);
         }
         validSetCopy.add(innerCopy);
     }

     //二值化化学蛋白质相互作用值
     binarizeCpiAttributes(learnSetCopy, 阈值[i]);
     //生成要通过Weka运行的Arff文件
     generateARFF(文件名 + “LearningThreshold” + 阈值[i] + “折叠” + j + “.arff”, attributeNames, learnSetCopy);

     binarizeCpiAttributes(validSetCopy, 阈值[i]);
     generateARFF(文件名 + “ValidThreshold” + 阈值[i] + “Fold” + j + “.arff”, attributeNames, validSetCopy);

     //创建学习和有效arff文件的实例
     实例learningInstances = DataSource.read(fileName + &quot;LearningThreshold&quot; + Threshold[i] + &quot;Fold&quot; + j + &quot;.arff&quot;);
     实例 validInstances = DataSource.read(fileName + &quot;ValidThreshold&quot; + Threshold[i] + &quot;Fold&quot; + j + &quot;.arff&quot;);

     //设置学习和有效集的类标签
     if(learningInstances.classIndex() == -1)
     {
         LearningInstances.setClassIndex(learningInstances.numAttributes()-1);
     }

     if(validInstances.classIndex() == -1)
     {
         validInstances.setClassIndex(validInstances.numAttributes()-1);
     }

     RandomForest cls = new RandomForest();
     字符串[] 选项 = {
         “-P”、“100”、
         “-I”、“100”、
         “-num-slots”、“1”、
         “-K”、“0”、
         “-M”、“1.0”、
         “-V”、“0.001”、
         “-S”、“1”
     };
     cls.setOptions(选项);
     cls.buildClassifier(learningInstances);

     评估 eval = 新评估(learningInstances);
     eval.evaluateModel(cls​​, validInstances);

     System.out.println(&quot;ROC曲线下面积：&quot; + eval.areaUnderROC(1));

     // 根据需要打印或使用 rocAuc 值
     System.out.println(“处理阈值：”+threshold[i]);
}

这是我得到的输出的示例。我确实认为它们应该更高，这让我怀疑我所做的是否正确：
ROC 曲线下面积：0.6000602772754672 使用阈值处理：0.4 ROC 曲线下面积：0.5848854731766124 使用阈值处理：0.5 ROC 曲线下面积：0.594831223628692 使用阈值处理：0.6 ROC 曲线下面积：0.5600 51235684147 处理阈值：0.7 
我在这里所做的是否正确，这是获取 ROC 面积值的正确方法还是我从 Weka API 获取其他值？
我尝试通读所提供的 Weka 文档，但对某些部分感到困惑。]]></description>
      <guid>https://stackoverflow.com/questions/77895012/java-weka-api-getting-roc-area-values</guid>
      <pubDate>Sun, 28 Jan 2024 13:56:04 GMT</pubDate>
    </item>
    <item>
      <title>Tesseract 的训练自定义数据集以及版本之间的差异</title>
      <link>https://stackoverflow.com/questions/77894617/training-custom-dataset-for-tesseract-and-difference-between-versions</link>
      <description><![CDATA[我正在尝试构建自定义数据集，然后对其进行训练，以改进tesseract 的 OCR。
然而，我很难理解确切的步骤或正确的方法。请注意，我在机器学习方面的经验很少，尤其是在神经网络方面。
在开始我的问题之前，我想说，我认为 LSTM 是从 Tesseract 4 开始添加的，并且对于较低版本，默认使用自适应分类器。
我在这里查看了文档：https://github.com/tesseract-ocr/tessdoc  但老实说，我很困惑，因为我的经验为零，并且通过查看一些教程，我发现了很多差异。
我的问题是：

在创建数据集时，我只需要基本上将 .jpeg 图像转换为 .tiff 格式，并为每个 .tiff 创建 .box 文件，该文件基本上是每个字符及其坐标（标签）的容器。这是低于 4 版本的超正方体训练模型的方法吗？
这是问题 1 的后续问题。我正在使用 JTessBoxEditor (https://github.com/nguyenq /jTessBoxEditor）用于创建框文件，我认为它太慢了，因为您无法使用指针选择字符（边界框），但您必须使用文本字段手动完成，是否有更快的方法？
我见过的另一种训练模型的方法基本上是拥有一组图像（.tiff 或 .png），这些图像需要是单行，并将内容转录在 .gt.txt 文件中。基本上，您只需提供整行的文本，而不是指定每个坐标的坐标。我认为这是从 tesseract 4 开始训练 LSTM 模型的方法，对吗？

我尝试识别的图像上的文本如下：








如果您还可以提供有关我的案例场景的确切步骤的详细说明，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77894617/training-custom-dataset-for-tesseract-and-difference-between-versions</guid>
      <pubDate>Sun, 28 Jan 2024 11:33:15 GMT</pubDate>
    </item>
    <item>
      <title>在 python 中使用 Elastic Net 运行回归不断得到 NAN</title>
      <link>https://stackoverflow.com/questions/77894355/running-a-regression-using-elastic-net-in-python-keep-getting-nan</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77894355/running-a-regression-using-elastic-net-in-python-keep-getting-nan</guid>
      <pubDate>Sun, 28 Jan 2024 10:03:08 GMT</pubDate>
    </item>
    <item>
      <title>如何在 google colab 中使用更多 GPU RAM？</title>
      <link>https://stackoverflow.com/questions/77893929/how-do-i-use-more-of-the-gpu-ram-in-google-colab</link>
      <description><![CDATA[我正在 pytorch 中从事这个深度学习项目，其中我有 2 个完全连接的神经网络，我需要训练然后测试它们。但是当我在 google colab 中运行代码时，它并不比在我的 PC 上的 CPU 上运行快多少。顺便说一句，我有 colab pro。它还使用 A100 GPU 40GB GPU RAM 中的 0.6 个。
导入火炬
导入火炬视觉
导入 torchvision.transforms 作为变换
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim


设备 = torch.device(“cuda:0”)
# 定义变换
变换 = 变换.Compose([
    变换.ToTensor(),
    变换.Normalize((0.5,),(0.5,))
]）

# 加载 FashionMNIST 数据集
trainset = torchvision.datasets.FashionMNIST（&#39;./data&#39;，download=True，train=True，transform=transform）
测试集 = torchvision.datasets.FashionMNIST(&#39;./data&#39;, download=True, train=False, transform=transform)

# 创建数据加载器
trainloader = torch.utils.data.DataLoader(trainset,batch_size=1,shuffle=True,num_workers=2)
testloader = torch.utils.data.DataLoader(testset,batch_size=1,shuffle=False,num_workers=2)

# 为类定义常量
类 = (&#39;T 恤/上衣&#39;, &#39;裤子&#39;, &#39;套头衫&#39;, &#39;连衣裙&#39;, &#39;外套&#39;,
           “凉鞋”、“衬衫”、“运动鞋”、“包”、“踝靴”）




# 定义全连接神经网络
FCNN 类（nn.Module）：
    def __init__(自身, num_layers=1):
        超级（FCNN，自我）.__init__()
        self.num_layers = num_layers
        self.fc_layers = nn.ModuleList()
        如果 self.num_layers == 1:
            self.fc_layers.append(nn.Linear(28 * 28, 1024))
        elif self.num_layers == 2：
            self.fc_layers.append(nn.Linear(28 * 28, 1024))
            self.fc_layers.append(nn.Linear(1024, 1024))
        self.output_layer = nn.Linear(1024, 10)

    def 前向（自身，x）：
        x = x.view(-1, 28 * 28)
        对于 self.fc_layers 中的层：
            x = nn.function.relu(层(x))
        x = self.output_layer(x)
        返回x

# 修改train函数以将输入和标签移动到GPU
def train(网络, 标准, 优化器, epochs=15):
    对于范围内的纪元（纪元）：
        运行损失 = 0.0
        对于 i，enumerate(trainloader, 0) 中的数据：
            输入，标签=数据[0].to（设备），数据[1].to（设备）
            优化器.zero_grad()

            输出 = 净值（输入）
            损失=标准（输出，标签）
            loss.backward()
            优化器.step()

            running_loss += loss.item()
            如果我% 2000 == 1999：
                print(&#39;[%d, %5d] 损失: %.2f&#39; %
                      (epoch + 1, i + 1, running_loss / 2000))
                运行损失 = 0.0

# 定义函数来测试准确性
定义测试（净）：
    正确 = 0
    总计 = 0
    使用 torch.no_grad()：
        对于测试加载器中的数据：
            图像、标签=数据
            输出=净（图像）
            _, 预测 = torch.max(outputs.data, 1)
            总计 += labels.size(0)
            正确+=（预测==标签）.sum().item()

    print(&#39;准确率：%d %%&#39; % (
            100 * 正确/总计))

＃ 主功能
如果 __name__ == “__main__”：
    # 定义网络
    net1 = FCNN(num_layers=1)
    net2 = FCNN(num_layers=2)
    net2.to（设备）

    # 定义损失函数和优化器
    标准 = nn.CrossEntropyLoss()
    优化器1 = optim.SGD(net1.parameters(), lr=0.001, 动量=0.0)
    optimer2 = optim.SGD(net2.parameters(), lr=0.001, 动量=0.0)

    # 使用 1 个 FC 层训练和测试网络
    #print(“训练网络1层...”)
    #train(net1, 标准, 优化器1)
    #测试（网络1）

    # 使用 2 个 FC 层训练和测试网络
    print(&quot;2层训练网络...&quot;)
    训练（net2、标准、优化器2）
    测试（网络2）

尝试在google colab中使用不同的GPU
尝试添加此行以始终使用 CUDA 核心：
设备 = torch.device(“cuda:0”),

并让网络使用该设备：
 设备 = torch.device(“cuda:0”)
]]></description>
      <guid>https://stackoverflow.com/questions/77893929/how-do-i-use-more-of-the-gpu-ram-in-google-colab</guid>
      <pubDate>Sun, 28 Jan 2024 07:11:15 GMT</pubDate>
    </item>
    <item>
      <title>运行由 resipy 库组成的代码时出现错误 - from resipy import R2</title>
      <link>https://stackoverflow.com/questions/77893785/getting-error-while-running-the-code-which-consists-of-resipy-library-from-res</link>
      <description><![CDATA[无法导入 meshCalc 扩展，请参阅以下错误：
无法从“resipy.cext”（未知位置）导入名称“meshCalc”
-------------------------------------------------- ------------------------
ImportError Traceback（最近一次调用最后一次）
文件〜\ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ resipy \ meshTools.py：38
     37 尝试：
---&gt; 38 从 resipy.cext 导入 meshCalc 作为 mc
     39 除了异常 e：

ImportError：无法从“resipy.cext”（未知位置）导入名称“meshCalc”

在处理上述异常的过程中，又出现了一个异常：

异常回溯（最近一次调用最后一次）
[2] 中的单元格，第 9 行
      7 导入操作系统
      8 导入时间
----&gt; 9 从 resipy 导入 R2
     11 模型运行次数 = 1
     12 tic = 时间.time()

文件〜\ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ resipy \ __ init __.py：2
      1 名称 =“resipy”
----&gt; 2 从resipy.Project导入ResIPy_version、sysinfo
      3.从resipy.Project导入项目，R2
      4.从resipy.Surve导入Survey

文件〜\ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ resipy \ Project.py：40
     38 从 resipy.parsers 导入 geomParser
     39 从 resipy.r2in 导入 write2in
---&gt; 40 导入 resipy.meshTools 作为 mt
     41 从resipy.meshTools导入cropSurface
     42 从 resipy.template 导入 startAnmt、endAnmt

文件〜\ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ resipy \ meshTools.py：42
     40 print(&#39;无法导入 meshCalc 扩展，请参阅以下错误：&#39;)
     41 print(e)# 需要编译meshCalc
---&gt; 42 raise Exception(&#39;无法导入 meshCalc 扩展来解决问题尝试，&#39;\
     43&#39;更新 ResIPy、更新 Numpy 或重新编译扩展。&#39;)
     45 # 导入 pyvista（如果可用）
     46 尝试：

异常：无法导入 meshCalc 扩展来解决问题，请尝试更新 ResIPy、更新 Numpy 或重新编译扩展。

我收到此错误。

我正在尝试运行由 (from resipy import R2) 行组成的代码。我正在尝试执行，但以下软件包出现错误。
任何帮助将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/77893785/getting-error-while-running-the-code-which-consists-of-resipy-library-from-res</guid>
      <pubDate>Sun, 28 Jan 2024 06:02:30 GMT</pubDate>
    </item>
    <item>
      <title>编写代码或寻找算法来找到最佳配置</title>
      <link>https://stackoverflow.com/questions/77893756/writing-code-or-finding-a-algorithm-to-find-a-optimal-configuration</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77893756/writing-code-or-finding-a-algorithm-to-find-a-optimal-configuration</guid>
      <pubDate>Sun, 28 Jan 2024 05:41:40 GMT</pubDate>
    </item>
    <item>
      <title>寻求多特征输入的无代码 ML 解决方案来预测多个二进制输出 [关闭]</title>
      <link>https://stackoverflow.com/questions/77892428/seeking-no-code-ml-solution-for-multi-feature-input-to-predict-multiple-binary-o</link>
      <description><![CDATA[我是 ML 世界的新手，但我一直在使用 Microsoft Azure 环境及其现成的模型训练、测试和部署环境。
我已经成功找到了预测模型的选项和算法：多个特征作为输入，单个变量作为输出。
例如，给定个人的：(a) 工作范围、(b) 性别、(c) 酒精消费，我构建的系统能够预测（真/假）该人是否应该执行练习“E1”。
现在，从技术上讲，人们可以训练 20 个与此类似的模型，并使用关于练习“E2”、“E3”等的“SINGLE”二进制输出。
但是，我觉得必须有一种更简单的方法。
这样，为了训练模型，我会为其提供一个 CSV 文件，每个场景/人一行：
第一人：“数据录入员”，男，否==&gt; E1：是，E2：否，E3：是
第二个人：“按摩师”，女，是==&gt; E1：是，E2：是，E3：否
等等...
我向亲爱的社区成员提出的问题是：
什么算法/环境最适合此类任务？ （也许 Microsoft Azure 不是最好的？）
请注意，我希望无需任何编码即可执行上述设置、培训、测试和部署。
我不介意未来所有这些的编码版本；但目前我喜欢学习在无代码环境中执行此操作。
提前非常感谢，
我第一次尝试使用 AzureML，但无法通过使用 Azure ML 中的自动化 ML 功能获得多输出预测模型。我最终部署了一个能够仅预测分类模型目标值之一的模型。]]></description>
      <guid>https://stackoverflow.com/questions/77892428/seeking-no-code-ml-solution-for-multi-feature-input-to-predict-multiple-binary-o</guid>
      <pubDate>Sat, 27 Jan 2024 19:02:14 GMT</pubDate>
    </item>
    <item>
      <title>我有我的自定义训练模型（best.pt），它检测人和车头灯两件事。现在我想要根据这些条件输出</title>
      <link>https://stackoverflow.com/questions/77891961/i-have-my-custom-trained-model-best-pt-it-detects-two-things-person-and-headl</link>
      <description><![CDATA[你能帮我一下吗......
我有我的自定义训练模型（best.pt），它检测人和车头灯两件事。现在我想要根据以下条件输出： 1. 如果模型仅检测到车头灯返回 0, 2. 如果模型仅检测到人返回 1, 3. 如果模型检测到车头灯和人都返回 0。
&lt;前&gt;&lt;代码&gt;导入cv2
从 ultralytics 导入 YOLO

video_path = &#39;数据/video1.mp4&#39;
video_out_path = &#39;输出.mp4&#39;

cap = cv2.VideoCapture(video_path)

# 检查视频文件是否打开成功
如果不是 cap.isOpened():
    print(“错误：无法打开视频文件。”)
    出口（）

ret, 框架 = cap.read()

# 检查第一帧是否读取成功
如果不转：
    print(“错误：无法读取视频的第一帧。”)
    出口（）

cap_out = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*&#39;MP4V&#39;), cap.get(cv2.CAP_PROP_FPS),
                          (int(cap.get(3)), int(cap.get(4)))) # 使用 cap.get(3) 和 cap.get(4) 获取宽度和高度

模型 = YOLO(“bestall5.pt”)

检测阈值 = 0.5
休息时：
    结果列表=模型（框架）

    检测到头灯=假
    检测到的人=假

    # 遍历结果列表
    对于 results_list 中的结果：
        # 检查当前结果是否具有必要的属性
        if hasattr(结果, &#39;xyxy&#39;):
            对于 results.xyxy 中的结果：
                x1, y1, x2, y2, 分数, class_id = result.tolist()
                x1, x2, y1, y2 = int(x1), int(x2), int(y1), int(y2)

                # 假设class_id是模型类列表中类的索引
                类名=模型.名称[类id]

                if class_name == “车头灯”且分数&gt;检测阈值：
                    检测到头灯=真
                elif class_name == &quot;人&quot;;且分数&gt;检测阈值：
                    检测到的人 = True

    # 根据指定条件输出
    如果检测到头灯和检测到人：
        输出=0
    elif headlight_Detected：
        输出=0
    elif person_Detected：
        输出=1
    别的：
        输出 = -1 # 未检测到人或前灯

    print(“输出：”, 输出)

    cap_out.write(帧)

    cv2.imshow(&#39;目标检测&#39;, 框架)
    
    # 如果按下“q”键则中断循环
    如果 cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
        休息

    ret, 框架 = cap.read()

cap.release()
cap_out.release()
cv2.destroyAllWindows()

我尝试了这个，但只得到 -1 作为输出，但我的视频既有车头灯又有人物]]></description>
      <guid>https://stackoverflow.com/questions/77891961/i-have-my-custom-trained-model-best-pt-it-detects-two-things-person-and-headl</guid>
      <pubDate>Sat, 27 Jan 2024 16:40:44 GMT</pubDate>
    </item>
    <item>
      <title>安装斗争</title>
      <link>https://stackoverflow.com/questions/77889759/installation-struggle</link>
      <description><![CDATA[我们正在使用 Qiskit 工具包进行一个量子计算项目。但我们在导入或安装软件包和库时遇到了困难。在 Qiskit 中我们如何导入库和包？
澄清如何从外包安装库的疑问。]]></description>
      <guid>https://stackoverflow.com/questions/77889759/installation-struggle</guid>
      <pubDate>Sat, 27 Jan 2024 01:36:23 GMT</pubDate>
    </item>
    <item>
      <title>使用随机森林时，scikit 中的“ValueError：max_features 必须位于 (0, n_features] ”</title>
      <link>https://stackoverflow.com/questions/42072721/valueerror-max-features-must-be-in-0-n-features-in-scikit-when-using-rand</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/42072721/valueerror-max-features-must-be-in-0-n-features-in-scikit-when-using-rand</guid>
      <pubDate>Mon, 06 Feb 2017 16:32:32 GMT</pubDate>
    </item>
    </channel>
</rss>