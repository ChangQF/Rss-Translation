<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 28 Apr 2024 12:25:47 GMT</lastBuildDate>
    <item>
      <title>决策树信息增益与特征重要性</title>
      <link>https://stackoverflow.com/questions/78398063/decision-trees-information-gain-vs-feature-importance</link>
      <description><![CDATA[我基于 Sklearn 用 Python 编写了一个决策树，但是当我计算结果并显示决策树（以及 20 个最重要的特征）时，特征“A”被忽略了。最重要，也作为根节点。
但是，当我计算每个特征的信息增益并将结果显示在列表中时，特征“B”会出现。具有最高的信息增益(特征“A”也具有相当高的信息增益，但不如特征B)。尽管如此，特征 A 被用作根节点......所以我的问题是：我是否犯了编程错误，或者这是一种可能的情况（根据定义，具有最高信息增益的特征不被用作根节点）。 
在另一个主题中，有人写了以下内容：
&lt;块引用&gt;
对于使用信息增益的决策树，算法选择
提供最大信息增益的属性（这是
也是导致熵减少最大的属性）。

还有（尤其是这部分非常有趣）：
&lt;块引用&gt;
决策树算法是“贪婪的”算法。从某种意义上说，他们总是
选择产生最大信息增益的属性
正在考虑当前节点（分支），而无需稍后重新考虑
添加后续子分支后的属性。所以要回答你的
第二个问题：决策树算法尝试放置属性
在树根部附近信息增益最大。注意
由于算法的贪婪行为，决策树算法
不一定会产生一棵提供最大可能的树
熵的总体减少。

所以在这种情况下，它没有理由选择类别 B 而不是类别 A，这意味着我可能犯了一个编码错误..？]]></description>
      <guid>https://stackoverflow.com/questions/78398063/decision-trees-information-gain-vs-feature-importance</guid>
      <pubDate>Sun, 28 Apr 2024 11:50:34 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法评估模型是否能够识别有影响的变量（使用 make_classification 生成的变量）？</title>
      <link>https://stackoverflow.com/questions/78398017/is-there-a-way-to-evaluate-whether-a-model-is-able-to-identify-the-variables-tha</link>
      <description><![CDATA[我有一个关于 scikit-learn 的 make_classification 的问题。我使用 make_classification（二元分类任务）创建了一个数据集，目的是测试不同模型区分重要特征和不太重要特征的能力。
如何设置一个实验来评估模型是否能够识别有影响的变量？
我查看了 make_classification 的文档，但不幸的是我没有进一步了解。
我设置了以下内容：
X,y = make_classification(n_samples=50000, n_features=10, n_informative=5,
                    n_redundant=2、n_repeated=0、n_classes=2、n_clusters_per_class=2、
                          类间隔=1，
                   Flip_y=0.01，权重=[0.9,0.1]，shuffle=True，random_state=42）

谢谢您，我们非常感谢任何想法或建议。]]></description>
      <guid>https://stackoverflow.com/questions/78398017/is-there-a-way-to-evaluate-whether-a-model-is-able-to-identify-the-variables-tha</guid>
      <pubDate>Sun, 28 Apr 2024 11:37:08 GMT</pubDate>
    </item>
    <item>
      <title>如何将稀疏分类熵给出的预测类的二维数组输出转换为预测类</title>
      <link>https://stackoverflow.com/questions/78397693/how-to-convert-2d-array-output-of-predicted-classes-given-by-sparse-categorical</link>
      <description><![CDATA[我在大学接到一项任务，要编写一个简单文本数据集的分类器。我有 5 个类：[&#39;0&#39;、&#39;1&#39;、&#39;2&#39;、&#39;3&#39;、&#39;4&#39;]。我不太了解神经网络，并且使用互联网我写了一些东西 =)
但是
如果我使用 categorical_crossentropy 它会给我一个错误
参数“目标”和“输出”必须具有相同的等级 (ndim)。收到：目标。形状=（无，），输出.形状=（无，6）

如果我使用“sparse_categorical_crossentropy”，它会在输出中提供二维数组。
x_train，x_test，y_train，y_test = train_test_split（X_scaled，y_encoded，test_size = 0.2，random_state = 42）

x_train = x_train / 255
x_测试 = x_测试 / 255

def b_m(马力):
    模型=顺序（）
    activation_choice = hp.Choice(&#39;activation&#39;, value=[&#39;relu&#39;, &#39;sigmoid&#39;, &#39;tanh&#39;, &#39;elu&#39;, &#39;selu&#39;])
    model.add(密集(单位=hp.Int(&#39;units_input&#39;,
                                   最小值=512，
                                   最大值=1024，
                                   步骤=32),
                    input_dim=2,
                    激活=activation_choice））
    model.add(密集(单位=hp.Int(&#39;units_hidden&#39;,
                                   最小值=128，
                                   最大值=600，
                                   步骤=32),
                    激活=activation_choice））
    model.add（密集（5，激活=&#39;softmax&#39;））
    模型.编译(
        优化器=hp.Choice(&#39;优化器&#39;, 值=[&#39;adam&#39;,&#39;rmsprop&#39;,&#39;SGD&#39;]),
        损失=&#39;sparse_categorical_crossentropy&#39;,
        #loss=&#39;categorical_crossentropy&#39;,
        指标=[&#39;准确性&#39;])
    返回模型

调谐器 = kt.Hyperband(b_m,
                     目标=&#39;val_accuracy&#39;,
                     最大纪元=10,
                     因子=3，
                     目录=&#39;test_dir&#39;）

stop_early = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, 耐心=5)

tuner.search（x_train，y_train，epochs = 50，validation_split = 0.2，callbacks = [stop_early]）

best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

模型=tuner.hypermodel.build(best_hps)
历史= model.fit（x_train，y_train，epochs = 50，validation_split = 0.2）

val_acc_per_epoch = 历史.history[&#39;val_accuracy&#39;]
best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1

超级模型=tuner.hypermodel.build(best_hps)

hypermodel.fit（x_train，y_train，epochs = best_epoch，validation_split = 0.2）

eval_result = hypermodel.evaluate(x_test, y_test)
print(&quot;[损失，测试]:&quot;, eval_result)

y_pred = hypermodel.predict(x_test)


所以，我有三个问题：1.在这种情况下最好使用什么类型的损失？
2.如果使用categorical_crossentropy更好，那么如何修复出现的错误
3.如果使用sparse_categorical_crossentropy更好，那么如何将输出从2d数组转换为带有类的一维数组
请帮忙
我尝试重塑输入数据并更改模型的参数，但什么也没发生]]></description>
      <guid>https://stackoverflow.com/questions/78397693/how-to-convert-2d-array-output-of-predicted-classes-given-by-sparse-categorical</guid>
      <pubDate>Sun, 28 Apr 2024 09:26:46 GMT</pubDate>
    </item>
    <item>
      <title>打包 Cython 扩展时出现“ImportError”</title>
      <link>https://stackoverflow.com/questions/78397654/getting-an-importerror-when-packaging-a-cython-extension</link>
      <description><![CDATA[我们已经尝试解决此错误一段时间了。我正在开发一个名为 mlsauce 的包。当我克隆包并运行 python3 -m pip install 时。 --verbose（在虚拟环境中），我得到已成功安装 mlsauce-0.17.1。然而在运行时，我得到：
回溯（最近一次调用）：
  文件“/workspaces/codespaces-blank/mlsauce/examples/adaopt_classifier.py”，第 13 行，在  中。
    将mlsauce导入为ms
  文件“/workspaces/codespaces-blank/mlsauce/mlsauce/__init__.py”，第 58 行，位于  中。
    从 .adaopt 导入 AdaOpt
  文件“/workspaces/codespaces-blank/mlsauce/mlsauce/adaopt/__init__.py”，第 1 行，在  中
    从 .adaopt 导入 AdaOpt
  文件“/workspaces/codespaces-blank/mlsauce/mlsauce/adaopt/adaopt.py”，第 11 行，在  中。
    导入适配器
ModuleNotFoundError：没有名为“adaoptc”的模块

不过我可以在存储库中看到 .so 文件：
存储库的树结构
如何解决这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/78397654/getting-an-importerror-when-packaging-a-cython-extension</guid>
      <pubDate>Sun, 28 Apr 2024 09:14:21 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：形状 (64,100) 和 (10,100) 未对齐：100 (dim 1) != 10 (dim 0)</title>
      <link>https://stackoverflow.com/questions/78397046/valueerror-shapes-64-100-and-10-100-not-aligned-100-dim-1-10-dim-0</link>
      <description><![CDATA[我在训练 2 层神经网络时遇到这个错误
我尝试了这段代码，但它给出了上述错误。
将 numpy 导入为 np
从 numpy.random 导入 randn
&lt;前&gt;&lt;代码&gt;N、D_输入、H、D_输出 = 64、1000、100、10
x, y = randn(N, D_in), randn(N, D_out)
w1, w2 = randn(D_in, H), randn(D_out, H)

对于范围（2000）内的 t：
    h = 1 / (1 + np.exp(-x.dot(w1)))
    y_pred = h.dot(w2)
    损失 = np.square(y_pred - y).sum()
    打印（t，损失）

    grad_y_pred = 2.0 * (y_pred -y)
    grad_w2 = h.T.dot(grad_y_pred)
    grad_h = grad_y_pred.dot(w2.T)
    grad_w1 = x.T.dot(grad_h * h * (1 - h))

    w1 -= 1e-4 * grad_w1
    w2 -= 1e-4 * grad_w2
]]></description>
      <guid>https://stackoverflow.com/questions/78397046/valueerror-shapes-64-100-and-10-100-not-aligned-100-dim-1-10-dim-0</guid>
      <pubDate>Sun, 28 Apr 2024 04:12:08 GMT</pubDate>
    </item>
    <item>
      <title>机器学习：numpy，处理 NaN 值的问题</title>
      <link>https://stackoverflow.com/questions/78396645/machine-learning-numpy-issue-dealing-with-nan-values</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78396645/machine-learning-numpy-issue-dealing-with-nan-values</guid>
      <pubDate>Sat, 27 Apr 2024 23:19:52 GMT</pubDate>
    </item>
    <item>
      <title>多级数据的分类模型</title>
      <link>https://stackoverflow.com/questions/78396178/classification-models-for-multilevel-data</link>
      <description><![CDATA[我正在研究一个机器学习项目，准确地说是分类。我的数据集包含 217 个国家的社会、人口和经济指数，每个国家 60 年。目标变量是二进制的。我想训练随机森林和 xgboost 模型，我想知道：我可以用 Caret 训练这些模型吗？他们能够理解这种结构并处理多级数据吗？
如果是的话，我想用这种方式训练模型：
&lt;前&gt;&lt;代码&gt;#tree
ctrl_tree &lt;- trainControl(方法 = “cv”，数字 = 10，classProbs = TRUE，summaryFunction=twoClassSummary)
树 &lt;- train(Target~.,data=under, method = “rpart”,tuneLength = 10, metric=“ROC”, trControl = ctrl_tree)


#XGBoost
设置种子(76)
ctrl_xgb &lt;- trainControl(方法=“cv”，数字=10，搜索=“网格”，summaryFunction = TwoClassSummary，classProbs = TRUE)
param_grid_xgb &lt;- Expand.grid(nrounds=500, max_depth = c(3, 6, 9),eta = c(0.01, 0.1, 0.3),
  伽马 = c(0, 0.2, 0.4)，子样本 = c(0.8, 0.9, 1)，colsample_bytree = c(0.8, 0.9, 1)，
  min_child_weight=c(1, 5, 10))
xgb&lt;-train(Target~.,data=under,method=“xgbTree”,metric=“ROC”,tuneGrid=param_grid_xgb,
           trControl=ctrl_xgb,详细程度=0)
]]></description>
      <guid>https://stackoverflow.com/questions/78396178/classification-models-for-multilevel-data</guid>
      <pubDate>Sat, 27 Apr 2024 19:37:36 GMT</pubDate>
    </item>
    <item>
      <title>如何绘制多类分类中所有类的 SHAP 摘要图</title>
      <link>https://stackoverflow.com/questions/78396068/how-to-plot-shap-summary-plots-for-all-classes-in-multiclass-classification</link>
      <description><![CDATA[我正在使用 XGBoost 和 SHAP 来分析多类分类问题中的特征重要性，并且需要帮助一次性绘制所有类的 SHAP 摘要图。目前，我一次只能生成一个类的绘图。
SHAP 版本：0.45.0
Python版本：3.10.12

这是我的代码：
将 xgboost 导入为 xgb
导入形状
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.datasets 导入 make_classification
从 sklearn.metrics 导入 precision_score

# 生成合成数据
X，y = make_classification（n_samples = 500，n_features = 20，n_informative = 4，n_classes = 6，random_state = 42）
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 训练用于多类分类的 XGBoost 模型
模型 = xgb.XGBClassifier(objective=“multi:softprob”, random_state=42)
model.fit(X_train, y_train)

然后我尝试绘制形状值：
# 创建一个 SHAP TreeExplainer
解释器 = shap.TreeExplainer(模型)

# 计算测试集的SHAP值
shap_values = 解释器.shap_values(X_test)

# 尝试绘制所有类的摘要
shap.summary_plot（shap_values，X_test，plot_type =“酒吧”）

我得到了这个交互图：

我在此帖子：
shap.summary_plot(shap_values[:,:,0], X_test,plot_type=&quot;bar&quot;)

它给出了 0 类的正常条形图：

然后我可以对类 1、2、3 等执行相同的操作。
问题是，如何为所有类别制作汇总图？即，显示某个特征对每个类的贡献的单个图？]]></description>
      <guid>https://stackoverflow.com/questions/78396068/how-to-plot-shap-summary-plots-for-all-classes-in-multiclass-classification</guid>
      <pubDate>Sat, 27 Apr 2024 19:02:16 GMT</pubDate>
    </item>
    <item>
      <title>如何使用机器学习实时检测Windows网络异常？</title>
      <link>https://stackoverflow.com/questions/78396046/how-to-detect-real-time-windows-network-anomaly-using-ml</link>
      <description><![CDATA[我正在开发一个项目来检测网络中的异常检测，并且我的目标是其中的 Windows 网络异常检测。但我遇到了一些问题。

如何获取异常检测的 ML 模型的数据集，即我已经搜索过，大多数数据集都是 HDFS、服务器等。Windows 网络异常数据集没有特定的数据集

如何实时获取数据以进行异常检测。就像我希望将数据从用户电脑发送到管理员电脑，然后每 20 分钟发送到 api，看看是否有任何异常。


我对这两方面感到困惑。谁能引导我走上正确的道路？]]></description>
      <guid>https://stackoverflow.com/questions/78396046/how-to-detect-real-time-windows-network-anomaly-using-ml</guid>
      <pubDate>Sat, 27 Apr 2024 18:51:52 GMT</pubDate>
    </item>
    <item>
      <title>将灰狼优化器应用于我的支持向量回归模型</title>
      <link>https://stackoverflow.com/questions/78395857/applying-a-gray-wolf-optimiser-to-my-support-vector-regression-model</link>
      <description><![CDATA[这是我正在使用的 SVR 模型。我只需要稍微提高模型的准确性，并想尝试 GWO。我需要包含什么代码？
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler
进口火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
从 torch.optim.lr_scheduler 导入 ReduceLROnPlateau
从 scipy 导入统计数据

# 使用 PyTorch 定义 SVR 模型
类 SVRModel(nn.Module):
    def __init__(自身):
        超级（SVRModel，自我）.__init__()
        self.fc1 = nn.Linear(in_features=7, out_features=64) # 如果需要调整输入和输出特征
        self.fc2 = nn.Linear(in_features=64, out_features=32)
        self.fc3 = nn.Linear(in_features=32, out_features=1) # 输出层

    def 前向（自身，x）：
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        返回x


# 将数据转换为 PyTorch 张量
X_train_tensor = torch.tensor(scaled_X_train, dtype=torch.float32)
Y_train_tensor = torch.tensor(Y_train.reshape(-1, 1), dtype=torch.float32)
X_test_tensor = torch.tensor(scaled_X_test, dtype=torch.float32)

# 定义超参数
C = 1
ε = 0.1
lr = 0.01
历元 = 1000

# 实例化SVR模型
模型 = SVRModel()

# 定义损失函数和优化器
标准 = nn.MSELoss()
优化器 = optim.Adam(model.parameters(), lr=lr)

# 定义学习率调度器
调度程序=ReduceLROnPlateau（优化器，模式=&#39;min&#39;，因子=0.5，耐心=5，详细=True）

# 训练循环
对于范围内的纪元（纪元）：
    模型.train()
    优化器.zero_grad()
    输出=模型（X_train_tensor）
    损失 = 标准（输出，Y_train_tensor）
    loss.backward()
    优化器.step()
    
    # 步骤调度器
    调度程序.step(损失)
    
    print(f&#39;Epoch [{epoch+1}/{epochs}], 损失: {loss.item():.4f}&#39;)

＃ 评估
模型.eval()
使用 torch.no_grad()：
    Y_pred_tensor = 模型(X_test_tensor)
    Y_pred = Y_pred_tensor.numpy().flatten()

# 计算相关性
corr = stats.pearsonr(Y_test, Y_pred)[0]
print(f&#39;相关性：{corr}&#39;)

# 创建结果数据框
result_df = pd.DataFrame({&#39;svr_predicted&#39;: Y_pred, &#39;true&#39;: Y_test})
result_df[&#39;svr_error&#39;] = abs(result_df[&#39;true&#39;] - result_df[&#39;svr_predicted&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/78395857/applying-a-gray-wolf-optimiser-to-my-support-vector-regression-model</guid>
      <pubDate>Sat, 27 Apr 2024 17:43:56 GMT</pubDate>
    </item>
    <item>
      <title>模型训练时间过长</title>
      <link>https://stackoverflow.com/questions/78395684/model-training-taking-too-long-time</link>
      <description><![CDATA[我正在尝试训练 XGBRegressor，但代码执行时间太长。我做错了什么吗？代码如下：
&lt;前&gt;&lt;代码&gt;%%时间
!pip 安装 xgboost
将 xgboost 导入为 xgb
测试 = dd.read_csv(&#39;/kaggle/input/leap-atmospheric-physicals-ai-climsim/test.csv&#39;)
#dd 指 dask.dataframe
样本 ID = 测试[&#39;样本 ID&#39;]

测试 = test.drop(&#39;sample_id&#39;, axis = 1)
X = df_train.drop(目标 + [&#39;sample_id&#39;], axis = 1)
预测 = {}
提交={}
提交[&#39;样本id&#39;] = 样本id
对于 i，枚举中的目标（目标）：
    y = df_train[目标]
    X_train，X_test，y_train，y_test = train_test_split（X，y，random_state = 42，test_size = 0.33）
    dtr = xgb.XGBRegressor(详细 = False)
    dtr.fit(X_train, y_train)
    y_hat = dtr.predict(X_test)
    提交[目标] = dtr.predict(测试)
    预测[目标] = y_hat
    print(f&#39;r2_score for {target} : {r2_score(y_hat, y_test)}&#39;)

训练数据很大，但即使记录应该有效，但我在输出中看不到任何内容。]]></description>
      <guid>https://stackoverflow.com/questions/78395684/model-training-taking-too-long-time</guid>
      <pubDate>Sat, 27 Apr 2024 16:34:34 GMT</pubDate>
    </item>
    <item>
      <title>多类问题的层次分类方法</title>
      <link>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</link>
      <description><![CDATA[有一个多类分类任务。我的目标是使用每父节点本地分类器 (LCPN) 方法来解决这个问题。
让我解释一下如何使用 MWE。
假设我有这个虚拟数据集：
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification
从 scipy.cluster 导入层次结构

X, y = make_classification(n_samples=1000, n_features=10, n_classes=5,
                             n_信息=4）

我想出了这些类之间的距离矩阵：
d = np.array(
[[ 0.、201.537、197.294、200.823、194.517]、
 [201.537, 0., 199.449, 202.941, 196.703],
 [197.294, 199.449, 0., 198.728, 192.354],
 [200.823, 202.941, 198.728, 0., 195.972],
[[194.517, 196.703, 192.354, 195.972, 0.]]
）

因此，我确定了类层次结构，如下所示：
hc = hierarchy.linkage(d, method=&#39;complete&#39;)

得到的树状图如下：
dendrogram = hierarchy.dendrogram(hc, labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;, &#39;D&#39;, &#39;F&#39;])
树状图


我使用hierarchy.to_tree()以树状结构进行说明：

我的问题：
如何按照 LCPN 方法在每个内部节点（包括根）处安装分类器，例如 DecisionTreeClassifier 或 SVM，以像在树中一样进行上图？]]></description>
      <guid>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</guid>
      <pubDate>Sat, 20 Apr 2024 14:08:05 GMT</pubDate>
    </item>
    <item>
      <title>我应该把reuse_actors=True放在哪里？</title>
      <link>https://stackoverflow.com/questions/76354078/where-should-i-put-reuse-actors-true</link>
      <description><![CDATA[运行以下代码后，它会显示
&lt;块引用&gt;
INFO trainable.py:172 – Trainable.setup 花费了 2940.989 秒。如果您的可训练初始化速度很慢，请考虑设置reuse_actors=True以减少actor创建开销

导入光线
ray.init(地址=“自动”, _temp_dir=&#39;/home/ray_dir&#39;)

rnd = 随机.种子(8)
grid_cv = StratifiedKFold(n_splits=3,random_state=rnd, shuffle=True)

从 xgboost.callback 导入 EarlyStopping
Early_stopping = EarlyStopping(轮数 = 50, 最大化 = True, save_best = True)
clf = xgb.XGBClassifier(
                tree_method=&#39;gpu_hist&#39;,
                最大bin=512，
                学习率 = 0.0001,
                n_估计器=1000，
                目标=&#39;二进制：逻辑&#39;，reg_alpha=0.01，
                scale_pos_weight = pos_weight, eval_metric= &#39;aucpr&#39;,
                回调=[early_stopping],
                详细程度 = 0,
                线程数 = 96
                ）


参数 = {
    &#39;eta&#39;: [0.01, 0.1, 0.3],
    &#39;min_child_weight&#39;: [1,3,8,16],
    &#39;最大深度&#39;:[25,50,100,500],
    &#39;colsample_bytree&#39;: [0.4,0.6,0.8],
    &#39;子样本&#39;: [0.4,0.6,0.8],
    &#39;伽玛&#39;：[0,0.5,2,10],
}

gs = TuneGridSearchCV（估计器 = clf、param_grid = param、cv = grid_cv、n_jobs = -1、refit = True、return_train_score = True、verbose = 3、评分 = &#39;average_ precision&#39;、use_gpu = True ）

gs.fit(X_train, y_train, eval_set= eval_set_xgboost, verbose=True)

我应该在代码中的何处添加 reuse_actors=True ？]]></description>
      <guid>https://stackoverflow.com/questions/76354078/where-should-i-put-reuse-actors-true</guid>
      <pubDate>Mon, 29 May 2023 00:25:33 GMT</pubDate>
    </item>
    <item>
      <title>如何在Python中使用softmax输出进行神经网络和机器学习来解释多项Logit模型？ [复制]</title>
      <link>https://stackoverflow.com/questions/60482320/how-to-use-softmax-output-in-python-for-neural-network-and-machine-learning-to-i</link>
      <description><![CDATA[它涉及使用机器学习和神经网络的 softmax 函数输出来理解和解释多项 Logit 模型。]]></description>
      <guid>https://stackoverflow.com/questions/60482320/how-to-use-softmax-output-in-python-for-neural-network-and-machine-learning-to-i</guid>
      <pubDate>Mon, 02 Mar 2020 03:49:09 GMT</pubDate>
    </item>
    <item>
      <title>平滑后的GPS数据对比</title>
      <link>https://stackoverflow.com/questions/27709732/gps-data-comparison-after-smoothing</link>
      <description><![CDATA[我正在尝试比较用于平滑 GPS 数据的多种算法。我想知道比较结果以查看哪一个提供更好的平滑效果的标准方法应该是什么。
我正在考虑一种机器学习方法。基于分类器创建汽车模型并检查哪些轨道提供更好的行为。
对于在这方面有更多经验的人来说，这是一个好方法吗？还有其他方法可以做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/27709732/gps-data-comparison-after-smoothing</guid>
      <pubDate>Tue, 30 Dec 2014 17:21:04 GMT</pubDate>
    </item>
    </channel>
</rss>