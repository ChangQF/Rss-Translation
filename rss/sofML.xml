<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 16 Jan 2025 21:15:12 GMT</lastBuildDate>
    <item>
      <title>使用在 CICIDS2017 和 Suricata 上训练的机器学习设计自适应入侵检测系统</title>
      <link>https://stackoverflow.com/questions/79363012/designing-an-adaptive-intrusion-detection-system-using-machine-learning-trained</link>
      <description><![CDATA[我如何开发一个有效的自适应入侵检测系统，利用机器学习技术根据动态网络流量分析自动修改现有的 Suricata 规则，同时最大限度地减少误报并确保这些自动生成的规则的安全实施？
具体来说，我正在寻找以下方面的指导：
鉴于我当前的功能集导致模型即使在模拟攻击期间也将所有流量归类为良性，应该从网络流量数据和 Suricata 日志中选择或设计哪些功能来训练能够实时准确检测已知和新威胁的机器学习模型？
考虑到需要从 CICIDS2017 等初始数据集中学习，同时不断适应我自己网络不断变化的流量模式和新出现的威胁，哪些机器学习架构和训练方法最适合这种自适应 IDS 用例？
我如何设计一个集成管道，自动从 Suricata 中提取训练数据，验证机器学习模型输出，从高置信度检测中生成优化的 Suricata 规则，并结合分析师反馈，随着时间的推移迭代提高系统的准确性和稳健性？
最终目标是入侵检测系统可以动态调整以适应不断变化的网络条件并提供可靠、可操作的安全警报，而不会让分析师承受过多的误报或引入可能破坏合法流量的错误配置规则。我很感激任何关于成功实施这种自适应、ML 驱动的 IDS 所需的特征工程、建模技术、系统架构和自动化机制的见解或建议。
目前我使用此功能，但即使我创建 DDoS 攻击或 suricate 创建警报，模型也会将所有日志视为 BENIGN。模型在 CICIDS2017 数据上进行训练。
selected_features = [ 
# 基于流的特征
&#39;流持续时间&#39;、&#39;流字节/秒&#39;、&#39;流数据包/秒&#39;、&#39;转发数据包总长度&#39;、&#39;回程数据包总长度&#39;,

# 时间特征
&#39;流 IAT 平均值&#39;、&#39;流 IAT 标准&#39;、&#39;流 IAT 最大值&#39;、&#39;流 IAT 最小值&#39;,
&#39;转发 IAT 总数&#39;、&#39;回程 IAT 总数&#39;,

# 数据包特征
&#39;转发数据包长度最大值&#39;、&#39;转发数据包长度最小值&#39;,
&#39;回程数据包长度最大值&#39;、&#39;回程数据包长度最小值&#39;,
&#39;数据包长度平均值&#39;、&#39;数据包长度标准&#39;、&#39;数据包长度方差&#39;,

# TCP 标志
&#39;SYN 标志计数&#39;, &#39;FIN 标志计数&#39;, &#39;RST 标志计数&#39;,
&#39;PSH 标志计数&#39;, &#39;ACK 标志计数&#39;, &#39;URG 标志计数&#39;,

# 附加功能
&#39;总转发数据包&#39;, &#39;总反向数据包&#39;,
&#39;转发报头长度&#39;, &#39;反向报头长度&#39;,
&#39;活动平均值&#39;, &#39;活动标准&#39;, &#39;空闲平均值&#39;,
&#39;Init_Win_bytes_forward&#39;, &#39;Init_Win_bytes_backward&#39;
]

我尝试使用此功能，但没有用。我已经创建了文件来测试这个，我使用这些命令 sudo 来创建一个测试文件。
hping3 -2 -p 80 -S --flood --rand-source 192.168.0.target
sudo hping3 -S -p 80 --flood 192.168.0.target
sudo hping3 -1 --flood 192.168.0.target
sudo nmap -sS 192.168.0.target
sudo nmap -sS -sV -p- 192.168.0.target
sudo nmap -sU 192.168.0.target
sudo nmap -sU 192.168.0.target
hydra -l user -P wordlist.txt 192.168.0.target ssh
hydra -l user -P wordlist.txt ftp://192.168.0.target
hydra -l admin -P wordlist.txt 192.168.0.target http-get /admin/
for i in {1…1000}; do curl http://192.168.0.target/; done
while true; do
curl http://192.168.0.target
sleep 10
done
for endpoint in / /about /contact /login; do
curl http://192.168.0.target$endpoint
sleep 5
done
]]></description>
      <guid>https://stackoverflow.com/questions/79363012/designing-an-adaptive-intrusion-detection-system-using-machine-learning-trained</guid>
      <pubDate>Thu, 16 Jan 2025 20:38:02 GMT</pubDate>
    </item>
    <item>
      <title>如何处理中间阶段的 NaN 损失</title>
      <link>https://stackoverflow.com/questions/79362342/how-to-handle-nan-losses-at-intermediate-epochs</link>
      <description><![CDATA[import torch
import torch.nn as nn

class PINN(nn.Module):
def __init__(self, input_dim, output_dim, hidden_​​layers, neurons_per_layer):
super(PINN, self).__init__()
layer = []
layer.append(nn.Linear(input_dim, neurons_per_layer))
for _ in range(hidden_​​layers):
layer.append(nn.Linear(neurons_per_layer, neurons_per_layer))
layer.append(nn.Linear(neurons_per_layer, output_dim))
self.network = nn.Sequential(*layers)

def forward(self, x):
return self.network(x)

# 示例：生成随机输入数据
inputs = torch.rand((1000, 3)) # 3D 输入坐标

model = PINN(input_dim=3, output_dim=3, hidden_​​layers=4, neurons_per_layer=64)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 10000
for epoch in range(epochs):
optimizer.zero_grad() 

nn_output = model(inputs) # 计算 NN 预测
# 根据 nn_output 计算损失
loss = loss_func(nn_output) # --&gt; loss = NaN，如何继续？

loss.backward() 
optimizer.step()

将输入传递到网络后，基于物理的损失函数（NN 输出函数）可能会产生 NaN，因为 NN 输出不合适。
在中间阶段，我可以接受违反物理定律，但优化器（例如 Adam）是否能够从返回的 NaN 损失中恢复？
（MATLAB 中的许多优化器可以正确处理 NaN，并希望目标函数在无法进行前向评估的点返回 NaN。这是一个尝试不同点的指标。）]]></description>
      <guid>https://stackoverflow.com/questions/79362342/how-to-handle-nan-losses-at-intermediate-epochs</guid>
      <pubDate>Thu, 16 Jan 2025 16:17:08 GMT</pubDate>
    </item>
    <item>
      <title>所有样本的前向传递</title>
      <link>https://stackoverflow.com/questions/79361940/forward-pass-with-all-samples</link>
      <description><![CDATA[import torch
import torch.nn as nn

class PINN(nn.Module):
def __init__(self, input_dim, output_dim, hidden_​​layers, neurons_per_layer):
super(PINN, self).__init__()
layer = []
layer.append(nn.Linear(input_dim, neurons_per_layer))
for _ in range(hidden_​​layers):
layer.append(nn.Linear(neurons_per_layer, neurons_per_layer))
layer.append(nn.Linear(neurons_per_layer, output_dim))
self.network = nn.Sequential(*layers)

def forward(self, x):
return self.network(x)

# 示例：生成随机输入数据
inputs = torch.rand((1000, 3)) # 3D 输入坐标

model = PINN(input_dim=3, output_dim=3, hidden_​​layers=4, neurons_per_layer=64)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 10000
for epoch in range(epochs):
optimizer.zero_grad() 
nn_output = model(inputs) # 计算 NN 预测
# 计算 nn_output 的梯度
loss.backward() 
optimizer.step() 

我想实现一个物理信息 NN，其中输入是 N 个 3d 点 (x,y,z)，NN 输出是此时的矢量值量，即输入维度和输出维度都相同。
要计算每个时期的损失，我需要有该量的值在所有点。示例：对于 N=1000 点，我需要所有 1000 个 NN 预测，然后才能继续进行损失计算。
在我的代码中，我基本上将一个 1000x3 对象提供给输入层，假设 pytorch 将每一行（1x3）分别传递给网络，最后将其再次组织为 1000x3 对象。
pytorch 是否像那样工作，还是我必须重新考虑这种方法？]]></description>
      <guid>https://stackoverflow.com/questions/79361940/forward-pass-with-all-samples</guid>
      <pubDate>Thu, 16 Jan 2025 14:23:21 GMT</pubDate>
    </item>
    <item>
      <title>通过手动汇总值来重现 LGBMRegressor 预测</title>
      <link>https://stackoverflow.com/questions/79361226/reproduce-lgbmregressor-predictions-by-manually-aggregate-the-values</link>
      <description><![CDATA[我正在尝试自己重现 LGBMRegressor 预测，因此当我成功时，我会将平均值转换为中位数。但目前看来我做不到。
这是我为检查是否可以重现结果而创建的简单脚本。
我需要 reg_y_hat 与 self_y_hat 相同。
我遗漏了什么？如果我知道训练中的哪些样本落到每个叶子上，我就可以自己汇总预测...
 import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split

# 生成一些随机回归数据
np.random.seed(42)
X = np.random.rand(100, 5)
y = 4 * X[:, 0] - 2 * X[:, 1] + np.random.rand(100) * 0.1

# 分割数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 LGBMRegressor
model = lgb.LGBMRegressor(objective=&#39;regression&#39;, n_estimators=2, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)

# 常规预测：
reg_y_hat = model.predict(X_test)

# 获取初始预测（y_train 的平均值）
init_pred = np.mean(y_train)

# 获取训练叶子值
train_leaf_indices = model.predict(X_train, pred_leaf=True)
leaf_samples = {(i, leaf_id): [] for i in range(model.n_estimators) for leaf_id in np.unique(train_leaf_indices[:, i])}

# 存储每个叶子的相应目标值
for i, row in enumerate(train_leaf_indices):
for j, leaf_id in enumerate(row):
leaf_samples[(j, leaf_id)].append(y_train[i])

#计算每个叶子的平均值：
leaf_agg = {}
for key, values in leaf_samples.items():
leaf_agg[key] = np.mean(values)

# 通过聚合平均值并添加初始预测进行预测：
preds = []
test_leaf_indices = model.predict(X_test, pred_leaf=True)
for row_indices in test_leaf_indices:
row_pred = init_pred
for i, leaf_index in enumerate(row_indices):
row_pred += model.learning_rate * (leaf_agg[(i, leaf_index)] - init_pred) # 仅初始预测后叶子的残差贡献
preds.append(row_pred)
self_y_hat = np.array(preds)

# 验证结果
print(&#39;reg_y_hat 之间的差异和 self_y_hat:&#39;, np.abs(reg_y_hat - self_y_hat).sum())
]]></description>
      <guid>https://stackoverflow.com/questions/79361226/reproduce-lgbmregressor-predictions-by-manually-aggregate-the-values</guid>
      <pubDate>Thu, 16 Jan 2025 10:39:14 GMT</pubDate>
    </item>
    <item>
      <title>代码无法在 AWS sagemaker 上成功运行</title>
      <link>https://stackoverflow.com/questions/79361126/code-not-running-sucessfully-on-aws-sagemaker</link>
      <description><![CDATA[当我运行应该创建训练作业的单元时，我没有在 sagemaker 笔记本上收到任何更新？我在 AWS Sagemaker 上运行以下代码，当我运行此代码时，它显示训练作业已创建并且没有进一步的更新。当我检查训练作业时，我可以看到状态已完成，但代码仍在笔记本上运行并且没有显示任何更新。有人可以帮我为什么会这样吗？
kmeans.fit(kmeans.record_set(scaled_data)) 
]]></description>
      <guid>https://stackoverflow.com/questions/79361126/code-not-running-sucessfully-on-aws-sagemaker</guid>
      <pubDate>Thu, 16 Jan 2025 10:15:35 GMT</pubDate>
    </item>
    <item>
      <title>有效覆盖 pytorch 数据集</title>
      <link>https://stackoverflow.com/questions/79360229/override-pytorch-dataset-efficiently</link>
      <description><![CDATA[我想继承 torch.utils.data.Dataset 类来加载我的自定义图像数据集，比如说用于分类任务。这是官方 pytorch 网站的示例，位于此 链接:
import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
self.img_labels = pd.read_csv(annotations_file)
self.img_dir = img_dir
self.transform = transform
self.target_transform = target_transform

def __len__(self):
return len(self.img_labels)

def __getitem__(self, idx):
img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
image = read_image(img_path)
label = self.img_labels.iloc[idx, 1]
if self.transform:
image = self.transform(image)
if self.target_transform:
label = self.target_transform(label)
return image, label

我注意到：

在 __getitem__ 中，我们将图像从磁盘读取到内存中。这意味着如果我们训练模型几个时期，我们会多次将同一幅图像重新读入内存。据我所知，这是一个代价高昂的操作
每次从磁盘读取图像时都会应用一次变换，在我看来，这几乎是一个多余的操作。

我理解，在非常大的数据集中，我们无法将数据完全放入内存中，因此我们别无选择，只能以这种方式读取数据（因为我们必须在一个时期内迭代所有数据），我想知道，如果我的所有数据都可以放入内存中，那么在 __init__ 函数中从磁盘读取所有数据不是更好的方法吗？
通过我在计算机视觉方面的一点经验，我注意到在 变换 中将图像裁剪成固定大小的图像非常常见。那么为什么我们不应该裁剪图像一次并将其存储在磁盘上的其他地方，而在整个训练过程中只读取裁剪后的图像呢？在我看来，这似乎是一种更有效的方法。
我理解，一些用于增强而不是规范化的转换最好应用于 __getitem__ 中，以便获得随机生成的数据而不是固定的数据。
你能为我澄清一下这个主题吗？
如果我缺少的是常识，请用正确的方法指导我找到代码库。]]></description>
      <guid>https://stackoverflow.com/questions/79360229/override-pytorch-dataset-efficiently</guid>
      <pubDate>Thu, 16 Jan 2025 02:38:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 yolov5 创建的模型执行脚本时出现图像大小调整问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/79360063/image-resizing-issue-while-executing-script-with-a-model-created-using-yolov5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79360063/image-resizing-issue-while-executing-script-with-a-model-created-using-yolov5</guid>
      <pubDate>Thu, 16 Jan 2025 00:27:41 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-learn 中 .fit() 方法的用例？[重复]</title>
      <link>https://stackoverflow.com/questions/79360038/use-cases-for-the-fit-method-in-scikit-learn</link>
      <description><![CDATA[是否存在 .fit() 比使用 .fit_transform() 更实用的情况/用例？例如，当标签编码时：
encoder = LabelEncoder()
title = data[&#39;title&#39;]
encoder.fit(title)
title_encoded =coder.transform(title)

vs
encoder = LabelEncoder()
title = data[&#39;title&#39;]
title_encoded =coder.fit_transform(title)
]]></description>
      <guid>https://stackoverflow.com/questions/79360038/use-cases-for-the-fit-method-in-scikit-learn</guid>
      <pubDate>Thu, 16 Jan 2025 00:04:04 GMT</pubDate>
    </item>
    <item>
      <title>F1-score、IOU 和 Dice Score 的实现</title>
      <link>https://stackoverflow.com/questions/79359767/implementation-of-f1-score-iou-and-dice-score</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79359767/implementation-of-f1-score-iou-and-dice-score</guid>
      <pubDate>Wed, 15 Jan 2025 21:33:49 GMT</pubDate>
    </item>
    <item>
      <title>使用 GCP 服务进行图像分类[关闭]</title>
      <link>https://stackoverflow.com/questions/79356912/use-gcp-service-for-image-classification</link>
      <description><![CDATA[我的任务：我将图像作为来自物联网设备的输入，将其发送到云端，进行图像分类，并将结果发送回某个 URL。
我做了什么：我尝试了 TF 服务，docker 镜像与我的本地模型，并在我的设备上进行分类。
有没有更好的方法来实现它。]]></description>
      <guid>https://stackoverflow.com/questions/79356912/use-gcp-service-for-image-classification</guid>
      <pubDate>Wed, 15 Jan 2025 02:26:56 GMT</pubDate>
    </item>
    <item>
      <title>stable_baselines3：为什么比较 ep_info_buffer 与评估时奖励不匹配？</title>
      <link>https://stackoverflow.com/questions/79353843/stable-baselines3-why-the-reward-does-not-match-comparing-ep-info-buffer-vs-eva</link>
      <description><![CDATA[我正在使用 stable_baselines3 库，这时我发现了一些意想不到的东西。
这里有一个简单的代码来重现这个问题：
import gymnasium as gym

from stable_baselines3 import DQN

env = gym.make(&quot;CartPole-v1&quot;)

model = DQN(&quot;MlpPolicy&quot;, env, verbose=0, stats_window_size=100_000)
model.learn(total_timesteps=100_000)

看看最后一集的奖励：
print(model.ep_info_buffer[-1])


{&#39;r&#39;: 409.0, &#39;l&#39;: 409, &#39;t&#39;: 54.87983

但是如果我使用以下代码评估模型：
obs, info = env.reset()
total_reward = 0
while True:
action, _states = model.predict(obs, deterministic=True)
obs, reward, termed, truncated, info = env.step(action)
total_reward = total_reward + reward
if termed or truncated:
obs, info = env.reset()
break

print(&quot;total_reward {}&quot;.format(total_reward))


total_reward 196.0

我得到了不同的奖励，这是我没有预料到的。
我预计会得到与 409 相同的奖励model.ep_info_buffer[-1]。
为什么会有这种差异？.ep_info_buffer 与每集奖励不同吗？]]></description>
      <guid>https://stackoverflow.com/questions/79353843/stable-baselines3-why-the-reward-does-not-match-comparing-ep-info-buffer-vs-eva</guid>
      <pubDate>Tue, 14 Jan 2025 02:14:32 GMT</pubDate>
    </item>
    <item>
      <title>如何为 Transformer 实现位置方向前馈神经网络？</title>
      <link>https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers</link>
      <description><![CDATA[我很难理解 transformer 架构中的位置式前馈神经网络。

让我们以机器翻译任务为例，其中输入是句子。从图中我了解到，对于每个单词，不同的前馈神经网络用于自注意子层的输出。前馈层应用了类似的线性变换，但每个变换的实际权重和偏差是不同的，因为它们是两个不同的前馈神经网络。
参考链接，这是PositionWiseFeedForward神经网络的类
class PositionwiseFeedForward(nn.Module):
“实现 FFN 方程。”
def __init__(self, d_model, d_ff, dropout=0.1):
super(PositionwiseFeedForward, self).__init__()
self.w_1 = nn.Linear(d_model, d_ff)
self.w_2 = nn.Linear(d_ff, d_model)
self.dropout = nn.Dropout(dropout)

def forward(self, x):
return self.w_2(self.dropout(F.relu(self.w_1(x))))

我的问题是：
我没有看到任何与位置相关的信息。这是一个具有两层的简单全连接神经网络。假设 x 是句子中每个单词的嵌入列表，句子中的每个单词都由上面的层使用相同的权重和偏差进行转换。（如果我错了，请纠正我）
我期望找到类似将每个单词嵌入传递到单独的 Linear 层的方法，该层将具有不同的权重和偏差，以实现与图片中所示的类似效果。]]></description>
      <guid>https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers</guid>
      <pubDate>Mon, 02 Jan 2023 05:59:25 GMT</pubDate>
    </item>
    <item>
      <title>(使用 cpu)Pytorch：IndexError：索引超出自身范围。 (使用 cuda)断言“srcIndex < srcSelectDimSize”失败。 如何解决？</title>
      <link>https://stackoverflow.com/questions/69596496/with-cpupytorch-indexerror-index-out-of-range-in-self-with-cudaassertion</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/69596496/with-cpupytorch-indexerror-index-out-of-range-in-self-with-cudaassertion</guid>
      <pubDate>Sat, 16 Oct 2021 14:28:36 GMT</pubDate>
    </item>
    <item>
      <title>在 lightgbm 或 XGBoost 中校准概率</title>
      <link>https://stackoverflow.com/questions/60772387/calibrating-probabilities-in-lightgbm-or-xgboost</link>
      <description><![CDATA[我需要帮助校准 lightgbm 中的概率
下面是我的代码
cv_results = lgb.cv(params, 
lgtrain, 
nfold=10,
stratified=False ,
num_boost_round = num_rounds,
verbose_eval=10,
early_stopping_rounds = 50, 
seed = 50)

best_nrounds = cv_results.shape[0] - 1

lgb_clf = lgb.train(params, 
lgtrain, 
num_boost_round=10000 ,
valid_sets=[lgtrain,lgvalid],
early_stopping_rounds=50,
verbose_eval=10)

ypred = lgb_clf.predict(test, num_iteration=lgb_clf.best_iteration)
]]></description>
      <guid>https://stackoverflow.com/questions/60772387/calibrating-probabilities-in-lightgbm-or-xgboost</guid>
      <pubDate>Fri, 20 Mar 2020 10:29:00 GMT</pubDate>
    </item>
    <item>
      <title>sklearn 中的 fit 方法</title>
      <link>https://stackoverflow.com/questions/34727919/fit-method-in-sklearn</link>
      <description><![CDATA[我问了自己关于 sklearn 中的 fit 方法的各种问题。
问题 1：当我这样做时：
from sklearn.decomposition import TruncatedSVD
model = TruncatedSVD()
svd_1 = model.fit(X1)
svd_2 = model.fit(X2)

变量 model 的内容在此过程中是否发生了任何变化？
问题 2：当我这样做时：
from sklearn.decomposition import TruncatedSVD
model = TruncatedSVD()
svd_1 = model.fit(X1)
svd_2 = svd_1.fit(X2)

svd_1 发生了什么？换句话说，svd_1 已经被拟合了，我再次拟合它，那么它的组件发生了什么？]]></description>
      <guid>https://stackoverflow.com/questions/34727919/fit-method-in-sklearn</guid>
      <pubDate>Mon, 11 Jan 2016 17:49:15 GMT</pubDate>
    </item>
    </channel>
</rss>