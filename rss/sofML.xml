<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 29 Mar 2024 06:18:02 GMT</lastBuildDate>
    <item>
      <title>在数字分类混合数据帧上应用 RandomForestRegressor 来预测两列标签集时得分较低</title>
      <link>https://stackoverflow.com/questions/78242202/low-score-when-applying-randomforestregressor-on-a-numeric-categorical-mixed-dat</link>
      <description><![CDATA[我在 Kaggle 上使用此保险数据集 保险数据集
尝试构建一个简单的回归器来预测最后两列 [&#39;coverage_level&#39;,&#39;charges&#39;]，同时使用所有其他 10 列作为特征输入到回归器模型中。
我意识到用作特征的 10 列既是数字类型又是分类类型，因此我使用 LabelEncoder 进行了一些转换：
df2 = df.copy()
# 姜
le = 标签编码器()
le.fit(df2.gender.drop_duplicates())
df2.gender = le.transform(df2.gender)
...其余分类列如“吸烟者”、“地区”等。

然后我在转换后的数据帧上应用了最小最大缩放器：
inputs = df2[[“年龄”、“性别”、“bmi”、“儿童”、“吸烟者”、“地区”、“医疗历史”、
         “家庭医疗史”、“运动频率”、“职业”]]
目标 = df2[[“coverage_level”, “charges”]]

缩放器 = MinMaxScaler()
scaledInputs = np.array(scaler.fit_transform(输入))

X_train，X_test，y_train，y_test = train_test_split（scaledInputs，目标，test_size = 0.20，random_state = 42）

最后是训练和测试部分：
rf_model = RandomForestRegressor(n_estimators=10, random_state=42)

# 拟合训练集
rf_model.fit(X_train, y_train)
rf_outputs = rf_model.predict(X_test)

rf_mse =mean_squared_error(y_test, rf_outputs)
rf_score = rf_model.score(X_test, y_test)

但是性能非常低，得分为0.27，mse接近2615601。
我尝试了一些修复。第一个不是仅缩放输入，而是在馈送之前缩放了两个目标列 [&#39;coverage_level&#39;,&#39;charges&#39;]，但是，它根本没有帮助。第二个修复是使用one-hot编码代替标签编码，但仍然没有增益。
请提出一些解决此问题的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78242202/low-score-when-applying-randomforestregressor-on-a-numeric-categorical-mixed-dat</guid>
      <pubDate>Fri, 29 Mar 2024 05:50:10 GMT</pubDate>
    </item>
    <item>
      <title>如何解决：InvalidArgumentError：图形执行错误？</title>
      <link>https://stackoverflow.com/questions/78242076/how-i-resolve-invalidargumenterror-graph-execution-error</link>
      <description><![CDATA[我是计算机视觉和分类方面的专家，我正在尝试使用 keras VGG16 模型进行迁移学习，但我不断收到此代码下面的错误，任何人都可以帮助我或至少给我一个和平的建议？
导入tensorflow为tf
从tensorflow.keras导入模型、层
将 matplotlib.pyplot 导入为 plt
从全局导入全局
导入操作系统

IMG_大小 = 224
批次大小 = 32
通道 = 3
纪元 = 30

数据集 = tf.keras.preprocessing.image_dataset_from_directory(
    “../笔记本/数据集”，
    随机播放=真，
    图像大小=（IMG_SIZE，IMG_SIZE），
    批量大小=批量大小，
    
）

类名 = 数据集.类名
类名

def preprocess_image(图像):
    图像 = tf.image.resize(图像, [IMG_SIZE, IMG_SIZE])
    image /= 255.0 # 标准化为 [0,1] 范围
    打印（“嗨”）
    打印（图像）
    返回图像

# 应用预处理和增强
数据集 = 数据集. 地图(
    lambda x, y: (preprocess_image(x), y),
    num_parallel_calls=tf.data.experimental.AUTOTUNE
）

def apply_augmentation（图像，标签）：
    # 随机水平翻转
    图像 = tf.image.random_flip_left_right(图像)
    # 随机旋转
    图像 = tf.image.rot90(图像, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))
    # 随机亮度调节
    图像 = tf.image.random_brightness(图像, max_delta=0.1)
    返回图像、标签

数据集 = 数据集. 地图(
    应用增强，
    num_parallel_calls=tf.data.experimental.AUTOTUNE
）


train_size = int(0.8 * len(数据集))
val_size = int(0.1 * len(数据集))
test_size = len(数据集) - train_size - val_size

train_dataset = dataset.take(train_size)
val_dataset = dataset.skip(train_size).take(val_size)
test_dataset = dataset.skip(train_size).skip(val_size)

vgg = tf.keras.applications.VGG16(
    输入形状=（IMG_SIZE，IMG_SIZE，3），
    权重=&#39;imagenet&#39;,
    include_top=False
）


对于 vgg.layers 中的图层：
    可训练层 = False

x = 层.Flatten()(vgg.输出)
预测 = 层.Dense(3, 激活=&#39;relu&#39;)(x)

模型 = tf.keras.models.Model(输入=vgg.输入，输出=预测)
模型.summary()

模型.编译(
    损失=&#39;sparse_categorical_crossentropy&#39;,
    优化器=&#39;亚当&#39;,
    指标=[&#39;准确性&#39;]
）

历史=模型.fit(
    训练数据集，
    验证数据=val_数据集，
    纪元=10，
    steps_per_epoch=train_size，
    验证步骤=val_size
）


当我开始训练模型时出现以下错误，
2024-03-29 09:36:18.734909：W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES 在稀疏_xent_op.cc:103 处失败：INVALID_ARGUMENT：收到超出范围的标签值 3 [0, 3) 的有效范围。标签值：2 3 1 0 1 2 2 2 1 2 2 0 0 0 3 0 0 0 2 0 3 1 0 2 2 2 2 0 3 0 2 0
2024-03-29 09:36:18.734957: W tensorflow/core/framework/local_rendezvous.cc:404] 本地集合点正在中止，状态为：INVALID_ARGUMENT：收到的标签值 3 超出了 [0, 3 的有效范围）。标签值：2 3 1 0 1 2 2 2 1 2 2 0 0 0 3 0 0 0 2 0 3 1 0 2 2 2 2 0 3 0 2 0
     [[{{function_node __inference_one_step_on_data_2794}}{{节点compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]

InvalidArgumentError Traceback（最近一次调用最后一次）
输入 In [15], in ()
----&gt; 1 历史记录 = model.fit(
      2 训练数据集，
      3validation_data=val_dataset，
      4 epoch=10，
      5steps_per_epoch=train_size，
      6 验证步骤=val_size
      7）

文件 ~/anaconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122，位于filter_traceback..error_handler(*args, **kwargs)
    第119章
    120 # 要获取完整的堆栈跟踪，请调用：
    121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
    123最后：
    124 删除filtered_tb

文件〜/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53，在quick_execute（op_name，num_outputs，输入，attrs，ctx，名称）中
     51 尝试：
     52 ctx.ensure_initialized()
---&gt; 53 张量 = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54 个输入、属性、输出数）
     55 除了 core._NotOkStatusException 为 e：
     56 如果名称不是 None：

InvalidArgumentError：图形执行错误：

在节点compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits处检测到（最近一次调用最后）：
。
。
。

我检查了这个问题，但找不到答案，如何可以回复：InvalidArgumentError：图形执行错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78242076/how-i-resolve-invalidargumenterror-graph-execution-error</guid>
      <pubDate>Fri, 29 Mar 2024 05:01:02 GMT</pubDate>
    </item>
    <item>
      <title>Pycharm 调试不适用于 Tensorflow。我该如何解决？</title>
      <link>https://stackoverflow.com/questions/78241816/pycharm-debug-is-not-working-with-tensorflow-how-do-i-resolve-it</link>
      <description><![CDATA[我已成功安装以下内容：
tensorflow（最新版本2.16.1）
keras（最新版本3.1.1

我使用的是pycharm 2023.3.5（社区版）。我有一些导入的代码行，包括张量流：
&lt;前&gt;&lt;代码&gt;...
从tensorflow.keras导入后端为K
...

每当我调试代码时，都会收到如下错误：
回溯（最近一次调用最后一次）：
文件“C:\Program Files\JetBrains\PyCharm Community Edition 2023.3.5\plugins\python-ce\helpers\pydev\_pydevd_bundle\pydevd_xml.py”，第 177 行，在 _get_type 中
if isinstance(o, t[0]):
   ^^^^^^^^^^^^^^^^^^^^
文件“C:\Program Files\Python312\Lib\site-packages\tensorflow\python\platform\flags.py”，第 73 行，在 __getattribute__ 中
返回 self.__dict__[&#39;__wrapped&#39;].__getattribute__(name)
       ~~~~~~~~~~~~~^^^^^^^^^^^^^
关键错误：&#39;__wrapped&#39;

我想相信问题不是由张量流引起的，但我似乎无法弄清楚确切的问题。我已经上网但无济于事。我得到的最接近的解决方案是这个 问题，但是，它似乎我作为一个不同的问题。请这个崇高平台上的博学之士来帮助我。]]></description>
      <guid>https://stackoverflow.com/questions/78241816/pycharm-debug-is-not-working-with-tensorflow-how-do-i-resolve-it</guid>
      <pubDate>Fri, 29 Mar 2024 03:17:26 GMT</pubDate>
    </item>
    <item>
      <title>包含clip_by_value的LSTM冻结层导致android studio在部署时崩溃</title>
      <link>https://stackoverflow.com/questions/78241639/lstm-frozen-layer-containing-clip-by-value-causing-android-studio-to-crash-when</link>
      <description><![CDATA[我对 ML 很陌生，但我正在尝试将冻结的 LSTM 模型部署到 android studio 中，但我收到此错误，该错误涉及在 lstm 层中使用的 Clipbyvalue 操作，而我没有明确声明该操作。
java.lang.IllegalArgumentException：没有注册 OpKernel 来支持具有这些属性的 Op &#39;ClipByValue&#39;。注册设备：[CPU]，注册内核：&lt;无注册内核&gt; [[节点：lstm_1/while/clip_by_value = ClipByValue[T=DT_FLOAT](lstm_1/while/add_2，lstm_1/while/Const，lstm_1/while/Const_1)]]
我目前使用的是tensorflow 1.8，并尝试使用更高版本来使用TFLite，但遇到了问题，这就是为什么我决定降级以尝试使用冻结图功能。
下面是我在android studio中的java代码
公共类 pbClassifier {


    静止的{
        System.loadLibrary(“tensorflow_inference”);
    }

    私有 TensorFlowInferenceInterface 推理接口；
    私有静态最终字符串MODEL_FILE =“model.pb”;
    私有静态最终字符串INPUT_NODE =“lstm_1_input”；
    private static Final String[] OUTPUT_NODES = {“输出/Softmax”};
    私有静态最终字符串 OUTPUT_NODE =“输出/Softmax”；
    私有静态最终长[] INPUT_SIZE = {1, 200, 6};
    私有静态最终 int OUTPUT_SIZE = 7;


    公共 pbClassifier（最终上下文上下文）{
        inferenceInterface = new TensorFlowInferenceInterface(context.getAssets(), MODEL_FILE);
    }

    公共浮点预测（浮点[]数据）{
        float[] 结果 = new float[OUTPUT_SIZE];
        inferenceInterface.feed(INPUT_NODE, 数据, INPUT_SIZE);
        inferenceInterface.run(OUTPUT_NODES);
        inferenceInterface.fetch(OUTPUT_NODE, 结果);
        返回结果[1]；
    }


}

这是用于模型训练和导出的 python 代码
&lt;前&gt;&lt;代码&gt;模型 = 顺序()
# RNN层
model.add(LSTM(128, input_shape = (200, 6), return_sequences = True, kernel_regularizer = l2(0.000001), name = &#39;lstm_1&#39;))
# 对每个时间序列单独应用密集操作
model.add(TimeDistributed(Dense(64,activation=&#39;relu&#39;), name=&#39;time_distributed&#39;))
# 压平图层
model.add(Flatten(name=&#39;flatten&#39;))
# 使用 ReLu 的密集层
model.add（密集（64，激活=&#39;relu&#39;，名称=&#39;dense_1&#39;））
# Softmax层
model.add(Dense(2, 激活 = &#39;softmax&#39;, name=&#39;输出&#39;))

# 编译模型
model.compile（损失=&#39;sparse_categorical_crossentropy&#39;，优化器= Adam（），指标= [&#39;准确性&#39;]）

从 keras.callbacks 导入 ModelCheckpoint

回调 = [ModelCheckpoint(&#39;model.h5&#39;, save_weights_only=False, save_best_only=True, verbose=1)]

历史= model.fit（X_train，y_train，epochs = 10，validation_split = 0.20，batch_size = 128，
                    详细 = 1，回调 = 回调）

从 keras 导入后端为 k
从tensorflow.python.tools导入freeze_graph，optimize_for_inference_lib

input_node_name = [&#39;lstm_1_input&#39;]
output_node_name = &#39;输出/Softmax&#39;
模型名称 = &#39;坠落模型&#39;

tf.train.write_graph(k.get_session().graph_def, &#39;models&#39;, model_name + &#39;_graph.pbtxt&#39;)
保存程序 = tf.train.Saver()
saver.save(k.get_session(), &#39;models/&#39; + model_name + &#39;.chkp&#39;)

freeze_graph.freeze_graph(&#39;models/&#39; +model_name + &#39;_graph.pbtxt&#39;, None, False, &#39;models/&#39; +model_name+&#39;.chkp&#39;,
                          output_node_name, &#39;save/restore_all&#39;, &#39;save/Const:0&#39;, &#39;models/frozen_&#39;+model_name+&#39;.pb&#39;,
                          确实，“”）

如果有任何关于如何通过阻止图层使用clip_by_value函数或可能将其更改为clip_norm函数以查看是否有效来解决此问题的建议，我将不胜感激。
这也是来自 Netron 的图像
非常感谢]]></description>
      <guid>https://stackoverflow.com/questions/78241639/lstm-frozen-layer-containing-clip-by-value-causing-android-studio-to-crash-when</guid>
      <pubDate>Fri, 29 Mar 2024 01:56:04 GMT</pubDate>
    </item>
    <item>
      <title>在 Databricks AutoML 运行的最佳试用笔记本的“加载数据”部分中访问 df_loaded 和/或 run_id</title>
      <link>https://stackoverflow.com/questions/78241331/access-df-loaded-and-or-run-id-in-load-data-section-of-best-trial-notebook-of-da</link>
      <description><![CDATA[下面的代码块是通过执行 Databricks AutoML 运行自动生成的最佳试用笔记本的一部分。
导入mlflow
导入操作系统
导入uuid
进口舒蒂尔
将 pandas 导入为 pd

# 创建临时目录以从 MLflow 下载输入数据
input_temp_dir = os.path.join(os.environ[&quot;SPARK_LOCAL_DIRS&quot;], &quot;tmp&quot;, str(uuid.uuid4())[:8])
os.makedirs(input_temp_dir)


# 下载工件并将其读入 pandas DataFrame
input_data_path = mlflow.artifacts.download_artifacts（run_id =“e2a4a93aafb24aa9956e83f6b7ab3e28”，artifact_path =“数据”，dst_path = input_temp_dir）

df_loaded = pd.read_parquet(os.path.join(input_data_path, “training_data”))
# 删除临时数据
Shutil.rmtree(input_temp_dir)

# 预览数据
df_loaded.head(5)


上面代码块中的 run_id e2a4a93aafb24aa9956e83f6b7ab3e28，我可以从运行 automl.regress 返回的 AutoMLSummary 中获取它吗？如果我使用summary.best_trial.mlflow_run_id，我会得到不同的值。那么这个 run_id 是什么以及如何获取它？

除了上面的代码块之外，还有没有办法获取已加载到 df_loaded 中的数据集？它本质上是我输入到 automl.regress 中的输入数据集，只不过它有一列指示每一行是否是训练、验证和测试子集的一部分。


我对 Databricks AutoML 相当陌生，因此不确定完成此任务的最佳方法是什么。
提前致谢。
正如我所提到的，我尝试从summary.best_trial.mlflow_run_id 中获取run_id，但值不匹配。我尝试阅读 automl 和 mlflow 的文档，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78241331/access-df-loaded-and-or-run-id-in-load-data-section-of-best-trial-notebook-of-da</guid>
      <pubDate>Thu, 28 Mar 2024 23:32:11 GMT</pubDate>
    </item>
    <item>
      <title>如何提高回归模型中的 R2 分数？预测房价</title>
      <link>https://stackoverflow.com/questions/78241166/how-can-i-improve-r2-score-in-my-regression-model-predicting-house-prices</link>
      <description><![CDATA[我已经在房屋定价数据集上训练了一些数据。
我得到的 R-2 分数还不错，接近 0.5，如下所示：

我想问如何提高 R-2 分数并获得接近实际价格的更精确预测。我还带来了价格和实际价格如下：
 价格
0 5250000 7.904547e+06
1 3950000 5.272666e+06
2 9500000 1.541611e+07
3 8000000 1.135316e+07
4 12750000 9.812656e+06
.. ... ...
99 7000000 9.222798e+06
100 6750000 7.002278e+06
101 6500000 8.844441e+06
102 6500000 7.946185e+06
103 5275000 1.005468e+07

[104 行 x 2 列]

在预测它们的价格时，我的想法是模型中的某些地区的行为与其他具有特征的地区不同。所以我想到了：也许我可以为每个地区写一个新的方程，优化每个地区每个特征的系数，然后对我写的所有 104 个方程取平均值。这样我就可以最小化yhat和实际价格之间的差异。我想知道机器学习中是否有一种方法！？
&lt;预&gt;&lt;代码&gt;[-1720170.96599959 -450112.9969811 -241807.76731698 7269.75674741
    66318.19738872 1220655.10520284 881134.39040993 2901100.72147558
  1256062.85997242 1831204.62088706 -707473.49663603 2799885.41237361
   797886.35464379]-10796920.246326108
Karlibayir Mh. 的编码值：district0=0、district1=5、district2=0
为您想要的值：Karlibayir Mh. 90 2 2 5 1 1 1 1 3 3 预测价格为：
预计价格：10660621.44001897

正如您在上面看到的，在我的代码中，我从用户那里获取一些参数来预测用户的价格。地区、所需房屋的平方米、建筑年龄、公寓楼层、所需建筑的层数、浴室数量、有电梯、有停车场、是否有陡峭的小巷、公寓使用的材料质量、附近的豪华程度
我还没有在数据集上实现随机森林、XG boosting 和其他模型。最后我想说的是，令人惊讶的是，当我进行 Kfolding 时，我得到的 R-2 分数更少！我不知道为什么。
感谢您的回复并提前抽出时间。
顺便说一句，这是我的相关热图：
]]></description>
      <guid>https://stackoverflow.com/questions/78241166/how-can-i-improve-r2-score-in-my-regression-model-predicting-house-prices</guid>
      <pubDate>Thu, 28 Mar 2024 22:33:50 GMT</pubDate>
    </item>
    <item>
      <title>Meta在视频流平台上的无缝通信模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78240811/metas-seamless-communication-model-on-video-streaming-platform</link>
      <description><![CDATA[我想知道如何在youtube或其他平台等视频流媒体平台上使用meta的无缝通信模型？有谁知道怎么做吗？
目前我可以使用存储的音频/视频在本地计算机上运行它。我不知道如何在流媒体平台上运行它。]]></description>
      <guid>https://stackoverflow.com/questions/78240811/metas-seamless-communication-model-on-video-streaming-platform</guid>
      <pubDate>Thu, 28 Mar 2024 20:51:05 GMT</pubDate>
    </item>
    <item>
      <title>使用 MAPIE 进行保形预测，当 alpha 很大时，我得到空的预测集</title>
      <link>https://stackoverflow.com/questions/78240714/using-mapie-for-conformal-predictions-i-get-empty-predictions-sets-when-alpha-is</link>
      <description><![CDATA[我正在使用 MAPIE Python 库进行保形预测。在 MapieClassifier 中，我选择 method=&#39;score&#39;。当 alpha 很小时（1% 到 5% 之间），我得到非空预测集。然而，当我查看从 1% 到 99% 的整个 alpha 范围（例如 alphas = np.arange(0.01,1.00,0.01)）时，预测集中的平均类数逐渐下降为零，尽管 C. Molnar 的书 Python 保形预测简介 (2023) 第 27 页指出 « 用于多类任务的预测集是一组一个或多个类。 »，即它是目标模式集的非空子集。我在两个分类数据集上使用了默认的 XGBoost 模型，并得到了带有大 alpha 的空预测集的现象，其中之一是 Dry Bean 数据集。
这是预测集的平均基数作为 alpha 函数的图表。

据我们观察，平均值最终会低于 1。
我还尝试了 Molnar 书中第 25-28 页的代码，该代码没有明确使用 MAPIE，但进行了类似的计算，并得到了相同的结果：当 alpha 变大时，预测集最终为空。正如 Molnar 书中第 27 页所述，我期望得到非空预测集。我观察到，当 alpha 变大时，q_level 会下降，这反过来又会使 q_hat 变小。当 q_level 在 0.7 到 0.9 之间时，作为 q_level 函数的 q_hat 似乎下降得很快（见下图）。

问题：

随着 alpha 变大，预测集会变空，这是可以预料到的吗？
我是否应该将 method=&#39;score&#39; 更改为其他方法来获取非空预测集？
我缺少关于 MAPIE 的良好实践吗？也许人们应该避免使用大阿尔法，或使用方法 score 或其他方法。直觉上，在我看来，我应该总是得到一个非空的预测集。
]]></description>
      <guid>https://stackoverflow.com/questions/78240714/using-mapie-for-conformal-predictions-i-get-empty-predictions-sets-when-alpha-is</guid>
      <pubDate>Thu, 28 Mar 2024 20:28:12 GMT</pubDate>
    </item>
    <item>
      <title>难以理解如何使用机器学习数据集中的字符串数据列表 - 在进行预测之前扩展的功能</title>
      <link>https://stackoverflow.com/questions/78240648/trouble-understanding-how-to-use-list-of-string-data-in-a-machine-learning-datas</link>
      <description><![CDATA[我已经花了几个周末试图解决这个问题，我一直在观看一些教程并阅读，但我仍然缺少数据集实际可行的关键部分。我一直在努力寻找有关实际数据集创建的任何资源。所有教程似乎都只是使用即插即用数据集，没有任何数据背后的推理。
我一直在尝试创建一个预测模型来预测 Google Colab + Tensorflow、Pandas 和 Numpy 中的足球赛事。例如，在此数据集上，这是为了预测玩家是否会在游戏中投篮。
我最初将所有这些都采用嵌套 JSON 格式。我首先使用 Pandas json_normalize() 方法进行了尝试。
我遇到的问题是，当涉及到实际预测时，模型的输入大约是 20,000 个特征或类似的高值。
所以我尝试扁平化我的结构，并使所有内容尽可能通用。不过，我仍然在为同样的事情而苦苦挣扎，因为这些功能的规模正在爆炸式增长。所以我认为我的完整数据集有 153 个不同的列，包括结果列。
存在多种类型的数据
例如：

&lt;标题&gt;

列名称
类型
示例数据
注释


&lt;正文&gt;

PlayerTeam_HomeOrAway
字符串
首页
价值观是“在家”或“离开”。可以转换为数字/布尔值


玩家团队游戏分析
数量
5



玩家团队平均投篮次数
十进制
9.6



位置
字符串
固件
不同的球员位置，FW、DC、MC等


gamesWithShots_game_1_sub
布尔值
错误



PlayerStyles_Strengths_Strong
列表
传球、持球、空中决斗
根据相关玩家的不同，长度可以不同。



所以我认为问题出在 PlayerStyles_Strengths_Strong 等列上。我已经设法将嵌套结构分解为单个值。我有很多这样的专栏，内容涉及弱点、团队优势等。
但是我不明白如何在 CSV 文件中构造这些数据。我希望将其视为单个记录，但它似乎是每个新记录的“热编码”。我在这里可能完全错了，这只是我迄今为止的初步研究。这就是为什么当我尝试使用相同的数据集结构运行预测时，它告诉我功能不匹配。
我不确定这是否是我需要直接使用 Pandas、Tensorflow 做的事情，或者是否是 CSV 结构问题。
我的第一个解决方案想法是为每种类型的力量等添加一列。然后，如果玩家/团队具有该特征，则将 1 / 0 分配到该字段中。我会将其写入将 JSON 转换为 CSV 的 Python 脚本中。在我经历这个费力的过程之前，我想我应该尝试一下，看看是否有一些明显的我遗漏的东西，我的人工智能建模知识正如上面提到的，是我从 YouTube、Udemy 和一些网站上拼凑起来的。 Medium + GeekForGeek 文章。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78240648/trouble-understanding-how-to-use-list-of-string-data-in-a-machine-learning-datas</guid>
      <pubDate>Thu, 28 Mar 2024 20:11:40 GMT</pubDate>
    </item>
    <item>
      <title>集成学习[关闭]</title>
      <link>https://stackoverflow.com/questions/78240346/ensemble-learning</link>
      <description><![CDATA[我有许多不同的数据（图像、时间序列等），我将分别使用它们来创建模型，并且我找到了以下代码作为示例。如果我使用这样的算法，我会做正确的事情吗？或者还有其他方法吗？
将 numpy 导入为 np
从 sklearn.ensemble 导入 VotingClassifier
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.metrics 导入 precision_score

# 图像数据训练模型
图像模型类：
    def __init__(自身):
        # 此处必须包含经过训练的图像模型
        经过

    def 预测（自身，X）：
        # 例如，我们在这里进行随机猜测
        返回 np.random.randint(0, 2, size=len(X))

# 时间序列数据的训练模型
类时间序列模型：
    def __init__(自身):
        # 这里需要涉及一个经过训练的时间序列模型
        经过

    def 预测（自身，X）：
        # 例如，我们在这里进行随机猜测
        返回 np.random.randint(0, 2, size=len(X))

# 数据加载和准备（使用的示例数据）
X_image = np.random.rand(100, 10) # 样本图像数据（100个样本，每个样本10个）
X_time_series = np.random.rand(100, 20) # 样本时间序列数据（100个样本，每个20个特征）
y = np.random.randint(0, 2, size=100) # 随机标签

# 创建模型
图像模型=图像模型()
time_series_model = TimeSeriesModel()

# 创建集成学习模型
ensemble_model = VotingClassifier(估计器=[
    （&#39;图像模型&#39;，图像模型），
    (&#39;时间系列模型&#39;, 时间系列模型)
]，投票=&#39;硬&#39;）

# 训练模型（这里应该使用用真实数据训练的模型）
ensemble_model.fit([X_image, X_time_series], y)

# 进行预测（这里应该用真实数据进行预测）
y_pred = ensemble_model.predict([X_image, X_time_series])

# 评估模型的成功
准确度=准确度_分数（y，y_pred）
print(&quot;集成学习模型的准确率得分：&quot;, 准确度)


基于这个示例代码，我有很多不同的数据，我如何用它们创建模型，这就是我想问的。我在上面找到了一个示例代码，但我不能确定它的准确性。它在使用Fit方法的同时结合了特征。不知道这个说法对不对。]]></description>
      <guid>https://stackoverflow.com/questions/78240346/ensemble-learning</guid>
      <pubDate>Thu, 28 Mar 2024 19:00:25 GMT</pubDate>
    </item>
    <item>
      <title>如何输入 4 个值（“开盘价”、“最高价”、“最低价”、“总交易量”）来建模并预测未来 x 天的相同 4 个值？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78240071/how-to-input-4-values-open-price-high-price-low-price-total-traded-qu</link>
      <description><![CDATA[我正在开发一个 ML 模型，该模型应该输入并预测以下股票数据：
&#39;开盘价&#39;、&#39;最高价&#39;、&#39;最低价&#39;、&#39;总交易量&#39;
我认为我的代码有问题。如何正确预测未来 x 天的情况？
运行代码的先决条件：
!git 克隆 https://github.com/NSEDownload/NSEDownload
#安装NSEDownload库
!pip3 install NSEDownload/dist/*

以下是我的代码：
from sklearn.model_selection import TimeSeriesSplit
从 sklearn.preprocessing 导入 MinMaxScaler
将 pandas 导入为 pd
将 numpy 导入为 np
从 keras.models 导入顺序
从 keras.layers 导入 LSTM，密集
将 matplotlib.pyplot 导入为 plt
将 matplotlib.dates 导入为 mdates
从 NSE 下载进口库存

股票名称 = &#39;TCS&#39;

# 获取股票数据
df = stocks.get_data(stock_symbol=stock_name, full_data=True)
df.index = pd.to_datetime(df.index)
df.to_csv(f&#39;{stock_name}.csv&#39;)

# 使用过去的天数
n_天 = 10

print(&quot;数据框形状（行、列）：&quot;, df.shape)
print(“是否存在空值？”, df.isnull().values.any())

# 设置目标变量
Training_df = pd.DataFrame(df[&#39;最后价格&#39;])
#选择特征
features = [&#39;开盘价&#39;, &#39;最高价&#39;, &#39;最低价&#39;, &#39;总交易量&#39;]

# 缩放（标准化）
缩放器 = MinMaxScaler()
Training_df_transform = scaler.fit_transform(df[特征])
Training_df_transform= pd.DataFrame(列=特征，数据=training_df_transform，索引=df.index)

时间分割= TimeSeriesSplit(n_splits=10)
对于 timesplit.split(df.index) 中的 train_index、test_index：
    X_train, X_test = Training_df_transform.iloc[:len(train_index)], Training_df_transform.iloc[len(train_index): (len(train_index) + len(test_index))]
    y_train, y_test = Training_df.iloc[:len(train_index)].values.ravel(), Training_df.iloc[len(train_index): (len(train_index) + len(test_index))].values.ravel()

# 创建长度为 n_days 的序列
X_train = np.array([X_train[i : i + n_days] for i in range(len(X_train) - n_days)])
X_test = np.array([X_test[i : i + n_days] for i in range(len(X_test) - n_days)])

# 调整 y_train 和 y_test 以匹配新的 X_train 和 X_test
y_train = y_train[n_days:]
y_test = y_test[n_days:]

# 重塑 X_train 和 X_test 以匹配模型期望的形状
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))

# 定义并训练模型
模型=顺序（）
model.add(LSTM(32，input_shape=(n_days，X_train.shape[2])，activation=&#39;relu&#39;，return_sequences=False))
model.add(Dense(4)) # 更改此设置以匹配输出特征的数量
model.compile(loss=&#39;mean_squared_error&#39;, 优化器=&#39;adam&#39;)
model.fit（X_train，y_train，epochs = 50，batch_size = 8，verbose = 1，shuffle = False）

＃ 预测
y_pred = model.predict(X_test)
# 绘制代码
plt.plot(df.index[-len(y_test):], y_test, label=&#39;实际收盘价&#39;)
plt.plot(df.index[-len(y_test):], y_pred[:, 0], label=&#39;预测收盘价&#39;)
# 设置 x 轴的主要定位器和格式化程序
plt.gca().xaxis.set_major_locator(mdates.MonthLocator(间隔=2))
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %Y&#39;))
plt.title(&#39;测试模型预测&#39; + stock_name)
plt.xlabel(&#39;月份&#39;)
plt.ylabel(&#39;股票价格（卢比）&#39;)
plt.图例()
# 旋转 x 轴标签以获得更好的可读性
plt.xticks（旋转=45）
plt.tight_layout()
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78240071/how-to-input-4-values-open-price-high-price-low-price-total-traded-qu</guid>
      <pubDate>Thu, 28 Mar 2024 17:51:09 GMT</pubDate>
    </item>
    <item>
      <title>如何在Python中加载非常大的时间序列文件进行分析？</title>
      <link>https://stackoverflow.com/questions/78240015/how-to-load-very-big-timeseries-files-in-python-to-do-analysis</link>
      <description><![CDATA[我有一些 .gz 文件，它们包含一些时间序列的数据。当然，我想对此做一些时间序列分析。
我尝试过这个：
导入gzip f=gzip.open(&#39;data.csv.gz&#39;,&#39;r&#39;) file_content=f.read() print(file_content)
但是它加载了 20 分钟，我手动停止了它。
我的问题是，我应该如何阅读这个？我对使用 Dask、Spark 有一些想法，还是应该直接放弃这些行？
尝试查找互联网行业标准。]]></description>
      <guid>https://stackoverflow.com/questions/78240015/how-to-load-very-big-timeseries-files-in-python-to-do-analysis</guid>
      <pubDate>Thu, 28 Mar 2024 17:38:55 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中手动对某一层的输出进行反量化并为下一层重新量化？</title>
      <link>https://stackoverflow.com/questions/78239906/how-to-manually-dequantize-the-output-of-a-layer-and-requantize-it-for-the-next</link>
      <description><![CDATA[我正在开展学校项目，该项目要求我对模型的每一层执行手动量化。具体来说，我想手动实现：
&lt;块引用&gt;
量化激活，结合量化权重A-A层-
量化输出 - 反量化输出 - 重新量化输出，组合
量化权重 B - 层 B - ...

我知道Pytorch已经有量化函数，但该函数仅限于int8。我想从bit = 16到bit = 2进行量化，然后比较它们的准确性。
我遇到的问题是，量化后，某个层的输出变大了多个数量级（bit = 16），并且我不知道如何将其反量化回来。我正在使用相同的激活和权重的最小值和最大值来执行量化。这是一个例子：
激活 = [1,2,3,4]
权重 = [5,6,7,8]
激活和权重的最小值和最大值 = 1, 8
预期非量化输出 = 70

量化位 = 16
量化激活 = [-32768, -23406, -14044, -4681]
量化权重 = [4681, 14043, 23405, 32767]
量化输出 = -964159613
反量化输出，最小值 = 1，最大值 = 8 = -102980

这个计算对我来说很有意义，因为输出涉及激活值和权重的相乘，它们的幅度增加也相乘。如果我用原始的最小值和最大值执行一次反量化，那么有一个更大的输出是合理的。
Pytorch 如何处理反量化？我尝试定位Pytorch的量化，但是找不到。如何对输出进行反量化？]]></description>
      <guid>https://stackoverflow.com/questions/78239906/how-to-manually-dequantize-the-output-of-a-layer-and-requantize-it-for-the-next</guid>
      <pubDate>Thu, 28 Mar 2024 17:17:53 GMT</pubDate>
    </item>
    <item>
      <title>空间特征可以用作机器学习模型的输入吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78228843/can-spatial-features-be-used-as-an-input-for-a-machine-learning-model</link>
      <description><![CDATA[我的数据集包含 Oracle 地理数据库中的线和要素网络。我还有一个数据集，其中包含用户根据此数据放置的点。
目标是提供 2 个输入，即资产​​和周围网络，并预测需要放置点的位置。
我一直在寻找这种机器学习的例子。但只能找到图像识别和解释的例子。我最接近的是用于预测停车场形状的形状文件掩码的示例。在那里，他们使用了粗糙的掩模和卫星图像，结果是预测停车场的新多边形形状。但这不是我想要的。
我此时看到的解决方案是创建网络的光栅图像并将其用作输入，但我确信这是一种迂回的方法，这意味着生成的模型需要一个图像作为还有一个输入。
预先感谢您的指导。]]></description>
      <guid>https://stackoverflow.com/questions/78228843/can-spatial-features-be-used-as-an-input-for-a-machine-learning-model</guid>
      <pubDate>Wed, 27 Mar 2024 00:54:36 GMT</pubDate>
    </item>
    <item>
      <title>CatBoostRegressor 与 loss_function='Lq'</title>
      <link>https://stackoverflow.com/questions/76498616/catboostregressor-with-loss-function-lq</link>
      <description><![CDATA[我不知道如何指定“q” “Lq”中的变量损失函数。我收到以下错误消息：
CatBoostError：/src/catboost/catboost/private/libs/options/catboost_options.cpp:82：参数 q 对于 Lq 丢失是必需的

我的代码如下：
from catboost import CatBoostRegressor
从 sklearn.datasets 导入 make_regression
从 sklearn.model_selection 导入 train_test_split
将 numpy 导入为 np

# 生成人工回归数据集
X, y = make_regression(n_samples=1000, n_features=10, random_state=42)

# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建一个 CatBoostRegressor 对象
模型 = CatBoostRegressor(loss_function=&#39;Lq&#39;)

# 拟合模型
model.fit(X_train, y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/76498616/catboostregressor-with-loss-function-lq</guid>
      <pubDate>Sun, 18 Jun 2023 00:20:48 GMT</pubDate>
    </item>
    </channel>
</rss>