<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 29 Jan 2024 21:12:07 GMT</lastBuildDate>
    <item>
      <title>为什么带有 Logistic 回归的递归特征消除 (RFE) 会在二元分类中选择意外的特征？</title>
      <link>https://stackoverflow.com/questions/77902343/why-is-recursive-feature-elimination-rfe-with-logistic-regression-selecting-un</link>
      <description><![CDATA[我正在处理一个包含 94 个样本的数据集，每个样本属于 5 个组中的一组，并且可以通过 610 个特征进行描述。对于这五个组中的每一组，我想确定能够最好地表征该组的最小特征数。为此，我为每个组设置了一个二元分类问题，运行逻辑回归，然后使用调整后的逻辑回归模型运行 RFE，以获得提供最大平衡精度的最小数量的特征。然而，当我检查 RFE 认为哪些功能对于特定组而言是重要的时，我发现 RFE 正在选择所有组共有的或感兴趣组中不存在的功能作为对该组来说重要的功能。我本以为 RFE 会挑选出该组感兴趣的最独特的功能，虽然每个组都有一些独特的功能，但 RFE 显然不会将它们选择为前 10-30 个功能。对选定数量的特征运行 RFE 时，准确率仍然很高 (97.8%)。我是否误解了 RFE 的要点，或者我的代码有问题？我在运行之前记录了数据转换。我使用此处提供的示例代码中的 RFE： https://machinelearningmastery.com/ rfe-feature-selection-in-python/
我尝试了提供的示例代码，但 RFE 提供了奇怪的结果。]]></description>
      <guid>https://stackoverflow.com/questions/77902343/why-is-recursive-feature-elimination-rfe-with-logistic-regression-selecting-un</guid>
      <pubDate>Mon, 29 Jan 2024 20:05:10 GMT</pubDate>
    </item>
    <item>
      <title>哪些文本情感分析模型具有超过 3 个通用输出？</title>
      <link>https://stackoverflow.com/questions/77902073/which-text-sentiment-analysis-models-have-more-than-the-3-generic-outputs</link>
      <description><![CDATA[我是一名 UG EE 学生，正在攻读我的五年计划。我的未来规划涉及多模式情感分析。
我试图实现我的 fyp 的音频特定部分（其中涉及提取音频，然后提取梅尔频率系数，绘制这些系数，还将音频转换为语音，最后输出情感）。
我遇到了一个问题，所有用于文本情感分析的 python 库都有只输出 3（负面、正面、中性）的函数。
在我的年度计划中，我想要更多的变化（愤怒、快乐、惊讶、无聊、中立等）
我的 fyp 的视觉特定部分有 7 种情绪。
无论如何，我还必须在本五年计划结束时应用多模态融合，但现在我的问题是是否有一个函数或模型可以输出多种情绪？（不是负面、正面、中性）]]></description>
      <guid>https://stackoverflow.com/questions/77902073/which-text-sentiment-analysis-models-have-more-than-the-3-generic-outputs</guid>
      <pubDate>Mon, 29 Jan 2024 19:11:19 GMT</pubDate>
    </item>
    <item>
      <title>这个神经网络预测能这么好吗？</title>
      <link>https://stackoverflow.com/questions/77902001/can-this-neural-network-prediction-be-so-good</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77902001/can-this-neural-network-prediction-be-so-good</guid>
      <pubDate>Mon, 29 Jan 2024 18:56:49 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 模型无法推广到随机数据集</title>
      <link>https://stackoverflow.com/questions/77901965/xgboost-model-cannot-be-generalized-to-a-random-dataset</link>
      <description><![CDATA[我目前正在研究回归问题。我通过从表中选择 30000 条记录形成了一个数据集，并使用 XGBoost Regressor 进行了训练。数据集按大约 60%、20% 和 20% 的比例分为训练、测试和验证。训练后（使用训练数据和测试数据），模型给出的样本外 R2 分数为 0.92[根据验证数据计算的 R2 分数]。但是，如果从父数据存储中选择不同的 30000 行来形成新数据集，我会看到 r2 分数为负数。很明显，我的模型可能过度拟合数据；但是我很想知道如何克服这一挑战？
我尝试了超参数优化和交叉验证。事实上，我用深度学习模型进行了测试，也遇到了同样的问题。我感觉我的特征工程需要改进，但不确定如何确定。另外，如果我遗漏了任何其他行动项目，请详细说明。]]></description>
      <guid>https://stackoverflow.com/questions/77901965/xgboost-model-cannot-be-generalized-to-a-random-dataset</guid>
      <pubDate>Mon, 29 Jan 2024 18:46:33 GMT</pubDate>
    </item>
    <item>
      <title>GPU 内存不足 - Tensorflow 对象检测 API</title>
      <link>https://stackoverflow.com/questions/77901877/gpu-out-of-memory-tensorflow-object-detection-api</link>
      <description><![CDATA[我目前正在使用“Tensorflow 对象检测 API”框架训练 EfficientDetD7 来检测图像中的对象。
我使用的是带有 80 GB VRAM 的 NVIDIA H100 GPU。
我只能使用等于或小于 2 的批量大小进行训练。经过一些测试，我意识到批量中的每个图像占用 33 Gb。因此，批量大小等于 1 消耗 33 Gb，批量大小等于 2 消耗 66 Gb。
有什么办法可以减少内存消耗吗？我已经在使用 Tensorflow 的内存增长。]]></description>
      <guid>https://stackoverflow.com/questions/77901877/gpu-out-of-memory-tensorflow-object-detection-api</guid>
      <pubDate>Mon, 29 Jan 2024 18:28:41 GMT</pubDate>
    </item>
    <item>
      <title>SHAP 图表难以阅读的问题</title>
      <link>https://stackoverflow.com/questions/77901671/the-problem-of-having-a-hard-to-read-shap-chart</link>
      <description><![CDATA[我为具有六个输入和两个输出、1000 个观测值的数据集绘制了 SHAP 图表。我还选择了瀑布型 SHAP 图表。
我遇到的问题是这些点出现在彼此的顶部（垂直），如附图所示。 在此处输入图片描述
所以我发现很难解释输入对输出的影响。
这是我的代码：
&lt;前&gt;&lt;代码&gt;数据集 =
x = df.iloc[:, 0:6]
y = df.iloc[:, [6, 7]]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
rf_model = RandomForestRegressor()
rf_pipeline = 管道([
    (&#39;缩放器&#39;, StandardScaler()),
    （&#39;回归&#39;，rf_model）
]）
rf_pipeline.fit(x_train, y_train)

解释器 = shap.TreeExplainer(rf_model)
shap_values = 解释器.shap_values(x_test)

shap.summary_plot(shap_values[0], x_test)
plt.title(&#39;输出 1 的 SHAP 值&#39;)

shap.summary_plot(shap_values[1], x_test)
plt.title(&#39;输出 2 的 SHAP 值&#39;)

问题很可能与数据的范围有关，因为绘制图表时，x 轴从大约 -350 到 30 开始。当我缩小范围时，第一个变量的点（位于顶部）消失。
你们以前遇到过这样的问题吗？您是如何解决的？]]></description>
      <guid>https://stackoverflow.com/questions/77901671/the-problem-of-having-a-hard-to-read-shap-chart</guid>
      <pubDate>Mon, 29 Jan 2024 17:50:19 GMT</pubDate>
    </item>
    <item>
      <title>在 librosa 中重新调整音频时面临问题</title>
      <link>https://stackoverflow.com/questions/77901612/facing-problem-in-reshampling-audio-in-librosa</link>
      <description><![CDATA[我正在尝试使用我的数据集微调 wev2vec2 模型。因此我加载了音频。现在想将它们下采样到 16khz。但 librosa.reshape 函数给出了我无法解决的错误。错误是“resample() 需要 1 个位置参数，但给出了 3 个”
首先我尝试使用 sr 16khz 的 librosa 加载它。但由于我在这方面的经验较少。因此，我在项目的后期部分遇到了问题。我找到了一个以不同方式完成的代码。所以。我尝试使用他的方法，但现在面临问题。
这部分工作正常
数据库={}
音频 = []
PSR = []
对于 df[&#39;audio&#39;] 中的路径：
  Speech_array,sr = torchaudio.load(路径)
  audios.append(speech_array[0].numpy())
  psr.追加(sr)
数据库[&#39;音频&#39;] = 音频
数据库[&#39;psr&#39;] = psr

每个索引都出现错误
导入librosa
将 numpy 导入为 np

# 假设“database”是包含“audio”和“psr”列的 DataFrame

# 存储新采样率的列表
新_sr = []

# 对每个音频信号重新采样并存储新的采样率
对于范围内的 i(len(database[&#39;psr&#39;]))：
    尝试：
        audio_signal = np.asarray(database[&#39;audio&#39;][i]) # 将音频转换为 numpy 数组
        Original_sr = database[&#39;psr&#39;][i] # 原始采样率

        # 检查音频信号是否为单声道（单声道）
        如果audio_signal.ndim == 1：
            # 对单声道音频信号重新采样
            resampled_audio = librosa.resample(audio_signal,original_sr,16000)
        别的：
            # 对多通道音频的每个通道分别重新采样
            重新采样通道 = []
            对于 audio_signal 中的通道：
                resampled_channel = librosa.resample（通道，original_sr，16000）
                resampled_channels.append(resampled_channel)
            resampled_audio = np.array(resampled_channels)

        # 将重新采样的音频存储回 DataFrame 中
        数据库[&#39;音频&#39;][i] = resampled_audio

        # 存储新的采样率（16000 Hz）
        new_sr.append(16000)
    除了异常 e：
        print(f“处理索引 {i} 处的音频时出错：{e}”)

# 向 DataFrame 添加新的采样率
数据库[&#39;newsr&#39;] = new_sr
]]></description>
      <guid>https://stackoverflow.com/questions/77901612/facing-problem-in-reshampling-audio-in-librosa</guid>
      <pubDate>Mon, 29 Jan 2024 17:37:05 GMT</pubDate>
    </item>
    <item>
      <title>无法在 celery 工作线程中使用 YOLO 加载和预测图像</title>
      <link>https://stackoverflow.com/questions/77901605/not-able-to-load-and-predict-on-images-using-yolo-inside-celery-worker</link>
      <description><![CDATA[这是我的代码
导入系统
从 Question_Detection.inference_object_detection.check_model 导入 verify_model
从记录器导入记录器
从 ultralytics 导入 YOLO


类 ModelNotFoundError（异常）：
    经过


def load_model():
    尝试：
        成功，model_path = verify_model()
        logger.info(“模型加载开始”)
        如果没有成功：
            raise ModelNotFoundError(&quot;未找到模型！&quot;)
        logger.info(“123”)
        记录器.info（模型路径）
        _模型 = YOLO(模型路径)
        logger.info(“456”)
        如果 _model 为 None：
            raise ModelNotFoundError(“模型未加载！”)
        logger.info(“789”)
        返回 _model, _model.names
    除了 ModelNotFoundError 为 e：
        记录器.错误(e)
        sys.exit(1) # 以非零退出代码退出系统以指示错误
    除了异常 e：
        记录器.错误(e)
        返回无，无



def 推理（图像）：
    ”“”
    图像推理
    :param images: 可以是单个图像、图像列表或目录
    :return: 结果列表
    ”“”
    logger.info(“开始图像推理！”)
    模型，类别 = load_model()
    如果模型为无：
        raise ModelNotFoundError(“模型未加载！”)

    res = 模型（图像），类别
    logger.info(&quot;图像推理完成！&quot;)
    返回资源

如果我正常调用推理，它就像黄油一样工作
但是，当同一个函数被另一个 celery 工作函数调用时，这不起作用。
它甚至不会抛出任何错误。
调试时，model_path之后没有记录任何内容，基本上没有到达456行。
任何人都可以帮助调试为什么我无法加载模型。
正如您可能在代码中看到的那样，它使用 YOLO]]></description>
      <guid>https://stackoverflow.com/questions/77901605/not-able-to-load-and-predict-on-images-using-yolo-inside-celery-worker</guid>
      <pubDate>Mon, 29 Jan 2024 17:35:56 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 wandb 的 pytorch 框架在 Kaggle 中解决这个问题？</title>
      <link>https://stackoverflow.com/questions/77901559/how-to-solve-this-issue-in-kaggle-with-pytorch-framework-for-wandb</link>
      <description><![CDATA[ # 构建模型
    模型 = 构建模型()

    # 初始化运行
    运行 = wandb.init(entity = &#39;scortinhas&#39;,
                     项目 = &#39;mnist-教程&#39;,
                     配置=CFG，
                     保存代码=真，
                     #组=&#39;安&#39;,
                     #job_type = &#39;火车&#39;
    ）

我正在使用 Kaggle 的教程测试 W 和 B，每次我遇到这个问题并且运行确切的问题时我都无法解决它。这是下面的错误 -


我尝试升级wandb并重新启动内核，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77901559/how-to-solve-this-issue-in-kaggle-with-pytorch-framework-for-wandb</guid>
      <pubDate>Mon, 29 Jan 2024 17:28:38 GMT</pubDate>
    </item>
    <item>
      <title>如何通过 AI/ML 分析日志</title>
      <link>https://stackoverflow.com/questions/77900640/how-to-analyze-logs-via-ai-ml</link>
      <description><![CDATA[我正在做一个小项目，尝试分析日志并在出现错误时提供根本原因和解决问题的建议。
我有一个 Databricks 作业不断运行以提供日志。我想实时分析这些日志，如果发现异常，实时提供建议。
由于这些日志是非结构化数据，我在想是否可以使用 NLP 来解决这个问题。基本了解日志的上下文并提供建议。
我曾使用机器学习模型对结构化数据进行异常检测，但这是我第一次使用非结构化数据。
我需要以下方面的帮助

如何/在哪里存储可训练的日志？我遇到了矢量数据库，但不确定它是否有帮助。
如何在 llama、gpt 模型之上训练这些基于服务的日志？由于这些模型已经拥有“世俗知识”，因此它们应该能够提供 70% 的建议。但 Databricks 作业给出了一些特定于我的服务的日志，我认为这些日志需要培训。
鉴于有现成的训练模型，如何向模型提供实时日志以使其理解上下文？

由于我是人工智能工作的新手，任何建议/程序/文档/博客/包/Azure 服务都会有很大帮助！！]]></description>
      <guid>https://stackoverflow.com/questions/77900640/how-to-analyze-logs-via-ai-ml</guid>
      <pubDate>Mon, 29 Jan 2024 15:02:18 GMT</pubDate>
    </item>
    <item>
      <title>部署在 Streamlit Coumminity Cloud 上的机器学习模型给出了错误的预测</title>
      <link>https://stackoverflow.com/questions/77899985/machine-learning-model-after-deployed-on-streamlit-coumminity-cloud-is-giving-wr</link>
      <description><![CDATA[我创建了一个垃圾邮件预测机器学习模型，为了创建一个界面，我去了 pycharm 并编写了 app.py 代码，还有另外两个文件 &lt; app.py 文件中使用了 code&gt;vectorizer.pkl 和 model.pkl，在终端上我编写了 streamlit run app.py，它把我带到了本地主机 8501 浏览器，现在一切都很顺利，做出了正确的预测。我想公开这个应用程序以在 GitHub 上分享链接。我在 streamlit 社区云上创建了一个帐户，将其与我的 GitHub 链接起来，并部署了该应用程序。 GitHub 存储库包含所有必要的文件，包括带有整个 ml 模型、app.py、model.pkl,requirements.txt 的笔记本， 向量化器.pkl。现在的问题是，使用公共链接部署的应用程序对于完全相同的输入给出了不同的预测（即说“不是垃圾邮件”而不是“垃圾邮件”）。我将输出附在此处，请告诉我该怎么做。
单击这些链接可查看附加的图像：


app.py 文件的 Github 链接：https:// github.com/ardra1111/Spam-Gaurd/blob/main/app.py
Streamlit 云应用程序的链接：https://spam-gaurd-ir5t9kbcsjpdkus4cveywj.streamlit.app/ 
当我第一次在 streamlit.io 上部署该应用程序时，它运行良好。几周前，但由于不活动，链接不起作用，所以我删除了该应用程序，并重新开始将其部署为新应用程序，我不明白下一步应该做什么来解决此问题？
我第一次尝试使用渲染将其部署在那里，并且遇到了同样的问题，请帮我解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77899985/machine-learning-model-after-deployed-on-streamlit-coumminity-cloud-is-giving-wr</guid>
      <pubDate>Mon, 29 Jan 2024 13:14:02 GMT</pubDate>
    </item>
    <item>
      <title>在 Colab 上访问 TensorFlow Open Image open_images_v4/200k 时出现问题；继续下载原始数据集</title>
      <link>https://stackoverflow.com/questions/77896924/trouble-access-to-tensorflow-open-image-open-images-v4-200k-on-colab-keep-downl</link>
      <description><![CDATA[所以我尝试在 Google Colab 上导入 open_images_v4/200k 数据集，但它不断下载超过 500 GiB 的原始文件，我没有足够的存储空间，也不需要使用那么大的数据集.
我尝试过使用 ds, info = tfds.load(&#39;open_images_v4/200k&#39;, with_info=True) 进行基本导入
然后开始下载，预计大小为 565.11 GiB。第二次我得到了明显的错误，我没有足够的存储空间。
我只想使用 60.70 GiB v4/200k。
请帮忙！]]></description>
      <guid>https://stackoverflow.com/questions/77896924/trouble-access-to-tensorflow-open-image-open-images-v4-200k-on-colab-keep-downl</guid>
      <pubDate>Mon, 29 Jan 2024 00:35:37 GMT</pubDate>
    </item>
    <item>
      <title>让谢尔曼-莫里森更新更加高效</title>
      <link>https://stackoverflow.com/questions/77884077/make-sherman-morrison-update-more-efficient</link>
      <description><![CDATA[我需要计算 CIFAR10 数据集子集的点上的参数梯度的协方差矩阵。为此，我有以下代码：
from torch.func import function_call, vmap, grad

model1 = LogisticModel().to(设备)

def loss_fn（预测，目标）：
  损失 = nn.CrossEntropyLoss()
  回波损耗（预测、目标）

defcompute_loss（参数，缓冲区，样本，目标）：
  批次=样本.unsqueeze(0)
  目标 = target.unsqueeze(0)

  预测 = function_call(model1, (params, buffers), (batch,))
  损失= loss_fn（预测，目标）
  回波损耗

ft_compute_grad = grad(compute_loss)
ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(无, 无, 0, 0))

def sherman_morrison_update(A, u, v):
  vT = v.T
  金=A@u

  α = 1/(1 + vT@Au)
  A = A - alpha*torch.outer(Au, vT@A)
  返回A

testloader1 = DataLoader（test_dataset，batch_size = 512）
params = {k: v.detach() for k, v in model1.named_pa​​rameters()}
buffers = {k: v.detach() for k, v in model1.named_buffers()}
w = 0

p_covs = {p:torch.eye(q.flatten().shape[0]).to(device) for p,q in param_grads.items()}
param_grad_mean = {p:torch.zeros(q​​.flatten().shape[0]).to(device) for p,q in param_grads.items()}

对于 tqdm(testloader1) 中的 x,y：
  param_grads = ft_compute_sample_grad(参数、缓冲区、x.to(设备)、y.to(设备))
  对于 p，q，zip 中的平均值（param_grads.values（），p_covs，param_grad_mean）：
    对于 p 中的 p_grad：
      w += 1
      diff = p_grad.flatten() - param_grad_mean[平均值]
      param_grad_mean[平均值] += diff / w
      p_covs[q] = sherman_morrison_update(A=p_covs[q], u=diff, v= diff)

现在，这是非常低效的，并且在每次迭代中执行此操作都非常耗时。
此外，我们无法真正同时获取所有点的参数梯度，因为这会导致内存问题（因此我转向 Sherman-Morrison）。
有没有办法提高效率？谢尔曼-莫里森的更好实施？还有什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/77884077/make-sherman-morrison-update-more-efficient</guid>
      <pubDate>Fri, 26 Jan 2024 03:37:37 GMT</pubDate>
    </item>
    <item>
      <title>识别 SMOTE 生成的合成样本</title>
      <link>https://stackoverflow.com/questions/77870847/identify-the-synthetic-samples-generated-by-smote</link>
      <description><![CDATA[我有一个带标签的数据集，X 形状为 7000 x 2400，y 形状为 7000。数据严重不平衡，因此我尝试使用 SMOTE 生成合成样本。不过，我想确定 SMOTE 实际生成的合成样本。
作为示例，下面是一个代码片段：
将 pandas 导入为 pd
将 numpy 导入为 np
从 sklearn.datasets 导入 load_iris
从 imblearn.over_sampling 导入 SMOTE

虹膜 = load_iris()

X = 虹膜[&#39;数据&#39;]
y = 虹膜[&#39;目标&#39;]

#数据是平衡的，所以我故意去掉了一些样本
X = X[:125,::]
y = y[:125]

过采样 = SMOTE()
X_smt, y_smt = oversample.fit_resample(X, y)

数组 X_smt 和 y_smt 既有原始样本又有合成样本。是否有一种简单的方法可以通过索引或其他机制来识别合成样本？]]></description>
      <guid>https://stackoverflow.com/questions/77870847/identify-the-synthetic-samples-generated-by-smote</guid>
      <pubDate>Wed, 24 Jan 2024 06:04:18 GMT</pubDate>
    </item>
    <item>
      <title>Flower - 联邦学习的每一轮模型精度都是相同的[关闭]</title>
      <link>https://stackoverflow.com/questions/77855250/model-accuracy-is-the-same-after-every-round-with-flower-federated-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77855250/model-accuracy-is-the-same-after-every-round-with-flower-federated-learning</guid>
      <pubDate>Sun, 21 Jan 2024 14:33:27 GMT</pubDate>
    </item>
    </channel>
</rss>