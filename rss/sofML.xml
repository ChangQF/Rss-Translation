<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 18 Dec 2023 21:11:57 GMT</lastBuildDate>
    <item>
      <title>KeyError：“[Index(['names_of_categorical_columns'], dtype='object', name='name_of_index_column')] 都不在 [index] 中”</title>
      <link>https://stackoverflow.com/questions/77681457/keyerror-none-of-indexnames-of-categorical-columns-dtype-object-name</link>
      <description><![CDATA[我编写这段代码是为了获取一个变量中的所有分类列，并使用 OrdinalEncoder() 对它们进行编码。
我的代码：
s = (X_train.dtypes == &#39;对象&#39;)
object_cols = 列表(s[s].index)

ordinal_encoder = OrdinalEncoder(handle_unknown = &#39;use_encoded_value&#39;,unknown_value = 999)

# 进行复制以避免更改原始数据
label_X_train = X_train.copy()
label_X_valid = X_valid.copy()

# 将序数编码器应用于具有分类数据的每一列
label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])
label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])

我收到此错误（在问题标题中）并且无法处理它。您能告诉我如何解决这个问题吗？
我尝试使用 OrdinalEncoder() 对分类数据进行编码，然后再使用它来训练 XGBRegressor() 模型。]]></description>
      <guid>https://stackoverflow.com/questions/77681457/keyerror-none-of-indexnames-of-categorical-columns-dtype-object-name</guid>
      <pubDate>Mon, 18 Dec 2023 19:59:08 GMT</pubDate>
    </item>
    <item>
      <title>precisionAtRecall 从 0 开始</title>
      <link>https://stackoverflow.com/questions/77681250/precisionatrecall-starts-with-0</link>
      <description><![CDATA[在使用张量流进行模型训练期间，我目睹了以下指标更新：
在此处输入图像描述
两个 precisionAtRecall 值分别对应召回值 0.8 和 0.7。我不明白为什么 1) precisionAtRecall_0.8 = 0 而 precisionAtRecall_0.7 &gt; 0？ 2) 为什么 precisionAtRecall_0.8 = 0 在 0 和看似合理的数字之间振荡？
我期望 precisionAtRecall_0.8 &lt; precisionAtRecall_0.7 但如果 precisionAtRecall_0.7 &gt; 非零0]]></description>
      <guid>https://stackoverflow.com/questions/77681250/precisionatrecall-starts-with-0</guid>
      <pubDate>Mon, 18 Dec 2023 19:09:02 GMT</pubDate>
    </item>
    <item>
      <title>微调朴素贝叶斯模型以进行文本分类（多类别结果）</title>
      <link>https://stackoverflow.com/questions/77681240/fine-tuning-naive-bayesian-model-for-text-classification-multi-categorical-outc</link>
      <description><![CDATA[我有一个包含数千个有关美国经济问题的 Reddit 帖子的数据集。
我们随机抽取了总帖子数 40% 的样本，其中每个帖子都包含“责备”内容。具有三个可能值“共和党”、“民主党”的指示符。或“美联储”它捕获了给定帖子中每个事件中谁受到指责。
我的目标是使用 40% 的人工注释帖子来预测“责备”的价值。我的数据集中剩余 60% 的帖子中存在变量。我看过一些研究论文使用朴素贝叶斯模型来预测分类变量，但我的案例的结果并不令人印象深刻。因此，我想知道这里使用的 BERT 模型是否更合适，或者我是否错误地运行了贝叶斯模型。
这是我的代码：
#加载包
图书馆（tidyverse）
需要（读xl）
需要（writexl）
图书馆（量子）
库（quanteda.textmodels）
库（插入符号）

#数据示例：
dput(reddit_corpus[1:5,c(1,2,3,4,8)])


输出：
结构(列表(id = c(1933, 7161, 4661, 2885, 5102), 用户名 = c(“the_dog”,
&quot;Empyrean Cobalt&quot;、&quot;Engineer&quot;、&quot;AuraKUPO&quot;、&quot;kyo_465&quot;), post = c(&quot;认为 jhk AT Pinoy 等都应该感谢 cecas 为他们减轻了压力&quot;,
“ScamDetector说我希望你们都意识到UOB实际上没有回答Clement的问题点击展开UOB只是使用相同的模板像机器人模式一样回复”，
“netzach 说这个 Daniels 的家伙点击展开我认为人们并没有完全理解 WP 和 Daniel Goh”，
“TS仍然是学生，公司必须以现金支付FT CPF，只有sinkies需要从工资中支付一部分CPF”，
“ponpokku 说至少他们不会像 CECA 那样降级，对当地人不会构成威胁 CECA ish 你必须加倍工作才能 kio 他们的 sai 单击展开我不想雇用那些告诉我有比他们更糟糕的人”
), date = c(&quot;2021-07-13 00:00:00 UTC&quot;, &quot;2020-09-22 00:00:00 UTC&quot;,
“2021-08-07 00:00:00 UTC”、“2021-04-07 00:00:00 UTC”、“2021-07-23 00:00:00 UTC”
), Collective_action = c(0, 0, 0, 0, 0)), row.names = c(NA, -5L
), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;))

构建模型：reddit_corpus
#生成 1109 个数字（2913 的 38%），无需替换
设置种子(300)
id_train &lt;- 样本(1:2913, 1109, 替换 = FALSE)
头（id_train，10）

#获取训练集
dfmat_training &lt;- dfm_subset(dfmt_sing, id_numeric %in% id_train)

#获取测试集（id_train中没有的文档）
dfmat_test &lt;- dfm_subset(dfmt_sing, !id_numeric %in% id_train)

#使用textmodel_nb()训练朴素贝叶斯分类器
tmod_nb &lt;- textmodel_nb(dfmat_training, dfmat_training$responsibility_attribution)
摘要(tmod_nb)

dfmat_matched &lt;- dfm_match(dfmat_test, features = featnames(dfmat_training))

结果：
#检查分类器的工作情况
实际类 &lt;- dfmat_matched$responsibility_attribution
Predicted_class &lt;- 预测(tmod_nb, newdata = dfmat_matched)
tab_class &lt;- 表（实际类，预测类）
选项卡类

输出：
 预测类
实际类 1 2 3 4
           1 240 6 5 45
           2 67 2 1 7
           3 79 0 10 15
           4 128 6 5 75

混淆矩阵
#我们可以使用caret包中的函数confusionMatrix()来评估分类的性能
fusionMatrix(tab_class, mode = “一切”, Positive = “pos”)

总体统计
                                          
               准确度：0.4732
                 95% 置信区间：(0.4355, 0.5112)

                     类别：1 类别：2 类别：3 类别：4
灵敏度 0.4669 0.142857 0.47619 0.5282
特异性 0.6836 0.889217 0.85970 0.7468
预测值 0.8108 0.025974 0.09615 0.3505
负预测值 0.3063 0.980456 0.98126 0.8595
精度 0.8108 0.025974 0.09615 0.3505
召回率 0.4669 0.142857 0.47619 0.5282
F1 0.5926 0.043956 0.16000 0.4213
患病率 0.7438 0.020260 0.03039 0.2055
检出率 0.3473 0.002894 0.01447 0.1085
检测率 0.4284 0.111433 0.15051 0.3097
平衡精度 0.5753 0.516037 0.66795 0.6375
]]></description>
      <guid>https://stackoverflow.com/questions/77681240/fine-tuning-naive-bayesian-model-for-text-classification-multi-categorical-outc</guid>
      <pubDate>Mon, 18 Dec 2023 19:07:27 GMT</pubDate>
    </item>
    <item>
      <title>解释 AutoGluon 表格分类器的文本特征 [关闭]</title>
      <link>https://stackoverflow.com/questions/77680786/explaining-text-features-of-autogluon-tabular-classifier</link>
      <description><![CDATA[我正在训练 AutoGluon 表格分类器。
我的所有数据都是表格格式，并且大多数特征都是数字。然而，训练中使用的一些特征是文本特征。是否可以解释文本特征如何影响预测？例如，我感兴趣的是是否有任何特定的单词会导致预测发生这样或那样的变化。我尝试使用 SHAP 的 KernelExplainer，但我不确定这是否是正确的方法或如何在这种情况下使用它。
我最初使用 AutoGluon 的 TabularPredictor.feature_importance，但这只能告诉我哪些特征在最终预测中权重最大。然而，权重最高的特征是文本特征。我无法找出 AutoGluon 在该功能中具体查看什么内容来进行预测，无论是特定单词还是其他单词。]]></description>
      <guid>https://stackoverflow.com/questions/77680786/explaining-text-features-of-autogluon-tabular-classifier</guid>
      <pubDate>Mon, 18 Dec 2023 17:28:53 GMT</pubDate>
    </item>
    <item>
      <title>如何使用自定义数据集格式训练自定义 Transformer 模型</title>
      <link>https://stackoverflow.com/questions/77680618/how-to-train-a-customized-transformer-model-with-custom-dataset-formatting</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77680618/how-to-train-a-customized-transformer-model-with-custom-dataset-formatting</guid>
      <pubDate>Mon, 18 Dec 2023 16:56:58 GMT</pubDate>
    </item>
    <item>
      <title>加权机器学习集成给出的结果很差[关闭]</title>
      <link>https://stackoverflow.com/questions/77680479/weighted-machine-learning-ensemble-giving-poor-results</link>
      <description><![CDATA[我正在为基于文本的 csv 文件情感分析实施加权集成模式。我尝试过根据准确度、精确度、召回率进行权重分配，但它给我的结果与单个 ML 或深度学习模型相似或更差 🤔
我已经制作了大约 23 个具有不同组合的模型
。
Python
输入是基于文本的推文
输出为3类分类
词嵌入是tfidf
我怎样才能提高性能以及我应该使用什么权重指标？重量度量的方程应该是..]]></description>
      <guid>https://stackoverflow.com/questions/77680479/weighted-machine-learning-ensemble-giving-poor-results</guid>
      <pubDate>Mon, 18 Dec 2023 16:33:10 GMT</pubDate>
    </item>
    <item>
      <title>如何将 OHLCV 交易数据加载到 TensorflowJS 卷积层中？</title>
      <link>https://stackoverflow.com/questions/77680420/how-to-load-ohlcv-trading-data-into-tensorflowjs-convolution-layer</link>
      <description><![CDATA[我的目标
获取本周前 5 个股票交易日的高位数据，看看我是否可以预测周末价格上涨或下跌。
数据
在交易中，我们使用 OHLCV 数据来描述交易日的情况：
（O=当日开盘价，H=当日最高价，L=当日最低价，C=当日收盘价，V=当日成交量）。
//1天的数据
[o、h、l、c、v]

// 例如
[10、12、9、11、100]

因此 5 天的数据如下：
//5天的数据
[[o、h、l、c、v]、[o、h、l、c、v]、[o、h、l、c、v]、[o、h、l、c、v]、[ o、h、l、c、v]]

// 例如
[[10, 12, 9, 11, 100], [11, 13, 11, 12, 200], [12, 12, 11, 11, 150], [11, 14, 11, 14, 300], [ 14, 14, 11, 12, 200]]

我读到，处理这些数据的最佳方法是使用卷积层，特别是 Tensorflow 中的 conv2d 层。据我了解，该层需要 4dTensor 作为输入。因此，我使用tensor4d()从包含4周训练数据（4倍5天数据）的数据数组创建这样的张量
// 4 次 5 天的 javascript 数组
常量数据 = [10, 12, 9, 11, 100, 11, 13, 11, 12, 200, 12, 12, 11, 11, 150, 11, 14, 11, 14, 300, 14, 14, 11, 12, 200, 10, 12, 9, 11, 100, 11, 13, 11, 12, 200, 12, 12, 11, 11, 150, 11, 14, 11, 14, 300, 14, 14, 11, 12, 200, 10, 12, 9, 11, 100, 11, 13, 11, 12, 200, 12, 12, 11, 11, 150, 11, 14, 11, 14, 300, 14, 14, 11, 12, 200, 10, 12, 9, 11, 100, 11, 13, 11, 12, 200, 12, 12, 11, 11, 150, 11, 14, 11, 14, 300, 14, 14, 11, 12, 200]

// 转换为 4d 张量
const input_tensor = tf.tensor4d(data, [data.length / (5 * 5), 5, 5, 1]);

目标由每周 1 个结果组成（周末交易的结果）。
const target = [-1, 2, -3, 1] // % 盈利或亏损

// 转换为一维张量
const target_tensor = tf.tensor1d(目标);

模型
我的模型看起来像这样，是根据此处找到的示例构建的：https:// blog.quantinsti.com/卷积神经网络/
const model = tf.sequential();
model.add(tf.layers.conv2d({

    激活：&#39;relu&#39;，
    过滤器：32，
    inputShape: [5, 5, 1], // 5 个 ohlcv，共 5 个值，每个值 1
    内核大小：1，
}));
model.add(tf.layers.maxPooling2d([2, 2]));
model.add(tf.layers.conv2d({

    激活：&#39;relu&#39;，
    过滤器：64，
    内核大小：1，
}));
model.add(tf.layers.maxPooling2d([2, 2]));
model.add(tf.layers.flatten());
model.add(tf.layers.dense({

    激活：&#39;relu&#39;，
    单位：64，
}));
model.add(tf.layers.dense({

    单位：1，
    激活：&#39;relu&#39;，
}));

我使用之前创建的张量训练模型
//训练模型
等待 model.fit(input_tensor, target_tensor, {
                    
    批量大小：32，
    纪元：50，
    随机播放：正确，
});

错误
在此步骤中，我收到以下错误
unhandledRejection: TypeError: 无法将 undefined 或 null 转换为对象

最后一步
model.predict([10, 12, 9, 11, 100], [5, 1])

问题

我使用卷积层是因为我想保留 1 天的 OHLCV 数据之间的关系，我的模型是否属于这种情况？有必要吗？
我正确创建了 4d 和 1d 张量吗？
内核大小为 [1, 1] 的卷积层是否有意义？似乎违背了卷积层的目的？
如何修复该错误？
我是走在正确的道路上还是完全看错了方向？在这种情况下，您能否为我指明如何构建模型的正确方向？
]]></description>
      <guid>https://stackoverflow.com/questions/77680420/how-to-load-ohlcv-trading-data-into-tensorflowjs-convolution-layer</guid>
      <pubDate>Mon, 18 Dec 2023 16:22:48 GMT</pubDate>
    </item>
    <item>
      <title>在手动交叉验证和 cross_val_score 之间获取不同的分数值</title>
      <link>https://stackoverflow.com/questions/77680320/getting-different-score-values-between-manual-cross-validation-and-cross-val-sco</link>
      <description><![CDATA[我创建了一个 python for 循环，将训练数据集分割成分层的 KFold，并在循环内使用分类器来训练它。然后使用经过训练的模型通过验证数据进行预测。使用此过程实现的指标与使用 cross_val_score 函数实现的指标完全不同。我期望使用这两种方法得到相同的结果。
此代码用于文本分类，我使用 TF-IDF 对文本进行矢量化
这是代码：
手动实现交叉验证的代码：
#导入指标函数来衡量模型的性能
从 sklearn.metrics 导入 f1_score、accuracy_score、 precision_score、recall_score
从 sklearn.model_selection 导入 StratifiedKFold
data_validation = [] # 用于存储使用交叉验证的模型验证结果的列表
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
准确度值 = []
f1_val = []

# 使用ravel函数将多维数组展平为一维
对于 (skf.split(X_train, y_train)) 中的 train_index、val_index：
    X_tr, X_val = X_train.ravel()[train_index], X_train.ravel()[val_index]
    y_tr, y_val = y_train.ravel()[train_index] , y_train.ravel()[val_index]
    tfidf=TfidfVectorizer()
    X_tr_vec_tfidf = tfidf.fit_transform(X_tr) # 对训练折叠进行向量化
    X_val_vec_tfidf = tfidf.transform(X_val) # 对验证折叠进行向量化
    #实例化模型
    模型= MultinomialNB(alpha=0.5, fit_prior=False)
    #用我们的训练数据集训练空模型
    model.fit(X_tr_vec_tfidf, y_tr)
    Predictions_val = model.predict(X_val_vec_tfidf) # 使用验证数据集进行预测
    acc_val = 准确度_分数（y_val，预测_val）
    Accuracy_val.append(acc_val)
    f_val = f1_score（y_val，预测值）
    f1_val.append(f_val)

avg_accuracy_val = np.mean(accuracy_val)
avg_f1_val = np.mean(f1_val)

# 存储指标的临时列表
temp = [&#39;NaiveBayes&#39;]
temp.append(avg_accuracy_val) #验证准确率分数
temp.append(avg_f1_val) #验证f1分数
data_validation.append(临时)
#使用数据帧创建一个表，其中包含所有经过训练和测试的 ML 模型的指标
result = pd.DataFrame(data_validation, columns = [&#39;算法&#39;,&#39;准确度分数：验证&#39;,&#39;F1-分数：验证&#39;])
result.reset_index(drop=True, inplace=True)
结果

输出：
 算法准确度分数：验证 F1-Score：验证
0 天真的贝叶斯 0.77012 0.733994

现在使用 cross_val_score 函数的代码：
从 sklearn.model_selection 导入 cross_val_score
从 sklearn.feature_extraction.text 导入 CountVectorizer、TfidfVectorizer
分数 = [&#39;准确度&#39;, &#39;f1&#39;]
#使用NLP技术TF-IDF对训练和测试数据集进行文本向量化
tfidf=TfidfVectorizer()
X_tr_vec_tfidf = tfidf.fit_transform(X_train)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
nb=多项式NB(alpha=0.5, fit_prior=False)
对于 [“accuracy”, “f1”] 中的分数：
    print (f&#39;{score}: {cross_val_score(nb,X_tr_vec_tfidf,y_train,cv=skf,scoring=score).mean()} &#39;)

输出：
准确度：0.7341283583255231
f1：0.7062017090972422

可以看出，使用这两种方法的准确性和 f1 指标有很大不同。当我使用 KNeighborsClassfier 时，指标的差异更加严重。]]></description>
      <guid>https://stackoverflow.com/questions/77680320/getting-different-score-values-between-manual-cross-validation-and-cross-val-sco</guid>
      <pubDate>Mon, 18 Dec 2023 16:05:36 GMT</pubDate>
    </item>
    <item>
      <title>处理 CNN 二元分类的分布外样本和异常</title>
      <link>https://stackoverflow.com/questions/77679785/handling-out-of-distribution-samples-and-anomalies-for-cnn-binary-classification</link>
      <description><![CDATA[我对机器学习领域比较陌生，并且对创建基本自定义模型有基本的了解。
分配给我的任务
我的经理要求我开发一种机器学习模型，能够在攻击性图像发送到服务器之前识别它们。这些图像可以分为不同的类别，例如武器或成人内容。
问题
但是，我在模型检测异常或分布外样本的能力方面遇到了问题。
我尝试过的
我尝试了不同的方法，利用&#39;binary_crossentropy&#39;作为我的损失函数，但遇到了同样的问题。我还构建了一个包含两个类的模型：

12,278 与武器相关的图片 class_name = 武器
3,000 张食物 商品图片 class_name = 其他

但是，我不确定 “其他” 类别中应包含哪些内容。使用 MobileNet 作为基础模型构建模型后，它成功地准确预测了武器，并提供了 0.4 到 0.6 范围内的食品预测，这似乎是可以接受的。然而，当我引入大象或汽车的图像时，模型倾向于将它们预测为武器，显示的置信度接近 1。
我不知道如何解决这个问题。我尝试研究这个问题，并发现了一些关于分布外检测的线索以及与异常或离群值相关的概念。当模型的输入包含不属于训练数据的图像时，如何获得不确定性值？
如果您能提供任何指导、建议，甚至参考能够有效解决此问题的视频或资源，我将不胜感激。感谢您的帮助。
数据加载：
train_ds, test_ds = keras.utils.image_dataset_from_directory(
    目录=“/内容/武器”，
    标签=“推断”，
    label_mode =“二进制”，
    批量大小=32，
    子集=“两者”，
    图像大小=(224,224),
    验证分割=0.2，
    随机播放=真，
    种子=1337
）

现在标准化输入图像数据：
def process（图像，标签）：
  图像=tf.cast(图像/255, tf.float32)
  返回图像、标签
train_ds = train_ds.map(进程)
test_ds = test_ds.map(进程)

创建 CNN 模型：
input_shape = (224,224,3)
mobilenet = MobileNet(input_shape,weights=&#39;imagenet&#39;,include_top=False)
模型=顺序（）
 
model.add（移动网络）
模型.add(压平())
model.add（密集（256，激活=&#39;relu&#39;））
模型.add(Dropout(0.5))
model.add（密集（1，激活=&#39;sigmoid&#39;））
模型.summary()



sgd = SGD(学习率=0.0001，动量=0.9，nesterov=True)
model.compile（损失=&#39;binary_crossentropy&#39;，优化器=sgd，指标=[&#39;准确性&#39;]）
历史= model.fit（train_ds，validation_data = test_ds，batch_size = 4，epochs = 6）


纪元 6/6
382/382 [==============================] - 58s 150ms/步 - 损失：0.0042 - 精度：0.9984 - val_loss ：0.0063 - val_accuracy：0.997
]]></description>
      <guid>https://stackoverflow.com/questions/77679785/handling-out-of-distribution-samples-and-anomalies-for-cnn-binary-classification</guid>
      <pubDate>Mon, 18 Dec 2023 14:34:38 GMT</pubDate>
    </item>
    <item>
      <title>运行 fmri 深度学习模型时出现错误</title>
      <link>https://stackoverflow.com/questions/77677389/getting-error-while-running-fmri-deep-learning-model</link>
      <description><![CDATA[我正在处理 fmri 数据，我有两组数据集疾病和正常数据的形状是正常数据形状：(91, 109, 91, 1200)
疾病数据形状：(91, 109, 91, 210)
我写了python脚本
将 numpy 导入为 np
将 nibabel 导入为 nib
从 sklearn.model_selection 导入 train_test_split


# 定义正常和疾病数据文件夹的路径
疾病数据路径 = glob(&#39;/media/aish/Backup Plus1/ABIDE/scan_data001/**/**/**/**/**/**/**/**/swa*&#39;)
Normal_data_path = glob(&#39;/media/aish/rs2/hcp/s*&#39;)

# 加载正常和疾病数据
正常数据 = []
疾病数据 = []
对于normal_data_path[0:400]中的文件：
    Normal_data.append(nib.load(file).get_data())
对于疾病数据路径中的文件：
    疾病数据.append(nib.load(文件).get_data())

print(“正常数据形状：”, normal_data[1].shape)
print(“疾病数据形状：”,疾病数据[1].shape)

# 创建标签
标签 = np.concatenate((np.zeros(len(normal_data)), np.ones(len(disease_data))))

# 将数据分为训练集和测试集
X_train，X_test，y_train，y_test = train_test_split（np.array（正常数据+疾病数据），标签，test_size = 0.2，random_state = 42）

但是我收到错误
完整错误是
ValueError Traceback（最近一次调用最后一次）
[7]，第 32 行中的单元格
     29 个标签 = np.concatenate((np.zeros(len(normal_data)), np.ones(len(disease_data))))
     31 # 将数据分为训练集和测试集
---&gt; 32 X_train，X_test，y_train，y_test = train_test_split（np.array（正常数据+疾病数据），标签，test_size = 0.2，random_state = 42）
     34 # 定义CNN模型
     35 模型 = 顺序()

ValueError：无法将输入数组从形状 (91,109,91,1200) 广播到形状 (91,109,91)

我无法解决此错误]]></description>
      <guid>https://stackoverflow.com/questions/77677389/getting-error-while-running-fmri-deep-learning-model</guid>
      <pubDate>Mon, 18 Dec 2023 06:53:58 GMT</pubDate>
    </item>
    <item>
      <title>LLM模型非常慢</title>
      <link>https://stackoverflow.com/questions/77199972/llm-model-is-very-slow</link>
      <description><![CDATA[我正在 nvidia g5 上运行 34b LLM 模型。 8xlarge 实例（1 个 Nvidia A10G GPU、24GB GPU RAM、32 个 vCPU、128GB RAM）
这是推理代码
从变压器导入 AutoTokenizer,LlamaForCausalLM, AutoConfig, AutoModelForCausalLM
从加速导入 infer_auto_device_map, init_empty_weights
进口火炬

model_path = “Phind/Phind-CodeLlama-34B-v2”

model = LlamaForCausalLM.from_pretrained(model_path, device_map=“自动”, offload_folder=“卸载”, torch_dtype=torch.float16, offload_state_dict = True)
tokenizer = AutoTokenizer.from_pretrained(model_path)

defgenerate_one_completion（提示：str）：
    tokenizer.pad_token = tokenizer.eos_token
    输入=分词器（提示，return_tensors =“pt”，截断= True，max_length = 4096）

    ＃ 产生
    generate_ids = model.generate(inputs.input_ids.to(“cuda”), max_new_tokens=384, do_sample=True, top_p=0​​.75, top_k=10, 温度=0.1)
    完成= tokenizer.batch_decode（generate_ids，skip_special_tokens = True，clean_up_tokenization_spaces = False）[0]
    完成 = 完成.replace(prompt, &quot;&quot;).split(&quot;\n\n\n&quot;)[0]

    返回完成

text = “你好，请问是你吗？”
打印（生成_一个_完成（文本））


加载检查点分片 - 这需要 4 分钟。如何才能加快速度？
即使是简单的推理也需要 60 秒以上。代码填写/提示需要 10 多分钟。可以在此 ec2 实例上加速吗？
]]></description>
      <guid>https://stackoverflow.com/questions/77199972/llm-model-is-very-slow</guid>
      <pubDate>Fri, 29 Sep 2023 06:54:14 GMT</pubDate>
    </item>
    <item>
      <title>如何使用GAN生成拉曼光谱的合成数据样本？</title>
      <link>https://stackoverflow.com/questions/76906588/how-to-generate-synthetic-data-samples-of-raman-spectroscopy-by-using-gan</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76906588/how-to-generate-synthetic-data-samples-of-raman-spectroscopy-by-using-gan</guid>
      <pubDate>Tue, 15 Aug 2023 14:05:29 GMT</pubDate>
    </item>
    <item>
      <title>无法在python中安装lap==0.4.0库</title>
      <link>https://stackoverflow.com/questions/76463707/unable-to-install-lap-0-4-0-library-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76463707/unable-to-install-lap-0-4-0-library-in-python</guid>
      <pubDate>Tue, 13 Jun 2023 09:55:26 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：X 有 1 个特征，但 LinearRegression 期望有 2 个特征作为输入</title>
      <link>https://stackoverflow.com/questions/73132252/valueerror-x-has-1-features-but-linearregression-is-expecting-2-features-as-in</link>
      <description><![CDATA[我正在使用 pywebio 为我的机器学习程序创建一个小型脚本运行用户界面。当不使用小型 UI 时，运行线性回归 predict() 函数时不会出现任何错误。
UI 正在从用户处检索两个数字：&#39;age&#39; 和 &#39;salary&#39;。这两个数字被输入到一个 numpy 数组中，并且当我收到有关 numpy 数组形状的错误时，该 numpy 数组已从一维数组重新调整为二维数组。
现在，当 sklearn 文档指出线性回归 predict() 时，我收到一条有关 predict() 方法仅接收 1 个特征而不是 2 个特征的错误消息&gt; 方法总是获取“self”和另一个特征。我该如何修复这个错误？
这是我的用户界面代码：
age = int(input(“请输入您的年龄：”, type=NUMBER))
工资 = int(input(&quot;请输入您的工资：&quot;, type=NUMBER))

条目 = np.array([年龄, 薪水])
reshape_entry = Entry.reshape(-1, 1)

估计 = regr.predict(reshape_entry)

这是错误消息：
ValueError Traceback（最近一次调用最后一次）
输入 In [21], in ()

输入[21]，在retirement_ui()中

文件 ~\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:362，在 LinearModel.predict(self, X) 中
    348 def 预测（自身，X）：
    第349章
    350 使用线性模型进行预测。
    第351章
   （...）
    360 返回预测值。
    第361章
--&gt;第362章

文件〜\ anaconda3 \ lib \ site-packages \ sklearn \ Linear_model \ _base.py：345，在LinearModel._decision_function（self，X）中
    第342章
    第343章
--&gt; [第 345 章]
    [第 346 章]

文件 ~\anaconda3\lib\site-packages\sklearn\base.py:585，在 BaseEstimator._validate_data(self, X, y, Reset, validate_separately, **check_params)
    第582章
    第584章
--&gt;第585章
    第587章 回来

文件 ~\anaconda3\lib\site-packages\sklearn\base.py:400，在 BaseEstimator._check_n_features(self, X, Reset) 中
    第397章 回归
    第399章
--&gt;正文 正文_第400章
    [401] 第401话
    [402] f“期待{self.n_features_in_}特征作为输入。”
    第403章）

ValueError：X 有 1 个特征，但 LinearRegression 期望有 2 个特征作为输入。
]]></description>
      <guid>https://stackoverflow.com/questions/73132252/valueerror-x-has-1-features-but-linearregression-is-expecting-2-features-as-in</guid>
      <pubDate>Wed, 27 Jul 2022 04:29:07 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何最佳实践来为基于文本的分类准备特征？</title>
      <link>https://stackoverflow.com/questions/22087407/is-there-any-best-practice-to-prepare-features-for-text-based-classification</link>
      <description><![CDATA[我们收到了许多来自客户的反馈和问题报告。它们是纯文本。我们正在尝试为这些文档构建一个自动分类器，以便未来反馈/问题可以自动路由到正确的支持团队。除了文本本身之外，我认为我们应该将客户资料、案例提交区域等内容纳入分类器中。我认为这可以为分类器做出更好的预测提供更多线索。
目前，所有选择用于训练的特征都是基于文本内容的。如何包含上述元特征？
添加1
我目前的做法是首先对原始文本（包括标题和正文）进行一些典型的预处理，例如删除停用词、词性标记和提取重要词。然后，我将标题和正文转换为单词列表，并以某种稀疏格式存储它们，如下所示：
&lt;块引用&gt;
实例 1：单词 1：单词 1 计数，单词 2：单词 2 计数，...
实例 2：wordX:word1 计数，wordY:word2 计数，...

对于其他非文本功能，我计划将它们添加为单词“columns”之后的新列。所以最终的实例将如下所示：
&lt;块引用&gt;
实例 1：word1:word1 计数，...，特征 X:值，特征 Y:值
]]></description>
      <guid>https://stackoverflow.com/questions/22087407/is-there-any-best-practice-to-prepare-features-for-text-based-classification</guid>
      <pubDate>Fri, 28 Feb 2014 05:57:21 GMT</pubDate>
    </item>
    </channel>
</rss>