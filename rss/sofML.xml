<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 10 Oct 2024 06:24:20 GMT</lastBuildDate>
    <item>
      <title>微调 Transformer 模型未能提高性能</title>
      <link>https://stackoverflow.com/questions/79072711/fine-tuning-transformer-model-not-improving-performance</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79072711/fine-tuning-transformer-model-not-improving-performance</guid>
      <pubDate>Thu, 10 Oct 2024 04:16:15 GMT</pubDate>
    </item>
    <item>
      <title>如何使用深度学习来解决由合成数据组成的拼图游戏？[关闭]</title>
      <link>https://stackoverflow.com/questions/79072271/how-can-i-use-deep-learning-to-solve-a-jigsaw-puzzle-composed-of-synthetic-data</link>
      <description><![CDATA[我花了一些时间研究 Python 中的拼图生成器，该生成器接收图像、行数 (M) 和列数 (N)，并将原始图像分解为 M*N 个 png 图像块输出到文件夹中。这些图像是正方形，带有不规则形状的制表符和空格，因此每个块只能放在一个位置。
接下来我想做的是创建一个拼图解算器，它可以接收这些 png 图像，提取一些关键特征并确定它们的位置。
这里是图像的示例。如果您好奇它是如何实现的，您还可以查看生成器代码。
到目前为止，这些碎片没有任何旋转，但我希望将来能够处理这个问题。由于碎片是方形的，因此无法仅使用尺寸来确定方向，因此在提取边缘进行比较时，我无法轻松缩小它们的范围。
我曾考虑使用 SIFT 进行特征检测，但图像的可见层没有重叠，因此这种方法失败了。
我是机器学习的初学者，但我想尝试通过这个项目将我的技能付诸实践。我的主要问题是我不知道从哪里开始。我遇到过制作拼图解算器的不同方法，但其中大多数都是使用拼图碎片的照片，而不是合成数据，因此形状不同。我看到的另一种方法是使用深度学习来分析图像片段，但我也不确定从哪里开始实施这种方法。]]></description>
      <guid>https://stackoverflow.com/questions/79072271/how-can-i-use-deep-learning-to-solve-a-jigsaw-puzzle-composed-of-synthetic-data</guid>
      <pubDate>Wed, 09 Oct 2024 22:50:24 GMT</pubDate>
    </item>
    <item>
      <title>当批处理大小不等于 1 时，UNet 执行过程中会出现错误</title>
      <link>https://stackoverflow.com/questions/79071235/an-error-occurs-during-the-execution-of-unet-when-the-batch-size-is-not-equal-to</link>
      <description><![CDATA[我尝试使用 DDIM 反演教程中提供的代码运行稳定扩散模型。但是，当输入的批处理大小设置为大于 1 的值（例如 32）时，我遇到以下错误：
RuntimeError：张量 a（131072）的大小必须与非单维度 1 上的张量 b（4096）的大小匹配。
看起来 131072 可能来自 32 x 4096，表明张量维度不匹配。发生错误的具体行是：
noisy_residual = self.unet(input, t.to(input.device), **denoise_kwargs).sample

这是我的代码中与反演过程相关的部分：
## 反演 (https://github.com/huggingface/diffusion-models-class/blob/main/unit4/01_ddim_inversion.ipynb)
def invert_process(self, guide_scale, input, denoise_kwargs):

pred_images = []
pred_latents = []

decrypt_kwargs = {&#39;vae&#39;: self.vae}

# 反转时间步&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
timesteps = reversed(self.scheduler.timesteps)
num_inference_steps = len(self.scheduler.timesteps)

with torch.no_grad():
for i in tqdm(range(0, num_inference_steps)):

t = timesteps[i]
self.cur_t = t.item()

# 对于稳定扩散的文本条件
if &#39;encoder_hidden_​​states&#39; in denoise_kwargs.keys():
bs = denoise_kwargs[&#39;encoder_hidden_​​states&#39;].shape[0]
input = torch.cat([input] * bs)

# 预测噪声残差
noisy_residual = self.unet(input, t.to(input.device), **denoise_kwargs).sample
noise_pred = noisy_residual

#对于稳定扩散的文本条件
if noisy_residual.shape[0] == 2:
# 执行指导
noise_pred_text, noise_pred_uncond = noisy_residual.chunk(2)
noisy_residual = noise_pred_uncond + guide_scale * (noise_pred_text - noise_pred_uncond)
input, _ = input.chunk(2)

current_t = max(0, self.cur_t - (1000//num_inference_steps)) #t
next_t = t # min(999, t.item() + (1000//num_inference_steps)) # t+1
alpha_t = self.scheduler.alphas_cumprod[current_t].to(self.device)
alpha_t_next = self.scheduler.alphas_cumprod[next_t].to(self.device)

latents = input

# 反转更新步骤（重新安排更新步骤以获得 x(t)（新潜伏）作为 x(t-1)（当前潜伏）的函数
# 向潜伏添加噪声

latents = (latents - (1-alpha_t).sqrt()*noise_pred)*(alpha_t_next.sqrt()/alpha_t.sqrt()) + (1-alpha_t_next).sqrt()*noise_pred

input = latents

pred_latents.append(latents)
pred_images.append(decode_latent(latents, **decode_kwargs))

return pred_images, pred_latents


可能导致当批量大小大于 1 时，张量大小不匹配？如何在模型中保持批量大小大于 1 的同时解决此问题？
如能提供任何帮助，我们将不胜感激！
我尝试将 t 的大小更改为形状为 (批量大小,) 的张量。
此外，我确认批量大小为 1 时模型可以正常工作。]]></description>
      <guid>https://stackoverflow.com/questions/79071235/an-error-occurs-during-the-execution-of-unet-when-the-batch-size-is-not-equal-to</guid>
      <pubDate>Wed, 09 Oct 2024 16:32:07 GMT</pubDate>
    </item>
    <item>
      <title>在小数据集上生成合成数据[关闭]</title>
      <link>https://stackoverflow.com/questions/79071218/generating-synthetic-data-on-small-dataset</link>
      <description><![CDATA[我有一个只有 5 个数据点的小数据集，包括材料成分、机械性能和物理性能。由于数据量太小，无法进行预测，我尝试生成合成数据。我使用过 GAN、VAE、高斯混合模型和 Copula 模型。其中，C-Vine Copula 模型比其他模型的结果更好。但我仍然面临问题：
*使用 C-Vine Copula，分布中存在 40% 的误差，合成数据和真实数据之间的关系中存在 7% 的误​​差。这使得数据质量不足以进行预测。
*当使用这些合成数据预测物理特性时，我得到了很好的验证分数，但在新的、看不见的数据点上得到了非常差的结果——可能是由于过度拟合或数据质量差造成的。
*我还尝试使用原始真实数据（5 个数据点）预测物理特性，但结果并不准确。
我不知道如何提高合成数据的质量，或者是否有更好的方法可以尝试进行预测。对此有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79071218/generating-synthetic-data-on-small-dataset</guid>
      <pubDate>Wed, 09 Oct 2024 16:26:17 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 RL 优化 AGV 路径规划以提高能源效率。我不明白为什么网络没有学习 [关闭]</title>
      <link>https://stackoverflow.com/questions/79069663/im-optimizing-agv-path-planning-for-energy-efficiency-using-rl-i-cant-figure</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79069663/im-optimizing-agv-path-planning-for-energy-efficiency-using-rl-i-cant-figure</guid>
      <pubDate>Wed, 09 Oct 2024 10:02:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Java Android 中实现 HDBSCAN 聚类</title>
      <link>https://stackoverflow.com/questions/79069600/how-to-implement-hdbscan-clustering-in-java-android</link>
      <description><![CDATA[我想将 HDBscan 算法实现到 Java Android 应用程序中。我正在将 C# 移植到 Java。在 C# 中，它们是使用名为 Hdbscansharp 的库完成的。我尝试在 Java 中使用 ELKI，但没有成功。原始 C# 代码是
 double avgTgtSpd = (tgt1.speed + tgt2.speed) / 2;
filteredHits.Add(new FilteredHit(avgTgtDist, avgTgtSpd, currTgtDirection)); 
aggregateSpeed += avgTgtSpd;

// HDBSCAN Clustering
double[][] twoDfilteredHits =filteredHits.Select(hit =&gt; new double[] { hit.pos 
}).ToArray(); // 将过滤后的命中结果放入 hdbscan lib 可以使用的格式中
HdbscanResult hdbscanResult = HdbscanRunner.Run(new HdbscanParameters&lt;double[]&gt;
{
DataSet = twoDfilteredHits.ToArray(),
MinPoints = 3, 
MinClusterSize = 4
DistanceFunction = new HdbscanSharp.Distance.ManhattanDistance()
});

我能够将代码移植到 java 中的“twoDfilter”，但在 HDBSCan 的实现中卡住了。
如何在 java 中获取“hdbscanResult”？]]></description>
      <guid>https://stackoverflow.com/questions/79069600/how-to-implement-hdbscan-clustering-in-java-android</guid>
      <pubDate>Wed, 09 Oct 2024 09:47:31 GMT</pubDate>
    </item>
    <item>
      <title>将 ML 模型从一个 Azure Databricks 工作区复制到另一个 Databricks 工作区</title>
      <link>https://stackoverflow.com/questions/79065062/copy-a-ml-model-from-one-azure-databricks-workspace-to-another-databricks-worksp</link>
      <description><![CDATA[我运行了以下代码以在基于 Azure Databricks 的 mlflow 中导出 ML 模型，但我似乎收到了此错误

MLflow 主机或令牌配置不正确

我无法找出问题所在。工作区的 URL 和 PAT 令牌都是正确的。
export_import 工具有很多错误。它需要 mlfow 库，但 Databricks ML Runtime 附带的是 mlflow-skinny。
import mlflow
import os
from mlflow_export_import.model.export_model import ModelExporter
from mlflow.tracking import MlflowClient

# 使用工作区 URL 设置 Databricks MLflow 跟踪 URI
mlflow.set_tracking_uri(&quot;https://adb-xxxyyymmmnnnyyy.1.azuredatabricks.net/&quot;)

# 设置两个令牌以实现兼容性
os.environ[&quot;DATABRICKS_TOKEN&quot;] = &quot;mnop6672ec8e20c7d219eb2A-3&quot;
os.environ[&quot;MLFLOW_TRACKING_TOKEN&quot;] = &quot;mnop6672ec8e20c7d219eb2A-3&quot;

# 初始化 MLflow 客户端（无需传递跟踪 URI，因为它是全局设置的）
mlflow_client = MlflowClient()

# 使用 MLflow 客户端初始化 ModelExporter
exporter = ModelExporter(mlflow_client)

# 导出模型
exporter.export_model(
model_name=&quot;Signature_Test&quot;,
output_dir=&quot;/tmp/mlflow_export/model&quot;,
stage=None, # 使用&quot;None&quot; 导出所有阶段，或指定&quot;Staging&quot; 或&quot;Production&quot;
export_metadata_tags=True
)
]]></description>
      <guid>https://stackoverflow.com/questions/79065062/copy-a-ml-model-from-one-azure-databricks-workspace-to-another-databricks-worksp</guid>
      <pubDate>Tue, 08 Oct 2024 08:39:33 GMT</pubDate>
    </item>
    <item>
      <title>无法加载 Arcface 模型</title>
      <link>https://stackoverflow.com/questions/79063234/arcface-model-cant-be-loaded</link>
      <description><![CDATA[我正在开发人脸识别系统，其中我使用 arcface 作为其算法，我从 github 下载了 arcface 模型，链接为 https://github.com/Martlgap/livefaceidapp/blob/main/model.onnx%5C。但我遇到了这个错误：
ERROR:root:发生错误：KerasTensor 不能用作 
TensorFlow 函数的输入。KerasTensor 是形状和 dtype 的符号占位符，
用于构建 Keras 函数模型或 Keras 函数。您只能
将其用作 Keras 层或 Keras 操作的输入（来自命名空间 
`keras.layers` 和 `keras.operations`）。您可能正在执行类似以下操作：

x = Input(...)
...
tf_fn(x) # 无效。

您应该做的是将 `tf_fn` 包装在一个层中：

class MyLayer(Layer):
def call(self, x):
return tf_fn(x)

x = MyLayer()(x)

我已经向 ai 或 gpt 询问了这个问题，它提到了 tensorflow 库的兼容性，但是当我在 github 上看到要求时，没有需要特定版本的 tensorflow。我也已经检查了模型是否在新的 py 文件上运行良好，并且它运行良好，我使用 model.onnx 生成嵌入，它也运行良好。
我的人脸识别脚本发生了什么，如何解决？
以防你们想知道我是如何初始化 arcface 模型的，这里是
import cv2
import os
import time
import logs
import numpy as np
import pandas as pd
from tkinter import font as tkFont
from PIL import Image, ImageTk
import torch
from arcface import ArcFace 
import psycopg2
import datetime
import face_alignment 
import onnxruntime
from retinaface import RetinaFace

# ArcFace model
arcface_model = onnxruntime.InferenceSession(r&quot;RetinaFace_ArcFace\model.onnx&quot;)

DATABASE_CONFIG = {
&#39;database&#39;: &#39;postgres&#39;, 
&#39;user&#39;: &#39;postgres&#39;, 
&#39;password&#39;: &#39;root&#39;, 
&#39;host&#39;: &#39;localhost&#39;, 
&#39;port&#39;: 5432 
}

class Face_Recognizer:
def __init__(self):
self.font = cv2.FONT_HERSHEY_SIMPLEX
# FPS
self.frame_time = 0
self.frame_start_time = 0
self.fps = 0
self.fps_show = 0
self.start_time = time.time()

# cnt for frame
self.frame_cnt = 0

# 将人脸特征保存到数据库中
self.face_features_known_list = []
# / 在数据库中保存人脸名称
self.face_name_known_list = []

# 列表保存第 N-1 帧和第 N 帧中 ROI 的质心位置
self.last_frame_face_centroid_list = None
self.current_frame_face_centroid_list = None

# 列表保存第 N-1 帧和第 N 帧中物体的名称
self.last_frame_face_name_list = [None]
self.current_frame_face_name_list = None

# 第 N-1 帧和第 N 帧中人脸的 cnt
self.last_frame_face_cnt = 0
self.current_frame_face_cnt = 0

# 保存识别时 faceX 的 e-distance
self.current_frame_face_X_e_distance_list = []

# 保存当前捕获的人脸位置和名称
self.current_frame_face_position_list = []
# 保存当前帧中人物的特征
self.current_frame_face_feature_list = []

# 上一帧和当前帧中 ROI 质心之间的 e 距离
self.last_current_frame_centroid_e_distance = 0

# 在“reclassify_interval”帧后重新分类
self.reclassify_interval_cnt = 0
self.reclassify_interval = 10

# 边界框持久性
self.bbox_history = {} 
self.bbox_persistence_frames = 12

# 连接到数据库
self.connect_to_db()

def get_face_database(self):
if os.path.exists(&quot;data/features_arcface.csv&quot;):
path_features_known_csv = &quot;data/features_arcface.csv&quot;
csv_rd = pd.read_csv(path_features_known_csv, header=None)
for i in range(csv_rd.shape[0]):
features_someone_arr = []
self.face_name_known_list.append(csv_rd.iloc[i][0])
for j in range(1, 513):
features_someone_arr.append(float(csv_rd.iloc[i][j]))
self.face_features_known_list.append(features_someone_arr)
login.info(&quot;数据库中的人脸： %d&quot;, len(self.face_features_known_list))
return 1
else:
login.warning(&quot;未找到&#39;features_arcface.csv&#39;！&quot;)
login.warning(&quot;先运行代码生成人脸特征！&quot;)
返回 0
]]></description>
      <guid>https://stackoverflow.com/questions/79063234/arcface-model-cant-be-loaded</guid>
      <pubDate>Mon, 07 Oct 2024 18:34:59 GMT</pubDate>
    </item>
    <item>
      <title>执行 3D U-net 时，每次执行都会得到截然不同的指标，有时准确率、召回率、DICE 和 IoU 的指标都会 >99.99%</title>
      <link>https://stackoverflow.com/questions/79062464/executing-a-3d-u-net-i-get-widely-different-metrics-in-each-execution-sometimes</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79062464/executing-a-3d-u-net-i-get-widely-different-metrics-in-each-execution-sometimes</guid>
      <pubDate>Mon, 07 Oct 2024 15:02:22 GMT</pubDate>
    </item>
    <item>
      <title>如何解决“未知的图像文件格式。需要 JPEG、PNG、GIF、BMP 之一”？[关闭]</title>
      <link>https://stackoverflow.com/questions/79060211/how-to-resolve-unknown-image-file-format-one-of-jpeg-png-gif-bmp-required</link>
      <description><![CDATA[我正在构建一个 U-Net 模型来检测乳腺癌，我从这里获取了数据集：https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset
尽管所有图像都是 png 格式，但在尝试训练我的模型时，会出现错误，指出我的图像格式不正确。
错误如下：
---------------------------------------------------------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
Cell In[51]，第 7 行
5 train_dataset = image_ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
6 print(image_ds.element_spec)
----&gt; 7 model_history = unet.fit(train_dataset, epochs=EPOCHS)

文件 ~\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 最后：
124 delfiltered_tb

文件 ~\anaconda3\Lib\site-packages\tensorflow\python\eager\execute.py:53，在 quick_execute(op_name, num_outputs, input, attrs, ctx, name) 中
51 尝试：
52 ctx.ensure_initialized()
---&gt; 53 张量 = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
54 输入、属性、输出)
55 除 core._NotOkStatusException 外，因为 e:
56 如果名称不为 None:

InvalidArgumentError：图形执行错误：

在节点处检测到，decode_image/DecodeImage 定义在（最近一次调用最后一次）：
&lt;堆栈跟踪不可用&gt;
传递给 MapDataset:3 转换的用户定义函数中的错误，迭代器：Iterator::Root::Prefetch::BatchV2::Shuffle::MemoryCacheImpl::Filter::ParallelMapV2：未知图像文件格式。需要 JPEG、PNG、GIF、BMP 之一。
[[{{node decrypt_image/DecodeImage}}]]
[[IteratorGetNext]] [Op:__inference_one_step_on_iterator_9520]

错误仅在尝试训练模型时发生，它引用此函数：
def preprocess_image(image, mask, target_size=(256, 256)):
try:
# 安全地解码图像和掩码
image = tf.io.decode_image(image, channels=3, expand_animations=False)
mask = tf.io.decode_image(mask, channels=1, expand_animations=False)

# 检查未定义或零维度
if image.shape is None or image.shape[0] == 0 or image.shape[1] == 0:
print(f&quot;Error: Image has undefined or zero Dimensions: {image.shape}&quot;)
return None, None

if mask.shape is None or mask.shape[0] == 0 or mask.shape[1] == 0:
print(f&quot;Error: Mask 具有未定义或零维度：{mask.shape}&quot;)
return None, None

# 确保图像恰好有 3 个通道 (RGB)
if image.shape[-1] != 3:
print(f&quot;Error: Image does not have 3 channels (found {image.shape[-1]}).&quot;)
return None, None

# 将图像标准化为范围 [0, 1]
image = tf.image.convert_image_dtype(image, tf.float32)

# 将图像和 mask 的大小调整为目标尺寸 (256*256)
image = tf.image.resize(image, target_size, method=&#39;nearest&#39;)
mask = tf.image.resize(mask, target_size, method=&#39;nearest&#39;)

#将 mask 转换为二进制（0 或 1）格式以用于分类任务
mask = tf.cast(tf.math.reduce_max(mask, axis=-1, keepdims=True) &gt; 0, tf.float32) # 确保二进制 mask

return image, mask

except Exception as e:
print(f&quot;Error during preprocessing: {str(e)}&quot;)
return None, None

# 将预处理函数应用于数据集
image_ds = dataset.map(preprocess_image)

# 过滤掉 preprocess_image 返回的 None 值
image_ds = image_ds.filter(lambda img, mask: img is not None and mask is not None)

我尝试了多种方法尝试使用 chatgpt 修复此问题，但似乎没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/79060211/how-to-resolve-unknown-image-file-format-one-of-jpeg-png-gif-bmp-required</guid>
      <pubDate>Sun, 06 Oct 2024 22:45:18 GMT</pubDate>
    </item>
    <item>
      <title>LogisticRegression 未返回正确结果</title>
      <link>https://stackoverflow.com/questions/79057824/logisticregression-not-returning-correct-results</link>
      <description><![CDATA[我试图使用基于训练数据的逻辑回归对测试数据中的一系列点进行分类预测。
我得到了一个没有错误的输出，但我被告知结果是错误的（错误的输出将被视为错误，但运行代码时没有错误）。
训练数据是 375 个点中 4 个不同类别的集合，每个点有 3 个变量，因此绘制在 3D 图形上。我在该集合上运行了一个计数程序，发现超过 50% 的点属于第 2 类。我的初步结果是所有测试点都属于第 2 类。我尝试将训练数据排序为几个不同的集合：随机选择 125 个条目（这是测试数据的大小），找到所有类别的最小数量，然后使用每个类别中相同数量的点创建一个训练集。
无排序 = 所有第 2 类
随机排序 = 所有第 2 类
等类数排序 = 给我一个答案，其中点被归类在所有 4 个类别中，但当我将它们插入在线最终测试表格时，我的准确率得分为 26%，这与随机机会相同。所以，我没有正确处理数据，我不确定在哪里。我希望有更多回归分类经验的人能为我指明正确的方向。
在调用 LogisticRegression 之前，我是否需要重新格式化（转换）train_X、train_y 和 test_X 数组？如果需要，该怎么做？也许我只是给它提供了格式错误的数据？
# 形成表格以推动逻辑回归
train_X = []
train_y = []
for i in range(len(train_table)):
train_X.append(
[train_table.x.iloc[i], train_table.y.iloc[i], train_table.z.iloc[i]]
)
train_y.append(train_table.label.iloc[i])

test_X = []
for k in range(len(test_table)):
test_X.append([test_table.x.iloc[k], test_table.y.iloc[k], test_table.z.iloc[k]])

# 尝试使用和不使用规范化
clf = LogisticRegression().fit(normalize(train_X), train_y)
# clf = LogisticRegression().fit(train_X, train_y)

predict = clf.predict(test_X[:])
prob = clf.predict_proba(test_X[:])

results = pd.DataFrame(
sort_results(test_table, predict, prob),
columns=[&quot;&quot;, &quot;timestamp&quot;, &quot;UTC time&quot;, &quot;label&quot;, &quot;accuracy&quot;],
)
]]></description>
      <guid>https://stackoverflow.com/questions/79057824/logisticregression-not-returning-correct-results</guid>
      <pubDate>Sat, 05 Oct 2024 19:41:26 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn StackingClassifier 非常慢且 CPU 使用率不一致</title>
      <link>https://stackoverflow.com/questions/73013164/sklearn-stackingclassifier-very-slow-and-inconsistent-cpu-usage</link>
      <description><![CDATA[我最近一直在尝试使用 sklearn 中的 StackingClassifier 和 StackingRegressor，但我注意到它总是很慢，并且 CPU 使用效率低下。假设（仅出于此示例的目的）我想使用 StackingClassifier 堆叠随机森林和 lightgbm，同时使用 lightgbm 作为最终分类器。在这种情况下，我预计运行 StackingClassifier 所需的时间大致等于运行单个随机森林所需的时间 + 运行 2 个单独的 lightgbm 所需的时间 + 一些小的余量（所以基本上是各部分的总和 + 训练 StackingClassifier 本身的时间 + 小的余量），但在实践中似乎需要几倍的时间。示例：
import numpy as np
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import StackingClassifier
import lightgbm as ltb
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold

X,y = load_iris(return_X_y=True)
cv = StratifiedKFold(n_splits=10)
lgbm = ltb.LGBMClassifier(n_jobs=4)
rf = RandomForestClassifier()

首先是 LightGBM，按照实际时间计算，在我的计算机上大约需要 140 毫秒：
%%time
scores = cross_val_score(lgbm, X, y,评分=&#39;accuracy&#39;, cv=cv, n_jobs=4, error_score=&#39;raise&#39;)
np.mean(scores)

这只是一个随机森林，对我来说大约需要 220 毫秒：
%%time
scores = cross_val_score(rf, X, y, 评分=&#39;accuracy&#39;, cv=cv, n_jobs=-1, error_score=&#39;raise&#39;)
np.mean(scores)

现在有一个将这两者结合起来的 StackingClassifier。由于它基本上运行了上述两个代码块 + 另一轮 lightgbm，我预计它大约需要 250+120+120=490 毫秒，但实际上需要大约 3000 毫秒，超过 6 倍：
%%time
estimators = [
(&#39;rf&#39;, rf),
(&#39;lgbm,&#39;, lgbm)
]

clf = StackingClassifier(
estimators=estimators, final_estimator=lgbm, passthrough=True)

scores = cross_val_score(clf, X, y,scoring=&#39;accuracy&#39;, cv=cv, n_jobs=4, error_score=&#39;raise&#39;)
np.mean(scores) 

我还注意到（在更大的数据集上运行完全相同的代码时，我需要足够长的时间才能监控我的 CPU 使用率），而 StackingClassifier 的 CPU 使用率则到处都是。
例如，运行单个 lightgbm 的 CPU 使用率：
运行单个 lightgbm 的 CPU 使用率
（基本上始终为 100%，因此 CPU 使用效率很高）
将 lightgbm 作为 stackingclassifier 运行时的 CPU 使用率
（到处都是，通常远没有接近 100%）
我做错了什么导致 StackingClassifier 比各部分的总和慢这么多吗？]]></description>
      <guid>https://stackoverflow.com/questions/73013164/sklearn-stackingclassifier-very-slow-and-inconsistent-cpu-usage</guid>
      <pubDate>Sun, 17 Jul 2022 15:44:30 GMT</pubDate>
    </item>
    <item>
      <title>使用哪个 Python 库来对调查数据进行定性分析？[关闭]</title>
      <link>https://stackoverflow.com/questions/60967882/which-python-library-to-use-for-qualitative-analysis-of-survey-data</link>
      <description><![CDATA[我有一个数据集，其中包含大约 300 人完成的问卷调查的回复。该问卷调查涉及公共交通中的用户体验和行为。我们对 3 家公交公司进行了调查。大多数问题都是“是/否”、“3 家公司中最好的”或“3 家公司中最差的”。
如果可能的话，我想建立一个模型，根据答案推荐三家公司中最好的一家。问题包括“公交车的可用性、公交车的可靠性、用户的偏好和公交车的物理维护”。
我希望模型能够分析数据集并返回最好的公交公司，该公司将很容易获得、干净且维护良好、可靠并且用户会更喜欢它。
此外，诸如“您喜欢哪辆公交车？”之类的问题的答案应该在决策中占有更大的权重。
我对机器学习还很陌生，希望有人能建议从哪种算法开始训练模型。]]></description>
      <guid>https://stackoverflow.com/questions/60967882/which-python-library-to-use-for-qualitative-analysis-of-survey-data</guid>
      <pubDate>Wed, 01 Apr 2020 09:39:52 GMT</pubDate>
    </item>
    <item>
      <title>基于不同场景的输入和机器学习查询</title>
      <link>https://stackoverflow.com/questions/52772655/different-scenario-based-queries-on-imputing-and-machine-learning</link>
      <description><![CDATA[我正在学习插补和模型训练。以下是我在训练数据集时遇到的几个问题。请提供答案。

假设我有一个包含 1000 个观测值的数据集。现在我要一次性在完整数据集上训练模型。我的另一种方法是，我将数据集分为 80% 和 20%，先在 80% 的数据上训练我的模型，然后在 20% 的数据上训练我的模型。这相同还是不同？基本上，如果我在新数据上训练我已经训练过的模型，这意味着什么？

插补相关

另一个问题与插补有关。假设我有一些船上乘客的数据集，其中只有头等舱乘客被分配舱位。有一列包含舱位号（分类），但很少有观测值有这些舱位号。现在我知道这个列很重要，所以我不能删除它，因为它有很多缺失值，所以大多数算法都不起作用。如何处理这种类型的列的插补？

在插补验证数据时，我们是否使用与插补训练数据相同的值进行插补，或者插补值是否再次从验证数据本身计算得出？

如何以字符串的形式插补数据，例如机票号（如 A-123）。该列很重要，因为第一个字母表示乘客的等级。因此，我们不能删除它。

]]></description>
      <guid>https://stackoverflow.com/questions/52772655/different-scenario-based-queries-on-imputing-and-machine-learning</guid>
      <pubDate>Fri, 12 Oct 2018 05:11:09 GMT</pubDate>
    </item>
    <item>
      <title>如何将机器学习分类方法应用于一维时间序列数据？</title>
      <link>https://stackoverflow.com/questions/50519856/how-do-i-apply-machine-learning-classification-methods-to-1d-time-series-data</link>
      <description><![CDATA[我在各种锻炼（深蹲、俯卧撑、仰卧起坐、波比跳）过程中获得了 IMU 数据（加速度计、磁力计和陀螺仪）。这些锻炼是在单个 1D 时间序列信号中完成的，我想使用机器学习分类方法来识别信号中的不同锻炼。我不想将信号压缩为 0D 峰值并以此方式构建我的特征，而是保持时间域完整。下图显示了包含四种锻炼的加速度计的示例数据。
因此，我的问题是 - 哪种方法最有效？ K-means 聚类在 0D 意义上是完美的，那么是否有 1D 等效物？
]]></description>
      <guid>https://stackoverflow.com/questions/50519856/how-do-i-apply-machine-learning-classification-methods-to-1d-time-series-data</guid>
      <pubDate>Fri, 25 May 2018 00:18:01 GMT</pubDate>
    </item>
    </channel>
</rss>