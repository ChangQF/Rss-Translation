<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 18 Mar 2024 00:58:17 GMT</lastBuildDate>
    <item>
      <title>用于检测/分类手绘图像中的形状的模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78177334/model-to-detect-classify-shape-in-a-handdrawn-image</link>
      <description><![CDATA[给定一个手绘图像，我需要弄清楚该对象是什么并用正确的绘图替换它。对象可以是基本形状，如矩形或圆形，也可以是简单的对象，如飞机、苹果或计算机。
例如：

我找到了一个名为 AutoDraw 的工具 (https://www.autodraw.com/)，但是它仅适用于墨迹笔画。它依赖于用户的顺序和准确的笔划来对图像进行分类。但是，对于静态图像，我们没有此笔划信息。
是否有任何机器学习模型或库可用于在不依赖笔划顺序的情况下对静态手绘图像进行分类？]]></description>
      <guid>https://stackoverflow.com/questions/78177334/model-to-detect-classify-shape-in-a-handdrawn-image</guid>
      <pubDate>Sun, 17 Mar 2024 23:36:44 GMT</pubDate>
    </item>
    <item>
      <title>如何计算每个图像，同时测试 yolo 在该图像中检测到的 cpu 利用率？</title>
      <link>https://stackoverflow.com/questions/78176760/how-to-calculate-for-each-image-while-testing-how-much-cpu-utilization-the-yolo</link>
      <description><![CDATA[我使用的是yolo5。我想估计 yolo 处理某个图像需要多少 CPU 利用率。我该怎么做？
我尝试使用 P-Sutil 来计算 CPU 利用率，但是我无法获得每个图像的 CPU 利用率
这是我的代码
# !pip install -U ultralytics
进口火炬
导入时间
导入 psutil
从 keras.datasets 导入 cifar10

# 加载YOLOv5模型
# model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;, pretrained=True)

# 加载CIFAR-10数据集并选择10张图像
(_, _), (x_test, _) = cifar10.load_data()
x_test_subset = x_test[:10] # 从测试集中选择前 10 张图像

# 循环遍历图像
对于 i，枚举中的 img(x_test_subset)：
    # 测量 YOLOv5 推理之前的 CPU 时间
    start_cpu = psutil.cpu_times().user + psutil.cpu_times().system

    # 将图像转换为PIL格式并进行YOLOv5推理
    开始时间 = 时间.time()
    results = model(&quot;/content/yolo.jpg&quot;, size=640) # 可以根据需要调整大小
    结束时间 = time.time()

    # 测量 YOLOv5 推理后的 CPU 时间
    end_cpu = psutil.cpu_times().user + psutil.cpu_times().system

    # 计算 YOLOv5 推理的 CPU 利用率
    cpu_utilization_percentage = ((end_cpu - start_cpu) / psutil.cpu_count()) / (end_time - start_time) * 100
    
    # 计算检测到的对象数量
    num_objects = len(结果.xyxy[0])

    # 打印当前图像的结果
    print(&quot;图像 {}: 检测到的对象: {} - YOLO CPU 利用率 (%): {:.2f}&quot;.format(i+1, num_objects, cpu_utilization_percentage))
`
]]></description>
      <guid>https://stackoverflow.com/questions/78176760/how-to-calculate-for-each-image-while-testing-how-much-cpu-utilization-the-yolo</guid>
      <pubDate>Sun, 17 Mar 2024 20:07:08 GMT</pubDate>
    </item>
    <item>
      <title>我无法使用 Gradio Client API 使用图像进行预测</title>
      <link>https://stackoverflow.com/questions/78176532/i-cant-use-the-gradio-client-api-to-make-a-prediction-using-images</link>
      <description><![CDATA[我正在尝试按照以下示例将图像发送到 Gradio Client API：
从“@gradio/client”导入{ client }；

const response_0 = 等待 fetch(“https://raw.githubusercontent.com/gradio-app/gradio/main/test/test_files/bus.png”);
const exampleImage =等待response_0.blob();
                        
const app = 等待客户端(“airvit2/pet_classifier”);
const 结果 =等待 app.predict(“/预测”, [
                exampleImage, // &#39;img&#39; 图像组件中的 blob
    ]);

console.log(结果.数据);

但它返回此错误：
&lt;前&gt;&lt;代码&gt;{
    “类型”：“状态”，
    “端点”：“/预测”，
    “fn_index”：0，
    “时间”：“2024-03-17T18:36:53.270Z”，
    “队列”：正确，
    “消息”：空，
    “阶段”：“错误”，
    “成功”：假
}

这是我的 Gradio 代码：
from fastai.vision.all import *
将渐变导入为 gr

学习 = load_learner(&#39;model.pkl&#39;)

def 预测（img）：
    print(&quot;图片：&quot;, img)
    img = 加载图像(img)
    # img = PILImage.create(img)
    pred, pred_idx, probs = learn.predict(img)
    返回预测值

gr.Interface(fn = 预测，输入 = gr.Image(type=“pil”，高度 = 224，宽度 = 224)，输出 = gr.Label(num_top_classes = 3)).launch(share = True)


我尝试将图像格式更改为 Blob，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78176532/i-cant-use-the-gradio-client-api-to-make-a-prediction-using-images</guid>
      <pubDate>Sun, 17 Mar 2024 18:57:11 GMT</pubDate>
    </item>
    <item>
      <title>创建一个新模型，它将检查用户是否在网站上（是否更改选项卡）[关闭]</title>
      <link>https://stackoverflow.com/questions/78176381/creating-a-new-model-where-it-will-check-if-the-user-is-on-the-site-or-not-cha</link>
      <description><![CDATA[我想创建一个模型，如果用户在会话期间更改选项卡，它将通知管理员。它将向管理员发送有关此事的详细通知。但我不知道从哪里开始以及做什么。我只需要注意一下应该如何在 ML 应用程序/网络中实现它。
因为我是新手，所以我尝试搜索是否有任何类型的 Git 或教程。但没找到。]]></description>
      <guid>https://stackoverflow.com/questions/78176381/creating-a-new-model-where-it-will-check-if-the-user-is-on-the-site-or-not-cha</guid>
      <pubDate>Sun, 17 Mar 2024 18:08:21 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv8 自定义模型不进行预测</title>
      <link>https://stackoverflow.com/questions/78176290/yolov8-custom-model-not-making-predictions</link>
      <description><![CDATA[我使用自定义训练的 Yolov8 模型来预测物理门是关闭还是打开。我已经在自定义数据集上训练了 Yolov8，但即使传递用于训练的相同数据，它也不会进行任何检测。
我使用了大约 300 张图像的数据集。
这是我的代码：
导入操作系统

从 ultralytics 导入 YOLO
导入CV2


VIDEOS_DIR = os.path.join(&#39;.&#39;, &#39;视频&#39;)

video_path = os.path.join(VIDEOS_DIR, &#39;样本门.mp4&#39;)
video_path_out = &#39;{}_out.mp4&#39;.format(video_path)

cap = cv2.VideoCapture(video_path)
ret, 框架 = cap.read()
H、W、_ = 框架.形状
out = cv2.VideoWriter(video_path_out, cv2.VideoWriter_fourcc(*&#39;MP4V&#39;), int(cap.get(cv2.CAP_PROP_FPS)), (W, H))

model_path = os.path.join(&#39;.&#39;, &#39;运行&#39;, &#39;检测&#39;, &#39;训练&#39;, &#39;权重&#39;, &#39;last.pt&#39;)


model = YOLO(model_path) # 加载自定义模型


休息时：

    结果=模型（框架）[0]
    对于 results.boxes.data.tolist() 中的结果：
        x1, y1, x2, y2, 分数, class_id = 结果
        打印（x1，y1，x2，y2）

        cv2.矩形(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 4)
        cv2.putText(frame, results.names[int(class_id)].upper(), (int(x1), int(y1 - 10)),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3, cv2.LINE_AA)

    输出.write(帧)
    ret, 框架 = cap.read()

cap.release()
out.release()
cv2.destroyAllWindows()

以下是训练结果：https://i.stack.imgur。 com/huyZR.png]]></description>
      <guid>https://stackoverflow.com/questions/78176290/yolov8-custom-model-not-making-predictions</guid>
      <pubDate>Sun, 17 Mar 2024 17:43:55 GMT</pubDate>
    </item>
    <item>
      <title>Mamba 架构的“hidden_​​states”形式</title>
      <link>https://stackoverflow.com/questions/78176169/form-of-hidden-states-for-mamba-architecture</link>
      <description><![CDATA[我正在使用 HuggingFace 实现来尝试最近的 Mamba 架构。它的形式为
MambaForCausalLM(
  （骨干）：MambaModel（
    （嵌入）：嵌入(50280, 768)
    （层）：模块列表（
      (0-23): 24 x 曼巴布块(
        （范数）：MambaRMSNorm()
        （混音器）：MambaMixer（
          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)
          （动作）：SiLU()
          （in_proj）：线性（in_features = 768，out_features = 3072，偏差= False）
          （x_proj）：线性（in_features = 1536，out_features = 80，偏差= False）
          （dt_proj）：线性（in_features = 48，out_features = 1536，偏差= True）
          （out_proj）：线性（in_features = 1536，out_features = 768，偏差= False）
        ）
      ）
    ）
    (norm_f): MambaRMSNorm()
  ）
  （lm_head）：线性（in_features = 768，out_features = 50280，偏差= False）
）

当我在传入虚拟输入句子后获取模型outputs.hidden_​​state时，它的长度为25。对我来说，这意味着第一个条目是 model.backbone.embeddings 的输出，其余条目来自主干中的 24 层，没有任何来自  &gt;lm_head。所以我想改变 LM 头（例如，通过用随机矩阵替换它）应该对 outputs.hidden_​​state[-1] 没有影响，我想它是  的输出model.backbone.layers[-1]，尽管它显然会影响 outputs.logits。然而，当我这样做时，outputs.hidden_​​state[-1] 发生了巨大的变化。为什么是这样？当我添加一个钩子来手动跟踪每个 24 层的激活时，最终的激活不会随着 lm_head 的更改而改变，正如我所期望的那样。所以我猜 outputs.hidden_​​states 包含的内容与我的想法不同。这里发生了什么？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78176169/form-of-hidden-states-for-mamba-architecture</guid>
      <pubDate>Sun, 17 Mar 2024 17:06:34 GMT</pubDate>
    </item>
    <item>
      <title>如何从短信屏幕截图中提取格式正确的对话[关闭]</title>
      <link>https://stackoverflow.com/questions/78176164/how-do-i-extract-a-properly-formatted-conversation-from-a-text-message-screensho</link>
      <description><![CDATA[给定任何平台上短信对话的屏幕截图，如何提取正确排序和格式的对话？我的输出应该类似于：
你：嗨
他们：嗨
你：你好吗？
他们：好
他们：那你呢？

仅使用任何 OCR 库的主要问题是它会捕获屏幕上的无关文本，例如时间戳、单元格提供程序、通知等，而我无法可靠地区分这些文本和实际对话。我也不想使用 GPT-4V 或任何其他视觉法学硕士，因为它既慢又贵。为了快速、廉价且准确，最好的方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78176164/how-do-i-extract-a-properly-formatted-conversation-from-a-text-message-screensho</guid>
      <pubDate>Sun, 17 Mar 2024 17:04:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 的 Google Colab Bert 实例化错误</title>
      <link>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</link>
      <description><![CDATA[我正在尝试在 Colab 上使用 Tensorflow 构建 Bert 模型。这段代码几周前就可以完美运行。现在，如果我尝试实例化模型，则会收到以下错误：
初始化 TF 2.0 模型 TFBertModel 时未使用 PyTorch 模型的某些权重：[&#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls .predictions.transform.LayerNorm.weight&#39;、&#39;cls.predictions.bias&#39;、&#39;cls.seq_relationship.bias&#39;、&#39;cls.predictions.transform.dense.bias&#39;、&#39;cls.seq_relationship.weight&#39;]
- 如果您从在其他任务或其他架构上训练的 PyTorch 模型初始化 TFBertModel（例如，从 BertForPreTraining 模型初始化 TFBertForSequenceClassification 模型），这是预期的。
- 如果您从希望完全相同的 PyTorch 模型初始化 TFBertModel（例如，从 BertForSequenceClassification 模型初始化 TFBertForSequenceClassification 模型），则不会出现这种情况。
TFBertModel 的所有权重都是从 PyTorch 模型初始化的。
如果您的任务与检查点模型训练的任务类似，您就可以使用 TFBertModel 进行预测，而无需进一步训练。
-------------------------------------------------- ------------------------
TypeError Traceback（最近一次调用最后一次）
&lt;ipython-input-14-b0e769ef7​​890&gt;在&lt;细胞系：7&gt;()
      5 SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
      6 SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
----&gt; 7 SC_pooler_output = SC_bert_model(SC_input_layer, Attention_mask=SC_mask_layer)[1] # 第二个输出，che è il pooler_output
      8
      9 # 辍学层的Aggiungi

36帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/type_spec.py 在 type_spec_from_value(value) 中
   1002 3，“无法将 %r 转换为张量：%s” % (类型(值).__name__, e))
   1003
-&gt;第1004章
   第1005章 1005
   1006

TypeError：调用层“嵌入”时遇到异常（类型 TFBertEmbeddings）。

无法为名称构建 TypeSpec：“tf.debugging.assert_less_5/assert_less/Assert/Assert”
op：“断言”
输入：“tf.debugging.assert_less_5/assert_less/All”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_0”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_1”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_2”
输入：“占位符”
输入：“tf.debugging.assert_less_5/assert_less/Assert/Assert/data_4”
输入：“tf.debugging.assert_less_5/assert_less/y”
属性{
  键：“总结”
  价值 {
    我：3
  }
}
属性{
  键：“T”
  价值 {
    列表 {
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_STRING
      类型：DT_INT32
      类型：DT_STRING
      类型：DT_INT32
    }
  }
}
 不支持的类型。

调用层“embeddings”接收的参数（类型 TFBertEmbeddings）：
  • input_ids=
  •position_ids=无
  • token_type_ids=
  • input_embeds=无
  •过去的键值长度=0
  • 训练=False

模型的代码是：
SC_input_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“input_ids”)
SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
SC_pooler_output = SC_bert_model（SC_input_layer，attention_mask = SC_mask_layer）[1]

# Dropout 层的Aggiungi
SC_dropout_layer = Dropout(dropout_rate)(SC_pooler_output)
SC_output_layer = 密集（6，激活=&#39;sigmoid&#39;）（SC_dropout_layer）
SC_model = 模型(输入=[SC_input_layer, SC_mask_layer], 输出=SC_output_layer)

我发现安装tensorflow 2.10.0可以工作，但是使用Google Colab时我的CUDA版本有问题，并且使用tensorflow 2.10它无法识别CPU。
该代码几周前就可以工作，有人有解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78176160/google-colab-bert-instantiation-error-using-tensorflow</guid>
      <pubDate>Sun, 17 Mar 2024 17:03:42 GMT</pubDate>
    </item>
    <item>
      <title>Hugging Face 的无头 GPT2 模型在保存时抛出错误 - 如何添加输入和输出层以及自定义 PositionalEmbedding</title>
      <link>https://stackoverflow.com/questions/78175539/headless-gpt2-model-from-hugging-face-throws-error-on-saving-how-to-add-input</link>
      <description><![CDATA[我想使用 GPT2 对序列数据进行回归任务，因此尝试从 Hugging Face 中找出无头 TFGPT2，代码如下：
配置 = GPT2Config(n_embd = embed_dim, n_head=num_heads)
基础模型 = TFGPT2Model（配置）
输入形状 = (1, 嵌入尺寸)
input1 = 层.Input(shape=input_shape, dtype=tf.float32)
positional_encoding = PositionalEmbedding(sequence_length, embed_dim)
解码器输入=位置编码（输入1）
Z = base_model.call(inputs_embeds=decoder_inputs)
输出=layers.TimeDistributed（keras.layers.Dense（embed_dim，激活=“relu”））（Z.last_hidden_​​state）
模型= keras.Model（输入1，输出）
model.compile(loss=“mean_squared_error”，optimizer=tf.keras.optimizers.Adam(beta_1=0.9，beta_2=0.98，epsilon=1.0e-9)，metrics=[tf.keras.metrics.RootMeanSquaredError()] ）
历史= model.fit（数据集，validation_data = val_dataset，epochs = epoch_len，verbose = 1）
tf.keras. saving. save_model(模型, r&#39;/drive/model_huggingface&#39;)

另请注意，我还使用自定义 PositionalEmbedding 类，因此可选的 input_embeds 参数传递给模型。
该模型训练并学习数据，但在尝试保存时会抛出错误：
AssertionError：尝试导出引用“未跟踪”资源的函数。由函数捕获的 TensorFlow 对象（例如 tf.Variable）必须通过将其分配给被跟踪对象的属性或直接分配给主对象的属性来“跟踪”。请参阅以下信息：
    函数名称 = b&#39;__inference_signature_wrapper_452514&#39;
    捕获的张量 = 
    可追踪引用此张量 = ;
    内部张量 = Tensor(“452144:0”, shape=(), dtype=resource)

我认为这是因为我向该模型添加了头部和自定义层。请让我知道您对我对如何实现此模型的解释的看法。]]></description>
      <guid>https://stackoverflow.com/questions/78175539/headless-gpt2-model-from-hugging-face-throws-error-on-saving-how-to-add-input</guid>
      <pubDate>Sun, 17 Mar 2024 14:03:14 GMT</pubDate>
    </item>
    <item>
      <title>在 UNet 模型的 FluxTraining.jl 中将数据从 DataLoader 传递到 Learner 时出现问题</title>
      <link>https://stackoverflow.com/questions/78175117/trouble-with-passing-data-from-dataloader-to-learner-in-fluxtraining-jl-for-unet</link>
      <description><![CDATA[我正在尝试使用 FluxTraining.jl 训练 UNet 模型 u，但在将数据从 DataLoader 正确传递到 Learner 时遇到困难。
上下文：
我有两个数据集：一个用于名为“w”的输入图像，另一个用于名为“w”的输入图像。尺寸为 256x256x3x20（20 个观察值，3 个 RGB 通道），另一个用于地面实况比较，称为“wp”尺寸为 256x256x1x20（20 个观察值，1 个灰度通道）。
我使用 DataLoader 定义数据迭代器，如下所示：
trainiter = DataLoader((w, wp), 4)

然后，我尝试使用以下代码将数据传递给学习者：
学习者 = 学习者(
    你，
    损失，
    回调 = [
        指标（准确度），
        检查点（“trainingData/modelSaves/”），
        记录器后端
    ],
    优化器=选择
）

#一个纪元出现错误
纪元！（学习者，TrainingPhase（），培训师）

问题：
运行代码时，我遇到一个错误，表明损失函数（请参阅帖子底部）正在接收尺寸为 256x256x1x20 而不是预期的 256x256x3x20 的输入数据 x。数据似乎没有从 DataLoader 正确传递到 Learner。
如何正确地将数据从 DataLoader 传递到 FluxTraining.jl 中的 Learner？
在使用 FluxTraining 之前，我能够接受相关培训
Flux.train!(loss, Flux.params(u),rep, opt, cb = () -&gt; @show(loss(w, wp)))，其中 rep =迭代器.repeated((w, wp), 100)。在所有情况下，ADAM() 都是我的优化器（opt）。
作为参考，我的损失函数是：
函数损失(x, y)
    @显示尺寸(x)
    @显示尺寸(y)
    Flux.dice_coeff_loss(u(x), y)
结尾

我尝试了对代码的各种修改，例如将 DataLoader 语法更改为 DataLoader((w,w), 4) 或 DataLoader(w, 4) ，但我仍然面临以下问题：要么将单个 Float32 而不是数组传递到模型中，要么输入数据的维度仍然不正确。
我还尝试循环遍历训练器中的所有 xs 和 ys 并调用损失函数。在这种情况下，它工作得很好，所以我认为这与我使用纪元的方式不一样！功能。]]></description>
      <guid>https://stackoverflow.com/questions/78175117/trouble-with-passing-data-from-dataloader-to-learner-in-fluxtraining-jl-for-unet</guid>
      <pubDate>Sun, 17 Mar 2024 11:50:28 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归实现 - 损失不收敛且模型结果不佳</title>
      <link>https://stackoverflow.com/questions/78175088/logistic-regression-implementation-loss-is-not-converging-and-poor-model-resul</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78175088/logistic-regression-implementation-loss-is-not-converging-and-poor-model-resul</guid>
      <pubDate>Sun, 17 Mar 2024 11:42:37 GMT</pubDate>
    </item>
    <item>
      <title>在吉他指法谱特征提取的 CNN 中保留空间感知</title>
      <link>https://stackoverflow.com/questions/78173966/preserving-spatial-awareness-in-a-cnn-for-guitar-tablature-feature-extraction</link>
      <description><![CDATA[我正在构建一个卷积神经网络 (CNN)，以从吉他指法谱图像中提取音符、小节等特征。虽然我在检测图像中的这些特征方面取得了进展，但在特征提取过程中我仍在努力保持空间意识。
对于那些不熟悉标签的人来说，简单来说，有六行代表吉他上的六根弦，数字表示琴弦上的品格位置。

我正在寻求有关如何改进 CNN 架构或调整训练过程以应对这些挑战的建议。具体来说，我对能够帮助保留空间意识，同时仍保持特征提取的高精度的技术或方法感兴趣。
任何见解、建议或推荐资源将不胜感激。
这是迄今为止我的方法的细分：
数据准备：我收集了吉他谱图像的数据集，其中包含音符、和弦和小节等各种特征。每张图像都标有相应的特征。
模型架构：我设计了一个用于特征提取的 CNN 架构。它由卷积层和用于特征检测的池化层组成。
训练：我使用标记数据集训练了 CNN，以学习吉他指法谱图像中存在的不同特征的模式。
虽然该模型可以准确地检测图像中的各个特征，但它缺乏空间意识，而这对于理解指法谱的结构布局至关重要。例如，音符和小节的相对位置对于准确解释音乐至关重要。
CNN 似乎只专注于识别单个特征，而不考虑它们的空间关系，即每个音符在相关字符串上的位置。]]></description>
      <guid>https://stackoverflow.com/questions/78173966/preserving-spatial-awareness-in-a-cnn-for-guitar-tablature-feature-extraction</guid>
      <pubDate>Sun, 17 Mar 2024 03:20:15 GMT</pubDate>
    </item>
    <item>
      <title>机器学习模型仅返回 0 分。我做错了什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78170278/the-machine-learning-models-are-only-returning-a-score-of-0-what-am-i-doing-wro</link>
      <description><![CDATA[在 Jupyter-Notebook 中，我创建了一个函数，可以对不同的 sklearn 机器学习模型进行拟合和评分。使用的数据集有超过 400000 行和 103 列，因此我分为两个不同的数据集：训练数据集和验证数据集。但是当我在函数中使用数据时，我想要测试的所有 4 个模型的得分均为 0。
这是我的代码：
# 分割数据集
df_val = df_tmp[df_tmp[&#39;年份&#39;] == 2012]
df_train = df_tmp[df_tmp[&#39;年份&#39;] != 2012]

# 将数据集分为训练和测试
X_train, y_train = df_train.drop(&#39;年&#39;, axis=1), df_train[&#39;年&#39;]
X_val, y_val = df_val.drop(&#39;年份&#39;, axis=1), df_val[&#39;年份&#39;]

# 将模型放入字典中
测试模型 = {
    “套索”：套索()，
    “ElasticNet”：ElasticNet()，
    “RandomForestRegressor”：RandomForestRegressor()，
    “山脊”：山脊()
}

# 创建函数来评估两个模型
def fit_and_score(test_models, X_train, X_val, y_train, y_val):
    
    # 记录模型分数的字典
    模型分数 = {}
    
    ＃ 环形
    for name, model in test_models.items(): # name, model = key, value
        # 拟合模型
        model.fit(X_train, y_train)
        # 评估模型并将其分数附加到 models_scores
        models_scores[名称] = model.score(X_val, y_val)
        
    返回模型分数

首先我想也许我没有正确编写函数，所以我单独测试了模型，它们仍然得分为 0。之后我决定测试是否我的数据有问题（我不认为这是它，bcz 我从一个旧的 Kaggle 竞赛中得到它，推土机竞赛），所以我用相同的数据训练并安装了一个模型，希望我的分数是 1，但我得到了 0.31。我真的不知道该怎么办]]></description>
      <guid>https://stackoverflow.com/questions/78170278/the-machine-learning-models-are-only-returning-a-score-of-0-what-am-i-doing-wro</guid>
      <pubDate>Sat, 16 Mar 2024 01:04:49 GMT</pubDate>
    </item>
    <item>
      <title>自动编码器整形问题</title>
      <link>https://stackoverflow.com/questions/78165698/autoencoder-shaping-issue</link>
      <description><![CDATA[我的自动编码器出现问题，因为我错误地调整了输出。目前自动编码器的编码与此类似。
我收到此错误：
&lt;块引用&gt;
ValueError：尺寸必须相等，但为 2000 和 3750
&#39;{{节点mean_absolute_error/sub}} =
Sub[T=DT_FLOAT](sequential_8/sequential_7/conv1d_transpose_14/BiasAdd,
IteratorGetNext:1)&#39;，输入形状：[?,2000,3], [?,3750,3]。

如果可能的话，有人可以帮助调整架构吗？我似乎忘记了最初为此调整所做的原始修改。
导入tensorflow为tf
从tensorflow.keras.models导入模型
从tensorflow.keras.layers导入输入，Conv1D，MaxPooling1D，UpSampling1D，连接
从tensorflow.keras.callbacks导入EarlyStopping

# 提供的编码器
编码器 = tf.keras.models.Sequential([
    tf.keras.layers.Reshape([3750, 3], input_shape=[3750, 3]),
    tf.keras.layers.Conv1D(32，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(64，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(128，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(256，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(512，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512)
]）

#潜在空间

解码器 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(512 * 125, input_shape=[512]),
    tf.keras.layers.Reshape([125, 512]),
    tf.keras.layers.Conv1DTranspose（512，kernel_size = 5，strides = 1，padding =“相同”，激活=“relu”），
    tf.keras.layers.UpSampling1D（大小=2），
    tf.keras.layers.Conv1DTranspose（256，kernel_size = 5，strides = 1，padding =“相同”，激活=“relu”），
    tf.keras.layers.UpSampling1D（大小=2），
    tf.keras.layers.Conv1DTranspose（128，kernel_size = 5，strides = 1，padding =“相同”，激活=“relu”），
    tf.keras.layers.UpSampling1D（大小=2），
    tf.keras.layers.Conv1DTranspose（64，kernel_size = 5，strides = 1，padding =“相同”，激活=“relu”），
    tf.keras.layers.UpSampling1D（大小=2），
    # 调整内核大小和填充以匹配输入形状
    tf.keras.layers.Conv1DTranspose(3，kernel_size=5，strides=1，padding=“相同”，激活=“线性”)
]）

# 向编码器和解码器添加更多具有更大内核大小的层。
ae = tf.keras.models.Sequential([编码器，解码器])

ae.编译(
    损失=“均方误差”，
    优化器=tf.keras.optimizers.Adam(learning_rate=0.00001)
）
# 定义早期停止标准
Early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, 耐心=30, mode=&#39;min&#39;)

历史= ae.fit（X_train，X_train，batch_size = 8，epochs = 150，validation_data =（X_val，X_val），callbacks = [early_stopping]）```
]]></description>
      <guid>https://stackoverflow.com/questions/78165698/autoencoder-shaping-issue</guid>
      <pubDate>Fri, 15 Mar 2024 08:56:02 GMT</pubDate>
    </item>
    <item>
      <title>由于 ValueError，autoencoder.fit 不起作用</title>
      <link>https://stackoverflow.com/questions/78163348/autoencoder-fit-doesnt-work-becaue-of-a-valueerror</link>
      <description><![CDATA[我不明白我的问题是什么。它应该可以工作，只是因为它是张量流文档中的标准自动编码器。
这是错误
第 64 行，通话中
    解码 = self.decoder(编码)
ValueError：调用 Autoencoder.call() 时遇到异常。

无效的数据类型：&lt;0x7fb471cc1c60 处的属性对象&gt;

Autoencoder.call() 收到的参数：
  x=tf.Tensor(形状=(32,28,28),dtype=float32)

这是我的代码
(x_train, _), (x_test, _) = Fashion_mnist.load_data()

x_train = x_train.astype(&#39;float32&#39;) / 255.
x_test = x_test.astype(&#39;float32&#39;) / 255.

打印（x_train.shape）
打印（x_test.shape）

类自动编码器（模型）：
  def __init__(自身，latent_dim，形状)：
    super(自动编码器, self).__init__()
    self.latent_dim = Latent_dim
    self.shape = 形状
    self.encoder = tf.keras.Sequential([
      层.Flatten(),
      层.Dense（latent_dim，激活=&#39;relu&#39;），
    ]）
    self.decoder = tf.keras.Sequential([
      层.Dense（tf.math.reduce_prod（形状），激活=&#39;sigmoid&#39;），
      图层.重塑（形状）
    ]）

  def 调用（自身，x）：
    编码 = self.encoder(x)
    打印（编码）
    解码 = self.decoder(编码)
    打印（解码）
    返回解码后的内容


形状 = x_test.shape[1:]
潜伏暗度 = 64
自动编码器 = 自动编码器（latent_dim，形状）

autoencoder.compile(optimizer=&#39;adam&#39;, loss=losses.MeanSquaredError())

自动编码器.fit(x_train, x_train,
                纪元=10，
                随机播放=真，
                验证数据=（x_test，x_test））

我尝试更改数据库并尝试了不同的形状]]></description>
      <guid>https://stackoverflow.com/questions/78163348/autoencoder-fit-doesnt-work-becaue-of-a-valueerror</guid>
      <pubDate>Thu, 14 Mar 2024 20:39:06 GMT</pubDate>
    </item>
    </channel>
</rss>