<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 20 Jun 2024 15:16:48 GMT</lastBuildDate>
    <item>
      <title>如何将200万份简历高精度地匹配到200个职位？</title>
      <link>https://stackoverflow.com/questions/78647918/how-to-match-2-million-resumes-to-200-jobs-with-high-accuracy</link>
      <description><![CDATA[我面临的挑战是将 200 万份简历与 200 个活跃职位空缺进行匹配，目标准确率为 80%。我的目标是简化我们的招聘流程，确保候选人与职位的准确匹配。以下是详细信息：
问题陈述
目标：有效地将大量简历与较少数量的职位描述进行匹配。
目标：
高精度：目标准确率至少为 80%。
可扩展性：处理大量数据。
自动化：最大限度地减少人工干预。
可能的技术和方法
自然语言处理 (NLP)：
文本预处理：清理、标记化和规范化。
特征提取：TF-IDF、Word2Vec、GloVe、BERT 嵌入。
机器学习模型：
监督学习：对标记数据进行训练。
无监督学习：聚类技术。
深度学习模型：
基于 Transformer 的模型：BERT、RoBERTa，用于更好地理解上下文。
相似度测量：
余弦相似度。
高级指标：针对数据量身定制的自定义指标。
使用的工具
矢量数据库
OpenAI 的 Ada-002 嵌入模型
专业知识请求
有人处理过类似的问题或使用过上述技术吗？任何关于实现我们准确度目标的最佳方法的见解或建议都将不胜感激。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78647918/how-to-match-2-million-resumes-to-200-jobs-with-high-accuracy</guid>
      <pubDate>Thu, 20 Jun 2024 13:56:09 GMT</pubDate>
    </item>
    <item>
      <title>时间序列预测，其中历史值也会由于滞后而更新</title>
      <link>https://stackoverflow.com/questions/78646921/time-series-forecasting-where-historical-values-also-gets-updated-due-to-lag</link>
      <description><![CDATA[我正在对未来 4 周的 covid 病例进行时间序列预测。

传入数据频率：每周
要预测的周数：4

数据的主要问题是，数据滞后约 8 周，这意味着特定周的值将在接下来的 8 周内更新 8 周，同时添加下周的值数据。
特定周（Epiweek 1）的值如下所示：
Load_Date 值
1-Jan-24 为 10 ，
8-Jan-24 为 11 ，
15-Jan-24 为 14 ，
22-Jan-24 为 15 ，
29-Jan-24 为 16 ，
6-Feb-24 为 18 ，
13-Feb-24 为23 ,
20-Feb-24 是 26 ,
27-Feb-24 是 26 ,
6-Mar-24 是 26 ,

在这种情况下：
未修订值为 10
修订值为 26

如您在以上数据中看到的 - 数据在第 9 周稳定下来。同样，我们有其他一周的值。
当我应用 Auto_Arima、Garch 模型时，当我将我的预测与未修订的数据进行比较时，我可以看到良好的结果，但是当我将它们与修订值（在建模时不可用）进行比较时，我看到更多的 MAPE。
考虑到历史值也会更新 7-8 周，我该如何改善结果？]]></description>
      <guid>https://stackoverflow.com/questions/78646921/time-series-forecasting-where-historical-values-also-gets-updated-due-to-lag</guid>
      <pubDate>Thu, 20 Jun 2024 10:31:39 GMT</pubDate>
    </item>
    <item>
      <title>Mobilenet 与 resnet</title>
      <link>https://stackoverflow.com/questions/78646834/mobilenet-vs-resnet</link>
      <description><![CDATA[Q1-为什么我们不像在 mobile-net v2 中那样在 resnet50 中添加 skip connection 后移除 relu 以获得更好的性能？
Q2-为什么我们没有在 skip connection 中使用卷积层来匹配 mobile-net v2 中层尺寸变化时的维度，就像我们在 resnet 中那样，当层尺寸变化时匹配输出通道？
我尝试在 web 和 chatgpt 上搜索，但答案并不令人满意。它们都像“架构是以那种方式提出的”。]]></description>
      <guid>https://stackoverflow.com/questions/78646834/mobilenet-vs-resnet</guid>
      <pubDate>Thu, 20 Jun 2024 10:13:24 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 - 年龄和性别检测 [关闭]</title>
      <link>https://stackoverflow.com/questions/78646157/machine-learning-age-and-gender-detection</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78646157/machine-learning-age-and-gender-detection</guid>
      <pubDate>Thu, 20 Jun 2024 07:54:19 GMT</pubDate>
    </item>
    <item>
      <title>所有时期的损失和准确率相同</title>
      <link>https://stackoverflow.com/questions/78645720/same-loss-and-accuracy-for-all-epochs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78645720/same-loss-and-accuracy-for-all-epochs</guid>
      <pubDate>Thu, 20 Jun 2024 06:01:17 GMT</pubDate>
    </item>
    <item>
      <title>CUDAError：使用 Gymnasium 的 RL 环境内存不足</title>
      <link>https://stackoverflow.com/questions/78645476/cudaerror-not-enough-memory-for-an-rl-environment-using-gymnasium</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78645476/cudaerror-not-enough-memory-for-an-rl-environment-using-gymnasium</guid>
      <pubDate>Thu, 20 Jun 2024 04:14:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 DARTS 对多个时间序列进行全局模型训练导致 NaN 损失</title>
      <link>https://stackoverflow.com/questions/78645163/training-global-model-on-multiple-time-series-with-darts-results-in-nan-loss</link>
      <description><![CDATA[我尝试使用 DARTS 库在多个时间序列上训练全局模型，但训练和验证损失都遇到了 NaN 值。在单个时间序列上进行训练时，它工作正常。我总共有 6 个时间序列。
我怀疑一个可能的问题是每个时间序列的开始和结束时间戳都不同。这会导致多个时间戳，其中只有时间序列子集的数据可用。例如，在某些时间戳，6 个时间序列中可能只有 2 个有数据可用。
这是我的代码：
from darts import TimeSeries, Scaler
from darts.models import NBEATSModel

train_series = []
val_series = []
scalers = []

for idx in train_df[&#39;region_city_item_encoded&#39;].unique():
sub_train_df = train_df[train_df[&#39;region_city_item_encoded&#39;] == idx].sort_values(&#39;timestamp&#39;).reset_index(drop=True)
sub_val_df = val_df[val_df[&#39;region_city_item_encoded&#39;] == idx].sort_values(&#39;timestamp&#39;).reset_index(drop=True)

cur_time_train_series = TimeSeries.from_dataframe(sub_train_df, time_col=&#39;timestamp&#39;, value_cols=&#39;demand&#39;, fill_missing_dates=True)
train_static_covariates = sub_train_df[[&#39;region_city_item_encoded&#39;]].drop_duplicates().reset_index(drop=True)
cur_time_train_series = cur_time_train_series.with_static_covariates(train_static_covariates)

cur_val_series = TimeSeries.from_dataframe(sub_val_df, time_col=&#39;timestamp&#39;, value_cols=&#39;demand&#39;, fill_missing_dates=True)
val_static_covariates = sub_val_df[[&#39;region_city_item_encoded&#39;]].drop_duplicates().reset_index(drop=True)
cur_val_series = cur_val_series.with_static_covariates(val_static_covariates)

scaler = Scaler()
cur_time_train_series = scaler.fit_transform(cur_time_train_series)
cur_val_series = scaler.transform(cur_val_series)

train_series.append(cur_time_train_series)
val_series.append(cur_val_series)
scalers.append(scaler)

model = NBEATSModel(
input_chunk_length=24,
output_chunk_length=12,
n_epochs=100,
random_state=0
)
model.fit(series=train_series, val_series=val_series)

我尝试了以下方法：

确保使用以下方法填充缺失的日期fill_missing_dates=True。
对每个时间序列分别应用缩放。

训练了 DART 上可用的所有模型

通过添加静态协变量和不使用静态协变量进行训练

]]></description>
      <guid>https://stackoverflow.com/questions/78645163/training-global-model-on-multiple-time-series-with-darts-results-in-nan-loss</guid>
      <pubDate>Thu, 20 Jun 2024 01:28:38 GMT</pubDate>
    </item>
    <item>
      <title>对训练数据集使用决策树模型后仅生成一个节点</title>
      <link>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</link>
      <description><![CDATA[1我正在尝试构建一个决策树模型，该模型基于预测变量预测结果变量（名为：结果）。实际上，我已经对一些&quot;&gt;2 级&quot;变量应用了独热编码，以便稍微扩展预测变量的 n [我的数据]。
我首先探索了数据，然后将其拆分为 80/20 拆分并运行模型，但在训练数据集上运行的模型最终只有一个节点，没有分支。查看类似的帖子，我发现我的数据不平衡，因为通过检查类分配的 prop.table（结果变量），大多数是负面的，而不是正面的。关于在此数据上创建正确树的任何建议
这是我的代码：
将数据拆分为测试和训练数据（80％训练和20％测试数据）
set.seed(1234)
pd &lt;- sample(2, nrow(data_hum_mod), replace = TRUE, prob = c(0.8,0.2))
data_hum_train &lt;- data_hum_mod[pd==1,]
data_hum_test&lt;- data_hum_mod[pd==2,]

拆分后的数据探索
检查数据维度
dim(data_hum_train); dim(data_hum_test)
#确保分离后的数据在每个结果类别（即阳性/阴性 toxo）中的 n 值是平衡的
prop.table(table(data_hum_train$Results)) * 100
prop.table(table(data_hum_test$Results)) *100

这给出了以下结果：
(训练)
阴性 阳性 
75.75758 24.24242

和
(测试)
阴性 阳性 
54.54545 45.45455

检查缺失值
anyNA(data_hum_mod)
#确保没有任何变量为零或接近零方差。
nzv(data_hum_mod)
构建模型（使用 party 包）
install.packages(&#39;party&#39;)
library(party)

data_human_train_tree&lt;- ctree(Results ~., data = data_hum_train,
controls = ctree_control(mincriterion = 0.1))
data_human_train_tree
plot(data_human_train_tree)

使用此代码，我获得了此图
使用其他包（如 C50 和 rpart）也得到了相同的结果
您能对此提出建议吗？我读过关于多数类的子采样（这里是负面结果），如何在 R 中实现这一点？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</guid>
      <pubDate>Thu, 20 Jun 2024 01:04:59 GMT</pubDate>
    </item>
    <item>
      <title>Excel 中的逻辑回归</title>
      <link>https://stackoverflow.com/questions/78644668/logistic-regression-in-excel</link>
      <description><![CDATA[我有两个优化模型：
LR-P1：

LR-P2：

我期望两个模型都得到相同的最优值，但我无法计算模型 LR-P1。我正在进行所有计算，但 excel 求解器无法找到最优值。当我将所有系数设为 0.1 时，求解器只会说找到了最佳值，但不会更改决策变量。
我的问题是，我正在进行所有计算，但 excel 给出了 NUM 错误，而模型 LR-P2 没有。这是因为目标函数对于 LR-P1 来说太小，以至于 excel 求解器无法对其进行交换，从而导致数值问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78644668/logistic-regression-in-excel</guid>
      <pubDate>Wed, 19 Jun 2024 21:13:58 GMT</pubDate>
    </item>
    <item>
      <title>控制 Azure ML 命令源代码的上传位置</title>
      <link>https://stackoverflow.com/questions/78643575/control-where-source-code-for-azure-ml-command-gets-uploaded</link>
      <description><![CDATA[我正在 Azure 机器学习工作室的笔记本中工作，并使用以下代码块通过 命令函数 实例化作业。
来自 azure.ai.ml 导入命令、输入、输出
来自 azure.ai.ml.entities 导入数据
来自 azure.ai.ml.constants 导入 AssetTypes

subscription_id = &quot;&lt;subscription_id&gt;&quot;
resource_group = &quot;&lt;resource_group&gt;&quot;
working = &quot;&lt;workspace&gt;&quot;
storage_account = &quot;&lt;storage_account&gt;&quot;
输入路径 = &quot;&lt;输入路径&gt;&quot;
输出路径 = &quot;&lt;输出路径&gt;&quot;

input_dict = {
&quot;input_data_object&quot;: 输入(
type=AssetTypes.URI_FILE, 
path=f&quot;azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{storage_account}/paths/{input_path}&quot;
)
}

output_dict = {
&quot;output_folder_object&quot;: 输出(
type=AssetTypes.URI_FOLDER,
path=f&quot;azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{storage_account}/paths/{output_path}&quot;,
)
}

job = command(
code=&quot;./src&quot;, 
command=&quot;python 01_read_write_data.py -v --input_data=${{inputs.input_data_object}} --output_folder=${{outputs.output_folder_object}}&quot;,
inputs=input_dict,
outputs=output_dict,
environment=&quot;&lt;asset_env&gt;&quot;,
compute=&quot;&lt;compute_cluster&gt;&quot;,
)

returned_job = ml_client.create_or_update(job)

此操作成功运行，但每次运行时，如果存储在 ./src 目录中的代码发生变化，则会将新副本上传到默认的 blob 存储帐户。我不介意这一点，但每次运行时，代码都会上传到我的 blob 存储帐户根目录下的新容器中。因此，我的默认存储帐户会因容器而变得杂乱无章。我已阅读使用 command() 函数实例化 command 对象的文档，但我没有看到可用于控制 ./src 代码上传位置的参数。有什么方法可以控制吗？]]></description>
      <guid>https://stackoverflow.com/questions/78643575/control-where-source-code-for-azure-ml-command-gets-uploaded</guid>
      <pubDate>Wed, 19 Jun 2024 16:06:28 GMT</pubDate>
    </item>
    <item>
      <title>理解 Transformers 的结果，通过梯度下降进行情境学习</title>
      <link>https://stackoverflow.com/questions/78639577/understanding-the-results-of-transformers-learn-in-context-with-gradient-descent</link>
      <description><![CDATA[我正在尝试实现这篇论文：
https://arxiv.org/pdf/2212.07677
（这是他们的代码）：
https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd
我正在努力匹配他们的实验结果。具体来说，在他们最简单的 GD 模型（单层、单头、无 softmax）上，他们在测试数据上获得了大约 0.20 的恒定低损失。从概念上讲，我不太明白为什么会这样。
据我所知，这个模型只对数据进行了一次梯度下降迭代，那么为什么它会达到如此低的损失？为什么损失在训练步骤中会保持恒定/接近恒定？我们不是在 GD 模型中训练学习率吗？]]></description>
      <guid>https://stackoverflow.com/questions/78639577/understanding-the-results-of-transformers-learn-in-context-with-gradient-descent</guid>
      <pubDate>Tue, 18 Jun 2024 20:43:45 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 tfa.losses.TripletSemiHardLoss 训练具有三重损失的暹罗网络？</title>
      <link>https://stackoverflow.com/questions/78635866/how-to-train-a-siamese-network-with-triplet-loss-using-tfa-losses-tripletsemihar</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78635866/how-to-train-a-siamese-network-with-triplet-loss-using-tfa-losses-tripletsemihar</guid>
      <pubDate>Tue, 18 Jun 2024 06:47:18 GMT</pubDate>
    </item>
    <item>
      <title>将图像置于中心并在导出时添加背景</title>
      <link>https://stackoverflow.com/questions/78581619/center-an-image-and-adding-a-background-at-export</link>
      <description><![CDATA[我想自动完成所有这些操作：

选择图像中的对象
在此对象上裁剪我的图像
裁剪为 1:1 的宽高比，在此对象周围留出一点空隙
以 800x800px 的 JPG 格式导出我的图像，我的对象位于图像中心，背景为白色。

我在 win11 64 位上
我做了什么：

安装 Python 并创建环境
安装opencv-python-headless、pillow、numpy、Pytorch以用于 CUDA 11.8
克隆存储库 segment-anything.git 并使用 PIP 安装它
下载sam_vit_b_01ec64.pth

像这样对 py 文件进行编码：
import os
import cv2
import numpy as np
from PIL import Image
from fragment_anything import sam_model_registry, SamAutomaticMaskGenerator

def load_image(image_path):
return cv2.imread(image_path)

def save_image(image, path):
cv2.imwrite(path + &#39;.jpg&#39;, image)

def select_object(image):
sam = sam_model_registry[&quot;vit_b&quot;](checkpoint=&quot;sam_vit_b_01ec64.pth&quot;)
mask_generator = SamAutomaticMaskGenerator(sam)
mask = mask_generator.generate(image)
largest_mask = max(masks, key=lambda x: x[&#39;area&#39;])
返回 largest_mask[&#39;segmentation&#39;]

def crop_to_object(image, mask):
x, y, w, h = cv2.boundingRect(mask.astype(np.uint8))
padding = 5
x = max(0, x - padding)
y = max(0, y - padding)
w = min(image.shape[1] - x, w + 2 * padding)
h = min(image.shape[0] - y, h + 2 * padding)

cropped_image = image[y:y+h, x:x+w]
返回 cropped_image

def resize_to_square(image, size=800):
h, w = image.shape[:2]
scale = size / max(h, w)
new_h, new_w = int(h * scale), int(w * scale)
resized_image = cv2.resize(image, (new_w, new_h), 插值=cv2.INTER_LANCZOS4)

new_image = np.ones((size, size, 3), dtype=np.uint8) * 255

top = (size - new_h) // 2
left = (size - new_w) // 2
bottom = top + new_h
right = left + new_w

new_image[top:top+new_h, left:left+new_w] = resized_image

return new_image

def process_image(image_path, output_path):

image = load_image(image_path)
mask = select_object(image)
cropped_image = crop_to_object(image, mask)
final_image = resize_to_square(cropped_image, 800)
save_image(final_image, output_path + &#39;.jpg&#39;)

def process_folder(input_folder, output_folder):

如果 os.path.exists(output_folder):
os.makedirs(output_folder)

对于 root、_、os.walk(input_folder) 中的文件：
对于 filename in files:
如果 filename.lower().endswith((&#39;.png&#39;, &#39;.jpg&#39;, &#39;.jpeg&#39;, &#39;.bmp&#39;, &#39;.tiff&#39;)):
input_path = os.path.join(root, filename)

relative_path = os.path.relpath(input_path, input_folder)
output_path = os.path.join(output_folder,relative_path)

output_dir = os.path.dirname(output_path)
如果 os.path.exists(output_dir):
os.makedirs(output_dir)

尝试：
process_image(input_path, output_path)
print(f&quot;已处理 {input_path}&quot;)
except Exception as e:
print(f&quot;无法处理 {input_path}：{e}&quot;)

if __name__ == &quot;__main__&quot;:
input_folder = &quot;&quot;
output_folder = &quot;&quot;
process_folder(input_folder, output_folder)

发生了什么：
我导入了基本图像，我想要预期结果，并且我获得了结果
我得到了一些不同的基本结果：

基本白色背景 -&gt; 结果
Base-nobg -&gt; 结果

有人能帮我理解我错过了什么吗？
提前谢谢，
Cyril]]></description>
      <guid>https://stackoverflow.com/questions/78581619/center-an-image-and-adding-a-background-at-export</guid>
      <pubDate>Wed, 05 Jun 2024 14:13:43 GMT</pubDate>
    </item>
    <item>
      <title>如果数据只有一个特征，则使用 array.reshape(-1, 1) 重塑数据；如果数据只有一个样本，则使用 array.reshape(1, -1)</title>
      <link>https://stackoverflow.com/questions/58663739/reshape-your-data-either-using-array-reshape-1-1-if-your-data-has-a-single-fe</link>
      <description><![CDATA[当我从我的数据中预测一个样本时，它给出了重塑错误，但我的模型具有相同的行数。这是我的代码：
import pandas as pd
from sklearn.linear_model import LinearRegression
import numpy as np
x = np.array([2.0 , 2.4, 1.5, 3.5, 3.5, 3.5, 3.5, 3.7, 3.7])
y = np.array([196, 221, 136, 255, 244, 230, 232, 255, 267])

lr = LinearRegression()
lr.fit(x,y)

print(lr.predict(2.4))

错误是
如果它包含单个样本。格式（数组））
ValueError：预期为 2D 数组，但得到的是标量数组：
array=2.4。
如果您的数据只有一个特征，则使用 array.reshape(-1, 1) 重塑数据；如果它包含一个样本，则使用 array.reshape(1, -1)。
]]></description>
      <guid>https://stackoverflow.com/questions/58663739/reshape-your-data-either-using-array-reshape-1-1-if-your-data-has-a-single-fe</guid>
      <pubDate>Fri, 01 Nov 2019 17:56:16 GMT</pubDate>
    </item>
    <item>
      <title>Python 中更快的 kNN 分类算法</title>
      <link>https://stackoverflow.com/questions/51688568/faster-knn-classification-algorithm-in-python</link>
      <description><![CDATA[我想从头开始编写自己的 kNN 算法，原因是我需要对特征进行加权。问题是，尽管删除了 for 循环并使用了内置的 numpy 功能，我的程序仍然很慢。
有人能建议一种加快速度的方法吗？我没有使用 np.sqrt 来计算 L2 距离，因为它没有必要，而且实际上会减慢整个过程。
class GlobalWeightedKNN:
&quot;&quot;&quot;
具有特征权重的 k-NN 分类器

返回：k-NN 的预测。
&quot;&quot;&quot;

def __init__(self):
self.X_train = None
self.y_train = None
self.k = None
self.weights = None
self.predictions = list()

def fit(self, X_train, y_train, k, weights): 
self.X_train = X_train
self.y_train = y_train
self.k = k
self.weights = weights

def predict(self, testing_data):
&quot;&quot;&quot;
获取查询案例的 2d 数组。

返回 k-NN 分类器的预测列表
&quot;&quot;&quot;

np.fromiter((self.__helper(qc) for qc in testing_data), float) 
return self.predictions

def __helper(self, qc):
neighbours = np.fromiter((self.__weighted_euclidean(qc, x) for x in self.X_train), float)
neighbours = np.array([neighbours]).T 
indexes = np.array([range(len(self.X_train))]).T
neighbours = np.append(indexes, neighbours, axis=1)

# 按第二列排序 - 距离
neighbours = neighbours[neighbours[:,1].argsort()] 
k_cases = neighbours[ :self.k]
indexes = [x[0] for x in k_cases]

y_answers = [self.y_train[int(x)] for x in indexes]
answer = max(set(y_answers), key=y_answers.count) # 获取最常见的值
self.predictions.append(answer)

def __weighted_euclidean(self, qc, other):
&quot;&quot;&quot;
自定义加权欧几里得距离

返回：浮点数
&quot;&quot;&quot;

return np.sum( ((qc - other)**2) * self.weights )
]]></description>
      <guid>https://stackoverflow.com/questions/51688568/faster-knn-classification-algorithm-in-python</guid>
      <pubDate>Sat, 04 Aug 2018 18:39:24 GMT</pubDate>
    </item>
    </channel>
</rss>