<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 30 Dec 2023 18:17:10 GMT</lastBuildDate>
    <item>
      <title>可教机器出口降低精度</title>
      <link>https://stackoverflow.com/questions/77736961/teachable-machine-export-decreasing-accuracy</link>
      <description><![CDATA[我试图用可教学机器制作图像识别模型，但为了测试该模型，我想将其导出到笔记本上，以便我可以快速运行它通过许多图像。然而，当我将模型导出到 Kaggle 时，导出版本的精度低于 Teachable machine 中的原始模型。我决定在两个模型上测试一张图像，在可教学机器上测试一张图像给了我正确的分类，但在 Kaggle 上我得到了错误的分类。如果我为每个人使用相同的模型，这是怎么发生的？
以下是我用于创建模型的步骤：
登录可示教机器
每类输入900张图像（6类）
在可示教机器中训练模型（400 epoch）
将模型从可示教机器导出到 TensorFlow Keras
将模型上传到 Kaggle 笔记本上
在Kaggle中，使用模型对一张图像进行分类（得到错误的分类）
在可教机器中使用模型（与导出到 Kaggle 的模型相同）对同一图像进行分类（获得正确的分类）
我怀疑 Teachable 机器可能是用 TensorFlow JS 制作的，但当它导出为 Keras 模型时，它会失去一些准确性，但我在这方面的经验很少，所以我可能是错的。
当我导出模型时，是否有办法保持可示教机器的准确性？谢谢！
我尝试使用 jupyter 笔记本和 google collab 而不是 Kaggle，但我无法弄清楚如何将文件导入其中。]]></description>
      <guid>https://stackoverflow.com/questions/77736961/teachable-machine-export-decreasing-accuracy</guid>
      <pubDate>Sat, 30 Dec 2023 18:13:38 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 层不兼容执行预测时出错</title>
      <link>https://stackoverflow.com/questions/77736223/tensorflow-layer-incompatibilty-error-while-performing-prediction</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77736223/tensorflow-layer-incompatibilty-error-while-performing-prediction</guid>
      <pubDate>Sat, 30 Dec 2023 14:27:28 GMT</pubDate>
    </item>
    <item>
      <title>我可以将什么词嵌入预训练模型或算法用于阿拉伯语，特别是人与人之间使用的语言（埃及口语）</title>
      <link>https://stackoverflow.com/questions/77736220/what-word-embeddings-pretrained-model-or-algorithm-i-can-use-with-arabic-languag</link>
      <description><![CDATA[我正在研究一个文本分类模型，其中我只有一个包含 2 列的数据集，Users_slang、formal_name
我试图将产品的每一个正式名称映射到市场上或人们之间日常使用的每一个俚语。
在对数据进行预处理、标记化之后，我正在尝试寻找适用于阿拉伯语言的最佳词嵌入方式，尤其是口语（埃及语）而不是古典阿拉伯语言
您有什么建议或建议？
我没有找到Word2Vec是否支持此类问题，但是它会起作用吗？
如果您对这种解决方案有任何建议，我将很乐意接受！
我还没有尝试实现词嵌入，但我发现在一般阿拉伯语上使用预训练模型会产生巨大的差异，我希望它能很好地工作，但我不知道会喜欢了解更多。]]></description>
      <guid>https://stackoverflow.com/questions/77736220/what-word-embeddings-pretrained-model-or-algorithm-i-can-use-with-arabic-languag</guid>
      <pubDate>Sat, 30 Dec 2023 14:26:55 GMT</pubDate>
    </item>
    <item>
      <title>高斯判别分析是否不适用于x较大且y较大的数据？</title>
      <link>https://stackoverflow.com/questions/77735867/is-gaussian-discriminant-analysis-unsuitable-for-data-where-x-is-bigger-and-y-is</link>
      <description><![CDATA[例如，x（肿瘤大小）越大，y（诊断概率）越大。这样的数据是不是不适合高斯判别分析模型。
因为这样的数据并不是当x接近平均值μ时y（诊断概率）最大化的情况]]></description>
      <guid>https://stackoverflow.com/questions/77735867/is-gaussian-discriminant-analysis-unsuitable-for-data-where-x-is-bigger-and-y-is</guid>
      <pubDate>Sat, 30 Dec 2023 12:10:56 GMT</pubDate>
    </item>
    <item>
      <title>变压器解码不正确</title>
      <link>https://stackoverflow.com/questions/77735779/incorrect-transformer-decoding</link>
      <description><![CDATA[我在序列到序列翻译任务（英语到西班牙语）上应用了标准转换器。我已启用在解码器的自注意力部分中引起屏蔽，并且我正在使用 keras_nlp 中的正弦位置嵌入层。在训练中，损失在短短 5 个时期内就下降到 0.0084，这是不寻常的，然后在解码测试句子时，所有模型生成的都是重复的 &lt;&lt;开始&gt;象征。显然，我做错了什么，但我无法识别它，所以如果有人能指出我到底错在哪里，我将不胜感激。
变压器编码器：
def Transformer_encoder(输入、head_size、num_heads、ff_dim、dropout=0)：
# 标准化和注意力
attn_output = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(输入, 输入)
attn_output = 辍学（辍学）（attn_output）
out1 = LayerNormalization(epsilon=1e-6)(输入 + attn_output)

# 前馈部分
ffn_output = 密集（ff_dim，激活 =“relu”）（out1）
ffn_output = 辍学（辍学）（ffn_output）
ffn_output = 密集(inputs.shape[-1])(ffn_output)
返回 LayerNormalization(epsilon=1e-6)(out1 + ffn_output)

变压器解码器：
def Transformer_decoder(输入、enc_outputs、head_size、num_heads、ff_dim、dropout=0)：
# 标准化和注意力
x = 层标准化（epsilon=1e-6）（输入）
mha = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)
a = mha(查询=x，键=x，值=x，use_causal_mask=True)
x = 辍学（辍学）（a）
分辨率 = x + 输入

# 编码器-解码器注意力
x = 层标准化(epsilon=1e-6)(res)
x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, enc_outputs)
x = 辍学（辍学）（x）
分辨率 = x + 分辨率

# 前馈部分
x = 层标准化(epsilon=1e-6)(res)
x = 密集(ff_dim, 激活=“relu”)(x)
x = 辍学（辍学）（x）
x = 密集(输入.形状[-1])(x)
返回 x + 分辨率

构建模型：
def build_transformer(vocab_size_eng, vocab_size_spa, max_len_eng, max_len_spa, head_size=64, num_heads=4, ff_dim=64, dropout=0.1):
# 编码器
编码器输入=输入（形状=（max_len_eng，））
x_enc = 嵌入(vocab_size_eng, head_size)(encoder_inputs)
x_enc_pos = keras_nlp.layers.SinePositionEncoding()(x_enc)
x_enc = x_enc + x_enc_pos

对于 _ 在范围（2）中：
    x_enc = Transformer_encoder(x_enc, head_size, num_heads, ff_dim, dropout)

# 解码器
解码器输入=输入（形状=（max_len_spa，））
x_dec = 嵌入(vocab_size_spa, head_size)(decoder_inputs)
x_dec_pos = keras_nlp.layers.SinePositionEncoding()(x_dec)
x_dec = x_dec + x_dec_pos

对于 _ 在范围（2）中：
    x_dec = Transformer_decoder(x_dec, x_enc, head_size, num_heads, ff_dim, dropout)

# 输出层
输出=密集（vocab_size_spa，激活=&#39;softmax&#39;）（x_dec）

# 定义并返回模型
模型 = 模型（输入=[编码器输入，解码器输入]，输出=输出）
返回模型

Colab 笔记本链接及结果供进一步参考：https://colab。 Research.google.com/drive/1MXrKkme53wLHoeriaFGY29P6pkeuB213?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/77735779/incorrect-transformer-decoding</guid>
      <pubDate>Sat, 30 Dec 2023 11:33:44 GMT</pubDate>
    </item>
    <item>
      <title>对所有特征进行编码还是仅对字符串特征进行编码？</title>
      <link>https://stackoverflow.com/questions/77735091/encode-all-features-or-just-string-features</link>
      <description><![CDATA[假设我有这个数据框：
DataFrame
当我尝试时，我发现对所有特征进行编码的 MSE 低于对某些指定特征进行编码的 MSE。但不知道哪种方式才是正确的？我想知道，我应该对 bmi、smoker 和region 使用 LabelEncoder 还是对所有功能使用 LabelEncoder？正确的做法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77735091/encode-all-features-or-just-string-features</guid>
      <pubDate>Sat, 30 Dec 2023 06:51:28 GMT</pubDate>
    </item>
    <item>
      <title>通过减少 val_loss 和增加 val_accuracy 来改进模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77735071/improving-the-model-by-decreasing-val-loss-and-increasing-val-accuracy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77735071/improving-the-model-by-decreasing-val-loss-and-increasing-val-accuracy</guid>
      <pubDate>Sat, 30 Dec 2023 06:41:04 GMT</pubDate>
    </item>
    <item>
      <title>使用JAXopt进行约束优化（非负）</title>
      <link>https://stackoverflow.com/questions/77734910/using-jaxopt-on-constrained-optimizationnon-negative</link>
      <description><![CDATA[我正在尝试对我的损失函数进行优化（使用 JAX），该函数来自一个基本物理模型，该模型是以不同速率增长的两层。因为增长率总是正的。我想对我的优化器应用非负约束。我查了一下，发现了一些关于 JAXopt 的文档。但是，我不太确定如何将其包含在我的案例中，因为我的损失函数的定义与他们的示例非常不同。
这是一个简短的代码（65 行），它捕获了我想要展示的所有功能。我想使用 JAXopt 的投影梯度确保参数 {raw_vs1, raw_vs2} 均为非负。
导入 jax.numpy 作为 jnp
从 jax 导入随机、宽松、梯度、jit
将 matplotlib.pyplot 导入为 plt
进口光税
从 jaxopt 导入 ProjectedGradient
从 jaxopt.projection 导入projection_non_negative


# 优化代码设置
尼克斯 = 1000
新台币 = 100
密钥 = random.PRNGKey(0)
dt = 0.01
reg_a = 1.0
reg_b = 1.0
reg_c = 1.0
d = 3
ds = 0.003

xs = jnp.linspace(-1.0, 1.0, Nx)
参数 = {
    &#39;raw_vs1&#39;: random.uniform(key, shape=(Nt, Nx)),
    &#39;raw_vs2&#39;: random.uniform(key, shape=(Nt, Nx))
}
目标 = jnp.power(xs, 4)
y0 = jnp.power(xs, 2)
lengths0 = jnp.power(xs, 0) * 0.001 # 将每个时间步每个点的长度初始化为 0.1

def 模拟（参数，dt）：
    vs1 = 参数[&#39;raw_vs1&#39;]
    vs2 = 参数[&#39;raw_vs2&#39;]
    vs = vs1 - vs2
    vf = (vs1 + vs2 )/ 2

    def _step(进位, t):
        y，长度=进位
        y_new = y + vs[t] * dt
        lengths_new = lengths + vf[t] * dt # 在时间步 t 更新每个索引的长度
        返回（y_new，lengths_new），（y_new，lengths_new）

    Carry = (y0, lengths0) # 具有第一个时间步长的初始进位
    (yf,_), (ys, lengths_final) = lax.scan(_step, 进位, jnp.arange(Nt))
    print(&quot;lengths_final 的形状：&quot;, lengths_final.shape)
    print(&quot;yf 的形状：&quot;, yf.shape)
    print(&quot;ys 的形状：&quot;, ys.shape)
    返回 yf, ys, lengths_final

def损失（参数，dt，reg_a = reg_a，reg_b = reg_b，reg_c = reg_c，target_length = d）：
    yf, ys, leng = 模拟(params, dt)
    dt_term = jnp.sum(jnp.power(jnp.diff(ys, axis=0), 2))
    dx_term = jnp.sum(jnp.power(jnp.diff(ys, axis=1), 2))
    target_term = jnp.power(yf - 目标, 2)
    总长度 = jnp.sum(leng[-1])
    长度项 = (总长度 - 目标长度) ** 2
    返回 reg_a*dt_term + reg_b*dx_term + jnp.sum(reg_c*target_term) + reg_c*length_term


# 优化设置
g_loss = jit(grad(损失))
优化器 = optax.adam(0.01)
opt_state = 优化器.init(params)

# 优化循环
损失_t = []
对于范围内的 i（2000）：
    梯度 = g_loss(参数, dt)
    更新， opt_state = optimizationr.update(grads, opt_state)
    params = optax.apply_updates(params, 更新)
    loss_t.append(损失(params, dt))
]]></description>
      <guid>https://stackoverflow.com/questions/77734910/using-jaxopt-on-constrained-optimizationnon-negative</guid>
      <pubDate>Sat, 30 Dec 2023 05:01:58 GMT</pubDate>
    </item>
    <item>
      <title>使用自我训练的 AI 模型生成导入定义[关闭]</title>
      <link>https://stackoverflow.com/questions/77733066/generate-import-definitions-using-a-self-trained-ai-modell</link>
      <description><![CDATA[在我们的应用程序中，我们有一个导入模块，用户可以在其中指定如何从文本文件（以及其他文件类型、XML 和 ODBC）中提取数据。与将文本文件导入 Excel 有点类似 - 但有更多的可能性。大多数文本文件都具有不符合标准的专有结构。导入模块可以处理各种结构，例如简单的 csv 文件以及嵌套的键值对等。
我们有这方面的样本数据（可能有 100 个或数百个示例）。
所以我正在考虑创建和训练一个人工智能模型，该模型可以为使用的任何新文件创建这样的导入定义。用户只需打开文件，AI 模型就会创建第一个导入定义（如何以及在何处提取应用程序中表和列的文本）。
所以这里的输入是具有未知结构的文本，输出是已定义的结构。
我在 C++ 编程方面有经验，但在 AI 方面没有经验。
如何才能做到这一点？我还无法找到一些与这个问题至少有点相似的示例或教程。]]></description>
      <guid>https://stackoverflow.com/questions/77733066/generate-import-definitions-using-a-self-trained-ai-modell</guid>
      <pubDate>Fri, 29 Dec 2023 16:48:15 GMT</pubDate>
    </item>
    <item>
      <title>在 R 中对新数据集进行预测</title>
      <link>https://stackoverflow.com/questions/77732371/make-prediction-on-new-data-set-in-r</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77732371/make-prediction-on-new-data-set-in-r</guid>
      <pubDate>Fri, 29 Dec 2023 14:03:24 GMT</pubDate>
    </item>
    <item>
      <title>在 WSL conda 环境中安装 lightgbm GPU</title>
      <link>https://stackoverflow.com/questions/77728334/install-lightgbm-gpu-in-a-wsl-conda-env</link>
      <description><![CDATA[--------------------原来的问题------------------------- --------
如何安装LightGBM？
我检查了多个来源，但仍然无法安装。
我尝试了 pip 和 conda 但都返回错误：
[LightGBM] [警告] 目前不支持在 CUDA 中使用稀疏特征。
[LightGBM] [致命] 此版本中未启用 CUDA Tree Learner。
请使用 CMake 选项 -DUSE_CUDA=1 重新编译

我尝试过的内容如下：
git clone --recursive https://github.com/microsoft/LightGBM
cd LightGBM/
mkdir -p 构建
光盘构建
cmake -DUSE_GPU=1 ..
使-j$(nproc)
cd ../python-package
点安装。

-------------------- 下面是我的解决方案（cuda）--------------------- ------------
谢谢各位的回复。我尝试了一些方法，最终效果如下：
首先，确保已安装 cmake（在 wsl 下）：
sudo apt-get update
sudo apt-get 安装 cmake
须藤 apt-get 安装 g++

那么，
git clone --recursive https://github.com/microsoft/LightGBM
cd光GBM
mkdir 构建
光盘构建
cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..
使-j4

目前，安装尚未链接到任何 conda env。为此，在 vscode 终端（或仍然是 wsl）下，conda 激活一个 env，然后创建一个 jupyter 笔记本进行测试：
确保lib_lightgbm.so位于LightGBM/python-package下，如果没有，则复制到该文件夹​​中。
然后在jupyter笔记本中：
导入系统
将 numpy 导入为 np
sys.path.append(&#39;/mnt/d/lgm-test2/LightGBM/python-package&#39;)
将 lightgbm 导入为 lgb

最后一点是，您可以参考 Jame 的回复，设备需要设置为“cuda”而不是“gpu”。]]></description>
      <guid>https://stackoverflow.com/questions/77728334/install-lightgbm-gpu-in-a-wsl-conda-env</guid>
      <pubDate>Thu, 28 Dec 2023 17:34:48 GMT</pubDate>
    </item>
    <item>
      <title>AutoTrain 高级 CLI：错误：无法识别的参数：--fp16 --use-int4</title>
      <link>https://stackoverflow.com/questions/77664921/autotrain-advanced-cli-error-unrecognized-arguments-fp16-use-int4</link>
      <description><![CDATA[我目前在使用提供的自动训练工具在 Colab 笔记本中使用 LLM 模型微调数据时遇到问题。错误消息表明 autotrain 无法识别参数“--fp16”和“--use-int4”。我已经检查了文档和语法，但问题仍然存在。您能否提供解决此问题的指导或提供有关任何潜在解决方案的见解？谢谢。
/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13：
 UserWarning：无法加载图像Python扩展：&#39;/usr/local/lib/python3.10/dist-packages/torchvision/image.so：未定义符号：_ZN3c104cuda9SetDeviceEi&#39;如果您不打算使用`torchvision中的图像功能。 io`，你可以忽略这个警告。否则，您的环境可能有问题。在从源代码构建“torchvision”之前，您是否安装了“libjpeg”或“libpng”？ warn( 用法: autotrain  [] AutoTrain 高级 CLI: 错误: 无法识别的参数: --fp16 --use-int4

错误的屏幕截图
直到昨天，这段代码在这个 https://github.com/huggingface/autotrain-advanced 存储库中给出的 colab 笔记本上运行良好微调LLM，现在出现此错误。]]></description>
      <guid>https://stackoverflow.com/questions/77664921/autotrain-advanced-cli-error-unrecognized-arguments-fp16-use-int4</guid>
      <pubDate>Fri, 15 Dec 2023 07:53:31 GMT</pubDate>
    </item>
    <item>
      <title>整洁模型变量的重要性变量的函数，工作流程[关闭]</title>
      <link>https://stackoverflow.com/questions/73010592/function-for-importance-variables-of-tidy-models-variables-workflow</link>
      <description><![CDATA[我需要一个在变量重要性工作流中使用的函数，该函数返回类似于 %IncMSE 的内容，我使用了 VIP 包中的函数，但它只返回绘图，我想要列名格式及其重要性在侧面。以下是我需要的输出格式：
]]></description>
      <guid>https://stackoverflow.com/questions/73010592/function-for-importance-variables-of-tidy-models-variables-workflow</guid>
      <pubDate>Sun, 17 Jul 2022 09:17:02 GMT</pubDate>
    </item>
    <item>
      <title>MLFLOW 工件存储在 ftp 服务器上但未显示在 ui 中</title>
      <link>https://stackoverflow.com/questions/68728492/mlflow-artifacts-stored-on-ftp-server-but-not-showing-in-ui</link>
      <description><![CDATA[我在远程跟踪服务器上训练期间使用 MLFLOW 存储一些参数和指标。现在我还尝试添加一个 .png 文件作为工件，但由于 MLFLOW 服务器远程运行，我将该文件存储在 ftp 服务器上。我通过以下方式提供了 ftp 服务器地址和 MLFLOW 路径：
mlflow 服务器 --backend-store-uri sqlite:///mlflow.sqlite --default-artifact-root ftp://user:password@1.2.3.4/artifacts/ --host 0.0.0.0 &amp;

现在我训练一个网络并通过运行来存储工件：
mlflow.set_tracking_uri(remote_server_uri)
mlflow.set_experiment(“默认”)
mlflow.pytorch.autolog()

使用 mlflow.start_run()：
    mlflow.log_params(flow_params)
    训练师.fit(模型)
    训练器.test()
    mlflow.log_artifact(“confusion_matrix.png”)
mlflow.end_run()

我将 .png 文件保存在本地，然后使用 mlflow.log_artifact(“confusion_matrix.png”) 将其记录到与实验对应的右侧文件夹中的 ftp 服务器。到目前为止，一切正常，只是该工件没有显示在在线 mlflow ui 中。记录的参数和指标正常显示。工件面板保持空白，仅显示
未记录任何工件
使用日志工件 API 存储 MLflow 运行的文件输出。

我发现了类似的线程，但仅限于在本地 mlflow 存储上遇到相同问题的用户。不幸的是，我无法将这些修复应用于我的问题。有人知道如何解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/68728492/mlflow-artifacts-stored-on-ftp-server-but-not-showing-in-ui</guid>
      <pubDate>Tue, 10 Aug 2021 14:15:42 GMT</pubDate>
    </item>
    <item>
      <title>如何将 Tensorflow model.json 转换为 model.pb 文件</title>
      <link>https://stackoverflow.com/questions/61344113/how-to-convert-tensorflow-model-json-to-model-pb-file</link>
      <description><![CDATA[我正在尝试将 model.json 格式的张量流模型转换为 model.pb 格式。试图寻找好的来源，但没有找到。我有 model.json 文件和二进制权重文件。最初我对它们进行了转换，以便它可以在浏览器上运行，但现在需要它们作为 .pb 文件。请帮忙。 
https://www.tensorflow.org/js/tutorials/conversion/import_saved_model  ]]></description>
      <guid>https://stackoverflow.com/questions/61344113/how-to-convert-tensorflow-model-json-to-model-pb-file</guid>
      <pubDate>Tue, 21 Apr 2020 12:57:33 GMT</pubDate>
    </item>
    </channel>
</rss>