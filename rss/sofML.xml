<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 22 Mar 2024 09:15:11 GMT</lastBuildDate>
    <item>
      <title>使用 Minkowski 距离查找人脸识别的决策阈值</title>
      <link>https://stackoverflow.com/questions/78204941/find-decision-threshold-for-face-recognition-using-minkowski-distance</link>
      <description><![CDATA[假值的明可夫斯基距离
真实值的明可夫斯基距离
如何计算阈值来判断该值的真假？我知道是否 dist &lt;阈值为真，否则不是。但是我如何知道阈值或可以进行哪些训练来找到最佳预测阈值？
我尝试过使用精确召回曲线，但阈值令人困惑，我需要可以直接与我得到的明可夫斯基距离结果进行比较的阈值。]]></description>
      <guid>https://stackoverflow.com/questions/78204941/find-decision-threshold-for-face-recognition-using-minkowski-distance</guid>
      <pubDate>Fri, 22 Mar 2024 08:33:18 GMT</pubDate>
    </item>
    <item>
      <title>结合在不同数据上训练的两个模型</title>
      <link>https://stackoverflow.com/questions/78204630/combining-two-models-trained-on-different-data</link>
      <description><![CDATA[我使用相同的深度学习网络训练了 2 个不同的模型，但用于训练模型的数据集不同。
模型 1：用于检测在数据集 1 上训练的人脸的单类模型
模型 2：用于检测在数据集 2 上训练的眼睛的单类模型
我想用这两个类训练一个模型，以节省推理时间和内存。训练新模型的问题是，数据集 1 中标记/注释的类未在数据集 2 中注释，反之亦然。因此，如果我组合两个数据集来训练单个模型，则模型的性能将降低数据中丢失的标签。我没有足够的带宽来标记/注释丢失的图像，因为两个数据集中的图像数量都超过 10 万。
欢迎任何建议/解决方案。
我已经尝试通过使用在数据集 1 上训练的模型来预测并使用数据集 2 上的预测标签作为地面实况来自动化注释过程，但由于模型的错误预测也被接受为地面实况，因此此过程没有多大帮助模型准确率。]]></description>
      <guid>https://stackoverflow.com/questions/78204630/combining-two-models-trained-on-different-data</guid>
      <pubDate>Fri, 22 Mar 2024 07:24:15 GMT</pubDate>
    </item>
    <item>
      <title>使用神经网络进行谓词</title>
      <link>https://stackoverflow.com/questions/78204356/predicate-using-neural-network</link>
      <description><![CDATA[我有这两列
日期结果
Px110000Dy10.16281
Px220000Dy20.20151
Px330000Dy30.2288
Px440000Dy40.26576
Px550000Dy50.27538
Px660000Dy60.29192
Px770000Dy70.31618
Px880000Dy80.33647
Px990000Dy90.34819
Px10100000Dy100.3508

如何在神经网络中推导模型来预测未来的结果？
我要使用神经网络 TensorFlow]]></description>
      <guid>https://stackoverflow.com/questions/78204356/predicate-using-neural-network</guid>
      <pubDate>Fri, 22 Mar 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>为 CNN 注释图像有多大必要？如果可以，最快的方法是什么？</title>
      <link>https://stackoverflow.com/questions/78204285/how-necessary-is-it-to-annotate-images-for-a-cnn-if-so-whats-the-fastest-meth</link>
      <description><![CDATA[我有一个带有 keras 和 tensorflow 的 CNN，并且很好奇注释图像的必要性。我的数据集由大约 35k 个图像（7 个类别）组成，单手注释每个图像会花费太多时间。如果有必要，注释图像最快的方法是什么？另外，我应该使用什么类型的注释（例如 bboxes）？
目前我的 CNN 通常具有大约 93^ val 准确度和 97% 训练准确度，但现实生活中的结果和混淆矩阵表明它表现不佳（平均精度约为 40%）。注释值得花时间吗？]]></description>
      <guid>https://stackoverflow.com/questions/78204285/how-necessary-is-it-to-annotate-images-for-a-cnn-if-so-whats-the-fastest-meth</guid>
      <pubDate>Fri, 22 Mar 2024 05:54:54 GMT</pubDate>
    </item>
    <item>
      <title>时间序列滚动窗口功能</title>
      <link>https://stackoverflow.com/questions/78204216/time-series-rolling-windows-feature</link>
      <description><![CDATA[我正在用 Python 创建机器学习模型，但有一些问题。如果我根据我的销售额（目标）列创建滚动平均值特征，是否有必要对其进行移动？
举个例子：
假设我的数据集中有第 01~10 天。例如，如果我在第 10 天的行中创建 7 天的平均滚动窗口列，它将考虑第 7 天作为该行的值来计算滚动平均值。现在，如果我要预测第 11 天，即明天，我需要这一天的销售值才能获得滚动平均值，这没有意义。
因此，我认为始终获取最后 7 天而不考虑当前的情况更有意义。
有人可以帮忙吗？
我在 Python 上尝试过，但我不明白添加此功能的最常见方法是什么]]></description>
      <guid>https://stackoverflow.com/questions/78204216/time-series-rolling-windows-feature</guid>
      <pubDate>Fri, 22 Mar 2024 05:35:21 GMT</pubDate>
    </item>
    <item>
      <title>给定标签集之外的短 2-3 个标记文本或用户搜索查询的序列标签 - NER</title>
      <link>https://stackoverflow.com/questions/78204207/sequence-labelling-for-short-2-3-token-text-or-user-search-queries-out-of-given</link>
      <description><![CDATA[我正在从事一个 NER 项目，并一直在尝试开发一个可部署的模型。
我有 3 种不同类型实体的虚拟电子商务数据，每个实体都有大约 1K 个子实体。训练数据（大小约为 200K）是通过 3K 标签的组合综合创建的。
我尝试使用查询分类模型（带有 3K 标签）开发 FLAIR 序列标签。
FLAIR 模型 （F1 得分：60%） 的表现严重低于分类模型（F1 得分：80%） ）。
我不愿意开发序列标签模块的原因是因为我希望序列标签器也能够检测并提出新实体。
你能帮助我解决哪里可能出错以及我可以尝试哪些其他模型吗？]]></description>
      <guid>https://stackoverflow.com/questions/78204207/sequence-labelling-for-short-2-3-token-text-or-user-search-queries-out-of-given</guid>
      <pubDate>Fri, 22 Mar 2024 05:30:33 GMT</pubDate>
    </item>
    <item>
      <title>Apache Spark 在深度学习模型训练阶段的范围</title>
      <link>https://stackoverflow.com/questions/78204101/apache-sparks-scope-in-deep-learning-model-training-phase</link>
      <description><![CDATA[我注意到 Apache Spark 被大量用于训练数据准备，但我很好奇它与 PyTorch/TensorFlow 一起在训练阶段的潜在作用，特别是在同时具有 CPU 和 GPU 的环境中。我想知道 Spark 在数据加载或缓存方面是否比 PyTorch/TensorFlow 有任何优势，特别是在分布式训练场景中。
虽然 PyTorch 和 TensorFlow 都支持分布式训练，但我很想知道 Spark 的功能是否可以提高性能或减少延迟，特别是在处理可能超出 GPU 内存容量的大型数据集时。]]></description>
      <guid>https://stackoverflow.com/questions/78204101/apache-sparks-scope-in-deep-learning-model-training-phase</guid>
      <pubDate>Fri, 22 Mar 2024 04:51:30 GMT</pubDate>
    </item>
    <item>
      <title>在 aws elastic beanstalk 中创建环境时遇到 docker 错误</title>
      <link>https://stackoverflow.com/questions/78204096/facing-docker-error-while-creating-environment-in-aws-elastic-beanstalk</link>
      <description><![CDATA[实际上，我正在 Beanstalk 中使用 Docker 部署 ML 模型。首先，我将 Docker 镜像（包含我的 ML 模型）上传到 Docker Hub。然后，我使用 docker-compose.yml 将其部署到 Beanstalk 中。在 Beanstalk 中，我使用 Docker 作为平台，并且我的模型需要 GPU 支持。为此，我使用了深度学习 AMI GPU CUDA 11.5.2 (Amazon Linux 2) 20230104，它是通过 NVIDIA CUDA、cuDNN、NCCL、GPU 驱动程序、Docker、NVIDIA-Docker 和 EFA 支持构建的。但是，当我使用此配置构建环境时，遇到以下错误：
**[ERROR]** 执行命令 [app-deploy] - [Track pids in healthd] 期间发生错误。停止运行该命令。错误：更新进程 [docker eb-docker-compose-events eb-docker-compose-log eb-docker-events cfn-hup healthd] pid 符号链接失败，出现错误 读取 pid 源文件 /var/pids/docker.pid 失败，出现错误:open /var/pids/docker.pid: 没有这样的文件或目录。

意味着我的环境正在构建，但它给出了错误消息，例如：Env 构建成功，但有一些错误。我在 eb.engine.log 中发现了此错误消息。
此外，我通过 SSH 检查了 EC2 实例，它显示 NVIDIA 驱动程序、NVIDIA CUDA 和 Docker 已安装（使用以下命令验证：nvidia-smi、docker -v）。我尝试了多种不同的深度学习 AMI，但所有这些 AMI 都遇到了同样的问题。另外，在尝试不同的 AMI 时，我注意到一件奇怪的事情：如果我使用默认设置（例如使用默认 Docker AMI 的 Docker 平台）构建环境，它会成功构建，不会出现任何错误。但是，当我在配置中传递不同的 AMI ID 时，无法正确构建环境。
我是 AWS Elastic Beanstalk 的新手，但我已经彻底准备好官方 Beanstalk 文档。尽管如此，我相信我在环境创建过程中可能会遗漏一些东西。有谁知道如何解决这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/78204096/facing-docker-error-while-creating-environment-in-aws-elastic-beanstalk</guid>
      <pubDate>Fri, 22 Mar 2024 04:49:51 GMT</pubDate>
    </item>
    <item>
      <title>如何将我的 fastai resnet50/vision_learner 训练模型导出到 torchserve 中？</title>
      <link>https://stackoverflow.com/questions/78203794/how-do-i-export-my-fastai-resnet50-vision-learner-trained-model-into-torchserve</link>
      <description><![CDATA[我的目标是将我用 Fastai 训练的模型部署到 Torchserve 中。我正在关注 本教程，但卡在了他为 pytorch 创建模型类的部分。
他提到要在 Torchserve 中运行我们的模型，我们需要以下内容：

模型类
从 pytorch 导出的权重（pth 文件）
处理程序

其中，我得到两个：重量和处理程序。然而，我陷入困境的是模型类。他创建了一个类文件，但我不知道他从哪里获得DynamicUnet作为该类的基础，也不知道他如何将该类与unet_learner混合以创建自定义PyTorch模型类。你能帮我为在学习器 vision_learner 下训练的模型和 resnet50 的预训练模型建立一个模型类吗？]]></description>
      <guid>https://stackoverflow.com/questions/78203794/how-do-i-export-my-fastai-resnet50-vision-learner-trained-model-into-torchserve</guid>
      <pubDate>Fri, 22 Mar 2024 02:55:25 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么类型的人工智能模型来生成练习题？</title>
      <link>https://stackoverflow.com/questions/78203711/what-type-of-ai-model-should-i-use-to-generate-practice-questions</link>
      <description><![CDATA[我有一组英语多项选择题，我想使用 AI 生成更多问题来测验自己。我知道网上有一些平台可以实现这一点，但我想挑战自己，创建自己的简单人工智能架构。在对它进行一些英语问题训练后，我希望它能够生成新问题来帮助我学习。
我应该使用哪种机器学习/智能模型作为基线？非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/78203711/what-type-of-ai-model-should-i-use-to-generate-practice-questions</guid>
      <pubDate>Fri, 22 Mar 2024 02:17:43 GMT</pubDate>
    </item>
    <item>
      <title>人工智能物体检测</title>
      <link>https://stackoverflow.com/questions/78203585/ai-object-detection</link>
      <description><![CDATA[我正在尝试使用计算机视觉和人工智能来识别图像中的硬币。
我使用的货币是波斯尼亚货币，问题是一些硬币的颜色和设计相同，唯一的区别是它们的大小。
我知道答案可能是否定的，但是有没有办法可以使用它们的大小来区分它们？]]></description>
      <guid>https://stackoverflow.com/questions/78203585/ai-object-detection</guid>
      <pubDate>Fri, 22 Mar 2024 01:29:24 GMT</pubDate>
    </item>
    <item>
      <title>lightfm python 依赖项使用</title>
      <link>https://stackoverflow.com/questions/78203344/lightfm-python-dependency-usage</link>
      <description><![CDATA[将 numpy 导入为 np
从 lightfm.datasets 导入 fetch_movielens
从 lightfm 导入 LightFM

数据 = fetch_movielens(min_ rating=4.0)

打印（repr（数据[&#39;火车&#39;]））
打印（repr（数据[&#39;测试&#39;]））

模型 = LightFM(损失=&#39;扭曲&#39;)

model.fit(data[&#39;train&#39;], epochs=30, num_threads=2)

defsample_recommendation（模型，数据，user_ids）：
    n_users, n_items = 数据[&#39;train&#39;].shape
    
    对于 user_ids 中的 user_id：
        known_positives = data[&#39;item_labels&#39;][data[&#39;train&#39;].tocsr()[user_id].indices]
        
        分数 = model.predict(user_id, np.arange(n_items))
        
        # 修复此处的标签索引
        top_items = 数据[&#39;item_labels&#39;][np.argsort(-scores)]
        
        print(&quot;用户 %s&quot; % user_id)
        print(&quot;已知的积极结果：&quot;)

        对于known_positives[:3]中的x：
            打印(“%s”%x)
            
        print(&quot;推荐：&quot;)

        # 打印最推荐的商品
        对于 top_items[:3] 中的 x：
            打印(“%s”%x)
            
样本推荐（模型，数据，[3,10,56]）


如果我运行我的代码，只输出电影标题的第一个字母，它应该是完整的电影标题，如何修复它？
我问过ai其中一个chat gpt，他们给出代码建议后，代码仍然不起作用，只显示每部电影的第一个字母。]]></description>
      <guid>https://stackoverflow.com/questions/78203344/lightfm-python-dependency-usage</guid>
      <pubDate>Thu, 21 Mar 2024 23:44:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么从 RBF SVM 模型中提取的 Shap 值有时都等于 0？</title>
      <link>https://stackoverflow.com/questions/78193811/why-are-the-shap-values-extracted-from-a-rbf-svm-model-sometimes-all-equal-to-0</link>
      <description><![CDATA[1我正在测试分类任务的不同模型。 （据我所知）仅当我从 RBF SVM 模型（图中的示例）提取形状值时（并非总是），才会出现此问题。
我计划获得形状值图的更经典表示，其中所有点在 y 轴上具有不同的值。在这里，似乎我所有的点都有 0 值
我的代码有点不同，但它的作用如下：
explainer = shap.KernelExplainer(model[&#39;model&#39;].predict, shap.kmeans(X_processed, 100))
shap_values = 解释器.shap_values(X_processed)

shap_fig = shap.summary_plot(shap_values=shap_values, features=X_processed, feature_names=feature_names, show=False, max_display=10)
图, ax = plt.gcf(), plt.gca()
ax.set_yticklabels(标签，旋转=45，字体大小=11)
ax.set_xticklabels(np.round(ax.get_xticks(), 2),fontsize=11)
plt.title(plot_title, 字体大小 = 18)
Fig.tight_layout(pad=0, w_pad=0, h_pad=0)
图.show()

在本例中，以下是图像形式的结果。仅包含 0 个值的形状值图
我不明白的是，对于另一个数据集，即使使用 RBF SVM 模型，形状值图也符合预期。
我已经尝试使用“X_preprocessed”而不是“shap.kmeans(X_processed, 100)”在 shap.KernelExplainer() 中，我在 shap.kmeans() 中尝试了不同的数字作为参数，但结果保持不变。
有人已经遇到过这种行为吗？或者有什么想法为什么会发生这种情况？]]></description>
      <guid>https://stackoverflow.com/questions/78193811/why-are-the-shap-values-extracted-from-a-rbf-svm-model-sometimes-all-equal-to-0</guid>
      <pubDate>Wed, 20 Mar 2024 13:41:46 GMT</pubDate>
    </item>
    <item>
      <title>ViT 模型的 HuggingFace Inference API 问题 - “图像特征提取”错误</title>
      <link>https://stackoverflow.com/questions/78192634/issue-with-huggingface-inference-api-for-vit-model-image-feature-extraction</link>
      <description><![CDATA[我的 Vision Transformer (ViT) 模型 rshrott/vit-base-renovation2 的推理 API 遇到问题。
https://huggingface.co/rshrott/vit-base-renovation2 
当我尝试使用 API 时，收到以下错误：
&lt;前&gt;&lt;代码&gt;{
“错误”：“HfApiJson（反序列化（错误（“未知变体图像特征提取，预期音频分类，音频到音频，音频源分离，自动语音识别，特征提取之一，文本分类、标记分类、问答、翻译、摘要、文本生成、text2text-生成、填充掩模、零样本分类、零样本图像分类、会话、表格问答、图像分类、图像分割、图像到文本、文本到语音、...视觉问答、视频分类、文档问答、图像到图像、深度估计，行：1 ，栏目：318）））”
}

有趣的是，当我直接在 Python 中使用 Transformer 管道时，模型按预期工作：
从转换器导入管道
从 PIL 导入图像
导入请求

管道=管道（模型=“rshrott/vit-base-renovation2”）
url = &#39;https://example.com/image.jpeg&#39;
图像= Image.open(requests.get(url,stream=True).raw)
preds = 管道(图像)

此代码运行没有任何问题并返回预期的预测。但是，通过推理 API 使用同一模型时会遇到错误。我怀疑可能存在与预期任务类型相关的配置问题，但我不确定如何解决它。
为什么会出现此错误以及如何修复它？我已经检查了型号卡和配置，但我似乎无法找到“图像特征提取”的来源或原因。]]></description>
      <guid>https://stackoverflow.com/questions/78192634/issue-with-huggingface-inference-api-for-vit-model-image-feature-extraction</guid>
      <pubDate>Wed, 20 Mar 2024 10:44:57 GMT</pubDate>
    </item>
    <item>
      <title>使用遗传算法优化面部情绪识别模型超参数</title>
      <link>https://stackoverflow.com/questions/78157230/optimizing-facial-emotion-recognition-model-hyperparameters-using-genetic-algori</link>
      <description><![CDATA[我正在构建一个面部情绪识别系统，可以对快乐、悲伤、愤怒、惊讶等情绪进行分类。我已经使用 TensorFlow/Keras 训练了一个卷积神经网络模型，目前它的准确率达到了50%左右。然而，我相信微调超参数可能会进一步提高准确性。
现在，我有兴趣优化模型的超参数以实现更高的准确性。我听说过使用遗传算法进行超参数优化，但我不确定如何继续。有人可以指导我如何应用遗传算法来微调模型的超参数吗？具体来说，如何修改我的代码以纳入遗传算法以进行超参数优化？
这是我的代码摘要：
将张量流导入为 tf
从tensorflow.keras.preprocessing.image导入ImageDataGenerator
从tensorflow.keras导入模型，层

# 数据增强
增强器 = ImageDataGenerator(
    重新缩放=1.0/255，
    剪切范围=0.2，
    缩放范围=0.2，
    水平翻转=真
）

# 加载数据并将图像大小调整为 48x48 像素
Augmented_trained_data = Augmentor.flow_from_directory(
    “面部识别数据集/训练”，
    目标大小=(48, 48),
    批量大小=32，
    color_mode=“灰度”，
    class_mode=“分类”
）

Augmented_validation_data = Augmentor.flow_from_directory(
    “面部识别数据集/验证”，
    目标大小=(48, 48),
    批量大小=32，
    color_mode=“灰度”，
    class_mode=“分类”
）

Augmented_testing_data = Augmentor.flow_from_directory(
    “面部识别数据集/测试”，
    目标大小=(48, 48),
    批量大小=32，
    color_mode=“灰度”，
    class_mode=“分类”
）

# 模型定义
模型 = models.Sequential([
    层.Conv2D(32, (2, 2), 激活=“relu”, input_shape=(48, 48, 1)),
    层.MaxPool2D((2, 2)),
    层.Conv2D(64, (2, 2), 激活=“relu”),
    层.MaxPool2D((2, 2)),
    层.Conv2D(128, (2, 2), 激活=“relu”),
    层.MaxPool2D((2, 2)),
    层.Flatten(),
    层.密集（128，激活=“relu”），
    层数.Dropout(0.25),
    层.密集（6，激活=“softmax”）
]）

# 模型编译
模型.编译(
    优化器=&#39;亚当&#39;,
    损失=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
    指标=[“准确度”]
）

# 模型训练
模型.拟合(
    增强训练数据，
    验证数据=增强验证数据，
    纪元=10
）

# 模型评估
test_loss, test_accuracy = model.evaluate(augmented_testing_data)
print(f&quot;测试准确度: {test_accuracy * 100:.2f}%&quot;)&#39;&#39;&#39;


]]></description>
      <guid>https://stackoverflow.com/questions/78157230/optimizing-facial-emotion-recognition-model-hyperparameters-using-genetic-algori</guid>
      <pubDate>Wed, 13 Mar 2024 22:53:36 GMT</pubDate>
    </item>
    </channel>
</rss>