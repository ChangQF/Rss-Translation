<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 26 Mar 2024 15:13:49 GMT</lastBuildDate>
    <item>
      <title>衡量类别预测的一致性</title>
      <link>https://stackoverflow.com/questions/78226139/measure-class-prediction-consistency</link>
      <description><![CDATA[假设我对同一对象有一系列类别预测。然后我想测量该序列的一致性。例如，像 class_a, class_a, class_a, class_a 这样的一致预测序列应该给出高分。 class_a、class_b、class_a、class_c 等不一致的序列应该会导致较低的分数。
最好它适用于任意数量的可能类别，并且还考虑预测置信度。序列 class_a (0.9)、class_b (0.9)、class_a (0.9)、class_c (0.9) 的得分应低于 class_a (0.9)、class_b (0.2)、class_a (0.8) ), class_c (0.3)，因为当预测与高置信度不一致时，它就不好。
我可以自己构建一些东西，但我想知道是否有一个标准的 sklearn （或类似）函数？提前致谢！
对这个问题的评论建议Spearman 相关系数 或 坎德尔相关系数。我也会研究一下。]]></description>
      <guid>https://stackoverflow.com/questions/78226139/measure-class-prediction-consistency</guid>
      <pubDate>Tue, 26 Mar 2024 14:41:53 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlowLite 错误：“Interpreter.GetOutputTensor(int)”由于其保护级别而无法访问</title>
      <link>https://stackoverflow.com/questions/78224205/tensorflowlite-error-interpreter-getoutputtensorint-is-inaccessible-due-to</link>
      <description><![CDATA[我有一个 tflite 模型，它将图像作为输入并预测其类别。我希望它在我的统一项目中使用。当我使用chatgpt给出的代码时，出现以下错误。任何人都可以帮忙，我对unity和c#不太了解
Assets\Samples\Detection\Scripts\PythonBridge.cs(72,9)：错误 CS0246：找不到类型或命名空间名称“Tensor”（您是否缺少 using 指令或程序集引用？）
Assets\Samples\Detection\Scripts\PythonBridge.cs(72,43)：错误 CS0122：“Interpreter.GetOutputTensor(int)”由于其保护级别而无法访问
使用UnityEngine；
使用 TensorFlowLite；
使用系统.IO；
使用 System.Collections.Generic；

公共类对象检测：MonoBehaviour
{
    [序列化字段]
    [FilePopup(“*.tflite”)]
    公共字符串 modelPath = “model.tflite”;


    [序列化字段]
    私有 TextAsset 标签文件；

    [序列化字段]
    私有Texture2D输入图像；

    私人口译员；
    私有列表&lt;字符串&gt;标签;

    私有常量 int IMAGE_SIZE = 224;
    私有常量 int 通道 = 3;

    私有无效开始（）
    {
        加载模型();
        加载标签（）；
        预处理图像();
        运行推理（）；
    }

    私有无效LoadModel（）
    {
        解释器=新解释器(File.ReadAllBytes(modelPath));
    }

    私有无效 LoadLabels()
    {
        标签 = new List();
        使用 (StringReader reader = new StringReader(labelFile.text))
        {
            串线；
            while ((line = reader.ReadLine()) != null)
            {
                labels.Add(line.Trim());
            }
        }
    }

    私有无效 PreprocessImage()
    {
        Texture2D resizedImage = ResizeImage(inputImage, IMAGE_SIZE, IMAGE_SIZE);
        Color32[] 像素 = resizedImage.GetPixels32();

        float[] imgArray = new float[IMAGE_SIZE * IMAGE_SIZE * CHANNELS];
        for (int i = 0; i &lt; 像素.长度; i++)
        {
            imgArray[i * 3] = 像素[i].r / 255.0f；
            imgArray[i * 3 + 1] = 像素[i].g / 255.0f;
            imgArray[i * 3 + 2] = 像素[i].b / 255.0f;
        }

        解释器.SetInputTensorData(0, imgArray);
    }

    私有无效 RunInference()
    {
        解释器.Invoke();

        // 检索输出并处理预测
        张量输出Tensor =terpreter.GetOutputTensor(0);
        float[] 结果 = outputTensor.Data();

        // 寻找概率最高的类
        int 最大索引 = 0;
        浮动最大概率 = 0f;
        for (int i = 0; i &lt; results.Length; i++)
        {
            if (结果[i] &gt; 最大概率)
            {
                最大概率=结果[i]；
                最大索引 = i;
            }
        }

        字符串预测标签=标签[maxIndex];
        Debug.Log(“预测对象：”+predictedLabel);
    }

    私有Texture2D ResizeImage（Texture2D源，int宽度，int高度）
    {
        RenderTexture rt = RenderTexture.GetTemporary(宽度，高度，24);
        RenderTexture.active = rt;
        Graphics.Blit(源, rt);
        Texture2D 结果 = new Texture2D(宽度, 高度);
        结果.ReadPixels(new Rect(0, 0, 宽度, 高度), 0, 0);
        结果.Apply();
        RenderTexture.active = null;
        RenderTexture.ReleaseTemporary(rt);
        返回结果；
    }
}


我尝试在chatgpt中解决，但它没有更新。我在 c# 中使用了 .h5 和 python，并得到了输出。但导出为 apk 时不起作用。于是搜了一下，发现tensorflowlite可以解决这个问题]]></description>
      <guid>https://stackoverflow.com/questions/78224205/tensorflowlite-error-interpreter-getoutputtensorint-is-inaccessible-due-to</guid>
      <pubDate>Tue, 26 Mar 2024 09:24:43 GMT</pubDate>
    </item>
    <item>
      <title>此代码不适用于tensorflow 2.16.0+版本</title>
      <link>https://stackoverflow.com/questions/78223936/this-code-is-not-working-on-tensorflow-2-16-0-version</link>
      <description><![CDATA[检查点 = ModelCheckpoint(
    &#39;./base.model&#39;,
    监视器=&#39;val_accuracy&#39;,
    详细=1，
    save_best_only=真，
    模式=&#39;最大&#39;,
    save_weights_only=假,
    保存频率=1
）
提前停止=提前停止(
    监视器=&#39;val_loss&#39;,
    最小增量=0.001，
    耐心=30，
    详细=1，
    模式=&#39;自动&#39;
）

opt1 = tf.keras.optimizers.Adam()

回调= [检查点，提前停止]

这不适用于tensorflow 2.16.1
但是，正在 google colab 上开发 2.15.0
我如何修复我的代码或如何安装tensorflow 2.15.0？
我尝试了pip install tensorflow=2.15.0
但是，它显示错误]]></description>
      <guid>https://stackoverflow.com/questions/78223936/this-code-is-not-working-on-tensorflow-2-16-0-version</guid>
      <pubDate>Tue, 26 Mar 2024 08:41:28 GMT</pubDate>
    </item>
    <item>
      <title>无法在 LightGBM 中检索 best_iteration</title>
      <link>https://stackoverflow.com/questions/78223783/cant-retrieve-best-iteration-in-lightgbm</link>
      <description><![CDATA[我使用 Optuna 来优化我的 LightGBM 模型。同时，我使用 LightGBM 回调 early_stopping(50) 提前停止迭代。
这是我的代码：
def 目标（试验、train_set、valid_set、num_iterations）：
    
    参数 = {
        &#39;目标&#39;：&#39;二进制&#39;，
        &#39;任务&#39;:&#39;训练&#39;,
        &#39;提升&#39;：&#39;gbdt&#39;，
        &#39;度量&#39;：[&#39;auc&#39;]，
        &#39;n_estimators&#39;：num_iterations，
        “冗长”：-1，
        &#39;feature_pre_filter&#39;：假，
        &#39;学习率&#39;: Trial.suggest_float(&#39;学习率&#39;, 0.01, 0.3),
        &#39;num_leaves&#39;: Trial.suggest_int(&#39;num_leaves&#39;, 2, 256),
        &#39;最大深度&#39;: Trial.suggest_int(&#39;最大深度&#39;, 3, 12),
        &#39;min_data_in_leaf&#39;: Trial.suggest_int(&#39;min_data_in_leaf&#39;, 20, 10000),
        &#39;lambda_l1&#39;: Trial.suggest_float(&#39;lambda_l1&#39;, 1e-10, 10.0, log=True),
        &#39;lambda_l2&#39;: Trial.suggest_float(&#39;lambda_l2&#39;, 1e-10, 10.0, log=True),
        &#39;min_gain_to_split&#39;: Trial.suggest_float(&#39;min_gain_to_split&#39;, 0, 15),
        &#39;feature_fraction&#39;: Trial.suggest_float(&#39;feature_fraction&#39;, 0.2, 1.0),
        &#39;bagging_fraction&#39;: Trial.suggest_float(&#39;bagging_fraction&#39;, 0.2, 1.0),
        &#39;bagging_freq&#39;: Trial.suggest_int(&#39;bagging_freq&#39;, 1, 10),
    }

    
    pruning_callback = optuna.integration.LightGBMPruningCallback(Trial, &#39;auc&#39;, valid_name=&#39;valid_set&#39;)
    
    模型 = lgb.train(
        参数，
        训练集=训练集，
        valid_sets=[训练集, valid_set],
        valid_names=[&#39;train_set&#39;, &#39;valid_set&#39;],
        回调=[修剪_回调，
                   提前停止(100)
                  ]
    ）
    Trial.set_user_attr(key=&#39;best_booster&#39;, value=模型)

    prob_pred = model.predict(feature_test, num_iteration=model.best_iteration)
    返回 roc_auc_score(label_test, prob_pred, labels=[0,1])

func = lambda 试验：目标（试验=试验，train_set=train_set，valid_set=valid_set，num_iterations=num_iterations）

迭代次数 = 1000

研究 = optuna.create_study(
    修剪器=optuna.pruners.HyperbandPruner(),
    方向=&#39;最大化&#39;
）

我将最佳试验中的最佳模型设置为study对象中的user_attr。这是我的代码：
def save_best_booster（研究，试用）：
    如果 Study.best_Trial.number == Trial.number：
        Study.set_user_attr(key=&#39;best_booster&#39;, value=Trial.user_attrs[&#39;best_booster&#39;])
        Study.set_user_attr(key=&#39;best_eval_result&#39;, value=Trial.user_attrs[&#39;best_eval_result&#39;])

研究.优化(
    功能，
    n_试验=30，
    show_progress_bar=真，
    回调=[save_best_booster]
）

最后我从user_attr中检索到了最佳模型（最佳助推器）。这是我的代码：
试验 = Study.best_Trial

best_model=study.user_attrs[&#39;best_booster&#39;]

由于设置了 early_stopping 回调，训练输出日志显示如下内容：
提前停止，最佳迭代是：
[30]train_set的auc：0.982083 valid_set的auc：0.874471
训练直到 100 轮验证分数没有提高

假设高于 valid_set 的 auc: 0.874471 的 auc 值确实是所有迭代中的最佳值，则 best_iteration 应为 [30]，如上所示。
但是，我通过调用 best_model.best_iteration 得到了 -1，如下所示：
在： print(best_model.best_iteration)

输出：-1

我的问题是：如何从 study 对象检索的最佳模型中获取正确的 best_iteration 值？
感谢谁能解决我的问题！
期待您的回复:)]]></description>
      <guid>https://stackoverflow.com/questions/78223783/cant-retrieve-best-iteration-in-lightgbm</guid>
      <pubDate>Tue, 26 Mar 2024 08:12:26 GMT</pubDate>
    </item>
    <item>
      <title>有关正确/错误解决方案的 C 代码数据集</title>
      <link>https://stackoverflow.com/questions/78223653/c-code-dataset-on-correct-incorrect-solutions</link>
      <description><![CDATA[我正在寻找有关体育编程任务解决方案的公共数据集（例如来自 LeetCode、timus...）。问题是，为了微调我的模型，我需要正确和错误的解决方案。例如，在 Leetcode 上，人们可以找到通过每项任务的所有测试的解决方案（但不确定在未经许可的情况下使用它们是否公平），但根本没有公开开放的不正确解决方案那里。
所需的编程语言是 C。
有人可以帮我吗？
我检查了 HuggingFace 上的所有“代码”数据集，同样什么也没有。]]></description>
      <guid>https://stackoverflow.com/questions/78223653/c-code-dataset-on-correct-incorrect-solutions</guid>
      <pubDate>Tue, 26 Mar 2024 07:46:53 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证的结果与模型的结果不一致[关闭]</title>
      <link>https://stackoverflow.com/questions/78222206/the-results-of-cross-validation-are-not-consistent-with-the-results-of-the-model</link>
      <description><![CDATA[亲爱的朋友们，我尝试交叉验证模型的指标，但不幸的是，交叉验证获得的指标结果与模型本身获得的结果不同。您认为我的错误可能出在哪里？我稍后将结果发送给您，谢谢
我是机器学习的初学者，我可能会犯严重的错误，所以我很抱歉并感谢您简单地解释我犯了错误的地方。
验证集评估：
&lt;小时/&gt;
MAE：34.57542797378782
微信：2954.5644670979195
均方根误差：54.35590554022552
中位数 20.041333295976983
最大错误216.74916470399785
R2方形0.9934581170065216
adj R2 方形 0.9933419889060457
&lt;小时/&gt;
测试集评估：
&lt;小时/&gt;
MAE：35.570507511427564
微信：3581.9881545100743
均方根误差：59.849713069571806
中位数 19.050213268160405
最大错误231.7576860160998
R2 方形 0.9924448195299899
adj R2 方形 0.9923107039003447
&lt;小时/&gt;
训练集评估：
&lt;小时/&gt;
MAE：37.58118663257971
微信：3867.839605475602
均方根误差：62.19195772345169
中位数 22.345116352581698
最大错误356.67261623088007
R2方形0.9917375305558865
adj R2 方形 0.9915908595006656
&lt;小时/&gt;
十倍交叉验证的指标：
交叉验证调整后的 r2 为 0.7768730842469048
交叉验证 r2 为 0.780764832777482
交叉验证中值绝对误差为 239.7965052529973
交叉验证的 mae 为 -248.53113336060642
交叉验证的 MSE 为 -85255.02879895715
交叉验证最大误差为-458.90474866238003
交叉验证的 rmse 为 -280.7536820695581
&lt;小时/&gt;
我是机器学习的初学者，我可能会犯严重的错误，所以我很抱歉并感谢您简单地解释我犯了错误的地方。]]></description>
      <guid>https://stackoverflow.com/questions/78222206/the-results-of-cross-validation-are-not-consistent-with-the-results-of-the-model</guid>
      <pubDate>Mon, 25 Mar 2024 23:14:49 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Flower 和 Tensorflow 来结束联邦学习中服务器的额外参数？</title>
      <link>https://stackoverflow.com/questions/78221905/how-to-end-extra-parameters-to-server-in-federated-learning-with-flower-and-tens</link>
      <description><![CDATA[我想将带有模型更新的额外参数发送到服务器，然后将服务器中的这些额外参数用于其他目的。我在这个项目中使用 Flower 和 Tensorflow。在发送额外参数之前，我的模型运行良好。目前我有这些代码客户端模型 server.py。
如何在服务器中成功发送额外参数或值并接收它？
感谢您的帮助。
我尝试在 get_parameter 方法中发送附加参数，并使用 FedAvg 策略接收它。但我一次又一次地遇到这个错误。 错误]]></description>
      <guid>https://stackoverflow.com/questions/78221905/how-to-end-extra-parameters-to-server-in-federated-learning-with-flower-and-tens</guid>
      <pubDate>Mon, 25 Mar 2024 21:38:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 MS-COCO 数据集、标准化和灰度 [关闭]</title>
      <link>https://stackoverflow.com/questions/78221719/working-with-ms-coco-dataset-normalization-and-grayscale</link>
      <description><![CDATA[我正在尝试构建 SuperPoint 网络的修改版本以进行兴趣点检测。 SuperPoint 网络适用于灰度图像。我正在使用 MS-COCO 数据集进行训练。我的困惑是，我应该将图像转换为灰度，然后使用平均值和标准差（灰度数据集的）进行标准化，还是首先标准化 RGB（使用 RGB 数据集的平均值和标准差），然后转换为灰度？然后我需要将其缩放到 [-1,1]。
任何提示或解释将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78221719/working-with-ms-coco-dataset-normalization-and-grayscale</guid>
      <pubDate>Mon, 25 Mar 2024 20:52:13 GMT</pubDate>
    </item>
    <item>
      <title>如何删除 Huggingface 的 Transformers GPT2 预训练模型中的层？</title>
      <link>https://stackoverflow.com/questions/78219076/how-to-remove-layers-in-huggingfaces-transformers-gpt2-pre-trained-models</link>
      <description><![CDATA[我的代码：
从转换器导入 GPT2Config、GPT2Model
从变压器导入 AutoTokenizer、AutoModelForMaskedLM、AutoModelForCausalLM
模型 = AutoModelForCausalLM.from_pretrained(“openai-community/gpt2”)
打印（解码器）

这是控制台的输出，列出了模型架构：
&lt;前&gt;&lt;代码&gt;GPT2LMHeadModel(
  （变压器）：GPT2Model（
    (wte)：嵌入(50257, 768)
    (wpe)：嵌入(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): 模块列表(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        ）
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (动作): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        ）
      ）
    ）
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  ）
  （lm_head）：线性（in_features = 768，out_features = 50257，偏差= False）
）

我想删除第一层：
(wte)：嵌入(50257, 768)

我尝试过以下方法：
def deleteEncodingLayers(model, num_layers_to_keep): # 必须传入完整的bert模型
    oldModuleList = model.bert.encoder.layer
    newModuleList = nn.ModuleList()

    # 现在迭代所有层，只保留相关层。
    对于范围内的 i(0, len(num_layers_to_keep))：
        newModuleList.append(oldModuleList[i])

    # 创建模型的副本，使用新列表修改它，然后返回
    copyOfModel = copy.deepcopy(模型)
    copyOfModel.bert.encoder.layer = newModuleList

    返回模型副本

但是没有成功。谁知道怎么解决？]]></description>
      <guid>https://stackoverflow.com/questions/78219076/how-to-remove-layers-in-huggingfaces-transformers-gpt2-pre-trained-models</guid>
      <pubDate>Mon, 25 Mar 2024 12:28:15 GMT</pubDate>
    </item>
    <item>
      <title>用于物体方向估计的模板匹配模型仅在平面内旋转时快速收敛，但在全 3D 方向时失败</title>
      <link>https://stackoverflow.com/questions/78218374/template-matching-model-for-object-orientation-estimation-converges-fast-with-in</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78218374/template-matching-model-for-object-orientation-estimation-converges-fast-with-in</guid>
      <pubDate>Mon, 25 Mar 2024 10:23:10 GMT</pubDate>
    </item>
    <item>
      <title>使用 MPI 优化 Optuna 参数</title>
      <link>https://stackoverflow.com/questions/78218072/optuna-parameter-optimisation-with-mpi</link>
      <description><![CDATA[我有一些机器学习代码，它使用 SVM（来自 scikit-learn）和预计算内核，我想使用 optuna 对其进行优化，因此代码简单地看起来有点像这样
def 目标（试用：试用，fast_check=True，target_meter=0，return_info=False）：
     #设置参数
     C = Trial.suggest_float(“C”,0.0​​1,5)
     tol = Trial.suggest_loguniform(“tol”,1e-4,1e-1)
     内核参数 = ...

     #构建火车内核
     内核训练 = ...

     #构建测试内核
     内核测试 = ...

     #火车服务
     svc = SVC(内核=“预计算”, C=C, tol=tol)
     svc.fit(kernel_train, train_labels)
     test_predict = svc.predict(kernel_test)
     test_auc = roc_auc_score(test_labels,test_predict)

     返回测试_auc

Study = optuna.create_study(direction=“最大化”,study_name=&#39;study_1&#39;)
研究.优化（目标，n_Trials=40）


但是，由于我正在计算的内核的复杂性，我使用 mpi4py 来并行计算，但同​​时使用 optuna 和 MPI 时遇到一些问题。
显然，我想要多个处理器上的内核代码，但是当我创建研究并优化它时，我不想在处理器上创建多个不同的研究，我只想对根进行优化的一项研究（我假设？）。我已经尝试了下面的方法，它有效，但是当我不使用 MPI 时，它的优化效果不佳，我认为这正在创建多项研究并优化它们，这似乎效率不高。似乎更难以收敛到最佳参数。
从 mpi4py 导入 MPI

mpi_comm = MPI.COMM_WORLD
排名 = mpi_comm.Get_rank()
n_procs = mpi_comm.Get_size()
根=0

def目标（试验：试验，fast_check = True，target_meter = 0，return_info = False）：
     #设置参数
     C = Trial.suggest_float(“C”,0.0​​1,5)
     tol = Trial.suggest_loguniform(“tol”,1e-4,1e-1)
     内核参数 = ...

     #使用 MPI 构建训练内核
     内核训练 = ...

     #使用MPI构建测试内核
     内核测试 = ...

     #火车服务
     如果排名==根：
           svc = SVC(内核=“预计算”, C=C, tol=tol)
           svc.fit(kernel_train, train_labels)
           test_predict = svc.predict(kernel_test)
           test_auc = roc_auc_score(test_labels,test_predict)
     别的：
           测试_auc = 0
     test_auc = mpi_comm.bcast(test_auc, root=0)

如果排名==根：
     Study = optuna.create_study(direction=“最大化”,study_name=&#39;study_1&#39;)
别的：
     研究 = 0
 研究= mpi_comm.bcast（研究，根= 0）

研究.优化（目标，n_Trials=40）

这是一个非常小众的问题，但只是想知道是否有人对这些软件包有任何经验，并且可以帮助建议如何运行多处理代码，同时仅优化一个处理器上的参数。如果有任何术语不正确，我深表歉意，我是使用这两个软件包的新手，因此请耐心等待。]]></description>
      <guid>https://stackoverflow.com/questions/78218072/optuna-parameter-optimisation-with-mpi</guid>
      <pubDate>Mon, 25 Mar 2024 09:20:08 GMT</pubDate>
    </item>
    <item>
      <title>使用扩散模型和Detectron2进行图像分割调试</title>
      <link>https://stackoverflow.com/questions/78217946/image-segmentation-debugging-using-the-diffusion-model-and-detectron2</link>
      <description><![CDATA[我正在 Publaynet 数据集上训练一个基于 detectorron2 构建的扩散模型，用于实例分割。但多次迭代后我得到的输出是将整个文档分割为如图所示。它不会对文档中的表格和文本等单个元素进行分段。损失函数在学习率为 0.00005 时下降得非常好。总损失减少至 1.6。 Loss_bbox 约为 0.200，loss:giou 为 0.3573。
我进行了健全性检查，模型中的输入似乎是正确的。我可视化了输入和边界框，并检查了目标。一切似乎都是正确的。我不知道是否应该进一步训练模型或更改任何其他超参数。我尝试了多个学习率，这个学习率似乎不错。 NUM_PROPSALS 是 500。我应该提高到 1000。我应该特别关心哪些超参数。该代码不是从头开始的。我从这里获取仓库 https://github.com/chenhaoxing/DiffusionInst。所以我不是从头开始构建模型。如果有人有任何想法请告诉我。
以下是配置文件中的超参数列表：
&lt;前&gt;&lt;代码&gt;型号：
  META_ARCHITECTURE：“DiffusionInst”
  权重：“Detectron2://ImageNetPretrained/torchvision/R-50.pkl”
  PIXEL_MEAN: [123.675, 116.280, 103.530]
  像素_STD：[58.395、57.120、57.375]
  骨干：
    名称：“build_resnet_fpn_backbone”
  资源网：
    OUT_FEATURES：[“res2”、“res3”、“res4”、“res5”]
  FPN：
    IN_FEATURES：[“res2”、“res3”、“res4”、“res5”]
  投资回报率_头：
    IN_FEATURES：[“p2”、“p3”、“p4”、“p5”]
  ROI_BOX_HEAD：
    POOLER_TYPE：“ROIAlignV2”
    POOLER_RESOLUTION：7
    POOLER_SAMPLING_RATIO：2
解算器：
  IMS_PER_BATCH：2
  BASE_LR：0.0000125
  步骤：（210000、250000）
  最大迭代次数：270000
  热身系数：0.01
  WARMUP_ITERS：1000
  WEIGHT_DECAY：0.0001
  优化器：“ADAMW”
  BACKBONE_MULTIPLIER: 1.0 # 与 BASE_LR 保持相同。
  CLIP_GRADIENTS：
    已启用：正确
    CLIP_TYPE：“完整模型”
    CLIP_VALUE：1.0
    标准类型：2.0
种子：40244023
输入：
  最小尺寸列车：（480、512、544、576、608、640、672、704、736、768、800）
  庄稼：
    启用：假
    类型：“绝对范围”
    尺寸：（384、600）
  格式：“RGB”
测试：
  EVAL_PERIOD：20000
数据加载器：
  FILTER_EMPTY_ANNOTATIONS：错误
  NUM_WORKERS：2
版本：2
]]></description>
      <guid>https://stackoverflow.com/questions/78217946/image-segmentation-debugging-using-the-diffusion-model-and-detectron2</guid>
      <pubDate>Mon, 25 Mar 2024 08:54:20 GMT</pubDate>
    </item>
    <item>
      <title>R 混淆矩阵 - 错误：“数据”和“参考”应该是具有相同级别的因素</title>
      <link>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</link>
      <description><![CDATA[尽管还有其他针对相同错误消息的报告，但没有一个对我的情况有帮助。
我已经准备了自己的数据，分割如下，但无法获得混淆矩阵。
test_index &lt;- createDataPartition(y =workingData$PM10, times = 1, p = 0.5, list = FALSE)
train_set &lt;-工作数据[-test_index,]
test_set &lt;-工作数据[test_index,]

train_knn &lt;- train(PM10 ~. , method= &quot;knn&quot; , data = train_set)

y_hatknn &lt;- 预测(train_knn, train_set, type = “raw”)

fusionMatrix(y_hatknn, test_set$PM10)

上面最后一行给出
错误：“data”和“reference”应该是具有相同级别的因素。

我想上传数据进行复制，但可以提供基本的：
&lt;前&gt;&lt;代码&gt;str（工作数据）
“数据帧”：3653 obs。 3 个变量：
&#39; $ 日期 : 数字 2e+07 2e+07 2e+07 2e+07 2e+07 ...
&#39; $ Rain_mm: 数字 0.1 6.7 0 1.4 0.8 1.8 15.3 0 2.6 3.8 ...
&#39; $ PM10 : 数字 -1 -1 -1 -1 -1 ...

PM10 是污染 PM10 水平。
如何解决？
添加更多信息：
在原始错误之后：
&lt;块引用&gt;
confusionMatrix(y_hatknn, test_set$PM10)
错误：data 和 reference 应该是具有相同水平的因子。

我尝试将其设置为因素...
&lt;块引用&gt;
confusionMatrix(y_hatknn, as.factor(test_set$PM10))
错误：data 和 reference 应该是具有相同水平的因子。

以预测为因素...
&lt;块引用&gt;
confusionMatrix(as.factor(y_hatknn), test_set$PM10)
错误：data 和 reference 应该是具有相同水平的因子。

以两个参数为因素...
&lt;块引用&gt;
confusionMatrix(as.factor(y_hatknn), as.factor(test_set$PM10))
fusionMatrix.default(as.factor(y_hatknn), as.factor(test_set$PM10)) 中的错误：
数据的级别不能多于参考

确实需要得到整理，Stack坚持关闭我的帖子，写下gmail中navarrodan007的解决方案]]></description>
      <guid>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</guid>
      <pubDate>Fri, 22 Mar 2024 09:39:08 GMT</pubDate>
    </item>
    <item>
      <title>尽管验证准确度很高，为什么我的神经网络对属于某一类的测试图像预测出错误的类标签？</title>
      <link>https://stackoverflow.com/questions/71841718/why-does-my-neural-network-predict-the-incorrect-class-label-for-test-images-bel</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/71841718/why-does-my-neural-network-predict-the-incorrect-class-label-for-test-images-bel</guid>
      <pubDate>Tue, 12 Apr 2022 11:20:17 GMT</pubDate>
    </item>
    <item>
      <title>Python scipy/numpy 中相关性的层次聚类？</title>
      <link>https://stackoverflow.com/questions/2907919/hierarchical-clustering-on-correlations-in-python-scipy-numpy</link>
      <description><![CDATA[如何在 scipy/numpy 中的相关矩阵上运行层次聚类？我有一个 100 行 x 9 列的矩阵，我想根据 9 个条件中每个条目的相关性进行分层聚类。我想使用 1-pearson 相关性作为聚类的距离。假设我有一个包含 100 x 9 矩阵的 numpy 数组 X，我该怎么做？
我尝试使用 hcluster，基于此示例：
Y=pdist(X, &#39;seuclidean&#39;)
Z=联动(Y, &#39;单&#39;)
树状图(Z, color_threshold=0)

但是，pdist 不是我想要的，因为那是欧几里德距离。有什么想法吗？
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/2907919/hierarchical-clustering-on-correlations-in-python-scipy-numpy</guid>
      <pubDate>Tue, 25 May 2010 19:39:00 GMT</pubDate>
    </item>
    </channel>
</rss>