<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 21 Aug 2024 21:15:20 GMT</lastBuildDate>
    <item>
      <title>KerasTensors 上的二元交叉熵不起作用</title>
      <link>https://stackoverflow.com/questions/78898962/binary-crossentropy-on-kerastensors-not-working</link>
      <description><![CDATA[我正尝试在 tf/keras 中实现此 VAE，但 binary_crossentropy 似乎出了点问题。
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Lambda, Input, Dense
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.models import Model

...

def build_vae():
...

rebuild_loss = binary_crossentropy(inputs, output) * image_size
kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
kl_loss = K.sum(kl_loss, axis=-1)
kl_loss *= -0.5
vae_loss = K.mean(reconstruction_loss + kl_loss)

我在 binary_crossentropy 行上收到错误。确认 inputs 和 outputs 属于 KerasTensor 类型后，回溯状态为：

ValueError：KerasTensor 不能用作 TensorFlow 函数的输入。KerasTensor 是形状和 dtype 的符号占位符，用于构建 Keras Functional 模型或 Keras Functions。您只能将其用作 Keras 层或 Keras 操作的输入（来自命名空间 keras.layers 和 keras.operations）。

如何修复此问题？此外，我很好奇为什么 tensorflow.keras.losses.binary_crossentropy 不接受 KerasTensors，或者为什么它不被视为 Keras 函数，即使它在 keras 库中。
我尝试将 binary_crossentropy 包装在 keras Layer 子类中，该行有效，但随后所有 K 函数都会出错。如果我必须对所有使用的 K 函数重复该过程，我会感到惊讶]]></description>
      <guid>https://stackoverflow.com/questions/78898962/binary-crossentropy-on-kerastensors-not-working</guid>
      <pubDate>Wed, 21 Aug 2024 20:53:11 GMT</pubDate>
    </item>
    <item>
      <title>即使训练=False，Tensorflow 模型仍可进行训练</title>
      <link>https://stackoverflow.com/questions/78898892/tensorflow-model-still-trains-even-with-training-false</link>
      <description><![CDATA[以下是简单的重现代码
import tensorflow as tf
import tensorflow.keras as keras

inp = keras.Input((3, 3))
layer = keras.layers.Dense(1)
tar = layer(inp)

tar2 = layer(inp, training=False)
model2 = keras.Model(inp, tar2)
model2.compile(loss=&#39;mse&#39;, optimizer=keras.optimizers.Adam(0.01))

# fit
a = tf.random.normal((1, 3, 3))
b = tf.random.normal((1, 3, 1))

model2.fit(a, b)

如果您在训练之前/之后检查 model2.trainable_variables，您可以轻松检查模型的参数model2 已更改，这意味着它已进行过训练。
我该怎么做才能在训练期间不更新特定时间的某个层？
我需要这样做的原因是，我的方案是重用我之前制作的一些层，并且我不希望在仍然更新模型中的其他层时再次训练这些层。如下所示。
inp1 = keras.Input((2, 3))
inp2 = keras.Input((4, 3))

layer1 = keras.layers.Dense(1)
intermediate_output1 = layer1(inp1)
intermediate_output2 = layer1(inp2, training=False) # 我不想为 inp2 再次训练该层。

已添加 = tf.concat([intermediate_output1, middle_output2], axis=1)
layer2 = keras.layers.Dense(2)
final_output = layer2(已添加)
final_output.shape # [无, 6, 4]
]]></description>
      <guid>https://stackoverflow.com/questions/78898892/tensorflow-model-still-trains-even-with-training-false</guid>
      <pubDate>Wed, 21 Aug 2024 20:28:32 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练模型时，cv2 图像形状出现错误</title>
      <link>https://stackoverflow.com/questions/78897608/cv2-error-in-image-shape-while-trying-to-train-model</link>
      <description><![CDATA[我有 imgs、mask 和 labels 数据，我想使用 pytorch 和 dataloaders 创建分割模型
我在尝试训练自定义模型时收到此错误，我认为问题来自自定义数据集类
cv2.error: OpenCV(4.10.0) D:\a\opencv-python\opencv-python\opencv\modules\core\src\matrix_transform.cpp:784: error: (-215:Assertion failed) _src.dims() &lt;= 2 in function &#39;cv::flip&#39;

我的模型开始训练后，我立即收到此错误
这是我的自定义数据集代码：
class DatasetSeg (Dataset):

def __init__ (self, img_dir, mask_dir, label_dir, transform = None):

self.img_dir = img_dir
self.mask_dir = mask_dir
self.label_dir = label_dir
self.transform = transform
self.images = [img for img in os.listdir(img_dir) if img.endswith(&#39;.png&#39;)] 
self.indexes = self.getIndex()

def getIndex(self):

indexes = []
for img in self.images:
indexes.append(int(img[4:8])) #图像被命名为 rgb_XXXX.png
return indexes

def __len__(self):
return len(self.images)

def __getitem__(self, idx):

index = self.indexes[idx]

# 打印尺寸和类型以供调试

img_filename = f&#39;rgb_{index:04d}.png&#39;
mask_filename = f&#39;semantic_segmentation_{index:04d}.png&#39;
labels_filename = f&#39;semantic_segmentation_labels_{index:04d}.json&#39;

img_path = os.path.join(self.img_dir, img_filename)
mask_path = os.path.join(self.mask_dir, mask_filename)
label_path = os.path.join(self.label_dir, labels_filename)

使用 open(label_path, &#39;r&#39;) 作为文件：
labels = json.load(file)

image = Image.open(img_path).convert(&quot;RGB&quot;)
image = np.array(image)

mask = Image.open(mask_path).convert(&quot;RGBA&quot;)
mask = np.array(mask)

class_id_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.uint8)

color_to_class_id = {}

for class_id, color_str in enumerate(labels.keys()):
color = tuple(map(int, color_str.strip(&#39;()&#39;).split(&#39;,&#39;)))
color_to_class_id[color] = class_id

for color, class_id in color_to_class_id.items():

# color_array = np.array(color, dtype=mask.dtype) 
match = np.all(mask == color, axis=-1) #将像素与当前 RGBA 颜色匹配
# print(match)
class_id_mask[match] = class_id #将类 ID 分配给匹配的像素

image = np.transpose(image, (2,0,1))

if self.transform:
formed = self.transform(image=image, mask=class_id_mask)
image = formed[&#39;image&#39;]
mask = formed[&#39;mask&#39;]

print(image.shape, class_id_mask.shape)
return image, class_id_mask

我尝试添加调试打印来查看图像的尺寸，这是 getitem 中打印的输出：(3, 720, 1280) (720, 1280)。]]></description>
      <guid>https://stackoverflow.com/questions/78897608/cv2-error-in-image-shape-while-trying-to-train-model</guid>
      <pubDate>Wed, 21 Aug 2024 14:44:54 GMT</pubDate>
    </item>
    <item>
      <title>根据 scikit-learn ColumnTransformer 访问用于归纳和规范化新数据的值</title>
      <link>https://stackoverflow.com/questions/78896943/accessing-the-values-used-to-impute-and-normalize-new-data-basded-upon-scikit-le</link>
      <description><![CDATA[使用 scikit-learn，我在训练集上构建机器学习模型，然后在测试集上对其进行评估。在训练集上，我使用 ColumnTransformer 执行数据插补和缩放，然后使用 Kfold CV 构建逻辑回归模型，最终模型用于预测测试集上的值。最终模型还使用其来自 ColumnTransformer 的结果来插补测试集上的缺失值。例如，最小-最大标量将从训练集中获取最小值和最大值，并在缩放测试集时使用这些值。我如何才能看到这些从训练集中得出然后用于预测测试集的缩放值？我在 scikit-learn 文档中找不到有关它的任何信息。以下是我使用的代码：
来自 sklearn.linear_model 导入 SGDClassifier
来自 sklearn.model_selection 导入 RepeatedStratifiedKFold
来自 sklearn.model_selection 导入 GridSearchCV
来自 sklearn.compose 导入 ColumnTransformer
来自 sklearn.impute 导入 SimpleImputer
来自 sklearn.pipeline 导入 Pipeline
来自 sklearn.preprocessing 导入 MinMaxScaler、OneHotEncoder

def preprocessClassifierLR(categorical_vars, numeric_vars):###categorical_vars 和 numeric_vars 是定义 X 中存在的分类和数字变量的列名的列表

categorical_pipeline = Pipeline(steps=[(&#39;mode&#39;, SimpleImputer(missing_values=np.nan, strategies=&quot;most_frequent&quot;)),
(&quot;one_hot_encode&quot;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))])

numeric_pipeline = Pipeline(steps=[(&#39;numeric&#39;, SimpleImputer(strategy=&quot;median&quot;)),
(&quot;scaling&quot;, MinMaxScaler())])

col_transform = ColumnTransformer(transformers=[(&quot;cats&quot;, categorical_pipeline, categorical_vars),
(&quot;nums&quot;, numeric_pipeline, numeric_vars)])

lr = SGDClassifier(loss=&#39;log_loss&#39;, penalty=&#39;elasticnet&#39;)
model_pipeline = Pipeline(steps=[(&#39;preprocess&#39;, col_transform),
(&#39;classifier&#39;, lr)])

random_grid_lr = {&#39;classifier__alpha&#39;: [1e-1, 0.2, 0.5],
&#39;classifier__l1_ratio&#39;: [1e-3, 0.5]}

kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=47)

param_search = GridSearchCV(model_pipeline, random_grid_lr,scoring=&#39;roc_auc&#39;, cv=kfold, refit=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

param_search = preprocessClassifierLR(categorical_vars, numeric_vars)
train_mod = param_search.fit(X_train, y_train)
print(&quot;Mod AUC:&quot;, train_mod.best_score_)

test_preds = train_mod.predict_proba(X_)[:,1]

我无法提供真实数据，但 X 是一个包含独立变量的数据框，y 是二元结果变量。train_mod 是一个包含列变换器和 SGD 分类器步骤的管道。我可以通过运行 train_mod.best_params_ 轻松从分类器中获取类似的参数信息，例如最佳 lambda 和 alpha 值，但我无法找出用于列变换器的统计数据，例如 1) 用于分类特征的简单插补器的模式，2) 用于数字特征的简单插补器的中值，以及 3) 用于缩放数字特征的最小值和最大值。如何访问此信息？
我假设 train_mod.best_estimator_[&#39;preprocess&#39;].transformers_ 包含此信息，类似于 train_mod.best_params_ 为我提供从模型训练中得出的 alpha 和 lambda 值，然后将其应用于测试集。]]></description>
      <guid>https://stackoverflow.com/questions/78896943/accessing-the-values-used-to-impute-and-normalize-new-data-basded-upon-scikit-le</guid>
      <pubDate>Wed, 21 Aug 2024 12:24:53 GMT</pubDate>
    </item>
    <item>
      <title>尝试加载 hydra 的配置时出现问题</title>
      <link>https://stackoverflow.com/questions/78896800/problem-when-trying-to-load-the-config-of-hydra</link>
      <description><![CDATA[我正在 google collab 中运行 python 脚本。当我执行此操作时，我收到配置错误，但我不确定如何修复它
代码片段：
import hydra
from pathlib import Path # 导入文件路径处理的路径
import sys # 导入 sys 模块

@hydra.main(config_path=&quot;cfgs&quot;, config_name=&quot;config.yaml&quot;)
def main(cfg):
print(cfg) # 打印解析的配置
from train import Workspace as W
root_dir = Path.cwd()

working = W(cfg)

snap = root_dir / &#39;snapshot.pt&#39;

if snap.exists():
print(f&#39;resuming: {snapshot}&#39;)
working.load_snapshot()

working.train()

if __name__ == &#39;__main__&#39;:
main() 

输出我得到的是：
usage: colab_kernel_launcher.py [--help] [--hydra-help] [-- 
version] [--cfg {job,hydra,all}]
[--resolve] [--package PACKAGE] [-- 
run] [--multirun]
[--shell-completion] [--config-path 
CONFIG_PATH]
[--config-name CONFIG_NAME] [--config- 
dir CONFIG_DIR]
[--info 
[{all,config,defaults,defaults-tree,plugins,searchpath}]]
[overrides ...]
colab_kernel_launcher.py：错误：无法识别的参数：-f
发生异常，使用 %tb 查看完整回溯。
SystemExit：2
]]></description>
      <guid>https://stackoverflow.com/questions/78896800/problem-when-trying-to-load-the-config-of-hydra</guid>
      <pubDate>Wed, 21 Aug 2024 11:54:28 GMT</pubDate>
    </item>
    <item>
      <title>在进行连续文本分析后更新和添加数据点时，如何计算和更新权重和分数？</title>
      <link>https://stackoverflow.com/questions/78896688/how-to-calculate-and-update-weights-and-score-when-updating-and-adding-data-poin</link>
      <description><![CDATA[我正在做文本分析并得到一组带有分数的主题。我正在存储这些。
我将其保存如下。
{

&quot;Topic&quot;: {

&quot;TopicName&quot;: &quot;/Computers &amp;电子/编程”,

“分数”: 35.371166

},

“TotalCountOfTopic”: 70

},

{

“主题”: {

“主题名称”: “/科学/计算机科学”,

“分数”: 35.900078

},

“TotalCountOfTopic”: 69

},

{

“主题”: {

“主题名称”: “/商业与Industrial&quot;,

&quot;Score&quot;: 38.20758

},

&quot;TotalCountOfTopic&quot;: 47

}

接下来我想进行第二批文本分析，并更新这些数字。
我采取的步骤如下。
将之前运行的分数与我已有的分数平均，添加新分数并找到中位数。
var oldWeight = user.TotalAnalysis[oldTopicIndex].TotalCountOfTopic * user.TotalAnalysis[oldTopicIndex].Topic?.Score;

var totalCount = user.TotalAnalysis[oldTopicIndex].TotalCountOfTopic + 1; // 增加总数

var updatedTopic = new TopicAverage()
{

Topic = new Topic()
{

Score = (oldWeight + newTopic.Score) / totalCount,// 修正后的加权平均计算

TopicName = newTopic.TopicName
}, 
TotalCountOfTopic = totalCount
};

我认为这是正确的做法，但我很好奇社区是怎么想的。]]></description>
      <guid>https://stackoverflow.com/questions/78896688/how-to-calculate-and-update-weights-and-score-when-updating-and-adding-data-poin</guid>
      <pubDate>Wed, 21 Aug 2024 11:26:23 GMT</pubDate>
    </item>
    <item>
      <title>GAN 模型中图形嵌入的反向传播运行时错误</title>
      <link>https://stackoverflow.com/questions/78896269/runtimeerror-on-backpropagation-in-a-gan-model-for-graph-embeddings</link>
      <description><![CDATA[我正在尝试学习如何在图形嵌入上创建 GAN，但一直遇到错误

RuntimeError：尝试第二次向后遍历图形（或在已释放已保存的张量后直接访问它们）。调用 .backward() 或 autograd.grad() 时，图形的已保存中间值将被释放。如果您需要第二次向后浏览图表，或者在调用向后调用后需要访问已保存的张量，请指定 retain_graph=True。

class Generator(nn.Module):
def __init__(self, label_dim, noise_dim, embedding_dim):
super(Generator, self).__init__()
self.fc1 = nn.Linear(label_dim + noise_dim, 64)
self.fc2= nn.Linear(64, embedding_dim)

def forward(self, label, noise):
label = label.unsqueeze(1).float()
x = torch.cat([label, noise], dim=1)
x = torch.relu(self.fc1(x))
x = self.fc2(x)
return x

class Discriminator(nn.Module):
def __init__(self, embedding_dim):
super(Discriminator, self).__init__()
self.fc1 = nn.Linear(embedding_dim, 64)
self.fc2 = nn.Linear(64, 1)

def forward(self, x):
x = torch.relu(self.fc1(x))
x = torch.sigmoid(self.fc2(x))
return x

subset_size =1000
batch_size = 32
num_epochs = 100
noise_dim = 32
embedding_dim = 32
label_dim = 1

graph_embeddings = torch.stack(graph_embeddings_list).to(device)

dataset = TensorDataset(graph_embeddings,indexed_labels_tensor)

subset_indices = torch.randperm(len(graph_embeddings))[:subset_size]
subset = torch.utils.data.Subset(dataset, subset_indices)

dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True)

device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

generator = Generator(label_dim=label_dim, noise_dim=noise_dim, embedding_dim=embedding_dim).to(device)
discriminator = Discriminator(embedding_dim=embedding_dim).to(device)

optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002)
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002)

criterion = nn.BCELoss()

for epoch in range(num_epochs):
for n, (real_embeddings, labels) in enumerate(dataloader):

real_samples_labels = torch.ones((real_embeddings.size(0), 1)).to(device=device)
generated_samples_labels = torch.zeros((real_embeddings.size(0), 1)).to(device=device)

print(&quot;生成的样本形状：&quot;, generated_samples_labels.shape)
print(&quot;真实样本形状：&quot;, real_samples_labels.shape)

real_embeddings = real_embeddings.to(device=device)
labels = labels.to(device=device)

latent_space_samples = torch.randn((labels.size(0), noise_dim)).to(device=device)
generated_samples = generator(labels, latent_space_samples)

all_samples = torch.cat((real_embeddings, generated_samples)).to(torch.float32)
all_samples_labels = torch.cat((real_samples_labels, generated_samples_labels)).to(torch.float32)

discriminator.zero_grad()

output_discriminator = discriminator(all_samples)

print(&quot;判别器输出形状：&quot;, output_discriminator.shape)
print(&quot;判别器输出：&quot;, output_discriminator)

loss_discriminator = criterion(output_discriminator, all_samples_labels)

print(&quot;判别器后向损失：&quot;, loss_discriminator.item())
loss_discriminator.backward() 
print(&quot;已完成鉴别器的反向传递&quot;)
optimizer_D.step()

generator.zero_grad()

latent_space_samples = torch.randn((labels.size(0), noise_dim)).to(device=device)
generated_samples = generator(labels, latent_space_samples)

output_discriminator_generated = discriminator(generated_samples) 
loss_generator = criterion(output_discriminator_generated, real_samples_labels)

print(f&quot;生成器反向传递之前 - epoch {epoch}, batch {n}&quot;)
loss_generator.backward(create_graph=True)
print(f&quot;生成器反向传递之后 - epoch {epoch}, batch {n}&quot;)
optimizer_G.step()

if n == len(dataloader) - 1:
print(f&quot;Epoch: {epoch} Loss D.: {loss_discriminator.item()}&quot;)
print(f&quot;Epoch: {epoch} Loss G.: {loss_generator.item()}&quot;)


我尝试创建一个简单的 GAN 来计算与标签相关的图嵌入。但错误发生在 loss_discriminator.backward() 部分。
我认为该过程是正确的。我尝试分离并重新计算 loss_generation 中的 output_discriminator，但都无济于事。]]></description>
      <guid>https://stackoverflow.com/questions/78896269/runtimeerror-on-backpropagation-in-a-gan-model-for-graph-embeddings</guid>
      <pubDate>Wed, 21 Aug 2024 09:50:24 GMT</pubDate>
    </item>
    <item>
      <title>如何在使用 model.summary() 时阻止 Jupyter 重新启动？</title>
      <link>https://stackoverflow.com/questions/78895569/how-to-stop-jupyter-from-restarting-when-using-model-summary</link>
      <description><![CDATA[我在 jupyter 上使用 keras，并尝试运行代码 model.summary()。但是，jupyter 不断自行重启。
我曾尝试将 max_buffer_size 增加到 17gb，但没有成功。如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78895569/how-to-stop-jupyter-from-restarting-when-using-model-summary</guid>
      <pubDate>Wed, 21 Aug 2024 07:03:20 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“ChannelAug”的模块</title>
      <link>https://stackoverflow.com/questions/78895108/modulenotfounderror-no-module-named-channelaug</link>
      <description><![CDATA[当我使用
sh run_train_regdb.sh

控制台显示：
ModuleNotFoundError：没有名为“ChannelAug”的模块

但是 conda 和 pip 无法安装此模块
我不知道如何修复此问题，我应该安装哪个模块？]]></description>
      <guid>https://stackoverflow.com/questions/78895108/modulenotfounderror-no-module-named-channelaug</guid>
      <pubDate>Wed, 21 Aug 2024 04:08:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在一个 cv2 相机屏幕中运行两个计算机视觉模型</title>
      <link>https://stackoverflow.com/questions/78894715/how-to-run-two-computer-vision-models-in-one-cv2-camera-screen</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78894715/how-to-run-two-computer-vision-models-in-one-cv2-camera-screen</guid>
      <pubDate>Tue, 20 Aug 2024 23:51:16 GMT</pubDate>
    </item>
    <item>
      <title>回归模型一遍又一遍地得到相同的输出</title>
      <link>https://stackoverflow.com/questions/78894555/getting-the-same-output-over-and-over-again-for-regression-model</link>
      <description><![CDATA[我的回归模型与自动驾驶汽车有关，数据由 5 个传感器的幅度和一个转弯角度组成。
DataParser.Parse(out double[][] input, out double[] output,&quot;data.csv&quot;, (0, 4), (4, 5));
var model = new FanChenLinSupportVectorRegression&lt;Gaussian&gt;();
var svm = model.Learn(input, output);

Console.WriteLine(svm.Score(new double[] { 95.0, 0.0, 67.8, 0.0, 0.0 }));

我决定进行实验，看看 FanChenLin 回归算法是否有效。
现在，我面临的问题是，对于输出，我得到的是 217.88 的常数值。我已经生成了一个模型训练数据，它可能有误，但我希望它在输入不同时给出不同的输出，但在这里，尽管改变了输出，我仍然得到了 217.88 的值。
我编写的数据解析器似乎得到了正确的输入和输出。
我搞不清楚哪里出了问题。
我尝试更改复杂度等值，但尽管它们给出了不同的输出，但该输出在不同情况下仍然是恒定的输入。
37.454011884736246,3.142918568673425,64.20316461542878,5.16817211686077,10.31238688359326,21.365509618766623
95.07143064099162,63.641041 12637804,8.413996499504883,53.1354631568148,90.25529066795667,61.22639465322385 73.1993941811405,31.435598107632668,16.162 871409461378,54.06351216101065,50.5252 37244785714,0.16392599623276283 59.86584841970366,50.85706911647028,89.85541885270793,63.742990149820656,82.64574661077417 ,338.3506582186952 15.601864044243651 ,90.7566473926093,60.642905965958995,72.60913337226616,32.00496010306117,1.6476821446117356 15.599452033620265,24.92922291 4887493,0.9197051616629648,97.58520794 625346,89.55232284962005,270.35942257808824 5.8083612168199465,41.038292303562976,10.147154286603211,51.630034830119534,38 .92016787341631,283.0714170955975 86. 61761457749351,75.55511385430486,66.35017691080559,32.2956472941246,1.083765148029836,62.743931568541655 60.11150117432088 ,22.879816549162246,0.5061583846218687 ,79.51861947687037,90.53819764192636,270.3330873043567 70.80725777960456,7.697990982879299,16.080805141749867,27.083225126 207424,9.128667678613356,69.182165231 95599 2.0584494295802447,28.9751452913768,54.87337893665861,43.89714207056361,31.93136375904149,320.77412786375123 96.9909 8521619943,16.122128725400444,69.18951 976926932,7.845638134226595,95.0061967050805,8.435865969658153 83.24426408004217,92.96976523425731,65.19612595026005,2.535 074341545751,95.06071469375561,50.331 93776194861 21.233911067827616,80.8120379564417,22.42693094605598,96.26484146779251,57.34378881232861,293.50639083221773 1 8.182496720710063,63.34037565104234,71 .2179221347536,83.59801205122058,63.183721216979926,317.50014925812815 18.34045098534338,87.14605901877177,23.724908749680 008,69.59742060936979,44.844552197831 98,339.3199373713304 30.42422429595377,80.36720768991145,32.539969815926774,40.89529444142699,29.321077169806454,51.271442 802352 52.475643163223786,18.657005888 603585,74.64914051180241,17.329432007084577,32.8664545369916,15.667156378272637 43.194501864211574,89.25589984899777,64.96 328990472146,15.643704267108605,67.25 184560770384,37.33686449574435 29.122914019804192,53.93422419156507,84.9223410494178,25.024289816459532,75.237452943768,34 8.5473288270272 61.18528947223795,80.7 4401551640625,65.76128923003434,54.92266647061205,79.15790437258485,6.806163770302646 13.949386065204184,89.60912999234932 ,56.830860333547164,71.45959227000624, 78.96181427945538,320.4909103298581
29.214464853521815,31.800347497186387,9.367476782809248,66.01973767177313,9.120610304869036,303.55070797479505
]]></description>
      <guid>https://stackoverflow.com/questions/78894555/getting-the-same-output-over-and-over-again-for-regression-model</guid>
      <pubDate>Tue, 20 Aug 2024 22:18:47 GMT</pubDate>
    </item>
    <item>
      <title>利用分割模型对直肠内超声图像中的肿瘤分期进行分类</title>
      <link>https://stackoverflow.com/questions/78893937/leveraging-segmentation-model-for-tumor-stage-classification-in-endorectal-ultra</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78893937/leveraging-segmentation-model-for-tumor-stage-classification-in-endorectal-ultra</guid>
      <pubDate>Tue, 20 Aug 2024 18:28:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 sklearn python 获取预测</title>
      <link>https://stackoverflow.com/questions/78890391/how-use-sklearn-python-get-predicion</link>
      <description><![CDATA[我有一张表，我想传递 features = &quot;train_1, train_2, train_3, train_4&quot; 和 target_result = result_cor。
我想知道什么时候值是 = &quot;1 或 2&quot;在我的预测中：
关注我的数据
关注我的代码：
从 enum 导入 auto
从 sklearn.svm 导入 LinearSVC
从 sklearn.model_selection 导入 train_test_split
从 sklearn.metrics 导入 accuracy_score
从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.metrics 导入 classes_report
从 sklearn 导入 svm
从 sklearn.linear_model 导入 LogisticRegression
导入 pandas 作为 pd
导入 numpy 作为 np
导入 matplotlib.pyplot 作为 plt
导入 math
导入 seaborn 作为 sns

sheet_id = &#39;1CfnVwuqysTYNPKLVhgjJ44Af8VDcdN1l&#39; dados = pd.read_excel(f&#39;https://docs.google.com/spreadsheets/export?id={sheet_id}&amp;format=xlsx&#39;) bads.head() # 实现更多数量的数据 x = bados[[&#39;train_1&#39;,&#39;train _2&#39;,&#39;train_3&#39;,&#39;train_4&#39;]] # Gabarito 或 corretos y = bados[[&#39;result_cor&#39;]] # 将 x e y e testes de x e y 分开 treino_x, teste_x, treino_y, teste_y = train_test_split(x,y,test_size=0.33) # 模型类型 modelo = DecisionTreeClassifier() # 训练效果 modelo.fit(x,np.ravel(y,order=&quot;c&quot;)) # 预测新值 model_predict = [0,1,0,1] treino_x[:1] = model_predict model_predict = treino_x[:1] result_cor = [1] treino_y[:1] = result_cor result_cor = treino_y[:1] # 预测新模型 previsoes = modelo.predict(model_predict) # 检查准确率 precision = precision_score(result_cor,previsoes) * 100 print(f&#39;A acuracia é: {round(accuracy,2)}&#39;)


但结果始终为 100.0 % 或 0.0。我需要知道我的 result_cor 出现在模型训练模型的 model_predict 中的次数百分比
请帮忙]]></description>
      <guid>https://stackoverflow.com/questions/78890391/how-use-sklearn-python-get-predicion</guid>
      <pubDate>Tue, 20 Aug 2024 02:18:19 GMT</pubDate>
    </item>
    <item>
      <title>Val_accuracy 正在改变，有时它在补码之间交替（100％-val_acc）</title>
      <link>https://stackoverflow.com/questions/78885395/val-accuracy-inst-changing-and-sometimes-it-alternates-between-it-complement-10</link>
      <description><![CDATA[我被分配根据我读过的一篇论文来实现一个机器学习模型。
这篇论文实现了一个用于属性分类的多任务学习模型（带标签的图像是模型输入，带标签的意思是属性注释，每幅图像有 40 个）。
这是一个多任务学习模型，因为在模型输入层和 40 个属性分支之后有一个共享的密集层，每个分支都有自己的损失函数（所有分支的二元交叉熵）和自己的 S 型激活函数（在最后一层，用于预测 40 个属性中的每一个是否存在于图像中）。
经过大量艰苦的努力，它终于开始在所有分支上返回所有 S 型函数的概率，但只有 val_accuracy 的概率是错误的：val_loss 和损失（训练损失）越来越小，acc（训练准确率）也在正常的概率值范围内，除了 val_accuracy 总是相同的值或它的补码。
例如（仅举 5 个时期为例）：
40 个分支之一的一个属性预测的准确率：
5_o_Clock_Shadow_Accuracy
0 0.823665
1 0.891178
2 0.891178
3 0.891178

同一属性的损失：
 5_o_Clock_Shadow_loss
0 0.921046
1 0.701494
2 0.913597
3 0.765397
4 0.894950

val_loss：
val_5_o_Clock_Shadow_loss
0 730232.750000
1 300412.500000
2 376215.843750
3 0.747685
4 1.607191

最后是 val_Accuracy：
val_5_o_Clock_Shadow_Accuracy
0 0.882382
1 0.117618
2 0.882382
3   0.882382 4 0.882382  我的模型： def subnet(shared_layers_output, i): att_branch = Dense(512, name=&#39;dense_&#39;+str(i)+&#39;_1&#39;)(shared_layers_output) att_branch = ReLU()(att_branch) att_branch = BatchNormal ization()(att_branch) att_branch = Dropout(0.5)(att_branch) att_branch = Dense(512, name=&#39;dense_&#39;+str(i)+&#39;_2&#39;)(att_branch) att_branch = ReLU()(att_branch) att_branch = BatchNormalization()(att_branch) att_branch = Dropout(0.5)(att_branch)

branch_output = Dense(1, name=att_list[i],activation=&#39;sigmoid&#39;)(att_branch)

return branch_output

def multi_task_model():

#输入
input_layer = Input(shape=(512,), name=&#39;input_layer&#39;)

#共享网络（1 个网络）
shared_x = Dense(512, name=&#39;shared_dense_layer&#39;)(input_layer)
shared_x = ReLU()(shared_x)
shared_x = BatchNormalization()(shared_x)
shared_x = Dropout(0.5)(shared_x)

branch_outputs = list()
for i in range(40):
branch_outputs.append(subnet(shared_x, i))

model = Model(input_layer, branch_outputs, name=&#39;model&#39;)

返回模型


训练和测试输入形状：(n_samples, 512)
训练和测试标签输入形状：(40, n_samples)
学习率：1e-03

5_o_Clock_Shadow 损失、val_loss、acc 和 val_acc 超过 5 个时期
 损失 val_loss acc val_acc
0 0.422385 1.949578 0.864272 0.8873
1 0.354094 151.987991 0.888797 0.1127
2 0.354356 58.867992 0.888797 0.1127
3 0.352891 94.257980 0.888797 0.1127
4 0.353390 10.997763 0.888797 0.1127
]]></description>
      <guid>https://stackoverflow.com/questions/78885395/val-accuracy-inst-changing-and-sometimes-it-alternates-between-it-complement-10</guid>
      <pubDate>Sun, 18 Aug 2024 18:38:14 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：无法同步创建数据集（名称已存在）</title>
      <link>https://stackoverflow.com/questions/78429387/valueerror-unable-to-synchronously-create-dataset-name-already-exists</link>
      <description><![CDATA[当我尝试将模型保存为 h5 时
caption_model.save(&quot;/kaggle/working/mymodel.h5&quot;)

我出现了这个错误
ValueError Traceback (most recent call last)
Cell In[19], line 1
----&gt; 1 caption_model.save(&quot;/kaggle/working/mymodel.h5&quot;)

文件 /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 中引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 /opt/conda/lib/python3.10/site-packages/h5py/_hl/group.py:183，在 Group.create_dataset(self, name, shape, dtype, data, **kwds) 中
180 parent_path，name = name.rsplit(b&#39;/&#39;, 1)
181 group = self.require_group(parent_path)
--&gt; 183 dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds)
184 dset = dataset.Dataset(dsid)
185 返回 dset

文件 /opt/conda/lib/python3.10/site-packages/h5py/_hl/dataset.py:163，位于 make_new_dset(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)
160 其他：
161 sid = h5s.create_simple(shape, maxshape)
--&gt; 163 dset_id = h5d.create(parent.id, name, tid, sid, dcpl=dcpl, dapl=dapl)
165 if (data is not None) and (not isinstance(data, Empty)):
166 dset_id.write(h5s.ALL, h5s.ALL, data)

File h5py/_objects.pyx:54, in h5py._objects.with_phil.wrapper()

File h5py/_objects.pyx:55, in h5py._objects.with_phil.wrapper()

File h5py/h5d.pyx:137, in h5py.h5d.create()

ValueError: 无法同步创建数据集（名称已存在）

你们中有人遇到过这个问题吗？或者知道如何解决吗？是吗？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78429387/valueerror-unable-to-synchronously-create-dataset-name-already-exists</guid>
      <pubDate>Sat, 04 May 2024 14:51:33 GMT</pubDate>
    </item>
    </channel>
</rss>