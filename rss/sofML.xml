<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 13 Mar 2024 06:18:29 GMT</lastBuildDate>
    <item>
      <title>提高特殊神经网络的准确性</title>
      <link>https://stackoverflow.com/questions/78151448/increase-accuracy-in-a-special-neural-network</link>
      <description><![CDATA[事实上，我正在训练一个神经网络来玩 2048 游戏，并且大部分时间都达到 2048。
（如果您不了解该游戏，可以此处玩它来了解我在说什么）
首先，我收集了近 30,000 个游戏的数据集，在 csv 文件中达到 2048 个。每个游戏包含大约 1000 个棋盘游戏状态，可以说我的数据集中有 3000 万个棋盘状态。实际上有 3000 万行，如下图所示。

正如你所见，我每行有 17 列。前 16 列显示每个图块值的 2 的幂（预计 0 表示该图块中没有任何内容）。
最后一列是它根据这个棋盘游戏移动的方向
方向帮助-&gt; （0：上，1：右，2：下，3：左）
例如上图显示了董事会的这种状态：
&lt;前&gt;&lt;代码&gt; 32 64 128 32
 8 32 8 2
 8 0 0 0
 0 2 0 0

此状态的方向为 0，等于向上
所以我创建了一个具有 16 个输入的神经网络（当然我将其更改为 16*11 输入并将输入作为 one-hot 编码传递）、一些隐藏层和 4 个输出。
我在超参数和改变层结构方面进行了很多尝试和错误。
最后我得到了这样的结果。
导入 pandas 作为 pd
从 keras.models 导入顺序
从 keras.layers 导入密集、批量标准化、激活
从 keras.optimizers 导入 Adam
从 keras.callbacks 导入 EarlyStopping
从 sklearn.preprocessing 导入 OneHotEncoder
将 numpy 导入为 np

# 加载你的数据集
数据 = pd.read_csv(&#39;mainDataSet.csv&#39;)

# 分离输入 (X) 和输出 (y)
X = data.iloc[:, 0:16] # 输入特征
y = pd.get_dummies(data.iloc[:, -1]) # 将输出转换为 one-hot 编码

# 定义 one-hot 编码的类别（标签从 0 到 10）
类别 = [i for i in range(11)]

# 对每个输入列应用 one-hot 编码
X_encoded = pd.concat([pd.get_dummies(pd.Categorical(X[col],categories=categories), prefix=col,
prefix_sep=&#39;_&#39;) 对于 X] 中的列，轴=1)



# 定义模型
模型=顺序（）
model.add(Dense(16*11, input_dim=16*11,activation=&#39;relu&#39;)) # 默认包含偏差
model.add(BatchNormalization()) # 添加批量归一化层
model.add(Dense(256,activation=&#39;relu&#39;)) # 默认情况下包含偏差
model.add(BatchNormalization()) # 添加批量归一化层
model.add(Dense(256,activation=&#39;relu&#39;)) # 默认情况下包含偏差
model.add(BatchNormalization()) # 添加批量归一化层
model.add(Dense(256,activation=&#39;relu&#39;)) # 默认情况下包含偏差
model.add(BatchNormalization()) # 添加批量归一化层
model.add(Dense(256,activation=&#39;relu&#39;)) # 默认情况下包含偏差
model.add(BatchNormalization()) # 添加批量归一化层d
model.add(Dense(4,activation=&#39;softmax&#39;)) # 具有 4 个方向节点的输出层

# 定义提前停止回调，以在验证损失停止改善时停止训练
Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 5，restore_best_weights = True）

# 使用 Adam 优化器和默认学习率编译模型
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=&#39;adam&#39;, 指标=[&#39;accuracy&#39;])

# 使用小批量梯度下降和附加回调来训练模型
model.fit（X_encoded，y，epochs = 100，batch_size = 64，validation_split = 0.2，callbacks =
[早停]）

model.save(&#39;2048_model.h5&#39;)

最终它给了我 88% 的准确度和 90% 的验证准确度，但这对于我正在做的事情来说还不够。
那么您建议采取哪些方法来使我的模型在预测方面表现更好？
或者甚至您可以对我的神经网络进行哪些更改以使其更加高效？
你知道我是人工智能世界的新手，我可能不知道很多事情。因此，请随意告知您所知道的任何信息来改进此模型。]]></description>
      <guid>https://stackoverflow.com/questions/78151448/increase-accuracy-in-a-special-neural-network</guid>
      <pubDate>Wed, 13 Mar 2024 05:36:20 GMT</pubDate>
    </item>
    <item>
      <title>简单的 Pytorch LSTM 模型无法学习反事实创建</title>
      <link>https://stackoverflow.com/questions/78151232/simple-pytorch-lstm-model-not-learning-for-counterfactual-creation</link>
      <description><![CDATA[我正在 Pytorch 中构建 LSTM，以创建水流时间序列的反事实预测。为了预测第 i 天的水流 ($\hat{y}_i$)，我的特征集 $X_i$ 包含当天以及过去几天的天气数据。其他模型已经很好地学习了信号（即 LGBM），即使使用惩罚回归，特征集也可以解释水流的大部分变化。
尽管如此，LSTM 似乎并没有学到太多东西，而是学到了平均值。这与我在学习过程中传递数据的方式有关吗？
（丢弃超参数，这些超参数是随机初始化的。稍后我将使用 optuna 对其进行优化）
非常感谢您的帮助！
代码：
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

类 LSTMModel(nn.Module):
    def __init__(自身、输入大小、隐藏大小、层数、输出大小):
        super(LSTMModel, self).__init__()
        self.hidden_​​size = 隐藏大小
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size,hidden_​​size,num_layers,batch_first=True)
        self.fc = nn.Linear(隐藏大小, 输出大小)

    def 前向（自身，x）：
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_​​size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_​​size).to(device)
        
        # print(f“lstm 调用开始处的 x 大小：{x.size()}”)

        out, _ = self.lstm(x, (h0, c0))

        # print(f&quot;前向调用开始时的 x 大小：{x.size()}&quot;)
        out = self.fc(out[:, -1, :]) # 获取最后一个时间步的输出
        返回

dta =“../ml_dataset_Jan24.dta”

数据 = pd.read_stata(dta)

data[&#39;datetime&#39;] = data.apply(lambda x: pd.to_datetime(f&quot;{int(x[&#39;年&#39;])}-{int(x[&#39;月&#39;])}-{int(x[&#39;天&#39;])}&quot;，格式=&#39;%Y-%m-%d&#39;)，轴=1)
data.set_index(&#39;datetime&#39;, drop=True, inplace=True)
data.drop(columns=[&#39;年&#39;,&#39;月&#39;,&#39;日&#39;], inplace=True)

# 创建特征和目标集
X_train = data.loc[(data.index&gt;&#39;1990-12-31&#39;) &amp; (data.index&lt;&#39;2006-01-01&#39;), data.columns != &#39;level_cs&#39;].values
y_train = data.loc[(data.index&gt;&#39;1990-12-31&#39;) &amp; (data.index&lt;&#39;2006-01-01&#39;), &#39;level_cs&#39;].values


X_train = np.array(X_train)
X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
y_train = np.array(y_train)
y_train = y_train.reshape(y_train.shape[0], 1, 1)


X = torch.from_numpy(X_train).float()
y = torch.from_numpy(y_train).float()


# 定义超参数
input_size = 1332 # 特征数量
隐藏大小 = 128
层数 = 5
输出大小 = 1

# 定义训练设备（CPU 或 GPU）
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

# 实例化模型
模型 = LSTMModel(input_size,hidden_​​size,num_layers,output_size).to(device)

# 定义损失函数和优化器
标准 = nn.MSELoss()
优化器 = torch.optim.Adam(model.parameters(), lr=0.001)

# 训练模型
纪元数 = 500
对于范围内的纪元（num_epochs）：
    模型.train()
    优化器.zero_grad()

    # print(f&#39;X 尺寸: &#39;, X.size())
    输出 = 模型(X.to(设备))
    损失 = 标准（输出，y.to（设备））
    loss.backward()
    优化器.step()
    
    print(f&#39;Epoch [{epoch+1}/{num_epochs}], 损失: {loss.item():.4f}&#39;)

# 将模型设置为评估模式
模型.eval()

# 推论
# 使用 torch.no_grad():
# 预测 = 模型(X_test.to(设备))

# print(“样本预测：”, 预测)`

Y_test 预测基本上只是平均值
我批量传递了数据，Y_train 的时间戳与 X_train 中的时间戳相同，因为我想使用日期“i”中的数据来预测水流“i”。该功能集还包括过去的天气数据，我不确定是否应该如此，或者该数据的重要性是否会被记忆机制保留]]></description>
      <guid>https://stackoverflow.com/questions/78151232/simple-pytorch-lstm-model-not-learning-for-counterfactual-creation</guid>
      <pubDate>Wed, 13 Mar 2024 04:21:51 GMT</pubDate>
    </item>
    <item>
      <title>多元回归任务，其中有些变量很容易预测，有些变量很难预测，有些则无法预测</title>
      <link>https://stackoverflow.com/questions/78151203/multivariate-regression-task-where-some-variables-are-easy-some-hard-and-some-i</link>
      <description><![CDATA[我正在尝试在 keras 中创建一个多元回归模型，但无论输入是否正确，模型最终都会预测单个值。
我尝试修复学习率、批量大小和模型架构，但到目前为止我无法修复它。
经过进一步检查，我知道输出向量中的某些值对于模型来说很难或不可能预测（例如从他喜欢的汽车品牌推断出一个人的身高），但由于问题的性质，我不知道这些难以或不可能预测的变量是哪些。
我目前正在使用 MSE 损失。我怀疑我必须使用或创建一个自定义损失函数来处理此类问题。
Chatgpt提出了异方差损失，但我不明白它的意思。]]></description>
      <guid>https://stackoverflow.com/questions/78151203/multivariate-regression-task-where-some-variables-are-easy-some-hard-and-some-i</guid>
      <pubDate>Wed, 13 Mar 2024 04:12:10 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试使用数据集包创建数据集时，出现“无法转换，因为列名称不匹配”错误</title>
      <link>https://stackoverflow.com/questions/78151170/im-getting-couldnt-cast-because-column-names-dont-match-error-while-i-was-t</link>
      <description><![CDATA[DataFrame 结构
上图显示了我的数据结构。
from sklearn.model_selection import train_test_split
从数据集中导入特征、ClassLabel、值、数据集、DatasetDict

df_train, df_tmp = train_test_split(
        movie_df,stratify=movie_df[“标签”], test_size=0.2)

df_val, df_test = train_test_split(
        df_tmp,stratify=df_tmp[“标签”], test_size=0.5)

ds_features = Features({“text”: Value(“string”), “label”: ClassLabel(names=labels)})

数据集 = DatasetDict({
    “火车”：Dataset.from_pandas(df_train.reset_index(drop=True),features=ds_features),
    “有效”：Dataset.from_pandas(df_val.reset_index(drop=True),features=ds_features),
    “测试”：Dataset.from_pandas(df_test.reset_index(drop=True),features=ds_features)})

数据集

这段代码给了我一个值错误，如下所示：
错误
错误
我期待类似的东西，但不具有相同的值：
DatasetDict({
    火车：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：13267
    })
    有效：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：1658
    })
    测试：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：1659
    })
})

谁能告诉我我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78151170/im-getting-couldnt-cast-because-column-names-dont-match-error-while-i-was-t</guid>
      <pubDate>Wed, 13 Mar 2024 04:00:13 GMT</pubDate>
    </item>
    <item>
      <title>有人有可行的 Atari 突破性深度 Q 学习实现吗？</title>
      <link>https://stackoverflow.com/questions/78151115/does-anyone-have-an-atari-breakout-deep-q-learning-implementation-that-works</link>
      <description><![CDATA[我一直在尝试使用 Keras 提供的实现，但收到此错误：
19 # 由于 Deepmind 辅助函数而使用基线 Atari 环境
---&gt; 20 env = make_atari(“BreakoutNoFrameskip-v4”)
21 # 扭曲帧，灰度，放四帧并缩放到更小的比例
22 env=wrap_deepmind(env,frame_stack=True,scale=True)
3帧
/usr/local/lib/python3.10/dist-packages/gym/envs/atari/environment.py 种子（自身，种子）
185
186 如果不是 hasattr(roms, self._game):
--&gt; 187 引发错误。错误（
188 f&#39;我们无法找到游戏“{self._game}”。注意：Gym 不再分发 ROM。 &#39;
189 f“如果您拥有将必要的 ROM 用于研究目的的许可证，您可以下载它们”
错误：我们无法找到“Breakout”游戏。注意：Gym 不再分发 ROM。如果您拥有将必要的 ROM 用于研究目的的许可证，您可以通过 pip installgym[accept-rom-license] 下载它们。否则，您应该尝试导入“Breakout”通过命令ale-import-roms。如果您认为这是一个错误，也许您的“Breakout”副本是错误的。不受支持。要检查是否属于这种情况，请尝试提供环境变量 PYTHONWARNINGS=default::ImportWarning:ale_py.roms。有关更多信息，请参阅：https://github.com/mgbellemare/Arcade-Learning -环境#rom-管理
任何帮助将不胜感激，谢谢。
我尝试过许多版本的 Atari 环境（这里括号里的东西
就这一行 --&gt; env = make_atari(“BreakoutNoFrameskip-v4”).我尝试过不同的版本等。一个有趣的注意事项是，当我使用“ALE/Breakout-v5”时，我遇到了不同的错误。
NamespaceNotFound Traceback（最近一次调用最后一次）
 在&lt;细胞系：81&gt;()
79
80 # 由于 Deepmind 辅助函数而使用 Baseline Atari 环境
---&gt; 81 env = make_atari(“ALE/Breakout-v5”)
82 # 扭曲帧，灰度，放四帧并缩放到更小的比例
83 env = wrap_deepmind(env,frame_stack=True,scale=True)
4帧
_check_namespace_exists(ns) 中的/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py
178）
179
--&gt; 180 raise error.NamespaceNotFound(f“未找到命名空间 {ns}。{suggestion_msg}”)
181
182
NamespaceNotFound：未找到命名空间 ALE。您是否为 ALE 安装了正确的软件包？
最终，我非常迷失，如果有人可以链接我可以运行的 Atari Breakout 的功能版本，我将非常感激。]]></description>
      <guid>https://stackoverflow.com/questions/78151115/does-anyone-have-an-atari-breakout-deep-q-learning-implementation-that-works</guid>
      <pubDate>Wed, 13 Mar 2024 03:36:22 GMT</pubDate>
    </item>
    <item>
      <title>使用时间序列数据和离散质量测量进行制造质量预测</title>
      <link>https://stackoverflow.com/questions/78150896/manufacturing-qaulity-prediction-with-timeseries-data-and-discrete-quality-measu</link>
      <description><![CDATA[我有 18 个批次的制造过程的时间序列数据，每个批次有 6 个变量的 2000 行时间序列数据。我还每批次进行 5 次输入质量测量，并尝试预测每批次测量一次的最终产品质量。以下是我拥有的所有数据的示例格式。我能否知道可以使用这些数据构建什么样的模型，以及如何处理时间序列数据以得出结合了时间序列数据和输入条件信息的预测模型。我已经尝试过一些模型，将时间序列数据转换为宽格式，使每个时间戳成为一个变量，并合并输入条件以创建 PLS 预测模型，其准确度为 50%。我想探索机器学习方法来对这些数据进行建模，但我是机器学习的新手。任何想法表示赞赏。谢谢
]]></description>
      <guid>https://stackoverflow.com/questions/78150896/manufacturing-qaulity-prediction-with-timeseries-data-and-discrete-quality-measu</guid>
      <pubDate>Wed, 13 Mar 2024 02:08:13 GMT</pubDate>
    </item>
    <item>
      <title>使用 sklearn，其中标签是多个输入的组合</title>
      <link>https://stackoverflow.com/questions/78150382/using-sklearn-where-the-label-a-combination-of-multiple-inputs</link>
      <description><![CDATA[我正在对分类标签相互关联的数据集进行数据分析。
我的标签跟踪实验条件。
就我而言，标签跟踪两种化学物质的组合浓度，这些化学物质产生由 n 个特征测量的输出。
使用分类标签代替化学物质组合的浓度是最佳做法，还是有更好的方法？
以下是分类标签与其代表的现实生活状况之间的转换示例。

&lt;标题&gt;

条件
化学1
化学2


&lt;正文&gt;

1
1
0


2
2
0


3
0
1


4
0
2


5
1
1


6
1
2


]]></description>
      <guid>https://stackoverflow.com/questions/78150382/using-sklearn-where-the-label-a-combination-of-multiple-inputs</guid>
      <pubDate>Tue, 12 Mar 2024 22:39:35 GMT</pubDate>
    </item>
    <item>
      <title>pytorch sdpa 与相对位置嵌入的兼容性</title>
      <link>https://stackoverflow.com/questions/78150102/pytorch-sdpa-compatibility-with-relative-positional-embeddings</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78150102/pytorch-sdpa-compatibility-with-relative-positional-embeddings</guid>
      <pubDate>Tue, 12 Mar 2024 21:22:17 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow 中，如何使用图像中的对象和相应标签创建数据集</title>
      <link>https://stackoverflow.com/questions/78149741/in-tensorflow-how-do-i-make-a-dataset-with-objects-corresponding-labels-from-a</link>
      <description><![CDATA[我有一个符号键，我正在寻找有关提取这些符号及其文本作为数据集标签的方法的建议；稍后用于匹配工程图上的这些符号。
我只是在寻找正确方向的推动力，我有 Tensorflow 的经验；只要非常确定如何解决这个问题即可。
简单示例
https://i.stack.imgur.com/5RBau.png 
复杂示例
https://i.stack.imgur.com/pRLWf.png 
正在寻找正确方向的推动力，感谢大家的帮助！
我没有尝试太多，因为我很困惑首先要尝试什么，我以前没有使用过这样的文档。]]></description>
      <guid>https://stackoverflow.com/questions/78149741/in-tensorflow-how-do-i-make-a-dataset-with-objects-corresponding-labels-from-a</guid>
      <pubDate>Tue, 12 Mar 2024 19:57:15 GMT</pubDate>
    </item>
    <item>
      <title>尝试在 Python 中使用多处理库，但我遇到了它冻结但不抛出错误的问题</title>
      <link>https://stackoverflow.com/questions/78149659/trying-to-use-the-multiprocessing-library-in-python-but-i-am-running-into-issues</link>
      <description><![CDATA[所以我在咨询chatgpt后编写了这段代码，它在很大程度上有效：
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.metrics 导入 precision_score
来自多处理导入池，cpu_count

def评估_子集（模型，评分，X_in，y_in，子集=无）：
    #model = RandomForestClassifier(n_estimators=100, random_state=42)
    列表分数 = []
    对于 skf.split(X_in, y_in) 中的 train_index、test_index：
        X_train, y_train = X_in.values[train_index], y_in.values[train_index]
        X_test, y_test = X_in.values[test_index], y_in.values[test_index]
        model.fit(X_train[:, 子集], y_train)
        y_pred = model.predict(X_test[:, 子集])
        list_scores.append(评分(y_test, y_pred))
    返回 np.mean(list_scores)

def stepwise_add_selection（模型，评分，X_in，y_in，n_processes =无）：
    如果 n_processes 为 None：
        n_processes = cpu_count()

    池 = 池（进程=n_进程）
    剩余特征 = 设置（范围（X_in.shape[1]））
    选定的特征 = []
    最佳准确度 = 0
    而剩余特征：
        结果=[]
        对于剩余特征中的特征：
            子集 = 选定的特征 + [特征]
            results.append(pool.apply_async(evaluate_subset, args=(模型, 评分, X_in, y_in, 子集)))
        精度 = [结果中的 res.get()]
        best_index = np.argmax(精度)
        print(“当前最佳”)
        打印（最大（精度））
        print(&quot;上一个最佳值&quot;)
        打印（最佳准确度）
        打印（选定的特征）
        如果 best_accuracy &lt;最大（精度）：
            selected_features.append(列表(剩余特征)[(best_index)])
            最佳准确度 = 准确度[最佳索引]
        别的：
            休息
        
    池.close()
    池.join()

    返回selected_features，best_accuracy


但是，我正在尝试创建另一个删除功能的贪婪搜索：
&lt;前&gt;&lt;代码&gt;
def stepwise_feature_removal（模型，评分，X_in，y_in，n_processes =无）：
    剩余特征 = 设置（范围（X_train.shape[1]））
    选定的特征 = 列表（剩余特征）
    best_accuracy=evaluate_subset（模型，评分，X_in，y_in，selected_features）
    print(&quot;初始准确度分数：&quot;, best_accuracy)
    而剩余特征：
        结果=[]
        最坏的特征 = 无
        池 = 池（进程=n_进程）
        对于剩余特征中的特征：
            temp_features = 选定的_features[:]
            temp_features.remove（功能）
            results.append(pool.apply_async(evaluate_subset, args=(模型, 评分, X_in, y_in, temp_features)))
        池.close()
        池.join()
        精度 = [结果中的 res.get()]
        best_index = np.argmax(精度)
        如果准确度[best_index] &gt;最佳准确度：
            最佳准确度 = 准确度[最佳索引]
            最坏的特征 = temp_features[最佳索引]
        print(“当前最佳”)
        打印（准确度）
        print(&quot;上一个最佳值&quot;)
        打印（最佳准确度）
        print(&quot;已删除的功能：&quot;)
        打印（最差特征）

        如果最坏的特征不是无：
            selected_features.remove(worst_feature)
            剩余特征.删除（最差特征）
        别的：
            休息

    返回selected_features，best_accuracy

在功能删除方法中，我遇到的问题是程序停止运行。它并不表明存在错误或任何情况。我添加了 pool.close() 和 pool.join() 但它没有解决问题。
提前致谢。
我正在尝试编写一个贪婪特征缩减函数，其工作原理与贪婪特征添加函数类似。不知道为什么会结冰，所以这也会有帮助。
编辑：我应该澄清当我使用 imblearn 包运行此代码时出现的问题。如果没有 imblearn，可能会发生多处理并且程序会运行。
def use_pipeline(clf, resample = False):
    如果重新采样==假：
        管道 = make_pipeline(MinMaxScaler(), clf)
    别的：
        管道= make_pipeline（重新采样，MinMaxScaler（），clf）
    回水管
sm = SMOTE（随机状态 = 38）
pipeline_clf = use_pipeline(clf1, sm)
stepwise_feature_removal(pipe_clf, matthews_corrcoef, X_train, y_train, 15)
]]></description>
      <guid>https://stackoverflow.com/questions/78149659/trying-to-use-the-multiprocessing-library-in-python-but-i-am-running-into-issues</guid>
      <pubDate>Tue, 12 Mar 2024 19:36:45 GMT</pubDate>
    </item>
    <item>
      <title>Amazon Sagemaker 在后台从 jupyter 笔记本运行代码</title>
      <link>https://stackoverflow.com/questions/78149372/amazon-sagemaker-run-code-from-jupyter-notebook-in-background</link>
      <description><![CDATA[我正在 Amazon Sagemkaer 笔记本实例上运行代码（在普通的 jupyter 笔记本中，而不是 jupyterLab）。

如何在后台运行代码并关闭浏览器选项卡？当我关闭 jupyter 笔记本选项卡时，程序停止，我想避免这种情况。我读到我不应该在笔记本本身中进行处理，而应该使用 Sagemaker 处理作业。如何在更高的 i 上运行如下所示的简单代码单元

df_new[&#39;predicted_values&#39;] = df_original.progress_apply(lambda x: LLM_pretrained_model.predict( x[&#39;comment_body&#39;] )


12 小时后，内核崩溃，提示我需要再次登录。我怎样才能避免这种情况？由于我的数据量较大，程序至少需要 28 小时才能运行

是否可以从 sagemaker jupyter 笔记本（不是 jupyterLab 笔记本）将代码推送到 GitHub？

]]></description>
      <guid>https://stackoverflow.com/questions/78149372/amazon-sagemaker-run-code-from-jupyter-notebook-in-background</guid>
      <pubDate>Tue, 12 Mar 2024 18:39:54 GMT</pubDate>
    </item>
    <item>
      <title>结合区块链和机器学习可以实现什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78149327/what-can-i-implement-combining-blockchain-and-machine-learning</link>
      <description><![CDATA[我想知道我们如何使用区块链和机器学习，也许有人可以给我一个项目的想法，我可以实施它来制作我的研究项目。我想结合机器学习 + 区块链（关于网络安全领域）。如果您能给我一些论文或 github 相关代码的想法，那就太好了。
对于这个项目我想学习：

区块链基础知识并实现一些区块链
学习机器学习并以某种方式将机器学习集成到该区块链中
]]></description>
      <guid>https://stackoverflow.com/questions/78149327/what-can-i-implement-combining-blockchain-and-machine-learning</guid>
      <pubDate>Tue, 12 Mar 2024 18:31:34 GMT</pubDate>
    </item>
    <item>
      <title>深度学习 (LSTM) 项目实习生 [已关闭]</title>
      <link>https://stackoverflow.com/questions/78146798/project-intern-working-on-deep-learnning-lstm</link>
      <description><![CDATA[我使用深度学习开发了单词预测模型。我已经保存并加载了我的模型，但我想知道如何使用该模型进行预测，而无需下次运行纪元
我已经开发了下一个单词预测模型，并对它进行了训练，然后保存了它，但现在我想重用该模型，而无需再次运行纪元]]></description>
      <guid>https://stackoverflow.com/questions/78146798/project-intern-working-on-deep-learnning-lstm</guid>
      <pubDate>Tue, 12 Mar 2024 11:40:33 GMT</pubDate>
    </item>
    <item>
      <title>在 SageMaker 上的 TensorFlow Recommenders 中初始化 FactorizedTopK 时出错：“无法将‘计数器’转换为形状”</title>
      <link>https://stackoverflow.com/questions/78144515/error-initializing-factorizedtopk-in-tensorflow-recommenders-on-sagemaker-cann</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78144515/error-initializing-factorizedtopk-in-tensorflow-recommenders-on-sagemaker-cann</guid>
      <pubDate>Tue, 12 Mar 2024 03:28:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 Huggingface MT5 模型中执行批量编码时会得到不同的嵌入？</title>
      <link>https://stackoverflow.com/questions/78139855/why-do-i-get-different-embeddings-when-i-perform-batch-encoding-in-huggingface-m</link>
      <description><![CDATA[我正在尝试使用 HuggingFace 的 mt5-base 模型对一些文本进行编码。我使用的模型如下所示
从转换器导入 MT5EncoderModel、AutoTokenizer

模型 = MT5EncoderModel.from_pretrained(“google/mt5-base”)
tokenizer = AutoTokenizer.from_pretrained(“google/mt5-base”)

def get_t5_embeddings(文本):
    last_hidden_​​state = model(input_ids=tokenizer(texts, return_tensors=“pt”, padding=True).input_ids).last_hidden_​​state
    pooled_sentence = torch.max(last_hidden_​​state, 暗淡=1)
    返回 pooled_sentence[0].detach().numpy()

当我注意到相同的文本与其自身的余弦相似度分数较低时，我正在做一些实验。我做了一些挖掘，意识到如果我批量进行编码，模型会返回非常不同的嵌入。为了验证这一点，我运行了一个小实验，逐步生成 Hello 的嵌入和 10 个 Hello 的列表。并检查列表中 Hello 和第一个 Hello 的嵌入（两者应该相同）。
对于范围 (1, 10) 内的 i：
    print(i, (get_t5_embeddings([“你好”])[0] == get_t5_embeddings([“你好”]*i)[0]).sum())

这将返回嵌入中相互匹配的值的数量。
结果是这样的：
&lt;前&gt;&lt;代码&gt;1 768
2 768
3 768
4 768
5 768
6 768
7 768
8 27
9 27

每次运行它时，如果批量大小超过 768，就会出现不匹配情况。
为什么我会得到不同的嵌入以及如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78139855/why-do-i-get-different-embeddings-when-i-perform-batch-encoding-in-huggingface-m</guid>
      <pubDate>Mon, 11 Mar 2024 10:14:53 GMT</pubDate>
    </item>
    </channel>
</rss>