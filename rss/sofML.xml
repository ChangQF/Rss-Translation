<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Tue, 11 Mar 2025 15:20:23 GMT</lastBuildDate>
    <item>
      <title>如何使用Python SDK将环境变量传递到亚马逊萨吉式制造商的自定义培训脚本？</title>
      <link>https://stackoverflow.com/questions/79500324/how-can-i-pass-environment-variables-to-a-custom-training-script-in-amazon-sagem</link>
      <description><![CDATA[我正在使用Amazon Sagemaker中的脚本进行自定义模型，并使用Python SDK启动这项工作。我想将一些环境变量（例如API键或配置标志）传递到培训作业，以便通过OS.Environ在脚本中访问它们。
这是我的代码的简化版本：
 来自sagemaker.stimator导入估算器

估算器=估算器（
    image_uri =&#39;123456789012.dkr.ecr.us-west-2.amazonaws.com/my-custom-image：最新图像&#39;，
    角色=角色，
    instance_count = 1，
    instance_type =&#39;ml.g5.xlarge&#39;，
    entry_point =&#39;train.py&#39;，
    source_dir =&#39;src&#39;，
    环境= {
        &#39;my_api_key&#39;：&#39;abcdef123456&#39;，
        &#39;debug_mode&#39;：&#39;true&#39;
    }
）
 
在我的培训脚本中，我尝试读取变量：
 导入OS

api_key = os.environ.get（&#39;my_api_key&#39;）
打印（＆quot; api键：＆quot; api_key）
 
这是使用Python SDK将环境变量传递给萨吉人培训工作的正确方法吗？我应该注意任何局限性或最佳实践，特别是对于诸如API键之类的敏感信息？]]></description>
      <guid>https://stackoverflow.com/questions/79500324/how-can-i-pass-environment-variables-to-a-custom-training-script-in-amazon-sagem</guid>
      <pubDate>Tue, 11 Mar 2025 10:00:30 GMT</pubDate>
    </item>
    <item>
      <title>当我试图运行命令spartlit运行main.py时，为什么我会得到RuntimeError：没有运行事件循环，并且在我的VS代码中？</title>
      <link>https://stackoverflow.com/questions/79500227/why-am-i-getting-runtimeerror-no-running-event-loop-and-in-my-vs-code-when-i-am</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79500227/why-am-i-getting-runtimeerror-no-running-event-loop-and-in-my-vs-code-when-i-am</guid>
      <pubDate>Tue, 11 Mar 2025 09:34:06 GMT</pubDate>
    </item>
    <item>
      <title>如何删除具有不同文件名和大小但在Android中相同的内容的重复图像？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79499987/how-to-delete-duplicate-images-with-different-file-names-and-sizes-but-identical</link>
      <description><![CDATA[我在我的Android设备上有大量从iPhone传输的图像。不幸的是，我现在有许多重复的图像：

没有相同的文件名。
没有相同的文件大小或分辨率。
但包含相同的视觉内容（相同图像）。

我想根据其内容查找并删除这些重复的图像（不是基于文件名，大小或分辨率）。
我尝试通过Google App搜索文件中的内置选项，但找不到根据内容检测重复图像的任何选项。
是否有任何可以根据内容扫描和删除重复图像的Android应用程序或工具？另外，我可以使用任何Python脚本或开源工具来实现这一目标吗？我的目标是自动删除所有具有相同内容的重复图像，无论其文件名，大小或分辨率如何。]]></description>
      <guid>https://stackoverflow.com/questions/79499987/how-to-delete-duplicate-images-with-different-file-names-and-sizes-but-identical</guid>
      <pubDate>Tue, 11 Mar 2025 07:53:05 GMT</pubDate>
    </item>
    <item>
      <title>如何实现具有有限数据和较大姿势/样式变化的强大动漫角色搜索系统？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79499907/how-to-implement-a-robust-anime-character-search-system-with-limited-data-and-la</link>
      <description><![CDATA[我正在努力实施“动漫角色搜索”系统，我非常感谢您可能拥有的任何建议或参考。
使用经典的计算机视觉技术检测动漫图像是相对简单的。例如，一个众所周知的示例是Trace.Moe，它使用Lucene Image Search（LIRE）和其他传统方法准确地指出了给定屏幕截图的动漫剧集和时间戳。这种方法不一定依靠现代深度学习。它仅根据旧视觉算法使用特征提取。
然而，随着AI的兴起，许多人开始使用CNN，VIT或其他深度学习模型（通常来自拥抱面）进行图像特征提取和基于向量的搜索。在我自己的设置中，如果目标动漫角色图像已经索引，我可以达到100％的准确性（即使没有高级矢量数据库（例如基于HNSW的解决方案）），因为系统很容易检索相同或近乎功能的匹配。  
核心问题是，动漫角色的嵌入可能对姿势或样式的轻微变化也极为敏感。如果角色仅移动位置，则嵌入空间中的距离可能会飙升。我尝试通过对比度学习解决这个问题（特别是2022年左右的对比度方法），但到目前为止，结果一直不足。 
一个很大的挑战是数据集本身的性质：有很多字符标签，但每个字符相对较少，样式差异很大。有时，给定角色只有一个参考图像。当只有一两个图像开始时，典型的增强方法无济于事。我考虑使用ControlNet或类似技术生成更多图像来模拟不同的姿势和观点，但是由于参考图像，GAN或扩散模型很少，因此难以产生高质量的一致变化。
我还研究了诸如佛罗伦萨，dinov2和剪辑之类的自动接地方法，以解析或分割图像，然后尝试统一共享功能，但我不确定实践中的效果如何。总的来说，我感到卡住了。该域与标准图像搜索有所不同，因为数据有限，变化很大，即使在完全不同的姿势或艺术风格下，系统也需要识别相同的字符。
即使刮擦Kaggle和其他来源也只能产生几十万张图像，这远远不够覆盖那里的各种动漫角色。数据收集本身是一个巨大的挑战。
你们中有人处理类似问题吗？您是否知道建立强大的动漫角色搜索系统的最佳实践或相关参考，尤其是在此类数据筛选条件下？您有任何指针或建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79499907/how-to-implement-a-robust-anime-character-search-system-with-limited-data-and-la</guid>
      <pubDate>Tue, 11 Mar 2025 07:11:13 GMT</pubDate>
    </item>
    <item>
      <title>与拆分数量少的类交叉验证的错误[关闭]</title>
      <link>https://stackoverflow.com/questions/79499678/error-on-crossvalidation-with-classes-that-have-less-samples-than-number-of-spli</link>
      <description><![CDATA[我目前正在研究这个问题：
 https://github.com/scikit-com/scikit-learn/scikit-learn/scikit-learn/scikit-learn/scikit-learn/issues/30832  
我正在研究修复它的可能方法。
说明：
使用logistic回归和交叉验证时，在交叉验证时，样本少于拆分数量的类别少于每倍的样本，导致执行Python程序时出现错误。
即使这可能是一个数据问题，也应该有更好的方法来处理它。
复制错误的代码：
 来自sklearn.linear_model导入logisticRegressioncv
导入numpy作为NP
n，m = 20，5
x = np.random.randn（n，m）
y = np.random.randint（0，2，n）
y [-3：] = [3，4，5]
logisticRegressioncv（）。fit（x，y）
 
我想到的一些方法：

将系数设置为代表性不足的类中的0; 
根据我们拥有的真实数据自动创建新数据（即使仅是1个示例）; 
重复数据，直到最低样本的类达到分裂次数； 
简单地提出一个更有意义的例外。

这只是一个数据问题，我应该提出一个例外，还是我可以在这里做更多的事情？
您能给我一些有关解决此问题的好方法的提示或想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79499678/error-on-crossvalidation-with-classes-that-have-less-samples-than-number-of-spli</guid>
      <pubDate>Tue, 11 Mar 2025 04:44:50 GMT</pubDate>
    </item>
    <item>
      <title>我可以将Pytorch与Django Web框架集成在一起吗？</title>
      <link>https://stackoverflow.com/questions/79499340/can-i-integrate-pytorch-with-django-web-framework</link>
      <description><![CDATA[我希望使用Django在网站上创建一些游戏。我想对游戏进行一些机器学习，以便玩家可以与机器学习模型进行比赛。 Django和Pytorch的结合是否可以？我听说了一些称为ONNX的东西，可以帮助将模型提供到前端，我只是想仔细检查与Django一起使用的，而不仅仅是与Nodejs一起使用。如果它不起作用，那么我会感谢任何其他解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/79499340/can-i-integrate-pytorch-with-django-web-framework</guid>
      <pubDate>Mon, 10 Mar 2025 23:06:51 GMT</pubDate>
    </item>
    <item>
      <title>生成partialdependedateata函数在用于多类分类模型时返回错误</title>
      <link>https://stackoverflow.com/questions/79498849/generatepartialdependencedata-function-returns-error-when-used-for-multiclass-cl</link>
      <description><![CDATA[我已经使用MLR构建了XGBoost多类分类模型，我想为某些功能可视化部分依赖性。但是，如果我尝试使用 generatePartialDependedAta（）我会收到以下错误：

 Melt.data.table中的错误（AS.Data.table（OUT），MEATH.VARS = target，variable.name = if（td $ type ===：&#39;METAY.VARS&#39;中的一个或多个值无效。

我已经检查了 task.desc 在 task&gt; task 对象和 factor.levels.levels.levels 中的差异。此外，我毫不费力地使用相同的函数生成具有不同目标变量的回归XGBoost的数据。
我的目的是有问题，还是这是一个错误？
这是使用 palmerpenguins 数据集的示例：
 ＃库
图书馆（整洁）
图书馆（MLR）

Peng＆lt;  -  Palmerpenguins ::企鹅

＃数据分区
set.seed（1234）
Intrain＆lt ;-创建Atapartition（
  y =彭$种，
  p = 0.7，
  列表= f
）

＃构建任务
train_class＆lt;  -  peng [intrain，]％＆gt;％select（-sex，-year）％＆gt;％ 
  CreateMummyFeatures（target =;物种＆quots; cols =;岛; 
  makeClassIftask（data =。，target =;物种；）

＃建立学习者
xgb_class_learner＆lt;  -  makelearner（
  ＆quot“ classif.xgboost”
  predict.type =&#39;响应;
）

＃构建模型
XGB_CLASS＆lt;  - 火车（XGB_CLASS_LEARNER，TRAIN_CLASS）

＃产生部分依赖性
GeneratePartialDependedateData（XGB_Class，Train_class）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79498849/generatepartialdependencedata-function-returns-error-when-used-for-multiclass-cl</guid>
      <pubDate>Mon, 10 Mar 2025 18:27:01 GMT</pubDate>
    </item>
    <item>
      <title>如何训练网络以检测LIDAR PointCloud对象[关闭]</title>
      <link>https://stackoverflow.com/questions/79498544/how-to-train-a-network-to-detect-lidar-pointcloud-objects</link>
      <description><![CDATA[我正在使用OS1激光雷达传感器，因此我可以访问点云数据集。我需要能够识别对象。
我知道如何预处理数据，如何注释数据，并且我一直在阅读有关尖头柱和深入学习以学习如何训练网络的信息，但是没有存储库来解释如何在自定义数据上进行操作。 
如何训练网络以获取自定义数据？你有消息来源吗？他们中的大多数与汽车或行人有关，但我想确定自己的物体。
我一直在使用MATLAB可视化和注释我感兴趣的对象，但是我无法继续下一步，因为我不了解它们。
https://www.mathworks.com/help/lidar/ug/object-detection-with-point-clouds.html]]></description>
      <guid>https://stackoverflow.com/questions/79498544/how-to-train-a-network-to-detect-lidar-pointcloud-objects</guid>
      <pubDate>Mon, 10 Mar 2025 16:00:24 GMT</pubDate>
    </item>
    <item>
      <title>损失的计算梯度W.R.T学习率Pytorch</title>
      <link>https://stackoverflow.com/questions/79498420/computing-gradient-of-loss-w-r-t-learning-rate-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79498420/computing-gradient-of-loss-w-r-t-learning-rate-pytorch</guid>
      <pubDate>Mon, 10 Mar 2025 15:11:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么投票表决为“努力”的投票表现有所不同？</title>
      <link>https://stackoverflow.com/questions/79474361/why-votingclassifer-performance-with-voting-set-to-hard-is-different-with-diff</link>
      <description><![CDATA[我想从Sklearn和不同参数进行比较性能测试投票classifier。我使用了param网格，然后发现一些难以理解的东西。
我准备了三个分类器
  gnb = gaussiannb（）＃准确性0.795
lr = logisticRegress（）＃准确性0.7925
RFC = RandomforestClassifier（）＃准确性0.94

 
然后我做了两个VaitingClassifiers。两者都有有效的设置为“硬”。但是重量不同。该决定是由多数投票做出的，但其准确性是不同的，这是如何可能的？
  vc_hard_equals = fotingClassifier（estionators = [[
        （&#39;naivebayes＆quot; gnb）， 
        （“ LogisticRegression＆quot”，lr）， 
        （&#39;Randomforest＆quot＆quot; rfc）
    ]，， 
    投票=“硬＆quot” 
    权重=（1，1，1），＃等于权重
    ）
vc_hard_forest_priority = fotingClassifier（估算= [[
        （&#39;naivebayes＆quot; gnb）， 
        （“ LogisticRegression＆quot”，lr）， 
        （“ rancomforest”，rfc），]， 
    投票=“硬＆quot” 
    权重=（1，1，3），＃更大的随机孔（在这种情况下最好的型号）
    ）

vc_hard_equals.fit（x_train，y_train）
vc_hard_forest_priority.fit（x_train，y_train）

print（vc_hard_equals.score（x_test，y_test））＃0.832
print（vc_hard_forest_priority.score（x_test，y_test））＃0.915
 ]]></description>
      <guid>https://stackoverflow.com/questions/79474361/why-votingclassifer-performance-with-voting-set-to-hard-is-different-with-diff</guid>
      <pubDate>Fri, 28 Feb 2025 02:14:28 GMT</pubDate>
    </item>
    <item>
      <title>在Palantir Foundry模型培训参数（平均，SD）中评估过程是否从“火车数据”到“测试数据”？</title>
      <link>https://stackoverflow.com/questions/79469004/do-evaluate-process-in-palantir-foundry-model-training-parameters-mean-sd-fro</link>
      <description><![CDATA[如果我正确理解了该过程，则在机器学习中缩放测试数据时，应使用从培训数据中学到的缩放参数（如平均值和标准偏差）来转换测试数据，而不是测试数据本身。。
所以正确的步骤是：

将数据分开：将数据集分为培训和测试集。
缩放训练数据：计算和应用缩放参数（例如平均值，标准偏差）到训练数据。
将相同的参数应用于测试数据

要实现上述步骤，我使用：

  fit_transform 缩放“培训数据”，
 转换携带“培训数据”参数以“测试数据” 

但是，当我评估“测试数据”时，我如何在Palantir铸造模型中实现这一目标，我看不到评估配置的选项。有谁知道Palantir是否在评估配置中构建功能？携带参数过程会自动发生吗？如果没有，我该怎么做才能实现？
  fit_transform 然后变换在Palantir Foundry模型培训中等效]]></description>
      <guid>https://stackoverflow.com/questions/79469004/do-evaluate-process-in-palantir-foundry-model-training-parameters-mean-sd-fro</guid>
      <pubDate>Wed, 26 Feb 2025 08:32:02 GMT</pubDate>
    </item>
    <item>
      <title>如何在本地运行DeepSeek模型</title>
      <link>https://stackoverflow.com/questions/79468013/how-to-run-deepseek-model-locally</link>
      <description><![CDATA[我试图根据他们的说明在本地运行DeepSeek，但它不能带来一些愚蠢的错误（我将稍后显示）。
这就是我正在做的。

从此处下载最小型号（3.5GB） noreferrer“&gt; https://huggingface.co/deepseek-ai/deepseek-r1-distill-qwen-1.5b  
按照此处的步骤操作： https://github.com/deepseek-ai/deepseek-v3?tab=readMe-Readme-ov-file#6-how-to-to-to-run-locally  

 2.1获取这个项目
 https://github.com/deepseek-ai/deepseek-ai/deepseek-ai/deepseek-v3.git 
 2.2运行码头容器类似于预先创建的卷以放置模型
  docker run  -  gpus all -it -it -name deepSeek01 -rm -mount source = deepSeekv3，target =/root/deepSeekv3 python：3.10 -Slim bash
 
我正在使用python：3.10-slim，因为这里（ https://github.com/deepseek-ai/deepseek-v3?tab=readmereadme-readme-ov-file#6-how-how-to-run-locally ）
＆quot&#39; linux只有python 3.10。 Mac和Windows不支持。
 2.3安装最新更新
apt-get Update 
 2.4获取此文件 https://github.com/deepseek-ai/deepseek-v3/blob/main/main/inference/requirements.txt 并安装要求
  pip install -r sumpliont.txt
 
 2.5将模型复制到安装在Docker容器上的音量。这5个文件来自此处 https：//hugging.co/deepseek-aiek-ai/deepseek-ai/deepseek-ai/deepseek-ai/deepseek/deepseek-ipseek-r1-r1-r1-r1-pp&gt;   config.json
generation_config.json
模型。系统
tokenizer.json
tokenizer_config.json
 
 2.6在此处编写的模型转换 https://github.com/deepseek-ai/deepseek-v3?tab=readme-readme-ov-file#model-weights-conversion 通过此命令
  python convert.py-hf-ckpt-path/root/deepSeekv3/source_model -save-path/root/deepSeekv3/converted_model -n-experts 256-model-parelally 16
 
在此步骤中（转换模型）我得到了此错误
  trackback（最近的最新通话）：
  file＆quort＆quort＆quot deepseekv3/inference/convert.py&quot;，第96行，in＆lt; module＆gt;
    main（args.hf_ckpt_path，args.save_path，args.n_experts，args.model_parallel）
  file＆quot＆quot&#39;deepseekv3/inference/convert.py&quot;，第63行，在main中
    主张映射中的密钥
断言
 
因此，基本上，下一步没有意义，因为这是必不可少的步骤。
我的问题：

我做错了什么？
 YouTube上有一些视频，其中DeepSeek与Ollama一起安装了。真的需要吗？我是否应该像他们在这里描述的那样能够运行它， https://github.com/deepseek-ai/deepseek-v3?tab=readmereadme-readme-ov-file#6-how-to-run-locally ？

更新1 
为了调试一点，我添加了这2行。
  print（＆quot;丢失键：＆quot;键）
打印（可用键：＆quot; list（mapping.keys（）））
 
缺少键是以下内容：
  embed_tokens
input_layernorm
down_proj
gate_proj
UP_PROJ
post_attention_layernorm
k_proj
 
虽然所有这些都确实存在于模型中。
另外，@hans Kilian在评论中提到，我可能会放一些文件，而这些文件不需要到source_model文件夹中。
我在convert.py中检查了第11行，其中一些键在模型中不存在。]]></description>
      <guid>https://stackoverflow.com/questions/79468013/how-to-run-deepseek-model-locally</guid>
      <pubDate>Tue, 25 Feb 2025 22:14:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么拥抱面提供的DeepSeek代码会导致“未知量化类型”错误？</title>
      <link>https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t</link>
      <description><![CDATA[我正在使用huggingface的此代码：
此代码直接从 deepseek上的huggingface网站页面上的页面

 来自变形金刚导入管道

消息= [
{&#39;&#39;：＆quot“ user quot”内容“：;
这是给出的
pipe =管道（＆quot&#39;text-generation＆quot; deepseek-ai/deepseek-r1＆quort; trust_remote_code = true）
管道（消息）
 

，但我无法加载模型。当我这样做时，我会得到这个问题：
  file＆quot＆lt; ...＆gt;/site-packages/transformers/quantizers/auto.py＆quot;，第97行，在from_dict

提高价值Error（

ValueError：未知量化类型，获得FP8-支持类型为： 
[&#39;awq&#39;，&#39;bitsandbytes_4bit&#39;，&#39;bitsandbytes_8bit&#39;，&#39;gptq&#39;，&#39;aqlm&#39;，&#39;quanto&#39;，&#39;eetq&#39;，&#39;eetq&#39;， 
&#39;HQQ&#39;，“压缩张量”，“ fbgemm_fp8&#39;，&#39;torchao&#39;，&#39;bitnet&#39;]
 
我尝试了不同的代码：
 导入火炬
generate_text = pipeline（model =; deepSeek-ai/deepSeek-r1; torch_dtype = torch.bfloat16，trust_remote_code = true，device_map =; auto;
generate_text（消息）
 
这给出以下错误：

raise ValueError( ValueError: Unknown quantization type, got fp8 - supported types are: [&#39;awq&#39;, &#39;bitsandbytes_4bit&#39;, &#39;bitsandbytes_8bit&#39;, &#39;gptq&#39;, &#39;aqlm&#39;, &#39;quanto&#39;, &#39;eetq&#39;, &#39;higgs&#39;, &#39;hqq&#39;, &#39;compressed-tensors&#39;, &#39;fbgemm_fp8&#39;, &#39;torchao&#39;，&#39;bitnet&#39;，&#39;vptq&#39;] 

我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t</guid>
      <pubDate>Sun, 09 Feb 2025 03:05:30 GMT</pubDate>
    </item>
    <item>
      <title>在线差异更新批处理数据 /颜色频道的有效算法更新</title>
      <link>https://stackoverflow.com/questions/75545944/efficient-algorithm-for-online-variance-update-over-batched-data-color-channel</link>
      <description><![CDATA[我有大量的多维数据（图像），并希望计算所有轴（颜色通道）的方差。内存明智，我无法创建一个大数组来计算一个步骤的方差。因此，我需要分批加载数据并以在线方式以某种方式更新当前差异。
 玩具示例 
最后，批处理明智的更新在线应匹配 recript_var 。
但是，我很难为此找到有效的算法。
 导入numpy作为np
np.random.seed（0）
＃正确计算方差
all_data = np.random.randint（0，9，（9，3））＃＆lt;  - 不适合记忆
recripe_var = all_data.var（axis = 0）
＃创建批次
batches = all_data.Reshape（-1，3，3）

在线_var = 0
批处理批量：
   batch_var = batch.var（轴= 0）
   在线_var =？  ＃如何正确更新此
surstert np.allclose（recript_var，online_var）
 

我找到了

如何以有效考虑整个批次的有效方式更新多个新观察的方差？]]></description>
      <guid>https://stackoverflow.com/questions/75545944/efficient-algorithm-for-online-variance-update-over-batched-data-color-channel</guid>
      <pubDate>Thu, 23 Feb 2023 14:10:26 GMT</pubDate>
    </item>
    <item>
      <title>机器学习算法选择</title>
      <link>https://stackoverflow.com/questions/15292547/machine-learning-algorithm-selection</link>
      <description><![CDATA[我是机器学习的新手。我的问题是制作一台机器，根据学生的位置和感兴趣的领域为学生选择大学。即，应该在同一城市中选择与学生地址相同的大学。我对选择算法感到困惑，我可以将perceptron算法用于此任务。 ]]></description>
      <guid>https://stackoverflow.com/questions/15292547/machine-learning-algorithm-selection</guid>
      <pubDate>Fri, 08 Mar 2013 11:07:51 GMT</pubDate>
    </item>
    </channel>
</rss>