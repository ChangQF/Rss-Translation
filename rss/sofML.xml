<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 29 Mar 2024 15:14:13 GMT</lastBuildDate>
    <item>
      <title>如何在 python 中模拟 Microsoft Excel 的求解器功能（GRG 非线性）？</title>
      <link>https://stackoverflow.com/questions/78244486/how-can-i-emulate-microsoft-excels-solver-functionality-grg-nonlinear-in-pyth</link>
      <description><![CDATA[查看演示 Excel 求解器使用的屏幕截图
(https://i.stack.imgur.com/eK3C7.png)
我的任务是自动化某个 Excel 工作表。该工作表恰好使用名为 Solver 的 Excel 插件实现了逻辑。它使用单元格 $O$9 中的单个值 (-1.95624)（这是图中用红色和蓝色墨水突出显示的计算结果）作为输入值，然后使用名为的算法返回 C、B1 和 B2 的三个值“GRG非线性回归”。我的任务是用 Python 模拟这个逻辑。以下是我的尝试。主要问题是我没有得到与 Excel 的 Solver 插件计算出的 C、B1 和 B2 相同的值。
导入 numpy、scipy、matplotlib
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 plt
从 scipy.optimize 导入 curve_fit
从 scipy.optimize 导入 Differential_evolution
进口警告

xData = numpy.array([-2.59772914040242,-2.28665528866907,-2.29176070881848,-2.31163972446061,-2.28369414349715,-2.27911303233721,-2.282 22332344644,-2.39089535619106,-2.32144325648778,-2.17235002006179,-2.22906032068685,-2.42044014499938,-2.71639505549322,-2.65 462061336346,- 2.47330475191616,-2.33132910807216,-2.33025978869114,-2.61175064230516,-2.92916553244925,-2.987503044973,-3.00367414706232, -1.45507812104723]) # 使用与参数相同的表名
yData = numpy.array([0.0692847120775066,0.0922342111029099,0.0918076382491768,0.0901635409944003,0.0924824386284127,0.092867647175396, 0.092605957740688,20.0838696111204451,0.0893625419994501,0.102261091024881,0.097171046758256,70.0816272542472914,0.0620128251 290935,0.0657047909578125,0.0777509345715382,0.088561321341585,0.088647672874835,90.0683859871424735,0.0507304952495273,0.047 9936476914665,0.0472601632188253,0.18922126828463 ]) # 使用与参数相同的表名

def func(x, a, b, Offset): # 带偏移量的 Sigmoid A 来自 zunzun.com
    返回 1.0 / (1.0 + numpy.exp(-a * (x-b))) + 偏移量


# 遗传算法最小化（误差平方和）的函数
def sumOfSquaredError(parameterTuple):
    warnings.filterwarnings(“ignore”) # 不通过遗传算法打印警告
    val = func(xData, *parameterTuple)
    返回 numpy.sum((yData - val) ** 2.0)


defgenerate_Initial_Parameters():
    # 用于边界的最小值和最大值
    maxX = max(x数据)
    minX = min(x数据)
    maxY = max(y数据)
    minY = min(yData)

    参数范围 = []
    parameterBounds.append([minX, maxX]) # 的搜索范围
    parameterBounds.append([minX, maxX]) # b 的搜索范围
    parameterBounds.append([0.0, maxY]) # Offset 的搜索范围

    #“种子”用于可重复结果的 numpy 随机数生成器
    结果 = Differential_evolution(sumOfSquaredError,parameterBounds,seed=3)
    返回结果.x

# 生成初始参数值
遗传参数=generate_Initial_Parameters()

# 曲线拟合测试数据
参数，协方差 = curve_fit（func，xData，yData，遗传参数，maxfev = 50000）

# 将参数转换为Python内置类型
params = [float(param) for param in params] # 将 numpy float64 转换为 Python float
C、B1、B2 = 参数
OutputDataSet = pd.DataFrame({“C”：[C]，“B1”：[B1]，“B2”：[B2]，“ProType”：[input_value_1]，“RegType”：[input_value_2 ]})


有什么想法会有帮助吗？提前致谢
这是我的尝试：
鉴于 xData 和 yData 的这些数据集，正确的输出应该是：
C= -2.35443383，B1 = -14.70820051，B2 = 0.0056217]]></description>
      <guid>https://stackoverflow.com/questions/78244486/how-can-i-emulate-microsoft-excels-solver-functionality-grg-nonlinear-in-pyth</guid>
      <pubDate>Fri, 29 Mar 2024 15:08:18 GMT</pubDate>
    </item>
    <item>
      <title>哪种 ARIMA 模型最适合此数据？</title>
      <link>https://stackoverflow.com/questions/78244469/what-kind-of-arima-model-would-be-best-fit-for-this-data</link>
      <description><![CDATA[我正在尝试用Python学习时间序列预测和预测。我绘制了总电子含量的 ACF 和 PACF，它们具有季节性，即 TEC 值在白天达到最大值，在夜间达到最小值。总体数据没有上升或下降趋势，Adfuller检验的检验统计值为-3.67
我得到了以下图表，其中 ACF 逐渐减少，PACF 也逐渐减少，现在我很困惑哪个是 ARIMA 模型的最佳系数。
Timeseries TEC 的 ACF 和 PACF 图
注意：我想预测震后10天和震前20天，然后与实际值进行比较，得到差异值并显示地震对总电子含量的影响。
TEC 值也会受到地磁风暴的影响，因此接下来我将训练一个机器学习模型来对地震的影响和空间天气的影响进行分类。
如果有人想查看我的数据，我可以分享它。
谢谢！
我正在尝试拟合 ARIMA 模型来预测地震前 20 天和地震后 10 天的时间序列 TEC 值。我的目标是获得特定时间范围内的预报，然后将其与实际值进行比较，看看由于地震造成的差异有多大。
我对数据选择 AR 和 MA 系数感到惊讶。]]></description>
      <guid>https://stackoverflow.com/questions/78244469/what-kind-of-arima-model-would-be-best-fit-for-this-data</guid>
      <pubDate>Fri, 29 Mar 2024 15:04:50 GMT</pubDate>
    </item>
    <item>
      <title>与训练阶段相比，测试阶段的决定系数值更高</title>
      <link>https://stackoverflow.com/questions/78244042/higher-coefficient-of-determination-values-in-the-testing-phase-compared-to-the</link>
      <description><![CDATA[我使用元启发式算法和人工神经网络开发了七种不同的混合机器学习模型。有趣的是，与训练阶段相比，大多数模型在测试阶段的决定系数值更高。这种差异提出了一个问题：这种现象背后的原因是什么？
如果可能的话，请为您的话提供参考。]]></description>
      <guid>https://stackoverflow.com/questions/78244042/higher-coefficient-of-determination-values-in-the-testing-phase-compared-to-the</guid>
      <pubDate>Fri, 29 Mar 2024 13:34:23 GMT</pubDate>
    </item>
    <item>
      <title>运行排列重要性时，我们是否会排列测试集中的列？</title>
      <link>https://stackoverflow.com/questions/78243995/do-we-permute-columns-in-the-test-set-when-running-permutation-importance</link>
      <description><![CDATA[我一直在查看有关排列重要性的文档和相关教程，但似乎没有人清楚地了解他们实际排列的内容。
为了澄清，分步过程如下：

将数据集拆分为 X_train、X_val 和 X_test

在 X_train 上训练数据，使用 X_val 例如找到最佳纪元

在 X_test 上运行经过训练的模型，记下我们正在测量的指标

排列 X_test 中的特征，并在此排列后的 X_test 数据集上运行相同的模型

记下相同的指标并比较两者

对每个变量重复此操作，而不更改模型。


旁白问题：是否值得重复运行此排列过程，其中 X_train、X_val 和 X_test 随着每次重复而变化。我知道最终的模型会有所不同，但我想广泛了解通用模型（具有固定超参数）在不同数据集上训练时的表现，因为保持 X_test 固定可能会扭曲某些特征的感知重要性。
提前感谢您。]]></description>
      <guid>https://stackoverflow.com/questions/78243995/do-we-permute-columns-in-the-test-set-when-running-permutation-importance</guid>
      <pubDate>Fri, 29 Mar 2024 13:25:13 GMT</pubDate>
    </item>
    <item>
      <title>在没有 model.fit 的情况下通过 Keras 使用权重和偏差</title>
      <link>https://stackoverflow.com/questions/78243749/using-weights-and-biases-with-keras-without-model-fit</link>
      <description><![CDATA[我在实施 WandB 来跟踪我的 RL 训练算法时遇到问题。我正在使用 Keras 编写我的训练代码。但是，我不使用 model.fit 函数。相反，我使用 @tf.function 对 Q 网络和策略网络执行更新（我正在使用 DDPG 学习算法）。如果我不使用 model.fit，如何初始化运行并使用跟踪回调，因此我无法将回调作为参数传递。互联网上关于将 Keras 与 WandB 一起使用的每个指南/教程都假设我正在使用 model.fit，但事实并非如此。你能帮忙吗？
非常感谢您的帮助。
到目前为止我还没有尝试过任何事情，因为我不知道从哪里开始。我在互联网上找不到任何人有类似的代码或问题。]]></description>
      <guid>https://stackoverflow.com/questions/78243749/using-weights-and-biases-with-keras-without-model-fit</guid>
      <pubDate>Fri, 29 Mar 2024 12:28:07 GMT</pubDate>
    </item>
    <item>
      <title>使用上游-下游 ML 模型，上游是 Wav2Vec 2.0 转换器，下游是 CNN。模型的准确性趋于稳定，为什么？</title>
      <link>https://stackoverflow.com/questions/78243585/using-an-upstream-downstream-ml-model-with-the-upstream-being-wav2vec-2-0-trans</link>
      <description><![CDATA[尝试将 wav2vec 2.0 与 CNN 结合使用进行语音情感识别。已定义四个类别。所有音频都已根据 wav2vec 2.0 模型的需要进行了预处理、充分截断/填充和重新采样。这就是模型的定义方式：
class SimpleNN(nn.Module)：
    def __init__(自身，输入大小，隐藏大小，输出大小)：
        超级(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(输入大小，隐藏大小)
        self.fc2 = nn.Linear(隐藏大小, 输出大小)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)

    def 前向（自身，x）：
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        返回x

tokenizer = Wav2Vec2Tokenizer.from_pretrained(“facebook/wav2vec2-base-960h”)
模型 = Wav2Vec2Model.from_pretrained(“facebook/wav2vec2-base-960h”)

# 修改最后一层以匹配输出类的数量
label_mapping = {&#39;OAF_Fear&#39;：0，&#39;OAF_angry&#39;：1，&#39;OAF_happy&#39;：2，&#39;OAF_neutral&#39;：3}
num_classes = len(label_mapping)

model.lm_head = nn.Linear（in_features=model.config.hidden_​​size，out_features=num_classes，bias=True）

对于 model.parameters() 中的参数：
    param.requires_grad = False
对于 model.lm_head.parameters() 中的参数：
    param.requires_grad = True

input_size = 768 # 从预训练模型中提取的特征的大小
隐藏大小 = 256
output_size = num_classes # 情感类别的数量
学习率 = 0.001
纪元数 = 50
批量大小 = 32
root_dir =“/content/drive/MyDrive/BTP_hanan_dataset/Dataset/TESS”

类 FullModel(nn.Module):
    def __init__(self, wav2vec_model, simple_nn_model):
        超级（FullModel，自我）.__init__()
        self.wav2vec_model = wav2vec_model
        self.simple_nn_model = simple_nn_model

    def 前向（自身，x）：
        # 从预训练模型中获取隐藏状态
        隐藏状态 = self.wav2vec_model(x)[0]
        
        # 聚合隐藏状态（例如，通过平均或最大池化）
        aggregate_hidden_​​state = torch.mean(hidden_​​states, dim=1) # 示例：求平均值
        
        # 通过简单的神经网络
        输出= self.simple_nn_model（聚合隐藏状态）
        
        返回输出
simple_nn = SimpleNN(输入大小、隐藏大小、输出大小)

对于 simple_nn.parameters() 中的参数：
    param.requires_grad = True

# 将预训练模型和简单的神经网络组合成一个模型
full_model = FullModel(模型, simple_nn)

# 损失函数和优化器
标准 = nn.CrossEntropyLoss()
优化器 = optim.Adam(full_model.parameters(), lr=learning_rate)


预训练模型的最后一层已经训练完成，其参数被传递给简单的 CNN。该模型的准确率停滞在 35%。
在两个不同的数据集上进行了尝试，但没有任何改进。提前停止在 7 - 10 个 epoch 后触发，耐心 = 5。我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78243585/using-an-upstream-downstream-ml-model-with-the-upstream-being-wav2vec-2-0-trans</guid>
      <pubDate>Fri, 29 Mar 2024 11:48:46 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TensorFlow 中对多个类进行分类</title>
      <link>https://stackoverflow.com/questions/78243492/how-to-classify-multiple-classes-in-tensorflow</link>
      <description><![CDATA[我已经关注了 youtube 上的教程，该教程向我展示了如何对 2 个数据集进行分类（咳嗽，不是咳嗽），但现在我需要添加一个额外的类，即打喷嚏，因此需要训练 3 个类上（咳嗽，打喷嚏，其他），我不知道该怎么做。请帮忙！！！
在代码中，模型在 2 个类别（咳嗽、not_cough）上进行训练并且表现相当不错，但我无法让它在多个类别（例如咳嗽、打喷嚏、其他）上工作。
导入操作系统
从 matplotlib 导入 pyplot 作为 plt
将张量流导入为 tf
将tensorflow_io导入为tfio
从tensorflow.keras.models导入顺序，load_model
从tensorflow.keras.layers导入Conv2D、Dense、Flatten、MaxPool2D、Dropout、TimeDistributed、Reshape
从tensorflow.keras.optimizers.legacy导入Adam
从 keras 导入层
从 keras.utils 导入到_categorical

def load_wav_16k_mono(文件名):
    # 加载编码后的wav文件
    file_contents = tf.io.read_file(文件名)
    # 解码 wav（按通道的张量）
    wav，sample_rate = tf.audio.decode_wav（文件内容，desired_channels = 1）
    # 删除尾随轴
    wav = tf.squeeze(wav, 轴=-1)
    样本率 = tf.cast(样本率，dtype=tf.int64)
    # 从 44100Hz 到 16000Hz - 音频信号的幅度
    wav = tfio.audio.resample(wav,rate_in=sample_rate,rate_out=16000)
    返回波形

def 预处理（文件路径，标签）：
    wav = load_wav_16k_mono(文件路径)
    wav = wav[:8000]
    Zero_padding = tf.zeros([8000] - tf.shape(wav), dtype=tf.float32)
    wav = tf.concat([zero_padding, wav],0)
    
    频谱图 = tf.signal.stft(wav,frame_length=100,frame_step=20)
    频谱图 = tf.abs(频谱图)
    频谱图= tf.expand_dims（频谱图，轴= 2）
    返回频谱图、标签


def get_CNN(input_shape):
    模型=顺序（）
    model.add(Conv2D(16, (3,3), 激活=&#39;relu&#39;, input_shape=input_shape))
    model.add(Conv2D(16, (3,3), 激活=&#39;relu&#39;))
    model.add(MaxPool2D((2,2)))
    模型.add(压平())
    model.add（密集（128，激活=&#39;relu&#39;））
    model.add（密集（1，激活=&#39;softmax&#39;））
    
    model.compile(&#39;Adam&#39;, loss=&#39;BinaryCrossentropy&#39;, 指标=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision(),&#39;accuracy&#39;])
    model.summary() # 删除一些最大池层以减少参数
    返回模型
    

def main():
    POS_COUGH = “./data/咳嗽”
    NEG_COUGH =“./data/not_cough”
  
    #POS_SPEECH =“./数据/语音”

    pos_cough = tf.data.Dataset.list_files(POS_COUGH+&#39;\*.wav&#39;)
    neg_cough = tf.data.Dataset.list_files(NEG_COUGH+&#39;\*.wav&#39;)
    
    #pos_speech = tf.data.Dataset.list_files(POS_SPEECH +&#39;\*.wav&#39;)

    咳嗽标签 = tf.data.Dataset.from_tensor_slices(tf.ones(len(pos_cough)))
    
    not_cough_labels = tf.data.Dataset.from_tensor_slices(tf.ones(len(neg_cough)))
    
    # 添加标签并合并正负样本
    咳嗽 = tf.data.Dataset.zip((pos_cough, 咳嗽_标签))
    
    not_cough = tf.data.Dataset.zip((neg_cough, not_cough_labels))
   
    阴性= not_cough
    阳性=咳嗽
    # 连接两个相同的元素
    数据=正数.连接（负数）

    ### 2. 创建 Tensorflow 数据管道
    数据 = data.map(预处理)
    数据 = data.cache()
    数据 = data.shuffle(buffer_size=1000)
    数据 = 数据.batch(16)
    数据 = 数据.预取(8)
    
    ## 3. 将数据拆分为训练数据和测试数据
    火车 = data.take(int(len(数据) * 0.7))
    test = data.skip(int(len(data) * 0.7)).take(int(len(data) - len(data) * 0.7)) #test.as_numpy_iterator().next()

    输入形状频谱图 = (396, 65,1)
    模型 = get_CNN(input_shape_spectrogram)
    hist = model.fit(train, epochs=2,validation_data=test)
]]></description>
      <guid>https://stackoverflow.com/questions/78243492/how-to-classify-multiple-classes-in-tensorflow</guid>
      <pubDate>Fri, 29 Mar 2024 11:24:58 GMT</pubDate>
    </item>
    <item>
      <title>训练和测试分开，目标类别的每个名称和比例都出现在训练和测试中</title>
      <link>https://stackoverflow.com/questions/78242480/train-and-test-split-in-such-a-way-that-each-name-and-proportion-of-tartget-clas</link>
      <description><![CDATA[我正在尝试解决一个机器学习问题，如果一个人是否会交付订单。高度不平衡的数据集。这是我的数据集的一瞥
[{&#39;order_id&#39;: &#39;1bjhtj&#39;, &#39;送货员&#39;: &#39;约翰&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1aec&#39;, &#39;送货员&#39;: &#39;约翰&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1cgfd&#39;, &#39;送货员&#39;: &#39;约翰&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bceg&#39;, &#39;送货员&#39;: &#39;汤姆&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a2fg&#39;, &#39;送货员&#39;: &#39;汤姆&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1cbsf&#39;, &#39;送货员&#39;: &#39;汤姆&#39;, &#39;目标&#39;: 1},
 {&#39;order_id&#39;: &#39;1bc5&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a22&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bzc5&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1av22&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bsc5&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 1},
 {&#39;order_id&#39;: &#39;1a2t2&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bc5b&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a22a&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1c5bv&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;vb2er&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bs5s&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a22n&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;122a&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 1},
 {&#39;order_id&#39;: &#39;1cw5bv&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;vb=er&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1b5s&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a2n&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 1}]



这是我的桌子：
&lt;前&gt;&lt;代码&gt;|订单 ID |送货员 |目标|
|----------|--------------|--------|
| 1bjhtj |约翰 | 0 |
| 1aec |约翰 | 0 |
| 1cgfd |约翰 | 0 |
| 1bceg |汤姆| 0 |
| 1a2fg | 1a2fg汤姆| 0 |
| 1cbsf |汤姆| 1 |
| 1BC5 | 1BC5 |周杰伦 | 0 |
| 1a22 | 1a22周杰伦 | 0 |
| 1bzc5 | 1bzc5周杰伦 | 0 |
| 1av22 | 1av22周杰伦 | 0 |
| 1BSC5 |周杰伦 | 1 |
| 1a2t2 | 1a2t2 |周杰伦 | 0 |
| 1bc5b | 1bc5b |周杰伦 | 0 |
| 1a22a | 1a22a玛丽| 0 |
| 1c5bv | 1c5bv玛丽| 0 |
| VB2er |玛丽| 0 |
| 1bs5s | 1bs5s |玛丽| 0 |
| 1a22n | 1a22n |玛丽| 0 |
| 122a | 122a詹姆斯 | 1 |
| 1cw5bv | 1cw5bv詹姆斯 | 0 |
| vb=呃|詹姆斯 | 0 |
| 1b5s | 1b5s |詹姆斯 | 0 |
| 1a2n |詹姆斯 | 1 |


我希望我的机器学习模型能够理解每个人的属性并预测这两个
案例：
将传送“0”和
不会传送“1”
我想以这样的方式分割我的训练和测试，使其保留几行名称和几行目标类，以便它学习所有模式。
到目前为止我已经用过这个
X = df.drop(columns = “目标”)
y = df.目标
X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.7,stratify=y)

它确实给了我每个送货员的输出，但它错过了我们可以以“1”和“1”的方式分割“James”的部分。火车上将会有另一个“1”将在测试中。
谁能帮助我以不同的方式解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/78242480/train-and-test-split-in-such-a-way-that-each-name-and-proportion-of-tartget-clas</guid>
      <pubDate>Fri, 29 Mar 2024 07:21:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在 FastAPI 中处理 ML 模型 API 的多个并行请求</title>
      <link>https://stackoverflow.com/questions/78242301/how-to-handle-multiple-parallel-requests-in-fastapi-for-an-ml-model-api</link>
      <description><![CDATA[我使用 FastAPI 构建了一个 ML 模型 API，该 API 主要需要使用 GPU。我想让这个 API 至少服务一定数量的并行请求。为了实现这一点，我尝试将所有函数设置为 def 而不是 async def，以便它可以同时处理请求（如前所述 此处 ，此处和此处）。目前，我遇到的情况是，如果发出一个请求，需要 3 秒才能获得输出，如果发出 3 个并行请求，则所有用户都将在 9 秒内获得输出。在这里，所有用户都同时获得输出，但正如您所看到的，时间随着请求数量的增加而增加。然而，我真正想要的是所有用户都能在 3 秒内获得输出。
我尝试过一些方法，例如 ThreadPoolExecutor (此处)，ProcessPoolExecutor (此处），Asyncio (此处)，run_in_threadpool (这里），但这些方法都不适合我。
这是我的 api 代码使用简单 def 的样子：
from fastapi import Depends、FastAPI、文件、UploadFile、响应
进口uvicorn

类 Model_loading():
    def __init__():
        self.model = torch.load(&#39;model.pth&#39;)


应用程序 = Fastapi()
model_instance = Model_loading()

def gpu_based_processing(x):

   ---- 进行一些基于 GPU 的计算 ----

   返回结果


@app.post(&#39;/模型测试&#39;)
def my_function(文件: UploadFile = File(...)):
    
   ---- 进行一些初始预处理 ----

   输出 = gpu_based_processing(x)

   返回响应（内容=输出，media_type=“图像/jpg”）


此外，我还观察到向上述 API 发出 20 个并行请求会导致 CUDA 内存不足错误的行为。即使只有 20 个请求，它也无法处理它们。如何解决 CUDA 内存问题并管理同时处理多个并行请求？]]></description>
      <guid>https://stackoverflow.com/questions/78242301/how-to-handle-multiple-parallel-requests-in-fastapi-for-an-ml-model-api</guid>
      <pubDate>Fri, 29 Mar 2024 06:26:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 Geotiff 图像分割</title>
      <link>https://stackoverflow.com/questions/78238294/segmentation-with-geotiff-image</link>
      <description><![CDATA[我想用 python 进行 k-means 分割。我的代码适用于 jpg 图像，但是当我尝试使用 geotiff 时，它只会使图像变成黑白。我怎么解决这个问题？下面是我的代码；
导入 matplotlib 作为 mpl
将 matplotlib.pyplot 导入为 plt
将 matplotlib.image 导入为 mpimg
从 sklearn.cluster 导入 KMeans

从 PIL 导入图像
Image.MAX_IMAGE_PIXELS = 无

从 google.colab 导入驱动器
驱动器.mount（&#39;/内容/驱动器&#39;）

# Dosya yolu，Google Drive&#39;ınızda dosyanın bulunduğu yol

file_path = &#39;/content/drive/MyDrive/Colab Notebooks/ortofoto.tif&#39;

# TIFF dosyasını oku
图像 = mpimg.imread(文件路径)
[文本]([https://stackoverflow.com](https://stackoverflow.com))

# 多斯亚伊·戈斯特
plt.imshow(图像)
plt.show()

X=图像.reshape(-1, 4)
kmeans=KMeans(n_clusters=2, n_init=10).fit(X)

segmented_img=kmeans.cluster_centers_[kmeans.labels_]
splited_img=segmented_img.reshape(image.shape)
plt.imshow(segmented_img/255)
]]></description>
      <guid>https://stackoverflow.com/questions/78238294/segmentation-with-geotiff-image</guid>
      <pubDate>Thu, 28 Mar 2024 12:48:44 GMT</pubDate>
    </item>
    <item>
      <title>Bert Istantiation TypeError：“NoneType”对象不可调用 Tensorflow</title>
      <link>https://stackoverflow.com/questions/78227490/bert-istantiation-typeerror-nonetype-object-is-not-callable-tensorflow</link>
      <description><![CDATA[我正在尝试使用 TensorFlow 实例化 BERT 模型。这段代码直到几天前都运行正常，但现在我收到了这个错误。我已经使用相同的代码在其他笔记本中的 Kaggle 上实例化 BERT 模型，并且运行良好。谁能给我提示吗？我应该注意，我正在使用 Transformers 版本 4.31.0 来修复另一个错误。我使用的代码是：
# Creazione del modello BERT per BYTECODE
BC_input_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“input_ids”)
BC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
BC_pooler_output = BC_bert_model(BC_input_layer)[1]

BC_dropout_layer = Dropout(dropout_rate)(BC_pooler_output)
BC_output_layer = 密集（6，激活=&#39;sigmoid&#39;）（BC_dropout_layer）

BC_model = 模型（输入=BC_input_layer，输出=BC_output_layer）

错误是
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
TypeError Traceback（最近一次调用最后一次）
[10] 中的单元格，第 19 行
     17 SC_input_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“input_ids”)
     18 SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
---&gt; 19 SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
     20 SC_pooler_output = SC_bert_model(SC_input_layer, Attention_mask=SC_mask_layer)[1] # 第二个输出，che è il pooler_output
     22 # 辍学层的Aggiungi

文件 /opt/conda/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:2894，在 TFPreTrainedModel.from_pretrained(cls、pretrained_model_name_or_path、config、cache_dir、ignore_mismatched_sizes、force_download、local_files_only、token、revision、*model_args、 **夸格斯）
   第2892章
   第2893章
-&gt;第2894章
   第2896章
   第2897章

文件 /opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:224，位于 Layer.__new__..build_wrapper(*args, **kwargs)
    第221章
    222 def build_wrapper（*args，**kwargs）：
    223 with backend.name_scope(obj.name, caller=obj)：
--&gt;第224章
    225 # 记录构建配置。
    [第 226 章]

文件 /opt/conda/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1131，在 TFPreTrainedModel.build(self, input_shape) 中
   第1129章
   第1130章
-&gt;攀上漂亮女局长之后1131
   第1132章
   第1133章

类型错误：“NoneType”对象不可调用
]]></description>
      <guid>https://stackoverflow.com/questions/78227490/bert-istantiation-typeerror-nonetype-object-is-not-callable-tensorflow</guid>
      <pubDate>Tue, 26 Mar 2024 18:29:11 GMT</pubDate>
    </item>
    <item>
      <title>用于多标签分类的堆叠集成学习</title>
      <link>https://stackoverflow.com/questions/78214688/stacking-ensamble-learning-for-multilabelclassification</link>
      <description><![CDATA[我有两个 BERT 模型来实现代码中漏洞检测的多标签分类。一名接受过源代码培训，另一名接受过编译代码培训。他们实现的任务是多标签分类，因此两个模型的单个输出都是一个包含 6 个元素的数组，每个元素可以是 0 或 1，指示漏洞是否存在。
我想在这两个模型之上构建一个经典的 ML 分类器（如随机森林、SVM 或逻辑回归等），实现称为 Stacking 的集成技术。知道我正在处理多标签分类，我该如何实现这一点？
我的主要疑问是如何堆叠输出（这将成为最终分类器的输入），并且它应该是模型将接收的唯一输入，或者如果我必须输入其他内容。]]></description>
      <guid>https://stackoverflow.com/questions/78214688/stacking-ensamble-learning-for-multilabelclassification</guid>
      <pubDate>Sun, 24 Mar 2024 13:28:23 GMT</pubDate>
    </item>
    <item>
      <title>OCR 结果不一致：训练和测试期间的预测不同</title>
      <link>https://stackoverflow.com/questions/78202518/inconsistent-ocr-results-different-predictions-during-training-and-testing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78202518/inconsistent-ocr-results-different-predictions-during-training-and-testing</guid>
      <pubDate>Thu, 21 Mar 2024 20:02:00 GMT</pubDate>
    </item>
    <item>
      <title>TF2 和 python 中的 BERT 预处理器模型存在问题</title>
      <link>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</link>
      <description><![CDATA[我正在尝试使用 BERT 来做一个文本分类项目。但是我一直遇到这个错误
`
ValueError Traceback（最近一次调用最后一次）
单元格 In[37]，第 4 行
      2 text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
      3 bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
----&gt; 4 preprocessed_text = bert_preprocess(text_input)
      5 bert_encoder = hub.KerasLayer(encoder_url,
      6 可训练=真，
      7 名称=&#39;BERT_编码器&#39;)
      8 个输出 = bert_encoder(preprocessed_text)
ValueError：调用层“预处理”时遇到异常（类型 KerasLayer）。
KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。

调用层“预处理”接收的参数（类型 KerasLayer）：
  输入=
  • 培训=无

KerasTensor 是象征性的：它是形状和数据类型的占位符。它没有任何实际的数值。您无法将其转换为 NumPy 数组。



构建此模型时：
&lt;前&gt;&lt;代码&gt;
preprocess_url = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3&#39;
编码器网址 = &#39;https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/bert-en-uncased-l-12-h-768-a-12/versions/2&#39;

# Bert 层
text_input = tf.keras.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
bert_preprocess = hub.KerasLayer(preprocess_url, name=&#39;预处理&#39;)
预处理文本 = bert_preprocess(text_input)
bert_encoder = hub.KerasLayer(encoder_url,
                              可训练=真，
                              名称=&#39;BERT_编码器&#39;)
输出= bert_encoder（预处理文本）

# 神经网络层
l = tf.keras.layers.Dropout(0.1)(输出[&#39;pooled_output&#39;])
l = tf.keras.layers.Dense(num_classes, 激活=&#39;softmax&#39;, name=&#39;输出&#39;)(l)

# 构建最终模型
模型 = tf.keras.Model(输入=[text_input], 输出=[l])

我看过无数的教程，甚至使用了张量流文档上的教程，即使我复制和粘贴，它们仍然不起作用。我尝试过不同版本的 tf、tf-text 和 tf-hub。我在这个项目中使用了tensorflow-gpu-jupyter docker 容器。
这是我安装库的方法：
!pip install “tensorflow-text”
!pip install “tf-models-official”
!pip install “tensorflow-hub”

版本是：
张量流：2.16.1
张量流文本：2.16.1
张量流中心：0.16.1
我看到的有关此问题的所有其他论坛都说要执行 tf.config.run_functions_eagerly(True) 但这不起作用。
任何事情都会有所帮助。如果您知道如何解决请回答。]]></description>
      <guid>https://stackoverflow.com/questions/78183834/issue-with-bert-preprocessor-model-in-tf2-and-python</guid>
      <pubDate>Tue, 19 Mar 2024 01:42:01 GMT</pubDate>
    </item>
    <item>
      <title>将 Detectron2 模型转换为 torchscript</title>
      <link>https://stackoverflow.com/questions/73619217/convert-detectron2-model-to-torchscript</link>
      <description><![CDATA[我想将 detectorron2 &#39;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml 模型&#39; 转换为 torchscript。
我用过托克
我的代码如下。
&lt;前&gt;&lt;代码&gt;导入cv2

将 numpy 导入为 np

进口火炬
从 detector2 导入 model_zoo
从 detector2.config 导入 get_cfg
从 detectorron2.engine 导入 DefaultPredictor
从 detector2.modeling 导入 build_model
从 detectorron2.export.flatten 导入 TracingAdapter
导入操作系统

ModelPath=&#39;/home/jayasanka/working_files/create_torchsript/model.pt&#39;
将 open(&#39;savepic.npy&#39;, &#39;rb&#39;) 作为 f：
    图像 = np.load(f)

#------------------------------------------------- ------------------------------------------------

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(“COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml”))

cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 # 你的类数 + 1

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, ModelPath)

cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.60 # 设置该模型的测试阈值

预测器 = DefaultPredictor(cfg)



我使用了 TracingAdapter 和跟踪函数。我不太了解其背后的概念是什么。
&lt;前&gt;&lt;代码&gt;# im = cv2.imread(图像)
im = torch.tensor(图像)

def inference_func（模型，图像）：
    输入= [{“图像”：图像}]
    返回 model.inference(inputs, do_postprocess=False)[0]

包装器= TracingAdapter（预测器，im，inference_func）
包装器.eval()
Traced_script_module= torch.jit.trace（包装器，（im，））
traced_script_module.save(“torchscript.pt”)

它给出了下面给出的错误。
回溯（最近一次调用最后一次）：
  文件“script.py”，第 49 行，位于  中。
    Traced_script_module= torch.jit.trace（包装器，（im，））
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/jit/_trace.py”，第 744 行，跟踪中
    _模块_类，
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/jit/_trace.py”，第 959 行，在trace_module 中
    参数名称，
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/nn/modules/module.py”，第 1051 行，在 _call_impl 中
    返回forward_call（*输入，**kwargs）
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/nn/modules/module.py”，第 1039 行，位于 _slow_forward
    结果 = self.forward(*输入, **kwargs)
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/detectron2/export/flatten.py”，第 294 行，向前
    输出 = self.inference_func(self.model, *inputs_orig_format)
  文件“script.py”，第 44 行，inference_func
    返回 model.inference(inputs, do_postprocess=False)[0]
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/yacs/config.py”，第 141 行，在 __getattr__ 中
    引发属性错误（名称）
属性错误：推理


你能帮我解决这个问题吗？
还有其他方法可以轻松做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/73619217/convert-detectron2-model-to-torchscript</guid>
      <pubDate>Tue, 06 Sep 2022 08:50:15 GMT</pubDate>
    </item>
    </channel>
</rss>