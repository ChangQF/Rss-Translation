<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 07 Nov 2024 06:23:38 GMT</lastBuildDate>
    <item>
      <title>如何在 PyTorch 中为 Nvidia GeForce RTX 3050 Ti 启用 CUDA？</title>
      <link>https://stackoverflow.com/questions/79165030/how-can-i-enable-cuda-in-pytorch-for-nvidia-geforce-rtx-3050-ti</link>
      <description><![CDATA[我需要帮助，如果您能帮助我将不胜感激。我想在我的显卡（Nvidia GeForce RTX 3050 Ti）上运行 PyTorch 库（我在 PyCharm 的虚拟环境中运行该库）。但是，它在 CPU 上运行，每当我使用命令“import torch”和“print(&quot;cuda is available:&quot;, torch.cuda.is_available())”时，它总是返回 False。
在我的系统上，安装了 CUDA 版本 12.6。我还安装了 PyTorch for CUDA 版本 12.4，因为它是 PyTorch 网站上可用的最新版本。请帮我根据我的显卡类型安装什么。]]></description>
      <guid>https://stackoverflow.com/questions/79165030/how-can-i-enable-cuda-in-pytorch-for-nvidia-geforce-rtx-3050-ti</guid>
      <pubDate>Thu, 07 Nov 2024 04:30:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 Flask 进行机器学习项目时出现“TemplateNotFound：index.html”错误</title>
      <link>https://stackoverflow.com/questions/79164308/getting-templatenotfound-index-html-error-while-doing-a-machine-learning-proj</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79164308/getting-templatenotfound-index-html-error-while-doing-a-machine-learning-proj</guid>
      <pubDate>Wed, 06 Nov 2024 21:07:11 GMT</pubDate>
    </item>
    <item>
      <title>如何根据workflow_map结果拟合和预测每个模型？</title>
      <link>https://stackoverflow.com/questions/79164281/how-can-i-fit-and-predict-each-model-from-workflow-map-results</link>
      <description><![CDATA[我正在使用 tidymodels 训练各种分类模型，并使用 parsnip 调整这些模型中的超参数。作为 tidymodels 的新手，我按照教程完成了一些任务。但是，我不确定如何使用 workflow_map 中调整后的超参数来拟合和预测每个模型，而不是仅提取最佳工作流并进行拟合。我感兴趣的是了解每个模型的拟合和预测结果，并识别任何未预测的情况。我目前无法弄清楚如何使用复合函数（如 workflow_map）来拟合和预测具有一组超参数的一系列工作流。我目前的方法是手动替换每一个，这可能有点低效。这是我的部分代码（实际上，我用了 16 个模型，这里我粘贴了其中 2 个）：
set.seed(123)
splits &lt;- initial_split(Mydata, strata = Y,prop = 0.70)
TrainData &lt;- training(splits)
TestData &lt;- testing(splits)

base_rec &lt;- recipe( Y ~ ., data = TrainData)
dummy_rec&lt;-recipe( Y ~ ., data = TrainData)%&gt;%
step_dummy(all_nominal_predictors())

library(ranger)
rf_spec &lt;- 
rand_forest(
mtry = tune(),
min_n = tune(),
trees = tune()
) %&gt;% 
set_engine(&quot;ranger&quot;) %&gt;% 
set_mode(&quot;classification&quot;)%&gt;% 
翻译()

bt_spec &lt;- 
bart(
trees = tune(),
Prior_terminal_node_coef = tune(),
Prior_terminal_node_expo = tune(),
Prior_outcome_range = tune()
) %&gt;% 
set_engine(&quot;dbarts&quot;)%&gt;% 
set_mode(&quot;classification&quot;) %&gt;% 
翻译()

all_workflows &lt;- 
Workflow_set(
preproc = list(base_recipe = base_rec),
models = list(
rf=rf_spec, bart=bt_spec)
)%&gt;%
option_add(control = control_grid(
extract = function(x) x,
parallel_over = &quot;resamples&quot;,
save_pred = TRUE,
save_workflow = TRUE))

#创建重采样对象以供以后调整
set.seed(123)
folds &lt;- vfold_cv(TrainData, v = 5)

wf_tuning &lt;- 
all_workflows %&gt;% 
working_map(&quot;tune_grid&quot;, 
seed=12345, 
resamples = folds, 
grid = 25, 
metrics = metric_set(accuracy), 
verbose = TRUE)
autoplot(wf_tuning)

如果我手动提取调整后的超参数，然后拟合并预测模型，我应该执行相同的程序 25*16 次。您能否提供一些更有效的方法的建议？]]></description>
      <guid>https://stackoverflow.com/questions/79164281/how-can-i-fit-and-predict-each-model-from-workflow-map-results</guid>
      <pubDate>Wed, 06 Nov 2024 21:00:33 GMT</pubDate>
    </item>
    <item>
      <title>如何使用公开可用的数据集来训练 ML 模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79163375/how-do-i-use-publicly-available-datasets-to-train-an-ml-model</link>
      <description><![CDATA[到目前为止，我一直使用 .csv 文件，它们很容易使用。但现在我需要建立一个面部识别模型，其中有一些特定的数据集需要我使用。所有这些数据集都是不同的格式，比如 .tar。有些数据集只包含原始图像。
我该怎么做？
我得到了这个代码作为指南。
这个函数 get_images 旨在从
指定的目录及其子文件夹中加载图像，使用 OpenCV 调整它们的大小，
然后返回调整大小后的图像及其相应的标签。
它遍历给定目录中的每个子文件夹，检查有效的图像文件（扩展名为 jpg、png 或 gif），使用 OpenCV 读取每个图像，将它们调整为指定大小（在本例中为 100x100 像素），并将调整大小后的图像及其标签存储在单独的列表中（X 表示图像，y 表示标签）。最后，它会打印一条消息，表示所有图像都已加载，并返回图像和标签的列表。
import os
import cv2

def get_images(image_directory):
# 初始化列表以存储图像及其标签
X = []
y = []

# 定义图像文件扩展名
extensions = (&#39;jpg&#39;, &#39;png&#39;, &#39;gif&#39;)

# 遍历给定目录中的子文件夹
for subfolder in os.listdir(image_directory):
print(&quot;Loading images in %s&quot; % subfolder)

# 检查该项目是否为目录
if os.path.isdir(os.path.join(image_directory, subfolder)):
# 获取子文件夹中的文件列表
subfolder_files = os.listdir(os.path.join(image_directory, subfolder))

# 遍历文件子文件夹
for file in subfolder_files:
# 检查文件是否具有有效的图像扩展名
if file.endswith(extensions):
# 使用 OpenCV 读取图像
img = cv2.imread(os.path.join(image_directory, subfolder, file))

# 调整图像大小
img = cv2.resize(img, (100, 100))

# 将调整大小后的图像添加到列表 X
X.append(img)

# 将图像的标签添加到列表 y
y.append(subfolder)

print(&quot;所有图像均已加载&quot;)
return X, y
]]></description>
      <guid>https://stackoverflow.com/questions/79163375/how-do-i-use-publicly-available-datasets-to-train-an-ml-model</guid>
      <pubDate>Wed, 06 Nov 2024 16:06:30 GMT</pubDate>
    </item>
    <item>
      <title>YOLOX 不在 mmengine::model 注册表中</title>
      <link>https://stackoverflow.com/questions/79163058/yolox-is-not-in-the-mmenginemodel-registry</link>
      <description><![CDATA[这是我的配置文件：
model = dict(
type=&#39;YOLOX&#39;, # YOLOX 架构
backbone=dict(type=&#39;CSPDarknet&#39;, deep_factor=1.0, widen_factor=1.0), # YOLOX 主干
neck=dict(type=&#39;YOLOXPAFPN&#39;, in_channels=[256, 512, 1024], out_channels=[256, 512, 1024]),
bbox_head=dict(
type=&#39;YOLOXHead&#39;,
num_classes=1, # 根据数据集中的类数更新此文件
in_channels=256,
feat_channels=256
),
train_cfg=dict(assigner=dict(type=&#39;SimOTAAssigner&#39;, center_radius=2.5)),
test_cfg=dict(score_thr=0.01, nms=dict(type=&#39;nms&#39;, iou_threshold=0.65))
)

以下代码列出了 YOLOX：
from mmdet.registry import MODELS

# 打印注册表中所有可用模型
print(MODELS.module_dict.keys())

但是，运行此代码：
from mmengine.config import Config
from mmengine.runner import Runner
from mmdet.utils import register_all_modules

# 注册 MMYOLO 和 MMDetection 的所有模块
register_all_modules()

def train_model(config_file):
# 加载配置
cfg = Config.fromfile(config_file)

# 确保检查点和日志的工作目录
cfg.work_dir = &#39;./checkpoints&#39; 

# 构建运行器
runner = Runner.from_cfg(cfg)

# 开始训练
runner.train()
print(&quot;训练完成！检查点已保存到 &#39;./checkpoints&#39;。&quot;)

# 配置文件路径
config_file = &#39;configs/yolov7/yolox_subset_coco.py&#39;

# 训练模型
train_model(config_file)

产生此错误：
KeyError: &#39;YOLOX 不在 mmengine::model 注册表中。请检查 `YOLOX` 的值是否正确或是否按预期注册。更多详细信息请参阅 https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#import-the-custom-module&#39;

YOLOv7 模型也是如此，事实上，这也是我想要使用的模型。您认为这里的错误是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79163058/yolox-is-not-in-the-mmenginemodel-registry</guid>
      <pubDate>Wed, 06 Nov 2024 14:32:15 GMT</pubDate>
    </item>
    <item>
      <title>AWS Sagemaker ClientError：未指定训练通道（清单文件错误）</title>
      <link>https://stackoverflow.com/questions/79162377/aws-sagemaker-clienterror-train-channel-is-not-specified-manifest-file-error</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79162377/aws-sagemaker-clienterror-train-channel-is-not-specified-manifest-file-error</guid>
      <pubDate>Wed, 06 Nov 2024 11:22:30 GMT</pubDate>
    </item>
    <item>
      <title>如何获取使用字典作为输入的 PyTorch 模型的摘要</title>
      <link>https://stackoverflow.com/questions/79162161/how-to-get-a-summary-of-a-pytorch-model-that-uses-dictionary-as-an-input</link>
      <description><![CDATA[我的模型以字典作为输入，例如x = {&#39;image&#39;: torch.tensor, &#39;number&#39;: torch.tensor}，模型如下所示：
class MyModel(nn.Module):
def __init__(self):
super(MyModel, self).__init__()
self.imgmodule = ImgModule()
self.nummodule = NumModule()
self.predict = nn.Linear(input_size, 100)

def forward(self, x):
xImg = self.imgmodule(x[&#39;image&#39;])
xNum = self.nummodule(x[&#39;number&#39;])
x = self.predict(torch.cat([xImg, xNum], dim=1))
return x 

如何获取模型摘要，类似于 pytorch-summary 包中提供的摘要？
到目前为止，我尝试过像这样使用它这个：
来自 torchsummary 导入 summary
summary(model, input_size=[(3, 224, 224), (1, )])

但我收到错误：
TypeError：MyModel.forward() 需要 2 个位置参数，但给出了 3 个
]]></description>
      <guid>https://stackoverflow.com/questions/79162161/how-to-get-a-summary-of-a-pytorch-model-that-uses-dictionary-as-an-input</guid>
      <pubDate>Wed, 06 Nov 2024 10:22:03 GMT</pubDate>
    </item>
    <item>
      <title>比较两幅相似图像的有效方法</title>
      <link>https://stackoverflow.com/questions/79162036/efficient-way-to-compare-two-similar-images</link>
      <description><![CDATA[我想识别图像中的方框。我有一个这些方框的数据库，存储它们的 ocr 和图像。我使用 ocr 搜索并粗略地转换了脸部。大多数时候它都能正常工作，但有时会返回错误的脸部和错误的转换。由于我有源图像，我想利用它们来评估搜索识别结果。我将检测到的方框区域转换为源图像并将它们调整为相同大小（因此它们从相似的角度看，大小也相似）。我使用 hog、alexnet 的倒数第二层、vitmae 和我自己训练的卷积网络作为嵌入特征。但它们都不太好用。我也尝试了关键点特征。但它花费的时间比要求的要长得多。当区分具有相同字体但不同大小的脸部时，它也会失败。
还有其他有效的方法来比较两张相似的图像吗？]]></description>
      <guid>https://stackoverflow.com/questions/79162036/efficient-way-to-compare-two-similar-images</guid>
      <pubDate>Wed, 06 Nov 2024 09:41:04 GMT</pubDate>
    </item>
    <item>
      <title>对特定层的参数进行自动求导</title>
      <link>https://stackoverflow.com/questions/79161323/autograd-on-a-specific-layer-s-parameters</link>
      <description><![CDATA[我正在尝试获取特定层参数的雅可比矩阵。下面是我的网络模型，我在其上应用了 functional_call。
def fm(params, input):
return functional_call(self.model, params, input.unsqueeze(0)).squeeze(0)

def floss(func_params, input):
fx = fm(func_params, input)
return fx

我过去常常用这种方式计算所有参数的雅可比矩阵
func_params = dict(self.model.named_pa​​rameters())
per_sample_grads = vmap(jacrev(floss, 0), (None, 0))(func_params, input)

现在，我只需要获取特定层参数的梯度，这是我的方法。
def grad(f, param):
return torch.autograd.grad(f, param) 

out = vmap(floss, (None, 0))(func_params,input)
gradf = vmap(grad, (0, None))(out, func_params[&#39;model.0.weight&#39;])

但是，错误提示“张量的元素 0 不需要 grad，也没有 grad_fn”
因为，我已经尝试过了
grad = self.grad(out[0], func_params[&#39;model.0.weight&#39;])

并且成功了。我真的不知道如何解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79161323/autograd-on-a-specific-layer-s-parameters</guid>
      <pubDate>Wed, 06 Nov 2024 04:19:56 GMT</pubDate>
    </item>
    <item>
      <title>我如何知道现有 chromadb 集合正在使用什么嵌入模型？</title>
      <link>https://stackoverflow.com/questions/79161260/how-can-i-know-what-embedding-model-of-a-existing-chromadb-collection-is-using</link>
      <description><![CDATA[我正在研究 chromadb。当我处理一些现有的集合时，我总是遇到错误：

chromadb.errors.InvalidDimensionException：嵌入维度 384 与集合维度 4096 不匹配

我知道这是因为我的嵌入模型与集合创建时选择的嵌入模型不匹配。
所以我想知道如何检查现有集合正在使用什么嵌入模型？]]></description>
      <guid>https://stackoverflow.com/questions/79161260/how-can-i-know-what-embedding-model-of-a-existing-chromadb-collection-is-using</guid>
      <pubDate>Wed, 06 Nov 2024 03:17:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 JAX 训练模型时跟踪测试/验证损失</title>
      <link>https://stackoverflow.com/questions/79158791/tracking-test-val-loss-when-training-a-model-with-jax</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79158791/tracking-test-val-loss-when-training-a-model-with-jax</guid>
      <pubDate>Tue, 05 Nov 2024 10:58:38 GMT</pubDate>
    </item>
    <item>
      <title>Weka RandomForest m_Classifiers 为空且仅在构建 Spring Boot 项目后才出现 NullPointerException 和无法找到允许的类错误</title>
      <link>https://stackoverflow.com/questions/79157997/nullpointerexception-with-weka-randomforest-m-classifiers-is-null-and-cant-find</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79157997/nullpointerexception-with-weka-randomforest-m-classifiers-is-null-and-cant-find</guid>
      <pubDate>Tue, 05 Nov 2024 07:17:06 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用函数 shap.Explainer 会根据输入的不同顺序获得不同的 shap 值？</title>
      <link>https://stackoverflow.com/questions/79152799/why-i-get-different-shap-values-according-to-the-different-order-of-inputs-by-us</link>
      <description><![CDATA[我训练了一个二分类模型，并想使用 shap.Explainer 来分析特征贡献。
代码如下：
def f(x):
return model.predict_proba(x)[:, 1]

X100 = shap.utils.sample(X_train, 100)

explainer = shap.Explainer(f, X100, seed=2023)
shap_values = explainer(data.iloc[[0,1,2,3], :])

shap_values.values 的结果如下：




Feature 1
...




sample 0
-0.009703
...


样本 1
-0.009297
...


样本 2
-0.007699
...


样本 3
0.032624
...



但是当输入顺序改变时：
def f(x):
return model.predict_proba(x)[:, 1]

X100 = shap.utils.sample(X_train, 100)

explainer = shap.Explainer(f, X100, seed=2023)
shap_values = explainer(data.iloc[[1,0,2,3], :])

样本 0 和样本 1 的结果已更改：




特征 1
...




样本 1
-0.010012
...


样本0
-0.008277
...


样本 2
-0.007699
...


样本 3
0.032624
...



我不知道有什么区别。]]></description>
      <guid>https://stackoverflow.com/questions/79152799/why-i-get-different-shap-values-according-to-the-different-order-of-inputs-by-us</guid>
      <pubDate>Sun, 03 Nov 2024 13:22:12 GMT</pubDate>
    </item>
    <item>
      <title>需要从图像中分别分割出每个数字</title>
      <link>https://stackoverflow.com/questions/79147122/need-to-segment-each-number-from-the-image-separately</link>
      <description><![CDATA[我使用 MNIST 数据集创建了一个 CNN 模型。我想对图像中存在的数字序列进行预测。该技术涉及分割每张图像并将其输入到模型中，但我在从图像中分割数字时遇到了困难，因为存在两种不同类型的图像。我需要一种强大的技术来消除图像中存在的所有噪音和阴影并分别分割每个数字。
我也在这里分享这些图片。
我正在寻找强大的技术和代码。





更新问题
分割的主要目标
我正在寻找一种从上面的图像中分割或分离每个数字的方法。我相信一定有一种独特或强大的方法可以用于所有类型的图像（如上所示）。我可以对二值图像和彩色图像应用单独的方法，但我想学习一种适用于上述图像的单一方法。]]></description>
      <guid>https://stackoverflow.com/questions/79147122/need-to-segment-each-number-from-the-image-separately</guid>
      <pubDate>Fri, 01 Nov 2024 06:27:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pytorch 进行随机选择？</title>
      <link>https://stackoverflow.com/questions/59461811/random-choice-with-pytorch</link>
      <description><![CDATA[我有一个图片张量，想从中随机选择。我正在寻找 np.random.choice() 的等价物。
import torch

pictures = torch.randint(0, 256, (1000, 28, 28, 3))

假设我想要 10 张这样的图片。]]></description>
      <guid>https://stackoverflow.com/questions/59461811/random-choice-with-pytorch</guid>
      <pubDate>Mon, 23 Dec 2019 22:14:52 GMT</pubDate>
    </item>
    </channel>
</rss>