<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 07 Oct 2024 15:18:21 GMT</lastBuildDate>
    <item>
      <title>如何微调时间序列转换器超参数以超越 LSTM 性能？</title>
      <link>https://stackoverflow.com/questions/79062287/how-to-finetune-time-series-transformer-hyper-parameters-to-beat-the-lstm-perfor</link>
      <description><![CDATA[我正在尝试在时间序列数据上训练 ML 模型。输入是 10 个时间序列，本质上是传感器数据。输出是另一组三个时间序列。我给模型输入 100 个窗口。因此，输入形状变为 (100, 10)。我想预测单个时间步长的输出时间序列值。因此，输出形状变为 (1, 3)。（如果我创建大小为 x 的小批量，输入和输出形状将变为 (x, 100, 10) 和 (x, 1, 3)）。
我的方法是首先在较少的记录上对模型进行过度拟合。看看模型是否确实在学习/能够过度拟合数据。然后添加一些正则化（主要是 dropout），然后尝试在完整数据集上训练模型。
首先，我尝试在小数据集上过度拟合 LSTM 模型并可视化结果。它表现不错。所以，我尝试在整个数据集上训练它。它表现还不错，但在某些地方仍然很挣扎。我尝试的LSTM模型如下：
class LSTMModel(nn.Module):
def __init__(self, in_dim=10, hidden_​​size=1400, num_layers=1, output_size=3):
super(LSTMModelV3, self).__init__()

self.lstm_1 = nn.LSTM(in_dim, hidden_​​size, num_layers, batch_first=True) 
self.lstm_2 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True)
self.lstm_3 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True)
self.lstm_4 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True)
self.fc = nn.Linear(hidden_​​size, output_size)

def forward(self, x):
x, _ = self.lstm_1(x)
x, _ = self.lstm_2(x)
x, _ = self.lstm_3(x)
x, _ = self.lstm_4(x)
output = self.fc(x[:, -1, :])
return output

我也尝试添加 dropouts，但没有带来任何显著的改进。因此，我尝试训练 PatchTST transformer 模型。首先，我尝试过拟合较小的模型，效果很好。事实上，当我可视化输出时，我意识到它能够比 LSTM 模型获得更紧密的过拟合。因此，我尝试在整个数据集上对其进行训练。但性能与 LSTM 完全不相上下。
我尝试的 PatchTST 初始版本如下：
config = PatchTSTConfig(
num_input_channels=10,
context_length=100,
num_targets=3,
patch_length=10,
patch_stride=5,
prediction_length=1,
num_hidden_​​layers=5,
num_attention_heads=3,
d_model=300,
)

model = PatchTSTForRegression(config)

以此为基本配置，我尝试对其进行不同的更改以进行超参数优化：

d_model = 600
d_model = 800
d_model = 600, num_hidden_​​layer = 7
d_model = 600，patch_stride = 7
d_model = 300，patch_stride = 7，num_hidden_​​layers = 8

还有一些组合。选择这些超参数组合是为了让我能够在具有 24GB 内存的 GPU 中拟合模型。但是，没有任何配置会产生与 LSTM 相当的验证损失。这些是 LSTM 与 PatchTST 的曲线：

相应的学习率曲线如下：

如果 7 个 epoch 内性能没有提高，我过去常常会降低学习率。
我这里遗漏了什么？我是否错过了任何与时间序列转换器相关的见解？
我正在使用 AdamW 优化器。
PS1：是的，基础 LR 从 0.00005 开始，然后逐步下降到 0.000005、0.0000005、0.00000005。我知道这些非常小。但是，一开始我尝试用更大的基数训练 LSTM，如 0.001、LR 0.005、0.0005 等，但根本不起作用。只有在从 0.00005 开始后，一切才开始起作用。可能是因为我的传感器值本身非常小。
PS2：似乎 LSTM val 损失已经接近 0。但这只是因为我在图中运行的验证损失更高 PatchTST。如果我删除它们并添加 LSTM 过度拟合运行，那么它看起来像这样：
]]></description>
      <guid>https://stackoverflow.com/questions/79062287/how-to-finetune-time-series-transformer-hyper-parameters-to-beat-the-lstm-perfor</guid>
      <pubDate>Mon, 07 Oct 2024 14:08:26 GMT</pubDate>
    </item>
    <item>
      <title>3D加工零件特征识别（点云、网格）</title>
      <link>https://stackoverflow.com/questions/79062004/3d-machining-part-feature-recognition-point-cloud-mesh</link>
      <description><![CDATA[我有一个加工部件 (.STL)，想要识别（并提取）它的加工特征。有些特征很简单，但有些更复杂，这就是为什么我认为机器学习方法会很合适，因为我无法用数学方式描述该特征。
有一个 FeatureNet，它基本上可以完成这项工作，但它无法识别多个特征，并且代码无法按预期工作。
我还知道 AAGNet，它可以完成我想要的工作，但它使用 .STEP 作为输入，但我有一个网格（如果我转换它，则是点云）。
由于有更多的点云存储库，我认为我可以使用它们来解决我的问题。像 FPFH 这样的东西是正确的方向吗，还是我走错了路？
如果我使用机器学习方法，我可以轻松创建标记数据集。]]></description>
      <guid>https://stackoverflow.com/questions/79062004/3d-machining-part-feature-recognition-point-cloud-mesh</guid>
      <pubDate>Mon, 07 Oct 2024 12:41:25 GMT</pubDate>
    </item>
    <item>
      <title>验证多元线性回归模型的 AUC 计算</title>
      <link>https://stackoverflow.com/questions/79061528/validating-auc-calculation-for-a-multiple-linear-regression-model</link>
      <description><![CDATA[我正在尝试计算多元线性回归模型（两个变量）的 AUC 值，因此我正在处理以下代码并想与您确认。
我有两个变量，分别名为 Sa_T1 和 Sa_07，x 轴和 y 轴，z 轴上有 log_EDPs，我确实使用多元线性回归来确定以下关系：
log_EDPs = coef_log_Sa_T1 * log_Sa_T1 + coef_log_Sa_07 * log_Sa_07 + 截距
结果分别为以下值：0.3364、0.6530、-7.1452。现在我想计算 AUC，我根据我之前的单变量案例代码开发了以下代码：
# 系数
coef_log_Sa_T1 = 0.3364
coef_log_Sa_07 = 0.6530
intercept = -7.1452

# 使用回归模型预测的 log(edp)
predicted_log_edp = coef_log_Sa_T1 * log_Sa_T1 + coef_log_Sa_07 * log_Sa_07 + 截距

# 基于阈值的 log(edp)
threshold_SA = np.log(np.median(log_EDPs))
y_true = (log_EDPs &gt;= Threshold_SA).astype(int) 

y_pred_binary = (predicted_log_edp &gt;= Threshold_SA).astype(int)

y_pred_prob = (predicted_log_edp - predicted_log_edp.min()) / (predicted_log_edp.max() - predict_log_edp.min()) 

# 根据预测概率计算 AUC
auc_prob = roc_auc_score(y_true, y_pred_prob)
print(f&quot;AUC (probabilities): {auc_prob}&quot;)

结果 AUC 值为 0.9802。
代码是否进行了正确的计算？]]></description>
      <guid>https://stackoverflow.com/questions/79061528/validating-auc-calculation-for-a-multiple-linear-regression-model</guid>
      <pubDate>Mon, 07 Oct 2024 10:12:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 DDP（分布式数据并行）时，在多 GPU 上获得相同的损失，但梯度不同</title>
      <link>https://stackoverflow.com/questions/79061290/get-same-loss-but-different-grad-on-multi-gpus-when-using-ddp-distributeddatapa</link>
      <description><![CDATA[当将 torch.nn.parallel.DistributedDataParallel 添加到单 GPU 训练代码时，我遇到一个问题，即在不同的 GPU 上得到相同的损失但不同的梯度。与之前的单 GPU 训练代码相比，我确信损失是正确的，但在 loss.backward() 之后，我观察了各层的权重和偏差的梯度，发现在 all_gather 之前它们在不同的 GPU 上是不同的，在 all_gather 和计算损失之间的各层的梯度在不同的 GPU 上是相同的。

这是一个对比学习代码，所以我 all_gather 所有 GPU 上的张量来计算共同的最终损失。

以下是该模型的部分代码：
import torch.nn as nn
import torch
from config.base_config import Config
from modules.transformer import Transformer
from modules.stochastic_module import StochasticText
from modules.basic_utils import AllGather
allgather = AllGather.apply
from modules.tokenization_clip 导入 SimpleTokenizer

class CLIPStochastic(nn.Module):
def __init__(self, config: Config):
super(CLIPStochastic, self).__init__()
self.config = config

从 transformers 导入 CLIPModel
如果 config.clip_arch == &#39;ViT-B/32&#39;:
self.clip = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
elif config.clip_arch == &#39;ViT-B/16&#39;:
self.clip = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch16&quot;)
else:
引发 ValueError

self.task_config = config
config.pooling_type = &#39;transformer&#39;
self.pool_frames = Transformer(config)
self.stochastic = StochasticText(config)

def forward(self, data, return_all_frames=False, is_train=True):
batch_size = data[&#39;video&#39;].shape[0]
text_data = data[&#39;text&#39;] # text_data[&quot;input_ids&quot;].shape = torch.Size([16, 17])
video_data = data[&#39;video&#39;] # [16, 12, 3, 224, 224]
video_data = video_data.reshape(-1, 3, self.config.input_res, self.config.input_res) # [192, 3, 224, 224]

if is_train:

text_features = self.clip.get_text_features(**text_data)
video_features = self.clip.get_image_features(video_data)

video_features = video_features.reshape(batch_size, self.config.num_frames, -1) # [bs, #F, 512]

text_features = allgather(text_features,self.task_config)
video_features = allgather(video_features,self.task_config)
torch.distributed.barrier()

video_features_pooled = self.pool_frames(text_features, video_features)

# @WJM：执行随机文本
text_features_stochstic, text_mean, log_var = self.stochastic(text_features, video_features)

if return_all_frames:
return text_features, video_features, video_features_pooled, text_features_stochstic, text_mean, log_var

return text_features, video_features_pooled, text_features_stochstic, text_mean, log_var

else:

text_features = self.clip.get_text_features(**text_data)
video_features = self.clip.get_image_features(video_data)

video_features = video_features.reshape(batch_size, self.config.num_frames, -1)
video_features_pooled = self.pool_frames(text_features, video_features)

# @WJM：文本的重新参数化（独立于文本条件池化）
text_features_stochstic, _, _ = self.stochastic(text_features, video_features)

if return_all_frames:
return text_features, video_features, video_features_pooled, text_features_stochstic

return text_features, video_features_pooled, text_features_stochstic


allgather 函数如下：
class AllGather(torch.autograd.Function):
&quot;&quot;&quot;对张量执行 allgather 的 autograd 函数。&quot;&quot;&quot;

@staticmethod
def forward(ctx, tensor, args):
output = [torch.empty_like(tensor) for _ in range(args.world_size)]
torch.distributed.all_gather(output, tensor)
ctx.rank = local_rank
ctx.batch_size = tensor.shape[0]
return torch.cat(output, dim=0)

@staticmethod
def behind(ctx, grad_output):
local_grad = grad_output[ctx.batch_size * ctx.rank : ctx.batch_size * (ctx.rank + 1)]
return local_grad, None

我尝试在 AllGather 类中向后添加 all_reduce，代码如下，但似乎不起作用，可能是因为 DDP 自带了同步梯度函数？
def behind(ctx, *grads):
all_gradients = torch.stack(grads)
torch.distributed.all_reduce(all_gradients)
return all_gradients[torch.distributed.get_rank()]
]]></description>
      <guid>https://stackoverflow.com/questions/79061290/get-same-loss-but-different-grad-on-multi-gpus-when-using-ddp-distributeddatapa</guid>
      <pubDate>Mon, 07 Oct 2024 09:03:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用销售数据（价格、SKU、UPC、单位、日期）来预测盈利产品？[关闭]</title>
      <link>https://stackoverflow.com/questions/79061087/how-can-i-use-my-sales-data-price-sku-upc-units-date-to-predict-profitable</link>
      <description><![CDATA[作为店主，我想使用销售数据来预测下个季度（3 个月）哪些商品的库存利润最高，这样我就可以优化库存并实现销售最大化。
验收标准：

我可以上传我的销售数据，包括：
每件商品的销售价格
SKU（库存单位）
UPC（通用产品代码）
产品名称
每件商品的销售单位（在 1 到 10 之间随机分配）
销售日期（在 1 月 1 日至 12 月 30 日之间随机分配）

如何构建模型？
我尝试使用逻辑回归，也使用了时间序列分析，但由于数据集很小，因此可视化效果不合适。我希望首先将数据集可视化并从中提取有意义的特征，然后进行可视化，之后我希望构建能够以最高准确度和更少错误预测销售量的模型。]]></description>
      <guid>https://stackoverflow.com/questions/79061087/how-can-i-use-my-sales-data-price-sku-upc-units-date-to-predict-profitable</guid>
      <pubDate>Mon, 07 Oct 2024 07:57:31 GMT</pubDate>
    </item>
    <item>
      <title>ConvLSTM 能否处理降雨事件和地形模型中的时空数据以进行洪水预测？[关闭]</title>
      <link>https://stackoverflow.com/questions/79061076/can-convlstm-handle-spatiotemporal-data-from-rainfall-events-and-terrain-models</link>
      <description><![CDATA[我正在开展一个洪水预测项目，我想使用 ConvLSTM 模型根据降雨事件作为输入来预测特定区域的径流和洪水深度。
为了准备数据，我使用了 QGIS 和 SWMM：
QGIS 用于创建研究区域的地形模型（海拔、坡度）。
SWMM 用于模拟各种降雨事件并计算每个路口的洪水值，计算不同情景下特定区域的径流和洪水深度值。
我打算使用这些数据训练 ConvLSTM 模型来预测每个路口的径流和周边地区的洪水深度。
通常，ConvLSTM 用于处理连续的 2D 图像数据，例如视频。但是，就我而言，我处理的是时空数据，其中包括降雨事件（例如，随时间变化的降雨强度）和地形信息（例如，海拔、坡度）。这些数据随时间变化而形成 2D 网格，但它并不代表实际的图像序列；相反，它代表降雨和地形数据。
我的问题是：

是否可以使用 TensorFlow 的 ConvLSTM 模型来处理 2D 网格数据（如降雨和地形信息）而不是图像序列？

如果可以，在 TensorFlow 中构建 ConvLSTM 模型以将时空网格数据作为输入来预测洪水深度和径流时，我应该考虑什么？


最初，我计划使用图像数据训练 ConvLSTM 模型。但是，这种方法需要通过 SWMM 和 QGIS 生成图像，因此很难根据实时降雨数据预测洪水。为了解决这个问题，我设计了上述方法，以便模型可以仅使用实时降雨数据工作。我想知道这种方法在理论上和实践上是否可行。]]></description>
      <guid>https://stackoverflow.com/questions/79061076/can-convlstm-handle-spatiotemporal-data-from-rainfall-events-and-terrain-models</guid>
      <pubDate>Mon, 07 Oct 2024 07:55:06 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 TensorFlow 训练中的这个问题？未知图像文件格式。需要 JPEG、PNG、GIF、BMP 之一</title>
      <link>https://stackoverflow.com/questions/79060211/how-do-i-fix-this-problem-with-my-tensorflow-training-unknown-image-file-format</link>
      <description><![CDATA[我正在构建一个 U-Net 模型来检测乳腺癌，我从这里获取了数据集：https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset
尽管所有图像都是 png 格式，但在尝试训练我的模型时，会出现错误，指出我的图像格式不正确。
错误如下：
---------------------------------------------------------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
Cell In[51]，第 7 行
5 train_dataset = image_ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
6 print(image_ds.element_spec)
----&gt; 7 model_history = unet.fit(train_dataset, epochs=EPOCHS)

文件 ~\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 最后：
124 delfiltered_tb

文件 ~\anaconda3\Lib\site-packages\tensorflow\python\eager\execute.py:53，在 quick_execute(op_name, num_outputs, input, attrs, ctx, name) 中
51 尝试：
52 ctx.ensure_initialized()
---&gt; 53 张量 = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
54 输入、属性、输出)
55 除 core._NotOkStatusException 外，因为 e:
56 如果名称不为 None:

InvalidArgumentError：图形执行错误：

在节点处检测到，decode_image/DecodeImage 定义在（最近一次调用最后一次）：
&lt;堆栈跟踪不可用&gt;
传递给 MapDataset:3 转换的用户定义函数中的错误，迭代器：Iterator::Root::Prefetch::BatchV2::Shuffle::MemoryCacheImpl::Filter::ParallelMapV2：未知图像文件格式。需要 JPEG、PNG、GIF、BMP 之一。
[[{{node decrypt_image/DecodeImage}}]]
[[IteratorGetNext]] [Op:__inference_one_step_on_iterator_9520]

错误仅在尝试训练模型时发生，它引用此函数：
def preprocess_image(image, mask, target_size=(256, 256)):
try:
# 安全地解码图像和掩码
image = tf.io.decode_image(image, channels=3, expand_animations=False)
mask = tf.io.decode_image(mask, channels=1, expand_animations=False)

# 检查未定义或零维度
if image.shape is None or image.shape[0] == 0 or image.shape[1] == 0:
print(f&quot;Error: Image has undefined or zero Dimensions: {image.shape}&quot;)
return None, None

if mask.shape is None or mask.shape[0] == 0 or mask.shape[1] == 0:
print(f&quot;Error: Mask 具有未定义或零维度：{mask.shape}&quot;)
return None, None

# 确保图像恰好有 3 个通道 (RGB)
if image.shape[-1] != 3:
print(f&quot;Error: Image does not have 3 channels (found {image.shape[-1]}).&quot;)
return None, None

# 将图像标准化为范围 [0, 1]
image = tf.image.convert_image_dtype(image, tf.float32)

# 将图像和 mask 的大小调整为目标尺寸 (256*256)
image = tf.image.resize(image, target_size, method=&#39;nearest&#39;)
mask = tf.image.resize(mask, target_size, method=&#39;nearest&#39;)

#将 mask 转换为二进制（0 或 1）格式以用于分类任务
mask = tf.cast(tf.math.reduce_max(mask, axis=-1, keepdims=True) &gt; 0, tf.float32) # 确保二进制 mask

return image, mask

except Exception as e:
print(f&quot;Error during preprocessing: {str(e)}&quot;)
return None, None

# 将预处理函数应用于数据集
image_ds = dataset.map(preprocess_image)

# 过滤掉 preprocess_image 返回的 None 值
image_ds = image_ds.filter(lambda img, mask: img is not None and mask is not None)

我尝试了多种方法尝试使用 chatgpt 修复此问题，但似乎没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/79060211/how-do-i-fix-this-problem-with-my-tensorflow-training-unknown-image-file-format</guid>
      <pubDate>Sun, 06 Oct 2024 22:45:18 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中多元时间序列预测的 LSTM 模型中的验证损失和早期停止</title>
      <link>https://stackoverflow.com/questions/76345515/validation-loss-and-early-stopping-in-an-lstm-model-for-multivariate-time-series</link>
      <description><![CDATA[我正在尝试训练一个 LSTM 模型来预测油价，并遵循一些教程。
我的数据集：



日期
美元指数
油价




2019 年 10 月 12 日
50
66


2019 年 10 月 13 日
51
60



其中油价是目标列。
序列大小 = 7，输出 = 1。
我无法添加验证数据并打印除训练和测试损失之外的验证损失。
这是我的代码和尝试：
 #split 为训练、验证和测试（数据集大小为 2380。因此 150 用于测试，100 用于验证，其余用于训练）
X_train = X_seq[:-150]
y_train = y_seq[:-150]
X_test = X_seq[-150:]
y_test = y_seq[-150:] 
X_val = X_train[-100:]
y_val = y_train [-100:]
X_train= X_train [:-100]
y_train = y_train[:-100]

LSTM 模型
class LSTM(nn.Module):
def __init__(self, num_classes, input_size, hidden_​​size, num_layers):
super().__init__()
self.num_classes = num_classes # 输出大小
self.num_layers = num_layers # lstm 中的循环层数量
self.input_size = input_size # 输入大小
self.hidden_​​size = hidden_​​size # 每个 lstm 层中的神经元
# LSTM 模型
self.lstm = nn.LSTM(input_size=input_size, hidden_​​size=hidden_​​size, 
num_layers=num_layers, batch_first=True, dropout=0.2) # lstm
self.fc_1 = nn.Linear(hidden_​​size, 128) # 完全连接
self.fc_2 = nn.Linear(128, num_classes) # 完全连接最后一层
self.relu = nn.ReLU()

def forward(self,x):
# 隐藏状态
h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_​​size))
# 单元状态
c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_​​size))
# 通过 LSTM 传播输入
output, (hn, cn) = self.lstm(x, (h_0, c_0)) # (输入、隐藏和内部状态)
hn = hn.view(-1, self.hidden_​​size) # 重塑密集层的数据
out = self.relu(hn)
out = self.fc_1(out) # 第一个密集
out = self.relu(out) # relu
out = self.fc_2(out) # 最终输出
return out

这是循环：
def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test,
X_val , y_val,):
for epoch in range(n_epochs):
lstm.train()
output = lstm.forward(X_train) # 前向传递
optimiser.zero_grad() # 计算梯度，手动设置为 0
# 获取损失函数
loss = loss_fn(outputs, y_train)
#val_loss = loss_fn(y_val, y_test).item()
#####
###
loss.backward() # 计算损失函数的损失
optimiser.step() # 从损失改进，即反向传播测试损失
lstm.eval()
test_preds = lstm(X_test) 
test_loss = loss_fn(test_preds, y_test)
if epoch % 100 == 0:
print(&quot;Epoch: %d, 训练损失: %1.5f, 测试损失: %1.5f&quot; % (epoch, 
loss.item(), 
test_loss.item()))

这是我对模型的调用方式：
n_epochs = 1000 
learning_rate = 0.001 

input_size = 3 # 特征数量
hidden_​​size = 2 # 隐藏状态的特征数量
num_layers = 1 # 堆叠的 lstm 层数
num_classes = 1 # 输出类数

lstm = LSTM(num_classes, input_size, hidden_​​size, num_layers)

loss_fn = torch.nn.MSELoss() # 回归的均方误差
optimiser = torch.optim.Adam(lstm.parameters(), lr=learning_rate)

training_loop(n_epochs=n_epochs,lstm=lstm, optimiser=optimiser, loss_fn=loss_fn, X_train=X_train_tensors,y_train=y_train_tensors, X_test=X_test_tensors, y_test=y_test_tensors, X_val=X_val_tensors,y_val=y_val_tensors) 


如何通过训练时要考虑的验证集并计算验证损失并在此基础上进行提前停止？]]></description>
      <guid>https://stackoverflow.com/questions/76345515/validation-loss-and-early-stopping-in-an-lstm-model-for-multivariate-time-series</guid>
      <pubDate>Sat, 27 May 2023 05:51:12 GMT</pubDate>
    </item>
    <item>
      <title>如何格式化时间序列数据以用于 PyTorch LSTM 分类？</title>
      <link>https://stackoverflow.com/questions/76321333/how-can-i-format-my-time-series-data-for-pytorch-lstm-classification</link>
      <description><![CDATA[如何预处理时间序列数据以解决分类问题并将其输入 PyTorch LSTM 模型？我有如下图所示的数据集。

此处，event_type 是目标列，这是一个二元分类问题。我想使用 LSTM 训练此数据集。]]></description>
      <guid>https://stackoverflow.com/questions/76321333/how-can-i-format-my-time-series-data-for-pytorch-lstm-classification</guid>
      <pubDate>Wed, 24 May 2023 08:05:02 GMT</pubDate>
    </item>
    <item>
      <title>如何将 Stanza 导出为 ONNX 格式？</title>
      <link>https://stackoverflow.com/questions/70205743/how-to-export-stanza-to-onnx-format</link>
      <description><![CDATA[如何将 Stanza 导出为 ONNX 格式？
似乎不可能只是简单地训练模型。]]></description>
      <guid>https://stackoverflow.com/questions/70205743/how-to-export-stanza-to-onnx-format</guid>
      <pubDate>Thu, 02 Dec 2021 19:59:27 GMT</pubDate>
    </item>
    <item>
      <title>如何建立随机森林和粒子群优化器的混合模型来寻找产品的最优折扣？</title>
      <link>https://stackoverflow.com/questions/63413064/how-to-build-hybrid-model-of-random-forest-and-particle-swarm-optimizer-to-find</link>
      <description><![CDATA[我需要找到每种产品（例如 A、B、C）的最佳折扣，以便最大化总销售额。我为每种产品都建立了随机森林模型，将折扣和季节与销售额进行映射。我如何组合这些模型并将它们提供给优化器以找到每个产品的最佳折扣？
选择模型的原因：

RF：它能够提供更好的（相对于线性模型）预测因子和响应（sales_uplift_norm）之间的关系。
PSO：在许多白皮书中都有建议（可在researchgate/IEEE 获得），也可以在此处和此处的python 包中找到。

输入数据：样本数据用于在产品级别构建模型。数据一览如下：

我的想法/步骤：

针对每个产品构建 RF 模型
 # 预处理数据
products_pre_processed_data = {key:pre_process_data(df, key) for key, df in df_basepack_dict.items()}
# rf 模型
products_rf_model = {key:rf_fit(df) for key, df in products_pre_processed_data .items()}




将模型传递给优化器

目标函数：最大化sales_uplift_norm（RF 模型的响应变量）
约束：

总支出（A + B + C 的支出&lt;= 20），支出 = 产品总销售量 * 折扣百分比 * 产品 mrp_of_products
产品下限（A、B、C）：[0.0, 0.0, 0.0] # 折扣百分比下限
产品上限（A、B、C）：[0.3, 0.4, 0.4] # 折扣百分比上限




sudo/sample code # 因为我无法找到将 product_models 传递到优化器。
从 pyswarm 导入 pso
def obj(x):
model1 = products_rf_model.get(&#39;A&#39;)
model2 = products_rf_model.get(&#39;B&#39;)
model3 = products_rf_model.get(&#39;C&#39;)
return -(model1 + model2 + model3) # -ve 符号表示最大化

def con(x):
x1 = x[0]
x2 = x[1]
x3 = x[2]
return np.sum(units_A*x*mrp_A + unit_B*x*mrp_B + unit_C* x *spend_C)-20 # 支出预算

lb = [0.0, 0.0, 0.0]
ub = [0.3, 0.4, 0.4]

xopt, fopt = pso(obj, lb, ub, f_ieqcons=con)

如何将 PSO 优化器（如果我没有遵循正确的优化器，可以使用任何其他优化器）与 RF 一起使用？
添加用于模型的函数：
def pre_process_data(df,product):
data = df.copy().reset_index()
# print(data)
bp = product
print(&quot;-------product: {}-------&quot;.format(bp))
# 预处理步骤
print(&quot;pre process df.shape {}&quot;.format(df.shape))
#1. 响应变量转换
response = data.sales_uplift_norm # 已转换

#2.预测器数值变量转换 
numeric_vars = [&#39;discount_percentage&#39;] # 可能包括 mrp、深度
df_numeric = data[numeric_vars]
df_norm = df_numeric.apply(lambda x: scale(x), axis = 0) # 中心和尺度

#3. char 字段 dummification
#选择类别字段
cat_cols = data.select_dtypes(&#39;category&#39;).columns
#选择字符串字段
str_to_cat_cols = data.drop([&#39;product&#39;], axis = 1).select_dtypes(&#39;object&#39;).astype(&#39;category&#39;).columns
# 合并所有分类字段
all_cat_cols = [*cat_cols,*str_to_cat_cols]
# print(all_cat_cols)

#将 cat 转换为 dummies
df_dummies = pd.get_dummies(data[all_cat_cols])

#4.将 num 和 char df 组合在一起
df_combined = pd.concat([df_dummies.reset_index(drop=True), df_norm.reset_index(drop=True)], axis=1)

df_combined[&#39;sales_uplift_norm&#39;] = response
df_processed = df_combined.copy()
print(&quot;post process df.shape {}&quot;.format(df_processed.shape))
# print(&quot;model fields: {}&quot;.format(df_processed.columns))
return(df_processed)

def rf_fit(df, random_state = 12):

train_features = df.drop(&#39;sales_uplift_norm&#39;, axis = 1)
train_labels = df[&#39;sales_uplift_norm&#39;]

#随机森林回归器
rf = RandomForestRegressor(n_estimators = 500,
random_state = random_state,
bootstrap = True,
oob_score=True)
# RF 模型
rf_fit = rf.fit(train_features, train_labels)

return(rf_fit)
]]></description>
      <guid>https://stackoverflow.com/questions/63413064/how-to-build-hybrid-model-of-random-forest-and-particle-swarm-optimizer-to-find</guid>
      <pubDate>Fri, 14 Aug 2020 12:47:25 GMT</pubDate>
    </item>
    <item>
      <title>我的目标变量在时间上分布不均匀</title>
      <link>https://stackoverflow.com/questions/59667381/my-target-variable-is-not-evenly-distributed-in-time</link>
      <description><![CDATA[我尝试使用机器学习来预测哪些客户会购买特定产品（购买产品是我的目标变量）。我拥有大量有关客户的特征和足够的历史数据。
我的问题是，我的目标变量具有很强的季节性——大多数产品在 12 月售出，其他月份的销量很少。
我必须做什么来弥补这种不平衡？目标变量是否需要进行一些调整？我需要模型在所有月份都具有一致的性能。]]></description>
      <guid>https://stackoverflow.com/questions/59667381/my-target-variable-is-not-evenly-distributed-in-time</guid>
      <pubDate>Thu, 09 Jan 2020 15:34:43 GMT</pubDate>
    </item>
    <item>
      <title>是否可以将单一回归技术应用于具有不同模式的数据？</title>
      <link>https://stackoverflow.com/questions/52666845/is-it-possible-to-apply-a-single-regression-technique-to-data-that-has-different</link>
      <description><![CDATA[我想根据温度估算多种不同产品的销售量，有些产品之间存在关系。对于其中一种产品，销售额和温度之间的关系绘制出来后如下所示：

这只是一种产品，但这里有一个总体趋势，即从 10 度之后销售额会增加。对于其他产品，关系可能更线性，其他产品可能有多项式关系，而其他产品可能根本没有关系。另一个产品的例子是，其销量和温度之间没有相关性，可能是这个产品：

首先，我想从一个产品中预测一些东西，所以我使用了第一个图中的产品来尝试建模。我最终将数据拆分，这样我就得到了一个数据框，其中包含从 -5 度到 10 度的所有值，并执行了线性回归，类似地，我将 10 度拆分到 30 度以执行线性回归，如下所示：

这里的一个问题是，我正在做各种各样的事情来将我的数据仅适合一种产品。我有一个包含 1000 种产品的数据集，我希望能够根据温度估算某些产品的销量。我想以某种方式循环遍历我的所有数据集，找出哪些数据集在销售和温度之间有某种关系，然后自动应用该特定产品的最佳回归模型，在给定某个温度 X 的情况下估算该产品的销售量。
我看过很多不同的神经网络回归教程，但我根本不知道如何开始或搜索什么，或者我尝试做的事情是否可行？]]></description>
      <guid>https://stackoverflow.com/questions/52666845/is-it-possible-to-apply-a-single-regression-technique-to-data-that-has-different</guid>
      <pubDate>Fri, 05 Oct 2018 13:34:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 sklearn LogisticRegression 和 RandomForest 模型的 Predict() 总是预测少数类 (1)</title>
      <link>https://stackoverflow.com/questions/51968669/predict-with-sklearn-logisticregression-and-randomforest-models-always-predict</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/51968669/predict-with-sklearn-logisticregression-and-randomforest-models-always-predict</guid>
      <pubDate>Wed, 22 Aug 2018 14:03:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 sklearn 的 RandomForestRegressor 进行预测</title>
      <link>https://stackoverflow.com/questions/46694664/prediction-using-sklearns-randomforestregressor</link>
      <description><![CDATA[我的数据如下...
date,locale,category,site,alexa_rank,sessions,user_logins
20170110,US,1,google,1,500,5000
20170110,EU,1,google,2,400,2000
20170111,US,2,facebook,2,400,2000

... 等等。这只是我想出的一个玩具数据集，但与原始数据相似。
我正在尝试使用 sklearn 的 RandomForestRegressor 构建一个模型来预测特定网站将有多少用户登录和会话。
我做了一些常规工作，将类别编码为标签，并根据今年前八个月的数据训练了我的模型，现在我想预测第九个月的登录和会话。我创建了一个针对登录进行训练的模型，以及另一个针对会话进行训练的模型。
我的测试数据集具有相同的形式：
date,locale,category,site,alexa_rank,sessions,user_logins
20170910,US,1,google,1,500,5000
20170910,EU,1,google,2,400,2000
20170911,US,2,facebook,2,400,2000

理想情况下，我希望传入测试数据集时不包含我需要预测的列，但 RandomForestRegressor 抱怨训练集和测试集之间的维度不同。
当我以当前形式传入测试数据集时，模型会预测 sessions 中的 精确 值，并且在大多数情况下，user_logins 列为零，否则值会有微小变化。
我将测试数据中的 sessions 和 user_logins 列归零，并将其传递给模型，但模型预测几乎全部为零。

我的工作流程正确吗？我是否正确使用了 RandomForestRegressor？
当我的测试数据集确实包含实际值时，我如何如此接近实际值？测试数据中的实际值是否用于预测？
如果模型正常工作，如果我将要预测的列（sessions 和 user_logins）归零，我不应该得到相同的预测值吗？
]]></description>
      <guid>https://stackoverflow.com/questions/46694664/prediction-using-sklearns-randomforestregressor</guid>
      <pubDate>Wed, 11 Oct 2017 17:55:37 GMT</pubDate>
    </item>
    </channel>
</rss>