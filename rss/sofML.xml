<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 15 Jan 2024 03:16:41 GMT</lastBuildDate>
    <item>
      <title>如何创建一个功能齐全的时尚推荐系统？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77817212/how-can-i-create-a-fully-functional-fashion-recommender-system</link>
      <description><![CDATA[我想构建一个功能齐全的时尚推荐系统，使用机器学习模型和用户行为分析来提出建议。我需要做什么才能开始？
我已经开始从 Kaggle 下载一些数据集，但我不太清楚如何预处理和清理数据集，因为我是新手。]]></description>
      <guid>https://stackoverflow.com/questions/77817212/how-can-i-create-a-fully-functional-fashion-recommender-system</guid>
      <pubDate>Mon, 15 Jan 2024 00:39:41 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 ML.net 预测间隔</title>
      <link>https://stackoverflow.com/questions/77817042/machine-learning-ml-net-prediction-intervals</link>
      <description><![CDATA[我有一个机器学习 ml.net 二元分类模型，对其进行了训练和评估，为我提供了一个包含很多指标的 CalibrateBinaryClassificationMetrics，但没有一个对我有帮助。
我想以一种可以用预测区间来预测的方式来分析用于评估模型的预测，该预测区间告诉我，真实值在 lowerBound 和 upperBound 之间有 x% 的把握。
我不是 100% 确定这适用于回归模型，但我也想找到一种使用二元分类模型的方法，因此插入了我在回归模型上所做的操作：
//真实结果（1为真，0为假）
浮动[]实际值；

//预测
float[] 预测值；

//使用的样本数
int 样本计数 = 实际值. 长度;

//自变量的数量
int nPredictors；

//PRESS - 预测残差平方和
双击=actualValues.Zip(predictedValues,(实际,预测)=&gt;Math.Pow(实际-预测,2)).Sum();

//PSE - 预测标准误差
double pse = Math.Sqrt(press / (sampleCount - nPredictors));

然后，对于 95% 的置信度：
double PredictedValue = model.Predict(obj);

双 lowerBound = 预测值 - zScore@95 * pse;
双higherBound = 预测值 + zScore@95 * pse;

这是一种有效的方法吗？
我发现不是因为这个：
假设我们知道某事发生的概率为 50%，例如 n = 45000，有 80 个预测变量
int nSamples = 45000;
int nPredictors = 80；

//如果真实结果为真：(1 - 0.5)^2 = 0.25
//如果真实结果为假：(0 - 0.5)^2 = 0.25
//所以，对于 45000 个样本，我们可以说
双击 = 45000 * 0.25 = 11250；
双 pse = Math.Sqrt(11250 / (45000 - 80)) = 0.5004450

这似乎没问题，因为它可以是正的，也可以是假的，而且我们总是有 50% 的正确或错误的差距。
因此，对于 0.5 的预测，对于 zScore@95 = 1.96，我应该说：
双下界 = 0.5 - 0.97587 = -0.47587
双倍上限 = 0.5 + 0.97587 = 1.47587

我认为真实概率介于 -0.47587 和 1.47587 之间是有道理的，但这不是我想知道的。
我想估计一个错误，即我的模型 95% 确定发生的真实概率就在那里。
我应该考虑模型的可变性吗？
比如 Brier 分数之类的？
就像，在这种情况下，荆棘分数将是：
double brierScore = ActualValues.Zip(predictedValues, (实际, 预测) =&gt; Math.Pow(预测 - 实际, 2)).Sum() / nSamples;
//在本例中，0.25 / 45000 = 5.556E-6

这似乎更有可能，但我不知道如何根据 Brier 分数创建预测区间...
我该如何计算这样的东西？]]></description>
      <guid>https://stackoverflow.com/questions/77817042/machine-learning-ml-net-prediction-intervals</guid>
      <pubDate>Sun, 14 Jan 2024 23:15:31 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 LogisticRegression 模型返回准确度 0？</title>
      <link>https://stackoverflow.com/questions/77816612/why-is-my-logisticregression-model-returns-accuracy-0</link>
      <description><![CDATA[我正在尝试训练一个模型来预测客户的总购买金额。
我的数据如下所示：https://i.stack.imgur.com/JbM9J .png
以下是进一步的步骤：
# 预处理

def preprocess_input(df):
    df = df.copy()

    #删除用户ID列
    df = df.drop(&#39;用户ID&#39;, axis=1)

    #二进制编码
    df[“性别”] = df[“性别”].replace({“女”:0, “男”:1})

    #将 df 分割为 x 和 y
    y = df[“已购买”]
    X = df.drop([“已购买”], axis=1)

    #训练-测试分割
    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True, random_state=1)

    #缩放x
    定标器=标准定标器()
    缩放器.fit(X_train)
    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)
    X_test = pd.DataFrame(scaler.transform(X_test),index=X_test.index,columns=X_test.columns)

    返回X_train，X_test，y_train，y_test


X_train, X_test, y_train, y_test = preprocess_input(数据)

# 训练/结果

模型=逻辑回归()
model.fit(X_train, y_train)

acc = model.score(X_test, y_test)
print(&quot;测试准确度: {:.3f}&quot;.format(acc * 100))

但是结果是
测试精度：0.000

这里可能出了什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/77816612/why-is-my-logisticregression-model-returns-accuracy-0</guid>
      <pubDate>Sun, 14 Jan 2024 20:35:17 GMT</pubDate>
    </item>
    <item>
      <title>对于大多数列来自 one-hot 编码的高维数据，应使用哪种特征选择方法？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77816559/what-feature-selection-method-should-be-used-for-high-dimensionality-data-with-m</link>
      <description><![CDATA[我正在尝试用 Python 解决与体育相关的多类分类问题，我的目标是训练自定义神经网络和 SVM。我已经执行了事先的数据清理，并使用非数值的 one-hot 编码对我的数据进行了编码。然而，由于一种热编码，数据帧现在相当大。所以我想进行特征选择，但是经过一些研究后发现有很多种特征选择方法。我应该采取什么具体方法？
数据包含我使用标签编码器编码的数字输入特征和分类目标变量（主队获胜、客队获胜或平局）。]]></description>
      <guid>https://stackoverflow.com/questions/77816559/what-feature-selection-method-should-be-used-for-high-dimensionality-data-with-m</guid>
      <pubDate>Sun, 14 Jan 2024 20:13:29 GMT</pubDate>
    </item>
    <item>
      <title>神经网络介绍聊天机器人项目：数据加载器错误[关闭]</title>
      <link>https://stackoverflow.com/questions/77816432/neural-network-intro-chatbot-project-dataloader-error</link>
      <description><![CDATA[我正在开发一个介绍性聊天机器人项目（链接到我正在关注的教程：https://github.com/patrickloeber/pytorch-chatbot/tree/master）。我使用了他们的确切代码并遇到了错误：
&lt;块引用&gt;
回溯（最近一次调用最后一次）：文件
“/Users/sarayu/Desktop/MLLearning/ChatbotIntroProj/train.py”，第 81 行，
在
for(words, labels) in train_loader: 文件“/Users/sarayu/Desktop/MLLearning/ChatbotIntroProj/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py”，
第 630 行，在下一个
data = self._next_data() 文件“/Users/sarayu/Desktop/MLLearning/ChatbotIntroProj/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py”，
第 1328 行，在 _next_data 中
idx, data = self._get_data() 文件“/Users/sarayu/Desktop/MLLearning/ChatbotIntroProj/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py”，
第 1294 行，在 _get_data 中
成功，data = self._try_get_data() 文件“/Users/sarayu/Desktop/MLLearning/ChatbotIntroProj/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py”，
第 1145 行，在 _try_get_data 中
引发 RuntimeError(f&#39;DataLoader 工作程序 (pid(s) {pids_str}) 意外退出&#39;) from e RuntimeError: DataLoader 工作程序 (pid(s) 47712,
47713) 意外退出

当我尝试运行 train.py 文件时
我运行了该文件，并收到了错误消息。不确定从哪里开始调试]]></description>
      <guid>https://stackoverflow.com/questions/77816432/neural-network-intro-chatbot-project-dataloader-error</guid>
      <pubDate>Sun, 14 Jan 2024 19:28:12 GMT</pubDate>
    </item>
    <item>
      <title>如何将字符串变量转为数值变量？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77815850/how-to-turn-a-string-variable-into-a-numeric-variable</link>
      <description><![CDATA[我正在做一个分类模型（随机森林和 SVM），但是为了做到这一点，我必须将要使用的所有变量都转换为数字变量。
例如，我的这些列包含以下信息：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

材料名称
关键性


&lt;正文&gt;

座环
非关键


干气密封
生产关键




“材料名称”列有超过100个变量，并且“Criticality”具有100多个变量。有 4 个类别。
对于“关键性”专栏，我尝试进行 One-Hot 编码
df = pd.get_dummies(df, columns=[&#39;Criticality&#39;])

但是对于“材料名称”我不知道最好的方法是什么。]]></description>
      <guid>https://stackoverflow.com/questions/77815850/how-to-turn-a-string-variable-into-a-numeric-variable</guid>
      <pubDate>Sun, 14 Jan 2024 16:40:34 GMT</pubDate>
    </item>
    <item>
      <title>拟合 GridSearchCV 和 VotingClassifier 的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/77815549/problem-with-fitting-gridsearchcv-and-votingclassifier</link>
      <description><![CDATA[我在 Sklearn 中使用 GreadSearch 时遇到问题。这是我的 2 段代码：
from sklearn.compose import ColumnTransformer, make_column_selector
从 sklearn.preprocessing 导入 StandardScaler

哦= OneHotEncoder（）
定标器=标准定标器()

numeric_columns = make_column_selector(dtype_include=&#39;number&#39;)(X_ros)

变压器 = ColumnTransformer([
    （&#39;编码器&#39;，哦，object_columns），
    （&#39;缩放器&#39;，缩放器，数字列）
]）

从 sklearn.linear_model 导入 LogisticRegressionCV
从 sklearn.neighbors 导入 KNeighborsClassifier
从 sklearn.ensemble 导入 VotingClassifier
从 sklearn.model_selection 导入 GridSearchCV

分类器={
    &#39;logistic_reg&#39;: LogisticRegressionCV(solver=&#39;saga&#39;),
    &#39;knn&#39;: KNeighborsClassifier()
}
模型=管道（[
    （&#39;预处理器&#39;，变压器），
    (&#39;分类器&#39;, VotingClassifier(分类器))
]）

参数 = [{
    &#39;logistic_reg__Cs&#39;: 列表(范围(1, 11)),
    &#39;logistic_reg__penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;],
    &#39;logistic_reg__class_weight&#39;：[无，&#39;平衡&#39;]，
    &#39;knn__n_neighbors&#39;: 列表(范围(1, 16)),
    &#39;knn__weights&#39;: [&#39;均匀&#39;, &#39;距离&#39;]
}
gridsearch = GridSearchCV(模型，参数，评分=&#39;f1&#39;，n_jobs=-1，cv=5)

但是我们有一个错误：
ValueError：需要解压的值太多（预期为 2）

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77815549/problem-with-fitting-gridsearchcv-and-votingclassifier</guid>
      <pubDate>Sun, 14 Jan 2024 15:07:17 GMT</pubDate>
    </item>
    <item>
      <title>为多数类和少数类生成综合数据</title>
      <link>https://stackoverflow.com/questions/77815288/generate-synthetic-data-for-majority-and-minority-classes</link>
      <description><![CDATA[我正在研究一个分类问题，我尝试为多数类和少数类生成合成数据，因为我想在合成数据上训练我的模型并在实际数据上进行测试，我正在使用下面的代码，但我是无法生成多数类的合成数据，我该怎么办？
代码：
每种类型的 # 列
Continuous_cols = [&#39;amt&#39;, &#39;BAL&#39;, &#39;MOB&#39;, &#39;TPOP&#39;]
boolean_cols = [&#39;EVER_L3M&#39;,&#39;EVER_L3M&#39;,&#39;EVER_L6M&#39;]
target_col = &#39;目标&#39;

将 pandas 导入为 pd
从 imblearn.over_sampling 导入 SMOTENC，RandomOverSampler

def oversample_both_classes(df, Continuous_cols, boolean_cols, target_col):
    # 单独的特征和目标
    X = df.drop(target_col, 轴=1)
    y = df[目标列]

    # 计算布尔索引和连续索引
    boolean_indices = [X.columns.get_loc(col) for col in boolean_cols]
    Continuous_indices = [X.columns.get_loc(col) for col in Continuous_cols]

    
    分类指数 = 布尔指数 + 连续指数

    # 使用 SMOTENC 对少数类进行过采样
    smotenc = SMOTENC(random_state=42, categorical_features=boolean_indices)
    X_resampled_minority, y_resampled_minority = smotenc.fit_resample(X, y)

    # 使用 RandomOverSampler 对多数类进行过采样
    ros = RandomOverSampler(random_state=42)
    X_resampled_majority, y_resampled_majority = ros.fit_resample(X, y)

    
    X_resampled = pd.concat([pd.DataFrame(X_resampled_minority, columns=X.columns),
                            pd.DataFrame(X_resampled_majority, columns=X.columns)])
    y_resampled = pd.concat([pd.Series(y_resampled_minority), pd.Series(y_resampled_majority)])

    
    resampled_df = pd.DataFrame(X_resampled, columns=X.columns)
    resampled_df[target_col] = y_resampled

    返回重采样_df


oversampled_df_PL = oversample_both_classes(df, Continuous_cols, boolean_cols, target_col)
]]></description>
      <guid>https://stackoverflow.com/questions/77815288/generate-synthetic-data-for-majority-and-minority-classes</guid>
      <pubDate>Sun, 14 Jan 2024 13:50:11 GMT</pubDate>
    </item>
    <item>
      <title>针对复杂的多标签问题训练神经网络</title>
      <link>https://stackoverflow.com/questions/77814783/training-neural-network-on-a-complex-multi-label-problem</link>
      <description><![CDATA[我正在尝试在大型胸部 X 射线数据集 CheXpert 上训练神经网络 (https ://stanfordmlgroup.github.io/competitions/chexpert/）。您基本上拥有的是一组大约 (320, 390) 的灰度 X 射线图像和 14 个类，其中多个类可以同时为真：

我将这些标签编码为大小为 14 的向量，其中每个元素表示一个类的存在。我将所有不确定值（-1，NaN）转换为负数（0），正类标记为（1）。标签就会变成例如 [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1]。
然后我构建了以下神经网络：
data_augmentation = 顺序(
    [
        Layers.RandomFlip(“水平”),
        层.RandomRotation(0.1),
        层.RandomZoom(0.2),
    ]
）

输入 = tf.keras.Input(形状=(320, 390, 1))

x = 层.重新缩放(1./255)(输入)

x = 层.Conv2D（过滤器= 32，kernel_size = 3，激活=“relu”）（x）

x = 层数.MaxPooling2D(pool_size=2)(x)

x = 层.Conv2D（过滤器= 64，kernel_size = 3，激活=“relu”）（x）

x = 层数.MaxPooling2D(pool_size=2)(x)

x = 层.Conv2D（过滤器= 128，kernel_size = 3，激活=“relu”）（x）

x = 层数.MaxPooling2D(pool_size=2)(x)

x = 层.Conv2D（过滤器= 256，kernel_size = 3，激活=“relu”）（x）

x = 层数.MaxPooling2D(pool_size=2)(x)

x = 层.Conv2D（过滤器= 512，kernel_size = 3，激活=“relu”）（x）

x = 层数.MaxPooling2D(pool_size=2)(x)

x = 层.Dropout(0.5)(x)

x = 层.Flatten()(x)
输出=层.Dense(FEATURES_SIZE,激活=“sigmoid”)(x)
模型= tf.keras.Model（输入=输入，输出=输出）

模型.summary()

model.compile(loss=“binary_crossentropy”,
              优化器=“亚当”，
              指标=[
                  &#39;准确性&#39;，
                  指标.AUC(),
                  指标.Precision(),
                  指标.Recall()
              ]
）

回调 = [
    模型检查点(
        文件路径=“chexpert_convnet1.tf”，
        监视器=“val_auc”，
        save_best_only=真），
    提前停止(
      监视器=&#39;val_auc&#39;,
      耐心=4，
      恢复最佳权重=真）
]

历史=模型.fit(
    训练数据集，
    纪元=20，
    验证数据=验证数据集，
    回调=回调
）

但是假设训练进行得不太顺利：
纪元 1/20
803/803 [==============================] - 256s 285ms/步 - 损耗：0.3541 - 准确度：0.1143 - auc ：0.8130 - 精度：0.5535 - 召回率：0.2665 - val_loss：0.3387 - val_accuracy：0.1296 - val_auc：0.8357 - val_ precision：0.6213 - val_recall：0.2707
纪元 2/20
803/803 [================================] - 224s 272ms/步 - 损失：0.3321 - 准确度：0.1542 - auc ：0.8403 - 精度：0.6029 - 召回率：0.3794 - val_loss：0.3290 - val_accuracy：0.1536 - val_auc：0.8462 - val_ precision：0.6011 - val_recall：0.4224
纪元 3/20
803/803 [================================] - 223s 270ms/步 - 损耗：0.3250 - 准确度：0.1668 - auc ：0.8487 - 精度：0.6168 - 召回率：0.4034 - val_loss：0.3242 - val_accuracy：0.1407 - val_auc：0.8523 - val_ precision：0.6055 - val_recall：0.4673
纪元 4/20
803/803 [================================] - 219s 266ms/步 - 损失：0.3197 - 准确度：0.1784 - auc ：0.8547 - 精度：0.6266 - 召回率：0.4182 - val_loss：0.3195 - val_accuracy：0.1724 - val_auc：0.8570 - val_ precision：0.6441 - val_recall：0.3875
纪元 5/20
803/803 [==============================] - 225s 272ms/步 - 损失：0.3143 - 准确度：0.1868 - auc ：0.8606 - 精度：0.6382 - 召回率：0.4286 - val_loss：0.3175 - val_accuracy：0.1676 - val_auc：0.8599 - val_ precision：0.6145 - val_recall：0.4828
纪元 6/20
803/803 [==============================] - 231s 276ms/步 - 损失：0.3085 - 准确度：0.1966 - auc ：0.8667 - 精度：0.6524 - 召回率：0.4439 - val_loss：0.3148 - val_accuracy：0.1820 - val_auc：0.8620 - val_ precision：0.6471 - val_recall：0.4198
纪元 7/20
803/803 [==============================] - 230s 275ms/步 - 损耗：0.3019 - 准确度：0.2087 - auc ：0.8737 - 精度：0.6616 - 召回率：0.4571 - val_loss：0.3156 - val_accuracy：0.1584 - val_auc：0.8610 - val_ precision：0.6570 - val_recall：0.3934

我不知道从这里该去哪里。有没有人有类似的经验？]]></description>
      <guid>https://stackoverflow.com/questions/77814783/training-neural-network-on-a-complex-multi-label-problem</guid>
      <pubDate>Sun, 14 Jan 2024 11:03:36 GMT</pubDate>
    </item>
    <item>
      <title>文件夹图像的图像序列分类</title>
      <link>https://stackoverflow.com/questions/77789033/image-sequence-classification-from-folder-images</link>
      <description><![CDATA[我有 3164 张图像，分为 3 个文件夹，属于 3 个类
我的代码是这样的：
导入pathlib
将张量流导入为 tf

data_dir = pathlib.Path(&#39;/图像路径/&#39;)

批量大小 = 3
图片高度 = 200
图像宽度 = 200

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  数据目录，
  验证分割=0.2，
  子集=“训练”，
  种子=123，
  图像大小=（img_高度，img_宽度），
  批量大小=批量大小）

class_names = train_ds.class_names # 0,1,2

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  数据目录，
  验证分割=0.2，
  子集=“验证”，
  种子=123，
  图像大小=（img_高度，img_宽度），
  批量大小=批量大小）

val_batches = tf.data.experimental.cardinality(val_ds)
test_dataset = val_ds.take(val_batches / 5)

我的模型是：
模型 = Sequential()
model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2), 激活=&#39;relu&#39;, padding=&#39;相同&#39;), input_shape=(3 ,200, 200, 3))) #
model.add(TimeDistributed(Conv2D(32, (3,3), kernel_initializer=“he_normal”,activation=&#39;relu&#39;)))
model.add(TimeDistributed(MaxPooling2D((2, 2), 步长=(2, 2))))

model.add(TimeDistributed(Conv2D(64, (3,3), padding=&#39;same&#39;,activation=&#39;relu&#39;)))
model.add(TimeDistributed(Conv2D(64, (3,3), padding=&#39;same&#39;,activation=&#39;relu&#39;)))
model.add(TimeDistributed(MaxPooling2D((2, 2), 步长=(2, 2))))

model.add(TimeDistributed(Conv2D(128, (3,3), padding=&#39;same&#39;,activation=&#39;relu&#39;)))
model.add(TimeDistributed(Conv2D(128, (3,3), padding=&#39;same&#39;,activation=&#39;relu&#39;)))
model.add(TimeDistributed(MaxPooling2D((2, 2), 步长=(2, 2))))

model.add(TimeDistributed(Conv2D(256, (3,3), padding=&#39;same&#39;,activation=&#39;relu&#39;)))
model.add(TimeDistributed(Conv2D(256, (3,3), padding=&#39;same&#39;,activation=&#39;relu&#39;)))
model.add(TimeDistributed(MaxPooling2D((2, 2), 步长=(2, 2))))

model.add(TimeDistributed(Conv2D(512, (3,3), padding=&#39;same&#39;,activation=&#39;relu&#39;)))
model.add(TimeDistributed(Conv2D(512, (3,3), padding=&#39;same&#39;,activation=&#39;relu&#39;)))
model.add(TimeDistributed(MaxPooling2D((2, 2), 步长=(2, 2))))

model.add(TimeDistributed(Flatten()))

模型.add(Dropout(0.5))
model.add(LSTM(256, return_sequences=True, dropout=0.5))
model.add（密集（3，激活=&#39;softmax&#39;））
 
模型.summary()

model.compile（优化器= Adam（lr = 0.0001），损失=&#39;categorical_crossentropy&#39;，指标= [&#39;准确性&#39;]）

回调 = [
    keras.callbacks.ReduceLROnPlateau(详细=1),
    keras.callbacks.ModelCheckpoint(
        &#39;路径/权重。{epoch:02d}-{val_loss:.2f}.hdf5&#39;,
        详细=1),
]
历史=模型.fit(
    火车_ds，
    验证数据=val_ds，
    详细=1，
    纪元=50，
    回调=回调
）

当我第一次运行它时，它给出的输入 0 的形状不兼容，但我像这样更改了 train_ds：
train_ds = train_ds.batch(3)

它给出的错误为
形状（无，无）和（无，无，3）不兼容

我想将这些图像分为三组（带有标签集），分为 3 个类别
我的模型或数据集的准备有问题，但我找不到解决办法。]]></description>
      <guid>https://stackoverflow.com/questions/77789033/image-sequence-classification-from-folder-images</guid>
      <pubDate>Tue, 09 Jan 2024 18:37:56 GMT</pubDate>
    </item>
    <item>
      <title>RCV1 数据集的采样和特征选择[关闭]</title>
      <link>https://stackoverflow.com/questions/77776934/sampling-and-feature-selection-on-rcv1-dataset</link>
      <description><![CDATA[我有兴趣在 RCV1数据集。然而，这个数据集非常大，数据的维度为 (804414, 47236)，目标的维度为 (804414, 103)。此外，数据的很大一部分包含零。
每次我尝试训练模型时，我都会收到内存错误或异常值和不相关的数据。
我在谷歌colab中使用python。
为了使这些算法更容易运行，我正在考虑采用采样或特征选择等方法。如何做到这一点以及哪些技术有效？]]></description>
      <guid>https://stackoverflow.com/questions/77776934/sampling-and-feature-selection-on-rcv1-dataset</guid>
      <pubDate>Mon, 08 Jan 2024 06:07:13 GMT</pubDate>
    </item>
    <item>
      <title>我使用 XGB 分类器训练了数据集</title>
      <link>https://stackoverflow.com/questions/77776124/ive-trained-dataset-using-xgb-classifier</link>
      <description><![CDATA[我从我的队友那里得到了我们项目的这部分代码，我在本地遇到了这个错误，我已经使用 XGB 分类器训练了数据集。
我的代码是：
# XGBoost 分类器模型
从 xgboost 导入 XGBClassifier

# 实例化模型
xgb = XGBClassifier()

# 拟合模型
xgb.fit(X_train,y_train)

然后我得到了这个错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ValueError Traceback（最近一次调用最后一次）
[70] 中的单元格，第 8 行
      5 xgb = XGBClassifier()
      7#拟合模型
----&gt; 8 xgb.fit(X_train,y_train)

文件 ~/anaconda3/envs/project/lib/python3.10/site-packages/xgboost/core.py:730，在 require_keyword_args..throw_if..inner_f(*args, **kwargs ）
    728 k, arg in zip(sig.parameters, args)：
    第729章
--&gt;第730章

文件〜/anaconda3/envs/project/lib/python3.10/site-packages/xgboost/sklearn.py:1471，在XGBClassifier.fit（self，X，y，sample_weight，base_margin，eval_set，eval_metric，early_stopping_rounds，verbose， xgb_model、sample_weight_eval_set、base_margin_eval_set、feature_weights、回调）
   第1466章
   第1467章
   第1468章
   第1469章
   第1470章
-&gt;第1471章
   攀上漂亮女局长之后1472 ”
   第1473章
   第1474章
   第1476章
   第1478章

ValueError：从“y”的唯一值推断出无效的类。

预期：[0 1]，得到[-1 1]，，我听说 y_train 必须在较新的更新中进行编码，但我对这些事情有点陌生，我也不知道如何做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/77776124/ive-trained-dataset-using-xgb-classifier</guid>
      <pubDate>Mon, 08 Jan 2024 03:09:32 GMT</pubDate>
    </item>
    <item>
      <title>拟合良好，但测试期间分类错误。应该做什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77748142/good-fit-but-false-classification-during-test-what-should-be-done</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77748142/good-fit-but-false-classification-during-test-what-should-be-done</guid>
      <pubDate>Tue, 02 Jan 2024 19:24:02 GMT</pubDate>
    </item>
    <item>
      <title>改进 LSTM 模型对数据集噪声的拟合</title>
      <link>https://stackoverflow.com/questions/70681628/improving-an-lstm-models-fit-to-a-datasets-noise</link>
      <description><![CDATA[我是一名数据分析师，试图通过机器学习来提高我的知识。
我已经完成了时间序列数据集的模型，其中每个点相隔 1 天，没有间隙。我尝试的具体模型类型是使用tensorflow的keras的多层自回归双向LSTM，请参阅下面的模型具体代码：
模型 = keras.Sequential()
model.add(双向(LSTM(
            单位 = 128，
            input_shape = (X_train.shape[1], X_train.shape[2]),
            return_sequences=True）））
model.add(双向(LSTM(
            单位 = 64，
            input_shape = (X_train.shape[1], X_train.shape[2]),
            return_sequences=True）））
model.add(双向(LSTM(
            单位 = 32，
            input_shape = (X_train.shape[1], X_train.shape[2]),
            return_sequences=True）））
model.add(双向(LSTM(
            单位 = 16,
            input_shape = (X_train.shape[1], X_train.shape[2]),
            return_sequences=False）））
model.add(keras.layers.Dense(16))
model.add(keras.layers.Dropout(rate = 0.5))
model.add(keras.layers.Dense(1))
model.compile(loss=&#39;mean_squared_error&#39;, 优化器=&#39;Adam&#39;)
历史=模型.fit(
    X_列车，y_列车，
    历元 = 100,
    批量大小=128，
    验证分割= 0.2，
    随机播放=假
）
打印（模型.摘要（））

一位高级员工告诉我，对于这个特定的学习任务来说，这可能有点过分了，但我想添加它以实现完全透明。
请参阅下面的摘要：
 图层（类型）输出形状参数#
=================================================== ===============
 双向（Bidirectiona（无、50、256）133120
 l)

 bidirection_1（双向（无、50、128）164352
 纳尔)

 bidirection_2（双向（无、50、64）41216
 纳尔)

 bidirection_3（双向（无，32）10368
 纳尔)

 密集（密集）（无，16）528

 辍学（辍学）（无，16）0

 密集_1（密集）（无，1）17

=================================================== ===============
总参数：349,601
可训练参数：349,601
不可训练参数：0
_________________________________________________________________

模型报告损失值（100 个时期后，使用均方误差）：
&lt;块引用&gt;
损失：0.0040 - val_loss：0.0050（过拟合）

使用以下方法导出的 RMSE：math.sqrt(mean_squared_error(y_train,train_predict)) 和 math.sqrt(mean_squared_error(y_test,test_predict)) 以及  sklearn.metrics 和上述包中的内置函数 mean_squared_error。
&lt;块引用&gt;
训练 RMSE：28.795422522129595

&lt;块引用&gt;
测试 RMSE：34.17014386085355

对于图形表示：


我终于回答了我的问题；我如何更好地拟合我的模型以更接近地表示数据中的噪声，因为我认为这是导致高 RMSE 值的原因。
我研究了注意力机制，希望能够突出显示数据中的特定波峰和波谷，但似乎这些机制最好与面向图像/文本预测的模型一起使用。我可以尝试训练更多的纪元，但模型已经有点过度拟合，因此这会进一步加剧这个特定问题。
我知道这是一个相当开放式的问题，但我已尽力“展示我的工作”，并提前感谢您。]]></description>
      <guid>https://stackoverflow.com/questions/70681628/improving-an-lstm-models-fit-to-a-datasets-noise</guid>
      <pubDate>Wed, 12 Jan 2022 12:33:42 GMT</pubDate>
    </item>
    <item>
      <title>给定足够的隐藏神经元，神经网络可以逼近任何函数吗？</title>
      <link>https://stackoverflow.com/questions/25609347/can-neural-networks-approximate-any-function-given-enough-hidden-neurons</link>
      <description><![CDATA[我知道具有任意数量隐藏层的神经网络可以近似非线性函数，但是，它可以近似：

&lt;前&gt;&lt;代码&gt;f(x) = x^2

我想不通怎么会这样。这似乎是神经网络的一个非常明显的局限性，可能会限制它的功能。例如，由于这一限制，神经网络可能无法正确近似统计中使用的许多函数，例如指数移动平均线，甚至方差。
说到移动平均线，循环神经网络可以正确地近似吗？我了解前馈神经网络甚至单个线性神经元如何使用滑动窗口技术输出移动平均值，但是如果没有 X 数量的隐藏层（X 是移动平均大小），循环神经网络将如何做到这一点？
另外，假设我们不知道原始函数f，它恰好获取最后500个输入的平均值，然后如果高于3则输出1，如果高于3则输出0如果不是的话。但暂时假装我们不知道这一点，它是一个黑匣子。
循环神经网络如何近似这一点？我们首先需要知道它应该有多少个时间步，但我们没有。也许 LSTM 网络可以，但即便如此，如果它不是简单的移动平均线，而是指数移动平均线怎么办？我认为即使是 LSTM 也做不到。
更糟糕的是，如果我们想要学习的f(x,x1)只是简单的

&lt;前&gt;&lt;代码&gt;f(x,x1) = x * x1

这看起来非常简单明了。神经网络可以学习吗？我不明白怎么办。
我是否在这里遗漏了一些重要的东西，或者机器学习算法是否极其有限？除了神经网络之外，还有其他学习技术可以真正做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/25609347/can-neural-networks-approximate-any-function-given-enough-hidden-neurons</guid>
      <pubDate>Mon, 01 Sep 2014 15:51:03 GMT</pubDate>
    </item>
    </channel>
</rss>