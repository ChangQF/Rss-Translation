<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 17 Jul 2024 12:29:23 GMT</lastBuildDate>
    <item>
      <title>DSPy 无法检索 ChromaDB 中带有文本嵌入的段落</title>
      <link>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</link>
      <description><![CDATA[我正在使用 DSPy 和 ChromaDB 为 pdf 文件开发 RAG 应用程序。
首先，我从 pdf 中获取文本并将其作为块添加到 Chromadb。还添加了块的嵌入。并尝试使用 DSPy 检索与查询相关的块。但是它出现了错误
存储数据和嵌入
def store_document_in_chromadb(text):
chunks = chunk_document(text)
ids = [f&#39;chunk_{i}&#39; for i in range(len(chunks))]
embeddings = [get_embedding(chunk).tolist() for chunk in chunks]

collection.add(ids=ids, documents=chunks, embeddings=embeddings)

我尝试像这样检索相关块，
retriever_model = ChromadbRM(&quot;contracts_collection&quot;, &#39;db/&#39;, k=2)
dspy.settings.configure(lm=llama2_model, rm=retriever_model)

class GenerateAnswer(dspy.Signature): 
“”“”根据给出的上下文回答问题。“”“”
context = dspy.InputField(desc=&quot;可能包含相关上下文&quot;)
question = dspy.InputField()
answer = dspy.OutputField(desc=&quot;通常为 5 到 10 个单词&quot;)

class RAG(dspy.Module): 
def __init__(self, num_passages=2):
super().__init__()
self.retrieve = dspy.Retrieve(k=num_passages)
self.generate_answer = dspy.ChainOfThought(GenerateAnswer)

def forward(self, question):
context = self.retrieve(question).passages
prediction = self.generate_answer(context=context, question=question)
return dspy.Prediction(context=context, answer=prediction.answer)

with dspy.context(lm=llama2_model, rm=retriever_model):
module = RAG()
response = module(&quot;总支出是多少&quot;)
print(response)

当我运行此程序时，出现此错误
InvalidDimensionException：嵌入维度 384 与集合维度 768 不匹配
但是当我从 ChromaDB 中删除嵌入时，它会正确检索相关块。
有人知道为什么使用嵌入时没有出现此错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</guid>
      <pubDate>Wed, 17 Jul 2024 08:03:30 GMT</pubDate>
    </item>
    <item>
      <title>请帮助我构建我的二元分类项目[关闭]</title>
      <link>https://stackoverflow.com/questions/78758049/please-help-me-to-structure-my-binary-classification-project</link>
      <description><![CDATA[我正在开发一个二元分类项目。最初，我得到了一个包含 3290 行和 15 列的真实数据的数据集。然后，我使用 CTGAN 网络生成了包含 100000 行的合成数据集。然后，我将这两个数据集混洗，得到 1 个数据集。我的目标变量高度不平衡（是：23175，否：76825）。我对我的项目有以下问题？

有 7 个分类预测因子，其中有 4 个二元分类列（性别、婚姻状况等），其他是非二元分类变量（省、区等）。我应该使用什么编码技术？

处理这里的数据不平衡问题是否重要？如果重要，我应该使用什么技术来处理这个不平衡问题？

我的数值变量都不是正态分布的。处理这个问题是否重要？如果是，我需要使用哪些技术（例如，如果需要，进行转换）？

我需要标准化或规范化我的数据吗？如果是，为什么？

我应该在这里使用哪些特征选择技术？

我的项目的顺序是什么。请按以下顺序排列。（数据不平衡问题处理/编码分类变量/转换数值数据/标准化或规范化数值数据/特征选择\建模）

最后我可以使用神经网络来实现这一点吗？如果可以，我可以使用哪些 NN 类型


我知道这是一个很长的问题，感谢您花时间和精力回答这些问题。
我期待上述问题的答案。]]></description>
      <guid>https://stackoverflow.com/questions/78758049/please-help-me-to-structure-my-binary-classification-project</guid>
      <pubDate>Wed, 17 Jul 2024 06:57:24 GMT</pubDate>
    </item>
    <item>
      <title>如何在 XGBoost 决策树的叶节点上显示预测类标签？</title>
      <link>https://stackoverflow.com/questions/78757774/how-do-i-display-predicted-class-labels-on-the-leaf-nodes-of-an-xgboost-decision</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78757774/how-do-i-display-predicted-class-labels-on-the-leaf-nodes-of-an-xgboost-decision</guid>
      <pubDate>Wed, 17 Jul 2024 05:22:10 GMT</pubDate>
    </item>
    <item>
      <title>无法安装 pytorch-forecasting：没有名为“distutils”的模块错误</title>
      <link>https://stackoverflow.com/questions/78757738/unable-to-install-pytorch-forecasting-no-module-named-distutils-error</link>
      <description><![CDATA[当我尝试在 VSCode 终端中运行 pip install pytorch-forecasting 时，我收到错误消息，提示没有 没有名为“distutils”的模块。
首先，PyTorch 已安装，并且使用 conda install pytorch-forecasting pytorch&gt;=1.7 -c pytorch -c conda-forge 在 Anaconda 上安装了 pytorch-forecasting。
我已经看到了 pip install setuptools 的解决方案。我在我的终端上成功运行了安装 setuptools 的命令，但它并没有摆脱“没有名为“distutils”的模块”错误。
我需要帮助来绕过“没有名为“distutils”的模块”错误，以便我可以安装 pytorch-forecasting 包。]]></description>
      <guid>https://stackoverflow.com/questions/78757738/unable-to-install-pytorch-forecasting-no-module-named-distutils-error</guid>
      <pubDate>Wed, 17 Jul 2024 05:09:49 GMT</pubDate>
    </item>
    <item>
      <title>如何将多头自注意力输出的形状更改为可以馈送到卷积层的形状？</title>
      <link>https://stackoverflow.com/questions/78757193/how-to-change-shape-of-multi-head-self-attention-output-to-a-shape-that-can-be-f</link>
      <description><![CDATA[我遇到了这样的错误：
MHSA（多头自注意力）的输出如下：
torch.Size([20, 197, 768])


批次大小为 20
序列长度为 197（之前为 196，添加类标记后变为 197）
嵌入维度为 768

我想将其重塑以适应以下格式，以便将其馈送到卷积层：
torch.Size([batch_size, channel, width, height])

我尝试通过使用以下方法添加新维度来实现此目的方法：
torch.unsqueeze(1)
torch.transpose(1, 3)

这成功地允许馈送到卷积层。但是，我不确定这种方法是否正确，如果不正确，请纠正我。
目前，我正在尝试一种不同的方法：
new_size = int(math.sqrt(sequence_length))
torch.transpose(1, 2).view(batch_size, embed_dim, new_size, new_size)

这导致错误，指出形状对于大小为 (some_number) 的输入无效。这是因为序列长度（197）不是完全平方的，得出的是一个十进制数，而视图函数需要输入一个整数，平方运算在转换为整数后得出 16，但 batch_size * 768 * 16 * 16 不等于 batch_size * 197 * 768，导致错误
我的分析正确吗？我该如何解决这个问题？还有没有更好的方法？]]></description>
      <guid>https://stackoverflow.com/questions/78757193/how-to-change-shape-of-multi-head-self-attention-output-to-a-shape-that-can-be-f</guid>
      <pubDate>Wed, 17 Jul 2024 00:24:06 GMT</pubDate>
    </item>
    <item>
      <title>需要什么类型的相机或计算机视觉硬件/软件来跟踪寻找特定运动的人[关闭]</title>
      <link>https://stackoverflow.com/questions/78757027/what-type-of-camera-or-computer-vision-hardware-software-be-needed-to-track-a-hu</link>
      <description><![CDATA[我需要找到一个摄像头和相应的计算机视觉技术，可以跟踪人类直到做出特定动作，在这种情况下它将触发另一个系统事件。例如，这个摄像头应该检测其框架内的人的存在，如果人坐下，它将告诉触发连接到摄像头系统的 LED 以打开。任何想法或指导都值得赞赏。
我研究过像 arduinos Nicla Vision 这样的计算机视觉系统，但并不认为它有能力检测到这种变化。]]></description>
      <guid>https://stackoverflow.com/questions/78757027/what-type-of-camera-or-computer-vision-hardware-software-be-needed-to-track-a-hu</guid>
      <pubDate>Tue, 16 Jul 2024 23:02:38 GMT</pubDate>
    </item>
    <item>
      <title>当已知目标值的特征向量时，如何使用监督式机器学习进行时间序列预测？</title>
      <link>https://stackoverflow.com/questions/78757005/how-to-use-supervised-ml-for-time-series-predictions-when-the-feature-vector-for</link>
      <description><![CDATA[我尝试使用 LSTM 预测仪器的连续“偏移”校准值。这些偏移值之前已被证明与用作特征的一对温度值有很好的相关性。这些偏移值显示出周期性，因此为模型选择了 LSTM。但是，我发现的所有 LSTM 示例都使用来自先前数据点序列的特征向量来预测目标。我觉得这可能无法捕捉序列中每个特征向量与其偏移之间的关系。而且由于在预测中只使用了先前数据点序列的特征向量，因此无法有效地利用该温度偏移关系。
为了解决这个问题，我已经将当前数据点的特征向量添加到用于训练模型的向量序列中，当然，也添加到用于预测目标偏移的序列中。
但是，我如何才能对要预测的偏移的特征向量赋予更大的权重，以使模型能够利用温度偏移关系？
谢谢！ 🖖
创建序列的代码如下所示：
def create_sequences(x, y, time_steps = 24):
xs, ys = [], []
for i in range(len(x) - time_steps - 1):
x_temp = x[i:(i + time_steps + 1)] #在目标值之前创建一个 time_steps 序列
xs.append(x_temp)
ys.append(y[i + time_steps]) #目标值是序列之后的值
return np.array(xs), np.array(ys)
]]></description>
      <guid>https://stackoverflow.com/questions/78757005/how-to-use-supervised-ml-for-time-series-predictions-when-the-feature-vector-for</guid>
      <pubDate>Tue, 16 Jul 2024 22:49:31 GMT</pubDate>
    </item>
    <item>
      <title>词到词机器翻译的评估指标/算法</title>
      <link>https://stackoverflow.com/questions/78756460/evaluation-metric-algorithm-for-word-to-word-machine-translation</link>
      <description><![CDATA[我正在寻找一种科学合理的方法来评估我的学士论文的单词翻译。在寻找的过程中，我发现大多数方法都是基于 n-gram 精度来进行句子评估。我正在将数据库列和表名从英语翻译成德语。我可以选择包含基本事实，但我更喜欢不包含它的方法。我的翻译和原文也包含缩写。
现在我使用 OpenAIs 最新的嵌入模型，然后计算嵌入之间的距离。但结果很大程度上取决于模型。我研究过使用字符 n-gram 的 ChrF。但如上所述，不需要基本事实的方法会更理想。]]></description>
      <guid>https://stackoverflow.com/questions/78756460/evaluation-metric-algorithm-for-word-to-word-machine-translation</guid>
      <pubDate>Tue, 16 Jul 2024 19:42:06 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用 GCP TPU 进行多处理，但 shell 意外死机</title>
      <link>https://stackoverflow.com/questions/78756024/attempting-multiprocessing-with-gcp-tpus-but-the-shell-dies-unexpectedly</link>
      <description><![CDATA[我可以访问美国地区的 32 个可抢占 Cloud TPU v4 芯片，并尝试使用以下 Python 代码运行我的 PyTorch 模型：
import os 
import sys 
import pickle 
import torch_xla.distributed.xla_multiprocessing as xmp 
from transformers 
import AutoTokenizer 
import main_utils as utils 
import multiprocessing as mp

lock = mp.Manager().Lock()

def _mp_fn(i): 

A_tasks, B_tasks, desire_output_lengths, keys = utils.get_total_tasks()

import torch
import torch_xla.core.xla_model as xm
from models_mamba import MambaForCausalLM

DEVICE_NAME = xm.xla_device()
tokenizer = AutoTokenizer.from_pretrained(&quot;~/Mamba-1B/&quot;)
model = MambaForCausalLM.from_pretrained(&quot;~/Mamba-1B/&quot;, torch_dtype=&quot;auto&quot;,
device_map=&quot;auto&quot;, low_cpu_mem_usage=True)
model = model.to_empty(device=&#39;cpu&#39;)
model.apply(lambda module: module.reset_parameters() if hasattr(module, &#39;reset_parameters&#39;) else None)
model = model.to(xm.xla_device())

params_to_save = [&quot;out_proj_y&quot;]

def generate_logits(task, logits_list, desire_output_length):
for i in range(48):
layer_to_save = [i + 1]
input_ids = tokenizer.encode(task, return_tensors=&quot;pt&quot;).to(设备名称)
model.saved_activation(params_to_save, layer_to_save, precision=&#39;r&#39;)
output = model.generate(input_ids, max_length=desired_output_length, no_repeat_ngram_size=2)

saved_dict = model.reset_everything_and_save()
param = saved_dict[f&quot;out_proj_y_{i + 1}&quot;][0, -1, :].to(设备名称)

xm.all_gather(param)
logits = model.get_unembed_for_layer(param, norm=&quot;False&quot;)

xm.all_gather(logits)
logits_list.append(logits.to(&quot;cpu&quot;))

del input_ids、output、saved_dict、param、logits

task_dict = {}
for A_task、B_task、dol、key in zip(A_tasks、B_tasks、desired_output_lengths、keys):
A_logits = []
B_logits = []
generate_logits(A_task、A_logits、dol)
generate_logits(B_task、B_logits、dol)

aux_dict = {&#39;A&#39;: A_logits, &#39;B&#39;: B_logits}
task_dict[key] = aux_dict

device = xm.xla_device()
with lock:
print(f&#39;Process {i}, Device {device}&#39;)

xm.mark_step()
with open(f&#39;~/main_file.pickle&#39;, &#39;wb&#39;) as handle:
pickle.dump(task_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)

if name == &quot;main&quot;: 
xmp.spawn(_mp_fn, args=(), nprocs=8, start_method=&#39;fork&#39;)`

要执行此代码，我使用 Cloud Shell 和以下命令（设置 TPU 并在所有 8 个工作器上安装库之后）：
gcloud compute tpus tpu-vm ssh ${TPU_NAME} --zone=${ZONE} --project=${PROJECT_ID} --worker=all --command=&quot;PJRT_DEVICE=TPU python3 ~test.py&quot;
我希望模型在所有 TPU 上执行 _mp_fn() 中定义的任务，按照指定的方式收集和保存 logit，最后将结果存储在 main_file.pickle 中。但是，在 Cloud Shell 中启动命令后，shell 在模型加载期间打印了一些预期的警告，但没有继续进行。相反，过了一段时间，shell 会话意外终止。我不确定如何排除故障？]]></description>
      <guid>https://stackoverflow.com/questions/78756024/attempting-multiprocessing-with-gcp-tpus-but-the-shell-dies-unexpectedly</guid>
      <pubDate>Tue, 16 Jul 2024 17:38:54 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 模型无法训练</title>
      <link>https://stackoverflow.com/questions/78753201/lstm-model-doesnt-train</link>
      <description><![CDATA[我正在尝试使用深度学习来查找粒子的化学状态。作为输入，我有粒子在 X_train 中随时间的位置，形状为 (num_train,sequence_length)。 （我的序列长度为 100），输出是形状为 (num_train,1) 的 Y_train 中包含的转换帧（介于 1 和 100 之间）。
这是一个序列示例（https://i.sstatic.net/Ddmhjc24.jpg），转换位于第 84 帧。
所有数据都是用非常具体的算法生成的，但是该算法不会生成非常复杂的数据，我认为自己很容易找到转换，但我希望这个深度学习模型能够正常工作。
这是 LSTM 代码：
# 过滤

# 定义 LSTM 模型
model = Sequential([
LSTM(64, input_shape=(sequence_length, 1), return_sequences=False), Dense(64,activation=&#39;relu&#39;), Dense(1) ]) # 模型编译器 model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;) # 回归的均方误差 # 模型摘要 model.summary() # 模型模型嵌入 model.fit( X_train, Y_train, epochs=40, batch_size=32,validation_data=(X_test, Y_test)) # 新预测示例预测 = model.predict(X_test) print(prediction)  结果： 模型：“顺序”
_________________________________________________________________
层（类型）输出形状参数 # 
====================================================================
lstm (LSTM) (无，64) 16896 

密集 (密集) (无，64) 4160 

密集_1 (密集) (无，1) 65 

============================================================================
总参数：21121 (82.50 KB)
可训练参数： 21121 (82.50 KB)
不可训练参数：0 (0.00 字节)
_________________________________________________________________
Epoch 1/10
631/631 [==============================] - 35s 50ms/step - 损失：1043.6710 - val_loss：840.6771
Epoch 2/10
631/631 [==============================] - 30s 48ms/step - 损失：840.9444 - val_loss：839.9596
Epoch 3/10
631/631 [===============================] - 32s 50ms/步 - 损失：841.6289 - val_loss：840.7188
Epoch 4/10
631/631 [=============================] - 30s 48ms/步 - 损失：840.9946 - val_loss：840.6344
Epoch 5/10
631/631 [===============================] - 33s 52ms/步 - 损失：841.8745 - val_loss：839.9298
Epoch 6/10
631/631 [==============================] - 31s 49ms/步 - 损失：841.6499 - val_loss：839.8434
Epoch 7/10
631/631 [=============================] - 31s 49ms/步 - 损失：841.2045 - val_loss：840.0717
Epoch 8/10
631/631 [===============================] - 30s 48ms/步 - 损失：842.0576 - val_loss： 840.2137
纪元 9/10
631/631 [=============================] - 33s 52ms/步 - 损失：842.7056 - val_loss：840.5657
纪元 10/10
631/631 [=============================] - 30s 48ms/步 - 损失：841.5714 - val_loss：839.8404
70/70 [================================] - 2s 16ms/步
[[52.569366]
[52.569286]
[52.569378]
...
[52.569344]
[52.569313]
[52.56937 ]]

如您所见，当我测试训练后的模型时，无论输入是什么，输出都是相同的。 val_loss 不会随着 epoch 的数量而改善。 这就是问题所在，我不明白发生了什么。
我是深度学习的初学者，所以也许我犯了一个非常简单的错误。 但是我仔细检查了我的数据，X_train 已标准化，我尝试在我的模型上添加一些 drop out 和其他层，但没有任何变化。
也许使用 LSTM 无法做到这一点，但我认为数据非常简单。 我真的想尝试找到一种方法来使用深度学习来找到它。 我 d]]></description>
      <guid>https://stackoverflow.com/questions/78753201/lstm-model-doesnt-train</guid>
      <pubDate>Tue, 16 Jul 2024 07:31:40 GMT</pubDate>
    </item>
    <item>
      <title>即使经过数百个时期，pytorch AdamW 的 LR 仍未衰减</title>
      <link>https://stackoverflow.com/questions/78752899/lr-not-decaying-for-pytorch-adamw-even-after-hundreds-of-epochs</link>
      <description><![CDATA[我有以下使用 Pytorch 中的 AdamW 优化器的代码：
optimizer = AdamW(params=self.model.parameters(), lr=0.00005)

我尝试使用 wandb 进行登录，如下所示：
lrs = {f&#39;lr_group_{i}&#39;: param_group[&#39;lr&#39;]
for i, param_group in enumerate(self.optimizer.param_groups)}
wandb.log({&quot;train_loss&quot;: avg_train_loss, &quot;val_loss&quot;: val_loss, **lrs})

请注意 weight_decay 参数的默认值为 0.01（对于 AdamW）。
当我检查 wandb 仪表板时，它显示 AdamW 的 LR 即使在 200 个 epoch 之后也相同，并且根本没有衰减。我尝试了几次。

为什么 LR 衰减没有发生？
此外，它仅显示一个参数组的 LR。为什么会这样？似乎我在这里错过了一些基本的东西。有人可以指出吗？]]></description>
      <guid>https://stackoverflow.com/questions/78752899/lr-not-decaying-for-pytorch-adamw-even-after-hundreds-of-epochs</guid>
      <pubDate>Tue, 16 Jul 2024 06:09:44 GMT</pubDate>
    </item>
    <item>
      <title>如何查看 YOLOv6 中的评估指标？</title>
      <link>https://stackoverflow.com/questions/78680846/how-to-see-evaluation-metrics-in-yolov6</link>
      <description><![CDATA[我有以下输出，但无法弄清楚如何评估，因为没有 F1 分数 或 混淆矩阵。
平均召回率 (AR) @[ IoU=0.50:0.95 | area= small |maxDets=100] = -1.000

平均召回率 (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.250

平均召回率 (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.410

20/499 0.001595 0.6697 0 1.393: 100%|██████████| 12/12 [00:

21/499 0.001594 0.6417 0 1.353: 100%|██████████| 12/12 [00:

22/499 0.001594 0.6727 0 1.431: 100%|██████████| 12/12 [00:

我训练了 400 个 epoch，这只是输出的一小部分。我也看不到 mAP。
我有这行代码要评估
!python tools/eval.py --data Fabric-Defect-2/data.yaml --weights runs/train/exp/weights/best_ckpt.pt --device 0

有没有办法获得详细的评估指标，例如 F1 分数、混淆矩阵 和 mAP？]]></description>
      <guid>https://stackoverflow.com/questions/78680846/how-to-see-evaluation-metrics-in-yolov6</guid>
      <pubDate>Fri, 28 Jun 2024 05:55:12 GMT</pubDate>
    </item>
    <item>
      <title>获取“model.fit”keras API 参数的值</title>
      <link>https://stackoverflow.com/questions/77581428/get-values-of-model-fit-keras-api-parameters</link>
      <description><![CDATA[我正在尝试使用自定义回调函数获取 Kera 顺序模型的详细信息。我需要提取 model.fit() API 中设置的参数的所有值，例如 batch_size、epochs、validation_split 等。但我无法在 Keras 的回调中访问它们。您知道如何自动获取这些值吗？
我使用的是 Python 3.10 和 Keras 2.8。]]></description>
      <guid>https://stackoverflow.com/questions/77581428/get-values-of-model-fit-keras-api-parameters</guid>
      <pubDate>Thu, 30 Nov 2023 20:13:45 GMT</pubDate>
    </item>
    <item>
      <title>如何计算伯努利朴素贝叶斯的联合对数似然</title>
      <link>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</link>
      <description><![CDATA[对于使用 BernoulliNB 的分类问题，如何计算联合对数似然。联合似然由以下公式计算，其中 y(d) 是实际输出（不是预测值）的数组，x(d) 是特征的数据集。
我阅读了这个答案并阅读了文档，但它并没有完全满足我的目的。有人可以帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</guid>
      <pubDate>Wed, 17 Oct 2018 18:08:50 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：模块'_Box2D'没有属性'RAND_LIMIT_swigconstant'</title>
      <link>https://stackoverflow.com/questions/50037674/attributeerror-module-box2d-has-no-attribute-rand-limit-swigconstant</link>
      <description><![CDATA[我正在尝试在强化学习上运行 lunar_lander，但运行时出现错误。
另外我的电脑是osx系统。
这是月球着陆器的代码：
import numpy as np
import gym
import csv

from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from keras.optimizers import Adam

from rl.agents.dqn import DQNAgent
from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy
from rl.memory import SequentialMemory

import io
import sys
import csv

# 路径环境改变以使一切正常工作
# export DYLD_FALLBACK_LIBRARY_PATH=$DYLD_FALLBACK_LIBRARY_PATH:/usr/lib

# 获取环境并提取操作数量。
ENV_NAME = &#39;LunarLander-v2&#39;
env = gym.make(ENV_NAME)
np.random.seed(123)
env.seed(123)
nb_actions = env.action_space.n

# 接下来，我们建立一个非常简单的模型。
model = Sequential()
model.add(Flatten(input_shape=(1,) + env.observation_space.shape))
model.add(Dense(16))
model.add(Activation(&#39;relu&#39;))
model.add(Dense(16))
model.add(Activation(&#39;relu&#39;))
model.add(Dense(16))
model.add(Activation(&#39;relu&#39;))
model.add(Dense(nb_actions))
model.add(Activation(&#39;linear&#39;))
#print(model.summary())

# 最后，我们配置并编译我们的代理。您可以使用每个内置的 Keras 优化器和
# 甚至指标！
memory = SequentialMemory(limit=300000, window_length=1)
policy = EpsGreedyQPolicy()
dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,
target_model_update=1e-2, policy=policy)
dqn.compile(Adam(lr=1e-3), metrics=[&#39;mae&#39;])

# 训练完成后，我们保存最终权重。
dqn.load_weights(&#39;dqn_{}_weights.h5f&#39;.format(ENV_NAME))

# 重定向 stdout 以捕获测试结果
old_stdout = sys.stdout
sys.stdout = mystdout = io.StringIO()

# 评估我们的算法的几个情节。
dqn.test(env, nb_episodes=200, visualize=False)

# 重置 stdout
sys.stdout = old_stdout

results_text = mystdout.getvalue()

# 打印结果文本
print(&quot;results&quot;)
print(results_text)

# 从结果中提取奖励列表
total_rewards = list()
for idx, line in enumerate(results_text.split(&#39;\n&#39;)):
if idx &gt; 0 and len(line) &gt; 1：
reward = float(line.split(&#39;:&#39;)[2].split(&#39;,&#39;)[0].strip())
total_rewards.append(reward)

# 打印奖励和平均值
print(&quot;total rewards&quot;, total_rewards)
print(&quot;average total reward&quot;, np.mean(total_rewards))

# 将总奖励写入文件
f = open(&quot;lunarlander_rl_rewards.csv&quot;,&#39;w&#39;)
wr = csv.writer(f)
for r in total_rewards:
wr.writerow([r,])
f.close()

错误如下：
回溯（最近一次调用最后一次）：
文件“/s/user/Document/Semester2/Advanced Machine Learning/Lab/Lab6/lunar_lander_ml_states_player.py”，第 23 行，在 &lt;module&gt; 中
env = gym.make(ENV_NAME)
文件“/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/gym/envs/registration.py”，第 167 行，在 make 中
return registry.make(id)
文件“/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/gym/envs/registration.py”，第 119 行，在 make 中
env = spec.make()
文件“/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/gym/envs/registration.py”，第 85 行，在 make 中
cls = load(self._entry_point)
文件“/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/gym/envs/registration.py”，第 14 行，加载中
result = entry_point.load(False)
文件“/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/pkg_resources/__init__.py”，第 2405 行，加载中
return self.resolve()
文件“/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/pkg_resources/__init__.py”，第 2411 行，解析中
module = __import__(self.module_name, fromlist=[&#39;__name__&#39;], level=0)
文件“/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/gym/envs/box2d/__init__.py”，第 1 行，&lt;module&gt;
从 gym.envs.box2d.lunar_lander 导入 LunarLander
文件“/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/gym/envs/box2d/lunar_lander.py”，第 4 行，位于 &lt;module&gt;
导入 Box2D
文件“/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/Box2D/__init__.py”，第 20 行，位于 &lt;module&gt;
从 .Box2D 导入 *
文件“/s/user/anaconda/envs/untitled/lib/python3.6/site-packages/Box2D/Box2D.py”，第 435 行，位于 &lt;module&gt;
_Box2D.RAND_LIMIT_swigconstant(_Box2D)
AttributeError: 模块 &#39;_Box2D&#39; 没有属性 &#39;RAND_LIMIT_swigconstant&#39;

我尝试按照 https://github.com/pybox2d/pybox2d/blob/master/INSTALL.md 的指南重新安装 Box2d
但仍然不起作用，有人能帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/50037674/attributeerror-module-box2d-has-no-attribute-rand-limit-swigconstant</guid>
      <pubDate>Thu, 26 Apr 2018 07:55:13 GMT</pubDate>
    </item>
    </channel>
</rss>