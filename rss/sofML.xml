<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 02 Sep 2024 21:14:50 GMT</lastBuildDate>
    <item>
      <title>在 Google colab 中克隆 GitHub 代码并解决库和依赖项版本不匹配错误</title>
      <link>https://stackoverflow.com/questions/78941693/cloning-github-code-in-google-colab-and-solving-libraries-and-dependency-version</link>
      <description><![CDATA[由于本地机器资源有限，我想使用 GitHub 中的代码在 Google colab 中使用 ML 进行面部表情识别。一个问题是 Google colab 的环境与其他环境不同，后者的代码结构很容易理解，比如 vs code。另一个问题是由于代码版本太旧而导致的库版本和依赖项错误。
以下是 GitHub 链接：
https://github.com/Talented-Q/POSTER_V2
我尝试在我的 Google colab 中从 GitHub 克隆代码，但执行文件非常复杂，因为文件太多，Google colab 环境与我们在本地（如 vs code）中使用的环境完全不同。而且我还收到了很多与库版本和依赖项相关的错误，解决起来非常有挑战性。]]></description>
      <guid>https://stackoverflow.com/questions/78941693/cloning-github-code-in-google-colab-and-solving-libraries-and-dependency-version</guid>
      <pubDate>Mon, 02 Sep 2024 18:29:56 GMT</pubDate>
    </item>
    <item>
      <title>RandomForest 模型未按预期工作</title>
      <link>https://stackoverflow.com/questions/78941615/randomforest-model-not-working-as-expected</link>
      <description><![CDATA[我想尝试预测股票价格。我知道股市基本上是一个随机过程，因此我不期望任何正回报。我编写了一个小脚本，使用随机森林回归器，该回归器已根据过去 20 年左右的 AAPL 股票数据（过去 100 天除外）进行训练。我使用过去 100 天作为验证。
根据开盘价/收盘价/最高价/最低价和交易量，我在数据框中创建了另外两列：收盘价的涨跌百分比和 days_since_start_column，因为如果我的判断正确，模型无法根据日期时间进行学习。
无论如何，这是代码的其余部分：
df = pd.read_csv(&#39;stock_data.csv&#39;)
df = df[::-1].reset_index()
df[&#39;timestamp&#39;] = pd.to_datetime(df[&#39;timestamp&#39;])
df[&#39;% Difference&#39;] = df[&#39;close&#39;].pct_change()

splits = [
{&#39;date&#39;: &#39;2020-08-31&#39;, &#39;ratio&#39;: 4},
{&#39;date&#39;: &#39;2014-06-09&#39;, &#39;ratio&#39;: 7},
{&#39;date&#39;: &#39;2005-02-28&#39;, &#39;ratio&#39;: 2},
{&#39;date&#39;: &#39;2000-06-21&#39;, &#39;ratio&#39;: 2}
]

用于 split 中的 split:
split[&#39;date&#39;] = pd.to_datetime(split[&#39;date&#39;])
split_date = split[&#39;date&#39;]
ratio = split[&#39;ratio&#39;]
df.loc[df[&#39;timestamp&#39;] &lt; split_date, &#39;close&#39;] /= 比率

df[&#39;days_since_start&#39;] = (df[&#39;timestamp&#39;] - df[&#39;timestamp&#39;].min()).dt.days
#data = r.json()
target = df.close
features = [&#39;days_since_start&#39;,&#39;open&#39;,&#39;high&#39;,&#39;low&#39;,&#39;volume&#39;]

X_train = (df[features][:-100])
X_validation = df[features][-100:]

y_train = df[&#39;close&#39;][:-100]
y_validation = df[&#39;close&#39;][-100:]

#X_train,X_validation,y_train,y_validation = train_test_split(df[features][:-100],target[:-100],random_state=0)

model = RandomForestRegressor()
model.fit(X_train,y_train)
predictions = model.predict(X_validation)

predictions_df = pd.DataFrame(columns=[&#39;days_since_start&#39;,&#39;close&#39;])
predictions_df[&#39;close&#39;] = predictions
predictions_df[&#39;days_since_start&#39;] = df[&#39;timestamp&#39;][-100:].values
plt.xlabel(&#39;日期&#39;)
#plt.scatter(df.loc[X_validation.index, &#39;timestamp&#39;], predictions, color=&#39;red&#39;, label=&#39;预测收盘价&#39;, alpha=0.6)
plt.plot(df.timestamp[:-100],df.close[:-100],color=&#39;black&#39;)
plt.plot(df.timestamp[-100:],df.close[-100:],color=&#39;green&#39;)
plt.plot(predictions_df.days_since_start,predictions_df.close,color=&#39;red&#39;)
plt.show()

我用黑色绘制了过去几年截至最近 100 天的收盘价，用绿色绘制了最近 100 天的收盘价，用红色绘制了最近 100 天的预测收盘价。这是结果（过去 100 天）：

为什么价格大幅上涨后模型保持平稳？我在训练过程中做错了什么吗？我的验证数据集太小了吗？还是这只是超参数调整的问题？]]></description>
      <guid>https://stackoverflow.com/questions/78941615/randomforest-model-not-working-as-expected</guid>
      <pubDate>Mon, 02 Sep 2024 17:53:54 GMT</pubDate>
    </item>
    <item>
      <title>huggingface_hub/file_download.py 收到 urllib3 ConnectTimeoutError</title>
      <link>https://stackoverflow.com/questions/78941230/huggingface-hub-file-download-py-receives-urllib3-connecttimeouterror</link>
      <description><![CDATA[尝试复制代码：https://github.com/ximinng/DiffSketcher.Do 所有环境配置操作均成功完成，但在尝试运行代码启动模型时出现错误：在此处输入图像描述、在此处输入图像描述、在此处输入图像描述。如何处理？请帮忙！！！！！！我尝试用 huggingface 上发布的其他相同模型替换模型（runwayml/stable-diffusion-v1-5-ov）
替代模型：在此处输入图片说明。
但我只能更改代码中的这一行，无法解决。我不知道如何直接替换它：在此处输入图片说明]]></description>
      <guid>https://stackoverflow.com/questions/78941230/huggingface-hub-file-download-py-receives-urllib3-connecttimeouterror</guid>
      <pubDate>Mon, 02 Sep 2024 15:43:01 GMT</pubDate>
    </item>
    <item>
      <title>异常检测：如何找出数据集最后一天的800个混合样本？</title>
      <link>https://stackoverflow.com/questions/78940634/anomaly-detection-how-to-find-out-the-800-mixed-sample-on-the-last-day-of-the-d</link>
      <description><![CDATA[有 8000 名患者。 4 年内共进行了 22666 组测试。
每位患者至少进行了两组在不同日期进行的测试。
每组测试有 11 个不同的实验室
（实验室：白蛋白、ALP、ALT、AST、BUN、肌酐、GGT、葡萄糖、LDH、总胆红素、总蛋白）
每 11 个测试彼此独立，参考范围各不相同。
医院 A 的患者临时涌入导致最后一天数据集中的标本溢出（4000 个测试集）
计算机错误导致 4000 个测试集中的 800 多个测试集的结果在患者高峰当天被逆转。
如何找到被逆转的 800 个标本？
我尝试使用非混合日进行训练（不包括最后一天的数据）使用 xgboost，预测最后一天测试的 11 个测试结果。我试图找出实际值和预测值之间的差异，以找出具有多个异常值的样本。但 F1 分数很差。我还尝试使用 min-max 或 z-score 进行规范化，但仍然没有改善。我想我必须研究不同的方法。有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78940634/anomaly-detection-how-to-find-out-the-800-mixed-sample-on-the-last-day-of-the-d</guid>
      <pubDate>Mon, 02 Sep 2024 13:10:44 GMT</pubDate>
    </item>
    <item>
      <title>并行化原生单批次 PyTorch 模型</title>
      <link>https://stackoverflow.com/questions/78940523/paralellizing-a-natively-single-batch-pytorch-model</link>
      <description><![CDATA[是否可以并行化（原生）单批模型？
通常，并行化是通过 torch.bmm（批处理矩阵乘法）而不是 torch.matmul 来完成的，并且专门为批处理固定一个维度。但是，例如对于 torch.tensordot 函数，这不可用。
因此，如果有这样的模型，是否可以并行计算批处理的每个梯度？理想情况下，并行化应该适用于训练和推理。
代码示例：
import torch
import torch.nn as nn

class LinearMultidimModel(nn.Module):
def __init__(self, input_dim, output_dim):
super(LinearMultidimModel, self).__init__()
self.weight = nn.Parameter(torch.randn(input_dim, hidden_​​dim, output_dim))
self.bias = nn.Parameter(torch.randn(output_dim))

def forward(self, x):
# 使用 torch.tensordot 执行线性变换
out = torch.tensordot(x, self.weight, dims=[[0,1],[0,1]]) + self.bias
return out

# 示例用法
input_dim = 3
hidden_​​dim=2
output_dim = 1
model = LinearMultidimModel(input_dim, output_dim)

# 虚拟输入
x = torch.randn(input_dim, hidden_​​dim)# 但是如果我想放入一个批次，torch.randn(batch_size, input_dim, hidden_​​dim) 怎么办？
output = model(x)
print(output)

请记住，如果没有 hidden_​​dim，它会原生地进行并行化，可以完全删除 hidden_​​dim 并使用 获得结果
x = torch.randn(5, input_dim)。
我尝试过使用 Einsum，但它适用于固定数量的隐藏维度...]]></description>
      <guid>https://stackoverflow.com/questions/78940523/paralellizing-a-natively-single-batch-pytorch-model</guid>
      <pubDate>Mon, 02 Sep 2024 12:42:44 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Vast.Ai 托管机器学习 Flask 应用程序[关闭]</title>
      <link>https://stackoverflow.com/questions/78940345/how-to-host-mochine-learning-flask-application-with-vast-ai</link>
      <description><![CDATA[我想托管我的 Flask 应用程序，该应用程序包含其推理在 gpu 上进行的模型，这就是我决定使用 vast.ai 的原因，它提供便宜且优质的 gpu，但我不知道如何为我的模型托管设置 vast.ai
请问您能否提供一些关于如何在 vast.ai 上托管我的项目的教程或指南，它与其他实时服务器有何不同？]]></description>
      <guid>https://stackoverflow.com/questions/78940345/how-to-host-mochine-learning-flask-application-with-vast-ai</guid>
      <pubDate>Mon, 02 Sep 2024 11:58:30 GMT</pubDate>
    </item>
    <item>
      <title>我无法训练 vit_base_patch16_224 模型来为屏幕截图创建高质量的嵌入</title>
      <link>https://stackoverflow.com/questions/78939966/im-failing-to-train-a-vit-base-patch16-224-model-for-creating-high-quality-embe</link>
      <description><![CDATA[我创建了一个使用 vit_base_patch16_224 作为编码器的自动编码器。我使用大约 100_000 张屏幕截图进行了几个轮次的训练，但解码器似乎根本没有学会解码。损失似乎在减少。也许你们中的一些人对我可能做错的事情有一些有用的建议。
这是自动编码器类：
class ViTAutoEnc(nn.Module):
def __init__(self, vit_model=&#39;vit_base_patch16_224&#39;):
super(ViTAutoEnc, self).__init__()

# 使用预先训练的 Vision Transformer 作为编码器
self.encoder = create_model(vit_model, pretrained=True)
self.encoder.head = nn.Identity()
# 从 ViT 模型中提取特征维度
self.vit_embedding_dim = self.encoder.embed_dim

self.decoder = nn.Sequential(
nn.Linear(768, 768*7*7),
nn.Unflatten(1, torch.Size([768, 7, 7])),
nn.ConvTranspose2d(768, 512, kernel_size=4, stride=2, padding=1), # 7x7 -&gt; 14x14
nn.ReLU(inplace=True),
nn.Conv2d(512, 512, kernel_size=3, padding=1),
nn.ReLU(inplace=True),

nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1), # 14x14 -&gt; 28x28
nn.ReLU(inplace=True),
nn.Conv2d(256, 256, kernel_size=3, padding=1),
nn.ReLU(inplace=True),

nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # 28x28 -&gt; 56x56
nn.ReLU(inplace=True),
nn.Conv2d(128, 128, kernel_size=3, padding=1),
nn.ReLU(inplace=True),

nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # 56x56 -&gt; 112x112
nn.ReLU(inplace=True),
nn.Conv2d(64, 64, kernel_size=3, padding=1),
nn.ReLU(inplace=True),

nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), # 112x112 -&gt; 224x224
nn.ReLU(inplace=True),
nn.Conv2d(32, 32, kernel_size=3, padding=1),
nn.ReLU(inplace=True),

nn.Conv2d(32, 3, kernel_size=1), # 调整通道
nn.Sigmoid() # 将输出标准化为 [0, 1] 
)

count = sum(p.numel() for p in self.decoder.parameters() if p.requires_grad)
print(f&#39;解码器有 {count/10**6:.2f} 百万个可训练参数&#39;)

count = sum(p.numel() for p in self.encoder.parameters() if p.requires_grad)
print(f&#39;编码器有 {count/10**6:.2f} 百万个可训练参数&#39;)

def forward(self, x, decrypt=True):
# 编码器通过ViT

x = self.encoder.forward_features(x).mean(dim=1) # 从 ViT 获取特征 (batch_size, vit_embedding_dim)
x = F.normalize(x, p=2, dim=1) # 沿特征维度进行归一化

# 解码器传递
if decrypt:
x = self.decoder(x) # 解码
return x

这是项目 git。
损失看起来像这样

编码器有 8580 万个参数，解码器有 4116 万个参数。我使用的是 Adam 优化器 lr=0.00001。批量大小为 64。]]></description>
      <guid>https://stackoverflow.com/questions/78939966/im-failing-to-train-a-vit-base-patch16-224-model-for-creating-high-quality-embe</guid>
      <pubDate>Mon, 02 Sep 2024 10:21:45 GMT</pubDate>
    </item>
    <item>
      <title>水印灰度图像 PSNR 较好，但水印提取 BER 较高</title>
      <link>https://stackoverflow.com/questions/78939872/high-ber-in-watermark-extraction-despite-good-psnr-in-watermarking-grayscale-ima</link>
      <description><![CDATA[我正在开展一个为灰度图像添加水印的项目，尽管重建图像的峰值信噪比 (PSNR) 很高，但提取的水印中还是出现了高误码率 (BER) 的问题。下面是我使用的详细过程：

输入准备：


我从 128x128 灰度图像开始，我将其称为“原始”图像。

我将这些图像重塑为 126x128，然后将它们与 16x16 二进制水印图像（其中像素值为 0 或 1）连接起来。

此连接产生 128x128 的“连接”作为我的模型输入的图像。



模型架构：


第一个 CNN：该网络将 128x128 连接图像作为输入，并输出 128x128 灰度图像。目标是使此输出图像尽可能接近原始 128x128 灰度图像。

第一个 CNN 的损失函数：我使用 PSNR 的倒数（1/PSNR（原始，输出））作为此 CNN 的损失。 PSNR 通常在 40 dB 以上，这表明重建效果良好。

第二个 CNN：第二个网络获取第一个 CNN 的输出并尝试提取 16x16 水印图像。

第二个 CNN 的损失函数：我使用原始水印和提取的水印之间的误码率 (BER) 作为此 CNN 的损失。不幸的是，BER 仍然非常高，大约为 50%。


问题：
虽然重建图像的 PSNR 很好，但高 BER 导致总损失约为 50%，这对我的应用来说是不理想的。
这是损失计算的代码：
def psnr_loss(output, target):
mse_loss = nn.MSELoss()(output, target) # 均方误差
if mse_loss == 0:
return torch.tensor(float(&#39;inf&#39;)).to(output.device) # 处理零除的情况
L = 255.0 # 图像中的最大像素值
psnr_value = 10 * torch.log10(L**2 / mse_loss)
return psnr_value # 返回 PSNR（单位为 dB）

def ber_loss(output, target, Threshold=0.5):
# 对输出进行阈值处理，将其转换为二进制（0 或 1）
output_binary = (output &gt; Threshold).float()

# 计算误码数
bit_errors = torch.sum(torch.abs(output_binary - target))

# 计算总位数
total_bits = target.numel()

# 将 BER 计算为误码与总位数的比率
ber = bit_errors / total_bits

return ber

# 组合损失类
class CombinedLoss(nn.Module):
def __init__(self, ber_weight=1.0):
super(CombinedLoss, self).__init__()
self.ber_weight = ber_weight

def forward(self, watermark_output, watermark_target, image_output, image_target):
# 计算逆 PSNR 以最小化
psnr = 1 / psnr_loss(image_output, image_target)

# 计算 BER（误码率）
ber = ber_loss(watermark_output, watermark_target)

# 返回组合损失
return psnr + self.ber_weight * ber

这里我还附上了训练的输出。
带有损失、PSNR 和 BER 的模型输出]]></description>
      <guid>https://stackoverflow.com/questions/78939872/high-ber-in-watermark-extraction-despite-good-psnr-in-watermarking-grayscale-ima</guid>
      <pubDate>Mon, 02 Sep 2024 09:54:18 GMT</pubDate>
    </item>
    <item>
      <title>使用集成 Intel(R) UHD Graphics 620 (iGPU) 时，OpenVINO 推理运行速度比 CPU 慢得多</title>
      <link>https://stackoverflow.com/questions/78939115/openvino-inference-is-running-much-slower-than-cpu-when-using-integrated-intelr</link>
      <description><![CDATA[Python : 3.10.0
Windows : 10
openvino : 2024.3.0
可用设备：
CPU
不可变属性：
可用设备：“”
RANGE_FOR_ASYNC_INFER_REQUESTS：1 1 1
RANGE_FOR_STREAMS：1 8
EXECUTION_DEVICES：CPU
FULL_DEVICE_NAME：Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz
OPTIMIZATION_CAPABILITIES：FP32 FP16 INT8 BIN EXPORT_IMPORT
DEVICE_TYPE：集成
DEVICE_ARCHITECTURE : intel64
可变属性：
NUM_STREAMS : 1
AFFINITY : NONE
INFERENCE_NUM_THREADS : 0
PERF_COUNT : NO
INFERENCE_PRECISION_HINT : f32
PERFORMANCE_HINT : LATENCY
EXECUTION_MODE_HINT : PERFORMANCE
PERFORMANCE_HINT_NUM_REQUESTS : 0
ENABLE_CPU_PINNING : YES
SCHEDULING_CORE_TYPE : ANY_CORE
MODEL_DISTRIBUTION_POLICY : &quot;&quot;
ENABLE_HYPER_THREADING : 是
DEVICE_ID : &quot;&quot;
CPU_DENORMALS_OPTIMIZATION : 否
LOG_LEVEL : LOG_NONE
CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE : 1
DYNAMIC_QUANTIZATION_GROUP_SIZE : 0
KV_CACHE_PRECISION : f16
GPU
不可变属性:
AVAILABLE_DEVICES : 0
RANGE_FOR_ASYNC_INFER_REQUESTS : 1 2 1
RANGE_FOR_STREAMS : 1 2
OPTIMAL_BATCH_SIZE : 1
MAX_BATCH_SIZE : 1
DEVICE_ARCHITECTURE : GPU: vendor=0x8086 arch=v9.0.0
FULL_DEVICE_NAME : Intel(R) UHD Graphics 620 (iGPU)
DEVICE_UUID : 00000000000000000000000000000000
DEVICE_LUID : 0000000000000000
DEVICE_TYPE : 集成
DEVICE_GOPS : {f16:844.8,f32:422.4,i8:422.4,u8:422.4} /&gt;
OPTIMIZATION_CAPABILITIES : FP32 BIN FP16 EXPORT_IMPORT
GPU_DEVICE_TOTAL_MEM_SIZE : 3379195904
GPU_UARCH_VERSION : 9.0.0
GPU_EXECUTION_UNITS_COUNT : 24
GPU_MEMORY_STATISTICS : &quot;&quot;
可变属性：
PERF_COUNT : 否
MODEL_PRIORITY : 中等
GPU_HOST_TASK_PRIORITY : 中等
GPU_QUEUE_PRIORITY : 中等
GPU_QUEUE_THROTTLE : 中等
GPU_ENABLE_LOOP_UNROLLING : 是
GPU_DISABLE_WINOGRAD_CONVOLUTION : 否
CACHE_DIR : &quot;&quot;
CACHE_MODE : optimize_speed
PERFORMANCE_HINT : LATENCY
EXECUTION_MODE_HINT : PERFORMANCE
COMPILATION_NUM_THREADS : 8
NUM_STREAMS : 1
PERFORMANCE_HINT_NUM_REQUESTS : 0
INFERENCE_PRECISION_HINT : f16
ENABLE_CPU_PINNING : 否
DEVICE_ID : 0
IR 模型基于 CodeFormer并压缩为 fp16。
配置
import openvino as ov
import openvino.properties.hint as hints
core = ov.Core()
# 性能情况
device_property = {
&quot;GPU&quot;: {
hints.execution_mode: hints.ExecutionMode.PERFORMANCE,
hints.performance_mode : hints.PerformanceMode.LATENCY,
hints.inference_precision: ov.Type.f16,
},
&quot;CPU&quot;: {
hints.execution_mode: hints.ExecutionMode.PERFORMANCE,
hints.performance_mode : hints.PerformanceMode.LATENCY,
hints.inference_precision: ov.Type.f32,
}
}

core.set_property(&quot;HETERO&quot;, {&quot;MULTI_DEVICE_PRIORITIES&quot;: &quot;GPU,CPU&quot;})
core.set_property(&quot;GPU&quot;, device_property[&quot;GPU&quot;])
core.set_property(&quot;CPU&quot;, device_property[&quot;CPU&quot;])

使用以下方式运行推理时
compiled_model = core.compile_model(model=model, device_name=&quot;CPU&quot;)

耗时 4.196 秒。并且
compiled_model = core.compile_model(model=model, device_name=&quot;GPU&quot;)

耗时 22.595 秒。
是因为配置错误还是集成 GPU 的限制？
还有其他提高性能的建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78939115/openvino-inference-is-running-much-slower-than-cpu-when-using-integrated-intelr</guid>
      <pubDate>Mon, 02 Sep 2024 06:09:19 GMT</pubDate>
    </item>
    <item>
      <title>如果模型在训练集上表现正常但在验证集上表现异常，这意味着什么[关闭]</title>
      <link>https://stackoverflow.com/questions/78938961/what-does-it-mean-if-a-model-acts-normal-on-a-training-set-but-is-abnormal-on-va</link>
      <description><![CDATA[我尝试将堆叠在一起的 25x25 像素图像分类为 50x25 像素图像是相同 (1) 还是不同 (0)。我使用 keras 创建 NN 层。 Keras 顺序层如下所示：
layers.Input((2*imsize,imsize,3)), # 具有 3 个通道的输入形状
layers.Reshape((2,imsize,imsize,3)), # 将输入转换为两个 25x25 图像
layers.LayerNormalization(axis=[-1,-2,-3]), # 规范化图像
layers.Flatten(), # 展平数组
layers.Dense(16,activation=&#39;relu&#39;), # 16 个输出隐藏层
layers.Dense(2,activation=&#39;softmax&#39;) 

然后我使用 adam 优化器编译了这些层，损失和准确率如下：
ml.compile(optimizer=&#39;adam&#39;,
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
metrics=[&#39;accuracy&#39;])

之后，我使用 epoch=20 和 batch_size=100 训练模型。我根据 epoch 绘制了这些结果。
结果


当前评估：
我目前的观察是

模型过度拟合，因为它仅在训练集上表现正常？
模型是学习到了错误的东西，因为损失在验证集上不减反增

我的问题是：我对模型的评估正确吗？我应该如何理解这个结果以便改进它？
更新：
相同（1）的示例数据集：

不同（0）的示例数据集：
]]></description>
      <guid>https://stackoverflow.com/questions/78938961/what-does-it-mean-if-a-model-acts-normal-on-a-training-set-but-is-abnormal-on-va</guid>
      <pubDate>Mon, 02 Sep 2024 04:51:20 GMT</pubDate>
    </item>
    <item>
      <title>自定义链接矩阵、树状图标签不正确</title>
      <link>https://stackoverflow.com/questions/78938314/custom-linkage-matrix-dendrogram-labels-arent-correct</link>
      <description><![CDATA[我创建了自己的链接函数，它生成具有正确 scipy 格式（n-1x4）的链接矩阵。
当我尝试将树状图函数与用于链接的标签一起使用时，树状图显示在迭代中某些序列被合并，但链接矩阵显示其他序列在同一迭代中被合并。
添加图像：
树状图序列 ID
这些是实际合并的序列：“步骤 1：合并 80c8714e-1899-407e-960d-5391bfba9a75 和 b4a3e9f4-aae9-41d4-930a-9d2422e1b37f”
如您所见，这些是不同的序列。
我想问一下我是否遗漏了什么有一种方法可以使用链接矩阵来创建序列标签，因为它们是正确的。
（此外，树状图显示正确的输出，只有标签不正确）
这是树状图的代码
sequence_string_data = [code_from_before]
linkage_matrix = clustering(sequence_string_data)
seq_ids = list(sequences.keys())
seq_labels = [f&quot;{seq_id} (size={len(sequences[seq_id])})&quot; for seq_id in seq_ids]

plt.figure(figsize=(15, 10))
dendro = dendrogram(linkage_matrix, labels=seq_labels, leaf_rotation=90, leaf_font_size=8)

我尝试更改这两个函数以查看是否还有其他问题，但无济于事。]]></description>
      <guid>https://stackoverflow.com/questions/78938314/custom-linkage-matrix-dendrogram-labels-arent-correct</guid>
      <pubDate>Sun, 01 Sep 2024 20:25:26 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch `DataSet.__getitem__()` 调用时 `index` 大于 `__len__()`</title>
      <link>https://stackoverflow.com/questions/78937759/pytorch-dataset-getitem-called-with-index-bigger-than-len</link>
      <description><![CDATA[我有以下 torch 数据集（我已经用随机数生成替换了从文件中读取数据的实际代码，以使其可重现性最小）：
from torch.utils.data import Dataset
import torch 

class TempDataset(Dataset):
def __init__(self, window_size=200):

self.window = window_size

self.x = torch.randn(4340, 10, dtype=torch.float32) # None
self.y = torch.randn(4340, 3, dtype=torch.float32) 

self.len = len(self.x) - self.window + 1 # = 4340 - 200 + 1 = 4141 
# 因此，最后一个窗口起始索引 = 4140 
# 最后一个窗口的范围从 4140 到 4339，即总共 200 个元素

def __len__(self):
return self.len

def __getitem__(self, index):

# AFAIU，下面的 if 条件永远不应计算为 True，因为最后一个索引
# __getitem__ 调用应该是 self.len - 1
if index == self.len: 
print(&#39;self.__len__(): &#39;, self.__len__())
print(&#39;Tried to access eleemnt @ index: &#39;, index)

return self.x[index: index + self.window], self.y[index + self.window - 1]

ds = TempDataset(window_size=200)
print(&#39;len: &#39;, len(ds))
counter = 0 # 尚未读取任何记录
for x, y in ds:
counter += 1 # 上面的行从数据集中读取了另一条记录
print(&#39;counter: &#39;,计数器)

它打印：
len: 4141
self.__len__(): 4141
尝试访问元素 @ index: 4141
counter: 4141

据我所知，__getitem__() 被调用，index 范围从 0 到 __len__()-1。如果这是正确的，那么为什么它试图用索引 4141 调用 __getitem__()，而数据本身的长度是 4141？
我注意到的另一件事是，尽管使用 index = 4141 进行调用，但它似乎没有返回任何元素，这就是为什么 counter 停留在 4141
我的眼睛（或大脑）在这里错过了什么？
PS：虽然它不会有任何效果，只是为了确认，我还尝试用 torch DataLoader 包装 DataSet，它仍然表现相同。]]></description>
      <guid>https://stackoverflow.com/questions/78937759/pytorch-dataset-getitem-called-with-index-bigger-than-len</guid>
      <pubDate>Sun, 01 Sep 2024 15:52:18 GMT</pubDate>
    </item>
    <item>
      <title>数据分类不适用于 BERT 模型</title>
      <link>https://stackoverflow.com/questions/78936387/data-classification-doesnt-work-with-bert-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78936387/data-classification-doesnt-work-with-bert-model</guid>
      <pubDate>Sun, 01 Sep 2024 00:59:26 GMT</pubDate>
    </item>
    <item>
      <title>在 colab 上安装 nvstrings</title>
      <link>https://stackoverflow.com/questions/78933173/installing-nvstrings-on-colab</link>
      <description><![CDATA[我在 Google colab 上有这样的代码：
%%capture
!pip install cupy-cuda122
!pip install -U spacy[cuda122] # 安装支持 CUDA 的 spaCy
!pip install -U spacy-transformers # 安装 spaCy transformers
!git clone https://github.com/rapidsai/rapidsai-csp-utils.git
!python rapidsai-csp-utils/colab/pip-install.py
!python -m spacy download en_core_web_trf

import spacy
spacy.prefer_gpu()
model_nlp=spacy.load(&quot;en_core_web_trf&quot;)
import cudf
resume_dict={&#39;resume&#39;:[&#39;hello&#39;,&#39;how&#39;,&#39;are&#39;,&#39;you&#39;]}
df=cudf.DataFrame(resume_dict)

op = model_nlp(df.resume[0])

这会出现错误
RuntimeError: 预期所有张量都在同一设备上，但发​​现至少有两个设备，cuda:0 和 cpu！ （在方法 wrapper_CUDA__index_select 中检查参数索引的参数时）

要将数据框文本发送到 gpu 进行处理，我正在考虑使用 nvstrings.to_device() 方法。
我正在尝试在我的 Google Colab 笔记本上运行 !pip install nvstrings，但遇到了错误
收集 nvstrings
使用缓存的 nvstrings-0.6.1.post1.tar.gz (1.2 kB)
准备元数据 (setup.py) ... 完成
为收集的包构建 wheel：nvstrings
错误：subprocess-exited-with-error

× python setup.py bdist_wheel 未成功运行。
│ 退出代码：1
╰─&gt; 请参阅上面的输出。

注意：此错误源自子进程，可能不是 pip 的问题。
为 nvstrings 构建 wheel (setup.py) ... 错误
错误：为 nvstrings 构建 wheel 失败
为 nvstrings 运行 setup.py clean
无法构建 nvstrings
错误：错误：无法为某些基于 pyproject.toml 的项目 (nvstrings) 构建可安装的 wheel

如何克服这个问题？
我需要转换器模型解析实际数据框中的每一列，从而仅提供所需的特征。]]></description>
      <guid>https://stackoverflow.com/questions/78933173/installing-nvstrings-on-colab</guid>
      <pubDate>Fri, 30 Aug 2024 17:30:48 GMT</pubDate>
    </item>
    <item>
      <title>Yolov9 C++ 推理输出的 x、y、宽度和高度不符合预期</title>
      <link>https://stackoverflow.com/questions/78923447/yolov9-c-inference-outputs-for-x-y-width-and-height-not-as-expected</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78923447/yolov9-c-inference-outputs-for-x-y-width-and-height-not-as-expected</guid>
      <pubDate>Wed, 28 Aug 2024 12:54:32 GMT</pubDate>
    </item>
    </channel>
</rss>