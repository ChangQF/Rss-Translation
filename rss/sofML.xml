<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 06 May 2024 18:21:14 GMT</lastBuildDate>
    <item>
      <title>PyTorch 中 2D 输入的集成梯度实现</title>
      <link>https://stackoverflow.com/questions/78438413/integrated-gradients-implementation-for-2d-input-in-pytorch</link>
      <description><![CDATA[我正在尝试为 GNN 实现积分梯度计算（在文章中描述）我正在与.具体来说，我使用论文中的Eq3
我的网络输入是一个 NxM 矩阵，表示 N 个顶点的图，每个顶点都有一个 M 维的特征向量。我对论文中的方法的范围感到非常困惑，考虑到他们是针对 N 维的输入推导出来的，在我的例子中，我想为每个节点获得一个分数。
在我当前的实现中，我获得了一个 NxM 矩阵，通过执行 torch.sum(dim=1) 来折叠该矩阵，但这感觉并不那么干净
这是我当前在 PyTroch 中的实现
graph_copy = input_graph.detach().clone()
# 公式中的k/m
k_m = torch.linspace(0, 1, n_steps)
# 存储样本的数组
input_features = [baseline.clone()] # 第一个样本是基线
# x-x&#39;
diff = 原始输入特征 - 基线

# 填充样本数组
对于范围内的 i(1, k_m.shape[0])：
    temp = 基线 + k_m[i] * diff
    input_features_path.append（临时）

梯度= []
对于范围内的 i(k_m.shape[0])：
    ### 将每个输入的向量梯度归零
    input_features[i].requires_grad = True
    model.zero_grad()
    任务.zero_grad()
    
    temp_model_out = 模型（图 = graph_copy，输入 = input_features[i]）[&#39;graph_feature&#39;][0]
    temp_mlp_output = 任务.mlp(temp_model_out)
    temp_prob = Softmax(temp_mlp_output, 暗淡 = 0)

    temp_gradient = grad(输出 = temp_prob[true_label_id], 输入 = input_features_path[i])
    梯度.append(temp_gradient[0])

    input_features_path[i].requires_grad = False

使用 torch.no_grad()：
    ig_scores = original_input_feature * torch.stack(梯度, 暗淡 = 2). 平均值(暗淡 = 2)
    Final_ig_scores = ig_scores.sum(dim = 1)
    排序，索引= torch.sort（final_ig_scores，降序= True）


感谢您为解决我的困惑提供的任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78438413/integrated-gradients-implementation-for-2d-input-in-pytorch</guid>
      <pubDate>Mon, 06 May 2024 18:08:43 GMT</pubDate>
    </item>
    <item>
      <title>有没有更好的方法使用 python 解析非结构化 Excel 表？</title>
      <link>https://stackoverflow.com/questions/78438227/is-there-a-better-way-of-parsing-unstructured-excel-tables-using-python</link>
      <description><![CDATA[我正在尝试从 excel 表中提取一些表格信息。每个 excel 表中都有多个表格，并且 excel 表中的表格结构往往不同。我想出了一个仅适用于一张表的逻辑，但每个 excel 在文件中的结构都不同。有没有办法提取所有子表，无论结构如何？我正在寻找一种通用方法，而现有的库不适合这项任务。
]]></description>
      <guid>https://stackoverflow.com/questions/78438227/is-there-a-better-way-of-parsing-unstructured-excel-tables-using-python</guid>
      <pubDate>Mon, 06 May 2024 17:28:19 GMT</pubDate>
    </item>
    <item>
      <title>训练 BigGan 模型时的运行时错误和其他错误</title>
      <link>https://stackoverflow.com/questions/78437895/runtime-error-and-other-errors-while-training-biggan-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78437895/runtime-error-and-other-errors-while-training-biggan-model</guid>
      <pubDate>Mon, 06 May 2024 16:18:20 GMT</pubDate>
    </item>
    <item>
      <title>如何解决对象检测模型中的错误分类问题？</title>
      <link>https://stackoverflow.com/questions/78437775/how-to-address-misclassification-in-an-object-detection-model</link>
      <description><![CDATA[我正在训练一个物体检测模型，该模型可以识别一个类别，即松鼠。该模型倾向于认为大多数事物都是松鼠，例如鸟类、狗和人类。该模型没有人类、鸟类或狗的类别，也没有经过训练来识别这些东西。我该如何解决这个问题，以便错误分类不会持续存在？
我希望我的模型只识别松鼠]]></description>
      <guid>https://stackoverflow.com/questions/78437775/how-to-address-misclassification-in-an-object-detection-model</guid>
      <pubDate>Mon, 06 May 2024 15:53:08 GMT</pubDate>
    </item>
    <item>
      <title>索引错误：索引 42 超出尺寸 42 的轴 1 的范围</title>
      <link>https://stackoverflow.com/questions/78437580/indexerror-index42-out-of-bounds-for-axis-1-with-size-42</link>
      <description><![CDATA[我正在用 python 编码，我的代码似乎是错误的，我必须输入机器学习模型的数据才能基于它生成输出。我编写了这段代码，以便我可以尝试根据算法从私钥中检索钱包地址。我正在为我的业务做这个，但尚未完成，所以这只是测试代码。
错误是说indexerror：index42超出了尺寸为42的轴1的范围。我认为我的输入数据有问题。到目前为止我的代码是：
将 numpy 导入为 np
从张量流导入keras
从tensorflow.keras.layers导入LSTM，密集

# 根据您指定的连接定义您自己的输入输出对
输入输出对 = {
    &#39;0x7BB18376dC84cCFC4C4c9Fe8367Bf306a213C43C&#39;：&#39;00000000000000000000000000000000000000000000000000000000000000480&#39;，
    &#39;0xeB50d45213864761dEC218e2F0996B591F030007&#39;：&#39;00000000000000000000000000000000000000000000000000000000000000481&#39;，
    # 根据需要添加更多对
}

# 从输入和输出字符串中提取唯一字符
all_chars = set(&#39;&#39;.join(input_output_pairs.keys()) + &#39;&#39;.join(input_output_pairs.values()))

# 创建 char_indices 和indices_char 字典
char_indices = {char: i for i, char in enumerate(all_chars)}
index_char = {i: i 的 char，enumerate 中的 char(all_chars)}

# 输入和输出字符串的最大长度
max_len = max(len(input_str) 对于 input_output_pairs.keys() 中的 input_str)

# 标记化
# 标记化
x = np.zeros((len(input_output_pairs), max_len, len(all_chars)), dtype=np.bool_)
y = np.zeros((len(input_output_pairs), max_len, len(all_chars)), dtype=np.bool_)

对于 i，枚举（input_output_pairs.items（））中的（input_str，output_str）：
    对于 t，枚举中的字符（input_str）：
        x[i, t, char_indices[char]] = 1
    对于 t，枚举中的 char（output_str）：
        y[i, t, char_indices[char]] = 1

# 定义模型架构
模型 = keras.Sequential()
model.add(LSTM(128, input_shape=(max_len, len(all_chars)))) # 删除 return_sequences=True
model.add(Dense(64,activation=&#39;softmax&#39;)) # 调整单位以匹配所需的输出长度
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizationr=&#39;adam&#39;)

# 训练模型
model.fit(x,y,batch_size=32,epochs=100,validation_split=0.2)

# 使用模型根据新的输入字符串生成输出字符串

# 使用模型根据新的输入字符串生成输出字符串
    # 使用模型根据新的输入字符串生成输出字符串
def 生成输出（输入字符串）：
    x_pred = np.zeros((1, max_len, len(all_chars)))
    对于 t，枚举中的字符（input_string）：
        x_pred[0, t, char_indices[char]] = 1

    生成的字符串 = &#39;&#39;
    for _ in range(max_len): # 循环每个时间步长
        pred = model.predict(x_pred, verbose=0)[0]
        print(&quot;预测形状：&quot;, pred.shape)
        next_char = Index_char[np.argmax(pred)] # 预测下一个字符
        generated_string += next_char
        print(“到目前为止生成的字符串：”, generated_string)
        if next_char == &#39;#&#39;: # 字符串结束标记
            休息
        print(&quot;更新前输入形状：&quot;, x_pred.shape)
        x_pred[:, :-1, :] = x_pred[:, 1:, :] # 将输入移动一个时间步长
        x_pred[:, -1, :] = 0 # 清除最后一个时间步
        x_pred[0, -1, char_indices[next_char]] = 1 # 更新下一个时间步的输入
        print(&quot;更新后输入形状：&quot;, x_pred.shape)
    返回生成的字符串




# 用法示例
input_string = &#39;0xbF16A1161d087cf151475824C34dAd398F81B684&#39;
生成输出 = 生成输出（输入字符串）
print(“生成的输出：”, generated_output)

有人可以帮我解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78437580/indexerror-index42-out-of-bounds-for-axis-1-with-size-42</guid>
      <pubDate>Mon, 06 May 2024 15:18:22 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习从图像中提取系统发育树信息</title>
      <link>https://stackoverflow.com/questions/78437477/extracting-phylogenetic-tree-information-from-images-using-machine-learning</link>
      <description><![CDATA[有多种机器学习模型（Claude、chatGPT 等）可用于从图像中提取机器可读的信息。有没有人见过从已发表的系统发育树图像（互联网上有很多）成功提取 Newick 格式数据（或同等数据）的案例？]]></description>
      <guid>https://stackoverflow.com/questions/78437477/extracting-phylogenetic-tree-information-from-images-using-machine-learning</guid>
      <pubDate>Mon, 06 May 2024 15:00:15 GMT</pubDate>
    </item>
    <item>
      <title>关于用于食谱成分提取的 spaCy NER 模型注释的反馈</title>
      <link>https://stackoverflow.com/questions/78437443/feedback-on-spacy-ner-model-annotations-for-recipe-ingredient-extraction</link>
      <description><![CDATA[我目前正在训练 spaCy NER 模型，以专门识别和分类食谱中的成分线。目标是从各种食谱中准确提取成分及其数量、单位和制备说明。下面是我如何注释数据的概述：
跨越标签

数量：与单位相关的数字（例如“2”、“3/4”）
成分：实际成分名称（例如“糖”、“牛奶”）
测量单位：测量单位（例如“杯”、“汤匙”）
说明：准备说明（例如“切细丁”、“分开”）

关系标签

quantity_of：将量程标签数量与成分关联起来
action_to：将跨度标签说明与成分关联起来
unit_of：将跨度标签测量单位与数量相关

我提供了两个示例：一个简单的成分系列和一个复杂的成分系列。
简单行
复杂线
我正在寻求有关以下几点的反馈：

注释过度：是否存在太多类别或过于详细的注释，无法有效学习和实际应用？是否应该合并或省略某些类别？
训练数据量：通常建议使用多少带注释的数据来在此类 NER 任务中实现稳健的模型？我想确保该模型在各种配方格式中都是可靠的。

您可以分享的任何见解、经验或资源都会非常有帮助。谢谢！
我尝试过的：
我使用类似的标签设置训练了一个模型，减去关系标签和“测量单位”。跨越标签。我使用来自随机食谱的约 700 个样本对模型进行了训练。结果不佳；该模型很难识别某些方面，特别是在数量及其单位不相邻的情况下（例如 3 瓣大蒜）]]></description>
      <guid>https://stackoverflow.com/questions/78437443/feedback-on-spacy-ner-model-annotations-for-recipe-ingredient-extraction</guid>
      <pubDate>Mon, 06 May 2024 14:52:57 GMT</pubDate>
    </item>
    <item>
      <title>keras 中的 2 个模型/第二列火车继续第一个模型</title>
      <link>https://stackoverflow.com/questions/78437211/2-models-in-keras-second-trains-continues-on-the-first-model</link>
      <description><![CDATA[我在使用 Keras 时遇到问题，我正在一个接一个地训练两个单独的模型。然而，第二个模型似乎继续使用第一个模型的训练数据或状态。
def create_model():
    图像大小 = 150
    输入形状=（图像大小，图像大小，3）

    pre_trained_model = VGG16（input_shape = input_shape，include_top = False，weights =“imagenet”）
        
    对于 pre_trained_model.layers[:15] 中的层：
        可训练层 = False

    对于 pre_trained_model.layers[15:] 中的层：
        层.可训练= True
        
    last_layer = pre_trained_model.get_layer(&#39;block5_pool&#39;)
    最后的输出=最后的层.输出
        
 …………

    模型 = 模型(pre_trained_model.input, x)

    model.compile(loss=&#39;binary_crossentropy&#39;,
                优化器=优化器.SGD（learning_rate=1e-4，动量=0.9），
                指标=[&#39;acc&#39;])

    模型.summary()
    返回模型

初始化：
&lt;前&gt;&lt;代码&gt;模型 = create_model()

数据生成：
train_datagen = ImageDataGenerator(
    重新缩放=1./255，
    旋转范围=40，
    width_shift_range=0.2，
    height_shift_range=0.2，
    剪切范围=0.2，
    缩放范围=0.2，
    水平翻转=真，）

train_generator = train_datagen.flow_from_directory(
    data_path+“/training_set”，
    目标大小=(150, 150),
    批量大小=5，
    class_mode=&#39;二进制&#39;
）

test_datagen = ImageDataGenerator（重新缩放=1./255）

validation_generator = test_datagen.flow_from_directory(
        data_path+“/test_set”，
        目标大小=(150, 150),
        批量大小=5，
        class_mode=&#39;二进制&#39;）

培训：
历史 = model.fit(
    火车发电机，
    步骤_per_epoch=70, # Anzahl der Batches pro Epoch
    纪元=20，
    验证数据=验证生成器，
    validation_steps=20 # 验证批次的安扎尔
）

第一次训练后，我将所有内容初始化为相同的：
model1 = create_model()

......
history1 = model1.fit(
    火车发电机，
    步骤_per_epoch=70, # Anzahl der Batches pro Epoch
    纪元=20，
    验证数据=验证生成器，
    validation_steps=20 # 验证批次的安扎尔
）

虽然我用
tf.keras.backend.clear_session()

并在单独的函数中实例化每个模型。我处于 acc 为 1.00 的第二个纪元。
我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78437211/2-models-in-keras-second-trains-continues-on-the-first-model</guid>
      <pubDate>Mon, 06 May 2024 14:13:34 GMT</pubDate>
    </item>
    <item>
      <title>如何为 2130 个类别的分类模型配置密集层维度？</title>
      <link>https://stackoverflow.com/questions/78436988/how-to-configure-dense-layers-dimension-for-a-classification-model-of-2130-class</link>
      <description><![CDATA[我正在使用 2130 进行汉字分类任务。设置密集层的最佳实践是什么？
&lt;前&gt;&lt;代码&gt;类数 = 2130
SResnetModel = 顺序(
    [
        重新缩放(1./255, input_shape=(224, 224, 3)),
        tflow.keras.applications.ResNet50(
            include_top=假，
            池化=&#39;平均&#39;,
        ),
        展平（），
        密集（2048，激活=&#39;relu&#39;），
        密集（类数，激活 = &#39;softmax&#39;）
    ]
）
SResnetModel.summary()

这是摘要。

我做了一些搜索，发现最好让倒数第二个密集层的尺寸大于类的数量，但我如何配置它。]]></description>
      <guid>https://stackoverflow.com/questions/78436988/how-to-configure-dense-layers-dimension-for-a-classification-model-of-2130-class</guid>
      <pubDate>Mon, 06 May 2024 13:33:30 GMT</pubDate>
    </item>
    <item>
      <title>如何激励强化学习的有效探索？</title>
      <link>https://stackoverflow.com/questions/78435811/how-to-incentivise-effective-exploration-in-reinforcement-learning</link>
      <description><![CDATA[我正在体育馆研究稀疏的山地汽车环境，其中明显的问题是奖励非常稀疏。每一步的奖励是 -1，如果智能体达到目标，则该情节终止。这意味着代理有动力尽快实现目标。
问题是，从我的发现来看，似乎没有一个算法能够始终“幸运”。并在合理的时间内达到目标。
我不想设计特定的奖励，例如动能等。我想要的东西可以推广到不同的环境，理想情况下不需要调整参数。
我认为的解决方案是，探索对我来说需要更有目的性，而不仅仅是随机选择行动。
我研究了神经密度模型（NDM）探索，这看起来很有趣，但我发现它在这个任务中效果不佳，可能是因为状态空间太简单了。
还有其他有前途的技术可以用来使强化学习更加一致吗？尤其是奖励非常稀少？]]></description>
      <guid>https://stackoverflow.com/questions/78435811/how-to-incentivise-effective-exploration-in-reinforcement-learning</guid>
      <pubDate>Mon, 06 May 2024 09:47:41 GMT</pubDate>
    </item>
    <item>
      <title>Imblearn 语法无效[重复]</title>
      <link>https://stackoverflow.com/questions/78435597/imblearn-invalid-syntax</link>
      <description><![CDATA[虽然我能够在 jupyter 笔记本中安装 Imblearn，但尝试通过以下代码导入管道：
from imblearn.pipeline import Pipeline

我收到错误：
File &quot;C:\Users\ibl165795\AppData\Roaming\Python\Python37\site-
packages\imblearn\utils\_metadata_requests.py&quot;, line 1492
def process_routing(_obj, _method, /, **kwargs):
SyntaxError: 语法无效
]]></description>
      <guid>https://stackoverflow.com/questions/78435597/imblearn-invalid-syntax</guid>
      <pubDate>Mon, 06 May 2024 09:02:47 GMT</pubDate>
    </item>
    <item>
      <title>为我的 Npy 数据集定义 ML 模型时出现问题</title>
      <link>https://stackoverflow.com/questions/78435504/problem-in-defining-a-ml-model-for-my-npy-dataset</link>
      <description><![CDATA[我需要帮助为我的数据定义火炬模型。我尝试了各种方法，但似乎没有任何效果。与输入尺寸和形状相关的错误不断出现。我该如何解决这些问题？
将 numpy 导入为 np
进口火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
从 torch.utils.data 导入 DataLoader，TensorDataset
导入 torch.nn.function 作为 f

# 从 .npy 文件加载数据
data = np.load(“其他py文件/project_files/data/train/data.npy”)
print(&quot;数据形状：&quot;, data.shape) # (401, 701, 255)

数据大小 = 数据.形状[0] * 数据.形状[1] * 数据.形状[2]
print(“数据大小：”, data_size) # 71680755

# 从 .npy 文件加载标签数据
labels = np.load(“其他py文件/project_files/data/train/label.npy”)
print(&quot;标签数据形状:&quot;, labels.shape) # (401, 701, 255)

# 将 numpy 数组转换为 PyTorch 张量
data_tensor = torch.Tensor(数据)
labels_tensor = torch.Tensor(标签)


类 MyModel(nn.Module):
    def __init__(自身):
        超级（MyModel，自我）.__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.fc_input_size = data_size
        self.fc = nn.Linear(self.fc_input_size, 2)

    def 前向（自身，x）：
        x = self.pool(f.relu(self.conv1(x)))
        x = self.pool(f.relu(self.conv2(x)))
        x = x.view(-1, self.fc_input_size)
        x = self.fc(x)
        返回x

模型 = MyModel()
打印（模型）

标准 = nn.CrossEntropyLoss()
优化器 = optim.Adam(model.parameters(), lr=0.001)

数据集 = TensorDataset(data_tensor, labels_tensor)
dataloader = DataLoader(数据集,batch_size=32,shuffle=True)

纪元数 = 10
对于范围内的纪元（num_epochs）：
    运行损失 = 0.0
    对于 i，enumerate(dataloader, 0) 中的数据：
        输入，标签=数据
        优化器.zero_grad()
        outputs = model(inputs.unsqueeze(1)) # 通道维度
        损失=标准（输出，标签）
        loss.backward()
        优化器.step()

        running_loss += loss.item()
        如果我％100==99：
            print(f&quot;[{epoch + 1}, {i + 1}] 损失: {running_loss / 100}&quot;)
            运行损失 = 0.0


使用 torch.no_grad()：
    Predictions = model(data_tensor.unsqueeze(1)) # 通道维度

控制台输出：
已连接到 pydev 调试器（版本 223.8836.43）
数据形状：(401, 701, 255)
数据大小：71680755
标签数据形状：(401, 701, 255)
我的模型（
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), 步幅=(1, 1), 填充=(1, 1))
  （池）：MaxPool2d（kernel_size = 2，stride = 2，padding = 0，dilation = 1，ceil_mode = False）
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), 步幅=(1, 1), 填充=(1, 1))
  （fc）：线性（in_features = 71680755，out_features = 2，偏差= True）
）

文件“C:\Users\PC1\PycharmProjects\Project1\newmodel2.py”，第 36 行，向前
    x = x.view(-1, self.fc_input_size)
运行时错误：形状“[-1, 71680755]”对于大小 22579200 的输入无效
python-BaseException
]]></description>
      <guid>https://stackoverflow.com/questions/78435504/problem-in-defining-a-ml-model-for-my-npy-dataset</guid>
      <pubDate>Mon, 06 May 2024 08:45:53 GMT</pubDate>
    </item>
    <item>
      <title>如何保留机器学习课程中的数学概念？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78435140/how-to-retain-mathematical-concepts-from-machine-learning-courses</link>
      <description><![CDATA[我最近完成了 Deeplearning.ai 在 Coursera 上提供的机器学习专业化以及 ML 和 DS 数学课程。虽然我设法完成了这些课程，但我在保留所教授的数学概念和公式方面面临着挑战。这些课程涵盖了广泛的主题，现在回想起来，我意识到我不记得大部分材料，尤其是详细的数学部分。
我正在寻找有效的策略或工具，以帮助更好地长期掌握和保留这些数学概念。我仍然觉得我还没有完全掌握这些材料。我将不胜感激任何有关学习技巧、资源或任何有助于巩固我对这些概念的理解的具体练习的建议。此外，社区是否有针对适合强化这些知识的后续课程或材料的建议？
我期待一个指导方针。]]></description>
      <guid>https://stackoverflow.com/questions/78435140/how-to-retain-mathematical-concepts-from-machine-learning-courses</guid>
      <pubDate>Mon, 06 May 2024 07:31:53 GMT</pubDate>
    </item>
    <item>
      <title>如何基于掩码相乘矩阵并排除元素？</title>
      <link>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</guid>
      <pubDate>Mon, 29 Apr 2024 19:07:12 GMT</pubDate>
    </item>
    <item>
      <title>预测新数据时保存的 GAMLSS 模型出现问题</title>
      <link>https://stackoverflow.com/questions/77837026/issue-with-saved-gamlss-model-while-predicting-for-new-data</link>
      <description><![CDATA[我有一个经过 GAMLSS 训练的模型，已使用 saveRDS() 以 .rda 格式保存。
例如，我将模型训练为：
gamlss_model&lt;- gamlss(res~pb(x)+pb(y), family=BCTo, data = test)

当我在清除所有环境变量后加载上述模型，并对新数据使用预测函数时：
预测（model_old，newdata = new_data）

我收到以下错误：
eval(Call$data) 中的错误：未找到对象“test”

但是这个测试是旧数据集，在这里应该没有任何意义。我无法理解这有什么问题。因此，我无法运行 REST API。
当我的所有环境变量在 GAMLSS 模型训练后都存在时，那么当我立即使用预测时，它就可以工作了！但我想稍后使用预测。]]></description>
      <guid>https://stackoverflow.com/questions/77837026/issue-with-saved-gamlss-model-while-predicting-for-new-data</guid>
      <pubDate>Thu, 18 Jan 2024 05:10:18 GMT</pubDate>
    </item>
    </channel>
</rss>