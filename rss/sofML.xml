<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 09 May 2024 12:25:29 GMT</lastBuildDate>
    <item>
      <title>我的情绪分析给出了错误的预测</title>
      <link>https://stackoverflow.com/questions/78454399/my-sentiment-analysis-is-giving-wrong-predictions</link>
      <description><![CDATA[我有一个包含两列的数据框：推文和标签（攻击性语言、仇恨言论、无仇恨和攻击性）。
我清理了推文并创建了我的模型。
但在建模之后，我的所有测试文本都给出了相同的预测结果：“没有仇恨和攻击性”
# 清理我的文本
def clean_text(文本):
    文本 = str(文本).lower()
    文本 = re.sub(&#39;\[.*?\]&#39;, &#39;&#39;, 文本)
    text = re.sub(&#39;https?://\S+|www\.\S+&#39;, &#39;&#39;, text)
    文本 = re.sub(&#39;&lt;.*?&gt;+&#39;, &#39;&#39;, 文本)
    text = re.sub(&#39;[%s]&#39; % re.escape(string.punctuation), &#39;&#39;, text)
    文本 = re.sub(&#39;\n&#39;, &#39;&#39;, 文本)
    文本 = re.sub(&#39;\w*\d\w*&#39;, &#39;&#39;, 文本)
    text = [text.split(&#39; &#39;) 中的单词逐字，如果单词不在停用词中]
    文本 =“”.join(文本)
    text = [stemmer.stem(word) for word in text.split(&#39; &#39;)]
    文本=“”.join(文本)
    
    返回文本

数据[“推文”] = 数据[“推文”].apply(clean_text)


#将我的列转换为数组
将 numpy 导入为 np
x = np.array(data[“tweet”])
y = np.array(数据[“标签”])


# 拟合推文列以进行建模
从 sklearn.feature_extraction.text 导入 CountVectorizer
从 sklearn.model_selection 导入 train_test_split
count_vec = CountVectorizer()
X = count_vec.fit_transform(x)

X_train、X_test、y_train、y_test = train_test_split(X、y、test_size = 0.33、random_state = 42)


从 sklearn.tree 导入 DecisionTreeClassifier

dc_tree = 决策树分类器()
dc_tree.fit(X_train, y_train)

##测试我的模型
test_text = &quot;杀掉所有的人并将其烧毁&quot;;
test_data = count_vec.transform([test_text]).toarray()

打印（dc_tree.预测（测试数据））
#Output：[&#39;没有仇恨和攻击性&#39;] #Expectation：[&#39;仇恨言论&#39;]


test_text = “实践爱和耐心，过上美好的生活！”
test_data2 = count_vec.transform([test_text]).toarray()
打印（dc_tree.预测（test_data2））
#Output：[&#39;没有仇恨和攻击性&#39;]

]]></description>
      <guid>https://stackoverflow.com/questions/78454399/my-sentiment-analysis-is-giving-wrong-predictions</guid>
      <pubDate>Thu, 09 May 2024 12:19:20 GMT</pubDate>
    </item>
    <item>
      <title>feature_weights 参数没有影响 Xgboost</title>
      <link>https://stackoverflow.com/questions/78454026/the-feature-weights-parameter-has-no-effect-xgboost</link>
      <description><![CDATA[xgboost 有一个 parameter feature_weights 应该影响模型选择特征的概率，也就是说，我们可以给每个特征更多或更少的权重，但似乎该参数不起作用还是我做错了什么？
X &lt;- as.matrix(iris[,-5])
Y &lt;- ifelse(iris$Species==&quot;setosa&quot;, 1, 0)

库（xgboost）
dm1 &lt;- xgb.DMatrix(X, 标签 = Y)
#我为每个特征设置不同的概率
dm2 &lt;- xgb.DMatrix(X, 标签 = Y, feature_weights = c(1, 0, 0, 0.01))
params &lt;- list(objective = “binary:logistic”, eval_metric = “logloss”)

设置.种子(1)



xgb1 &lt;- xgboost（数据 = dm1，参数 = 参数，nrounds = 10，print_every_n = 5）

[1] 火车对数损失：0.448305
[6] 火车对数损失：0.090220
[10]训练对数损失：0.033148



xgb2 &lt;- xgboost（数据 = dm2，参数 = 参数，nrounds = 10，print_every_n = 5）

[1] 火车对数损失：0.448305
[6] 火车对数损失：0.090220
[10]训练对数损失：0.033148

但是模型的行为完全相同，似乎参数feature_weights被简单地忽略了]]></description>
      <guid>https://stackoverflow.com/questions/78454026/the-feature-weights-parameter-has-no-effect-xgboost</guid>
      <pubDate>Thu, 09 May 2024 11:10:59 GMT</pubDate>
    </item>
    <item>
      <title>输入列的架构不匹配预期的字符串或字符串向量，得到 UInt32（参数“inputSchema”）</title>
      <link>https://stackoverflow.com/questions/78453914/schema-mismatch-for-input-column-expected-string-or-vector-of-string-got-uint32</link>
      <description><![CDATA[未处理的异常。 System.ArgumentOutOfRangeException：输入列“QuestionKey”的架构不匹配：预期的字符串或字符串向量，得到 UInt32（参数“inputSchema”）
代码：
// 尝试不同的文本特征化技术
var tokenizedText = mlContext.Transforms.Text.TokenizeIntoWords(&quot;Tokens&quot;, &quot;Question&quot;);
var wordEmbeddings = mlContext.Transforms.Text.ApplyWordEmbedding(&quot;特征&quot;, &quot;令牌&quot;, WordEmbeddingEstimator.PretrainedModelKind.SentimentSpecificWordEmbedding);
 var concatenatedFeatures = mlContext.Transforms.Concatenate(&quot;FeaturesConcatenated&quot;, &quot;Features&quot;);

        // 将“问题”列转换为 KeyType
        var modelPipeline = mlContext.Transforms.Conversion.MapValueToKey(&quot;QuestionKey&quot;, &quot;Question&quot;)
            .Append(mlContext.Transforms.Text.TokenizeIntoWords(“Tokens”, “QuestionKey”))
            .Append(mlContext.Transforms.Text.ApplyWordEmbedding(“特征”, “令牌”, WordEmbeddingEstimator.PretrainedModelKind.SentimentSpecificWordEmbedding))
            .Append(mlContext.Transforms.Categorical.OneHotEncoding(“QuestionEncoded”, “QuestionKey”))
            .Append(mlContext.Transforms.Conversion.MapValueToKey(“标签”, “答案”))
            .Append(mlContext.Transforms.Concatenate(&quot;FeaturesConcatenated&quot;, &quot;Features&quot;));

        
        // 使用 AutoML 或手动调整进行超参数调整实验
        var trainer = mlContext.MulticlassClassification.Trainers.LightGbm(labelColumnName: &quot;答案&quot;, featureColumnName: &quot;问题&quot;);

        var TrainingPipeline = modelPipeline.Append(trainer);

        // 训练模型
        var TrainingModel = TrainingPipeline.Fit(dataView);

        // 将训练好的模型保存到文件中
        mlContext.Model.Save(trainedModel, dataView.Schema, modelPath);

        返回训练好的模型；
    }

我正在使用包含两列问题和答案的数据集构建一个常见问题解答聊天机器人。该数据集包含大约 30000 个 QnA。我被困在这里尝试不同的技术。让我通过这个。]]></description>
      <guid>https://stackoverflow.com/questions/78453914/schema-mismatch-for-input-column-expected-string-or-vector-of-string-got-uint32</guid>
      <pubDate>Thu, 09 May 2024 10:49:39 GMT</pubDate>
    </item>
    <item>
      <title>如何配置作业yaml和Yolov8数据集ymal来访问Azure上的数据资产？</title>
      <link>https://stackoverflow.com/questions/78453842/how-to-configure-job-yaml-and-yolov8-dataset-ymal-to-access-data-asset-on-azure</link>
      <description><![CDATA[我目前正在使用 Azure ML CLI v2 在 Azure ML 工作室中使用 Yolov8 训练自定义模型。
问题：
当我在 Azure ML 上运行作业时，收到一条错误消息“权限被拒绝”
错误代码：ScriptExecution.StreamAccess.Authentication
本机错误：来自输入数据源的流式传输错误
    StreamError(PermissionDenied(Some(此请求无权使用此权限执行此操作。)))
=&gt;访问流时权限被拒绝。原因：一些（该请求无权使用该权限执行该操作。）
    PermissionDenied(Some(此请求无权使用此权限执行此操作。))
错误消息：尝试访问流时身份验证失败。确保您设置了正确的权限。好的（该请求无权使用该权限执行该操作。）| session_id=da7b713c-6cc8-4f6d-b24f-b54ab37e14ef

Yaml 文件：

job.yaml：

$schema：https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
实验名称：yolov8-实验

命令： |
  sed -i“s|路径：.*$|路径：${{inputs.training_data}}|”自定义数据集.yaml
  # 训练模型
  yolo 任务=检测训练数据=custom_dataset.yaml 模型=${{inputs.model_to_train }} epochs=10 项目=yolov8-实验名称=实验

输入：
  训练数据：
    类型：uri_文件夹
    模式：ro_mount
    path: azure:data_asset:1 #我已经从本地文件创建了数据资产。
  模型到训练：
    类型：自定义模型
    路径：azureml:yolov8l:1

code: /training-code/ #custom_dataset.yaml 存储在本地的路径
环境：azureml：yolov8-环境：1
计算：azureml：compute_cluster


custom_dataset.yaml 文件：

路径：../数据集
火车：/图像/火车/
测试：/图像/测试/
值：/图像/测试/

NC=2

# 类名
名称：[class1，class2]

我参考了以下文章：
中-文章-yolov8-training-azure-cli
在 Azure ML 上训练模型使用 CLI v2 - Microsoft 培训
我目前在确定继续进行 Azure 设置所需的权限时遇到问题。我已使用“az login”成功登录，并成功创建了各种组件，例如环境、计算集群和数据资产。
但是，我不确定进一步操作所需的具体权限。有人可以提供有关我的设置需要授予哪些权限才能正常运行的指导吗？任何见解或建议将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78453842/how-to-configure-job-yaml-and-yolov8-dataset-ymal-to-access-data-asset-on-azure</guid>
      <pubDate>Thu, 09 May 2024 10:35:51 GMT</pubDate>
    </item>
    <item>
      <title>房地产数据集中多标签回归问题的挑战</title>
      <link>https://stackoverflow.com/questions/78453678/challenges-with-a-multi-label-regression-problem-in-a-real-estate-dataset</link>
      <description><![CDATA[我目前正在进行一项研究，旨在预测纽约房产的售价和交易执行时间。为此，我有一个在纽约出售的各种房产的数据集，其中包含有关房产特征的多个功能。
我想要实现的是多输出回归问题。我正在和我的教授讨论这个问题，在最终的协议中，他要求我解释为什么预测交易执行时间可能会出现问题。
这是我的想法，我想知道这对您是否有意义，或者是否还有其他我没有考虑到的问题：
我认为，主要问题是评估结果的准确性。问题在于，交易执行时间可能会受到数据集中不存在的无数不可量化的外部因素的影响。例如，房地产经纪人的熟练程度、他们在房地产行业的人脉、他们出售房产的决心、天气状况、买家的经验等等。
因此，交易执行时间的多个值可能表现出相同的特征模式。因此，我们正在处理多标签回归问题。因此，创建负责尽可能准确地预测交易执行时间的模型可能会隐藏一些困难。
我还应该考虑其他数学或机器学习优化因素吗？]]></description>
      <guid>https://stackoverflow.com/questions/78453678/challenges-with-a-multi-label-regression-problem-in-a-real-estate-dataset</guid>
      <pubDate>Thu, 09 May 2024 10:06:01 GMT</pubDate>
    </item>
    <item>
      <title>确定适当的统计测试来比较生存分析中 ML/DL 模型之间的性能差异</title>
      <link>https://stackoverflow.com/questions/78453674/determining-the-appropriate-statistical-test-for-comparing-performance-differenc</link>
      <description><![CDATA[我进行了一项实验，在生存分析任务中训练和测试了八个 ML 和 DL 模型，每个模型都经过超参数优化。调整后，每个模型在训练数据上训练一次，在测试数据上测试一次，得到八个代表模型性能的 c 指数分数。
现在，我想确定这些模型之间的性能是否存在显著差异。由于我有多个模型，每个模型有一组测试结果，我应该使用什么统计假设检验来评估性能差异的显著性？我应该考虑 Kruskal-Wallis 检验、方差分析还是其他检验？此外，我该如何解释从所选测试中获得的结果？任何见解或指导都将不胜感激。
]]></description>
      <guid>https://stackoverflow.com/questions/78453674/determining-the-appropriate-statistical-test-for-comparing-performance-differenc</guid>
      <pubDate>Thu, 09 May 2024 10:05:23 GMT</pubDate>
    </item>
    <item>
      <title>我无法使用tensorflow和keras</title>
      <link>https://stackoverflow.com/questions/78453593/i-cannot-use-tensorflow-and-keras</link>
      <description><![CDATA[所以我尝试使用keras来加载模型。但是当我导入 keras lib 时，出现“导入错误：导入 _pywrap_tfe 时 DLL 加载失败：找不到指定的过程”。
&#39;，然后我尝试
导入tensorflow为tf
从张量流导入keras

但后来我得到了“导入错误：无法从“tensorflow”（未知位置）导入名称“keras””
我使用的是最新版本的tensorflow（2.16.1）和keras（3.3.3）
任何帮助。
我希望它再次正常工作]]></description>
      <guid>https://stackoverflow.com/questions/78453593/i-cannot-use-tensorflow-and-keras</guid>
      <pubDate>Thu, 09 May 2024 09:49:55 GMT</pubDate>
    </item>
    <item>
      <title>VAE 中的哑解码器</title>
      <link>https://stackoverflow.com/questions/78453467/dumb-decoder-in-a-vae</link>
      <description><![CDATA[我正在 VAE 架构中预训练编码器。我的目标是传输编码器的权重并针对分类任务对其进行微调。因为我的最终目标是分类问题，而不是生成问题，是编码“哑”问题解码器的一般做法？
例如：
 # 哑解码器
        self.decoder = nn.Sequential(
            nn.Linear(latent_size, 5000),
            nn.Sigmoid()
        ）

        # 使解码器不可训练
        对于 self.decoder.parameters() 中的参数：
            param.requires_grad = False
]]></description>
      <guid>https://stackoverflow.com/questions/78453467/dumb-decoder-in-a-vae</guid>
      <pubDate>Thu, 09 May 2024 09:25:49 GMT</pubDate>
    </item>
    <item>
      <title>使用自定义数据微调 LayoutLMv2 以进行文档问答</title>
      <link>https://stackoverflow.com/questions/78453431/fine-tuning-layoutlmv2-for-document-question-answering-using-custom-data</link>
      <description><![CDATA[我想微调 LayoutLMv2，以实现自定义数据的文档问答。有人可以帮我了解如何为此任务准备数据吗？
我需要以正确的格式准备数据，但由于这方面的资源非常少，这变得非常困难。我请求您帮助解决此事。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78453431/fine-tuning-layoutlmv2-for-document-question-answering-using-custom-data</guid>
      <pubDate>Thu, 09 May 2024 09:17:22 GMT</pubDate>
    </item>
    <item>
      <title>为什么混合精度训练中使用的梯度缩放可以导致 float32 模型中更大的学习率？</title>
      <link>https://stackoverflow.com/questions/78453294/why-gradient-scaling-used-in-mix-precision-training-could-lead-to-bigger-learnin</link>
      <description><![CDATA[背景：
Gradient Scaling原用于混合精度训练（部分模型权重为float16，部分为float 32），其目的是减少float16存储的小梯度的下溢。
最近，我在做一些实验，发现一些结果让我感到困惑。在整个模型具有 float32 权重的情况下，如果我使用梯度缩放更新模型，与不使用它相比，我可以使用更大的 LR（学习率）来进行收敛。
缩放版本更新代码摘录：
loss_scaler = torch.cuda.amp.GradScaler()
loss_scaler.scale(loss).backward()
loss_scaler.unscale_（优化器）
loss_scaler.step（优化器）
loss_scaler.update()

未缩放版本更新代码摘录：
loss.backward()
优化器.step()

问题：
为什么会发生这种情况？是否应该取消缩放后的渐变并将其转换为原始渐变？我期望的是他们的 LR 对于模型训练应该是相同的？
以下是 ChatGPT 的解释：
即使float32也可能下溢，而scale使得这些下溢梯度有助于权重更新，使训练更加稳定，因此lr可以更大。
我不知道这个解释是否合理，因为LR有很大不同。 （1.5e-4 与 1.5e-8）]]></description>
      <guid>https://stackoverflow.com/questions/78453294/why-gradient-scaling-used-in-mix-precision-training-could-lead-to-bigger-learnin</guid>
      <pubDate>Thu, 09 May 2024 08:49:42 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用 KL 散度损失进行知识蒸馏。损失太高</title>
      <link>https://stackoverflow.com/questions/78452655/trying-to-perform-knowledge-distillation-using-kl-divergence-loss-the-loss-is</link>
      <description><![CDATA[KL 散度损失过高
我正在尝试进行知识提炼。对于我的学生损失，我使用了交叉熵损失，对于我的知识蒸馏损失，我尝试使用 KL 散度损失。
这是我用于 KL 散度损失的代码。
class KLDivLoss(nn.Module):
    def __init__(self,ignore_index=-1, reduction=&quot;batchmean&quot;, log_target=False):
        super(KLDivLoss, self).__init__()
        self.reduction = reduction
        self.log_target = log_target
        self.ignore_index = ignore_index

    def forward(self, preds_S, preds_T, T =1.0, alpha = 1.0):
        preds_T[0] = preds_T[0].detach()  # 分离教师预测
        pred_1 = torch.sigmoid(preds_T[0]/T) # 白色
        pred_0 = 1 - pred_1
        preds_teacher = torch.cat((pred_0, pred_1), dim=1)
        assert preds_S[0].shape == preds_teacher.shape, “输入和目标形状必须匹配 KLDivLoss”
       stu_prob = F.log_softmax(preds_S[0]/T, dim=1)
        kd_loss = F.kl_div(stu_prob, 
                           preds_teacher, 
                             reduction=&#39;batchmean&#39;,
                           ) * T * T
        return {&#39;loss&#39;: kd_loss}

我从中得到的值非常大。我只是从学生模型中添加了知识蒸馏损失和交叉熵损失。由于我的 CE 损失非常小，这全都来自 KLdiv 损失。你能告诉我如何减少损失吗？或者如果我做错了什么。
在此处输入图片说明
我尝试使用 KL div 损失，其中温度 =1
我的老师模型以张量 [8,1,224,224] 的形式给出输出，因为它用于像素的二进制预测，而我的学生模型以 [8,2,224,224] 的形式给出输出，其中 0 属于黑色类，1 属于白色。
因此，为了将它们与 KL div 损失相匹配，我使用 sigmoid 函数来获取白色类的概率和 1 - 黑色的白色概率。然后将它们连接起来形成一个大小为 [8,2,224,224] 的张量，这与学生张量相似。
然后我尝试执行 KL 发散。我遭受的损失非常大]]></description>
      <guid>https://stackoverflow.com/questions/78452655/trying-to-perform-knowledge-distillation-using-kl-divergence-loss-the-loss-is</guid>
      <pubDate>Thu, 09 May 2024 06:27:53 GMT</pubDate>
    </item>
    <item>
      <title>谁能帮我解决输入形状问题？我尝试了多种解决方案，但无法解决该错误？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78452124/can-anyone-help-me-solve-the-input-shape-problem-i-tried-several-solutions-but</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78452124/can-anyone-help-me-solve-the-input-shape-problem-i-tried-several-solutions-but</guid>
      <pubDate>Thu, 09 May 2024 03:28:23 GMT</pubDate>
    </item>
    <item>
      <title>Yolo 模型中的增量分类器和表示学习</title>
      <link>https://stackoverflow.com/questions/78448470/incremental-classifier-and-representation-learning-in-yolo-models</link>
      <description><![CDATA[我的 YOLO 模型遇到问题。
最初，我用 7 个类对其进行了训练。现在，我想向模型添加 4 个新类。然而，当我将原始 7 个类别的数据与新的 4 个类别的数据结合起来时，训练时间和相关的云成本显着增加。有什么好的解决方案可以有效地将这些额外的类合并到模型中而不增加训练时间和成本？
我的期望是减少增量学习的成本和培训时间。]]></description>
      <guid>https://stackoverflow.com/questions/78448470/incremental-classifier-and-representation-learning-in-yolo-models</guid>
      <pubDate>Wed, 08 May 2024 12:20:14 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 3.10.0 在 PySpark 中加载 LightGBM 库时出错</title>
      <link>https://stackoverflow.com/questions/78441794/error-loading-lightgbm-library-in-pyspark-with-python-3-10-0</link>
      <description><![CDATA[描述：
在 Python 3.10.0 中使用 PySpark 时，我遇到了 LightGBM 回归器和分类器的问题。
环境：
PySpark 版本：3.2.1
Python版本：3.10.0
Py4j版本：0.10.9.5
Spark jar 包：com.microsoft.azure:synapseml_2.12:0.11.0
错误消息：
java.lang.UnsatisfiedLinkError：无法加载库：/var/folders/dz/mc23060n2kq52djyhcxl9kmh0000gp/T/mml-natives17452036633252549823/lib_lightgbm.dylib
重现步骤：

在 Python 中将 LightGBM 回归器或分类器与 PySpark DataFrame 结合使用
3.10.0。
遇到上述错误。

所做的尝试：
我按照此处提供的解决方案进行操作，并对已安装的 libomp 进行了符号链接，但问题仍然存在。
详细的错误堆栈：
py4j.protocol.Py4JJavaError：调用 o5147.fit 时发生错误。
E：java.lang.UnsatisfiedLinkError：无法加载库：/var/folders/dz/mc23060n2kq52djyhcxl9kmh0000gp/T/mml-natives17452036633252549823/lib_lightgbm.dylib
E 位于 java.base/java.lang.ClassLoader.loadLibrary（来源未知）
E 位于 java.base/java.lang.Runtime.load0（来源未知）
E 位于 java.base/java.lang.System.load（来源未知）
E 位于 com.microsoft.azure.synapse.ml.core.env.NativeLoader.loadLibraryByName(NativeLoader.java:66)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMUtils$.initializeNativeLibrary(LightGBMUtils.scala:33)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMBase.train(LightGBMBase.scala:37)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMBase.train$(LightGBMBase.scala:36)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMRegressor.train(LightGBMRegressor.scala:39)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMRegressor.train(LightGBMRegressor.scala:39)
E at org.apache.spark.ml.Predictor.fit(Predictor.scala:151)
E at org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0（本机方法）
E 位于 java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke（来源未知）
E 位于 java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke（来源未知）
E 位于 java.base/java.lang.reflect.Method.invoke（来源未知）
E 位于 py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E 位于 py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
E 位于 py4j.Gateway.invoke(Gateway.java:282)
E 位于 py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E 位于 py4j.commands.CallCommand.execute(CallCommand.java:79)
E 在 py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E 位于 py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E 位于 java.base/java.lang.Thread.run（来源未知）
附加说明：

该问题似乎与在 Python 3.10.0 中加载 LightGBM 库有关
我已检查指定路径中是​​否存在 lib_lightgbm.dylib。
如果您能提供有关解决此问题的任何见解或建议，我们将不胜感激。
]]></description>
      <guid>https://stackoverflow.com/questions/78441794/error-loading-lightgbm-library-in-pyspark-with-python-3-10-0</guid>
      <pubDate>Tue, 07 May 2024 10:23:19 GMT</pubDate>
    </item>
    <item>
      <title>如何在python中添加L1规范化？</title>
      <link>https://stackoverflow.com/questions/48782758/how-to-add-l1-normalization-in-python</link>
      <description><![CDATA[我正在尝试从头开始编写逻辑回归代码。在这段代码中，我认为我的成本导数是我的正则化，但我的任务是添加 L1norm 正则化。你如何在Python中添加这个？是否应该在我定义成本导数的地方添加此内容？感谢任何正确方向的帮助。
def Sigmoid(z):
    返回 1/(1 + np.exp(-z))

def 假设(theta, X):
    返回 Sigmoid(X @ theta)

def Cost_Function(X,Y,theta,m):
    hi = 假设(theta, X)
    _y = Y.reshape(-1, 1)
    J = 1/float(m) * np.sum(-_y * np.log(hi) - (1-_y) * np.log(1-hi))
    返回J

def Cost_Function_Derivative(X,Y,theta,m,alpha):
    hi = 假设(theta,X)
    _y = Y.reshape(-1, 1)
    J = alpha/float(m) * X.T @ (hi - _y)
    返回J

def Gradient_Descent(X,Y,θ,m,alpha):
    new_theta = theta - Cost_Function_Derivative(X,Y,theta,m,alpha)
    返回 new_theta

定义精度(theta):
    正确 = 0
    长度 = len(X_test)
    预测=（假设（theta，X_test）&gt; 0.5）
    _y = Y_test.reshape(-1, 1)
    正确=预测==_y
    my_accuracy = (np.sum(正确) / 长度)*100
    print (&#39;LR 精度:&#39;, my_accuracy, &quot;%&quot;)

def Logistic_Regression(X,Y,alpha,theta,num_iters):
    m = 长度（Y）
    对于范围内的 x（num_iters）：
        new_theta = Gradient_Descent(X,Y,theta,m,alpha)
        θ = 新_θ
        如果 x % 100 == 0：
            打印 #(&#39;θ: &#39;, θ)
            print #(&#39;成本：&#39;, Cost_Function(X,Y,theta,m))
    精度(θ)
ep = .012
初始_theta = np.random.rand(X_train.shape[1],1) * 2 * ep - ep
阿尔法 = 0.5
迭代次数 = 10000
Logistic_Regression(X_train,Y_train,alpha,initial_theta,迭代)
]]></description>
      <guid>https://stackoverflow.com/questions/48782758/how-to-add-l1-normalization-in-python</guid>
      <pubDate>Wed, 14 Feb 2018 08:34:48 GMT</pubDate>
    </item>
    </channel>
</rss>