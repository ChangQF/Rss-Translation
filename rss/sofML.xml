<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 11 Feb 2025 12:33:19 GMT</lastBuildDate>
    <item>
      <title>我们可以编写一个可以从图像中检测和识别人的身高体重的 Python 脚本吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79429730/can-we-make-a-python-script-that-can-detect-and-identify-person-height-weight-fr</link>
      <description><![CDATA[我尝试了某人的 Git，但效果不太好，因为他提供的示例图像也没有给出准确的结果。我想要一个 Python 脚本，它获取图像路径，扫描该图像并在该图像中查找人，返回人的身高和体重。我知道这不可能 100% 准确，但应该有 90% 以上正确率。

输入 - 示例图片
输出 - 身高：117.8 厘米，体重：68.6 公斤


输入 - 示例图片2
输出 - 身高：170.8 厘米，体重：77.12 公斤]]></description>
      <guid>https://stackoverflow.com/questions/79429730/can-we-make-a-python-script-that-can-detect-and-identify-person-height-weight-fr</guid>
      <pubDate>Tue, 11 Feb 2025 10:58:38 GMT</pubDate>
    </item>
    <item>
      <title>我该如何解决“RuntimeError：张量 a（64）的大小必须与非单例维度 1 处的张量 b（6）的大小匹配”？</title>
      <link>https://stackoverflow.com/questions/79429107/how-can-i-resolve-runtimeerror-the-size-of-tensor-a-64-must-match-the-size-o</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79429107/how-can-i-resolve-runtimeerror-the-size-of-tensor-a-64-must-match-the-size-o</guid>
      <pubDate>Tue, 11 Feb 2025 06:48:18 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 Tensorflow C++ API 执行预测</title>
      <link>https://stackoverflow.com/questions/79428265/failed-to-execute-a-prediction-with-the-tensorflow-c-api</link>
      <description><![CDATA[我创建并训练了一个模型来识别一些卡通人物。
我使用的是 TensorFlow 2.18，使用 TF Python API 训练的模型足够准确。
准确率相当高。
训练结束后，使用 tensorflow.saved_model.save() 保存模型。
我使用 python API 测试模型，一切正常。
现在我尝试使用 C++ API 测试模型。

加载模型：OK
将图像转换为 Tensor：OK
执行推理：错误如下

2025-02-10 21:07:16.084677：I tensorflow/core/framework/local_rendezvous.cc:405] 本地会合正在中止，状态为：FAILED_PRECONDITION：找不到变量 Sequence/conv2d_3/kernel。这可能意味着变量已被删除。在 TF1 中，它也可能意味着变量未初始化。调试信息：容器 = localhost，状态错误消息 = 资源 localhost/sequential/conv2d_3/kernel/N10tensorflow3VarE 不存在。

请注意，未找到变量可能因执行而异。
我尽我所能，知道 C++ API 文档没有很好的文档记录。
为了加载模型，我使用以下代码片段：
/// 尝试加载模型 

m_status = LoadSavedModel(m_sessionOptions, 
m_runOptions,filePath, tensorflow::kSavedModelTagServe, &amp;m_bundle); 

执行推理：
// 执行推理
tensorflow::Status status = 
m_bundle.GetSession()-&gt;Run(
{{&quot;serving_default_inputs:0&quot;, inputTensor}},
{&quot;StatefulPartitionedCall:0&quot;},
{},
&amp;outputs);

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79428265/failed-to-execute-a-prediction-with-the-tensorflow-c-api</guid>
      <pubDate>Mon, 10 Feb 2025 20:44:06 GMT</pubDate>
    </item>
    <item>
      <title>Flaml 中是否有与 Optuna get_param_importances 等效的功能？</title>
      <link>https://stackoverflow.com/questions/79428150/is-there-an-equivalent-of-optuna-get-param-importances-in-flaml</link>
      <description><![CDATA[我正在尝试使用 FLAML 对我的模型进行超参数调整，我想看看每个超参数对目标值的贡献。类似于 Optuna 的 get_param_importances 或 plot_param_importances。有没有办法用 FLAML 来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/79428150/is-there-an-equivalent-of-optuna-get-param-importances-in-flaml</guid>
      <pubDate>Mon, 10 Feb 2025 19:48:01 GMT</pubDate>
    </item>
    <item>
      <title>使用合规的列名在 Neuton.AI 上上传数据，但它仍然通知列名中有无效数据</title>
      <link>https://stackoverflow.com/questions/79427914/upload-data-on-neuton-ai-using-compliant-column-name-however-it-still-notify-t</link>
      <description><![CDATA[我正在使用 Neuton.AI 训练我的小型 AI 模型，我有一个 csv 文件数据集，由 Excel 编辑，格式如下：



ax1
ay1
az1
gx1
gy1
gz1
...
ax50
ay50
az50
gx50
gy50
g z50




数据
数据
数据
数据
数据
数据
数据
...
数据
数据
数据
数据
数据
dat a
数据


数据
数据
数据
数据
数据
数据
数据
...
数据
数据
数据
数据
数据
数据


...















问题是，即使我尝试多次以不同的方式更改列名，它只会给我相同的错误消息：
所有列名（CSV 文件标题中的值）必须仅包含字母（a-z、A-Z）、
数字（0-9）、连字符（-）或下划线（_）。更改名称并再次下载
数据集。

错误消息屏幕截图
如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/79427914/upload-data-on-neuton-ai-using-compliant-column-name-however-it-still-notify-t</guid>
      <pubDate>Mon, 10 Feb 2025 17:56:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 opencv 和 mediapipe 进行姿势估计时遇到此错误[关闭]</title>
      <link>https://stackoverflow.com/questions/79427900/facing-this-error-when-doing-pose-estimation-with-opencv-and-mediapipe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79427900/facing-this-error-when-doing-pose-estimation-with-opencv-and-mediapipe</guid>
      <pubDate>Mon, 10 Feb 2025 17:49:19 GMT</pubDate>
    </item>
    <item>
      <title>降尺度操作没有捕捉到峰值[关闭]</title>
      <link>https://stackoverflow.com/questions/79427067/downscaling-operation-not-capturing-the-peaks</link>
      <description><![CDATA[我正在使用随机森林回归模型将我的数据集从较粗分辨率缩小到较高分辨率。我尝试了交叉验证和调整参数。在检查验证分数时，我得到了相当好的 MSE 和 R2 值。但是当我使用相同的模型来预测新数据集的目标变量时，该模型无法捕捉数据最初几年的峰值。可能是其中一个特征具有主导重要性。当绘制主导特征的数据时，数据全年都很稳定。
这让趋势偏离了方向
具有最高重要性的数据之一]]></description>
      <guid>https://stackoverflow.com/questions/79427067/downscaling-operation-not-capturing-the-peaks</guid>
      <pubDate>Mon, 10 Feb 2025 12:29:38 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习和 NER 自动进行网页抓取以提取产品数据</title>
      <link>https://stackoverflow.com/questions/79426509/automating-web-scraping-with-machine-learning-and-ner-for-product-data-extractio</link>
      <description><![CDATA[我目前正在做数据和人工智能实习。我的工作是通过从制造商的网站上检索信息（产品名称、图片、描述、零件编号/SKU、技术规格、数据表等）来构建产品数据库。
挑战在于有超过 300 家不同的制造商，每家都有自己的网站和结构，这使得传统的网页抓取不切实际且难以维护。为了克服这个问题，我正在考虑使用人工智能和机器学习来使我的抓取代理能够适应每个页面 HTML 结构的变化。
我已经下载并手动标记了 50 个产品页面。以下是我的数据集：
 # 列 非空 计数 Dtype 
--- ------ -------------- ----- 
0 text 52 非空对象
1 product_name 49 非空对象
2 html_product_name 51 非空对象
3 image_url 50 非空对象
4 html_image_url 50 非空对象
5 description 32 非空对象
6 html_description 51 非空对象
7 part_number 35 非空对象
8 html_part_number 36 非空对象
9 html_specification 44 非空对象
10 datasheet_url 40 非空对象
11 html_datasheet_url 41 非空对象
12 specifications 2 非空对象

文本列包含产品页面的清理后的 HTML，而其他列则表示目标字段——需要识别和提取的 HTML 的特定部分。
这个问题看起来与命名实体识别 (NER) 非常相似。我如何训练机器学习模型以成功从原始 HTML 中提取这些字段？最好的方法是什么（例如，微调转换器模型、序列标记或其他方法）？
提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/79426509/automating-web-scraping-with-machine-learning-and-ner-for-product-data-extractio</guid>
      <pubDate>Mon, 10 Feb 2025 08:46:11 GMT</pubDate>
    </item>
    <item>
      <title>ML 模型保存</title>
      <link>https://stackoverflow.com/questions/79426066/ml-models-saving</link>
      <description><![CDATA[我正在训练一个机器学习模型，将阿尔茨海默病分为四类。在运行训练周期后，我使用代码将模型保存在 .pth 文件中。但是，在下载保存的模型并将其连接到我的接口后，它没有产生任何结果。进一步检查后发现，模型似乎为空。
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm.auto import tqdm
from torchvision import models

# 设置设备（如果可用，则为 GPU）
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

# 定义 ResNet18 模型
class ResNet18(nn.Module):
def __init__(self, num_classes):
super(ResNet18, self).__init__()
self.model = models.resnet18(pretrained=False) # 设置为 True 以使用预训练权重
self.model.fc = nn.Linear(self.model.fc.in_features, num_classes) # 修改最后一层

def forward(self, x):
return self.model(x)

# 使用数据集中的类数初始化模型
num_classes = len(train_data_simple.classes) # 根据增强数据集进行调整
model_resnet = ResNet18(num_classes=num_classes).to(device)

# 损失函数和优化器（SGD 和 CrossEntropy）
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(model_resnet.parameters(), lr=0.1, motivation=0.9)

# 训练循环（使用 tqdm 的进度条）
def train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs):
for epoch in tqdm(range(epochs)):
model.train() # 将模型设置为训练模式
running_loss = 0.0
correct_train, total_train = 0, 0

for batch, (X, y) in enumerate(train_dataloader):
X, y = X.to(device), y.to(device)

optimizer.zero_grad() # 将梯度归零

output = model(X) # 前向传播
loss = loss_fn(outputs, y) # 计算损失

loss.backward() # 反向传播
optimizer.step() # 更新权重

running_loss += loss.item()
_, predicted = torch.max(outputs.data, 1)
total_train += y.size(0)
correct_train += (predicted == y).sum().item()

# 打印训练损失和准确率
print(f&quot;Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_dataloader):.4f}&quot;)
print(f&quot;Training Accuracy: {100 * correct_train / total_train:.2f}%&quot;)

# 验证
model.eval() # 将模型切换到评估模式进行验证
correct_val, total_val = 0, 0
with torch.no_grad():
for X_val, y_val in test_dataloader:
X_val, y_val = X_val.to(device), y_val.to(device)
val_outputs = model(X_val)
_, predicted = torch.max(val_outputs.data, 1)
total_val += y_val.size(0)
correct_val += (predicted == y_val).sum().item()

# 打印验证准确率
print(f&quot;验证准确率：{100 * correct_val / total_val:.2f}%&quot;)

return model

# 示例训练
NUM_EPOCHS = 30 # 根据您的需要调整 epoch要求
# 使用增强训练数据加载器
trained_model_resnet = train(model_resnet, train_dataloader_simple, test_dataloader_simple, optimizer, loss_fn, NUM_EPOCHS)


这是我在不同单元格中运行以保存模型的代码
# 保存训练好的模型
model_path = &quot;trained_resnet18_model.pth&quot;
torch.save(model_resnet.state_dict(), model_path)
print(f&quot;模型已保存到 {model_path}&quot;)

我正在训练一个机器学习模型，使用 PyTorch 将阿尔茨海默病分为四类。在经过几个 epoch 的训练后，我想使用类似 model.save(&#39;my_model.h5&#39;) 或 torch.save(model.state_dict(), &#39;model.pth&#39;) 的代码保存训练好的模型。
我是否应该将保存训练好的模型的代码添加到定义训练 epoch 和运行训练循环的同一单元格中，还是最好将其放在不同的单元格中？放置位置是否会影响模型的保存方式或我以后如何加载它？]]></description>
      <guid>https://stackoverflow.com/questions/79426066/ml-models-saving</guid>
      <pubDate>Mon, 10 Feb 2025 04:15:24 GMT</pubDate>
    </item>
    <item>
      <title>从 GPU 内存中清除 tf.data.Dataset</title>
      <link>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</guid>
      <pubDate>Fri, 07 Feb 2025 12:02:12 GMT</pubDate>
    </item>
    <item>
      <title>随机森林回归中的树木数量[关闭]</title>
      <link>https://stackoverflow.com/questions/56505551/number-of-trees-in-random-forest-regression</link>
      <description><![CDATA[我正在学习随机森林回归模型。我知道它会形成许多树（模型），然后我们可以通过平均所有树的结果来预测目标变量。我对决策树回归算法也有一定了解。我们如何才能形成最佳数量的树？
例如，我有一个数据集，我正在预测人员薪水，我只有两个输入变量，即“工作年限”和“绩效分数”，那么我可以使用这样的数据集形成多少棵随机树？随机森林树是否取决于输入变量的数量？任何好的例子都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/56505551/number-of-trees-in-random-forest-regression</guid>
      <pubDate>Sat, 08 Jun 2019 10:33:36 GMT</pubDate>
    </item>
    <item>
      <title>R 中回归森林中的特征选择和预测精度</title>
      <link>https://stackoverflow.com/questions/45935917/feature-selection-and-prediction-accuracy-in-regression-forest-in-r</link>
      <description><![CDATA[我正尝试解决一个回归问题，其中输入特征集的大小约为 54。
使用具有单个预测因子“X1”的 OLS 线性回归，我无法解释 Y 的变化 - 因此我尝试使用回归森林（即随机森林回归）查找其他重要特征。后来发现所选的“X1”是最重要的特征。
我的数据集有约 14500 个条目。我已将其按 9:1 的比例分为训练集和测试集。
我有以下问题：

在尝试查找重要特征时，我应该在整个数据集上运行回归森林，还是仅在训练数据上运行？

找到重要特征后，是否应使用前几个特征重新构建模型，以查看特征选择是否以较小的预测能力成本加快了计算速度？

目前，我已经使用训练集和所有特征构建了模型，并且我正在使用它对测试集进行预测。我正在从训练集中计算 MSE 和 R 平方。我在训练数据上获得了高 MSE 和低 R2，而在测试数据上则相反（如下所示）。这不寻常吗？


 &gt;森林 &lt;- randomForest(fmla, dTraining, ntree=501, significance=T)

&gt; 平均值((dTraining$y - 预测(森林, 数据=dTraining))^2)

&gt; &gt; 0.9371891

&gt; rSquared(dTraining$y, dTraining$y - 预测(森林, 数据=dTraining))

&gt; &gt; 0.7431078

&gt; 平均值((dTest$y - 预测(森林, newdata=dTest))^2)

&gt; &gt; 0.009771256

&gt; rSquared(dTest$y, dTest$y - 预测(森林, newdata=dTest))

&gt; &gt; 0.9950448

有没有建议说 R 平方和 MSE 是否是这个问题的良好指标，或者我是否需要查看其他指标来评估模型是否良好？]]></description>
      <guid>https://stackoverflow.com/questions/45935917/feature-selection-and-prediction-accuracy-in-regression-forest-in-r</guid>
      <pubDate>Tue, 29 Aug 2017 09:51:00 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的聚类时间序列数据</title>
      <link>https://stackoverflow.com/questions/45604143/clustering-time-series-data-in-python</link>
      <description><![CDATA[我尝试使用不同的聚类技术在 Python 中对时间序列数据进行聚类。K 均值法没有给出良好的结果。以下图像是我使用凝聚聚类法进行聚类后得到的图像。我还尝试了动态时间扭曲。这两个似乎给出了类似的结果。
我理想情况下希望第二幅图中的时间序列有两个不同的聚类。第一幅图像是快速增加的聚类。第二幅图像没有增加，有点稳定，第三幅图像是下降趋势的聚类。我想知道哪些时间序列既稳定又受欢迎（这里的流行是指高计数）。我尝试了层次聚类，但结果显示层次太多，我不确定如何选择层次级别。有人能解释一下如何将第二幅图中的时间序列分成两个不同的聚类，一个计数低，另一个计数高吗？可以做到吗？或者我应该直接选择一个阈值将它们一分为二？
快速增加的集群：

计数稳定的集群：

下降趋势的集群：

这非常非常模糊，但这是我的层次聚类的结果。 

我知道这个特定的图像根本没有用，但这对我来说也像是死胡同。 
一般来说，如果你想区分趋势，例如对于 YouTube 视频，如何只有一些被选入“趋势”部分，而另一些被选入“本周趋势”部分？我理解“趋势”部分的视频是那些与第一张图片具有相似特征的视频。 “本周热门”部分收集了一些视频，这些视频的观看次数非常高，但数量相当稳定（即没有出现快速增长）。我知道，对于 YouTube，除了观看次数之外，还有很多其他因素需要考虑。对于第二张图片，我试图做的类似于“本周热门”部分。我想挑选那些观看次数非常高的视频。在这种情况下，我该如何分割时间序列？
我知道 DTW 可以捕捉趋势。DTW 给出的结果与上面的图片相同。它已经确定了第二张图片中的趋势是“稳定的”。但它没有捕捉到这里的“数量”元素。我希望既能捕捉到趋势，又能捕捉到数量，在这种情况下是稳定和高数量。
上面的图片是基于计数聚类的时间序列。我是否错过了其他可以实现这一点的聚类技术？即使只是计数，我如何根据自己的需求进行不同的聚类？
任何想法都将不胜感激。提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/45604143/clustering-time-series-data-in-python</guid>
      <pubDate>Thu, 10 Aug 2017 03:51:00 GMT</pubDate>
    </item>
    <item>
      <title>Python 监督机器学习</title>
      <link>https://stackoverflow.com/questions/35438540/python-supervised-machine-learning</link>
      <description><![CDATA[我正在尝试了解如何使用 scikit 进行监督机器学习，因此我编写了一些属于两个不同集合的数据：集合 A 和集合 B。集合 A 中有 18 个元素，集合 B 中有 18 个元素。每个元素都有三个变量。见下文：
#SetA
Variable1A = [ 3,4,4,5,4,5,5,6,7,7,5,4,5,6,4,9,3,4]
Variable2A = [ 5,4,4,3,4,5,4,5,4,3,4,5,3,4,3,4,4,3]
Variable3A = [ 7,8,4,5,6,7,3,3,3,4,4,9,7,6,8,6,7,8]

#SetB
Variable1B = [ 7,8,11,12,7,9,8,7,8,11,15,9,7,6,9,9,7,11]
Variable2B = [ 1,2,3,3,4,2,4,1,0,1,2,1,3,4,3,1,2,3]
Variable3B = [ 12,18,14,15,16,17,13,13,14,14,19,17,16,18,16,17,18]

如何使用 scikit 使用监督机器学习，以便当我引入新的 setA 和 setB 数据时，它可以尝试识别哪些新数据属于 setA 或 setB。
抱歉，数据集很小且“虚构”。我只是想使用 scikit 在其他数据集上应用相同的方法。]]></description>
      <guid>https://stackoverflow.com/questions/35438540/python-supervised-machine-learning</guid>
      <pubDate>Tue, 16 Feb 2016 16:59:48 GMT</pubDate>
    </item>
    <item>
      <title>新闻文章聚类</title>
      <link>https://stackoverflow.com/questions/25228219/clustering-of-news-articles</link>
      <description><![CDATA[我的情况很简单：我有一堆新闻文章（目前约 1k 篇），我知道其中一些文章涵盖了相同的故事/主题。我现在想根据共同的故事/主题对这些文章进行分组，即根据它们的相似性。
到目前为止，我所做的是应用基本的 NLP 技术，包括停用词删除和词干提取。我还计算了每篇文章的 tf-idf 向量，并且还可以基于这些 tf-idf 向量计算余弦相似度。但现在对文章进行分组有点困难。我看到了两种主要方法——可能相关——来做到这一点：
1) 机器学习/聚类：我已经对现有的聚类库进行了一些尝试，或多或少取得了成功；请参阅此处。一方面，k-means 等算法需要簇数作为输入，而我不知道。其他算法需要的参数也不直观（对我来说是这样）。
2) 图形算法：我可以将我的数据表示为图形，其中文章是节点，加权边表示文章之间的成对（余弦）相似性。例如，我可以先删除所有低于某个阈值的边，然后可以应用图形算法来寻找强连通子图。
简而言之，我不确定从这里开始最好去哪里——我在这个领域还很新。我想知道是否存在一些最佳实践，或者某些方法/算法可以（不）应用于某些场景的指导方针。
（编辑：忘记链接到我的相关问题）]]></description>
      <guid>https://stackoverflow.com/questions/25228219/clustering-of-news-articles</guid>
      <pubDate>Sun, 10 Aug 2014 11:39:05 GMT</pubDate>
    </item>
    </channel>
</rss>