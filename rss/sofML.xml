<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 03 Aug 2024 18:20:02 GMT</lastBuildDate>
    <item>
      <title>无法解决 QiskitMachineLearningError：'输入数据的形状不正确，最后一个维度不等于输入的数量：0，但得到：2'</title>
      <link>https://stackoverflow.com/questions/78828998/not-able-to-resolve-qiskitmachinelearningerror-input-data-has-incorrect-shape</link>
      <description><![CDATA[我收到错误： ---&gt;
32 vqc.fit(X_train, X_test)
33
34 # 评估分类器
15 帧
/usr/local/lib/python3.10/dist-packages/qiskit_machine_learning/neural_networks/neural_network.py in _validate_input(self, input_data)
132
133 if shape[-1] != self._num_inputs:
--&gt; 134 引发 QiskitMachineLearningError(
135 f&quot;输入数据的形状不正确，最后一个维度&quot;
136 f&quot;不等于输入的数量：&quot;
QiskitMachineLearningError：&#39;输入数据的形状不正确，最后一个维度不等于输入的数量：0，但得到：2。&#39;
来自 qiskit 导入 QuantumCircuit、transpile、assemble
来自 qiskit_aer 导入 Aer
来自 qiskit_machine_learning.algorithms 导入 VQC
来自 qiskit_algorithms.optimizers 导入 COBYLA
来自 qiskit.circuit.library 导入 TwoLocal
来自 sklearn.preprocessing 导入 StandardScaler
来自 sklearn.model_selection 导入 train_test_split
导入 numpy 作为 np

# 用于演示目的的样本数据
# 将其替换为您的实际数据
scaled_data = np.random.rand(150, 2) # 用您的缩放数据替换
y = np.random.randint(0, 3, size=150) # 用您的标签替换

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(scaled_data, y, test_size=0.3, random_state=42)

# 根据特征维度定义量子比特的数量
num_qubits = X_train.shape[1]

# 使用正确的参数定义量子特征图和 ansatz
feature_map = TwoLocal(num_qubits=num_qubits, entanglement=&#39;linear&#39;) #, rotation_blocks=[&#39;ry&#39;, &#39;rz&#39;], entanglement_gate=&#39;cz&#39;)
ansatz = TwoLocal(num_qubits=num_qubits, entanglement=&#39;linear&#39;)#, rotation_blocks=[&#39;ry&#39;, &#39;rz&#39;], entanglement_gate=&#39;cz&#39;)

# 定义优化器
optimizer = COBYLA()

# 使用唯一参数名称初始化 VQC 分类器
vqc = VQC(feature_map=feature_map, ansatz=ansatz, optimizer=optimizer)

# 训练量子分类器
vqc.fit(X_train, X_test)

# 评估分类器
accuracy = vqc.score(X_test, y_test)
print(f&#39;Quantum Classifier Accuracy: {accuracy:.2f}&#39;)

# 进行预测的示例
predictions = vqc.predict(X_test)
print(&#39;Predictions:&#39;, predictions)
]]></description>
      <guid>https://stackoverflow.com/questions/78828998/not-able-to-resolve-qiskitmachinelearningerror-input-data-has-incorrect-shape</guid>
      <pubDate>Sat, 03 Aug 2024 14:52:35 GMT</pubDate>
    </item>
    <item>
      <title>R 中用于分类任务的 Brier 分数</title>
      <link>https://stackoverflow.com/questions/78828870/brier-score-in-r-for-a-classification-task</link>
      <description><![CDATA[我针对分类问题训练了几个模型，我想计算我所做预测的 Brier 分数。
尽管如此，我不太确定应该将什么传递给 DescTools 中的公式（即 BrierScore(x, pred = NULL, scaled = FALSE, ...)）。
到目前为止，这是我为其中一个模型的概率创建的数据框：
 X0 X1 real_scores max_prob max_source
1 0.024 0.976 1 0.976 1
2 0.910 0.090 0 0.910 0
3 0.524 0.476 0 0.524 0
4 0.942 0.058 0 0.942 0
5 0.944 0.056 0 0.944 0
6 0.074 0.926 1 0.926          1 7 0.254 0.746 0 0.746 1 8 0.864 0.136 0 0.864 0 9 0.522 0.478 0 0.522 0 10 0.422 0.578 1 0.578 1 11 0.772 0.228 0 0.772 0 12 0.564 0.436 0 0.564 0 13 0.968 0.032 0 0.968 0 14 0.760 0.240 0 0.760 0 15 0.978 0.022 0 0.978 0 16 0.818 0.182 1    0.818 0 17 0.730 0.270 0 0.730 0 18 0.824 0.176 0 0.824 0 19 0.962 0.038 0 0.962 0 20 0.514 0.486 0 0.514 0 21 0.360 0.640 0 0.640 1 22 0.708 0.292 0 0.708 0 23 0.940 0.060 0 0.940 0 24 0.916 0.084 0 0.916 0 25 0.606 0.394 1 0.606 0 26 0.838 0.162 0 0.838 0
27 0.742 0.258 0 0.742 0
28 0.850 0.150 1 0.850 0

其中：

X0 = 负类的概率
X1 = 正类的概率
real_scores = 1（正类）；0（负类）
max_prob = 两个类（X0、X1）的最大概率
max_source = 最大概率来自哪个类别（X0 或 X1）。

我不明白的是，我是否必须仅将 X1（正类）的概率、两者的概率或最大值传递给公式。]]></description>
      <guid>https://stackoverflow.com/questions/78828870/brier-score-in-r-for-a-classification-task</guid>
      <pubDate>Sat, 03 Aug 2024 14:02:24 GMT</pubDate>
    </item>
    <item>
      <title>Kaldi：对齐脚本未生成输出（ali.1.gz）</title>
      <link>https://stackoverflow.com/questions/78828864/kaldi-alignment-script-not-generating-output-ali-1-gz</link>
      <description><![CDATA[我正在尝试使用 Kaldi 训练语音识别模型。我已成功运行 steps/nnet3/align.sh 脚本，但 exp/chain/tree_sp 目录中未创建预期的 ali.1.gz 文件。
我已检查 exp/chain/tree_sp/log 目录中的终端输出和日志文件，但没有错误消息。该脚本似乎运行正常，但缺少所需的输出。
您能否建议此问题的潜在原因或进一步调试的步骤？如何解决？
我已成功运行 steps/nnet3/align.sh 脚本（终端或日志文件中没有错误）。
我已经检查过 exp/chain/tree_sp 目录中是否存在 ali.JOB.gz 文件，但它们不存在（未创建）
我希望 steps/nnet3/align.sh 脚本能够在 exp/chain/tree_sp 目录中成功创建 ali.1.gz 文件（或多个 ali.JOB.gz 文件）。此文件对于 Kaldi 管道中的后续训练步骤至关重要。
&quot;ls exp/chain/tree_sp
0.mdl cmvn_opts final.mat final.mdl final.occs full.mat log num_jobs phone.txt splice_opts tree&quot;
当我运行 local/chain/tuning/run_tdnn_1j.sh 时出现此错误
&quot;Traceback (most recent call last):
File &quot;/mnt/d/kaldi/egs/mini_librispeech/s5/steps/nnet3/chain/train.py&quot;, line 651, in main
train(args, run_opts)
File &quot;/mnt/d/kaldi/egs/mini_librispeech/s5/steps/nnet3/chain/train.py&quot;, line 287, in训练
chain_lib.check_for_required_files(args.feat_dir, args.tree_dir,
文件“/mnt/d/kaldi/egs/mini_librispeech/s5/steps/libs/nnet3/train/chain_objf/acoustic_model.py”，第 378 行，位于 check_for_required_files
引发异常（&#39;预期 {0} 存在。&#39;.format(file))
异常：预期 exp/chain/tree_sp/ali.1.gz 存在。“]]></description>
      <guid>https://stackoverflow.com/questions/78828864/kaldi-alignment-script-not-generating-output-ali-1-gz</guid>
      <pubDate>Sat, 03 Aug 2024 14:01:13 GMT</pubDate>
    </item>
    <item>
      <title>想要对 SAM 等预训练模型进行微调，以便在微观水样数据集中进行细菌分割。很难找到所需的数据集</title>
      <link>https://stackoverflow.com/questions/78828560/want-to-fine-tune-a-pretrained-model-like-sam-for-bacterial-segmentation-in-micr</link>
      <description><![CDATA[我正在开展一个机器学习项目，旨在使用显微图像测试水的纯度。该项目的目标是：
对样本图像中存在的各种细菌进行分割。识别不同类型的细菌。根据识别出的细菌数量和类型评估水的纯度。
我很难找到并准备一个包含不同类型细菌的水样显微图像的合适数据集。
考虑到我的任务的特殊性，我应该采取什么方法来微调像 SAM（任何分割模型）这样的预训练模型来进行细菌分割？任何关于超参数、数据增强或其他训练策略的提示都会有所帮助。
获取数据集困难。]]></description>
      <guid>https://stackoverflow.com/questions/78828560/want-to-fine-tune-a-pretrained-model-like-sam-for-bacterial-segmentation-in-micr</guid>
      <pubDate>Sat, 03 Aug 2024 11:32:03 GMT</pubDate>
    </item>
    <item>
      <title>当数据集中的每个数据都是 csv 文件时，机器学习方法</title>
      <link>https://stackoverflow.com/questions/78828422/machine-learning-methode-for-when-each-data-in-dataset-is-a-csv-file</link>
      <description><![CDATA[我目前正在开展一个项目，该项目涉及使用传感器测量金属物体周围的磁流。传感器每毫秒记录一次磁流，每秒可进行 1000 次测量。这些值存储在 CSV 文件中，其中第一列表示沿 X 轴的测量值，第二列对应于 Y 轴测量值，第三列表示 Z 轴测量值，第四列包含以毫秒为单位的时间。
我的任务是测量 50 种不同金属物体的磁流，为每个物体创建一个单独的 CSV 文件。最终，我计划将这些单独的文件用作训练机器学习模型的数据集。目标是使用机器学习和这些数据根据金属的磁流特性确定金属的类型（如果您感兴趣，可以搜索“mfl 方法”）。但是，我不确定如何处理这种特殊情况，因为大多数机器学习代码都要求数据集中的每个数据点都是一行。在这种情况下，每个数据点都是一个包含多行的 CSV 文件。
如果您能提供任何指导，我将不胜感激
目前我不知道该怎么做]]></description>
      <guid>https://stackoverflow.com/questions/78828422/machine-learning-methode-for-when-each-data-in-dataset-is-a-csv-file</guid>
      <pubDate>Sat, 03 Aug 2024 10:09:57 GMT</pubDate>
    </item>
    <item>
      <title>“AttributeError:‘NoneType’对象没有属性‘cget_managed_ptr’”是什么意思？</title>
      <link>https://stackoverflow.com/questions/78827974/what-does-the-attributeerror-nonetype-object-has-no-attribute-cget-managed</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78827974/what-does-the-attributeerror-nonetype-object-has-no-attribute-cget-managed</guid>
      <pubDate>Sat, 03 Aug 2024 06:12:06 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中有效地将大型 .txt 文件拆分为训练集和测试集？</title>
      <link>https://stackoverflow.com/questions/78827762/how-can-i-efficiently-split-a-large-txt-file-into-training-and-test-sets-in-pyt</link>
      <description><![CDATA[我有一个非常大的 .txt 文件（几 GB），我需要将其拆分为机器学习项目的训练集和测试集。由于内存限制，将整个文件读入内存然后拆分的常用方法不可行。我正在寻找一种高效拆分文件而不使内存过载的方法。
我尝试使用 scikit-learn 进行拆分，但它会将整个文件加载到内存中，这会导致性能问题，不适合我的大型数据集。]]></description>
      <guid>https://stackoverflow.com/questions/78827762/how-can-i-efficiently-split-a-large-txt-file-into-training-and-test-sets-in-pyt</guid>
      <pubDate>Sat, 03 Aug 2024 03:36:13 GMT</pubDate>
    </item>
    <item>
      <title>独热编码掩码的 resample_poly</title>
      <link>https://stackoverflow.com/questions/78827743/resample-poly-of-one-hot-encoded-masking</link>
      <description><![CDATA[我有这些张量：
X_test = X_unseen_flutter[0,0,:][None, :] # (批次大小，振幅长度) -&gt; (1, 3208)
y_true = y_unseen_flutter[0,0,:][None, :] # (批次大小，掩码长度，类别数量) -&gt; (1, 3208, 4) (独热编码)

我可以对 X_test 进行重新采样，但我不知道 y_true：
from scipy.signal import resample_poly

X_test_resampled = resample_poly(X_test, up=512, down=3208, axis=1) # (1, 512)
y_true_resampled = # ??? 我期望形状 (1, 512, 4)

除了独热编码标签外，resample_poly 的等价物是什么？
我希望有一个函数可以做到这一点，它接受 tensor, up, down, mask_axis, class_axis]]></description>
      <guid>https://stackoverflow.com/questions/78827743/resample-poly-of-one-hot-encoded-masking</guid>
      <pubDate>Sat, 03 Aug 2024 03:21:27 GMT</pubDate>
    </item>
    <item>
      <title>CSV 与 Pandas Dataframe 之间的转换不正确</title>
      <link>https://stackoverflow.com/questions/78827542/csv-to-from-pandas-dataframe-not-transforming-correctly</link>
      <description><![CDATA[我有一个 csv 文件，其中包含标题、文本和 url 列的新闻文章。当我将文件导入 pandas df 时，长度似乎与 csv 文件中的行数不同。经过检查，我注意到一些文章被分成第二行，并进一步分成许多额外的列，所有 url 都位于单个文章的“第二”行的某个较远的列中。Pandas 正确地解释了这个问题，并合并了文本并将 url 放在正确的列下。
虽然 Pandas 正确地解释了这一点，但我无法将 df（清理后）保存到新的 csv，因为新的清理后的 csv 文件不会反映相同的问题，这会破坏清理后的特征工程和 NLP 任务。这些“额外”的行完全搞砸了诸如计算单词/动词/名词以及获取每个句子的被动语态和音节之类的事情。我尝试过检测/删除换行符（\n 和 \r\n），但没有用。我在读取 csv 时尝试过不同的编码和引用值（见下文），但没有用。我尝试过合并行，但做不到，因为 pandas 看不到 csv 中显示的“第二”行。
我遗漏了什么吗？知道发生了什么吗？
df = pd.read_csv(&#39;filepath.csv&#39;, encoding=&#39;utf-8&#39;, engine=&#39;python&#39;, on_bad_lines=&#39;skip&#39;, quoting=1)

df= df.rename(columns={&#39;Headline&#39;: &#39;title&#39;, &#39;Article text&#39;: &#39;text&#39;, &#39;Url&#39;: &#39;url&#39;})

df= df[[&#39;title&#39;, &#39;text&#39;, &#39;url&#39;]]

df[&#39;text&#39;] = df[&#39;text&#39;].str.replace(&#39;\r\n&#39;, &#39; &#39;, regex=True)
df.to_csv(&#39;df_investigation.csv&#39;, index=False) 
]]></description>
      <guid>https://stackoverflow.com/questions/78827542/csv-to-from-pandas-dataframe-not-transforming-correctly</guid>
      <pubDate>Fri, 02 Aug 2024 23:42:35 GMT</pubDate>
    </item>
    <item>
      <title>无法抑制来自 transformers/src/transformers/modeling_utils.py 的警告</title>
      <link>https://stackoverflow.com/questions/78827482/cant-suppress-warning-from-transformers-src-transformers-modeling-utils-py</link>
      <description><![CDATA[我对 AutoModel AutoTokenizer 类的实现相当简单：
from transformers import AutoModel, AutoTokenizer
import numpy as np
from rank_bm25 import BM25Okapi
from sklearn.neighbors import NearestNeighbors

class EmbeddingModels:

def bert(self, model_name, text):
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
input = tokenizer(text, return_tensors=&quot;pt&quot;, truncation=True, padding=True)
output = model(**inputs)
embeddings = output.last_hidden_​​state.mean(dim=1).detach().numpy()
return embeddings

def create_chunks(self, text, chunk_size):
return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

但我无法让这个警告消失：
包含“beta”的参数名称将在内部重命名为“bias”。
请使用其他名称来抑制此警告。
包含“gamma”的参数名称将在内部重命名为“weight”。
请使用其他名称来抑制此警告。

我的存储库中没有任何地方提到 beta 或 gamma 这个词。
更新软件包，使用 import warnings 抑制警告]]></description>
      <guid>https://stackoverflow.com/questions/78827482/cant-suppress-warning-from-transformers-src-transformers-modeling-utils-py</guid>
      <pubDate>Fri, 02 Aug 2024 23:04:25 GMT</pubDate>
    </item>
    <item>
      <title>VAE：潜在分布。后部塌陷，多个潜在 [关闭]</title>
      <link>https://stackoverflow.com/questions/78827397/vae-latent-distribution-posterior-collapse-multiple-latents</link>
      <description><![CDATA[在探索 VAE 一段时间后，我有两个问题。在标准 VAE 设置中，我们假设 1 个形状为 (BHWD) 的潜在变量：mu 和 var，以及先验 N(0, I)。

潜在分布：我阅读了一些关于卡方分布的资料，想知道潜在变量的 L2 范数 (B,) 是否是潜在变量呈高斯分布的良好指标。在标准 VAE 训练中，我发现它的值稳定在 (D-1)**0.5 附近，这符合中心卡方分布的描述。接下来的问题是，如果 L2 范数不等于预期值，我们可以说潜在分布比高斯分布更复杂吗？

后验崩溃：（1）后验崩溃的症状是什么？它是否必须严格为：mu~0、var~1 和 KL~0？（2）如何解释 var 的平均值。我观察到它也受到 D 的影响。此外，如果极小的 var 表示模型对输入非常确定，爆炸的 var 会告诉我们编码器缺乏能力？一般来说，我们更喜欢较小的 var 还是存在一个理想值。

给定一张图像，我尝试将其转换为 Y/UV 通道并分别学习两组潜在值。具体来说，我对两者应用了标准流程：使用一个/两个编码器为 Y 和 UV 输入生成 mu 和 var，分别计算两个相对于正常 proir 的 KL，对两者进行后验采样，在解码之前将它们连接在一起。解码器的工作是重建原始 RGB 图像。 我想，如果 VAE 能够为图像构建可插值/平滑的潜在空间，它也应该能够处理 Y/UV 潜在值并将来自同一图像的潜在值对齐。 不幸的是，我在实验中没有观察到它。重建很好，但潜在的统计数据（为方便起见，我将它们标记为 1 和 2）非常混乱。 (1) 很难从 mu1 和 mu2 读取，因为它们的值非常接近 0。但是，我总是能发现 var2 爆炸（请参阅我在第 2 点中提出的问题），可能高达 200。 (2) 通常，var1 看起来更像高斯，因为它的 L2 范数在 (d-1)**0.5 处收敛，但 var2 的值稍大一些（请参阅我在第 1 点中提出的问题）。 (3) 我还计算了 mu1 和 mu2 之间的 cos 相似度 和 L2 距离。它们大多是正交的，这与高维向量自然彼此正交的说法相符。并且 L2(mu1, mu2) 和 L2(mu2, origin) 具有相似的值，大于 L1(mu1, origin)。消化这些统计数据的正确方法是什么？或者一般来说，VAE 框架不适合学习两个独立但相关的高斯类潜在变量？


进行了大量实验并尝试更好地理解高维潜在空间。]]></description>
      <guid>https://stackoverflow.com/questions/78827397/vae-latent-distribution-posterior-collapse-multiple-latents</guid>
      <pubDate>Fri, 02 Aug 2024 22:17:58 GMT</pubDate>
    </item>
    <item>
      <title>除非 label_mode=None，否则图像目录无法打开</title>
      <link>https://stackoverflow.com/questions/78827338/image-directory-failing-to-open-unless-label-mode-none</link>
      <description><![CDATA[除非 image_dataset_from_directory 的 label_mode 参数为 None，否则我的图像目录无法打开。
我尝试将类型切换为“int”、“categorical”和“binary”，但无济于事。我还尝试为该类制作一个包装器，然后稍后切换属性（事后看来，这根本行不通），但这也行不通。我需要以某种方式将其保持在 None 之外，因为我稍后会评估生成器的准确性，而 model.evaluate() 无法处理 None 值。这是我目前的代码：
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import tensorflow
from tensorflow import keras
from keras import layer, preprocessing, Sequential
from sklearn.neighbors import KernelDensity
import glob

class CustomImageDataset:
def __init__(self, directory, image_size, batch_size, label_mode):
self.dataset = tensorflow.keras.preprocessing.image_dataset_from_directory(
directory,
image_size=image_size,
batch_size=batch_size,
label_mode=label_mode
)
self.label_mode = label_mode

def __iter__(self):
return iter(self.dataset)

def __len__(self):
return len(self.dataset)

def map(self, *args, **kwargs):
返回 self.dataset.map(*args, **kwargs)

def batch(self, *args, **kwargs):
返回 self.dataset.batch(*args, **kwargs)

def prefetch(self, *args, **kwargs):
返回 self.dataset.prefetch(*args, **kwargs)

SIZE = 8
batch_size = 64

train_generator = preprocessing.image_dataset_from_directory(
r&#39;C:\Users\{}\Downloads\archive (1)\noncloud_train&#39;, 
image_size=(SIZE, SIZE),
batch_size=batch_size,
label_mode=None
)

validation_generator = preprocessing.image_dataset_from_directory(
r&#39;C:\Users\{}\Downloads\archive (1)\noncloud_test&#39;,
image_size=(SIZE, SIZE),
batch_size=batch_size,
label_mode=None
)

anomaly_generator = CustomImageDataset(
r&#39;C:\Users\{}\Downloads\archive (1)\cloud&#39;,
image_size=(SIZE, SIZE),
batch_size=batch_size,
label_mode=None
)

rescaling_layer = layer.Rescaling(1./255)

def change_inputs(images):
x = tensorflow.image.resize(rescaling_layer(images),[SIZE, SIZE], method=tensorflow.image.ResizeMethod.NEAREST_NEIGHBOR)
return x, x

# 对数据集应用预处理
train_dataset = train_generator.map(change_inputs)
validation_dataset = validation_generator.map(change_inputs)
anomaly_dataset = anomaly_generator.map(change_inputs)

test = anomaly_generator.label_mode
def check_none_in_dataset(dataset):
for batch in dataset:
images, labels = batch
if images is None or labels is None:
print(&quot;Found None in dataset&quot;)
return True
print(&quot;No None values in dataset&quot;)
return False

# 检查验证数据集
print(&quot;正在检查验证数据集中是否有 None 值：&quot;)
c = check_none_in_dataset(validation_dataset)
print(c)

def print_labels_from_dataset(dataset, num_batches=1):
for images, labels in dataset.take(num_batches):
print(&quot;标签（应与images):&quot;)
print(labels.numpy()) # 打印标签以检查它们是否是预期值（不是 None）
print(labels.numpy() == images.numpy())

print(&quot;验证数据集标签：&quot;)
bat = print_labels_from_dataset(validation_dataset)

print(&quot;异常数据集标签：&quot;)
cow = print_labels_from_dataset(anomaly_dataset)

model = Sequential()
# 编码器
model.add(layers.Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;,input_shape=(SIZE, SIZE, 3)))
model.add(layers.MaxPooling2D((2, 2),padding=&#39;same&#39;)) 
model.add(layers.Conv2D(32, (3, 3),激活=&#39;relu&#39;，padding=&#39;same&#39;))
model.add(layers.MaxPooling2D((2, 2)，padding=&#39;same&#39;))
model.add(layers.Conv2D(16, (3, 3)，激活=&#39;relu&#39;，padding=&#39;same&#39;))
model.add(layers.MaxPooling2D((2, 2)，padding=&#39;same&#39;))

# Deconder
model.add(layers.Conv2D(16, (3, 3)，激活=&#39;relu&#39;，padding=&#39;same&#39;))
model.add(layers.UpSampling2D((2, 2)))
model.add(layers.Conv2D(32, (3, 3)，激活=&#39;relu&#39;，padding=&#39;same&#39;))
model.add(layers.UpSampling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;))
model.add(layers.UpSampling2D((2, 2)))

model.add(layers.Conv2D(3, (3, 3), 激活=&#39;sigmoid&#39;, 填充=&#39;same&#39;))

model.compile(optimizer=&#39;adam&#39;, 损失=&#39;mean_squared_error&#39;, 指标=[&#39;mse&#39;])
model.summary()

history = model.fit(
train_dataset,
steps_per_epoch = 1500 // batch_size,
epochs = 1000,
validation_data = validation_dataset,
validation_steps = 225 // batch_size,
shuffle = True
)

# 检查侦察。 val 数据和异常图像之间的错误
validation_error = model.evaluate(validation_generator)
anomaly_error = model.evaluate(anomaly_generator)
]]></description>
      <guid>https://stackoverflow.com/questions/78827338/image-directory-failing-to-open-unless-label-mode-none</guid>
      <pubDate>Fri, 02 Aug 2024 21:47:44 GMT</pubDate>
    </item>
    <item>
      <title>PipeOp classif.avg（mlr3）错误：对“prob”的断言失败：包含缺失值（元素 1）[关闭]</title>
      <link>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</link>
      <description><![CDATA[当我运行代码时，该代码在堆叠学习器（glmnet 和 rpart）上执行特征选择和超参数调整，我收到以下错误消息：
assert_binary(truth, prob = prob, positive = positive, na_value = na_value) 中出错：
“prob”上的断言失败：包含缺失值（元素 1）。
这发生在 PipeOp classif.avg 的 $train()

但是，当我使用 classif.debug 时，预测中没有 NA。任何建议都将不胜感激。
注意：我简化了要调整的参数数量和要选择的特征数量，以减少执行时间，现在使用 classif.debug 只需 15 秒。
这是我的数据：https://www.dropbox.com/scl/fi/hkjs79i89gjz0j5mjlbj8/Data.csv?rlkey=08yuzet3mjr9gcezkryo93vqm&amp;st=hfv2cbeo&amp;dl=0
这是我的代码：
set.seed(1)
data &lt;- read.csv(&quot;C:/Users/Marine/Downloads/Data.csv&quot;)
data &lt;- data[,c(&quot;x&quot;, &quot;y&quot;, &quot;presence&quot;, &quot;V01&quot;, &quot;V02&quot;)]
## dim(data)
data$presence &lt;- as.factor(data$presence)
##摘要（数据）
任务 &lt;- mlr3spatial::as_task_classif_st（x = 数据，目标 = “存在”，正 = “1”，坐标名称 = c（“x”，“y”），crs = “+proj=longlat +datum=WGS84 +no_defs +type=crs”）
摘要（任务）

learner_glmnet &lt;- mlr3::lrn（“classif.glmnet”，预测类型 = “prob”，s = 0.01）
learner_rpart &lt;- mlr3::lrn（“classif.rpart”，预测类型 = “prob”，cp = to_tune（1e-04，1e-1，对数尺度 = TRUE))
learner_glmnet_cv &lt;- mlr3pipelines::PipeOpLearnerCV$new(learner = learner_glmnet, id = &quot;glmnet_cv&quot;, param_vals = list(resampling.method = &quot;cv&quot;, resampling.folds = 2))
learner_rpart_cv &lt;- mlr3pipelines::PipeOpLearnerCV$new(learner = learner_rpart, id = &quot;rpart_cv&quot;, param_vals = list(resampling.method = &quot;cv&quot;, resampling.folds = 2))

learner_avg &lt;- mlr3pipelines::LearnerClassifAvg$new(id = &quot;classif.avg&quot;)
learner_avg$predict_type &lt;- &quot;prob&quot;
learner_avg$param_set$values$measure &lt;- &quot;classif.auc&quot;

learner_debug &lt;- lrn(&quot;classif.debug&quot;, predict_type = &quot;prob&quot;)

level_0_graph &lt;- mlr3pipelines::gunion(list(learner_glmnet_cv, learner_rpart_cv)) %&gt;&gt;% mlr3pipelines::po(&quot;featureunion&quot;)
level_0_and_1_graph &lt;- level_0_graph %&gt;&gt;% learner_avg
## level_0_and_1_graph &lt;- level_0_graph %&gt;&gt;% learner_debug
level_0_and_1_graph_learner &lt;- mlr3::as_learner(level_0_and_1_graph)

tuning &lt;- mlr3tuning::auto_tuner(tuner = mlr3tuning::tnr(&quot;grid_search&quot;), 
learner = level_0_and_1_graph_learner,
resampling = mlr3::rsmp(&quot;cv&quot;, folds = 2),
measure = mlr3::msr(&quot;classif.auc&quot;),
terminator = mlr3tuning::trm(&quot;evals&quot;, n_evals = 2, k = 0))

feature_selection &lt;- mlr3fselect::auto_fselector(fselector = mlr3fselect::fs(&quot;sequence&quot;, strategies = &quot;sfs&quot;, min_features = 2),
learner = tuning,
resampling = mlr3::rsmp(&quot;cv&quot;, folds = 2),
measure = mlr3::msr(&quot;classif.auc&quot;),
terminator = mlr3tuning::trm(&quot;evals&quot;, n_evals = 2, k = 0))

system.time(stacking &lt;- mlr3::resample(task = task, 
learner = feature_selection, 
resampling = mlr3::rsmp(&quot;cv&quot;, folds = 2),
store_models = TRUE))

测试 &lt;- as.data.table(stacking$prediction())
which(is.na(test))
测试 &lt;- as.data.table(stacking$predictions()[[1]])
which(is.na(test))
测试 &lt;- as.data.table(stacking$predictions()[[2]])
which(is.na(test))
]]></description>
      <guid>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</guid>
      <pubDate>Thu, 18 Jul 2024 07:56:11 GMT</pubDate>
    </item>
    <item>
      <title>优化大数据集上的 Pandas 性能</title>
      <link>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</link>
      <description><![CDATA[我正在使用 pandas 处理一个大型数据集（约 1000 万行和 50 列），在数据操作和分析过程中遇到了严重的性能问题。这些操作包括过滤、合并和聚合数据，目前执行时间太长。
我读过几种优化技术，但不确定哪种技术最有效且适用于我的情况。以下是有关我的工作流程的一些细节：
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台具有 16GB RAM 的机器上运行分析。
社区能否分享优化 pandas 在大型数据集上的性能的最佳实践？
1.内存管理技术。
2.执行 groupby 和 apply 的有效方法。
3.处理大型数据集的 pandas 替代方案。
4. 有没有关于并行处理或有效利用多核的技巧。
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台有 16GB RAM 的机器上运行分析。]]></description>
      <guid>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</guid>
      <pubDate>Tue, 16 Jul 2024 02:24:48 GMT</pubDate>
    </item>
    <item>
      <title>LangChain 与 AmzonBedrock</title>
      <link>https://stackoverflow.com/questions/78705377/langchain-with-amzonbedrock</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78705377/langchain-with-amzonbedrock</guid>
      <pubDate>Thu, 04 Jul 2024 06:30:23 GMT</pubDate>
    </item>
    </channel>
</rss>