<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 13 Jan 2024 03:14:37 GMT</lastBuildDate>
    <item>
      <title>如何使用 XGBoost 改进建模？</title>
      <link>https://stackoverflow.com/questions/77809914/how-to-improve-modeling-with-xgboost</link>
      <description><![CDATA[我正在尝试使用 XGBoost 建模时间序列，但我觉得我的数据集非常不连续（如下图）。该数据集记录了 2018 年至 2023 年测量的可可价格价值。
数据集可以在我的 GitHub 存储库中找到：
https://raw.githubusercontent。 com/nunesisabella/Analise-Preditiv-Cacau/main/Cacau%20NY%20Futuros_2018-2023.csv
建模如下：
model_XGB = xgb.XGBRegressor(n_estimators=1000，early_stopping_rounds = 50)

model_XGB.fit(X_train, y_train,
          eval_set=[(X_train, y_train), (X_test, y_test)],
          详细=20)

是否可以通过改进超参数或类似的方法来使测试集获得更好的结果？或者我的数据集确实缺乏季节性，即使是好的建模方法也很难处理它？&lt;​​/p&gt;
]]></description>
      <guid>https://stackoverflow.com/questions/77809914/how-to-improve-modeling-with-xgboost</guid>
      <pubDate>Sat, 13 Jan 2024 00:36:40 GMT</pubDate>
    </item>
    <item>
      <title>用于比较两个向量进行回归的损失函数[关闭]</title>
      <link>https://stackoverflow.com/questions/77809542/loss-function-for-comparing-two-vectors-for-regression</link>
      <description><![CDATA[我想创建一个损失函数来训练我的模型进行回归，其中预测的 Y&#39; 应等于标签 Y。其中 Y 和 Y&#39; 是 (1,16) 二进制胜利者（1 或 0 ）的向量。 
我应该使用binary_cross_entropy还是MSE？
PS：输入和输出之间没有关系，我想通过使用模型从 X 预测 Y 来在它们之间建立关系。
示例：
&lt;前&gt;&lt;代码&gt;X=[0,1,0,1,1,0,1,1,0,1,1,0,1,0,1,0,0,0,0,1,1 ,0,0,0,1,1,0,1,0,1,0,0]
Y = [1,1,1,0,0,0,1,1,1,1,1,0,1,0,0,1]

错误预测= [1,1,1,0,0,0,1,1,1,1,1,0,0,0,0,0]（在 2 中抖动）位）。
所以我想从 X 预测 Y ...什么是最好的成本函数？]]></description>
      <guid>https://stackoverflow.com/questions/77809542/loss-function-for-comparing-two-vectors-for-regression</guid>
      <pubDate>Fri, 12 Jan 2024 22:18:52 GMT</pubDate>
    </item>
    <item>
      <title>两步 ML 预测模型（二元然后连续）：第二步是否应该仅针对正因变量进行训练？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77809477/two-step-ml-prediction-model-binary-then-continuous-should-second-step-be-tra</link>
      <description><![CDATA[我正在创建一个两步 ML 预测模型来预测库存产品的销售情况。我的训练集中有很多销售额要么为 0，要么为负。第一步是一个二元模型，用于预测产品是否应该库存，即预期销量为正 (1) 或不库存，即（销量为负或 0）(0)。
第二步是使用连续模型来预测预计将从库存中受益的商品的销售情况。在这种情况下，我应该在第二步中在整个训练集上训练我的模型，还是只训练训练集中 sales&gt;0 的那些行？
我做了后者，但不确定我是否应该在第二步中使用完整的数据集。]]></description>
      <guid>https://stackoverflow.com/questions/77809477/two-step-ml-prediction-model-binary-then-continuous-should-second-step-be-tra</guid>
      <pubDate>Fri, 12 Jan 2024 21:59:58 GMT</pubDate>
    </item>
    <item>
      <title>citeseer和cora（图链接预测），代码准确性问题[关闭]</title>
      <link>https://stackoverflow.com/questions/77808697/citeseer-and-cora-graph-link-prediciton-problem-whith-code-accuracy</link>
      <description><![CDATA[大家好，我有一个正在训练 citeseer 数据集的代码，该数据集的准确率约为 62%，我需要将其至少提高到 75%，或者使用其中的另一个数据集，该数据集的准确率超过 80%，我使用的 cora 的准确率为 82%准确性，但我需要另一个数据集，或者正如我所说，使 citeseer 75
导入networkx为nx
将 numpy 导入为 np
从 sklearn.model_selection 导入 train_test_split，GridSearchCV
从 sklearn.metrics 导入 precision_score
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.svm 导入 SVC
从 Node2Vec 导入 Node2Vec
将 pandas 导入为 pd
导入操作系统

# 选择数据集 [ Comic, citeseer, cora ]
数据集=“漫画”

os.system(“cls”)
print(&quot;正在加载数据集...&quot;)
# 加载节点和边数据集
如果数据集==“漫画”：
    G = nx.有向图()
    对于索引，pd.read_csv(&#39;comic_nodes.csv&#39;).iterrows():G.add_node(row[&#39;node&#39;], type=row[&#39;type&#39;]) 中的行
    对于索引，行 in pd.read_csv(&#39;comic_edges.csv&#39;).iterrows():G.add_edge(row[&#39;hero&#39;], row[&#39;comic&#39;])
    node_features = np.array([G.nodes[node].get(&#39;type&#39;, &#39;unknown&#39;) for Node in G.nodes()])
    node_labels = np.array([G.nodes[node].get(&#39;type&#39;, &#39;unknown&#39;) for Node in G.nodes()])
    node_index = {node_id: i for i, node_id in enumerate(G.nodes())}
别的：
    G = nx.read_edgelist(f&quot;{dataset}.cites&quot;, create_using=nx.DiGraph())
    node_data = np.loadtxt(f&quot;{dataset}.content&quot;, dtype=str)
    node_data = [node_data 中数据的数据，如果 G.nodes() 中的数据[0]]
    node_ids, node_features, node_labels = zip(*[(data[0], data[1:-1].astype(int), data[-1]) 对于node_data中的数据])
    node_index = {node_id: i for i, node_id in enumerate(node_ids)}
节点= [node_id为node_id，_排序（node_index.items（），key = lambda item：item [1]）]

os.system(“cls”)
# 使用node2vec生成游走
node2vec = Node2Vec(G，维度=256，walk_length=70，num_walks=200，工人=4)
模型 = node2vec.fit(窗口=20, min_count=0, sg=1, epochs=20)
embeddings = np.array([model.wv[node] 用于节点中的节点])

# 将数据分成训练集和测试集（20%用于测试）
X_train，X_test，y_train，y_test = train_test_split（嵌入，node_labels，test_size = 0.01，random_state = 42）
缩放 = StandardScaler()
X_train = Scale.fit_transform(X_train)
X_test = Scale.transform(X_test)

best_params = {&#39;C&#39;:8}
##### 网格搜索
# grid_search = GridSearchCV(SVC(kernel=&#39;rbf&#39;, gamma=&#39;scale&#39;, random_state=42), {&#39;C&#39;: range(1, 11)}, cv=4)
# grid_search.fit(X_train, y_train)
# best_params = grid_search.best_params_
##### print(&quot;最佳超参数：&quot;, best_params)

# 使用最佳超参数训练 SVM
eclf = SVC(内核=&#39;rbf&#39;, C=best_params[&#39;C&#39;], gamma=&#39;scale&#39;, random_state=42)
eclf = eclf.fit(X_train, y_train)

os.system(“cls”)
print(“准确度：”, precision_score(y_test, eclf.predict(X_test)))
]]></description>
      <guid>https://stackoverflow.com/questions/77808697/citeseer-and-cora-graph-link-prediciton-problem-whith-code-accuracy</guid>
      <pubDate>Fri, 12 Jan 2024 18:44:56 GMT</pubDate>
    </item>
    <item>
      <title>图像预处理|预测范围 [0.44 - 0.55] [关闭]</title>
      <link>https://stackoverflow.com/questions/77808063/image-preprocessing-predict-in-range-0-44-0-55</link>
      <description><![CDATA[我正在尝试解决二元分类任务。我有鳄鱼和短吻鳄的数据集。我有一个形状为 (2894, 2) 的矩阵 x，还有一个带有标签（0 或 1）且形状为 2894 的变量 y。
模型 = tf.keras.Sequential()[
    tf.keras.layers.Dense(256，激活=&#39;relu&#39;),
    tf.keras.layers.Dropout(速率=(0.12)),
    tf.keras.layers.Dense(128, 激活=&#39;relu&#39;),
    tf.keras.layers.Dropout(速率=(0.12)),
    tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
]）

loss_fn = tf.keras.losses.BinaryCrossentropy()
model.compile(optimizer=&#39;adam&#39;,loss=loss_fn,metrics=[&#39;accuracy&#39;])

但我的预测每次都在 (0.44–0.55) 范围内
x_train [[像素, 像素], [像素, 像素], [像素, 像素]] 的示例
另外，我对每个测试和训练变量都使用此方法：
shuffle(minmax.fit_transform(np.concatenate((x_train_aligator, x_train_crocodile), axis=0)), random_state=1)

现在，我对 x_train 和 x_test 使用 StandardScaler() 和 MinMaxScaler()。此外，测试数据的 random_state=1 和 random_state=2 也有相似之处。但我不知道为什么我的结果每次都在一个范围内。
我尝试过对数据和随机保存生成进行不同的操作。]]></description>
      <guid>https://stackoverflow.com/questions/77808063/image-preprocessing-predict-in-range-0-44-0-55</guid>
      <pubDate>Fri, 12 Jan 2024 16:42:15 GMT</pubDate>
    </item>
    <item>
      <title>Filter_Value 选择 TDA R</title>
      <link>https://stackoverflow.com/questions/77807593/filter-value-selection-tda-r</link>
      <description><![CDATA[我对 R 中 TDA 库（GSSTDA、TDAMapper、Mapper 等）的 filter_value 参数感到有点困惑。
例如，给出 GSSTDA 中的 Mapper 对象：
&lt;前&gt;&lt;代码&gt;映射器(
  完整数据，
  过滤器值，
  间隔数 = 5,
  重叠百分比 = 40,
  distance_type =“cor”，
  clustering_type =“分层”，
  num_bins_when_clustering = 10,
  连接类型=“单一”，
  最优聚类模式=“”，
  na.rm = TRUE
）

&lt;块引用&gt;
参数
过滤值
对输入应用过滤函数后得到的向量
矩阵，即具有每个过滤函数值的向量
包括样本。


有人可以通过一个众所周知的数据（例如 Iris）的示例来描述如何手动选择 filter_value 吗？
https://search.r-project.org /CRAN/refmans/GSSTDA/html/mapper.html]]></description>
      <guid>https://stackoverflow.com/questions/77807593/filter-value-selection-tda-r</guid>
      <pubDate>Fri, 12 Jan 2024 15:21:09 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：传递的 save_path 不是有效的检查点：./param_model/model(8, 100, 3)_75_10_-100_0</title>
      <link>https://stackoverflow.com/questions/77807281/valueerror-the-passed-save-path-is-not-a-valid-checkpoint-param-model-model</link>
      <description><![CDATA[#---生成通道----
通道，set_location_user =generate_channel（params_system，num_samples = 1，
location_user_initial=location_user, Rician_factor=Rician_factor)
y，y_real_tmp =generate_received_pilots_batch（通道，phase_shifts，导频，noise_power_db，Pt = Pt_u）
y_ks_tmp =去相关（y，飞行员）
y_ks = np.concatenate([y_ks_tmp.real, y_ks_tmp.imag], axis=1)
y_real = (y_real_tmp - y_mean) / y_std
y_ks_real = (y_ks - y_ks_mean) / y_ks_std
set_location_user = (set_location_user - location_mean) / (location_std+1e-15)

# ----------------------------------神经网络波束成形------------------------ --------------
model_path = &#39;./param_model/model&#39; + str(params_system) + &#39;_&#39; + str(len_pilot) + &#39;_&#39; + str(
    Rician_factor) + &#39;_&#39; + str(noise_power_db) + &#39;_&#39; + str(input_flag)
#print(“模型路径：”, model_path)

这个粗体文本给出了错误。
使用Python 3.6
如何纠正]]></description>
      <guid>https://stackoverflow.com/questions/77807281/valueerror-the-passed-save-path-is-not-a-valid-checkpoint-param-model-model</guid>
      <pubDate>Fri, 12 Jan 2024 14:30:02 GMT</pubDate>
    </item>
    <item>
      <title>使用 tSNE 后在 MNIST 数据集上应用 KMeans [关闭]</title>
      <link>https://stackoverflow.com/questions/77805020/applying-kmeans-on-the-mnist-dataset-after-using-tsne</link>
      <description><![CDATA[我在 MNIST 数据集上使用 tSNE，并得到了非常好的结果（当我可视化该图时，所有 10 个标签都分离得很好）。
现在，我想对从 tSNE 建模获得的数据应用 KMeans，并再次将其可视化。
不幸的是，这次我得到了非常糟糕的结果 - 集群看起来非常错误。
我知道 t-SNE 空间中的点之间的距离和关系不一定会保留原始高维空间中存在的结构，当将 KMeans 等聚类算法直接应用于 t 时，这可能会导致误导性的解释。 -SNE嵌入。
还有什么我可以做得更好的吗？
代码示例：
tsne = TSNE(n_components=2, perplexity=15,learning_rate=200, exaggeration=1)
x_train_tsne = tsne.fit(x_train)

kmeans = KMeans(n_clusters=10, n_init=1, init=&#39;kmeans++&#39;)
labels_kmeans = kmeans.fit_predict(x_train)

对于 np.unique(labels_kmeans) 中的数字：
   索引 = (labels_kmeans == 数字)
   plt.scatter(x_train_tsne[索引，0]，x_train_tsne[索引，1]，s=5，alpha=0.8，标签=str(数字))
]]></description>
      <guid>https://stackoverflow.com/questions/77805020/applying-kmeans-on-the-mnist-dataset-after-using-tsne</guid>
      <pubDate>Fri, 12 Jan 2024 07:35:51 GMT</pubDate>
    </item>
    <item>
      <title>UserWarning：X 没有有效的功能名称，但 KNeighborsClassifier 配备了功能名称 warnings.warn</title>
      <link>https://stackoverflow.com/questions/77804804/userwarning-x-does-not-have-valid-feature-names-but-kneighborsclassifier-was-f</link>
      <description><![CDATA[ ID 曾经_已婚 毕业 性别 职业 支出_分数细分 家庭_身材 年龄 工作_经历
0 462809 0 0 1 5 2 3 3 4 1
1 462643 1 1 0 2 0 0 2 18 15
2 466315 1 1 0 2 2 1 0 44 1
3 461735 1 1 1 7 1 1 1 44 0
4 462669 1 1 0 3 1 0 5 20 15
……………………………………
8063 464018 0 0 1 9 2 3 6 4 0
8064 464685 0 0 1 4 2 3 3 15 3
8065 465406 0 1 0 5 2 3 0 14 1
8066 467299 0 1 0 5 2 1 3 8 1
8067 461879 1 1 1 4 0 1 2 17 0
8068行×10列

data1=data.drop([&quot;ID&quot;,&quot;分段&quot;],axis=1)

从 sklearn.model_selection 导入 train_test_split
     x_train,x_test,y_train,y_test=train_test_split(data1,data.Segmentation,test_size=0.20,random_state=50)

 从 sklearn.neighbors 导入 KNeighborsClassifier
 knn=KNeighborsClassifier(n_neighbors=17)
 knn.fit(x_train,y_train)
 tahmin=knn.predict(x_test)

 knn.score(x_test,y_test)
 #0.4838909541511772
 knn.predict([[1,1,0,2,0,2,18,15]])

 UserWarning：X 没有有效的功能名称，但 KNeighborsClassifier 已安装了功能名称
  #警告.警告(
数组([1])

当我做出预测时，我并没有预料到这个警告。]]></description>
      <guid>https://stackoverflow.com/questions/77804804/userwarning-x-does-not-have-valid-feature-names-but-kneighborsclassifier-was-f</guid>
      <pubDate>Fri, 12 Jan 2024 06:45:16 GMT</pubDate>
    </item>
    <item>
      <title>使用 LOOCV 进行 K 最近邻的问题</title>
      <link>https://stackoverflow.com/questions/77804296/problem-conducting-k-nearest-neighbors-using-loocv</link>
      <description><![CDATA[我有一个示例表，我想对其进行 KKNN 分类。变量 V4 是响应，我希望分类器查看新数据点是否将分类为 0 或 1（实际数据有 12 列，第 12 列是响应，但我仍然会简化示例
库(kknn)

数据 &lt;- data.frame(
  V1=c(1.2,2.5,3.1,4.8,5.2),
  V2=c(0.7, 1.8, 2.3, 3.9, 4.1),
  V3=c(2.3, 3.7, 1.8, 4.2, 5.5),
  V4= c(0, 1, 0, 1, 0)
）

现在，我想使用 for 循环通过 LOOCV 构建 kknn 分类。假设 kknn=3
for (i in 1:nrow(data)) {
  train_data &lt;- 数据[-i, 1:3]
  train_data_response &lt;- data.frame(data[-i, 4])
  colnames(train_data_response) &lt;- “响应”
  test_set &lt;- 数据[i, 3]
  模型 &lt;- kknn(公式=train_data_response ~ ., data.frame(train_data),
                data.frame(test_set)，k=3，scale=TRUE)
}

现在我收到以下错误：
model.frame.default(公式，数据=训练)中的错误：
  变量“train_data_response”的类型（列表）无效

有什么办法可以解决这个错误吗？我认为 kknn 接受矩阵或数据帧。我的训练和测试数据确实是数据框，那么什么给出了？
另外，我是否正确执行了 LOOCV？]]></description>
      <guid>https://stackoverflow.com/questions/77804296/problem-conducting-k-nearest-neighbors-using-loocv</guid>
      <pubDate>Fri, 12 Jan 2024 04:15:37 GMT</pubDate>
    </item>
    <item>
      <title>初始化 VAE 权重 [关闭]</title>
      <link>https://stackoverflow.com/questions/77804014/initializing-vae-weights</link>
      <description><![CDATA[我正在训练遵循以下整体架构的 VAE：

变压器编码器
Mu/Logvar -&gt;重新参数化-&gt;潜在z
变压器解码器

根据典型的 VAE 设置，Mu 和 Logvar 只是两个前馈网络。然而，当我用标准值（例如权重为 0.5，偏差为 0）初始化它们时，我发现模型的初始 KL 损失巨大 - 例如5,000-20,000+。
当然，这个下降得相当快，但模型仍然花费数百个时期将 KL 损失从 300 降至 &lt;50。
一个“解决方法”我发现将权重初始化为低得多的值，并使用学习率预热。但初始化权重非常小：
def init_weights(self, initrange=0.0001) -&gt; &gt;没有任何：
        self.embedding_layer.weight.data.uniform_(-0.5, 0.5)
        
        nn.init.uniform_(self.fc_mu.weight, -initrange, initrange)
        nn.init.uniform_(self.fc_logvar.weight, -initrange, initrange)
        nn.init.zeros_(self.fc_mu.bias)
        nn.init.zeros_(self.fc_logvar.bias)

这样做的结果是一个更加稳定的 KL 损失（开始时约为 1.5），但我担心它会阻止我的解码器学习有意义的表示。实际的重建损失因此受到巨大影响。
这是 VAE 的已知问题吗？有什么我可以尝试的特定初始化技巧吗？或者也许我应该从正常值开始，让模型训练的时间明显更长？]]></description>
      <guid>https://stackoverflow.com/questions/77804014/initializing-vae-weights</guid>
      <pubDate>Fri, 12 Jan 2024 02:14:48 GMT</pubDate>
    </item>
    <item>
      <title>联邦学习全局聚合后准确率下降</title>
      <link>https://stackoverflow.com/questions/77798059/the-accuracy-decreased-after-global-aggregation-in-federated-learning</link>
      <description><![CDATA[我正在开展一个联合学习项目。我编写了一段代码来刺激联邦学习的过程。然而，每次迭代进行全局聚合后，全局模型的测试精度会下降很多，并且在接下来的迭代中保持不变。我使用的聚合算法是FedAvg。我尝试将我的代码分成不同的单元来找出问题所在。
对于本地训练，所选客户训练 3 轮。在这个实验中，将选择所有五个客户端进行训练和聚合，我用于本地的模型是从 torchvision 分叉的 vgg16，数据集是 MNIST，并以 i.i.d 方式分割每个客户端： 
for id, net_id in enumerate(selected):
    logging.info(“训练所选设备 %s。” % (str(net_id)))
    结果 = Userlists[net_id].train(hparams[&#39;n_local_epochs&#39;])
    logging.info(&#39;&gt;&gt; 局部模型 %d: 局部精度: %f in round %d\n&#39; % (id, result[&#39;local_test_acc&#39;], step+1))

在本地模型聚合之前，我使用全局服务器的测试数据来测试本地模型的准确性，
tesc，conf = Misc.compute_accuracy(Userlists[2].model，test_dl_global，get_confusion_matrix=True，device=hparams[&#39;device&#39;])
打印（测试）
&gt; 0.2478966346153846
tesc，conf = Misc.compute_accuracy（Userlists [3] .model，test_dl_global，get_confusion_matrix = True，device = hparams [&#39;device&#39;]）
打印（测试）
&gt; 0.14413060897435898
tesc,conf=misc.compute_accuracy(Userlists[4].model,test_dl_global,get_confusion_matrix=True,device=hparams[&#39;device&#39;])
打印（测试）
&gt; 0.17387820512820512

我使用下面的聚合代码来聚合所选客户端的权重：
&lt;前&gt;&lt;代码&gt;total_sum = 0.0
对于选定的 client_idx：
    Total_sum += 用户列表[client_idx].data_len
    
    
global_para = global_model.state_dict()
client_weights = [torch.tensor( Userlists[client_idx].data_len/total_sum, device=hparams[&#39;device&#39;]) for client_idx in selected]

使用 torch.no_grad()：
    对于顺序，枚举中的 idx（选定）：
        logging.info(f“对于客户端 {idx}”)
        net_para = Userlists[idx].model.state_dict()
        
        如果订单 == 0：
            对于 net_para.keys() 中的键：
                global_para[key] = net_para[key] * client_weights[订单]
        别的：
            对于 net_para.keys() 中的键：
                global_para[key] += net_para[key] * client_weights[订单]


global_model.load_state_dict(global_para)
tesc,conf=misc.compute_accuracy(global_model,train_dl_global,get_confusion_matrix=True,device=hparams[&#39;device&#39;])

全局测试精度下降并保持不变
&lt;前&gt;&lt;代码&gt;&gt; 0.11236666666666667

尽管我尝试增加本地训练的纪元，局部准确率提高到 40%，但全局准确率仍然落入与之前相同的值。我的聚合代码中是否有错误的地方？
测试精度应与本地精度保持在同一水平。]]></description>
      <guid>https://stackoverflow.com/questions/77798059/the-accuracy-decreased-after-global-aggregation-in-federated-learning</guid>
      <pubDate>Thu, 11 Jan 2024 06:30:41 GMT</pubDate>
    </item>
    <item>
      <title>带有我自己的预训练模型的 Sagemaker 批处理变压器</title>
      <link>https://stackoverflow.com/questions/77781734/sagemaker-batch-transformer-with-my-own-pre-trained-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77781734/sagemaker-batch-transformer-with-my-own-pre-trained-model</guid>
      <pubDate>Mon, 08 Jan 2024 15:54:18 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch RuntimeError：CUDA 内存不足且有大量可用内存</title>
      <link>https://stackoverflow.com/questions/71498324/pytorch-runtimeerror-cuda-out-of-memory-with-a-huge-amount-of-free-memory</link>
      <description><![CDATA[在训练模型时，我遇到了以下问题：
运行时错误：CUDA 内存不足。尝试分配 304.00 MiB（GPU 0；8.00 GiB 总容量；已分配 142.76 MiB；6.32 GiB 空闲；PyTorch 总共保留 158.00 MiB）分配的内存尝试设置 max_split_size_mb 以避免碎片。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档
正如我们所看到的，当尝试分配 304 MiB 内存时会发生错误，而 6.32 GiB 是空闲的！问题是什么？正如我所看到的，建议的选项是设置 max_split_size_mb 以避免碎片。它会有帮助吗？如何正确地做到这一点？
这是我的 PyTorch 版本：
火炬==1.10.2+cu113
火炬视觉==0.11.3+cu113
火炬音频===0.10.2+cu113]]></description>
      <guid>https://stackoverflow.com/questions/71498324/pytorch-runtimeerror-cuda-out-of-memory-with-a-huge-amount-of-free-memory</guid>
      <pubDate>Wed, 16 Mar 2022 13:53:45 GMT</pubDate>
    </item>
    <item>
      <title>sklearn LogisticRegressionCV 是否使用最终模型的所有数据</title>
      <link>https://stackoverflow.com/questions/51830558/does-sklearn-logisticregressioncv-use-all-data-for-final-model</link>
      <description><![CDATA[我想知道sklearn中LogisticRegressionCV的最终模型（即决策边界）是如何计算的。假设我有一些 Xdata 和 ylabels，这样
Xdata # 形状为 (n_samples,n_features)
ylabels # 形状是 (n_samples,)，它是二进制的

现在我跑步
从 sklearn.linear_model 导入 LogisticRegressionCV
clf = LogisticRegressionCV(Cs=[1.0],cv=5)
clf.fit(Xdata,ylabels)

这仅考虑一个正则化参数和 CV 中的 5 个折叠。因此，clf.scores_ 将是一个字典，其中一个键的值是一个形状为 (n_folds,1) 的数组。通过这五次折叠，您可以更好地了解模型的表现。
但是，我对从 clf.coef_ 获得的内容感到困惑（我假设 clf.coef_ 中的参数是 clf.coef_ 中使用的参数&gt;clf.predict）。我认为可能有几个选择：

clf.coef_ 中的参数来自在所有数据上训练模型
clf.coef_ 中的参数来自最佳评分折叠
clf.coef_ 中的参数以某种方式对折叠进行平均。

我想这是一个重复的问题，但在我的一生中，我无法在网上、sklearn 文档或 LogisticRegressionCV 的源代码中找到简单的答案。我发现的一些相关帖子是：

GridSearchCV 最终模型
scikit-learn LogisticRegressionCV：最佳系数
在 sklearn 中使用交叉验证和 AUC-ROC 建立逻辑回归模型
通过交叉验证评估 Logistic 回归
]]></description>
      <guid>https://stackoverflow.com/questions/51830558/does-sklearn-logisticregressioncv-use-all-data-for-final-model</guid>
      <pubDate>Mon, 13 Aug 2018 21:06:28 GMT</pubDate>
    </item>
    </channel>
</rss>