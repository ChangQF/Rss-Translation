<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 12 Jun 2024 12:28:15 GMT</lastBuildDate>
    <item>
      <title>在什么时候我可以说我的机器学习模型的准确性很好[关闭]</title>
      <link>https://stackoverflow.com/questions/78612402/at-what-point-can-i-say-the-accuracy-of-my-machine-learning-model-is-good</link>
      <description><![CDATA[对于机器学习模型来说，多少百分比的准确率是令人满意的。
每当我创建一个模型时，我通常能得到大约 70% 到 80% 的准确率。如果我尝试使用 GridSearch、集成算法、通过适当的缩放和特征选择将模型的准确率提高到 90%，我很难达到 90%。我能得到的最高准确率是增加 1% 或 2%。有时准确率保持不变而没有任何增加。
我的问题是：
这是否意味着准确率不能远远超过我现有的准确率（即 90%）。因此，任何增量都可能导致模型过度拟合？
或者
还有其他我不知道的提高模型准确率的方法吗？
那么，在什么时候我可以说我有一个好的模型]]></description>
      <guid>https://stackoverflow.com/questions/78612402/at-what-point-can-i-say-the-accuracy-of-my-machine-learning-model-is-good</guid>
      <pubDate>Wed, 12 Jun 2024 11:36:04 GMT</pubDate>
    </item>
    <item>
      <title>推荐系统 - 奇异值分解 (SVD) 提供随机结果</title>
      <link>https://stackoverflow.com/questions/78612024/recommender-system-singular-value-decomposition-svd-is-providing-random-resu</link>
      <description><![CDATA[有时，输出的质量仅通过目测来评估。查看下面提供的示例，很明显预期的用户评分为 2 和 3（请参考空数据单元格）。
这些示例经过简化 - 我使用了更大的数据集，但仍然获得了相同的结果。
表 1

用户 电影 1 电影 2 电影 3

1 - 3 4

2 2 3 4

3 2 3 4

表 2

用户 电影 1 电影 2 电影 3

1 - 3 3

2 3 3 3

3 3 3 3

ALS 模型提供了这些评分，但 SVD 模型却惨遭失败。有人知道这是为什么吗？为什么我们会得到 2 和 3 以外的结果？]]></description>
      <guid>https://stackoverflow.com/questions/78612024/recommender-system-singular-value-decomposition-svd-is-providing-random-resu</guid>
      <pubDate>Wed, 12 Jun 2024 10:21:28 GMT</pubDate>
    </item>
    <item>
      <title>增加输入数据集，并通过训练当前输入和输出找到相应的增加的输出</title>
      <link>https://stackoverflow.com/questions/78611300/increasing-the-input-dataset-and-finding-the-corresponding-increased-output-by-t</link>
      <description><![CDATA[输入 excel 文件 (trainn.xlsx) 表示汽车百分比，输出 excel 文件 (pred.xlsx) 表示与输入值不同的输出值。有 160 个输入和 160 个输出。我想通过训练当前数据集来生成更多 340 个输入和相应的 340 个输出（每列）。是否可以通过生成新数据将输入和输出的值都增加到 500？我当前的代码：
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.utils import resample

# 加载数据
train = pd.read_excel(&#39;/mnt/data/trainn.xlsx&#39;)
pred = pd.read_excel(&#39;/mnt/data/pred.xlsx&#39;)

# 分离特征和目标
X = train.drop([&#39;Emission CO (gm)&#39;], axis=1)
y = train[&#39;Emission CO (gm)&#39;]

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=43)

# 训练 RandomForestRegressor
rf = RandomForestRegressor()
rf.fit(X_train, y_train)

# 在预测数据集上预测排放量
y_pred = rf.predict(pred)
pred[&#39;predicted Emission CO (gm)&#39;] = y_pred

# 将预测结果保存到 CSV 文件
pred.to_csv(&#39;/mnt/data/predicted_emission_co_data.csv&#39;, index=False)

# 将数据集扩充到 500 个条目
while len(train) &lt; 500:
train = pd.concat([train, resample(train, replace=True, n_samples=500-len(train), random_state=43)], ignore_index=True)

# 将增强后的数据集保存到 Excel 文件
train.to_excel(&#39;/mnt/data/augmented_trainn.xlsx&#39;, index=False)

print(&quot;模型训练完成，数据集增强。&quot;)


代码将输入数据增加到 500，但没有增加相应的输出数据（pred.slsx）。它仍然是 160。帮帮我吧]]></description>
      <guid>https://stackoverflow.com/questions/78611300/increasing-the-input-dataset-and-finding-the-corresponding-increased-output-by-t</guid>
      <pubDate>Wed, 12 Jun 2024 07:54:29 GMT</pubDate>
    </item>
    <item>
      <title>在实践代码中，它说 keras 层无效，</title>
      <link>https://stackoverflow.com/questions/78611171/in-practice-code-it-is-saying-the-keras-layer-is-not-valid</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78611171/in-practice-code-it-is-saying-the-keras-layer-is-not-valid</guid>
      <pubDate>Wed, 12 Jun 2024 07:26:52 GMT</pubDate>
    </item>
    <item>
      <title>机器学习模型的输入</title>
      <link>https://stackoverflow.com/questions/78610947/input-to-machine-learning-model</link>
      <description><![CDATA[我是机器学习的新手。最近我训练了一个基于 bert 的模型，我已经研究了很长时间了。训练后，我在模型目录中得到了几个文件 - pytorch_model.bin、training_args.bin、merges.txt、vocab.json。现在我想通过向模型提供输入并检查其输出来测试模型。但我不知道该怎么做。
我试着在网上查找，有人建议我使用 Gradio。]]></description>
      <guid>https://stackoverflow.com/questions/78610947/input-to-machine-learning-model</guid>
      <pubDate>Wed, 12 Jun 2024 06:27:31 GMT</pubDate>
    </item>
    <item>
      <title>无法为强化学习问题分配内存</title>
      <link>https://stackoverflow.com/questions/78610849/unable-to-allocate-memory-for-a-reinforcement-learning-problem</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78610849/unable-to-allocate-memory-for-a-reinforcement-learning-problem</guid>
      <pubDate>Wed, 12 Jun 2024 05:59:34 GMT</pubDate>
    </item>
    <item>
      <title>我正在寻找方法或脚本来微调 DreamFusion 模型，以实现文本到 3D 和图像到 3D 模型的生成 [关闭]</title>
      <link>https://stackoverflow.com/questions/78610807/im-finding-method-or-script-to-fine-tune-dreamfusion-model-for-text-to-3d-and-i</link>
      <description><![CDATA[我想针对特定领域（珠宝设计）微调 DreamFusion 模型，但所有开源模型的精度都不足以生成我想要的 3D 模型。
我找到了一种用于形状模型的方法，但它的效果和预期的一样好。]]></description>
      <guid>https://stackoverflow.com/questions/78610807/im-finding-method-or-script-to-fine-tune-dreamfusion-model-for-text-to-3d-and-i</guid>
      <pubDate>Wed, 12 Jun 2024 05:41:33 GMT</pubDate>
    </item>
    <item>
      <title>对于复杂而长的神经网络，反向传播中的梯度是如何计算的？</title>
      <link>https://stackoverflow.com/questions/78610739/how-are-gradients-calculated-in-backpropagation-for-complex-long-neural-network</link>
      <description><![CDATA[我理解如何找到反向传播算法的偏导数，但一旦有多个隐藏层，就无法手动跟踪偏导数，我不知道如何将其实现到代码中。我对如何逐层向下跟踪值感到非常困惑，而且我还没有看到解释反向传播算法的资源。有人可以解释一下或指出合适的资源吗？我是一名初学者，我正在尝试从头开始构建一个神经网络，但结果很难理解，因为它比我能跟踪的要长，而且我不知道如何在代码中实现。
对于具有多层和数千个连接的神经网络，这是否超出了初学者的范围。我见过一个 chatgpt 解释，其中用操作序列构建了一个图，并以相反的顺序遍历它以了解偏导数阶。]]></description>
      <guid>https://stackoverflow.com/questions/78610739/how-are-gradients-calculated-in-backpropagation-for-complex-long-neural-network</guid>
      <pubDate>Wed, 12 Jun 2024 05:19:50 GMT</pubDate>
    </item>
    <item>
      <title>我应该调整哪些超参数来提高准确性？</title>
      <link>https://stackoverflow.com/questions/78610497/which-hyperparameters-should-i-adjust-to-improve-accuracy</link>
      <description><![CDATA[我想知道如何在多标签分类问题中提高准确率并降低损失。
如果你查看 sklearn 参考，你会发现在多类和多输出算法中提到了多标签，我现在正在测试它。
（https://scikit-learn.org/stable/modules/multiclass.html）
样本数据使用sklearn.datasets中的make_multilabel_classification有10个特征，通过修改n_classes创建一个数据集。
当multilabel有两个类时，似乎准确率和损失都比较令人满意。
from numpy import mean
from numpy import std
from sklearn.datasets import make_multilabel_classification
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score, hamming_loss

# define dataset
X, y = make_multilabel_classification(n_samples=10000, n_features=10, n_classes=2, random_state=1)

# 总结数据集形状
print(X.shape, y.shape)
# 总结前几个示例
for i in range(10):
print(X[i], y[i])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
print(scaler.mean_)
print(scaler.var_)

x_train_std = scaler.transform(X_train)
x_test_std = scaler.transform(X_test)

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train_std, y_train)

pred = knn.predict(x_test_std)

print(accuracy_score(y_test, pred))
print(hamming_loss(y_test, pred))

accuracy_score: 0.8345, hamming_loss: 0.08875
但是，随着类别数超过3，准确率得分逐渐下降，损失增加。
# define dataset
X, y = make_multilabel_classification(n_samples=10000, n_features=10, n_classes=3, random_state=1)

n_classes= 3 --&gt; accuracy_score: 0.772, hamming_loss: 0.116
n_classes= 4 --&gt; accuracy_score: 0.4875, hamming_loss: 0.194125
使用 RandomForestClassifier 算法和 MLPClassifier 算法时（如参考中所示）或使用 ClassifierChain(estimator=SVC) 使用不支持多标签分类的算法时，情况也类似。
我应该调整哪些超参数来提高准确率？]]></description>
      <guid>https://stackoverflow.com/questions/78610497/which-hyperparameters-should-i-adjust-to-improve-accuracy</guid>
      <pubDate>Wed, 12 Jun 2024 03:24:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中对离散概率函数进行梯度下降编码？</title>
      <link>https://stackoverflow.com/questions/78610447/how-do-i-code-gradient-descent-over-a-discrete-probability-function-in-pytorch</link>
      <description><![CDATA[我正在尝试编写梯度下降算法，以最小化一维数组 X 和较小的一维数组 A 之间的卷积的香农熵，其中要优化的参数是 A 的条目。但是，要计算熵，我需要先计算分布的离散概率。但是，我相信这会破坏 PyTorch 内部的梯度计算。
这是我的损失函数：
def loss_function(A):
return Shentropy(F.conv1d(padded_input, A.unsqueeze(0).unsqueeze(0), padding=0))

def Shentropy(wf):
unique_elements, counts = wf.unique(return_counts=True)
entrsum = 0
for x in counts:
p = x/len(wf) #计算 x 的概率
entrsum-= p*torch.log2(p) #Shannon 熵公式 
return entrsum

但是，这给了我以下错误：
RuntimeError：张量的元素 0 不需要梯度，也没有grad_fn
我尝试将 wf.unique(return_counts=True) 与 wf.softmax(dim=0) 交换，代码确实以这种方式运行。但是，softmax 不适用于熵公式（给出错误的结果）。
有没有其他方法可以使其可微分，从而不破坏梯度或损害公式？或者我应该使用某种“离散梯度”？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78610447/how-do-i-code-gradient-descent-over-a-discrete-probability-function-in-pytorch</guid>
      <pubDate>Wed, 12 Jun 2024 02:58:28 GMT</pubDate>
    </item>
    <item>
      <title>如何将 JSON 中的标记坐标叠加到 JPG 图像上以进行 CNN 训练？[关闭]</title>
      <link>https://stackoverflow.com/questions/78606586/how-to-overlay-labeled-coordinates-from-json-onto-jpg-images-for-cnn-training</link>
      <description><![CDATA[我正在开展一个计算机视觉项目，该项目涉及检测和分割 MRI 扫描中的骨折。作为该项目的一部分，我让专家直接在图像上标记骨折区域。此过程会生成一个 JSON 文件，其中包含以下信息：

标记区域的坐标
标记区域的名称
标记图像的名称

我面临的挑战是将这些坐标从 JSON 文件转移到相应的 JPG 图像上，以准备进行 CNN 训练。
以下是我的 JSON 文件结构示例：
&quot;item&quot;: {
&quot;name&quot;: &quot;img-00003-00082.jpg&quot;,
&quot;team&quot;: {
&quot;name&quot;: &quot;Mask&quot;,
&quot;slug&quot;: &quot;mask&quot;
&quot;file_name&quot;: &quot;img-00003-00082.jpg&quot;,
&quot;annotations&quot;: [
{
&quot;bounding_box&quot;: {
&quot;h&quot;: 142.16649999999993,
&quot;w&quot;: 124.14549999999997,
&quot;x&quot;: 679.8006,
&quot;y&quot;: 425.7789
},
&quot;name&quot;: &quot;Broken&quot;,
&quot;polygon&quot;: {
&quot;paths&quot;: [
[
{
&quot;x&quot;: 695.1519,
&quot;y&quot;: 567.9454
},
{
&quot;x&quot;: 679.8006,
&quot;y&quot;: 530.5683
},


到目前为止，我已经设法从 JSON 文件中提取了必要的坐标。但是，我很难将这些坐标叠加到 JPG 图像上以生成 CNN 的训练数据。
我的问题：

如何准确地将 JSON 文件中的坐标叠加到相应的 JPG 图像上？
是否有任何推荐的库或方法专门适合 Python 中的这项任务？
]]></description>
      <guid>https://stackoverflow.com/questions/78606586/how-to-overlay-labeled-coordinates-from-json-onto-jpg-images-for-cnn-training</guid>
      <pubDate>Tue, 11 Jun 2024 09:28:24 GMT</pubDate>
    </item>
    <item>
      <title>实现神经网络的岭回归方程</title>
      <link>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</link>
      <description><![CDATA[我试图在 MATLAB 中复制以下方程，以使用岭回归训练找到神经网络的最佳输出权重矩阵。
使用岭回归训练后的神经网络输出权重矩阵：

此方程来自 Mantas Lukosevicius 提供的回声状态网络指南，可在以下位置找到：https://www.researchgate.net/publication/319770153_A_practical_guide_to_applying_echo_state_networks（见第 11 页）
我的尝试如下。我认为外括号（红色）使其成为非传统的双重求和，这意味着 Voss 提出的方法（见 https://www.mathworks.com/matlabcentral/answers/1694960-nested-loops-for-double-summation）无法遵循。请注意，y_i 是一个 T x 1 向量，而 y_i_target 也是一个 T x 1 向量。Wout_i 是一个 N x 1 向量，其中 N 是神经网络中的节点数。我为每个 i^th 目标训练信号生成三个 Ny x 1 向量 Wout_i,y_i,y_i_target，其中 Ny 是训练信号的数量。Wout 的最终输出是一个 N x 1 向量，其中向量中的每个元素都是网络中每个节点的最佳权重。
N = 100; % 神经网络节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度 
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
outer_sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 针对每个第 i 个目标训练信号收集的每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
inner_sum = sum(((y_i&#39;-y_i_target).^2)+reg*norm(Wout_i)^2);
outer_sum(i) = inner_sum;
end
outer_sum = outer_sum.*(1/Ny);
[minval, minidx] = min(outer_sum);
Wout = cell2mat(Wouts(minidx));

我对 Wout 的最终答案是 N 乘以 1，正如它应该的那样，但我对我的答案不确定。我特别不确定我是否正确地完成了关于 Wout 操作的双重求和和 arg min。有什么方法可以验证我的答案吗？
替代方法：
我尝试了另一种方法/尝试，如下所示：
N = 100; % 神经网络中的节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
Sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 为每个第 i 个目标训练信号收集每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
Sum(i) = (1/Ny)*sum((y_i&#39;-y_i_target).^2);
end
[minval, minidx] = min(Sum);
Wout = cell2mat(Wouts(minidx));

我相信这次尝试比第一次更好，但我不确定它是否仍然正确。]]></description>
      <guid>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</guid>
      <pubDate>Sat, 08 Jun 2024 22:31:47 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 Elixir Nx/Schorar 中进行 ELISA 分析？</title>
      <link>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</link>
      <description><![CDATA[我已阅读 Medium 上的文章 ELISA Analysis in Python。
上述文章使用 SciPy 的 curve_fit 函数根据 4 参数逻辑回归 (4PL) 模型找到近似曲线，如下所示：
from scipy.optimize import curve_fit

x = [1.95, 3.91, 7.381, 15.63, 31.25, 62.5, 125,250, 500, 1000]
y = [0.274, 0.347, 0.392, 0.420, 0.586, 1.115, 1.637, 2.227, 2.335, 2.372]

def log4pl(x, A, B, C, D):
return(((A - D) / (1.0 + ((x / C) ** B))) + D)

params, _ = curve_fit(log4pl, x, y)
A, B, C, D = params[0], params[1], params[2], params[3]

我想使用 Nx/Scholar 库。
可能吗？如果您能给我任何提示，我将不胜感激。

[更新]
快速浏览一下 Python scipy.optimize 源代码，似乎 curve_fit 在内部使用了 Fortran 的 MINPACK 库。
据我所知，没有简单的方法可以从 Elixir 使用 MINPACK。
因此，我得出结论，目前在 Elixir 中进行 ELISA 分析很困难。
欢迎提供任何其他信息。]]></description>
      <guid>https://stackoverflow.com/questions/78565463/is-elisa-analysis-in-elixir-nx-schorar-possible</guid>
      <pubDate>Sun, 02 Jun 2024 04:29:18 GMT</pubDate>
    </item>
    <item>
      <title>如何将生成的数据转换为 Pandas 数据框</title>
      <link>https://stackoverflow.com/questions/67266153/how-to-convert-generated-data-into-pandas-dataframe</link>
      <description><![CDATA[from sklearn.datasets import make_classification
df = make_classification(n_samples=10000, n_features=9, n_classes=1, random_state = 18,
class_sep=2, n_informative=4)

创建数据后。它是元组，将元组转换为 pandas 数据框后
 df = pd.DataFrame(data, columns=[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;,&quot;7&quot;,&quot;8&quot;,&quot;9&quot;])

所以我得到了 9 个特征（列），但是当我尝试插入 9 个列时，它显示。

ValueError：传递值的形状为 (2, 1)，索引暗示 (2, 9)

基本上我想生成数据并将其转换为 pandas 数据框，但无法获取它。
错误是：]]></description>
      <guid>https://stackoverflow.com/questions/67266153/how-to-convert-generated-data-into-pandas-dataframe</guid>
      <pubDate>Mon, 26 Apr 2021 11:54:24 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 模型输入形状</title>
      <link>https://stackoverflow.com/questions/66488807/pytorch-model-input-shape</link>
      <description><![CDATA[我加载了一个自定义 PyTorch 模型，我想找出它的输入形状。类似这样的内容：
model.input_shape

是否可以获取此信息？

更新： print() 和 summary() 不显示此模型的输入形状，因此它们不是我要找的。]]></description>
      <guid>https://stackoverflow.com/questions/66488807/pytorch-model-input-shape</guid>
      <pubDate>Fri, 05 Mar 2021 07:59:52 GMT</pubDate>
    </item>
    </channel>
</rss>