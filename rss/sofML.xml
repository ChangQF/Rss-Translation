<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 26 Dec 2023 06:17:39 GMT</lastBuildDate>
    <item>
      <title>缺失数据的随机森林建模：寻求不需要插补或数据删除的包或方法</title>
      <link>https://stackoverflow.com/questions/77715672/random-forest-modeling-with-missing-data-seeking-packages-or-approaches-that-do</link>
      <description><![CDATA[我有一个包含多个变量的数据集，其中包含缺失值，并且我不希望估算或丢弃它们。我有兴趣在处理缺失的观察结果时将随机森林模型拟合到这些数据。谁能推荐专门设计的软件包或方法，用于将随机森林拟合到缺失值的数据，而不需要插补或删除不完整的记录？”]]></description>
      <guid>https://stackoverflow.com/questions/77715672/random-forest-modeling-with-missing-data-seeking-packages-or-approaches-that-do</guid>
      <pubDate>Tue, 26 Dec 2023 04:28:14 GMT</pubDate>
    </item>
    <item>
      <title>MediaPipe 静态存储视频人脸标志检测</title>
      <link>https://stackoverflow.com/questions/77715244/mediapipe-static-stored-video-face-landmark-detection</link>
      <description><![CDATA[该帖子已被隐藏。你刚刚删除了这篇文章。
关闭。这个问题需要更加有针对性。目前不接受答案。
更新问题，使其仅关注一个问题。这将有助于其他人回答问题。您可以编辑问题或发布新问题。
22 小时前关闭。
此帖子已于 19 小时前编辑并提交审核。
我一直在尝试让媒体管道来检测静态（存储）视频中的面部标志，但所有在线指南和教程都使用实时摄像头源。在 Python 中很容易，但我必须在 JavaScript 中完成。
我发现这两个指南最相关，但都使用实时摄像头。
 https://medium.com/@mamikonyanmichael/what-is-media-pipe-and-how-to-use-it-in-react-53ff418e5a68
https://github.com/jays0606/mediapipe-facelandmark-demo 
如何在静态（本地存储）视频而不是 JavaScript 中的实时摄像头源上运行 Mediapipe 的人脸检测？]]></description>
      <guid>https://stackoverflow.com/questions/77715244/mediapipe-static-stored-video-face-landmark-detection</guid>
      <pubDate>Tue, 26 Dec 2023 00:07:55 GMT</pubDate>
    </item>
    <item>
      <title>如何从图像中测量物体的长度</title>
      <link>https://stackoverflow.com/questions/77714997/how-to-measure-length-of-object-from-image</link>
      <description><![CDATA[我目前正在开展最后一年的项目，重点是使用来自不同平面的超声图像进行胎儿健康分析。完成模型训练阶段后，我在使用新图像测试模型时遇到了挑战。问题在于准确计算股骨长度或头围等测量值，因为我缺乏有关图像像素分辨率的信息。数据的缺失导致测量结果与预期的正常范围不符。我非常感谢有关解决此像素分辨率挑战的指导，以确保更可靠和更精确的测量。
我尝试了所有公式，但最终我需要每个图像的像素值
整个公式将乘以每个图像特定像素分辨率值]]></description>
      <guid>https://stackoverflow.com/questions/77714997/how-to-measure-length-of-object-from-image</guid>
      <pubDate>Mon, 25 Dec 2023 21:17:00 GMT</pubDate>
    </item>
    <item>
      <title>根据游戏数据计算偏差和偏差锐度</title>
      <link>https://stackoverflow.com/questions/77714958/calculating-deviation-and-sharpness-of-deviation-from-game-data</link>
      <description><![CDATA[我发现这个网站可以计算足球比赛的赔率/上盘/下盘。
我想知道如何实现类似的功能。
我可以通过抓取bet365来获取赛季数据，这非常简单。
但是，我对如何计算“偏差”感到困惑。和“偏差锐度” （表中的列名称）基于任何给定比赛中的实时事件。]]></description>
      <guid>https://stackoverflow.com/questions/77714958/calculating-deviation-and-sharpness-of-deviation-from-game-data</guid>
      <pubDate>Mon, 25 Dec 2023 20:58:12 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 raytune 同时优化多个目标（最小化 MSE 和最大化 R^2）？</title>
      <link>https://stackoverflow.com/questions/77714892/how-do-i-optimize-multiple-objectives-minimizing-mse-and-maximizing-r2-simult</link>
      <description><![CDATA[我正在尝试将 MSE_test 优化为最小值，将 R^2 优化为最高，但在尝试弄清楚如何同时执行这两个操作时遇到困难。
我现在拥有的这段代码仅优化 MSE_test
def main():
    # 定义超参数搜索空间
    配置={
        “lr”：tune.loguniform(0.0001, 0.1),
        “时代”：tune.randint(70, 100)
    }

    分析=调.运行（
        调整.with_parameters（train_model，X_train = X_train_normalized，Y_train = Y_train，X_test = X_test_normalized，Y_test = Y_test），
        配置=配置，
        num_samples=15, # 根据您的资源调整此值
        metric=“mse_test”, # 针对测试集上较低的 MSE 进行优化
        模式＝“分钟”，
        Progress_reporter=CLIReporter(metric_columns=[“mse_train”、“r2_train”、“mse_test”、“r2_test”])
    ）

    best_config = Analysis.get_best_config(metric=“mse_test”, mode=“min”)
    print(&quot;最佳超参数：&quot;, best_config)

如果 __name__ == “__main__”：
    主要的（）


我这样做了，但它单独完成了这项工作，并给了我两个我正在尝试优化的不同值。
# 针对 mse_test 进行优化
mse_analysis=tune.run（
    调整.with_parameters（train_model，X_train = X_train_normalized，Y_train = Y_train，X_test = X_test_normalized，Y_test = Y_test），
    配置=配置，
    样本数=10，
    指标=“mse_test”，
    模式＝“分钟”，
    Progress_reporter=CLIReporter(metric_columns=[“mse_train”、“r2_train”、“mse_test”、“r2_test”])
）

# 针对 r2_test 进行优化
r2_analysis=tune.run(
    调整.with_parameters（train_model，X_train = X_train_normalized，Y_train = Y_train，X_test = X_test_normalized，Y_test = Y_test），
    配置=配置，
    样本数=10，
    度量=“r2_test”，
    模式=“最大”，
    Progress_reporter=CLIReporter(metric_columns=[“mse_train”、“r2_train”、“mse_test”、“r2_test”])
）

# 检索最佳配置
best_config_mse = mse_analysis.get_best_config(metric=“mse_test”, mode=“min”)
best_config_r2 = r2_analysis.get_best_config(metric=“r2_test”, mode=“max”)

print(“mse_test 的最佳超参数：”, best_config_mse)
print(“r2_test 的最佳超参数：”, best_config_r2)

]]></description>
      <guid>https://stackoverflow.com/questions/77714892/how-do-i-optimize-multiple-objectives-minimizing-mse-and-maximizing-r2-simult</guid>
      <pubDate>Mon, 25 Dec 2023 20:21:58 GMT</pubDate>
    </item>
    <item>
      <title>用于机器学习的 python Rest api aiohttp 加载时间错误</title>
      <link>https://stackoverflow.com/questions/77714772/python-rest-api-for-machine-learning-aiohttp-bad-load-times</link>
      <description><![CDATA[我在 python 中使用 aiohttp。我有一个机器学习项目，我想通过restapi 提供该项目。我对这些函数进行了计时，它们执行起来并不需要很长时间。他们不会等待结果，而是检查进度并开始任务。问题是等待时间相当长。我试图找到问题所在，它不是函数本身，而是 aiohttp 来运行函数或发送结果。函数本身不是问题。（等待时间10s，函数执行2e-05）。我只是创建一个包含该项目的类。有人对如何确保快速响应时间有建议吗？有没有办法为 api 预分配资源或在单独的线程中生成机器学习项目。]]></description>
      <guid>https://stackoverflow.com/questions/77714772/python-rest-api-for-machine-learning-aiohttp-bad-load-times</guid>
      <pubDate>Mon, 25 Dec 2023 19:30:24 GMT</pubDate>
    </item>
    <item>
      <title>Tesseract 在简单的手写测试中找不到文本。有没有什么办法解决这一问题？</title>
      <link>https://stackoverflow.com/questions/77714612/tesseract-is-not-finding-text-in-simple-handwriting-test-is-there-any-way-to-fi</link>
      <description><![CDATA[我正在尝试为纸质测试的自动评分提供更好的解决方案。
问题是从测试中提取矩形区域并对手写输入进行 OCR。
虽然手写显然具有挑战性，但这个问题比一般阅读手写要简单得多：

文本方向已知
我可以准确指定我期望的答案和/或合法的字符集。
我愿意从引擎获得概率，如果概率太低，请叫人来裁决（最好不要）。

Tesseract 声称可以手写，可以使用 mingw 在 Linux 和 Windows 上运行，所以看起来不错。
我从表单中提取了手写数据样本。这是示例：

在这种情况下，矩形的边界没有被裁剪掉，但我期望它能够找到我的 64。它失败了。
当我裁剪边界框时，它起作用了。
虽然在这种情况下，我可以解决问题，但我想知道是否可以采取任何措施来提高识别率，因为边界框似乎无害，而且我担心任何微不足道的噪音都会破坏检测。

我可以使用更好的开源包吗？

有没有办法改进我的应用程序的培训？
我想我可以创造一种“语言”对于单个字母，对于整数使用不同的语言，并加载多个超正方引擎，每个引擎专门针对一种问题类型。

内部 API 是否有办法为其提供潜在字符串/字符集的列表，即暗示提高准确性？

]]></description>
      <guid>https://stackoverflow.com/questions/77714612/tesseract-is-not-finding-text-in-simple-handwriting-test-is-there-any-way-to-fi</guid>
      <pubDate>Mon, 25 Dec 2023 18:24:54 GMT</pubDate>
    </item>
    <item>
      <title>为什么支持向量机的 varImp() 出现错误？</title>
      <link>https://stackoverflow.com/questions/77714417/why-am-i-getting-an-error-with-varimp-for-support-vector-machine</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77714417/why-am-i-getting-an-error-with-varimp-for-support-vector-machine</guid>
      <pubDate>Mon, 25 Dec 2023 17:08:22 GMT</pubDate>
    </item>
    <item>
      <title>无法在 F# 中将内存数据传递到 AutoML。不明白为什么它不能编译</title>
      <link>https://stackoverflow.com/questions/77714343/cant-pass-in-memory-data-to-automl-in-f-not-understanding-why-it-doesnt-com</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77714343/cant-pass-in-memory-data-to-automl-in-f-not-understanding-why-it-doesnt-com</guid>
      <pubDate>Mon, 25 Dec 2023 16:43:24 GMT</pubDate>
    </item>
    <item>
      <title>Mediapipe-model-maker安装问题</title>
      <link>https://stackoverflow.com/questions/77713844/mediapipe-model-maker-installation-issue</link>
      <description><![CDATA[希望你们一切都好。我正在研究图像分类模型，为此，我正在安装 mediapipe-model-marker，但遇到以下错误。
这是我的 python 和 pip 版本
在此处输入图像描述
这是我用来安装此软件包的 pip 命令。
pip install mediapipe-model-maker 
这是错误
在此处输入图像描述
我尝试在我的 venv 中下载 medipipe-model-marker 包来使用图像分类模型。
我希望下载这个包。]]></description>
      <guid>https://stackoverflow.com/questions/77713844/mediapipe-model-maker-installation-issue</guid>
      <pubDate>Mon, 25 Dec 2023 13:36:29 GMT</pubDate>
    </item>
    <item>
      <title>SARIMAX 错误：ValueWarning：已提供日期索引，但它没有关联的频率信息，因此在例如预测</title>
      <link>https://stackoverflow.com/questions/77713776/sarimax-errorvaluewarninga-date-index-has-been-provided-but-it-has-no-associat</link>
      <description><![CDATA[我有这样的数据

&lt;表类=“s-表”&gt;
&lt;标题&gt;

时间
湿度_(%)


&lt;正文&gt;

15:21:02
31.03


15:23:05
30.98




当我运行代码时出现此错误：
ValueWarning：已提供日期索引，但它没有关联的频率信息，因此在例如预测。
有熟悉这方面的人可以帮助我吗？
不知道是什么原因
我写了这段代码：
导入 pandas 作为 pd
将 statsmodels.api 导入为 sm


data.set_index(&#39;时间&#39;, inplace=True)


# 将数据分为训练集和测试集
train_data = data.iloc[:-17000] # 使用除最后 8 个样本之外的所有样本进行训练
test_data = data.iloc[-17000:] # 使用最后8个样本进行测试

# 将 SARIMAX 模型拟合到训练数据
模型 = sm.tsa.SARIMAX(train_data[&#39;湿度_(%)&#39;], order=(1, 0, 0))
model_fit = model.fit()
]]></description>
      <guid>https://stackoverflow.com/questions/77713776/sarimax-errorvaluewarninga-date-index-has-been-provided-but-it-has-no-associat</guid>
      <pubDate>Mon, 25 Dec 2023 13:13:17 GMT</pubDate>
    </item>
    <item>
      <title>尝试从 inceptionv3 架构中提取特征时出现图形断开连接错误</title>
      <link>https://stackoverflow.com/questions/77713548/graph-disconnected-error-when-trying-to-extract-features-from-inceptionv3-archit</link>
      <description><![CDATA[我正在尝试从架构中间提取一些特征并将其用于另一个模型。
base_model = InceptionV3(weights=&#39;imagenet&#39;, include_top=False)
input_tensor = Input(shape=(299, 299, 3)) # InceptionV3 的输入形状
InceptionA_feature_extractor = models.Model(inputs=input_tensor,outputs=base_model.get_layer(&#39;mixed2&#39;).output)

在尝试执行此操作时，我收到以下错误，可能的原因是什么？
ValueError：图形已断开连接：无法获取张量 KerasTensor 的值（type_spec=TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name=&#39;input_6&#39;), name=&#39; input_6&#39;，描述=“由层&#39;input_6&#39;创建”）在层“conv2d_376”。访问以下先前层没有问题：[]
]]></description>
      <guid>https://stackoverflow.com/questions/77713548/graph-disconnected-error-when-trying-to-extract-features-from-inceptionv3-archit</guid>
      <pubDate>Mon, 25 Dec 2023 11:49:34 GMT</pubDate>
    </item>
    <item>
      <title>自定义梯度提升分类器实现。训练无进展</title>
      <link>https://stackoverflow.com/questions/77710582/custom-gradient-boosting-classifier-implementation-no-training-progress</link>
      <description><![CDATA[我正在尝试实现一个GradientBoostingClassifier
我从 StatQuest 视频中获取了该算法（梯度提升第 4 部分（共 4 部分）：分类详细信息）并尝试使用 numpy + sklearn.DecisionTreeRegressor 作为基本模型来实现它。
这是我的代码：
从 sklearn.tree 导入 DecisionTreeRegressor

定义 sigmoid(x):
  如果x&gt; 0:
    z = np.exp(-x)
    返回 1/(1+z)
  别的：
    z = np.exp(x)
    返回 z/(1+z)

类 GradientBoostingClassifier()：
  def __init__(self, n_estimators=20, lr=0.1):
    self.n_estimators = n_estimators
    self.lr = lr
    self.training_history = {
        “log_loss”：[]，“roc_auc”：[]，“pr_auc”：[]
    }
    self.base_learners = []

  def fit(自身, X, y):
    数据 = X.copy()
    特征=数据.列
    # 1. 使用常量值初始化模型：
    p = y.sum() / len(y)
    赔率 = p / (1-p)
    log_odds = np.log(赔率)
    数据[&#39;cur_log_odds&#39;] = log_odds
    数据[&#39;预测&#39;] = 数据[&#39;cur_log_odds&#39;].apply(sigmoid)

    self.training_history[&#39;log_loss&#39;].append( log_loss(y, data[&#39;prediction&#39;]) )
    self.training_history[&#39;roc_auc&#39;].append( roc_auc_score(y, data[&#39;预测&#39;]) )
    self.training_history[&#39;pr_auc&#39;].append(average_ precision_score(y, data[&#39;prediction&#39;]) )

    # 2. 对于 m = 1 到 M：
    for _ in tqdm(range(self.n_estimators)):
      # 2.1 计算所谓的伪残差：
      数据[&#39;残差&#39;] = (数据[&#39;预测&#39;] - y)

      # 2.2 拟合基学习器回归器来预测伪残差：
      base_learner = DecisionTreeRegressor（max_深度 = 3，min_samples_split = 2，random_state = 42）
      base_learner.fit(数据[特征],数据[&#39;残差&#39;])
      self.base_learners.append(base_learner)

      # 2.3 对于每个叶子计算其输出对数几率
      # 从回归树中获取叶子数
      数据[&#39;叶子&#39;] = base_learner.apply(数据[特征])

      # 将输出对数奇数计算为 sum(residuals) / sum(old_prediction * (1 - old_prediction))
      leafs_output = data.groupby(&#39;leaf&#39;, as_index=False).apply(
          lambda d: d[&#39;残差&#39;].sum() / (0.00001+(d[&#39;预测&#39;] * (1-d[&#39;预测&#39;]) ).sum())
      ).rename(columns={无: &#39;lambda_odds&#39;})

      数据 = data.merge(leafs_output, on=&#39;leaf&#39;)
      # 2.4 更新 current_log_odds = current_log_odds + lr*predicted_log_odds
      数据[&#39;cur_log_odds&#39;] += self.lr*data[&#39;lambda_odds&#39;]
      数据 = data.drop(&#39;lambda_odds&#39;, axis=1)

      数据[&#39;预测&#39;] = 数据[&#39;cur_log_odds&#39;].apply(sigmoid)

      self.training_history[&#39;log_loss&#39;].append( log_loss(y, data[&#39;prediction&#39;]) )
      self.training_history[&#39;roc_auc&#39;].append( roc_auc_score(y, data[&#39;预测&#39;]) )
      self.training_history[&#39;pr_auc&#39;].append(average_ precision_score(y, data[&#39;prediction&#39;]) )

    返回数据

  def Predict_proba(自身, X):
    经过

问题是，即使经过 1000 次迭代 (n_estimators = 1000)，我的 roc_auc 和 pr_auc 分数也接近随机模型给出的分数（roc_auc=0.5，pr_auc=0.29，这是正类的比例）。
我做错了什么？
即使在 n_estimators = 10 的情况下，sklearn GradientBoostingClassifier 实现在同一数据集上也能给出更高的分数]]></description>
      <guid>https://stackoverflow.com/questions/77710582/custom-gradient-boosting-classifier-implementation-no-training-progress</guid>
      <pubDate>Sun, 24 Dec 2023 12:11:08 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络不学习</title>
      <link>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</link>
      <description><![CDATA[我正在尝试在包含 1500 张图像（15 个类别）的训练集上训练用于图像识别的卷积神经网络。有人告诉我，采用这种架构和从均值为 0、标准差为 0.01 的高斯分布得出的初始权重以及初始偏差值为 0 的情况，在适当的学习率下，它应该达到 30 左右的准确度%。
但是，它根本没有学到任何东西：准确率与随机分类器相似，并且训练后的权重仍然遵循正态分布。我做错了什么？
这是神经网络
class simpleCNN(nn.Module)：
  def __init__(自身):
    super(simpleCNN,self).__init__() #初始化模型

    self.conv1=nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,stride=1) #输出图像大小为(size+2*padding-kernel)/stride --&gt;62*62
    self.relu1=nn.ReLU()
    self.maxpool1=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像62/2--&gt;31*31

    self.conv2=nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,stride=1) #输出图像为29*29
    self.relu2=nn.ReLU()
    self.maxpool2=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像为29/2--&gt;14*14（MaxPool2d近似大小与floor）

    self.conv3=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1) #输出图像为12*12
    self.relu3=nn.ReLU()

    self.fc1=nn.Linear(32*12*12,15) #16 个通道 * 16*16 图像（64*64，步幅为 2 的 2 个 maxpooling），15 个输出特征=15 个类
    self.softmax = nn.Softmax(dim=1)

  def 前向（自身，x）：
    x=self.conv1(x)
    x=self.relu1(x)
    x=self.maxpool1(x)

    x=self.conv2(x)
    x=self.relu2(x)
    x=self.maxpool2(x)

    x=self.conv3(x)
    x=self.relu3(x)

    x=x.view(-1,32*12*12)

    x=self.fc1(x)
    x=self.softmax(x)

    返回x

初始化：
def init_weights(m):
  如果 isinstance(m,nn.Conv2d) 或 isinstance(m,nn.Linear)：
    nn.init.normal_(m.weight,0,0.01)
    nn.init.zeros_(m.bias)

模型 = simpleCNN()
模型.应用（init_weights）

训练函数：
loss_function=nn.CrossEntropyLoss()
优化器=optim.SGD(model.parameters(),lr=0.1,动量=0.9)

def train_one_epoch(epoch_index,loader):
  运行损失=0

  对于 i，枚举（加载器）中的数据：

    input,labels=data #获取小批量
    输出=模型（输入）#前向传递

    loss=loss_function(outputs,labels) #计算损失
    running_loss+=loss.item() #总结到目前为止处理的小批量的损失

    Optimizer.zero_grad() #重置梯度
    loss.backward() #计算梯度
    optimizer.step() #更新权重

  return running_loss/(i+1) # 每个小批量的平均损失


培训：
&lt;前&gt;&lt;代码&gt;纪元=20

best_validation_loss=np.inf

对于范围内的纪元（EPOCHS）：
  print(&#39;纪元{}:&#39;.format(纪元+1))

  模型.train(True)
  train_loss=train_one_epoch(epoch,train_loader)

  运行验证损失=0.0

  模型.eval()

  with torch.no_grad(): # 禁用梯度计算并减少内存消耗
    对于 i，枚举中的 vdata（validation_loader）：
      vinputs,vlabels=vdata
      v输出=模型（v输入）
      vloss=loss_function(v输出,v标签)
      running_validation_loss+=vloss.item()
  验证损失=运行验证损失/(i+1)
  print(&#39;LOSS 训练：{} 验证：{}&#39;.format(train_loss,validation_loss))

  if validation_loss
使用默认初始化，它的效果会好一点，但我应该使用高斯达到 30%。
您能发现一些可能导致它无法学习的问题吗？我已经尝试过不同的学习率和动力。]]></description>
      <guid>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</guid>
      <pubDate>Fri, 22 Dec 2023 14:06:23 GMT</pubDate>
    </item>
    <item>
      <title>使用人工智能 (AI) 预测股票价格</title>
      <link>https://stackoverflow.com/questions/2686981/using-artificial-intelligence-ai-to-predict-stock-prices</link>
      <description><![CDATA[给定一组与  非常相似的数据Motley Fool CAPS 系统，个人用户可以在其中输入各种股票的买入和卖出建议。我想做的是显示每个建议，并且我猜想它是否是未来股票价格（或每股收益或其他）的良好预测器&lt;5&gt;（即相关系数= 1）的评级（1-5）或一个可怕的预测变量（即相关系数 = -1）或介于两者之间。
每个推荐都标记给特定用户，以便可以随着时间的推移进行跟踪。我还可以根据 SP500 价格等来跟踪市场方向（看涨/看跌）。我认为在模型中有意义的组件是：

&lt;前&gt;&lt;代码&gt;用户
方向（长/空）
市场方向
股票部门

我们的想法是，有些用户在牛市中比熊市中表现更好（反之亦然），而有些用户在空头方面比多头方面更好，然后是上述的组合。我可以自动标记市场方向和板块（基于当时的市场和推荐的股票）。
我的想法是，我可以呈现一系列屏幕，并允许我通过显示特定时间段内的可用数据绝对值、市场和部门表现来对每个单独的推荐进行排名。我会按照详细的列表对股票进行排名，以便排名尽可能客观。我的假设是单个用户正确的概率不超过 57% - 但谁知道呢。
我可以加载系统并说“让我们将推荐排名为 90 天后股票价值的预测指标”；这将代表一组非常明确的排名。
现在，关键是 - 我想创建某种机器学习算法，可以识别一系列时间的模式，以便当建议流入应用程序时，我们维护该股票的排名（即类似于相关系数） ）关于该推荐（除了过去的一系列推荐之外）影响价格的可能性。
现在这是超级关键。我从未上过人工智能课程/读过人工智能书籍/更不用说特定于机器学习的内容。因此，我正在寻找指导 - 我可以适应的类似系统的示例或描述。寻找信息或任何一般帮助的地方。或者甚至推动我朝着正确的方向开始......
我的希望是用 F# 来实现这一点，并能够通过 F# 中的新技能、机器学习的实现以及我可以包含在技术组合或博客空间中的潜在内容（应用程序/源代码）给我的朋友留下深刻的印象； 
感谢您提前提供任何建议。]]></description>
      <guid>https://stackoverflow.com/questions/2686981/using-artificial-intelligence-ai-to-predict-stock-prices</guid>
      <pubDate>Wed, 21 Apr 2010 22:24:38 GMT</pubDate>
    </item>
    </channel>
</rss>