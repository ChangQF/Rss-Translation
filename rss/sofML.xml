<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 09 Sep 2024 09:18:46 GMT</lastBuildDate>
    <item>
      <title>大型语言模型中的 d 维参数向量是否具有相等的平方和？</title>
      <link>https://stackoverflow.com/questions/78964442/are-d-dim-vectors-of-parameters-in-large-language-models-have-equal-sums-of-squa</link>
      <description><![CDATA[我正在做大型语言模型中的力学分析研究，想解释注意力模块的参数和FF模块的参数。但我有一个疑问，大型语言模型中的d维参数向量（Wq Wk Wv Wo矩阵的列向量和K V矩阵的行向量）在预训练之后是否仍然具有相等的平方和，并且这些参数的初始化满足假设。我发现了一些类似的研究（训练自然语言生成模型中的表示退化问题、IsoScore：测量嵌入空间利用的均匀性、语境化的词表示有多语境化？比较 BERT、ELMo 和 GPT-2 嵌入的几何形状、通过频谱控制改进神经语言生成），但它们专注于词嵌入的各向同性，并指出嵌入不是各向同性的。
初始化时，这些参数满足假设，条目从均值为零、方差有限的分布 D 中独立同分布采样。该说法的证明很简单。但我不知道如何调查它们在预训练后是否仍然满足，是否有一些与此相关的研究？]]></description>
      <guid>https://stackoverflow.com/questions/78964442/are-d-dim-vectors-of-parameters-in-large-language-models-have-equal-sums-of-squa</guid>
      <pubDate>Mon, 09 Sep 2024 07:57:18 GMT</pubDate>
    </item>
    <item>
      <title>是否可以从 e1071 中选出距离超平面最远的那些被 SVM 错误分类的样本？</title>
      <link>https://stackoverflow.com/questions/78964423/is-it-possible-to-select-those-misclassified-samples-by-svm-from-e1071-that-are</link>
      <description><![CDATA[我正在使用 R 中的 e1071 库中的 SVM 模型，我的问题只是我是否可以识别模型错误分类且距离 SVM 超平面最远的样本。
我的代码如下：
best_model &lt;- list(
model = svm(x = omicDataReduced[, -which(colnames(omicDataReduced) == classVariable)],
y = omicDataReduced[[classVariable]],
kernel = best_kernel,
cost = best_C,
probability = TRUE,
decision.values = TRUE),
error = best_error,
cost = best_C,
kernel = best_kernel
)

我的数据集有 790 个样本和 18,710 个预测变量。
在此先谢谢您，如果您如果需要其他任何东西，请随时询问。
此致，
José Adrián。
]]></description>
      <guid>https://stackoverflow.com/questions/78964423/is-it-possible-to-select-those-misclassified-samples-by-svm-from-e1071-that-are</guid>
      <pubDate>Mon, 09 Sep 2024 07:52:54 GMT</pubDate>
    </item>
    <item>
      <title>具有一个主导变量或仅 1 个变量的 ML 模型</title>
      <link>https://stackoverflow.com/questions/78964182/ml-model-with-a-dominant-or-just-1-variable</link>
      <description><![CDATA[您能否推荐一些学术资源，例如研究论文或书籍章节，用于研究使用具有主导变量的机器学习模型的效果，这些模型可以解释目标中的大部分方差，或者仅使用一个变量构建的模型？具体来说，我有兴趣了解与这些方法相关的潜在缺点、偏见或局限性。]]></description>
      <guid>https://stackoverflow.com/questions/78964182/ml-model-with-a-dominant-or-just-1-variable</guid>
      <pubDate>Mon, 09 Sep 2024 06:46:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 k-fold 进行逻辑回归并找到最佳阈值</title>
      <link>https://stackoverflow.com/questions/78964032/using-k-fold-for-logistic-regression-and-finding-optimal-threshold</link>
      <description><![CDATA[我的数据集非常不平衡。
我正在尝试使用 k 折并尝试拟合。但我不明白这会带来什么不同，因为我们只会在原始数据集上测试它？
我已经对数据进行了拆分、独热编码和缩放。此外，我还使用 smote-enn 对其进行了重新采样。]]></description>
      <guid>https://stackoverflow.com/questions/78964032/using-k-fold-for-logistic-regression-and-finding-optimal-threshold</guid>
      <pubDate>Mon, 09 Sep 2024 05:40:29 GMT</pubDate>
    </item>
    <item>
      <title>为什么 nn.Linear(in_features, out_features) 在 PyTorch 中使用形状为 (out_features, in_features) 的权重矩阵？</title>
      <link>https://stackoverflow.com/questions/78963755/why-does-nn-linearin-features-out-features-use-a-weight-matrix-of-shape-out</link>
      <description><![CDATA[我试图理解为什么 PyTorch 的 nn.Linear(in_features, out_features) 层的权重矩阵具有形状 (out_features, in_features) 而不是 (in_features, out_features)。
从基本矩阵乘法的角度来看，具有形状 (in_features, out_features) 似乎可以消除在乘法过程中转置权重矩阵的需要。例如，对于形状为 (batch_size, in_features) 的输入张量 x，与形状为 (in_features, out_features) 的权重矩阵相乘将直接产生形状为 (batch_size, out_features) 的输出，而无需转置操作。
但是，PyTorch 将权重矩阵定义为 (out_features, in_features)，这意味着它在前向传递过程中会被转置。这种设计有什么好处？它如何与线性代数和神经网络实现的更广泛原则保持一致？这种选择背后是否有任何效率或一致性考虑使其更可取？
我很感激任何关于这一设计决策背后理由的见解。]]></description>
      <guid>https://stackoverflow.com/questions/78963755/why-does-nn-linearin-features-out-features-use-a-weight-matrix-of-shape-out</guid>
      <pubDate>Mon, 09 Sep 2024 03:08:49 GMT</pubDate>
    </item>
    <item>
      <title>无法让 MMCV 构建来检测我的 Conda Env 的 Cuda</title>
      <link>https://stackoverflow.com/questions/78963624/cant-get-mmcv-build-to-detect-my-conda-envs-cuda</link>
      <description><![CDATA[我通过 conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia 安装了 pytorch，然后安装了 conda install cuda=11.8 -c nvidia 以获取构建工具，但是在运行 pip install -U openmim 然后运行 ​​mim install mmcv 之后，我得到了一个 Thedetected CUDA version (12.5) mismatches the version that was used to compile PyTorch (11.8). Please make sure to use the same CUDA editions. 错误，即使运行 nvcc -V 显示 cuda 11.8。我的主机正在运行 cuda 12.5。我该如何解决这个问题？（以及 cuda、cudatoolkit、cudatoolkit-dev 和 cuda-toolkit 等所有 cuda conda 包之间有什么区别？）]]></description>
      <guid>https://stackoverflow.com/questions/78963624/cant-get-mmcv-build-to-detect-my-conda-envs-cuda</guid>
      <pubDate>Mon, 09 Sep 2024 01:22:33 GMT</pubDate>
    </item>
    <item>
      <title>结合语音、面部表情和文本数据进行实时心理健康监测的挑战</title>
      <link>https://stackoverflow.com/questions/78963600/challenges-in-combining-speech-facial-expression-and-text-data-for-real-time-m</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78963600/challenges-in-combining-speech-facial-expression-and-text-data-for-real-time-m</guid>
      <pubDate>Mon, 09 Sep 2024 01:01:00 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 的机器学习模型中处理具有数值的对象数据类型变量？[关闭]</title>
      <link>https://stackoverflow.com/questions/78963448/how-to-deal-with-object-data-type-variables-with-numeric-values-in-machine-learn</link>
      <description><![CDATA[我尝试在 Python 中构建机器学习模型，我有几个变量的数据类型为“object”，值如下：411,71 等。我的问题是：

我是否必须将这种具有 411,71 等值的对象变量更改为数字类型才能使用 xgboost 或随机森林等算法？

如何在我的数据集中找到适合将数据类型更改为数字的“object”类型的变量，即具有 411,71 等值的变量，以及在这种情况下是否也需要将“，”更改为“。”？你能给我展示一下 Python Pandas 中的示例代码来做到这一点吗？

]]></description>
      <guid>https://stackoverflow.com/questions/78963448/how-to-deal-with-object-data-type-variables-with-numeric-values-in-machine-learn</guid>
      <pubDate>Sun, 08 Sep 2024 22:50:51 GMT</pubDate>
    </item>
    <item>
      <title>如何在增加训练数据时调整 LightGBM 参数（如“min_child_samples”）？</title>
      <link>https://stackoverflow.com/questions/78963446/how-to-adjust-lightgbm-parameters-like-min-child-samples-when-increasing-train</link>
      <description><![CDATA[我正在训练 LightGBM 模型，目前面临着增加训练数据量时参数调整的困境。
最初，我将数据集分成 90% 用于训练，10% 用于测试。使用网格搜索，我找到了模型的最佳参数。现在，我想利用 100% 的数据来训练模型，使其尽可能强大。
我的问题是关于 min_child_samples 等参数，它与数据量有关。当我将数据从 90% 增加到 100% 时，我应该将 min_child_samples 保持为与 90% 数据训练期间找到的值相同吗？还是应该因为数据量增加了而进行调整？
有人可以提供如何处理此问题的指导，或者分享任何最佳实践吗？]]></description>
      <guid>https://stackoverflow.com/questions/78963446/how-to-adjust-lightgbm-parameters-like-min-child-samples-when-increasing-train</guid>
      <pubDate>Sun, 08 Sep 2024 22:50:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 Raspberry Pi 5 在 Edge TPU 上运行 YOLOv8 分割模型时出现 KeyError</title>
      <link>https://stackoverflow.com/questions/78963308/keyerror-when-running-yolov8-segmentation-model-on-edge-tpu-with-raspberry-pi-5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78963308/keyerror-when-running-yolov8-segmentation-model-on-edge-tpu-with-raspberry-pi-5</guid>
      <pubDate>Sun, 08 Sep 2024 21:01:01 GMT</pubDate>
    </item>
    <item>
      <title>优化心理健康应用程序中的实时多模式数据集成和机器学习[关闭]</title>
      <link>https://stackoverflow.com/questions/78962995/optimizing-real-time-multimodal-data-integration-and-machine-learning-in-a-menta</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78962995/optimizing-real-time-multimodal-data-integration-and-machine-learning-in-a-menta</guid>
      <pubDate>Sun, 08 Sep 2024 17:57:14 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 中的 Autograd Trainstep 中的 Lightning</title>
      <link>https://stackoverflow.com/questions/78956646/autograd-in-pytorch-lightning-in-trainstep</link>
      <description><![CDATA[我想实现一个基于 Pytorch Lightning 的 ML 训练，其中我使用 autograd 功能进行训练损失计算：
def training_step(self, batch, batch_idx):
x, y = batch
y_hat = self(x)
loss = self.loss_function(y_hat, y)
return loss

X 的每个样本 x 都是一个二维向量 x = [v, a]。
在训练步骤中，我想计算 y_hat 相对于 的梯度。到 v。
损失进一步通过以下方式计算：
loss = mse(y,y_hat) + mse(gradient,gradient_hat)

其中给出了（真实）梯度。
到目前为止，尝试了 y_hat.backward() 的（典型）方法，但无法使其工作：
def training_step(self, batch, batch_idx):
x, y = batch
x.requires_grad_(True) # 确保我们跟踪 x 的梯度
y_hat = self(x)

# 计算 y_hat 相对于 v 的梯度（x[:, 0]）
v = x[:, 0]
grads = torch.autograd.grad(y_hat, v, grad_outputs=torch.ones_like(y_hat), create_graph=True)[0] # ...
]]></description>
      <guid>https://stackoverflow.com/questions/78956646/autograd-in-pytorch-lightning-in-trainstep</guid>
      <pubDate>Fri, 06 Sep 2024 10:08:53 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Keras 中使用 flow_from_directory 和多个目录实现多输出神经网络</title>
      <link>https://stackoverflow.com/questions/78951880/how-to-use-flow-from-directory-with-multiple-directories-for-multi-output-neural</link>
      <description><![CDATA[我需要使用 Keras 中的 flow_from_directory 从多个目录加载图像。我的目录结构如下：
Images_folder/ 
═── Carpet_1/ 
│ ═── training/ 
│ │ ═── class_1/ 
│ │ ═── class_2/ 
│ ═── validation/ 
═── Carpet_2/ 
│ ═── training/ 
│ │ ═── class_1/ 
│ │ ═── class_2/ 
│ ═── validation/ 
...

每个“Carpet”目录（例如 Carpet_1、Carpet_2）包含相同的一组类（class_1、class_2 等）。我想使用来自所有这些目录的图像来训练 CNN。我的目标是构建一个多输出神经网络，其中一个输出预测“Carpet”数字（1、2、3、...），另一个输出预测该地毯内的类别。
鉴于这种结构，我如何使用 ImageDataGenerator 或 Keras 中的任何其他方法来加载和预处理这些图像？有没有办法将所有这些目录中的图像组合成一个生成器，同时仍然允许我区分不同的地毯？]]></description>
      <guid>https://stackoverflow.com/questions/78951880/how-to-use-flow-from-directory-with-multiple-directories-for-multi-output-neural</guid>
      <pubDate>Thu, 05 Sep 2024 07:56:05 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用 Google Teachable 机器模型作为对象检测模型吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78951811/can-i-use-google-teachable-machine-model-as-object-detection-model</link>
      <description><![CDATA[我正在开发一款带有对象检测功能的移动自动收银应用程序。我面临的问题是，自定义项目的变体太多了（大约 250 个类别）。如果我要对以前的模型（如 MobileNet 或 YOLO）进行微调，这将花费太多时间，因为我仍然需要创建 POS、数据库和集成热敏打印机。
那么，我是否可以只使用 Google Teachable Machine 来处理我的对象检测数据集（假设背景相同），而不是对以前的模型进行微调？
应用程序的工作方式是，收银员将在白色背景上拍摄买家想要购买的商品的照片，然后应用程序将自动检测出哪些商品在画面中（使用 Google Teachable Machine .tflite 模型）。
Teachable Machine .tflite 模型是否可以替换下面代码中的 modelPath？
private fun runObjectDetection(bitmap: Bitmap) {
// 步骤 1：创建 TFLite 的 TensorImage 对象
val image = TensorImage.fromBitmap(bitmap)

// 步骤 2：初始化检测器对象
val options = ObjectDetector.ObjectDetectorOptions.builder()
.setMaxResults(5)
.setScoreThreshold(0.5f)
.build()
val detector = ObjectDetector.createFromFileAndOptions(
this, // 应用程序上下文
**&quot;model.tflite&quot;, **
options
)
// 步骤 3：将给定的图像输入模型并打印检测结果
val results = detector.detect(image)

// 步骤 4：解析检测结果并显示
debugPrint(results)

val resultToDisplay = results.map {
// 获取 top-1 类别并制作显示文本
val category = it.categories.first()
val text = &quot;${category.label}, ${category.score.times(100).toInt()}%&quot;

// 创建数据对象，用于显示检测结果
DetectionResult(it.boundingBox, text)
}

// 将检测结果绘制到位图上并显示。
val imgWithResult = drawDetectionResult(bitmap, resultToDisplay)
runOnUiThread {
inputImageView.setImageBitmap(imgWithResult)
}
}
]]></description>
      <guid>https://stackoverflow.com/questions/78951811/can-i-use-google-teachable-machine-model-as-object-detection-model</guid>
      <pubDate>Thu, 05 Sep 2024 07:38:34 GMT</pubDate>
    </item>
    <item>
      <title>我的 pytorch 模型 FPS 低且延迟严重</title>
      <link>https://stackoverflow.com/questions/78943962/low-fps-and-lot-of-delay-with-my-pytorch-model</link>
      <description><![CDATA[我有一个任务，使用 OpenCV 处理来自 Hikvision IP 摄像机的流媒体视频。最初，视频的 FPS 约为 20-25，延迟为 2-3 秒。然而，随着代码运行时间的延长，FPS 迅速下降，延迟增加。最终，FPS 下降到 2-3，视频冻结。
class VideoLoader:
@staticmethod
def load_video(video_path):
cap = cv2.VideoCapture(video_path)
cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter.fourcc(&#39;M&#39;, &#39;J&#39;, &#39;P&#39;, &#39;G&#39;))
cap.set(cv2.CAP_PROP_FPS, 25)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) 
assert cap.isOpened(), &quot;Video dosyası okunurken hata oluştu&quot;
return cap

我尝试过使用 GPU、采用多线程和降低分辨率等方法，但这些方法效果都不太好。您认为问题是什么，我该如何解决它
def frame_reader(cap, frame_queue):
while cap.isOpened():
success, frame = cap.read()
if not success:
break
if not frame_queue.full(): # 检查队列是否有空间
frame_queue.put(frame)
cap.release()
frame_queue.put(None) # 流结束信号

def video_processor(frame_queue, output_queue, model, class_names, playing_sounds):
Threshold = 0.5 # 测试结果
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model.to(device)

while True:
frame = frame_queue.get()
if frame is None: # 流结束信号
break
start_time = time.time()
img = cv2.resize(frame, (640, 480))
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float().unsqueeze(0).to(device)
img_tensor /= 255.0 # 标准化

def video_detection(video_source):
cap = VideoLoader.load_video(video_source)
model = YOLO(&quot;YOLO-Weights/ppe.pt&quot;)
class_names = [&#39;safety-glasses&#39;, &#39;gloves&#39;, &#39;orange-vest&#39;, &#39;yellow-vest&#39;,]
playing_sounds = set()

frame_queue = Queue(maxsize=5)
output_queue =队列（最大大小=5）

reader_thread = threading.Thread（目标=frame_reader，参数=（cap，frame_queue），守护进程=True）
processor_thread = threading.Thread（目标=video_processor，参数=（frame_queue，output_queue，model，class_names，played_sounds），守护进程=True）

reader_thread.start()
processor_thread.start()
]]></description>
      <guid>https://stackoverflow.com/questions/78943962/low-fps-and-lot-of-delay-with-my-pytorch-model</guid>
      <pubDate>Tue, 03 Sep 2024 10:50:27 GMT</pubDate>
    </item>
    </channel>
</rss>