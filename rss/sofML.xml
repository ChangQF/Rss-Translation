<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 30 Nov 2023 12:26:33 GMT</lastBuildDate>
    <item>
      <title>是否可以微调稳定扩散模型或任何其他扩散模型以支持孟加拉语等不同语言？</title>
      <link>https://stackoverflow.com/questions/77577322/is-it-possible-to-fine-tune-stable-diffusion-model-or-any-other-diffusion-model</link>
      <description><![CDATA[我是一名本科生，对AI了解不多。
我需要一个文本到图像生成器扩散模型，可以将孟加拉文本作为输入并生成图像。是否有任何以孟加拉语文本作为输入的扩散模型？或者我可以微调扩散模型以支持孟加拉语吗？如果可能的话，哪种扩散模型最适合这项任务？]]></description>
      <guid>https://stackoverflow.com/questions/77577322/is-it-possible-to-fine-tune-stable-diffusion-model-or-any-other-diffusion-model</guid>
      <pubDate>Thu, 30 Nov 2023 09:34:51 GMT</pubDate>
    </item>
    <item>
      <title>如何让训练有素的 LSTM 预测了解即将发生的已知特殊事件？</title>
      <link>https://stackoverflow.com/questions/77577239/how-to-make-a-well-trained-lstm-forecast-aware-of-the-upcoming-known-special-eve</link>
      <description><![CDATA[我根据每日销售数据训练了 LSTM 模型，其中包括对销售影响很大的节假日和特殊活动（作为二进制指标）。但是，虽然在测试集之外进行预测，但该模型并未考虑有影响力的已知未来假期。我怎样才能让我的模型意识到这一点？
我使用包含销售和假期的数据集作为二进制指标，直到记录销售的最后一天。我有未来假期二进制指标数据集，但它不包含在此处。问题是如何使用它进行预测。这是我的初始代码，它在测试集上给出了良好的结果：
data = df_resampled[[&#39;销售&#39;, &#39;Hol1&#39;, &#39;Hol2&#39;, &#39;Hol3&#39;, &#39;Hol4&#39;, &#39;Hol5&#39;,&#39;周末&#39;,&#39;夏季&#39;]].values

# 标准化数据

缩放器 = MinMaxScaler(feature_range=(0, 1))
data_scaled = 缩放器.fit_transform(数据)

# 分割数据

train_size = int(len(data_scaled) * 0.8)
训练，测试 = data_scaled[:train_size], data_scaled[train_size:]

# LSTM 训练序列

def create_sequences(数据, seq_length):
序列、目标 = []、[]
对于范围内的 i（len（数据）- seq_length）：
seq = 数据[i:i + seq_length]
目标=数据[i + seq_length]
序列.append(seq)
目标.append(目标)
返回 np.array(序列), np.array(目标)

序列长度 = 10
train_sequences, train_targets = create_sequences(train, seq_length)
test_sequences, test_targets = create_sequences(测试, seq_length)

# 重塑输入（样本、时间步长、特征）

train_sequences = np.reshape(train_sequences, (train_sequences.shape[0], train_sequences.shape[1], train_sequences.shape[2]))
test_sequences = np.reshape(test_sequences, (test_sequences.shape[0], test_sequences.shape[1], test_sequences.shape[2]))

# 具有附加功能的 LSTM 模型

模型=顺序（）
model.add(LSTM(单位=50, input_shape=(train_sequences.shape[1], train_sequences.shape[2])))
model.add(密集(单位=1))
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)

# 训练模型

model.fit（train_sequences，train_targets [：，0]，epochs = 50，batch_size = 32）

# 对测试数据进行预测

test_predictions = model.predict(test_sequences)

# 反转至原始比例

test_predictions_inv = scaler.inverse_transform(np.concatenate((test_predictions, test_targets[:, 1:]), axis=1))[:, 0]
test_targets_inv = 缩放器.inverse_transform(test_targets)[:, 0]
]]></description>
      <guid>https://stackoverflow.com/questions/77577239/how-to-make-a-well-trained-lstm-forecast-aware-of-the-upcoming-known-special-eve</guid>
      <pubDate>Thu, 30 Nov 2023 09:22:08 GMT</pubDate>
    </item>
    <item>
      <title>llama 模型来获取 pdf 上下文中问题的答案</title>
      <link>https://stackoverflow.com/questions/77576528/llama-model-to-get-answers-of-the-questions-which-are-in-the-context-of-pdf</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77576528/llama-model-to-get-answers-of-the-questions-which-are-in-the-context-of-pdf</guid>
      <pubDate>Thu, 30 Nov 2023 07:14:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用同一台计算机为多个用户高效加载深度学习模型</title>
      <link>https://stackoverflow.com/questions/77575664/how-to-efficiently-load-deep-learning-model-for-multiple-users-using-the-same-co</link>
      <description><![CDATA[我正在制作一个使用多个深度学习模型的简单软件。该软件安装在计算机上。并且有多个用户同时使用该软件（使用远程桌面连接）。
我认为如果每个用户单独加载模型，将会出现内存不足的问题。
在这种情况下加载模型的最佳方法是什么？我们可以加载模型一次，然后让每个用户都访问同一个加载的模型吗？]]></description>
      <guid>https://stackoverflow.com/questions/77575664/how-to-efficiently-load-deep-learning-model-for-multiple-users-using-the-same-co</guid>
      <pubDate>Thu, 30 Nov 2023 03:12:24 GMT</pubDate>
    </item>
    <item>
      <title>修改来自google的ml教程代码没有给出预期的结果</title>
      <link>https://stackoverflow.com/questions/77575529/modifying-ml-tutorial-code-from-google-does-not-give-expected-result</link>
      <description><![CDATA[有一个很好的使用tensorflow lib的ml python代码的迷你示例。
Google 代码实验室教程
它（正确地）从线性方程预测一个数字。但仅仅制作一个小模型来训练模型并预测二次函数就会得到完全错误的结果。
导入tensorflow为tf
将 numpy 导入为 np
从张量流导入keras

模型 = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer=&#39;sgd&#39;, loss=&#39;mean_squared_error&#39;)

# 从原始教程修改 -&gt; y = 2x^2-1
xs = np.array([-3.0, -2.0, -1.0, 0.0, 2.0, 3.0, 4.0, 5.0], dtype=float)
ys = np.array([ 17.0, 7.0, 1.0, -1.0, 7.0, 17.0, 31.0, 49.0], dtype=float)

model.fit(xs, ys, epochs=5000)

打印（模型.预测（[1.0]））

给出结果：
&lt;前&gt;&lt;代码&gt;&gt;&gt;&gt;打印（模型.预测（[1.0]））
1/1 [================================] - 0s 84ms/步
[[15.999977]]
&gt;&gt;&gt;&gt;&gt;

我本来预计大约。 1.0。
不知道出了什么问题。]]></description>
      <guid>https://stackoverflow.com/questions/77575529/modifying-ml-tutorial-code-from-google-does-not-give-expected-result</guid>
      <pubDate>Thu, 30 Nov 2023 02:28:17 GMT</pubDate>
    </item>
    <item>
      <title>fiass 在查找相似图像方面比余弦相似度更好吗？我们应该标准化嵌入吗？</title>
      <link>https://stackoverflow.com/questions/77574460/is-fiass-better-than-cosine-similarity-in-finding-similar-images-should-we-nor</link>
      <description><![CDATA[我正在研究一个产品识别人工智能项目。任务如下：我们必须找到公司销售的图像中的物体，然后我们必须生成 6 个相似的产品，为此我们使用接地恐龙进行零镜头物体检测，然后进行剪辑以计算余弦相似度在裁剪图像的嵌入与具有相同产品类别或类别的图像的嵌入之间，我们的问题如下：
我们希望它检测的类别非常相似，包括凳子、桌子、书桌、架子，因此有时它会对同一对象标记两次，或者将其标记为与真实标签类似的内容。
其次，同一类别的两个看起来不相似的对象之间的余弦相似度有时非常高，而完全相同产品的图像之间的余弦相似度则较低
我们应该做什么任何帮助将非常感激，问题是因为恐龙还是剪辑，我们是否解决检测或相似性
使用 fiass libraray 代替余弦相似度更好吗？它会生成更多相似的图像
我们尝试增加 dino 的阈值，并在提示中使用单个类多次运行它，这样我们可以设置高阈值而不丢失数据
我们的导师建议我们使用伪标签和 100 个带注释的图像对恐龙进行半监督学习
我们无法训练分类器，因为我们的数据集中只有 200 张图像
我们应该在计算余弦相似度之前对嵌入进行归一化]]></description>
      <guid>https://stackoverflow.com/questions/77574460/is-fiass-better-than-cosine-similarity-in-finding-similar-images-should-we-nor</guid>
      <pubDate>Wed, 29 Nov 2023 20:59:25 GMT</pubDate>
    </item>
    <item>
      <title>RedshiftML - 再训练模型</title>
      <link>https://stackoverflow.com/questions/77573576/redshiftml-retraining-models</link>
      <description><![CDATA[有人有使用 redshiftML 的经验并有更新模型的指导吗？我让我们的 BI 团队在 Redshift 中启用了此功能，但我们的组织没有使用此工具的经验。当我尝试更新模型时，我最终遇到了各种类型的错误，我希望有人能给我指出正确的方向
删除模型时，出现以下错误。这是权限问题吗？
[Amazon](500310) 无效操作：函数 149004315 的缓存查找失败 
重新训练模型时，出现以下错误。是否有关于更换/重新训练模型的指导？
[Amazon](500310) 无效操作：函数“model_name”已存在具有相同参数类型的
没有太多文档，因此我最初按照以下说明开始：https://docs.aws.amazon.com/redshift/latest/dg/r_DROP_MODEL.html
如果我无法克服删除模型的错误，那么我想我最终将不得不设置每日模型版本控制并使数据库膨胀
https://aws.amazon .com/blogs/big-data/implement-model-versioning-with-amazon-redshift-ml/]]></description>
      <guid>https://stackoverflow.com/questions/77573576/redshiftml-retraining-models</guid>
      <pubDate>Wed, 29 Nov 2023 18:09:54 GMT</pubDate>
    </item>
    <item>
      <title>pandas 中的 read_parquet 和 read_table 有什么区别？</title>
      <link>https://stackoverflow.com/questions/77572902/whats-the-difference-between-read-parquet-and-read-table-in-pandas</link>
      <description><![CDATA[我在尝试读取镶木地板文件时遇到了挑战。最初，我怀疑该文件可能已损坏。然而，改变读取方法后，文件被成功处理。这个解决方案花了相当长的时间才确定，主要是因为我在机器学习领域相对缺乏经验。
这些方法有什么区别？为什么第一种方法给我一个损坏的文件？
正在工作
data = pd.read_parquet(&#39;data.parquet&#39;,engine=&#39;fastparquet&#39;)

无法正常工作，文件损坏并抛出异常 utf-8，我们尝试在记事本中修复
data = pq.read_table(&#39;data.parquet&#39;)
df = data.to_pandas()
]]></description>
      <guid>https://stackoverflow.com/questions/77572902/whats-the-difference-between-read-parquet-and-read-table-in-pandas</guid>
      <pubDate>Wed, 29 Nov 2023 16:24:05 GMT</pubDate>
    </item>
    <item>
      <title>为模型创建输入时，keras 的 Sequential 和 Concatenate 有什么区别？</title>
      <link>https://stackoverflow.com/questions/77571304/what-is-the-difference-between-sequential-and-concatenate-of-keras-while-creatin</link>
      <description><![CDATA[为模型创建输入时，keras 的 Sequential 和 Concatenate 有什么区别？
我见过两种使用 Sequential 创建图层的方法，其中我们只需定义输入的形状，但另一种方法是使用 Concatenate 创建输入。
这两种方法有什么区别？
以下是方法：
方法 1：
模型 = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(单位=1, input_shape=(2,), kernel_regularizer=&#39;l1&#39;, 激活=tf.sigmoid))


model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=my_learning_rate),
                损失=tf.keras.losses.BinaryCrossentropy(),
                指标=指标）

方法 2：
链接：colab。
&lt;前&gt;&lt;代码&gt; my_inputs = {
    # 用于训练模型的特征。
    &#39;中位数收入&#39;: tf.keras.Input(shape=(1,)),
    &#39;total_rooms&#39;: tf.keras.Input(shape=(1,))
  }
# 使用连接层将输入层连接成单个张量。
  # 作为密集层的输入。例如：[input_1[0][0]、input_2[0][0]]
  concatenated_inputs = tf.keras.layers.Concatenate()(my_inputs.values())
  密集=层.密集（单位= 1，名称=&#39;dense_layer&#39;，激活= tf.sigmoid）
  密集输出 = 密集（连接输入）
  “”“创建并编译一个简单的分类模型。”“”
  我的输出 = {
    &#39;密集&#39;：密集输出，
  }
  模型= tf.keras.Model（输入= my_inputs，输出= my_outputs）

  # 调用compile方法将各层构造成模型
  # TensorFlow 可以执行。请注意，我们使用了不同的损失
  # 用于分类的函数而不是用于回归的函数。
  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=my_learning_rate),
                损失=tf.keras.losses.BinaryCrossentropy(),
                指标=指标）
]]></description>
      <guid>https://stackoverflow.com/questions/77571304/what-is-the-difference-between-sequential-and-concatenate-of-keras-while-creatin</guid>
      <pubDate>Wed, 29 Nov 2023 12:51:28 GMT</pubDate>
    </item>
    <item>
      <title>如何实现基于内容的语音搜索过滤？</title>
      <link>https://stackoverflow.com/questions/77562907/how-to-implement-content-based-filtering-for-voice-search</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77562907/how-to-implement-content-based-filtering-for-voice-search</guid>
      <pubDate>Tue, 28 Nov 2023 10:01:16 GMT</pubDate>
    </item>
    <item>
      <title>如何使用OrdinalEncoder()设置自定义顺序？</title>
      <link>https://stackoverflow.com/questions/72170947/how-to-use-ordinalencoder-to-set-custom-order</link>
      <description><![CDATA[我的二手车价格预测数据集中有一列名为“Owner_Type”的列。它有四个唯一值，即[&#39;第一&#39;、&#39;第二&#39;、&#39;第三&#39;、&#39;第四&#39;]。现在最有意义的顺序是 First &gt; &gt;第二个&gt;第三&gt;第四，价格相对于该订单下降。如何使用 OrdinalEncoder() 为值指定此顺序？请帮帮我，谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/72170947/how-to-use-ordinalencoder-to-set-custom-order</guid>
      <pubDate>Mon, 09 May 2022 11:05:12 GMT</pubDate>
    </item>
    <item>
      <title>Google AI Platform 训练 - 等待作业完成</title>
      <link>https://stackoverflow.com/questions/64806003/google-ai-platform-training-wait-for-the-job-to-finish</link>
      <description><![CDATA[我构建了一个包含大量并行进程的 AI Platform 管道。每个进程都会在 AI 平台上启动一个训练作业，如下所示：
gcloud ai-platform 作业提交培训...

然后它必须等待作业完成才能进入下一步。为此，我尝试将参数 --stream-logs 添加到上述命令中。通过这种方式，它会传输所有日志，直到作业完成。
问题是，有这么多并行进程，我用完了获取日志的请求：
超出配额指标“读取请求”和限制“每分钟读取请求”的配额
服务“logging.googleapis.com”

但我不需要实际流式传输日志，我只需要一种方法来告诉进程“等待”直到训练工作完成。有没有更聪明、更简单的方法来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/64806003/google-ai-platform-training-wait-for-the-job-to-finish</guid>
      <pubDate>Thu, 12 Nov 2020 14:39:20 GMT</pubDate>
    </item>
    <item>
      <title>keras中train_on_batch()有什么用？</title>
      <link>https://stackoverflow.com/questions/49100556/what-is-the-use-of-train-on-batch-in-keras</link>
      <description><![CDATA[train_on_batch() 与 fit() 有何不同？什么情况下我们应该使用train_on_batch()？]]></description>
      <guid>https://stackoverflow.com/questions/49100556/what-is-the-use-of-train-on-batch-in-keras</guid>
      <pubDate>Sun, 04 Mar 2018 21:13:27 GMT</pubDate>
    </item>
    <item>
      <title>pyspark：名称错误：名称“spark”未定义</title>
      <link>https://stackoverflow.com/questions/39541204/pyspark-nameerror-name-spark-is-not-defined</link>
      <description><![CDATA[我是从官方文档网站复制pyspark.ml示例：
http://spark.apache.org /docs/latest/api/python/pyspark.ml.html#pyspark.ml.Transformer
data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),(Vectors.dense([9.0, 8.0]),) , (Vectors.dense([8.0, 9.0]),)]
df = Spark.createDataFrame(数据, [“特征”])
kmeans = KMeans(k=2, 种子=1)
模型 = kmeans.fit(df)

但是，上面的示例无法运行并给出以下错误：

&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
NameError Traceback（最近一次调用最后一次）
&lt;ipython-input-28-aaffcd1239c9&gt;在&lt;模块&gt;()中
      1 从 pyspark 导入 *
      2 数据 = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),(Vectors.dense([9.0, 8.0]),), (Vectors.dense ([8.0, 9.0]),)]
----&gt; 3 df = Spark.createDataFrame(数据, [“特征”])
      4 kmeans = KMeans(k=2, 种子=1)
      5 模型 = kmeans.fit(df)

NameError：名称“spark”未定义

需要设置哪些附加配置/变量才能运行示例？]]></description>
      <guid>https://stackoverflow.com/questions/39541204/pyspark-nameerror-name-spark-is-not-defined</guid>
      <pubDate>Fri, 16 Sep 2016 23:05:11 GMT</pubDate>
    </item>
    <item>
      <title>如何摆脱 pandas 将 Excel 工作表中的大量数字转换为指数？</title>
      <link>https://stackoverflow.com/questions/38689125/how-to-get-rid-of-pandas-converting-large-numbers-in-excel-sheet-to-exponential</link>
      <description><![CDATA[在 Excel 工作表中，我有两列数字很大。
但是当我使用 read_excel() 读取 Excel 文件并显示数据框时，
这两列以带有指数的科学格式打印。
如何摆脱这种格式？
谢谢
Pandas 输出
]]></description>
      <guid>https://stackoverflow.com/questions/38689125/how-to-get-rid-of-pandas-converting-large-numbers-in-excel-sheet-to-exponential</guid>
      <pubDate>Sun, 31 Jul 2016 23:08:26 GMT</pubDate>
    </item>
    </channel>
</rss>