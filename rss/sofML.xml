<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 20 Dec 2023 21:11:38 GMT</lastBuildDate>
    <item>
      <title>基于机器学习的收入预测[关闭]</title>
      <link>https://stackoverflow.com/questions/77694003/machine-learning-based-revenue-prediction</link>
      <description><![CDATA[针对给定数据，比较预测性能并确定不同机器学习技术中最合适的建模算法，我如何找到合适的解决方案，我对主题的信息非常有限，我该如何处理它，请提出任何建议，我完全开放
任何在线论坛或人工智能等...来帮助我处理机器学习项目：游戏行业中基于机器学习的收入预测。我所拥有的数据说明：该数据由 Google Play Store 和 AppStore 中发布的填字游戏的游戏数据组成。此外，该数据由 30000 行训练数据和 5000 行测试数据组成。您可以在下面找到列说明。]]></description>
      <guid>https://stackoverflow.com/questions/77694003/machine-learning-based-revenue-prediction</guid>
      <pubDate>Wed, 20 Dec 2023 19:29:56 GMT</pubDate>
    </item>
    <item>
      <title>关于机器学习和数据框架的问题</title>
      <link>https://stackoverflow.com/questions/77693773/problem-about-machine-learning-and-dataframe</link>
      <description><![CDATA[我发现训练数据集层存在问题：
ValueError：层“sequential_4”的输入 0与图层不兼容：预期形状=（无，7），发现形状=（无，5）

这是我的代码：
导入 pandas 作为 pd
从 sklearn.model_selection 导入 train_test_split
从 keras.models 导入顺序
从 keras.layers 导入密集、Dropout
从 keras.optimizers 导入 Adam
从 keras.regularizers 导入 l2
从 sklearn.metrics 导入mean_squared_error
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np

def carica_dataset():
数据集 = pd.read_csv(“数据集.csv”)
返回数据集

def carica_modello():
    数据集 = carica_dataset()
    数据集 = pd.get_dummies(数据集, columns=[&#39;Località&#39;])
    打印（数据集）
    X = dataset.drop(列=[&#39;Prezzo&#39;])
    y = 数据集[&#39;Prezzo&#39;]

    X_train, X_test, y_train, y_test = train_test_split(X, y)

    模型=顺序（）

    model.add（密集（64，激活=&#39;relu&#39;，input_dim = X_train.shape [1]，kernel_regularizer = l2（0.1）））
    模型.add(Dropout(0.5))
    model.add（密集（32，激活=&#39;relu&#39;，kernel_regularizer=l2（0.1）））
    模型.add(Dropout(0.5))
    model.add（密集（16，激活=&#39;relu&#39;，kernel_regularizer=l2（0.1）））
    模型.add(Dropout(0.5))
    model.add(密集(8, 激活=&#39;relu&#39;, kernel_regularizer=l2(0.1)))
    模型.add(Dropout(0.5))
    model.add(密集(1,激活=&#39;线性&#39;,kernel_regularizer=l2(0.1)))
    亚当 = 亚当()

    model.compile(loss=&#39;mean_squared_error&#39;, 优化器=adam, 指标=[&#39;准确性&#39;])

    model.fit（X_train，y_train，epochs = 100，batch_size = 64）


    返回模型

 数据集 = carica_dataset()
 模型 = carica_modello()
 字段={
    &#39;Superficie&#39;：浮动，
    &#39;Numero di stanze da letto&#39;: int,
    &#39;Numero di bagni&#39;: int,
    &#39;Anno di costruzione&#39;: int,
    &#39;Località&#39;: str
     }
 用户数据 = {}

对于 fields.items() 中的键、值：
    而真实：
        尝试：
            user_input = input(f&quot;inserisci il valore di: {key}&quot;)
            用户数据[键] = 值(用户输入)
            休息
        除了值错误：
            print(f“inserisci un valore valido per {key}”)
dataframe = pd.DataFrame([用户数据])
dataframe = pd.get_dummies(dataframe, columns=[&#39;Località&#39;])

valori = dataframe.values

预测 = model.predict(valori)[0][0]
print(f&#39;La predizione del prezzo è: {预测} €&#39;)

我尝试更改层数，但每次都发现同样的问题，我该怎么办？
我的数据集有 6 列 -1，这是我需要预测的列，所以 5 列]]></description>
      <guid>https://stackoverflow.com/questions/77693773/problem-about-machine-learning-and-dataframe</guid>
      <pubDate>Wed, 20 Dec 2023 18:35:12 GMT</pubDate>
    </item>
    <item>
      <title>如何创建一个curl req来创建具有谷歌云存储管道规范的顶点管道？</title>
      <link>https://stackoverflow.com/questions/77692960/how-to-create-a-curl-req-to-create-a-vertex-pipeline-with-a-google-cloud-storage</link>
      <description><![CDATA[我正在关注此文档：
我有以下curl请求（更改了一些命名以不公开信息）：
&lt;前&gt;&lt;代码&gt;curl -X POST \
    -H“授权：承载$(gcloud auth print-access-token)” \
    -H“内容类型：application/json；字符集=utf-8” \
    -d&#39;{
        &quot;display_name&quot;:&quot;训练&quot;,
        “最大并发运行计数”：10，
        “创建管道作业请求”：{
                “父”：“项目/项目名称/位置/us-west2”，
                “管道作业”：{
                    “显示名称”：“训练”，
                    “pipelineSpec”：“gs://bucket_name/artifacts/pipeline.yaml”，
                }
            }
    }&#39; \
    “https://us-west2-aiplatform.googleapis.com/v1/projects/project_name/locations/us-west2/schedules”

我收到错误：
&lt;前&gt;&lt;代码&gt;{
  “错误”：{
    “代码”：400，
    “message”：““schedule.create_pipeline_job_request.pipeline_job.pipeline_spec”(type.googleapis.com/google.protobuf.Struct) 的值无效，\“gs://project_name/artifacts/pipeline.yaml\” ”，
    “状态”：“INVALID_ARGUMENT”，
    “详细信息”：[
      {
        “@type”：“type.googleapis.com/google.rpc.BadRequest”，
        “字段违规”：[
          {
            “字段”：“schedule.create_pipeline_job_request.pipeline_job.pipeline_spec”，
            “description”：““schedule.create_pipeline_job_request.pipeline_job.pipeline_spec”(type.googleapis.com/google.protobuf.Struct) 的值无效，\“gs://project_name/artifacts/pipeline.yaml\” ”
          }
        ]
      }
    ]
  }
}

看起来curl请求必须有一个以JSON格式指定的管道规范。是否可以在curl请求中指定GCS路径？
我的用例是使用 Cloud Scheduler 创建每周运行的管道，从 GCS 文件中提取。我不想使用内置的重复管道创建，因为如果管道规范发生更改，则需要再次创建它。]]></description>
      <guid>https://stackoverflow.com/questions/77692960/how-to-create-a-curl-req-to-create-a-vertex-pipeline-with-a-google-cloud-storage</guid>
      <pubDate>Wed, 20 Dec 2023 16:04:46 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TIF 文件 (.tif) 上执行 CNN？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77692904/how-to-perform-cnn-on-tif-file-tif</link>
      <description><![CDATA[我尝试使用 Python 编程语言中的 Tensorflow 库访问 TIF 文件 (.tif)，但出现以下错误：

您能否向我提供使用 TIF 图像文件实现 CNN 的初始步骤？]]></description>
      <guid>https://stackoverflow.com/questions/77692904/how-to-perform-cnn-on-tif-file-tif</guid>
      <pubDate>Wed, 20 Dec 2023 15:56:31 GMT</pubDate>
    </item>
    <item>
      <title>典型相关分析[关闭]</title>
      <link>https://stackoverflow.com/questions/77692744/canonical-correlation-analysis</link>
      <description><![CDATA[我正在学习统计学课程，今天我的老师问了我一个问题，要求我提出规范相关分析的替代方案。她希望我开始寻找其他解决方案，但我什至不知道从哪里开始。
我尝试在互联网上查找不同的解决方案并滚动浏览 SciKit Learn 文档，但没有找到任何有用的东西。我希望有人在这方面比我有更多的知识。那么有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77692744/canonical-correlation-analysis</guid>
      <pubDate>Wed, 20 Dec 2023 15:30:38 GMT</pubDate>
    </item>
    <item>
      <title>Python，分层采样[关闭]</title>
      <link>https://stackoverflow.com/questions/77692098/python-stratified-sampling</link>
      <description><![CDATA[我正在使用规范建模框架。
我有 791 个受试者的样本。我有关于年龄、性别和站点（站点 1 或 2）的数据。我想使用 Python 提取大约 50% 的受试者子样本，涵盖整个年龄和性别范围。我的子样本应反映原始样本的位点 1：位点 2 比例。
我尝试使用sklearn train_test_split，但有些分层 2.]]></description>
      <guid>https://stackoverflow.com/questions/77692098/python-stratified-sampling</guid>
      <pubDate>Wed, 20 Dec 2023 13:54:53 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch中的动态量化量化后开始随机训练</title>
      <link>https://stackoverflow.com/questions/77692089/dynamic-quantization-in-pytorch-starts-random-training-after-quantization</link>
      <description><![CDATA[当我运行以下动态量化代码时，它开始使用一些随机自然图像进行 100 个时期的训练，我不想再次进行训练。我有预训练的权重，我只是想量化我的预训练的权重以减少推理时间：
从 ultralytics 导入 YOLO
进口火炬
导入火炬.量化

模型=YOLO(&#39;pre_trained_weights.pt&#39;)

model.load_state_dict(torch.load(&#39;checkpoint.pth&#39;)) #不知道这一步是否必要

qmodel = torch.quantization.quantize_dynamic(模型, dtype = torch.quint8)

我尝试了上面的代码，我希望我只是想量化我的预训练权重以减少推理时间]]></description>
      <guid>https://stackoverflow.com/questions/77692089/dynamic-quantization-in-pytorch-starts-random-training-after-quantization</guid>
      <pubDate>Wed, 20 Dec 2023 13:53:49 GMT</pubDate>
    </item>
    <item>
      <title>端到端 ML 项目上的模型训练器问题 - ValueError：至少需要一个数组或数据类型</title>
      <link>https://stackoverflow.com/questions/77691153/model-trainer-issue-on-end-to-end-ml-project-valueerror-at-least-one-array-or</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77691153/model-trainer-issue-on-end-to-end-ml-project-valueerror-at-least-one-array-or</guid>
      <pubDate>Wed, 20 Dec 2023 11:18:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyG 时视频数据集的过度拟合问题</title>
      <link>https://stackoverflow.com/questions/77690723/overfitting-issue-with-video-dataset-while-using-pyg</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77690723/overfitting-issue-with-video-dataset-while-using-pyg</guid>
      <pubDate>Wed, 20 Dec 2023 10:11:21 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 MALLET Java API 运行 DMR 主题模型？</title>
      <link>https://stackoverflow.com/questions/77689759/how-can-i-run-dmr-topic-model-using-mallet-java-api</link>
      <description><![CDATA[我有两个数据集text.txt和另一个metadata.txt。我将数据格式化为每行一个文档。我想将 MALLET Java Api 用于 DMR 主题模型。但是，我无法这样做。我对 Java 和这种复杂的建模完全陌生。另外，我对主题建模没有任何先验知识。我在这里寻找专业知识。你能帮我解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77689759/how-can-i-run-dmr-topic-model-using-mallet-java-api</guid>
      <pubDate>Wed, 20 Dec 2023 07:05:09 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何在机器学习中正确使用交叉验证技术（cross_val_score）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77689532/how-should-i-use-cross-validation-technique-cross-val-score-in-machine-learnin</link>
      <description><![CDATA[我很难理解应该如何正确使用交叉验证技术（在本例中为cross_val_score）。
我正在研究机器学习回归问题。所有预处理步骤（插补、变换、缩放、编码等）均已完成。现在我需要决定使用哪种机器学习回归算法。
数据由 2 个文件组成：

train.csv - 独立特征 + 目标（1460 行）
test.csv - 仅独立特征（1459 行）

我的想法是使用 cross_val_score 选择最佳基本回归模型，然后在最佳基本模型上使用 GridSearchCV 来为其找到最佳超参数。
1) 如果我想使用 cross_val_scrore，我应该使用 train_test_split 分割 train.csv 数据并使用 X_train 和 y_train （1168 行）还是可以使用 X_train_full 和 y_train_full （1460 行）？&lt; /p&gt;
train_df = pd.read_csv(&#39;train.csv&#39;)

X_train_full = train_df.drop(&#39;目标&#39;), 轴 = 1
y_train_full = train_df[&#39;目标&#39;]

X_train，X_holdout，y_train，y_holdout = train_test_split（X_train_full，y_train_full，test_size = 0.2，random_state = 22）

2) 下面是我用来决定需要使用哪种基本回归机器学习模型的代码。我使用了 X_train 和 y_train，认为我应该以某种方式使用 X_holdout 和 y_holdout 来验证/确认决定，但我不知道这是否是正确的方法以及在这种情况下下一步该怎么做。
base_models = [

    (“线性回归”, LinearRegression()),
    (“岭回归”，Ridge())，
    (“套索回归”，Lasso())，
    （“弹性网络回归”，ElasticNet()），
    (“决策树”，DecisionTreeRegressor())，
    (“KNN 回归器”，KNeighborsRegressor())，
    (“SVR”，SVR())，
    （“随机森林”，RandomForestRegressor（）），
    (“额外树回归器”，ExtraTreesRegressor())，
    (“AdaBoost回归器”，AdaBoostRegressor())，
    (“梯度增强回归器”，GradientBoostingRegressor())，
    (“XGBoost”, XGBRegressor()),
    (“LightGBM”，LGBMRegressor())，
    (“CatBoost”，CatBoostRegressor(loss_function=&#39;RMSE&#39;，logging_level=&#39;Silent&#39;))

    ]

# 初始化列表来存储数据

基本模型名称 = []
base_model_rmse_scores = []

# 初始化各个交叉验证折叠分数的列表

cv_fold_scores = [[] for _ in range(10)] # 假设折叠 10 次

# 迭代模型并附加到列表

对于名称，base_models 中的模型：

    # 获取每次折叠的交叉验证分数

    分数 = np.sqrt(-cross_val_score(模型, X_train, y_train, 评分=“neg_mean_squared_error”, cv=10, n_jobs = -1))

    # 附加模型名称

    base_model_names.append(名称)

    # 添加跨折叠的均方根误差 (RMSE)

    base_model_rmse_scores.append(np.mean(分数))

    # 附加单独的折叠分数

    对于 i，枚举中的分数（分数）：
        cv_fold_scores[i].append(分数)

    # 从列表创建 DataFrame

    base_models_results_df = pd.DataFrame({“模型”: base_model_names, “Mean_RMSE”: base_model_rmse_scores})

    # 添加各个交叉验证折叠分数的列

    对于我，枚举中的fold_scores（cv_fold_scores）：
        base_models_results_df[f&quot;Fold_{i+1}_RMSE&quot;] = Fold_scores

    # 设置“模型”列作为索引

    base_models_results_df = base_models_results_df.set_index(“模型”)

    # 按均方根误差 (RMSE) 升序对 DataFrame 进行排序

    base_models_results_df = base_models_results_df.sort_values（by=“Mean_RMSE”，升序=True）

    基础模型结果df


从上图可以看出，CatBoost 是最好的基础模型，其平均 RMSE 为 0.319293。
3) 我可以做些什么来确保这个特定模型最适合使用吗？是否会过度拟合或不足？我可以使用 X_holdout 和 y_holdout 集以某种方式验证它吗？如果我需要使用 X_train_full 和 y_train_full，这种情况下该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/77689532/how-should-i-use-cross-validation-technique-cross-val-score-in-machine-learnin</guid>
      <pubDate>Wed, 20 Dec 2023 06:11:25 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在安装requirements.txt时收到此错误？</title>
      <link>https://stackoverflow.com/questions/77689351/why-am-i-getting-this-error-while-installing-requirements-txt</link>
      <description><![CDATA[PS C:\Users\SAI BHUVAN\Documents\EndToEndMLProject&gt; pip install -r 要求.txt
获取文件:///C:/Users/SAI%20BHUVAN/Documents/EndToEndMLProject（来自-rrequirements.txt（第10行））
  安装构建依赖项...完成
  检查构建后端是否支持 build_editable ...完成
  获取构建可编辑的需求...完成
  准备可编辑元数据 (pyproject.toml) ...错误
  错误：子进程退出并出现错误

  × 准备可编辑元数据 (pyproject.toml) 未成功运行。
  │ 退出代码：1
  ╰─&gt; [21行输出]
      回溯（最近一次调用最后一次）：
        文件“C:\Users\SAI BHUVAN\AppData\Local\Programs\Python\Python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，第 353 行，在  中
          主要的（）
        文件“C:\Users\SAI BHUVAN\AppData\Local\Programs\Python\Python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，第 335 行，在 main 中
          json_out[&#39;return_val&#39;] = hook(**hook_input[&#39;kwargs&#39;])
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        文件“C：\ Users \ SAI BHUVAN \ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ pip \ _vendor \ pyproject_hooks \ _in_process \ _in_process.py”，第181行，在prepare_metadata_for_build_editable中
          返回钩子（元数据目录，配置设置）
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        文件“C：\ Users \ SAI BHUVAN \ AppData \ Local \ Temp \ pip-build-env-fmoiyisg \ overlay \ Lib \ site-packages \ setuptools \ build_meta.py”，第446行，在prepare_metadata_for_build_editable中
          返回 self.prepare_metadata_for_build_wheel(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        文件“C:\Users\SAI BHUVAN\AppData\Local\Temp\pip-build-env-fmoiyisg\overlay\Lib\site-packages\setuptools\build_meta.py”，第368行，在prepare_metadata_for_build_wheel中
          self._bubble_up_info_directory(metadata_directory, “.egg-info”)
        文件“C:\Users\SAI BHUVAN\AppData\Local\Temp\pip-build-env-fmoiyisg\overlay\Lib\site-packages\setuptools\build_meta.py”，第 337 行，位于 _bubble_up_info_directory 中
          info_dir = self._find_info_directory(元数据目录, 后缀)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^
        文件“C:\Users\SAI BHUVAN\AppData\Local\Temp\pip-build-env-fmoiyisg\overlay\Lib\site-packages\setuptools\build_meta.py”，第 348 行，位于 _find_info_directory 中
          assert len(candidates) == 1, f“找到多个 {suffix} 目录”
                 ^^^^^^^^^^^^^^^^^^^^^^
      AssertionError：找到多个 .egg-info 目录
      [输出结束]

  注意：此错误源自子进程，并且可能不是 pip 的问题。
错误：元数据生成失败

× 生成包元数据时遇到错误。
╰─&gt;请参阅上面的输出。

注意：这是上面提到的包的问题，​​而不是 pip 的问题。
提示：详细信息请参见上文。

我试图解决这个问题，但无法解决。]]></description>
      <guid>https://stackoverflow.com/questions/77689351/why-am-i-getting-this-error-while-installing-requirements-txt</guid>
      <pubDate>Wed, 20 Dec 2023 05:19:33 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Hubert 模型获得音频嵌入</title>
      <link>https://stackoverflow.com/questions/77685045/how-to-get-audio-embeddings-using-hubert-model</link>
      <description><![CDATA[示例代码
导入火炬
从变压器导入 Wav2Vec2Processor、HubertForCTC
从数据集导入load_dataset

处理器 = Wav2Vec2Processor.from_pretrained(“facebook/hubert-large-ls960-ft”)
模型 = HubertForCTC.from_pretrained(“facebook/hubert-large-ls960-ft”)
input_values = 处理器(&#39;来自音频文件的数组。, return_tensors=“pt”).input_values

之后如何获得嵌入？模型中没有最后的隐藏状态。
尝试了我提到的代码块。此外，尝试创建没有最后一层的模型，然后将其输入。我的另一个问题是，假设我有不同时间维度的剪辑，那么如何创建固定嵌入？沿着时间轴平均可以吗？或者需要不同的方法。]]></description>
      <guid>https://stackoverflow.com/questions/77685045/how-to-get-audio-embeddings-using-hubert-model</guid>
      <pubDate>Tue, 19 Dec 2023 12:04:19 GMT</pubDate>
    </item>
    <item>
      <title>如何为此笔记本创建 Sagemaker 端点？</title>
      <link>https://stackoverflow.com/questions/77660996/how-can-i-create-a-sagemaker-endpoint-for-this-notebook</link>
      <description><![CDATA[我创建了一个 VectorDB (FAISS) 并将 PDF 输入到其中。然后我使用 AWS Bedrock 的 Langchain 包装器来调用它。我知道现在存在 Kowledge Base，但至少在 SageMaker 笔记本中，我有更多的控制权。该模型在 SageMaker Notebook 中完美运行，当我提出问题时，它会返回答案。
我想做的是创建一个小网页（并通过 HTTP/REST API），只需在文本字段中提交问题并在文本字段中接收答案。我猜如果链中某个地方没有 Lambda 函数，这很难做到，或者也许不是？
当我查看 Sagemaker 控制台的推理选项卡下时，没有模型或没有端点，或者没有&lt; /strong&gt; 端点配置（因为我没有从 Sagemaker 选择模型，所以我只是在 Python 笔记本中使用 langchain LLM 和 Bedrock，如下所示）。
&lt;前&gt;&lt;代码&gt;导入boto3
导入 json

bedrock = boto3.client(service_name=&quot;bedrock&quot;)
bedrock_runtime = boto3.client(service_name=“bedrock-runtime”)



从 langchain.llms.bedrock 导入 Bedrock
从 langchain.chains 导入 RetrievalQA
从 langchain.prompts 导入 PromptTemplate

嵌入 = BedrockEmbeddings(model_id=“amazon.titan-embed-text-v1”,
                               客户端=bedrock_runtime）

最终我将文档嵌入到 FAISS Vector 数据库中，我查询的就是这个数据库
db = FAISS.from_documents（文档，嵌入）


模型泰坦 = {
    “最大令牌计数”：512，
    “停止序列”：[]，
    “温度”：0.0，
    “顶部P”：0.5
}

# 亚马逊泰坦模型
llm = 基岩(
    model_id=&quot;amazon.titan-text-express-v1&quot;,
    客户端=bedrock_runtime，
    model_kwargs=model_titan,
）

然后定义一个提示......
提示 = 提示模板(
    template=prompt_template, input_variables=[“上下文”, “问题”]
）

并查询数据库：
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=“东西”，
    检索器=db.as_retriever(
        search_type=“相似度”，
    ),
    return_source_documents=真，
    chain_type_kwargs={“提示”: 提示},
）



query =“未来的技术是什么样的？”

结果 = qa({“查询”: 查询})

print(f&#39;查询: {结果[“查询”]}\n&#39;)
print(f&#39;结果: {结果[“结果”]}\n&#39;)
print(f&#39;上下文文档：&#39;)
对于结果 [“source_documents”] 中的 srcdoc：
      打印（f&#39;{srcdoc}\n&#39;）

这恰好返回了我在 Sagemaker 中需要的内容，我只需要从外部查询数据库即可。
我不想让 lambda 函数每次都重建链。我考虑的是效率，我需要的只是在 lambda 函数中传递查询并返回结果。]]></description>
      <guid>https://stackoverflow.com/questions/77660996/how-can-i-create-a-sagemaker-endpoint-for-this-notebook</guid>
      <pubDate>Thu, 14 Dec 2023 14:49:20 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“tensorflow.keras.preprocessing”（未知位置）导入名称“image_dataset_from_directory”</title>
      <link>https://stackoverflow.com/questions/64690693/importerror-cannot-import-name-image-dataset-from-directory-from-tensorflow</link>
      <description><![CDATA[为什么我会遇到这个问题？我可以从 kera.preprocessing 导入图像模块。但无法导入 image_dataset_from_directory。 TF版本：1.14]]></description>
      <guid>https://stackoverflow.com/questions/64690693/importerror-cannot-import-name-image-dataset-from-directory-from-tensorflow</guid>
      <pubDate>Thu, 05 Nov 2020 03:11:22 GMT</pubDate>
    </item>
    </channel>
</rss>