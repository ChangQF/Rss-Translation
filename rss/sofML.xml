<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 18 Oct 2024 01:16:19 GMT</lastBuildDate>
    <item>
      <title>如何在创建模型时考虑不同的列</title>
      <link>https://stackoverflow.com/questions/79100145/how-to-consider-varying-columns-while-creating-a-model</link>
      <description><![CDATA[我有一个发送警报的监控服务。我正在创建一个模型，如果警报在过去 1 个月内发生超过 3 次，该模型将标记警报。
我可以使用 IsolationForest 实现这一点，并指定模型中每个警报要考虑的字段。
但是，我面临的问题是警报的字段可能会有所不同。
考虑以下 2 个警报
AlertName Date FQDN DBName
磁盘使用率 90% 10/17/2024 00:00:000 test.com 
DB 已重新启动。10/17/2024 01:00:000 db1

在上面的例子中，如果它是磁盘使用率 90% 警报，那么我应该使用 FQDN 字段
如果是 DB 已重新启动，我应该使用 DBName 字段。
对于每个警报，用于确定其是否为重复警报的字段会有所不同，而我无法控制这些字段。
是否可以开发一个模型，该模型会动态考虑不同警报的不同列，而我无需指定要为每种警报类型考虑哪一列？]]></description>
      <guid>https://stackoverflow.com/questions/79100145/how-to-consider-varying-columns-while-creating-a-model</guid>
      <pubDate>Thu, 17 Oct 2024 23:57:47 GMT</pubDate>
    </item>
    <item>
      <title>重塑自定义策略网络中的错误</title>
      <link>https://stackoverflow.com/questions/79100066/reshaping-error-in-my-custom-policy-network</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79100066/reshaping-error-in-my-custom-policy-network</guid>
      <pubDate>Thu, 17 Oct 2024 22:55:39 GMT</pubDate>
    </item>
    <item>
      <title>使用 FastAPI 和 GPT 的 RAG 系统</title>
      <link>https://stackoverflow.com/questions/79099986/rag-system-using-fastapi-and-gpt</link>
      <description><![CDATA[我有一项任务，使用 RAG 和 FastAPI 在特定数据集上建立 Web 应用程序。
信息位于链接 https://github.com/drrahulsuresh/bounce/tree/dev
但是我的查询显示未获得相关数据。
有人可以帮我解决这个问题吗？
我尝试使用 sql，甚至将 excel 转换为 json 或 csv。我认为问题出在复杂的嵌套数据集上。我不知道如何通过 rag 将数据处理成可读格式。]]></description>
      <guid>https://stackoverflow.com/questions/79099986/rag-system-using-fastapi-and-gpt</guid>
      <pubDate>Thu, 17 Oct 2024 22:12:24 GMT</pubDate>
    </item>
    <item>
      <title>是否有用于机器学习训练代码的自动回归测试库？[关闭]</title>
      <link>https://stackoverflow.com/questions/79099561/is-there-a-library-for-automated-regression-testing-for-machine-learning-trainin</link>
      <description><![CDATA[我想以自动化的方式测试我的深度学习训练代码，以确保重构不会改变我训练的更新步骤。我想我还需要某种度量来覆盖我的计算图。
是否有一个库可以为 pytorch 或 jax 执行此操作？我不想测试经过训练的模型（有库可以做到这一点），而是测试训练代码。]]></description>
      <guid>https://stackoverflow.com/questions/79099561/is-there-a-library-for-automated-regression-testing-for-machine-learning-trainin</guid>
      <pubDate>Thu, 17 Oct 2024 19:16:16 GMT</pubDate>
    </item>
    <item>
      <title>图像预处理顺序[关闭]</title>
      <link>https://stackoverflow.com/questions/79098008/image-preprocessing-order</link>
      <description><![CDATA[我有一个数据集，其中包含多个 3D 图像，每个图像都有相应的坐标。我想以相同的方式预处理所有这些图像，但是我不确定应该按什么顺序进行。首先，我想将图像重新采样为 1x1x1。我还想裁剪感兴趣的坐标，最后，我想将图像大小调整为 128 x 128 x 128，以供我的深度学习模型使用。
坐标分布在图像周围，不在同一位置。这使其更加混乱，因为裁剪的大小会有所不同。此外，我不想每个图像都有补丁，因此 ResizeWithPadOrCrop（来自 monai）会很困难，因为我希望包含所有坐标。
有人对我应该按什么顺序进行操作有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79098008/image-preprocessing-order</guid>
      <pubDate>Thu, 17 Oct 2024 12:11:23 GMT</pubDate>
    </item>
    <item>
      <title>微调细分任何模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79097002/fine-tuning-segment-anything-model</link>
      <description><![CDATA[我一直在尝试用 Python 微调 SAM 模型，但我发现大多数教程都要求我们在微调后提供提示。 难道不能微调模型，以便它可以在不提供提示的情况下处理该特定数据集吗？
我尝试按照教程操作，但没有输出任何相关结果。我会将笔记本附在这里。]]></description>
      <guid>https://stackoverflow.com/questions/79097002/fine-tuning-segment-anything-model</guid>
      <pubDate>Thu, 17 Oct 2024 07:29:00 GMT</pubDate>
    </item>
    <item>
      <title>像素映射到网络输入</title>
      <link>https://stackoverflow.com/questions/79096878/pixel-mapping-to-network-inputs</link>
      <description><![CDATA[我正在嵌入式设备中使用对象检测模型，需要运行测试来比较嵌入式平台和 PC 上的性能。为了测试的完整性，我需要确保在两种情况下像素的映射方式相同。我了解 Pytorch 如何将图像转换为形状为 CxHxW 的张量，但我要问的是这些像素究竟是如何映射到输入的。那么，例如，图像左上角的像素（所有 3 个通道中的值）是分配给前向传递中的第一个输入，还是最后一个输入？]]></description>
      <guid>https://stackoverflow.com/questions/79096878/pixel-mapping-to-network-inputs</guid>
      <pubDate>Thu, 17 Oct 2024 06:59:11 GMT</pubDate>
    </item>
    <item>
      <title>Gymnasium 自定义环境“太多值无法解压”错误</title>
      <link>https://stackoverflow.com/questions/79084313/gymnasium-custom-environment-too-many-values-to-unpack-error</link>
      <description><![CDATA[我正在尝试使用具有体育馆和稳定基线的自定义群体聚集环境。我有一个自定义策略和训练循环。
我的行动和观察空间如下：
min_action = np.array([-5, -5] * len(self.agents), dtype=np.float32)
max_action = np.array([5, 5] * len(self.agents), dtype=np.float32)

min_obs = np.array([-np.inf, -np.inf, -2.5, -2.5] * len(self.agents), dtype=np.float32)
max_obs = np.array([np.inf, np.inf, 2.5, 2.5] * len(self.agents), dtype=np.float32)

训练代码：
import numpy as np
import torch as th
from Parameters import *
from stable_baselines3 import PPO
from main import FlockingEnv, CustomMultiAgentPolicy
from Callbacks import TQDMProgressCallback, LossCallback
import os
from stable_baselines3.common.vec_env import DummyVecEnv

if os.path.exists(Results[&quot;Rewards&quot;]):
os.remove(Results[&quot;Rewards&quot;])
print(f&quot;File {Results[&#39;Rewards&#39;]} has been removed.&quot;)

if os.path.exists(&quot;training_rewards.json&quot;):
os.remove(&quot;training_rewards.json&quot;)
print(f&quot;文件 training_rewards 已被删除。&quot;) 

def seed_everything(seed):
np.random.seed(seed)
os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
th.manual_seed(seed)
th.cuda.manual_seed(seed)
th.backends.cudnn.deterministic = True
env.seed(seed)
env.action_space.seed(seed)

loss_callback = LossCallback()
env = DummyVecEnv([lambda: FlockingEnv()])

seed_everything(SimulationVariables[&quot;Seed&quot;])

# # 模型训练
model = PPO(CustomMultiAgentPolicy, env, tensorboard_log=&quot;./ppo_Agents_tensorboard/&quot;, verbose=1)
model.set_random_seed(SimulationVariables[&quot;ModelSeed&quot;])
progress_callback = TQDMProgressCallback(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;])
# 训练模型
model.learn(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;], callback=[progress_callback, loss_callback])

错误：
使用 cuda 设备
回溯（最近一次调用最后一次）：
文件 &quot;D:\Thesis_\FlockingFinal\MultiAgentFlocking\Training.py&quot;，行45，在&lt;module&gt;中
model.learn(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;], callback=[progress_callback, loss_callback]) 
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\ppo\ppo.py&quot;，第 315 行，在 learn 中
return super().learn(
^^^^^^^^^^^^^^^
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py&quot;，第 287 行，在 learn 中
total_timesteps, callback = self._setup_learn(
^^^^^^^^^^^^^^^^^^^
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\common\base_class.py&quot;, 第 423 行, 在 _setup_learn
self._last_obs = self.env.reset() # 类型: ignore[assignment]
^^^^^^^^^^^^^^^^^
文件 &quot;C:\Python312\Lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py&quot;, 第 77 行, 在 reset
obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: 太需要解包的值很多（预计为 2 个）

我也在 gym 中使用了类似的种子函数，但没有出现错误，我以为是它导致了错误，但即使我不使用它，错误也不会消失。]]></description>
      <guid>https://stackoverflow.com/questions/79084313/gymnasium-custom-environment-too-many-values-to-unpack-error</guid>
      <pubDate>Sun, 13 Oct 2024 22:45:48 GMT</pubDate>
    </item>
    <item>
      <title>GNU Octave 是多线程的吗？</title>
      <link>https://stackoverflow.com/questions/79050512/is-gnu-octave-multi-threaded</link>
      <description><![CDATA[根据这个老问题的答案，GNU Octave 似乎是一个单线程应用程序。
但是，我正在试验一个名为nnet的旧 Octave 神经网络包，并惊讶地发现我的 Octave 程序使用了笔记本电脑的所有 4 个核心。自从我链接的问题提出以来，情况有变化吗？GNU Octave 现在是多线程的吗？据我所知，我没有看到 nnet 内部有任何并行实现。
有关我的安装的一些信息：

我的操作系统是 Linux Mint 20
我的机器有 4 个处理单元（这是 nproc 在我的终端中显示的内容）
我的 Octave 版本是 5.2.0（如果这有区别的话，我正在使用 GUI）

我的代码相当简单，只导入了 nnet 包，没有其他内容。当我查看运行程序时的资源时，我看到所有核心都已使用（下面是 htop 屏幕截图）

这是我正在做的事情：
pkg load nnet

starttime = clock();

# 取自 http://matlab.izmiran.ru/help/toolbox/nnet/newff.html
Pr = -1:0.00005:1;
Tr = 3*sin(pi*Pr)-cos(pi*Pr);
Prmin = min(Pr);
Prmax = max(Pr);
net = newff([Prmin Prmax],[3 2 1],{&#39;tansig&#39;,&#39;logsig&#39;,&#39;purelin&#39;},&#39;trainlm&#39;);
[net] = train(net,Pr,Tr,[],[],[]);
[netoutput] = sim(net,Pr);

etime(clock(),starttime)

% 测试结果 
plot(Pr,Tr,&#39;b+&#39;);
hold on; 
plot(Pr,netoutput,&#39;r-&#39;);
hold off;

编辑
根据评论中的 @JérômeRichard 提示和 @NickJ 建议，我通过在终端中执行 export OMP_NUM_THREADS=1 来启动 Octave，只为 BLAS 分配 1 个线程。该脚本的速度是原始设置的两倍（根据上面发布的 htop 屏幕截图，默认设置是 4）。我确保我的程序只使用一个核心和 htop。]]></description>
      <guid>https://stackoverflow.com/questions/79050512/is-gnu-octave-multi-threaded</guid>
      <pubDate>Thu, 03 Oct 2024 12:19:13 GMT</pubDate>
    </item>
    <item>
      <title>根据 HistGratientBoostingClassifier 绘制决策树</title>
      <link>https://stackoverflow.com/questions/78636029/plot-a-decision-tree-from-histgratientboostingclassifier</link>
      <description><![CDATA[我有一个 HistGradientBoostingClassifier 模型，我想绘制一个或多个决策树，但我找不到原生函数来执行此操作，我可以访问 Tree 预测器对象及其节点，但为了将其绘制到 sklearn.tree.plot_tree 函数中，它需要是 DecisionTree 类型的对象
我试过这个：
from sklearn.tree import plot_tree

plot_tree(RF_90._predictors[0][0])

出现此错误：

InvalidParameterError：plot_tree 的 &#39;decision_tree&#39; 参数必须
是 &#39;sklearn.tree._classes.DecisionTreeClassifier&#39; 的实例或
&#39;sklearn.tree._classes.DecisionTreeRegressor&#39;。得到的是
&lt;sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor
对象位于 0x7f676ebf0310&gt;。

注意：RF_90 是 HistGradientBoostingClassifier 拟合模型]]></description>
      <guid>https://stackoverflow.com/questions/78636029/plot-a-decision-tree-from-histgratientboostingclassifier</guid>
      <pubDate>Tue, 18 Jun 2024 07:26:34 GMT</pubDate>
    </item>
    <item>
      <title>LLM Studio 无法下载模型并出现错误：无法获取本地颁发者证书</title>
      <link>https://stackoverflow.com/questions/78379820/llm-studio-fail-to-download-model-with-error-unable-to-get-local-issuer-certif</link>
      <description><![CDATA[在 LLM 工作室中，当我尝试下载任何模型时，我遇到以下错误：
下载失败：无法获取本地颁发者证书
]]></description>
      <guid>https://stackoverflow.com/questions/78379820/llm-studio-fail-to-download-model-with-error-unable-to-get-local-issuer-certif</guid>
      <pubDate>Wed, 24 Apr 2024 16:03:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中有效地实现非全连接线性层？</title>
      <link>https://stackoverflow.com/questions/70269663/how-to-efficiently-implement-a-non-fully-connected-linear-layer-in-pytorch</link>
      <description><![CDATA[我制作了一个示例图，展示了我试图实现的缩小版本：

因此，顶部两个输入节点仅完全连接到顶部三个输出节点，并且相同的设计适用于底部两个节点。到目前为止，我已经想出了两种在 PyTorch 中实现此目的的方法，但都不是最佳方法。
第一种方法是创建一个包含许多较小线性层的 nn.ModuleList，并在前向传递期间，通过它们迭代输入。对于图表的示例，它看起来像这样：
class Module(nn.Module):
def __init__(self):
self.layers = nn.Module([nn.Linear(2, 3) for i in range(2)])

def forward(self, input):
output = torch.zeros(2, 3)
for i in range(2):
output[i, :] = self.layers[i](input.view(2, 2)[i, :])
return output.flatten()

因此，这完成了图中的网络，主要问题是它非常慢。我认为这是因为 PyTorch 必须按顺序处理 for 循环，而不能并行处理输入张量。
要“矢量化”模块以便 PyTorch 可以更快地运行它，我有这个实现：
class Module(nn.Module):
def __init__(self):
self.layer = nn.Linear(4, 6)
self.mask = # 创建 1 和 0 的掩码来“阻止”某些层连接

def forward(self, input):
prune.custom_from_mask(self.layer, name=&#39;weight&#39;, mask=self.mask)
return self.layer(input)

这也完成了图表的网络，通过使用权重修剪来确保完全连接层中的某些权重始终为零（例如，连接顶部输入节点和底部输出节点的权重将始终为零，因此它实际上是“断开连接的”）。这个模块比上一个模块快得多，因为没有 for 循环。现在的问题是这个模块占用了更多的内存。这可能是因为，即使大多数层的权重为零，PyTorch 仍将网络视为它们存在。此实现本质上保留了比需要更多的权重。
有人遇到过这个问题并想出了有效的解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/70269663/how-to-efficiently-implement-a-non-fully-connected-linear-layer-in-pytorch</guid>
      <pubDate>Wed, 08 Dec 2021 03:41:06 GMT</pubDate>
    </item>
    <item>
      <title>在 scikit learn 中实现自定义损失函数</title>
      <link>https://stackoverflow.com/questions/54267745/implementing-custom-loss-function-in-scikit-learn</link>
      <description><![CDATA[我想在 scikit learn 中实现自定义损失函数。我使用以下代码片段：
def my_custom_loss_func(y_true,y_pred):
diff3=max((abs(y_true-y_pred))*y_true)
return diff3

score=make_scorer(my_custom_loss_func,greater_ is_better=False)
clf=RandomForestRegressor()
mnn= GridSearchCV(clf,score)
knn = mnn.fit(feam,labm) 

传递给 my_custom_loss_func 的参数应该是什么？我的标签矩阵称为 labm。我想计算实际输出与预测输出（由模型）乘以真实输出之间的差值。如果我使用 labm 代替 y_true，那么我应该使用什么代替 y_pred？]]></description>
      <guid>https://stackoverflow.com/questions/54267745/implementing-custom-loss-function-in-scikit-learn</guid>
      <pubDate>Sat, 19 Jan 2019 13:47:47 GMT</pubDate>
    </item>
    <item>
      <title>Python 中图像增强的图像增强管道中的错误</title>
      <link>https://stackoverflow.com/questions/50887274/error-in-image-augmentation-pipeline-for-image-augmentation-in-python</link>
      <description><![CDATA[据我所知，路径是正确的，我也遵循了 Augmentor 文档。
代码：
import Augmentor
import os
import warnings
warnings.filterwarnings(&#39;ignore&#39;)
import keras
import glob

for img in glob.glob(&quot;C:\\Users\\Diganta\\Desktop\\Courses and Projects\\Projects\\Bennet\\irregular*.jpg&quot;):
p = Augmentor.Pipeline(img)
p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)
p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)
p.sample(100)

这确实运行了，但没有创建包含增强图像的输出文件夹按照 Augmentor 文档中指定的目录进行操作]]></description>
      <guid>https://stackoverflow.com/questions/50887274/error-in-image-augmentation-pipeline-for-image-augmentation-in-python</guid>
      <pubDate>Sat, 16 Jun 2018 10:54:47 GMT</pubDate>
    </item>
    <item>
      <title>DNNCLassifier Tensorflow 上的 label_keys 类型错误</title>
      <link>https://stackoverflow.com/questions/44219077/label-keys-type-error-on-dnnclassifier-tensorflow</link>
      <description><![CDATA[我想将标签嵌入到 Tensorflow 中的 DNNClassifier 模型中。
与文档示例此处不同，我收到以下错误消息：
label_keys_values = [&quot;satan&quot;, &quot;ipsweep&quot;, &quot;nmap&quot;, &quot;portsweep&quot;] 
m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,
feature_columns=deep_columns,
n_classes=4,
hidden_​​units=[12, 4],
label_keys=label_keys_values)
m.fit(input_fn=train_input_fn, steps=200)

文件 &quot;embedding_model_probe.py&quot;，第 118 行，位于 &lt;module&gt;
m.fit(input_fn=train_input_fn, steps=200)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py&quot;，第 281 行，位于 new_func
return func(*args, **kwargs)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py&quot;，第 430 行，位于 fit
loss = self._train_model(input_fn=input_fn, hooks=hooks)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py&quot;，第 927 行，位于 _train_model
model_fn_ops = self._get_train_ops(features, labels)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py&quot;，第 1132 行，在 _get_train_ops 中
return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py&quot;，第 1103 行，在 _call_model_fn 中
model_fn_results = self._model_fn(features, labels, **kwargs)
文件&quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py&quot;，第 180 行，在 _dnn_model_fn
logits=logits)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py&quot;，第 1004 行，在 create_model_fn_ops
labels = self._transform_labels(mode=mode, labels=labels)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py&quot;，第 1033 行，在 _transform_labels
&quot;label_ids&quot;: table.lookup(labels_tensor),
文件 &lt;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lookup/lookup_ops.py&gt;，第 179 行，在查找中
(self._key_dtype, keys.dtype))
TypeError: 签名不匹配。密钥必须是 dtype 

&lt; dtype: &#39;string&#39;&gt;，得到 &lt; dtype: &#39;int64&#39;&gt;

另一方面，如果我将 label_key_values 列设为 numpy.array，则会出现以下错误：
label_keys_values = np.array([&quot;satan&quot;, &quot;ipsweep&quot;, &quot;nmap&quot;, &quot;portsweep&quot;], dtype=&#39;string&#39;)

回溯（最近一次调用最后一次）：
文件 &quot;embedding_model_probe.py&quot;，第 116 行，位于 &lt;module&gt;
label_keys=label_keys_values)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py&quot;，第 337 行，位于 __init__
label_keys=label_keys)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py&quot;，第 331 行，位于 multi_class_head
label_keys=label_keys)
文件 &quot;/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py&quot;，第 986 行，位于 __init__
如果 label_keys 和 len(label_keys) != n_classes:
ValueError:具有多个元素的数组的真值不明确。请使用 a.any() 或 a.all()
]]></description>
      <guid>https://stackoverflow.com/questions/44219077/label-keys-type-error-on-dnnclassifier-tensorflow</guid>
      <pubDate>Sat, 27 May 2017 16:16:58 GMT</pubDate>
    </item>
    </channel>
</rss>