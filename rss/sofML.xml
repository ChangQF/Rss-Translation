<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 26 Mar 2024 03:13:53 GMT</lastBuildDate>
    <item>
      <title>如何使用 Flower 和 Tensorflow 来结束联邦学习中服务器的额外参数？</title>
      <link>https://stackoverflow.com/questions/78221905/how-to-end-extra-parameters-to-server-in-federated-learning-with-flower-and-tens</link>
      <description><![CDATA[我想将带有模型更新的额外参数发送到服务器，然后将服务器中的这些额外参数用于其他目的。我在这个项目中使用 Flower 和 Tensorflow。在发送额外参数之前，我的模型运行良好。目前我有这些代码客户端模型 server.py。
如何在服务器中成功发送额外参数或值并接收它？
感谢您的帮助。
我尝试在 get_parameter 方法中发送附加参数，并使用 FedAvg 策略接收它。但我一次又一次地遇到这个错误。 错误]]></description>
      <guid>https://stackoverflow.com/questions/78221905/how-to-end-extra-parameters-to-server-in-federated-learning-with-flower-and-tens</guid>
      <pubDate>Mon, 25 Mar 2024 21:38:40 GMT</pubDate>
    </item>
    <item>
      <title>这段代码应该做什么？它持续执行并且不会停止</title>
      <link>https://stackoverflow.com/questions/78221744/what-does-this-code-is-supposed-to-do-it-keeps-executing-and-does-not-stop</link>
      <description><![CDATA[我正在使用 Google Colab 上的 CelebA 数据集开发生成模型。一切都运行良好，但它要执行这个单元，它会继续执行。我应该在执行时做一些事情吗？为什么一直执行3、4个小时？
这是代码：
导入时间
迭代次数 = 15000
批量大小 = 16

RES_DIR = &#39;res2&#39;
FILE_PATH = &#39;%s/生成_%d.png&#39;
如果不是 os.path.isdir(RES_DIR):
    os.mkdir(RES_DIR)

CONTROL_SIZE_SQRT = 6
control_vectors = np.random.normal(size=(CONTROL_SIZE_SQRT**2, LATENT_DIM)) / 2

开始=0
d_损失= []
a_损失= []
图片已保存 = 0
对于范围内的步长（iters）：
    开始时间 = 时间.time()
    Latent_vectors = np.random.normal(size=(batch_size, LATENT_DIM))
    生成=生成器.预测（潜在向量）

    真实=图像[开始：开始+批量大小]
    组合图像 = np.concatenate([生成, 真实])

    标签 = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])
    标签 += .05 * np.random.random(labels.shape)

    d_loss = discriminator.train_on_batch(combined_images, labels)
    d_losses.append(d_loss)

    Latent_vectors = np.random.normal(size=(batch_size, LATENT_DIM))
    误导目标 = np.zeros((batch_size, 1))

    a_loss = gan.train_on_batch（潜在向量，误导目标）
    a_losses.append(a_loss)

    开始+=批量大小
    如果开始&gt; images.shape[0] - 批量大小：
        开始=0

    如果步骤 % 50 == 49：
        gan.save_weights(&#39;/gan.h5&#39;)

        print(&#39;%d/%d: d_loss: %.4f, a_loss: %.4f.(%.1f sec)&#39; % (step + 1, iters, d_loss, a_loss, time.time() - start_time))

        control_image = np.zeros((宽度 * CONTROL_SIZE_SQRT, 高度 * CONTROL_SIZE_SQRT, 通道))
        control_generate = 生成器.predict(control_vectors)

        对于范围内的 i（CONTROL_SIZE_SQRT ** 2）：
            x_off = i % CONTROL_SIZE_SQRT
            y_off = i // CONTROL_SIZE_SQRT
            control_image[x_off * 宽度:(x_off + 1) * 宽度, y_off * 高度:(y_off + 1) * 高度, :] = control_ generated[i, :, :, :]
        im = Img.fromarray(np.uint8(control_image * 255))#.save(StringIO(), &#39;jpeg&#39;)
        im.save(FILE_PATH % (RES_DIR, images_saved))
        图片已保存 += 1

什么时候执行完成？]]></description>
      <guid>https://stackoverflow.com/questions/78221744/what-does-this-code-is-supposed-to-do-it-keeps-executing-and-does-not-stop</guid>
      <pubDate>Mon, 25 Mar 2024 20:58:02 GMT</pubDate>
    </item>
    <item>
      <title>如何微调任何生成模型？自动列车</title>
      <link>https://stackoverflow.com/questions/78221298/how-can-i-fine-tune-the-any-generative-model-autotrain</link>
      <description><![CDATA[如何微调 Realistic_Vision_V6.0_B1_noVAE 模型并生成自己的图像？这是huggingface 链接。
我在稳定的扩散基础 xl 模型上使用了 autotrain。但在现实视觉模型上使用它似乎不太正确。模型生成不良图像。
&lt;块引用&gt;
自动训练 Dreambooth --型号 SG161222/Realistic_Vision_V6.0_B1_noVAE
--image-path input_images/ --prompt “摩诃人的照片” --分辨率 1024 --批量大小 1 --步数 500 --混合精度 fp16 --梯度累积 4 --lr 1e-4 --项目名称
现实愿景

这给了我 pytorch_lora_weights.safetensors 文件，我用它来生成我自己的图像。
pipeline.load_lora_weights（“模型/”，weight_name =“pytorch_lora_weights.safetensors”）
这种微调 Realistic_Vision_V6.0_B1_noVAE 模型的技术是否正确？或者任何像稳定扩散这样的模型？]]></description>
      <guid>https://stackoverflow.com/questions/78221298/how-can-i-fine-tune-the-any-generative-model-autotrain</guid>
      <pubDate>Mon, 25 Mar 2024 19:17:08 GMT</pubDate>
    </item>
    <item>
      <title>从 9 年的 Java 经验转向 AI/ML 职业道路是一个明智的决定吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78220858/is-it-good-decision-to-move-to-ai-ml-career-path-from-9-years-of-java-experience</link>
      <description><![CDATA[您好，我作为软件开发人员和 API 开发人员拥有 9 年的 Java 经验。
现在，在这个不断变化的技术时代，每个人都在谈论人工智能。
那么，将职业道路转向与 AI/ML 相关的方向是个好决定吗？
搜索了很多youtybe]]></description>
      <guid>https://stackoverflow.com/questions/78220858/is-it-good-decision-to-move-to-ai-ml-career-path-from-9-years-of-java-experience</guid>
      <pubDate>Mon, 25 Mar 2024 17:42:06 GMT</pubDate>
    </item>
    <item>
      <title>使用变压器模型改进列车准点预测：模型设置和性能问题</title>
      <link>https://stackoverflow.com/questions/78220853/improving-train-punctuality-prediction-using-a-transformer-model-model-setup-an</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78220853/improving-train-punctuality-prediction-using-a-transformer-model-model-setup-an</guid>
      <pubDate>Mon, 25 Mar 2024 17:41:18 GMT</pubDate>
    </item>
    <item>
      <title>波士顿房价数据集的 ANN 损失并未减少</title>
      <link>https://stackoverflow.com/questions/78220834/ann-loss-not-reducing-for-boston-house-price-data-set</link>
      <description><![CDATA[from sklearn.datasets import load_boston

波士顿 = load_boston()
boston_data = pd.DataFrame(boston.data)
boston_data.columns=boston.feature_names

boston_data[&#39;PRICE&#39;]=boston.target # 因变量，要预测的值

# 对于当前数据集，进行对数转换以减少异常值，然后对数据集进行归一化

boston_data_log=np.log1p(boston_data)
boston_data_log=(boston_data_log-boston_data_log.min())/(boston_data_log.max()-boston_data_log.min())
boston_data_log[&#39;价格&#39;]=boston_data[&#39;价格&#39;]

## 构建人工神经网络

# Log_transformed 和标准化数据集

从 sklearn.model_selection 导入 train_test_split
x_train,x_test,y_train,y_test=train_test_split(boston_data_log.drop(columns=[&#39;PRICE&#39;]).values,boston_data_log[&#39;PRICE&#39;].values,test_size=0.2)


x_train=torch.FloatTensor(x_train)
x_test=torch.FloatTensor(x_test)

y_train=torch.FloatTensor(y_train)
y_test=torch.FloatTensor(y_test)

进口火炬
将 torch.nn 导入为 nn
导入 torch.nn.function 作为 F

ANN 类（nn.Module）：
    def __init__(自身,输入层,隐藏层_1,隐藏层_2,隐藏层_3,隐藏层_4,隐藏层_5,输出层):
        超级().__init__()
        self.full_connected_1=nn.Linear(input_layer,hidden_​​layer_1)
        self.full_connected_2=nn.Linear(hidden_​​layer_1,hidden_​​layer_2)
        self.full_connected_3=nn.Linear(hidden_​​layer_2,hidden_​​layer_3)
        self.full_connected_4=nn.Linear(hidden_​​layer_3,hidden_​​layer_4)
        self.full_connected_5=nn.Linear(hidden_​​layer_4,hidden_​​layer_5)
        self.output_layer=nn.Linear(hidden_​​layer_5,output_layer)
    defforward_prop（自身，x）：
        x=F.relu(self.complete_connected_1(x))
        x=F.relu(self.complete_connected_2(x))
        x=F.relu(self.complete_connected_3(x))
        x=F.relu(self.complete_connected_4(x))
        x=F.relu(self.complete_connected_5(x))
        x=self.output_layer(x)
        返回x
        

火炬.manual_seed(20)
模型=ANN(x_train.shape[1],200,200,200,200,200,1)

loss_function=nn.MSELoss()
优化器=torch.optim.Adam(model.parameters(),lr=0.01)

纪元=500
loss_cumul_list=[]
循环列表=[]
对于范围内的 i（纪元）：
    y_pred=model.forward_prop(x_train)
    损失=loss_function(y_pred,y_train)
    loss_cumul_list.append(loss.item())
    循环列表.append(i+1)
    优化器.zero_grad()
    loss.backward()
    优化器.step()

损失累积列表

预测=[]
使用 torch.no_grad()：
    对于 i，枚举（x_test）中的数据：
        y_pred=model.forward_prop(数据)
        预测.append(y_pred.item())

从 sklearn.metrics 导入 r2_score,mean_squared_error,mean_absolute_error
error_r2=r2_score(y_test,预测)
error_mse=mean_squared_error(y_test,预测)
error_mae=mean_absolute_error(y_test,预测)

我一直试图使用 Pytorch 在波士顿房价数据集上应用 ANN，但在 50 个 epoch 后，损失并没有进一步减少太多。在网上看到过一些使用张量流的类似笔记本，但模型在那里工作得很好。无法理解这里的问题。
我一直试图使用 Pytorch 在波士顿房价数据集上应用 ANN，但在 50 个 epoch 后，损失并没有进一步减少太多。在网上看到过一些使用张量流的类似笔记本，但模型在那里工作得很好。无法理解这里的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78220834/ann-loss-not-reducing-for-boston-house-price-data-set</guid>
      <pubDate>Mon, 25 Mar 2024 17:39:02 GMT</pubDate>
    </item>
    <item>
      <title>Walker2d AI Gym 好像不会学习</title>
      <link>https://stackoverflow.com/questions/78220075/walker2d-ai-gym-doesnt-seem-to-learn</link>
      <description><![CDATA[我一直在尝试在 MuJoCO Walker2D 环境中进行基本的演员评论家代理学习。在模型、损失、超参数发生许多变化之后，我似乎没有取得任何进展（代理在 25 万步上没有任何明显的差异。
我意识到我可能有很多错误/问题，但我真的完全不知道下一步要尝试什么。
我最近的代理如下：
&lt;前&gt;&lt;代码&gt; def __init__(
      自己，
      num_actions: int):
    “”“初始化。”“”
    超级().__init__()
    self.common=layers.Dense(20，激活=“relu”，kernel_initializer=&#39;random_normal&#39;)
    self.common4 = 层.Dense(20, 激活=“relu”, kernel_initializer=&#39;random_normal&#39;)
    self.actor_means=layers.Dense(num_actions,activation=“tanh”,kernel_initializer=&#39;random_normal&#39;)
    self.critic=layers.Dense(1,activation=“线性”)#1个节点给出策略的值估计
    #下几行是给独立的第二评论家的
    self.crit_dense1 = 层.Dense(20, 激活=“relu”, kernel_initializer=&#39;random_normal&#39;)
    self.crit2 = 层.Dense(1, 激活=“线性”, kernel_initializer=&#39;random_normal&#39;)

  def call(self, 输入：tf.Tensor) -&gt;元组[tf.张量，tf.张量，tf.张量]：
    a = tf.expand_dims(输入, 0)
    b = self.common(a)
    d = self.common4(b)
    手段 = self.actor_means(d)
    q2 = self.crit_dense1(a)
    q2 = self.crit2(q2)
    返回方式，q2

情节是：
 for t in tf.range(max_steps)：
    # 运行模型并获取动作概率和批评值
    action_dist_t，值=模型（状态）
    值 = value.write(t, 值)
    dist = tfd.Normal(loc=action_dist_t, 比例=[0.1,0.1,0.1,0.1,0.1,0.1])
    动作=dist.sample([1])[0]

    动作 = tf.clip_by_value(动作, -0.99, 0.99)
    action_probs = action_probs.write(t, dist.prob(action_dist_t))#- 动作
    actions = actions.write(t, 动作)
    # 对环境应用动作以获得下一个状态和奖励

    状态，奖励，完成= env_step(tf.cast(tf.squeeze(action), tf.float32)) #
    #tf.print(str(奖励))
    状态.set_shape(初始状态_形状)
    # 储存奖励
    奖励 = 奖励.write(t, 奖励)

    如果 tf.cast(完成, tf.bool):
      休息


损失为：
defcompute_loss(
    action_probs：tf.张量，
    值：tf.张量，
    返回：tf.张量，
    动作：tf.张量
  ）-&gt; tf.张量：#
  “”“计算演员-评论家的综合损失。”“”“

  优势 =（返回 - 值）#+ 0.01
  actor_loss = -tf.math.reduce_sum(优势*action_probs, axis=-1) #tf.math.abs

  Criteria_loss = huber_loss（值，回报）


  返回 actor_loss + Criteria_loss

和训练步骤：
def train_step(
    初始状态：tf.张量，
    模型：tf.keras.Model，
    优化器：tf.keras.optimizers.Optimizer，
    伽玛：浮动，
    max_steps_per_episode: int) -&gt;; tf.张量：
  “”““运行模型训练步骤。”“”

  使用 tf.GradientTape() 作为磁带：

    # 运行模型一集以收集训练数据
    action_probs、值、奖励、动作 = run_episode(
        初始状态、模型、每集最大步数）

    # 计算预期收益
    返回 = get_expected_return(奖励, 伽马)

    # 将训练数据转换为适当的 TF 张量形状
    action_probs、值、回报、动作 = [
        tf.expand_dims(x, 1) for x in [action_probs, 值, 返回, 动作]]

    # 计算损失值以更新我们的网络
    损失=计算损失（action_probs，值，回报，行动）

  grads = Tape.gradient(loss, model.trainable_variables)
  grads = [tf.clip_by_value(grad, -0.5, 0.5) for grads]
  #grads,_ = tf.clip_by_global_norm(grads, -3)
  # 计算损失的梯度

  #for var, zip 中的 grad(model.trainable_variables, grads):
   # print(f&quot;变量: {var.name}, 梯度范数: {tf.norm(grad)}&quot;)
  # 将梯度应用于模型参数
  优化器.apply_gradients(zip(grads, model.trainable_variables))

  Episode_reward = tf.math.reduce_sum(奖励)
  批评者估计 = tf.math.reduce_sum(值)

我尝试过替代损失（NLL，CoPilot 建议的自定义损失）。我尝试过不同的模型架构（更多层、更宽层、输出分布）和一系列超参数（伽玛范围（0.25-0.99）、学习率（0.1,0.01,0.001）。我也尝试过使用和不使用梯度裁剪，总和而不是均值减少。最大奖励保持 &lt;5。]]></description>
      <guid>https://stackoverflow.com/questions/78220075/walker2d-ai-gym-doesnt-seem-to-learn</guid>
      <pubDate>Mon, 25 Mar 2024 15:16:46 GMT</pubDate>
    </item>
    <item>
      <title>在 python 中将管道重新编写为类时出错</title>
      <link>https://stackoverflow.com/questions/78219825/error-when-rewiriting-a-pipeline-as-a-class-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78219825/error-when-rewiriting-a-pipeline-as-a-class-in-python</guid>
      <pubDate>Mon, 25 Mar 2024 14:32:03 GMT</pubDate>
    </item>
    <item>
      <title>“MENACE”tictax脚趾计算机需要多少场比赛来训练</title>
      <link>https://stackoverflow.com/questions/78219696/how-many-games-will-a-menace-tic-tax-toe-computer-take-to-train</link>
      <description><![CDATA[我最近读到了唐纳德·米奇 (Donald Michie) 设计的用火柴盒建造的“计算机”，它可以自学如何玩井字游戏。这是关于它的维基百科文章：
https://en.m.wikipedia.org/wiki/Matchbox_Educable_Noughts_and_Crosses_Engine 
我觉得它看起来很有趣，所以我决定用 Python 制作一个数字版本，以供娱乐和练习。我已经让它对随机动作进行了大约 10,000 场比赛，但它仍然经常输给我。
需要玩多少场游戏才能让“火柴盒电脑”的工作方式与 Michie 设计的完美匹配？带有实际火柴盒的原始计算机是否达到了完美的播放效果？
我问这个问题是因为我担心我的程序可能无法正常工作。]]></description>
      <guid>https://stackoverflow.com/questions/78219696/how-many-games-will-a-menace-tic-tax-toe-computer-take-to-train</guid>
      <pubDate>Mon, 25 Mar 2024 14:12:37 GMT</pubDate>
    </item>
    <item>
      <title>在时间序列 ARIMA 分析中出现错误“TypeError：没有要绘制的数值数据”</title>
      <link>https://stackoverflow.com/questions/78218276/getting-error-typeerror-no-numeric-data-to-plot-in-a-time-series-arima-analys</link>
      <description><![CDATA[我正在尝试遵循一个教程，其中使用差异数据进行 ARIMA 时间序列分析：
以下是python代码：
def 差异（数据集）：
    差异=列表（）
    对于范围内的 i(1, len(数据集))：
        值 = 数据集[i] - 数据集[i - 1]
        diff.append(值)
    返回系列（差异）

系列 = pd.read_csv(&#39;dataset.csv&#39;)
X = series.values # 构建列表的错误可以在这里看到
X = X.astype(&#39;float32&#39;)
平稳 = 差值(X)
固定.索引 = 系列.索引[1:]
...
固定.plot()
pyplot.show()

当过程到达绘图阶段时，我收到错误：
&lt;块引用&gt;
类型错误：没有要绘制的数字数据

回溯起来，我发现正在解析的数据产生了一个数组的集合。将集合stationary保存为*.csv文件会给我一个如下列表：
&lt;前&gt;&lt;代码&gt;[11.]
[0.]
[16.]
[45.]
[27.]
[-141。]
[46]

有人可以告诉我这里出了什么问题吗？
PS。我已经排除了库导入的部分
编辑 1
数据集的一部分复制如下：
年份，观测值
1994,21
1995,62
1996,56
1997,29
1998,38
1999,201
]]></description>
      <guid>https://stackoverflow.com/questions/78218276/getting-error-typeerror-no-numeric-data-to-plot-in-a-time-series-arima-analys</guid>
      <pubDate>Mon, 25 Mar 2024 10:07:18 GMT</pubDate>
    </item>
    <item>
      <title>用于多标签分类的堆叠集成学习</title>
      <link>https://stackoverflow.com/questions/78214688/stacking-ensamble-learning-for-multilabelclassification</link>
      <description><![CDATA[我有两个 BERT 模型来实现代码中漏洞检测的多标签分类。一名接受过源代码培训，另一名接受过编译代码培训。他们实现的任务是多标签分类，因此两个模型的单个输出都是一个包含 6 个元素的数组，每个元素可以是 0 或 1，指示漏洞是否存在。
我想在这两个模型之上构建一个经典的 ML 分类器（如随机森林、SVM 或逻辑回归等），实现称为 Stacking 的集成技术。知道我正在处理多标签分类，我该如何实现这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78214688/stacking-ensamble-learning-for-multilabelclassification</guid>
      <pubDate>Sun, 24 Mar 2024 13:28:23 GMT</pubDate>
    </item>
    <item>
      <title>我做了什么来纠正属性错误。请帮助我[关闭]</title>
      <link>https://stackoverflow.com/questions/78210052/what-did-i-do-to-correct-the-attribute-error-please-help-me</link>
      <description><![CDATA[&lt;前&gt;&lt;代码&gt;batch_size = 100
对于范围 (25) 内的 i：
    num_batches = int(mnist.train.num_examples/batch_size)
    总成本 = 0
    对于范围内的 j（num_batches）：
        batch_x,batch_y = mnist.train.next_batch(batch_size)
        c, _ = sess.run([成本,优化], feed_dict={x:batch_x, y:batch_y, keep_prob:0.8})
        总成本 += c
    打印（总成本）

属性错误
                            回溯（最近一次调用最后一次）
单元格 In[65]，第 3 行
      1 批量大小 = 100
      2 对于范围 (25) 内的 i：
----&gt; 3 num_batches = int(mnist.train.num_examples/batch_size)
      4 总成本 = 0
      5 对于 j 在范围内（num_batches）：

AttributeError：模块“keras.datasets.mnist”没有属性“train”
]]></description>
      <guid>https://stackoverflow.com/questions/78210052/what-did-i-do-to-correct-the-attribute-error-please-help-me</guid>
      <pubDate>Sat, 23 Mar 2024 07:25:50 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 矩阵乘法形状错误：“RuntimeError：mat1 和 mat2 形状无法相乘”</title>
      <link>https://stackoverflow.com/questions/78196998/pytorch-matrix-multiplication-shape-error-runtimeerror-mat1-and-mat2-shapes-c</link>
      <description><![CDATA[我是 PyTorch 的新手，正在创建一个多输出线性回归模型，根据字母为单词着色。 （这将帮助有字素颜色联觉的人更轻松地阅读。）它接收单词并输出 RGB 值。每个单词都表示为 45 个浮点数 [0,1] 的向量，其中 (0, 1] 代表字母，0 代表该位置不存在字母。每个样本的输出应该是一个向量 [r-value, g -值，b-值]。
我懂了
&lt;块引用&gt;
运行时错误：mat1 和 mat2 形状无法相乘（90x1 和 45x3）

当我尝试在训练循环中运行我的模型时。
查看现有的 Stack Overflow 帖子，我认为这意味着我需要重塑我的数据，但我不知道如何/在哪里以解决此问题的方式进行此操作。特别是考虑到我不知道那个 90x1 矩阵来自哪里。
我的模型
我一开始很简单；在我可以让单个层发挥作用之后，可以出现多个层。
类 ColorPredictor(torch.nn.Module):
    #构造函数
    def __init__(自身):
        super(ColorPredictor, self).__init__()
        self.linear = torch.nn.Linear(45, 3, device= device) #编码词向量的长度 &amp; r,g,b 向量的大小
        
    ＃ 预言
    defforward(self, x: torch.Tensor) -&gt;;火炬.张量：
        y_pred = self.线性(x)
        返回 y_pred

我如何加载数据
# 数据集类
数据类（数据集）：
    # 构造函数
    def __init__(自身，输入，输出)：
        self.x = input # 编码词向量列表
        self.y = 输出 # 将 r、g、b 值转换为火炬张量的 Pandas 数据帧
        self.len = len(输入)
    
    # 吸气剂
    def __getitem__(自身，索引)：
        返回 self.x[索引], self.y[索引]
    
    # 获取样本数
    def __len__(自身):
        返回 self.len

# 创建训练/测试分割
train_size = int(0.8 * len(数据))
train_data = 数据(输入[:train_size], 输出[:train_size])
test_data = 数据(输入[train_size:], 输出[train_size:])

# 为训练和测试集创建 DataLoaders
train_loader = DataLoader（数据集= train_data，batch_size = 2）
test_loader = DataLoader（数据集= test_data，batch_size = 2）

发生错误的测试循环
对于范围内的纪元（纪元）：
    ＃ 火车
    model.train() #训练模式
    对于 train_loader 中的 x,y：
        y_pred = model(x) #此处错误
        损失=标准(y_pred, y)
        优化器.zero_grad()
        loss.backward()
        优化器.step()
      

错误回溯


新尝试：
将 45x1 输入张量更改为 2x45 输入张量，第二列全为零。这适用于第一次运行 train_loader 循环，但在第二次运行 train_loader 循环期间，我得到另一个矩阵乘法错误，这次是大小为 90x2 和 45x3 的矩阵。]]></description>
      <guid>https://stackoverflow.com/questions/78196998/pytorch-matrix-multiplication-shape-error-runtimeerror-mat1-and-mat2-shapes-c</guid>
      <pubDate>Thu, 21 Mar 2024 01:00:23 GMT</pubDate>
    </item>
    <item>
      <title>使用 OneClassSVM 绘制 SHAP 绘图需要很长时间</title>
      <link>https://stackoverflow.com/questions/73279201/shap-plots-taking-ages-with-oneclasssvm</link>
      <description><![CDATA[我正在尝试解释我的 OneClassSVM 模型，但计算时间非常长。我使用了 36 次折叠的交叉验证，因此希望将所有折叠的结果合并到一个 SHAP 图上，以便我可以充分解释哪些特征对模型贡献最大。
到目前为止，我认为对我想要解释的数据进行采样会加快处理速度（确实减少了时间），但折叠一次仍然需要大约 8 小时，并且有 36 次折叠。
请注意，我的训练集约为 2400 个，测试集约为 1400 个，每个集有 88 个特征。
导入形状
从 sklearn.svm 导入 OneClassSVM
将 numpy 导入为 np

# 这些是二维数组，其中每个元素都是用于训练/测试折叠的所选数据的 DataFrame
shap_train = np.load(&#39;shap_train.npy&#39;,allow_pickle=True)
shap_test = np.load(&#39;shap_test.npy&#39;,allow_pickle=True)

clf = OneClassSVM(nu=0.35)

折叠 = len(shap_train)
形状值 = []
shap_data_test = []

对于范围内的折叠（折叠）：
        解释器 = shap.Explainer(clf.fit_predict, shap_train[fold])
        # 采样1/3的数据
        数据 = shap_test[fold].sample(frac=(1/3))
        shap_values.append(解释器(数据))
        shap_data_test.append（数据）

# 存储稍后绘图的 SHAP 值
np.save(&#39;shap_data.npy&#39;, np.array(shap_values))
np.save(&#39;shap_data_test.npy&#39;, np.array(shap_data_test))


我对需要为所有折叠生成形状值的方法提出了质疑，但我知道某些折叠的性能比其他折叠更好，因此希望全面了解哪些功能贡献最大。
我将此脚本部署在具有 Intel(R) Xeon(R) CPU E5-2667 v4 @ 3.20GHz 和 64GB RAM 的 Debian 服务器上。]]></description>
      <guid>https://stackoverflow.com/questions/73279201/shap-plots-taking-ages-with-oneclasssvm</guid>
      <pubDate>Mon, 08 Aug 2022 14:06:46 GMT</pubDate>
    </item>
    <item>
      <title>在 XGBoost 的 GridSearchCV 中评分</title>
      <link>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</link>
      <description><![CDATA[我目前正在尝试使用 XGBoost 第一次分析数据。我想使用 GridsearchCV 找到最佳参数。我想最小化均方根误差，为此，我使用“rmse”作为 eval_metric。然而，网格搜索中的评分没有这样的指标。我在这个网站上发现“neg_mean_squared_error”的作用相同，但我发现这给出了与 RMSE 不同的结果。当我计算“neg_mean_squared_error”的绝对值的根时，我得到的值约为 8.9，而另一个函数给出的 RMSE 约为 4.4。
我不知道出了什么问题，也不知道如何让这两个函数达成一致/给出相同的值？
由于这个问题，我得到了错误的“best_params_”值，这给了我比我最初开始调整的一些值更高的 RMSE。
谁能解释一下如何在网格搜索中获得 RMSE 分数或者为什么我的代码给出不同的值？ 
提前致谢。
def modelfit（alg、trainx、trainy、useTrainCV=True、cv_folds=10、early_stopping_rounds=50）：
    如果使用TrainCV：
        xgb_param = alg.get_xgb_params()
        xgtrain = xgb.DMatrix(trainx, label=trainy)
        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()[&#39;n_estimators&#39;], nfold=cv_folds,
                          指标=&#39;rmse&#39;，early_stopping_rounds=early_stopping_rounds）
        alg.set_params(n_estimators=cvresult.shape[0])

    # 将算法拟合到数据上
    alg.fit(trainx, trainy, eval_metric=&#39;rmse&#39;)

    # 预测训练集：
    dtrain_predictions = alg.predict(trainx)
    # dtrain_predprob = alg.predict_proba(trainy)[:, 1]
    打印（dtrain_预测）
    打印（np.sqrt（mean_squared_error（trainy，dtrain_predictions）））

    # 打印模型报告：
    print(&quot;\n模型报告&quot;)
    print(&quot;RMSE : %.4g&quot; % np.sqrt(metrics.mean_squared_error(trainy, dtrain_predictions)))

 参数_test2 = {
 &#39;最大深度&#39;:[6,7,8],
 &#39;min_child_weight&#39;:[2,3,4]
}

grid2 = GridSearchCV(估计器 = xgb.XGBRegressor( 学习率 =0.1, n_estimators=2000, max_深度=5,
 min_child_weight=2，gamma=0，子样本=0.8，colsample_bytree=0.8，
 目标=&#39;reg：线性&#39;，nthread=4，scale_pos_weight=1，random_state=4），
 param_grid = param_test2，评分=&#39;neg_mean_squared_error&#39;，n_jobs=4，iid=False，cv=10，详细=20）
grid2.fit(X_train,y_train)
# best_estimator 的平均交叉验证分数
打印（grid2.best_params_，np.sqrt（np.abs（grid2.best_score_））），打印（np.sqrt（np.abs（grid2.score（X_train，y_train））））
modelfit(grid2.best_estimator_, X_train, y_train)
打印（np.sqrt（np.abs（grid2.score（X_train，y_train））））
]]></description>
      <guid>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</guid>
      <pubDate>Fri, 11 May 2018 16:46:16 GMT</pubDate>
    </item>
    </channel>
</rss>