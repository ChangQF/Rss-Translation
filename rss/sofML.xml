<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 04 Jun 2024 12:27:30 GMT</lastBuildDate>
    <item>
      <title>请问如何改进我的混合 1D CNN 和 Bi-LSTM 模型以实现高精度</title>
      <link>https://stackoverflow.com/questions/78575294/please-how-can-improve-my-hybrid-1d-cnn-and-bi-lstm-model-for-high-accuracy</link>
      <description><![CDATA[我正在构建一个混合 1D CNN 和 Bi-LSTM 模型，用于预测心脏病。然而，该模型的准确率是 0.73，但我想将其提高到 0.80 及以上。请就如何改进此模型提供任何帮助。谢谢。我期望准确率能稍微提高一点。
我的输入形状如下所示 (70000,13)
import tensorflow as tf 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling2D, MaxPooling1D, LSTM, Bidirectional, Dense, Flatten, Dropout, Input, BatchNormalization, Reshape
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils.class_weight import compute_class_weight 

dataset = pd.read_csv(&#39;heart_disease.csv&#39;)
dataset.shape

#预处理数据集
X = dataset.drop(columns=[&#39;disease&#39;])
y = dataset[&#39;disease&#39;]

`#标准化特征
scaler = StandardScaler()
X = scaler.fit_transform(X)
# print(X)

#将数据分成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(&quot;训练集大小：&quot;, X_train.shape)
print(&quot;测试集大小：&quot;, X_test.shape)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#重塑数据1D CNN + Bi-LSTM 模型
X_train_dl = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_dl = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

#print(X_train_dl)
#print(X_test_dl)

# 构建混合模型

model = Sequential()
model.add(Input(shape=(X_train_dl.shape[1], X_train_dl.shape[2])))

model.add(Conv1D(filters=32, kernel_size=2,activation=&#39;relu&#39;))

model.add(MaxPooling1D(pool_size=2))
model.add(BatchNormalization(momentum=0.99))

model.add(Conv1D(filters=64, kernel_size=2,activation=&#39;relu&#39;))
model.add(MaxPooling1D(pool_size=2))
model.add(BatchNormalization(momentum=0.99))

model.add(Bidirectional(LSTM(50, return_sequences=True)))
model.add(Dropout(0.5))
model.add(Flatten())

`model.add(Dense(128,activation=&#39;relu&#39;, kernel_regularizer=tf.keras.regularizers.l2(0.01)))
model.add(Dropout(0.5))

model.add(Dense(1,activation=&#39;sigmoid&#39;))

#编译模型

model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate = 0.0001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

# 提前停止回调
early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, waiting=20, restore_best_weights=True)

# 训练模型
history = model.fit(X_train_dl, y_train, epochs=10, batch_size=32, validation_data=(X_test_dl, y_test), callbacks=[early_stopping])

# 保存模型
model.save(&#39;my_model.keras&#39;)

# 评估模型
loss accuracy = model.evaluate(X_test_dl, y_test)
print(f &quot;混合模型 (1D CNN + Bi-LSTM) 准确率： {准确度：.2f}&quot;)

]]></description>
      <guid>https://stackoverflow.com/questions/78575294/please-how-can-improve-my-hybrid-1d-cnn-and-bi-lstm-model-for-high-accuracy</guid>
      <pubDate>Tue, 04 Jun 2024 12:04:25 GMT</pubDate>
    </item>
    <item>
      <title>人工智能集成来分析数据</title>
      <link>https://stackoverflow.com/questions/78575195/ai-integration-to-analyze-data</link>
      <description><![CDATA[我是 AI/ML 新手。我有一个挂载点的 6 个月数据集（分钟级）利用率。例如：
时间 -- 利用率
4-Jan-24 5:01 -- 10 GB
4-Jan-24 5:02 -- 11 GB
4-Jan-24 5:03 -- 9 GB
4-Jan-24 5:04 -- 12 GB

---

4-Aug-24 5:04 -- 20 GB
4-Aug-24 5:04 -- 15 GB
4-Aug-24 5:04 -- 30 GB

现在，如果我想获取未来 2/3 个月的趋势/预测。我该如何实现？
获取数据预测]]></description>
      <guid>https://stackoverflow.com/questions/78575195/ai-integration-to-analyze-data</guid>
      <pubDate>Tue, 04 Jun 2024 11:47:03 GMT</pubDate>
    </item>
    <item>
      <title>BIRCH 无法分配，太大</title>
      <link>https://stackoverflow.com/questions/78574823/birch-unable-to-allocate-its-too-big</link>
      <description><![CDATA[MemoryError: 无法为形状为 (49604310962128,) 且数据类型为 float64 的数组分配 361. TiB
from sklearn.cluster import Birch
from sklearn.metrics import pairwise_distances
branching_factor = 50
n_clusters = 1000000 # 设置一个较大的聚类数
threshold = 0.5

# 创建BIRCH聚类器
birch = Birch(n_clusters=n_clusters, Threshold=threshold, Branching_factor=branching_factor)

# 训练BIRCH聚类器
birch.fit(reduced_data)

包含10,000,000个样本，每个样本为一行，第一个样本号，剩余64个样本特征。数据来自DNA序列最大簇数为1000000，如果超过1000000，多余的簇会被合并到第1000000个簇中。
哪种聚类方案最好，如何做]]></description>
      <guid>https://stackoverflow.com/questions/78574823/birch-unable-to-allocate-its-too-big</guid>
      <pubDate>Tue, 04 Jun 2024 10:31:38 GMT</pubDate>
    </item>
    <item>
      <title>排名模型中的聚合以更改观察单位</title>
      <link>https://stackoverflow.com/questions/78574761/aggregation-in-the-ranking-model-to-change-the-unit-of-observation</link>
      <description><![CDATA[设置：
我需要建立一个排名模型来对酒店进行排名（让我们考虑一个没有个性化的搜索系统），因此应用程序中的最终观察单位必须是酒店。但在训练和测试我的模型期间，我有一个客户查询作为观察单位（因此我的数据如下所示：query_id + hotel + features + target）。
问题：
从初始观察单位到最终观察单位的最佳方法是什么？在对每个查询进行预测并计算平均/中位数分数后，按酒店分组汇总结果是否是个好主意？有什么想法吗？
提前谢谢您~~
现在我唯一的想法是在 Catboost 模型中计算平均/中位数预测分数，并按酒店分组。但这似乎不是最好的方法……]]></description>
      <guid>https://stackoverflow.com/questions/78574761/aggregation-in-the-ranking-model-to-change-the-unit-of-observation</guid>
      <pubDate>Tue, 04 Jun 2024 10:17:48 GMT</pubDate>
    </item>
    <item>
      <title>模型校准</title>
      <link>https://stackoverflow.com/questions/78574055/model-calibration</link>
      <description><![CDATA[我正在研究概率校准，如果我有一个高度不平衡的数据集（例如来自 Kaggle 的欺诈检测数据集），我对如何正确评估校准有一些疑问。
我知道有很多校准技术，例如 Platt Scaling、Isotonic Regression、Beta Calibration、SplineCalib 和 Venn-ABERS。此外，还有多种方法可以衡量校准指标（例如 ECE、Brier Score、对数损失和校准曲线）的效果。
此外，我想在增强算法上测试它：XGBoost、LGBM
那么，

对于不平衡的数据集，我应该使用哪种校准模型？
哪种指标更适合这样的任务？
您能否提供一些资源（书籍、YouTube 视频、文章）以了解有关不同方法的更多信息以及何时应该使用它们？

我在 XGBoost 模型上尝试了 Platt Scaling 和 Isotonic Regression，并比较了校准前后的 ECE、Brier Score 和对数损失指标，它们的值更差。]]></description>
      <guid>https://stackoverflow.com/questions/78574055/model-calibration</guid>
      <pubDate>Tue, 04 Jun 2024 07:56:41 GMT</pubDate>
    </item>
    <item>
      <title>如何改进 CNN 二元分类模型</title>
      <link>https://stackoverflow.com/questions/78573842/how-to-improve-cnn-binary-classification-model</link>
      <description><![CDATA[我是 AI 和 ML 的新手，正在尝试开发一个模型来对一组脑肿瘤的 MRI 扫描图像进行分类。它有两个类别（“健康”和“肿瘤”）。我的数据集有 4600 张图像（2087 张为健康，2513 张为肿瘤）。我使用了 15 个 epoch，准确度在逐渐提高。但验证损失和验证准确度却时高时低。我觉得这是因为模型过度拟合。有什么建议可以提高验证准确度？我必须在模型中添加更多或更少的层吗？按升序添加 Conv2D 的过滤器数量是否正确？谢谢。
这是我如何分割训练和验证数据。
training_data, validation_data = tf.keras.utils.image_dataset_from_directory(data_dir,
batch_size=32,
image_size=(128, 128),
validation_split=.2,
subset=&#39;both&#39;,
seed=42)

输出：
找到属于 2 个类别的 4514 个文件。
使用 3612 个文件进行训练。
使用 902 个文件进行验证。
图像如下所示

在这里我重新缩放数据
training_data = training_data.map(lambda x,y: (x/255,y))
validation_data = validation_data.map(lambda x,y: (x/255,y))

我按如下方式进行了数据增强
data_augmentation = tf.keras.Sequential(
[
tf.keras.layers.RandomFlip(&quot;horizo​​ntal&quot;),
tf.keras.layers.RandomRotation(0.2),
tf.keras.layers.RandomZoom(0.2),
])

这是我的模型
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.InputLayer(shape=(128, 128, 3)))

model.add(data_augmentation)

model.add(tf.keras.layers.Conv2D(32, kernel_size=3,activation=&#39;relu&#39;)),
model.add(tf.keras.layers.BatchNormalization()),
model.add(tf.keras.layers.MaxPooling2D(2, 2)),

model.add(tf.keras.layers.Conv2D(64, kernel_size=3,激活=&#39;relu&#39;)),
model.add(tf.keras.layers.BatchNormalization()),
model.add(tf.keras.layers.MaxPooling2D(2, 2)),

model.add(tf.keras.layers.Conv2D(128, kernel_size=3, 激活=&#39;relu&#39;)),
model.add(tf.keras.layers.BatchNormalization()),
model.add(tf.keras.layers.MaxPooling2D(2, 2)),

model.add(tf.keras.layers.Flatten()),

model.add(tf.keras.layers.Dense(128,激活=&#39;relu&#39;)),
model.add(tf.keras.layers.BatchNormalization()),
model.add(tf.keras.layers.Dropout(0.3)),

model.add(tf.keras.layers.Dense(32, 激活=&#39;relu&#39;)),
model.add(tf.keras.layers.BatchNormalization()),
model.add(tf.keras.layers.Dropout(0.3)),

model.add(tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;))
]]></description>
      <guid>https://stackoverflow.com/questions/78573842/how-to-improve-cnn-binary-classification-model</guid>
      <pubDate>Tue, 04 Jun 2024 07:08:06 GMT</pubDate>
    </item>
    <item>
      <title>如何使用随机森林计算预测的置信区间？</title>
      <link>https://stackoverflow.com/questions/78572538/how-to-calculate-confidence-interval-for-forecast-using-random-forest</link>
      <description><![CDATA[我正在计算名为 &quot;spot&quot; 的变量的预测（数据的未来结果）。我正在使用随机森林和名为 &quot;DTCI&quot; 的独立变量来协助预测 &quot;spot&quot;。预测以每月频率进行，与数据频率相同。我想根据每个月的上限和下限获得每个预测月份的置信区间。它与附图中所做的类似，带有绿色限制。

我尝试使用 GradientBoostingRegressor 构建间隔，如下所示：
# 设置下限和上限分位数
inf = 0.1
sup = 0.9

# 每个模型必须独立
lower_model = GradientBoostingRegressor(loss=&quot;quantile&quot;, alpha=inf)
upper_model = GradientBoostingRegressor(loss=&quot;quantile&quot;, alpha=sup)

lower_model.fit(X_train, y_train)
upper_model.fit(X_train, y_train)

predictions = pd.DataFrame(y_hat_forecast_spot)

predictions[&quot;inf&quot;] = lower_model.predict(X_fore)
predictions[&quot;sup&quot;] = upper_model.predict(X_fore)

然而，结果并没有我预期的趋势。由于它是一个时间序列，我想象（如上图所示）限值应该以置信区域变大的方式增长。换句话说，日期越远，预测就越困难，因此与之相关的误差或间隔就越大。
我使用 GradientBoostingRegressor 得到的结果（如下所示）的间隔会随时间变化而不是增长。

GradientBoostingRegressor 适合时间序列吗？或者有其他函数可以更好地理解时间序列吗？]]></description>
      <guid>https://stackoverflow.com/questions/78572538/how-to-calculate-confidence-interval-for-forecast-using-random-forest</guid>
      <pubDate>Mon, 03 Jun 2024 21:17:18 GMT</pubDate>
    </item>
    <item>
      <title>加载预先训练的 json 模型时出错</title>
      <link>https://stackoverflow.com/questions/78570246/error-when-loading-a-pre-trained-json-model</link>
      <description><![CDATA[这是我收到的错误：
回溯（最近一次调用）：
文件“C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\anti.py”，第 14 行，位于 &lt;module&gt;
model = tf.keras.models.model_from_json(loaded_model_json)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\models\model.py&quot;，第 575 行，位于 model_from_json
return serialization_lib.deserialize_keras_object(
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件&quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\saving\serialization_lib.py&quot;，第 694 行，位于 deserialize_keras_object
cls = _retrieve_class_or_fn(
^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\Richard.Joy\Desktop\Final-antispoofing_models\antispoof_env\Lib\site-packages\keras\src\saving\serialization_lib.py&quot;，第 812 行，位于 _retrieve_class_or_fn
raise TypeError(
TypeError：无法找到类“Functional”。确保自定义类已用修饰`@keras.saving.register_keras_serializable()`。完整对象配置：{&#39;class_name&#39;: &#39;Functional&#39;, &#39;config&#39;:.....(**json 文件的内容**........&#39;keras_version&#39;: &#39;2.15.0&#39;, &#39;backend&#39;: &#39;tensorflow&#39;)

以下是库版本：
keras 3.3.3
opencv-python 4.9.0.80
tensorflow 2.16.1
python 3.12.3

以下是我的代码：
import cv2
import tensorflow as tf
from tensorflow.keras.preprocessing.image import img_to_array 
import os
import numpy as np

root_dir = os.getcwd()
# 加载人脸检测模型
trained_face_data = cv2.CascadeClassifier(cv2.data.haarcascades + &#39;haarcascade_frontalface_default.xml&#39;)
# 加载反欺骗模型图
json_file = open(&#39;C:/Users/Richard.Joy/Desktop/Final-antispoofing_models/Antispoofing_model_mobilenet.json&#39;,&#39;r&#39;)
loaded_model_json = json_file.read()
json_file.close()
model = tf.keras.models.model_from_json(loaded_model_json)
# 加载反欺骗模型权重
model.load_weights(&#39;C:/Users/Richard.Joy/Desktop/Final-antispoofing_models/project_antispoofing_model_97-0.957895.h5&#39;)
print(&quot;模型从中加载disk&quot;)

video = cv2.VideoCapture(0)
while True:
try:
ret,frame = video.read()
gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
faces = training_face_data.detectMultiScale(gray,1.3,5)
for (x,y,w,h) in faces: 
face = frame[y-5:y+h+5,x-5:x+w+5]
resized_face = cv2.resize(face,(160,160))
resized_face = resized_face.astype(&quot;float&quot;) / 255.0
resized_face = np.expand_dims(resized_face, axis=0)
# 将人脸 ROI 传递给训练过的活体检测器
# 模型以确定人脸是“真”还是“假”
preds = model.predict(resized_face)[0]
print(preds)
if preds&gt; 0.5:
标签 = &#39;poof&#39;
cv2.putText(frame, 标签, (x,y - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)
cv2.rectangle(frame, (x, y), (x+w,y+h),
(0, 0, 255), 2)
else:
标签 = &#39;eal&#39;
cv2.putText(frame, 标签, (x,y - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)
cv2.rectangle(frame, (x, y), (x+w,y+h),
(0, 255, 0), 2)
cv2.imshow(&#39;frame&#39;, frame)
key = cv2.waitKey(1)
if key == ord(&#39;q&#39;):
break
except Exception as e: 
pass
video.release() 
cv2.destroyAllWindows()

我该怎么办？我尝试降级库，但也遇到了错误。]]></description>
      <guid>https://stackoverflow.com/questions/78570246/error-when-loading-a-pre-trained-json-model</guid>
      <pubDate>Mon, 03 Jun 2024 12:21:33 GMT</pubDate>
    </item>
    <item>
      <title>我的训练被随机终止，没有错误日志</title>
      <link>https://stackoverflow.com/questions/78568551/my-training-gets-killed-randomly-without-an-error-log</link>
      <description><![CDATA[我一直在尝试在集群计算机上训练 trackformer 模型。它显示了一条 Killed 消息，并且没有任何日志。

将权重传递到模型中时发生错误
尝试 dmesg 时，我得到了以下输出
[Mon Jun 3 07:24:26 2024] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=task_0,mems_allowed=0-1,oom_memcg=/system.slice/slurmstepd.scope/job_19894856,task_memcg=/system
.slice/slurmstepd.scope/job_19894856/step_batch/user/task_0,task=python,pid=702977,uid=1380211
[2024 年 6 月 3 日星期一 07:24:26] 内存 cgroup 内存不足：已终止进程 702977 (python) total-vm:16068556kB, anon-rss:2144556kB, file-rss:123156kB, shmem-rss:28256kB, UID:1380211 pgtab
les:10612kB oom_score_adj:0

我试图在自定义数据集上训练 trackformer 模型。但训练过程随机停止。]]></description>
      <guid>https://stackoverflow.com/questions/78568551/my-training-gets-killed-randomly-without-an-error-log</guid>
      <pubDate>Mon, 03 Jun 2024 05:38:39 GMT</pubDate>
    </item>
    <item>
      <title>随机森林/决策树输出概率设计：使用正输出叶样本/总输出叶样本</title>
      <link>https://stackoverflow.com/questions/78561885/random-forest-decision-tree-output-probability-design-using-positive-output-l</link>
      <description><![CDATA[我正在使用 python 和 scikitlearn 设计一个二元分类器随机森林模型，我想在其中检索我的测试集是两个标签之一的概率。据我了解，predict_proba(xtest) 将给我以下结果：
投票给分类器的树数/树数

我发现这太不精确了，因为某些树节点可能将我的（非确定性）样本分成相当精确的叶子（100 个 a 类，0 个 b 类）和不精确的叶子（5 个 a 类，3 个 b 类）。我想要一个“概率”的实现，将我的 n 个分类器输出叶子中的样本总数作为主导，将输出叶子中总体选择的分类器的总数作为分子（即使对于选择大多数树没有选择的类的树及其输出叶子也是如此）。
例如（简单）：
2 棵树：
树 1： 
--- 5, 0 类 A（已选择） 
10 
--- 2, 3 类 B（未选择） 

树 2： 
--- 3, 2 类 A（已选择） 
10 
--- 5, 0 类 B（未选择）

predict_proba 结果：
选择类 A 的树数 (2) / 树数 (2) = 1.0

期望结果：
输出叶子中的 A 类样本数 (8) / 输出叶子中的样本总数 (10) = 0.8

有人知道如何做到这一点，或者他们正在使用什么实现？
我有一个想法，就是遍历每棵树，检索它们的概率，然后取平均值。但是，这会给样本较少的输出叶子带来更高的偏差（选举团风格）。
如何直接访问特定样本的决策树输出叶子的样本数量及其类别（或者甚至只是叶子索引，然后从那里开始）？在随机森林的情况下，对它们求和并取平均值？
如果不行，就完全切换平台/库？或者可能只是增加分类器的数量（不是最佳的）？
一些可能有用的文档？：
dtc.tree_.n_node_samples
dtc.tree_[node_index].n_node_samples ?
]]></description>
      <guid>https://stackoverflow.com/questions/78561885/random-forest-decision-tree-output-probability-design-using-positive-output-l</guid>
      <pubDate>Fri, 31 May 2024 19:44:58 GMT</pubDate>
    </item>
    <item>
      <title>Synthcity DECAF 生成人工智能中的形状误差</title>
      <link>https://stackoverflow.com/questions/78548544/synthcity-decaf-shape-error-in-generative-artificial-intelligence</link>
      <description><![CDATA[我正在尝试使用 DECAF 生成器生成新数据，但出现无法解决的错误。
我使用的代码与主 repo 文档中提到的代码完全相同（链接 Synthcity Docs）：
from sklearn.datasets import load_iris
from synthcity.plugins import Plugins

X, y = load_iris(as_frame = True, return_X_y = True)
X[&quot;target&quot;] = y

plugin = Plugins().get(&quot;decaf&quot;, n_iter = 100)
plugin.fit(X)

plugin.generate(50)

我不断得到
ValueError：传递值的形状为 (150, 1)，索引暗示 (150, 3)

无论我做什么。我有点惊讶错误竟然会发生，因为它实际上是作者的一个案例研究。
有人能解释或更重要的是解决这个错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78548544/synthcity-decaf-shape-error-in-generative-artificial-intelligence</guid>
      <pubDate>Wed, 29 May 2024 09:25:58 GMT</pubDate>
    </item>
    <item>
      <title>加载 json 模型时 Python tensorflow keras 出错：无法找到类“Sequential”</title>
      <link>https://stackoverflow.com/questions/78170750/python-tensorflow-keras-error-when-load-a-json-model-could-not-locate-class-se</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78170750/python-tensorflow-keras-error-when-load-a-json-model-could-not-locate-class-se</guid>
      <pubDate>Sat, 16 Mar 2024 05:59:37 GMT</pubDate>
    </item>
    <item>
      <title>如何在 M3 Mac 上安装 ML-Agents：Onnx 和 Protobuf 问题</title>
      <link>https://stackoverflow.com/questions/77934861/how-to-install-ml-agents-on-m3-mac-onnx-protobuf-issues</link>
      <description><![CDATA[我正尝试使用 conda 在我的 M3 Mac 上安装 ML-Agents。我按照网络文档 (https://unity-technologies.github.io/ml-agents/Installation/) 中列出的说明进行操作，但遇到了 protobuf 版本问题。我将其更改为 3.6，但这不适用于 onnx，因为 onnx 似乎需要 4.25。我真的不确定如何解决这个问题，如果能得到任何帮助，我将不胜感激！
以下是我运行到此点的命令：

conda create -n mlagents python=3.10.12 &amp;&amp; conda 激活 mlagents
git clone --branch release_21 https://github.com/Unity-Technologies/ml-agents.git
python -m pip install ./ml-agents-envs (有效)
python -m pip install ./ml-agents
]]></description>
      <guid>https://stackoverflow.com/questions/77934861/how-to-install-ml-agents-on-m3-mac-onnx-protobuf-issues</guid>
      <pubDate>Sun, 04 Feb 2024 06:53:10 GMT</pubDate>
    </item>
    <item>
      <title>Facebook Prophet 错误：cmdstanpy - 错误 - 链 [1] 错误：由信号 11 终止 未知错误 -11</title>
      <link>https://stackoverflow.com/questions/75413943/facebook-prophet-error-cmdstanpy-error-chain-1-error-terminated-by-signa</link>
      <description><![CDATA[我正在尝试在时间序列数据上训练 Facebook 的预言机。
我使用的数据，一列名为 ds，另一列名为 y
但是在拟合过程中，我首先收到此错误：
第一个错误屏幕截图
接着
第二个错误屏幕截图
有人知道为什么会这样吗？谢谢！
我尝试在线搜索问题，但找不到解决方案。我尝试安装 cmdstanpy 版本 0.9.5 并重新安装 prophecy，但没有成功，所以我有点卡住了。我正在运行 python 3.7.12 版本。]]></description>
      <guid>https://stackoverflow.com/questions/75413943/facebook-prophet-error-cmdstanpy-error-chain-1-error-terminated-by-signa</guid>
      <pubDate>Fri, 10 Feb 2023 16:48:37 GMT</pubDate>
    </item>
    <item>
      <title>可根据需要动态加载多个 CoreML 模型</title>
      <link>https://stackoverflow.com/questions/45272644/multiple-and-dynamically-loaded-coreml-models-on-demand</link>
      <description><![CDATA[我正在开发一个 iOS 11 应用，以利用新的 CoreML 框架。这个想法是使用不同的 .mlmodel 文件来处理视频输入，并为用户提供选择所需模型进行分类的能力。
我这里有两个问题：

无法为每个 xcode 项目添加多个模型。xcode 停止
生成强类型模型，我无法实例化它们
无法动态加载 .mlmodel（假设我按需下载了它）。有没有办法动态加载 .mlmodel？
]]></description>
      <guid>https://stackoverflow.com/questions/45272644/multiple-and-dynamically-loaded-coreml-models-on-demand</guid>
      <pubDate>Mon, 24 Jul 2017 04:28:55 GMT</pubDate>
    </item>
    </channel>
</rss>