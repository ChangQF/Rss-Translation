<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 11 Mar 2024 21:14:50 GMT</lastBuildDate>
    <item>
      <title>为什么 plt.show() 不能与 plt.plot 一起使用？ （展示模型训练和测试结果时）</title>
      <link>https://stackoverflow.com/questions/78143385/why-isnt-plt-show-working-with-plt-plot-when-showing-model-training-and-test</link>
      <description><![CDATA[我已经训练了 CNN 模型并正在尝试显示结果。然而，尽管下面的代码没有错误（这是我用 matplotlib 显示图形的代码块），但没有显示图形。我看过很多教程，但似乎没有一个有帮助？
这是代码：
 defplot_accuracies(历史):
        ”“”绘制精度历史记录“”
        准确度 = [x[&#39;val_acc&#39;] for x in历史]
        plt.plot(准确度, &#39;-x&#39;)
        plt.xlabel(&#39;纪元&#39;)
        plt.ylabel(&#39;准确度&#39;)
        plt.title(&#39;准确率与纪元数&#39;);
    
    
    情节准确度（历史）
    
    
    defplot_losses（历史）：
        ”“”绘制每个时期的损失“”
        train_losses = [x.get(&#39;train_loss&#39;) for x in历史]
        val_losses = [x[&#39;val_loss&#39;] 对于历史中的 x]
        plt.plot(train_losses, &#39;-bx&#39;)
        plt.plot(val_losses, &#39;-rx&#39;)
        plt.xlabel(&#39;纪元&#39;)
        plt.ylabel(&#39;损失&#39;)
        plt.legend([&#39;训练&#39;, &#39;验证&#39;])
        plt.title(&#39;损失与历元数&#39;);
    
    
    情节损失（历史）
    
    plt.show()

任何帮助都会很棒！]]></description>
      <guid>https://stackoverflow.com/questions/78143385/why-isnt-plt-show-working-with-plt-plot-when-showing-model-training-and-test</guid>
      <pubDate>Mon, 11 Mar 2024 20:57:11 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络节点迭代更新时输入输出维度的一致性</title>
      <link>https://stackoverflow.com/questions/78143339/graph-neural-network-nodes-input-and-output-dimension-consistency-amid-iterativ</link>
      <description><![CDATA[我试图弄清楚使用图神经网络学习嵌入后节点的输入维度（n）和输出维度（m）是否可以不同，我想是的。但是，我正在努力弄清楚这是如何实现的，如下所示：
一般在迭代步骤中：
h_u^(k+1) = update(h_u^(k) + 聚合({h_v^(k) | u 的 v 邻居})
现在，h_u(0) 的维度是输入维度，因此在生成 h_u(1) 时，聚合函数和更新函数的输入是 n 的向量，这里的输出具有维度 m（主要是矩阵乘法和完成了加权求和类型的聚合，并将维度调整为 m)，那么从下一次迭代开始，如何通过相同的函数或矩阵来处理维度 m（而不是 n）的向量进行更新？
我恳请 AI-ML 社区提供解释。]]></description>
      <guid>https://stackoverflow.com/questions/78143339/graph-neural-network-nodes-input-and-output-dimension-consistency-amid-iterativ</guid>
      <pubDate>Mon, 11 Mar 2024 20:47:10 GMT</pubDate>
    </item>
    <item>
      <title>我的 LSTM 模型可以自行学习特征工程吗？</title>
      <link>https://stackoverflow.com/questions/78142401/can-my-lstm-model-learn-feature-engineering-on-its-own</link>
      <description><![CDATA[我有一个时间序列数据集，我正在其上训练 LSTM 模型以执行多类分类。
我的数据集有 7 列 =&gt; x1,x2,x3...x7
并且有 4 个标签 =&gt; f1,f2,f3,f4
由于我拥有数据集的领域知识，因此我确切地知道需要完成哪些特征工程。
例如，我需要通过在每一行应用一些规则来从当前功能创建 4 个新功能：-

newx1 是由 =&gt; 创建的if (x2==x3) then 1, else 0
newx2 是由 =&gt; 创建的if (x1==x4 and x1&gt;x5) then 1, else 0
newx3 是由 =&gt; 创建的if ((x1-x6)/x1&gt;x7) 则 1，否则 0
newx4 是由 =&gt; 创建的if ((x6-x1)x1/&gt;x7) then 1. else 0

如果我在 newx1、newx2、newx3、newx4 上训练 LSTM 模型，测试数据的准确率将达到 100%。
但是，在原始特征 (x1,x2...x7) 上对其进行训练时，测试数据的准确度降低了 85-90%。
我试图解决的问题要求我的准确率高于 99%，因此仅仅 90% 的准确率是不够的。
我想知道我的 LSTM 模型是否可以自行学习特征工程的规则，或者我是否必须更改我的模型？
注意：我无法手动应用特征工程规则，因为我正在多个数据集上训练 LSTM 模型，并且每个数据集都需要自己的特征工程规则。我想让它尽可能通用。
LSTM模型：
def create_lstm_model(MaxTimeslice, H, LR, num_classes, dropout_rate=0.1, l2_reg=0.001):

    ip = 输入(形状=(MaxTimeslice, H))
    x = LSTM(32, return_sequences=True, dropout=dropout_rate, kernel_regularizer=l2(l2_reg))(ip)
    x = LSTM(16, dropout=dropout_rate, kernel_regularizer=l2(l2_reg))(x)
    
   
    x = 密集（单位=16，激活=&#39;relu&#39;）（x）
    multiclass_output = Dense（单位=num_classes，激活=&#39;softmax&#39;）（x）

    模型=模型（输入=ip，输出=multiclass_output）

    model.compile(loss=“categorical_crossentropy”,metrics=[“accuracy”],
    优化器=RMSprop(learning_rate=LR))

    返回模型
]]></description>
      <guid>https://stackoverflow.com/questions/78142401/can-my-lstm-model-learn-feature-engineering-on-its-own</guid>
      <pubDate>Mon, 11 Mar 2024 17:19:56 GMT</pubDate>
    </item>
    <item>
      <title>用于图像分类的早期融合模型，存在评估准确性的问题</title>
      <link>https://stackoverflow.com/questions/78141978/early-fusion-model-for-image-classification-with-problems-evaluating-accuracy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78141978/early-fusion-model-for-image-classification-with-problems-evaluating-accuracy</guid>
      <pubDate>Mon, 11 Mar 2024 15:58:59 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 中的 Brian2 模拟器实现液体状态机用于分类任务？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78141953/how-to-implement-liquid-state-machine-for-a-classification-task-using-brian2-sim</link>
      <description><![CDATA[我是一名正在攻读本科生的学生，目前正在研究用于分类相关任务的液态机。但是，即使花了几天时间研究 LSM，我也无法找到如何实现它们进行分类的良好开端。
如果没有适当的资源，学习概念也非常困难。那么，有人可以帮助我进行研究吗？]]></description>
      <guid>https://stackoverflow.com/questions/78141953/how-to-implement-liquid-state-machine-for-a-classification-task-using-brian2-sim</guid>
      <pubDate>Mon, 11 Mar 2024 15:53:53 GMT</pubDate>
    </item>
    <item>
      <title>使用稳定基线3进行作业调度[关闭]</title>
      <link>https://stackoverflow.com/questions/78141506/job-scheduling-using-stable-baselines3</link>
      <description><![CDATA[我正在解决一个调度问题，其中有机器、作业和食谱。我在使用稳定基线实现 DQN 时遇到问题。在自定义环境中，我发现 PPO 表现良好，但 DQN 很糟糕。我已经测试过更改超参数。可能是什么问题呢？有什么我错过的吗？
我有一个遵循稳定基线3和健身房（体育馆）界面的自定义环境，但从稳定基线导入的 DQN 模型与 PPO 相比表现非常糟糕。通过将 DQN 和 PPO 与 FIFO（先进先出）和 EDD（最早到期日）等模型进行比较来完成评估。 DQN 的表现甚至比这些传统模型更差。]]></description>
      <guid>https://stackoverflow.com/questions/78141506/job-scheduling-using-stable-baselines3</guid>
      <pubDate>Mon, 11 Mar 2024 14:48:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 Power Automate 将我的 Excel 输入连接到 Azure ML Studio API 端点并进行预测</title>
      <link>https://stackoverflow.com/questions/78140665/using-power-automate-to-connect-my-excel-inputs-to-azure-ml-studio-api-endpoint</link>
      <description><![CDATA[我正在研究如何使用 Power Automate 配置一个流程，该流程可以将我的 Excel 文件输入与我的 Azure ML Studio API 端点连接起来。理想情况下，当添加或修改 Excel 中的某一列值时，将触发此流程。

基于 iris 数据集，我在 Azure ML Studio 中使用 API 端点创建了一个简单的机器学习模型。该模型需要 4 个输入，并根据这些输入返回预测标签。

我创建了一个 Excel 文件，其中有 4 列（sepal_length、sepal_width、petal_length、petal_width），每列都包含一个值，如 Excel 输入屏幕截图中所示。
在 Power Automate 中，我能够使用“列出表中存在的行”操作连接到我的 Excel 文件。在此操作中，我可以找到我的表“Table1”。
我应该在 Power Automate 中采取哪些后续步骤来连接到 API 并能够接收预测结果？
这就是我的流程当前的样子。

因此，我在将输入数据转换为可以作为 API 输入提供的格式时遇到了一些困难。下面的屏幕截图显示了我如何使用 Bearer 令牌和特定 URI 连接到我的 API。
]]></description>
      <guid>https://stackoverflow.com/questions/78140665/using-power-automate-to-connect-my-excel-inputs-to-azure-ml-studio-api-endpoint</guid>
      <pubDate>Mon, 11 Mar 2024 12:36:57 GMT</pubDate>
    </item>
    <item>
      <title>将 Android ML-Kit 鸟类分类器与 Python 结合使用</title>
      <link>https://stackoverflow.com/questions/78139883/using-android-ml-kit-bird-classifier-with-python</link>
      <description><![CDATA[我测试了 ML Kit 中的 Android Vision 快速入门应用程序。正如您在图片中看到的，这里可以进行对象跟踪。现在我正在使用 Python 尝试相同的模型 (bird_classifier.tflite)。这非常有效，但是我如何在这里获取边界框呢？无论我做什么，反馈都是：该模型仅包含一个张量。但为什么它可以在 Android 应用程序中运行呢？
有人可以给我看一个代码示例吗？
测试图片
image = Image.fromarray(screenshot)
image_pred = image.resize((宽度,高度), Image.ANTIALIAS)
结果=classify_image(interpreter, image_pred)
# TrackingID = results[i0][0]
# 分数 = 结果[i0][1]
# Boxes = ?]]></description>
      <guid>https://stackoverflow.com/questions/78139883/using-android-ml-kit-bird-classifier-with-python</guid>
      <pubDate>Mon, 11 Mar 2024 10:19:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 Huggingface MT5 模型中执行批量编码时会得到不同的嵌入？</title>
      <link>https://stackoverflow.com/questions/78139855/why-do-i-get-different-embeddings-when-i-perform-batch-encoding-in-huggingface-m</link>
      <description><![CDATA[我正在尝试使用 HuggingFace 的 mt5-base 模型对一些文本进行编码。我使用的模型如下所示
从转换器导入 MT5EncoderModel、AutoTokenizer

模型 = MT5EncoderModel.from_pretrained(“google/mt5-base”)
tokenizer = AutoTokenizer.from_pretrained(“google/mt5-base”)

def get_t5_embeddings(文本):
    last_hidden_​​state = model(input_ids=tokenizer(texts, return_tensors=“pt”, padding=True).input_ids).last_hidden_​​state
    pooled_sentence = torch.max(last_hidden_​​state, 暗淡=1)
    返回 pooled_sentence[0].detach().numpy()

当我注意到相同的文本与其自身的余弦相似度分数较低时，我正在做一些实验。我做了一些挖掘，意识到如果我批量进行编码，模型会返回非常不同的嵌入。为了验证这一点，我运行了一个小实验，逐步生成 Hello 的嵌入和 10 个 Hello 的列表。并检查列表中 Hello 和第一个 Hello 的嵌入（两者应该相同）。
对于范围 (1, 10) 内的 i：
    print(i, (get_t5_embeddings([“你好”])[0] == get_t5_embeddings([“你好”]*i)[0]).sum())

这将返回嵌入中相互匹配的值的数量。
结果是这样的：
&lt;前&gt;&lt;代码&gt;1 768
2 768
3 768
4 768
5 768
6 768
7 768
8 27
9 27

每次运行它时，如果批量大小超过 768，就会出现不匹配情况。
为什么我会得到不同的嵌入以及如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78139855/why-do-i-get-different-embeddings-when-i-perform-batch-encoding-in-huggingface-m</guid>
      <pubDate>Mon, 11 Mar 2024 10:14:53 GMT</pubDate>
    </item>
    <item>
      <title>持久化模型如何提高准确性？</title>
      <link>https://stackoverflow.com/questions/78127879/how-does-persisting-the-model-increase-accuracy</link>
      <description><![CDATA[导入 pandas 作为 pd
从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.model_selection 导入 train_test_split
从 sklearn.metrics 导入 precision_score, f1_score

Whitewine_data = pd.read_csv(&#39;winequality-white.csv&#39;,
分隔符=&#39;;&#39;)

变量= [&#39;alcohol_cat&#39;, &#39;酒精&#39;, &#39;硫酸盐&#39;, &#39;密度&#39;,
“总二氧化硫”、“柠檬酸”、“挥发性酸度”、
‘氯化物’]

X = Whitewine_data[变量]
y = Whitewine_data[&#39;质量&#39;]
X_train, X_test, y_train, y_test = train_test_split(X, y,
测试大小=0.2）

模型 = DecisionTreeClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
准确度=准确度_得分(y_test, y_pred)
f1 = f1_score(y_test, y_pred, 平均值=&#39;加权&#39;)

预测 = model.predict([[0.27, 0.36, 0.045, 170, 1.001,
0.45, 8.9, 0]])
print(f&#39;预测输出：{预测}&#39;)
print(f&#39;准确率: {准确率 * 100}%&#39;)
print(f&#39;F1 分数: {f1 * 100}% &#39;)

这个初始模型的准确度得分为 57%
================================================== ===============
whitewine_data = pd.read_csv(&#39;winequality-white.csv&#39;,
分隔符=&#39;;&#39;)

# 要从数据集中删除的变量 - 不是输入
变量
变量 = [&#39;固定酸度&#39;,&#39;残留糖&#39;,&#39;游离硫
二氧化硫&#39;, &#39;pH&#39;, &#39;质量&#39;, &#39;isSweet&#39;]

X = Whitewine_data.drop(变量，轴=1)
y = Whitewine_data[&#39;质量&#39;]

X_train, X_test, y_train, y_test = train_test_split(X, y,
测试大小=0.2）

模型 = DecisionTreeClassifier()
model.fit(X_train, y_train)

joblib.dump(模型, &#39;WhiteWine_Quality_Predictor.joblib&#39;)

创建保存的模型
================================================== ===============
whitewine_data = pd.read_csv(&#39;winequality-white.csv&#39;,
分隔符=&#39;;&#39;)

变量 = [&#39;挥发酸度&#39;, &#39;柠檬酸&#39;, &#39;氯化物&#39;,
&#39;二氧化硫总量&#39;、&#39;密度&#39;、&#39;硫酸盐&#39;、&#39;酒精&#39;、
&#39;酒精_猫&#39;]

X_test = Whitewine_data[变量]
y_test = Whitewine_data[&#39;质量&#39;]

模型 = joblib.load(&#39;WhiteWine_Quality_Predictor.joblib&#39;)

y_pred = model.predict(X_test)

f1 = f1_score(y_test, y_pred, 平均值=&#39;加权&#39;)
准确度=准确度_分数（y_test，y_pred）
预测 = model.predict([[0.27, 0.36, 0.045, 170, 1.001,
0.45, 10.9, 3]])

print(f&#39;F1分数: {f1 * 100}%&#39;)
print(f&#39;模型精度: {accuracy * 100}%&#39;)
print(f&#39;预测输出：{预测}&#39;)

调用保存的模型现在的准确率达到 92%
问题：调用已保存的模型如何导致增加
我看到的准确性]]></description>
      <guid>https://stackoverflow.com/questions/78127879/how-does-persisting-the-model-increase-accuracy</guid>
      <pubDate>Fri, 08 Mar 2024 13:07:01 GMT</pubDate>
    </item>
    <item>
      <title>Pytroch 分割模型(.pt) 未转换为 CoreML</title>
      <link>https://stackoverflow.com/questions/78091161/pytroch-segmentation-model-pt-not-converting-to-coreml</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78091161/pytroch-segmentation-model-pt-not-converting-to-coreml</guid>
      <pubDate>Sat, 02 Mar 2024 01:26:48 GMT</pubDate>
    </item>
    <item>
      <title>SageMaker 实验跟踪重复</title>
      <link>https://stackoverflow.com/questions/76821347/sagemaker-experiment-tracking-duplication</link>
      <description><![CDATA[我正在尝试通过 AWS SageMaker 使用脚本模式训练模型。
我想使用 AWS SageMaker Experiments 以及训练作业中的一些计算指标来跟踪此训练作业。当我开始训练作业时，会成功创建一个新的实验运行，该实验运行跟踪所有提供的超参数（例如，nesimators）。
但是，如前所述，此外，我还想跟踪自定义脚本中的其他指标（例如准确性）。在这里，我在拟合模型之前使用 load_run()，然后使用 run.log_metric() 记录指标。但是，当我这样做时，SageMaker 会在 UI 中创建一个新的单独实验条目，这意味着我的超参数和指标单独存储在两个单独的实验运行中：

我希望在一次实验运行中看到所有指标和超参数的组合。我做错了什么？
这是我用来启动训练过程的缩写代码：
&lt;前&gt;&lt;代码&gt;
exp_name = “sklearn-脚本模式-实验”

与运行（
    实验名称=实验名称，
    sagemaker_session=sess,
）运行时：

    sklearn_estimator = SKLearn(&#39;train.py&#39;,
                                    instance_type=&#39;ml.m5.large&#39;,
                                    Framework_version=&#39;1.0-1&#39;,
                                    role=“arn:aws:iam:::role/service-role/AmazonSageMaker-ExecutionRole-”,
                                    超参数={&#39;nestimators&#39;: 100},
                                    环境={“区域”：区域}）

    sklearn_estimator.fit({&#39;train&#39;: f&#39;s3://{BUCKET}/{S3_INPUT_PATH}&#39;})

这是缩写的train.py：
 #在这里解析参数...等等...


    模型 = RandomForestClassifier(n_estimators=args.nesimators,
                                   最大深度=5，
                                   随机状态=1）

    使用 load_run(sagemaker_session=sagemaker_session) 作为运行：

        模型.fit(X, y)

        run.log_metric(name = &quot;最终测试损失&quot;, value = 0.9)
]]></description>
      <guid>https://stackoverflow.com/questions/76821347/sagemaker-experiment-tracking-duplication</guid>
      <pubDate>Wed, 02 Aug 2023 15:20:27 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 模型未进行训练</title>
      <link>https://stackoverflow.com/questions/45359111/pytorch-model-is-not-training</link>
      <description><![CDATA[我有一个问题，一周内都无法解决。我正在尝试构建 CIFAR-10 分类器，但每批之后的损失值都是随机跳跃的，即使在同一批次上，准确性也没有提高（我什至不能用一批来过度拟合模型），所以我猜唯一可能的原因is - 权重没有更新。 
我的模块类
类 Net(nn.Module):
def __init__(自身):
    超级（网络，自我）.__init__()
    self.conv_pool = nn.Sequential(
        nn.Conv2d(3, 64, 3, 填充=1),
        ReLU(),
        nn.MaxPool2d(2, 2),
        nn.Conv2d(64, 128, 3, 填充=1),
        ReLU(),
        nn.MaxPool2d(2, 2),
        nn.Conv2d(128, 256, 3, 填充=1),
        ReLU(),
        nn.MaxPool2d(2, 2),
        nn.Conv2d(256, 512, 3, 填充=1),
        ReLU(),
        nn.MaxPool2d(2, 2),
        nn.Conv2d(512, 512, 1),
        ReLU(),
        nn.MaxPool2d(2, 2))

    self.fcnn = nn.Sequential(
        nn.线性(512, 2048),
        ReLU(),
        nn.线性(2048, 2048),
        ReLU(),
        nn.线性(2048, 10)
    ）

def 前向（自身，x）：
    x = self.conv_pool(x)
    x = x.view(-1, 512)
    x = self.fcnn(x)
    返回x

我正在使用的优化器：

&lt;前&gt;&lt;代码&gt;net = Net()
标准 = nn.CrossEntropyLoss()
优化器 = optim.SGD(net.parameters(), lr=0.001, 动量=0.9)

我的火车功能：
def train():
for epoch in range(5): # 多次循环数据集
    对于范围内的 i（0，df_size）：
        # 获取数据

        尝试：
            图像、标签 = loadBatch(ds, i)
        除了基本异常：
            继续

        ＃ 裹
        输入=变量（图像）

        优化器.zero_grad()

        输出 = 净值（输入）

        损失=标准（输出，变量（标签））

        loss.backward()
        优化器.step()
        acc = 测试（图像，标签）
        print(&quot;损失：&quot; + str(loss.data[0]) + &quot; 准确率 %：&quot; + str(acc) + &quot; 迭代：&quot; + str(i))

        如果我% 40 == 39：
            torch.save(net.state_dict(), &quot;model_save_cifar&quot;)

    print(&quot;完成纪元&quot; + str(纪元))

我使用batch_size = 20，image_size = 32 (CIFAR-10)
loadBatch 函数返回图像的 LongTensor 20x3x32x32 和标签的 LongTensor 20x1 的元组
如果您能帮助我，或者提出可能的解决方案，我会非常高兴（我猜测这是因为 NN 中的顺序模块，但我传递给优化器的参数似乎是正确的）]]></description>
      <guid>https://stackoverflow.com/questions/45359111/pytorch-model-is-not-training</guid>
      <pubDate>Thu, 27 Jul 2017 19:07:27 GMT</pubDate>
    </item>
    <item>
      <title>种子在随机森林中起什么作用？</title>
      <link>https://stackoverflow.com/questions/36307429/what-does-seed-do-in-random-forest</link>
      <description><![CDATA[我知道通常使用种子设置，以便我们可以重现相同的结果。但是，在随机森林部分中设置种子实际上是做什么的。它是否会更改 R 中的 randomForest() 函数的任何参数，例如 nTree 或 sampSize 。 
我每次都为随机森林模型使用不同的种子，但想知道不同的种子如何影响随机森林模型。]]></description>
      <guid>https://stackoverflow.com/questions/36307429/what-does-seed-do-in-random-forest</guid>
      <pubDate>Wed, 30 Mar 2016 11:26:58 GMT</pubDate>
    </item>
    <item>
      <title>randomForest R 包的奇怪结果</title>
      <link>https://stackoverflow.com/questions/27324066/weird-results-with-the-randomforest-r-package</link>
      <description><![CDATA[我有一个包含 10,000 行和两列的数据框、段（具有 32 个值的因子）和目标（具有两个值“是”和“否”的因子，每个值 5,000 个）。我正在尝试使用随机森林来使用分段作为特征对目标进行分类。
训练随机森林分类器后：
&lt;前&gt;&lt;代码&gt;&gt;森林 &lt;- randomForest(目标 ~ 段，数据)

混淆矩阵强烈偏向“否”：
&lt;前&gt;&lt;代码&gt;&gt;打印（森林$混乱）

      否 是 类错误
无 4872 76 0.01535974
是 5033 19 0.99623911

在 10,000 行中，不到 100 行被分类为“是”（即使原始计数为 50/50）。如果我切换标签的名称，我会得到相反的结果：
&lt;前&gt;&lt;代码&gt;&gt; data$target &lt;- as.factor(ifelse(data$target == &#39;是&#39;, &#39;否&#39;, &#39;是&#39;))
&gt;森林 &lt;- randomForest(目标 ~ 段，数据 = 数据)
&gt;打印（森林$混乱）

      否 是 类错误
无 4915 137 0.02711797
是 4810 138 0.97210994

所以这不是一个真正的信号...而且，原始的交叉表是相对平衡的：
&lt;前&gt;&lt;代码&gt;&gt;表（数据$目标，数据$段）
 
         1 10 11 12 13 14 15 16 17 18 19 2 20 21 22 23 24 25 26 27 28 29 3 30 31 32 4 5 6 7 8 9
  否 1074 113 121 86 68 165 210 70 120 127 101 132 90 108 171 122 95 95 76 72 105 71 234 58 83 72 290 162 262 192 64 139
  是 1114 105 136 120 73 201 209 78 130 124 90 145 81 104 155 128 79 85 83 70 93 78 266 70 93 76 291 160 235 194 49 137

看起来 randomForest 采用第一个标签，并且几乎总是为其分配点。澄清一下，数据框是具有更多功能的更大表格的子集 - 我刚刚发现这个特定功能以某种方式导致了这个结果，无论包含多少其他功能。我想知道我是否遗漏了随机森林分类器的一些基本知识，或者是否存在一些编码问题或其他错误导致了这个奇怪的结果。
原始数据集可在此处作为 RDS 获取：
https://www.dropbox.com/s/rjq6lmvd78d6aot /weird_random_forest.RDS?dl=0]]></description>
      <guid>https://stackoverflow.com/questions/27324066/weird-results-with-the-randomforest-r-package</guid>
      <pubDate>Fri, 05 Dec 2014 20:21:16 GMT</pubDate>
    </item>
    </channel>
</rss>