<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 20 Jan 2025 18:22:00 GMT</lastBuildDate>
    <item>
      <title>开发机器学习管道的想法</title>
      <link>https://stackoverflow.com/questions/79372212/ideas-for-developing-a-machine-learning-pipeline</link>
      <description><![CDATA[我目前正在开展一个项目，该项目将融合来自时间序列生物信号和图像的数据，以预测心理评分。
有很多方法可以解决这个问题，我正在寻找有关各种可能性的想法。
其中一个生物信号被认为几乎瞬时与大脑活动相关（想想 EEG，但它不是 EEG），第二个生物信号相当慢，形成时间超过 30 秒（滞后指标）。这些信号将在用户检查照片时收集。我可以量化他们正在看的地方。我还能够在监督环境中处理标记数据。我们可以使用独立测量（不会集成到管道中）来确定此任务的用户心理分数，但这对于监督学习很有用。
对于此应用程序，我无法访问大量用户数据，并且需要依赖管道某些组件的预训练模型。
我相信生物信号中事件的顺序很重要，因此我认为我只能使用 RNN（非常慢）或 transformer。
如果您正在开发这样的系统，您的管道会是什么样子，您会关注哪些主要活动？]]></description>
      <guid>https://stackoverflow.com/questions/79372212/ideas-for-developing-a-machine-learning-pipeline</guid>
      <pubDate>Mon, 20 Jan 2025 17:38:23 GMT</pubDate>
    </item>
    <item>
      <title>寻找一个从一组预定义的单词生成文本的人工智能模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79372199/looking-for-an-ai-model-for-generating-text-from-a-set-of-predefined-words</link>
      <description><![CDATA[有人能推荐一个 AI 模型吗？该模型可以根据一组预定义的单词生成可用于网页的文本。例如，给定单词“热带森林、风、树精”，它应该创建一个 20 个单词的句子，例如：“热带森林在微风中摇曳，神秘的树精向她周围的古树低声诉说秘密。”有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79372199/looking-for-an-ai-model-for-generating-text-from-a-set-of-predefined-words</guid>
      <pubDate>Mon, 20 Jan 2025 17:33:59 GMT</pubDate>
    </item>
    <item>
      <title>个性化通知 [关闭]</title>
      <link>https://stackoverflow.com/questions/79371791/personalized-notification</link>
      <description><![CDATA[您能否为我提供为我的新闻应用制作个性化通知模型的路线图？
我尝试发出通知，但没有成功。如何使用人工智能和机器学习制作这种类型的系统？制作最佳通知以发送给用户设备的最佳流程是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79371791/personalized-notification</guid>
      <pubDate>Mon, 20 Jan 2025 15:19:38 GMT</pubDate>
    </item>
    <item>
      <title>有关于训练集和测试集的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79369405/have-a-problem-about-training-set-and-testing-set</link>
      <description><![CDATA[当我使用交叉验证并打印结果时。测试集的准确率高于测试集。
训练：随机森林分类器准确率为 0.9437
测试：随机森林分类器准确率为 0.9527
]]></description>
      <guid>https://stackoverflow.com/questions/79369405/have-a-problem-about-training-set-and-testing-set</guid>
      <pubDate>Sun, 19 Jan 2025 17:17:05 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“torch”（未知位置）导入名称“Tensor”</title>
      <link>https://stackoverflow.com/questions/79367182/importerror-cannot-import-name-tensor-from-torch-unknown-location</link>
      <description><![CDATA[我尝试从 PyTorch 导入 Tensor：
从 torch 导入 Tensor

但我一直收到此错误：
ImportError：无法从“torch”（未知位置）导入名称“Tensor”

我尝试过的方法：

检查 PyTorch 是否已安装（pip show torch），我使用的是版本 2.5.1。
重新安装 PyTorch：
pip uninstall torch
pip install torch


在 Python shell 中测试了导入，但错误依然存在。

环境：

Python版本：3.10
PyTorch 版本：2.5.1
操作系统：Windows 10
虚拟环境：是

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79367182/importerror-cannot-import-name-tensor-from-torch-unknown-location</guid>
      <pubDate>Sat, 18 Jan 2025 13:04:35 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们不屏蔽 transformer 中除了多头注意力之外的其他层？[关闭]</title>
      <link>https://stackoverflow.com/questions/79366294/why-we-dont-mask-other-layers-besides-the-multihead-attention-in-transformers</link>
      <description><![CDATA[通常在训练 NLP 任务时，我们需要将序列填充到 max_len，以便可以以批处理方式高效处理它们。但是，这些填充的标记不应影响训练（模型参数的更新），因为它们是“虚构的”。
每个人都在谈论掩盖注意力操作的必要性。这是有道理的，因为有效标记不应该关注虚拟标记。但是，我们仍然需要考虑所有其他层（例如 FF 中的线性层、规范化层等）不受填充的影响。
为了简单起见，考虑单个 Transformer-Encoder 层：

从图中可以清楚地看出，FF 和最后的规范化层将填充的标记处理为其他每个标记。为了正确屏蔽所有层，我们应该屏蔽损失吗？
例如，假设我们想使用 Transformer 编码器对序列进行分类。我们传递一个形状为 (B, N, E) 的输入 x，其中 B 是批量大小，N 是最大序列长度，E 是嵌入维度。输出具有相同的形状，我们将使用它进行分类。我们可以通过为每个序列提取一个“全局”向量来实现，然后将其传递给特定于任务的头部，如下所示：
x =coder_layer(x)
x = x.mask_fill(mask, -torch.inf) # 在最大操作期间屏蔽填充。
x = torch.max(x, dim=1)[0] # 忽略索引，简化为形状 (B, E)。
loss = loss_fn(cls_head(x), y)

由于 max 操作对任何未选定元素返回 0 梯度，因此所有参数都不会受到填充的影响。我的理解正确吗？]]></description>
      <guid>https://stackoverflow.com/questions/79366294/why-we-dont-mask-other-layers-besides-the-multihead-attention-in-transformers</guid>
      <pubDate>Fri, 17 Jan 2025 23:05:49 GMT</pubDate>
    </item>
    <item>
      <title>如何利用多个 CPU 进行 YOLO 训练？</title>
      <link>https://stackoverflow.com/questions/79365374/how-to-utilize-multiple-cpus-for-training-of-yolo</link>
      <description><![CDATA[我可以访问一个没有 GPU 的大型 CPU 集群。通过在多个 CPU 节点之间并行化，是否可以加快 YOLO 训练速度？
文档说 device 参数指定用于训练的计算设备：单个 GPU（device=0）、多个 GPU（device=0,1）、CPU（device=cpu）或 Apple 芯片的 MPS（device=mps）。
那么多个 CPU 呢？]]></description>
      <guid>https://stackoverflow.com/questions/79365374/how-to-utilize-multiple-cpus-for-training-of-yolo</guid>
      <pubDate>Fri, 17 Jan 2025 16:05:46 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的神经网络在训练数据上具有很高的准确率，但在测试数据上只有 10％ 的准确率？[关闭]</title>
      <link>https://stackoverflow.com/questions/79350490/why-does-my-neural-network-have-high-accuracy-on-training-data-but-only-10-accu</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79350490/why-does-my-neural-network-have-high-accuracy-on-training-data-but-only-10-accu</guid>
      <pubDate>Sun, 12 Jan 2025 18:42:22 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 nltk 函数</title>
      <link>https://stackoverflow.com/questions/78862426/unable-to-use-nltk-functions</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78862426/unable-to-use-nltk-functions</guid>
      <pubDate>Mon, 12 Aug 2024 15:17:29 GMT</pubDate>
    </item>
    <item>
      <title>在扩展中访问 NetLogo 扩展</title>
      <link>https://stackoverflow.com/questions/78851057/accessing-netlogo-extensions-within-an-extension</link>
      <description><![CDATA[我正在尝试开发一个 NetLogo 扩展来与不同的 LLMS（在线、离线）进行通信。LLM 调用返回 JSON 格式的字符串。我想解析 JSON 并将其转换为嵌套的 TABLE 对象，以访问 NetLogo Table 扩展。
是否有办法让一个扩展访问和使用另一个扩展中的类？]]></description>
      <guid>https://stackoverflow.com/questions/78851057/accessing-netlogo-extensions-within-an-extension</guid>
      <pubDate>Fri, 09 Aug 2024 03:21:02 GMT</pubDate>
    </item>
    <item>
      <title>每个类别至少应该有多少张图像用于训练 YOLO？</title>
      <link>https://stackoverflow.com/questions/55356982/how-many-imagesminimum-should-be-there-in-each-classes-for-training-yolo</link>
      <description><![CDATA[我正在尝试在自定义数据集上实现 YOLOv2。每个类别所需的最低图像数量是多少？]]></description>
      <guid>https://stackoverflow.com/questions/55356982/how-many-imagesminimum-should-be-there-in-each-classes-for-training-yolo</guid>
      <pubDate>Tue, 26 Mar 2019 12:18:21 GMT</pubDate>
    </item>
    <item>
      <title>Catboost：l2_leaf_reg 的合理值是多少？</title>
      <link>https://stackoverflow.com/questions/47728776/catboost-what-are-reasonable-values-for-l2-leaf-reg</link>
      <description><![CDATA[在较大的数据集（约 1M 行，500 列）上运行 catboost，我得到：
训练已停止（迭代 0 时解退化，可能 l2 正则化太小，尝试增加它）。
我如何猜测 l2 正则化值应该是多少？它是否与 y 的平均值、变量数、树深度有关？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/47728776/catboost-what-are-reasonable-values-for-l2-leaf-reg</guid>
      <pubDate>Sat, 09 Dec 2017 12:51:46 GMT</pubDate>
    </item>
    <item>
      <title>神经网络为何如此有效？</title>
      <link>https://stackoverflow.com/questions/38595451/why-do-neural-networks-work-so-well</link>
      <description><![CDATA[我理解使用前向传播和反向传播进行梯度下降训练神经网络的所有计算步骤，但我试图弄清楚为什么它们比逻辑回归效果好得多。
目前我能想到的只有：
A) 神经网络可以学习自己的参数
B) 权重比简单的逻辑回归多得多，因此可以进行更复杂的假设
有人能解释一下为什么神经网络总体上效果这么好吗？我是一个相对初学者。]]></description>
      <guid>https://stackoverflow.com/questions/38595451/why-do-neural-networks-work-so-well</guid>
      <pubDate>Tue, 26 Jul 2016 16:38:31 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Google 云中设置 TensorFlow？</title>
      <link>https://stackoverflow.com/questions/37231866/how-do-i-set-up-tensorflow-in-the-google-cloud</link>
      <description><![CDATA[如何在 Google 云中设置 TensorFlow？我了解如何创建 Google Compute Engine 实例，以及如何在本地运行 TensorFlow；最近的 Google 博客文章建议应该有一种方法可以在云中创建 Google Compute Engine 实例并运行 TensorFlow 应用程序：

机器学习项目可以有多种规模，正如我们在开源产品 TensorFlow 中看到的那样，项目通常需要扩大规模。一些小任务最好使用在桌面上运行的本地解决方案来处理，而大型应用程序则需要托管解决方案的规模和可靠性。 Google Cloud Machine Learning 旨在支持全方位，并提供从本地到云环境的无缝过渡。

即使我对此有一点了解，但考虑到微软 Azure 等竞争平台所提供的功能，肯定有一种方法可以在 Google Cloud 中设置 TensorFlow 应用程序（本地开发并“无缝”扩展到云中，大概使用 GPU）。
例如，我想在 IDE 中本地工作，调整项目的功能和代码，在那里运行有限的训练和验证，并定期将代码推送到云中，在那里使用（任意）更多的资源运行训练，然后保存和下载训练好的模型。或者更好的是，只需使用可调资源在云中运行图表（或图表的一部分）。
有没有办法做到这一点；有计划吗？如何在 Google Cloud 中设置 TensorFlow？]]></description>
      <guid>https://stackoverflow.com/questions/37231866/how-do-i-set-up-tensorflow-in-the-google-cloud</guid>
      <pubDate>Sat, 14 May 2016 21:05:28 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中，批量越大计算时间越短吗？</title>
      <link>https://stackoverflow.com/questions/35158365/will-larger-batch-size-make-computation-time-less-in-machine-learning</link>
      <description><![CDATA[我正在尝试调整超参数，即 CNN 中的批量大小。我有一台 i7 核的电脑，RAM 为 12GB，我正在使用 CIFAR-10 数据集训练 CNN 网络，该数据集可在此博客中找到。现在，首先我阅读并了解了机器学习中的批量大小：

首先假设我们正在进行在线学习，即我们使用的迷你批量大小为 1。对在线学习的明显担忧是，使用仅包含单个训练示例的迷你批量将导致我们对梯度的估计出现重大错误。
但事实上，错误并不是一个问题。原因是单个梯度估计不需要非常准确。我们需要的只是一个足够准确的估计，使我们的成本函数趋于不断下降。这就像你试图到达磁北极，但每次看的时候，你的指南针都会偏离 10-20 度。只要你经常停下来检查指南针，并且指南针平均能正确指向方向，你最终就能到达磁北极。
基于这个论点，听起来我们应该使用在线学习。事实上，情况比这更复杂。我们知道，我们可以使用矩阵技术同时计算小批量中所有示例的梯度更新，而不是循环遍历它们。根据我们的硬件和线性代数库的细节，这可以使计算（例如）大小为 100 的小批量的梯度估计的速度快得多，而不是通过分别循环 100 个训练示例来计算小批量梯度估计。它可能只需要（比如说）50 倍的时间，而不是 100 倍的时间。现在，乍一看，这似乎对我们没有太大帮助。
对于大小为 100 的小批量，权重的学习规则如下所示：
其中总和是小批量中的训练示例。这与在线学习相比。
即使进行小批量更新只需要 50 倍的时间，进行在线学习似乎仍然更好，因为我们会更频繁地更新。但是，假设在
小批量情况下，我们将学习率提高 100 倍，那么
更新规则将变为
这很像以
学习率为 η 执行单独的在线学习实例。但它只需要执行单个在线学习实例的 50 倍时间。不过，使用较大的 minibatch 似乎完全有可能加快速度。


现在我尝试使用 MNIST 数字数据集，并运行一个示例程序，首先设置批处理大小 1。我记下了完整数据集所需的训练时间。然后我增加了批处理大小，我注意到它变得更快了。

但是，如果使用此代码和github 链接进行训练，更改批量大小不会减少训练时间。如果我使用 30 或 128 或 64，它保持不变。他们说他们获得了 92% 的准确率。经过两三个时期后，他们的准确率已经超过 40%。但是，当我在计算机上运行代码时，除了批量大小之外没有做任何改变，我在 10 个时期后得到了更糟糕的结果，只有 28%，并且测试准确率在接下来的时期内一直停留在那里。然后我想，因为他们使用了 128 的批量大小，需要使用那个。然后我用了同样的方法，但它变得更糟，在 10 个时期后只给出 11% 并且卡在那里。为什么会这样？]]></description>
      <guid>https://stackoverflow.com/questions/35158365/will-larger-batch-size-make-computation-time-less-in-machine-learning</guid>
      <pubDate>Tue, 02 Feb 2016 16:12:17 GMT</pubDate>
    </item>
    </channel>
</rss>