<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 14 Jun 2024 03:17:06 GMT</lastBuildDate>
    <item>
      <title>我如何将所有 csv 文件数据绘制到一张图表中并以可解释的方式表示它？</title>
      <link>https://stackoverflow.com/questions/78620038/how-can-i-plot-all-csv-files-data-into-one-graph-and-represent-it-in-such-a-way</link>
      <description><![CDATA[我使用一些传感器收集了时间序列数据。数据集包含两个类别的 60 个样本，每个类别有 30 个样本。每个样本有 50 行和 11 列，标签以注释方式完成，即样本的文件名是样本数据的标签。现在，我想以一种应该表示与时间相关的数据的方式来可视化数据。 （例如 x 轴上的时间和 y 轴上的传感器值）。
这是来自数据集的样本图像（样本图像 1）（样本图像 2）
这是我的数据集的链接：https://drive.google.com/drive/folders/1aRIR5ei3Gr0RdS8QXM6hrqPQ2cJdRyEp
我还提供了代码，我曾尝试将其可视化，但帮助不大。
提供的代码未提供所需的输出。输出图像之一是：

此图是通过更改 go.Scatter() 中的 x=df.columns、y=df.iloc[0] 的值生成的
我想生成一个图表，其中 x 轴上有 50 个点（50 行）作为时间点，y 轴上绘制有 11 列数据。
任何帮助都将不胜感激！谢谢 &amp;问候
代码：
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import plotly.offline as pyo
import os

# 获取目录中的 csv 文件列表
PATH = &quot;E:\\Sankalp\\Practice_Stuff\\DummyData\\&quot;
fileNames = os.listdir(PATH)
fileNames = [file for file in fileNames if &#39;.csv&#39; in file]

# 创建图形
fig = go.Figure()
x_axis_values = list(range(50))

# 循环遍历每个 csv 文件并向图形添加轨迹
for file in fileNames:
if file.startswith(&#39;bye bye&#39;): 
df = pd.read_csv(os.path.join(PATH, file),header=None)
fig.add_trace(go.Scatter(x=x_axis_values, y=df.iloc[:,:], name=file, mode=&#39;lines+markers&#39;,line=dict(color=&quot;#2efd70&quot;)))
elif file.startswith(&#39;welcome&#39;):
df = pd.read_csv(os.path.join(PATH, file),header=None)
fig.add_trace(go.Scatter(x=x_axis_values, y=df.iloc[:,:], name=file, mode=&#39;lines+markers&#39;,line=dict(color=&quot;#ff0000&quot;)))

# 显示图表
fig.update_layout(title=&#39;\&#39;Welcome\&#39;&#39; 的趋势图, xaxis_title=&#39;Time&#39;, yaxis_title=&#39;Values&#39;)
fig.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78620038/how-can-i-plot-all-csv-files-data-into-one-graph-and-represent-it-in-such-a-way</guid>
      <pubDate>Thu, 13 Jun 2024 20:25:49 GMT</pubDate>
    </item>
    <item>
      <title>在不同的 Python 环境中训练的相同 XGBClassifier 模型得出的预测结果明显不同</title>
      <link>https://stackoverflow.com/questions/78619966/the-same-xgbclassifier-model-trained-in-different-python-environment-made-notice</link>
      <description><![CDATA[我尝试将在旧的 Python 环境中训练的 XGBClassifier 模型转移到新的环境中。
以下是新旧环境中关键软件包的版本信息。
旧环境

python=3.6.0
scikit-learn==0.22.2.post1
xgboost==0.90
pickleshare==0.7.5
numpy==1.18.1

新环境

python=3.11.9
scikit-learn==1.4.2
xgboost==2.0.3
pickleshare==0.7.5
numpy==1.26.4

在新旧环境中分别使用同一组超参数和相同的数据，预测的概率明显不同。
我还注意到，拟合管道对象的大小以及训练模型所需的时间发生了显着变化。
拟合管道对象的大小旧 vs. 新： 30 MB vs. 7 MB
训练时间旧 vs. 新： 4:38:46 vs. 0:06:40
对于我在旧环境中训练的模型和新环境中训练的模型之间的差异，您有什么看法吗？
提前谢谢您！我非常感谢您的帮助！
以下是我用来训练模型的关键 Python 代码。
def create_pipeline(model_params, cat_indices):
&quot;&quot;&quot;
创建管道
:param model_params：管道中 XGBoost 分类器的模型参数
:param cat_indices：X 中分类特征的索引
&quot;&quot;&quot;

cat_transformer = Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;missing&#39;)),
(&#39;one_hot_encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))])

preprocessor = ColumnTransformer(
transformers=[(&#39;cat&#39;, cat_transformer, cat_indices)],
remainder=&#39;passthrough&#39;)

xgb = XGBClassifier(objective=&quot;binary:logistic&quot;, eval_metric=&quot;auc&quot;, missing=np.nan, use_label_encoder=False)
xgb.set_params(**model_params)

full_pipeline_model = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor),
(&#39;model&#39;, xgb)])
return full_pipeline_model

model_params = {
&#39;n_estimators&#39;: 500,
&#39;alpha&#39;: 9.73974803929248e-06,
&#39;gamma&#39;: 19,
&#39;lambda&#39;: 0.557185777864069,
&#39;learning_rate&#39;: 0.029438952461179668,
&#39;max_depth&#39;: 13,
&#39;scale_pos_weight&#39;: 5,
&#39;subsample&#39;: 0.687206238714661
}

cat_indices = [X.columns.get_loc(col) for col in cat_cols]

fitted_pipeline = create_pipeline(model_params, cat_indices).fit(X.values, y.values)
pickle.dump(fitted_pipeline, open(&quot;fitted_pipeline_final1.pkl&quot;, &quot;wb&quot;))


我预计从两个模型获得的预测概率非常相似，因为我使用了相同的超参数集和相同的数据。预测概率明显不同的原因可能是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78619966/the-same-xgbclassifier-model-trained-in-different-python-environment-made-notice</guid>
      <pubDate>Thu, 13 Jun 2024 20:07:18 GMT</pubDate>
    </item>
    <item>
      <title>分析 mediapipe 训练姿势的最佳方法是什么，以查看模型是否训练正确</title>
      <link>https://stackoverflow.com/questions/78619772/what-is-the-best-way-to-analyze-mediapipe-trained-poses-to-see-if-the-models-wer</link>
      <description><![CDATA[正如您从问题中读到的那样，我正在寻找分析训练过的鼻子模型的最佳方法。
比如，是否有某种方法可以查看训练过的模型的骨架模式，mediapipe 可以识别训练过的姿势
我尝试打印 npy 文件的值，但我只得到一个非常大的矩阵]]></description>
      <guid>https://stackoverflow.com/questions/78619772/what-is-the-best-way-to-analyze-mediapipe-trained-poses-to-see-if-the-models-wer</guid>
      <pubDate>Thu, 13 Jun 2024 19:14:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 S3 存储桶中的训练数据训练 YOLOv8？</title>
      <link>https://stackoverflow.com/questions/78619753/how-to-train-yolov8-with-traingin-data-in-s3-bucket</link>
      <description><![CDATA[似乎 model.train 需要 data.yml 文件的路径，并且该文件需要有训练和验证集的路径。s3 引用似乎不是实际路径，我看到人们使用数据生成器使用 S3 进行训练。有人知道如何使用 YOLOv8 做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/78619753/how-to-train-yolov8-with-traingin-data-in-s3-bucket</guid>
      <pubDate>Thu, 13 Jun 2024 19:08:12 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 datumaro 合并 comment.xml 和视频以获取仅具有标记图像的 yolo 数据集？</title>
      <link>https://stackoverflow.com/questions/78619747/how-can-i-use-datumaro-to-merge-the-annotations-xml-video-to-get-a-yolo-datase</link>
      <description><![CDATA[我是机器学习领域的新手，我刚刚发现这些
datum project import --format cvat -n cvat1 comments.xml
datum project import --format video_frames -n vid1 video.mp4

我发现https://github.com/cvat-ai/cvat/issues/1251这个
datum project export -e &#39;/item/annotation&#39; --filter-mode &#39;i+a&#39; -f --save-images &lt; your_target_format &gt; --

但我不知道如何实现这一点

https://openvinotoolkit.github.io/datumaro/latest/docs/data-formats/formats/yolo_ultralytics.html

为什么

cvat 在 docker 中运行，比主机 (macOS) 慢
cvat 导出帧很慢，每次导出图像都需要准备所有帧，即使是一点点标签更改
我希望我可以使用 annotations.xml+video/frames 来获得更快的导出
我希望我可以使用 jpg 而不是 png - MP4 视频，jpg 较好，png 很大。
]]></description>
      <guid>https://stackoverflow.com/questions/78619747/how-can-i-use-datumaro-to-merge-the-annotations-xml-video-to-get-a-yolo-datase</guid>
      <pubDate>Thu, 13 Jun 2024 19:07:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 detector2 区分灰度图像中的两种颜色并掩盖它们？</title>
      <link>https://stackoverflow.com/questions/78619402/how-would-you-use-detectron2-to-distinguish-between-two-colors-in-a-grayscale-im</link>
      <description><![CDATA[我刚开始使用detectron2，我计划将它用于一个项目。该项目包括使用该模型区分灰度图像中的对象。该图像由形状奇怪的灰色单元格组成，而其余空间为黑色。我的任务是使用该模型并描绘出灰色单元格的形状。
示例图像：
单元格的灰度图像
我曾尝试使用预先存在的模型来解决这个问题，但它们无法识别出物体的存在。解决这个问题的最佳方法是什么？
此外，我愿意使用不同的机器学习模型。我只是想找到一种区分灰色和黑色的方法。
提前非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/78619402/how-would-you-use-detectron2-to-distinguish-between-two-colors-in-a-grayscale-im</guid>
      <pubDate>Thu, 13 Jun 2024 17:36:00 GMT</pubDate>
    </item>
    <item>
      <title>机器学习视觉模型以 95% 的准确率预测新数据相同的标签</title>
      <link>https://stackoverflow.com/questions/78619195/machine-learning-visual-model-with-95-accuracy-predicts-new-data-the-same-label</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78619195/machine-learning-visual-model-with-95-accuracy-predicts-new-data-the-same-label</guid>
      <pubDate>Thu, 13 Jun 2024 16:45:35 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：X 有 2 个特征，但 StandardScaler 需要 3 个特征作为输入</title>
      <link>https://stackoverflow.com/questions/78619143/valueerror-x-has-2-features-but-standardscaler-is-expecting-3-features-as-inpu</link>
      <description><![CDATA[我的代码中出现此错误“
ValueError：X 有 2 个特征，但 StandardScaler 需要 3 个特征作为输入。”
我正在使用神经网络训练我的模型以预测 RSC（雷达截面）的幅度。]]></description>
      <guid>https://stackoverflow.com/questions/78619143/valueerror-x-has-2-features-but-standardscaler-is-expecting-3-features-as-inpu</guid>
      <pubDate>Thu, 13 Jun 2024 16:31:11 GMT</pubDate>
    </item>
    <item>
      <title>不一致的否定 [关闭]</title>
      <link>https://stackoverflow.com/questions/78619130/inconsistent-nos</link>
      <description><![CDATA[`
#表示发现样本 [154,53] 的编号不一致
#这是来自 kaggle 的糖尿病预测模型
x_train,x_test,y_train_test=train_test_split(x, y, test_size=0.2`,random_state=42)在此处输入图片描述
我尝试更改随机状态值，但仍然出现相同的错误，如果有人知道如何解决它。请在 3 天内尽快完成。]]></description>
      <guid>https://stackoverflow.com/questions/78619130/inconsistent-nos</guid>
      <pubDate>Thu, 13 Jun 2024 16:28:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Kotlin 上的移动应用程序中使用 imshow()？</title>
      <link>https://stackoverflow.com/questions/78617059/how-to-use-imshow-in-mobile-app-on-kotlin</link>
      <description><![CDATA[我决定创建一个检测物体的计算机视觉模型。它从相机获取实时图像并显示检测到的物体的矩形。但我不知道如何可视化 OpenCV imshow() 函数。
我的模型使用 Yolov8，我尝试将其转换为 tflite，但 Tensorflow 已更新，因此目前无法实现。因此，在移动应用上部署模型的一种方法是将结果发送到屏幕上。但我不知道 Kotlin，对我来说这太难了]]></description>
      <guid>https://stackoverflow.com/questions/78617059/how-to-use-imshow-in-mobile-app-on-kotlin</guid>
      <pubDate>Thu, 13 Jun 2024 09:38:51 GMT</pubDate>
    </item>
    <item>
      <title>如何按照物体在最顶层的顺序检测和识别它们，然后对它们进行分层并为它们分配 ID？</title>
      <link>https://stackoverflow.com/questions/78605533/how-to-detect-and-identify-objects-in-the-order-that-they-are-on-top-then-layer</link>
      <description><![CDATA[
我尝试过过滤掉它们的线条，现在该如何确定哪个物体被隐藏了
我也尝试过使用 yoloV8 来过滤物体，但仍然无法确定哪个物体被另一个物体遮挡了，有人能帮我吗？
CODE:
import cv2
import numpy as np

# 加载图像 + mask、灰度、高斯模糊、Otsu 阈值
image = cv2.imread(&quot;./anhtest/11.png&quot;) # 这是原始图像
original = image.copy()
mask = cv2.imread(&quot;./anhtest/11.png&quot;) # 这是从 U-2-Net 生成的掩码
gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
bg_removed = cv2.bitwise_and(image, image, mask=thresh)

# HSV 颜色阈值
hsv = cv2.cvtColor(bg_removed, cv2.COLOR_BGR2HSV)
lower = np.array([0, 0, 0])
upper = np.array([179, 33, 255])
hsv_mask = cv2.inRange(hsv, lower, upper)
isolated = cv2.bitwise_and(bg_removed, bg_removed, mask=hsv_mask)
isolated = cv2.cvtColor(isolated, cv2.COLOR_BGR2GRAY)
isolated = cv2.threshold(isolated, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# 变形操作以去除小伪影和噪音
open_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))
opening = cv2.morphologyEx(isolated, cv2.MORPH_OPEN, open_kernel, iterations=1)
close_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))
close = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, close_kernel, iterations=1)

# 查找轮廓并按最大轮廓面积排序
cnts = cv2.findContours(close, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
cnts = cnts[0] if len(cnts) == 2 else cnts[1]
cnts = sorted(cnts, key=cv2.contourArea, reverse=True)
for c in cnts:
cv2.drawContours(original, [c], -1, (36,255,12), 3)
break

cv2.imshow(&quot;bg_removed&quot;, bg_removed)
cv2.imshow(&quot;hsv_mask&quot;, hsv_mask)
cv2.imshow(&#39;isolated&#39;,isolated)
cv2.imshow(&#39;original&#39;,original)
cv2.waitKey()
]]></description>
      <guid>https://stackoverflow.com/questions/78605533/how-to-detect-and-identify-objects-in-the-order-that-they-are-on-top-then-layer</guid>
      <pubDate>Tue, 11 Jun 2024 05:23:24 GMT</pubDate>
    </item>
    <item>
      <title>搜索具有相似文本的文档</title>
      <link>https://stackoverflow.com/questions/78599128/search-for-documents-with-similar-texts</link>
      <description><![CDATA[我有一个包含三个属性的文档：标签、位置和文本。
目前，我正在使用 LangChain/pgvector/embeddings 对它们全部进行索引。
我得到了满意的结果，但我想知道是否有更好的方法，因为我想查找一个或多个具有特定标签和位置的文档，但文本可能会有很大差异，但含义仍然相同。出于这个原因，我考虑使用嵌入/向量数据库。
这是否也是使用 RAG（检索增强生成）来“教”的一个例子LLM 不知道的一些常见缩写？
import pandas as pd

from langchain_core.documents import Document
from langchain_postgres import PGVector
from langchain_postgres.vectorstores import PGVector
from langchain_openai.embeddings import OpenAIEmbeddings

connection = &quot;postgresql+psycopg://langchain:langchain@localhost:5432/langchain&quot;
embeddings = OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)
collection_name = &quot;notas_v0&quot;

vectorstore = PGVector(
embeddings=embeddings,
collection_name=collection_name,
connection=connection,
use_jsonb=True,
)

# 开始索引

# df = pd.read_csv(&quot;notes.csv&quot;)
# df = df.dropna() # .head(10000)
# df[&quot;tags&quot;] = df[&quot;tags&quot;].apply(
# lambda x: [tag.strip() for tag in x.split(&quot;,&quot;) if tag.strip()]
# )

# long_texts = df[&quot;Texto Longo&quot;].tolist()
# wc = df[&quot;Centro Trabalho Responsável&quot;].tolist()
# notes = df[&quot;Nota&quot;].tolist()
# tags = df[&quot;tags&quot;].tolist()

# documents = list(
# map(
# lambda x: Document(
# page_content=x[0], metadata={&quot;wc&quot;: x[1], &quot;note&quot;: x[2], &quot;tags&quot;: x[3]}
# ),
# zip(long_texts, wc, notes, tags),
# )
# )

# print(
# [
# vectorstore.add_documents(documents=documents[i : i + 100])
# for i in range(0, len(documents), 100)
# ]
# )
# print(&quot;Done.&quot;)

### END INDEX

### BEGIN QUERY

result = vectorstore.similarity_search_with_relevance_scores(
&quot;EVTD202301222707&quot;,
filter={&quot;note&quot;: {&quot;$in&quot;: [&quot;15310116&quot;]}, &quot;tags&quot;: {&quot;$in&quot;: [&quot;abcd&quot;, &quot;xyz&quot;]}},
k=10, # 结果限制
)

### END QUERY
]]></description>
      <guid>https://stackoverflow.com/questions/78599128/search-for-documents-with-similar-texts</guid>
      <pubDate>Sun, 09 Jun 2024 16:40:32 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中手动对某一层的输出进行去量化，并为下一层进行重新量化？</title>
      <link>https://stackoverflow.com/questions/78239906/how-to-manually-dequantize-the-output-of-a-layer-and-requantize-it-for-the-next</link>
      <description><![CDATA[我正在做一个学校项目，需要我对模型的每一层进行手动量化。具体来说，我想手动实现：

量化激活，结合量化权重 A - 层 A -
量化输出 - 去量化输出 - 重新量化输出，结合量化权重 B - 层 B - ...

我知道 Pytorch 已经有一个量化函数，但该函数仅限于 int8。我想执行从 bit = 16 到 bit = 2 的量化，然后比较它们的准确性。
我遇到的问题是，量化后，层的输出大了几个量级（bit = 16），我不知道如何将其去量化。我正在使用激活和权重的相同最小值和最大值执行量化。因此，这里有一个例子：
激活 = [1,2,3,4]
权重 = [5,6,7,8]
激活和权重的最小值和最大值 = 1, 8
预期的非量化输出 = 70

使用位量化 = 16
量化激活 = [-32768, -23406, -14044, -4681]
量化权重 = [4681, 14043, 23405, 32767]
量化输出 = -964159613
使用最小值 = 1、最大值 = 8 反量化输出 = -102980

这个计算对我来说很有意义，因为输出涉及激活和权重的乘积，它们的幅度增加也相乘。如果我使用原始的最小值和最大值执行一次反量化，则输出会大得多，这是合理的。
Pytorch 如何处理反量化？我试图找到 Pytorch 的量化，但找不到它。如何对输出进行反量化？]]></description>
      <guid>https://stackoverflow.com/questions/78239906/how-to-manually-dequantize-the-output-of-a-layer-and-requantize-it-for-the-next</guid>
      <pubDate>Thu, 28 Mar 2024 17:17:53 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 将特征重要性作为列表而不是绘图</title>
      <link>https://stackoverflow.com/questions/63060367/xgboost-get-feature-importance-as-a-list-of-columns-instead-of-plot</link>
      <description><![CDATA[我想知道是否可以将特征重要性作为列表而不是图表来获取。这就是我所拥有的
xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)
import matplotlib.pyplot as plt

xgb.plot_importance(xg_reg)
plt.rcParams[&#39;figure.figsize&#39;] = [5,5]
plt.show()

这给了我这个图

我想只获取主要特征的列表，因为我有超过 800 个不同的特征。]]></description>
      <guid>https://stackoverflow.com/questions/63060367/xgboost-get-feature-importance-as-a-list-of-columns-instead-of-plot</guid>
      <pubDate>Thu, 23 Jul 2020 17:57:31 GMT</pubDate>
    </item>
    <item>
      <title>如何在 python 中使用 opencv 计算车辆数量？</title>
      <link>https://stackoverflow.com/questions/58628229/how-to-count-vehicles-using-opencv-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/58628229/how-to-count-vehicles-using-opencv-in-python</guid>
      <pubDate>Wed, 30 Oct 2019 15:00:11 GMT</pubDate>
    </item>
    </channel>
</rss>