<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 05 Jan 2024 18:17:01 GMT</lastBuildDate>
    <item>
      <title>为什么 cartpole 奖励不收敛</title>
      <link>https://stackoverflow.com/questions/77766359/why-cartpole-reward-is-not-converge</link>
      <description><![CDATA[通过此图像，训练损失和期望值随着时间的推移而收敛，但每个情节的回报没有收敛，即使是伟大的情节。
这是我的训练循环代码：
对于范围内的剧集（500）：
    状态，信息 = env.reset()
    状态 = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
    总奖励 = 0

    对于 count() 中的 t：
        env.render()
        状态 = torch.FloatTensor(状态).to(设备)
        动作 = agent.selectAction(状态,agent.learn_step_counter)
        观察、奖励、完成、_ = env.step(action.item())[:4]

        总奖励+=奖励
        奖励 = torch.tensor([奖励], 设备=设备)
        next_state = torch.tensor(观察, dtype=torch.float32, device=device).unsqueeze(0)
        
        replay_buffer.push（状态，动作，next_state，奖励）
        
        状态 = 下一个状态
        
        代理内存 = replay_buffer
        代理.learn()
        如果完成：
            休息

在此处输入图像描述
有什么方法可以帮助看到收敛吗？]]></description>
      <guid>https://stackoverflow.com/questions/77766359/why-cartpole-reward-is-not-converge</guid>
      <pubDate>Fri, 05 Jan 2024 17:51:33 GMT</pubDate>
    </item>
    <item>
      <title>Python 中级，有志于 ML。朋友建议使用 Java/C++ 进行内存管理，并使用 Web 开发来获得更好的初学者机会。寻求建议[已关闭]</title>
      <link>https://stackoverflow.com/questions/77766281/intermediate-in-python-aspiring-for-ml-friend-suggests-java-c-for-memory-man</link>
      <description><![CDATA[我的 Python 水平处于中级，希望在机器学习 (ML) 领域发展职业生涯。然而，我的一位精通 Web 开发的朋友建议我不要坚持使用 Python。相反，他们建议学习其他语言，如 Java 或 C++，因为它们可以提供对内存管理的更深入的理解。此外，他们建议我考虑转向网络开发，并为初学者提供了更多的范围和机会。对于此事的任何指导或见解，我将不胜感激。
我探索了这两个领域的机会，但发现自己对 ML 比 Web 开发更感兴趣。]]></description>
      <guid>https://stackoverflow.com/questions/77766281/intermediate-in-python-aspiring-for-ml-friend-suggests-java-c-for-memory-man</guid>
      <pubDate>Fri, 05 Jan 2024 17:33:45 GMT</pubDate>
    </item>
    <item>
      <title>AWS ElasticBean CodePipeline 部署一次又一次失败。我缺少什么？</title>
      <link>https://stackoverflow.com/questions/77766259/aws-elasticbean-codepipeline-deployment-failed-again-and-again-what-am-i-missi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77766259/aws-elasticbean-codepipeline-deployment-failed-again-and-again-what-am-i-missi</guid>
      <pubDate>Fri, 05 Jan 2024 17:28:46 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关对多标签 NER 数据集进行分层以实现平衡训练/测试拆分的建议 [已关闭]</title>
      <link>https://stackoverflow.com/questions/77765015/seeking-advice-on-stratifying-a-multi-label-ner-dataset-for-balanced-train-test</link>
      <description><![CDATA[我正在开展一个命名实体识别 (NER) 项目，我们拥有广泛的数据集，专为医疗保健量身定制。我们的数据集包含大约 3,000 个不同的疾病实体（实体类型），每个实体都用开头 (B-Tag) 和内部 (I-Tag) 标签的唯一编号进行注释。
以下是我们在 Huggingface 框架模型格式中使用的数据格式示例：
&lt;前&gt;&lt;代码&gt;{
    “ner_tags”：[1,2,2,4,5,0,7,8,8],
    “标记”：[“首席”、“主诉”、“：”、“糖尿病”、“糖尿病”、“和”、“慢性”、“肾脏”、“疾病” ;]
}

在此示例中，句子中的每个标记都与表示其实体类型的 NER 标记相关联。
我面临的挑战是如何正确地将这个数据集分为训练集、验证集和测试集。鉴于不同实体的数量众多，至关重要的是每组都代表整个疾病范围。这对于确保我们正在开发的 NER 模型经过良好训练并能够有效地泛化未知数据至关重要。
我正在寻求以下方面的建议：

分割大型多实体 NER 数据集的有效策略。
确保所有实体在每个子集中得到充分代表（训练、验证、测试）。
在 NER 中处理大量实体类型时的任何具体注意事项或技术。

我对能够处理数据集复杂性、确保所有实体的平衡和全面表示的方法特别感兴趣。]]></description>
      <guid>https://stackoverflow.com/questions/77765015/seeking-advice-on-stratifying-a-multi-label-ner-dataset-for-balanced-train-test</guid>
      <pubDate>Fri, 05 Jan 2024 13:47:15 GMT</pubDate>
    </item>
    <item>
      <title>我应该在代码中添加什么或者代码有什么问题？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77764698/what-should-i-add-to-the-code-or-what-is-wrong-with-the-code</link>
      <description><![CDATA[&lt;前&gt;&lt;代码&gt;

    导入操作系统


os.environ[&#39;LOKY_MAX_CPU_COUNT&#39;] = &#39;6&#39;
导入 pandas**强文本** 作为

从 sklearn.preprocessing 导入 StandardScaler


从 sklearn.neighbors 导入 KNeighborsClassifier


从 sklearn.metrics 导入 precision_score


从 sklearn.impute 导入 SimpleImputer
从 sklearn.metrics 导入分类报告


egitim_data = pd.read_excel(r&#39;C:\Users\memo3\OneDrive\Masaüstü\ZSCOREEGITIMDATA.xlsx&#39;)
test_data = pd.read_excel(r&#39;C:\Users\memo3\OneDrive\Masaüstü\ZSCORETESTDATA.xlsx&#39;)


print(&quot;训练数据中的 NaN 值：&quot;)
打印（egitim_data.isnull（）。sum（））


print(&quot;\n测试数据中的 NaN 值：&quot;)
打印（test_data.isnull（）。sum（））


X_train = egitim_data.drop(&#39;标签&#39;, axis=1)
y_train = egitim_data[&#39;标签&#39;]

X_test = test_data.drop(&#39;标签&#39;, axis=1)
y_test = test_data[&#39;标签&#39;]


imputer = SimpleImputer(策略=&#39;均值&#39;)

X_train_scaled = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)


X_test_scaled = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)


定标器=标准定标器()
X_train_scaled = 缩放器.fit_transform(X_train_scaled)
X_test_scaled = 缩放器.transform(X_test_scaled)


knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train_scaled, y_train)


y_pred_test = knn_model.predict(X_test_scaled)


准确度测试 = 准确度分数(y_test, y_pred_test)
print(“测试数据的模型准确度：”, precision_test)



分类代表=分类报告（y_test，y_pred_test，zero_division = 1）
print(&quot;分类报告：\n&quot;,classification_rep)


我这样编辑了代码。首先，我有7200条数据，我将它们分为90％的训练数据和10％的测试数据，并创建了两个excel文件。我在 Excel 中使用 Z 分数标准化编辑了这两个数据文件。后来，当我编辑代码时，它给出了以下输出。不过，不应该是46%，而且仍然有错误或缺失，但我找不到它。
&lt;前&gt;&lt;代码&gt;
训练数据中的 NaN 值：
流动持续时间 0
转发 IAT 分钟 0
Bwd IAT 最小值 0
转发 IAT 混合 0
Bwd IAT 最大 0
转发 IAT 平均值 0
Bwd IAT 平均值 0
流量包/秒 0
流字节/秒 0
流量 IAT 最小值 0
流量 IAT 最大 0
流量 IAT 平均值 0
流量 IAT 标准 0
活跃分钟 0
主动平均值 0
活跃最大 0
主动标准 0
空闲分钟 0
空闲平均值 0
空闲最大 0
空闲标准 0
标签0
数据类型：int64

测试数据中的 NaN 值：
流动持续时间 0
转发 IAT 分钟 0
Bwd IAT 最小值 0
转发 IAT 混合 0
Bwd IAT 最大 0
转发 IAT 平均值 0
Bwd IAT 平均值 0
流量包/秒 0
流字节/秒 0
流量 IAT 最小值 0
流量 IAT 最大 0
流量 IAT 平均值 0
流量 IAT 标准 0
活跃分钟 0
主动平均值 0
活跃最大 0
主动标准 0
空闲分钟 0
空闲平均值 0
空闲最大 0
空闲标准 0
标签0
数据类型：int64
测试数据上的模型精度：0.46111111111111114
分类报告：
                  精确召回率 f1-score 支持

音频流 0.61 0.42 0.50 90
       浏览 0.31 0.32 0.31 90
           聊天 0.28 0.34 0.31 90
  文件传输 0.60 0.71 0.65 90
           邮寄 0.55 0.39 0.45 90
            P2P 0.36 0.06 0.10 90
视频流 0.28 0.59 0.38 90
           网络电话 0.96 0.86 0.91 90

       准确度 0.46720
      宏观平均 0.49 0.46 0.45 720
   加权平均 0.49 0.46 0.45 720


进程已完成，退出代码为 0












]]></description>
      <guid>https://stackoverflow.com/questions/77764698/what-should-i-add-to-the-code-or-what-is-wrong-with-the-code</guid>
      <pubDate>Fri, 05 Jan 2024 12:46:17 GMT</pubDate>
    </item>
    <item>
      <title>IOPub 数据速率超过 Jupyter Notebook [重复]</title>
      <link>https://stackoverflow.com/questions/77763798/iopub-data-rate-exceeded-jupyter-notebook</link>
      <description><![CDATA[`IOPub` 数据速率超出。

‘Jupyter’服务器将暂时停止发送输出
给客户端以避免崩溃。
要更改此限制，请设置配置变量
`--ServerApp.iopub_data_rate_limit`。

当前值：
ServerApp.iopub_data_rate_limit=1000000.0（字节/秒）
ServerApp.rate_limit_window=3.0（秒）

我尝试更新整个 Jupyter 笔记本，但仍然没有解决问题，我还尝试根据我的数据集更新值，但仍然显示同样的错误。]]></description>
      <guid>https://stackoverflow.com/questions/77763798/iopub-data-rate-exceeded-jupyter-notebook</guid>
      <pubDate>Fri, 05 Jan 2024 10:04:06 GMT</pubDate>
    </item>
    <item>
      <title>实时跟踪脚本中 BYTETracker 初始化的问题</title>
      <link>https://stackoverflow.com/questions/77763541/issue-with-bytetracker-initialization-in-live-tracking-script</link>
      <description><![CDATA[尝试使用 ByteTrack 库初始化实时跟踪脚本中的 BYTETracker 时，我遇到了 TypeError。该错误具体发生在 BYTETracker 类的 __init__ 方法中。
这是我的代码的相关部分：
跟踪器 = [BYTETracker(ByteTrackArgument), BYTETracker(ByteTrackArgument), BYTETracker(ByteTrackArgument)]

我遇到的错误消息是：
TypeError：+ 不支持的操作数类型：“type”和“float”

我尝试通过创建一个 ByteTrackArgument 实例来解决这个问题，如下所示：
跟踪器 = [BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())]

但是，问题仍然存在。值得注意的是，我在脚本中使用 OpenCV 中的 FaceDetectorYN 进行人脸检测。
对于可能导致此错误的原因（尤其是与使用 FaceDetectorYN 结合使用）以及如何解决该错误有任何见解吗？
其他上下文：

我正在使用 ByteTrack 库进行实时跟踪。 链接
错误发生在 BYTETracker 类的 __init__ 方法中。
我将 ByteTrackArgument 的实例传递给 BYTETracker 构造函数。
使用 OpenCV 中的 FaceDetectorYN 执行人脸检测。 链接

这是我的完整代码：
&lt;前&gt;&lt;代码&gt;导入cv2
从 bytetracker 导入 BYTETracker
将 numpy 导入为 np

print(&quot;OpenCV 版本&quot;, cv2.__version__)

类 ByteTrackArgument：
    轨迹阈值 = 0.5
    轨道缓冲区 = 50
    匹配阈值 = 0.8
    纵横比阈值 = 10.0
    最小框面积 = 1.0
    mot20 = 假

MIN_THRESHOLD = 0.5 # 根据需要调整此阈值

# 初始化 ByteTrackArgument
byte_track_argument = ByteTrackArgument()

# 初始化BYTETracker
跟踪器 = [BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())]
def start_webcam_tracking():
    cap = cv2.VideoCapture(0) # 使用 0 作为默认网络摄像头，或提供网络摄像头 URL

    如果不是 cap.isOpened():
        print(“错误：无法打开相机。”)
        返回

    而真实：
        ret, 框架 = cap.read()
        如果不转：
            print(“错误：无法从相机读取帧。”)
            休息

        # 人脸检测代码
        检测器 = cv2.FaceDetectorYN.create(r&quot;C:\Users\gratu\live tracker\face_detection_yunet_2023mar.onnx&quot;, &quot;&quot;, (2200, 1200), Score_threshold=MIN_THRESHOLD)
        img_W = int(frame.shape[1])
        img_H = int(frame.shape[0])
        detector.setInputSize((img_W, img_H))

        检测= detector.detect(frame)[1]

        如果检测不是无：
            用于检测中的检测：
                x, y, 宽度, 高度 = 地图(int, 检测[:4])
                cv2.矩形(框架, (x, y), (x + 宽度, y + 高度), (0, 255, 0), 2)

                # 使用面部边界框更新跟踪器
                tracker.update(np.array([[x, y, x + 宽度, y + 高度]]), [frame.shape[0],frame.shape[1]])

        # 从 BYTETracker 获取跟踪结果
        online_targets = tracker.get_online_targets()

        如果 online_targets 不是 None：
            对于 online_targets 中的目标：
                x, y, x2, y2 = target # 根据BYTETracker的输出格式修改这部分
                cv2.矩形(框架, (x, y), (x2, y2), (255, 0, 0), 2)

        cv2.imshow(&#39;具有人脸检测和跟踪功能的网络摄像头&#39;, frame)

        如果 cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
            休息

    cap.release()
    cv2.destroyAllWindows()

如果 __name__ == “__main__”：
    start_webcam_tracking()

这是我尝试复制的教程，他们使用 yolox 进行人物检测，我尝试对人脸检测做同样的事情
链接]]></description>
      <guid>https://stackoverflow.com/questions/77763541/issue-with-bytetracker-initialization-in-live-tracking-script</guid>
      <pubDate>Fri, 05 Jan 2024 09:10:23 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 正则化 Alpha - 权重还是叶子？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77763321/lightgbm-regularization-alpha-weights-or-leaves</link>
      <description><![CDATA[我已经阅读了有关 XGBoost 和 LightGBM 的论文以及它们的大部分文档，但无法找到明确的声明表明除了 GOSS &amp; EFB用于更快的学习，基本算法与XGBoost相同。
具体来说，XGBoost 目标函数中唯一的 L1 式正则化是 gamma*T，其中 gamma 是超参数，T 是叶子数量。 LightGBM 有 reg_alpha 参数，根据他们的文档，该参数应用 L1 正则化，该参数是否会惩罚叶子的数量，或者，在更传统的意义上，惩罚每个叶子的贡献的绝对值？
附注参考回归案例，我从未使用过该模型进行分类，因此不知道哪些部分仍然有效。]]></description>
      <guid>https://stackoverflow.com/questions/77763321/lightgbm-regularization-alpha-weights-or-leaves</guid>
      <pubDate>Fri, 05 Jan 2024 08:25:18 GMT</pubDate>
    </item>
    <item>
      <title>我的 tfidf 向量自动编码器对于不同的文本输入产生相同的输出</title>
      <link>https://stackoverflow.com/questions/77762883/my-autoencoder-for-tfidf-vectors-is-yielding-the-same-output-for-different-text</link>
      <description><![CDATA[我一直在尝试实现一个自动编码器来完成降维任务，输入到最近邻模型中，我意识到所有邻居最终彼此之间的距离为零，我意识到问题源于自动编码器，这对于不同的文本产生了类似的输出。
我已经尝试更改层数、正则化器以及输出层的激活（我使用线性，因为这应该是文本数据的最佳选择）。
我的 desc 和 req 数据帧的形状分别为 (31080, 471494) (31080, 169214)，大部分是稀疏数据，这是我在初始原始文本数据上使用 tfidf 获得的。
类 SparseDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, X, 批量大小):
        自我.X = X
        self.batch_size = 批量大小
        self.n_samples = X.shape[0]

    def __len__(自身):
        返回 int(np.ceil(self.n_samples / self.batch_size))

    def __getitem__(self, idx):
        开始 = idx * self.batch_size
        结束=分钟（开始+ self.batch_size，self.n_samples）
        batch_X = self.X[开始:结束].toarray()
        return batch_X, batch_X # 自动编码器获取与输入和目标相同的数据


组合_vecs_sparse = hstack([df_desc.sparse.to_coo(), df_req.sparse.to_coo()])


# 定义提前停止回调
Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 3，restore_best_weights = True）

# 分割数据
X_train, X_val = train_test_split(combined_vecs_sparse, test_size=0.2, random_state=42)
X_train = X_train.tocsr()
X_val = X_val.tocsr()
before_memory = psutil.Process().memory_info().rss
input_dim = X_train.shape[1] # 特征数量

# 定义自动编码器结构
input_layer = 输入（形状=（input_dim，））

编码=密集（128，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（input_layer）
编码=密集（64，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（编码）
编码 = Dense(32,activation=&#39;relu&#39;,kernel_regularizer=l2(0.001))(encoded) # 编码表示
解码=密集（64，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（编码）
解码=密集（128，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（解码）
解码=密集（input_dim，激活=&#39;sigmoid&#39;）（解码）

自动编码器=模型（输入层，解码）

# 编译并训练自动编码器
autoencoder.compile（优化器=&#39;adam&#39;，损失=&#39;mse&#39;）


如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77762883/my-autoencoder-for-tfidf-vectors-is-yielding-the-same-output-for-different-text</guid>
      <pubDate>Fri, 05 Jan 2024 06:41:35 GMT</pubDate>
    </item>
    <item>
      <title>无论输入图像如何，具有 TensorFlow Lite 模型的 Flask API 始终预测相同的类别</title>
      <link>https://stackoverflow.com/questions/77762697/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</link>
      <description><![CDATA[我正在开发 Flask API，以使用 TensorFlow Lite 模型执行推理，该模型是在阿尔茨海默病 5 类图像数据集上训练的，这些图像是 [“AD - 阿尔茨海默病”、“CN - 认知正常”、“EMCI - 早期轻度”认知障碍”、“LMCI - 晚期轻度认知障碍”、“MCI - 轻度认知障碍”]。
该模型在我的训练环境中运行良好，但当我将其部署到 Flask API 中时，出现了问题。 API 一致地为每张图像预测相同的类别（“MCI - 轻度认知障碍”），而在我的 Colab 笔记本中训练的模型则准确地预测各种类别。该 API 稍后将与 React Native App 集成。
使用不同的数据集训练模型两次，但问题仍然存在。我现在已经走进了死胡同，不知道如何解决它。
TFLite 模型代码：
https://colab.research.google.com/drive/1xxW8v5ZBKLvlGrofL2fBy9WYk_Fn5Dj_?usp=分享
FlaskAPI 代码：
fromflask导入Flask，request，jsonify
将张量流导入为 tf
导入CV2
将 numpy 导入为 np
从 PIL 导入图像
导入io

应用程序=烧瓶（__名称__）

解释器 = tf.lite.Interpreter(model_path=“latest_model.tflite”)
解释器.allocate_tensors()

class_names = [“CN-认知正常”、“AD-阿尔茨海默病”、“EMCI-早期轻度认知障碍”、“MCI-轻度认知障碍”、“LMCI-晚期轻度认知障碍”]

def preprocess_image(图像):
    图像 = cv2.resize(图像, (150, 150))
    图像 = image.astype(&#39;float32&#39;) / 255.0
    图像 = np.expand_dims(图像, 轴=0)
    返回图像


@app.route(&#39;/predict&#39;,methods=[&#39;POST&#39;])
def 预测（）：
    尝试：
        文件 = request.files[&#39;文件&#39;]
        image_file = Image.open(io.BytesIO(file.read()))
        图像 = cv2.cvtColor(np.array(image_file), cv2.COLOR_RGB2BGR)

    
        如果不是（image.shape[0] &gt;= 150 且 image.shape[1] &gt;= 150 且 image.shape[2] == 3）：
        return jsonify({&quot;error&quot;: &quot;无效的图像形状&quot;})


        图像 = image.astype(&#39;float32&#39;) / 255.0

        预处理图像 = 预处理图像（图像）

        terpreter.set_tensor(interpreter.get_input_details()[0][&#39;index&#39;], preprocessed_image)
        解释器.invoke()

        output_tensor =terpreter.get_tensor(interpreter.get_output_details()[0][&#39;index&#39;])
        Predicted_class_index = np.argmax(output_tensor, axis=1)[0]
        预测类名称 = 类名称[预测类索引]

        结果 = {“预测”：预测类名称，“输出张量”：output_tensor.tolist()}
        返回 jsonify(结果)
    除了异常 e：
       返回 jsonify({“错误”: str(e)})

如果 __name__ == &#39;__main__&#39;:
应用程序运行（调试=真）

尝试记录输出张量，但这是我从 Flask API 获得的输出。知道输出张量表明偏向于 MCI 类，但为什么它在 colab 环境中完美运行，而不是在 Flask API 中运行良好（如果是这样的话）？
此外，除了使用 Flask API 来将模型与我的 React Native 应用程序集成之外，您还建议我使用其他更好的方法吗？
&lt;前&gt;&lt;代码&gt;{

“输出张量”：[

[

0.0004518234636634588,

0.0004140451201237738,

0.002781340153887868,

0.7277416586875916,

0.2686111330986023

]

],

“预测”：“MCI-轻度认知障碍”

}
]]></description>
      <guid>https://stackoverflow.com/questions/77762697/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</guid>
      <pubDate>Fri, 05 Jan 2024 05:43:52 GMT</pubDate>
    </item>
    <item>
      <title>尽管成本函数收敛到 0，但什么可能导致火车精度下降 [关闭]</title>
      <link>https://stackoverflow.com/questions/77762381/what-could-be-causing-the-decline-in-train-accuracy-despite-the-cost-function-co</link>
      <description><![CDATA[我正在开展一个旨在识别绘画风格的项目。我的模型是提取绘画特征的 Inception 和用于测量绘画风格之间相似度的 Siamese 网络的组合。
但是，我在训练过程中遇到了两个问题。首先，成本函数似乎收敛于 0，其次，训练精度随着时间的推移而下降。
首先，我尝试将批量大小从 1 增加到 5。
其次，我尝试打乱训练数据集，因为模型是根据绘画风格是否匹配来交替学习的。
第三，我尝试将学习率从 0.1 降低到 0.01。
尽管做出了这些努力，问题仍然存在。
我正在寻求深入了解为什么会发生这些问题。

为了让模型能够在匹配样式和非匹配样式之间交替学习，我编写了以下代码。

对于范围 (0,80) 内的 i：
  对于范围 (0,20) 内的 j：
    对于范围 (0,2) 内的 k：
      TRAINING_image.append(图像[200*j+i+k])
      TRAINING_label.append(标签[200*j+i+k])

对于范围 (80, 100) 内的 i：
  对于范围 (0,20) 内的 j：
    对于范围 (0,2) 内的 k：
      TEST_image.append(图像[200*j+i+k])
      TEST_label.append(标签[200*j+i+k])


以下代码用于模型训练。这段代码中的模型指的是异常模型。

for i, (_image1, _label1) in enumerate(train_loader):
    优化器.zero_grad()

    image1 = _image1.to(设备)
    标签1 = _标签1[0]
    矢量1_张量 = 模型(图像1)

    if (i == 0): # 异常情况
      标签2 = 标签1
      矢量2_张量 = 矢量1_张量

    目标向量 = [标签 1 == 标签 2]
    target_tensor = torch.tensor(target_vector).float()
    目标张量 = 目标张量.to(设备)

    成本，预测=损失（向量1_张量，向量2_张量，标签1 ==标签2，阈值）
    成本.向后()
    优化器.step()

    如果预测==（标签1==标签2）：
      正确预测 += 1

    #张量重用以减少计算
    标签2 = 标签1
    矢量2_张量=矢量1_张量.detach()


以下代码是关于损失函数的。我使用Inception模型的avg pool的输出张量进行欧几里德距离测量。

类 ContrastiveLoss(nn.Module):
    def __init__(自身，边距)：
        super(ContrastiveLoss, self).__init__()
        self.margin = 保证金

    defforward（自身，输出1，输出2，标签，阈值）：
        euclidean_distance = nn.function.pairwise_distance(output1, output2, keepdim = True)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                      (标签) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        预测 = euclidean_distance.item() &lt;临界点

        返回loss_对比，预测
]]></description>
      <guid>https://stackoverflow.com/questions/77762381/what-could-be-causing-the-decline-in-train-accuracy-despite-the-cost-function-co</guid>
      <pubDate>Fri, 05 Jan 2024 03:37:00 GMT</pubDate>
    </item>
    <item>
      <title>当我在 jupyter 笔记本上运行简单的 cnn 模型时，CPU 使用率较低</title>
      <link>https://stackoverflow.com/questions/77730719/low-cpu-usage-when-i-run-a-simple-cnn-model-on-jupyter-notebook</link>
      <description><![CDATA[我在 Jupyter 笔记本上运行了一个非常简单的 CNN 模型，但过程非常慢。我在我的旧笔记本电脑（核心 i7U 10gen）上运行相同的程序。只花了一分半钟，但在我的新笔记本电脑（酷睿 i9 13900hx 和 rtx4060）上花了 30 分钟！它们都是在 CPU 上运行的，但在我的旧电脑上，CPU 使用率为 100%，在我的新电脑上，大约为 20%。然后，我在 PyCharm 中运行相同的程序，一切正常！这让我很困惑，我尝试了很多方法但都不起作用。我想知道真正的问题出在哪里？我的 Jupyter 笔记本还是其他东西？
我尝试在不同的 PC、不同的 IDE 平台上运行相同的程序。我想知道真正的问题出在哪里。]]></description>
      <guid>https://stackoverflow.com/questions/77730719/low-cpu-usage-when-i-run-a-simple-cnn-model-on-jupyter-notebook</guid>
      <pubDate>Fri, 29 Dec 2023 07:23:45 GMT</pubDate>
    </item>
    <item>
      <title>如何修复 KerasTensor 传递给 TF API 时出现的错误？</title>
      <link>https://stackoverflow.com/questions/71808327/how-to-fix-error-where-a-kerastensor-is-passed-to-a-tf-api</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/71808327/how-to-fix-error-where-a-kerastensor-is-passed-to-a-tf-api</guid>
      <pubDate>Sat, 09 Apr 2022 13:03:40 GMT</pubDate>
    </item>
    <item>
      <title>如何在 BigQuery 中使用外部回归器训练 Arima_PLUS 模型？</title>
      <link>https://stackoverflow.com/questions/67624116/how-to-use-external-regressors-for-training-arima-plus-model-in-bigquery</link>
      <description><![CDATA[我在大查询上创建了一个模型，是否可以包含额外的列作为外部回归量？
例如，我想包含日期、用户、每个会话的页面、跳出率等以预测用户。
创建或替换模型 bqml_tutorial.create_model
选项
(model_type=&#39;ARIMA_PLUS&#39;,
time_series_timestamp_col=&#39;日期&#39;,
time_series_data_col=&#39;用户&#39;,
auto_arima=真，
数据频率 = &#39;自动频率&#39;,
decompose_time_series=真）
作为
从“bqml_tutorial.cvrate”中选择日期、简历作为用户 ORDER BY Date
]]></description>
      <guid>https://stackoverflow.com/questions/67624116/how-to-use-external-regressors-for-training-arima-plus-model-in-bigquery</guid>
      <pubDate>Thu, 20 May 2021 16:07:01 GMT</pubDate>
    </item>
    <item>
      <title>用于机器学习算法的 csv 流</title>
      <link>https://stackoverflow.com/questions/44240145/csv-stream-for-machine-learning-algorithms</link>
      <description><![CDATA[我有一个很大的 CSV 文件（大约 5GB）。
我试图逐行读取整个文件，并尝试应用最典型的算法（SVM、朴素贝叶斯、线性回归等）。
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd
导入 csv

i_f = open(&#39;top2Mmm.csv&#39;, &#39;r&#39; )
reader = csv.reader( i_f, 分隔符 = &#39;;&#39; )
对于读卡器中的行：
print(“斐乐 -&gt;”, 行)

我刚刚成功阅读了 CSV，但我不知道如何获取每一行并构建模型。
我从一个较小的文件开始以加快该过程，但我不知道如何使该过程正常工作。
有什么线索或提示吗？]]></description>
      <guid>https://stackoverflow.com/questions/44240145/csv-stream-for-machine-learning-algorithms</guid>
      <pubDate>Mon, 29 May 2017 10:23:16 GMT</pubDate>
    </item>
    </channel>
</rss>