<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 06 Feb 2024 00:58:07 GMT</lastBuildDate>
    <item>
      <title>我尝试安装Installing TensorFlow, CUDA, cuDNN with Anaconda for GeForce GTX 1050 [关闭]</title>
      <link>https://stackoverflow.com/questions/77944645/i-tried-installing-installing-tensorflow-cuda-cudnn-with-anaconda-for-geforce</link>
      <description><![CDATA[我尝试为 GeForce GTX 1050 安装“使用 Anaconda 安装 TensorFlow、CUDA、cuDNN”。
我使用了以下提到的文章：https://medium.com/@shaikhmuhammad/installing-tensorflow-cuda-cudnn-with-anaconda-for-geforce-gtx-1050-ti-79c1eb94eb7a
我与文章中遵循的步骤的唯一区别是：

我有 nvidia 1050 而不是 1050ti
我在 c/users/lokes 中设置了我的环境，而不是在桌面上

我收到了这个错误，我已将其附加在屏幕截图中，但我无法找到解决该问题的方法。]]></description>
      <guid>https://stackoverflow.com/questions/77944645/i-tried-installing-installing-tensorflow-cuda-cudnn-with-anaconda-for-geforce</guid>
      <pubDate>Tue, 06 Feb 2024 00:08:48 GMT</pubDate>
    </item>
    <item>
      <title>如何校准 opensearch 异常检测 [关闭]</title>
      <link>https://stackoverflow.com/questions/77943830/how-to-calibrate-opensearch-anomaly-detection</link>
      <description><![CDATA[我正在尝试校准 OpenSearch 异常检测，但有几个问题。
我的大部分流程都以秒为单位记录：COUNT 1 - Sale:February 5, 2024 @ 15:50:49.000。有些时段我们没有任何记录，一般周末的流量很低。
在下面的两个配置中，我遇到了在周末开始（周六和周日）和周一开始时发出警报的问题。因为周六和周日流量下降很多，周一又增加，恢复正常流量。
配置1：

探测器间隔：10m，
窗口延迟：1m，
木瓦尺寸：8。

配置2：

探测器间隔：1m，
窗口延迟：1m，
木瓦尺寸：8。

这种类型的变体可接受的配置是什么？

]]></description>
      <guid>https://stackoverflow.com/questions/77943830/how-to-calibrate-opensearch-anomaly-detection</guid>
      <pubDate>Mon, 05 Feb 2024 20:34:27 GMT</pubDate>
    </item>
    <item>
      <title>我想制作一个职业建议模型[关闭]</title>
      <link>https://stackoverflow.com/questions/77943423/i-want-to-make-a-career-suggestion-model</link>
      <description><![CDATA[有一个数据集，其中包含职位名称和描述。当一个人输入他的技能时，我需要输出他应该做什么类别的工作。我已经使用余弦相似度创建了它。（如果你能告诉我更好的方法，那也会有帮助）现在我需要提供建议，以提高他的技能。
如果他输入统计和Python技能。模型应该可以说是学习数据科学。或者学习数据分析。
你能给我一个关于如何做到这一点的建议吗？非常感谢这种方法。
（发展法学硕士不是一种选择）
我被困在这里了。非常感谢任何帮助]]></description>
      <guid>https://stackoverflow.com/questions/77943423/i-want-to-make-a-career-suggestion-model</guid>
      <pubDate>Mon, 05 Feb 2024 19:11:21 GMT</pubDate>
    </item>
    <item>
      <title>如何训练 Tortoise TTS 模型将英语视频配音为 URDU 语言 [关闭]</title>
      <link>https://stackoverflow.com/questions/77942946/how-to-train-tortoise-tts-model-for-dubing-english-video-to-urdu-language</link>
      <description><![CDATA[我正在做AI配音平台。我想将英语语音视频配音为乌尔都语语音。我已经实现了以下许多模块

从视频中提取音频（完成）
从音频中提取文本（完成）
将英语文本转换为乌尔都语文本（完成）

现在，我想使用这个URDU文本（从输入中提取和转换，如上所述[1,2,3]）和参考英语音频（从TTS 模型中的视频广告在 [1]) 中进行了讨论。
我想要的是 --&gt;该 TTS 模型从参考音频中提取特征和韵律样本，并说出具有完美语音克隆的乌尔都语文本。]]></description>
      <guid>https://stackoverflow.com/questions/77942946/how-to-train-tortoise-tts-model-for-dubing-english-video-to-urdu-language</guid>
      <pubDate>Mon, 05 Feb 2024 17:38:45 GMT</pubDate>
    </item>
    <item>
      <title>InvalidParameterError：GridSearchCV 的“评分”参数必须是其中的 str</title>
      <link>https://stackoverflow.com/questions/77942581/invalidparametererror-the-scoring-parameter-of-gridsearchcv-must-be-a-str-amo</link>
      <description><![CDATA[当我尝试将训练数据集拟合到 GridsearchCV 时，它指出评分必须是以下各项中的一个：
{&#39;jaccard_samples&#39;, &#39; precision_macro&#39;, &#39;balanced_accuracy&#39;, ...&lt;允许类的长列表，请参阅编辑历史记录&gt;...},
一个可调用对象、“list”的实例、“tuple”的实例、“dict”的实例或 None。
而是得到了{&#39;精确度&#39;、&#39;召回率&#39;、&#39;准确度&#39;、&#39;f1&#39;}。”

尽管我的评分列表是：“{&#39; precision&#39;,&#39;f1&#39;,&#39;recall&#39;,&#39;accuracy&#39;}”。
我在 VS Code 中使用 Jupyter 内核扩展运行代码。我尝试将其更改为仅对“召回”进行评分，并且它有效，但是我希望有多个评分变量。]]></description>
      <guid>https://stackoverflow.com/questions/77942581/invalidparametererror-the-scoring-parameter-of-gridsearchcv-must-be-a-str-amo</guid>
      <pubDate>Mon, 05 Feb 2024 16:42:58 GMT</pubDate>
    </item>
    <item>
      <title>Keras 中的多输出 ResNet 模型：损失字典和训练问题</title>
      <link>https://stackoverflow.com/questions/77942574/multi-output-resnet-model-in-keras-issue-with-loss-dictionary-and-training</link>
      <description><![CDATA[我正在使用 Keras 和 ResNet50 进行多输出分类任务。该数据集由面部图像组成，每张图像都有四个相关标签，分别代表无聊、投入、困惑和沮丧的强度，每个标签的范围从 1 到 4。

框架：图像的绝对路径。
无聊：无聊的强度（1 到 4）。
参与度：参与度（1 到 4）。
混乱：混乱的强度（1 到 4）。
挫败感：挫败感的强度（1 到 4）。

我想创建一个基于 ResNet 的模型，具有多个输出来同时预测这四个标签。
从tensorflow导入keras
从tensorflow.keras导入输入
从tensorflow.keras.applications导入ResNet50
从tensorflow.keras导入层、模型

基础模型 = ResNet50(input_shape=(128, 128, 3),
                      include_top=假，
                      权重=&#39;imagenet&#39;）

模型 = models.Sequential([
    基本模型，
    图层.GlobalAveragePooling2D(),
    层.Dense(64, 激活=&#39;relu&#39;),
    层.Dense（4，激活=&#39;softmax&#39;，名称=&#39;无聊&#39;），
    层.Dense（4，激活=&#39;softmax&#39;，名称=&#39;参与&#39;），
    层.Dense(4, 激活=&#39;softmax&#39;, name=&#39;Confusion&#39;),
    层.Dense（4，激活=&#39;softmax&#39;，名称=&#39;挫败感&#39;）
]）


model.compile(优化器=&#39;亚当&#39;,
              损失={&#39;无聊&#39;: &#39;categorical_crossentropy&#39;,
                    &#39;参与度&#39;: &#39;categorical_crossentropy&#39;,
                    &#39;混乱&#39;: &#39;categorical_crossentropy&#39;,
                    &#39;挫败感&#39;: &#39;categorical_crossentropy&#39;},
              指标=[&#39;准确性&#39;])
模型.summary()

&lt;前&gt;&lt;代码&gt;错误：
ValueError：发现与任何模型输出都不对应的意外损失或指标：dict_keys([&#39;Boredom&#39;, &#39;Engagement&#39;, &#39;Confusion&#39;])。有效模式输出名称：[&#39;Frustration&#39;]。收到的结构是：{&#39;Boredom&#39;：&#39;categorical_crossentropy&#39;，&#39;Engagement&#39;：&#39;categorical_crossentropy&#39;，&#39;Confusion&#39;：&#39;categorical_crossentropy&#39;}

我真的不知道出了什么问题，名字是匹配的]]></description>
      <guid>https://stackoverflow.com/questions/77942574/multi-output-resnet-model-in-keras-issue-with-loss-dictionary-and-training</guid>
      <pubDate>Mon, 05 Feb 2024 16:41:58 GMT</pubDate>
    </item>
    <item>
      <title>coco注释数据集的滑动窗口[关闭]</title>
      <link>https://stackoverflow.com/questions/77941194/sliding-window-for-coco-annotated-dataset</link>
      <description><![CDATA[我正在尝试使用大图像（20.000px）数据集训练模型，以检测异常。
由于缺陷可能非常小，所以我使用滑动窗口方法将图像切割成 448x488px 的块（使用 SAHI python 包）。
我遇到以下问题：
每个断层都用多边形注释。但正如您所看到的，多边形比断层本身要大一些。所以现在滑动窗口（红色）与多边形（蓝色）重叠。这意味着该窗口被分类为有故障，而实际上它是正确的。

现在这会影响准确性、召回率和精确度分数。我想过在每个窗口上使用最小面积注释阈值，但这是不可能的，因为有些错误非常小。
还尝试使注释更加精确，这导致召回率提高了 20%。
对于这种情况有好的做法吗？]]></description>
      <guid>https://stackoverflow.com/questions/77941194/sliding-window-for-coco-annotated-dataset</guid>
      <pubDate>Mon, 05 Feb 2024 13:08:31 GMT</pubDate>
    </item>
    <item>
      <title>如果在多类分类中删除相关嵌入，f1 分数会降低而 AUC 会增加吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77940963/can-f1-score-decrease-and-auc-increase-if-correlated-embeddings-are-removed-in-m</link>
      <description><![CDATA[我在 2 个场景中使用手套嵌入构建了一个神经网络模型。
场景一：
我使用手套嵌入在具有 3725 个唯一标记的 3 个类别的平衡数据集上构建了一个神经网络模型。我正在执行多类分类。
我得到的训练和测试样本 F1 分数分别为 93.2% 和 74.78%。
AUC得分分别为0.815和0.782。
我有 3725 个令牌，其中 25 个令牌在嵌入之间具有相关性 &gt; 0.95。
在场景 B 中，我删除了这些高度相关的标记。
场景 - B：
我再次使用手套嵌入在 3 个类的平衡数据集上构建了一个神经网络模型，这一次只有 3705 个唯一标记。
我得到的训练和测试样本 F1 分数分别为 74.2% 和 60.8%。
AUC得分分别为0.905和0.794。
即从场景 A 到场景 B，我的 f1 分数正在下降，但 AUC 正在增加。
问题：
当减小嵌入矩阵的大小时，文本分类模型中的 F1 分数是否会降低，但 AUC 会增加？
可能是什么原因？]]></description>
      <guid>https://stackoverflow.com/questions/77940963/can-f1-score-decrease-and-auc-increase-if-correlated-embeddings-are-removed-in-m</guid>
      <pubDate>Mon, 05 Feb 2024 12:32:57 GMT</pubDate>
    </item>
    <item>
      <title>预处理新数据以从 PyCaret 中的现有模型进行预测[关闭]</title>
      <link>https://stackoverflow.com/questions/77938501/preprocessing-new-data-for-predictions-from-an-existing-model-in-pycaret</link>
      <description><![CDATA[摘要
我试图使用 Python 的 PyCaret 库开发一个 ML 模型来分析具有 34 个特征（列）的数据集。我运行 setup() 函数并注意到，由于编码，它将原始数据集扩展至 58 个特征。它还进行了插补和转换。经过这些步骤后，该库将基础数据分为训练集和测试集。
最终根据训练数据集从compare_models()中选择合适的模型。由此，我在测试集上使用 Predict_model() 进行了预测，并对结果感到满意。
我的提供商现在如何向我提供新数据，我想针对新数据运行 Predict_model() ，以便我可以将这些预测返回给我的提供商。为此，我使用了 Predict_model() 函数的“data”参数，但是，我得到的错误低于以下最小可行代码示例。
这个问题旨在了解如何确保我收到的新数据经过适当的预处理，以供在 Predict_model() 的“数据”参数中使用，或者，如果我的整个概念不准确，那么适当的方法应该是什么来满足最终的要求目标是根据开发的原始模型对我收到的新数据进行预测。
带注释的最小可行代码示例
&lt;前&gt;&lt;代码&gt;# 库
导入 pycaret
从 pycaret.regression 导入 *
将 pandas 导入为 pd

# 通过从 CSV 导入创建一个新的 DF
# 这个 DF 有 34 列
baseDf = pd.read_csv(“wave1data.csv”)

# 在 baseDf 上进行设置
# 转换/编码的 DF 结果有 58 列，并在训练/测试数据集之间进行分割
s = setup(baseDf, target = “我的目标功能”, session_id = 64)

# 基本模型对比
最好=比较模型（）

# 预测测试分割
wave1_pred = 预测模型（最佳）

# 此时，我有一个理想的模型，但它基于具有 58 列和多个转换/插补的 DF
# 我尝试使用以下方法对全新的未见过的数据进行预测：
wave2Df = pd.read_csv(“wave2data.csv”)
wave2_pred = Predict_model（最佳，数据= wave2Df）

抛出错误
&lt;块引用&gt;
------------------------------------------------------------ ---------------------------- KeyError Traceback（最近调用
最后）在&lt;细胞系：2&gt;（）
1 # 预测第 2 波
----&gt; 2wave2_pred=predict_model(最佳，数据=wave2Df)
5帧
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py 中
_raise_if_missing(self, key, 索引器, axis_name) 6131 6132 not_found =
列表(ensure_index(key)[missing_mask.nonzero()[0]].unique())
-&gt;第6133章 6134 第6135章
KeyError：“[&#39;Endorsed By&#39;] 不在索引中”

（如果我理解正确的话，wave2Df 数据与“最佳”数据的结构不同，这是有道理的。）
汇总查询

如何克服这些错误并使用我根据新传入数据（在本例中为 wave2Df 的内容）生成的“最佳”模型来运行 Predict_model()？
我是否应该采取完全不同的方式来实现相同的目标？
]]></description>
      <guid>https://stackoverflow.com/questions/77938501/preprocessing-new-data-for-predictions-from-an-existing-model-in-pycaret</guid>
      <pubDate>Mon, 05 Feb 2024 03:32:23 GMT</pubDate>
    </item>
    <item>
      <title>从多个 csv 文件构建时间序列数据集</title>
      <link>https://stackoverflow.com/questions/77934056/build-a-time-series-dataset-from-multiple-csv-files</link>
      <description><![CDATA[这里是新手👋
我有超过 6K 个不同的 CSV 文件，其中包含 1 分钟时间范围内的股票数据。 csv 文件包含 [timestamp, open, close, high, low] 列。数据范围为9:30-16:00，每分钟有一个数据。
我正在尝试解决两个问题。 1- 将所有这些数据输入模型进行训练 2- 在输入模型之前动态构建时间序列。由于大小，不可能将所有内容都保存在内存中。此外，由于文件大小的原因，以时间序列格式保存文件也是不可能的。
对于每个训练样本，我想构建按天分组的开盘价、收盘价、最低价、最高价的 t-20。这意味着 9:30 不会有 t-20 个步骤，因此在这种情况下，除了 t=0 之外，将用零填充。 9:31 将有 t=0 和 -1，其余为零。 9:50； t0 = 9:50，t-1 = 9:49，t-2 = 9:48... t-19 =9:31，t-20 = 9:30。等等。这将给出形状张量（M [训练示例]、20 [20 个时间步长]、4 [开盘价、收盘价、最低价、最高价或每个步骤]）。
为了解决问题1，我正在考虑使用tf.data.experimental.make_csv_dataset函数。然而，为了解决问题 2，我必须编写一个函数来映射行并构建时间步长。
您将如何实现这一目标？如果这是实现这一目标的最佳且最有效的方法，您将如何构建映射函数？还有什么其他方法可以执行此操作？]]></description>
      <guid>https://stackoverflow.com/questions/77934056/build-a-time-series-dataset-from-multiple-csv-files</guid>
      <pubDate>Sat, 03 Feb 2024 23:03:35 GMT</pubDate>
    </item>
    <item>
      <title>在 Rust-linfa 中加载用于预测的线性回归模型</title>
      <link>https://stackoverflow.com/questions/77932307/loading-a-linear-regression-model-back-up-for-prediction-in-rust-linfa</link>
      <description><![CDATA[我一直在研究 Rust 机器学习的 linfa，特别是线性回归模型。我希望能够保存和加载经过训练的线性回归模型，但我无法找到实现此目的的方法。
方法 1：
到目前为止，我的方法是获取训练中涉及的主要参数，这些参数可以从 linfa 的线性回归实现中获取，并将它们存储在一个可以存储为 JSON 文件的结构中（通过 serde_json 完成）。然而，在此之后我不知道如何将其加载回来进行训练。
以上内容详情如下：
存储训练参数的结构：
struct ModelJson {
    系数：Vec f64 ，
    拦截：f64，
}

存储过程：
let model = lin_reg.fit(&amp;dataset)?;
让 model_json = ModelJson {
    系数： model.params().to_vec(),
    拦截： model.intercept(),
};

存储的数据看起来如何：
{“系数”:[-0.00017907873576254802,-0.00100659702068151,-0.0008275037845519519,0.0004613216043979551,0.00103006349345 99436]，“拦截”：50.525680622870084}

方法 2：
关于序列化和反序列化整个模型，我发现以下信息表明 linfa 中支持相同的操作。
加载和保存模型
这引出了我的第二种方法，其中我使用了 linfa-linear 的 serde 功能（包含 LinearRegression 模型），首先在我的 Cargo.toml 中包含以下内容：
linfa-clustering = {version=&quot;0.7.0&quot;, features=[&quot;serde&quot;]}
根据我对实现的理解，此功能为 LinearRegression 实现了以下功能：
Serde 序列化和反序列化实现 - 派生
上述实现：
&lt;前&gt;&lt;代码&gt;#[cfg_attr(
    特征=“serde”，
    派生（序列化，反序列化），
    serde(crate = “serde_crate”)
)]
/// 可用于进行预测的拟合线性回归模型。
pub struct FittedLinearRegression; {
    截距：F，
    参数：Array1,
}

发现于： linfa-线性导出实现
我的实现如下：
let model = lin_reg.fit(&amp;dataset)?;
让序列化 = serde_json::to_string(&amp;model).unwrap();

但是此方法出现以下错误：
不满足特征边界 `FittedLinearRegression: serde::ser::Serialize`
以下其他类型实现了特征 `serde::ser::Serialize`：
  布尔值
  字符
  大小
  i8
  i16
  i32
  i64
  i128
和其他 133 个rustcClick 以获取完整的编译器诊断
main.rs(82, 22)：此调用引入的绑定所需

是否有其他方法可以做到这一点，或者是否有某种方法可以使这些方法之一发挥作用？]]></description>
      <guid>https://stackoverflow.com/questions/77932307/loading-a-linear-regression-model-back-up-for-prediction-in-rust-linfa</guid>
      <pubDate>Sat, 03 Feb 2024 13:44:24 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“anomalib.engine”的模块</title>
      <link>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</link>
      <description><![CDATA[# 导入需要的模块

从 anomalib.data 导入 MVTec
从 anomalib.models 导入 Patchcore
从 anomalib.engine 导入引擎

错误：
ModuleNotFoundError：没有名为“anomalib.engine”的模块

我正在尝试运行这个......已经遵循库安装并看到了
https://anomalib.readthedocs.io/en/latest/markdown/ get_started/anomalib.html
我认为要么是因为引擎已被修改，要么是被库删除了......
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</guid>
      <pubDate>Sat, 03 Feb 2024 05:25:02 GMT</pubDate>
    </item>
    <item>
      <title>配置 Kaggle 以在两个 T4 GPU 之间进行分布式训练和内存共享</title>
      <link>https://stackoverflow.com/questions/77875716/configuring-kaggle-for-distributed-training-and-memory-sharing-across-two-t4-gpu</link>
      <description><![CDATA[我正在尝试在 Kaggle 上使用以下命令来训练 Dreambooth：
！加速启动 --num_cpu_threads_per_process=2 “./sdxl_train.py” \
  --pretrained_model_name_or_path=“stabilityai/stable-diffusion-xl-base-1.0” \
  --train_data_dir=“数据集/img” \
  --reg_data_dir=“数据集/reg” \
  --output_dir=“输出” \
  --output_name=“SDXLDreambooth” \
  --save_model_as=“安全张量” \
  --train_batch_size=1 \
  --max_train_steps=8000 \
  --save_every_n_steps=4001 \
  --optimizer_type=“adafactor” \
  --optimizer_argsscale_parameter=Falserelative_step=Falsewarmup_init=False\
  --xformers \
  --lr_scheduler=“constant_with_warmup” \
  --lr_warmup_steps=100 \
  --learning_rate=2.5e-6 \
  --max_grad_norm=0.0 \
  --分辨率=“1024,1024” \
  --save_ precision =“fp16” \
  --save_n_epoch_ratio=1 \
  --max_data_loader_n_workers=1 \
  --persistent_data_loader_workers \
  --mixed_ precision =“fp16” \
  --full_fp16 \
  --logging_dir=&quot;日志&quot; \
  --log_prefix=“最后” \
  --gradient_checkpointing \
  --caption_extension=“.txt” \
  --no_half_vae \
  --缓存潜伏

并添加 --train_text_encoder 会在具有 16 GB VRAM 的 P100 GPU 上出现内存不足错误，即使启用了所有优化。我已经测试过在 Modal 上具有 24 GB VRAM 的 L4 GPU 上运行相同的命令，并且它成功运行最大 VRAM 利用率约为 18 GB。
但是，我注意到 Kaggle 还提供了使用两个 T4 GPU（每个 GPU 具有 16 GB VRAM）的选项，这让我想知道是否可以更改那里的环境配置（包括例如更改脚本， DeepSpeed 配置、加速配置等）以允许在 2 个 GPU 之间共享内存，其总组合内存 (32 GB) 应允许命令运行（需要 18 GB 内存）。
Kaggle 设置的默认行为似乎是在 2 个 GPU 之间复制配置并并行处理训练，因此使用 --train_text_encoder 选项，脚本将需要每个 GPU 18 GB ，导致内存不足错误。
我应该如何配置环境，以便允许两个 GPU 之间共享内存，并避免收到内存不足错误？
&lt;小时/&gt;
编辑：以下是示例笔记本和一些运行的一些链接：

笔记本；
OOM 使用两个 15 GB T4 GPU 运行；
使用 1 个 16 GB P100 GPU 成功运行.

这些都没有启用 --train-text-encoder，因为它会导致 OOM 错误。以下是第一次运行时 T4 GPU 的内存利用率：
]]></description>
      <guid>https://stackoverflow.com/questions/77875716/configuring-kaggle-for-distributed-training-and-memory-sharing-across-two-t4-gpu</guid>
      <pubDate>Wed, 24 Jan 2024 19:22:44 GMT</pubDate>
    </item>
    <item>
      <title>我不明白为什么我的 k 均值算法不能用于异常检测</title>
      <link>https://stackoverflow.com/questions/77754284/i-dont-understand-why-my-k-means-algorithm-isnt-working-for-anomaly-detection</link>
      <description><![CDATA[我已经实现了一种聚类算法来检测田纳西州伊士曼过程数据集上的异常，但结果很奇怪，因为它没有标记任何内容。请问我做错了什么？
附上结果混淆矩阵
导入 pandas 作为 pd
将 numpy 导入为 np
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.cluster 导入 KMeans
从 sklearn.metrics 导入 precision_score
将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns
从sklearn.metrics导入confusion_matrix

def fit_preprocess(data_path):
    # 从文件中加载数据
    数据 = pd.read_csv(data_path)

    # 为特定列定义延迟映射
    延迟列映射 = {
        “XMEAS(23)”: 6、“XMEAS(24)”: 6、“XMEAS(25)”: 6、“XMEAS(26)”: 6、“XMEAS(27)”: 6、
        “XMEAS(28)”: 6、“XMEAS(29)”: 6、“XMEAS(30)”: 6、“XMEAS(31)”: 6、“XMEAS(32)”: 6、
        “XMEAS（33）”：6，“XMEAS（34）”：6，“XMEAS（35）”：6，“XMEAS（36）”：6，“XMEAS（37）”：15，
        “XMEAS（38）”：15，“XMEAS（39）”：15，“XMEAS（40）”：15，“XMEAS（41）”：15
    }

    # 根据映射调整时间延迟
    Sample_time = 3 # 采样时间（以分钟为单位）
    对于列，delay_columns_mappings.items() 中的延迟：
        shift_steps = 延迟 // 采样时间
        数据[列] = 数据[列].shift(-shift_steps)

    # 填充因移位而出现的 NaN 值
    数据.ffill(inplace=True)

    # 标准化特征，不包括标签列
    feature_columns = [data.columns 中的 col 的 col，如果 col != &#39;label&#39;]
    定标器=标准定标器()
    数据[特征列] = scaler.fit_transform(数据[特征列])

    # 存储并返回预处理参数（每个特征的平均值和标准差）
    预处理参数 = {
        &#39;平均值&#39;：scaler.mean_，
        &#39;std&#39;：scaler.scale_
    }

    返回预处理参数

def load_and_preprocess(data_path, preprocess_params):
    # 加载数据集
    数据 = pd.read_csv(data_path)

    # 分离特征和标签
    feature_columns = [data.columns 中的 col 的 col，如果 col != &#39;label&#39;]
    X = 数据[特征列]
    y = 数据[&#39;标签&#39;]

    定标器=标准定标器()
    scaler.mean_ = preprocess_params[&#39;mean&#39;]
    scaler.scale_ = preprocess_params[&#39;std&#39;]
    X_scaled = 缩放器.transform(X)
    返回 X_scaled, y

def fit_model(X):
    # 训练模型
    模型 = KMeans(n_clusters=2, n_init=10, random_state=0)
    模型.拟合(X)
    返回模型

def 预测（X，模型）：
    # 预测数据点的聚类
    集群 = model.predict(X)

    # 确定异常簇（假设为较小的簇）
    anomaly_cluster = np.argmin(np.bincount(簇))

    # 将异常簇中的数据点标记为异常
    异常 = 集群 == anomaly_cluster

    # 将布尔标志转换为整数（0 表示正常，1 表示异常）
    返回异常.astype(int)


这是 k 均值算法的实现，该脚本尝试标准化数据集并从训练集预测故障并与测试集进行比较。]]></description>
      <guid>https://stackoverflow.com/questions/77754284/i-dont-understand-why-my-k-means-algorithm-isnt-working-for-anomaly-detection</guid>
      <pubDate>Wed, 03 Jan 2024 19:38:18 GMT</pubDate>
    </item>
    <item>
      <title>java应用程序中的异常检测</title>
      <link>https://stackoverflow.com/questions/49857024/anomaly-detection-in-java-application</link>
      <description><![CDATA[我正在尝试将异常检测模块集成到现有的java应用程序中，以允许用户从不同的算法和预测模型中进行选择
Egads 库看起来相当乐观，但我不确定它是否符合我的目的，在如果有新数据进来，我应该存储和更新现有模型还是再次传递整个数据。另外，如果我只想预测 15 分钟的时间窗口，那么结果中仅传递 15 分钟的数据肯定不准确。
可能还有其他有用的技术，并且有人可以分享他执行类似任务的经验。不幸的是，找不到任何其他用于此目的的 java 库。]]></description>
      <guid>https://stackoverflow.com/questions/49857024/anomaly-detection-in-java-application</guid>
      <pubDate>Mon, 16 Apr 2018 12:10:34 GMT</pubDate>
    </item>
    </channel>
</rss>