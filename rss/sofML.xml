<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 18 Jul 2024 09:16:08 GMT</lastBuildDate>
    <item>
      <title>将safetensors模型格式（LLaVA模型）转换为gguf格式</title>
      <link>https://stackoverflow.com/questions/78763327/convert-safetensors-model-formatllava-model-into-gguf-format</link>
      <description><![CDATA[我想在 ollama 中进行 LLaVA 推理，因此我需要将其转换为 gguf 文件格式。
我的模型具有文件格式 safetensors。（使用 lora 训练）
似乎 ollama 仅支持 llama，但不支持 llava，如下所示，
https://github.com/ollama/ollama/blob/main/docs/import.md
我遵循了 llama.cpp 的说明，并在此处使用了代码 convert_lora_to_gguf.py，
https://github.com/ggerganov/llama.cpp/blob/master/convert_lora_to_gguf.py
但是我收到类似以下错误：
ERROR:lora-to-gguf:不支持 Model LlavaLlamaForCausalLM
如果我在模型文件的 config.json 中写入 llama 模型并运行以下代码，则会收到另一个错误。
 model_instance.gguf_writer.add_string(gguf.Keys.General.TYPE, gguf.GGUFType.ADAPTER)
model_instance.gguf_writer.add_string(gguf.Keys.Adapter.TYPE, &quot;lora&quot;)
model_instance.gguf_writer.add_float32(gguf.Keys.Adapter.LORA_ALPHA, float(alpha))
model_instance.gguf_writer.add_quantization_version(gguf.GGML_QUANT_VERSION)
logger.info(&quot;Exporting model...&quot;)
model_instance.write()
logger.info(f&quot;模型已成功导出至 {model_instance.fname_out}&quot;)

Traceback (most recent call last):
File &quot;C:\Users\jjjy2\OneDrive\Desktop\VLM_FastAPI\ollama\convert_lora_to_gguf.py&quot;, line 373, in &lt;module&gt;
model_instance.gguf_writer.add_string(gguf.Keys.General.FILE_TYPE, gguf.GGUFType.ADAPTER)
AttributeError: module &#39;gguf&#39; has no attribute &#39;GGUFType&#39;

似乎所有代码和 gguf 包都不支持 llava，只支持 llama。我必须将我自己训练的模型转换为 gguf。我无法使用 hugging face 的 gguf llava 模型进行推理。
有没有办法转换它？]]></description>
      <guid>https://stackoverflow.com/questions/78763327/convert-safetensors-model-formatllava-model-into-gguf-format</guid>
      <pubDate>Thu, 18 Jul 2024 08:47:53 GMT</pubDate>
    </item>
    <item>
      <title>删除 /u202f 并检查正则表达式，以便它相应地匹配 am 和 pm</title>
      <link>https://stackoverflow.com/questions/78763256/remove-u202f-and-check-the-regex-so-that-it-matches-am-and-pm-accordingly</link>
      <description><![CDATA[我写了一个模式，即正则表达式
17/07/24, 5:43pm - Ryan: 无法关联
即
模式 = r&#39;\d{1,2}/\d{1,2}/\d{2,4},\s\d{1,2}:\d{2}\s(?:am|pm)\s-\s&#39;
我尝试创建它的数据框以将日期时间与消息内容分开。但是在为模式创建不同的 df 后，显示的输出是
&#39;17/07/24, 5:43\u202fpm - &#39;,
我需要删除 unicode \u202f，并且无论时间是上午还是下午，都只显示下午
我尝试使用不同的正则表达式代码，但我对此了解不多]]></description>
      <guid>https://stackoverflow.com/questions/78763256/remove-u202f-and-check-the-regex-so-that-it-matches-am-and-pm-accordingly</guid>
      <pubDate>Thu, 18 Jul 2024 08:32:39 GMT</pubDate>
    </item>
    <item>
      <title>神经网络成本不变</title>
      <link>https://stackoverflow.com/questions/78763209/neural-network-cost-not-changing</link>
      <description><![CDATA[我似乎无法弄清楚为什么我的成本打印函数每次都给我相同的成本而我的权重和偏差却没有改变。非常感谢您的帮助，我尝试将其放入聊天机器人中，但他们也认为没有错误。
我尝试在各个地方更改代码，但成本仍然没有下降。
import numpy as np

def initialize_params(n_x, n_h, n_y):
W1 = np.random.randn(n_h, n_x) * 0.01
b1 = np.zeros((n_h, 1))
W2 = np.random.randn(n_y, n_h) * 0.01
b2 = np.zeros((n_y, 1))

parameters = {&quot;W1&quot;: W1, &quot;W2&quot;: W2, &quot;b1&quot;: b1, &quot;b2&quot;: b2}
return参数

def sigmoid(x):
返回 1 / (1 + np.exp(-x))

def forward_propagation(X, 参数):
W1 = 参数[&quot;W1&quot;]
b1 = 参数[&quot;b1&quot;]
W2 = 参数[&quot;W2&quot;]
b2 = 参数[&quot;b2&quot;]

Z1 = np.dot(W1, X) + b1
A1 = np.tanh(Z1)
Z2 = np.dot(W2, A1) + b2
A2 = sigmoid(Z2)

cache = {&quot;Z1&quot;: Z1, &quot;A1&quot;: A1, &quot;Z2&quot;: Z2, &quot;A2&quot;: A2}
assert(A2.shape == (1, X.shape[1]))
返回 A2,缓存

def cost_function(A2, Y):
m = Y.shape[1] 
logprob = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))
cost = -1/m * np.sum(logprob) 
返回成本

def reverse_propagation(parameters, cache, X, Y):
m = X.shape[1]
W1 = 参数[&quot;W1&quot;]
W2 = 参数[&quot;W2&quot;]
A1 = 缓存[&quot;A1&quot;]
A2 = 缓存[&quot;A2&quot;]

dZ2 = A2 - Y
dW2 = 1/m * np.dot(dZ2, A1.T)
db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)
dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))
dW1 = 1/m * np.dot(dZ1, X.T)
db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)

grads = {&quot;dW1&quot;: dW1, &quot;dW2&quot;: dW2, &quot;db1&quot;: db1, &quot;db2&quot;: db2}
return grads

def update_parameters(parameters, grads, learning_rate):
W1 = 参数[&quot;W1&quot;]
b1 = 参数[&quot;b1&quot;]
W2 = 参数[&quot;W2&quot;]
b2 =参数[&quot;b2&quot;]

dW1 = grads[&quot;dW1&quot;]
db1 = grads[&quot;db1&quot;]
dW2 = grads[&quot;dW2&quot;]
db2 = grads[&quot;db2&quot;]

W1 = W1 - 学习率 * dW1
b1 = b1 - 学习率 * db1
W2 = W2 - 学习率 * dW2
b2 = b2 - 学习率 * db2

参数 = {&quot;W1&quot;: W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2}
返回参数

def nn_model(X, Y, n_x, n_h, n_y, num_iterations=10000, learning_rate=0.01):
参数 = 初始化参数(n_x, n_h, n_y)

对于 i in range(num_iterations):
A2, 缓存 = 前向传播(X, 参数)
成本 = 成本函数(A2, Y)
梯度 = 后向传播(参数, 缓存, X, Y)
参数 = 更新参数(参数, 梯度, 学习率)

如果 i % 1000 == 0:
打印(f&quot;迭代 {i}: 成本 {cost}&quot;)

返回参数

def test_xor():
X = np.array([[0, 0, 1, 1],
[0, 1, 0, 1]])
Y = np.array([[0, 1, 1, 0]])

n_x = X.shape[0]
n_h = 4
n_y = Y.shape[0]

参数 = nn_model(X, Y, n_x, n_h, n_y)

A2, _ = forward_propagation(X, 参数)
predictions = (A2 &gt; 0.5).astype(int)

print(&quot;预测：&quot;, predictions)
print(&quot;真标签：&quot;, Y)

断言 np.array_equal(predictions, Y), &quot;测试失败！&quot;
print(&quot;XOR 测试通过！&quot;)

test_xor()
]]></description>
      <guid>https://stackoverflow.com/questions/78763209/neural-network-cost-not-changing</guid>
      <pubDate>Thu, 18 Jul 2024 08:23:17 GMT</pubDate>
    </item>
    <item>
      <title>安装后无法在 Google Colab 中导入“bm3d”包</title>
      <link>https://stackoverflow.com/questions/78762914/unable-to-import-bm3d-package-in-google-colab-after-installation</link>
      <description><![CDATA[我正在尝试使用 bm3d 包在 Google Colab 中进行图像去噪。几天前我可以毫无问题地使用它。但是，现在我遇到了导入错误，即使包安装成功。

安装 bm3d 包：
!pip install bm3d

尝试导入 bm3d 包：
import bm3d


观察到的错误：
ImportError：无法从“scipy._lib.deprecation”导入名称“_sub_module_deprecation”（/usr/local/lib/python3.10/dist-packages/scipy/_lib/deprecation.py）
我想尝试在 colab 中导入 bm3d，并希望使用它来对图像进行去噪（去除噪音）。]]></description>
      <guid>https://stackoverflow.com/questions/78762914/unable-to-import-bm3d-package-in-google-colab-after-installation</guid>
      <pubDate>Thu, 18 Jul 2024 07:16:33 GMT</pubDate>
    </item>
    <item>
      <title>tf.test.is_built_with_cuda() 返回 false</title>
      <link>https://stackoverflow.com/questions/78762583/tf-test-is-built-with-cuda-return-false</link>
      <description><![CDATA[我使用 Ubuntu/Linux 用 Anaconda 创建虚拟环境，并使用 VSCode 进行机器学习开发。
问题是，我安装 CUDA 和 cuDNN 后，使用 tf.test.is_built_with_cuda() 测试 GPU 是否可以使用，但它返回了 FALSE。
以下是我安装 CUDA 和 cuDNN 所遵循的步骤：
wget https://repo.anaconda.com/pkgs/main/linux-64/cudatoolkit-10.0.130-0.conda
conda install cudatoolkit-10.0.130-0.conda 
wget https://repo.anaconda.com/pkgs/main/linux-64/cudnn-7.6.5-cuda10.0_0.conda
conda install cudnn-7.6.5-cuda10.0_0.conda

版本
Python 3.6.13 :: Anaconda, Inc.
tensorflow 1.14.0 pypi_0 pypi
tensorflow-gpu 1.14.0 pypi_0 pypi
cudatoolkit 10.0.130 0 &lt;unknown&gt;
cudnn 7.6.5 cuda10.0_0 &lt;unknown&gt;
]]></description>
      <guid>https://stackoverflow.com/questions/78762583/tf-test-is-built-with-cuda-return-false</guid>
      <pubDate>Thu, 18 Jul 2024 05:44:08 GMT</pubDate>
    </item>
    <item>
      <title>双子座本月印刷日</title>
      <link>https://stackoverflow.com/questions/78762138/gemini-printing-days-of-the-month</link>
      <description><![CDATA[我尝试从下图中打印垃圾日，但无论我怎么尝试，都无法正确打印这些天。
我尝试了不同的温度和提示技术，但都不起作用。
我正在寻找双子座模型的提示
]]></description>
      <guid>https://stackoverflow.com/questions/78762138/gemini-printing-days-of-the-month</guid>
      <pubDate>Thu, 18 Jul 2024 01:57:06 GMT</pubDate>
    </item>
    <item>
      <title>XGBoostError：参数详细程度的值 -1 超出界限 [0,3]</title>
      <link>https://stackoverflow.com/questions/78761783/xgboosterror-value-1-for-parameter-verbosity-exceed-bound-0-3</link>
      <description><![CDATA[错误消息如标题所示。根据下面的代码，这对我来说毫无意义：
clf = xgboost.XGBClassifier(verbosity=1)
print (clf.__class__, clf.verbosity) 
# prints &lt;class &#39;xgboost.sklearn.XGBClassifier&#39;&gt; 1
clf.fit(X=train_data_iter[features].fillna(0), y=train_data_iter[&#39;y&#39;]) # 错误在这里出现

值显然是 1，但不知何故却变成了 -1？我不明白。]]></description>
      <guid>https://stackoverflow.com/questions/78761783/xgboosterror-value-1-for-parameter-verbosity-exceed-bound-0-3</guid>
      <pubDate>Wed, 17 Jul 2024 22:12:23 GMT</pubDate>
    </item>
    <item>
      <title>将 Tensorflow 模型转换为 CoreML 模型时出错</title>
      <link>https://stackoverflow.com/questions/78761234/error-converting-tensorflow-model-to-coreml-model</link>
      <description><![CDATA[NotImplementedError Traceback（最近一次调用最后一次）
Cell In[19]，第 1 行
----&gt; 1 mlmodel = ct.convert(model, convert_to=&quot;mlmodel&quot;, source=&quot;tensorflow&quot;)

文件 ~/anaconda3/lib/python3.11/site-packages/coremltools/converters/_converters_entry.py:551，在 convert(model, source, input, output, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline)
539 exact_target = _determine_target(convert_to, minimum_deployment_target)
540 _validate_conversion_arguments(
541 model,
542 exact_source,
(...)
549 minimum_deployment_target,
550 )
--&gt; 551 need_fp16_cast_pass = _need_fp16_cast_pass(compute_precision, exact_target)
553 如果 pass_pipeline 为 None:
554 pass_pipeline = PassPipeline()

文件 ~/anaconda3/lib/python3.11/site-packages/coremltools/converters/_converters_entry.py:624，位于 _need_fp16_cast_pass(compute_precision, convert_to)
620 def _need_fp16_cast_pass(
621 compute_precision: Optional[Union[precision, FP16ComputePrecision]], convert_to: Text
622 ) -&gt; bool:
623 if convert_to not in (&quot;mlprogram&quot;, &quot;neuralnetwork&quot;, &quot;milinternal&quot;, &quot;milpython&quot;):
--&gt; 624 raise NotImplementedError(f&quot;后端转换器 {convert_to} 未实现&quot;)
626 if compute_precision is None:
627 return convert_to != &quot;neuralnetwork&quot;

NotImplementedError: 后端转换器 mlmodel 未实现

不知道这个错误是什么意思。我正在尝试将使用 tensorflow 构建的 CNN 模型转换为 coreml 模型，但我一直收到上述错误。
mlmodel = ct.convert(model, convert_to=&quot;mlmodel&quot;, source=&quot;tensorflow&quot;)

这是在从保存的 .h5 文件导入模型后完成的。]]></description>
      <guid>https://stackoverflow.com/questions/78761234/error-converting-tensorflow-model-to-coreml-model</guid>
      <pubDate>Wed, 17 Jul 2024 19:08:55 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Huggingface Hub 上成功设置和检索 HuggingfaceDataset 的元数据信息？</title>
      <link>https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat</link>
      <description><![CDATA[我有许多数据集，它们是从字典中创建的，如下所示：
info = DatasetInfo(
description=&quot;my happy lil dataset&quot;,
version=&quot;0.0.1&quot;,
homepage=&quot;https://www.myhomepage.co.uk&quot;
)
train_dataset = Dataset.from_dict(prepare_data(data[&quot;train&quot;]), info=info)
test_dataset = Dataset.from_dict(prepare_data(data[&quot;test&quot;]), info=info)
validation_dataset = Dataset.from_dict(prepare_data(data[&quot;validation&quot;]),info=info)

然后我将它们组合成一个 DatasetDict。
# 创建 DatasetDict
dataset = DatasetDict(
{&quot;train&quot;: train_dataset, &quot;test&quot;: test_dataset, &quot;validation&quot;: validation_dataset}
)

到目前为止，一切顺利。如果我访问 dataset[&#39;train&#39;].info.description，我会看到预期结果“My happy lil dataset”。
因此，我将其推送到集线器，如下所示：
dataset.push_to_hub(f&quot;{organization}/{repo_name}&quot;, commit_message=&quot;Some commit message&quot;)

这也成功了。
但是，当我从集线器拉回数据集并访问与其相关的信息时，我没有获取数据集的描述，而是只得到了一个空字符串；像这样：
pulled_data = full = load_dataset(&quot;f{organization}/{repo_name}&quot;, use_auth_token = True)

# 我希望以下内容打印出&quot;my happy lil dataset&quot;
print(pulled_data[&quot;train&quot;].info.description)
# 但是，它返回的是 &#39;&#39;

我是否错误地从集线器加载了数据？我是否只推送了我的数据集，而没有推送信息？
我觉得我遗漏了一些显而易见的东西，但我真的不确定。]]></description>
      <guid>https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat</guid>
      <pubDate>Wed, 17 Jul 2024 13:23:04 GMT</pubDate>
    </item>
    <item>
      <title>DSPy 无法检索 ChromaDB 中带有文本嵌入的段落</title>
      <link>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</link>
      <description><![CDATA[我正在使用 DSPy 和 ChromaDB 为 pdf 文件开发 RAG 应用程序。
首先，我从 pdf 中获取文本并将其作为块添加到 Chromadb。还添加了块的嵌入。并尝试使用 DSPy 检索与查询相关的块。但是它出现了错误
存储数据和嵌入
def store_document_in_chromadb(text):
chunks = chunk_document(text)
ids = [f&#39;chunk_{i}&#39; for i in range(len(chunks))]
embeddings = [get_embedding(chunk).tolist() for chunk in chunks]

collection.add(ids=ids, documents=chunks, embeddings=embeddings)

我尝试像这样检索相关块，
retriever_model = ChromadbRM(&quot;contracts_collection&quot;, &#39;db/&#39;, k=2)
dspy.settings.configure(lm=llama2_model, rm=retriever_model)

class GenerateAnswer(dspy.Signature): 
“”“”根据给出的上下文回答问题。“”“”
context = dspy.InputField(desc=&quot;可能包含相关上下文&quot;)
question = dspy.InputField()
answer = dspy.OutputField(desc=&quot;通常为 5 到 10 个单词&quot;)

class RAG(dspy.Module): 
def __init__(self, num_passages=2):
super().__init__()
self.retrieve = dspy.Retrieve(k=num_passages)
self.generate_answer = dspy.ChainOfThought(GenerateAnswer)

def forward(self, question):
context = self.retrieve(question).passages
prediction = self.generate_answer(context=context, question=question)
return dspy.Prediction(context=context, answer=prediction.answer)

with dspy.context(lm=llama2_model, rm=retriever_model):
module = RAG()
response = module(&quot;总支出是多少&quot;)
print(response)

当我运行此程序时，出现此错误
InvalidDimensionException：嵌入维度 384 与集合维度 768 不匹配
但是当我从 ChromaDB 中删除嵌入时，它会正确检索相关块。
为什么使用嵌入时没有出现此错误？]]></description>
      <guid>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</guid>
      <pubDate>Wed, 17 Jul 2024 08:03:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在 XGBoost 决策树的叶节点上显示预测类标签？</title>
      <link>https://stackoverflow.com/questions/78757774/how-do-i-display-predicted-class-labels-on-the-leaf-nodes-of-an-xgboost-decision</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78757774/how-do-i-display-predicted-class-labels-on-the-leaf-nodes-of-an-xgboost-decision</guid>
      <pubDate>Wed, 17 Jul 2024 05:22:10 GMT</pubDate>
    </item>
    <item>
      <title>如何将多头自注意力输出的形状更改为可以馈送到卷积层的形状？</title>
      <link>https://stackoverflow.com/questions/78757193/how-to-change-shape-of-multi-head-self-attention-output-to-a-shape-that-can-be-f</link>
      <description><![CDATA[我遇到了这样的错误：
MHSA（多头自注意力）的输出如下：
torch.Size([20, 197, 768])


批次大小为 20
序列长度为 197（之前为 196，添加类标记后变为 197）
嵌入维度为 768

我想将其重塑以适应以下格式，以便将其馈送到卷积层：
torch.Size([batch_size, channel, width, height])

我尝试通过使用以下方法添加新维度来实现此目的方法：
torch.unsqueeze(1)
torch.transpose(1, 3)

这成功地允许馈送到卷积层。但是，我不确定这种方法是否正确，如果不正确，请纠正我。
目前，我正在尝试一种不同的方法：
new_size = int(math.sqrt(sequence_length))
torch.transpose(1, 2).view(batch_size, embed_dim, new_size, new_size)

这导致错误，指出形状对于大小为 (some_number) 的输入无效。这是因为序列长度（197）不是完全平方的，得出的是一个十进制数，而视图函数需要输入一个整数，平方运算在转换为整数后得出 16，但 batch_size * 768 * 16 * 16 不等于 batch_size * 197 * 768，导致错误
我的分析正确吗？我该如何解决这个问题？还有没有更好的方法？]]></description>
      <guid>https://stackoverflow.com/questions/78757193/how-to-change-shape-of-multi-head-self-attention-output-to-a-shape-that-can-be-f</guid>
      <pubDate>Wed, 17 Jul 2024 00:24:06 GMT</pubDate>
    </item>
    <item>
      <title>当已知目标值的特征向量时，如何使用监督式机器学习进行时间序列预测？</title>
      <link>https://stackoverflow.com/questions/78757005/how-to-use-supervised-ml-for-time-series-predictions-when-the-feature-vector-for</link>
      <description><![CDATA[我尝试使用 LSTM 预测仪器的连续“偏移”校准值。这些偏移值之前已被证明与用作特征的一对温度值有很好的相关性。这些偏移值显示出周期性，因此为模型选择了 LSTM。但是，我发现的所有 LSTM 示例都使用来自先前数据点序列的特征向量来预测目标。我觉得这可能无法捕捉序列中每个特征向量与其偏移之间的关系。而且由于在预测中只使用了先前数据点序列的特征向量，因此无法有效地利用该温度偏移关系。
为了解决这个问题，我已经将当前数据点的特征向量添加到用于训练模型的向量序列中，当然，也添加到用于预测目标偏移的序列中。
但是，我如何才能对要预测的偏移的特征向量赋予更大的权重，以允许模型利用温度偏移关系？
创建序列的代码如下所示：
def create_sequences(x, y, time_steps = 24):
xs, ys = [], []
for i in range(len(x) - time_steps - 1):
x_temp = x[i:(i + time_steps + 1)] #在目标值之前创建一个 time_steps 序列
xs.append(x_temp)
ys.append(y[i + time_steps]) #目标值为序列之后的值
return np.array(xs), np.array(ys)
]]></description>
      <guid>https://stackoverflow.com/questions/78757005/how-to-use-supervised-ml-for-time-series-predictions-when-the-feature-vector-for</guid>
      <pubDate>Tue, 16 Jul 2024 22:49:31 GMT</pubDate>
    </item>
    <item>
      <title>即使经过数百个时期，pytorch AdamW 的 LR 仍未衰减</title>
      <link>https://stackoverflow.com/questions/78752899/lr-not-decaying-for-pytorch-adamw-even-after-hundreds-of-epochs</link>
      <description><![CDATA[我有以下使用 Pytorch 中的 AdamW 优化器的代码：
optimizer = AdamW(params=self.model.parameters(), lr=0.00005)

我尝试使用 wandb 进行登录，如下所示：
lrs = {f&#39;lr_group_{i}&#39;: param_group[&#39;lr&#39;]
for i, param_group in enumerate(self.optimizer.param_groups)}
wandb.log({&quot;train_loss&quot;: avg_train_loss, &quot;val_loss&quot;: val_loss, **lrs})

请注意 weight_decay 参数的默认值为 0.01（对于 AdamW）。
当我检查 wandb 仪表板时，它显示 AdamW 的 LR 即使在 200 个 epoch 之后也相同，并且根本没有衰减。我尝试了几次。

为什么 LR 衰减没有发生？
此外，它仅显示一个参数组的 LR。为什么会这样？似乎我在这里错过了一些基本的东西。有人可以指出吗？]]></description>
      <guid>https://stackoverflow.com/questions/78752899/lr-not-decaying-for-pytorch-adamw-even-after-hundreds-of-epochs</guid>
      <pubDate>Tue, 16 Jul 2024 06:09:44 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 中的空间数据管理机器学习模型中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</guid>
      <pubDate>Thu, 11 Jul 2024 05:01:17 GMT</pubDate>
    </item>
    </channel>
</rss>