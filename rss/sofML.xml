<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 07 Aug 2024 21:16:18 GMT</lastBuildDate>
    <item>
      <title>数值数据中的异常值检测问题</title>
      <link>https://stackoverflow.com/questions/78845677/issues-with-outlier-detection-in-numerical-data</link>
      <description><![CDATA[我目前正在进行一个数据分析项目，我使用 Z 分数来检测数据集数值列中的异常值。但是，我遇到了一个问题，合法的数据点被标记为异常值，我不确定为什么会发生这种情况。
我正在做的事情是：
缺失值的插补：我使用 sklearn.impute 中的 IterativeImputer 来填充数值列中的缺失值。
异常值检测：我计算每个数值列的 Z 分数，以使用阈值 3 来检测异常值。
例如，我有一条关于埃及古典式摔跤运动员 Yasser Abdel Rahman Sakr 的记录，他具有以下属性：
体重：120 公斤
身高：180 厘米
尽管这些是合理的测量值，但我的代码将此记录标记为异常值。其他记录也出现了此问题。
以下是我的代码的相关部分：
 import numpy as np
import pandas as pd
from sklearn.impute import IterativeImputer

# 假设“数据”已定义并加载
numeric_cols = data.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]).columns
categorical_cols = data.select_dtypes(include=[&#39;object&#39;]).columns

# 在数字列中插入缺失值
mice_imputer = IterativeImputer(max_iter=10, random_state=0)
df_numeric = pd.DataFrame(mice_imputer.fit_transform(data[numeric_cols]), columns=numeric_cols)

# 将插入的数字列与原始分类列合并
df_MICE = pd.concat([df_numeric, data[categorical_cols]], axis=1)

# 存储异常信息的字典
outliers_info = {}

for col in numeric_cols:
# 计算平均值和标准差
mean = df_MICE[col].mean()
std_dev = df_MICE[col].std()

# 如果 std_dev 为零，则避免除以零
if std_dev == 0:
print(f&quot;列 {col} 的标准差为零。跳过异常值检测。)
继续

# 计算 Z 分数
z_scores = (df_MICE[col] - mean) / std_dev

# 定义异常值阈值
阈值 = 3

# 查找异常值
outliers = df_MICE[np.abs(z_scores) &gt;阈值]

# 将异常值的数量和异常值样本存储在字典中
outliers_info[col] = {
&#39;count&#39;: len(outliers),
&#39;sample&#39;: outliers.head(1) # 一个异常值的样本
}

# 打印每个数值列的异常值数量和样本
for col, info in outliers_info.items():
print(f&#39;Column: {col}&#39;)
print(f&#39;Number of outliers: {info[&quot;count&quot;]}&#39;)
if info[&#39;count&#39;] &gt; 0：
print(&#39;样本异常值：&#39;)
print(info[&#39;sample&#39;])
else:
print(&#39;无异常值。&#39;)
print() # 打印空白行以提高可读性

问题：
尽管是真实且可信的记录，但 Yasser Abdel Rahman Sakr 的体重和身高被标记为异常值。其他记录也会出现此问题。
问题：
什么原因导致合法数据点被标记为异常值？
是否有任何改进或替代方法可以更好地处理此情况下的异常值检测？
我应该考虑其他因素或异常值检测方法吗？
感谢您提供的任何指导或建议。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78845677/issues-with-outlier-detection-in-numerical-data</guid>
      <pubDate>Wed, 07 Aug 2024 21:07:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Mediapipe 根据特定面部区域过滤面部标志坐标？</title>
      <link>https://stackoverflow.com/questions/78845589/how-to-filter-face-landmark-coordinates-by-specific-facial-regions-using-mediapi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78845589/how-to-filter-face-landmark-coordinates-by-specific-facial-regions-using-mediapi</guid>
      <pubDate>Wed, 07 Aug 2024 20:41:07 GMT</pubDate>
    </item>
    <item>
      <title>YOLov8 无法在本地机器上做出正确的预测</title>
      <link>https://stackoverflow.com/questions/78845245/yolov8-not-making-correct-prediction-on-local-machine</link>
      <description><![CDATA[我尝试在本地机器上运行 yolo v8，但做出了错误的预测，即它在图像顶部预测了很多 100% 的 bbox。所有 bbox 都在那里
本地机器结果
但是，如果我在 kaggle/colab 上运行它，它工作正常
vm 结果
vm/local 上的两台机器都是基于 cpu 的，没有使用 gpu。]]></description>
      <guid>https://stackoverflow.com/questions/78845245/yolov8-not-making-correct-prediction-on-local-machine</guid>
      <pubDate>Wed, 07 Aug 2024 18:44:35 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Matlab 中使用 knnsearch 设置 k 值</title>
      <link>https://stackoverflow.com/questions/78844904/how-to-set-k-value-using-knnsearch-in-matlab</link>
      <description><![CDATA[我有一个代码来对图像进行分类。
training1 = xlsread(&#39;Data Train&#39;);

% 提及训练数据矩阵在 excel 文件中的位置
training = [training1(:,1) training1(:,2) training1(:,3) training1(:,4) training1(:,5) training1(:,6) training1(:,7) training1(:,8) training1(:,9) training1(:,10) training1(:,11) training1(:,12) training1(:,13) training1(:,14) training1(:,15) training1(:,16) training1(:,17) training1(:,18) training1(:,19) training1(:,20) training1(:,21) training1(:,22) training1(:,23) training1(:,24)];

% 提及输入数据变量
Z=[MeanR MeanG MeanB MeanH MeanS MeanV VarRed VarGreen VarBlue VarH VarS VarV RangeR RangeG RangeB RangeH RangeS RangeV sdR sdG sdB sdH sdS sdV];

%执行 knn 分类
result = knnsearch(training,Z);

if (result&gt;=1 &amp;&amp; result&lt;=20)
set(handles.EditBox,&#39;string&#39;,&#39;Raw&#39;);
elseif (result&gt;=21 &amp;&amp; result&lt;=40)
set(handles.EditBox,&#39;string&#39;,&#39;Undercook&#39;);
elseif (result&gt;=41 &amp;&amp; result&lt;=60)
set(handles.EditBox,&#39;string&#39;,&#39;Cook&#39;);
elseif (result&gt;=61 &amp;&amp; result&lt;=80)
set(handles.EditBox,&#39;string&#39;,&#39;Rotten&#39;);
end

knnsearch 语法是否仅将 k 的默认值设置为 1？
如何才能将 knnsearch 中的 k 值改为 5？
当我尝试将其更改为
k = 5;
result = knnsearch(training,Z,&#39;K&#39;,k); 

系统不显示分类结果。
您的帮助将真正帮助我更多地了解这个 knn 项目！]]></description>
      <guid>https://stackoverflow.com/questions/78844904/how-to-set-k-value-using-knnsearch-in-matlab</guid>
      <pubDate>Wed, 07 Aug 2024 17:01:04 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降算法中的学习率</title>
      <link>https://stackoverflow.com/questions/78844901/learning-rate-in-gradient-descent-algorithm</link>
      <description><![CDATA[在梯度下降算法中，我根据它们的导数更新B和M值，然后将它们与学习率值相乘，但是当我对L使用相同的值，例如0.0001时，它不能正常工作。减小或增加L值不起作用。作为一种解决方法，我不得不为b和m值设置不同的L值。这是正常的还是有错误？
import pandas as pd
import matplotlib.pyplot as plt
import time
import random

# Veri seti
veri_seti = &quot;study_score_decreasing.csv&quot; #study_score_decreasing.csv #study_score_increasing.csv 
data = pd.read_csv(veri_seti)

# 梯度下降 Fonksiyonu
def gradient_descent(m_next, b_next, points, L):
m_gradient = 0
b_gradient = 0
n = len(points)

for i in range(n):
x = points.iloc[i].study_time
y = points.iloc[i].score

m_gradient += -(2/n) * x * (y - (m_next * x + b_next))
b_gradient += -(2/n) * (y - (m_next * x + b_next))

m = m_next - m_gradient * 0.0001 #(L = 0.0001)
b = b_next - b_gradient * 0.1 #(L = 0.1)

return m, b

# 图形选项 图表
def show_graph(m, b):
plt.scatter(data.study_time, data.score, color=&quot;red&quot;)
x_range = range(int(data.study_time.min()), int(data.study_time.max()) + 1)
plt.plot(x_range, [m * x + b for x in x_range], color=&quot;blue&quot;)
plt.xlabel(&#39;学习时间&#39;)
plt.ylabel(&#39;分数&#39;)
plt.title(&#39;学习时间与分数&#39;)
plt.show()
time.sleep(0.001)
print(&quot;=&gt; F(X):&quot;, round(m, 1), &quot;X +&quot;, round(b, 3))

# Ana Fonksiyon
def main(m, b, L, epochs):
print(&quot;=&gt; F(X):&quot;, m, &quot;X&quot;, b)

for i in range(epochs):
m, b = gradient_descent(m, b, data, L)
show_graph(m, b)

# 基础说明
main(random.uniform(-1, 110), random.uniform(-10, 10), 0.1, 250)

我逐个更新了L值，得到了合乎逻辑的结果，但是用一个共同的L值，为什么解看起来不合逻辑？]]></description>
      <guid>https://stackoverflow.com/questions/78844901/learning-rate-in-gradient-descent-algorithm</guid>
      <pubDate>Wed, 07 Aug 2024 16:59:10 GMT</pubDate>
    </item>
    <item>
      <title>使用 7 个类别训练图像分类器，但我的模型过度拟合，导致模型的准确性在训练过程中表现异常</title>
      <link>https://stackoverflow.com/questions/78844629/training-image-classifier-with-7-classes-but-my-model-is-overfitting-resulting-t</link>
      <description><![CDATA[我正在为特定汽车发动机部件的 7 种不同模型类型训练图像分类器。每个类别都有 308 张灰度图像，分辨率均为 1014x760。这些图像主要由白色屏幕上的发动机部件组成，每次拍照后都会旋转 60 度，因此数据集由看起来彼此非常相似的图片组成。我想训练我的模型 50 个 epoch，但在第 30 个 epoch 之后，准确率达到 1.0，而验证准确率停留在 0.2 左右。为什么结果这么奇怪？是不是图片太相似了？
import numpy as np
import pickle
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.utils import to_categorical
import time

name = &quot;Core_Classifier_{}&quot;.format(int(time.time()))

tensorboard = TensorBoard(log_dir=&quot;logs/{}&quot;.format(name))

X = pickle.load(open(&quot;X.pickle&quot;, &quot;rb&quot;))
y = pickle.load(open(&quot;y.pickle&quot;, &quot;rb&quot;))

X = X/255.0 # 标准化颜色值
y = to_categorical(y, num_classes=7)

model = Sequential()

model.add(Conv2D(64, (3, 3), input_shape = X.shape[1:]))
model.add(Activation(&quot;relu&quot;))
model.add(MaxPooling2D(pool_size = (2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation(&quot;relu&quot;))

model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Flatten())

model.add(Dense(64))
model.add(Activation(&quot;relu&quot;)) # idk 是否需要

model.add(Dense(7))
model.add(Activation(&quot;softmax&quot;))

model.compile(loss = &quot;categorical_crossentropy&quot;,
optimizer = &quot;adam&quot;,
metrics = [&quot;accuracy&quot;])

model.fit(X, y, batch_size = 64, epochs = 50, validation_split = 0.1, callbacks = [tensorboard])

以下是通过 tensorboard 表示的图形
我添加了2个model.add(Dropout(0.2))函数，但结果没有太大变化。
import numpy as np
import pickle
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.utils import to_categorical
import time

name = &quot;Core_Classifier_{}&quot;.format(int(time.time()))

tensorboard = TensorBoard(log_dir=&quot;logs/{}&quot;.format(name))

X = pickle.load(open(&quot;X.pickle&quot;, &quot;rb&quot;))
y = pickle.load(open(&quot;y.pickle&quot;, &quot;rb&quot;))

X = X/255.0 # 标准化颜色值
y = to_categorical(y, num_classes=7)

model = Sequential()

model.add(Conv2D(64, (3, 3), input_shape = X.shape[1:]))
model.add(Activation(&quot;relu&quot;))
model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(64, (3, 3)))
model.add(Activation(&quot;relu&quot;))
model.add(MaxPooling2D(pool_size = (2, 2)))
model.add(Dropout(0.2))
model.add(Flatten())

model.add(Dense(64))
model.add(Activation(&quot;relu&quot;)) # idk 是否需要

model.add(Dense(7))
model.add(Activation(&quot;softmax&quot;))

model.compile(loss = &quot;categorical_crossentropy&quot;,
optimizer = &quot;adam&quot;,
metrics = [&quot;accuracy&quot;])

model.fit(X, y, batch_size = 64, epochs = 50, validation_split = 0.1, callbacks = [tensorboard])

训练结果反馈退出函数]]></description>
      <guid>https://stackoverflow.com/questions/78844629/training-image-classifier-with-7-classes-but-my-model-is-overfitting-resulting-t</guid>
      <pubDate>Wed, 07 Aug 2024 15:51:31 GMT</pubDate>
    </item>
    <item>
      <title>训练扩散器/UNet2DConditionModel 时形状不匹配</title>
      <link>https://stackoverflow.com/questions/78844279/shapes-mismatch-while-training-diffusers-unet2dconditionmodel</link>
      <description><![CDATA[我正在尝试从头开始训练扩散器/UNet2DConditionModel。目前，我在 unet 转发时遇到错误：mat1 和 mat2 形状无法相乘（288x512 和 1280x512）。我注意到 mat1 的第一个维度（288）可能会因数据集批次而异。
如何修复矩阵形状错误？我是否需要用零填充 mat1 以使其形状与 mat2 相同：1280x512，或者我设置的模型初始化参数无效。我将不胜感激任何帮助。
这是我的训练代码
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms as T
from transformers import CLIPModel, CLIPProcessor, CLIPTextModel, AutoTokenizer
from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler
import pandas as pd
from PIL import Image
import io
from tqdm.auto import tqdm

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# Dataset 类，返回图像和相应的文本标题
class TextImgDataset(Dataset):
def __init__(self, fp: str):
self.df = pd.read_parquet(fp)
self.transform = T.Compose([
T.Lambda(lambda img: img.convert(&#39;RGB&#39;) if img.mode != &#39;RGB&#39; else img),
T.Resize((64, 64)),
T.ToTensor(),
])

def __len__(self) -&gt; int:
return self.df.shape[0]

def __getitem__(self, idx) -&gt; (torch.Tensor, str):
row = self.df.iloc[idx]
img_bytes = io.BytesIO(row[&#39;image&#39;][&#39;bytes&#39;])
image = Image.open(img_bytes)
image_tensor = self.transform(image)
caption = row[&#39;text&#39;]

return image_tensor, caption

# 初始化模型
clip_model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;).to(device)
clip_processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

text_encoder = CLIPTextModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

vae = AutoencoderKL.from_single_file(
&quot;https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors&quot;).to(
device)

unet = UNet2DConditionModel(
in_channels=4,
out_channels=4,
layer_per_block=2,
sample_size=64,
block_out_channels=(128, 256, 512, 512),
down_block_types=(&quot;DownBlock2D&quot;, &quot;DownBlock2D&quot;, &quot;DownBlock2D&quot;, &quot;AttnDownBlock2D&quot;),
up_block_types=(&quot;AttnUpBlock2D&quot;, &quot;UpBlock2D&quot;, &quot;UpBlock2D&quot;, &quot;UpBlock2D&quot;),
).to(device)

noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule=&quot;linear&quot;)

# DataLoader
# 我使用的数据集来自这里 https://huggingface.co/datasets/pranked03/flowers-blip-captions
dataset = TextImgDataset(fp=&#39;~/dataset.parquet&#39;)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

#优化器
optimizer_vae = torch.optim.Adam(vae.parameters(), lr=1e-4)
optimizer_unet = torch.optim.Adam(unet.parameters(), lr=1e-4)

# 训练循环
num_epochs = 10

unet.train()

for epoch in range(num_epochs):
for batch in tqdm(dataloader):
images, captions = batch
images = images.to(device)
latents = vae.encode(images).latent_dist.sample()
latents = latents * vae.config.scaling_factor

noise = torch.randn_like(latents)
bsz = latents.shape[0]
timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()

inputs = tokenizer(captions, padding=True, return_tensors=&quot;pt&quot;, truncation=True)
outputs = text_encoder(**inputs).last_hidden_​​state.to(device)

noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
#noisy_latents 形状：[16, 4, 8, 8]
#timesteps 形状：torch.Size([16])
#encoder_hidden_​​states 形状：torch.Size([16, 18, 512])
pred = unet(sample=noisy_latents, timestep=timesteps,coder_hidden_​​states=outputs, return_dict=False) # 在此处获取错误

# .... 其余代码（反向传播和采样）

]]></description>
      <guid>https://stackoverflow.com/questions/78844279/shapes-mismatch-while-training-diffusers-unet2dconditionmodel</guid>
      <pubDate>Wed, 07 Aug 2024 14:35:26 GMT</pubDate>
    </item>
    <item>
      <title>构建模拟 SVM 模型的自定义分类器</title>
      <link>https://stackoverflow.com/questions/78843755/building-a-custom-classifier-that-simulates-svm-model</link>
      <description><![CDATA[我在代码中使用了以下 SVM：
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classes_report, confusion_matrix, f1_score

# 加载数据
data = pd.read_csv(&#39;data.csv&#39;)

# 分离特征 (X) 和目标变量 (y)
X = data.drop(columns=&#39;label&#39;)
y = data[&#39;label&#39;]

# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 标准化特征
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 定义小网格搜索的参数网格
param_grid = {
&#39;C&#39;: [0.1, 1, 10],
&#39;gamma&#39;: [&#39;scale&#39;, 0.01, 0.1]
}

# 执行带有交叉验证的网格搜索
grid_search = GridSearchCV(SVC(kernel=&#39;rbf&#39;), param_grid, cv=3,scoring=&#39;f1_weighted&#39;, verbose=2, n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)

# 来自网格搜索的最佳参数
best_params = grid_search.best_params_
print(f&#39;Best parameters: {best_params}\n&#39;)

# 训练 SVM 模型使用最佳参数
svm_best = SVC(kernel=&#39;rbf&#39;, C=best_params[&#39;C&#39;], gamma=best_params[&#39;gamma&#39;])
svm_best.fit(X_train_scaled, y_train)

# 对测试集进行预测
y_pred_best = svm_best.predict(X_test_scaled)

# 对改进模型的评估
print(&quot;改进的 SVM 模型评估&quot;)
print(confusion_matrix(y_test, y_pred_best))
print(classification_report(y_test, y_pred_best))
improved_f1 = f1_score(y_test, y_pred_best, average=&#39;weighted&#39;)
print(f&#39;改进的加权 F1 分数： {improved_f1}\n&#39;)


如您所见，我直接使用来自 sklearn 的 SVM 模型。我如何创建一个名为“分类器”的类，它将执行相同的操作并获得相同的结果？可能吗？
提前谢谢 :)
我尝试创建类并使用每个函数的参数，但结果总是更糟。]]></description>
      <guid>https://stackoverflow.com/questions/78843755/building-a-custom-classifier-that-simulates-svm-model</guid>
      <pubDate>Wed, 07 Aug 2024 12:39:48 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“datachain.lib”的模块；“datachain”不是一个包</title>
      <link>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</link>
      <description><![CDATA[
为什么我会遇到 datachain.lib 模块的 ModuleNotFoundError？
我需要采取其他步骤才能在项目中正确使用 datachain 包吗？

我正在开发一个 Python 项目，在尝试导入模块时遇到以下错误：
import os
os.environ[&quot;PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION&quot;] = &quot;python&quot;
import tensorflow as tf
import numpy as np
from PIL import Image
from datachain.lib.dc import Column, DataChain

错误消息：

ModuleNotFoundError：没有名为“datachain.lib”的模块； &#39;datachain&#39; 不是包

详细信息：

我已使用 pip 安装了 datachain：pip install datachain。
运行 pip list 后，datachain 的安装版本为 0.2.18。
运行 pip list 时会列出 datachain 包。
我已验证该包已正确安装并位于我的 Python 环境中。
]]></description>
      <guid>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</guid>
      <pubDate>Wed, 07 Aug 2024 09:53:07 GMT</pubDate>
    </item>
    <item>
      <title>根据面积或宽度提取并替换像素值</title>
      <link>https://stackoverflow.com/questions/78842299/extract-and-replace-pixel-values-based-on-their-area-or-width</link>
      <description><![CDATA[我有一张如下所示的分割图像
车辆边缘有一条非常细的不同颜色的线（人开车的红色区域）。我想通过根据相邻像素分配不同的标签 ID（红色或黑色）来消除这条细线。我知道如何根据所需的颜色甚至 ID 提取像素。但在这种情况下，颜色或 ID 不是固定的，它可能是不同图像中的不同颜色或 ID。我无法想出一种方法来提取这些像素。有人能帮我提取属于细线的像素吗？]]></description>
      <guid>https://stackoverflow.com/questions/78842299/extract-and-replace-pixel-values-based-on-their-area-or-width</guid>
      <pubDate>Wed, 07 Aug 2024 07:17:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 的暹罗网络指南[关闭]</title>
      <link>https://stackoverflow.com/questions/78842114/guidelines-for-siamese-network-using-r</link>
      <description><![CDATA[我的数据库中存储了 20,000 多张艺术品图片（绘画、雕塑、罐子等）。实际作品分布在多个仓库中。理想情况下，实物应该贴上标签（带有其 ID、QR 码等），这些标签是纸质的，因此可能会受损、印刷不良、无法读取、完全丢失甚至放错位置。我的目标是创建一个模型，该模型接收输入（任何仓库的某人发送的图像），从可用数据中识别完全相同的艺术品并返回其 ID、详细信息等。
在我的例子中，样本是静态的、固定的（不会有“新”艺术品，除非客户购买更多），因此模型永远不会“看到”新图像，这让我认为过度拟合可能是模型最理想的结果（这意味着大量的数据增强和大量的 epoch）。
请注意，每个类（艺术作品）只有一个图像可用。这就是无法改变的情况。
所选的编程语言是 R，主要是 tensorflow 和 keras3 库。
话虽如此，我很难找到解决方案，因为每个文档都依赖于相同的 cat vs dogs 或 mnist 数据集。我的问题是：

暹罗网络是否是用于此目的的正确算法？
我可以采取哪些方法来提高准确性？

仅出于测试目的，我抽取了 10 个样本，从每个样本中生成了 9 个其他样本（数据增强、应用旋转、垂直/水平翻转、随机饱和度因子、随机亮度因子等）。后来，为每个类别创建了 5 个正对和 5 个负对。最后，我运行了一个暹罗网络，但准确率似乎停留在 49%。]]></description>
      <guid>https://stackoverflow.com/questions/78842114/guidelines-for-siamese-network-using-r</guid>
      <pubDate>Wed, 07 Aug 2024 06:29:33 GMT</pubDate>
    </item>
    <item>
      <title>什么是代币、Top K 和 Top P？[关闭]</title>
      <link>https://stackoverflow.com/questions/78841275/what-are-tokens-top-k-and-top-p</link>
      <description><![CDATA[我正在学习使用 Google AI Studio，在生成代码片段时，我遇到了这些术语：
constgenerationConfig = {
temperature: 1,
topP: 0.95,
topK: 64,
maxOutputTokens: 8192,
responseMimeType:&quot;text/plain&quot;,
};

我很难理解这些术语的含义。topP、topK 和 maxOutputTokens 是什么。我想了解这些，以便正确使用它们。]]></description>
      <guid>https://stackoverflow.com/questions/78841275/what-are-tokens-top-k-and-top-p</guid>
      <pubDate>Tue, 06 Aug 2024 22:55:28 GMT</pubDate>
    </item>
    <item>
      <title>无法抑制来自 transformers/src/transformers/modeling_utils.py 的警告</title>
      <link>https://stackoverflow.com/questions/78827482/cant-suppress-warning-from-transformers-src-transformers-modeling-utils-py</link>
      <description><![CDATA[我对 AutoModel AutoTokenizer 类的实现相当简单：
from transformers import AutoModel, AutoTokenizer
import numpy as np
from rank_bm25 import BM25Okapi
from sklearn.neighbors import NearestNeighbors

class EmbeddingModels:

def bert(self, model_name, text):
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
input = tokenizer(text, return_tensors=&quot;pt&quot;, truncation=True, padding=True)
output = model(**inputs)
embeddings = output.last_hidden_​​state.mean(dim=1).detach().numpy()
return embeddings

def create_chunks(self, text, chunk_size):
return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

但我无法让这个警告消失：
包含“beta”的参数名称将在内部重命名为“bias”。
请使用其他名称来抑制此警告。
包含“gamma”的参数名称将在内部重命名为“weight”。
请使用其他名称来抑制此警告。

我的存储库中没有任何地方提到 beta 或 gamma 这个词。
更新软件包，使用 import warnings 抑制警告]]></description>
      <guid>https://stackoverflow.com/questions/78827482/cant-suppress-warning-from-transformers-src-transformers-modeling-utils-py</guid>
      <pubDate>Fri, 02 Aug 2024 23:04:25 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 功能 CNN 分类器在 Mac 上 AUC 在 0.5 左右震荡，在 Colab 上学习正常</title>
      <link>https://stackoverflow.com/questions/78825103/tensorflow-functional-cnn-classifier-oscillates-around-0-5-auc-on-mac-learns-no</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78825103/tensorflow-functional-cnn-classifier-oscillates-around-0-5-auc-on-mac-learns-no</guid>
      <pubDate>Fri, 02 Aug 2024 10:56:08 GMT</pubDate>
    </item>
    <item>
      <title>如何优化 PyTorch 和 Ultralytics Yolo 代码以利用 GPU？</title>
      <link>https://stackoverflow.com/questions/78687946/how-to-optimize-pytorch-and-ultralytics-yolo-code-to-utilize-gpu</link>
      <description><![CDATA[我正在做一个涉及对象检测和跟踪的项目。对于对象检测，我使用 yolov8，对于跟踪，我使用 SORT 跟踪器。运行以下代码后，我的 GPU 使用率始终低于 10%，而 CPU 使用率始终超过 40%。我安装了 cuda、cudnn，并使用 cuda 安装了 torch。我还编译了支持 cuda 的 opencv。我正在使用 RTX 4060 ti，但看起来它没有被使用。
有没有办法进一步优化下面的代码，以便所有工作都由 GPU 而不是 CPU 处理？
from src.sort import *
import cv2
import time
import torch
import numpy as np
from ultralytics import YOLO

device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
print(f&quot;Using device: {device}&quot;)
sort_tracker = Sort(max_age=20, min_hits=2, iou_threshold=0.05)
model = YOLO(&#39;yolov8s.pt&#39;).to(device)

cap = cv2.VideoCapture(0)

while True:
ret, frame = cap.read() 
if not ret:
print(&quot;**未收到帧**&quot;)
继续

results = model(frame)
dets_to_sort = np.empty((0, 6))
for result in results:
for obj in result.boxes:
bbox = obj.xyxy[0].cpu().numpy().astype(int)
x1, y1, x2, y2 = bbox

conf = obj.conf.item()
class_id = int(obj.cls.item())
dets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, class_id])))

tracked_dets = sort_tracker.update(dets_to_sort)
for det in tracked_dets:
x1, y1, x2, y2 = [int(i) for i in det[:4]]
track_id = int(det[8]) if det[8] 不为 None else 0
class_id = int(det[4])
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
cv2.putText(frame, f&quot;{track_id}&quot;, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)

frame = cv2.resize(frame, (800, int(frame.shape[0] * 800 / frame.shape[1])), interpolation=cv2.INTER_NEAREST)
cv2.imshow(&quot;Frame&quot;, frame)
key = cv2.waitKey(1)
如果 key == ord(&quot;q&quot;):
break
如果 key == ord(&quot;p&quot;):
cv2.waitKey(-1)

cap.release()
cv2.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/78687946/how-to-optimize-pytorch-and-ultralytics-yolo-code-to-utilize-gpu</guid>
      <pubDate>Sun, 30 Jun 2024 07:43:52 GMT</pubDate>
    </item>
    </channel>
</rss>