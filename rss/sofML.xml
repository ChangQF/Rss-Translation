<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 24 May 2024 21:14:27 GMT</lastBuildDate>
    <item>
      <title>作为初学者，我应该使用什么机器学习框架来构建 React 中的推荐系统？</title>
      <link>https://stackoverflow.com/questions/78530720/what-machine-learning-framework-should-i-be-using-for-a-recommender-system-in-re</link>
      <description><![CDATA[我最近才开始学习 React 和少量的机器学习，但作为 A Level 计算机科学项目的一部分，我想实现一个推荐系统，但我不知道该去哪里。我在tensorflow.js和brain.js上进行了搜索，并试图理解并决定我可能使用什么，但我还没有真正找到我理解的解释。
我想要明确解释的问题是：

什么机器学习库最适合像我这样的初学者的推荐系统？
我怎样才能学习如何使用该库？
如果这对我的项目造成太大的时间限制（我只有到四月份的时间），那么我该如何调整现有的推荐系统以适应我的系统？
]]></description>
      <guid>https://stackoverflow.com/questions/78530720/what-machine-learning-framework-should-i-be-using-for-a-recommender-system-in-re</guid>
      <pubDate>Fri, 24 May 2024 21:13:20 GMT</pubDate>
    </item>
    <item>
      <title>机器学习算法</title>
      <link>https://stackoverflow.com/questions/78530580/machine-learning-algos</link>
      <description><![CDATA[哪种机器学习算法最适合稀疏数据集的文本分类任务？
我尝试过的：
我进行了一项实验，比较了三种流行的监督学习算法的性能：朴素贝叶斯、支持向量机 (SVM) 和随机森林，用于文本分类任务。
我预期会发生什么：
根据之前的研究和理论理解，我预计 SVM 表现最好，其次是随机森林，然后是朴素贝叶斯。我预计 SVM 能够有效处理文本数据中典型的高维特征空间，并提供稳健的分类边界。由于随机森林能够处理噪声和不相关的特征，因此预计会表现良好。朴素贝叶斯虽然简单，但预计可以提供基准性能。
实际结果：
令人惊讶的是，朴素贝叶斯在分类精度方面优于 SVM 和随机森林。这个结果是出乎意料的，特别是考虑到文本数据的高维性和稀疏性。进一步的分析表明，朴素贝叶斯的简单性使其能够很好地推广到数据集，而支持向量机和随机森林分别面临过度拟合和特征选择问题。然而，朴素贝叶斯在处理不平衡类方面表现不佳，这可能会限制其在某些场景下的适用性。
这一意外结果凸显了实证评估的重要性，并表明机器学习算法的性能可能会根据数据集和手头任务的具体特征而有所不同。]]></description>
      <guid>https://stackoverflow.com/questions/78530580/machine-learning-algos</guid>
      <pubDate>Fri, 24 May 2024 20:36:02 GMT</pubDate>
    </item>
    <item>
      <title>wandb 的超带算法在哪些时期检查改进？</title>
      <link>https://stackoverflow.com/questions/78530549/at-which-epochs-does-the-hyperband-algorithm-of-wandb-checks-for-improvement</link>
      <description><![CDATA[我正在尝试做什么：-
我正在尝试使用 wandb 库的超参数调整（又名扫描）功能（链接到其官方页面）。我正在尝试应用贝叶斯超频带算法。
现在，正如这些页面中提到的（如何定义扫描配置), (与提前终止选项相关的参数是什么 ），在提前终止下我们必须提到4个参数（通常），它们是min_iter，s，eta和max_iter，它看起来像下面这样。
#__________________________________________________________________________________________________________________
我的疑问总结：-
总而言之，我想知道的是，
给定所有 4 个：- min_iter、s、eta 和 max_iter

超频带算法将在哪个时期检查改进？

考虑到我正在尝试进行贝叶斯超带，将在第一个括号中评估多少次运行，以及在连续括号中将评估多少次运行？

是否有任何方法或经验法则来决定这 4 个参数（min_iter、s、eta 和 max_iter）采用什么值比较合适？

请更详细地解释参数 s 和 eta（尤其是 eta），即使用一些基础数学知识（如果可能，请保持简单）。


#__________________________________________________________________________________________________________________
我的疑问是什么？ （更详细/上下文地解释）：-
（这里），他们有一些&lt; /em&gt; 解释了超频带算法在哪个时期（他们的）实现检查改进并决定是否终止运行。
当我们只关心每次运行的最小迭代次数时
当我们只关心每次运行的最小迭代次数时
但是当我们同时关心每次运行的最小和最大迭代次数时该怎么办？
就像下面这个...
early_terminate：
  类型：超频带
  分钟迭代数：10
  秒：3
  预计时间：4
  最大迭代数：50

#__________________________________________________________________________________________________________________
我已经尝试过：-
我什至尽力阅读原始论文并尝试了解正在发生的事情（或可能发生的事情）（超频带算法原始论文链接），但未能得到满意的答案。
我什至尝试访问他们的 github 页面，那里有示例，但他们只展示了如何编写配置，没有深入解释它的作用。]]></description>
      <guid>https://stackoverflow.com/questions/78530549/at-which-epochs-does-the-hyperband-algorithm-of-wandb-checks-for-improvement</guid>
      <pubDate>Fri, 24 May 2024 20:25:42 GMT</pubDate>
    </item>
    <item>
      <title>PPO 仅适用于单个 epoch 和未剪裁的损失</title>
      <link>https://stackoverflow.com/questions/78530486/ppo-only-working-with-a-single-epoch-and-unclipped-loss</link>
      <description><![CDATA[我正在尝试实现 PPO 来击败 cartpole-v2，如果我将事情保持为 A2C（即，没有剪切损失和单个纪元），当我使用剪切损失和多个 epoch 时，我设法让它工作它没有学习纪元，大约一周以来一直试图在我的实现中找到问题，但我找不到问题所在。
完整代码
这是负责优化的函数：
def finish_episode():
    # 计算损失并执行反向传播
    R = 0
    已保存的动作 = actor.已保存的动作
    返回 = []
    ε = 0.3
    num_epochs = 1 # 当 num_epochs 大于 1 时我的网络将无法学习

    对于 actor.rewards[::-1] 中的 r：
        R = r + 0.99 * R # 伽玛值为 0.99
        返回.插入(0, R)
    返回= torch.tensor（返回，设备=设备）
    返回 = (返回 - returns.mean()) / (returns.std() + eps)

    old_probs、state_values、状态、actions = zip(*saved_actions)

    old_probs = torch.stack(old_probs).to(设备)
    state_values = torch.stack(state_values).to(设备)
    states = torch.stack(states).to(device)
    actions = torch.stack(actions).to(device)

    优点 = 回报 - state_values.squeeze()

    对于范围内的纪元（num_epochs）：

        new_probs = actor(states).gather(1, actions.unsqueeze(-1)).squeeze()

        比率 = 新概率 / 旧概率

        surr1 = 比率 * 优势
        surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * 优点

        #actor_loss = -torch.min(surr1, surr2).mean() # 当使用这个（剪辑的）损失时，我的网络将无法学习
        actor_loss = -surr1.mean()

        actor_optimizer.zero_grad()
        actor_loss.backward(retain_graph=True)
        actor_optimizer.step()

        如果纪元 == num_epochs - 1：
            ritic_loss = F.smooth_l1_loss(state_values.squeeze(), 返回)
            
            批评家优化器.zero_grad()
            ritic_loss.backward(retain_graph=False)
            批评家优化器.step()

    删除演员.奖励[:]
    删除 actor.saved_actions[:]

尝试了不同的超参数，使用 gae 而不是完整的蒙特卡洛重新调整/优点，在梳理我的代码时我看不出有什么问题。]]></description>
      <guid>https://stackoverflow.com/questions/78530486/ppo-only-working-with-a-single-epoch-and-unclipped-loss</guid>
      <pubDate>Fri, 24 May 2024 20:05:01 GMT</pubDate>
    </item>
    <item>
      <title>即使我在高维空间中工作，KNN 的 _fit_method 也会给出 KD 树</title>
      <link>https://stackoverflow.com/questions/78530448/fit-method-for-knn-gives-kd-tree-even-though-im-working-in-a-high-dimensional</link>
      <description><![CDATA[因此，由于 sklearn 中的 KNeighborsClassifier 类在使用 auto （默认）时根据 fit 方法的值找到最佳算法，因此当使用 ._fit_method 访问算法时，我得到的 kd 树没有任何意义，因为我的维度数据太高了
我有 32 个维度和 49720032 个训练数据点，不满足方程：
N&gt;2^k
可能的原因是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78530448/fit-method-for-knn-gives-kd-tree-even-though-im-working-in-a-high-dimensional</guid>
      <pubDate>Fri, 24 May 2024 19:53:33 GMT</pubDate>
    </item>
    <item>
      <title>Vision Transformer 模型训练和验证准确度停留在 50</title>
      <link>https://stackoverflow.com/questions/78530442/vision-transformer-model-training-and-validation-accuracy-stuck-at-50</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78530442/vision-transformer-model-training-and-validation-accuracy-stuck-at-50</guid>
      <pubDate>Fri, 24 May 2024 19:51:18 GMT</pubDate>
    </item>
    <item>
      <title>如何处理推荐系统中的cod邮政功能？</title>
      <link>https://stackoverflow.com/questions/78529077/how-to-deal-with-cod-postal-feature-in-recommendation-systems</link>
      <description><![CDATA[我有一个带有邮政编码列的数据集。它们具有一定的意义，我想将其用作一项功能。我正处于预处理阶段，但仍然不确定我将使用的算法。
我需要有关使用邮政编码列作为功能的最佳方法的建议。
提前致谢！！
我发现你可以使用 one-hot 编码，但就我而言，我正在处理大量唯一的邮政编码（例如 3513 个不同的邮政编码），one-hot 编码（或虚拟编码）是不切实际的，因为它引入的高维度。]]></description>
      <guid>https://stackoverflow.com/questions/78529077/how-to-deal-with-cod-postal-feature-in-recommendation-systems</guid>
      <pubDate>Fri, 24 May 2024 14:06:08 GMT</pubDate>
    </item>
    <item>
      <title>制作简单天气预报模型的指南[关闭]</title>
      <link>https://stackoverflow.com/questions/78528754/guide-on-making-a-simple-weather-prediction-model</link>
      <description><![CDATA[我想制作一个简单的天气预报模型，根据过去的数据（我拥有的数据）进行训练，并预测未来的一些天气情况，例如温度、最低温度、最高温度和降水量。我怎样才能做到呢？我有一个很大的天气数据集：有关数据集的所有功能/详细信息
我尝试在互联网上搜索，但到目前为止没有答案。
我的问题是，哪种机器学习模型最适合使用，但我不确定。
我还想知道天气预报最好由机器学习来完成。
您可以查看我的数据特征 ((https://i.sstatic.net/OFQTMn18.png)) 这对机器学习训练有好处吗？
感谢您提前提供的帮助，非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/78528754/guide-on-making-a-simple-weather-prediction-model</guid>
      <pubDate>Fri, 24 May 2024 13:04:56 GMT</pubDate>
    </item>
    <item>
      <title>无法在 ML.net 中加载动态数据</title>
      <link>https://stackoverflow.com/questions/78528640/unable-to-load-dynamic-data-in-ml-net</link>
      <description><![CDATA[我的输入数据为List&gt;
我想将此数据作为 Enumerable 加载到 MLContext
var mlContext = new MLContext();

mlContext.Data.LoadFromEnumerable(inputData);

我也无法为输入数据预定义一个强静态类，因为每次数据列都会发生变化
在 dataView 中，加载时我得到的列架构为 0
在转换时，我没有发现任何列异常
我尝试过创建动态强类型类
生成的架构定义并加载数据
我想使用加载的数据转换列]]></description>
      <guid>https://stackoverflow.com/questions/78528640/unable-to-load-dynamic-data-in-ml-net</guid>
      <pubDate>Fri, 24 May 2024 12:43:35 GMT</pubDate>
    </item>
    <item>
      <title>如何调整法学硕士以给出完整且详细的答案</title>
      <link>https://stackoverflow.com/questions/78528561/how-to-tune-llm-to-give-full-length-and-detailed-answers</link>
      <description><![CDATA[我正在构建一个应用程序，您可以在其中从模型列表中选择一个开源模型并询问一般问题。我正在使用 searchxng 在网络上搜索上下文。虽然所有这些都运行良好并且我能够得到结果，但我无法得到详细或完整的答案。例如，如果我问谁是 2007 年 f1 世界冠军，我得到的答案是莱科宁。
我希望我的答案结构正确。我理想的答案是“2007 年 F1 世界冠军是基米·莱科宁”。
我以这种方式使用拥抱面变压器管道：
model_name = Question.model
问题答案=管道（
“问答”，
模型=AutoModelForQuestionAnswering.from_pretrained(model_name),
tokenizer=AutoTokenizer.from_pretrained(model_name),
device=0 # 使用 GPU（如果可用）
）

响应=question_answerer（问题=question.question，上下文=summarized_content，batch_size=16）

返回响应

目前，我使用的是 deepset/roberta-base-squad2 模型
我尝试向模型发送大量上下文，希望得到详细的答案。我也尝试了很多不同的模型，但得到了相似的结果]]></description>
      <guid>https://stackoverflow.com/questions/78528561/how-to-tune-llm-to-give-full-length-and-detailed-answers</guid>
      <pubDate>Fri, 24 May 2024 12:29:17 GMT</pubDate>
    </item>
    <item>
      <title>进行超参数调整后浅层卷积神经网络中的过度拟合</title>
      <link>https://stackoverflow.com/questions/78528272/overfitting-in-a-shallow-convolutional-neural-network-after-doing-hyper-paramete</link>
      <description><![CDATA[我正在 Google Colab 中研究用于 3 类分类问题的 CNN 模型。我有一个平衡类数据集，其中包含 1680 个训练样本（每个 560 个）和 420 个测试样本（每个 140 个）。我构建了一个浅层网络来训练我的数据。但是我的模型在 7/8 时期后过度拟合。我已经应用了所有可能的技术，如 Dropout、l2 正则化、数据增强、Batch Normalization、降低学习率，但模型性能没有任何变化。
我在 30 个时期实现了损失：0.7746 - 准确度：0.6673 - val_loss：0.8310 - val_accuracy：0.6071。
#模型架构
模型 = models.Sequential()
model.add(layers.Conv2D(8, (3, 3), 激活=&#39;relu&#39;, input_shape=(28, 28, 3)))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(16, (3, 3), 激活=&#39;relu&#39;))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.5))
model.add(layers.Flatten())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(3,activation=&#39;softmax&#39;))

模型.summary()

型号：“sequential_4”
_________________________________________________________________
 层（类型）输出形状参数#   
=================================================== ===============
 conv2d_8（Conv2D）（无、26、26、8）224       
                                                                 
 batch_normalization_8（蝙蝠（无、26、26、8）32        
 ch归一化）                                                
                                                                 
 max_pooling2d_4（最大池化（无、13、13、8）0         
 g2D)                                                            
                                                                 
 conv2d_9（Conv2D）（无、11、11、16）1168      
                                                                 
 batch_normalization_9（蝙蝠（无、11、11、16）64        
 ch归一化）                                                
                                                                 
 max_pooling2d_5（最大池化（无、5、5、16）0         
 g2D)                                                            
                                                                 
 dropout_7（辍学）（无、5、5、16）0         
                                                                 
 flatten_4（压平）（无，400）0         
                                                                 
 dropout_8（辍学）（无，400）0         
                                                                 
 密集_5（密集）（无，3）1203      
                                                                 
=================================================== ===============
总参数：2691 (10.51 KB)
可训练参数：2643 (10.32 KB)
不可训练参数：48（192.00 字节）

learning_rate=0.001、batch_size=8、optimizer=Adam、loss=&#39;categorical_crossentropy&#39; 和
target_size=(28,28)
分类报告
                     精确召回率 f1-score 支持

中等耐受 0.33 0.66 0.43 140
        敏感 0.33 0.13 0.18 140
           耐受 0.39 0.23 0.29 140

           精度 0.34420
          宏观平均 0.35 0.34 0.30 420
       加权平均 0.35 0.34 0.30 420

尽管我在中等容忍类别中具有较高的 f1 分数，但我的模型仍将看不见的图像预测为易感类别。以下是训练和验证准确性和损失的图表。


任何人都可以帮助我理解这背后的原因，并帮助我改进我的模型，以便我的模型能够很好地泛化。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78528272/overfitting-in-a-shallow-convolutional-neural-network-after-doing-hyper-paramete</guid>
      <pubDate>Fri, 24 May 2024 11:29:02 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的 .predict() 给出属性错误</title>
      <link>https://stackoverflow.com/questions/78527509/predict-in-python-gives-an-attribute-error</link>
      <description><![CDATA[def train_model（x_train，y_train，dropout_prob，lr，batch_size，epochs）：
    nn_model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, 激活=&#39;relu&#39;, input_shape=(10,)),
        tf.keras.layers.Dropout(dropout_prob),
        tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
        tf.keras.layers.Dropout(dropout_prob),
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
        ]）

    nn_model.compile(keras.optimizers.Adam(lr),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

    历史记录 = nn_model.fit(
        x_train,y_train,epochs=epochs,batch_size=batch_size,validation_split=0.2,verbose=0
    ）

    情节历史（历史）

    返回 nn_model，历史记录


最小损失模型 = train_model(x_train, y_train, 0.2, 0.005, 128, 100)
预测=least_loss_model.predict(x_test)
打印（预测）

这给出了以下属性错误：
回溯（最近一次调用最后一次）：
  文件“C:\Users\~\ai.py”，第 162 行，在  中
    预测=least_loss_model.predict(x_test)
                ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError：“元组”对象没有属性“预测”

我已经尝试过predicted=least_loss_model.predict_proba(x_test)]]></description>
      <guid>https://stackoverflow.com/questions/78527509/predict-in-python-gives-an-attribute-error</guid>
      <pubDate>Fri, 24 May 2024 09:02:15 GMT</pubDate>
    </item>
    <item>
      <title>Prophet 1.1.5 模型在通过 model_to_json 和 model_from_json 保存和加载后损坏</title>
      <link>https://stackoverflow.com/questions/77949186/prophet-1-1-5-model-is-corrupted-after-being-saved-and-loaded-via-model-to-json</link>
      <description><![CDATA[当模型拟合然后通过 model_to_json 和 model_from_json 保存和加载时，它不会提供准确的预测。如果跳过保存和加载过程并在拟合后立即使用模型进行预测，则不会出现问题。
加载 Prophet 模型后，显示 1970 年的所有数据相隔 1 秒，而不是 2023/24 相隔 15 分钟/1 小时。预测数据最终也以 1970 年代相隔 1 秒 (make_future_dataframe)。此外，预测数据往往不太准确。
当拟合和预测结合在一起时，绕过保存和加载阶段，Prophet 的行为符合预期。仅当我将模型保存到云存储桶或文件系统中时，才会出现此问题。
我比较了plot命令的输出。在保存和加载之前，其输出是准确的并且符合预期。
我怎样才能保存和保存？加载模型而不损坏模型？建议采取哪些故障排除步骤？]]></description>
      <guid>https://stackoverflow.com/questions/77949186/prophet-1-1-5-model-is-corrupted-after-being-saved-and-loaded-via-model-to-json</guid>
      <pubDate>Tue, 06 Feb 2024 16:06:33 GMT</pubDate>
    </item>
    <item>
      <title>检测视频中的手部方向（旋转）</title>
      <link>https://stackoverflow.com/questions/75908800/detect-hand-orientation-rotation-in-a-video</link>
      <description><![CDATA[我想检测视频中我的手的旋转，但我仍然不知道如何正确地做到这一点。
我尝试使用 PCA 方法，但它仅适用于图像，不适用于视频。
我可以正确检测到手及其地标]]></description>
      <guid>https://stackoverflow.com/questions/75908800/detect-hand-orientation-rotation-in-a-video</guid>
      <pubDate>Sat, 01 Apr 2023 20:54:44 GMT</pubDate>
    </item>
    <item>
      <title>跨 MRI 切片的最大池化</title>
      <link>https://stackoverflow.com/questions/67124432/max-pooling-across-mri-slices</link>
      <description><![CDATA[我正在尝试实现用于 MRI 扫描诊断的机器学习模型。
我有形状为 (x, 256, 256, 3) 的输入，其中有 3 个颜色通道，其中 x 是序列中的切片数量。
我阅读了 MRNet 论文，我想实施TensorFlow Keras 中的类似架构。我不想使用 AlexNet 特征提取器，而是使用 VGG16。
论文中的模型架构：
&lt;块引用&gt;
我们的预测系统的主要构建模块是 MRNet，一个卷积网络
神经网络 (CNN) 将 3 维 MRI 系列映射到概率 [15]（图 2）。这
MRNet 的输入尺寸为 s × 3 × 256 × 256，其中 s 是 MRI 中的图像数量
系列（3 是颜色通道数）。首先，将每个二维 MRI 图像切片
通过基于AlexNet的特征提取器，获得包含每个切片特征的s×256×7×7张量。然后应用全局平均池化层将这些特征减少到 s × 256。然后我们跨切片应用最大池化以获得 256 维
向量，它被传递到全连接层和 sigmoid 激活函数
获得 0 到 1 范围内的预测。

到目前为止一切顺利。我有一个顺序模型，第一步添加特征提取器，然后应用 GlobalAveragePooling2D() 将特征减少到形状 (x, 512)。然后我必须在切片上使用 MaxPool，但我没有办法解决这个问题。
feature_extractor = VGG16(weights=&#39;imagenet&#39;, include_top=False, input_shape=(256, 256, 3))
模型=顺序（）
model.add(feature_extractor) #输出形状: (x, 8, 8, 512)
model.add(GlobalAveragePooling2D()) #输出形状：(x, 512)
# 这里我必须在切片上添加一个层女巫池。
model.add( ) #输出形状(1, 512)

model.add（密集（1，激活=&#39;sigmoid&#39;））
model.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

示例 Scan 的形状为 (44, 256, 256, 3)。当它运行 VGG16 时，其特征的维度为 (44, 8, 8, 512)。经过 GlobalAverage Pooling 后，我得到了 (44, 512)。然后必须以某种方式将该二维数组转换为 (1, 512) 的形状。我的意思是，如果我对一个简单的 2-D NumPy 数组进行操作，我需要一个像 np.max 这样的函数在 0 轴上
np.max(x, 轴=0)

也许你可以给我一个提示或对此有一个方法。
非常感谢您的帮助:)
################################################ ##################################
编辑：2021年5月1日
我尝试了你的方法@Aaron Keesing，但是拟合模型根本不会以某种方式训练它。 25 个 epoch 后，我仍然具有相同的准确性。准确度是我的 2 个类别的分布（我只是在冠状平面上进行训练并且异常）

在这个案例中，例如我有 500 个案例，其中 80% 的案例确实有异常，20% 的案例没有。
# 数据集训练，总共 500 个案例
绝对：
 半月板异常
1 0 0 184
               1 118
0 0 0 100
1 1 1 63
               0 35
数据类型：int64
相对的：
 半月板异常
1 0 0 0.368
               1 0.236
0 0 0 0.200
1 1 1 0.126
               0 0.070

#################################################### #########
# 数据集有效，总共100个案例
绝对：
 半月板异常
1 1 1 27
0 0 0 25
1 1 0 23
          0 0 20
               1 5
数据类型：int64
相对的：
 半月板异常
1 1 1 0.27
0 0 0 0.25
1 1 0 0.23
          0 0 0.20
               1 0.05
]]></description>
      <guid>https://stackoverflow.com/questions/67124432/max-pooling-across-mri-slices</guid>
      <pubDate>Fri, 16 Apr 2021 11:31:24 GMT</pubDate>
    </item>
    </channel>
</rss>