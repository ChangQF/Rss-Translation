<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 28 Nov 2024 09:19:20 GMT</lastBuildDate>
    <item>
      <title>在 Python 中运行贝叶斯寻找最佳超参数时出错</title>
      <link>https://stackoverflow.com/questions/79233155/error-while-running-bayesian-for-finding-best-hyperparameter-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79233155/error-while-running-bayesian-for-finding-best-hyperparameter-in-python</guid>
      <pubDate>Thu, 28 Nov 2024 08:53:05 GMT</pubDate>
    </item>
    <item>
      <title>我已经参加了几门 Python 入门课程，但现在我不知道该如何继续学习 [关闭]</title>
      <link>https://stackoverflow.com/questions/79233021/i-have-taken-several-introduction-python-courses-but-now-i-am-stuck-in-how-to-p</link>
      <description><![CDATA[我想进入科技行业，但我的大学学位侧重于跨学科课程，而且我可以选修的编程课程并不多。
所以，几个月前，我决定在线学习编程，并且对机器学习特别感兴趣。
我开始学习 Coursera 上的 Python for Everybody、Python 和数据结构课程，然后我学习了如何使用 LLM 进行基本的 Python 自动化。到目前为止，我已经在 Python 上创建了计算器 GUI 和待办事项列表项目。
但是，我仍然很难解决 Hacker Rank 上的基本编程问题，更不用说 LeetCode 了。所以，我决定回去再学一次基础知识。最近，我完成了 Kaggle 编程入门和 Python 课程，你猜怎么着，我仍然很难解决简单级别的编程问题。
我觉得自己陷入困境，不知道如何继续我的编码之旅。我也在想，也许我必须先从 FCC 或 Odin 项目开始，然后最终在 Kaggle 上学习更多关于 ML 的知识。
有谁能给我一些建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79233021/i-have-taken-several-introduction-python-courses-but-now-i-am-stuck-in-how-to-p</guid>
      <pubDate>Thu, 28 Nov 2024 08:00:04 GMT</pubDate>
    </item>
    <item>
      <title>我想训练一个简单的人工智能模型用于实践。它可以非常简单。对我来说，最好的操作步骤是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79232599/i-want-to-train-a-simple-ai-model-for-practise-purposes-it-can-be-very-simple</link>
      <description><![CDATA[我是一名大学生，我想训练一个简单的人工智能模型，只是为了好玩。它可以像绘制几个点并估计与图相对应的函数一样简单。
我有编程和计算机硬件的知识基础。我只是对构建一个简单的人工智能模型的步骤感到困惑，以便它可以训练输入数据并输出它所学到的东西。我听说Python是人工智能的语言。设置Python环境后我应该做什么？是否有某个网站上的AI模板可供我使用？我在Youtube上搜索过，但没有很多关于从头开始训练人工智能模型的有用信息。如果您能给我一些建议，我将不胜感激。
我想学习构建一个简单的人工智能模型的过程，该模型可以通过输入数据进行训练并输出一些东西。]]></description>
      <guid>https://stackoverflow.com/questions/79232599/i-want-to-train-a-simple-ai-model-for-practise-purposes-it-can-be-very-simple</guid>
      <pubDate>Thu, 28 Nov 2024 04:32:11 GMT</pubDate>
    </item>
    <item>
      <title>从 PHP 脚本调用时，Flask API 始终预测相同的类</title>
      <link>https://stackoverflow.com/questions/79232596/flask-api-always-predicts-the-same-class-when-called-from-php-script</link>
      <description><![CDATA[我使用 Flask 开发了一个 API，用于集成一个在胸部 X 光片图像上训练的深度学习模型，用于肺炎检测。为了测试 PHP 和 Flask API 之间的连接，我成功使用了 Fashion MNIST 模型（用于服装分类），并且该模型可以正确预测类别。
但是，当我尝试使用我的自定义训练模型进行肺炎检测（在评估期间表现非常好）时，我注意到即使图像处于“正常”状态，该模型也始终预测类别“肺炎”类。
模型性能（在测试集上）：
测试损失：11.41%
测试准确率：97.10%
测试精度：97.46%
测试召回率：98.60%
测试AUC：98.33%
采取的步骤：
训练：我使用 ResNet50 层和附加自定义层训练模型。
测试：我在包含正常和肺炎图像的测试集上测试了模型，它表现良好。
通过 Flask API 进行测试：我尝试通过 PHP 将图像发送到 Flask API，但模型始终预测“肺炎”。
Flask API 代码：
从 flask 导入 Flask、请求、jsonify
从 tensorflow 导入keras
import numpy as np
from PIL import Image

app = Flask(__name__)

# 加载模型
model = keras.models.load_model(&#39;path_to_your_model&#39;)

# 类名
class_names = [&#39;NORMAL&#39;, &#39;PNEUMONIA&#39;]

@app.route(&#39;/predict&#39;, methods=[&#39;POST&#39;])
def predict():
if &#39;image&#39; not in request.files:
return jsonify({&#39;error&#39;: &#39;No image file provided&#39;}), 400

image = request.files[&#39;image&#39;]

try:
# 将图像转换为 RGB 并调整其大小
img = Image.open(image).convert(&#39;RGB&#39;)
img = img.resize((224, 224))

# 将图像转换为数组并扩展维度以匹配模型输入
img_array = np.array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0) # 添加批次维度

# 进行预测
prediction = model.predict(img_array)

# 获取预测类别
predict_class = class_names[int(prediction[0] &gt; 0.5)]

return jsonify({
&#39;prediction&#39;: predict_class,
&#39;probability&#39;: float(prediction[0])
})
except Exception as e:
return jsonify({&#39;error&#39;: f&quot;An error occurred: {str(e)}&quot;}), 500

if __name__ == &#39;__main__&#39;:
app.run(debug=True)


将图像发送到 Flask API 的 PHP 代码：
&lt;?php
if (isset($_POST[&#39;submit&#39;])) {
if (isset($_FILES[&#39;img&#39;]) &amp;&amp; $_FILES[&#39;img&#39;][&#39;error&#39;] == 0) {
$image = $_FILES[&#39;img&#39;][&#39;name&#39;];
$image_tmp_name = $_FILES[&#39;img&#39;][&#39;tmp_name&#39;];
$folder = &#39;uploaded_img/&#39;;
$image_folder = $folder . $image;

if (!is_dir($folder)) {
mkdir($folder, 0777, true);
}

if (move_uploaded_file($image_tmp_name, $image_folder)) {
$url = &#39;http://localhost:5000/predict&#39;;
$cfile = new CURLFile($image_folder, &#39;image/jpeg&#39;, $image);
$data = array(&#39;image&#39; =&gt; $cfile);

$ch = curl_init();
curl_setopt($ch, CURLOPT_URL, $url);
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, $data);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($ch);
curl_close($ch);

if ($response === false) {
echo &quot;预测错误！&quot;;
} else {
$result = json_decode($response, true);
echo &quot;预测：&quot; . $result[&#39;prediction&#39;];
}
}
}
}
?&gt;


问题：尽管该模型在本地测试时运行良好，并且在 NORMAL 和 PNEUMONIA 图像上具有良好的性能指标，但当我通过 Flask API（通过 PHP）对其进行测试时，该模型始终预测 &quot;PNEUMONIA&quot;，即使图像来自 &quot;NORMAL&quot;类。
我发送图像的方式或 Flask API 处理图像的方式是否存在问题？]]></description>
      <guid>https://stackoverflow.com/questions/79232596/flask-api-always-predicts-the-same-class-when-called-from-php-script</guid>
      <pubDate>Thu, 28 Nov 2024 04:30:21 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中的 GaussianProcessRegressor 估计器可以在多核上并行化吗？</title>
      <link>https://stackoverflow.com/questions/79232519/is-the-gaussianprocessregressor-estimator-in-scikit-learn-able-to-be-parallelize</link>
      <description><![CDATA[在具有 8 个内核（16 个线程）的机器上使用 GaussianProcessRegressor 时，我没有注意到任何性能改进，尽管我只使用物理内核。所以我想知道，sklearn.gaussian_process 中的 GaussianProcessRegressor 类是否能够利用多个处理器/内核/线程？
#当前场景
4 个内核情况下的时间：0.57
8 个内核情况下的时间：0.56
加速不明显。这次只是将 fit_transform 作用于数据块。因此没有计时开销。]]></description>
      <guid>https://stackoverflow.com/questions/79232519/is-the-gaussianprocessregressor-estimator-in-scikit-learn-able-to-be-parallelize</guid>
      <pubDate>Thu, 28 Nov 2024 03:36:56 GMT</pubDate>
    </item>
    <item>
      <title>通过建模预测需求</title>
      <link>https://stackoverflow.com/questions/79232517/demand-prediction-through-modeling</link>
      <description><![CDATA[在此处输入图片说明我的公司正在开展一个按客户预测需求的项目。客户分为 6 个行业组，我们有按客户划分的过去需求数据。当我按行业组检查模式时，结果如下，由于确定了特定模式，我对数据进行了预处理（时间序列分解），似乎可以看到特定的趋势和周期。由于残差不是随机的，我认为我需要添加解释变量，但即使我添加变量，我也不认为我可以使其完全随机。因为它是时间序列数据，并且需求模式的特征因行业组而异，所以我想按行业组进行拆分并应用每个模型。我正在考虑将 SARIMAX 作为候选模型，
问题是：

为什么 LSTM 不适合这种情况
我应该如何预处理和建模？
我还应该考虑哪些其他因素？

2 年的需求模式和时间序列分解（趋势、季节性）
在此处输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/79232517/demand-prediction-through-modeling</guid>
      <pubDate>Thu, 28 Nov 2024 03:35:48 GMT</pubDate>
    </item>
    <item>
      <title>为什么有些模型架构使用加法运算符而不是减法，反之亦然？</title>
      <link>https://stackoverflow.com/questions/79232424/why-do-some-model-architectures-use-the-addition-operator-instead-of-subtraction</link>
      <description><![CDATA[为什么有些模型架构使用加法运算符而不是减法运算符，反之亦然？例如，在 ResNet 中，更改运算符是否会影响模型（F(x) + x -&gt; F(x) - x）？模型是否只需通过翻转符号就可以轻松学习？]]></description>
      <guid>https://stackoverflow.com/questions/79232424/why-do-some-model-architectures-use-the-addition-operator-instead-of-subtraction</guid>
      <pubDate>Thu, 28 Nov 2024 02:39:18 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Hugging Face Trainer 或 SFT Trainer 中记录第零步的训练损失？</title>
      <link>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</link>
      <description><![CDATA[我正在使用 Hugging Face Trainer（或 SFTTrainer）进行微调，我想在步骤 0（在执行任何训练步骤之前）记录训练损失。我知道有一个用于评估的 eval_on_start 选项，但我找不到在训练开始时记录训练损失的直接等效方法。
是否有办法使用 Trainer 或 SFTTrainer 在步骤 0（在任何更新之前）记录初始训练损失？理想情况下，我希望使用类似于 eval_on_start 的方法。
以下是我迄今为止尝试过的方法：
解决方案 1：自定义回调
我实现了自定义回调，以在训练开始时记录训练损失：
from transformers import TrainerCallback

class TrainOnStartCallback(TrainerCallback):
def on_train_begin(self, args, state, control, logs=None, **kwargs):
# 在步骤 0 记录训练损失
logs = logs or {}
logs[&quot;train/loss&quot;] = None # 如果可用，用初始值替换 None
logs[&quot;train/global_step&quot;] = 0
self.log(logs)

def log(self, logs):
print(f&quot;Logging at start: {logs}&quot;)
wandb.log(logs)

# 将回调添加到 Trainer
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
train_dataset=train_dataset,
eval_dataset=eval_dataset,
args=training_args,
optimizers=(optimizer, scheduler),
callbacks=[TrainOnStartCallback()],
)

这有效，但感觉有点过头了。它会在训练开始时记录任何步骤之前的指标。
解决方案 2：手动记录
或者，我在开始训练之前手动记录训练损失：
wandb.log({&quot;train/loss&quot;: None, &quot;train/global_step&quot;: 0})
trainer.train()

问题：
Trainer 或 SFTTrainer 中是否有任何内置功能可以在第 0 步记录训练损失？或者自定义回调或手动记录是这里的最佳解决方案吗？如果是这样，是否有更好的方法来实现此功能？与 eval_on_start 类似，但 train_on_start？
交叉：https://discuss.huggingface.co/t/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer/128188]]></description>
      <guid>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</guid>
      <pubDate>Thu, 28 Nov 2024 00:23:35 GMT</pubDate>
    </item>
    <item>
      <title>如何摆脱 Unity ML 中找不到类型或命名空间名称“Keypoint”的错误</title>
      <link>https://stackoverflow.com/questions/79231502/how-to-get-rid-of-the-type-or-namespace-name-keypoint-could-not-be-found-error</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79231502/how-to-get-rid-of-the-type-or-namespace-name-keypoint-could-not-be-found-error</guid>
      <pubDate>Wed, 27 Nov 2024 18:36:11 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么 ML/优化算法来找到最大输出的最佳输入值？[关闭]</title>
      <link>https://stackoverflow.com/questions/79229880/what-ml-optimization-algorithm-should-i-use-to-find-the-optimum-input-values-for</link>
      <description><![CDATA[我有一些数据，需要找到 X*Y 的最佳组合，以便因变量 A、B、C 达到最大值。
所有这些都是简单的数值，就像科学实验的观察结果一样。因此，A、B、C 是在同一范围内具有不同值的性能变量，而 X 和 Y 是两个测量变量。
基本上，我想制作一个程序来不断处理未来实验中涌入的任何数据。
我遇到过有人使用 ANN 回归来解决类似的问题，还有人建议使用梯度下降。由于我必须从头开始完成这项任务，如果我能建议我应该从哪种算法开始，我将非常高兴。]]></description>
      <guid>https://stackoverflow.com/questions/79229880/what-ml-optimization-algorithm-should-i-use-to-find-the-optimum-input-values-for</guid>
      <pubDate>Wed, 27 Nov 2024 10:35:42 GMT</pubDate>
    </item>
    <item>
      <title>寻找洞察聚类机器学习项目[关闭]</title>
      <link>https://stackoverflow.com/questions/79228750/looking-for-insights-clustering-machine-leaning-prohect</link>
      <description><![CDATA[我正在为高中开展一个涉及聚类（k 均值和 DBSCAN）的机器学习项目
我抓取了一个电子商务网站 (StockX)，并对某些商品（包括设计师品牌等）的零售价值 (x) 和转售价值 (y) 进行聚类。然后对它们进行聚类。
最终的聚类结果非常基础，有 3 个聚类 - 围绕低零售/转售、中等零售/转售和高零售/转售价格。
我想知道你们是否对我可以用数据和我的项目做的更细微的事情有什么建议。如果有更多有趣的发现，我可以尝试挖掘出来。]]></description>
      <guid>https://stackoverflow.com/questions/79228750/looking-for-insights-clustering-machine-leaning-prohect</guid>
      <pubDate>Wed, 27 Nov 2024 01:58:21 GMT</pubDate>
    </item>
    <item>
      <title>大型多 GPU ML 训练作业的 GPU 间流量 [关闭]</title>
      <link>https://stackoverflow.com/questions/79228728/inter-gpu-traffic-for-large-multi-gpu-ml-training-jobs</link>
      <description><![CDATA[对于具有不同并行类型（如数据、张量、管道等）的分布式多 GPU 大型机器学习作业，我正在寻找点对点、全对全、全归约等 GPU 间流量的模式和百分比。是否有关于此的研究/数据？
大多数研究都讨论数据并行，其中全归约类型的流量占分配梯度的大多数。]]></description>
      <guid>https://stackoverflow.com/questions/79228728/inter-gpu-traffic-for-large-multi-gpu-ml-training-jobs</guid>
      <pubDate>Wed, 27 Nov 2024 01:44:11 GMT</pubDate>
    </item>
    <item>
      <title>Keras 神经网络回归模型优先考虑 2 个输出值，如何才能让它更好地概括？[关闭]</title>
      <link>https://stackoverflow.com/questions/79228286/keras-neural-network-regression-model-prioritizes-2-output-values-over-the-rest</link>
      <description><![CDATA[我正在尝试使用其他特征预测附加数据集中的“温度”值。执行代码时，模型对两个值有明显的偏差。我正在使用 plotly 图显示预测的准确性，其中包含真实值和预测值以及表示最佳预测的线。我的预测准确性
因此，如您所见，有两个主要的信息集群，表明我的模型主要选择这两个值作为输出。我不知道为什么会发生这种情况，也不知道我可以做些什么来补救。我将链接我正在使用的 .csv 文件和代码（google Drive / Colab）
此外，当删除预处理步骤时，它会产生类似的效果，但有三个主要集群而不是两个。 https://drive.google.com/drive/folders/1uSHTVAmW-UXhutZa5-Tf1QcB_e9cl_DR?usp=sharing
我尝试过：

删除预处理步骤。
添加特征工程和通过相关性进行数据选择。
排除分类值。
我已经试验了神经网络的大小和密度，增加或减少以查看它是否是欠拟合问题。
我已经引入了最多 100 次试验的自动超参数选择。
我在神经网络中添加了批量和特征规范化。
我已经手动调整了超参数的值，例如时期、激活函数等。
我使用了 KMeans 来降低密度。
我尝试了不同的数据分割。

我预计分布会略有变化，但它总是水平聚集（与预测值一起）
总之，这是我的神经网络还是预处理的问题？]]></description>
      <guid>https://stackoverflow.com/questions/79228286/keras-neural-network-regression-model-prioritizes-2-output-values-over-the-rest</guid>
      <pubDate>Tue, 26 Nov 2024 21:09:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么基于 Tensorflow.js 的天气预测模型无法预测正确的天气</title>
      <link>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78536168/why-tensorflow-js-based-weather-prediction-model-is-unable-to-predict-correct-we</guid>
      <pubDate>Sun, 26 May 2024 18:36:09 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Windows 机器上安装 Rasa</title>
      <link>https://stackoverflow.com/questions/78483192/unable-to-install-rasa-in-windows-machine</link>
      <description><![CDATA[我尝试在我的 Windows 10 笔记本电脑上使用命令 pip install rasa 安装 Rasa。
我安装了 Python 3.11 版。
但是我收到以下错误：
获取构建 wheel 的要求未成功运行。
│ 退出代码：1
╰─&gt; [20 行输出]
回溯（最近一次调用最后一次）：
文件“C:\python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，行 
353，在&lt;module&gt; 中
main()
文件“C:\python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，第 335 行，
在 main 中 
json_out[&#39;return_val&#39;] = hook(**hook_input[&#39;kwargs&#39;])
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

我遗漏了什么？之前，我尝试安装 chatterbot 模块，也遇到了类似的错误。
此外，我也尝试使用 pip install chatterbot，但仍然出现错误。]]></description>
      <guid>https://stackoverflow.com/questions/78483192/unable-to-install-rasa-in-windows-machine</guid>
      <pubDate>Wed, 15 May 2024 10:10:42 GMT</pubDate>
    </item>
    </channel>
</rss>