<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 06 Apr 2024 18:16:20 GMT</lastBuildDate>
    <item>
      <title>Word2Vec Hierarchical Softmax 中的内部顶点是什么？</title>
      <link>https://stackoverflow.com/questions/78285447/whats-inside-inner-vertices-in-word2vec-hierarchical-softmax</link>
      <description><![CDATA[我有一个关于分层 Softmax 的问题。实际上，我不太明白内部顶点（不是叶顶点）中存储的内容。我清楚地理解这个算法的主要思想，但是每一步我们都计算输入词嵌入与内部顶点的词嵌入的点积。那么这些内部顶点内部有哪些向量呢？是否是随机初始化的 embedding_size 向量，然后它们的坐标由于反向传播步骤而改变？]]></description>
      <guid>https://stackoverflow.com/questions/78285447/whats-inside-inner-vertices-in-word2vec-hierarchical-softmax</guid>
      <pubDate>Sat, 06 Apr 2024 18:15:41 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 pycaret 获得第二最佳模型参数？</title>
      <link>https://stackoverflow.com/questions/78285377/how-to-get-the-second-best-model-parameter-using-pycaret</link>
      <description><![CDATA[这是我在 google collab 上的代码
# 导入库
从 pycaret.classification 导入 *


exp_clf = 设置（数据，目标=&#39;目标&#39;，规范化=True）


最佳模型 = 比较模型()

这就是结果

此代码将显示最佳模型参数
显示(best_model)

这就是结果

我想知道其他模型参数而不是最佳模型（光梯度增强机），因为 AUC 对我来说不够高。如何显示随机森林分类器的参数？
我尝试了这个，希望它能向我显示第二最佳模型（随机森林）的参数
 显示(best_model[1])

这就是结果
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
TypeError Traceback（最近一次调用最后一次）
&lt;ipython-input-20-63c1d322b8ae&gt;在&lt;细胞系：1&gt;()
----&gt; 1 个显示器（best_model[1]）

类型错误：“LGBMClassifier”对象不可下标
]]></description>
      <guid>https://stackoverflow.com/questions/78285377/how-to-get-the-second-best-model-parameter-using-pycaret</guid>
      <pubDate>Sat, 06 Apr 2024 17:53:04 GMT</pubDate>
    </item>
    <item>
      <title>我认为我的模型过度拟合？有什么建议么？</title>
      <link>https://stackoverflow.com/questions/78285324/i-think-my-model-is-overfitting-any-suggestions</link>
      <description><![CDATA[所以我正在使用来自kaggle的deepfake检测数据集，我的模型似乎过度拟合，大约有2000张图像，其中1k用于“真实”类，1k用于“假”类。所有图像都是面孔。我使用的是 VGG16，因为它众所周知适合深度伪造面部检测。
以下是过去 10 个周期的结果：
找到属于 2 个类别的 1632 个图像。
找到属于 2 个类别的 204 张图像。
找到属于 2 个类别的 205 张图像。
纪元 1/10
51/51 [==============================] - 1234s 24s/步 - 损失：0.1841 - 准确度：0.9577 - val_loss ：0.7258 - val_accuracy：0.6719
纪元 2/10
51/51 [================================] - 1168s 23s/步 - 损失：0.1397 - 准确度：0.9712 - val_loss ：0.7331 - val_accuracy：0.6406
纪元 3/10
51/51 [================================] - 1186s 23s/步 - 损失：0.1215 - 准确度：0.9743 - val_loss ：0.7938 - val_accuracy：0.6719
纪元 4/10
51/51 [==============================] - 1187s 23s/步 - 损失：0.0914 - 准确度：0.9884 - val_loss ：0.8304 - val_accuracy：0.6615
纪元 5/10
51/51 [================================] - 1188s 23s/步 - 损失：0.0705 - 准确度：0.9939 - val_loss ：0.9022 - val_accuracy：0.6562
纪元 6/10
51/51 [================================] - 1155s 23s/步 - 损失：0.0683 - 准确度：0.9896 - val_loss ：0.9072 - val_accuracy：0.6354
纪元 7/10
51/51 [==============================] - 1154s 23s/步 - 损失：0.0541 - 准确度：0.9951 - val_loss ：0.8924 - val_accuracy：0.6719
纪元 8/10
51/51 [==============================] - 1175s 23s/步 - 损失：0.0403 - 准确度：0.9975 - val_loss ：0.9353 - val_accuracy：0.6510
纪元 9/10
51/51 [==============================] - 1189s 23s/步 - 损失：0.0420 - 准确度：0.9969 - val_loss ：1.0692 - val_accuracy：0.6198
纪元 10/10
51/51 [==============================] - 1185s 23s/步 - 损失：0.0288 - 准确度：0.9988 - val_loss ：1.0250 - val_accuracy：0.6510
/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103：UserWarning：您正在通过“model.save()”将模型保存为 HDF5 文件。此文件格式被视为旧格式。我们建议使用原生 Keras 格式，例如`model.save(&#39;my_model.keras&#39;)`。
  saving_api.save_model(
6/6 [================================] - 135s 21s/步 - 损失：1.1160 - 准确度：0.6042
测试精度：0.6041666865348816，结果如下

如果有人想知道，这里是过去 10 个时期的代码：
从 google.colab 导入驱动器
导入操作系统
将张量流导入为 tf
从tensorflow.keras.preprocessing.image导入ImageDataGenerator
从tensorflow.keras.models导入load_model

train_dir = &#39;/content/drive/MyDrive/dataset/train&#39;
val_dir = &#39;/content/drive/MyDrive/dataset/val&#39;
test_dir = &#39;/content/drive/MyDrive/dataset/test&#39;

＃参数
批量大小 = 32
历元 = 5
图像形状 = (224, 224)

save_model_path = &#39;/content/drive/MyDrive/dataset/trained_model_updated.h5&#39;
模型 = load_model(保存的模型路径)

#预处理
train_datagen = ImageDataGenerator(重新缩放=1./255)
val_datagen = ImageDataGenerator(重新缩放=1./255)
test_datagen = ImageDataGenerator（重新缩放=1./255）

train_generator = train_datagen.flow_from_directory(
    火车目录，
    目标大小=图像形状，
    批量大小=批量大小，
    class_mode=&#39;二进制&#39;
）

val_generator = val_datagen.flow_from_directory(
    val_dir,
    目标大小=图像形状，
    批量大小=批量大小，
    class_mode=&#39;二进制&#39;
）

test_generator = test_datagen.flow_from_directory(
    测试目录，
    目标大小=图像形状，
    批量大小=批量大小，
    class_mode=&#39;二进制&#39;
）


model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;binary_crossentropy&#39;,
              指标=[&#39;准确性&#39;])


历史=模型.fit(
    火车发电机，
    steps_per_epoch=train_generator.samples //batch_size,
    纪元=纪元，
    验证数据=val_generator，
    validation_steps=val_generator.samples //batch_size
）


model.save(&#39;/content/drive/MyDrive/dataset/trained_model_updated.h5&#39;)

test_loss, test_acc = model.evaluate(test_generator,steps=test_generator.samples //batch_size)
print(f&#39;测试准确度：{test_acc}&#39;)


我运行了该模型大约 20 个 epoch，并不断更改参数，应用数据增强。尽管该模型对训练数据产生了良好的准确性，但对验证和测试集而言仍停滞在 67% 左右。
在前 5 个 epoch 中，模型的验证准确率确实从 53% 提高到 67%，但在最后 15 个 epoch 中仍然停滞不前。
为了确认，我还尝试使用来自互联网的一些图像，但它没有正确识别这些图像，并且会错误地将一些假图像分类为“真实”。
我很确定它的过度拟合是正确的吗？
我没有计算资源来训练更大的数据集，除了增加数据集大小之外还有其他解决方案吗？任何建议，将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78285324/i-think-my-model-is-overfitting-any-suggestions</guid>
      <pubDate>Sat, 06 Apr 2024 17:38:39 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：“密集”层需要 1 个输入。收到了 2 个</title>
      <link>https://stackoverflow.com/questions/78285249/valueerror-layer-dense-expected-1-inputs-received-2-instead</link>
      <description><![CDATA[我正在尝试训练 VGG16 模型来识别狗的品种。问题是我在训练模型后无法加载模型。我不断收到此错误：ValueError：层“密集”需要 1 个输入。收到了 2 个
从 keras.src.legacy.preprocessing.image 导入 ImageDataGenerator
从 keras.src. saving 导入 load_model
从 sklearn.preprocessing 导入 LabelEncoder
从 keras.utils 导入到_categorical
从 keras.applications 导入 VGG16
从 keras 导入图层、模型

def load_and_crop_image(image_path、annotation_path、save_dir=&#39;cropped_images&#39;):

    # 解析 XML 文件的边界框
    树 = ET.parse(annotation_path)
    根 = 树.getroot()
    bndbox = root.find(&quot;.//object/bndbox&quot;)
    xmin = int(bndbox.find(&#39;xmin&#39;).text)
    ymin = int(bndbox.find(&#39;ymin&#39;).text)
    xmax = int(bndbox.find(&#39;xmax&#39;).text)
    ymax = int(bndbox.find(&#39;ymax&#39;).text)

    # 加载并裁剪图像
    图像 = Image.open(图像路径)
    裁剪图像 = image.crop((xmin, ymin, xmax, ymax))
    cropped_image_resized =cropped_image.resize((224, 224)) # 调整大小以适合 VGG16 输入大小

    如果不是 os.path.exists(save_dir):
        os.makedirs（保存目录）

    image_file_name = os.path.basename(image_path)
    save_path = os.path.join(save_dir, image_file_name)

    #cropped_image_resized.save(save_path)

    # print(f“将裁剪后的图像保存到：{save_path}”)

    返回 np.array(cropped_image_resized)


def load_dataset（images_dir，annotations_dir）：
    图片 = []
    标签=[]

    对于 os.listdir(annotations_dir) 中的品种：
        breed_annotations_dir = os.path.join（annotations_dir，breed）
        breed_images_dir = os.path.join（images_dir，breed）

        对于os.listdir(breed_annotations_dir)中的annotation_file：
            注释路径 = os.path.join(breed_annotations_dir, 注释文件)
            image_file_name = comment_file.split(&#39;.&#39;)[0] + &#39;.jpg&#39;
            image_path = os.path.join(breed_images_dir, image_file_name)

            如果 os.path.exists(image_path):
                图像 = load_and_crop_image(图像路径, 注释路径)
                图像.追加（图像）

                labels.append(品种)

    返回 np.array(图像), np.array(标签)

images_dir = &#39;图像&#39;
注释_dir = &#39;注释&#39;
图像，标签= load_dataset（images_dir，annotations_dir）

# 对标签进行编码
label_encoder = LabelEncoder()
编码标签 = label_encoder.fit_transform(标签)
分类标签 = to_categorical(编码标签)

def create_model(num_classes):
    基础模型 = VGG16(权重=&#39;imagenet&#39;, include_top=False, input_shape=(224, 224, 3))
    base_model.trainable = False # 冻结基础模型

    模型 = models.Sequential([
        基本模型，
        图层.GlobalAveragePooling2D(),
        层.Dense（1024，激活=&#39;relu&#39;），
        层数.Dropout(0.5),
        层.Dense（num_classes，激活=&#39;softmax&#39;）
    ]）

    model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
    返回模型


模型 = create_model(num_classes=categorical_labels.shape[1])

数据生成 = ImageDataGenerator(重新缩放=1。/ 255,
                             验证分割=0.1，
                             旋转范围=10，
                             width_shift_range=0.1,
                             height_shift_range=0.1，
                             剪切范围=0.1，
                             缩放范围=0.1，
                             水平翻转=真，
                             垂直翻转=真
                             ）
train_generator = datagen.flow(图像、​​categorical_labels、batch_size=32、subset=&#39;training&#39;)
validation_generator = datagen.flow(图像、​​categorical_labels、batch_size=32、subset=&#39;validation&#39;)




历史= model.fit（train_generator，epochs = 1，validation_data = validation_generator）

model.save(&#39;dog_breed_classifier.keras&#39;, overwrite=True)

print(&quot;模型保存成功。&quot;)

尝试：
    模型 = load_model(&#39;dog_breed_classifier.keras&#39;)
    print(“模型加载成功。”)
除了异常 e：
    print(f“加载模型时发生错误：{e}”)

如果我注释掉这一行history = model.fit(train_generator, epochs=1,validation_data=validation_generator)，那么模型就会成功加载。
如果我在 model.fit(...) 之前保存并加载模型，则会收到相同的错误。]]></description>
      <guid>https://stackoverflow.com/questions/78285249/valueerror-layer-dense-expected-1-inputs-received-2-instead</guid>
      <pubDate>Sat, 06 Apr 2024 17:14:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 make_classification 生成数据时可以设置特征的重要性吗？ make_classif 认为哪些功能很重要？</title>
      <link>https://stackoverflow.com/questions/78285036/can-i-set-the-importance-of-the-features-when-generating-data-with-make-classifi</link>
      <description><![CDATA[我有一个关于 scikit-learn 的 make_classification 的问题。
我使用 make_classification 创建了一个数据集，以测试不同模型区分重要特征和不太重要特征的能力。
所以我想相应地设置 make_classification 中的功能。这意味着，我想预先知道哪些是更重要的功能，哪些是不太重要的功能。
如果可能的话，我还想设置或调整哪些是更重要的功能。
我设置了以下内容：
&lt;前&gt;&lt;代码&gt;
X,y = make_classification(n_samples=50000, n_features=10, n_informative=5,
                    n_redundant=2、n_repeated=0、n_classes=2、n_clusters_per_class=2、
                          类间隔=1，
                   Flip_y=0.01，权重=[0.9,0.1]，shuffle=True，random_state=42）

在 make_classification 的文档中，有关于权重和比例的信息，但这似乎不适合了解或塑造特征的重要性。
我的问题不是关于在使用特定模型或不同模型时如何确定特征重要性。
我的问题是：

在使用 make_classification 生成数据时，我可以确定特征的重要性吗？ make_classification 认为哪些功能很重要？
是否可以设置或影响 make_classification 中变量的重要性？
所有信息功能都很重要吗？达到同样程度？他们之间有秩序吗？我可以通过某种方式对此进行调整吗？
我如何识别哪些信息丰富？

后续问题：

是否有另一种方法来生成合成数据来满足定义特征重要性的要求/或预先知道哪些特征更重要？

谢谢您，我们非常感谢任何想法或建议。]]></description>
      <guid>https://stackoverflow.com/questions/78285036/can-i-set-the-importance-of-the-features-when-generating-data-with-make-classifi</guid>
      <pubDate>Sat, 06 Apr 2024 16:04:55 GMT</pubDate>
    </item>
    <item>
      <title>神经网络对不同输入的相同预测</title>
      <link>https://stackoverflow.com/questions/78284988/neural-network-same-prediction-for-different-inputs</link>
      <description><![CDATA[我正在尝试在 Matlab 中构建一个神经网络，而不使用深度学习工具箱，其中一个隐藏层可以预测图像显示的是脑肿瘤还是健康的大脑。我使用的数据库包含 4000 张图像（2000 张脑肿瘤图像和 2000 张健康大脑图像）。
我面临的问题是准确率为 50%，并且每张图像的预测都是相同的。结果，混淆矩阵的一列始终为 0。我尝试更改学习率，尝试更改隐藏层上的神经元数量，但没有任何改变输出。我使用的隐藏层和输出层的激活函数都是 sigmoid，并且使用的优化算法是梯度下降。]]></description>
      <guid>https://stackoverflow.com/questions/78284988/neural-network-same-prediction-for-different-inputs</guid>
      <pubDate>Sat, 06 Apr 2024 15:52:17 GMT</pubDate>
    </item>
    <item>
      <title>需要有关使用梯度带的张量流手动反向传播的帮助</title>
      <link>https://stackoverflow.com/questions/78284916/need-help-regarding-tensorflow-manual-back-propagation-using-gradient-tape</link>
      <description><![CDATA[我的问题与这些事情有关：强化学习和张量流手动反向传播。
我的问题是：我正在研究强化学习系统。我通常批量收集经验，然后使用奖励进行反向传播。我使用我自己的自定义损失函数。我使用 EPSILON-GREEDY 策略进行探索。
现在的问题是，我的模型一直以高概率预测一类，我想问题是，我的模型根本不关心随机动作。这是因为，我使用所采取行动的概率来计算损失，因此，如果我从预测中获取随机概率，可以说[0.1,0.3,0.6]，例如我的随机行动是0，概率为0.1。假设它是对该状态的正确预测，所以我给出高奖励和低损失，现在我的模型在某种程度上认为它由于其 ARGMAX 值而获得了高奖励，即 0.6 概率或动作 3，但事实并非如此知道它因为随机动作 0 而获得了那么高的奖励。
我只是想知道如何解决这个问题。因为这是上周发生的事情。我尝试了chat-gpt和gemini建议的所有方法，但它们不起作用，现在我需要一个擅长keas和tensorflow的人，一个有使用tensorflow经验的人
提前谢谢您:)]]></description>
      <guid>https://stackoverflow.com/questions/78284916/need-help-regarding-tensorflow-manual-back-propagation-using-gradient-tape</guid>
      <pubDate>Sat, 06 Apr 2024 15:29:53 GMT</pubDate>
    </item>
    <item>
      <title>如何正确表示模型的数据</title>
      <link>https://stackoverflow.com/questions/78284870/how-to-correctly-represent-data-for-a-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78284870/how-to-correctly-represent-data-for-a-model</guid>
      <pubDate>Sat, 06 Apr 2024 15:16:11 GMT</pubDate>
    </item>
    <item>
      <title>为用于 QA 的 BERT 训练准备文本语料库</title>
      <link>https://stackoverflow.com/questions/78284470/text-corpus-preparation-for-bert-training-for-qa</link>
      <description><![CDATA[我最近开始学习 NLP（自学），并一直在研究使用 NLP 来代替“无用”的词来帮助教育方面。教师。为此，我想训练一个 BERT 模型，该模型可以使用提取式 QA 来回答学生根据特定主题的特定文本语料库提出的问题。
但是，我需要帮助准备用于训练的文本数据，因为它不是我习惯的常用 csv 格式，而且看起来有点不同。数据如下所示，长达数百页：
那么，我该如何为训练 BERT 做准备呢？或者你们有更好的方法可以建议我做我正在尝试的事情吗？]]></description>
      <guid>https://stackoverflow.com/questions/78284470/text-corpus-preparation-for-bert-training-for-qa</guid>
      <pubDate>Sat, 06 Apr 2024 13:03:09 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：模块“tensorflow.python.keras.layers”没有属性“BatchNormalization”</title>
      <link>https://stackoverflow.com/questions/78284460/attributeerror-module-tensorflow-python-keras-layers-has-no-attribute-batchn</link>
      <description><![CDATA[我目前正在开发一个项目，需要在 TensorFlow Keras 中使用 BatchNormalization。我已经从tensorflow.python.keras导入了层，但是当我尝试使用BatchNormalization时，遇到以下错误：
AttributeError：模块“tensorflow.python.keras.layers”没有属性“BatchNormalization”
这是我的代码片段：
从tensorflow.python.keras导入模型，输入
从tensorflow.python.keras.optimizers导入adam_v2作为Adam
从tensorflow.python.keras导入层
类 InceptionBlock(layers.Layer):
    def __init__(self, f, pooling=True):
        super(InceptionBlock, self).__init__()
        
        self.f = f
        self.pooling = 池化
   
        self.conva0 = 层.Conv2D(self.f, (1, 1), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;)
        self.batch_norma0=layers.BatchNormalization()
        self.conva1 = 层.Conv2D(self.f, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;)
        self.batch_norma1=layers.BatchNormalization()
        self.conva2 = 层.Conv2D(self.f, (1, 3), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;)
        self.batch_norma2=layers.BatchNormalization()
        self.poola = groups.MaxPooling2D(pool_size=(2, 2))
        self.conva3 = 层.Conv2D(self.f, (3, 1), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;)
        self.batch_norma3=layers.BatchNormalization()
            

我的tensorflow版本是2.16.1，python版本是3.11.9。我检查了文档，似乎 BatchNormalization 应该在图层模块中可用。有人能解释一下为什么我可能会遇到这个错误吗？我应该使用其他方法在 TensorFlow Keras 中导入 BatchNormalization 吗？
如有任何帮助，我们将不胜感激。
谢谢！
我正在尝试在我的 InceptionBlock 路径中使用 BatchNormalization。]]></description>
      <guid>https://stackoverflow.com/questions/78284460/attributeerror-module-tensorflow-python-keras-layers-has-no-attribute-batchn</guid>
      <pubDate>Sat, 06 Apr 2024 13:00:44 GMT</pubDate>
    </item>
    <item>
      <title>我可以重新训练 AutoModelForSequenceClassification 以生成文本吗？</title>
      <link>https://stackoverflow.com/questions/78284197/can-i-retrain-an-automodelforsequenceclassification-for-text-generation</link>
      <description><![CDATA[我的目标是微调 Mistral 7b 以编写短意识流（文本完成，而不是遵循指令）。
我有一个大型数据库（100 万行），其中包含从互联网上抓取的短文本。我手动将 15k 行标记为 good (1k) 和 bad（其余 14k）示例。我的计划是训练 AutoModelForSequenceClassification在这些示例上标记其他 985k 行。
通过这种方式，我希望收集大约 20k 意识流的好例子来微调 Mistral 7b。
但仅对good示例进行微调并不会使用bad示例中的信息，这些示例的数量要多得多。因此，我正在考虑使用 Mistral 7b 作为 AutoModelForSequenceClassification 的基本模型（遵循 这篇 Medium 文章），然后重新训练生成的 AutoModelForSequenceClassification 以进行文本补全。这需要移除分类头并添加新的/重新训练的 LoRA 组件。
您认为这可行吗？这是否会削弱模型（例如，需要重新学习语法），或者这是否是将坏反例的信息合并到文本生成中的有效方法？或者至少为 LoRA 文本生成微调提供一个良好的初始化点？]]></description>
      <guid>https://stackoverflow.com/questions/78284197/can-i-retrain-an-automodelforsequenceclassification-for-text-generation</guid>
      <pubDate>Sat, 06 Apr 2024 11:32:55 GMT</pubDate>
    </item>
    <item>
      <title>py_call_impl 中的错误（可调用，call_args$未命名，call_args$命名）</title>
      <link>https://stackoverflow.com/questions/78283892/error-in-py-call-implcallable-call-argsunnamed-call-argsnamed</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78283892/error-in-py-call-implcallable-call-argsunnamed-call-argsnamed</guid>
      <pubDate>Sat, 06 Apr 2024 09:49:16 GMT</pubDate>
    </item>
    <item>
      <title>微调t5变压器产生重复输出</title>
      <link>https://stackoverflow.com/questions/78283102/fine-tuned-t5-transformer-generates-repetitive-output</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78283102/fine-tuned-t5-transformer-generates-repetitive-output</guid>
      <pubDate>Sat, 06 Apr 2024 03:13:49 GMT</pubDate>
    </item>
    <item>
      <title>Python Phonemizer 库在 ubuntu VM 中找不到 espeak 库</title>
      <link>https://stackoverflow.com/questions/78210991/python-phonemizer-library-cant-find-espeak-library-in-ubuntu-vm</link>
      <description><![CDATA[尽管该模型在 Windows 本地计算机上运行良好，但根据此安装指南将路径传递到 espeak-ng 库时 https://bootphon.github.io/phonemizer/install.html ，我无法使其在 Ubuntu 22.04.4 LTS (x86-64) 下的虚拟机中工作。当运行我的脚本通过 wav2vec2phoneme 转录音素时，我收到以下消息
回溯（最近一次调用最后一次）：
文件“/dialrec/phoneme_transcription/phoneme_recognizers/transcribe.py”，第 50 行，位于
phoneme_recognizer = Wav2Vec2Phoneme()
文件“/dialrec/phoneme_transcription/phoneme_recognizers/wav2vec2phoneme.py”，第 24 行，init 中
self.processor = Wav2Vec2Processor.from_pretrained(&quot;facebook/wav2vec2-xlsr-53-espeak-cv-ft&quot;)
文件“/usr/local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py”，第 52 行，在 from_pretrained 中
返回 super().from_pretrained(pretrained_model_name_or_path, **kwargs)
文件“/usr/local/lib/python3.10/site-packages/transformers/processing_utils.py”，第 465 行，from_pretrained
args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)
文件“/usr/local/lib/python3.10/site-packages/transformers/processing_utils.py”，第 511 行，位于 _get_arguments_from_pretrained
args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))
文件“/usr/local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py”，第 837 行，在 from_pretrained 中
返回 tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
文件“/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py”，第 2086 行，from_pretrained
返回 cls._from_pretrained(
文件“/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py”，第 2325 行，位于 _from_pretrained
分词器 = cls(*init_inputs, **init_kwargs)
文件“/usr/local/lib/python3.10/site-packages/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py”，第 153 行， init
self.init_backend(self.phonemizer_lang)
文件“/usr/local/lib/python3.10/site-packages/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py”，第 202 行， init_backend
self.backend = BACKENDS[self.phonemizer_backend](phonemizer_lang, language_switch=&quot;remove-flags&quot;)
文件“/usr/local/lib/python3.10/site-packages/phonemizer/backend/espeak/espeak.py”，第 45 行，在 init 中
超级().init(
文件“/usr/local/lib/python3.10/site-packages/phonemizer/backend/espeak/base.py”，第 39 行，在 init 中
超级().init(
文件“/usr/local/lib/python3.10/site-packages/phonemizer/backend/base.py”，第 77 行，在 init 中
引发 RuntimeError( # pragma: nocover
运行时错误：您的系统上未安装 espeak

为了安装 espeak，我按照以下步骤操作：

apt-get 安装 espeak-ng
pip3 安装phonemizer
pip3 install espeakng（也尝试过 pip3 install py-espeak-ng）

Espeak 肯定安装在 /usr/lib/x86_64-linux-gnu/libespeak-ng.so.1 和 /usr/bin/espeak-ng 下。
我尝试了以下方法：

无需额外步骤
设置环境变量 PHONEMIZER_ESPEAK_LIBRARY=&#39;/usr/lib/x86_64-linux-gnu/libespeak-ng.so.1&#39; 和 PHONEMIZER_ESPEAK_PATH=&#39;/usr/bin/espeak-ng&#39;。
直接在脚本中设置环境变量
os.environ[&#39;PHONEMIZER_ESPEAK_LIBRARY&#39;] = &#39;/usr/lib/x86_64-linux-gnu/libespeak-ng.so.1&#39;
os.environ[&#39;PHONEMIZER_ESPEAK_PATH&#39;] = &#39;/usr/bin/espeak-ng&#39;

如果有任何帮助，我将不胜感激。提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/78210991/python-phonemizer-library-cant-find-espeak-library-in-ubuntu-vm</guid>
      <pubDate>Sat, 23 Mar 2024 13:14:00 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 scikit learn 中将装袋技术与两种不同的算法结合使用？</title>
      <link>https://stackoverflow.com/questions/26283045/is-it-possible-to-use-the-bagging-technique-with-two-different-algorithms-in-sci</link>
      <description><![CDATA[是否可以将装袋技术与两种不同的算法（例如逻辑回归和随机森林）或（几乎）任何其他算法结合使用？
我需要一些能够返回平均概率或组合概率和预测的东西。这将用于分类任务。]]></description>
      <guid>https://stackoverflow.com/questions/26283045/is-it-possible-to-use-the-bagging-technique-with-two-different-algorithms-in-sci</guid>
      <pubDate>Thu, 09 Oct 2014 16:06:55 GMT</pubDate>
    </item>
    </channel>
</rss>