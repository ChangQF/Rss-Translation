<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 29 Jul 2024 12:29:39 GMT</lastBuildDate>
    <item>
      <title>如何使用 Python 和计算机视觉检测图像中的人是否赤裸上身？</title>
      <link>https://stackoverflow.com/questions/78806780/how-to-detect-if-a-person-is-shirtless-in-an-image-using-python-and-computer-vis</link>
      <description><![CDATA[我有一个数据集，其中包含一个人的多张自拍照，每张都是从胸部以上拍摄的，格式为自拍。我的目标是识别每张图像中的人是否赤裸上身。这些图像并不露骨，而是随意的自拍照，其中的人可能穿着也可能没穿着衬衫。
限制：
我无法从头开始训练神经网络模型 - 我的数据集不是那么大。
我更喜欢更强大且随时可用的解决方案。
该解决方案应该可以用 Python 实现并使用计算机视觉技术。
是否有任何预先训练的模型或库可以帮助完成这项特定任务？
可以使用哪些方法或技术来准确识别图像中的人是否赤裸上身？
任何指导或建议都将不胜感激。谢谢！
我尝试使用 Python 中的 NSFW-Detector 库（https://pypi.org/project/nsfw-detector/），但它没有什么帮助，因为它专注于检测露骨内容，而不是识别一个人是否赤裸上身。]]></description>
      <guid>https://stackoverflow.com/questions/78806780/how-to-detect-if-a-person-is-shirtless-in-an-image-using-python-and-computer-vis</guid>
      <pubDate>Mon, 29 Jul 2024 11:19:21 GMT</pubDate>
    </item>
    <item>
      <title>在 javafx 应用程序中集成 pickle 模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78805706/integrating-pickle-model-in-javafx-application</link>
      <description><![CDATA[我试图将 python pickle 模型集成到 javafx 中，我尝试使用 py4j 进行连接，但没有成功，一直出现连接错误
我尝试使用 py4j 将 javafx 代码与读取模型的 python 代码连接起来，但没有成功，还尝试使用 onnx 运行时，但由于某种原因，使用 onnx 运行时生成的模型已损坏。我想要的是让模型预测结果并将结果发送到 javafx 进行显示]]></description>
      <guid>https://stackoverflow.com/questions/78805706/integrating-pickle-model-in-javafx-application</guid>
      <pubDate>Mon, 29 Jul 2024 06:43:34 GMT</pubDate>
    </item>
    <item>
      <title>不同的 Beta 分布样本</title>
      <link>https://stackoverflow.com/questions/78805636/different-beta-distribution-samples</link>
      <description><![CDATA[给定此 Python 代码

success = np.array([1, 2, 3, 4, 5])
failure = np.array([12, 13, 14, 15, 16])

beta_samples = np.random.beta(success + 1, Failure + 1, size = 5)
print(beta_samples)

beta_samples2 = [np.random.beta(success[i] + 1, Failure[i] + 1) for i in range(5)]
print(beta_samples2)

Python 代码的输出为：
[0.22023725 0.18643029 0.14765836 0.12017337 0.39477445]
[0.22605254862542162, 0.12566539444840888, 0.13290508184544317, 0.2718239044172474, 0.1464185003569084]

为什么我使用相同的 success 和 failure 数组，却得到两个完全不同的 Beta 分布样本？
哪一个是正确的，哪一个是错误的？
这是因为稍后我需要使用 argmax(...) 函数选择这些 Beta 样本中最大值的位置/索引，而目前，这两个样本都会给我不同的索引/位置。]]></description>
      <guid>https://stackoverflow.com/questions/78805636/different-beta-distribution-samples</guid>
      <pubDate>Mon, 29 Jul 2024 06:22:48 GMT</pubDate>
    </item>
    <item>
      <title>Python\Python312\Lib\站点包\torch\lib\fbgemm.dll</title>
      <link>https://stackoverflow.com/questions/78805219/python-python312-lib-site-packages-torch-lib-fbgemm-dll</link>
      <description><![CDATA[在此处输入图片描述
我正尝试从 Hugging Face 导入 GPT-2 Transformer 模型，但当我尝试导入它时，我遇到了错误。即使我尝试只导入 Torch，我也会收到同样的错误。
我尝试重新安装 Torch 并做了所有事情，包括更新 Visual C++ Redistributable 软件包和更新我的驱动程序，但问题仍然存在。]]></description>
      <guid>https://stackoverflow.com/questions/78805219/python-python312-lib-site-packages-torch-lib-fbgemm-dll</guid>
      <pubDate>Mon, 29 Jul 2024 02:23:30 GMT</pubDate>
    </item>
    <item>
      <title>关于 CatBoostClassifier 默认参数的这些说法准确吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78804284/are-these-claims-about-the-default-parameters-of-catboostclassifier-accurate</link>
      <description><![CDATA[当您实例化 CatBoostClassifier 而不指定任何参数时，模型会为所有参数使用其内置的默认值：

iterations：默认值为 1000。
learning_rate：默认值是自动确定的，但通常在 0.03 左右。
depth：默认值为 6。
loss_function：对于二元分类任务，默认值为“Logloss”。
random_seed：默认值为随机值。

我想知道上述 6 条陈述是否正确。我尝试进行研究，但找不到有关此的任何信息。
此外，如果知道 CatBoost 的调整机制到底是什么，那将非常棒。他们如何开始第一次迭代？]]></description>
      <guid>https://stackoverflow.com/questions/78804284/are-these-claims-about-the-default-parameters-of-catboostclassifier-accurate</guid>
      <pubDate>Sun, 28 Jul 2024 16:33:30 GMT</pubDate>
    </item>
    <item>
      <title>处理 tensorflow/keras 中 Conv1D 层维数的正确方法是什么？</title>
      <link>https://stackoverflow.com/questions/78804130/what-is-the-proper-way-to-deal-with-dimensionality-in-conv1d-layers-in-tensorflo</link>
      <description><![CDATA[我一直想尽办法尝试使用 Keras 中的 Conv1D 解决这个问题。基本上，我有一个 100 x 1 的向量 bathyZ。我想在将它与 2 个标量输入 Tperiod 和 AMP_WK 合并以预测另一个 100 x 1 向量之前对其进行一些卷积。（bathyZ 具有我希望获取的空间变异性）。这是我第一次使用 tensorflow 数据集并进行解析/反序列化。我不断遇到我似乎无法理解的形状错误。
主要是，似乎第一个密集层没有在其输入中收到正确的尺寸，我无法弄清楚原因，因为所有东西的输入尺寸对我来说似乎都有意义。为什么它期望 6402 并得到（无，165）？我正在调试单个记录，因此批量大小很小，如果这是一个问题的话。无论如何它都应该可以工作，对吧？我的理解是，我的输入应该是 (None,100,1)，以允许 100 x 1 序列的不同批次大小。我尝试过几种重塑方法，但似乎都没有奏效，所以也许我错过了一些更基本的东西
错误消息：
ValueError：调用 Functional.call() 时遇到异常。

层“dense_110”的输入 0与层不兼容：预期输入形状的轴 -1 的值为 6402，但收到的输入形状为 (None, 165)

Functional.call() 收到的参数：
• 输入={&#39;bathyZ&#39;: &#39;tf.Tensor(shape=(None, 100, 1), dtype=float32)&#39;, &#39;AMP_WK&#39;: &#39;tf.Tensor(shape=(None, 1), dtype=float32)&#39;, &#39;Tperiod&#39;: &#39;tf.Tensor(shape=(None, 1), dtype=float32)&#39;}
• 训练=True
• 掩码={&#39;bathyZ&#39;: &#39;None&#39;, &#39;AMP_WK&#39;: &#39;None&#39;, &#39;Tperiod&#39;: &#39;None&#39;}

代码：
feature_description = {
&#39;bathyZ&#39;: tf.io.FixedLenFeature([], tf.string),
&#39;bathyZ_shape&#39;: tf.io.FixedLenFeature([3], tf.int64),
&#39;AMP_WK&#39;: tf.io.FixedLenFeature([], tf.float32),
&#39;Tperiod&#39;: tf.io.FixedLenFeature([], tf.float32),
&#39;skew&#39;: tf.io.FixedLenFeature([], tf.string),
&#39;skew_shape&#39;: tf.io.FixedLenFeature([3], tf.int64),
}

def _parse_function(proto):
# 解析
parsed_features = tf.io.parse_single_example(proto, feature_description)

# 解码/重塑序列化张量
bathyZ = parsed_features[&#39;bathyZ&#39;]
bathyZ = tf.io.parse_tensor(bathyZ, out_type=tf.float32)
bathyZ = tf.reshape(bathyZ, [100, 1])

skew = parsed_features[&#39;skew&#39;]
skew = tf.io.parse_tensor(skew, out_type=tf.float32)
skew = tf.reshape(skew, [100, 1])

# 获取其他输入，重塑
AMP_WK = parsed_features[&#39;AMP_WK&#39;]
Tperiod = parsed_features[&#39;Tperiod&#39;]

AMP_WK = tf.reshape(AMP_WK, [1])
Tperiod = tf.reshape(Tperiod, [1])

# 创建元组
输入 = {&#39;bathyZ&#39;: bathyZ, &#39;AMP_WK&#39;: AMP_WK, &#39;Tperiod&#39;: Tperiod}
输出 = {&#39;skew&#39;: skew}

返回输入、输出

# 创建 TFRecordDataset 并映射解析函数
tfrecord_path = &#39;ML_0004.tfrecord&#39;
数据集 = tf.data.TFRecordDataset(tfrecord_path)
数据集 = dataset.map(_parse_function)

# 模型

def create_model():
# 张量输入分支（形状：100 个时间步，1 个特征）
bathyZ = 输入（形状=（100, 1），名称=&#39;bathyZ&#39;）
x = 层。Conv1D（32，3，激活=&#39;relu&#39;， padding=&#39;same&#39;)(bathyZ)
x = layer.Conv1D(64, 3,activation=&#39;relu&#39;, padding=&#39;same&#39;)(x)
x = layer.Flatten()(x)

# 标量输入
AMP_WK = Input(shape=(1,), name=&#39;AMP_WK&#39;)
Tperiod = Input(shape=(1,), name=&#39;Tperiod&#39;)

# 合并所有分支
combined = layer.concatenate([x, AMP_WK, Tperiod])

# 全连接层
z = layer.Dense(64,activation=&#39;relu&#39;)(combined)
z = layer.Dense(128,activation=&#39;relu&#39;)(z)

# 输出层（张量输出，与输入张量形状相同）
skew = layer.Dense(100,activation=&#39;linear&#39;, name=&#39;skew&#39;)(z)

# 创建模型
model = models.Model(inputs=[bathyZ, AMP_WK, Tperiod], output=skew)
返回模型

# 示例用法：
model = create_model()
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;)
model.summary()
dataset = dataset.batch(1) 
model.fit(dataset)

我尝试在解析和预处理步骤中将各种输入张量重塑为 [1,100]、[1,100,1] 等不同版本，并跟踪形状的演变。但我总是遇到某种尺寸错误]]></description>
      <guid>https://stackoverflow.com/questions/78804130/what-is-the-proper-way-to-deal-with-dimensionality-in-conv1d-layers-in-tensorflo</guid>
      <pubDate>Sun, 28 Jul 2024 15:26:09 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降应用</title>
      <link>https://stackoverflow.com/questions/78804107/gradient-descent-application</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78804107/gradient-descent-application</guid>
      <pubDate>Sun, 28 Jul 2024 15:15:45 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：PdfReader.getNumPages（）缺少 1 个必需的位置参数：“self”[重复]</title>
      <link>https://stackoverflow.com/questions/78803533/typeerror-pdfreader-getnumpages-missing-1-required-positional-argument-self</link>
      <description><![CDATA[from os import listdir
from os.path import isfile, join
import PyPDF2
from PyPDF2 import PdfReader
path=&#39;C:/Users/fc/Supreme_court.pdf/&#39;
files = [join(path, f) for f in listdir(path) if isfile(join(path, f)) and join(path, f).endswith(&#39;.pdf&#39;)]

for f in files:
with open(f, &#39;rb&#39;) as file_handle:
pdf_reader = PyPDF2.PdfReader(file_handle, strict=False)
page_text = &#39;&#39;
for page_num in range(0,PdfReader.getNumPages()):
page = pdf_reader.getPage(page_num)
page_text += page.extract_text()
# 将纯文本字符串写入同名文件
with open(f.replace(&#39;.pdf&#39;, &#39;.txt&#39;), &#39;a+&#39;) as text_file_handle:
text_file_handle.writelines(page_text)

TypeError: PdfReader.getNumPages() 缺少 1 个必需的位置参数：&#39;self&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/78803533/typeerror-pdfreader-getnumpages-missing-1-required-positional-argument-self</guid>
      <pubDate>Sun, 28 Jul 2024 10:39:10 GMT</pubDate>
    </item>
    <item>
      <title>Runge-Kutta 求解器无法取得进展（RuntimeWarning：迭代没有取得良好进展）</title>
      <link>https://stackoverflow.com/questions/78803278/runge-kutta-solver-cannot-make-progress-runtimewarning-iteration-is-not-making</link>
      <description><![CDATA[我收到此警告：
RuntimeWarning：迭代没有取得良好进展，以过去十次迭代的改进来衡量。 

我正在尝试从工业制冷系统的数字孪生构建自定义 RL 环境。我正在模拟特定时间步骤内的过程，并在一段时间内（即 24 小时）循环执行该步骤。
我尝试更改 RK 求解器的 time_step_methods 数量和 configure 方法中的 time_step_minutes。可能是我的动作空间/观察空间或奖励函数定义不明确？
我的代码片段：
class RefrigerationEnv(gym.Env):
def __init__(self, num_evaporators=7, time_step_seconds=10, time_step_minutes=15):
super(RefrigerationEnv, self).__init__()

self.num_evaporators = num_evaporators
self.time_step_seconds = time_step_seconds
self.time_step_minutes = time_step_minutes

tz = dateutil.tz.gettz(&quot;America/Los_Angeles&quot;)
self.start_datetime = datetime(2024, 5, 12, 10, 0, 0, tzinfo=tz)

self.end_datetime_episode = self.start_datetime + timedelta(hours=12)
self.simulation_time = self.start_datetime # 初始化模拟时间

# 动作空间
self.action_space = space.Box(
low=np.array([100, -1, 20] + [-15] * num_evaporators), # 每个设定点的下限
high=np.array([180, 12, 80] + [10] * num_evaporators), # 每个设定点的上限
dtype=np.float32
)

# 观察空间
self.observation_space = space.Dict(
{
&quot;refrigerator_temp__c&quot;: space.Box(low=-30, high=10, shape=(1,), dtype=np.float64),
&quot;energy_consumption&quot;: space.Box(low=0.0, high=400.0, shape=(1,), dtype=np.float64),
&quot;ambient_temp&quot;: space.Box(low=-10, high=50, shape=(1,), dtype=np.float64),
&quot;time_of_day&quot;: space.Box(low=0, high=24, shape=(1,), dtype=np.float64),
}
)

# 初始化模拟器
self.simulator = configure_example_simulation_from_path(
process_config_path=Path(&quot;simulation_payload/simple_flowsheet_config.json&quot;),
control_config_path=Path(&quot;simulation_payload/simple_control_config.yaml&quot;),
disorders=self.create_disturbances(),
settings=self.create_settings(),
start_datetime=self.start_datetime,
end_datetime=self.start_datetime + timedelta(minutes=self.time_step_minutes), # 模拟 1 小时
solver=RK12(max_step__s=self.time_step_seconds,relative_tolerance=0.00001),
)
]]></description>
      <guid>https://stackoverflow.com/questions/78803278/runge-kutta-solver-cannot-make-progress-runtimewarning-iteration-is-not-making</guid>
      <pubDate>Sun, 28 Jul 2024 08:21:22 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 文档解释</title>
      <link>https://stackoverflow.com/questions/78803273/xgboost-docs-explanation</link>
      <description><![CDATA[我正在 https://xgboost.readthedocs.io/en/stable/tutorials/model.html 上阅读有关 XGBoost 的文章，我无法弄清楚下面屏幕截图中的方程式如何进入下一步
在此处输入图片描述
我试图扩展方程式来思考为什么事情会这样发展，但我仍然不明白。]]></description>
      <guid>https://stackoverflow.com/questions/78803273/xgboost-docs-explanation</guid>
      <pubDate>Sun, 28 Jul 2024 08:17:04 GMT</pubDate>
    </item>
    <item>
      <title>MoviePy 文件数量限制</title>
      <link>https://stackoverflow.com/questions/78802929/moviepy-limit-on-number-of-files</link>
      <description><![CDATA[我正在尝试将大约 300 个 mp4 视频（来自 Ekman-6 数据集）转换为 mp3 音频文件。我目前正在使用 MoviePy 的 VideoFileClip（在 Google Colab 中）进行此转换：
id = 0

for i in range(6):
path = directory + folders[i]
cnt = 0
for file in os.listdir(path):
filename = os.fsdecode(file)
clip = VideoFileClip(path + filename)
audio = clip.audio
audio.write_audiofile(to_directory + folders[i] + str(id) + &quot;.mp3&quot;)
audio.close()
clip.close()
id += 1
cnt += 1
if cnt == 50:
break

但是，在转换第 47 个文件时，我收到以下错误：
-------------------------------------------------------------------------------
AttributeError Traceback（最近一次调用last)
&lt;ipython-input-4-c6ad7f378072&gt; in &lt;cell line: 7&gt;()
12 clip = VideoFileClip(directory + folders[i] + file)
13 audio = clip.audio
---&gt; 14 audio.write_audiofile(to_directory + folders[i] + str(id) + &quot;.mp3&quot;)
15 audio.close()
16 clip.close()

AttributeError: &#39;NoneType&#39; 对象没有属性 &#39;write_audiofile&#39;

我不确定是什么导致了这个错误。所以，我在网上搜索，发现另一个 stackoverflow 帖子说这个问题是由于 mp4 文件没有音频而引起的。所以，我检查了导致错误的文件，但它确实有音频。为了 100% 确保 MoviePy 正确地将该文件转换为音频，我还单独对该文件进行了转换。这次，MoviePy 成功地将文件转换为音频。
为什么 MoviePy 单独处理该文件时可以正常工作，而在转换之前的 46 个文件后却不行？使用 MoviePy 转换的文件数量/视频长度是否有限制？我在网上找不到有关此问题的任何信息。]]></description>
      <guid>https://stackoverflow.com/questions/78802929/moviepy-limit-on-number-of-files</guid>
      <pubDate>Sun, 28 Jul 2024 04:07:19 GMT</pubDate>
    </item>
    <item>
      <title>在 YOLO 推理中，GPU 性能不如 CPU 性能</title>
      <link>https://stackoverflow.com/questions/78802177/gpu-performance-worse-than-cpu-performance-on-yolo-inferences</link>
      <description><![CDATA[我正在使用 YoloDotNet NuGet 包来测试 YOLO 模型的性能。我正在为我的学位论文做这个测试。但是，我遇到了一个问题，GPU 性能明显比 CPU 性能差。
环境：

YoloDotNet 版本：v2.0
CPU：AMD ryzen 7 7800X3D
GPU：4070 super
CUDA/cuDNN 版本：cuda 11.8 和 cudnn 8.9.7
.NET 版本：8

重现步骤：
var sw = new Stopwatch();
for (var i = 0; i &lt; 500; i++)
{
var file = $@&quot;C:\Users\Utente\Documents\assets\images\input\frame_{i}.jpg&quot;;

使用 var image = SKImage.FromEncodedData(file);
sw.Restart();
var results = yolo.RunObjectDetection(image, confidence: 0.25, iou: 0.7);
sw.Stop();
image.Draw(results);

image.Save(file.Replace(&quot;input&quot;, $&quot;output_{yolo_version}{version}_{target}&quot;).Replace(&quot;.jpg&quot;, $&quot;_detect_{yolo_version}{version}_{target}.jpg&quot;),
SKEncodedImageFormat.Jpeg);
times.Add(sw.Elapsed.TotalMilliseconds);
Console.WriteLine($&quot;图像 {i} 所用时间：{sw.Elapsed.TotalMilliseconds:F2} 毫秒&quot;);

这是我对检测进行时间测量的方式。
要加载模型，我在 GPU 情况下使用此设置
yolo = new Yolo(new YoloOptions
{
OnnxModel = @$&quot;C:\Users\Utente\Documents\assets\model\yolov{yolo_version}{version}_{target}.onnx&quot;,
ModelType = ModelType.ObjectDetection, // 模型类型
Cuda = true, // 使用 CPU 或 CUDA 进行 GPU 加速推理。默认值 = true
GpuId = 0, // 根据 id 选择 Gpu。默认值 = 0
PrimeGpu = true, // 先预分配 GPU。默认值 = false
});
Console.WriteLine(yolo.OnnxModel.ModelType);
Console.WriteLine($&quot;使用 GPU 版本 {yolo_version}{version}&quot;);

使用 yolov8 的性能指标：
CPU 推理时间：
版本 m 的总时间：25693 毫秒

版本 m 每幅图像的平均时间：51.25 毫秒

GPU 推理时间：
版本 m 的总时间：34459.73 毫秒

版本 m 每幅图像的平均时间：69.74 毫秒

我想发布有关时间的图表，但我没有足够的声誉
该问题针对不同大小的模型自行呈现。我仅打印了 m 大小以方便可视化。
预期行为是使用 GPU 的推理应该比使用 CPU 的推理更快。
但使用 GPU 后性能并没有提高。]]></description>
      <guid>https://stackoverflow.com/questions/78802177/gpu-performance-worse-than-cpu-performance-on-yolo-inferences</guid>
      <pubDate>Sat, 27 Jul 2024 18:33:48 GMT</pubDate>
    </item>
    <item>
      <title>在搜索系统结果上计算 NDCG 时，处理假阳性和假阴性有哪些不同的方法？</title>
      <link>https://stackoverflow.com/questions/78798774/what-are-the-different-ways-to-handle-false-positives-and-false-negatives-when-c</link>
      <description><![CDATA[上下文：
我正在使用 NDCG（归一化折扣累积增益）来评估包含相关性分数的地面实况数据集上的语义搜索系统。我想为此使用 sklearn 的 ndcg_score()。
问题：有哪些处理方法：

假阳性文档：对于给定的查询，那些出现在搜索系统的响应中但不出现在地面实况数据中的文档
假阴性文档：对于给定的查询，那些出现在地面实况数据中但不出现在搜索系统的响应中的文档

一种可能性是插入预测分数 = 0 来表示假阴性并忽略假阳性。但我并不完全确定这是否是正确的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78798774/what-are-the-different-ways-to-handle-false-positives-and-false-negatives-when-c</guid>
      <pubDate>Fri, 26 Jul 2024 15:02:50 GMT</pubDate>
    </item>
    <item>
      <title>只有输入张量可以作为位置参数传递</title>
      <link>https://stackoverflow.com/questions/78360982/only-input-tensors-may-be-passed-as-positional-arguments</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78360982/only-input-tensors-may-be-passed-as-positional-arguments</guid>
      <pubDate>Sun, 21 Apr 2024 09:15:40 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Keras 对 CNN 模型中的多个输入数据进行交叉验证</title>
      <link>https://stackoverflow.com/questions/59277549/how-to-do-cross-validation-with-multiple-input-data-in-cnn-model-with-keras</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/59277549/how-to-do-cross-validation-with-multiple-input-data-in-cnn-model-with-keras</guid>
      <pubDate>Wed, 11 Dec 2019 01:07:35 GMT</pubDate>
    </item>
    </channel>
</rss>