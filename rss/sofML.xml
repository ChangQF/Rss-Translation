<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 23 Sep 2024 15:18:08 GMT</lastBuildDate>
    <item>
      <title>受不同聚类大小约束的 KMeans</title>
      <link>https://stackoverflow.com/questions/79015120/kmeans-constrained-with-different-cluster-size</link>
      <description><![CDATA[我有一个包含商店坐标的数据框，我想根据供应商应该访问该商店的日期将它们划分为簇。例如，假设供应商应该访问 180 家商店。他应该在周一到周五访问 30-34 家商店，周六，他应该访问其他日子的 60%。
你们知道我该怎么做吗？使用 kmeans-constrained，我只能将它们划分为大小相等的簇。也许我需要使用某种解算器或在集群之间移动点以达到我想要的数字，但我不知道如何做到这一点。
以下是将它们均等划分的代码：
# 循环遍历供应商集群
for vendor in df[&quot;vendor&quot;].unique():

# 每个供应商的商店数量
n_shops = df.loc[df[&quot;vendor&quot;] == vendor][&quot;cod_shop&quot;].count()

# 索引
idx = df.loc[df[&quot;vendor&quot;] == vendor].index

# 一周中各天的集群数量
num_clusters = 5 # 星期一至星期五

# 集群的平均大小
avg_size = n_shops / (num_clusters + 0.6)

# 定义限制
min_shops = round(avg_size - n_shops * pct, 0)
max_shops = math.ceil(avg_size + n_shops * pct)

# 模型
kmeans = KMeansConstrained(n_clusters=num_clusters, size_min=min_shops, size_max=max_shops, random_state=42)
labels = kmeans.fit_predict(df.loc[df[&quot;vendor&quot;] == vendor][[&quot;latitude&quot;, &quot;longitude&quot;]])

# 向数据框添加标签
df.loc[idx, &quot;visit_day&quot;] = labels
]]></description>
      <guid>https://stackoverflow.com/questions/79015120/kmeans-constrained-with-different-cluster-size</guid>
      <pubDate>Mon, 23 Sep 2024 14:42:09 GMT</pubDate>
    </item>
    <item>
      <title>图像拼接中的泊松混合导致图像模糊、鬼影重重</title>
      <link>https://stackoverflow.com/questions/79014990/poisson-blending-in-image-stitching-results-in-blurred-ghostly-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79014990/poisson-blending-in-image-stitching-results-in-blurred-ghostly-images</guid>
      <pubDate>Mon, 23 Sep 2024 14:11:23 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 coremltools 将 TensorFlow ConcreteFunction 或 AutoTrackable 对象转换为 Core ML？</title>
      <link>https://stackoverflow.com/questions/79014855/how-to-convert-a-tensorflow-concretefunction-or-autotrackable-object-to-core-ml</link>
      <description><![CDATA[我正在尝试使用 coremltools 将 TensorFlow 对象检测模型 (ssd_mobilenet_v1_coco) 转换为 Core ML 格式。该模型采用 SavedModel 格式，但我在尝试转换时遇到了各种问题。以下是我到目前为止采取的步骤以及我得到的错误：
我到目前为止所做的：

加载了 TensorFlow SavedModel：
import tensorflow as tf

model = tf.saved_model.load(&quot;ssd_mobilenet_v1_coco_2017_11_17/saved_model&quot;)
concrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]`



尝试使用 coremltools 将模型转换为 Core ML：


mlmodel = ct.convert(concrete_func, source=&quot;tensorflow&quot;)
mlmodel.save(&quot;ssd_mobilenet_v1_coco.mlmodel&quot;) 

尝试转换 ConcreteFunction 时，我不断收到以下错误：
NotImplementedError：预期模型格式：[SavedModel | concrete_function | tf.keras.Model | .h5 | GraphDef]，得到了 ConcreteFunction

我尝试将模型导出为 SavedModel 并将该路径直接传递给 coremltools：
tf.saved_model.save(model, &quot;exported_saved_model&quot;)
mlmodel = ct.convert(&quot;exported_saved_model&quot;, source=&quot;tensorflow&quot;)

如何使用 coremltools 成功将此 TensorFlow 模型（或 ConcreteFunction）转换为 Core ML？是否有特定的方法来处理 AutoTrackable 或 ConcreteFunction 对象？在此转换过程中我遗漏了什么或做错了什么？
其他信息：
• TensorFlow 版本：X.X.X（例如 2.10.0）
• coremltools 版本：X.X.X（例如 5.0b3）
• Python 版本：X.X.X（例如 3.10）
]]></description>
      <guid>https://stackoverflow.com/questions/79014855/how-to-convert-a-tensorflow-concretefunction-or-autotrackable-object-to-core-ml</guid>
      <pubDate>Mon, 23 Sep 2024 13:36:52 GMT</pubDate>
    </item>
    <item>
      <title>有没有方法可以从具有 200 行的初始数据集扩展我们的数据集。我希望从中至少获得 2000 行来应用 ML 模型</title>
      <link>https://stackoverflow.com/questions/79014580/is-there-any-method-about-how-to-expand-our-dataset-from-initial-dataset-having</link>
      <description><![CDATA[我的项目是利用 ML 技术预测抑郁程度或向孩子的父母提出一些预防措施建议。
我想应用 ML 模型根据我们的数据集预测抑郁程度或其症状。因此，我需要至少 2000 个训练数据元组来训练它们。我怎样才能在不改变属性之间的关系（相关性）的情况下实现这一点。
我的数据集包含许多属性，例如屏幕时间、抑郁程度。如何使用一些代码来解决这个问题。
我尝试使用 CTGAN，但它给了我很多错误。]]></description>
      <guid>https://stackoverflow.com/questions/79014580/is-there-any-method-about-how-to-expand-our-dataset-from-initial-dataset-having</guid>
      <pubDate>Mon, 23 Sep 2024 12:21:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 NumPy 实现基本神经网络的问题</title>
      <link>https://stackoverflow.com/questions/79014083/problem-implementing-a-basic-neural-network-with-numpy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79014083/problem-implementing-a-basic-neural-network-with-numpy</guid>
      <pubDate>Mon, 23 Sep 2024 09:49:44 GMT</pubDate>
    </item>
    <item>
      <title>构建 OCR 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/79013996/building-ocr-model</link>
      <description><![CDATA[问题：
我正在研究一个机器学习模型，其中的输入是图像和实体名称，目标是从图像中提取相应的实体值。例如，如果实体名称是“高度”，并且图像包含门的高度，则模型应该提取此值（例如 6 英尺）以及正确的单位。
输入：
图像：包含对象（例如门）和相关信息（例如高度或其他相关测量值）。
实体名称：关键字，例如“高度”或“重量”指定要从图像中提取的值。
输出：
与图像中的实体相对应的值及其单位（例如，“6 英尺”）。
挑战：
实体值可以出现在图像的不同部分，具有不同的文本格式、字体或样式。
需要识别和提取测量单位（例如米、英尺）以及值。
问题：
处理此类任务的最佳方法或模型架构是什么？
是否有任何特定技术或预训练模型可以帮助将图像和实体名称作为输入结合起来以从图像中提取相应的值？
我应该如何预处理图像和标签以训练此类任务的模型？
任何有关框架和工具的指导、参考或建议都将不胜感激！
我尝试使用带有 CTC 损失的 CNN+RNN，但我的损失接近 20 并且没有进一步减少
这里我附上了我的 google cloab 链接
笔记本链接]]></description>
      <guid>https://stackoverflow.com/questions/79013996/building-ocr-model</guid>
      <pubDate>Mon, 23 Sep 2024 09:27:35 GMT</pubDate>
    </item>
    <item>
      <title>我可以做些什么来提高我在 Kaggle 泰坦尼克号竞赛中的表现？[关闭]</title>
      <link>https://stackoverflow.com/questions/79013898/what-can-i-do-to-improve-my-performance-on-kaggles-titanic-contest</link>
      <description><![CDATA[我尝试了所有方法，使用决策树和随机森林来预测泰坦尼克号上的幸存者。但都不起作用。
我使用决策树和随机森林模型来预测谁会在泰坦尼克号上幸存。
我选择的特征包括“Pclass”、“性别”、“年龄”、“票价”和“舱位”，对于舱位，我使用 OneHotEncoder 来转换数据，对于性别，我使用 LabelEncoder。
然后我使用决策树来训练模型。当我设置 max_depth=4 并提交时，我的模型在预测中达到了最高的准确度分数，即 0.7799。后来，无论我做什么，我都无法获得更高的分数。我使用了 train_test_split、RandomForestClassifier，我使用了 GridSearch 来测试不同的参数。这些都不起作用，我的分数总是低于 0.7799。每当我设置 max_depth=4 时，无论是决策树还是随机森林，分数始终为 0.7799
如何使用 DecisionTree 或 RandomForest 提高我的性能？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79013898/what-can-i-do-to-improve-my-performance-on-kaggles-titanic-contest</guid>
      <pubDate>Mon, 23 Sep 2024 08:58:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Deepface Deepface.represent 从 ROI 获取嵌入时出错</title>
      <link>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</link>
      <description><![CDATA[我在使用 Deepface 从 Retinaface 识别的裁剪 ROI 获取嵌入时遇到了问题。
我正尝试使用一些名人的数据集（图像）学习对象识别，并可能考虑将其用于我的个人照片库。我尝试使用 Haar Cascade 进行人脸检测，并使用 Open Cv 中的 LBPHFaceRecognize 进行人脸识别，效果很好。然后我想尝试使用 Retinafce 进行人脸检测并获得 ROI。ROI 存储在列表中，并使用 Deepface 从选定的 ROI 获取嵌入并存储在另一个列表中。我正在尝试将嵌入存储到列表中，但我一直得到
 raise ValueError(
ValueError: 无法在 numpy 数组中检测到人脸。请确认图片

是人脸照片或考虑将 force_detection 参数设置为 False。
虽然所有图像都有一张被清楚检测到的人脸。这是我的代码供参考：
import os
import cv2 as cv
from retinaface import RetinaFace
from deepface import DeepFace
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

artist = [&#39;50cent&#39;] # type: ignore #MJ the GOAT!! , &#39;Kanye&#39;, &#39;Eminem&#39;, &#39;MichaelJackson&#39;
ROOT_DIR = &#39;asset/Face_Recon_Dataset&#39; #图像数据集的路径
faces_roi =[]
labels = []
embeddings = []
#现在在脸部坐标上画一个矩形
#脸部范围有：
# x1, y1) = (28, 51) #左上角
# (x2, y2) = (61, 98) #右下角
&quot;&quot;&quot; 这定义了检测到的脸部周围的矩形边界框。
- x1 (28)：脸部的左边缘
- y1 (51)：脸部的上边缘
- x2 (61)：脸部的右边缘
- y2 (98)：脸部的下边缘&quot;&quot;&quot;

def get_roi():
for artist_name in artist:
# 获取艺术家姓名的索引
label = artist.index(artist_name)
image_folder = os.path.join(ROOT_DIR,artist_name) # 获取包含图像的实际文件夹
for artist_images in os.listdir(image_folder): # 列出该目录中的所有图像
image = os.path.join(image_folder,artist_images)
resp = RetinaFace.detect_faces(image)
# 确保人脸存在
if isinstance(resp,dict):
img = cv.imread(image)
for face_id, face_data in resp.items():
# print(face_id)
# print(&quot;x1: &quot;, face_data[&#39;facial_area&#39;][0])
# print(&quot;y1: &quot;, face_data[&#39;facial_area&#39;][1])
# print(&quot;x2: &quot;, face_data[&#39;facial_area&#39;][2])
# print(&quot;y2: &quot;, face_data[&#39;facial_area&#39;][3], &quot;\n&quot;)
# 读取图像

# 检测人脸
x1 = face_data[&#39;facial_area&#39;][0]
y1 = face_data[&#39;facial_area&#39;][1]
x2 = face_data[&#39;facial_area&#39;][2]
y2 = face_data[&#39;facial_area&#39;][3]

# 为人脸绘制边界框 
# faces_rect = cv.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
face_roi = img[y1:y2,x1:x2]

#用其名称标记裁剪后的 roi 人脸
faces_roi.append(face_roi)

labels.append(label)
print(len(faces_roi))
print(len(labels))
print(&quot;已标记和索引的图像&quot;)
print(&quot;正在初始化嵌入过程.....&quot;)
get_embeddings()

def get_embeddings():
&quot;&quot;&quot; 使用 deepface 从每个面部 roi 中提取嵌入&quot;&quot;&quot;
print(&quot;Satarting embedding: 🚀🚀 &quot;)
for roi in faces_roi:
face_roi_resized = cv.resize(roi, (160, 160)) # 将人脸 ROI 调整为 160x160 像素
embedding = DeepFace.represent(face_roi_resized, model_name=&quot;Facenet&quot;)
print(embedding)
embeddings.append(embedding)
print(&quot;Vectors storage in list..&quot;)

get_roi()

# 是时候使用 svm 分类器测试和训练这个坏家伙了
# 将嵌入和索引标记为 numpy 数组
X = np.array(embeddings) #feature
y = np.array(labels) #label

# 将数据分成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 SVM 分类器
svm_model = SVC(kernel=&#39;linear&#39;) # 线性核是嵌入的良好默认值
svm_model.fit(X_train, y_train)

# 评估模型
y_pred = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;SVM 模型准确率：{accuracy * 100:.2f}%&quot;)


有人能帮我理解为什么即使 ROI 已被裁剪，该错误仍然持续存在吗？解决该错误的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</guid>
      <pubDate>Mon, 23 Sep 2024 08:03:12 GMT</pubDate>
    </item>
    <item>
      <title>面部皮肤健康分析 API [关闭]</title>
      <link>https://stackoverflow.com/questions/79013615/face-skin-health-analysis-api</link>
      <description><![CDATA[我想创建一个 React Native 应用来评估皮肤健康状况。我需要测量诸如光泽、斑点、皱纹、纹理、黑眼圈、眼袋、发红、油性、毛孔和水分等因素，并按 1 到 100 的等级显示每个因素的摘要。我遇到了一些用于此目的的 API，但它们非常昂贵。有没有使用 Python 和 OpenCV 的解决方案？是否有可用的模型或指南可以帮助我学习和开发项目的 API？
我尝试了 Google ML Vision 和 Microsoft API，但它们不符合我的要求。我找到了一些 API，但它们非常昂贵。现在我正在寻找自定义模型或数据集。]]></description>
      <guid>https://stackoverflow.com/questions/79013615/face-skin-health-analysis-api</guid>
      <pubDate>Mon, 23 Sep 2024 07:38:51 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用回归算法而不是分类算法？（描述中提供的数据集））[关闭]</title>
      <link>https://stackoverflow.com/questions/79013513/why-are-regression-algorithms-used-instead-of-classification-algorithms-datase</link>
      <description><![CDATA[众所周知，在 ML 中，如果依赖特征本质上是连续的，则应用回归模型。但是，如果依赖特征本质上是分类的，则使用分类算法。
正如您在这张图（https://i.sstatic.net/9Q3wfudK.png）中看到的那样，最大值为。大量数据点重复出现，表明它们正在形成类别。
那么，为什么这里使用回归？
这是数据集：（https://drive.google.com/file/d/1vTIiQ0NZKgBI-EfpGzfPKHx1VaAdEYdH/view?usp=sharing）
我和同学、老师讨论了这个问题。他们都说回归是用来预测的，但没人能解释他们是如何得出应该用回归来代替分类的结论的。]]></description>
      <guid>https://stackoverflow.com/questions/79013513/why-are-regression-algorithms-used-instead-of-classification-algorithms-datase</guid>
      <pubDate>Mon, 23 Sep 2024 07:10:30 GMT</pubDate>
    </item>
    <item>
      <title>在 nn.Transformer 中使用填充掩码时，损失返回为 Nan</title>
      <link>https://stackoverflow.com/questions/79013493/loss-is-returned-as-nan-when-using-padding-mask-in-nn-transformer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79013493/loss-is-returned-as-nan-when-using-padding-mask-in-nn-transformer</guid>
      <pubDate>Mon, 23 Sep 2024 07:04:53 GMT</pubDate>
    </item>
    <item>
      <title>如何使用不同的深度神经网络机器学习算法利用水稻作物图像预测甲烷排放量和用水量[关闭]</title>
      <link>https://stackoverflow.com/questions/79012121/how-to-predict-methane-emmision-and-water-usage-using-image-of-rice-crops-using</link>
      <description><![CDATA[我有水稻作物的图像，但我不知道如何使用深度神经网络的图像来预测甲烷排放量和用水量，而无需数据集的数值。我只有带土壤的水稻作物图像。
我尝试使用图像进行预测。我找不到预测甲烷排放量和用水量的正确代码。
我需要相同的代码]]></description>
      <guid>https://stackoverflow.com/questions/79012121/how-to-predict-methane-emmision-and-water-usage-using-image-of-rice-crops-using</guid>
      <pubDate>Sun, 22 Sep 2024 16:42:44 GMT</pubDate>
    </item>
    <item>
      <title>gym_super_mario_bros 的 DummyVecEnv 构造函数存在问题</title>
      <link>https://stackoverflow.com/questions/78085766/trouble-with-dummyvecenv-constructor-with-gym-super-mario-bros</link>
      <description><![CDATA[摘要：我想要做的就是使用 DummyVecEnv 构造函数来包装我的 gym 环境，即
env = DummyVecEnv([lambda: env])，但执行此操作时我不断收到错误。当前使用 https://pypi.org/project/gym-super-mario-bros/ 作为 Super Mario 的 gym 包装器。我感觉我误解了我应该为构造函数提供哪些参数，但我是 Python 新手，很难理解我遗漏了什么。
我在从 gym_super_mario_bros.make() 返回的环境中使用 DummyVecEnv 构造函数时遇到困难。我一直收到错误“您尝试创建多个环境，但创建它们的函数返回了相同的实例，而不是创建不同的对象”。
我最初尝试将我的环境包装在 DummyVecEnv 中，就像我在许多论坛上看到的那样：
env = gym_super_mario_bros.make(&quot;SuperMarioBros-v0&quot;) 
env = JoypadSpace(env, SIMPLE_MOVEMENT) 
env = GrayScaleObservation(env, keep_dim=True) 
env = DummyVecEnv([lambda: env]) #This line 错误

但我收到此错误：
文件c:\Users\truem\AppData\Local\Programs\Python\Python311\Lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py:30，在 DummyVecEnv.init(self, env_fns) 中
29 def init(self, env_fns: List[Callable[[], gym.Env]]):
---&gt; 30 self.envs = [_patch_env(fn()) for fn in env_fns]
31 if len(set([id(env.unwrapped) for env in self.envs])) != len(self.envs):
32 raise ValueError(
33 &quot;您尝试创建多个环境，但创建它们的函数返回了同一个实例&quot;
34 &quot;而不是创建不同的对象。&quot;(...)
39 &quot;请阅读 https://github.com/DLR-RM/stable-baselines3/issues/1151 了解更多信息。&quot;
40 )

因此，我尝试按照 github 链接中列出的修复程序以及文档中提供的示例进行操作
https://stable-baselines.readthedocs.io/en/master/guide/examples.html
我继续创建了一个新的辅助函数 create_default_environment，它应该创建新的 env 实例并返回它们，但尽管如此，它仍然不起作用。
这是我的代码现在的样子：
#将我的 env 创建包装在函数中
def create_default_environment():
newEnv = gym_super_mario_bros.make(&quot;SuperMarioBros-v0&quot;) 
newEnv = JoypadSpace(newEnv, SIMPLE_MOVEMENT) 
return GrayScaleObservation(newEnv, keep_dim=True)
#在调用辅助函数的列表中创建 lambda
env = DummyVecEnv([lambda: create_default_environment()]) #仍然失败

我继续将我的初始 env 构造包装在辅助函数 create_default_environment() 中，我已经验证该函数每次调用都会返回一个新的 env 实例。然后我使用该包装器插入 DummyVecEnv 构造函数：
env = DummyVecEnv([lambda: create_default_environment()])
但编译器仍然抱怨我传入的函数列表没有返回新实例。
我一直在尝试模拟我在 baselines 提供的示例代码中看到的内容：
from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines import PPO2

env = DummyVecEnv(\[lambda: gym.make(&quot;HalfCheetahBulletEnv-v0&quot;)]) #为什么这个可以工作而我的不行？

但我不确定我做错了什么。我是 python 新手，所以如果我遗漏了什么明显的东西，请原谅我。]]></description>
      <guid>https://stackoverflow.com/questions/78085766/trouble-with-dummyvecenv-constructor-with-gym-super-mario-bros</guid>
      <pubDate>Fri, 01 Mar 2024 05:53:20 GMT</pubDate>
    </item>
    <item>
      <title>不同分类器的 TPR 和 FPR 曲线 - R 中的 kNN、NaiveBayes、决策树</title>
      <link>https://stackoverflow.com/questions/34335074/tpr-fpr-curve-for-different-classifiers-knn-naivebayes-decision-trees-in-r</link>
      <description><![CDATA[我正在尝试理解并绘制不同类型分类器的 TPR/FPR。我在 R 中使用 kNN、NaiveBayes 和决策树。使用 kNN，我执行以下操作：
clnum &lt;- as.vector(diabetes.trainingLabels[,1], mode = &quot;numeric&quot;)
dpknn &lt;- knn(train = diabetes.training, test = diabetes.testing, cl = clnum, k=11, prob = TRUE)
prob &lt;- attr(dpknn, &quot;prob&quot;)
tstnum &lt;- as.vector(diabetes.testingLabels[,1], mode = &quot;numeric&quot;)
pred_knn &lt;- prediction(prob, tstnum)
pred_knn &lt;- performance(pred_knn, &quot;tpr&quot;, &quot;fpr&quot;)
plot(pred_knn, avg= &quot;threshold&quot;, colorize=TRUE, lwd=3, main=&quot;Knn=11 的 ROC 曲线&quot;)

其中 diabetes.trainingLabels[,1] 是我想要预测的标签（类）向量，diabetes.training 是训练数据，diabetes.testing 是测试数据。
该图如下所示：

存储在 prob 属性中的值是一个数字向量（0 到 1 之间的小数）。我将类标签因子转换为数字，然后可以将其与 ROCR 库中的预测/性能函数一起使用。不能 100% 确定我做得对，但至少它有效。
但是对于 NaiveBayes 和决策树，在预测函数中指定 prob/raw 参数，我得到的不是单个数字向量，而是一个列表向量或矩阵，其中指定了每个类的概率（我猜），例如：
diabetes.model &lt;- naiveBayes(class ~ ., data = diabetesTrainset)
diabetes.predicted &lt;- predict(diabetes.model, diabetesTestset, type=&quot;raw&quot;)

并且 diabetes.predicted 是：
tested_negative checked_positive
[1,] 5.787252e-03 0.9942127
[2,] 8.433584e-01 0.1566416
[3,] 7.880800e-09 1.0000000
[4,] 7.568920e-01 0.2431080
[5,] 4.663958e-01 0.5336042

问题是如何使用它来绘制 ROC 曲线，以及为什么在 kNN 中我得到一个向量，而对于其他分类器，我得到两个类别的向量是分开的？]]></description>
      <guid>https://stackoverflow.com/questions/34335074/tpr-fpr-curve-for-different-classifiers-knn-naivebayes-decision-trees-in-r</guid>
      <pubDate>Thu, 17 Dec 2015 12:51:31 GMT</pubDate>
    </item>
    <item>
      <title>开源神经网络库 [关闭]</title>
      <link>https://stackoverflow.com/questions/11477145/open-source-neural-network-library</link>
      <description><![CDATA[我正在寻找一个开源神经网络库。到目前为止，我已经研究过 FANN、WEKA 和 OpenNN。我还应该看看其他的吗？当然，标准是文档、示例和易用性。]]></description>
      <guid>https://stackoverflow.com/questions/11477145/open-source-neural-network-library</guid>
      <pubDate>Fri, 13 Jul 2012 19:32:11 GMT</pubDate>
    </item>
    </channel>
</rss>