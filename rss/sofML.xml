<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 29 Dec 2023 12:22:45 GMT</lastBuildDate>
    <item>
      <title>为什么“sklearn.svm.LinearSVC”的执行时间比“sklearn.svm.SVC”更长？</title>
      <link>https://stackoverflow.com/questions/77731956/why-is-sklearn-svm-linearsvc-taking-longer-to-execute-than-sklearn-svm-svc</link>
      <description><![CDATA[我正在使用 scikit-learn 中的 LinearSVC 和 SVC 类执行超参数调整，尽管我使用  执行的搜索量增加了 10 倍SVC类比LinearSVC执行时间短很多，可能是什么原因呢？我认为 LinearSVC 更优化。
我正在使用 Olivetti 面孔数据集
这是我正在执行的两个搜索：
从 sklearn.svm 导入 LinearSVC
从 sklearn.svm 导入 SVC
从 sklearn.model_selection 导入 GridSearchCV

#LinearSVC超参数调优------------------------
参数网格 = [
    {&#39;svc__C&#39;: np.logspace(-3,3, num=10)}
]

full_pipeline = 管道([
    (“预处理”, StandardScaler(with_mean=False)),
    (“svc”,LinearSVC(random_state=0))
    ]）

svc_rnd_search = GridSearchCV(full_pipeline, param_grid=param_grid, cv=10,
                           评分=&#39;准确度&#39;,n_jobs=-1)

# 测量执行时间
开始时间 = 时间()

#运行搜索
svc_rnd_search.fit(X_train, y_train)

# 计算执行时间
结束时间 = 时间()
执行时间毫秒 = (结束时间 - 开始时间) * 1000
print(f&quot;执行时间: {execution_time_ms:.3f}ms&quot;)

#SVC超参数调优------------------------------------------------
参数网格 = [
    {&#39;svc__C&#39;: np.logspace(-2,3, num=10),
     &#39;svc__gamma&#39;: np.logspace(-5,1, num=10),
     &#39;svc__kernel&#39;: [&#39;rbf&#39;]}
]

full_pipeline = 管道([
    (“预处理”, StandardScaler(with_mean=False)),
    (“svc”,SVC())
    ]）

svc_rnd_search = GridSearchCV(full_pipeline, param_grid=param_grid, cv=10,
                           评分=&#39;准确度&#39;,n_jobs=-1)

# 测量执行时间
开始时间 = 时间()

#运行搜索
svc_rnd_search.fit(X_train, y_train)

# 计算执行时间
结束时间 = 时间()
执行时间毫秒 = (结束时间 - 开始时间) * 1000
print(f&quot;执行时间: {execution_time_ms:.3f}ms&quot;)

第一个代码块 (LinearSVC) 的执行时间为 1087635 毫秒，而 SVC 类的执行时间为 36961 毫秒。]]></description>
      <guid>https://stackoverflow.com/questions/77731956/why-is-sklearn-svm-linearsvc-taking-longer-to-execute-than-sklearn-svm-svc</guid>
      <pubDate>Fri, 29 Dec 2023 12:18:51 GMT</pubDate>
    </item>
    <item>
      <title>递归特征消除如何决定支持某个特征？</title>
      <link>https://stackoverflow.com/questions/77731220/how-does-a-recursive-feature-elimination-decides-to-support-a-feature</link>
      <description><![CDATA[RFE 使用的基本算法是什么？RFE 如何决定支持哪些功能以及不支持哪些功能？]]></description>
      <guid>https://stackoverflow.com/questions/77731220/how-does-a-recursive-feature-elimination-decides-to-support-a-feature</guid>
      <pubDate>Fri, 29 Dec 2023 09:39:26 GMT</pubDate>
    </item>
    <item>
      <title>请帮我修复[关闭]</title>
      <link>https://stackoverflow.com/questions/77730925/please-help-me-to-fix</link>
      <description><![CDATA[我正在尝试使用贝叶斯网络进行一些简单的概率计算。当我尝试计算“clusterIndex”的值时对于属于我的数据集的示例，我没有问题，否则它会给我以下错误。我不明白这意味着什么以及如何解决它。
这是我的代码：
def bNetCreation(dataSet):
    # Crea archi in modo tale che ogni feature dipenda da clusterIndex
    边缘=[]
    对于 dataSet.columns 中的列：
        如果列！= &#39;clusterIndex&#39;：
            Edges.append((&#39;clusterIndex&#39;, 列))
    Edges.append((&#39;节奏&#39;,&#39;舞蹈能力&#39;))
    Edges.append((&#39;能量&#39;,&#39;舞蹈能力&#39;))
    Edges.append((&#39;响度&#39;,&#39;能量&#39;))
    Edges.append((&#39;节奏&#39;,&#39;能量&#39;))
    Edges.append((&#39;响度&#39;,&#39;言语性&#39;))
    Edges.append((&#39;活性&#39;,&#39;声学性&#39;))
    Edges.append((&#39;言语性&#39;,&#39;活跃度&#39;))
    Edges.append((&#39;响度&#39;,&#39;活跃度&#39;))
    Edges.append((&#39;danceability&#39;,&#39;valence&#39;))

    模型=贝叶斯网络（边缘）
    model.fit（数据集，估计器=MaximumLikelihoodEstimator，n_jobs=-1）
    以 open(&#39;modello.pkl&#39;, &#39;wb&#39;) 作为输出：
        pickle.dump（模型，输出）
    可视化贝叶斯网络（模型）
    返回模型
def prediciCluster(bayesianNetwork: BayesianNetwork, 示例, DifferentialColumn):
    推理=变量消除（贝叶斯网络）
    结果= inference.query(变量=[differentialColumn],evidence=example)
    打印（结果）

这是来自 inference.query 的错误：
返回 self.name_to_no[var][state_name]
       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^KeyError：0.6167826357877246
]]></description>
      <guid>https://stackoverflow.com/questions/77730925/please-help-me-to-fix</guid>
      <pubDate>Fri, 29 Dec 2023 08:21:41 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：使用 URL 'http://127.0.0.1:8237' 初始化剩余存储时出错：模块 'jwt' 没有属性 'encode'</title>
      <link>https://stackoverflow.com/questions/77730757/runtimeerror-error-initializing-rest-store-with-url-http-127-0-0-18237-mo</link>
      <description><![CDATA[第一个命令“python run_deployment.py --config deploy”运行成功并建议我运行下一个命令 - zenml up
“zenml up”生成以下错误。图片 1图片 2图片 3图片 4图片 5图片 6图片 7
我正在 YouTube 上关注 Ayush 的 MLOPs 课程。感谢您提前提供的帮助。
我尝试过 1. pip install jwt
2.pip安装PyJWT
3. pip卸载jwt
4. pip安装jwt==1.3.0
5. pip install --upgrade --force-reinstall PyJWT
6. pip install --upgrade --force-reinstall jwt]]></description>
      <guid>https://stackoverflow.com/questions/77730757/runtimeerror-error-initializing-rest-store-with-url-http-127-0-0-18237-mo</guid>
      <pubDate>Fri, 29 Dec 2023 07:34:39 GMT</pubDate>
    </item>
    <item>
      <title>当我在 jupyter 笔记本上运行简单的 cnn 模型时，CPU 使用率较低</title>
      <link>https://stackoverflow.com/questions/77730719/low-cpu-usage-when-i-run-a-simple-cnn-model-on-jupyter-notebook</link>
      <description><![CDATA[我在 Jupyter 笔记本上运行了一个非常简单的 CNN 模型，但过程非常慢。我在我的旧笔记本电脑（核心 i7U 10gen）上运行相同的程序。只花了一分半钟，但在我的新笔记本电脑（酷睿 i9 13900hx 和 rtx4060）上花了 30 分钟！它们都是在 CPU 上运行的，但在我的旧电脑上，CPU 使用率为 100%，在我的新电脑上，大约为 20%。然后，我在 PyCharm 中运行相同的程序，一切正常！这让我很困惑，我尝试了很多方法但都不起作用。我想知道真正的问题出在哪里？我的 Jupyter 笔记本还是其他东西？
我尝试在不同的 PC、不同的 IDE 平台上运行相同的程序。我想知道真正的问题出在哪里。]]></description>
      <guid>https://stackoverflow.com/questions/77730719/low-cpu-usage-when-i-run-a-simple-cnn-model-on-jupyter-notebook</guid>
      <pubDate>Fri, 29 Dec 2023 07:23:45 GMT</pubDate>
    </item>
    <item>
      <title>即使训练和测试数据集具有良好的准确性，模型也无法正确分类[关闭]</title>
      <link>https://stackoverflow.com/questions/77730582/model-not-classifying-correctly-even-with-good-accuracy-on-training-and-test-dat</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77730582/model-not-classifying-correctly-even-with-good-accuracy-on-training-and-test-dat</guid>
      <pubDate>Fri, 29 Dec 2023 06:36:45 GMT</pubDate>
    </item>
    <item>
      <title>当所有列都是 float64 和 int64 时，为什么 dtype: object</title>
      <link>https://stackoverflow.com/questions/77730058/why-is-the-dtype-object-when-all-columns-are-float64-and-int64</link>
      <description><![CDATA[打印（cleaned_train.dtypes）
打印(“--”)
打印（cleaned_test.dtypes）
观察年份 int64
Insured_Period float64
住宅 int64
Building_Painted float64
Building_Fenced float64
建筑类型 float64
声明 float64
建筑尺寸 float64
地理代码 float64
数据类型：对象
--
观察年份 int64
Insured_Period float64
住宅 int64
Building_Painted float64
Building_Fenced float64
建筑类型 float64
声明 float64
建筑尺寸 float64
地理代码 float64
数据类型：对象
尝试获取 dtype:numeric 但得到了对象]]></description>
      <guid>https://stackoverflow.com/questions/77730058/why-is-the-dtype-object-when-all-columns-are-float64-and-int64</guid>
      <pubDate>Fri, 29 Dec 2023 03:03:31 GMT</pubDate>
    </item>
    <item>
      <title>在 WSL conda 环境中安装 lightgbm GPU</title>
      <link>https://stackoverflow.com/questions/77728334/install-lightgbm-gpu-in-a-wsl-conda-env</link>
      <description><![CDATA[如何安装LightGBM？
我检查了多个来源，但仍然无法安装。
我尝试了 pip 和 conda 但都返回错误：
[LightGBM] [警告] 目前不支持在 CUDA 中使用稀疏特征。
[LightGBM] [致命] 此版本中未启用 CUDA Tree Learner。
请使用 CMake 选项 -DUSE_CUDA=1 重新编译

我尝试过的内容如下：
git clone --recursive https://github.com/microsoft/LightGBM
cd LightGBM/
mkdir -p 构建
光盘构建
cmake -DUSE_GPU=1 ..
使-j$(nproc)
cd ../python-package
点安装。
]]></description>
      <guid>https://stackoverflow.com/questions/77728334/install-lightgbm-gpu-in-a-wsl-conda-env</guid>
      <pubDate>Thu, 28 Dec 2023 17:34:48 GMT</pubDate>
    </item>
    <item>
      <title>我来解决以下错误，尝试微调 NER 的 roberta 模型</title>
      <link>https://stackoverflow.com/questions/77725013/me-to-resolve-below-error-trying-to-fine-tune-roberta-model-for-ner</link>
      <description><![CDATA[帮助我解决以下错误，尝试对 NER 的 roberta 模型进行精细化处理。我使用基本模型并尝试微调我的数据。
进口火炬
导入 json
从 torch.utils.data 导入数据集
从转换器导入 RobertaForTokenClassification、RobertaTokenizer
从 Transformers 导入 Trainer、TrainingArguments
从 torch.nn 导入 CrossEntropyLoss
 model_name = “roberta-base”
            tokenizer = RobertaTokenizer.from_pretrained(model_name)
            模型 = RobertaForTokenClassification.from_pretrained(model_name)

            NERDataset 类（数据集）：
                def __init__(自身、文本、实体、分词器):
                    self.texts = 文本
                    self.entities = 实体
                    self.tokenizer = 分词器

                def __len__(自身):
                    返回 len(self.texts)

                def __getitem__(self, idx):
                    文本 = self.texts[idx]
                    实体 = self.entities[idx]

                    # 直接从实体中提取标签
                    #labels = [标签[“标签”] 实体中的实体 实体中的标签]
                    labels = [label[“label”] 对于实体​​中的实体 对于实体​​中的标签，如果 isinstance(label, dict) 和“label”是在标签中]

                    

                    # 对输入文本进行标记
                    编码 = self.tokenizer(
                        文本，
                        return_tensors =“pt”，
                        截断=真，
                        填充=“最大长度”，
                        最大长度=512，
                        标签=标签
                    ）

                    返回 {
                        “input_ids”：编码[“input_ids”].squeeze()，
                        “attention_mask”：编码[“attention_mask”].squeeze()，
                        “标签”：编码[“标签”].squeeze() if labels else None
                    }

            # 从 JSONL 文件加载数据
            def load_data_from_jsonl(file_path):
                数据 = []
                以 open(file_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) 作为文件：
                    对于文件中的行：
                        尝试：
                            示例 = json.loads(行)
                            数据.追加（示例）
                        除了 json.JSONDecodeError 为 e：
                            print(f&quot;在线解码 JSON 时出错：{line}&quot;)
                            打印(e)
                返回数据

            # 用法示例：
            jsonl_file_path = “./admin.jsonl”
            数据 = load_data_from_jsonl(jsonl_file_path)

            # 从数据中提取文本和实体
            texts = [“John 是个好孩子”,“Goole 是软件”]
            实体=[
                [
                    {“标签”：“人”，“姓名”：“约翰”}，
                    {“标签”：“位置”，“城市”：“纽约”}
                ],
                [
                    {“标签”:“组织”,“名称”:“Google”},
                    {“标签”：“日期”，“年份”：“2023”}
                ]
            ]

            # 创建自定义数据集的实例
            数据集= NERDataset（文本=文本，实体=实体，标记器=标记器）

            # 使用训练器中的数据集
            训练参数 = 训练参数（
                输出目录=“./输出”,
                num_train_epochs=3,
                per_device_train_batch_size=16，
                evaluation_strategy=“纪元”，
            ）

            教练=教练（
                型号=型号，
                参数=训练参数，
                train_dataset=数据集，
                # ...其他培训师参数...
            ）

            训练师.train()
]]></description>
      <guid>https://stackoverflow.com/questions/77725013/me-to-resolve-below-error-trying-to-fine-tune-roberta-model-for-ner</guid>
      <pubDate>Thu, 28 Dec 2023 04:28:04 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络不学习</title>
      <link>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</link>
      <description><![CDATA[我正在尝试在包含 1500 张图像（15 个类别）的训练集上训练用于图像识别的卷积神经网络。有人告诉我，采用这种架构和从均值为 0、标准差为 0.01 的高斯分布得出的初始权重以及初始偏差值为 0 的情况，在适当的学习率下，它应该达到 30 左右的准确度%。
但是，它根本没有学到任何东西：准确率与随机分类器相似，并且训练后的权重仍然遵循正态分布。我做错了什么？
这是神经网络
class simpleCNN(nn.Module)：
  def __init__(自身):
    super(simpleCNN,self).__init__() #初始化模型

    self.conv1=nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,stride=1) #输出图像大小为(size+2*padding-kernel)/stride --&gt;62*62
    self.relu1=nn.ReLU()
    self.maxpool1=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像62/2--&gt;31*31

    self.conv2=nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,stride=1) #输出图像为29*29
    self.relu2=nn.ReLU()
    self.maxpool2=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像为29/2--&gt;14*14（MaxPool2d近似大小与floor）

    self.conv3=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1) #输出图像为12*12
    self.relu3=nn.ReLU()

    self.fc1=nn.Linear(32*12*12,15) #16 个通道 * 16*16 图像（64*64，步幅为 2 的 2 个 maxpooling），15 个输出特征=15 个类
    self.softmax = nn.Softmax(dim=1)

  def 前向（自身，x）：
    x=self.conv1(x)
    x=self.relu1(x)
    x=self.maxpool1(x)

    x=self.conv2(x)
    x=self.relu2(x)
    x=self.maxpool2(x)

    x=self.conv3(x)
    x=self.relu3(x)

    x=x.view(-1,32*12*12)

    x=self.fc1(x)
    x=self.softmax(x)

    返回x

初始化：
def init_weights(m):
  如果 isinstance(m,nn.Conv2d) 或 isinstance(m,nn.Linear)：
    nn.init.normal_(m.weight,0,0.01)
    nn.init.zeros_(m.bias)

模型 = simpleCNN()
模型.应用（init_weights）

训练函数：
loss_function=nn.CrossEntropyLoss()
优化器=optim.SGD(model.parameters(),lr=0.1,动量=0.9)

def train_one_epoch(epoch_index,loader):
  运行损失=0

  对于 i，枚举（加载器）中的数据：

    input,labels=data #获取小批量
    输出=模型（输入）#前向传递

    loss=loss_function(outputs,labels) #计算损失
    running_loss+=loss.item() #总结到目前为止处理的小批量的损失

    Optimizer.zero_grad() #重置梯度
    loss.backward() #计算梯度
    optimizer.step() #更新权重

  return running_loss/(i+1) # 每个小批量的平均损失


培训：
&lt;前&gt;&lt;代码&gt;纪元=20

best_validation_loss=np.inf

对于范围内的纪元（EPOCHS）：
  print(&#39;纪元{}:&#39;.format(纪元+1))

  模型.train(True)
  train_loss=train_one_epoch(epoch,train_loader)

  运行验证损失=0.0

  模型.eval()

  with torch.no_grad(): # 禁用梯度计算并减少内存消耗
    对于 i，枚举中的 vdata（validation_loader）：
      vinputs,vlabels=vdata
      v输出=模型（v输入）
      vloss=loss_function(v输出,v标签)
      running_validation_loss+=vloss.item()
  验证损失=运行验证损失/(i+1)
  print(&#39;LOSS 训练：{} 验证：{}&#39;.format(train_loss,validation_loss))

  if validation_loss
使用默认初始化，效果会好一些，但使用高斯应该可以达到 30%。
您能发现一些可能导致它无法学习的问题吗？我已经尝试过不同的学习率和动力。]]></description>
      <guid>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</guid>
      <pubDate>Fri, 22 Dec 2023 14:06:23 GMT</pubDate>
    </item>
    <item>
      <title>llama.cpp 抱歉，您的 MOSTLY_Q4_1_SOME_F16 类型的 GGJTv1 文件不符合转换条件</title>
      <link>https://stackoverflow.com/questions/77337548/llama-cpp-sorry-your-ggjtv1-file-of-type-mostly-q4-1-some-f16-is-not-eligible-f</link>
      <description><![CDATA[这是我用来转换的存储库
https://github.com/ggerganov/llama.cpp
python3.10 ~/llama.cpp/convert-llama-ggml-to-gguf.py --input ~/llama.cpp/models/gpt4-x-alpaca-13b-native-4bit-128g /ggml-model-q4_1.bin --输出 ~/llama.cpp/models/gpt4-x-alpaca-13b-native-4bit-128g/ggml-model-q4_1.gguf

ValueError：GGJTv2 中的量化已更改。只能转换早于 GGJTv2 的未量化 GGML 文件。抱歉，您的 MOSTLY_Q4_1_SOME_F16 类型的 GGJTv1 文件不符合转换条件。
如您所见，我无法将 bin 文件转换为 gguf 文件。
我从这里得到这个文件
https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/tree/main/gpt4-x-alpaca-13b-ggml -q4_1-from-gptq-4bit-128g
它给了我
ggml-model-q4_1.bin
这个问题源于我只是尝试直接从 llama.cpp/main.py 运行模型，这就是发生的情况
&lt;预&gt;&lt;代码&gt;./main -m ~/llama.cpp/models/gpt4-x-alpaca-13b-native-4bit-128g/ggml-model-q4_1.bin -t 4 -c 2048 -n 2048 - -颜色-i--指示

失败了
&lt;块引用&gt;
日志启动 main: build = 1407 (465219b) main: 使用 cc 构建 (Ubuntu
9.4.0-1ubuntu1~20.04.2) 9.4.0 对于 x86_64-linux-gnu main：seed = 1697867527 gguf_init_from_file：无效的魔术字符 tjgg。错误
加载模型：llama_model_loader：无法加载模型
模型/gpt4-x-alpaca-13b-native-4bit-128g/ggml-model-q4_1.bin
llama_load_model_from_file：加载模型失败
llama_init_from_gpt_params：错误：无法加载模型
&#39;models/gpt4-x-alpaca-13b-native-4bit-128g/ggml-model-q4_1.bin&#39; 主要：
错误：无法加载模型

如有任何帮助，我们将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77337548/llama-cpp-sorry-your-ggjtv1-file-of-type-mostly-q4-1-some-f16-is-not-eligible-f</guid>
      <pubDate>Sat, 21 Oct 2023 19:48:00 GMT</pubDate>
    </item>
    <item>
      <title>我需要清楚地预测 X 测试、X 训练、y 测试、y 训练</title>
      <link>https://stackoverflow.com/questions/70721510/i-need-clarity-with-prediction-of-x-test-x-train-y-test-y-train</link>
      <description><![CDATA[在线性回归模型中，假设我们有 3 个自变量（年龄、身高、性别）和 1 个因变量（糖尿病），然后我们将模型拆分为 X 训练，即（例如 70%）自变量数据对于训练，X测试-&gt;即30%的自变量数据用于测试
y 火车-&gt;即（例如70%）用于训练的因变量数据，y检验-&gt;即30%的因变量数据用于测试
因此，当我们预测 X 检验或预测 X 检验时，我们是在预测自变量的值还是在预测因变量（糖尿病？）]]></description>
      <guid>https://stackoverflow.com/questions/70721510/i-need-clarity-with-prediction-of-x-test-x-train-y-test-y-train</guid>
      <pubDate>Sat, 15 Jan 2022 12:37:46 GMT</pubDate>
    </item>
    <item>
      <title>从huggingface特征提取管道中获取句子嵌入</title>
      <link>https://stackoverflow.com/questions/64685243/getting-sentence-embedding-from-huggingface-feature-extraction-pipeline</link>
      <description><![CDATA[如何从 Huggingface 的特征提取管道中获取整个句子的嵌入？
我了解如何获取每个标记的特征（如下），但如何获取整个句子的整体特征？
feature_extraction = pipeline(&#39;feature-extraction&#39;, model=“distilroberta-base”, tokenizer=“distilroberta-base”)
features = feature_extraction(“我是句子”)
]]></description>
      <guid>https://stackoverflow.com/questions/64685243/getting-sentence-embedding-from-huggingface-feature-extraction-pipeline</guid>
      <pubDate>Wed, 04 Nov 2020 17:52:22 GMT</pubDate>
    </item>
    <item>
      <title>鉴于您有多个虚拟列，如何预测值？</title>
      <link>https://stackoverflow.com/questions/63432916/how-to-predict-values-given-that-you-have-multiple-dummy-columns</link>
      <description><![CDATA[我有一个类似于以下内容的数据框：
 薪资 职务 Raiting Company_Name 地点 资历
0 100 SE 5 苹果 SF 副总裁
1 120 DS 4 三星 la Jr
2 230 QA 5 谷歌 sd Sr


（我的 df 具有比这更多的分类特征）
通常，当从模型进行预测时，它会像这样
in[1]: inModel_name.predict(catagory_1, catagory_2,..etc)
输出[2]：预测变量

然而，在使用pd.get_dummies之后，根据您创建的分类特征的数量，您会获得更多的列，这使得我之前提到的方法在尝试预测数据时变得不切实际。如何引用多列而不是手动输入 0。]]></description>
      <guid>https://stackoverflow.com/questions/63432916/how-to-predict-values-given-that-you-have-multiple-dummy-columns</guid>
      <pubDate>Sun, 16 Aug 2020 03:43:39 GMT</pubDate>
    </item>
    <item>
      <title>SVM 二元分类器为所有测试数据预测一类</title>
      <link>https://stackoverflow.com/questions/57991396/svm-binary-classifier-predicts-one-class-for-all-of-test-data</link>
      <description><![CDATA[我有一个包含 10 个特征的分类问题，我必须预测 1 或 0。当我训练 SVC 模型时，通过训练测试分割，数据测试部分的所有预测值均为 0。数据具有以下 0-1 计数：

0：1875
1：1463

训练模型的代码如下：
从 sklearn.svm 导入 SVC
模型 = SVC()
model.fit(X_train, y_train)
pred= model.predict(X_test)
从 sklearn.metrics 导入 precision_score
准确度分数（y_测试，预测）

为什么它在所有情况下都预测 0？]]></description>
      <guid>https://stackoverflow.com/questions/57991396/svm-binary-classifier-predicts-one-class-for-all-of-test-data</guid>
      <pubDate>Wed, 18 Sep 2019 11:10:06 GMT</pubDate>
    </item>
    </channel>
</rss>