<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 11 Feb 2024 15:13:52 GMT</lastBuildDate>
    <item>
      <title>Python机器学习pytorch测试/训练epoch结果问题</title>
      <link>https://stackoverflow.com/questions/77977231/python-machine-learning-pytorch-test-train-epoch-results-problem</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77977231/python-machine-learning-pytorch-test-train-epoch-results-problem</guid>
      <pubDate>Sun, 11 Feb 2024 15:07:50 GMT</pubDate>
    </item>
    <item>
      <title>字符串“loss”被传递给 metric_name() 而不是指标名称</title>
      <link>https://stackoverflow.com/questions/77977110/string-loss-being-passed-to-metric-name-instead-of-metric-name</link>
      <description><![CDATA[我是 ML 新手，现在只是在学习教程，但收到错误：
ValueError：无法解释指标标识符：丢失
在：
\keras\src\metrics\__init__.py:205，在 get(identifier) 中
看起来在wrapper.py第532行metric_name(key)应该接收损失函数的名称，但它实际上接收字符串“loss”
以下是错误的相关代码：
def buildNetwork():
    分类器=顺序（）
    classificator.add（密集（单位= 20，激活=&#39;relu&#39;，kernel_initializer =&#39;random_uniform&#39;，input_shape =（30，）））
    classificator.add（密集（单位= 20，激活=&#39;relu&#39;，kernel_initializer =&#39;random_uniform&#39;））
    classificator.add（密集（单位= 1，激活=&#39;sigmoid&#39;））
    优化器= keras.optimizers.Adam(learning_rate=0.001,weight_decay=0.000001)
    classificator.compile（优化器=优化器，损失=&#39;binary_crossentropy&#39;，指标=[&#39;binary_accuracy&#39;]）
    返回分类器


分类器 = KerasClassifier(model=buildNetwork,batch_size=10,epochs=100, loss=&#39;binary_crossentropy&#39;)

分数 = cross_val_score(估计器=分类器, X=数据, y=真相, cv=10, error_score=&#39;raise&#39;)

如果在keras_metric_get中我手动设置identifier =“binary_crossentropy”它工作正常
我不知道是否是兼容性问题，但我有 Keras 3.0.8、TF 2.15.0 和 SciKeras 0.12.0]]></description>
      <guid>https://stackoverflow.com/questions/77977110/string-loss-being-passed-to-metric-name-instead-of-metric-name</guid>
      <pubDate>Sun, 11 Feb 2024 14:32:54 GMT</pubDate>
    </item>
    <item>
      <title>关于 Evolve 图卷积网络的机器学习</title>
      <link>https://stackoverflow.com/questions/77977043/machine-learning-about-evolve-graph-convolution-network</link>
      <description><![CDATA[如何使用 Jupyter Notebook 的 Evolve Graph 卷积网络代码，使用 Kaggle 上的 Elliptic_Bitcoin_Heist 数据。
我无法编码。大家都可以帮助我吗？我来自越南。我已经阅读了该程序 https://github.com/IBM/EvolveGCN.git 但是我不知道如何使用它。我尝试在 ubuntu 上运行，但总是出错，我无法安装它。我想用jupyter笔记本编码]]></description>
      <guid>https://stackoverflow.com/questions/77977043/machine-learning-about-evolve-graph-convolution-network</guid>
      <pubDate>Sun, 11 Feb 2024 14:10:41 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit-learn 估计器作为变压器</title>
      <link>https://stackoverflow.com/questions/77976770/use-scikit-learn-estimator-as-transformer</link>
      <description><![CDATA[我想创建自己的 scikit-learn 转换器，用于对包含分类的数字特征进行编码，例如邮政编码或行业代码（NAICS、MCC 等）。在这些类型的代码中有一个结构：例如MCC 3000-3999 是“旅行和娱乐”，它进一步细分为更细粒度的类别，例如“航空公司”、“汽车租赁”等。我们不能将它们用作序数特征，但如果我们将它们视为纯分类特征（例如，通过 One -Hot-Encoding）我们需要选择在代码结构的哪个级别应用特征编码。
为了解决这个问题，我创建了自己的 scikit-learn 变压器，它是 TargetEncoder 使用决策树。代码如下所示。重要的是要认识到，在模型训练期间，应使用样本外决策树回归分数来避免过度拟合。因此，我实现了自己的 fit_transform 函数来生成这些样本外分数：
从 sklearn.tree 导入 DecisionTreeRegressor

从 sklearn.base 导入 TransformerMixin、BaseEstimator
从 sklearn.model_selection 导入 cross_val_predict

类 TaxonomyEncoder（TransformerMixin，BaseEstimator）：

def __init__(自身, n_leafs=10, cv=3):
    self.n_leafs = n_leafs
    自我简历 = 简历

def fit(self, X, y=None):
    self.tree_ = DecisionTreeRegressor(max_leaf_nodes=self.n_leafs).fit(X,y)
    返回自我

def 变换（自身，X）：
    返回 self.tree_.predict(X).reshape(-1,1)

def fit_transform(self, X, y=None):
    self.tree_ = DecisionTreeRegressor(max_leaf_nodes=self.n_leafs)
    返回 cross_val_predict(self.tree_, X, y, cv=self.cv).reshape(-1,1)

转换器工作正常，除非在 ColumnTransformer 中使用：
从 sklearn.compose 导入 ColumnTransformer

变压器 = ColumnTransformer([(&#39;分类法&#39;, TaxonomyEncoder(), [&#39;mcc&#39;])])
变压器.fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
Transformer.transform(df[[&#39;mcc&#39;]])

然后我得到决策树尚未拟合的错误：
NotFittedError：此 DecisionTreeRegressor 实例尚未安装。在使用此估计器之前，请使用适当的参数调用“fit”。

显然，scikit-learn 在导致此错误的表面下进行了一些检查。请注意，实际上没有理由在 cross_val_predict 函数中重新拟合决策树。我该如何解决这个问题？
下面显示了重现该错误的完整工作示例：
导入 pandas 作为 pd
df = pd.DataFrame({&#39;mcc&#39;:[3000,3500,7339], &#39;y&#39;:[0,0,1]})

te = TaxonomyEncoder().fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
te.transform(df[[&#39;mcc&#39;]])

给出：
数组([[0.],
       [0.],
       [1.]])

并且 fit_transform 也给出了预期的结果：
te.fit_transform(df[[&#39;mcc&#39;]], df[&#39;y&#39;])

数组([[0.],
       [0.],
       [0.]])

但是当包装在 ColumnTransformer 中时，事情就会出错：
transformer = ColumnTransformer([(&#39;taxonomy&#39;, TaxonomyEncoder(), [&#39;mcc&#39;])])
变压器.fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
Transformer.transform(df[[&#39;mcc&#39;]])
]]></description>
      <guid>https://stackoverflow.com/questions/77976770/use-scikit-learn-estimator-as-transformer</guid>
      <pubDate>Sun, 11 Feb 2024 12:34:35 GMT</pubDate>
    </item>
    <item>
      <title>我在实现 Graycomatrix 方法时收到“JpegImageFile”对象不可下标错误</title>
      <link>https://stackoverflow.com/questions/77976714/i-am-getting-an-jpegimagefile-object-is-not-subscriptable-error-while-implemen</link>
      <description><![CDATA[代码错误
我试图获取 2 个图像的 GLCM，每个图像分为 4 个补丁，但发生了此错误。这2张图片都是jpg的。该方法还可以获得补丁之间的相异性和相关性。我从这里得到了代码 https://scikit-image.org/文档/stable/auto_examples/features_detection/plot_glcm.html]]></description>
      <guid>https://stackoverflow.com/questions/77976714/i-am-getting-an-jpegimagefile-object-is-not-subscriptable-error-while-implemen</guid>
      <pubDate>Sun, 11 Feb 2024 12:14:45 GMT</pubDate>
    </item>
    <item>
      <title>分类任务问题</title>
      <link>https://stackoverflow.com/questions/77976439/classification-task-issues</link>
      <description><![CDATA[我有一个关于二元变量分类任务的概念性问题。我可用的数据集如下：

id，设备 ID
性别，目标变量
日
小时
内容、具有许多唯一值的分类变量

这些数据代表客户进行的网络搜索，并且出现了两个问题：

目标变量高度不平衡，女性较多，男性较少。
内容变量有许多唯一值。

考虑到我的时间有限，并且模型结果背后的推理比模型本身更重要，任何人都可以提出解决这两个问题的最佳方法吗？
我正在寻求这两个问题的解决方案，以下是我的考虑：

对内容变量应用诸如单热编码之类的技术会导致维度问题。我可以考虑设置阈值并考虑最常见的值，同时将其他值分类为“其他”。
应用目标编码会反映出类别不平衡的问题。

提前非常感谢您。]]></description>
      <guid>https://stackoverflow.com/questions/77976439/classification-task-issues</guid>
      <pubDate>Sun, 11 Feb 2024 10:35:58 GMT</pubDate>
    </item>
    <item>
      <title>我的线性回归有什么错误吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77976197/is-there-any-bug-in-my-linear-regression</link>
      <description><![CDATA[我不知道为什么这个线性回归的成本函数在迭代后会增加。你能指出这个错误吗？
&lt;前&gt;&lt;代码&gt;W=np.zeros((4,1))
b=0。

def 向前（X，W）：
  z=np.dot(W.T,X)+b
  返回 z

z=向前（X，W）

def计算成本（y_hat，y）：
  返回 1/10844*np.power(y_hat-y,2)

计算成本(y_hat=z,y=Y).mean()

def back_prop(y_hat=z,X=X):
  dz=(y_hat-Y)
  dW=1/5422*np.sum(np.dot(dz,X.T))
  db=1/5422*np.sum(dz)
  W1=W-0.001*dW
  b1=b-0.001*db
  返回W1,b1

W,b=back_prop()

计算成本(y_hat=z,y=Y).mean()

对于范围（10）内的纪元：
  z=向前（X，W）
  W,b=back_prop()

计算成本(y_hat=z,y=Y).mean()

如果您想查看完整的代码，请点击此处的代码和数据集链接
我预计成本函数会下降。但它上升了希望你们能帮忙。
在此处输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/77976197/is-there-any-bug-in-my-linear-regression</guid>
      <pubDate>Sun, 11 Feb 2024 09:14:35 GMT</pubDate>
    </item>
    <item>
      <title>如何在ImageDataGenerator中裁剪图像数据和坐标数据？</title>
      <link>https://stackoverflow.com/questions/77975877/how-to-crop-image-data-and-coordinate-data-in-imagedatagenerator</link>
      <description><![CDATA[我的任务是头影测量地标定位。我在此数据框中显示了带有地标点坐标的图像路径。

&lt;标题&gt;

文件名
X1
Y1
X2
Y2
X3
Y3


&lt;正文&gt;

/Images_data/binary0006.png
89
80
39
135
58
105


/Images_data/binary0008.png
37
70
86
42
89
154


/Images_data/binary0007.png
50
76
138
56
97
187


/Images_data/binary0003.png
55
92
145
78
59
199


/Images_data/binary0005.png
91
64
55
43
48
253


/Images_data/binary0004.png
100
76
70
51
78
211



我需要创建随机裁剪图像和标签 (X1,Y1,...,X3,Y3)
我尝试在下面定义随机裁剪函数
def random_crop(图像):
    高度、宽度 = image.shape[:2]
    random_array = np.random.random(size=4);
    w = int((宽度*0.5) * (1+random_array[0]*0.5))
    h = int((高度*0.5) * (1+random_array[1]*0.5))
    x = int(random_array[2] * (宽度-w))
    y = int(random_array[3] * (高度-h))

    image_crop = 图像[y:h+y, x:w+x, 0:3]
    image_crop = tf.image.resize(image_crop,(180, 180))
    返回图像_裁剪

和数据生成器
datagen = ImageDataGenerator(rescale=1./255,
                             验证分割=0.2，
                             旋转范围=0.2，
                             宽度偏移范围=0.2，
                             height_shift_range=0.2，
                             剪切范围=0.2，
                             缩放范围=0.2，
                             水平翻转=真，
                             fill_mode=&#39;最近&#39;,
                             预处理函数=随机裁剪）

rom tensorflow.keras.preprocessing.image 导入 ImageDataGenerator
img_高度, img_宽度 = 180, 180
批量大小 = 8
train_generator = datagen.flow_from_dataframe(
    数据框=df，
    目录=data_dir,
    x_col=&quot;图像编号&quot;,
    y_col=[“X1”、“Y1”、“X2”、“Y2”、“X3”、“Y3”、“X4”、“Y4”]，
    目标大小=（img_高度，img_宽度），
    批量大小=批量大小，
    class_mode=“原始”，
    随机播放=真，
    种子=9，
    子集＝“训练”
）

validation_generator = datagen.flow_from_dataframe(
    数据框=df，
    目录=data_dir,
    x_col=&quot;图像编号&quot;,
    y_col=[“X1”、“Y1”、“X2”、“Y2”、“X3”、“Y3”、“X4”、“Y4”]，
    目标大小=（img_高度，img_宽度），
    验证分割=0.2，
    批量大小=批量大小，
    class_mode=“原始”，
    随机播放=真，
    种子=9，
    子集＝“验证”
）

随机裁剪适用于图像数据，但我不确定标签数据，我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77975877/how-to-crop-image-data-and-coordinate-data-in-imagedatagenerator</guid>
      <pubDate>Sun, 11 Feb 2024 06:53:49 GMT</pubDate>
    </item>
    <item>
      <title>如何在苹果 M1 Pro 芯片组上的 XGBoost 中启用 GPU</title>
      <link>https://stackoverflow.com/questions/77975756/how-to-enable-gpu-in-xgboost-on-apple-m1-pro-chipset</link>
      <description><![CDATA[我尝试在带有设备 = cuda 的 Windows 上使用 GPU 进行 XGBoost 训练，它有效并且训练时间大大减少，现在我想在我的 Mac M1 Pro 上进行此实验。
如何在 m1 pro 芯片组上启用 GPU 的 XGBoost。
我尝试查找无法找到信息的文档。]]></description>
      <guid>https://stackoverflow.com/questions/77975756/how-to-enable-gpu-in-xgboost-on-apple-m1-pro-chipset</guid>
      <pubDate>Sun, 11 Feb 2024 05:51:44 GMT</pubDate>
    </item>
    <item>
      <title>针对网络日志数据微调 LLM [关闭]</title>
      <link>https://stackoverflow.com/questions/77975336/fine-tune-llm-for-network-log-data</link>
      <description><![CDATA[我想我明白什么是微调。
但我想知道这是否可能。
我实时向我的 llm 模型提供网络日志，假设如果我向它询问有关某个 IP 的信息，它应该理解日志并提供详细信息。
也许我可以实时清理数据，将其存储在数据库中并使用 LLM 智能查询数据库。
很想听听您的意见。
我尝试阅读 Medium 和 YouTube 上的博客。]]></description>
      <guid>https://stackoverflow.com/questions/77975336/fine-tune-llm-for-network-log-data</guid>
      <pubDate>Sun, 11 Feb 2024 01:52:15 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-Learn 1.3.0 中的 MinMaxScaler 在同一系统上的相同输入值的不同 IDE 中给出不同的结果</title>
      <link>https://stackoverflow.com/questions/77974757/minmaxscaler-from-scikit-learn-1-3-0-gives-different-result-in-different-ide-for</link>
      <description><![CDATA[我对模型的输入数组进行了预处理，如下所示，然后使用 MinMaxScaler 对其进行转换。缩放器已经根据训练数据进行了拟合。
&lt;前&gt;&lt;代码&gt;[
   {
        “SCHEME_CODE”：“10003”，
        “CURRENCY_CODE”：“NGN”，
        “IS_CREDIT”：1，
        “IS_PEP”：1，
        “SOL_ID”：“73”，
        “最高”：0，
        “电视音量”：0，
        “T值”：0，
        “STR”：0，
        “TM”：0，
        “点击率”：0，
        “DATE_OPENED”：“2009-07-15 00:00:00”
    }
]

在jupyter笔记本上缩放后的输出是
&lt;预&gt;&lt;代码&gt;[[-4.11930983e-04 -5.84393172e-02 -3.79097742e-04 0.00000000e+00
0.00000000e+00 0.00000000e+00 6.59090909e-01 5.45454545e-01
4.66666667e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]]

注意：为了简洁和清晰起见，我删除了其他预处理步骤。
我使用 scikit-learn 版本 1.3.0 中的 joblib 保存了 MinMaxScaler。
我的系统上运行的是python version 3.9.13。在 Jupyter Lab 的 python 脚本中的同一系统上，我取消了缩放器并转换了相同的输入。我得到的值在数组的第 6、7 和 8 个元素处略有不同，如下所示。
&lt;预&gt;&lt;代码&gt;[[-4.11930983e-04 -5.84393172e-02 -3.79097742e-04 0.00000000e+00
0.00000000e+00 0.00000000e+00 -4.50000000e+01 -9.09090909e-02
-3.33333333e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]]

我在训练模型的同一个 Jupyter Notebook 会话中进一步使用原始缩放器转换输入，并且得到了与在 Jupyter Notebook 中取消缩放器时相同的数组。 
&lt;预&gt;&lt;代码&gt;[[-4.11930983e-04 -5.84393172e-02 -3.79097742e-04 0.00000000e+00
0.00000000e+00 0.00000000e+00 6.59090909e-01 5.45454545e-01
4.66666667e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]]

我在 vscode 中解开了标量并转换了 python 脚本中的相同输入，我得到了与 Jupyter Lab 不同的值。
&lt;预&gt;&lt;代码&gt;[[-4.11930983e-04 -5.84393172e-02 -3.79097742e-04 0.00000000e+00
0.00000000e+00 0.00000000e+00 -4.50000000e+01 -9.09090909e-02
-3.33333333e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]]

我已审阅了此处对问题的回答和建议，并且此处，但他们的问题与我的不同。&lt; /p&gt;
总结一下：当我在另一个 IDE（例如 jupyter lab、vscode）中取消缩放器以转换相同的输入但在同一系统上时，MinMaxScaler 会给出不同的 Transform 输出。&lt; /p&gt;
这里可能存在什么问题以及如何解决它？]]></description>
      <guid>https://stackoverflow.com/questions/77974757/minmaxscaler-from-scikit-learn-1-3-0-gives-different-result-in-different-ide-for</guid>
      <pubDate>Sat, 10 Feb 2024 21:16:59 GMT</pubDate>
    </item>
    <item>
      <title>创建神经网络和训练的问题</title>
      <link>https://stackoverflow.com/questions/77974163/issues-with-creating-a-neural-network-traning</link>
      <description><![CDATA[我已经被模型无法训练的问题困扰了一段时间，我希望得到一些帮助。这是部分代码
link =“https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2”
input_layer = tf.keras.layers.Input(shape=(224, 224, 3), dtype=tf.float32, name=“输入”, trainable=False)
feature_extractor = hub.KerasLayer(link, trainable=False)

模型 = tf.keras.Sequential([
    输入层，
    特征提取器，
    tf.keras.layers.Dense(data_info.features[&#39;label&#39;].num_classes,activation=“softmax”)
]）

这是我收到的错误：
TypeError Traceback（最近一次调用最后一次）
[64] 中的单元格，第 7 行
      4 input_layer = tf.keras.layers.Input(shape=(224, 224, 3), dtype=tf.float32, name=“输入”)
      5 feature_extractor = hub.KerasLayer(link, trainable=False)
----&gt; 7 模型 = tf.keras.Sequential([
      8 输入层，
      9 特征提取器，
     10 tf.keras.layers.Dense(data_info.features[&#39;label&#39;].num_classes, 激活=“softmax”)
     11]）
     13 model.compile(optimizer=“adam”,loss=“sparse_categorical_crossentropy”,metrics=[“accuracy”])
     14 模型.summary()

文件 c:\Users\steel\AppData\Local\Programs\Python\Python310\lib\site-packages\tensorflow\python\trackable\base.py:204，在 no_automatic_dependency_tracking.._method_wrapper(self, *args) , **夸格斯)
    第202章
    203 尝试：
--&gt; [第 204 章]
    205 最后：
    206 self._self_setattr_tracking = previous_value # pylint：禁用=受保护访问

文件 c:\Users\steel\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\src\utils\traceback_utils.py:70，位于filter_traceback..error_handler(*args, * *夸格斯）
     67、filtered_tb = _process_traceback_frames（e.__traceback__）
     68 # 要获取完整的堆栈跟踪，请调用：
     69 # `tf.debugging.disable_traceback_filtering()`
---&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
...


层“keras_layer_28”接收的调用参数（类型 KerasLayer）：
  输入= tf.Tensor（形状=（无，224，224，3），dtype = float32）
  • 培训=无

我尝试了许多不同的方法，例如添加或删除输入层以及更改该层中的一些参数，但我运气不佳。我想知道这是否是因为我提取的网络造成的，但想首先得到一些确认，因为我不能 100% 确定如何确定这是否是问题所在。]]></description>
      <guid>https://stackoverflow.com/questions/77974163/issues-with-creating-a-neural-network-traning</guid>
      <pubDate>Sat, 10 Feb 2024 17:59:42 GMT</pubDate>
    </item>
    <item>
      <title>当有多个输入时，如何使用检索器使用 langchain 创建抹布链？</title>
      <link>https://stackoverflow.com/questions/77964228/how-can-i-create-a-rag-chain-with-langchain-using-a-retriever-when-having-multip</link>
      <description><![CDATA[我在尝试了解如何使用“|”时遇到一些问题。声明链时langchain中的管道符号。
prompt_template = “””
  仅根据以下上下文进行响应：
  {语境}

作为负责优化给定项目的经验丰富的专家，您的专业知识至关重要
应对挑战并抓住机遇。

问题和机遇：
{问题和机会}

业务目标：
{业务_目标}

项目介绍：
{描述}

请以 JSON 格式提供全面的回复，包括以下内容
成分：

1. 建议的解决方案：
 - 制定详细的计划来克服已发现的挑战并利用
机会。

2. 技术细节：
 - 对该项目指定的技术进行深入分析。
 - 指定要使用的编程语言、框架和平台。
 - 示例：Python、Azure、Pytorch、Tensorflow、AWS、Openai、LLM...

 示例输出（JSON 格式）：
{{

 &quot;solution&quot;: &quot;这里有您的详细解决方案&quot;,
 “技术”：[“技术1”、“技术2”、...]，
 }}
”“”

提示 = ChatPromptTemplate.from_template(prompt_template)

然后我构建我的检索
retriever = vectordb.as_retriever()

和我的法学硕士
llm = AzureChatOpenAI(
    api_key=openai_api_key,
    api_version=openai_api_version,
    azure_endpoint=openai_api_base,
    模型=llm_模型）

然后我添加我的输出解析器
from langchain.output_parsers import ResponseSchema, StructuredOutputParser
从 langchain.callbacks 导入 get_openai_callback

Solution_schema = ResponseSchema(名称=“解决方案”，描述=“给定的”)
Technologies_schema = ResponseSchema（名称=“技术”，描述=“给定的”）

响应模式= [解决方案模式，
                    技术_架构]

输出解析器 = StructuredOutputParser.from_response_schemas(response_schemas)

最后，当我尝试使用链条将所有这些组合在一起时，我惨遭失败
&lt;前&gt;&lt;代码&gt;rag_chain = (
    {“上下文”：检索器，“问题和机会”：RunnablePassthrough()，“业务目标”：RunnablePassthrough()，“描述”：RunnablePassthrough()}
    |迅速的
    |勒姆
    |输出解析器
）

rag_chain.invoke（问题和机会，业务目标，描述）

收到此错误：
TypeError Traceback（最近一次调用最后一次）
&lt;ipython-input-30-3a4e499badd2&gt;在&lt;细胞系：1&gt;()
----&gt; 1 rag_chain.invoke（问题和机会，业务目标，描述）

类型错误：RunnableSequence.invoke() 接受 2 到 3 个位置参数，但给出了 4 个
]]></description>
      <guid>https://stackoverflow.com/questions/77964228/how-can-i-create-a-rag-chain-with-langchain-using-a-retriever-when-having-multip</guid>
      <pubDate>Thu, 08 Feb 2024 19:12:09 GMT</pubDate>
    </item>
    <item>
      <title>就地修剪 nn.Linear 权重会导致意外错误，需要稍微奇怪的解决方法。需要解释</title>
      <link>https://stackoverflow.com/questions/77959410/pruning-nn-linear-weights-inplace-causes-unexpected-error-requires-slightly-wei</link>
      <description><![CDATA[失败
导入火炬

def 测试1():
  层 = nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试1()

有错误
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
RuntimeError Traceback（最近一次调用最后一次）
&lt;ipython-input-3-bb36a010bd86&gt;在&lt;细胞系：10&gt;()
      8 x = 5 - torch.sum(layer(torch.ones(90)))
      9 x.backward()
---&gt; 10 测试1()
     11 # 这也有效
     12

2帧
/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py 向后（张量，grad_tensors，retain_graph，create_graph，grad_variables，输入）
    249 # 一些 Python 版本打印多行函数的第一行
    [第 250 章]
--&gt; 251 Variable._execution_engine.run_backward( # 调用 C++ 引擎来运行向后传递
    252个张量，
    第253章

RuntimeError: 函数 TBackward0 在索引 0 返回无效渐变 - 得到 [10, 90] 但预期形状与 [10, 100] 兼容

这有效
导入火炬

def test2():
  层 = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  del x #主要变化
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试2()

这也有效
导入火炬
def test3():
  层 = torch.nn.Linear(100, 10)
  x = 5 - torch.sum(layer(torch.ones(100)))
  x.backward()
  层.权重.数据 = 层.权重.数据[:, :90]
  层.权重.grad.数据 = 层.权重.grad.数据[:, :90]
  layer.weight = torch.nn.Parameter(layer.weight) #主要变化
  x = 5 - torch.sum(layer(torch.ones(90)))
  x.backward()
测试3()

我在尝试实现一篇关于模型剪枝（Temporal Neuron Variance Pruning）的论文时遇到了这个问题。我相信这与 autograd 图有关，但我不确定到底发生了什么。我已经看到了有关修剪的链接，并使用第三个片段让我的代码正常工作。我现在正在尝试找出为什么 1 和 2 不起作用。是否有一些解释为什么这些几乎相同的代码片段有效或失败？
我想弄清楚的要点 -

什么是TBackward0
在哪里定义的
哪里引发了运行时错误
为什么需要与旧形状兼容 - 特别是当梯度已正确修改时（我假设我已正确编辑张量，因为情况 2、3 有效）
我可以更改其他内容（除了 2 个工作案例之外）来实现此功能吗？
]]></description>
      <guid>https://stackoverflow.com/questions/77959410/pruning-nn-linear-weights-inplace-causes-unexpected-error-requires-slightly-wei</guid>
      <pubDate>Thu, 08 Feb 2024 05:17:32 GMT</pubDate>
    </item>
    <item>
      <title>Conda 环境未显示在 JupyterLab 桌面中</title>
      <link>https://stackoverflow.com/questions/69529374/conda-environment-not-showing-in-jupyterlab-desktop</link>
      <description><![CDATA[我在系统中安装了 anaconda，环境名为 ML_env，Python 版本为 3.6.13。 JupyterLab 浏览器可以在安装了各种机器学习库的环境中顺利运行。
最近，我安装了 JupyterLab 桌面版 - 版本 3.1.13-1。我想将 JupyerLab Desktop 的内核更改为 anaconda 环境ML_env。但是，我没有获得任何更改内核的选项。请查看随附的屏幕截图。
有没有办法将JupyterLab桌面的内核更改为conda环境？请帮忙。 ]]></description>
      <guid>https://stackoverflow.com/questions/69529374/conda-environment-not-showing-in-jupyterlab-desktop</guid>
      <pubDate>Mon, 11 Oct 2021 16:12:47 GMT</pubDate>
    </item>
    </channel>
</rss>