<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 19 Aug 2024 21:16:57 GMT</lastBuildDate>
    <item>
      <title>GradCam：层 Sequenced_1 从未被调用，因此没有定义的输出</title>
      <link>https://stackoverflow.com/questions/78889743/gradcam-the-layer-sequential-1-has-never-been-called-and-thus-has-no-defined-ou</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78889743/gradcam-the-layer-sequential-1-has-never-been-called-and-thus-has-no-defined-ou</guid>
      <pubDate>Mon, 19 Aug 2024 20:51:58 GMT</pubDate>
    </item>
    <item>
      <title>这个 CNN 的最后三层使用的是全连接层还是卷积层？</title>
      <link>https://stackoverflow.com/questions/78888560/is-this-cnn-using-fully-connected-or-convolution-layers-for-the-final-three-laye</link>
      <description><![CDATA[在本文使用 CNN 准确检测唤醒词的开始和结束的第 2 部分中，图表显示了 CNN 架构。
最后三层标记为 FC，通常指完全连接层。但是，当它声明 FC1：3x3、FC2：1x1 和 FC3：1x1 时，似乎在每个完全连接层旁边都指定了内核大小。这可能意味着它们实际上是卷积层？
有人能给我解释一下最后三层吗？
以下是论文中指定的层：
Conv 1：9x5
72x60x96
Max Pooling：2x3
36x20x96

Conv2：7x3，步幅：3x1
10x18x192
Max Pooling：1z2
10x9x192

Conv3：4x3
7x7x192

Conv4：3x3
5x5x192

Conv5：3x3
3x3x192

FC1：3x3
1x1x500
FC2：1x1
1x1x500
FC3：1x1
1x1x500
Alexa
]]></description>
      <guid>https://stackoverflow.com/questions/78888560/is-this-cnn-using-fully-connected-or-convolution-layers-for-the-final-three-laye</guid>
      <pubDate>Mon, 19 Aug 2024 15:04:26 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 2.17.0 模型指标无法识别 - 显示 ['loss', 'compile_metrics'] 而不是预期指标</title>
      <link>https://stackoverflow.com/questions/78888263/tensorflow-2-17-0-model-metrics-not-recognized-showing-loss-compile-metri</link>
      <description><![CDATA[我正在尝试使用 TensorFlow 和 Keras 构建和训练一个简单的神经网络，但遇到了一个问题，即我在 model.compile 中指定的指标无法正确识别。
具体来说，在编译和训练模型后，当我打印指标和 metrics_names 时，我看到的是 [&#39;loss&#39;, &#39;compile_metrics&#39;]，而不是预期的指标（[&#39;loss&#39;, &#39;accuracy&#39;, &#39;precision&#39;, &#39;recall&#39;]）。
这是我使用的代码
import tensorflow as tf
import numpy as np
from tensorflow import keras

# 创建一个简单的模型
def create_model():
model = tf.keras.Sequential([
tf.keras.layers.Dense(16,激活=&#39;relu&#39;，输入形状=（3，）），
tf.keras.layers.Dense（1，激活=&#39;sigmoid&#39;）
])
返回模型

# 测试函数以演示`metrics`和`metrics_names`
def test_metrics_properties（）：
# 创建并编译模型
model = create_model（）
model.compile（optimizer=&#39;adam&#39;，
loss=keras.losses.BinaryCrossentropy（），
metrics=[
tf.keras.metrics.BinaryAccuracy（name=&#39;accuracy&#39;），
tf.keras.metrics.Precision（name=&#39;precision&#39;），
tf.keras.metrics.Recall（name=&#39;recall&#39;）
])

# 在训练之前检查`metrics`和`metrics_names`
print（“训练之前：”）
print(&quot;Metrics:&quot;, [m.name for m in model.metrics])
print(&quot;Metrics names:&quot;, model.metrics_names)

# 生成虚拟数据
x = np.random.random((100, 3))
y = np.random.randint(0, 2, size=(100, 1))

# 训练模型
model.fit(x, y, epochs=5, verbose=0)

# 训练后检查 `metrics` 和 `metrics_names`
print(&quot;\n训练后：&quot;)
print(&quot;Metrics:&quot;, [m.name for m in model.metrics])
print(&quot;Metrics names:&quot;, model.metrics_names)

if __name__ == &quot;__main__&quot;:
test_metrics_properties()

这是我看到的输出：
/path/to/your/env/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: 
UserWarning：不要将 `input_shape`/`input_dim` 参数传递给层。
使用顺序模型时，最好使用 `Input(shape)` 对象作为模型中的第一层。
super().__init__(activity_regularizer=activity_regularizer, **kwargs)

训练前：
指标：[&#39;loss&#39;, &#39;compile_metrics&#39;]
指标名称：[&#39;loss&#39;, &#39;compile_metrics&#39;]

训练后：
指标：[&#39;loss&#39;, &#39;compile_metrics&#39;]
指标名称：[&#39;loss&#39;, &#39;compile_metrics&#39;]


我已确保指标在编译方法中得到正确定义。
对于我构建的其他神经网络模型，它有同样的问题，我可以访问训练历史中每个单独指标的值，但使用 model.metrics_name 无法正确返回指标名称。
]]></description>
      <guid>https://stackoverflow.com/questions/78888263/tensorflow-2-17-0-model-metrics-not-recognized-showing-loss-compile-metri</guid>
      <pubDate>Mon, 19 Aug 2024 13:58:12 GMT</pubDate>
    </item>
    <item>
      <title>nvidia-smi 命令的 GPU 实用程序和 GPU 内存使用情况</title>
      <link>https://stackoverflow.com/questions/78887886/gpu-util-and-gpu-memory-usage-for-nvidia-smi-command</link>
      <description><![CDATA[我正在使用 8 个 GPU 运行微调实验，nvidia-smi 命令给出了以下输出
2024 年 8 月 19 日星期一 12:16:17 
+-----------------------------------------------------------------------------------------------------+ 
| NVIDIA-SMI 535.161.08 驱动程序版本：535.161.08 CUDA 版本：12.2 | |-----------------------------------------------------+----------------------+----------------------+ 
| GPU 名称 Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | 
|============================================+======================================| 
| 0 NVIDIA A100-SXM4-40GB 开启 | 00000000:10:1C.0 关闭 | 0 | | N/A 61C P0 118W / 400W | 6223MiB / 40960MiB | 4% 默认 | | | | 已禁用 |
+-----------------------------------------+-----------+-------------------------+ 
| 1 NVIDIA A100-SXM4-40GB 开启 | 00000000:10:1D.0 关闭 | 0 | | N/A 52C P0 88W / 400W | 9153MiB / 40960MiB | 14% 默认 | | | | 已禁用 |
+-----------------------------------------+----------------------+----------------------+ 
| 2 NVIDIA A100-SXM4-40GB 开启 | 00000000:20:1C.0 关闭 | 0 | | N/A 64C P0 112W / 400W | 9153MiB / 40960MiB | 14% 默认 | | | | 已禁用 |
+-----------------------------------------+-------------------------+-------------------------+ 
| 3 NVIDIA A100-SXM4-40GB 开启 | 00000000:20:1D.0 关闭 | 0 | | N/A 53C P0 93W / 400W | 9153MiB / 40960MiB | 21% 默认 | | | | 已禁用 |
+-----------------------------------------+----------------------+----------------------+ 
| 4 NVIDIA A100-SXM4-40GB 开启 | 00000000:90:1C.0 关闭 | 0 | | N/A 62C P0 103W / 400W | 9153MiB / 40960MiB | 21% 默认 | | | | 已禁用 |
+-----------------------------------------+-------------------------+-------------------------+ 
| 5 NVIDIA A100-SXM4-40GB 开启 | 00000000:90:1D.0 关闭 | 0 | | N/A 53C P0 116W / 400W | 9153MiB / 40960MiB | 0% 默认 | | | | 已禁用 |
+-----------------------------------------+----------------------+----------------------+ 
| 6 NVIDIA A100-SXM4-40GB 开启 | 00000000:A0:1C.0 关闭 | 0 | | N/A 65C P0 388W / 400W | 9191MiB / 40960MiB | 6% 默认 | | | | 已禁用 |
+-----------------------------------------+-------------------------+-------------------------+ 
| 7 NVIDIA A100-SXM4-40GB 开启 | 00000000:A0:1D.0 关闭 | 0 | | N/A 44C P0 83W / 400W | 423MiB / 40960MiB | 0% 默认 | | | | 已禁用 |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------------------+ 
| 进程：| | GPU GI CI PID 类型 进程名称 GPU 内存 | | ID ID 使用情况 | 
|====================================================================================================| 
| 0 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 1 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 2 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 3 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 4 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 5 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 6 N/A N/A 198 C /usr/bin/python3 0MiB | 
| 7 N/A N/A 198 C /usr/bin/python3 0MiB |
+-----------------------------------------------------------------------------------+

虽然 GPU 利用率超过 0%，但所有 8 个 GPU 的 GPU 内存使用率均为 0。我不明白为什么在 GPU 利用率上升时内存使用率会为 0？]]></description>
      <guid>https://stackoverflow.com/questions/78887886/gpu-util-and-gpu-memory-usage-for-nvidia-smi-command</guid>
      <pubDate>Mon, 19 Aug 2024 12:22:46 GMT</pubDate>
    </item>
    <item>
      <title>处理 CNN 数据集批次大小的非精确划分</title>
      <link>https://stackoverflow.com/questions/78887768/handling-non-exact-division-in-batch-size-for-datasets-in-cnn</link>
      <description><![CDATA[我正在开展一个 CNN 项目，需要一些帮助来调整步骤和批处理参数。以下是我的数据集摘要：
训练图像：5360
验证图像：1151
测试图像：1147

我已设置以下参数：
迭代次数：20
批次大小：16

要计算每个迭代的步数，我使用此代码：
nb_train_steps = np.ceil(train_data_gen.samples / batch_size).astype(int)
nb_validation_steps = np.ceil(valid_data_gen.samples / batch_size).astype(int)
nb_test_steps = np.ceil(test_data_gen.samples / batch_size).astype(int)

但是，我遇到了以下错误训练期间：
Epoch 2/20
2024-08-19 01:58:11.161693：I tensorflow/core/framework/local_rendezvous.cc:404] 本地会合正在中止，状态为：OUT_OF_RANGE：序列结束
[[{{node IteratorGetNext}}]]
C:\Users\anuja\AppData\Local\Programs\Python\Python312\Lib\contextlib.py:158：UserWarning：您的输入数据不足；中断训练。确保您的数据集或生成器至少可以生成 `steps_per_epoch * epochs` 批次。构建数据集时可能需要使用 `.repeat()` 函数。
self.gen.throw(value)
2024-08-19 01:58:11.190224: I tensorflow/core/framework/local_rendezvous.cc:404] 本地会合正在中止，状态为：OUT_OF_RANGE：序列结束
[[{{node IteratorGetNext}}]]
回溯（最近一次调用最后一次）：
文件“c:\Users\anuja\Desktop\Project\Scripts\model.py”，第 140 行，位于 &lt;module&gt;
custom_model.fit(
文件 &quot;C:\Users\anuja\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;，第 122 行，位于 error_handler 中
raise e.with_traceback(filtered_tb) from None
文件 &quot;C:\Users\anuja\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\backend\tensorflow\trainer.py&quot;，第 354 行，位于 fit 中
&quot;val_&quot; + name: val for name, val in val_logs.items()
^^^^^^^^^^^^^^
AttributeError: &#39;NoneType&#39; 对象没有属性 &#39;items&#39;

问题似乎是验证和测试数据集有不能被批次大小完全整除的图像数量。例如：

对于验证集：1151 / 16 = 71.9375。当四舍五入到 72 时，程序会期望目录中不存在的额外图像。
我预计测试集也会出现类似的问题。

我考虑过删除图像，使总数很容易被 16 整除（例如，将 1151 减少到 1136，这样 1136 / 16 = 71），但这感觉很浪费。我知道 .repeat() 函数可能会有所帮助，但我不确定它会如何影响模型，特别是在过度拟合方面。
这是我的数据生成代码：
datagen = ImageDataGenerator(rescale=1./255)

train_data_gen = datagen.flow_from_directory(
directory=train_path,
target_size=(224, 224),
batch_size=batch_size,
class_mode=&#39;categorical&#39;
)

valid_data_gen = datagen.flow_from_directory(
directory=val_path,
target_size=(224, 224),
batch_size=batch_size,
class_mode=&#39;categorical&#39;
)

test_data_gen = datagen.flow_from_directory(
directory=test_path,
target_size=(224, 224),
batch_size=batch_size,
class_mode=&#39;categorical&#39;,
shuffle=False
)

我没有使用任何数据增强，因为我的数据集相当平衡。
这里是 model.fit():
custom_model.fit(
train_data_gen,
steps_per_epoch=nb_train_steps,
validation_data=valid_data_gen,
validation_steps=nb_validation_steps,
epochs=nb_epochs,
callbacks=[es, chkpt]
)

我的问题：

我应该如何在不删除任何数据的情况下处理这种情况？
使用 .repeat() 会如何影响模型，尤其是过度拟合？
有没有更好的方法来处理这个问题？

我使用：
TensorFlow 版本：2.17.0
Keras 版本：3.4.1
]]></description>
      <guid>https://stackoverflow.com/questions/78887768/handling-non-exact-division-in-batch-size-for-datasets-in-cnn</guid>
      <pubDate>Mon, 19 Aug 2024 11:53:50 GMT</pubDate>
    </item>
    <item>
      <title>如何优化 Google Colab 中的视频帧捕获和处理以实现实时 YOLO 对象检测？</title>
      <link>https://stackoverflow.com/questions/78887653/how-can-i-optimize-video-frame-capture-and-processing-in-google-colab-for-real-t</link>
      <description><![CDATA[我已经训练了一个 YOLO V 10X 模型来检测工业或建筑工人的安全参数，包括安全帽、夹克、靴子、手套等个人防护装备。但是，我无法在 Colab 环境中进行实时检测。我必须将该模型应用于通过网络摄像头进行视频捕获的实时检测。如何解决这个问题。
我尝试了 colab 环境来解决同样的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78887653/how-can-i-optimize-video-frame-capture-and-processing-in-google-colab-for-real-t</guid>
      <pubDate>Mon, 19 Aug 2024 11:27:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用基于机器学习的插件优化 Android Studio 中的实时能耗分析</title>
      <link>https://stackoverflow.com/questions/78887516/how-can-i-optimize-real-time-energy-consumption-analysis-in-android-studio-using</link>
      <description><![CDATA[我目前正在为 Android Studio 开发一个插件，该插件使用机器学习模型检测 Android 代码中的能源气味。该插件旨在在开发人员编写或修改代码时实时分析代码，识别潜在的能源效率低下并提供优化建议。
挑战：该插件需要执行复杂的分析，包括使用预先训练的 ML 模型进行实时特征提取和推理，而不会明显减慢 IDE 的速度或中断开发人员的工作流程。
关键问题：

实时处理：如何高效地实时处理代码更改，确保分析跟上开发人员的输入，而不会导致 IDE 出现明显的滞后或卡顿？
以下是我目前如何挂接代码编辑器的事件监听器来触发分析的片段。我已经使用 Kotlin 实现了这一点。

val editorListener = object : DocumentListener {
override fun documentChanged(event: DocumentEvent) {
val newText = event.document.text
analyzeCode(newText)
}
}

private fun analyzeCode(code: String) {
executorService.submit {
val features = extractFeatures(code)
val result = model.predict(features)
updateUI(result)
}
}


每次文档更改时都会触发 analyzeCode 方法，但随着代码库的增长，这可能会导致性能问题。

资源管理：可以采用哪些策略来最大限度地减少插件在能量气味检测期间的 CPU 和内存使用量，尤其是在处理大型代码库时？

目前，我正在尝试使用ExecutorService 用于后台处理，但保持插件轻量级仍然具有挑战性：
private val executorService = Executors.newSingleThreadExecutor()

private fun extractFeatures(code: String): FeatureVector {
// 此处为特征提取逻辑
}

private fun updateUI(result: AnalysisResult) {
// 使用分析结果更新 IDE，而不阻塞主线程
}



模型优化：是否有特定的技术可以优化 ML 模型（例如量化、模型修剪）以减少推理期间的计算开销，同时保持准确性？

我正在使用 TensorFlow Lite 进行推理，但即使使用轻量级模型，仍然存在一些滞后：
val explainer = Interpreter(loadModelFile())

private fun predict(features: FeatureVector): AnalysisResult {
val output = Array(1) { FloatArray(1) }
interpretation.run(features.toArray(), output)
return AnalysisResult(output[0][0])
}



异步操作：如何有效地实现异步操作或将密集型任务卸载到后台线程，而不会影响实时反馈循环？

当前方法将任务卸载到后台线程，但在提供反馈方面仍然存在明显的延迟：
executorService.submit {
val result = model.predict(features)
SwingUtilities.invokeLater {
updateUI(result)
}
}


我通过挂接到 Android Studio 代码编辑器的文档更改事件，将实时分析功能集成到插件中。我使用 ExecutorService 将 ML 模型推理卸载到后台线程，并实施 TensorFlow Lite 进行轻量级推理。目标是使分析与开发人员的代码更改保持同步，而不会导致任何明显的性能问题。
我希望插件能够实时高效地分析代码，在开发人员输入时立即提供有关能量气味的反馈，而不会导致 IDE 中出现任何滞后或卡顿。
虽然分析按预期运行，但处理过程中存在明显的延迟，尤其是在较大的项目中。IDE 偶尔会变得响应迟缓，影响开发人员的工作流程。尽管使用了后台线程和轻量级模型，但性能仍然不是最佳的，尤其是在快速连续进行多次代码更改时。]]></description>
      <guid>https://stackoverflow.com/questions/78887516/how-can-i-optimize-real-time-energy-consumption-analysis-in-android-studio-using</guid>
      <pubDate>Mon, 19 Aug 2024 10:49:55 GMT</pubDate>
    </item>
    <item>
      <title>我如何将自己的自定义图像转换为 vggface2 模型以供自己使用？[关闭]</title>
      <link>https://stackoverflow.com/questions/78886847/how-can-i-my-own-custom-images-to-vggface2-model-for-my-own-use</link>
      <description><![CDATA[我从 GitHub vggface2.pt 下载了 vggface 模型，我可以向该模型添加我自己的自定义图像和标签吗？例如，我有 20 张自己的图像，我想添加到 vggface 供自己使用，我可以这样做吗？
我下载了 vggface 模型，我想将自己的图像添加到模型中并对其进行训练]]></description>
      <guid>https://stackoverflow.com/questions/78886847/how-can-i-my-own-custom-images-to-vggface2-model-for-my-own-use</guid>
      <pubDate>Mon, 19 Aug 2024 07:50:24 GMT</pubDate>
    </item>
    <item>
      <title>如何从 yolov8 检测到的图像中的对象中提取边界框</title>
      <link>https://stackoverflow.com/questions/78886670/how-to-extract-bounding-boxes-from-the-object-detected-in-an-image-for-yolov8</link>
      <description><![CDATA[from ultralytics import YOLO
import cv2

def detect_and_visualize_objects_yolov8(image_path, model, confidence_threshold=0.5):
image = cv2.imread(image_path)
results = model(image)

boxes = []
confidences = []

for result in results:
for box in result.boxes:
if box.conf &gt; confidence_threshold:
# 提取边界框坐标
--&gt; x1, y1, x2, y2 = box.xyxy[0].item(), box.xyxy[1].item(), box.xyxy[2].item(), box.xyxy[3].item()
x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
w = x2 - x1
h = y2 - y1
boxes.append([x1, y1, w, h])
confidences.append(float(box.conf))

# 绘制边界框
cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
label = f&#39;{box.cls} {box.conf:.2f}&#39;
cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# 计算检测到的物体的总面积
total_area = sum(w * h for _, _, w, h in boxes)

# 显示带有边界框的图像
cv2.imshow(&#39;Detected Objects&#39;, image)
cv2.waitKey(0)
cv2.destroyAllWindows()

return total_area

# 加载 YOLOv8 模型

model = YOLO(&#39;yolov8n.pt&#39;)

# 用法

image_path = &#39;data.jpg&#39;
total_area = detect_and_visualize_objects_yolov8(image_path, model)
print(f&#39;Total area of​​detectedobjects: {total_area}&#39;)

我在提取边界框坐标时遇到了问题。我的目标是编写一段代码，用于提取在推理 yolov8 模型时检测到的对象的面积总和。运行上述代码会导致此错误
 RuntimeError：无法将具有 4 个元素的 Tensor 转换为 Scalar

这是图片...
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78886670/how-to-extract-bounding-boxes-from-the-object-detected-in-an-image-for-yolov8</guid>
      <pubDate>Mon, 19 Aug 2024 07:04:18 GMT</pubDate>
    </item>
    <item>
      <title>回归决策树中用户定义的杂质</title>
      <link>https://stackoverflow.com/questions/78884108/user-defined-impurity-in-regression-decision-trees</link>
      <description><![CDATA[我正在从 R 迁移到 PySpark。我有一个创建回归树的过程，该树目前使用 R 的 rpart 算法构建。
在 PySpark 中配置时，我无法看到指定自定义
自定义杂质函数的选项。我有一个倾斜的数据集，我不想在公式中使用均值和方差/标准差作为节点杂质的标准，而是想使用更适合我的倾斜数据的指标。
如何在 PySpark 中定义自定义杂质函数？
我查看了决策树回归的文档，并且impurity 参数的文档仅提到对 variance 的支持

impurity = Param(parent=&#39;undefined&#39;, name=&#39;impurity&#39;, doc=&#39;用于信息增益计算的标准（不区分大小写）。支持的选项：方差&#39;)

是否有任何解决方法来定义自定义杂质函数？]]></description>
      <guid>https://stackoverflow.com/questions/78884108/user-defined-impurity-in-regression-decision-trees</guid>
      <pubDate>Sun, 18 Aug 2024 08:38:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv10 和 RTSP 流的车牌识别系统中的高 RAM 和存储使用率 [关闭]</title>
      <link>https://stackoverflow.com/questions/78883152/high-ram-and-storage-usage-in-license-plate-recognition-system-with-yolov10-and</link>
      <description><![CDATA[我正在开发一个车牌识别系统，使用来自安全摄像头的 RTSP 流来识别带有阿拉伯字母/数字的埃及车牌。我的设置包括：

YOLOv10 模型 1：检测和跟踪汽车。

YOLOv10 模型 2：检测车内的车牌。

YOLOv10 模型 3：对车牌上的字符和数字进行 OCR。


我正在使用 Python 推理库，将模型导出为 .engine 格式以实现 GPU 加速。
问题：

RAM 使用情况：系统每路摄像头信号消耗高达 8 GB 的 RAM。

存储使用情况：需要 15-20 GB 的存储空间来管理软件包。

性能：尽管使用了 GPU，但系统仍然资源密集。


我预计 GPU 加速会显著降低 RAM 和存储需求，但我没有看到预期的效率。类似产品 Plate Recognizer 的运行资源要少得多（0.5 GB RAM，无 GPU）（参考链接）。
这是车牌识别器使用的软件包列表，也许有人可以帮助我了解它们的工作原理，高效：
certifi==2024.6.2
cffi==1.16.0
charset-normalizer==3.3.2
configobj==5.0.8
cryptography==41.0.1
idna==3.7
Levenshtein==0.21.1
ntplib==0.4.0
numpy==1.24.4
opencv-python-headless==4.7.0.72
openvino==2023.3.0
openvino -telemetry==2024.1.0
persist-queue==0.8.1
psutil==5.9.5
pycparser==2.22
python-dateutil==2.8.2
python-Levenshtein==0.21.1
rapidfuzz==3.9.3
requests==2.32.3
rollbar==0.16.3
scipy==1.10.1
six==1.16.0
urllib3==2.2.1

什么我尝试过：

模型优化：导出到 .engine 进行 GPU 加速。

流管理：使用 Python 推理库来处理 RTSP 流。


问题：

如何减少此设置中的 RAM 和存储使用量？

是否有可能更有效的替代模型或方法？

有任何提高性能的一般技巧吗？

]]></description>
      <guid>https://stackoverflow.com/questions/78883152/high-ram-and-storage-usage-in-license-plate-recognition-system-with-yolov10-and</guid>
      <pubDate>Sat, 17 Aug 2024 19:53:15 GMT</pubDate>
    </item>
    <item>
      <title>确保数据标记、数据注释的质量</title>
      <link>https://stackoverflow.com/questions/78882012/ensure-quality-of-data-labeling-data-annotation</link>
      <description><![CDATA[我有一个包含两百万数据图像、视频和文本的数据集。这些都未标记。我想雇佣来自世界各地的工人来标记它们。这是一个庞大的人数。我如何确保我的员工标记的数据的质量？我担心他们只是为了赚钱而大量浪费工作。
P/S：我不能使用 Scale&#39;AI 等其他公司为我标记。
对于简单的分类。我可以使用像 CAPCHA 这样的方法。它效果很好，但对于其他情况，如绘制边界框 = 或分割，我不知道如何检查标签数据的质量。]]></description>
      <guid>https://stackoverflow.com/questions/78882012/ensure-quality-of-data-labeling-data-annotation</guid>
      <pubDate>Sat, 17 Aug 2024 10:47:26 GMT</pubDate>
    </item>
    <item>
      <title>Azure ML Prompt 流程创建和克隆失败</title>
      <link>https://stackoverflow.com/questions/78880051/azure-ml-prompt-flow-creation-and-cloning-failed</link>
      <description><![CDATA[创建或克隆提示流都会导致以下错误 - “此请求无权执行此操作”。但是，我同时拥有此 AML 的“贡献者”和“数据科学家”角色。此外，由于创建按钮呈灰色，因此也无法创建运行时。我不确定这两个问题是否相关。
有人遇到过同样的问题吗？
AML 屏幕截图 1 (https://i.sstatic.net/zOehilI5.png)
AML 屏幕截图 2 (https://i.sstatic.net/fzShpSh6.png)
我找不到有关此问题的任何有用文档。我看过的所有教程也没有同样的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78880051/azure-ml-prompt-flow-creation-and-cloning-failed</guid>
      <pubDate>Fri, 16 Aug 2024 16:28:42 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 TensorFlow 中张量包含 Nan 或 inf 值错误？[关闭]</title>
      <link>https://stackoverflow.com/questions/78879458/how-to-resolve-tensors-contains-nan-or-inf-values-error-in-tensorflow</link>
      <description><![CDATA[def ActivityNetV3(n_sensors, window_size):
#inputs = 输入(shape=(n_sensors, window_size, 1))
输入 = 输入(shape=(n_sensors, window_size, 1))
print(&quot;模型内部&quot;)
print(inputs.shape)
x = Conv2D(
filters=1, kernel_size=(1, 1), weights=[np.array([[[[1.0]]]])], padding=&#39;same&#39;,
激活=None, trainable=False, use_bias=False,
)(输入)
x = BatchNormalization(center=False, scale=True,epsilon=1e-5)(x)

x = ZeroPadding2D(padding=((0,0),(1,1)))(x) # (输入)
x = Conv2D(
32,
kernel_size=(n_sensors, tconfig.KERNEL_SIZE),
strides=1,
padding=&quot;valid&quot;,
use_bias=False,
name=&quot;Conv00&quot;,
)(x)
x = BatchNormalization(
name=&quot;Conv00/BatchNorm&quot;
)(x)
x = Activation(tconfig.ACTIVATION)(x)
x = PrintLayer(&quot;初始 Conv2D 和 BatchNorm 之后：&quot;)(x)
x = check_numerics(x, &quot;初始 Conv2D 和 BatchNorm 之后&quot;)

x = stack_fn(x, None)
last_conv_ch = _depth(K.int_shape(x)[tconfig.CHANNEL_AXIS] * 3) # 3
x = Conv2D(
last_conv_ch,
kernel_size=1,
padding=&quot;same&quot;,
use_bias=False,
name=&quot;Conv_1&quot;,
)(x)
x = BatchNormalization(
name=&quot;Conv_1/BatchNorm&quot;
)(x)
x = Activation(tconfig.ACTIVATION)(x)
x = PrintLayer(&quot;在 stack_fn 和附加 Conv2D 之后：&quot;)(x)
x = check_numerics(x, &quot;在初始 Conv2D 和 BatchNorm 之后&quot;)

x = GlobalAvgPool2D(keepdims=True)(x)
x = Conv2D(
tconfig.LAST_POINT_CH,
kernel_size=1,
padding=&quot;same&quot;,
use_bias=True,
name=&quot;Conv_2&quot;,
)(x)
x = Activation(tconfig.ACTIVATION)(x)
x = Dropout(tconfig.DROPOUT_RATE)(x) 

x = Conv2D(
5, kernel_size=1, padding=&quot;same&quot;, name=&quot;Logits&quot;
)(x)
x = Flatten()(x)
输出 = x

模型 = 模型(输入=输入，输出=输出)
返回模型

def get_siamese_model(n_sensors, window_size):
base_network = ActivityNetV3(n_sensors, window_size)
输入_a = 输入(形状=(n_sensors, window_size,1))
输入_b = 输入(形状=(n_sensors, window_size,1))

编码_a = base_network(输入_a)
编码_b = base_network(输入_b)

L1_layer = Lambda(lambda 张量：K.abs(张量[0] - 张量[1]))
L1_distance = L1_layer([编码_a, 编码_b])
预测 = 密集(1, 激活=&#39;sigmoid&#39;)(L1_distance)
siamese_net =模型（输入=[输入_a，输入_b]，输出=预测）

返回 siamese_net

def get_generator（X_train，X_val，y_train，y_val，s）：
当 True 时：
对，目标=get_batch（X_train，X_val，y_train，y_val，s）
产量（对，目标）

def get_batch（X_train，X_val，y_train，y_val，s）：
rng=np.random.default_rng（）
如果 s == &#39;train&#39;：
X=X_train.copy（）
类别 = y_train.shape[0]
elif s == &#39;val&#39;：
X=X_val.copy（）
类别 = y_val.shape[0]

人口 = X.shape[0]
n_sensors，时间步长，一 = X.shape[1]，X.shape[2]， X.shape[3]
#print(population, n_sensors, timesteps,one)
categories = rng.choice(population,size=(tconfig.BATCH_SIZE,),replace=False)
pair=[np.zeros((tconfig.BATCH_SIZE,n_sensors,timesteps,1)) for _ in range(2)]
goals = np.zeros((tconfig.BATCH_SIZE,))
goals[tconfig.BATCH_SIZE//2:]=1
for i in range(tconfig.BATCH_SIZE):
category=categories[i]
# print(category)
idx_1=rng.integers(0,population-1)
# print(idx_1)
# print(X[category,idx_1])
#pairs[0][i,:,:,0]=X[idx_1].reshape(n_sensors,timesteps)
pairs[0][i,:,:,0]=X[idx_1,:,:,0]
idx_2=rng.integers(0,population-1)
if i&gt;=tconfig.BATCH_SIZE//2:
category_2=category
else:
category_2 = rng.choice(np.delete(np.arange(population),category))
pairs[1][i,:,:,0]=X[category_2,:,:,0]
#pairs[1][i,:,:,0]=X[category_2]#.reshape(n_sensors,timesteps)
return pair,targets

错误：
不是数字 (NaN) 或在梯度中检测到无穷大 (Inf) 值。
b&#39;初始 Conv2D 和 BatchNorm 之后&#39;：张量具有 NaN 值

我尝试检查 inf 值和 Nan 值，也尝试更改学习率和其他模型参数相关内容，但对我都不起作用。
此外，我已按顺序调试代码，通过打印张量的中间输出来检查潜在错误
我使用的形状为 (None,3,100,1)]]></description>
      <guid>https://stackoverflow.com/questions/78879458/how-to-resolve-tensors-contains-nan-or-inf-values-error-in-tensorflow</guid>
      <pubDate>Fri, 16 Aug 2024 13:54:12 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在 GPU 上加载一次 YOLO 模型并将其提供给多个 Python 进程？</title>
      <link>https://stackoverflow.com/questions/78603046/is-it-possible-to-load-a-yolo-model-on-gpu-once-and-give-it-to-multiple-python-p</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78603046/is-it-possible-to-load-a-yolo-model-on-gpu-once-and-give-it-to-multiple-python-p</guid>
      <pubDate>Mon, 10 Jun 2024 14:56:16 GMT</pubDate>
    </item>
    </channel>
</rss>