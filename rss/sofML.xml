<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Tue, 01 Apr 2025 03:42:09 GMT</lastBuildDate>
    <item>
      <title>需要透彻了解因果ML研究论文的背景吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79547011/required-background-for-thorough-understanding-of-causal-ml-research-papers</link>
      <description><![CDATA[我有兴趣在因果推理和机器学习的交集中进行研究，尤其是在因果发现和因果代表学习方面。通过到目前为止，通过我的探索，我发现对以下书籍进行研究至关重要，然后再阅读该领域的研究。

通过墨菲和主教的书（可以选择任何人）的强大ML基金会
理解机器学习（第1部分）的理论ML背景，通常在提出休闲学习理论之前引用。
 Judea Pearl的因果关系，以深入了解因果推论，然后是Bernhard Scholkopf因果发现的因果推断的要素。

我的问题是：
这些书足以准备该主题的研究吗？如果没有，您将添加到此列表中？
成功完成这些书籍的一些基本先决条件是什么？例如贝叶斯因果关系的可能性？还是其他？]]></description>
      <guid>https://stackoverflow.com/questions/79547011/required-background-for-thorough-understanding-of-causal-ml-research-papers</guid>
      <pubDate>Mon, 31 Mar 2025 18:37:31 GMT</pubDate>
    </item>
    <item>
      <title>时间序列预测模型，带有XGBoost和Dask大数据集崩溃</title>
      <link>https://stackoverflow.com/questions/79547006/time-series-forecasting-model-with-xgboost-and-dask-large-datasets-crashing</link>
      <description><![CDATA[我正在Python建立一个时间序列预测模型，以预测公用事业公司不同客户类型的每小时KWH负载。该数据集包含约8100万行，在2  -  4年内为2300个客户提供每小时负载数据。客户类型由二进制列表示：EV，HP，太阳能和TOU。数据集具有以下变量：
   -  read_date：datetime64 [us]
   - 仪表：字符串
  -KWH：float64
   - 城市：弦
   - 温度：float64
  -EV：INT64
   - 太阳能：INT64
  -HP：INT64
  -TOU：INT64
   - 小时：INT32
   - 天：INT32
   - 月份：INT32
   - 年：INT64
  -day_of_week：int32
   - 季节：弦
  -customer_type：字符串
  -HOUR_SIN：FLOAT64
  -HOUR_COS：FLOAT64
  -month_sin：float64
  -month_cos：float64
  -Day_of_week_sin：float64
  -Day_of_week_cos：float64
  -Day_sin：float64
  -Day_cos：float64
   -  is_holiday：int64
  -City_Reading：INT64
  -City_lynnfield：INT64
  -City_NorthReading：INT64
  -City_wilmington：INT64
   - 季_WINTER：INT64
  -Season_spring：INT64
   -  sepen_summer：int64
   -  sepen_fall：INT64
 
After cleaning the data, I dropped the following features from both the training and test datasets: meter, customer_type, season, read_date, city, day, month, hour, day_of_week.我的目标变量是小时kWh负载。
我试图使用dask构建XGBoost模型以进行分发，但它一直在以下错误崩溃：
  essertionError：错误
2025-03-31 14：12：26,995-分布式。
 
我正在使用128GB RAM和Intel I7-14700K 3.40 GHz处理器的本地计算机工作。我正在寻找有关如何处理此大型数据集预测时间序列的指导，以及如何在使用DASK进行分发时避免崩溃。这是我的示例代码：
 ＃导入必要的库
导入numpy作为NP
导入dask.dataframe作为DD
导入dask.array作为da
导入XGBoost为XGB
来自dask.distribed Import客户端
来自dask.diarostics导入进步键 
来自sklearn.metrics incort cone_absolute_error，mean_squared_error，r2_score
进口警告
导入matplotlib.pyplot作为PLT
从TQDM导入TQDM

＃使用dask加载数据（大型镶木文件有效）
some_feats_dd = dd.read_parquet（&#39;pre_ml_some_features.parquet＆quort＆quot;）

＃重命名dataFrame
df_processed = some_feats_dd

＃基于读取_DATE进行训练和测试的数据
df_train = df_processed [df_processed [＆quot; 2025]＃在2025年之前保持行
df_test = df_processed [df_processed [&#39;Year; eart; quot; quot; quot; quort; quot; quort; quot&#39;== 2025]＃从2025年开始保持行

＃排除列并准备训练的功能和目标变量
dublude_cols = [kwh＆quot&#39;米，&#39;customer_type&#39;&#39;&#39; 
                ＆quot&#39;&#39;

＃准备培训功能（x）和目标变量（y）
x_train = df_train.drop（columns = ubl_cols）
y_train = df_train [＆quot; kwh＆quot;]

＃计算总长度并确保精确3个块
train_size = len（y_train.compute（））
test_size = len（df_test）＃无需计算，dask可以推断

＃用强制3个块将y_train和y_test转换为dask阵列
y_train = da.from_array（y_train.compute（），chunks =（train_size // 3，））
y_test = da.from_array（df_test [＆quot; kwh;]。compute（），chunks =（test_size // test_size // 2，））

＃确保与x_train和x_test的分区匹配
x_train = x_train.repartition（npartitions = 3）
x_test = x_test.repartition（npartitions = 3）

＃启动DASK客户端以进行并行处理
客户端=客户端（）

＃打印D​​ask仪表板URL
打印（f＆quot“ dask仪表板

＃从xgboost.dask使用daskdmatrix
dask_train_data = xgb.dask.daskdmatrix（客户端，x_train，y_train）

＃设置XGBoost的参数
params = {
    “目标”：“ reg：squaredErr”，＃回归任务
    &#39;eval_metric&#39;：&#39;rmse&#39;，
    &#39;tree_method&#39;：“历史”，＃使用基于直方图的方法来更快训练
    &#39;冗长&#39;：1，＃启用基本记录
}

＃初始化dask-xgboost模型
dask_gbr = xgb.dask.daskxgbregressor（**参数）

＃使用DASK训练模型（这将自动并行化）
使用进度栏（）：＃显示训练期间的进度
    dask_gbr.fit（dask_train_data）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79547006/time-series-forecasting-model-with-xgboost-and-dask-large-datasets-crashing</guid>
      <pubDate>Mon, 31 Mar 2025 18:33:41 GMT</pubDate>
    </item>
    <item>
      <title>如何提高机器学习模型的准确性[关闭]</title>
      <link>https://stackoverflow.com/questions/79546591/how-to-increase-the-accuracy-of-a-machine-learning-model</link>
      <description><![CDATA[我正在尝试训练一个模型，以通过我从Kaggle发现的情感文本数据库进行文本进行情感检测。目前，我正在使用验证的模型：“ distilbert-base-base uncord” 。但是，准确性太低为0.5。我正在尝试学习一种增加此方法以进行更好估计的方法。
任何人可以帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/79546591/how-to-increase-the-accuracy-of-a-machine-learning-model</guid>
      <pubDate>Mon, 31 Mar 2025 15:08:39 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：数据加载器对象不可订阅</title>
      <link>https://stackoverflow.com/questions/79546578/typeerror-dataloader-object-is-not-subscriptable</link>
      <description><![CDATA[我正在创建一个AI模型来生成人群的密度图。将数据集分为两个，一个用于培训，一个用于验证，我创建了两个数据集，然后尝试使用 torch.utils.data.dataloader（test_set，batch_size = batch_size = batch_size，shuffle = false））。之后，要测试数据，我迭代并使用下一个函数获取数据集的下一个元素，然后获得TypeError。
我正在使用Kaggle的数据集从Kaggle使用：这是完整的代码：
  batch_size = 8 
设备=&#39;cuda：0&#39;如果torch.cuda.is_available（）else&#39;cpu&#39;

train_root_dir =＆quot; data/part_a/train_data/＆quot
init_training_set = dataloader（train_root_dir，shuffle = true）

＃将培训集的一部分分为验证集
train_size = int（0.9 * len（init_training_set））
val_size = len（init_training_set）-train_size

train_indices = list（range（train_size））
val_indices = list（range（train_size，len（init_training_set））））））
train_dataset = torch.utils.data.dataset.subset（init_training_set，train_indices）
val_dataset = torch.utils.data.dataset.subset（init_training_set，val_indices）

train_loader = torch.utils.data.dataloader（train_dataset，batch_size = batch_size，shuffle = true）
val_loader = torch.utils.data.dataloader（val_dataset，batch_size = batch_size，shuffle = false）

test_root_dir =＆quot; data/part_a/test_data/＆quot
test_set = dataloader（test_root_dir，shuffle = false）
test_loader = torch.utils.data.dataloader（test_set，batch_size = batch_size，shuffle = false）

dataiter = iter（train_loader）
ex_images，ex_dmaps，ex_n_people = next（dataiter）


＃显示图像和密度图
plot_corresponding_pairs（ex_images，ex_dmaps）
 
具体错误是：
  trackback（最近的最新通话）：
 第61行，in＆lt;模块＆gt;
    对于ex_images，ex_dmaps，ex_n_people in train_loader中
typeError：“数据加载程序”对象不可订阅
 ]]></description>
      <guid>https://stackoverflow.com/questions/79546578/typeerror-dataloader-object-is-not-subscriptable</guid>
      <pubDate>Mon, 31 Mar 2025 15:02:15 GMT</pubDate>
    </item>
    <item>
      <title>与Skywise -Slate仪表板相关的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79545514/issue-related-to-skywise-slate-dashboard</link>
      <description><![CDATA[我正在努力修改空客Skywise中现有的仪表板。我的任务涉及：
 提取数据： 
用户单击“获取”时按钮，系统应从用户指定的列中获取数据并将其存储以供以后使用。
 手动描述输入： 
在“手动描述”中单元格，用户将手动粘贴来自空中客车数据库的文本，对应于特定的FUID ID。
 生成TDD评估： 
用户点击“生成评估”时，系统应：

使用先前获取的数据和手册说明
参考。
利用ML/AI模型来自动生成TDD评估
针对不同错误类型的预定义参考脚本。

我正在寻找有关如何实施此功能的指导。具体来说，我需要以下帮助：

提取和存储用户指定的列数据。
处理手动说明并将其链接到Fuid ID。
使用使用ML/AI基于ML/AI的自动生成TDD评估
参考脚本。
]]></description>
      <guid>https://stackoverflow.com/questions/79545514/issue-related-to-skywise-slate-dashboard</guid>
      <pubDate>Mon, 31 Mar 2025 04:58:02 GMT</pubDate>
    </item>
    <item>
      <title>如何使用R中的自定义内核进行SVM？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79545276/how-to-do-svm-with-a-custom-kernel-in-r</link>
      <description><![CDATA[ 内核方程  
我试图使用上面的自定义内核进行SVM，但我不知道该如何编码，有人知道我应该在哪里看吗？哪些软件包允许包括自定义内核？该内核应允许更高阶的相互作用，而不仅仅是常规多项式内核。]]></description>
      <guid>https://stackoverflow.com/questions/79545276/how-to-do-svm-with-a-custom-kernel-in-r</guid>
      <pubDate>Sun, 30 Mar 2025 23:33:57 GMT</pubDate>
    </item>
    <item>
      <title>当给出额外的较大V12权重（Yolov12x）时，Yolo为什么要下载Nano V11型号（Yolov11n）？</title>
      <link>https://stackoverflow.com/questions/79545081/why-does-yolo-download-a-nano-v11-model-yolov11n-when-given-the-extra-larger-v</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79545081/why-does-yolo-download-a-nano-v11-model-yolov11n-when-given-the-extra-larger-v</guid>
      <pubDate>Sun, 30 Mar 2025 19:43:01 GMT</pubDate>
    </item>
    <item>
      <title>将TensorFlow模型伪像到SageMaker</title>
      <link>https://stackoverflow.com/questions/79363695/deploying-a-tensorflow-model-artifact-to-sagemaker</link>
      <description><![CDATA[我正在尝试将张量流模型部署到萨吉式端点。我在generic_graph.pb上有模型伪像，并在labels.txt。
我首先创建一个带有以下内容的焦油文件：
 ＃＃MODEL目录结构 
＃＃Model.tar.gz
＃└ - ＆lt; model_name＆gt;
＃└ - ＆lt; version_number＆gt;
＃├├─pa
＃└ - 变量
＃├├─标签.txt
 
我将文件上传到S3中的存储桶中。然后，我尝试使用以下代码部署模型：
  sagemaker_session = sagemaker.session（）
角色=&#39;my-lole&#39;

model = tensorflowmodel（model_data =&#39;s3：//my-bucket/model.tar.gz&#39;，
                    角色=角色，
                    framework_version =&#39;2.3.0&#39;）


preditionor = model.deploy（prinity_instance_count = 1，instance_type =&#39;ml.m5.large&#39;）
 
我在我的CloudWatch日志中不断遇到以下错误：
  valueerror：找不到SavedModel捆绑包！
 
不确定还有什么尝试。]]></description>
      <guid>https://stackoverflow.com/questions/79363695/deploying-a-tensorflow-model-artifact-to-sagemaker</guid>
      <pubDate>Fri, 17 Jan 2025 05:19:16 GMT</pubDate>
    </item>
    <item>
      <title>Importerror：使用“ BitsandBytes” 8位量化需要加速</title>
      <link>https://stackoverflow.com/questions/78595127/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate</link>
      <description><![CDATA[从HuggingFace下载模型时，我遇到了错误。它在Google Colab上工作，但不在我的Windows机器上工作。我正在使用python 3.10.0。
错误代码如下：
  e：\ internships \ consciusai \ .venv \ lib \ lib \ site-packages \ huggingface_hub \ file _download.py：1132：1132：futureWarning：futureWarning：`remume_download&#39;s depected and depected and depected and将在版本1.0.0.0.0.0.0.0中删除。下载总是在可能的情况下恢复。如果要强制新下载，请使用`force_download = true`。
  警告。
未使用的Kwargs：[&#39;_load_in_4bit&#39;，&#39;_load_in_8bit&#39;，&#39;QUAT_METHOD&#39;]。这些Kwargs在＆lt; class&#39;Transformers.utils.quantization_config.bitsandbytesconfig&#39;＆gt;中不使用这些夸大。
e：\ internships \ consciusai \ .venv \ lib \ lib \ lib \ site-packages \ transformers \ ventrizers \ venterizers \ auto.py：159：userWarning：您通过`jentization_config`或等效参数to`from_pretrenained&#39;&#39;&#39;但您正在加载的模型已经加载了量化量化量化量化量化。将使用来自模型的`Quantization_Config`。
  WARNINGS.WARN（WARNNING_MSG）
Trackback（最近的最新电话）：
  file＆quort e：\ induthships \ consciusai \ emaim_2.py”，第77行，in＆lt; module＆gt;
    主要的（）
  file＆quot e：\ internships \ consciusai \ emaim_2.py; ,，第71行，主要
    摘要= summarize_email（内容）
  file＆quot e：\ internships \ consciusai \ emaim_2.py”，第22行，在summarize_email中
    管道=变形金刚。Pipeline（
  file＆quot&#39;e：\ internships \ consciusai \ .venv \ lib \ lib \ site-packages \ transformers \ pipelines \ pipelines \ __ init __ init __ in Int __.py＆quort&#39;＆quort&#39;＆quort&#39;＆quort&#39;＆quote＆quote＆quote＆quort in 906
    框架，型号= peash_framework_load_model（
  file＆quot; e：\实习\ consciusai \ .venv \ lib \ lib \ site-packages \ Transformers \ pipelines \ pipelines \ base.py＆quid＆quot＆quot＆quot＆quid＆quort＆quot＆quot＆quot＆quot＆quid＆quot＆quid＆quot＆quot＆quot＆quot＆quid＆quid＆quot＆quot＆quid＆quot＆quid＆quot＆quid＆quort of 283
    model = model_class.from_pretrataining（模型，** kwargs）
  file＆quot; e：\ internships \ consciusai \ .venv \ lib \ lib \ site-packages \ transformers \ transformers \ models \ auto \ auto \ auto_factory.py;
    返回model_class.from_pretaining（
  file＆quort;
    hf_quantizer.validate_environment（
  file＆quot;
    提高侵居者（
Importerror：使用`bitsandBytes` 8位量化都需要加速：`pip安装加速&#39;和最新版本的bitsandbytes：`pip install -i https：//pypi.org/simple/ bitsandble/ bitsandbytes&#39;
 
这是我使用的代码：
  def summarize_email（content）：
    model_id =＆quot&#39;unsploth/llama-3-8b-instruct-bnb-4bit;

    管道=变形金刚。Pipeline（
        ＆quot“文字生成”
        模型= model_id，
        model_kwargs = {
            ＆quot“ torch_dtype”：torch.float16，
            ＆quot&#39;ventalization_config;：{&#39;load_in_4bit＆quort;：true}，
            ＆quot“ low_cpu_mem_usage＆quot”：true，
        }，，
    ）

    消息= [
        {&#39;&#39;：＆quot; quot“ system; quot” content; quot; quot; quot;
        {&#39;：＆quot“ user quot” contents“ contents”：’汇总给我的电子邮件+内容}，
    这是给出的

    提示= pipeline.tokenizer.apply_chat_template（
            消息，
            tokenize = false，
            add_generation_prompt = true
    ）

    终结者= [
        pipeline.tokenizer.eos_token_id，
        pipeline.tokenizer.convert_tokens_to_ids（“”）
    这是给出的

    输出=管道（
        迅速的，
        max_new_tokens = 256，
        eos_token_id =终止者，
        do_sample = true，
        温度= 0.6，
        top_p = 0.9，
    ）
 
我正在尝试使用“不舒服/Llama-3-8B-Instruct-Bnb-4bit”总结文本。来自拥抱面。
它确实在Google Colab和Kaggle上总结了文本，但没有在本地机器上进行。]]></description>
      <guid>https://stackoverflow.com/questions/78595127/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate</guid>
      <pubDate>Sat, 08 Jun 2024 08:37:18 GMT</pubDate>
    </item>
    <item>
      <title>无法下载MMCV 1.3.0并构建车轮</title>
      <link>https://stackoverflow.com/questions/77479005/not-able-to-download-mmcv-1-3-0-and-build-wheels</link>
      <description><![CDATA[当我尝试安装 mmcv-full == 1.3.0 时，它无法下载并构建轮子（我已经更新了车轮）
  错误无法为MMCV-Full构建车轮，这是安装pyproject.toml项目所需的
 但是当我尝试使用时
 MIM安装mmcv-full  
错误消息：
  RROR：MMCV-Full的建筑轮失败
  运行设置。
无法构建mmcv-full
错误：无法为MMCV-Full构建车轮，这是安装pyproject.toml的项目所需的
 
可以下载 mmcv-full 的最新版本，但是由于我试图克隆的存储库需要使用 MMCV版本1.3.0 。
我正在使用 Windows 11 ，想知道我应该如何下载版本。]]></description>
      <guid>https://stackoverflow.com/questions/77479005/not-able-to-download-mmcv-1-3-0-and-build-wheels</guid>
      <pubDate>Tue, 14 Nov 2023 08:00:38 GMT</pubDate>
    </item>
    <item>
      <title>自定义SVM多项式内核</title>
      <link>https://stackoverflow.com/questions/69573659/custom-svm-polynomial-kernel</link>
      <description><![CDATA[我被要求作为分配，以开发SVM的自定义多项式（学位= 3,4,5）内核，并将其准确性与Sklearn的内置多核进行比较（应该几乎相同）。
我试图遵循多项式内核定义，但我的结果似乎并不相似，这是我的代码：
  def poly_kernel_fn（x，y）：
＃实现多项式内核
＃ -  args：2个形状的numpy阵列[n_samples，n_features]
＃ - 返回：计算的形状内核矩阵[n_samples，n_samples]

k = np.zeros（（x. shape [0]，y.shape [0]））
k =（x.dot（y.t） + 1）** 4
返回k

clfpoly = svm.svc（kernel =&#39;poly&#39;，度= 4）
clfpoly.fit（x_train，y_train）
zpoly = clfpoly.predict（x_test）
打印（“具有内置3D多项式内核的准确性为：”，Quecy_score（Y__Test，Zpoly）*100，“％＆quot”）

clf = svm.svc（kernel = poly_kernel_fn）
clf.fit（x_train，y_train）
z = clf.predict（x_test）
打印（“自定义RBF内核的准确性是：”，“ ecucre_score（y_test，z）*100，; quot”％＆quot;
 
精度结果如下：

内置4D多项式内核的准确性为：56.999999999999999％
内核的准确性为：59.0％

如果我将多项式等级更改为3或5，它会更改更多，因此我不知道我做错了什么还是只是不可能匹配内置的精度。]]></description>
      <guid>https://stackoverflow.com/questions/69573659/custom-svm-polynomial-kernel</guid>
      <pubDate>Thu, 14 Oct 2021 15:46:08 GMT</pubDate>
    </item>
    <item>
      <title>如何计算幼稚贝叶斯分类器中的证据？</title>
      <link>https://stackoverflow.com/questions/60454210/how-to-calculate-evidence-in-naive-bayes-classifier</link>
      <description><![CDATA[我在Python写了一个简单的多项式幼稚贝叶斯分类器。该代码预测
整个过程基于我从 nofollow noreferrer“&gt; wikipedia文章关于幼稚的贝耶斯：
  
因此，第一步是从文章中提取特征。为此，我使用Sklearn的Count Vectorizer。它计算词汇中所有单词的出现数量：

 来自Sklearn.feature_extraction.text Import CountVectorizer
vectorizer = countvectorizer（stop_words =&#39;英语&#39;，min_df = 5，ngram_range =（1,1））
功能= vectorizer.fit_transform（data.news）.toArray（）
打印（功能。形状）
（2225，9138）
 
结果，我在数据集中获得了每个文章的9138个功能。

下一步是为每个标签计算P（x  i  | c  k ）。它由多项式分布公式给出：

   i计算p  ki 如下：
  def count_word_probability（功能）：
  v_size =功能。形状[1]
  alpha = 1
  total_counts_for_each_word = np.sum（功能，轴= 0）
  total_count_of_words = np.sum（total_counts_for_each_word）
  probs =（alpha + total_counts_for_each_word） /（（v_size * alpha） + total_count_of_words）
  返回概率
 
基本上，此函数的作用是计算所有文章中带有特定标签（例如业务）的每个单词的总频率，并除以所有文章中带有该标签的单词总数。它还适用拉普拉斯平滑（alpha = 1）以说明0频率的单词。

接下来，我计算P（C  k ），这是标签的先验概率。我只是将一个类别中的文章总数除以所有类别的文章总数：

  labels_probs = [len（data.index [data [&#39;category_id&#39;] == i]） / len（data）for Range（5）]
 

这些是缩放术语和恒定项的函数（p（x）相应：

 将数学导入数学
来自Scipy.特定进口阶乘

def scaing_term（doc）：
  term = Math.factorial（np.sum（doc）） / np.prod（fortorial（doc））
  返回期限 
 
缩放函数上面的缩放函数将文章中的单词总和划分为段落的产物。
  def nb_constant（文章，labels_probs，word_probs）：
  s_term = scaing_term（文章）
  证据= [np.log（s_term） + np.sum（文章 * np.log（word_probs [i]））） + np.log（labels_probs [i]）for Range（len（word_probs））
  证据= np.sum（证据）
  返回证据
 
 o，上面的最后一个函数计算分母（先验概率p（x）。它总结了所有文章类别的UPS p（x | c  k ）：
  
和最终的天真贝叶斯分类器看起来像这样：

  def naive_bayes（文章，label_probs，words_probs）：
  class_probs = []
  s_term = scaing_term（文章）
  constant_term = nb_constant（文章，label_probs，words_probs）
  对于范围（Len（Label_probs））的Cl）：
    class_prob =（np.log（s_term） + np.sum（文章 * np.log（words_probs [cl]））） + np.log（label_probs [cl]）） / constant_term
    class_probs.append（class_prob）
  class_probs = np.exp（np.array（class_probs））
  返回class_probs
 
没有恒定术语的情况下，此功能为我馈送的任何自定义文本都会输出正确的标签。但是所有类别的分数都是均匀的，接近零。当我除以恒定术语以获取总和最高为零的实际概率值时，我会得到所有类别的奇怪结果，例如所有类别的概率。我绝对缺少理论上的东西，因为我对概率理论和数学不了解。]]></description>
      <guid>https://stackoverflow.com/questions/60454210/how-to-calculate-evidence-in-naive-bayes-classifier</guid>
      <pubDate>Fri, 28 Feb 2020 14:56:20 GMT</pubDate>
    </item>
    <item>
      <title>如何计算Bernoulli幼稚贝叶斯的联合日志样本</title>
      <link>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</link>
      <description><![CDATA[For a classification problem using BernoulliNB , how to calculate the joint log-likelihood?关节可能性应通过下方公式计算，其中y（d）是实际输出的数组（不是预测值），而x（d）是特征的数据集。
  我阅读 href =“ https://github.com/scikit-learn/scikit-learn/blob/bac89c2/sklearn/naive_bayes.py#l839“ rel =“ nofollow noreferrer”]]></description>
      <guid>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</guid>
      <pubDate>Wed, 17 Oct 2018 18:08:50 GMT</pubDate>
    </item>
    <item>
      <title>幼稚的贝叶斯分类器从头开始实现</title>
      <link>https://stackoverflow.com/questions/19349567/naive-bayes-classifier-implementation-from-scratch</link>
      <description><![CDATA[我正在尝试自己实施我的第一个天真的贝叶斯分类器，以更好地理解。因此，我的数据集来自 http://archive.ics.uci.uci.uci.uci.edu/ml/datasets/datasets/datasets/Adult  Adadult  American Census Data，Spersus sass seals seals＆lt; &#39;＆gt; 50k&#39;）。
这是我的python代码：
 导入系统
导入CSV

word_stats = {}＃{&#39;word&#39;：{&#39;class1&#39;：cnt，&#39;class2&#39;：cnt&#39;}}}
word_cnt = 0

targets_stats = {}＃{&#39;class1&#39;：3234，&#39;class2&#39;：884}每个类中有多少个单词
class_stats = {}＃{&#39;class1&#39;：7896，&#39;class2&#39;：3034}每个类中有多少行
items_cnt = 0

def train（数据集，目标）：
    global word_stats，words_cnt，targets_stats，items_cnt，class_stats

    num = len（数据集）
    对于Xrange（num）中的项目：
        class_stats [targets [item]] = class_stats.get（targets [item]，0） + 1

        对于i在Xrange（len（dataset [item]）））：
            word = dataset [item] [i]
            如果不是words_stats.has_key（word）：
                word_stats [word] = {}

            TGT =目标[项目]

            cnt = word_stats [word] .get（tgt，0）
            word_stats [word] [tgt] = cnt + 1

            targets_stats [tgt] = targets_stats.get（tgt，0） + 1
            word_cnt += 1

    items_cnt = num

DEF分类（DOC，TGT_SET）：
    global words_stats，words_cnt，targets_stats，items_cnt

    probs = {}＃概率本身p（c | w）= p（w | c） * p（c） / p（w）
    PC = {}＃probability of Clofe in Document set p（c）中
    pwc = {}＃probability在特定类中的单词设置。 P（W | C）
    pw = 1 #1＃documet set中的单词集

    doc中的单词：
        如果在words_stats中没有单词：
            继续#dirty，非常肮脏 
        pw = pw * float（sum（word_stats [word] .values（））） / word_cnt

    对于TGT_SET中的TGT：
        PC [tgt] = class_stats [tgt] / float（items_cnt）
        doc中的单词：
            如果在words_stats中没有单词：
                继续#dirty，非常肮脏
            tgt_wrd_cnt = word_stats [word] .get（tgt，0）
            pwc [tgt] = pwc.get（tgt，1） * float（tgt_wrd_cnt） / targets_stats [tgt]

        probs [tgt] =（pwc [tgt] * pc [tgt]） / pw

    l =排序（probs.items（），key = lambda i：i [1]，反向= true）
    打印概率
    返回L [0] [0]

def check_results（数据集，目标）：
    num = len（数据集）
    tgt_set = set（目标）
    正确= 0
    错误= 0

    对于Xrange（num）中的项目：
        res =分类（dataset [item]，tgt_set）
        如果res ==目标[项目]：
            正确=正确 + 1
        别的：
            错误=不正确 + 1

    打印“正确：”，float（正确） / num，&#39;不正确：&#39;，float（不正确） / num
            
def load_data（fil）：
    数据= []
    tgts = []

    阅读器= csv.reader（fil）
    对于读者中的行：
        d = [X.Strip（）in in in in in in in in in in]
        如果 &#39;？&#39;在D：
            继续

        如果不是Len（D）：
            继续
        
        data.append（d [： -  1]）
        tgts.append（D [-1：] [0]）

    返回数据，TGTS

如果__name__ ==&#39;__ -main __&#39;：
    如果Len（sys.argv）＆lt; 3：
        打印&#39;./program train_data.txt test_data.txt&#39;
        sys.exit（1）

    文件名= sys.argv [1]
    fil = open（文件名，&#39;r&#39;）
    数据，tgt = load_data（fil）
    火车（数据，TGT）

    test_file = open（sys.argv [2]，&#39;r&#39;）
    test_data，test_tgt = load_data（test_file）

    check_results（test_data，tgt）
 
它给出了〜61％的正确结果。当我打印概率时，我会得到以下内容：
  {&#39;＆lt; = 50K&#39;：0.07371606889800396，&#39;＆gt; 50K&#39;：15.325378327213354}
 
但是，在正确的分类器的情况下，我希望看到这两个概率的总和等于1。
起初，我认为问题是在浮动底流中，并试图以对数进行所有计算，但结果是相似的。
我知道省略一些单词会影响准确性，但是概率是错误的。
我做错了什么或不明白？
出于您的说服力，我在这里上传了数据集和Python脚本：
&lt;A href =“ https://dl.dropboxusercontent.com/u/36180992/adult.tar.gz”]]></description>
      <guid>https://stackoverflow.com/questions/19349567/naive-bayes-classifier-implementation-from-scratch</guid>
      <pubDate>Sun, 13 Oct 2013 19:52:37 GMT</pubDate>
    </item>
    <item>
      <title>天真的贝叶斯分类器</title>
      <link>https://stackoverflow.com/questions/9677603/naive-bayes-classifier</link>
      <description><![CDATA[我正在使用Scikit-learn来查找文档的TF-IDF重量，然后使用幼稚
贝叶斯分类器对文本进行分类。但是文档中所有单词的TF-IDF权重除了少数单词。但据我所知，负值意味着不重要的术语。那么，是否有必要将整个TF-IDF值传递给贝叶斯分类器？如果我们只需要通过其中的一些，我们该怎么做？与LinearSVC相比，贝叶斯分类器的好坏是多么好吗？除了使用TF-IDF以外，是否有更好的方法可以在文本中找到标签？]]></description>
      <guid>https://stackoverflow.com/questions/9677603/naive-bayes-classifier</guid>
      <pubDate>Tue, 13 Mar 2012 02:28:44 GMT</pubDate>
    </item>
    </channel>
</rss>