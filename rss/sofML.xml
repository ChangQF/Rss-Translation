<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 26 Jul 2024 15:16:17 GMT</lastBuildDate>
    <item>
      <title>在搜索系统结果上计算 NDCG 时，处理假阳性和假阴性有哪些不同的方法？</title>
      <link>https://stackoverflow.com/questions/78798774/what-are-the-different-ways-to-handle-false-positives-and-false-negatives-when-c</link>
      <description><![CDATA[上下文：
我正在使用 NDCG（归一化折扣累积增益）来评估包含相关性分数的地面实况数据集上的语义搜索系统。我想为此使用 sklearn 的 ndcg_score()。
有什么方法可以处理

假阳性文档：对于给定的查询，那些出现在搜索系统的响应中但不出现在地面实况数据中的文档
假阴性文档：对于给定的查询，那些出现在地面实况数据中但不出现在搜索系统的响应中的文档

一种可能性是插入预测分数 = 0 来表示假阴性并忽略假阳性。但我并不完全确定这是否是正确的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78798774/what-are-the-different-ways-to-handle-false-positives-and-false-negatives-when-c</guid>
      <pubDate>Fri, 26 Jul 2024 15:02:50 GMT</pubDate>
    </item>
    <item>
      <title>解决医学 MRI 数据集中 DICOM 数据数量不平衡的问题</title>
      <link>https://stackoverflow.com/questions/78798297/solving-the-imbalanced-numbers-of-dicom-data-in-medical-mri-datasets</link>
      <description><![CDATA[有一个 DICOM 格式的脑部 MRI 数据集，我们想用它们来训练模型，每个 ID 都有 3 个文件夹，文件夹包含 16 到 20 个文件，现在我的问题是如何平衡它们以使用它们？
该算法可能会将给定的 T1 或 T2 或 T2-flair MRI-DICOM 数据排序为正常和异常]]></description>
      <guid>https://stackoverflow.com/questions/78798297/solving-the-imbalanced-numbers-of-dicom-data-in-medical-mri-datasets</guid>
      <pubDate>Fri, 26 Jul 2024 13:22:36 GMT</pubDate>
    </item>
    <item>
      <title>对图像中的手写和印刷文本进行分类、分割和提取</title>
      <link>https://stackoverflow.com/questions/78798241/classifying-segmenting-and-extracting-handwritten-and-printed-text-in-the-image</link>
      <description><![CDATA[该项目的目标是首先分割（或绘制边界框）并分类图像中的手写和印刷文本，然后从图像中提取手写和印刷文本。印刷文本可以轻松提取，但问题是难以准确提取手写文本。上图是将用于推理的示例图像。我曾使用 pytesseract 库提取文本，但它在手写文本上失败了。

示例输出在此处给出。（黄色-&gt;手写，绿色-&gt;印刷）但这里在每个单词上都画有 BB。是否可以在整行手写或印刷文本上分割或绘制 BB？如果没有，逐字分词或 BB 也可以。
因此，主要任务是使用模型对手写和印刷文本进行分类，然后使用 OCR 模型对手写文本进行准确度较高的分类。这些只是根据我的知识做出的假设。如果有其他最佳方法，请指导我。]]></description>
      <guid>https://stackoverflow.com/questions/78798241/classifying-segmenting-and-extracting-handwritten-and-printed-text-in-the-image</guid>
      <pubDate>Fri, 26 Jul 2024 13:11:48 GMT</pubDate>
    </item>
    <item>
      <title>语义分割中的数据增强</title>
      <link>https://stackoverflow.com/questions/78798068/data-augmentation-in-semantic-segmentation</link>
      <description><![CDATA[我正在尝试对我的数据集执行数据增强，但我得到的却是空白的白色图像，在蒙版上运行良好，但图像存在问题，我不知道如何解决这个问题，帮帮我。 这是原始图像+蒙版
增强图像+蒙版
增强代码：
seed = 24
batch_size = 8

img_datagen_args = dict(rescale = 1/255,
rotation_range = 5,
zoom_range = 0.1,
Horizo​​ntal_flip = True,
Vertical_flip = True,
fill_mode = &#39;nearest&#39;)

mask_datagen_args = dict(rescale = 1/255,
rotation_range = 5,
zoom_range = 0.1,
Horizo​​ntal_flip = True,
Vertical_flip = True,
Fill_mode = &#39;nearest&#39;,
Preprocessing_function = lambda x: np.where(x&gt;0, 1, 0).astype(x.dtype))

img_datagen = ImageDataGenerator(**img_datagen_args)
img_generator = img_datagen.flow_from_directory(&#39;/content/split_dataset/train/images/&#39;,
Seed = seed,
Batch_size = batch_size,
Class_mode = None)

mask_datagen = ImageDataGenerator(**mask_datagen_args)
mask_generator = mask_datagen.flow_from_directory(&#39;/content/split_dataset/train/masks/&#39;,
Seed = seed,
Batch_size = batch_size,
Color_mode = &#39;grayscale&#39;,
Class_mode = None)

valid_img_generator = img_datagen.flow_from_directory(&#39;/content/split_dataset/test/images/&#39;,
seed = seed,
batch_size = batch_size,
class_mode = None)

valid_mask_generator = mask_datagen.flow_from_directory(&quot;/content/split_dataset/test/masks/&quot;,
seed = seed,
batch_size = batch_size,
color_mode = &#39;grayscale&#39;,
class_mode = None)

train_generator = zip(img_generator, mask_generator)
valid_generator = zip(valid_img_generator, valid_mask_generator)

可视化代码：
import matplotlib.pyplot as plt

# 获取一批图像和掩码
img_batch, mask_batch = next(zip(img_generator, mask_generator))

# 绘制一些增强图像
fig, ax = plt.subplots(1, 4, figsize=(20, 5))
for i in range(4):
ax[i].imshow(img_batch[i]) # 对灰度图像使用 cmap=&#39;gray&#39;
ax[i].set_title(f&quot;Augmented Image {i+1}&quot;)
ax[i].axis(&#39;off&#39;)
plt.show()

# 绘制相应的 mask
fig, ax = plt.subplots(1, 4, figsize=(20, 5))
for i in range(4):
ax[i].imshow(mask_batch[i]) # 如果是灰度图像，则挤压以删除通道维度
ax[i].set_title(f&quot;Augmented Mask {i+1}&quot;)
ax[i].axis(&#39;off&#39;)
plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/78798068/data-augmentation-in-semantic-segmentation</guid>
      <pubDate>Fri, 26 Jul 2024 12:32:26 GMT</pubDate>
    </item>
    <item>
      <title>模型（DecisionTreeClassifier）给出了错误的结果</title>
      <link>https://stackoverflow.com/questions/78797339/the-modeldecisiontreeclassifier-gives-wrong-results</link>
      <description><![CDATA[我正在参加一门机器学习课程，其作业是实现 DecisionTreeClassifier 的拟合方法。以下是更多信息的链接：
https://stepik.org/lesson/333977/step/8?auth=login&amp;unit=317388
https://stepik.org/lesson/333977/step/3?auth=login&amp;unit=317388
这是我的代码：
import numpy as np
import pandas as pd

class MyTreeClf:
def __init__(self, max_depth=5, min_samples_split=2, max_leafs=20):
self.max_depth = max_depth
self.min_samples_split = min_samples_split
self.max_leafs = max_leafs
self.tree = None
self.leafs_cnt = 0

def node_entropy(self, probs):
return -np.sum([p * np.log2(p) for p in probs if p &gt; 0])

def node_ig(self, x_col, y, split_value):
left_mask = x_col &lt;= split_value
right_mask = x_col &gt; split_value

如果 len(x_col[left_mask]) == 0 或 len(x_col[right_mask]) == 0:
返回 0

left_probs = np.bincount(y[left_mask]) / len(y[left_mask])
right_probs = np.bincount(y[right_mask]) / len(y[right_mask])

entropy_after = len(y[left_mask]) / len(y) * self.node_entropy(left_probs) + len(y[right_mask]) / len(y) * self.node_entropy(right_probs)
entropy_before = self.node_entropy(np.bincount(y) / len(y))

返回 entropy_before - entropy_after

def get_best_split(self, X: pd.DataFrame，y：pd.Series）：
best_col，best_split_value，best_ig = None，None，-np.inf

对于 X.columns 中的 col：
sorted_unique_values = np.sort(X[col].unique())

对于 range(1，len(sorted_unique_values)) 中的 i：
split_value = (sorted_unique_values[i - 1] + sorted_unique_values[i]) / 2

ig = self.node_ig(X[col]，y，split_value)

如果 ig &gt; best_ig:
best_ig = ig
best_col = col
best_split_value = split_value

返回 best_col、best_split_value、best_ig

def fit(self, X: pd.DataFrame, y: pd.Series,depth=0):
如果depth == 0:
self.tree = {}

best_col、best_split_value、best_ig = self.get_best_split(X, y)

如果depth &lt; self.max_depth 和 len(y) &gt;= self.min_samples_split 和 self.leafs_cnt &lt; self.max_leafs 和 best_col 不为 None:
left_mask = X[best_col] &lt;= best_split_value
right_mask = X[best_col] &gt; best_split_value

self.tree[depth] = {&#39;col&#39;: best_col, &#39;split&#39;: best_split_value, &#39;left&#39;: {}, &#39;right&#39;: {}}

self.fit(X[left_mask], y[left_mask],depth + 1)
self.fit(X[right_mask], y[right_mask],depth + 1)
else:
class_label = y.mode()[0]
self.tree[depth] = {&#39;class&#39;: class_label}
self.leafs_cnt += 1

我已验证node_entropy、node_ig和get_best_split方法正常工作。但是，fit方法对第一个样本返回4，而预期结果是2。
测试包括三个数据集和参数max_depth、min_samples_split和max_leafs。他们为第一个样本提供了以下参数：
样本输入：{&quot;max_depth&quot;: 3, &quot;min_samples_split&quot;: 2, &quot;max_leafs&quot;: 1}
样本输出：2 (leafs_cnt)
不幸的是，他们没有提供这些数据集]]></description>
      <guid>https://stackoverflow.com/questions/78797339/the-modeldecisiontreeclassifier-gives-wrong-results</guid>
      <pubDate>Fri, 26 Jul 2024 09:48:47 GMT</pubDate>
    </item>
    <item>
      <title>线性模型的 SHAP 值与手动计算的值不同</title>
      <link>https://stackoverflow.com/questions/78796974/shap-values-for-linear-model-different-from-those-calculated-manually</link>
      <description><![CDATA[我训练一个线性模型来预测房价，然后我手动比较 Shapley 值计算结果与 SHAP 库返回的值，发现它们略有不同。
我的理解是，对于线性模型，Shapley 值由以下公式给出：
coeff * features for obs - coeffs * mean(features in training set)

或者如 SHAP 文档中所述：coef[i] * (x[i] - X.mean(0)[i])，其中 i 是一个特征。
问题是，为什么 SHAP 返回的值与手动计算不同？
代码如下：
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
来自 sklearn.preprocessing 导入 MinMaxScaler
导入 shap

X, y = fetch_california_housing(return_X_y=True, as_frame=True)

X = X.drop(columns = [&quot;Latitude&quot;, &quot;Longitude&quot;, &quot;AveBedrms&quot;])

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.3, random_state=0,
)

scaler = MinMaxScaler().set_output(transform=&quot;pandas&quot;).fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

linreg = LinearRegression().fit(X_train, y_train)
coeffs = pd.Series(linreg.coef_, index=linreg.feature_names_in_)

X_test.reset_index(inplace=True, drop=True)
obs = 6188

# 手动 shapley 计算
effect = coeffs * X_test.loc[obs]
effect - coeffs * X_train.mean()

返回结果：
MedInc 0.123210
HouseAge -0.459784
AveRooms -0.128162
Population 0.032673
AveOccup -0.001993
dtype: float64

SHAP 库返回的结果略有不同：
explainer = shap.LinearExplainer(linreg, X_train)
shap_values = explainer(X_test)
shap_values[obs]

结果如下：
.values =
array([ 0.12039244, -0.47172515, -0.12767778, 0.03473923, -0.00251017])

.base_values =
2.0809714707337523

.data =
array([0.25094137, 0.01960784, 0.06056066, 0.07912217, 0.00437137])

设置为忽略交互：
explainer.feature_perturbation

返回
&#39;interventional&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/78796974/shap-values-for-linear-model-different-from-those-calculated-manually</guid>
      <pubDate>Fri, 26 Jul 2024 08:22:09 GMT</pubDate>
    </item>
    <item>
      <title>为什么在训练期间我的 TensorFlow Siamese 网络中的所有变量的梯度都为 None？</title>
      <link>https://stackoverflow.com/questions/78796915/why-are-gradients-none-for-all-variables-in-my-tensorflow-siamese-network-during</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78796915/why-are-gradients-none-for-all-variables-in-my-tensorflow-siamese-network-during</guid>
      <pubDate>Fri, 26 Jul 2024 08:08:30 GMT</pubDate>
    </item>
    <item>
      <title>GradientBoostedClassifier() 中的 min_samples_leaf 行为怪异</title>
      <link>https://stackoverflow.com/questions/78796671/min-samples-leaf-in-gradientboostedclassifier-having-weird-behavior</link>
      <description><![CDATA[尝试在 GradientBoostedClassifer() 中调整 min_samples_leaf。我看到了偏差/方差权衡的预期结果。但是，为了测试边界，我让 min_samples_leaf &gt;训练数据集中有 n_samples，预计会出现错误或其他问题，但我仍然得到与模型调整时类似的结果：
df = df_a # 样本数 = 347
df=df.sample(frac=1) 
train_proportion = 0.8 
n = len(df)
t = int(train_proportion * n)

# 单独的训练和测试集
y = df[&#39;detected&#39;]
X = df.loc[:, ~df.columns.isin([&#39;detected&#39;])]

# 训练集中的样本
train_x = X.iloc[:t,:].reset_index().iloc[:,1:]
# 测试集中的样本
test_x = X.iloc[t:,:].reset_index().iloc[:,1:]
# 训练集中的目标
train_y = pd.Series(y[:t].reset_index().iloc[:,1:].iloc[:,0])
#测试集中的目标
test_y = pd.Series(y[t:].reset_index().iloc[:,1:].iloc[:,0])

clf = GradientBoostingClassifier(n_estimators = 100, max_depth = 10, random_state= 0, min_samples_leaf=500)
clf.fit(train_x,train_y)
print(clf.score(train_x,train_y))
print(clf.score(test_x,test_y))

输出：
0.924187725631769
0.9142857142857143

为什么会这样？我预计会出现错误或不会进行拆分。文档中似乎没有说明如果 min_samples_leaf &gt; n_samples 会发生什么。对 int 的唯一要求是范围 [1,inf]。对此也没有其他说明。
我当时想也许它会将 min_samples_leaf 重置为某个可用值，但所有子树都没有深度，也没有进行拆分：subtree]]></description>
      <guid>https://stackoverflow.com/questions/78796671/min-samples-leaf-in-gradientboostedclassifier-having-weird-behavior</guid>
      <pubDate>Fri, 26 Jul 2024 07:05:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用低质量 PDF 文件来提高 RAG 性能？</title>
      <link>https://stackoverflow.com/questions/78796435/how-to-improve-rag-performance-with-low-quality-pdf-files</link>
      <description><![CDATA[我正在做一个 RAG 项目。其中一个步骤是从 pdf 文件中提取文本。我发现，如果输入的 pdf 质量好，它工作得很好，但是当输入的 pdf 质量差时，我的 RAG 无法回答一些问题。如果我必须处理低质量的 pdf 文件，我该如何改进我的 RAG？
我使用 PyMuPDF 直接从 PDF 中提取文本。
我使用 Chroma 作为我的矢量数据库。
我使用 BAAI/bge-m3 作为我的嵌入模型。]]></description>
      <guid>https://stackoverflow.com/questions/78796435/how-to-improve-rag-performance-with-low-quality-pdf-files</guid>
      <pubDate>Fri, 26 Jul 2024 06:01:26 GMT</pubDate>
    </item>
    <item>
      <title>在 Keras 中实现具有多个输入的循环神经网络训练算法</title>
      <link>https://stackoverflow.com/questions/78796429/implementation-of-training-algorithm-for-recurrent-neural-networks-with-multiple</link>
      <description><![CDATA[因此，我在一本名为“使用 Scikit-Learn、Keras 和 TensorFlow 进行机器学习实践”的书中阅读了有关 RNN 的知识，该书的作者是 Aurelien Geron。我遇到了一个使用 Keras 的序列到序列 RNN 的非常简单的实现：
seq2seq_model = tf.keras.Sequential([
tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 5]),
tf.keras.layers.Dense(14)
])

这应该采取任意数量的时间步骤（在本例中为 56），每个步骤有 5 个特征，并创建一个向量，其中包含每个时间步骤每天未来 2 周的预测。
这是从 repo 中获取的：
https://github.com/ageron/handson-ml3/blob/main/15_processing_sequences_using_rnns_and_cnns.ipynb
但理解这个问题并不需要它。
因此，时间序列数据以 56 的序列长度传递。因此，在调用 predict 之后，我们有效地获得了 56 个预测向量，每个向量有 14 天的预测（前 55 个用于训练，最后一个是实际预测）。我的问题是：如果有 32 个输入神经元，每个神经元都采用前一个时间步骤，如果前 31 个时间步骤之前的时间步骤不足以作为输入传递给所有神经元，我们如何获得 56 个预测？所有缺失的特征都设置为 0 吗？在这些情况下，前向传递是如何完成的？当我们尝试训练模型使用完整的 56 个时间步骤数据进行预测时，这些情况下的反向传播有何用处？我正在寻找 Keras 特定的答案以及对此的一些一般见解。
我尝试运行此代码：
X = mulvar_valid.to_numpy()[np.newaxis, :seq_length]
y_pred_14 = seq2seq_model.predict(np.asarray(X).astype(np.float32))
print(len(y_pred_14[0]))

发现预测有 56 个向量。我预计一个 32 大小的窗口会在 56 个时间步骤中滚动以产生大约 20 个预测向量。]]></description>
      <guid>https://stackoverflow.com/questions/78796429/implementation-of-training-algorithm-for-recurrent-neural-networks-with-multiple</guid>
      <pubDate>Fri, 26 Jul 2024 06:00:48 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError: ‘list’ 对象没有属性 ‘shape’ 错误</title>
      <link>https://stackoverflow.com/questions/78796342/attributeerror-list-object-has-no-attribute-shape-error</link>
      <description><![CDATA[我目前正在尝试遵循教程，因为我刚刚开始学习机器学习。
我正在尝试预测股票价格。这是我的代码：
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as web
import numpy as np

从 sklearn.preprocessing 导入 MinMaxScaler
从 tensorflow.python.keras.models 导入 Sequential
从 tensorflow.python.keras.layers 导入 Dense、Dropout
从 tensorflow.python.keras.layers.recurrent 导入 LSTM

company = &#39;TSLA&#39;

start=&#39;2012-01-01&#39;
end=&#39;2024-03-01&#39;

data = web.download(company, start=start, end=end)

scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(data[&#39;Close&#39;].values.reshape(-1,1))

prediction_days = 60

x_train = []
y_train = []

for x in range(prediction_days, len(scaled_data)):
x_train.append(scaled_data[x-prediction_days:x, 0])
y_train.append(scaled_data[x, 0])

model = Sequential()

model.add(LSTM(units = 50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units = 50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units = 50))
model.add(Dropout(0.2))

我期望它什么都不输入，但是我收到此错误：
但它显示为错误，
回溯（最近一次调用）：
文件
&quot;c:\Users\User1\OneDrive\Documents\Desktop\python\projects\machine\stock_price_predictor.py&quot;，
第 32 行，在 &lt;module&gt;
model.add(LSTM(units = 50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
^^^^^^^^^^^^^
AttributeError: &#39;list&#39; 对象没有属性 &#39;shape&#39;

你们有人知道如何解决这个问题吗？我尝试将其转换为 np.array，但没有任何效果。这是我的尝试：
&#39;
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as web
import numpy as np

从 sklearn.preprocessing 导入 MinMaxScaler
从 tensorflow.python.keras.models 导入 Sequential
从 tensorflow.python.keras.layers 导入 Dense、Dropout
从 tensorflow.python.keras.layers.recurrent 导入 LSTM

company = &#39;TSLA&#39;

start=&#39;2012-01-01&#39;
end=&#39;2024-03-01&#39;

data = web.download(company, start=start, end=end)

scaler = MinMaxScaler(feature_range=(0,1))
scaled_data=.fit_transform(data[&#39;Close&#39;].values.reshape(-1,1))

prediction_days = 60

x_train = np.array([])
y_train = np.array([])

对于 x 在 range(prediction_days, len(scaled_data)) 中：
x_train = np.append(x_train, scaled_data[x-prediction_days:x, 0])
y_train = np.append(y_train, scaled_data[x, 0])

model = Sequential()

model.add(LSTM(units = 50, return_sequences=True, input_shape= 
(x_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units = 50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units = 50))
model.add(Dropout(0.2))

&#39;
但我却得到了这个错误
&#39;
Traceback (most recent call last):
File 
&quot;c:\Users\User1\OneDrive\Documents\Desktop\python\projects\machine 
learning\stock_price_predictor.py&quot;, line 32, in &lt;module&gt;
model.add(LSTM(units = 50, return_sequences=True, input_shape= 
(x_train.shape[1], 1))) 
~~~~~~~~~~~~~^^^
IndexError: 元组索引超出范围

&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78796342/attributeerror-list-object-has-no-attribute-shape-error</guid>
      <pubDate>Fri, 26 Jul 2024 05:31:21 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：参数 clone_function 和 input_tensors 仅支持顺序模型或功能模型</title>
      <link>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</link>
      <description><![CDATA[我正在使用Quantization perceived training，参考网上的lstm代码，想把QAT放进lstm，结果遇到了ValueError。
ValueError Traceback (most recent call last)
&lt;ipython-input-11-00669bb76f9d&gt; in &lt;cell line: 6&gt;()
4 return layer
5 
----&gt; 6 annotated_model = tf.keras.models.clone_model(
7 model,
8 clone_function=apply_quantization_to_dense,

/usr/local/lib/python3.10/dist-packages/tf_keras/src/models/cloning.py in clone_model(model, input_tensors, clone_function)
544 # 自定义模型类的情况
545 if clone_function or input_tensors:
--&gt; 546 raise ValueError(
547 &quot;参数 clone_function 和 input_tensors &quot;
548 &quot;仅支持 Sequential 模型 &quot;

ValueError: 参数 clone_function 和 input_tensors 仅支持 Sequential 模型或 Functional 模型。收到类型为“Sequential”的模型，其中 clone_function=&lt;function apply_quantization_to_dense 位于0x78b727ec4040&gt; 和 input_tensors=None

这是我的代码
import keras
从 keras.layers 导入 LSTM
从 keras.layers 导入 Dense、Activation
从 keras.datasets 导入 mnist
从 keras.models 导入 Sequential
从 keras.optimizers 导入 Adam

learning_rate = 0.001
training_iters = 20
batch_size = 128
display_step = 10

n_input = 28
n_step = 28
n_hidden = 128
n_classes = 10

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(-1, n_step, n_input)
x_test = x_test.reshape(-1, n_step, n_input)
x_train = x_train.astype(&#39;float32&#39;)
x_test = x_test.astype(&#39;float32&#39;)
x_train /= 255
x_test /= 255

y_train = keras.utils.to_categorical(y_train, n_classes)
y_test = keras.utils.to_categorical(y_test, n_classes)

model = Sequential()
model.add(LSTM(n_hidden,
batch_input_shape=(None, n_step, n_input),
unroll=True))

model.add(Dense(n_classes))
model.add(Activation(&#39;softmax&#39;))

adam = Adam(lr=learning_rate)
model.summary()
model.compile(optimizer=adam,
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

model.fit(x_train, y_train,
batch_size=batch_size,
epochs=training_iters,
verbose=1,
validation_data=(x_test, y_test))

scores = model.evaluate(x_test, y_test, verbose=0)
print(&#39;LSTM 测试分数：&#39;, scores[0])
print(&#39;LSTM 测试准确率：&#39;, scores[1])

def apply_quantization_to_dense(layer):
if isinstance(layer, tf.keras.layers.LSTM):
return tfmot.quantization.keras.quantize_annotate_layer(layer)
return layer

annotated_model = tf.keras.models.clone_model(
model,
clone_function=apply_quantization_to_dense,
)
qat_model = tfmot.quantization.keras.quantize_apply(annotated_model)
qat_model.summary()
]]></description>
      <guid>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</guid>
      <pubDate>Fri, 26 Jul 2024 03:41:57 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：不支持 y 的稀疏多标签指标 - 如何处理具有稀疏数据的多标签分类？</title>
      <link>https://stackoverflow.com/questions/78795297/valueerror-sparse-multilabel-indicator-for-y-is-not-supported-how-to-handle-m</link>
      <description><![CDATA[我只是一个初学者，我还在学习稀疏矩阵以及它们如何与其他东西一起工作。
这是我遇到的问题，在网上搜索后找不到合适的答案。
我使用默认参数 sparse_output=True 对分类标签进行了 OneHotEncoded，
当我尝试在训练测试拆分后使用 transformed_X 和目标 y 拟合 RandomForestClassifier 时，它显示了此错误。
#seed
np.random.seed(42)

#one hot encoding imports
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer as ct

#Data splitting
X = f_data.drop(&#39;attended&#39;, axis = 1)
y = f_data[&#39;attended&#39;]

#select columns
cat_col = [&#39;days_before&#39;,&#39;day_of_week&#39;,&#39;time&#39;,&#39;category&#39;]

#初始化编码器 
enc = OneHotEncoder()

#使用 ct 拟合编码器
transformer = ct([(&#39;enc&#39;,enc,cat_col)], remainder = &#39;passthrough&#39;)
transformed_X = transformer.fit_transform(X)
transformed_X

&lt;1480x36 稀疏矩阵，类型为 &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;压缩稀疏行格式中存储了 10360 个元素&gt;
#BaseLine 模型
np.random.seed(42)

#imports
from sklearn.model_selection import train_test_split as tts
from sklearn.ensemble import RandomForestClassifier

#splitting
X_train,Y_train,X_test,Y_test = tts(transformed_X,y, test_size = 0.2)

#model fitting
model = RandomForestClassifier()
model.fit(X_train,Y_train)

#model score
blsc = model.score(X_test,Y_test)
print(f&#39;Baseline Model Score is : {blsc}&#39;)

-------------------------------------------------------------------------------
ValueError Traceback (most recent call last)
Cell在[416]中，第 13 行
11 #modelling
12 model = RandomForestClassifier()
---&gt; 13 model.fit(X_train,Y_train)
15 #模型得分
16 blsc = model.score(X_test,Y_test)

文件 G:\Md Jaffer\UDEMY\Machine Learning Course ZTM\Projects\HeartDesease_Classification\env\Lib\site-packages\sklearn\base.py:1474，在 _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)
1467 estimator._validate_params()
1469 使用 config_context(
1470 skip_parameter_validation=(
1471 prefer_skip_nested_validation 或 global_skip_validation
1472 )
1473 ):
-&gt; 1474 返回 fit_method(estimator, *args, **kwargs)

文件 G:\Md Jaffer\UDEMY\Machine Learning Course ZTM\Projects\HeartDesease_Classification\env\Lib\site-packages\sklearn\ensemble\_forest.py:361，位于 BaseForest.fit(self, X, y, sample_weight)
359 # 验证或转换输入数据
360 if issparse(y):
--&gt; 361 引发 ValueError(&quot;不支持 y 的稀疏多标签指标。&quot;)
363 X, y = self._validate_data(
364 X,
365 y,
(...)
369 force_all_finite=False,
370 )
371 # _compute_missing_values_in_feature_mask 检查 X 是否有缺失值，
372 # 如果底层树基础估计器无法处理缺失值，则会引发错误。
373 # 仅需标准即可确定树是否支持
374 # 缺失值。

ValueError: 不支持 y 的稀疏多标签指标。

我尝试设置 sparse_output=False，但结果显示样本数量不一致。标签编码后的实际形状为 (1480 x 36)
---------------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[444]，第 13 行
11 #modelling
12 model = RandomForestClassifier()
---&gt; 13 model.fit(X_train,Y_train)
15 #model score
16 blsc = model.score(X_test,Y_test)

ValueError：发现输入变量的样本数量不一致：[1184, 296]
]]></description>
      <guid>https://stackoverflow.com/questions/78795297/valueerror-sparse-multilabel-indicator-for-y-is-not-supported-how-to-handle-m</guid>
      <pubDate>Thu, 25 Jul 2024 20:21:08 GMT</pubDate>
    </item>
    <item>
      <title>安装 tf-models-official 时出现元数据生成失败</title>
      <link>https://stackoverflow.com/questions/78786800/metadata-generation-failed-when-installing-tf-models-official</link>
      <description><![CDATA[我尝试使用 !pip install tf-models-official 安装 tf-models-official，当它开始收集 kaggle&gt;=1.3.9 时，它返回以下错误：
收集 kaggle&gt;=1.3.9（来自 tf-models-official）
使用缓存的 kaggle-1.6.15.tar.gz (9.1 kB)
安装构建依赖项...完成
获取构建 wheel 的要求...完成
准备元数据（pyproject.toml）...错误
错误：子进程退出并出现错误

× 准备元数据（pyproject.toml）未成功运行。
│ 退出代码：1
╰─&gt; [35 行输出]
回溯（最近一次调用）：
文件 &quot;/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;，第 353 行，位于 &lt;module&gt;
main()
文件 &quot;/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;，第 335 行，在 main 中
json_out[&#39;return_val&#39;] = hook(**hook_input[&#39;kwargs&#39;])
文件 &quot;/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py&quot;，第 152 行，在 prepare_metadata_for_build_wheel 中
whl_basename = backend.build_wheel(metadata_directory, config_settings)
文件&quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/build.py&quot;，第 58 行，在 build_wheel 中
return os.path.basename(next(builder.build(directory=wheel_directory,versions=[&#39;standard&#39;])))
文件 &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/plugin/interface.py&quot;，第 155 行，在 build 中
artifact = version_api[version](directory,**build_data)
文件 &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/wheel.py&quot;，第 475 行，在build_standard
for included_file in self.recurse_included_files():
File &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/plugin/interface.py&quot;, line 176, in recurse_included_files
Yield from self.recurse_selected_project_files()
File &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/plugin/interface.py&quot;, line 180, in recurse_selected_project_files
if self.config.only_include:
File &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/config.py&quot;，第 806 行，在 only_include 中
only_include = only_include_config.get(&#39;only-include&#39;, self.default_only_include()) 或 self.packages
文件 &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/wheel.py&quot;，第 260 行，在 default_only_include 中
return self.default_file_selection_options.only_include
文件 &quot;/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/functools.py&quot;，第 981 行，在__get__
val = self.func(instance)
文件 &quot;/tmp/pip-build-env-fqrzl9xw/overlay/lib/python3.10/site-packages/hatchling/builders/wheel.py&quot;，第 248 行，位于 default_file_selection_options
raise ValueError(message)
ValueError：无法使用以下启发式方法确定要将哪些文件发送到 wheel 内：https://hatch.py​​pa.io/latest/plugins/builder/wheel/#default-file-selection

最可能的原因是没有与您的项目 (kaggle) 名称匹配的目录。

必须在 `tool.hatch.build.targets.wheel` 表中定义至少一个文件选择选项，请参阅：https://hatch.py​​pa.io/latest/config/build/

例如，如果您打算发送一个名为 `foo` 的目录，该目录位于项目根目录的 `src` 目录中，则可以定义以下内容：

[tool.hatch.build.targets.wheel]
packages = [&quot;src/foo&quot;]
[输出结束]

注意：此错误源自子进程，可能不是 pip 的问题。
错误：metadata-generation-failed

× 生成包元数据时遇到错误。
╰─&gt; 请参阅上面的输出。

注意：这是上面提到的包的问题，​​而不是 pip。
提示：请参阅上文了解详情。

我能够在 2 周前安装，现在在新的 jupyter 笔记本内核上突然无法安装。我尝试在旧内核上重新安装，也出现了同样的错误。有人知道如何解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78786800/metadata-generation-failed-when-installing-tf-models-official</guid>
      <pubDate>Wed, 24 Jul 2024 07:02:59 GMT</pubDate>
    </item>
    <item>
      <title>没有名为“tensorflow.keras.layers.experimental.preprocessing”的模块</title>
      <link>https://stackoverflow.com/questions/63542803/no-module-named-tensorflow-keras-layers-experimental-preprocessing</link>
      <description><![CDATA[代码下方
import numpy as np
np.random.seed(0)
from sklearn import datasets
import matplotlib.pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format =&#39;retina&#39;

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD

错误消息下方
-------------------------------------------------------------------------------
ModuleNotFoundError Traceback (most recent call last)
~\Anaconda3\lib\site-packages\keras\__init__.py in &lt;module&gt;
2 try:
----&gt; 3 from tensorflow.keras.layers.experimental.preprocessing import RandomRotation
4 except ImportError:

ModuleNotFoundError: 没有名为“tensorflow.keras.layers.experimental.preprocessing”的模块

在处理上述异常期间，发生了另一个异常：

ImportError Traceback（最近一次调用最后一次）
&lt;ipython-input-5-943507dd87a6&gt; in &lt;module&gt;
6 get_ipython().run_line_magic(&#39;config&#39;, &quot;InlineBackend.figure_format =&#39;retina&#39;&quot;)
7 
----&gt; 8 从 keras.models 导入 Sequential
9 从 keras.layers 导入 Dense
10 从 keras.optimizers 导入 SGD

~\Anaconda3\lib\site-packages\keras\__init__.py in &lt;module&gt;
4 except ImportError:
5 raise ImportError(
----&gt; 6 &#39;Keras 需要 TensorFlow 2.2 或更高版本。&#39;
7 &#39;通过 `pip install tensorflow`&#39; 安装 TensorFlow)
8 

ImportError: Keras 需要 TensorFlow 2.2 或更高版本。通过 `pip install tensorflow` 安装 TensorFlow

注意：`我认为，主要问题是 Tensorflow 版本。我使用了一些命令，如下所示，
conda create -n tf tensorflow
conda activate tf

我还使用了以下命令
conda create -n tf-gpu tensorflow-gpu
conda activate tf-gpu

但是它不起作用，请帮助解决错误。]]></description>
      <guid>https://stackoverflow.com/questions/63542803/no-module-named-tensorflow-keras-layers-experimental-preprocessing</guid>
      <pubDate>Sun, 23 Aug 2020 02:18:29 GMT</pubDate>
    </item>
    </channel>
</rss>