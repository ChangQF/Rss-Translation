<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 25 Aug 2024 15:15:25 GMT</lastBuildDate>
    <item>
      <title>我正在进行网页抓取，我抓取了一个网站，我做的一切都正确无误，但为什么输出为空？像这样 [] [关闭]</title>
      <link>https://stackoverflow.com/questions/78911337/i-am-doing-web-scraping-i-took-one-website-i-did-everthing-correctly-but-i-g</link>
      <description><![CDATA[我正在尝试使用 BeautifulSoup 抓取电影名称
我使用的代码片段是：
scraped_movies = soup.find_all(&quot;td&quot;, class_=&quot;titleColumn&quot;)
1.
但是，scraped_movies 为空，这意味着 soup.find_all 没有找到任何带有 td 标签和 titleColumn 类的元素]]></description>
      <guid>https://stackoverflow.com/questions/78911337/i-am-doing-web-scraping-i-took-one-website-i-did-everthing-correctly-but-i-g</guid>
      <pubDate>Sun, 25 Aug 2024 13:33:52 GMT</pubDate>
    </item>
    <item>
      <title>使用对最终目标有贡献的特征子集来训练核岭回归</title>
      <link>https://stackoverflow.com/questions/78911183/training-kernel-ridge-regression-with-subsets-of-features-contributing-to-the-fi</link>
      <description><![CDATA[我正在使用 Python 中的核岭回归 (KRR) 和 scikit-learn 研究机器学习问题。我的目标是训练一个内核，其中特征子集有助于目标预测。
例如，我们知道在标准情况下，训练将针对以下内容进行：
特征数据 X ((n_samples, n_features)) -&gt; Y (n_samples)
我想要做的是，训练回归模型具有维度映射：(n_subset) -&gt; y_subset
因此，(n_features) 由大小为 (n_subset) 的子组组成，这些子组本质上具有类似的起源，它们的模式重复相似，但值并不严格相同。
假设 (n_features) = (n_subset)*(n_terms)。
作为一个过早的解决方案，我已经实现了一个模型，其中每个特征子集对最终预测的贡献相同。以下是示例代码：
import numpy as np
from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import GridSearchCV

def train_kernel_with_weights(X, Y, n_subset, kernel_param_grid):
nset, nfeat_total = X.shape
n_kerterm = nfeat_total // n_subset # 内核预测项的数量，将加总为总数
X_sub_rearr = X.reshape(nset * n_kerterm, n_subset)

kr1 = GridSearchCV(KernelRidge(), param_grid=kernel_param_grid, cv=5)

# 此处，测试了“等权重”方案，任意假设每个 (n_subset)-&gt;y_subset 项
# 对总 Y 的贡献相等。
kr1.fit(X_sub_rearr, np.repeat(Y/n_kerterm, n_kerterm))

return kr1

# 给定一个具有映射 (n_subset -&gt; value) 的核，获取子集总和预测值
def predict_krrsum_1(n_subset, kr1, X):
nset, nfeat_total = X.shape
n_kerterm = nfeat_total // n_subset

# 使用 n_subset 分割每个数据点的特征（可能是三元组）
X_sub_rearr = X.reshape(nset*n_kerterm,n_subset)
Y_krrsub_flat = kr1.predict(X_sub_rearr) # 1d 数组 (nset*n_kerterm)
Y_krrsub = Y_krrsub_flat.reshape(-1,n_kerterm) # (nset, n_kerterm)
Y_krrsum_1 = np.sum(Y_krrsub,axis=1)
return Y_krrsum_1

那么我的问题是：对于核训练函数：train_kernel_with_weights，
我该如何优化子集的权重，而不是任意使用相等的权重？]]></description>
      <guid>https://stackoverflow.com/questions/78911183/training-kernel-ridge-regression-with-subsets-of-features-contributing-to-the-fi</guid>
      <pubDate>Sun, 25 Aug 2024 12:10:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 在低资源语言和葡萄牙语之间进行机器翻译的语言模型</title>
      <link>https://stackoverflow.com/questions/78911175/a-language-model-for-machine-translation-between-a-low-resource-language-and-por</link>
      <description><![CDATA[我正在尝试使用 Tensorflow 训练一种语言模型，用于在低资源语言和葡萄牙语之间进行机器翻译。不幸的是，我收到以下错误：
PS C:\Users\myuser\PycharmProjects\teste&gt; python .\tensorflow_model.py 
2024-08-23 21:29:50.839647：I tensorflow/core/platform/cpu_feature_guard.cc:182] 此 TensorFlow 二进制文件经过优化，可在性能关键型操作中使用可用的 CPU 指令。
要启用以下指令：SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA，在其他操作中，使用适当的编译器标志重建 TensorFlow。
回溯（最近一次调用）：
文件“.\tensorflow_model.py”，第 52 行，位于 &lt;module&gt;
数据集 = tf.data.Dataset.from_tensor_slices((src_tensor, tgt_tensor)).shuffle(BUFFER_SIZE)
文件 &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py&quot;，第 831 行，在 from_tensor_slices 中
返回 from_tensor_slices_op._from_tensor_slices(tensors, name)
文件 &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\data\ops\from_tensor_slices_op.py&quot;，第 25 行，在 _from_tensor_slices 中
返回 _TensorSliceDataset(tensors, name=name)
文件&quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\data\ops\from_tensor_slices_op.py&quot;，第 45 行，在 __init__
batch_dim.assert_is_compatible_with(
File &quot;C:\Users\myuser\PycharmProjects\teste\.venv\lib\site-packages\tensorflow\python\framework\tensor_shape.py&quot;，第 300 行，在 assert_is_compatible_with
raise ValueError(&quot;Dimensions %s and %s are notcompatible&quot; %
ValueError: Dimensions 21 and 22 are notcompatible

我该如何克服这个错误？
import tensorflow as tf
import numpy as np
import re
import os

# Clean数据
def preprocess_sentence(sentence):
sentence = sentence.lower().strip()
sentence = re.sub(r&quot;([?.!,¿])&quot;, r&quot; \1 &quot;, sentence)
sentence = re.sub(r&#39;[&quot; &quot;]+&#39;, &quot; &quot;, sentence)
sentence = re.sub(r&quot;[^a-zA-Z?.!,¿]+&quot;, &quot; &quot;, sentence)
sentence = sentence.strip()
sentence = &#39;&lt;start&gt; &#39; + sentence + &#39; &lt;end&gt;&#39;
返回句子

#加载数据的函数
def load_data(file_path_src, file_path_tgt):
src_sentences = open(file_path_src, &#39;r&#39;, encoding=&#39;utf-8&#39;).read().strip().split(&#39;\n&#39;)
tgt_sentences = open(file_path_tgt, &#39;r&#39;, encoding=&#39;utf-8&#39;).read().strip().split(&#39;\n&#39;)

src_sentences = [preprocess_sentence(sentence) for sentence in src_sentences]
tgt_sentences = [preprocess_sentence(sentence) for sentence in tgt_sentences]

返回 src_sentences, tgt_sentences

#加载数据
src_sentences, tgt_sentences = load_data(&#39;src_language.txt&#39;, &#39;portuguese.txt&#39;)

#标记化
src_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=&#39;&#39;)
tgt_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=&#39;&#39;)

src_tokenizer.fit_on_texts(src_sentences)
tgt_tokenizer.fit_on_texts(tgt_sentences)

src_tensor = src_tokenizer.texts_to_sequences(src_sentences)
tgt_tensor = tgt_tokenizer.texts_to_sequences(tgt_sentences)

src_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_tensor, padding=&#39;post&#39;)
tgt_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_tensor, padding=&#39;post&#39;)

BUFFER_SIZE = len(src_tensor)

#创建数据集
dataset = tf.data.Dataset.from_tensor_slices((src_tensor, tgt_tensor)).shuffle(BUFFER_SIZE) 
]]></description>
      <guid>https://stackoverflow.com/questions/78911175/a-language-model-for-machine-translation-between-a-low-resource-language-and-por</guid>
      <pubDate>Sun, 25 Aug 2024 12:06:55 GMT</pubDate>
    </item>
    <item>
      <title>Resnet 训练和评估模式不一致</title>
      <link>https://stackoverflow.com/questions/78911068/resnet-inconsistency-between-train-and-eval-mode</link>
      <description><![CDATA[我正在尝试在 torch 中实现 Resnet。但我发现前向传递的输出在训练和评估模式之间差异很大。由于训练和评估模式除了 batch norm 和 dropout 之外不影响任何东西，所以我不知道结果是否有意义。
下面是我的测试代码：
import torch
from torch import nn
from torchvision import models

class resnet_lstm(torch.nn.Module):
def __init__(self):
super(resnet_lstm, self).__init__()
resnet = models.resnet50(pretrained=True)
self.share = torch.nn.Sequential()
self.share.add_module(&quot;conv1&quot;, resnet.conv1)
self.share.add_module(&quot;bn1&quot;, resnet.bn1) # 使用 BatchNorm3d
self.share.add_module(&quot;relu&quot;, resnet.relu)
self.share.add_module(&quot;maxpool&quot;, resnet.maxpool)
self.share.add_module(&quot;layer1&quot;, resnet.layer1)
self.share.add_module(&quot;layer2&quot;, resnet.layer2)
self.share.add_module(&quot;layer3&quot;, resnet.layer3)
self.share.add_module(&quot;layer4&quot;, resnet.layer4)
self.share.add_module(&quot;avgpool&quot;, resnet.avgpool)
self.fc = nn.Sequential(nn.Linear(2048, 512),
nn.ReLU(),
nn.Linear(512, 7))

def forward(self, x):
x = x.view(-1, 3, 224, 224)
x = self.share(x)
返回x

model = resnet_lstm()

input_ = torch.randn(1, 3, 224, 224)
model.train()
print(&quot;训练模式输出&quot;, model(input_))
model.eval()
print(&quot;评估模式输出&quot;, model(input_))


终端输出：
训练模式输出 tensor([[[[0.3603]],

[[0.5518]],

[[0.4599]],

...,

[[0.3381]],

[[0.4445]],

[[0.3481]]]], grad_fn=&lt;MeanBackward1&gt;)
评估模式输出tensor([[[[0.1582]],

[[0.1822]],

[[0.0000]],

...,

[[0.0567]],

[[0.0054]],

[[0.3605]]]], grad_fn=&lt;MeanBackward1&gt;)

可以看到，两种模式的输出差别很大，会不会影响性能呢？]]></description>
      <guid>https://stackoverflow.com/questions/78911068/resnet-inconsistency-between-train-and-eval-mode</guid>
      <pubDate>Sun, 25 Aug 2024 11:07:12 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：形状'[30, 167, 512]'对于大小为 7695360 的输入无效</title>
      <link>https://stackoverflow.com/questions/78910546/runtimeerror-shape-30-167-512-is-invalid-for-input-of-size-7695360</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78910546/runtimeerror-shape-30-167-512-is-invalid-for-input-of-size-7695360</guid>
      <pubDate>Sun, 25 Aug 2024 06:06:35 GMT</pubDate>
    </item>
    <item>
      <title>在拆分之前仅对 tf.data.Dataset 进行一次混洗</title>
      <link>https://stackoverflow.com/questions/78910366/shuffle-a-tf-data-dataset-before-split-only-once</link>
      <description><![CDATA[我正在使用 tf.data.Dataset 来为我的模型的 model.fit() 方法提供数据。
整个数据集无法放入我的 RAM 中，因此我需要按批次加载它，因此我考虑使用数据集生成器 (tf.data.Dataset) 在训练期间加载它并预取批次。
我用来获取训练、验证和测试数据集的函数如下所示：
def get_all_datasets(batch_size=64):
path = &#39;a/path&#39;
img_folder = &#39;src&#39;
mask_folder = &#39;masks&#39;
im_path = os.path.join(path, img_folder)
mk_path = os.path.join(path, mask_folder)

path = &#39;another/path/&#39;
img_folder = &#39;src&#39;
mask_folder = &#39;masks&#39;
im_path2 = os.path.join(path, img_folder)
mk_path2 = os.path.join(path, mask_folder)

# 创建数据集
ds1 = create_dataset(im_path, mk_path, 256, 256)
ds2 = create_dataset(im_path2, mk_path2, 256, 256)
ds = ds1.concatenate(ds2)

# 在拆分之前对整个数据集进行一次打乱
ds = ds.shuffle(buffer_size=tf.data.experimental.cardinality(ds).numpy(),
reshuffle_each_iteration=True)

# 拆分为训练集、验证集和测试集
total_size = tf.data.experimental.cardinality(ds).numpy()
train_size = int(total_size * 0.7)
val_size = int(total_size * 0.15)
test_size = total_size - train_size - val_size

# 创建单独的数据集
train_dataset = ds.take(train_size)
remaining = ds.skip(train_size)
val_dataset = remaining.take(val_size)
test_dataset = remaining.skip(val_size)

# 缓存数据集以防止重新洗牌/重新加载
train_dataset = train_dataset.cache().shuffle(batch_size).batch(batch_size).p​​refetch(buffer_size=tf.data.AUTOTUNE)
val_dataset = val_dataset.cache().batch(batch_size).p​​refetch(buffer_size=tf.data.AUTOTUNE)
test_dataset = test_dataset.cache().batch(batch_size).p​​refetch(buffer_size=tf.data.AUTOTUNE)

return train_dataset, val_dataset, test_dataset

第一次洗牌是为了使训练、验证和测试中的“场景”比例相等。这种洗牌只能进行一次，以避免污染训练、验证和测试集。
我不知道如何避免每次都重新洗牌整个集合。也许这个数据集方法也不合适。

在训练期间，整个数据集被洗牌两次，一次用于训练，另一次用于验证。我看到 shuffle 缓冲区已填满整个数据集基数。
我还考虑过在 shuffle 一次后创建三个文件夹（train、val 和 test）以获取拆分，然后仅使用 shuffle 方法对训练数据进行 shuffle，但如果这是解决问题的最简单方法，我会感到惊讶...
我尝试在 shuffle 之前设置一个 if 语句来检查 shuffle 是否已完成，但 shuffle 仍然发生。我的想法是执行图已经“安排好”，并且在惰性解释期间将全局变量设置为 False。]]></description>
      <guid>https://stackoverflow.com/questions/78910366/shuffle-a-tf-data-dataset-before-split-only-once</guid>
      <pubDate>Sun, 25 Aug 2024 03:01:31 GMT</pubDate>
    </item>
    <item>
      <title>模型vgg16的维度问题（图像分割）</title>
      <link>https://stackoverflow.com/questions/78910294/dimension-problem-with-model-vgg16-image-segmentation</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78910294/dimension-problem-with-model-vgg16-image-segmentation</guid>
      <pubDate>Sun, 25 Aug 2024 01:39:57 GMT</pubDate>
    </item>
    <item>
      <title>GAN 中的损失函数</title>
      <link>https://stackoverflow.com/questions/49988496/loss-functions-in-gans</link>
      <description><![CDATA[我正在尝试构建一个简单的 mnist GAN，不用多说，它没有成功。我搜索了很多，修复了大部分代码。虽然我真的不明白损失函数是如何工作的。
这是我所做的：
loss_d = -tf.reduce_mean(tf.log(discriminator(real_data))) # 最大化
loss_g = -tf.reduce_mean(tf.log(discriminator(generator(noise_input), trainable = False))) # 最大化，因为 d(g) 而不是 1 - d(g)
loss = loss_d + loss_g

train_d = tf.train.AdamOptimizer(learning_rate).minimize(loss_d)
train_g = tf.train.AdamOptimizer(learning_rate).minimize(loss_g)

我的损失值为 -0.0。你能解释一下如何处理 GAN 中的损失函数吗？]]></description>
      <guid>https://stackoverflow.com/questions/49988496/loss-functions-in-gans</guid>
      <pubDate>Mon, 23 Apr 2018 19:21:53 GMT</pubDate>
    </item>
    <item>
      <title>（MNIST - GAN）鉴别器和生成器误差在第一次迭代后降至接近零</title>
      <link>https://stackoverflow.com/questions/46731089/mnist-gan-discriminator-and-generator-error-dropping-close-to-zero-after-fir</link>
      <description><![CDATA[为了深入了解生成对抗网络，我尝试基于此斯坦福大学作业使用 tensorflow 自己为 MNIST 数据集实现 GAN。
我仔细检查并研究了给定练习的解决方案，并通过了测试。但是，我的生成器只会产生噪音。
我很确定我正确使用了辅助函数，所有测试都通过了，并且我在网上找到了显示完全相同实现的参考资料。因此，可能出错的地方只是鉴别器和生成器架构：
def discriminator(x):
with tf.variable_scope(&quot;discriminator&quot;):
l_1 = leaky_relu(tf.layers.dense(x, 256,activation=None))
l_2 = leaky_relu(tf.layers.dense(l_1, 256,activation=None))
logits = tf.layers.dense(l_2, 1,activation=None)
return logits

def generator(z):
with tf.variable_scope(&quot;generator&quot;):
l_1 = tf.maximum(tf.layers.dense(z, 1024,activation=None), 0)
l_2 = tf.maximum(tf.layers.dense(l_1, 1024,activation=None), 0)
img = tf.tanh(tf.layers.dense(l_2, 784,activation=None))
return img

我发现生成器和鉴别器错误在第一次迭代中下降到接近零。
Iter: 0, D: 1.026, G:0.6514
Iter: 50, D: 2.721e-05, G:5.066e-06
Iter: 100, D: 1.099e-05, G:3.084e-06
Iter: 150, D: 7.546e-06, G:1.946e-06
Iter: 200, D: 3.386e-06, G：1.226e-06
...

如果学习率较低，例如 1e-7，鉴别器和生成器的错误率会缓慢衰减，但最终会降至零，只会产生噪音。
Iter：0，D：1.722，G：0.6772
Iter：50，D：1.704，G：0.665
Iter：100，D：1.698，G：0.661
Iter：150，D：1.663，G：0.6594
Iter：200，D：1.661，G：0.6574
...

我已启动并运行 TensorFlow 图进行实验，但到目前为止未能从中解读出任何有意义的东西。
如果您有任何建议或可以推荐一种调试技术，我将非常乐意听取。
根据要求，这是我的 GAN - Loss 代码：
def gan_loss(logits_real, logits_fake):
labels_real = tf.ones_like(logits_real)
labels_fake = tf.zeros_like(logits_fake)

d_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_real, labels=labels_real)
d_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_fake, labels=labels_fake)
D_loss = tf.reduce_mean(d_loss_real + d_loss_fake)

G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_fake, labels=labels_fake))
返回 D_loss, G_loss
]]></description>
      <guid>https://stackoverflow.com/questions/46731089/mnist-gan-discriminator-and-generator-error-dropping-close-to-zero-after-fir</guid>
      <pubDate>Fri, 13 Oct 2017 13:26:59 GMT</pubDate>
    </item>
    <item>
      <title>在生成对抗网络中，如何使用鉴别器的输出来训练生成器</title>
      <link>https://stackoverflow.com/questions/44728913/how-the-generator-is-trained-with-the-output-of-discriminator-in-generative-adve</link>
      <description><![CDATA[最近我了解了生成对抗网络。
对于生成器的训练，我有点困惑它是如何学习的。 这里是 GAN 的实现：
`# 训练生成器
z = Variable(xp.random.uniform(-1, 1, (batchsize, nz), dtype=np.float32))
x = gen(z)
yl = dis(x)
L_gen = F.softmax_cross_entropy(yl, Variable(xp.zeros(batchsize, dtype=np.int32)))
L_dis = F.softmax_cross_entropy(yl, Variable(xp.ones(batchsize, dtype=np.int32)))

# 训练鉴别器

x2 = Variable(cuda.to_gpu(x2))
yl2 = dis(x2)
L_dis += F.softmax_cross_entropy(yl2, Variable(xp.zeros(batchsize, dtype=np.int32)))

#print &quot;forward done&quot;

o_gen.zero_grads()
L_gen.backward()
o_gen.update()

o_dis.zero_grads()
L_dis.backward()
o_dis.update()`

因此，正如论文中提到的那样，它会为生成器计算损失。
但是，它会根据判别器输出调用生成器后向函数。判别器输出只是一个数字（而不是数组）。 
但我们知道，一般来说，为了训练网络，我们会计算最后一层的损失函数（最后一层输出与实际输出之间的损失），然后计算梯度。例如，如果输出是 64*64，那么我们将其与 64*64 图像进行比较，然后计算损失并进行反向传播。
但是，在我看到的生成对抗网络中的代码中，我看到他们从鉴别器输出（只是一个数字）计算生成器的损失，然后他们调用生成器的反向传播。生成器的最后一层例如是 64*64 像素，但鉴别器损失是 1*1（这与通常的网络不同）所以我不明白它是如何导致生成器被学习和训练的？
我认为如果我们连接两个网络（连接生成器和鉴别器），然后调用反向传播但只更新生成器参数，这是有意义的，它应该可以工作。但我在代码中看到的内容完全不同。
所以我想问这怎么可能？
谢谢 ]]></description>
      <guid>https://stackoverflow.com/questions/44728913/how-the-generator-is-trained-with-the-output-of-discriminator-in-generative-adve</guid>
      <pubDate>Fri, 23 Jun 2017 19:45:46 GMT</pubDate>
    </item>
    <item>
      <title>使用训练有素的字符级 LSTM 模型生成文本</title>
      <link>https://stackoverflow.com/questions/43391452/generate-text-with-a-trained-character-level-lstm-model</link>
      <description><![CDATA[我训练了一个模型，目的是生成句子，如下所示：
我输入 2 个序列作为训练示例：x 是字符序列，y 是相同的移位 1 的序列。该模型基于 LSTM，使用 tensorflow 创建。
我的问题是：由于模型接受一定大小的输入序列（在我的情况下为 50），我如何仅给它单个字符作为种子进行预测？我在一些例子中看到，经过训练后，它们只需输入单个字符即可生成句子。 
这是我的代码：
 使用 tf.name_scope(&#39;input&#39;)：
x = tf.placeholder(tf.float32, [batch_size, truncated_backprop], name=&#39;x&#39;)
y = tf.placeholder(tf.int32, [batch_size, truncated_backprop], name=&#39;y&#39;)

使用 tf.name_scope(&#39;weights&#39;)：
W = tf.Variable(np.random.rand(n_hidden, num_classes), dtype=tf.float32)
b = tf.Variable(np.random.rand(1, num_classes), dtype=tf.float32)

input_series = tf.split(x, truncated_backprop, 1)
labels_series = tf.unstack(y, axis=1)

使用 tf.name_scope(&#39;LSTM&#39;)：
cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)
cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)
cell = tf.contrib.rnn.MultiRNNCell([cell] * n_layers)

states_series, current_state = tf.contrib.rnn.static_rnn(cell, input_series, \
dtype=tf.float32)

logits_series = [tf.matmul(state, W) + b for state in states_series]
prediction_series = [tf.nn.softmax(logits) for logits in logits_series]

损失 = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) \
for logits, labels, in zip(logits_series, labels_series)]
total_loss = tf.reduce_mean(losses)

train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)
]]></description>
      <guid>https://stackoverflow.com/questions/43391452/generate-text-with-a-trained-character-level-lstm-model</guid>
      <pubDate>Thu, 13 Apr 2017 11:45:57 GMT</pubDate>
    </item>
    <item>
      <title>使用生成模型还是判别模型进行分类？</title>
      <link>https://stackoverflow.com/questions/43197513/use-generative-or-discriminative-model-for-classification</link>
      <description><![CDATA[我是机器学习的初学者！只是想了解一下我应该如何处理分类问题。鉴于手头的问题是分类一个对象是属于 A 类还是 B 类，我想知道我应该使用生成模型还是判别模型。我有两个问题。

判别模型似乎在分类问题上做得更好，因为它只关心如何绘制决策边界，而不关心其他事情。

问：但是，如果数据集很小，只有大约 80 个 A 类对象和不到 10 个 B 类对象需要训练和测试，判别模型会过度拟合，因此生成模型会表现更好吗？

此外，由于 A 类对象和 B 类对象的数量差异非常大，训练的模型很可能只能识别 A 类对象。即使模型将所有对象归类为 A 类，这仍然会产生非常高的准确度得分。 

问：鉴于没有其他方法可以增加 B 类数据集的大小，有什么想法可以减少这种偏见？ ]]></description>
      <guid>https://stackoverflow.com/questions/43197513/use-generative-or-discriminative-model-for-classification</guid>
      <pubDate>Tue, 04 Apr 2017 02:58:41 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 中的步骤和时期有什么区别？</title>
      <link>https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow</link>
      <description><![CDATA[在大多数模型中，都有一个 steps 参数，表示在数据上运行的步骤数。但我在大多数实际使用中看到，我们还会执行拟合函数 N epochs。
用 1 个 epoch 运行 1000 步和用 10 个 epoch 运行 100 步有什么区别？在实践中哪一个更好？连续 epoch 之间有任何逻辑变化吗？数据混洗？]]></description>
      <guid>https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow</guid>
      <pubDate>Tue, 12 Jul 2016 23:20:22 GMT</pubDate>
    </item>
    <item>
      <title>如何在 sklearn GMM 混合模型中处理分类数据</title>
      <link>https://stackoverflow.com/questions/30984019/how-handle-categorical-data-in-sklearn-gmm-mixture-model</link>
      <description><![CDATA[有没有办法在 sklearn GMM 模块中输入分类观察值？
我的数据看起来有点像：
User,Siet_category,user_segment

UserA,Sports:News,efk-457
UserB,Music:Entertainment,asl-567
UserC,Sports:News,asl-567
UserD,Sports:News,efk-457

user_segment 是我的数据集中的类（大约有 10 个类）。
我认为这是 10 种不同分布的混合。
我想要做的是给出一个测试用户和站点类别，我想知道该测试用例属于哪个类/分布。
我知道我可以选择判别模型，但我想看看生成模型在这种情况下的表现。]]></description>
      <guid>https://stackoverflow.com/questions/30984019/how-handle-categorical-data-in-sklearn-gmm-mixture-model</guid>
      <pubDate>Mon, 22 Jun 2015 15:31:32 GMT</pubDate>
    </item>
    <item>
      <title>生成、判别和参数、非参数算法/模型之间的区别</title>
      <link>https://stackoverflow.com/questions/23821521/difference-between-generative-discriminating-and-parametric-nonparametric-algo</link>
      <description><![CDATA[在SO中，我找到了以下关于生成算法和判别算法的解释：
“生成算法模拟数据的生成方式，以便对信号进行分类。它提出了一个问题：根据我的生成假设，哪个类别最有可能生成此信号？
判别算法不关心数据是如何生成的，它只是对给定信号进行分类。”
并且这里是参数和非参数的定义算法
“参数化：数据来自特定形式的概率分布，直至未知参数。
非参数化：数据来自某个未指定的概率分布。
”
那么从本质上讲，我们可以说生成算法和参数化算法假设了底层模型，而判别算法和非参数算法不假设任何模型吗？
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/23821521/difference-between-generative-discriminating-and-parametric-nonparametric-algo</guid>
      <pubDate>Fri, 23 May 2014 05:25:57 GMT</pubDate>
    </item>
    </channel>
</rss>