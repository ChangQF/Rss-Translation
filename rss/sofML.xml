<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 23 Aug 2024 21:14:28 GMT</lastBuildDate>
    <item>
      <title>AWS SageMaker 预测和测试数据</title>
      <link>https://stackoverflow.com/questions/78907538/aws-sagemaker-predictions-and-test-data</link>
      <description><![CDATA[import pandas as pd
import itertools
import numpy as np
import s3fs
from sagemaker.predictor import Predictor
from sagemaker.serializers import CSVSerializer

# 为您的 CSV 文件定义 S3 路径
import pandas as pd
import s3fs

# 为您的 CSV 文件定义 S3 路径
s3_path = &quot;s3://{}/{}/{}.csv&quot;

# 带有附加检查的读取文件函数
def read_and_check_csv(s3_path):
fs = s3fs.S3FileSystem()
with fs.open(s3_path) as f:
try:
# 尝试读取 CSV 文件
df = pd.read_csv(f, header=None, low_memory=False)
# 检查行长度是否一致
if not df.apply(lambda x: len(x.dropna()), axis=1).nunique() == 1:
raise ValueError(&quot;检测到不一致的行长度&quot;)
print(&quot;文件读取成功，似乎为 CSV 格式。&quot;)
return df
except Exception as e:
print(f&quot;无法读取 CSV 文件：{e}&quot;)
return None

# 读取并检查 CSV 文件
df = read_and_check_csv(s3_path)

如果 df 不为 None:
print(df.head())
否则:
print(&quot;文件无法读取或不是有效的 CSV 格式。&quot;)

# 定义用于切片数据的索引
a = [50 * i for i in range(3)]
b = [40 + i for i in range(10)]
indices = [i + j for i, j in itertools.product(a, b)]

# 准备测试数据
test_data = shape.iloc[indices[:-1]]
test_X = test_data.iloc[:, 1:]

# 确保所有行的列数相同
min_cols = test_X.shape[1]
test_X = test_X.dropna(axis=1, how=&#39;all&#39;) # 删除所有 NaN 值的列

# 验证没有具有不同值的行长度
test_X = test_X.apply(lambda x: x.dropna().reset_index(drop=True), axis=1)

# 使用 SageMaker 端点名称初始化预测器
predictor = Predictor(endpoint_name=&#39;sagemaker-xgboost-2024-08-23-19-59-10-793&#39;)

# 确保预测器使用 CSV 序列化器
predictor.serializer = CSVSerializer()

# 将 DataFrame 转换为端点所需的格式
test_X_csv = test_X.to_csv(index=False, header=False, sep=&#39;,&#39;)

# 进行预测
try:
predictions = predictor.predict(test_X_csv)
# 打印预测
print(predictions.decode(&#39;utf-8&#39;))
except Exception as e:
print(f&quot;Error making预测：{e}&quot;)


我在 aws sagemaker Jupyter 实验室中为我的 xgboost 框架使用上述脚本。在此脚本之前，我正在运行以下代码来设置端点。
predictor = estimator.deploy(
initial_instance_count=1, instance_type=&quot;ml.m5.2xlarge&quot;
)

我添加了一些错误处理，这就是我了解到我的测试文件似乎没有被正确读取的地方。我得到的实际错误是：
无法读取 CSV 文件：检测到不一致的行长度
无法读取文件或文件不是有效的 CSV 格式。
进行预测时出错：调用 InvokeEndpoint 操作时发生错误 (ModelError)：从主服务器收到客户端错误 (415)，消息为“加载 csv 数据失败，出现异常，请确保数据为 csv 格式：
&lt;class &#39;ValueError&#39;&gt;
设置带有序列的数组元素。请求的数组在 1 维之后具有非均匀形状。检测到的形状为 (29,) + 非均匀部分。”

关于如何修复此问题有任何见解吗？]]></description>
      <guid>https://stackoverflow.com/questions/78907538/aws-sagemaker-predictions-and-test-data</guid>
      <pubDate>Fri, 23 Aug 2024 20:58:39 GMT</pubDate>
    </item>
    <item>
      <title>关于机器学习方法和工具的问题</title>
      <link>https://stackoverflow.com/questions/78907467/questions-about-ml-methods-and-tooling</link>
      <description><![CDATA[这是一个关于机器学习领域的一般性问题。希望这是提出这个问题的正确地方。
我选修了一门非参数计量经济学课程，并喜欢学习核估计，因为它是传统参数回归的更准确和科学的版本。我的教授是非参数的铁杆信徒，嘲笑数据科学行业使用随机森林之类的东西，称其为前沿技术。我有点相信他，但有人能帮我理解人们是如何（或为什么不）使用核估计的吗？到目前为止，我注意到 Featuretools 似乎在使用内核估计和带宽选择缩小特征范围方面有类似的方法。
学术界从事类似工作的人员（计量经济学家和计算机科学家/机器学习研究人员）之间似乎也存在巨大差距，这也让我感到困惑。
更重要的是，我对此很陌生（数学和经济学本科，经济学硕士，过去 4 年分析师/数据工程师，目前失业），并且正在尝试寻找可以利用我现有知识的有趣项目，因此建议/见解很有帮助。提前谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78907467/questions-about-ml-methods-and-tooling</guid>
      <pubDate>Fri, 23 Aug 2024 20:24:29 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助验证谷歌/机器学习课程提供的数据</title>
      <link>https://stackoverflow.com/questions/78907434/need-help-verifying-data-provided-by-google-machine-learning-course</link>
      <description><![CDATA[在 Google 提供的机器学习课程简介中的梯度下降页面中，提供了特征和相应的标签、MSE 损失函数、初始数据集和结果。我在验证他们的结果时遇到了困难，我想知道是否有人可以帮助确认是我犯了错误还是他们犯了错误。非常感谢任何帮助或反馈。
谢谢！
我有以下内容：
import pandas as pd
import numpy as np

data = [3.5, 18], [3.69, 15], [3.44, 18], [3.43, 16], [4.34, 15], [4.42, 14], [2.37, 24]
initial_data_df = pd.DataFrame(data,columns=[&#39;pounds&#39;,&#39;mpg&#39;])

number_of_iterations = 6
weight = 0 # 初始化权重
bias = 0 # 初始化权重
weight_slope = 0
bias_slope = 0
final_results_df = pd.DataFrame()
learning_rate = 0.01

对于 i 在范围内（迭代次数）：
损失 = 计算损失（初始数据 df、权重、偏差）
最终结果 df = 更新结果（最终结果 df、权重、偏差、损失）
权重斜率 = 查找权重斜率（初始数据 df、权重、偏差）
偏差斜率 = 查找偏差斜率（初始数据 df、权重、偏差）
权重 = 新权重更新（权重、学习率、权重斜率）
偏差 = 新偏差更新（偏差、学习率、偏差斜率）
打印（最终结果 df）

def 计算损失（df、权重、偏差）：
损失总和 = []
对于 i 在范围内（0、长度（df））：
loss_summation.append((df[&#39;mpg&#39;][i]-((weight*df[&#39;pounds&#39;][i])+bias))**2)
return (sum(loss_summation)//len(df))

def update_results(df,weight,bias,loss):
if df.empty:
df = pd.DataFrame([[weight,bias,loss]],columns=[&#39;weight&#39;,&#39;bias&#39;,&#39;loss&#39;])
else:
df = pd.concat([df,pd.DataFrame([[weight,bias,loss]],columns=df.columns)])
return df

def find_weight_slope(df,weight,bias):
weight_update_summation = []
for i in range(0,len(df)):
wx_plus_b = (weight*df[&#39;pounds&#39;][i])+bias
wx_plus_b_minus_y = wx_plus_b-df[&#39;mpg&#39;][i]
weight_update_summation.append(2*(wx_plus_b_minus_y*df[&#39;pounds&#39;][i]))
return sum(weight_update_summation)//len(df)

def find_bias_slope(df,weight,bias):
bias_update_summation = []
for i in range(0,len(df)):
wx_plus_b = (weight*df[&#39;pounds&#39;][i])+bias
wx_plus_b_minus_y = wx_plus_b-df[&#39;mpg&#39;][i]
bias_update_summation.append(2*wx_plus_b_minus_y)
total_sum = sum(bias_update_summation)
return total_sum//len(df)

def new_weight_update(old_weight,lr,slope):
return old_weight-1*lr*slope

def new_bias_update(old_bias,lr,slope):
return old_bias-1*lr*slope

得出：
iter weight bias loss
0 0.00 0.00 303.0
0 1.20 0.35 170.0
0 2.06 0.60 102.0
0 2.67 0.79 67.0
0 3.10 0.93 50.0
0 3.41 1.04 41.0

这与提供的解决方案不同在网站上：
迭代权重偏差损失（MSE）
1 0 0 303.71
2 1.2 0.34 170.67
3 2.75 0.59 67.3
4 3.17 0.72 50.63
5 3.47 0.82 42.1
6 3.68 0.9 37.74
]]></description>
      <guid>https://stackoverflow.com/questions/78907434/need-help-verifying-data-provided-by-google-machine-learning-course</guid>
      <pubDate>Fri, 23 Aug 2024 20:09:14 GMT</pubDate>
    </item>
    <item>
      <title>具有规范化数据的 Tensorflow 模型</title>
      <link>https://stackoverflow.com/questions/78907202/tensorflow-models-with-normalized-data</link>
      <description><![CDATA[我正在用 TensorFlow 学习 Python 机器学习的初级课程，其中一节课讲到，规范化 X_train 和 X_test 数据可以提高算法的性能。
具体来说，我们被告知要对数值特征使用 MinMaxScaler()，并在 X_train 上安装缩放器，然后转换 X_train 和 X_test。 Python 代码如下所示：
Scaler = MinMaxScaler()
Scaler.fit(X_train)

X_train_scaled = Scaler.transform(X_train)
X_test_scaled = Scaler.transform(X_test)

#将缩放数据集转换为张量
X_train_scaled = tf.convert_to_tensor(X_train_scaled)
X_test_scaled = tf.convert_to_tensor(X_test_scaled)

#创建模型
tf.random.set_seed(42)

model_3 = tf.keras.Sequential([
tf.keras.layers.Dense(601,name=&quot;input_layer1&quot;,activation=&quot;relu&quot;),
tf.keras.layers.Dense(1,name=&quot;output_layer&quot;)
], name=&quot;Third_model&quot;)

model_3.compile(loss=tf.keras.losses.mae,
optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
metrics=[&quot;mae&quot;])

#fit on scaled training data
model_3.fit(X_train_scaled, y_train, epochs=100,verbose=1)

#predict on scaled test data
y_pred_3 = model_3.predict(X_test_scaled)
y_pred_3_int = tf.cast(y_pred_3, tf.int64)

当模型呈现出从未见过的特征时，它是如何工作的（X_newfeatures）？在转换 X_newfeatures 集之前，没有训练数据来适应缩放器。
我必须在 X_newfeatures 上适应缩放器吗？然后在缩放的 X_newfeatures_scaled 上进行预测？类似于...
Scaler = MinMaxScaler()

#适合什么？？？
Scaler.fit(X_newfeatures) 

X_newfeatures_scaled = Scaler.transform(X_newfeatures )
X_newfeatures_scaled = tf.convert_to_tensor(X_newfeatures_scaled)

#预测 X_newfeatures_scaled？
y_pred_newfeatures = loaded_model_3.predict(X_newfeatures_scaled)
y_pred_newfeatures = tf.cast(y_pred_newfeatures , tf.int64)

如果我运行上面的代码片段，我的模型总是预测 0，无论 X_newfeatures_scaled 集是什么样子（0 是有效标签之一，但不是唯一标签）。
如果我跳过 MinMaxScaler 的拟合部分，模型将返回绝对的垃圾。
我是否遗漏了什么或做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78907202/tensorflow-models-with-normalized-data</guid>
      <pubDate>Fri, 23 Aug 2024 18:35:01 GMT</pubDate>
    </item>
    <item>
      <title>如何加快我的 YOLOv8n 模型推理？</title>
      <link>https://stackoverflow.com/questions/78907188/how-can-i-speed-up-my-yolov8n-model-inference</link>
      <description><![CDATA[我已经用一些自定义数据训练了自己的 YoloV8n 模型，它实际上运行得非常好。但是，我的脚本中存在相当多的延迟。这是我的代码：
with mss.mss() as sct:
while True:
loop_start = time.time()

# 捕获屏幕
capture_start = time.time()
img = sct.grab((monitor[&quot;left&quot;], monitor[&quot;top&quot;], monitor[&quot;left&quot;] + monitor[&quot;width&quot;], monitor[&quot;top&quot;] + monitor[&quot;height&quot;]))
img = np.array(img)
img = cv.cvtColor(img, cv.COLOR_RGBA2RGB)
capture_end = time.time()

# 执行 YOLO 检测
detection_start = time.time()
results = model(img, verbose=False) # 此行导致速度变慢
detection_end = time.time()

# 处理结果并绘制边界框
draw_start = time.time()
#img = draw_boxes(results, img)
draw_end = time.time()

# FPS 计算

elapsed_time = time.time() - t0
avg_fps = (n_frames / elapsed_time)
print(&quot;平均 FPS: &quot; + str(avg_fps))
n_frames += 1

因此，使用 MSS 进行屏幕捕获时，我能够捕获大约 64fps（这非常好），但是当我使用模型推理时，fps 会下降到大约 32。我使用的是 gtx1080ti，运行此代码时，gpu 使用率仅为 20% 左右。32 fps 对我来说有点太慢了，45+ 左右就很完美了。我想知道我能做些什么，还是硬件有限，或者 Python 太慢了？我不介意尝试用 C++ 重做这个，但我对自己的 C++ 技能不太有信心。
到目前为止我尝试过的事情：
从源代码构建 opencv 并使用 gpu 加速 img = cv.cvtColor(img, cv.COLOR_RGBA2RGB)，但我没有成功，我不认为瓶颈在那里。现在我降到了最小的 yolov8 模型，也许我可以试试 yoloNAS？我已经完成了其余函数 o(1) 时间操作，甚至在代码的不同部分添加了线程，用于我的鼠标需要执行的一些操作。]]></description>
      <guid>https://stackoverflow.com/questions/78907188/how-can-i-speed-up-my-yolov8n-model-inference</guid>
      <pubDate>Fri, 23 Aug 2024 18:30:11 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法在使用带有 GPU 的 Colab 时在 CPU 上运行特定单元？</title>
      <link>https://stackoverflow.com/questions/78907042/is-there-a-way-to-run-a-particular-cell-on-the-cpu-while-using-colab-with-a-gpu</link>
      <description><![CDATA[我正在使用 CoLab 运行一些机器学习模型。我正在使用 pip 安装库，使用 GPU 时，有些库需要很长时间才能安装。有没有办法在 CPU 上运行一个特定的单元来加快速度？]]></description>
      <guid>https://stackoverflow.com/questions/78907042/is-there-a-way-to-run-a-particular-cell-on-the-cpu-while-using-colab-with-a-gpu</guid>
      <pubDate>Fri, 23 Aug 2024 17:41:39 GMT</pubDate>
    </item>
    <item>
      <title>Python Matplotlib ROC 图表</title>
      <link>https://stackoverflow.com/questions/78906819/python-matplotlib-roc-chart</link>
      <description><![CDATA[我正在使用机器学习模型，其中我已经通过对未见数据的测试生成了结果。我正在尝试使用 sklearn 创建 ROC 曲线，但对应该使用哪些列感到有些困惑。有人可以确认我是否做对了吗？我使用了两个类（0 = 真实和 1 = 假）。结果的准确率为 57%。0.61% 的 AUC 看起来差不多，但我是否使用了正确的列？
我已经阅读了 sklearn 网站上有关 roc_curve 函数的说明，但我不清楚。例如，y_score 参数提到目标分数应该是正类的概率估计。我需要对我的 CSV 进行子集化吗？如果需要，我应该如何最好地应用它？
下图是 CSV 文件结构的屏幕截图，其中包含测试结果。该模型将结果保存到 CSV 文件中，然后将其读回以创建 ROC 图表。

plt.rcParams.update({&#39;font.size&#39;: 7})
plt.subplot(2, 5, 4) 
fpr, tpr, 阈值 = metrics.roc_curve(data[&#39;Correct&#39;], data[&#39;P1 Probability&#39;], drop_intermediate=True, pos_label=1)
auc3 = metrics.roc_auc_score(data[&#39;Correct&#39;], data[&#39;P1 Probability&#39;])
plt.plot(fpr,tpr,label=&quot;data 1, auc=&quot; + str(auc3), marker=&#39;o&#39;, color = &quot;gray&quot;, linewidth=1)
plt.plot([0, 1], [0, 1], &#39;k--&#39;)
for xitem,yitem in np.nditer([fpr,tpr]):
etiqueta = &quot;({:.2f}, {:.2f})&quot;.format(xitem, yitem)
plt.annotate(etiqueta, (xitem,yitem), textcoords=&quot;offset pixels&quot;,xytext=(0,10),ha=&quot;center&quot;, color = &quot;b&quot;)
plt.legend(loc=4)
plt.xlabel(&#39;假阳性率&#39;)
plt.ylabel(&#39;真阳性率&#39;)
plt.title(&#39;ROC 曲线 - 阳性类别（假）&#39;)
plt.savefig(&#39;ROC&#39;,dpi=300)


plt.rcParams.update({&#39;font.size&#39;: 7})
plt.subplot(2, 5, 4) 
fpr, tpr, 阈值 = metrics.roc_curve(data[&#39;Actual&#39;], data[&#39;P1 Probability&#39;], drop_intermediate=True, pos_label=1)
auc3 = metrics.roc_auc_score(data[&#39;Actual&#39;], data[&#39;P1概率&#39;])
plt.plot(fpr,tpr,label=&quot;data 1, auc=&quot; + str(auc3), marker=&#39;o&#39;, color = &quot;gray&quot;, linewidth=1)
plt.plot([0, 1], [0, 1], &#39;k--&#39;)
for xitem,yitem in np.nditer([fpr,tpr]):
etiqueta = &quot;({:.2f}, {:.2f})&quot;.format(xitem, yitem)
plt.annotate(etiqueta, (xitem,yitem), textcoords=&quot;offset pixels&quot;,xytext=(0,10),ha=&quot;center&quot;, color = &quot;b&quot;)
plt.legend(loc=4)
plt.xlabel(&#39;假阳性率&#39;)
plt.ylabel(&#39;真阳性率&#39;)
plt.title(&#39;ROC 曲线 - 阳性类别（假）&#39;)
plt.savefig(&#39;ROC&#39;,dpi=300)

]]></description>
      <guid>https://stackoverflow.com/questions/78906819/python-matplotlib-roc-chart</guid>
      <pubDate>Fri, 23 Aug 2024 16:35:27 GMT</pubDate>
    </item>
    <item>
      <title>如何在 glue mrpc 基准上检查 GTP 模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/78906413/how-is-the-gtp-model-checked-on-the-glue-mrpc-benchmark</link>
      <description><![CDATA[GPT 建立在 Transformer 解码器上，这意味着模型以自回归的方式处理数据序列，根据前一个单词预测下一个单词。它只使用 Transformer 的解码器部分，与也使用编码器的模型不同。微软研究释义语料库 (MRPC) 是一个基准数据集，广泛用于自然语言处理 (NLP) 任务，特别是用于评估释义识别模型。释义识别是确定两个句子是否具有相同含义的任务。
示例：
| idx | 标签 | 句子 1 | 句子 2 |
|-----|-------|----------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|
| 0 | 0 | 相同的探测车将充当机器人地质学家，寻找过去水的证据。| 探测车充当机器人地质学家，在六个轮子上移动。|
| 1 | 0 |在 OfficeMax 收购完成后，博伊西的销售额中不到 20% 将来自木材和纸张生产。| 在 OfficeMax 收购完成后，假设这些业务未出售，博伊西的销售额中不到 20% 将来自木材和纸张生产。|

但是，蓝色基准被准备为 2 个序列，并检查它们是否相似。
如何检查胶水上的 GTP 性能？您是否必须以某种方式解开最后一层 GPT？您是否需要修改 GPT 架构？]]></description>
      <guid>https://stackoverflow.com/questions/78906413/how-is-the-gtp-model-checked-on-the-glue-mrpc-benchmark</guid>
      <pubDate>Fri, 23 Aug 2024 14:35:39 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 ResourceExhaustedError：图形执行错误？</title>
      <link>https://stackoverflow.com/questions/78906162/how-to-resolve-resourceexhaustederror-graph-execution-error</link>
      <description><![CDATA[我在对新数据进行预测时遇到错误
model.predict(X_val)

错误内容：
ResourceExhaustedError

ResourceExhaustedError：图形执行错误：

我正在使用 tensorflow，并使用了诸如减少批量大小、使用数据生成器等方法，但未能解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/78906162/how-to-resolve-resourceexhaustederror-graph-execution-error</guid>
      <pubDate>Fri, 23 Aug 2024 13:29:34 GMT</pubDate>
    </item>
    <item>
      <title>KNN 归纳模型对所有缺失值单元格返回“nan”值，而不是分类列中的类别</title>
      <link>https://stackoverflow.com/questions/78905500/knn-imputation-model-returns-nan-value-for-all-missing-value-cells-instead-of</link>
      <description><![CDATA[在数据预处理步骤中，我使用 KNN 插补模型来填充分类列中缺失的值。但是，我的代码在所有缺失值单元格中都插入值“nan”，而不是非缺失值单元格中的其他类别。
数据源文件：https://drive.google.com/file/d/1GvwzPrDfqvkg0l7yLz6_gYv2aLSLbYtR/view?usp=sharing
这是我的代码：
import pandas as pd
from sklearn.impute import KNNImputer
from sklearn.preprocessing import LabelEncoder

# 加载数据集
df_train = pd.read_csv(r&quot;F:\OneDrive - CTY CP DP Pharmacity\Documents\Minh 2024\Kaggle\House_Prices\train.csv&quot;)

# 包含缺失值的列列表，用于估算
columns_with_missing = [&#39;BsmtQual&#39;, &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;, &#39;BsmtFinType1&#39;]

# 用于编码和估算缺失值的函数
def encode_and_impute(df, columns):
label_encoders = {}
df_encoded = df.copy()

# 对分类列进行编码
for col in columns:
le = LabelEncoder()
df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))
label_encoders[col] = le

# 应用 KNN imputer
knn_imputer = KNNImputer(n_neighbors=5)
df_imputed = pd.DataFrame(knn_imputer.fit_transform(df_encoded[columns]), columns=columns)

# 将插补的列解码回原始类别
for col in columns:
df_imputed[col] = label_encoders[col].inverse_transform(df_imputed[col].astype(int))

return df_imputed

# 插补缺失值并更新原始 DataFrame
df_train[columns_with_missing] = encode_and_impute(df_train, columns_with_missing)

# 显示结果以进行检查
df_check = df_train[columns_with_missing]
print(df_check[df_check[&#39;BsmtQual&#39;]== &#39;nan&#39;].head(10))  当前输出：  BsmtQual BsmtCond BsmtExposure BsmtFinType1 17 nan nan nan 39 nan nan nan 90 nan nan nan 102 nan nan nan nan 156 nan nan nan nan 182 nan nan nan 259 nan nan nan nan 342 nan nan nan nan 362 nan nan nan nan 371 nan nan nan nan  预期输出： 所有“nan”值将与相应列中的其他类别一起]]></description>
      <guid>https://stackoverflow.com/questions/78905500/knn-imputation-model-returns-nan-value-for-all-missing-value-cells-instead-of</guid>
      <pubDate>Fri, 23 Aug 2024 10:41:32 GMT</pubDate>
    </item>
    <item>
      <title>根据 scikit-learn ColumnTransformer 访问用于归纳和规范化新数据的值</title>
      <link>https://stackoverflow.com/questions/78896943/accessing-the-values-used-to-impute-and-normalize-new-data-based-upon-scikit-lea</link>
      <description><![CDATA[使用 scikit-learn，我在训练集上构建机器学习模型，然后在测试集上对其进行评估。在训练集上，我使用 ColumnTransformer 执行数据插补和缩放，然后使用 Kfold CV 构建逻辑回归模型，最终模型用于预测测试集上的值。最终模型还使用其来自 ColumnTransformer 的结果来插补测试集上的缺失值。例如，最小-最大标量将从训练集中获取最小值和最大值，并在缩放测试集时使用这些值。我如何才能看到这些从训练集中得出然后用于预测测试集的缩放值？我在 scikit-learn 文档中找不到有关它的任何信息。以下是我使用的代码：
来自 sklearn.linear_model 导入 SGDClassifier
来自 sklearn.model_selection 导入 RepeatedStratifiedKFold
来自 sklearn.model_selection 导入 GridSearchCV
来自 sklearn.compose 导入 ColumnTransformer
来自 sklearn.impute 导入 SimpleImputer
来自 sklearn.pipeline 导入 Pipeline
来自 sklearn.preprocessing 导入 MinMaxScaler、OneHotEncoder

def preprocessClassifierLR(categorical_vars, numeric_vars):###categorical_vars 和 numeric_vars 是定义 X 中存在的分类和数字变量的列名的列表

categorical_pipeline = Pipeline(steps=[(&#39;mode&#39;, SimpleImputer(missing_values=np.nan, strategies=&quot;most_frequent&quot;)),
(&quot;one_hot_encode&quot;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))])

numeric_pipeline = Pipeline(steps=[(&#39;numeric&#39;, SimpleImputer(strategy=&quot;median&quot;)),
(&quot;scaling&quot;, MinMaxScaler())])

col_transform = ColumnTransformer(transformers=[(&quot;cats&quot;, categorical_pipeline, categorical_vars),
(&quot;nums&quot;, numeric_pipeline, numeric_vars)])

lr = SGDClassifier(loss=&#39;log_loss&#39;, penalty=&#39;elasticnet&#39;)
model_pipeline = Pipeline(steps=[(&#39;preprocess&#39;, col_transform),
(&#39;classifier&#39;, lr)])

random_grid_lr = {&#39;classifier__alpha&#39;: [1e-1, 0.2, 0.5],
&#39;classifier__l1_ratio&#39;: [1e-3, 0.5]}

kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=47)

param_search = GridSearchCV(model_pipeline, random_grid_lr,scoring=&#39;roc_auc&#39;, cv=kfold, refit=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

param_search = preprocessClassifierLR(categorical_vars, numeric_vars)
train_mod = param_search.fit(X_train, y_train)
print(&quot;Mod AUC:&quot;, train_mod.best_score_)

test_preds = train_mod.predict_proba(X_)[:,1]

我无法提供真实数据，但 X 是一个包含独立变量的数据框，y 是二元结果变量。train_mod 是一个包含列变换器和 SGD 分类器步骤的管道。我可以通过运行 train_mod.best_params_ 轻松从分类器中获取类似的参数信息，例如最佳 lambda 和 alpha 值，但我无法找出用于列变换器的统计数据，例如 1) 用于分类特征的简单插补器的模式，2) 用于数字特征的简单插补器的中值，以及 3) 用于缩放数字特征的最小值和最大值。如何访问此信息？
我假设 train_mod.best_estimator_[&#39;preprocess&#39;].transformers_ 包含此信息，类似于 train_mod.best_params_ 为我提供从模型训练中得出的 alpha 和 lambda 值，然后将其应用于测试集。]]></description>
      <guid>https://stackoverflow.com/questions/78896943/accessing-the-values-used-to-impute-and-normalize-new-data-based-upon-scikit-lea</guid>
      <pubDate>Wed, 21 Aug 2024 12:24:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hugging Face Transformers 训练 GPT-2 模型时如何修复分段错误？</title>
      <link>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</guid>
      <pubDate>Tue, 06 Aug 2024 21:47:06 GMT</pubDate>
    </item>
    <item>
      <title>如何使用pandas将大数据块拆分为x_train和y_train数据以供机器学习？</title>
      <link>https://stackoverflow.com/questions/67367698/how-to-use-pandas-chunk-for-large-data-into-split-the-data-for-x-train-and-y-tra</link>
      <description><![CDATA[df_chunk=pd.read_csv(filename,chunk=1000)
X_train,Y_train,X_test,Y_test=train_test.split(df_chunk)

如何使用 df_chunk 将其拆分为 x 和 y 训练数据？]]></description>
      <guid>https://stackoverflow.com/questions/67367698/how-to-use-pandas-chunk-for-large-data-into-split-the-data-for-x-train-and-y-tra</guid>
      <pubDate>Mon, 03 May 2021 10:51:38 GMT</pubDate>
    </item>
    <item>
      <title>MLflow：如何将实验状态返回为失败</title>
      <link>https://stackoverflow.com/questions/61112113/mlflow-how-to-return-experiment-status-as-failed</link>
      <description><![CDATA[我已经写出了我的代码，它具有以下形式 
def train(run_name, log_basepath, logger, parameters):

try:
metrics = training_functions(parameters) # &lt;---- 可能失败的代码

# 演示将属性加载到给定的运行
with mlflow.start_run(run_name=run_name):

# 日志参数
mlflow.log_params(parameters)

# 日志指标
mlflow.log_metrics(metrics)

# 定义实验标签
mlflow.set_tags(tags)

# 上传相关文件
mlflow.log_artifact(artifact_abspath)

# 主模型文件
mlflow.log_artifact(log_basepath)

except Exception as e:
logger.error(&#39;\n模型训练失败。返回以下错误:\n {} \n\n&#39;.format(e))
logger.info( &#39;第 {} 行错误&#39;.format(sys.exc_info()[-1].tb_lineno))

# 演示将属性加载到给定运行
使用 mlflow.start_run(run_name=run_name):
# 定义运行标签
logger.info(&#39;实验名称：{}&#39;.format(experiment_name))
logger.info(&#39;运行名称：{}&#39;.format(run_name))
logger.info(&#39;运行失败&#39;)

mlflow.log_artifact(log_basepath)

我的目标是如果发生故障，将日志发送到 mlflow 服务器。我的问题是，如何让 mlflow 将此标记为失败？]]></description>
      <guid>https://stackoverflow.com/questions/61112113/mlflow-how-to-return-experiment-status-as-failed</guid>
      <pubDate>Thu, 09 Apr 2020 00:28:12 GMT</pubDate>
    </item>
    <item>
      <title>如何确保训练阶段不会面临 OOM？</title>
      <link>https://stackoverflow.com/questions/58366819/how-to-make-sure-the-training-phase-wont-be-facing-an-oom</link>
      <description><![CDATA[根据我的经验，有两种 OOM 情况。一种是当您的模型和小批量所需的内存大于您拥有的内存时。在这种情况下，训练阶段将永远不会开始。解决这个问题的方法是使用较小的批量大小。尽管如果我能计算出我的硬件可以为某个特定模型管理的最大批量大小，那就太好了。但即使我第一次尝试找不到最大的批量大小，我也总能通过反复试验找到它（因为这个过程马上就失败了）。
我面临的 OOM 的第二种情况是当训练过程开始时，它会持续一段时间。甚至可能是几个时期。但后来由于某种未知原因，它面临 OOM。对我来说，这种情况令人沮丧。因为它可能随时发生，你永远不知道正在进行的训练是否会结束。到目前为止，我已经浪费了好几天的训练时间，而我以为一切都进展顺利。
我认为需要澄清一些问题。首先，我说的是带有 GPU 的个人计算机。其次，GPU 专用于计算，不用于显示。如果我错了，请纠正我，但我相信这意味着训练过程在不同时间点需要不同的内存大小。怎么会这样？再次，我如何确保我的训练阶段不会面临 OOM？
以这次运行为例：
3150/4073 [========================&gt;.......] - ETA：53:39 - 损失：0.3323
2019-10-13 21:41:13.096320：W tensorflow/core/common_runtime/bfc_allocator.cc:314] 分配器 (GPU_0_bfc) 在尝试分配 60.81MiB（四舍五入为 63766528）时内存不足。当前分配摘要如下。

经过三个小时的训练，TensorFlow 要求的内存超过了我的硬件可以提供的内存。我的问题是，为什么此时增加内存分配，而不是在进程开始时增加？
[更新]
鉴于 Eager 模式的已知问题，我将对我的案例进行一些说明。我没有在 Eager 模式下编码。我的训练代码如下所示：
strategy = tf.distribute.OneDeviceStrategy(device=&quot;/gpu:0&quot;)
training_dataset = tf.data.Dataset.from_tensor_slices(...)
validation_dataset = tf.data.Dataset.from_tensor_slices(...)

withstrategy.scope():
model = create_model()

model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;)

poc​​ket = EarlyStopping(monitor=&#39;val_loss&#39;, min_delta=0.001,
patience=5, verbose=1,
restore_best_weights = True)

history = model.fit(training_dataset.shuffle(buffer_size=1000).batch(30),
epochs=3,
回调=[pocket]，
validation_data=validation_dataset.shuffle(buffer_size=1000).batch(30)，
workers=3，use_multiprocessing=True)
]]></description>
      <guid>https://stackoverflow.com/questions/58366819/how-to-make-sure-the-training-phase-wont-be-facing-an-oom</guid>
      <pubDate>Sun, 13 Oct 2019 18:58:25 GMT</pubDate>
    </item>
    </channel>
</rss>