<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 26 Jun 2024 15:16:26 GMT</lastBuildDate>
    <item>
      <title>如何为 RNN 编码多个文本列</title>
      <link>https://stackoverflow.com/questions/78673211/how-to-encode-multiple-text-column-for-rnn</link>
      <description><![CDATA[我有多个文本特征，需要对其进行编码才能在 RNN 模型中使用。建议通过 TextVectorization 来处理训练文本
 A B C D Target
0 一些文本 A 一些文本 B 一些文本 C 一些文本 D 0
1 一些文本 A 一些文本 B 一些文本 C 一些文本 D 1
2 一些文本 A 一些文本 B 一些文本 C 一些文本 D 0
3 一些文本 A 一些文本 B 一些文本 C 一些文本 D 1
4 一些文本 A 一些文本 B 一些文本 C 一些文本 D 1

目前 train_dataset 包含一列和一个 Target 标签
VOCAB_SIZE = 1000
encoder = tf.keras.layers.TextVectorization(
max_tokens=VOCAB_SIZE)
encoder.adapt(train_dataset.map(lambda text, label: text))
]]></description>
      <guid>https://stackoverflow.com/questions/78673211/how-to-encode-multiple-text-column-for-rnn</guid>
      <pubDate>Wed, 26 Jun 2024 14:58:57 GMT</pubDate>
    </item>
    <item>
      <title>在 Google Colab 中重新连接</title>
      <link>https://stackoverflow.com/questions/78672618/reconnecting-in-google-colab</link>
      <description><![CDATA[我在 google colab 中使用 来自 python 3.8。安装转换器后，收到以下警告，几分钟后连接断开。
我该如何防止这种情况发生？
警告：此运行时中先前已导入以下包：[cycler、kiwisolver、six] 您必须重新启动运行时才能使用新安装的版本。重新启动将丢失所有运行时状态，包括局部变量。
]]></description>
      <guid>https://stackoverflow.com/questions/78672618/reconnecting-in-google-colab</guid>
      <pubDate>Wed, 26 Jun 2024 13:09:15 GMT</pubDate>
    </item>
    <item>
      <title>无法运行 xgboost 导入 XGBRegressor：Python3</title>
      <link>https://stackoverflow.com/questions/78672076/unable-to-run-xgboost-import-xgbregressor-python3</link>
      <description><![CDATA[当我运行以下代码时，出现错误：
来自 xgboost 导入 XGBRegressor

-------------------------------------------------------------------------- XGBoostError Traceback（最近一次调用
最后）Cell In[64]，第 19 行
17 来自 sklearn.metrics 导入 mean_squared_error
18 来自 sklearn 导入指标
---&gt; 19 来自 xgboost 导入 XGBRegressor 文件 /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/init.py:6
1 &quot;&quot;&quot;XGBoost：eXtreme Gradient Boosting 库。
2
3 贡献者：https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md
4 &quot;&quot;&quot;
----&gt; 6 来自 . import tracker # noqa
7 来自 . import collective, dask
8 来自 .core import (
9 Booster,

我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78672076/unable-to-run-xgboost-import-xgbregressor-python3</guid>
      <pubDate>Wed, 26 Jun 2024 11:17:02 GMT</pubDate>
    </item>
    <item>
      <title>选择 Hyperopt 管道参数的问题</title>
      <link>https://stackoverflow.com/questions/78672059/problems-with-selecting-hyperopt-pipeline-parameters</link>
      <description><![CDATA[我有一个代码，用于迭代模型本身和整个管道的超参数
preprocessor = ColumnTransformer(
[
(&#39;OneHotEncoder&#39;, OneHotEncoder(drop=&#39;if_binary&#39;, sparse_output=False), binary_cols),
(&#39;CatBoostEncoder&#39;, CatBoostEncoder(random_state=RANDOM_STATE), non_binary_cat_cols),
(&#39;StandardScaler&#39;, StandardScaler(), num_cols)
],
verbose_feature_names_out=False,
remainder=&#39;drop&#39;
)

pipe_final = ImbPipeline([
(&#39;preprocessor&#39;, preprocessor),
(&#39;target_imbalance&#39;, ADASYN()),
(&#39;selection&#39;, PCA()),
(&#39;models&#39;, CatBoostClassifier(random_state=RANDOM_STATE))
])

# CatBoostClassifier 的参数
param_grid = {
&#39;models__iterations&#39;: [1000, 2000, 3000],
&#39;models__class_weights&#39;: [&#39;Balanced&#39;, None],
&#39;target_imbalance&#39;: [ADASYN(random_state=RANDOM_STATE), SMOTETomek(random_state=RANDOM_STATE),
SMOTE(random_state=RANDOM_STATE, k_neighbors=10), &#39;passthrough&#39;],
&#39;preprocessor__StandardScaler&#39;: [StandardScaler(), RobustScaler(), MinMaxScaler(),
PowerTransformer(), QuantileTransformer(),
Normalizer()，PolynomialFeatures（degree=2，include_bias=False），&#39;passthrough&#39;]，
&#39;selection&#39;：[PCA（random_state=RANDOM_STATE，n_components=“mle”，svd_solver=“full”），
SelectKBest（mutual_info_classif，k=40），
SelectKBest（f_classif，k=40），
SelectKBest（chi2，k=40），
SelectPercentile（mutual_info_classif，百分位数=10），
SelectPercentile（f_classif，百分位数=10），
SelectFromModel（CatBoostClassifier（random_state=RANDOM_STATE）），
SelectFromModel（LogisticRegression（random_state=RANDOM_STATE）），
SelectFromModel（RandomForestClassifier（random_state=RANDOM_STATE）），
&#39;passthrough&#39;],
}

gs = GridSearchCV(
pipe_final, 
param_grid, 
cv=5, 
scoring=&#39;roc_auc&#39;, 
n_jobs=-1
)

# Запускаем поиск гиперпараметров
gs.fit(X, y_enc)

这需要很长时间才能完成，我想加快速度。为此，我想使用 OptunaSearchCV。我是否理解正确，使用 OptunaSearchCV 我可以枚举模型本身的超参数，但不能枚举整个管道，因为没有可以设置 SelectKBest(f_classif, k=40)、RobustScaler() 等的分布？
抱歉，如果我的措辞在某些地方不准确，我使用谷歌翻译，因为......我不是母语人士]]></description>
      <guid>https://stackoverflow.com/questions/78672059/problems-with-selecting-hyperopt-pipeline-parameters</guid>
      <pubDate>Wed, 26 Jun 2024 11:14:26 GMT</pubDate>
    </item>
    <item>
      <title>Copilot for Microsoft 365 中的搜索结果优化 [关闭]</title>
      <link>https://stackoverflow.com/questions/78671906/search-results-optimization-in-copilot-for-microsoft-365</link>
      <description><![CDATA[语义索引如何在组织内的 Copilot for Microsoft 365 搜索研究中对文档进行排名？
我们如何操纵 Copilot M365 对我们组织的搜索结果，以便只有具有特定元数据或格式或标记的文档显示在搜索结果的顶部？
问候
我研究了很多文章，发现以下工具之一可以完成这项工作，但我不知道怎么做？

语义索引
RAG（Retravel-Augmented Generation）
Copilot 插件
机器学习技术
或者其他东西……
]]></description>
      <guid>https://stackoverflow.com/questions/78671906/search-results-optimization-in-copilot-for-microsoft-365</guid>
      <pubDate>Wed, 26 Jun 2024 10:44:35 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Transformer 使用正弦和余弦函数进行位置嵌入</title>
      <link>https://stackoverflow.com/questions/78671510/why-do-transformers-use-both-sine-and-cosine-functions-for-positional-embeddings</link>
      <description><![CDATA[在 Google Research 发表的论文《Attention is All You Need》中提到的 Transformers 架构中，在位置编码部分，作者使用了正弦和余弦函数的组合进行位置嵌入。
我的问题是：由于每个级别的频率不同，仅使用正弦函数或余弦函数是否足够？添加余弦函数对公式到底有什么贡献？
PE(pos,2i)=sin(pos/ 10000*(2i/d))
PE(pos,2i+1)=cos⁡(pos/10000*(2i/d))

你可以参考这篇文章：
Transformer 架构：位置编码]]></description>
      <guid>https://stackoverflow.com/questions/78671510/why-do-transformers-use-both-sine-and-cosine-functions-for-positional-embeddings</guid>
      <pubDate>Wed, 26 Jun 2024 09:26:58 GMT</pubDate>
    </item>
    <item>
      <title>为什么它返回 null [关闭]</title>
      <link>https://stackoverflow.com/questions/78671410/why-is-it-returning-null</link>
      <description><![CDATA[从 fastapi 导入 FastAPI、文件、上传文件
导入 uvicorn
导入 numpy 作为 np
从 io 导入 BytesIO
从 PIL 导入图像
导入 tensorflow 作为 tf
app = FastAPI()

MODEL = tf.keras.models.load_model(&quot;../models/1&quot;)
CLASS_NAMES = [&quot;早疫病&quot;, &quot;晚疫病&quot;, &quot;健康&quot;]
@app.get(&quot;/ping&quot;)
async def ping():
返回&quot;hello world&quot;

def read_file_as_image(data) -&gt; np.ndarray:
image = np.array(Image.open(BytesIO(data)))
返回图像

@app.post(&quot;/predict&quot;)
async def predict(
file: UploadFile = File(...)
):
image = read_file_as_image(await file.read())
img_batch = np.expand_dims(image,0)

predictions = MODEL.predict(img_batch)
predict_class = CLASS_NAMES[np.argmax(predictions[0])]
confidence = np.max(predictions[0])
返回 {
&#39;class&#39; : predict_class,
&#39;confidence&#39; : float(confidence)
}

if __name__ == &quot;__main__&quot;:
uvicorn.run(app, host=&#39;localhost&#39;, port=8000)

我尝试使用 Fastapi 获取我的预测模型的输出，但它返回 null
它应该返回类名和置信度，但它只是返回 null]]></description>
      <guid>https://stackoverflow.com/questions/78671410/why-is-it-returning-null</guid>
      <pubDate>Wed, 26 Jun 2024 09:09:31 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的 ElasticNetCV：获取具有相应 MSE 的超参数完整网格？</title>
      <link>https://stackoverflow.com/questions/78671295/elasticnetcv-in-python-get-full-grid-of-hyperparameters-with-corresponding-mse</link>
      <description><![CDATA[我在 Python 中用三个分割拟合了一个 ElasticNetCV：
import numpy as np
from sklearn.linear_model import LinearRegression

#样本数据：
num_samples = 100 # 样本数量
num_features = 1000 # 特征数量
X = np.random.rand(num_samples, num_features)
Y = np.random.rand(num_samples)

#模型
l1_ratios = np.arange(0.1, 1.1, 0.1)
tscv=TimeSeriesSplit(max_train_size=None, n_splits=3)
regr = ElasticNetCV(cv=tscv.split(X), random_state=42,l1_ratio=l1_ratios)
regr.fit(X,Y)

现在我想获取超参数组合的整个网格以及相应的 MSE 作为数据框，我尝试了以下方法。但是，问题在于，生成的数据框显示的超参数组合的 MSE 并未显示为 ElasticNetCV 对象中的最小值，该对象可以通过 regr.alpha_ 和 regr.l1_ratio_ 获得
:
mse_path = regr.mse_path_
alpha_path = regr.alphas_

# 重塑 mse_path，使 l1_ratios、n_alphas、cross_validation_step 作为单独的列
mse_values = mse_path.flatten()
alpha_values = alpha_path.flatten()
l1_values=np.tile(l1_ratios ,int(alpha_values.shape[0]/l1_ratios.shape[0]))
repeated_l1_ratios = np.repeat(l1_ratios, 100)

# mse维度为 (11, 100, 3)
array_3d = mse

# 将 3D 数组展平为 2D 数组
# 每个形状为 (100, 3) 的子数组都将成为新 2D 数组中的一行
array_2d = array_3d.reshape(-1, 3)

# 从 2D 数组创建 DataFrame
df = pd.DataFrame(array_2d, columns=[&#39;MSE Split1&#39;, &#39;MSE Split2&#39;, &#39;MSE Split3&#39;])

df[&#39;alpha_values&#39;] = alpha_values
df[&#39;l1_values&#39;] = duplicate_l1_ratios

以下结果导致超参数组合不是真实的组合。因此，在组合 MSE 和超参数值时，会出现问题：
# 计算三个分割中每行的最小 MSE
df[&#39;Min MSE&#39;] = df[[&#39;MSE Split1&#39;, &#39;MSE Split2&#39;, &#39;MSE Split3&#39;]].min(axis=1)

# 确定总体最小 MSE 的行
min_mse_row_index = df[&#39;Min MSE&#39;].idxmin()

# 检索最小 MSE 的行
min_mse_row = df.loc[min_mse_row_index]

print(&quot;所有分割中 MSE 最小的行：&quot;)
print(min_mse_row)
]]></description>
      <guid>https://stackoverflow.com/questions/78671295/elasticnetcv-in-python-get-full-grid-of-hyperparameters-with-corresponding-mse</guid>
      <pubDate>Wed, 26 Jun 2024 08:49:51 GMT</pubDate>
    </item>
    <item>
      <title>在实施有关使用 Python 进行 ML 集成的 Github 项目时遇到了困难</title>
      <link>https://stackoverflow.com/questions/78670360/got-stuck-in-implementing-a-github-project-about-ml-integration-with-python</link>
      <description><![CDATA[我是一名新开发人员，正在尝试不同的项目，我正在尝试实现这个 Git 存储库（https://github.com/jowpereira/MetaTrader-Python-Integration），它本质上是将 ML 模型与策略测试器集成：实现价格预测的回归模型。
安装先决条件后，每次运行时，我都会遇到这个问题
从 CMD 获取消息
它转换为 &lt;&lt;--等待配置文件--&gt;&gt;
以下是任何感兴趣的人的代码
def main():
file = File()
file.delete_file(PATH_COMMON.format(INIT_ARCHIVE))
file.delete_file(PATH_COMMON.format(INIT_OK_ARCHIVE))
file.delete_file(PATH_COMMON.format(TESTE_ARCHIVE))
file.delete_file(PATH_COMMON.format(TESTE_PREDICT))

while True:
print(&quot;&lt;&lt;--Aguardando o配置文件--&gt;&gt;&quot;)
typerun = file.check_init_param(PATH_COMMON.format(INIT_ARCHIVE))
time.sleep(1)

file.save_file_csv(PATH_COMMON.format(INIT_OK_ARCHIVE))

if typerun == 0:
print(&#39;&lt;&lt;-- {} --&gt;&gt;&#39;.format(&quot;Script Python - Modo Teste&quot;))
while True:
accept = file.check_open_file(PATH_COMMON.format(TESTE_ARCHIVE))
file.delete_file(PATH_COMMON.format(TESTE_ARCHIVE))

if len(receive) == 0:
continue

if accept[&quot;command&quot;][0].upper() == &quot;START&quot;:
header = [&quot;open&quot;, &quot;close&quot;]
payload = []

for i in range(len(receive)):
row = [
str(receive[&quot;open&quot;][i]),
str(receive[&quot;close&quot;][i])
]
payload.append(row)

df = pd.DataFrame(payload, columns=header)
smoothed_df = df.ewm(alpha=0.1).mean()
smoothed_df[&#39;macd&#39;], smoothed_df[&#39;signal&#39;] = macd(smoothed_df)
smoothed_df[&#39;ema&#39;] = ema(smoothed_df)
smoothed_df[&#39;rsi&#39;] = rsi(smoothed_df)
selected_df = smoothed_df[[&#39;open&#39;, &#39;macd&#39;, &#39;ema&#39;, &#39;rsi&#39;, &#39;close&#39;]].dropna()

# 预览
next_week_prediction = predict_next_week(selected_df, MODEL_PATH, window_size)
file.save(PATH_COMMON.format(TESTE_PREDICT), str(next_week_prediction[0]))
continue

elif accept[&quot;command&quot;][0].upper() == &quot;STOP&quot;:
file.delete_file(PATH_COMMON.format(TESTE_PREDICT))
continue

if name == &quot;ma​​in&quot;:
main()
我已经给作者发短信了，但他没有回复我，请问有谁知道如何解决这个问题吗？
按照 Git 存储库中的说明，安装了所有先决条件和安装 Metatrader 5 也在 CMD 中，当我收到以前的错误消息时，但不是，我不知道配置文件是什么意思]]></description>
      <guid>https://stackoverflow.com/questions/78670360/got-stuck-in-implementing-a-github-project-about-ml-integration-with-python</guid>
      <pubDate>Wed, 26 Jun 2024 04:38:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 CycleGAN-and-pix2pix 开源仓库中的 cycleGan 模型进行图像推理</title>
      <link>https://stackoverflow.com/questions/78669614/image-inference-with-cyclegan-model-from-cyclegan-and-pix2pix-open-source-repo</link>
      <description><![CDATA[我已经根据CycleGAN-and-pix2pix 开源API在google colab上训练了一个cycleGan模型。
对于训练过程，我使用了 !python train.py --dataroot /content/drive/MyDrive/project/dataset --name F2F --model cycle_gan --display_id -1 
为了从我使用的文件夹中推断图像集
opt = TestOptions()
#defined 选项出现在这里
dataset = create_dataset(opt) 
初始化模型
model = create_model(opt)
model.setup(opt)
model.eval()
data_iter = iter(dataset.dataloader)
data_dict = next(data_iter)
input_image_tensor = data_dict[&#39;A&#39;]
data = {&#39;A&#39;: input_image_tensor,&#39;A_paths&#39;: &#39;&#39;}
model.set_input(data)
model.test()
visuals = model.get_current_visuals()
output_image = visuals[&#39;fake&#39;]
output_image_np = output_image.squeeze().cpu().numpy().transpose(1, 2, 0)
output_image_np = ((output_image_np - output_image_np.min()) / (output_image_np.max() - output_image_np.min()) * 255).astype(np.uint8)
output_image_np = cv2.cvtColor(output_image_np, cv2.COLOR_BGR2RGB)
cv2_imshow(output_image_np)


上述代码片段按预期工作并产生了良好的结果。
现在，我想推断单个图像而不经过加载器。
我试图模仿 create_dataset(opt) 中的图像预处理，它是一个 API 函数，使用以下代码：
def preprocess(image):
if image.ndim == 2 or image.shape[2] == 1:
image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
elif image.shape[2] == 4:
image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)
elif image.shape[2] == 3:
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
pil_image = transforms.ToPILImage()(image)
transform_pipeline = transforms.Compose([
transforms.Resize(286),
transforms.CenterCrop(256),
transforms.ToTensor(),
transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
image_tensor = transform_pipeline(pil_image)
image_tensor = image_tensor.unsqueeze(0)

返回 image_tensor

input_image = cv2.imread(&#39;/content/drive/MyDrive/dataset/testA/image_1.jpg&#39;)
input_image_tensor = preprocess(input_image)
data = {&#39;A&#39;: input_image_tensor,&#39;A_paths&#39;: &#39;&#39;}
model.set_input(data)
model.test()
visuals = model.get_current_visuals()
output_image = visuals[&#39;fake&#39;]
output_image_np = output_image.squeeze().cpu().numpy().transpose(1, 2, 0)
output_image_np = ((output_image_np - output_image_np.min()) / (output_image_np.max() - output_image_np.min()) * 255).astype(np.uint8)
output_image_np = cv2.cvtColor(output_image_np, cv2.COLOR_BGR2RGB)
cv2_imshow(output_image_np)

但生成的图像非常模糊，并且没有像使用 create_dataset(opt) 函数中的加载器那样提供精细调整的结果。
如能提供任何有关如何实现此目的的帮助，我将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78669614/image-inference-with-cyclegan-model-from-cyclegan-and-pix2pix-open-source-repo</guid>
      <pubDate>Tue, 25 Jun 2024 21:42:50 GMT</pubDate>
    </item>
    <item>
      <title>绘制预测掩码的问题</title>
      <link>https://stackoverflow.com/questions/78669554/issue-with-plotting-predicted-masks</link>
      <description><![CDATA[我目前正在进行一个深度学习项目“叶病分割”。我已经训练了一个模型超过 50 个时期，并获得了以下准确度和损失指标：
训练损失：19.4736，训练准确度：0.9395
验证损失：19.6197，验证准确度：0.9100
测试损失：19.6148，测试准确度：0.9123
但是，当我绘制预测的蒙版时，它们看起来不准确。我的绘图代码有问题吗？
def plot_predictions(model, images, mask, num_samples=5):
predictions = model.predict(images[:num_samples])
for i in range(num_samples):
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.title(&#39;真实图像&#39;)
plt.imshow(images[i])
plt.subplot(1, 3, 2)
plt.title(&#39;地面真相面具&#39;)
plt.imshow(masks[i], cmap=&#39;gray&#39;) # 假设面具已经是二进制的
plt.subplot(1, 3, 3)
plt.title(&#39;预测面具&#39;)
plt.imshow(predictions[i][:, :, 0], cmap=&#39;gray&#39;) # 转换预测面具转换为二进制
plt.show()

plot_predictions(model, test_images.numpy(), test_masks_L, num_samples=5)

原始图像-蒙版-预测蒙版
请检查我的代码并帮助找出可能导致此问题的任何错误？]]></description>
      <guid>https://stackoverflow.com/questions/78669554/issue-with-plotting-predicted-masks</guid>
      <pubDate>Tue, 25 Jun 2024 21:18:52 GMT</pubDate>
    </item>
    <item>
      <title>Polars - 性能问题 - 尝试为每行创建一个新的数据框</title>
      <link>https://stackoverflow.com/questions/78668432/polars-issues-with-performance-attempting-to-create-a-new-dataframe-per-row</link>
      <description><![CDATA[我需要在大型 df 的每一行上运行另一个库的算法，但在将我的代码转换为极坐标表达式以获得更好的性能时遇到了麻烦。以下是几个示例 DF：
df_products = pl.DataFrame({
&#39;SKU&#39;:[&#39;apple&#39;,&#39;banana&#39;,&#39;carrot&#39;,&#39;date&#39;],
&#39;DESCRIPTION&#39;: [
&quot;Wire Rope&quot;,
&quot;Connector&quot;,
&quot;Tap&quot;,
&quot;Zebra&quot;
],
&#39;CATL3&#39;: [
&quot;Fittings&quot;,
&quot;Tube&quot;,
&quot;Tools&quot;,
&quot;Animal&quot;
],
&#39;YELLOW_CAT&#39;: [
&quot;Rope Accessories&quot;,
&quot;Tube Fittings&quot;,
&quot;Forming Taps&quot;,
&quot;Striped&quot;
],
&#39;INDEX&#39;: [0, 5, 25, 90],
&#39;嵌入&#39;: [
[1, 2, 3],
[4, 5, 6],
[7, 8, 9],
[10,11,12]
],
})

df_items_sm_ex = pl.DataFrame({
&#39;PRODUCT_INFO&#39;:[&#39;苹果&#39;,&#39;香蕉&#39;,&#39;胡萝卜&#39;],
&#39;搜索相似性分数&#39;: [
[1., 0.87, 0.54, 0.33],
[1., 0.83, 0.77, 0.55],
[1., 0.92, 0.84, 0.65]
],
&#39;搜索位置&#39;: [
[0, 5, 25, 90],
[1, 2, 151, 373],
[3, 5, 95, 1500]
],
&#39;SKU&#39;:[&#39;apple&#39;,&#39;banana&#39;,&#39;carrot&#39;],
&#39;YELLOW_CAT&#39;: [
&quot;绳索配件&quot;,
&quot;管接头&quot;,
&quot;成型丝锥&quot;
],
&#39;CATL3&#39;: [
&quot;接头&quot;,
&quot;管&quot;,
&quot;工具&quot;
],
&#39;EMBEDDINGS&#39;: [
[1, 2, 3],
[4, 5, 6],
[7, 8, 9]
],
})

现在是代码
对于每一行，我有 3 个主要操作：生成基本新数据框、对数据框进行预处理/清理/运行预测、将数据框写入几个 SQL 表。我注意到步骤 1 和 2 很容易花费最长的时间来执行：
df_items_sm_ex.select(
pl.struct(df_items_sm_ex.columns)
.map_elements(lambda row: build_complements(
row, df_items, rfc, rfc_comp, engine, current_datetime
)))

def build_complements(row, df_products, ml, ml_comp, engine, current_datetime):
try:
#步骤 1 - 生成基本新数据框
output_df = build_candidate_dataframe(row, df_products)
#步骤 2 - 对数据框进行预处理/清理/运行预测
output_df = process_candidate_output(df_products, output_df, ml, ml_comp)
#步骤 3 将数据框写入 SQL
write_validate_complements(output_df, row, current_datetime, engine)
except Exception as e:
print(f&#39;exception: {repr(e)}&#39;)

def build_candidate_dataframe(row, df_products):
df_len = len(row[&#39;SEARCH_SIMILARITY_SCORE&#39;])
schema = {&#39;QUERY&#39;: str,
&#39;SIMILARITY_SCORE&#39;: pl.Float32, 
&#39;POSITION&#39;: pl.Int64,
&#39;QUERY_SKU&#39;: str, 
&#39;QUERY_LEAF&#39;: str,
&#39;QUERY_CAT&#39;: str,
&#39;QUERY_EMBEDDINGS&#39;: pl.List(pl.Float32)
}
output_df = pl.DataFrame({&#39;QUERY&#39;: [row[&#39;PRODUCT_INFO&#39;]] * df_len,
&#39;SIMILARITY_SCORE&#39;: row[&#39;SEARCH_SIMILARITY_SCORE&#39;], 
&#39;POSITION&#39;: row[&#39;SEARCH_POSITION&#39;],
&#39;QUERY_SKU&#39;: [row[&#39;SKU&#39;]] * df_len, 
&#39;QUERY_LEAF&#39;: [row[&#39;YELLOW_CAT&#39;]] * df_len,
&#39;QUERY_CAT&#39;: [row[&#39;CATL3&#39;]] * df_len,
&#39;QUERY_EMBEDDINGS&#39;: [row[&#39;EMBEDDINGS&#39;]] * df_len
}, schema=schema).sort(&quot;SIMILARITY_SCORE&quot;, descending=True)

output_df = output_df.join(df_products[[&#39;SKU&#39;, &#39;EMBEDDINGS&#39;, &#39;INDEX&#39;, &#39;DESCRIPTION&#39;, &#39;CATL3&#39;, &#39;YELLOW_CAT&#39;]], left_on=[&#39;POSITION&#39;], right_on=[&#39;INDEX&#39;], how=&#39;left&#39;)
output_df = output_df.rename({&quot;DESCRIPTION&quot;: &quot;SIMILAR_PRODUCT_INFO&quot;, &quot;CATL3&quot;: &quot;SIMILAR_PRODUCT_CAT&quot;, &quot;YELLOW_CAT&quot;: &quot;SIMILAR_PRODUCT_LEAF&quot;})
return output_df

def process_candidate_output(df_products, output_df, ml, ml_comp):
combined_embeddings = (output_df.to_pandas()[&#39;QUERY_EMBEDDINGS&#39;] + output_df.to_pandas()[&#39;EMBEDDINGS&#39;]) / 2
output_df = output_df.with_columns(pl.Series(name=&#39;COMBINED_EMBEDDINGS&#39;, values=combined_embeddings))
output_df = output_df[[&#39;QUERY&#39;, &#39;QUERY_SKU&#39;, &#39;QUERY_CAT&#39;, &#39;QUERY_LEAF&#39;, &#39;SIMILAR_PRODUCT_INFO&#39;, &#39;SIMILAR_PRODUCT_CAT&#39;, &#39;SIMILAR_PRODUCT_LEAF&#39;, &#39;SIMILARITY_SCORE&#39;, &#39;COMBINED_EMBEDDINGS&#39;, &#39;SKU&#39;, &#39;POSITION&#39;]]
output_df = output_df.filter(
pl.col(&#39;SKU&#39;) != output_df[&#39;QUERY_SKU&#39;][0]
)
#ML 预测
output_df = predict_complements(output_df, ml)
output_df = output_df.filter(
pl.col(&#39;COMPLEMENTARY_PREDICTIONS&#39;) == 1
)
#其他 ML 预测
output_df = predict_required_accessories(output_df, ml_comp)
output_df = output_df.sort(by=&#39;LABEL_PROBABILITY&#39;, descending=True)
return output_df
]]></description>
      <guid>https://stackoverflow.com/questions/78668432/polars-issues-with-performance-attempting-to-create-a-new-dataframe-per-row</guid>
      <pubDate>Tue, 25 Jun 2024 16:01:45 GMT</pubDate>
    </item>
    <item>
      <title>我想通过机器学习预测未来 60 天的股票价格</title>
      <link>https://stackoverflow.com/questions/78665213/i-want-to-predict-stock-price-in-next-60-days-by-machine-learning</link>
      <description><![CDATA[我想预测未来60天的股价，但是写完代码后，预测结果却向后。有人能指点我吗？我该怎么做？我修改了代码，但是不起作用。
我的代码是
import math
from mplfinance.original_flavor import candlestick_ohlc
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.metrics import mean_squared_error

Tencent2.head()
#定义特征和目标
##features = [&#39;Open&#39;, &#39;Vol.&#39;]
##target = &#39;Price&#39;

#绘制折线图
Tencent2[&#39;Date&#39;] = pd.to_datetime(Tencent2[&#39;日期&#39;])
plt.figure(figsize=(15,6))
plt.plot(Tencent2[&#39;日期&#39;], Tennis2[&#39;收盘价&#39;], marker=&#39;.&#39;)
plt.title(&#39;按月收盘价&#39;, fontsize=15)
plt.xlabel(&#39;日期&#39;, fontsize=13)
plt.ylabel(&#39;收盘价&#39;, fontsize=13)
plt.xticks(rotation=45)
plt.show()

#绘制蜡烛图
matplotlib_date = mdates.date2num(Tencent2[&#39;日期&#39;])
ohlc = np.vstack((matplotlib_date,Tencent2[&#39;Open&#39;],Tencent2[&#39;High&#39;],Tencent2[&#39;Low&#39;],Tencent2[&#39;Close&#39;])).T
plt.figure(figsize=(15,6))
ax = plt.subplot()
candlestick_ohlc(ax,ohlc,width=0.8,colorup=&#39;g&#39;,colordown=&#39;r&#39;)
ax.xaxis_date()
plt.title(&#39;收盘价变动&#39;)
plt.xlabel(&#39;日期&#39;)
plt.ylabel(&#39;收盘价&#39;)
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

#重置索引
Tencent2.set_index(&#39;Date&#39;, inplace=True)

#创建一个只有“Close”列的新数据框
data =腾讯2.filter([&#39;Close&#39;])

#将数据框转换为numpy数组
dataset = data.values
#获取训练模型的行数
training_data = math.ceil( len(dataset) * 0.7 )

#缩放数据
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(dataset)

#创建训练数据集
#创建缩放后的训练数据集
train_data = scaled_data[0:training_data, :]
#将数据拆分为x_train和y_train数据集
x_train = []
y_train = []
#我们创建一个循环
for i in range(60, len(train_data)):
x_train.append(train_data[i-60:i, 0]) 
y_train.append(train_data[i, 0]) 
if i &lt;= 60:
print(x_train)
print(y_train)
print()

#将 x_train 和 y_train 转换为 numpy 数组
x_train, y_train = np.array(x_train), np.array(y_train)

#重塑数据
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
x_train.shape

#构建 LSTM 模型
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(LSTM(50, return_sequences=False))
model.add(Dense(25))
model.add(Dense(1))

#编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)

#训练模型
model.fit(x_train, y_train, batch_size=1, epochs=1)

#创建测试数据集
#创建一个包含从索引 1738 到 2247 的缩放值的新数组
test_data = scaled_data[training_data - 60:]
#创建数据集 X_test 和 y_test
X_test = []
y_test = dataset[training_data:, :]
for i in range(60, len(test_data)):
X_test.append(test_data[i-60:i, 0])

#将数据转换为 numpy 数组
X_test = np.array(X_test)

#重塑数据
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

#获取模型对 X_test 数据集的预测价格值
predictions = model.predict(X_test)
predictions = scaler.inverse_transform(predictions)

#评估模型（获取均方根误差 (RMSE)）
rmse = np.sqrt( np.mean( predictions - y_test )**2 )

#绘制数据
train = data[:training_data]
valid = data[training_data:]
valid[&#39;Predictions&#39;] = predictions
#可视化数据
plt.figure(figsize=(16,8))
plt.title(&#39;Model&#39;)
plt.xlabel(&#39;Date&#39;, fontsize=18)
plt.ylabel(&#39;Close Price&#39;, fontsize=18)
plt.plot(train[&#39;Close&#39;])
plt.plot(valid[[&#39;Close&#39;, &#39;Predictions&#39;]])
plt.legend([&#39;Train&#39;, &#39;Validation&#39;, &#39;Predictions&#39;], loc=&#39;lower right&#39;)
plt.show()


我想预测未来 60 天的价格，但价格是过去预测的。我该如何预测 2024-06 之后的价格？
谢谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78665213/i-want-to-predict-stock-price-in-next-60-days-by-machine-learning</guid>
      <pubDate>Tue, 25 Jun 2024 02:51:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 Cord-V2 数据集微调 LayoutLmv3</title>
      <link>https://stackoverflow.com/questions/78606543/fine-tuning-layoutlmv3-using-cord-v2-dataset</link>
      <description><![CDATA[我正在使用 CORD-v2 数据集对 LayoutLMv3 进行微调。我在数据预处理部分遇到了困难，特别是如何从图像中正确提取总量 (TTC)。我在网上找到的示例似乎使用了较旧的 CORD 数据集，该数据集的格式不同。新的 CORD-v2 数据集仅包含图像和地面实况标签。
如何解决这个问题？
我尝试过 YouTube 和 Hugging Face 中的示例，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78606543/fine-tuning-layoutlmv3-using-cord-v2-dataset</guid>
      <pubDate>Tue, 11 Jun 2024 09:22:25 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Google Earth Engine 中执行 k 倍交叉验证？</title>
      <link>https://stackoverflow.com/questions/75536700/how-to-perform-a-k-fold-cross-validation-in-google-earth-engine</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/75536700/how-to-perform-a-k-fold-cross-validation-in-google-earth-engine</guid>
      <pubDate>Wed, 22 Feb 2023 18:19:55 GMT</pubDate>
    </item>
    </channel>
</rss>