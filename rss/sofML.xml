<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 06 Nov 2024 06:24:34 GMT</lastBuildDate>
    <item>
      <title>对特定层的参数进行自动求导</title>
      <link>https://stackoverflow.com/questions/79161323/autograd-on-a-specific-layer-s-parameters</link>
      <description><![CDATA[我正在尝试获取特定层参数的雅可比矩阵。下面是我的网络模型，我在其上应用了 functional_call。
def fm(params,input):
return functional_call(self.model,params,input.unsqueeze(0)).squeeze(0)

def floss(func_params,input):
fx = fm(func_params, input)
return fx

我过去常常用这种方式计算所有参数的雅可比矩阵
func_params=dict(self.model.named_pa​​rameters())
per_sample_grads =vmap(jacrev(floss,0), (None, 0))(func_params, input)

现在，我只需要获取特定层参数的梯度，这是我的方法。
def grad(f,param):
return torch.autograd.grad(f,param) 
out = vmap(floss,(None,0))(func_params,input)
gradf=vmap(grad,(0,None))(out,func_params[&#39;model.0.weight&#39;])

但是，错误提示“张量的元素 0 不需要 grad，也没有 grad_fn”
因为，我已经尝试过了
grad=self.grad(out[0],func_params[&#39;model.0.weight&#39;])

而且成功了。我真的不知道该如何解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79161323/autograd-on-a-specific-layer-s-parameters</guid>
      <pubDate>Wed, 06 Nov 2024 04:19:56 GMT</pubDate>
    </item>
    <item>
      <title>我如何知道现有 chromadb 集合正在使用什么嵌入模型？</title>
      <link>https://stackoverflow.com/questions/79161260/how-can-i-know-what-embedding-model-of-a-existing-chromadb-collection-is-using</link>
      <description><![CDATA[我正在研究 chromadb。当我处理一些现有的集合时，我总是遇到错误：

chromadb.errors.InvalidDimensionException：嵌入维度 384 与集合维度 4096 不匹配

我知道这是因为我的嵌入模型与集合创建时选择的嵌入模型不匹配。
所以我想知道如何检查现有集合正在使用什么嵌入模型？]]></description>
      <guid>https://stackoverflow.com/questions/79161260/how-can-i-know-what-embedding-model-of-a-existing-chromadb-collection-is-using</guid>
      <pubDate>Wed, 06 Nov 2024 03:17:21 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们需要在 keras model.fit() 中使用 y 变量？[关闭]</title>
      <link>https://stackoverflow.com/questions/79159003/why-do-we-need-a-y-variable-in-keras-model-fit</link>
      <description><![CDATA[我正在处理手写数字数据集。数据加载如下：
(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

这是为对数字进行分类而创建的神经网络的代码：
model = keras.Sequential([
keras.layers.Dense(10, input_shape=(784,),activation=&#39;sigmoid&#39;)
])

model.compile(
optimizer=&#39;adam&#39;, 
loss = &#39;sparse_categorical_crossentropy&#39;,
metrics = [&#39;accuracy&#39;]
)
model.fit(X_train_flattened, y_train, epochs=5)

问题是，model.fit() 中的 y_train 的作用是什么。这似乎是一个分类问题，网络只需要输入（x_train_flattened）即可进行训练。]]></description>
      <guid>https://stackoverflow.com/questions/79159003/why-do-we-need-a-y-variable-in-keras-model-fit</guid>
      <pubDate>Tue, 05 Nov 2024 11:56:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 JAX 训练模型时跟踪测试/验证损失</title>
      <link>https://stackoverflow.com/questions/79158791/tracking-test-val-loss-when-training-a-model-with-jax</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79158791/tracking-test-val-loss-when-training-a-model-with-jax</guid>
      <pubDate>Tue, 05 Nov 2024 10:58:38 GMT</pubDate>
    </item>
    <item>
      <title>Weka RandomForest m_Classifiers 为空且仅在构建 Spring Boot 项目后才出现 NullPointerException 和无法找到允许的类错误</title>
      <link>https://stackoverflow.com/questions/79157997/nullpointerexception-with-weka-randomforest-m-classifiers-is-null-and-cant-find</link>
      <description><![CDATA[我正在开发一个 Spring Boot 项目，我使用 Weka 的 RandomForest 模型并使用从 PostgreSQL 数据库检索的数据对其进行训练。当我正常启动项目（IDE 中的运行命令）时，项目运行完美，所有预测都正常工作。但是，当我构建项目然后尝试运行它时，我遇到了以下两个错误：
错误 1
错误 2
我想知道为什么这些错误只在构建项目后出现。可能是什么原因造成的？我该如何解决？
问题：
我通过调用 buildClassifier 来训练 RandomForest 模型，它在常规运行中训练成功。
但是，在构建项目后，我在 classifyInstance 方法调用期间收到 m_Classifiers 为空错误，这表明分类器数组未初始化。此外，在构建环境中找不到 RandomTree，尽管它在正常运行期间可用。
我尝试过的解决方案：
验证 trainingSet 和 testSet 是否包含实例（它们不为空）。
确认在常规运行中成功使用 buildClassifier 训练模型。
使用 values[2] = Double.NaN 设置一个空的（缺失的）acc 类值。
通过 Maven 在 pom.xml 中添加了 Weka 依赖项（weka-stable，版本 3.8.6）。
问题：
为什么这些错误只在构建项目后出现？如何解决从构建运行时 m_Classifiers 为空且找不到 RandomTree 的问题？
其他信息：

Weka 版本：3.8.6
Java 版本：1.8
使用 Spring Tool Suite (STS) 并通过 Maven 添加 Weka 依赖项。
]]></description>
      <guid>https://stackoverflow.com/questions/79157997/nullpointerexception-with-weka-randomforest-m-classifiers-is-null-and-cant-find</guid>
      <pubDate>Tue, 05 Nov 2024 07:17:06 GMT</pubDate>
    </item>
    <item>
      <title>Unet 输入和输出形状之间的差异</title>
      <link>https://stackoverflow.com/questions/79157733/unet-discrepency-between-input-and-output-shape</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79157733/unet-discrepency-between-input-and-output-shape</guid>
      <pubDate>Tue, 05 Nov 2024 05:15:54 GMT</pubDate>
    </item>
    <item>
      <title>GAN 图像生成给出疯狂的输出[关闭]</title>
      <link>https://stackoverflow.com/questions/79157642/gan-image-generation-gives-crazy-output</link>
      <description><![CDATA[我正在尝试使用 GAN 创建一个生成器网络，该网络可以将 32x32 图像转换为 128x128 图像。我最初训练了一个 CNN，并取得了一些成功：

为了获得更准确的结果，我使用了一种 GAN 架构，该架构为生成器网络实现了更低的损失分数。然而，当我尝试进行预测时，我收到的图像如下所示：

似乎生成器网络正在寻找某种方法来打破鉴别器网络。我该如何解决这个问题，并找到一种让生成器真正生成高清图像的方法？
这是我的 GAN 的样子：
generator = UpsamplingCNN().to(device)
discriminator = Discriminator().to(device)

criterion = nn.BCELoss()

## 不同的学习率
optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))

num_generator_updates = 5
lambda_gp = 10
num_epochs = 50 
## 标签平滑
real_label = 0.9
fake_label = 0.0

for epoch in range(num_epochs):
generator.train()
discriminator.train()

for i, (low_res, high_res) in enumerate(tqdm(train_loader)):
# 将数据移动到正确的设备
low_res = low_res.to(device)
high_res = high_res.to(device)

# 为真实图像和虚假图像添加噪声
noise_std_dev = 0.05
noisy_real_images = high_res + noise_std_dev * torch.randn_like(high_res).to(device)
noisy_real_images = torch.clamp(noisy_real_images, 0.0, 1.0)

fake_images = generator(low_res)
noisy_fake_images = fake_images + noise_std_dev * torch.randn_like(fake_images).to(device)
noisy_fake_images = torch.clamp(noisy_fake_images, 0.0, 1.0)

# 真标签和假标签
output_real = discriminator(noisy_real_images).view(-1)
labels_real = torch.full((output_real.size(0),), real_label, dtype=torch.float, device=device)
loss_real = criterion(output_real, labels_real)

# 使用嘈杂的假图像进行训练
output_fake = discriminator(noisy_fake_images.detach()).view(-1)
labels_fake = torch.full((output_fake.size(0),), fake_label, dtype=torch.float, device=device)
loss_fake = criterion(output_fake, labels_fake)

# 计算梯度惩罚
gradient_penalty = compute_gradient_penalty(discriminator, high_res, fake_images, device)

# 带梯度惩罚的鉴别器总损失
loss_D = loss_real + loss_fake + lambda_gp * gradient_penalty
loss_D.backward()

optimizer_D.step()

## 多个生成器更新
for _ in range(num_generator_updates):
optimizer_G.zero_grad()

# 生成假图像并计算生成器的损失
fake_images = generator(low_res)
output_fake_for_G = discriminator(fake_images).view(-1)
labels_fake_for_G = torch.full((output_fake_for_G.size(0),),real_label, dtype=torch.float, device=device)
loss_G = criterion(output_fake_for_G, labels_fake_for_G)
loss_G.backward()

optimizer_G.step()

if i % 10 == 0:
print(f&#39;Epoch [{epoch+1}/{num_epochs}], Batch [{i}/{len(train_loader)}], &#39;
f&#39;Loss_D: {loss_real.item() + loss_fake.item():.4f}, Loss_G: {loss_G.item():.4f}&#39;)

我还对判别器使用了梯度惩罚，如下所示：
def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):
alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)
interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)
d_interpolates = discriminator(interpolates)
fake = torch.ones(d_interpolates.size(), device=device, require_grad=False)

gradients = torch.autograd.grad(
output=d_interpolates,
input=interpolates,
grad_outputs=fake,
create_graph=True,
retain_graph=True,
only_inputs=True
)[0]

gradients = gradients.view(gradients.size(0), -1)
gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
return gradient_penalty
]]></description>
      <guid>https://stackoverflow.com/questions/79157642/gan-image-generation-gives-crazy-output</guid>
      <pubDate>Tue, 05 Nov 2024 03:55:19 GMT</pubDate>
    </item>
    <item>
      <title>我们可以使用 SAM2 来追踪类似的物体吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79157637/can-we-use-sam2-for-tracking-similar-objects</link>
      <description><![CDATA[有没有什么方法（或资源）可以使用 SAM2 来跟踪视频中的统一对象？
我发现 SAM2 可以更快地进行分割，我想用它来检测/分割和跟踪我的数据集上只有 1 个类（自定义类：0，统一对象）的对象。
可用的 SOTA 跟踪器（如 sort、botsort、bytetrack 和 deepsort）无法跨帧跟踪对象（但检测效果良好）。]]></description>
      <guid>https://stackoverflow.com/questions/79157637/can-we-use-sam2-for-tracking-similar-objects</guid>
      <pubDate>Tue, 05 Nov 2024 03:50:39 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中如何处理重叠数据</title>
      <link>https://stackoverflow.com/questions/79157457/how-to-deal-with-overlapping-data-in-machine-learning</link>
      <description><![CDATA[我正在创建一个机器学习模型来确定用户是否是机器人，我使用 seaborn 绘制了配对图并意识到大多数数据是重叠的。下面是我为标准化、拆分和部署模型编写的代码。该图显示了该模型在超过 40000 个样本的情况下的表现。如您所见，模型正在猜测，我正在尝试找出原因。

X = new_df[[&#39;Retweet Count&#39;, &#39;Mention Count&#39;, &#39;Follower Count&#39;, &#39;Tweet&#39;, &#39;Hashtags&#39;, &#39;Verified&#39;, &#39;Created At&#39;]]
y = new_df[[&#39;Bot Label&#39;]].values

y = y.ravel() # 确保 y 是一维数组而不是二维数组

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

Scaler = StandardScaler()
X_train_scaled = Scaler.fit_transform(X_train)
X_test_scaled = Scaler.transform(X_test)

从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.datasets 导入 make_classification
从 sklearn.metrics 导入 Confusion_matrix

rfc = RandomForestClassifier(n_estimators = 1000)
rfc.fit(X_train_scaled, y_train)
y_pred = rfc.predict(X_test_scaled)
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

]]></description>
      <guid>https://stackoverflow.com/questions/79157457/how-to-deal-with-overlapping-data-in-machine-learning</guid>
      <pubDate>Tue, 05 Nov 2024 01:41:31 GMT</pubDate>
    </item>
    <item>
      <title>验证数据的输入形状无效</title>
      <link>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</link>
      <description><![CDATA[我正在使用 Tensorflow 在 Python 中开发一个简单的 ML 模型。代码如下：
import tensorflow as tf
import pandas as pd

# 加载 CSV 数据
def load_data(filename):
data = pd.read_csv(filename)
X = data[[&#39;X0&#39;,&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;]]
Y = data[[&#39;Y0&#39;,&#39;Y1&#39;]]
return tf.data.Dataset.from_tensor_slices((X.values, Y.values))

training_data = load_data(&quot;binarydatatraining.csv&quot;)
print(training_data)

# 构建一个简单的神经网络模型
model = tf.keras.models.Sequential([
tf.keras.layers.Dense(4,activation=&#39;relu&#39;),
tf.keras.layers.Dense(2)
])
# 编译模型
model.compile(optimizer=&#39;adam&#39;,
loss=&#39;mean_squared_error&#39;)

# 加载验证数据
validation_data = load_data(&quot;binarydatavalidation.csv&quot;)
print(validation_data)

# 训练模型
model.summary()
model.fit(training_data.batch(9), epochs=5)
model.summary()
model.fit(training_data.batch(9), epochs=1, validation_data = validation_data, validation_steps = 2)

一切都运行正常，直到我开始包含验证数据，该数据具有与训练数据相同数量的参数。然后我收到错误
ValueError：调用 Sequential.call() 时遇到异常。

[1m输入 Tensor(&quot;sequence_1/Cast:0&quot;, shape=(4,), dtype=float32) 的输入形状无效。预期形状 (None, 4)，但输入具有不兼容的形状 (4,)[0m

Sequential.call() 收到的参数：
• 输入=tf.Tensor(shape=(4,), dtype=int64)
• 训练=False
• 掩码=None

打印验证和训练数据集显示它们具有相同的维度，并且运行 print(training_data) 和 print(validation_data) 都给出
&lt;_TensorSliceDataset element_spec=(TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(2,), dtype=tf.int64, name=None))&gt;

如何正确设置验证数据以与 model.fit 内联运行？]]></description>
      <guid>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</guid>
      <pubDate>Tue, 29 Oct 2024 21:59:29 GMT</pubDate>
    </item>
    <item>
      <title>如何对多个特征应用多个估计量来选择具有最高 f1 分数的组合？</title>
      <link>https://stackoverflow.com/questions/79134937/how-to-apply-multiple-estimator-on-multiple-number-of-features-to-select-the-com</link>
      <description><![CDATA[我想对多个特征使用多个估计器算法运行递归特征消除，并在测试数据上保留最高的 f1 分数组合。
如何创建一个代码，可以在测试数据上生成并显示具有最佳算法的最佳特征数量（最高 f1，其次是最高 ROC），而不是逐一查看结果？我的数据集不平衡。
我尝试了下面的代码，它可以为不同数量的特征生成不同估计器的结果。但我仍然需要逐一查看结果。如何做到这一点？
estimators = [(&#39;Logistic Regression&#39;, LogisticRegression()),
(&#39;Random Forest&#39;, RandomForestClassifier())]
num_features_to_select = [4,5,7,9,11,15]

for estimator_name, estimator in estimators:
for n_features in num_features_to_select:
# 创建 RFE 对象
rfe = RFE(estimator=estimator, n_features_to_select=n_features)
# 将 RFE 与数据拟合
rfe.fit(X_resampled,Y_resampled)
# 获取选定的特征
selector = X_resampled.columns[rfe.support_]
X_train_selected = X_resampled[selector]
X_test_selected = X_test[selector]
log_reg_model = sm.Logit(Y_resampled, X_train_selected).fit()
pred_test = log_reg_model.predict(X_test_selected)
pred_test_1 = np.where(pred_test&gt;0.5,1,0)
logit_roc_auc = roc_auc_score(Y_test, pred_test)
fpr, tpr, Thresholds = roc_curve(Y_test, pred_test)
precision, recall, Thresholds = precision_recall_curve(Y_test, pred_test)
# 使用交叉验证评估模型性能
scores = cross_val_score(estimator, X_resampled[selected_features], Y_resampled, cv=5)
# 打印结果
print(f&#39;Estimator: {estimator_name}, 特征数量: {n_features}, 平均 CV 分数: {scores.mean()}&#39;)
print(f&#39;估计器：{estimator_name}，特征数量：{n_features}，ROC：{logit_roc_auc}&#39;)
print(f&#39;估计器：{estimator_name}，特征数量：{n_features}，f1 分数：{f1_score(Y_test, pred_test_1)}&#39;)
print(f&#39;估计器：{estimator_name}，特征数量：{n_features}，PRC AUC：{auc(recall,precision)}&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79134937/how-to-apply-multiple-estimator-on-multiple-number-of-features-to-select-the-com</guid>
      <pubDate>Mon, 28 Oct 2024 19:57:20 GMT</pubDate>
    </item>
    <item>
      <title>DSPy 无法检索 ChromaDB 中带有文本嵌入的段落</title>
      <link>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</link>
      <description><![CDATA[我正在使用 DSPy 和 ChromaDB 为 pdf 文件开发 RAG 应用程序。
首先，我从 pdf 中获取文本并将其作为块添加到 Chromadb。还添加了块的嵌入。并尝试使用 DSPy 检索与查询相关的块。但是它出现了错误
存储数据和嵌入
def store_document_in_chromadb(text):
chunks = chunk_document(text)
ids = [f&#39;chunk_{i}&#39; for i in range(len(chunks))]
embeddings = [get_embedding(chunk).tolist() for chunk in chunks]

collection.add(ids=ids, documents=chunks, embeddings=embeddings)

我尝试像这样检索相关块，
retriever_model = ChromadbRM(&quot;contracts_collection&quot;, &#39;db/&#39;, k=2)
dspy.settings.configure(lm=llama2_model, rm=retriever_model)

class GenerateAnswer(dspy.Signature): 
“”“”根据给出的上下文回答问题。“”“”
context = dspy.InputField(desc=&quot;可能包含相关上下文&quot;)
question = dspy.InputField()
answer = dspy.OutputField(desc=&quot;通常为 5 到 10 个单词&quot;)

class RAG(dspy.Module): 
def __init__(self, num_passages=2):
super().__init__()
self.retrieve = dspy.Retrieve(k=num_passages)
self.generate_answer = dspy.ChainOfThought(GenerateAnswer)

def forward(self, question):
context = self.retrieve(question).passages
prediction = self.generate_answer(context=context, question=question)
return dspy.Prediction(context=context, answer=prediction.answer)

with dspy.context(lm=llama2_model, rm=retriever_model):
module = RAG()
response = module(&quot;What is the Total Spend&quot;)
print(response)

当我运行此程序时，出现此错误
InvalidDimensionException：嵌入维度 384 与集合维度 768 不匹配
但是当我从 ChromaDB 中删除嵌入时，它会正确检索相关块。
为什么使用嵌入时没有出现此错误？]]></description>
      <guid>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</guid>
      <pubDate>Wed, 17 Jul 2024 08:03:30 GMT</pubDate>
    </item>
    <item>
      <title>layoutlmv3：尽管已完成推理，但 postprocess 方法仍未返回超过 512 个标记的数据</title>
      <link>https://stackoverflow.com/questions/76899333/layoutlmv3-issue-with-postprocess-method-not-returning-data-beyond-512-tokens-d</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76899333/layoutlmv3-issue-with-postprocess-method-not-returning-data-beyond-512-tokens-d</guid>
      <pubDate>Mon, 14 Aug 2023 13:24:26 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降线性回归后非正则化 theta</title>
      <link>https://stackoverflow.com/questions/59015958/denormalizing-thetas-after-a-linear-regression-with-gradient-descent</link>
      <description><![CDATA[我有以下一组数据：
km,price
240000,3650
139800,3800
150500,4400
185530,4450
176000,5250
114800,5350
166800,5800
89000,5990
144500,5999
84000,6200
82029,6390
630 60,6390
74000,6600
97500,6800
67000,6800
76025,6900
48235,6900
93000,6990
60949,7490
65674,7555
54000,7990
68500,7990
22899,7990
61789,8290

之后对它们进行归一化，我执行梯度下降，得到以下 theta：
θ0 = 0.9362124793084768
θ1 = -0.9953762249792935

如果我输入归一化里程，然后对预测价格进行非归一化，我可以正确预测价格，即：
50000 公里里程的要价：
归一化里程：0.12483129971764294
归一化价格：(mx + c) = 0.8119583714362707
实际价格：7417.486843464296

我正在寻找的是恢复我的西塔回到非标准化值，但无论我尝试哪个方程，我都无法做到这一点。有办法吗？]]></description>
      <guid>https://stackoverflow.com/questions/59015958/denormalizing-thetas-after-a-linear-regression-with-gradient-descent</guid>
      <pubDate>Sun, 24 Nov 2019 08:36:39 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit-learn 对文本进行标记</title>
      <link>https://stackoverflow.com/questions/29980037/tokenizing-text-with-scikit-learn</link>
      <description><![CDATA[我有以下代码从一组文件（文件夹名称是类别名称）中提取特征以进行文本分类。
import sklearn.datasets
from sklearn.feature_extraction.text import TfidfVectorizer

train = sklearn.datasets.load_files(&#39;./train&#39;, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decrypt_error=&#39;strict&#39;, random_state=0)
print len(train.data)
print train.target_names

vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train.data)

它抛出以下堆栈跟踪：
回溯（最近一次调用最后一次）：
文件“C:\EclipseWorkspace\TextClassifier\main.py”，第 16 行，位于&lt;module&gt;
X_train = vectorizer.fit_transform(train.data)
文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 1285 行，位于 fit_transform
X = super(TfidfVectorizer, self).fit_transform(raw_documents)
文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 804 行，位于 fit_transform
self.fixed_vocabulary_)
文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 739 行，位于 _count_vocab
for feature in analyze(doc):
文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 236 行，位于 &lt;lambda&gt;
tokenize(preprocess(self.decode(doc))), stop_words)
文件“C:\Python27\lib\site-packages\sklearn\feature_extraction\text.py”，第 113 行，在解码中
doc = doc.decode(self.encoding, self.decode_error)
文件“C:\Python27\lib\encodings\utf_8.py”，第 16 行，在解码中
return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: &#39;utf8&#39; 编解码器无法解码位置 32054 中的字节 0xff：起始字节无效

我运行的是 Python 2.7。我该如何让它工作？
编辑：
我刚刚发现，这对于使用 utf-8 编码的文件非常有效（我的文件是 ANSI 编码）。有什么方法可以让 sklearn.datasets.load_files() 使用 ANSI 编码吗？]]></description>
      <guid>https://stackoverflow.com/questions/29980037/tokenizing-text-with-scikit-learn</guid>
      <pubDate>Fri, 01 May 2015 00:39:09 GMT</pubDate>
    </item>
    </channel>
</rss>