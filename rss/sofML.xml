<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 31 Jul 2024 03:17:29 GMT</lastBuildDate>
    <item>
      <title>优化序列以最小化涉及求和与序列长度的自定义评分函数</title>
      <link>https://stackoverflow.com/questions/78814217/optimizing-a-sequence-to-minimize-a-custom-score-function-involving-summation-an</link>
      <description><![CDATA[背景：
这是我随机想到的一个问题，我觉得它很有趣，我暂时想不出可行的解决方案。
正文：
问题定义
我试图找到一个序列 S，使得对于从 1 到 n 的每个数字 i，使用以下公式计算并最小化分数 f(i)：
f(i) = a * |S| + (1-a) * N(S, i)
其中：

|S| 是序列 S 的长度。
N(S, i) 是 S 中加起来达到目标​​ i 所需的最小元素数。序列 S 中的元素可以重复使用。
a 是一个介于 0 和 1 之间的参数，用于平衡 |S| 的贡献并将 N(S, i) 添加到分数中。

目标是找到这样一个序列 S，使得从 1 到 n 的每个 i 产生的最大分数 f(i) 尽可能低。
我尝试过的方法
我最初尝试使用蛮力方法来生成可能的序列并对其进行评估，但这种方法计算成本高，并且对于较大的数字（例如 n = 20 及以上）不可行。
问题

是否有更有效的算法或方法来解决这个问题，可能使用动态规划或其他优化技术？是否有类似于此问题结构的已知问题或数学框架，可以指导解决方案的开发？你们有办法解决这个问题吗？

想法
我考虑过使用随机近似来近似解决方案，但我无法执行这个想法。
观察：

我不知道如何证明，但 {1,2,3,…,N} 序列可以形成 1 到 (1+N)N/2 之间的任何数字。因此，如果 a=1，则最佳序列应为 {1,2,3,…,x}，其中 argminx (1+x)x/2 &gt;= target

如果 a=0，(1-a)=1，则 N(S,i) 始终为 1，因为我们可以形成一个序列 {1,2,3,…,T}，这样我们就可以仅使用单个数字本身来表示低于目标的任何数字

根据观察 1 和观察 2，我猜测最佳序列应介于 {1,2,…x} 和 {1,2,…,T} 的组合之间，并且总共会有 T-x+1 次这样的迭代。


]]></description>
      <guid>https://stackoverflow.com/questions/78814217/optimizing-a-sequence-to-minimize-a-custom-score-function-involving-summation-an</guid>
      <pubDate>Wed, 31 Jul 2024 02:23:03 GMT</pubDate>
    </item>
    <item>
      <title>Pandas-Profiling 与 scikit-learn 发生冲突</title>
      <link>https://stackoverflow.com/questions/78814203/pandas-profiling-being-conflicted-with-scikit-learn</link>
      <description><![CDATA[当我尝试在 jupyter notebook 中安装 pandas profiling 时，我遇到一个错误，如下所示：“错误：pip 的依赖解析器目前没有考虑到所有已安装的软件包。此行为是以下依赖冲突的根源。
scikit-learn 1.5.1 需要 joblib&gt;=1.2.0，但您的 joblib 1.1.1 不兼容。”
现在，当我尝试升级 joblib 的版本时，我会遇到新的错误，如下所示：“错误：pip 的依赖解析器目前没有考虑到所有已安装的软件包。此行为是以下依赖冲突的根源。
pandas-profiling 3.2.0 需要 joblib~=1.1.0，但您的 joblib 1.4.2 不兼容。”
再次升级 irreparabled-learn &amp; scikit-learn 会给我同样的不兼容错误。
我该如何缓解这个错误
我希望 pandas profiling 能够正确安装，没有任何冲突和错误]]></description>
      <guid>https://stackoverflow.com/questions/78814203/pandas-profiling-being-conflicted-with-scikit-learn</guid>
      <pubDate>Wed, 31 Jul 2024 02:12:55 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn 预测需要很长时间</title>
      <link>https://stackoverflow.com/questions/78814028/sklearn-prediction-takes-forever</link>
      <description><![CDATA[我已经被这个问题困扰好几天了，真的需要一些帮助。我在 sklearn 中遇到了几种常见的机器学习方法的严重性能问题。我正在研究一个概率预测（二元分类）问题，数据集包含 500 万个观测值和 100 个特征，使用 sklearn 中的 LogisticRegression()、MLPClassifier()、RandomForestClassifier() 和 LinearSVC() 等模型。
例如，这是我用于 L2 逻辑回归的设置，使用交叉验证从网格 c_grid = [1e-15, 1e-10, 1e-5, 1e-1, 10] 中找到最佳正则化项 C：
lr = LogisticRegression(class_weight=class_weight,
solver=&#39;sag&#39;, # 我也尝试了 &#39;liblinear&#39;
max_iter=10000,
tol=0.1,
random_state=seed,
penalty=&#39;l2&#39;)

C = [1e-15, 1e-10, 1e-5, 1e-1, 10]
c_grid = {&quot;C&quot;: C}
c_grid = {k: v for k, v in c_grid.items() if v is not None}

...

cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True) 
clf = GridSearchCV(estimator=lr, 
param_grid=c_grid, 
scoring=&#39;roc_auc&#39;,
cv=cv, 
return_train_score=True).fit(X_train, Y_train) 
best_model = clf.best_estimator_
prob = clf.predict_proba(X_train)[:, 1]
pred = clf.predict(X_train)

但是整个训练过程耗时将近20个小时。对于这种大小的数据集来说，这是否正常，或者可能是由于参数或设置不正确造成的？例如，我调整了 LogisticRegression 中的各种参数，但似乎都没有改善这种情况。
此外，当我尝试使用 best_model 来计算测试结果时
prob = clf.predict_proba(X_test)[:, 1]
pred = clf.predict(X_test)

这似乎需要很长时间才能完成。我尝试使用类似这样的方法并行化该过程
X_test_batches = np.array_split(X_test, N)
args = [(best_model, batch) for batch in X_test_batches]

with Pool(N) as pool:
prob_batches = pool.map(predict_batch, args)
prob = np.concatenate(prob_batches)
pred = (prob &gt;= 0.5)

但它也没有太大帮助，所以最终我不得不手动实现我自己的预测函数（显然它只适用于物流，但不适用于我想要测试的其他模型）
z = np.dot(X_test, best_model.coef_.T) + best_model.intercept_
prob = 1 / (1 + np.exp(-z))

鉴于训练和测试都花费了不合理的长时间，我猜测问题可能出在 clf.predict_proba() 和 clf.predict() 上。但是，我希望 sklearn 能够有效处理包含数百万个观测值的数据集？如能得到任何帮助，我将不胜感激，谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78814028/sklearn-prediction-takes-forever</guid>
      <pubDate>Wed, 31 Jul 2024 00:04:22 GMT</pubDate>
    </item>
    <item>
      <title>如何让 Python 脚本在后台运行同时仍与前端交互？[关闭]</title>
      <link>https://stackoverflow.com/questions/78813752/how-to-have-a-python-script-run-in-the-background-while-still-interacting-with-t</link>
      <description><![CDATA[嘿，我有这个网站，它允许人们添加新的预测。首先，他们点击一个新的预测和广告数据（csvs），然后选择他们想要使用的机器学习模型，然后它应该运行python代码并将csv输出到具有相关预测参数的数据库。
我的问题是我应该如何运行python代码，因为这个代码可能需要几个小时才能运行，然后我必须考虑许多试图做出新预测的用户。我研究了任务调度和redis。我也听说我可以使用一些AWS服务，但我不确定这里最好的选择是什么，因为我想尽可能地防止内存错误和超时，同时确保代码在稳定的环境中运行。
顺便说一下，我使用flask作为后端并在前端做出反应]]></description>
      <guid>https://stackoverflow.com/questions/78813752/how-to-have-a-python-script-run-in-the-background-while-still-interacting-with-t</guid>
      <pubDate>Tue, 30 Jul 2024 21:37:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 opencv 检测该焊接带中的缺陷（例如孔洞）[关闭]</title>
      <link>https://stackoverflow.com/questions/78813507/how-can-i-detect-defectssuch-as-holes-in-this-welding-strip-using-opencv</link>
      <description><![CDATA[参考图：带孔的焊条：https://i.sstatic.net/VaVQX3th.jpg
正常焊条：https://i.sstatic.net/MBcyyIyp.jpg
我在使用 opencv 检测缺陷（如孔、不均匀性）时遇到问题。我是 opencv 新手，尝试过轮廓检测、边缘检测，但没有得到想要的结果。我想使用 opencv 构建一个算法，检测这些孔并标记它们，而不标记任何其他不必要的东西，这些东西不是缺陷。
这是我在代码中使用的方法
import cv2
import numpy as np
from matplotlib import pyplot as plt

# 加载图像
image_path = &quot;sample weld strip.jpg&quot;
image = cv2.imread(image_path)

# 将图像转换为 HSV 颜色空间
hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

# 定义焊缝条的 HSV 范围
lower_hsv = (1, 1, 1)
upper_hsv = (177, 255, 255)

# 应用 HSV 掩码
mask = cv2.inRange(hsv_image, lower_hsv, upper_hsv)
masked_image = cv2.bitwise_and(image, image, mask=mask)

# 转换为灰度
gray_image = cv2.cvtColor(masked_image, cv2.COLOR_BGR2GRAY)

# 应用高斯模糊
blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)

# 使用 Canny 进行边缘检测
edges = cv2.Canny(blurred_image, 50, 150)

# 查找轮廓
contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 在重要轮廓（孔洞）周围绘制边界框
holes_image = image.copy()
for contour in contours:
area = cv2.contourArea(contour)
if 10 &lt;area &lt; 25：# 根据您的需要调整此阈值
x, y, w, h = cv2.boundingRect(contour)
cv2.rectangle(holes_image, (x, y), (x+w, y+h), (0, 0, 255), 2)

我正在寻求有关此问题的帮助或指导。任何形式的意见或帮助都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78813507/how-can-i-detect-defectssuch-as-holes-in-this-welding-strip-using-opencv</guid>
      <pubDate>Tue, 30 Jul 2024 20:06:30 GMT</pubDate>
    </item>
    <item>
      <title>如何进行布尔分类处理</title>
      <link>https://stackoverflow.com/questions/78813351/how-to-boolean-categorical-proccessing</link>
      <description><![CDATA[将 pandas 导入为 pd
从 sklearn.impute 导入 SimpleImputer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler、OneHotEncoder、OrdinalEncoder
从 sklearn.pipeline 导入 Pipeline

data = pd.read_csv(&#39;Datasets/StudentScore.csv&#39;)

target = &#39;MathScore&#39;
x = data.drop(data[[target, &#39;Unnamed: 0&#39;]], axis=1)
y = data[target]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

# 数值处理
num_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)),
(&#39;scaler&#39;, StandardScaler())
])

x_train[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]] = num_transformer.fit_transform(x_train[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]])
x_test[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]] = num_transformer.transform(x_test[[&#39;ReadingScore&#39;, &#39;WritingScore&#39;]])

# 序数处理
education_levels = [&quot;high school&quot;, &quot;some high school&quot;, &quot;some college&quot;, &quot;associate&#39;s degree&quot;, &quot;bachelor&#39;s degree&quot;,
&quot;master&#39;s degree&quot;]

ord_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encoder&#39;, OrdinalEncoder(categories=[education_levels])),
])

x_train[[&#39;ParentEduc&#39;]] = ord_transformer.fit_transform(x_train[[&#39;ParentEduc&#39;]])
x_test[[&#39;ParentEduc&#39;]] = ord_transformer.transform(x_test[[&#39;ParentEduc&#39;]])

# 名义处理
nom_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encoder&#39;, OneHotEncoder())
])

x_train[[&#39;EthnicGroup&#39;]] = nom_transformer.fit_transform(x_train[[&#39;EthnicGroup&#39;]])
x_test[[&#39;EthnicGroup&#39;]] = nom_transformer.transform(x_test[[&#39;EthnicGroup&#39;]])

# 布尔处理
bool_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
(&#39;encoder&#39;, OneHotEncoder(sparse_output=False)),
])

x_train[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]] = bool_transformer.fit_transform(
x_train[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]])
x_test[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]] = bool_transformer.transform(x_train[[&#39;Gender&#39;, &#39;LunchType&#39;, &#39;TestPrep&#39;]])


我在尝试创建管道来处理布尔分类特征时遇到错误。具体来说，在训练集和测试集中的特征的 fit_transform 步骤中，我在 #nominal processing 和 #boolean processing 部分中收到了“ValueError：列的长度必须与键的长度相同”。你能帮我吗？谢谢！
`
如上所述：我在尝试创建管道来处理布尔分类特征时遇到错误。具体来说，在训练集和测试集中的特征的 fit_transform 步骤中，我在 #nominal processing 和 #boolean processing 部分中收到了“ValueError：列的长度必须与键的长度相同”。你能帮我吗？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78813351/how-to-boolean-categorical-proccessing</guid>
      <pubDate>Tue, 30 Jul 2024 19:12:18 GMT</pubDate>
    </item>
    <item>
      <title>将两类图像分类器结合在一起</title>
      <link>https://stackoverflow.com/questions/78813144/combine-two-class-of-image-classifier-together</link>
      <description><![CDATA[我制作了两个模型，一个用于狗与猫的分类（它还告诉品种），另一个用于车辆分类（它还告诉汽车的型号），有没有办法将这两个文件结合起来，以便我可以用它来预测我想要的东西（使用 API 的概念）
我的意思是说，在进行预测时，我需要在特定模型中特别上传文件，但我想知道是否有第三种方法，我只需要上传图像，然后它就会提供预测]]></description>
      <guid>https://stackoverflow.com/questions/78813144/combine-two-class-of-image-classifier-together</guid>
      <pubDate>Tue, 30 Jul 2024 18:08:20 GMT</pubDate>
    </item>
    <item>
      <title>如何判断汽车配置的相似性？</title>
      <link>https://stackoverflow.com/questions/78811479/how-to-determine-the-similarity-of-a-cars-configuration</link>
      <description><![CDATA[有一个车辆配置的数据集：

ID
品牌
型号
代数
发动机类型
发动机排量
气缸数
车身类型
变速箱类型
发动机代码
制造年份

任务：
确定一个配置与另一个配置的相似程度。
我假设将数据集中的每个条目表示为一个向量，并计算向量之间的余弦相似度。
但是对于如何以数值形式表示值存在误解，例如车身类型：轿车、跨界车、轿跑车等。]]></description>
      <guid>https://stackoverflow.com/questions/78811479/how-to-determine-the-similarity-of-a-cars-configuration</guid>
      <pubDate>Tue, 30 Jul 2024 11:38:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 opencv 提取验证码</title>
      <link>https://stackoverflow.com/questions/78810349/extract-captcha-with-opencv</link>
      <description><![CDATA[我需要提取验证码，我正在研究 opencv。我的目标是可靠地解决这种形式的验证码。
原始验证码
我的临时解决方案是：

用阈值转换二值图像
检测凸面
删除网格

但是现在我不知道下一步该怎么写代码。
有人能给我提供这个问题的关键字或解决方案吗？
我的代码：
import cv2
import numpy as np

image_path = &#39;captcha_wb.png&#39;
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

scale_percent = 200
width = int(image.shape[1] * scale_percent / 100)
height = int(image.shape[0] * scale_percent / 100)
dim = (width, height)
resized_image = cv2.resize(image, dim, interpolation = cv2.INTER_LINEAR)

# kernel = np.ones((2, 2), np.uint8) 

# 使用 cv2.erode() 方法 
# erode_image = cv2.erode(resized_image, kernel, cv2.BORDER_REFLECT)

_,thresholded_image = cv2.threshold(resized_image, 128, 255, cv2.THRESH_BINARY)

# contours, _= cv2.findContours(thresholded_image, cv2.RETR_TREE, 
# cv2.CHAIN_APPROX_SIMPLE) 

cv2.imshow(&#39;Adaptive Gaussian&#39;,thresholded_image)

如果 cv2.waitKey(0) &amp; 0xff == 27: 
cv2.destroyAllWindows() 
]]></description>
      <guid>https://stackoverflow.com/questions/78810349/extract-captcha-with-opencv</guid>
      <pubDate>Tue, 30 Jul 2024 07:38:09 GMT</pubDate>
    </item>
    <item>
      <title>CVAE 合成数据分布范围过窄</title>
      <link>https://stackoverflow.com/questions/78809995/cvae-synthetic-data-distributed-too-narrowly</link>
      <description><![CDATA[我有一个包含三个特征的数据集，两个浮点特征和一个具有 33 个类别的分类特征。（此处称为 Float_A、Float_B 和 Cat_A）。
我正在尝试训练 CVAE 以生成合成数据。使用以下 sklearn 转换器转换数据：
df=df[[&quot;float_A&quot;,&quot;float_B&quot;,&quot;categorical_A&quot;]]

transformers=[(&#39;float_A&#39;,Pipeline(steps=[(&#39;imputer&#39;,SimpleImputer(strategy=&#39;mean&#39;,add_indicator=True)),
(&#39;scaler&#39;,RobustScaler(quantile_range=(5,95)))]),
[&#39;float_A&#39;]),
(&#39;float_B&#39;,
Pipeline( steps=[(&#39;imputer&#39;,SimpleImputer(strategy=&#39;mean&#39;,add_indicator=True)),
(&#39;scaler&#39;,MinMaxScaler())]),
[&#39;float_B&#39;]),
(&#39;cats&#39;,OneHotEncoder(),categorical_columns)]`

transformer=ColumnTransformer(transformers,remainder=&#39;passthrough&#39;)

transformed_df=transformer.fit_transform(df)

我的第二个浮点数有一个 S 形激活函数，声明如下：
Def sample(self,z):
reconstructed=self.decoder(z)
# 将 S 形激活应用于浮点数特征。
reconstructed[:,self.float_B_idx]=torch.sigmoid(reconstructed[:,self.float_B_idx])
returnreconstructed

Def forward(self,x):
z_mean,z_log_var=torch.chunk(self.encoder(x),2,dim=1)
z=self.reparameterize(z_mean,z_log_var)
reconstructed=self.decoder(z)
#将 sigmoid 激活应用于浮点特征。
reconstructed[:,self.float_B_idx]=torch.sigmoid(reconstructed[:,self.float_B_idx])
return reconstructed,z_mean,z_log_var

一旦 CVAE 经过训练（训练和验证损失似乎按应有的方式减少），我尝试使用以下方法生成随机样本：
random_latent_vectors=torch.randn(num_samples,latent_dim)

使用 torch.no_grad()：
gen_df=model.sample(random_latent_vectors).detach().cpu().numpy()

但是 gen_df 中的所有样本都非常“未展开”。
FloatA、FloatB、 Cat[0:2]…

[[0.11782782 0.286538 0.646666 0.266387 0.09747571]
[0.0963359 0.29775462 0.58443785 0.29296008 0.1101962]
[0.1300626 0.31274286 0.59086925 0.30710378 0.10169853]
[0.1232817 0.32317564 0.56470346 0.29102385 0.11446829]
[0.13240162 0.28100765 0.6230704 0.29497638 0.08924796]]

然后，当我在 gen_df 上调用 scaler.inverse_transform 时，我几乎在每一行上都得到了相同的结果。
我尝试了各种方法，我的一个类别非常占主导地位（~90%），因此使用 imblearn 进行了一些类别不平衡欠采样，使其仅占 50% 的主导地位，但仍然获得 100% 的样本。
我尝试为我的 CVAE 添加更多层和复杂性，但再次被证明是徒劳的。]]></description>
      <guid>https://stackoverflow.com/questions/78809995/cvae-synthetic-data-distributed-too-narrowly</guid>
      <pubDate>Tue, 30 Jul 2024 05:57:45 GMT</pubDate>
    </item>
    <item>
      <title>RNN 建模数据准备</title>
      <link>https://stackoverflow.com/questions/78809490/rnn-modelling-data-preparation</link>
      <description><![CDATA[我正在准备用于 rnn 模型的顺序数据，但我将时间数据放在不同的列中，其中天数格式为 0 表示工作日，1 表示周末。时间是否应采用单一数据格式列以用于模型？
此外，我还应该如何准备数据以计算与传感器数据的距离。我添加了数据和距离问题的屏幕截图。
在此处输入图片说明
在此处输入图片说明]]></description>
      <guid>https://stackoverflow.com/questions/78809490/rnn-modelling-data-preparation</guid>
      <pubDate>Tue, 30 Jul 2024 01:26:23 GMT</pubDate>
    </item>
    <item>
      <title>通过预训练模型预测对象不起作用</title>
      <link>https://stackoverflow.com/questions/78809007/predicting-an-object-over-an-pretrained-model-is-not-working</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78809007/predicting-an-object-over-an-pretrained-model-is-not-working</guid>
      <pubDate>Mon, 29 Jul 2024 20:50:17 GMT</pubDate>
    </item>
    <item>
      <title>TFLM“Interpreter->Invoke()”问题导致硬故障</title>
      <link>https://stackoverflow.com/questions/78808999/issues-with-tflm-interpreter-invoke-causing-hard-fault</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78808999/issues-with-tflm-interpreter-invoke-causing-hard-fault</guid>
      <pubDate>Mon, 29 Jul 2024 20:48:04 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降应用</title>
      <link>https://stackoverflow.com/questions/78804107/gradient-descent-application</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78804107/gradient-descent-application</guid>
      <pubDate>Sun, 28 Jul 2024 15:15:45 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit-learn 在朴素贝叶斯分类器中混合分类数据和连续数据</title>
      <link>https://stackoverflow.com/questions/14254203/mixing-categorial-and-continuous-data-in-naive-bayes-classifier-using-scikit-lea</link>
      <description><![CDATA[我正在使用 Python 中的 scikit-learn 开发一种分类算法来预测某些客户的性别。除此之外，我想使用朴素贝叶斯分类器，但我的问题是我有分类数据（例如：“在线注册”、“接受电子邮件通知”等）和连续数据（例如：“年龄”、“会员期限”等）的混合。我以前没有用过 scikit，但我认为高斯朴素贝叶斯适合连续数据，而伯努利朴素贝叶斯可用于分类数据。但是，由于我想在我的模型中同时拥有分类数据和连续数据，我真的不知道如何处理这个问题。任何想法都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/14254203/mixing-categorial-and-continuous-data-in-naive-bayes-classifier-using-scikit-lea</guid>
      <pubDate>Thu, 10 Jan 2013 09:08:22 GMT</pubDate>
    </item>
    </channel>
</rss>