<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 10 May 2024 15:14:25 GMT</lastBuildDate>
    <item>
      <title>MNIST - mnist.train_images() 问题 - HTTPError: Forbidden</title>
      <link>https://stackoverflow.com/questions/78460997/mnist-problem-with-mnist-train-images-httperror-forbidden</link>
      <description><![CDATA[我目前正在学习神经网络，我想使用 train_images() 函数，但我无法这样做。如果我运行以下代码：
导入 mnist

图像 = mnist.train_images()

，我会得到：
runfile(&#39;C:/Users/deriv/untitled0.py&#39;, wdir=&#39;C:/Users/deriv&#39;)
回溯（最近一次调用最后一次）：

  compat_exec 中的文件 ~\anaconda3\Lib\site-packages\spyder_kernels\py3compat.py:356
    exec（代码、全局变量、局部变量）

  文件 c:\users\deriv\untitled0.py:3
    图像 = mnist.train_images()

  train_images 中的文件 ~\anaconda3\Lib\site-packages\mnist\__init__.py:161
    返回 download_and_parse_mnist_file(&#39;train-images-idx3-ubyte.gz&#39;)

  download_and_parse_mnist_file 中的文件 ~\anaconda3\Lib\site-packages\mnist\__init__.py:143
    fname = download_file(fname, target_dir=target_dir, force=force)

  download_file 中的文件 ~\anaconda3\Lib\site-packages\mnist\__init__.py:59
    urlretrieve(url, target_fname)

  urlretrieve 中的文件 ~\anaconda3\Lib\urllib\request.py:241
    将 contextlib.close(urlopen(url, data)) 作为 fp：

  urlopen 中的文件 ~\anaconda3\Lib\urllib\request.py:216
    返回 opener.open(url, 数据, 超时)

  文件 ~\anaconda3\Lib\urllib\request.py:525 打开
    响应=方法（请求，响应）

  http_response 中的文件 ~\anaconda3\Lib\urllib\request.py:634
    响应 = self.parent.error(

  文件 ~\anaconda3\Lib\urllib\request.py:563 错误
    返回 self._call_chain(*args)

  _call_chain 中的文件 ~\anaconda3\Lib\urllib\request.py:496
    结果 = func(*args)

  http_error_default 中的文件 ~\anaconda3\Lib\urllib\request.py:643
    引发 HTTPError(req.full_url, 代码, msg, hdrs, fp)

HTTP 错误：禁止

我使用pip install正确安装了mnist，但是，我不知道为什么** mnist.train_images()** 会导致错误。抱歉，如果这是一个简单的问题，但是它会对我有很大帮助。
我不知道是否应该直接从 http://下载文件/yann.lecun.com/exdb/mnist/。但是我无法这样做，因为我没有访问此资源的权限。]]></description>
      <guid>https://stackoverflow.com/questions/78460997/mnist-problem-with-mnist-train-images-httperror-forbidden</guid>
      <pubDate>Fri, 10 May 2024 15:02:52 GMT</pubDate>
    </item>
    <item>
      <title>我做了什么？ :) 迁移学习方法分类所需的帮助</title>
      <link>https://stackoverflow.com/questions/78460958/what-have-i-done-help-needed-in-classifying-a-transfer-learning-approach</link>
      <description><![CDATA[我想我有一些菜鸟问题，但是，我正在尝试对方法进行分类。
在这些方法的情况下，一组特征 A（源域？）通过数值方法转换为一组特征 B（目标域）。在具体情况下，这些特征是在荷载下具有预曲率的梁 (A) 和直梁 (B) 的中心线位置矢量。我有数值方法将预曲梁的中心线位置数据转换为直梁。转换后，接受过直梁训练的学习器正在估计输入数据的负载。
它是什么样的迁移学习？
我倾向于将其称为基于映射，但在文献中我发现了三个类别：

即时
基于功能
基于模型
基于关系

我假设在将数据提供给例如之前转换数据神经网络将是基于特征的。
或者我完全一无所知，这不被认为是迁移学习。
我很高兴得到你的帮助。
提前致谢并欢呼。]]></description>
      <guid>https://stackoverflow.com/questions/78460958/what-have-i-done-help-needed-in-classifying-a-transfer-learning-approach</guid>
      <pubDate>Fri, 10 May 2024 14:55:52 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：反序列化类“BatchNormalization”时出错</title>
      <link>https://stackoverflow.com/questions/78460953/typeerror-error-when-deserializing-class-batchnormalization</link>
      <description><![CDATA[TypeError：使用 config={&#39;name&#39;: &#39;bn_conv1&#39;, &#39;trainable&#39;: False, &#39;momentum&#39;: 0.99, &#39;epsilon&#39;: 1e-05, &#39;center&#39;: True 反序列化类 &#39;BatchNormalization&#39; 时出错， &#39;scale&#39;：True，&#39;beta_initializer&#39;：{&#39;class_name&#39;：&#39;零&#39;，&#39;config&#39;：{}}，&#39;gamma_initializer&#39;：{&#39;class_name&#39;：&#39;Ones&#39;，&#39;config&#39;：{}}，&#39;moving_mean_initializer &#39;：{&#39;class_name&#39;：&#39;零&#39;，&#39;config&#39;：{}}，&#39;moving_variance_initializer&#39;：{&#39;class_name&#39;：&#39;Ones&#39;，&#39;config&#39;：{}}，&#39;beta_regularizer&#39;：无，&#39;gamma_regularizer&#39;：无，&#39;beta_constraint&#39;：无，&#39;gamma_constraint&#39;：无，&#39;freeze&#39;：True}。
遇到异常：无法识别的关键字参数传递给 BatchNormalization：{&#39;freeze&#39;: True}
这是我在使用 ImageAI 对象检测时遇到的问题。
我真的不知道如何处理它，所以我会尝试你所说的一切。谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78460953/typeerror-error-when-deserializing-class-batchnormalization</guid>
      <pubDate>Fri, 10 May 2024 14:55:08 GMT</pubDate>
    </item>
    <item>
      <title>克利夫兰心脏病数据集 - 如何提高测试准确性？</title>
      <link>https://stackoverflow.com/questions/78460930/cleveland-heart-disease-dataset-how-to-improve-test-accurarcy</link>
      <description><![CDATA[我使用带有反向传播的多层感知器来预测心脏病，并且使用此处链接的数据集：https://drive.google.com/file/d/1ZuVXGbE6UVQFJ5ab5m1k4LzvDNTtLqYQ/view?usp=sharing
它有 303 条记录和 4 个输出类别 (0、1、2、3、4)，以 0 到 4 的等级表示心脏病的严重程度。
数据集中存在缺失数据，我通过将缺失值替换为相应特征的平均值来处理。
这是我设置用于训练模型的参数：

隐藏层神经元数量 = 100，单隐藏层

输出层神经元数量 = 5

隐藏层激活函数=logsig

输出层激活函数=softmax

训练函数 = trainlm

学习率 = 0.001

验证失败次数上限 = 10

最大纪元 = 5000

最小梯度 = 1.99e-8


但无论我做什么——调整学习率，都不会。隐藏层等 - 测试精度保持在 55% 到 60% 之间，但训练精度可以达到 80% 以上，而且我也需要测试精度 &gt;80%。
我怎样才能实现我的目标？请帮我解决这个问题，谢谢。
%加载数据集
数据 = readtable(&#39;C:\Users\User\Desktop\processed.cleveland.csv&#39;);

% 将表格转换为矩阵
X = table2array(数据(:, 1:13)); % 假设前13列是输入特征
y = table2array(数据(:, 14)); % 假设最后一列是输出标签

% 处理缺失值（如果有）
% 用相应特征的平均值替换缺失值
X = fillmissing(X, &#39;线性&#39;); % 或“最近”

% % 标准化输入特征
% X = 归一化(X);

% Z 分数标准化
mu = nanmean(X); % 计算每个特征的平均值，忽略 NaN
西格玛 = nanstd(X); % 计算每个特征的标准差，忽略 NaN
X = (X - mu) ./ 西格玛; % 执行 Z 分数标准化

% 将数据分成训练集和测试集
cv = cvpartition(大小(X,1),&#39;HoldOut&#39;,0.5); % 30% 的数据用于测试
idxTrain = 训练(cv); % 训练集索引
idxTest = 测试(cv); % 测试集索引

X_train = X(idxTrain,:);
y_train = y(idxTrain,:);

X_test = X(idxTest,:);
y_test = y(idxTest,:);

% 定义 MLP 架构
隐藏层大小 = [100]; % 具有 10 个神经元的单个隐藏层

% 选择输出层的激活函数
输出层激活 = &#39;softmax&#39;; % Softmax 用于多类分类

% 创建 MLP 模型
网络=模式网络(hiddenLayerSize);

% 设置隐藏层的激活函数
对于 i = 1:numel(hiddenLayerSize)
    net.layers{i}.transferFcn = &#39;logsig&#39;; % 应用 ReLU
    net.layers{i}.userdata.dropoutFraction = 0.5; % 辍学分数（根据需要调整）
结尾

% 设置输出层的激活函数
net.layers{end}.transferFcn = outputLayerActivation;

% 设置训练函数
net.trainFcn = &#39;trainlm&#39;;

% 设置训练选项
net.trainParam.lr = 0.001; % 学习率
net.trainParam.max_fail = 10; % 最大验证失败次数
net.trainParam.epochs = 5000; % 最大纪元
net.trainParam.min_grad = 1.99e-8; % 最小梯度

% 使用训练数据训练 MLP 模型
net = train(net, X_train&#39;, ind2vec(y_train&#39;+1)); % &#39;+1&#39; 将标签转换为基于 1 的索引

% 使用测试数据测试训练后的模型
y_pred = net(X_test&#39;);

这是我得到的结果，希望可以作为参考：






]]></description>
      <guid>https://stackoverflow.com/questions/78460930/cleveland-heart-disease-dataset-how-to-improve-test-accurarcy</guid>
      <pubDate>Fri, 10 May 2024 14:52:24 GMT</pubDate>
    </item>
    <item>
      <title>ValueError: matmul: 输入操作数 1 的核心维度 0 不匹配，gufunc 签名为 (n?,k),(k,m?)->(n?,m?)（大小 5 与 3 不同）</title>
      <link>https://stackoverflow.com/questions/78460776/valueerror-matmul-input-operand-1-has-a-mismatch-in-its-core-dimension-0-with</link>
      <description><![CDATA[将 numpy 导入为 np
从 numpy.linalg 导入 inv
从 scipy.linalg 导入 pinv

# 定义必要的函数
def create_laplacian_from_adjacency(adj_matrix):
    Degree_matrix = np.diag(adj_matrix.sum(axis=1))

    laplacian_matrix = Degree_matrix - adj_matrix
    
    返回拉普拉斯矩阵

def dirichlet_energy(L, X):
    “”““用于平滑度量化的狄利克雷能量。”“””
    返回 np.trace(X.T @ L @ X)

def update_C(X, X_tilde, L, C, gamma, alpha, lam, J):
    “”“”使用具有主函数近似的梯度下降来更新C。
    p, k = C.shape
    C_old = np.copy(C)
    
    梯度_f = (-2 * gamma * L @ C_old @ inv(C_old.T @ L @ C_old + J) +
                  alpha * (C_old @ X_tilde - X) @ X_tilde.T +
                  2 * L @ C_old @ X_tilde @ X_tilde.T + lam * C_old @ np.ones((k, k)))
    
    # 主要函数优化步骤（简化方法）
    t = 0.01 # 学习率，需要根据实际应用进行调整
    C_new = pinv(C_old - t * 梯度_f)
    C_new = np.maximum(C_new, 0) # 强制非负性
    返回C_new

def update_X(X, L, C, alpha):
    “”“”基于更新的C来更新X(tilda)。“”“”
    inv_matrix = inv((2/alpha) * (C.T @ L @ C)) + (C.T @ C)
    X_tilde_new = inv_matrix @ C.T @ X
    返回 X_tilde_new

def fgc_algorithm(X, L, alpha, gamma, lam, iterations=5):
    “”“”执行特征图粗化算法。“”“”
    p, n = X.形状
    k = 3 # 假设粗化的降维为 3
    C = np.random.rand(p, k)*0.1
    J = np.full((k, k), 1/k)
    X_代字号 = pinv(C)@X

    对于范围内的 i（迭代）：
        C = update_C(X, X_tilde, L, C, gamma, alpha, lam, J)
        X_tilde = update_X(X, L, C, alpha)
        当前能量 = dirichlet_energy(L, X_tilde)
        print(f&quot;迭代 {i}: 狄利克雷能量 = {current_energy}&quot;)

    L_c=C.T@L@C
    返回 C、L_c、X_tilde

＃ 例子：
X = np.random.rand(5, 7) # 10 个节点的随机特征
adj_matrix = np.array([
    [0, 1, 0, 0, 0],
    [1, 0, 1, 1, 1],
    [0, 1, 0, 1, 0],
    [0, 1, 1, 0, 1],
    [1, 1, 0, 1, 0]
]）
L = create_laplacian_from_adjacency(adj_matrix) # 创建样本拉普拉斯矩阵
alpha, gamma, lam = 0.1, 1, 0.5 # 正则化参数

C、L_c、X_tilde = fgc_algorithm(X、L、alpha、gamma、lam)
print(&quot;更新的 C 矩阵：\n&quot;, C)
print(&quot;L_C 矩阵:\n&quot;, L_c)
print(&quot;更新后的特征矩阵 X(tilda):\n&quot;, X_tilde)


我正在尝试实现特色粗化图算法，但每次代码到达第 34 行时：inv_matrix = inv((2/alpha) * (C.T @ L @ C)) + (C.T @ C),
出现了上述错误。
我已经尝试检查所有内容，但根据我的说法，矩阵的尺寸是正确的，所以我不太明白为什么会出现这个问题？
如果您碰巧明白这一点，请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78460776/valueerror-matmul-input-operand-1-has-a-mismatch-in-its-core-dimension-0-with</guid>
      <pubDate>Fri, 10 May 2024 14:28:29 GMT</pubDate>
    </item>
    <item>
      <title>Keras HTR 网络在训练期间产生 inf 损失</title>
      <link>https://stackoverflow.com/questions/78460737/keras-htr-network-producing-inf-loss-during-training</link>
      <description><![CDATA[上下文：
我正在为 HTR 训练 Keras 网络。它由一个自定义 CNN 基础、2 个 BiLSTM 层、一个密集层和一个用于自定义损失计算的 CTCLayer 组成。 CTCLayer 实现取自“模型”。 此示例的部分。我使用的优化器是 Adam。
我的数据集中的所有图像都经过预处理，以便每个像素值介于 0.01 和 1.0 之间。没有 NaN 或 inf 值。
所有文本标签都会转换为整数序列，并用 0 填充到最大标签长度。例如，7 个字母的单词变为 [0, 0, 0, 1, 3, 5, 15, 19, 11, 1, 0, 0, 0]。字符的最大可能整数代码为 33。
问题：
当我尝试拟合模型时，训练损失开始下降。然而，在第一个 epoch 完成大约 80% 后，损失变为 inf 并且直到该 epoch 结束才改变。在接下来的 epoch 中也会发生同样的情况，损失首先是一个适当减小的数值，然后突然变为 inf。与不同，ctc 的训练损失为 inf，而验证损失似乎很正常，在我的例子中，验证损失也是inf。
我尝试过的：
我根据这个问题的建议尝试使用clipvalue（设置为0.5）：Keras 损失突然从小值变为 inf。
我还尝试使用clipnorm（设置为1.0），将学习率从0.001降低到0.0001，以及不同的批量大小（&lt; em&gt;64，128）。
这些都没有任何帮助。
奇怪的是，当我在数据集的一小部分（800 训练图像 + 标签和 100 验证图像 + 标签而不是 &lt; em&gt;6996 图像 + 标签用于训练，1166 用于验证），这个问题根本没有出现。
这是我第一次处理 CTC 损失，所以我假设我一定错过了一些东西。任何建议将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78460737/keras-htr-network-producing-inf-loss-during-training</guid>
      <pubDate>Fri, 10 May 2024 14:20:50 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何网站允许仅放置代码并接受来自我的手机的输入？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78459442/is-there-any-site-that-allows-to-just-place-the-code-and-accepts-input-from-my-p</link>
      <description><![CDATA[只需编辑这个即可。如果我不知道如何写或解释这个问题，我很抱歉。从昨天开始我就没有睡够。
我几个月来一直被这个问题困扰。我有一个开发应用程序的项目。我在 jupyter 笔记本中编写代码。我想将代码放在网上或服务器上，并接受来自手机的输入。
我的应用是识别图像上的疾病是否是这个。我已经编写了代码，并且它正在运行。我还在其中写入了一个单元格，它接受单个图像并使用识别的模型输出可能的结果。
现在，您可能会将我重定向到一些 Tflite 或 TensorFlow ml 模型，但我的模型是定制的，与这些东西不兼容。我使用多层感知器编写了一个定制的机器学习模型，但无法将其转换为与 Tflite 兼容的代码。
我的最后手段是开发一个应用程序，将图像发送到该网站，在那里处理我的代码，然后将结果返回到我的手机。就这么简单，但我不知道它是否简单，但我没有办法做到。
有什么网站可以让我这样做吗？或者有没有办法或其他方法可以做到这一点？
非常感谢任何回应或帮助。我们论文的截止日期是下周，我现在压力很大，也很担心。非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/78459442/is-there-any-site-that-allows-to-just-place-the-code-and-accepts-input-from-my-p</guid>
      <pubDate>Fri, 10 May 2024 10:14:31 GMT</pubDate>
    </item>
    <item>
      <title>进行线性回归时结果不佳</title>
      <link>https://stackoverflow.com/questions/78459352/bad-results-while-doing-linear-regression</link>
      <description><![CDATA[我试图使用一组数据进行多元线性回归。我尝试使用用于生成回归系数的同一组 X 来预测 Y。虽然一组数据的实际值与预测值之间的差异较小（正如预期的那样），但另一组数据的实际值与预测值之间的差异较大。两个数据集代表同一组参数（相同的物理量）。我是否做错了什么或者我可以做些什么来改进？
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd
将 statsmodels.api 导入为 sm
……
……
# 构建设计矩阵
X = np.column_stack((tir1, tir1_z, bt_diff, bt_diff_z, bt_diff_sst, s_theta, np.ones_like(tir1)))

sst = np.array([i+273.15 for i in selected_buoy_sst])

# 拟合OLS模型
模型 = sm.OLS(sst, X)
结果 = model.fit()

Predicted_sst_same_data = results.predict(X)

# 计算实际SST和预测SST之间的差异
差异 = sst - 预测的 sst_same_data`


# 使用不同的数据
X_n = np.column_stack((tir1, tir1_z, bt_diff, bt_diff_z, bt_diff_sst, s_theta, np.ones_like(tir1)))
sst = 皮肤温度数组
# 拟合OLS模型
模型 = sm.OLS(sst_n, X_n)
结果 = model.fit()

# 打印回归结果的摘要
打印（结果.summary（））

Predicted_sst_same_data = results.predict(X)

# 计算实际SST和预测SST之间的差异
差异 = sst - 预测的 sst_same_data


如果需要的话我可以上传数据。这取决于我的 x 和 Y 值吗？]]></description>
      <guid>https://stackoverflow.com/questions/78459352/bad-results-while-doing-linear-regression</guid>
      <pubDate>Fri, 10 May 2024 10:02:13 GMT</pubDate>
    </item>
    <item>
      <title>用于确定点平面象限的 Kohonen 神经网络</title>
      <link>https://stackoverflow.com/questions/78456978/kohonen-neural-network-for-determining-the-quadrant-of-a-point-plane</link>
      <description><![CDATA[我需要实现一个 Kohonen 神经网络（无老师）来确定某个点所在平面（从 1 到 8）的象限。坐标 [x, y, z] 的向量被输入到神经网络的输入端。在输出端，我们得到一个向量，通过这个向量我们可以确定该点属于哪个象限。
这是我训练神经网络的代码：
import numpy as np
import numpy.typing as npt
import matplotlib.pyplot as plt

def generate_dataset(
size: int,
high: float = 10.0
) -&gt; list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]]:
dataset = []

quadrants_multipliers = {
1: (1, 1, 1),
2: (-1, 1, 1),
3: (-1, -1, 1),
4: (1, -1, 1),
5: (1, 1, -1),
6: (-1, 1, -1),
7: (-1, -1, -1),
8: (1, -1, -1)
}

for quadrant in range(1, 8 + 1):
dataset += [
(
np.array([[x, y, z]]) * quadrants_multipliers[quadrant],
np.eye(8)[quadrant - 1].reshape(1, 8)
)对于 zip 中的 x、y、z（
np.random.uniform（low=0.0，high=high，size=size），
np.random.uniform（low=0.0，high=high，size=size），
np.random.uniform（low=0.0，high=high，size=size）
)
]

返回数据集

def train（
dataset：list[tuple[npt.NDArray[npt.NDArray[float]]，npt.NDArray[npt.NDArray[float]]]]，
learning_rate：float，
epochs：int
) -&gt; npt.NDArray[npt.NDArray[float]]:

W = np.random.randn(3, 8)

对于范围（1，epochs + 1）内的 epoch：
random_indexes = np.random.choice(len(dataset), size=len(dataset), replace=False)
对于 random_indexes 中的索引：
x = dataset[index][0] # [[x, y, z]]
y = dataset[index][1]

x_normalized = x / np.sqrt(np.sum(x ** 2))

z = np.dot(x_normalized, W)

W[0][z.argmax()] += learning_rate * (x[0][0] - W[0][z.argmax()])
W[1][z.argmax()] += learning_rate * (x[0][1] - W[1][z.argmax()])
W[2][z.argmax()] += learning_rate * (x[0][2] - W[2][z.argmax()])

learning_rate = learning_rate / epoch

返回 W

def calc_accuracy(
dataset: list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]],
W: npt.NDArray[npt.NDArray[float]]) -&gt;;浮点数：
正确 = 0
对于数据集中的 x、y：
z = np.dot(x, W)
如果 z.argmax() == y.argmax()：
正确 += 1
返回 (正确 / len(数据集)) * 100

def draw_accuracy_epoch_plots(
数据集：list[list[tuple[npt.NDArray[npt.NDArray[float]], npt.NDArray[npt.NDArray[float]]]]],
epoch_number：int
) -&gt;无：
对于索引，数据集在枚举（数据集）中：
accuracy_list = []
accuracy_list_2 = []
对于范围（1，epoch_number + 1）中的时期：
W = train（数据集，learning_rate=0.7，epochs=epochs）
accuracy_list.append（calc_accuracy（数据集，W））
accuracy_list_2.append（calc_accuracy（generate_dataset（15），W））

plt.subplot（3，2，（2 * index）+ 1）
plt.plot（accuracy_list）

plt.subplot（3，2，（2 * index）+ 2）
plt.plot（accuracy_list_2）

def main（） -&gt;无：
draw_accuracy_epoch_plots([generate_dataset(250), generate_dataset(500), generate_dataset(1000)], 200)

plt.tight_layout()
plt.show()

if __name__ == &quot;__main__&quot;:
main()


在代码中，我计算了不同时期的神经网络预测准确率。结果，我得到的准确率从 0 到 45%，这对我来说不太合适。同时，神经网络经常产生 0% 的匹配。
如果改变学习率和时期没有帮助，我该如何解决这个问题？我可以添加层吗（我还没有找到这个问题的答案）？或者也许我错误地构建了学习算法？]]></description>
      <guid>https://stackoverflow.com/questions/78456978/kohonen-neural-network-for-determining-the-quadrant-of-a-point-plane</guid>
      <pubDate>Thu, 09 May 2024 21:05:03 GMT</pubDate>
    </item>
    <item>
      <title>如何将FastAI分类器集成到sklearn VotingClassifier中？</title>
      <link>https://stackoverflow.com/questions/78435090/how-to-integrate-fastai-classifier-into-sklearn-votingclassifier</link>
      <description><![CDATA[我有一堆表格数据，我成功地训练了一个 RandomForestClassifier、一个 GradientBoostingClassifier 和一个深度学习模型（来自 fastai 的表格学习器代码&gt;) 与他们一起。我在结果中注意到，每个模型在特定标签上都比其他模型做得更好，每个模型都不同。我想知道是否可以将所有模型放入 VotingClassifier （来自 sklearn 的模型）。我对 RandomForestClassifier 和 GradientBoostingClassifier 没有任何问题，但我没有找到任何有关将表格学习器放入 VotingClassifier 中的信息。可以这样做吗？]]></description>
      <guid>https://stackoverflow.com/questions/78435090/how-to-integrate-fastai-classifier-into-sklearn-votingclassifier</guid>
      <pubDate>Mon, 06 May 2024 07:21:01 GMT</pubDate>
    </item>
    <item>
      <title>如何基于掩码相乘矩阵并排除元素？</title>
      <link>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</link>
      <description><![CDATA[我有以下输入矩阵
inp_tensor = torch.tensor(
        [[0.7860, 0.1115, 0.0000, 0.6524, 0.6057, 0.3725, 0.7980, 0.0000],
        [1.0000, 0.1115, 0.0000, 0.6524, 0.6057, 0.3725, 0.0000, 1.0000]])

以及我想要排除的元素的索引（在本例中它们是零元素，但它们可以是任何值）
mask_indices = torch.tensor(
[[7, 2],
[2, 6]])

如何从与以下矩阵的乘法中排除这些元素：
my_tensor = torch.tensor(
        [[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.2566, 0.7936, 0.9408],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.2666, 0.6274, 0.2696],
        [0.4414, 0.2969, 0.8317]])

也就是说，不要将其相乘，包括这些值（本例中为零）：
a = torch.mm(inp_tensor, my_tensor)
打印（一）
张量([[1.7866, 2.5468, 1.6330],
        [2.2041、2.5388、2.3315]]）

我想排除（零）元素（以及 my_tensor 的相应行），这样它们就不会参与计算图：
inp_tensor = torch.tensor(
        [[0.7860, 0.1115, 0.6524, 0.6057, 0.3725, 0.7980]]) # 根据索引删除元素（这里是零）

my_tensor = torch.tensor(
        [[0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.2666, 0.6274, 0.2696]]) # 删除对应的零元素行

b = torch.mm(inp_tensor, my_tensor)
打印(b)
&gt;&gt;&gt;&gt;&gt;张量([[1.7866, 2.5468, 1.6330]])

inp_tensor = torch.tensor([[1.0000, 0.1115, 0.6524, 0.6057, 0.3725, 1.0000]]) # 根据索引删除元素（这里是零）

my_tensor = torch.tensor(
        [
        [0.8823, 0.9150, 0.3829],
        [0.9593, 0.3904, 0.6009],
        [0.1332, 0.9346, 0.5936],
        [0.8694, 0.5677, 0.7411],
        [0.4294, 0.8854, 0.5739],
        [0.4414, 0.2969, 0.8317]]) # 删除对应的零元素行

c = torch.mm(inp_tensor, my_tensor)
打印（三）
&gt;&gt;&gt;&gt;&gt;张量([[2.2041, 2.5388, 2.3315]])
打印（火炬.cat（[b，c]））
&gt;&gt;&gt;&gt;&gt;张量([[1.7866, 2.5468, 1.6330],
        [2.2041、2.5388、2.3315]]）

我需要它是高效的（即，没有for循环），因为我的张量非常大，并且还需要保持梯度（即，如果我调用optimizer.backward( ）更新计算图中的相关参数）
请注意，inp_tensor 的每一行都有相同数量的要删除的元素（例如，本示例中的零个元素）。因此，mask_indices 的每一行也将具有相同数量的元素（例如，本例中为 2）。]]></description>
      <guid>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</guid>
      <pubDate>Mon, 29 Apr 2024 19:07:12 GMT</pubDate>
    </item>
    <item>
      <title>sklearn ImportError：无法导入名称 plot_roc_curve</title>
      <link>https://stackoverflow.com/questions/60321389/sklearn-importerror-cannot-import-name-plot-roc-curve</link>
      <description><![CDATA[我正在尝试按照示例。但是，以下导入在 python2 和 python3 中都会出现 ImportError。
从 sklearn.metrics 导入plot_roc_curve

错误：
回溯（最近一次调用最后一次）：
  文件“”，第 1 行，在  中
导入错误：无法导入名称plot_roc_curve

python-2.7 sklearn 版本：0.20.2。
python-3.6 sklearn 版本：0.21.3。
我发现以下导入工作正常，但与 plot_roc_curve 不太一样。
from sklearn.metrics import roc_curve

plot_roc_curve 是否已弃用？有人可以尝试一下代码并让我知道 sklearn 版本是否有效吗？]]></description>
      <guid>https://stackoverflow.com/questions/60321389/sklearn-importerror-cannot-import-name-plot-roc-curve</guid>
      <pubDate>Thu, 20 Feb 2020 13:44:41 GMT</pubDate>
    </item>
    <item>
      <title>如何在Python中添加L1标准化？</title>
      <link>https://stackoverflow.com/questions/48782758/how-to-add-l1-normalization-in-python</link>
      <description><![CDATA[我正在尝试从头开始编写逻辑回归代码。在这段代码中，我认为我的成本导数是我的正则化，但我的任务是添加 L1norm 正则化。你如何在Python中添加这个？是否应该在我定义成本导数的地方添加此内容？感谢任何正确方向的帮助。
def Sigmoid(z):
    返回 1/(1 + np.exp(-z))

def 假设(theta, X):
    返回 Sigmoid(X @ theta)

def Cost_Function(X,Y,theta,m):
    hi = 假设(theta, X)
    _y = Y.reshape(-1, 1)
    J = 1/float(m) * np.sum(-_y * np.log(hi) - (1-_y) * np.log(1-hi))
    返回J

def Cost_Function_Derivative(X,Y,theta,m,alpha):
    hi = 假设(theta,X)
    _y = Y.reshape(-1, 1)
    J = alpha/float(m) * X.T @ (hi - _y)
    返回J

def Gradient_Descent(X,Y,θ,m,alpha):
    new_theta = theta - Cost_Function_Derivative(X,Y,theta,m,alpha)
    返回 new_theta

定义精度(theta):
    正确 = 0
    长度 = len(X_test)
    预测=（假设（theta，X_test）&gt; 0.5）
    _y = Y_test.reshape(-1, 1)
    正确=预测==_y
    my_accuracy = (np.sum(正确) / 长度)*100
    print (&#39;LR 精度:&#39;, my_accuracy, &quot;%&quot;)

def Logistic_Regression(X,Y,alpha,theta,num_iters):
    m = 长度（Y）
    对于范围内的 x（num_iters）：
        new_theta = Gradient_Descent(X,Y,theta,m,alpha)
        θ = 新_θ
        如果 x % 100 == 0：
            打印 #(&#39;θ: &#39;, θ)
            print #(&#39;成本：&#39;, Cost_Function(X,Y,theta,m))
    精度(θ)
ep = .012
初始_theta = np.random.rand(X_train.shape[1],1) * 2 * ep - ep
阿尔法 = 0.5
迭代次数 = 10000
Logistic_Regression(X_train,Y_train,alpha,initial_theta,迭代)
]]></description>
      <guid>https://stackoverflow.com/questions/48782758/how-to-add-l1-normalization-in-python</guid>
      <pubDate>Wed, 14 Feb 2018 08:34:48 GMT</pubDate>
    </item>
    <item>
      <title>人工神经网络相对于支持向量机有哪些优势？ [关闭]</title>
      <link>https://stackoverflow.com/questions/11632516/what-are-advantages-of-artificial-neural-networks-over-support-vector-machines</link>
      <description><![CDATA[人工神经网络 (ANN) 和 支持向量机 (SVM) 是监督机器学习和分类。通常并不清楚哪种方法对于特定项目更好，我确信答案总是“这取决于情况”。通常，会结合使用两者以及贝叶斯分类。
Stack Overflow 上已经有人提出了有关 ANN 与 SVM 的问题：

ANN 和 SVM 分类

ANN、SVM 和 KNN 分类器有什么区别？

支持向量机或用于文本处理的人工神经网络


在这个问题中，我想具体了解 ANN 的哪些方面（具体来说，多层感知器）可能需要在 SVM 上使用？我之所以问这个问题，是因为很容易回答相反的问题：支持向量机通常优于人工神经网络，因为它们避免了人工神经网络的两个主要弱点：
(1) 人工神经网络通常收敛于局部最小值而不是全局最小值，这意味着它们本质上“错过了大局”。有时（或只见树木不见森林）
(2) 如果训练持续时间太长，人工神经网络通常会过度拟合，这意味着对于任何给定的模式，人工神经网络可能会开始将噪声视为该模式的一部分。
SVM 不会遇到这两个问题。然而，支持向量机完全取代人工神经网络这一点并不明显。那么，人工神经网络相对于支持向量机有哪些具体优势可以使其适用于某些情况呢？我已经列出了 SVM 相对于 ANN 的具体优势，现在我想查看 ANN 优势列表（如果有的话）。]]></description>
      <guid>https://stackoverflow.com/questions/11632516/what-are-advantages-of-artificial-neural-networks-over-support-vector-machines</guid>
      <pubDate>Tue, 24 Jul 2012 13:59:11 GMT</pubDate>
    </item>
    <item>
      <title>用于文本处理的支持向量机或人工神经网络[关闭]</title>
      <link>https://stackoverflow.com/questions/2434536/support-vector-machine-or-artificial-neural-network-for-text-processing</link>
      <description><![CDATA[我们需要在支持向量机和快速人工神经网络 (FANN.a库）用于一些文本处理项目。
它包括上下文拼写纠正，然后将文本标记为某些短语及其同义词。
哪种方法才是正确的？或者是否有替代方案...比 FANN 和 SVM 更合适？]]></description>
      <guid>https://stackoverflow.com/questions/2434536/support-vector-machine-or-artificial-neural-network-for-text-processing</guid>
      <pubDate>Fri, 12 Mar 2010 17:24:45 GMT</pubDate>
    </item>
    </channel>
</rss>