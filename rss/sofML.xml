<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 01 Sep 2024 18:20:03 GMT</lastBuildDate>
    <item>
      <title>Pytorch `DataSet.__getitem__()` 调用时 `index` 大于 `__len__()`</title>
      <link>https://stackoverflow.com/questions/78937759/pytorch-dataset-getitem-called-with-index-bigger-than-len</link>
      <description><![CDATA[我有以下 torch 数据集（我已经用随机数生成替换了从文件中读取数据的实际代码，以使其可重现性最小）：
from torch.utils.data import Dataset
import torch 

class TempDataset(Dataset):
def __init__(self, window_size=200):

self.window = window_size

self.x = torch.randn(4340, 10, dtype=torch.float32) # None
self.y = torch.randn(4340, 3, dtype=torch.float32) 

self.len = len(self.x) - self.window + 1 # = 4340 - 200 + 1 = 4141 
# 因此，最后一个窗口起始索引 = 4140 
# 最后一个窗口的范围从 4140 到 4339，即总共 200 个元素

def __len__(self):
return self.len

def __getitem__(self, index):

# AFAIU，下面的 if 条件永远不应计算为 True，因为最后一个索引
# __getitem__ 调用应该是 self.len - 1
if index == self.len: 
print(&#39;self.__len__(): &#39;, self.__len__())
print(&#39;Tried to access eleemnt @ index: &#39;, index)

return self.x[index: index + self.window], self.y[index + self.window - 1]

ds = TempDataset(window_size=200)
print(&#39;len: &#39;, len(ds))
counter = 0 # 尚未读取任何记录
for x, y in ds:
counter += 1 # 上面的行从数据集中读取了另一条记录
print(&#39;counter: &#39;,计数器)

它打印：
len: 4141
self.__len__(): 4141
尝试访问元素 @ index: 4141
counter: 4141

据我所知，__getitem__() 被调用，index 范围从 0 到 __len__()-1。如果这是正确的，那么为什么它试图用索引 4141 调用 __getitem__()，而数据本身的长度是 4141？
我注意到的另一件事是，尽管使用 index = 4141 进行调用，但它似乎没有返回任何元素，这就是为什么 counter 停留在 4141
我的眼睛（或大脑）在这里错过了什么？
PS：虽然它不会有任何效果，只是为了确认，我还尝试用 torch DataLoader 包装 DataSet，它仍然表现相同。]]></description>
      <guid>https://stackoverflow.com/questions/78937759/pytorch-dataset-getitem-called-with-index-bigger-than-len</guid>
      <pubDate>Sun, 01 Sep 2024 15:52:18 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow fit() 函数：steps_per_epoch = 1 导致损失出现震荡</title>
      <link>https://stackoverflow.com/questions/78937446/tensorflow-fit-function-steps-per-epoch-1-causes-oscillations-in-loss</link>
      <description><![CDATA[我有一个简单的卷积神经网络，其输入大小为 (5,5,3)。训练代码添加如下。如您所见，输入是随机生成的，并且仅使用单个条目来训练模型。
tf.random.set_seed(1)
X_train = tf.random.uniform((1,5,5,3), seed = 1)
y_train = np.array([[0, .5, 0., 0., 0., .0 , 0 , 0, 0, 0 ]])

model = tf.keras.Sequential()
model.add(Conv2D(filters = 6, kernel_size=5, strides = 1,activation= &quot;relu&quot;, input_shape = (5,5,3)))
model.add(Flatten())
model.add(Dense(10,激活=&quot;sigmoid&quot;))
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

epochs = 40
history = model.fit(
X_train,
y_train,
epochs = epochs,
batch_size = 1,
steps_per_epoch = 1,
verbose = 0,
)
plt.plot(history.history[&quot;loss&quot;])

训练进行了 40 个 epoch，显然 steps_per_epoch 等于 1。但是，将 1 分配给 steps_per_epoch 会使训练过程变得奇怪。这样做会导致损失立即降至 0，然后再次上升。图片如下：
图片：steps_per_epoch = 1 时的损失
另一方面，不将 steps_per_epoch 传递给 fit 函数，或者传递 0 或大于 1 的数字，可以解决问题，我们从学习过程中看到正常行为。
图片：当我们不将 steps_per_epoch 传递给 fit 函数时产生的损失
这个问题从何而来？有人能解释一下这个问题吗？当我们只有一个训练数据并且 batch_size 为 1 时，steps_per_epoch 不应该等于 1 吗？
P.s. 我使用的是 Tensorflow 2.17.0
P.s. 我尝试过改变学习率，但没有解决问题。当 LR 降低到 0.0001 时，训练过程中的损失图如下所示：
图像：lr = 0.0001
为了解决这个问题，我做了很多尝试和错误，并意识到这是由 steps_per_epoch 等于 1 引起的。]]></description>
      <guid>https://stackoverflow.com/questions/78937446/tensorflow-fit-function-steps-per-epoch-1-causes-oscillations-in-loss</guid>
      <pubDate>Sun, 01 Sep 2024 13:29:15 GMT</pubDate>
    </item>
    <item>
      <title>在训练过程中，损失波动很大[关闭]</title>
      <link>https://stackoverflow.com/questions/78936923/during-the-training-the-loss-fluctuates-a-lot</link>
      <description><![CDATA[我正在使用神经网络 LSTM 来训练手势识别。我使用 mediapipe 来保存手的坐标，总共有 26 个符号需要识别。我的神经网络是：
model = tf.keras.models.Sequential([ #tf.keras.layers.Input((21 * 2, )), tf.keras.layers.InputLayer(input_shape=(21 * 2, )), tf.keras.layers.Reshape((21, 2), input_shape=(21 * 2, )), tf.keras.layers.Dropout(0.1), tf.keras.layers.LSTM(38, input_shape=[21, 2]), tf.keras.layers.Dropout(0.1), tf.keras.layers.Dense(34,activation=&#39;relu&#39;), tf.keras.layers.Dropout(0.1), tf.keras.layers.Dense(30,activation=&#39;relu&#39;), tf.keras.layers.Dense(NUM_CLASSES,activation=&#39;softmax&#39;)])

在此处输入图片说明
在训练过程中，损失波动很大，我不明白为什么会发生这种情况。
在此处输入图片说明
在此处输入图片说明
使用model.evaluate()，结果是：损失：0.5481 - 准确率： 0.8467
请帮帮我。如何改进我的工作？]]></description>
      <guid>https://stackoverflow.com/questions/78936923/during-the-training-the-loss-fluctuates-a-lot</guid>
      <pubDate>Sun, 01 Sep 2024 08:52:56 GMT</pubDate>
    </item>
    <item>
      <title>来自 Ollama 的 Llava 运行得太晚，无法在笔记本电脑上运行结果，无法在图像上运行以获取描述 [关闭]</title>
      <link>https://stackoverflow.com/questions/78936526/llava-from-ollama-running-too-late-to-get-the-result-on-my-laptop-running-on-an</link>
      <description><![CDATA[Ollama (LLaVA) 存在问题。我想制作一个对话式图像识别聊天机器人，所以我发现 LLaVA 可以做到这一点。但是，Neural Nine 的 YouTube 频道上的相同代码在 10 秒内即可运行，但对我来说，最多需要 3 分钟。
您可能认为这可能是由于 RAM 造成的，因为我使用的是配备 8GB DDR4 的 i5-8th Gen，而配备 16GB DDR4 的 i5-11th Gen 上的相同设置也是如此。因此，必须纠正某些问题才能提高速度。
我试图获取有关如何运行和加载包以减少时间的详细信息，如果可能的话，使用更快的代码。]]></description>
      <guid>https://stackoverflow.com/questions/78936526/llava-from-ollama-running-too-late-to-get-the-result-on-my-laptop-running-on-an</guid>
      <pubDate>Sun, 01 Sep 2024 04:03:50 GMT</pubDate>
    </item>
    <item>
      <title>数据分类不适用于 BERT 模型</title>
      <link>https://stackoverflow.com/questions/78936387/data-classification-doesnt-work-with-bert-model</link>
      <description><![CDATA[我需要使用输入 CSV 来训练模型，其中包含错误消息和错误分类。然后，当仅使用错误消息进行测试时，它应该会自动分类。
我使用了 BERT 模型，这是代码：
import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
# 加载训练数据并打印列
training_data = pd.read_excel(&quot;C:\\Users\\foobar\\Documents\\Training Data.xlsx&quot;)
print(training_data.columns) # 打印以验证列名
# 确保列名中没有空格
training_data.columns = training_data.columns.str.strip()
# 根据输出使用正确的列名
error_messages = training_data[&quot;Error Message&quot;].dropna()
error_classification = training_data[&quot;Error Classification&quot;].dropna()
# 确保长度一致
min_length = min(len(error_messages), len(error_classification))
error_messages = error_messages.iloc[:min_length]
error_classification = error_classification.iloc[:min_length]
# 使用 LabelEncoder 将标签转换为数字
label_encoder = LabelEncoder()
error_classification_encoded = label_encoder.fit_transform(error_classification)
# 将数据拆分为训练集和验证集
train_messages, val_messages, train_labels, val_labels = train_test_split(
error_messages, error_classification_encoded, test_size=0.2, random_state=42
)
# 从 Hugging Face 加载 BERT 模型和 tokenizer
tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
bert_model = TFBertModel.from_pretrained(&#39;bert-base-uncased&#39;)
# 对数据进行标记和预处理
def preprocess_text(texts):
return tokenizer(
texts.tolist(),
max_length=128,
truncation=True,
padding=&#39;max_length&#39;,
return_tensors=&#39;tf&#39;
)
train_tokens = preprocess_text(train_messages)
val_tokens = preprocess_text(val_messages)
# 定义模型架构
input_ids = tf.keras.Input(shape=(128,), dtype=tf.int32, name=&#39;input_ids&#39;)
attention_mask = tf.keras.Input(shape=(128,), dtype=tf.int32, name=&#39;attention_mask&#39;)
bert_output = bert_model([input_ids,tention_mask])[1]
x = tf.keras.layers.Dense(64,activation=&quot;relu&quot;)(bert_output)
output = tf.keras.layers.Dense(len(label_encoder.classes_),activation=&quot;softmax&quot;)(x)
model = tf.keras.Model(inputs=[input_ids,attention_mask],outputs=output)
# 编译模型
model.compile(optimizer=&quot;adam&quot;,loss=&quot;sparse_categorical_crossentropy&quot;,metrics=[&quot;accuracy&quot;])
# 将标签转换为 NumPy 数组并确保形状正确
train_labels = np.array(train_labels).astype(int).reshape(-1)
val_labels = np.array(val_labels).astype(int).reshape(-1)
# 打印形状和类型以供调试
print(f&quot;train_input shape: {train_tokens[&#39;input_ids&#39;].shape}&quot;)
print(f&quot;val_input shape: {val_tokens[&#39;input_ids&#39;].shape}&quot;)
print(f&quot;train_labels shape: {train_labels.shape}&quot;)
# 训练模型
model.fit(
[train_tokens[&#39;input_ids&#39;], train_tokens[&#39;attention_mask&#39;]],
train_labels,
validation_data=([val_tokens[&#39;input_ids&#39;], val_tokens[&#39;attention_mask&#39;]], val_labels),
epochs=10
)
# 加载测试数据
testing_data = pd.read_excel(&quot;C:\\Users\\foobar\\Documents\\Testing Data.xlsx&quot;)
test_messages = testing_data[&quot;Error Message&quot;]
# 预处理测试数据
test_tokens = preprocess_text(test_messages)
# 对测试数据执行预测
predictions = model.predict([test_tokens[&#39;input_ids&#39;], test_tokens[&#39;attention_mask&#39;]])
predicted_labels = np.argmax(predictions, axis=1)
# 将标签解码回原始格式
decoded_predictions = label_encoder.inverse_transform(predicted_labels)
# 使用预测的错误分类更新输出 CSV
output_data = pd.DataFrame({&quot;Error Message&quot;: test_messages, &quot;Error Classification&quot;:coded_predictions})
output_data.to_csv(&quot;C:\\Users\\foobar\\Documents\\Output.csv&quot;, index=False)

训练输入包含 HTTP 错误消息和分类：（样本）

测试输入如下：

但是 Output.csv 输出结果不正确，出现 5** 错误，错误地显示“客户端错误”。
]]></description>
      <guid>https://stackoverflow.com/questions/78936387/data-classification-doesnt-work-with-bert-model</guid>
      <pubDate>Sun, 01 Sep 2024 00:59:26 GMT</pubDate>
    </item>
    <item>
      <title>使用flutter计算图像中的对象数[关闭]</title>
      <link>https://stackoverflow.com/questions/78935944/count-objects-in-image-by-using-flutter</link>
      <description><![CDATA[我有一张包含很多物体的图像，所以我想检测每个物体，然后计算该图像中重复的物体并返回每个重复物体的数量，但我不知道是否可以使用flutter来实现这一点？
我搜索了更多关于这个主题的内容，但大多数都与使用Tflite有关，并且该包与最新版本的flutter不匹配，而且我根本找不到任何关于使用flutter计算图像中物体的示例。
图像将如下所示：
]]></description>
      <guid>https://stackoverflow.com/questions/78935944/count-objects-in-image-by-using-flutter</guid>
      <pubDate>Sat, 31 Aug 2024 19:26:04 GMT</pubDate>
    </item>
    <item>
      <title>我已将现有的 ipynb 文件导入 Azure Python 笔记本。运行计算机时导入库时出现错误</title>
      <link>https://stackoverflow.com/questions/78934759/i-have-imported-an-existing-ipynb-file-into-an-azure-python-notebook-there-are</link>
      <description><![CDATA[我已将现有的 ipynb 文件导入 Azure 笔记本。并尝试使用 Azure 计算机运行。
在库导入部分，我收到以下错误：
来自 sksurv.linear_model 导入 CoxPHSurvivalAnalysis
来自 sksurv.util 导入 Surv

我已采取以下步骤：

pip install Cmake (PyPI)
pip install scikit-survival (PyPI)

它仍然在导入库中显示错误，例如以上。
我希望修复库中的错误。]]></description>
      <guid>https://stackoverflow.com/questions/78934759/i-have-imported-an-existing-ipynb-file-into-an-azure-python-notebook-there-are</guid>
      <pubDate>Sat, 31 Aug 2024 09:21:51 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 验证拆分导致 75% 的类别过度拟合，25% 的类别为随机噪声</title>
      <link>https://stackoverflow.com/questions/78934378/tensorflow-validation-split-leading-to-75-of-classes-overfitted-25-random-noi</link>
      <description><![CDATA[我正在执行机器学习任务，其中验证分割似乎存在数据泄漏。如果我调整分割，它会移动过度拟合的类的数量。
X_train = resize_images(X_train, (512, 512))

y_train = label_enc.transform(y_train)
n_classes = len(list(label_enc.classes_))

X_train = np.asarray(X_train)

X_train, y_train = add_extra_dim(X_train, y_train, n_classes)

model = build_model()

model.fit(X_train, y_train, epochs=50, validation_split = 0.25, callbacks=[callback])

def build_model():
model = ATCNet(shape=(100, 100, 1), n_classes=n_classes)

top3 = tf.keras.metrics.TopKCategoricalAccuracy(k=3, name=&quot;top3&quot;)
top5 = tf.keras.metrics.TopKCategoricalAccuracy(k=5, name=&quot;top5&quot;)
model.compile(optimizer=Adam(), 
loss=&#39;categorical_crossentropy&#39;, 
metrics=[&#39;accuracy&#39;, top3, top5])
返回模型

[[输入图片描述在此输入图片描述在此描述](https://i.sstatic.net/nS2vvCJP.png)](https://i.sstatic.net/oCSLgoA4.png)
我已测试，不存在任何数据泄露。
我已将问题隔离到验证拆分，但我找不到任何有关如何修复它或我做错了什么的信息]]></description>
      <guid>https://stackoverflow.com/questions/78934378/tensorflow-validation-split-leading-to-75-of-classes-overfitted-25-random-noi</guid>
      <pubDate>Sat, 31 Aug 2024 05:41:32 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn)</title>
      <link>https://stackoverflow.com/questions/78934120/runtimeerror-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-gra</link>
      <description><![CDATA[我正在训练一个自动编码器，针对我的具体用例，我在 PyTorch 中实现了基于 Wasserstein 度量（地球移动度量）的自定义损失。我使用这个的一般想法是，下面代码片段中的 pred 和 target 是一批向量，比如 batch_size x vector_length
def wasserstein_distance(pred, target, num_bins=200):
# 获取列数（特征）
num_columns = pred.size(1)

# 计算每列的 Wasserstein 距离
distances = []
for i in range(num_columns):
# 获取列的预测值和目标值
pred_col = pred[:, i]
target_col = target[:, i]

# 确定列的直方图范围
min_val = min(pred_col.min().item(), target_col.min().item())
max_val = max(pred_col.max().item(), target_col.max().item())

# 计算预测值和目标值的直方图
pred_hist = torch.histc(pred_col, bins=num_bins, min=float(min_val), max=float(max_val))
target_hist = torch.histc(target_col, bins=num_bins, min=float(min_val), max=float(max_val))

# 将直方图标准化以形成概率分布
pred_hist /= pred_hist.sum()
target_hist /= target_hist.sum()

# 计算累积分布函数 (CDF)
pred_cdf = torch.cumsum(pred_hist, dim=0)
target_cdf = torch.cumsum(target_hist, dim=0)

# 计算 Wasserstein 距离（地球移动距离）
wasserstein_dist = torch.sum(torch.abs(pred_cdf - target_cdf))

distances.append(wasserstein_dist)

# 计算所有列的 Wasserstein 距离的平均值
mean_wasserstein_distance = torch.mean(torch.stack(distances))

return mean_wasserstein_distance

我在以下训练函数中使用它：
def train_model(model: nn.Module, data_loader: torch.utils.data.DataLoader, epoch_count: int, learning_rate: float) -&gt; np.ndarray:
print(&quot;##### 开始训练模型 #####&quot;)
model.train()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model.to(device)
loss = []

for epoch in range(epoch_count):
total_loss = 0.0
total_samples = 0

for batch in data_loader:
x = batch[0]
x = x.to(device)
optimizer.zero_grad()
pred_ae = model(x)
loss_ae = wasserstein_distance(pred_ae, x[:, 6:])

loss_ae.backward()
optimizer.step()

total_loss += loss_ae.item()
total_samples += x.size(0)

avg_loss = total_loss / total_samples
loss.append(avg_loss)

print(f&#39;Epoch: {epoch} Loss per unit: {avg_loss}&#39;)

print(&quot;##### FINISHED TRAINING OF MODEL #####&quot;)
return model, np.array(losses)


但是，我收到以下错误：
File &quot;&quot;, line 432, in &lt;module&gt;
model,losses = train_model(model,data,50,0.0001)
文件&quot;&quot;，第 237 行，在 train_model 中
loss_ae.backward()
文件&quot;C:\Users\aksha\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_tensor.py&quot;，第 522 行，在 Backward 中
torch.autograd.backward(
文件&quot;C:\Users\aksha\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\autograd\__init__.py&quot;，第 266 行，在 Backward 中
Variable._execution_engine.run_backward( # 调用 C++ 引擎运行反向传递
RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn

我确信每个requires_grad 默认为 true（我没有手动更改任何内容）。我怀疑问题是我为损失返回了一个值，而 Backward 需要每个张量都有一个值？但这不是我真正想要的训练方案，我希望每个更新都是特定于批次的（具体来说，学习每个批次中每列的分布）]]></description>
      <guid>https://stackoverflow.com/questions/78934120/runtimeerror-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-gra</guid>
      <pubDate>Sat, 31 Aug 2024 01:05:47 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 FAISS 减少大型人脸数据库的人脸识别中的误报？</title>
      <link>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</link>
      <description><![CDATA[我正在开发一个使用人脸识别的考勤跟踪系统。
该系统的工作原理如下：

1. 人脸检测：使用 Ultra Face 检测人脸。
2. 人脸编码：使用 FaceNet 对检测到的人脸进行编码。
3. 人脸比较：将编码的人脸与现有数据库进行比较以标记出勤率
4.使用的库：OpenCV 和 FAISS。
5.来源：CCTV摄像机镜头。

考勤系统说明：
当一个人走到摄像机前时，系统使用Ultra Face检测人脸，并使用FaceNet进行编码。然后将编码的人脸与现有数据库进行比较。如果相似度（余弦相似度）小于0.25，则标记出勤。
问题：
最初，数据库中的人数少于100人，比较时间是可以接受的。随着人数的增加，比较时间明显变长。每个人在数据库中都有5张图片。为了加快比较速度，我改用FAISS库。虽然FAISS显著缩短了比较时间，但也增加了误报（错误地标记出勤）。
人脸比较的旧方法：
for db_name, db_encode in encoding_dict.items():
尝试：
dist = cosine(db_encode, f_e[1])
除 ValueError 为 e 外：
print(&quot;&gt;&gt;&gt;&gt;&gt;&gt; : &quot;,f_e[1],&quot;\n&quot;,type(f_e[1]))
继续
if dist &lt;识别_t：
name = db_name
distance = dist

cv2.rectangle(img, (f_e[0][0], f_e[0][1]), (f_e[0][2], f_e[0][3]), (0, 255, 0), 1)
cv2.putText(img, f&#39;{name}:{distance - 1:.2f}&#39;, (f_e[0][0], f_e[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

使用 FAISS 的新方法：
class StaffCustManagement：
def __init__(self, staff_n_neighbours=4, identification_t=0.80):
self.staff_db：Custom_DB = Custom_DB（db_name =“mydatabase”，col_name =“staff”）
self.staff_names，self.staff_encodings = self.staff_load_encodings（）
self.staff_n_neighbours：int = staff_n_neighbours
self.staff_ini_faiss（）
self.recognition_t：float = identification_t

def staff_load_encodings（self） -&gt; Tuple[List[str], List[np.ndarray]]:
staff_names, staff_encodings = [], []
for document in self.staff_db.find_all_data():
staff_names.append(document[&#39;_id&#39;])
staff_encodings.append(ArrayEncDec.decode_from_base64(b64_str=document[&#39;encoding&#39;]))
return staff_names, staff_encodings

def staff_ini_faiss(self):
if self.staff_names and self.staff_encodings:
Dimensions = 128
self.staff_index_faiss = faiss.IndexFlatL2(dimensions)
faiss_embeddings = np.array(self.staff_encodings, dtype=&#39;float32&#39;)
faiss.normalize_L2(faiss_embeddings)
self.staff_index_faiss.add(faiss_embeddings)

def find_staff_cust(self, current_encode: np.ndarray) -&gt; Tuple[str, float]:
name = &quot;Unknown&quot;
distance = float(&quot;inf&quot;)
if len(self.staff_names) == 0:
return name, distance
target_rep = np.expand_dims(current_encode, axis=0)
# faiss.normalize_L2(target_rep)
distances, neighbours = self.staff_index_faiss.search(target_rep, self.staff_n_neighbours)
print(&quot;Distances&quot;, distances)
print(&quot;neighbors&quot;, neighbours)
if distances[0][0] &gt;= self.recognition_t:
return self.staff_names[neighbors[0][0]].split(&#39;-&#39;)[0], distances[0][0]
return name, distance

问题：
如何在使用 FAISS 进行人脸比较时减少误报我的出勤跟踪系统如何做到这一点？虽然 FAISS 大大缩短了比较时间，但准确性却受到影响，导致出勤标记不正确。是否有任何最佳实践或替代方法可以在大型数据库中保持高精度？]]></description>
      <guid>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</guid>
      <pubDate>Fri, 12 Jul 2024 10:33:51 GMT</pubDate>
    </item>
    <item>
      <title>UnicodeEncodeError：'charmap'编解码器无法对位置 19-38 的字符进行编码：字符映射到 <undefined></title>
      <link>https://stackoverflow.com/questions/78367946/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-19-38-c</link>
      <description><![CDATA[我正在开发一个基于 Flask 的 Web 应用程序，用户可以上传图像以使用机器学习模型进行预测。上传的图像存储在本地目录中，并使用预先训练的模型进行预测。但是，当我点击预测按钮时
是什么导致了这个 UnicodeEncodeError？
我该如何解决这个问题，以确保我的应用程序能够正确处理图像上传和预测？
在 Flask 环境中，尤其是在 Windows 上，是否有处理字符编码的最佳实践？
==app.py====
@app.route(&#39;/uploadimage&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;])
def upload_image():

file = request.files[&#39;my_image&#39;]
# 获取预测
predict_label = predict_label(img_path)
# 使用 flash 消息返回预测标签
flash(f&quot;Prediction: {predicted_label}&quot;, &quot;success&quot;)
os.remove(img_path) # 处理后删除临时文件
return render_template(&#39;uploadimage.html&#39;) # 对于 GET 请求，呈现表单


即使我设置了环境变量“UTF-8”，我仍然收到此错误
错误
文件“C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\encodings\cp1252.py”，第 19 行，位于 encode 中
返回codecs.charmap_encode(input,self.errors,encoding_table)[0]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: &#39;charmap&#39; 编解码器无法对位置 19-38 中的字符进行编码：字符映射到未定义。
============
即使我得到了一个最简单的代码来测试编码
标题是“要测试您的控制台是否可以处理 UTF-8，请尝试输出带有特殊字符或 Unicode 字符的文本：&quot;
print(&quot;UTF-8 test: àéîöü — 中文 — العربية&quot;)


错误与此相同好吧
print(&quot;UTF-8 测试：����� � \u4e2d\u6587 � \u0627\u0644\u0639\u0631\u0628\u064a\u0629&quot;)
文件 &quot;C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\encodings\cp1252.py&quot;，第 19 行，在编码中
返回 codecs.charmap_encode(input,self.errors,encoding_table)[0]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: &#39;charmap&#39; 编解码器无法对位置中的字符进行编码20-21：字符映射到 ]]></description>
      <guid>https://stackoverflow.com/questions/78367946/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-19-38-c</guid>
      <pubDate>Mon, 22 Apr 2024 17:25:20 GMT</pubDate>
    </item>
    <item>
      <title>InvalidArgumentError：不兼容的形状：[40,5] 与 [40,4]</title>
      <link>https://stackoverflow.com/questions/70524543/invalidargumenterror-incompatible-shapes-40-5-vs-40-4</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/70524543/invalidargumenterror-incompatible-shapes-40-5-vs-40-4</guid>
      <pubDate>Wed, 29 Dec 2021 20:14:34 GMT</pubDate>
    </item>
    <item>
      <title>使用 Android 相机进行人脸检测。？</title>
      <link>https://stackoverflow.com/questions/52826977/face-detection-using-android-camera</link>
      <description><![CDATA[是否可以使用手机摄像头扫描护照图像并保存其详细信息，然后使用 Android 手机摄像头检测该人并获取其详细信息？（人脸检测）]]></description>
      <guid>https://stackoverflow.com/questions/52826977/face-detection-using-android-camera</guid>
      <pubDate>Tue, 16 Oct 2018 02:08:22 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的 `CrossEntropyLoss()`</title>
      <link>https://stackoverflow.com/questions/49390842/crossentropyloss-in-pytorch</link>
      <description><![CDATA[交叉熵公式：

但为什么下面的代码给出的是 loss = 0.7437 而不是 loss = 0（因为 1*log(1) = 0）？
import torch
import torch.nn as nn
from torch.autograd import Variable

output = Variable(torch.FloatTensor([0,0,0,1])).view(1, -1)
target = Variable(torch.LongTensor([3]))

criterion = nn.CrossEntropyLoss()
loss = 标准（输出，目标）
print（loss）# 0.7437
]]></description>
      <guid>https://stackoverflow.com/questions/49390842/crossentropyloss-in-pytorch</guid>
      <pubDate>Tue, 20 Mar 2018 17:39:44 GMT</pubDate>
    </item>
    <item>
      <title>我实现的交叉熵函数有什么问题？</title>
      <link>https://stackoverflow.com/questions/47377222/what-is-the-problem-with-my-implementation-of-the-cross-entropy-function</link>
      <description><![CDATA[我正在学习神经网络，我想用 Python 编写一个函数 cross_entropy。其定义如下：

其中 N 是样本数量，k 是类别数量，log 是自然对数，如果样本 i 属于 j 类，则 t_i,j 为 1，否则为 0，p_i,j 是样本 i 属于 j 类的预测概率。
为了避免对数的数值问题，请将预测值限制在 [10^{−12}, 1 − 10^{−12}] 范围内。
根据上述说明，我通过将预测值限制在 [epsilon, 1 − epsilon] 范围内写下代码，然后根据上述公式计算 cross_entropy：
def cross_entropy(predictions, target, epsilon=1e-12):
&quot;&quot;&quot;
计算目标（编码为独热向量）和预测值之间的交叉熵。
输入：预测值 (N, k) ndarray
目标值 (N, k) ndarray
返回：标量
&quot;&quot;&quot;
predictions = np.clip(predictions, epsilon, 1. - epsilon)
ce = - np.mean(np.log(predictions) * target) 
return ce

以下代码将用于检查函数 cross_entropy 是否正确：
predictions = np.array([[0.25,0.25,0.25,0.25],
[0.01,0.01,0.01,0.96]])
targets = np.array([[0,0,0,1],
[0,0,0,1]])
ans = 0.71355817782 #正确答案
x = cross_entropy(predictions,目标)
print(np.isclose(x,ans))

上述代码的输出为 False，也就是说我定义函数 cross_entropy 的代码不正确。然后我打印 cross_entropy(predictions, target) 的结果。它给出了 0.178389544455，正确的结果应该是 ans = 0.71355817782。有人能帮我检查一下我的代码有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/47377222/what-is-the-problem-with-my-implementation-of-the-cross-entropy-function</guid>
      <pubDate>Sun, 19 Nov 2017 13:08:52 GMT</pubDate>
    </item>
    </channel>
</rss>