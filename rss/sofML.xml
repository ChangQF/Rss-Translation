<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 14 Dec 2024 09:17:10 GMT</lastBuildDate>
    <item>
      <title>XGBoost 警告未找到可见的 GPU，正在将设备设置为 CPU</title>
      <link>https://stackoverflow.com/questions/79280367/xgboost-warning-no-visible-gpu-is-found-setting-device-to-cpu</link>
      <description><![CDATA[系统信息

XGBoost 版本：2.1.3
NVIDIA 驱动程序版本：565.57.01
CUDA 版本：12.6（来自 nvcc）和 12.7（来自 nvidia-smi）
GPU：Tesla T4
操作系统：Ubuntu 24.04
Python 版本：3.11.10
torch.cuda.is_available()：True

尽管系统显示 CUDA 和 GPU 可用，但我遇到了来自 XGBoost 的以下警告：

XGBoost 警告：/workspace/src/context.cc:43：未找到可见的 GPU，将设备设置为CPU。

请帮我解决这个问题。谢谢！如果您需要任何其他信息，请告诉我。]]></description>
      <guid>https://stackoverflow.com/questions/79280367/xgboost-warning-no-visible-gpu-is-found-setting-device-to-cpu</guid>
      <pubDate>Sat, 14 Dec 2024 09:12:14 GMT</pubDate>
    </item>
    <item>
      <title>结合 RNN 和 FFN</title>
      <link>https://stackoverflow.com/questions/79280265/combine-rnn-and-ffn</link>
      <description><![CDATA[在 FFN 中，我们有一些输入和一些输出，并以此为基础训练模型。在 RNN 中，输入是序列的一段，输出是同一序列的下一个时间步。但是，在我的场景中，我将关节旋转作为输入，将顶点位置作为随时间变化的输出。我不知道如何在 RNN 中处理两个不同的序列（关节旋转和顶点位置）。
我有时间依赖性，并且这两个序列也是相互依赖的。我应该结合使用 FFN 和 RNN 来解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79280265/combine-rnn-and-ffn</guid>
      <pubDate>Sat, 14 Dec 2024 07:42:00 GMT</pubDate>
    </item>
    <item>
      <title>拟合非线性混合模型 [迁移]</title>
      <link>https://stackoverflow.com/questions/79279411/fitting-a-nonlinear-mixed-model</link>
      <description><![CDATA[我试图拟合一个非线性混合模型 (nLMM)，以测试某些生物的丰度是否受到导致丰度显著增加的事件后的采样期的影响。
数据显示了一条重要的曲线，这些生物的丰度在事件发生后激增（事件发生在采样期：-1 和 1 之间），但随后下降。
我试图构建一个非线性混合模型，但我发现理解如何构建模型非常具有挑战性（例如，model &lt;- lmer(abundance ~ samples_period + (1 | rep), data = data）。我非常感谢任何帮助来确定丰度是否受到采样期的影响。
data &lt;- data.frame(
abundant = c(79, 72, 58, 61, 88, 123, 119, 96, 67, 78, 143, 75, 105, 46, 58, 
127, 173, 181, 67, 120, 64, 30, 49, 47, 104, 83, 146, 118, 53, 
98, 223, 257, 255, 292, 354, 133, 129, 140, 27, 55, 68, 148, 
122, 132, 77, 121, 108, 109),
rep = c(&quot;T1&quot;, &quot;T2&quot;, &quot;T3&quot;, &quot;T1&quot;, &quot;T2&quot;, &quot;T3&quot;, “T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T3”、“T1”、“T2”、“T3”、“
“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“T3”、“T1”、“T2”、“ “T3”，“T1”，“T2”，“T3”，
“T1”，“T2”，“T3”，“T1”，“T2”，“T3”，“T1”，“T2”，“T3”，“T1”，“T2”，“T3”，“T1”，“T2”，“T3”，“T1”，“T2”，“T3”），
sampling_period_consecutive = c(1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 
6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 
10, 11, 11, 11, 12, 12, 12, 13, 13, 13, 14, 
14, 14, 15, 15, 15, 16, 16, 16),
采样周期 = c(-5, -5, -5, -4, -4, -4, -3, -3, -3, -2, -2, -2, -1, -1, 
-1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 
6, 11, 11, 11, 22, 22, 22, 34, 34, 34, 46, 46, 58, 
58, 58)
)

]]></description>
      <guid>https://stackoverflow.com/questions/79279411/fitting-a-nonlinear-mixed-model</guid>
      <pubDate>Fri, 13 Dec 2024 19:32:50 GMT</pubDate>
    </item>
    <item>
      <title>pytorch CNN 是否关心图像大小？</title>
      <link>https://stackoverflow.com/questions/79279124/does-pytorch-cnn-care-about-image-size</link>
      <description><![CDATA[我最近在玩 CNN，我有如下粘贴的代码。我的问题是，这适用于任何图像大小吗？我不清楚哪个参数或通道（如果有的话）关心图像大小？如果是这样的话，模型如何知道它需要多少个神经元，这不是图像大小的函数吗？
关于预训练模型的相关点 - 如果我使用预训练模型，我是否需要重新格式化我的图像以使其与模型最初训练的图像相同，或者它如何工作？
class CNN(nn.Module):
def __init__(self, num_classes, num_channels=1):
super(CNN, self).__init__()
self.num_classes = num_classes
self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=3, padding=1)
self.relu1 = nn.ReLU()
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
self.relu2 = nn.ReLU()
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
self.fc = nn.Linear(64*7*7, num_classes)
]]></description>
      <guid>https://stackoverflow.com/questions/79279124/does-pytorch-cnn-care-about-image-size</guid>
      <pubDate>Fri, 13 Dec 2024 17:24:14 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 CPU 上使用 llama_cpp 运行 LLaMA 13B 模型会花费过多时间并且产生较差的输出？</title>
      <link>https://stackoverflow.com/questions/79279016/why-does-running-llama-13b-model-with-llama-cpp-on-cpu-take-excessive-time-and-p</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79279016/why-does-running-llama-13b-model-with-llama-cpp-on-cpu-take-excessive-time-and-p</guid>
      <pubDate>Fri, 13 Dec 2024 16:45:38 GMT</pubDate>
    </item>
    <item>
      <title>将加速光线行进应用于 NeRF 实现</title>
      <link>https://stackoverflow.com/questions/79278867/applying-accelerated-raymarching-to-nerf-implementation</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79278867/applying-accelerated-raymarching-to-nerf-implementation</guid>
      <pubDate>Fri, 13 Dec 2024 15:46:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么训练损失和测试损失之间有这么大的差异？[关闭]</title>
      <link>https://stackoverflow.com/questions/79278629/why-is-there-such-a-big-difference-between-train-and-test-loss</link>
      <description><![CDATA[当前设备：cuda
100%|██████████| 1/1 [03:08&lt;00:00, 188.25s/it]
Epoch：1 | train_loss：333.7638 | train_acc：0.1961 | test_loss：2.1727 | test_acc：0.2039 | 

我正在训练图像分类模型 (AlexNet)。我该怎么做才能改变这种情况，或者这是正常的吗？
我尝试更改转换中的某些内容，但根本没有用]]></description>
      <guid>https://stackoverflow.com/questions/79278629/why-is-there-such-a-big-difference-between-train-and-test-loss</guid>
      <pubDate>Fri, 13 Dec 2024 14:21:16 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost/XGBRanker 生成概率而不是排名分数</title>
      <link>https://stackoverflow.com/questions/79278625/xgboost-xgbranker-to-produce-probabilities-instead-of-ranking-scores</link>
      <description><![CDATA[我有一个学生考试成绩的数据集，如下所示：
班级 ID 班级规模 学生编号 智商 学习时间 分数
1 3 3 101 10 98
1 3 4 99 19 80
1 3 6 130 3 95
2 4 4 93 5 50
2 4 5 103 9 88
2 4 8 112 12 99
2 4 1 200 10 100 

我想建立一个机器学习模型，尝试使用 IQ 和 Hours_Studied 预测谁将成为班级第一名（即最高 Score），对于任何给定的 Class_ID特征。
由于这是一个排名问题，因此自然的一类学习模型是使用 XGBoost 中的 XGBRanker 或 lightgbm 中的 LGBMRanker。
这是我使用 xgboost 的代码：
from sklearn.model_selection import GroupShuffleSplit
import xgboost as xgb

gss = GroupShuffleSplit(test_size=.40, n_splits=1, random_state = 7).split(df, groups=df[&#39;Class_ID&#39;])

X_train_inds, X_test_inds = next(gss)

train_data = df.iloc[X_train_inds]
X_train = train_data.loc[:, ~train_data.columns.isin([&#39;Class_ID&#39;,&#39;Student_Number&#39;,&#39;Score&#39;])]
y_train = train_data.loc[:, train_data.columns.isin([&#39;Score&#39;])]

groups = train_data.groupby(&#39;Class_ID&#39;).size().to_frame(&#39;Class_size&#39;)[&#39;Class_size&#39;].to_numpy()

test_data = df.iloc[X_test_inds]

X_test = test_data.loc[:, ~test_data.columns.isin([&#39;Student_Number&#39;,&#39;Score&#39;])]
y_test = test_data.loc[:, test_data.columns.isin([&#39;Score&#39;])]

model = xgb.XGBRanker( 
tree_method=&#39;hist&#39;,
device=&#39;cuda&#39;,
booster=&#39;gbtree&#39;,
objective=&#39;rank:pairwise&#39;,
enable_categorical=True,
random_state=42, 
learning_rate=0.1,
colsample_bytree=0.9, 
eta=0.05, 
max_depth=6, 
n_estimators=175, 
subsample=0.75 
)

model.fit(X_train, y_train, group=groups, verbose=True)

def predict(model, df):
return model.predict(df.loc[:, ~df.columns.isin([&#39;Class_ID&#39;,&#39;Student_Number&#39;])])

predictions = (X_test.groupby(&#39;Class_ID&#39;)
.apply(lambda x: predict(model, x)))

代码运行良好，具有合理的预测能力。但是，输出是“相关性得分”列表，而不是概率列表。但似乎 XGBRanker 和 LGBMRanker 都没有属性 predict_proba，该属性返回获得班级最高分的概率。
所以我的问题是，有没有办法将 相关性得分 转换为概率，或者是否有其他自然类别的排名模型可以处理此类问题？
编辑在这个问题中，我只关心最终名列前茅的人（或者可能是前三名），所以排名并不是那么重要（例如，知道学生 4 排名第 11 位，学生 8 排名第 12 位并不那么重要），所以我想一种方法是在 xgboost 中使用分类而不是排名。但我想知道还有其他方法吗。]]></description>
      <guid>https://stackoverflow.com/questions/79278625/xgboost-xgbranker-to-produce-probabilities-instead-of-ranking-scores</guid>
      <pubDate>Fri, 13 Dec 2024 14:20:37 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：微调 llama 时，张量的元素 0 不需要 grad 且没有 grad_fn</title>
      <link>https://stackoverflow.com/questions/79277352/runtimeerror-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-gra</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79277352/runtimeerror-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-gra</guid>
      <pubDate>Fri, 13 Dec 2024 06:02:07 GMT</pubDate>
    </item>
    <item>
      <title>如何在ipynb中保存XGBoost模型并在javascript中加载以便调用模型并提示用户输入并获取预测值？</title>
      <link>https://stackoverflow.com/questions/79277258/how-to-save-xgboost-model-in-ipynb-and-load-in-javascript-in-order-to-call-the-m</link>
      <description><![CDATA[如何将训练好的模型（本地的 ipynb 或 python 文件）链接到 javascript（前端）？

我有一个训练好的 XGB 模型，使用一些特征（浮点数）来预测一个值（碳强度）。
我想将模型保存为文件并加载模型以在 javascript 中使用。它将要求用户在 javascript 中输入一条记录（22 个特征）并传递给 ML 模型，最终返回并打印预测值（碳强度）。
我知道模型可以保存为 json/pickle/joblib 文件。

以下是 python 中的模型训练代码：
# 1. 加载数据
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# 从 Excel 文件加载数据集
df = pd.read_excel(r&quot;C:\Users\RY\Desktop\Jupiter\test.xlsx&quot;, sheet_name=&quot;Sheet1&quot;)

# 仅保留相关列（不包括前两列）
df = df.iloc[:, 2:]

# 根据模块对特征进行分类（锅炉、涡轮机、电力、煤炭、碳排放）
# 特征：1（日期）+ 7（锅炉）+ 6（涡轮机）+ 3（电力）+ 5（煤炭质量）+ 2（碳排放）+ 1（运行时间）+ 1（预测碳强度）

# 为每个模块定义特征名称（根据您的实际列名称进行调整）
boiler_features = [&#39;锅炉给水温度&#39;, &#39;空气供应温度&#39;, &#39;氧气水平&#39;,
&#39;空气预热器泄漏率&#39;, &#39;计算烟气温度&#39;,
&#39;含氧量 (%)&#39;, &#39;烟气流量&#39;]

turbine_features = [&#39;主蒸汽温度&#39;, &#39;主蒸汽压力&#39;, &#39;再热蒸汽温度&#39;,
&#39;排汽温度&#39;, &#39;真空&#39;, &#39;平均负荷&#39;]

power_features = [&#39;负荷&#39;, &#39;电厂耗电量&#39;, &#39;运行小时数&#39;]

coal_features = [&#39;碳含量 (空气干燥, %)&#39;, &#39;挥发性物质 (接收, %)&#39;, 
&#39;灰分含量 (接收, %)&#39;, &#39;净热值 (kJ/kg, 接收)&#39;, 
&#39;水分含量 (%)&#39;]

carbon_emission_features = [&#39;CO2 浓度&#39;, &#39;碳排放量&#39;, &#39;碳强度&#39;]

# 合并指定特征顺序
sorted_columns = boiler_features + turbine_features + power_features + coal_features + carbon_emission_features
df = df[sorted_columns]

# 将数据拆分为特征 (X1) 和目标 (y1)
X1 = df.drop(&#39;Carbon Intensity&#39;, axis=1).drop(&#39;Carbon Emission&#39;, axis=1)
y1 = df[&#39;Carbon Intensity&#39;]

# 2. 将数据拆分为训练集和测试集
from sklearn.model_selection import train_test_split
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)

# 显示特征数据集的基本统计数据
X1.describe()

# 3. 训练模型
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error

# 初始化 XGBoost 回归器
xgb_model = XGBRegressor(
n_estimators=300, # 树的数量
max_depth=3, # 最大树深度
learning_rate=0.1, # 学习率
subsample=0.8, # 每棵树的样本比例
colsample_bytree=0.8, # 每棵树的特征比例
random_state=42
)

# 在训练数据上训练模型
xgb_model.fit(X1_train, y1_train)

# 在测试集上进行预测
xgb_y1_pred = xgb_model.predict(X1_test)

# 4. 对一条记录进行预测
# 示例：预测单个数据的碳强度point
xgb_model.predict([X1_train.iloc[0].values])

# 5. 保存模型
# 将训练好的模型保存为 JSON 文件
xgb_model.save_model(&quot;xgb_model.json&quot;)


如何在 javascript 中保存并加载模型？
一条记录输入和输出示例：
[array([ 2.42860000e+02, 1.97800000e+01, 4.59000000e+00, 2.43000000e+00,
1.27770000e+02, 9.60153257e-02, 6.51605003e+05, 5.42320000e+02,
1.26000000e+01, 5.12910000e+02, 3.18000000e+01, -9.78800000e+01,
1.97080000e+02, 1.93096475e+02, 5.45000000e+00, 2.40000000e+01,
5.36500000e+01, 2.41800000e+01, 2.18900000e+01, 1.86225000e+04,
1.53000000e+01, 3.33225000e+03])]

array([865.6147], dtype=float32)
]]></description>
      <guid>https://stackoverflow.com/questions/79277258/how-to-save-xgboost-model-in-ipynb-and-load-in-javascript-in-order-to-call-the-m</guid>
      <pubDate>Fri, 13 Dec 2024 04:51:33 GMT</pubDate>
    </item>
    <item>
      <title>无监督递归特征消除</title>
      <link>https://stackoverflow.com/questions/79275101/unsupervised-recursive-feature-elimination</link>
      <description><![CDATA[我想知道是否有人能帮忙提供一些无监督递归特征消除 (uRFE) 的 R 代码？
简而言之，我运行了一个无监督的 SOM，用 k-medoids 进行分区，但也想使用轮廓分数来识别 uRFE。
下面附上了一些虚拟代码。如果可能的话，我希望您能提供一些代码来帮助我在 SOM 上执行 uRFE，从而确定哪些特征在聚类时最相关
library(aweSOM)

#IRIS 数据

full.data &lt;- iris
train.data &lt;- full.data[, c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;)]
train.data &lt;- scale(train.data)

#SOM 初始化 + MAP

set.seed(1465)
init &lt;- somInit(train.data, 4, 4)
iris.som &lt;- kohonen::som(train.data, grid = kohonen::somgrid(4, 4, &quot;hexagonal&quot;), 
rlen = 100, alpha = c(0.05, 0.01), radius = c(2.65,-2.65), 
dist.fcts = &quot;sumofsquares&quot;, init = init)

#SOM 聚类

superclust_pam &lt;- cluster::pam(iris.som$codes[[1]], 3)
superclasses_pam &lt;- superclust_pam$clustering
]]></description>
      <guid>https://stackoverflow.com/questions/79275101/unsupervised-recursive-feature-elimination</guid>
      <pubDate>Thu, 12 Dec 2024 12:27:01 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：层“dense_2”需要 1 个输入，但它收到了 2 个输入张量</title>
      <link>https://stackoverflow.com/questions/78846949/valueerror-layer-dense-2-expects-1-inputs-but-it-received-2-input-tensors</link>
      <description><![CDATA[我无法加载我的模型，它一直显示错误
ValueError：层“dense_2”需要 1 个输入，但它收到了 2 个输入张量。收到的输入：[&lt;KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, name=keras_tensor_2896&gt;, &lt;KerasTensor shape=(None, 7, 7, 1280), dtype=float32, sparse=False, name=keras_tensor_2897&gt;]
这是我的代码
image_generator = ImageDataGenerator(
rescale=1./255,
rotation_range=20,
zoom_range=0.2,
width_shift_range=0.2,
height_shift_range=0.2,
Horizo​​ntal_flip=True,
validation_split=0.2
)

train_dataset = image_generator.flow_from_directory(
directory=path_to_dataset,
target_size=(224, 224),
batch_size=32,
subset=&#39;training&#39;
)

validation_dataset = image_generator.flow_from_directory(
directory=path_to_dataset,
target_size=(224, 224),
batch_size=32,
subset=&#39;validation&#39;
)

# 加载数据集中子文件夹中的 (num_classes) 类
num_classes = len(train_dataset.class_indices)

from tensorflow.keras.applications.mobilenet import MobileNet

# 加载 MobileNet 模型
pre_trained_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),
include_top=False,
weights=&#39;imagenet&#39;)

pre_trained_model.summary()

# 打印数据集信息以供调试
print(f&quot;训练数据集形状：{train_dataset.image_shape}&quot;)
print(f&quot;验证数据集形状：{validation_dataset.image_shape}&quot;)

pre_trained_model.trainable = False

# 为预训练模型添加自定义层
model = tf.keras.Sequential([
pre_trained_model,
tf.keras.layers.GlobalAveragePooling2D(),
tf.keras.layers.Dense(1024,activation=&#39;relu&#39;),
tf.keras.layers.Dropout(0.5),
tf.keras.layers.Dense(num_classes,activation=&#39;softmax&#39;) 
])

# 编译模型
#from tensorflow.keras.optimizers import RMSprop
model.compile(optimizer=Adam(learning_rate=0.0001),
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

batch=40
history = model.fit(train_dataset,
validation_data=validation_dataset,
epochs=20,
steps_per_epoch = train_dataset.samples//batch,
validation_steps =validation_dataset.samples//batch,
verbose = 1
)

# 加载模型
model_save_path = &#39;/content/drive/MyDrive/Machine Learning/saved_models/model_plastik.h5&#39;

# 加载模型，确保必要时已编译
loaded_model = tf.keras.models.load_model(model_save_path) 

# 现在您可以根据需要修改已加载的模型
# 例如，如果您想提取子模型：
input_layer_index = 0 # 替换为实际索引
dense_2_index = 3 # 替换为实际索引
loaded_model = tf.keras.models.Model(inputs=loaded_model.layers[input_layer_index].input, 
outputs=loaded_model.layers[dense_2_index].output)

# 检查已加载模型的配置
for i, layer in enumerate(loaded_model.layers):
print(f&quot;Layer {i}: {layer.name} - 输入形状：{layer.input_shape} - 输出形状：{layer.output_shape}&quot;)

print(&quot;已成功加载修订模型。&quot;)

我尝试加载模型，并希望它能够加载以进行测试]]></description>
      <guid>https://stackoverflow.com/questions/78846949/valueerror-layer-dense-2-expects-1-inputs-but-it-received-2-input-tensors</guid>
      <pubDate>Thu, 08 Aug 2024 07:06:54 GMT</pubDate>
    </item>
    <item>
      <title>“发现输入变量的样本数量不一致”我在 train_test_split 过程中做错了什么吗？</title>
      <link>https://stackoverflow.com/questions/75085236/found-input-variables-with-inconsistent-numbers-of-samples-have-i-done-somethi</link>
      <description><![CDATA[我正在尝试逻辑回归模型，并运行一些测试，但我一直收到此错误。不太确定我和其他人做了什么不同的事情
from sklearn import preprocessing
X = df.iloc[:,:len(df.columns)-1]
y = df.iloc[:,len(df.columns)-1]ere

这是我分离列的方式
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

TTS
logReg = LogisticRegression(n_jobs=-1)
logReg.fit(X_train, y_train)

y_pred = logReg.predict(X_train)

mae = mean_absolute_error(y_test, y_pred)
print(&quot;MAE:&quot; , mae)

ValueError Traceback（最近一次调用最后一次）
Cell In [112]，第 1 行
----&gt; 1 mae = mean_absolute_error(y_test, y_pred)
2 print(&quot;MAE:&quot; , mae)

文件 ~\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_regression.py:196, in mean_absolute_error(y_true, y_pred, sample_weight, multioutput)
141 def mean_absolute_error(
142 y_true, y_pred, *, sample_weight=None, multioutput=&quot;uniform_average&quot;
143 ):
144 &quot;&quot;&quot;平均绝对误差回归损失。
145 
146 更多信息请阅读 :ref:`用户指南 &lt;mean_absolute_error&gt;`。
(...)
194 0.85...
195 “” “”
--&gt; 196 y_type, y_true, y_pred, multioutput = _check_reg_targets(
197 y_true, y_pred, multioutput
198 )
199 check_consistent_length(y_true, y_pred, sample_weight)
200 output_errors = np.average(np.abs(y_pred - y_true), weights=sample_weight, axis=0)

文件 ~\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\metrics\_regression.py:100，位于 _check_reg_targets(y_true, y_pred, multioutput, dtype)
66 def _check_reg_targets(y_true, y_pred, multioutput, dtype=&quot;numeric&quot;):
67 &quot;&quot;&quot;检查 y_true 和 y_pred 是否属于同一回归任务。
68 
69 参数
(...)
98 正确的关键字。
99 &quot;&quot;&quot;
--&gt; 100 check_consistent_length(y_true, y_pred)
101 y_true = check_array(y_true, Ensure_2d=False, dtype=dtype)
102 y_pred = check_array(y_pred, Ensure_2d=False, dtype=dtype)

文件 ~\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\utils\validation.py:387，在 check_consistent_length(*arrays) 中
385 uniques = np.unique(lengths)
386 if len(uniques) &gt; 1:
--&gt; 387 raise ValueError(
388 &quot;找到样本数量不一致的输入变量：%r&quot;
389 % [int(l) for l in lengths]
390 )

ValueError: 找到样本数量不一致的输入变量：[25404, 101612]

我以为是我拆分列的方式不对，但这似乎不是问题所在
当测试规模为 50/50 时，它可以工作，但其他测试规模都不起作用]]></description>
      <guid>https://stackoverflow.com/questions/75085236/found-input-variables-with-inconsistent-numbers-of-samples-have-i-done-somethi</guid>
      <pubDate>Wed, 11 Jan 2023 15:15:49 GMT</pubDate>
    </item>
    <item>
      <title>PySpark 中的特征选择</title>
      <link>https://stackoverflow.com/questions/53528481/feature-selection-in-pyspark</link>
      <description><![CDATA[我正在研究一个形状为 1,456,354 X 53 的机器学习模型。我想对我的数据集进行特征选择。我知道如何使用以下代码在 python 中进行特征选择。
from sklearn.feature_selection import RFECV,RFE

logreg = LogisticRegression()
rfe = RFE(logreg, step=1, n_features_to_select=28)
rfe = rfe.fit(df.values,arrythmia.values)
features_bool = np.array(rfe.support_)
features = np.array(df.columns)
result = features[features_bool]
print(result)

但是，我找不到任何文章可以展示如何在 pyspark 中执行递归特征选择。 
我尝试在 pyspark 中导入 sklearn 库，但它给出了一个错误 sklearn 模块未找到。我在 google dataproc 集群上运行 pyspark。
有人能帮我在 pyspark 中实现这个吗]]></description>
      <guid>https://stackoverflow.com/questions/53528481/feature-selection-in-pyspark</guid>
      <pubDate>Wed, 28 Nov 2018 21:36:15 GMT</pubDate>
    </item>
    <item>
      <title>根据传感器数据进行步态/行走分析</title>
      <link>https://stackoverflow.com/questions/39732545/gait-walk-analysis-from-sensor-data</link>
      <description><![CDATA[我组装了一块地毯，里面有 8 个压力传感器。您可以在图片中看到传感器的排列。整个地毯为 80x80 厘米。每个传感器在被按下时都会输出数字信号（0 或 1）。微控制器每 100 毫秒读取一次所有传感器，并输出一个有效载荷字节，其中每个位包含一个三角形的信息。我将所有这些字节存储在一个 100 字节长的数组中。
我需要从这个数组计算步态（方向，用户前进的角度）。用户只是在原地行进，双脚交替抬起和放下。你知道我可以用来做这种分析的任何算法吗？我应该使用机器学习/神经网络吗？语言并不重要，我只需要找出分析这个字节数组的正确方法。谢谢！
]]></description>
      <guid>https://stackoverflow.com/questions/39732545/gait-walk-analysis-from-sensor-data</guid>
      <pubDate>Tue, 27 Sep 2016 19:10:22 GMT</pubDate>
    </item>
    </channel>
</rss>