<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 12 Dec 2023 21:12:09 GMT</lastBuildDate>
    <item>
      <title>Labeled Cluster ：根据训练数据将点云分为几类</title>
      <link>https://stackoverflow.com/questions/77648966/labeled-cluster-divide-a-point-cloud-into-categories-based-on-the-training-dat</link>
      <description><![CDATA[输入：
N 点（X，Y）（A 点是屏幕中红色形状的中心）。
输出 =（屏幕中的青色矩形）
将这些点聚类到 K 组（K &lt;= N）。屏幕中的青色矩形是我们期望的输出。
我们有数千个标记数据（如屏幕中的数据）。
如何使用标记数据（我们已经手动完成）来训练可以对新数据进行聚类的算法。
在此处输入图片描述
我尝试了 sklearn.cluster 但这是针对未标记的数据...]]></description>
      <guid>https://stackoverflow.com/questions/77648966/labeled-cluster-divide-a-point-cloud-into-categories-based-on-the-training-dat</guid>
      <pubDate>Tue, 12 Dec 2023 20:46:27 GMT</pubDate>
    </item>
    <item>
      <title>神经网络在时间序列中查找子序列？</title>
      <link>https://stackoverflow.com/questions/77648688/neural-network-to-find-subsequence-in-time-series</link>
      <description><![CDATA[

蓝线代表时间序列
红色矩形子序列

是否可以通过机器学习方法找到这些子序列？
PS：这是合成数据，在真实数据中经典算法会失败]]></description>
      <guid>https://stackoverflow.com/questions/77648688/neural-network-to-find-subsequence-in-time-series</guid>
      <pubDate>Tue, 12 Dec 2023 19:48:08 GMT</pubDate>
    </item>
    <item>
      <title>数学图数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/77648552/math-graph-dataset</link>
      <description><![CDATA[只是想知道是否有人知道包含通过某些方程获得的可能图形的任何数据集，因为我想制作和训练一个人工智能模型，当给定图像时，它将获取图像的轮廓，并返回一系列数学方程绘制时，例如在 Desmos 图形计算器 中，给出图像的轮廓。如果您有任何建议，请告诉我！谢谢！
我尝试拍摄照片并单独标记每张照片，但这需要很长时间！我希望我能找到一个至少包含一些基本图表的数据集，我可以从那里继续。]]></description>
      <guid>https://stackoverflow.com/questions/77648552/math-graph-dataset</guid>
      <pubDate>Tue, 12 Dec 2023 19:20:44 GMT</pubDate>
    </item>
    <item>
      <title>FFNN 帮助进行超频带调整</title>
      <link>https://stackoverflow.com/questions/77648526/ffnn-help-on-hyperband-tuning</link>
      <description><![CDATA[我正在开发一个时间序列玩具问题来学习前馈神经网络。在本例中，我想根据一些输入来预测 Nova Dehli 的温度。
我开发了模型，但现在我尝试使用超频带调整来调整参数。
问题是我觉得它消耗的时间很奇怪。运行整个代码总是需要 21 秒。无论我将 hyperband_iterations 从 1 增加到 100000000000 还是将因子从 3 更改为 3000。总是 21 秒。我认为这就是运行所谓的“最佳模型”的 250 个周期所需的时间。
# 定义回归模型
def model_builder（马力）：
    模型 = keras.Sequential()

    hp_units1 = hp.Int(&#39;单位1&#39;, min_value=1, max_value=500, 步骤=1)
    model.add(keras.layers.Dense(units=hp_units1,activation=&#39;relu&#39;,input_shape=(Xtrain_np.shape[1],)))

    hp_units2 = hp.Int(&#39;units2&#39;, min_value=1, max_value=500, step=1)
    model.add(keras.layers.Dense(units=hp_units2,activation=&#39;relu&#39;))

    model.add(keras.layers.Dense(1))

    # 调整优化器的学习率
    hp_learning_rate = hp.Choice(&#39;learning_rate&#39;, 值=[1e0, 1e-1, 1e-2, 1e-3, 1e-4 ])

    # 使用“mean_squared_error”进行回归任务
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                  损失=&#39;均方误差&#39;，
                  metrics=[&#39;mae&#39;]) # 平均绝对误差是回归的常用指标

    返回模型

**#调整**
调谐器 = kt.Hyperband(hypermodel = model_builder,
                     目标=&#39;val_loss&#39;，
                     最大纪元=500，
                     系数=30，
                     hyperband_iterations=1,
                     种子=无，
                     超参数=True,
                     tune_new_entries=真，
                     允许新条目=真，
                     每次试验的最大重试次数=5，
                     最大连续失败试验=5)

stop_early = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, 耐心=400)

# 寻找最佳参数
tuner.search（Xtrain_np，ytrain_np，epochs = 400，validation_data =（Xval_np，yval_np），回调= [stop_early]）

我尝试更改调整中的所有参数，例如 hyperband_iterations、factor 和 max_epochs。所花费的时间没有任何改变。]]></description>
      <guid>https://stackoverflow.com/questions/77648526/ffnn-help-on-hyperband-tuning</guid>
      <pubDate>Tue, 12 Dec 2023 19:13:24 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：调用层“查询”（类型 EinsumDense）时遇到异常。形状必须为 3 级，但为 1 级</title>
      <link>https://stackoverflow.com/questions/77648267/valueerror-exception-encountered-when-calling-layer-query-type-einsumdense</link>
      <description><![CDATA[我正在做关于 NLP 的项目，我在 Transformer 模型上遇到了这个问题。
问题说：
形状必须为 3 级，但为 1 级

关于我的数据集，它看起来像这样：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

API 序列
标签


&lt;正文&gt;

调用API
0


呼叫病毒
1




这是我的代码：
类变压器：
    def __init__(自身，数据，名称=“”，batch_size=16)：
        向量 = np.stack(data.iloc[:, 0].values)
        标签 = data.iloc[:, 1].values

        …………

        # 输入
        向量 = 向量.reshape(-1, 1)

        输入=输入（形状=（向量.形状[0]，向量.形状[1]））

        # 变压器块 1
        attn_output_1 = MultiHeadAttention（num_heads = 8，key_dim = 32）（输入，输入）
        attn_output_1 = LayerNormalization()(attn_output_1)
        ff_output_1 = 密集（单位=32，激活=&#39;relu&#39;）（attn_output_1）
        ff_output_1 = 密集（单位=50）（ff_output_1）
        残差_输出_1 = LayerNormalization()(输入 + ff_output_1)

我尝试了很多方法，比如.reshape()，但不起作用]]></description>
      <guid>https://stackoverflow.com/questions/77648267/valueerror-exception-encountered-when-calling-layer-query-type-einsumdense</guid>
      <pubDate>Tue, 12 Dec 2023 18:16:18 GMT</pubDate>
    </item>
    <item>
      <title>组合 2 个或更多具有不同响应时间的 ML 模型</title>
      <link>https://stackoverflow.com/questions/77648139/combine-out-of-2-or-more-ml-models-with-different-response-times</link>
      <description><![CDATA[我有几个模型，它们对提示的响应时间不同。我想将它们的输出和供应结合到不同的机器学习模型中。这些模型具有不同的运行时，因此更多的是异步方式。
我怎样才能实现这个目标？]]></description>
      <guid>https://stackoverflow.com/questions/77648139/combine-out-of-2-or-more-ml-models-with-different-response-times</guid>
      <pubDate>Tue, 12 Dec 2023 17:53:10 GMT</pubDate>
    </item>
    <item>
      <title>MNIST 的 CLIP 零样本准确率为 30%？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77647994/zero-shot-accuracy-of-clip-for-mnist-is-30</link>
      <description><![CDATA[《Learning Transferable Visual Models From Natural Language Supervision》论文指出，CLIP 在 MNIST 上的准确率约为 88%。我从这里从 CLIP 下载了他们的实现： https://github.com/openai/CLIP
我在 MNIST 上对其进行了测试，仅获得 30% 左右的准确率。有谁知道我做错了什么？
导入火炬
导入剪辑
从 PIL 导入图像
导入火炬视觉
导入 torchvision.transforms 作为变换
将 numpy 导入为 np
设备=“cuda”； if torch.cuda.is_available() else “cpu”
模型，预处理=clip.load(“ViT-B/32”，device=device)

Training_set = torchvision.datasets.MNIST(&#39;./data&#39;,train=True,transform=preprocess,download=True)
validation_set = torchvision.datasets.MNIST(&#39;./data&#39;,train=False,transform=preprocess,download=True)

trainloader = torch.utils.data.DataLoader(training_set,batch_size=32,
                                          洗牌=真，num_workers=0）
testloader = torch.utils.data.DataLoader(validation_set,batch_size=32,
                                         洗牌=假，num_workers=0）
类 = [&#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;,
    &#39;5&#39;、&#39;6&#39;、&#39;7&#39;、&#39;8&#39;、&#39;9&#39;]

正确 = 0
总计 = 0
对于测试加载器中的 i、j：
    文本 = Clip.tokenize(classes).to(device)
    标签 = [j 中 x 的类 [x]]
    使用 torch.no_grad()：
        logits_per_image, logits_per_text = 模型(i, 文本)
        probs = logits_per_image.softmax(dim=-1).cpu().numpy()
    结果 = [np.argmax(probs, axis=1) 中 x 的类[x]]
    对于 zip 中的 i、j（结果、标签）：
        如果我==j：
            正确+=1
打印（正确/len（testloader.dataset））
]]></description>
      <guid>https://stackoverflow.com/questions/77647994/zero-shot-accuracy-of-clip-for-mnist-is-30</guid>
      <pubDate>Tue, 12 Dec 2023 17:28:36 GMT</pubDate>
    </item>
    <item>
      <title>使用部分地面实况信息的图像恢复[关闭]</title>
      <link>https://stackoverflow.com/questions/77647934/image-restoration-using-partial-ground-truth-information</link>
      <description><![CDATA[我正在开展一个图像恢复项目，其中有四张图像，每张图像都揭示了真实情况的不同部分，而其余部分则被遮挡。我还有真实图像来计算损失。
我熟悉 GAN 等较旧的架构，但尚未探索 Vision Transformers 或扩散模型。我渴望了解类似的方法或论文来解决像我这样的挑战。您能推荐一些相关的想法、技术或论文吗？
我记得读过有关 GAN 模型的文章，这些模型可以解决与此类似的任务，但我想尝试一些现代方法。]]></description>
      <guid>https://stackoverflow.com/questions/77647934/image-restoration-using-partial-ground-truth-information</guid>
      <pubDate>Tue, 12 Dec 2023 17:16:50 GMT</pubDate>
    </item>
    <item>
      <title>Python：fillna()函数输出单词“None”[重复]</title>
      <link>https://stackoverflow.com/questions/77647622/python-fillna-function-is-outputting-the-word-none</link>
      <description><![CDATA[我正在尝试为一堂课进行练习，其中我必须使用 fillna() 函数将列的缺失值替换为数据的中位数。在被称为“housing”的数据框中，有一列标题为“Mas Vnr Area”。其中有几行具有“NA”。价值观。为了填写和验证这些更改，在阅读必要的 .csv 文件后，我一直使用以下代码行（使用 PyCharm IDE）：
median_val = housing[&quot;Mas Vnr Area&quot;].median()

print(housing[&quot;Mas Vnr Area&quot;].fillna(median_val,inplace = True))

但是，每次我运行该程序时，这部分代码都只输出单词“None”。我已经多次核实“住房”是真的。里面有必要的信息，但我每次都会得到这个输出。有谁知道可能是什么原因造成的？]]></description>
      <guid>https://stackoverflow.com/questions/77647622/python-fillna-function-is-outputting-the-word-none</guid>
      <pubDate>Tue, 12 Dec 2023 16:25:12 GMT</pubDate>
    </item>
    <item>
      <title>我的训练功能仅几步之后就意外结束</title>
      <link>https://stackoverflow.com/questions/77646034/my-training-function-ends-unexpectedly-after-only-a-few-steps</link>
      <description><![CDATA[我正在尝试运行 Pix2Pix，但是我的训练功能在前 1k 步中突然停止，没有错误。我使用 PyTorch 来创建鉴别器和生成器。下面是包含 2 个负责训练的函数的代码，一个用于训练每个步骤，一个用于拟合模型。
训练步骤函数：
def train_step(input_image, 目标, 步骤):
    生成器.train()
    判别器.train()

    # 前向传递
    gen_output = 生成器（输入图像）

    Disc_real_output = 鉴别器（输入图像，目标）
    Disc_ generated_output = 鉴别器（input_image，gen_output）

    # 计算损失
    gen_total_loss, gen_gan_loss, gen_l1_loss = 生成器_loss(disc_generate_output,
        生成输出，目标）
    光盘损失 = 判别器损失（光盘真实输出，光盘生成输出）

    # 向后传递
    Generator_optimizer.zero_grad()
    discriminator_optimizer.zero_grad()

    gen_total_loss.backward(retain_graph=True)
    discriminator_optimizer.zero_grad() # 清除生成器梯度
        判别器后向传递
    Disc_loss.backward()

    # 更新权重
    生成器优化器.step()
    discriminator_optimizer.step()

    # 日志记录
    使用 torch.no_grad()：
        writer.add_scalar(&#39;gen_total_loss&#39;, gen_total_loss.item(), global_step=step // 1000)
        writer.add_scalar(&#39;gen_gan_loss&#39;, gen_gan_loss.item(), global_step=step // 1000)
        writer.add_scalar(&#39;gen_l1_loss&#39;, gen_l1_loss.item(), global_step=step // 1000)
        writer.add_scalar(&#39;disc_loss&#39;,disc_loss.item(),global_step=step // 1000)

拟合函数：
def fit(train_loader, test_loader, 步骤):
   example_target, example_input = next(iter(test_loader))
   开始 = 时间.time()

   对于枚举（train_loader）中的步骤（目标，input_image）：
    如果（步骤）% 1000 == 0：
        显示.clear_output(等待=True)

        如果步骤！= 0：
            print(f&#39;1000 步所用时间: {time.time()-start:.2f} 秒\n&#39;)

        开始 = 时间.time()

        生成图像（生成器，示例_输入，示例_目标）
        print(f&quot;步长: {step//1000}k&quot;)

    train_step（输入图像，目标，步骤）

    # 训练步骤
    如果（步长+1）% 10 == 0：
        打印（&#39;。&#39;，结束=&#39;&#39;，刷新= True）

    # 每 5k 步保存（检查点）模型
    如果（步长 + 1）% 5000 == 0：
        火炬.保存（{
            &#39;generator_state_dict&#39;：generator.state_dict(),
            &#39;discriminator_state_dict&#39;: discriminator.state_dict(),
            &#39;generator_optimizer_state_dict&#39;：generator_optimizer.state_dict(),
            &#39;discriminator_optimizer_state_dict&#39;: discriminator_optimizer.state_dict(),
        }, f&#39;检查点_{step + 1}.pt&#39;)

我是使用 GAN 的新手，我不确定这里的问题是什么。我尝试检查训练循环期间是否发生任何异常并打印它，但没有打印任何内容。]]></description>
      <guid>https://stackoverflow.com/questions/77646034/my-training-function-ends-unexpectedly-after-only-a-few-steps</guid>
      <pubDate>Tue, 12 Dec 2023 12:15:15 GMT</pubDate>
    </item>
    <item>
      <title>如何利用 GPU 减少 xgboost 的处理时间？</title>
      <link>https://stackoverflow.com/questions/77643788/how-can-i-reduce-processing-time-with-xgboost-by-utilizing-my-gpu</link>
      <description><![CDATA[我正在关注数据营的本教程，他们有一件事提到的是利用 GPU 来加快处理时间。他们甚至说它“速度极快”。
然而，我看到了相反的结果。对于下面的代码块，在 10k 提升的情况下，我看到在我的 params 中传递 “hist” 大约需要 30 秒，而在 ” 中传递则只需一分多钟。 gpu_hist&quot; 与我的 params 一起传递。
使用 “gpu_hist” 时，我的 GPU 的使用率上限为 40%，使用 “hist” 时，所有 24 个逻辑核心的使用率上限为 100%
params = {“objective”: “reg:squarederror”, “tree_method”: “gpu_hist”, “subsample”: 0.8,
    “colsample_bytree”：0.8}

evals = [(dtrain_reg, “训练”),(dtest_reg, “验证”)]

n = 10000


模型 = xgb.train(
   参数=参数，
   dtrain=dtrain_reg,
   num_boost_round=n,
   评估=评估，
   详细评估=50，
）

我正在尝试在 jupyter 笔记本的 VSCode 中运行它。

我已安装 CUDA 工具包和 cuDNN
我已检查它们是否已添加到路径中
我已确保安装了正确版本的 xgboost 来使用 GPU。
数据集有 53k 行 10 列，所以我不认为数据集太小
我已确认兼容性（使用 RTX 2060）

我问过 chatGPT，在网上搜索过，甚至问过我正在学习的课程中的导师，但无法诊断为什么“gpu_hist”花费了这么长时间。 vs 只是“历史”。
4 个月前，Stack Overflow 上还有另一个类似问题其响应为零。]]></description>
      <guid>https://stackoverflow.com/questions/77643788/how-can-i-reduce-processing-time-with-xgboost-by-utilizing-my-gpu</guid>
      <pubDate>Tue, 12 Dec 2023 05:02:17 GMT</pubDate>
    </item>
    <item>
      <title>重新排列 LGBMClassifier Predict_proba 输出列</title>
      <link>https://stackoverflow.com/questions/77639975/rearranging-lgbmclassifier-predict-proba-outputs-columns</link>
      <description><![CDATA[我正在训练一个 LGBMClassifier，以便使用其 predict_proba 方法。目标有 3 个类别：a、b 和 c。我想确保模型 predict_proba 按 b、a、c 的顺序输出列的概率。
有没有办法确保 LGBMClassifier predict_proba 的输出具有上述顺序？
导入 pandas 作为 pd
从 lightgbm 导入 LGBMClassifier
将 numpy 导入为 np

＃数据
特征 = [&#39;feat_1&#39;]
目标=&#39;目标&#39;
df = pd.DataFrame({
    &#39;feat_1&#39;：np.random.uniform（大小= 100），
    &#39;目标&#39;:np.random.choice(a=[&#39;b&#39;,&#39;c&#39;,&#39;a&#39;], size=100)
})

＃训练
模型 = LGBMClassifier()
model.fit(df[特征], df[目标])
打印（模型.classes_）

&lt;块引用&gt;
[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]

我尝试过的事情

只需重新排列 .classes_ 属性即可。
model.classes_ = [&#39;b&#39;,&#39;a&#39;,&#39;c&#39;]

&lt;块引用&gt;
AttributeError：无法设置属性“classes_”


根据 .classes_ 属性手动重新排列列。

desired_order = [&#39;b&#39;,&#39;a&#39;,&#39;c&#39;]
Correct_idx = [list(model._classes).index(val) for val indesired_order]
model.predict_proba(测试[特征])[:, Correct_idx]

#2 有效，但我不必在每次 predict_proba 调用时重新排列列顺序。]]></description>
      <guid>https://stackoverflow.com/questions/77639975/rearranging-lgbmclassifier-predict-proba-outputs-columns</guid>
      <pubDate>Mon, 11 Dec 2023 13:35:28 GMT</pubDate>
    </item>
    <item>
      <title>如何修复我的感知器来识别数字？</title>
      <link>https://stackoverflow.com/questions/77594625/how-can-i-fix-my-perceptron-to-recognize-numbers</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77594625/how-can-i-fix-my-perceptron-to-recognize-numbers</guid>
      <pubDate>Sun, 03 Dec 2023 14:03:49 GMT</pubDate>
    </item>
    <item>
      <title>CNN 架构的问题</title>
      <link>https://stackoverflow.com/questions/75060717/issue-with-cnn-architecture</link>
      <description><![CDATA[我正在尝试实现 CNN 架构，但是输出的形状存在问题。集合的形状如下：
x_train.shape、y_train.shape、x_test.shape、y_test.shape

&lt;前&gt;&lt;代码&gt;((1203, 162, 1), (1203, 7), (402, 162, 1), (402, 7))

架构设置如下：
input_x = tf.keras.layers.Input(shape = (x_train.shape[1],1))
conv_1 = tf.keras.layers.Conv1D(filters=16,kernel_size=3,padding=“相同”,activation=“relu”)(input_x)
pool_1 = tf.keras.layers.MaxPooling1D(2)(conv_1)
conv_2 = tf.keras.layers.Conv1D(filters=32,kernel_size=3,padding=“相同”,activation=“relu”)(pool_1)
pool_2 = tf.keras.layers.MaxPooling1D(2)(conv_2)

展平 = tf.keras.layers.Flatten()(pool_2)
密集= tf.keras.layers.Dense（512，激活=“relu”）（展平）
fb = tf.keras.layers.Dropout(0.4)（密集）
fb = tf.keras.layers.Dense(512，激活=“relu”)(fb)
fb = tf.keras.layers.Dropout(0.4)(fb)

输出= tf.keras.layers.Dense（8，激活=“softmax”）（fb）
model_branching_summed = tf.keras.models.Model(输入=input_x，输出=输出)
model_branching_summed.summary()
model_branching_summed.compile(optimizer=SGD(learning_rate=0.01,momentum=0.8),loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])


历史= model_branching_summed.fit（x_train，y_train，batch_size = 128，epochs = 100，validation_data =（x_test，y_test），callbacks = [rlrp]）

但是当我运行模型时，它给出以下错误：
ValueError Traceback（最近一次调用最后一次）
[192] 中的单元格，第 5 行
      1 rlrp =ReduceLROnPlateau(监视器=&#39;损失&#39;,因子=0.4,详细=0,耐心=2,min_lr=0.0001)
      2 #(min_lr=0.000001)
----&gt; 5 历史=model_branching_summed.fit（x_train，y_train，batch_size = 128，epochs = 100，validation_data =（x_test，y_test），callbacks = [rlrp]）

ValueError：形状（无，7）和（无，8）不兼容

哪里出错了？]]></description>
      <guid>https://stackoverflow.com/questions/75060717/issue-with-cnn-architecture</guid>
      <pubDate>Mon, 09 Jan 2023 17:09:09 GMT</pubDate>
    </item>
    <item>
      <title>将循环转换为双循环</title>
      <link>https://stackoverflow.com/questions/72054967/converting-recurrent-to-bi-recurrent</link>
      <description><![CDATA[我想将下面的 RNN 转换为双向 RNN，我该怎么做？
&lt;前&gt;&lt;代码&gt;模型 = RNN()
模型.summary()
model.compile(loss=&#39;categorical_crossentropy&#39;,optimizer=RMSprop(),metrics=[&#39;accuracy&#39;])
model.fit(X_train,Y_train,batch_size=10,epochs=20,
          验证分割=0.1）
]]></description>
      <guid>https://stackoverflow.com/questions/72054967/converting-recurrent-to-bi-recurrent</guid>
      <pubDate>Fri, 29 Apr 2022 08:27:10 GMT</pubDate>
    </item>
    </channel>
</rss>