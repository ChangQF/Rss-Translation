<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 17 May 2024 09:16:20 GMT</lastBuildDate>
    <item>
      <title>为什么我的神经网络（从头开始）回归仅学习数据集的第一个样本？</title>
      <link>https://stackoverflow.com/questions/78494357/why-does-my-neural-network-from-scratch-regression-learn-only-first-sample-of</link>
      <description><![CDATA[实现一个具有 16（特征）+1（偏差）输入和 1 个输出的回归任务的神经网络，我只使用 numpy 和向量化，当我在训练集上训练它时，输入的第一个样本是只有一个人学得很好，其他人都学了一点，但一点也不好。
我在反向传播操作中做错了什么吗？
通过在训练样本中添加值为 1 的特征，在第一层中实现偏差。
我尝试了不同的学习率和网络维度，但没有任何变化。这是我得到的输出，其中 l 是标签，y 是预测值，前 5 行是损失级数：
&lt;前&gt;&lt;代码&gt;损失：[8702.85226111]
损失：[6.46234854e-27]
损失：[1.61558713e-27]
损失：[4.03896783e-28]
损失：[0。]


左：131.042274 右：[131.042274]
左：64.0 右：[103.78313187]
l：89.429199 y：[30.54333083]
l：111.856492 y：[108.32052489]
左：69.3899 右：[57.11792288]

这是我用于此任务的 Colab 笔记本：
https://colab.research.google.com/drive/1SNEjgZQkmQW9LV8PSxE_Lx4VIQSjf1rP?usp=分享]]></description>
      <guid>https://stackoverflow.com/questions/78494357/why-does-my-neural-network-from-scratch-regression-learn-only-first-sample-of</guid>
      <pubDate>Fri, 17 May 2024 08:18:24 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：层equential_1从未被调用，因此没有定义的输入</title>
      <link>https://stackoverflow.com/questions/78493968/valueerror-the-layer-sequential-1-has-never-been-called-and-thus-has-no-defined</link>
      <description><![CDATA[我正在开发一个基于人工智能的项目，其中我训练了一个 CNN 模型，其名称是 cifar10.h5。我想传递这个预训练模型中的某些图像，并从倒数第二层提取这些图像的特征向量。
我的代码片段如下：-
original_model = load_model(&#39;/kaggle/input/models/cifar10.h5&#39;)
layer_before_softmax_output=original_model.layers[-2].output
Layer_before_softmax_model = keras.models.Model（输入=original_model.input，输出=layer_before_softmax_output）

我期望layer_before_softmax_model将成为我的新模型，输出层作为原始模型的倒数第二层。但我收到以下错误：-
ValueError：层equential_1从未被调用，因此没有定义的输入。
]]></description>
      <guid>https://stackoverflow.com/questions/78493968/valueerror-the-layer-sequential-1-has-never-been-called-and-thus-has-no-defined</guid>
      <pubDate>Fri, 17 May 2024 06:55:42 GMT</pubDate>
    </item>
    <item>
      <title>我的逻辑回归机器学习模型有问题。请帮我纠正我的代码</title>
      <link>https://stackoverflow.com/questions/78493918/i-am-having-problem-with-my-logistics-regression-machine-learning-model-please</link>
      <description><![CDATA[我的模型精度很差。此数据取自 https://archive.ics.uci .edu/dataset/15/breast+cancer+wisconsin+original 显示逻辑回归模型的准确度为 96%，所以问题确实出在我的模型中。我在 R 中构建了以下模型。
# 导入数据集
tumor_study &lt;- read.csv(“breast-cancer-wisconsin.data”, header = FALSE, na.strings = “NA”)

# 添加列名
特征&lt;-c(“id_number”，“ClumpThickness”，“Uniformity_CellSize”，
              “Uniformity_CellShape”、“边缘粘附”、
              “SingleEpithelial_CellSize”、“BareNuclei”、“Bland_Chromatin”、
              “Normal_Nucleoli”、“Mitoses”、“Class”）

colnames(tumor_study) &lt;- 特征

# 清洗数据
# 删除第一列（id_number）
肿瘤研究 &lt;- 肿瘤研究[,-1]

# 转换“?” BareNuclei 列中为 NA，然后为数字
tumor_study$BareNuclei[tumor_study$BareNuclei == &quot;?&quot;] &lt;- NA
tumor_study$BareNuclei &lt;- as.numeric(tumor_study$BareNuclei)

# 删除 BareNuclei 中缺失值的行
tumor_study &lt;-tumor_study[!is.na(tumor_study$BareNuclei),]

# 将类转换为因子
tumor_study$Class &lt;- 因子(tumor_study$Class, level = c(2, 4), labels = c(“良性”, “恶性”))

# 将数据集分为训练集和测试集
库（caTools）
设置.种子(123)
split &lt;-sample.split(tumor_study$Class, SplitRatio = 0.8)
Training_set &lt;-tumor_study[split == TRUE,]
test_set &lt;-tumor_study[split == FALSE,]

# 应用特征缩放
训练集[, 1:9] &lt;- 比例(训练集[, 1:9])
test_set[, 1:9] &lt;- 比例(test_set[, 1:9])

# 构建逻辑回归模型
分类器 &lt;- glm(公式 = Class ~ ., family = 二项式, data = Training_set)

# 预测训练集的概率
prob_y_train &lt;- 预测（分类器，类型 = &#39;响应&#39;，newdata = Training_set[,-10]）
Predicted_y_training &lt;- ifelse(prob_y_train &gt;= 0.5,“良性”,“恶性”)

# 使用 test_set 进行预测
prob_y_test &lt;- 预测（分类器，类型 = &#39;响应&#39;，newdata = test_set[,-10]）
Predicted_y_test &lt;- ifelse(prob_y_test &gt;= 0.5,“良性”,“恶性”)

# 使用混淆矩阵检查准确性
cm_test &lt;- 表(test_set[,10], Predicted_y_test)
打印（厘米_测试）

## 如果你检查准确度...它接近 2%

我无法找出模型中的问题。一定有问题。我期待有人告诉我错误在哪里。]]></description>
      <guid>https://stackoverflow.com/questions/78493918/i-am-having-problem-with-my-logistics-regression-machine-learning-model-please</guid>
      <pubDate>Fri, 17 May 2024 06:43:30 GMT</pubDate>
    </item>
    <item>
      <title>反馈管理器需要具有单一签名推断的模型</title>
      <link>https://stackoverflow.com/questions/78493742/feedback-manager-requires-a-model-with-a-single-signature-inference</link>
      <description><![CDATA[我在尝试运行机器运行模型时遇到了此错误，该模型应该为驾驶员睡意检测项目提供动力
”
W0000 00:00:1715924294.765512 2256 inference_feedback_manager.cc:114] 反馈管理器需要具有单个签名推理的模型。禁用对反馈张量的支持。”
模型架构如下：
&lt;前&gt;&lt;代码&gt;#**型号**
从 keras.layers 导入 BatchNormalization
模型 = tf.keras.models.Sequential()
# 输入形状是所需的图像大小 145 x 145，颜色为 3 字节

#这是第一个卷积
   model.add(Conv2D(16, 3, 激活=&#39;relu&#39;, input_shape=X_train.shape[1:]))
   model.add(BatchNormalization())
   model.add(MaxPooling2D())
   tf.keras.layers.Dropout(0.3)

# 第二次卷积
   model.add(Conv2D(32, 5, 激活=&#39;relu&#39;))
   model.add(BatchNormalization())
   model.add(MaxPooling2D())
   tf.keras.layers.Dropout(0.3)

# 第三次卷积
  model.add(Conv2D(64, 10, 激活=&#39;relu&#39;))
  model.add(BatchNormalization())
  model.add(MaxPooling2D())
  tf.keras.layers.Dropout(0.3)

# 第四次卷积
  model.add(Conv2D(128, 12, 激活=&#39;relu&#39;))
  model.add(BatchNormalization())

# 将结果压平以输入 DNN
  模型.add(压平())
  model.add（密集（128，激活=&#39;relu&#39;））
  模型.add(Dropout(0.25))
  model.add（密集（64，激活=&#39;relu&#39;））
# 只有 1 个输出神经元。
  model.add（密集（1，激活=&#39;sigmoid&#39;））

  model.compile(loss=“binary_crossentropy”，metrics=[“accuracy”]，optimizer=Adam(lr=0.001))
  历史= model.fit（train_generator，epochs = 10，batch_size = 32，validation_data = test_generator）

# 定义服务签名
  输入签名 = [
      tf.TensorSpec(shape=[None, 145, 145, 3], dtype=tf.float32, name=&#39;input_tensor&#39;)
  ]

@tf.function(input_signature=input_signature)
defserving_fn（输入）：
    返回模型（输入）

export_dir = &#39;E:\系统项目\项目&#39;
tf.saved_model.save（serving_fn，export_dir）

# 加载模型进行推理
load_model = tf.saved_model.load(&#39;my_model.keras&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78493742/feedback-manager-requires-a-model-with-a-single-signature-inference</guid>
      <pubDate>Fri, 17 May 2024 05:59:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在流模式下应用 .map() 函数并将其保留为拥抱脸部数据集的迭代器而不将其加载到内存中？</title>
      <link>https://stackoverflow.com/questions/78493588/how-to-apply-map-function-and-keep-it-as-an-iterator-for-a-hugging-face-datas</link>
      <description><![CDATA[我目前正在使用 Hugging Face 数据集库，需要使用 .map() 函数对多个数据集（例如 ds_khan 和 ds_mathematica）应用转换，但以模仿流的方式（即，不加载整个数据集进入内存）。我特别感兴趣的是交错这些转换后的数据集，同时保持数据处理尽可能懒惰，类似于streaming=True。
这是我当前代码的相关部分：
从数据集导入 load_dataset, interleave_datasets

def get_hf_khan_ds(path_2_ds: str, split: str = &#39;train&#39;):
    path_2_ds = os.path.expanduser(path_2_ds)
    数据集= load_dataset（&#39;json&#39;，data_files = [path_2_ds]，split = split，streaming = True）
    Problem_as_text = lambda 示例：{&#39;text&#39;: example[&#39;problem&#39;]}
    返回dataset.map（problem_as_text，remove_columns = dataset.column_names）

def main():
    ds_khan = get_hf_khan_ds(&#39;~/gold-ai-olympiad/data/amps/khan/train.jsonl&#39;)
    ds_mathematica = get_hf_khan_ds(&#39;~/gold-ai-olympiad/data/amps/mathematica/train.jsonl&#39;)
    interleaved_datasets = interleave_datasets([ds_khan, ds_mathematica], 概率=[0.5, 0.5])
    对于 interleaved_datasets.take(10) 中的示例：
        打印（样本）

如果 __name__ == &#39;__main__&#39;:
    主要的（）

此设置旨在处理和交错数据集，而不将它们完全加载到内存中。但是，我不确定这种方法是否按照我的意图正确实现了流式计算和惰性求值。
问题：

此代码是否以流式或迭代器式方式正确应用转换？
如果不是，我该如何修改它以确保每个数据集仅根据需要进行处理，而不预加载整个内容？
是否有更有效的方法来交错这些数据集，同时保持流式传输方法？

任何关于如何有效地使用 .map() 和 Streaming=True 来交错数据集的建议或见解将不胜感激（注意我确实在磁盘中有数据集，但最终我想使用 HF 数据集）。
输出：
 表 = cls._concat_blocks(块, axis=0)
地图：100%|█████████████████████████████████████████████ ████████████████████████████████████████| 103059/103059 [00:06&lt;00:00, 16218.25 个示例/秒]
地图：100%|█████████████████████████████████████████████ ████████████████████████████████████████| 103059/103059 [00:07&lt;00:00, 13633.64 个示例/秒]
地图：100%|█████████████████████████████████████████████ ████████████████████████████████████████| 103059/103059 [00:08&lt;00:00, 12444.64 个示例/秒]
/lfs/ampere1/0/brando9/miniconda/envs/gold_ai_olympiad/lib/python3.11/site-packages/datasets/table.py:1421：FutureWarning：promote 已被 Promotion_options=&#39;default&#39; 取代。
  表= cls._concat_blocks（块，轴= 0）
下载数据文件：100%|███████████████████████████████████████████ █████████████████████████████████████████| 1/1 [00:00&lt;00:00, 3792.32it/s]
提取数据文件：100%|███████████████████████████████████████████ ███████████████████████████████████████████| 1/1 [00:00&lt;00:00, 479.84it/s]
生成训练分割：20 个示例 [00:00，3628.29 个示例/秒]
地图：100%|█████████████████████████████████████████████ █████████████████████████████████████████████████| 20/20 [00:00&lt;00:00, 2792.11 示例/秒]
地图：100%|█████████████████████████████████████████████ █████████████████████████████████████████████████| 20/20 [00:00&lt;00:00, 3525.66 示例/秒]
地图：100%|█████████████████████████████████████████████ █████████████████████████████████████████████████| 20/20 [00:00&lt;00:00, 3415.97 示例/秒]
/lfs/ampere1/0/brando9/miniconda/envs/gold_ai_olympiad/lib/python3.11/site-packages/datasets/table.py:1421：FutureWarning：promote 已被 Promotion_options=&#39;default&#39; 取代。
  表= cls._concat_blocks（块，轴= 0）
概率=[0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
/lfs/ampere1/0/brando9/miniconda/envs/gold_ai_olympiad/lib/python3.11/site-packages/datasets/table.py:1421：FutureWarning：promote 已被 Promotion_options=&#39;default&#39; 取代。
  表= cls._concat_blocks（块，轴= 0）
完毕！时间：50.09秒、0.83分钟、0.01小时

我没想到输出会显示整个数据集......这让我很困惑。而且加载时间太长，这很糟糕。
参考：https://discuss.huggingface.co/t/how-to-apply-map-function -and-keep-it-as-an-iterator-for-a-hugging-face-dataset-in-streaming-mode-without-loading-it-to-memroy/87110]]></description>
      <guid>https://stackoverflow.com/questions/78493588/how-to-apply-map-function-and-keep-it-as-an-iterator-for-a-hugging-face-datas</guid>
      <pubDate>Fri, 17 May 2024 05:14:40 GMT</pubDate>
    </item>
    <item>
      <title>我试图运行此命令！pip install tensorflow-gpu in google colab 出现错误</title>
      <link>https://stackoverflow.com/questions/78493587/i-was-trying-to-run-this-command-pip-install-tensorflow-gpu-in-google-colab-to</link>
      <description><![CDATA[收集tensorflow-gpu
使用缓存的tensorflow-gpu-2.12.0.tar.gz (2.6 kB)
错误：子进程退出并出现错误
× python setup.py Egg_info 未成功运行。
│ 退出代码：1
╰─&gt;请参阅上面的输出。
注意：此错误源自子进程，并且可能不是 pip 的问题。
准备元数据（setup.py）...错误
错误：元数据生成失败
× 生成包元数据时遇到错误。
╰─&gt;请参阅上面的输出。
注意：这是上面提到的包的问题，​​而不是 pip 的问题。
提示：详细信息请参见上文。现在该怎么办
检查了blackbox ai，但没有帮助]]></description>
      <guid>https://stackoverflow.com/questions/78493587/i-was-trying-to-run-this-command-pip-install-tensorflow-gpu-in-google-colab-to</guid>
      <pubDate>Fri, 17 May 2024 05:14:37 GMT</pubDate>
    </item>
    <item>
      <title>将图像数据集转换为 csv 文件 [关闭]</title>
      <link>https://stackoverflow.com/questions/78493421/convert-a-image-dataset-into-a-csv-file</link>
      <description><![CDATA[这是我的目录结构，class_names 是标签，我正在寻找一种将 class_names 转换为标签并将图像转换为 csv 文件的方法。请给我一个方法。目录结构(https://i.sstatic.net/7xQpQreK.png)
我正在寻找如何在 python 中做到这一点]]></description>
      <guid>https://stackoverflow.com/questions/78493421/convert-a-image-dataset-into-a-csv-file</guid>
      <pubDate>Fri, 17 May 2024 04:05:42 GMT</pubDate>
    </item>
    <item>
      <title>UndefinedMetricWarning：精度定义不明确，在没有预测样本的标签中设置为 0.0。使用“zero_division”错误</title>
      <link>https://stackoverflow.com/questions/78493410/undefinedmetricwarning-precision-is-ill-defined-and-being-set-to-0-0-in-labels</link>
      <description><![CDATA[我正在 VS Code 上使用 Python 来开始使用 Wine Quality ML 进行训练。
但我不断得到
“UndefinedMetricWarning：精度定义不明确，在没有预测样本的标签中设置为 0.0。使用 zero_division 参数来控制此行为。”
我的[准确率][召回率、fi-score] 一直显示为空白：
 精确召回率 f1-score 支持

           3 0.00 0.00 0.00 4
           4 1.00 0.06 0.11 17
           5 0.69 0.69 0.69 231
           6 0.57 0.62 0.60 226
           7 0.38 0.42 0.40 50
           8 0.00 0.00 0.00 5

    准确度 0.60 533
   宏观平均 0.44 0.30 0.30 533
加权平均 0.61 0.60 0.59 533

我已经设置了：
mlp.fit(X_train, y_train)

预测 = mlp.predict(X_test)
打印（分类报告（y_test，预测，zero_division = 0））
打印（预测，&#39;\n&#39;）

我的代码中的其他所有内容似乎都正常工作：
df = pd.read_csv(&#39;winequality-red.csv&#39;, sep =&#39;;&#39;)


X = df.drop(&#39;质量&#39;, 轴 = 1)
y = df[&#39;质量&#39;]

X_train，X_test，y_train，y_test = train_test_split（X，y，test_size = 0.333，random_state = 0）
X_训练，X_测试

定标器=标准定标器()
缩放器.fit(X_train)

X_train = 缩放器.transform(X_train)
X_test = 缩放器.transform(X_test)

打印（df.head（））
打印(df.describe().transpose())

打印（X_train，&#39;\n&#39;）
打印（X_test，&#39;\n&#39;）

mlp = MLPClassifier(hidden_​​layer_sizes=(11, 11)，激活=&#39;逻辑&#39;，alpha=1e-06，batch_size=&#39;auto&#39;，beta_1=0.9，beta_2=0.999，early_stopping=False，epsilon=1e-08，learning_rate= &#39;自适应&#39;，
                    Learning_rate_init=0.5，max_iter=10000，动量=0.4，n_iter_no_change=10，nesterovs_momentum=True，power_t=0.5，random_state=1，shuffle=True，solver=&#39;sgd&#39;，tol=1e-09，validation_fraction=0.1，
                    详细=10，warm_start=False）

mlp.fit(X_train, y_train)

预测 = mlp.predict(X_test)
打印（分类报告（y_test，预测，zero_division = 0））
打印（预测，&#39;\n&#39;）

打印（confusion_matrix（y_test，预测），&#39;\n&#39;）
打印（分类报告（y_测试，预测））


e = y_test - 预测
e
]]></description>
      <guid>https://stackoverflow.com/questions/78493410/undefinedmetricwarning-precision-is-ill-defined-and-being-set-to-0-0-in-labels</guid>
      <pubDate>Fri, 17 May 2024 03:59:52 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 或 transformer 模型是否有可逆的实现？</title>
      <link>https://stackoverflow.com/questions/78493118/is-there-any-reversible-implementation-for-lstm-or-transformer-models</link>
      <description><![CDATA[可以训练 LSTM 和转换器来注入数据流（例如键入的文本）并输出概率分数流，即序列到序列。
这对于文体评分、仇恨语言检测等很有用。
我想象用户通过基于 Web 的 GUI 与模型交互。
如果使用后端模型（为了论证：LSTM）是为了在用户文本块旁边动态显示分数，那么在大多数情况下，键入的字符可以通过模型逐一输入-一。
然而，现实世界的书写工具用户经常会出现拼写错误，从而在继续书写之前多次按退格键。这会破坏 seq2seq 模型的流程。可以想象，整个文本块需要再次通过模型输入，以达到先前点（用户“退格”到的点）出现的模型状态。
或者，对于较小的模型，可以在每次按键时缓存模型的状态，但我认为这在大多数情况下是不可行的。
我的问题是：有没有能够“倒带”状态的 seq2seq 模型？ IE。输入字符不是使用输入字符来确定下一个输出项，而是相反地到达，其预期目的是倒带存储单元的内部状态？这太好了，因为前一个示例中的退格键不会触发 LSTM 从最顶部传递整个文本。]]></description>
      <guid>https://stackoverflow.com/questions/78493118/is-there-any-reversible-implementation-for-lstm-or-transformer-models</guid>
      <pubDate>Fri, 17 May 2024 01:47:00 GMT</pubDate>
    </item>
    <item>
      <title>在 python 上从头开始实现 XGBoost</title>
      <link>https://stackoverflow.com/questions/78492767/implementing-xgboost-from-scratch-on-python</link>
      <description><![CDATA[我尝试编写自己的实现
class XGBoost_own(BaseEstimator, ClassifierMixin):
def __init__(self, n_estimators=100, learning_rate=0.2, max_depth=3,
loss=&#39;logistic&#39;, reg_alpha=0, reg_lambda=1, random_state=0):

self.n_estimators = n_estimators
self.max_depth = max_depth
self.learning_rate = learning_rate
self.loss = loss
self.reg_alpha = reg_alpha
self.reg_lambda = reg_lambda
self.initialization = lambda y: np.mean(y) * np.ones([y.shape[0]])
self.loss_by_iter = []
self.trees_ = []
self.boosted_pred = None
self.random_state = random_state

def _sigmoid(self, predictions):
return 1 / (1 + np.exp(-predictions))

def _softmax(self, predictions):
exp = np.exp(predictions)
return exp / np.sum(exp, axis=1, keepdims=True)

def _compute_loss_gradient(self, F, y):
if self.loss == &#39;logistic&#39;:
return y - self._sigmoid(F)
else:
raise ValueError(&quot;不支持的损失函数&quot;)

def _compute_regularization_penalty(self):
if self.reg_alpha == 0:
return 0
elif self.reg_alpha &gt; 0 且 self.reg_lambda == 0:
返回 self.reg_alpha * np.sum(np.abs(tree.coef_))
elif self.reg_alpha == 0 且 self.reg_lambda &gt; 0:
返回 0.5 * self.reg_lambda * np.sum(tree.coef_ ** 2)
elif self.reg_alpha &gt; 0 且 self.reg_lambda &gt; 0:
return self.reg_alpha * np.sum(np.abs(tree.coef_)) + 0.5 * self.reg_lambda * np.sum(tree.coef_ ** 2)
else:
raise ValueError(&quot;正则化参数组合无效&quot;)

def fit(self, X, y):
self.X = X
self.y = y
self.boosted_pred = np.zeros_like(y)
F = self.initialization(y)

for t in range(self.n_estimators):
residuals = self._compute_loss_gradient(F, y)
tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=self.random_state)
tree.fit(X, residuals)
self.trees_.append(tree)
update = self.learning_rate * tree.predict(X)
F += update
self.boosted_pred += update

return self

def predict_proba(self, X):
F = np.zeros(X.shape[0])
for tree in self.trees_:
F += self.learning_rate * tree.predict(X)
F = np.clip(F, -700, 700)
if self.loss == &#39;logistic&#39;:
return self._sigmoid(F).reshape(-1, 1)
elif self.loss == &#39;softmax&#39;:
return self._softmax(F)
else:
raise ValueError(&quot;不支持的损失函数&quot;)

def predict(self, X):
proba = self.predict_proba(X)
return np.argmax(proba, axis=1)

def get_params(self, deep=True):
return {
&#39;n_estimators&#39;: self.n_estimators,
&#39;learning_rate&#39;: self.learning_rate,
&#39;max_depth&#39;: self.max_depth,
&#39;loss&#39;: self.loss,
&#39;reg_alpha&#39;: self.reg_alpha,
&#39;reg_lambda&#39;: self.reg_lambda,
&#39;random_state&#39;: self.random_state,
}

通过函数运行模型。也许我实现得不正确。我很困惑。此外，有人已经做到了，我很高兴看到 LightGBM、CatBoost、基于直方图的梯度提升的自写实现
def hyperparameter_tuning(models, param_grid, Xre_train, Yre_train, Xre_test, Yre_test, results):
for name, model in models.items():
print(f&quot;Tuning hyperparameters for {name}...&quot;)
param_grid_name = param_grid[name]
grid_search = GridSearchCV(estimator=model, param_grid=param_grid_name,scoring=&#39;accuracy&#39;, cv=5, n_jobs=-1)
grid_search.fit(Xre_train, Yre_train)
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
Yre_pred = best_model.predict(Xre_test)
accuracy = accuracy_score(Yre_test, Yre_pred)
precision = precision_score(Yre_test, Yre_pred, average=&#39;weighted&#39;)
recall = recall_score(Yre_test, Yre_pred, average=&#39;weighted&#39;)
f1 = f1_score(Yre_test, Yre_pred, average=&#39;weighted&#39;)
roc_auc = roc_auc_score(Yre_test, best_model.predict_proba(Xre_test), multi_class=&#39;ovr&#39;, average=&#39;weighted&#39;)
chaos = chaos_matrix(Yre_test, Yre_pred)

我收到错误。多类分类的数据集
numpy.core._exceptions._UFuncOutputCastingError：无法从中转换 ufunc &#39;add&#39; 输出dtype(&#39;float64&#39;) 转换为 dtype(&#39;int32&#39;)，转换规则为 &#39;same_kind&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78492767/implementing-xgboost-from-scratch-on-python</guid>
      <pubDate>Thu, 16 May 2024 23:00:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 BERT 对连续数据流进行分类</title>
      <link>https://stackoverflow.com/questions/78491661/categorising-continuous-data-streams-with-bert</link>
      <description><![CDATA[我正在研究构建分类系统，该系统基本上将通过各种管道实时摄取连续的数据流，例如 Twitter 帖子和这些帖子中的评论，以文章形式来自网站源的数据。
我希望系统将数据组织在主题和子主题中，例如：
主题——笔记本电脑
副主题——宏碁推出全新 XYZ 笔记本电脑
我无意使用生成式人工智能来编写子主题，我更期待从数据源中形成句子，例如，如果宏碁推出 XYZ 笔记本电脑，那么所有 Twitter 帖子和文章都会包含类似的内容“宏碁透露/推出了等等”。
我只需要一个从哪里开始的方向，因为我有点迷失如何实现它
我什至从来没有超越过构建数据流。更不用说设置模型来进行分类了]]></description>
      <guid>https://stackoverflow.com/questions/78491661/categorising-continuous-data-streams-with-bert</guid>
      <pubDate>Thu, 16 May 2024 17:44:49 GMT</pubDate>
    </item>
    <item>
      <title>TypeError“NoneType”对象不可下标</title>
      <link>https://stackoverflow.com/questions/78491414/typeerror-nonetype-object-is-not-subscriptable</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78491414/typeerror-nonetype-object-is-not-subscriptable</guid>
      <pubDate>Thu, 16 May 2024 16:44:52 GMT</pubDate>
    </item>
    <item>
      <title>有 0 张图像属于 0 个类别，我被卡住了</title>
      <link>https://stackoverflow.com/questions/78490995/there-are-0-images-belonging-to-0-classes-and-im-stuck</link>
      <description><![CDATA[我尝试让程序使用 google Colab 运行我的 google 驱动器中的图像，但它在 model.fit(train_generator, epochs=10,validation_data=test_generator) 处不断出现错误
我尝试将批次大小调整为 10，并将所有图像大小调整为 100，我认为它会起作用，但它仍然给了我
找到属于 0 个类别的 0 个图像。
找到属于 0 个类别的 0 张图片。

我将训练和测试图像放入两个单独的文件夹中，分别名为“training_data”和“testing_data”。
!pip 安装tensorflow
导入CV2
从tensorflow.keras.preprocessing.image导入img_to_array
将 numpy 导入为 np
将张量流导入为 tf
从tensorflow.keras.preprocessing.image导入ImageDataGenerator
从tensorflow.keras.applications导入EfficientNetB0
从tensorflow.keras.layers导入密集，GlobalAveragePooling2D
从tensorflow.keras.models导入模型
从sklearn.metrics导入confusion_matrix、accuracy_score、 precision_score、recall_score

#上传图片
def load_image(图像位置):
    # 使用OpenCV加载图像
    图像 = cv2.imread(图像位置)

   #检查图片是否上传
    如果图像为无：
        print(“错误：找不到图像”，image_location)
        返回无

    resized_image= cv2.resize(图像, (100, 100))
    image_array = img_to_array(调整大小的图像)

    图像数组 /= 255.0

    返回图像数组


example_image_location = &#39;/content/drive/MyDrive/机器学习文件夹/training_data/LightGreen_Crayon_Testing.jpg&#39;

# 加载并预处理示例图像
示例图像 = 加载图像（示例图像位置）


如果 example_image 不是 None：

    # 收集/处理数据
  train_dir = &#39;/content/drive/MyDrive/机器学习文件夹/training_data&#39;
  test_dir = &#39;/content/drive/MyDrive/机器学习文件夹/testing_data&#39;

  train_datagen = ImageDataGenerator（重新缩放=1。/ 255）
  test_datagen = ImageDataGenerator（重新缩放=1。/ 255）

  train_generator = train_datagen.flow_from_directory(train_dir, target_size=(100, 100), batch_size=10, class_mode=&#39;binary&#39;)

  test_generator = test_datagen.flow_from_directory(test_dir, target_size=(100, 100), batch_size=10, class_mode=&#39;binary&#39;)

    ＃ 建筑模型
  base_model = EfficientNetB0(weights=&#39;imagenet&#39;, include_top=False)
  x = 基础模型.输出
  x = GlobalAveragePooling2D()(x)
  x = 密集（1024，激活=&#39;relu&#39;）（x）
  预测=密集（1，激活=&#39;sigmoid&#39;）（x）
  模型 = 模型（输入=base_model.输入，输出=预测）

  对于 base_model.layers 中的图层：
    可训练层 = False
    model.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
    model.fit（train_generator，epochs = 10，validation_data = test_generator）
]]></description>
      <guid>https://stackoverflow.com/questions/78490995/there-are-0-images-belonging-to-0-classes-and-im-stuck</guid>
      <pubDate>Thu, 16 May 2024 15:27:25 GMT</pubDate>
    </item>
    <item>
      <title>RandomSearchCV 和数据泄漏</title>
      <link>https://stackoverflow.com/questions/78490506/randomsearchcv-and-data-leakage</link>
      <description><![CDATA[我正在寻找避免数据泄露的最佳实践。我有 1 个需要模式插补的功能。该模型是 XGBoost 分类器。
这些是我计划的步骤：

将数据随机拆分为 80% 训练集 - 20% 测试集
分别对训练和测试集应用模式插补
在训练集上执行 RandomSearchCV 以进行超参数调整
使用找到的最佳参数集在整个训练集上训练模型
在看不见的测试集上评估模型

现在，我的疑问是：对整个训练集执行模式插补然后执行 RandomSearchCV 可以吗？我认为数据泄露仅适用于使用看不见的测试集进行评估。
或者我应该在 RandomSearchCV 的每个折叠中执行插补以避免数据泄露？如果是这样，我该怎么做？我看到了 Sklearn 管道，但我不知道如何将模式插补仅应用于我需要的特定特征
提前谢谢您！
附言：如果有任何错误，也欢迎对所有步骤提出反馈！]]></description>
      <guid>https://stackoverflow.com/questions/78490506/randomsearchcv-and-data-leakage</guid>
      <pubDate>Thu, 16 May 2024 14:09:48 GMT</pubDate>
    </item>
    <item>
      <title>无法加载可示教机器模型</title>
      <link>https://stackoverflow.com/questions/78237621/unable-to-load-the-teachable-machine-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78237621/unable-to-load-the-teachable-machine-model</guid>
      <pubDate>Thu, 28 Mar 2024 10:51:19 GMT</pubDate>
    </item>
    </channel>
</rss>