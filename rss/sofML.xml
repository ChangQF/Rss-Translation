<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 25 Sep 2024 01:16:39 GMT</lastBuildDate>
    <item>
      <title>如何在 PyTorch 中训练眼睛验证（而非识别）模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</link>
      <description><![CDATA[我想知道我们如何训练一对一图像验证模型。模型会拍摄两张图像并验证它们是否相同。
我在网上搜索过，但只能找到有关识别（一对多）的答案。
如何在文本或代码中创建这样的模型？
为了澄清起见，我说的相同是指眼睛相同，即它们属于同一个人。这是一个验证模型。]]></description>
      <guid>https://stackoverflow.com/questions/79019854/how-to-train-an-eye-verification-not-recognition-model-in-pytorch</guid>
      <pubDate>Tue, 24 Sep 2024 18:10:07 GMT</pubDate>
    </item>
    <item>
      <title>如何为排名模型生成数据集？[关闭]</title>
      <link>https://stackoverflow.com/questions/79019494/how-generate-dataset-for-ranking-model</link>
      <description><![CDATA[我正在尝试创建两阶段推荐系统：使用矩阵分解生成候选对象，并使用 Lambdarank 排名模型对其进行排名。我有两个选项来生成数据集：

取 128 个项目
（相关项目 + 随机项目填充），使用第一个模型对其进行评分和排序，然后使用此序列训练第二个模型（因此我们始终具有相对值。
对所有项目进行评分，排序并取前 128 个项目，然后进行训练（我们可能没有相关项目，但我认为这更自然，因为在生产中我们将以这种方式进行预测）。

那么，哪个更好？此外，在训练中使用小权重（类似于隐式 ALS）填充项目是否有意义？]]></description>
      <guid>https://stackoverflow.com/questions/79019494/how-generate-dataset-for-ranking-model</guid>
      <pubDate>Tue, 24 Sep 2024 16:14:00 GMT</pubDate>
    </item>
    <item>
      <title>类型错误：无法找到类“Sequential”</title>
      <link>https://stackoverflow.com/questions/79019296/typeerror-could-not-locate-class-sequential</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79019296/typeerror-could-not-locate-class-sequential</guid>
      <pubDate>Tue, 24 Sep 2024 15:17:10 GMT</pubDate>
    </item>
    <item>
      <title>如何将预测值合并回数据集？</title>
      <link>https://stackoverflow.com/questions/79018990/how-to-merge-predicted-value-back-to-the-data-set</link>
      <description><![CDATA[我已经在 Python 中训练了一个 XGboost 模型，并将概率列表作为输出。我如何将这些概率带到原始数据集，以便在一个 DF 中拥有数据 + 预测值？假设我的原始原始测试 df 称为 df_raw。
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)
model = XGBClassifier(n_estimators=1500, max_depth=5, n_jobs=-1, min_child_weight=2, 
early_stopping_rounds=25)
model.fit(X_train, y_train, eval_set=[(X_test, y_test)])
test_outputs = model.predict_proba(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/79018990/how-to-merge-predicted-value-back-to-the-data-set</guid>
      <pubDate>Tue, 24 Sep 2024 14:08:01 GMT</pubDate>
    </item>
    <item>
      <title>cuDNN 错误：CUDNN_STATUS_EXECUTION_FAILED</title>
      <link>https://stackoverflow.com/questions/79018072/cudnn-error-cudnn-status-execution-failed</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79018072/cudnn-error-cudnn-status-execution-failed</guid>
      <pubDate>Tue, 24 Sep 2024 10:04:05 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 smote 将过采样数据存储在单独的变量中？</title>
      <link>https://stackoverflow.com/questions/79016928/how-can-i-store-the-oversampled-data-using-smote-in-a-separate-variable</link>
      <description><![CDATA[应用 Smote 过采样技术后，我只想将新生成的值存储到 X2 和 y2。X2 的独立特征和 y2 的目标变量
import numpy as np
import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
dataset = pd.read_csv(&#39;https://archive.ics.uci.edu/static/public/17/data.csv&#39;)
X = dataset.iloc[:, 1:-1].values
y = dataset.iloc[:, -1].values
le = LabelEncoder()
y = le.fit_transform(y)
smt = SMOTE()
X1, y1 = smt.fit_resample(X, y)
#在单独的变量中使用 smote 对数据进行过采样
#X2 = ?
#y2 = ?

]]></description>
      <guid>https://stackoverflow.com/questions/79016928/how-can-i-store-the-oversampled-data-using-smote-in-a-separate-variable</guid>
      <pubDate>Tue, 24 Sep 2024 04:07:41 GMT</pubDate>
    </item>
    <item>
      <title>通过模型的大规模测试预测毒性测定[关闭]</title>
      <link>https://stackoverflow.com/questions/79016340/predicting-toxicity-assay-through-mass-testing-of-models</link>
      <description><![CDATA[我目前正在创建一个模型来预测污染对生物体的毒性测定。由于没有合适的数据集，我还没有尝试任何东西。我只是想问问我的代码是否合适。欢迎提出批评。此外，如果我遗漏了什么或应该包括什么，请告诉我。
此外，我正在考虑更多模型，例如 RandomForestRegressor、Boosting（AdaBoost、GradientBoost）。我应该考虑这些吗？此外，当我最终获得数据时，是否有任何模型我应该从测试中删除？
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv(&#39;&#39;) # 包含数据的 csv 文件（浓度和死亡率）

# 基本图表
sns.scatterplot(data = df, x = &#39;Concentration&#39;, y = &#39;Mortality&#39;) 

# 训练与测试的基本划分
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=101) 

# 线性模型
from sklearn.linear_model import LinearRegression 
lr_model = LinearRegression()
lr_model.fit(X_train,y_train)
lr_preds = lr_model.predict(X_test)
from sklearn.metrics import mean_absolute_error, mean_squared_error
mean_absolute_error(y_test, lr_preds)
np.sqrt(mean_absolute_error(y_test, lr_preds))
concentration_range = np.arange(0,100) # 根据最小/最大浓度调整
concentration_preds = lr_model.predict(concentration_range.reshape(-1,1))
plt.figure(figsize = (12,6),dpi = 200)
sns.scatterplot(data = df, x = &#39;Concentration&#39;, y = &#39;信号&#39;)
plt.plot(concentration_range,concentration_preds)

# 多项式模型
# 用于测试模型的函数
def run_model(model, X_train, y_train, X_test, y_test):
# 拟合模型
model.fit(X_train,y_train)

# 获取指标
preds = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test,preds))
mae = mean_absolute_error(y_test, preds)
print(f&#39;MAE: {mae}&#39;)
print(f&#39;RMSE: {rmse}&#39;)

# 绘制结果模型信号范围
density_range = np.arange(0,100) # 再次调整
density_preds = model.predict(concentration_range.reshape(-1,1))

plt.figure(figsize = (12,8), dpi = 200)
sns.scatterplot(x = &#39;Concentration&#39;, y = &#39;Mortality&#39;, data = df, color = &#39;black&#39;)
plt.plot(concentration_range, density_preds)

来自 sklearn.pipeline 导入 make_pipeline
来自 sklearn.preprocessing 导入 PolynomialFeatures

pipe = make_pipeline(PolynomialFeatures(degree = 2),LinearRegression()) # degree 可调整
run_model(pipe, X_train, y_train, X_test, y_test)

# K-Nearest Neighbors 模型
来自 sklearn.neighbors 导入 KNeighborsRegressor
k_values = [1,2,3,4,5,6,7,8,9,10]
for k in k_values:

model = KNeighborsRegressor(n_neighbors=k)
run_model(model, X_train,y_train,X_test, y_test)

# 决策树模型
from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()
run_model(model, X_train, y_train, X_test, y_test)

# SVR 模型
from sklearn.svm import SVR # 支持向量回归
from sklearn.model_selection import GridSearchCV
svr = SVR()
param_grid = {&#39;C&#39;:[0.01,0.1,1,5,10,100,1000],
&#39;gamma&#39;:[&#39;auto&#39;,&#39;scale&#39;]}

grid = GridSearchCV(svr, param_grid)
run_model(grid, X_train,y_train,X_test, y_test)
]]></description>
      <guid>https://stackoverflow.com/questions/79016340/predicting-toxicity-assay-through-mass-testing-of-models</guid>
      <pubDate>Mon, 23 Sep 2024 21:25:16 GMT</pubDate>
    </item>
    <item>
      <title>有人知道如何修复库错误吗？</title>
      <link>https://stackoverflow.com/questions/79015388/does-anyone-know-how-to-fix-library-error</link>
      <description><![CDATA[我遇到了这个错误，尝试了所有方法，但还是无法解决：
ModuleNotFoundError Traceback (most recent call last)
&lt;ipython-input-13-6c7180fd4822&gt; in &lt;cell line: 1&gt;()
----&gt; 1 from crewapi_module import CrewAPI # 用正确的导入路径替换
2 
3 crew_api = CrewAPI(api_key=&quot;your_crewai_api_key&quot;)
4 
5 # 现在您可以与 API 交互，例如：

ModuleNotFoundError：没有名为“crewapi_module”的模块
]]></description>
      <guid>https://stackoverflow.com/questions/79015388/does-anyone-know-how-to-fix-library-error</guid>
      <pubDate>Mon, 23 Sep 2024 15:52:13 GMT</pubDate>
    </item>
    <item>
      <title>受不同聚类大小约束的 KMeans</title>
      <link>https://stackoverflow.com/questions/79015120/kmeans-constrained-with-different-cluster-size</link>
      <description><![CDATA[我有一个包含商店坐标的数据框，我想根据供应商应该访问该商店的日期将它们划分为簇。例如，假设供应商应该访问 180 家商店。他应该在周一到周五访问 30-34 家商店，周六，他应该访问其他日子的 60%。
我该怎么做？使用 kmeans-constrained，我只能将它们划分为大小相等的簇。也许我需要使用某种解算器或在集群之间移动点以达到我想要的数字，但我不知道如何做到这一点。
以下是将它们均等划分的代码：
# 循环遍历供应商集群
for vendor in df[&quot;vendor&quot;].unique():

# 每个供应商的商店数量
n_shops = df.loc[df[&quot;vendor&quot;] == vendor][&quot;cod_shop&quot;].count()

# 索引
idx = df.loc[df[&quot;vendor&quot;] == vendor].index

# 一周中各天的集群数量
num_clusters = 5 # 星期一至星期五

# 集群的平均大小
avg_size = n_shops / (num_clusters + 0.6)

# 定义限制
min_shops = round(avg_size - n_shops * pct, 0)
max_shops = math.ceil(avg_size + n_shops * pct)

# 模型
kmeans = KMeansConstrained(n_clusters=num_clusters, size_min=min_shops, size_max=max_shops, random_state=42)
labels = kmeans.fit_predict(df.loc[df[&quot;vendor&quot;] == vendor][[&quot;latitude&quot;, &quot;longitude&quot;]])

# 向数据框添加标签
df.loc[idx, &quot;visit_day&quot;] = labels
]]></description>
      <guid>https://stackoverflow.com/questions/79015120/kmeans-constrained-with-different-cluster-size</guid>
      <pubDate>Mon, 23 Sep 2024 14:42:09 GMT</pubDate>
    </item>
    <item>
      <title>构建 OCR 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/79013996/building-ocr-model</link>
      <description><![CDATA[我正在研究一个机器学习模型，其中输入是图像和实体名称，目标是从图像中提取相应的实体值。例如，如果实体名称是“高度”，并且图像包含门的高度，则模型应提取此值（例如 6 英尺）以及正确的单位。
输入：
图像：包含对象（例如门）和相关信息（例如高度或其他相关测量值）。
实体名称：关键字，例如“高度”或“重量”，指定要从图像中提取的值。
输出：
与图像中的实体相对应的值及其单位（例如“6 英尺”）。
挑战：
实体值可以出现在图像的不同部分，具有不同的文本格式、字体或样式。
需要识别和提取测量单位（例如，米、英尺）以及值。
问题：

处理此类任务的最佳方法或模型架构是什么？
是否有任何特定技术或预训练模型可以帮助将图像和实体名称组合为输入，以从图像中提取相应的值？
我应该如何预处理图像和标签以训练此类任务的模型？
是否有任何关于框架和工具的指导、参考或建议？

我尝试使用带有 CTC 损失的 CNN+RNN，但我的损失接近 20 并且没有进一步减少
这里我附上了我的 google cloab 链接
笔记本链接]]></description>
      <guid>https://stackoverflow.com/questions/79013996/building-ocr-model</guid>
      <pubDate>Mon, 23 Sep 2024 09:27:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 Deepface Deepface.represent 从 ROI 获取嵌入时出错</title>
      <link>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</link>
      <description><![CDATA[我在使用 Deepface 从 Retinaface 识别的裁剪 ROI 获取嵌入时遇到了问题。
我正尝试使用一些名人的数据集（图像）学习对象识别，并可能考虑将其用于我的个人照片库。我尝试使用 Haar Cascade 进行人脸检测，并使用 Open Cv 中的 LBPHFaceRecognize 进行人脸识别，效果很好。然后我想尝试使用 Retinafce 进行人脸检测并获得 ROI。ROI 存储在列表中，并使用 Deepface 从选定的 ROI 获取嵌入并存储在另一个列表中。我正在尝试将嵌入存储到列表中，但我一直得到
 raise ValueError(
ValueError: 无法在 numpy 数组中检测到人脸。请确认图片

是人脸照片或考虑将 force_detection 参数设置为 False。
虽然所有图像都有一张被清楚检测到的人脸。这是我的代码供参考：
import os
import cv2 as cv
from retinaface import RetinaFace
from deepface import DeepFace
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

artist = [&#39;50cent&#39;] # type: ignore #MJ the GOAT!! , &#39;Kanye&#39;, &#39;Eminem&#39;, &#39;MichaelJackson&#39;
ROOT_DIR = &#39;asset/Face_Recon_Dataset&#39; #图像数据集的路径
faces_roi =[]
labels = []
embeddings = []
#现在在脸部坐标上画一个矩形
#脸部范围有：
# x1, y1) = (28, 51) #左上角
# (x2, y2) = (61, 98) #右下角
&quot;&quot;&quot; 这定义了检测到的脸部周围的矩形边界框。
- x1 (28)：脸部的左边缘
- y1 (51)：脸部的上边缘
- x2 (61)：脸部的右边缘
- y2 (98)：脸部的下边缘&quot;&quot;&quot;

def get_roi():
for artist_name in artist:
# 获取艺术家姓名的索引
label = artist.index(artist_name)
image_folder = os.path.join(ROOT_DIR,artist_name) # 获取包含图像的实际文件夹
for artist_images in os.listdir(image_folder): # 列出该目录中的所有图像
image = os.path.join(image_folder,artist_images)
resp = RetinaFace.detect_faces(image)
# 确保人脸存在
if isinstance(resp,dict):
img = cv.imread(image)
for face_id, face_data in resp.items():
# print(face_id)
# print(&quot;x1: &quot;, face_data[&#39;facial_area&#39;][0])
# print(&quot;y1: &quot;, face_data[&#39;facial_area&#39;][1])
# print(&quot;x2: &quot;, face_data[&#39;facial_area&#39;][2])
# print(&quot;y2: &quot;, face_data[&#39;facial_area&#39;][3], &quot;\n&quot;)
# 读取图像

# 检测人脸
x1 = face_data[&#39;facial_area&#39;][0]
y1 = face_data[&#39;facial_area&#39;][1]
x2 = face_data[&#39;facial_area&#39;][2]
y2 = face_data[&#39;facial_area&#39;][3]

# 为人脸绘制边界框 
# faces_rect = cv.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
face_roi = img[y1:y2,x1:x2]

#用其名称标记裁剪后的 roi 人脸
faces_roi.append(face_roi)

labels.append(label)
print(len(faces_roi))
print(len(labels))
print(&quot;已标记和索引的图像&quot;)
print(&quot;正在初始化嵌入过程.....&quot;)
get_embeddings()

def get_embeddings():
&quot;&quot;&quot; 使用 deepface 从每个面部 roi 中提取嵌入&quot;&quot;&quot;
print(&quot;Satarting embedding: 🚀🚀 &quot;)
for roi in faces_roi:
face_roi_resized = cv.resize(roi, (160, 160)) # 将人脸 ROI 调整为 160x160 像素
embedding = DeepFace.represent(face_roi_resized, model_name=&quot;Facenet&quot;)
print(embedding)
embeddings.append(embedding)
print(&quot;Vectors storage in list..&quot;)

get_roi()

# 是时候使用 svm 分类器测试和训练这个坏家伙了
# 将嵌入和索引标记为 numpy 数组
X = np.array(embeddings) #feature
y = np.array(labels) #label

# 将数据分成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 SVM 分类器
svm_model = SVC(kernel=&#39;linear&#39;) # 线性核是嵌入的良好默认值
svm_model.fit(X_train, y_train)

# 评估模型
y_pred = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;SVM 模型准确率：{accuracy * 100:.2f}%&quot;)

为什么即使 ROI 已被裁剪，该错误仍然如此持续，解决此错误的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</guid>
      <pubDate>Mon, 23 Sep 2024 08:03:12 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用回归算法而不是分类算法？[关闭]</title>
      <link>https://stackoverflow.com/questions/79013513/why-are-regression-algorithms-used-instead-of-classification-algorithms</link>
      <description><![CDATA[众所周知，在 ML 中，如果依赖特征本质上是连续的，则应用回归模型。但是，如果依赖特征本质上是分类的，则使用分类算法。
正如您在这张图（https://i.sstatic.net/9Q3wfudK.png）中看到的那样，最大值为。大量数据点重复出现，表明它们正在形成类别。
那么，为什么这里使用回归？
这是数据集：（https://drive.google.com/file/d/1vTIiQ0NZKgBI-EfpGzfPKHx1VaAdEYdH/view?usp=sharing）
我和同学、老师讨论了这个问题。他们都说回归是用来预测的，但没人能解释他们是如何得出应该用回归来代替分类的结论的。]]></description>
      <guid>https://stackoverflow.com/questions/79013513/why-are-regression-algorithms-used-instead-of-classification-algorithms</guid>
      <pubDate>Mon, 23 Sep 2024 07:10:30 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何 Python 库可以使用相机检测被检测物体的纬度和经度？[关闭]</title>
      <link>https://stackoverflow.com/questions/62105606/is-there-any-python-libraries-to-detect-latitude-and-longitude-of-detected-objec</link>
      <description><![CDATA[我想使用带有经度和纬度的卫星摄像机来检测物体。]]></description>
      <guid>https://stackoverflow.com/questions/62105606/is-there-any-python-libraries-to-detect-latitude-and-longitude-of-detected-objec</guid>
      <pubDate>Sat, 30 May 2020 16:40:13 GMT</pubDate>
    </item>
    <item>
      <title>什么是 Killed:9 以及如何在 macOS 终端中修复？</title>
      <link>https://stackoverflow.com/questions/51833310/what-is-killed9-and-how-to-fix-in-macos-terminal</link>
      <description><![CDATA[我有一段用于机器学习项目的简单 Python 代码。我有一个相对较大的自发语音数据库。我开始训练我的语音模型。由于这是一个庞大的数据库，我让它连夜工作。早上我醒来时看到终端中出现一个神秘的
Killed: 9
行。没有其他内容。没有其他错误消息或需要处理的内容。代码运行良好约 6 小时，占整个过程的 75%，所以我真的不明白哪里出了问题。
什么是 Killed:9 以及如何修复它？浪费数小时的计算时间非常令人沮丧……
如果这很重要，我正在使用 macOS Mojave 测试版。提前谢谢您！]]></description>
      <guid>https://stackoverflow.com/questions/51833310/what-is-killed9-and-how-to-fix-in-macos-terminal</guid>
      <pubDate>Tue, 14 Aug 2018 03:28:58 GMT</pubDate>
    </item>
    <item>
      <title>Keras 中“Flatten”起什么作用？</title>
      <link>https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras</link>
      <description><![CDATA[我正在尝试了解 Keras 中 Flatten 函数的作用。下面是我的代码，这是一个简单的两层网络。它接收形状为 (3, 2) 的二维数据，并输出形状为 (1, 4) 的一维数据：
model = Sequential()
model.add(Dense(16, input_shape=(3, 2)))
model.add(Activation(&#39;relu&#39;))
model.add(Flatten())
model.add(Dense(4))
model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;SGD&#39;)

x = np.array([[[1, 2], [3, 4], [5, 6]]])

y = model.predict(x)

print y.shape

这会打印出 y 具有形状 (1, 4)。但是，如果我删除 Flatten 行，则它会打印出 y 具有形状 (1, 3, 4)。
我不明白这一点。根据我对神经网络的理解，model.add(Dense(16, input_shape=(3, 2))) 函数正在创建一个隐藏的完全连接层，其中包含 16 个节点。这些节点中的每一个都连接到每个 3x2 输入元素。因此，第一层输出处的 16 个节点已经是“平坦的”。因此，第一层的输出形状应该是 (1, 16)。然后，第二层将其作为输入，并输出形状为 (1, 4) 的数据。
那么，如果第一层的输出已经是“平坦的”并且形状为 (1, 16)，为什么我需要进一步将其平坦化？]]></description>
      <guid>https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras</guid>
      <pubDate>Wed, 05 Apr 2017 16:48:24 GMT</pubDate>
    </item>
    </channel>
</rss>