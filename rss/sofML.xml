<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 19 Jan 2024 18:18:05 GMT</lastBuildDate>
    <item>
      <title>如何提高逻辑回归和分类的模型精度？</title>
      <link>https://stackoverflow.com/questions/77847401/how-to-increase-the-model-accuracy-of-logistic-regression-and-classification</link>
      <description><![CDATA[我正在尝试根据 yfinance 的收盘数据编写股票预测脚本。我用sklearn做了一个简单的预测模型，但是一点也不准确。我怎样才能更好地训练这个模型，使其随着时间的推移变得更加准确/基于我所进行的训练。我是机器学习的初学者，所以请放轻松（笑）。我将在此处添加 python 代码。
导入 pandas 作为 pd
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

从 sklearn.model_selection 导入 train_test_split

从 sklearn.neighbors 导入 KNeighborsClassifier
从sklearn导入邻居
从 sklearn.model_selection 导入 GridSearchCV
从 sklearn.metrics 导入 precision_score

从 sklearn.neighbors 导入 KNeighborsRegressor
从sklearn导入邻居


将 yfinance 导入为 yf

从 pandas_datareader 将数据导入为 pdr
amzn = yf.Ticker(“AMZN”)
hist = amzn.history(period=&quot;1mo&quot;)
yf.pdr_override()
data_test = pdr.get_data_yahoo(“AMZN”，开始=“2017-01-01”，结束=“2017-04-30”)
打印（数据测试）


data = pdr.get_data_yahoo(“AMZN”, start=“2023-12-01”, end=“2024-01-15”)
#data = yf.download(“AAL”)

plt.figure(figsize=(16,8))
plt.plot(data[&#39;收盘价&#39;], label=&#39;收盘价&#39;)

#分类问题是买(+1)还是卖(-1)
数据[&#39;打开 - 关闭&#39;] = 数据[&#39;打开&#39;] - 数据[&#39;关闭&#39;]
数据[&#39;高 - 低&#39;] = 数据[&#39;高&#39;] - 数据[&#39;低&#39;]
数据 = data.dropna()

#输入特征来预测是买入还是卖出

X = 数据[[&#39;开盘价 - 收盘价&#39;, &#39;最高价 - 最低价&#39;]]
X.head()

Y = np.where(data[&#39;Close&#39;].shift(-1)&gt;data[&#39;Close&#39;],1,-1)

X_train、X_test、y_train、y_test = train_test_split(X、Y、test_size=.25、random_state = 44)

#使用gridsearch找到最佳参数
params = {&#39;n_neighbors&#39;:[2,3,4,5,6,7,8,9,10,11,12,13,14,15]}
knn = 邻居.KNeighborsClassifier()
模型 = GridSearchCV(knn, 参数, cv = 5)

#拟合模型
model.fit(X_train, y_train)

#准确率分数
precision_train = precision_score(y_train, model.predict(X_train))
准确度测试 = 准确度分数(y_test, model.predict(X_test))

print(&#39;Train_data 准确度：%.2f&#39; %accuracy_train)
print(&#39;测试数据准确度：%.2f&#39; %accuracy_test)

Predictions_classification = model.predict(X_test)
实际预测数据 = pd.DataFrame({&#39;实际类别&#39;:y_test, &#39;预测类别&#39;:predictions_classification})

#print(实际预测数据.head(10))


y=数据[&#39;关闭&#39;]

X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=.25, random_state= 44)



#使用网格搜索找到最佳参数
params = {&#39;n_neighbors&#39;:[2,3,4,5,6,7,8,9,10,12,13,14,15]}
knn_reg = Neighbors.KNeighborsRegressor()
model_reg = GridSearchCV(knn_reg, 参数, cv=5)

#拟合模型并做出预测
model_reg.fit(X_train_reg,y_train_reg)
预测 = model_reg.predict(X_test_reg)
#打印（预测）

rms=np.sqrt(np.mean(np.power((np.array(y_test)-np.array(预测)),2)))

valid = pd.DataFrame({&#39;实际收盘价&#39;:y_test_reg, &#39;预测收盘价&#39;:predictions})
打印（有效）

我尝试使用网格排序，并下载了最近一个月左右的股票收盘数据。然后我将其分类为 1（买入）或 -1（卖出），当我打印出准确度分数时，它只有微不足道的 0.51，请帮助我让它变得更复杂。]]></description>
      <guid>https://stackoverflow.com/questions/77847401/how-to-increase-the-model-accuracy-of-logistic-regression-and-classification</guid>
      <pubDate>Fri, 19 Jan 2024 16:07:24 GMT</pubDate>
    </item>
    <item>
      <title>如果新的交叉验证迭代出现，是否需要重新创建神经网络层？</title>
      <link>https://stackoverflow.com/questions/77847336/whether-should-be-the-neuron-network-layers-recreated-if-the-new-iteration-of-cr</link>
      <description><![CDATA[我有一个关于交叉验证的问题。
k=5，
将有 4/5 训练数据集和 1/5 验证数据集
 data = np.concatenate([self.training_data1, self.training_data2], axis=1)
        
        kf = KFold(n_splits=k, shuffle=True) # k 折叠交叉验证
        kf.get_n_splits(data) # 返回交叉验证器中的分割迭代次数
     
        损失CV = 0
        val_loss_cv = 0
    
        对于 kf.split(data) 中的 train_index、val_index：
            
            logging.info(f“train_index:{train_index.shape}”)
            logging.info(f&quot;val_index:{val_index.shape}&quot;)

            self.is_train = tf.Variable(initial_value=True, trainable=False, dtype=tf.bool, name=“is_train”)
   
            loss_cv, val_loss_cv = self.train(train_index, val_index)
            loss_cv += loss_cv
            val_loss_cv += val_loss_cv

loss_cv = loss_cv / k
val_loss_cv = val_loss_cv / k

每次迭代都有新的train_index、val_index。（例如在train_index0、val_index0之后，下一次迭代将从train_index1、val_index1开始）
这些数据集将被加载到函数 self.train(train_index, val_index) 中。
在 train() 函数中，有一个使用神经元网络层创建的自动编码器层。
当新的交叉验证索引（train_index1，val_index1）在新的迭代中出现时，是否应该使用新的初始权重和偏差重新创建新的神经元网络层？
如果我在神经元网络层中使用继承的权重和偏差，而不是创建新的神经元网络层，结果是否会导致过拟合？]]></description>
      <guid>https://stackoverflow.com/questions/77847336/whether-should-be-the-neuron-network-layers-recreated-if-the-new-iteration-of-cr</guid>
      <pubDate>Fri, 19 Jan 2024 15:56:07 GMT</pubDate>
    </item>
    <item>
      <title>使用 spaCy 模型对文本中的实体进行分类但不定位实体</title>
      <link>https://stackoverflow.com/questions/77847222/classifying-but-not-locating-entities-within-a-text-with-spacy-model</link>
      <description><![CDATA[有没有办法“强制”经过训练的 spaCy 模型对文本中的给定字符串进行分类，而不是定位？
就我而言，我想构建一个 NER 模型，将文本中的地名分类为特定类别，例如“城市”或“国家”。所以模型不需要定位地名。对于每个文本文件，它需要处理其中的地名列表、字典等作为参数，然后根据周围的文本为该地名分配最可能的类。
此外，是否有一个选项可以让 spaCy“忽略”地名的各个字符，而仅通过查看周围的文本来训练模型？
如果我尝试在不指定标签的情况下设置实体，则 print(doc.ents) 返回一个空集。
doc = nlp(“德国距离日本比莫斯科更远”)

doc.set_ents([跨度(doc, 0, 1), 跨度(doc, 4, 5), 跨度(doc, 6, 7)])

打印（文档）
对于 doc.ents 中的 ent：
    打印（ent.text，ent.label_）
]]></description>
      <guid>https://stackoverflow.com/questions/77847222/classifying-but-not-locating-entities-within-a-text-with-spacy-model</guid>
      <pubDate>Fri, 19 Jan 2024 15:36:34 GMT</pubDate>
    </item>
    <item>
      <title>是否有一个值可以表达多种因素对结果的影响？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77846958/is-there-an-value-that-expresses-the-effect-of-multiple-factors-to-the-outcome</link>
      <description><![CDATA[我有多个变量，例如 X_1、X_2、...、X_n，它们不一定是独立的，但预计会影响 Y。
是否有一个值可以指示 X_1、X_2、...、X_n 对 Y 总共有多大影响？
例如，如果还有其他变量 Z_1、Z_2、..._、Z_m 也影响 Y，但与 X_1 ~X_n 无关，我们进行线性回归并得到 Y=w_1X_1 +w_2X_2+ ...+w_nX_n+ w_n +1 Z_1 +...+w_n+m Z_m ，“w_1X_1 +...+w_nX_n”可能是一个指标。]]></description>
      <guid>https://stackoverflow.com/questions/77846958/is-there-an-value-that-expresses-the-effect-of-multiple-factors-to-the-outcome</guid>
      <pubDate>Fri, 19 Jan 2024 14:53:49 GMT</pubDate>
    </item>
    <item>
      <title>如何知道 VGGish 正确运行并查询音频分类的嵌入</title>
      <link>https://stackoverflow.com/questions/77846542/how-to-know-vggish-runs-correctly-and-queries-about-embeddings-for-audio-classif</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77846542/how-to-know-vggish-runs-correctly-and-queries-about-embeddings-for-audio-classif</guid>
      <pubDate>Fri, 19 Jan 2024 13:45:06 GMT</pubDate>
    </item>
    <item>
      <title>从头开始的 DQN 给出错误形状的输出</title>
      <link>https://stackoverflow.com/questions/77846372/dqn-from-scratch-giving-wrong-shaped-output</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77846372/dqn-from-scratch-giving-wrong-shaped-output</guid>
      <pubDate>Fri, 19 Jan 2024 13:21:14 GMT</pubDate>
    </item>
    <item>
      <title>Ultralytics 训练回调中的 gRPC 服务器流</title>
      <link>https://stackoverflow.com/questions/77846210/grpc-server-streaming-in-ultralytics-training-callback</link>
      <description><![CDATA[我正在开发一个使用 ultralytics 包、YOLOv8 模型和 gRPC 的 ML 训练服务器和客户端。服务器是用Python编写的，客户端是用C#编写的。我一切顺利，可以从客户端毫无问题地开始培训过程。
现在我想向客户报告培训过程，以直观地向用户更新培训的进展情况。
为此，我创建了这个小原型文件：
syntax = “proto3”;

包training_client；

服务 培训服务 {
  rpc StartTraining(StartTrainingRequest) 返回 (stream InTrainingResponse) {}
}

消息开始训练请求{}

消息指标
{
  字符串名称=1；
  浮点值 = 2；
}

消息训练响应
{
  int32 纪元 = 1；
  重复的 Metric 指标 = 2；
}

我的基本服务器实现如下所示：
来自多处理导入进程、队列

导入gpc
从 ultralytics.engine.trainer 导入 BaseTrainer

导入training_pb2_grpc
从 ultralytics 导入 YOLO
从并发.futures 导入 ThreadPoolExecutor

从 Training_pb2 导入 InTrainingResponse，指标

def on_train_epoch_end(训练器: BaseTrainer):
    print(&quot;将值放入队列&quot;)
    TrainingServicer.progress_queue.put((trainer.epoch, trainer.metrics))

def on_train_end(训练器: BaseTrainer):
    print(&quot;训练结束&quot;)
    TrainingServicer.progress_queue.put(None) # 训练完成，打破无限循环

类 TrainingServicer(training_pb2_grpc.TrainingServiceServicer):
    进度队列 = 队列()

    def __init__(自身):
        超级().__init__()
        self.model = YOLO(“yolov8m.pt”)
        self.model.add_callback(“on_train_epoch_end”, on_train_epoch_end)
        self.model.add_callback(“on_train_end”, on_train_end)

    def run_training（自我）：
        self.model.train（数据=“数据集/data.yaml”，纪元= 15，imgsz = 512，批次= 2，设备= 0）

    def StartTraining（自身，请求，上下文）：
        Training_thread = Process(目标=self.run_training)
        训练线程.start()

        而真实：
            尝试：
                项目 = TrainingServicer.progress_queue.get(timeout=1)
                如果项目为无：
                    休息
                纪元，指标 = 项目
                resp = InTrainingResponse(epoch=epoch)
                对于metrics.items()中的k、v：
                    resp.metrics.append(指标(名称=k,值=v))
                print(&quot;产量训练更新&quot;)
                产量响应
            除了异常 e：
                print(&quot;队列为空或没有新数据可用&quot;)
                继续


定义服务（）：
    服务器 = grpc.server(ThreadPoolExecutor(max_workers=10))
    Training_pb2_grpc.add_TrainingServiceServicer_to_server（TrainingServicer（），服务器）
    server.add_secure_port(&#39;[::]:30008&#39;, grpc.local_server_credentials())
    print(&quot;启动服务器&quot;)
    服务器.start()
    server.wait_for_termination()


如果 __name__ == &#39;__main__&#39;:
    服务（）

我的基本客户是这样的：
使用 Grpc.Net.Client;
使用 Grpc.Core；
使用 TrainingClient；

使用 var Channel = GrpcChannel.ForAddress(“http://localhost:30008”);
var client = new TrainingService.TrainingServiceClient(通道);

使用 var call = client.StartTraining(new StartTrainingRequest());
等待foreach（call.ResponseStream.ReadAllAsync（）中的var纪元）
{
     Console.WriteLine($&quot;已收到 Epoch {epoch.Epoch} 和 {epoch.Metrics}&quot;);
}

我现在的问题是回调已成功将项目放入队列中，但队列的使用者部分从未收到它们，这表明对我来说两个队列是不同的实例，我通过检查它们的内存地址很快确认了这一点。
对 model.train() 的调用是阻塞的，这就是为什么我尝试在不同的进程中运行它，以便能够将每个结果返回给 RPC。
我对multiprocessing.Queue的理解是，这个实现使用共享内存空间来允许不同的进程共享数据，但我似乎无法正确使用它。
上面解释了：
尝试使用多处理来使用生产者/消费者类型的模式来向客户端报告训练中期的训练指标。]]></description>
      <guid>https://stackoverflow.com/questions/77846210/grpc-server-streaming-in-ultralytics-training-callback</guid>
      <pubDate>Fri, 19 Jan 2024 12:52:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 keras 预测任何数字（没有类/标签）[关闭]</title>
      <link>https://stackoverflow.com/questions/77846035/using-keras-to-predict-any-number-without-classes-labels</link>
      <description><![CDATA[我是 keras 新手，我正在尝试制作一个可以预测人年龄的模型。
有没有办法让 .predict() 预测任何数字，而不是必须给它一个范围或桶（类）来从中选择（预测概率）？
我尝试了（许多变体）提供类/标签，并且预测概率与我的范围（如 0 岁到 100 岁）成线性比例。我也尝试了一整个星期来搜索类似的问题。]]></description>
      <guid>https://stackoverflow.com/questions/77846035/using-keras-to-predict-any-number-without-classes-labels</guid>
      <pubDate>Fri, 19 Jan 2024 12:21:08 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：给定 groups=1，权重大小为 [128, 64, 4, 4]，预期输入 [1, 128, 65, 65] 有 64 个通道，但得到了 128 个通道</title>
      <link>https://stackoverflow.com/questions/77843263/runtimeerror-given-groups-1-weight-of-size-128-64-4-4-expected-input1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77843263/runtimeerror-given-groups-1-weight-of-size-128-64-4-4-expected-input1</guid>
      <pubDate>Fri, 19 Jan 2024 01:31:36 GMT</pubDate>
    </item>
    <item>
      <title>Python dgl 库 API 更新</title>
      <link>https://stackoverflow.com/questions/77837193/python-dgl-library-api-updates</link>
      <description><![CDATA[这是我的代码：
def 标准化（自我，logits）：
    self.\_logits_name = “\_logits”
    self.\_normalizer_name = “\_norm”
    self.g.edata\[self.\_logits_name\] = logits

    self.g.update_all(fn.copy_u(self._logits_name, self._logits_name),
                     fn.sum(self._logits_name, self._normalizer_name))
    返回 self.g.edata.pop(self._logits_name), self.g.ndata.pop(self._normalizer_name)

def edge_softmax(自身):

    如果 self.l0 == 0:
        分数 = self.softmax(self.g, self.g.edata.pop(&#39;a&#39;))
    别的：
        分数，归一化器 = self.normalize(self.g.edata.pop(&#39;a&#39;))
        self.g.ndata[&#39;z&#39;] = 标准化器[:,0,:].unsqueeze(1)

    self.g.edata[&#39;a&#39;] = 分数[:,0,:].unsqueeze(1)

这是堆栈跟踪：
回溯（最近一次调用最后一次）：
文件“/datasets/\_deepnote_work/train.py”，第 211 行，位于 \ 中
主要（参数）

文件“/datasets/\_deepnote_work/train.py”，第 130 行，在 main 中
logits = 模型（特征）

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1518 行，位于 \_wrapped_call_impl
返回 self.\_call_impl(\*args, \*\*kwargs)

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1527 行，位于 \_call_impl
返回forward_call(\*args, \*\*kwargs)

文件“/datasets/\_deepnote_work/gat.py”，第 209 行，向前
h，边缘 = self.gat_layers\[0\](h，边缘)

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1518 行，位于 \_wrapped_call_impl
返回 self.\_call_impl(\*args, \*\*kwargs)

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1527 行，位于 \_call_impl
返回forward_call(\*args, \*\*kwargs)

文件“/datasets/\_deepnote_work/gat.py”，第 105 行，向前
self.edge_softmax()

文件“/datasets/\_deepnote_work/gat.py”，第 166 行，edge_softmax
分数，归一化器 = self.normalize(self.g.edata.pop(&#39;a&#39;))

文件“/datasets/\_deepnote_work/gat.py”，第 157 行，标准化
self.g.update_all(fn.copy_u(self.\_logits_name, self.\_logits_name),

文件“/root/venv/lib/python3.9/site-packages/dgl/heterograph.py”，第 5110 行，位于 update_all
ndata = core.message_passing()
文件“/root/venv/lib/python3.9/site-packages/dgl/core.py”，第 398 行，message_passing
ndata = invoke_gspmm(g, mfunc, rfunc)

文件“/root/venv/lib/python3.9/site-packages/dgl/core.py”，第 361 行，invoke_gspmm
x = alldata\[mfunc.target\]\[mfunc.in_field\]

文件“/root/venv/lib/python3.9/site-packages/dgl/view.py”，第 80 行，在 _getitem_ 中
返回 self.\_graph.\_get_n_repr(self.\_ntid, self.\_nodes)\[key\]

文件“/root/venv/lib/python3.9/site-packages/dgl/frame.py”，第 688 行，在 _getitem_ 中
返回 self.\_columns\[name\].data
关键错误：&#39;\_logits&#39;

我查看了DGLEdgeBatch的文档，但没有找到任何解决方案
链接到文档：https://docs.dgl.ai/en/1.1.x/api/python/udf.html#edge-wise-user-defined-function
阅读 DGL 的文档并尝试了一些替代函数。但他们没有工作。
如何修复/更新代码？
编辑：
抱歉没说清楚。但上面的代码是旧的。我尝试运行它并收到很多运行时错误。我相信自那以后 API 发生了很多变化。您能否建议我进行一些更改以使代码正常工作？]]></description>
      <guid>https://stackoverflow.com/questions/77837193/python-dgl-library-api-updates</guid>
      <pubDate>Thu, 18 Jan 2024 06:02:39 GMT</pubDate>
    </item>
    <item>
      <title>相关矩阵的累积 AOC 计算 [关闭]</title>
      <link>https://stackoverflow.com/questions/77830147/cumulative-aoc-calculation-for-a-correlation-matrix</link>
      <description><![CDATA[我正在使用一个非常简单的数据集（胎儿健康分类）进行练习，使用支持向量机、相关指标和典型模型指标（没什么特别的）进行一些练习。我想做以下事情：

采用（与目标）最相关的变量并计算 SVM 模型；然后保留 AUC 结果。
采用第二个最相关的变量（与目标）并使用第一个和第二个变量，计算 SVM 模型；然后保留 AUC 结果。
依此类推......直到到达最后一个变量

之后，我需要创建一个显示以下信息的图表：

X轴：累计变量数
Y 轴：模型中包含的每个变量数量对应的 AUC

我有以下代码；我认为这是合理的。然而，它被卡住了。我不得不中断迭代，因为它们似乎没有结束。关于如何修复循环有什么建议吗？
**导入参考文件的一些行**

df = pd.read_csv(&quot;ASI_casoPractico.csv&quot;, sep = &quot;;&quot;)

# 导入库

将 pandas 导入为 pd
从 sklearn.svm 导入 SVC
从 sklearn.metrics 导入 roc_auc_score
从 sklearn.model_selection 导入 train_test_split
将 matplotlib.pyplot 导入为 plt

# 相关矩阵

corr_matrix = df.corr().abs()
排序校正 =
corr_matrix[&#39;目标&#39;].sort_values(升序=False)

# 创建按相关性排序的变量列表

Sorted_vars = Sorted_corr.index.tolist()

# 为结果创建空列表

结果=[]

# 使用 SVM 进行变量迭代和模型生成

对于范围内的 i(1, len(sorted_vars) + 1)：

  # 选择相关性最好的变量
  选定的变量 = 排序的变量[:i]

  # 训练和测试的数据分开

  X_train = df[selected_vars]
  y_train = df[&#39;目标&#39;]

  # 训练支持向量机

  svm = SVC(内核=&#39;线性&#39;, 概率=True)
  svm.fit(X_train, y_train)

  # 计算AUC
  y_pred = svm.predict_proba(X_train)[:, 1]
  auc = roc_auc_score(y_train, y_pred)

  # 将值添加到列表中
  结果.append([i, auc])

# 为结果创建数据框
results_df = pd.DataFrame(结果, columns=[&#39;变量&#39;, &#39;AUC&#39;])

# 图
results_df.plot(x=&#39;变量&#39;, y=&#39;AUC&#39;, kind=&#39;线&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/77830147/cumulative-aoc-calculation-for-a-correlation-matrix</guid>
      <pubDate>Wed, 17 Jan 2024 05:28:50 GMT</pubDate>
    </item>
    <item>
      <title>在小型训练数据集上训练的文本转语音模型</title>
      <link>https://stackoverflow.com/questions/77406851/text-to-speech-model-that-trains-on-small-training-dataset</link>
      <description><![CDATA[我需要一个模型，可以使用包含转录本和最多 20 个句子的 wav 文件的数据集进行训练。
我尝试在这样的情况下训练 https://github.com/coqui-ai/TTS数据集，它根本没有训练得很好。这个推论只是噪音而不是文字。
我正在研究 https ://github.com/microsoft/SpeechT5/tree/main/SpeechLM#pre-trained-and-fine-tuned-models 但他们使用的微调数据集似乎也有超过 100 小时的音频内容。
解决这个问题的最佳研究模型是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77406851/text-to-speech-model-that-trains-on-small-training-dataset</guid>
      <pubDate>Thu, 02 Nov 2023 04:00:19 GMT</pubDate>
    </item>
    <item>
      <title>面对强化学习的问题</title>
      <link>https://stackoverflow.com/questions/76695094/facing-a-problem-with-reinforcement-learning</link>
      <description><![CDATA[导入健身房
从 stable_baselines3 导入 A2C

env =gym.make(&#39;LunarLander-v2&#39;, render_mode=&quot;人类&quot;)
env.reset()

模型 = A2C(“MlpPolicy”, env, verbose=1)
model.learn(total_timesteps=1000)

集数 = 10

对于范围内的 ep（剧集）：
    obs = env.reset()
    完成=假
    虽然没有完成：
        动作、_状态、_情节、_determ = model.predict(obs)
        obs、奖励、完成、info = env.step(action)
        env.render()

env.close()

我上面的代码产生以下输出：
DeprecationWarning：“np.bool8”是“np.bool_”的已弃用别名。 （已弃用 NumPy 1.24）
  如果不是 isinstance(终止, (bool, np.bool8)):
------------------------------------------------
|推出/ | |
| ep_len_mean | 89.2 | 89.2
| ep_rew_mean | -227 | -227
|时间/| |
|帧率 | 43 | 43
|迭代| 100 | 100
|已用时间 | 11 | 11
|总时间步数 | 500 | 500
|火车/ | |
|熵损失 | -1.29 | -1.29
|解释方差 | -0.0216 | -0.0216
|学习率 | 0.0007 | 0.0007
| n_更新 | 99 | 99
|政策损失 | 2.79 | 2.79
|价值损失 | 12.3 | 12.3
------------------------------------------------
------------------------------------------------
|推出/ | |
| ep_len_mean | 107 | 107
| ep_rew_mean | -209 | -209
|时间/ | |
|帧率 | 45 | 45
|迭代| 200 | 200
|已用时间 | 21 | 21
|总时间步数 | 1000 | 1000
|火车/ | |
|熵损失 | -0.864 |
|解释方差 | -0.00161 | -0.00161
|学习率 | 0.0007 | 0.0007
| n_更新 | 199 | 199
|政策损失 | -16.6 | -16.6
|价值损失 | 228 | 228

随后出现此错误：
&lt;前&gt;&lt;代码&gt;------------------------------------
回溯（最近一次调用最后一次）：
  文件“c:\Appu\Courses\Fun items\Reinforcement Learning\c1.py”，第 17 行，位于  中。
    动作、_states、_episode、_determ = model.predict(obs)
                                         ^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\sarav\AppData\Local\Programs\Python\Python311\Lib\site-packages\stable_baselines3\common\base_class.py”，第 555 行，在预测中
    返回 self.policy.predict（观察、状态、episode_start、确定性）
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\sarav\AppData\Local\Programs\Python\Python311\Lib\site-packages\stable_baselines3\common\policies.py”，第 346 行，在预测中
    观察，vectorized_env = self.obs_to_tensor(观察)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\sarav\AppData\Local\Programs\Python\Python311\Lib\site-packages\stable_baselines3\common\policies.py”，第 260 行，位于 obs_to_tensor 中
    观察 = np.array(观察)
                  ^^^^^^^^^^^^^^^^^^^^^^^
ValueError：使用序列设置数组元素。请求的数组在 1 维之后具有不均匀的形状。检测到的形状为(2,)+不均匀部分。

当我运行代码时，它会运行几个时间步，然后退出并出现上述错误。有什么解决办法吗？]]></description>
      <guid>https://stackoverflow.com/questions/76695094/facing-a-problem-with-reinforcement-learning</guid>
      <pubDate>Sat, 15 Jul 2023 17:51:40 GMT</pubDate>
    </item>
    <item>
      <title>简单线性 Sigmoid 神经网络不学习</title>
      <link>https://stackoverflow.com/questions/64269084/simple-linear-sigmoid-neural-network-not-learning</link>
      <description><![CDATA[我正在学习 pytorch 并尝试将网络训练为异或门。一切都进行得很顺利，但它就是不学习。它确实改变了它的权重，但它会收敛到每个输入的结果，这远远超出了预期结果。
我尝试过许多学习率和权重初始化。
因此输入是 A 门和 B 门，如果两者相等则应返回 1，否则应返回 0，如下所示：
&lt;前&gt;

    [0,0] =&gt; 1
    [0,1] =&gt; 0
    [1,0] =&gt; 0
    [1,1] =&gt; 1


这是我对模型进行建模和训练的尝试：
&lt;前&gt;

    导入火炬作为火炬
    将 torch.nn 导入为 nn
    
    网络类（nn.Module）：
        
        def __init__(自身):
            超级（网络，自我）.__init__()
            self.x1 = nn.Linear(2,4)
            self.s1 = nn.Sigmoid()
            self.x2 = nn.Linear(4,1)
            self.s2 = nn.Sigmoid()
        
        定义初始化（自身）：
            nn.init.uniform_(self.x1.weight)
            nn.init.uniform_(self.x2.weight)
    
        def前锋（自我，功绩）：
            f1 = torch.tensor(feats).float()
            xr1= 自身.x1(f1)
            xs1= self.s1(xr1)
            xr2= 自身.x2(xs1)
            输出 = self.s2(xr2)
            返回
    
        def 火车（自我，val_expected，feats_next）：
            val_expected_tensor = torch.tensor(val_expected)
            标准 = nn.MSELoss()
            优化器 = torch.optim.SGD(self.parameters(), lr=0.01)
            def 闭包():
                优化器.zero_grad()
                resp = self.forward(feats_next)
                误差 = 标准（分别，val_expected_tensor）
                error.backward()
                返回错误
            优化器.step(闭包)
    
    网络=网络（）
    .net.init()
    
    对于 ([0.,0.],[0.,1.],[1.,0.],[1.,1.]) 中的输入：
        响应=net.forward（输入）
        打印（响应）
    
    打印（“--火车开始-”）
    对于范围（1000）内的 i：
        net.train([1.],[0.,0.])
        net.train([0.],[1.,0.])
        net.train([0.],[0.,1.])
        net.train([1.],[1.,1.])
    print (&quot;---火车结束---&quot;)
    
    对于 ([0.,0.],[0.,1.],[1.,0.],[1.,1.]) 中的输入：
        响应=net.forward（输入）
        打印（响应）


这是一次以 0.001 学习率进行 100000 次迭代的运行：
&lt;前&gt;

    张量([0.7726], grad_fn=)
    张量([0.7954], grad_fn=)
    张量([0.8229], grad_fn=)
    张量([0.8410], grad_fn=)
    --列车启动-
    *.........*........*.........*.........*......... *.........*........*.........*.........*.........
    ---火车结束---
    张量([0.6311], grad_fn=)
    张量([0.6459], grad_fn=)
    张量([0.6770], grad_fn=)
    张量([0.6906], grad_fn=)


我真的迷路了。这不应该起作用吗？]]></description>
      <guid>https://stackoverflow.com/questions/64269084/simple-linear-sigmoid-neural-network-not-learning</guid>
      <pubDate>Thu, 08 Oct 2020 19:04:05 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中处理 nan/null 的分类器</title>
      <link>https://stackoverflow.com/questions/30317119/classifiers-in-scikit-learn-that-handle-nan-null</link>
      <description><![CDATA[我想知道 scikit-learn 中是否有处理 nan/null 值的分类器。我认为随机森林回归器可以处理这个问题，但当我调用 predict 时出现错误。
X_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])
y_train = np.array([1, 2])
clf = RandomForestRegressor(X_train, y_train)
X_test = np.array([7, 8, np.nan])
y_pred = clf.predict(X_test) # 失败！

我不能使用任何带有缺失值的 scikit-learn 算法来调用预测吗？
编辑。
现在我想起来，这是有道理的。这在训练期间不是问题，但是当您预测变量为空时如何分支时？也许你可以将两种方式分开并平均结果？看来只要距离函数忽略空值，k-NN 就应该可以正常工作。
编辑 2（我年纪更大、更聪明）
一些 GBM 库（例如 xgboost）使用三叉树而不是二叉树正是为了这个目的：2 个子节点用于是/否决策，1 个子节点用于缺失决策。 sklearn 使用二叉树&lt; /a&gt;]]></description>
      <guid>https://stackoverflow.com/questions/30317119/classifiers-in-scikit-learn-that-handle-nan-null</guid>
      <pubDate>Tue, 19 May 2015 05:02:35 GMT</pubDate>
    </item>
    </channel>
</rss>