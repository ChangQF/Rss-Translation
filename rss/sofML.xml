<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 05 Feb 2024 12:25:12 GMT</lastBuildDate>
    <item>
      <title>AFK 期间如何在 google collab 上训练模型？</title>
      <link>https://stackoverflow.com/questions/77940576/how-do-i-train-model-on-google-collab-while-being-afk</link>
      <description><![CDATA[我想在 Google Collab 上训练模型。问题是它需要大约。训练时间为五个小时，如果我离开电脑，Google Collab 将结束我的训练。因此，我想知道是否有一种方法可以在远离计算机的情况下训练我的模型。
我在网上看到了一些解决方案，但它们来自较旧的帖子，有些人说它们不再起作用。]]></description>
      <guid>https://stackoverflow.com/questions/77940576/how-do-i-train-model-on-google-collab-while-being-afk</guid>
      <pubDate>Mon, 05 Feb 2024 11:29:14 GMT</pubDate>
    </item>
    <item>
      <title>识别视频中的不同步骤</title>
      <link>https://stackoverflow.com/questions/77940396/recognition-different-steps-in-a-video</link>
      <description><![CDATA[我可以访问多个视频。我想创建一个机器/深度学习模型，可以识别新的看不见的视频中的不同步骤。
例如：让我简单地举一个咖啡的例子。所以我有几个人们煮咖啡的视频。步骤 1：冲泡咖啡（例如：时间 0:00 至 0:20），步骤 2：加糖（例如：时间 0:23 至 0:31），步骤 3：搅拌（例如：时间 0:33 至 0: 45）。
我应该如何标记我的镜头以及我应该从什么方向选择一个好的模型？
我还没有尝试过任何东西，因为我陷入了上述问题。
有人能指出我正确的方向吗？]]></description>
      <guid>https://stackoverflow.com/questions/77940396/recognition-different-steps-in-a-video</guid>
      <pubDate>Mon, 05 Feb 2024 10:59:34 GMT</pubDate>
    </item>
    <item>
      <title>允许模型在二元分类问题中返回中性响应</title>
      <link>https://stackoverflow.com/questions/77940271/allow-model-to-return-a-neutral-response-in-binary-classification-problem</link>
      <description><![CDATA[我正在尝试使用 Keras 的二元分类来解决问题。我想知道是否有任何方法可以通过添加中性响应（例如：跳过）而不是正常响应（是/否）来提高准确性，因此如果我的模型在预测时对提供的数据没有信心，则可以返回跳过。
我尝试构建一个初始的二元分类模型（保持简单以避免过度拟合），然后通过该模型验证所有训练数据，并更新响应表以将行值更改为中性值（如果预测错误）。之后创建一个包含 3 个类别（是、否、跳过）的分类模型。然而，这个新模型的 val_acc 和 val_loss 并没有太大的改进。你们有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/77940271/allow-model-to-return-a-neutral-response-in-binary-classification-problem</guid>
      <pubDate>Mon, 05 Feb 2024 10:36:26 GMT</pubDate>
    </item>
    <item>
      <title>加载注释txt格式并与图像匹配</title>
      <link>https://stackoverflow.com/questions/77939896/load-annotation-txt-format-and-match-with-images</link>
      <description><![CDATA[SI 有一组 txt 格式的图像和注释。我正在尝试加载注释，然后将图像匹配，以便稍后打印出图像与图像上的边界框。这个怎么做？在互联网上找不到任何相关信息。
&lt;前&gt;&lt;代码&gt;1
61.5535714286 71.6428571429 96.5535714286 104.5
95.8392857143 71.6428571429 117.982142857 95.9285714286
92.2678571429 85.9285714286 125.839285714 115.214285714

我已经加载了图像和标签。我尝试使用 Path 加载标签并将其放入列表中，但后来我意识到每个 txt 文件上面都有一个奇怪的数字，所以我用 pandas 创建了一个跳行，现在我不知道如何实现pandas 到我的 dict 函数中
导入 matplotlib.pyplot 作为 plt
将 matplotlib.patches 作为补丁导入
从 PIL 导入图像
将 pandas 导入为 pd

# 加载图像
图像 = Image.open(“数据”)

标签 = [路径（“标签”）]


pdlabel =“标签”；
df = pd.read_csv(pdlabel,skiprows=[0])



格式化 = dict(fieldnames=[ &#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;], delimiter=&#39; &#39;)

def process_labels(label_path_list): #TODO 寻找跳过行的新方法
    返回 {
        fop.stem: [dict(a) for a in csv.DictReader(fop.open(), **formatted)]
        对于 label_path_list 中的 fop
    }

标签 = process_labels(df)
打印（标签）
打印（标签）


Fig, ax = plt.subplots()

ax.imshow(图像)

矩形 = patch. 矩形(**标签)

ax.add_patch(矩形)

plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/77939896/load-annotation-txt-format-and-match-with-images</guid>
      <pubDate>Mon, 05 Feb 2024 09:37:51 GMT</pubDate>
    </item>
    <item>
      <title>我可以在 R 中重新创建 Python 模型吗？获得类似结果的机会有多大？</title>
      <link>https://stackoverflow.com/questions/77939149/can-i-recreate-a-python-model-in-rwhat-is-the-chance-that-i-get-similar-results</link>
      <description><![CDATA[我们正在 R 中开发用于价格预测的机器学习模型，我们将这些模型（R 对象）部署在 API 中。现在我们计划迁移到dataiku平台进行模型开发。但这里的问题是 dataiku 有 python 后端，我们只能创建 python 模型。我们无法在现有 API 中部署此模型，因为它严重依赖于 R。
是否有办法在 R 中重新创建 python 模型并输出 R 对象，以便我们可以继续使用相同的下游流程？
处理这种情况的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77939149/can-i-recreate-a-python-model-in-rwhat-is-the-chance-that-i-get-similar-results</guid>
      <pubDate>Mon, 05 Feb 2024 07:12:07 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的多消息保留[关闭]</title>
      <link>https://stackoverflow.com/questions/77939086/multiple-message-preserving-in-machine-learning</link>
      <description><![CDATA[多消息保留的定义
请给我一个机器学习或深度学习中多重消息保存的答案
1
在阈值加密方案中，经销商生成 (PK,SK1,…,SKn)(��,��1,…,����) 并将密钥分发给索引为 1,…,n1,…,� 的用户，如果组合器获得 t+1�+1 个部分解密的密文，则可以检索明文。
当只有一条消息 M� 时，我们可以先使用公钥 PK�� 加密 m� 得到 C�，然后使用 t� 私钥片段 Ski����, i∈[0,…,t]�ε[ 0,…,�]，对C�进行部分解密，命名为SKi(C)���(�)(i∈[0,…,t]�ε[0,…,�])，然后执行对t�结果进行一些操作以恢复原始Message M�。对于这种情况，(t,n)(�,�)-阈值 Paillier 方案是可行的。
但是当我们有多个消息 Mi��, i∈[0,…,t]�ε[0,…,�] 时，我们也使用 PK�� 分别对其进行加密，命名为 PK(Mi)=Ci� �(��)=��, i∈[0,…,t]�ε[0,…,�]，我们定义每条消息只能使用相应的私钥片段部分解密，如 SKi(Ci )���(��), i∈[0,…,t]�ε[0,…,�].那么，我们是否可以使用这些解密结果来恢复每条原始消息 Mi��(i∈[0,…,t]�ε[0,…,�]) 或者得到所有消息的总和 ΣMiΣ�� ？
哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈]]></description>
      <guid>https://stackoverflow.com/questions/77939086/multiple-message-preserving-in-machine-learning</guid>
      <pubDate>Mon, 05 Feb 2024 06:56:56 GMT</pubDate>
    </item>
    <item>
      <title>将机器学习模型部署到云上的工具和技术。使用微软Azure [关闭]</title>
      <link>https://stackoverflow.com/questions/77938824/tools-technique-to-deploy-ml-models-onto-the-cloud-using-microsoft-azure</link>
      <description><![CDATA[我正在寻找有关在 Microsoft Azure 上部署机器学习模型的指南。具体来说，我想知道针对此任务推荐哪些工具和技术。任何有关最佳实践的建议将不胜感激。

在 Microsoft Azure 上部署机器学习模型的推荐工具和技术是什么？

我应该遵循哪些最佳实践来确保顺利部署以及与其他 Azure 服务集成？

如何监控和管理已部署的模型以确保最佳性能和可靠性？

您是否有推荐的教程或资源来帮助您开始在 Azure 上部署机器学习模型？


我使用 Python 和 TensorFlow 或 Scikit-learn 等流行库开发了一个机器学习模型。现在，我需要将此模型部署到 Microsoft Azure 上，以便其他应用程序和服务可以访问和使用它。]]></description>
      <guid>https://stackoverflow.com/questions/77938824/tools-technique-to-deploy-ml-models-onto-the-cloud-using-microsoft-azure</guid>
      <pubDate>Mon, 05 Feb 2024 05:39:56 GMT</pubDate>
    </item>
    <item>
      <title>预处理新数据以根据 PyCaret 中的现有模型进行预测</title>
      <link>https://stackoverflow.com/questions/77938501/preprocessing-new-data-for-predictions-from-an-existing-model-in-pycaret</link>
      <description><![CDATA[我在 PyCaret 中有一个经过训练的模型，并且我能够在 setup() 期间对原始训练/测试拆分的测试数据进行预测。我还知道 Predict_model() 的“data”参数用于传递新的/未见过的数据。
我遇到的问题是：看不见的数据的列结构与模型预期的不同，因为模型执行了许多特征转换和编码。我正在尝试找出如何通过相同的预处理步骤运行我的新的未见数据，以便预测 1) 可能，2) 有意义。
我尝试过跑步
predict_model（最佳，数据= new_test_data）

我收到 KeyErrors：
KeyError：“[&#39;Some Feature 1&#39;] 不在索引中”

我已经阅读了有关 get_config 的文档，也许还阅读了 transform() 成员函数，但尚未成功确保对 new_test_data 执行相同的预处理步骤，以便可以使用现有模型对其进行预测。&lt; /p&gt;]]></description>
      <guid>https://stackoverflow.com/questions/77938501/preprocessing-new-data-for-predictions-from-an-existing-model-in-pycaret</guid>
      <pubDate>Mon, 05 Feb 2024 03:32:23 GMT</pubDate>
    </item>
    <item>
      <title>在Mini Batch梯度下降中应用StandardScaler，应用错误</title>
      <link>https://stackoverflow.com/questions/77937974/applying-standardscaler-in-mini-batch-gradient-descent-error-in-application</link>
      <description><![CDATA[ValueError：需要 2D 数组，却得到 1D 数组：
数组=[0。 0. 0. ... 0. 0. 1.]。
如果数据具有单个特征，则使用 array.reshape(-1, 1) 重塑数据；如果数据包含单个样本，则使用 array.reshape(1, -1) 重塑数据。

当我尝试运行代码时，我不断收到此错误：
# 分离特征 (X) 和目标变量 (y)
X = np.array(df[&#39;默认&#39;])
y = np.array(df[&#39;默认&#39;])
l = len(X)

# 将数据分为训练集和测试集

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 添加特征标量器 Z 分数标准化

定标器=标准定标器()
X_train_scaled = 缩放器.fit_transform(X_train)
X_test_scaled = 缩放器.transform(X_test)


# 实现小批量梯度

类 mini_batch_gradient_descent:
    
    def create_batch(self,X_train,y_train,batch_size):
        小批量=[]
        数据=np.stack((X_train,y_train),轴=1)
        np.random.shuffle(数据)
        batches=X_train.shape[0]//batch_size
        对于范围内的 i（批次）：
            mini_batch=数据[i*batch_size:(i+1)*batch_size]
            mini_batches.append((mini_batch[:,0], mini_batch[:,1]))
        如果 X_train.shape[0]/batch_size!=0:
            mini_batch=数据[i*batch_size:]
            mini_batches.append((mini_batch[:, 0], mini_batch[:,1]))
        返回小批量
    
    def fit(self,X_train,y_test,alpha,epochs,batch_size):
        self.m=np.random.randn(1,1)
        self.c=np.random.randn(1,1)
        l=len(X_train)
        对于范围内的 i（纪元）：
            批次= self.create_batch（X_train，y_train，batch_size）
            对于批量批次：
                xb=批次[0]
                yb=批次[1]
                xb=xb.reshape(1, xb.shape[0])
                截距=np.sum((np.dot(self.m,xb)+self.c)-yb)
                斜率=np.sum((np.dot(self.m,xb)+self.c)-yb)
                self.m=self.m-alpha*(斜率/l)
                self.c=self.c-alpha*(斜率/l)
    
    def 斜率截距():
        print(f&quot;斜率为 {self.m[0][0]}&quot;)
        print(f&quot;截距为 {self.c[0][0]}&quot;)
        
    
    def 预测（自我，X_test）：
        X_test=X_test.reshape(X_test.shape[0],1)
        self.m=self.m.reshape(self.m.shape[1],self.m.shape[0])
        结果=np.dot(X_test, self.m)+self.c
        返回结果

我尝试使用 loc/iloc，但它一直收到错误。
我正在使用数据帧，然后转换为 np.array 来运行程序。它可以在没有功能缩放器的情况下工作，但当我尝试实现缩放器时，它开始给我错误。不确定在功能扩展方面我还有什么其他选择。]]></description>
      <guid>https://stackoverflow.com/questions/77937974/applying-standardscaler-in-mini-batch-gradient-descent-error-in-application</guid>
      <pubDate>Sun, 04 Feb 2024 23:19:53 GMT</pubDate>
    </item>
    <item>
      <title>不知道如何在此 ML 程序中进行用户输入</title>
      <link>https://stackoverflow.com/questions/77937096/dont-know-how-to-do-user-input-in-this-ml-program</link>
      <description><![CDATA[导入 pandas 作为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.metrics 导入 r2_score、mean_squared_error

# 加载数据集
dp = pd.read_csv(&#39;https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv&#39;)

# 分离特征（x）和目标变量（y）
y = dp[&#39;logS&#39;]
x = dp.drop(&#39;logS&#39;, 轴=1)

# 分割数据
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=100)

# 线性回归模型
lr = 线性回归()
lr.fit(x_train, y_train)
y_train_pred_lr = lr.predict(x_train)
y_test_pred_lr = lr.predict(x_test)

# 随机森林回归模型
k1 = RandomForestRegressor（最大深度=2，随机状态=100）
k1.fit(x_train, y_train)
y_train_pred_rf = k1.predict(x_train)
y_test_pred_rf = k1.predict(x_test)

# 评估线性回归模型
y_train_mse_lr =mean_squared_error(y_train, y_train_pred_lr)
y_train_r2_lr = r2_score(y_train, y_train_pred_lr)
y_test_mse_lr = 均方误差(y_test, y_test_pred_lr)
y_test_r2_lr = r2_score(y_test, y_test_pred_lr)

# 评估随机森林回归模型
y_train_mse_rf =mean_squared_error(y_train, y_train_pred_rf)
y_train_r2_rf = r2_score(y_train, y_train_pred_rf)
y_test_mse_rf =mean_squared_error(y_test, y_test_pred_rf)
y_test_r2_rf = r2_score(y_test, y_test_pred_rf)

# 创建数据框
rs_lr = pd.DataFrame({“方法”: [“线性回归”],
                      “训练MSE”：[y_train_mse_lr]，
                      “训练R2”：[y_train_r2_lr]，
                      “测试 MSE”：[y_test_mse_lr]，
                      “测试 R2”：[y_test_r2_lr]})

rs_rf = pd.DataFrame({“方法”: [“随机森林回归器”],
                      “训练MSE”：[y_train_mse_rf]，
                      “训练R2”：[y_train_r2_rf]，
                      “测试 MSE”：[y_test_mse_rf]，
                      “测试 R2”：[y_test_r2_rf]})

# 连接数据帧
结局 = pd.concat([rs_lr, rs_rf],ignore_index=True)
打印（结局）

加载数据集：它使用 Pandas 从 URL 加载数据集。该数据集与分子溶解度相关，包含各种分子描述符。
数据准备：它将特征（x）和目标变量（y）从数据集中分离出来。本例中的目标变量是溶解度的对数 (logS)。
数据拆分：它使用 scikit-learn 中的 train_test_split 函数将数据集拆分为训练集和测试集。训练集用于训练模型，测试集用于评估模型的性能。
模型训练：它使用训练数据训练两个回归模型 - 线性回归模型 (lr) 和随机森林回归模型 (k1)。
预测：它使用经过训练的模型对训练集和测试集进行预测。
模型评估：它使用均方误差 (MSE) 和 R 平方 (R2) 分数评估两个模型的性能。这些指标可以深入了解模型对数据的拟合程度。
创建 DataFrame：它创建两个单独的 DataFrame（rs_lr 和 rs_rf）来存储每个模型的评估指标。
串联：它将两个 DataFrame 连接成一个最终的 DataFrame（结局）。此 DataFrame 总结了两种模型的训练和测试性能。
打印结果：最后，它打印串联的 DataFrame（结局），其中包括线性回归和随机森林回归模型的方法名称、训练 MSE、训练 R2、测试 MSE 和测试 R2。
该程序的目标是比较线性回归和随机森林回归模型在根据描述符预测分子溶解度方面的性能。
我希望用户输入值。那么我该如何修改代码呢？]]></description>
      <guid>https://stackoverflow.com/questions/77937096/dont-know-how-to-do-user-input-in-this-ml-program</guid>
      <pubDate>Sun, 04 Feb 2024 18:32:58 GMT</pubDate>
    </item>
    <item>
      <title>在 Flask 框架中集成 ML 模型时似乎无法解决此错误</title>
      <link>https://stackoverflow.com/questions/77892140/cant-seem-to-solve-this-error-while-integrating-a-ml-model-in-a-flask-framework</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77892140/cant-seem-to-solve-this-error-while-integrating-a-ml-model-in-a-flask-framework</guid>
      <pubDate>Sat, 27 Jan 2024 17:33:27 GMT</pubDate>
    </item>
    <item>
      <title>知道如何处理 mglearn 导入错误吗？</title>
      <link>https://stackoverflow.com/questions/75208167/any-idea-how-to-deal-with-mglearn-import-error</link>
      <description><![CDATA[我是一名新的数据科学训练营学生。最近我买了一本书《Introduction to Machine Learning with Pyhton》。然而，本书大量使用了 mglearn 库。当我想导入库时出现错误。 （你可以从下面看到。）我无法演示书中提供的示例。有什么办法可以解决这个问题吗？
提前非常感谢！
ImportError Traceback（最近一次调用最后一次）
[3] 第 1 行中的单元格
----&gt; 1 导入mglearn

文件 c:\Users\murad\AppData\Local\Programs\Python\Python310\lib\site-packages\mglearn\__init__.py:1
----&gt; 1 来自 .导入地块
      2 从 .导入工具
      3 从.plots导入cm3，cm2

文件 c:\Users\murad\AppData\Local\Programs\Python\Python310\lib\site-packages\mglearn\plots.py:5
      3 从.plot_animal_tree导入plot_animal_tree
      4 从.plot_rbf_svm_parameters导入plot_svm
----&gt; 5 从.plot_knn_regression导入plot_knn_regression
      6 从.plot_knn_classification导入plot_knn_classification
      7 从.plot_2d_separator导入plot_2d_classification，plot_2d_separator

文件 c:\Users\murad\AppData\Local\Programs\Python\Python310\lib\site-packages\mglearn\plot_knn_regression.py:7
      4 从sklearn.neighbors导入KNeighborsRegressor
      5 从 sklearn.metrics 导入 euclidean_distances
----&gt; 7 从.datasets导入make_wave
      8 从.plot_helpers导入cm3
     11 defplot_knn_regression(n_neighbors=1):

文件 c:\Users\murad\AppData\Local\Programs\Python\Python310\lib\site-packages\mglearn\datasets.py:5
      3 导入操作系统
      4 从 scipy 导入信号
----&gt; 5 从 sklearn.datasets 导入 load_boston
      6 从sklearn.preprocessing导入MinMaxScaler、PolynomialFeatures
      7 从 .make_blobs 导入 make_blobs

文件 c:\Users\murad\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\datasets\__init__.py:156，在 __getattr__(name) 中
    105 如果名称==“load_boston”：
    [第 106 章]
    第107章
    108 自 1.2 版本以来，`load_boston` 已从 scikit-learn 中删除。
   （...）
    第154章
    155）
--&gt; 156 引发导入错误（消息）
    157 尝试：
    158 return globals()[名称]

导入错误：
自 1.2 版本以来，`load_boston` 已从 scikit-learn 中删除。

波士顿房价数据集存在伦理问题：
在[1]中进行了调查，该数据集的作者设计了一个
不可逆变量“B”假设种族自我隔离有
对房价产生积极影响[2]。此外，该项目的目标是
导致创建该数据集的研究是为了研究
空气质量的影响，但没有充分证明
这个假设的有效性。

因此，scikit-learn 维护者强烈反对使用
...
[2] 小哈里森、大卫和丹尼尔·鲁宾菲尔德。
“享乐的房价和对清洁空气的需求。”
环境经济与管理杂志5.1（1978）：81-102。
&lt;https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air&gt;

我试图从网上寻找答案，但找不到任何东西。]]></description>
      <guid>https://stackoverflow.com/questions/75208167/any-idea-how-to-deal-with-mglearn-import-error</guid>
      <pubDate>Mon, 23 Jan 2023 10:26:04 GMT</pubDate>
    </item>
    <item>
      <title>CNN模型检测脑肿瘤的Val_accuracy保持不变；我认为我增强数据和预处理的方式是错误的</title>
      <link>https://stackoverflow.com/questions/71198909/val-accuracy-staying-the-same-for-cnn-model-to-detect-brain-tumors-im-thinking</link>
      <description><![CDATA[我使用的数据集是https://www.kaggle.com/clarksaben /ct-头部扫描
defaugment_data(图像):
    图像 = [图像]
    对于范围 (6) 内的 i：
        变换 = A.Compose([
        A.调整大小(256, 256),
        A. 随机裁剪(224, 224),
        A.OneOf([A.Horizo​​ntalFlip(p=1),
                 A.随机旋转90(p=1),
                 A.VerticalFlip(p=1),
                 A.模糊(),
                 A.RandomBrightnessContrast(p=1)], p = 1),
        A.OneOf([A.MotionBlur(p=1),
                 A.光学畸变(p=1),
                 A.高斯噪声(p=1),
                 A.RandomGamma(p=1),
                 A.CLAHE(p=1)], p=1),
        ]）
        变换后的图像 = 变换（图像 = 图像）
        images.append(transformed_image[&#39;图像&#39;])
    返回图像

def load_data(数据):
    paths = [&#39;../input/ct-head-scans/&#39; + x for x in data[&#39;ID&#39;]]
    图片 = []
    对于路径中的路径：
        if path.find(&#39;gaus&#39;) == -1 且 path.find(&#39;elastic&#39;) == -1 且 path.find(&#39;contrast&#39;) == -1 且 path.find(&#39;gamma&#39;) == -1 和 path.find(&#39;clahe&#39;) == -1 和 path.find(&#39;blur&#39;) == -1：
            图像 = cv2.imread(路径, 0)
            图像 = cv2.bilingualFilter(图像, 2, 50, 50)
            增强=增强数据（图像）
            对于增强中的图像：
                img_data = aImage.astype(&#39;float32&#39;)
        
                img_data = cv2.resize(img_data,(256,256)) / 255
                img_data = img_data.reshape((256,256,1))
                图像.append(img_data)
    返回图像、数据[&#39;肿瘤存在&#39;]

def定义_模型（）：
    模型=顺序（）
    model.add(Conv2D(16, (5, 5), 激活 = &#39;relu&#39;, input_shape=(256, 256, 1)))
    模型.add(Dropout(0.5))
    model.add(MaxPooling2D(2, 2))
    model.add(Conv2D(16, (5, 5), 激活 = &#39;relu&#39;))
    model.add(MaxPooling2D(2, 2))
    model.add(Conv2D(32, (5, 5), 激活 = &#39;relu&#39;))
    model.add(MaxPooling2D(2, 2))
    model.add(Conv2D(32, (5, 5), 激活 = &#39;relu&#39;))
    model.add(MaxPooling2D(2, 2))
    model.add(Conv2D(64, (5, 5), 激活 = &#39;relu&#39;))
    model.add(MaxPooling2D(2, 2))
    模型.add(压平())
    model.add（密集（10，激活=&#39;sigmoid&#39;））
    模型.add(压平())
    model.add（密集（128，激活=&#39;relu&#39;））
    模型.add(压平())
    model.add（密集（1，激活=&#39;sigmoid&#39;））
    模型.add(压平())
    opt = tf.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer=opt,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
    返回模型

我一直在尝试添加新图层并切换参数，但似乎没有任何效果。我相信问题可能是我增强数据和进行预处理的方式，但我不确定如何改进这一点。实施示例会很棒！]]></description>
      <guid>https://stackoverflow.com/questions/71198909/val-accuracy-staying-the-same-for-cnn-model-to-detect-brain-tumors-im-thinking</guid>
      <pubDate>Sun, 20 Feb 2022 21:22:41 GMT</pubDate>
    </item>
    <item>
      <title>有状态 LSTM VAE：无效参数：您必须为占位符张量“decoder_input”提供一个值，dtype float 和形状 [batch_size, Latent_dim]</title>
      <link>https://stackoverflow.com/questions/70821374/stateful-lstm-vae-invalid-argument-you-must-feed-a-value-for-placeholder-tenso</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/70821374/stateful-lstm-vae-invalid-argument-you-must-feed-a-value-for-placeholder-tenso</guid>
      <pubDate>Sun, 23 Jan 2022 11:03:37 GMT</pubDate>
    </item>
    <item>
      <title>训练 GLM Lasso 模型后对新数据进行预测</title>
      <link>https://stackoverflow.com/questions/61193285/make-predictions-on-new-data-after-training-the-glm-lasso-model</link>
      <description><![CDATA[我在 r 的 glmnet 库中使用 lasso 对 13,000 行标签训练了一个分类模型。我检查了我的准确性，看起来不错，现在我想对数据集的其余部分（即 300,000 行）进行预测。我的方法是使用经过训练的模型来标记其余的行。我不确定这是否是进行近似标记的最有效策略。
但是，当我尝试标记其余数据时，我遇到了此错误：
asMethod(object) 中的错误：Cholmod 错误“问题太大”位于文件 ../Core/cholmod_dense.c，第 105 行

即使我将数据集分解为 5000 行进行预测，我仍然会遇到相同的错误。
这是我的代码：
库(glmnet)
#原始数据集的子集
data.text &lt;- data.text_filtered %&gt;% 过滤器(!label1 == &quot;NA&quot;)

#Quanteda 语料库
data_corpus &lt;- corpus(data.text$text, docvars = data.frame(labels = data.text$label1))

设置.种子(1234)

dataShuffled &lt;- corpus_sample(data_corpus, size = 12845)

dataDfm &lt;- dfm_trim( dfm(dataShuffled, verbose = FALSE), min_termfreq = 10)

#训练分类器的模型
套索 &lt;- cv.glmnet(x = dataDfm[1:10000,], y = trainclass[1:10000],
    alpha = 1，nfolds = 5，族 =“二项式”）

#plot 套索图
情节（套索）

#预测
dataPreds &lt;- 预测(lasso, dataDfm[10000:2845,], type=&quot;class&quot;)
(movTable &lt;- 表(dataPreds, docvars(dataShuffled, “标签”)[10000:2845]))

对数据集的其余部分进行预测。该数据集有 300,000 行。
data.text_NAs &lt;- data.text_filtered %&gt;% 过滤器(label1 == &quot;NA&quot;)

data_NADfm &lt;- dfm_trim( dfm(corpus(data.text_NAs$text), verbose = FALSE), min_termfreq = 10)

data.text_filtered &lt;- data.text_filtered %&gt;% mutate(label = Predict(lasso, as.matrix(data_NADfm), type=“class”, s=“lambda.1se”)

]]></description>
      <guid>https://stackoverflow.com/questions/61193285/make-predictions-on-new-data-after-training-the-glm-lasso-model</guid>
      <pubDate>Mon, 13 Apr 2020 17:29:14 GMT</pubDate>
    </item>
    </channel>
</rss>