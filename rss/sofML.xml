<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 10 Feb 2025 18:23:59 GMT</lastBuildDate>
    <item>
      <title>机器学习模型建议使用一个传感器的输出来从其他传感器产生输出</title>
      <link>https://stackoverflow.com/questions/79427928/machine-learning-model-suggestions-for-using-an-output-from-one-sensor-to-genera</link>
      <description><![CDATA[我是ML的新手，如果有人可以建议使用其他传感器的输出来从传感器中产生输出，我将非常感谢。。
传感器应该显示同一件事，但只使用不同的方法来获取连续的时间信号数据。我有来自两个传感器的时间同步数据记录，并希望根据这些数据训练模型。然后，我想提供一个传感器记录作为模型的输入，并从另一个传感器中获取谓语信号记录。我是否有关于我可以用来实现这一目标的机器学习方法的建议？任何帮助将不胜感激，谢谢。
注意：数据是两个传感器的连续时间记录的不同集，每个传感器的记录大约为一分钟。
我一直在做MATLAB ONRAMP，并在机器学习上观看YouTube视频，但是由于我对从哪里开始或在学习中投入什么，因为我的系统是如此，所以我不确定要花费什么时间特定于任务。我只想对可能有效的方法提出一些建议，以便我知道要研究哪些领域。]]></description>
      <guid>https://stackoverflow.com/questions/79427928/machine-learning-model-suggestions-for-using-an-output-from-one-sensor-to-genera</guid>
      <pubDate>Mon, 10 Feb 2025 18:02:32 GMT</pubDate>
    </item>
    <item>
      <title>但是，使用合规列名称上传数据。但是，它仍然通知列名中有无效的数据</title>
      <link>https://stackoverflow.com/questions/79427914/upload-data-on-neuton-ai-using-compliant-column-name-however-it-still-notify-t</link>
      <description><![CDATA[所以我正在使用neton.ai训练我的小型AI模型，我有一个由Excel编辑的CSV文件数据集，格式为：
 


 ax1 
 ay1 
 az1 
 gx1 
 gy1 
 gz1 
 ... 
 ax50 
 ay50 
 az50 
 gx50 
 gy50 
 gz50 




数据
数据
数据
数据
数据
数据
 ... 
数据
数据
数据
数据
数据
数据


数据
数据
数据
数据
数据
数据
 ... 
数据
数据
数据
数据
数据
数据


 ... 
 
 
 
 
 
 
 
 
 
 
 
 


 
问题是我在标题中提到的，即使我尝试以不同的方式更改列名，它也会给我相同的错误消息：
所有列名（CSV文件标头中的值）必须仅包含字母（A-Z，A-Z），数字（0-9），连字符（ - ）或下划线（_）。更改名称并再次下载数据集。
 错误消息屏幕屏幕示例 
我是新来的，所以我愿意收到任何建议。
有人帮我~~~ ]]></description>
      <guid>https://stackoverflow.com/questions/79427914/upload-data-on-neuton-ai-using-compliant-column-name-however-it-still-notify-t</guid>
      <pubDate>Mon, 10 Feb 2025 17:56:26 GMT</pubDate>
    </item>
    <item>
      <title>使用OpenCV和MediaPipe进行姿势估算时面对此错误</title>
      <link>https://stackoverflow.com/questions/79427900/facing-this-error-when-doing-pose-estimation-with-opencv-and-mediapipe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79427900/facing-this-error-when-doing-pose-estimation-with-opencv-and-mediapipe</guid>
      <pubDate>Mon, 10 Feb 2025 17:49:19 GMT</pubDate>
    </item>
    <item>
      <title>分类器的机器学习部分依赖图</title>
      <link>https://stackoverflow.com/questions/79427690/machine-learning-partial-dependence-plots-of-classifier</link>
      <description><![CDATA[我制作了以下图：  
请忽略我没有完成的那种丑陋，但是您只能专注于第一行。 Y轴是我计算的条件期望（请参见底部的代码），这是[0,1]中的概率，X轴是我的自变量（底部的区域或卧室）。我对自己的情节有些困惑，所有期望都显着高于0.5，这意味着我将获得更多的积极价值，但这些都应该是少数群体。我用smote使他们达到50/50的余额，但我仍然认为有条件的期望应该不那么高，尤其是因为我的模型表现良好。
以下是我的蒙特卡洛集成，以进行有条件的期望。
  def calculate_partial_dependence（model，x，features_index，grid_resolution = 100，sample_num = 20000）：
    “”
    计算功能对模型预测的部分依赖性。

    参数：
     - 型号：模型
    -X：数据集
    -Features_Index：为其计算部分依赖性的功能的索引。
    -grid_resolution：该功能网格中的点数。
    -sample_num：网格中每个点的均匀绘制数量

    返回：
     -  feature_values：计算部分依赖的特征的值。
    -pd_values：部分依赖值。
    “”
    f = x [：，feature_index]
    x_deleted = np.delete（x，feature_index，axis = 1）

    a = np.min（f）
    b = np.max（f）
    值= np.linspace（start = a，
                    停止= b，
                    num = grid_resolution）
    期望= np.seros_like（values）

    kde = kerneldents（）
    如果x_deleted.shape [1]！= 0：
        对于IDX，枚举（值）中的条件：
            #find附近的X，然后计算“条件ish”密度，因为我们没有足够的值来实现实际条件密度
            XL = 0
            Q = 5
            xl＆lt; = 10：
                R =（条件-Q*（B-A）/GRID_RESOLUTION，条件 + Q*（B-A）/GRID_RESOLITY）
                #filter指数r
                x_temp = x_deleted [（f＆gt; = r [0]）＆amp; （f＆lt; = r [1]），：]
                xl = x_temp.shape [0]
                Q += 1

            #Generate样本分布
            generator = kde.fit（x_temp）.sample（sample_num）
            col = np.ones（（Sample_num，1））*条件
            样本= np.concatenate（（col，Generator），轴= 1）

            #Predict tagrip概率
            output = model.predict_proba（示例）[：，1]
            
            自从蒙特卡洛以来 
            期望[idx] = np.mean（输出）

    别的： 
        ＃NULL模型只有一个变量，因此我不需要``条件&#39;&#39;
        期望= model.predict_proba（values）[：，1]
    返回值，期望

 
我在这里误会了什么吗？我也尝试了像Sklearn这样的普通图书馆，以达到有条件的期望，他们给了我相同的答案。我的模型是腥的，还是我不明白我在看什么。
 注意：这是从统计/交叉验证的堆栈交换中进行的，我没有收到任何答案，但它仍然升起。]]></description>
      <guid>https://stackoverflow.com/questions/79427690/machine-learning-partial-dependence-plots-of-classifier</guid>
      <pubDate>Mon, 10 Feb 2025 16:22:18 GMT</pubDate>
    </item>
    <item>
      <title>降尺度操作不捕获峰[关闭]</title>
      <link>https://stackoverflow.com/questions/79427067/downscaling-operation-not-capturing-the-peaks</link>
      <description><![CDATA[我正在使用随机的森林回归模型将数据集降低从更粗的分辨率到更高分辨率。我尝试了交叉验证并调整参数。在检查验证分数时，我的MSE和R2值都相当好。但是，当我使用相同的模型预测新数据集的目标变量时，该模型将无法捕获数据初始数年的峰值。可能是其中一个功能具有主要的重要性。在绘制主要功能的数据时，全年数据稳定。
这使趋势偏离了
具有最重要性的数据之一  ]]></description>
      <guid>https://stackoverflow.com/questions/79427067/downscaling-operation-not-capturing-the-peaks</guid>
      <pubDate>Mon, 10 Feb 2025 12:29:38 GMT</pubDate>
    </item>
    <item>
      <title>如何在大型数据集上训练XGBoost并改善欺诈检测？</title>
      <link>https://stackoverflow.com/questions/79426998/how-to-train-xgboost-on-a-large-dataset-and-improve-fraud-detection</link>
      <description><![CDATA[我只是从ML开始，所以我感谢任何建议。
我正在尝试培训一个模型以进行交易中的欺诈检测。数据高度不平衡（〜96％的正常人比〜4％欺诈）。
第一期 - 记忆消耗
培训文件为32 GB，但是即使仅阅读100万行，我也会收到一个内存分配错误：
  XGBOOST.CORE.XGBOOSTERROR：BAD_MALLOC：无法分配255479999900字节。
 
第二期 - 预测质量差
我正在对100k行进行训练，但是无论我如何调整XGBoost，该模型都几乎都无法检测到欺诈案件。
在这种情况下，XGBoost的最佳类平衡技术是什么？我应该如何处理这个数据集？您建议更改什么？
在此处输入图像描述 
  df = pd.read_csv（&#39;train.csv&#39;，nrows = 100000） 

df.drop（[&#39;transaction_id&#39;，&#39;card_holder_first_name&#39;，&#39;card_holder_last_name&#39;，&#39;is_verified&#39;，&#39;browser&#39;，&#39;browser_version&#39;，&#39;browser_version&#39;，&#39;
    &#39;operation_system&#39;，&#39;operation_system_version&#39;，&#39;card_id&#39;，&#39;ip_address&#39;，&#39;merchant_customer_id&#39;，&#39;merchant_id&#39;，&#39;user_agent&#39;，&#39;user_agent&#39;，
    &#39;merchant_customer_last_name&#39;，&#39;merchant_customer_first_name&#39;，&#39;merchant_customer_phone&#39;，&#39;merchant_customer_email&#39;，&#39;bin&#39;，&#39;bin&#39;，&#39;&#39;
    &#39;clockal_source&#39;，&#39;transaction_source&#39;，&#39;merchant_city&#39;，&#39;merchant_shop_id&#39;，&#39;merchant_shop_name&#39;，&#39;order_number&#39;]， 
    轴= 1，Inplace = true）

df [&#39;bank&#39;]。替换（&#39;&#39;，&#39;_&#39;，regex = true，inplace = true）

df [&#39;create_at&#39;] = pd.to_datetime（df [&#39;create_at&#39;]）
df [&#39;seconds_since_midnight&#39;] = df [&#39;create_at&#39;]。dt.hour * 3600 + df [&#39;create_at&#39;]。dt.minute * 60 + df [&#39;create_at&#39;]。dt.dt.second
df [&#39;day_of_week&#39;] = df [&#39;create_at&#39;]。dt.weekday

df.drop（&#39;create_at&#39;，axis = 1，Inplace = true）

df.loc [pd.isna（df [&#39;merchant_language&#39;]），&#39;merchant_language&#39;] =&#39;unknown&#39;

df.loc [pd.isna（df [&#39;payment_type&#39;]），&#39;payment_type&#39;] = 0

x = df.drop（&#39;is_fraud&#39;，axis = 1）.copy（）

y = df [&#39;is_fraud&#39;]。copy（）

x_encoded = pd.get_dummies（x，columns = [&#39;merchant_country&#39;，      
                                        &#39;transaction_type&#39;，
                                        “商人_language”，
                                        &#39;平台&#39;，
                                        &#39;ip_country&#39;，
                                        &#39;银行&#39;，
                                        “ cardbrand&#39;，
                                        &#39;cardcountry&#39;，
                                        &#39;cardtype&#39;， 
                                        &#39;payment_type&#39;]））


x_train，x_test，y_train，y_test = train_test_split（x_encoded，y，andury_state = 42，strate = y）


clf_xgb = xgb.xgbClassifier（
    objective =;二进制：logistic; quot;
    种子= 42，
    eval_metric =＆quot; aucpr; quot;
    早期_stopping_rounds = 10，
    max_depth = 6，  
    子样本= 0.8，  
    colsample_bytree = 0.8 
）

clf_xgb.fit（
    x_train，
    y_train，
    eval_set = [（x_test，y_test）]，
    冗长= true
）

disp =混淆matrixdisplay.from_estimator（
    clf_xgb，  
    x_test，  
    y_test，  
    display_labels = [不是欺诈者“  
    cmap =&#39;blues;
）
disp.plot（values_format =&#39;d&#39;）

plt.show（）
 
我是ML的新手，所以我还没有尝试过很多技术。我尝试了不同的XGBoost参数，并减少了数据集大小以适合内存，但是该模型仍在努力检测欺诈情况。我不确定最好的方法是处理如此大的不平衡数据集，因此我感谢任何建议。]]></description>
      <guid>https://stackoverflow.com/questions/79426998/how-to-train-xgboost-on-a-large-dataset-and-improve-fraud-detection</guid>
      <pubDate>Mon, 10 Feb 2025 11:56:48 GMT</pubDate>
    </item>
    <item>
      <title>测试分类问题的数据配置[关闭]</title>
      <link>https://stackoverflow.com/questions/79426228/test-data-configuration-for-a-classification-problem</link>
      <description><![CDATA[我创建了一个用于培训神经网络的二进制分类问题的数据集。培训数据来自与特定环境（例如2D地图环境）有关的集合。对于测试案例，我考虑了来自同一2D地图的数据点，但培训集中不存在数据点。
这将是有效的测试用例设计？]]></description>
      <guid>https://stackoverflow.com/questions/79426228/test-data-configuration-for-a-classification-problem</guid>
      <pubDate>Mon, 10 Feb 2025 06:23:53 GMT</pubDate>
    </item>
    <item>
      <title>ML模型保存</title>
      <link>https://stackoverflow.com/questions/79426066/ml-models-saving</link>
      <description><![CDATA[我正在训练一种机器学习模型，将阿尔茨海默氏病分为四类。运行培训时期后，我使用代码将模型保存在.pth文件中。但是，在下载了保存的模型并将其与我的接口连接起来后，它没有产生任何结果。经过进一步检查，似乎模型是空的。
 导入火炬
导入Torch.nn作为nn
导入Torch.optim作为最佳
来自TQDM.Auto Import TQDM
来自Torchvision导入模型

＃设置设备（GPU如果可用）
设备= torch.device（&#39;cuda&#39;如果torch.cuda.is_available（）else&#39;cpu&#39;）

＃定义Resnet18模型
类Resnet18（nn.Module）：
    def __init __（self，num_classes）：
        super（resnet18，self）.__ init __（）
        self.model = model.Resnet18（预审计= false）＃设置为true以使用预训练的权重
        self.model.fc = nn.linear（self.model.fc.in_features，num_classes）＃修改最终层

    def向前（self，x）：
        返回self.model（x）

＃用数据集中的类数初始化模型
num_classes = len（train_data_simple.classes）＃根据您的增强数据集进行调整
model_resnet = resnet18（num_classes = num_classes）。

＃损失功能和优化器（SGD和Crossentropy）
loss_fn = nn.Crossentropyloss（）
优化器= optim.sgd（model_resnet.parameters（），lr = 0.1，动量= 0.9）

＃训练循环（使用TQDM进行进度栏）
def train（型号，train_dataloader，test_dataloader，优化器，loss_fn，epochs）：
    对于TQDM中的时期（范围（时代））：
        model.train（）＃将模型设置为训练模式
        Running_loss = 0.0
        recript_train，total_train = 0，0

        对于枚举（train_dataloader）的批次（x，y）：
            x，y = x.to（设备），y.to（设备）

            优化器.zero_grad（）＃零梯度

            输出=模型（x）＃前向通行证
            损失= loss_fn（输出，y）＃计算损失

            lose.backward（）＃backpropagation
            Optimizer.step（）＃更新权重

            running_loss += loss.item（）
            _，预测= torch.max（outputs.data，1）
            total_train += y.size（0）
            recripe_train +=（预测== y）.sum（）。item（）

        ＃打印培训损失和准确性
        打印（f＆quot; epoch [{epoch+1}/{epochs}]，损失：{runn_loss/len（train_dataloader）：。4f}＆quort;）
        打印（f＆quot“训练精度：{100 * recript_train / total_train：.2f}％＆quot”）

        ＃ 验证
        model.eval（）＃开关模型到评估模式进行验证
        recript_val，total_val = 0，0
        使用Torch.no_grad（）：
            对于x_val，y_val在test_dataloader中：
                x_val，y_val = x_val.to（设备），y_val.to（设备）
                val_outputs =模型（x_val）
                _，预测= torch.max（val_outputs.data，1）
                total_val += y_val.size（0）
                prection_val +=（预测== y_val）.sum（）。item（）。

        ＃打印验证精度
        打印（f＆quot“验证精度：{100 * recript_val / total_val：.2f}％＆quort”）

    返回模型

＃示例培训
num_epochs = 30＃根据您的要求调整时期
＃使用增强培训数据加载器
trained_model_resnet = train（model_resnet，train_dataloader_simple，test_dataloader_simple，optimizer，optimizer，lose_fn，num_epochs）

 
这是我在不同单元格中运行的代码来保存模型
 ＃保存训练有素的模型
model_path =＆quot trained_resnet18_model.pth＆quot;
TORCH.SAVE（model_resnet.state_dict（），model_path）
打印（f＆quot“模型保存到{model_path}”
 
我正在训练一种机器学习模型，将阿尔茨海默氏病分为四类，使用pytorch分为四类。经过几个时期的训练后，我想使用诸如model.save（&#39;my_model.h5&#39;）或torch.save（model.state_dict（），&#39;model.pth&#39;）的代码保存训练的模型。
我应该添加代码以将训练的模型保存在定义训练时期并运行训练环的同一单元格中，还是将其放在其他单元格中更好？放置是否会影响模型的保存方式，或者以后如何加载？]]></description>
      <guid>https://stackoverflow.com/questions/79426066/ml-models-saving</guid>
      <pubDate>Mon, 10 Feb 2025 04:15:24 GMT</pubDate>
    </item>
    <item>
      <title>在扩散模型中U形网络的上下抽样阶段中不同的缩小参数</title>
      <link>https://stackoverflow.com/questions/79422978/different-concate-dimension-parameters-in-the-up-and-down-sampling-phase-of-u-sh</link>
      <description><![CDATA[我遇到的错误是
 文件＆quot＆quot＆quot＆quot&gt;
    x = torch.cat（（x，residual_x），dim = 1） 
RuntimeError：张量的尺寸必须匹配，除了尺寸1。预期尺寸24，但在列表中获得张量1的尺寸25。
 
该模型的训练数据集输入是Paviau。
调试后要找到问题的输出向量如下：
  down = torch.size（[4，64，25，64，64]）
UP = Torch.Size（[4，64，24，64，64]）
 
整个代码如下：
 
    def向前（self，x，timeStep，feature = false）：
        ＃嵌入式时间
        t = self.time_mlp（timeStep）
        ＃初始转换
        x = self.conv0（x）
        ＃UNET
        ristual_inputs = []
        在self.downs中进行下降：
            x = down（x，t）
            ristual_inputs.append（x）
        为了自我。
            ristual_x = ristual_inputs.pop（）
            ＃print（down =; quord =; residual_x.shape，＆quot up =; quord =; x.Shape）
            ＃添加残留X作为其他频道
            x = torch.cat（（x，residual_x），dim = 1）* 
            如果特征：
                self.features.append（x.detach（）。cpu（）。numpy（））
            x =向上（x，t）
        返回self.output（x）
         
 ]]></description>
      <guid>https://stackoverflow.com/questions/79422978/different-concate-dimension-parameters-in-the-up-and-down-sampling-phase-of-u-sh</guid>
      <pubDate>Sat, 08 Feb 2025 10:14:34 GMT</pubDate>
    </item>
    <item>
      <title>从GPU内存中清除tf.data.dataset</title>
      <link>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</link>
      <description><![CDATA[在实施使用 tf.data.dataset 作为KERAS模型的输入的训练环时，我遇到了问题。我的数据集具有以下格式的元素规范：
 （{&#39;data&#39;：tensorSpec（shape =（15000，1），dtype = tf.float32），&#39;index&#39;：tensorspec&#39;：tensorspec（shape =（2，2，2，2， ），dtype = tf.int64）}，tensorspec（shape =（1，），dtype = tf.int32））
 
因此，基本上，每个样本均以元组（x，y）构建形状（15000，1），另一个形状索引（2，）（在培训期间不使用索引）， y 是单个标签。
The tf.data.Dataset is created using dataset = tf.data.Dataset.from_tensor_slices((X, y)), where X是两个密钥的命令：

 数据：形状的NP数组（200k，1500，1）， index  with 
 索引：形状的NP数组（200K，2） 

和 y 是形状的单个数组（200k，1） 
我的数据集有大约200k的培训样本（运行底漆后）和200k验证样本。
呼叫 tf.data.dataset.from_tensor_slices 我注意到GPU内存使用情况中有一个尖峰，在创建培训 tf.dataset ，and和创建验证 tf.dataset 。之后，更多16GB
创建 tf.dataSet 后，我运行了一些操作（例如，洗牌，批处理和预拿方），并调用 model.fit.fit 。我的型号大约有500K可训练的参数。
我遇到的问题是  拟合模型。我需要在一些其他数据上运行推断，因此我使用此数据创建了一个新的 tf.dataset ，再次使用 tf.dataset.from_tensor_slices 。但是，我注意到培训和验证 tf.dataset 仍然存在于GPU内存中，这导致我的脚本随着新的 tf.dataset  i而遇到的不含内存问题 i想要推断。
我在两个 tf.dataset 上尝试调用 del ，然后随后调用 gc.collect（）清除RAM，而不是GPU内存。另外，我尝试禁用我应用的某些操作，例如预摘要，也可以使用批处理大小，但是这些都没有用。我还尝试调用 keras.backend.clear_session（），但它也无助于清除GPU内存。我还尝试从 numba 导入 cuda ，但是由于我的安装，我无法使用它来清除内存。我有什么办法可以从gpu内存中清除 tf.data.dataset ？]]></description>
      <guid>https://stackoverflow.com/questions/79420818/clearing-tf-data-dataset-from-gpu-memory</guid>
      <pubDate>Fri, 07 Feb 2025 12:02:12 GMT</pubDate>
    </item>
    <item>
      <title>用Java编写的AI分类均匀，奇数行不通[关闭]</title>
      <link>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79413494/ai-written-in-java-which-classifies-even-and-odd-numbers-doesnt-work</guid>
      <pubDate>Wed, 05 Feb 2025 02:09:06 GMT</pubDate>
    </item>
    <item>
      <title>随机森林回归中的树木数[关闭]</title>
      <link>https://stackoverflow.com/questions/56505551/number-of-trees-in-random-forest-regression</link>
      <description><![CDATA[我正在学习随机的森林回归模型。我知道它形成了许多树（模型），然后我们可以通过平均所有树的结果来预测目标变量。我也对决策树回归算法有一个下降的理解。我们如何形成最佳树木数量？
例如，我有一个数据集，我可以在其中预测人的薪水，而我只有两个输入变量是“年度经验”，“绩效得分”，那么我可以使用这样的数据集形成多少个随机树？随机林木是否取决于输入变量的数量？任何好例子都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/56505551/number-of-trees-in-random-forest-regression</guid>
      <pubDate>Sat, 08 Jun 2019 10:33:36 GMT</pubDate>
    </item>
    <item>
      <title>r的回归森林中的特征选择和预测准确性</title>
      <link>https://stackoverflow.com/questions/45935917/feature-selection-and-prediction-accuracy-in-regression-forest-in-r</link>
      <description><![CDATA[我正在尝试解决一个回归问题，其中输入功能集的大小〜54。
使用单个预测指标“ x1”使用OLS线性回归，我无法解释y-的变化 - 因此，我试图使用回归森林（即随机森林回归）找到其他重要特征。后来发现选定的“ X1”是最重要的功能。
我的数据集有〜14500个条目。我将其分为训练和测试集，比率为9：1。
我有以下问题：

 试图找到重要功能时，我应该在整个数据集上或仅在培训数据上运行回归森林吗？

 找到重要的功能后，是否应该使用顶级功能重新构建模型，以查看功能选择是否以较小的预测能力成本加速计算？

 目前，我已经使用训练集和所有功能构建了模型，并且我将其用于预测测试集。我正在计算训练集中的MSE和R平方。我在培训数据上获得了高MSE和低R2，并反向测试数据（如下所示）。这是不寻常的吗？


 ＆gt;森林＆lt;  -  rancomalforest（fmla，dtraining，ntree = 501，重要性= t）
    
     ＆gt;平均值（（dtraining $ y-预测（森林，data = dtraining））^2）
    
     ＆gt; ＆gt; 0.9371891
    
     ＆gt; rsquared（dtraining $ y，dtraining $ y-预测（森林，data = dtraining））

     ＆gt; ＆gt; 0.7431078
    
     ＆gt;平均值（（dtest $ y-预测（森林，newdata = dtest））^2）

     ＆gt; ＆gt; 0.009771256

     ＆gt; rsquared（dtest $ y，dtest $ y-预测（森林，newdata = dtest））
    
     ＆gt; ＆gt; 0.9950448
 
是否有R-Squared和MSE是此问题的好指标，或者我需要查看其他一些指标来评估该模型是否良好？]]></description>
      <guid>https://stackoverflow.com/questions/45935917/feature-selection-and-prediction-accuracy-in-regression-forest-in-r</guid>
      <pubDate>Tue, 29 Aug 2017 09:51:00 GMT</pubDate>
    </item>
    <item>
      <title>Python中的聚类时间序列数据</title>
      <link>https://stackoverflow.com/questions/45604143/clustering-time-series-data-in-python</link>
      <description><![CDATA[我正在尝试使用不同的聚类技术将Python聚集时间序列数据。 K-均值没有很好的效果。以下图像是使用聚集聚类聚类后的群集后的图像。我还尝试了动态的时间扭曲。这两个似乎给出了类似的结果。 
我理想地拥有第二张图像中时间序列的两个不同的簇。第一个图像是用于快速增加的群集。第二种不像稳定的稳定，第三个是降低趋势的集群。我想知道哪个时间序列既稳定又流行（在这里流行，我的意思是高度计数）。我尝试了分层聚类，但结果显示了太多的层次结构，我不确定如何选择层次结构。有人可以阐明如何将第二张图像中的时间序列分为两个不同的群集，一个群集计数低，另一个群体计数很高？有可能吗？或者我应该在视觉上选择一个阈值将它们切成两个？
快速增加的群集：
    
稳定计数的集群：
    
趋势下降的集群：
    
这是非常非常模糊的，但这是我分层聚类的结果。 
   
我知道这个特殊的图像根本没有用，但这对我来说也是一个死胡同。 
一般而言，如果您想区分趋势，例如为YouTube视频说，如何只为“流行趋势”部分挑选一些，而其他一些则是“本周趋势”部分？我了解“趋势”部分视频是显示与第一张图像相似的特征的视频。 “本周趋势”部分有一系列视频，这些视频的观点计数很高，但在计数方面保持稳定（即没有显示快速增加）。我知道，在YouTube的情况下，除了观看计数外，还有许多其他因素。有了第二张图像，我要做的事情类似于“本周趋势”部分。我想挑选那些很高的计数。在这种情况下，如何拆分时间序列？ 
我知道DTW捕获了趋势。 DTW给出了与上图相同的结果。它已经确定了第二张图像中的趋势，即“稳定”。但这并未在此处捕获“计数”元素。我希望趋势和计数被捕获，在这种情况下，稳定且计数很高。
上面的图像是根据计数聚类的时间序列。我是否错过了其他可以实现这一目标的聚类技术？即使仅计数，我如何根据我的需求不同？
任何想法都将不胜感激。预先感谢！]]></description>
      <guid>https://stackoverflow.com/questions/45604143/clustering-time-series-data-in-python</guid>
      <pubDate>Thu, 10 Aug 2017 03:51:00 GMT</pubDate>
    </item>
    <item>
      <title>Python监督机器学习</title>
      <link>https://stackoverflow.com/questions/35438540/python-supervised-machine-learning</link>
      <description><![CDATA[我试图了解如何使用 scikit 进行监督机器学习，因此我已经编造了一些属于两个不同集的数据：集合A和集合B。我有18个元素集合中的18个元素。每个元素都有三个变量。请参阅下文：
  #seta
变量1a = [3,4,4,5,4,5,5,6,7,7,5,4,5,6,4,9,3,4]
变量2A = [5,4,4,4,3,4,5,4,5,4,3,4,5,3,4,3,4,4,3]
变量3A = [7,8,4,5,6,7,3,3,3,4,4,9,7,6,8,6,7,8]


#setB
变量1b = [7,8,11,12,7,9,8,7,8,11,15,9,7,6,9,7,11]]
变量2b = [1,2,3,3,4,4,2,4,1,0,1,2,1,3,4,3,1,2,3]
变量3b = [12,18,14,15,16,17,13,13,13,14,14,14,19,17,16,18,16,17,18]
 
我将如何使用
对数据集很小的道歉，并且“构成”。我只想在其他数据集上使用Scikit应用相同的方法。]]></description>
      <guid>https://stackoverflow.com/questions/35438540/python-supervised-machine-learning</guid>
      <pubDate>Tue, 16 Feb 2016 16:59:48 GMT</pubDate>
    </item>
    </channel>
</rss>