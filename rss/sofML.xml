<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 23 Jan 2025 15:17:28 GMT</lastBuildDate>
    <item>
      <title>TensorFlow Lite：Android Studio 中的 java.lang.AssertionError：“不支持数据类型 INT32”</title>
      <link>https://stackoverflow.com/questions/79380176/tensorflow-lite-java-lang-assertionerror-does-not-support-data-type-int32-in</link>
      <description><![CDATA[我正在 Android Studio 中开发 Android 应用程序并使用 TensorFlow Lite 模型。运行应用程序时，我遇到以下错误：
java.lang.AssertionError：TensorFlow Lite 不支持数据类型 INT32
以下是我的代码的相关部分：
// 准备输入张量
val inputFeature0 = TensorBuffer.createFixedSize(inputShape, DataType.FLOAT32)
inputFeature0.loadArray(flatArray)

// 运行推理
val output = model?.process(inputFeature0)
val rawOutputBuffer = output?.outputFeature0AsTensorBuffer

// 根据数据类型将原始数据提取为 IntArray 或 FloatArray
val outputArray = when (rawOutputBuffer?.dataType) {
DataType.INT32 -&gt; rawOutputBuffer.intArray // 直接访问 INT32 数据
DataType.FLOAT32 -&gt; rawOutputBuffer.floatArray.map { it.toInt() }.toIntArray() // 将 FloatArray 转换为 IntArray
else -&gt;抛出 IllegalArgumentException(&quot;不支持的输出张量数据类型：${rawOutputBuffer?.dataType}&quot;)
}


输入张量的类型为 FLOAT32，并且使用 TensorBuffer.createFixedSize() 和 loadArray() 正确加载输入数据。

在处理模型的输出张量 (outputFeature0AsTensorBuffer) 时，我添加了检查以处理 FLOAT32 和 INT32 输出。

尽管如此，应用程序崩溃并显示错误，表明 TensorFlow Lite 不支持 INT32。


我有什么尝试：
确保输入张量使用 FLOAT32。
验证 TensorFlow Lite 模型是否与 FLOAT32 数据类型兼容。
检查输出张量数据类型并添加对 FLOAT32 和 INT32 的处理。
我的预期：
我预计模型推理可以顺利运行，因为我已经处理了 FLOAT32 和 INT32 输出情况。
问题：
为什么即使输入张量使用 FLOAT32，TensorFlow Lite 也会抛出此错误？
该错误是否与 TensorFlow Lite 内部管理张量维度或元数据等数据类型的方式有关？
如何解决此错误并确保 TensorFlow Lite 成功运行推理？]]></description>
      <guid>https://stackoverflow.com/questions/79380176/tensorflow-lite-java-lang-assertionerror-does-not-support-data-type-int32-in</guid>
      <pubDate>Thu, 23 Jan 2025 07:30:52 GMT</pubDate>
    </item>
    <item>
      <title>如何在固定的BBOX中将YOLOv8model与Deepsort连接起来？</title>
      <link>https://stackoverflow.com/questions/79378146/how-can-i-connect-yolov8model-with-deepsort-in-a-fixed-bbox</link>
      <description><![CDATA[我正在生成一个可以检测摩托车和汽车的模型来提取各自的信息。
但在将 YOLOv8 模型（这是我自定义的模型）与 Deepsort 算法连接的过程中，我发现了几个问题。

起初，自定义模型（YOLOv8）可以检测到每辆车，提取的视频显示完美的边界框
与 Deepsort 连接后，它漏掉了几辆车，提取的视频有错误的边界框（它们太大，不适合每辆车）
我在 YOLOv8 2 Deepsort 之间找不到错误的结果。

请帮帮我
import cv2
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort

# 初始化 YOLO 模型
model_path = &quot;/content/drive/MyDrive/Capstone/best_motorcycle_detector_NIGHT8.pt&quot;
model = YOLO(model_path)
model.to(&#39;cuda&#39;) # 使用 GPU

# 初始化 DeepSORT
tracker = DeepSort(max_age=200, n_init=1, nn_budget=200)

# 帮助程序将 YOLO 结果转换为 DeepSORT 格式
def yolo_to_deepsort(yolo_results, target_classes):
detections = []
for det in yolo_results[0].boxes:
x1, y1, x2, y2 = map(float, det.xyxy[0].cpu().numpy())
confidence = float(det.conf.cpu().numpy().item())
class_id = int(det.cls.cpu().numpy())
if class_id in target_classes:
detections.append([(x1, y1, x2, y2),置信度])
返回检测

# 主处理循环
video_path = &quot;/content/drive/MyDrive/Capstone/11.15 1200-1400/1320-1400.mp4&quot;
cap = cv2.VideoCapture(video_path)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

output_path = &quot;/content/drive/MyDrive/Capstone/Results/processed_video.avi&quot;
video_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*&#39;MJPG&#39;), fps, (frame_width, frame_height))

target_classes = [2, 3] # 汽车 (2)、摩托车 (3)

while cap.isOpened():
ret, frame = cap.read()
if not ret:
break

# 运行 YOLO 模型
results = model(frame, conf=0.3)

# 将 YOLO 结果转换为 DeepSORT 格式
detections = yolo_to_deepsort(results, target_classes)

# 更新跟踪器
tracks = tracker.update_tracks(detections, frame=frame)

# 绘制边界框
for track in tracks:
if not track.is_confirmed():
continue
x1, y1, x2, y2 = map(int, track.to_tlbr())
track_id = track.track_id
label = f&quot;ID {track_id}&quot;
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# 保存帧
video_writer.write(frame)

cap.release()
video_writer.release()


找出 YOLO 中的协调性
Deepsort 的输入和输出
与其他算法结合，但 Deepsort 更适合我的视频
]]></description>
      <guid>https://stackoverflow.com/questions/79378146/how-can-i-connect-yolov8model-with-deepsort-in-a-fixed-bbox</guid>
      <pubDate>Wed, 22 Jan 2025 14:30:52 GMT</pubDate>
    </item>
    <item>
      <title>对于 pytorch 模型 (Yolov3) 来说，“前向/后向通道大小”太大</title>
      <link>https://stackoverflow.com/questions/79378022/the-forward-backward-passage-size-is-too-large-for-the-pytorch-model-yolov3</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79378022/the-forward-backward-passage-size-is-too-large-for-the-pytorch-model-yolov3</guid>
      <pubDate>Wed, 22 Jan 2025 14:07:39 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试使用 GNN-LSTM 预测尼日利亚某州未来确诊的脑膜炎病例数</title>
      <link>https://stackoverflow.com/questions/79377500/i-am-trying-to-predict-the-future-number-of-confirmed-cases-of-meningitis-for-a</link>
      <description><![CDATA[以下是代码：
 # 初始化模型
# 初始化模型
model = GNN_LSTM(input_dim=window_size, gcn_hidden_​​dim=16, lstm_hidden_​​dim=32, predict_steps=20)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.MSELoss()
print(f&quot;Shape of time_series_features: {time_series_features.shape}&quot;)

# 训练循环
for epoch in range(200):
model.train()
optimizer.zero_grad()

# 前向传递
out = model(data, time_series_features) # 无需解压，应符合模型输入预期
zamfara_predictions = out[target_idx] # 提取Zamfara 的预测

# 使用 train_mask 计算损失
adapted_train_mask = train_mask[-predict_steps:]

# 打印形状以供调试
print(f&quot;Shape of zamfara_predictions: {zamfara_predictions.shape}&quot;)
print(f&quot;Shape of zamfara_features[-predict_steps:]: {zamfara_features[-predict_steps:].shape}&quot;)
print(f&quot;Shape of adapted_train_mask: {adjusted_train_mask.shape}&quot;)

# 使用调整后的掩码计算损失
loss = criterion(
zamfara_predictions.squeeze()[adjusted_train_mask],
zamfara_features[-predict_steps:][adjusted_train_mask]
)

loss.backward()
optimizer.step()

# 验证
if (epoch + 1) % 20 == 0:
model.eval()
with torch.no_grad():
print(f&quot;zamfara_predictions (squeezed) 的形状：{zamfara_predictions.squeeze().shape}&quot;)
print(f&quot;zamfara_features 的形状：{zamfara_features.shape}&quot;)
print(f&quot;val_mask 的形状：{val_mask.shape}，类型：{val_mask.dtype}&quot;)
print(f&quot;test_mask 的形状：{test_mask.shape}，类型：{test_mask.dtype}&quot;)

# 验证损失计算
val_loss = criterion(
zamfara_predictions.squeeze()[val_mask],
zamfara_features[val_mask]
)
print(f&quot;Epoch {epoch+1}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}&quot;)

# 测试
model.eval()
with torch.no_grad():
test_loss = criterion(zamfara_predictions.squeeze()[test_mask], zamfara_features[test_mask])
print(f&quot;Test Loss: {test_loss.item()}&quot;)

# 指标计算
true_values = zamfara_features[test_mask].numpy()
predictions = zamfara_predictions.squeeze()[test_mask].numpy()

mse = mean_squared_error(true_values, predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(true_values, predictions)
print(f&quot;Test MSE: {mse:.4f}&quot;)
print(f&quot;Test RMSE: {rmse:.4f}&quot;)
print(f&quot;Test MAE: {mae:.4f}&quot;)

# Zamfara 的预测案例

我正在使用 Google Colab 编译该程序。编译后，代码中突出显示了此行：loss = criterion( zamfara_predict，并出现以下错误：
TypeError：&#39;int&#39; 对象不可调用。

代码由 meningitis_​​cases_graph.graphml 组成，它是尼日利亚 37 个州确诊脑膜炎病例的图表表示，edges.csv 是尼日利亚这些州的形状文件。我们想要预测尼日利亚某个州“赞法拉”未来 20 天的确诊病例数。如何解决此错误？]]></description>
      <guid>https://stackoverflow.com/questions/79377500/i-am-trying-to-predict-the-future-number-of-confirmed-cases-of-meningitis-for-a</guid>
      <pubDate>Wed, 22 Jan 2025 11:15:34 GMT</pubDate>
    </item>
    <item>
      <title>MATLAB 神经网络预测收敛到 1</title>
      <link>https://stackoverflow.com/questions/79376474/matlab-neural-network-predictions-converging-to-1</link>
      <description><![CDATA[我正在尝试训练神经网络但遇到了障碍。我有一个简化版的我面临的问题：
设置：
NN = trainnet(X_data, Y_data, NN, &#39;crossentropy&#39;, options);

X_data 大小为 25455x3，其中 3 列中的每一列都经过了归一化（平均值 = 0 和标准差 = 1）
Y_data 大小为 25455x1，值为 1 或 0（二元分类）（sum(Y_data)= 11541(~45%))
NN 是从以下代码中新生成的 dlnetwork：

 featureInputLayer(3, &quot;Name&quot;, &quot;InputLayer&quot;)
fullyConnectedLayer(32, &quot;Name&quot;, &quot;HiddenLayer1&quot;, &quot;WeightsInitializer&quot;, &quot;he&quot;) 
reluLayer(&quot;Name&quot;, &quot;ReLU&quot;)
dropoutLayer(0.2, &quot;Name&quot;, &quot;Dropout&quot;) 
fullyConnectedLayer(1, &quot;Name&quot;, &quot;OutputLayer&quot;, &quot;WeightsInitializer&quot;, &quot;glorot&quot;)
sigmoidLayer(&quot;Name&quot;, &quot;SigmoidOutput&quot;) 
];
NN = dlnetwork(NN);


选项：

选项 = trainingOptions(&quot;adam&quot;, ...
LearnRateSchedule = &quot;piecewise&quot;, ...
LearnRateDropFactor = 0.2, ...
LearnRateDropPeriod = 5, ...
MaxEpochs = 1, ... 
MiniBatchSize = 128, ...
ExecutionEnvironment = &quot;cpu&quot;, ...
Plots = &quot;none&quot;);

问题：
当我运行单行 NN = trainnet(X_data, Y_data, NN, &#39;crossentropy&#39;, options); 时，模型运行 194 次迭代，最终 trainingloss 为 0.044545，但模型在类似测试数据上的测试准确率仅为 ~45%，但更令人担忧的是，使用类似数据生成的预测非常偏向 1。事实上，最低预测是 0.6876，平均值是 0.9030。这是一个巨大的飞跃，对我来说毫无意义。我希望模型的预测保持大约 0.5 的正常值（暂时不考虑任何学习）或者可能稍微少一点以匹配略低的 1 个标签数量。为什么会发生这种情况，我该如何解决？
注意：我真正做的是运行一个循环，该循环运行我上面提到的代码行。我描述的问题只是循环的第一次迭代；随着循环的继续，偏差变得更加极端，直到所有预测都只有 1.00000。每次循环中的数据都不同，但它非常相似，并且其中存在应该可以学习的趋势（从其他类型的回归建模中发现）。]]></description>
      <guid>https://stackoverflow.com/questions/79376474/matlab-neural-network-predictions-converging-to-1</guid>
      <pubDate>Wed, 22 Jan 2025 03:36:04 GMT</pubDate>
    </item>
    <item>
      <title>解释 TensorFlow 决策森林中的变量重要性方法</title>
      <link>https://stackoverflow.com/questions/79376427/explaining-variable-importance-methods-in-tensorflow-decision-forests</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79376427/explaining-variable-importance-methods-in-tensorflow-decision-forests</guid>
      <pubDate>Wed, 22 Jan 2025 03:01:46 GMT</pubDate>
    </item>
    <item>
      <title>当我使用 knn 时收到错误：-215：断言失败）test_samples.type() == CV_32F && test_samples.cols == samples.cols 在函数“findNearest”中</title>
      <link>https://stackoverflow.com/questions/79375305/receiving-error-when-i-use-knn-215assertion-failed-test-samples-type-cv</link>
      <description><![CDATA[我正在开展一个项目，通过 OCR 从图像中读取手写数字。我使用这个 OpenCV 源代码来尝试它是否有效：https://docs.opencv.org/4.x/d8/d4b/tutorial_py_knn_opencv.html。
这是到目前为止有效的代码：
import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt

img = cv.imread(&#39;digits.png&#39;)
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)

# 现在我们将图像拆分为 5000 个单元格，每个单元格大小为 20x20
cells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]

# 将其转换为 Numpy 数组：其大小为 (50,100,20,20)
x = np.array(cells)

# 现在我们准备训练数据和测试数据
train = x[:,:50].reshape(-1,400).astype(np.float32) # Size = (2500,400)
test = x[:,50:100].reshape(-1,400).astype(np.float32) # Size = (2500,400)

# 为训练和测试数据创建标签
k = np.arange(10)
train_labels = np.repeat(k,250)[:,np.newaxis]
test_labels = train_labels.copy()

#启动 kNN，在训练数据上进行训练，然后使用 k=1 的测试数据进行测试
knn = cv.ml.KNearest_create()
knn.train(train, cv.ml.ROW_SAMPLE, train_labels)
ret,result,neighbours,dist = knn.findNearest(test,k=5)

# 现在我们检查分类的准确性
# 为此，将结果与 test_labels 进行比较，并检查哪些是错误的
matches = result==test_labels
correct = np.count_nonzero(matches)
accuracy = correct*100.0/result.size
print( accuracy )

# 保存数据
np.savez(&#39;knn_data.npz&#39;,train=train, train_labels=train_labels)

# 现在保存 kin_data.npz 后加载数据
使用np.load(&#39;knn_data.npz&#39;) 作为数据：
print( data.files )
train = data[&#39;train&#39;]
train_labels = data[&#39;train_labels&#39;]

现在让我们继续尝试手写数字：
在此处输入图像描述
但是我收到一条错误消息：错误：OpenCV(4.10.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/ml/src/knearest.cpp:313: 错误：(-215：断言失败) test_samples.type() == CV_32F &amp;&amp; test_samples.cols == samples.cols 在函数“findNearest”中
此消息的原因是什么？我该如何修复它？]]></description>
      <guid>https://stackoverflow.com/questions/79375305/receiving-error-when-i-use-knn-215assertion-failed-test-samples-type-cv</guid>
      <pubDate>Tue, 21 Jan 2025 17:12:30 GMT</pubDate>
    </item>
    <item>
      <title>训练 Hugging Face Transformer 期间 GPU 利用率几乎始终为 0</title>
      <link>https://stackoverflow.com/questions/79375287/gpu-utilization-almost-always-0-during-training-hugging-face-transformer</link>
      <description><![CDATA[我正在使用我的发票数据对 Donut Cord-v2 模型进行微调，该发票数据在预处理并作为数据集保存在磁盘上时大小约为 360 GB。我几乎完全按照这个笔记本进行操作，只是我有 6 个训练周期而不是 3 个。
我在单个 Nvidia H100 SXM GPU / Intel Xeon® Gold 6448Y / 128 GB RAM 上进行训练。
每当我开始训练并使用 htop 和 nvidia-smi 检查 CPU 和 GPU 利用率时，我都会看到 CPU 的利用率为 10-12%，由 python 使用，GPU 内存几乎一直被占用 90%，但 GPU 利用率几乎始终为 0。如果我不断刷新 nvidia-smi 的输出，则每 10-12 秒一次，利用率将跳转到100% 然后立即回到 0。我不禁感觉到我的 CPU 和 GPU 之间存在瓶颈，CPU 尝试不断处理数据并将其发送到 GPU，GPU 处理速度非常快，并且只是闲置，等待来自 CPU 的下一批。我从磁盘加载已经预处理的数据集，如下所示：
from datasets import load_from_disk
processed_dataset = load_from_disk(r&quot;/dataset/dataset_final&quot;)

我的处理器配置如下：
from transformers import DonutProcessor

new_special_tokens = [] # 将添加到 tokenizer 的新 token
task_start_token = &quot;&lt;s&gt;&quot; # 任务 token 的启动
eos_token = &quot;&lt;/s&gt;&quot; # tokenizer 的 eos token

processor = DonutProcessor.from_pretrained(&quot;naver-clova-ix/donut-base-finetuned-cord-v2&quot;)

# 向 tokenizer 添加新的特殊 token
processor.tokenizer.add_special_tokens({&quot;additional_special_tokens&quot;: new_special_tokens + [task_start_token] + [eos_token]})

# 我们更新了一些与预训练不同的设置；即图像的大小 + 无需旋转
processor.feature_extractor.size = [1200,1553] # 应为 (宽度, 高度)
processor.feature_extractor.do_align_long_axis = False

我的模型配置是：
import torch
from transformers import VisionEncoderDecoderModel, VisionEncoderDecoderConfig

#print(torch.cuda.is_available())

# 从 huggingface.co 加载模型
model = VisionEncoderDecoderModel.from_pretrained(&quot;naver-clova-ix/donut-base-finetuned-cord-v2&quot;)

# 调整嵌入层的大小以匹配词汇表大小
new_emb = model.decoder.resize_token_embeddings(len(processor.tokenizer))
print(f&quot;新嵌入大小： {new_emb}&quot;)
# 调整我们的图像大小和输出序列长度
model.config.encoder.image_size = process.feature_extractor.size[::-1] # (height, width)
model.config.decoder.max_length = len(max(processed_dataset[&quot;train&quot;][&quot;labels&quot;], key=len))

# 添加解码器启动的任务令牌
model.config.pad_token_id = process.tokenizer.pad_token_id
model.config.decoder_start_token_id = process.tokenizer.convert_tokens_to_ids([&#39;&lt;s&gt;&#39;])[0]

我的训练代码是：
import gc
gc.collect()

torch.cuda.empty_cache()

from transformers import Seq2SeqTrainingArguments，Seq2SeqTrainer

导入日志记录
logging.basicConfig(level=logging.INFO)

# 训练参数
training_args = Seq2SeqTrainingArguments(
output_dir=r&quot;/trained&quot;, # 指定本地目录保存模型
num_train_epochs=6,
learning_rate=2e-5,
per_device_train_batch_size=8,
weight_decay=0.01,
fp16=True,
logs_steps=50,
save_total_limit=2,
evaluation_strategy=&quot;no&quot;,
save_strategy=&quot;epoch&quot;,
predict_with_generate=True,
report_to=&quot;none&quot;,
# 禁用推送到集线器
push_to_hub=False

)

# 创建训练器
trainer = Seq2SeqTrainer(
model=model,
args=training_args,
train_dataset=processed_dataset[&quot;train&quot;],
)

# 开始训练
trainer.train()

使用 360 GB 数据集完成 6 个 epoch 的训练预计需要 54 小时。当我在装有 Intel i9 11900KF / RTX 3050 的 PC 上运行完全相同的代码时，我发现 GPU 利用率一直保持在 100%。我的代码中是否存在瓶颈？为什么 CPU 会继续处理已经预处理的数据集？ Cuda 12.6
编辑：
由于我的 RAM 和 CPU 核心数量允许，将 Seq2SeqTrainer 的 dataloader_num_workers 参数更改为 &gt;0 值是否有意义？（并且 CPU 利用率最高为 10-12%）]]></description>
      <guid>https://stackoverflow.com/questions/79375287/gpu-utilization-almost-always-0-during-training-hugging-face-transformer</guid>
      <pubDate>Tue, 21 Jan 2025 17:09:03 GMT</pubDate>
    </item>
    <item>
      <title>使用 nb 方法进行交叉验证</title>
      <link>https://stackoverflow.com/questions/79338629/cross-validation-with-nb-method</link>
      <description><![CDATA[我尝试在 WESBROOK 数据集上使用 k 倍交叉验证。它使用 caret 包中的 train 函数来执行此操作。到目前为止，此函数已与 svm、knn 和 rpart 等方法配合使用，但使用 nb（朴素贝叶斯）方法时，我收到以下错误：
错误 { : 
任务 1 失败 - “未在 newdata 中找到对象中使用的所有变量名称”

我的 train 函数如下所示：
k_folds &lt;- 5
train_control &lt;- trainControl(method = &quot;cv&quot;, number = k_folds, classProbs = TRUE, summaryFunction = twoClassSummary)

nb_model &lt;- train(
TOTLGIVE ~ ., data = train_data,
method = &quot;nb&quot;,
trControl = train_control
)

我检查了一下，没有缺失数据，训练集和测试集中的列名及其类型相同。]]></description>
      <guid>https://stackoverflow.com/questions/79338629/cross-validation-with-nb-method</guid>
      <pubDate>Wed, 08 Jan 2025 09:50:09 GMT</pubDate>
    </item>
    <item>
      <title>如何在处理 EOS 代币时计算拥抱人脸模型的教师强制准确度 (TFA)？</title>
      <link>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</guid>
      <pubDate>Thu, 21 Nov 2024 00:25:48 GMT</pubDate>
    </item>
    <item>
      <title>我正在进行 Yolov8 模型训练，但准确率只有 70% [关闭]</title>
      <link>https://stackoverflow.com/questions/78919106/im-doing-yolov8-model-training-but-the-accuracy-rate-is-70</link>
      <description><![CDATA[我目前正在为我的项目训练 YOLOv8 模型。目标是训练该模型从发送给聊天机器人的照片中识别产品的股票代码。
我有 1,522 个股票代码，每个股票代码大约有 10-15 张照片。虽然照片数量有点少，但我们的客户通常会向我们发送训练中使用的相同照片。例如，他们可能会从我们的 Instagram 个人资料中截取屏幕截图并将其发送给我们。
我正在训练模型，但它的准确率只有 70% 左右。我相信问题可能与超参数有关，但我对它们不太熟悉。我非常感谢您提供的任何建议或建议。
hyp.yaml
# 学习率和动量 Ayarları
lr0: 0.01 # 关闭状态
lrf: 0.01 # Final öğrenme oranı (lr0 ile çarpılır)
动量：0.9#SGD动量
Weight_decay: 0.0005 # L2 正则化（权重衰减）
Warmup_epochs: 2.0 # Isınma epoch sayısı
Warmup_momentum: 0.8 # Isınma süresince başlangıç Momentumu
Warmup_bias_lr: 0.1 # 是否存在偏差
#Kayip Fonksiyonu （损失函数）Ayarları
box: 0.05 # Box kaybı kazancı (GIoU/DIoU/CIoU)
cls: 0.5 # Sınıf kaybı kzancı
iou: 0.2 # IoU eşiği (标签 için)
kobj: 1.0 # 不可以
# 增强 Ayarları (Veri artırma)
hsv_h: 0.005 # Görüntü HSV-Hue artırma（分数）- Çok küçük değişiklikler
hsv_s: 0.1 # Görüntü HSV-Saturation artırma（分数） - Çok küçük değişiklikler
hsv_v: 0.1 # Görüntü HSV-Value artırma（分数） - Çok küçük değişiklikler
度数：0.0 # Görüntü döndürme (+/- derece)
翻译：0.1 # Görüntü kaydırma (+/- 分数)a
比例：0.5 # Görüntü ölçekleme (+/- kazanç)
剪切力：0.0 # Görüntü kaydırma (+/- derece)
透视图：0.0 # Görüntü perspektifi（+/- 分数），0-0.001 arası
Flipud: 0.0 # Görüntüyü yukarıdan aşağıya çevirme (olasılık)
Fliplr: 0.5 # Görüntüyü sağdan sola çevirme （奥拉斯利克）
马赛克: 0.0 # 马赛克艺术 (olasılık) - Bu durumda kapalı
mixup: 0.0 # Mixup artırma (olasılık) - Bu durumda kapalı
copy_paste: 0.0 # 复制粘贴艺术 (olasılık) - Bu durumda kapalı

火车.py =
从 ultralytics 导入 YOLO
导入万数据库
从 wandb.integration.ultralytics 导入 add_wandb_callback
# WandB oturumunu başlatın
如果 __name__ == “__main__”：
    wandb.login()
    wandb.init(project=&quot;ultralytics&quot;, job_type=&quot;training&quot;)
    # 模型尤克莱因
    模型 = YOLO(&#39;yolov8n.pt&#39;)
    # WandB 回调&#39;ini ekleyin
    add_wandb_callback（模型，enable_model_checkpointing = True）
    # 模型 eğitin
    模型.火车(
        data=&#39;y.yaml&#39;, # Veri kümesi yapılandırma dosyası
        epochs=100, # Eğitim epoch sayısı
        batch=16, # 批量博语图
        project=&#39;my_project&#39;, # Proje adı (varsayılan: 运行/训练)
        name=&#39;exp&#39;, # 名称 (varsayılan: exp)
        cfg=&#39;hyp.yaml&#39; # 超参数 ayarları
    ）
    #WandB oturumunu sonlandırın
    wandb.finish()
]]></description>
      <guid>https://stackoverflow.com/questions/78919106/im-doing-yolov8-model-training-but-the-accuracy-rate-is-70</guid>
      <pubDate>Tue, 27 Aug 2024 13:16:26 GMT</pubDate>
    </item>
    <item>
      <title>如何暂时禁用 MLFlow？</title>
      <link>https://stackoverflow.com/questions/61088651/how-to-disable-mlflow-temporarily</link>
      <description><![CDATA[是否可以暂时禁用 MLFlow 以调试代码或添加新功能？如果不禁用，它会保存大量实际上无用或未完成的执行。
或者最好的策略是使用不调用 mlflow.start_run() 的类似代码？]]></description>
      <guid>https://stackoverflow.com/questions/61088651/how-to-disable-mlflow-temporarily</guid>
      <pubDate>Tue, 07 Apr 2020 20:14:15 GMT</pubDate>
    </item>
    <item>
      <title>加载 pickle NotFittedError：TfidfVectorizer - 词汇表不适合</title>
      <link>https://stackoverflow.com/questions/57213165/loading-pickle-notfittederror-tfidfvectorizer-vocabulary-wasnt-fitted</link>
      <description><![CDATA[多标签分类
我正在尝试使用 scikit-learn/pandas/OneVsRestClassifier/logistic 回归预测多标签分类。构建和评估模型有效，但尝试对新样本文本进行分类无效。
场景 1：
一旦我构建了一个模型，就使用名称 (sample.pkl) 保存该模型并重新启动我的内核，但是当我在对样本文本进行预测期间加载已保存的模型 (sample.pkl) 时，它给出了错误：
 NotFittedError：TfidfVectorizer - 词汇表不适合。

我构建了模型并评估了模型，然后我使用名称 sample.pkl 保存了该模型。我重启了内核，然后加载模型对样本文本进行预测 NotFittedError: TfidfVectorizer - 词汇表不适合
推理
import pickle,os
import collections
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import Counter
from nltk.corpus import stopwords
import json, nltk, re, csv, pickle
from sklearn.metrics import f1_score # 性能矩阵
from sklearn.multiclass import OneVsRestClassifier # 二元相关性
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split 
from sklearn.feature_extraction.text import TfidfVectorizer 
来自 sklearn.feature_extraction.text 导入 CountVectorizer
来自 sklearn.preprocessing 导入 MultiLabelBinarizer
来自 sklearn.model_selection 导入 train_test_split
来自 sklearn.linear_model 导入 LogisticRegression
stop_words = set(stopwords.words(&#39;english&#39;))

def cleanHtml(sentence):
&#39;&#39;&#39;&#39; 删除标签 &#39;&#39;&#39;
cleanr = re.compile(&#39;&lt;.*?&gt;&#39;)
cleantext = re.sub(cleanr, &#39; &#39;, str(sentence))
return cleantext

def cleanPunc(sentence): 
&#39;&#39;&#39; 函数用于清除单词中的任何标点符号或特殊字符 &#39;&#39;&#39;
cleaned = re.sub(r&#39;[?|!|\&#39;|&quot;|#]&#39;,r&#39;&#39;,sentence)
cleaned = re.sub(r&#39;[.|,|)|(|\|/]&#39;,r&#39; &#39;,cleaned)
cleaned = cleaned.strip()
cleaned = cleaned.replace(&quot;\n&quot;,&quot; &quot;)
return cleaned

def keepAlpha(sentence):
&quot;&quot;&quot; 保留 alpha 句子 &quot;&quot;&quot;
alpha_sent = &quot;&quot;
for word in sentence.split():
alpha_word = re.sub(&#39;[^a-z A-Z]+&#39;, &#39; &#39;, word)
alpha_sent += alpha_word
alpha_sent += &quot; &quot;
alpha_sent = alpha_sent.strip()
return alpha_sent

def remove_stopwords(text):
&quot;&quot;&quot; 删除停用词 &quot;&quot;&quot;
no_stopword_text = [w for w in text.split() if not w in stop_words]
return &#39; &#39;.join(no_stopword_text)

test1 = pd.read_csv(&quot;C:\\Users\\abc\\Downloads\\test1.csv&quot;)
test1.columns

test1.head()
siNo plot movie_name gender_new
1 故事以 Hannah 开始... 唱歌 [戏剧,青少年]
2 Debbie 最喜欢的乐队是 Dream... 最忠实的粉丝 [戏剧]
3 这个祖鲁家庭的故事是... 回来，非洲 [戏剧,纪录片]

出现错误
当我对示例文本进行推理时，我在这里出现错误
def infer_tags(q):
q = cleanHtml(q)
q = cleanPunc(q)
q = keepAlpha(q)
q = remove_stopwords(q)
multilabel_binarizer = MultiLabelBinarizer()
tfidf_vectorizer = TfidfVectorizer()
q_vec = tfidf_vectorizer.transform([q])
q_pred = clf.predict(q_vec)
return multilabel_binarizer.inverse_transform(q_pred)

for i in range(5):
print(i)
k = test1.sample(1).index[0] 
print(&quot;电影：&quot;, test1[&#39;movie_name&#39;][k], &quot;\n预测类型：&quot;, infer_tags(test1[&#39;plot&#39;][k])), print(&quot;实际类型：&quot;,test1[&#39;genre_new&#39;][k], &quot;\n&quot;)


已解决
我解决了将 tfidf 和 multibiniraze 保存到 pickle 模型中的问题
从 sklearn.externals 导入 joblib
pickle.dump(tfidf_vectorizer, open(&quot;tfidf_vectorizer.pickle&quot;, &quot;wb&quot;))
pickle.dump(multilabel_binarizer, open(&quot;multibinirizer_vectorizer.pickle&quot;, &quot;wb&quot;))
vectorizer = joblib.load(&#39;/abc/downloads/tfidf_vectorizer.pickle&#39;)
multilabel_binarizer = joblib.load(&#39;/abc/downloads/multibinirizer_vectorizer.pickle&#39;)

def infer_tags(q):
q = cleanHtml(q)
q = cleanPunc(q)
q = keepAlpha(q) 
q = remove_stopwords(q)
q_vec = vectorizer .transform([q])
q_pred = rf_model.predict(q_vec)
return multilabel_binarizer.inverse_transform(q_pred)

我通过以下链接找到了解决方案
,https://stackoverflow.com/questions/32764991/how-do-i-store-a-tfidfvectorizer-for-future-use-in-scikit-learn&gt;]]></description>
      <guid>https://stackoverflow.com/questions/57213165/loading-pickle-notfittederror-tfidfvectorizer-vocabulary-wasnt-fitted</guid>
      <pubDate>Fri, 26 Jul 2019 04:21:05 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow keras 序列模型 - 如何仅预测最后一步的输出</title>
      <link>https://stackoverflow.com/questions/53825156/tensorflow-keras-sequence-models-how-to-only-predict-output-of-last-step</link>
      <description><![CDATA[我正在使用 tf.keras 库开发一个序列模型。
假设我有 5 个时间步骤，每个时间步骤本质上都有一个输出。
每个时间步骤中的特征数量为 - 比如 - 4。我正在研究分类问题，输出可以是 0、1 或 2 之一。
例如，以下是一个训练示例。
步骤 1 输入：`[0, 5, 4, 5]` 和输出 = `0`
步骤 2 输入：`[1, 2, 2, 7]` 和输出 = `1`
步骤 3 输入：`[7, 5, 3, 4]` 和输出 = `0`
步骤 4 输入：`[4, 5, 1, 2]` 和输出 = `1`
步骤 5 输入：`[8, 5, 4, 5]` 和输出 = `2`

在训练我的模型时，我希望以这样的方式训练它：
在步骤 1 中，如果您的输入是 [0, 5, 4, 5]，则此时间步的输出为 0。

在步骤 2 中，如果您的输入是 [1, 2, 2, 7]，则此时间步的输出为 1。
以此类推....
但在后期制作中，我只希望我的模型能够估计最后一个时间步的输出。例如：
步骤 1 输入：`[0, 5, 4, 5]` 且输出 = `0`
步骤 2 输入：`[1, 2, 2, 7]` 且输出 = `1`
步骤 3 输入：`[7, 5, 3, 4]` 且输出 = `0`
步骤 4 输入：`[4, 5, 1, 2]` 且输出 = `1`
步骤 5 输入：`[8, 5, 4, 5]` 且输出 = **`?`**

基于此，在训练我的模型时，我对应该如何构建和训练我的模型有点困惑？由于我只对最后一步的输出感兴趣，但仍希望在训练阶段通过提供最后一步之前的先前步骤的输出来帮助我的模型，我想知道我应该如何构建模型？
如果我提供所有时间步的输出作为预期输入，据我所知，损失/成本是基于此计算的。例如，如果第 3 个时间步的输出计算错误，成本将增加。这可能是预料之中的，但对我来说重要的是最后一步的输出，我主要感兴趣的是在最后一步做出正确的预测。在这种情况下，如何使用 tf.keras 构建我的模型？
（或者，如果我仍然需要以某种方式训练我的模型，以便它尝试分别估计每个时间步的输出。最后，我仍然希望仅基于最后一步的输出来计算准确度。）]]></description>
      <guid>https://stackoverflow.com/questions/53825156/tensorflow-keras-sequence-models-how-to-only-predict-output-of-last-step</guid>
      <pubDate>Tue, 18 Dec 2018 01:24:53 GMT</pubDate>
    </item>
    <item>
      <title>KD树最近邻搜索如何工作？</title>
      <link>https://stackoverflow.com/questions/4418450/how-does-the-kd-tree-nearest-neighbor-search-work</link>
      <description><![CDATA[我正在查看 KD 树的 Wikipedia 页面。例如，我用 Python 实现了列出的构建 kd 树的算法。
但是，使用 KD 树进行 KNN 搜索的算法会切换语言，并且并不完全清楚。英语解释开始有意义，但其中的部分（例如他们“展开递归”以检查其他叶节点的区域）对我来说真的没有任何意义。
这是如何工作的，以及如何在 Python 中使用 KD 树进行 KNN 搜索？这并不是一个“给我发代码！”类型的问题，我也不指望这样。请简单解释一下 :)]]></description>
      <guid>https://stackoverflow.com/questions/4418450/how-does-the-kd-tree-nearest-neighbor-search-work</guid>
      <pubDate>Sat, 11 Dec 2010 19:14:25 GMT</pubDate>
    </item>
    </channel>
</rss>