<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 25 Jan 2025 18:20:16 GMT</lastBuildDate>
    <item>
      <title>NNUE 国际象棋教练</title>
      <link>https://stackoverflow.com/questions/79386852/nnue-chess-trainer</link>
      <description><![CDATA[我需要帮助完成我的 NNUE 项目，我想将其集成到我的国际象棋引擎 Jomfish 中。我已经开始了这个项目，但由于我对 nnue 还比较陌生，所以无法进一步推进。我用我的 repo 训练了我的第一个模型：https://github.com/github-jimjim/NNUE-Trainer，但结果非常糟糕，出现了完全错误的情况。我的问题是，我该如何做得更好，以便 NNUE 至少可以大致给出评分。
我在一个 csv 文件中收集了 10 000 个 FENS 及其评估。由于我的硬件不够强大，我无法处理更多的 FENS。之后，我使用 NNUE.py 脚本用 1000 个 epoch 训练我的模型。然后我运行了evaluate.py，结果很糟糕，输出应该是-5.9，但NNUE给了我+0.8。]]></description>
      <guid>https://stackoverflow.com/questions/79386852/nnue-chess-trainer</guid>
      <pubDate>Sat, 25 Jan 2025 14:01:27 GMT</pubDate>
    </item>
    <item>
      <title>训练NER模型时出现KeyError错误</title>
      <link>https://stackoverflow.com/questions/79386736/keyerror-message-when-trainineg-ner-model</link>
      <description><![CDATA[
我参加了一个 NLP 黑客马拉松来预测 Ner_tags，但我似乎无法使用数据框来训练模型，它说：KeyError 和
弹出 train_df 的索引 这是我目前在 Kaggle Notebook 上的所有代码 [[导入所有库][1]]-&gt; [[下载数据集（eval 和
测试数据集相同）][2]] -&gt; [ [转换为数据框][3]] -&gt; [ [添加
句子_id 并将列拆分为单词和 NER_tags][4]] -&gt; [
[使用 simplestransformer 配置参数][5]] -&gt; [ [DF 检查
eval_df 是否超出][6]] -&gt; [ [DF 检查 2][7]] -&gt; [ [模型
训练和 KeyError 消息][8]] -&gt; [1]:
https://i.sstatic.net/v81T9mro.png [2]:
https://i.sstatic.net/oV7wOSA4.png [3]:
https://i.sstatic.net/mdJdOJVD.png [4]:
https://i.sstatic.net/oFx6afA4.png [5]:
https://i.sstatic.net/7IyVQieK.png [6]:
https://i.sstatic.net/xFVfvTCi.png [7]:
https://i.sstatic.net/Ol6eeWo1.png [8]:
https://i.sstatic.net/gw6bzXhI.png 请帮忙，这次黑客马拉松将在 12 小时后结束
]]></description>
      <guid>https://stackoverflow.com/questions/79386736/keyerror-message-when-trainineg-ner-model</guid>
      <pubDate>Sat, 25 Jan 2025 12:44:39 GMT</pubDate>
    </item>
    <item>
      <title>y 只有 4 个或更少的值，而我的预处理 DataFrame 有超过 4,000 行。回归可能吗？</title>
      <link>https://stackoverflow.com/questions/79386587/y-has-only-four-values-or-less-while-my-preprocessed-dataframe-has-more-than-4</link>
      <description><![CDATA[我的数据集是根据首尔某个特定地区的单人家庭数量来预测该地区的美食数量。
问题：原始数据的行数超过 100,000。但是，我必须分析 2 个限制区域，每个区域有 2 个真实值。
我使用了无监督学习（KMeans Clustering）、引导程序，并故意给出噪音以适应模型。但似乎不起作用。有人能告诉我如何解决这个问题吗？
我的导师希望我建立一个回归模型。
但看起来这是不可能的，就像 chatGPT 说了 20 次一样。
代码（带有任意噪声的 y，每 2 个群集的 r2_score 为 97.8%，但被拒绝）：
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings(&quot;ignore&quot;)
\# 加载数据 rest_data_path = &#39;서울시 휴게음식점 인허가 정보.csv&#39;
one_person_data_path = &#39;1인가구(연령별).csv&#39;
rest_data = pd.read_csv(rest_data_path, encoding=&#39;utf-8&#39;)
one_person_data = pd.read_csv(one_person_data_path, encoding=&#39;utf-8&#39;)

# 预处理数据
one_person_data = one_person_data.rename(columns={&#39;자치구별(2)&#39;: &#39;자치구&#39;})
one_person_data_cleaned = one_person_data[one_person_data[&#39;자치구&#39;] != &#39;자치구별(2)&#39;]
one_person_data_cleaned[&#39;2023_합계&#39;] = one_person_data_cleaned.loc[:, &#39;2023&#39;:&#39;2023.15&#39;].apply(
pd.to_numeric, errors=&#39;coerce&#39;).sum(axis=1)
one_person_summary = one_person_data_cleaned[[&#39;자치구&#39;, &#39;2023_합계&#39;]]

rest_data_cleaned = rest_data.rename(columns={&#39;지번주소&#39;: &#39;주소&#39;})
rest_data_cleaned[&#39;자치구&#39;] = rest_data_cleaned[&#39;주소&#39;].str.split(&#39; &#39;).str[1]

merged_data = pd.merge(rest_data_cleaned, one_person_summary, on=&#39;자치구&#39;, how=&#39;left&#39;)

# 添加噪音
np.随机.种子(42) 
噪声 = np.random.normal(0, 0.05 * merged_data[&#39;2023_합계&#39;].std(),大小=merged_data.shape[0])
merged_data[&#39;y&#39;] = merged_data[&#39;2023_합계&#39;] + 噪声

# 保存数据
result_continuous = merged_data[[&#39;자치구&#39;, &#39;주소&#39;, &#39;2023_합계&#39;, &#39;y&#39;]].dropna()

＃ 输出
结果_连续&#39;

输出
&lt;块引用&gt;
 자치구 주소 2023_합계 y


&lt;块引用&gt;
\5 관악구 서울특별시 관악구 봉천동 1562-17 159036.0 158422.996610
\6 관악구 서울특별시 관악구 봉천동 1562-17 142454.0 146588.600626
\18 관악구 서울특별시 관악구 신림동 1538-14 159036.0 156658.665625
\19 관악구 서울특별시 관악구 신림동 1538-14 142454.0 138756.390843
\46 관악구 서울특별시 관악구 신림동 1519-22 159036.0 157829.983096
\... ... ... ... 
\142131 도봉구 서울특별시 도봉구 창동 808 동아청솔아파트 상і동 209호 46250.0 43593.821116
\142138 관악구 서울특별시 관악구 신림동 1458-4 센트레빌 13차 159036.0 157758.991278
\142139 관악구 서울특별시 관악구 신림동 1458-4 센트레빌 13차 142454.0 140637.617424
\142142  관악구 서울특별시 관악구 봉천동 928-1 우형빌딩 159036.0 158889.888720
\142143 관악구 서울특별시 관악구 봉천동 928-1 우형빌딩 142454.0 142926.767535
\14162行×4列\

from sklearn.preprocessing import OneHotEncoder
从 sklearn.preprocessing 导入StandardScaler

columns_to_encode = [&#39;자치구&#39;]

columns_to_exclude = [&#39;2023_합계&#39;]

encoder = OneHotEncoder(sparse_output=False) # drop=&#39;first&#39; 取消排序
encoded_columns =coder.fit_transform(result_continuous[columns_to_encode])
encoded_column_names =coder.get_feature_names_out(columns_to_encode)

encoded_df = pd.DataFrame(encoded_columns, columns=encoded_column_names, index=result_continuous.index)

processed_data = result_continuous.drop(columns=columns_to_encode + columns_to_exclude)
final_data = pd.concat([processed_data,encoded_df], axis=1)
Final_data[&#39;주소_hash&#39;] = result_continuous[&#39;주소&#39;].apply(hash)
Final_data = Final_data.drop(columns=[&#39;주소&#39;]) # 원래 주소 열 제거
y = np.log1p(final_data[[&#39;y&#39;]])
Final_data.drop([&#39;y&#39;], axis=1, inplace=True)
定标器=标准定标器()
Final_data_scaled = 缩放器.fit_transform(final_data)
最终数据缩放，y

输出
&lt;块引用&gt;
（数组（[[ 0.72281968,-0.72281968,1.14388039],
[0.72281968，-0.72281968，1.14388039]，
[0.72281968，-0.72281968，-0.74799056]，
...,
[0.72281968，-0.72281968，0.60622321]，
[0.72281968，-0.72281968，0.14385426]，
[0.72281968，-0.72281968，0.14385426]]），`
y
5 11.973030
6 11.895392
18 11.961831
19 11.840482
46 11.969280
... ...
142131 10.682694
142138 11.968830
142139 11.853949
142142 11.975973
142143 11.870095

[14162 行 x 1 列]]]></description>
      <guid>https://stackoverflow.com/questions/79386587/y-has-only-four-values-or-less-while-my-preprocessed-dataframe-has-more-than-4</guid>
      <pubDate>Sat, 25 Jan 2025 11:08:40 GMT</pubDate>
    </item>
    <item>
      <title>不平衡的二元分类-修复我对此解决方案的实现[关闭]</title>
      <link>https://stackoverflow.com/questions/79385651/imbalanced-binary-classification-fix-my-implementation-of-this-solution</link>
      <description><![CDATA[我查看了以下帖子：
处理高度不平衡数据的正确方法 - 二元分类
发帖者 @skillsmuggler 说：
上述问题在处理医疗数据集和其他类型的故障检测时非常常见，其中一个类别（不良影响）总是代表性不足。
解决这个问题的最佳方法是生成折叠并应用交叉验证。折叠应以平衡每个折叠中的类别的方式生成。在您的案例中，这将创建 20 个折叠，每个折叠都具有相同的代表性不足的类别和不同比例的代表性过高的类别。
生成平衡折叠
生成平衡折叠并使用交叉验证也会产生更好的通用和稳健模型。在您的案例中，20 个折叠可能看起来太苛刻，因此您可以创建 10 个折叠，每个折叠的类别比率为 2:1。
/结束。
这对我来说没有意义。
所以他的意思是：创建 10-20 个折叠，每个折叠都有 100% 的少数类和多数类的不同随机子集。好的，完成了。我现在该做什么？如果我以传统的 CV 方式对模型进行评分（使用所有其他折叠进行训练，对一个折叠进行评分），我们将得到疯狂的目标泄漏，因为模型看到的确实是与训练时相同的观察结果。
我在这里遗漏了什么？他是否建议在这些 1:1 分割上训练 20 个弱分类器，然后使用集成评分对测试集进行评估？
作为参考，我的正类频率为 1%。我还没有成功使用任何嵌入式方法来处理不平衡（加权）。我用于评估的指标是：召回率、马修相关系数和平均精度得分。]]></description>
      <guid>https://stackoverflow.com/questions/79385651/imbalanced-binary-classification-fix-my-implementation-of-this-solution</guid>
      <pubDate>Fri, 24 Jan 2025 21:15:58 GMT</pubDate>
    </item>
    <item>
      <title>为什么 PyTorch RetinaNet ResNet50 FPN V2 在配备 T4 GPU 的 Google Colab 上训练速度如此之慢？</title>
      <link>https://stackoverflow.com/questions/79385141/why-is-pytorch-retinanet-resnet50-fpn-v2-training-so-slow-on-google-colab-with-a</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79385141/why-is-pytorch-retinanet-resnet50-fpn-v2-training-so-slow-on-google-colab-with-a</guid>
      <pubDate>Fri, 24 Jan 2025 17:19:08 GMT</pubDate>
    </item>
    <item>
      <title>Bert 模型未使用 JAX 进行学习。结果没有改变</title>
      <link>https://stackoverflow.com/questions/79383687/bert-model-not-learning-using-jax-results-dont-change</link>
      <description><![CDATA[我正在使用 TPU 上的 JAX 训练垃圾邮件分类的 BERT 模型。我的模型没有学习，其结果也没有改变。
Epoch 0：训练损失 = 2.7961559295654297：训练准确度：0.30608975887298584 评估损失 = 3.6600053310394287：评估准确度 = 0.0
Epoch 1：训练损失 = 2.7961559295654297：训练准确度：0.30608975887298584 评估损失 = 3.6600053310394287：评估准确度 = 0.0
Epoch 2：训练损失 = 2.7961559295654297：训练准确度： 0.30608975887298584 Eval Loss = 3.6600053310394287: Eval Accuracy = 0.0

训练代码：
@jax.pmap
def train_step(state, batch, labels):
def loss_fn(params):

# 将批次中的所有内容放入模型并传递模型参数
logits = model(**batch, params = state.params).logits
loss = compute_loss(logits, labels) # 计算损失

return loss, logits

# 将损失函数转换为梯度微分函数
grad_fn = jax.value_and_grad(loss_fn, has_aux = True) # has_aux 允许返回 logits
# 从批次中获取损失和梯度grad_fn
(loss, logits), grads = grad_fn(state.params)
# 使用产生的梯度更新模型状态
new_state = state.apply_gradients(grads = grads)

return loss, logits, new_state

for epoch in range(epochs):
epoch_losses, epoch_accuracies = [], []
for batch in train_dataset:

batch[&quot;input_ids&quot;] = jnp.array(batch[&quot;input_ids&quot;])
batch[&quot;attention_mask&quot;] = jnp.array(batch[&quot;attention_mask&quot;])
batch[&quot;token_type_ids&quot;] = jnp.array(batch[&quot;token_type_ids&quot;])

# 我们将在多个数据集上复制该值设备 (tpus)
batch_inputs = {k: jax.device_put_replicated(v, jax.devices()) for k, v in batch.items() if k != &quot;Category&quot;}
batch_labels = jax.device_put_replicated(batch[&quot;Category&quot;], jax.devices()) # 在设备之间复制标签

# 从数据中删除 none
batch_labels = safe_convert_to_jax_array(jnp.array(batch_labels))
batch_labels = batch_labels.transpose(1, 0)

loss, logits, state = train_step(state, batch_inputs, batch_labels)

cls_logits = logits[:, :, 0, :] 
分类_logits = cls_logits[:, :, :2]

prediction_labels = jnp.argmax(classification_logits, axis = -1)
accuracy = compute_accuracy(predicted_labels, batch_labels)

初始化状态的代码：
class TrainState(train_state.TrainState):
pass

# 我们的模型参数
params = model.params
# 为我们的训练创建初始状态
state = TrainState.create(apply_fn = model.__call__, params = params, tx = optimizer)

def safe_convert_to_jax_array(input_data, default_value = 0):
# 用 default_value 替换 None 值
return jnp.array([default_value if x is None else x for x in input_data])

# 在 tpu 上复制状态
state = jax.device_put_replicated(state, jax.devices())

要查看完整代码：https://www.kaggle.com/code/yousefr/bert-spam-classification-using-jax-and-tpus
此外，我尝试调整学习率，但没有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/79383687/bert-model-not-learning-using-jax-results-dont-change</guid>
      <pubDate>Fri, 24 Jan 2025 08:33:23 GMT</pubDate>
    </item>
    <item>
      <title>社区连接器管道的 Apps 脚本中未检测到 BigQuery ML 模型训练完成</title>
      <link>https://stackoverflow.com/questions/79382851/bigquery-ml-model-training-completion-not-detected-in-apps-script-for-community</link>
      <description><![CDATA[我正在 lookerstudio 社区连接器中构建 Google Apps Script 管道，该管道创建 BigQuery ML 模型，然后将其用于异常检测。尽管实现了模型存在性检查和等待函数，但我仍然收到此错误：

错误：异常检测失败：{ 
&quot;error&quot;：{ 
&quot;code&quot;：400，
&quot;message&quot;：&quot;无效的表值函数 ML.DETECT_ANOMALIES\n该模型尚不可用...&quot;，
&quot;status&quot;：&quot;INVALID_ARGUMENT&quot; 
}
}


这是我的工作流程：

创建每日汇总表
创建 ARIMA 模型
等待模型训练
创建异常结果表

相关代码片段：
function modelExists(projectId, datasetId, modelId, accessToken) {
var url = &#39;https://bigquery.googleapis.com/bigquery/v2/projects/&#39; + 
projectId + &#39;/datasets/&#39; + datasetId + &#39;/models/&#39; + modelId;
var options = {
method: &#39;get&#39;,
headers: { Authorization: &#39;Bearer &#39; + accessToken },
muteHttpExceptions: true
};

尝试 {
var response = UrlFetchApp.fetch(url, options);
if (response.getResponseCode() === 200) {
var modelInfo = JSON.parse(response.getContentText());
// 检查 ACTIVE 状态或 creationTime 是否存在
return (modelInfo.state === &#39;ACTIVE&#39; || !!modelInfo.creationTime);
}
return false;
} catch (e) {
return false;
}
}

function waitForModel(projectId, datasetId, modelId, accessToken, timeout, interval) {
timeout = timeout || 480000; // 8 分钟
interval = interval || 5000; // 5 秒

var startTime = Date.now();
while (Date.now() - startTime &lt; timeout) {
var exist = modelExists(projectId, datasetId, modelId, accessToken);
if (存在) {
Logger.log(&#39;模型 &#39; + modelId + &#39; 已准备就绪&#39;);
return true;
}
Logger.log(&#39;正在等待模型 &#39; + modelId + &#39;... 当前状态： &#39; + getModelState(projectId, datasetId, modelId, accessToken));
Utilities.sleep(interval);
}
throw new Error(&#39;等待模型超时 &#39; + modelId);
}

function getModelState(projectId, datasetId, modelId, accessToken) {
var url = &#39;https://bigquery.googleapis.com/bigquery/v2/projects/&#39; + 
projectId + &#39;/datasets/&#39; + datasetId + &#39;/models/&#39; + modelId;
var options = {
method: &#39;get&#39;,
headers: { Authorization: &#39;Bearer &#39; + accessToken },
muteHttpExceptions: true
};

try {
var response = UrlFetchApp.fetch(url, options);
if (response.getResponseCode() === 200) {
var modelInfo = JSON.parse(response.getContentText());
return modelInfo.state || &#39;UNKNOWN&#39;;
}
return &#39;NOT_FOUND&#39;;
} catch (e) {
return &#39;ERROR&#39;;
}
}

管道执行：
const createModelQuery = `CREATE MODEL ...`;
fetchBigQuery(createModelQuery); 

// 2. 等待模型
waitForModel(projectId, datasetId, modelName, accessToken);

// 3. 创建结果表 &lt;- 此处失败
const detectQuery = `SELECT * FROM ML.DETECT_ANOMALIES(...)`;
fetchBigQuery(detectQuery);

问题：
即使 waitForModel 成功完成，后续的 ML.DETECT_ANOMALIES 调用仍会失败，并显示“模型不可用”。当我在 BigQuery 控制台中手动检查时，模型最终会在几分钟后可用。
问题：
为什么我的 waitForModel 函数无法正确检测模型训练完成情况，我如何确保管道等到模型真正准备就绪？]]></description>
      <guid>https://stackoverflow.com/questions/79382851/bigquery-ml-model-training-completion-not-detected-in-apps-script-for-community</guid>
      <pubDate>Thu, 23 Jan 2025 23:17:36 GMT</pubDate>
    </item>
    <item>
      <title>使用 Yolov3 进行光学音乐识别</title>
      <link>https://stackoverflow.com/questions/79382146/optical-music-recognition-using-yolov3</link>
      <description><![CDATA[我正在尝试编写一个模型（Yolov3）来检测乐谱上的各种音乐符号。但所有适合此目的的数据集都只建立在印刷乐谱上。有没有办法以某种方式将模型适应手写字符？预训练 darknet-53 会对此有所帮助吗？如果我训练 darknet-53 识别手写和印刷字符，这会产生什么影响？
Yolov3 架构：Yolov3]]></description>
      <guid>https://stackoverflow.com/questions/79382146/optical-music-recognition-using-yolov3</guid>
      <pubDate>Thu, 23 Jan 2025 18:12:00 GMT</pubDate>
    </item>
    <item>
      <title>如何在固定的BBOX中将YOLOv8model与Deepsort连接起来？</title>
      <link>https://stackoverflow.com/questions/79378146/how-can-i-connect-yolov8model-with-deepsort-in-a-fixed-bbox</link>
      <description><![CDATA[我正在生成一个可以检测摩托车和汽车的模型来提取各自的信息。
但在将 YOLOv8 模型（这是我自定义的模型）与 Deepsort 算法连接的过程中，我发现了几个问题。

起初，自定义模型（YOLOv8）可以检测到每辆车，提取的视频显示完美的边界框
与 Deepsort 连接后，它漏掉了几辆车，提取的视频有错误的边界框（它们太大，不适合每辆车）
我在 YOLOv8 2 Deepsort 之间找不到错误的结果。

请帮帮我
import cv2
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort

# 初始化 YOLO 模型
model_path = &quot;/content/drive/MyDrive/Capstone/best_motorcycle_detector_NIGHT8.pt&quot;
model = YOLO(model_path)
model.to(&#39;cuda&#39;) # 使用 GPU

# 初始化 DeepSORT
tracker = DeepSort(max_age=200, n_init=1, nn_budget=200)

# 帮助程序将 YOLO 结果转换为 DeepSORT 格式
def yolo_to_deepsort(yolo_results, target_classes):
detections = []
for det in yolo_results[0].boxes:
x1, y1, x2, y2 = map(float, det.xyxy[0].cpu().numpy())
confidence = float(det.conf.cpu().numpy().item())
class_id = int(det.cls.cpu().numpy())
if class_id in target_classes:
detections.append([(x1, y1, x2, y2),置信度])
返回检测

# 主处理循环
video_path = &quot;/content/drive/MyDrive/Capstone/11.15 1200-1400/1320-1400.mp4&quot;
cap = cv2.VideoCapture(video_path)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

output_path = &quot;/content/drive/MyDrive/Capstone/Results/processed_video.avi&quot;
video_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*&#39;MJPG&#39;), fps, (frame_width, frame_height))

target_classes = [2, 3] # 汽车 (2)、摩托车 (3)

while cap.isOpened():
ret, frame = cap.read()
if not ret:
break

# 运行 YOLO 模型
results = model(frame, conf=0.3)

# 将 YOLO 结果转换为 DeepSORT 格式
detections = yolo_to_deepsort(results, target_classes)

# 更新跟踪器
tracks = tracker.update_tracks(detections, frame=frame)

# 绘制边界框
for track in tracks:
if not track.is_confirmed():
continue
x1, y1, x2, y2 = map(int, track.to_tlbr())
track_id = track.track_id
标签 = f&quot;ID {track_id}&quot;
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# 保存帧
video_writer.write(frame)

cap.release()
video_writer.release()


找出 YOLO 中的协调性
Deepsort 的输入和输出
与其他算法连接，但 Deepsort 更适合我的视频

这是视频和模型权重（YOLOv8）:)
Deepsort 是一种跟踪检测到的对象的跟踪算法。
-File : extracted_weights =&gt; 定制的 YOLOv8 模型的权重
-File : 11.15 1320-1400_detected_video.avi =&gt; 通过定制的 YOLOv8 模型检测摩托车和汽车的视频
-File : 11.15 1320-1400_deepsort_processed_video.avi =&gt;跟踪（使用深度排序算法）检测到的物体（通过定制的 YOLOv8 模型）的视频
https://drive.google.com/file/d/1-03M2L42RtP6hauVP7fKSSqETEm8LtR6/view?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/79378146/how-can-i-connect-yolov8model-with-deepsort-in-a-fixed-bbox</guid>
      <pubDate>Wed, 22 Jan 2025 14:30:52 GMT</pubDate>
    </item>
    <item>
      <title>如何在微调过程中正确设置 pad token（而不是 eos）以避免模型无法预测 EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>pyspark 实现的 ALS 是如何处理每个用户-项目组合的多个评级的？</title>
      <link>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</link>
      <description><![CDATA[我观察到 ALS 的输入数据不需要每个用户-项目组合都有唯一的评分。
这是一个可重现的示例。
# 示例数据框
df = spark.createDataFrame([(0, 0, 4.0),(0, 1, 2.0), 
(1, 1, 3.0), (1, 2, 4.0), 
(2, 1, 1.0), (2, 2, 5.0)],[&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;])

df.show(50,0)
+----+----+------+
|user|item|rating|
+----+----+------+
|0 |0 |4.0 |
|0 |1 |2.0 |
|1 |1 |3.0 |
|1 |2 |4.0 |
|2 |1 |1.0 |
|2 |2 |5.0 |
+----+----+------+

可以看到，每个用户-商品组合只有一个评分（理想情况）。
如果我们将这个数据框传递到 ALS，它将为您提供如下预测：
# 拟合 ALS
from pyspark.ml.recommendation import ALS
als = ALS(rank=5, 
maxIter=5, 
seed=0,
regParam = 0.1,
userCol=&#39;user&#39;,
itemCol=&#39;item&#39;,
ratingsCol=&#39;rating&#39;,
nonnegative=True)
model = als.fit(df)

# 来自 als 的预测
all_comb = df.select(&#39;user&#39;).distinct().join(broadcast(df.select(&#39;item&#39;).distinct()))
predictions = model.transform(all_comb)

predictions.show(20,0)
+----+----+----------+
|user|item|prediction|
+----+----+----------+
|0 |0 |3.9169915 |
|0 |1 |2.031506 |
|0 |2 |2.3546133 |
|1 |0 |4.9588947 |
|1 |1 |2.8347554 |
|1 |2 |4.003007 |
|2 |0 |0.9958025 |
|2 |1 |1.0896711 |
|2 |2 |4.895194 |
+----+----+----------+

到目前为止，一切对我来说都是有意义的。但是如果我们有一个包含多个用户-项目评分组合的数据框，如下所示 -
# 示例数据框
df = spark.createDataFrame([(0, 0, 4.0), (0, 0, 3.5),
(0, 0, 4.1),(0, 1, 2.0),
(0, 1, 1.9),(0, 1, 2.1),
(1, 1, 3.0), (1, 1, 2.8),
(1, 2, 4.0),(1, 2, 3.6),
(2, 1, 1.0), (2, 1, 0.9),
(2, 2, 5.0),(2, 2, 4.9)],
[&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;])
df.show(100,0)
+----+----+------+
|user|item|rating|
+----+----+------+
|0 |0 |4.0 |
|0 |0 |3.5 |
|0 |0 |4.1 |
|0 |1 |2.0 |
|0 |1 |1.9 |
|0 |1 |2.1 |
|1 |1 |3.0 |
|1 |1 |2.8 |
|1 |2 |4.0 |
|1 |2 |3.6 |
|2 |1 |1.0 |
|2 |1 |0.9 |
|2 |2 |5.0 |
|2 |2 |4.9 |
+----+----+------+

如您在上面的数据框中看到的那样，一个用户-项目组合有多条记录。例如 - 用户“0”多次对项目“0”进行评分，即分别为 4.0、3.5 和 4.1。
如果我将此输入数据框传递给 ALS 会怎样？这会起作用吗？
我最初认为它不应该起作用，因为 ALS 应该根据用户-项目组合获得唯一评级，但当我运行它时，它起作用了，让我感到惊讶！
# 拟合 ALS
als = ALS(rank=5, 
maxIter=5, 
seed=0,
regParam = 0.1,
userCol=&#39;user&#39;,
itemCol=&#39;item&#39;,
ratingsCol=&#39;rating&#39;,
nonnegative=True)
model = als.fit(df)

# 来自 als 的预测
all_comb = df.select(&#39;user&#39;).distinct().join(broadcast(df.select(&#39;item&#39;).distinct()))
predictions = model.transform(all_comb)

predictions.show(20,0)
+----+----+----------+
|user|item|prediction|
+----+----+----------+
|0 |0 |3.7877638 |
|0 |1 |2.020348 |
|0 |2 |2.4364853 |
|1 |0 |4.9624424 |
|1 |1 |2.7311888 |
|1 |2 |3.8018093 |
|2 |0 |1.2490809 |
|2 |1 |1.0351425 |
|2 |2 |4.8451777 |
+----+----+----------+

为什么它会起作用？我以为它会失败，但它没有，而且还给了我预测。
我尝试查看研究论文、ALS 的有限源代码和互联网上可用的信息，但找不到任何有用的东西。
是取这些不同评分的平均值然后将其传递给 ALS 还是其他什么？
有人遇到过类似的事情吗？或者知道 ALS 内部如何处理此类数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</guid>
      <pubDate>Tue, 26 Apr 2022 10:37:44 GMT</pubDate>
    </item>
    <item>
      <title>处理高度不平衡数据的正确方法——二元分类</title>
      <link>https://stackoverflow.com/questions/59409967/proper-way-to-handle-highly-imbalanced-data-binary-classification</link>
      <description><![CDATA[我有一个非常大的数据集，有 6000 万行和 11 个特征。
这是高度不平衡的数据集，20:1（信号：背景）。
正如我所看到的，有两种方法可以解决这个问题：
第一：欠采样/过采样。
我有两个问题/疑问。
如果我在训练测试分割之前进行欠采样，我会丢失大量数据。
但更重要的是，如果我在平衡的数据集上训练模型，我会丢失有关信号数据频率的信息（比如说良性肿瘤的频率高于恶性肿瘤的频率），并且由于模型经过训练和评估，因此模型将表现良好。但如果将来某个时候我要在新数据上尝试我的模型，它的表现会很差，因为真实数据是不平衡的。
如果我在训练测试拆分后进行了欠采样，我的模型将欠拟合，因为它将在平衡数据上进行训练，但在不平衡数据上进行验证/测试。
第二 - 类别权重惩罚
我可以将类别权重惩罚用于 XBG、随机森林、逻辑回归吗？
所以，我正在寻找解决此类问题的解释和想法。]]></description>
      <guid>https://stackoverflow.com/questions/59409967/proper-way-to-handle-highly-imbalanced-data-binary-classification</guid>
      <pubDate>Thu, 19 Dec 2019 12:37:16 GMT</pubDate>
    </item>
    <item>
      <title>如何解决“无法将类强制转换为data.frame？</title>
      <link>https://stackoverflow.com/questions/58870663/how-to-solve-cannot-coerce-class-to-data-frame</link>
      <description><![CDATA[问题出现在第 20 行：x3 &lt;- lm(Salary ~ ...

as.data.frame.default(data) 中的错误：无法将类‘c(&quot;train&quot;, &quot;train.formula&quot;)’强制转换为 data.frame

如何解决？
attach(Hitters)
Hitters

library(caret)
set.seed(123)
# 定义训练控制
set.seed(123) 
train.control &lt;- trainControl(method = &quot;cv&quot;, number = 10)
# 训练模型
x2 &lt;- train(Salary ~., data = x, method = &quot;lm&quot;,
trControl = train.control)
# 总结结果
print(x)
x3 &lt;- lm(Salary ~ poly(AtBat,3) + poly(Hits,3) + poly(Walks,3) + poly(CRuns,3) + poly(CWalks,3) + poly(PutOuts,3), data = x2)
summary(x3)
MSE = mean(x3$residuals^2)
print(&quot;均方误差：&quot;)
print(MSE)
]]></description>
      <guid>https://stackoverflow.com/questions/58870663/how-to-solve-cannot-coerce-class-to-data-frame</guid>
      <pubDate>Fri, 15 Nov 2019 05:09:08 GMT</pubDate>
    </item>
    <item>
      <title>Keras 进度条中的准确度是什么意思？</title>
      <link>https://stackoverflow.com/questions/52559086/what-does-the-accuracy-mean-in-the-keras-progress-bar</link>
      <description><![CDATA[在 Keras 中，您将获得类似以下内容：
Epoch 1/1
60000/60000 [==============================] - 297s 5ms/step - 损失：0.7048 - acc：0.7669

60000/60000 [==============================] - 179s 3ms/step
训练集：
acc：94.60%

10000/10000 [================================] - 30s 3ms/step
测试集：
acc： 95.10%

但我是这样拟合的：

model.fit(X_train, oh_y_train,
batch_size=512,
epochs=1,
verbose=1)

.fit() 方法中没有验证数据，它从第 1 个时期测量准确率的是什么？
最终准确率差别很大。]]></description>
      <guid>https://stackoverflow.com/questions/52559086/what-does-the-accuracy-mean-in-the-keras-progress-bar</guid>
      <pubDate>Fri, 28 Sep 2018 16:07:45 GMT</pubDate>
    </item>
    <item>
      <title>ALS（交替最小二乘）算法对用户的多个排名</title>
      <link>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</link>
      <description><![CDATA[嗨，经过大量研究，我们决定使用 Google Cloud 基础架构，并在我们的产品推荐系统中使用 ALS 算法（一种协同过滤方法 - https://cloud.google.com/solutions/recommendations-using-machine-learning-on-compute-engine#Training-the-models ），详细说明如下：
我们有两种类型的客户。第一类是附近销售产品的公司，第二类是打算从这些公司购买产品的消费者

每个消费者都可以搜索附近的公司或按行业搜索公司（例如杂货店、干洗店、肉店等）
当消费者找到一家公司时，他/她可以执行以下操作（他可以一次执行多项操作）
2.1. 仅查看公司简介
2.2. 将公司添加到收藏夹
2.3. 开始与公司聊天
2.4. 从公司下订单
2.5.给公司评分和评论

所以我不明白的是：上面描述的每件商品都被确定为我们数据库中的某些评分列，例如：
查看公司简介：10 分
从公司下订单：20 分
给公司打星或评论：20 分
因此，对于同一用户，每件商品都是单独的评分。
在我们的数据库中，对于用户-公司对，可能会有超过 1 行
例如：
第 1 行：user18-company18-10pts（查看过一次个人资料）
第 2 行：user18-company18-20pts（从公司下订单）
第 3 行：user18-company19-10pts
我不确定这个算法，它是计算该用户对同一家公司的所有评分的总和（我到底想要什么）还是只是寻找单个用户对单个公司的评分的单行？（我想要的是这个 ALS 算法来总结该用户-公司对的第 1 行和第 2 行）
有人知道吗？这对我们的推荐系统非常重要。因为我正在寻找的算法需要计算用户所有评分的总和，以便推荐另一家公司。因为我们的商业模式与电影评分系统不同
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</guid>
      <pubDate>Tue, 01 May 2018 09:53:59 GMT</pubDate>
    </item>
    </channel>
</rss>