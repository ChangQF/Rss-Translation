<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 12 Jan 2024 21:12:41 GMT</lastBuildDate>
    <item>
      <title>citeseer和cora（图链接预测），代码准确性问题[关闭]</title>
      <link>https://stackoverflow.com/questions/77808697/citeseer-and-cora-graph-link-prediciton-problem-whith-code-accuracy</link>
      <description><![CDATA[大家好，我有一个正在训练 citeseer 数据集的代码，该数据集的准确率约为 62%，我需要将其至少提高到 75%，或者使用其中的另一个数据集，该数据集的准确率超过 80%，我使用的 cora 的准确率为 82% % 准确度，但我还需要一个数据集（我对图形训练不太了解，如果您有包含 .cites 和 .content 文件的数据集，请给我 tnx ）
导入networkx为nx
将 numpy 导入为 np
从 sklearn.model_selection 导入 train_test_split，GridSearchCV
从 sklearn.metrics 导入 precision_score
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.svm 导入 SVC
从 Node2Vec 导入 Node2Vec
将 pandas 导入为 pd
导入操作系统

# 选择数据集 [ Comic, citeseer, cora ]
数据集=“漫画”

os.system(“cls”)
print(&quot;正在加载数据集...&quot;)
# 加载节点和边数据集
如果数据集==“漫画”：
    G = nx.有向图()
    对于索引，pd.read_csv(&#39;comic_nodes.csv&#39;).iterrows():G.add_node(row[&#39;node&#39;], type=row[&#39;type&#39;]) 中的行
    对于索引，pd.read_csv(&#39;comic_edges.csv&#39;).iterrows():G.add_edge(row[&#39;hero&#39;], row[&#39;comic&#39;]) 中的行
    node_features = np.array([G.nodes[node].get(&#39;type&#39;, &#39;unknown&#39;) for Node in G.nodes()])
    node_labels = np.array([G.nodes[node].get(&#39;type&#39;, &#39;unknown&#39;) for Node in G.nodes()])
    node_index = {node_id: i for i, node_id in enumerate(G.nodes())}
别的：
    G = nx.read_edgelist(f&quot;{dataset}.cites&quot;, create_using=nx.DiGraph())
    node_data = np.loadtxt(f&quot;{dataset}.content&quot;, dtype=str)
    node_data = [node_data 中数据的数据，如果 G.nodes() 中的数据[0]]
    node_ids, node_features, node_labels = zip(*[(data[0], data[1:-1].astype(int), data[-1]) 对于node_data中的数据])
    node_index = {node_id: i for i, node_id in enumerate(node_ids)}
节点= [node_id为node_id，_排序（node_index.items（），key = lambda item：item [1]）]

os.system(“cls”)
# 使用node2vec生成游走
node2vec = Node2Vec(G，维度=256，walk_length=70，num_walks=200，工人=4)
模型 = node2vec.fit(窗口=20, min_count=0, sg=1, epochs=20)
embeddings = np.array([model.wv[node] 用于节点中的节点])

# 将数据分成训练集和测试集（20%用于测试）
X_train，X_test，y_train，y_test = train_test_split（嵌入，node_labels，test_size = 0.01，random_state = 42）
缩放 = StandardScaler()
X_train = Scale.fit_transform(X_train)
X_test = Scale.transform(X_test)

best_params = {&#39;C&#39;:8}
##### 网格搜索
# grid_search = GridSearchCV(SVC(kernel=&#39;rbf&#39;, gamma=&#39;scale&#39;, random_state=42), {&#39;C&#39;: range(1, 11)}, cv=4)
# grid_search.fit(X_train, y_train)
# best_params = grid_search.best_params_
##### print(&quot;最佳超参数：&quot;, best_params)

# 使用最佳超参数训练 SVM
eclf = SVC(内核=&#39;rbf&#39;, C=best_params[&#39;C&#39;], gamma=&#39;scale&#39;, random_state=42)
eclf = eclf.fit(X_train, y_train)

os.system(“cls”)
print(“准确度：”, precision_score(y_test, eclf.predict(X_test)))
]]></description>
      <guid>https://stackoverflow.com/questions/77808697/citeseer-and-cora-graph-link-prediciton-problem-whith-code-accuracy</guid>
      <pubDate>Fri, 12 Jan 2024 18:44:56 GMT</pubDate>
    </item>
    <item>
      <title>图像预处理|预测范围 [0.44 - 0.55] [关闭]</title>
      <link>https://stackoverflow.com/questions/77808063/image-preprocessing-predict-in-range-0-44-0-55</link>
      <description><![CDATA[我正在尝试解决二元分类任务。我有鳄鱼和短吻鳄的数据集。我有一个形状为 (2894, 2) 的矩阵 x，还有一个带有标签（0 或 1）且形状为 2894 的变量 y。
模型 = tf.keras.Sequential()[
    tf.keras.layers.Dense(256，激活=&#39;relu&#39;),
    tf.keras.layers.Dropout(速率=(0.12)),
    tf.keras.layers.Dense(128, 激活=&#39;relu&#39;),
    tf.keras.layers.Dropout(速率=(0.12)),
    tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
]）

loss_fn = tf.keras.losses.BinaryCrossentropy()
model.compile(optimizer=&#39;adam&#39;,loss=loss_fn,metrics=[&#39;accuracy&#39;])

但我的预测每次都在 (0.44–0.55) 范围内
x_train [[像素, 像素], [像素, 像素], [像素, 像素]] 的示例
另外，我对每个测试和训练变量都使用此方法：
shuffle(minmax.fit_transform(np.concatenate((x_train_aligator, x_train_crocodile), axis=0)), random_state=1)

现在，我对 x_train 和 x_test 使用 StandardScaler() 和 MinMaxScaler()。此外，测试数据的 random_state=1 和 random_state=2 也有相似之处。但我不知道为什么我的结果每次都在一个范围内。
我尝试过对数据和随机保存生成进行不同的操作。]]></description>
      <guid>https://stackoverflow.com/questions/77808063/image-preprocessing-predict-in-range-0-44-0-55</guid>
      <pubDate>Fri, 12 Jan 2024 16:42:15 GMT</pubDate>
    </item>
    <item>
      <title>Filter_Value 选择 TDA R</title>
      <link>https://stackoverflow.com/questions/77807593/filter-value-selection-tda-r</link>
      <description><![CDATA[我对 R 中 TDA 库（GSSTDA、TDAMapper、Mapper 等）的 filter_value 参数感到有点困惑。
例如，给出 GSSTDA 中的 Mapper 对象：
&lt;前&gt;&lt;代码&gt;映射器(
  完整数据，
  过滤器值，
  间隔数 = 5,
  重叠百分比 = 40,
  distance_type =“cor”，
  clustering_type =“分层”，
  num_bins_when_clustering = 10,
  连接类型=“单一”，
  最优聚类模式=“”，
  na.rm = TRUE
）

&lt;块引用&gt;
参数
过滤值
对输入应用过滤函数后得到的向量
矩阵，即具有每个过滤函数值的向量
包括样本。


有人可以通过一个众所周知的数据（例如 Iris）的示例来描述如何手动选择 filter_value 吗？
https://search.r-project.org /CRAN/refmans/GSSTDA/html/mapper.html]]></description>
      <guid>https://stackoverflow.com/questions/77807593/filter-value-selection-tda-r</guid>
      <pubDate>Fri, 12 Jan 2024 15:21:09 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：传递的 save_path 不是有效的检查点：./param_model/model(8, 100, 3)_75_10_-100_0</title>
      <link>https://stackoverflow.com/questions/77807281/valueerror-the-passed-save-path-is-not-a-valid-checkpoint-param-model-model</link>
      <description><![CDATA[#---生成通道----
通道，set_location_user =generate_channel（params_system，num_samples = 1，
location_user_initial=location_user, Rician_factor=Rician_factor)
y，y_real_tmp =generate_received_pilots_batch（通道，phase_shifts，导频，noise_power_db，Pt = Pt_u）
y_ks_tmp =去相关（y，飞行员）
y_ks = np.concatenate([y_ks_tmp.real, y_ks_tmp.imag], axis=1)
y_real = (y_real_tmp - y_mean) / y_std
y_ks_real = (y_ks - y_ks_mean) / y_ks_std
set_location_user = (set_location_user - location_mean) / (location_std+1e-15)

# ----------------------------------神经网络波束成形------------------------ --------------
model_path = &#39;./param_model/model&#39; + str(params_system) + &#39;_&#39; + str(len_pilot) + &#39;_&#39; + str(
    Rician_factor) + &#39;_&#39; + str(noise_power_db) + &#39;_&#39; + str(input_flag)
#print(“模型路径：”, model_path)

这个粗体文本给出了错误。
使用Python 3.6
如何纠正]]></description>
      <guid>https://stackoverflow.com/questions/77807281/valueerror-the-passed-save-path-is-not-a-valid-checkpoint-param-model-model</guid>
      <pubDate>Fri, 12 Jan 2024 14:30:02 GMT</pubDate>
    </item>
    <item>
      <title>无法对具有 4 个键列和一个索引列的缩放数据执行逆变换</title>
      <link>https://stackoverflow.com/questions/77806320/failure-to-perform-inverse-transform-of-scaled-data-with-4-key-columns-and-an-in</link>
      <description><![CDATA[我正在尝试使用简单的 LSTM 模型来预测太阳能产量，因此，每当我尝试运行代码行时都会遇到问题，它会给我错误
这是我用来预测错误点的完整代码
df[&#39;DateTime&#39;] = df[&#39;Date&#39;].astype(str) + &#39; &#39; + df[&#39;Time&#39;].astype(str)
df = df.drop([&#39;日期&#39;, &#39;时间&#39;], axis=1)
df = df.set_index(&#39;日期时间&#39;)
features = [&#39;温度(°C)&#39;, &#39;风速(m/s)&#39;, &#39;SolarIrrad(W/m2)&#39;]
目标 = &#39;太阳能光伏发电（瓦）&#39;

train_size = int(len(df) * 0.8)
训练，测试 = df.iloc[:train_size], df.iloc[train_size:]

定标器=标准定标器()
scaler.fit(训练[特征])
train_scaled = scaler.transform(train[特征])
test_scaled = scaler.transform(测试[特征])

def create_sequences(数据, target_index, seq_length):
    X、y = []、[]
    对于范围内的 i(len(data)-seq_length)：
        序列=数据[i:(i+seq_length)]
        目标值 = 数据[i+seq_length, 0]
        # 将序列和目标附加到列表中
        X.append(序列)
        y.append(目标值)
    # 将列表转换为 numpy 数组
    X = np.array(X)
    y = np.array(y)
    返回 X, y
序列长度 = 4

X_train, y_train = create_sequences(train_scaled, 目标, seq_length)
X_test, y_test = create_sequences(test_scaled, 目标, seq_length)

模型=顺序（）
model.add(LSTM(单位=50，激活=&#39;relu&#39;，input_shape=(X_train.shape[1]，X_train.shape[2])))
model.add(密集(单位=1))
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;)

Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 5，restore_best_weights = True）

历史= model.fit（X_train，y_train，epochs = 50，batch_size = 32，validation_split = 0.2，callbacks = [early_stopping]，verbose = 1）

y_pred = model.predict(X_test)

concatenated_pred = np.concatenate([X_test_last_col, y_pred_last_col], axis=-1)

错误：
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-43-cc4afc30ab89&gt;在&lt;细胞系：6&gt;()
4
5 # 沿最后一个轴连接
6 concatenated_pred = np.concatenate([X_test_last_col, y_pred_last_col], axis=-1)
7
8 # 逆变换
/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py in concatenate(*args, **kwargs)
ValueError：所有输入数组必须具有相同的维数，但索引 0 处的数组有 3 个维度，索引 1 处的数组有 2 个维度

我期待它成功地进行逆变换]]></description>
      <guid>https://stackoverflow.com/questions/77806320/failure-to-perform-inverse-transform-of-scaled-data-with-4-key-columns-and-an-in</guid>
      <pubDate>Fri, 12 Jan 2024 11:43:14 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Roberta 计算单词和句子嵌入？</title>
      <link>https://stackoverflow.com/questions/77805776/how-to-calculate-word-and-sentence-embedding-using-roberta</link>
      <description><![CDATA[我正在尝试使用 Roberta 计算单词和句子嵌入，对于单词嵌入，我从 RobertaModel 类中提取最后一个隐藏状态 outputs[0]，但是我不确定这是否是正确的计算方法。
至于句子嵌入，我不知道如何计算它们，这是我尝试过的代码：
从 Transformers 导入 RobertaModel、RobertaTokenizer
进口火炬

模型 = RobertaModel.from_pretrained(&#39;roberta-base&#39;)
tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)
Captions = [“示例标题”、“lorem ipsum”、“这只鸟是黄色的，有红色翅膀”、“嗨”、“示例”]

encoded_captions = [tokenizer.encode(caption) 用于字幕中的字幕]

# 用 0 将序列填充到相同的长度
max_len = max(len(seq) 用于编码字幕中的 seq)
padded_captions = [seq + [0] * (max_len - len(seq)) 对于encoded_captions中的seq]

# 转换为批量大小为 5 的 PyTorch 张量
input_ids = torch.tensor(padded_captions)

输出=模型(input_ids)
word_embedding = 输出[0].连续()
句子嵌入 = ??????

如何使用 Roberta 计算单词和句子嵌入？]]></description>
      <guid>https://stackoverflow.com/questions/77805776/how-to-calculate-word-and-sentence-embedding-using-roberta</guid>
      <pubDate>Fri, 12 Jan 2024 10:05:01 GMT</pubDate>
    </item>
    <item>
      <title>使用 tSNE 后在 MNIST 数据集上应用 KMeans [关闭]</title>
      <link>https://stackoverflow.com/questions/77805020/applying-kmeans-on-the-mnist-dataset-after-using-tsne</link>
      <description><![CDATA[我在 MNIST 数据集上使用 tSNE，并得到了非常好的结果（当我可视化该图时，所有 10 个标签都分离得很好）。
现在，我想对从 tSNE 建模获得的数据应用 KMeans，并再次将其可视化。
不幸的是，这次我得到了非常糟糕的结果 - 集群看起来非常错误。
我知道 t-SNE 空间中的点之间的距离和关系可能不一定保留原始高维空间中存在的结构，当将 KMeans 等聚类算法直接应用于 t 时，这可能会导致误导性的解释。 -SNE嵌入。
还有什么我可以做得更好的吗？
代码示例：
tsne = TSNE(n_components=2, perplexity=15,learning_rate=200, exaggeration=1)
x_train_tsne = tsne.fit(x_train)

kmeans = KMeans(n_clusters=10, n_init=1, init=&#39;kmeans++&#39;)
labels_kmeans = kmeans.fit_predict(x_train)

对于 np.unique(labels_kmeans) 中的数字：
   索引 = (labels_kmeans == 数字)
   plt.scatter(x_train_tsne[索引，0]，x_train_tsne[索引，1]，s=5，alpha=0.8，标签=str(数字))
]]></description>
      <guid>https://stackoverflow.com/questions/77805020/applying-kmeans-on-the-mnist-dataset-after-using-tsne</guid>
      <pubDate>Fri, 12 Jan 2024 07:35:51 GMT</pubDate>
    </item>
    <item>
      <title>我想通过上传图像来查找“车辆品牌和型号”[关闭]</title>
      <link>https://stackoverflow.com/questions/77804868/i-want-to-find-vehicle-make-and-model-by-uploading-the-image</link>
      <description><![CDATA[我想创建一个Python模型来在上传车辆图像后识别车辆的品牌和型号。
导入deeplake
从张量流导入keras
从tensorflow.keras导入层

加载训练和测试子集
train_dataset = deeplake.load(“hub://activeloop/stanford-cars-train”)
test_dataset = deeplake.load(“hub://activeloop/stanford-cars-test”)

创建 TensorFlow 数据加载器
train_dataloader = train_dataset.tensorflow()
test_dataloader = test_dataset.tensorflow()

定义常量
image_height, image_width, num_channels = (224, 224, 3) # 根据您的数据集进行调整
num_classes = 196 # 数据集中的汽车类别数量

假设您的数据集有一个“images”键
input_layer=layers.Input(shape=(image_height, image_width, num_channels), name=&#39;images&#39;)
x = 层.Conv2D(32, (3, 3), 激活=&#39;relu&#39;)(input_layer)
x = 层数.MaxPooling2D((2, 2))(x)
x = 层.Conv2D(64, (3, 3), 激活=&#39;relu&#39;)(x)
x = 层数.MaxPooling2D((2, 2))(x)
x = 层.Conv2D(128, (3, 3), 激活=&#39;relu&#39;)(x)
x = 层.Flatten()(x)
x = 层.Dense(256, 激活=&#39;relu&#39;)(x)
输出层=层.Dense（num_classes，激活=&#39;softmax&#39;，名称=&#39;car_models&#39;）（x）

模型= keras.Model（输入=输入层，输出=输出层）

编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

指定纪元数和其他训练参数
&lt;前&gt;&lt;代码&gt;num_epochs = 10

训练模型
model.fit（train_dataloader，epochs = num_epochs，validation_data = test_dataloader）

在测试集上评估模型
test_loss, test_accuracy = model.evaluate(test_dataloader)
print(f&#39;测试准确度: {test_accuracy * 100:.2f}%&#39;)

对新图像进行预测
假设您有一个新图像（将“your_image_path”替换为实际路径）
来自tensorflow.keras.preprocessing导入图像
将 numpy 导入为 np

new_image_path = &#39;/content/bmw2.jpg&#39;
img = image.load_img(new_image_path, target_size=(image_height, image_width))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0) # 添加批量维度
img_array /= 255.0 # 标准化像素值

做出预测
预测 = model.predict(img_array)
Predicted_class = np.argmax(预测[0])

将预测的类别映射到实际的汽车品牌和型号（您可能需要从类别索引到品牌和型号的映射）
class_mapping = {} # 定义类映射
Predicted_make_model = class_mapping.get(predicted_class,“未知”)

print(f&#39;给定图像的预测品牌和型号为：{predicted_make_model}&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/77804868/i-want-to-find-vehicle-make-and-model-by-uploading-the-image</guid>
      <pubDate>Fri, 12 Jan 2024 06:57:25 GMT</pubDate>
    </item>
    <item>
      <title>UserWarning：X 没有有效的功能名称，但 KNeighborsClassifier 配备了功能名称 warnings.warn</title>
      <link>https://stackoverflow.com/questions/77804804/userwarning-x-does-not-have-valid-feature-names-but-kneighborsclassifier-was-f</link>
      <description><![CDATA[ ID 曾经_已婚 毕业 性别 职业 支出_分数细分 家庭_身材 年龄 工作_经历
0 462809 0 0 1 5 2 3 3 4 1
1 462643 1 1 0 2 0 0 2 18 15
2 466315 1 1 0 2 2 1 0 44 1
3 461735 1 1 1 7 1 1 1 44 0
4 462669 1 1 0 3 1 0 5 20 15
……………………………………
8063 464018 0 0 1 9 2 3 6 4 0
8064 464685 0 0 1 4 2 3 3 15 3
8065 465406 0 1 0 5 2 3 0 14 1
8066 467299 0 1 0 5 2 1 3 8 1
8067 461879 1 1 1 4 0 1 2 17 0
8068行×10列

data1=data.drop([&quot;ID&quot;,&quot;分段&quot;],axis=1)

从 sklearn.model_selection 导入 train_test_split
     x_train,x_test,y_train,y_test=train_test_split(data1,data.Segmentation,test_size=0.20,random_state=50)

 从 sklearn.neighbors 导入 KNeighborsClassifier
 knn=KNeighborsClassifier(n_neighbors=17)
 knn.fit(x_train,y_train)
 tahmin=knn.predict(x_test)

 knn.score(x_test,y_test)
 #0.4838909541511772
 knn.predict([[1,1,0,2,0,2,18,15]])

 UserWarning：X 没有有效的功能名称，但 KNeighborsClassifier 已安装了功能名称
  #警告.警告(
数组([1])

当我做出预测时，我并没有预料到这个警告。]]></description>
      <guid>https://stackoverflow.com/questions/77804804/userwarning-x-does-not-have-valid-feature-names-but-kneighborsclassifier-was-f</guid>
      <pubDate>Fri, 12 Jan 2024 06:45:16 GMT</pubDate>
    </item>
    <item>
      <title>使用 LOOCV 进行 K 最近邻的问题</title>
      <link>https://stackoverflow.com/questions/77804296/problem-conducting-k-nearest-neighbors-using-loocv</link>
      <description><![CDATA[我有一个示例表，我想对其进行 KKNN 分类。变量 V4 是响应，我希望分类器查看新数据点是否将分类为 0 或 1（实际数据有 12 列，第 12 列是响应，但我仍然会简化示例
库(kknn)

数据 &lt;- data.frame(
  V1=c(1.2,2.5,3.1,4.8,5.2),
  V2=c(0.7, 1.8, 2.3, 3.9, 4.1),
  V3=c(2.3, 3.7, 1.8, 4.2, 5.5),
  V4= c(0, 1, 0, 1, 0)
）

现在，我想使用 for 循环通过 LOOCV 构建 kknn 分类。假设 kknn=3
for (i in 1:nrow(data)) {
  train_data &lt;- 数据[-i, 1:3]
  train_data_response &lt;- data.frame(data[-i, 4])
  colnames(train_data_response) &lt;- “响应”
  test_set &lt;- 数据[i, 3]
  模型 &lt;- kknn(公式=train_data_response ~ ., data.frame(train_data),
                data.frame(test_set)，k=3，scale=TRUE)
}

现在我收到以下错误：
model.frame.default(公式，数据=训练)中的错误：
  变量“train_data_response”的类型（列表）无效

有什么办法可以解决这个错误吗？我认为 kknn 接受矩阵或数据帧。我的训练和测试数据确实是数据框，那么什么给出了？
另外，我是否正确执行了 LOOCV？]]></description>
      <guid>https://stackoverflow.com/questions/77804296/problem-conducting-k-nearest-neighbors-using-loocv</guid>
      <pubDate>Fri, 12 Jan 2024 04:15:37 GMT</pubDate>
    </item>
    <item>
      <title>初始化 VAE 权重 [关闭]</title>
      <link>https://stackoverflow.com/questions/77804014/initializing-vae-weights</link>
      <description><![CDATA[我正在训练遵循以下整体架构的 VAE：

变压器编码器
Mu/Logvar -&gt;重新参数化-&gt;潜在z
变压器解码器

根据典型的 VAE 设置，Mu 和 Logvar 只是两个前馈网络。然而，当我用标准值（例如权重为 0.5，偏差为 0）初始化它们时，我发现模型的初始 KL 损失巨大 - 例如5,000-20,000+。
当然，这个下降得相当快，但模型仍然花费数百个时期将 KL 损失从 300 降至 &lt;50。
一个“解决方法”我发现将权重初始化为低得多的值，并使用学习率预热。但初始化权重非常小：
def init_weights(self, initrange=0.0001) -&gt; &gt;没有任何：
        self.embedding_layer.weight.data.uniform_(-0.5, 0.5)
        
        nn.init.uniform_(self.fc_mu.weight, -initrange, initrange)
        nn.init.uniform_(self.fc_logvar.weight, -initrange, initrange)
        nn.init.zeros_(self.fc_mu.bias)
        nn.init.zeros_(self.fc_logvar.bias)

这样做的结果是一个更加稳定的 KL 损失（开始时约为 1.5），但我担心它会阻止我的解码器学习有意义的表示。实际的重建损失因此受到巨大影响。
这是 VAE 的已知问题吗？有什么我可以尝试的特定初始化技巧吗？或者也许我应该从正常值开始，让模型训练的时间明显更长？]]></description>
      <guid>https://stackoverflow.com/questions/77804014/initializing-vae-weights</guid>
      <pubDate>Fri, 12 Jan 2024 02:14:48 GMT</pubDate>
    </item>
    <item>
      <title>联邦学习全局聚合后准确率下降</title>
      <link>https://stackoverflow.com/questions/77798059/the-accuracy-decreased-after-global-aggregation-in-federated-learning</link>
      <description><![CDATA[我正在开展一个联合学习项目。我编写了一段代码来刺激联邦学习的过程。然而，每次迭代进行全局聚合后，全局模型的测试精度会下降很多，并且在接下来的迭代中保持不变。我使用的聚合算法是FedAvg。我尝试将我的代码分成不同的单元来找出问题所在。
对于本地训练，所选客户训练 3 轮。在这个实验中，将选择所有五个客户端进行训练和聚合，我用于本地的模型是从 torchvision 分叉的 vgg16，数据集是 MNIST，并以 i.i.d 方式分割每个客户端： 
for id, net_id in enumerate(selected):
    logging.info(“训练所选设备 %s。” % (str(net_id)))
    结果 = Userlists[net_id].train(hparams[&#39;n_local_epochs&#39;])
    logging.info(&#39;&gt;&gt; 局部模型 %d: 局部精度: %f in round %d\n&#39; % (id, result[&#39;local_test_acc&#39;], step+1))

在本地模型聚合之前，我使用全局服务器的测试数据来测试本地模型的准确性，
tesc，conf = Misc.compute_accuracy(Userlists[2].model，test_dl_global，get_confusion_matrix=True，device=hparams[&#39;device&#39;])
打印（测试）
&gt; 0.2478966346153846
tesc，conf = Misc.compute_accuracy（Userlists [3] .model，test_dl_global，get_confusion_matrix = True，device = hparams [&#39;device&#39;]）
打印（测试）
&gt; 0.14413060897435898
tesc,conf=misc.compute_accuracy(Userlists[4].model,test_dl_global,get_confusion_matrix=True,device=hparams[&#39;device&#39;])
打印（测试）
&gt; 0.17387820512820512

我使用下面的聚合代码来聚合所选客户端的权重：
&lt;前&gt;&lt;代码&gt;total_sum = 0.0
对于选定的 client_idx：
    Total_sum += 用户列表[client_idx].data_len
    
    
global_para = global_model.state_dict()
client_weights = [torch.tensor( Userlists[client_idx].data_len/total_sum, device=hparams[&#39;device&#39;]) for client_idx in selected]

使用 torch.no_grad()：
    对于顺序，枚举中的 idx（选定）：
        logging.info(f“对于客户端 {idx}”)
        net_para = Userlists[idx].model.state_dict()
        
        如果订单 == 0：
            对于 net_para.keys() 中的键：
                global_para[key] = net_para[key] * client_weights[订单]
        别的：
            对于 net_para.keys() 中的键：
                global_para[key] += net_para[key] * client_weights[订单]


global_model.load_state_dict(global_para)
tesc,conf=misc.compute_accuracy(global_model,train_dl_global,get_confusion_matrix=True,device=hparams[&#39;device&#39;])

全局测试精度下降并保持不变
&lt;前&gt;&lt;代码&gt;&gt; 0.11236666666666667

虽然我尝试增加本地训练的纪元，局部准确率提高到 40%，但全局准确率仍然落入与之前相同的值。我的聚合代码中是否有错误的地方？
测试精度应与本地精度保持在同一水平。]]></description>
      <guid>https://stackoverflow.com/questions/77798059/the-accuracy-decreased-after-global-aggregation-in-federated-learning</guid>
      <pubDate>Thu, 11 Jan 2024 06:30:41 GMT</pubDate>
    </item>
    <item>
      <title>带有我自己的预训练模型的 Sagemaker 批处理变压器</title>
      <link>https://stackoverflow.com/questions/77781734/sagemaker-batch-transformer-with-my-own-pre-trained-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77781734/sagemaker-batch-transformer-with-my-own-pre-trained-model</guid>
      <pubDate>Mon, 08 Jan 2024 15:54:18 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch RuntimeError：CUDA 内存不足且有大量可用内存</title>
      <link>https://stackoverflow.com/questions/71498324/pytorch-runtimeerror-cuda-out-of-memory-with-a-huge-amount-of-free-memory</link>
      <description><![CDATA[在训练模型时，我遇到了以下问题：
运行时错误：CUDA 内存不足。尝试分配 304.00 MiB（GPU 0；8.00 GiB 总容量；已分配 142.76 MiB；6.32 GiB 空闲；PyTorch 总共保留 158.00 MiB）分配的内存尝试设置 max_split_size_mb 以避免碎片。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档
正如我们所看到的，当尝试分配 304 MiB 内存时会发生错误，而 6.32 GiB 是空闲的！问题是什么？正如我所看到的，建议的选项是设置 max_split_size_mb 以避免碎片。它会有帮助吗？如何正确地做到这一点？
这是我的 PyTorch 版本：
火炬==1.10.2+cu113
火炬视觉==0.11.3+cu113
火炬音频===0.10.2+cu113]]></description>
      <guid>https://stackoverflow.com/questions/71498324/pytorch-runtimeerror-cuda-out-of-memory-with-a-huge-amount-of-free-memory</guid>
      <pubDate>Wed, 16 Mar 2022 13:53:45 GMT</pubDate>
    </item>
    <item>
      <title>sklearn LogisticRegressionCV 是否使用最终模型的所有数据</title>
      <link>https://stackoverflow.com/questions/51830558/does-sklearn-logisticregressioncv-use-all-data-for-final-model</link>
      <description><![CDATA[我想知道sklearn中LogisticRegressionCV的最终模型（即决策边界）是如何计算的。假设我有一些 Xdata 和 ylabels，这样
Xdata # 形状为 (n_samples,n_features)
ylabels # 形状是 (n_samples,)，它是二进制的

现在我跑步
从 sklearn.linear_model 导入 LogisticRegressionCV
clf = LogisticRegressionCV(Cs=[1.0],cv=5)
clf.fit(Xdata,ylabels)

这仅考虑一个正则化参数和 CV 中的 5 个折叠。因此，clf.scores_ 将是一个字典，其中一个键的值是一个形状为 (n_folds,1) 的数组。通过这五次折叠，您可以更好地了解模型的表现。
但是，我对从 clf.coef_ 获得的内容感到困惑（我假设 clf.coef_ 中的参数是 clf.coef_ 中使用的参数&gt;clf.predict）。我认为可能有几个选择：

clf.coef_ 中的参数来自在所有数据上训练模型
clf.coef_ 中的参数来自最佳评分折叠
clf.coef_ 中的参数以某种方式对折叠进行平均。

我想这是一个重复的问题，但在我的一生中，我无法在网上、sklearn 文档或 LogisticRegressionCV 的源代码中找到简单的答案。我发现的一些相关帖子是：

GridSearchCV 最终模型
scikit-learn LogisticRegressionCV：最佳系数
在 sklearn 中使用交叉验证和 AUC-ROC 建立逻辑回归模型
通过交叉验证评估 Logistic 回归
]]></description>
      <guid>https://stackoverflow.com/questions/51830558/does-sklearn-logisticregressioncv-use-all-data-for-final-model</guid>
      <pubDate>Mon, 13 Aug 2018 21:06:28 GMT</pubDate>
    </item>
    </channel>
</rss>