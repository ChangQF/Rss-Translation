<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 14 Dec 2023 01:00:11 GMT</lastBuildDate>
    <item>
      <title>使用 Marqo 进行矢量搜索的单个或多个键值对数据结构？</title>
      <link>https://stackoverflow.com/questions/77657071/single-or-multiple-key-value-pair-data-structure-for-vector-search-with-marqo</link>
      <description><![CDATA[我正在使用 Marqo Cloud 为工作项目实施矢量搜索。我的文档（产品）有一些数据，其结构可以如下：
单个键值对，例如：标签：红色、斑点、尼龙、休闲
或者每个标签标题包含多个键值对，例如：
红色
设计：斑点
材质: 尼龙
风格：休闲
在矢量搜索中，这些数据结构中的一种会比另一种表现得更好吗？或者差异可能可以忽略不计？]]></description>
      <guid>https://stackoverflow.com/questions/77657071/single-or-multiple-key-value-pair-data-structure-for-vector-search-with-marqo</guid>
      <pubDate>Wed, 13 Dec 2023 23:34:17 GMT</pubDate>
    </item>
    <item>
      <title>8 位量化是否应该使 GPU 上的耳语推理速度更快？</title>
      <link>https://stackoverflow.com/questions/77656929/should-8bit-quantization-make-whisper-inference-faster-on-gpu</link>
      <description><![CDATA[我正在对拥抱脸变压器进行耳语推理。
load_in_8bit 量化由 bitsandbytes 提供。
如果在 NVIDIA T4 GPU 上以 8 位模式加载 Whisper-large-v3，则对示例文件的推理需要更长的时间 (5 倍)。 nvidia-smi 中的 GPU 利用率为 33%。
量化不应该提高 GPU 上的推理速度吗？
https://pytorch.org/docs/stable/quantization.html
类似问题：

https://discuss .huggingface.co/t/enabling-load-in-8bit-makes-inference-much-slower/38596

&lt;代码&gt;
进口火炬

从转换器导入 WhisperFeatureExtractor、WhisperTokenizerFast
从 Transformers.pipelines.audio_classification 导入 ffmpeg_read

MODEL_NAME =“openai/whisper-large-v3”

tokenizer = WhisperTokenizerFast.from_pretrained(MODEL_NAME)
feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)

model_8bit = AutoModelForSpeechSeq2Seq.from_pretrained(
     “openai/whisper-large-v3”，
    device_map=&#39;自动&#39;,
    load_in_8bit=真）

样本=“样本.mp3”；第27章 长

使用 torch.inference_mode()：
    将 open(sample, &quot;rb&quot;) 作为 f：
        输入 = f.read()
        输入= ffmpeg_read（输入，feature_extractor.sampling_rate）

        input_features = feature_extractor（输入，sampling_rate = feature_extractor.sampling_rate，return_tensors =&#39;pt&#39;）[&#39;input_features&#39;]

        input_features = torch.tensor(input_features, dtype=torch.float16, device=&#39;cuda&#39;)

        forced_decoder_ids_output = model_8bit.generate(input_features=input_features, return_timestamps=False)

        out = tokenizer.decode(forced_decoder_ids_output.squeeze())
        打印出）
]]></description>
      <guid>https://stackoverflow.com/questions/77656929/should-8bit-quantization-make-whisper-inference-faster-on-gpu</guid>
      <pubDate>Wed, 13 Dec 2023 22:43:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在文本分类中得到一长串零？</title>
      <link>https://stackoverflow.com/questions/77656547/why-do-i-get-a-long-list-of-zeros-in-classification-of-text</link>
      <description><![CDATA[我有 500 条来自 YouTube 的俄语评论。我使用 youtokentome 库对它们进行标记。
df[&#39;textOriginal&#39;].to_csv(&#39;text.txt&#39;, index=False, header=False)

model_path = &#39;tokenizer.model&#39;
yttm.BPE.train(data=&#39;text.txt&#39;, model=model_path, vocab_size=5000)

tokenizer = yttm.BPE(模型=模型路径)

df[&#39;tokens&#39;] = df[&#39;textOriginal&#39;].apply(lambda x: tokenizer.encode(x, output_type=yttm.OutputType.ID))

文本标记示例
接下来，我给出张量中的标记列表。
tokens_tensor = df[&#39;tokens&#39;].apply(lambda x: torch.tensor(x)).tolist()
tokens_tensor = torch.nn.utils.rnn.pad_sequence（tokens_tensor，batch_first=True）

接下来，我想将文本分为 3 类。为此，我使用 nn.Embedding+nn.LIST+ nn.Linear。
但是模型的返回值我不清楚。我得到一长串零。
如何获得对象的分类？
我的模型代码：
&lt;前&gt;&lt;代码&gt;embedding_dim = 300
词汇大小 = 5000
隐藏大小 = 512
输出暗度 = 3

导入 torch.nn.function 作为 F

类 MyModel(nn.Module):
    def __init__(自身、vocab_size、embedding_dim、hidden_​​size、output_dim、dropout_rate=0.5):
        超级（MyModel，自我）.__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_​​size=hidden_​​size,batch_first=True)
        self.线性 = nn.Linear(hidden_​​size, output_dim)


    defforward(self, input_seq):
        嵌入 = self.embedding(input_seq)
        lstm_out, _ = self.lstm(嵌入)
        lstm_out = lstm_out[:, -1, :]
        x = self.线性(lstm_out)
        返回 F.log_softmax(x, 暗淡=1)
]]></description>
      <guid>https://stackoverflow.com/questions/77656547/why-do-i-get-a-long-list-of-zeros-in-classification-of-text</guid>
      <pubDate>Wed, 13 Dec 2023 21:08:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么这个用于线性回归的 cpp 代码不能很好地工作，而 python 对应的代码却可以？我做错了什么[关闭]</title>
      <link>https://stackoverflow.com/questions/77656109/why-does-this-cpp-code-for-linear-regression-not-work-well-whereas-the-python-co</link>
      <description><![CDATA[所以我正在学习 Andrew ng 的机器学习课程，在学习了单个特征的线性回归之后，我试图让它在 cpp 中工作，但与按我预期工作的 python 代码相比，代码提供了非常奇怪的结果它...
#include ;
使用命名空间 std；

结构梯度结果{
    浮动w_结果；
    浮动b_结果；
};

结构计算梯度{
    浮动 dj_dw_结果；
    浮动 dj_db_结果；
};

ComputeGradient 计算(float x[], float y[], float w, float b, float s)
{
    浮动 dj_dw = 0.0;
    浮动 dj_db = 0.0;

    for (int i = 0; i &lt; s; i++)
    {
        浮点数 f_wb = w * x[i] + b;
        浮点数 dj_dw_i = (f_wb - y[i]) * x[i];
        浮点数 dj_db_i = (f_wb - y[i]);
        dj_dw += dj_dw_i;
        dj_db += dj_db_i;
    }
    dj_dw = dj_dw / s;
    dj_db = dj_db / s;

    计算梯度cResult；
    cResult.dj_dw_result = dj_dw;
    cResult.dj_db_result = dj_db;

    返回结果；
}

GradientResult 梯度下降(float w, float b, float x[], float y[], float s, int 迭代器, float alpha)
{
    ComputeGradient cResult = 计算(x, y, w, b, s);

    浮点数 local_w = w;
    浮点数 local_b = b;

    for (int i = 0; i &lt; 迭代器; i++)
    {
        浮动 dj_dw = cResult.dj_dw_result;
        浮动 dj_db = cResult.dj_db_result;
        local_w = local_w - alpha * dj_dw;
        local_b = local_b - alpha * dj_db;
    }

    GradientResult结果；
    结果.w_result = local_w;
    结果.b_result = local_b;

    返回结果；
}

int main()
{

    浮点数 xtrain[] = {2.4, 5.3, 9.7, 6.2, 13.6, 29.8};
    浮点 ytrain[] = {6.9, 14.3, 23.6, 17.8, 29.2, 42.6};

    浮点 s = sizeof(xtrain) / sizeof(float);
    浮动w_init = 0.0;
    浮动b_init = 0.0;
    int迭代= 1000000；
    浮动学习率= 0.0001；

    GradientResult 结果 = 梯度下降（w_init，b_init，xtrain，ytrain，s，迭代次数，learning_rate）;
    浮动 w = 结果.w_结果;
    浮动 b = 结果.b_结果;

    计算&lt;&lt; w &lt;&lt;结束&lt;&lt; b &lt;&lt;结束；
    
    for(int i = 0; i &lt; s; i++)
    {
        浮动 ans = w * xtrain[i] + b;
        计算&lt;&lt;一个&lt;&lt;结束；
    }

}

这是 cpp 代码，这是 python 代码：
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
从 lab_utils_uni 导入 plt_intuition、plt_stationary、plt_update_onclick、soup_bowl
plt.style.use(&#39;./deeplearning.mplstyle&#39;)

xtrain = np.array([2.4, 5.3, 9.7, 6.2, 13.6, 29.8])
ytrain = np.array([6.9, 14.3, 23.6, 17.8, 29.2, 42.6])

def计算成本（x，y，w，b）：
    成本=0
    m = x.shape[0]
    
    对于范围 (m) 内的 i：
        f_wb = w * x[i] + b
        成本 = 成本 + (f_wb - y[i])**2
    总成本 = 1/(2 * m) * 成本
    
    返回总成本

def 计算梯度(x, y, w, b):
    m = x.shape[0]
    dj_dw = 0
    DJ_数据库 = 0
    
    对于范围 (m) 内的 i：
        f_wb = w * x[i] + b
        dj_dw_i = (f_wb - y[i]) * x[i]
        dj_db_i = (f_wb - y[i])
        dj_dw += dj_dw_i
        dj_db += dj_db_i
    dj_dw = dj_dw/m
    dj_db = dj_db/m
    
    返回 dj_dw、dj_db

def梯度下降（x，y，w_in，b_in，alpha，迭代，成本函数，梯度函数）：
    本地_w = w_in
    本地_b = b_in
    wb_历史记录 = []
    
    对于范围内的 i（迭代）：
        dj_dw, dj_db = 梯度函数(x, y, local_w, local_b)
        
        local_w = local_w - (alpha * dj_dw)
        local_b = local_b - (alpha * dj_db)
        
        wb_history.append([local_w, local_b])
        
    返回 local_w、local_b、wb_history

w_init = 0
b_init = 0

迭代器 = 1000000
阿尔法 = 0.0001
wb_历史记录 = []

Final_w，final_b，wb_history = 梯度下降（xtrain，ytrain，w_init，b_init，alpha，迭代器，compute_cost，computeGradient）

对于范围 (6) 内的 i：
    print(final_w * xtrain[i] + Final_b, &quot;\n&quot;)
打印（最终_w，最终_b）


你能帮我找出我做错了什么吗
功能的输入是这样的：
2.4、5.3、9.7、6.2、13.6、29.8
与输出配对：
6.9、14.3、23.6、17.8、29.2、42.6
这是 python 的结果：
&lt;前&gt;&lt;代码&gt;11.737688309201088
15.264764800070795
20.616191200011038
16.35937474551312
25.359500963594435
45.062479981556244

这是 cpp 的结果：
&lt;前&gt;&lt;代码&gt;86530.3
188406
342975
220022
479980
1.04908e+006
]]></description>
      <guid>https://stackoverflow.com/questions/77656109/why-does-this-cpp-code-for-linear-regression-not-work-well-whereas-the-python-co</guid>
      <pubDate>Wed, 13 Dec 2023 19:26:27 GMT</pubDate>
    </item>
    <item>
      <title>Hessian 矩阵最后一层的特征值比其他层小得多[关闭]</title>
      <link>https://stackoverflow.com/questions/77655218/eigenvalues-of-hessian-matrix-final-layer-much-smaller-than-others</link>
      <description><![CDATA[我在各种简单的神经网络模型（例如前馈网络、LeNet CNN 和单层注意力模型）中遇到了 Hessian 特征值的一致模式。具体来说，最终分类层中的 Hessian 特征值非常小（小于 1e-7），与前面层中的值要大得多。有趣的是，模型深处的特征值大小似乎有增加的趋势，但在分类层突然减小。
这一观察似乎违反直觉，特别是考虑到较大的 Hessian 特征值应对应于最不可概括且对数据最敏感的层（请参阅 https://arxiv.org/pdf/1611.01838.pdf)。
我排除了一些事情：

这些模型正在有效地融合并产生具有竞争力的结果
基准数据集上的性能。
各个层的权重大小相似，不是特别大
最后一层很小。
我尝试过各种初始化，甚至合并了
LogSoftmax 非线性进入最后一层。

这是我计算粗麻布和特征值的方法：
defcompute_hessian（参数，损失）：
    “”“”计算给定参数和损失的 Hessian 矩阵。
    # 确保计算损失相对于参数的梯度
    first_grad = torch.autograd.grad(loss, param, create_graph=True)[0]
    dummy_param = torch.ones_like(param)
    hessian = torch.autograd.grad(first_grad, param, grad_outputs=dummy_param, create_graph=True)[0]
    返回粗麻布

def 计算特征值（hessian）：
    ”“”获取特征值“”
    #svd
    特征值 = torch.linalg.svdvals(hessian)
    sum_eigenevalues = torch.sum(特征值)
返回特征值，sum_eigenevalues.item()

作为背景，hessian 矩阵是损失对参数的二阶导数。它通常用于理解损失函数的形状。获取 hessian 的特征值意味着要么获取 hessian 的 SVD（其形状与参数相同），要么构造 (hessian.T hessian) 的语法矩阵。特征值揭示了损失景观的主曲率（即最大下降或上升的方向）。正特征值表明您处于该方向的局部最小值，负值意味着您可能会进一步下降。较小的特征值通常对应于损失景观中较平坦的区域，这通常与神经网络背景下更好的泛化相关。]]></description>
      <guid>https://stackoverflow.com/questions/77655218/eigenvalues-of-hessian-matrix-final-layer-much-smaller-than-others</guid>
      <pubDate>Wed, 13 Dec 2023 16:40:25 GMT</pubDate>
    </item>
    <item>
      <title>如何重新加入 scikit-learn Logistic 回归在相同索引处预测的概率？</title>
      <link>https://stackoverflow.com/questions/77655113/how-to-rejoin-scikit-learn-logistic-regression-predicted-probabilities-at-same-i</link>
      <description><![CDATA[我已经使用 scikit-learn 的逻辑回归成功地对一些数据运行了逻辑回归，我只想对附加到帧的数据帧进行预测。我有理由怀疑他们预测的概率没有被连接回他们应该加入的行。
这是发生回归的代码，以及将预测附加到原始数据帧的代码。
# 将预测变量 (ind_cols) 和响应变量分成两组以输入逻辑回归函数。

X = full_sample[ind_cols].to_pandas()

y = full_sample[dep_col].to_pandas()

lm = 逻辑回归（
    fit_intercept=真
）

lm.fit(X, y)

# 我想要预测的 DataFrame 是 ret_df，预测变量是 [ind_cols]。

y_pred = lm.predict_proba(ret_df[ind_cols].to_pandas())

y_final = pd.DataFrame(y_pred, columns=[&#39;Prob_0&#39;, &#39;Prob_1&#39;])

ret_df_out = pd.merge(ret_df.to_pandas(), y_final, how=&#39;left&#39;, left_index=True, right_index=True)

ret_df_out.show()

我认为这些值没有连接到相应的行的原因是因为我已经在 R Studio 中进行了精确的回归。在这里，当我查看正面和负面响应的预测值分布时，它们对于 scikit-learn 回归实际上是相同的。
预测是否有可能没有连接回其原始行？它可以解释真 0 和真 1 的预测值分布之间的相似性。]]></description>
      <guid>https://stackoverflow.com/questions/77655113/how-to-rejoin-scikit-learn-logistic-regression-predicted-probabilities-at-same-i</guid>
      <pubDate>Wed, 13 Dec 2023 16:24:42 GMT</pubDate>
    </item>
    <item>
      <title>最后一行的结果没有像我想象的那样出现[关闭]</title>
      <link>https://stackoverflow.com/questions/77654857/result-of-the-last-line-doesnt-appear-as-i-thought-it-would-be</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77654857/result-of-the-last-line-doesnt-appear-as-i-thought-it-would-be</guid>
      <pubDate>Wed, 13 Dec 2023 15:46:40 GMT</pubDate>
    </item>
    <item>
      <title>优化 url 的文本分析</title>
      <link>https://stackoverflow.com/questions/77654806/optimizing-text-analysis-from-urls</link>
      <description><![CDATA[我正在尝试获取一组网址，
分析他们的文本并返回这些 url 中的前 5 个主题。我正在尝试使用来自 google 的 c# 和 LanguageServiceClient 来做到这一点。
我遇到的问题是下载一大组网址的文本非常耗时，其次我不想单独调用谷歌来获取每页的主题，而是一个大的主题。
有什么建议吗？欢迎使用其他库或 API
我可以一项一项地完成，但如果我想做 10000 个以上的网址，则无法扩展]]></description>
      <guid>https://stackoverflow.com/questions/77654806/optimizing-text-analysis-from-urls</guid>
      <pubDate>Wed, 13 Dec 2023 15:38:14 GMT</pubDate>
    </item>
    <item>
      <title>Llama2 回归语言模型 (huggingface)</title>
      <link>https://stackoverflow.com/questions/77654285/llama2-language-model-for-regression-huggingface</link>
      <description><![CDATA[我尝试利用给定整个输入序列的模型的最后一个隐藏状态，调整 Llama2 来解决回归任务。
如果随后提出问题“2+2 的答案是什么”，则应回答 4（虚拟问题，用于解释问题）。&lt; /p&gt;
为此，我将在 pytorch 模型中使用它
导入火炬
将 torch.nn 导入为 nn
从 Transformer 导入 LlamaModel、LlamaTokenizer

类 TransformerModel(nn.Module):
    def __init__(self, 模型名称:str, 附加层大小:int = 1):
        super(TransformerModel, self).__init__()
        self.transformer = LlamaModel.from_pretrained(model_name, torch_dtype=torch.float32, cache_dir=“hugginface_cache/models”)
        self.tokenizer = LlamaTokenizer.from_pretrained(model_name,cache_dir=“hugginface_cache/tokenizer”)

        # 添加一个带有一个输出的附加层
        self.additional_layer = nn.Linear(self.transformer.config.hidden_​​size,additional_layer_size)
        
    defforward(self, input_text):
        # 对输入文本进行标记
        input_ids = self.tokenizer(input_text, return_tensors=“pt”).input_ids.to(“cuda”)
        打印（“输入ID：”，输入ID）

        # 获取变压器的输出
        输出 = self.transformer(input_ids)
        
        # 使用整个最后的隐藏状态作为附加层的输入
        最后隐藏状态 = 输出.最后隐藏状态
        打印（&#39;last_hidden_​​state_shape：&#39;，last_hidden_​​state.size（））

        # 应用附加层
        附加输出= self.附加层（最后隐藏状态）

        返回额外的输出


model_url = “meta-llama/Llama-2-7b-hf”

模型 = TransformerModel(model_url)

但是，对于给定的输入模型（“Hello world！”），输出是大小为 1,4,1 的张量。
我可以验证标记生成器是否将字符串拆分为 4 个标记，我预计这会导致问题。但是，我不确定如何解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77654285/llama2-language-model-for-regression-huggingface</guid>
      <pubDate>Wed, 13 Dec 2023 14:22:48 GMT</pubDate>
    </item>
    <item>
      <title>神经网络在时间序列中查找子序列？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77648688/neural-network-to-find-subsequence-in-time-series</link>
      <description><![CDATA[

蓝线代表时间序列
红色矩形子序列

是否可以通过机器学习方法找到这些子序列？
PS：这是合成数据，在真实数据中经典算法是无效的。这是一个例子：.
]]></description>
      <guid>https://stackoverflow.com/questions/77648688/neural-network-to-find-subsequence-in-time-series</guid>
      <pubDate>Tue, 12 Dec 2023 19:48:08 GMT</pubDate>
    </item>
    <item>
      <title>Python 进程因内存不足 256Gb 而被终止</title>
      <link>https://stackoverflow.com/questions/73888672/python-process-being-killed-due-to-out-of-256gb-memory</link>
      <description><![CDATA[训练数据集是一个 42GB JSON 文件。 mesh 是医学主题标题，将其视为 ID 或标签。 Neighbors_mesh 是一个 28,000 维列表，其中包含有关彼此接近的网格的信息。我们通过 KNN 训练 1.07 M 数据的网格项获得了这些数据。 MLB 拟合变换返回 0 和 1 的 28,000 维向量。但默认情况下每个元素都是 int64。我尝试通过 mask.astype(int__) 来减少它。仍然是 32 位。
运行大约 1M 次迭代后，迭代会阻塞 256GB 内存，但仍然会被杀死。
我的python版本是3.9
该机器拥有256GB内存、20GB交换内存、48核CPU和GPU。
def build_dataset（train_path，neighbors，journal_mesh，MeSH_id_pair_file，index_dic）：

    映射 ID = {}
    打开（MeSH_id_pair_file，&#39;r&#39;）作为f：
        对于 f 中的行：
            (键, 值) = line.split(&#39;=&#39;)
            映射_id[键] = value.strip()

    meshIDs = 列表(mapping_id.values())
    meshIDs = label2index(meshIDs,index_dic)
    meshIDs_str = [str(x) 表示 meshIDs 中的 x]

    print(&#39;标签总数%d&#39; % len(meshIDs_str))
    mlb = MultiLabelBinarizer(类=meshIDs_str)
    mlb.fit(meshIDs_str)

    pmid_neighbors，neighbors_mesh = read_neighbors（neighbors，index_dic）

    f = open(train_path, 编码=“utf8”)
    对象 = ijson.items(f, &#39;articles.item&#39;)
    

    数据集 = []
    print(&quot;对象：&quot;, type(对象))
    print(&quot;pmid 邻居：&quot;, type(pmid_neighbors))

    对于 i，枚举中的 obj(tqdm(objects))：
        数据点 = {}
        尝试：
            ids = obj[“pmid”]
            标题 = obj[&#39;标题&#39;].strip()
            标题 = header.translate(str.maketrans(&#39;&#39;, &#39;&#39;, &#39;[]&#39;))
            摘要 = obj[“abstractText”].strip()
            clean_abstract = Abstract.translate(str.maketrans(&#39;&#39;, &#39;&#39;, &#39;[]&#39;))
            if len(heading) == 0 或 header == &#39;处理中&#39;:
                print(&#39;论文&#39;, ids, &#39;没有标题！&#39;)
                继续
            elif len(clean_abstract) == 0:
                print(&#39;论文&#39;, ids, &#39;没有摘要！&#39;)
                继续
            别的：
                mesh_id = obj[&#39;网格&#39;]
                日记 = obj[&#39;日记&#39;]
                年 = obj[&#39;年&#39;]
                mesh_from_journal=journal_mesh[journal]
                mesh_from_neighbors = []
                如果我&lt; len(pmid_neighbors) 和 ids == pmid_neighbors[i]:
                    mesh_from_neighbors = Neighbors_mesh[i]
                mesh_from_journal_str = [str(x) 表示 mesh_from_journal 中的 x]
                mesh_from_neighbors_str = [str(x) for x in mesh_from_neighbors]
                网格=列表（设置（mesh_from_journal_str + mesh_from_neighbors_str））
                mask = mlb.fit_transform([网格])
                掩码 = mask.astype(np.int_)
                掩码 = mask.tolist()
                print(&quot;网格大小：&quot;, sys.getsizeof(mask))
                print(&quot;网格内容大小：&quot;, sys.getsizeof(mask[0][0]))
                print(&quot;网格内容类型：&quot;, type(mask[0][0]))
                data_point[&#39;pmid&#39;] = id
                data_point[&#39;标题&#39;] = 标题
                data_point[&#39;abstractText&#39;] = clean_abstract
                data_point[&#39;meshID&#39;] = mesh_id
                data_point[&#39;meshMask&#39;] = 掩码
                data_point[&#39;年份&#39;] = 年份
                数据集.append(data_point)
                print(&quot;数据集大小：&quot;, sys.getsizeof(dataset))
        

        除了属性错误：
            print(f&#39;pmid 发生异常: {obj[&quot;pmid&quot;].strip()}&#39;, AttributeError.args())


    pubmed = {&#39;文章&#39;: 数据集}
    返回发布
]]></description>
      <guid>https://stackoverflow.com/questions/73888672/python-process-being-killed-due-to-out-of-256gb-memory</guid>
      <pubDate>Wed, 28 Sep 2022 23:06:08 GMT</pubDate>
    </item>
    <item>
      <title>如果我的模型在最后一层使用 sigmoid 和二元交叉熵进行训练，我可以输出类的概率而不是 0/1 吗？</title>
      <link>https://stackoverflow.com/questions/70159955/if-my-model-is-trained-using-sigmoid-at-the-final-layer-and-binary-crossentropy</link>
      <description><![CDATA[我使用 sigmoid 函数训练了一个最后带有密集层的 CNN 模型：
model.add(layers.Dense(1,activation=&#39;sigmoid&#39;))

我还使用二进制交叉熵进行了编译：
model.compile(loss=&#39;binary_crossentropy&#39;,
              优化器 = &#39;亚当&#39;,
              指标=[tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),&#39;准确性&#39;])

二值图像分类的 f1 分数较低，我的模型预测一个类别优于另一个类别。所以我决定根据我的 sigmoid 函数在最后一层的输出概率添加一个阈值：
c = load_img(&#39;/home/kenan/Desktop/COV19D/validation/covid/ct_scan_19/120.jpg&#39;,
             color_mode=&#39;灰度&#39;,
             目标大小 = (512,512))
c=img_to_array(c)
c= np.expand_dims(c, 轴=0)
pred = model.predict_proba(c)
预测
y_classes = ((model.predict(c)&gt; 0.99)+0).ravel()
y_类

我想在代码中使用“pred”作为该类的概率，但它始终为 0 或 1，如下所示：
Out[113]: array([[1.]], dtype=float32)

为什么它不给出预测 [0,1] 之间类别的概率而不是 1？有没有办法获得我的情况下的类概率而不是 0 或 1？]]></description>
      <guid>https://stackoverflow.com/questions/70159955/if-my-model-is-trained-using-sigmoid-at-the-final-layer-and-binary-crossentropy</guid>
      <pubDate>Mon, 29 Nov 2021 18:56:08 GMT</pubDate>
    </item>
    <item>
      <title>gTTS 中没有声音</title>
      <link>https://stackoverflow.com/questions/63464494/no-sound-in-gtts</link>
      <description><![CDATA[我正在尝试使用 gTTS 将文本转换为语音。
导入子流程
从 gtts 导入 gTTS

mytext = &#39;Rasa Bot 用户您好，我是机器人&#39;
语言=&#39;en&#39;
myobj = gTTS(文本 = mytext, lang=语言)
myobj.save(“welcome.mp3”)
subprocess.call([&#39;mpg321&#39;,&#39;welcome.mp3&#39;,&#39;--play-and-exit&#39;])


但是我好像听不到任何声音。我在 Ubuntu 中使用 PyCharm 执行此操作。
终端内容如下：
(venv) rome@rome-VirtualBox:~/Desktop/rasa/intr2$ python ttos.py
mpg321：无法识别的选项“--play-and-exit”
适用于第 1、2 和 3 层的高性能 MPEG 1.0/2.0/2.5 音频播放器。
版本0.3.2-1（2012/03/25）。乔·德鲁 (Joe Drew) 撰写并拥有版权，
现在由 Nanakos Chrysostomos 等人维护。
使用不同人的代码。请参阅“自述文件”了解更多信息！
本软件绝对不提供任何保证！使用风险自负！

正在播放welcome.mp3 中的MPEG 流...
MPEG 2.0 第三层，32 kbit/s，24000 Hz 单声道

请帮忙！！]]></description>
      <guid>https://stackoverflow.com/questions/63464494/no-sound-in-gtts</guid>
      <pubDate>Tue, 18 Aug 2020 08:15:35 GMT</pubDate>
    </item>
    <item>
      <title>设置分类器参数，无需拟合即可使用</title>
      <link>https://stackoverflow.com/questions/48252006/set-parameters-for-classifier-and-use-it-without-fitting</link>
      <description><![CDATA[我正在使用 python 和 scikit-learn 进行一些分类。
是否可以重用分类器学习到的参数？
例如：
从 sklearn.svm 导入 SVC

cl = SVC(...) # 使用一些超参数创建 svm 分类器
cl.fit(X_train, y_train)
参数 = cl.get_params()

让我们将这个 params 作为字符串字典存储在某处，甚至写入 json 文件。假设，我们稍后想要使用这个经过训练的分类器对某些数据进行一些预测。尝试恢复它：
params = ... # 检索以字典形式存储在某处的这些参数
data = ... # 我们要预测的数据
cl = SVC(...)
cl.set_params(**参数)
预测 = cl.predict(数据)

如果我这样做，我会得到 NonFittedError 和以下堆栈跟踪：
文件“C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py”，第 548 行，在预测中
    y = super(BaseSVC, self).predict(X)
  文件“C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py”，第 308 行，在预测中
    X = self._validate_for_predict(X)
  文件“C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py”，第 437 行，在 _validate_for_predict 中
    check_is_fitted(自我,&#39;support_&#39;)
  文件“C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\utils\validation.py”，第 768 行，在 check_is_fitted 中
    引发 NotFittedError(msg % {&#39;name&#39;: type(estimator).__name__})
sklearn.exceptions.NotFittedError：此 SVC 实例尚未安装。在使用此方法之前，请使用适当的参数调用“fit”。

是否可以为分类器设置参数并在不拟合的情况下进行预测？我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/48252006/set-parameters-for-classifier-and-use-it-without-fitting</guid>
      <pubDate>Sun, 14 Jan 2018 17:04:03 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI 健身房玩家模式</title>
      <link>https://stackoverflow.com/questions/46762083/openai-gym-player-mode</link>
      <description><![CDATA[有谁知道如何作为玩家运行 OpenAI 健身房环境之一。就像让人类玩家玩一轮车竿一样？我已经看到有 env.mode = &#39; human&#39; 但我无法让它正常运行。我尝试按照此处给出的示例 https://www.pinchofintelligence.com/getting -started-openai-gym/ 但它似乎对我不起作用。
如果您能提供任何帮助，我们将不胜感激。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/46762083/openai-gym-player-mode</guid>
      <pubDate>Mon, 16 Oct 2017 02:21:38 GMT</pubDate>
    </item>
    </channel>
</rss>