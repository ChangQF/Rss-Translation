<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Sun, 13 Apr 2025 04:26:58 GMT</lastBuildDate>
    <item>
      <title>从本地模型创建一个全局模型</title>
      <link>https://stackoverflow.com/questions/79571154/create-a-global-model-from-local-models</link>
      <description><![CDATA[ 当前场景： 
所以我有一项任务。我有一个具有时间戳，org_id，no_of_calls_on_premise，no_of_of_calls_cloud，bw_savings的数据。这是每天汇总的数据
（我也将时间戳分开，而不是直接将其用于诸如Day，Day_of_week，is_weekend，Quarter等各个领域））
现在，我已经训练了XGBoost，Random Forest和Sarimax模型等不同模型，它们具有不错的测试准确性。这项培训是对2024年的数据进行的。
训练后，我使用每个模型使用上述变量从所有3个模型中的所有模型中预测BW_SAVINGS，用于1月，2月和3月。现在，由于我已经是BW_SAVINGS，所以我可以从所有3个中获得最佳预测值，但是我做了很多组织的R2分数小于0。
现在，我也必须部署此模型，这里的数据将用于没有现有bw_savings的新组织，因此无法确认我的模型是否在几个月内表现出色。目前，我从上述模型中获取3个预测值的中位数。
 问题： 
因此，这里缺少的真正的是，这些模型正在同时培训组织的所有数据，这导致缺失趋势和不同的组织具有不同的趋势。即使我们离开趋势，每个组织的呼叫规模也会有所不同。这似乎是一个问题，因为R2得分为负。提高我的算法的任何建议。类似它为每个组织训练一个一个org，并将单个培训结合到全球模型中，以便它可以掌握每个组织中不同尺度的概念 ]]></description>
      <guid>https://stackoverflow.com/questions/79571154/create-a-global-model-from-local-models</guid>
      <pubDate>Sun, 13 Apr 2025 02:45:29 GMT</pubDate>
    </item>
    <item>
      <title>简单线性回归模型的剩余分析</title>
      <link>https://stackoverflow.com/questions/79571067/residual-analysis-for-simple-linear-regression-model</link>
      <description><![CDATA[我正在尝试进行简单线性回归的残差分析。我需要证明残差遵循近似正态分布。
我正在使用的CSV文件具有10年级分数百分比的值，而学生的薪水是
运行以下代码后，我的情节看起来像这样：
  
书中的情节看起来像这样：
 我期望我的情节像书一样出现，因为数据是相同的。我已经进行了仔细检查以确保我不会丢失任何数据等。
 数据如下：

薪水百分比
62 270000
76.33 200000
72 240000
60 250000
61 180000
55 300000
70 260000
68 235000
82.8 425000
59 240000
58 250000
60 180000
66 428000
83 450000
68 300000
37.33 240000
79 252000
68.4 280000
70 231000
59 224000
63 120000
50 260000
69 300000
52 120000
49 120000
64.6 250000
50 180000
74 218000
58 360000
67 150000
75 250000
60 200000
55 300000
78 330000
50.08 265000
56 340000
68 177600
52 236000
54 265000
52 200000
76 393000
64.8 360000
74.4 300000
74.5 250000
73.5 360000
57.58 180000
68 180000
69 270000
66 240000
60.8 300000
 
代码如下：
 ＃导入所有必需的库以构建回归模型
导入熊猫作为pd导入numpy作为np
导入statsmodels.api作为sm
来自sklearn.model_selection导入train_test_split

＃将数据集加载到数据框中
mba_salary_df = pd.read_csv（&#39;MBA Salary.csv&#39;）

＃将1的常数项添加到数据集
x = sm.add_constant（MBA_SALARY_DF [&#39;10年级的百分比]）

＃分别将数据集分别为火车和测试设置分别为80:20
train_x，test_x，train_y，test_y = train_test_split（x，y，train_size = 0.8，andural_state = 100）

＃适合回归模型
mba_salary_lm = sm.ols（train_y，train_x）.fit（）

mba_salary_resid = mba_salary_lm.resid 

probplot = sm.probplot（mba_salary_resid） 

plt.figure（无花果=（8，6）） 

probplot.ppplot（line = &#39;45&#39;） 

plt.title（回归标准化残差的正常p-p图；） 

plt.show（）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79571067/residual-analysis-for-simple-linear-regression-model</guid>
      <pubDate>Sun, 13 Apr 2025 00:11:50 GMT</pubDate>
    </item>
    <item>
      <title>如何从传感器数据中正确实现滑动窗口以实时活动识别？</title>
      <link>https://stackoverflow.com/questions/79570885/how-to-properly-implement-sliding-windows-for-real-time-activity-recognition-fro</link>
      <description><![CDATA[我们正在使用可穿戴传感器（加速度计和陀螺仪）将连续数据发送到Firebase的实时秋季检测系统。我们的机器学习模型接受了以80 Hz收集的20秒数据窗口的培训（每个窗口总计1600个样本）。主要的挑战是正确地实现滑动窗口逻辑以连续收集传感器数据并进行实时预测。
我们当前的方法涉及对新批次的传感器数据进行持续轮询燃料，并将其归一化，并在最大大小为1600的Deque中缓冲这些样品。一旦缓冲液达到1600个样品，我们将进行预测并相应地重置缓冲区。&gt; 。。
但是，预测不准确，我们怀疑我们的滑动窗户或缓冲方法可能存在缺陷。
这是一个简化的片段，说明了我们实施的相关部分：
  buffer_size = 1600＃20秒窗口在80 Hz处
step_interval = 5＃每5秒预测一次
data_buffer = deque（maxlen = buffer_size）
last_prediction_time = time.time（）

而真：
    样本= fetch_from_firebase（）＃获取新数据批次（每秒80个样本）

    对于样本中的样本：
        ＃归一化并将每个传感器样品附加到缓冲区
        读取= np.array（[
            示例[&#39;ax&#39;]，样本[&#39;ay&#39;]，样本[&#39;az&#39;]，
            样本[&#39;gx&#39;]，样本[&#39;gy&#39;]，样品[&#39;gz&#39;]
        ]） / 10000.0
        data_buffer.append（读取）

    ＃预测逻辑（缓冲区填充后每个step_interval秒）
    current_time = time.time（）
    如果len（data_buffer）== buffer_size和（current_time -last_prediction_time）＆gt; = step_interval：
        窗口= np.array（data_buffer）.RESHAPE（1，-1）
        window_scaled = sualer.transform（窗口）
        预测= model.predict（window_scaled）[0]

        last_prediction_time = current_time
        handing_prediction（预测）
 

这是实现连续滑动窗口的正确方法
实时传感器数据？
我们目前可以处理缓冲区或计时间隔的方法
导致预测不准确？
是否有更好的实践或替代策略
有效预测的实时传感器数据？
]]></description>
      <guid>https://stackoverflow.com/questions/79570885/how-to-properly-implement-sliding-windows-for-real-time-activity-recognition-fro</guid>
      <pubDate>Sat, 12 Apr 2025 19:59:33 GMT</pubDate>
    </item>
    <item>
      <title>预测时TensorFlow给出了错误：输入形状的预期轴-1具有值2034，但以形状接收到输入（无，118336）</title>
      <link>https://stackoverflow.com/questions/79569631/tensorflow-gives-error-when-predicting-expected-axis-1-of-input-shape-to-have</link>
      <description><![CDATA[我一直在尝试创建一个可以识别图像的神经网络，但是当我尝试适合我的模型时，我会收到以下错误：
  valueerror：调用sequenty.call（）时遇到异常。 
输入0层“密度_82”与该层不兼容：预期轴-1 
具有值2304的输入形状，但以形状接收到输入（无，118336）
 
我正在通过合作构建它，我正在使用的代码如下：
 导入numpy作为NP
导入matplotlib.pyplot作为PLT
导入TensorFlow作为TF
来自tensorflow.keras.models导入顺序
来自tensorflow.keras.layers导入conv2d，maxpooling2d，扁平，密集
来自Tensorflow.keras.utils导入到_categorical

从Google.Colab Import Drive
从google.colab.patches导入cv2_imshow
drive.mount（&#39;/content/drive&#39;）


img_height = 180
img_width = 180

normalization_layer = tf.keras.layers.Rescaling（1./255）
normalized_ds = triending.map（lambda x，y ：（ a rustaranization_layer（x），y​​））

autotune = tf.data.autotune

训练=训练。
testing = testing.cache（）。prefetch（buffer_size = autotune）

num_classes = 9

模型=顺序（[
  conv2d（32，（3，3），激活=&#39;relu&#39;，input_shape =（32，32，3）），
  maxpooling2d（2，2），
  conv2d（64，（3，3），激活=&#39;relu&#39;），，
  maxpooling2d（2，2），
  flatten（），
  密集（128，激活=&#39;relu&#39;），
  密集（num_classes，激活=&#39;softmax&#39;）
）））

model.compile（优化器=&#39;adam&#39;，loss =&#39;apcorical_crossentropopy&#39;，量表= [&#39;准确性&#39;]）

model.summary（）

历史= model.fit（训练，batch_size = 64，epochs = 20，验证_data =测试）
 
训练和测试变量是使用创建的预取介台
  tf.keras.utils.image_dataset_from_directory
 
如何解决此错误。]]></description>
      <guid>https://stackoverflow.com/questions/79569631/tensorflow-gives-error-when-predicting-expected-axis-1-of-input-shape-to-have</guid>
      <pubDate>Fri, 11 Apr 2025 20:02:06 GMT</pubDate>
    </item>
    <item>
      <title>如何根据命名级别设计卷积神经网络[封闭]</title>
      <link>https://stackoverflow.com/questions/79569309/how-to-design-a-convolutional-neural-network-based-on-the-nomenclatural-rank</link>
      <description><![CDATA[我正在尝试弄清楚如何设计CNN结构，该体系结构可以使用其命名级别的等级对超过2,000种动物进行分类。这意味着我的模型将在分类层次结构的每个级别上预测图像的类别，从而逐渐缩小分类。例如，如果我有一张猫的图片，我的模型将首先将其分类为属于门的门，然后将其缩小到哺乳动物类，然后是食肉动物的顺序，依此类推，直到达到最终水平，即物种。我最初的想法是为每个级别创建多个CNN模型，但事实证明这很耗时。有人对我如何创建这种体系结构有任何想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79569309/how-to-design-a-convolutional-neural-network-based-on-the-nomenclatural-rank</guid>
      <pubDate>Fri, 11 Apr 2025 16:20:56 GMT</pubDate>
    </item>
    <item>
      <title>如何使用随机森林来预测数据集中的空白？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79567319/how-do-i-use-a-random-forest-to-predict-gaps-in-a-dataset</link>
      <description><![CDATA[我有一个用来制作一个随机森林的数据集（分为测试和培训数据）。我已经做了随机的森林并产生了预测（下面的代码），但是我不知道如何进行这些预测并使用它们来生成一个完整的数据表，其中包含缝隙填充值。
  #DATA表头
时间戳＆lt;  -  c（2019-05-31 17：00：00：00：00 2019-05-31 17：30：00：00 2019-05-31 18：00：00：00：00：00：00：00：00：00：00：00 2019-05-31 18：00：30：00：00：00：00：00＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot 2019-05-31 19：30：00； 2019-05-31 20：00：00：00：00 2019-05-31 20：30：30：00 2019-05-05-05-31 21：00：00：00：00：00：00：00：00;
RH＆LT; -c（38、40、41、42、44、49、65、72、74、77）
FCH4＆lt;  -  C（0.045，-0.002，0.001，0.004，0，-0.013，0.004，-0.003，-0.001，-0.002）
距离＆lt;  -  c（1000,1000,180,125.35,1000,180,1000,5.50,180,1000）
ta＆lt; -c（29.52，29.01，29.04，28.39，27.87，26.68，23.28，21.16，19.95，19.01）
fe＆lt;  -  c（95.16，68.95，68.62，39.24，35.04，27.26，-2.60，5.09,7.28，2.08）

dd＆lt;  -  data.Frame（Timestamp，RH，FCH4，距离，TA，FE）

＃制造RF和预测
set.seed（1）
Intraining＆lt;  -  CreateTataPartition（DD $ FCH4，P = 0.65，List = false）
训练＆lt;  -  dd [intraining，]
测试＆lt;  -  dd [ -  intraining，]

set.seed（1）
pfpfit＆lt;  -  Randomforest（FCH4〜。，训练，NTREE = 500，type =＆quot&#39;回归＆quot;）
预测＆lt;  - 预测（pfpfit，newdata =测试）
 
因此，对于上述代码，我有预测模型，但是我不知道如何将其应用于我已经拥有差距的数据集（下面）。我也不知道在变量中差距不是我要差距填充的变量（我想差距填充FCH4）是否有问题，但是我也有FE和TA的空白）。
我想差距填充数据集的一个示例如下：
  #DATA表头
    时间戳＆lt;  -  c（2019-05-31 17：00：00：00：00 2019-05-31 17：30：00：00 2019-05-31 18：00：00：00：00：00：00：00：00：00：00：00 2019-05-31 18：00：30：00：00：00：00：00＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot 2019-05-31 19：30：00； 2019-05-31 20：00：00：00：00 2019-05-31 20：30：30：00 2019-05-05-05-31 21：00：00：00：00：00：00：00：00;
    RH＆LT; -c（38、40、41、42、44、49、65、72、74、77）
    FCH4＆lt; -c（Na，-0.002，0.001，0.004，Na，-0.013，0.004，Na，-0.001，-0.002）
    距离＆lt;  -  c（1000,1000,180,125.35,1000,180,1000,5.50,180,1000）
    ta＆lt; -c（29.52，29.01，NA，28.39，27.87，26.68，23.28，NA，19.95，19.01）
    fe＆lt; -c（Na，Na，68.62，39.24，35.04，27.26，-2.60，Na，7.28，2.08）
dd＆lt;  -  data.Frame（Timestamp，RH，FCH4，距离，TA，FE）
 
我希望填充的数据集看起来像这样：
  #DATA表头
    时间戳＆lt;  -  c（2019-05-31 17：00：00：00：00 2019-05-31 17：30：00：00 2019-05-31 18：00：00：00：00：00：00：00：00：00：00：00 2019-05-31 18：00：30：00：00：00：00：00＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot 2019-05-31 19：30：00； 2019-05-31 20：00：00：00：00 2019-05-31 20：30：30：00 2019-05-05-05-31 21：00：00：00：00：00：00：00：00;
    RH＆LT; -c（38、40、41、42、44、49、65、72、74、77）
    FCH4＆lt;  -  C（0.045，-0.002，0.001，0.004，0，-0.013，0.004，-0.003，-0.001，-0.002）
    距离＆lt;  -  c（1000,1000,180,125.35,1000,180,1000,5.50,180,1000）
    ta＆lt; -c（29.52，29.01，29.04，28.39，27.87，26.68，23.28，21.16，19.95，19.01）
    fe＆lt;  -  c（95.16，68.95，68.62，39.24，35.04，27.26，-2.60，5.09,7.28，2.08）
dd＆lt;  -  data.Frame（Timestamp，RH，FCH4，距离，TA，FE）
 
（我意识到这三个数据集大多是相同的，并且您不应该在与培训数据相同的数据上测试。这仅仅是为了使某些内容运行。我可以自己完善它。））]]></description>
      <guid>https://stackoverflow.com/questions/79567319/how-do-i-use-a-random-forest-to-predict-gaps-in-a-dataset</guid>
      <pubDate>Thu, 10 Apr 2025 18:12:26 GMT</pubDate>
    </item>
    <item>
      <title>如何将不同长度的多视图特征融合以进行关系提取（例如，生物Biobert +其他功能）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79567194/how-to-fuse-multi-view-features-of-different-lengths-for-relation-extraction-e</link>
      <description><![CDATA[我正在研究一项关系提取任务，特别是药物 - 药物相互作用（DDI）提取。我从多个功能来源提取了嵌入：

  biobert嵌入式（令牌级，可变长度取决于句子）

 其他功能（例如，POS标签，依赖关系路径或N-gram功能），其长度可能与Biobert输出不同。


我尝试了一种简单的技术，在汇总生物伯特和其他功能之后，我只是串联了这些功能。然后，我将合并的向量传递给多层感知器（MLP）进行分类，但并不能提高性能。
我相信这可能是由于不正确的对准或融合策略不佳所致。
我的问题是：从不同视图（和长度）中融合功能的某些常见或最佳实践技术以进行此类关系提取任务？]]></description>
      <guid>https://stackoverflow.com/questions/79567194/how-to-fuse-multi-view-features-of-different-lengths-for-relation-extraction-e</guid>
      <pubDate>Thu, 10 Apr 2025 16:43:17 GMT</pubDate>
    </item>
    <item>
      <title>我刚刚训练，使用keras image_from_dataset训练的事实会影响我的指标</title>
      <link>https://stackoverflow.com/questions/79566385/does-the-fact-that-i-have-just-train-test-split-using-keras-image-from-dataset</link>
      <description><![CDATA[我一直在试图弄清楚为什么训练后我的指标很低。我的F1得分为75％，这根本不是我所期望的。我审查了代码，我怀疑问题是火车，验证拆分，但我正在评估验证集的模型 y_pred =（model.predict（val_ds）＆gt; 0.5）.astype（int）（int）。我阅读了KERAS文档，以查看我是否犯了错误，或者是否有办法将数据集拆分为火车，验证，测试拆分，但我找不到任何可能指出是否犯错的东西。这是我的代码：
 将TensorFlow导入为TF
来自Tensorflow.keras.applications导入Densenet169
来自tensorflow.keras.layers导入密集，globalaveration -pooling2d，Randomflip，RandomRotation，辍学
来自Tensorflow.keras.models导入模型，顺序
来自Tensorflow.keras.optimizer导入Adam
从tensorflow.keras.regulinizer导入l2
来自tensorflow.keras.applications.densenet导入preprocess_input
来自sklearn.metrics导入混淆_matrix，f1_score
导入numpy作为NP

＃定义常数
img_size =（224，224）＃匹配densenet169输入大小
batch_size = 32
时代= 15
data_dir =＆quot; colon_images＆quot;
class_names = [＆quast; cancyous&#39;&#39;正常＆quot;]
split_ratio = 0.2

＃直接加载数据集在224x224
def create_dataset（子集）：
    返回tf.keras.utils.image_dataset_from_directory（
        data_dir，
        验证_split = split_ratio，
        子集=子集
        种子= 42，
        image_size = img_size，＃直接以目标大小加载
        batch_size = batch_size，
        label_mode =&#39;binary&#39;
    ）

train_ds = create_dataset（“训练”）
val_ds = create_dataset（&#39;验证＆quot;）

＃通过增强进行预处理（仅在培训期间活跃）
预处理=顺序（[
    Randomflip（“水平”，“），
    随机旋转（0.1），
    tf.keras.layers.lambda（preprocess_input）＃正确归一化
）））

＃构建模型
base_model = densenet169（
    权重=&#39;Imagenet&#39;，
    include_top = false，
    input_shape = img_size +（3，）
）

输入= tf.keras.input（shape = img_size +（3，））
X =预处理（输入）
x = base_model（x）
x = globalaveragepooling2d（）（x）
x =密集（512，激活=&#39;relu&#39;，kernel_regularizer = l2（0.01））（x）
x =辍学（0.5）（x）＃正则化
输出=密集（1，激活=&#39;Sigmoid&#39;）（x）
模型=模型（输入，输出）

＃阶段1：火车顶层
base_model.trainable = false
model.compile（
    优化器= ADAM（1E-3），
    损失=&#39;binary_crossentropy&#39;，
    量表= [&#39;fecycy&#39;，tf.keras.metrics.auc（name =&#39;auc&#39;），
             tf.keras.metrics.precision（name =&#39;precision&#39;），
             tf.keras.metrics.Recall（name =&#39;recember&#39;）]
）

＃用回调监视AUC的火车
早期_stop = tf.keras.callbacks.earlystopping（
    Monitor =&#39;Val_auc&#39;，耐心= 3，模式=&#39;max&#39;，详细= 1
）
检查点= tf.keras.callbacks.modelcheckpoint（
    &#39;best_model.h5&#39;，save_best_only = true，monitor =&#39;val_auc&#39;，mode =&#39;max&#39;
）

历史= model.fit（
    train_ds，
    验证_data = val_ds，
    时代= epochs，
    回调= [早期_STOP，检查点]
）

＃阶段2：微调整个模型
base_model.trainable = true
model.compile（
    优化器= ADAM（1E-5），＃非常低的学习率
    损失=&#39;binary_crossentropy&#39;，
    量表= [&#39;facer&#39;，tf.keras.metrics.auc（name =&#39;auc&#39;）]
）

history_fine = model.fit（fit）（
    train_ds，
    验证_data = val_ds，
    时代= epochs，
    onirome_epoch = history.epoch [-1]，
    回调= [早期_STOP，检查点]
）

＃ 评估
y_pred =（model.predict（val_ds）＆gt; 0.5）.astype（int）
y_true = np.concatenate（[y for _，y in val_ds]，axis = 0）

打印（f＆quot f1分数：{f1_score（y_true，y_pred）：。3f}＆quot;）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79566385/does-the-fact-that-i-have-just-train-test-split-using-keras-image-from-dataset</guid>
      <pubDate>Thu, 10 Apr 2025 10:29:08 GMT</pubDate>
    </item>
    <item>
      <title>模块“ keras.layers”没有属性“实验”</title>
      <link>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</link>
      <description><![CDATA[您好，我试图调整大小和重新列出我的数据集，如下所示，但我遇到了此错误：
 attributeError：模块&#39;keras.layers&#39;没有属性&#39;实验&#39;
 
resize_and_rescale = tf.keras.Sequeential（[
    layers.experiment.preprocessing.resizing（image_size，image_size），
    layers.expermentim.preprocessing.Rescaling（1.0/255）
）））

 ]]></description>
      <guid>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</guid>
      <pubDate>Wed, 14 Dec 2022 00:43:49 GMT</pubDate>
    </item>
    <item>
      <title>批归一化，是还是否？ [关闭]</title>
      <link>https://stackoverflow.com/questions/58612783/batch-normalization-yes-or-no</link>
      <description><![CDATA[我使用TensorFlow 1.14.0和Keras 2.2.4。以下代码实现了一个简单的神经网络：
 导入numpy作为NP
np.random.seed（1）
导入随机
随机。种子（2）
导入TensorFlow作为TF
tf.set_random_seed（3）

来自Tensorflow.keras.models导入模型，顺序
来自tensorflow.keras.layers导入输入，密集，激活


x_train = np.random.normal（0,1，（100,12））

型号=顺序（）
ADD（密集（8，Input_Shape =（12，））））
＃model.add（tf.keras.layers.batchnormalization（））
ADD（激活（&#39;Linear&#39;））
型号（密集（12））
ADD（激活（&#39;Linear&#39;））
model.compile（优化器=&#39;adam&#39;，loss =&#39;mean_squared_error&#39;）
model.fit（x_train，x_train，epochs = 20，验证_split = 0.1，shuffle = false，derbose = 2）
 
 20个时期后的最终val_loss为0.7751。当我删除添加批处理标准化层的唯一评论行时，val_loss更改为1.1230。
我的主要问题更为复杂，但是发生同样的事情。由于我的激活是线性的，因此是否在激活之后或之前将批处理归一化都无关紧要。 
 问题：为什么批处理归一化无济于事？我有什么可以更改的，以便批处理归一化可以改善结果而不更改激活功能？
 发表评论后更新： 
一个带有一个隐藏层和线性激活的NN有点像PCA。有很多论文。对我来说，此设置在隐藏层和输出的激活函数的所有组合中给出了最小的MSE。
一些陈述线性激活的资源是指PCA：
  https://arxiv.org/pdf/pdf/1702.07800.pdf    
   https：////www.quora.com/www.quora.com/www.quora.com/www.quora/]]></description>
      <guid>https://stackoverflow.com/questions/58612783/batch-normalization-yes-or-no</guid>
      <pubDate>Tue, 29 Oct 2019 17:38:38 GMT</pubDate>
    </item>
    <item>
      <title>何时需要替换实体提取？</title>
      <link>https://stackoverflow.com/questions/52580818/when-is-entity-replacement-necessary-for-relation-extraction</link>
      <description><![CDATA[在此中作者确实更换实体，因为“我们不希望模型根据特定实体名称学习，但我们希望它根据文本的结构学习”。
这通常是正确的，还是取决于数据集或使用的模型？]]></description>
      <guid>https://stackoverflow.com/questions/52580818/when-is-entity-replacement-necessary-for-relation-extraction</guid>
      <pubDate>Sun, 30 Sep 2018 18:22:13 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们需要时代？</title>
      <link>https://stackoverflow.com/questions/42716181/why-do-we-need-epochs</link>
      <description><![CDATA[在课程中没有任何关于时代的信息，但实际上它们无处不在。
如果优化器在一次通过中找到最佳的重量，为什么我们需要它们。为什么该模型改进？]]></description>
      <guid>https://stackoverflow.com/questions/42716181/why-do-we-need-epochs</guid>
      <pubDate>Fri, 10 Mar 2017 10:33:20 GMT</pubDate>
    </item>
    <item>
      <title>分类和聚类的特征之间的关系[封闭]</title>
      <link>https://stackoverflow.com/questions/42172920/relation-between-features-for-classification-and-clustering</link>
      <description><![CDATA[假设我已经对某些数据实现了分类算法，并确认了分类算法的最佳功能组合。如果有一天我从同一资源中获取数据，而这些数据缺乏以前的分类任务中的目标功能，我可以将功能的最佳组合直接用于聚类任务吗？ （我知道我可以使用我训练的模型来预测数据的目标，但是我只想知道分类和聚类算法之间的最佳功能组合是否相同）
我已经搜索了网站和我所知道的任何资源，但是我找不到问题的答案。有人可以告诉我还是只是给我一个链接？]]></description>
      <guid>https://stackoverflow.com/questions/42172920/relation-between-features-for-classification-and-clustering</guid>
      <pubDate>Sat, 11 Feb 2017 06:13:45 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络中的分批归一化[封闭]</title>
      <link>https://stackoverflow.com/questions/38553927/batch-normalization-in-convolutional-neural-network</link>
      <description><![CDATA[我想知道有关在CNN中应用批处理标准化的一些细节。
I read this paper https://arxiv.org/pdf/1502.03167v3.pdf and could understand the BN algorithm applied on a data but in the end they mentioned that a将应用于CNN时需要轻微的修改：

对于卷积层，我们还希望归一化遵守卷积特性 - 以便以相同的方式将同一特征图的不同元素（在不同位置）进行标准化。为了实现这一目标，我们将所有位置的小批次中的所有激活共同标准化。在alg。 1，我们让b是小批量和空间位置的两个元素的特征映射中的所有值集 - 因此，对于MINI尺寸m和大小p×Q的特征映射，我们使用尺寸m&#39;= | b |的有效的小批量的小批量。 = M·Pq。我们每个特征图学习一对参数γ（k）和β（k），而不是每个激活。 alg。 2进行了类似的修改，因此在推断中，BN转换在给定特征映射中适用于每个激活的相同线性变换。

当他们说时，我完全感到困惑

使得在不同位置的同一特征图的不同元素以相同的方式归一化

我知道特征映射的含义，不同的元素是每个功能映射中的权重。但是我不明白位置或空间位置的含义。
我根本无法理解以下句子：

在ALG中。 1，我们让b为小批次和空间位置的两个元素的元素中的特征图中的所有值集

有人可以在更简单的任期内详细说明我吗？]]></description>
      <guid>https://stackoverflow.com/questions/38553927/batch-normalization-in-convolutional-neural-network</guid>
      <pubDate>Sun, 24 Jul 2016 15:54:58 GMT</pubDate>
    </item>
    <item>
      <title>使用翻转图像进行机器学习数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/11780650/using-flipped-images-for-machine-learning-dataset</link>
      <description><![CDATA[我有一个二进制分类问题。我正在尝试训练神经网络以识别图像的对象。目前，我大约有1500个50x50图像。
问题是，通过水平翻转相同图像设置的我目前的训练是否是个好主意？ （图像不是对称的）]]></description>
      <guid>https://stackoverflow.com/questions/11780650/using-flipped-images-for-machine-learning-dataset</guid>
      <pubDate>Thu, 02 Aug 2012 15:16:47 GMT</pubDate>
    </item>
    </channel>
</rss>