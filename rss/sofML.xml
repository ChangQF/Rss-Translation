<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 28 Nov 2023 09:14:19 GMT</lastBuildDate>
    <item>
      <title>为什么图像、音视频、文本被视为非结构化数据？</title>
      <link>https://stackoverflow.com/questions/77562031/why-images-audio-video-clips-text-are-regarded-as-unstructured-data</link>
      <description><![CDATA[一般来说，在机器学习或数据科学中，表格数据被视为结构化数据，而其他数据类型（例如图像、音频/视频剪辑、文本等）被视为非结构化数据。
我很困惑，以图像为例，它们只是以矩阵或高维张量的形式存储在计算机中，显然具有一定的结构，为什么还称为非结构化数据？
与其他类型的数据类似，这些数据都是数字化的，因此具有各种存储结构以便计算机轻松读取和处理，为什么它们都被视为非结构化数据？
所以我想知道非结构化数据和表格数据（结构化数据）之间的本质区别是什么。]]></description>
      <guid>https://stackoverflow.com/questions/77562031/why-images-audio-video-clips-text-are-regarded-as-unstructured-data</guid>
      <pubDate>Tue, 28 Nov 2023 07:39:15 GMT</pubDate>
    </item>
    <item>
      <title>ML-DL 图像（表单）验证</title>
      <link>https://stackoverflow.com/questions/77561863/ml-dl-image-form-validation</link>
      <description><![CDATA[我有一个图像，它是表单的屏幕截图，我想创建一个模型，将图像作为输入并验证它是否具有相同的表单结构。
我读到 CNN 可能有帮助，或者图像相似性也可能是最好的方法
另一种思考方法是从图像中提取测试并与输入图像中的文本进行比较
所以有什么建议我应该搜索哪些主题来定义和解决这个问题
我尝试开始将标记图像作为数据集来标记表单标题和部分，但不确定这是否有用]]></description>
      <guid>https://stackoverflow.com/questions/77561863/ml-dl-image-form-validation</guid>
      <pubDate>Tue, 28 Nov 2023 07:00:36 GMT</pubDate>
    </item>
    <item>
      <title>“NoneType”对象不可订阅（ewm 方法）</title>
      <link>https://stackoverflow.com/questions/77561683/nonetype-object-is-not-subscriptableewm-method</link>
      <description><![CDATA[当 True 时：
    df = Bithumb.get_candlestick（“XRP”，chart_intervals =“1m”）

    k = df[&#39;close&#39;].ewm(span=12, adjustment=False, min_periods=12).mean()
    d = df[&#39;close&#39;].ewm(span=26, adjustment=False, min_periods=26).mean()
    MACD = k - d
    df[&#39;macd&#39;] = MACD
    信号 = macd.ewm(span=9, adjustment=False, min_periods=9).mean()
    df[&#39;信号&#39;] = 信号

在这一行，发生了 Nonetype 错误：
k = df[&#39;close&#39;].ewm(span=12, adjustment=False, min_periods=12).mean()

这是因为 ewm 方法产生的 NAN 值吗？]]></description>
      <guid>https://stackoverflow.com/questions/77561683/nonetype-object-is-not-subscriptableewm-method</guid>
      <pubDate>Tue, 28 Nov 2023 06:12:13 GMT</pubDate>
    </item>
    <item>
      <title>如何使用tensorflow训练ML模型进行人脸比较并在java中使用它？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77561597/how-to-train-a-ml-model-for-face-comparison-using-tensorflow-and-use-it-in-java</link>
      <description><![CDATA[我想比较两张脸，无论它们是否是同一个人。为此，我使用预先训练的模型 (FaceNet) 来获取面部嵌入并比较两个面部。
为了使用 FaceNet 模型，我使用了这个 github 链接。但我无法这样做，因为它是 5 年前的代码，并且给我带来了折旧错误。
我有 LFW 数据集（成对的图像），我想自己训练一个模型，并想用它在 java 中比较人脸（使用人脸嵌入）。
指导我如何使用tensorflow训练模型并在java中使用它进行面部比较？]]></description>
      <guid>https://stackoverflow.com/questions/77561597/how-to-train-a-ml-model-for-face-comparison-using-tensorflow-and-use-it-in-java</guid>
      <pubDate>Tue, 28 Nov 2023 05:46:10 GMT</pubDate>
    </item>
    <item>
      <title>用于线性回归的随机梯度下降算法的意外输出</title>
      <link>https://stackoverflow.com/questions/77560377/unexpected-output-with-stochastic-gradient-descent-algorithm-for-linear-regressi</link>
      <description><![CDATA[在为我的 ML 作业实现 SGD 算法时，我得到了意外的输出。
这是我的训练数据的一部分，通常有 320 行：

我首先做了一些数据预处理：
导入 pandas 作为 pd
从 sklearn.preprocessing 导入 StandardScaler
将 numpy 导入为 np

train_data = pd.read_csv(&#39;carseats_train.csv&#39;)
train_data.replace({&#39;是&#39;: 1, &#39;否&#39;: 0}, inplace=True)
onehot_tr = pd.get_dummies(test_data[&#39;ShelveLoc&#39;], dtype=int, prefix_sep=&#39;_&#39;, prefix=&#39;ShelveLoc&#39;)
train_data = train_data.drop(&#39;ShelveLoc&#39;, axis=1)
train_data = train_data.join(onehot_tr)


train_data_Y = train_data.iloc[:, 0]
train_data_X = train_data.drop(&#39;销售额&#39;, axis=1)


然后实现这样的算法：
&lt;前&gt;&lt;代码&gt;学习率 = 0.01
epoch_num = 50
初始w = 0.1
截距 = 0.1
w_matrix = np.ones((12, 1)) * 初始w

对于范围内的 e（epoch_num）：
    对于范围内的 i(len(train_data_X))：

        x_i = train_data_X.iloc[i].to_numpy()
        y_i = train_data_Y.iloc[i]
        
        y_估计 = np.dot(x_i, w_matrix) + 截距
        
        grad_w = x_i.reshape(-1, 1) * (y_i - y_估计)
    
        grad_intercept = (y_i - y_估计)
        
       
        w_matrix = w_matrix - 2 * 学习率 * grad_w
        截距 = 截距 - 2 * 学习率 * 梯度截距
        
        

print(&quot;最终权重：\n&quot;, w_matrix)
print(&quot;最终拦截：&quot;,拦截)

但是输出是
最终权重：
 [[南]
 [楠]
 [楠]
 [楠]
 [楠]
 [楠]
 [楠]
 [楠]
 [楠]
 [楠]
 [楠]
 [楠]]
最终截距：[nan]

我用不同的学习率运行它，我也尝试了收敛阈值，但仍然得到相同的结果..我不明白为什么我的代码给了我nans..
有人能看到这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77560377/unexpected-output-with-stochastic-gradient-descent-algorithm-for-linear-regressi</guid>
      <pubDate>Mon, 27 Nov 2023 22:46:46 GMT</pubDate>
    </item>
    <item>
      <title>如何提取给定文档集的顶级分类器特征</title>
      <link>https://stackoverflow.com/questions/77560320/how-to-extract-top-classifier-features-for-a-given-set-of-documents</link>
      <description><![CDATA[我有一个经过二元分类任务训练的逻辑回归分类器。我想提取 X 中给定文档集的顶级分类器特征（信息最丰富的系数）。这些文档的索引存储在名为 idx_list 的列表中。
我尝试使用以下代码提取 X 中所有文档的主要特征：
 def most_informative_feature_for_binary_classification（分类器，向量化器，n=20）：
        类标签 = 分类器.classes_
        feature_names = vectorizer.get_feature_names_out()
        topn_class1 = 排序(zip(classifier.coef_[0], feature_names))[:n]
        topn_class2 = 排序(zip(classifier.coef_[0], feature_names))[-n:]
        print(&#39;0 类主要功能： ----------------------&#39;)
        class0_feat =[]
        对于 coef，topn_class1 中的壮举：
            #print (class_labels[0], coef, feat)
            打印（壮举）
            class0_feat.append(feat )
    
        class0_feat = [str(x) for x in class0_feat]
        使用 open(&#39;../../classification/result/class0_top_features_top_&#39;+str(top_features_nb)+&#39;_&#39;+network+&#39;.txt&#39;,&#39;w&#39;) 作为 f：
            f.write(&#39;\n&#39;.join(class0_feat))
        
        print(&#39;1 类主要功能： ----------------------&#39;)
        class1_feat = []
        对于 coef，相反的壮举（topn_class2）：
            #print (class_labels[1], coef, feat)
            打印（壮举）
            class1_feat.append(壮举)

此代码适用于提取 X 中所有文档的主要特征，但我想提取 idx_list 定义的一组特定文档的主要特征。
使用 Sklearn 对文本文档进行分类：
向量化器 = TfidfVectorizer(input=&#39;文件名&#39;, min_df=mindf, max_df = maxdf)
        X = 矢量化器.fit_transform(friend_files)
        
        print(&quot;X 形状：&quot;,X.shape)

        y = list(username_labels.values()) # 0 或 1

        clf = 逻辑回归()

        clf.fit(X, y)
        most_informative_feature_for_binary_classification3（clf，矢量化器，n=10）

如何修改代码以提取 idx_list 指定文档的顶级特征？]]></description>
      <guid>https://stackoverflow.com/questions/77560320/how-to-extract-top-classifier-features-for-a-given-set-of-documents</guid>
      <pubDate>Mon, 27 Nov 2023 22:32:30 GMT</pubDate>
    </item>
    <item>
      <title>神经网络意外预测</title>
      <link>https://stackoverflow.com/questions/77560144/neuralnet-unexpected-prediction</link>
      <description><![CDATA[我试图了解神经网络包是如何工作的。
我使用的是 mnist 数据集，其中包含对应于不同图片的 60.000 行和代表图片每个像素的 785 列（除了第一个像素）
与图片标签对应的列）。
initial_data &lt;- read.csv(file = &#39;train.csv&#39;, header = TRUE)

数据如下所示：
 标签 Pixel1 Pixel2 Pixel3 Pixel4 Pixel5 Pixel6 ...
1 5 0 0 3 0 1 0 ...
2 3 0 0 0 7 0 0 ...
ETC

首先，我删除方差等于 0 的像素。因为它们无法提供评估图片中写入的数字的信息。
filtered_data &lt;-initial_data %&gt;%
  select_if(函数(列) var(列) != 0)

# 显示新过滤数据的维度
暗淡（过滤数据）

然后我对数据进行标准化，以确保每个功能的贡献相同
到模型中，算法不受较大尺度特征的影响。
filtered_data &lt;- as.data.frame(scale(filtered_data[-1]))

现在我进行数据分区（80% 训练和 20% 测试）。
filtered_data$label &lt;-initial_data$label
filtered_data &lt;-filtered_data %&gt;% select(标签, everything())
索引 &lt;- createDataPartition(filtered_data$label, p = 0.8, list = FALSE)

# 创建训练集和验证集
训练数据&lt;-过滤数据[索引，]
valid_data &lt;-filtered_data[-index, ]

# 通过预测变量和标签分隔
训练数据X &lt;- 训练数据[-1]
训练数据Y &lt;- 训练数据[1]
validation_data_X &lt;-validation_data[-1]
validation_data_Y &lt;-validation_data[1]

现在我生成一个非常简单的神经网络并进行预测
input_variables &lt;- 粘贴（名称（training_data_X），collapse =＆quot; +＆quot;）
输出变量 &lt;- 名称(training_data_Y)[1]
content_formula &lt;- 粘贴（输出变量，“~”，输入变量）

simple_nn_model &lt;- 神经网络（内容公式，数据 = 训练数据，隐藏 = 1，
                             act.fct =“逻辑”，线性输出= FALSE）

Predictions_simple_model &lt;- 预测（simple_nn_model，newdata =validation_data_X）

问题：我希望对象predictions_simple_model包含10列（每列代表0到9之间的一个数字），并且它们的值范围应该从0到1（取决于预测者所做的预测）模型）。但是，相反，我获得了一列，并且它们的所有值都等于 1。
&lt;前&gt;&lt;代码&gt;&gt;预测简单模型
           [,1]
137 1.0000000
171 1.0000000
213 1.0000000
225 1.0000000
236 1.0000000
420 1.0000000
第576章 1.0000000
615 1.0000000
899 1.0000000
ETC

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77560144/neuralnet-unexpected-prediction</guid>
      <pubDate>Mon, 27 Nov 2023 21:52:43 GMT</pubDate>
    </item>
    <item>
      <title>将 pyspark 数据帧保存为 RecordIO protobuf</title>
      <link>https://stackoverflow.com/questions/77559860/save-pyspark-dataframe-as-recordio-protobuf</link>
      <description><![CDATA[我想以 RecordIO protobuf 格式保存我的 pyspark 数据帧。我正在使用 Amazon EMR 运行我的 pyspark 脚本，并且我想使用 AWS SageMaker 来训练机器学习模型。
SageMaker 管道模式仅接受 RecordIO protobuf 作为输入，因此我的问题
我尝试将我的 pyspark 数据帧保存为 recordio protobuf，如下所示：
output_path = f“s3://my_path/output_processed”
df_transformed.write.format(“sagemaker”).mode(“覆盖”).save(output_path)

但是当我运行 sagemaker 模型时，即使我的数据帧没有缺失值，我也会收到缺失值的错误。可能是什么问题以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/77559860/save-pyspark-dataframe-as-recordio-protobuf</guid>
      <pubDate>Mon, 27 Nov 2023 20:49:10 GMT</pubDate>
    </item>
    <item>
      <title>如何将文本描述映射到类别？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77559226/how-do-i-map-text-descriptions-to-categories</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77559226/how-do-i-map-text-descriptions-to-categories</guid>
      <pubDate>Mon, 27 Nov 2023 18:39:59 GMT</pubDate>
    </item>
    <item>
      <title>使用 H2O 随机森林进行递归特征消除</title>
      <link>https://stackoverflow.com/questions/77549868/recursive-feature-elimination-with-h2o-random-forest</link>
      <description><![CDATA[我正在 python 中使用 h2o 包来构建一个相当复杂的模型。
它有大约 1500 个特征，但我知道其中大多数并不重要，我想提取给定大小（假设为 100）的子集，以最大化模型的 R 平方。
是否有一些方法已经在 python 中为 h2o 实现了这个？
否则我需要自己编码，但这也意味着多次运行模型，而且我不确定我是否会以正确的方式编码。
一种可能的编码方法是：

保存模型的 R2，然后删除 k 个不太重要的特征
创建第二个模型，但不删除已删除的特征
计算新模型的 R2 并与之前的 R2 进行比较。使用指标来决定是保留新模型还是坚持旧模型。
迭代这些步骤，直到上一步选择旧模型作为最佳模型
我很确定这不会给我功能的“最佳子集”，但我真的希望它足够了。

我想到的第二种方法如下：

设置新模型中所需的特征数 N 和迭代次数 K
保存原始模型 R2 作为参考
从原始模型中随机提取 N 个特征，使用它们的相对重要性作为被提取的概率（更重要的特征更有可能被提取）
保存每个型号的功能列表和新 R2
迭代 K 次后停止算法并比较 R2
选择 R2 最接近原始特征的一组特征
]]></description>
      <guid>https://stackoverflow.com/questions/77549868/recursive-feature-elimination-with-h2o-random-forest</guid>
      <pubDate>Sat, 25 Nov 2023 23:03:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在 argparse 中为 AzureML 中的管道添加元组？</title>
      <link>https://stackoverflow.com/questions/77459218/how-to-add-tuple-in-argparse-for-the-pipeline-in-azureml</link>
      <description><![CDATA[我想对我在管道中执行的函数进行argparse元组。为了简单起见，我将跳过读取数据和其他与主题不太相关的步骤。看起来像这样：
def model_train_sales(X_train, order: tuple,seasonal_order: tuple):

    模型 = sm.tsa.SARIMAX(X_train[&#39;sales&#39;], order=order,seasonal_order=seasonal_order)
    结果 = model.fit()

    返回模型、结果

def main():

    解析器 = argparse.ArgumentParser()

    parser.add_argument(&quot;--order&quot;, type=tuple)
    parser.add_argument(&quot;--seasonal_order&quot;, type=tuple)

    args = parser.parse_args()

    模型，结果 = model_train_sales(X_train[&#39;sales&#39;], order=args.order,
    seasonal_order=args.seasonal_order)

此时一切都很好，但是当您开始构建管道时，解析变量的类型不同。
来自 azure.ai.ml 导入命令
从 azure.ai.ml 导入输入、输出

演示模型训练组件 = 命令（
    name=&#39;我的萨里玛管道&#39;,
    display_name=&#39;我的描述&#39;,
    description=&#39;长描述。&#39;,
    输入={
        “订单”：输入（类型=&#39;&lt;类型&gt;&#39;），
        “seasonal_order”：输入（type=&#39;&#39;），
    },
    输出=字典（
        df = 输出（类型=“uri_folder”，模式=“rw_mount”）
    ),
    代码 = feature_creation_src_dir,
    命令=“”“python sarima_model.py \
              --order ${{inputs.order}} --seasonal_order ${{inputs.seasonal_order}} \
              --df ${{输出.df}}
              ”“”，
    环境= f“{pipeline_job_env.name}”{pipeline_job_env.version}”，
）

在这里，我在处签名了我不确定应该是哪种类型的地方。我知道类型是有限的，可以是 string、integer、number 或 bool。
有什么方法可以解析其中的元组吗？或者唯一的方法是分别解析 p, d, q 和 P, D, Q, S 并将它们组合成主函数中的元组？]]></description>
      <guid>https://stackoverflow.com/questions/77459218/how-to-add-tuple-in-argparse-for-the-pipeline-in-azureml</guid>
      <pubDate>Fri, 10 Nov 2023 10:17:33 GMT</pubDate>
    </item>
    <item>
      <title>在单个虚拟机中使用 mlflow 为多个 ML 模型提供服务</title>
      <link>https://stackoverflow.com/questions/70620074/serving-multiple-ml-models-using-mlflow-in-a-single-vm</link>
      <description><![CDATA[我已在虚拟机中设置了 mlflow 服务，并且可以使用 mlflowserve 命令为模型提供服务。
想知道我们是否可以在单个虚拟机中托管多个模型？
我正在使用以下命令在虚拟机中使用 mlflow 来提供模型。
命令：
/mlflow 模型服务 -m 模型:/$模型名称/$版本 --no-conda -p 443 -h 0.0.0.0

以上命令创建一个模型服务并在 443 端口上运行它。
是否可以使用其中的模型名称创建如下所示的端点？
当前网址：
https://localhost:443/incalls
预期网址：
https://localhost:443/模型名称/调用？]]></description>
      <guid>https://stackoverflow.com/questions/70620074/serving-multiple-ml-models-using-mlflow-in-a-single-vm</guid>
      <pubDate>Fri, 07 Jan 2022 10:48:20 GMT</pubDate>
    </item>
    <item>
      <title>错误：尝试在自定义 HF 数据集上使用 trainer.train() 时，vars() 参数必须具有 __dict__ 属性？</title>
      <link>https://stackoverflow.com/questions/69539538/error-vars-argument-must-have-dict-attribute-when-trying-to-use-trainer-t</link>
      <description><![CDATA[我有以下模型正在尝试微调（CLIP_ViT + 分类头）。这是我的模型定义：
CLIPNN 类（nn.Module）：

    def __init__(self, num_labels, pretrained_name=“openai/clip-vit-base-patch32”, dropout=0.1):
        超级().__init__()
        self.num_labels = num_labels
        # 加载预训练的 Transformer &amp;处理器
        self.transformer = CLIPVisionModel.from_pretrained(pretrained_name)
        self.processor = CLIPProcessor.from_pretrained(pretrained_name)
        # 初始化其他层（头部在变压器主体之后）
        self.classifier = nn.Sequential(
            nn.Linear(512, 128, 偏差=True),
            nn.ReLU(inplace=True),
            nn.Dropout(p=dropout, inplace=False),
            nn.Linear(128, self.num_labels, 偏差=True))
        
        defforward（自我，输入，标签=无，**kwargs）：
            logits = self.classifier(输入)
            损失=无
            如果标签不是无：
                loss_fct = nn.CrossEntropyLoss()
                损失 = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

            返回序列分类器输出（
                损失=损失，
                对数=对数，
            ）

我还有以下数据集定义：
class CLIPDataset(nn.utils.data.Dataset)：
    def __init__(自身、嵌入、标签):
        self.embeddings = 嵌入
        self.labels = 标签

    def __getitem__(self, idx):
        item = {“嵌入”: nn.Tensor(self.embeddings[idx])}
        item[&#39;labels&#39;] = nn.LongTensor([self.labels[idx]])
        归还物品

    def __len__(自身):
        返回 len(self.labels)


注意：这里我假设模型是预先计算的嵌入并且不计算嵌入，我知道如果我想微调 CLIP 基础模型，这不是正确的逻辑，我只是​​想得到我的代码可以工作。
类似这样的事情会引发错误：
模型 = CLIPNN(num_labels=2)
train_data = CLIPDataset(train_data, y_train)
test_data = CLIPDataset(test_data, y_test)

教练=教练（
    模型=模型，args=training_args，train_dataset=train_data，eval_dataset=test_data
）
训练师.train()

&lt;块引用&gt;
类型错误回溯（最近一次调用）
----&gt; 1 个trainer.train()
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/trainer.py
在火车中（自我，resume_from_checkpoint，审判，ignore_keys_for_eval，
**kwargs）第1256章 self.control = self.callback_handler.on_epoch_begin（args，self.state，self.control）
1257 → 1258 为步骤，输入 enumerate(epoch_iterator): 1259 1260 #
如果恢复训练，请跳过任何已训练的步骤
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py 在 next(self) 515 如果 self._sampler_iter 为 None: 516 self._reset() →
517 数据 = self._next_data() 518 self._num_yielded += 1 519 if
self._dataset_kind == _DatasetKind.Iterable 和 \
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self) 555 def _next_data(self): 556 索引 =
self._next_index() # 可能引发 StopIteration → 557 data =
self._dataset_fetcher.fetch(index) # 可能会引发 StopIteration 558 如果
self._pin_memory: 559 数据 = _utils.pin_memory.pin_memory(data)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py
在 fetch(self, possible_batched_index) 45 else: 46 data =
self.dataset[possible_batched_index] —&gt; 47 返回
self.collat​​e_fn(数据)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/data/data_collat​​or.py
在 default_data_collat​​or(features, return_tensors) 64 65 如果
return_tensors == “pt”: —&gt;; 66 返回
torch_default_data_collat​​or(features) 67 elif return_tensors == “tf”:
68 返回 tf_default_data_collat​​or(features)
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/data/data_collat​​or.py
在 torch_default_data_collat​​or(features) 80 81 如果没有
isinstance(features[0], (dict, BatchEncoding)): —&gt;; 82 个特征 =
[vars(f) for f in features] 83first = features[0] 84batch = {}
~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/transformers/data/data_collat​​or.py
在 (.0) 80 81 中，如果不是 isinstance(features[0], (dict, BatchEncoding))：
—&gt; 82 特征 = [特征中 f 的 vars(f)] 83 第一个 = 特征[0] 84
批次 = {}
类型错误：vars() 参数必须具有 dict 属性

知道我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/69539538/error-vars-argument-must-have-dict-attribute-when-trying-to-use-trainer-t</guid>
      <pubDate>Tue, 12 Oct 2021 11:12:03 GMT</pubDate>
    </item>
    <item>
      <title>重新启动运行时后模型加载得到不同的结果</title>
      <link>https://stackoverflow.com/questions/68116676/model-load-get-different-result-after-restart-runtime</link>
      <description><![CDATA[你好，我是神经网络新手，我在训练模型后使用谷歌colab编写了一个模型CNN架构Resnet50，然后保存模型，然后加载模型而不重新启动运行时得到相同的结果，但为什么当重新启动运行时谷歌colab并运行xtrain，ytest时,x_val,y_val 然后再次加载模型得到不同的结果
这是我设置参数的代码
#超参数和回调
批量大小 = 128
纪元数 = 120
输入形状 = (48, 48, 1)
类数 = 7

#编译模型。
从 keras.optimizers 导入 Adam，SGD
模型 = ResNet50(input_shape = (48, 48, 1)，类 = 7)
优化器 = SGD(learning_rate=0.0005)
model.compile(optimizer=optimizer,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

模型.summary()
历史=模型.fit(
    data_generator.flow(xtrain, ytrain,),
    步骤_per_epoch = len（xtrain）/批量大小，
    纪元=num_epochs，
    详细=1，
    验证数据=（x_val，y_val））

将 matplotlib.pyplot 导入为 plt
model.save(&#39;Fix_Model_resnet50editSGD5st.h5&#39;)

#plot 图表
准确度=历史记录.历史[&#39;准确度&#39;]
val_accuracy = 历史.history[&#39;val_accuracy&#39;]
损失=历史.历史[‘损失’]
val_loss = 历史.history[&#39;val_loss&#39;]
num_epochs = 范围(len(准确度))
plt.plot(num_epochs, 准确度, &#39;r&#39;, label=&#39;训练 acc&#39;)
plt.plot(num_epochs, val_accuracy, &#39;b&#39;, label=&#39;验证 acc&#39;)
plt.title(&#39;训练和验证准确性&#39;)
plt.ylabel(&#39;准确度&#39;)
plt.xlabel(&#39;纪元&#39;)
plt.图例()
plt.figure()
plt.plot(num_epochs, loss, &#39;r&#39;, label=&#39;训练损失&#39;)
plt.plot(num_epochs, val_loss, &#39;b&#39;, label=&#39;验证损失&#39;)
plt.title(&#39;训练和验证损失&#39;)
plt.ylabel(&#39;损失&#39;)
plt.xlabel(&#39;纪元&#39;)
plt.图例()
plt.show()

#加载模型
从 keras.models 导入 load_model
model_load = load_model(&#39;Fix_Model_resnet50editSGD5st.h5&#39;)

model_load.summary()


testdatamodel = model_load.evaluate(xtest, ytest)
print(&quot;测试损失&quot; + str(testdatamodel[0]))
print(&quot;测试 Acc:&quot; + str(testdatamodel[1]))

traindata = model_load.evaluate(xtrain, ytrain)
print(&quot;测试损失&quot; + str(traindata[0]))
print(&quot;测试准确率：&quot; + str(traindata[1]))

valdata = model_load.evaluate(x_val, y_val)
print(&quot;测试损失&quot; + str(valdata[0]))
print(&quot;测试记录：&quot; + str(valdata[1]))

-训练并保存模型后，然后运行加载模型，无需重新启动运行时 google colab ：
如你所见
测试得到损失：0.9411 - 准确度：0.6514
训练损失：0.7796 - 准确度：0.7091
ModelEvaluateTest &amp;火车
重启运行时colab后再次运行加载模型：
测试损失：0.7928 - 准确度：0.6999
训练损失：0.8189 - 准确度：0.6965
重新启动运行时评估测试和训练]]></description>
      <guid>https://stackoverflow.com/questions/68116676/model-load-get-different-result-after-restart-runtime</guid>
      <pubDate>Thu, 24 Jun 2021 13:25:12 GMT</pubDate>
    </item>
    <item>
      <title>如何从预训练模型加载保存的分词器</title>
      <link>https://stackoverflow.com/questions/58417374/how-to-load-the-saved-tokenizer-from-pretrained-model</link>
      <description><![CDATA[我使用 Huggingface 转换器在 Pytorch 中微调了预训练的 BERT 模型。所有训练/验证都是在云中的 GPU 上完成的。
训练结束时，我保存模型和分词器，如下所示：
best_model.save_pretrained(&#39;./saved_model/&#39;)
tokenizer.save_pretrained(&#39;./saved_model/&#39;)

这会在 saved_model 目录中创建以下文件：
&lt;前&gt;&lt;代码&gt;config.json
添加的_token.json
Special_tokens_map.json
tokenizer_config.json
词汇表.txt
pytorch_model.bin

现在，我将 saved_model 目录下载到我的计算机中，并希望加载模型和分词器。我可以像下面这样加载模型
model = torch.load(&#39;./saved_model/pytorch_model.bin&#39;,map_location=torch.device(&#39;cpu&#39;))
但是如何加载分词器呢？我是 pytorch 的新手，不确定，因为有多个文件。也许我没有以正确的方式保存模型？]]></description>
      <guid>https://stackoverflow.com/questions/58417374/how-to-load-the-saved-tokenizer-from-pretrained-model</guid>
      <pubDate>Wed, 16 Oct 2019 15:57:36 GMT</pubDate>
    </item>
    </channel>
</rss>