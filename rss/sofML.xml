<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 05 Dec 2023 18:18:05 GMT</lastBuildDate>
    <item>
      <title>我有 300 个课程标题，需要将它们分组为未知数量的主题。本次作业推荐采用什么无监督学习方法</title>
      <link>https://stackoverflow.com/questions/77608183/i-have-300-course-titles-and-need-to-group-them-into-an-unknown-number-of-topics</link>
      <description><![CDATA[此问题涉及将 300 个课程标题分类为几个总体主题，而事先不知道这些主题的数量或性质。挑战在于确定这些主题的数量和内容。在此背景下，我们正在寻找一种合适的无监督学习方法来完成这一任务。
我很困惑，不知道该怎么做]]></description>
      <guid>https://stackoverflow.com/questions/77608183/i-have-300-course-titles-and-need-to-group-them-into-an-unknown-number-of-topics</guid>
      <pubDate>Tue, 05 Dec 2023 17:31:29 GMT</pubDate>
    </item>
    <item>
      <title>缺陷的时间演变：预测剩余缺陷</title>
      <link>https://stackoverflow.com/questions/77607265/temporal-evolution-of-defects-predicting-remaining-defect</link>
      <description><![CDATA[我在一家公司的人工智能论文是预测 10 分钟或 Xtime 后纸张上剩余的缺陷
-墨水被打印在金属片上
- 打印后直接出现一些缺陷（打印步骤后1秒拍照）
-但过了一段时间，一些缺陷消失了，一些缺陷仍然存在
我的目标是打印一张新纸张并拍摄即时照片后：使用机器/深度学习预测 10 分钟或 Xtime 后剩余的缺陷
说明（涂上墨水后涂抹更多并覆盖疤痕等缺陷）
我需要帮助，任何想法都会被采纳
我有多种资源，如相机、机器等
我愿意接受任何想法]]></description>
      <guid>https://stackoverflow.com/questions/77607265/temporal-evolution-of-defects-predicting-remaining-defect</guid>
      <pubDate>Tue, 05 Dec 2023 15:12:18 GMT</pubDate>
    </item>
    <item>
      <title>我如何开始学习数据科学和机器学习请建议一条明确的路径？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77607227/how-can-i-start-learning-data-science-and-machine-learning-please-suggest-a-well</link>
      <description><![CDATA[我希望以机器学习工程师的身份从事数据科学和机器学习的职业，但是我找不到一条有明确指导和良好布局的道路，这个领域似乎是多样化的，我只是感到困惑。
人们只是告诉要学习 python 和 numpy、pandas、用于可视化的 matplot 等库，这很模糊，我已经知道 python 我找不到合适的资源来从基础理论开始学习数据科学和机器学习基础知识，并在实践中通过建议提供帮助参考资料、书籍、讲座、课程我才刚刚开始。]]></description>
      <guid>https://stackoverflow.com/questions/77607227/how-can-i-start-learning-data-science-and-machine-learning-please-suggest-a-well</guid>
      <pubDate>Tue, 05 Dec 2023 15:04:53 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助......Apple ML 给出奇怪的结果[关闭]</title>
      <link>https://stackoverflow.com/questions/77607078/need-help-apple-ml-giving-out-weird-results</link>
      <description><![CDATA[只是我想要创建的一些背景：
我的目标是使用 WLASL 数据集创建一个将手语转换为文本的模型。现在，从一开始就从 Kaggle 下载这个模型，虽然数据集看起来相当全面，但每个类别的视频数量从 5-13 个不等，这显然需要训练的内容相当少。我决定尝试 Apple Create ML，而不是像 Tensorflow 或更复杂的深度学习框架，因为这样会简单得多。由于数据集在每个类别的视频方面非常有限，因此我在“手部动作分类器”中使用了所有 6 个数据增强。 （水平翻转、旋转、平移、缩放、插帧、丢帧）。虽然我知道这无法保存模型，但它肯定会大大提高准确性。请注意，我没有使用数据集中的所有 2000 个类（单词），而是仅使用了 300 个类的子集。我获得了 16% 的验证准确率，以及 90% 的所有增强训练准确率，因此我的模型显然过度拟合。所以我对 25 个类进行了同样的尝试，这次我获得了 42% 的验证准确率，以及 100% 的训练准确率。再次，过度拟合。我进入实时预览，几乎我尝试的每个迹象都被预测为错误。
现在，我决定使用“模型源”在侧边栏中。我不太确定它们的用途，但这是我尝试过的：
我将数据子集分成 2 个单独的模型源（16 个类，但数量仍然很高），分别获得了 83% 的验证准确率和 90% 的验证准确率。这两个模型源都使用所有数据增强。我的模型显然过度拟合，在两个来源中都有 100% 的训练准确度，但将其分成两个模型显然提高了我的准确度，当我在“实时预览”中测试这一点时，我自己做的每个 ASL 标志都能够以超过 90% 的置信度准确猜出每个单词。
所以我的问题是，即使我的数据有限（虽然增强确实增加了很多，但显然性能差异不应该这么大），我的模型如何表现得这么好？此外，将一个模型拆分为单独的模型源是否可行？我不确定“模型来源”有什么用？甚至是，所以我尝试了这个，不知怎的，我得到了更好的结果。如果可行，我如何将它们实现到一个快速应用程序中。我现在有点困惑，所以希望有人能告诉我发生了什么事。如果这不是一个可行的解决方案，有人可以提供另一个解决方案来说明我如何使用这个数据集吗？事先了解它会非常有帮助，但即使你不知道，你能帮助我吗？
非常感谢大家:)
PS：这是它的链接-：
Kaggle链接：https://www.kaggle.com/datasets/risangbaskoro/ wlasl 处理
原始论文github页面：https://github.com/dxli94/WLASL
抱歉这么长的消息。如果您需要任何图像来获得更多见解或更好的知识以提供更好的帮助，我很乐意提供它们。
再次感谢您]]></description>
      <guid>https://stackoverflow.com/questions/77607078/need-help-apple-ml-giving-out-weird-results</guid>
      <pubDate>Tue, 05 Dec 2023 14:43:09 GMT</pubDate>
    </item>
    <item>
      <title>运行“docker build -t file-name”时构建 docker 映像会导致错误。</title>
      <link>https://stackoverflow.com/questions/77606730/building-the-docker-image-results-in-error-while-running-docker-build-t-file-n</link>
      <description><![CDATA[我正在开发一个机器学习模型，我正在尝试使用 Docker 将其容器化，
运行命令时
docker build -t &lt;文件名&gt; &lt;位置&gt;
下面是docker文件。
&lt;前&gt;&lt;代码&gt;来自 python:3.11-alpine
复制 。 /应用程序
工作目录/应用程序
运行 pip install -rrequirements.txt
CMD Streamlit 运行 viz_app.python

下面是我收到的错误。
22.34 注意：此错误源自子进程，并且可能不是 pip 的问题。
22.34 错误：子进程退出并出现错误
22.34
22.34 × 用于安装构建依赖项的 pip 子进程未成功运行。
22.34 │ 退出代码：1
22.34 ╰─&gt;请参阅上面的输出。
22.34
22.34 注意：此错误源自子进程，并且可能不是 pip 的问题。
22.36
22.36 [通知] pip 新版本发布：23.0.1 -&gt; 23.3.1
22.36 [注意] 要更新，请运行： pip install --upgrade pip
------
Dockerfile:4
--------------------
   2 |复制 。 /应用程序
   3 |工作目录/应用程序
   4 | &gt;&gt;&gt;&gt;&gt;运行 pip install -rrequirements.txt
   5 | CMD Streamlit 运行 viz_app.python
--------------------
错误：无法解决：进程“/bin/sh -c pip install -rrequirements.txt”未成功完成：退出代码：1

我尝试使用以下方法解决特定问题。

我尝试通过提及“RUN pip install --upgrade pip”来升级 pip
还尝试安装需求文件的不同版本的软件包。

为了进一步参考，我添加了下面的requirement.txt 文件的包。
&lt;前&gt;&lt;代码&gt;numpy
熊猫
流光溢彩
scikit学习
绘图库
OpenCV-Python
张量流
分割模型
]]></description>
      <guid>https://stackoverflow.com/questions/77606730/building-the-docker-image-results-in-error-while-running-docker-build-t-file-n</guid>
      <pubDate>Tue, 05 Dec 2023 13:51:20 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow.js 散点图显示预测维护与实际维护相距甚远，我不明白为什么</title>
      <link>https://stackoverflow.com/questions/77606726/tensorflow-js-scatterplot-showing-predicted-vs-actual-maintenance-very-far-apart</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77606726/tensorflow-js-scatterplot-showing-predicted-vs-actual-maintenance-very-far-apart</guid>
      <pubDate>Tue, 05 Dec 2023 13:50:52 GMT</pubDate>
    </item>
    <item>
      <title>端到端 ML 项目上的模型训练器问题 - TypeError：__init__() 获得意外的关键字参数“trained_model_file_path”</title>
      <link>https://stackoverflow.com/questions/77606532/model-trainer-issue-on-end-to-end-ml-project-typeerror-init-got-an-unex</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77606532/model-trainer-issue-on-end-to-end-ml-project-typeerror-init-got-an-unex</guid>
      <pubDate>Tue, 05 Dec 2023 13:22:19 GMT</pubDate>
    </item>
    <item>
      <title>搜索用于 AI 模型训练的癌症相关医疗数据集 [关闭]</title>
      <link>https://stackoverflow.com/questions/77604720/searching-for-cancer-related-healthcare-datasets-for-ai-model-training</link>
      <description><![CDATA[我正在开发一个专注于医疗保健的人工智能 (AI) 模型，特别是癌症诊断、治疗和患者结果。我正在寻找能够深入了解各种类型的癌症、患者人口统计数据、治疗计划、生存率和其他相关临床数据的数据集。
详细信息：

所需数据类型：患者案例研究、临床试验数据、癌症成像数据集、遗传信息、治疗反应和纵向研究数据。
预期用途：训练 AI 模型，以协助肿瘤学的早期检测、个性化治疗计划和预后评估。
所需的特异性：涵盖多种癌症类型、阶段和治疗方式的数据。

我尝试过的：
在国家癌症研究所 (NCI) 和癌症影像档案库等存储库中搜索医学数据集。
审查了通过医疗保健提供者和研究网络（例如监测、流行病学和最终结果 (SEER) 计划）提供的数据集。
探索与癌症研究相关的数据集的学术出版物。
在哪里可以找到其他与癌症相关的医疗保健数据集？对公共和专有数据集（经过必要的道德批准才能使用）的建议将非常有价值。]]></description>
      <guid>https://stackoverflow.com/questions/77604720/searching-for-cancer-related-healthcare-datasets-for-ai-model-training</guid>
      <pubDate>Tue, 05 Dec 2023 08:23:48 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 1D 贝叶斯 CNN（通过使用工作 CNN 的 Convolution1DFlipout 和 DenseFlipout 替换卷积层和密集层而制成）无法训练？</title>
      <link>https://stackoverflow.com/questions/77602609/why-does-my-1d-bayesian-cnn-made-by-replacing-the-convolution-and-dense-layers</link>
      <description><![CDATA[我有一个 CNN 模型，它将波形（形状为 (601,3)，其中 601 是时间步数，3 是通道数）分类为噪声或信号。如下：
# 导入
将 numpy 导入为 np
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler
将张量流导入为 tf
从张量流导入keras
从tensorflow.keras导入层

随机种子 = 42

tf.random.set_seed(random_seed)

模型 = keras.Sequential([
    Layers.Input(shape=(601, 3)), # 一维数据的输入形状
    层.Conv1D（32，kernel_size = 16，激活=&#39;relu&#39;），
    层.Conv1D（64，kernel_size = 16，激活=&#39;relu&#39;），
    层.Conv1D（128，kernel_size = 16，激活=&#39;relu&#39;），
    层.Flatten(),
    层.Dense(80, 激活=&#39;relu&#39;),
    层.Dense(80, 激活=&#39;relu&#39;),
    层.Dense(2, 激活=&#39;softmax&#39;)
]）

优化器 = keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=优化器, 指标=[&#39;准确性&#39;])

纪元数 = 40
批量大小 = 48

历史= model.fit(X_train, y_train_encoded, epochs=num_epochs, batch_size=batch_size,
                    validation_data=(X_test, y_test_encoded), 详细=2)

# X_train 形状: (num_train_samples,601,3)
# X_test 形状: (num_test_samples,601,3)
# y_train_encoded 形状：(num_train_samples,2)
# y_test_encoded 形状：(num_test_samples,2)

上述模型在第 12 个 epoch 时运行良好，在所有 epoch 训练后准确率超过 99%。
当我尝试通过分别用 Convolution1DFlipout 和 DenseFlipout 层替换 Conv1D 和 Dense 层来将上述 CNN 转换为贝叶斯 CNN 时，问题就出现了。
# 导入
将tensorflow_probability导入为tfp

tfd = tfp.分布
tfpl = tfp.层

随机种子 = 42

tf.random.set_seed(random_seed)

num_training_samples = X_train.shape[0]
kl_divergence_fn = lambda q, p, _: tfd.kl_divergence(q, p) / num_training_samples

模型 = keras.Sequential([
    层.输入(形状=(601, 3)),
    tfpl.Convolution1DFlipout(
        32、kernel_size=16、activation=tf.nn.relu、kernel_divergence_fn=kl_divergence_fn、bias_divergence_fn=kl_divergence_fn)、
    tfpl.Convolution1DFlipout(
        64、kernel_size=16、activation=tf.nn.relu、kernel_divergence_fn=kl_divergence_fn、bias_divergence_fn=kl_divergence_fn)、
    tfpl.Convolution1DFlipout(
        128、kernel_size=16、activation=tf.nn.relu、kernel_divergence_fn=kl_divergence_fn、bias_divergence_fn=kl_divergence_fn)、
    层数.MaxPooling1D(pool_size=2),
    层.Flatten(),
    tfpl.DenseFlipout(80，激活=tf.nn.relu，kernel_divergence_fn=kl_divergence_fn，bias_divergence_fn=kl_divergence_fn),
    tfpl.DenseFlipout(80，激活=tf.nn.relu，kernel_divergence_fn=kl_divergence_fn，bias_divergence_fn=kl_divergence_fn),
    tfpl.DenseFlipout(2，激活=tf.nn.softmax，kernel_divergence_fn=kl_divergence_fn，bias_divergence_fn=kl_divergence_fn)
]）

优化器 = keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=优化器, 指标=[&#39;准确性&#39;])

# 训练模型（与之前相同）
纪元数 = 40
批量大小 = 48

历史= model.fit(X_train, y_train_encoded, epochs=num_epochs, batch_size=batch_size,
                    validation_data=(X_test, y_test_encoded), 详细=2)

这个模型似乎没有收敛。有人可以帮我解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77602609/why-does-my-1d-bayesian-cnn-made-by-replacing-the-convolution-and-dense-layers</guid>
      <pubDate>Mon, 04 Dec 2023 21:17:27 GMT</pubDate>
    </item>
    <item>
      <title>后端的大.pkl数据没有推送到github中</title>
      <link>https://stackoverflow.com/questions/77600252/large-pkl-data-for-backend-is-not-pushed-in-github</link>
      <description><![CDATA[我正在学习机器学习。最近，我从 tmdb 数据集制作了电影推荐模型，我使用 .pkl （二进制）文件中的模型处理数据。使用该数据制作后端，但是数据太大，无法推送到 github，我无法托管网站。
我正在尝试将已处理的数据推送到后端，但无法部署，因为它超出了文件大小的限制]]></description>
      <guid>https://stackoverflow.com/questions/77600252/large-pkl-data-for-backend-is-not-pushed-in-github</guid>
      <pubDate>Mon, 04 Dec 2023 14:33:48 GMT</pubDate>
    </item>
    <item>
      <title>在 LightGBM 中使用不同 boosting 类型的数据采样方法</title>
      <link>https://stackoverflow.com/questions/77578111/use-of-data-sample-methods-with-different-boosting-types-in-lightgbm</link>
      <description><![CDATA[我的问题
我不太清楚所有参数的用法以及它们如何相互交互（或应该使用）。
我所知道的
据我了解，LightGBM中有3种算法：

GBDT，默认的，使用 boosting
DART 是一种带有 dropout 的 boosting 算法
随机森林，不使用增强（确实如此，但仅在一次迭代中）

并且有两种数据采样策略：

Bagging，默认设置，用于集成学习
GOSS 选择更多对误差梯度贡献最大的数据（我们的想法是，我们需要对远离基线的数据进行更多训练），而对“弱”数据进行更少的训练。数据点（对误差梯度贡献较小的数据点）。

问题
所以我的问题如下：

Bagging 和 GOSS 似乎能够协同工作，但 data_sample_method 参数阻止我们这样做，因为它是一个字符串参数。这是有意为之的行为吗？
在选择 GOSS 数据样本方法时，如果我们提供 bagging_fraction 和 bagging_freq 参数，会发生什么情况？文档没有提到这两个需要 badding 示例方法。
LightGBM的主要创新似乎是GOSS，但它并不是默认选择，这样选择的动机是什么？
最后，我们能够将 boosting_type=goss 作为参数传递。当我们这样做时会发生什么？算法会是GBDT，而数据样本策略是goss吗？

非常感谢您抽出时间。
祝你有美好的一天。]]></description>
      <guid>https://stackoverflow.com/questions/77578111/use-of-data-sample-methods-with-different-boosting-types-in-lightgbm</guid>
      <pubDate>Thu, 30 Nov 2023 11:34:21 GMT</pubDate>
    </item>
    <item>
      <title>如何在多个 GPU 上使用 Huggingface Trainer？</title>
      <link>https://stackoverflow.com/questions/75814047/how-to-use-huggingface-trainer-with-multiple-gpus</link>
      <description><![CDATA[假设我有以下模型（来自此脚本）：
从变压器导入 AutoTokenizer、GPT2LMHeadModel、AutoConfig

配置 = AutoConfig.from_pretrained(
    “gpt2”，
    vocab_size=len(分词器),
    n_ctx=上下文长度，
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
）
模型 = GPT2LMHeadModel(配置)

我目前正在为 Trainer 使用此训练参数：
从 Transformers 导入 Trainer、TrainingArguments

args = 训练参数(
    output_dir=“codeparrot-ds”，
    per_device_train_batch_size=32，
    per_device_eval_batch_size=32，
    评价_策略=“步骤”，
    评估步骤=5_000，
    记录步骤=5_000，
    梯度累积步数=8，
    num_train_epochs=1,
    权重衰减=0.1，
    热身步骤=1_000,
    lr_scheduler_type=“余弦”,
    学习率=5e-4,
    保存步骤=5_000，
    fp16=正确，
    Push_to_hub=真，
）

教练=教练（
    型号=型号，
    分词器=分词器，
    参数=参数，
    data_collat​​or = data_collat​​or，
    train_dataset=tokenized_datasets[“火车”],
    eval_dataset=tokenized_datasets[“有效”],
）
训练师.train()

我如何对此进行调整，以便训练器将使用多个 GPU（例如 8 个）？
我发现这个所以问题，但他们没有使用培训师，刚刚使用了 PyTorch 的 DataParallel
model = torch.nn.DataParallel(model, device_ids=[0,1])

Huggingface 有关使用多个 GPU 进行训练的文档对我来说并不是很清楚，也不明白没有使用 Trainer 的示例。相反，我发现这里他们添加了参数他们的 python 文件带有 nproc_per_node，但这对于他们的脚本来说似乎过于具体，并且不清楚一般如何使用。这与他们论坛上的此讨论相反“Trainer 类自动处理多 GPU 训练，您无需执行任何特殊操作。”。所以这很令人困惑，因为一方面他们提到在多个 GPU 上进行训练需要做一些事情，并且还说训练器会自动处理它。所以我不知道该怎么办。]]></description>
      <guid>https://stackoverflow.com/questions/75814047/how-to-use-huggingface-trainer-with-multiple-gpus</guid>
      <pubDate>Wed, 22 Mar 2023 15:10:23 GMT</pubDate>
    </item>
    <item>
      <title>Lightgbm 中“is_unbalance”参数的使用</title>
      <link>https://stackoverflow.com/questions/68738225/use-of-is-unbalance-parameter-in-lightgbm</link>
      <description><![CDATA[我尝试在模型训练中使用“is_unbalance”参数来解决二元分类问题，其中正类约为 3%。如果我设置参数“is_unbalance”，我会观察到二进制对数损失在第一次迭代中下降，但随后继续增加。仅当我启用此参数“is_unbalance”时，我才会注意到此行为。否则，log_loss 会稳步下降。感谢您对此的帮助。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/68738225/use-of-is-unbalance-parameter-in-lightgbm</guid>
      <pubDate>Wed, 11 Aug 2021 08:05:52 GMT</pubDate>
    </item>
    <item>
      <title>训练+测试集是否必须与预测集不同（以便您需要对所有列应用时移）？ （没有时间序列！）[关闭]</title>
      <link>https://stackoverflow.com/questions/59210109/does-the-trainingtesting-set-have-to-be-different-from-the-predicting-set-so-t</link>
      <description><![CDATA[TLDR：
这个问题与经典的机器学习时间序列分析无关，而是试图将每月的列作为纯粹的特征来处理，就像任何永恒的特征一样。我分享了这个问题，因为我在工作中遇到了这个挑战，最后，该模型在这种设置下运行良好，将非每月（永恒）功能与每月功能混合在一起。因此，这只是一个使用每月数据列作为特征的问题，模型并不关心是12月还是6月，它只关心这些特征是过去多少个月，以便它从模式中学习大约 x 个月前的数据。这些特征不是按月份名称来命名的，而是按它们回溯到过去的月份来命名的，例如，reality_month_1、reality_month_2 表示回溯 1 或 2 个月的财富。
&lt;小时/&gt;
我知道我们应该仅在测试集上测试经过训练的分类器的一般规则。
但现在出现了问题：当我准备好经过训练和测试的分类器时，我可以将其应用到作为训练和测试集基础的同一数据集吗？&lt; /em&gt; 或者我是否必须将其应用于与训练+测试集不同的新预测集？
如果我预测时间序列的标签列怎么办（稍后编辑：我并不是想在这里创建经典的时间序列分析，而是只是从典型数据库中广泛选择列，每周、每月或随机存储的数据，我将其转换为单独的特征列，每个特征列为一周/一个月/一年...），我是否必须转移全部将训练+测试集的特征（不仅是时间序列标签列的过去列，还包括所有其他正常特征）设置回数据没有“知识”的时间点与预测集的拦截？
然后，我将根据过去 n 个月的特征来训练和测试分类器，针对未移动且最新的标签列进行评分，然后根据最近未移动的特征进行预测。移位和未移位的特征具有相同的列数，我通过将移位特征的列名称分配给未移位的特征来对齐移位和未移位的特征。
附注：
p.s.1：https://en.wikipedia.org/wiki/Dependent_and_independent_variables&lt;的一般方法/a&gt;
在数据挖掘工具（用于多元统计和机器学习）中，因变量被分配为目标变量（或在某些工具中为标签属性），而自变量可能被分配为常规变量。[ 8]为训练数据集和测试数据集提供了目标变量的已知值，但应对其他数据进行预测。
p.s.2：在这个基本教程中，我们可以看到预测集有所不同：https://scikit-learn.org/stable/tutorial/basic/tutorial.html
我们使用 [:-1] Python 语法选择训练集，它会生成一个包含所有 &gt; 的新数组。但digits.data 中的最后一项：[…] 现在您可以预测新值。在这种情况下，您将使用digits.data [-1:]中的最后一个图像进行预测。通过预测，您将从训练集中确定与最后一个图像最匹配的图像。]]></description>
      <guid>https://stackoverflow.com/questions/59210109/does-the-trainingtesting-set-have-to-be-different-from-the-predicting-set-so-t</guid>
      <pubDate>Fri, 06 Dec 2019 09:16:37 GMT</pubDate>
    </item>
    <item>
      <title>正则化参数在正则化中如何工作？</title>
      <link>https://stackoverflow.com/questions/44742122/how-does-regularization-parameter-work-in-regularization</link>
      <description><![CDATA[在机器学习成本函数中，如果我们想最小化两个参数（例如 theta3 和 theta4）的影响，似乎我们必须给出一个较大的正则化参数值，如下式所示。

我不太清楚为什么更大的正则化参数会减少而不是增加影响。这个功能是如何工作的？]]></description>
      <guid>https://stackoverflow.com/questions/44742122/how-does-regularization-parameter-work-in-regularization</guid>
      <pubDate>Sun, 25 Jun 2017 00:26:41 GMT</pubDate>
    </item>
    </channel>
</rss>