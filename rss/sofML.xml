<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Wed, 19 Mar 2025 18:25:12 GMT</lastBuildDate>
    <item>
      <title>为什么尽管使用了stratifiedkfold，为什么我的precision-recall曲线嘈杂？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79520530/why-is-my-precision-recall-curve-noisy-despite-using-stratifiedkfold</link>
      <description><![CDATA[我正在研究Kaggle上的泰坦尼克号生存挑战，并试图为我的模型绘制Precision-Recall曲线。但是，曲线看起来非常嘈杂且锯齿状 - 不像我预期的那么光滑。
我知道数据集是不平衡的，因此我使用StratifiedKfold在交叉验证过程中维持班级比例。这是我所做的一般流程：

使用 StratifiedKfold （N_SPLITS = 3）将数据分开
培训了SVM分类器（SVC）
使用 cross_val_predict（） with  method =&#39;deciption_function&#39;获得得分
使用precision_recall_curve（）绘制Precision-Recall曲线

即使在那之后，曲线仍然看起来很吵。
这主要是由于类不平衡引起的，还是可能与：有关

使用SVM的Decision_Function输出？
数据集大小？
我如何构造交叉验证或评分的可能问题？

Here&#39;s my Kaggle notebook for reference: https://www.kaggle.com/code/jayasuryanmutyala/titanic-survival &lt;img alt =“ precision-recall曲线”]]></description>
      <guid>https://stackoverflow.com/questions/79520530/why-is-my-precision-recall-curve-noisy-despite-using-stratifiedkfold</guid>
      <pubDate>Wed, 19 Mar 2025 14:45:26 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 @React-Native-Ml-Kit/Face-tetection获得68个面部地标？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79519896/how-to-get-68-facial-landmarks-using-react-native-ml-kit-face-detection</link>
      <description><![CDATA[我使用@react-native-ml-kit/face-tetection 在我的反应本机项目中实现面部识别。但是，我只获得20个面部标志，而我需要68个完整的面部标记（包括下巴，眉毛，眼睛，鼻子和嘴唇）才能准确识别。
我尝试过的
我已经配置了@react-native-ml-kit/face-detection 带有以下选项：
 从&#39;@react-native-ml-kit/face-tetection&#39;导入视觉&#39;;

const options = {perfermancemode：&#39;cuccurate&#39;，landmarkmode：&#39;all&#39;，classificationmode：&#39;all&#39;，contourmode：&#39;all&#39;，};
const结果=等待vision.detectfaces（imageuri，options）;
console.log（结果）;
 
尽管启用了地标：&#39;all&#39;和Contourmode：&#39;ash oft&#39;，输出仅包含20个地标，而不是68。
问题

 我如何获得所有68个面部地标，而不仅仅是20？

 是否需要特定的配置或预处理步骤？

  @react-native-ml-kit/face-detection 支持68个地标，还是我应该使用替代库？


附加上下文

 我检查了正式文档，但找不到启用详细的68点地标检测的选项。

 如果@react-native-ml-kit/face-detection 不支持这一点，是否有任何建议与React Antial合作的替代方案？

]]></description>
      <guid>https://stackoverflow.com/questions/79519896/how-to-get-68-facial-landmarks-using-react-native-ml-kit-face-detection</guid>
      <pubDate>Wed, 19 Mar 2025 10:26:45 GMT</pubDate>
    </item>
    <item>
      <title>使用RLLIB，KL损失差异和稳定性问题的PPO培训[关闭]</title>
      <link>https://stackoverflow.com/questions/79519678/ppo-training-with-rllib-kl-loss-divergence-and-stability-problems</link>
      <description><![CDATA[我面临在相当复杂的环境中训练代理商的困难。我简要描述它以供参考。

 obs：12（ +-1之间）
 ACT：5（平均）（ + -1）， + 5（log_std）
简短的情节（专家代理人将以大约7个步骤解决）
环境的相当复杂的动态（空间轨迹）
步骤返回约为-3 / +5，最后一步是成功（+10）或影响（-2）。或多或少，最低剧集的回报为-13，最大返回为30。

我为自定义网络尝试了不同的配置。我现在想使用的那个由3个网组成：

平均网络：2 HID Lay，128个神经元，tanh + 1外，5个神经元，tanh 
 log_std net：2 HID Lay，128个神经元，Tanh + 1 Out Lay，5个神经元，Tanh 
 vf：2 HID Lay，256个神经元，tanh + 1外，1个神经元，线性
我还尝试了一个具有均值 / log_std结构的版本和不同的n神经元 /层（例如256平均 / log_std和VF的1024，3个HID层）&lt; / li&gt;

要在培训开始时获得合理的性病，我将log_std net的输出层的偏见设置为10，因此tanh返回几乎1。
 low_bound + 0.5 *（up_bound -low_bound） *（self._policy_net_log_std（features） + 1） with  low_bound = -5.0  and code&gt; and  up_bound = 0.0 。这意味着，在培训开始时，性病几乎是1，应随着培训的进行而减小。我还尝试了不同的低 /上界，也尝试了不同的偏见，以免以任何一个限制启动培训。&lt; / p&gt;
对于哪些关注的超参数，此处也已经测试了不同的配置。

伽玛：0.99 
 lr：3e-3，1e-3，3e-4，1e-4等
 lambda_：0.97，0.95，0.90 
 entropy_coeff：3E-3，3e-4，3e-5，3e-6 
 clip_param：3/2/1e-1，3/2/1e-2 
 vf_loss_coeff = 1 
 vf_clip_param = 10，无
 train_batch_size_per_learner = 1024，2048 
 minibatch_size = 64、128、256、512 
 num_epoch = 1，3，5，10 

这是问题所在。对于某些配置，所有内容似乎都可以到培训的某个点。特别是，高初始性病和低LR似乎是最有希望的配置。但是，突然之间， curr_kl_coeff 和 sean_kl_loss 中出现不稳定性。有时也 policy_loss 爆炸。
只有一种配置“工作” (256 neurons, 2 hid lay, for mean and log_std (separated) and log_std biases to 10 / 1024 neurons, 2 hid lay, for vf) with lr = 3e-4, lambda_ = 0.95, entropy_coeff = 3e-4, clip_param = 3e-1, vf_clip_param = None, train_batch_size_per_learner = 2048, minibatch_size = 128，num_epochs = 5。但是，即使正确学习，熵也保持恒定并且没有减少，这使我认为性病也没有减少。因此，培训似乎是成功的，但完全是随机的。
在所有失败的情况下，在培训的某个点，梯度似乎爆炸：
 警告Torch_learner.py:260-跳过此更新。如果更新了Nan/Inf 
不应完全跳过梯度，而是NAN/INF梯度设置为零 
将torch_skip_nan_gradients设置为false。
 
有人面临类似问题 /找到解决方案吗？&lt; / p&gt;]]></description>
      <guid>https://stackoverflow.com/questions/79519678/ppo-training-with-rllib-kl-loss-divergence-and-stability-problems</guid>
      <pubDate>Wed, 19 Mar 2025 09:10:49 GMT</pubDate>
    </item>
    <item>
      <title>Tesseract OCR LSTMTRAINing |无法正确更新Eng_custom.traindata</title>
      <link>https://stackoverflow.com/questions/79519232/tesseract-ocr-lstmtraining-could-not-update-eng-custom-traineddata-properly</link>
      <description><![CDATA[我有一些文本，这是tiff文件上的单个单词，旨在训练eng_custom.tradicdata。
目前，我在下面使用语法，看起来很理智，并且在上一步之前不会产生任何错误。
 重要：
我不想将当前方法更改为我的目标，即用相同的参数训练1000个TIFF文件中的每一个，因为我为每个TIFF准备了相应的镶嵌和盒子。
  #make lstmf文件 
  tesseract test_sample.tiff test_sample \
      -tessdata-dir/home/j/img2/tess_files \
      -PSM 7 -OEEM 1 -L ENG_CUSTOM \
      /home/j/tesseract/tessdata/configs/lstm.train
echo“ test_sample.lstmf” ＆gt; single_lstmf_file.txt
 
  #train LSTM模型 
  lstMtraining \
  -model_output tess_training.lstm \
  -continue_from/home/j/img2/tess_files/eng.lstm \
  -traineddata/home/j/img2/tess_files/eng_custom.traineddata \
  -train_listfile single_lstmf_file.txt \
  -max_iterations 1
 
停止训练和最终确定模型
  lstMtraining -Stop_training \
  -continue_from tess_training.lstm_checkpoint \
  -traineddata/home/j/img2/tess_files/eng_custom.traineddata \
  -model_output/home/j/img2/tess_files/eng_final.lstm
 
使用新LSTM模型更新训练Data 
  mkdir -p/home/j/img2/base_model  
combine_tessdata -u/home/j/img2/tess_files/eng_custom.traineddata/home/j/img2/base_model/eng_custom  
cp/home/j/img2/tess_files/eng_final.lstm/home/j/img2/base_model/eng.lstm
combine_tessdata/home/j/img2/base_model/eng_custom
cp/home/j/img2/base_model/eng_custom.traineddata/home/j/img2/tess_files/eng_custom.traineddata
 
 ，但我在最后一步中遇到问题： 
  j@j：〜/t $ tesseract test_sample.tiff stdout -l eng_custom -tessdata -dir/home/j/j/img2/tess_files/
索引＆gt; = 0：错误：assert失败：在file/home/j/tesseract4/src/ccutil/strngs.cpp中，行266
中止（核心倾倒）
 
 问题：
如何修改上面的命令，以便我可以将eng_final.lstm与eng_custom.traineddata 组合
 环境： 

/home/j/img2/tess_files/
eng.traineddata eng_custom.traineddata eng.lstm eng_final.lstm
/home/j/img2/base_model/
ENG_CUSTOM.BIGRAM-DAWG ENG_CUSTOM.NORMPROTO 
eng_custom.word-dawg eng_custom.freq-dawg 
eng_custom.number-dawg eng.lstm eng_custom.inttemp 
eng_custom.pffmtable eng.lstm-number-dawg eng_custom.lstm 
eng_custom.punc-dawg eng.lstm-punc-dawg
eng_custom.lstm-number-dawg eng_custom.shapetable 
eng.lstm-recoder eng_custom.lstm-punc-dawg eng_custom.traineddata 
ENG.LSTM-UNICHARSET ENG_CUSTOM.LSTM-RECODER 
eng_custom.unicharambigs eng.lstm-word-dawg
ENG_CUSTOM.LSTM-UNICHARSET ENG_CUSTOM.UNICHARSET ENG.VERSION
eng_custom.lstm-word-dawg eng_custom.version 
]]></description>
      <guid>https://stackoverflow.com/questions/79519232/tesseract-ocr-lstmtraining-could-not-update-eng-custom-traineddata-properly</guid>
      <pubDate>Wed, 19 Mar 2025 05:56:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的高准确性MNIST分类器在React.js中的用户画数字上失败？</title>
      <link>https://stackoverflow.com/questions/79518843/why-does-my-high-accuracy-mnist-classifier-fail-on-user-drawn-digits-in-react-js</link>
      <description><![CDATA[我在MNIST数据集上训练了一个简单的二进制分类器，在测试数据上达到了99％+的精度。但是，当我将其集成到React.js应用中时，用户在画布上画数字时，该模型完全无法识别它们。我有这些方法的敏感性类别
  //步骤功能（Heaviside功能）
  //它无法学习诸如XOR之类的模式，XOR需要多个层。仅线性可分离数据
  //由于它不是可区分的，因此不能使用诸如propogation之类的梯度基准方法进行优化
  stepfunction（value）{
    返回值＆gt; = 0？ 1：0;
  }

  preditiveLabel（功能）{
    令总计= this.biasterm;
    for（让i = 0; i＆lt; trauty.length; i ++）{
      总计 +=功能[i] * this.weightparams [i];
    }
    返回this.Stepfunction（Total）;
  }

  TrainModel（训练样本，TargetLabels）{
    for（让i = 0; i＆lt; trainingsamples.length; i ++）{
      令SampleInput =训练样本[i];
      const predict predictOutput = this.predictLabel（sampleInput）;
      const actuallabel = targetLabels [i];

      if（Actuallabel！== predictationOutput）{
        for（令J = 0; J＆lt; this.weightparams.length; J ++）{
          this.WeightParams [J] +=
            this.learningrate *
            （Actuallabel-预测输出） *
            样品input [j];
        }
        this.biassterm += this.learningrate *（actuallabel -predictalOutput）;
      }
    }
  }
 
这在测试数据上达到了99％+精度。但是它在用户绘制的图像上表现不佳。

 我将火车输入和测试输入标准化
 // traininputs和测试输入
功能预处理图（Pixelarray）{
返回pixelarray.map（（（row）=＆gt; row.map（（（pixel）=＆gt; pixel / 255.0）））;
} 

  i除去低强度像素以滤波噪声。
  const processedimages = testimages.data.map（（（image）=＆gt;
     //我用此变量noings_removal_threshold付款
     image.map（（（row）=＆gt;
       row.map（（（pixel）=＆gt;（pixel＆gt; = noyes_removal_threshord？pixel：0））
     ）
   ）；
 


我知道MNIST数字是居中，薄且定义明确的，因此我相应地绘制数字。
我的MNIST模型在训练过程中达到了很高的精度，但是当用户在画布上写数字时，它无法很好地预测。导致此差距的关键因素是什么，我如何改善现实世界的性能？]]></description>
      <guid>https://stackoverflow.com/questions/79518843/why-does-my-high-accuracy-mnist-classifier-fail-on-user-drawn-digits-in-react-js</guid>
      <pubDate>Wed, 19 Mar 2025 00:04:15 GMT</pubDate>
    </item>
    <item>
      <title>寻找具有REST API集成的AI或机器学习工具[已关闭]</title>
      <link>https://stackoverflow.com/questions/79517734/looking-for-ai-or-machine-learning-tools-with-rest-api-integration</link>
      <description><![CDATA[我正在研究可以通过REST API集成到我的React.js应用程序中的工具或AI解决方案。具体来说，我正在寻找可以：的解决方案

协助测验创建和分级（从提示中生成测验或使用现有问题，具有添加新的问题）。
分析和等级用户提取的文本/期刊，提供简短的反馈和可能的评分。

您有任何建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79517734/looking-for-ai-or-machine-learning-tools-with-rest-api-integration</guid>
      <pubDate>Tue, 18 Mar 2025 14:14:17 GMT</pubDate>
    </item>
    <item>
      <title>pytorch中的CNN模型时，损失不会减少[封闭]</title>
      <link>https://stackoverflow.com/questions/79517689/loss-is-not-reducing-when-traiing-cnn-model-in-pytorch</link>
      <description><![CDATA[我正在使用带有3个类的数据集培训CNN模型（我同时尝试使用自定义CNN和VGG19模型）。从一个时期开始，损失几乎保持恒定。我将在下面附上我的Colab链接以供您参考：
我为第一个时期的训练损失看起来像这样：

 epoch：0 |火车损失：1.4142，火车准确性：0.3319 |测试损失：
1.1003，测试精度：0.3333 
时代：1 |火车损失：1.1002，火车准确性：0.3369 |测试损失：
1.1008，测试准确性：0.3333 
时代：2 |火车损失：1.1006，火车准确性：0.3344 |测试损失：
1.0991，测试精度：0.3333 
时代：3 |火车损失：1.1006，火车准确性：0.3313 |测试损失：
1.0990，测试精度：0.3333 

我已经尝试了更长的时期的相同方法，但不会收敛。因此，对于50个时期，损失和准确性也保持不变。
即使损失减少，也很慢。
我在不同数据集上使用了相同的模型，因此，对于100个时期，损失和准确性也保持不变。]]></description>
      <guid>https://stackoverflow.com/questions/79517689/loss-is-not-reducing-when-traiing-cnn-model-in-pytorch</guid>
      <pubDate>Tue, 18 Mar 2025 13:58:38 GMT</pubDate>
    </item>
    <item>
      <title>当我使用Torch.Distribed.Pipelining训练模型时，为什么损失不减少？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79516152/why-doesnt-the-loss-decrease-when-i-use-torch-distributed-pipelining-to-train-t</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79516152/why-doesnt-the-loss-decrease-when-i-use-torch-distributed-pipelining-to-train-t</guid>
      <pubDate>Tue, 18 Mar 2025 02:47:07 GMT</pubDate>
    </item>
    <item>
      <title>将缩放器保存在模型中</title>
      <link>https://stackoverflow.com/questions/73053195/saving-a-scaler-in-a-model</link>
      <description><![CDATA[我正在使用以下缩放器：
  sualer = preprocessing.minmaxscaler（）

x_train = pd.dataframe（scaler.fit_transform（dataset_train）， 
                              列= dataset_train.columns， 
                              index = dataset_train.index）

x_train.sample（frac = 1）

x_test = pd.dataframe（scaler.transform（dataset_test）， 
                             列= dataset_test.columns， 
                             index = dataset_test.index）
 
我正在用于异常检测的模型是顺序的（）。
我知道如何保存模型，但是我的问题是如何将缩放器保存在模型中，因此我可以简单地将模型应用于新的DF中。]]></description>
      <guid>https://stackoverflow.com/questions/73053195/saving-a-scaler-in-a-model</guid>
      <pubDate>Wed, 20 Jul 2022 14:09:04 GMT</pubDate>
    </item>
    <item>
      <title>如何使用Sklearn MinMaxScaler（）缩放预测数据？</title>
      <link>https://stackoverflow.com/questions/56177767/how-to-scale-back-predicted-data-with-sklearn-minmaxscaler</link>
      <description><![CDATA[我正在创建一个神经网络来denoise音乐。 
模型的输入是一个从0到1缩放的数组。这是使用Sklearn Minmaxscaler实现的。数据的原始范围从-1到1。模型的输出也是一个阵列从0到1。。

我在预测信息时不能将数据扩展到-1至1。 
我的代码类似于：
  data = load（data_path）
sualer = minmaxscaler（feature_range =（0,1））
data = data.Reshape（-1,1）
data = sualer.fit_transform（数据）

型号= load_model（model_path）
predicted_data = model.predict（数据）

predicted_data = sualer.inverse_transform（predicted_data）
 
我收到错误：
 此MinMaxScaler实例尚未拟合。在使用此方法之前，请使用适当的参数调用“适合”。
 
但是，数据已经拟合，我不想再次适合它。 
为什么我确切遇到此错误？ MinMaxScaler是否仍然能够对未插入数据进行unverse_transform？ 
关于此错误有任何建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/56177767/how-to-scale-back-predicted-data-with-sklearn-minmaxscaler</guid>
      <pubDate>Thu, 16 May 2019 23:26:26 GMT</pubDate>
    </item>
    <item>
      <title>python中的更快的KNN分类算法</title>
      <link>https://stackoverflow.com/questions/51688568/faster-knn-classification-algorithm-in-python</link>
      <description><![CDATA[我想从头开始编码自己的KNN算法，原因是我需要加重功能。问题是，尽管删除了循环并使用内置的Numpy功能，但我的程序仍然非常慢。
谁能提出一种加快速度的方法？我不为L2距离使用 np.sqrt ，因为它是不必要的，并且实际上会大大减慢。
 类GlobalWeightedKnn：
    ”“”
    具有功能重量的K-NN分类器

    回报：K-NN的预测。
    ”“”

    def __init __（自我）：
        self.x_train =无
        self.y_train =无
        self.k =无
        self.pewights =无
        self.predictions = list（）

    def fit（self，x_train，y_train，k，weights）：        
        self.x_train = x_train
        self.y_train = y_train
        self.k = k
        self.weights =权重

    def预测（self，testing_data）：
        ”“”
        采用2D查询案例。

        返回K-NN分类器的预测列表
        ”“”

        np.fromiter（（（self .__ helper（QC））用于testing_data中的QC），float）  
        返回自我预测


    def __helper（Self，QC）：
        邻居= np.fromiter（（（self .__加权_euclidean（qc，x）for x self.x_train中的x），float），float）
        邻居= np.array（[[邻居]） 
        indexes = np.array（[range（len（self.x_train）））]
        邻居= np.append（索引，邻居，轴= 1）

        ＃排序第二列 - 距离
        邻居=邻居[邻居[：1] .argsort（）]  
        k_cases =邻居[：self.k]
        indexes = [x [0]对于k_case中的x]

        y__answers = [self.y_train [int（x）for Indexes中的x]
        答案= max（set（y_answers），key = y_answers.count）＃获取最常见的值
        self.predictions.append（答案）


    def __weighted_euclidean（self，QC，其他）：
        ”“”
        定制加权欧几里得距离

        返回：浮点号
        ”“”

        返回np.sum（（（（（QC-其他）** 2）*自我）
 ]]></description>
      <guid>https://stackoverflow.com/questions/51688568/faster-knn-classification-algorithm-in-python</guid>
      <pubDate>Sat, 04 Aug 2018 18:39:24 GMT</pubDate>
    </item>
    <item>
      <title>如何在Openai创建新的健身房环境？</title>
      <link>https://stackoverflow.com/questions/45068568/how-to-create-a-new-gym-environment-in-openai</link>
      <description><![CDATA[我有一项作业来制作AI代理，该代理将学习使用ML玩视频游戏。我想使用OpenAI健身房创建一个新的环境，因为我不想使用现有环境。如何创建一个新的自定义环境？
另外，我有什么其他方法可以开始开发制作AI代理在没有Openai Gym的帮助的情况下玩特定的视频游戏？]]></description>
      <guid>https://stackoverflow.com/questions/45068568/how-to-create-a-new-gym-environment-in-openai</guid>
      <pubDate>Wed, 12 Jul 2017 22:32:21 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法在Scikit-Learn中保存预处理对象？ [复制]</title>
      <link>https://stackoverflow.com/questions/42843352/is-there-a-way-to-save-the-preprocessing-objects-in-scikit-learn</link>
      <description><![CDATA[我正在建立一个神经网，目的是对未来的新数据进行预测。我首先使用sklearn.preprocessing进行训练数据，然后训练模型，然后进行一些预测，然后关闭程序。将来，当新数据进来时，我必须使用相同的预处理量表来转换新数据，然后再将其放入模型中。目前，我必须加载所有旧数据，安装预处理器，然后与这些预处理器转换新数据。我有没有办法保存预处理对象对象（例如sklearn.preprocessing.standardscaler），以便我可以加载旧对象而不是必须重新制作它们？]]></description>
      <guid>https://stackoverflow.com/questions/42843352/is-there-a-way-to-save-the-preprocessing-objects-in-scikit-learn</guid>
      <pubDate>Thu, 16 Mar 2017 19:36:36 GMT</pubDate>
    </item>
    <item>
      <title>SCI-KIT学习：调查错误分类的数据</title>
      <link>https://stackoverflow.com/questions/34550577/sci-kit-learn-investigating-incorrectly-classified-data</link>
      <description><![CDATA[我想分析使用SCI-KIT Learn通过模型对模型进行错误分类的数据，以便可以改善功能生成。我有一种方法可以做到这一点，但是我既是Sci-Kit学习的新手，又是Pandas，所以我想知道是否有更有效/直接的方法来实现这一目标。这似乎是标准工作流程的一部分，但是在我所做的研究中，我没有找到任何直接解决此映射的内容，从模型分类到通过功能矩阵到原始数据。 
这是我正在使用的上下文/工作流程，以及我设计的解决方案。下面是示例代码。 
上下文。我的工作流程看起来像这样：

从一堆JSON BLOBS开始，原始数据。这是Pandas DataFrame。 
提取建模的相关作品，将其称为数据。这是Pandas DataFrame。
此外，我们还有所有数据的真实数据，因此我们将其称为真相或y。 
在SCI-KIT学习中创建功能矩阵，称之为X。这是一个大的稀疏矩阵。 
创建一个随机的森林对象，称之为森林。 
创建功能矩阵的随机子集，用于使用SCI-KIT训练和测试学习split_train_test（）函数。
在上面的训练数据X_Train上训练森林，这是一个稀疏的矩阵。 
获取假阳性和假阴性结果的索引。这些是x_test的索引，一个稀疏的矩阵。
从一个误报索引转到x_test返回原始数据
如有必要，请从数据转到原始数据。

解决方案。 

将索引阵列传递到split_test_train（）函数，该功能将在索引数组上应用相同的随机器，并将其返回为火车和测试数据（idx_test）的索引
收集误报和假否定的索引，这些都是nd.arrays 
使用这些查找索引数组中的原始位置，例如index = idx_test [false_example] false_example in false_neg array 
使用该索引查找原始数据，data.iloc [索引]是原始数据
然后data.index [index]如果需要，将索引值返回到原始数据中

此处使用推文关联的代码。同样，这起作用了，但是是否有更直接/更聪明的方法可以做到这一点？ 
 ＃取一个我们的原始数据样本
data = tweet_df [0：100] [&#39;texts&#39;]
y = tweet_df [0：100] [&#39;真相&#39;]

＃创建功能向量
vec = tfidfvectorizer（Analyzer =“ char”，ngram_range =（1,2））
x = vec.fit_transform（数据）＃现在是特征矩阵

＃将功能矩阵拆分为火车/测试子集，将索引返回原始X中
＃数组索引
indices = np.Arange（X.Shape [0]）
x_train，x_test，y_train，y_test，idx_train，idx_test = train_test_split（x，x，y，indices，test_size = 0.2，tast_size = 0.2，randy_state = state）

＃适合并测试模型
森林= RandomforestClassifier（）
Forest.fit（x_train，y_train）
预测=森林。预定（x_test）

＃在测试集中获取false_negative和false_positives的索引
false_neg，false_pos = tweet_fns.check_predictions（预测，y_test）

＃映射测试集（特征）中的假阴性索引回到其原始数据（文本）
打印“假否定性：\ n”
pd.options.display.max_colwidth = 140
因为我在false_neg中：
    Original_index = idx_test [i]
    打印数据
 
和checkpredictions函数：
  def check_predictions（预测，真相）：
    ＃从模型中获取1 dim的预测阵列和1 dim真相向量并计算相似性
    ＃在预测中返回虚假负面和误报的索引。 

    真相=真相。版本（布尔）
    预测=预测。版本（bool）
    打印总和（预测==真相），&#39;&#39;，&#39;，len（truth），“或”，float（sum（precution == thrip）））/float（len（truth）），“匹配”

    ＃误报
    打印“误报：”，总和（预测＆amp;〜真相）
    ＃假否定性
    打印“假否定因素：”，总和（〜预测＆amp;真相）
    false_neg = np.nonzero（〜预测＆amp; truth）＃这些是数组的元组
    false_pos = np.nonzero（预测＆amp;〜truth）
    返回falses_neg [0]，false_pos [0]＃我们只希望阵列返回
 ]]></description>
      <guid>https://stackoverflow.com/questions/34550577/sci-kit-learn-investigating-incorrectly-classified-data</guid>
      <pubDate>Thu, 31 Dec 2015 18:54:58 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证与一部分的精确评分</title>
      <link>https://stackoverflow.com/questions/20772601/cross-validation-with-precision-scoring-for-a-subset-of-classes</link>
      <description><![CDATA[我有一个用于分类的数据集，其中3个类标签 [0,1,2] 。
我想运行交叉验证并尝试多个估计器，但是我有兴趣以1和2类的精度进行评分。我不在乎类0的精度，而且我不希望它的得分来放弃简历优化。我也不在乎任何课程的召回。换句话说，我想确保每当1或2的预测时，都会有很高的信心。
所以问题是，如何运行 cross_val_score 并告诉其评分功能以忽略类0？的精度

  update ：根据接受的答案，这是一个示例答案代码：
  def custom_precision_score（y_true，y_pred）：
  precision_tuple，recker_tuple，fscore_tuple，support_tuple = metrics.precision_recall_fscore_support（y__true，y__pred）
  precision_tuple = precision_tuple [1：]
  support_tuple = support_tuple [1：]
  加权_precision = np.average（precision_tuple，strights = support_tuple）
  返回加权_precision

custom_scorer = metrics.make_scorer（custom_precision_score）

scores = cross_validation.cross_val_score（clf，featursarray，targetarray，cv = 10，评分= custom_scorer）
 ]]></description>
      <guid>https://stackoverflow.com/questions/20772601/cross-validation-with-precision-scoring-for-a-subset-of-classes</guid>
      <pubDate>Wed, 25 Dec 2013 12:03:54 GMT</pubDate>
    </item>
    </channel>
</rss>