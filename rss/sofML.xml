<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 30 Nov 2024 01:19:17 GMT</lastBuildDate>
    <item>
      <title>如何在拆分后合并 tarin 和测试数据集但每行回到其原始位置或索引</title>
      <link>https://stackoverflow.com/questions/79238631/how-to-merge-tarin-and-test-dataset-after-splitting-but-each-row-back-to-its-ori</link>
      <description><![CDATA[import pandas as pd
import numpy as np
from sklearn.datasets import fetch_openml
# 加载糖尿病数据集 
diabetes = fetch_openml(&quot;diabetes&quot;, version=1, as_frame=True)
diabetes_df = diabetes.data
diabetes_df[&#39;target&#39;] = diabetes.target # 添加目标列

from sklearn.model_selection import train_test_split
# 拆分数据
train, test = train_test_split(diabetes_df, test_size=0.3, random_state=42)

我想合并训练和测试数据集，但与原始数据索引相同。
注意：但在出现一些失误后，我将数据集拆分为训练和测试数据集，并分别估算训练和测试数据集]]></description>
      <guid>https://stackoverflow.com/questions/79238631/how-to-merge-tarin-and-test-dataset-after-splitting-but-each-row-back-to-its-ori</guid>
      <pubDate>Sat, 30 Nov 2024 00:21:23 GMT</pubDate>
    </item>
    <item>
      <title>在 counterfactual-vulnerability-detection 存储库中运行数据预处理脚本时出错</title>
      <link>https://stackoverflow.com/questions/79238608/error-while-running-data-preprocessing-script-in-counterfactual-vulnerability-de</link>
      <description><![CDATA[我想在 CPU 上运行存储库 https://github.com/zahrazarezadeh1999/counterfactual-vulnerability-detection/tree/main 中的代码。
我正在运行一个名为 data_pre.py 的 Python 脚本作为项目的一部分，在处理数据时遇到了多个问题。最初，该脚本尝试访问位于 /home/user20/Desktop/counterfactual-vulnerability-detection-main/cfexplainer/storage/cache/minimal_datasets/minimal_bigvul.pq 的文件，但失败并出现“没有这样的文件或目录”错误。该文件似乎是该过程的关键部分，它的缺失阻止了脚本按预期继续运行。尽管出现了这个错误，但脚本仍继续运行一些操作，并显示数据处理的进度条，但很明显，丢失的文件导致了下游的重大问题。
后来，脚本又抛出了另一个错误，这次与 pandas 中的 DataFrame 操作有关。错误消息指出：ValueError：无法将没有列的 DataFrame 设置为列 mod_prop。似乎由于文件丢失或流程早期的其他问题，预计应该有数据的 DataFrame 要么没有正确加载，要么是空的。这个错误完全停止了脚本的执行，让我无法完成任务。
我使用 GPU 运行代码并遇到了同样的错误。这次，我尝试在 CPU 上运行它，但再次遇到了同样的错误。您建议如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79238608/error-while-running-data-preprocessing-script-in-counterfactual-vulnerability-de</guid>
      <pubDate>Fri, 29 Nov 2024 23:57:50 GMT</pubDate>
    </item>
    <item>
      <title>计算/计算 k 模式的 SSB（簇之间）[关闭]</title>
      <link>https://stackoverflow.com/questions/79238050/computing-calculating-ssb-between-clusters-for-k-modes</link>
      <description><![CDATA[我正在尝试计算用于验证/计算 K 模式性能的指标。我正在做我的论文，我需要根据二元变量（疾病）对患者进行分组
我最近读到 SSW 和 SSB 指标可能合适（https://math.stackexchange.com/questions/1009297/variances-for-k-means-clustering）。我只知道如何从 kmodes 函数中提取 SSW（此处回复了 K-Modes Cluster Validation），但我不知道如何计算 SSB（簇之间）...您能给出一个如何计算它的例子吗？
我试过计算指标，但我不知道如何计算组间方差]]></description>
      <guid>https://stackoverflow.com/questions/79238050/computing-calculating-ssb-between-clusters-for-k-modes</guid>
      <pubDate>Fri, 29 Nov 2024 18:24:49 GMT</pubDate>
    </item>
    <item>
      <title>这些 `[0]` 在创建变量时是否有意义</title>
      <link>https://stackoverflow.com/questions/79236682/do-those-0-make-sense-in-making-the-variable</link>
      <description><![CDATA[使用 HuggingFace 工具集微调 Gemma 的指南位于：https://huggingface.co/blog/gemma-peft
链接到以下行：https://huggingface.co/blog/gemma-peft#:~:text=Quote%3A%20%7Bexample-,%5B%27quote%27%5D%5B0%5D,-%7D%5CnAuthor%3A
数据输入格式化函数是：
def formatting_func(example):
text = f&quot;Quote: {example[&#39;quote&#39;][0]}\nAuthor: {example[&#39;author&#39;][0]}&lt;eos&gt;&quot;
return [text]

这些 [0] 有意义吗？它们看起来不对，因为当打印出 text 变量时，我可以看到它们只是字符而不是字符串。]]></description>
      <guid>https://stackoverflow.com/questions/79236682/do-those-0-make-sense-in-making-the-variable</guid>
      <pubDate>Fri, 29 Nov 2024 10:14:43 GMT</pubDate>
    </item>
    <item>
      <title>运行 BayesSearchCV 查找 ANN 回归的最佳超参数时出错</title>
      <link>https://stackoverflow.com/questions/79236534/error-while-running-bayessearchcv-for-finding-best-hyperparameter-of-ann-regress</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79236534/error-while-running-bayessearchcv-for-finding-best-hyperparameter-of-ann-regress</guid>
      <pubDate>Fri, 29 Nov 2024 09:40:18 GMT</pubDate>
    </item>
    <item>
      <title>发生推理时间延迟 [关闭]</title>
      <link>https://stackoverflow.com/questions/79236337/inference-time-delay-occured</link>
      <description><![CDATA[我正在研究 GFPGAN 模型。我想使用 Python 的多处理功能同时实现多帧处理功能。
当我运行一个进程时，我会遇到大约 0.25 秒的延迟。当我同时运行两个进程时，延迟会增加到大约 0.45 秒。当三个进程同时运行时，延迟会增加到大约 0.65 秒。
我确信并行处理正在运行，但在同时运行多个进程时，我会遇到额外的延迟。
这是什么原因造成的，我如何在使用多个进程时减少额外的延迟？
我为每个进程实现了加载模型文件 (*.pth) 和推理函数，并同时调用这些进程。我为每个进程创建了时间戳；因此，推理函数的开始时间相同，但结束时间略有不同。]]></description>
      <guid>https://stackoverflow.com/questions/79236337/inference-time-delay-occured</guid>
      <pubDate>Fri, 29 Nov 2024 08:32:20 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中 BayesianFPN 的张量维度不匹配</title>
      <link>https://stackoverflow.com/questions/79236267/mismatch-in-tensor-dimensions-in-bayesianfpn-with-reinforcement-learning</link>
      <description><![CDATA[我正在实施一个计算机视觉项目。我在这个项目中使用了 FPN（带有 ResNet50 主干）和 BayesianFPN。该网络位于强化学习代理之下。实施后，它抛出了我一个
RuntimeError：张量 a（64）的大小必须与非单例维度 3 上的张量 b（256）的大小匹配


是什么导致代码抛出此错误？
是因为训练图像（RGB 图像）和验证图像（二进制掩码）不匹配吗？
我是否遗漏了一些内部维度更改？

以下是代码。这是我想要实现的 BayesianFPNwithRL 类。
我尝试重塑权重张量。但没有成功。即使解压或扩展它也没有。
供参考：

RGB 图像尺寸 - 1280x720px;
二进制掩码尺寸：1280x720px

# Bayesian FPN with RL
class BayesianFPNWithRL(nn.Module):
def __init__(self, backbone_with_fpn, rl_agent, dropout_p=0.2):
super(BayesianFPNWithRL, self).__init__()
self.backbone_with_fpn = backbone_with_fpn
self.dropout = nn.Dropout(p=dropout_p)
self.rl_agent = rl_agent

def forward(self, x, mc_samples=10, train_rl=False):

fpn_outputs = self.backbone_with_fpn(x)
keys = list(fpn_outputs.keys())
features = [fpn_outputs[key] for key in keys]

common_size = features[0].shape[2:] 
features = [F.interpolate(f, size=common_size, mode=&quot;nearest&quot;) for f in features]

if not self.training:
sampled_features = []
for _ in range(mc_samples):
sampled_features.append([self.dropout(f) for f in features])
features = [
torch.mean(torch.stack([sample[i] for sample in sampled_features]), dim=0)
for i in range(len(features))
]

global_features = [f.mean(dim=(2, 3)) for f in features] 
rl_input = torch.cat(global_features, dim=1) 

action, log_prob = self.rl_agent.select_action(rl_input)

weights = torch.zeros(len(features), device=x.device)
weights[action] = 1.0 

selected_features = sum(w * f for w, f in zip(weights, features))

if train_rl:
return selected_features, log_prob
return selected_features

if __name__ == &quot;__main__&quot;:

resnet = resnet50(weights = ResNet50_Weights.DEFAULT)
return_layers = {
&#39;layer1&#39;: &#39;0&#39;,
&#39;layer2&#39;: &#39;1&#39;,
&#39;layer3&#39;: &#39;2&#39;,
&#39;layer4&#39;: &#39;3&#39;
}
in_channels_list = [256, 512, 1024, 2048]
out_channels = 256
backbone_with_fpn = BackboneWithFPN(resnet, return_layers, in_channels_list, out_channels)

rl_agent = RLAgent(input_dim=1280, hidden_​​dim=512, action_space=4)

bayesian_fpn_rl = BayesianFPNWithRL(backbone_with_fpn, rl_agent).to(&#39;cuda&#39;)

optimizer = torch.optim.Adam(bayesian_fpn_rl.parameters(), lr=1e-4)

for epoch in range(10): 
for images, ground_truth_masks in dataloader:
images, ground_truth_masks = images.to(&#39;cuda&#39;), ground_truth_masks.to(&#39;cuda&#39;)

model_output, log_prob = bayesian_fpn_rl(images, train_rl=True)

predict_mask = (model_output &gt; 0.5).int()

reward = compute_reward(predicted_mask, ground_truth_masks)

loss = rl_loss(log_prob, reward)

optimizer.zero_grad()
loss.backward()
optimizer.step()

print(f&quot;Epoch [{epoch + 1}], Loss: {loss.item():.4f}, Reward: {reward:.4f}&quot;)

]]></description>
      <guid>https://stackoverflow.com/questions/79236267/mismatch-in-tensor-dimensions-in-bayesianfpn-with-reinforcement-learning</guid>
      <pubDate>Fri, 29 Nov 2024 08:06:22 GMT</pubDate>
    </item>
    <item>
      <title>“TypeError：类型为‘numpy.float32’的对象没有 len()” - DeepSORT 与 YOLO 集成</title>
      <link>https://stackoverflow.com/questions/79235328/typeerror-object-of-type-numpy-float32-has-no-len-deepsort-integration</link>
      <description><![CDATA[我正在将 YOLOv8 与 DeepSORT 集成以进行多对象跟踪，但在将检测数据传递给 DeepSORT update_tracks() 函数时遇到了 TypeError。
错误消息：

速度：4.5ms 预处理，332.2ms 推理，0.6ms 后处理每个形状为 (1, 3, 480, 640) 的图像 DeepSORT 检测：[[ 107.22
186.92 639.26 479.49 0.83611]] 回溯（最近一次调用）：文件 &quot;/home/roy/environments/001-opencv/004-opencv.py&quot;，
第 127 行，在 detect_customers() 文件
&quot;/home/roy/environments/001-opencv/004-opencv.py&quot;，
第 84 行，在 detect_customers tracks = tracker.update_tracks(deep_sort_detections,
frame=frame) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件
&quot;/home/roy/environments/001-opencv/lib/python3.12/site-packages/deep_sort_realtime/deepsort_tracker.py&quot;，
第 195 行，在 update_tracks 中断言 len(raw_detections[0][0])==4
^^^^^^^^^^^^^^^^^^^^^^^^^^^^ TypeError：类型为“numpy.float32”的对象
没有 len()

代码部分（检测和跟踪）：
def detect_customers():
# 初始化 YOLO 和 DeepSORT
model = YOLO(&quot;yolov8s.pt&quot;) # 加载 YOLO 模型
tracker = DeepSort(max_age=30, n_init=3)

cap = cv2.VideoCapture(0) # 将 0 替换为视频源
active_customers = {}

while True:
ret, frame = cap.read()
if not ret:
break

results = model(frame) # 执行 YOLO 推理
detections = []

for result in results:
for box in result.boxes:
# 提取边界框和置信度得分
x1, y1, x2, y2 = box.xyxy[0].tolist() # 将边界框转换为列表
confidence = float(box.conf[0]) # 置信度得分

# 以所需格式附加检测
detection = [float(x1), float(y1), float(x2), float(y2), float(confidence)]
detections.append(detection)

# 处理空检测
if len(detections) == 0:
deep_sort_detections = np.empty((0, 5)) # 空数组表示没有检测
else:
deep_sort_detections = np.array(detections, dtype=np.float32) # 转换为具有适当结构的 NumPy 数组

# 调试：打印传递给 DeepSORT 的检测
print(&quot;Detections for DeepSORT:&quot;, deep_sort_detections)

# 更新跟踪器
tracks = tracker.update_tracks(deep_sort_detections, frame=frame)
for track in tracks:
if not track.is_confirmed():
continue

track_id = track.track_id
ltrb = track.to_ltrb() # 转换为 (left, top, right, bottom)
cv2.rectangle(frame, (int(ltrb[0]), int(ltrb[1])), (int(ltrb[2]), int(ltrb[3])), (0, 255, 0), 2)
cv2.putText(frame, f&quot;ID: {track_id}&quot;, (int(ltrb[0]), int(ltrb[1]) - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# 显示框架
cv2.imshow(&quot;客户检测&quot;, frame)
if cv2.waitKey(1) &amp; 0xFF == 27：# 按 ESC 退出
break

cap.release()
cv2.destroyAllWindows()

问题摘要：

错误描述：将 deep_sort_detections 传递给 tracker.update_tracks() 时，我收到 TypeError：类型为 &#39;numpy.float32&#39; 的对象没有 len()。
检测格式：我将检测格式化为 [[x1, y1, x2, y2,
confidence], ...] 并将其转换为 NumPy 数组，其中 dtype=np.float32。但是，DeepSORT 似乎需要不同的格式，或者存在某种类型问题。

问题：

如何正确格式化检测数据以与 DeepSORT 的 update_tracks() 方法兼容？
我是否遗漏了数据的结构或传递给 DeepSORT 的方式？
]]></description>
      <guid>https://stackoverflow.com/questions/79235328/typeerror-object-of-type-numpy-float32-has-no-len-deepsort-integration</guid>
      <pubDate>Thu, 28 Nov 2024 21:25:10 GMT</pubDate>
    </item>
    <item>
      <title>我如何才能以某种方式融合嵌入以提高效率和分数？</title>
      <link>https://stackoverflow.com/questions/79233998/how-can-i-fuse-embeddings-in-a-manner-such-that-it-increase-efficiency-and-score</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79233998/how-can-i-fuse-embeddings-in-a-manner-such-that-it-increase-efficiency-and-score</guid>
      <pubDate>Thu, 28 Nov 2024 13:01:00 GMT</pubDate>
    </item>
    <item>
      <title>Python scorecardpy：UnboundLocalError：赋值前引用了局部变量“card_df”</title>
      <link>https://stackoverflow.com/questions/79219306/python-scorecardpy-unboundlocalerror-local-variable-card-df-referenced-befor</link>
      <description><![CDATA[我使用 scorecardpy 函数来获取模型：
import scorecardpy as ac
card=sc.scorecard(bins_adj, lr, X_train.columns)

然后我尝试使用以下代码保存此模型：
import numpy as np
np.save(&#39;card.npy&#39;,card)

之后我尝试重新加载此模型：
card=np.load(&#39;card.npy&#39;,allow_pickle=True)

然后我想使用该模型获取分数：
score=sc.scorecard_ply(data_train, card, print_step=0)

但它给出了错误：
UnboundLocalError Traceback（最近一次调用最后一次）
单元格在 [91]，第 1 行
score=sc.scorecard_ply(data_train, card, print_step=0)

文件 ~/.local/lib/python3.9/site-packages/scorecardpy/scorecard.py:330，在 scorecard_ply(dt, card, only_total_score, print_step, replace_blank_na, var_kp)
card_df=card.copy(deep=True)
# x 变量
xs=card_df.loc[card_df.variable != &#39;basepoints&#39;, &#39;variable&#39;].unique()
# x 变量的长度
xs_len=len(xs)

UnboundLocalError：局部变量“card_df”在赋值前被引用

如何解决此问题有问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79219306/python-scorecardpy-unboundlocalerror-local-variable-card-df-referenced-befor</guid>
      <pubDate>Sun, 24 Nov 2024 04:16:39 GMT</pubDate>
    </item>
    <item>
      <title>如何在微调期间正确设置 pad token（而不是 eos）以避免模型无法预测 EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么 AWS SageMaker 要运行 Web 服务器进行批量转换？</title>
      <link>https://stackoverflow.com/questions/58985124/why-does-aws-sagemaker-run-a-web-server-for-batch-transform</link>
      <description><![CDATA[我正在创建自己的 Docker 容器以用于 SageMaker，我想知道当我想要执行批量转换作业时，serve 命令为什么会创建一个 Flask 应用程序来提供数据预测。只需解开模型并在我想要预测的数据集上运行模型的预测方法，不是更简单吗？我不需要 Web API/端点。我只需要每天自动生成一次预测。]]></description>
      <guid>https://stackoverflow.com/questions/58985124/why-does-aws-sagemaker-run-a-web-server-for-batch-transform</guid>
      <pubDate>Thu, 21 Nov 2019 23:11:19 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 中的步骤和时期有什么区别？</title>
      <link>https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow</link>
      <description><![CDATA[在大多数模型中，都有一个 steps 参数，表示在数据上运行的步骤数。但我在大多数实际使用中看到，我们还会执行拟合函数 N epochs。
用 1 个 epoch 运行 1000 步和用 10 个 epoch 运行 100 步有什么区别？在实践中哪一个更好？连续 epoch 之间有任何逻辑变化吗？数据混洗？]]></description>
      <guid>https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow</guid>
      <pubDate>Tue, 12 Jul 2016 23:20:22 GMT</pubDate>
    </item>
    <item>
      <title>文本分类器</title>
      <link>https://stackoverflow.com/questions/15274781/text-categorization-classifiers</link>
      <description><![CDATA[有人知道好的开源文本分类模型吗？我知道斯坦福分类器、Weka、Mallet 等，但它们都需要训练。
我需要将新闻文章分类为体育/政治/健康/游戏/等。有没有预先训练过的模型？
Alchemy、OpenCalais 等不是选择。我需要开源工具（最好是 Java 语言的）。]]></description>
      <guid>https://stackoverflow.com/questions/15274781/text-categorization-classifiers</guid>
      <pubDate>Thu, 07 Mar 2013 15:16:36 GMT</pubDate>
    </item>
    <item>
      <title>grid.py 运行需要多长时间？</title>
      <link>https://stackoverflow.com/questions/2415557/how-much-time-does-grid-py-take-to-run</link>
      <description><![CDATA[我正在使用 libsvm 进行二分类。我想尝试 grid.py ，因为据说它可以改善结果。我在不同的终端中为五个文件运行了这个脚本，该脚本已经运行了 12 多个小时。
这是我的 5 个终端现在的状态：
[root@localhost tools]# python grid.py sarts_nonarts_feat.txt&gt;grid_arts.txt
警告：空 z 范围 [61.3997:61.3997]，正在调整到 [60.7857:62.0137]
第 2 行：警告：无法勾勒非网格数据的轮廓。请使用“set dgrid3d”。
警告：z 范围为空 [61.3997:61.3997]，正在调整至 [60.7857:62.0137]
第 4 行：警告：无法勾勒非网格数据的轮廓。请使用“set dgrid3d”。

[root@localhost tools]# python grid.py sgames_nongames_feat.txt&gt;grid_games.txt
警告：z 范围为空 [64.5867:64.5867]，正在调整至 [63.9408:65.2326]
第 2 行：警告：无法勾勒非网格数据的轮廓。请使用“set dgrid3d”。
警告：z 范围为空 [64.5867:64.5867]，正在调整至 [63.9408:65.2326]
第 4 行：警告：无法勾勒非网格数据的轮廓。请使用“set dgrid3d”。

[root@localhost tools]# python grid.py sref_nonref_feat.txt&gt;grid_ref.txt
警告：z 范围为空 [62.4602:62.4602]，正在调整至 [61.8356:63.0848]
第 2 行：警告：无法勾勒非网格数据的轮廓。请使用“set dgrid3d”。
警告：z 范围为空 [62.4602:62.4602]，正在调整至 [61.8356:63.0848]
第 4 行：警告：无法勾勒非网格数据的轮廓。请使用“set dgrid3d”。

[root@localhost tools]# python grid.py sbiz_nonbiz_feat.txt&gt;grid_biz.txt
警告：z 范围为空 [67.9762:67.9762]，正在调整至 [67.2964:68.656]
第 2 行：警告：无法勾勒非网格数据的轮廓。请使用“set dgrid3d”。
警告：z 范围为空 [67.9762:67.9762]，正在调整至 [67.2964:68.656]
第 4 行：警告：无法勾勒非网格数据的轮廓。请使用“set dgrid3d”。

[root@localhost tools]# python grid.py snews_nonnews_feat.txt&gt;grid_news.txt
第 494 行输入格式错误
回溯（最近一次调用）：
文件“grid.py”，第 223 行，运行中
如果 rate 为 None：引发“get no rate”
TypeError：异常必须是类或实例，而不是 str

我已将输出重定向到文件，但这些文件现在不包含任何内容。此外，还创建了以下文件：

sbiz_nonbiz_feat.txt.out
sbiz_nonbiz_feat.txt.png
sarts_nonarts_feat.txt.out
sarts_nonarts_feat.txt.png
sgames_nongames_feat.txt.out
sgames_nongames_feat.txt.png
sref_nonref_feat.txt.out
sref_nonref_feat.txt.png
snews_nonnews_feat.txt.out（--&gt; 为空）

.out 文件中只有一行信息。“.png”文件是一些 GNU PLOTS。
但我不明白上述 GNUplots/警告传达了什么。我应该重新运行它们吗？
如果每个输入文件包含大约 144000 行，这个脚本可能需要多长时间？]]></description>
      <guid>https://stackoverflow.com/questions/2415557/how-much-time-does-grid-py-take-to-run</guid>
      <pubDate>Wed, 10 Mar 2010 08:59:52 GMT</pubDate>
    </item>
    </channel>
</rss>