<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 03 Mar 2025 18:24:38 GMT</lastBuildDate>
    <item>
      <title>当批次尺寸= 1且批次尺寸=训练批次尺寸时，不同的测试结果</title>
      <link>https://stackoverflow.com/questions/79481464/different-test-results-when-batch-size-1-and-batch-size-training-batch-size</link>
      <description><![CDATA[我已经训练了一个UNET，该UNET在所有下样本和Uplample层中都使用BatchNorm2D。训练后，我加载了最好的检查站，并在测试数据上运行，并注意到在测试期间，如果将批次大小设置为1，则性能会严重降低。但是，如果我将其设置为与训练批量大小相同的大小（64），则性能更好。
为了进一步检查，我只需将1s的张量传递给我的型号，一次批次大小为1，另一个时间为64，两种情况下的输出都不同。这里发生了一些奇怪的事情，我不知道什么。

我有track_running_stats =在训练期间true 
我在这里尝试了解决方案： https://discuss.pytorch.org/t/performance-highly-degraded-when-when-when-eval-is-activated-in-the-test-phase/3323/67?page=3  

我没有运气尝试使用各种批次尺寸在经过训练的型号上获得相同的结果。对于上下文，两种情况之间的误差很大。]]></description>
      <guid>https://stackoverflow.com/questions/79481464/different-test-results-when-batch-size-1-and-batch-size-training-batch-size</guid>
      <pubDate>Mon, 03 Mar 2025 15:58:04 GMT</pubDate>
    </item>
    <item>
      <title>过度拟合客户流失数据集的火车数据[关闭]</title>
      <link>https://stackoverflow.com/questions/79481378/overfitting-on-train-data-for-a-customer-churn-dataset</link>
      <description><![CDATA[我从kaggle培训了客户流失数据集上的逻辑回归模型（ https://www.kaggle.com/datasets/muhammadshahidazeem/customer-churn-- ）。但是，我经历了一个严重的拟合案例：
火车准确性：0.94 
测试精度：0.57 
这表明我的模型概括不佳。
为避免数据泄漏，我实施了一条管道来处理分类功能并确保正确的预处理。此外，我使用了用分层的k折交叉验证的高参数调整来优化模型参数。
这是导入必要库后我的代码：
  train_data =&#39;customer-churn-dataset-training-master.csv&#39;
test_data =&#39;Customer-Churn-Dataset-testing-Master.csv&#39;
 
管道设置
  ordinal_encoded_features = [&#39;订阅类型&#39;，&#39;合同长度&#39;]
label_encoded_features = [&#39;性别&#39;]

ordinal_cat = [
    [&#39;BASIC&#39;，&#39;标准&#39;，&#39;premium&#39;]，
    [&#39;每月&#39;，&#39;季度&#39;，&#39;年度&#39;]
这是给出的

预处理器= columntransFormer（[[
    （&#39;onehot&#39;，onehotencoder（handle_unknown =&#39;nighore&#39;，drop =&#39;first&#39;），label_encoded_features），
    （&#39;ordinal&#39;，ordinalencoder（类别= ordinal_cat，handle_unknown =&#39;use_encoded_value&#39;，unknown_value = -1），， 
    ordinal_encoded_features）
]，剩余=&#39;PassThrough&#39;）

log_pipeline = {
    “逻辑回归”：管道（[[
        （“预处理程序”，预处理器），
        （&#39;sualer&#39;，minmaxscaler（）），
        （&#39;classifier&#39;，logisticRegress（Random_State = 42））
    ）））
}
 
用分层的k折调音超参数调谐
  param_grid = {
    “逻辑回归”：{
        &#39;Classifier__c&#39;：[0.1、1、5、10、15、20]，
        &#39;classifier__penalty&#39;：[&#39;l1&#39;，&#39;l2&#39;]，
        &#39;classifier__solver&#39;：[&#39;liblinear&#39;，&#39;saga&#39;]，
        &#39;classifier__class_weight&#39;：[none，&#39;balanced&#39;]
    }
}

kf = stratifiedkfold（n_splits = 5，shuffle = true，andury_state = 42）
log_model = {}
best_score = {}

对于名称，请在log_pipeline.items（）中使用管道：
    Random_Search = RandomizedSearchCV（PIPE，PARAM_GRID [name]，cv = kf，评分=&#39;f1&#39;，n_jobs = -1， 
                                       Random_State = 42，n_iter = 20）
    Random_search.fit（x_train，y_train）
    
    log_model [name] = rando_search.best_estimator_
    best_score [name] = Random_search.best_score_
 
我尝试的是：

 使用MinMaxScaler进行特征缩放。

 实施了分层的k折交叉验证。

 使用随机搜索范围调整了超参数。


尽管如此，测试准确性仍然明显低于训练准确性。
这个过度拟合问题的原因是什么？我可以做出任何改进以改善概括性能吗？]]></description>
      <guid>https://stackoverflow.com/questions/79481378/overfitting-on-train-data-for-a-customer-churn-dataset</guid>
      <pubDate>Mon, 03 Mar 2025 15:25:00 GMT</pubDate>
    </item>
    <item>
      <title>自动编码器学习问题 - 验证比培训更好吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79480958/auto-encoder-learning-issue-validation-better-than-training</link>
      <description><![CDATA[我正在开发一种自动编码器，该自动编码器将基于树的表达式编码为潜在向量并将其解码回原始形式。
我的数据集由像这样格式化的基因编程树组成：
  prog2（whileLoop（swapad），prog2（clri，inci））
prog2（prog2（prog2（clri，movbmaxiter）），而勒洛普（clri）），而勒洛普（nileLoop（swapad）））
而LileLoop（INCI）
 
我对我的自动编码器的学习结果有疑问，并想验证我的解释是否正确或我的培训是否有问题。
查看所附屏幕截图，我们可以看到两个图：

一个代表培训和验证集的损失曲线。
另一个显示了训练和验证集的精度曲线。

 问题： 
验证损失低于训练损失，这似乎是不寻常的。我的假设是：

由于我在模型的体系结构中使用辍学，这可以解释为什么验证似乎表现更好。在训练过程中，辍学会停用某些神经元，引入噪声和略有降解的性能。但是，在验证期间，辍学是禁用的，使模型的预测更加稳定，并给人以更高精度的印象。

 问题： 
这种解释是合理的，还是在我的培训过程中表明问题？
 训练/varrion/varion/val损失和准确性  
为了诊断问题，我尝试了以下内容：

降低了辍学率，以查看它是否影响了验证性能。
测试了不同的激活功能和批准化以稳定训练。
]]></description>
      <guid>https://stackoverflow.com/questions/79480958/auto-encoder-learning-issue-validation-better-than-training</guid>
      <pubDate>Mon, 03 Mar 2025 12:20:05 GMT</pubDate>
    </item>
    <item>
      <title>如何使用外部来源保持数据更新以供聊天机器人进行更新？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79480836/how-to-keep-data-updated-for-a-chatbot-using-external-sources</link>
      <description><![CDATA[我正在研究聊天机器人，我需要确保其使用的数据始终是最新的。问题是，我使用的数据可以随着时间而变化，因此我需要一种自动更新它的方法。
我正在考虑使用外部资源（例如API或数据集）来保持所有新鲜事物。最好的方法是什么？我应该寻找API，还是还有另一种方法，例如Web刮擦或其他方法来定期更新机器人数据？]]></description>
      <guid>https://stackoverflow.com/questions/79480836/how-to-keep-data-updated-for-a-chatbot-using-external-sources</guid>
      <pubDate>Mon, 03 Mar 2025 11:17:44 GMT</pubDate>
    </item>
    <item>
      <title>使用此条带明智的OCR（或其他分类模型）技术，是否可以使用草书写作角色分割？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79480579/is-cursive-writing-character-segmentation-even-possible-using-this-strip-wise-oc</link>
      <description><![CDATA[我需要从用户手写文本示例中提取字符实例。实际上，我试图创建一个数据库，说明用户如何从提交的文本样本中写出特定的字母。这是我对草书角色分割问题的尝试。
我在问题上的方法：

我摄取图像的小条，这些图像从图像的左端开始，并逐渐增加，直到整个图像被覆盖 
我在每个条带上运行OCR模型。
我应该能够看到“信心”中的尖峰。每当字符完全位于窗口内时模型。
因此，我应该具有新字符开始的位置的像素值。

这个想法在理论上似乎是可能的。
我发现A 类似的方法 之前在Stack Exchange中介绍了。
我运行了我的应该通过更大的改进的OCR模型或实际上专门针对数字识别训练的另一个模型来实质上改善。。
我认为它也可以在手写字符上工作，因为Trocr不像Tesseract不同，也应该用于手写。
当我运行它时
诸如Quic（Quick“ Quick”）之类的单词正在快速阅读。这意味着Trocr的语义引擎不会轻易通过中途结束的单词来说服。
如下所示，总体趋势表明句子中置信度得分的趋势增加。
 

可以通过分割每个单词，然后运行TROCR来解决第二个问题。这会失败。事实证明，特罗克（Trocr）在识别没有上下文的单词方面很糟糕。当我发送一片“狗”一片时，它开始返回“ diores”之类的单词。
我也对单个角色进行了trocr，它也失败了。
我的问题：

使用CNN或基于学习的角色分类模型可以更好地工作吗？如果是，我在哪里可以找到一个细分草书的数据集？
无论如何，是否有继续使用trocr，以至于它不在乎单词含义？
这种方法能否希望超越最近的分割技术，例如 this ？？
]]></description>
      <guid>https://stackoverflow.com/questions/79480579/is-cursive-writing-character-segmentation-even-possible-using-this-strip-wise-oc</guid>
      <pubDate>Mon, 03 Mar 2025 09:08:56 GMT</pubDate>
    </item>
    <item>
      <title>如何在机器学习模型中使用目标寻求功能</title>
      <link>https://stackoverflow.com/questions/79480354/how-to-use-goal-seek-functionality-in-machine-learning-model</link>
      <description><![CDATA[我正在使用XGBoost算法进行销售预测，并具有一些输入功能。我的预测月是1月25日，2月25日，3月25日。我想检查如果我有固定的目标值1月，2月，3月，输入值的任何两个特定列输入值更改。我正在获得输出，但输出为所有3个月的值相同。以下是我的代码。
  target_values = {
    &#39;2025-01-01&#39;：12000，
    &#39;2025-02-01&#39;：15000，
    &#39;2025-03-01&#39;：16000
}

base_data = pd.dataframe（index = pd.to_dateTime（[[&#39;2025-01-01&#39;，&#39;2025-02-01&#39;，&#39;2025-03-01&#39;]））））））））））
base_data [&#39;ds&#39;] = base_data.index

base_data [&#39;montry&#39;] = base_data [&#39;ds&#39;]。dt.month
base_data [&#39;day_of_year&#39;] = base_data [&#39;ds&#39;]。dt.dayofyear
base_data [&#39;sin_month&#39;] = np.sin（2 * np.pi * base_data [&#39;montry&#39;] / 12）
base_data [&#39;cos_month&#39;] = np.cos（2 * np.pi * base_data [&#39;montry&#39;] / 12）

如果test_data.index.duplicated（）。任何（）：
    打印（警告：test_data具有重复的索引值。重置索引。”
    test_data = test_data.reset_index（drop = true）

last_row = test_data.iloc [-1] .copy（）

static_cols = [&#39;a&#39;，&#39;b&#39;，&#39;c&#39;，&#39;d&#39;，&#39;e&#39;， 
               &#39;f&#39;，&#39;g&#39;，&#39;h&#39;]


对于static_cols中的col：
    base_data [col] = last_row [col] 

base_data [&#39;sb&#39;] = last_row [&#39;sb&#39;]
base_data [&#39;leads&#39;] = last_row [&#39;leads&#39;]

def predition_y（inputs，base_row，scaleer，型号）：
    行= base_row.copy（）
    行[&#39;sb&#39;] =输入[0]
    行[&#39;leds&#39;] =输入[1]
    x =行[[&#39;sb&#39;，&#39;a&#39;，&#39;b&#39;，&#39;sin_month&#39;，&#39;cos_month&#39;，&#39;c&#39;，&#39;d&#39;，&#39;e&#39;，e&#39;， 
             &#39;f&#39;，&#39;g&#39;，&#39;h&#39;， 
             &#39;leads&#39;]]。values.reshape（1，-1）
    X_SCALED = Scaleer.Transform（x）
    预测= model.predict（x_scaled）[0]
    print（for {base_row.name}的输入：sb = {inputs [0]：。2f}，leds = {inputs [1]：。2f}，预测y = {prediction y = {.2f}＆quort;）
    返回预测

def Objective_function（输入，目标，base_row，scaer，Model）：
    predicted_y = predive_y（输入，base_row，safleer，模型）
    print（{base_row.name}的目标：{target}，预测y：{prediction_y：.2f}，差异：{（prediction_y -target）** 2：.2f}
    返回（预测_y-目标）** 2

结果= {}
对于日期，target_values.items（）中的target：
    base_row = base_data.loc [date]
    onitire_guess = [base_row [&#39;sb&#39;]，base_row [&#39;leads&#39;]]
    
    打印（用目标{target};）
    print（f＆quot;初始猜测：sb = {initial_guess [0]：。2f}，leds = {initial_guess [1] :. 2f}＆quort;）
    
    结果=最小化（
        objective_function，
        initial_guess，
        args =（target，base_row，sualer，model13），
        方法=&#39;Powell&#39;， 
        bounds = [（0，无），（0，none）]，
        选项= {&#39;disp&#39;：true} 
    ）
    
    如果结果。
        结果[date] = {&#39;sb&#39;：result.x [0]，&#39;leads&#39;：result.x [1]}
    别的：
        结果[date] = {&#39;错误&#39;：&#39;优化失败&#39;}

对于日期，results.items（）中的vals：
    打印（f＆quot; \ nfor {date}：＆quot;）
    如果瓦尔中的“错误”：
        打印（val [&#39;错误&#39;]）
    别的：
        print（f＆quot; sb = {vals [&#39;sb&#39;]：。2f}，leds = {vals [&#39;leads&#39;] :. 2f}＆quot;）

print（f＆quot {date}}：＆quot;）
print（f＆quot;初始猜测：sb = {initial_guess [0]：。2f}，leds = {initial_guess [1] :. 2f}＆quort;）
打印（f＆quot;成功：{result.success}，最终值：sb = {result.x [0]：。2f}，leds = {result.x [1]：。2f}＆quort;）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79480354/how-to-use-goal-seek-functionality-in-machine-learning-model</guid>
      <pubDate>Mon, 03 Mar 2025 07:02:20 GMT</pubDate>
    </item>
    <item>
      <title>如何在保留OCR的手写数字/字符的同时删除虚线边界框</title>
      <link>https://stackoverflow.com/questions/79479925/how-to-remove-dotted-boundary-boxes-while-preserving-handwritten-digits-characte</link>
      <description><![CDATA[我正在使用包含手写数字和字母&#39;x&#39;的图像（表示应该将其视为“空”的图像），其中每个字符都写在一个虚线框中。我正在尝试删除角色周围的那些虚线盒，以供以后的OCR。另一个问题是，这些图像的质量不同，并且这些虚线经常合并为坚固的线条。我尝试仅隔离字符，这些字符经常与这些盒子重叠，但无处不在。
以下是IM与以下图像的示例：
        有点成功的管道是（使用Python和OpenCV）：

灰度
 otsu 
 rode＆amp;扩张
理想情况下，只有4个字符

但是，在某些情况下，笔迹非常微弱，在侵蚀期间也被侵蚀。我还尝试通过检测水平和垂直线来扩展图像和绘图盒，然后将其删除，但是由于很多角色要么与盒子重叠或超越它，因此这种方法也产生了可疑的结果。。
我不确定如何进行或如何处理；有任何建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79479925/how-to-remove-dotted-boundary-boxes-while-preserving-handwritten-digits-characte</guid>
      <pubDate>Sun, 02 Mar 2025 23:44:25 GMT</pubDate>
    </item>
    <item>
      <title>Autokeras需要在Python中重写Keras代码</title>
      <link>https://stackoverflow.com/questions/79479570/autokeras-need-to-rewrite-keras-code-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79479570/autokeras-need-to-rewrite-keras-code-in-python</guid>
      <pubDate>Sun, 02 Mar 2025 18:57:13 GMT</pubDate>
    </item>
    <item>
      <title>Tsonorflow模型的内存足迹如何在TFLITE转换后增加？</title>
      <link>https://stackoverflow.com/questions/79478810/how-come-memory-footprint-of-a-tensorflow-model-is-increasing-after-tflite-conve</link>
      <description><![CDATA[训练了一个简单的张量流模型，该模型包含一些LSTM和密集的进料层。训练后，我将模型量化并转换为 tf.lite 用于边缘部署的格式。这是代码的相关部分。
  ...
model_size：int = sum（weight.numpy（）。型号中的nbytes。
打印（f&#39;model大小：{model_size /（1024）:. 2f} kb&#39;）
tf_lite_converter = tf.lite.tfliteconverter.from_keras_model（型号=模型）
tf_lite_converter.optimizations = [tf.lite.optimize.optimize_for_size]]
tf_lite_converter.target_spec.supported_types = [tf.float16]
tflite_model：bytes = tf_lite_converter.convert（）
打印（tf Lite模型的f&#39;size是{len（tflite_model）/1024} kb&#39;）
 
作为 tf_lite 只是一个字节数组，我只是将其长度除以1024以获取内存中的大小。
原始型号大小：33.4 kb
压缩和定量后：55 kb 
那么，如果TF_Lite转换器增加内存中的大小，那么这可能会有什么可能甚至有益？或者，我是否错误地测量了（记忆中的）尺寸？任何线索如何进行公平的比较？
注意
我已经比较了磁盘中的尺寸（因为我坚​​持模型），是的，Tflite显示出明显的压缩益处。但这是要找到好处的地方吗？]]></description>
      <guid>https://stackoverflow.com/questions/79478810/how-come-memory-footprint-of-a-tensorflow-model-is-increasing-after-tflite-conve</guid>
      <pubDate>Sun, 02 Mar 2025 08:44:09 GMT</pubDate>
    </item>
    <item>
      <title>如何将“ data.irearner”模型转换为azure ml中的“ model.pkl”？</title>
      <link>https://stackoverflow.com/questions/79477586/how-to-convert-data-ilearner-model-to-model-pkl-in-azure-ml</link>
      <description><![CDATA[我在Azure ML中创建了一个管道，该管道使用提升决策树回归训练模型。从我的理解来看，该模型被保存为  data.Ilerner  。。
但是，我无法将此模型转换为  model.pkl  格式，可以使用 joblib 。加载。
 问题： 

如何在 azure ml 中创建 model.pkl 为提升决策树回归模型？
在

  我试图使用以下python脚本加载和转换模型：
 导入lightgbm作为lgb
导入约伯利布

＃加载LightGBM型号
model = lgb.booster（model_file =＆quot; data.ilerner; quot;）

＃另存为泡菜文件
Joblib.dump（Model，“ Model.pkl”） 
 
但是运行脚本时，我会收到以下错误：
 ％python3 convert_to_model_pkl.py 
[LightGBM] [致命]模型文件数据中未知模型格式或子模型类型
Trackback（最近的最新电话）：
  file＆quot＆quot＆quot＆tomasz.olchawa/ng/ml/convert_to_model_pkl.py&quot;，5，第5行，in＆lt; module＆gt;
    model = lgb.booster（model_file =＆quot; data.ilerner; quot;）
  file＆quot＆quot＆quot tomasz.olchawa/ng/ml/myenv/lib/python3.13/site-packages/lightgbm/basic.py&quot; line 3697，in __init__ in __init__
    _safe_call（
    ~~~~~~~~~~^
        _lib.lgbm_boostercreatefrommodelfile（
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^”
    ...＆lt; 3行＆gt; ...
        ）
        ^
    ）
    ^
  file＆quot＆quot /../ ng/ml/myenv/lib/python3.13/site-packages/lightgbm/basic.py，第313行，在_safe_call中
    提高lightgbmerror（_lib.lgbm_getlasterror（）。解码（&#39;utf-8＆quot））
lightgbm.basic.lightgbmerror：未知模型格式或模型文件中的子模型类型
 ]]></description>
      <guid>https://stackoverflow.com/questions/79477586/how-to-convert-data-ilearner-model-to-model-pkl-in-azure-ml</guid>
      <pubDate>Sat, 01 Mar 2025 12:45:07 GMT</pubDate>
    </item>
    <item>
      <title>管道Future Warning：此管道实例尚未拟合</title>
      <link>https://stackoverflow.com/questions/79475986/pipeline-futurewarning-this-pipeline-instance-is-not-fitted-yet</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79475986/pipeline-futurewarning-this-pipeline-instance-is-not-fitted-yet</guid>
      <pubDate>Fri, 28 Feb 2025 15:25:11 GMT</pubDate>
    </item>
    <item>
      <title>在序列编码中，whand_unknown = use_encoded_values做什么？</title>
      <link>https://stackoverflow.com/questions/79471646/in-ordinal-encoder-what-does-handle-unknown-use-encoded-values-do</link>
      <description><![CDATA[我已经完成了研究，但我对文档和双子座的答案不满意。  use_encoded_value 它是什么意思？我必须通过一个论点作为编码值吗？如果是这样，您可以举例说明它的用法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79471646/in-ordinal-encoder-what-does-handle-unknown-use-encoded-values-do</guid>
      <pubDate>Thu, 27 Feb 2025 05:09:19 GMT</pubDate>
    </item>
    <item>
      <title>BigQuery ML时间序列模型评估保持返回零</title>
      <link>https://stackoverflow.com/questions/79471261/bigquery-ml-time-series-model-evaluate-keeps-returning-null</link>
      <description><![CDATA[我正在使用BigQuery ML来训练ARIMA_PLUS模型，以预测CPU使用情况。该模型成功训练，但是当我运行ml。评估时，所有结果值均为null。
 模型训练查询 
 创建或替换模型`project.dataset.arima_model`
选项（
  model_type =&#39;arima_plus&#39;，
  time_series_timestamp_col =&#39;timestamp_column&#39;，
  time_series_id_col = [&#39;id_column_1&#39;，&#39;id_column_2&#39;]，
  time_series_data_col =&#39;data_column&#39;，
  forecast_limit_lower_bound = 0，
  forecast_limit_upper_bound = 100
） 作为
选择data_column，id_column_1，id_column_2，timestamp_column
来自`project.dataset.source_table`
在“ 2025-02-5”和&#39;2025-02-12&#39;之间的日期（timestamp_column）;
 
 评估查询 
 选择 * 
来自ml.evaluate（
  模型`project.dataset.arima_model`，
  （（
    选择data_column，id_column_1，id_column_2，timestamp_column
    来自`project.dataset.source_table`
    在“ 2025-02-13&#39;和&#39;2025-02-20&#39;之间的日期（timestamp_column）
  ），
  结构（
    true作为persim_gregation， 
    10作为地平线， 
    0.9作为信心_level
  ）
）；
 
 ml.Evaluate成功运行，但返回所有ID的null
查询结果 ]]></description>
      <guid>https://stackoverflow.com/questions/79471261/bigquery-ml-time-series-model-evaluate-keeps-returning-null</guid>
      <pubDate>Wed, 26 Feb 2025 23:29:23 GMT</pubDate>
    </item>
    <item>
      <title>预测促进问题：newmfinal必须为1 <newmfinal <mfinal</title>
      <link>https://stackoverflow.com/questions/78604530/predict-boosting-problem-newmfinal-must-be-1newmfinalmfinal</link>
      <description><![CDATA[我正在尝试使用 precadion.boosting（）  adabag  package使用adaboost算法进行预测，但是发生错误：

&#39;precade.boosting中的错误（adaboost，train01_new，newmfinal = 9）：
newmfinal必须为1＆lt; newmfinal＆lt; mfinal; 

这是脚本：
  install.packages（“ adabag; quot”）
图书馆（“ Adabag”）
adaboost＆lt;  -  boosting.cv（factor_new〜rfs+li+sdi+sdi+ldi+dr+dbt+dbt+dbt+fct+fii+ditp+adcg+adcg+addg+roa+roa+roi+roi+roe+roe，data = triar = triar = train = triar = train = triar = trie，boos = true，mfinal = 10，mfinal = 10，v = 5，par = 5，par = true = rp = rp = rp = rp = rp。
preditive_adaboost_cv_train＆lt;  -  predict.boosting（adaboost，train01_new，newmfinal = 9）
 
我使用newmfinal = 9，因为错误建议，但错误仍在这里。]]></description>
      <guid>https://stackoverflow.com/questions/78604530/predict-boosting-problem-newmfinal-must-be-1newmfinalmfinal</guid>
      <pubDate>Mon, 10 Jun 2024 21:09:02 GMT</pubDate>
    </item>
    <item>
      <title>使用开发设备或火车组</title>
      <link>https://stackoverflow.com/questions/45909024/using-the-dev-set-or-the-train-set</link>
      <description><![CDATA[有人可以清除我的疑问。
在评估模型时，我们应该尝试一个较小的集合。开发设置是一个小集。因此，我们在开发设置上尝试一些东西，然后得出结论，然后去火车训练并检查它。 
或
我们训练训练集并在开发设置上评估模型。将开发设置为基准。]]></description>
      <guid>https://stackoverflow.com/questions/45909024/using-the-dev-set-or-the-train-set</guid>
      <pubDate>Sun, 27 Aug 2017 20:24:55 GMT</pubDate>
    </item>
    </channel>
</rss>