<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 18 Dec 2023 15:14:12 GMT</lastBuildDate>
    <item>
      <title>处理 CNN 二元分类的分布外样本和异常</title>
      <link>https://stackoverflow.com/questions/77679785/handling-out-of-distribution-samples-and-anomalies-for-cnn-binary-classification</link>
      <description><![CDATA[我对机器学习领域比较陌生，并且对创建基本自定义模型有基本的了解。
分配给我的任务
我的经理要求我开发一种机器学习模型，能够在攻击性图像发送到服务器之前识别它们。这些图像可以分为不同的类别，例如武器或成人内容。
问题
但是，我在模型检测异常或分布外样本的能力方面遇到了问题。
我尝试过的
我尝试了不同的方法，利用&#39;binary_crossentropy&#39;作为我的损失函数，但遇到了同样的问题。我还构建了一个包含两个类的模型：

12,278 与武器相关的图片 class_name = 武器
3,000 张食物 商品图片 class_name = 其他

但是，我不确定 “其他” 类别中应包含哪些内容。使用 MobileNet 作为基础模型构建模型后，它成功地准确预测了武器，并提供了 0.4 到 0.6 范围内的食品预测，这似乎是可以接受的。然而，当我引入大象或汽车的图像时，模型倾向于将它们预测为武器，显示的置信度接近 1。
我不知道如何解决这个问题。我尝试研究这个问题，并发现了一些关于分布外检测的线索以及与异常或离群值相关的概念。当模型的输入包含不属于训练数据的图像时，如何获得不确定性值？
如果您能提供任何指导、建议，甚至参考能够有效解决此问题的视频或资源，我将不胜感激。感谢您的帮助。
数据加载：
train_ds, test_ds = keras.utils.image_dataset_from_directory(
    目录=“/内容/武器”，
    标签=“推断”，
    label_mode =“二进制”，
    批量大小=32，
    子集=“两者”，
    图像大小=(224,224),
    验证分割=0.2，
    随机播放=真，
    种子=1337
）

现在标准化输入图像数据：
def process（图像，标签）：
  图像=tf.cast(图像/255, tf.float32)
  返回图像、标签
train_ds = train_ds.map(进程)
test_ds = test_ds.map(进程)

创建 CNN 模型：
input_shape = (224,224,3)
mobilenet = MobileNet(input_shape,weights=&#39;imagenet&#39;,include_top=False)
模型=顺序（）
 
model.add（移动网络）
模型.add(压平())
model.add（密集（256，激活=&#39;relu&#39;））
模型.add(Dropout(0.5))
model.add（密集（1，激活=&#39;sigmoid&#39;））
模型.summary()



sgd = SGD(学习率=0.0001，动量=0.9，nesterov=True)
model.compile（损失=&#39;binary_crossentropy&#39;，优化器=sgd，指标=[&#39;准确性&#39;]）
历史= model.fit（train_ds，validation_data = test_ds，batch_size = 4，epochs = 6）


纪元 6/6
382/382 [==============================] - 58s 150ms/步 - 损失：0.0042 - 精度：0.9984 - val_loss ：0.0063 - val_accuracy：0.997
]]></description>
      <guid>https://stackoverflow.com/questions/77679785/handling-out-of-distribution-samples-and-anomalies-for-cnn-binary-classification</guid>
      <pubDate>Mon, 18 Dec 2023 14:34:38 GMT</pubDate>
    </item>
    <item>
      <title>将 .fasta 文件集合合并为 1 个 fasta 文件 [关闭]</title>
      <link>https://stackoverflow.com/questions/77679249/menggabungkan-kumpulan-file-fasta-menjadi-1-file-fasta</link>
      <description><![CDATA[在此处输入图像描述
我想将以下 .fasta 文件合并为 1 个数据文件。请帮助我将这组 .fasta 文件合并为 1 个文件。谢谢
它应该只是 1 个 .fasta 文件，其中包含已合并的数据内容。]]></description>
      <guid>https://stackoverflow.com/questions/77679249/menggabungkan-kumpulan-file-fasta-menjadi-1-file-fasta</guid>
      <pubDate>Mon, 18 Dec 2023 12:59:39 GMT</pubDate>
    </item>
    <item>
      <title>使用YOLOv5旧模型进行预测</title>
      <link>https://stackoverflow.com/questions/77678969/using-yolov5-old-models-to-make-a-prediction</link>
      <description><![CDATA[我目前正在尝试使用 .pt 文件作为 YOLOv5 模型，该模型是在 2 年前训练的，每当我尝试这样做时，我都会收到以下错误。
运行时错误：PytorchStreamReader 读取 zip 存档失败：找不到中心目录。

每次我尝试使用不同的 model.pt 文件时，它都会下载最新版本（例如：yolov5mu.pt）。
我在 VSCode 和 anaconda 提示符下运行类似的命令：
yolo 检测预测 model=yolov5/yolov5m.pt source=&#39;datasets\Testing_images\20210823_112536.jpg&#39;，在 VSCode 上。

yolo任务=检测模式=预测模型=ganuza.pt源=&#39;test1.jpg&#39;show=True，在anaconda提示符下。

我按照教程进行操作，文件夹组织看起来不错。如果我使用另一种模型（例如它下载的模型），它就可以正常工作。
有谁知道发生了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/77678969/using-yolov5-old-models-to-make-a-prediction</guid>
      <pubDate>Mon, 18 Dec 2023 12:06:31 GMT</pubDate>
    </item>
    <item>
      <title>将问题标记为简单、中等和困难 [关闭]</title>
      <link>https://stackoverflow.com/questions/77678858/labelling-questions-as-easy-medium-and-hard</link>
      <description><![CDATA[我目前正在从事一个机器学习项目，专注于开发问题标签模型。 挑战在于确定如何将问题分类为“简单”、“中等”或“困难”。更具体地说，我正在寻求有关哪些指标可以准确确定问题难度级别的指导简单、中等或困难。如果有人有机器学习经验，我们将不胜感激您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/77678858/labelling-questions-as-easy-medium-and-hard</guid>
      <pubDate>Mon, 18 Dec 2023 11:44:50 GMT</pubDate>
    </item>
    <item>
      <title>将变量从一个模块导入到另一个模块</title>
      <link>https://stackoverflow.com/questions/77678535/importing-variables-from-one-module-to-other</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77678535/importing-variables-from-one-module-to-other</guid>
      <pubDate>Mon, 18 Dec 2023 10:44:18 GMT</pubDate>
    </item>
    <item>
      <title>模型评估中端到端 MLops 项目出现错误</title>
      <link>https://stackoverflow.com/questions/77678457/error-on-end-to-end-mlops-project-in-model-evaluation</link>
      <description><![CDATA[我正在遵循端到端 MLOPS 数据科学项目实施与部署中显示的流程 在 Krish Naik 的频道上。
我在 05_model_evaluation 上运行此模型评估管道单元.ipynb
&lt;前&gt;&lt;代码&gt;尝试：
    配置=配置管理器()
    model_evaluation_config = config.get_model_evaluation_config()
    model_evaluation_config = ModelEvaluation(config=model_evaluation_config)
    model_evaluation_config.log_into_mlflow()
除了异常 e：
    提高e

我收到此错误：
FileNotFoundError: [Errno 2] 没有这样的文件或目录: &#39;config\\config.yaml&#39;

这是回溯：
FileNotFoundError Traceback（最近一次调用最后一次）
c:\Users\HP\Desktop\livesitter demo\MLops\research\05_model_evaluation.ipynb 单元格 11 第 7 行
      5 model_evaluation_config.log_into_mlflow()
      6 除了异常 e：
----&gt; 7 提高 e

c:\Users\HP\Desktop\livesitter demo\MLops\research\05_model_evaluation.ipynb 单元 11 第 2 行
      1 次尝试：
----&gt; 2 配置=配置管理器（）
      3 model_evaluation_config = config.get_model_evaluation_config()
      4 model_evaluation_config = ModelEvaluation(config=model_evaluation_config)

c:\Users\HP\Desktop\livesitter demo\MLops\research\05_model_evaluation.ipynb 单元 11 第 8 行
      2 def __init__（
      3 自我,
      4 config_filepath = CONFIG_FILE_PATH,
      5 params_filepath = PARAMS_FILE_PATH,
      6 schema_filepath = SCHEMA_FILE_PATH):
----&gt; 8 self.config = read_yaml(config_filepath)
      9 self.params = read_yaml(params_filepath)
     10 self.schema = read_yaml(schema_filepath)

文件 c:\Users\HP\AppData\Local\Programs\Python\Python311\Lib\site-packages\ensure\main.py:849，在 WrappedFunctionReturn.__call__(self, *args, **kwargs)
    第841章
...
---&gt; 29 将 open(path_to_yaml) 作为 yaml_file：
     30 内容 = yaml.safe_load(yaml_file)
     31 logger.info(f&quot;yaml 文件: {path_to_yaml} 加载成功&quot;)

FileNotFoundError：[Errno 2]没有这样的文件或目录：&#39;config\\config.yaml&#39;
输出被截断。作为可滚动元素查看或在文本编辑器中打开。调整单元格输出设置...
]]></description>
      <guid>https://stackoverflow.com/questions/77678457/error-on-end-to-end-mlops-project-in-model-evaluation</guid>
      <pubDate>Mon, 18 Dec 2023 10:29:06 GMT</pubDate>
    </item>
    <item>
      <title>LLava AI 在本地主机上运行 - 说 {"detail":"Not Found"}</title>
      <link>https://stackoverflow.com/questions/77678285/llava-ai-running-on-localhost-says-detailnot-found</link>
      <description><![CDATA[我克隆了这个项目https://github.com/haotian-liu/LLaVA ，并尝试让它运行。
但是，我在本地主机上收到的唯一消息是此窗口显示 {“detail”：“Not Found”}

我已正确安装最新的 CUDA 驱动程序（GPU 可与 PrivateGPT 配合使用），并确保我已在 conda 中激活 llava 环境。
这里是分别运行的控制器、Gradio Web 服务器和模型：
控制器：

广播：

型号：

什么可能导致此问题？]]></description>
      <guid>https://stackoverflow.com/questions/77678285/llava-ai-running-on-localhost-says-detailnot-found</guid>
      <pubDate>Mon, 18 Dec 2023 10:00:59 GMT</pubDate>
    </item>
    <item>
      <title>分割模型推理延迟问题</title>
      <link>https://stackoverflow.com/questions/77678168/segmentation-model-inference-latency-issue</link>
      <description><![CDATA[我使用了 pyannote 的开源分割模型和 Diart diarization 的说话者二值化存储库，使用 diart==0.5.1，
在 diart/blocks/segmentation.py 中，我进行了以下更改::
 与 torch.no_grad()：
        wave=rearrange(self.formatter.cast(waveform),“批量采样通道-&gt;批量通道采样”)
        # 波火炬.Tensor (1, 1, 80000)
        打印（wave.get_device（））
        开始 = 时间.time()
        输出 = self.model(wave.to(self.device)).cpu()
        停止=时间.time()
        print(&#39;分段时间:&#39;)
        打印（停止-开始）
        # 输出：torch.Tensor (1, 293, 3)
    返回 self.formatter.restore_type(输出)

在输出中，seg timeL 0.4s
但是如果我尝试在 diart 存储库之外进行推断（在独立的存储库中）：
defegmentation_model(self,batch:np.ndarray) -&gt;; np.ndarray：
    块 = torch.tensor(batch)
    print(chunks.get_device()) # -1

    使用 torch.no_grad()：
        尝试：
            打印（块.形状）
            输出 = self.model(chunks.to(self.device)).cpu()
        除了 RuntimeError 作为例外：
            如果 is_oom_error（异常）：
                引发内存错误（
                    f&quot;batch_size ({self.batch_size: d}) 可能太大。 ”
                    f“尝试使用较小的值，直到内存错误消失。”
                ）
            别的：
                引发异常

    返回输出.numpy()

此处分段时间：10s
资源、输入格式、形状、类型一切都是相同的
为什么延迟不同？
期望延迟相同]]></description>
      <guid>https://stackoverflow.com/questions/77678168/segmentation-model-inference-latency-issue</guid>
      <pubDate>Mon, 18 Dec 2023 09:38:17 GMT</pubDate>
    </item>
    <item>
      <title>运行 fmri 深度学习模型时出现错误</title>
      <link>https://stackoverflow.com/questions/77677389/getting-error-while-running-fmri-deep-learning-model</link>
      <description><![CDATA[我正在处理 fmri 数据，我有两组数据集疾病和正常数据的形状是正常数据形状：(91, 109, 91, 1200)
疾病数据形状：(91, 109, 91, 210)
我写了python脚本
将 numpy 导入为 np
将 nibabel 导入为 nib
从 sklearn.model_selection 导入 train_test_split


# 定义正常和疾病数据文件夹的路径
疾病数据路径 = glob(&#39;/media/aish/Backup Plus1/ABIDE/scan_data001/**/**/**/**/**/**/**/**/swa*&#39;)
Normal_data_path = glob(&#39;/media/aish/rs2/hcp/s*&#39;)

# 加载正常和疾病数据
正常数据 = []
疾病数据 = []
对于normal_data_path[0:400]中的文件：
    Normal_data.append(nib.load(file).get_data())
对于疾病数据路径中的文件：
    疾病数据.append(nib.load(文件).get_data())

print(“正常数据形状：”, normal_data[1].shape)
print(“疾病数据形状：”,疾病数据[1].shape)

# 创建标签
标签 = np.concatenate((np.zeros(len(normal_data)), np.ones(len(disease_data))))

# 将数据分为训练集和测试集
X_train，X_test，y_train，y_test = train_test_split（np.array（正常数据+疾病数据），标签，test_size = 0.2，random_state = 42）

但是我收到错误
完整错误是
ValueError Traceback（最近一次调用最后一次）
[7]，第 32 行中的单元格
     29 个标签 = np.concatenate((np.zeros(len(normal_data)), np.ones(len(disease_data))))
     31 # 将数据分为训练集和测试集
---&gt; 32 X_train，X_test，y_train，y_test = train_test_split（np.array（正常数据+疾病数据），标签，test_size = 0.2，random_state = 42）
     34 # 定义CNN模型
     35 模型 = 顺序()

ValueError：无法将输入数组从形状 (91,109,91,1200) 广播到形状 (91,109,91)

我无法解决此错误]]></description>
      <guid>https://stackoverflow.com/questions/77677389/getting-error-while-running-fmri-deep-learning-model</guid>
      <pubDate>Mon, 18 Dec 2023 06:53:58 GMT</pubDate>
    </item>
    <item>
      <title>如何避免非终端深度 Q 学习中每个动作的无限变化</title>
      <link>https://stackoverflow.com/questions/77676680/how-to-avoid-infinite-changes-per-action-in-non-terminal-deep-q-learning</link>
      <description><![CDATA[据我所知，梯度下降的 Deep Q 学习遵循以下过程：

初始化随机权重和偏差

从起始状态开始执行操作

确定奖励

通过梯度下降根据当前时间步长更改权重和偏差

通过梯度下降根据之前的时间步长来更改权重和偏差，但使用奖励 * 折扣因子 ^ 步数而不仅仅是奖励。

重复第 2 步


在无限的时间段内，这应该导致每一步都会导致权重和偏差发生变化，目标是梯度下降的当前奖励+预期未来回报*折扣因子，与贝尔曼匹配方程。然而，根据这种方法，在每个步骤中，我们需要进行与包含该步骤及其之前的每个步骤相同的更改量。在深度 Q 学习的非终结情况下，（据我所知）这应该会导致无限量的所需处理时间。
在我目前的案例中，我正在尝试在恐龙游戏上运行深度 Q 学习，并且假设恐龙可能永远不会死亡，因此可能会导致上述问题。
当折扣因子^步骤低于某个阈值时，潜在的解决方案可能只是简单地舍入为0，或者在某个点任意终止情节并重新开始，但这两种解决方案都没有似乎不完全正确。
Atari DQN 研究论文
在 Atari 论文中，他们似乎使用有限的内存大小来保存所需的所有状态，然后仅随机选择一个状态来执行梯度下降。这是正确的解释吗？这可能是我面临的问题的解决方案吗？还有其他可能的解决方案吗？
编辑：
看来我们的梯度下降标签是立即奖励+未来回报，但我们不是通过继续玩这一集来寻找未来回报，而是使用当前的 Q 函数估计来寻找未来返回。这似乎仍然有点违反直觉，因为我们部分地使用自己的函数作为梯度下降的目标，但即时奖励的知识似乎使函数收敛于解决方案。
来源：
https://youtu.be/rFwQDDbYTm4?t=1394]]></description>
      <guid>https://stackoverflow.com/questions/77676680/how-to-avoid-infinite-changes-per-action-in-non-terminal-deep-q-learning</guid>
      <pubDate>Mon, 18 Dec 2023 02:35:47 GMT</pubDate>
    </item>
    <item>
      <title>在电脑上训练我的模型，然后在微控制器上使用它[关闭]</title>
      <link>https://stackoverflow.com/questions/77675723/train-my-model-on-pc-then-use-it-on-microcontroller</link>
      <description><![CDATA[如果我想在我的 PC 上训练一个模型（无论是 ML、NN 还是 CNN），因为我有强大的 GPU，是否可以在 Arduino 或 Raspberry Pi Pico 等微控制器上导出或保存这个训练模型以直接使用它？或者我需要从头开始重新训练这些模型？]]></description>
      <guid>https://stackoverflow.com/questions/77675723/train-my-model-on-pc-then-use-it-on-microcontroller</guid>
      <pubDate>Sun, 17 Dec 2023 19:33:23 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 中的数据采样方法</title>
      <link>https://stackoverflow.com/questions/77578111/data-sample-methods-in-lightgbm</link>
      <description><![CDATA[我的问题
我不太清楚所有参数的用法以及它们如何相互交互（或应该使用）。
我所知道的
据我了解，LightGBM中有3种算法：

GBDT，默认的，使用 boosting
DART 是一种带有 dropout 的 boosting 算法
随机森林，不使用增强（确实如此，但仅在一次迭代中）

并且有两种数据采样策略：

Bagging，用于集成学习。这是默认值，但关联参数的值（例如 bagging_freq）会使 bagging 停用。
GOSS 选择更多对误差梯度贡献最大的数据（我们的想法是，我们需要对远离基线的数据进行更多训练），而对“弱”数据进行更少的训练。数据点（对误差梯度贡献较小的数据点）。

问题
所以我的问题如下：

为什么 Bagging 和 GOSS 不兼容？它们似乎不会影响同一件事。
LightGBM 的主要创新似乎是 GOSS，但它并不是默认选择，这样做的动机是什么？
最后，我们能够将 boosting_type=goss 作为参数传递。当我们这样做时会发生什么？算法会是GBDT，而数据样本策略是goss吗？

非常感谢您抽出时间。
祝你有美好的一天。]]></description>
      <guid>https://stackoverflow.com/questions/77578111/data-sample-methods-in-lightgbm</guid>
      <pubDate>Thu, 30 Nov 2023 11:34:21 GMT</pubDate>
    </item>
    <item>
      <title>LLM模型非常慢</title>
      <link>https://stackoverflow.com/questions/77199972/llm-model-is-very-slow</link>
      <description><![CDATA[我正在 nvidia g5 上运行 34b LLM 模型。 8xlarge 实例（1 个 Nvidia A10G GPU、24GB GPU RAM、32 个 vCPU、128GB RAM）
这是推理代码
从变压器导入 AutoTokenizer,LlamaForCausalLM, AutoConfig, AutoModelForCausalLM
从加速导入 infer_auto_device_map, init_empty_weights
进口火炬

model_path = “Phind/Phind-CodeLlama-34B-v2”

model = LlamaForCausalLM.from_pretrained(model_path, device_map=“自动”, offload_folder=“卸载”, torch_dtype=torch.float16, offload_state_dict = True)
tokenizer = AutoTokenizer.from_pretrained(model_path)

defgenerate_one_completion（提示：str）：
    tokenizer.pad_token = tokenizer.eos_token
    输入=分词器（提示，return_tensors =“pt”，截断= True，max_length = 4096）

    ＃ 产生
    generate_ids = model.generate(inputs.input_ids.to(“cuda”), max_new_tokens=384, do_sample=True, top_p=0​​.75, top_k=10, 温度=0.1)
    完成= tokenizer.batch_decode（generate_ids，skip_special_tokens = True，clean_up_tokenization_spaces = False）[0]
    完成 = 完成.replace(prompt, &quot;&quot;).split(&quot;\n\n\n&quot;)[0]

    返回完成

text = “你好，请问是你吗？”
打印（生成_一个_完成（文本））


加载检查点分片 - 这需要 4 分钟。如何才能加快速度？
即使是简单的推理也需要 60 秒以上。代码填写/提示需要 10 多分钟。可以在此 ec2 实例上加速吗？
]]></description>
      <guid>https://stackoverflow.com/questions/77199972/llm-model-is-very-slow</guid>
      <pubDate>Fri, 29 Sep 2023 06:54:14 GMT</pubDate>
    </item>
    <item>
      <title>如何使用GAN生成拉曼光谱的合成数据样本？</title>
      <link>https://stackoverflow.com/questions/76906588/how-to-generate-synthetic-data-samples-of-raman-spectroscopy-by-using-gan</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76906588/how-to-generate-synthetic-data-samples-of-raman-spectroscopy-by-using-gan</guid>
      <pubDate>Tue, 15 Aug 2023 14:05:29 GMT</pubDate>
    </item>
    <item>
      <title>Perzeptron 算法 - 代码错误 - Python 3 [重复]</title>
      <link>https://stackoverflow.com/questions/51912598/perzeptron-algorithm-code-error-python-3</link>
      <description><![CDATA[我正在阅读德语书籍“Machine Learning with Python”作者：塞巴斯蒂安·拉什卡。
我在 Windows 机器上使用 anaconda 和spyder（包括 ipython 控制台）。
在第 3 章中，他依赖于基于“Perzeptron 模型”的算法。
按照作者的指示，代码应如下所示：
从sklearn.metrics导入accuracy_score
print(&#39;Korrektklassifizierungsrate: %.2f&#39; % precision_score(y_test, y_pred))

从 matplotlib.colors 导入 ListedColormap
defplot_decision_region(X, y, 分类器, 分辨率=0.02):
    
    # Markierungen 和 Farben einstellen
    标记 = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;)
    颜色 = (&#39;红色&#39;, &#39;蓝色&#39;, &#39;浅绿色&#39;, &#39;灰色&#39;, &#39;青色&#39;)
    cmap = ListedColormap(颜色[:len(np.unique(y))])
    
    #Plotten der Entscheidungsgrenze
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, \
         分辨率），np.arange（x2_min，x2_max，分辨率））
    Z = classifier.predict(np.array([xx1.ravel(), \
                                     xx2.ravel()]).T)
    
    Z = Z.reshape(xx1.shape)
                  plt.contourf(xx1,xx2,Z,alpha=0.4,cmap=cmap),
                  plt.xlim(xx1.min(), xx1.max())
                  plt.ylim(xx2.min(), xx2.max())
    
    #Plotten aller 示例
    对于 idx，枚举中的 cl(np.unique(y))：
        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],
                    α=0.8，c=cmap(idx)，
                    标记=标记[idx]，标签=cl)
    
    #Examplare Testdatenmenge hervorheben
    如果测试idx：
        X_test, y_test = X[test_idx, :], y[test_idx]
        plt.scatter(X_test[:, 0], X_test[:, 1], c=&#39;&#39;,
                    alpha=1.0，线宽=1，标记=&#39;o&#39;s=55，标签=&#39;测试集&#39;）
        
X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X=X_combined_std,
                      y=y_组合，
                      分类器=ppn,
                      test_idx=范围(105,150))
plt.xlabel(&#39;Länge des Blütenblatts [standardisiert]&#39;)
plt.ylabel(&#39;Breite des Blütenblatts [standardisiert]&#39;)
plt.legend(loc=&#39;左上&#39;)
plt.show()

 文件“”，第 19 行
    plt.contourf(xx1,xx2,Z,alpha=0.4,cmap=cmap),
    ^
IndentationError：意外缩进

所以，我不确定默认值是什么。我真的很想理解这个错误，如果有人能帮助我，我将不胜感激。难道，它与方程 cmap=cmap 有关吗？]]></description>
      <guid>https://stackoverflow.com/questions/51912598/perzeptron-algorithm-code-error-python-3</guid>
      <pubDate>Sat, 18 Aug 2018 21:13:42 GMT</pubDate>
    </item>
    </channel>
</rss>