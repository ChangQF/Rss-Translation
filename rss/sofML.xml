<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 19 Oct 2024 12:30:48 GMT</lastBuildDate>
    <item>
      <title>在 Colab 中微调 Llama 3.1 时的上下文长度限制</title>
      <link>https://stackoverflow.com/questions/79104305/context-length-limitation-when-fine-tuning-llama-3-1-in-colab</link>
      <description><![CDATA[我正在通过 Unsloth 库，使用带有自定义数据集（使用 LoRA 技术）的 A100 GPU 在 Google Colab Pro 中对 Llama 3.1 模型进行微调。下面是我正在使用的 LoRA 代码：
max_seq_length = 2048
model = FastLanguageModel.get_peft_model(
model,
r=16, # 选择任意数字 &gt; 0 ！建议 8、16、32、64、128
target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
&quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down​​_proj&quot;],
lora_alpha=16,
lora_dropout=0, # 支持任意，但 = 0 是经过优化的
bias=&quot;none&quot;, # 支持任意，但 = &quot;none&quot; 是经过优化的

use_gradient_checkpointing=&quot;unsloth&quot;, # 对于非常长的上下文，为 True 或 &quot;unsloth&quot;
random_state=3407,
use_rslora=False, # 我们支持等级稳定的 LoRA
loftq_config=None, # 和 LoftQ
)
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
train_dataset=dataset,
dataset_text_field=&quot;text&quot;,
max_seq_length=max_seq_length,
dataset_num_proc=2,
packing=False, # 可以使短序列的训练速度提高 5 倍。
args=TrainingArguments(
per_device_train_batch_size=2,
gradient_accumulation_steps=4,
warmup_steps=5,
# num_train_epochs = 1, # 将其设置为 1 次完整的训练运行。
max_steps=60,
learning_rate=2e-4,
fp16=not is_bfloat16_supported(),
bf16=is_bfloat16_supported(),
logs_steps=1,
optim=&quot;adamw_8bit&quot;,
weight_decay=0.01,
lr_scheduler_type=&quot;linear&quot;,
seed=3407,
output_dir=&quot;outputs&quot;,
),
)

加载模型时，我们必须指定最大序列长度，这会限制其上下文窗口。 Llama 3.1 支持高达 128k 的上下文长度，但在本例中我将其设置为 2048，因为它消耗更多的计算和 VRAM。此外，dtype 参数会自动检测您的 GPU 是否支持 BF16 格式，以便在训练期间获得更高的稳定性（此功能仅限于 Ampere 和较新的 GPU）。
我的问题：

如果我在训练时将 max_seq_length 设置为 2048，那么训练后我的模型的上下文长度是多少，128k 还是 2048？
训练模型后，我们可以使用 128k 的上下文长度吗，还是仍然限制为 2048？
]]></description>
      <guid>https://stackoverflow.com/questions/79104305/context-length-limitation-when-fine-tuning-llama-3-1-in-colab</guid>
      <pubDate>Sat, 19 Oct 2024 05:59:26 GMT</pubDate>
    </item>
    <item>
      <title>numpy nd 数组连接用于创建数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/79104179/numpy-nd-array-concatenation-for-dataset-creation</link>
      <description><![CDATA[我正在处理一个数据集，其中训练数据具有此形状（33104,6,128,1），标签具有此形状（33104,1），我该如何连接标签以适合其最后一列？
我该如何连接标签以适合其最后一列？]]></description>
      <guid>https://stackoverflow.com/questions/79104179/numpy-nd-array-concatenation-for-dataset-creation</guid>
      <pubDate>Sat, 19 Oct 2024 04:01:10 GMT</pubDate>
    </item>
    <item>
      <title>训练模型预测不一致</title>
      <link>https://stackoverflow.com/questions/79104144/trained-model-prediction-are-not-consistent</link>
      <description><![CDATA[我用一个小型数据集训练了一个 VGG16 (224x224x3) 迁移学习图像分类模型，该数据集包含来自 3 位艺术家的画作。该数据集有大约 80 幅画作用于训练，25 幅用于测试。图像上有增强。图像被标记为真（属于艺术家）或假（不属于艺术家），比例约为 50-50。训练补丁总数（224x224x3）约为 20000。该模型旨在判断一幅画是否属于艺术家。这是训练损失和准确度图：

该图表明该模型在测试数据集上表现良好。然后将壁灯等图像（模型从未见过这些图像）输入模型，并询问此壁灯图像是否属于艺术家。显然答案是否定的。但最初有几次模型确实给出了“否”的答案，但现在它始终给出“是”的答案，这显然是错误的。我对训练模型性能的经验有限。什么会导致训练模型的预测不一致？训练数据集太小还是其他原因？]]></description>
      <guid>https://stackoverflow.com/questions/79104144/trained-model-prediction-are-not-consistent</guid>
      <pubDate>Sat, 19 Oct 2024 03:19:59 GMT</pubDate>
    </item>
    <item>
      <title>我们如何知道Ranker（Weka）要设置哪些参数？[关闭]</title>
      <link>https://stackoverflow.com/questions/79103647/how-do-we-know-what-parameters-for-ranker-weka-to-set</link>
      <description><![CDATA[我正在使用 Weka。所以我有几个问题无法通过手册弄清楚：

我们如何知道 Ranker 应该设置哪些参数？我的意思是，阈值和 numToSelect。对此有什么解释吗？
当我通过资源管理器选择属性并保存修改后的数据集时，它始终是 N+1 属性（N 个选定的属性 + 类/标签）。为什么？标签/类不也是属性吗？
]]></description>
      <guid>https://stackoverflow.com/questions/79103647/how-do-we-know-what-parameters-for-ranker-weka-to-set</guid>
      <pubDate>Fri, 18 Oct 2024 20:54:50 GMT</pubDate>
    </item>
    <item>
      <title>高效的 Net V2 M ONNX 模型在小输入上的推断速度明显较慢 [关闭]</title>
      <link>https://stackoverflow.com/questions/79103592/efficient-net-v2-m-onnx-model-infers-significantly-slower-on-small-input</link>
      <description><![CDATA[当我将 Efficient net v2 m 模型从 Pytorch 转换为不同大小输入的 Onnx 时，我注意到一种奇怪且无法解释的行为。
在我的 RTX 4090 上，1280X1280 大小图像上的 ONNX 模型在 35 毫秒内推断出批处理大小为 1。当我将图像大小缩小到大约 192X192（批处理大小相同为 1）时，运行时间几乎保持不变。这是可以理解的，因为固定开销占主导地位，例如初始化时间、线程池预热、与 GPU 之间的低效数据传输，最重要的是，计算库针对 GPU 上的矢量化和 SIMD 指令进行了优化。
然而，令人困惑的是，一旦我开始将输入图像大小减小到 192X192 以下，运行时间就会急剧增加。对于 64X64 图像，批处理大小为 1 时运行时间为 &gt;100ms。我完全理解为什么在较小的图像上推理不应该更快，但我不明白为什么它会更慢（而且慢得多）。
当我增加较小图像的批处理大小时，每批的运行时间会大幅改善（不仅仅是每幅图像的运行时间）。对于批处理大小为 16 的图像，推理 192X192 图像每批需要 25 毫秒（每幅图像不到 2 毫秒），而批处理大小为 1 时则需要 &gt;100ms。同样，我对此没有任何解释。固定开销和优化的 SIMD 矢量化将决定每幅图像的摊销运行时间应该随着批处理大小的增加而改善。但是，我观察到整个批次的运行时间也得到了改善。
对于较大的图像（例如 1280X1280），增加批次大小会增加每个批次的运行时间（尽管是亚线性的，这是完全可以预料的 - 随着批次大小的增加，每个图像的运行时间仍然会缩短到一定限度，之后，对于几乎无法放入 GPU 内存的更高批次大小，每个图像的运行时间也会增加约 10%）。
但是在 CPU 上运行时，处理时间会随着输入大小的增加而单调增加，正如预期的那样。
当我要求它对所有输入进行处理时，我已经验证了 ONNX 模型在 GPU 上成功运行。事实上，对于小输入，CPU 推理时间比 GPU 更快（这是可以理解的，因为有固定的 I/O 和其他开销）
注意：由于我在整个实验过程中将动态轴设置为 None，因此我为具有不同输入大小的同一 torch 模型保存了多个版本的 ONNX 模型。使用或不使用 onnx-sim 几乎不会对运行时间产生影响（处理速度差异小于 10-15%）。我在 C++ 中以 OrtCUDAProviderOptions 作为执行提供程序运行 onnx 模型，使用或不使用 GraphOptimizationLevel 几乎没有区别。
神经网络的输出对于所有输入都符合预期，因此我不希望我的代码中出现任何错误。
TL;DR
我的 ONNX 模型对于中等大小图像的运行速度比 GPU 上的小图像更快。对于较小的图像，增加批次大小会大幅减少每批次的处理时间（而不仅仅是 SIMD 并行化所预期的每张图像的摊销时间）。]]></description>
      <guid>https://stackoverflow.com/questions/79103592/efficient-net-v2-m-onnx-model-infers-significantly-slower-on-small-input</guid>
      <pubDate>Fri, 18 Oct 2024 20:32:42 GMT</pubDate>
    </item>
    <item>
      <title>决策树，Knn 算法 [关闭]</title>
      <link>https://stackoverflow.com/questions/79103501/decision-tree-knn-algorithms</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79103501/decision-tree-knn-algorithms</guid>
      <pubDate>Fri, 18 Oct 2024 19:59:23 GMT</pubDate>
    </item>
    <item>
      <title>我应该在正弦位置编码中交错正弦和余弦吗？</title>
      <link>https://stackoverflow.com/questions/79103455/should-i-interleave-sin-and-cosine-in-sinusoidal-positional-encoding</link>
      <description><![CDATA[我正在尝试实现正弦位置编码。我发现了两个给出不同编码的解决方案。我想知道其中一个是错误的还是两个都是正确的。我展示了两种选项的编码结果的视觉图。
class SinusoidalPosEmb(nn.Module):
def __init__(self, dim):
super().__init__()
self.dim = dim
def forward(self, x):
device = x.device
half_dim = self.dim // 2
emb = math.log(10000) / (half_dim - 1)
emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
emb = x[:, None] * emb[None, :]
emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
return emb


2)
class TransformerPositionalEmbedding(nn.Module):
&quot;&quot;&quot;
摘自论文《Attention Is All You Need》第 3.5 节
&quot;&quot;&quot;
def __init__(self, dimension, max_timesteps=1000):
super(TransformerPositionalEmbedding, self).__init__()
assert dimension % 2 == 0, &quot;Embedding 维度必须是偶数&quot;
self.dimension = dimension
self.pe_matrix = torch.zeros(max_timesteps, dimension)
# 收集嵌入向量中的所有偶数维度
even_indices = torch.arange(0, self.dimension, 2)
# 使用对数变换计算项以加快计算速度
# (https://stackoverflow.com/questions/17891595/pow-vs-exp-performance)
log_term = torch.log(torch.tensor(10000.0)) / self.dimension
div_term = torch.exp(even_indices * -log_term)
# 根据奇数/偶数时间步长预先计算位置编码矩阵
timesteps = torch.arange(max_timesteps).unsqueeze(1)
self.pe_matrix[:, 0::2] = torch.sin(timesteps * div_term)
self.pe_matrix[:, 1::2] = torch.cos(timesteps * div_term)
def forward(self, timestep):
# [bs, d_model]
return self.pe_matrix[timestep]

]]></description>
      <guid>https://stackoverflow.com/questions/79103455/should-i-interleave-sin-and-cosine-in-sinusoidal-positional-encoding</guid>
      <pubDate>Fri, 18 Oct 2024 19:35:08 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 3D UNet 在进行二元分割时总是预测黑色？</title>
      <link>https://stackoverflow.com/questions/79103203/why-does-my-3d-unet-always-predict-black-when-doing-binary-segmentation</link>
      <description><![CDATA[我的输入图像是灰度图像（uint8），目标图像是二值图像（bool）。我将输入图像转换为 0~1 并对其进行归一化，然后将其输入到 3D UNet 中。
这是我用于 3D UNet 模型的 代码。
这是我用于训练的代码：
unet = UNet3D(in_channels=1, num_classes=1).to(device)

bce_loss = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(unet.parameters(), lr=0.01, motivation=0.9, weight_decay=0.0005)

for epoch in range(args.epochs):
unet.train()
train_loss = 0.0
for step, (x, y_true, _) in enumerate(train_loader): 
x, y_true = x.to(device=device, dtype=torch.float), y_true.to(device=device, dtype=torch.float)
optimizer.zero_grad()
y_pred = unet(x) # y_pred - (batch_size,1,depth,width,height)

loss = bce_loss(y_pred, y_true) # y_true - (batch_size,1,depth,width,height)
train_loss += loss.item()
loss.backward()
optimizer.step()

print(&quot;Epoch %d, 平均训练交叉熵损失 %9.6f&quot; % (epoch + 1, train_loss / len(train_loader)))

我的二元交叉熵损失没有收敛到一个较小的数字。预测的最大值不能超过 0.5，所以我无法预测掩码。
我尝试使用不同的学习率，将损失函数改为 bce 损失 + dice 损失，但仍然不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/79103203/why-does-my-3d-unet-always-predict-black-when-doing-binary-segmentation</guid>
      <pubDate>Fri, 18 Oct 2024 18:00:36 GMT</pubDate>
    </item>
    <item>
      <title>如何估计 CoreML 模型的参数数量？</title>
      <link>https://stackoverflow.com/questions/79103126/how-to-esitmate-the-number-of-parameters-for-coreml-models</link>
      <description><![CDATA[我正在比较修剪对 CoreML 模型的影响。
虽然我可以轻松测量文件大小的变化（以 kB 为单位），但我很难估计模型参数数量的变化，因为 CoreML 没有提供像 PyTorch 的 model.parameters() 这样的直接方法。我如何估计或计算修剪后的 CoreML 模型中的参数数量？]]></description>
      <guid>https://stackoverflow.com/questions/79103126/how-to-esitmate-the-number-of-parameters-for-coreml-models</guid>
      <pubDate>Fri, 18 Oct 2024 17:35:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 RAG 和 FastAPI 的 WebApp [关闭]</title>
      <link>https://stackoverflow.com/questions/79099986/webapp-using-rag-and-fastapi</link>
      <description><![CDATA[我正在开展一个项目，需要实现一个检索增强生成 (RAG) 后端系统，该系统可以分析和比较两个提供的调查结果数据集。目标是创建一个具有 Python FastAPI 后端和 ReactJS 前端的 Web 应用程序。该应用程序应允许用户：
探索两个数据集。
交叉比较数据集。
从用户查询中生成 AI 驱动的见解。
后端应根据用户查询从数据集中检索相关数据，并将检索到的数据传递给文本生成模型以生成上下文响应。
这两个数据集都是 Excel 格式，但我已将它们转换为 CSV，甚至尝试使用 SQL 和 JSON 来处理数据。数据集复杂且嵌套，标题在行和列中重复，使数据看起来像矩阵，这使处理变得复杂。
最大的问题似乎是数据集本身的复杂性。它包含带有行和列标题的嵌套信息，因此很难提取有意义的文本。检索和生成的组合并没有产生相关的见解。GPT-2 倾向于重复自己或提供通用的、非信息性的响应，例如：

圣诞节是我们所有人庆祝和庆祝的时刻。这是我们所有人庆祝的时刻。这是一个庆祝的时刻……

我如何处理具有重复行和列标题的复杂数据集，并将其转换为 RAG 管道可用的格式？
任何帮助或指导都将不胜感激！
GitHub 存储库：https://github.com/drrahulsuresh/bounce/tree/dev]]></description>
      <guid>https://stackoverflow.com/questions/79099986/webapp-using-rag-and-fastapi</guid>
      <pubDate>Thu, 17 Oct 2024 22:12:24 GMT</pubDate>
    </item>
    <item>
      <title>如何将我的数据集不重复地分成测试和训练？</title>
      <link>https://stackoverflow.com/questions/79096421/how-to-split-my-dataset-into-test-and-train-without-repitition</link>
      <description><![CDATA[我正在开发一个 Python 脚本来测试一个算法。我有一个数据集，需要将其分成 80% 用于训练，20% 用于测试。但是，我想保存测试集以供进一步分析，确保与之前的测试集不重叠。
虽然我的代码总体运行良好，但我遇到了一个问题：由于随机选择过程，测试数据集有时包含之前测试运行中已经选择的记录。
在流程结束时，所有 100% 的记录都应在其中一次运行中进行测试
举个例子说明：

在第一次运行中，我的数据集 {0,1,2,3,4,5,6,7,8,9 被拆分为训练集 {0,1,2,4,5,7,8,9 和测试集 {3,6。
在第二次运行中，训练集为 {0,1,2,3,4,5,7,9，测试集为{6,8。

如您所见，记录 {6 被选中两次进行测试，我想避免这种情况。
我如何修改代码以确保每次随机选择 20% 的测试集，但排除任何之前选择的记录？
这是当前代码：
df = pd.read_csv(&quot;CustomersInfo.csv&quot;)
y = df[&#39;CustomerRank&#39;]
X = df.drop(&#39;CustomerRank&#39;, axis=1, errors=&#39;ignore&#39;)

#----------------------------------------------------------------------------------
#这是需要修复的部分
for RandStat in [11, 22, 33, 44, 55]:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RandStat)
#-------------------------------------------------------------------

clf = XGBClassifier(random_state=RandStat)
clf.fit(X_train, y_train)
fnStoreAnalyse(y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/79096421/how-to-split-my-dataset-into-test-and-train-without-repitition</guid>
      <pubDate>Thu, 17 Oct 2024 03:50:34 GMT</pubDate>
    </item>
    <item>
      <title>将 ONNX 模型从版本 9 升级到 11</title>
      <link>https://stackoverflow.com/questions/69899046/upgrade-onnx-model-from-version-9-to-11</link>
      <description><![CDATA[我正在使用 ONNX 模型，需要对其进行量化以减小其大小，为此，我遵循官方文档中的说明：
import onnx
from onnxruntime.quantization import quantize_dynamic, QuantType

model_fp32 = &#39;path/to/the/model.onnx&#39;
model_quant = &#39;path/to/the/model.quant.onnx&#39;
quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8)

但是当我运行它时，我收到以下警告：
警告：root：原始模型 opset 版本为 9，不支持量化。请将模型更新为 opset &gt;= 11。自动将模型更新为 opset 11。请验证量化模型。

我测试了量化模型，但没有成功，它生成此错误：
 INVALID_GRAPH：从 model_a2_quant.onnx 加载模型失败：这是一个无效的模型。 Node:Upsample__477 中的错误：为 Upsample 注册的 Op 在 domain_version 11 中已弃用

此时我有什么替代方案可以量化模型？
我从这个 repo 中获得了张量流中的原始模型：https://github.com/ciber-lab/pictor-ppe
并使用此代码将其转换为 ONNX：
# 输入和输出
input_tensor = 输入（shape=(input_shape[0], input_shape[1], 3) ) # 输入
num_out_filters = ( num_anchors//3 ) * ( 5 + num_classes ) # 输出

## 构建并加载模型
model = yolo_body(input_tensor, num_out_filters)

weight_path = &#39;ONNX_demo/models/pictor-ppe-v302-a1-yolo-v3-weights.h5&#39;

model.load_weights( weight_path )

tf.saved_model.save(model, &quot;ONNX_demo/models/save_model&quot;)

# 将其转换为 ONNX 格式：

python3 -m tf2onnx.convert --saved-model &quot;ONNX_demo/models/save_model&quot; --output &quot;ONNX_demo/models/model.onnx&quot;
]]></description>
      <guid>https://stackoverflow.com/questions/69899046/upgrade-onnx-model-from-version-9-to-11</guid>
      <pubDate>Tue, 09 Nov 2021 13:31:13 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以获得包含世界上几乎所有国家护照的护照图像数据集？</title>
      <link>https://stackoverflow.com/questions/60039938/where-can-i-get-passport-images-dataset-that-contain-passport-of-almost-all-coun</link>
      <description><![CDATA[我正在训练一个 OCR 模型，用于从护照中识别 MRZ。为了训练我的模型以获得更高的准确性，我需要用尽可能多的图片来训练它。我试图在 KAGGLE 上找到护照的数据集，但找不到。
有人能告诉我从哪里可以获得包含几乎所有国家或北美和南美护照的护照图像数据集吗？
非常感谢您的帮助。
祝好，
Asma]]></description>
      <guid>https://stackoverflow.com/questions/60039938/where-can-i-get-passport-images-dataset-that-contain-passport-of-almost-all-coun</guid>
      <pubDate>Mon, 03 Feb 2020 13:11:08 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 生存模型预测</title>
      <link>https://stackoverflow.com/questions/53521427/xgboost-prediction-for-survival-model</link>
      <description><![CDATA[Xgboost 的文档暗示，使用 Cox PH 损失训练的模型的输出将是个人预测乘数的指数（相对于基线风险）。有没有办法从这个模型中提取基线风险，以预测每个人的整个生存曲线？

survival:cox：右删失生存时间数据的 Cox 回归
（负值被视为右删失）。请注意，预测
是在风险比尺度上返回的（即，比例风险函数 h(t) = h0(t) * HR 中的 HR = exp(marginal_prediction)）
]]></description>
      <guid>https://stackoverflow.com/questions/53521427/xgboost-prediction-for-survival-model</guid>
      <pubDate>Wed, 28 Nov 2018 14:13:15 GMT</pubDate>
    </item>
    <item>
      <title>sample.int(m, k) 中的错误：无法抽取大于总体的样本</title>
      <link>https://stackoverflow.com/questions/25745215/error-in-sample-intm-k-cannot-take-a-sample-larger-than-the-population</link>
      <description><![CDATA[首先，我要说的是，我对机器学习、kmeans 和 r 还很陌生，这个项目是一种学习更多这方面知识的方法，也是将这些数据呈现给我们的 CIO 的一种方式，这样我就可以在开发新的帮助台系统时使用它。
我有一个 60K 行的文本文件。该文件包含教师在 3 年期间输入的帮助台工单的标题。
我想创建一个 r 程序，获取这些标题并创建一组类别。例如，与打印问题相关的术语，或与投影仪灯泡相关的一组术语。我使用 r 打开文本文档，清理数据，删除停用词和其他我认为不必要的词。我已获得频率 &gt;= 400 的所有术语列表，并将其保存到文本文件中。
但现在我想将 kmeans 聚类（如果可以完成或合适）应用于同一数据集，看看我是否可以提出类别。
下面的代码包括将写出使用的术语列表 &gt;= 400 的代码。它位于末尾，并被注释掉。
library(tm) #加载文本挖掘库
library(SnowballC)
options(max.print=5.5E5) 
setwd(&#39;c:/temp/&#39;) #将 R 的工作目录设置为靠近我的文件的位置
ae.corpus&lt;-Corpus(DirSource(&quot;c:/temp/&quot;),readerControl=list(reader=readPlain))
summary(ae.corpus) #检查发生了什么在
ae.corpus &lt;- tm_map(ae.corpus, tolower)
ae.corpus &lt;- tm_map(ae.corpus, removePunctuation)
ae.corpus &lt;- tm_map(ae.corpus, removeNumbers)
ae.corpus &lt;- tm_map(ae.corpus, stemDocument, language = &quot;english&quot;) 
myStopwords &lt;- c(stopwords(&#39;english&#39;), &lt;a very long list of other words&gt;)
ae.corpus &lt;- tm_map(ae.corpus, removeWords, myStopwords) 

ae.corpus &lt;- tm_map(ae.corpus, PlainTextDocument)

ae.tdm &lt;- DocumentTermMatrix(ae.corpus, control = list(minWordLength = 5))

dtm.weight &lt;- weightTfIdf(ae.tdm)

m &lt;- as.matrix(dtm.weight)
rownames(m) &lt;- 1:nrow(m)

#euclidian 
norm_eucl &lt;- function(m) {
m/apply(m,1,function(x) sum(x^2)^.5)
}
m_norm &lt;- norm_eucl(m)

results &lt;- kmeans(m_norm,25)

#list clusters

clusters &lt;- 1:25
for (i in clusters){
cat(&quot;Cluster &quot;,i,&quot;:&quot;,findFreqTerms(dtm.weight[results$cluster==i],400,&quot;\n\n&quot;))
}

#inspect(ae.tdm)
#fft &lt;- findFreqTerms(ae.tdm, lowfreq=400)

#write(fft, file = &quot;dataTitles.txt&quot;,
# ncolumns = 1,
# append = FALSE, sep = &quot; &quot;)

#str(fft)

#inspect(fft)

当我使用 RStudio 运行此程序时，我得到：
&gt;结果 &lt;- kmeans(m_norm,25)


sample.int(m, k) 中的错误：当“replace = FALSE”时无法获取大于总体的样本

我不太清楚这是什么意思，而且我在网上没有找到很多关于这方面的信息。有什么想法吗？
TIA]]></description>
      <guid>https://stackoverflow.com/questions/25745215/error-in-sample-intm-k-cannot-take-a-sample-larger-than-the-population</guid>
      <pubDate>Tue, 09 Sep 2014 12:55:28 GMT</pubDate>
    </item>
    </channel>
</rss>