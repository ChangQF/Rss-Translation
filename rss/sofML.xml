<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 03 May 2024 06:19:33 GMT</lastBuildDate>
    <item>
      <title>如何将数据从 CFF 资源管理器导出到 csv 或 xlsx 文件？</title>
      <link>https://stackoverflow.com/questions/78422974/how-do-i-export-data-from-cff-explorer-to-a-csv-or-xlsx-file</link>
      <description><![CDATA[我正在尝试从 CFF 资源管理器获取数据进行一些分析。如何导出数据？
我尝试查看不同的东西，但没有可用的来源。是否有可用的扩展或者我必须为此专门编写代码？如果是的话怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/78422974/how-do-i-export-data-from-cff-explorer-to-a-csv-or-xlsx-file</guid>
      <pubDate>Fri, 03 May 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>哪种机器学习算法最适合此类数据？</title>
      <link>https://stackoverflow.com/questions/78422849/which-machine-learning-algorithm-will-be-best-for-this-type-of-data</link>
      <description><![CDATA[数据集配对图
这是我的数据的配对图，其中 Qmax、0.2max、0.1Qmax 和 3Qmin 构成特征集，Qmin 是要预测的值。
查看绘图和数据集，我可以说它是非线性的，如果我尝试构建预测模型，简单的回归将不起作用。因此，我需要关于应该尝试哪些算法来获得此类数据的最佳模型的建议。
我尝试使用多项式回归、RidgeCV 和 LassoCV，其中 LassoCV 给出了最好的结果，但误差仍然存在，而且明显很大。]]></description>
      <guid>https://stackoverflow.com/questions/78422849/which-machine-learning-algorithm-will-be-best-for-this-type-of-data</guid>
      <pubDate>Fri, 03 May 2024 05:33:41 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助切换到新的工作角色[关闭]</title>
      <link>https://stackoverflow.com/questions/78422693/need-help-in-switching-to-a-new-job-role</link>
      <description><![CDATA[我担任数据分析师已经两年了，我正在考虑换工作。我想到了三个选择：数据工程、机器学习和 DevOps。您能告诉我哪一款需求量大且报酬丰厚吗？
我无法弄清楚哪一个适合我。大家可以推荐一下吗？]]></description>
      <guid>https://stackoverflow.com/questions/78422693/need-help-in-switching-to-a-new-job-role</guid>
      <pubDate>Fri, 03 May 2024 04:35:06 GMT</pubDate>
    </item>
    <item>
      <title>VGG 上的迁移学习能否更好地与 Across the Spider-Verse 风格保持一致，以改进风格迁移？</title>
      <link>https://stackoverflow.com/questions/78422590/can-transfer-learning-on-vgg-better-align-with-across-the-spider-verse-styles-fo</link>
      <description><![CDATA[我们的项目正在致力于使用 Across the Spider-verse 中的样式来实现样式迁移。到目前为止，我们已经获得了一些不错的输出，但我们想知道是否可以更好地将 VGG 模型与我们所依赖的宇宙中的样式保持一致，以获得更好的输出。
这个想法是通过在 VGG 上使用迁移学习来识别电影中的不同宇宙，我们可以调整 VGG 以注意到每个不同宇宙中使用的一些特征（例如更好地识别格温的画笔笔触或迈尔斯的半色调） .
我们对此进行了实验，最终得到了一个似乎难以识别内容的 VGG 模型 - 当我们运行整个模型时，我们的图像已删除所有细节（例如极其模糊或块状）。
这让我们想知道这种实现是否可行。似乎没有任何其他论文使用除 VGG 之外的任何内容（1).]]></description>
      <guid>https://stackoverflow.com/questions/78422590/can-transfer-learning-on-vgg-better-align-with-across-the-spider-verse-styles-fo</guid>
      <pubDate>Fri, 03 May 2024 03:43:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python、Keras 和 TensorFlow 进行 Epoch 数据训练 [关闭]</title>
      <link>https://stackoverflow.com/questions/78422261/epoch-data-training-using-python-and-keras-and-tensorflow</link>
      <description><![CDATA[我在尝试训练数据时遇到这样的错误
我希望得到一个解决方案，以便我的纪元训练可以恢复处理图像
还有一条消息出现在 Epoch 1/10
C:\Users\LISA\AppData\Roaming\Python\Python311\site-packages\keras\src\trainers\data_adapters\py_dataset_adapter.py:121: UserWarning: 你的 `PyDataset` 类应该调用 `super().__init__(* *kwargs)` 在其构造函数中。 `**kwargs` 可以包括 `workers`、`use_multiprocessing`、`max_queue_size`。不要将这些参数传递给“fit()”，因为它们将被忽略。
  self._warn_if_super_not_used()
737/737 ────────────────────────────── 0s 3s/步 - 准确度：0.9137 - 损失：0.3580
Epoch 1：val_accuracy 从 -inf 提高到 0.90875，将模型保存到 vgg16_uas_classsification.keras
737/737 ────────────────────────────── 2388s 3s/步 - 准确度：0.9137 - 损失：0.3580 - val_accuracy：0.9087 - val_loss：0.3 562
纪元 2/10
c:\Program Files\Python311\Lib\contextlib.py:158: UserWarning: 您的输入数据不足；中断训练。确保您的数据集或生成器可以生成至少“steps_per_epoch * epochs”批次。构建数据集时，您可能需要使用“.repeat()”函数。
  self.gen.throw（类型，值，回溯）
]]></description>
      <guid>https://stackoverflow.com/questions/78422261/epoch-data-training-using-python-and-keras-and-tensorflow</guid>
      <pubDate>Fri, 03 May 2024 00:57:09 GMT</pubDate>
    </item>
    <item>
      <title>机器学习部署和测试的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78422251/issues-with-machine-learning-deployment-and-testing</link>
      <description><![CDATA[我目前正在构建一个机器学习模型，并将其与使用 Python Flask 的网站集成进行部署。我已经成功地训练了模型，并在训练过程中将数据处理成特征和特定数据类型。但是，我不确定如何在预测期间处理实时用户输入数据。有人可以指导我如何在使用 Flask 部署期间将用户输入转换为模型所需的功能和数据类型吗？任何见解或代码示例将不胜感激。
谢谢！”
这就是我正在尝试做的事情。使用xgb_model
fromflask导入Flask、request、jsonify、render_template
将 pandas 导入为 pd
将 numpy 导入为 np
进口泡菜
从类别_编码器导入目标编码器
导入操作系统

base_dir = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(base_dir, &#39;fraudpredict.pickle.pkl&#39;)

# 加载模型
model = pickle.load(open(&#39;fraudpredict.pickle&#39;, &#39;rb&#39;))

应用程序 = Flask(__name__)


def 预处理(df):
    df = df.replace(&#39;?&#39;, np.NaN)
    df[&#39;collision_type&#39;].fillna(df[&#39;collision_type&#39;].mode()[0], inplace=True)
    df[&#39;property_damage&#39;].fillna(&#39;NO&#39;, inplace=True)
    df[&#39;police_report_available&#39;].fillna(&#39;NO&#39;, inplace=True)
    df[&#39;authorities_contacted&#39;].fillna(&#39;NO&#39;, inplace=True)
    df = df.drop(
        columns=[&#39;_c39&#39;、&#39;policy_number&#39;、&#39;policy_bind_date&#39;、&#39;incident_date&#39;、&#39;incident_location&#39;、&#39;auto_model&#39;])
    df[&#39;incident_month&#39;] = pd.to_datetime(df[&#39;incident_date&#39;], error=&#39;coerce&#39;).dt.month
    df[&#39;incident_day&#39;] = pd.to_datetime(df[&#39;incident_date&#39;], error=&#39;coerce&#39;).dt.day
    df[&#39;fraud_reported&#39;] = df[&#39;fraud_reported&#39;].replace((&#39;Y&#39;, &#39;N&#39;), (0, 1))

    # 对分类变量进行编码
    编码器=目标编码器()
    categorical_features = [&#39;auto_make&#39;, &#39;police_report_available&#39;, &#39;property_damage&#39;,
                            &#39;incident_city&#39;、&#39;incident_state&#39;、&#39;authorities_contacted&#39;、&#39;incident_severity&#39;、
                            &#39;collision_type&#39;, &#39;incident_type&#39;, &#39;insured_relationship&#39;, &#39;insured_hobbies&#39;,
                            &#39;受保职业&#39;、&#39;受保教育级别&#39;、&#39;受保性别&#39;、&#39;保单_csl&#39;、
                            &#39;政策状态&#39;]
    对于 categorical_features 中的特征：
        df[特征] = 编码器.fit_transform(df[特征], df[&#39;fraud_reported&#39;])

    返回df


@application.route(&#39;/&#39;)
定义索引（）：
    # 渲染输入表单
    返回 render_template(&#39;index.html&#39;)


@application.route(&#39;/predict&#39;,methods=[&#39;POST&#39;])
def 预测（）：
    data = request.form.to_dict() # 从表单中收集数据
    df = pd.DataFrame([数据])
    df_processed = 预处理(df)
    预测 = model.predict(df_processed)
    如果预测[0] == 1，则结果 =“检测到欺诈”，否则“未检测到欺诈”
    返回 render_template(&#39;index.html&#39;, 结果=结果)


如果 __name__ == &#39;__main__&#39;:
    应用程序.运行（调试= True）
]]></description>
      <guid>https://stackoverflow.com/questions/78422251/issues-with-machine-learning-deployment-and-testing</guid>
      <pubDate>Fri, 03 May 2024 00:51:23 GMT</pubDate>
    </item>
    <item>
      <title>自监督模型收敛到一个常数</title>
      <link>https://stackoverflow.com/questions/78421910/self-supervised-model-converging-to-a-constant</link>
      <description><![CDATA[我试图训练 Barlow 双胞胎模型进行图像分类。尽管如此，在完成模型训练后，我遇到了一个问题。似乎模型已成为一个常数，无论给出的两幅图像有多么不同，它总是返回数字 2046，小数部分略有变化。
该模型试图将互相关矩阵最小化为单位矩阵。
有没有办法解决这个问题。
def off_diagonal(x):
# 返回方阵非对角线元素的扁平视图

class BarlowTwins(nn.Module):
def __init__(self, lambd ,batch_size):
super().__init__()
self.batch_size = batch_size
self.lambd = lambd
self.backbone = torchvision.models.resnet34(zero_init_residual=True, weights=&#39;DEFAULT&#39;)
self.backbone.fc = nn.Identity()
self.sizes = [512,2048,2048,2048]

# 投影仪
_sizes = [512,4096,4096,4096]

layer = []
for i in range(len(self.sizes) - 2):
layers.append(nn.Linear(self.sizes[i], self.sizes[i + 1], bias=False))
layers.append(nn.BatchNorm1d(self.sizes[i + 1]))
layers.append(nn.ReLU(inplace=True))
layers.append(nn.Linear(self.sizes[-2], self.sizes[-1], bias=False))
self.projector = nn.Sequential(*layers)

# 表示 z1 和 z2 的规范化层
self.bn = nn.BatchNorm1d(self.sizes[-1], affine=False)

def forward(self, y1, y2):
z1 = self.projector(self.backbone(y1))
z2 = self.projector(self.backbone(y2))

# 经验互相关矩阵
c = self.bn(z1).T @ self.bn(z2)

# 对所有 gpu 之间的互相关矩阵求和
c.div_(self.batch_size)

on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()
# print(&#39;c&#39;, c)
val = torch.diagonal(c).sum()

off_diag = off_diagonal(c).pow_(2).sum()
# print(&#39;off_diag&#39;, off_diag)

loss = on_diag + self.lambd * off_diag 
return loss, val

from tqdm import tqdm

def intiate_p(model, epoch_n, loader, print_freq, lr, motivation, weight_decay):
epoch_tqdm = tqdm(range(epoch_n))
param_weights = []
param_biases = []
r = print_freq
func = model

optimizer = optim.SGD(model.parameters(), lr=lr, motivation=momentum, weight_decay=weight_decay)

scheduler = optim.lr_scheduler.PolynomialLR(optimizer, total_iters=40, power=2.0)

loss = []

# target = torch.tensor(2048, dtype=torch.float32)
start_time = time.time()
stats_file = open(&#39;stats.txt&#39;, &#39;a&#39;, buffering=1)

for epoch in epoch_tqdm:

for step, ((y1, y2), _) in enumerate(loader, start=epoch * len(loader)):

optimizer.zero_grad()
loss = func.forward(y1, y2)[0]
loss.append(loss)
loss.backward()
optimizer.step()
scheduler.step()
epoch_tqdm.set_description(f&quot;the Loss is: {abs(loss -2048)} &quot; )

return loss

这里的问题是，我不确定我对模型的评估方法是否正确。因为我随机将两幅图像作为 y1 和 y2 输入到我的模型中进行 100 次迭代，但结果保持不变。
旁注：我尝试了许多不同的训练变量值，我能得到的最佳损失是 100。
md = BarlowTwins(batch_size=64, lambd=0.005)
t = intiate_p(model=md, epoch_n=20, loader=loader,lr=0.4,momentum=0.3 ,print_freq=10, weight_decay=0.0001)
# 损失收敛到大约 100，而它从大约 2000 开始
]]></description>
      <guid>https://stackoverflow.com/questions/78421910/self-supervised-model-converging-to-a-constant</guid>
      <pubDate>Thu, 02 May 2024 22:03:12 GMT</pubDate>
    </item>
    <item>
      <title>进一步发展数据科学和机器学习 [关闭]</title>
      <link>https://stackoverflow.com/questions/78421369/going-further-witth-data-science-and-ml</link>
      <description><![CDATA[我想知道哪些应用程序、平台或书籍等可以极大地帮助我提升水平并深入了解以下内容：

熊猫
Numpy
Scipy
Keras 和 Tensorflow
Pytorch
Scikit 学习
Matplotlib

除了 pytorch 之外，我对所有这些都有经验，但我真的想走得更远，因为我未来的潜在工作需要这些。]]></description>
      <guid>https://stackoverflow.com/questions/78421369/going-further-witth-data-science-and-ml</guid>
      <pubDate>Thu, 02 May 2024 19:29:01 GMT</pubDate>
    </item>
    <item>
      <title>从 json 文件创建 BIO 格式的句子 - 训练 NER 模型</title>
      <link>https://stackoverflow.com/questions/78420651/create-bio-format-to-a-sentence-from-a-json-file-to-train-ner-model</link>
      <description><![CDATA[我有一个 JSON 文件，将用作 NER 模型的数据。
它有一个句子和该特定句子中的相关实体。
我想创建一个函数，根据实体为每个句子生成 BIO 标记的字符串
例如 JSON 文件中的以下对象
&lt;代码&gt;{
      &quot;request&quot;: &quot;我想13.3 飞往纽约&quot;,
      “实体”：[
        {“start”: 16, “end”: 23, “text”: “纽约”, “category”: “DESTINATION”},
        {“开始”：32，“结束”：35，“文本”：“13.3”，“类别”：“日期”}
      ]
}

“我想乘坐 13.3 号飞往纽约”
相应的BIO标签将是
“O O O O O B-目的地 I-目的地 O O B-日期”
其中 B 类别是该类别的开头
I 类代表内部，O 代表外部。
我正在寻找一个 Python 代码来迭代 JSON 文件中的每个对象，从而为其生成 BIO 标签。
如有必要，更改 JSON 格式]]></description>
      <guid>https://stackoverflow.com/questions/78420651/create-bio-format-to-a-sentence-from-a-json-file-to-train-ner-model</guid>
      <pubDate>Thu, 02 May 2024 16:56:56 GMT</pubDate>
    </item>
    <item>
      <title>使用多个性能指标执行递归特征消除</title>
      <link>https://stackoverflow.com/questions/78420559/performing-recursive-feature-elimination-with-multiple-performance-metrics</link>
      <description><![CDATA[我正在尝试使分类器尽可能简洁。为此，我使用交叉验证递归地删除功能。
在我的模型中，精度是最重要的指标。但是，我还想看看随着模型中输入的功能减少，其他指标将如何演变。
特别是，我想评估模型的召回率和 F1 分数。
rfe = RFECV(
    estimator=clf, # 一个 XGBClassifier 实例
    步骤=1，
    min_features_to_select=1,
    cv=cv, # 一个 StratifiedKFold 实例
    评分=&#39;精度&#39;,
    # 评分=[&#39;f1&#39;, &#39;精度&#39;, &#39;召回&#39;], # 抛出错误
    详细=1，
    n_职位=1
）

我注释掉了将指标列表传递给 scoring 参数的行，因为它抛出 InvalidParameterError （也就是说，它不高兴我向它传递了一个列表）。
有没有办法将多个指标传递给 RFECV 实例？]]></description>
      <guid>https://stackoverflow.com/questions/78420559/performing-recursive-feature-elimination-with-multiple-performance-metrics</guid>
      <pubDate>Thu, 02 May 2024 16:37:03 GMT</pubDate>
    </item>
    <item>
      <title>如何使用拥抱脸部变压器来测试具有 LLM 的数据集？</title>
      <link>https://stackoverflow.com/questions/78420480/how-to-use-hugging-face-transformers-for-testing-a-dataset-with-llms</link>
      <description><![CDATA[我正在努力复制此存储库的结果，但使用其他 LLM，例如 LLAMA。我正在使用 google colab，我已经克隆了存储库并安装了所需的软件包。
他们说你可以使用 hugging face transformers 中的任何模型，但我不知道从哪里获取“model”和“model_args”参数：
# 在 ENEM 2022 上运行 GPT-4V 的 CoT 3-shot
python main.py \
--model chatgpt \
--model_args engine=gpt-4-vision-preview \
--tasks enem_cot_2022_blind,enem_cot_2022_images,enem_cot_2022_captions \
--description_dict_path description.json \
--num_fewshot 3 \
--conversation_template chatgpt

如果您转到 hugging face 中的 Llama 模型 并单击“在 Transformers 中使用”你得到这个：
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Meta-Llama-3-8B&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Meta-Llama-3-8B&quot;)

所以我尝试使用&quot;model = meta-llama&quot; 和 &quot;model_args = Meta-Llama-3-8B&quot;，但没有用。
像这样：
!python main.py \
--model meta-llama \
--model_args Meta-Llama-3-8B \
--tasks enem_cot_2022_blind,enem_cot_2022_captions \
--description_dict_path description.json \
--num_fewshot 3

我得到：
选定的任务：[&#39;enem_cot_2022_blind&#39;, &#39;enem_cot_2022_captions&#39;]
回溯（最近一次调用）：
文件 &quot;/content/gpt-4-enem/main.py&quot;，第 112 行，在 &lt;module&gt;
main()
文件 &quot;/content/gpt-4-enem/main.py&quot;，第 81 行，在 main 中
results = evaluator.simple_evaluate(
文件 &quot;/content/gpt-4-enem/lm_eval/utils.py&quot;，第 164 行，在 _wrapper 中
return fn(*args, **kwargs)
文件 &quot;/content/gpt-4-enem/lm_eval/evaluator.py&quot;，第 66 行，在 simple_evaluate 中
lm = lm_eval.models.get_model(model).create_from_arg_string(
文件 &quot;/content/gpt-4-enem/lm_eval/models/__init__.py&quot;，第 16 行，在 get_model 中
return MODEL_REGISTRY[model_name]
KeyError: &#39;meta-llama&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/78420480/how-to-use-hugging-face-transformers-for-testing-a-dataset-with-llms</guid>
      <pubDate>Thu, 02 May 2024 16:20:28 GMT</pubDate>
    </item>
    <item>
      <title>使用“loss.backward()”函数优化模型参数问题</title>
      <link>https://stackoverflow.com/questions/78420392/optimizing-model-parameters-issue-with-loss-backward-function</link>
      <description><![CDATA[导入GC

从 tqdm.notebook 导入 tqdm

将 matplotlib.pyplot 导入为 plt

torch.set_printoptions(sci_mode=False)


梯度范数 = []

损失=[]

对于 tqdm 中的纪元（范围（纪元））：

    model_path = f“/kaggle/working/Training/ace_state_dict_{epoch+1}.pth”

    torch.save(model.state_dict(), model_path)

    模型.train()

    总损失= 0

    
    对于batch_idx，批量枚举(tqdm(train_dataloader, desc=f&#39;Epoch {epoch + 1}/{epochs}&#39;))：

        优化器.zero_grad()

        logits = model(batch[“输入”])

        目标=批处理[“目标”]

        损失 = loss_fn(logits.view(-1, logits.size(-1)), Targets.float()) / 1000000000

        loss.backward()

        # 计算梯度范数

        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1)

        gradient_norms.append(grad_norm)

        优化器.step()

        调度程序.step()

        总损失 += loss.item()
        

        如果batch_idx％100==0：

            print(f&#39;Batch {batch_idx}/{len(train_dataloader)}，损失：{total_loss/(batch_idx+1)}，梯度范数：{grad_norm}&#39;)

    
    avg_loss = 总损失 / len(train_dataloader)

    损失.append(avg_loss)

    print(f&quot;列车损失：{avg_loss}&quot;)`

在我上面的代码中，loss.backward() 函数未按预期工作，导致 grad_norm 始终注册为 0.0 。这阻碍了每次迭代期间模型参数的优化。有任何指导或建议来纠正这个问题吗？作为参考，您可以在以下位置找到我的模型的完整源代码：
https://www.kaggle.com/code/cutedeadu943/transformerchatbot
我尝试了超参数和学习率的各种组合，但不幸的是，问题仍然存在。此外，我尝试通过使用 torch.nn.utils.clip_grad_norm_() 来解决这个问题，但没有成功。所有相关张量和模型参数均使用 requires_grad=True 设置。有什么见解或替代方法可以解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78420392/optimizing-model-parameters-issue-with-loss-backward-function</guid>
      <pubDate>Thu, 02 May 2024 16:00:27 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练 GAN 模型时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/78420289/getting-valueerror-while-trying-to-train-gan-model</link>
      <description><![CDATA[我正在尝试训练 GAN 模型来检测糖尿病视网膜病变图像，但它抛出错误。请帮忙。
图像数据集不为空我已尝试查看它
错误是：-
纪元 1/50
回溯（最近一次调用最后一次）：
  文件“C:\Users\asus\OneDrive\Desktop\project\DR-GAN\TrainModel.py”，第 65 行，在  中
    分类器.fit（X，Y，batch_size = 32，epochs = 50）
  文件“C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
    从 None 引发 e.with_traceback(filtered_tb)
  文件“C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\backend\tensorflow\nn.py”，第 553 行，在 categorical_crossentropy 中
    引发值错误（
ValueError：参数“target”和“output”必须具有相同的形状。收到：target.shape=(无，3)，output.shape=(无，5)

火车模型文件的代码是：
将 numpy 导入为 np
导入imutils
导入系统
导入CV2
导入操作系统
从tensorflow.keras.utils导入到_categorical
从 keras.models 导入 model_from_json
从 keras.layers 导入 MaxPooling2D
从 keras.layers 导入密集、丢弃、激活、扁平化
从 keras.layers 导入 Convolution2D
从 keras.models 导入顺序

图片 = []
图像标签 = []
目录=&#39;数据集&#39;
文件列表 = os.listdir(目录)
索引 = 0
对于 list_of_files 中的文件：
    子文件 = os.listdir(目录+&#39;/&#39;+文件)
    对于子文件中的子项：
        路径=目录+&#39;/&#39;+文件+&#39;/&#39;+子
        img = cv2.imread(路径)
        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        如果 img 为 None：
          print(&#39;路径错误：&#39;, 路径)
        别的：
         img = cv2.resize(img, (32,32))
         im2arr = np.array(img)
         im2arr = im2arr.reshape(32,32,3)
         图像.append(im2arr)
         image_labels.append(文件)
    打印（文件）

X = np.asarray(图像)
Y = np.asarray(image_labels)
Y = to_categorical(Y)
img = X[20].reshape(32,32,3)
cv2.imshow(&#39;ff&#39;,cv2.resize(img,(250,250)))
cv2.waitKey(0)
print(&quot;形状 == &quot;+str(X.shape))
print(&quot;形状==&quot;+str(Y.shape))
打印（Y）
X = X.astype(&#39;float32&#39;)
X = X/255

np.save(“model/img_data.txt”,X)
np.save(“model/img_label.txt”,Y)

X = np.load(&#39;model/img_data.txt.npy&#39;)
Y = np.load(&#39;model/img_label.txt.npy&#39;)
打印（Y）
img = X[20].reshape(32,32,3)
cv2.imshow(&#39;ff&#39;,cv2.resize(img,(250,250)))
cv2.waitKey(0)

classifier = Sequential() #alexnet 迁移学习代码在这里
classifier.add(Convolution2D(32, 3, 3, input_shape = (32, 32, 3), 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D((2, 2) , padding=&#39;相同&#39;))
classifier.add(Convolution2D(32, 3, 3, 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D((2, 2) , padding=&#39;相同&#39;))
分类器.add(Flatten())
classifier.add（密集（单位= 128，激活=&#39;relu&#39;））
classifier.add(Dense(单位 = 5, 激活 = &#39;softmax&#39;))
classifier.compile（优化器=&#39;adam&#39;，损失=&#39;categorical_crossentropy&#39;，指标= [&#39;准确性&#39;]）
分类器.fit（X，Y，batch_size = 32，epochs = 50）

我尝试过更改尺寸，但它不起作用，我无法理解这是版本错误还是代码错误，因此为了解决同样的问题，请提供解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78420289/getting-valueerror-while-trying-to-train-gan-model</guid>
      <pubDate>Thu, 02 May 2024 15:39:54 GMT</pubDate>
    </item>
    <item>
      <title>ConvLSTM 模型的数据预处理过程中遇到问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78418879/having-trouble-during-preprocessing-of-data-for-a-convlstm-model</link>
      <description><![CDATA[我有 10 年（2014-24）的 GHRSST 数据，我将其分为两部分：训练（2014-2021）和测试（2021-2024）数据集。训练数据集大小为 4.18GB，测试数据集大小为 1.98GB。我正在尝试构建一个 ConvLSTM 模型来预测未来几天的 SST 数据，但是我似乎无法通过预处理阶段。我正在尝试在 google colab 上执行以下代码：
导入火炬
将 numpy 导入为 np
进口达斯克

# NetCDF 文件的路径
dataset_path = &#39;/content/drive/MyDrive/train_dataset_10.nc&#39;
# 使用 dask chunks 打开数据集
ds = xr.open_mfdataset(dataset_path, chunks={&#39;时间&#39;: 1, &#39;纬度&#39;: 50, &#39;经度&#39;: 50})

# 提取SST数据；假设变量名为“analysis_sst”，并使用计算将其转换为 numpy 来加载数据
sst_data = ds[&#39;analysisd_sst&#39;].compute() # 这将确保数据加载到内存中

上述代码需要 30 分钟执行，并占用 10.2/12.7GB 可用系统 RAM。这也会导致以下代码出现问题，该代码使用过多的 CPU（尽管 GPU 处于打开状态）并且由于缺少 RAM 而导致内核崩溃：
# 清除 CUDA 中所有未使用的内存
torch.cuda.empty_cache()

# 将数据转换为张量，出于兼容性原因确保它首先在 CPU 上
sst_tensor = torch.tensor(sst_data.values, dtype=torch.float32)

# 如果有可用的 GPU，则将张量传输到 GPU
如果 torch.cuda.is_available():
    sst_tensor = sst_tensor.to(&#39;cuda&#39;)

print(&quot;SST 张量的形状：&quot;, sst_tensor.shape)

defscale_data_gpu(data_tensor,batch_size):
    scaled_data = torch.full_like(data_tensor, float(&#39;nan&#39;)) # 为填充 NaN 的缩放数据初始化张量

    # 批量处理数据
    对于范围内的开始（0，data_tensor.shape [0]，batch_size）：
        结束 = 开始 + 批次大小
        批处理 = data_tensor[开始:结束]

        # 为有效（非 NaN）数据点创建掩码
        valid_mask = ~torch.isnan(batch)

        if valid_mask.any(): # 确保至少有一些有效数据
            data_min = torch.min(batch[valid_mask])
            data_max = torch.max(batch[valid_mask])

            # 缩放批次，但仅在有效的情况下应用
            batch_scaled = (batch - data_min) / (data_max - data_min)
            scaled_data[开始:结束][valid_mask] = batch_scaled[valid_mask]

    返回缩放数据

# 在 GPU 上应用缩放，仅考虑有效（非 NaN）值
使用 torch.no_grad()：
  sst_scaled_tensor = scale_adata_gpu（data_tensor = sst_tensor，batch_size = 64）

在上面的代码中，我试图掩盖标记为“NaN”的值因为它代表了我的数据集中的地形，然后缩放数据并批量处理它以准备训练。
我该如何进行这项工作？这个过程需要多长时间？有更有效的方法吗？
我尝试过使用 Dask，但没有成功，而且 GEE 不支持 CNN。]]></description>
      <guid>https://stackoverflow.com/questions/78418879/having-trouble-during-preprocessing-of-data-for-a-convlstm-model</guid>
      <pubDate>Thu, 02 May 2024 11:34:30 GMT</pubDate>
    </item>
    <item>
      <title>“numpy.ndarray”在 Ai 模型中插入 Flask 时出现问题</title>
      <link>https://stackoverflow.com/questions/78418741/numpy-ndarray-a-problem-in-flask-inserting-in-ai-model</link>
      <description><![CDATA[我想将字符串插入到机器学习模型中，但它一直这样说：
 预测 = model.predict(email_features_array)
                 ^^^^^^^^^^^^^^
AttributeError：“numpy.ndarray”对象没有属性“预测”

不知道是什么问题，换了好几次解码方式。
这是我在进行矢量化后使用的函数：
def 预测():
    email_text =“你好世界”
    CV = CountVectorizer()
    email_features_array = cv.fit_transform([email_text])
    # email_features_array = fit_count_vectorizer(email_text)
    打印（电子邮件特征数组）
    # 假设`model`已经被定义和训练
    预测 = model.predict(email_features_array)

    # 应用预训练模型来预测电子邮件被钓鱼的概率
    概率 = model.predict_proba(email_features_array)
    如果预测[0] == 1：
        结果=“网络钓鱼”
        概率得分 = 概率[0][1] * 100
        打印（结果，概率分数）
    别的：
        结果=&#39;合法&#39;
        概率得分 = 概率[0][0] * 100
        打印（结果，概率分数）

我预计解码方式有问题，但不确定。]]></description>
      <guid>https://stackoverflow.com/questions/78418741/numpy-ndarray-a-problem-in-flask-inserting-in-ai-model</guid>
      <pubDate>Thu, 02 May 2024 11:10:21 GMT</pubDate>
    </item>
    </channel>
</rss>