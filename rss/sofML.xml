<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 17 Oct 2024 21:17:24 GMT</lastBuildDate>
    <item>
      <title>是否有用于机器学习训练代码的自动回归测试库？[关闭]</title>
      <link>https://stackoverflow.com/questions/79099561/is-there-a-library-for-automated-regression-testing-for-machine-learning-trainin</link>
      <description><![CDATA[我想以自动化的方式测试我的深度学习训练代码，以确保重构不会改变我训练的更新步骤。我想我还需要某种度量来覆盖我的计算图。
是否有一个库可以为 pytorch 或 jax 执行此操作？我不想测试经过训练的模型（有库可以做到这一点），而是测试训练代码。]]></description>
      <guid>https://stackoverflow.com/questions/79099561/is-there-a-library-for-automated-regression-testing-for-machine-learning-trainin</guid>
      <pubDate>Thu, 17 Oct 2024 19:16:16 GMT</pubDate>
    </item>
    <item>
      <title>图像预处理顺序[关闭]</title>
      <link>https://stackoverflow.com/questions/79098008/image-preprocessing-order</link>
      <description><![CDATA[我有一个数据集，其中包含多个 3D 图像，每个图像都有相应的坐标。我想以相同的方式预处理所有这些图像，但是我不确定应该按什么顺序进行。首先，我想将图像重新采样为 1x1x1。我还想裁剪感兴趣的坐标，最后，我想将图像大小调整为 128 x 128 x 128，以供我的深度学习模型使用。
坐标分布在图像周围，不在同一位置。这使其更加混乱，因为裁剪的大小会有所不同。此外，我不想每个图像都有补丁，因此 ResizeWithPadOrCrop（来自 monai）会很困难，因为我希望包含所有坐标。
有人对我应该按什么顺序进行操作有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79098008/image-preprocessing-order</guid>
      <pubDate>Thu, 17 Oct 2024 12:11:23 GMT</pubDate>
    </item>
    <item>
      <title>如何在客户端之间部署我的联邦学习模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79097780/how-to-deploy-my-federated-learning-model-across-clients</link>
      <description><![CDATA[我有要部署的模型，但我不知道如何跨客户端部署。我已经使用 tensorflow 创建了 adaboost 模型，但我不知道如何跨客户端部署该模型。
请提出一些方法来解决我的问题。]]></description>
      <guid>https://stackoverflow.com/questions/79097780/how-to-deploy-my-federated-learning-model-across-clients</guid>
      <pubDate>Thu, 17 Oct 2024 11:07:56 GMT</pubDate>
    </item>
    <item>
      <title>为什么我针对二元分类的多元二维卷积 LSTM 模型的超参数调整是错误的？</title>
      <link>https://stackoverflow.com/questions/79097667/why-is-my-hyperparameter-tuning-for-a-multivariate-2d-convolutional-lstm-model-f</link>
      <description><![CDATA[我尝试编写一个 2D 卷积 LSTM 模型。
我有 7514 个样本。
每个样本包含 180 分钟的数据。
有 5 个特征。
样本由连续的 120 分钟周期组成，其中每分钟的目标值为 0，然后是 60 分钟周期，每分钟的目标值为 1。
120 分钟周期表示正常活动，而 60 分钟周期表示异常活动。
序列是 Conv2D - Batch Normalization - Activation - LSTM - Fully Connected。
我不完全理解时间序列的 Conv2D 参数。
import pandas as pd
import io
import tensorflow as tf
import keras
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import LSTM, Dense, BatchNormalization, Flatten, Conv2D, Reshape, TimeDistributed
import numpy as np
from sklearn.preprocessing import LabelEncoder

colnames=(&#39;Bx&#39;, &#39;By&#39;, &#39;Bz&#39;, &#39;Vx&#39;, &#39;Density&#39;, &#39;Labels&#39;, &#39;ID&#39;)
df = pd.read_csv(&#39;200304periodnanreplacedwithID&#39;, sep=&#39;\s+&#39;, names=colnames)
dfc=df.drop([&#39;Labels&#39;, &#39;ID&#39;], axis=1)
rmv=dfc[dfc.apply(sum, axis = 1) == 0].index
df1=df.drop(rmv)
counts = df1[&#39;ID&#39;].value_counts()
df1=df1[df1[&#39;ID&#39;].isin(counts.index[counts == 180])]
df1=df1.drop([&#39;ID&#39;], axis=1)

Xcols=[x for x in df1.columns if x!= &#39;Labels&#39;]
features=len(Xcols)
model= Sequential()
X=df1[Xcols]
X=np.resize(X, (X.shape[0], 1, X.shape[1]))
y=df1[&#39;Labels&#39;]

def basic_conv2D(n_filters=7514, fsize=5, window_size=180, n_features=5):
new_model = keras.Sequential()
new_model.add(tf.keras.layers.Conv2D(180, (3, fsize), padding=&#39;same&#39;, input_shape=(window_size, n_features, 1)))
new_model.add(BatchNormalization())
new_model.add(tf.keras.layers.Activation(&#39;relu&#39;))
new_model.add(TimeDistributed(Flatten()))
new_model.add(tf.keras.layers.LSTM(120, return_sequences=True))
new_model.add(tf.keras.layers.Dense(1))
adm = keras.optimizers.Adam(learning_rate=0.01)
new_model.compile(optimizer=adm, loss=&#39;binary_crossentropy&#39;, metrics=[keras.metrics.Recall(), keras.metrics.Precision()])
返回 new_model

m2 = basic_conv2D(n_filters=7514, fsize=5, window_size=1, n_features=5)
m2.summary()

m2_hist = m2.fit(X, y, batch_size=180, shuffle=False, validation_split=0.3, epochs=5)
]]></description>
      <guid>https://stackoverflow.com/questions/79097667/why-is-my-hyperparameter-tuning-for-a-multivariate-2d-convolutional-lstm-model-f</guid>
      <pubDate>Thu, 17 Oct 2024 10:32:43 GMT</pubDate>
    </item>
    <item>
      <title>微调细分任何模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79097002/fine-tuning-segment-anything-model</link>
      <description><![CDATA[我一直在尝试用 Python 微调 SAM 模型，但我发现大多数教程都要求我们在微调后提供提示。 难道不能微调模型，以便它可以在不提供提示的情况下处理该特定数据集吗？
我尝试按照教程操作，但没有输出任何相关结果。我会将笔记本附在这里。]]></description>
      <guid>https://stackoverflow.com/questions/79097002/fine-tuning-segment-anything-model</guid>
      <pubDate>Thu, 17 Oct 2024 07:29:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 Noise2Self 的 AI 降噪器无法正确训练</title>
      <link>https://stackoverflow.com/questions/79095888/noise2self-based-ai-denoiser-failing-to-train-properly</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79095888/noise2self-based-ai-denoiser-failing-to-train-properly</guid>
      <pubDate>Wed, 16 Oct 2024 21:30:55 GMT</pubDate>
    </item>
    <item>
      <title>Gymnasium 自定义环境“太多值无法解压”错误</title>
      <link>https://stackoverflow.com/questions/79084313/gymnasium-custom-environment-too-many-values-to-unpack-error</link>
      <description><![CDATA[我正在尝试使用具有体育馆和稳定基线的自定义群体聚集环境。我有一个自定义策略和训练循环。
我的行动和观察空间如下：
min_action = np.array([-5, -5] * len(self.agents), dtype=np.float32)
max_action = np.array([5, 5] * len(self.agents), dtype=np.float32)

min_obs = np.array([-np.inf, -np.inf, -2.5, -2.5] * len(self.agents), dtype=np.float32)
max_obs = np.array([np.inf, np.inf, 2.5, 2.5] * len(self.agents), dtype=np.float32)

训练代码：
import numpy as np
import torch as th
from Parameters import *
from stable_baselines3 import PPO
from main import FlockingEnv, CustomMultiAgentPolicy
from Callbacks import TQDMProgressCallback, LossCallback
import os
from stable_baselines3.common.vec_env import DummyVecEnv

if os.path.exists(Results[&quot;Rewards&quot;]):
os.remove(Results[&quot;Rewards&quot;])
print(f&quot;File {Results[&#39;Rewards&#39;]} has been removed.&quot;)

if os.path.exists(&quot;training_rewards.json&quot;):
os.remove(&quot;training_rewards.json&quot;)
print(f&quot;文件 training_rewards 已被删除。&quot;) 

def seed_everything(seed):
np.random.seed(seed)
os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
th.manual_seed(seed)
th.cuda.manual_seed(seed)
th.backends.cudnn.deterministic = True
env.seed(seed)
env.action_space.seed(seed)

loss_callback = LossCallback()
env = DummyVecEnv([lambda: FlockingEnv()])

seed_everything(SimulationVariables[&quot;Seed&quot;])

# # 模型训练
model = PPO(CustomMultiAgentPolicy, env, tensorboard_log=&quot;./ppo_Agents_tensorboard/&quot;, verbose=1)
model.set_random_seed(SimulationVariables[&quot;ModelSeed&quot;])
progress_callback = TQDMProgressCallback(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;])
# 训练模型
model.learn(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;], callback=[progress_callback, loss_callback])

错误：
使用 cuda 设备
回溯（最近一次调用最后一次）：
文件 &quot;D:\Thesis_\FlockingFinal\MultiAgentFlocking\Training.py&quot;，行45，在&lt;module&gt;中
model.learn(total_timesteps=SimulationVariables[&quot;LearningTimeSteps&quot;], callback=[progress_callback, loss_callback]) 
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\ppo\ppo.py&quot;，第 315 行，在 learn 中
return super().learn(
^^^^^^^^^^^^^^^
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py&quot;，第 287 行，在 learn 中
total_timesteps, callback = self._setup_learn(
^^^^^^^^^^^^^^^^^^^
文件&quot;C:\Python312\Lib\site-packages\stable_baselines3\common\base_class.py&quot;，第 423 行，在 _setup_learn
self._last_obs = self.env.reset() # 类型：ignore[assignment]
^^^^^^^^^^^^^^^^^
文件 &quot;C:\Python312\Lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py&quot;，第 77 行，在 reset
obs 中，self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError：太需要解包的值很多（预计为 2 个）

我也在 gym 中使用了类似的种子函数，但没有出现错误，我以为是它导致了错误，但即使我不使用它，错误也不会消失。]]></description>
      <guid>https://stackoverflow.com/questions/79084313/gymnasium-custom-environment-too-many-values-to-unpack-error</guid>
      <pubDate>Sun, 13 Oct 2024 22:45:48 GMT</pubDate>
    </item>
    <item>
      <title>GNU Octave 是多线程的吗？</title>
      <link>https://stackoverflow.com/questions/79050512/is-gnu-octave-multi-threaded</link>
      <description><![CDATA[根据这个老问题的答案，GNU Octave 似乎是一个单线程应用程序。
但是，我正在试验一个名为nnet的旧 Octave 神经网络包，并惊讶地发现我的 Octave 程序使用了笔记本电脑的所有 4 个核心。自从我链接的问题提出以来，情况有变化吗？GNU Octave 现在是多线程的吗？据我所知，我没有看到 nnet 内部有任何并行实现。
有关我的安装的一些信息：

我的操作系统是 Linux Mint 20
我的机器有 4 个处理单元（这是 nproc 在我的终端中显示的内容）
我的 Octave 版本是 5.2.0（如果这有区别的话，我正在使用 GUI）

我的代码相当简单，只导入了 nnet 包，没有其他内容。当我查看运行程序时的资源时，我看到所有核心都已使用（下面是 htop 屏幕截图）

这是我正在做的事情：
pkg load nnet

starttime = clock();

# 取自 http://matlab.izmiran.ru/help/toolbox/nnet/newff.html
Pr = -1:0.00005:1;
Tr = 3*sin(pi*Pr)-cos(pi*Pr);
Prmin = min(Pr);
Prmax = max(Pr);
net = newff([Prmin Prmax],[3 2 1],{&#39;tansig&#39;,&#39;logsig&#39;,&#39;purelin&#39;},&#39;trainlm&#39;);
[net] = train(net,Pr,Tr,[],[],[]);
[netoutput] = sim(net,Pr);

etime(clock(),starttime)

% 测试结果 
plot(Pr,Tr,&#39;b+&#39;);
hold on; 
plot(Pr,netoutput,&#39;r-&#39;);
hold off;

编辑
根据评论中的 @JérômeRichard 提示和 @NickJ 建议，我通过在终端中执行 export OMP_NUM_THREADS=1 来启动 Octave，只为 BLAS 分配 1 个线程。该脚本的速度是原始设置的两倍（根据上面发布的 htop 屏幕截图，默认设置是 4）。我确保我的程序只使用 htop 中的一个核心。]]></description>
      <guid>https://stackoverflow.com/questions/79050512/is-gnu-octave-multi-threaded</guid>
      <pubDate>Thu, 03 Oct 2024 12:19:13 GMT</pubDate>
    </item>
    <item>
      <title>根据 HistGratientBoostingClassifier 绘制决策树</title>
      <link>https://stackoverflow.com/questions/78636029/plot-a-decision-tree-from-histgratientboostingclassifier</link>
      <description><![CDATA[我有一个 HistGradientBoostingClassifier 模型，我想绘制一个或多个决策树，但我找不到原生函数来执行此操作，我可以访问 Tree 预测器对象及其节点，但为了将其绘制到 sklearn.tree.plot_tree 函数中，它需要是 DecisionTree 类型的对象
我试过这个：
from sklearn.tree import plot_tree

plot_tree(RF_90._predictors[0][0])

出现此错误：

InvalidParameterError：plot_tree 的 &#39;decision_tree&#39; 参数必须
是 &#39;sklearn.tree._classes.DecisionTreeClassifier&#39; 的实例或
&#39;sklearn.tree._classes.DecisionTreeRegressor&#39;。得到的是
&lt;sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor
对象位于 0x7f676ebf0310&gt;。

注意：RF_90 是 HistGradientBoostingClassifier 拟合模型]]></description>
      <guid>https://stackoverflow.com/questions/78636029/plot-a-decision-tree-from-histgratientboostingclassifier</guid>
      <pubDate>Tue, 18 Jun 2024 07:26:34 GMT</pubDate>
    </item>
    <item>
      <title>LLM Studio 无法下载模型并出现错误：无法获取本地颁发者证书</title>
      <link>https://stackoverflow.com/questions/78379820/llm-studio-fail-to-download-model-with-error-unable-to-get-local-issuer-certif</link>
      <description><![CDATA[在 LLM 工作室中，当我尝试下载任何模型时，我遇到以下错误：
下载失败：无法获取本地颁发者证书
]]></description>
      <guid>https://stackoverflow.com/questions/78379820/llm-studio-fail-to-download-model-with-error-unable-to-get-local-issuer-certif</guid>
      <pubDate>Wed, 24 Apr 2024 16:03:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中有效地实现非全连接线性层？</title>
      <link>https://stackoverflow.com/questions/70269663/how-to-efficiently-implement-a-non-fully-connected-linear-layer-in-pytorch</link>
      <description><![CDATA[我制作了一个示例图，展示了我试图实现的缩小版本：

因此，顶部两个输入节点仅完全连接到顶部三个输出节点，并且相同的设计适用于底部两个节点。到目前为止，我已经想出了两种在 PyTorch 中实现此目的的方法，但都不是最佳方法。
第一种方法是创建一个包含许多较小线性层的 nn.ModuleList，并在前向传递期间，通过它们迭代输入。对于图表的示例，它看起来像这样：
class Module(nn.Module):
def __init__(self):
self.layers = nn.Module([nn.Linear(2, 3) for i in range(2)])

def forward(self, input):
output = torch.zeros(2, 3)
for i in range(2):
output[i, :] = self.layers[i](input.view(2, 2)[i, :])
return output.flatten()

因此，这完成了图中的网络，主要问题是它非常慢。我认为这是因为 PyTorch 必须按顺序处理 for 循环，而不能并行处理输入张量。
要“矢量化”模块以便 PyTorch 可以更快地运行它，我有这个实现：
class Module(nn.Module):
def __init__(self):
self.layer = nn.Linear(4, 6)
self.mask = # 创建 1 和 0 的掩码来“阻止”某些层连接

def forward(self, input):
prune.custom_from_mask(self.layer, name=&#39;weight&#39;, mask=self.mask)
return self.layer(input)

这也完成了图表的网络，通过使用权重修剪来确保完全连接层中的某些权重始终为零（例如，连接顶部输入节点和底部输出节点的权重将始终为零，因此它实际上是“断开连接的”）。这个模块比上一个模块快得多，因为没有 for 循环。现在的问题是这个模块占用了更多的内存。这可能是因为，即使大多数层的权重为零，PyTorch 仍会将网络视为它们存在。此实现本质上保留了比需要更多的权重。
有人遇到过这个问题并想出了有效的解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/70269663/how-to-efficiently-implement-a-non-fully-connected-linear-layer-in-pytorch</guid>
      <pubDate>Wed, 08 Dec 2021 03:41:06 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 中的分类返回相同的分类值</title>
      <link>https://stackoverflow.com/questions/68315278/classification-in-lstm-returns-same-value-for-classification</link>
      <description><![CDATA[我有 10000 个数据，每个数据都有 0 和 1 的标签。我想使用 LSTM 进行分类，因为这是时间序列数据。
input_dim = 1
hidden_​​dim = 32
num_layers = 2
output_dim = 1

# 在这里我们将模型定义为一个类
class LSTM(nn.Module):
def __init__(self, input_dim, hidden_​​dim, num_layers, output_dim):
super(LSTM, self).__init__()
self.hidden_​​dim = hidden_​​dim
self.num_layers = num_layers
self.lstm = nn.LSTM(input_dim, hidden_​​dim, num_layers, batch_first=True)
self.fc = nn.Linear(hidden_​​dim, output_dim)

def forward(self, x):
#初始化隐藏层和单元状态
h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_​​dim).requires_grad_()
c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_​​dim).requires_grad_()

#我们需要分离，因为我们正在进行截断时间反向传播 (BPTT)
#如果不这样做，我们将一直反向传播到起点，即使经过另一个批次
out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))

#索引上一个时间步骤的隐藏状态
# out.size() --&gt; 100, 32, 100
# out[:, -1, :] --&gt; 100, 100 --&gt;只想要最后一步隐藏状态！
out = self.fc(out[:, -1, :])

# 用于二项分类
m = torch.sigmoid(out)

return m

model = LSTM(input_dim=input_dim, hidden_​​dim=hidden_​​dim, output_dim=output_dim, num_layers=num_layers)
loss = nn.BCELoss()

optimiser = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.00006)

num_epochs = 100

# 要展开的步骤数
seq_dim =look_back-1 

for t in range(num_epochs):
y_train_class = model(x_train)

output = loss(y_train_class, y_train)

# 将梯度归零，否则它们将在梯度之间累积epochs
optimiser.zero_grad(set_to_none=True)

# 反向传播
output.backward()

# 更新参数
optimiser.step()


这是结果的示例
此代码最初来自 kaggle，我对其进行了编辑以进行分类。请告诉我我做错了什么？
编辑 1：
添加数据加载器
从 torch.utils.data 导入 DataLoader
从 torch.utils.data 导入 TensorDataset
x_train = torch.from_numpy(x_train).type(torch.Tensor)
y_train = torch.from_numpy(y_train).type(torch.Tensor)
x_test = torch.from_numpy(x_test).type(torch.Tensor)
y_test = torch.from_numpy(y_test).type(torch.Tensor)

train_dataloader = DataLoader(TensorDataset(x_train, y_train), batch_size=128, shuffle=True)
test_dataloader = DataLoader(TensorDataset(x_test, y_test), batch_size=128, shuffle=True)

我意识到在检查结果之前我忘记了反转转换。当我这样做时，我从分类中得到了不同的值，但是所有值都在 0.001-0.009 的范围内，所以当我对它们进行四舍五入时，结果是相同的。所有分类都标记为 0。]]></description>
      <guid>https://stackoverflow.com/questions/68315278/classification-in-lstm-returns-same-value-for-classification</guid>
      <pubDate>Fri, 09 Jul 2021 10:29:00 GMT</pubDate>
    </item>
    <item>
      <title>在 R 中对 Xgboost 进行超参数调整以进行二元分类</title>
      <link>https://stackoverflow.com/questions/60538664/xgboost-hyperparameter-tuning-in-r-for-binary-classification</link>
      <description><![CDATA[我正在尝试对 xgboost-二元分类进行超参数调整，但是我得到了错误：
as.matrix(cv.res)[, 3] 中的错误：下标超出范围
此外：警告消息：&#39;early.stop.round&#39; 已弃用。
改用 &#39;early_stopping_rounds&#39;。
请参阅帮助（“弃用”）和帮助（“xgboost-deprecated”）。

如何解决？
请参阅下面的代码片段`
X_Train &lt;- as(X_train, &quot;dgCMatrix&quot;)

GS_LogLoss = data.frame(&quot;Rounds&quot; = numeric(), 
&quot;Depth&quot; = numeric(),
&quot;r_sample&quot; = numeric(),
&quot;c_sample&quot; = numeric(), 
&quot;minLogLoss&quot; = numeric(),
&quot;best_round&quot; = numeric())

for (rounds in seq(50,100, 25)) {

for (depth in c(4, 6, 8, 10)) {

for (r_sample in c(0.5, 0.75, 1)) {

for (c_sample in c(0.4, 0.6, 0.8, 1)) {

for (imb_scale_pos_weight in c(5, 10, 15, 20, 25)) {

for (wt_gamma in c(5, 7, 10)) {

for (wt_max_delta_step in c(5,7,10)) {

for (wt_min_child_weight in c(5,7,10,15)) {

set.seed(1024)
eta_val = 2 / rounds
cv.res = xgb.cv(data = X_Train, nfold = 2, label = y_train, 
nrounds = rounds, 
eta = eta_val, 
max_depth =depth,
subsample = r_sample, 
colsample_bytree = c_sample,
early.stop.round = 0.5*rounds,
scale_pos_weight= imb_scale_pos_weight,
max_delta_step = wt_max_delta_step,
gamma = wt_gamma,
objective=&#39;binary:logistic&#39;, 
eval_metric = &#39;auc&#39;,
verbose = FALSE)

print(paste(rounds,depth,r_sample,c_sample,min(as.matrix(cv.res)[,3])))
GS_LogLoss[nrow(GS_LogLoss)+1,] = c(rounds, 
depth, 
r_sample, 
c_sample, 
min(as.matrix(cv.res)[,3]), 
which.min(as.matrix(cv.res)[,3]))

}
}
}
}
}
}
}
}

`]]></description>
      <guid>https://stackoverflow.com/questions/60538664/xgboost-hyperparameter-tuning-in-r-for-binary-classification</guid>
      <pubDate>Thu, 05 Mar 2020 05:24:56 GMT</pubDate>
    </item>
    <item>
      <title>使用 LSTM 构建二元分类模型</title>
      <link>https://stackoverflow.com/questions/58285521/build-a-binary-classification-model-with-lstm</link>
      <description><![CDATA[我有一个 csv 格式的数据集，其中包含 49 列，其中一些是字符串，一些是整数。
我添加了一个新列用作标签，名为“input”，其相应标签为 0 和 1。
以下是数据集的示例：

要求考虑所有这些特征列进行模型训练。
我有哪些选项可以训练这个模型？
我应该遵循哪些步骤？]]></description>
      <guid>https://stackoverflow.com/questions/58285521/build-a-binary-classification-model-with-lstm</guid>
      <pubDate>Tue, 08 Oct 2019 11:31:16 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 scikit 线性回归找到系数的特征名称？</title>
      <link>https://stackoverflow.com/questions/34649969/how-to-find-the-features-names-of-the-coefficients-using-scikit-linear-regressio</link>
      <description><![CDATA[我使用 scikit 线性回归，如果我更改特征的顺序，系数仍会按相同顺序打印，因此我想知道特征与系数的映射。
#训练模型
model_1_features = [&#39;sqft_living&#39;, &#39;bathrooms&#39;, &#39;bedrooms&#39;, &#39;lat&#39;, &#39;long&#39;]
model_2_features = model_1_features + [&#39;bed_bath_rooms&#39;]
model_3_features = model_2_features + [&#39;bedrooms_squared&#39;, &#39;log_sqft_living&#39;, &#39;lat_plus_long&#39;]

model_1 = linear_model.LinearRegression()
model_1.fit(train_data[model_1_features], train_data[&#39;price&#39;])

model_2 = linear_model.LinearRegression()
model_2.fit(train_data[model_2_features], train_data[&#39;price&#39;])

model_3 = linear_model.LinearRegression()
model_3.fit(train_data[model_3_features], train_data[&#39;price&#39;])

# 提取系数
print model_1.coef_
print model_2.coef_
print model_3.coef_
]]></description>
      <guid>https://stackoverflow.com/questions/34649969/how-to-find-the-features-names-of-the-coefficients-using-scikit-linear-regressio</guid>
      <pubDate>Thu, 07 Jan 2016 07:58:04 GMT</pubDate>
    </item>
    </channel>
</rss>