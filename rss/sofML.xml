<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 14 Nov 2024 15:18:04 GMT</lastBuildDate>
    <item>
      <title>我在 SVR 建模中做错了什么？</title>
      <link>https://stackoverflow.com/questions/79188969/what-have-i-done-wrong-in-my-svr-modeling</link>
      <description><![CDATA[我尝试使用 SVR 模型预测风速，因为我的数据集都是连续数字。我按照惯例对数据进行训练测试分割，并使用 minmax 缩放器对数据集进行归一化。
# 训练 - 测试分割
X_train , X_test , y_train , y_test = train_test_split (X , y , test_size =0.3 , random_state =42)

# 归一化数据 - MinMax 缩放器
scaler = MinMaxScaler ()
df_normalized = scaler.fit_transform(df)
df_normalized = pd.DataFrame(df_normalized, columns=df.columns)
X_train = scaler.fit_transform ( X_train )
X_test = scaler.transform ( X_test )


# 支持向量回归 (SVR) 模型
svr_model = SVR(kernel=&#39;rbf&#39;)
svr_model.fit(X_train, y_train)

# 预测和评估 SVR 结果
y_pred = svr_model.predict(X_test)

# 可视化 SVR 结果 
plt.scatter(X_train, y_train, color = &#39;magenta&#39;, label = &#39;Actual Data&#39;)
plt.plot(X_test, y_pred, color = &#39;blue&#39;, label = &#39;SVR Prediction&#39;)
plt.title(&quot;SVR - 风速预测&quot;)
plt.xlabel(&#39;Position Level&#39;)
plt.ylabel(&#39;Wind Speed&#39;)
plt.legend()
plt.show()

plt.scatter(X_train, y_train, color = &#39;magenta&#39;, label = &#39;Actual&#39;) 存在问题数据&#39;)
引发 ValueError(&quot;x 和 y 必须大小相同&quot;)
ValueError: x 和 y 必须大小相同
我尝试使用 X_grid 和 Y_grid，但问题相同。有人能帮我解决这个问题吗 ;-;]]></description>
      <guid>https://stackoverflow.com/questions/79188969/what-have-i-done-wrong-in-my-svr-modeling</guid>
      <pubDate>Thu, 14 Nov 2024 13:30:38 GMT</pubDate>
    </item>
    <item>
      <title>无法训练我的 UNET 多类别细分模型</title>
      <link>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</link>
      <description><![CDATA[我尝试使用 pytorch 从头开始​​制作 UNET，但什么也没得到，只有黑色蒙版作为模型的输出。我需要分割汽车上的损坏，所以我实现了彩色图。我确信 70% 的数据集有问题，正是这张彩色图的问题。这是多类预测任务，所以我使用交叉熵损失函数。我将提供我的数据集和训练文件的代码。
#dataset.py
import os
from PIL import Image
from torch.utils.data import Dataset
import numpy as np
import torch

class Segm_Dataset(Dataset):
def __init__(self, image_dir, mask_dir, color_map):
self.image_dir = image_dir
self.mask_dir = mask_dir
self.image_files = os.listdir(self.image_dir)
self.mask_files = os.listdir(self.mask_dir)
self.color_map = color_map

def __len__(self):
return len(self.image_files)

def __getitem__(self, idx):
image_path = os.path.join(self.image_dir, self.image_files[idx])
mask_path = os.path.join(self.mask_dir, self.mask_files[idx])
image = np.array(Image.open(image_path).convert(&#39;RGB&#39;))
mask = np.array(Image.open(mask_path).convert(&#39;RGB&#39;), dtype=np.float32)
label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int64)

for color, label in self.color_map.items():
color_array = np.array(color, dtype=np.float32)
mask_area = np.all(mask == color_array, axis=-1)
label_mask[mask_area] = label

image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)
label_mask = torch.tensor(label_mask, dtype=torch.long)

返回图像，label_mask

#train.py
从模型导入 UNET
从 tqdm 导入 tqdm
从数据集导入 Segm_Dataset
导入 torch
从 torch.utils.data 导入 DataLoader
导入 torch.nn 作为 nn
导入 torch.optim 作为 optim
导入 os

LEARNING_RATE = 1e-4
BATCH_SIZE = 5
NUM_EPOCHS = 10
NUM_WORKERS = 2
IMAGE_HEIGHT = 180
IMAGE_WIDTH = 180
PIN_MEMORY = True
LOAD_MODEL = False
TRAIN_IMG_DIR = r&#39;data\train\images&#39;
TRAIN_MASK_DIR = r&#39;data\train\masks&#39;
VAL_IMG_DIR = r&#39;data\val\images&#39;
VAL_MASK_DIR = r&#39;data\val\masks&#39;
SAVED_MODELS_PATH = r&#39;saved_models&#39;

color_map = {
(19, 164, 201): 0, # 缺失部分：#13A4C9
(166, 255, 71): 1, # 破损部分：#A6FF47
(180, 45, 56): 2, # 划痕：#B42D38
(225, 150, 96): 3, # 破裂：#E19660
(144, 60, 89): 4, # 凹痕：#903C59
(167, 116, 27): 5, # 剥落：#A7741B
(180, 14, 19): 6, # 油漆剥落：#B40E13
(115, 194, 206): 7, # 腐蚀：#73C2CE
}

train_dataset = Segm_Dataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, color_map)
train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)

val_dataset = Segm_Dataset(VAL_IMG_DIR, VAL_MASK_DIR, color_map)
val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)

model = UNET(in_channels=3, out_channels=len(color_map))
model = model.cuda() if torch.cuda.is_available() else model

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

for epoch in range(NUM_EPOCHS):
train_loop = tqdm(enumerate(train_loader), total=len(train_loader))

for batch_index, (data, target) in train_loop: 
#前向传递
scores = model(data)
train_loss = criterion(scores, target)

#后向传递
optimizer.zero_grad()
train_loss.backward()

#梯度下降或优化器步骤
optimizer.step()

if batch_index % 10 == 0:
current_batch = batch_index
val_loss = 0
with torch.no_grad():
for val_data, val_targets in val_loader:
val_scores = model(val_data)
val_loss = criterion(val_scores, val_targets)

#更新进度条
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

else:
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

checkpoint = {
&#39;epoch&#39;: epoch + 1,
&#39;model_state_dict&#39;: model.state_dict(),
&#39;optimizer_state_dict&#39;: optimizer.state_dict(),
&#39;train_loss&#39;: train_loss.item(),
&#39;val_loss&#39;: val_loss.item()
}

torch.save(checkpoint, os.path.join(SAVED_MODELS_PATH, f&#39;unet_epoch_{epoch}.pth&#39;))

一些训练时期：
时期：[9/10]：100%|████████████████| 888/888 [34:24&lt;00:00, 2.32s/it, train_loss=0.000271, val_batch=880, val_loss=0.000278]
纪元：[10/10]：100%|█████████████████| 888/888 [34:29&lt;00:00, 2.33s/it, train_loss=0.000163, val_batch=880, val_loss=0.000167]]]></description>
      <guid>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</guid>
      <pubDate>Thu, 14 Nov 2024 09:17:27 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：预期隐藏[0]大小（2，50，1024），在PyTorch中得到[1，50，1024]</title>
      <link>https://stackoverflow.com/questions/79187818/runtimeerror-expected-hidden0-size-2-50-1024-got-1-50-1024-in-pytorc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79187818/runtimeerror-expected-hidden0-size-2-50-1024-got-1-50-1024-in-pytorc</guid>
      <pubDate>Thu, 14 Nov 2024 07:55:37 GMT</pubDate>
    </item>
    <item>
      <title>寻求顶点项目的建议：本地 Raspberry Pi 与使用深度学习的多物联网系统的基于云的解决方案</title>
      <link>https://stackoverflow.com/questions/79186425/seeking-advice-for-capstone-project-local-raspberry-pi-vs-cloud-based-solution</link>
      <description><![CDATA[我们的项目重点是开发一种系统，通过使用传感器、电机、摄像头以及最重要的微处理器控制器，根据参观者的脸部高度调整博物馆中绘画的高度，从而提高行动不便人士的可访问性。我们的目标是升级系统，以便您可以同时管理多幅画作。每幅画都有自己的组件，此外还有一个微处理器控制器，可以识别我们在某个通信协议中与哪个控制器对话。
我们的系统需要深度学习软件来识别坐在轮椅上的人
深度学习软件会很重，需要强大的资源，所以我们正在讨论选择哪种方法，并寻求您的专业建议。
选项是：

在本地运行软件：每个绘画系统将使用 Raspberry Pi 5 ai 套件，该套件的价格高达很多，能够运行深度学习软件。

优点：易于实现，特别是如果您正在处理一幅画。
缺点：当有多个绘图时很复杂，需要在每个控制器中重新安装，维护复杂且成本高。

使用强大的云服务器：用更便宜的控制器替换 Raspberry Pi （esp32），并建立控制器与云之间的通信。

优点：更易于维护、经济、可扩展。
缺点：系统更复杂。
问题：
例如：当 4 个人坐在轮椅上，每个人同时接近博物馆中的不同画作并需要识别时，云是否能承受资源负载？
后续问题，是否需要同步编程，以便每个对云的请求（来自每个绘图的不同访问者的请求）都等待前一个请求的完成？这意味着为每个绘画系统通信的 ML 程序托管 1 个云？
TL;DR：争论使用 Raspberry Pi 为每个绘图提供本地解决方案，这更昂贵但简单，以及使用它的基于云的解决方案，这便宜但复杂。]]></description>
      <guid>https://stackoverflow.com/questions/79186425/seeking-advice-for-capstone-project-local-raspberry-pi-vs-cloud-based-solution</guid>
      <pubDate>Wed, 13 Nov 2024 19:43:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 Apache Spark ML 进行预测</title>
      <link>https://stackoverflow.com/questions/79186067/prediction-with-apache-spark-ml</link>
      <description><![CDATA[我是 Apache Spark ML 的新手。
我想预测按年龄和国家/地区划分的余额。作为输入，我有一个以下格式的 CSV 文件：
RowNumber,Age,Country,Balance

模型已构建，也可以针对测试数据进行训练。到目前为止一切正常。
我现在的问题是当我想对新客户记录进行预测时
Dataset&lt;Row&gt; newCustomer = spark.createDataFrame(Collections.singletonList(
new Customer(28, ‘Germany’)), Customer.class);
Dataset&lt;Row&gt; newCustomerPrediction = model.transform(newCustomer);

我收到以下错误消息：
java.lang.IllegalArgumentException：CountryIndex 不存在。可用：年龄、国家。

如何获取新数据集的预测？
public static void main(String[] args) {

SparkSession spark = SparkSession
.builder()
.master(&quot;local[*]&quot;) 
.appName(&quot;JavaGeneralizedLinearRegressionExample&quot;)
.getOrCreate();

Dataset&lt;Row&gt; data = spark.read()
.option(&quot;header&quot;, &quot;true&quot;)
.option(&quot;inferSchema&quot;, &quot;true&quot;)
.option(&quot;delimiter&quot;, &quot;,&quot;) // oder &quot;,&quot;您可以使用 Dataiformat
.csv(&quot;/data/testdaten_v4.csv&quot;);

StringIndexer countryIndexer = new StringIndexer()
.setInputCol(&quot;Country&quot;)
.setOutputCol(&quot;CountryIndex&quot;)
.setHandleInvalid(&quot;skip&quot;);
OneHotEncoder countryEncoder = new OneHotEncoder()
.setInputCol(&quot;CountryIndex&quot;)
.setOutputCol(&quot;CountryVec&quot;);

VectorAssembler assembler = new VectorAssembler()
.setInputCols(new String[]{&quot;Age&quot;, &quot;CountryVec&quot;}) // 可以添加其他 Features
.setOutputCol(&quot;features&quot;);

StandardScaler scaler = new StandardScaler()
.setInputCol(&quot;features&quot;)
.setOutputCol(&quot;scaledFeatures&quot;);

LinearRegression lr = new LinearRegression()
.setLabelCol(&quot;Balance&quot;)
.setFeaturesCol(&quot;scaledFeatures&quot;)
.setMaxIter(100)
.setRegParam(0.3)
.setElasticNetParam(0.8);

Pipeline pipeline = new Pipeline()
.setStages(new PipelineStage[]{countryIndexer, countryEncoder, assembler, scaler, lr});

PipelineModel model = pipeline.fit(data);

Dataset&lt;Row&gt;[] splits = data.randomSplit(new double[]{0.8, 0.2}, 42);
数据集&lt;Row&gt; trainData = splits[0];
数据集&lt;Row&gt; testData = splits[1];

数据集&lt;Row&gt; predictions = model.transform(testData);
predictions.select(&quot;Age&quot;, &quot;Country&quot;, &quot;Balance&quot;, &quot;prediction&quot;).show();

RegressionEvaluator evaluator = new RegressionEvaluator()
.setLabelCol(&quot;Balance&quot;)
.setPredictionCol(&quot;prediction&quot;)
.setMetricName(&quot;rmse&quot;);
double rmse = evaluator.evaluate(predictions);

数据集&lt;Row&gt; newCustomer = spark.createDataFrame(Collections.singletonList(
new Customer(28, &quot;Germany&quot;)), Customer.class);
Dataset&lt;Row&gt; newCustomerPrediction = model.transform(newCustomer);
newCustomerPrediction.select(&quot;prediction&quot;).show();

spark.stop();
}

public static class Customer {
private int Age;
private String Country;

public Customer(int age, String country) {
this.Age = age;
this.Country = country;
}

public int getAge() { return Age; }
public String getCountry() { return Country; } 
}
]]></description>
      <guid>https://stackoverflow.com/questions/79186067/prediction-with-apache-spark-ml</guid>
      <pubDate>Wed, 13 Nov 2024 17:42:53 GMT</pubDate>
    </item>
    <item>
      <title>使用《Python 深度学习》中的 Keras 进行多类分类的准确率与教科书上的准确率相差甚远</title>
      <link>https://stackoverflow.com/questions/79185545/multiclass-classifier-using-keras-from-deep-learning-with-python-yields-very-d</link>
      <description><![CDATA[以下是 François Chollet 所著《使用 Python 进行深度学习》第 4 章中多类分类器的代码。教科书提到此代码将产生&gt;95% 的训练准确率，但我的环境似乎与教科书相比产生了非常低的准确率&lt;50%。
Keras 版本 - 3.6
Tensorflow - 2.18
硬件 - Apple M1 Pro
import keras
from tensorflow.keras.datasets import reuters
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import layer
import matplotlib.pyplot as plt
import numpy as np

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

def vectorize_sequences(sequences, dimension=10000):
results = np.zeros((len(sequences), dimension))
for i, serial in enumerate(sequences):
for j in series:
results[i, j] = 1.
return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

y_train = to_categorical(train_labels)
y_test = to_categorical(test_labels)

model = keras.Sequential([
layer.Dense(64,activation=&quot;relu&quot;),
layer.Dense(64,activation=&quot;relu&quot;),
layer.Dense(46,activation=&quot;softmax&quot;)
])

model.compile(
optimizer=&quot;rmsprop&quot;,
loss=&quot;categorical_crossentropy&quot;,
metrics=[&quot;accuracy&quot;]
)

# 留出验证集
x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = y_train[:1000]
partial_y_train = y_train[1000:]

# 训练模型

history = model.fit(
partial_x_train,
partial_y_train,
epochs=10,
batch_size=512,
validation_data=(x_val, y_val)
)

# 绘制训练图 &amp;验证准确率
history_dict = history.history
loss_values = history_dict[&quot;loss&quot;]
val_loss_values = history_dict[&quot;val_loss&quot;]
epochs = range(1, len(loss_values) + 1)
acc = history_dict[&quot;accuracy&quot;]
val_acc = history_dict[&quot;val_accuracy&quot;]
plt.plot(epochs, acc, &quot;bo&quot;, label=&quot;Training acc&quot;)
plt.plot(epochs, val_acc, &quot;b&quot;, label=&quot;Validation acc&quot;)
plt.xlabel(&quot;Epochs&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.legend()
plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/79185545/multiclass-classifier-using-keras-from-deep-learning-with-python-yields-very-d</guid>
      <pubDate>Wed, 13 Nov 2024 15:19:50 GMT</pubDate>
    </item>
    <item>
      <title>如何仅考虑直到 best_iteration 的迭代来计算 XGBoost 上的 feature_importances_？</title>
      <link>https://stackoverflow.com/questions/79184686/how-to-compute-feature-importances-on-xgboost-considering-only-iterations-up-to</link>
      <description><![CDATA[我训练了一个带有提前停止的 XGBRegressor 模型。据我所知，model.feature_importances_ 会根据所有历史记录计算特征重要性（即还考虑由 early_stopping_rounds 量化的“耐心”迭代）。尽管如此，我只需要在模型上计算出 best_iteration 之前的特征重要性。
这是一个示例代码：
from xgboost import XGBRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# 准备数据
X, y = make_regression(n_samples=1000, n_features=20, noise=0.1)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# 使用早期停止训练模型
xgb_model = XGBRegressor(n_estimators=1000, early_stopping_rounds=100, eval_metric=&quot;rmse&quot;)
xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
xgb_model.feature_importances_

但是这给了我考虑整个模型的特征重要性，而不仅仅是 best_iteration:
best_iteration = xgb_model.best_iteration

我无法使用 n_estimators=best_iteration 重新训练新的 XGBRegressor，因为它几乎会使运行时间翻倍（此代码段是更大代码的一部分）。
有没有办法在不重新训练的情况下实现这一点？
请注意，不幸的是，.feature_importances_ 没有 iteration_range 选项。]]></description>
      <guid>https://stackoverflow.com/questions/79184686/how-to-compute-feature-importances-on-xgboost-considering-only-iterations-up-to</guid>
      <pubDate>Wed, 13 Nov 2024 11:28:59 GMT</pubDate>
    </item>
    <item>
      <title>与 ChatGPT 的实时屏幕共享集成 [关闭]</title>
      <link>https://stackoverflow.com/questions/79184307/real-time-screen-sharing-integration-with-chatgpt</link>
      <description><![CDATA[我正在寻找一种与 ChatGPT 实时共享屏幕的方法，以便 AI 助手可以看到屏幕上发生的一切，并逐步指导我完成特定任务。
理想情况下，我正在寻找具有以下功能的集成或软件解决方案：
直接在 ChatGPT 内或以最少的设置进行实时屏幕共享，以便 ChatGPT 可以响应我屏幕上的内容。
安全稳定的连接，以确保数据隐私和安全。
如果可能的话，可以选择聊天或音频通信以提高互动性。
有没有人知道这样的解决方案或对如何使用 ChatGPT 实现这一点有建议？我很感激任何建议或其他想法！
提前谢谢！
我一直尝试截取屏幕截图并将其放入 GPT，但这非常耗时。]]></description>
      <guid>https://stackoverflow.com/questions/79184307/real-time-screen-sharing-integration-with-chatgpt</guid>
      <pubDate>Wed, 13 Nov 2024 09:50:49 GMT</pubDate>
    </item>
    <item>
      <title>模型严重偏向少数群体，但准确率、召回率和精确率得分都非常好[关闭]</title>
      <link>https://stackoverflow.com/questions/79184102/model-extremely-biased-towards-the-minority-class-but-the-accuracy-recall-and-pr</link>
      <description><![CDATA[我正在使用 xgboost 分类器来预测下一次事件发生的时间。这是一个二元分类，0:1 的比例是 1.6:1
验证集和验证预测中的类分布相似，性能指标非常好，但是混淆矩阵显示几乎所有样本都被归入类 1（少数类），而不是 0（多数类），你们知道这是为什么吗？
我尝试处理 scale_pos_weight：

我首先将 scale_pos_weight 提高到 1.6，但它给了我更多类 1 偏斜的结果。
我将 scale_pos_weight 降低到 0.25。对于类别 0，我得到了真阳性结果，并且假阳性明显较少，但对于类别 1，我得到了相等的真阳性和假阳性。它们加起来占样本的约 55%，这意味着模型仍然偏向类别 0。
]]></description>
      <guid>https://stackoverflow.com/questions/79184102/model-extremely-biased-towards-the-minority-class-but-the-accuracy-recall-and-pr</guid>
      <pubDate>Wed, 13 Nov 2024 09:01:14 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python 通过决策路径复制随机森林回归预测？</title>
      <link>https://stackoverflow.com/questions/79183884/how-to-use-python-to-replicate-random-forest-regression-prediction-using-decisio</link>
      <description><![CDATA[我试图测试我是否理解了 RandomForestRegressor 在模型拟合后产生预测的方式。我使用加州住房示例来训练一个简单的模型并预测我的测试集中的第一个值。然后我使用 apply() 获取所有训练数据的节点和每个回归树中感兴趣的测试样本。预测结果为1.41307
from sklearn import datasets
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import numpy as np

X, y = datasets.fetch_california_housing(as_frame=True, return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

rf = RandomForestRegressor(random_state=0)
rf.fit(X_train, y_train)

training_nodes = rf.apply(X_train)
test_nodes = rf.apply(X_test.iloc[[0]])

rf.predict( X_test.iloc[[0]] )
array([1.41307])


要复制预测，我需要：1. 在所有回归树中找到我感兴趣的测试数据的节点 2. 在所有回归树中找到与我感兴趣的测试数据共享相同节点的所有训练数据。3. 对于每棵树，计算 y_train 的平均值并存储它们。4. 计算存储的平均 y_train 的平均值。我得到的结果是 1.4053，与 rf.predict 相比有很大差异。有人能告诉我我错在哪里吗，正确的步骤是什么？
matching_samples_per_tree = []

target_values_per_tree = []

for tree_idx in range(training_nodes.shape[1]):
# 查找训练样本与此树中的测试样本属于同一叶子的索引
matching_indices = np.where(training_nodes[:, tree_idx] == test_nodes[0][tree_idx])[0]

matching_samples_per_tree.append(matching_indices)

# 检索并存储这些匹配训练样本的目标值
target_values_per_tree.append(y_train.iloc[matching_indices].values)

means_of_each_array = []

for vals in target_values_per_tree:
mean_value = np.mean(vals)
means_of_each_array.append(mean_value)

np.mean(means_of_each_array) 
#1.405348333333333
]]></description>
      <guid>https://stackoverflow.com/questions/79183884/how-to-use-python-to-replicate-random-forest-regression-prediction-using-decisio</guid>
      <pubDate>Wed, 13 Nov 2024 07:55:53 GMT</pubDate>
    </item>
    <item>
      <title>为什么从直觉上讲，规范化有利于神经网络的学习？[关闭]</title>
      <link>https://stackoverflow.com/questions/79183566/why-normalisation-is-good-for-learning-in-neural-netwrok-in-intutive-sense</link>
      <description><![CDATA[我一直在想这个问题，当我们进行标准化时，我们会丢失信息的幅度部分，而只保留方向信息，那么为什么丢失这个幅度信息不会影响学习，梯度的幅度部分代表什么？]]></description>
      <guid>https://stackoverflow.com/questions/79183566/why-normalisation-is-good-for-learning-in-neural-netwrok-in-intutive-sense</guid>
      <pubDate>Wed, 13 Nov 2024 05:52:27 GMT</pubDate>
    </item>
    <item>
      <title>VSCode 安装 hugginface relik 库时出错</title>
      <link>https://stackoverflow.com/questions/79182549/vscode-install-error-for-the-hugginface-relik-library</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79182549/vscode-install-error-for-the-hugginface-relik-library</guid>
      <pubDate>Tue, 12 Nov 2024 19:53:20 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch + Ray Tune 报告 ImplicitFunc 太大，不知道哪个引用很大</title>
      <link>https://stackoverflow.com/questions/79181943/pytorch-ray-tune-reporting-implicitfunc-is-too-large-no-idea-which-reference</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79181943/pytorch-ray-tune-reporting-implicitfunc-is-too-large-no-idea-which-reference</guid>
      <pubDate>Tue, 12 Nov 2024 16:21:39 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 transformers 在本地加载模型</title>
      <link>https://stackoverflow.com/questions/79169173/fail-to-use-transformers-to-load-model-locally</link>
      <description><![CDATA[我已使用 transformers 函数将整个模型
下载到本地目录：/home/marcus/Desktop/project/OCR_transformer_practices/models/moondream2
代码如下：
from huggingface_hub import snap_download

# 指定模型 ID 和修订版本
model_id = &quot;vikhyatk/moondream2&quot;
revision = &quot;2024-08-26&quot;

# 指定要下载模型的目录
download_directory = &quot;/home/marcus/Desktop/project/OCR_transformer_practices/models/moondream2&quot; # 将其更改为您想要的路径

# 将模型文件下载到指定目录
local_model_path = snapping_download(repo_id=model_id, revision=revision, local_dir=download_directory)

模型保存在目录中：
当我使用以下代码通过 transformers 从本地目录加载模型时：
from PIL import Image
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path
import os

# 获取父目录
project_dir = Path(__file__).parent
model_folder_name = &#39;models/moondream2&#39;
model_dir = str(project_dir/model_folder_name)

# 使用正确的模型 ID 加载 tokenizer 和模型
# model_id = &quot;vikhyatk/moondream2&quot;
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained( pretrained_model_name_or_path=model_dir, use_safetensors=True, trust_remote_code=True,)

弹出错误消息：
回溯（最近一次调用最后一次）：
文件“/home/marcus/Desktop/project/OCR_transformer_practices/moondream_test.py”，第 15 行，位于&lt;module&gt;
model = AutoModelForCausalLM.from_pretrained( pretrained_model_name_or_path=model_dir, use_safetensors=True, trust_remote_code=True,)
文件 &quot;/home/marcus/Desktop/project/OCR_transformer_practices/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py&quot;，第 553 行，在 from_pretrained 中
model_class = get_class_from_dynamic_module(
文件 &quot;/home/marcus/Desktop/project/OCR_transformer_practices/.venv/lib/python3.10/site-packages/transformers/dynamic_module_utils.py&quot;，第 552 行，在 get_class_from_dynamic_module 中
return get_class_in_module(class_name, final_module, force_reload=force_download)
文件 &quot;/home/marcus/Desktop/project/OCR_transformer_practices/.venv/lib/python3.10/site-packages/transformers/dynamic_module_utils.py&quot;，第 237 行，在 get_class_in_module 中
module_files: List[Path] = [module_file] + sorted(map(Path, get_relative_import_files(module_file)))
文件 &quot;/home/marcus/Desktop/project/OCR_transformer_practices/.venv/lib/python3.10/site-packages/transformers/dynamic_module_utils.py&quot;，第 128 行，在 get_relative_import_files 中
new_imports.extend(get_relative_imports(f))
文件&quot;/home/marcus/Desktop/project/OCR_transformer_practices/.venv/lib/python3.10/site-packages/transformers/dynamic_module_utils.py&quot;, line 97, in get_relative_imports
with open(module_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
FileNotFoundError: [Errno 2] 没有这样的文件或目录：&#39;/home/marcus/.cache/huggingface/modules/transformers_modules/moondream2/fourier_features.py&#39;

如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/79169173/fail-to-use-transformers-to-load-model-locally</guid>
      <pubDate>Fri, 08 Nov 2024 07:38:50 GMT</pubDate>
    </item>
    <item>
      <title>尝试保存自定义 Keras 模型时出现“TypeError：不支持的整数大小 (0)”</title>
      <link>https://stackoverflow.com/questions/79032646/typeerror-unsupported-integer-size-0-when-attempted-to-save-custom-keras-mo</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79032646/typeerror-unsupported-integer-size-0-when-attempted-to-save-custom-keras-mo</guid>
      <pubDate>Fri, 27 Sep 2024 19:14:51 GMT</pubDate>
    </item>
    </channel>
</rss>