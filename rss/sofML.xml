<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 20 Dec 2023 18:16:58 GMT</lastBuildDate>
    <item>
      <title>我正在尝试在 python 3.8 中下载并安装tensorflow</title>
      <link>https://stackoverflow.com/questions/77693569/im-trying-to-dowloand-install-tensorflow-in-python-3-8</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77693569/im-trying-to-dowloand-install-tensorflow-in-python-3-8</guid>
      <pubDate>Wed, 20 Dec 2023 17:52:57 GMT</pubDate>
    </item>
    <item>
      <title>如何创建一个curl req来创建具有谷歌云存储管道规范的顶点管道？</title>
      <link>https://stackoverflow.com/questions/77692960/how-to-create-a-curl-req-to-create-a-vertex-pipeline-with-a-google-cloud-storage</link>
      <description><![CDATA[我正在关注此文档：
我有以下curl请求（更改了一些命名以不公开信息）：
&lt;前&gt;&lt;代码&gt;curl -X POST \
    -H“授权：承载$(gcloud auth print-access-token)” \
    -H“内容类型：application/json；字符集=utf-8” \
    -d&#39;{
        &quot;display_name&quot;:&quot;训练&quot;,
        “最大并发运行计数”：10，
        “创建管道作业请求”：{
                “父”：“项目/项目名称/位置/us-west2”，
                “管道作业”：{
                    “显示名称”：“训练”，
                    “pipelineSpec”：“gs://bucket_name/artifacts/pipeline.yaml”，
                }
            }
    }&#39; \
    “https://us-west2-aiplatform.googleapis.com/v1/projects/project_name/locations/us-west2/schedules”

我收到错误：
&lt;前&gt;&lt;代码&gt;{
  “错误”：{
    “代码”：400，
    “message”：““schedule.create_pipeline_job_request.pipeline_job.pipeline_spec”(type.googleapis.com/google.protobuf.Struct) 的值无效，\“gs://project_name/artifacts/pipeline.yaml\” ”，
    “状态”：“INVALID_ARGUMENT”，
    “详细信息”：[
      {
        “@type”：“type.googleapis.com/google.rpc.BadRequest”，
        “字段违规”：[
          {
            “字段”：“schedule.create_pipeline_job_request.pipeline_job.pipeline_spec”，
            “description”：““schedule.create_pipeline_job_request.pipeline_job.pipeline_spec”(type.googleapis.com/google.protobuf.Struct) 的值无效，\“gs://project_name/artifacts/pipeline.yaml\” ”
          }
        ]
      }
    ]
  }
}

看起来curl请求必须有一个以JSON格式指定的管道规范。是否可以在curl请求中指定GCS路径？
我的用例是使用 Cloud Scheduler 创建每周运行的管道，从 GCS 文件中提取。我不想使用内置的重复管道创建，因为如果管道规范发生更改，则需要再次创建它。]]></description>
      <guid>https://stackoverflow.com/questions/77692960/how-to-create-a-curl-req-to-create-a-vertex-pipeline-with-a-google-cloud-storage</guid>
      <pubDate>Wed, 20 Dec 2023 16:04:46 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TIF 文件 (.tif) 上执行 CNN？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77692904/how-to-perform-cnn-on-tif-file-tif</link>
      <description><![CDATA[我尝试使用 Python 编程语言中的 Tensorflow 库访问 TIF 文件 (.tif)，但出现以下错误：

您能否向我提供使用 TIF 图像文件实现 CNN 的初始步骤？]]></description>
      <guid>https://stackoverflow.com/questions/77692904/how-to-perform-cnn-on-tif-file-tif</guid>
      <pubDate>Wed, 20 Dec 2023 15:56:31 GMT</pubDate>
    </item>
    <item>
      <title>典型相关分析[关闭]</title>
      <link>https://stackoverflow.com/questions/77692744/canonical-correlation-analysis</link>
      <description><![CDATA[我正在学习统计学课程，今天我的老师问了我一个问题，要求我提出规范相关分析的替代方案。她希望我开始寻找其他解决方案，但我什至不知道从哪里开始。
我尝试在互联网上查找不同的解决方案并滚动浏览 SciKit Learn 文档，但没有找到任何有用的东西。我希望有人在这方面比我有更多的知识。那么有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77692744/canonical-correlation-analysis</guid>
      <pubDate>Wed, 20 Dec 2023 15:30:38 GMT</pubDate>
    </item>
    <item>
      <title>Python，分层抽样</title>
      <link>https://stackoverflow.com/questions/77692098/python-stratified-sampling</link>
      <description><![CDATA[我正在使用规范建模框架。
我有 791 个受试者的样本。我有关于年龄、性别和站点（站点 1 或 2）的数据。我想使用 Python 提取大约 50% 的受试者子样本，涵盖整个年龄和性别范围。我的子样本应反映原始样本的位点 1：位点 2 比例。
我尝试使用sklearn train_test_split，但有些分层 2.]]></description>
      <guid>https://stackoverflow.com/questions/77692098/python-stratified-sampling</guid>
      <pubDate>Wed, 20 Dec 2023 13:54:53 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch中的动态量化量化后开始随机训练</title>
      <link>https://stackoverflow.com/questions/77692089/dynamic-quantization-in-pytorch-starts-random-training-after-quantization</link>
      <description><![CDATA[当我运行以下动态量化代码时，它开始使用一些随机自然图像进行 100 个时期的训练，我不想再次进行训练。我有预训练的权重，我只是想量化我的预训练的权重以减少推理时间：
从 ultralytics 导入 YOLO
进口火炬
导入火炬.量化

模型=YOLO(&#39;pre_trained_weights.pt&#39;)

model.load_state_dict(torch.load(&#39;checkpoint.pth&#39;)) #不知道这一步是否必要

qmodel = torch.quantization.quantize_dynamic(模型, dtype = torch.quint8)

我尝试了上面的代码，我希望我只是想量化我的预训练权重以减少推理时间]]></description>
      <guid>https://stackoverflow.com/questions/77692089/dynamic-quantization-in-pytorch-starts-random-training-after-quantization</guid>
      <pubDate>Wed, 20 Dec 2023 13:53:49 GMT</pubDate>
    </item>
    <item>
      <title>_pickle.UnpicklingError：无效的加载键，“v”</title>
      <link>https://stackoverflow.com/questions/77691997/pickle-unpicklingerror-invalid-load-key-v</link>
      <description><![CDATA[最近在 netify 应用上部署我的项目时遇到此错误
6:39:50 PM：4秒内添加了329个包，审核了330个包
6:39:50 PM：107 个包裹正在寻求资金
6:39:50 PM：运行“npmfund”了解详细信息
6:39:50 PM：1 个中等严重程度的漏洞
6:39:50 PM：要解决所有问题，请运行：
6:39:50 PM：npm 审核修复
6:39:50 PM：运行“npmaudit”以获取详细信息。
下午 6:39:50：&gt; movie@0.0.0 构建
下午 6:39:50：&gt;维特构建
6:39:50 PM：vite v5.0.2 构建生产...
6:39:50 PM：转变...
6:39:52 PM：✓ 52 个模块已完成转换。
6:39:52 PM：渲染块...
6:39:52 PM：计算 gzip 大小...
6:39:52 PM: dist/index.html 0.53 kB │ gzip: 0.33 kB
6:39:52 PM：dist/assets/back-dm-GU_Ur.jpg 514.14 kB
6:39:52 PM: dist/assets/index-CLk4a_k1.css 12.90 kB │ gzip: 3.41 kB
6:39:52 PM: dist/assets/index-3jaT-T7k.js 209.08 kB │ gzip: 68.51 kB
6:39:52 PM：✓ 内置 1.60 秒
6:39:52 PM：加载数据--------------------
6:39:52 PM：回溯（最近一次调用）：
6:39:52 PM：文件“app.py”，第 12 行，在  中
6:39:52 PM：电影 = load_data.load_movies()
6:39:52 PM：文件“/opt/build/repo/utils/load_data.py”，第 7 行，在 load_movies 中
6:39:52 PM：返回 pkl.load(open(&quot;./data/movies.pkl&quot;,&quot;rb&quot;))
6:39:52 PM：_pickle.UnpicklingError：无效的加载密钥，“v”。
下午 6:39:52：​
6:39:52 PM：“build.command”失败的
6:39:52 下午：────────────────────────────────────────────── ──────────────────────
下午 6:39:52：​
6:39:52 PM：错误消息
6:39:52 PM：命令失败，退出代码 1：cd client &amp;&amp; npm 安装&amp;&amp; npm run build &amp;&amp; cd ..&amp;&amp; python app.py (https://ntl.fyi/exit-code-1)
下午 6:39:52：​
6:39:52 PM：位置错误
6:39:52 PM：在 Netlify 应用程序的构建命令中：
6:39:52 PM：cd 客户端 &amp;&amp; npm 安装&amp;&amp; npm run build &amp;&amp; cd ..&amp;&amp;蟒蛇应用程序.py
下午 6:39:52：​
6:39:52 PM：已解决的配置
6:39:52 PM：构建：
6:39:52 PM：基础：/opt/build/repo
6:39:52 PM：命令：cd client &amp;&amp; npm 安装 &amp;&amp; npm run build &amp;&amp; cd ..&amp;&amp;蟒蛇应用程序.py
6:39:52 PM：命令来源：ui
6:39:52 PM：发布：/opt/build/repo
6:39:52 PM：发布来源：默认
6:39:53 PM：由于用户错误，构建失败：构建脚本返回非零退出代码：2
6:39:53 PM：构建失败：无法构建网站
6:39:53 PM：在 24.831 秒内完成处理构建请求
6:39:53 PM：“构建站点”阶段失败：构建脚本返回非零退出代码：2

构建命令
pip install -r requests.txt ；光盘客户端； npm 安装； npm 运行构建；光盘 .. ; python app.py
主要目标
在后端（flask）中使用 pkl 模型来预测前端用户的类似电影（react）
我已经使用 git lfs 在 github 上部署了 pkl 文件
主要目标
在后端（flask）中使用 pkl 模型来预测前端用户的类似电影（react）
我已经使用 git lfs 在 github 上部署了 pkl 文件]]></description>
      <guid>https://stackoverflow.com/questions/77691997/pickle-unpicklingerror-invalid-load-key-v</guid>
      <pubDate>Wed, 20 Dec 2023 13:38:05 GMT</pubDate>
    </item>
    <item>
      <title>端到端 ML 项目上的模型训练器问题 - ValueError：至少需要一个数组或数据类型</title>
      <link>https://stackoverflow.com/questions/77691153/model-trainer-issue-on-end-to-end-ml-project-valueerror-at-least-one-array-or</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77691153/model-trainer-issue-on-end-to-end-ml-project-valueerror-at-least-one-array-or</guid>
      <pubDate>Wed, 20 Dec 2023 11:18:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyG 时视频数据集的过度拟合问题</title>
      <link>https://stackoverflow.com/questions/77690723/overfitting-issue-with-video-dataset-while-using-pyg</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77690723/overfitting-issue-with-video-dataset-while-using-pyg</guid>
      <pubDate>Wed, 20 Dec 2023 10:11:21 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 MALLET Java API 运行 DMR 主题模型？</title>
      <link>https://stackoverflow.com/questions/77689759/how-can-i-run-dmr-topic-model-using-mallet-java-api</link>
      <description><![CDATA[我有两个数据集text.txt和另一个metadata.txt。我将数据格式化为每行一个文档。我想将 MALLET Java Api 用于 DMR 主题模型。但是，我无法这样做。我对 Java 和这种复杂的建模完全陌生。另外，我对主题建模没有任何先验知识。我在这里寻找专业知识。你能帮我解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77689759/how-can-i-run-dmr-topic-model-using-mallet-java-api</guid>
      <pubDate>Wed, 20 Dec 2023 07:05:09 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何在机器学习中正确使用交叉验证技术（cross_val_score）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77689532/how-should-i-use-cross-validation-technique-cross-val-score-in-machine-learnin</link>
      <description><![CDATA[我很难理解应该如何正确使用交叉验证技术（在本例中为cross_val_score）。
我正在研究机器学习回归问题。所有预处理步骤（插补、变换、缩放、编码等）均已完成。现在我需要决定使用哪种机器学习回归算法。
数据由 2 个文件组成：

train.csv - 独立特征 + 目标（1460 行）
test.csv - 仅独立特征（1459 行）

我的想法是使用 cross_val_score 选择最佳基本回归模型，然后在最佳基本模型上使用 GridSearchCV 来为其找到最佳超参数。
1) 如果我想使用 cross_val_scrore，我应该使用 train_test_split 分割 train.csv 数据并使用 X_train 和 y_train （1168 行）还是可以使用 X_train_full 和 y_train_full （1460 行）？&lt; /p&gt;
train_df = pd.read_csv(&#39;train.csv&#39;)

X_train_full = train_df.drop(&#39;目标&#39;), 轴 = 1
y_train_full = train_df[&#39;目标&#39;]

X_train，X_holdout，y_train，y_holdout = train_test_split（X_train_full，y_train_full，test_size = 0.2，random_state = 22）

2) 下面是我用来决定需要使用哪种基本回归机器学习模型的代码。我使用了 X_train 和 y_train，认为我应该以某种方式使用 X_holdout 和 y_holdout 来验证/确认决定，但我不知道这是否是正确的方法以及在这种情况下下一步该怎么做。
base_models = [

    (“线性回归”, LinearRegression()),
    (“岭回归”，Ridge())，
    (“套索回归”，Lasso())，
    （“弹性网络回归”，ElasticNet()），
    (“决策树”，DecisionTreeRegressor())，
    (“KNN 回归器”，KNeighborsRegressor())，
    (“SVR”，SVR())，
    （“随机森林”，RandomForestRegressor（）），
    (“额外树回归器”，ExtraTreesRegressor())，
    (“AdaBoost回归器”，AdaBoostRegressor())，
    (“梯度增强回归器”，GradientBoostingRegressor())，
    (“XGBoost”, XGBRegressor()),
    (“LightGBM”，LGBMRegressor())，
    (“CatBoost”，CatBoostRegressor(loss_function=&#39;RMSE&#39;，logging_level=&#39;Silent&#39;))

    ]

# 初始化列表来存储数据

基本模型名称 = []
base_model_rmse_scores = []

# 初始化各个交叉验证折叠分数的列表

cv_fold_scores = [[] for _ in range(10)] # 假设折叠 10 次

# 迭代模型并附加到列表

对于名称，base_models 中的模型：

    # 获取每次折叠的交叉验证分数

    分数 = np.sqrt(-cross_val_score(模型, X_train, y_train, 评分=“neg_mean_squared_error”, cv=10, n_jobs = -1))

    # 附加模型名称

    base_model_names.append(名称)

    # 添加跨折叠的均方根误差 (RMSE)

    base_model_rmse_scores.append(np.mean(分数))

    # 附加单独的折叠分数

    对于 i，枚举中的分数（分数）：
        cv_fold_scores[i].append(分数)

    # 从列表创建 DataFrame

    base_models_results_df = pd.DataFrame({“模型”: base_model_names, “Mean_RMSE”: base_model_rmse_scores})

    # 添加各个交叉验证折叠分数的列

    对于我，枚举中的fold_scores（cv_fold_scores）：
        base_models_results_df[f&quot;Fold_{i+1}_RMSE&quot;] = Fold_scores

    # 设置“模型”列作为索引

    base_models_results_df = base_models_results_df.set_index(“模型”)

    # 按均方根误差 (RMSE) 升序对 DataFrame 进行排序

    base_models_results_df = base_models_results_df.sort_values（by=“Mean_RMSE”，升序=True）

    基础模型结果df


从上图可以看出，CatBoost 是最好的基础模型，其平均 RMSE 为 0.319293。
3) 我可以做些什么来确保这个特定模型最适合使用吗？是否会过度拟合或不足？我可以使用 X_holdout 和 y_holdout 集以某种方式验证它吗？如果我需要使用 X_train_full 和 y_train_full，这种情况下该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/77689532/how-should-i-use-cross-validation-technique-cross-val-score-in-machine-learnin</guid>
      <pubDate>Wed, 20 Dec 2023 06:11:25 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在安装requirements.txt时收到此错误？</title>
      <link>https://stackoverflow.com/questions/77689351/why-am-i-getting-this-error-while-installing-requirements-txt</link>
      <description><![CDATA[PS C:\Users\SAI BHUVAN\Documents\EndToEndMLProject&gt; pip install -r 要求.txt
获取文件:///C:/Users/SAI%20BHUVAN/Documents/EndToEndMLProject（来自-rrequirements.txt（第10行））
  安装构建依赖项...完成
  检查构建后端是否支持 build_editable ...完成
  获取构建可编辑的需求...完成
  准备可编辑元数据 (pyproject.toml) ...错误
  错误：子进程退出并出现错误

  × 准备可编辑元数据 (pyproject.toml) 未成功运行。
  │ 退出代码：1
  ╰─&gt; [21行输出]
      回溯（最近一次调用最后一次）：
        文件“C:\Users\SAI BHUVAN\AppData\Local\Programs\Python\Python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，第 353 行，在  中
          主要的（）
        文件“C:\Users\SAI BHUVAN\AppData\Local\Programs\Python\Python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，第 335 行，在 main 中
          json_out[&#39;return_val&#39;] = hook(**hook_input[&#39;kwargs&#39;])
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        文件“C：\ Users \ SAI BHUVAN \ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ pip \ _vendor \ pyproject_hooks \ _in_process \ _in_process.py”，第181行，在prepare_metadata_for_build_editable中
          返回钩子（元数据目录，配置设置）
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        文件“C：\ Users \ SAI BHUVAN \ AppData \ Local \ Temp \ pip-build-env-fmoiyisg \ overlay \ Lib \ site-packages \ setuptools \ build_meta.py”，第446行，在prepare_metadata_for_build_editable中
          返回 self.prepare_metadata_for_build_wheel(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        文件“C:\Users\SAI BHUVAN\AppData\Local\Temp\pip-build-env-fmoiyisg\overlay\Lib\site-packages\setuptools\build_meta.py”，第368行，在prepare_metadata_for_build_wheel中
          self._bubble_up_info_directory(metadata_directory, “.egg-info”)
        文件“C:\Users\SAI BHUVAN\AppData\Local\Temp\pip-build-env-fmoiyisg\overlay\Lib\site-packages\setuptools\build_meta.py”，第 337 行，位于 _bubble_up_info_directory 中
          info_dir = self._find_info_directory(元数据目录, 后缀)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^
        文件“C:\Users\SAI BHUVAN\AppData\Local\Temp\pip-build-env-fmoiyisg\overlay\Lib\site-packages\setuptools\build_meta.py”，第 348 行，位于 _find_info_directory 中
          assert len(candidates) == 1, f“找到多个 {suffix} 目录”
                 ^^^^^^^^^^^^^^^^^^^^^^
      AssertionError：找到多个 .egg-info 目录
      [输出结束]

  注意：此错误源自子进程，并且可能不是 pip 的问题。
错误：元数据生成失败

× 生成包元数据时遇到错误。
╰─&gt;请参阅上面的输出。

注意：这是上面提到的包的问题，​​而不是 pip 的问题。
提示：详细信息请参见上文。

我试图解决这个问题，但无法解决。]]></description>
      <guid>https://stackoverflow.com/questions/77689351/why-am-i-getting-this-error-while-installing-requirements-txt</guid>
      <pubDate>Wed, 20 Dec 2023 05:19:33 GMT</pubDate>
    </item>
    <item>
      <title>找不到满足张量流要求的版本（来自版本：无）没有找到张量流的匹配分布</title>
      <link>https://stackoverflow.com/questions/77689277/could-not-find-a-version-that-satisfies-the-requirement-tensorflow-from-version</link>
      <description><![CDATA[即使我已经升级了我的 python 及其 64 位，我仍然遇到这个问题。
错误：找不到满足张量流要求的版本（来自版本：无）
错误：找不到张量流的匹配分布

我的Python是最新的（3.12.1）。
我已尝试更新 Python，但仍然遇到有关 Tensorflow 安装的问题。如何解决这个问题？我什至安装了 C++ vc_redist.x64.exe]]></description>
      <guid>https://stackoverflow.com/questions/77689277/could-not-find-a-version-that-satisfies-the-requirement-tensorflow-from-version</guid>
      <pubDate>Wed, 20 Dec 2023 04:55:01 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Hubert 模型获得音频嵌入</title>
      <link>https://stackoverflow.com/questions/77685045/how-to-get-audio-embeddings-using-hubert-model</link>
      <description><![CDATA[示例代码
导入火炬
从变压器导入 Wav2Vec2Processor、HubertForCTC
从数据集导入load_dataset

处理器 = Wav2Vec2Processor.from_pretrained(“facebook/hubert-large-ls960-ft”)
模型 = HubertForCTC.from_pretrained(“facebook/hubert-large-ls960-ft”)
input_values = 处理器(&#39;来自音频文件的数组。, return_tensors=“pt”).input_values

之后如何获得嵌入？模型中没有最后的隐藏状态。
尝试了我提到的代码块。此外，尝试创建没有最后一层的模型，然后将其输入。我的另一个问题是，假设我有不同时间维度的剪辑，那么如何创建固定嵌入？沿着时间轴平均可以吗？或者需要不同的方法。]]></description>
      <guid>https://stackoverflow.com/questions/77685045/how-to-get-audio-embeddings-using-hubert-model</guid>
      <pubDate>Tue, 19 Dec 2023 12:04:19 GMT</pubDate>
    </item>
    <item>
      <title>HuggingFace AutoModelForCasualLM “仅解码器架构”警告，即使在设置 padding_side='left' 后也是如此</title>
      <link>https://stackoverflow.com/questions/74748116/huggingface-automodelforcasuallm-decoder-only-architecture-warning-even-after</link>
      <description><![CDATA[我正在使用
AutoModelForCausalLM 和 AutoTokenizer 使用 DialoGPT 生成文本输出。
无论出于何种原因，即使使用 Huggingface 提供的示例，我也会收到此警告：
&lt;块引用&gt;
正在使用仅解码器架构，但检测到右填充！为了正确的生成结果，请在初始化分词器时设置 padding_side=&#39;left&#39;。

从变压器导入 AutoModelForCausalLM, AutoTokenizer
进口火炬


tokenizer = AutoTokenizer.from_pretrained(“microsoft/DialoGPT-medium”)
模型 = AutoModelForCausalLM.from_pretrained(“microsoft/DialoGPT-medium”)

# 我们聊5行吧
对于范围（5）中的步骤：
    # 对新的用户输入进行编码，添加 eos_token 并在 Pytorch 中返回一个张量
    new_user_input_ids = tokenizer.encode(input(“&gt;&gt;用户:”) + tokenizer.eos_token, return_tensors=&#39;pt&#39;)

    # 将新的用户输入标记附加到聊天历史记录中
    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) 如果步骤 &gt; 0 其他 new_user_input_ids

    # 生成响应，同时将总聊天历史记录限制为 1000 个令牌，
    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)

    # 漂亮地打印机器人最后的输出令牌
    print(“DialoGPT: {}”.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0],skip_special_tokens=True)))

代码由 微软在 Huggingface 的模型卡上
我尝试将 padding_side=&#39;left&#39; 添加到标记生成器，但这不会改变任何内容。
显然（从一些阅读来看）DialoGPT 无论如何都希望在右侧填充？
我无法弄清楚这一点，当我尝试谷歌搜索时几乎没有结果。
我能够像这样抑制警告：
from Transformers.utils 导入日志记录

记录.set_verbosity_info()

但这似乎不是最好的答案？]]></description>
      <guid>https://stackoverflow.com/questions/74748116/huggingface-automodelforcasuallm-decoder-only-architecture-warning-even-after</guid>
      <pubDate>Fri, 09 Dec 2022 20:39:39 GMT</pubDate>
    </item>
    </channel>
</rss>