<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 06 May 2024 21:13:41 GMT</lastBuildDate>
    <item>
      <title>RNN/LSTM 损失函数</title>
      <link>https://stackoverflow.com/questions/78438859/rnn-lstm-loss-function</link>
      <description><![CDATA[我正在尝试生成一个 RNN，它能够在给定按时间顺序排列的输入列表的情况下随着时间的推移预测某些值。问题是这些值太随机了，我无法用 Keras 中的损失函数来拟合这些值。新的想法是尝试生成基于快速傅里叶变换的损失函数。这可能吗？我不知道如何生成在模型中使用的自定义损失函数。
我留下了我测试过的一次测试运行的图表，在本例中我使用 MSE 作为函数，结果很清楚，但您也可以看到最后 200 个值的随机性​​我为了测试而分开的
使用最后 200 个数字进行测试
使用 FFT 生成损失函数是否可行？您还可以建议什么来获得更精确的结果？]]></description>
      <guid>https://stackoverflow.com/questions/78438859/rnn-lstm-loss-function</guid>
      <pubDate>Mon, 06 May 2024 19:57:04 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中 2D 输入的集成梯度实现</title>
      <link>https://stackoverflow.com/questions/78438413/integrated-gradients-implementation-for-2d-input-in-pytorch</link>
      <description><![CDATA[我正在尝试为 GNN 实现积分梯度计算（在文章中描述）我正在与.具体来说，我使用论文中的Eq3
我的网络输入是一个 NxM 矩阵，表示 N 个顶点的图，每个顶点都有一个 M 维的特征向量。我对论文中的方法的范围感到非常困惑，考虑到他们是针对 N 维的输入推导出来的，在我的例子中，我想为每个节点获得一个分数。
在我当前的实现中，我获得了一个 NxM 矩阵，通过执行 torch.sum(dim=1) 来折叠该矩阵，但这感觉并不那么干净
这是我当前在 PyTroch 中的实现
graph_copy = input_graph.detach().clone()
# 公式中的k/m
k_m = torch.linspace(0, 1, n_steps)
# 存储样本的数组
input_features = [baseline.clone()] # 第一个样本是基线
# x-x&#39;
diff = 原始输入特征 - 基线

# 填充样本数组
对于范围内的 i(1, k_m.shape[0])：
    temp = 基线 + k_m[i] * diff
    input_features_path.append（临时）

梯度= []
对于范围内的 i(k_m.shape[0])：
    ### 将每个输入的向量梯度归零
    input_features[i].requires_grad = True
    model.zero_grad()
    任务.zero_grad()
    
    temp_model_out = 模型（图 = graph_copy，输入 = input_features[i]）[&#39;graph_feature&#39;][0]
    temp_mlp_output = 任务.mlp(temp_model_out)
    temp_prob = Softmax(temp_mlp_output, 暗淡 = 0)

    temp_gradient = grad(输出 = temp_prob[true_label_id], 输入 = input_features_path[i])
    梯度.append(temp_gradient[0])

    input_features_path[i].requires_grad = False

使用 torch.no_grad()：
    ig_scores = original_input_feature * torch.stack(梯度, 暗淡 = 2). 平均值(暗淡 = 2)
    Final_ig_scores = ig_scores.sum(dim = 1)
    排序，索引= torch.sort（final_ig_scores，降序= True）


感谢您为解决我的困惑提供的任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78438413/integrated-gradients-implementation-for-2d-input-in-pytorch</guid>
      <pubDate>Mon, 06 May 2024 18:08:43 GMT</pubDate>
    </item>
    <item>
      <title>有没有更好的方法使用 python 解析非结构化 Excel 表？</title>
      <link>https://stackoverflow.com/questions/78438227/is-there-a-better-way-of-parsing-unstructured-excel-tables-using-python</link>
      <description><![CDATA[我正在尝试从 excel 表中提取一些表格信息。每个 excel 表中都有多个表格，并且 excel 表中的表格结构往往不同。我想出了一个仅适用于一张表的逻辑，但每个 excel 在文件中的结构都不同。有没有办法提取所有子表，无论结构如何？我正在寻找一种通用方法，而现有的库不适合这项任务。
]]></description>
      <guid>https://stackoverflow.com/questions/78438227/is-there-a-better-way-of-parsing-unstructured-excel-tables-using-python</guid>
      <pubDate>Mon, 06 May 2024 17:28:19 GMT</pubDate>
    </item>
    <item>
      <title>训练 BigGan 模型时的运行时错误和其他错误</title>
      <link>https://stackoverflow.com/questions/78437895/runtime-error-and-other-errors-while-training-biggan-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78437895/runtime-error-and-other-errors-while-training-biggan-model</guid>
      <pubDate>Mon, 06 May 2024 16:18:20 GMT</pubDate>
    </item>
    <item>
      <title>如何解决对象检测模型中的错误分类问题？</title>
      <link>https://stackoverflow.com/questions/78437775/how-to-address-misclassification-in-an-object-detection-model</link>
      <description><![CDATA[我正在训练一个对象检测模型，该模型可以识别一个类别，即松鼠。该模型倾向于认为大多数事物都是松鼠，例如鸟类、狗和人类。该模型没有针对人类、鸟类或狗的类，也没有接受过识别这些事物的训练。如何解决此问题以便错误分类不会持续存在？
我希望我的模型只识别松鼠]]></description>
      <guid>https://stackoverflow.com/questions/78437775/how-to-address-misclassification-in-an-object-detection-model</guid>
      <pubDate>Mon, 06 May 2024 15:53:08 GMT</pubDate>
    </item>
    <item>
      <title>索引错误：索引 42 超出尺寸 42 的轴 1 的范围</title>
      <link>https://stackoverflow.com/questions/78437580/indexerror-index42-out-of-bounds-for-axis-1-with-size-42</link>
      <description><![CDATA[我正在用 python 编码，我的代码似乎是错误的，我必须输入机器学习模型的数据才能基于它生成输出。我编写了这段代码，以便我可以尝试根据算法从私钥中检索钱包地址。我正在为我的业务做这个，但尚未完成，所以这只是测试代码。
错误是说indexerror：index42超出了尺寸为42的轴1的范围。我认为我的输入数据有问题。到目前为止我的代码是：
将 numpy 导入为 np
从张量流导入keras
从tensorflow.keras.layers导入LSTM，密集

# 根据您指定的连接定义您自己的输入输出对
输入输出对 = {
    &#39;0x7BB18376dC84cCFC4C4c9Fe8367Bf306a213C43C&#39;：&#39;00000000000000000000000000000000000000000000000000000000000000480&#39;，
    &#39;0xeB50d45213864761dEC218e2F0996B591F030007&#39;：&#39;00000000000000000000000000000000000000000000000000000000000000481&#39;，
    # 根据需要添加更多对
}

# 从输入和输出字符串中提取唯一字符
all_chars = set(&#39;&#39;.join(input_output_pairs.keys()) + &#39;&#39;.join(input_output_pairs.values()))

# 创建 char_indices 和indices_char 字典
char_indices = {char: i for i, char in enumerate(all_chars)}
index_char = {i: i 的 char，枚举中的 char(all_chars)}

# 输入和输出字符串的最大长度
max_len = max(len(input_str) 对于 input_output_pairs.keys() 中的 input_str)

# 标记化
# 标记化
x = np.zeros((len(input_output_pairs), max_len, len(all_chars)), dtype=np.bool_)
y = np.zeros((len(input_output_pairs), max_len, len(all_chars)), dtype=np.bool_)

对于 i，枚举（input_output_pairs.items（））中的（input_str，output_str）：
    对于 t，枚举中的字符（input_str）：
        x[i, t, char_indices[char]] = 1
    对于 t，枚举中的 char(output_str)：
        y[i, t, char_indices[char]] = 1

# 定义模型架构
模型 = keras.Sequential()
model.add(LSTM(128, input_shape=(max_len, len(all_chars)))) # 删除 return_sequences=True
model.add(Dense(64,activation=&#39;softmax&#39;)) # 调整单位以匹配所需的输出长度
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizationr=&#39;adam&#39;)

# 训练模型
model.fit(x,y,batch_size=32,epochs=100,validation_split=0.2)

# 使用模型根据新的输入字符串生成输出字符串

# 使用模型根据新的输入字符串生成输出字符串
    # 使用模型根据新的输入字符串生成输出字符串
def 生成输出（输入字符串）：
    x_pred = np.zeros((1, max_len, len(all_chars)))
    对于 t，枚举中的字符（input_string）：
        x_pred[0, t, char_indices[char]] = 1

    生成的字符串 = &#39;&#39;
    for _ in range(max_len): # 循环每个时间步长
        pred = model.predict(x_pred, verbose=0)[0]
        print(&quot;预测形状：&quot;, pred.shape)
        next_char = Index_char[np.argmax(pred)] # 预测下一个字符
        generated_string += next_char
        print(“到目前为止生成的字符串：”, generated_string)
        if next_char == &#39;#&#39;: # 字符串结束标记
            休息
        print(&quot;更新前输入形状：&quot;, x_pred.shape)
        x_pred[:, :-1, :] = x_pred[:, 1:, :] # 将输入移动一个时间步长
        x_pred[:, -1, :] = 0 # 清除最后一个时间步
        x_pred[0, -1, char_indices[next_char]] = 1 # 更新下一个时间步的输入
        print(&quot;更新后输入形状：&quot;, x_pred.shape)
    返回生成的字符串




# 用法示例
input_string = &#39;0xbF16A1161d087cf151475824C34dAd398F81B684&#39;
生成输出 = 生成输出（输入字符串）
print(“生成的输出：”, generated_output)

有人可以帮我解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78437580/indexerror-index42-out-of-bounds-for-axis-1-with-size-42</guid>
      <pubDate>Mon, 06 May 2024 15:18:22 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习从图像中提取系统发育树信息</title>
      <link>https://stackoverflow.com/questions/78437477/extracting-phylogenetic-tree-information-from-images-using-machine-learning</link>
      <description><![CDATA[有多种机器学习模型（Claude、chatGPT 等）可用于从图像中提取机器可读的信息。有没有人见过从已发表的系统发育树图像（互联网上有很多）成功提取 Newick 格式数据（或同等数据）的案例？]]></description>
      <guid>https://stackoverflow.com/questions/78437477/extracting-phylogenetic-tree-information-from-images-using-machine-learning</guid>
      <pubDate>Mon, 06 May 2024 15:00:15 GMT</pubDate>
    </item>
    <item>
      <title>关于用于食谱成分提取的 spaCy NER 模型注释的反馈</title>
      <link>https://stackoverflow.com/questions/78437443/feedback-on-spacy-ner-model-annotations-for-recipe-ingredient-extraction</link>
      <description><![CDATA[我目前正在训练一个 spaCy NER 模型，以专门识别和分类食谱中的配料。目标是从各种食谱中准确提取配料及其数量、单位和准备说明。以下是我如何注释数据的概述：
跨标签

数量：与单位相关的数字（例如，“2”、“3/4”）
成分：实际成分名称（例如，“糖”、“牛奶”）
测量单位：测量单位（例如，“杯”、“汤匙”）
说明：准备说明（例如，“切成小块”、“分开”）

关系标签

quantity_of：将跨度标签数量与成分相关联
action_to：将跨度标签说明与成分相关联
unit_of：将跨度标签测量单位与数量相关联

我提供了两个示例：一个简单的成分行和一个复杂的成分行。
简单行
复杂行
我正在寻求有关以下几点的反馈：

注释过度：是否存在类别太多或注释过于详细，不利于有效学习和实际应用？是否应该合并或省略某些类别？
训练数据量：在这种类型的 NER 任务中，通常建议使用多少注释数据来实现稳健的模型？我想确保模型在各种食谱格式中都是可靠的。

您可以分享的任何见解、经验或资源都将非常有帮助。谢谢！
我尝试过的方法：
我使用类似的标签设置训练了一个模型，减去关系标签和“测量单位”跨度标签。我使用来自随机食谱的大约 700 个样本训练了该模型。结果不尽如人意；该模型很难识别某些方面，特别是在数量及其单位不相邻的情况下（例如 3 瓣大蒜）]]></description>
      <guid>https://stackoverflow.com/questions/78437443/feedback-on-spacy-ner-model-annotations-for-recipe-ingredient-extraction</guid>
      <pubDate>Mon, 06 May 2024 14:52:57 GMT</pubDate>
    </item>
    <item>
      <title>如何为 2130 个类别的分类模型配置密集层维度？</title>
      <link>https://stackoverflow.com/questions/78436988/how-to-configure-dense-layers-dimension-for-a-classification-model-of-2130-class</link>
      <description><![CDATA[我正在使用 2130 进行汉字分类任务。设置密集层的最佳实践是什么？
&lt;前&gt;&lt;代码&gt;类数 = 2130
SResnetModel = 顺序（
    [
        重新缩放(1./255, input_shape=(224, 224, 3)),
        tflow.keras.applications.ResNet50(
            include_top=假，
            池化=&#39;平均&#39;,
        ),
        展平（），
        密集（2048，激活=&#39;relu&#39;），
        密集（类数，激活 = &#39;softmax&#39;）
    ]
）
SResnetModel.summary()

这是摘要。

我做了一些搜索，发现最好让倒数第二个密集层的尺寸大于类的数量，但我如何配置它。]]></description>
      <guid>https://stackoverflow.com/questions/78436988/how-to-configure-dense-layers-dimension-for-a-classification-model-of-2130-class</guid>
      <pubDate>Mon, 06 May 2024 13:33:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么这个模型在某些目标上表现良好，但在某些目标上表现不佳？</title>
      <link>https://stackoverflow.com/questions/78435762/why-is-this-model-peforming-on-some-targets-well-but-on-some-not</link>
      <description><![CDATA[我正在开发一个监督机器学习项目。我将 horse_data(size,weight,peformance,...) 作为输入，并将配方的成分作为输出。我想预测给定马数据的成分。
这是我的 horse_data 的摘要：HorseData
这是我的目标（秘诀）的摘要：
FirstPartTargets
第二部分目标
第三方目标
我想用这些数据训练一个机器学习模型。在本例中是随机森林回归器，因为输入有许多分类变量（保留、表现、工作类型、种族和种族类型）。
resDf = pd.DataFrame(columns=[&#39;训练 R^2 分数&#39;,&#39;测试 R^2 分数&#39;,&#39;训练 MSE&#39;,&#39;测试 MSE&#39;,&#39;训练 RMSE&#39;,&#39;测试 RMSE&#39; ,&#39;训练 MSAE&#39;,&#39;测试 MSAE&#39;])

参数网格 = {
    &#39;n_estimators&#39;: [100,200,1000],
    &#39;最大深度&#39;: [10,20,30],
    &#39;min_samples_split&#39;:[2,5,10],
    &#39;min_samples_leaf&#39;:[1,2,4]
}
对于 Y.columns 中的 ing：
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y[ing], test_size=0.2, random_state=52)

    gridModel = make_pipeline(GridSearchCV(估计器=RandomForestRegressor(),cv=10,param_grid=param_grid,n_jobs=-1,scoring=&#39;neg_mean_squared_error&#39;,verbose=True))
    gridModel.fit(X_train,Y_train)
    y_pred_train = gridModel.predict(X_train)
    
    train_mse_error =mean_squared_error(y_pred=y_pred_train,y_true=Y_train)
    train_mse_absoulte_error = Mean_absolute_error(y_pred=y_pred_train,y_true=Y_train)
    train_r2_score = r2_score(y_pred=y_pred_train,y_true=Y_train)
    train_rmse = train_mse_error ** (0.5)
    
    y_pred = gridModel.predict(X_test)
    test_mse_error =mean_squared_error(y_pred=y_pred,y_true=Y_test)
    test_mse_absoulte_error = Mean_absolute_error(y_pred=y_pred,y_true=Y_test)
    test_r2_score = r2_score(y_pred=y_pred,y_true=Y_test)
    test_rmse = test_mse_error ** (0.5)
    resDf.loc[ing] = [train_r2_score,test_r2_score,train_mse_error,test_mse_error,train_rmse,test_rmse,train_mse_absoulte_error,test_mse_absoulte_error]

结果如下：
FirstPartResult
第二部分结果
问题是有时我不明白结果。我的理解是我的模型过度拟合，因为在每一行中，测试集上的分数和错误都高于训练集上的分数和错误。但有些行我不明白。 VitaminA 在训练集上有很好的 r2 分数，但在测试集上很差（对我来说，这是过度拟合）。
但是训练集和测试集的 RMSE 都非常高。
同样令人困惑的是“schwefel”。它在训练集上的 r2score 很差，在测试集上的得分也很糟糕。但我在系统中看不到为什么会得到这些结果。
问题是特征还是目标有时范围很大？]]></description>
      <guid>https://stackoverflow.com/questions/78435762/why-is-this-model-peforming-on-some-targets-well-but-on-some-not</guid>
      <pubDate>Mon, 06 May 2024 09:34:58 GMT</pubDate>
    </item>
    <item>
      <title>指导法学硕士 - 从文本中错误地提取数据继续</title>
      <link>https://stackoverflow.com/questions/78435586/instruct-llms-extract-data-from-text-wrongly-continues</link>
      <description><![CDATA[我正在尝试微调开源 LLM，现在让我们继续使用 Mistral-7b-instruct 模型。
我的任务如下：我有电子邮件，代表“价格请求”对于我们的客户发送的货物。
客户在邮件中告诉我们取货地址、发货人、收货人等信息。
我最初的想法是使用 DORA 训练不同的适配器，每个适配器都接受从电子邮件中提取不同实体的训练。
我的数据集创建如下：我有电子邮件和注释“基于电子邮件，我找到了这个 [ENTITY]：entity_here”
我已经创建了一条系统消息，并使用 chat_template 以 Mistral 接受的方式创建数据集，使用此 chat_template：
“{%- for messages in messages %}”
  “{%- if message[&#39;角色&#39;] == &#39;系统&#39;-%}”
      ”{{- &#39;&#39; + 消息[&#39;内容&#39;] -}}”
  “{%-else-%}”
      “{%- if message[&#39;角色&#39;] == &#39;用户&#39;-%}”
          “{{-&#39;[INST]&#39; + message[&#39;content&#39;].rstrip() + &#39;[/INST]&#39;-}}”
      “{%-else-%}”
          ”{{-&#39;&#39; + 消息[&#39;内容&#39;] + &#39;&#39; -}}”
      “{%-endif-%}”
  “{%-endif-%}”
“{%-endfor-%}”
“{%- if add_ Generation_prompt -%}”
    “{{-&#39;&#39;-}}”
“{%-endif-%}”

现在解决问题了。该模型似乎了解了需要提取的内容，它生成了不错的答案，其格式与训练它的助手相同，问题是在生成答案后，它不断生成与电子邮件无关的附加文本到任务，E.G. “请联系我们......”
例如，当我针对同一任务微调 GPT3.5 时，模型能够准确提取我需要的内容，这表明我做错了什么。
有人对我哪里出错有建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78435586/instruct-llms-extract-data-from-text-wrongly-continues</guid>
      <pubDate>Mon, 06 May 2024 09:01:03 GMT</pubDate>
    </item>
    <item>
      <title>向 Python Streamlit 饮食推荐应用程序添加饮食偏好按钮 [关闭]</title>
      <link>https://stackoverflow.com/questions/78433479/adding-dietary-preference-button-to-python-streamlit-diet-recommendation-app</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78433479/adding-dietary-preference-button-to-python-streamlit-diet-recommendation-app</guid>
      <pubDate>Sun, 05 May 2024 19:19:19 GMT</pubDate>
    </item>
    <item>
      <title>如何导入 Gensim？ Pip Install 有效，但我无法在没有出现 ImportError: Cannot import name 'triu' 错误的情况下运行 Import Gensim</title>
      <link>https://stackoverflow.com/questions/78427675/how-do-i-import-gensim-pip-install-worked-but-i-cant-run-import-gensim-withou</link>
      <description><![CDATA[我正在尝试执行此 Word2Vec 代码并收到错误：
&lt;块引用&gt;
名称错误：名称“gensim”未定义`

w2v_model = gensim.models.Word2Vec（docgen，min_count = 5，sg = 1，seed = 22122，workers = 1）

当我导入 gensim 时，出现此错误：ImportError: Cannot import name &#39;triu&#39; from &#39;scipy.linalg&#39; (/opt/conda/lib/python3.9/site-packages/scipy/ linalg/__init__.py)
如何修复代码以便能够调用 Gensim？我需要 Word2Vec 才能运行，之前没有遇到过这个问题；我今天早上运行这个模型，一切都很好，但现在突然出现错误。
我看到你必须升级 Scipy，而且它似乎有效。这是完整的代码：
!pip install scipy==1.10.1
!pip 安装 gensim

来自 numpy import triu
导入gensim
从 gensim 导入语料库、模型

w2v_model = gensim.models.Word2Vec(docgen, min_count=3, sg=1, 种子=22122, 工人=1)
]]></description>
      <guid>https://stackoverflow.com/questions/78427675/how-do-i-import-gensim-pip-install-worked-but-i-cant-run-import-gensim-withou</guid>
      <pubDate>Sat, 04 May 2024 03:23:37 GMT</pubDate>
    </item>
    <item>
      <title>如何基于掩码相乘矩阵并排除元素？</title>
      <link>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</guid>
      <pubDate>Mon, 29 Apr 2024 19:07:12 GMT</pubDate>
    </item>
    <item>
      <title>预测新数据时保存的 GAMLSS 模型出现问题</title>
      <link>https://stackoverflow.com/questions/77837026/issue-with-saved-gamlss-model-while-predicting-for-new-data</link>
      <description><![CDATA[我有一个经过 GAMLSS 训练的模型，已使用 saveRDS() 以 .rda 格式保存。
例如，我将模型训练为：
gamlss_model&lt;- gamlss(res~pb(x)+pb(y), family=BCTo, data = test)

当我在清除所有环境变量后加载上述模型，并对新数据使用预测函数时：
预测（model_old，newdata = new_data）

我收到以下错误：
eval(Call$data) 中的错误：未找到对象“test”

但是这个测试是旧数据集，在这里应该没有任何意义。我无法理解这有什么问题。因此，我无法运行 REST API。
当我的所有环境变量在 GAMLSS 模型训练后都存在时，那么当我立即使用预测时，它就可以工作了！但我想稍后使用预测。]]></description>
      <guid>https://stackoverflow.com/questions/77837026/issue-with-saved-gamlss-model-while-predicting-for-new-data</guid>
      <pubDate>Thu, 18 Jan 2024 05:10:18 GMT</pubDate>
    </item>
    </channel>
</rss>