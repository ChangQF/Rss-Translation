<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 01 Jan 2025 01:22:04 GMT</lastBuildDate>
    <item>
      <title>使用 ResNet50 + BiLSTM 的识别模型的损失函数太高且准确率为 0.0000e+00</title>
      <link>https://stackoverflow.com/questions/79320980/loss-function-too-high-and-accuracy-0-0000e00-for-recognition-model-using-resne</link>
      <description><![CDATA[我正在使用 ResNet50 和 BiLSTM 的组合进行手写句子识别任务。我的数据集包括手写句子的图像，每个句子都表示为一个图像序列。但是，我在训练期间遇到了模型性能问题：
损失函数从 9.1130 开始，并且保持较高水平。
准确率始终为 0.0000e+00，即使在 20 个 epoch 之后也是如此。
图像存储在文件夹中，每个句子一个文件夹，每个句子有 10 个样本。
标签存储在带有句子 ID 及其对应文本标签的 CSV 文件中。
我使用 tensorflow。
# 数据集路径

dataset_path = &quot;/content/drive/My Drive/Images&quot;
csv_path = &quot;/content/drive/My Drive/Labels/lines-labels.csv&quot;

# 数据生成器：
# 我实现了一个自定义数据生成器来加载图像序列及其标签：

class DataGenerator(Sequence):
def __init__(self, dataset_path, labels_df, batch_size, seq_length, img_width, img_height):
# 初始化参数...

def __len__(self):
# 批次数...

def __getitem__(self, index):
# 加载一批序列...

该模型使用 ResNet50 基础进行特征提取，使用双向 LSTM 进行序列建模：
def build_resnet_bilstm_model(input_shape, num_classes):
resnet_base = ResNet50(weights=&#39;imagenet&#39;, include_top=False, input_shape=(224, 224, 3))

for layer in resnet_base.layers:
layer.trainable = False

input_layer = 输入（shape=input_shape）
time_distributed = TimeDistributed（resnet_base）（input_layer）
time_distributed = TimeDistributed（Flatten（））（time_distributed）

bilstm = Bidirectional（LSTM（256，return_sequences=False））（time_distributed）
dropout = Dropout（0.5）（bilstm）

output_layer = Dense（num_classes，activation=&#39;softmax&#39;）（dropout）

model = Model（inputs=input_layer，outputs=output_layer）
model.compile（optimizer=Adam（learning_rate=0.001），loss=&#39;categorical_crossentropy&#39;，metrics=[&#39;accuracy&#39;]）

返回模型

训练：
epochs：20，patience：5， min_lr:1e-6
什么原因可能导致此问题？是否与以下因素有关：

图像或标签的预处理不正确？
数据生成器或序列长度 (SEQ_LENGTH = 10) 存在问题？
模型架构，特别是 ResNet50 和 BiLSTM 的集成？

如果您能提供任何建议或见解以改进模型的训练行为，我将不胜感激。
模型架构]]></description>
      <guid>https://stackoverflow.com/questions/79320980/loss-function-too-high-and-accuracy-0-0000e00-for-recognition-model-using-resne</guid>
      <pubDate>Wed, 01 Jan 2025 00:37:11 GMT</pubDate>
    </item>
    <item>
      <title>自定义神经网络中的回归为所有输入提供相同的输出</title>
      <link>https://stackoverflow.com/questions/79320551/regression-in-custom-neural-network-giving-same-output-for-all-inputs</link>
      <description><![CDATA[我最近开始研究神经网络，在使用 tensorflow 或 pytorch 等库之前，我考虑编写自己的神经网络，以便深入了解网络内部发生的事情。我根据神经网络的数学知识开发了代码。现在网络出现了一些奇怪的行为。首先，网络有时表现非常好，例如使用 scikit learn 的 iris 数据集和一个小配置，我能够将网络的准确率提高到 99.3%。但有时它的表现太差了。此外，在一个问题中，我注意到使用 ReLU 激活函数作为第一层，然后所有其余的 sigmoid 给出了近 63% 的准确率，但是当我将第一层设为 sigmoid 和第二层设为 ReLU 并其余所有 sigmoid 时，准确率上升到 93%。现在的主要问题是回归，它似乎根本不起作用。该模型似乎对所有输入都输出相同的东西。
以下是网络的工作原理，它是单层的简单代码。当我们声明一个层时，它需要 4 个参数 —— 输入数量、神经元数量、激活函数、激活函数的微分。现在我们可以使用 .forward(inputs) 方法转发该层。对于反向传播，我们有 Backward() 方法，它采用 d（损失函数）/d（激活输出）和学习率。它还会自动返回 d（损失函数）/d（前一层的激活输出），可以将其放在前一层进行反向传播。此外，由于对于 softmax 激活，微分有点复杂，我作弊了一点，假设 softmax 激活函数只会在最后一层使用，因此对于具有 softmax 函数的神经层，我们不需要传递任何东西，只需学习率，其余的它会计算（因此你可以忽略 softmax_diff 函数，它没用）。有了所有这些上下文，下面是代码
import numpy as np

def ReLU(z):
return np.maximum(0,z)
def ReLU_diff(a):
return (a&gt;0).astype(int)
def sigmoid(z):
z = np.clip(z, -500, 500)
return 1 / (1 + np.exp(-z))
def sigmoid_diff(a):
return a * (1 - a)
def tanh(z):
return np.tanh(z)
def tanh_diff(a):
return 1 - a ** 2
def softmax(z):
z = z - np.max(z, axis=1, keepdims=True)
return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)
def softmax_diff(a):
return a * (1 - a)
def linear(z):
return z
def linear_diff(a):
return np.ones(a.shape)

class Layer():
def __init__(self, numberOfInputs,numberOfNeurons,activationFunction,activationFunctionDiff):
self.numberOfInputs = numberOfInputs
self.numberOfNeurons = numberOfNeurons
self.activationFunction =activationFunction
self.activationFunctionDiff =activationFunctionDiff
self.weights = np.random.randn(numberOfNeurons, numberOfInputs) * np.sqrt(2.0/numberOfInputs) # He 初始化
self.biases = np.zeros((numberOfNeurons,1)) # 将偏差初始化为零
def forward(self,inputs):
self.inputs = 输入
self.z = np.dot(输入，self.weights.T) + self.biases.T
self.outputs = self.activationFunction(self.z)
返回 self.outputs
def behind(self, dl_da, learning_rate,outPuts=None):

如果 self.activationFunction == softmax:
dz = dz = self.outputs - outPuts
否则：
da_dz = self.activationFunctionDiff(self.outputs)
dz = dl_da * da_dz
dl_da_prev = np.dot(dz,self.weights)
self.backParam = dl_da_prev
dw = np.zeros((self.numberOfNeurons, self.numberOfInputs))
for i in range(dz.shape[0]):
dws = np.dot(dz[i].reshape(self.numberOfNeurons,1), self.inputs[i].reshape(1,self.numberOfInputs))
dw += dws
dw = dw / dz.shape[0]
db = np.sum(dz, axis=0, keepdims=True).T / dz.shape[0]
self.weights = self.weights - learning_rate * dw
self.biases = self.biases - learning_rate * db
return dl_da_prev


现在我来介绍一下我如何使用这段代码
from sklearn.datasets import fetch_california_housing
data = fetch_california_housing()
X, y = data.data, data.target
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
layer1 = Layer(8,8,ReLU,ReLU_diff)
layer2 = Layer(8,1,linear,linear_diff)
y_t = np.array(y_train).reshape(len(y_train),1)
for i in range(1000):
outputs = layer1.forward(X_train)
outputs = layer2.forward(outputs)
dl = -2*(y_t-outputs)
backParam = layer2.backward(dl,0.001)
backParam = layer1.backward(backParam,0.001)

因此，最终输出位于 layer2.outputs 中，与输出相同。但首先输出是完全错误的，其次所有输入都一样。为什么会发生这种情况？
我已经考虑了很久，并尝试了 ChatGPT 和其他模型的帮助。我不明白错误在哪里？
谢谢帮助！]]></description>
      <guid>https://stackoverflow.com/questions/79320551/regression-in-custom-neural-network-giving-same-output-for-all-inputs</guid>
      <pubDate>Tue, 31 Dec 2024 17:57:05 GMT</pubDate>
    </item>
    <item>
      <title>Frechet Inception Distance (FID) 是否适合评估 GAN 在 Iris 等表格数据集上的表现</title>
      <link>https://stackoverflow.com/questions/79320482/is-frechet-inception-distance-fid-suitable-for-evaluating-gan-performance-on-t</link>
      <description><![CDATA[我是生成式 AI 的新手，正在使用表格数据集（例如 Iris 数据集）评估联合学习设置中 GAN 模型的性能。我正在探索不同的指标来评估生成器的性能。
我遇到了 Frechet Inception Distance (FID)，它广泛用于基于图像的 GAN，以衡量生成样本的质量和多样性。但是，我的数据集是表格的，而不是基于图像的，我不确定 FID 是否适用于这种情况。
FID 是否适合评估在 Iris 等表格数据集上训练的 GAN？
注意：我不是在寻找基于可视化的比较（例如 PCA、t-SNE）。我的目标是使用单一的定量指标来衡量生成器的性能。]]></description>
      <guid>https://stackoverflow.com/questions/79320482/is-frechet-inception-distance-fid-suitable-for-evaluating-gan-performance-on-t</guid>
      <pubDate>Tue, 31 Dec 2024 17:16:47 GMT</pubDate>
    </item>
    <item>
      <title>为何我无法包裹 LGBM？</title>
      <link>https://stackoverflow.com/questions/79320289/why-cant-i-wrap-lgbm</link>
      <description><![CDATA[我使用 LGBM 预测数值量的相对变化。我使用 MSLE（均方对数误差）损失函数来优化我的模型并获得正确的误差缩放比例。由于 MSLE 不是 LGBM 的原生功能，因此我必须自己实现它。但幸运的是，数学可以大大简化。这是我的实现；
class MSLELGBM(LGBMRegressor):
def __init__(self, **kwargs): 
super().__init__(**kwargs)

def predict(self, X):
return np.exp(super().predict(X))

def fit(self, X, y, eval_set=None, callbacks=None):
y_log = np.log(y.copy())
print(super().get_params()) # 这不会打印任何 kwargs
if eval_set:
eval_set = [(X_eval, np.log(y_eval.copy())) for X_eval, y_eval in eval_set]
super().fit(X, y_log, eval_set=eval_set, callbacks=callbacks)

如您所见，它非常简单。我基本上只需要对模型目标应用对数变换，并对预测取指数以返回我们自己的非对数世界。
但是，我的包装器不起作用。我使用以下命令调用该类；
model = MSLELGBM(**lgbm_params)
model.fit(data[X_cols_all], data[y_col_train]) 

我收到以下异常；

-----------------------------------------------------------------------------------------
KeyError Traceback (most recent call last)
Cell In[31], line 38
32 callbacks = [
33 lgbm.early_stopping(10, verbose=0), 
34 lgbm.log_evaluation(period=0),
35 ]
37 model = MSLELGBM(**lgbm_params)
---&gt; 38 model.fit(data[X_cols_all], data[y_col_train]) 
40 feature_importances_df = pd.DataFrame([model.booster_.feature_importance(importance_type=&#39;gain&#39;)], columns=X_cols_all).T.sort_values(by=0, accending=False)
41 feature_importances_df.iloc[:30]

单元格 In[31]，第 17 行
15 if eval_set:
16 eval_set = [(X_eval, np.log(y_eval.copy())) for X_eval, y_eval in eval_set]
---&gt; 17 super().fit(X, y_log, eval_set=eval_set, callbacks=callbacks)

文件 c:\X\.venv\lib\site-packages\lightgbm\sklearn.py:1189，在 LGBMRegressor.fit(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)
1172 def fit( # type: ignore[override]
1173 self,
1174 X: _LGBM_ScikitMatrixLike,
(...)
1186 init_model: Optional[Union[str, Path, Booster, LGBMModel]] = None,
1187 ) -&gt; &quot;LGBMRegressor&quot;:
1188 &quot;&quot;&quot;Docstring 继承自 LGBMModel。&quot;&quot;&quot;
...
--&gt; 765 if isinstance(params[&quot;random_state&quot;], np.random.RandomState):
766 params[&quot;random_state&quot;] = params[&quot;random_state&quot;].randint(np.iinfo(np.int32).max)
767 elif isinstance(params[&quot;random_state&quot;], np.random.Generator):

KeyError: &#39;random_state&#39;

我不知道 random_state 为何从 fit 方法中缺失，因为该函数甚至不需要它。我感觉这是一个复杂的软件工程问题，超出了我的理解范围。有人知道发生了什么吗？
如果有帮助的话，我尝试使用更简单的非 lgbm 结构来说明我想要的内容；

我只想将我提供给 MSLELGBM 的任何参数传递给原始 LGBM，但这样做时我遇到了很多问题。]]></description>
      <guid>https://stackoverflow.com/questions/79320289/why-cant-i-wrap-lgbm</guid>
      <pubDate>Tue, 31 Dec 2024 15:25:17 GMT</pubDate>
    </item>
    <item>
      <title>我想使用现有的 mongodb 数据库创建一个 rag 应用程序，这是一个大型数据库，将来可以扩展并拥有许多集合[关闭]</title>
      <link>https://stackoverflow.com/questions/79319727/i-want-to-create-a-rag-application-using-existing-mongodb-database-which-is-a-bi</link>
      <description><![CDATA[数据库集合还包括关系集合。根据用户查询，我希望应用程序能够有效地从不同的集合中检索所需的数据。
我之前尝试将整个数据库转换为向量嵌入，但这种方法太慢了。同样，为此目的使用聚合管道也很慢，并且会消耗大量处理资源。
我还尝试使用 openai 平台，即使用提示将数据提供给 chatgpt，然后从中检索数据，但输入大小太大，无法处理]]></description>
      <guid>https://stackoverflow.com/questions/79319727/i-want-to-create-a-rag-application-using-existing-mongodb-database-which-is-a-bi</guid>
      <pubDate>Tue, 31 Dec 2024 10:23:26 GMT</pubDate>
    </item>
    <item>
      <title>数据准备：用于 MMAction2 骨架动作识别的 2d 骨架数据 [关闭]</title>
      <link>https://stackoverflow.com/questions/79319079/data-preparation-2d-skeleton-data-for-mmaction2-skeleton-based-action-recogniti</link>
      <description><![CDATA[我正在构建一个用于动作识别的管道。我基本上是用 YOLO 姿势生成 2D 骨架数据，然后我想使用 2D 骨架数据进行动作识别。
对于动作识别，我使用此列表中的第一个（骨骼）：
https://mmaction2.readthedocs.io/en/latest/model_zoo/skeleton.html#ntu120-xsub-2d
我的问题在于 2D 骨架数据的准备：
YOLO-Pose 具有以下结构：
&quot;我捕获的 YOLO-Pose-Structure&quot;
但是 MMAction ST-GCN-Skeleton-based-recognition 需要不同的结构作为输入。
如果我没记错的话，动作识别的输入文件必须是 JSON 格式。此外，它还需要具有有关如何准备 2D 骨架数据的特定格式。我尝试浏览 mmaction2 文档，但找不到任何东西。我尝试谷歌搜索，但缺少合适的词来查找正确的文档。最后的手段 ChatGPT 让我头疼不已。]]></description>
      <guid>https://stackoverflow.com/questions/79319079/data-preparation-2d-skeleton-data-for-mmaction2-skeleton-based-action-recogniti</guid>
      <pubDate>Tue, 31 Dec 2024 02:56:30 GMT</pubDate>
    </item>
    <item>
      <title>将骨干网络 SAM1 更改为 SAM2 [关闭]</title>
      <link>https://stackoverflow.com/questions/79319026/change-backbone-sam1-sam2</link>
      <description><![CDATA[我将 Hi-SAM（文本分割）模型的主干从 SAM1 更改为 SAM2
但模型训练效果不佳
是否可以用 SAM2 替换 SAM1 的主干？
dice: 0.9284, dice_hr: 0.9073, focal: 0.2233, focal_hr: 0.2115, iou_mse: 0.0007, iou_mse_hr: 0.0004████████████████████████████████████████████████████████████████████████████▊ | 296/314 [07:30&lt;00:39，2.19 秒/时]
骰子：0.9400，骰子时间：0.9326，焦距：0.1165，焦距时间：0.1104，iou_mse：0.0036，iou_mse_hr：0.0000██████████████████████████████████████████████████████████████████████████████▎ | 297/314 [07:30&lt;00:59，3.51 秒/时]
骰子：0.9576，骰子时间：0.9385，焦距：0.1073，焦距时间：0.1062，iou_mse：0.0000，iou_mse_hr：0.0004██████████████████████████████████████████████████████████████████████████████▊ | 298/314 [07:33&lt;00:41，2.58 秒/时]
骰子：0.9656，骰子时间：0.9489，焦距：0.1200，焦距时间：0.0978，iou_mse：0.0000，iou_mse_hr：0.0000████████████████████████████████████████████████████████████████████████████▎ | 299/314 [07:34&lt;00:39，2.67 秒/时]
列车：[001] 损失：2.1323：96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 300/314 [07:34&lt;00:27, 1.97s/it]libpng 警告：iCCP：已知不正确的 sRGB 配置文件
dice：0.9616，dice_hr：0.9379，focal：0.0929，focal_hr：0.0776，iou_mse：0.0003，iou_mse_hr：0.0002██████████████████████████████████████████████████████████████████████████████▊ | 300/314 [07:35&lt;00:27，1.97 秒/时]
骰子：0.9287，骰子时间：0.9306，焦距：0.5704，焦距时间：0.7822，iou_mse：0.0001，iou_mse_hr：0.0002████████████████████████████████████████████████████████████████████████████████▎ | 301/314 [07:35&lt;00:23，1.80 秒/时]
骰子：0.9257，骰子时间：0.8996，焦距：0.1794，焦距时间：0.1899，iou_mse：0.0001，iou_mse_hr：0.0002████████████████████████████████████████████████████████████████████████████████▊ | 302/314 [07:36&lt;00:16，1.38 秒/时]
骰子：0.9361，骰子时间：0.9250，焦距：0.2103，焦距时间：0.1946，iou_mse：0.0009，iou_mse_hr：0.0001████████████████████████████████████████████████████████████████████████████████▎ | 303/314 [07:37&lt;00:13，1.20 秒/时]
骰子：0.9332，骰子时间：0.9257，焦距：0.3758，焦距时间：0.3885，iou_mse：0.0015，iou_mse_hr：0.0000██████████████████████████████████████████████████████████████████████████████▊ | 304/314 [07:38&lt;00:10, 1.08s/it]
dice: 0.9594, dice_hr: 0.9600, focal: 0.2075, focal_hr: 0.2865, iou_mse: 0.0000, iou_mse_hr: 0.0003████████████████████████████████████████████████████████████████████████████▍ | 305/314 [07:38&lt;00:08, 1.03it/s]

loss 没有减少
我验证了输入数据没有问题。发生了什么？
输出掩码为黑色，什么都没有显示。]]></description>
      <guid>https://stackoverflow.com/questions/79319026/change-backbone-sam1-sam2</guid>
      <pubDate>Tue, 31 Dec 2024 02:16:18 GMT</pubDate>
    </item>
    <item>
      <title>多元线性回归的梯度下降[关闭]</title>
      <link>https://stackoverflow.com/questions/79319005/gradient-descent-for-multiple-linear-regression</link>
      <description><![CDATA[我正在研究多元线性回归的梯度下降 (GD)，并注意到每个权重 (w_j) 的偏导数或梯度仅在其对应的特征值 (x_ij) 上有所不同。

现在，我总是试图直观地理解为什么某些事物是这样的，而且由于我的物理学背景，我经常试图从物理上理解。不过，对于这个，我似乎想不出除了将其理解为采用线性组合的 PD 的直接结果之外的任何东西。
有人对这个算法还有其他直观的看法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79319005/gradient-descent-for-multiple-linear-regression</guid>
      <pubDate>Tue, 31 Dec 2024 01:56:08 GMT</pubDate>
    </item>
    <item>
      <title>加载的 Keras 模型在预测时出现错误（可能与掩蔽有关）</title>
      <link>https://stackoverflow.com/questions/79318939/loaded-keras-model-throws-error-while-predicting-likely-issues-with-masking</link>
      <description><![CDATA[我目前正在开发和测试一个依赖大量数据进行训练的 RNN，因此尝试将训练文件和测试文件分开。我有一个文件，用于创建、训练和保存 tensorflow.keras 模型到文件 &#39;model.keras&#39;。然后，我将这个模型加载到另一个文件中并预测一些值，但出现以下错误：
无法将元素 {&#39;class_name&#39;: &#39;__tensor__&#39;, &#39;config&#39;: {&#39;dtype&#39;: &#39;float64&#39;, &#39;value&#39;: [0.0, 0.0, 0.0, 0.0]}} 转换为 Tensor。请考虑将元素转换为受支持的类型。请参阅 https://www.tensorflow.org/api_docs/python/tf/dtypes 了解受支持的 TF dtypes
顺便说一句，我曾尝试使用训练模型的文件中完全相同的数据运行 model.predict，并且运行顺利。模型加载肯定是问题所在，而不是用于预测的数据。
这个神秘的 float64 张量是我传递到掩码层的值。我不明白为什么 keras 无法将这个 JSON 对象识别为张量并以此方式应用掩码操作。我在下面附上了我的代码片段，为清晰和简洁起见进行了编辑：
model_generation.py：
# 创建模型

model = tf.keras.Sequential([
tf.keras.layers.Input((352, 4)),
tf.keras.layers.Masking(mask_value=tf.convert_to_tensor(np.array([0.0, 0.0, 0.0, 0.0]))),
tf.keras.layers.GRU(50, return_sequences=True,activation=&#39;tanh&#39;),
tf.keras.layers.Dropout(0.2),
tf.keras.layers.GRU(50,activation=&#39;tanh&#39;),
tf.keras.layers.Dropout(0.2),
tf.keras.layers.Dense(units=1,activation=&#39;sigmoid&#39;)])

# 编译模型...
# 训练模型...
model.save(&#39;model.keras&#39;)

model.predict(data) # 此行在此处有效

model_testing.py
model = tf.keras.models.load_model(&#39;model.keras&#39;)

model.predict(data) # 此行生成错误

编辑：
将加载命令移至与训练相同的文件中，仍然收到完全相同的错误消息。]]></description>
      <guid>https://stackoverflow.com/questions/79318939/loaded-keras-model-throws-error-while-predicting-likely-issues-with-masking</guid>
      <pubDate>Tue, 31 Dec 2024 00:53:11 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn StackingClassifier：Stacking 模型的手动 Python 实现与 sklearn.ensemble.StackingClassifier 之间的差异[关闭]</title>
      <link>https://stackoverflow.com/questions/79318736/sklearn-stackingclassifier-differences-between-manual-python-implementation-of</link>
      <description><![CDATA[我尝试构建一个 Python 类 CustomStackingClassifier()，以实现集成机器学习中的 Stacking 方法。在此实现中，基础分类器的输出设置为预测概率，并使用 StratifiedKFold 进行模型训练。元分类器的输入矩阵具有维度 (样本、模型 * 类)。
此代码本质上手动复制了 sklearn.ensemble.StackingClassifier() 的功能。但是，在使用葡萄酒数据集进行测试并比较两种方法的结果后，我发现了差异。尽管花了很多时间，但我还是无法确定问题所在。我将非常感谢社区的任何帮助或见解。非常感谢！代码如下：
class CustomStackingClassifier(BaseEstimator, ClassifierMixin):

def init(self, base_classifiers, meta_classifier, n_splits=5):
&quot;&quot;&quot;
param base_classifiers: 估计器列表
param meta_classifier: final_estimator
param n_splits: cv
&quot;&quot;&quot;

self.base_classifiers = base_classifiersself
meta_classifier = meta_classifierself
n_splits = n_splits

def fit(self, X, y):
&quot;&quot;&quot;
:param X: 训练数据
:param y: 训练标签
&quot;&quot;&quot;

n_samples = X.shape[0]
n_classifiers = len(self.base_classifiers)
n_classes = len(np.unique(y)) # 获取类别数量

base_probabilities_1 = np.zeros((n_samples, n_classifiers * n_classes)) # 用于存储基分类器的预测概率

# 通过 StratifiedKFold 设置交叉验证，与 StackingClassifier 一致
kf = StratifiedKFold(n_splits=self.n_splits, shuffle=False, random_state=None)

# 重置数据的索引
X_re_index = X.reset_index(drop=True)
y_re_index = y.reset_index(drop=True)

# 训练每个基分类器并生成预测概率
for i, clf in enumerate(self.base_classifiers):
fold_probabilities = np.zeros((n_samples, n_classes)) 

# 训练并预测每个折叠
for train_index, val_index in kf.split(X_re_index,y_re_index):
X_train, X_val = X_re_index.iloc[train_index], X_re_index.iloc[val_index]
y_train, y_val = y_re_index.iloc[train_index], y_re_index.iloc[val_index]

# 训练基础分类器
clf.fit(X_train, y_train)

# 预测验证集上的概率
fold_probabilities[val_index] = clf.predict_proba(X_val)

# 将每个基础分类器的预测概率保存到 base_probabilities 中
base_probabilities_1[:, i * n_classes: (i + 1) * n_classes] = fold_probabilities

# 使用基础分类器的预测概率训练元分类器
self.meta_classifier.fit(base_probabilities_1, y_re_index)

return self

def predict(self, X):
&quot;&quot;&quot;
:param X: 测试数据
&quot;&quot;&quot;
# 获取每个基础分类器的预测概率
base_probabilities = np.column_stack([clf.predict_proba(X) for clf in self.base_classifiers])

# 使用元分类器预测最终标签
return self.meta_classifier.predict(base_probabilities)

def predict_proba(self, X):
&quot;&quot;&quot;
:param X: 测试数据
&quot;&quot;&quot;
# 获取每个基分类器的预测概率
base_probabilities = np.column_stack([clf.predict_proba(X) for clf in self.base_classifiers])

# 使用元分类器获取预测概率
return self.meta_classifier.predict_proba(base_probabilities)

希望澄清一下CustomStackingClassifier()是否存在逻辑问题，如果有问题，希望得到指导和修改建议。如果实现正确，为什么与sklearn.ensemble.StackingClassifier()相比结果有差异？
需要注意的是，基分类器和元分类器的hyperparameters和random_state都已修复，因此可以排除这些因素是原因。]]></description>
      <guid>https://stackoverflow.com/questions/79318736/sklearn-stackingclassifier-differences-between-manual-python-implementation-of</guid>
      <pubDate>Mon, 30 Dec 2024 22:14:19 GMT</pubDate>
    </item>
    <item>
      <title>SVM 调优过程：由 `vectbl_recycle_rhs_rows()` 中的错误引起：</title>
      <link>https://stackoverflow.com/questions/79318610/svm-tuning-process-caused-by-error-in-vectbl-recycle-rhs-rows</link>
      <description><![CDATA[我不明白SVM模型调优过程的输出。
这是我的代码。
basic_recipe &lt;-
recipe(target ~ 
loan_type + New_versus_Repeat + 
Total_Amount + Total_Amount_to_Repay +
disbursement_date + due_date + duration +
Amount_Funded_By_Lender + Lender_portion_Funded + Lender_portion_to_be_repaid, 
data = train) |&gt;
step_string2factor(loan_type, levels = all_loan_type) |&gt;
step_string2factor(New_versus_Repeat, levels = all_new_versus_repeat) |&gt;
step_other(loan_type, Threshold = 0.1 / 100) |&gt;
step_date(all_date()) |&gt;
step_log(duration, Total_Amount, Total_Amount_to_repay, Amount_Funded_By_Lender, Lender_portion_to_be_repaid, base = 10, offset = 0.001) |&gt;
step_normalize(Total_Amount, Total_Amount_to_repay, Amount_Funded_By_Lender, Lender_portion_to_be_repaid, Lender_portion_to_be_repaid, na_rm = TRUE) |&gt;
step_cut(duration, breaks = ggplot_build(p)$data[[1]]$x) |&gt;
step_dummy(loan_type, New_versus_Repeat, duration, one_hot = TRUE)

svm_model &lt;-
svm_rbf(mode = &quot;classification&quot;, cost = tune(), rbf_sigma = tune()) |&gt;
set_engine(&quot;kernlab&quot;)

control_gd &lt;- control_grid(
verbose = TRUE,
save_pred = FALSE,
parallel_over = &quot;everything&quot;
)

set.seed(1)

svm_wf &lt;-
working() |&gt;
add_model(svm_model) |&gt;
add_recipe(basic_recipe)

svm_grid &lt;-
grid_regular(
cost(),
rbf_sigma(),
levels = c(3, 3)
)

svm_tune &lt;- 
svm_wf |&gt;
tune_grid(
resamples = folds,
grid = svm_grid,
metrics = metrics,
control = control_gd
)

这是第一次重新采样的输出 Resample01
Resample01：预处理器 1/1
✓ Resample01：预处理器 1/1
i Resample01：预处理器 1/1，模型 1/9
! Resample01：预处理器 1/1，模型 1/9：变量 `&#39; 常量。无法缩放数据。
✓ Resample01：预处理器 1/1，模型 1/9
i Resample01：预处理器 1/1，模型 1/9（提取）
i Resample01：预处理器 1/1，模型 1/9（预测）
x Resample01：预处理器 1/1，模型 1/9（预测）：
`$&lt;-` 中出现错误：
！分配的数据 `orig_rows` 必须与现有数据兼容。
✖ 现有数据有 6456 行。
✖ 分配的数据有 6492 行。
ℹ 仅回收大小为 1 的向量。
由 `vectbl_recycle_rhs_rows()` 中的错误导致：
！无法将大小为 6492 的输入回收到大小为 6456 的输入。
i Resample01：预处理器 1/1
✓ Resample01：预处理器 1/1
i Resample01：预处理器 1/1，模型 2/9
! Resample01：预处理器 1/1，模型 2/9：变量 `&#39; 常量。无法缩放数据。
✓ Resample01：预处理器 1/1，模型 2/9
i Resample01：预处理器 1/1，模型 2/9（提取）
i Resample01：预处理器 1/1，模型 2/9（预测）
x Resample01：预处理器 1/1，模型 2/9（预测）：
`$&lt;-` 中出现错误：
! 分配的数据 `orig_rows` 必须与现有数据兼容。
✖ 现有数据有 6456 行。
✖ 分配的数据有 6492 行。
ℹ 仅回收大小为 1 的向量。
由 `vectbl_recycle_rhs_rows()` 中的错误引起：
！无法将大小为 6492 的输入回收为大小为 6456。


我不明白这些错误。
有人能帮我理解吗？我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/79318610/svm-tuning-process-caused-by-error-in-vectbl-recycle-rhs-rows</guid>
      <pubDate>Mon, 30 Dec 2024 20:59:42 GMT</pubDate>
    </item>
    <item>
      <title>如何改进基本数据集上的机器学习？[关闭]</title>
      <link>https://stackoverflow.com/questions/79318363/how-to-improve-machine-learning-on-basic-data-set</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79318363/how-to-improve-machine-learning-on-basic-data-set</guid>
      <pubDate>Mon, 30 Dec 2024 18:46:42 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中为 Nvidia GeForce RTX 3050 Ti 启用 CUDA？</title>
      <link>https://stackoverflow.com/questions/79165030/how-can-i-enable-cuda-in-pytorch-for-nvidia-geforce-rtx-3050-ti</link>
      <description><![CDATA[我想在我的显卡（Nvidia GeForce RTX 3050 Ti）上运行 PyTorch 库（我在 PyCharm 的虚拟环境中运行该库）。但是，它在 CPU 上运行，每当我使用命令 import torch 和 print(&quot;cuda is available:&quot;, torch.cuda.is_available()) 时，它总是返回 False。
我安装了 CUDA 版本 12.6。我还安装了 PyTorch for CUDA 版本 12.4，因为它是 PyTorch 网站上可用的最新版本。考虑到我的显卡类型，我应该安装什么？]]></description>
      <guid>https://stackoverflow.com/questions/79165030/how-can-i-enable-cuda-in-pytorch-for-nvidia-geforce-rtx-3050-ti</guid>
      <pubDate>Thu, 07 Nov 2024 04:30:29 GMT</pubDate>
    </item>
    <item>
      <title>线性回归的梯度下降不收敛</title>
      <link>https://stackoverflow.com/questions/42869949/gradient-descent-on-linear-regression-not-converging</link>
      <description><![CDATA[我已经在 J​​avaScript 中实现了一个非常简单的梯度下降线性回归算法，但在查阅了多个来源并尝试了几种方法后，我无法让它收敛。
数据是绝对线性的，它只是数字 0 到 30 作为输入，x*3 作为其正确的输出以供学习。
这是梯度下降背后的逻辑：
train(input, output) {
const predictOutput = this.predict(input);
const delta = output - predictOutput;

this.m += this.learningRate * delta * input;
this.b += this.learningRate * delta;
}

predict(x) {
return x * this.m + this.b;
}

我从不同的地方获取了公式，包括：

Udacity 深度学习基础纳米学位的练习
Andrew Ng 的线性回归梯度下降课程（也在这里）
斯坦福的 CS229 讲义
我从卡内基梅隆大学找到的其他 PDF 幻灯片

我已经尝试过：

将输入和输出值标准化为 [-1, 1] 范围
将输入和输出值标准化为 [0, 1] 范围
将输入和输出值标准化为平均值 = 0 和标准差 = 1
降低学习率（1e-7 是我使用的最低值）
拥有一个完全没有偏差的线性数据集（y = x * 3)
具有非零偏差的线性数据集 (y = x * 3 + 2)
使用 -1 和 1 之间的随机非零值初始化权重

尽管如此，权重 (this.b 和 this.m) 仍未接近任何数据值，并且它们发散到无穷大。
我显然做错了什么，但我不知道是什么。

更新：以下是一些背景信息，可能有助于弄清楚我的问题到底是什么：
我正在尝试对线性函数的简单近似进行建模，并通过线性回归伪神经元进行在线学习。这样，我的参数就是：

权重：[this.m，this.b]
输入：[x，1]
激活函数：恒等函数 z(x) = x

因此，我的网络将表示为 y = this.m * x + this.b * 1，模拟我想要近似的数据驱动函数 (y = 3 * x)。
我希望我的网络“学习”参数 this.m = 3 和 this.b = 0，但似乎我陷入了局部最小值。
我的误差函数是均方误差： 
error(allInputs, allOutputs) {
let error = 0;
for (let i = 0; i &lt; allInputs.length; i++) {
const x = allInputs[i];
const y = allOutputs[i];
const predictOutput = this.predict(x);
const delta = y - predictOutput;

error += delta * delta;
}

return error / allInputs.length;
}

我更新权重的逻辑将是（根据我迄今为止检查过的来源）wi -= alpha * dError/dwi
为了简单起见，我将我的权重称为 this.m 和 this.b，这样我们就可以将其与我的 JavaScript 代码联系起来。我还将 y^ 称为预测值。
从这里开始：
error = y - y^
= y - this.m * x + this.b

dError/dm = -x
dError/db = 1

因此，将其应用于权重校正逻辑：
this.m += alpha * x
this.b -= alpha * 1

但这似乎根本不正确。]]></description>
      <guid>https://stackoverflow.com/questions/42869949/gradient-descent-on-linear-regression-not-converging</guid>
      <pubDate>Sat, 18 Mar 2017 02:53:48 GMT</pubDate>
    </item>
    <item>
      <title>支持向量机糟糕的结果-Python</title>
      <link>https://stackoverflow.com/questions/38685875/support-vector-machine-bad-results-python</link>
      <description><![CDATA[我正在研究 SVM 并实现了此代码，它太基础、太原始并且花费太多时间，但我只是想看看它实际上是如何工作的。不幸的是，它给了我糟糕的结果。我错过了什么？一些编码错误或数学错误？如果您想查看数据集，请在此处链接。我从 UCI 机器学习存储库中获取了它。感谢您的支持。
def hypo(x,q):
return 1/(1+np.exp(-x.dot(q)))

data=np.loadtxt(&#39;LSVTVoice&#39;,delimiter=&#39;\t&#39;);

x=np.ones(data.shape)
x[:,1:]=data[:,0:data.shape[1]-1]
y=data[:,data.shape[1]-1]

q=np.zeros(data.shape[1])
C=0.002

##均值归一化
for i in range(q.size-1):
x[:,i+1]=(x[:,i+1]-x[:,i+1].mean())/(x[:,i+1].max()-x[:,i+1].min());

对于范围（2000）内的 i：
h=x.dot(q)
对于范围（q.size）内的 j：
q[j]=q[j]-(C*np.sum( -y*np.log(hypo(x,q))-(1-y)*np.log(1-hypo(x,q))) ) + (0.5*np.sum(q**2))

对于范围（y.size）内的 i：
如果 h[i]&gt;=0：
打印 y[i],&#39;1&#39; 
否则：
打印 y[i],&#39;0&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/38685875/support-vector-machine-bad-results-python</guid>
      <pubDate>Sun, 31 Jul 2016 16:04:48 GMT</pubDate>
    </item>
    </channel>
</rss>