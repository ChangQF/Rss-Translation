<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 19 Oct 2024 03:21:36 GMT</lastBuildDate>
    <item>
      <title>训练模型预测不一致</title>
      <link>https://stackoverflow.com/questions/79104144/trained-model-prediction-are-not-consistent</link>
      <description><![CDATA[我用一个小型数据集训练了一个 VGG16 (224x224x3) 迁移学习图像分类模型，该数据集包含来自 3 位艺术家的画作。该数据集有大约 80 幅画作用于训练，25 幅用于测试。图像上有增强。图像被标记为真（属于艺术家）或假（不属于艺术家），比例约为 50-50。训练补丁总数（224x224x3）约为 20000。该模型旨在判断一幅画是否属于艺术家。这是训练损失和准确度图：

该图表明该模型在测试数据集上表现良好。然后将壁灯等图像（模型从未见过这些图像）输入模型，并询问此壁灯图像是否属于艺术家。显然答案是否定的。但最初有几次模型确实给出了“否”的答案，但现在它始终给出“是”的答案，这显然是错误的。我对训练模型性能的经验有限。什么会导致训练模型的预测不一致？训练数据集太小还是其他原因？]]></description>
      <guid>https://stackoverflow.com/questions/79104144/trained-model-prediction-are-not-consistent</guid>
      <pubDate>Sat, 19 Oct 2024 03:19:59 GMT</pubDate>
    </item>
    <item>
      <title>我们如何知道Ranker（Weka）要设置什么参数？</title>
      <link>https://stackoverflow.com/questions/79103647/how-do-we-know-what-parameters-for-ranker-weka-to-set</link>
      <description><![CDATA[我正在使用 Weka。所以我有几个问题无法通过手册弄清楚：

我们如何知道 Ranker 应该设置哪些参数？我的意思是，阈值和 numToSelect。对此有什么解释吗？
当我通过资源管理器选择属性并保存修改后的数据集时，它始终是 N+1 属性（N 个选定的属性 + 类/标签）。为什么？标签/类不也是属性吗？
]]></description>
      <guid>https://stackoverflow.com/questions/79103647/how-do-we-know-what-parameters-for-ranker-weka-to-set</guid>
      <pubDate>Fri, 18 Oct 2024 20:54:50 GMT</pubDate>
    </item>
    <item>
      <title>高效 Net V2 M ONNX 模型在小输入上的推理速度明显较慢</title>
      <link>https://stackoverflow.com/questions/79103592/efficient-net-v2-m-onnx-model-infers-significantly-slower-on-small-input</link>
      <description><![CDATA[当我将 Efficient net v2 m 模型从 Pytorch 转换为不同大小输入的 Onnx 时，我注意到一种奇怪且无法解释的行为。
在我的 RTX 4090 上，1280X1280 大小图像上的 ONNX 模型在 35 毫秒内推断出批处理大小为 1。当我将图像大小缩小到大约 192X192（批处理大小相同为 1）时，运行时间几乎保持不变。这是可以理解的，因为固定开销占主导地位，例如初始化时间、线程池预热、与 GPU 之间的低效数据传输，最重要的是，计算库针对 GPU 上的矢量化和 SIMD 指令进行了优化。
然而，令人困惑的是，一旦我开始将输入图像大小减小到 192X192 以下，运行时间就会急剧增加。对于 64X64 图像，批处理大小为 1 时运行时间为 &gt;100ms。我完全理解为什么在较小的图像上推理不应该更快，但我不明白为什么它会更慢（而且慢得多）。
当我增加较小图像的批处理大小时，每批的运行时间会大幅改善（不仅仅是每幅图像的运行时间）。对于批处理大小为 16 的图像，推理 192X192 图像每批需要 25 毫秒（每幅图像不到 2 毫秒），而批处理大小为 1 时则需要 &gt;100ms。同样，我对此没有任何解释。固定开销和优化的 SIMD 矢量化将决定每幅图像的摊销运行时间应该随着批处理大小的增加而改善。但是，我观察到整个批次的运行时间也得到了改善。
对于较大的图像（例如 1280X1280），增加批次大小会增加每个批次的运行时间（尽管是亚线性的，这是完全可以预料的 - 随着批次大小的增加，每个图像的运行时间仍然会缩短到一定限度，之后，对于几乎无法放入 GPU 内存的更高批次大小，每个图像的运行时间也会增加约 10%）。
但是在 CPU 上运行时，处理时间会随着输入大小的增加而单调增加，正如预期的那样。
当我要求它对所有输入进行处理时，我已经验证了 ONNX 模型在 GPU 上成功运行。事实上，对于小输入，CPU 推理时间比 GPU 更快（这是可以理解的，因为有固定的 I/O 和其他开销）
注意：由于我在整个实验过程中将动态轴设置为 None，因此我为具有不同输入大小的同一 torch 模型保存了多个版本的 ONNX 模型。使用或不使用 onnx-sim 几乎不会对运行时间产生影响（处理速度差异小于 10-15%）。我在 C++ 中以 OrtCUDAProviderOptions 作为执行提供程序运行 onnx 模型，使用或不使用 GraphOptimizationLevel 几乎没有区别。
神经网络的输出对于所有输入都符合预期，因此我不希望我的代码中出现任何错误。
TL;DR
我的 ONNX 模型对于中等大小图像的运行速度比 GPU 上的小图像更快。对于较小的图像，增加批次大小会导致每批次的处理时间大幅减少（而不仅仅是 SIMD 并行化所预期的每张图像的摊销时间）。]]></description>
      <guid>https://stackoverflow.com/questions/79103592/efficient-net-v2-m-onnx-model-infers-significantly-slower-on-small-input</guid>
      <pubDate>Fri, 18 Oct 2024 20:32:42 GMT</pubDate>
    </item>
    <item>
      <title>多类别分类中的准确率和 F1 分数 = 1.0 的问题</title>
      <link>https://stackoverflow.com/questions/79103501/issues-with-accuracy-and-f1-score-1-0-in-multi-class-classification</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79103501/issues-with-accuracy-and-f1-score-1-0-in-multi-class-classification</guid>
      <pubDate>Fri, 18 Oct 2024 19:59:23 GMT</pubDate>
    </item>
    <item>
      <title>我应该在正弦位置编码中交错正弦和余弦吗？</title>
      <link>https://stackoverflow.com/questions/79103455/should-i-interleave-sin-and-cosine-in-sinusoidal-positional-encoding</link>
      <description><![CDATA[我正在尝试实现正弦位置编码。我发现了两个给出不同编码的解决方案。我想知道其中一个是错误的还是两个都是正确的。我展示了两种选项的编码结果的视觉图。
class SinusoidalPosEmb(nn.Module):
def __init__(self, dim):
super().__init__()
self.dim = dim
def forward(self, x):
device = x.device
half_dim = self.dim // 2
emb = math.log(10000) / (half_dim - 1)
emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
emb = x[:, None] * emb[None, :]
emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
return emb


2)
class TransformerPositionalEmbedding(nn.Module):
&quot;&quot;&quot;
摘自论文《Attention Is All You Need》第 3.5 节
&quot;&quot;&quot;
def __init__(self, dimension, max_timesteps=1000):
super(TransformerPositionalEmbedding, self).__init__()
assert dimension % 2 == 0, &quot;Embedding 维度必须是偶数&quot;
self.dimension = dimension
self.pe_matrix = torch.zeros(max_timesteps, dimension)
# 收集嵌入向量中的所有偶数维度
even_indices = torch.arange(0, self.dimension, 2)
# 使用对数变换计算项以加快计算速度
# (https://stackoverflow.com/questions/17891595/pow-vs-exp-performance)
log_term = torch.log(torch.tensor(10000.0)) / self.dimension
div_term = torch.exp(even_indices * -log_term)
# 根据奇数/偶数时间步长预先计算位置编码矩阵
timesteps = torch.arange(max_timesteps).unsqueeze(1)
self.pe_matrix[:, 0::2] = torch.sin(timesteps * div_term)
self.pe_matrix[:, 1::2] = torch.cos(timesteps * div_term)
def forward(self, timestep):
# [bs, d_model]
return self.pe_matrix[timestep]

]]></description>
      <guid>https://stackoverflow.com/questions/79103455/should-i-interleave-sin-and-cosine-in-sinusoidal-positional-encoding</guid>
      <pubDate>Fri, 18 Oct 2024 19:35:08 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 3D UNet 在进行二元分割时总是预测黑色？</title>
      <link>https://stackoverflow.com/questions/79103203/why-does-my-3d-unet-always-predict-black-when-doing-binary-segmentation</link>
      <description><![CDATA[我的输入图像是灰度图像（uint8），目标图像是二值图像（bool）。我将输入图像转换为 0~1 并对其进行归一化，然后将其输入到 3D UNet 中。
这是我用于 3D UNet 模型的 代码。
这是我用于训练的代码：
unet = UNet3D(in_channels=1, num_classes=1).to(device)

bce_loss = nn.BCEWithLogitsLoss()
optimizer = optim.SGD(unet.parameters(), lr=0.01, motivation=0.9, weight_decay=0.0005)

for epoch in range(args.epochs):
unet.train()
train_loss = 0.0
for step, (x, y_true, _) in enumerate(train_loader): 
x, y_true = x.to(device=device, dtype=torch.float), y_true.to(device=device, dtype=torch.float)
optimizer.zero_grad()
y_pred = unet(x) # y_pred - (batch_size,1,depth,width,height)

loss = bce_loss(y_pred, y_true) # y_true - (batch_size,1,depth,width,height)
train_loss += loss.item()
loss.backward()
optimizer.step()

print(&quot;Epoch %d, 平均训练交叉熵损失 %9.6f&quot; % (epoch + 1, train_loss / len(train_loader)))

我的二元交叉熵损失没有收敛到一个较小的数字。预测的最大值不能超过 0.5，所以我无法预测掩码。
我尝试使用不同的学习率，将损失函数改为 bce 损失 + dice 损失，但仍然不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/79103203/why-does-my-3d-unet-always-predict-black-when-doing-binary-segmentation</guid>
      <pubDate>Fri, 18 Oct 2024 18:00:36 GMT</pubDate>
    </item>
    <item>
      <title>如何估计 CoreML 模型的参数数量？</title>
      <link>https://stackoverflow.com/questions/79103126/how-to-esitmate-the-number-of-parameters-for-coreml-models</link>
      <description><![CDATA[我正在比较修剪对 CoreML 模型的影响。
虽然我可以轻松测量文件大小的变化（以 kB 为单位），但我很难估计模型参数数量的变化，因为 CoreML 没有提供像 PyTorch 的 model.parameters() 这样的直接方法。我如何估计或计算修剪后的 CoreML 模型中的参数数量？]]></description>
      <guid>https://stackoverflow.com/questions/79103126/how-to-esitmate-the-number-of-parameters-for-coreml-models</guid>
      <pubDate>Fri, 18 Oct 2024 17:35:25 GMT</pubDate>
    </item>
    <item>
      <title>在 XGBoost 预测模型中更新未来预测的傅里叶项和滞后</title>
      <link>https://stackoverflow.com/questions/79102211/updating-fourier-terms-and-lags-for-future-predictions-in-xgboost-forecasting-mo</link>
      <description><![CDATA[我构建了一个 XGBoost 预测模型，该模型结合了滞后特征、日期时间特征和傅立叶项。我的目标是对每个新的工作日进行预测。该模型使用 5 倍交叉验证进行训练，我从调整过程中保存了最佳超参数。之后，我使用这些最佳参数重新训练整个模型。
我面临的挑战是使用预测日的正确特征更新未来数据框：
对于日期时间特征，我可以轻松更新它们。
但是，更新傅立叶项 (FT) 和滞后是我遇到的难题。
这是我尝试过的方法：
我将历史数据框与新的预测数据框结合起来，将 Y 列重命名为 pred。
对于第一个预测日，我使用最后已知的实际值。
对于后续的预测日，我需要使用历史数据和新预测值的组合来更新傅立叶项和滞后。
尽管尝试了几次，我还是无法让更新过程正常工作。傅立叶项没有按预期对齐，我很难根据历史数据和预测数据的组合调整滞后。
问题：如何使用历史预测和新预测的组合正确更新未来几天的傅立叶项和滞后？以下是数据框。 pred 列是实际历史数据的 Y 变量（国家计数）的副本。

# 使用 NaN 初始化预测列
future_df_all_countries[&#39;pred&#39;] = np.nan

# 使用已经训练过的 `best_model` 对未来数据进行预测
# 循环遍历 future_df_all_countries 中的每一行
for i in range(len(future_df_all_countries)):
if not future_df_all_countries[&#39;is_actual&#39;].iloc[i]: # 仅预测是否为未来数据
if i == 0:
# 对于第一个预测日，使用最后一个实际计数
last_actual_count = future_df_all_countries.loc[future_df_all_countries[&#39;is_actual&#39;] == True, &#39;Country count&#39;].iloc[-1]
future_df_all_countries.at[future_df_all_countries.index[i], &#39;pred&#39;] = last_actual_count
else:
# 使用已经训练好的模型对当天进行预测
future_df_all_countries.at[future_df_all_countries.index[i], &#39;pred&#39;] = best_model.predict(future_df_all_countries[FEATURES].iloc[[i]])[0]

# 计算傅里叶变换的函数
def calculate_fourier_transform(group, components):
data_FT = group[[&#39;pred&#39;]] # 使用 &#39;pred&#39; 列进行 FT 计算
if data_FT[&#39;pred&#39;].isnull().all():
return pd.DataFrame(index=group.index) # 如果全部为 NaN，则返回空 DF
country_count_fft = np.fft.fft(np.asarray(data_FT[&#39;pred&#39;].tolist()))
ifft_results = pd.DataFrame(index=group.index)

# 使用指定的列名进行更新
for i, num_ in enumerate(components):
fft_list = np.copy(country_count_fft)
fft_list[num_:-num_] = 0
ifft_results[f&#39;ifft_{num_}_components&#39;] = np.fft.ifft(fft_list).real

return ifft_results

# 用于傅里叶变换的组件列表
component_list = [70, 80, 90] # 使用指定的组件

#使用“pred”列计算所有行的 FT
fourier_results = calculate_fourier_transform(future_df_all_countries, component_list)

# 使用 FT 结果更新 future_df_all_countries
for col in fourier_results.columns:
future_df_all_countries[col] = fourier_results[col]

# 根据“pred”创建滞后特征
def create_lag_features(df, target_col=&#39;pred&#39;, group_col=&#39;SHIP_TO_COUNTRY&#39;, lags=[5, 10, 15, 30]):
for lag in lags:
df[f&#39;lag_{lag}_day&#39;] = df.groupby(group_col)[target_col].shift(lag)
return df

# 应用滞后特征创建
future_df_all_countries = create_lag_features(future_df_all_countries)
]]></description>
      <guid>https://stackoverflow.com/questions/79102211/updating-fourier-terms-and-lags-for-future-predictions-in-xgboost-forecasting-mo</guid>
      <pubDate>Fri, 18 Oct 2024 13:07:36 GMT</pubDate>
    </item>
    <item>
      <title>使用 RAG 和 FastAPI 的 WebApp</title>
      <link>https://stackoverflow.com/questions/79099986/webapp-using-rag-and-fastapi</link>
      <description><![CDATA[我有一个项目，使用 RAG 和 FastAPI 在特定数据集上建立 Web 应用程序。
目标是创建一个 Web 应用程序，实现检索增强生成 (RAG) 后端系统，以分析和比较两个提供的调查结果数据集。任务是构建一个与运行 FastAPI 的 Python 后端通信的前端。该应用程序应允许用户使用 AI 驱动的交互探索、分析和交叉比较数据集。
必要的文件在链接中（后端使用 Python，前端使用 Reactjs）https://github.com/drrahulsuresh/bounce/tree/dev
csv 文件包含在 git 链接中
但是，用户查询未成功执行。
有人可以帮我解决这个问题吗？
我尝试了 SQL，甚至将 Excel 转换为 JSON 或 CSV。我认为问题出在复杂的嵌套数据集上。我不知道如何通过 rag 将数据处理成可读格式。]]></description>
      <guid>https://stackoverflow.com/questions/79099986/webapp-using-rag-and-fastapi</guid>
      <pubDate>Thu, 17 Oct 2024 22:12:24 GMT</pubDate>
    </item>
    <item>
      <title>如何将我的数据集不重复地分成测试和训练？</title>
      <link>https://stackoverflow.com/questions/79096421/how-to-split-my-dataset-into-test-and-train-without-repitition</link>
      <description><![CDATA[我正在开发一个 Python 脚本来测试一个算法。我有一个数据集，需要将其分成 80% 用于训练，20% 用于测试。但是，我想保存测试集以供进一步分析，确保与之前的测试集不重叠。
虽然我的代码总体运行良好，但我遇到了一个问题：由于随机选择过程，测试数据集有时包含之前测试运行中已经选择的记录。
在流程结束时，所有 100% 的记录都应在其中一次运行中进行测试
举个例子说明：

在第一次运行中，我的数据集 {0,1,2,3,4,5,6,7,8,9 被拆分为训练集 {0,1,2,4,5,7,8,9 和测试集 {3,6。
在第二次运行中，训练集为 {0,1,2,3,4,5,7,9，测试集为{6,8。

如您所见，记录 {6 被选中两次进行测试，我想避免这种情况。
我如何修改代码以确保每次随机选择 20% 的测试集，但排除任何之前选择的记录？
这是当前代码：
df = pd.read_csv(&quot;CustomersInfo.csv&quot;)
y = df[&#39;CustomerRank&#39;]
X = df.drop(&#39;CustomerRank&#39;, axis=1, errors=&#39;ignore&#39;)

#----------------------------------------------------------------------------------
#这是需要修复的部分
for RandStat in [11, 22, 33, 44, 55]:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RandStat)
#-------------------------------------------------------------------

clf = XGBClassifier(random_state=RandStat)
clf.fit(X_train, y_train)
fnStoreAnalyse(y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/79096421/how-to-split-my-dataset-into-test-and-train-without-repitition</guid>
      <pubDate>Thu, 17 Oct 2024 03:50:34 GMT</pubDate>
    </item>
    <item>
      <title>训练T5时如何添加EOS？</title>
      <link>https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5</guid>
      <pubDate>Tue, 15 Oct 2024 04:22:59 GMT</pubDate>
    </item>
    <item>
      <title>将 ONNX 模型从版本 9 升级到 11</title>
      <link>https://stackoverflow.com/questions/69899046/upgrade-onnx-model-from-version-9-to-11</link>
      <description><![CDATA[我正在使用 ONNX 模型，需要对其进行量化以减小其大小，为此，我遵循官方文档中的说明：
import onnx
from onnxruntime.quantization import quantize_dynamic, QuantType

model_fp32 = &#39;path/to/the/model.onnx&#39;
model_quant = &#39;path/to/the/model.quant.onnx&#39;
quantized_model = quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QUInt8)

但是当我运行它时，我收到以下警告：
警告：root：原始模型 opset 版本为 9，不支持量化。请将模型更新为 opset &gt;= 11。自动将模型更新为 opset 11。请验证量化模型。

我测试了量化模型，但没有成功，它生成此错误：
 INVALID_GRAPH：从 model_a2_quant.onnx 加载模型失败：这是一个无效的模型。 Node:Upsample__477 中的错误：为 Upsample 注册的 Op 在 domain_version 11 中已弃用

此时我有什么替代方案可以量化模型？
我从这个 repo 中获得了张量流中的原始模型：https://github.com/ciber-lab/pictor-ppe
并使用此代码将其转换为 ONNX：
# 输入和输出
input_tensor = 输入（shape=(input_shape[0], input_shape[1], 3) ) # 输入
num_out_filters = ( num_anchors//3 ) * ( 5 + num_classes ) # 输出

## 构建并加载模型
model = yolo_body(input_tensor, num_out_filters)

weight_path = &#39;ONNX_demo/models/pictor-ppe-v302-a1-yolo-v3-weights.h5&#39;

model.load_weights( weight_path )

tf.saved_model.save(model, &quot;ONNX_demo/models/save_model&quot;)

# 将其转换为 ONNX 格式：

python3 -m tf2onnx.convert --saved-model &quot;ONNX_demo/models/save_model&quot; --output &quot;ONNX_demo/models/model.onnx&quot;
]]></description>
      <guid>https://stackoverflow.com/questions/69899046/upgrade-onnx-model-from-version-9-to-11</guid>
      <pubDate>Tue, 09 Nov 2021 13:31:13 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以获得包含世界上几乎所有国家护照的护照图像数据集？</title>
      <link>https://stackoverflow.com/questions/60039938/where-can-i-get-passport-images-dataset-that-contain-passport-of-almost-all-coun</link>
      <description><![CDATA[我正在训练一个 OCR 模型，用于从护照中识别 MRZ。为了训练我的模型以获得更高的准确性，我需要使用尽可能多的图片来训练它。我试图在 KAGGLE 上找到护照的数据集，但找不到。
有人能告诉我从哪里可以获得包含几乎所有国家或北美和南美护照的护照图像数据集吗？
非常感谢您的帮助。
祝好，
Asma]]></description>
      <guid>https://stackoverflow.com/questions/60039938/where-can-i-get-passport-images-dataset-that-contain-passport-of-almost-all-coun</guid>
      <pubDate>Mon, 03 Feb 2020 13:11:08 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 生存模型预测</title>
      <link>https://stackoverflow.com/questions/53521427/xgboost-prediction-for-survival-model</link>
      <description><![CDATA[Xgboost 的文档暗示，使用 Cox PH 损失训练的模型的输出将是个人预测乘数的指数（相对于基线风险）。有没有办法从这个模型中提取基线风险，以预测每个人的整个生存曲线？

survival:cox：右删失生存时间数据的 Cox 回归
（负值被视为右删失）。请注意，预测
是在风险比尺度上返回的（即，比例风险函数 h(t) = h0(t) * HR 中的 HR = exp(marginal_prediction)）
]]></description>
      <guid>https://stackoverflow.com/questions/53521427/xgboost-prediction-for-survival-model</guid>
      <pubDate>Wed, 28 Nov 2018 14:13:15 GMT</pubDate>
    </item>
    <item>
      <title>sample.int(m, k) 中的错误：无法抽取大于总体的样本</title>
      <link>https://stackoverflow.com/questions/25745215/error-in-sample-intm-k-cannot-take-a-sample-larger-than-the-population</link>
      <description><![CDATA[首先，我要说的是，我对机器学习、kmeans 和 r 还很陌生，这个项目是一种学习更多这方面知识的方法，也是将这些数据呈现给我们的 CIO 的一种方式，这样我就可以在开发新的帮助台系统时使用它。
我有一个 60K 行的文本文件。该文件包含教师在 3 年期间输入的帮助台工单的标题。
我想创建一个 r 程序，获取这些标题并创建一组类别。例如，与打印问题相关的术语，或与投影仪灯泡相关的一组术语。我使用 r 打开文本文档，清理数据，删除停用词和其他我认为不必要的词。我已获得频率 &gt;= 400 的所有术语列表，并将其保存到文本文件中。
但现在我想将 kmeans 聚类（如果可以完成或合适）应用于同一数据集，看看我是否可以提出类别。
下面的代码包括将写出使用的术语列表 &gt;= 400 的代码。它位于末尾，并被注释掉。
library(tm) #加载文本挖掘库
library(SnowballC)
options(max.print=5.5E5) 
setwd(&#39;c:/temp/&#39;) #将 R 的工作目录设置为靠近我的文件的位置
ae.corpus&lt;-Corpus(DirSource(&quot;c:/temp/&quot;),readerControl=list(reader=readPlain))
summary(ae.corpus) #检查发生了什么在
ae.corpus &lt;- tm_map(ae.corpus, tolower)
ae.corpus &lt;- tm_map(ae.corpus, removePunctuation)
ae.corpus &lt;- tm_map(ae.corpus, removeNumbers)
ae.corpus &lt;- tm_map(ae.corpus, stemDocument, language = &quot;english&quot;) 
myStopwords &lt;- c(stopwords(&#39;english&#39;), &lt;a very long list of other words&gt;)
ae.corpus &lt;- tm_map(ae.corpus, removeWords, myStopwords) 

ae.corpus &lt;- tm_map(ae.corpus, PlainTextDocument)

ae.tdm &lt;- DocumentTermMatrix(ae.corpus, control = list(minWordLength = 5))

dtm.weight &lt;- weightTfIdf(ae.tdm)

m &lt;- as.matrix(dtm.weight)
rownames(m) &lt;- 1:nrow(m)

#euclidian 
norm_eucl &lt;- function(m) {
m/apply(m,1,function(x) sum(x^2)^.5)
}
m_norm &lt;- norm_eucl(m)

results &lt;- kmeans(m_norm,25)

#list clusters

clusters &lt;- 1:25
for (i in clusters){
cat(&quot;Cluster &quot;,i,&quot;:&quot;,findFreqTerms(dtm.weight[results$cluster==i],400,&quot;\n\n&quot;))
}

#inspect(ae.tdm)
#fft &lt;- findFreqTerms(ae.tdm, lowfreq=400)

#write(fft, file = &quot;dataTitles.txt&quot;,
# ncolumns = 1,
# append = FALSE, sep = &quot; &quot;)

#str(fft)

#inspect(fft)

当我使用 RStudio 运行此程序时，我得到：
&gt;结果 &lt;- kmeans(m_norm,25)


sample.int(m, k) 中的错误：当“replace = FALSE”时无法获取大于总体的样本

我不太清楚这是什么意思，而且我在网上没有找到很多关于这方面的信息。有什么想法吗？
TIA]]></description>
      <guid>https://stackoverflow.com/questions/25745215/error-in-sample-intm-k-cannot-take-a-sample-larger-than-the-population</guid>
      <pubDate>Tue, 09 Sep 2014 12:55:28 GMT</pubDate>
    </item>
    </channel>
</rss>