<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 20 Jan 2024 18:16:41 GMT</lastBuildDate>
    <item>
      <title>XGBoostError：basic_string::调整大小</title>
      <link>https://stackoverflow.com/questions/77852154/xgboosterror-basic-stringresize</link>
      <description><![CDATA[我想实现一个 xgboost 模型。为此，我已经通过 skopt.BayesSearchCV 执行了超参数调整，如以下代码所示：
从 xgboost 导入 XGBRegressor
从 skopt 导入 BayesSearchCV
from skopt.space import 实数、分类、整数

种子 = 8

xgb_clf = XGBRegressor(random_state=SEED)

搜索空间 = {
    &#39;xgb_clf__max_深度&#39;：整数（2,30），
    &#39;xgb_clf__subsample&#39;：真实（0.1，1.0），
    &#39;xgb_clf__colsample_bytree&#39;：真实（0.1，1.0），
    &#39;xgb_clf__colsample_bylevel&#39;：真实（0.1，1.0），
    &#39;xgb_clf__reg_alpha&#39;：实数（1.0，100.0），
    &#39;xgb_clf__reg_lambda&#39;：实数（1.0，100.0），
    &#39;xgb_clf__n_estimators&#39;: 整数(10, 1000),
    &#39;xgb_clf__learning_rate&#39;：真实（0.01，0.1，先验=&#39;对数均匀&#39;）
}

opt = BayesSearchCV(xgb_clf, search_space, cv=10, n_iter=30, 评分=&#39;roc_auc&#39;, random_state=SEED)

此外，我编写了一些代码，应该逐行训练我的模型，其中行是保存在另一个目录中的 2D numpy 数组，标签（年龄）也保存在单独的文件夹中。因为我想根据文档逐行训练它（https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor.fit）必须设置 xgb_model 参数。这需要保存我的模型，我在下面的代码中尝试过：
导入pickle

MODEL_PATH =“ML/模型”
os.makedirs(MODEL_PATH)
PATH_TRAIN_IMAGES =“ML/preprocessed_arrays/train”
PATH_TRAIN_LABELS =“ML/preprocessed_labels/train”
文件名 = MODEL_PATH + &#39;/xgb_model.sav&#39;

pickle.dump(opt, open(文件名, &#39;wb&#39;))

例如zip中的instance_file、label_file(os.listdir(PATH_TRAIN_IMAGES), os.listdir(PATH_TRAIN_LABELS))：
    实例= np.loadtxt（PATH_TRAIN_IMAGES +“/”+实例文件）
    label = np.loadtxt(PATH_TRAIN_IMAGES + “/” + label_file)
    opt.fit(实例、标签、xgb_model=文件名)
    # 将模型保存到磁盘
    pickle.dump(opt, open(文件名, &#39;wb&#39;))

不幸的是，这段代码抛出了一个 XGBoostError: basic_string::resize 错误，我不明白。
我已经尝试对此错误进行一些研究，但这似乎是一个罕见的错误。
如何保存模型、逐行训练并避免此错误？提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/77852154/xgboosterror-basic-stringresize</guid>
      <pubDate>Sat, 20 Jan 2024 18:11:12 GMT</pubDate>
    </item>
    <item>
      <title>深度学习如何成为机器学习的子集，而机器学习如何成为人工智能的子集？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77852041/how-is-deep-learning-a-subset-of-machine-learning-and-machine-learning-a-subset</link>
      <description><![CDATA[像什么是清晰的现实生活例子来解释它们作为一个子集？
我对人工智能和机器学习还很陌生。请认为我是一个愚蠢的机器人，并将数据提供给我。
.]]></description>
      <guid>https://stackoverflow.com/questions/77852041/how-is-deep-learning-a-subset-of-machine-learning-and-machine-learning-a-subset</guid>
      <pubDate>Sat, 20 Jan 2024 17:33:54 GMT</pubDate>
    </item>
    <item>
      <title>调整线性 SVM 参数期间出现“警告：达到最大迭代次数”</title>
      <link>https://stackoverflow.com/questions/77851738/warning-reaching-max-number-of-iterations-during-tuning-of-parameters-for-lin</link>
      <description><![CDATA[我有这个数据集：https： //www.kaggle.com/datasets/mirbektoktogaraev/should-this-loan-be-approved-or-denied 我已从中删除了 NewExist = 1 的所有实例以及包含 NA 值的所有实例。我还删除了 ChgOffPrinGr 列，因此还剩下 161,732 个实例和 25 个变量。我需要应用一系列以 MIS_Status 作为目标变量的机器学习模型。为了实现这一目标，我没有使用所有变量作为预测变量，因为显然，我需要消除所有包含“未来”的变量。值，即无法提前知道的值。因此，我有 161,732 个实例，包含 10 个变量：State（因子）、Bank（因子）、NAICS（因子）、UrbanRural（因子）、RevLineCr（因子）、LowDoc（因子）、Term（双精度）、NoEmp（双精度）、 FranchiseCode（因子）、GrAppv（双精度）、MIS_Status（因子、目标）。
我做的第一件事是预处理数据集以使用以下代码应用算法（因此我创建了配方）：
selected_columns &lt;- c(&quot;State&quot;, &quot;Bank&quot;, &quot;NAICS&quot;, &quot;UrbanRural&quot;, &quot;RevLineCr&quot;, &quot;LowDoc&quot;, &quot;Term&quot;, &quot;NoEmp&quot; ;、“特许经营代码”、“GrAppv”、“MIS_Status”）

df_selected &lt;- SBAnational[selected_columns]

low_var_cols &lt;- 插入符::nzv(df_selected)

df_no_zv &lt;- df_selected[, -low_var_cols]

设置.种子(123)

split_index &lt;- createDataPartition(df_no_zv$MIS_Status, p = 0.7, list = FALSE)
train_data &lt;- df_no_zv[split_index, ]
test_data &lt;- df_no_zv[-split_index, ]

data_recipe &lt;- 配方(MIS_Status ~ ., data = train_data) %&gt;%
  step_center(all_numeric(), -all_outcomes()) %&gt;%
  step_scale(all_numeric(), -all_outcomes()) %&gt;%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)

data_prep &lt;- 准备（data_recipe，训练 = train_data）

train_processed &lt;- 烘焙（data_prep，new_data = train_data）
test_processed &lt;- 烘焙（data_prep，new_data = test_data）

在本次迭代结束时，我发现自己拥有一个包含 113,214 个实例和 4,382 个变量的训练集。
此时，我决定首先通过调整参数来应用线性SVM。代码如下所示：
svm_model &lt;- svm(MIS_Status ~ ., data = train_processed, kernel = “线性”)

une_result &lt;- 调整（svm，MIS_Status ~ .，data = train_processed，kernel =“线性”，范围 = list（成本 = c（0.1, 1, 10, 100, 1000）））

问题是调优已运行超过 12 小时，并且我已遇到此警告消息 6 次：
&lt;前&gt;&lt;代码&gt;&gt; une_result &lt;- 调整（svm，MIS_Status ~ .，data = train_processed，kernel =“线性”，范围 = list（成本 = c（0.1, 1, 10, 100, 1000）））

警告：达到最大迭代次数

警告：达到最大迭代次数

警告：达到最大迭代次数

警告：达到最大迭代次数

警告：达到最大迭代次数

警告：达到最大迭代次数

你能给我一些建议来简化一切吗？
（MacBook Air M2 16GB 512GB）]]></description>
      <guid>https://stackoverflow.com/questions/77851738/warning-reaching-max-number-of-iterations-during-tuning-of-parameters-for-lin</guid>
      <pubDate>Sat, 20 Jan 2024 16:11:36 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用 google colab pro 作为人工智能服务器吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77851136/can-i-use-google-colab-pro-as-a-ai-server</link>
      <description><![CDATA[我正在尝试编写一个程序，该程序从用户那里获取文本，并在文本中找到食物的名称并将其提供给用户。现在我想知道我可以使用google colab pro作为人工智能服务器吗？
从用户那里获取文本并向其发送答案的最佳方式是什么？
快速 API 好吗？
我可以在 google colab 中运行我的小 llama 应用程序，但我需要在服务器上运行我的应用程序，以便我可以随时使用它。
这是小美洲驼代码：
从变压器导入 AutoTokenizer、FlaxLlamaForCausalLM

tokenizer = AutoTokenizer.from_pretrained(“afmck/testing-llama-tiny”)
模型 = FlaxLlamaForCausalLM.from_pretrained(“afmck/testing-llama-tiny”)

input = tokenizer(“你好，我的狗很可爱”，return_tensors =“np”)
输出=模型（**输入）

# 检索下一个令牌的日志
next_token_logits = 输出.logits[:, -1]
]]></description>
      <guid>https://stackoverflow.com/questions/77851136/can-i-use-google-colab-pro-as-a-ai-server</guid>
      <pubDate>Sat, 20 Jan 2024 13:07:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 TreeExplainer 绘制瀑布图</title>
      <link>https://stackoverflow.com/questions/77851097/waterfall-plot-with-treeexplainer</link>
      <description><![CDATA[在 SHAP 中使用 TreeExplainer，我无法绘制瀑布图。
错误消息：
&lt;前&gt;&lt;代码&gt;---&gt; 17 shap.plots.waterfall(shap_values[0], max_display=14)
类型错误：瀑布图需要一个“Explanation”对象作为
`shap_values` 参数。

由于我的模型是基于树的，因此我使用 TreeExplainer（因为使用 xgb.XGBClassifier）。
如果我使用Explainer而不是TreeExplainer，我可以绘制瀑布图。
我的代码如下：
导入 pandas 作为 pd

数据 = {
    &#39;a&#39;: [1, 2, 3, 3, 2, 1, 4, 5, 6, 7, 8, 1, 2, 3, 3, 2, 1, 4, 5, 6, 7, 8],
    &#39;b&#39;: [2, 1, 2, 3, 4, 6, 5, 8, 7, 9, 10, 2, 1, 2, 3, 4, 6, 5, 8, 7, 9, 10],
    &#39;c&#39;: [1, 5, 2, 4, 3, 9, 6, 8, 7, 10, 1, 1, 5, 2, 4, 3, 9, 6, 8, 7, 10, 1],
    &#39;d&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1],
    &#39;e&#39;: [1, 2, 3, 4, 3, 2, 1, 5, 4, 2, 1, 1, 2, 3, 4, 3, 2, 1, 5, 4, 2, 1],
    &#39;f&#39;: [1, 1, 2, 1, 2, 2, 3, 3, 3, 2, 1, 1, 1, 2, 1, 2, 2, 3, 3, 3, 2, 1],
    &#39;g&#39;: [3, 3, 2, 1, 3, 2, 1, 1, 1, 2, 2, 3, 3, 2, 1, 3, 2, 1, 1, 1, 2, 2],
    &#39;h&#39;: [1, 2, 1, 2, 3, 4, 5, 3, 4, 5, 5, 1, 2, 1, 2, 3, 4, 5, 3, 4, 5, 5],
    &#39;我&#39;: [1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 6, 1, 2, 1, 2, 3, 4, 5, 6, 5, 4, 6],
    &#39;j&#39;: [5, 4, 3, 2, 1, 1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1, 1, 2, 3, 4, 5, 6],
    &#39;k&#39;: [3, 3, 2, 1, 4, 3, 2, 2, 2, 1, 1, 3, 3, 2, 1, 4, 3, 2, 2, 2, 1, 1],
    &#39;r&#39;: [1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1]
}

df = pd.DataFrame(数据)

X = df.iloc[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]
y = df.iloc[:,11]

从 sklearn.model_selection 导入 train_test_split，GridSearchCV
X_train、X_test、y_train、y_test = train_test_split(X、y、test_size = 0.30、random_state = 42)

将 xgboost 导入为 xgb
从sklearn.metrics导入accuracy_score，confusion_matrix，classification_report
从 sklearn.model_selection 导入 GridSearchCV

参数网格 = {
    &#39;最大深度&#39;：[6]，
    “n_估计器”：[500]，
    “学习率”：[0.3]
}


grid_search_xgboost = GridSearchCV(
    估计器 = xgb.XGBClassifier(),
    参数网格=参数网格，
    CV = 3,
    详细 = 2,
    职位数 = -1
）

grid_search_xgboost.fit(X_train, y_train)

print(&quot;最佳参数：&quot;, grid_search_xgboost.best_params_)
best_model_xgboost = grid_search_xgboost.best_estimator_

导入形状

解释器 = shap.TreeExplainer(best_model_xgboost)
shap_values = 解释器.shap_values(X_train)

shap.summary_plot（shap_values，X_train，plot_type =“条”）

shap.summary_plot(shap_values, X_train)

对于 X_train.columns 中的名称：
    shap.dependence_plot(名称, shap_values, X_train)

shap.force_plot(explainer.expected_value, shap_values[0], X_train.iloc[0], matplotlib=True)

shap.decision_plot(explainer.expected_value, shap_values[:10], X_train.iloc[:10])

shap.plots.waterfall(shap_values[0], max_display=14)

问题出在哪里？]]></description>
      <guid>https://stackoverflow.com/questions/77851097/waterfall-plot-with-treeexplainer</guid>
      <pubDate>Sat, 20 Jan 2024 12:55:45 GMT</pubDate>
    </item>
    <item>
      <title>当机器学习回归的许多预测结果几乎相同时该怎么办？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77850915/what-to-do-when-machine-learning-regression-results-in-almost-the-same-number-fo</link>
      <description><![CDATA[我是机器学习新手
我从日常监控中获得了 200 行数据集 14 每日传感器读数（第 1 天到第 14 天），因为这是一种罕见的现象，所以我只获得 200 个数据集行。
目标范围（来自数据集列）为 100 至 2500 和 600 标准差，呈正态分布。
回归结果总是接近 1000（这是目标数据的平均值/平均值和中位数）。
我已经尝试过的：

（再次）检查并清理数据
使用 gridsearch 寻找最佳模型参数
尝试 3 种不同的模型（AdaBoost、梯度提升、随机森林）
回归结果仍然接近平均值/平均值&amp;目标数据集的中值。
]]></description>
      <guid>https://stackoverflow.com/questions/77850915/what-to-do-when-machine-learning-regression-results-in-almost-the-same-number-fo</guid>
      <pubDate>Sat, 20 Jan 2024 11:59:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow 训练文本生成模型时出现 MemoryError</title>
      <link>https://stackoverflow.com/questions/77850543/memoryerror-when-training-text-generation-model-with-tensorflow</link>
      <description><![CDATA[我在尝试使用 TensorFlow 训练文本生成模型时遇到内存错误。该错误发生在 model.fit 阶段，特别是在尝试创建形状 (401233, 12512) 和数据类型 int32 的数组时。系统无法分配所需的 18.7 GiB 内存。
file_path = &#39;story.txt&#39;
最大读取字节数 = 2 * 1024 * 1024 # 5 MB
​
打开（file_path，&#39;r&#39;，encoding=&#39;utf-8&#39;）作为文件：
    数据 = file.read(max_bytes_to_read)
分词器 = 分词器()
​
语料库 = data.lower().split(“\n”)
​
tokenizer.fit_on_texts（语料库）
​
总单词数 = len(tokenizer.word_index) + 1
​
输入序列 = []
​
对于语料库中的行：
  token_list = tokenizer.texts_to_sequences([行])[0]
  对于范围内的 i(1, len (token_list))：
    n_gram_sequence = token_list[:i+1]
    input_sequences.append(n_gram_sequence)
​
max_seq_len = max([len(i) for i in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences,maxlen= max_seq_len,padding=“pre”))
​
xs = 输入序列[:,:-1]
标签 = input_sequences[:,-1]
fromtensorflow.keras.utils import to_categorical # 专门针对 to_categorical 函数
ys = tf.keras.utils.to_categorical(标签, num_classes=total_words, dtype=&#39;int32&#39;)



模型=顺序（）
model.add(嵌入(total_words, 240, input_length=max_seq_len-1))
model.add(双向(LSTM(150)))
model.add（密集（total_words，激活=&#39;softmax&#39;））
​
​
model.compile(optimizer=“adam”,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
model.fit(xs, ys, epochs=10)
​

这是在 jupyter 笔记本中运行 model.fit 代码单元时出现的错误
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
MemoryError Traceback（最近一次调用）
[32] 中的单元格，第 8 行
      4 model.add（密集（total_words，激活=&#39;softmax&#39;））
      7 model.compile(optimizer=“adam”,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
----&gt; 8 model.fit(xs, ys, epochs=10)

文件 ~\anaconda3\lib\site-packages\keras\src\utils\traceback_utils.py:70，位于filter_traceback..error_handler(*args, **kwargs)
     67、filtered_tb = _process_traceback_frames（e.__traceback__）
     68 # 要获取完整的堆栈跟踪，请调用：
     69 # `tf.debugging.disable_traceback_filtering()`
---&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
     71 最后：
     72 删除filtered_tb

文件~\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py:86，在convert_to_eager_tensor(value, ctx, dtype)中
     66 &quot;&quot;&quot;将给定的“value”转换为“EagerTensor”。
     67
     68 请注意，此函数可以返回已创建常量的缓存副本
   （...）
     80 类型错误：如果 `dtype` 与 t 的类型不兼容。
     81、“”“”
     82 if isinstance(值, np.ndarray):
     83 # 显式创建一个副本，因为 EagerTensor 可能共享底层
     84 # 内存与输入数组。如果没有此副本，用户将能够
     85 # 在创建后通过更改输入数组来修改 EagerTensor。
---&gt; 86 值 = value.copy()
     87 if isinstance(value, ops.EagerTensor):
     88 如果 dtype 不是 None 并且 value.dtype != dtype:

MemoryError：无法为形状为 (401233, 12512) 和数据类型为 int32 的数组分配 18.7 GiB
]]></description>
      <guid>https://stackoverflow.com/questions/77850543/memoryerror-when-training-text-generation-model-with-tensorflow</guid>
      <pubDate>Sat, 20 Jan 2024 10:00:47 GMT</pubDate>
    </item>
    <item>
      <title>决策树回归和决策树分类器之间的区别[关闭]</title>
      <link>https://stackoverflow.com/questions/77850372/difference-between-decision-tree-regression-and-decision-tree-classifier-properl</link>
      <description><![CDATA[当输出变量是分类变量时使用分类树，而当输出变量是连续变量时使用回归树。我知道这是正确的，但我对这个主题还没有清楚的了解，如果您帮助我举例，请帮助我更好地获得最佳答案。]]></description>
      <guid>https://stackoverflow.com/questions/77850372/difference-between-decision-tree-regression-and-decision-tree-classifier-properl</guid>
      <pubDate>Sat, 20 Jan 2024 09:00:05 GMT</pubDate>
    </item>
    <item>
      <title>图文分类模型架构[关闭]</title>
      <link>https://stackoverflow.com/questions/77848540/image-text-classification-model-architecture</link>
      <description><![CDATA[图文分类的架构正确与否？
假设我们的数据集包含图像和相应的标题和类标签。现在我们的问题是模型以（图像+标题）作为输入并预测class_label。现在，如果我们使用图像和文本嵌入，然后对文本嵌入使用注意机制，并将处理后的文本嵌入与图像嵌入连接起来，然后使用 3 个完全连接的层并预测类标签（多类分类，如“汽车”、“踏板车”） 、“巴士”等）总共有 25 个班级。这个架构正确与否？]]></description>
      <guid>https://stackoverflow.com/questions/77848540/image-text-classification-model-architecture</guid>
      <pubDate>Fri, 19 Jan 2024 19:56:27 GMT</pubDate>
    </item>
    <item>
      <title>不平衡数据的隔离森林和SHAP过程</title>
      <link>https://stackoverflow.com/questions/77837871/isolation-forest-and-shap-process-for-imbalanced-data</link>
      <description><![CDATA[我想对类别为正常 99.93% 异常 0.07% 的数据使用隔离森林，并使用 SHAP 检查异常数据的特征之间的相关性。
于是，我参考了以下kaggle网站上的方法继续学习：https://www.kaggle.com/code/sabanasimbutt/anomaly-detection-using-unsupervised-techniques)
在这个 Kaggle 站点上，Class = 0 的数据和 Class = 1 的数据划分如下：
inliers = df[df.Class==0]
ins = inliers.drop([&#39;Class&#39;], axis=1)

离群值 = df[df.Class==1]
outs = outliers.drop([&#39;Class&#39;], axis=1)

为了查看学习中使用的特征与异常值（“Class == 1”的数据）之间的相关性，我按如下方式使用了 SHAP，并通过蜂群图检查了相关性。
&lt;前&gt;&lt;代码&gt;状态= 42
ISF = 隔离森林（random_state=状态）
ISF.fit(ins)

normal_isf = ISF.predict(ins)
欺诈_isf = ISF.predict(outs)

导入形状
解释器 = shap.TreeExplainer(ISF)
shap_values = 解释器(outs)
shap.plots.beeswarm(shap_values)

代码工作正常，但 beeswarn 的结果与我使用 shap_values ​​=explainer(ins) 时类似，即正常数据。我是不是搞错了？]]></description>
      <guid>https://stackoverflow.com/questions/77837871/isolation-forest-and-shap-process-for-imbalanced-data</guid>
      <pubDate>Thu, 18 Jan 2024 08:20:12 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pre-Train Bert 进行二元分类的 Shap 值：如何提取摘要图？</title>
      <link>https://stackoverflow.com/questions/77785423/shap-value-for-binary-classification-using-pre-train-bert-how-to-extract-summar</link>
      <description><![CDATA[我使用预训练 bert 模型进行二元分类。用小数据训练我的模型后，我想提取这样的摘要图 我想要的图&lt; /a&gt;.然而，我想用文字来代替这些重要的特征。
但是，我不确定一切都好，因为 shap_value 的形状只是二维的。其实，这是有道理的。尽管如此，我没有得到图表，因为如果我使用这段代码，我遇到了两个问题：
shap.summary_plot(shap_values[:,:10],feature_names=feature_importance[&#39;features&#39;].tolist(),features=comments_text)`

问题太不明智了：如果我用 shap_values 或 shap_values[0] 或  更改 shap_values[:,:10] shap_values.values vb.我总是遇到
516：断言 len(shap_values.shape) != 1，“汇总图需要一个矩阵
shap_values，而不是向量。” ==&gt; AssertionError：摘要图需要一个矩阵
shap_values，不是向量。

（拳头问题）
顺便说一句，我的 shap_value 由 10 个输入（shape_value.shape）组成。如果我选择范围从 1 到 147 的最大值，那么绘制图表就一切顺利。然而，此时，该图不合适：我的图仅由蓝点组成（-第二个问题-）。像这样只有蓝色。
注意：shap_values[:,:10]如果数字（10）改变不同的数字，图表显示不同的单词，但图表的总数相同（最多 20）。只有部分词序可以改变。
最小可重现示例：
&lt;前&gt;&lt;代码&gt;导入nlp
将 numpy 导入为 np
将 pandas 导入为 pd
将 scipy 导入为 sp
进口火炬
进口变压器
进口火炬
导入形状

# 加载 BERT 情感分析模型
tokenizer = Transformers.DistilBertTokenizerFast.from_pretrained(
    “distilbert-base-uncased”
）
模型 = Transformers.DistilBertForSequenceClassification.from_pretrained(
    “distilbert-base-uncased-finetuned-sst-2-english”
).cuda()


如果 torch.cuda.is_available():
    设备 = torch.device(“cuda”)
    print(&#39;我们将使用 GPU:&#39;, torch.cuda.get_device_name(0))

别的：
    print(&#39;没有可用的 GPU，请使用 CPU。&#39;)
    设备 = torch.device(“cpu”)

定义 f(x):
    # 对批量句子进行编码
    输入 = tokenizer.batch_encode_plus(x.tolist(), max_length=450,add_special_tokens=True, return_attention_mask=True,padding=&#39;max_length&#39;,truncation=True,return_tensors=&#39;pt&#39;)

    # 将张量发送到与模型相同的设备
    input_ids = 输入[&#39;input_ids&#39;].to(设备)
    注意掩码 = 输入[&#39;注意掩码&#39;].to(设备)
    ＃ 预测
    使用 torch.no_grad()：
        输出=模型（input_ids，attention_mask=attention_masks）[0].detach（）.cpu（）.numpy（）
    分数 = (np.exp(输出).T / np.exp(输出).sum(-1)).T
    val = sp.special.logit(scores[:, 1]) # 使用 1 与其余 logit 单位
    返回值
# 使用 token masker 构建一个解释器
解释器 = shap.Explainer(f, tokenizer )

imdb_train = nlp.load_dataset(“imdb”)[“火车”]
shap_values = 解释器(imdb_train[:10],fixed_context=1,batch_size=16)
队列 = {“”：shap_values}
team_labels = 列表(cohorts.keys())
team_exps = 列表(cohorts.values())
对于范围内的 i(len(cohort_exps))：
    如果 len(cohort_exps[i].shape) == 2:
        队列_exps[i] = 队列_exps[i].abs.mean(0)
特征=cohort_exps[0].data
特征名称=同类群组表达式[0].特征名称
#values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))], dtype=object)
值 = np.array([cohort_exps[i].i 在范围内的值(len(cohort_exps))])
feature_importance = pd.DataFrame(list(zip(feature_names, sum(values))), columns=[&#39;features&#39;, &#39;importance&#39;])
feature_importance.sort_values(by=[&#39;重要性&#39;], 升序=False, inplace=True)
shap.summary_plot(shap_values[:,:10],feature_names=feature_importance[&#39;features&#39;].tolist(),features=imdb_train[&#39;text&#39;][10:20],show=False)


上面的代码产生相同的结果。我花了大约200台电脑，但没有成功:(。我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/77785423/shap-value-for-binary-classification-using-pre-train-bert-how-to-extract-summar</guid>
      <pubDate>Tue, 09 Jan 2024 08:49:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 object_Detector.EfficientDetLite4Spec tensorflow lite 继续使用检查点进行训练</title>
      <link>https://stackoverflow.com/questions/69444878/how-to-continue-training-with-checkpoints-using-object-detector-efficientdetlite</link>
      <description><![CDATA[很重要的是，我已经在 config.yaml 中设置了我的 EfficientDetLite4 模型“grad_checkpoint=true”。并且它已经成功生成了一些检查点。但是，当我想继续基于这些检查点进行训练时，我不知道如何使用它们。
每次我训练模型时，它都会从头开始，而不是从检查点开始。
下图是我的colab文件系统结构：

下图显示了我的检查点存储的位置：

以下代码显示了我如何配置模型以及如何使用模型进行训练。
将 numpy 导入为 np
导入操作系统

从 tflite_model_maker.config 导入 ExportFormat
从 tflite_model_maker 导入 model_spec
从 tflite_model_maker 导入 o​​bject_ detector

将张量流导入为 tf
断言 tf.__version__.startswith(&#39;2&#39;)

tf.get_logger().setLevel(&#39;错误&#39;)
从absl导入日志记录
日志记录.set_verbosity（日志记录.错误）

训练数据、验证数据、测试数据 =
    object_Detector.DataLoader.from_csv(&#39;csv_path&#39;)

规格 = object_ detector.EfficientDetLite4Spec(
    uri=&#39;/内容/模型&#39;,
    model_dir=&#39;/content/drive/MyDrive/MathSymbolRecognition/CheckPoints/&#39;,
    hparams=&#39;grad_checkpoint=true,策略=gpus&#39;,
    epochs=50，batch_size=3，
    steps_per_execution=1， moving_average_decay=0，
    var_freeze_expr=&#39;(efficientnet|fpn_cells|resample_p6)&#39;,
    tflite_max_detections=25，策略=spec_strategy
）

model = object_ detector.create(train_data, model_spec=spec, batch_size=3,
    train_whole_model=True，validation_data=validation_data）
]]></description>
      <guid>https://stackoverflow.com/questions/69444878/how-to-continue-training-with-checkpoints-using-object-detector-efficientdetlite</guid>
      <pubDate>Tue, 05 Oct 2021 04:21:43 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 cross_val_score 与 cross_val_predict</title>
      <link>https://stackoverflow.com/questions/66034846/machinelearning-cross-val-score-vs-cross-val-predict</link>
      <description><![CDATA[在构建通用评估工具时，我遇到了以下问题，其中 cross_val_score.mean() 给出的结果与 cross_val_predict 略有不同。
为了计算测试分数，我有以下代码，它计算每次折叠的分数，然后计算所有折叠的平均值。
testing_score = cross_val_score(clas_model, algo_features, algo_featurest, cv=folds).mean()

为了计算 tp、fp、tn、fn，我有以下代码，它计算所有折叠的这些指标（我假设是总和）。
test_clas_predictions = cross_val_predict(clas_model, algo_features, algo_featurest, cv=folds)
test_cm = fusion_matrix(algo_featurest, test_clas_predictions)
test_tp = test_cm[1][1]
test_fp = test_cm[0][1]
test_tn = test_cm[0][0]
test_fn = test_cm[1][0]

这段代码的结果是：
 算法测试 test_tp test_fp test_tn test_fn
5 高斯NB 0.719762 25 13 190 71
4 Logistic回归 0.716429 24 13 190 72
2 决策树分类器 0.702381 38 33 170 58
0 梯度提升分类器 0.682619 37 36 167 59
3 KNeighborsClassifier 0.679048 36 36 167 60
1 随机森林分类器 0.675952 40 43 160 56

因此，选择第一行 cross_val_score.mean() 给出 0.719762 （测试）并通过计算分数 25+190/25+13+190+71=0.719063545150... ((tp+tn)/(tp+tn +fp+fn)) 略有不同。
我有机会从 quora 的一篇文章中读到这一点：“在 cross_val_predict() 中，元素的分组方式与 cross_val_score() 中的稍有不同。这意味着当您使用这些函数计算相同的指标时，您可能会得到不同的结果。”
这背后有什么特殊原因吗？]]></description>
      <guid>https://stackoverflow.com/questions/66034846/machinelearning-cross-val-score-vs-cross-val-predict</guid>
      <pubDate>Wed, 03 Feb 2021 20:02:56 GMT</pubDate>
    </item>
    <item>
      <title>卡方检验的计算</title>
      <link>https://stackoverflow.com/questions/64271948/computation-of-chi-square-test</link>
      <description><![CDATA[我试图了解如何针对以下输入计算 chi2 函数。
sklearn.feature_selection.chi2([[1, 2, 0, 0, 1],
                                [0, 0, 1, 0, 0],
                                [0, 0, 0, 2, 1]], [真, 假, 假])

对于 chi2，我得到以下结果 [2, 4, 0.5, 1, 0.25]。
我已经在维基百科上找到了以下计算公式（x_i也被称为观察值，m_i被称为预期值），但我不知道如何应用它。

我的理解是，我有三个类别的输入（行）和四个特征（列），chi2函数返回特征和类别之间是否存在相关性。第一列表示的特征在第一个类别中出现两次，并且 chi2 值为 4。
我想我已经弄清楚了

各列彼此独立，这是有道理的
如果我省略第三行，预期值将是列的总和，观察值只是相应单元格中的值，但这不适用于最后一列
带有 False 的两列似乎以某种方式组合在一起，但我还没有弄清楚如何组合。

如果有人可以帮助我，我将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/64271948/computation-of-chi-square-test</guid>
      <pubDate>Thu, 08 Oct 2020 23:27:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 cross_val_predict 与 cross_val_score 时，scikit-learn 分数不同</title>
      <link>https://stackoverflow.com/questions/62201597/scikit-learn-scores-are-different-when-using-cross-val-predict-vs-cross-val-scor</link>
      <description><![CDATA[我预计这两种方法都会返回非常相似的错误，有人可以指出我的错误吗？
计算 RMSE...
rf = RandomForestRegressor(random_state=555，n_estimators=100，max_深度=8)
rf_preds = cross_val_predict(rf, train_, 目标, cv=7, n_jobs=7)
print(&quot;使用 cv preds 的 RMSE 分数：{:0.5f}&quot;.format(metrics.mean_squared_error(targets, rf_preds, squared=False)))

分数 = cross_val_score(rf, train_, 目标, cv=7, 评分=&#39;neg_root_mean_squared_error&#39;, n_jobs=7)
print(&quot;使用 cv_score 的 RMSE 分数: {:0.5f}&quot;.format(scores.mean() * -1))


使用 cv preds 的 RMSE 分数：0.01658
使用 cv_score 的 RMSE 分数：0.01073
]]></description>
      <guid>https://stackoverflow.com/questions/62201597/scikit-learn-scores-are-different-when-using-cross-val-predict-vs-cross-val-scor</guid>
      <pubDate>Thu, 04 Jun 2020 18:21:18 GMT</pubDate>
    </item>
    </channel>
</rss>