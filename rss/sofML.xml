<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 22 Mar 2024 15:13:40 GMT</lastBuildDate>
    <item>
      <title>Azure ML Studio Web 服务始终返回相同的预测</title>
      <link>https://stackoverflow.com/questions/78207131/azure-ml-studio-web-service-always-returns-the-same-prediction</link>
      <description><![CDATA[我目前正在为我的课程开发一个小型项目，但遇到了障碍，希望能得到一些帮助。在 Azure 机器学习工作室中训练 SVM 模型并将其部署为 Web 服务后，我遇到了一个特殊问题 - 无论输入数据如何，该服务都会返回相同的预测。
这里有人遇到过类似的问题或者对可能出现的问题有什么建议吗？我已无计可施，任何建议或见解将不胜感激！
预先感谢您的帮助！
https://gallery.cortanaintelligence.com/Experiment/Binary-Classifier -SVM-Web
https://gallery.cortanaintelligence.com/Experiment/Binary-Classifiers-SVM
这是我到目前为止所做的事情：
确保所有预处理步骤与模型训练阶段使用的步骤相同（包括数据标准化和缺失值处理）。
仔细检查模型是否使用输入数据进行评分。
验证了 Web 服务的配置，特别是在构建响应时以确保不返回静态值。
确保输入数据的格式与预期模式匹配。
该模型在 ML Studio 环境中表现良好，并根据测试数据进行准确预测。但是，部署的 Web 服务似乎没有反映此行为并输出恒定值。]]></description>
      <guid>https://stackoverflow.com/questions/78207131/azure-ml-studio-web-service-always-returns-the-same-prediction</guid>
      <pubDate>Fri, 22 Mar 2024 15:06:25 GMT</pubDate>
    </item>
    <item>
      <title>对于用于训练的完全相同的数据，张量流预测较低</title>
      <link>https://stackoverflow.com/questions/78206996/tensorflow-prediction-is-low-for-the-exact-same-data-thats-been-used-for-traini</link>
      <description><![CDATA[张量流2.16.1
为了检测静音（小噪音），我使用多个波形文件训练了张量流模型。
所有波形文件均为单声道、16kHz、PCM16 格式。
MFCC 是使用 python_speech_features 包每 0.1 秒的数据计算一次。
mfcc_feat = np.mean(mfcc(block, # 1600 个浮点数的数组，可存储 0.1 秒的数据
                         16000,
                         数量=20，
                         温伦=160/16000,
                         胜步=160/16000,
                         nfilt=20),
                         轴=0）

最终数组 X 的形状为 (1500, 20)，并且使用以下方法定义、编译和训练模型
# 定义并编译模型
模型 = models.Sequential([
    Layers.Input(shape=(20,)), # 输入形状是 20(mfcc) 个浮点数的一维数组
    层.Dense(128, 激活=&#39;relu&#39;),
    层数.密集(20),
    层.Dense(X.shape[0], 激活=&#39;softmax&#39;)
]）

# 编译模型
model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;sparse_categorical_crossentropy&#39;,
              指标=[&#39;准确性&#39;])

# 训练模型
model.fit(X, y, epochs=10, verbose=0)


第一个图是测试声音文件，最后一个词的发音。
接下来的两个是使用阈值0.8的预测结果，它们是不同的。
用于训练的文件之一是从测试文件本身中提取的。
第二个图中的一个小块和第三个图中开头的两个小块与测试文件 (silence_test2) 中的相同，但预测值仍然较低 (&lt;0.1, 0.4, 0.7)。我可以想到三种可能性：

mfcc 计算不正确
模型定义不正确
训练时需要更多数据

原因是什么以及如何改进预测？除此之外，一般欢迎提出建议。]]></description>
      <guid>https://stackoverflow.com/questions/78206996/tensorflow-prediction-is-low-for-the-exact-same-data-thats-been-used-for-traini</guid>
      <pubDate>Fri, 22 Mar 2024 14:44:06 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn：训练期间的 ValueError 特征形状与验证期间的特征形状不同</title>
      <link>https://stackoverflow.com/questions/78206595/sklearn-valueerror-feature-shape-during-training-is-different-than-feature-sha</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78206595/sklearn-valueerror-feature-shape-during-training-is-different-than-feature-sha</guid>
      <pubDate>Fri, 22 Mar 2024 13:33:48 GMT</pubDate>
    </item>
    <item>
      <title>在 R 中从 PNG 中删除文本</title>
      <link>https://stackoverflow.com/questions/78206448/removing-text-from-a-png-in-r</link>
      <description><![CDATA[我一直在 R 中使用一个名为“aweSOM”的库。它是一个基于为自组织地图 (SOM) 提供 HTML 交互式视觉效果的库。
aweSOM 比类似的 SOM 包产生更好的视觉效果，所以我会使用它。然而，问题有两个：

HTML 交互式视觉效果不适合发布。
当我另存为 PNG 时，图像上仍保留有（交互式）文本
PNG“将鼠标悬停在绘图上以获取信息。”

因此，我想知道是否可以编写一个函数，将“绘图”保存为 PNG，但没有上面的交互式文本？
因此，有效地编写一个函数，仅保存特定大小的正方形，从而省略文本？
非常感谢您的反馈和帮助。
install.packages(“aweSOM”)
图书馆（aweSOM）

full.data &lt;- iris
train.data &lt;- full.data[, c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;)]
train.data &lt;- 规模(train.data)

设置.种子(1465)
init &lt;- somInit(train.data, 4, 4)
iris.som &lt;- kohonen::som(train.data, grid = kohonen::somgrid(4, 4, “六边形”),
                         rlen = 100, 阿尔法 = c(0.05, 0.01), 半径 = c(2.65,-2.65),
                         dist.fcts = “sumofsquares”, init = init)

superclust_pam &lt;- cluster::pam(iris.som$codes[[1]], 3)
superclasses_pam &lt;- superclust_pam$聚类

###########
#带有互动文本的问题情节
##########

情节&lt; - aweSOMplot（som = iris.som，类型=“云”，数据= full.data，
           变量= c(“种类”,“萼片.长度”,“萼片.宽度”,
                         “花瓣长度”、“花瓣宽度”)、
           超类 = superclasses_pam)


所有有关修复可视化的帮助将不胜感激。这里有一个小插曲：
https://cran .r-project.org/web/packages/aweSOM/vignettes/aweSOM.html#the-awesom-package
]]></description>
      <guid>https://stackoverflow.com/questions/78206448/removing-text-from-a-png-in-r</guid>
      <pubDate>Fri, 22 Mar 2024 13:05:59 GMT</pubDate>
    </item>
    <item>
      <title>如何使用CapsNet进行回归？</title>
      <link>https://stackoverflow.com/questions/78206055/how-to-use-capsnet-for-regression</link>
      <description><![CDATA[我正在尝试训练用于回归任务的 CapsNet 模型。我目前用于分类的代码如下。将激活函数更改为线性是否会采用回归代码，或者我还应该修改自定义胶囊层吗？
input_shape= (28, 28, 1)
n_class = len(np.unique(np.argmax(y_train, 1)))
路线 = 3

x = 层.Input(shape=input_shape)

conv1=layers.Conv2D(filters=256,kernel_size=9,strides=1,padding=&#39;valid&#39;,activation=&#39;relu&#39;,name=&#39;conv1&#39;)(x)
PrimaryCaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding=&#39;valid&#39;)
digitalcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, 路由=路由,
                            名称=&#39;digitcaps&#39;）（primarycaps）
out_caps = 长度(name=&#39;capsnet&#39;)(digitcaps)

# 解码器
y = 图层. 输入(形状=(n_class,))
masked_by_y = Mask()([digitcaps, y])
masked = Mask()(数字大写字母)

# 训练和预测中的共享解码器模型
解码器 = models.Sequential(name=&#39;解码器&#39;)
解码器.add（layers.Dense（512，激活=&#39;relu&#39;，input_dim = 16 * n_class））
解码器.add（layers.Dense（1024，激活=&#39;relu&#39;））
解码器.add（layers.Dense（np.prod（input_shape），激活=&#39;sigmoid&#39;））
解码器.add(layers.Reshape(target_shape=input_shape, name=&#39;out_recon&#39;))

model = models.Model([x, y], [out_caps, 解码器(masked_by_y)])
#model = models.Model(x, [out_caps, 解码器(屏蔽)])

# 编译模型
model.compile(optimizer=optimizers.Adam(lr=1e-3),loss=[margin_loss,&#39;mse&#39;],metrics={&#39;capsnet&#39;:&#39;accuracy&#39;})
模型.summary()

# 没有数据增强的训练：
model.fit([x_train，y_train]，[y_train，x_train]，batch_size=50，epochs=10，validation_data=[[x_test，y_test]，[y_test，x_test]])
]]></description>
      <guid>https://stackoverflow.com/questions/78206055/how-to-use-capsnet-for-regression</guid>
      <pubDate>Fri, 22 Mar 2024 11:54:59 GMT</pubDate>
    </item>
    <item>
      <title>Walker 2D Pybullet 环境</title>
      <link>https://stackoverflow.com/questions/78205900/walker-2d-pybullet-environment</link>
      <description><![CDATA[我正在尝试通过实现 SAC 算法来解决 Walker2DBulletEnv-v0 问题。在前 700 集左右，机器人保持平衡，返回约 600 分。这些结果是有希望的吗？我应该继续运行该程序以改善网络，还是应该尝试调整超参数。此外，如果这些结果没有希望，那么哪些结果被认为是“好的”；]]></description>
      <guid>https://stackoverflow.com/questions/78205900/walker-2d-pybullet-environment</guid>
      <pubDate>Fri, 22 Mar 2024 11:27:38 GMT</pubDate>
    </item>
    <item>
      <title>变压器网络中的位置编码[关闭]</title>
      <link>https://stackoverflow.com/questions/78205746/position-encoding-in-transformer-networks</link>
      <description><![CDATA[我目前正在深入研究 Transformer 网络的实现细节，特别是位置编码方面。我遇到了一些让我困惑的概念，非常感谢您的澄清。

交替正弦和余弦函数：在位置编码中，为什么 Transformer 网络利用交替正弦和余弦函数来编码位置信息？使用单个函数不足以达到此目的吗？

除以 10000^(2i/d)：我注意到在 Transformer 网络的许多实现中，在应用三角函数之前，位置值会除以 10000^(2i/d)。这种特殊划分背后的理由是什么？它如何影响编码过程？

将位置值添加到嵌入矩阵：参考“attention is all you need”（注意就是你所需要的）论文中提到需要将位置值添加到嵌入矩阵中。然而，当我用一个简单的例子尝试这个过程时，例如[我是一个男孩]，添加位置值后生成的嵌入值保持不变。例如，[3 1 4 2] 仍为 [5 5 5 5]。


有人可以阐明为什么会出现这种情况，并提供如何将位置值添加到嵌入矩阵的更清晰的解释吗？]]></description>
      <guid>https://stackoverflow.com/questions/78205746/position-encoding-in-transformer-networks</guid>
      <pubDate>Fri, 22 Mar 2024 11:00:27 GMT</pubDate>
    </item>
    <item>
      <title>R 混淆矩阵 - 错误：“数据”和“参考”应该是具有相同级别的因素</title>
      <link>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</link>
      <description><![CDATA[尽管还有其他针对相同错误消息的报告，但没有一个对我的情况有帮助。
我已经准备了自己的数据，分割如下，但无法获得混淆矩阵。
test_index &lt;- createDataPartition(y =workingData$PM10, times = 1, p = 0.5, list = FALSE)
train_set &lt;-工作数据[-test_index,]
test_set &lt;-工作数据[test_index,]

train_knn &lt;- train(PM10 ~. , method= &quot;knn&quot; , data = train_set)

y_hatknn &lt;- 预测(train_knn, train_set, type = “raw”)

fusionMatrix(y_hatknn, test_set$PM10)

上面最后一行给出
错误：“data”和“reference”应该是具有相同级别的因素。

如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</guid>
      <pubDate>Fri, 22 Mar 2024 09:39:08 GMT</pubDate>
    </item>
    <item>
      <title>时间序列滚动窗口功能[关闭]</title>
      <link>https://stackoverflow.com/questions/78204216/time-series-rolling-windows-feature</link>
      <description><![CDATA[如果我根据我的销售额（目标）列创建滚动平均值特征，是否有必要对其进行移动？
举个例子：
假设我的数据集中有第 01~10 天。例如，如果我在第 10 天的行中创建 7 天的平均滚动窗口列，它将考虑第 7 天作为该行的值来计算滚动平均值。现在，如果我要预测第 11 天，即明天，我需要这一天的销售值才能获得滚动平均值，这没有意义。
因此，我认为始终获取最后 7 天而不考虑当前的情况更有意义。
有人可以帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/78204216/time-series-rolling-windows-feature</guid>
      <pubDate>Fri, 22 Mar 2024 05:35:21 GMT</pubDate>
    </item>
    <item>
      <title>在 aws elastic beanstalk 中创建环境时出现 Docker 错误</title>
      <link>https://stackoverflow.com/questions/78204096/docker-error-while-creating-environment-in-aws-elastic-beanstalk</link>
      <description><![CDATA[我正在 Beanstalk 中使用 Docker 部署机器学习模型。首先，我将 Docker 镜像（包含我的 ML 模型）上传到 Docker Hub。然后，我使用 docker-compose.yml 将其部署到 Beanstalk 中。在 Beanstalk 中，我使用 Docker 作为平台，并且我的模型需要 GPU 支持。为此，我使用了深度学习 AMI GPU CUDA 11.5.2 (Amazon Linux 2) 20230104，它是通过 NVIDIA CUDA、cuDNN、NCCL、GPU 驱动程序、Docker、NVIDIA-Docker 和 EFA 支持构建的。但是，当我使用此配置构建环境时，遇到以下错误：
**[ERROR]** 执行命令 [app-deploy] 期间发生错误
- [跟踪 healthd 中的 pid]。停止运行该命令。错误：更新进程
[docker eb-docker-compose-events eb-docker-compose-log eb-docker-events cfn-hup healthd]
pid 符号链接失败，错误读取 pid 源文件 /var/pids/docker.pid 失败
错误：打开/var/pids/docker.pid：没有这样的文件或目录。

意味着我的环境正在构建，但它给出的错误消息如下：
Env 构建成功，但有一些错误。

我在 eb.engine.log 中发现了此错误消息。
此外，我通过 SSH 检查了 EC2 实例，它显示 NVIDIA 驱动程序、NVIDIA CUDA 和 Docker 已安装（使用以下命令验证：nvidia-smi、docker -v）。我尝试了多种不同的深度学习 AMI，但所有这些 AMI 都遇到了同样的问题。另外，在尝试不同的 AMI 时，我注意到一件奇怪的事情：如果我使用默认设置（例如使用默认 Docker AMI 的 Docker 平台）构建环境，它会成功构建，不会出现任何错误。但是，当我在配置中传递不同的 AMI ID 时，无法正确构建环境。
如何解决这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/78204096/docker-error-while-creating-environment-in-aws-elastic-beanstalk</guid>
      <pubDate>Fri, 22 Mar 2024 04:49:51 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么类型的人工智能模型来生成练习题？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78203711/what-type-of-ai-model-should-i-use-to-generate-practice-questions</link>
      <description><![CDATA[我有一组英语多项选择题，我想使用 AI 生成更多问题来测验自己。我知道网上有一些平台可以实现这一点，但我想挑战自己，创建自己的简单人工智能架构。在对它进行一些英语问题训练后，我希望它能够生成新问题来帮助我学习。
我应该使用哪种机器学习/智能模型作为基线？]]></description>
      <guid>https://stackoverflow.com/questions/78203711/what-type-of-ai-model-should-i-use-to-generate-practice-questions</guid>
      <pubDate>Fri, 22 Mar 2024 02:17:43 GMT</pubDate>
    </item>
    <item>
      <title>Seq2Seq LSTM 无法正确学习</title>
      <link>https://stackoverflow.com/questions/78201576/seq2seq-lstm-not-learning-properly</link>
      <description><![CDATA[我正在尝试使用 Pytorch 中的 LSTM 解决 seq-to-seq 问题。具体来说，我采用 5 个元素的序列来预测接下来的 5 个元素。我关心的是数据转换。我有大小为 [bs, seq_length, features] 的张量，其中 seq_length = 10 和 features = 1。每个特征都是一个介于 0 和 3 之间的整数（两者都包含在内）。
我认为输入数据必须使用 MinMaxScaler 转换为浮点范围 [0, 1]，以便使 LSTM 学习过程更容易。之后，我应用一个线性层，它将隐藏状态转换为相应的输出，其大小为特征。我在 Pytorch 中对 LSTM 网络的定义：
类 LSTM(nn.Module):
    def __init__(自身、input_dim、hidden_​​dim、output_dim、num_layers、dropout_prob):
        super(LSTM, self).__init__()
        self.lstm_layer = nn.LSTM(input_dim,hidden_​​dim,num_layers,dropout=dropout_prob)
        self.output_layer = nn.Linear（hidden_​​dim，output_dim）

    ...

    def 向前（自身，X）：
        out, (隐藏, 单元格) = self.lstm_layer(X)
        输出 = self.output_layer(输出)
        返回

我用来进行训练循环的代码如下：
def train_loop(t, checkpoint_epoch, dataloader, model, loss_fn, optimizationr):
    大小 = len(dataloader.dataset)
    对于批处理，枚举中的 X（数据加载器）：
        X = X[0].type(torch.float).to(设备)

        # X = torch.Size([batch_size, 10, input_dim])
        # 将序列拆分为输入和目标
        输入 = 变换(X[:, :5, :]) # 输入 = [batch_size, 5, input_dim]
        目标 = 变换(X[:, 5:, :]) # 目标 = [batch_size, 5, input_dim]

        # 预测（前向传递）
        使用自动转换（）：
            pred = 模型(输入) # pred = [batch_size, 5, input_dim]
            损失 = loss_fn(pred, 目标)

        # 反向传播
        优化器.zero_grad()
        scaler.scale(loss).backward()
        缩放器.step（优化器）
        定标器.update()

        如果批次 % 100 == 0:
            损失，当前 = loss.item(), 批次 * len(X)
            #print(f&quot;当前损耗:{loss:&gt;7f},[{current:&gt;5d}/{size:&gt;5d}]&quot;)

        # 删除变量并清空缓存
        del X、输入、目标、预测
        torch.cuda.empty_cache()

    回波损耗

我用于预处理数据的代码：
def main():
    代理数量 = 2
    # 打开HDF5文件
    将 h5py.File(&#39;dataset_&#39; + str(num_agents) + &#39;UAV.hdf5&#39;, &#39;r&#39;) 作为 f：
        # 访问数据集
        数据 = f[&#39;数据&#39;][:]
        # 转换为 PyTorch 张量
        data_tensor = torch.tensor(数据)

        大小 = data_tensor.size()
        序列长度 = 10
        重塑 = data_tensor.view(-1, 大小[2], 大小[3])

        r_size = reshape.size()
        重塑 = 重塑[:, :, 1:]
        reshape_v2 = reshape.view(r_size[0], -1)

        数据集 = create_dataset(reshape_v2.numpy(), seq_length)

        f.close()

    数据集 = TensorDataset(数据集)

    # 将数据集分为训练集和验证集
    train_size = int(0.8 * len(dataset)) # 80% 用于训练
    val_size = len(dataset) - train_size # 20% 用于验证
    train_dataset, val_dataset = random_split(数据集, [train_size, val_size])

    train_dataloader = DataLoader（train_dataset，batch_size = params [&#39;batch_size&#39;]，shuffle = True，pin_memory = True）
    val_dataloader = DataLoader（val_dataset，batch_size = params [&#39;batch_size&#39;]，shuffle = False，pin_memory = True）

尝试这个，模型没有正确学习，所以我想也许可以直接计算 targets （范围 [0, 1] 内的浮点值）和 pred 之间的损失code&gt; （我认为由于 LSTM 层的 tanh 激活函数，浮点值在 [-1, 1] 范围内），具有不同的尺度可能是错误的。然后，我尝试在前向传递中的线性层之后应用 sigmoid 激活函数，但也没有正确学习。我尝试了许多超参数组合的执行，但没有一个产生“正常”的结果。训练曲线。我还附上了 5000 epoch 的屏幕截图来说明训练过程：

我的问题是：

我的训练过程中似乎存在什么问题？
我所说的话是否被认为是错误的？
]]></description>
      <guid>https://stackoverflow.com/questions/78201576/seq2seq-lstm-not-learning-properly</guid>
      <pubDate>Thu, 21 Mar 2024 16:43:58 GMT</pubDate>
    </item>
    <item>
      <title>运行 XGBoost 时不使用 GPU</title>
      <link>https://stackoverflow.com/questions/76827589/gpu-not-used-when-running-xgboost</link>
      <description><![CDATA[我在 ML 世界中还是个新手，对于使用 XGBoost 模型完成的项目，我尝试使用 GPU 进行 GridSearch 和参数调整。不幸的是，我感觉我的 GPU 没有被使用，因为显示的有关每次折叠运行时间的信息在 CPU 上约为 0,1 秒，使用“gpu_hist”参数时约为 0,9 秒。
我安装了 CUDA 工具包，所以这不是问题。
这是传递“gpu_hist”树方法参数时发生的情况：
gpu_hist
如果没有它，就会发生以下情况：
无 GPU 参数
您知道如何让 GPU 工作吗？
此问题是否是由于我的 CPU 中的集成显卡造成的？
我在使用 jupyter Notebook 和 pycharm 时遇到同样的问题。
我的CPU：AMD Ryzen 9 7900x
我的 GPU：Nvidia GeForce RTX 3070
提前谢谢您！
我尝试运行 pytorch 命令
导入火炬
use_cuda = torch.cuda.is_available()

如果使用_cuda：
    print(&#39;__CUDNN 版本:&#39;, torch.backends.cudnn.version())
    print(&#39;__CUDA 设备数量:&#39;, torch.cuda.device_count())
    print(&#39;__CUDA 设备名称:&#39;,torch.cuda.get_device_name(0))
    print(&#39;__CUDA 设备总内存 [GB]:&#39;,torch.cuda.get_device_properties(0).total_memory/1e9)

看看我的 GPU 是否被识别，但一切似乎都正常，因为我得到：
__CUDNN 版本：8700
__CUDA 设备数量：1
__CUDA 设备名称：NVIDIA GeForce RTX 3070
__CUDA 设备总内存 [GB]：8.589410304]]></description>
      <guid>https://stackoverflow.com/questions/76827589/gpu-not-used-when-running-xgboost</guid>
      <pubDate>Thu, 03 Aug 2023 11:19:41 GMT</pubDate>
    </item>
    <item>
      <title>为什么仅在 CNN 中对通道进行批量归一化</title>
      <link>https://stackoverflow.com/questions/45799926/why-batch-normalization-over-channels-only-in-cnn</link>
      <description><![CDATA[我想知道，在卷积神经网络中，批量归一化是否应该分别应用于每个像素，或者我应该取每个通道的像素平均值？
我在Tensorflow的tf.layers.batch_normalization 建议对通道执行 bn，但如果我没记错的话，我使用了其他方法，效果很好。]]></description>
      <guid>https://stackoverflow.com/questions/45799926/why-batch-normalization-over-channels-only-in-cnn</guid>
      <pubDate>Mon, 21 Aug 2017 14:40:57 GMT</pubDate>
    </item>
    <item>
      <title>在稀疏机器学习中生成 uint64 或 uint32 特征 id 的良好哈希函数</title>
      <link>https://stackoverflow.com/questions/38654267/good-hash-function-for-generating-uint64-or-uint32-feature-id-in-sparse-machine</link>
      <description><![CDATA[我在特征字符串（例如查询和标题）上使用哈希函数来生成稀疏特征 id。该哈希函数应该是高效的，并且在 uint64 或 uint32 上具有良好的分布。有人能给我一些建议吗？
我在java中测试了两种方法。
第一个是java hashCode。我发现它在类似的 geohash 字符串上有很多冲突。
第二个是打击。它比 hashCode 有更多的冲突。
公共静态长基因(String s) {
        长哈希 = 5381；
        整数c;
        for (int i = 0; i &lt; s.length(); i++) {
            c = s.charAt(i);
            哈希 = ((哈希 &lt;&lt; 5) + 哈希) + c;
        }
        返回哈希值；
    }

（请忽略java long和c++ uint之间的区别。没问题）]]></description>
      <guid>https://stackoverflow.com/questions/38654267/good-hash-function-for-generating-uint64-or-uint32-feature-id-in-sparse-machine</guid>
      <pubDate>Fri, 29 Jul 2016 08:40:19 GMT</pubDate>
    </item>
    </channel>
</rss>