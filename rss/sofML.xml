<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 12 Apr 2024 12:24:33 GMT</lastBuildDate>
    <item>
      <title>这是对不平衡数据集进行过采样交叉验证的正确方法吗？</title>
      <link>https://stackoverflow.com/questions/78316022/is-this-the-right-way-to-do-cross-validation-with-oversampling-on-imbalance-data</link>
      <description><![CDATA[def stratified_cross_validation_metrics(模型, X, y, method=&#39;&#39;):
    指标={
        &#39;准确性&#39;： []，
        &#39;精确&#39;： []，
        &#39;记起&#39;： []，
        &#39;f1_score&#39;：[]
    }
    
    kf = StratifiedKFold(n_splits=10, shuffle=False)
    
    对于 kf.split(X, y) 中的 train_index、test_index：
        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]
        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]
        
        if method.lower() == &#39;adasyn&#39;:
            X_train_cv, y_train_cv = adasyn.fit_resample(X_train_cv, y_train_cv)
            X_test_cv, y_test_cv = adasyn.fit_resample(X_test_cv, y_test_cv)
        elif method.lower() == &#39;smote&#39;:
            X_train_cv, y_train_cv = smote.fit_resample(X_train_cv, y_train_cv)
            X_test_cv, y_test_cv = smote.fit_resample(X_test_cv, y_test_cv)
        
        model.fit(X_train_cv, y_train_cv)

        y_pred_cv = model.predict(X_test_cv)

        准确度=准确度_分数（y_test_cv，y_pred_cv）
        精度 = precision_score(y_test_cv, y_pred_cv, 平均值=&#39;加权&#39;)
        召回率=召回率（y_test_cv，y_pred_cv，平均值=&#39;加权&#39;）
        f1 = f1_score(y_test_cv, y_pred_cv, 平均值=&#39;加权&#39;)

        指标[&#39;准确度&#39;].append(准确度)
        指标[&#39;精度&#39;].append(精度)
        指标[&#39;recall&#39;].append(recall)
        指标[&#39;f1_score&#39;].append(f1)

    print(&quot;10 倍分层交叉验证的平均指标：&quot;)
    print(&quot;准确率：&quot;, np.mean(metrics[&#39;accuracy&#39;]))
    print(&quot;精度：&quot;, np.mean(metrics[&#39;精度&#39;]))
    print(&quot;召回率：&quot;, np.mean(metrics[&#39;recall&#39;]))
    print(&quot;F1 分数：&quot;, np.mean(metrics[&#39;f1_score&#39;]))

    df = pd.DataFrame(指标)
    
    返回df

我读到，当你使用过采样方法进行交叉验证时，你不应该首先过采样，这样它就不会泄漏到测试数据，所以我按照所示的方式执行函数，这是正确的方法还是我做错了什么？
对于不平衡数据集使用什么指标平均值比较好？]]></description>
      <guid>https://stackoverflow.com/questions/78316022/is-this-the-right-way-to-do-cross-validation-with-oversampling-on-imbalance-data</guid>
      <pubDate>Fri, 12 Apr 2024 11:37:37 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：操作数无法与形状一起广播 (10,1024) (1024,1)</title>
      <link>https://stackoverflow.com/questions/78316002/valueerror-operands-could-not-be-broadcast-together-with-shapes-10-1024-1024</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78316002/valueerror-operands-could-not-be-broadcast-together-with-shapes-10-1024-1024</guid>
      <pubDate>Fri, 12 Apr 2024 11:33:12 GMT</pubDate>
    </item>
    <item>
      <title>根据之前的元素，甚至不基于之前的元素，使用 LSTM 预测下一个元素</title>
      <link>https://stackoverflow.com/questions/78315830/predicting-next-elements-with-an-lstm-based-on-previous-ones-or-even-based-on</link>
      <description><![CDATA[我有一个时间序列问题，其中包括使用 LSTM 预测价格。该数据集是从 Python 库 yfinance 导入的。我在内置的极客教程中使用了示例代码Pytorch，并设法理解一切。我认为我不明白的唯一具体部分可以在这段代码片段中看到：
# 定义要预测的未来时间步数
预测步数 = 30

# 转换为 NumPy 并删除单一维度
sequence_to_plot = X_test.squeeze().cpu().numpy()

# 使用最后 30 个数据点作为起点
历史数据=序列到图[-1]
打印（历史数据.形状）

# 初始化一个列表来存储预测值
预测值 = []

# 使用训练好的模型来预测未来值
使用 torch.no_grad()：
    对于 _ 在范围内（num_forecast_steps*2）：
        # 准备历史数据张量
        历史数据张量 = torch.as_tensor(历史数据).view(1, -1, 1).float().to(设备)
        # 使用模型预测下一个值
        预测值 = 模型(历史数据张量).cpu().numpy()[0, 0]

        # 将预测值添加到forecasted_values列表中
        Forecasted_values.append(predicted_value[0])

        # 通过删除最旧的值并添加预测值来更新历史数据序列
        历史数据 = np.roll(历史数据, 移位=-1)
        历史数据[-1] = 预测值

        
# 生成未来日期
最后日期 = test_data.index[-1]

# 生成接下来的 30 个日期
future_dates = pd.date_range(start=last_date + pd.DateOffset(1), period=30)

# 将原始索引与未来日期连接起来
组合索引 = test_data.index.append(future_dates)

根据教程内容，使用滚动预测。这意味着，如果我是对的，带有输入元素的数组将用作预测下一个元素的窗口，该窗口将附加到窗口，以预测下一个元素，依此类推。我的问题可能与概念或代码的其他部分有关，但我不太理解这行代码：
# 将预测值追加到 Forecasted_values 列表中
Forecasted_values.append(predicted_value[0])

如果我想根据之前的 30 个元素来预测下一个元素，为什么我应该采用预测数组的第一个预测元素？ （对我来说，这意味着前 30 个输入序列的第二个元素）。如果我需要基于动态生成的值构建一个序列窗口，并且之前没有先前的元素，我该如何修改此示例？]]></description>
      <guid>https://stackoverflow.com/questions/78315830/predicting-next-elements-with-an-lstm-based-on-previous-ones-or-even-based-on</guid>
      <pubDate>Fri, 12 Apr 2024 11:01:41 GMT</pubDate>
    </item>
    <item>
      <title>读取包含字符串数据的 csv 文件时出现错误</title>
      <link>https://stackoverflow.com/questions/78315561/getting-error-while-reading-csv-file-with-string-data</link>
      <description><![CDATA[我正在编写 DeepLearning4j 代码来读取下面的 csv 文件：
键，值
员工 ID、同事 ID
员工证、工人证
员工证,员工证

下面是我尝试过的Java代码
 数据集 allData1;
    尝试 (RecordReader recordReader1 = new CSVRecordReader(1, &#39;,&#39;)) {
        recordReader1.initialize(new FileSplit(new ClassPathResource(“EmployeeData.txt”).getFile()));

        DataSetIterator 迭代器 = new RecordReaderDataSetIterator(recordReader1, 3, 1,1, true);
        allData1 = 迭代器.next();
    }

但是，导致以下错误..
线程“main”中出现异常java.lang.NumberFormatException：对于输入字符串：“员工 ID”
    在 java.base/jdk.internal.math.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2054)
    在 java.base/jdk.internal.math.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
    在 java.base/java.lang.Double.parseDouble(Double.java:651)
    在 org.datavec.api.writable.Text.toDouble(Text.java:590)
    在 org.datavec.api.util.ndarray.RecordConverter.toMinibatchArray(RecordConverter.java:207)
    在 org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.next（RecordReaderMultiDataSetIterator.java:153）
    在 org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next（RecordReaderDataSetIterator.java:346）
    在 org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next（RecordReaderDataSetIterator.java:421）
    在 org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:53)

我哪里出错了？
此代码可以正常处理 CSV 文件中的 int 和 float 值...]]></description>
      <guid>https://stackoverflow.com/questions/78315561/getting-error-while-reading-csv-file-with-string-data</guid>
      <pubDate>Fri, 12 Apr 2024 10:10:50 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python 检测 PDF 中选定的文本？</title>
      <link>https://stackoverflow.com/questions/78314645/how-to-detect-selected-text-from-a-pdf-using-python</link>
      <description><![CDATA[我有一个 Python 程序，可以使用 PDF 查看器打开 PDF 文件。查看 PDF 时，我用鼠标光标选择一些文本。有没有办法让我的 Python 程序检测我选择的文本？
我知道 PyMuPDF、PyPDF2 和 pdfplumber 等库可用于从 PDF 中提取文本。不过，我正在专门寻找一种方法来检测我在查看 PDF 时以交互方式选择的文本。
如果无法从鼠标光标直接检测，是否有任何替代方法或解决方法可以实现类似的结果？
有什么见解或建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78314645/how-to-detect-selected-text-from-a-pdf-using-python</guid>
      <pubDate>Fri, 12 Apr 2024 06:57:54 GMT</pubDate>
    </item>
    <item>
      <title>不同职业的人发布的文本数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/78314226/the-texts-dataset-posted-by-people-of-different-occupations</link>
      <description><![CDATA[最近，我一直在从事一些 NLP 任务：根据不同职业群体的帖子预测职业。我搜索了 Kaggle 等多个平台，但找不到这样的数据集。我怎样才能找到这个数据集？
我尝试了kaggle和google数据集，但不起作用]]></description>
      <guid>https://stackoverflow.com/questions/78314226/the-texts-dataset-posted-by-people-of-different-occupations</guid>
      <pubDate>Fri, 12 Apr 2024 04:54:13 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证可视化功能</title>
      <link>https://stackoverflow.com/questions/78313982/cross-validation-visualization-mulfunctions</link>
      <description><![CDATA[我受到 scikit-learn 的 交叉验证可视化指南，用于可视化每个 CV 分割中训练和测试索引的分布：
cmap_data = plt.cm.Paired
cmap_cv = plt.cm.coolwarm
defplot_cv_indices（cv，X，y，组，ax，n_splits，lw = 10）：
    “”“”为交叉验证对象的索引创建样本图。“”“”

    # 为每个 CV 分割生成训练/测试可视化
    对于 ii，枚举（cv.split（X = X，y = y，groups = group））中的（tr，tt）：
        # 用训练/测试组填写索引
        索引 = np.array([np.nan] * len(X))
        索引[tt] = 1
        索引[tr] = 0

        # 可视化结果
        斧头.分散（
            范围（len（索引）），
            [ii + 0.5] * len(索引),
            c=指数，
            标记=“_”，
            lw=lw,
            cmap=cmap_cv,
            vmin=-0.2,
            vmax=1.2，
        ）

    # 最后绘制数据类和组
    斧头.分散（
        范围(len(X)), [ii + 1.5] * len(X), c=y, 标记=“_”, lw=lw, cmap=cmap_data
    ）

    斧头.分散（
        范围(len(X)), [ii + 2.5] * len(X), c=组, 标记=“_”, lw=lw, cmap=cmap_data
    ）

    # 格式化
    yticklabels = list(range(n_splits)) + [“类”, “组”]
    斧头.设置（
        yticks=np.arange(n_splits + 2) + 0.5,
        yticklabels=yticklabels,
        xlabel=&quot;样本索引&quot;,
        ylabel=“CV迭代”，
        ylim=[n_splits + 2.2, -0.2],
        xlim=[0, 100],
    ）
    ax.set_title(“{}”.format(type(cv).__name__), fontsize=15)
    plt.show()

from sklearn.datasets import make_classification
从 sklearn.model_selection 导入 TimeSeriesSplit、KFold

图, ax = plt.subplots(figsize=(12, 5))
X, y = make_classification(
    n_样本=1000，
    n_特征=10，
    n_信息=3，
    n_冗余=0，
    n_重复=0，
    n_classes=2,
    随机状态=42，
    随机播放=假，
）
绘图CV索引（
    TimeSeriesSplit(n_splits=5, 间隙=10),
    X=X,
    y=y,
    组=无，
    斧头=斧头，
    n_splits=5,
）

上面的代码给了我：

我在寻找：

预期图的想法是，该图成功地可视化了每个分组中训练集和测试集之间的差距。此外，我在正常的 KFold 上运行了一个测试用例，而且 plot_cv_indices 函数似乎也无法正常运行。]]></description>
      <guid>https://stackoverflow.com/questions/78313982/cross-validation-visualization-mulfunctions</guid>
      <pubDate>Fri, 12 Apr 2024 03:12:54 GMT</pubDate>
    </item>
    <item>
      <title>手语项目（Ai）[已关闭]</title>
      <link>https://stackoverflow.com/questions/78313876/sign-language-project-ai</link>
      <description><![CDATA[目前，我们是一个团队在做毕业设计，做一个手语应用，当我们完成2个模型的训练和评估后，我们想把这2个模型放在main函数中，以便交给后端团队将模型链接到应用程序。这里的问题是
我们如何制作主函数以及完成它需要哪些步骤和库主函数？]]></description>
      <guid>https://stackoverflow.com/questions/78313876/sign-language-project-ai</guid>
      <pubDate>Fri, 12 Apr 2024 02:31:40 GMT</pubDate>
    </item>
    <item>
      <title>为什么SAC算法在计算q函数的损失时只采样一个“下一步动作”？</title>
      <link>https://stackoverflow.com/questions/78313821/why-sac-algorithm-only-sample-one-next-action-when-computing-the-loss-of-q-fun</link>
      <description><![CDATA[在SAC的原论文中，Q-value的损失函数写为：

在实际算法实现中，将V(s_{t+1})项替换为Q(s_{t+1}, a{t+1})，损失函数写为：
next_q1 = self.networks.q1_target(obs2, next_act)
next_q2 = self.networks.q2_target(obs2, next_act)
next_q = torch.min(next_q1, next_q2)
备份 = rew + (1 - 完成) * self.gamma * (next_q - self.__get_alpha() * next_logp)

我知道这两个公式在期望上是相等的，但第二个公式显然具有更高的方差，因为它只需要 a_{t+1} 的一个样本。考虑到 SAC 的策略是随机的，我想知道在这种情况下对多个操作（或 Q_next）进行采样是否会更好？
虽然会花费更多的时间，但可以显着降低Q的梯度方差。
有关于这个问题的研究或论文吗？最好能结合实验结果进行理论解释。]]></description>
      <guid>https://stackoverflow.com/questions/78313821/why-sac-algorithm-only-sample-one-next-action-when-computing-the-loss-of-q-fun</guid>
      <pubDate>Fri, 12 Apr 2024 02:10:58 GMT</pubDate>
    </item>
    <item>
      <title>训练时间融合网络 - 多少数据？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78313682/training-temporal-fusion-network-how-much-data</link>
      <description><![CDATA[关于多少数据足以训练时间融合变压器，是否有任何经验法则？更具体地说，我有大约 20 个特征，数据集大约有 50 万。这足够吗？对于多大的模型来说？
或者，我有大约 20 个产品，每个产品大约有 500k 行，而不是为每个“产品”训练不同的模型。也许我应该为所有产品训练一个模型以获得一个模型，然后定制它？
我知道这是非常高水平和模糊的，我只是在寻找一些经验法则和指导 - 我是否处于正确的范围，或者我应该去哪里？]]></description>
      <guid>https://stackoverflow.com/questions/78313682/training-temporal-fusion-network-how-much-data</guid>
      <pubDate>Fri, 12 Apr 2024 01:13:06 GMT</pubDate>
    </item>
    <item>
      <title>在运行 ML 项目时如何使用存储在 google Drive 中的数据集？</title>
      <link>https://stackoverflow.com/questions/78312337/how-can-i-use-the-dataset-that-is-stored-in-google-drive-while-running-ml-projec</link>
      <description><![CDATA[我正在运行 SoccerNet 项目，该项目为足球视频生成字幕。
我正在尝试将路径传递到存储数据集的 Google 云端硬盘，即 https://drive.google.com/drive/folders/{folder_id}
我正在运行的命令如下：
python main.py --SoccerNet_path=“https://drive.google.com/drive/folders/{folder_id}” --model_name=new_model --features=baidu_soccer_embeddings.npy --framerate=1 --pool=NetVLAD --window_size_caption=45 --window_size_spotting=15 --NMS_window=30 --num_layers=4 --first_stage=caption --pretrain --GPU=0

我收到操作系统错误：如下
OSError: [Errno 22] 无效参数: &#39;https:https://drive.google.com/drive/folders/{folder_id}?usp=drive_link\\england_epl\\2014-2015\\2015 -02-21 - 18-00 切尔西 1 - 1 伯恩利\\1_baidu_soccer_embeddings.npy&#39;

我尝试使用 google colab，但我没有得到 colab 中预期的输出，因为它没有生成应包含预期字幕的 json 文件。
我正在使用 VS 代码。]]></description>
      <guid>https://stackoverflow.com/questions/78312337/how-can-i-use-the-dataset-that-is-stored-in-google-drive-while-running-ml-projec</guid>
      <pubDate>Thu, 11 Apr 2024 18:15:53 GMT</pubDate>
    </item>
    <item>
      <title>如何让 Matrox Model Finder 在单个图像中多次查找同一模型？</title>
      <link>https://stackoverflow.com/questions/78311681/how-do-i-make-the-matrox-model-finder-look-for-the-same-model-multiple-times-in</link>
      <description><![CDATA[我是 Matrox 新手，所以这可能是一个非常初学者的问题。
我有一个托盘，里面有多个相同型号的物品。当我将 ModelFinder 步骤添加到程序中时，我添加了我正在寻找的模型，但它只显示了我注册的模型，我猜测是因为相机的扭曲。我如何让 Matrox 知道还有更多项目并且它们也是同一型号？

我添加了一个必须找到它的搜索区域，并且我选择了它的选项来查找所有出现的情况，但它只显示了一个而不是实际存在的 3/4。
]]></description>
      <guid>https://stackoverflow.com/questions/78311681/how-do-i-make-the-matrox-model-finder-look-for-the-same-model-multiple-times-in</guid>
      <pubDate>Thu, 11 Apr 2024 16:02:48 GMT</pubDate>
    </item>
    <item>
      <title>TensorBoard HParams 未显示超参数调整的准确性指标</title>
      <link>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</link>
      <description><![CDATA[我正在 TensorFlow 中进行超参数调整，并使用 TensorBoard 中的 HParams 插件设置了一个实验来记录不同的配置。我的模型正在使用 dropout 和学习率的变化进行训练，并且我正在记录这些参数以及模型的准确性。但是，当我打开 TensorBoard 并导航到 HParams 仪表板时，不会显示与每个试验相关的准确性指标。该表正确显示了超参数，但“准确性”列为空，即使我的代码使用“准确性”作为指标来编译模型并使用 hp.KerasCallback 进行日志记录。我已经验证模型训练是否正确，并且标量仪表板等其他 TensorBoard 功能显示了各个时期的准确性趋势。我正在寻求帮助来理解为什么 HParams 表中没有显示准​​确性以及如何解决此问题。
图片：准确度列中缺少值
我使用 TensorBoard 的 HParams 进行超参数调整的代码：
从tensorboard.plugins.hparams导入api作为hp
将张量流导入为 tf
从tensorflow.keras.layers导入Conv2D、MaxPooling2D、Dense、Flatten、Dropout

# 定义超参数
HP_DROPOUT = hp.HParam(&#39;dropout&#39;, hp.Discrete([0.2, 0.3, 0.4]))
HP_LEARNING_RATE = hp.HParam(&#39;learning_rate&#39;, hp.Discrete([1e-2, 1e-3]))

# 设置日志记录
log_dir = &#39;./tensorboard/nn_1&#39;
使用 tf.summary.create_file_writer(log_dir).as_default()：
    hp.hparams_config(
        hparams=[HP_DROPOUT, HP_LEARNING_RATE],
        指标=[hp.Metric(&#39;准确度&#39;,display_name=&#39;准确度&#39;)]
    ）

# 训练函数
def train_test_model(hparams, session_num):
    model_name = f“model_1_session_{session_num}”
    print(f&quot;使用超参数 {hparams} 训练 {model_name}...&quot;)
    模型 = tf.keras.Sequential([
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        MaxPooling2D(pool_size=(2, 2)),
        展平（），
        密集（10，激活=&#39;softmax&#39;）
    ]）
    模型.编译(
        损失=&#39;分类交叉熵&#39;，
        优化器=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),
        指标=[&#39;准确性&#39;]
    ）

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f&#39;{log_dir}/{model_name}&#39;)
    hparams_callback = hp.KerasCallback(writer=f&#39;{log_dir}/{model_name}&#39;, hparams=hparams)

    模型.拟合(
        x_train_reshape, y_train_,
        纪元=3，
        验证数据=（x_val_reshape，y_val），
        回调=[hparams_callback，tensorboard_callback]
    ）

# 对每组超参数进行训练
会话编号 = 0
对于 HP_DROPOUT.domain.values 中的 dropout_rate：
    对于 HP_LEARNING_RATE.domain.values 中的learning_rate：
        hparams = {
            HP_DROPOUT：辍学率，
            HP_LEARNING_RATE：学习率，
        }
        train_test_model(hparams, session_num)
        会话编号 += 1

]]></description>
      <guid>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</guid>
      <pubDate>Tue, 09 Apr 2024 12:14:56 GMT</pubDate>
    </item>
    <item>
      <title>使用 Docker 和 Flask 进行机器学习的性能问题</title>
      <link>https://stackoverflow.com/questions/50464643/performance-issues-with-machine-learning-using-docker-and-flask</link>
      <description><![CDATA[我有一些应用于 json 文件的 python3 代码，代码中有一些神经网络和随机森林。我将代码放入 Docker 容器中，但注意到这些 ML 任务在不使用 Docker 的情况下比使用 Docker 运行得更快。在 Docker 中，我使用 Flask 加载 json 文件并运行代码。当然，我在本地和 Docker 内部使用了相同版本的 python 模块，这些是：

theano 0.8.2
keras 2.0.5
scikit-learn 0.19.0

另外，Flask 是

0.12

起初，我认为 theano 在有 Docker 的情况下可能会使用不同的资源，但它同时运行单 CPU 和单线程。它也没有使用我的 GPU。当我意识到我的随机森林在 Docker 中运行速度也变慢时，我意识到这可能不是 theano。以下是我执行的一系列测试（我对每个测试进行了多次测试，我报告了平均时间，因为这些测试是稳定的）
没有 Docker，没有 Flask：

任务 1（theano + keras 代码）：1.0s 
任务 2（theano + keras 代码）：0.7s
任务 3（scikit-learn 代码）：0.25 秒

Docker (cpus=1) + Flask (调试模式 = True):

T1：6.5秒
T2：2.2秒
T3：0.58s

Docker (cpus=2) + Flask (调试模式 = True):

T1：5.5秒
T2：1.4秒
T3：0.55s

Docker (cpus=2) + Flask (调试模式 = False)：

T1：4.5秒
T2：1.2秒
T3：0.5秒

Docker (cpus=2)（无 Flask，仅调用本地完成的 json 文件）：

T1：2.8s
T2：1.1秒
T3：0.5秒

Flask（调试模式 = True）（无 Docker 容器）：

T1：2.8s
T2：1.5秒
T3：0.2秒

我猜 cpu=1 与 cpu=2 只是将一个 cpu 分配给代码，而第二个 cpu 只是接管一些其他工作。显然，当不使用 Flask 或 Docker 时，时间会有所减少，但我仍然无法达到没有 Docker 和 Flask 的速度。有谁猜到为什么会发生这种情况吗？
这是我们如何使用 Flask 运行应用程序的最小代码块
api = Flask(__name__)
pipeline = Pipeline() # 调用多个任务的私有类

@api.route(&quot;/&quot;,methods=[&#39;POST&#39;])
def 条目():
    数据 = request.get_json(force=True)
    数据 = pipeline.process(数据)
    # 这会调用不同的定时任务

如果 __name__ == &quot;__main__&quot;:
    api.run（调试= True，主机=&#39;0.0.0.0&#39;，线程= False）


PS。如果问题缺少任何内容，请原谅我，这是我的第一个 StackOverflow 问题]]></description>
      <guid>https://stackoverflow.com/questions/50464643/performance-issues-with-machine-learning-using-docker-and-flask</guid>
      <pubDate>Tue, 22 May 2018 09:48:58 GMT</pubDate>
    </item>
    <item>
      <title>使用 word2vec 对类别中的单词进行分类</title>
      <link>https://stackoverflow.com/questions/47666699/using-word2vec-to-classify-words-in-categories</link>
      <description><![CDATA[背景
我有带有一些示例数据的向量，每个向量都有一个类别名称（地点、颜色、名称）。
[&#39;约翰&#39;,&#39;杰伊&#39;,&#39;丹&#39;,&#39;内森&#39;,&#39;鲍勃&#39;] -&gt; “名字”
[&#39;黄色&#39;, &#39;红色&#39;, &#39;绿色&#39;] -&gt; &#39;颜色&#39;
[&#39;东京&#39;,&#39;北京&#39;,&#39;华盛顿&#39;,&#39;孟买&#39;] -&gt; “地方”

我的目标是训练一个模型，该模型采用新的输入字符串并预测它属于哪个类别。例如，如果新输入是“紫色”，那么我应该能够将“颜色”预测为正确的类别。如果新输入是“卡尔加里”，它应该将“地点”预测为正确的类别。
方法
我做了一些研究并发现了Word2vec。这个库有一个我可以使用的“相似性”和“最相似性”函数。所以我想到的一种强力方法如下：

接受新的输入。
计算它与每个向量中每个单词的相似度并取平均值。

例如，对于输入“粉红色”，我可以计算其与向量“名称”中单词的相似度，取平均值，然后对其他 2 个向量也执行此操作。给我最高相似度平均值的向量将是输入所属的正确向量。
问题
鉴于我在 NLP 和机器学习方面的知识有限，我不确定这是否是最好的方法，因此我正在寻求有关更好方法的帮助和建议来解决我的问题。我愿意接受所有建议，也请指出我可能犯的任何错误，因为我是机器学习和 NLP 世界的新手。]]></description>
      <guid>https://stackoverflow.com/questions/47666699/using-word2vec-to-classify-words-in-categories</guid>
      <pubDate>Wed, 06 Dec 2017 04:16:35 GMT</pubDate>
    </item>
    </channel>
</rss>