<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Wed, 19 Feb 2025 09:19:26 GMT</lastBuildDate>
    <item>
      <title>我是否将神经网络锁定错误？预测有时似乎是随机的</title>
      <link>https://stackoverflow.com/questions/79450720/am-i-chaining-neural-network-wrong-predictions-seem-random-sometimes</link>
      <description><![CDATA[我试图用2层（1st有2个神经元，2个神经元）来制作神经网络，这些神经元可以透过4个数字列表，如果第一个和第三个数字等于1，则答案应为1 ，在其他情况下= 0。
我根本不使用任何AI框架库，只需随机进行随机权重，而Numpy Yo会造成正方形损失。我知道，如果我不将种子设置为随机化剂，那将永远是随机的，但是我的答案太随机了。
我有测试数据集，其中第一个和最后一个列表应为1，第二和第三列是0：
  x2 = [
    [1，0，1，0]，＃答案：1
    [1，0，0，0]，＃答案：0
    [0，1，1，0]，＃答案：0
    [1，0，1，1]＃答案：1
这是给出的
 
但是，每次我运行代码时，第一和第二列表的答案比第三和第四列更随机。有时第二个答案比第一大。
我试图从以下方式更改权重调整：
  err3 = y1 [i]  -  neuron3.Acivate（py3 [i]）
err1 = y1 [i] -py1 [i]
err2 = y1 [i] -py2 [i]
neuron1.w [k] += x1 [i] [k] * numpy.mean（err3 ** 2） * err1 * lr
neuron2.w [k] += x1 [i] [k] * numpy.mean（err3 ** 2） * err2 * lr
＃（x1是培训数据集，[i]是列表索引，[k]是列表元素索引）
 
 to：
  neuron1.w [k] += x1 [i] [k] * numpy.mean（err3 ** 2） * numpy.mean（err1 ** 2） * lr
neuron2.w [k] += x1 [i] [k] * numpy.mean（err3 ** 2） * numpy.mean（err2 ** 2） * lr
 
但是我有Oveflow错误。
我当前拥有的所有代码：
 导入随机
导入数学
导入numpy

班级神经元：
    def __init __（self，weightsc）：
        self.w = []
        对于我的范围（weightsc）：
            self.w.append（random.random（））

    Def激活（Self，X）：
        返回最大（0，x）

    def预测（self，px）：
        值= 0
        对于我的范围（len（self.w））：
            值 += px [i] * self.w [i]
        返回值

    &#39;&#39;&#39;def train（self，count，x，y，lr = 0.1）：
        打印（“ \ ttraining开始：”）
        对于我的范围（count）：
            打印（&#39;epoch：＆quot; i）
            对于J范围（Len（x））的J：
                py = self.predict（x [j]）
                打印（py：＆quot; py）
                err = y [j]  -  self.activate（py）
                打印（“ err：＆quot”，err）
                对于K范围（Len（self.w））：
                    self.w [k] += err * x [j] [k] * lr
                    打印（＆quot; w [k]＆quot; self.w [k]）&#39;&#39;&#39;

x1 = [
    [0，0，0，0]，
    [1，0，1，0]，
    [1，1，0，1]，
    [0，1，1，0]，
    [1，1，0，0]，
    [1，0，1，1]，
    [1，1，1，0]
这是给出的

y1 = [0，1，0，0，0，0，1，1]

x2 = [
    [1，0，1，0]，
    [1，0，0，0]，
    [0，1，1，0]，
    [1，0，1，1]
这是给出的

py1 = []
py2 = []
py3 =​​ []

如果__name__ ==＆quot __ Main __＆quot;：
    LR = 0.001

    Neuron1 =神经元（4）
    Neuron2 =神经元（4）
    Neuron3 = Neuron（2）
    打印（＆quot; w1 =; n neuron1.w）
    打印（＆quot; w2 =; n neuron2.w）
    打印（＆quot; w3 =; neuron3.w）

    对于L范围（1000）：
        #print（&#39;epoch =＆quot; l）
        对于我的范围（Len（x1））：
            py1.append（
                神经元1.活化（激活
                    neuron1.predict（x1 [i]）
                ）
            ）
            #print（＆quot; layer1_pred1 =＆quot; py1 [i]）

            py22.append（
                神经元2.激活（激活
                    neuron2.predict（x1 [i]）
                ）
            ）
            #print（＆quot; layer1_pred2 =＆quot; py2 [i]）

            layer1_pred = [py1 [i]，py2 [i]]
            py3.append（
                neuron3.predict（
                    layer1_pred
                ）
            ）
            #print（＆quot; layer2_pred1 =＆quot; py3 [i]）

            err3 = y1 [i]  -  neuron3.Acivate（py3 [i]）
            #print（＆quot; err3 =＆quot; err3）

            对于J范围（Len（neuron3.w）））：
                neuron3.w [j] += lays1_pred [j] * numpy.mean（err3 ** 2） * lr

            err1 = y1 [i] -py1 [i]
            #print（＆quot; err1 =＆quot; err1）
            err2 = y1 [i] -py2 [i]
            #print（＆quot; err2 =＆quot; err2）

            对于范围内的K（Len（Neuron1.W））：
                neuron1.w [k] += x1 [i] [k] * numpy.mean（err3 ** 2） * err1 * lr
            对于K范围（Len（Neuron2.W）））：
                neuron2.w [k] += x1 [i] [k] * numpy.mean（err3 ** 2） * err2 * lr

        py1 = py2 = py3 =​​ []

    打印（“测试”）
    对于x2中的项目：
        pred1 = neuron1.activate（neuron1.predict（item））
        pred2 = neuron2.Acivate（neuron2.predict（item））
        print（neuron3.predict（[pred1，pred2]））
 ]]></description>
      <guid>https://stackoverflow.com/questions/79450720/am-i-chaining-neural-network-wrong-predictions-seem-random-sometimes</guid>
      <pubDate>Wed, 19 Feb 2025 08:47:17 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的验证损失低于我的训练损失，因为训练vae在时间序列数据上时？</title>
      <link>https://stackoverflow.com/questions/79450306/why-is-my-validation-loss-lower-than-my-training-loss-when-training-a-vae-on-tim</link>
      <description><![CDATA[我正在训练一个变异自动编码器（VAE）从顺序时间序列数据中提取潜在变量表示。我的模型体系结构相对简单：它在编码器中使用1个LSTM层（在32至128个单位之间），而解码器中使用了1个LSTM层。重建损失计算为原始数据及其重建之间的平方平方误差（MSE），总结在序列维度上，我还计算了KL差异。这两个损失合并为我的模型优化的总损失。
这里有一些有关我的设置的详细信息：
 数据集： 

训练集：通过滚动窗口生成的13K样品。
火车，验证和测试集首先分开，它们之间没有重叠。分裂后，将一个滚动窗口独立地应用于每组
为了保留系列的时间顺序，数据集没有被改组
验证和测试集：每个样本每个样本

 模型体系结构： 

编码器和解码器中的单个LSTM层
由于层和单元数量少，模型容量是适度的

 正则化 

使用0.2的辍学率来正规化模型

令人困惑的部分是，在模型的许多变化中，我一直观察到验证损失低于训练损失。这与通常的期望相反，即训练损失较低（由于模型在培训数据上直接优化），并且验证损失较高（由于概括错误）。 
    
我的问题：

 是什么导致验证损失低于培训损失？

 我应该如何解释这些训练和验证损失曲线？

 我可能缺少任何可能有助于诊断此问题的最佳实践或检查吗？

]]></description>
      <guid>https://stackoverflow.com/questions/79450306/why-is-my-validation-loss-lower-than-my-training-loss-when-training-a-vae-on-tim</guid>
      <pubDate>Wed, 19 Feb 2025 05:49:19 GMT</pubDate>
    </item>
    <item>
      <title>姿势估计和校正</title>
      <link>https://stackoverflow.com/questions/79450203/pose-estimation-and-correction</link>
      <description><![CDATA[我想建立一个系统来分析一个人在进行一定的练习时，如果不正确，请纠正其表格。我如何实施系统以提出建议以改善姿势。
用于检测，我使用了Mediapipe并构建了一个模型来使用几组视频来识别该动作。]]></description>
      <guid>https://stackoverflow.com/questions/79450203/pose-estimation-and-correction</guid>
      <pubDate>Wed, 19 Feb 2025 04:47:13 GMT</pubDate>
    </item>
    <item>
      <title>训练LSTM的时间序列不同</title>
      <link>https://stackoverflow.com/questions/79449572/train-lstm-for-time-series-with-varying-lengths</link>
      <description><![CDATA[我正在训练LSTM进行时间序列预测，其中数据来自不规则间隔的传感器。我正在使用最后5分钟的数据来预测下一个值，但是某些序列比其他序列大。
我的输入阵列的形状是（611,1200,15），其中（示例，时间段，功能）。每个样本的第二维度均未完成，因此我用NP.NAN值填充了丢失的数据。例如，示例（1，：，：）有1000个时间段和200 np.nan。
训练时，损失等于Nan。
我在做什么错？我该如何训练？
这是我尝试训练LSTM的尝试：
  def lstmfit（y，x，n_hidden = 1，n_neurons = 30，Learning_rate = 1E-2）：   
    lstm = sequention（）
    lstm.add（basking（mask_value = np.nan，input_shape =（none，x. shape [2]）））））））
        
    对于范围（n_hidden）的图层：
        lstm.add（lstm（n_neurons，， 
                      激活=“ tanh”
                      recurrent_activation =＆quot; sigmoid＆quot;
                      return_sequences = true））
        
    lstm.add（密集（1））
    
    lstm.compile（loss =; mse; optimizer =; adam＆quot;）
    
    早期_STOPPING =早期踩踏（Monitor =&#39;损失&#39;，耐心= 10，详细= 1，restore_best_weights = true）
  
    
    lstm.-fit（x，y.Reshape（-1），epochs = 100，callbacks = [arfore_stopping]）
    
    y_train_fit = lstm.predict（x）
    
    返回lstm，y_train_fit
 
模型的摘要：
  lstm.summary（）
型号：sequential_9＆quot
__________________________________________________________________________
 图层（类型）输出形状参数＃   
=============================================== ===============
 masking_7（掩模）（无，无，15）0         
                                                                 
 LSTM_6（LSTM）（无，无，30）5520      
                                                                 
 密集_10（密集）（无，无，1）31        
                                                                 
=============================================== ===============
总参数：5551（21.68 kb）
可训练的参数：5551（21.68 kb）
不可训练的参数：0（0.00字节）
__________________________________________________________________________
 
和训练的第一个时期：
 时期1/100
18/18 [=======================================
时代2/100
18/18 [========================================
时期3/100
18/18 [====================================
 ]]></description>
      <guid>https://stackoverflow.com/questions/79449572/train-lstm-for-time-series-with-varying-lengths</guid>
      <pubDate>Tue, 18 Feb 2025 20:47:34 GMT</pubDate>
    </item>
    <item>
      <title>推荐系统中的可伸缩性问题</title>
      <link>https://stackoverflow.com/questions/79449194/scalability-issue-in-recommender-system</link>
      <description><![CDATA[我是推荐系统的新手，目前我正在建立一个基于协作过滤的建议系统。在我的数据集中，当前有600个用户和9000个项目具有不同的评分。我创建了一个用户项目交互矩阵，并且正在Numpy进行所有操作。我正在使用Pearson相关系数作为与每个用户相对的顶级K相似用户的方法。我当前找到有关每个目标用户最相似的用户的当前代码具有O（m^2  n）的时间复杂性和O（m  n）的空间复杂性，其中m是用户和n是项目的数量。
考虑到这段时间的复杂性，对于大量用户来说，这是不可行的。在研究后，我发现降低尺寸可能是一种解决方案。但是我担心的是，如果我减少用户数量，那么整个系统将无法为建议部分做出公正的态度，因为我想为每个用户推荐。
那么，优化的不同方法是什么，以便在缩放缩放的情况下有助于？]]></description>
      <guid>https://stackoverflow.com/questions/79449194/scalability-issue-in-recommender-system</guid>
      <pubDate>Tue, 18 Feb 2025 17:52:29 GMT</pubDate>
    </item>
    <item>
      <title>minibatchkmeans bertopic不返回一半数据的主题</title>
      <link>https://stackoverflow.com/questions/79449168/minibatchkmeans-bertopic-not-returning-topics-for-half-of-data</link>
      <description><![CDATA[我正在尝试将推文数据集主题。我有大约5000万条推文。不幸的是，由于嵌入，如此大的数据集将不适合RAM（甚至128GB）。因此，我一直在努力根据 docs  docs  &gt; 
因此：
 来自bertopic.vectorizer inlinecountvectorizer inlinecountvectorizer
从bertopic.Dectorizer导入ClasStFidFtransFormer
来自Sklearn.Cluster Import Minibatchkmeans
导入numpy作为NP


class safeincrementalpca（regementalpca）：
    def partial_fit（self，x，y = none）：
        ＃确保输入是连续的，并且在float64中
        x = np.sascontiguularray（x，dtype = np.float64）
        返回super（）。partial_fit（x，y）
    
    def变换（self，x）：
        结果= super（）。变换（x）
        ＃强制输出为float64并连续
        返回np.sascontiguularray（结果，dtype = np.float64）


vectorizer_model = onlinecountVectorizer（stop_words =;英语）
ctfidf_model = classtfidftransformer（redy_frequent_words = true，bm25_weighting = true）
umap_model = safeincrementalpca（n_components = 100）
cluster_model = minibatchkmeans（n_clusters = 1000，andural_state = 0）

来自伯托进口的伯托

topic_model = bertopic（umap_model = umap_model，
                       hdbscan_model = cluster_model，

对于docs_delayed，emb_delayed in tqdm（zip（docs_partitions，embeddings_partitions），total = len（docs_partitions））：

    docs_pdf = docs_delayed.compute（）
    emb_pdf = emb_delayed.compute（）

    docs = docs_pdf [&#39;text;]。tolist（）
    embeddings = np.vstack（emb_pdf [&#39;embeddings&#39;]。tolist（））
    
    ＃部分适合您的模型（确保您的模型像许多Scikit-Learn估计器一样支持Partial_fit）
    topic_model.partial_fit（文档，嵌入）

 
然后将数据集转换为SQL数据库：
 
对于docs_delayed，emb_delayed in tqdm（zip（docs_partitions，embeddings_partitions），total = len（docs_partitions））：

    docs_pdf = docs_delayed.compute（）
    emb_pdf = emb_delayed.compute（）
    docs = docs_pdf [&#39;text;]。tolist（）
    embeddings = np.vstack（emb_pdf [&#39;embeddings&#39;]。tolist（））

    ＃3）在此碎片上涂抹伯托
    主题，probs = topic_model.transform（文档，嵌入）

    ＃将主题保存到数据框
    df_topics = pd.dataframe（{{
        ＆quot&#39;tweet_id＆quot;：docs_pdf [;
        主题“：主题，
        概率＆quot：概率
    }））

    ## Merge＆amp;存储在DB中
    docs_pdf [主题;] = df_topics [tope;
    docs_pdf [概率＆quot＆quort＆quort＆quotisy = df_topics [＆quot&#39;概率;]
    docs_pdf.to_sql（“ tweets”;引擎，发动机，if_exists =＆quot&#39;append＆quort; quot; index = false）
 
我已经尝试这样做了一段时间，这是我得到的最接近的示例。唯一的问题是，数据集的一半在末尾数据库中具有零主题。从我对理论的了解来看，Minibatchkmeans不应有任何异常值，因此所有推文应至少分配给至少一个主题，对吗？我已经检查了有关的未分类推文，他们的文档中没有任何内容表明很难对其进行分类（相对于其他分类）。
我很高兴听到有关可能出了什么问题以及如何解决此问题的任何建议！
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79449168/minibatchkmeans-bertopic-not-returning-topics-for-half-of-data</guid>
      <pubDate>Tue, 18 Feb 2025 17:42:59 GMT</pubDate>
    </item>
    <item>
      <title>为什么在训练LSTM模型时面对“ CUDA错误：设备端断言触发”？</title>
      <link>https://stackoverflow.com/questions/79448910/why-facing-cuda-error-device-side-assert-triggered-while-training-lstm-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79448910/why-facing-cuda-error-device-side-assert-triggered-while-training-lstm-model</guid>
      <pubDate>Tue, 18 Feb 2025 16:05:55 GMT</pubDate>
    </item>
    <item>
      <title>Qiskit Importerror</title>
      <link>https://stackoverflow.com/questions/79448915/qiskit-importerror</link>
      <description><![CDATA[我试图使用以下内容导入量子级：
 来自qiskit_machine_learning.kernels导入量子kernel
 
但是我遇到了这个错误：
 来自qiskit_machine_learning.kernels导入量子kernel
Importerror：无法从&#39;qiskit_machine_learning.kernels&#39;导入名称&#39;量子kernel&#39; 
（c：\ user \ pshre \ appdata \ local \ program \ python \ python \ python310 \ lib \ site-packages \ qiskit_machine_learning \ kernels \ kernels \ __ init__ init__.py）
 
 qiskit版本：0.8.2 
我已经更新了模块：
  pip安装 - 升级qiskit-machine学习
 ]]></description>
      <guid>https://stackoverflow.com/questions/79448915/qiskit-importerror</guid>
      <pubDate>Tue, 18 Feb 2025 16:05:55 GMT</pubDate>
    </item>
    <item>
      <title>OPENCV：从图像分割/提取打印机标签</title>
      <link>https://stackoverflow.com/questions/79448585/opencv-segmenting-extracting-printer-labels-from-image</link>
      <description><![CDATA[我有来自打印机的标签的镜头。
我想从框架中提取单个标签（即检测每个单独标签的边界），然后在mm中找到印刷矩形和标签的顶部边缘之间的距离。。
 这是录像中的框架 
我最初尝试与一些形态学操作一起尝试轮廓检测，但是标签内的印刷内容（矩形和数字）正在干扰边缘检测，因此很难仅隔离标签边框。 
有人解决了类似问题吗？哪些预处理技术或替代方法最适合仅可靠地分割标签边缘？]]></description>
      <guid>https://stackoverflow.com/questions/79448585/opencv-segmenting-extracting-printer-labels-from-image</guid>
      <pubDate>Tue, 18 Feb 2025 14:26:06 GMT</pubDate>
    </item>
    <item>
      <title>视觉Mamba实施</title>
      <link>https://stackoverflow.com/questions/79448241/vision-mamba-implementation</link>
      <description><![CDATA[我是Mamba模型的新用户，我读了一些论文，说它在图像分割任务上具有出色的性能。如果有人以前已经实施了它，是否有有关Mamba块的正确设置或输入图像补丁和尺寸的指导，可以导致最佳结果？
到目前为止，我的实施尚未显示出将Mamba块添加到我的代码中的任何优势，这是我实施的一个小片段：
  x = torch.rand（1，16，256，256）
norm = rmsnorm（16 ** 2）
mamba = mamba（16 ** 2）
_，c，h，_ = X.Shape
x =重新安排（x，&#39;b c（p1 pH）（p2 pw） - ＆gt; b（c p1 p2）（pH pw）&#39;，pH = 16，pw = 16）
x = mamba（norm（x）） + x
x =重新安排（x，&#39;b（c p）d  - ＆gt; b c p d&#39;，c = c）
x =重新安排（x，&#39;b c（p1 p2）（pH PW） - ＆gt; b c（p1 pH）（p2 pw）&#39;，p1 = h // 16，pH = 16）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79448241/vision-mamba-implementation</guid>
      <pubDate>Tue, 18 Feb 2025 12:23:34 GMT</pubDate>
    </item>
    <item>
      <title>这两个实现洛拉（低级适应）之间有什么区别吗？</title>
      <link>https://stackoverflow.com/questions/79447495/is-there-any-difference-between-these-two-implementations-of-lora-low-rank-adap</link>
      <description><![CDATA[我们都知道洛拉是一种低级适应方法，可以表达如下：x = w_0 * x +（a @ b） * x。我有两个不同的代码实现。它们之间有什么区别吗？
代码1：
  def向前（self，x）：
    x = x @ self.lora_a
    x = x @ self.lora_b
    x = self.scaling * x
    返回x
 
代码2：
  def向前（self，x）：
    x = x @（self.lora_a @ self.lora_b）
    x = self.scaling * x
    返回x
 
从数学角度来看，两者均似乎是等效的。但是，当我在玩具数据集上运行两个实现时，我观察到它们的性能有很小的差异 - 编码2的性能稍好。
为什么会发生这种轻微的差异？是否有基本的计算或优化细微差别可以解释这一点？
我不完全确定两个实现是否正确。我经常在GitHub存储库中看到代码1，但是我注意到代码2的性能稍好一些。为什么可能是这种情况？]]></description>
      <guid>https://stackoverflow.com/questions/79447495/is-there-any-difference-between-these-two-implementations-of-lora-low-rank-adap</guid>
      <pubDate>Tue, 18 Feb 2025 07:56:56 GMT</pubDate>
    </item>
    <item>
      <title>如何对混合VAR-LSTM模型执行样本外预测？</title>
      <link>https://stackoverflow.com/questions/79445942/how-to-perform-out-of-sample-forecast-for-a-hybrid-var-lstm-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79445942/how-to-perform-out-of-sample-forecast-for-a-hybrid-var-lstm-model</guid>
      <pubDate>Mon, 17 Feb 2025 15:56:31 GMT</pubDate>
    </item>
    <item>
      <title>ValueRror：X具有7个功能，但ColumnTransFormer期望13个功能</title>
      <link>https://stackoverflow.com/questions/79434756/valueerror-x-has-7-features-but-columntransformer-expects-13-features</link>
      <description><![CDATA[我有以下代码，我尝试预测使用泊松回归的工具价格。
 ＃---加载并准备数据---
y =火车[&#39;priceToday&#39;]
x = train.drop（columns = [&#39;pricetoday&#39;]）

＃定义非标准类型
non_standard_types = [&#39;nar;

＃为非标准创建标志功能
x [&#39;non_standard_flag;]

＃确定数值和分类列
num_features = [&#39;age&#39;&#39;&#39;
cat_features = [&#39;country; country; quot&#39;final_trans;]

＃定义预处理管道
预处理器= columntransformer（
    变形金刚= [
        （&#39;num&#39;，StandardScaler（），num_features），
        （&#39;cat&#39;，onehotencoder（handle_unknown =&#39;ignore&#39;），cat_features）
    ]，剩余=; drop; quot;
）

＃---火车/测试拆分---
＃创建一个重量列
train [sample_weight; quight&#39;&#39;] = train [type; type_ls; quot;]。应用（lambda x：1如果x == x ==;

火车[stratify_group&#39;&#39;]
x_train，x_val，y_train，y_val，train_weights，val_weights = train_test_split（
    x，y，train [sample_weight&#39;]，test_size = 0.2，andural_state = 42，stratefify = train [＆quot; stratify_group＆quort＆quort;
）
＃将预处理器安装在培训数据上一次
x_train_preprocessed = preprocessor.fit_transform（x_train）
x_val_preprocessed = preprocessor.transform（x_val）

＃定义模型
模型= {
    ＆quot“ poisson”：poissonRegressor（alpha = 0.01）
}

＃火车和评估模型
model_results = {}

对于model_name，model.items（）中的型号：
    model.fit（x_train_preprocessed，y_train，sample_weight = train_weights）

    ＃预测
    预测= model.predict（x_val_preprocessed）
    ＃计算指标
    r2 = r2_score（y_val，预测）

    model_results [model_name] = {
        “型号”：模型，
        ＆quot“ R2”：R2
    }
 
我有一个测试数据，我想将其价格与模型的预测价格进行比较。
我的测试数据是这样的：
 ＃确保新数据具有正确的格式
new_data = pd.dataframe（{{
    ＆quot;：：[12，24，36，48，60，72，84，12，24，36，48，60，60，72，84]，
    ＆quot”小时：[500，1000，1500，2000，2500，3000，3500，3500，500，1000，1500，2000，2500，2500，3000，3500]，
    ＆quot“ brand;
    ＆quot&#39;power＆quot; [150] * 7 + [80] * 7，
    ＆quot“ final_trans＆quot”：[＆quot; cv; quot&#39;] * 14，，
    ＆quot“ country”：[deu＆quot;] * 14，，
    &#39;type_ls＆quot;：[NAR，NAR，NAR，ST，ST，ST，ST，ST，ST，ST，ST，ST，ST，ST，ST，ST，ST] 
    ＆quot&#39;current_pred＆quot;：[105614，96681，88504，81018，74165，67892，62150，42608，39728，37043，37043，34540，32206，32206，30029，28000]
}））
 
我的代码是：
  new_df = pd.dataframe（new_data）
＃创建&#39;non_standard_flag&#39;
new_df [&#39;non_standard_flag;] = new_df [type; type_ls; quot;]。isin（non_standard_types）.astype（int）

＃选择预处理器所需的列
x_new = new_df [[&#39;age&#39;，&#39;power&#39;，&#39;小时&#39;，&#39;non_standard_flag&#39;，&#39;brand&#39;，&#39;country&#39;，&#39;final_trans&#39;]]]

x_new_preprocessed = preprocessor.transform（x_new） 

＃从培训数据中进行单次编码后获取列名
ohe = preprocessor.named_transformers _ [&#39;cat&#39;]
encoded_cat_columns = ohe.get_feature_names_out（cat_features）

＃创建数字功能的列名称
num_columns = num_features

＃组合列名称
all_columns = num_columns + list（encoded_cat_columns）

＃从预处理数据中创建数据框
x_new_preprocessed_df = pd.dataframe（x_new_preprocessed，columns = all_columns）

＃---用泊松模型预测---
poisson_model = model_results [＆quot; poisson; quot; quote; quode;]＃访问训练有素的泊松模型
predicted_prices = poisson_model.predict（x_new_preprocessed_df）

＃比较和存储结果---
new_df [&#39;prediction_price&#39;] = predicted_prices

＃计算预测和当前价格之间的差异
new_df [&#39;Price_difference&#39;] = new_df [&#39;Predicted_price&#39;]  -  new_df [&#39;current_pred&#39;]
 
但是，在这样做之后，我会出现错误：
  X具有7个功能，但是ColumnTransFormer期望13个功能
 
我有相同数量的列，所以我不明白为什么我有这个错误。]]></description>
      <guid>https://stackoverflow.com/questions/79434756/valueerror-x-has-7-features-but-columntransformer-expects-13-features</guid>
      <pubDate>Wed, 12 Feb 2025 23:51:03 GMT</pubDate>
    </item>
    <item>
      <title>培训LLM在图数据库中用于查询生成的LLM</title>
      <link>https://stackoverflow.com/questions/77613507/training-llm-for-query-generation-in-a-graph-database</link>
      <description><![CDATA[如果我已经开发了一个具有自己的查询语言的图形数据库。我必须找到一种方法来馈送图形，然后LLM应该能够生成我们数据库的查询。
我在Langchain中发现了类似的东西，我们可以将其喂入RDF文件，然后将生成Sparql查询。
所以我对此有很多疑问，因为我非常陌生：
是否可以像我们的数据库那样培训LLM上的全新技术。如果可能的话，那么如何。
我知道我们必须向LLM提供培训数据。因此，在这种情况下，将是我们数据库查询的数据集。如果是，那么我们必须在数据集中提供多少查询。
对不起，如果没有详细详细介绍，这只是我第二次在这里问。]]></description>
      <guid>https://stackoverflow.com/questions/77613507/training-llm-for-query-generation-in-a-graph-database</guid>
      <pubDate>Wed, 06 Dec 2023 13:34:06 GMT</pubDate>
    </item>
    <item>
      <title>数据科学家在教程中使用DataSource CSV，为什么不使用与数据库的连接？</title>
      <link>https://stackoverflow.com/questions/60547537/data-scientist-use-datasource-csv-in-tutorials-why-not-use-connection-to-databa</link>
      <description><![CDATA[当我在互联网上查看时，我发现了很多有关数据科学家的教程。 
我已经注意到它们都使用CSV作为数据库而不是数据库连接，即使您可以使用它。
我找不到答案为什么CSV，因为我所有的数据都存储在数据库中。
他们都使用CSV文件的原因是什么？]]></description>
      <guid>https://stackoverflow.com/questions/60547537/data-scientist-use-datasource-csv-in-tutorials-why-not-use-connection-to-databa</guid>
      <pubDate>Thu, 05 Mar 2020 14:24:25 GMT</pubDate>
    </item>
    </channel>
</rss>