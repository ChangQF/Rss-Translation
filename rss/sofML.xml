<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 01 Jan 2024 09:14:12 GMT</lastBuildDate>
    <item>
      <title>如何在机器学习方面取得优异成绩？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77741247/how-to-excel-in-machine-learning</link>
      <description><![CDATA[如何增强我的机器学习能力？我总是忘记使用 python 来实践模型。有时我也会对模型之间感到困惑。我一直在练习，但是如何才能增强它呢？请分享一些技巧。并分享一些理解机器学习模型中所有术语的技巧，如混淆矩阵、拟合和分类报告]]></description>
      <guid>https://stackoverflow.com/questions/77741247/how-to-excel-in-machine-learning</guid>
      <pubDate>Mon, 01 Jan 2024 07:31:42 GMT</pubDate>
    </item>
    <item>
      <title>“DataFrame”对象没有属性“c”</title>
      <link>https://stackoverflow.com/questions/77741177/dataframe-object-has-no-attribute-c</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77741177/dataframe-object-has-no-attribute-c</guid>
      <pubDate>Mon, 01 Jan 2024 06:55:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在 on_epoch_begin 回调中获取 Keras Tuner RandomSearch 的当前试用数量？</title>
      <link>https://stackoverflow.com/questions/77741026/how-can-i-get-the-current-trial-number-of-keras-tuner-randomsearch-in-on-epoch-b</link>
      <description><![CDATA[类 lr_tune(tf.keras.callbacks.Callback):
    def __init__(自我、审判、合成、最大审判):
        超级(learning_rate_tuner, self).__init__()

    def on_epoch_begin（自我，纪元，日志=无）：
        # 获取当前试用版：
        当前_审判= ???????


        # 检查优化器是否有学习率：
        如果不是 hasattr(self.model.optimizer, &#39;lr&#39;):
            raise ValueError(&#39;优化器必须具有“lr”属性。&#39;)
        
        # 获取学习率：
        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))
        sch_lr = self.set_lr（当前_试验，lr）
        tf.keras.backend.set_value(self.model.optimizer.lr, sch_lr)

我想使用 Keras Tuner 的 RandomSearch 方法根据当前的试验次数调整学习率。]]></description>
      <guid>https://stackoverflow.com/questions/77741026/how-can-i-get-the-current-trial-number-of-keras-tuner-randomsearch-in-on-epoch-b</guid>
      <pubDate>Mon, 01 Jan 2024 05:13:07 GMT</pubDate>
    </item>
    <item>
      <title>可用于 3D fmri 扫描的特征提取过程或工具</title>
      <link>https://stackoverflow.com/questions/77740924/feature-extraction-process-or-tools-that-can-be-used-on-a-3d-fmri-scan</link>
      <description><![CDATA[我正在开展一个项目，通过这些 fmri 扫描来检测自闭症。
我的数据是.nii格式，我想知道是否有任何特定的特征提取工具或流程以及需要提取的任何特定特征。
我无法继续，并且在这一步感到震惊。]]></description>
      <guid>https://stackoverflow.com/questions/77740924/feature-extraction-process-or-tools-that-can-be-used-on-a-3d-fmri-scan</guid>
      <pubDate>Mon, 01 Jan 2024 04:02:43 GMT</pubDate>
    </item>
    <item>
      <title>如何对短字符串标签进行编码或矢量化？</title>
      <link>https://stackoverflow.com/questions/77740868/how-do-i-encode-or-vectorize-short-string-labels</link>
      <description><![CDATA[我想预测金融交易列表中的类别字段，并在其中自动生成描述。这些并不完全是分类的，因为有时它们包含日期、帐号、商店地址等。（我现在有大约 15k 条记录用于训练和测试。）
什么是对这些进行编码的好方法，以便我可以运行一些监督训练来为新交易提供初步猜测？
我可以做一些事情，例如删除数字或日期或使用正则表达式来获得较小的子集，但是如果我没有获得所有正确的非重复模式，这似乎对错误很敏感。我正在考虑对字母进行某种单一的表示。我研究了矢量化，但这看起来像是针对较长的单词段落。
还有其他想法吗？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77740868/how-do-i-encode-or-vectorize-short-string-labels</guid>
      <pubDate>Mon, 01 Jan 2024 03:06:23 GMT</pubDate>
    </item>
    <item>
      <title>如何改进我的深度学习计算机视觉项目张量流</title>
      <link>https://stackoverflow.com/questions/77740755/how-to-improve-my-deep-learning-computer-vision-project-tensorflow</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77740755/how-to-improve-my-deep-learning-computer-vision-project-tensorflow</guid>
      <pubDate>Mon, 01 Jan 2024 01:32:02 GMT</pubDate>
    </item>
    <item>
      <title>arduino：如何在arduino中读取csv文件</title>
      <link>https://stackoverflow.com/questions/77740390/arduino-how-to-read-a-csv-file-in-arduino</link>
      <description><![CDATA[我正在尝试建造一个跟随我的脸的简单炮塔。对于面部跟踪功能，我使用 Python OpenCV 来跟踪我的面部并将 X 和 Y 坐标存储在 CSV 文件中。我如何在 Arduino 中读取该 CSV 文件？
另一个问题：我解决这个问题的方法是个好主意，还是有更好的方法来解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77740390/arduino-how-to-read-a-csv-file-in-arduino</guid>
      <pubDate>Sun, 31 Dec 2023 21:09:35 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中如何进行特征提取</title>
      <link>https://stackoverflow.com/questions/77740251/how-to-feature-extract-in-the-machine-learning</link>
      <description><![CDATA[我是机器学习领域的新手。下面提到我的数据集。
数据集
我做了数据可视化部分。下面提到我的编码。
 导入数学
    将 pandas 导入为 pd
    将 numpy 导入为 np
    将 matplotlib.pyplot 导入为 plt
    从 sklearn.preprocessing 导入 MinMaxScaler
    从 sklearn.metrics 导入mean_squared_error
    从 keras.models 导入顺序
    从 keras.layers 导入密集、LSTM、Dropout

    df = pd.read_csv(&#39;sample_data/covid19_full_data.csv&#39;)
    df

    df.isnull().sum()

    print(&#39;删除空值之前的数据集长度&#39;)
    长度（df）

    df[&#39;日期&#39;] = pd.to_datetime(df[&#39;日期&#39;])

    aggregate_data = df.groupby(&#39;日期&#39;).agg({
    &#39;new_cases&#39;: &#39;总和&#39;,
    &#39;new_deaths&#39;: &#39;总和&#39;,
    &#39;total_cases&#39;: &#39;总和&#39;,
    &#39;total_deaths&#39;: &#39;总和&#39;
    }).reset_index()
    聚合数据

    # 绘制随时间变化的 new_cases
    plt.figure(figsize=(14, 7))
    plt.plot(aggregated_data[&#39;date&#39;],aggregate_data[&#39;new_cases&#39;], label=&#39;新案例&#39;)
    plt.title(&#39;随着时间的推移新的 COVID-19 病例&#39;)
    plt.xlabel(&#39;日期&#39;)
    plt.ylabel(&#39;新增病例数&#39;)
    plt.图例()
    plt.show()

    plt.figure(figsize=(14, 7))
    plt.plot(aggreerated_data[&#39;date&#39;],aggregate_data[&#39;new_deaths&#39;], label=&#39;新死亡&#39;, color=&#39;red&#39;)
    plt.title(&#39;随着时间的推移新的 COVID-19 死亡人数&#39;)
    plt.xlabel(&#39;日期&#39;)
    plt.ylabel(&#39;新增死亡人数&#39;)
    plt.图例()
    plt.show()

    plt.figure(figsize=(14, 7))
    plt.plot(aggreerated_data[&#39;date&#39;],aggregate_data[&#39;total_cases&#39;],label=&#39;总病例数&#39;,color=&#39;绿色&#39;)
    plt.title(&#39;一段时间内的 COVID-19 病例总数&#39;)
    plt.xlabel(&#39;日期&#39;)
    plt.ylabel(&#39;案例总数&#39;)
    plt.图例()
    plt.show()

    plt.figure(figsize=(14, 7))
    plt.plot(aggreerated_data[&#39;date&#39;],aggregate_data[&#39;total_deaths&#39;],label=&#39;总死亡人数&#39;,color=&#39;black&#39;)
    plt.title(&#39;一段时间内的 COVID-19 死亡总数&#39;)
    plt.xlabel(&#39;日期&#39;)
    plt.ylabel(&#39;死亡总数&#39;)
    plt.图例()
    plt.show()

如何对我的方法进行特征提取部分。请编写特征提取部分的代码并解释每个步骤。]]></description>
      <guid>https://stackoverflow.com/questions/77740251/how-to-feature-extract-in-the-machine-learning</guid>
      <pubDate>Sun, 31 Dec 2023 19:50:12 GMT</pubDate>
    </item>
    <item>
      <title>C++神经网络中softmax层的问题</title>
      <link>https://stackoverflow.com/questions/77739953/problem-with-softmax-layer-in-c-neural-network</link>
      <description><![CDATA[我用 C++ 实现了一个简单的神经网络。对于密集连接的层来说，它似乎工作得很好，但是当我添加用于分类的 softmax 层时，我遇到了麻烦。
从输入生成激活值的 softmax 函数本身看起来是正确的，但是当我训练网络时，它在分类方面并没有变得更好。我认为问题出在反向传播代码中——它对于密集层来说效果很好。所以它可能是在 softmax 层的导数计算中。
我的代码可能有一些简单的问题？我尝试了很多变体但无法使其工作..
以下是激活函数的实现：
doubleactivation_function_sigmoid（常量双输入）
{
    返回 1 / (1 + exp(-输入));
}

双激活函数 sigmoid_derivative（常量双输入）
{
    返回输入*（1 - 输入）；
}

双activation_function_relu（常量双输入）
{
    返回 std::max(0.0, 输入);
}

双activation_function_relu_derivative（常量双输入）
{
    返回（输入&gt; 0.0）？ 1.0：0.0；
}

双激活函数_softmax（常量双输入）
{
    // 只需返回输入，稍后会进行分类
    返回输入；
}

双激活函数softmax_derivative（常量双输入）
{
    返回输入*（1 - 输入）；
}

成本函数的实现：
double cost_function_mse（常量双预测，常量双目标）
{
    // MSE 成本函数 - 下面的导数是一个简单的加法！
    返回 0.5 * pow((预测 - 目标), 2);
}

double cost_function_mse_derivative(const 双预测，const 双目标)
{
    // 均方误差
    返回预测-目标；
}

double cost_function_rmse(const 双预测，const 双目标)
{
    return sqrt(0.5 * pow((预测 - 目标), 2));
}

double cost_function_rmse_derivative（const 双预测，const 双目标）
{
    return (预测 - 目标) / sqrt(2.0);
}

double cost_function_crossEntropy(const 双预测，const 双目标)
{
    // 未使用，因为 softmax 是在整个数组上运行的。
    返回 - 目标*exp(预测);
}

double cost_function_crossEntropy_derivative(const 双预测，const 双目标)
{
    返回预测-目标；
}

反向传播函数：
双层::BackwardsPass(
    const层&amp;上一层，
    常量层* nextLayer,
    常量双倍学习率，
    常量列&amp;目标，
    CostFuncPtr cf,
    CostFuncPtr cfD)
{
    双累积误差 = 0;
    for (uint32 n=0; n &lt; numNeurons; n++)
    {
        const 双预测 = 激活值[n]；
        错误[n] = 0；
        
        if (nextLayer == nullptr)
        {
            // 这是输出层
            错误[n] = cfD(预测, 目标[n]); // 调用成本导数函数

            // 仅用于报告
            累积误差 += pow(cf(预测, 目标[n]),2);
        }
        别的
        {
            for (uint32 k=0; k &lt; nextLayer-&gt;numNeurons; k++)
            {
                错误[n] += nextLayer-&gt;权重[k][n] * nextLayer-&gt;梯度[k];
            }
        }

        if (!forClassification)
            梯度[n] = 错误[n] * afD(预测); // 调用激活导数函数
        别的
            梯度[n] = 预测-目标[n]；

    }

    // 更新权重
    for (uint32 n=0; n &lt; numNeurons; n++)
    {
        for (uint32 i = 0; i &lt; previousLayer.numNeurons; ++i)
        {
            // 输入是上一层神经元的激活值
            const 双输入 = previousLayer.activationValue[i];
            权重[n][i] -= 学习率 * 梯度[n] * 输入；
        }

        // 更新偏差
        偏差[n] -= 学习率 * 梯度[n]； // 偏置输入始终为 1，因此被省略
    }
    返回战俘（累积错误，2）；
}

在使用简单测试数据进行训练期间，预测并没有变得更好。]]></description>
      <guid>https://stackoverflow.com/questions/77739953/problem-with-softmax-layer-in-c-neural-network</guid>
      <pubDate>Sun, 31 Dec 2023 17:25:18 GMT</pubDate>
    </item>
    <item>
      <title>在 Flutter 中使用 Real-ESRGAN Onnx</title>
      <link>https://stackoverflow.com/questions/77739855/using-real-esrgan-onnx-in-flutter</link>
      <description><![CDATA[如何在 flutter 中实现 Real-ESRGAN ONNX。我这方面不专业。
有人可以帮助提供示例推理代码。我已将模型从 pytorch 转换为 onnx。
我尝试过一些示例数据并成功，但不知道还要做什么
&lt;前&gt;&lt;代码&gt;
` OrtSession？ _会议;
  细绳？进步;
  细绳？状态；
  Uint8List？增强；
  @覆盖
  无效初始化状态（）{
    super.initState();

    进度 =“正在初始化 ONNX 运行时...”;
    print(&#39;正在初始化 ONNX 运行时...&#39;);
    _initializeOrt();
    进度 =“正在创建 ONNX 会话...”;
    print(&#39;正在创建 ONNX 会话...&#39;);
    _createSession();
  }

  未来&lt;空&gt; _initializeOrt() 异步 {
    尝试 {
      等待 OrtEnv.instance.init();
      设置状态（（）{
        进度 =“ONNX 运行时初始化成功。”；
      });
      _createSession();
    } 捕获 (e) {
      设置状态（（）{
        进度 = &#39;初始化 ONNX 运行时时出错：$e&#39;;
      });
    }
  }

  未来&lt;空&gt; _createSession() 异步 {
    尝试 {
      最终会话选项 = OrtSessionOptions();
      const assetFileName = &#39;assets/RealESRGAN_x4plus_anime_6B.onnx&#39;;
      最终 rawAssetFile = 等待 rootBundle.load(assetFileName);
      最终字节= rawAssetFile.buffer.asUint8List();
      _session = OrtSession.fromBuffer(字节, sessionOptions);
      print(&#39;ONNX 会话创建成功。&#39;);
      设置状态（（）{
        进度 =“ONNX 会话创建成功。”；
      });
    } 捕获 (e) {
      设置状态（（）{
        进度 =“创建 ONNX 会话时出错：$e”；
      });
      print(&#39;创建 ONNX 会话时出错：$e&#39;);
    }
  }
  未来&lt;空&gt; _runInference() 异步 {
    print(&#39;运行推理...&#39;);
    设置状态（（）{
      infstatus = &quot;正在运行推理...&quot;;
    });

    最终形状 = [1, 3, 100, 100]；
    var data = Float32List.fromList(
        列表.填充(1 * 3 * 100 * 100, 0.5)); // 使用 Float32List 的示例数据
    最终 inputOrt = OrtValueTensor.createTensorWithDataList(data, shape);
    最终输入 = {&#39;input&#39;: inputOrt};
    最终 runOptions = OrtRunOptions();
    列表&lt;OrtValue??&gt;输出；

    尝试 {
      输出=等待_会话？.runAsync（runOptions，输入）；
      print(&#39;推理结果：${outputs?[0]}&#39;); // 在这里处理你的输出
      设置状态（（）{
        infstatus = &quot;推理结果：${outputs?[0]}&quot;;
      });

    } 捕获 (e) {
      print(&#39;推理过程中出错：$e&#39;);
    } 最后 {
      inputOrt.release();
      runOptions.release();
      输出？.forEach((元素) {
        元素？.release();
      });
    }

}
图书馆：
Onnx：https://github.com/gtbluesky/onnxruntime_flutter
Python 中的示例用法：
&lt;前&gt;&lt;代码&gt;`导入cv2
将 numpy 导入为 np
将 onnxruntime 导入为 rt
进口火炬
导入时间

# 使用 CUDA 执行提供程序加载 ONNX 模型
sess = rt.InferenceSession(&#39;RealESRGAN_x4plus_anime_6B.onnx&#39;)
print(“已加载模型。”)

# 加载输入图像
in_image = cv2.imread(&#39;input.jpg&#39;, cv2.IMREAD_UNCHANGED)
print(“已加载输入图像。”)

# 将 BGR 转换为 RGB 并转置维度
in_mat = cv2.cvtColor(in_image, cv2.COLOR_BGR2RGB)
in_mat = np.transpose(in_mat, (2, 1, 0))[np.newaxis]
in_mat = in_mat.astype(np.float32)
in_mat = in_mat / 255

# 测量推理时间的开始时间
开始时间 = 时间.time()

# 获取输入和输出名称
input_name = sess.get_inputs()[0].name
输出名称 = sess.get_outputs()[0].名称

print(&quot;输入名称：&quot;, input_name)
print(&quot;输出名称：&quot;,output_name)


# 将输入转换为 torch 张量并将其移至 GPU
in_mat = torch.tensor(in_mat)

# 运行推理
out_mat = sess.run([输出名称], {输入名称: in_mat.cpu().numpy()})[0]


# 测量并打印经过的时间
elapsed_time = time.time() - 开始时间
print(&#39;推理时间：&#39;,elapsed_time)
# 保存输出图像
out_mat = (out_mat.squeeze().transpose((2, 1, 0)) * 255).clip(0, 255).astype(np.uint8)
cv2.imwrite(&#39;输出.jpg&#39;, out_mat)
print(&quot;输出图像已保存。&quot;)`




]]></description>
      <guid>https://stackoverflow.com/questions/77739855/using-real-esrgan-onnx-in-flutter</guid>
      <pubDate>Sun, 31 Dec 2023 16:42:38 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的目标列上出现键错误，并且出现值错误：给定列不是数据帧的列</title>
      <link>https://stackoverflow.com/questions/77738345/why-am-i-having-a-key-error-on-my-target-column-and-a-value-error-that-says-a-g</link>
      <description><![CDATA[# 首先，我分割数据
目标=&#39;会员休闲&#39;
X_train = train.drop(目标, 轴=1)
y_train = 训练[目标]
# 然后，转换我的数字列和分类列
cat_trans = Pipeline([(“imputer”, SimpleImputer(strategy=“most_frequent”)),(&#39;encoder&#39;,OneHotEncoder(handle_unknown=“ignore”, drop=“first”,稀疏=False))])
num_trans = Pipeline([(“imputer”, SimpleImputer(strategy=“mean”)),(“scaler”, MinMaxScaler())])
预处理器 = ColumnTransformer(transformers=[(&#39;num&#39;, num_trans, num),(&#39;cat&#39;, cat_trans, cat)])
管道 = 管道([(&#39;预处理器&#39;, 预处理器)])
# 然后接下来的代码给了我错误
pipeline_fit = pipeline.fit(x_train)

键错误和值错误：给定列不是数据帧的列]]></description>
      <guid>https://stackoverflow.com/questions/77738345/why-am-i-having-a-key-error-on-my-target-column-and-a-value-error-that-says-a-g</guid>
      <pubDate>Sun, 31 Dec 2023 06:05:14 GMT</pubDate>
    </item>
    <item>
      <title>媒体管道是否与深脸一起使用进行人脸识别以获得更好的准确性</title>
      <link>https://stackoverflow.com/questions/77726072/is-media-pipe-is-use-with-deep-face-for-face-recognition-for-better-accuracy</link>
      <description><![CDATA[我使用深脸进行识别，但准确性不好，所以我尝试实现媒体管道，在​​其中提取地标，因此我将其交给深脸以获得更好的准确性。有什么办法可以做到这一点吗？
我从媒体管道中提取特征向量，但如何将其传递到深层脸部？有什么可行的方法吗？
是否使用媒体管道地标进行深度面部识别以提高准确性？]]></description>
      <guid>https://stackoverflow.com/questions/77726072/is-media-pipe-is-use-with-deep-face-for-face-recognition-for-better-accuracy</guid>
      <pubDate>Thu, 28 Dec 2023 09:38:05 GMT</pubDate>
    </item>
    <item>
      <title>Azure 机器学习工作室设计器 - 预测未来销售的算法</title>
      <link>https://stackoverflow.com/questions/77722671/azure-machine-learning-studio-designer-algorithm-to-predict-future-sales</link>
      <description><![CDATA[我正在 Azure 机器学习设计器中进行一项实验，使用线性回归算法构建可以预测未来销售的原型模型。
我在管理前向预测时遇到问题，我找到的所有示例，例如 Microsoft 提供的“回归 - 汽车价格预测（基本）”示例，均是我在管理前向预测时遇到的问题。 (https://github .com/Azure/MachineLearningDesigner/blob/master/articles/samples/regression-automobile-price-prediction-basic.md）用于处理已获得的给定数据集并预测一个缺失值。
我的数据集有 5 列（VoucherDate、Amount、BranchCode、Dolar oficial、Dolar blue）
数据集示例
管道执行
如何根据给定的当前数据来预测未来销售额？然后，我怎样才能看到计算出的所有行？因为在数据预览中我只能看到几行。
我开发了一个管道，可以预测数据集中的销售额（基于 MS 给出的示例），该管道由 SQL Azure 数据库中获得的销售额加上 2 个带有 UDS/ARS 汇率的变量组成报价。
管道执行已完成，评分数据集向我显示给定数据集金额的评分标签。但是，当我尝试生成一个新的数据集（包含我的信息和一些没有销售额的未来记录）时，管道给了我其他结果。]]></description>
      <guid>https://stackoverflow.com/questions/77722671/azure-machine-learning-studio-designer-algorithm-to-predict-future-sales</guid>
      <pubDate>Wed, 27 Dec 2023 15:17:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python 执行情感分析 [关闭]</title>
      <link>https://stackoverflow.com/questions/77714420/how-can-i-perform-sentiment-analysis-using-python</link>
      <description><![CDATA[我是一个尝试做 Python 项目的初学者。请亲自联系我，以便我可以分享我的代码。
尝试对社交媒体帖子进行情感分析，但在执行算法时遇到问题。]]></description>
      <guid>https://stackoverflow.com/questions/77714420/how-can-i-perform-sentiment-analysis-using-python</guid>
      <pubDate>Mon, 25 Dec 2023 17:08:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 和 numpy 进行梯度下降</title>
      <link>https://stackoverflow.com/questions/17784587/gradient-descent-using-python-and-numpy</link>
      <description><![CDATA[def 梯度(X_norm,y,theta,alpha,m,n,num_it):
    temp=np.array(np.zeros_like(theta,float))
    对于范围内的 i(0,num_it)：
        h=np.dot(X_norm,theta)
        #temp[j]=theta[j]-(alpha/m)*( np.sum( (h-y)*X_norm[:,j][np.newaxis,:] ) )
        temp[0]=theta[0]-(alpha/m)*(np.sum(h-y))
        temp[1]=theta[1]-(alpha/m)*(np.sum((h-y)*X_norm[:,1]))
        θ=温度
    返回θ



X_norm,平均值,std=featureScale(X)
#X 的长度（行数）
m=len(X)
X_norm=np.array([np.ones(m),X_norm])
n,m=np.shape(X_norm)
数量=1500
阿尔法=0.01
theta=np.zeros(n,float)[:,np.newaxis]
X_norm=X_norm.transpose()
θ=梯度(X_norm,y,θ,alpha,m,n,num_it)
打印θ

上面代码中我的 theta 是 100.2 100.2，但在 matlab 中应该是 100.2 61.09，这是正确的。]]></description>
      <guid>https://stackoverflow.com/questions/17784587/gradient-descent-using-python-and-numpy</guid>
      <pubDate>Mon, 22 Jul 2013 09:55:30 GMT</pubDate>
    </item>
    </channel>
</rss>