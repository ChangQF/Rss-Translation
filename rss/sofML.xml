<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 07 Dec 2023 09:14:28 GMT</lastBuildDate>
    <item>
      <title>自定义 haar 级联无法正确检测对象</title>
      <link>https://stackoverflow.com/questions/77617944/custom-haar-cascade-not-detecting-object-properly</link>
      <description><![CDATA[我是构建 Haar 级联的新手，我想创建一个用于舌头检测的 Haar 级联。我已尝试使用 Cascade Trainer GUI（版本 3.3.1）构建 Haar 级联 40 多次，但它无法正确检测。谁能帮我解决这个问题吗？
我创建了一个用于存放舌头图像（正样本“p”）的文件夹和另一个用于存放没有舌头的常见图像（负样本“n”）的文件夹，每个文件夹包含 150 个样本图像。一切都运行完美，没有任何错误，但测试图像没有检测到舌头。]]></description>
      <guid>https://stackoverflow.com/questions/77617944/custom-haar-cascade-not-detecting-object-properly</guid>
      <pubDate>Thu, 07 Dec 2023 06:00:31 GMT</pubDate>
    </item>
    <item>
      <title>多输出模块的 Keras 精度不会改变</title>
      <link>https://stackoverflow.com/questions/77617914/keras-accuracy-does-not-change-for-multi-output-module</link>
      <description><![CDATA[我才刚刚开始学习 keras 和机器学习模块，所以请耐心等待，即使我的代码看起来充满了低效和不准确的内容。所以基本上，我想让我的机器学习模块做的是预测欺诈案件的处罚/处罚，输入的形式为（损害金额（$），如果是累犯），目标的格式为（罚款（ $), 监狱(月), 社区服务(小时), 缓刑(月)) - 以下是我的代码：
fname = “文件路径.tsv”
全部输入 = []
所有输出 = []

将 open(fname) 作为 f：
    对于 i，enumerate(f) 中的行：
        如果我&lt; 3:#第一行
            print(&quot;标题:&quot;, line.strip().split(&#39;\t&#39;))
            继续
        fields = line.strip().split(&#39;\t&#39;)
        all_in.append([int(a.replace(&quot;,&quot;, &quot;&quot;)) for a in fields[5:7]])
        all_out.append([int(a.replace(&quot;,&quot;, &quot;&quot;)) for a in fields[7:11]])

case_in = np.array(all_in, dtype = “uint64”)
target_out = np.array(all_out, dtype = “uint64”)

Normalize_layer = tf.keras.layers.Normalization(axis=-1, name = “normalize_in”)
Normalize_layer.adapt(all_in)
Normalize_out = tf.keras.layers.Normalization(axis=-1, name = “normalize_out”)
Normalize_out.adapt(all_out)
denormalize_out = tf.keras.layers.Normalization(axis=-1, invert = True, name = “denormalize_out”)
denormalize_out.adapt(all_out)
缩放输出 = 标准化输出（全部输出）

输入= Normalize_layer（输入（形状= 2））
x = 密集（6，input_dim = 2，激活=“sigmoid”，use_bias = True）（输入）
x = 密集(4, 激活 = “sigmoid”)(x)
y_4 = 密集(1, 激活 = “sigmoid”, 名称 = “y_4”)(x)
惩罚 = Dense(3, 激活 = “sigmoid”, 名称 = “惩罚”)(x)
y_1 = 密集（1，激活=“sigmoid”，名称=“y_1”）（惩罚）
y_2 = 密集（1，激活=“sigmoid”，名称=“y_2”）（惩罚）
y_3 = 密集（1，激活=“sigmoid”，名称=“y_3”）（惩罚）

模型 = 模型（输入 = 输入，输出 = [y_1，y_2，y_3，y_4]）

模型.编译(
    优化器= SGD(learning_rate=0.01,weight_decay=1e-6,momentum=0.9,nesterov=True),
    损失={
        “y_1” ：“均方误差”，
        “y_2” ：“均方误差”，
        “y_3” ：“均方误差”，
        “y_4” ：“均方误差”
    },
    指标=[&#39;准确性&#39;]
）

模型.拟合(
    案例输入，
    横向扩展，
    batch_size = 10,##数据增加后，增加
    纪元 = 300，
    详细 = 2,
    验证分割= 0.8
）

对于 model.layers 中的图层：
    print(&quot;=====图层:&quot;, 图层名称,&quot;=====&quot;)
    if layer.get_weights() != []:
        权重=layer.get_weights()[0]
        偏差=layer.get_weights()[1]
        print(&quot;权重：&quot;)
        打印（权重）
        print(“偏差：”)
        打印（偏差）
    别的：
        print(&quot;权重：&quot;, [])

而且，一旦开始训练模型，准确性就根本不会改变。另外，当我试图解决这个问题时，我搞砸了一些东西，使大约 50 个数据集的数据只是 .fit 操作的一批，尽管我的批量大小为 10。我只是尝试了太多的事情来修复我的代码互联网上到处都是，这让一切变得更加混乱。
最初，我尝试对数据进行标准化，包括我的来龙去脉，这解冻了我的损失函数，但它并没有真正对准确性函数产生任何影响。我还将 ReLU 函数也更改为 sigmoid 函数，因为在某些时候，死 ReLU 似乎可能存在问题，因为我的层上的偏差没有从最初的 0 更新。之后，我不断地研究优化器、损失函数和纪元，但都无济于事。如果您能帮助我使该模块真正发挥其作用，我将不胜感激，而且对代码的一般反馈也会有很大帮助。]]></description>
      <guid>https://stackoverflow.com/questions/77617914/keras-accuracy-does-not-change-for-multi-output-module</guid>
      <pubDate>Thu, 07 Dec 2023 05:51:59 GMT</pubDate>
    </item>
    <item>
      <title>从 ampligraph 导入复杂数据时如何修复此错误 ImportError</title>
      <link>https://stackoverflow.com/questions/77617662/how-to-fix-this-error-importerror-while-importing-complex-from-ampligraph</link>
      <description><![CDATA[ImportError Traceback（最近一次调用最后一次）
&lt;ipython-input-41-449fec1eb93c&gt;在&lt;细胞系：28&gt;()
     26 从 imblearn.over_sampling 导入 RandomOverSampler、SMOTE、ADASYN
     27 从 imblearn.under_sampling 导入 ClusterCentroids、RandomUnderSampler、NearMiss、TomekLinks
---&gt; 28 从 ampligraph.latent_features 导入 ComplEx
     29

ImportError：无法从“ampligraph.latent_features”导入名称“ComplEx”（/usr/local/lib/python3.10/dist-packages/ampligraph/latent_features/__init__.py）

我尝试降级放大器来修复它，但它不起作用。我也尝试阅读新版本的文档，但我不知道，因为我是新手。`]]></description>
      <guid>https://stackoverflow.com/questions/77617662/how-to-fix-this-error-importerror-while-importing-complex-from-ampligraph</guid>
      <pubDate>Thu, 07 Dec 2023 04:28:22 GMT</pubDate>
    </item>
    <item>
      <title>考虑季节性预测下一个值</title>
      <link>https://stackoverflow.com/questions/77617162/predicting-next-value-considering-seasonality</link>
      <description><![CDATA[我的数据如下
日月目标
3 3 8
5 4 9

日期不必采用连续日期格式。该日期可以是 1/1/2023，然后下一个日期也可以是 2/2/2023。目前，我正在使用 LSTM 来预测下一组序列，并且我的 LSTM 模型更频繁地预测平均值，而不考虑季节性。在某些情况下，十月也有不同的值，模型无法预测。如果有任何替代模型或方法，请就此提出建议。]]></description>
      <guid>https://stackoverflow.com/questions/77617162/predicting-next-value-considering-seasonality</guid>
      <pubDate>Thu, 07 Dec 2023 01:19:56 GMT</pubDate>
    </item>
    <item>
      <title>将模型部署到 GCP 的 Vertex 时如何利用 L4 GPU</title>
      <link>https://stackoverflow.com/questions/77617088/how-to-utilize-an-l4-gpu-when-deploying-a-model-to-gcps-vertex</link>
      <description><![CDATA[我将一个模型部署到 GCP 上的 Vertex，用于部署它的配置代码如下所示：
dedicated_resources=dict(
    机器规格=字典（
        machine_type=“g2-standard-8”，
        Accelerator_type=“NVIDIA_L4”，
        Accelerator_count=1,
    ),
    min_replica_count = 2，
    最大副本数=10,
    autoscaling_metric_specs=[
        字典（
            metric_name=“aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle”，
            目标=20，
        ),
    ],
）


该模型配置为利用 GPU 资源，并且此配置已在 n1 机器上使用 P4 和 P100 运行。当在具有 L4 加速器的 g2 机器上运行模型时，请求会导致极高的延迟和 CPU 利用率，而 GPU 利用率则稳定在 0%。
我不知道接下来该去哪里，也不知道最好的故障排除选项是什么。]]></description>
      <guid>https://stackoverflow.com/questions/77617088/how-to-utilize-an-l4-gpu-when-deploying-a-model-to-gcps-vertex</guid>
      <pubDate>Thu, 07 Dec 2023 00:51:07 GMT</pubDate>
    </item>
    <item>
      <title>从包含单列的每一行中的子列表的 DataFrame 中获取特征</title>
      <link>https://stackoverflow.com/questions/77616611/getting-features-from-a-dataframe-containing-sublists-in-every-row-of-a-single-c</link>
      <description><![CDATA[我正在完成一项机器学习的学校作业，但遇到了麻烦。我这里有我的数据框：
6列DataFrame，subject_id，task_code，data_lw，t_start，t_end&lt; /p&gt;
问题始于 data_lw 列。此列的每一行都有一个包含 1000 到 1600 行数据的三列子列表。
列中单个元素的示例
我遇到的问题是提取特征。我之前通过这样做提取了特征：
导入 pandas 作为 pd
导入 tsfel
 
cfg=tsfel.get_features_by_domain(&#39;统计&#39;)
特征列表=[]
对于 df[&#39;data_lw&#39;] 中的索引：
  测试= tsfel.time_series_features_extractor（cfg，索引）
  feature_list.append（测试）

打印（测试）

但是，我总共得到了 120 个特征。这 120 个被分成三列，每列 40 个。我想它的作用是将它们逐行平均在一起，直到完成。当我告诉我的教授这一点时，他们说我应该每行获得 300 个特征，所以我想知道如何解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77616611/getting-features-from-a-dataframe-containing-sublists-in-every-row-of-a-single-c</guid>
      <pubDate>Wed, 06 Dec 2023 22:14:21 GMT</pubDate>
    </item>
    <item>
      <title>我应该在 Seq2seq 模型中的解码器输入上使用嵌入吗？</title>
      <link>https://stackoverflow.com/questions/77616375/should-i-use-embeddings-on-input-of-decoder-in-seq2seq-model</link>
      <description><![CDATA[设置：
我在 Pytorch 中使用序列到序列模型来预测下一个标记，并使用先前标记的序列。它对编码器和解码器使用相同的词汇。该模型是通过一批相同长度的（src，trg）张量对（对于teacher_forcing）进行训练的，尽管trg张量由一个标记组成，其余的都是“”。索引，损失函数会忽略它们。
问题：
我的朋友是一位经验丰富的机器学习工程师，他审查了我的作业，但不太确定我是否应该嵌入解码器输入序列。我尝试深入研究一下这个问题，但似乎我能找到的几乎所有 seq2seq 模型的示例都是为翻译而构建的，因此解码器和编码器中的每种语言分别有两种不同的嵌入。
下面是模型代码：
类编码器（nn.Module）：
    def __init__(self, input_dim, emb_dim = 50, hid_dim = 128, n_layers = 1, dropout = 0.05):
        超级().__init__()
        self.input_dim = input_dim
        self.embedding_dim = emb_dim
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)

    def 转发（自身，src）：
        嵌入 = self.embedding(src)
        输出，（隐藏，单元）= self.rnn（嵌入）
        返回隐藏单元格


解码器类（nn.Module）：

    def __init__(self, 嵌入, vocab_dim, emb_dim = 50 , hid_dim = 128, n_layers = 1, dropout = 0.05):
        超级().__init__()

        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.embeddings = 嵌入
        self.emb_dim = emb_dim
        self.fc_out = nn.Linear(hid_dim, vocab_dim)
        self.out = nn.Sequential(
            nn.Linear(hid_dim, vocab_dim),
            nn.Sigmoid()
        ）

    defforward（自身，输入，隐藏，单元格）：
        输入 = 输入.unsqueeze(0)
        嵌入 = self.embeddings(输入)
        输出，（隐藏，单元）= self.rnn（嵌入，（隐藏，单元））
        预测 = self.out(output.squeeze(0))
        返回预测、隐藏、单元格


类 Seq2seq(nn.Module):
    def __init__(自身、编码器、解码器、设备):
        超级().__init__()

        self.encoder = 编码器
        self.decoder = 解码器
        self.device = 设备

    def 前向（自身、src、trg、teacher_force_ratio=0.05）：
        批量大小 = trg.size(1)
        max_len = trg.size(0)
        trg_vocab_size = self.encoder.input_dim

        输出= torch.zeros（max_len，batch_size，trg_vocab_size）.to（self.device）
        隐藏，单元格 = self.encoder(src)
        输入 = trg[0, :]

        对于 t in range(1, max_len)：
            输出、隐藏、单元 = self.decoder(输入、隐藏、单元)
            输出[t] = 输出
            Teacher_force = random.random() &lt;教师力量比
            top1 = 输出.max(1)[1]
            输入 = (trg[t] if Teacher_force else top1)

        返回输出

和训练脚本：
model.train()
对于范围内的纪元（纪元）：
    对于batch_idx，tqdm中的批处理（枚举（tensorized_train_pairs））：
        input_data = 批次[0].to(设备)
        目标 = 批次[1].to(设备)

        输出=模型（输入数据，目标）

        输出 = 输出[1:].reshape(-1, 输出.shape[2])
        目标 = 目标[1:].reshape(-1)

        优化器.zero_grad()
        损失 = loss_func(输出, 目标)
        train_losses.append(损失)
        
        如果（batch_idx％1000==0）：
            print(f&quot;批次 idx:{batch_idx}&quot;, f&quot;loss: {loss}&quot;, f&quot;epoch:{epoch}&quot;)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.,norm_type=2)

        优化器.step()

此外，模型训练没有产生任何结果：损失函数（交叉熵）只是波动。我尝试了几种超参数组合，包括删除梯度裁剪，但没有效果。
这是损失图：
]]></description>
      <guid>https://stackoverflow.com/questions/77616375/should-i-use-embeddings-on-input-of-decoder-in-seq2seq-model</guid>
      <pubDate>Wed, 06 Dec 2023 21:20:41 GMT</pubDate>
    </item>
    <item>
      <title>我想使用具有多个输入的tensorflow js获得接下来的25个值预测</title>
      <link>https://stackoverflow.com/questions/77616362/i-want-to-get-next-25-value-predictions-using-tensorflow-js-with-multiple-input</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77616362/i-want-to-get-next-25-value-predictions-using-tensorflow-js-with-multiple-input</guid>
      <pubDate>Wed, 06 Dec 2023 21:19:20 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中使用分布式数据并行处理具有昂贵块读取时间的大型数据集？</title>
      <link>https://stackoverflow.com/questions/77615926/how-to-use-distributed-data-parallelism-in-pytorch-with-large-datasets-with-expe</link>
      <description><![CDATA[我有一个由 100 个 .npz 文件组成的数据集，其中包含形状为 (285341, 60664) 的 98% 稀疏 scipy CSR 矩阵，其中行是样本数，列是特征大小。我想在数据集上运行变分自动编码器。我可以访问大内存节点（每个 1.5TB RAM）、计算节点（总共 190GB RAM）和 4 个 GPU（总共 190GB RAM）。目前主要的瓶颈是加载数据，因为整个数据集无法立即装入内存。有哪些仍然允许随机洗牌并减少读取时间的建议？
已尝试过的内容：

使用 MapDataset 来分离不断填充队列并在加载时随机洗牌的加载线程。似乎只能在一个 GPU 上工作，但无法在工作人员之间分配。
IterableDataset 的功能与 MapDataset 相同，但使用了worker_init_fn 来分割负载，但不断出现未实现的错误。 （注意：类太大而且吓人，但如果需要的话可以附加，以及 DataLoader 初始化调用）
]]></description>
      <guid>https://stackoverflow.com/questions/77615926/how-to-use-distributed-data-parallelism-in-pytorch-with-large-datasets-with-expe</guid>
      <pubDate>Wed, 06 Dec 2023 19:51:51 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：“Flags”对象没有属性“c_contigious”</title>
      <link>https://stackoverflow.com/questions/77615883/attributeerror-flags-object-has-no-attribute-c-contiguous</link>
      <description><![CDATA[我正在阅读 Aurélien Géron 编写的《机器学习实践》一书，但遇到了以下错误。
代码：
y_train_large = (y_train.astype(&quot;int&quot;) &gt;= 7)
y_train_odd = (y_train.astype(“int”) % 2 == 1)
y_multilabel = np.c_[y_train_large, y_train_odd]

＃模型
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_multilabel)

y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)

最后一行产生以下错误：
&lt;前&gt;&lt;代码&gt;{
AttributeError: &#39;Flags&#39; 对象没有属性 &#39;c_contigious&#39;”
}

由于我正在关注这本书，所以我希望这段代码能够工作。我尝试过 Google Bard 和 Claude AI 聊天机器人的解决方案，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77615883/attributeerror-flags-object-has-no-attribute-c-contiguous</guid>
      <pubDate>Wed, 06 Dec 2023 19:42:47 GMT</pubDate>
    </item>
    <item>
      <title>安装 Toad 时子进程退出并出现错误</title>
      <link>https://stackoverflow.com/questions/77613056/subprocess-exited-with-error-while-installing-toad</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77613056/subprocess-exited-with-error-while-installing-toad</guid>
      <pubDate>Wed, 06 Dec 2023 12:29:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的一维贝叶斯 CNN（通过使用工作 CNN 的 Convolution1DFlipout 和 DenseFlipout 替换卷积层和密集层而制成）无法训练？</title>
      <link>https://stackoverflow.com/questions/77602609/why-does-my-1d-bayesian-cnn-made-by-replacing-the-convolution-and-dense-layers</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77602609/why-does-my-1d-bayesian-cnn-made-by-replacing-the-convolution-and-dense-layers</guid>
      <pubDate>Mon, 04 Dec 2023 21:17:27 GMT</pubDate>
    </item>
    <item>
      <title>spaCy 值错误：[E1041] 需要字符串、文档或字节作为输入，但得到：<class 'float'></title>
      <link>https://stackoverflow.com/questions/77596731/spacy-value-error-e1041-expected-a-string-doc-or-bytes-as-input-but-got</link>
      <description><![CDATA[我正在尝试使用 spaCy 对中文输入进行矢量化。
我的代码如下：


nlp = spacy.load(&#39;zh_core_web_md&#39;)

def tokenize_and_vectorize_textZH(文本):
    clean_tokensZH = []
    对于 nlp(text) 中的标记：
        if (不是 token.is_stop) &amp; (token.lemma_ != &#39;-PRON-&#39;) &amp; （不是 token.is_punct）：
          # -PRON- 是 spaCy 用于任何代词的特殊全包引理，我们要排除这些
            if (len(token.vector) != 300):
              打印（令牌）
            clean_tokensZH.append(token.vector)
    返回 np.array(clean_tokensZH)
    
    
all_summmed_vecsZH = []

def sum_vecsZH(输入):
  tokenized_vectorsZH = input.apply(tokenize_and_vectorize_textZH)
  tokenized_vectorZH = tokenized_vectorsZH.to_numpy()

  打印（len（tokenized_vectorsZH））
  #print(类型(标记化向量))

  对于 tokenized_vectorsZH 中的行：

    #打印（行）

    summed_vecZH = [0]*300 # 从 300 个零的列表开始

    for vec in row: # 循环遍历与行中每个标记对应的每个向量
      #if (len(vec) != 300):
        #打印（向量）
      summed_vecZH += vec

    all_summmed_vecs.append(summed_vecZH)

  #print(tokenized_vectors[0][0].向量)
  
  
#@title 应用矢量化
sum_vecsZH(X_trainZH)
打印（all_summmed_vecs）

sum_vecsZH(y_trainZH)
打印（all_summmed_vecs）

sum_vecsZH(X_testZH)
打印（all_summmed_vecs）

sum_vecsZH(y_testZH)
打印（all_summmed_vecs）



最后 8 行的预期输出应与此类似：
33384
33384
14308
14308
这是我的数据示例：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

not_cyberbullying
你看起来像个吉普赛人抱歉，我不遗憾


&lt;正文&gt;

不是网络欺凌
RT @Kurdsnews：土耳其国家在过去11年中杀害了241名儿童 http/t.co/JlvkE1epws #news ##GoogleÇeviriciTopluluğuKürtçeyideE...


性别
如果我叫你婊子你就生气了，那就别叫自己咕咕女。




这个错误的原因是什么？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77596731/spacy-value-error-e1041-expected-a-string-doc-or-bytes-as-input-but-got</guid>
      <pubDate>Mon, 04 Dec 2023 00:59:14 GMT</pubDate>
    </item>
    <item>
      <title>在 scikit-learn 中使用 skopt.BayesSearchCV 时如何修复“numpy.int”属性错误？</title>
      <link>https://stackoverflow.com/questions/76321820/how-to-fix-the-numpy-int-attribute-error-when-using-skopt-bayessearchcv-in-sci</link>
      <description><![CDATA[当我在官方文档上运行以下代码时，出现错误。
最小示例
from skopt import BayesSearchCV
从 sklearn.datasets 导入 load_digits
从 sklearn.svm 导入 SVC
从 sklearn.model_selection 导入 train_test_split

X, y = load_digits(n_class=10, return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=.25, random_state=0)

# log-uniform：理解为通过改变 x 对 p = exp(x) 进行搜索
选择 = BayesSearchCV(
    SVC(),
    {
        &#39;C&#39;: (1e-6, 1e+6, &#39;对数均匀&#39;),
        &#39;伽玛&#39;: (1e-6, 1e+1, &#39;对数均匀&#39;),
        &#39;level&#39;: (1, 8), # 整数值参数
        &#39;kernel&#39;: [&#39;线性&#39;, &#39;poly&#39;, &#39;rbf&#39;], # 分类参数
    },
    n_iter=32,
    简历=3
）

opt.fit(X_train, y_train)

最后一行产生错误：
&lt;块引用&gt;
属性错误
模块“numpy”没有属性“int”。
np.int 是内置 int 的已弃用别名。要避免现有代码中出现此错误，请单独使用 int。这样做不会改变任何行为并且是安全的。替换 np.int 时，您可能希望使用例如np.int64 或 np.int32 指定精度。如果您想查看当前的使用情况，请查看发行说明链接以获取更多信息。
别名最初在 NumPy 1.20 中已弃用；有关更多详细信息和指导，请参阅原始发行说明：
https://numpy.org/devdocs/release/1.20.0-notes .html#deprecations

如何解决这个问题？还有其他方法来实现贝叶斯搜索吗？
也许skopt的版本太旧了。还有其他方法来实现贝叶斯搜索吗？除了网格搜索、随机搜索和贝叶斯搜索之外，还有其他方法可以帮助我选择机器学习模型的超参数吗？]]></description>
      <guid>https://stackoverflow.com/questions/76321820/how-to-fix-the-numpy-int-attribute-error-when-using-skopt-bayessearchcv-in-sci</guid>
      <pubDate>Wed, 24 May 2023 09:06:15 GMT</pubDate>
    </item>
    <item>
      <title>在python中找到最终的回归方程</title>
      <link>https://stackoverflow.com/questions/59930627/finding-final-regression-equation-in-python</link>
      <description><![CDATA[如何找到包含所有变量系数的最终回归模型方程？有什么方法吗？ ]]></description>
      <guid>https://stackoverflow.com/questions/59930627/finding-final-regression-equation-in-python</guid>
      <pubDate>Mon, 27 Jan 2020 11:52:08 GMT</pubDate>
    </item>
    </channel>
</rss>