<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 11 Mar 2024 06:17:43 GMT</lastBuildDate>
    <item>
      <title>调用 OnActionReceived 或 RequestDecision 让 Unity ML-Agent 轮流执行，观察次数较少 (0)</title>
      <link>https://stackoverflow.com/questions/78138694/call-onactionreceived-or-requestdecision-to-make-unity-ml-agents-do-their-turn</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78138694/call-onactionreceived-or-requestdecision-to-make-unity-ml-agents-do-their-turn</guid>
      <pubDate>Mon, 11 Mar 2024 06:13:00 GMT</pubDate>
    </item>
    <item>
      <title>在 MERN Stack 应用程序中集成线性回归：Python 还是 JavaScript？</title>
      <link>https://stackoverflow.com/questions/78138619/integrating-linear-regression-in-mern-stack-application-python-or-javascript</link>
      <description><![CDATA[我使用 MERN 堆栈（MongoDB、Express.js、React、Node.js）开发了一个餐厅管理面板，现在正在寻求实现线性回归来预测销售或根据历史数据预测客户流量。后端完全采用 Node.js，但我正在考虑使用 Python 作为线性回归部分，因为它具有广泛的库和对数据科学的支持（如 NumPy、pandas 和 scikit-learn）。
我的困境是，是坚持使用 JavaScript/Node.js 以保持堆栈一致，还是将 Python 引入其中，以获取其卓越的机器学习功能。我担心将 Python 与现有 Node.js 后端集成的潜在复杂性以及如何管理两者之间的通信（如果这是我选择的路线）。另一方面，我也在考虑 Python 库可能为线性回归任务提供的性能和实现的简易性。
我希望了解以下方面的见解：
将 Python 进行线性回归集成到 MERN 堆栈应用程序的可行性和最佳实践。
如果我沿着这条路走，管理 Node.js 和 Python 之间的通信的潜在挑战和解决方案。
如果留在 JS 生态系统内，建议可以有效处理线性回归的 JavaScript 库。

最终，我正在寻找有关将线性回归合并到我的项目中的最佳路径的指导，权衡 Python 与 JavaScript 对于这个特定用例的优缺点。
我最初探索直接在 Node.js 中实现线性回归，希望保持一致的技术堆栈。我尝试了几个 JavaScript 库，例如用于基本统计操作的 simple-statistics 和用于更多面向机器学习的任务的 mljs，希望它们能够提供一种将线性回归模型应用到我的数据集的简单方法。
通过这些库，我成功地在 Node.js 中实现了基本的线性回归模型。我的期望是，这种方法不仅能够满足我所需的预测准确性，而且还可以通过避免跨语言集成来保持应用程序部署和维护的简单性。
然而，结果好坏参半。虽然我能够开发和运行线性回归模型，但我遇到了两个主要问题：
性能和可扩展性：JavaScript 解决方案适用于小型数据集，但当我尝试扩大数据大小以更接近地反映餐厅管理面板的实际使用场景时，性能未达到我的预期。处理时间比预期的要长，我开始担心这个解决方案的可扩展性。

功能集和易用性：虽然我使用的库提供了线性回归的基本功能，但我发现与我所知道的 Python 生态系统（例如 scikit）中可用的库相比，它们缺乏功能的广度和高级统计分析的易用性-学习）。例如，我想要更复杂的方法来处理模型拟合、诊断和验证，以提高预测准确性，这在 Python 库中似乎更容易访问。

这些经历让我考虑将 Python 作为实现项目的线性回归部分的替代方案，尽管我最初打算将所有内容保留在 Node.js 环境中。这里的期望不仅是实现更好的性能和可扩展性，还包括访问更丰富的数据分析和机器学习工具集，以增强我的餐厅管理面板中预测的功能和准确性。]]></description>
      <guid>https://stackoverflow.com/questions/78138619/integrating-linear-regression-in-mern-stack-application-python-or-javascript</guid>
      <pubDate>Mon, 11 Mar 2024 05:49:33 GMT</pubDate>
    </item>
    <item>
      <title>使用已弃用的框架为 NER 任务实现 BiLSTM-CRF</title>
      <link>https://stackoverflow.com/questions/78138589/implement-bilstm-crf-for-ner-task-using-deprecated-frameworks</link>
      <description><![CDATA[我一直在尝试为命名实体识别任务构建 BiLSTM-CRF 模型，显然，我一直在使用 TensorFlow 2.16 和 Keras 3.0。然而，在尝试使用已弃用的 keras_contrib 或 tensorflow_addons GitHub 资源实现 CRF 层时。我遇到了无数问题，因为这些框架可能与最新的 TF 和 Keras 版本不兼容。但是，我不愿意仅仅为了这个任务而降级到较低版本的 TF。我可以使用任何合适的替代方案来完成我的任务吗？
添加了以下代码。
from keras.layers import Embedding、SimpleRNN、Dense、LSTM、GRU、双向、TimeDistributed、输入
从 keras_contrib.layers 导入 CRF
从 keras_contrib.losses 导入 crf_loss
从 keras_contrib.metrics 导入 crf_accuracy
将张量流导入为 tf
从 keras.optimizers 导入 RMSprop
导入keras


优化器 = Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, amsgrad=False)
输入=输入（形状=（最大长度，））
模型=嵌入（vocab_size，embedding_dimension，embeddings_initializer =“均匀”，trainable = False）（输入）
模型= LSTM(360, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, kernel_initializer=keras.initializers.he_normal())(模型)
模型=双向（LSTM（180，return_sequences = True，dropout = 0.2，recurrent_dropout = 0.2，kernel_initializer = keras.initializers.he_normal（）））（模型）
模型= TimeDistributed（密集（27，激活=&#39;relu&#39;））（模型）
cRF = CRF(单位=27，sparse_target=True)
输出=crf（模型）
模型= keras.Model（输入，输出）
model.compile（优化器=优化器，损失=crf_loss，指标=[crf_accuracy，&#39;准确性&#39;]）
模型.summary()

输出：
&lt;前&gt;&lt;代码&gt;┏────────────────────────────────────────────────────────────────┳────────────────── ──────────────────────────────┳──────────────────────────────┓
┃ 层（类型） ┃ 输出形状 ┃ 参数# ┃
┡────────────────────────────────────────────────────────────────────╇────────────────────────────────────────────────────────────────────────────── ──────────────────╇────────────────────────────────┩
│ input_layer（输入层） │ （无，78） │ 0 │
├──────────────────────────────────┼──────────────── ──────────┼────────────────┤
│ 嵌入（Embedding） │ （无、78、300） │ 5,727,000 │
├──────────────────────────────────┼──────────────── ──────────┼────────────────┤
│ lstm (LSTM) │ (无, 78, 360) │ 951,840 │
├──────────────────────────────────┼──────────────── ──────────┼────────────────┤
│ 双向（双向） │ （无、78、360） │ 779,040 │
├──────────────────────────────────┼──────────────── ──────────┼────────────────┤
│ 时间分布 │ (无, 78, 27) │ 9,747 │
│ (时间分布) │ │ │
├──────────────────────────────────┼──────────────── ──────────┼────────────────┤
│ CRF (CRF) │ (无, 78, 27) │ 1,539 │
└──────────────────────────────────┴──────────────── ──────────┴────────────────┘


 总参数：7,469,166 (28.49 MB)
 可训练参数：7,469,166 (28.49 MB)
 不可训练参数：0 (0.00 B)
**

历史 = model.fit(
    火车文本，火车标签，
    验证数据=（val_text，val_labels），
    纪元=10，
    批量大小=32，
    详细=1，
    回调=[MacroF1Callback((val_text, val_labels), (train_text, train_labels))]
）

输出：
&lt;前&gt;&lt;代码&gt;----&gt; 7 历史记录 = model.fit(
      8 火车文本，火车标签，
      9 验证数据=(val_text, val_labels),

error_handler 中的 c:\keras\src\utils\traceback_utils.py(*args, **kwargs)
    121 # 要获取完整的堆栈跟踪，请调用：
    122 # `keras.config.disable_traceback_filtering()`
--&gt; 123 从 None 引发 e.with_traceback(filtered_tb)
    124最后：
    125 删除filtered_tb

调用中的 c:\Python310\lib\site-packages\keras_contrib\layers\crf.py(self, X, mask)
    290
    第291章
--&gt; 292 test_output = self.viterbi_decoding（X，掩码）
    293 其他：
    第294章

viterbi_decoding(self, X, mask) 中的 c:\Python310\lib\site-packages\keras_contrib\layers\crf.py
    第557章
...
**模块“keras.backend”没有属性“点”**

CRF.call() 收到的参数：
  • X=tf.Tensor(形状=(无, 78, 27), dtype=float32)
  • 掩码=无
]]></description>
      <guid>https://stackoverflow.com/questions/78138589/implement-bilstm-crf-for-ner-task-using-deprecated-frameworks</guid>
      <pubDate>Mon, 11 Mar 2024 05:38:32 GMT</pubDate>
    </item>
    <item>
      <title>使用 opacus 尝试训练 dp 模型，但遇到 `TypeError: __init__() Missing 1 requiredpositional argument: 'module'`</title>
      <link>https://stackoverflow.com/questions/78138246/using-opacus-try-to-train-a-dp-model-but-meet-typeerror-init-missing-1</link>
      <description><![CDATA[我正在使用 opacus 通过以下代码进行 dp 训练模型。然而，在运行PrivacyEngine线路时出现了一个错误。
transformed_data = self.table.transform(dataframe)
        加载器= DataLoader（transformed_data，batch_size = self.batch_size，shuffle = True）
        self.optimizer = torch.optim.Adam(self.parameters())
        # self.privacy_engine = PrivacyEngine(accountant=“rdp”, secure_mode=True)
        # self.privacy_engine = PrivacyEngine()
        self.net, self.optim, self.train_data = PrivacyEngine(accountant=“rdp”, # 错误 secure_mode=True).make_private_with_epsilon( \
            模块= self.net，\
            优化器 = self.optim,\
            data_loader = 加载器,\
            target_epsilon=self.epsilon_target,\
            target_delta=self._delta,\
            epochs=self.epoch_target,\
            max_grad_norm=self.max_grad_norm,\
            poisson_sampling=真，\
        ）
        对于范围内的 epoch_idx（self.start_epoch，self.epochs）：
            开始时间 = 时间.time()
            损失= self._train_epoch（加载器，约束，epoch_idx，show_progress = show_progress，** kwargs）
            train_loss = sum(损失) if isinstance(损失, 元组) else 损失
            结束时间 = time.time()
            如果详细的话：
                print(“epoch %d: 训练损失 %.3f, 时间成本 %.3fs” % (epoch_idx, train_loss, end_time - start_time))

错误如下：
 文件“/home/ruc/xiaotong/OpenDataGen/log/20240310/open-data-gen/src/model/dpautoregressive/dpmade.py”，第 401 行，适合
    self.net, self.optim, self.train_data = PrivacyEngine(accountant=“rdp”, secure_mode=True).make_private_with_epsilon( \
类型错误：__init__() 缺少 1 个必需的位置参数：“模块”

我很困惑，我认为函数中存在模块，但它告诉我缺少“模块”。
我认为这段代码应该是正确的。]]></description>
      <guid>https://stackoverflow.com/questions/78138246/using-opacus-try-to-train-a-dp-model-but-meet-typeerror-init-missing-1</guid>
      <pubDate>Mon, 11 Mar 2024 03:09:04 GMT</pubDate>
    </item>
    <item>
      <title>我可以将 MLflow 自动记录与 Vanilla PyTorch 一起使用吗？</title>
      <link>https://stackoverflow.com/questions/78138203/can-i-use-mlflow-autologging-with-vanilla-pytorch</link>
      <description><![CDATA[我在官方 MLflow 文档中发现了以下声明：
&lt;块引用&gt;
对普通 PyTorch（即仅子类 torch.nn.Module 的模型）的自动记录支持仅自动记录对 torch.utils.tensorboard.SummaryWriter 的 add_scalar 和 add_hparams 方法的调用到毫升流。

基于此，我假设即使使用普通 PyTorch，add_scalar 和 add_hparams 方法也会通过自动记录自动执行。
因此，我运行了以下示例代码，但 MLflow 中没有记录任何内容：
导入火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
导入火炬视觉
导入 torchvision.transforms 作为变换
导入流量

mlflow.autolog()

变换 = 变换.Compose([变换.ToTensor(), 变换.Normalize((0.5,), (0.5,))])
trainset = torchvision.datasets.FashionMNIST(root=&#39;./data&#39;,train=True,download=True,transform=transform)
trainloader = torch.utils.data.DataLoader(trainset,batch_size=4,shuffle=True)


类 Net(nn.Module):
    def __init__(自身):
        超级（网络，自我）.__init__()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)

    def 前向（自身，x）：
        x = x.view(-1, 28 * 28)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        返回x


净=净()

标准 = nn.CrossEntropyLoss()
优化器 = optim.SGD(net.parameters(), lr=0.01, 动量=0.9)

对于范围（2）中的纪元：
    运行损失 = 0.0
    对于 i，enumerate(trainloader, 0) 中的数据：
        输入，标签=数据
        优化器.zero_grad()
        输出 = 净值（输入）
        损失=标准（输出，标签）
        loss.backward()
        优化器.step()
        running_loss += loss.item()
        如果我% 2000 == 1999：
            print(f&#39;[{epoch + 1}, {i + 1}] 损失: {running_loss / 2000}&#39;)
            运行损失 = 0.0

我是不是做错了什么？
我尝试了以下版本：

mlflow==2.11.1（最新）
火炬==2.2.1
torchvision==2.2.1
张量板==2.16.2
]]></description>
      <guid>https://stackoverflow.com/questions/78138203/can-i-use-mlflow-autologging-with-vanilla-pytorch</guid>
      <pubDate>Mon, 11 Mar 2024 02:50:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 绘制梯度下降曲线</title>
      <link>https://stackoverflow.com/questions/78137739/plotting-of-gradient-descent-curves-by-using-keras</link>
      <description><![CDATA[我在 Keras 中实现了以下代码，该代码使用加州住房数据集，试图绘制 theta 1 和 theta 2 的值，以及随机梯度下降、批量梯度或小批量的选择如何影响结果： 
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
从 keras.models 导入顺序
从 keras.layers 导入密集
从 keras.optimizers 导入 SGD
从 sklearn.datasets 导入 fetch_california_housing
从 sklearn.preprocessing 导入 StandardScaler

# 加载加州住房数据集
加州住房 = fetch_加州住房()
X, y = california_housing.data, california_housing.target

# 标准化特征
定标器=标准定标器()
X_归一化 = 缩放器.fit_transform(X)

def build_model():
    模型=顺序（[
    密集（64，激活=“relu”，input_shape=（8，）），
    密集(64，激活=“relu”)，
    密集(1)
    ]）
    #model.compile（优化器=“rmsprop”，损失=“mse”，指标=[“mae”]）
    返回模型


sgd_optimizer = SGD(lr=0.001) # 随机梯度下降
minibatch_sgd_optimizer = SGD(lr=0.001) # 小批量梯度下降
batch_sgd_optimizer = SGD(lr=0.001) # 批量梯度下降


# 编译模型
模型=build_model()
model.compile(loss=&#39;mse&#39;, 优化器=sgd_optimizer)

theta1_sgd、theta2_sgd = []、[]
theta1_minibatch_sgd、theta2_minibatch_sgd = []、[]
theta1_batch_sgd、theta2_batch_sgd = []、[]


# 执行梯度下降并存储 theta 值的函数
def Perform_gradient_descent（优化器，batch_size=None）：
    theta1_列表、theta2_列表 = []、[]
    损失历史记录 = []
    for _ in range(5): # 纪元数
        历史记录=model.fit(X_normalized, y, epochs=1,batch_size=batch_size, verbose=0)
        loss_history.append(history.history[&#39;loss&#39;][0])
        weights = model.layers[0].get_weights()[0].flatten() # 获取当前 theta 值
        theta1_list.append(权重[0])
        theta2_list.append(权重[1])
    打印（theta1_列表，“”，theta2_列表）
    返回loss_history，theta1_list，theta2_list

# 使用不同的优化器执行梯度下降
loss_sgd, theta1_sgd, theta2_sgd = Perform_gradient_descent(sgd_optimizer, batch_size=1) # 随机梯度下降
loss_minibatch_sgd, theta1_minibatch_sgd, theta2_minibatch_sgd = Perform_gradient_descent(minibatch_sgd_optimizer, batch_size=32) # 小批量梯度下降
loss_batch_sgd, theta1_batch_sgd, theta2_batch_sgd = Perform_gradient_descent(batch_sgd_optimizer, batch_size=len(X_normalized)) # 批量梯度下降

# 绘制损失与纪元数的关系图
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(loss_sgd) + 1), loss_sgd, label=&#39;随机梯度下降&#39;)
plt.plot(range(1, len(loss_minibatch_sgd) + 1), loss_minibatch_sgd, label=&#39;小批量梯度下降&#39;)
plt.plot(range(1, len(loss_batch_sgd) + 1), loss_batch_sgd, label=&#39;批量梯度下降&#39;)
plt.xlabel(&#39;历元数&#39;)
plt.ylabel(&#39;损失&#39;)
plt.title(&#39;损失与历元数&#39;)
plt.图例()
plt.网格（真）
plt.show()

# 绘制梯度下降轨迹
plt.figure(figsize=(10, 6))
plt.plot(theta1_sgd,theta2_sgd,label=&#39;随机梯度下降&#39;,marker=&#39;o&#39;)
plt.plot(theta1_minibatch_sgd, theta2_minibatch_sgd, label=&#39;小批量梯度下降&#39;,marker=&#39;s&#39;)
plt.plot(theta1_batch_sgd, theta2_batch_sgd, label=&#39;批量梯度下降&#39;,marker=&#39;x&#39;)
plt.xlabel(&#39;Theta 1&#39;)
plt.ylabel(&#39;Theta 2&#39;)
plt.title(&#39;梯度下降轨迹&#39;)
#plt.xlim(-0.08, -0.05) # 设置 Theta 1 的限制
#plt.ylim(0.02, 0.03) # 设置 Theta 2 的限制
plt.图例()
plt.网格（真）
plt.show()

但是，我发现的问题是，有时保存 theta 值的列表的值是 Nan，而在其他情况下是正常值。当 epoch 数量增加到 10 以上时，我注意到了这一点，这是为什么？]]></description>
      <guid>https://stackoverflow.com/questions/78137739/plotting-of-gradient-descent-curves-by-using-keras</guid>
      <pubDate>Sun, 10 Mar 2024 22:46:54 GMT</pubDate>
    </item>
    <item>
      <title>每次运行时都会出现不同的 ValueError</title>
      <link>https://stackoverflow.com/questions/78137114/different-valueerror-each-time-i-run</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78137114/different-valueerror-each-time-i-run</guid>
      <pubDate>Sun, 10 Mar 2024 18:55:17 GMT</pubDate>
    </item>
    <item>
      <title>多值和多目标分类[关闭]</title>
      <link>https://stackoverflow.com/questions/78136790/multi-value-and-multi-target-classification</link>
      <description><![CDATA[首先，我想检查一下我对类、多类、多值和多目标的理解：
类别
如果我有3个属性（温度、天气和湿度），并且我正在预测天气条件是否有利于玩耍，并且Play是二元的（是、否），这是一个简单的分类，例如：

&lt;标题&gt;

温度
天气
湿度
播放


&lt;正文&gt;

...
...
...
是/否



多类别
类似的情况，但 Play 不是一个枚举列表，包含（互斥的）游戏方式。这将是一个“多类”分类：

&lt;标题&gt;

温度
天气
湿度
播放


&lt;正文&gt;

...
...
...
秋千踢球标签



多值
如果存在多个非互斥的结果，例如每种游戏方式都有其自己的属性，并且每个属性都是二元的（是/否），则这是“多值”：

&lt;标题&gt;

温度
天气
湿度
秋千
踢球
标签


&lt;正文&gt;

...
...
...
是/否
是/否
是/否



多目标
最后一个示例，如果每个可能的结果可能包含非二进制、互斥的值，则这将是多目标：

&lt;标题&gt;

温度
天气
湿度
雪
骑自行车
正在运行


&lt;正文&gt;

...
...
...
滑雪滑雪雪地摩托
山路碎石
跑步机步道跑道



这是正确的吗？
我正在使用 Weka 来预测多目标结果。我希望使用“多目标”分类器的版本(meka.classifiers.multitarget.CC)，但这些结果始终只是“1” （使用“double[][] Predictions = result.allPredictions();”）
|==== 预测 (N=3.0) =====&gt;
| 1 [ -1 -1 -1 -1 ] [ 1.000 1.000 1.000 ]
| 2 [ -1 -1 -1 -1 ] [ 1.000 1.000 1.000 ]
| 3 [ -1 -1 -1 -1 ] [ 1.000 1.000 1.000 ]
|================================&lt;
如果我使用“多标签”分类器（meka.classifiers.multilabel.CC）然后它给我我认为正确的结果，这些值是值列表中的正确索引：
|==== 预测 (N=3.0) =====&gt;
| 1 [ -1 -1 -1 -1 ] [ 2.000 1.000 0.000 ]
| 2 [ -1 -1 -1 -1 ] [ 0.000 1.000 2.000 ]
| 3 [ -1 -1 -1 -1 ] [ 1.000 0.000 2.000 ]
|================================&lt;
我当然可以使用多标签分类器，但它看起来并不是正确的使用方法。
我不明白什么？
我期待多目标能够给我多标签所提供的东西。]]></description>
      <guid>https://stackoverflow.com/questions/78136790/multi-value-and-multi-target-classification</guid>
      <pubDate>Sun, 10 Mar 2024 17:23:21 GMT</pubDate>
    </item>
    <item>
      <title>当第 33 次迭代从头开始训练线性回归时，MSE 始终变为 0</title>
      <link>https://stackoverflow.com/questions/78136597/mse-always-becomes-0-when-training-linear-regression-from-scratch-at-33th-iterat</link>
      <description><![CDATA[我正在尝试在 Kaggle 上的 Spotify 2023 数据集上从头开始训练多线性回归模型。
def min_max_scalar(df, col):
df[col] = (df[col] - min(df[col])) / (max(df[col]) - min(df[col]))
返回 df[列]

defmean_squared_error（实际，预测）：
返回 np.mean((实际 - 预测) ** 2)

Spotify_2023_data[&#39;streams&#39;] = min_max_scalar(spotify_2023_data, &#39;streams&#39;)

X = spotify_2023_data[[&#39;in_spotify_playlists&#39;, &#39;in_apple_playlists&#39;]]
Y = spotify_2023_data[&#39;流&#39;]
X[&#39;ones&#39;] = np.ones(len(X),) #进行拦截

阈值 = 1e-6
步长大小 = 5e-9
theta, theta_prev = np.array(np.repeat(5,3)), np.ones(3,) #随机权重

迭代= 0
训练错误 = []
测试错误 = []
训练大小 = np.arange(1,len(Y)+1)
训练迭代= []
#TODO：33 次，算法失控
for i in Training_size: #意味着通过改变训练数据的数量来显示偏差-方差权衡
    如果 i % 2 == 0: 继续
    X_train = X.iloc[:i]
    y_train = Y.iloc[:i]
    X_test = X.iloc[i:]
    y_test = Y.iloc[i:]

    而 np.linalg.norm(theta - theta_prev) &gt;阈值：课本中使用的#threshold机制来确定何时停止。不幸的是，我必须使用这个。
        θ_prev = θ
        梯度 = mse_gradient(theta, X_train, y_train)
        theta = theta_prev - step_size * 梯度
        if (np.isnan(theta)): 打印(theta)
        迭代 += 1

    print(&quot;i={}, results={}, mse={}&quot;.format(i, f(X_test, theta),mean_squared_error(y_test, f(X_test, theta))))
    print(&quot;theta={}&quot;.format(theta))
    打印（“iter =”，iter）
    迭代= 0
    Training_error.append(mean_squared_error(y_train, f(X_train, theta)))
    test_error.append(mean_squared_error(y_test, f(X_test, theta)))
    训练迭代.append(i)
        
    theta, theta_prev = np.array(np.repeat(5,3)), np.ones(3,)

无论我使用什么 i 值，它似乎总是在第 33 次迭代时产生 NaN 值。我检查过，这可能是因为 theta 此时跳到无穷大。我检查了使用的数据，似乎没有任何丢失的数据。我应该做什么才能让培训继续进行？]]></description>
      <guid>https://stackoverflow.com/questions/78136597/mse-always-becomes-0-when-training-linear-regression-from-scratch-at-33th-iterat</guid>
      <pubDate>Sun, 10 Mar 2024 16:19:49 GMT</pubDate>
    </item>
    <item>
      <title>是否可以对随机森林回归的某些特征进行加权？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78136531/is-it-possible-to-weight-some-features-for-a-random-forest-regression</link>
      <description><![CDATA[我一直在尝试训练一个随机森林模型，该模型根据到其他点的距离以及这些点的气候和地理数据等特征来预测点的特征。问题出现在结果上，它们并不糟糕，但还有改进的余地。该模型预测最重要的特征是点之间的距离，但我们认为它应该更加重视气候数据特征，而不太重视距离。
有没有办法让模型降低距离的重要性？也许给它们加权？或者问题可能出在我们为模型提供的数据上？
我尝试过多种模型，如 Lasso、Gradient Boosting 和 Random Forest。还尝试了不同类型的交叉验证。最好的结果来自具有交叉验证的随机森林，以避免某些空间自相关。尽管如此，该模型预计会比现在更好。我们怀疑它非常重视点之间的距离，但我们不知道如何改变这一点并获得更好的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78136531/is-it-possible-to-weight-some-features-for-a-random-forest-regression</guid>
      <pubDate>Sun, 10 Mar 2024 15:58:48 GMT</pubDate>
    </item>
    <item>
      <title>Pytroch 分割模型(.pt) 未转换为 CoreML</title>
      <link>https://stackoverflow.com/questions/78091161/pytroch-segmentation-model-pt-not-converting-to-coreml</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78091161/pytroch-segmentation-model-pt-not-converting-to-coreml</guid>
      <pubDate>Sat, 02 Mar 2024 01:26:48 GMT</pubDate>
    </item>
    <item>
      <title>如何重现带有modified_huber损失的SGDClassifier？</title>
      <link>https://stackoverflow.com/questions/78057268/how-do-i-reproduce-a-sgdclassifier-with-modified-huber-loss</link>
      <description><![CDATA[我有一个像这样定义的模型：
rng = 42
模型=管道（[
    (&#39;缩放器&#39;, RobustScaler()),
    (&#39;特征&#39;, SelectKBest(k=42)),
    (&#39;model&#39;, SGDClassifier(loss=&#39;modified_huber&#39;, shuffle=True, random_state=rng))
]）

当我使用完全相同的输入在两个单独的程序执行（一个是临时的，另一个是 cron 作业）中进行训练和预测时，我会得到不同的模型权重，从而得到预测结果。
我注意到“铰链”损失是唯一具有完全相同权重的可重现模型。其他损失函数是什么阻止了它们被重现？
我已经检查并仔细检查输入是否相同，并使用其他损失函数进行验证。]]></description>
      <guid>https://stackoverflow.com/questions/78057268/how-do-i-reproduce-a-sgdclassifier-with-modified-huber-loss</guid>
      <pubDate>Sun, 25 Feb 2024 18:57:43 GMT</pubDate>
    </item>
    <item>
      <title>如何使用数据加载器解决这个问题？</title>
      <link>https://stackoverflow.com/questions/77968976/how-can-i-resolve-this-problem-with-dataloaders</link>
      <description><![CDATA[我正在构建一些数据加载器来训练和测试机器学习模型。
我有一个名为“array”的元组列表像这样：
(Data(x=[468, 2], edge_index=[2, 1322], y=0, edge_weight=[1322]), &#39;morphed_img027485_img054553.png&#39;)
（数据（x=[468, 2]，edge_index=[2, 1322]，y=0，edge_weight=[1322]），&#39;morphed_img031737_img054553.png&#39;）

我像这样创建数据加载器：
data_loader = create_dataloader(数组，batch_size=60)
save_dataloader(data_loader, &#39;DataLoader 名称&#39;)

输出不是我所期望的，但它将所有数据合并到一个 DataBatch 中，如下所示：
[DataBatch(x=[936, 2]，edge_index=[2, 2644]，y=[2]，edge_weight=[2644]，batch=[936]，ptr=[3])， (&#39;morphed_img031737_img054553.png&#39;, &#39;morphed_img027485_img054553.png&#39;)]

为什么？如何拥有一个数据加载器，将所有数据像在数组中一样分开？]]></description>
      <guid>https://stackoverflow.com/questions/77968976/how-can-i-resolve-this-problem-with-dataloaders</guid>
      <pubDate>Fri, 09 Feb 2024 14:52:39 GMT</pubDate>
    </item>
    <item>
      <title>mat1 和 mat2 必须具有相同的 dtype</title>
      <link>https://stackoverflow.com/questions/75102134/mat1-and-mat2-must-have-the-same-dtype</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/75102134/mat1-and-mat2-must-have-the-same-dtype</guid>
      <pubDate>Thu, 12 Jan 2023 20:45:51 GMT</pubDate>
    </item>
    <item>
      <title>sklearn.decomposition.PCA 特征向量的简单图</title>
      <link>https://stackoverflow.com/questions/37976564/simple-plots-of-eigenvectors-for-sklearn-decomposition-pca</link>
      <description><![CDATA[我正在尝试了解主成分分析的工作原理，并在sklearn.datasets.load_iris数据集上对其进行测试。我了解每个步骤的工作原理（例如标准化数据、协方差、特征分解、排序最高特征值、使用 K 选定维度将原始数据转换为新轴）。
下一步是可视化这些特征向量在数据集上的投影位置（在PC1 vs. PC2 图上，对吗？）。
有人可以解释如何在降维数据集的 3D 图上绘制 [PC1、PC2、PC3] 特征向量吗？
另外，我是否正确绘制了这个 2D 版本？我不确定为什么我的第一个特征向量的长度较短。我应该乘以特征值吗？
&lt;小时/&gt;
以下是我为实现这一目标所做的一些研究：
我遵循的 PCA 方法来自：
https://plot.ly /ipython-notebooks/principal-component-analysis/#Shortcut---PCA-in-scikit-learn （虽然我不想使用 plotly。我想坚持使用pandas、numpy、sklearn、matplotlib、scipy 和 seaborn）
我一直在遵循本教程来绘制特征向量，它看起来非常简单：基本示例对于带有 matplotlib 的 PCA 但我似乎无法用我的数据复制结果。
我发现了这个，但对于我想要做的事情来说，它似乎过于复杂，而且我不想创建一个 FancyArrowPatch： 使用 matplotlib 和 np.linalg 绘制协方差矩阵的特征向量
&lt;小时/&gt;
我尝试使我的代码尽可能简单，以便遵循其他教程：
将 numpy 导入为 np
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 plt
从 sklearn.datasets 导入 load_iris
从 sklearn.preprocessing 导入 StandardScaler
来自 sklearn 导入分解
将seaborn导入为sns； sns.set_style(“whitegrid”, {&#39;axes.grid&#39; : False})

%matplotlib 内联
np.随机.种子(0)

# 鸢尾花数据集
DF_data = pd.DataFrame(load_iris().data,
                       索引 = [“iris_%d”; % i for i in range(load_iris().data.shape[0])],
                       列= load_iris().feature_names)

Se_targets = pd.Series(load_iris().target,
                       索引 = [“iris_%d”; % i for i in range(load_iris().data.shape[0])],
                       名称=“物种”）

# 缩放均值 = 0，var = 1
DF_standard = pd.DataFrame(StandardScaler().fit_transform(DF_data),
                           索引 = DF_data.index,
                           列= DF_data.列）

# Sklearn 用于主成分分析

# 变暗
m = DF_standard.shape[1]
K = 2

# PCA（我倾向于如何设置它）
M_PCA = 分解.PCA(n_components=m)
DF_PCA = pd.DataFrame(M_PCA.fit_transform(DF_standard),
                列=[“PC%d” % k for k in range(1,m + 1)]).iloc[:,:K]


# 绘制特征向量
#https://stackoverflow.com/questions/18299523/basic-example-for-pca-with-matplotlib

# 这就是事情变得奇怪的地方......
数据 = DF_标准

mu = data.mean(轴=0)
特征向量，特征值 = M_PCA.components_，M_PCA.explained_variance_ #特征向量，特征值，V = np.linalg.svd(data.T, full_matrices=False)
projected_data = DF_PCA #np.dot(数据，特征向量)

西格玛=projected_data.std(axis=0).mean()

图, ax = plt.subplots(figsize=(10,10))
ax.scatter(projected_data[“PC1”],projected_data[“PC2”])
对于轴，zip 中的颜色（特征向量[:K], [“红色”,“绿色”]）：
# start, end = mu, mu + sigma * axis ### 导致“ValueError：太多值无法解压（预期为 2）”

    # 所以我尝试了这个，但我认为它不正确
    开始，结束 = (mu)[:K], (mu + sigma * 轴)[:K]
    ax.annotate(&#39;&#39;, xy=结束,xytext=开始, arrowprops=dict(facecolor=颜色, width=1.0))
    
ax.set_aspect(&#39;等于&#39;)
plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/37976564/simple-plots-of-eigenvectors-for-sklearn-decomposition-pca</guid>
      <pubDate>Wed, 22 Jun 2016 19:20:15 GMT</pubDate>
    </item>
    </channel>
</rss>