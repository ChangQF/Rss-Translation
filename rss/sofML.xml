<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 24 Apr 2024 01:00:01 GMT</lastBuildDate>
    <item>
      <title>在电价预测神经网络的训练、验证和测试集中分割 3 年每小时数据的好方法是什么？</title>
      <link>https://stackoverflow.com/questions/78375436/what-is-a-good-approach-to-split-3-years-of-hourly-data-in-a-train-validation-a</link>
      <description><![CDATA[我想训练一个简单的神经网络来预测某个地区的电价。然而，我只有“有限”的可用数据（该地区连续 3 年的历史价格、发电量和负荷，间隔 1 小时 - 不允许导入其他数据）。文献建议以 60-20-20、70-15-15 或 80-10-10 的方式拆分以获得训练集、验证集和测试集。分裂“2年、0.5年、0.5年”训练集、验证集和测试集分别将导致 1 月至 6 月期间的超参数优化，并在 7 月至 12 月期间测试模型。直觉上，这对我来说不是一个好方法，因为这会导致验证和测试集中的季节性不匹配。这种直觉是否正确？如果正确，可以采取什么替代方法？
我搜索了很长一段时间，找到了一种名为“滚动交叉验证”的方法，但我不确定这是正确的方法，因为我对神经网络主题还很陌生。]]></description>
      <guid>https://stackoverflow.com/questions/78375436/what-is-a-good-approach-to-split-3-years-of-hourly-data-in-a-train-validation-a</guid>
      <pubDate>Wed, 24 Apr 2024 00:00:27 GMT</pubDate>
    </item>
    <item>
      <title>使用 Vector API 优化 Java 中 int16 向量点积的计算</title>
      <link>https://stackoverflow.com/questions/78375306/optimizing-the-calculation-of-the-dot-product-of-int16-vectors-in-java-using-vec</link>
      <description><![CDATA[TL;DR：使用 Java 的 Vector API 优化 16 位整数数组乘法而不溢出。
我正在尝试优化一个性能关键的循环，该循环应用激活函数并使用 Java 的（正在孵化的）Vector API 计算两个 int16 数组的点积。这是我当前的标量实现：
for (int i = 0; i &lt; HIDDEN_SIZE; i++)
{
    结果 += screlu(us.values[i]) * network.L1Weights[i]
        + screlu(them.values[i]) * network.L1Weights[i + HIDDEN_SIZE];
}

哪里
private static int screlu(短i)
{
    int v = Math.max(0, Math.min(i, QA));
    返回 v * v；
}

我尝试像这样矢量化它：
int[] usValues = new int[HIDDEN_SIZE];
int[] 他们值 = new int[HIDDEN_SIZE];

for (int i = 0; i &lt; HIDDEN_SIZE; i++)
{
    usValues[i] = (int) us.values[i];
    themValues[i] = (int) them.values[i];
}

IntVector sum = IntVector.zero(INT_SPECIES);

for (; i &lt; upperBound; i += INT_SPECIES.length())
{
    IntVector va = IntVector.fromArray(INT_SPECIES, usValues, i);
    IntVector vb = IntVector.fromArray(INT_SPECIES, themValues, i);
    IntVector vc = IntVector.fromArray(INT_SPECIES, network.L1Weights, i);
    IntVector vd = IntVector.fromArray(INT_SPECIES, network.L1Weights, i + HIDDEN_SIZE);

    va = va.max(0).min(QA);
    va = va.mul(va).mul(vc);

    vb = vb.max(0).min(QA);
    vb = vb.mul(vb).mul(vd);

    sum = sum.add(va).add(vb);
}

int 结果 = sum.reduceLanes(VectorOperators.ADD);

由于溢出，我不得不使用 32 位宽的通道，将吞吐量减半。结果，性能仅稍好一些。经过一番研究，我发现 _mm256_madd_epi16 等内在函数完全解决了我的问题，但我在文档中找不到任何有关它的信息。 Vector API 中是否存在等效操作，如果不存在，是否有其他解决方案来解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/78375306/optimizing-the-calculation-of-the-dot-product-of-int16-vectors-in-java-using-vec</guid>
      <pubDate>Tue, 23 Apr 2024 23:01:03 GMT</pubDate>
    </item>
    <item>
      <title>如何计算偏差、方差、噪声？</title>
      <link>https://stackoverflow.com/questions/78375275/how-can-be-calculated-bias-variance-noise</link>
      <description><![CDATA[在方程 Err(x) =bias^2 + var + sigma^2 的上下文中，其中bias 定义为 E[(f(x) - \hat{f}(x))^2] 且f(x)代表一个未知的真函数，在不知道f(x)的情况下如何计算偏差值？具体地，当使用“bias_variance_decomp”时，来自“mlxtend.evaluate”的函数库，它如何估计偏差分量？那么西格玛如何计算？]]></description>
      <guid>https://stackoverflow.com/questions/78375275/how-can-be-calculated-bias-variance-noise</guid>
      <pubDate>Tue, 23 Apr 2024 22:48:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 的神经网络 NarX 模型</title>
      <link>https://stackoverflow.com/questions/78374854/neural-network-narx-model-with-python</link>
      <description><![CDATA[我正在尝试使用 Python 进行 NarX 模型。当我运行以下代码时，出现以下错误。我该如何解决这个错误？这个错误是什么意思？
net.learn(epochs= 30, show_epoch_results=True, random_testing=False)

]]></description>
      <guid>https://stackoverflow.com/questions/78374854/neural-network-narx-model-with-python</guid>
      <pubDate>Tue, 23 Apr 2024 20:31:49 GMT</pubDate>
    </item>
    <item>
      <title>无法过度拟合多项式回归？</title>
      <link>https://stackoverflow.com/questions/78374435/failing-to-overfit-polynomial-regression</link>
      <description><![CDATA[我正在尝试将多项式回归过拟合到正弦曲线。据我所知，当有 N 数据样本和多项式次数 N-1 时，曲线应该穿过所有数据点，但是，在我的例如这不会发生。
我的代码如下：
从 sklearn.linear_model 导入 LinearRegression
从 sklearn.preprocessing 导入多项式特征

数 = 50
度 = 49

X = X = np.linspace(0, 2 * np.pi, N).reshape(-1, 1)
X = np.sort(X, 轴=0)
y = np.sin(X) + np.random.randn(N, 1) * 0.2

poly_features = 多项式特征（度=deg，include_bias=False）

X_poly = poly_features.fit_transform(X)

reg = 线性回归()
reg.fit(X_poly, y)

y_vals = reg.predict(X_poly)

plt.scatter(X, y)
plt.plot(X, y_vals, color=&#39;r&#39;)
plt.show()


你能解释一下我在这里的误解吗？]]></description>
      <guid>https://stackoverflow.com/questions/78374435/failing-to-overfit-polynomial-regression</guid>
      <pubDate>Tue, 23 Apr 2024 18:44:44 GMT</pubDate>
    </item>
    <item>
      <title>提高我正在尝试的这个 GitHub 项目的准确性的方法是什么</title>
      <link>https://stackoverflow.com/questions/78374110/what-is-the-way-to-increase-the-accuracy-of-this-github-project-im-trying</link>
      <description><![CDATA[我正在尝试运行此 GitHub 项目在我的电脑中。这是唇读模型。
该项目的模型不太准确，但正如使用说明中提到的那样，可以完美运行：
&lt;块引用&gt;
注意：predict_live.py 中实现的当前模型是一个不太准确的模型。我无法
由于文件太大，请上传 /training/3DCNN.ipynb 中显示的模型的权重。

我很困惑如何将其转变为更准确的模型，可能需要更多的数据集。目前，它非常不准确，因为它无法正确标记，请阅读下文。
以下是我在尝试运行 Jupyter Notebook“3DCNN.ipynb”时遇到的问题，其中：
dir_path = “./collected_data”
高度、宽度、通道 = 80、112、3

视频=[]
标签=[]
计数器 = 0

对于 os.walk(dir_path) 中的 root、dirs、文件：
    

    对于文件中的文件：
        如果文件==“data.txt”：

            # 从目录名中提取标签
            标签 = root.split(&quot;/&quot;)[-1]
            标签 = label.split(“_”)[0]
            #如果标签不在wanted_words中：
            ＃    继续
            计数器 += 1
            打印（计数器，结束=“”）

            将 open(os.path.join(root, file), &#39;r&#39;) 作为 f：
                data_str = f.read()

            # 将文本文件的内容作为 Python 表达式进行计算
            数据列表 = eval(data_str)
            
            # 将列表转换为 numpy 数组
            data_array = np.array(data_list)
            #print(data_array.shape)

            # 将数据重塑为 4D 形状数组（num_frames、高度、宽度、通道）
            num_frames = len(数据列表)
            帧 = data_array.reshape((num_frames, 高度, 宽度, 通道))
            # 将帧和标签附加到视频和标签数组中
            视频.append(帧)
            labels.append(标签)
打印（标签）

# 将视频和标签数组转换为 NumPy 数组
视频 = np.array(视频)
标签 = np.array(标签)

# 将视频和标签保存为单独的 .npy 文件
np.save(“videosCorrect.npy”, 视频)
np.save(“labelsCorrect.npy”, 标签)

以上部分结果如下：
1 2 3 4 5 6 7 8 9 10 11 ... 685 [&#39;已收集&#39;]....[&#39;已收集&#39;]

我原以为这工作正常，因为它确实对所有 685 个数据集文件进行了编号=&gt; print(counter) 然后开始发送垃圾邮件 [&#39;collected&#39;]，即 print(labels)。
我认为它还正确地分割了训练数据：
编码器 = LabelEncoder()
编码器.fit(标签)
编码标签 = 编码器.transform(标签)
标签 = 编码标签
label_dict = {6: &#39;你好&#39;, 5: &#39;狗&#39;, 10: &#39;我的&#39;, 12: &#39;你&#39;, 9: &#39;嘴唇&#39;, 3: &#39;猫&#39;, 11: &#39;读&#39;, 0: &#39;a&#39; , 4: &#39;演示&#39;, 7: &#39;这里&#39;, 8: &#39;是&#39;, 1: &#39;再见&#39;, 2: &#39;可以&#39;}

# 将数据分为训练集和验证集

X_train, X_test, y_train, y_test = train_test_split(视频, 标签, test_size=0.2, random_state=42)

print(&quot;训练集形状：&quot;, X_train.shape, y_train.shape)
print(&quot;测试集形状：&quot;, X_test.shape, y_test.shape)

删除视频

结果：
训练集形状：(548, 22, 80, 112, 3) (548,)
测试集形状：(137,22,80,112,3)(137,)

现在，当我运行 print(labels) 时，以下代码会导致另一个 0 的垃圾邮件，因此在循环运行时它不会正确分隔 label_dict： 
# 获取唯一的类
类= np.unique(标签)
打印（标签）
# 统计每个类出现的次数
计数 = np.bincount(标签, minlength=len(类))
打印（计数）
# 找到类名的最大长度以进行对齐
max_len = max([len(label_dict[i]) for i in range(len(label_dict))])
打印（最大长度）
打印（长度（label_dict））
# 打印类的分布
对于 i，计数 enumerate(counts)：
    类名 = label_dict[i].ljust(max_len)
    print(&quot;{} {} 计数&quot;.format(class_name, count))

结果：
[0 0 0 0 0 0 0 0 0 0... 0] #删除 0 的数量以使该帖子减少垃圾邮件... print(labels)
[685]#print（计数）
5 #打印（最大长度）
13 #print(len(label_dict))
a 685 counts #Here，它应该循环遍历 label_dict 的所有索引，但由于它没有正确标记，因此无法正确计数...

我不知道如何让 label_dict 正确计数。该数据集由 685 个标记为 a_1、a_2 等的文件夹组成。它对某人来说工作正常吗？]]></description>
      <guid>https://stackoverflow.com/questions/78374110/what-is-the-way-to-increase-the-accuracy-of-this-github-project-im-trying</guid>
      <pubDate>Tue, 23 Apr 2024 17:23:48 GMT</pubDate>
    </item>
    <item>
      <title>如何开发react和python之间的数据传输以进行联邦学习[关闭]</title>
      <link>https://stackoverflow.com/questions/78373881/how-to-develop-data-transfer-between-react-and-python-for-federated-learning</link>
      <description><![CDATA[我们的团队目前正在致力于创建联合学习应用程序。在我们的团队中，一组专注于算法和其他技术方面的开发，而其余成员则致力于构建客户端界面。
考虑到我们项目的性质（围绕联邦学习），我们正在探索通过 API 直接通信的替代方案。相反，我们正在考虑在应用程序本身内实现数据传输功能。
我们的计划涉及通过 API 与服务器和模型安全地共享加密数据以供一般使用。不过，我们仍在考虑如何构建和构建该应用程序以使其正式发布的具体细节。
请与我分享您的想法，谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78373881/how-to-develop-data-transfer-between-react-and-python-for-federated-learning</guid>
      <pubDate>Tue, 23 Apr 2024 16:36:00 GMT</pubDate>
    </item>
    <item>
      <title>在时间序列预测中根据不规则采样的训练数据进行预测，无需使用 Python 进行插值</title>
      <link>https://stackoverflow.com/questions/78373739/forecasting-from-irregularly-sampled-training-data-in-time-series-forecasting-wi</link>
      <description><![CDATA[有没有一种方法可以从不规则采样的数据中获得规则间隔的预测，如下所示：样本数据
我希望从上述测试数据中收集预测，该数据每隔 1 秒间隔有一个数据点。任何形式的帮助将不胜感激:)
我尝试在 LSTM 模型中插入“日期和时间”作为第二个参数，但这导致预测存在间隙并且间隔不规则。我还尝试对数据进行插值，但这导致我的预测非常不准确。]]></description>
      <guid>https://stackoverflow.com/questions/78373739/forecasting-from-irregularly-sampled-training-data-in-time-series-forecasting-wi</guid>
      <pubDate>Tue, 23 Apr 2024 16:07:43 GMT</pubDate>
    </item>
    <item>
      <title>Autodistill GroundedSAM 卡在标记第一张图像上？</title>
      <link>https://stackoverflow.com/questions/78370149/autodistill-groundedsam-stuck-on-labeling-first-image</link>
      <description><![CDATA[我正在尝试标记包含 200,000 张飞机图像的数据集。我正在使用自动蒸馏和 GroundedSAM 来尝试品尝。它已经卡在 0/219670 有一段时间了，速度为 0it/s
这是我的代码：
from autodistill_grounded_sam import GroundedSAM
从 autodistill.detection 导入 CaptionOntology
从 autodistill_yolov8 导入 YOLOv8

image_dir = r&#39;C:\Users\Colter\Desktop\plane_ detector\dataset\all_images&#39;
base_model = GroundedSAM(ontology=CaptionOntology({“飞机”: “飞机”}))
base_model.label(image_dir, 扩展名=“.jpg”)

我的输出在所附图像中： 
我希望进度条能慢慢增加，并希望程序能够标记我的所有图像以及图片中飞机的位置，但它卡在 0 上。]]></description>
      <guid>https://stackoverflow.com/questions/78370149/autodistill-groundedsam-stuck-on-labeling-first-image</guid>
      <pubDate>Tue, 23 Apr 2024 05:58:47 GMT</pubDate>
    </item>
    <item>
      <title>UnicodeEncodeError：“charmap”编解码器无法对位置 19-38 中的字符进行编码：字符映射到 <未定义></title>
      <link>https://stackoverflow.com/questions/78367946/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-19-38-c</link>
      <description><![CDATA[我正在开发一个基于 Flask 的 Web 应用程序，用户可以上传图像以使用机器学习模型进行预测。上传的图像存储在本地目录中，并使用预先训练的模型进行预测。然而，当我点击预测按钮时
是什么导致了这个 UnicodeEncodeError？
如何解决此问题以确保我的应用程序能够正确处理图像上传和预测？
是否有在 Flask 环境中处理字符编码的最佳实践，尤其是在 Windows 上？
==app.py====
@app.route(&#39;/uploadimage&#39;,methods=[&#39;GET&#39;, &#39;POST&#39;])
def upload_image():

        文件 = request.files[&#39;my_image&#39;]
        # 获取预测结果
        预测标签 = 预测标签(img_path)
        # 返回预测的标签和一条提示信息
        flash(f&quot;预测：{predicted_label}&quot;, &quot;成功&quot;)
        os.remove(img_path) # 处理后删除临时文件
    return render_template(&#39;uploadimage.html&#39;) # 对于 GET 请求，渲染表单


即使我设置了环境变量“UTF-8”，我仍然收到此错误
错误
文件“C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\encodings\cp1252.py”，第 19 行，编码
返回 codecs.charmap_encode(输入,self.errors,encoding_table)[0]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^
UnicodeEncodeError：“charmap”编解码器无法对位置 19-38 中的字符进行编码：字符映射为未定义。
============
即使我有一个最简单的代码来测试编码
标题是“要测试您的控制台是否可以处理 UTF-8，请尝试输出带有特殊字符或 Unicode 字符的文本：”
print(&quot;UTF-8 测试: àéîöü — 中文 — 阿拉伯语&quot;)


错误也相同
print(&quot;UTF-8 测试：����� � \u4e2d\u6587 � \u0627\u0644\u0639\u0631\u0628\u064a\u0629&quot;)
文件“C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\encodings\cp1252.py”，第 19 行，编码
返回 codecs.charmap_encode(输入,self.errors,encoding_table)[0]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^
UnicodeEncodeError：“charmap”编解码器无法对位置 20-21 中的字符进行编码：字符映射到 ]]></description>
      <guid>https://stackoverflow.com/questions/78367946/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-19-38-c</guid>
      <pubDate>Mon, 22 Apr 2024 17:25:20 GMT</pubDate>
    </item>
    <item>
      <title>多类问题的层次分类方法</title>
      <link>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</link>
      <description><![CDATA[有一个多类分类任务。我的目标是使用每父节点本地分类器 (LCPN) 方法来解决这个问题。
让我解释一下如何使用 MWE。
假设我有这个虚拟数据集：
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification
从 scipy.cluster 导入层次结构

X, y = make_classification(n_samples=1000, n_features=10, n_classes=5,
                             n_信息=4）

我想出了这些类之间的距离矩阵：
d = np.array(
[[ 0.、201.537、197.294、200.823、194.517]、
 [201.537, 0., 199.449, 202.941, 196.703],
 [197.294, 199.449, 0., 198.728, 192.354],
 [200.823, 202.941, 198.728, 0., 195.972],
[[194.517, 196.703, 192.354, 195.972, 0.]]
）

因此，我确定了类层次结构，如下所示：
hc = hierarchy.linkage(d, method=&#39;complete&#39;)

得到的树状图如下：
dendrogram = hierarchy.dendrogram(hc, labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;, &#39;D&#39;, &#39;F&#39;])
树状图


我使用hierarchy.to_tree()以树状结构进行说明：

我的问题：
如何按照 LCPN 方法在每个内部节点（包括根）处安装分类器，例如 DecisionTreeClassifier 或 SVM，以像在树中一样进行上图？]]></description>
      <guid>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</guid>
      <pubDate>Sat, 20 Apr 2024 14:08:05 GMT</pubDate>
    </item>
    <item>
      <title>从 torchensemble 中的基本模型获取嵌入</title>
      <link>https://stackoverflow.com/questions/78355585/getting-embeddings-from-the-base-model-in-torchensemble</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78355585/getting-embeddings-from-the-base-model-in-torchensemble</guid>
      <pubDate>Fri, 19 Apr 2024 18:25:20 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的有效张量乘法</title>
      <link>https://stackoverflow.com/questions/78330216/effective-tensor-multiplication-in-pytorch</link>
      <description><![CDATA[有谁知道我如何有效地计算两个张量乘法 - 例如，我有两个形状为 (15, 256) 和 (112, 256) 的张量，它们的乘积为形状为 (15, 112) 的张量可以是以7微秒计算。但是，如果我有像 A - (15, 100, 256) 和 B - (112, 2000, 256) 这样的张量，并且我会做出像 C = (A.reshape(-1, 256) @ B.reshape(256, - 1).reshape(15, 100, 112, 2000).permute(0, 2, 1, 3).max(-1).values.sum(-1) 得到形状为(15, 112)的张量，需要 1000 倍的时间才能计算得更快吗？
我知道第二个计算应该比第一个计算大得多，但也许它需要的时间比我的实现要少]]></description>
      <guid>https://stackoverflow.com/questions/78330216/effective-tensor-multiplication-in-pytorch</guid>
      <pubDate>Mon, 15 Apr 2024 17:49:57 GMT</pubDate>
    </item>
    <item>
      <title>在时间序列 ARIMA 分析中出现错误“TypeError：没有要绘制的数值数据”</title>
      <link>https://stackoverflow.com/questions/78218276/getting-error-typeerror-no-numeric-data-to-plot-in-a-time-series-arima-analys</link>
      <description><![CDATA[我正在尝试遵循一个教程，其中使用差异数据进行 ARIMA 时间序列分析：
以下是python代码：
def 差异（数据集）：
    差异=列表（）
    对于范围内的 i(1, len(数据集))：
        值 = 数据集[i] - 数据集[i - 1]
        diff.append(值)
    返回系列（差异）

系列 = pd.read_csv(&#39;dataset.csv&#39;)
X = series.values # 构建列表的错误可以在这里看到
X = X.astype(&#39;float32&#39;)
平稳 = 差值(X)
固定.索引 = 系列.索引[1:]
...
固定.plot()
pyplot.show()

当过程到达绘图阶段时，我收到错误：
&lt;块引用&gt;
类型错误：没有要绘制的数字数据

回溯起来，我发现正在解析的数据产生了一个数组的集合。将集合stationary保存为*.csv文件会给我一个如下列表：
&lt;前&gt;&lt;代码&gt;[11.]
[0.]
[16.]
[45.]
[27.]
[-141。]
[46]

有人可以告诉我这里出了什么问题吗？
PS。我已经排除了库导入的部分
编辑 1
数据集的一部分复制如下：
年份，观测值
1994,21
1995,62
1996,56
1997,29
1998,38
1999,201
]]></description>
      <guid>https://stackoverflow.com/questions/78218276/getting-error-typeerror-no-numeric-data-to-plot-in-a-time-series-arima-analys</guid>
      <pubDate>Mon, 25 Mar 2024 10:07:18 GMT</pubDate>
    </item>
    <item>
      <title>微调 GPT2 以实现生成式问答</title>
      <link>https://stackoverflow.com/questions/72199570/fine-tuning-gpt2-for-generative-question-anwering</link>
      <description><![CDATA[我正在尝试微调 gpt2 以实现生成式问答任务。
基本上我的数据格式类似于：
背景：马特今天撞坏了他的车。
问：马特这一天过得怎么样？
答案：不好
我正在查看 Huggingface 文档，以了解如何在自定义数据集上微调 GPT2，并且我确实在此地址找到了有关微调的说明：
https://github.com/huggingface/transformers/tree/main /examples/pytorch/语言建模
问题在于，他们没有提供任何关于如何准备数据以便模型可以从中学习的指导。他们提供了可用的不同数据集，但没有一个格式适合我的任务。
如果有更多经验的人可以帮助我，我将非常感激。
祝你有美好的一天！]]></description>
      <guid>https://stackoverflow.com/questions/72199570/fine-tuning-gpt2-for-generative-question-anwering</guid>
      <pubDate>Wed, 11 May 2022 10:38:19 GMT</pubDate>
    </item>
    </channel>
</rss>