<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 30 Aug 2024 12:30:14 GMT</lastBuildDate>
    <item>
      <title>标准 MLP 是二元分类任务的有效基准吗？</title>
      <link>https://stackoverflow.com/questions/78931677/is-a-standard-mlp-a-valid-benchmark-for-binary-classification-tasks</link>
      <description><![CDATA[我正在处理一些带标签的 3D 张量，并尝试执行二元分类。数据基本上只是氨基酸序列的参数，我没有使用残基的标记，而是尝试通过提供每种氨基酸的一些基本指标（电荷、疏水性等）为模型提供更多关于氨基酸特征的信息。
我展平了数据，只是为了尝试使用标准的 sklearn MLP，我可以制作自己的更高级的神经网络来适应这项任务，也就是说，我正在考虑使用双向 LSTM 来提取“语义” （找不到更好的词）这些氨基酸参数之间的关系。
但在开始做所有这些事情之前，我不确定 MLP（表现糟糕（准确率约 50%））是否是输入张量可分离性的良好基准——这意味着如果 MLP 无法管理，其他任何东西也无法管理。
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78931677/is-a-standard-mlp-a-valid-benchmark-for-binary-classification-tasks</guid>
      <pubDate>Fri, 30 Aug 2024 11:07:00 GMT</pubDate>
    </item>
    <item>
      <title>如何禁用早期停止“cito”R 包“取消训练，因为损失仍然高于基线”</title>
      <link>https://stackoverflow.com/questions/78930951/how-to-disable-early-stopping-cito-r-package-cancel-training-because-loss-is</link>
      <description><![CDATA[如何强制禁用似乎默认启用的提前停止功能。
set.seed(1)

X &lt;- matrix(rnorm(10000), ncol = 10)
Y &lt;- sample(0:1, nrow(X), replace = TRUE)

library(cito)
nn &lt;- dnn(as.factor(Y)~., X, 
epochs = 100, 
loss = &quot;softmax&quot;, 
verbose = TRUE, 
lr = 0.1,
activation = &quot;sigmoid&quot;,
plot = TRUE,
early_stopping = 1000)

这是模型训练时的输出，您可以看到提前停止是如何工作的
...
..
.
第 24 个时期的损失：0.696585，lr：0.10000
第 25 个时期的损失：0.693892，lr：0.10000
第 26 个时期的损失：0.694949，lr：0.10000
第 27 个时期的损失：0.695645，lr：0.10000
第 28 个时期的损失：0.697683，lr：0.10000
第 29 个时期的损失：0.696885，lr：0.10000
取消训练，因为损失仍然高于基线，请设置超参数。请参阅 vignette(&#39;B-Training_neural_networks&#39;) 获取帮助。
]]></description>
      <guid>https://stackoverflow.com/questions/78930951/how-to-disable-early-stopping-cito-r-package-cancel-training-because-loss-is</guid>
      <pubDate>Fri, 30 Aug 2024 07:48:40 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 自定义数据集 CPU OOM 问题</title>
      <link>https://stackoverflow.com/questions/78929887/pytorch-custom-dataset-cpu-oom-issue</link>
      <description><![CDATA[我的数据加载器中存在一个非常持久的内存问题，根据 num_workers 的不同，在任意数量的 epoch（5-6）后内存就会填满。
我有 85% 的把握认为问题出在数据集上，因为每次调用 getitem() 都会增加内存。
我的数据只是我在 getitem() 中加载和处理的目录列表。我没有收到任何 cuda 错误
def transformations(self,input):
i,j,h,w = self.crop.get_params(input[&#39;target_material&#39;][0], scale=(0.7, 1.0), ratio=(1.0, 1.0))
if self.use_modality1:
input[&quot;m1&quot;] = torch.cat([self.color_jitter(TF.resized_crop(sample, i, j, h, w, size=(256, 256))) for sample in input[&#39;m1&#39;]], dim=0)
if self.use_semantic:
input[&quot;m2&quot;] = torch.cat([TF.resized_crop(sample, i, j, h,w,size=(self.img_size, self.img_size), interpolation=TF.InterpolationMode.NEAREST) for sample in input[&#39;m2&#39;]], dim=0)
return input

def __getitem__(self, idx):
While True: 
source = self.data[idx]
for _ in 10: 
target = select_target(self.data) 
if (diff(source,target) &lt; .10):
continue
else:
sample = {}
if self.use_m1: 
sample[&#39;m1&#39;] = torch.stack([
to_tensor(normalize_images(np.transpose(cv2.resize(read_npz(m1_input), dsize=(256, 256), interpolation=cv2.INTER_AREA)[..., :3], (2, 0, 1)), max=255)) for m1_input 在 m1_dirs 中
], dim=0)
if self.m2: 
sample[&#39;m2&#39;] = torch.stack([
to_tensor(normalize_images(np.expand_dims(cv2.resize(sem_image, dsize=(256, 256), interpolation=cv2.INTER_NEAREST), axis=0), max=40)) for m2_input 在 m2_dirs 中
], dim=0)

if self.config[&quot;transform&quot;] and random.random()&lt;0.5 and self.train:
sample = self.transformations(sample)
else: 
if self.m1:
sample[&#39;m1&#39;] = torch.cat([m1_tensor for m1_tensor in sample[&#39;m1&#39;]], dim=0)
if self.m2: 
sample[&#39;m2&#39;] = torch.cat([m2_tensor for m2_tensor in sample[&#39;m2&#39;]], dim=0)

return sample


跟踪内存后，我发现即使读取所有数据（未应用转换）也会导致内存增加，并且这些内存不会释放，而且会不断累积。最终，我收到 OOM 错误，代码失败。]]></description>
      <guid>https://stackoverflow.com/questions/78929887/pytorch-custom-dataset-cpu-oom-issue</guid>
      <pubDate>Thu, 29 Aug 2024 22:59:52 GMT</pubDate>
    </item>
    <item>
      <title>多目标文本回归的最佳架构是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/78929656/what-is-the-best-architecture-for-multi-target-text-regression</link>
      <description><![CDATA[我正在使用 Google 的“Civil-Comments”数据集构建 AI 模型。它有 7 个不同的标签，每个标签都是浮点数，可以是 0 到 1 之间的任意值。
我读过的 Embedding Bags 表现不佳。我一直在研究 transformers 和循环系统（LSTM、GRU、RNN 等）。我该怎么办？
如果重要的话，使用 PyTorch。但我不是在寻找代码。]]></description>
      <guid>https://stackoverflow.com/questions/78929656/what-is-the-best-architecture-for-multi-target-text-regression</guid>
      <pubDate>Thu, 29 Aug 2024 21:09:48 GMT</pubDate>
    </item>
    <item>
      <title>建立基于多个时间序列的模型来预测新的独立时间序列</title>
      <link>https://stackoverflow.com/questions/78929174/building-a-model-based-on-multiple-time-series-to-make-a-prediction-on-a-new-ind</link>
      <description><![CDATA[我为物理实验模拟了时间序列数据。每个数据都有 n 个特征、一个目标和一个从 t = 0 到 10000 的时间列。我的目标是建立一个在上述多个时间序列上进行训练的模型，然后预测一个新的测试时间序列集（t = 0 到 10000）。从我的角度来看，这可能是一个时间序列问题，因为目标依赖于时间，但是，我的目标也不是时间序列，因为大多数时间序列模型都需要有目标的历史数据，而我的目标似乎不是对一个全新的时间序列进行预测。
如何正确解决这个问题？
我尝试使用 xgboost 和 randomforest 与 GroupKFold 进行交叉验证，并确保没有数据泄漏。但是，我的预测值无法与目标实际值的高频相匹配（n 个特征的波动较小）。]]></description>
      <guid>https://stackoverflow.com/questions/78929174/building-a-model-based-on-multiple-time-series-to-make-a-prediction-on-a-new-ind</guid>
      <pubDate>Thu, 29 Aug 2024 18:17:46 GMT</pubDate>
    </item>
    <item>
      <title>Google COLAB：单击编辑器中的任意位置即可重复和交换单词或字母</title>
      <link>https://stackoverflow.com/questions/78927737/google-colab-repeating-and-exchanging-words-or-letters-on-clicking-on-anywhere</link>
      <description><![CDATA[[在此处输入图片描述](https://i.sstatic.net/1vRZC63L.png)
如何解决此问题：Google COLAB：我编写了任何代码，然后在 colab 编辑器中单击该代码，它表现得很奇怪，并且一次又一次地重复单词。它会从点击处随机复制任何内容，然后粘贴到我单击的下一个位置。]]></description>
      <guid>https://stackoverflow.com/questions/78927737/google-colab-repeating-and-exchanging-words-or-letters-on-clicking-on-anywhere</guid>
      <pubDate>Thu, 29 Aug 2024 12:15:56 GMT</pubDate>
    </item>
    <item>
      <title>使用熊猫进行机器学习的基线预测[关闭]</title>
      <link>https://stackoverflow.com/questions/78927682/baseline-prediction-using-pandas-for-machine-learning</link>
      <description><![CDATA[我在阅读 Kaggle 教程时看到了这个，但无法理解。
如果您需要更多详细信息，请点击此链接：https://www.kaggle.com/code/kashnitsky/topic-1-exploratory-data-analysis-with-pandas/notebook
[在此处输入图片说明](https://i.ss在此处输入图片说明tatic.net/TLeRqcJj.png)
我真的不明白这是怎么发生的。]]></description>
      <guid>https://stackoverflow.com/questions/78927682/baseline-prediction-using-pandas-for-machine-learning</guid>
      <pubDate>Thu, 29 Aug 2024 12:02:35 GMT</pubDate>
    </item>
    <item>
      <title>如何用 pytorch 处理文本和数字特征？</title>
      <link>https://stackoverflow.com/questions/78926855/how-to-handle-text-and-number-features-with-pytorch</link>
      <description><![CDATA[我的数据集包含一些数字列和一个文本列。文本是一个句子。
我想使用 pytorch 将此数据集用于分类。但不知道如何处理文本列。
以前我使用 Catboost 和 text_features 选项，它运行良好。我还尝试将文本转换为嵌入，并像 Catboost 中的任何其他数字特征一样使用它，它也能正常工作。这是我所做的：
model = AutoModel.from_pretrained(&quot;DeepPavlov/rubert-base-cased&quot;, num_labels = 3, output_attentions = False, output_hidden_​​states = False)

def getEmbedding(text):

coded_input = tokenizer(text, return_tensors=&#39;pt&#39;, truncation=True,
max_length=max_len, add_special_tokens=True)

with torch.no_grad():

output = model(**encoded_input)
output = output[1].detach().cpu().numpy()[0]
torch.cuda.empty_cache()

return output

data[&#39;embedding&#39;] = data[&#39;text&#39;].apply(lambda x: getEmbedding(x))
data= pd.concat([data, data[&#39;embedding&#39;].apply(pd.Series)], axis=1)

但是现在我很好奇我使用带有 Bert 层的 pytorch 模型来处理文本，然后将结果与数字特征连接起来并将其传递到 Linear 层。类似于：
class BaselineNN(nn.Module):
def __init__(self, input_size, hidden1, out_size, drop1):
super(BaselineNN, self).__init__()
self.l1 = BertModel.from_pretrained(&#39;bert-base-uncased&#39;) # 用于文本特征的 bert 层
self.fc2 = nn.Linear(input_size, hidden1) # 用于数字特征的线性层
self.fc3 = nn.Linear(???, out_size) # 输出层

def forward(self, input1, input2, mask, token_type_ids):
_, output_1= self.l1(input1,tention_mask = mask, token_type_ids = token_type_ids)
output_2 = self.fc2(input2)
combined = torch.cat((output_1.view(output_1.size(0), -1),
output_2.view(output_2.size(0), -1)), dim=1)
out= self.fc2(combined)
return out

def init_weights(self, m):
if isinstance(m, nn.Linear):
torch.nn.init.kaiming_uniformal_(m.weight, mode=&#39;fan_in&#39;, nonlinearity=&#39;relu&#39;)
m.bias.data.fill_(0.01)

值得吗？我的意思是我可以将文本转换为嵌入，将嵌入转换为数字并在模型中使用它。
如果值得 - 我如何正确连接 Bert 输出和线性层输出以将其传递到下一层？]]></description>
      <guid>https://stackoverflow.com/questions/78926855/how-to-handle-text-and-number-features-with-pytorch</guid>
      <pubDate>Thu, 29 Aug 2024 08:35:02 GMT</pubDate>
    </item>
    <item>
      <title>如何将 CLIP 向量转换为 LLM 文本标记嵌入？</title>
      <link>https://stackoverflow.com/questions/78926828/how-to-convert-clip-vectors-to-an-llm-text-token-embeddings</link>
      <description><![CDATA[我希望将多种模态嵌入到您传统的基于文本的 LLM 中。为此，我需要将任何模态转换为 CLIP 向量（我已经完成了），现在我需要将此向量转换为 LLM 文本标记嵌入。有人能帮我完成这个转换吗？]]></description>
      <guid>https://stackoverflow.com/questions/78926828/how-to-convert-clip-vectors-to-an-llm-text-token-embeddings</guid>
      <pubDate>Thu, 29 Aug 2024 08:27:14 GMT</pubDate>
    </item>
    <item>
      <title>有没有针对低 TOPs 边缘 AI 设备上的多目标跟踪的解决方案？[关闭]</title>
      <link>https://stackoverflow.com/questions/78926753/any-solutions-for-multi-object-tracking-on-low-tops-edge-ai-device</link>
      <description><![CDATA[我正在尝试在低 AI 功率的 PTZ 摄像机上运行 MOT 算法。场景是在会议室或教室中跟踪某人（可能是中间的人，可以使用遥控器切换到其他人）。
到目前为止，我已经尝试过：
Byte-Track：我重新训练了 crowdhuman 数据集以使用 YOLOV5s 检测人头。fps 足够好（大约 14fps）以允许 PTZ 摄像机进行跟踪。但问题是，当目标被其他人遮挡时，很容易发生 ID 切换。（它纯粹基于卡尔曼滤波器）
Bot-SORT：重新训练 Fast-ReID 模型以获取 Reid 特征，骨干仍然是 YOLOV5s。但不可能对所有人都这样做，每个用户需要 14 毫秒。
我需要一种方法来尽可能避免 ID-Switch，同时保持良好的帧速率。对我的应用程序有什么好的解决方案吗？或者有什么建议可以优化当前的算法？]]></description>
      <guid>https://stackoverflow.com/questions/78926753/any-solutions-for-multi-object-tracking-on-low-tops-edge-ai-device</guid>
      <pubDate>Thu, 29 Aug 2024 08:02:36 GMT</pubDate>
    </item>
    <item>
      <title>YOLO 的数据训练标记策略</title>
      <link>https://stackoverflow.com/questions/78926680/data-training-labeling-policy-for-yolo</link>
      <description><![CDATA[我正在训练我的 YOLO 来检测飞机和无人机。在某些图片中，无法检测到该物体确实是一架飞机，它甚至看起来像一架无人机（图片是从很远的地方拍摄的），但我从上下文中知道它确实是一架。我还应该把它标记为飞机吗？]]></description>
      <guid>https://stackoverflow.com/questions/78926680/data-training-labeling-policy-for-yolo</guid>
      <pubDate>Thu, 29 Aug 2024 07:45:18 GMT</pubDate>
    </item>
    <item>
      <title>序列到序列 LSTM 用于分类</title>
      <link>https://stackoverflow.com/questions/78925733/sequence-to-sequence-lstm-for-classification</link>
      <description><![CDATA[我有一个包含两列的数据集：
past_events，future_events。
past_events 是 53 个数字代码的序列，如下所示
&#39;198&#39;、&#39;2000&#39;、&#39;197&#39;、&#39;85903&#39;，...
而 future_events 是 52 个数字代码的序列，如下所示
&#39;345&#39;、&#39;200&#39;、&#39;8904&#39;、&#39;23765&#39;，...
每个代码代表一个事件（代码按时间顺序排列）。
这个数据集中有近 3000 行。
我想构建一个 LSTM，它采用以下序列输入为 past_events，输出为 52 个事件的序列，这些事件是 future_events 的预测。
由于每个代码代表一个事件，在 future_events 中可能会出现 past_events 中不存在的代码，并且某些代码可能只出现在几行中，因此只出现在几个序列中。
这是我第一次做这种问题。如果是序列到值，我可以做到，但是对于这种分类的序列到序列，我不知道如何构建这个模型。
你能给我一些例子或一些解释这种问题的网站吗？
这是我尝试构建 LSTM，但没有成功
input_1 = Input(shape=sequence_length)

lstm_out = LSTM(128, return_sequences=True)(input_1)
lstm_out = Dropout(0.2)(lstm_out)
lstm_out = LSTM(64, return_sequences=True)(lstm_out)
lstm_out = Dropout(0.2)(lstm_out)
lstm_out = LSTM(64, return_sequences=True)(lstm_out)
lstm_out = Dropout(0.2)(lstm_out)

lstm_out = Lambda(lambda x: x[:, :52, :])(lstm_out)

# 注意机制
attention = Attention()([lstm_out, lstm_out])

dense_out = TimeDistributed(Dense(128,activation=&#39;relu&#39;))(attention)
dense_out = Dropout(0.2)(dense_out)

output_layer = TimeDistributed(Dense(num_classes,activation=&#39;softmax&#39;))(dense_out)

我能得到的最好结果是预测都是相同的数字代码，例如 52 &#39;367&#39;]]></description>
      <guid>https://stackoverflow.com/questions/78925733/sequence-to-sequence-lstm-for-classification</guid>
      <pubDate>Thu, 29 Aug 2024 00:33:59 GMT</pubDate>
    </item>
    <item>
      <title>如何使用机器学习来追踪公司客户的资料？[关闭]</title>
      <link>https://stackoverflow.com/questions/78925616/how-to-use-machine-learning-to-trace-the-profile-of-costumers-in-a-company</link>
      <description><![CDATA[我的目标是计算客户离开公司的流失风险。我想到这个方法：

生成一份代表客户历史中最突出特征的资料，并计算新客户之间的相似度。

我该如何追踪流失风险最高的人的资料？]]></description>
      <guid>https://stackoverflow.com/questions/78925616/how-to-use-machine-learning-to-trace-the-profile-of-costumers-in-a-company</guid>
      <pubDate>Wed, 28 Aug 2024 23:20:15 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 FAISS 减少大型人脸数据库的人脸识别中的误报？</title>
      <link>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</link>
      <description><![CDATA[我正在开发一个使用人脸识别的考勤跟踪系统。
该系统的工作原理如下：

1. 人脸检测：使用 Ultra Face 检测人脸。
2. 人脸编码：使用 FaceNet 对检测到的人脸进行编码。
3. 人脸比较：将编码的人脸与现有数据库进行比较以标记出勤率
4.使用的库：OpenCV 和 FAISS。
5.来源：CCTV摄像机镜头。

考勤系统说明：
当一个人走到摄像机前时，系统使用Ultra Face检测人脸，并使用FaceNet进行编码。然后将编码的人脸与现有数据库进行比较。如果相似度（余弦相似度）小于0.25，则标记出勤。
问题：
最初，数据库中的人数少于100人，比较时间是可以接受的。随着人数的增加，比较时间明显变长。每个人在数据库中都有5张图片。为了加快比较速度，我改用FAISS库。虽然FAISS显著缩短了比较时间，但也增加了误报（错误地标记出勤）。
人脸比较的旧方法：
for db_name, db_encode in encoding_dict.items():
尝试：
dist = cosine(db_encode, f_e[1])
除 ValueError 为 e 外：
print(&quot;&gt;&gt;&gt;&gt;&gt;&gt; : &quot;,f_e[1],&quot;\n&quot;,type(f_e[1]))
继续
if dist &lt;识别_t：
name = db_name
distance = dist

cv2.rectangle(img, (f_e[0][0], f_e[0][1]), (f_e[0][2], f_e[0][3]), (0, 255, 0), 1)
cv2.putText(img, f&#39;{name}:{distance - 1:.2f}&#39;, (f_e[0][0], f_e[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

使用 FAISS 的新方法：
class StaffCustManagement：
def __init__(self, staff_n_neighbours=4, identification_t=0.80):
self.staff_db：Custom_DB = Custom_DB（db_name =“mydatabase”，col_name =“staff”）
self.staff_names，self.staff_encodings = self.staff_load_encodings（）
self.staff_n_neighbours：int = staff_n_neighbours
self.staff_ini_faiss（）
self.recognition_t：float = identification_t

def staff_load_encodings（self） -&gt; Tuple[List[str], List[np.ndarray]]:
staff_names, staff_encodings = [], []
for document in self.staff_db.find_all_data():
staff_names.append(document[&#39;_id&#39;])
staff_encodings.append(ArrayEncDec.decode_from_base64(b64_str=document[&#39;encoding&#39;]))
return staff_names, staff_encodings

def staff_ini_faiss(self):
if self.staff_names and self.staff_encodings:
Dimensions = 128
self.staff_index_faiss = faiss.IndexFlatL2(dimensions)
faiss_embeddings = np.array(self.staff_encodings, dtype=&#39;float32&#39;)
faiss.normalize_L2(faiss_embeddings)
self.staff_index_faiss.add(faiss_embeddings)

def find_staff_cust(self, current_encode: np.ndarray) -&gt; Tuple[str, float]:
name = &quot;Unknown&quot;
distance = float(&quot;inf&quot;)
if len(self.staff_names) == 0:
return name, distance
target_rep = np.expand_dims(current_encode, axis=0)
# faiss.normalize_L2(target_rep)
distances, neighbours = self.staff_index_faiss.search(target_rep, self.staff_n_neighbours)
print(&quot;Distances&quot;, distances)
print(&quot;neighbors&quot;, neighbours)
if distances[0][0] &gt;= self.recognition_t:
return self.staff_names[neighbors[0][0]].split(&#39;-&#39;)[0], distances[0][0]
return name, distance

问题：
如何在使用 FAISS 进行人脸比较时减少误报我的出勤跟踪系统如何做到这一点？虽然 FAISS 大大缩短了比较时间，但准确性却受到影响，导致出勤标记不正确。是否有任何最佳实践或替代方法可以在大型数据库中保持高精度？]]></description>
      <guid>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</guid>
      <pubDate>Fri, 12 Jul 2024 10:33:51 GMT</pubDate>
    </item>
    <item>
      <title>Google Collab 显示运行时在运行数据集时断开连接</title>
      <link>https://stackoverflow.com/questions/75904671/google-collab-shows-runtime-disconnected-on-running-dataset</link>
      <description><![CDATA[我在大小为 (2700000x7) 的二进制数据上运行决策树的 ML 代码，但当我运行它时，Google Collab 崩溃了，并在大约 3 小时后显示运行时断开连接。RAM 利用率只有 3GB。
尝试将数据集大小减小到 (100000x7)，它可以正常工作，但超过这个大小就会崩溃。]]></description>
      <guid>https://stackoverflow.com/questions/75904671/google-collab-shows-runtime-disconnected-on-running-dataset</guid>
      <pubDate>Sat, 01 Apr 2023 06:49:04 GMT</pubDate>
    </item>
    </channel>
</rss>