<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 14 Apr 2024 03:52:22 GMT</lastBuildDate>
    <item>
      <title>使用 rf.fit() 时尝试在 pyspark 中使用随机森林时出错</title>
      <link>https://stackoverflow.com/questions/78322361/error-trying-to-use-random-forest-in-pyspark-when-using-rf-fit</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78322361/error-trying-to-use-random-forest-in-pyspark-when-using-rf-fit</guid>
      <pubDate>Sun, 14 Apr 2024 00:30:15 GMT</pubDate>
    </item>
    <item>
      <title>将数据拆分为训练集、验证集和测试集，ID 不重叠，并且仍然平衡目标类</title>
      <link>https://stackoverflow.com/questions/78322346/splitting-data-into-training-validation-and-test-sets-without-overlapping-ids</link>
      <description><![CDATA[假设数据由此组成，但规模要大得多。预测变量为 var1，响应变量为 var2。
data &lt;- data.frame(ID = c(001, 001, 001, 002, 002, 002, 002, 003, 003, 003, 003, 003,
                           004, 004, 004, 004, 004, 004, 005, 005, 005, 005, 005),
                   var1 = c(0102, 0210, 0405, 1002, 0318, 0629, 1201, 0923, 0702, 0710, 0801,
                            0203、0501、1204、0516、0112、1005、0221、1101、1125、1020、0112、0310）、
                   var2 = c(“冷”, “热”, “暖”, “冷”, “暖”, “热”, “热”, “暖”, “冷”,
                           “冷”、“冷”、“暖”、“热”、“冷”、“冷”、“冷”、“热”、“暖”、
                           “冷”、“热”、“暖”、“热”、“暖”))

我需要将数据分成训练集、验证集和测试集的集合比例，同时在每个集合中保留唯一的 ID（ID 不能出现在多个集合中），并且每个级别至少需要出现一次每组内的（“热”、“暖”、“冷”）。
我尝试使用数据分割包 caret(fx = createDataPartition()) 和 splitTools (fx = partition()) 以及 dplyr 采样，但它们应用的分组可确保每个 ID 出现在每个集合中.
我的目标是得到这样的东西：
validation_set &lt;- data.frame(IDs = c(001, 001, 001, 002, 002, 002, 002),
                   var1 = c(0102, 0210, 0405, 1002, 0318, 0629, 1201),
                   var2 = c(“冷”,“热”,“暖”,“冷”,“暖”,“热”,“热”))

训练集 &lt;- data.frame(IDs = c(003, 003, 003, 003, 003,
                                     004, 004, 004, 004, 004, 004),
                             var1 = c(0923, 0702, 0710, 0801,
                                      0203, 0501, 1204, 0516, 0112, 1005, 0221),
                             var2 = c(“暖”,“冷”,“冷”,“冷”,“暖”,“热”,“冷”,“冷”,“冷”,“ “热”，
                                      “温暖”））


test_set &lt;- data.frame(IDs = c(005, 005, 005, 005, 005),
                   var1 = c(1101, 1125, 1020, 0112, 0310),
                   var2 = c(“冷”,“热”,“暖”,“热”,“暖”))

在上面的示例结果中，在划分数据之前不使用数据增强方法的情况下，验证、训练和测试划分比例约为 0.30、0.48 和 0.22。
更大的目标是按照 0.70、0.20 和 0.10 的比例一致分割验证集、训练集和测试集。]]></description>
      <guid>https://stackoverflow.com/questions/78322346/splitting-data-into-training-validation-and-test-sets-without-overlapping-ids</guid>
      <pubDate>Sun, 14 Apr 2024 00:19:39 GMT</pubDate>
    </item>
    <item>
      <title>无法训练具有多个输出的 keras 模型</title>
      <link>https://stackoverflow.com/questions/78322340/can-not-train-keras-model-with-multiple-outputs</link>
      <description><![CDATA[我正在创建一个具有多个不同形状输出的 keras 模型，因为 model.fit 方法不允许我传递 y 值的列表或字典。如果重要的话，模型会拍一张脸的照片，找出几个特征，并尝试猜测这个人的名字。
模型定义：
输入 = tf.keras.Input(shape=(1024, 1024, 3))

稠密_1 = tf.keras.layers.Dense(8192)(输入)
密集_2 = tf.keras.layers.Dense(4096)(密集_1)
race_dense = tf.keras.layers.Dense(512)(dense_2)
性别_密度 = tf.keras.layers.Dense(512)(dense_2)
eye_distance_dense = tf.keras.layers.Dense(512)(dense_2)
name_dense1 = tf.keras.layers.Dense(4096)(输入)
name_dense2 = tf.keras.layers.Dense(2048)(name_dense1)
种族 = tf.keras.layers.Dense(1, name=&quot;race&quot;)(race_dense)
性别 = tf.keras.layers.Dense(1, name=“性别”)(gender_dense)
eye_distance = tf.keras.layers.Dense(1, name=&quot;eye_distance&quot;)(eye_distance_dense)
name_t = tf.keras.layers.Dense(32, name=“名称”)(name_dense2)

模型= tf.keras.Model（输入=输入，输出=[种族，性别，眼睛距离，姓名]）
model.compile(优化器=&#39;亚当&#39;,
              损失={&#39;race&#39;: &#39;sparse_categorical_crossentropy&#39;,
                    &#39;性别&#39;: &#39;sparse_categorical_crossentropy&#39;,
                    &#39;eye_distance&#39;: &#39;sparse_categorical_crossentropy&#39;,
                    &#39;名称&#39;：&#39;sparse_categorical_crossentropy&#39;}，
              指标=[&#39;准确性&#39;])

培训：
model.fit(np.array([图像]).astype(np.float32), ([
                    [比赛],
                    [性别],
                    [眼睛距离]，
                    [填充名称]
]))

我尝试过的 Y 值：
&lt;前&gt;&lt;代码&gt;[
                    [比赛],
                    [性别],
                    [眼睛距离]，
                    [填充名称]
]

和
&lt;前&gt;&lt;代码&gt;{
                    “种族”：[种族]，
                    “性别”：[性别]，
                    “眼睛距离”：[眼睛距离]，
                    “名称”：[填充名称]
}

无论我做什么，它总是会给我一个类似的错误：
ValueError: 无法找到可以处理输入的数据适配器：, ( 包含类型 {&#39;( 包含类型值 {&quot;&quot;})&#39;})
]]></description>
      <guid>https://stackoverflow.com/questions/78322340/can-not-train-keras-model-with-multiple-outputs</guid>
      <pubDate>Sun, 14 Apr 2024 00:15:42 GMT</pubDate>
    </item>
    <item>
      <title>了解梯度提升中的模型选择</title>
      <link>https://stackoverflow.com/questions/78322296/understanding-model-selection-in-gradient-boosting</link>
      <description><![CDATA[包含问题的图片
我目前正在研究梯度增强模型，并且遇到了一种我不确定的情况。在我的模型的第一阶段，拟合了决策树，这由模型的阶跃函数外观表示。
但是，当我检查第一阶段的残差时，它们似乎表现出二次模式。这促使我考虑在第二阶段使用 2 次多项式模型。
但我很困惑，因为问题陈述建议在第二阶段使用与第一阶段相同类型的模型（即决策树）。
决策树能否捕获残差中的二次模式？或者，尽管问题陈述提出了建议，但我应该在第二阶段考虑不同类型的模型？
任何关于如何处理这种情况的澄清将不胜感激]]></description>
      <guid>https://stackoverflow.com/questions/78322296/understanding-model-selection-in-gradient-boosting</guid>
      <pubDate>Sat, 13 Apr 2024 23:38:22 GMT</pubDate>
    </item>
    <item>
      <title>使用推荐引擎为两个用户推荐一部电影</title>
      <link>https://stackoverflow.com/questions/78321965/using-recommendation-engine-to-recommend-a-movie-for-two-users</link>
      <description><![CDATA[我使用 torch 和 Fastai 来训练数据并得出用户权重与物品权重。有了经过训练的数据，使用两个用户权重的组合向一对用户推荐电影的最佳方式是什么？是否像取一对用户的 n 个参数权重的平均值然后使用这些权重和余弦相似度函数找到最佳项目一样简单？我是机器学习和数据科学的新手，所以如果这是一个愚蠢的问题，我深表歉意。
movie_factors = learn.model.i_weight.weight
user_factors = learn.model.u_weight.weight

#随机选择两个用户
user1_factors = user_factors[43].data.cpu().numpy()
user2_factors = user_factors[54].data.cpu().numpy()

# 计算user1_factors和user2_factors的平均值
avg_u_factors = torch.from_numpy(np.array((user1_factors + user2_factors) / 2)).to(movie_factors.device)

距离 = nn.CosineSimilarity(dim=1)(movie_factors, avg_u_factors)
idx = distances.argsort(降序=True)[1]
dls.classes[&#39;标题&#39;][idx]
]]></description>
      <guid>https://stackoverflow.com/questions/78321965/using-recommendation-engine-to-recommend-a-movie-for-two-users</guid>
      <pubDate>Sat, 13 Apr 2024 20:41:42 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 pytorch 训练机器学习多项式回归模型</title>
      <link>https://stackoverflow.com/questions/78321929/im-training-a-model-of-machine-learning-polynomial-regression-using-pytorch</link>
      <description><![CDATA[我想将数据绘制成 plt.scatter 表单，但是当我尝试填充它时，它只是说 x 和 y 的大小不同，而且我还挤压了它们仅一维，以便更容易绘制，但仍然不起作用。
这是情节机制：
#使用 matplotlib.pyplot 中的散点图 (x,y) 可视化数据
defplot_predictions(train_features=X_train,
                     train_labels=y_train,
                     test_features=X_test,
                     测试标签=y_测试，
                     预测=无）：
    plt.figure(figsize= (10,7))

    plt.scatter(X_train, y_train, c=“g”, label=“训练数据”)

    plt.scatter(X_test, y_test, c=“b”, label=“测试数据”)

    如果预测不是 None：
        plt.scatter（test_features，预测，c =“r”，标签=“预测”）

    plt.legend(prop={“大小”: 14})

绘图预测（）

#这里尝试解决问题
Predictions_reshape=y_preds.squeeze(dim=1)
labels_reshape=y_train.squeeze(dim=1)
打印（len（y_train），len（y_preds））
打印（labels_reshape.shape，predictions_reshape.shape）

labels_reshape=y_train.detach().numpy()
Predictions_reshape=y_preds.detach().numpy()
图_预测（标签_重塑，预测=预测_重塑）

&lt;块引用&gt;
ValueError：x 和 y 的大小必须相同

我尝试压缩张量，使它们只有一个暗淡，并且我还检查了镜头是否相同，确实如此。]]></description>
      <guid>https://stackoverflow.com/questions/78321929/im-training-a-model-of-machine-learning-polynomial-regression-using-pytorch</guid>
      <pubDate>Sat, 13 Apr 2024 20:23:01 GMT</pubDate>
    </item>
    <item>
      <title>ML 查找四边形的角点</title>
      <link>https://stackoverflow.com/questions/78321889/ml-find-corners-of-quadrilateral</link>
      <description><![CDATA[伙计们！我的任务是使用 ML 模型找到四边形形状的 4 个角点。有时四边形的一个角度会丢失（例如页面的折叠角度）。
首先，我尝试使用 MobileNetV3Small 作为主干进行图像分割，因为模型应该小而快。效果很好，但找到角落仍然是一个问题。我尝试按照 官方 keras 关键点检测等示例查找图像的关键点， medium 教程，以及许多其他来源，但似乎没有什么对我有用。我已经尝试修改它们很多次了。测试和验证的损失函数都会下降，但输出甚至不接近所需的位置。也尝试过类似以下的方法：
def conv(模型, 大小, conv2d_kernel, dilation_rate=(1, 1), pooling_size=(2, 2)):
    model.add(Conv2D(大小, conv2d_kernel, dilation_rate=dilation_rate))
    model.add(激活(&#39;relu&#39;))
    model.add(MaxPooling2D(pool_size=max_pooling))
    模型.add(Dropout(0.1))

def 密集（模型，单位）：
    model.add(密集(单位))
    model.add(激活(&#39;relu&#39;))
    模型.add(Dropout(0.1))

模型=顺序（）
model.add(InputLayer(形状=(224, 224, 3)))

转换（模型，大小=32，conv2d_kernel=（2, 2））
转换（模型，大小=64，conv2d_kernel=（3, 3））
转换（模型，大小=128，conv2d_kernel=（3, 3））

模型.add(压平())
密集（模型，20）
密集（模型，20）

model.add(密集(8))
model.compile(优化器=RMSprop(),
              损失=损失.MeanSquaredLogarithmicError(),
              指标=[metrics.MeanAbsoluteError()])

还尝试了像 (8) 和 (4,2) 这样的输出形状，但似乎没有任何效果。任何帮助将不胜感激。
PS：还忘记添加数据集注释正确，或者至少这是我在绘图上看到的。还尝试将坐标标准化为 0 和 1 之间。我的输入是 (224,224,3)。]]></description>
      <guid>https://stackoverflow.com/questions/78321889/ml-find-corners-of-quadrilateral</guid>
      <pubDate>Sat, 13 Apr 2024 20:06:34 GMT</pubDate>
    </item>
    <item>
      <title>如何修剪unet模型</title>
      <link>https://stackoverflow.com/questions/78321877/how-to-pruning-an-unet-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78321877/how-to-pruning-an-unet-model</guid>
      <pubDate>Sat, 13 Apr 2024 20:03:54 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pytest 和假设进行可视化</title>
      <link>https://stackoverflow.com/questions/78321735/using-pytest-and-hypothesis-for-visualization</link>
      <description><![CDATA[我正在使用albumentation库进行图像增强，我也在为每个类似的旋转编写测试用例应该在50 - 90度之内，Blur=blur_limit min：3 max：99，我如何可视化我的测试用例在哪里未能使用假设
带有假设可视化的 pytest]]></description>
      <guid>https://stackoverflow.com/questions/78321735/using-pytest-and-hypothesis-for-visualization</guid>
      <pubDate>Sat, 13 Apr 2024 19:08:00 GMT</pubDate>
    </item>
    <item>
      <title>sklearn DummyClassifier 的预测不正确</title>
      <link>https://stackoverflow.com/questions/78320892/incorrect-prediction-from-sklearn-dummyclassifier</link>
      <description><![CDATA[我正在尝试对学校项目的数据集执行虚拟分类。这个想法是为了了解不同政党发表演讲的频率。我的想法是按以下方式编写此代码：
from sklearn.dummy import DummyClassifier
将 pandas 导入为 pd
导入bz2


以 bz2.open(“data/ch3/speeches-201718.json.bz2”) 作为源：
    Speechs_201718 = pd.read_json（来源）

以 bz2.open(“data/ch3/speeches-201819.json.bz2”) 作为源：
    Speechs_201819 = pd.read_json（来源）


训练数据、测试数据 = 演讲_201718、演讲_201819

train_partys_count = Training_data[&#39;party&#39;].value_counts()
test_partys_count = test_data[&#39;party&#39;].value_counts()
dummy_clf = DummyClassifier(策略=“most_frequent”)

X = train_party_count
y = train_party_count.index
dummy_clf.fit(X.值, y)
打印（X）
打印（y）

test_parties_count.index = pd.CategoricalIndex(test_parties_count.index,categories=train_parties_count.index,ordered=True)
X_test = test_partys_count.sort_index()
打印（X_测试）
pred_mfc = dummy_clf.predict(X_test.values)

print(&quot;Urval av prediktioner [0-4]: &quot;, pred_mfc[:5])


我得到以下输出：
在此处输入图片描述
正如您所看到的，预测应该是 S 时却是 C，什么可能是不正确的？
我尝试以多种方式定义训练和测试数据，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78320892/incorrect-prediction-from-sklearn-dummyclassifier</guid>
      <pubDate>Sat, 13 Apr 2024 14:26:18 GMT</pubDate>
    </item>
    <item>
      <title>我在重塑图像数据集时遇到错误</title>
      <link>https://stackoverflow.com/questions/78312092/i-am-facing-error-in-reshaping-our-image-dataset</link>
      <description><![CDATA[我遇到此错误文件“C:\Users\Kanishka Patel\anaconda3\Lib\site-packages\keras\src\ saving\serialization_lib.py”，第 600 行，在 deserialize_keras_object return deserialize_keras_object( ^^^^ ^^^^^^^^^^^^^^^^^^^^^^^ 文件“C:\Users\Kanishka Patel\anaconda3\Lib\site-packages\ker
我尝试将 217560 重塑为 [224,224,3]]]></description>
      <guid>https://stackoverflow.com/questions/78312092/i-am-facing-error-in-reshaping-our-image-dataset</guid>
      <pubDate>Thu, 11 Apr 2024 17:27:34 GMT</pubDate>
    </item>
    <item>
      <title>不平衡的糖尿病视网膜病变分割：过度拟合和损失函数选择</title>
      <link>https://stackoverflow.com/questions/78311946/imbalanced-diabetic-retinopathy-segmentation-overfitting-and-loss-function-choi</link>
      <description><![CDATA[我正在训练一个 U-net 模型，用于分割糖尿病视网膜病变图像中的微动脉瘤。数据集不平衡，背景类（健康组织）占主导地位，约为 96%，前景类（微动脉瘤）仅占 4%。
我遇到两个主要问题：
过度拟合：模型似乎过度拟合训练数据。训练损失显着下降，但验证损失趋于稳定甚至增加。
负损失函数：在使用二元交叉熵 (BCE) 损失时，我在训练期间观察到负损失值。
当前方法：
模型：具有 EfficientNetB0 主干的 U-net 和来自 ImageNet 的预训练权重。
数据增强：旋转和翻转用于数据增强。
损失函数：目前使用二元交叉熵（BCE）。
预期模型：
从训练数据中学习相关特征。
很好地推广到未见过的数据（验证集），训练和验证损失在整个训练过程中稳步下降。
实现稳定的 BCE 损失，反映模型区分前景（微动脉瘤）和背景（健康组织）的性能]]></description>
      <guid>https://stackoverflow.com/questions/78311946/imbalanced-diabetic-retinopathy-segmentation-overfitting-and-loss-function-choi</guid>
      <pubDate>Thu, 11 Apr 2024 16:57:37 GMT</pubDate>
    </item>
    <item>
      <title>OpenCV 新手，我如何安装/构建 opencv_traincascade</title>
      <link>https://stackoverflow.com/questions/78286577/new-to-opencv-how-do-i-install-build-opencv-traincascade</link>
      <description><![CDATA[所以我一直在从事机器学习项目，并且需要使用 opencv_traincascade 训练自定义数据集。但每当我尝试安装它时，它就永远无法工作。我还有其他东西，比如 opencv_annotation 和其他东西可以工作，但是 traincascade 或 event createsamples 不起作用。我必须手动构建这些吗？
我下载了mingw-gcc、cmake，在网上找不到可行的解决方案。顺便说一句，我有 opencv 4.9.0，手动安装在 anaconda 和我的 C: 驱动器中。我也尝试过寻找一些第三方，他们安装了整个 opencv 并且可以复制，但没有运气。任何帮助将不胜感激，谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78286577/new-to-opencv-how-do-i-install-build-opencv-traincascade</guid>
      <pubDate>Sun, 07 Apr 2024 03:46:34 GMT</pubDate>
    </item>
    <item>
      <title>如何将 tfidfvectorizer 的功能从英语修改为西班牙语</title>
      <link>https://stackoverflow.com/questions/78232328/how-to-modify-features-of-tfidfvectorizer-from-english-to-spanish</link>
      <description><![CDATA[我有一个 TfidfVectorizer 模型，该模型经过英语文本数据的训练来预测英语通话中的情绪。我想针对西班牙语文本调整此 TfidfVectorizer，以便我可以将其与使用原始英语 TfidfVectorizer 训练的现有 XGBoost 模型一起使用。我的目标是在将功能从英语转换为西班牙语的同时保留现有的权重，例如将“谢谢”翻译为西班牙语。致“谢谢”，并重复使用旧的权重。本质上，我想应用相同的 TfidfVectorizer，但修改了功能名称。
这些功能已从英语翻译为西班牙语，并且 TfidfVectorizer 已针对英语文本进行了训练。我需要一种方法来构建一个新的 TfidfVectorizer，它融合了旧的权重和新的西班牙语特征，而无需重新拟合模型或将整个文本语料库翻译成西班牙语。你能推荐一种Python方法来实现这一点吗？请添加相关的Python代码。]]></description>
      <guid>https://stackoverflow.com/questions/78232328/how-to-modify-features-of-tfidfvectorizer-from-english-to-spanish</guid>
      <pubDate>Wed, 27 Mar 2024 14:11:46 GMT</pubDate>
    </item>
    <item>
      <title>我想要一个 python 脚本将洋葱图像的背景更改为黑色，该怎么做？</title>
      <link>https://stackoverflow.com/questions/75859536/i-want-a-python-script-to-change-the-background-to-black-of-a-image-of-onion-ho</link>
      <description><![CDATA[我正在尝试将洋葱图像的背景颜色更改为黑色
我尝试使用 opencv 和 Pixellib 在 python 中编写代码，但它不起作用，我希望得到一些帮助来改变这一点]]></description>
      <guid>https://stackoverflow.com/questions/75859536/i-want-a-python-script-to-change-the-background-to-black-of-a-image-of-onion-ho</guid>
      <pubDate>Mon, 27 Mar 2023 19:10:07 GMT</pubDate>
    </item>
    </channel>
</rss>