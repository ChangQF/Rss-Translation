<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 22 Apr 2024 09:15:47 GMT</lastBuildDate>
    <item>
      <title>SpaCy 微调 GPU</title>
      <link>https://stackoverflow.com/questions/78364403/spacy-fine-tuning-gpu</link>
      <description><![CDATA[我使用 spaCy 和 classy 分类训练文本分类器。但模型训练时不使用gpu，fine-tuning时间很长
GPU信息
&lt;前&gt;&lt;代码&gt;$ nvidia-smi
2024 年 4 月 22 日星期一 09:41:13
+------------------------------------------------ --------------------------------------+
| NVIDIA-SMI 545.29.06 驱动程序版本：545.29.06 CUDA 版本：12.3 |
|------------------------------------------+----- ---------------+--------------------+
| GPU 名称持久性-M |总线 ID Disp.A |挥发性未校正。 ECC |
|风扇温度性能功率：使用/上限 |内存使用情况 | GPU-Util 计算 M。
| | |米格·M。
|============================================+======== ==============+======================|
| 0 NVIDIA GeForce GTX 1650 Ti 关闭 | 00000000:01:00.0 关闭 |不适用 |
|不适用 62C P8 1W / 50W | 6MiB / 4096MiB | 0% 默认 |
| | |不适用 |
+----------------------------------------------------+----- ---------------+--------------------+
                                                                                         
+------------------------------------------------ --------------------------------------+
|流程： |
| GPU GI CI PID 类型 进程名称 GPU 内存 |
| ID ID 用途 |
|=================================================== ========================================|
| 0 不适用 不适用 2154 G /usr/bin/gnome-shell 1MiB |
+------------------------------------------------ --------------------------------------+

GPU 可用于 cupy
导入 cupy 作为 cp
x = cp.array([1, 2, 3])
打印（x）

[1 2 3]

GPU 可用于手电筒
导入火炬
打印（火炬.__版本__）
打印（火炬.版本.cuda）

将张量流导入为 tf
打印（tf.__版本__）
打印（tf.test.gpu_device_name（））

12.1
2.15.0
/设备:GPU:0

我已经尝试过这个。
spacy.require_gpu()

还有这个
spacy.prefer_gpu(0)

nlp.add_pipe(
 &#39;classy_classification&#39;,
 配置={
      “数据”：train_samples，
      &#39;model&#39; : &#39;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&#39;,
      &#39;设备&#39;：&#39;GPU&#39;，
      “详细”：正确
    }
）
]]></description>
      <guid>https://stackoverflow.com/questions/78364403/spacy-fine-tuning-gpu</guid>
      <pubDate>Mon, 22 Apr 2024 06:47:09 GMT</pubDate>
    </item>
    <item>
      <title>无法导入 py 文件并出现以下错误消息：无法解析导入“load_images”(reportMissingImports)</title>
      <link>https://stackoverflow.com/questions/78364383/the-py-file-cannot-be-imported-and-the-following-error-message-appears-import</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78364383/the-py-file-cannot-be-imported-and-the-following-error-message-appears-import</guid>
      <pubDate>Mon, 22 Apr 2024 06:43:40 GMT</pubDate>
    </item>
    <item>
      <title>Python 机器学习代码故障排除：Fit 函数问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/78364124/troubleshooting-python-machine-learning-code-issues-with-fit-function</link>
      <description><![CDATA[我正在尝试构建一个机器学习模型，但效果不是很好。有人可以帮我吗？这是我的代码。我创建这段代码是为了我的练习和理解。如果可能的话，有人可以帮助我使用此代码吗？
我用来训练代码的数据位于此处
将 numpy 导入为 np

将 pandas 导入为 pd



# 数据准备

data = pd.read_csv(&#39;调查肺癌.csv&#39;)

测试数据 = data.head(300)



# 取消注释以使用特定列

# x1 = 测试数据[&#39;性别&#39;]

# x2 = 测试数据[&#39;年龄&#39;]



# 使用除“LUNG_CANCER”之外的所有列

x = 测试数据.列

Xc = np.array([testdata[i] for i in x if i != &#39;LUNG_CANCER&#39;])

X = np.column_stack((np.ones(300), Xc.T))



# 将“GENDER”编码为二进制

对于范围内的 i(len(X.T[1]))：

    如果 X.T[1][i] == &#39;M&#39;，则 X.T[1][i] = 1，否则 0



# 将“LUNG_CANCER”编码为二进制

y = testdata[&#39;LUNG_CANCER&#39;].apply(lambda x: 1 if x == &#39;yes&#39; else 0)



# 逻辑回归实现

逻辑回归类：

    def __init__(自身, X, y, lr=0.01, n=1000):

        自我.X = X

        自我.y = y

        self.weight = np.zeros(X.shape[1])

        self.lr = lr

        self.n_iter = n



    @静态方法

    定义 sigmoid(z):

        返回 1 / (1 + np.exp(-z))



    def 适合（自我）：

        对于 _ 在范围内（self.n_iter）：

            y_pred = np.dot(self.X, self.weight)

            pred = self.sigmoid(y_pred)

            dw = (1 / self.X.shape[0]) * np.dot(self.X.T, pred - self.y)

            self.weight -= self.lr * dw

        返回自重



    def 预测（自我，信息）：

        z = np.dot(信息, self.weight)

        prob = self.sigmoid(z)

        如果 prob &gt; 则返回 1 0.5 否则 0



# 测试模型

测试=逻辑回归（X，y）

打印（测试.fit（））

信息 = np.array([1, 1, 64, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2])

预测=测试.预测（信息）

打印（预测）
]]></description>
      <guid>https://stackoverflow.com/questions/78364124/troubleshooting-python-machine-learning-code-issues-with-fit-function</guid>
      <pubDate>Mon, 22 Apr 2024 05:31:10 GMT</pubDate>
    </item>
    <item>
      <title>我对我创建的 PINNs 模型有疑问 [关闭]</title>
      <link>https://stackoverflow.com/questions/78363881/i-have-doubts-about-the-pinns-model-that-i-created</link>
      <description><![CDATA[我尝试为一维表面波高程创建 PINN，输入为 (x,t)。经过长时间的尝试和错误，我意识到我的模型仍然不适合。我对之前创建的代码产生了怀疑，但又找不到错误所在，因为训练过程一直运行得很顺利。
我的代码哪里出错了？
导入tensorflow为tf
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

# 转换为 TensorFlow 张量
x_train = tf.convert_to_tensor(x, dtype=tf.float32)
t_train = tf.convert_to_tensor(t, dtype=tf.float32)
eta_train = tf.convert_to_tensor(eta_X, dtype=tf.float32)

# 组合 x 和 t 作为训练输入
输入 = tf.concat([x_train, t_train], axis=1)

# 定义物理信息神经网络 (PINN) 模型
PINN 类（tf.keras.Model）：
    def __init__(自身):
        超级（PINN，自我）.__init__()
        self.dense1 = tf.keras.layers.Dense(1000，激活=&#39;tanh&#39;，input_dim=2)
        self.dense2 = tf.keras.layers.Dense(1000, 激活=&#39;tanh&#39;)
        self.dense3 = tf.keras.layers.Dense(1000, 激活=&#39;tanh&#39;)
        self.dense4 = tf.keras.layers.Dense(1000, 激活=&#39;tanh&#39;)
        self.dense5 = tf.keras.layers.Dense(1000, 激活=&#39;tanh&#39;)
        self.dense6 = tf.keras.layers.Dense(1000, 激活=&#39;tanh&#39;)
        self.output_layer = tf.keras.layers.Dense(1, 激活=无)
        
    def 调用（自身，输入）：
        x = 输入[:, 0:1]
        t = 输入[:, 1:2]
        concat_input = tf.concat([x,t], 轴=1)
        hidden_​​1 = self.dense1(concat_input)
        隐藏_2 = self.dense2(隐藏_1)
        隐藏_3 = self.dense3(隐藏_2)
        隐藏_4 = self.dense4(隐藏_3)
        隐藏_5 = self.dense5(隐藏_4)
        隐藏_6 = self.dense6(隐藏_5)
        输出 = self.output_layer(hidden_​​6)
        返回输出
    
def物理损失（模型，x，t）：
    使用 tf.GradientTape(persistent=True) 作为磁带：
        磁带.watch(x)
        磁带.watch(t)
        u_pred = 模型(tf.concat([x,t], axis=1))
        u_x = Tape.gradient(u_pred, x)
        u_t = Tape.gradient(u_pred, t)
        删除磁带
    
    克=9.81
    h = 1
    
    损失 = u_t + np.sqrt(g*h) * u_x
    
    返回 tf.reduce_mean(tf.square(loss))

# 创建并编译PINN模型
模型 = PINN()
优化器 = tf.keras.optimizers.Adam(learning_rate=0.0001)

# 训练循环
纪元 = 200000
对于范围内的纪元（纪元）：
    使用 tf.GradientTape() 作为磁带：
        物理损失值=物理损失（模型，x_train，t_train）
        data_loss_value = tf.reduce_mean(tf.square(模型(输入) - eta_train))
        总损失 = 物理损失值 + 数据损失值
        
    梯度 = Tape.gradient(total_loss, model.trainable_variables)
    optimizer.apply_gradients(zip(梯度, model.trainable_variables))
    
    如果纪元% 1000 == 0:
        print(f&quot;纪元 {epoch}/{epochs}，总损失：{total_loss.numpy()}，物理损失：{physicals_loss_value.numpy()}，数据损失：{data_loss_value.numpy()}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78363881/i-have-doubts-about-the-pinns-model-that-i-created</guid>
      <pubDate>Mon, 22 Apr 2024 03:47:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 ONNX 模型进行量化。推理步骤</title>
      <link>https://stackoverflow.com/questions/78363618/quantization-using-onnx-model-inference-step</link>
      <description><![CDATA[我在 ONNX 量化方面遇到一些问题。
我将 ResNet18 转换为 ONNX 量化模型，并尝试实现一些专用硬件来复制操作。在量化步骤中，ONNX 参考文献中解释了量化的公式为：
y = 饱和 ((x / y_scale) + y_zero_point)。
对于去量化，公式为：
y = (x - x_zero_point) * x_scale
我使用对称量化（zero_point=0），所以它变成：
量化：
y = 饱和 (x / y_scale) 。
去量化：
y = x * x_scale
您可以在此处找到相关信息
https://onnxruntime.ai/docs/performance/model-optimizations/quantization。 html
问题是，如果我查看 ONNX 训练后在校准步骤中计算的缩放因子，缩放因子均小于 1。例如，它们的值介于 0 和 1 之间。如附图所示（使用 Netron 生成）。
用于量化的 ONNX 比例因子示例 
如果我的缩放因子小于一，那么量化和反量化操作就不再有意义。因为假设我想以 int8 进行量化，并且从 fp32 数字开始，我想要这样的东西：
xq = x_fp /s
这样我就可以减少用 int8 表示的 x_fp 的动态范围。但如果我的缩放因子小于 1，我实际上正在扩大范围！
因此，对于量化，我得到的范围更大而不是更小，而对于去量化，我得到的范围更小，而不是更大。
我在想我可能必须将缩放因子乘以我想要的量化位宽度，例如 uint8 的 2^8=256。
由于缩放因子的计算方式为 s= (2^N-1) / fp_range，因此他们可能只提供以下部分：
s_onnx = 1/fp_range
并让您精确地乘以所需的位。
有人可以帮我解决这个问题吗？
谢谢！
考虑到缩放因子在 0 和 1 之间，那么我期望这样的公式：
量化
x_q = x_fp * 比例
去量化
x_fp = x_q / 比例
而不是相反。]]></description>
      <guid>https://stackoverflow.com/questions/78363618/quantization-using-onnx-model-inference-step</guid>
      <pubDate>Mon, 22 Apr 2024 01:31:14 GMT</pubDate>
    </item>
    <item>
      <title>在 keras 调谐器中使用 F1 分数作为指标时遇到问题</title>
      <link>https://stackoverflow.com/questions/78363511/having-trouble-using-the-f1-score-as-a-metric-in-keras-tuner</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78363511/having-trouble-using-the-f1-score-as-a-metric-in-keras-tuner</guid>
      <pubDate>Mon, 22 Apr 2024 00:18:59 GMT</pubDate>
    </item>
    <item>
      <title>用于近似四象限图上 x,y 坐标的机器学习模型</title>
      <link>https://stackoverflow.com/questions/78363501/machine-learning-model-for-approximating-x-y-coordinates-on-four-quadrant-graph</link>
      <description><![CDATA[该项目适用于大学课程。我从 政治指南针 收集了数据，我想用它来近似 x 坐标和 y 坐标值使用 Twitter 数据的政治指南针。这只是一个片段：

&lt;标题&gt;

键
twitter_user_id
politician_name
twitter_handle
x_坐标
y_坐标
政党
选举年
国家
twitter_active_during_election


&lt;正文&gt;

8132862008
813286
巴拉克·奥巴马
巴拉克奥巴马
3
2
民主
2008
美国
正确


9390912008
939091
乔·拜登
乔拜登
3
3
民主
2008
美国
正确


150226332008
15022633
丹尼斯·库西尼奇
丹尼斯_库西尼奇
-2
-2
民主
2008
美国
正确


314286852008
31428685
比尔·理查森
理查森政府
4
4
民主
2008
美国
错误


154165052008
15416505
迈克·哈克比
GovMikeHuckabee
6
6
共和党
2008
美国
正确



我的教授告诉我使用两种逻辑回归模型 - 一种用于近似 x 值，另一种用于近似 y。我想确保这是一个可行的方法。根据我在网上阅读的内容，逻辑回归似乎是一个二元模型。我找不到提供除 yes/no 或 1/0 之外的输出的示例。
逻辑回归可以近似这些值吗？如果不是，什么模型是该项目的正确方法？]]></description>
      <guid>https://stackoverflow.com/questions/78363501/machine-learning-model-for-approximating-x-y-coordinates-on-four-quadrant-graph</guid>
      <pubDate>Mon, 22 Apr 2024 00:13:56 GMT</pubDate>
    </item>
    <item>
      <title>使用神经网络的多类别分类问题</title>
      <link>https://stackoverflow.com/questions/78363467/classification-problem-with-multi-categories-using-neural-networks</link>
      <description><![CDATA[在我的设置中，我遇到一个问题，其中每个实体（假设用户）都有一组分类属性。为了简单起见，我们可以将它们表示为数字，例如：

u1, [1,2,3]
u2 [0, 4]
u3 [0,1,2,3,4]

假设我们有多个类别MAX_CATEGORIES。因此，在我的玩具设置中，生成了标签：
def likes_movies(row):
    如果 (MAX_CATEGORY - 1) 在行中则返回 1，否则返回 0

我尝试了一个模型，其中每个类别都表示为嵌入，然后聚合它们：
类 SimpleMultiCategoricalClassifier(torch.nn.Module):
    def __init__(self, num_embeddings, embedding_dim):
        超级().__init__()

        self.embeddings = nn.EmbeddingBag(
                num_embeddings=num_embeddings,
                embedding_dim=embedding_dim, mode=“平均值”）

        self.net = nn.Sequential(
            nn.Linear(embedding_dim, 2),
            nn.ReLU()
        ）
        
    def 前向（自身，输入，偏移量）：
        x = self.embeddings(输入, 偏移量)
        返回 self.net(x)

但是我连最简单的功能都学不会。我的分类器损失只是在一个固定点上振荡，基本上没有学到任何东西。我做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78363467/classification-problem-with-multi-categories-using-neural-networks</guid>
      <pubDate>Sun, 21 Apr 2024 23:45:36 GMT</pubDate>
    </item>
    <item>
      <title>我收到错误 - ValueError: 在使用模型 ss3 进行投票分类器时，估计器 SS3 应该是分类器</title>
      <link>https://stackoverflow.com/questions/78362782/i-am-getting-an-error-valueerror-the-estimator-ss3-should-be-a-classifier-whi</link>
      <description><![CDATA[我正在研究一个使用投票的集成模型，用于 ss3 模型、svm 模型和 RoBERTa 模型，但我遇到了很多错误。
我尝试包含拟合函数、预测函数，并尝试在 ss3 类的 init 中添加 _estimator_type = &#39;classifier&#39; 行，但随后出现错误，表明这是不必要的。请帮助我消除这个错误并指导我如何制作集成模型。
这是我面临最多问题的代码部分：
SS3 级：
def __init__(自身):
    _estimator_type = &#39;分类器&#39;
    # self.train_df = 无
    self.cf = 无
    self. precision = 无，
    自我回忆=无，
    self.f1_score = 无，
    self.accuracy = 无，
    self.local_values = 无

def get_params(self, deep=True):
    返回 {
        &#39;_estimator_type&#39;: self._estimator_type,
    }

这是错误：
回溯（最近一次调用最后一次）：
第 236 行，在 _validate_estimators 中
引发值错误（
ValueError：估计器 SS3 应该是一个分类器。
进程已完成，退出代码为 1]]></description>
      <guid>https://stackoverflow.com/questions/78362782/i-am-getting-an-error-valueerror-the-estimator-ss3-should-be-a-classifier-whi</guid>
      <pubDate>Sun, 21 Apr 2024 18:48:18 GMT</pubDate>
    </item>
    <item>
      <title>Baseline3 TD3，reset() 方法值太多，无法解包错误</title>
      <link>https://stackoverflow.com/questions/78361630/baseline3-td3-reset-method-too-many-values-to-unpack-error</link>
      <description><![CDATA[环境是python 3.10，stable-baseline3 2.3.0，我正在尝试TD3算法。
无论我做什么，我都会遇到同样的错误。
据我所知，重置方法的返回值与定义的观察空间相同
我制作的环境有如下重置方法
def重置（自身，种子=0）：
    self.current_index = 0
    self.current_cash = self.start_cash
    self.done = False
    self.当前时间 = self.开始时间

    # 초기 관찰 상태 계산
    初始状态 = self.get_state() # 字典
    返回初始状态

它从来都不复杂，定义环境，模型也很好
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
从 stable_baselines3 导入 TD3

类 CustomFeatureExtractor(BaseFeaturesExtractor):
    def __init__(自我, 观察空间, features_dim=5):
        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim)
        self.model_alpha = ModelAlpha()
    
    defforward（自我，观察）：
        价格 = 观察结果[&#39;价格&#39;]
        位置 = 观测值[&#39;位置&#39;]
        数量 = 观察结果[&#39;数量&#39;]
        pnr = 观测值[&#39;pnr&#39;]
        
        return self.model_alpha(价格, torch.cat([位置, 数量, pnr]))
        
        
# 환경과 모델 설정
env = MarketEnvironment(蜡烛, &#39;2020-07-01 00:00:00&#39;, &#39;2023-12-31 23:59:00&#39;) # 여러분의 환경 설정
策略_kwargs = 字典（
    features_extractor_class=自定义特征提取器，
    features_extractor_kwargs=dict（features_dim=5）
）

模型 = TD3(“MultiInputPolicy”，env，policy_kwargs=policy_kwargs，batch_size=128，verbose=1)

Jupyter 提示符表示
使用CPU设备
使用 Monitor 包装器包装环境
将环境包装在 DummyVecEnv 中。
它运行良好，直到
model.learn（total_timesteps=1，log_interval=10，progress_bar=True）

这段代码。
无论我做了什么，它都会一遍又一遍地说
文件 ~\.conda\envs\mlbase-py3.10\lib\site-packages\stable_baselines3\common\off_policy_algorithm.py:297，在 OffPolicyAlgorithm._setup_learn(self、total_timesteps、callback、reset_num_timesteps、tb_log_name ， 进度条）
    第290章
    第291章
    292 和 self.env.num_envs &gt; 1
    293 而不是 isinstance(self.action_noise, VectorizedActionNoise)
    第294章）：
    第295章
--&gt;第297章
    298 总时间步数，
    299回调，
    300 重置_num_timesteps，
    第301章
    第302章
    第303章）

文件~\.conda\envs\mlbase-py3.10\lib\site-packages\stable_baselines3\common\base_class.py:425，在BaseAlgorithm._setup_learn(self,total_timesteps,callback,reset_num_timesteps,tb_log_name,progress_bar)中
    第423章
    第424章 断言self.env不是None
--&gt;第425章
    第426章
    第427章

文件 ~\.conda\envs\mlbase-py3.10\lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py:77，在 DummyVecEnv.reset(self) 中
     范围内的 env_idx 为 75(self.num_envs)：
     76 Maybe_options = {“选项”: self._options[env_idx]} if self._options[env_idx] else {}
---&gt; 77 obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
     78 self._save_obs（env_idx，obs）
     79 # 种子和选项仅使用一次

ValueError：需要解压的值太多（预期为 2）

我知道这个错误的reset()方法是在一个名为VecEnv的抽象类中
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78361630/baseline3-td3-reset-method-too-many-values-to-unpack-error</guid>
      <pubDate>Sun, 21 Apr 2024 12:58:27 GMT</pubDate>
    </item>
    <item>
      <title>KNN 类型错误和数据标准化</title>
      <link>https://stackoverflow.com/questions/78360476/knn-typeerror-and-data-normalization</link>
      <description><![CDATA[即使完成教程指示的所有操作，我也无法使预测工作。
从 sklearn.neighbors 导入 KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=3, metric=&#39;euclidean&#39;)
knn_model.fit(train_val_process, merge_labels.label)
y_pred_knn = knn.predict(test_data_process)

错误如下：
TypeError：KNeighborsClassifier.predict() 缺少 1 个必需的位置参数：
&#39;X&#39;

我应该将数据均值标准化为 1 std 0。这样好吗？
from sklearn.preprocessing import StandardScaler
定标器=标准定标器()
缩放器.fit(pd.concat([train_data, val_data]))
train_data_process = pd.DataFrame(scaler.transform(train_data), columns=train_data.columns)
val_data_process = pd.DataFrame(scaler.transform(val_data), columns=val_data.columns)
train_val_process = pd.concat([train_data, val_data])
test_data_process = pd.DataFrame(scaler.transform(test_data), columns=test_data.columns)
y_test = test_labels.label
]]></description>
      <guid>https://stackoverflow.com/questions/78360476/knn-typeerror-and-data-normalization</guid>
      <pubDate>Sun, 21 Apr 2024 05:13:50 GMT</pubDate>
    </item>
    <item>
      <title>梅尔频谱图的卷积自动编码器。不起作用</title>
      <link>https://stackoverflow.com/questions/78353717/convolutional-autoencoder-from-mel-spectogram-does-not-work</link>
      <description><![CDATA[在此处输入图像描述
# 将列表转换为 numpy 数组
data_array = np.array(data_list, dtype=&#39;float32&#39;)
data_array = np.array(data_array, dtype=&#39;float32&#39;) / 255.0 # 所以我的数据是从0到1
导入操作系统
导入keras
将 numpy 导入为 np
将张量流导入为 tf
从张量流导入keras
从 keras.layers 导入输入、Conv2D、BatchNormalization、MaxPooling2D、UpSampling2D、Flatten、Dense、Reshape、Dropout
从 keras.models 导入模型
从 sklearn.model_selection 导入 train_test_split
#从keras.preprocessing.image导入img_to_array，load_img
从 sklearn.model_selection 导入 train_test_split
#from keras.callbacks 导入 LearningRateScheduler
从 sklearn.model_selection 导入 train_test_split
从 keras.callbacks 导入 TensorBoard
导入时间
从 keras 导入正则化器

train_images, test_images = train_test_split(data_array, test_size=0.1) # 10% 用于测试
train_images, val_images = train_test_split(train_images, test_size=0.1) # 其余的 10% 用于验证

print(f&#39;训练集大小：{train_images.shape}&#39;)
print(f&#39;验证集大小：{val_images.shape}&#39;)
print(f&#39;测试集大小：{test_images.shape}&#39;)

# 超参数正确或接近正确？参数以纸质为准。
H、W、C = 256, 256, 1 # 1 排列 np 黑白
学习率 = 1e-3
批量大小 = 16
纪元 = 50 #Random 纪元
Latent_dim = 128 # 理解

l2_reg = 正则化器.l2(1e-4)

输入=输入（形状=（H，W，C））
x = Conv2D(32, (5, 5), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;, kernel_regularizer=l2_reg)(输入)
x = BatchNormalization()(x)
x = MaxPooling2D((4, 4), padding=&#39;same&#39;)(x) #固定池化
x = Conv2D(64, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;, kernel_regularizer=l2_reg)(x)
x = BatchNormalization()(x)
x = MaxPooling2D((4, 4), 填充=&#39;相同&#39;)(x)
x = Conv2D(128, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;, kernel_regularizer=l2_reg)(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2), 填充=&#39;相同&#39;)(x)

# X 是我最后一层的输出

# 关于 X\ 的瓶颈操作
瓶颈=展平()(x)
瓶颈=密集（latent_dim，激活=&#39;relu&#39;，kernel_regularizer=l2_reg）（bottleneck）#潜在空间（LS）
瓶颈 = Dropout(0.3)(bottleneck) # 应用 dropout 进行正则化
                                       #输出是瓶颈
#解码器
x = 密集（8* 8* 128，激活=&#39;relu&#39;，kernel_regularizer=l2_reg）（瓶颈）
x = 重塑((8, 8, 128))(x)
x = 上采样2D((2, 2))(x)
x = Conv2D(128, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;, kernel_regularizer=l2_reg)(x)
x = BatchNormalization()(x)

x = 上采样2D((4, 4))(x)
x = Conv2D(64, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;, kernel_regularizer=l2_reg)(x)
x = BatchNormalization()(x)

x = 上采样2D((4, 4))(x)
x = Conv2D(32, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;, kernel_regularizer=l2_reg)(x)
x = BatchNormalization()(x)
#填充？
#x = ZeroPadding2D(padding=((2, 2), (14, 14)))(x) # 大小要求？
# 最终重建
outputs = Conv2D(1, (5, 5), activate=&#39;sigmoid&#39;, padding=&#39;same&#39;, kernel_regularizer=l2_reg)(x) # 修改 1 因为之前有一个 3 : 没有意义，为什么是 sigmoid
#Sigmoid = 0 到 1 之间的值
#或Relu


# 完整的自动编码器模型
自动编码器=模型（输入，输出）
autoencoder.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;,metrics=[&#39;mae&#39;]) # 或 mse
 


# 模型架构
自动编码器.summary()
打印（train_images.shape，test_images.shape）

从 keras.callbacks 导入 EarlyStopping

#early_stopping = EarlyStopping（monitor=&#39;val_loss&#39;，耐心=5，restore_best_weights=True）
# train_images、val_images 在 CAE 开始时预加载

历史=自动编码器.fit(
    train_images, train_images, # 输入和目标
    纪元=纪元，
    批量大小=批量大小，
    洗牌=真，#True
    回调=[张量板],
    验证数据=（val_images，val_images），#validation_data=（val_images，val_images）
）
# 生成重建
rec_images = autoencoder.predict(val_images)[[在此处输入图像描述](https://i.stack.imgur.com/4g01e.png)](https://i.stack.imgur.com/trl9d.png)

我有 2550 个 2 秒的音频文件，我应用了 Mel 扫描图，仅使用 np 数组数据，我为我的 CAE 提供了这些尺寸 2562561。我已经应用了早期停止、主动学习和调节 L2 来提高我的 NN 学习，但我不知道为什么它不起作用。我对 NN 没有太多的经验，我想了解我在 NN 上做错了什么。我将在此处附上代码和结果。如果您想分享您的类似经验和这些问题的解决方案，请提前致谢。 在此处输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/78353717/convolutional-autoencoder-from-mel-spectogram-does-not-work</guid>
      <pubDate>Fri, 19 Apr 2024 12:38:40 GMT</pubDate>
    </item>
    <item>
      <title>如何通过 mRMRe 包找到最佳特征数？</title>
      <link>https://stackoverflow.com/questions/71996789/how-to-find-the-optimal-feature-count-by-mrmre-package</link>
      <description><![CDATA[我正在尝试使用 R 中的 mRMRe 包对基因表达数据集进行特征选择。我有包含超过 10K 个基因的 RNA seq 数据，我想找到适合分类模型的最佳特征。我想知道如何找到最佳特征数。这是我的代码，
mrEnsemble &lt;- mRMR.ensemble(data = Xdata, target_indices = c(1) ,feature_count = 100 ,solution_count = 1)
mrEnsemble_genes &lt;- as.data.frame(apply(solutions(mrEnsemble)[[1]], 2, function(x, y) { return(y[x]) }, y=featureNames(Xdata)))
查看（mrEnsemble_genes）

我刚刚设置了feature_count = 100，但我想知道如何在不设置数量的情况下找到分类的最佳特征数量。
提取 mrEnsemble_genes 后的结果将是基因列表，例如，
&lt;前&gt;&lt;代码&gt;gene05
基因08
基因45
基因67

他们的排名是根据相互信息计算出的分数吗？我的意思是排名第一的基因获得最高的 MI，它可能是对样本类别（即癌症和正常）进行分类的良好基因，对吗？谢谢]]></description>
      <guid>https://stackoverflow.com/questions/71996789/how-to-find-the-optimal-feature-count-by-mrmre-package</guid>
      <pubDate>Mon, 25 Apr 2022 08:50:24 GMT</pubDate>
    </item>
    <item>
      <title>如何从 Scikit-learn 中获取多类分类的特异性和阴性预测值？</title>
      <link>https://stackoverflow.com/questions/63526955/how-to-obtain-specificity-and-negative-predictive-value-from-scikit-learn-for-mu</link>
      <description><![CDATA[目前，scikit-learn 的默认分类报告 (sklearn.metrics.classification_report - 链接）不包括特异性和阴性预测值 (NPV)。
因此，我制作了自己的分类报告功能：
def custom_classification_report(y_true, y_pred):
    tp, fn, fp, tn = fusion_matrix(y_true, y_pred).ravel()
    acc = (tp+tn)/(tp+tn+fp+fn)
    森 = (tp)/(tp+fn)
    sp = (tn)/(tn+fp)
    ppv = (tp)/(tp+fp)
    净现值 = (tn)/(tn+fn)
    f1 = 2*(sen*ppv)/(sen+ppv)
    fpr = (fp)/(fp+tn)
    tpr = (tp)/(tp+fn)
    return ( &#39;2X2 混淆矩阵:&#39;, [&#39;TP&#39;, tp, &#39;FP&#39;, fp, &#39;FN&#39;, fn, &#39;TN&#39;, tn],
                &#39;准确度：&#39;, round(acc, 3),
                &#39;灵敏度/召回率：&#39;, round(sen, 3),
                &#39;特异性：&#39;，round(sp, 3),
                &#39;PPV/精度：&#39;, round(ppv, 3),
                &#39;净现值：&#39;，圆形（净现值，3），
                &#39;F1-分数：&#39;, round(f1, 3),
                &#39;误报率：&#39;, round(fpr, 3),
                &#39;真阳性率：&#39;, round(tpr, 3),
            ）

def auc_roc(y_true, y_pred_score):
    return (&#39;AUC-ROC:&#39;, round(roc_auc_score(y_true, y_pred_score), 3))

def avg_ precision(y_true, y_pred_score, target_name):
    return (&#39;平均精度:&#39;, round(average_ precision_score(y_true, y_pred_score, pos_label=target_name), 3))
    tpr = (tp)/(tp+fn)
    return ( &#39;2X2 混淆矩阵:&#39;, [&#39;TP&#39;, tp, &#39;FP&#39;, fp, &#39;FN&#39;, fn, &#39;TN&#39;, tn],
                &#39;准确度：&#39;, round(acc, 3),
                &#39;灵敏度/召回率：&#39;, round(sen, 3),
                &#39;特异性：&#39;，round(sp, 3),
                &#39;PPV/精度：&#39;, round(ppv, 3),
                &#39;净现值：&#39;，圆形（净现值，3），
                &#39;F1-分数：&#39;, round(f1, 3),
                &#39;误报率：&#39;, round(fpr, 3),
                &#39;真阳性率：&#39;, round(tpr, 3),
            ）

def auc_roc(self, y_true, y_pred_score):
    return (&#39;AUC-ROC:&#39;, round(roc_auc_score(y_true, y_pred_score), 3))

def avg_ precision(self, y_true, y_pred_score, target_name):
    return (&#39;平均精度:&#39;, round(average_ precision_score(y_true, y_pred_score, pos_label=target_name), 3))

当我使用它进行二元类分类时，它工作得很好 -
print(&#39;&gt;&gt; 自定义分类报告:\n&#39;, custom_classification_report(y_test, Predicted_labels), &#39;\n&#39;)

当我使用同一行代码 print(&#39;&gt;&gt; 自定义分类报告:\n&#39;, custom_classification_report(y_test, Predicted_labels), &#39;\n&#39;) 进行多类分类时，它给出错误 ValueError: 需要解包的值太多（预期为 4）。这是为什么，如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/63526955/how-to-obtain-specificity-and-negative-predictive-value-from-scikit-learn-for-mu</guid>
      <pubDate>Fri, 21 Aug 2020 16:48:48 GMT</pubDate>
    </item>
    <item>
      <title>R 错误中的 K 均值聚类</title>
      <link>https://stackoverflow.com/questions/46002289/k-means-clustering-in-r-error</link>
      <description><![CDATA[我有一个在 R 中创建的数据集。它的结构如下：

&lt;前&gt;&lt;代码&gt;&gt;头（btc_data）
           日期 btc_close eth_close vix_close gold_close DEXCHUS 更改
1647 2010-07-18 0.09 无 无 无 无 0
1648 2010-07-19 0.08 不适用 25.97 115.730 不适用 -1
1649 2010-07-20 0.07 不适用 23.93 116.650 不适用 -1
1650 2010-07-21 0.08 不适用 25.64 115.850 不适用 1
1651 2010-07-22 0.05 不适用 24.63 116.863 不适用 -1
1652 2010-07-23 0.06 不适用 23.47 116.090 不适用 1

我正在尝试使用 k 均值对观察结果进行聚类。但是，我收到以下错误消息：

&lt;前&gt;&lt;代码&gt;&gt; km &lt;- kmeans(trainingDS, 3)
do_one(nmeth) 中的错误：外部函数调用中的 NA/NaN/Inf (arg 1)
另外：警告消息：
在 storage.mode(x) &lt;- &quot;double&quot; 中：通过强制引入的 NA

这是什么意思？我对数据的预处理是否错误？我能做什么来修复它？我不能删除 NA，因为在 4500 个初始观察中，如果我运行完整案例，我只剩下 100 个观察。
基本上，我希望基于值为 -1,0,1 的 change 列形成 3 个簇。然后，我希望分析每个集群的组成部分，以找到最强的变化预测因素。还有哪些其他算法对此最有用？ 
我还尝试使用以下代码删除所有 NA 值，但仍然收到相同的错误消息：

&lt;前&gt;&lt;代码&gt;&gt; Complete_cases &lt;- btc_data[complete.cases(btc_data), ]
&gt; km &lt;- kmeans(complete_cases, 3, nstart = 20)
do_one(nmeth) 中的错误：外部函数调用中的 NA/NaN/Inf (arg 1)
另外：警告消息：
在 storage.mode(x) &lt;- &quot;double&quot; 中：通过强制引入的 NA

&gt; sum(!sapply(btc_data, is.finite))
[1]8008
&gt;总和（sapply（btc_data，is.nan））
[1] 0
&gt;
&gt; sum(!sapply(complete_cases, is.finite))
[1] 0
&gt;总和（sapply（complete_cases，is.nan））
[1] 0

这是数据的格式：

&lt;前&gt;&lt;代码&gt;&gt; sapply（btc_data，类）
      日期 btc_close eth_close vix_close gold_close DEXCHUS 更改
    “日期”“数字”“数字”“数字”“数字”“数字”“系数”
]]></description>
      <guid>https://stackoverflow.com/questions/46002289/k-means-clustering-in-r-error</guid>
      <pubDate>Fri, 01 Sep 2017 14:19:42 GMT</pubDate>
    </item>
    </channel>
</rss>