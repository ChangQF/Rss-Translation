<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 01 Oct 2024 01:22:17 GMT</lastBuildDate>
    <item>
      <title>徽标检测 - 图像分类[关闭]</title>
      <link>https://stackoverflow.com/questions/79040632/logo-detection-image-classification</link>
      <description><![CDATA[我一直在研究图像分类问题，以检查特定徽标图像是否存在于一组图像中。对于 X，数据集中确实存在许多表格，我已将其保存在文件夹“./dataset/my-label”中。
我还有另一个名为 ./test 的文件夹，其中存在许多图像，这些图像可能包含也可能不包含数据集中的图像。
为了检查这一点，我到目前为止已经尝试过 -

使用 https://huggingface.co/spaces/nathanjc/Logo_detection_YoloV7 作为参考，并尝试将其映射到此要求。但由于此代码检测并标记了所有可能的徽标，因此这对我的要求没有帮助。我只想要那些特定的徽标。我还希望能够将测试图像标记为“Logo”和“No Logo”。

使用 pytorch 从“./dataset/my-label”中训练了一个具有现有徽标图像集的模型，然后尝试进行预测。但由于训练数据集太小，准确度几乎可以忽略不计，不可接受。我还尝试将测试数据集中的许多徽标图像裁剪成训练图像。但这也没有产生正确的结果。此外，由于我的数据集是动态的，因此此选项将不可持续。


我可以选择使用一些付费服务，例如 Azure 或 GCP Vision API。但在此之前，我想评估可以构建的任何算法来解决此目的。
非常感谢您的建议/推荐。]]></description>
      <guid>https://stackoverflow.com/questions/79040632/logo-detection-image-classification</guid>
      <pubDate>Mon, 30 Sep 2024 18:27:06 GMT</pubDate>
    </item>
    <item>
      <title>使用随机搜索优化随机森林超参数：ranger 包的运行时间长且错误多</title>
      <link>https://stackoverflow.com/questions/79039614/optimizing-random-forest-hyperparameters-with-random-search-long-runtime-and-er</link>
      <description><![CDATA[我尝试使用“随机搜索”方法确定随机森林模型的参数 ntree、mtry 和 nodesize 的最优值。我还尝试使用交叉验证来评估每个组合的预测准确度并选择最佳超参数值。
我运行了模型，如下面的代码所示，但这个过程需要很长时间。我等了 4 个小时，但它仍然没有完成。我正在寻找专家建议，看看代码在当前状态下是否正确。我尝试使用 ranger 包，但遇到了错误“错误：调整参数网格应该有列 mtry、splitrule、min.node.size
&quot;。对于这个我想应用于连续数据的任务，您有更好的解决方案吗？
library(caret)
library(randomForest) 

# 将数据拆分为训练 (70%) 和验证 (30%) 集
set.seed(123)
train_index3 &lt;- createDataPartition(all_data3$LST, p = 0.7, list = FALSE)
train_data3 &lt;- all_data3[train_index3, ]
validation_data3 &lt;- all_data3[-train_index3, ]

# 设置使用随机搜索的交叉验证
train_control &lt;- trainControl(method = &quot;cv&quot;, number = 5, search = &quot;random&quot;)

# 创建要调整的超参数列表
ntree_values &lt;- seq(100, 500, by = 10)
nodesize_values &lt;- c(1:30)
mtry_values &lt;- c(1:30)

# 初始化数据框以存储结果
results &lt;- data.frame(ntree = numeric(), nodesize = numeric(), mtry = numeric(), 
R2 = numeric(), MAE = numeric())

# 手动执行随机搜索
for (ntree in ntree_values) {
for (nodesize in nodesize_values) {
for (mtry in mtry_values) {

# 为每个组合训练模型
set.seed(123)
rf_model &lt;- randomForest(
LST ~ .
data = train_data3,
method = &quot;rf&quot;,
mtry = mtry,
ntree = ntree,
nodesize = nodesize,
significance = TRUE
)

# 获取验证集上的预测
predictions &lt;- predict(rf_model, validation_data3)

# 计算性能指标 (R² 和 MAE)
R2 &lt;- caret::R2(predictions, validation_data3$LST)
MAE &lt;- caret::MAE(predictions, validation_data3$LST)

# 存储结果
results &lt;- rbind(results, data.frame(ntree = ntree, nodesize = nodesize, mtry = mtry, 
R2 = R2, MAE = MAE))
}
}
}

# 查看结果
print(results)

# 步骤 1：预测训练数据
train_predictions3 &lt;- predict(rf_model, newdata = train_data3[, -c(1:5)])

# 第 2 步：根据验证数据进行预测
validation_predictions3 &lt;- predict(rf_model, newdata = validation_data3[, -c(1:5)])
]]></description>
      <guid>https://stackoverflow.com/questions/79039614/optimizing-random-forest-hyperparameters-with-random-search-long-runtime-and-er</guid>
      <pubDate>Mon, 30 Sep 2024 13:37:08 GMT</pubDate>
    </item>
    <item>
      <title>用于压力检测的心电图分析</title>
      <link>https://stackoverflow.com/questions/79039477/ecg-analysis-for-stress-detection</link>
      <description><![CDATA[有人使用过数据集 SWELL 进行压力检测吗？您能帮我识别其中的各种参数列表吗？有人能告诉我数据集中与压力高度相关的最佳匹配特征吗？我应该使用回归分类进行压力预测吗？
我期待对数据集的准确解释]]></description>
      <guid>https://stackoverflow.com/questions/79039477/ecg-analysis-for-stress-detection</guid>
      <pubDate>Mon, 30 Sep 2024 13:02:42 GMT</pubDate>
    </item>
    <item>
      <title>了解 K 折交叉验证、模型训练和 R² 分数</title>
      <link>https://stackoverflow.com/questions/79039465/understanding-k-fold-cross-validation-model-training-and-r%c2%b2-scores</link>
      <description><![CDATA[我正在使用网格搜索设置中的 K 折交叉验证来调整超参数。我对模型的训练和评估方式有几个问题：

当我使用 GridSearchCV 时，模型会在多个折（假设为 10 个）上进行评估。对于每个超参数组合，模型会在 (K-1) 个折上进行训练，并在剩余的折上进行验证。当我在网格搜索后获得 best_grid 模型时，该模型是在哪些特定训练数据（即哪些折）上进行训练的？

当我调用 best_grid.predict(X_test) 时，该模型在哪个数据集上进行预测？它是否在网格搜索后在整个数据集上进行了训练，还是仍然基于交叉验证期间使用的折叠？

如果 best_grid 模型尚未在整个数据集上进行训练，我是否需要在进行预测之前再次明确地将其拟合到完整数据集？

我想获得 R² 训练分数，但我对使用以下代码时收到的分数感到困惑：
param_grid = {f&#39;regressor__regressor__{param}&#39;: values for param, values in model_info[&#39;params&#39;].items()}
grid_search = GridSearchCV(full_pipeline, param_grid, cv=stratified_kf.split(X, y_binned),scoring=&quot;r2&quot;, n_jobs=4, return_train_score=True)

grid_search.fit(X,y)

if grid_search.best_score_ &gt; best_score:
best_score = grid_search.best_score_
best_model = model_name
best_grid = grid_search

mean_train_score = best_grid.cv_results_[&#39;mean_train_score&#39;][best_grid.best_index_] #
print(mean_train_score) # 这里就是这个东西


]]></description>
      <guid>https://stackoverflow.com/questions/79039465/understanding-k-fold-cross-validation-model-training-and-r%c2%b2-scores</guid>
      <pubDate>Mon, 30 Sep 2024 12:59:27 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：在 Keras 模型中组合图像、掩码和 CSV 数据时输入形状无效</title>
      <link>https://stackoverflow.com/questions/79039003/valueerror-invalid-input-shape-when-combining-image-mask-and-csv-data-in-kera</link>
      <description><![CDATA[我正在使用 Keras 开发深度学习模型，尝试结合三种输入：图像、蒙版和 CSV 数据。我的模型的目标是根据医学图像（CT 扫描）和 CSV 数据预测脑出血的存在和类型。我使用 Attention U-Net 构建了模型，用于图像分割，并使用密集层进行 CSV 数据处理。
输入数据包括：

图像：灰度 CT 扫描，形状为 (256, 256, 1)。

蒙版：对应于分割的二进制蒙版，形状为 (256, 256, 1)。

CSV 数据：各种出血类型的数值数据，形状为 (6,)。


尝试使用 model.fit() 训练模型时遇到以下错误：
ValueError：输入 Tensor(&quot; functional_1/Cast:0&quot;, shape=(None, 6), dtype=float32) 的输入形状无效。预期形状为 (None, 256, 256, 1)，但输入具有不兼容的形状 (None, 6)
似乎具有形状 (None, 6) 的 CSV 数据被输入到模型中，其中预期图像数据（具有形状 (None, 256, 256, 1)）。我仔细检查了输入形状并使用 train_test_split 分割数据，但问题仍然存在。
这是我的代码的简化版本：
# 模型定义
inputs, output =tention_unet() # 图像的 UNet 模型

csv_input = layer.Input(shape=(6,), name=&#39;csv_input&#39;) # CSV 数据输入

mask_input = layer.Input(shape=(256, 256, 1), name=&#39;mask_input&#39;) # Mask 输入

# CSV 数据处理
csv_x = layer.Dense(64,activation=&#39;relu&#39;)(csv_input)csv_x = layer.Dense(32,activation=&#39;relu&#39;)(csv_x)

# 将 CSV 与 U-Net 输出相结合
flatten_outputs = layer.Flatten()(outputs)combined = layer.Concatenate()([flatten_outputs, csv_x])

# 最终输出
final_output = layer.Dense(1,activation=&#39;sigmoid&#39;,name=&#39;final_output&#39;)(combined)

# 模型编译
model = models.Model(inputs=[inputs, csv_input, mask_input],outputs=[outputs, final_output])model.compile(optimizer=Adam(),loss={&#39;final_output&#39;:&#39;binary_crossentropy&#39;,&#39;outputs&#39;:&#39;categorical_crossentropy&#39;},metrics=[&#39;accuracy&#39;,dice_coef,jaccard_index])

# 数据形状
print(f&quot;X_train 的形状：{X_train.shape}&quot;) # 图像的形状：(batch_size,256,256, 1)print(f&quot;csv_train 的形状：{csv_train.shape}&quot;) # CSV 的形状：(batch_size, 6)print(f&quot;y_train 的形状：{y_train.shape}&quot;) # mask 的形状：(batch_size, 256, 256, 1)

# 模型训练
model.fit({&#39;input_layer&#39;: X_train, &#39;csv_input&#39;: csv_train, &#39;mask_input&#39;: y_train},{&#39;final_output&#39;: csv_train, &#39;outputs&#39;: y_train},validation_data=({&#39;input_layer&#39;: X_val, &#39;csv_input&#39;: csv_val, &#39;mask_input&#39;: y_val},{&#39;final_output&#39;: csv_val, &#39;outputs&#39;: y_val})、epochs=50、batch_size=32、callbacks=callbacks)

我尝试过的步骤：

我打印了训练数据的形状，它们看起来都是正确的：

图像：(batch_size、256、256、1)
CSV：(batch_size、6)
掩码：(batch_size、256、256、1)


我检查了 model.fit() 中数据输入的顺序，以确保数据被传递到正确的层。

如何解决这个形状不匹配问题，并在模型中正确组合图像、掩码和 CSV 数据？]]></description>
      <guid>https://stackoverflow.com/questions/79039003/valueerror-invalid-input-shape-when-combining-image-mask-and-csv-data-in-kera</guid>
      <pubDate>Mon, 30 Sep 2024 10:44:55 GMT</pubDate>
    </item>
    <item>
      <title>如何减少过度拟合或者减少类别相似性的影响？</title>
      <link>https://stackoverflow.com/questions/79038878/how-to-reduce-overfitting-or-reduce-effect-from-class-similarity</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79038878/how-to-reduce-overfitting-or-reduce-effect-from-class-similarity</guid>
      <pubDate>Mon, 30 Sep 2024 10:10:00 GMT</pubDate>
    </item>
    <item>
      <title>如何加载预训练的 iresnet100 模型</title>
      <link>https://stackoverflow.com/questions/79038806/how-to-load-pretrained-iresnet100-model</link>
      <description><![CDATA[找到了这些代码行，但它们似乎不再起作用了，有人知道加载 iresnet100 模型的更新语法是什么吗？
上下文是我正在编辑使用 resnet50 预训练模型 model = models.resnet50(pretrained=True).cuda() 的现有 python 代码。我需要用 iresnet100 模型替换它
import insightface

embedder = insightface.iresnet100(pretrained=True)

AttributeError Traceback (most recent call last)
Cell In[28], line 2
1 import insightface
----&gt; 2 embedder = insightface.iresnet100(pretrained=True)
3 embedder.eval()

AttributeError: 模块“insightface”没有属性“iresnet100”

import torch
from insightface.model_zoo import get_model

embedder = get_model(&#39;buffalo_s&#39;, pretrained=True).cuda() # 使用适当的模型名称
embedder.eval() # 将模型设置为评估模式

试过了这些，但没用]]></description>
      <guid>https://stackoverflow.com/questions/79038806/how-to-load-pretrained-iresnet100-model</guid>
      <pubDate>Mon, 30 Sep 2024 09:48:03 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么输入来预测 rl 模型？它会被缩放还是被反转缩放？</title>
      <link>https://stackoverflow.com/questions/79038696/what-input-should-i-use-to-predict-rl-model-will-it-be-scaled-or-inv-scaled</link>
      <description><![CDATA[我正在使用 sb3 DQN 来训练股票数据，其中我的观察结果是最后 120 根蜡烛，具有 7 个特征，即开盘高低收盘时最小 rsi 等......。因此 obs 形状将是 (120,7)，输出将是离散的，具有 3 个 int 0、1、2（分别为持有、买入、卖出）。
我的问题是：

我只使用 minmaxscaler 缩放 obs，这是正确的吗？还是我需要缩放所有数据，在这种情况下，我有 200000 行 5 分钟蜡烛？


这是我自定义健身房环境中的 scale_data 乐趣
def scale_data(self,obs):
df1 = obs
df1[[&#39;OPEN&#39;, &#39;HIGH&#39;, &#39;LOW&#39;, &#39;CLOSE&#39;, &#39;rsi&#39;, &#39;TICKVOL&#39;]] = scaler.fit_transform(
obs[[&#39;OPEN&#39;, &#39;HIGH&#39;, &#39;LOW&#39;, &#39;CLOSE&#39;, &#39;rsi&#39;, &#39;TICKVOL&#39;]]) 

df1[[&#39;MINUTE&#39;,&#39;HOUR&#39;,&#39;DAY_OF_WEEK&#39;]] = obs[[&#39;MINUTE&#39;,&#39;HOUR&#39;,&#39;DAY_OF_WEEK&#39;]]

return df1.values



另一件事是，如果答案是缩放所有数据，那么在预测时要传递什么，因为在预测时我可能没有所有的数据，我想用最近的 120 个蜡烛数据进行预测。如果我们缩放所有数据并仅使用最后 120 个进行预测，那么缩放将完全不同！
]]></description>
      <guid>https://stackoverflow.com/questions/79038696/what-input-should-i-use-to-predict-rl-model-will-it-be-scaled-or-inv-scaled</guid>
      <pubDate>Mon, 30 Sep 2024 09:24:12 GMT</pubDate>
    </item>
    <item>
      <title>使用 NumPy 从头开始​​实现 AdaGrad 优化器的问题</title>
      <link>https://stackoverflow.com/questions/79037845/problem-implementing-adagrad-optimizer-from-scratch-with-numpy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79037845/problem-implementing-adagrad-optimizer-from-scratch-with-numpy</guid>
      <pubDate>Mon, 30 Sep 2024 03:42:07 GMT</pubDate>
    </item>
    <item>
      <title>XFormersMetadata.__init__() 收到意外的关键字参数“is_prompt”</title>
      <link>https://stackoverflow.com/questions/79036452/xformersmetadata-init-got-an-unexpected-keyword-argument-is-prompt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79036452/xformersmetadata-init-got-an-unexpected-keyword-argument-is-prompt</guid>
      <pubDate>Sun, 29 Sep 2024 13:03:20 GMT</pubDate>
    </item>
    <item>
      <title>如何在 pROC 中建立多因素模型？</title>
      <link>https://stackoverflow.com/questions/79036243/how-to-make-a-multifactor-model-in-proc</link>
      <description><![CDATA[我有一个数据表，其中包含响应“y”和一些预测变量，其中包括“X1”和“X2”。我可以使用 pROC 创建两个单因素模型：
roc1 &lt;- roc(data$y, data$X1)
roc2 &lt;- roc(data$y, data$X2)

但我正在尝试计算双因素模型的 ROC AUC：
t1 = data$X1
t2 = data$X2
t12 = cbind(t1, t2)
roc12 &lt;- roc(data$y, t12)

并收到一条错误消息：
响应和预测变量必须是长度相同的向量。

有没有办法在 pROC 中制作多因素模型？]]></description>
      <guid>https://stackoverflow.com/questions/79036243/how-to-make-a-multifactor-model-in-proc</guid>
      <pubDate>Sun, 29 Sep 2024 11:16:18 GMT</pubDate>
    </item>
    <item>
      <title>如何使用每个数据集在每次训练迭代中训练一个 LSTM 自动编码器？</title>
      <link>https://stackoverflow.com/questions/79035730/how-can-i-train-an-lstm-autoencoder-for-each-iteration-of-training-with-each-dat</link>
      <description><![CDATA[我一直在尝试构建和训练 LSTM 自动编码器。虽然我使用的参考仅训练了一次模型，但我添加了一个函数，如果每个数据集的每次训练迭代都结束，则多次运行训练。
不过，我并不确定我是否走在正确的轨道上。感觉我的代码在每次迭代中都有可能覆盖训练好的模型。
下面的 Python 代码是否真的会使用每个数据集对模型进行每次迭代训练（有 75 个 CSV 文件可用于训练此模型）？
以下是我在单个函数（trainModel()）中构建和训练模型的 Python 代码。
from sklearn.preprocessing import StandardScaler 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed
from tensorflow.keras.callbacks import EarlyStopping

# LSTM 网络以输入形状为（n_sample、n_timesteps、features）的等间隔子序列的形式接收输入。
# 我们将使用以下自定义函数来创建这些序列
def create_sequences(X, y, time_steps=1):
Xs, ys = [], []
for i in range(len(X) - time_steps):
v = X.iloc[i:(i + time_steps)].values
Xs.append(v)
ys.append(y.iloc[i + time_steps])
return np.array(Xs), np.array(ys)

def trainModel():
for i in range(75):
fileList = pd.read_csv(&quot;/content/drive/MyDrive/fileList.csv&quot;)
filename = fileList.iloc[i, 0]
temp = pd.read_csv(&quot;/content/drive/MyDrive/dataFolder/&quot;+filename+&quot;.csv&quot;)
train_size = int(len(temp[[&quot;time_abs(%Y-%m-%dT%H:%M:%S.%f)&quot;, &quot;velocity(m/s)&quot;]]))
train = df.iloc[0:train_size]

# 规范化数据
scalar = StandardScaler()
scalar = scalar.fit(train[[&#39;velocity(m/s)&#39;]])

train[&#39;velocity(m/s)&#39;] = scalar.transform(train[[&#39;velocity(m/s)&#39;]])

time_steps = 30

X_train, y_train = create_sequences(train[[&#39;velocity(m/s)&#39;]],train[&#39;velocity(m/s)&#39;],time_steps)

# 构建 LSTM 自动编码器

# 自动编码器是一种神经网络模型旨在学习输入的压缩表示。
# 它们使用监督学习方法进行训练，称为自监督。
# 在这种架构中，编码器 LSTM 模型逐步读取输入序列。
# 读取整个输入序列后，该模型的隐藏状态或输出表示
# 整个输入序列的内部学习表示为固定长度向量。
# 然后将该向量作为输入提供给解码器模型，解码器模型将其解释为输出序列中的每个步骤
# 生成。
# timesteps = X_train.shape[1]
num_features = X_train.shape[2]

model = Sequential()
model.add(LSTM(128,input_shape=(timesteps,num_features)))
model.add(Dropout(0.2))
model.add(RepeatVector(timesteps)) # 重复输入 n 次。
model.add(LSTM(128,return_sequences=True))
model.add(Dropout(0.2))
model.add(TimeDistributed(Dense(num_features))) # 将层应用于输入的每个时间片段。

model.compile(loss=&#39;mae&#39;,optimizer=&#39;adam&#39;)

# 训练自动编码器
early_stop = EarlyStopping(monitor=&#39;val_loss&#39;,patience=3,mode=&#39;min&#39;) # 如果监控指标相对于应用的 3 个时期的模式没有变化，则停止训练
history = model.fit(X_train,y_train,epochs=100,batch_size=32,validation_split=0.1,callbacks=[early_stop],shuffle=False)

model.save(&#39;anomaly_model.h5&#39;, overwrite=False)
model.save(&#39;anomaly_model_&#39;+ i +&#39;.h5&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/79035730/how-can-i-train-an-lstm-autoencoder-for-each-iteration-of-training-with-each-dat</guid>
      <pubDate>Sun, 29 Sep 2024 06:28:59 GMT</pubDate>
    </item>
    <item>
      <title>将基于 Bert 的 PyTorch 模型导出到 CoreML。如何让 CoreML 模型适用于任何输入？</title>
      <link>https://stackoverflow.com/questions/78704542/exporting-a-bert-based-pytorch-model-to-coreml-how-can-i-make-the-coreml-model</link>
      <description><![CDATA[我使用以下代码将基于 Bert 的 PyTorch 模型导出到 CoreML。
由于我使用
dummy_input = tokenizer(&quot;A French fan&quot;, return_tensors=&quot;pt&quot;)

在 macOS 上测试时，CoreML 模型仅适用于该输入。如何让 CoreML 模型适用于任何输入（即任何文本）？

导出脚本：
# -*- coding: utf-8 -*-
&quot;&quot;&quot;Core ML Export
pip install tr​​ansformers torch coremltools nltk
&quot;&quot;&quot;
导入 os
从 transformers 导入 AutoModelForTokenClassification、AutoTokenizer
导入 torch
导入 torch.nn 作为 nn
导入 nltk
导入 coremltools 作为 ct
nltk.download(&#39;punkt&#39;)
# 加载模型和 tokenizer
model_path = os.path.join(&#39;model&#39;)
model = AutoModelForTokenClassification.from_pretrained(model_path, local_files_only=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
# 修改模型的 forward 方法以返回元组
class ModifiedModel(nn.Module):
def __init__(self, model):
super(ModifiedModel, self).__init__()
self.model = model
self.device = model.device # 添加设备属性

def forward(self, input_ids,tention_mask, token_type_ids=None):
outputs = self.model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)
returnoutputs.logits

modified_model = ModifiedModel(model)

# 导出到 Core ML
def convert_to_coreml(model, tokenizer):
# 定义用于跟踪的虚拟输入
dummy_input = tokenizer(&quot;A French fan&quot;, return_tensors=&quot;pt&quot;)
dummy_input = {k: v.to(model.device) for k, v in dummy_input.items()}

# 使用虚拟输入跟踪模型
traced_model = torch.jit.trace(model,(
dummy_input[&#39;input_ids&#39;],dummy_input[&#39;attention_mask&#39;], dummy_input.get(&#39;token_type_ids&#39;)))

# 转换为 Core ML
输入 = [
ct.TensorType(name=&quot;input_ids&quot;, shape=dummy_input[&#39;input_ids&#39;].shape),
ct.TensorType(name=&quot;attention_mask&quot;, shape=dummy_input[&#39;attention_mask&#39;].shape)
]
if &#39;token_type_ids&#39; in dummy_input:
输入.append(ct.TensorType(name=&quot;token_type_ids&quot;, shape=dummy_input[&#39;token_type_ids&#39;].shape))

mlmodel = ct.convert(traced_model, 输入=inputs)

# 保存 Core ML 模型
mlmodel.save(&quot;model.mlmodel&quot;)
print(&quot;模型导出到 Core ML成功&quot;)

convert_to_coreml(modified_model, tokenizer)

要使用导出的模型：
import os
from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch
import torch.nn as nn
import nltk
import coremltools as ct
from coremltools.models import MLModel
import numpy as np
from transformers import AutoTokenizer
import nltk

nltk.download(&#39;punkt&#39;)

# 加载 Core ML 模型
model = MLModel(&#39;model.mlmodel&#39;)

# 加载 tokenizer
model_path = &#39;model&#39;
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)

def prepare_input(text, tokenizer):
tokens = nltk.tokenize.word_tokenize(text)
tokenized_inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=&quot;np&quot;)
input_ids = tokenized_inputs[&#39;input_ids&#39;].astype(np.int32)
tention_mask = tokenized_inputs[&#39;attention_mask&#39;].astype(np.int32)

input_data = {
&#39;input_ids&#39;: input_ids,
&#39;attention_mask&#39;:tention_mask
}

if &#39;token_type_ids&#39; in tokenized_inputs:
input_data[&#39;token_type_ids&#39;] = tokenized_inputs[&#39;token_type_ids&#39;].astype(np.int32)

return input_data, tokens

def predict(text):
# 准备输入
input_data, tokens = prepare_input(text, tokenizer)

# 进行预测
prediction = model.predict(input_data)

# 提取预测标签
logits = prediction[&#39;output&#39;] # 根据模型的输出调整此键
predicted_label = np.argmax(logits, axis=-1)[0]

# 显示结果
for word, label in zip(tokens, predicted_label):
print(f&quot;{word}: {model.model_description.outputDescriptions[0].dictionaryType.int64KeyType.stringDictionary[label]}&quot;)

# 用一个句子测试模型
predict(&quot;A French fan&quot;)

该脚本仅适用于示例“A French Fan”。当我尝试另一个示例 predict(&quot;A football fan is standing in the stadium.&quot;) 时，它会触发错误：
NSLocalizedDescription = &quot;MultiArray shape (1 x 12) does not match the shape (1 x 5) specified in the model description&quot;;


环境：

导出脚本：在 Ubuntu 20.04 上测试了 Python 3.10 和 torch 2.3.1（在 Windows 10 上不起作用）。
预测脚本：必须在 macOS 10.13+ 上运行，因为 CoreML 模型仅支持在 macOS 10.13+ 上进行预测。
]]></description>
      <guid>https://stackoverflow.com/questions/78704542/exporting-a-bert-based-pytorch-model-to-coreml-how-can-i-make-the-coreml-model</guid>
      <pubDate>Wed, 03 Jul 2024 23:39:36 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 进行迁移学习进行图像分类</title>
      <link>https://stackoverflow.com/questions/78401636/transfer-learning-using-keras-for-image-classification</link>
      <description><![CDATA[我正在尝试使用已经训练过的模型将学习转移到我将要创建的模型中，并且只修改最后几层。这样做的目的是使用已经训练过的模型（已经在数百万张图像上训练过）来帮助我的模型对食物项目识别进行分类。我对 Keras 还很陌生，我遇到了一个问题，我现在开始理解它，但不知道如何解决
# 从 TensorFlow Hub 加载模型
model_url = &quot;https://www.kaggle.com/models/tensorflow/resnet-50/TensorFlow2/classification/1&quot;
hub_layer = hub.KerasLayer(model_url, input_shape=(224, 224, 3))

# 创建 Sequential 模型
model = tf.keras.Sequential()

# 将 TensorFlow Hub 层添加到 Sequential 模型
model.add(hub_layer)

# 构建 Sequential 模型
model.build((None, 224, 224, 3))

# 模型摘要
model.summary()

错误：
-------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[56]，第 9 行
6 model = tf.keras.Sequential()
8 # 将 TensorFlow Hub 层添加到 Sequential 模型
----&gt; 9 model.add(hub_layer)
11 # 构建 Sequential 模型
12 model.build((None, 224, 224, 3))

文件 c:\Users\Karim\AppData\Local\Programs\Python\Python312\Lib\site-packages\keras\src\models\sequential.py:95，位于 Sequential.add(self, layer, rebuild)
93 layer = origin_layer
94 if not isinstance(layer, Layer):
---&gt; 95 raise ValueError(
96 &quot;只有 `keras.Layer` 的实例可以 &quot;
97 f&quot;添加到 Sequential 模型中。收到：{layer} &quot;
98 f&quot;（类型为 {type(layer)}）&quot;
99 )
100 if not self._is_layer_name_unique(layer):
101 raise ValueError(
102 &quot;添加到 Sequential 模型的所有层 &quot;
103 f&quot;应具有唯一名称。名称 &#39;{layer.name}&#39; 已经是 &quot;
104 &quot;此模型中层的名称。更新 `name` 参数 &quot;
105 &quot;以传递唯一名称。&quot;
106 )

ValueError：只有 `keras.Layer` 的实例可以添加到 Sequential 模型中。已收到：&lt;tensorflow_hub.keras_layer.KerasLayer 对象位于 0x00000190C6B8AD20&gt;（类型为 &lt;class &#39;tensorflow_hub.keras_layer.KerasLayer&#39;&gt;）
]]></description>
      <guid>https://stackoverflow.com/questions/78401636/transfer-learning-using-keras-for-image-classification</guid>
      <pubDate>Mon, 29 Apr 2024 09:15:19 GMT</pubDate>
    </item>
    <item>
      <title>从 K 折交叉验证中选择哪个模型</title>
      <link>https://stackoverflow.com/questions/45480894/which-model-to-pick-from-k-fold-cross-validation</link>
      <description><![CDATA[我读过关于交叉验证的文章，以及它如何用于选择最佳模型和估计参数，但我并不真正理解它的含义。
假设我建立一个线性回归模型并进行 10 倍交叉验证，我认为这 10 个模型中的每一个都会有不同的系数值，现在我应该从 10 个不同的模型中选择哪一个作为我的最终模型或估计参数。
或者我们使用交叉验证只是为了找到平均误差（在我们的例子中是 10 个模型的平均值）并与另一个模型进行比较？]]></description>
      <guid>https://stackoverflow.com/questions/45480894/which-model-to-pick-from-k-fold-cross-validation</guid>
      <pubDate>Thu, 03 Aug 2017 09:59:13 GMT</pubDate>
    </item>
    </channel>
</rss>