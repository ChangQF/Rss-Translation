<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 05 Nov 2024 06:24:19 GMT</lastBuildDate>
    <item>
      <title>GAN 图像生成给出疯狂的输出</title>
      <link>https://stackoverflow.com/questions/79157642/gan-image-generation-gives-crazy-output</link>
      <description><![CDATA[我正在尝试使用 GAN 创建一个生成器网络，该网络可以将 32x32 图像转换为 128x128 图像。我最初训练了一个 CNN，并取得了一些成功：

为了获得更准确的结果，我使用了一种 GAN 架构，该架构为生成器网络实现了更低的损失分数。然而，当我尝试进行预测时，我收到的图像如下所示：

似乎生成器网络正在寻找某种方法来打破鉴别器网络。我该如何解决这个问题，并找到一种让生成器真正生成高清图像的方法？
这是我的 GAN 的样子：
generator = UpsamplingCNN().to(device)
discriminator = Discriminator().to(device)

criterion = nn.BCELoss()

## 不同的学习率
optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))

num_generator_updates = 5
lambda_gp = 10
num_epochs = 50 
## 标签平滑
real_label = 0.9
fake_label = 0.0

for epoch in range(num_epochs):
generator.train()
discriminator.train()

for i, (low_res, high_res) in enumerate(tqdm(train_loader)):
# 将数据移动到正确的设备
low_res = low_res.to(device)
high_res = high_res.to(device)

# 为真实图像和虚假图像添加噪声
noise_std_dev = 0.05
noisy_real_images = high_res + noise_std_dev * torch.randn_like(high_res).to(device)
noisy_real_images = torch.clamp(noisy_real_images, 0.0, 1.0)

fake_images = generator(low_res)
noisy_fake_images = fake_images + noise_std_dev * torch.randn_like(fake_images).to(device)
noisy_fake_images = torch.clamp(noisy_fake_images, 0.0, 1.0)

# 真标签和假标签
output_real = discriminator(noisy_real_images).view(-1)
labels_real = torch.full((output_real.size(0),), real_label, dtype=torch.float, device=device)
loss_real = criterion(output_real, labels_real)

# 使用嘈杂的假图像进行训练
output_fake = discriminator(noisy_fake_images.detach()).view(-1)
labels_fake = torch.full((output_fake.size(0),), fake_label, dtype=torch.float, device=device)
loss_fake = criterion(output_fake, labels_fake)

# 计算梯度惩罚
gradient_penalty = compute_gradient_penalty(discriminator, high_res, fake_images, device)

# 带梯度惩罚的鉴别器总损失
loss_D = loss_real + loss_fake + lambda_gp * gradient_penalty
loss_D.backward()

optimizer_D.step()

## 多个生成器更新
for _ in range(num_generator_updates):
optimizer_G.zero_grad()

# 生成假图像并计算生成器的损失
fake_images = generator(low_res)
output_fake_for_G = discriminator(fake_images).view(-1)
labels_fake_for_G = torch.full((output_fake_for_G.size(0),),real_label, dtype=torch.float, device=device)
loss_G = criterion(output_fake_for_G, labels_fake_for_G)
loss_G.backward()

optimizer_G.step()

if i % 10 == 0:
print(f&#39;Epoch [{epoch+1}/{num_epochs}], Batch [{i}/{len(train_loader)}], &#39;
f&#39;Loss_D: {loss_real.item() + loss_fake.item():.4f}, Loss_G: {loss_G.item():.4f}&#39;)

我还对判别器使用了梯度惩罚，如下所示：
def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):
alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)
interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)
d_interpolates = discriminator(interpolates)
fake = torch.ones(d_interpolates.size(), device=device, require_grad=False)

gradients = torch.autograd.grad(
output=d_interpolates,
input=interpolates,
grad_outputs=fake,
create_graph=True,
retain_graph=True,
only_inputs=True
)[0]

gradients = gradients.view(gradients.size(0), -1)
gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
return gradient_penalty
]]></description>
      <guid>https://stackoverflow.com/questions/79157642/gan-image-generation-gives-crazy-output</guid>
      <pubDate>Tue, 05 Nov 2024 03:55:19 GMT</pubDate>
    </item>
    <item>
      <title>将位图转换为黑白以进行 Google MLKit 文本识别是否更好</title>
      <link>https://stackoverflow.com/questions/79157550/is-it-better-to-convert-a-bitmap-to-black-and-white-for-for-google-mlkit-text-re</link>
      <description><![CDATA[我正在使用 Google MLKit 进行文本识别，效果很好，但似乎很难识别彩色图像中的文本。我想知道是否最好将图像转换为灰度，或者在使用 ML 阅读器扫描之前我可以进行任何其他优化。似乎 Google 的 Google 镜头比普通的 ML 套件阅读器效果更好，所以他们一定在做其他事情。]]></description>
      <guid>https://stackoverflow.com/questions/79157550/is-it-better-to-convert-a-bitmap-to-black-and-white-for-for-google-mlkit-text-re</guid>
      <pubDate>Tue, 05 Nov 2024 02:58:20 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中如何处理重叠数据</title>
      <link>https://stackoverflow.com/questions/79157457/how-to-deal-with-overlapping-data-in-machine-learning</link>
      <description><![CDATA[
我正在创建一个机器学习模型来确定用户是否是机器人，我使用 seaborn 绘制了配对图并意识到大多数数据是重叠的。下面是我为标准化、拆分和部署模型编写的代码。该图显示了该模型在超过 40000 个样本的情况下的表现。如您所见，模型正在猜测，我正在尝试找出原因。


 X = new_df[[&#39;Retweet Count&#39;, &#39;Mention Count&#39;, &#39;Follower Count&#39;, &#39;Tweet&#39;, &#39;Hashtags&#39;, &#39;Verified&#39;, &#39;Created At&#39;]]
y = new_df[[&#39;Bot Label&#39;]].values

y = y.ravel() # 确保 y 是一维数组而不是二维数组

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

Scaler = StandardScaler()
X_train_scaled = Scaler.fit_transform(X_train)
X_test_scaled = Scaler.transform(X_test)

从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.datasets 导入 make_classification
从 sklearn.metrics 导入 Confusion_matrix

rfc = RandomForestClassifier(n_estimators = 1000)
rfc.fit(X_train_scaled, y_train)
y_pred = rfc.predict(X_test_scaled)
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

]]></description>
      <guid>https://stackoverflow.com/questions/79157457/how-to-deal-with-overlapping-data-in-machine-learning</guid>
      <pubDate>Tue, 05 Nov 2024 01:41:31 GMT</pubDate>
    </item>
    <item>
      <title>行业 RAG 技术 [关闭]</title>
      <link>https://stackoverflow.com/questions/79157397/industries-rag-technology</link>
      <description><![CDATA[RAG 技术如何通过检索相关医学研究并生成量身定制的治疗建议来增强患者护理？
RAG 技术如何通过检索相关医学研究并生成量身定制的治疗建议来增强患者护理？在临床环境中可以实施哪些具体用例来改善决策？]]></description>
      <guid>https://stackoverflow.com/questions/79157397/industries-rag-technology</guid>
      <pubDate>Tue, 05 Nov 2024 00:54:23 GMT</pubDate>
    </item>
    <item>
      <title>我的自定义 layernorm 函数有什么问题？</title>
      <link>https://stackoverflow.com/questions/79157138/whats-the-problems-in-my-custom-layernorm-function</link>
      <description><![CDATA[import numpy as np
import torch
import torch.nn. functional as F

def layer_norm(x, weight, bias, eps=1e-6):
# x 形状：[bs, h, w, c]
# 计算空间维度（高度、宽度）的平均值和方差
mean = np.mean(x, axis=(1, 2), keepdims=True) # 形状：(batch_size, 1, 1, channels)
var = np.var(x, axis=(1, 2), keepdims=True, ddof=0) # 使用 ddof=0 表示有偏方差

# 标准化
x_normalized = (x - mean) / np.sqrt(var + eps)

# 应用权重和偏差
out = weight[None, None, None, :] * x_normalized + bias[None, None, None, :]
return out

def test1(x):
x = np.transpose(x, (0, 2, 3, 1)) # 转置为 [bs, h, w, c]
weight = np.ones(channels)
bias = np.zeros(channels)

normalized_output = layer_norm(x, weight, bias)
return normalized_output

def test2(x):
global channels
x = np.transpose(x, (0, 2, 3, 1)) # 转置为 [bs, h, w, c]
x_tensor = torch.tensor(x, dtype=torch.float32)
weight = torch.ones(channels)
bias = torch.zeros(channels)

# 使用 PyTorch 的层规范，在最后一个维度（通道）上进行规范化
normalized_output = F.layer_norm(x_tensor, normalized_shape=(channels,), weight=weight, bias=bias)
return normalized_output.detach().numpy()

# 测试
batch, channels, height, width = 4, 3, 8, 8
# 生成随机输入
x = np.random.randint(-10, 10, (batch, channels, height, width))

# 计算两种实现的输出
layernorm1 = test1(x)
layernorm2 = test2(x)

# 检查输出是否接近
are_close = np.allclose(layernorm1, layernorm2, atol=1e-4)
print(&quot;Outputs are close:&quot;, are_close) # 如果它们足够接近，则应输出 True

var = np.var(x, axis=(1, 2), keepdims=True, ddof=0) # 使用 ddof=0对于有偏方差
var = np.var(x, axis=(1, 2), keepdims=True)

我的期望是 are_close==True,，这意味着 layernorm1 和 layernorm2 的距离非常小。由于 layernorm1 和 layernorm2 的形状很大，所以我会显示 layernorm1 和 layernorm2 的部分结果。 layernorm1[0,0,0:3,0:4] 数组([[ 0.35208505, 1.06448374, -0.52827179], [-1.6216472 , -1.7376534 , -1.07653225], [-1.12821414, 0.88935017, 1 .84752351]]) layernorm2[0,0,0:3,0:4] 数组([[ 0.07412489, 1.1859984 , -1.2601235 ], [-1.0690411 , -0.2672601 , 1.336302 ], [-1.3920445 , 0.4800153 , 0.9120291 ]], dtype=float32)
我尝试了带或不带 ddof=0 的 variacne 方法，打印语句中全部为 False。
我想知道如何实现类似于 Pytorch 内置 layernorm 函数的自定义 layernorm。
从代码角度看 layernorm 步骤是什么？
关于计算机视觉，layernorm 对特征图有什么作用？]]></description>
      <guid>https://stackoverflow.com/questions/79157138/whats-the-problems-in-my-custom-layernorm-function</guid>
      <pubDate>Mon, 04 Nov 2024 22:07:24 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Android Studio 制作一个仅扫描姓名和身份证号码的身份证扫描器？[关闭]</title>
      <link>https://stackoverflow.com/questions/79155848/how-will-i-make-an-id-scanner-that-will-scan-the-name-and-id-number-only-using-a</link>
      <description><![CDATA[我是一名学生，这将是我论文的系统。我们将实施一个基于移动设备的 OCR 扫描仪系统，该系统只能读取残障人士和老年人的身份证。我对如何做到这一点有一些想法，但我认为我需要更多的输入和逻辑。我正在考虑使用 Google ML Kit 进行机器学习和文本识别，并使用 TensorFlow 进行身份证的对象检测。
我的问题是，我国每个城市的残障人士和老年人身份证的格式和外观都不同，但我们没有时间收集所有这些信息。除了在对象检测中添加关键字（例如“老年人”、“残疾”等）以启动扫描仪外，是否有可能创建一个可以识别残障人士和老年人身份证的系统，而无需收集我国每种类型的老年人和残障人士身份证来开发算法？]]></description>
      <guid>https://stackoverflow.com/questions/79155848/how-will-i-make-an-id-scanner-that-will-scan-the-name-and-id-number-only-using-a</guid>
      <pubDate>Mon, 04 Nov 2024 14:34:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 优化多玩家对数线性学习的实现</title>
      <link>https://stackoverflow.com/questions/79155493/optimising-log-linear-learning-implementation-for-multiple-players-in-python</link>
      <description><![CDATA[我正在为潜在游戏类别测试对数线性学习，我想用两个以上的玩家来测试它。我在优化代码时遇到了问题。
目前，我正在利用对数线性学习会引发马尔可夫链这一事实，并且转换可以用转换矩阵来解释。然而，随着玩家数量的增加，转换矩阵的维度迅速增加，使得无法使用密集的 numpy 数组来存储转换矩阵。我尝试过使用 scipy.sparse 矩阵，但计算速度非常慢 - 矩阵的使用方式是将平稳分布与转换矩阵相乘。
我想使测试多个玩家成为可能，并优化代码的计算时间。
这是使用 numpy 的转换矩阵的公式。
self.action_profiles = enumerate(np.array(list(product(np.arange(self.no_actions), repeat = self.no_players))))

self.potential = np.zeros((self.no_action_profiles, 1))

P = np.zeros([self.no_action_profiles, self.no_action_profiles]) 

for idx, profile in self.action_profiles:

self.potential[idx] = self.potential_function(profile)

for player_id in range(self.no_players):

mask = np.arange(len(profile)) != player_id
opposites_actions = profile[mask] # 从动作配置文件中提取对手的动作

utility = np.array([self.utility_functions[player_id](i, opposites_actions) for i in range(self.no_actions)])
exp_values = np.exp(beta * utility)

p = exp_values/np.sum(exp_values)

i = idx - profile[player_id]*self.no_actions**(self.no_players - 1 - player_id)
stride = self.no_actions ** (self.no_players - 1 - player_id)

P[idx, i: i + self.no_actions**(self.no_players - player_id) : stride] += 1/self.no_players*p

self.P = P

并使用 scipy。
self.action_profiles = enumerate(np.array(list(product(np.arange(self.no_actions), repeat = self.no_players))))

self.potential = lil_matrix((self.no_action_profiles, 1))

# P = np.zeros([self.no_action_profiles, self.no_action_profiles]) 

P_row, P_col, P_data = [], [], []

for idx, profile in self.action_profiles:

self.potential[idx] = self.potential_function(profile)

for player_id in range(self.no_players):

mask = np.arange(len(profile)) != player_id
opposites_actions = profile[mask] # 从动作配置文件中提取对手的动作

utility = np.array([self.utility_functions[player_id](i, opposites_actions) for i in range(self.no_actions)])
exp_values = np.exp(beta * utility)

p = exp_values/np.sum(exp_values)

i = idx - profile[player_id]*self.no_actions**(self.no_players - 1 - player_id)
stride = self.no_actions ** (self.no_players - 1 - player_id)

for j, prob in enumerate(p):
P_row.append(idx)
P_col.append(i + j * stride)
P_data.append(prob / self.no_players) 

P = coo_matrix((P_data, (P_row, P_col)), shape=(self.no_action_profiles, self.no_action_profiles))

self.P = P.tocsr()

return self.P

它们的用法如下。
P = self.gameSetup.formulate_transition_matrix(beta)
mu0 = self.mu_matrix.copy()

self.expected_value = np.zeros((int(self.max_iter), 1))

P = np.linalg.matrix_power(P, scale_factor)

for i in range(self.max_iter):

mu = mu0 @ P

mu0 = mu

self.expected_value[i] = mu @ self.gameSetup.potential

self.expected_value = self.expected_value
self.stationary = mu
]]></description>
      <guid>https://stackoverflow.com/questions/79155493/optimising-log-linear-learning-implementation-for-multiple-players-in-python</guid>
      <pubDate>Mon, 04 Nov 2024 12:37:29 GMT</pubDate>
    </item>
    <item>
      <title>如何从背景噪音和其他非吉他声音中识别乐器声音？[关闭]</title>
      <link>https://stackoverflow.com/questions/79155281/how-to-identify-instrument-sound-from-background-noise-and-other-non-guitar-soun</link>
      <description><![CDATA[我准备开发一款吉他学习应用，用户可以在其中弹奏自己选择的歌曲。该应用可以判断他们弹奏的音符是否正确，以及是否按时弹奏。弹奏后，应用将根据用户的表现为其打分。一个问题是输入的音频可能很嘈杂，并且可能包含非吉他声音（例如，当你正在练习时，你的妈妈突然对你说“晚餐准备好了”，或者同一个房间里有一个人在弹钢琴）。这可能会欺骗音高检测库，使其认为用户弹奏了一个音符，但实际上他们并没有弹奏。
我在 Google 上搜索了音高检测，找到了一些基于算法的库，如 YIN，但我找不到可以从其他推断中识别乐器声音的库。这就是我创建这篇文章的原因。]]></description>
      <guid>https://stackoverflow.com/questions/79155281/how-to-identify-instrument-sound-from-background-noise-and-other-non-guitar-soun</guid>
      <pubDate>Mon, 04 Nov 2024 11:32:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们有时在机器学习中将数据加倍？[关闭]</title>
      <link>https://stackoverflow.com/questions/79154883/why-we-sometimes-double-data-in-machine-learning</link>
      <description><![CDATA[有人能解释一下为什么我需要像这样复制 train_x 和 train_y 吗？当我仅应用一次 pd.concat（例如 train_x = train_x）时，我的模型的性能似乎显著下降。我试图理解为什么与单独使用原始数据集相比，将数据加倍（使用 train_x 和 train_y 两次）可以改善我的结果。
train_x = pd.concat(\[train_x, train_x\])
train_y = pd.concat(\[train_y, train_y\])

我尝试保持 train_x 和 train_y 原样，但这种方法并没有产生好的结果。您能解释一下为什么加倍数据有帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/79154883/why-we-sometimes-double-data-in-machine-learning</guid>
      <pubDate>Mon, 04 Nov 2024 09:32:56 GMT</pubDate>
    </item>
    <item>
      <title>替代已弃用的 tensorflow.keras.preprocessing.image.ImageDataGenerator 的新库是什么？</title>
      <link>https://stackoverflow.com/questions/79154443/what-is-the-new-library-in-place-of-deprecated-tensorflow-keras-preprocessing-im</link>
      <description><![CDATA[ImageDataGenerator 库已弃用。
dataGen= ImageDataGenerator(width_shift_range=0.1, # 0.1 = 10% 如果大于 1 例如 10 那么它指的是像素数 例如 10 个像素
height_shift_range=0.1,
zoom_range=0.2, # 0.2 意味着可以从 0.8 到 1.2
sher_range=0.1, # 剪切角的大小
rotation_range=10) # 度
dataGen.fit(X_train)
batches= dataGen.flow(X_train,y_train,batch_size=20) # 请求数据生成器生成图像批次大小 = 否。每次调用时创建的图像数量
X_batch,y_batch = next(batches)

我遇到了这段代码，必须根据新库重写。有人能告诉我正确的库吗？如果可能的话，可以用新库重写这段代码吗？]]></description>
      <guid>https://stackoverflow.com/questions/79154443/what-is-the-new-library-in-place-of-deprecated-tensorflow-keras-preprocessing-im</guid>
      <pubDate>Mon, 04 Nov 2024 06:45:20 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 中的眼睛标志模型无法跟踪看不见的数据</title>
      <link>https://stackoverflow.com/questions/79149930/eye-landmark-model-in-tensorflow-not-tracking-with-unseen-data</link>
      <description><![CDATA[我一直在使用 UTKFace 数据集创建一个模型，该模型将绘制图像中眼睛周围的点。我使用的训练集大小约为 40000 张图像，包含缩放、旋转和平移的增强图像。它与训练集和验证集配合得很好。但当我用该集中的新增强图像测试它时，它不起作用。我尝试过稍微调整一些参数和训练时间，但结果并没有发生很好的变化。
我的层目前看起来像这样：


inputs = tf.keras.layers.Input(shape=(200, 200, 1))
x = tf.keras.layers.Conv2D(32, kernel_size=(5, 5))(inputs)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)

x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3))(x)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)

x = tf.keras.layers.Conv2D(128, kernel_size=(3, 3))(x)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)

x = tf.keras.layers.Conv2D(256, kernel_size=(3, 3))(x)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)

x = tf.keras.layers.Conv2D(512, kernel_size=(3, 3))(x)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)

# 线性层。
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dense(units=256, kernel_regularizer=tf.keras.regularizers.l2(0.1))(x)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.Dense(units=24)(x)




这是训练后验证集的样例输出，这就是我想要的结果。

这是来自验证集的图像。这也是正确的。

这是同一幅图像，但略有增强，但眼睛没有被跟踪。

如果您能提供任何关于如何改善结果的建议，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/79149930/eye-landmark-model-in-tensorflow-not-tracking-with-unseen-data</guid>
      <pubDate>Sat, 02 Nov 2024 03:49:47 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 Candle 从 NV-Embed 获取嵌入？</title>
      <link>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</link>
      <description><![CDATA[我想要做的是一个输出任意输入的嵌入的 CLI 程序。
为此，我想使用嵌入模型进行推理，我选择了 NV-Embed-v2。我选择的框架是 Candle，但我也查看了 Mistral-RS。
基本上，我想做的是这个代码片段：
https://huggingface.co/nvidia/NV-Embed-v2
但使用 Rust 和 Candle。
我尝试从 Mistral Candle 的示例，因为 NV-Embed 的 HF 页面显示：模型详细信息/仅基础解码器 LLM：Mistral-7B-v0.1。
我将原始代码中的模型 ID 替换为 nvidia/NV-Embed-v2，并能够从 Hugging Face 下载权重，但在加载配置时，我得到了这个：
错误：缺少第 101 行第 1 列的字段“vocab_size”

然后我将从 HF 加载的 JSON 配置中的值硬编码到新创建的 candle_transformers::models::mistral::Config 实例中。之后，Mistral::new(&amp;config, vb) 失败，并显示：
错误：找不到张量 model.embed_tokens.weight

有没有办法解决这个问题 — 也许还有其他一些基于 Candle 的开源作品可以作为我的灵感？或者，也许这是一个很容易诊断的常见错误？]]></description>
      <guid>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</guid>
      <pubDate>Thu, 31 Oct 2024 15:55:49 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“datachain.lib”的模块；“datachain”不是一个包</title>
      <link>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</link>
      <description><![CDATA[
为什么我会遇到 datachain.lib 模块的 ModuleNotFoundError？
我需要采取其他步骤才能在项目中正确使用 datachain 包吗？

我正在开发一个 Python 项目，在尝试导入模块时遇到以下错误：
import os
os.environ[&quot;PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION&quot;] = &quot;python&quot;
import tensorflow as tf
import numpy as np
from PIL import Image
from datachain.lib.dc import Column, DataChain

错误消息：
ModuleNotFoundError：没有名为“datachain.lib”的模块； &#39;datachain&#39; 不是包

详细信息：

我已使用 pip 安装了 datachain：pip install datachain。
通过运行 pip list 可看到 datachain 的安装版本为 0.2.18。
我已验证包已正确安装并位于我的 Python 环境中。
]]></description>
      <guid>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</guid>
      <pubDate>Wed, 07 Aug 2024 09:53:07 GMT</pubDate>
    </item>
    <item>
      <title>将 .ckpt 转换为 .h5</title>
      <link>https://stackoverflow.com/questions/74640695/convert-ckpt-to-h5</link>
      <description><![CDATA[我已经使用 resnet18 训练模型进行 mask R-CNN 检测。对于每个 epoch，它只创建一个“.ckpt”文件。
现在我想使用该 .ckpt 文件作为检测图像的检测器。我有使用“.h5”文件进行检测的 Python 代码。
请帮助我如何使用“.ckpt”文件进行检测。或者我如何将其转换为“.h5”？
谢谢
我曾尝试在训练过程中生成“.h5”文件而不是“.ckpt”，但对我来说不起作用。
现在我需要一种方法来使用“.ckpt”文件来检测图像中的对象。]]></description>
      <guid>https://stackoverflow.com/questions/74640695/convert-ckpt-to-h5</guid>
      <pubDate>Thu, 01 Dec 2022 10:50:58 GMT</pubDate>
    </item>
    <item>
      <title>如何创建用于回归的神经网络？</title>
      <link>https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression</link>
      <description><![CDATA[我正在尝试使用 Keras 来构建神经网络。我使用的数据是 https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics。我的代码如下：
import numpy as np
from keras.layers import Dense, Activation
from keras.models import Sequential
from sklearn.model_selection import train_test_split

data = np.genfromtxt(r&quot;&quot;&quot;file location&quot;&quot;&quot;, delimiter=&#39;,&#39;)

model = Sequential()
model.add(Dense(32,activation =&#39;relu&#39;,input_dim = 6))
model.add(Dense(1,))
model.compile(optimizer=&#39;adam&#39;,loss=&#39;mean_squared_error&#39;,metrics =[&#39;accuracy&#39;])

Y = data[:,-1]
X = data[:,:-1]

从这里我尝试使用model.fit(X,Y)，但模型的准确性似乎保持在 0。我是 Keras 的新手，所以这可能是一个简单的解决方案，提前致歉。
我的问题是，向模型添加回归以提高准确性的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression</guid>
      <pubDate>Tue, 27 Feb 2018 11:53:38 GMT</pubDate>
    </item>
    </channel>
</rss>