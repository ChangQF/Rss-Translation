<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 31 Jan 2024 12:23:25 GMT</lastBuildDate>
    <item>
      <title>时间序列预测访问日期与客户类别图不准确</title>
      <link>https://stackoverflow.com/questions/77912045/time-series-forecasting-visit-dates-with-customer-classes-graph-not-accurate</link>
      <description><![CDATA[我正在尝试对一堆类和日期时间进行时间序列预测，但由于某种原因我的图表看起来像这样，我的完整代码如下：
从 google.colab 导入驱动器
drive.mount(&#39;/content/gdrive&#39;,force_remount = True)

将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.metrics 导入mean_squared_error
将 matplotlib.pyplot 导入为 plt

data = pd.read_csv(&#39;gdrive/My Drive/Colab_Notebooks/classproject/classdata.csv&#39;, parse_dates=[&#39;time_date&#39;], index_col=&#39;time_date&#39;)
类id = 数据[&#39;类id&#39;]
时间日期 = 数据.索引.日期
数据[&#39;日期&#39;] = data.index.日期

类id = 数据[&#39;类id&#39;]
time_date = data.index.to_series()
m1 = class_id.ne(class_id.shift())
m2 = time_date.dt.date.ne(time_date.dt.date.shift())
data[&#39;count&#39;] = data.groupby((m1 | m2).cumsum()).cumcount().add(1).values

out = data[data.groupby(data.index.date).transform(&#39;size&#39;).gt(1)]

!pip 安装 pandas-datareader

将 pandas_datareader.data 作为 web 导入
导入日期时间

将 pandas 导入为 pd
pd.set_option(&#39;display.max_columns&#39;, None)
pd.set_option(&#39;display.max_rows&#39;, None)

将 matplotlib.pyplot 导入为 plt
将seaborn导入为sns

sns.set()

plt.ylabel(&#39;类别数量&#39;)
plt.xlabel(&#39;日期&#39;)
plt.xticks（旋转=45）

out.index = pd.to_datetime(out[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;)
plt.plot(out.index, out[&#39;count&#39;], )



而我从其中获取此时间序列代码的博客有这样的结果

所以我不确定是否应该继续XD
我的输入数据是这样的：
时间戳/class_id
2021-09-27 06:00:00 / A
2021-09-27 03:00:00 / A
2021-09-27 01:00:00 / A
2021-09-27 08:29:00 / C
2021-05-23 08:08:49 / B
2021-05-23 03:21:49 / B
2021-05-23 01:22:11 / C
处理它并添加计数和日期列后：
计数/时间戳/class_id/日期
1 / 2021-09-27 06:00:00 / A / 2021-09-27
2 / 2021-09-27 03:00:00 / A / 2021-09-27
3 / 2021-09-27 01:00:00 / A / 2021-09-27
1 / 2021-09-27 08:29:00 / C / 2021-09-27
1 / 2021-05-23 08:08:49 / B / 2021-05-23
2 / 2021-05-23 03:21:49 / B / 2021-05-23
1 / 2021-05-23 01:22:11 / C / 2021-05-23]]></description>
      <guid>https://stackoverflow.com/questions/77912045/time-series-forecasting-visit-dates-with-customer-classes-graph-not-accurate</guid>
      <pubDate>Wed, 31 Jan 2024 09:09:33 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Pytorch 在多节点 GPU 上结合模型和数据并行性</title>
      <link>https://stackoverflow.com/questions/77910976/how-to-combine-model-and-data-parallelism-on-multi-nodes-gpus-using-pytorch</link>
      <description><![CDATA[我有一个8节点的计算集群，每个节点有4个GPU（总共32个GPU，每个只有16Gb内存）。我在我的项目中使用 Pytorch。我可以在所有 32 个 GPU 上使用 DistributedDataParallel 进行数据并行化，但我正在训练的神经网络模型太大，无法放入一个 GPU 内存中，因此数据并行化在这里没有帮助。我尝试进行模型并行化，将模型切割为 4 个部分，但我只能在一个具有 4 个 GPU 的节点上运行它，并进行数据并行化。但我不知道如何将多节点上的数据和模型并行化结合起来。有谁知道如何做到这一点的任何示例（最小可行示例）或想法？
目前，我认为推动这一进程的最佳方法是分两步（或两个级别）进行：
(1) 在一个节点（4个GPU）上进行模型并行化
（2）使用DistributedDataParallel将步骤一中的模型复制到所有8个节点，进行数据并行化
您认为上述想法是个好方法吗？有一个例子如何做到这一点？谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/77910976/how-to-combine-model-and-data-parallelism-on-multi-nodes-gpus-using-pytorch</guid>
      <pubDate>Wed, 31 Jan 2024 05:24:42 GMT</pubDate>
    </item>
    <item>
      <title>使用 RTSP 摄像头进行面部定向</title>
      <link>https://stackoverflow.com/questions/77910835/face-orientor-using-rtsp-camera</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77910835/face-orientor-using-rtsp-camera</guid>
      <pubDate>Wed, 31 Jan 2024 04:36:18 GMT</pubDate>
    </item>
    <item>
      <title>系统生物中 ANN 模型预测的令人困惑的 SHAP 分析</title>
      <link>https://stackoverflow.com/questions/77910332/confusing-shap-analysis-of-ann-model-prediction-in-systems-bio</link>
      <description><![CDATA[我开发了一个 ANN 模型来根据 Elisa 数据预测蛋白质翻译后修饰模式。为了简单起见，如何、可行性和参数对于我的问题并不重要，并且省略了一些细节。
对于给定的蛋白质，我将其称为蛋白质 X，它具有泛素作为修饰，但没有磷酸化模式。
我用各种翻译后修饰模式训练了人工神经网络，但有一个关键信息：我的训练数据不包含任何泛素模式（假设有一个原因）
因此，当我使用一组抗体进行 ELISA 时，抗体 a 特异性针对泛素模式，抗体 b 特异性针对磷酸化模式。当我使用抗体 a、抗体 b（和其他抗体）预测蛋白质 x 修饰模式时，我们获得了相当好的准确性。
为了解释模型的工作原理，我运行了 SHAP 分析和二分图来显示特征重要性（抗体）和修改，但得到了令人困惑的结果

对于抗体 a，除泛素外，其他修饰模式的 SHAP 值存在正值和负值，泛素是其特异性的

对于抗体 b，我们还发现了除磷酸化之外的修饰模式的正向和负向 SHAP 值，而蛋白质 x 并不真正具有磷酸化。


那么我该如何解释为什么 SHAP 产生这种模式：1）抗体 a 与其靶标泛素没有任何 SHAP 相关性，但对其其他靶标有 SHAP 相关性，2）抗体 b 与其靶标也没有 SHAP 相关性，但与其他抗体有 SHAP 相关性。其他人代替。
这又是令人困惑的，因为我预计抗体 a 与泛素有 SHAP 相关性，但与其他蛋白没有 SHAP 相关性，然后抗体 b 不应该有任何 SHAP 相关性，因为它的目标是磷酸化，但蛋白 x 没有磷酸化。
我不太相信或无法将 SHAP 的一些限制联系起来，因为它显示了模型的隐藏模式/关系，而不是我们在“现实生活”中所期望的
有人可以对这个观察到的 SHAP 分析提供更细致的见解吗？]]></description>
      <guid>https://stackoverflow.com/questions/77910332/confusing-shap-analysis-of-ann-model-prediction-in-systems-bio</guid>
      <pubDate>Wed, 31 Jan 2024 01:27:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的机器学习模型总是预测相同的错误答案，即使预测已经在我的数据集中？</title>
      <link>https://stackoverflow.com/questions/77910272/why-is-my-machine-learning-model-always-predicting-the-same-wrong-answer-even-wh</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77910272/why-is-my-machine-learning-model-always-predicting-the-same-wrong-answer-even-wh</guid>
      <pubDate>Wed, 31 Jan 2024 01:07:06 GMT</pubDate>
    </item>
    <item>
      <title>在Python中查找特征列的哪些过滤集导致最大目标列</title>
      <link>https://stackoverflow.com/questions/77910177/find-which-filter-sets-of-feature-columns-leads-to-maximum-target-column-in-pyth</link>
      <description><![CDATA[我无法找到可以解决我的问题的机器学习模型或分类类型。我本以为这可能相当简单，但也许不是。
假设我有 10 个特征列和一个二进制目标列。目标列的数据集中应该有大致相等数量的 0 和 1。整组数据并不是强相关的，所以线性回归、逻辑回归、朴素贝叶斯等都没有得出强相关的模型。然而，我所寻找的是哪个数据系列导致目标列的最大平均值。
例如。对于特征集 A 到 J 如果我按（C = True、D = false、J = true）过滤数据集，则目标 X 的平均值现在为 56%。我正在寻找一种算法，可以找到导致最大目标列均值的方程。
我觉得这可以通过蛮力来完成（循环遍历所有可能的组合），但我希望有一种方法可以在现有的众多数据科学库之一中做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/77910177/find-which-filter-sets-of-feature-columns-leads-to-maximum-target-column-in-pyth</guid>
      <pubDate>Wed, 31 Jan 2024 00:30:49 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML（监督学习）进行元数据分类 [关闭]</title>
      <link>https://stackoverflow.com/questions/77909222/metadata-classification-with-ml-supervised-learning</link>
      <description><![CDATA[我正在从事一个对我来说似乎有点模糊的项目，我想知道你对我有什么建议吗？
我有与健康部分相关的元数据，我想将它们分类为“是否是个人信息？”。
我没有任何标记数据可用作训练数据集。但问题是，许多元数据字段只是 PHN 或 SIN 等数字，因此我无法将它们用作有价值的字段来帮助我找到关系并帮助创建模型本身。我还有一些字段，如果您将它们组合在一起，它们将被视为“个人信息”。我不知道应该从哪里开始。
这就是我现在所知道的一切。]]></description>
      <guid>https://stackoverflow.com/questions/77909222/metadata-classification-with-ml-supervised-learning</guid>
      <pubDate>Tue, 30 Jan 2024 20:25:37 GMT</pubDate>
    </item>
    <item>
      <title>为什么预测总是返回相同的值？</title>
      <link>https://stackoverflow.com/questions/77909165/why-are-the-predictions-always-returning-the-same-value</link>
      <description><![CDATA[我试图根据标题预测一篇文章的阅读量。我正在使用 DecisionTreeRegressor 和 TFIDF 对标题进行矢量化，以便在模型中使用它们。这是我的代码
导入 pandas 作为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.feature_extraction.text 导入 TfidfVectorizer
从 sklearn.tree 导入 DecisionTreeRegressor
从 sklearn.metrics 导入mean_squared_error
从 nltk.corpus 导入停用词
从 nltk.tokenize 导入 word_tokenize

# 加载你的数据集
# 将 &#39;your_dataset.csv&#39; 替换为实际文件名和路径
df = pd.read_csv(&#39;数据/Gerardo_ML_012524_7d_reduced.csv&#39;)

# 特征工程
# 假设“article_title”是包含标题的列
stop_words = set(stopwords.words(&#39;英语&#39;))
df[&#39;article_title&#39;] = df[&#39;article_title&#39;].apply(lambda x: &#39; &#39;.join([word.lower() for word in word_tokenize(x) if word.isalpha() and word.lower() not）在停用词中]））

# TF-IDF 矢量化
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
tfidf_matrix = tfidf_vectorizer.fit_transform(df[&#39;article_title&#39;]).toarray()
tfidf_df = pd.DataFrame(tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())

# 将 TF-IDF DataFrame 与原始 DataFrame 合并
df_combined = pd.concat([df, tfidf_df], 轴=1)

# 定义特征和目标变量
X = df_combined.drop([&#39;article_title&#39;, &#39;article_visits&#39;, &#39;article_ranking&#39;], axis=1)
y = df_combined[&#39;article_visits&#39;]

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 DecisionTreeRegressor 模型
dt_model = DecisionTreeRegressor(max_深度=9)
dt_model.fit(X_train, y_train)

# 在测试集上评估模型
y_pred = dt_model.predict(X_test)

然后，当我想使用这个经过训练的模型预测新标题时，所有预测都将获得相同的值。
# 将 &#39;new_headlines.csv&#39; 替换为包含新标题的文件
new_headlines_df = pd.read_csv(“数据/new_headlines.csv”)
new_tfidf_matrix = tfidf_vectorizer.transform(new_headlines_df[&#39;article_title&#39;]).toarray()
new_tfidf_df = pd.DataFrame(new_tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())
new_combined_df = pd.concat([new_headlines_df, new_tfidf_df], axis=1)

X = df_combined.drop([&#39;article_title&#39;, &#39;article_visits&#39;, &#39;article_ranking&#39;], axis=1)
# 对新头条新闻进行预测
new_predictions = dt_model.predict(new_combined_df.drop([&#39;article_title&#39;,&#39;article_visits&#39;, &#39;article_ranking&#39;], axis=1))
print(&#39;新头条新闻的预测：&#39;)
打印（新预测）

我尝试向模型添加更多变量，但仍然遇到相同的问题。对于全新的头条新闻，我总是得到相同的值。我做了一些 cross_val_score 分析以获得最佳超参数，在训练/测试期间，我得到了超过 0.99 的 R 方值（两种情况）低 RMSE 和低 MAE。新字符串总是获得相同的值。]]></description>
      <guid>https://stackoverflow.com/questions/77909165/why-are-the-predictions-always-returning-the-same-value</guid>
      <pubDate>Tue, 30 Jan 2024 20:13:21 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：调用层“time_distributed_4”时遇到异常（类型 TimeDistributed）</title>
      <link>https://stackoverflow.com/questions/77909132/valueerror-exception-encountered-when-calling-layer-time-distributed-4-type</link>
      <description><![CDATA[我尝试使用 Kvasir 数据集制作 CNN-LSTM 模型。我使用 image_dataset_from_directory 分割数据集，如下所示：
dataset_path = “/kaggle/working/dataset”
图像大小 = 224, 224
批量大小 = 64

train_ds = image_dataset_from_directory(
  数据集_路径，
  验证分割=0.2，
  子集=“训练”，
  label_mode =“分类”，
  种子=23，
  图像大小=图像大小，
  color_mode =“rgb”，
  批量大小=批量大小）


val_ds = image_dataset_from_directory(
  数据集_路径，
  验证分割=0.2，
  子集=“验证”，
  label_mode =“分类”，
  种子=23，
  图像大小=图像大小，
  color_mode=“RGB”，
  批量大小=批量大小）

这个函数给了我一个 BatchDataset。然后我将基数设置如下：
val_batches = tf.data.experimental.cardinality(val_ds)
test_ds = val_ds.take(val_batches // 2)
val_ds = val_ds.skip(val_batches // 2)

然后
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)

这段代码还给了我一个预取数据集。当我运行 print(train_ds) 时它给出：
&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 8), dtype=tf .float32，名称=无））&gt;

然后我添加了我的模型，
 model = tf.keras.models.Sequential([
    # 具有批量归一化和最大池化的卷积层
    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32, (3, 3), 激活=无,input_shape=(224, 224,3))),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),

    tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3, 3), 激活=无)),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),



    # 压平输出并添加密集层
    tf.keras.layers.TimeDistributed(Flatten()),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.LSTM(256,激活=&#39;tanh&#39;),
    
    
    
    tf.keras.layers.Dense(256),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),

    # 具有 8 个节点的输出层用于分类
    tf.keras.layers.Dense(8, 激活=&#39;softmax&#39;)
]）

# 编译模型

model.compile(优化器=AdamW(learning_rate=0.001,weight_decay=0.004,beta_1=0.9,beta_2=0.999,epsilon=1e-8),
          损失=分类交叉熵(),
          指标=[&#39;准确性&#39;])

当我适合这个模型时，它没有运行，并且出现错误：
ValueError：调用层“time_distributed_4”（类型 TimeDistributed）时遇到异常。
    
    层“conv2d_2”的输入0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、224、3）
    
    调用层“time_distributed_4”接收的参数（类型 TimeDistributed）：
      输入= tf.Tensor（形状=（无，224，224，3），dtype = float32）
      • 训练=真
      • 掩码=无

我不知道如何解决这个问题，你能帮助我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77909132/valueerror-exception-encountered-when-calling-layer-time-distributed-4-type</guid>
      <pubDate>Tue, 30 Jan 2024 20:05:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在多维复杂数据上训练模型？</title>
      <link>https://stackoverflow.com/questions/77825016/how-to-train-a-model-on-multidimensional-complex-data</link>
      <description><![CDATA[我有一个输入数据数组，它们是 5 个不同长度的数组。如何构建正确的张量和形式进行训练？
&lt;前&gt;&lt;代码&gt;[
[
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ],],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
[
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ],],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]
],
...
],
]
]]></description>
      <guid>https://stackoverflow.com/questions/77825016/how-to-train-a-model-on-multidimensional-complex-data</guid>
      <pubDate>Tue, 16 Jan 2024 10:22:33 GMT</pubDate>
    </item>
    <item>
      <title>我使用 XGB 分类器训练了数据集</title>
      <link>https://stackoverflow.com/questions/77776124/ive-trained-dataset-using-xgb-classifier</link>
      <description><![CDATA[我从我的队友那里得到了我们项目的这部分代码，我在本地遇到了这个错误，我已经使用 XGB 分类器训练了数据集。
我的代码是：
# XGBoost 分类器模型
从 xgboost 导入 XGBClassifier

# 实例化模型
xgb = XGBClassifier()

# 拟合模型
xgb.fit(X_train,y_train)

然后我得到了这个错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ValueError Traceback（最近一次调用最后一次）
[70] 中的单元格，第 8 行
      5 xgb = XGBClassifier()
      7#拟合模型
----&gt; 8 xgb.fit(X_train,y_train)

文件 ~/anaconda3/envs/project/lib/python3.10/site-packages/xgboost/core.py:730，在 require_keyword_args.. throw_if..inner_f(*args, **kwargs ）
    728 k, arg in zip(sig.parameters, args)：
    第729章
--&gt;第730章

文件〜/anaconda3/envs/project/lib/python3.10/site-packages/xgboost/sklearn.py:1471，在XGBClassifier.fit（self，X，y，sample_weight，base_margin，eval_set，eval_metric，early_stopping_rounds，verbose， xgb_model、sample_weight_eval_set、base_margin_eval_set、feature_weights、回调）
   第1466章
   第1467章
   第1468章
   第1469章
   第1470章
-&gt;第1471章
   攀上漂亮女局长之后1472 ”
   第1473章
   第1474章
   第1476章
   第1478章

ValueError：从“y”的唯一值推断出无效的类。

预期：[0 1]，得到[-1 1]，，我听说 y_train 必须在较新的更新中进行编码，但我对这些事情有点陌生，我也不知道如何做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/77776124/ive-trained-dataset-using-xgb-classifier</guid>
      <pubDate>Mon, 08 Jan 2024 03:09:32 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：缓冲区数据类型不匹配，预期为“double_t”，但得到“float” - hdbscanValidity_index</title>
      <link>https://stackoverflow.com/questions/76242882/valueerror-buffer-dtype-mismatch-expected-double-t-but-got-float-hdbscan</link>
      <description><![CDATA[我使用hdbscan包中的有效性索引，它根据以下论文实现DBCV评分：
https://www.dbs.ifi.lmu.de /~zimek/publications/SDM2014/DBCV.pdf
我正在做一个人脸聚类项目，使用有效性索引后提示错误。
这是代码：
dbcv_score_output = hdbscan.validity.validity_index(feature_vectors, archive_labels)
dbcv_分数_输出

完整错误：
hdbscan/validity.py:30: Ru​​ntimeWarning: 电源遇到溢出
  距离矩阵[距离矩阵！= 0] = (1.0 / 距离矩阵[

-------------------------------------------------- ------------------------
ValueError Traceback（最近一次调用最后一次）
文件〜/anaconda3/lib/python3.9/site-packages/hdbscan/validity.py:371，在validation_index（X，标签，指标，d，per_cluster_scores，mst_raw_dist，详细，** kwd_args）
    第356章 继续
    第358章
    [第 359 章]
    360X，
   （...）
    第367章
    第368章）
    [第 370 章]
--&gt;第371章
    [第 372 章]
    [第 374 章]

文件〜/anaconda3/lib/python3.9/site-packages/hdbscan/validity.py:165，在internal_minimum_spanning_tree(mr_distances)中
    136 def 内部最小跨度树（mr_distances）：
    第137章
    第138章
    139 个可达距离。给定最小生成树“内部”
   （...）
...
    167 为索引，枚举中的行(min_span_tree[1:], 1)：

文件 hdbscan/_hdbscan_linkage.pyx:15，在 hdbscan._hdbscan_linkage.mst_linkage_core()

ValueError：缓冲区数据类型不匹配，预期为“double_t”，但得到“float”

快速浏览输入及其类型：

特点：
dtype=float32
形状：（70201、320）


档案/集群（它是标签编码的）：
形状：(70201，)


当我尝试将功能类型更改为 double/float64 时，它显示了不同类型的错误：
hdbscan/validity.py:33: RuntimeWarning: true_divide 中遇到无效值
  结果 /= distance_matrix.shape[0] - 1
-------------------------------------------------- ------------------------
ValueError Traceback（最近一次调用最后一次）
文件〜/anaconda3/lib/python3.9/site-packages/hdbscan/validity.py:372，在validation_index（X，标签，指标，d，per_cluster_scores，mst_raw_dist，详细，** kwd_args）
    第358章
    [第 359 章]
    360X，
   （...）
    第367章
    第368章）
    [第 370 章]
    第371章
--&gt; [第 372 章]
    [第 374 章]
    第376章

文件〜/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:40，在_amax（a，轴，out，keepdims，初始，其中）
     38 def _amax(a, axis=None, out=None, keepdims=False,
     39 初始=_NoValue，其中=True）：
---&gt; 40 return umr_maximum(a, axis, None, out, keepdims, initial, where)

ValueError：零大小数组到没有标识的缩减操作最大值

我检查了存储库中的所有相关问题和修复，但没有效果。有什么建议或修复吗？]]></description>
      <guid>https://stackoverflow.com/questions/76242882/valueerror-buffer-dtype-mismatch-expected-double-t-but-got-float-hdbscan</guid>
      <pubDate>Sat, 13 May 2023 13:04:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 GridSearchCV 和 f2 分数评估多个模型</title>
      <link>https://stackoverflow.com/questions/67147606/evaluation-of-multiple-models-using-gridsearchcv-and-f2-score</link>
      <description><![CDATA[我正在尝试使用 GridSearchCV 对一些机器学习模型进行二元分类。我想根据分数、最佳参数和 f2 分数对模型进行分类。
对于分数和最佳参数，我使用了代码
分数 = []

对于 model_name，model_params.items() 中的 mp：
    clf = GridSearchCV(mp[&#39;model&#39;], mp[&#39;params&#39;], cv=5, return_train_score=False)
    clf.fit(X_测试, y_测试)
    分数.append({
        &#39;模型&#39;：模型名称，
        &#39;最佳得分&#39;: clf.best_score_,
        &#39;best_params&#39;：clf.best_params_
    })
    
df = pd.DataFrame(分数,列=[&#39;模型&#39;,&#39;best_score&#39;,&#39;best_params&#39;])

它给出了所有模型的best_score和best_params，但我无法找到所有模型的f2分数。代码应该是什么？]]></description>
      <guid>https://stackoverflow.com/questions/67147606/evaluation-of-multiple-models-using-gridsearchcv-and-f2-score</guid>
      <pubDate>Sun, 18 Apr 2021 10:38:43 GMT</pubDate>
    </item>
    <item>
      <title>部署时，SageMaker 无法提取容器的模型数据存档 tar.gz</title>
      <link>https://stackoverflow.com/questions/65881699/sagemaker-failed-to-extract-model-data-archive-tar-gz-for-container-when-deployi</link>
      <description><![CDATA[我正在尝试在 Amazon Sagemaker 中部署现有的 Scikit-Learn 模型。所以这个模型不是在 SageMaker 上训练的，而是在我的机器上本地训练的。
在我的本地（Windows）计算机上，我已将模型保存为 model.joblib 并将模型压缩为 model.tar.gz。
接下来，我已将此模型上传到我的 S3 存储桶 (&#39;my_bucket&#39;)，路径为 s3://my_bucket/models/model.tar.gz。我可以在 S3 中看到 tar 文件。
但是当我尝试部署模型时，它不断给出错误消息“无法提取模型数据存档”。
.tar.gz 是通过在 powershell 命令窗口中运行“tar -czf model.tar.gz model.joblib”在我的本地计算机上生成的。
上传到S3的代码
&lt;前&gt;&lt;代码&gt;导入boto3
s3 = boto3.client(“s3”,
              Region_name=&#39;eu-central-1&#39;,
              aws_access_key_id=AWS_KEY_ID,
              aws_secret_access_key=AWS_SECRET)
s3.upload_file(文件名=&#39;model.tar.gz&#39;, Bucket=my_bucket, Key=&#39;models/model.tar.gz&#39;)

用于创建估计器和部署的代码：
&lt;前&gt;&lt;代码&gt;导入boto3
从 sagemaker.sklearn.estimator 导入 SKLearnModel

...

model_data = &#39;s3://my_bucket/models/model.tar.gz&#39;
sklearn_model = SKLearnModel(model_data=model_data,
                             角色=角色，
                             Entry_point =“my-script.py”，
                             Framework_version =“0.23-1”）
预测器= sklearn_model.deploy（instance_type =“ml.t2.medium”，initial_instance_count = 1）

错误信息：
&lt;块引用&gt;
错误消息：UnexpectedStatusException：托管端点错误
sagemaker-scikit-learn-2021-01-24-17-24-42-204：失败。原因：失败
提取容器“container_1”的模型数据档案来自网址
“s3://my_bucket/models/model.tar.gz”。请确保对象
位于 URL 处的是有效的 tar.gz 存档

有没有办法查看存档无效的原因？]]></description>
      <guid>https://stackoverflow.com/questions/65881699/sagemaker-failed-to-extract-model-data-archive-tar-gz-for-container-when-deployi</guid>
      <pubDate>Mon, 25 Jan 2021 09:03:31 GMT</pubDate>
    </item>
    <item>
      <title>声明嵌入层时出现 ResourceExhaustedError (Keras)</title>
      <link>https://stackoverflow.com/questions/52547568/resourceexhaustederror-when-declaring-embeddings-layer-keras</link>
      <description><![CDATA[我正在为 NLP 创建一个神经网络，从嵌入层开始（使用预先训练的嵌入）。但是，当我在 Keras（Tensorflow 后端）中声明 Embedding 层时，出现 ResourceExhaustedError ：
ResourceExhaustedError：通过分配器 GPU_0_bfc 在 /job:localhost/replica:0/task:0/device:GPU:0 上分配形状为 [137043,300] 的张量并键入 float 时出现 OOM
 [[{{node embedding_4/random_uniform/RandomUniform}} = RandomUniform[T=DT_INT32，dtype=DT_FLOAT，seed=87654321，seed2=9524682，_device=&quot;/job:localhost/replica:0/task:0/device:GPU :0&quot;](embedding_4/random_uniform/shape)]]
 提示：如果您想在 OOM 发生时查看分配的张量列表，请将 report_tensor_allocations_upon_oom 添加到 RunOptions 以获取当前分配信息。

我已经查过Google：大多数ResourceExhaustedError发生在训练时，并且是因为GPU的RAM不够大。它可以通过减少批量大小来修复。
但就我而言，我什至没有开始训练！这行是问题所在：
q1 = 嵌入(nb_words + 1,
             参数[&#39;embed_dim&#39;].value,
             权重=[word_embedding_matrix],
             input_length=param[&#39;sentence_max_len&#39;].value)(问题1)

这里，word_embedding_matrix 是一个大小为 (137043, 300) 的矩阵，即预训练的嵌入。
据我所知，这不会占用大量内存（不像此处）：
137043 * 300 * 4 字节 = 53 kiB
这是使用的 GPU：

&lt;前&gt;&lt;代码&gt; +-------------------------------------------------------- ----------------------------------+
 | NVIDIA-SMI 396.26 驱动程序版本：396.26 |
 |------------------------------------------+----------------- ---+----------------------+
 | GPU 名称持久性-M|总线 ID Disp.A |挥发性未校正。 ECC |
 |风扇温度性能功率：使用/上限|内存使用情况 | GPU-Util 计算 M。
 |================================+================== ====+======================|
 | 0 GeForce GTX 108...关闭 | 00000000:02:00.0 关闭 |不适用 |
 | 23% 32C P8 16W / 250W | 6956MiB / 11178MiB | 0% 默认 |
 +--------------------------------------------+----------------- ---+----------------------+
 | 1 个 GeForce GTX 108...关闭 | 00000000:03:00.0 关闭 |不适用 |
 | 23% 30C P8 16W / 250W | 530MiB / 11178MiB | 0% 默认 |
 +--------------------------------------------+----------------- ---+----------------------+
 | 2 GeForce GTX 108...关闭| 00000000:82:00.0 关闭 |不适用 |
 | 23% 34C P8 16W / 250W | 333MiB / 11178MiB | 0% 默认 |
 +--------------------------------------------+----------------- ---+----------------------+
 | 3 GeForce GTX 108...关闭| 00000000:83:00.0 关闭 |不适用 |
 | 24% 46C P2 58W / 250W | 4090MiB / 11178MiB | 23% 默认 |
 +--------------------------------------------+----------------- ---+----------------------+

 +------------------------------------------------ ----------------------------+
 |进程：GPU 内存 |
 | GPU PID 类型 进程名称 用法 |
 |=================================================== ============================|
 | 0 1087 C uwsgi 1331MiB |
 | 0 1088 C uwsgi 1331MiB | 0 1088 C uwsgi 1331MiB
 | 0 1089 C uwsgi 1331MiB | 0 1089 C uwsgi 1331MiB
 | 0 1090 C uwsgi 1331MiB | 0 1090 C uwsgi 1331MiB
 | 0 1091 C uwsgi 1331MiB | 0 1091 C uwsgi 1331MiB
 | 0 4176 C /usr/bin/python3 289MiB | 0 4176 C /usr/bin/python3 289MiB
 | 1 2631 C ...e92/venvs/wordintent_venv/bin/python3.6 207MiB |
 | 1 4176 C /usr/bin/python3 313MiB | 1 4176 C /usr/bin/python3 313MiB
 | 2 4176 C /usr/bin/python3 323MiB | 2 4176 C /usr/bin/python3 323MiB
 | 3 4176 C /usr/bin/python3 347MiB | 3 4176 C /usr/bin/python3 347MiB
 | 3 10113 C 蟒蛇 1695MiB |
 | 3 13614 C python3 1347MiB |
 | 3 14116 C 蟒蛇 689MiB |
 +------------------------------------------------ ----------------------------+

有谁知道我为什么会遇到这个异常？]]></description>
      <guid>https://stackoverflow.com/questions/52547568/resourceexhaustederror-when-declaring-embeddings-layer-keras</guid>
      <pubDate>Fri, 28 Sep 2018 02:41:56 GMT</pubDate>
    </item>
    </channel>
</rss>