<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 15 Jul 2024 21:15:03 GMT</lastBuildDate>
    <item>
      <title>如何解决引导式利润计算中的零损失风险：机器学习</title>
      <link>https://stackoverflow.com/questions/78751839/how-to-fix-zero-risk-of-loss-in-bootstrapping-profit-calculation-machine-learni</link>
      <description><![CDATA[我刚刚为我的训练营完成了一个项目。然而，无论我如何进行利润计算或引导，我仍然面临零损失的风险。我不知道该如何解决这个问题。
### 利润计算的关键值
BUDGET = 100 * 10**6 # 1 亿美元
REVENUE_PER_BARREL = 4.5 # 每桶 4.5 美元
WELLS_SELECTED = 200 # 选定用于开发的油井数量
COST_PER_WELL = BUDGET / WELLS_SELECTED # 每口井的成本
sufficient_volume = COST_PER_WELL / REVENUE_PER_BARREL
### 利润计算函数
def calculate_profit(predictions, target, n_wells, revenue_per_barrel, cost_per_well):
selected_indices = predictions.sort_values(ascending=False).head(n_wells).index
selected_reserves = target.loc[selected_indices].sum()
revenue = selected_reserves * revenue_per_barrel * 1000
profits = revenue - (cost_per_well * n_wells)
return profits

### Bootstrapping 技术
def bootstrap_profit(predictions, target, n_wells, revenue_per_barrel, cost_per_well, n_samples=1000):
state = np.random.RandomState(42)
profits = []
for _ in range(n_samples):
sample_indices = state.choice(predictions.index, size=n_wells, replace=True)
sample_predictions = predictions.loc[sample_indices]
sample_target = target.loc[sample_indices]
利润 = calculate_profit(sample_predictions, sample_target, n_wells, revenue_per_barrel, cost_per_well)
利润.append(利润)
利润 = pd.Series(利润)
平均利润 = profits.mean()
下限 = profits.quantile(0.025)
上限 = profits.quantile(0.975)
损失风险 = (利润 &lt; 0).mean() * 100
返回平均利润, (下限, 上限), 损失风险
]]></description>
      <guid>https://stackoverflow.com/questions/78751839/how-to-fix-zero-risk-of-loss-in-bootstrapping-profit-calculation-machine-learni</guid>
      <pubDate>Mon, 15 Jul 2024 20:51:56 GMT</pubDate>
    </item>
    <item>
      <title>如何从 CLIP 模型获取多模态嵌入？</title>
      <link>https://stackoverflow.com/questions/78751682/how-to-get-multimodal-embeddings-from-clip-model</link>
      <description><![CDATA[我希望使用 CLIP 来获取多模态（图像和文本）数据行的单个嵌入。
假设我有以下模型：
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
import torchvision.transforms as transforms

model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

def convert_image_data_to_tensor(image_data):
return torch.tensor(image_data)

dataset = df[[&#39;image_data&#39;, &#39;text_data&#39;]].to_dict(&#39;records&#39;)

embeddings = []
for data in dataset:
image_tensor = convert_image_data_to_tensor(data[&#39;image_data&#39;])
text = data[&#39;text_data&#39;]

input = processing(text=text, images=image_tensor, return_tensors=True)
with torch.no_grad():
output = model(**inputs)

我想获取 output 中计算的嵌入。我知道 output 具有附加属性 text_embeddings 和 image_embeddings，但我不确定它们以后如何交互。如果我想为每个记录获取单个嵌入，我应该将这些属性连接在一起吗？是否有其他属性以其他方式将两者结合起来？
这些是存储在输出中的属性：
print(dir(output))

[&#39;__annotations__&#39;, &#39;__class__&#39;, &#39;__contains__&#39;, &#39;__dataclass_fields__&#39;, &#39;__dataclass_params__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__post_init__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__reversed__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;fromkeys&#39;, &#39;get&#39;, &#39;image_embeds&#39;, &#39;items&#39;, &#39;keys&#39;, &#39;logits_per_image&#39;, &#39;logits_per_text&#39;, &#39;loss&#39;, &#39;move_to_end&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;setdefault&#39;, &#39;text_embeds&#39;, &#39;text_model_output&#39;, &#39;to_tuple&#39;, &#39;update&#39;, &#39;values&#39;, &#39;vision_model_output&#39;]

此外，有没有办法指定 CLIP 输出的嵌入的大小？类似于如何在 BERT 配置中指定嵌入大小？
在此先感谢您的帮助。如果我误解了这里任何关键内容，请随时纠正我。]]></description>
      <guid>https://stackoverflow.com/questions/78751682/how-to-get-multimodal-embeddings-from-clip-model</guid>
      <pubDate>Mon, 15 Jul 2024 19:53:18 GMT</pubDate>
    </item>
    <item>
      <title>在 JAX 中对多个输入求导</title>
      <link>https://stackoverflow.com/questions/78751670/taking-derivatives-with-multiple-inputs-in-jax</link>
      <description><![CDATA[我试图在 JAX 中求函数的一阶和二阶导数，但是我这样做却得到了错误的数字或零。我有一个数组，每个变量有两列，每个输入有两行
import jax.numpy as jnp
import jax

rng = rng = jax.random.PRNGKey(1234)
array = jax.random.normal(rng, (2,2))

两个测试函数
def F1(arr):
return 1/arr

def F2(arr):
return jnp.array([arr[0]**2 + arr[1]**3])

以及两种取一阶和二阶导数的方法，其中一种方法使用 jax.grad()
def dF_m1(arr, F):
return jax.grad(lambda arr: F(arr)[0])(arr)

def ddF_m1(arr, F, dF):
return jax.grad(lambda arr: dF(arr, F)[0])(arr)

另一个使用 jax.jacobian()
def dF_m2(arr, F):
jac = jax.jacobian(lambda arr: F(arr))(arr)
return jnp.diag(jac)

def ddF_m2(arr, F, dF):
hess = jax.jacobian(lambda arr: dF(arr, F))(arr)
return jnp.diag(hess)

使用这两种方法计算每个函数的一阶和二阶导数（和误差）可得出以下结果
exact_dF1 = (-1/array**2)
exact_ddF1 = (2/array**3)

print(&quot;函数 1 使用所有 grad()&quot;)
dF1_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F1)
ddF1_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F1, dF_m1)
print(dF1_m1 - exact_dF1,&quot;\n&quot;)
print(ddF1_m1 - exact_ddF1,&quot;\n&quot;)

print(&quot;函数 1 使用所有 jacobian()&quot;)
dF1_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F1)
ddF1_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F1, dF_m2)
print(dF1_m2 - exact_dF1,&quot;\n&quot;)
print(ddF1_m2 - exact_ddF1,&quot;\n&quot;)

输出
函数 1 使用所有 grad()
[[ 0. 48.43877 ]
[ 0. 0.62903005]] 

[[ 0. 674.248 ]
[ 0. 0.9977852]] 

函数 1 使用所有 jacobian()
[[0. 0.]
[0. 0.]] 

[[0. 0.]
[0. 0.]] 

和
exact_dF2 = jnp.hstack( (2*array[:, 0:1], 3*array[:, 1:2]**2))
exact_ddF2 = jnp.hstack( (2 + 0*array[:, 0:1], 6*array[:, 1:2]))

print(&quot;函数 2 使用所有 grad()&quot;)
dF2_m1 = jax.vmap(dF_m1, in_axes=(0,None))(array, F2)
ddF2_m1 = jax.vmap(ddF_m1, in_axes=(0,None,None))(array, F2, dF_m1)
print(dF2_m1 - exact_dF2,&quot;\n&quot;)
print(ddF2_m1 - exact_ddF2,&quot;\n&quot;)

print(&quot;函数 2 使用所有 jacobian()&quot;)
dF2_m2 = jax.vmap(dF_m2, in_axes=(0,None))(array, F2)
ddF2_m2 = jax.vmap(ddF_m2, in_axes=(0,None,None))(array, F2, dF_m2)
print(dF2_m2 - exact_dF2,&quot;\n&quot;)
print(ddF2_m2 - exact_ddF2,&quot;\n&quot;)

输出
函数 2 使用所有 grad()
[[0. 0.]
[0. 0.]] 

[[0. 0.86209416]
[0. 7.5651155 ]] 

使用所有 jacobian() 的函数 2
[[ 0. -0.10149619]
[ 0. -6.925739 ]] 

[[0. 2.8620942]
[0. 9.565115 ]] 

我更愿意只对 F1 之类的东西使用 jax.grad()，但现在似乎只有 jax.jacobian 有效。这完全是因为我需要计算神经网络相对于其输入的高阶导数。感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78751670/taking-derivatives-with-multiple-inputs-in-jax</guid>
      <pubDate>Mon, 15 Jul 2024 19:47:48 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：除了最后一个维度之外，`labels.shape` 必须等于 `logits.shape`。收到：labels.shape=(240,) 和 logits.shape=(16, 14)</title>
      <link>https://stackoverflow.com/questions/78751134/valueerror-labels-shape-must-equal-logits-shape-except-for-the-last-dimensi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78751134/valueerror-labels-shape-must-equal-logits-shape-except-for-the-last-dimensi</guid>
      <pubDate>Mon, 15 Jul 2024 17:11:05 GMT</pubDate>
    </item>
    <item>
      <title>如何让 PyTorch Geometric DataLoader 批量创建多个图形而不是一个巨型图形？</title>
      <link>https://stackoverflow.com/questions/78750828/how-to-make-pytorch-geometric-dataloader-to-create-multiple-graphs-in-batch-inst</link>
      <description><![CDATA[PyTorch Geometric DataLoader 在使用大小大于 1 的批次时，会创建一个包含孤立子图的大图，而不是填充大小相等的图列表。不幸的是，这种方式不适合我的任务。那么有没有办法让 PyTorch Geometric DataLoader 创建一个图列表而不是一个大图？]]></description>
      <guid>https://stackoverflow.com/questions/78750828/how-to-make-pytorch-geometric-dataloader-to-create-multiple-graphs-in-batch-inst</guid>
      <pubDate>Mon, 15 Jul 2024 15:57:22 GMT</pubDate>
    </item>
    <item>
      <title>执行机器学习/时间序列任务时如何处理状态特征</title>
      <link>https://stackoverflow.com/questions/78750232/how-to-deal-with-state-feture-while-performaing-ml-time-series-task</link>
      <description><![CDATA[我正在研究一个收入预测模型，其中状态是关键因素之一。我最初对状态变量应用了独热编码，但没有获得所需的准确度，我的模型失败了。
以下是我到目前为止尝试过的方法：

对状态变量应用了独热编码，结果得到了一个高维稀疏矩阵。
尝试了不同的机器学习模型，但准确度仍然不理想。

我怀疑独热编码不是最好的方法，因为状态变量的基数很高。我该如何更好地处理这个问题，以提高我的模型的性能？
有什么建议或其他编码技术吗？]]></description>
      <guid>https://stackoverflow.com/questions/78750232/how-to-deal-with-state-feture-while-performaing-ml-time-series-task</guid>
      <pubDate>Mon, 15 Jul 2024 13:52:47 GMT</pubDate>
    </item>
    <item>
      <title>如何向 CNN_M_LSTM 模型添加 3 个输入参数？</title>
      <link>https://stackoverflow.com/questions/78749657/how-to-add-3-inputs-parameters-to-the-cnn-m-lstm-model</link>
      <description><![CDATA[我尝试将带有时间戳的能耗数据集和 covid 数据集输入到 CNN_M_LSTM 模型（库 Tensorflow）中。
能耗和时间戳的大小为 (70082, 2)
Covid 数据集的大小为 (744, 1)
我曾使用 tensorslice 和 zip 将数据打包在一起并对数据集进行窗口化：
这是我打包和窗口化能耗和时间戳数据集以及 covid 数据集的代码：
MAX_LENGTH = 96
BATCH_SIZE = 128 
TRAIN.SHUFFLE_BUFFER_SIZE = 1000

def windowed_dataset(series_energy,series_covid, window_size=MAX_LENGTH, batch_size=BATCH_SIZE, shuffle_buffer=TRAIN.SHUFFLE_BUFFER_SIZE):
&quot;&quot;&quot;
我们创建时间窗口来创建 X 和 y 特征。
例如，如果我们选择一个 30 的窗口，我们将创建一个由 30 个点组成的数据集作为 X
&quot;&quot;&quot;
dataset_energy = tf.data.Dataset.from_tensor_slices(series_energy) 
dataset_covid = tf.data.Dataset.from_tensor_slices(series_covid) 
dataset = tf.data.Dataset.zip(dataset_energy,dataset_covid)
dataset = dataset.window(96 + 1, shift=1) #
dataset = dataset.flat_map(lambda window_covid, window_series: tf.data.Dataset.zip((window_covid, window_series)).batch(96 + 1))
dataset = dataset.shuffle(1000)
dataset = dataset.map(lambda window_covid, window_series: (window_covid[:-1], window_series[-1][0])) 
dataset = dataset.padded_batch(128,drop_remainder=True).cache()

返回数据集

对于模型 CNN_M_LSTM，我创建了 2 个输入。这是我的模型：

def create_CNN_LSTM_model():
# 定义输入
input1 = tf.keras.layers.Input(shape=(96, 1), name=&quot;input1&quot;)
input2 = tf.keras.layers.Input(shape=(96, 2), name=&quot;input2&quot;)

# 定义模型的 CNN-LSTM 部分
x = tf.keras.layers.Conv1D(filters=128, kernel_size=3,activation=&#39;relu&#39;, strides=1, padding=&quot;causal&quot;)(input1)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Conv1D(filters=64, kernel_size=3,activation=&#39;relu&#39;, strides=1, padding=&quot;causal&quot;)(x)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.LSTM(16, return_sequences=True)(x)
x = tf.keras.layers.LSTM(8, return_sequences=True)(x)
x = tf.keras.layers.Flatten()(x)
output_lstm = tf.keras.layers.Dense(1)(x)

# 定义模型的密集部分
output_dense_1 = tf.keras.layers.Dense(1)(input2[:, -1, :])

# 连接 LSTM 和 Dense 层的输出
concatenated = tf.keras.layers.Concatenate()([output_dense_1, output_lstm])

# 添加更多密集层
x = tf.keras.layers.Dense(6,activation=tf.nn.leaky_relu)(concatenated)
output = tf.keras.layers.Dense(4)(x)
model_final = tf.keras.Model(inputs=[input1, input2],outputs=output)
# 定义最终模型
return model_final


我如何拟合我的模型：
model_cnn_m_lstm = create_CNN_LSTM_model()

# 编译模型
model_cnn_m_lstm.compile(
loss=tf.keras.losses.Huber(),
optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
metrics=[&quot;mse&quot;]
)

model_cnn_m_lstm.summary()

model_cnn_m_lstm.fit(train_dataset, epochs=100, batch_size=128)

错误是模型需要 2 个输入，但收到 1 个输入张量。我曾尝试将 covid 数据集、能量列和时间戳压缩到一个数据集。
我的期望是将 3 个输入输入到我的 CNN_M_LSTM 模型中。我还尝试单独输入数据，我收到数据形状错误，我尝试重新塑造它，但它也不起作用。有没有办法在我的 CNN_M_LSTM 模型中输入 3 个参数？]]></description>
      <guid>https://stackoverflow.com/questions/78749657/how-to-add-3-inputs-parameters-to-the-cnn-m-lstm-model</guid>
      <pubDate>Mon, 15 Jul 2024 11:49:20 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证和 MICE 归因 [关闭]</title>
      <link>https://stackoverflow.com/questions/78748357/cross-validation-and-mice-imputation</link>
      <description><![CDATA[我正在研究一个二元分类问题，其中有一些缺失数据。我最初的想法是使用 MiceForest。我还使用了分层 k 折技术（数据不平衡）。我还想尽量减少数据泄漏。

我应该何时使用 MiceForest 填补缺失值？针对每个折？还是一开始就填补整个数据集？
我应该使用 SMOTE 来解决每个折中的类别不平衡问题吗？因为我得到了很多误报（当仅使用分层 k 折而没有过度采样时）。

当我对整个数据集进行 mice 填补，用 smote 解决类别不平衡问题，然后进行交叉验证时，我获得了非常好的性能。我觉得这是过度拟合？这是因为数据泄漏吗？]]></description>
      <guid>https://stackoverflow.com/questions/78748357/cross-validation-and-mice-imputation</guid>
      <pubDate>Mon, 15 Jul 2024 06:37:02 GMT</pubDate>
    </item>
    <item>
      <title>HuggingFace：Llama-3-8B 合作检查点碎片加载进度在 25% 处停止</title>
      <link>https://stackoverflow.com/questions/78748213/huggingface-loading-checkpoint-shards-in-collab-for-llama-3-8b-stops-at-25</link>
      <description><![CDATA[我尝试使用 huggingface 在我的 Colab 笔记本中本地运行 Llama-3-8B 模型。加载模型时，检查点分片在 25% 处停止加载。我不明白问题可能是什么。
from transformers import AutoModelForCausalLM, AutoTokenizer

# 定义模型名称（这是一个占位符，请替换为实际模型名称）
model_name = &quot;meta-llama/Meta-Llama-3-8B&quot;

!huggingface-cli login --token $HF_TOKEN
# 加载 tokenizer 和模型
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 如果模型很大，将其移动到 GPU 可能会有所帮助
model.to(&#39;cuda&#39;)

HF_Token 已定义，出于隐私原因，此处未提及。
提示以下错误：
您的 token 已保存到 /root/.cache/huggingface/token
登录成功
/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: 
您的 Colab secrets 中不存在 secret `HF_TOKEN`。
要使用 Hugging Face Hub 进行身份验证，请在设置选项卡 (https://huggingface.co/settings/tokens) 中创建一个令牌，将其设置为 Google Colab 中的机密，然后重新启动会话。
您将能够在所有笔记本中重复使用此机密。
请注意，建议进行身份验证，但仍然可以选择访问公共模型或数据集。
warnings.warn(
词汇表中已添加特殊令牌，请确保对相关的词嵌入进行了微调或训练。
正在加载 检查点 分片：  25%
 1/4 [00:22&lt;01:07, 22.37s/it]
]]></description>
      <guid>https://stackoverflow.com/questions/78748213/huggingface-loading-checkpoint-shards-in-collab-for-llama-3-8b-stops-at-25</guid>
      <pubDate>Mon, 15 Jul 2024 05:44:13 GMT</pubDate>
    </item>
    <item>
      <title>二元分类中的 SHAP 值解释</title>
      <link>https://stackoverflow.com/questions/78740880/shap-value-explanations-in-binary-classification</link>
      <description><![CDATA[我尝试使用每个特征的 SHAP 值来解释我的二元分类模型。我想知道：
正的 SHAP 值是否意味着该特征对预测“1”类的贡献更大，而负的 SHAP 值是否意味着该特征对预测“0”类的贡献更大？
如果我使用绝对 SHAP 值差异来描述特征贡献变化，这个想法是否合理？]]></description>
      <guid>https://stackoverflow.com/questions/78740880/shap-value-explanations-in-binary-classification</guid>
      <pubDate>Fri, 12 Jul 2024 14:25:43 GMT</pubDate>
    </item>
    <item>
      <title>无法检测/删除图像数据集中两个位置不同的水印</title>
      <link>https://stackoverflow.com/questions/78736804/trouble-detecting-removing-two-watermarks-that-vary-location-across-image-datase</link>
      <description><![CDATA[我在从一组图片中删除水印时遇到了问题。这些水印彼此靠近，但又有所不同（见下文）。其中一个水印是红色方块，里面有白色文字。另一个是半透明的灰色句子。目的是处理图像以用于机器学习。
图像
解决问题的尝试：
由于水印在图像数据集中的位置各不相同，我尝试了以下操作：

复制图像并将其转换为 HSV 颜色空间
为感兴趣的区域选择一系列下限值和上限值（在分割图像并为每个通道构建直方图后选择这些值）
使用 cv2.inRange() 函数构建蒙版
使用蒙版在原始图像中修复水印

对于红色方块，前三个步骤完美无缺。但第三步只是有点奏效。水印比以前明显少了，但仍然很明显。对于文本，我无法在第 3 步中获得足够好的蒙版 - 它的颜色/像素强度与周围区域和文本本身太接近了。
这看起来更像是一个机器学习问题，这很好，但我想事先用尽其他选择。关于如何使用机器学习或算法方法解决这个问题有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78736804/trouble-detecting-removing-two-watermarks-that-vary-location-across-image-datase</guid>
      <pubDate>Thu, 11 Jul 2024 16:54:13 GMT</pubDate>
    </item>
    <item>
      <title>python 中某些函数的贬值：数据框的真值不明确</title>
      <link>https://stackoverflow.com/questions/78735084/depreciation-of-some-function-in-python-ambiguous-truth-value-of-dataframe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78735084/depreciation-of-some-function-in-python-ambiguous-truth-value-of-dataframe</guid>
      <pubDate>Thu, 11 Jul 2024 10:52:14 GMT</pubDate>
    </item>
    <item>
      <title>如何准确计算Doctr ocr中检测到的文本的绝对bbox坐标？</title>
      <link>https://stackoverflow.com/questions/78733724/how-to-accurately-calculate-absolute-bbox-coordinates-of-detected-text-in-doctr</link>
      <description><![CDATA[我一直试图在文档的图片上绘制 bbox，并尝试使用 mindee-doctr 进行 ocr 以查看检测到的文本行。我面临的问题是，我通过乘以相对坐标和页面尺寸计算出的 bbox 的绝对坐标，在原始图像上绘制时都向右上角偏移。有没有办法纠正这个问题？
这是我计算 bbox 的代码：
from doctr.models import ocr_predictor
from doctr.io import DocumentFile

# 使用 docTR 分析图像并获取结果
line_boundaries = []
model = ocr_predictor(pretrained=True) #设置preserve_aspect_ratio=False 或symmetric_pad=False 没有区别。
doc = DocumentFile.from_images(img_path)
result = model(doc)

# 提取每行的边界框坐标
for page in result.pages:
for block in page.blocks:
for line in block.lines:
# 将相对坐标与页面尺寸相乘，得到绝对坐标
x_min, y_min, x_max, y_max = round(line.geometry[0][0] * page.dimensions[0]), round(line.geometry[0][1] * page.dimensions[1]), round(line.geometry[1][0] * page.dimensions[0]), round(line.geometry[1][1] * page.dimensions[1])
line_boundaries.append((x_min, y_min, x_max, y_max))

这是 line_boundaries 的值：
[(531, 148, 1321, 184), (2725, 148, 3061, 177), (526, 254, 3071, 295), (526, 288, 3071, 332), (535, 324, 3071, 363), ... ]
这是我用来绘制方框的函数：
import cv2
from google.colab.patches import cv2_imshow # 代替 cv2.imshow 使用，因为它会导致 collab 崩溃

def draw_rectangles(image_path, line_boundaries):
&quot;&quot;&quot;
使用提供的线边界在图像上绘制矩形。

参数：
image_path：图像文件的路径。
line_boundaries：线边界列表，其中每个边界都是四个点的列表。

返回：
无
&quot;&quot;&quot;

# 加载图像
image = cv2.imread(img_path)

# 遍历线边界并绘制矩形
for bounding in line_boundaries:
#x1, y1, x2, y2 = int(boundary[0][0]), int(boundary[0][1]), int(boundary[2][0]), int(boundary[2][1])
x1, y1, x2, y2 = map(int, bounding) # 将坐标转换为整数
cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

# 用矩形显示图像
cv2_imshow(image) # 仅使用 cv2_imshow 代替 cv2.imshow 进行协作
cv2.waitKey(0)
cv2.destroyAllWindows()

这是带有在其上绘制的 bboxes。

我尝试过不使用舍入，但没有任何区别，也尝试过只使用预测器，但无济于事。在 ocr_predictor 中设置preserve_aspect_ratio=False 或symmetric_pad=False 也没有区别。]]></description>
      <guid>https://stackoverflow.com/questions/78733724/how-to-accurately-calculate-absolute-bbox-coordinates-of-detected-text-in-doctr</guid>
      <pubDate>Thu, 11 Jul 2024 05:38:22 GMT</pubDate>
    </item>
    <item>
      <title>带有 gpu 的 Lightgbm 分类器</title>
      <link>https://stackoverflow.com/questions/60360750/lightgbm-classifier-with-gpu</link>
      <description><![CDATA[model = lgbm.LGBMClassifier(
n_estimators=1250,
num_leaves=128,
learning_rate=0.009,
verbose=1
)

使用 LGBM 分类器，
现在有没有办法将其与 GPU 一起使用？]]></description>
      <guid>https://stackoverflow.com/questions/60360750/lightgbm-classifier-with-gpu</guid>
      <pubDate>Sun, 23 Feb 2020 09:20:03 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 聚类：预测（X）与 fit_predict（X）</title>
      <link>https://stackoverflow.com/questions/37106983/scikit-learn-clustering-predictx-vs-fit-predictx</link>
      <description><![CDATA[在 scikit-learn 中，一些聚类算法同时具有 predict(X) 和 fit_predict(X) 方法，例如 KMeans 和 MeanShift，而其他算法仅具有后者，例如 SpectralClustering。根据文档：
fit_predict(X[, y]): 对 X 执行聚类并返回聚类标签。
predict(X): 预测 X 中每个样本所属的最接近聚类。

我不太明白这两者之间的区别，在我看来它们似乎是等价的。]]></description>
      <guid>https://stackoverflow.com/questions/37106983/scikit-learn-clustering-predictx-vs-fit-predictx</guid>
      <pubDate>Mon, 09 May 2016 02:25:29 GMT</pubDate>
    </item>
    </channel>
</rss>