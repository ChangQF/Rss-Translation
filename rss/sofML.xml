<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 04 Aug 2024 21:14:47 GMT</lastBuildDate>
    <item>
      <title>通过 10 个时期，我生成了一个文件 training_checkpoints ('/content/training_checkpoints) 来存储 .weights.h5 文件（10 个时期）但是当我 tr</title>
      <link>https://stackoverflow.com/questions/78831928/via-10-epochs-i-generated-a-file-training-checkpoints-content-training-check</link>
      <description><![CDATA[# 训练模型并创建检查点以存储权重
import os

checkpoint_dir = &#39;./training_checkpoints&#39;

# 检查点文件的名称
checkpoint_prefix = os.path.join(checkpoint_dir, &quot;ckpt_{epoch}.weights.h5&quot;) # 将 &#39;.weights.h5&#39; 添加到文件名
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, save_weights_only = True)

EPOCHS = 10
history = model.fit(dataset, epochs = EPOCHS, callbacks = [checkpoint_callback])

checkpoint_dir = &#39;/content/training_checkpoints&#39;
latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)

if latest_checkpoint:
print(f&quot;最新检查点发现：{latest_checkpoint}&quot;)
model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)
model.load_weights(latest_checkpoint) 
model.build(tf.TensorShape([1, None]))
else:
print(&quot;未找到检查点。&quot;)

输出：未找到检查点。


我原本期望在完成 10 个 epoch 后生成最新的检查点，但却没有得到任何检查点……似乎代码无法检测到最新的检查点……有人能解释一下原因并帮忙解决吗？]]></description>
      <guid>https://stackoverflow.com/questions/78831928/via-10-epochs-i-generated-a-file-training-checkpoints-content-training-check</guid>
      <pubDate>Sun, 04 Aug 2024 19:16:56 GMT</pubDate>
    </item>
    <item>
      <title>将来自 OneHotencoder 的稀疏矩阵转换为数组，然后转换为 DataFrame [关闭]</title>
      <link>https://stackoverflow.com/questions/78831669/covert-a-sparse-matrix-arrived-from-onehotencoder-into-an-array-and-then-into-a</link>
      <description><![CDATA[我在将 OneHotencoder 传来的稀疏矩阵转换为数组，然后再转换为 DataFrame 的过程中遇到了错误，即
ValueError: 必须传递 2-D 输入。shape=()

实际上，它之前运行良好。但在重新启动内核后重新运行 shell 后遇到了这个问题。
对于以下代码：
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(handle_unknown=&#39;ignore&#39;)
temp = ohe.fit_transform(df[[&#39;encoded_data&#39;]])

temp = pd.DataFrame(np.array(temp))
]]></description>
      <guid>https://stackoverflow.com/questions/78831669/covert-a-sparse-matrix-arrived-from-onehotencoder-into-an-array-and-then-into-a</guid>
      <pubDate>Sun, 04 Aug 2024 17:09:05 GMT</pubDate>
    </item>
    <item>
      <title>python-error-the-truth-value-of-a-dataframe-is-ambiguous [关闭]</title>
      <link>https://stackoverflow.com/questions/78831608/python-error-the-truth-value-of-a-dataframe-is-ambiguous</link>
      <description><![CDATA[我无法使用以下代码解决错误：
main.html 不会返回推荐的产品，并在数据框上抛出此错误。
def content_based_recommendations(train_data, item_name, top_n=10):
if item_name not in train_data[&#39;Name&#39;].values:
print(f&quot;Item &#39;{item_name}&#39; not found in the training data.&quot;)
return pd.DataFrame()

tfidf_vectorizer = TfidfVectorizer(stop_words=&#39;english&#39;)
tfidf_matrix_content = tfidf_vectorizer.fit_transform(train_data[&#39;Tags&#39;])
cosine_similarities_content = cosine_similarity(tfidf_matrix_content, tfidf_matrix_content)
item_index = train_data[train_data[&#39;名称&#39;] == item_name].index[0]
similar_items = list(enumerate(cosine_similarities_content[item_index]))
similar_items = sorted(similar_items, key=lambda x: x[1], reverse=True)
top_similar_items = similar_items[1:top_n+1]
Recommended_item_indices = [x[0] for x in top_similar_items]
Recommended_items_details = train_data.iloc[recommended_item_indices][[&#39;名称&#39;, &#39;评论数&#39;, &#39;品牌&#39;, &#39;图片网址&#39;, &#39;评分&#39;]]
return Recommended_items_details

@app.route(&quot;/recommendations&quot;, methods=[&#39;POST&#39;])
def suggestions():
try:
prod = request.form.get(&#39;prod&#39;)
nbr = request.form.get(&#39;nbr&#39;)

如果不是 prod 或不是 nbr:
return render_template(&#39;main.html&#39;, message=&quot;缺少产品名称或推荐数量。&quot;)

try:
nbr = int(nbr)
except ValueError:
return render_template(&#39;main.html&#39;, message=&quot;无效的推荐数量。&quot;)

df = pd.read_csv(&#39;clean_data.csv&#39;)
print(df.to_string())
content_based_rec = content_based_recommendations(train_data, prod, top_n=nbr)

if content_based_rec.empty:
return render_template(&#39;main.html&#39;, message=&quot;没有可用于该产品的推荐。&quot;)
else:
random_product_image_urls = [random.choice(random_image_urls) for _ in range(len(content_based_rec))]
price = [40, 50, 60, 70, 100, 122, 106, 50, 30, 50]
return render_template(&#39;main.html&#39;, content_based_rec=content_based_rec, truncate=truncate,
random_product_image_urls=random_product_image_urls,
random_price=random.choice(price))

except Exception as e:
return render_template(&#39;main.html&#39;, message=f&quot;An error occurred: {str(e)}&quot;)

如何解决？
我期望模板上显示的是类似商品的列表。]]></description>
      <guid>https://stackoverflow.com/questions/78831608/python-error-the-truth-value-of-a-dataframe-is-ambiguous</guid>
      <pubDate>Sun, 04 Aug 2024 16:40:42 GMT</pubDate>
    </item>
    <item>
      <title>Ultralytics YOLOv8 姿势估计在自定义数据集上绘制边/颜色时不尊重顶点顺序</title>
      <link>https://stackoverflow.com/questions/78831319/ultralytics-yolov8-pose-estimation-not-respecting-vertices-order-when-drawing-ed</link>
      <description><![CDATA[我按照官方文档，使用 Python 中的 ultralytics 在自定义数据集上进行人体姿势估计训练。我有一个由我标记的 1 幅图像组成的小型数据集，预测工作正常，因此顶点识别和类别识别工作正常。
问题是程序绘制顶点的方式，包括边缘和颜色，与我的骨架不匹配。YOLO 训练中的鼻子被画在我的脚踝上。预测实际上工作正常，但它非常丑陋，因为它绘制了不应该存在的边缘（例如将膝盖顶点与肩部顶点连接起来）。此外，我的姿势估计对象与人类非常相似，因此我认为如果我有相同的骨架，训练会更快。
我已经做过和尝试过的事情：

密切关注官方文档中的标签，0 代表鼻子，1 代表左耳，依此类推。
尝试另一种标签，看看文档是否过时。这会导致 txt 中 train 和 val 的坐标顺序发生变化，因此我认为它会改变输出的颜色和边缘。它什么都不做，这对我来说很奇怪。
读取标准数据集 yaml 选项，例如 super-gradients on coco 中的选项，并复制“edge_links”、“edge_colors”和“keypoint_colors”选项。这样，即使我的顺序不同，它也应该解析这些选项并正确绘制它们。它也没有做任何事情，在我看来，我误解了 yaml 的编写方式。但我不知道是什么。
重要提示，我没有使用任何自动数据集创建工具，而是手动创建所有这些文件。由于图像版权原因，我无法使用 Roboflow，因此我使用本地注释器。我检查了在 Roboflow 中创建的数据集（来自 ultralytics 的 tiger 示例）以查看差异，但我没有看到它们，它们甚至没有定义这些东西，但结果只显示顶点，没有任何边缘。这对我来说也很奇怪。
我正在使用当前版本的 ultralytics，几天前刚刚安装了 pip。

我拥有的代码（由文档提取）是这样的。我的 yaml 是“checa.yaml”。
yaml_of_model=“yolov8n-pose.yaml”
model_to_load =“best.pt”
# 加载模型
model = YOLO(yaml_of_model) # 从 YAML 构建新模型
model = YOLO(model_to_load) # 加载预训练模型（建议用于训练）
model = YOLO(yaml_of_model).load(model_to_load) # 从 YAML 构建并传输权重
results = model.train(data=&quot;./checa.yaml&quot;, epochs=4, imgsz=1920)
results = model(&#39;./screenshot_1.jpg&#39;)

for result in results:
result.show()

我的 yaml 摘录：
# 数据集根目录
path: ...

# 训练和验证目录
train: ...
val: ...

# 关键点和维度的数量（x,y 为 2，x,y,visible 为 3）
kpt_shape: [17, 3]
flip_indexes: [ 0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15,]

edge_links:
- [0, 1]
- ...

edge_colors:
- [214, 39, 40] # Nose -&gt; LeftEye
- ...

keypoint_colors:
- [148, 103, 189]
- ...

# Classes 字典
names:
0: FirstClass
1: ...
]]></description>
      <guid>https://stackoverflow.com/questions/78831319/ultralytics-yolov8-pose-estimation-not-respecting-vertices-order-when-drawing-ed</guid>
      <pubDate>Sun, 04 Aug 2024 14:26:27 GMT</pubDate>
    </item>
    <item>
      <title>生成 512x512 照片的模型</title>
      <link>https://stackoverflow.com/questions/78831225/model-to-generate-512x512-photos</link>
      <description><![CDATA[我如何让这个模型生成 512x512 像素或更大的图像？现在它生成 64x64 像素的图像。我尝试更改模型中的某些值，但没有成功。这些卷积层如何工作，尤其是 Conv2D 和 Conv2DTranspose？我不明白图像在这些层中是如何调整大小的。
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layer
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt

cd /content/drive/MyDrive

dataset = keras.preprocessing.image_dataset_from_directory(
directory = &#39;Humans&#39;, label_mode = None, image_size = (64,64), batch_size = 32,
shuffle = True
).map(lambda x: x/255.0)

discriminator = keras.models.Sequential(
[
keras.Input(shape = (64,64,3)),
layer.Conv2D(64, kernel_size = 4, strides = 2, padding = &#39;相同&#39;),
layers.LeakyReLU(0.2),
layers.Conv2D(128, kernel_size = 4, strides = 2, padding = &#39;相同&#39;),
layers.LeakyReLU(0.2),
layers.Conv2D(128, kernel_size = 4, strides = 2, padding = &#39;相同&#39;),
layers.LeakyReLU(0.2),
layers.Flatten(),
layers.Dropout(0.2),
layers.Dense(1,activation = &#39;sigmoid&#39;)
]
)

latent_dim = 128
generator = keras.models.Sequential(
[
layers.Input(shape = (latent_dim,)),
layers.Dense(8*8*128),
layers.Reshape((8,8,128)),
layers.Conv2DTranspose(128, kernel_size = 4, strides = 2, padding = &#39;same&#39;),
layers.LeakyReLU(0.2),
layers.Conv2DTranspose(256, kernel_size = 4, strides = 2, padding = &#39;same&#39;),
layers.LeakyReLU(0.2),
layers.Conv2DTranspose(512, kernel_size = 4, strides = 2, padding = &#39;same&#39;),
layers.LeakyReLU(0.2),
layers.Conv2D(3, kernel_size = 5,padding = &#39;same&#39;,activation = &#39;sigmoid&#39;)
]
)

opt_gen = keras.optimizers.Adam(1e-4)
opt_disc = keras.optimizers.Adam(1e-4)
loss_fn = keras.losses.BinaryCrossentropy()

for epoch 在 range(500) 中：
对于 idx，real 在 enumerate(tqdm(dataset)) 中：
batch_size = real.shape[0]
random_latent_vectors = tf.random.normal(shape = (batch_size,latent_dim))
fake = generator(random_latent_vectors)

如果 idx % 50 == 0：
img = keras.preprocessing.image.array_to_img(fake[0])
img.save(f&#39;gen_images/generated_img{epoch}_{idx}_.png&#39;)

使用 tf.GradientTape() 作为 disc_tape：
loss_disc_real = loss_fn(tf.ones((batch_size,1)), discriminator(real))
loss_disc_fake = loss_fn(tf.zeros(batch_size,1), discriminator(fake))
loss_disc = (loss_disc_real+loss_disc_fake)/2

grads = disc_tape.gradient(loss_disc, discriminator.trainable_weights)

opt_disc.apply_gradients(
zip(grads, discriminator.trainable_weights)
)

with tf.GradientTape() as gen_tape:
fake = generator(random_latent_vectors)
output = discriminator(fake)
loss_gen = loss_fn(tf.ones(batch_size,1),output)

grads = gen_tape.gradient(loss_gen, generator.trainable_weights)
opt_gen.apply_gradients(
zip(grads, generator.trainable_weights)
)

我尝试更改图像大小和卷积层中的某些值，但它不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/78831225/model-to-generate-512x512-photos</guid>
      <pubDate>Sun, 04 Aug 2024 13:42:23 GMT</pubDate>
    </item>
    <item>
      <title>检测并定位图像中的大量不同物体[关闭]</title>
      <link>https://stackoverflow.com/questions/78830813/detect-and-localize-a-large-number-of-different-objects-in-an-image</link>
      <description><![CDATA[我有多个 Match 3 游戏的图片，例如 Candy Crush。下面给出了一个示例图片。我想检测和定位图片中所有不同颜色的物体。定位的意思是获取图片中每个物体的精确坐标。我尝试过不同的方法，例如边缘检测技术、轮廓和其他一些方法，但到目前为止效果并不理想。无法使用模板匹配，因为图像大小不同。我正在考虑切换到 ML 技术，特别是 CNN，但为此我必须创建一个庞大的数据集，而且不确定这是否可行。那么有没有可以解决上述问题的计算机视觉方法呢？
]]></description>
      <guid>https://stackoverflow.com/questions/78830813/detect-and-localize-a-large-number-of-different-objects-in-an-image</guid>
      <pubDate>Sun, 04 Aug 2024 10:33:33 GMT</pubDate>
    </item>
    <item>
      <title>如何将风格化效果应用于视频？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78830209/how-can-i-apply-a-stylized-effect-to-a-video</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78830209/how-can-i-apply-a-stylized-effect-to-a-video</guid>
      <pubDate>Sun, 04 Aug 2024 03:36:28 GMT</pubDate>
    </item>
    <item>
      <title>加载 Keras 模型时，“密集层”需要 1 个输入，但它收到了 2 个输入张量”</title>
      <link>https://stackoverflow.com/questions/78829665/layer-dense-expects-1-inputs-but-it-received-2-input-tensors-when-loading</link>
      <description><![CDATA[我正在使用 Kaggle 开发乳腺癌组织病理学图像的分类模型。该数据集包含 157,572 张图像（78,786 张 IDC 阴性和 78,786 张 IDC 阳性），每张图像的尺寸为 50x50 像素。
我使用 ResNet50 作为基础模型，并尝试保存并稍后加载经过训练的模型的最高效版本。但是，当我尝试加载已保存的模型时，我遇到了以下错误：
# 加载模型
model = load_model(&#39;/kaggle/working/resnet50_model.keras&#39;)
“密集”层需要 1 个输入，但它收到了 2 个输入张量。收到的输入：[&lt;KerasTensor shape=(None, 2, 2, 2048), dtype=float32, sparse=False, name=keras_tensor_566&gt;, &lt;KerasTensor shape=(None, 2, 2, 2048), dtype=float32, sparse=False, name=keras_tensor_567&gt;]

这是我的代码：
import tensorflow as tf
from tensorflow.keras import layer, models
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import load_model

# 数据增强管道
data_augmentation = tf.keras.Sequential([
layer.RandomFlip(&quot;horizo​​ntal_and_vertical&quot;),
layer.RandomRotation(0.5),
layer.RandomContrast(0.2),
layer.RandomBrightness(0.2),
layer.GaussianNoise(0.1)
])

# 预处理函数以规范化图像
def preprocess(image, label):
image = tf.image.resize(image, (50, 50))
image = tf.cast(image, tf.float32) / 255.0
return image, label

# 使用验证分割加载数据集
train_dir = &#39;path/to/data&#39;
batch_size = 64
img_height = 50
img_width = 50
validation_split = 0.2
test_split_ratio = 0.5

# 加载数据集
data_train = tf.keras.utils.image_dataset_from_directory(
train_dir,
validation_split=validation_split,
subset=&quot;training&quot;,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size
)

data_val = tf.keras.utils.image_dataset_from_directory(
train_dir,
validation_split=validation_split,
subset=&quot;validation&quot;,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size
)

# 将验证集拆分为验证集和测试集
def split_dataset(dataset, split_ratio=0.5):
dataset_size = len(dataset)
split = int(split_ratio * dataset_size)
train_dataset = dataset.take(split)
test_dataset = dataset.skip(split)
return train_dataset, test_dataset

data_val, data_test = split_dataset(data_val, split_ratio=test_split_ratio)

# 应用预处理和增强
data_train = data_train.map(lambda x, y: (data_augmentation(x, training=True), y)).map(preprocess)
data_val = data_val.map(preprocess)
data_test = data_test.map(preprocess)

# 预取数据以获得更好的性能
data_train = data_train.prefetch(tf.data.AUTOTUNE)
data_val = data_val.prefetch(tf.data.AUTOTUNE)
data_test = data_test.prefetch(tf.data.AUTOTUNE)

# 定义 ResNet50 模型
input_shape = (50, 50, 3)
base_model = ResNet50(weights=&#39;imagenet&#39;, include_top=False, input_shape=input_shape)
base_model.trainable = False

# 构建模型
model = models.Sequential([
layer.Input(shape=(img_height, img_width, 3)),
base_model,
layer.GlobalAveragePooling2D(),
layer.Dense(256,activation=&#39;relu&#39;),
layer.Dropout(0.5),
layer.Dense(1,activation=&#39;sigmoid&#39;)
])

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

# 用于提前停止和检查点的回调
early_stopping = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;,patient=5, restore_best_weights=True)

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
filepath=&#39;/kaggle/working/resnet50_model.keras&#39;,
monitor=&#39;val_loss&#39;,
mode=&#39;min&#39;,
save_best_only=True,
save_weights_only=False
)

# 训练模型
model.fit(data_train, epochs=50, validation_data=data_val, callbacks=[early_stopping, model_checkpoint_callback])

# 微调模型
base_model.trainable = True
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
model.fit(data_train, epochs=50, validation_data=data_val,回调=[early_stopping, model_checkpoint_callback])

# 保存模型
model.save(&#39;/kaggle/working/resnet50_model.keras&#39;)

# 加载最佳微调模型
model = load_model(&#39;/kaggle/working/resnet50_model.keras&#39;)

为了调试该问题，我在预处理阶段通过打印数据批次的形状来检查图像输入。结果如下：

训练数据 - 批次 0：图像形状：(64, 50, 50, 3)，标签形状：(64,)

验证数据 - 批次 0：图像形状：(64, 50, 50, 3)，标签形状：(64,)

测试数据 - 批次 0：图像形状：(64, 50, 50, 3)，标签形状：(64,)


我预计保存的模型可以顺利加载，这样我就可以使用训练后的权重和架构进行评估和预测。]]></description>
      <guid>https://stackoverflow.com/questions/78829665/layer-dense-expects-1-inputs-but-it-received-2-input-tensors-when-loading</guid>
      <pubDate>Sat, 03 Aug 2024 20:01:05 GMT</pubDate>
    </item>
    <item>
      <title>通过 OpenCV 快速查找重复图像</title>
      <link>https://stackoverflow.com/questions/78829582/fast-finding-of-the-duplicate-images-via-opencv</link>
      <description><![CDATA[我正在尝试通过 C# 添加一种在 MongoDB 中存储和查找重复图片的方法。我有一个应用程序，允许用户创建图像并将其保存在我的网站上。我将它们存储在 MongoDB 中，现在想查找重复项，但由于图像数量众多，我无法只用其他图像检查新图像。我希望有一些索引和标志来快速找到可能的重复项，然后完全检查它们。我使用 OpenCV 查找图片的描述符并存储它们，以便之后我可以轻松地用新图片检查它们。我可以使用什么作为标志或索引来不搜索整个数据库？有没有快速的方法？这是我的代码示例，它允许我找到描述符：
Mat img1 = Cv2.ImRead(&quot;somepic&quot;, ImreadModes.Grayscale); 
var orb = ORB.Create(); 
KeyPoint[] keyPoints; 
Mat descriptors = new Mat(); 
orb.DetectAndCompute(img1, null, out keyPoints, descriptors);

有一个通过描述符比较两幅图像的示例（左侧原始图像和右侧裁剪后的图像）。所以这种方法确实有效。但是如何在数百万张图片中快速做到这一点？
比较图像
我听说我可以将描述符分成 4-8 个部分并将它们用作索引，但无法保证我会找到可能的重复项。我也听说过 k-means，但我也不了解如何将其与描述符一起使用。或者也许还有其他没有机器学习的方法？
附言：我尝试过 PHash，但它对裁剪后的图片效果很糟糕。]]></description>
      <guid>https://stackoverflow.com/questions/78829582/fast-finding-of-the-duplicate-images-via-opencv</guid>
      <pubDate>Sat, 03 Aug 2024 19:14:23 GMT</pubDate>
    </item>
    <item>
      <title>在音频 AI 中实现迁移学习</title>
      <link>https://stackoverflow.com/questions/78828884/implementing-transfer-learning-in-audio-ai</link>
      <description><![CDATA[虽然我的任务是从音频中检测疾病，但我能否通过迁移一些预训练网络来实现 CNN 神经网络（以 MFCC 为输入）来进行语音处理？
我的意思是，虽然这两项任务彼此不同，但原则上这样做可以吗？
因为一般来说，我看到的迁移学习是当模型经过预训练时检测人，然后你进行微调以检测你想要的人]]></description>
      <guid>https://stackoverflow.com/questions/78828884/implementing-transfer-learning-in-audio-ai</guid>
      <pubDate>Sat, 03 Aug 2024 14:06:15 GMT</pubDate>
    </item>
    <item>
      <title>Kaldi steps/nnet3/align.sh 中缺少对齐文件 (ali.*.gz) [关闭]</title>
      <link>https://stackoverflow.com/questions/78828864/missing-alignment-files-ali-gz-in-kaldi-steps-nnet3-align-sh</link>
      <description><![CDATA[我正在尝试使用 Kaldi 训练语音识别模型。我已成功运行 steps/nnet3/align.sh 脚本，但 exp/chain/tree_sp 目录中未创建预期的 ali.1.gz 文件。
我已检查 exp/chain/tree_sp/log 目录中的终端输出和日志文件，但没有错误消息。该脚本似乎运行正常，但缺少所需的输出。
您能否建议此问题的潜在原因或进一步调试的步骤？如何解决？
我已成功运行 steps/nnet3/align.sh 脚本（终端或日志文件中没有错误）。我已检查 exp/chain/tree_sp 目录中是否存在 ali.JOB.gz 文件，但它们不存在（未创建），并且在运行 local/chain/tuning/run_tdnn_1j.sh 时出现此错误
Traceback（最近一次调用最后一次）：
文件“/mnt/d/kaldi/egs/mini_librispeech/s5/steps/nnet3/chain/train.py”，第 651 行，在 main
train(args, run_opts)
文件“/mnt/d/kaldi/egs/mini_librispeech/s5/steps/nnet3/chain/train.py”，第 287 行，在 train
chain_lib.check_for_required_files(args.feat_dir, args.tree_dir,
文件&quot;/mnt/d/kaldi/egs/mini_librispeech/s5/steps/libs/nnet3/train/chain_objf/acoustic_model.py&quot;，第 378 行，在 check_for_required_files 中
raise Exception(&#39;预期 {0} 存在。&#39;.format(file))
异常：预期 exp/chain/tree_sp/ali.1.gz 存在。
]]></description>
      <guid>https://stackoverflow.com/questions/78828864/missing-alignment-files-ali-gz-in-kaldi-steps-nnet3-align-sh</guid>
      <pubDate>Sat, 03 Aug 2024 14:01:13 GMT</pubDate>
    </item>
    <item>
      <title>当数据集中的每个数据都是 csv 文件时，机器学习方法[关闭]</title>
      <link>https://stackoverflow.com/questions/78828422/machine-learning-methode-for-when-each-data-in-dataset-is-a-csv-file</link>
      <description><![CDATA[我正在使用传感器测量金属物体周围的磁流。传感器每毫秒记录一次磁流，每秒可进行 1000 次测量。这些值存储在 CSV 文件中，其中第一列表示沿 X 轴的测量值，第二列表示沿 Y 轴的测量值，第三列表示沿 Z 轴的测量值，第四列包含以毫秒为单位的时间。
我的任务是测量 50 种不同金属物体的磁流，为每个物体创建单独的 CSV 文件。最终，我计划将这些单独的文件用作训练机器学习模型的数据集。目标是使用机器学习和这些数据根据金属的磁流特性确定金属的类型（如果您感兴趣，可以搜索“mfl 方法”）。但是，我不确定如何处理这种特殊情况，因为大多数机器学习代码都要求数据集中的每个数据点都是一行。在这种情况下，每个数据点都是一个包含多行的 CSV 文件。
您能提供任何指导吗？
目前我不知道该怎么办]]></description>
      <guid>https://stackoverflow.com/questions/78828422/machine-learning-methode-for-when-each-data-in-dataset-is-a-csv-file</guid>
      <pubDate>Sat, 03 Aug 2024 10:09:57 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中有效地将大型 .txt 文件拆分为训练集和测试集？</title>
      <link>https://stackoverflow.com/questions/78827762/how-can-i-efficiently-split-a-large-txt-file-into-training-and-test-sets-in-pyt</link>
      <description><![CDATA[我有一个非常大的 .txt 文件（几 GB），我需要将其拆分为机器学习项目的训练集和测试集。由于内存限制，将整个文件读入内存然后拆分的常用方法不可行。我正在寻找一种高效拆分文件而不使内存过载的方法。
我尝试使用 scikit-learn 进行拆分，但它会将整个文件加载到内存中，这会导致性能问题，不适合我的大型数据集。]]></description>
      <guid>https://stackoverflow.com/questions/78827762/how-can-i-efficiently-split-a-large-txt-file-into-training-and-test-sets-in-pyt</guid>
      <pubDate>Sat, 03 Aug 2024 03:36:13 GMT</pubDate>
    </item>
    <item>
      <title>如何利用 Pytorch 的 CrossEntropyLoss 应用类权重来解决多类多输出问题的不平衡数据分类问题</title>
      <link>https://stackoverflow.com/questions/78823685/how-to-apply-class-weights-to-using-pytorchs-crossentropyloss-to-solve-an-imbal</link>
      <description><![CDATA[我正在尝试使用加权损失函数来处理数据中的类别不平衡问题。我的问题是多类别和多输出问题。例如（我的数据有五个输出/目标列（output_1、output_2、output_3），每个目标列有三个类（class_0、class_1 和 class_2）。我目前正在使用 pytorch 的交叉熵损失函数https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html，我看到它有一个权重参数，但我的理解是，这个相同的权重将统一应用于每个输出/目标，但我想在每个输出/目标中为每个类应用单独的权重。
具体来说，我可以获得如下所示的数据



A
B
C
D
E
OUTPUT_1
OUTPUT_2
OUTPUT_3




5.65
3.56
0.94
9.23
6.43
0
2
1


7.43
3.95
1.24
7.22
&lt; td&gt;2.66
0
0
0


9.31
2.42
2.91
2.64
6.28
2
0
2


8.19
5.12
1.32
3.12
8.41
0
2
0


9.35
1.92
3.12
4.13
3.14
0
1
1


8.43
9.72
7.23
8.29
9.18
1
0
2


4.32
2.12
3.84
9.42
8.19
0
1
0


3.92
3.91
2.90
8.1 9
8.41
2
0
2


7.89
1.92
4.12
8.19
7.28
0
1
2
&lt; /tr&gt;

5.21
2.42
3.10
0.31
1.31
2
0
0



因此，
输出 1 中的比例为：0 = 0.6、1 = 0.1、2 = 0.3
输出 2 中的比例为：0 = 0.4、1 = 0.3、2 = 0.3
输出 3 中的比例为：0 = 0.4、1 = 0.2、2 = 0.4

我想根据每个输出列中的类分布应用类权重，以便重新规范化（或重新平衡？不确定这里要使用的术语是什么）第 1 类为 0.15，第 0 类和第 2 类各为 0.425（因此对于 output_1，权重将是 [0.425/0.6, 0.15/0.1, 0.425/0.3]，对于输出 2，它将是 [0.425/0.4, 0.15/0.3, 0.425/0.3] 等）。相反，我理解 pytorch 的 crossentropy 损失函数中的权重参数目前正在执行的操作是将单个类权重应用于每个输出列。我想知道我是否遗漏了什么，是否有办法使用 pytorch 的 crossentropyloss 函数来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78823685/how-to-apply-class-weights-to-using-pytorchs-crossentropyloss-to-solve-an-imbal</guid>
      <pubDate>Fri, 02 Aug 2024 03:34:55 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 分类器的 SHAP 解释中 expected_value 的计算</title>
      <link>https://stackoverflow.com/questions/77126001/calculation-of-expected-value-in-shap-explanations-of-xgboost-classifier</link>
      <description><![CDATA[我们如何理解SHAP explainer.expected_value？为什么经过sigmoid变换后，它与y_train.mean()不一样？
下面是代码摘要，供快速参考。完整代码可在此笔记本中找到：https://github.com/MenaWANG/ML_toy_examples/blob/main/explain%20models/shap_XGB_classification.ipynb
model = xgb.XGBClassifier()
model.fit(X_train, y_train)
explainer = shap.Explainer(model)
shap_test = explainer(X_test)
shap_df = pd.DataFrame(shap_test.values)

#对于每种情况，如果我们将所有特征的 shap 值加上预期值相加，我们就可以得到该情况的边际，然后可以将其转换为返回该情况的预测概率case:
np.isclose(model.predict(X_test, output_margin=True),explainer.expected_value + shap_df.sum(axis=1))
#True

但是为什么下面不成立？为什么经过 sigmoid 变换后，XGBoost 分类器的 explainer.expected_value 与 y_train.mean() 不一样？
expit(explainer.expected_value) == y_train.mean()
#False
]]></description>
      <guid>https://stackoverflow.com/questions/77126001/calculation-of-expected-value-in-shap-explanations-of-xgboost-classifier</guid>
      <pubDate>Mon, 18 Sep 2023 09:28:55 GMT</pubDate>
    </item>
    </channel>
</rss>