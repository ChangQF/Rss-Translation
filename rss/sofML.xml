<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 04 Jun 2024 18:19:16 GMT</lastBuildDate>
    <item>
      <title>如何枚举 ML.NET 训练管道中的所有训练师？</title>
      <link>https://stackoverflow.com/questions/78576996/how-to-enumerate-over-all-trainers-in-an-ml-net-training-pipeline</link>
      <description><![CDATA[是否可以从 ML.NET 应用程序中的特定训练器类别中获取所有训练器并对它们进行迭代？例如，使类似这样的工作（没有）。
var allTrainers = new object[]
{
mlContext.MulticlassClassification.Trainers.SdcaNonCalibrated(),
mlContext.MulticlassClassification.Trainers.LbfgsMaximumEntropy()
};

foreach (var trainer in allTrainers)
{
var trainingPipeline = dataProcessPipeline
.Append(trainer)
.Append(mlContext.Transforms.Conversion.MapKeyToValue(&quot;PredictedLabel&quot;));
}

它们似乎没有使用可以放入列表并分配给 Append() 的通用基类型。]]></description>
      <guid>https://stackoverflow.com/questions/78576996/how-to-enumerate-over-all-trainers-in-an-ml-net-training-pipeline</guid>
      <pubDate>Tue, 04 Jun 2024 17:52:31 GMT</pubDate>
    </item>
    <item>
      <title>纵向数据的机器学习模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78576862/machine-learning-models-for-longitudinal-data</link>
      <description><![CDATA[我是机器学习的新手，只是想了解它在 R 中的实现，尤其是使用 tidymodels。
我有数据，从统计分析的角度来看，可以通过混合线性模型处理。但是，我正在探索 ML 算法以获得更好的预测。我正在考虑神经网络、SVM 等算法。我想知道是否有办法让这些算法考虑到多个记录来自同一单元的事实。我可以使用 tidymodels 执行这种类型的建模吗？
我不确定这是否是提出这个问题的最佳地点，但如果有人能告诉我正确的地方，我会很高兴。]]></description>
      <guid>https://stackoverflow.com/questions/78576862/machine-learning-models-for-longitudinal-data</guid>
      <pubDate>Tue, 04 Jun 2024 17:15:59 GMT</pubDate>
    </item>
    <item>
      <title>为什么MobileNetv2微调时Conv中的数组形状不匹配？</title>
      <link>https://stackoverflow.com/questions/78576851/why-the-shape-of-array-in-conv-when-fine-tuning-mobilenetv2-doesnt-match</link>
      <description><![CDATA[我正在尝试从检查点（https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet，请参阅“Mobilenet V2 Imagenet 检查点”下的“float_v2_1.4_224”）对 MobileNetv2 模型进行微调，以完成使用 Deeplabv3 进行图像分割的任务。我按照 deeplab repo 中描述的方法，将 pascal VOC 数据集替换为我自己的数据集，该数据集由大小为 224x224 的图像组成（与用于训练该检查点的图像大小相同），然后使用此脚本作为参考，以确定要设置哪些标志来训练 mobilenetv2。（https://github.com/tensorflow/models/blob/master/research/deeplab/local_test_mobilenetv2.sh）
这是我正在使用的命令：
 python deeplab/train.py
--logtostderr 
--train_split=&quot;train&quot; 
--model_variant=&quot;mobilenet_v2&quot;
--output_stride=16
--train_crop_size=&quot;225,225&quot; #图像为 224x224，因此根据 tensorflow/research/deeplab 常见问题解答将其设置为此尺寸
--train_batch_size=4 
--training_number_of_steps=100
--fine_tune_batch_norm=true
--dataset=&quot;custom&quot; #正确定义为其自己的数据集
--tf_initial_checkpoint=&quot;./deeplab/data_resized/initial_checkpoint/mobilenet_v2_1.4_224.ckpt&quot;
--train_logdir=&quot;./deeplab/data_resized/checkpoint&quot;
--dataset_dir=&quot;./deeplab/data_resized/ltfrecord&quot;

但我一直收到以下错误：
 回溯（最近一次调用）：
文件 &quot;deeplab/train.py&quot;，第 464 行，位于 &lt;module&gt;
tf.app.run()
文件“C:\Users\Luca\.pyenv\pyenv-win\versions\3.7.6\lib\site-packages\tensorflow_core\python\platform\app.py”，第 40 行，运行中
_run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
文件“C:\Users\Luca\.pyenv\pyenv-win\versions\3.7.6\lib\site-packages\absl\app.py”，第 308 行，运行中
_run_main(main, args)
文件“C:\Users\Luca\.pyenv\pyenv-win\versions\3.7.6\lib\site-packages\absl\app.py”，第 254 行，运行中
sys.exit(main(argv))
文件 &quot;deeplab/train.py&quot;，第 444 行，在 main
ignore_missing_vars=True)
文件 &quot;C:\Users\Luca\Documents\marathon\facade_ml\training_gitclone\models-master\research\deeplab\utils\train_utils.py&quot;，第 221 行，在 get_model_init_fn
ignore_missing_vars=ignore_missing_vars)
文件 &quot;C:\Users\Luca\.pyenv\pyenv-win\versions\3.7.6\lib\site-packages\tensorflow_core\contrib\framework\python\ops\variables.py&quot;，第 690 行，在assign_from_checkpoint
(ckpt_name, str(ckpt_value.shape), str(var.get_shape())))
ValueError: Total对于 MobilenetV2/Conv/weights，新数组的大小必须保持不变 lh_shape: [(3, 3, 3, 48)], rh_shape: [(3, 3, 3, 32)]

我尝试将其他配置值（例如将“initialize_last_layer”更改为 false 并将“last_layers_contain_logits_only”更改为 false）更改为 false，但这没有帮助，而且 Google 和 GitHub 上似乎都不存在有关此问题的信息。一位用户建议将“depth_multiplier”标志更改为 0.5，但除了将 rh 形状更改为 [3,3,3,16] 之外，它没有任何效果。
感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78576851/why-the-shape-of-array-in-conv-when-fine-tuning-mobilenetv2-doesnt-match</guid>
      <pubDate>Tue, 04 Jun 2024 17:13:08 GMT</pubDate>
    </item>
    <item>
      <title>TrOCR 模型无法解释文本</title>
      <link>https://stackoverflow.com/questions/78576566/trocr-model-fails-to-interpret-text</link>
      <description><![CDATA[我尝试按照各种教程使用 HuggingFace 的 TrOCR 模型来读取收据中的文本。这与 SROIE 数据集的预训练任务相匹配，因此它应该表现得非常好，但即使我尝试在训练数据上运行生成，我也遇到了一些问题。
按照本教程，对裁剪文本的推理效果非常好 https://learnopencv.com/trocr-getting-started-with-transformer-based-ocr/。处理器将一小部分文本转换为 384x384 相位包裹图像。大多数文章将预处理器描述为调整大小 + 规范化，但显然还有更多的事情要做。


生成效果非常好：

“而且斗争只能持续一段时间”

当我尝试在 &gt;384x384 的收据图像上运行代码时，它确实只是重新缩放和标准化，并存在一些量化（？）问题：

预测不太好...

TAX

令人惊讶的是，只生成了 4 个 token（model.config 显示最大值为 20 个 token，我假设不能超过这个数字）。设置 max_new_tokens 似乎不会增加生成的 token 数量。增加 min_new_tokens 会产生乱码。
示例图像 - https://drive.google.com/file/d/1MMa-Nnw2yjoiRymoXw8b7MdgMD3oj-CH/view?usp=drive_link
model_name = &#39;microsoft/trocr-large-printed&#39;
processor = TrOCRProcessor.from_pretrained(model_name)
model = VisionEncoderDecoderModel.from_pretrained(model_name).to(&#39;cuda&#39;)
pixel_values = process(image, return_tensors=&#39;pt&#39;).pixel_values.to(&#39;cuda&#39;)
generated_ids = model.generate(pixel_values)
generated_text = processing.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(generated_ids)
print(generated_text)

有人能解释一下吗：

预处理中会发生什么以及如何修复？我应该在补丁中运行预测，还是可以进行微调？
为什么只有 4 个 token，max_tokens=20
还有其他突出的问题吗？
]]></description>
      <guid>https://stackoverflow.com/questions/78576566/trocr-model-fails-to-interpret-text</guid>
      <pubDate>Tue, 04 Jun 2024 16:03:45 GMT</pubDate>
    </item>
    <item>
      <title>skopt.gbrt_minimize（梯度提升回归树）——如果损失函数不可微，如何计算其梯度？</title>
      <link>https://stackoverflow.com/questions/78576495/skopt-gbrt-minimize-gradient-boosted-regression-trees-how-does-it-calculate</link>
      <description><![CDATA[我正在为 Jansen &amp; Rit 全脑模型优化 11 个参数，如下所示：
search_space = [
Integer(1, 10, name=&quot;A&quot;), Integer(10, 40, name=&quot;B&quot;), Integer(90, 200, name=&quot;C&quot;), 等...
]
gbrt_minimize(find_bold_loss, search_space, n_calls=80, n_initial_points=32, initial_point_generator=&#39;sobol&#39;)。
我不明白的是，我的损失函数非常复杂且不可微，因为它首先需要运行模型（使用欧拉方法并在每次迭代中引入高斯噪声）。那么，如果 gbrt 无法计算每次迭代的损失梯度，它如何工作？我对 skopt 和 GBRT 还不熟悉，所以如果能提供一些深入的解释就更好了！
注意：我已经用 gbrt 运行了模型，效果很好（比贝叶斯优化更好），所以它一定做对了什么……]]></description>
      <guid>https://stackoverflow.com/questions/78576495/skopt-gbrt-minimize-gradient-boosted-regression-trees-how-does-it-calculate</guid>
      <pubDate>Tue, 04 Jun 2024 15:50:01 GMT</pubDate>
    </item>
    <item>
      <title>OperatorNotAllowedInGraphError</title>
      <link>https://stackoverflow.com/questions/78576397/operatornotallowedingrapherror</link>
      <description><![CDATA[我正在使用索引和批处理大小提取一批数据，然后调用train_on_batch方法计算损失。但是，发生了以下错误：

-----------------------------------------------------------------------------------
OperatorNotAllowedInGraphError Traceback (most recent call last)
Cell In[3]，第 1 行
----&gt; 1 labels=c.cluster(X, y=None, iter_max=1e4)

文件 d:\ML5\keras_sdec.py:405，位于 DeepEmbeddingClustering.cluster(self, X, y, tol, update_interval, iter_max, save_interval, **kwargs)
401 self.DEC.loss = SDEC.sdec_loss(
402 add_loss=SDEC.add_loss(self.encoder.predict(X_batch),
403 a_batch))
404 # print(self.p.dtype)
--&gt; 405 loss = self.DEC.train_on_batch(X_batch, p_batch)
406 sys.stdout.write(&#39;Loss %f&#39; % loss)
407 index += 1

文件 d:\anaconda\envs\pytorch\Lib\site-packages\keras\src\backend\tensorflow\trainer.py:540，位于 TensorFlowTrainer.train_on_batch(self, x, y, sample_weight, class_weight, return_dict)
537 def data():
538 Yield (x, y, sample_weight)
--&gt; 540 logs = self.train_function(data())
541 logs = tree.map_structure(lambda x: np.array(x), logs)
542 if return_dict:

文件 d:\anaconda\envs\pytorch\Lib\site-packages\tensorflow\python\util\traceback_utils.py:153，在 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 中
151 except Exception as e:
152filtered_tb = _process_traceback_frames(e.__traceback__)
--&gt; 153 raise e.with_traceback(filtered_tb) from None
...
---&gt; 28 self.name = name 或 auto_name(self.__class__.__name__)
29 self.reduction = standardize_reduction(reduction)
30 self.dtype = dtype 或 backend.floatx()

OperatorNotAllowedInGraphError：不允许将符号 `tf.Tensor` 用作 Python `bool`。您可以尝试以下方法解决该问题：如果您在 Graph 模式下运行，请使用 Eager 执行模式或使用 @tf.function 修饰此函数。如果您使用的是 AutoGraph，您可以尝试使用 @tf.function 修饰此函数。如果这不起作用，那么您可能使用了不受支持的功能，或者您的源代码可能对 AutoGraph 不可见。有关更多信息，请参阅 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code。
输出被截断。作为可滚动元素查看或在文本编辑器中打开。调整单元格输出设置...

这是我遇到错误的代码。
else:
X_batch=X[index*self.batch_size:(index+1) * self.batch_size]
a_batch=a[index*self.batch_size:(index+1) * self.batch_size, index*self.batch_size:(index+1) * self.batch_size]
p_batch=self.p
p_batch=p_batch[index*self.batch_size:(index+1) * self.batch_size]

self.DEC.loss = SDEC.sdec_loss(
add_loss=SDEC.add_loss(self.encoder.predict(X_batch),
a_batch))
# print(self.p.dtype)
loss = self.DEC.train_on_batch(X_batch, p_batch)
sys.stdout.write(&#39;损失 %f&#39; % 损失)
index += 1
]]></description>
      <guid>https://stackoverflow.com/questions/78576397/operatornotallowedingrapherror</guid>
      <pubDate>Tue, 04 Jun 2024 15:31:47 GMT</pubDate>
    </item>
    <item>
      <title>如果我的模型检测到多个物体（有时是假的），我该如何使用 mAP、精度、召回率和 f1 验证我的 mask rcnn 模型</title>
      <link>https://stackoverflow.com/questions/78576156/how-do-i-validate-my-mask-rcnn-model-with-map-precision-recall-and-f1-if-my-mo</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78576156/how-do-i-validate-my-mask-rcnn-model-with-map-precision-recall-and-f1-if-my-mo</guid>
      <pubDate>Tue, 04 Jun 2024 14:49:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么会出现此错误？“TransformerMixin.fit_transform() 缺少 1 个必需的位置参数：‘X’”</title>
      <link>https://stackoverflow.com/questions/78575884/why-is-this-error-coming-up-transformermixin-fit-transform-missing-1-require</link>
      <description><![CDATA[我有这行代码
cols_to_scale = [&#39;tenure&#39;,&#39;MonthlyCharges&#39;,&#39;TotalCharges&#39;]

来自 sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler

df2[cols_to_scale] = scaler.fit_transform(df2[cols_to_scale])

这会引发此错误，为什么？
TypeError Traceback (most recent call last)
Cell In[77], line 6
3 来自 sklearn.preprocessing import MinMaxScaler
4 scaler = MinMaxScaler
----&gt; 6 df2[cols_to_scale] = scaler.fit_transform(df2[cols_to_scale])

TypeError: TransformerMixin.fit_transform() 缺少 1 个必需的位置参数：“X”

不知道为什么会发生此错误。]]></description>
      <guid>https://stackoverflow.com/questions/78575884/why-is-this-error-coming-up-transformermixin-fit-transform-missing-1-require</guid>
      <pubDate>Tue, 04 Jun 2024 13:58:35 GMT</pubDate>
    </item>
    <item>
      <title>PySpark 错误要求失败：A 和 B 维度不匹配</title>
      <link>https://stackoverflow.com/questions/78575590/pyspark-error-requirement-failed-a-b-dimension-mismatch</link>
      <description><![CDATA[我正在使用 PySpark 中的房屋贷款还款数据构建神经网络。执行了 EDA、数据预处理和特征工程步骤，但当涉及到模型时，我收到错误：

java.lang.IllegalArgumentException：要求失败：A 和 B 维度不匹配！

代码如下：
# 要转换为数字的列的列表 -&gt;解决不支持字符串的错误
columns_to_cast = [&#39;AMT_REQ_CREDIT_BUREAU_HOUR&#39;, &#39;AMT_REQ_CREDIT_BUREAU_DAY&#39;, &#39;AMT_REQ_CREDIT_BUREAU_WEEK&#39;,
&#39;AMT_REQ_CREDIT_BUREAU_MON&#39;, &#39;AMT_REQ_CREDIT_BUREAU_QRT&#39;, &#39;AMT_REQ_CREDIT_BUREAU_YEAR&#39;]

for col_name in columns_to_cast:
train_df = train_df.withColumn(col_name, col(col_name).cast(&quot;double&quot;))
test_df = test_df.withColumn(col_name, col(col_name).cast(&quot;double&quot;))

print(&quot;转换完成。正在检查schema:&quot;)
train_df.printSchema()

print(&quot;组装特征向量...&quot;)
# 组装特征向量
ohe_columns = [col + &#39;_ohe&#39; for col in categorical_columns]
numerical_columns = [col for col in train_df.columns if col not in ohe_columns + [&#39;SK_ID_CURR&#39;, &#39;TARGET&#39;]]

print(f&quot;数值列：{numerical_columns}&quot;)
print(f&quot;独热编码列：{ohe_columns}&quot;)

assembler = VectorAssembler(inputCols=numerical_columns + ohe_columns, outputCol=&quot;features&quot;)
train_df = assembler.transform(train_df)
test_df = assembler.transform(test_df)
print(&quot;特征向量组装完成。&quot;)

print(&quot;检查组装的特征向量模式：&quot;)
train_df.select(&quot;features&quot;).show(5, truncate=False)

print(&quot;缩放特征...&quot;)
# 缩放特征
scaler = StandardScaler(inputCol=&quot;features&quot;, outputCol=&quot;scaled_features&quot;)
scaler_model = scaler.fit(train_df)
train_df = scaler_model.transform(train_df)
test_df = scaler_model.transform(test_df)
print(&quot;缩放完成。&quot;)

print(&quot;检查缩放的特征模式：&quot;)
train_df.select(&quot;scaled_features&quot;).show(5, truncate=False)

print(&quot;定义神经网络结构...&quot;)
# 定义神经网络的层
num_features = len(numerical_columns + ohe_columns)
print(f&quot;输入特征数量：{num_features}&quot;)

layers = [
num_features, # 输入特征数量
64, # 隐藏层大小
32, # 隐藏层大小
2 # 类数
]

print(f&quot;神经网络层数：{layers}&quot;)

# 初始化多层感知器分类器
mlp = MultilayerPerceptronClassifier(
featuresCol=&#39;scaled_features&#39;,
labelCol=&#39;TARGET&#39;,
maxIter=100,
layer=layers,
blockSize=128,
seed=1234
)

print(&quot;训练model...&quot;)
# 训练模型
mlp_model = mlp.fit(train_df)
print(&quot;模型训练完成。&quot;)

print(&quot;对训练集进行预测...&quot;)
# 对训练集进行预测（用于评估目的）
train_predictions = mlp_model.transform(train_df)

print(&quot;评估模型...&quot;)
# 评估模型
evaluator = BinaryClassificationEvaluator(labelCol=&#39;TARGET&#39;, rawPredictionCol=&#39;rawPrediction&#39;, metricName=&#39;areaUnderROC&#39;)
auc_train = evaluator.evaluate(train_predictions)
print(f&#39;Training AUC: {auc_train}&#39;)

print(&quot;对测试集进行预测...&quot;)
# 对测试集进行预测
test_predictions = mlp_model.transform(test_df)

# 显示预测
test_predictions.select(&#39;SK_ID_CURR&#39;, &#39;prediction&#39;, &#39;probability&#39;).show()

print(&quot;准备提交文件...&quot;)
# 准备提交文件
submission = test_predictions.select(&#39;SK_ID_CURR&#39;, &#39;prediction&#39;)
submission.show()

# 将提交保存到 CSV 文件
submission.write.csv(&#39;./prediction&#39;, header=True)
print(&quot;提交文件已保存。&quot;)

请注意，我添加了打印语句来帮助我识别错误发生的位置，并且似乎错误是在评估过程中产生的。
打印语句：
定义神经网络结构...
输入特征数量：86
神经网络层：[86, 64, 32, 2]
训练模型...
模型训练完成。
对训练集进行预测...
评估模型...
（错误出现在这里）
这是什么意思？]]></description>
      <guid>https://stackoverflow.com/questions/78575590/pyspark-error-requirement-failed-a-b-dimension-mismatch</guid>
      <pubDate>Tue, 04 Jun 2024 13:07:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 和 opencv 计算盒子的尺寸</title>
      <link>https://stackoverflow.com/questions/78575461/calculate-the-dimensions-of-a-box-using-python-and-opencv</link>
      <description><![CDATA[我正在使用 Python 绘制盒子照片的轮廓，并使用 opencv 在盒子的轮廓上画线，我试图做的是计算，如附图所示。
绘制另一个不同颜色的轮廓并标记顶点 (tl、tr、br、bl)。
我正在尝试计算宽度 (BL + BR)、高度 (BL、TL) 和长度 (TL、TR)。
但是这样做时，它会计算出不同的大小，并采用其他参考。
我正在尝试计算框的大小，尝试获取宽度、高度和长度。
import cv2
import numpy as np
from scipy.spatial import distance as dist

def tup(point):
return (int(point[0]), int(point[1]))

# 按正确顺序排列点的函数
def order_points(pts):
rect = np.zeros((4, 2), dtype=&quot;float32&quot;)
s = pts.sum(axis=1)
rect[0] = pts[np.argmin(s)]
rect[2] = pts[np.argmax(s)]

diff = np.diff(pts, axis=1)
rect[1] = pts[np.argmin(diff)]
rect[3] = pts[np.argmax(diff)]

return rect

# 加载图像
img = cv2.imread(&quot;boxing.jpg&quot;)

# 缩小尺寸以适合屏幕
scale = 0.2
h, w = img.shape[:2]
h = int(scale * h)
w = int(scale * w)
img = cv2.resize(img, (w, h))
copy = np.copy(img)

# 转换为 hsv
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
h, s, v = cv2.split(hsv)

# 制作遮罩
mask = cv2.inRange(s, 30, 255)

# 扩大和侵蚀以去除小的孔
kernel = np.ones((5, 5), np.uint8)
mask = cv2.dilate(mask, kernel, iterations=1)
mask = cv2.erode(mask, kernel, iterations=1)

# 查找轮廓
contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
contour = max(contours, key=cv2.contourArea) # 使用最大轮廓

# 将轮廓近似为多边形
epsilon = 0.02 * cv2.arcLength(contour, True)
approx = cv2.approxPolyDP(contour, epsilon, True)

# 绘制轮廓
cv2.drawContours(img, [approx], -1, (0, 0, 200), 2)

# 计算框尺寸
if len(approx) == 6:
pts = approx.reshape(6, 2)

# 对点进行排序
rect = order_points(pts[:4])

# 计算尺寸
(tl, tr, br, bl) = rect
widthA = dist.euclidean(br, bl)
widthB = dist.euclidean(tr, tl)
heightA = dist.euclidean(tr, br)
heightB = dist.euclidean(tl, bl)

maxWidth = max(int(widthA), int(widthB))
maxHeight = max(int(heightA), int(heightB))

print(f&quot;Width: {maxWidth}px&quot;)
print(f&quot;Height: {maxHeight}px&quot;)

# 绘制带标签的点和线
colors = [(0, 255, 0), (0, 255, 255), (255, 0, 255), (255, 255, 0)]
标签 = [&quot;TL&quot;, &quot;TR&quot;, &quot;BR&quot;, &quot;BL&quot;]

对于 i，点在 enumerate(rect) 中：
cv2.circle(img, tup(point), 5, colors[i], -1)
cv2.putText(img, labels[i], tup(point), cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[i], 2)

cv2.line(img, tup(tl), tup(tr), (0, 255, 0), 2)
cv2.line(img, tup(tr), tup(br), (0, 255, 0), 2)
cv2.line(img, tup(br), tup(bl), (0, 255, 0), 2)
cv2.line(img, tup(bl), tup(tl), (0, 255, 0), 2)

# 显示
cv2.imshow(&quot;Image&quot;, img)
cv2.waitKey(0)
cv2.destroyAllWindows()

使用的图片：

处理后的图片：
]]></description>
      <guid>https://stackoverflow.com/questions/78575461/calculate-the-dimensions-of-a-box-using-python-and-opencv</guid>
      <pubDate>Tue, 04 Jun 2024 12:40:25 GMT</pubDate>
    </item>
    <item>
      <title>Early Stopping 在 Colab 上没有指标</title>
      <link>https://stackoverflow.com/questions/78572180/early-stopping-does-not-have-metrics-on-colab</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78572180/early-stopping-does-not-have-metrics-on-colab</guid>
      <pubDate>Mon, 03 Jun 2024 19:17:59 GMT</pubDate>
    </item>
    <item>
      <title>YoloV3-Tiny 训练时间</title>
      <link>https://stackoverflow.com/questions/78571568/yolov3-tiny-training-time</link>
      <description><![CDATA[我是 AI 新手，正在将 Cat 检测器作为一个项目进行训练。我目前正在 Google Colab 上工作，已经完成了初步训练，并且拥有用于进一步训练的最佳权重。我遇到的问题是我的新数据集大约有 9500 张图像，它给出的完成时间估计大约为 400 小时。我可以做哪些更改来减少时间？我只有 1 个类，这是我当前使用的 cfg：
批次为 64
细分为 16
最大批次为 150000
步骤为 120000 到 135000
除过滤器和类别外，所有其他设置均为默认设置，这些设置均已正确更改。
如果您需要更多信息，请告诉我。任何帮助都将不胜感激！
我尝试过调整批次大小、最大批次大小、细分和步骤，但并没有取得太大成功。]]></description>
      <guid>https://stackoverflow.com/questions/78571568/yolov3-tiny-training-time</guid>
      <pubDate>Mon, 03 Jun 2024 16:38:48 GMT</pubDate>
    </item>
    <item>
      <title>处理图像以提取神经网络的手写文本</title>
      <link>https://stackoverflow.com/questions/78571522/process-the-image-to-extract-handwritten-text-for-the-neural-network</link>
      <description><![CDATA[我想识别图像中的方块并提取其中的字母，但我无法获得整个字母。我只得到像下面显示的图像中的片段。
img = cv2.imread(img_path)
img_path = &quot;img.png&quot;

gray = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY)

# 平滑边缘以方便锐化
blur = cv2.GaussianBlur(gray, (5, 5), 0)
# sharpen_kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
sharpen_kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
sharpen = cv2.filter2D(blur, -1, sharpen_kernel)

# 应用自适应阈值创建二值图像并缩小间隙
thresh = cv2.adaptiveThreshold(sharpen, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV, 11, 2)
kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
close = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=2)

# 检测轮廓并根据面积大小进行过滤
contours = cv2.findContours(close, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)
contours = contours[0] if len(contours) == 2 else contours[1]
min_area = 350
max_area = 6200
image_number = 0
lower_blue = np.array([90, 50, 20]) 
upper_blue = np.array([150, 255, 255]) 
# 避免检测到的小/大随机轮廓
contours = filter(lambda contour: min_area &lt; cv2.contourArea(contour) &lt; max_area, contours)
# 对轮廓进行排序，使其变成“线”
sorted_contours = sorted(contours, key=lambda contour: cv2.boundingRect(contour)[0])
selected_rects = []
for c in sorted_contours:
x, y, w, h = cv2.boundingRect(c)
# 确保矩形不与已选定的矩形重叠
if not any(x &gt; rect[0] and y &gt; rect[1] and x + w &lt; rect[0] + rect[2] and y + h &lt; rect[1] + rect[3] for rect in selected_rects):
ROI = image[y:y + h, x:x + w]
ROI = cv2.GaussianBlur(ROI, (3, 3), 0)
hsv = cv2.cvtColor(ROI, cv2.COLOR_BGR2HSV)
mask = cv2.inRange(hsv, lower_blue, upper_blue)
blue_pixels = cv2.countNonZero(mask)
# 确保矩形内有蓝色，以避免出现随机矩形
if blue_pixels &gt; 100：
binary_image = cv2.resize(mask, (28, 28))
cv2.imwrite(f&#39;./resources/ROI_{image_number}.png&#39;, binary_image)
image_number += 1
selected_rects.append((x, y, w, h))

初始图像

一些提取的 ROI

不完整的蒙版

我尝试移除灰色边框，然后将所有非白色的像素转换为亮蓝色。但是，我无法在每种情况下都移除所有边框，而且有时由于图像是扫描的，所以会出现随机的黄色斑点。]]></description>
      <guid>https://stackoverflow.com/questions/78571522/process-the-image-to-extract-handwritten-text-for-the-neural-network</guid>
      <pubDate>Mon, 03 Jun 2024 16:27:31 GMT</pubDate>
    </item>
    <item>
      <title>有人能解释一下为什么会出现“FileNotFoundError：[Errno 2] 没有这样的文件或目录：'music_recommender.joblib'”错误吗？</title>
      <link>https://stackoverflow.com/questions/78566797/can-anyone-please-explain-why-the-error-filenotfounderror-errno-2-no-such-f</link>
      <description><![CDATA[这是我在 jupyter 上实现的代码
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import joblib

music_data=pd.read_csv(&#39;music.csv&#39;)
x=music_data.drop(columns=[&#39;genre&#39;])
y=music_data[&#39;genre&#39;]
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)
model=DecisionTreeClassifier()
model.fit(x_train,y_train)
model=joblib.load(&#39;music_recommender.joblib&#39;)

prediction=model.predict(x_test)
score=accuracy_score(y_test,prediction)
score

运行代码后出现此错误，请解释此处发生的情况
FileNotFoundError Traceback（最近一次调用最后一次）
Cell In[50]，第 13 行
---&gt; 13 model=joblib.load(&#39;music_recommender.joblib&#39;)
FileNotFoundError：[Errno 2] 没有这样的文件或目录：&#39;music_recommender.joblib&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/78566797/can-anyone-please-explain-why-the-error-filenotfounderror-errno-2-no-such-f</guid>
      <pubDate>Sun, 02 Jun 2024 14:56:17 GMT</pubDate>
    </item>
    <item>
      <title>使用单类 SVM 计算异常检测的异常分数</title>
      <link>https://stackoverflow.com/questions/53956538/calculating-anomaly-score-for-anomaly-detection-using-one-class-svm</link>
      <description><![CDATA[我对使用单类 SVM 计算异常检测的异常分数有疑问。我的问题是：如何使用 decision_function(X) 计算它，就像我在隔离森林中计算异常分数一样？
非常感谢，]]></description>
      <guid>https://stackoverflow.com/questions/53956538/calculating-anomaly-score-for-anomaly-detection-using-one-class-svm</guid>
      <pubDate>Fri, 28 Dec 2018 09:50:20 GMT</pubDate>
    </item>
    </channel>
</rss>