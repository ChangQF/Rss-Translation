<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 06 Mar 2024 21:12:10 GMT</lastBuildDate>
    <item>
      <title>为什么我的训练损失和验证损失非常低，但新数据集的均方误差却很高？</title>
      <link>https://stackoverflow.com/questions/78116573/why-my-training-loss-and-validation-loss-very-low-but-the-mse-for-a-new-dataset</link>
      <description><![CDATA[来自tensorflow.keras导入层
从tensorflow.keras.layers导入Dense
从tensorflow.keras.layers导入LSTM
从tensorflow.keras.models导入顺序
从tensorflow.keras导入正则化器


模型=顺序（[
    LSTM(单位=64, input_shape=(len(x.columns),1), kernel_regularizer=regularizers.l2(0.01)),
    密集(1)
]）

model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizationr=tf.keras.optimizers.Adam())
历史=模型.fit（x_train，y_train，validation_data =（x_val，y_val），epochs = 100，verbose = 1，batch_size = 32）
mse=model.evaluate(x_test, y_test)
打印（毫秒）

这是我得到的损失图
我得到的值非常高（30.2344）]]></description>
      <guid>https://stackoverflow.com/questions/78116573/why-my-training-loss-and-validation-loss-very-low-but-the-mse-for-a-new-dataset</guid>
      <pubDate>Wed, 06 Mar 2024 18:14:44 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 与 RTX 4090 在训练时间上存在巨大差异 [关闭]</title>
      <link>https://stackoverflow.com/questions/78116561/pytorch-with-rtx-4090-huge-difference-in-training-time</link>
      <description><![CDATA[我在大学计算机上用 PyTorch 训练了一个模型 48 个 epoch，大约需要 8 分钟。
我现在在家装了一台新电脑，但训练时间明显更长，48 个 epoch 大约需要 17.5 分钟。
规格：

大学：

RTX 4090
i9-13900KF
64GB 内存


首页：

RTX 4090
i9-14900KF
48GB 内存



我在两者上使用相同的代码，并且没有更改训练的任何其他参数。
目前我已经安装：

CUDA 版本 12.4
驱动程序版本 551.61
火炬版本2.2.1+cu121
Python 3.11

在训练期间，没有一个组件得到充分利用：

内存 20/48GB
CPU 33%（70°C）
光盘 1%
GPU 50% (40°C)
显存 3.2/24GB

我使用 Cinebench、MemTest86 和 3DMark 测试了我的 CPU、GPU 和 RAM，所有测试结果均符合预期，因此硬件应该可以正常工作。游戏过程中的表现也符合预期。温度也还可以。
除了 PyCharm 之外，只有 Discord 和 Opera 正在运行，这不会显着降低训练速度。
我认为驱动程序可能有问题，或者 torch、cuda 和驱动程序版本之间的兼容性有问题？
您可以在此处访问测试代码。
训练在train.py中开始。]]></description>
      <guid>https://stackoverflow.com/questions/78116561/pytorch-with-rtx-4090-huge-difference-in-training-time</guid>
      <pubDate>Wed, 06 Mar 2024 18:12:11 GMT</pubDate>
    </item>
    <item>
      <title>强化学习神经网络概率没有改变</title>
      <link>https://stackoverflow.com/questions/78116374/reinforcement-learning-neural-network-probabilities-arent-changing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78116374/reinforcement-learning-neural-network-probabilities-arent-changing</guid>
      <pubDate>Wed, 06 Mar 2024 17:35:37 GMT</pubDate>
    </item>
    <item>
      <title>在 MLJ 中找不到适合我的数据类型的模型</title>
      <link>https://stackoverflow.com/questions/78116043/can-not-find-a-model-in-mlj-suitable-for-my-data-type</link>
      <description><![CDATA[我想在 MLJ 中使用 RandomForestRegressor，但我的数据类型总是出现错误。
这是我的数据类型：
# X 的类型
500×1矩阵{Float64}：
 0.002
 0.004
 0.006
 0.008
 0.01
 ⋮
 0.992
 0.994
 0.996
 0.998
 1.0

#y 的类型
500 元素向量{任意}：
 [-26.12471826402402、-0.00019238911241092893、-8.487354458186491e-6、-4.247928689027347e-6、-1.4735850473179823e-6、-5.5989652116 83904e-6]
 [-26.124690243484718、-2.813962224679223e-5、0.00014993993007544892、-3.972667350815584e-5、-4.415345127384285e-5、4.7755059188 858695e-5]
 [-26.124700578434716、0.00020304547366412073、-1.941266595367752e-5、-6.8228290228677935e-6、-2.8075749891054436e-6、-5.95726165 1756696e-7]
 [-26.12470127817773、-0.0002052884039169811、-1.53​​54293043945422e-5、5.457931474439626e-6、-1.0654739228677101e-6、1.3601831927 445573e-6]
 [-26.124689854622336、-0.0001879523626138191、5.983618996563411e-6、-2.412535654516823e-5、1.7942953590588395e-5、-1.3323422657 25206e-5]
 ⋮
 [-26.12470204971627、-0.00020642317364671925、-1.6756339903056805e-5、7.454364967629523e-6、-2.539052483596649e-6、1.8465582632 964939e-6]
 [-26.12469951981032、0.00020256742611590717、-1.9380595701112835e-5、-6.199351626712257e-6、-3.5935735922532075e-6、-1.153947443 044423e-6]
 [-26.124700578434716、0.00020304547366412073、-1.9412665953733033e-5、-6.8228290228677935e-6、-2.8075749891054436e-6、-5.9572616 58418034e-7]
 [-26.124701332004584、-0.00020670402366529395、-1.6873028843567006e-5、7.897781219456945e-6、-3.3466380342517255e-6、1.43543246 5890507e-6]
 [-26.124682551931944、0.0001869595100869592、5.027911548549646e-6、1.9320833163805062e-5、1.57188​​013177878e-5、6.48586574647 5028e-6]

上面的数据可以在DecisionTree.jl中使用，但在MLJ.jl中不起作用。
这是我的 DecisionTree.jl 代码，它运行良好
模型 = build_forest(y, X)
apply_forest（模型，[0.75]）

这是我的 MLJ.jl 代码。
RandomForest = MLJ.@load RandomForestRegressor pkg = DecisionTree
myrandforest = 随机森林()
马赫=机器（myrandforest，X，y）|&gt; MLJ.适合！

和错误消息：
导入 MLJDecisionTreeInterface ✔
┌ 警告：数据参数的数量和/或类型与指定模型不匹配
│ 支持。通过指定“scitype_check_level=0”来抑制此类型检查。
│
│ 运行“@doc DecisionTree.RandomForestRegressor”以了解有关模型要求的更多信息。
│
│ 通常但非唯一地，监督模型是使用以下语法构建的
│ `machine(model, X, y)` 或 `machine(model, X, y, w)` 而大多数其他模型是
│ 用 `machine(model, X)` 构造。这里“X”是特征，“y”是目标，“w”
│ 样本或类别权重。
│
│ 一般来说，`machine(model, data...)`中的数据预计满足
│
│ scitype(数据) &lt;: MLJ.fit_data_scitype(模型)
│
│ 在本案中：
│
│ scitype(数据) = Tuple{AbstractMatrix{连续}, AbstractVector{AbstractVector{连续}}}
│
│ fit_data_scitype(model) = Tuple{Table{&lt;:Union{AbstractVector{&lt;:连续}, AbstractVector{&lt;:Count}, AbstractVector{&lt;:OrderedFactor}}}, AbstractVector{连续}}
└ @ MLJBase ~/.julia/packages/MLJBase/eCnWm/src/machines.jl:231
错误：ArgumentError：“Matrix{Float64}”不是表；有关将 AbstractVecOrMat 视为表的方法，请参阅“?Tables.table”
堆栈跟踪：
  [1] 列(m::Matrix{Float64})
    @表〜/.julia/packages/Tables/NSGZI/src/matrix.jl:6
  [2] Tables.Columns(x::Matrix{Float64})
    @表〜/.julia/packages/Tables/NSGZI/src/Tables.jl:271
  [3] 矩阵(表::Matrix{Float64};转置::Bool)
    @表〜/.julia/packages/Tables/NSGZI/src/matrix.jl:85
  [4] 矩阵(表::矩阵{Float64})
    @表〜/.julia/packages/Tables/NSGZI/src/matrix.jl:84
  [5] 重新格式化(::MLJDecisionTreeInterface.RandomForestRegressor, X::Matrix{Float64}, y::Vector{Any})
    @ MLJDecisionTreeInterface ~/.julia/packages/MLJDecisionTreeInterface/kPIDf/src/MLJDecisionTreeInterface.jl:460
  [6] fit_only!(mach::Machine{…}; rows::Nothing，verbosity::Int64，force::Bool，composite::Nothing)
    @ MLJBase ~/.julia/packages/MLJBase/eCnWm/src/machines.jl:659
  [7] 只适合！
    @ MLJBase ~/.julia/packages/MLJBase/eCnWm/src/machines.jl:607 [内联]
  [8] #适合！#63
    @ MLJBase ~/.julia/packages/MLJBase/eCnWm/src/machines.jl:778 [内联]
  [9] 适合！
    @ MLJBase ~/.julia/packages/MLJBase/eCnWm/src/machines.jl:775 [内联]
 [10] |&gt;(x::Machine{MLJDecisionTreeInterface.RandomForestRegressor, true}, f::typeof(StatsAPI.fit!))
    @基地./operators.jl:917
 [11] Mainf()
    @主要~/Code/JuliaML/src/dt.jl:53
 [12] 顶级范围
    @〜/代码/JuliaML/src/dt.jl:58
某些类型信息被截断。使用 show(err) 查看完整类型。

它不适用于 MLJ.jl。我想使用 RandomForestRegressor 预测一个包含 5 或 6 个浮点数的向量，模型必须是 RF。]]></description>
      <guid>https://stackoverflow.com/questions/78116043/can-not-find-a-model-in-mlj-suitable-for-my-data-type</guid>
      <pubDate>Wed, 06 Mar 2024 16:41:59 GMT</pubDate>
    </item>
    <item>
      <title>我在尝试运行高级自动训练时遇到错误[关闭]</title>
      <link>https://stackoverflow.com/questions/78115600/i-am-getting-an-error-while-trying-to-run-autotrain-advanced</link>
      <description><![CDATA[我使用 Llama-2-7B-Chat-Hf 模型作为基础，Orca Math Word Problems 作为训练数据。我遇到错误 - UnicodeDecodeError: &#39;utf-8&#39; 编解码器无法解码位置 7 中的字节 0x92: 无效的起始字节。
如何解决这个问题？
这是我尝试的图像 - 我尝试构建的图像&lt; /p&gt;
这是完整的错误：
回溯（最近一次调用最后一次）：
  文件“/app/env/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py”，第 428 行，在 run_asgi 中
    结果=等待应用程序（＃类型：忽略[func-returns-value]
  文件“/app/env/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py”，第 78 行，在 __call__ 中
    返回等待 self.app（范围，接收，发送）
  文件“/app/env/lib/python3.10/site-packages/fastapi/applications.py”，第 1106 行，在 __call__ 中
    等待超级（）.__call__（范围，接收，发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/applications.py”，第 122 行，在 __call__ 中
    等待 self.middleware_stack（范围，接收，发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/errors.py”，第 184 行，在 __call__ 中
    提高执行力
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/errors.py”，第 162 行，在 __call__ 中
    等待 self.app（范围，接收，_发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/sessions.py”，第 86 行，在 __call__ 中
    等待 self.app（范围，接收，send_wrapper）
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/exceptions.py”，第 79 行，在 __call__ 中
    提高执行力
  文件“/app/env/lib/python3.10/site-packages/starlette/middleware/exceptions.py”，第 68 行，在 __call__ 中
    等待 self.app（范围、接收、发送者）
  文件“/app/env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py”，第 20 行，在 __call__ 中
    提高e
  文件“/app/env/lib/python3.10/site-packages/fastapi/middleware/asyncexitstack.py”，第 17 行，在 __call__ 中
    等待 self.app（范围、接收、发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/routing.py”，第 718 行，在 __call__ 中
    等待route.handle（范围，接收，发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/routing.py”，第 276 行，在句柄中
    等待 self.app（范围、接收、发送）
  文件“/app/env/lib/python3.10/site-packages/starlette/routing.py”，第 66 行，在应用程序中
    响应=等待函数（请求）
  文件“/app/env/lib/python3.10/site-packages/fastapi/routing.py”，第 274 行，在应用程序中
    raw_response = 等待 run_endpoint_function(
  文件“/app/env/lib/python3.10/site-packages/fastapi/routing.py”，第 191 行，在 run_endpoint_function 中
    返回等待 dependent.call(**值)
  文件“/app/env/lib/python3.10/site-packages/autotrain/app.py”，第 452 行，handle_form
    dset = AutoTrainDataset(**dset_args)
  文件“”，第 13 行，位于 __init__ 中
  文件“/app/env/lib/python3.10/site-packages/autotrain/dataset.py”，第 204 行，在 __post_init__ 中
    self.train_df, self.valid_df = self._preprocess_data()
  文件“/app/env/lib/python3.10/site-packages/autotrain/dataset.py”，第 213 行，位于 _preprocess_data
    train_df.append(pd.read_csv(文件))
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py”，第 1026 行，在 read_csv 中
    返回_read（文件路径或缓冲区，kwds）
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py”，第 620 行，在 _read 中
    解析器 = TextFileReader(filepath_or_buffer, **kwds)
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py”，第 1620 行，位于 __init__ 中
    self._engine = self._make_engine(f, self.engine)
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/readers.py”，第 1898 行，在 _make_engine 中
    返回映射[引擎](f, **self.options)
  文件“/app/env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py”，第 93 行，在 __init__ 中
    self._reader = parsers.TextReader(src, **kwds)
  文件“parsers.pyx”，第 574 行，位于 pandas._libs.parsers.TextReader.__cinit__ 中
  文件“parsers.pyx”，第 663 行，位于 pandas._libs.parsers.TextReader._get_header
  文件“parsers.pyx”，第 874 行，位于 pandas._libs.parsers.TextReader._tokenize_rows 中
  文件“parsers.pyx”，第 891 行，位于 pandas._libs.parsers.TextReader._check_tokenize_status
  文件“parsers.pyx”，第 2053 行，位于 pandas._libs.parsers.raise_parser_error
UnicodeDecodeError：“utf-8”编解码器无法解码位置 7 中的字节 0x92：起始字节无效
]]></description>
      <guid>https://stackoverflow.com/questions/78115600/i-am-getting-an-error-while-trying-to-run-autotrain-advanced</guid>
      <pubDate>Wed, 06 Mar 2024 15:30:30 GMT</pubDate>
    </item>
    <item>
      <title>使用自然语言处理自动标题分组[关闭]</title>
      <link>https://stackoverflow.com/questions/78115517/automatic-headline-grouping-with-natural-language-processing</link>
      <description><![CDATA[我正在开发一个网络应用程序，该应用程序可以抓取金融文章并将其分类为“事件”。我对来自不同来源的约 200 篇文章进行了网络抓取。我想使用 NLP 模型将这些文章分组为“事件”并提供从文章中生成的摘要。
例如，假设一位首席执行官被解雇，并且有 10 / 200 篇关于该事件的文章，我称之为事件。我希望模型自动将文章分组为 n 个事件。
我不确定从哪里开始，因为我发现的大多数文本分组都需要我设置类别数。]]></description>
      <guid>https://stackoverflow.com/questions/78115517/automatic-headline-grouping-with-natural-language-processing</guid>
      <pubDate>Wed, 06 Mar 2024 15:18:56 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程数据错误</title>
      <link>https://stackoverflow.com/questions/78115412/gaussian-process-data-errors</link>
      <description><![CDATA[如何在ma​​tlab的fitrgp函数中插入误差数组来进行高斯过程回归？我有一个数组 x、其他数组 y 以及与 y 关联的标准差数组 delta_y。
只有x和y，我可以使用gprMdl = fitrgp(x,y)，但是如何添加delta_y作为y的错误栏？]]></description>
      <guid>https://stackoverflow.com/questions/78115412/gaussian-process-data-errors</guid>
      <pubDate>Wed, 06 Mar 2024 15:02:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 RGB 图像中应用多头注意力？</title>
      <link>https://stackoverflow.com/questions/78115215/how-to-apply-multiheadattention-in-rgb-image</link>
      <description><![CDATA[我想在卷积网络中使用多头注意力。
将(3,360,360)（RGB、高、宽）的图像作为多头注意力的输入，然后连接到Conv2d。但我有一些问题。
首先，从输入为 2D（seq、embedding 或 d_model）的情况开始，如 (6, 512)。
shape = (6,512) #（seq、嵌入或 d_model）
# 定义输入层
输入矩阵=输入（形状=形状）

# 使用 MultiHeadAttention 进行自注意力
层= MultiHeadAttention（num_heads = 4，key_dim = 2，use_bias = False）
注意输出，权重=层（输入矩阵，输入矩阵，return_attention_scores = True）

# 创建模型
模型 = tf.keras.Model(输入=input_matrix, 输出=attention_output)

weight_names = [&#39;查询&#39;, &#39;键&#39;, &#39;值&#39;, &#39;项目&#39;]
对于名称，在 zip(weight_names,layer.get_weights()) 中输出：
    print(名称, 输出形状)
打印（权重.形状）
--------------------
查询 (512, 4, 2) （为什么这里是嵌入数？, Heads, d_k）
键（512、4、2）
值 (512, 4, 2)
项目 (4, 2, 512)
（无、4、6、6）（批次、头、seq、seq）

这有助于我理解。
在此处输入图像描述
这是我的代码
将张量流导入为 tf
从tensorflow.keras.layers导入输入、Conv2D、MultiHeadAttention、Reshape
从tensorflow.keras导入后端
backend.set_image_data_format(&#39;channels_first&#39;)

形状 = (3,450,450) # (RGB, 高, 宽)
# 定义输入层
输入矩阵=输入（形状=形状）

# 使用 MultiHeadAttention 进行自注意力
层= MultiHeadAttention（num_heads = 4，key_dim = 2，use_bias = False，attention_axes =（1,2））
注意输出，权重=层（输入矩阵，输入矩阵，return_attention_scores = True）

# 使用Conv2D进行卷积运算
conv_output = Conv2D(filters=128, kernel_size=(2, 2),strides=(2,2),activation=&#39;relu&#39;)(attention_output)

# 创建模型
模型= tf.keras.Model（输入= input_matrix，输出= conv_output）

它可以运行，但我不确定它是否按照我的想法运行。因为权重（q，k，v）和attention_scores维度让我很困惑。首先，我认为 (None, 4, 3, 450, 3, 450) 应该是 (None, 4, 450, 450, 450, 450) (batch, Heads, height, width, height, width)，类似于 ( None,4,6,6)(batch,heads,seq,seq)，表示单词之间的关系。图像大小应该代表像素之间的关系。我该如何修改我的代码？其次，为什么权重(q,k,v)的维度没有改变？
weight_names = [&#39;查询&#39;, &#39;键&#39;, &#39;值&#39;, &#39;项目&#39;]
对于名称，在 zip(weight_names,layer.get_weights()) 中输出：
    print(名称, 输出形状)
打印（权重.形状）
--------------------------------
查询 (450, 4, 2)
键（450、4、2）
值（450、4、2）
项目（4、2、450）
（无、4、3、450、3、450）
]]></description>
      <guid>https://stackoverflow.com/questions/78115215/how-to-apply-multiheadattention-in-rgb-image</guid>
      <pubDate>Wed, 06 Mar 2024 14:34:58 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降权重不断变大</title>
      <link>https://stackoverflow.com/questions/78115138/gradient-descent-weights-keep-getting-larger</link>
      <description><![CDATA[为了熟悉梯度下降算法，我尝试创建自己的线性回归模型。对于少数数据点来说它效果很好。但是当尝试使用更多数据来拟合它时，w0 和 w1 的大小总是增加。有人可以解释一下这种现象吗？
类线性回归：
    def __init__(自身, x_向量, y_向量):

        self.x_vector = np.array(x_vector, dtype=np.float64)
        self.y_向量 = np.array(y_向量, dtype=np.float64)
        自身.w0 = 0
        自身.w1 = 0

    def _get_predicted_values(self, x):
        公式 = lambda x: self.w0 + self.w1 * x
        返回公式(x)

    def_get_gradient_matrix（自身）：
        预测 = self._get_predicted_values(self.x_vector)
        w0_hat = sum((self.y_向量 - 预测))
        w1_hat = sum((self.y_向量 - 预测) * self.x_向量)

        梯度矩阵 = np.array([w0_hat, w1_hat])
        梯度矩阵 = -2 * 梯度矩阵

        返回梯度矩阵

    def fit(自我，step_size=0.001，num_iterations=500)：
        for _ in range(1, num_iterations):
            梯度矩阵 = self._get_gradient_matrix()
            self.w0 -= 步长大小 * (梯度矩阵[0])
            self.w1 -= 步长大小 * (梯度矩阵[1])

    def _show_coeffiecients（自身）：
        print(f&quot;w0: {self.w0}\tw1: {self.w1}\t&quot;)

    def 预测（自身，x）：
        y = 自身.w0 + 自身.w1 * x
        返回y

# 这工作正常
x = [x 表示 x 在范围 (-3, 3) 内]
f = 拉姆达 x: 5 * x - 7
y = [f(x_val) for x_val in x]

模型 = 线性回归(x, y)
模型.fit(num_iterations=3000)

model.show_coeffiecients() #输出：w0：-6.99999999999994 w1：5.00000000000002

#虽然这不是
x = [x for x in range(-50, 50)] # 增加 x 值的数量
f = 拉姆达 x: 5 * x - 7
y = [f(x_val) for x_val in x]

模型 = 线性回归(x, y)
模型.fit(num_iterations=3000)

model.show_coefficients()

最后一行产生警告：
运行时警告：乘法中遇到溢出
w1_hat = sum((self.y_向量 - 预测) * self.x_向量)
公式 = lambda x: self.w0 + self.w1 * x
]]></description>
      <guid>https://stackoverflow.com/questions/78115138/gradient-descent-weights-keep-getting-larger</guid>
      <pubDate>Wed, 06 Mar 2024 14:22:16 GMT</pubDate>
    </item>
    <item>
      <title>为每月数据编写时间滞后，以合并训练测试分割模型的情绪和市场分析[关闭]</title>
      <link>https://stackoverflow.com/questions/78113308/coding-a-time-lag-for-monthly-data-to-merge-a-sentiment-and-market-analysis-for</link>
      <description><![CDATA[我们的教授为我们提供了以下代码：
代码1：
sentiment=pd.read_csv(&#39;/content/drive/MyDrive/Colab Notebooks/Python Code/Sentiment_quarterly.csv&#39;).drop(&#39;未命名：0&#39;,axis=1)
情绪
从日期时间导入日期时间
数据[&#39;数据&#39;]=0

代码2：
for k in tqdm(range(len(data[&#39;Datum&#39;])))：
    test_date=str(int(data.loc[k,&#39;bewertungsjahr&#39;]))+&#39;.&#39;+str(int(data.loc[k,&#39;bewertungsquartal&#39;]*3))
    data.loc[k,&#39;Datum&#39;]=str(pd.Timestamp(datetime.strptime(test_date, &#39;%Y.%m&#39;).date()).to_period(&#39;Q&#39;))

数据[&#39;数据&#39;][0]

代码3：
for k in tqdm(range(len(sentiment[&#39;Time&#39;]))):
    test_date=str(int(sentiment.loc[k,&#39;时间&#39;][0:4]))+&#39;.&#39;+str(int(sentiment.loc[k,&#39;时间&#39;][5])*3)
    情感.loc[k,&#39;Time&#39;]=pd.Timestamp(datetime.strptime(test_date, &#39;%Y.%m&#39;).date()).to_period(&#39;Q&#39;)

情绪[&#39;时间&#39;][0]

我们不想使用季度情绪.csv，而是使用每月.csv
季度数据中的时间格式是例如2014Q1 和月度数据显示 2014-01、2014-02...
如何调整代码以使其正常工作？
我尝试添加此&#39;.&#39;+str(int(data.loc[k,&#39;bewertungsquartal&#39;]*12))，这样也许还需要考虑几个月。]]></description>
      <guid>https://stackoverflow.com/questions/78113308/coding-a-time-lag-for-monthly-data-to-merge-a-sentiment-and-market-analysis-for</guid>
      <pubDate>Wed, 06 Mar 2024 09:47:41 GMT</pubDate>
    </item>
    <item>
      <title>我的感知器算法总是给我错误的线来分离二维线性可分离数据</title>
      <link>https://stackoverflow.com/questions/78112634/my-perceptron-algorithm-keeps-giving-me-the-wrong-line-for-separating-2d-linearl</link>
      <description><![CDATA[我正在为虚拟 2D 数据实现感知器模型。以下是我生成数据的方式
&lt;前&gt;&lt;代码&gt;#numpoint
n = 15
#f(x) = w0 + ax1 + bx2
#那么如果f(x) = 0
#x2 = (-w0 - ax1)/b
截距 = 30
一个= 4
b = 2
#生成0-20之间的随机点
x1 = np.random.uniform(-20, 20, n) #返回一个np数组
x2 = np.random.uniform(-20, 20, n)
y = []
#绘制f(x)
plt.plot(x1, (-截距 - a*x1)/b, &#39;k-&#39;)
plt.ylabel(“x2”)
plt.xlabel(“x1”)

#绘制彩色点
对于范围内的 i(0, len(x1))：
    f = 截距 + a * x1[i] + b * x2[i]
    如果（f &lt;= 0）：
        plt.plot(x1[i], x2[i], &#39;ro&#39;)
        y.追加(-1)
    如果（f&gt;0）：
        plt.plot(x1[i], x2[i], &#39;bo&#39;)
        y.追加(1)
y = np.array(y)
# 添加x0作为阈值
x0 = np.ones(n)
stacked_x = np.stack((x0,x1,x2))
堆叠_x

这是数据的可视化
在此处输入图像描述
这是我的感知器模型
类 PLA():
    def __init__(self, numPredictors):
        self.w = np.random.rand(1,numPredictors+1) #(1, numPredictors+1)
        self.iter = 0
    def fitModel（自身，xData，yData）：
        而（真）：
            yhat = np.matmul(self.w, xData).squeeze() #从(1,n)到(,n)
            比较 = np.sign(yhat) == yData
            ind = [i for i in range(0,len(compare)) if Compare[i] == False] #分类错误的索引
            打印（长度（ind））
            如果 len(ind) == 0:
                休息
            对于 ind 中的 i：
                update = yData[i]* xData[:, i] #1d 数组
                self.w = self.w + np.transpose(update[:,np.newaxis]) #转置以匹配权重的形状
            self.iter += 1

当我可视化模型时
&lt;前&gt;&lt;代码&gt;pla1 = PLA(2)
pla1.fitModel(stacked_x, y)
#绘制彩色点
对于范围内的 i(0, len(x1))：
    如果（y[i]==-1）：
        plt.plot(x1[i], x2[i], &#39;ro&#39;)
    如果（y[i]==1）：
        plt.plot(x1[i], x2[i], &#39;bo&#39;)
plt.plot(x1, (-pla1.w[0][0] - pla1.w[0][1]*x1)/(pla1.w[0][1]), &#39;g-&#39;, 标签 = “解放军”）
plt.plot(x1, (-截距 - a*x1)/b, &#39;k-&#39;, label = &quot;f(x)&quot;)
plt.xlabel(“x1”)
plt.ylabel(“x2”)
plt.图例()

我从感知器算法得到的线是不正确的
在此处输入图像描述
这是使用不同数据参数和样本大小的另一次运行 (n = 30)
在此处输入图像描述
我尝试在每次迭代时打印出更新，它按照我的预期工作。我不确定是什么导致我的算法停止，即使仍然存在错误分类的点。我已经被这个问题困扰了几天了。我非常感谢任何意见。]]></description>
      <guid>https://stackoverflow.com/questions/78112634/my-perceptron-algorithm-keeps-giving-me-the-wrong-line-for-separating-2d-linearl</guid>
      <pubDate>Wed, 06 Mar 2024 07:53:35 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降最小二乘代码问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78112607/problem-with-gradient-descent-least-squares-code</link>
      <description><![CDATA[我正在尝试在数据集上使用梯度下降。我写的是
&lt;前&gt;&lt;代码&gt;导入numpy
将 pandas 导入为 pd
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

数据 = pd.read_csv(&#39;C:/Users/Teacher/Downloads/data.csv&#39;)
X = data.iloc[:, 0] # 选择 data 中第一列的所有数据
Y = data.iloc[:, 1]
plt.scatter(X,Y)
plt.show()
n = 长度 (X)

a = 0
b = 0
L = .001

对于范围（1000）内的 i：
    y_预测 = a * X + b
    pd_a = (1 / n) * sum((y_预测 - Y) * X)
    pd_b = (1 / n) * sum(y_预测 - Y)
    a = a - L * pd_a
    b = b - L * pd_b
打印（a，b）
plt.scatter(X, Y)
c, d = numpy.polyfit(X, Y, 1)
打印（c，d）
plt.plot([min(X), max(X)], [a * x + b for x in [min(X), max(X)]], [c * x + d for x in [min( X), 最大值(X)]])
plt.show()

如果我定义 X 和 Y = np.random.rand(20)，那么一切似乎都工作正常，所以问题似乎出在 csv 的输入上。
然而，X 和 Y 的散点图仍然很好，即使我将它们定义为数据集的第一列和第二列，所以我不确定发生了什么。
编辑：这是定义 X = data.iloc[:, 0] 后的散点图图像
Y = data.iloc[:, 1]

这是代码末尾的绘图和线条的图像。

print(data.head())的结果：

编辑：仅读取 csv 的一行：

]]></description>
      <guid>https://stackoverflow.com/questions/78112607/problem-with-gradient-descent-least-squares-code</guid>
      <pubDate>Wed, 06 Mar 2024 07:48:04 GMT</pubDate>
    </item>
    <item>
      <title>torchserve ：即使 config.properties 指定其他值，batch_size 也始终为 1</title>
      <link>https://stackoverflow.com/questions/78111173/torchserve-batch-size-is-always-1-even-config-properties-specify-other-value</link>
      <description><![CDATA[我认为我的 torchserve 正确加载了 config.properties，因为我设置的工作人员数量是 2。但batch_size是1而不是20。
任何人都知道可能会出现什么问题吗？谢谢！
我已经检查并torchserve正确加载config.properties，可惜它忽略了config.properties中指定的batch_size和max_batch_delay。
这是我的 config.properties 供参考
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
log_file=/ml_server/logs/torchserve.log
default_workers_per_model=2
number_of_netty_threads=32
作业队列大小=1000
批量大小=20
最大批量延迟=10

下面是日志，worker的batchSize：1
ml-server | 2024-03-06T00：11：11,091 [信息] W-9001-model_1.0-stdout MODEL_LOG - model_name：_model，batchSize：1
机器学习服务器 | 2024-03-06T00：11：11,091 [信息] W-9000-model_1.0-stdout MODEL_LOG - model_name：_model，batchSize：1
]]></description>
      <guid>https://stackoverflow.com/questions/78111173/torchserve-batch-size-is-always-1-even-config-properties-specify-other-value</guid>
      <pubDate>Wed, 06 Mar 2024 00:19:31 GMT</pubDate>
    </item>
    <item>
      <title>Handpose tfjs 错误 - 在注册表中找不到后端</title>
      <link>https://stackoverflow.com/questions/62134812/handpose-tfjs-error-no-backend-found-in-registry</link>
      <description><![CDATA[尝试运行 Handpose tfjs 演示项目时，出现以下错误。

我的 package.json 文件具有以下依赖项：

&lt;前&gt;&lt;代码&gt;{
“名称”：“tensorflowJs”，
“版本”：“1.0.0”，
“描述”： ””，
“主要”：“index.js”，
“脚本”：{
  &quot;watch&quot;: &quot;跨环境 NODE_ENV=开发包index.html --no-hmr &quot;,
  &quot;build&quot;: &quot;cross-env NODE_ENV=生产包构建index.html --public-url ./&quot;
 },
“浏览器”：{
“加密”：假
 },
“关键字”：[]，
“作者”： ””，
“许可证”：“ISC”，
“依赖项”：{
  “@tensorflow-models/handpose”：“0.0.4”，
  &quot;@tensorflow/tfjs-backend-wasm&quot;: &quot;^2.0.0&quot;,
  &quot;@tensorflow/tfjs-converter&quot;: &quot;^1.7.4&quot;,
  &quot;@tensorflow/tfjs-core&quot;: &quot;^2.0.0&quot;,
  &quot;@tensorflow/tfjs-node&quot;: &quot;^2.0.0&quot;,
  &quot;引导程序&quot;: &quot;^4.5.0&quot;,
  “跨环境”：“^7.0.2”
 },
“开发依赖项”：{
  &quot;@babel/cli&quot;: &quot;^7.10.1&quot;,
  &quot;@babel/core&quot;: &quot;^7.10.2&quot;,
  &quot;@babel/plugin-transform-runtime&quot;: &quot;^7.10.1&quot;,
  &quot;@babel/polyfill&quot;: &quot;^7.10.1&quot;,
  &quot;@babel/preset-env&quot;: &quot;^7.10.2&quot;,
  &quot;babel-preset-env&quot;: &quot;^1.7.0&quot;,
  “包裹捆绑器”：“^1.12.4”
 }
}

注册表问题应该在版本 0.10.3 之后得到解决，但即使对于版本 2，我仍然面临这个问题。有谁知道为什么会出现这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/62134812/handpose-tfjs-error-no-backend-found-in-registry</guid>
      <pubDate>Mon, 01 Jun 2020 14:46:38 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程的平方协方差函数</title>
      <link>https://stackoverflow.com/questions/44085001/squared-covariance-function-of-gaussian-process</link>
      <description><![CDATA[这是我第一次尝试编写协方差函数。我有以下价值观，
&lt;预&gt;&lt;代码&gt;x = [-1.50 -1.0 -.75 -.40 -.25 0.00];
SF=1.27；
埃尔 = 1;
SN = 0.3;

平方指数协方差函数的公式为

我编写的 matlab 代码为：
K = sf^2*exp(-0.5*(squareform(pdist(x)).^2)/ell^2)+(sn)^2*eye(Ntr,Ntr);

其中，sf 是信号标准差，ell 是特征长度尺度，sn 是噪声标准差，Ntr code&gt; 训练输入数据的长度x。
但这没有给我任何结果。我的编码有什么错误吗？
一旦我计算出来，我想总结成矩阵形式，如下所示，

如果x_ = 0.2那么我们如何计算：
a) K_ =[k(x_,x1) k(x_,x2).........k(x_,xn)] 和 
b) K__ = k(x_,x_)
使用matlab？]]></description>
      <guid>https://stackoverflow.com/questions/44085001/squared-covariance-function-of-gaussian-process</guid>
      <pubDate>Sat, 20 May 2017 10:53:42 GMT</pubDate>
    </item>
    </channel>
</rss>