<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 25 Jan 2024 09:14:44 GMT</lastBuildDate>
    <item>
      <title>使用 textattack 进行 Bert 攻击并获取 ValueError(f"Failed to import file {args.dataset_from_file}")</title>
      <link>https://stackoverflow.com/questions/77878231/bert-attack-using-textattack-and-getting-valueerrorffailed-to-import-file-arg</link>
      <description><![CDATA[我想在我的预训练模型和数据集上使用文本攻击来生成对抗样本。所以我运行这个命令：
文本攻击攻击 --model-from-file model --dataset-from-file data/data.csv --attack-recipe bert-attack --log-to-txt data/output.txt

但是，我收到此错误
引发 ValueError(f“无法导入文件 {args.dataset_from_file}”) ValueError：无法导入文件 data/data.csv
我认为这可能是文件路径，所以我使用了绝对路径，但仍然是同样的错误。我的模型文件包含模型和分词器。我还检查了我的 csv 内容，我认为没有任何问题。
我的 csv 内容：
文本“我和妻子上周住在芝加哥阿菲尼亚酒店的单间套房，不值得支付额外费用，一开始就绝对是一场噩梦，房间被列为“超大”，但事实并非如此我们以前住过这家酒店，普通客房只小了大约 2 平方英尺，我相信唯一的额外空间是在衣柜里，这不是很有用，除非你打算住在那里，我们有两个儿子和我们在一起，所以我们必须腾出额外的睡眠空间，我们的大儿子睡在沙发上，期间多次醒来，抱怨有虫子在四处走动，他一定也很不舒服，因为我白天坐在沙发上，房间里就像一块石头服务很糟糕，晚上 10 点左右他们花了 47 分钟才给我妻子送来一个枕头，她不得不等待，因为我的家人永远不会回到这家酒店，我强烈建议你也远离这家酒店。 “阿菲尼亚芝加哥酒店是我住过的最糟糕的酒店之一，我作为客人受到的待遇如此差劲，当我要求无烟房间时，他们在我的房间里犯了一个错误，前台非常不通融。”预订时，由于某种奇怪的原因，没有服务员，所以我不得不把所有行李搬到电梯上，然后自己沿着长长的走廊到我的房间，如果这不是一次糟糕的住宿，我叫了客房服务，花了一个多小时如果房间里没有空调，我会说，如果您前往芝加哥进行任何类型的商务旅行，那么这次住宿的一切都非常痛苦，我希望您决定不选择这家酒店很惊讶我喜欢芝加哥这个城市，但这次住宿绝对让我的旅行成为一次相当负面的经历”
所以我希望在运行命令后得到对抗性输出，但我遇到了错误。]]></description>
      <guid>https://stackoverflow.com/questions/77878231/bert-attack-using-textattack-and-getting-valueerrorffailed-to-import-file-arg</guid>
      <pubDate>Thu, 25 Jan 2024 07:42:54 GMT</pubDate>
    </item>
    <item>
      <title>在 pyspark 数据帧的 groupby 上实现机器学习算法，然后获得组合结果</title>
      <link>https://stackoverflow.com/questions/77878122/implement-machine-learning-algorithm-on-groupby-from-pyspark-dataframe-and-then</link>
      <description><![CDATA[我尝试在完整的数据帧上实现机器学习算法，下面是代码，但我希望在 groupby 基础上应用该算法，因为我们有不同的组，例如 group_cols=[“col1”，“col2”， “col3”]并且会有不同的组合，因此需要将相似的组分组在一起并对其应用算法并获得具有异常值分数的最终数据帧。
spark = SparkSession.builder.appName(“LOFExample”).getOrCreate()

# 假设您有一个具有功能的 DataFrame &#39;df_actual_final&#39;
feature_columns = [&quot;col5&quot;] # 根据你实际的特征列进行调整
汇编器= VectorAssembler（inputCols = feature_columns，outputCol =“特征”）
df_assembled = assembler.transform(df_actual_final)

# 提取特征作为列表
extract_features_udf = F.udf(lambda 特征: features[0].item(), DoubleType())
df_features = df_assembled.withColumn(“feature_value”, extract_features_udf(col(“features”)))

# 将特征转换为 NumPy 数组
numpy_array = np.array(df_features.select(“feature_value”).collect())


# 训练局部离群因子模型
lof = LocalOutlierFactor(contamination=0.01) # 根据需要调整污染
outlier_scores = lof.fit_predict(numpy_array)
outlier_scores_list = outlier_scores.tolist()
outlier_df = Spark.createDataFrame(enumerate(outlier_scores_list), [“id”, “outlier_scores”])
   
df_assembled_pd = df_assembled.toPandas()
outlier_df_pd=outlier_df.toPandas()
df_concat = pd.concat([df_assembled_pd, outlier_df_pd], axis=1)
result_df=spark.createDataFrame(df_concat)

result_df = result_df.withColumn(“local_outlier_flag”, when(col(“outlier_scores”) ==-1, 1).otherwise(0))

]]></description>
      <guid>https://stackoverflow.com/questions/77878122/implement-machine-learning-algorithm-on-groupby-from-pyspark-dataframe-and-then</guid>
      <pubDate>Thu, 25 Jan 2024 07:18:46 GMT</pubDate>
    </item>
    <item>
      <title>无法在 wsl2 上下载 deepspeed 库</title>
      <link>https://stackoverflow.com/questions/77877824/not-able-to-download-the-deepspeed-library-on-wsl2</link>
      <description><![CDATA[我无法在 WSL2 上下载 deepspeed 库。
我需要下载一个名为 SwissArmyTransformer 的库。但这个库需要deepspeed库。
我要下载的库是SwissArmyTransformer。该库需要 deepspeed 库。我在 WSL 上运行它。 deepspeed 库根本没有被下载。它无法获取元数据。我必须下载 deepspeed 0.11.0 或更高版本。请帮助我]]></description>
      <guid>https://stackoverflow.com/questions/77877824/not-able-to-download-the-deepspeed-library-on-wsl2</guid>
      <pubDate>Thu, 25 Jan 2024 06:09:03 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用 FastAI 预测图像数据时出现断言错误</title>
      <link>https://stackoverflow.com/questions/77877552/assertionerror-when-trying-to-predict-image-data-with-fastai</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77877552/assertionerror-when-trying-to-predict-image-data-with-fastai</guid>
      <pubDate>Thu, 25 Jan 2024 04:31:05 GMT</pubDate>
    </item>
    <item>
      <title>极度右偏的特征分布</title>
      <link>https://stackoverflow.com/questions/77877492/extremely-right-skewed-feature-distribution</link>
      <description><![CDATA[
如何平滑这样的分布？
计数甚至介于特征的 0 和非零值之间。标签 (y) 不偏向于特征值（与 0 和非零类似的相关性）。
我想知道是否有任何处理可以改善特征分布并从而改善相关性？]]></description>
      <guid>https://stackoverflow.com/questions/77877492/extremely-right-skewed-feature-distribution</guid>
      <pubDate>Thu, 25 Jan 2024 04:07:56 GMT</pubDate>
    </item>
    <item>
      <title>Xavier Glorot 初始化公式</title>
      <link>https://stackoverflow.com/questions/77877374/xavier-glorot-initialization-formula</link>
      <description><![CDATA[尝试了解 Xavier Glorot 初始化公式的具体细节。请解释一下每个公式样本之间有什么区别，2和6系数从何而来，如果被除数需要输入和输出，在没有运行第一次前向传播的情况下，我们从哪里获得输出？&lt; /p&gt;
例如
在此处输入图片描述
基本上，我想通过代码重写这个公式，而不需要任何 Py 或任何其他库依赖项。
不确定我是否理解这些公式背后的逻辑，因为权重不是常数值，需要根据需要为每个反向传播进行更新，对吗？我知道通过防止 0、太低或太高的权重值，这些可以在每次迭代中产生的好处。只需要简化公式的理性即可。
大量 YouTube 教程、教科书和网络平台。很多人只讲基础介绍，但都是理论教程，相信很多人能背诵一本书，但不了解架构背后的原理。
只是想了解神经网络逻辑的组成。在前向传播过程中，数据被输入，数据在黑匣子内被解释并创建一个身份。随后比较该身份以确定神经元是否被激活。
除了数据值之外，权重应该是随机设置或初始化的，但这已被证明会导致错误，所以如果这些 Xavier 公式有效，那么需要哪些值以及它们来自哪里？]]></description>
      <guid>https://stackoverflow.com/questions/77877374/xavier-glorot-initialization-formula</guid>
      <pubDate>Thu, 25 Jan 2024 03:24:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 Top-N 特征方法去除特征的随机森林分类器</title>
      <link>https://stackoverflow.com/questions/77877253/random-forest-classifier-removing-features-using-top-n-features-method</link>
      <description><![CDATA[我是数据科学和机器学习技术和流程的新手。我正在开展一个个人项目，该项目使用随机森林分类器预测 NBA 比赛的获胜者。我试图删除和修改我的功能列表，以便提高准确性并减少噪音。
我实现了此处找到的解决方案：https:// datascience.stackexchange.com/questions/57697/decision-trees-should-we-discard-low-importance-features，其中我将循环遍历前 N 个最重要的特征并绘制出最终的准确性。在我的所有功能都经过该循环之后，我留下了一个如下所示的图：

正如您所看到的，生成的图表有点乱。我是否要删除具有负斜率的要素？或者说删除特征的门槛是多少？有没有更好的方法来计算噪声？鉴于我有如此多的特征，并且对训练数据上的模型准确性产生如此多的影响，我如何获得最准确的模型？]]></description>
      <guid>https://stackoverflow.com/questions/77877253/random-forest-classifier-removing-features-using-top-n-features-method</guid>
      <pubDate>Thu, 25 Jan 2024 02:32:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Flux `withgradient` 计算的损失与我计算的不匹配？</title>
      <link>https://stackoverflow.com/questions/77876955/why-doesnt-the-loss-calculated-by-flux-withgradient-match-what-i-have-calcula</link>
      <description><![CDATA[我正在尝试使用 Flux 训练一个简单的 CNN，但遇到了一个奇怪的问题...在训练过程中，损失似乎下降了（表明它正在工作），但尽管损失曲线表明“训练过的”模型仍然有效，模型输出非常糟糕，当我手动计算损失时，我注意到它与训练表明的结果不同（它表现得好像根本没有经过训练）。
然后我开始计算梯度内部与外部返回的损失，经过大量挖掘，我认为问题与 BatchNorm 层有关。考虑以下最小示例：
使用 Flux
x = rand(100,100,1,1) #假设一个灰度图像 100x100，具有 1 个通道（灰度）和 1 个批次
y = @。 5*x + 3 #输出图像，与输入值的一些关系（与此无关）
m = Chain(BatchNorm(1),Conv((1,1),1=&gt;1)) #非常简单的模型（实际上没有做任何事情，但说明了问题）
l_init = Flux.mse(m(x),y) #模型创建后的初始损失
l_grad, grad = Flux.withgradient(m -&gt; Flux.mse(m(x),y), m) #梯度计算的loss
l_final = Flux.mse(m(x),y) #使用模型再次计算损失（没有更新参数）
println(&quot;初始损失：$l_init&quot;)
println(&quot;用梯度计算的损失：$l_grad&quot;)
println(&quot;最终损失：$l_final&quot;)

上面的所有损失都会有所不同，有时会非常显着（刚才运行时我得到了 22.6、30.7 和 23.0），而我认为它们应该是相同的？
有趣的是，如果我删除 BatchNorm 层，输出都是相同的，即运行：
使用 Flux
x = rand(100,100,1,1) #假设一个灰度图像 100x100，具有 1 个通道（灰度）和 1 个批次
y = @。 5*x + 3 #输出图像
m = 链(Conv((1,1),1=&gt;1))
l_init = Flux.mse(m(x),y) #模型创建后的初始损失
l_grad, grad = Flux.withgradient(m -&gt; Flux.mse(m(x),y), m)
l_final = Flux.mse(m(x),y)
println(&quot;初始损失：$l_init&quot;)
println(&quot;用梯度计算的损失：$l_grad&quot;)
println(&quot;最终损失：$l_final&quot;)

每次损失计算都会产生相同的数字。
为什么包含 BatchNorm 层会像这样改变损失值？
我（有限）的理解是，这只是为了标准化输入值，我知道这可能会影响非标准化和标准化情况之间的损失，但我不明白为什么它会产生不同的损失值同一模型上的相同输入值，而没有更新该模型的任何参数？]]></description>
      <guid>https://stackoverflow.com/questions/77876955/why-doesnt-the-loss-calculated-by-flux-withgradient-match-what-i-have-calcula</guid>
      <pubDate>Thu, 25 Jan 2024 00:30:43 GMT</pubDate>
    </item>
    <item>
      <title>Transformer架构的输入大小问题</title>
      <link>https://stackoverflow.com/questions/77876582/transformer-architectures-input-size-problem</link>
      <description><![CDATA[所以，最近，我读到了《Attention is all you need》（注意力就是你所需要的）。论文，描述了变压器的架构。在 Transformer 中，有一个叫做 masked multi-head Attention 的组件，仅在解码器部分使用。
问题是，解码器的输入是编码器的输出以及之前生成的标记。并且之前每次迭代生成的token数量不同，但是线性层的神经元数量是相同的。因此，我们必须使用“pad tokens”来实现。屏蔽注意力用于对这些 pad token 给予 0 注意力。
编码器也是如此。输入可以是不同的大小，所以我们还必须使用填充令牌，但在这里，我们不使用屏蔽注意力，我很好奇，为什么？
或者我们不在那里使用填充令牌，而是使用其他东西？
我尝试向 chat-gpt 询问此事，但它没有给我合理的答案，所以我来到这里。]]></description>
      <guid>https://stackoverflow.com/questions/77876582/transformer-architectures-input-size-problem</guid>
      <pubDate>Wed, 24 Jan 2024 22:28:51 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow MobilenetV2 对象检测 Web 模型格式导出问题</title>
      <link>https://stackoverflow.com/questions/77876211/tensorflow-mobilenetv2-object-detection-web-model-format-export-problem</link>
      <description><![CDATA[我想微调 MobileNet v2 并导出为 json + 二进制格式。不幸的是，出口出现了问题。我将在最基本的示例中展示它 - 没有微调的普通 MobileNet v2：

下载并解压SSD MobileNet v2.
使用tensorflowjs_converter我可以将saved_model/导出为Web格式。它按预期工作：

tensorflowjs_converter --control_flow_v2=True --input_format=tf_saved_model --metadata= --saved_model_tags=serve --signature_name=serving_default --strip_debug_ops=True --weight_shard_size_bytes=4194304 saving_model/ web_model/


可以在浏览器中导入模型。只需加载 tfjs：

]]></description>
      <guid>https://stackoverflow.com/questions/77876211/tensorflow-mobilenetv2-object-detection-web-model-format-export-problem</guid>
      <pubDate>Wed, 24 Jan 2024 21:05:03 GMT</pubDate>
    </item>
    <item>
      <title>SetFit 训练未完成评估步骤</title>
      <link>https://stackoverflow.com/questions/77760620/setfit-training-not-finishing-evaluation-step</link>
      <description><![CDATA[我正在尝试使用 SetFit 训练简单的二元分类，但我的库有问题。我使用 Huggingface 来管理我的数据集。数据集由文本和标签列组成。如果我打印我的数据集，它看起来像这样：
dataset = load_dataset(“&lt;我的数据集&gt;”)
打印（数据集）

输出：
DatasetDict({
    火车：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：20
    })
    评估：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：10
    })
    测试：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：135
    })
})

这是我的培训代码：
# 使用预训练模型初始化 SetFit 模型并定义标签名称
模型 = SetFitModel.from_pretrained(
    “释义-多语言-mpnet-base-v2”，
    标签=[“阴性”,“阳性”],
）

# 定义训练参数
args = 训练参数(
    批量大小=32，
    num_epochs=8,
    evaluation_strategy=“纪元”，
    save_strategy=“纪元”,
    load_best_model_at_end=True
）

# 初始化训练器
教练=教练（
    型号=型号，
    参数=参数，
    train_dataset=数据集[“火车”],
    eval_dataset=数据集[“eval”],
    度量=“准确度”，
    column_mapping={&quot;text&quot;: &quot;text&quot;, &quot;label&quot;: &quot;label&quot;} # 将数据集列映射到训练器期望的文本/标签
）

# 训练模型
训练师.train()

但是我现在遇到的问题是训练行为非常奇怪。我没有受到任何培训或验证损失，评估步骤也没有完成。我不知道问题出在哪里。

另请注意，我稍微更改了参数以提高训练速度。它通常有更多的步骤等等。使用正常参数时它仍然表现得很奇怪。我还使用 SetFit 1.0.1 版本。我在 GitHub 存储库中没有发现任何与此相关的问题。]]></description>
      <guid>https://stackoverflow.com/questions/77760620/setfit-training-not-finishing-evaluation-step</guid>
      <pubDate>Thu, 04 Jan 2024 19:02:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用 AI 进行图像分割在每次循环迭代时的时间更长？</title>
      <link>https://stackoverflow.com/questions/77454517/why-is-image-segmentation-using-ai-longer-at-each-iteration-of-a-loop</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77454517/why-is-image-segmentation-using-ai-longer-at-each-iteration-of-a-loop</guid>
      <pubDate>Thu, 09 Nov 2023 15:43:46 GMT</pubDate>
    </item>
    <item>
      <title>数据帧的 .corr() 方法不返回理想值仅返回 -1 或 1</title>
      <link>https://stackoverflow.com/questions/77214404/corr-method-for-dataframe-not-returning-ideal-values-only-returns-either-1-o</link>
      <description><![CDATA[理想情况下，它应该为每个单元格返回 -1 到 1 之间的值，但具有相同列名和行名的单元格需要具有 1 值
在执行 corr() 之前尝试用 0 替换 NaN，它返回正确的值，但这些值对于程序的目的来说是不准确的
&lt;前&gt;&lt;代码&gt;# df
            电影A 电影B 电影C 电影D 电影E
安吉 0.000000 南 -0.500000 0.500000 南
安尼维什 1.166667 -0.333333 -0.833333 NaN NaN
杰伊 1.166667 -0.333333 南 -0.833333 南
卡蒂克 0.000000 -1.500000 南 南 1.5
纳曼 南 0.250000 南 -0.250000 南

# df.T.corr()
          安吉·阿尼维什·杰伊·卡西克·纳曼
安吉 1.0 1.0 -1.0 南 南
安尼维什 1.0 1.0 1.0 1.0 NaN
杰伊 -1.0 1.0 1.0 1.0 1.0
卡蒂克 NaN 1.0 1.0 1.0 NaN
纳曼 南 南 1.0 南 1.0
]]></description>
      <guid>https://stackoverflow.com/questions/77214404/corr-method-for-dataframe-not-returning-ideal-values-only-returns-either-1-o</guid>
      <pubDate>Mon, 02 Oct 2023 09:15:04 GMT</pubDate>
    </item>
    <item>
      <title>使用属性在 scikit-learn 中进行分层训练/测试分割</title>
      <link>https://stackoverflow.com/questions/75516592/stratified-train-test-split-in-scikit-learn-using-an-attribute</link>
      <description><![CDATA[我需要将数据分为训练集 (80%) 和测试集 (20%)。我目前使用下面的代码来做到这一点：
StratifiedShuffleSplit(n_splits=10,test_size=.2, train_size=.8, random_state=0)

我如何需要指定一个特定的属性来进行分割。我做不到]]></description>
      <guid>https://stackoverflow.com/questions/75516592/stratified-train-test-split-in-scikit-learn-using-an-attribute</guid>
      <pubDate>Tue, 21 Feb 2023 05:28:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在海量数据上训练机器学习模型？</title>
      <link>https://stackoverflow.com/questions/74886400/how-to-train-a-machine-learning-model-on-huge-amount-of-data</link>
      <description><![CDATA[关键点：数据集太大了，我几乎无法将其存储在硬件中。 （拍字节）
假设我的数据集中有数万亿行。该数据集太大，无法存储在内存中。我想在这个数据集上训练一个机器学习模型，比如逻辑回归。我该怎么办？
现在，我知道亚马逊/谷歌在大量数据上进行机器学习。他们怎样做呢？例如点击数据集，全局每个智能设备的输入都存储在一个数据集中。
拼命寻找新想法并乐于接受修正。
我的思路：

加载部分数据到内存
执行梯度下降

这样优化就是小批量下降。
现在的问题是，在优化中，无论是SGD还是mini Batch，在最坏的情况下，当它遍历完所有数据时就会停止。遍历整个数据集是不可能的。
所以我有了提前停止的想法。提前停止保留验证集，并在错误停止下降/收敛于验证集时停止优化。但由于数据集的大小，这可能不可行。
现在我正在考虑简单地随机采样训练集和测试集，并使用可行的大小来训练模型。]]></description>
      <guid>https://stackoverflow.com/questions/74886400/how-to-train-a-machine-learning-model-on-huge-amount-of-data</guid>
      <pubDate>Thu, 22 Dec 2022 09:16:33 GMT</pubDate>
    </item>
    </channel>
</rss>