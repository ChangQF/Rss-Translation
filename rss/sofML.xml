<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 03 May 2024 01:02:58 GMT</lastBuildDate>
    <item>
      <title>使用 Python、Keras 和 TensorFlow 进行 Epoch 数据训练</title>
      <link>https://stackoverflow.com/questions/78422261/epoch-data-training-using-python-and-keras-and-tensorflow</link>
      <description><![CDATA[saya mendapatkan error seperti ini saat mencoba 训练数据 saya
saya harap saya mendapatkan solusinya agar epoch Training saya dapat kembali berproses在此处输入图像描述
terdapat juga pesan yang muncul Epoch 1/10
C:\Users\LISA\AppData\Roaming\Python\Python311\site-packages\keras\src\trainers\data_adapters\py_dataset_adapter.py:121：UserWarning：您的 PyDataset 类应调用  super().__init__(**kwargs) 在其构造函数中。 **kwargs 可以包括 workers、use_multiprocessing、max_queue_size。不要将这些参数传递给 fit()，因为它们将被忽略。
self._warn_if_super_not_used()
737/737 ────────────────────────────── 0s 3s/步 - 准确度：0.9137 - 损失：0.3580
Epoch 1：val_accuracy 从 -inf 提高到 0.90875，将模型保存到 vgg16_uas_classsification.keras
737/737 ────────────────────────────── 2388s 3s/步 - 准确度：0.9137 - 损失：0.3580 - val_accuracy：0.9087 - val_loss：0.3562
纪元 2/10
c:\Program Files\Python311\Lib\contextlib.py:158: UserWarning: 您的输入数据不足；中断训练。确保您的数据集或生成器可以生成至少 steps_per_epoch * epochs 个批次。构建数据集时，您可能需要使用 .repeat() 函数。
self.gen.throw（类型，值，回溯）]]></description>
      <guid>https://stackoverflow.com/questions/78422261/epoch-data-training-using-python-and-keras-and-tensorflow</guid>
      <pubDate>Fri, 03 May 2024 00:57:09 GMT</pubDate>
    </item>
    <item>
      <title>机器学习部署和测试的问题</title>
      <link>https://stackoverflow.com/questions/78422251/issues-with-machine-learning-deployment-and-testing</link>
      <description><![CDATA[我目前正在构建一个机器学习模型，并将其与使用 Python Flask 的网站集成进行部署。我已经成功地训练了模型，并在训练过程中将数据处理成特征和特定数据类型。但是，我不确定如何在预测期间处理实时用户输入数据。有人可以指导我如何在使用 Flask 部署期间将用户输入转换为模型所需的功能和数据类型吗？任何见解或代码示例将不胜感激。
谢谢！”]]></description>
      <guid>https://stackoverflow.com/questions/78422251/issues-with-machine-learning-deployment-and-testing</guid>
      <pubDate>Fri, 03 May 2024 00:51:23 GMT</pubDate>
    </item>
    <item>
      <title>自监督模型收敛到一个常数</title>
      <link>https://stackoverflow.com/questions/78421910/self-supervised-model-converging-to-a-constant</link>
      <description><![CDATA[我试图训练巴洛双胞胎模型进行图像分类。尽管如此，我在完成模型训练后遇到了一个问题。模型似乎已经成为一个常数，无论两个给出的图像有多么不同，它总是返回数字 2046，小数部分略有变化。
该模型尝试将互相关矩阵最小化为单位矩阵。
有没有办法解决这个问题。
def off_diagonal(x):
    # 返回方阵非对角线元素的展平视图

BarlowTwins 类（nn.Module）：
    def __init__(自身, 羔羊,batch_size):
        超级().__init__()
        self.batch_size = 批量大小
        self.lambd = 羔羊
        self.backbone = torchvision.models.resnet34(zero_init_residual=True,weights=&#39;DEFAULT&#39;)
        self.backbone.fc = nn.Identity()
        self.size = [512,2048,2048,2048]


        ＃ 投影仪
        _尺寸 = [512,4096,4096,4096]

        层=[]
        对于范围内的 i(len(self.sizes) - 2)：
            层.追加（nn.Linear（self.sizes [i]，self.sizes [i + 1]，偏差= False））
            Layers.append(nn.BatchNorm1d(self.sizes[i + 1]))
            层.append(nn.ReLU(inplace=True))
        层.追加（nn.Linear（self.sizes [-2]，self.sizes [-1]，偏差= False））
        self.projector = nn.Sequential(*层数)

        # 表示 z1 和 z2 的归一化层
        self.bn = nn.BatchNorm1d(self.sizes[-1], affine=False)

    def 前进（自身，y1，y2）：
        z1 = self.投影仪(self.backbone(y1))
        z2 = self.projector(self.backbone(y2))

        # 经验互相关矩阵
        c = self.bn(z1).T @ self.bn(z2)

        # 对所有 GPU 之间的互相关矩阵求和
        c.div_(self.batch_size)

        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()
        # 打印(&#39;c&#39;, c)
        val = torch.diagonal(c).sum()

        off_diag = off_diagonal(c).pow_(2).sum()
        # print(&#39;off_diag&#39;, off_diag)
        
        损失 = on_diag + self.lambd * off_diag
        回波损耗，值

来自 tqdm 导入 tqdm



def intiate_p（模型，epoch_n，加载器，print_freq，lr，动量，weight_decay）：
    epoch_tqdm = tqdm(范围(epoch_n))
    参数权重 = []
    参数偏差 = []
    r = 打印频率
    函数=模型


    优化器= optim.SGD(model.parameters(),lr=lr,动量=动量,weight_decay=weight_decay)
  

    调度程序= optim.lr_scheduler.PolynomialLR（优化器，total_iters=40，power=2.0）


    损失=[]

    # 目标 = torch.tensor(2048, dtype=torch.float32)
    开始时间 = 时间.time()
    stats_file = open(&#39;stats.txt&#39;, &#39;a&#39;, 缓冲=1)

    
    对于 epoch_tqdm 中的纪元：
        
        对于 enumerate(loader, start=epoch * len(loader)) 中的步骤 ((y1, y2), _)：

            优化器.zero_grad()
            损失 = func.forward(y1, y2)[0]
            损失.追加（损失）
            loss.backward()
            优化器.step()
            调度程序.step()
            epoch_tqdm.set_description(f&quot;损失是: {abs(loss -2048)} &quot;)

  
    回波损耗

这里的问题是我不确定我的模型评估方法是否正确。因为我随机将两个图像作为 y1 和 y2 输入到我的模型中进行 100 次迭代，但结果保持不变。
旁注：我尝试了许多不同的训练变量值，我能得到的最佳损失是 100。
md = BarlowTwins(batch_size=64,lambda=0.005)
t = intiate_p(model=md, epoch_n=20, loader=loader,lr=0.4,momentum=0.3 ,print_freq=10,weight_decay=0.0001)
# 从 2000 左右开始，损失收敛到 100 左右
]]></description>
      <guid>https://stackoverflow.com/questions/78421910/self-supervised-model-converging-to-a-constant</guid>
      <pubDate>Thu, 02 May 2024 22:03:12 GMT</pubDate>
    </item>
    <item>
      <title>进一步发展数据科学和机器学习 [关闭]</title>
      <link>https://stackoverflow.com/questions/78421369/going-further-witth-data-science-and-ml</link>
      <description><![CDATA[我想知道哪些应用程序、平台或书籍等可以极大地帮助我提升水平并深入了解以下内容：

熊猫
Numpy
Scipy
Keras 和 Tensorflow
Pytorch
Scikit 学习
Matplotlib

除了 pytorch 之外，我对所有这些都有经验，但我真的想走得更远，因为我未来的潜在工作需要这些。]]></description>
      <guid>https://stackoverflow.com/questions/78421369/going-further-witth-data-science-and-ml</guid>
      <pubDate>Thu, 02 May 2024 19:29:01 GMT</pubDate>
    </item>
    <item>
      <title>Unsloth 未检测到 CUDA 和“str2optimizer32bit”</title>
      <link>https://stackoverflow.com/questions/78420830/unsloth-not-detecting-cuda-and-str2optimizer32bit</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78420830/unsloth-not-detecting-cuda-and-str2optimizer32bit</guid>
      <pubDate>Thu, 02 May 2024 17:30:58 GMT</pubDate>
    </item>
    <item>
      <title>使用多个性能指标执行递归特征消除</title>
      <link>https://stackoverflow.com/questions/78420559/performing-recursive-feature-elimination-with-multiple-performance-metrics</link>
      <description><![CDATA[我正在尝试使分类器尽可能简洁。为此，我使用交叉验证递归地删除功能。
在我的模型中，精度是最重要的指标。但是，我还想看看随着模型中输入的功能减少，其他指标将如何演变。
特别是，我想评估模型的召回率和 F1 分数。
rfe = RFECV(
    estimator=clf, # 一个 XGBClassifier 实例
    步骤=1，
    min_features_to_select=1,
    cv=cv, # 一个 StratifiedKFold 实例
    评分=&#39;精度&#39;,
    # 评分=[&#39;f1&#39;, &#39;精度&#39;, &#39;召回&#39;], # 抛出错误
    详细=1，
    n_职位=1
）

我注释掉了将指标列表传递给 scoring 参数的行，因为它抛出 InvalidParameterError （也就是说，它不高兴我向它传递了一个列表）。
有没有办法将多个指标传递给 RFECV 实例？]]></description>
      <guid>https://stackoverflow.com/questions/78420559/performing-recursive-feature-elimination-with-multiple-performance-metrics</guid>
      <pubDate>Thu, 02 May 2024 16:37:03 GMT</pubDate>
    </item>
    <item>
      <title>如何使用拥抱脸部变压器来测试具有 LLM 的数据集？</title>
      <link>https://stackoverflow.com/questions/78420480/how-to-use-hugging-face-transformers-for-testing-a-dataset-with-llms</link>
      <description><![CDATA[我正在努力复制此存储库的结果，但使用其他LLM比如骆驼。我正在使用 google colab，我已经克隆了存储库并安装了所需的软件包。
他们说你可以使用拥抱脸变形金刚中的任何模型，但我不知道在哪里可以获得“模型”和“model_args”参数：
# 在 ENEM 2022 上使用 GPT-4V 的 CoT 运行 3-shot
python main.py \
    --模型chatgpt \
    --model_args 引擎=gpt-4-vision-preview \
    --任务 enem_cot_2022_blind、enem_cot_2022_images、enem_cot_2022_captions \
    --description_dict_path 描述.json \
    --num_fewshot 3 \
    --conversation_template chatgpt

如果您转到抱脸的骆驼模型并且单击“在变形金刚中使用”你得到这个：
从变压器导入 AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained(“meta-llama/Meta-Llama-3-8B”)
模型 = AutoModelForCausalLM.from_pretrained(“meta-llama/Meta-Llama-3-8B”)

所以我尝试使用“model = meta-llama”和“model_args=Meta-Llama-3-8B”但这行不通。
就像这样：
!python main.py \
  --模型元美洲驼 \
  --model_args Meta-Llama-3-8B \
  --任务 enem_cot_2022_blind,enem_cot_2022_captions \
  --description_dict_path 描述.json \
  --num_fewshot 3

我得到：
选定的任务：[&#39;enem_cot_2022_blind&#39;、&#39;enem_cot_2022_captions&#39;]
回溯（最近一次调用最后一次）：
  文件“/content/gpt-4-enem/main.py”，第 112 行，在  中。
    主要的（）
  文件“/content/gpt-4-enem/main.py”，第 81 行，在 main 中
    结果 = evaluator.simple_evaluate(
  文件“/content/gpt-4-enem/lm_eval/utils.py”，第 164 行，位于 _wrapper 中
    返回 fn(*args, **kwargs)
  文件“/content/gpt-4-enem/lm_eval/evaluator.py”，第 66 行，位于 simple_evaluate 中
    lm = lm_eval.models.get_model(model).create_from_arg_string(
  文件“/content/gpt-4-enem/lm_eval/models/__init__.py”，第 16 行，在 get_model 中
    返回 MODEL_REGISTRY[模型名称]
KeyError：“元骆驼”
]]></description>
      <guid>https://stackoverflow.com/questions/78420480/how-to-use-hugging-face-transformers-for-testing-a-dataset-with-llms</guid>
      <pubDate>Thu, 02 May 2024 16:20:28 GMT</pubDate>
    </item>
    <item>
      <title>使用“loss.backward()”函数优化模型参数问题</title>
      <link>https://stackoverflow.com/questions/78420392/optimizing-model-parameters-issue-with-loss-backward-function</link>
      <description><![CDATA[导入GC

从 tqdm.notebook 导入 tqdm

将 matplotlib.pyplot 导入为 plt

torch.set_printoptions(sci_mode=False)


梯度范数 = []

损失=[]

对于 tqdm 中的纪元（范围（纪元））：

    model_path = f“/kaggle/working/Training/ace_state_dict_{epoch+1}.pth”

    torch.save(model.state_dict(), model_path)

    模型.train()

    总损失= 0

    
    对于batch_idx，批量枚举(tqdm(train_dataloader, desc=f&#39;Epoch {epoch + 1}/{epochs}&#39;))：

        优化器.zero_grad()

        logits = model(batch[“输入”])

        目标=批处理[“目标”]

        损失 = loss_fn(logits.view(-1, logits.size(-1)), Targets.float()) / 1000000000

        loss.backward()

        # 计算梯度范数

        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1)

        gradient_norms.append(grad_norm)

        优化器.step()

        调度程序.step()

        总损失 += loss.item()
        

        如果batch_idx％100==0：

            print(f&#39;Batch {batch_idx}/{len(train_dataloader)}，损失：{total_loss/(batch_idx+1)}，梯度范数：{grad_norm}&#39;)

    
    avg_loss = 总损失 / len(train_dataloader)

    损失.append(avg_loss)

    print(f&quot;列车损失：{avg_loss}&quot;)`

在我上面的代码中，loss.backward() 函数未按预期工作，导致 grad_norm 始终注册为 0.0 。这阻碍了每次迭代期间模型参数的优化。有任何指导或建议来纠正这个问题吗？作为参考，您可以在以下位置找到我的模型的完整源代码：
https://www.kaggle.com/code/cutedeadu943/transformerchatbot
我尝试了超参数和学习率的各种组合，但不幸的是，问题仍然存在。此外，我尝试通过使用 torch.nn.utils.clip_grad_norm_() 来解决这个问题，但没有成功。所有相关张量和模型参数均使用 requires_grad=True 设置。有什么见解或替代方法可以解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78420392/optimizing-model-parameters-issue-with-loss-backward-function</guid>
      <pubDate>Thu, 02 May 2024 16:00:27 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练 GAN 模型时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/78420289/getting-valueerror-while-trying-to-train-gan-model</link>
      <description><![CDATA[我正在尝试训练 GAN 模型来检测糖尿病视网膜病变图像，但它抛出错误。请帮忙。
图像数据集不为空我已尝试查看它
错误是：-
纪元 1/50
回溯（最近一次调用最后一次）：
  文件“C:\Users\asus\OneDrive\Desktop\project\DR-GAN\TrainModel.py”，第 65 行，在  中
    分类器.fit（X，Y，batch_size = 32，epochs = 50）
  文件“C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
    从 None 引发 e.with_traceback(filtered_tb)
  文件“C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\backend\tensorflow\nn.py”，第 553 行，在 categorical_crossentropy 中
    引发值错误（
ValueError：参数“target”和“output”必须具有相同的形状。收到：target.shape=(无，3)，output.shape=(无，5)

火车模型文件的代码是：
将 numpy 导入为 np
导入imutils
导入系统
导入CV2
导入操作系统
从tensorflow.keras.utils导入到_categorical
从 keras.models 导入 model_from_json
从 keras.layers 导入 MaxPooling2D
从 keras.layers 导入密集、丢弃、激活、扁平化
从 keras.layers 导入 Convolution2D
从 keras.models 导入顺序

图片 = []
图像标签 = []
目录=&#39;数据集&#39;
文件列表 = os.listdir(目录)
索引 = 0
对于 list_of_files 中的文件：
    子文件 = os.listdir(目录+&#39;/&#39;+文件)
    对于子文件中的子项：
        路径=目录+&#39;/&#39;+文件+&#39;/&#39;+子
        img = cv2.imread(路径)
        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        如果 img 为 None：
          print(&#39;路径错误：&#39;, 路径)
        别的：
         img = cv2.resize(img, (32,32))
         im2arr = np.array(img)
         im2arr = im2arr.reshape(32,32,3)
         图像.append(im2arr)
         image_labels.append(文件)
    打印（文件）

X = np.asarray(图像)
Y = np.asarray(image_labels)
Y = to_categorical(Y)
img = X[20].reshape(32,32,3)
cv2.imshow(&#39;ff&#39;,cv2.resize(img,(250,250)))
cv2.waitKey(0)
print(&quot;形状 == &quot;+str(X.shape))
print(&quot;形状==&quot;+str(Y.shape))
打印（Y）
X = X.astype(&#39;float32&#39;)
X = X/255

np.save(“model/img_data.txt”,X)
np.save(“model/img_label.txt”,Y)

X = np.load(&#39;model/img_data.txt.npy&#39;)
Y = np.load(&#39;model/img_label.txt.npy&#39;)
打印（Y）
img = X[20].reshape(32,32,3)
cv2.imshow(&#39;ff&#39;,cv2.resize(img,(250,250)))
cv2.waitKey(0)

classifier = Sequential() #alexnet 迁移学习代码在这里
classifier.add(Convolution2D(32, 3, 3, input_shape = (32, 32, 3), 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D((2, 2) , padding=&#39;相同&#39;))
classifier.add(Convolution2D(32, 3, 3, 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D((2, 2) , padding=&#39;相同&#39;))
分类器.add(Flatten())
classifier.add（密集（单位= 128，激活=&#39;relu&#39;））
classifier.add(Dense(单位 = 5, 激活 = &#39;softmax&#39;))
classifier.compile（优化器=&#39;adam&#39;，损失=&#39;categorical_crossentropy&#39;，指标= [&#39;准确性&#39;]）
分类器.fit（X，Y，batch_size = 32，epochs = 50）

我尝试过更改尺寸，但它不起作用，我无法理解这是版本错误还是代码错误，因此为了解决同样的问题，请提供解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78420289/getting-valueerror-while-trying-to-train-gan-model</guid>
      <pubDate>Thu, 02 May 2024 15:39:54 GMT</pubDate>
    </item>
    <item>
      <title>ConvLSTM 模型的数据预处理过程中遇到问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78418879/having-trouble-during-preprocessing-of-data-for-a-convlstm-model</link>
      <description><![CDATA[我有 10 年（2014-24）的 GHRSST 数据，我将其分为两部分：训练（2014-2021）和测试（2021-2024）数据集。训练数据集大小为 4.18GB，测试数据集大小为 1.98GB。我正在尝试构建一个 ConvLSTM 模型来预测未来几天的 SST 数据，但是我似乎无法通过预处理阶段。我正在尝试在 google colab 上执行以下代码：
导入火炬
将 numpy 导入为 np
进口达斯克

# NetCDF 文件的路径
dataset_path = &#39;/content/drive/MyDrive/train_dataset_10.nc&#39;
# 使用 dask chunks 打开数据集
ds = xr.open_mfdataset(dataset_path, chunks={&#39;时间&#39;: 1, &#39;纬度&#39;: 50, &#39;经度&#39;: 50})

# 提取SST数据；假设变量名为“analysis_sst”，并使用计算将其转换为 numpy 来加载数据
sst_data = ds[&#39;analysisd_sst&#39;].compute() # 这将确保数据加载到内存中

上述代码需要 30 分钟执行，并占用 10.2/12.7GB 可用系统 RAM。这也会导致以下代码出现问题，该代码使用过多的 CPU（尽管 GPU 处于打开状态）并且由于缺少 RAM 而导致内核崩溃：
# 清除 CUDA 中所有未使用的内存
torch.cuda.empty_cache()

# 将数据转换为张量，出于兼容性原因确保它首先在 CPU 上
sst_tensor = torch.tensor(sst_data.values, dtype=torch.float32)

# 如果有可用的 GPU，则将张量传输到 GPU
如果 torch.cuda.is_available():
    sst_tensor = sst_tensor.to(&#39;cuda&#39;)

print(&quot;SST 张量的形状：&quot;, sst_tensor.shape)

defscale_data_gpu(data_tensor,batch_size):
    scaled_data = torch.full_like(data_tensor, float(&#39;nan&#39;)) # 为填充 NaN 的缩放数据初始化张量

    # 批量处理数据
    对于范围内的开始（0，data_tensor.shape [0]，batch_size）：
        结束 = 开始 + 批次大小
        批处理 = data_tensor[开始:结束]

        # 为有效（非 NaN）数据点创建掩码
        valid_mask = ~torch.isnan(batch)

        if valid_mask.any(): # 确保至少有一些有效数据
            data_min = torch.min(batch[valid_mask])
            data_max = torch.max(batch[valid_mask])

            # 缩放批次，但仅在有效的情况下应用
            batch_scaled = (batch - data_min) / (data_max - data_min)
            scaled_data[开始:结束][valid_mask] = batch_scaled[valid_mask]

    返回缩放数据

# 在 GPU 上应用缩放，仅考虑有效（非 NaN）值
使用 torch.no_grad()：
  sst_scaled_tensor = scale_adata_gpu（data_tensor = sst_tensor，batch_size = 64）

在上面的代码中，我试图掩盖标记为“NaN”的值因为它代表了我的数据集中的地形，然后缩放数据并批量处理它以准备训练。
我该如何进行这项工作？这个过程需要多长时间？有更有效的方法吗？
我尝试过使用 Dask，但没有成功，而且 GEE 不支持 CNN。]]></description>
      <guid>https://stackoverflow.com/questions/78418879/having-trouble-during-preprocessing-of-data-for-a-convlstm-model</guid>
      <pubDate>Thu, 02 May 2024 11:34:30 GMT</pubDate>
    </item>
    <item>
      <title>“numpy.ndarray”在 Ai 模型中插入 Flask 时出现问题</title>
      <link>https://stackoverflow.com/questions/78418741/numpy-ndarray-a-problem-in-flask-inserting-in-ai-model</link>
      <description><![CDATA[我想将字符串插入到机器学习模型中，但它一直这样说：
 预测 = model.predict(email_features_array)
                 ^^^^^^^^^^^^^^
AttributeError：“numpy.ndarray”对象没有属性“预测”

不知道是什么问题，换了好几次解码方式。
这是我在进行矢量化后使用的函数：
def 预测():
    email_text =“你好世界”
    CV = CountVectorizer()
    email_features_array = cv.fit_transform([email_text])
    # email_features_array = fit_count_vectorizer(email_text)
    打印（电子邮件特征数组）
    # 假设`model`已经被定义和训练
    预测 = model.predict(email_features_array)

    # 应用预训练模型来预测电子邮件被钓鱼的概率
    概率 = model.predict_proba(email_features_array)
    如果预测[0] == 1：
        结果=“网络钓鱼”
        概率得分 = 概率[0][1] * 100
        打印（结果，概率分数）
    别的：
        结果=&#39;合法&#39;
        概率得分 = 概率[0][0] * 100
        打印（结果，概率分数）

我预计解码方式有问题，但不确定。]]></description>
      <guid>https://stackoverflow.com/questions/78418741/numpy-ndarray-a-problem-in-flask-inserting-in-ai-model</guid>
      <pubDate>Thu, 02 May 2024 11:10:21 GMT</pubDate>
    </item>
    <item>
      <title>如何将关系数据库集成到数据科学项目中？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78418612/how-to-integrate-a-relational-database-into-a-data-science-project</link>
      <description><![CDATA[我是一名数据科学家，主要使用 CSV 文件进行数据分析，但我现在正在探索在我的项目中使用关系数据库。我想了解将关系数据库集成到我的工作流程中的最佳实践。
我应该如何将数据从关系数据库（例如 PostgreSQL、MySQL）导入到我的数据科学环境（例如 Python、R）中？我应该直接在数据库中执行连接和探索性数据分析，还是应该将数据导出到 CSV 文件，然后继续分析？
我过去主要使用 CSV 文件，但现在我正在着手一个现实世界的数据科学项目，我需要在其中使用关系数据库。不过，我对此还比较陌生，正在寻求有关如何有效地将数据库集成到我的工作流程中的指导。]]></description>
      <guid>https://stackoverflow.com/questions/78418612/how-to-integrate-a-relational-database-into-a-data-science-project</guid>
      <pubDate>Thu, 02 May 2024 10:49:01 GMT</pubDate>
    </item>
    <item>
      <title>IndexError：列表索引超出streamlit范围[关闭]</title>
      <link>https://stackoverflow.com/questions/78418486/indexerror-list-index-out-of-range-in-streamlit</link>
      <description><![CDATA[因此，我正在尝试构建一个 Streamlit RAG 应用程序，该应用程序从 url 中提取信息并从中学习，然后用户可以向模型询问与 url 中的文章相关的问题，模型将提供合适的答案。
我在我的笔记本上执行了此操作，它工作得很好，只是在我的 Streamlit 应用程序中遇到 IndexError: list index out of range 错误，我将 GoogleGenerativeAIEmbeddings 与 FAISS 结合使用。
这是代码块
 main_placeholder = sl.empty()
    llm = ChatGoogleGenerativeAI(模型 = &#39;gemini-pro&#39;)
    如果 process_url_clicked:
        加载器 = UnstructedURLLoader(urls = urls)
        main_placeholder.text(&quot;数据加载...开始...✅✅✅&quot;)
        数据 = 加载器.load()
        text_splitter = RecursiveCharacterTextSplitter(
            分隔符 = [&#39;\n&#39;,&#39;\n\n&#39;,&#39;.&#39;,&#39;,&#39;],
            块大小 = 1000,
            块重叠 = 200
        ）
        main_placeholder.text(&quot;文本分割器...开始...✅✅✅&quot;)
        文档 = text_splitter.split_documents(数据)
        嵌入 = GoogleGenerativeAIEmbeddings(模型 = &#39;models/embedding-001&#39;)
        矢量索引= FAISS.from_documents（文档，嵌入）

这是来自 Streamlit 应用程序的回溯
IndexError：列表索引超出范围
追溯：
文件“C:\Python312\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py”，第 584 行，位于 _run_script
    exec（代码，模块.__dict__）
文件“C:\Users\owner\Desktop\Projects\nlp\main.py”，第 84 行，在  中
    vectorstore_openai = FAISS.from_documents（文档，嵌入）
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_core\vectorstores.py”，第 550 行，from_documents
    返回 cls.from_texts(文本、嵌入、元数据=元数据、**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_community\vectorstores\faiss.py”，第 931 行，from_texts
    返回 cls.__from(
           ^^^^^^^^^^^
文件“C:\Python312\Lib\site-packages\langchain_community\vectorstores\faiss.py”，第 888 行，位于 __from
    索引 = faiss.IndexFlatL2(len(embeddings[0]))
                                  ~~~~~~~~~~^^^

就像我上面说的，这在我的笔记本上完美运行，我很困惑为什么会发生这种情况]]></description>
      <guid>https://stackoverflow.com/questions/78418486/indexerror-list-index-out-of-range-in-streamlit</guid>
      <pubDate>Thu, 02 May 2024 10:25:27 GMT</pubDate>
    </item>
    <item>
      <title>我们如何以优雅的方式捕获使用optimizer.step()完成的更新？</title>
      <link>https://stackoverflow.com/questions/78392429/how-can-we-capture-update-done-with-optimizer-step-in-an-elegant-way</link>
      <description><![CDATA[我想实现一种方法，按照 Karpathy 视频中提到的想法，在使用 PyTorch 训练期间在 Tensorboard 中监控更新数据比率。我已经提出了一个解决方案，但我正在寻找一种更优雅且可配置的方法。
当前的实现直接修改训练循环如下：
对于步骤，在 data_loader 中进行批处理：
    x, y = 批次
    优化器.zero_grad()
    对于名称，model.named_pa​​rameters() 中的参数：
        if param.requires_grad 和“weight”名称：
            param.data_before_step = param.data.clone()
    输出=模型(x)
    损失 = loss_fn(输出, y)
    loss.backward()
    优化器.step()
    lr_scheduler.step()
    对于名称，model.named_pa​​rameters() 中的参数：
        if hasattr(param, “data_before_step”):
            更新 = param.data - param.data_before_step
            update_to_data = (update.std() / param.data_before_step.std()).log10().item()
            summary_writer.add_scalar(f“更新：数据比率 {name}”，update_to_data，epoch * len(data_loader) + 步骤)
            param.data_before_step = param.data.clone()

但是，这种方法直接在训练循环中添加代码，这可能会使代码变得混乱，如果我们想要使其可配置，则需要 if-else 语句，这会使代码更加混乱。
我还探索过使用 PyTorch hooks 来实现这一点。我已经成功实现了一个钩子来跟踪梯度：
类 GradToDataRatioHook：
    def __init__(自身、名称、参数、start_step、summary_writer):
        self.name = 姓名
        self.param = 参数
        self.summary_writer = 摘要_writer
        自我.毕业生 = []
        self.grads_to_data = []
        self.param.update_step = start_step

    def __call__(自我，毕业生)：
        self.grads.append(grad.std().item())
        self.grads_to_data.append((grad.std() / (self.param.data.std() + 1e-5)).log10().item())
        self.summary_writer.add_scalar(f&quot;Grad {self.name}&quot;, self.grads[-1], self.param.update_step)
        self.summary_writer.add_scalar(f&quot;梯度:数据比例{self.name}&quot;, self.grads_to_data[-1], self.param.update_step)
        self.param.update_step += 1

但是，实现类似的钩子来捕获更新似乎很棘手。据我了解， param.register_hook(...) 注册了钩子，该钩子在计算梯度时调用，即在 optimizer.step() 之前调用叫。虽然梯度和学习率为标准 SGD 提供了更新的直接值，但像 Adam 这样的现代优化器使更新过程变得更加复杂。我正在寻找一种以与优化器无关的方式捕获更新的解决方案，最好使用 PyTorch 挂钩。但是，任何建议或替代方法也将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78392429/how-can-we-capture-update-done-with-optimizer-step-in-an-elegant-way</guid>
      <pubDate>Fri, 26 Apr 2024 18:32:22 GMT</pubDate>
    </item>
    <item>
      <title>如何用 Python 制作人工智能自学习聊天机器人 [关闭]</title>
      <link>https://stackoverflow.com/questions/78370101/how-to-make-an-ai-self-learning-chatbot-in-python</link>
      <description><![CDATA[我一直在尝试用 Python 制作一个自学习聊天机器人，并尝试了不同的库，如 NLTK、TensorFlow、ChatBot 和 PyTorch，但所有这些库都在处理预定义的训练数据。我找不到任何选项来根据给定的输入自行训练模型并尝试不同类型的数据集。
Python 有什么方法可以实现这一点吗？我可以看到我们可以使用已经训练好的模型并对其进行微调，或者使用 DialogFlow 和 Rasa 来建立对话模型。然而，我正在寻找一种方法，我们可以用我们自己的数据来训练它，它可以从给定的数据中学习并对给定的提示产生自己的非预定答复。
也许您可以提供一个特定的代码片段，甚至只是代码或伪代码的概要供我使用？谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78370101/how-to-make-an-ai-self-learning-chatbot-in-python</guid>
      <pubDate>Tue, 23 Apr 2024 05:44:52 GMT</pubDate>
    </item>
    </channel>
</rss>