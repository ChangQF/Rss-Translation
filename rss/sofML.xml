<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 23 Oct 2024 12:33:03 GMT</lastBuildDate>
    <item>
      <title>是否有任何文本转音频的 AI 模型可以提供逼真的人声（免费）？[关闭]</title>
      <link>https://stackoverflow.com/questions/79117732/are-there-any-text-to-audio-ai-models-that-provide-realistic-human-voices-for-f</link>
      <description><![CDATA[我正在寻找有关 AI 模型的建议，这些模型可以将文本转换为具有高度逼真的人声的音频。我需要一种可以生成自然语音的解决方案，理想情况下，它还支持不同的语言、口音和语调。如果模型可以处理音调、情绪和节奏的变化，使输出更加逼真，那就太好了。有没有关于提供这种级别语音真实感的预建 API 或开源模型的建议？
我希望找到一种文本转语音模型，它可以提供更自然、更像人类的语音，更好地处理情绪基调、口音和语调。理想情况下，它应该易于实施、提供 API 访问或具有简化的开源设置。]]></description>
      <guid>https://stackoverflow.com/questions/79117732/are-there-any-text-to-audio-ai-models-that-provide-realistic-human-voices-for-f</guid>
      <pubDate>Wed, 23 Oct 2024 11:32:59 GMT</pubDate>
    </item>
    <item>
      <title>模块“keras.api.backend”没有属性“clip”</title>
      <link>https://stackoverflow.com/questions/79116828/module-keras-api-backend-has-no-attribute-clip</link>
      <description><![CDATA[我使用 Colab 进行编码并收到此错误：
AttributeError：模块“keras.api.backend”没有属性“clip”。

我尝试升级 TensorFlow 和 Keras，但仍然收到相同的错误。我在第一个 epoch 拟合模型时收到此错误。
我该如何修复它？
import fragmentation_models as sm
model_vgg16=sm.Unet(backbone_name=backbone,input_shape=(256,256,3),classes=4,activation=&quot;softmax&quot;,encoder_weights=&quot;imagenet&quot;,decoder_use_batchnorm=True,encoder_freeze=False )

model_vgg16.summary()

&quot;&quot;&quot;# loss and metrics&quot;&quot;&quot;

loss=&quot;categorical_crossentropy&quot;

dice_loss=sm.losses.DiceLoss()
focal_loss=sm.losses.CategoricalFocalLoss()

focal_dice_loss=sm.losses.categorical_focal_dice_loss

metric=[sm.metrics.IOUScore(threshold=0.5)]

&quot;&quot;&quot;# compile&quot;&quot;&quot;

lr=0.001
model_vgg16.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),
loss=[focal_dice_loss],
metrics=[metric])

history = model_vgg16.fit(preprocessed_x_train, ytrain_categorical, epochs=20,validation_data=(preprocessed_x_val,y_val_categorical),batch_size=32)

错误：
Epoch 1/20
--------------------------------------------------------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
&lt;ipython-input-76-887dcd97e6be&gt; 在 &lt;cell line: 1&gt;()
----&gt; 1 history = model_vgg16.fit(preprocessed_x_train, ytrain_categorical, epochs=20,
2 validation_data=(preprocessed_x_val,y_val_categorical),
3 batch_size=32)

3 帧
/usr/local/lib/python3.10/dist-packages/segmentation_models/base/ functional.py in categorical_focal_loss(gt, pr, gamma, alpha, class_indexes, **kwargs)
276 
277 # 剪辑以防止 NaN 和 Inf
--&gt; 278 pr = backend.clip(pr, backend.epsilon(), 1.0 - backend.epsilon())
279 
280 # 计算焦点损失

AttributeError: 模块 &#39;keras.api.backend&#39; 没有属性 &#39;clip&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/79116828/module-keras-api-backend-has-no-attribute-clip</guid>
      <pubDate>Wed, 23 Oct 2024 07:29:46 GMT</pubDate>
    </item>
    <item>
      <title>llama 的 llm 链抛出错误，输入应为可运行实例</title>
      <link>https://stackoverflow.com/questions/79116565/llm-chain-for-llama-is-throwing-error-that-input-should-be-an-instance-of-runnab</link>
      <description><![CDATA[我有以下代码
quantization_config = BitsAndBytesConfig(load_in_4bit=True,
llm_int4_enable_fp32_cpu_offload=True)

sumModel = &quot;meta-llama/Llama-3.2-1B&quot;

llm_llama_model = AutoModelForCausalLM.from_pretrained(
sumModel,
torch_dtype=torch.float32,
temperature =0,
device_map=&#39;auto&#39;,
quantization_config=quantization_config
)

llm_llama_tokenizer = AutoTokenizer.from_pretrained(sumModel)

summary_chain = load_summarize_chain(llm=llm_llama_model, chain_type=&#39;map_reduce&#39;,
verbose=True # 如果您想查看正在使用的提示，请设置 verbose=True
)

我收到以下错误：
ValidationError Traceback (most recent call last)
&lt;ipython-input-18-d8e6a6acabfd&gt;在 &lt;cell line: 1&gt;()
----&gt; 1 summary_chain = load_summarize_chain(llm=llm_llama_model, chain_type=&#39;map_reduce&#39;,
2 verbose=True # 如果要查看正在使用的提示，请设置 verbose=True
3 )

4 frames
/usr/local/lib/python3.10/dist-packages/pydantic/main.py in __init__(self, **data)
210 # `__tracebackhide__` 告诉 pytest 和其他一些工具从回溯中省略此函数
211 __tracebackhide__ = True
--&gt; 212 valided_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
213 if self is not valided_self:
214 warnings.warn(

ValidationError: LLMChain 的 2 个验证错误
llm.is-instance[Runnable]
输入应为 Runnable 的一个实例 [type=is_instance_of, input_value=LlamaForCausalLM(
(mode...es=128256, bias=False)
), input_type=LlamaForCausalLM]
有关更多信息，请访问 https://errors.pydantic.dev/2.9/v/is_instance_of
llm.is-instance[Runnable]
输入应为 Runnable 的一个实例 [type=is_instance_of, input_value=LlamaForCausalLM(
(mode...es=128256, bias=False)
), input_type=LlamaForCausalLM]
有关更多信息，请访问 https://errors.pydantic.dev/2.9/v/is_instance_of


错误指示什么以及如何解决？
Google 给了我几个链接，但相关链接提到要升级 langchain 或 pydantic。]]></description>
      <guid>https://stackoverflow.com/questions/79116565/llm-chain-for-llama-is-throwing-error-that-input-should-be-an-instance-of-runnab</guid>
      <pubDate>Wed, 23 Oct 2024 06:01:07 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：mat1 和 mat2 必须具有相同的 dtype，但得到的是 Long 和 Float</title>
      <link>https://stackoverflow.com/questions/79116516/runtimeerror-mat1-and-mat2-must-have-the-same-dtype-but-got-long-and-float</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79116516/runtimeerror-mat1-and-mat2-must-have-the-same-dtype-but-got-long-and-float</guid>
      <pubDate>Wed, 23 Oct 2024 05:42:30 GMT</pubDate>
    </item>
    <item>
      <title>LeNet的训练结果保持不变</title>
      <link>https://stackoverflow.com/questions/79116509/the-training-results-of-lenet-remain-unchanged</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79116509/the-training-results-of-lenet-remain-unchanged</guid>
      <pubDate>Wed, 23 Oct 2024 05:40:55 GMT</pubDate>
    </item>
    <item>
      <title>在单线段（触觉图）上进行图像相似性比较的理想方法是什么？</title>
      <link>https://stackoverflow.com/questions/79116100/ideal-way-to-do-image-similarity-comparisons-on-single-line-segments-tactile-ma</link>
      <description><![CDATA[我正在尝试寻找一种理想且最准确的方法来对手绘触觉地图和其模板进行图像相似性比较。目标是提供一个值来显示手绘与模板的相似程度。图像要么是：完整的触觉地图绘图，要么是相同地图绘图的最短路径。这些图像中没有轮廓，因此这是分析中最困难的部分，因为它们都是单线段。

手绘地图
触觉地图模板

我不知道该怎么做了。请提供您的想法和可能的解决方案来完成这个图像相似性任务！随时向我询问任何可能有助于您回答的问题。
我尝试过的方法有：

使用自定义 Matlab 脚本对手绘到模板执行不同的仿射变换（缩放、旋转、剪切、拉伸、translateX、translateY），根据不同的值提供从 0 到 1 的分数。这没有按预期工作，因为有时，转换不知道手绘的垂直线段应该与模板中的正确垂直线段对齐，特别是如果绘图未完成，具有模板图的大多数特征的完整绘图就可以了。将图像绘图分成象限会有帮助吗？

使用 Gemini 模型：1.5-Flash、1.5-Pro 和 1.0-Pro-vision 来比较两个输入图像，这两个图像也提供了不合适的分数（例如，一幅绘图，人类会给它 2 分（满分 10 分），但 Gemini 模型给了它 7 分（满分 10 分）。我还尝试使用 Gemini API 密钥对 Gemini-1.5-Flash-001 模型进行微调（我提供了 963 个相似度分数应该输出的示例（人类评分），但当我在 183 张验证图像上运行它时，它仍然给出了不合适的分数，比如给一幅糟糕的绘图更高的分数，而同一张地图的好的绘图的分数要低得多）。我曾尝试让模型给出分数推理的口头描述，但它产生了幻觉，并给出了完全错误的分割方向和方向的空间分析！例如：绘图将是垂直的线段，然后 90 度角右转水平线段，但它会表示完全不同的方向。我给 Gemini 的提示：
重要提示：完全忽略线条质量和样式的所有方面，包括平滑度、曲线、宽度和小的不规则性。将所有线条视为连接关键点的完美直线。

比较这两幅图像并提供 0 到 10 之间的相似度分数，其中 0 表示完全不同，高分表示所有局部特征都处于正确的位置，基于以下评分指南：

相似度评分指南标准：
- 整体结构布局
- 线段长度成比例
- 线段之间的角度允许 ±15° 变化
- 每次角度变化时纠正连续方向线段左转或右转变化

回答：
- 来自总评分标准的单个数字分数（0 到 10）
- 用 2-3 句话解释，不多说，说明分配的评分标准，突出第一幅和第二幅图像之间的关键相似点或差异


]]></description>
      <guid>https://stackoverflow.com/questions/79116100/ideal-way-to-do-image-similarity-comparisons-on-single-line-segments-tactile-ma</guid>
      <pubDate>Wed, 23 Oct 2024 00:36:59 GMT</pubDate>
    </item>
    <item>
      <title>使用计算机视觉查找图像中的异常/缺失部分</title>
      <link>https://stackoverflow.com/questions/79115794/finding-anomalies-missing-parts-in-image-using-computer-vision</link>
      <description><![CDATA[我正在尝试创建一个程序以便在工业环境中验证组装的组件。基本上，我希望能够检测到组件是否缺少任何部分，例如螺钉或支架等。
到目前为止我尝试过什么？

我的第一个想法是简单地拍摄一个正确的组件的参考图像，然后使用基于特征的相似度得分将所有其他图像与该图像进行比较（如在这个问题的答案中所述）。即使图像非常相似，但有一个明显的区别（请参阅发布的图像，忽略蓝色标记），这也不会产生任何有用的结果。
我还尝试过其他技术，例如结构相似性
指数，但根本不起作用。


第一张图片是故障组件的示例，缺少右下角的支架和螺钉，第二张图片是正确的组件。这只是一个例子，其他东西，例如左上角的螺丝或 4 个橡胶片也可能丢失。
我的问题：

为什么特征相似性效果如此不佳，即使图片除了缺少支架外基本上相同？
解决这个问题的合理方法是什么？

我的下一个想法是在正确组件的图像以及不正确组件的变体上训练分类器，但可能存在许多不同的变体，因此这可能会耗费大量资源。另一个想法是在正确组件的图像上训练异常检测模型，希望它能够检测到不正确组件图像中的异常。到目前为止，我还不确定任何方法。
此外，我们可以做出以下假设：

组件在图像中通常具有一致的方向。
图像中的光照条件将保持相当均匀。
]]></description>
      <guid>https://stackoverflow.com/questions/79115794/finding-anomalies-missing-parts-in-image-using-computer-vision</guid>
      <pubDate>Tue, 22 Oct 2024 21:32:38 GMT</pubDate>
    </item>
    <item>
      <title>为什么现代对象检测模型的 Github 存储库会显示在那个奇怪的命令行界面中？[关闭]</title>
      <link>https://stackoverflow.com/questions/79115741/why-github-repos-of-modern-object-detection-model-be-presented-in-that-weird-com</link>
      <description><![CDATA[我正在研究许多用于对象检测的 Github 存储库。然而，无论我在哪里看到，这些存储库中的训练/评估代码部分总是使用参数解析器在命令行界面中显示。这是我正在研究的一个 repo 中的一个例子：
parser.add_argument(&quot;--dataset_type&quot;, default=&quot;voc&quot;, type=str,
help=&#39;指定数据集类型。目前支持 voc 和 open_images。&#39;)

parser.add_argument(&#39;--datasets&#39;, nargs=&#39;+&#39;, help=&#39;数据集目录路径&#39;)
parser.add_argument(&#39;--validation_dataset&#39;, help=&#39;数据集目录路径&#39;)
parser.add_argument(&#39;--balance_data&#39;, action=&#39;store_true&#39;,
help=&quot;通过对更频繁的标签进行下采样来平衡训练数据。&quot;)

parser.add_argument(&#39;--net&#39;, default=&quot;vgg16-ssd&quot;,
help=&quot;网络架构，它可以是mb1-ssd、mb1-lite-ssd、mb2-ssd-lite 或 vgg16-ssd。&quot;)
parser.add_argument(&#39;--freeze_base_net&#39;, action=&#39;store_true&#39;,
help=&quot;冻结基础网络层。&quot;)
parser.add_argument(&#39;--freeze_net&#39;, action=&#39;store_true&#39;,
help=&quot;冻结除预测头之外的所有层。&quot;)

parser.add_argument(&#39;--mb2_width_mult&#39;, default=1.0, type=float,
help=&#39;MobilenetV2 的宽度乘数&#39;)

我的意思是，为什么？参数解析器使得阅读代码和查看变量去向变得非常非常困难。上面的代码中，参数解析器的代码有将近百行，看清参数是什么真的让我眼花缭乱。而且 Python 语法难道还不够方便吗，只需将所有训练参数转储到配置文件中即可？为什么每次训练时都要输入 3 行终端命令，而且为什么要将一个方便的 Python 程序变成看起来像命令行 ffmpeg 的东西？请有人给我解释一下。]]></description>
      <guid>https://stackoverflow.com/questions/79115741/why-github-repos-of-modern-object-detection-model-be-presented-in-that-weird-com</guid>
      <pubDate>Tue, 22 Oct 2024 21:10:51 GMT</pubDate>
    </item>
    <item>
      <title>如何对数据框中的单个列进行单列编码？</title>
      <link>https://stackoverflow.com/questions/79114762/how-do-i-onehotencode-a-single-column-in-a-dataframe</link>
      <description><![CDATA[我有一个名为“vehicles”的数据框，它有 8 列。其中 7 列是数字，但名为“Car_name”的列在数据框中是索引 1，是分类的。我需要对其进行编码
我试过这个代码，但不起作用
ohe = OneHotEncoding(categorical_features = [1])

vehicles_enc = ohe.fit_transform(vehicles).toarray()

TypeError: OneHotEncoder.__init__() 获得了一个意外的关键字参数“categorical_features”

然而，这在我使用的 YouTube 视频中运行良好。]]></description>
      <guid>https://stackoverflow.com/questions/79114762/how-do-i-onehotencode-a-single-column-in-a-dataframe</guid>
      <pubDate>Tue, 22 Oct 2024 15:12:04 GMT</pubDate>
    </item>
    <item>
      <title>决策树的修剪函数</title>
      <link>https://stackoverflow.com/questions/79113940/prune-function-for-decision-tree</link>
      <description><![CDATA[我正在从头开始创建决策树并实施修剪。目前，我认为我的代码中的问题是，当我修剪一棵树时，我创建的新叶节点不会放入原始树中，因此当我计算新的准确度时，它是有效的，并且我的所有分支都会被修剪。附件是我的树类、叶类和修剪函数的代码。
我的问题是，当我尝试计算新的准确度时，我为尝试修剪而创建的新叶节点似乎没有反映在原始树中。
我如何修复/重构我的代码，以便修剪更新当前树，这样当我计算新的准确度时，就可以确定修剪是否成功？
class TreeNode:
def __init__(self, feature, split,depth, left = None, right = None):
&quot;&quot;&quot;
self.feature = 节点分裂的特征
self.split = 节点分裂的特征的值
self.left = 左子节点
self.right = 右子节点
self.depth = 此时树的深度
&quot;&quot;&quot;
self.feature = 特征
self.split = 分裂
self.left = 左
self.right = 右
self.depth = 深度

def getLeft(self):
return self.left

def getRight(self):
return self.right

def getFeature(self):
return self.feature

def getSplit(self):
return self.split

def getDepth(self):
return self.depth

def eval(self, sample):
if sample[self.feature] &lt; self.split:
return self.left.eval(sample)
else:
return self.right.eval(sample)

class LeafNode:
def __init__(self, roomNumber, users,depth):
&quot;&quot;&quot;
self.roomNumer = 分配给叶子的房间号
self.depth = 树中叶子的深度
&quot;&quot;&quot;
self.roomNumber = roomNumber
self.depth =depth
self.users = users

def getRoomNumber(self):
返回 self.roomNumber

def getDepth(self):
返回 self.depth

def getUsers(self):
返回 self.users

def eval(self, sample):
self.users += 1
返回 self.getRoomNumber()

def pruneTree(original_tree, validation, node):
如果节点为 None:
返回 None
如果 isinstance(node, LeafNode):
返回节点
node.left = pruneTree(original_tree, validation, node.left)
node.right = pruneTree(original_tree, validation, node.right)
如果 isinstance(node.left, LeafNode) 和 isinstance(node.right, LeafNode):

current_accuracy = assess(validation, original_tree)

leftRoom, leftPopulation = node.left.getRoomNumber(), node.left.getUsers()

rightRoom, rightPopulation = node.right.getRoomNumber(), node.right.getUsers()

previous_feature, previous_split, previous_depth, previous_left, previous_right = node.getFeature(), node.getSplit(), node.getDepth(), node.getLeft(), node.getRight()

newRoom = -1

newPopulation = leftPopulation + rightPopulation

如果 rightPopulation &gt;= leftPopulation:
newRoom = rightRoom
否则:
newRoom = leftRoom

node = LeafNode(roomNumber = newRoom, users=newPopulation,depth = previous_depth)

new_accuracy = assess(validation, original_tree)

如果 new_accuracy &lt; current_accuracy:
node = TreeNode(split = previous_split, feature=previous_feature,depth=previous_depth)
node.left = previous_left
node.right = previous_right
返回节点

def assess(test_db, trained_tree):
num_correct = 0
对于 test_db 中的数据：
sample = data[:-1]
prediction = trained_tree.eval(sample)

如果 prediction == data[-1]:
num_correct += 1
返回 num_correct/len(test_db)

pruned_tree = pruneTree(tree, validation, tree)
]]></description>
      <guid>https://stackoverflow.com/questions/79113940/prune-function-for-decision-tree</guid>
      <pubDate>Tue, 22 Oct 2024 11:59:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 macOS 中加载 *.mil 文件？[关闭]</title>
      <link>https://stackoverflow.com/questions/79113430/how-to-load-a-mil-file-in-macos</link>
      <description><![CDATA[在 Apple 的文档中，模型中间语言 (MIL) 被描述为一种中间语言。我在 Apple 的系统中发现了许多 .mil 文件。您可以使用以下命令轻松找到它们：
find /System/Library -name &quot;*.mil&quot;

我正在尝试研究这些 .mil 文件并将它们转换为 Core ML 文件以运行它们。但是，我在 coremltools 中找不到允许我这样做的任何功能。您知道有什么方法可以实现这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/79113430/how-to-load-a-mil-file-in-macos</guid>
      <pubDate>Tue, 22 Oct 2024 09:45:03 GMT</pubDate>
    </item>
    <item>
      <title>过滤相关图像的自动化方法[关闭]</title>
      <link>https://stackoverflow.com/questions/79113214/automated-method-to-filter-relevant-images</link>
      <description><![CDATA[我有一组遥感图像，如下图所示。总共有大约 100,000 张图像，但只有少数与我的任务相关。这些图像中有很多都是我不需要的海洋之类的东西，或者只是损坏的照片。我感兴趣的是下图中第 2 和第 4 幅图像（它们包括森林区域、道路、村庄等）。我想知道是否有某种自动方法来选择这些图像。绘制颜色直方图时肯定存在模式，但我很难想出一种方法来从不相关的图像中过滤出相关的图像。这只是数据清理步骤。
我刚刚研究了深度聚类技术，但想知道是否有更简单的解决方案，因为它只是用于数据清理。
任何有关此任务的信息都非常感谢！
]]></description>
      <guid>https://stackoverflow.com/questions/79113214/automated-method-to-filter-relevant-images</guid>
      <pubDate>Tue, 22 Oct 2024 08:57:14 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法将 MS COCO 2017 测试数据集中的图像分成包含小、中、大物体的图像？</title>
      <link>https://stackoverflow.com/questions/79113120/is-there-a-way-to-segregate-images-in-ms-coco-2017-test-dataset-into-images-cont</link>
      <description><![CDATA[我是计算机视觉领域的新手，正在从事一项小任务，即在 MS COCO 测试数据集中分离包含小物体的图像。由于测试集没有注释，有什么方法可以完成此任务吗？如果您能提供任何帮助，我将不胜感激。
我尝试使用图像的宽度和高度参数，但在小图像部分只得到了 360 张图像。如何获取测试集的注释信息？]]></description>
      <guid>https://stackoverflow.com/questions/79113120/is-there-a-way-to-segregate-images-in-ms-coco-2017-test-dataset-into-images-cont</guid>
      <pubDate>Tue, 22 Oct 2024 08:33:45 GMT</pubDate>
    </item>
    <item>
      <title>如何修复 ApplePersistenceIgnoreState 错误？</title>
      <link>https://stackoverflow.com/questions/78826248/how-to-fix-applepersistenceignorestate-error</link>
      <description><![CDATA[我正在学习一个关于如何使用神经网络进行图像分类的 Neuralnine 教程。
我正在使用 Imac。
下面是代码：
import cv2 as cv
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import datasets, layer, models
#准备数据
(training_images, training_labels), (testing_images, testing_labels) = datasets.cifar10.load_data()
training_images, testing_images = training_images / 255, testing_images / 255

class_names = [&#39;Plane&#39;, &#39;Car&#39;, &#39;Bird&#39;, &#39;Cat&#39;, &#39;Deer&#39;, &#39;Dog&#39;, &#39;Frog&#39;, &#39;Horse&#39;, &#39;Ship&#39;, &#39;Truck&#39;]

for i in range(16):
plt.subplot(4,4,i+1)
plt.xticks([])
plt.yticks([])
plt.imshow(training_images[i], cmap=plt.cm.binary)
plt.xlabel(class_names[training_labels[i][0]])

plt.show()

training_images = training_images[:5000] #节省时间
training_labels = training_labels[:5000]
testing_images = testing_images[:4000]
testing_labels = testing_labels[:4000]

model = models.Sequential()
model.add(layers.Conv2D(32, (3,3),activation=&#39;relu&#39;,input_shape=(32,32,3)))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3,3),激活=&#39;relu&#39;))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3,3), 激活=&#39;relu&#39;))
model.add(layers.Flatten())
model.add(layers.Dense(64, 激活=&#39;relu&#39;))
model.add(layers.Dense(10, 激活=&#39;softmax&#39;))

model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])

model.fit(training_images, training_labels, epochs=10, validation_data=(testing_images, testing_labels))

我在终端中收到以下消息：
ApplePersistenceIgnoreState：现有状态将不会已触及。新状态将写入 /var/folders/2g/.../T/org.python.python.savedState
2024-08-02 16:29:19.788 Python[48146:6869495] 警告：未启用可恢复状态的安全编码！通过实现 NSApplicationDelegate.applicationSupportsSecureRestorableState: 并返回 YES 来启用安全编码。

完全不知道该怎么做]]></description>
      <guid>https://stackoverflow.com/questions/78826248/how-to-fix-applepersistenceignorestate-error</guid>
      <pubDate>Fri, 02 Aug 2024 15:36:34 GMT</pubDate>
    </item>
    <item>
      <title>将 onnx 模型转换为 keras</title>
      <link>https://stackoverflow.com/questions/58395644/convert-onnx-model-to-keras</link>
      <description><![CDATA[我尝试将 ONNX 模型转换为 Keras，但当我调用转换函数时，我收到以下错误消息 “TypeError：不可哈希类型：&#39;google.protobuf.pyext._message.RepeatedScalarContainer&#39;”
ONNX 模型输入：input_1
您可以在此处查看 ONNX 模型：https://ibb.co/sKnbxWY
import onnx2keras
from onnx2keras import onnx_to_keras
import keras
import onnx

onnx_model = onnx.load(&#39;onnxModel.onnx&#39;)
k_model = onnx_to_keras(onnx_model, [&#39;input_1&#39;])

keras.models.save_model(k_model,&#39;kerasModel.h5&#39;,overwrite=True,include_optimizer=True)


 文件“C:/../onnx2Keras.py”，第 7 行，位于 &lt;module&gt;
k_model = onnx_to_keras(onnx_model, [&#39;input_1&#39;])
文件“..\site-packages\onnx2keras\converter.py”，第 80 行，位于 onnx_to_keras
weights[onnx_extracted_weights_name] = numpy_helper.to_array(onnx_w)
TypeError：不可哈希类型：&#39;google.protobuf.pyext._message.RepeatedScalarContainer&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/58395644/convert-onnx-model-to-keras</guid>
      <pubDate>Tue, 15 Oct 2019 13:15:06 GMT</pubDate>
    </item>
    </channel>
</rss>