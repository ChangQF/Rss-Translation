<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 10 Oct 2024 18:22:21 GMT</lastBuildDate>
    <item>
      <title>我使用了 ML dev 建议的所有选项，但我想知道为什么测试准确率与训练分数相比太低？</title>
      <link>https://stackoverflow.com/questions/79075597/i-used-all-the-options-suggested-by-ml-dev-but-i-want-to-know-why-the-test-accur</link>
      <description><![CDATA[我有一个分为 Train、Val 和 Test 的文件夹。train 包含来自 4 个子文件夹的大约 4262 张图片，val 包含来自 4 个子文件夹的 915 张图片，test 包含来自 4 个子文件夹的 913 张图片。我在分割图像之前做了增强过程，将每张图片概括为 35 个副本，然后我根据 70/30 分割集手动分割，其中 15% 用于验证，15% 用于测试，我使用 VGG16 模型对四个文档进行分类，训练结果约为 96%，这是模型实现的最后结果
134/134 [================================] - 96s 717ms/step - 
loss: 0.0311 - accuracy: 0.9916 - val_loss: 1.8162 - val_accuracy: 0.5694 

为什么差距太大，问题出在哪里？我怎样才能达到至少 89% 的准确率？
我已经尝试过降低学习率、增加和减少 dropout 层、使用批量标准化、增加 epoch。我需要帮助来分析和指定我正在处理的问题。
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization
from tensorflow.keras.applications import VGG16
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers

# 定义目录路径
base_dir = &#39;/content/dataset/rose_edit&#39;
train_dir = f&quot;{base_dir}/train&quot;
val_dir = f&quot;{base_dir}/val&quot;
test_dir = f&quot;{base_dir}/test&quot;

# 加载数据集
train_dataset = tf.keras.preprocessing.image_dataset_from_directory(
train_dir,
image_size=(224, 224), # VGG16 需要 224x224 图像
batch_size=32,
label_mode=&#39;categorical&#39; # 使用 categorical 进行多类分类
)

val_dataset = tf.keras.preprocessing.image_dataset_from_directory(
val_dir,
image_size=(224, 224),
batch_size=32,
label_mode=&#39;categorical&#39;
)

test_dataset = tf.keras.preprocessing.image_dataset_from_directory(
test_dir,
image_size=(224, 224),
batch_size=32,
label_mode=&#39;categorical&#39;
)

# 初始化 VGG16 基础模型，无需顶层并冻结其层
vgg_base = VGG16(weights=&#39;imagenet&#39;, include_top=False, input_shape=(224, 224, 3))
vgg_base.trainable = False # 冻结 VGG16 层

# 构建模型
model = Sequential()
model.add(vgg_base)
model.add(Flatten()) # 将 VGG16 的输出展平为密集层

# 使用 L2 正则化的自定义密集层
model.add(Dense(256,activation=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.001)))
model.add(Dropout(0.5))
model.add(BatchNormalization())

model.add(Dense(64,activation=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.001)))
model.add(Dropout(0.5))
model.add(BatchNormalization())

# 输出层（4 个类，带 softmax）
model.add(Dense(4,activation=&#39;softmax&#39;))

# 编译模型
model.compile(optimizer=Adam(learning_rate=0.0001),
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

# 模型摘要
model.summary()

# 训练模型
history = model.fit(
train_dataset,
validation_data=val_dataset,
epochs=11, # 根据需要调整 epoch 数
)

# 在测试数据集上评估模型
test_loss, test_accuracy = model.evaluate(test_dataset)
print(f&quot;测试准确率： {test_accuracy * 100:.2f}%&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79075597/i-used-all-the-options-suggested-by-ml-dev-but-i-want-to-know-why-the-test-accur</guid>
      <pubDate>Thu, 10 Oct 2024 17:43:12 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能改善姿势分类？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79074518/how-can-i-improve-pose-classification</link>
      <description><![CDATA[我使用 mediapipe 进行姿势检测，我想对运动姿势进行分类，但大多数姿势都很相似，我使用的是 tensorflow 本身提供的姿势分类。
我面临的问题是，对每一帧都进行分类，这也会导致错误检测，能够正确摆姿势，但也会导致错误检测。
结果是连续的，就像所需的动作（如 crikerter 击中六分）恰好在第 23 帧，并且被正确识别，但我在第 25 和第 26 帧也得到了错误检测的动作，我想避免这种情况
我尝试纠正数据，提供增强数据，增加过滤置信度，获得更多嵌入（如角度、距离等）。]]></description>
      <guid>https://stackoverflow.com/questions/79074518/how-can-i-improve-pose-classification</guid>
      <pubDate>Thu, 10 Oct 2024 12:59:54 GMT</pubDate>
    </item>
    <item>
      <title>使用 Swin Transformer V2 Backbone 在 PyTorch 上定制 Faster R-CNN 模型</title>
      <link>https://stackoverflow.com/questions/79074104/customizing-a-faster-r-cnn-model-on-pytorch-with-swin-transformer-v2-backbone</link>
      <description><![CDATA[对于我的对象检测项目，我一直在使用 fasterrcnn_resnet50_fpn_v2 模型。我的输入图像是高分辨率的（大约 3000 x 4000 像素），我将它们拼接成 1200 x 1600 像素的图块以进行训练和推理。但是，我很难用这个模型有效地检测小物体（小到 10 x 10 像素）。
在寻找替代方案时，我读到了 SwinTransformer V2，我发现它很有前途，尤其是对于高分辨率图像的应用程序。由于我的数据集中的所有图像尺寸也是 1200 x 1600，我不想缩小它们的尺寸，所以我想自定义 Faster R-CNN 以使用 Swin V2 主干，并可能添加 FPN 并实现 Cascade R-CNN 头。但是，我面临的挑战是骨干、颈部和 RPN 头部之间的尺寸不匹配。
这是我目前想到的（我决定使用基础模型）；
import torch
from torch import nn
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.swin_transformer import swin_v2_b, Swin_V2_B_Weights
from torchvision.ops import MultiScaleRoIAlign
import torchvision.transforms as transforms
import request
from PIL import Image

NUM_CLASSES = 100 
trainable_layers = 2

class CustomSwin(nn.Module):
def __init__(self, backbone):
super().__init__()
self.backbone = backbone
self.out_channels = 1024

def forward(self, x):
return torch.permute(self.backbone(x), (0, 3, 1, 2))

backbone = swin_v2_b(weights=Swin_V2_B_Weights.DEFAULT)

# 删除分类头 
backbone.norm = nn.Identity()
backbone.permute = nn.Identity()
backbone.avgpool = nn.Identity()
backbone.flatten = nn.Identity()
backbone.head = nn.Identity()

# 冻结所有参数
for param in backbone.parameters():
param.requires_grad = False

# 取消冻结最后的 trainable_layers
for layer in list(backbone.features)[-trainable_layers:]:
for param in layer.parameters():
param.requires_grad = True

custom_backbone = CustomSwin(backbone)

# 为非常小的物体添加较小的尺寸
anchor_generator = AnchorGenerator(
sizes=((8, 16, 32, 64, 128, 256, 512),), aspects_ratios=((0.5, 1.0, 2.0),)
)

roi_pooler = MultiScaleRoIAlign(featmap_names=[&quot;0&quot;], output_size=7, samples_ratio=2)

model = FasterRCNN(
custom_backbone,
num_classes=NUM_CLASSES,
rpn_anchor_generator=anchor_generator,
box_roi_pool=roi_pooler,
min_size=1224, 
max_size=1632,
)

我不确定当前的实现是否是最佳的，或者添加 FPN（特征金字塔网络）或 Cascade R-CNN 等组件是否会增强模型的性能（我有一个相当大的数据集）。有人成功实施了这些修改吗？任何指导都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79074104/customizing-a-faster-r-cnn-model-on-pytorch-with-swin-transformer-v2-backbone</guid>
      <pubDate>Thu, 10 Oct 2024 11:07:43 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 PyTorch 调度程序似乎不能正常工作？</title>
      <link>https://stackoverflow.com/questions/79073506/why-my-pytorch-scheduler-doesnt-seem-to-work-properly</link>
      <description><![CDATA[我正在尝试使用一个简单的 PyTorch Scheduler 来训练 mobileNetV3Large。
这是负责训练的代码部分：
bench_val_loss = 1000
bench_acc = 0.0
epochs = 15
optimizer = optim.Adam(embeddingNet.parameters(), lr=1e-3) 
loss_optimizer = torch.optim.Adam(loss_fn.parameters(), lr=1e-3)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;, factor=0.1, waiting=3, Threshold=0.02)

for epoch in range(1, epochs + 1):

print(f&#39;current lr: {scheduler.get_last_lr()}&#39;)
loss=train(embeddingNet, loss_fn, device, train_dataloader, optimizer, loss_optimizer, epoch)
val_loss，准确率 =test(train_dataset，val_dataset，embeddingNet，accuracy_calculator，loss_fn，epoch，val_dataloader)
#val_loss = simpleTest(train_dataset，val_dataset，embeddingNet，accuracy_calculator，loss_fn，epoch，val_dataloader)

torch.save(embeddingNet.state_dict()，&#39;my/path/mobileNetV3L_ArcFaceLAST.pth&#39;)

如果准确率 &gt;= bench_acc:
bench_val_loss = val_loss
torch.save(embeddingNet.state_dict()，&#39;my/path/mobileNetV3L_ArcFaceBEST.pth&#39;)

scheduler.step(accuracy)

writer.add_scalars(&#39;训练与验证损失&#39;，
{&#39;训练&#39;：损失， &#39;Validation&#39;: val_loss},
global_step=epoch+1)

在这里您可以找到前 7 个训练日志
测试集准确率 (Precision@1) = 0.17834772304046048
当前 lr：[0.001]
Epoch 3：Loss = 39.68284225463867
Epoch 3：valLoss = 39.9765007019043
100%|██████████| 962/962 [01:43&lt;00:00, 9.28it/s]
100%|██████████| 370/370 [00:41&lt;00:00, 8.92it/s]
计算准确率
测试集准确率 (Precision@1) = 0.31242593533096324
当前 lr: [0.001]
Epoch 4: Loss = 39.4412841796875
Epoch 4: valLoss = 39.67761562450512
100%|██████████| 962/962 [01:45&lt;00:00, 9.11it/s]
100%|██████████| 370/370 [00:41&lt;00:00, 8.86it/s]
计算准确率
测试集准确率 (Precision@1) = 0.3633824276282377
当前 lr: [0.001]
Epoch 5: Loss = 39.09823989868164
Epoch 5: valLoss = 39.54649614901156
100%|██████████| 962/962 [01:42&lt;00:00, 9.37it/s]
100%|██████████| 370/370 [00:41&lt;00:00, 8.87it/s]
计算准确率
测试集准确率 (Precision@1) = 0.44244117149145085
当前 lr: [0.001]
Epoch 6: Loss = 38.70449447631836
Epoch 6: valLoss = 39.1865906792718
100%|██████████| 962/962 [01:45&lt;00:00, 9.15it/s]
100%|██████████| 370/370 [00:39&lt;00:00, 9.25it/s]
计算准确率
测试集准确率 (Precision@1) = 0.5167597765363129
当前 lr: [0.0001]

我不明白为什么调度程序决定降低学习率，即使准确率的增长速度比阈值更快。
错误在哪里？]]></description>
      <guid>https://stackoverflow.com/questions/79073506/why-my-pytorch-scheduler-doesnt-seem-to-work-properly</guid>
      <pubDate>Thu, 10 Oct 2024 08:51:32 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow Keras 面部识别计算机视觉图像分类模型，准确率高 (95%)/验证率低 (0%)</title>
      <link>https://stackoverflow.com/questions/79073365/tensorflow-keras-facial-recognition-computer-vision-image-classification-model-w</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79073365/tensorflow-keras-facial-recognition-computer-vision-image-classification-model-w</guid>
      <pubDate>Thu, 10 Oct 2024 08:13:20 GMT</pubDate>
    </item>
    <item>
      <title>微调 Transformer 模型并未提高性能</title>
      <link>https://stackoverflow.com/questions/79072711/fine-tuning-transformer-model-not-improving-performance</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79072711/fine-tuning-transformer-model-not-improving-performance</guid>
      <pubDate>Thu, 10 Oct 2024 04:16:15 GMT</pubDate>
    </item>
    <item>
      <title>如何使用深度学习来解决由合成数据组成的拼图游戏？[关闭]</title>
      <link>https://stackoverflow.com/questions/79072271/how-can-i-use-deep-learning-to-solve-a-jigsaw-puzzle-composed-of-synthetic-data</link>
      <description><![CDATA[我花了一些时间研究 Python 中的拼图生成器，该生成器接收图像、行数 (M) 和列数 (N)，并将原始图像分解为 M*N 个 png 图像块输出到文件夹中。这些图像是正方形，带有不规则形状的制表符和空格，因此每个块只能放在一个位置。
接下来我想做的是创建一个拼图解算器，它可以接收这些 png 图像，提取一些关键特征并确定它们的位置。
这里是图像的示例。如果您好奇它是如何实现的，您还可以查看生成器代码。
到目前为止，这些碎片没有任何旋转，但我希望将来能够处理这个问题。由于碎片是方形的，因此无法仅使用尺寸来确定方向，因此在提取边缘进行比较时，我无法轻松缩小它们的范围。
我曾考虑使用 SIFT 进行特征检测，但图像的可见层没有重叠，因此这种方法失败了。
我遇到的主要问题是我不知道从哪里开始。我遇到过制作拼图解算器的不同方法，但其中大多数都是使用拼图碎片的照片，而不是合成数据，因此形状不同。我见过的另一种方法是使用深度学习来分析图像片段，但我也不确定从哪里开始实施这种方法。]]></description>
      <guid>https://stackoverflow.com/questions/79072271/how-can-i-use-deep-learning-to-solve-a-jigsaw-puzzle-composed-of-synthetic-data</guid>
      <pubDate>Wed, 09 Oct 2024 22:50:24 GMT</pubDate>
    </item>
    <item>
      <title>当批处理大小不等于 1 时，UNet 执行过程中会出现错误</title>
      <link>https://stackoverflow.com/questions/79071235/an-error-occurs-during-the-execution-of-unet-when-the-batch-size-is-not-equal-to</link>
      <description><![CDATA[我尝试使用 DDIM 反演教程中提供的代码运行稳定扩散模型。但是，当输入的批处理大小设置为大于 1 的值（例如 32）时，我遇到以下错误：
RuntimeError：张量 a (131072) 的大小必须与非单例维度 1 上的张量 
b (4096) 的大小匹配。

看来 131072 可能来自 32 x 4096，表明张量维度不匹配。发生错误的具体行是：
noisy_residual = self.unet(input, t.to(input.device), **denoise_kwargs).sample

这是我的代码中与反演过程相关的部分：
## 反演 (https://github.com/huggingface/diffusion-models-class/blob/main/unit4/01_ddim_inversion.ipynb)
def invert_process(self, guide_scale, input, denoise_kwargs):

pred_images = []
pred_latents = []

decrypt_kwargs = {&#39;vae&#39;: self.vae}

# 反转时间步&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
timesteps = reversed(self.scheduler.timesteps)
num_inference_steps = len(self.scheduler.timesteps)

with torch.no_grad():
for i in tqdm(range(0, num_inference_steps)):

t = timesteps[i]
self.cur_t = t.item()

# 对于稳定扩散的文本条件
if &#39;encoder_hidden_​​states&#39; in denoise_kwargs.keys():
bs = denoise_kwargs[&#39;encoder_hidden_​​states&#39;].shape[0]
input = torch.cat([input] * bs)

# 预测噪声残差
noisy_residual = self.unet(input, t.to(input.device), **denoise_kwargs).sample
noise_pred = noisy_residual

#对于稳定扩散的文本条件
if noisy_residual.shape[0] == 2:
# 执行指导
noise_pred_text, noise_pred_uncond = noisy_residual.chunk(2)
noisy_residual = noise_pred_uncond + guide_scale * (noise_pred_text - noise_pred_uncond)
input, _ = input.chunk(2)

current_t = max(0, self.cur_t - (1000//num_inference_steps)) #t
next_t = t # min(999, t.item() + (1000//num_inference_steps)) # t+1
alpha_t = self.scheduler.alphas_cumprod[current_t].to(self.device)
alpha_t_next = self.scheduler.alphas_cumprod[next_t].to(self.device)

latents = input

# 反转更新步骤（重新安排更新步骤以获得 x(t)（新潜伏）作为 x(t-1)（当前潜伏）的函数
# 向潜伏添加噪声

latents = (latents - (1-alpha_t).sqrt()*noise_pred)*(alpha_t_next.sqrt()/alpha_t.sqrt()) + (1-alpha_t_next).sqrt()*noise_pred

input = latents

pred_latents.append(latents)
pred_images.append(decode_latent(latents, **decode_kwargs))

return pred_images, pred_latents


可能导致当批量大小大于 1 时，张量大小不匹配？如何在模型中保持批量大小大于 1 的同时解决此问题？
我尝试将 t 的大小更改为形状为 (批量大小,) 的张量。
此外，我确认当批量大小为 1 时模型可以正常工作。]]></description>
      <guid>https://stackoverflow.com/questions/79071235/an-error-occurs-during-the-execution-of-unet-when-the-batch-size-is-not-equal-to</guid>
      <pubDate>Wed, 09 Oct 2024 16:32:07 GMT</pubDate>
    </item>
    <item>
      <title>在小数据集上生成合成数据[关闭]</title>
      <link>https://stackoverflow.com/questions/79071218/generating-synthetic-data-on-small-dataset</link>
      <description><![CDATA[我有一个只有 5 个数据点的小数据集，包括材料成分、机械性能和物理性能。由于数据量太小，无法进行预测，我尝试生成合成数据。我使用过 GAN、VAE、高斯混合模型和 Copula 模型。其中，C-Vine Copula 模型比其他模型的结果更好。但我仍然面临问题：
*使用 C-Vine Copula，分布中存在 40% 的误差，合成数据和真实数据之间的关系中存在 7% 的误​​差。这使得数据质量不足以进行预测。
*当使用这些合成数据预测物理特性时，我得到了很好的验证分数，但在新的、看不见的数据点上得到了非常差的结果——可能是由于过度拟合或数据质量差造成的。
*我还尝试使用原始真实数据（5 个数据点）预测物理特性，但结果并不准确。
我不知道如何提高合成数据的质量，或者是否有更好的方法可以尝试进行预测。对此有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/79071218/generating-synthetic-data-on-small-dataset</guid>
      <pubDate>Wed, 09 Oct 2024 16:26:17 GMT</pubDate>
    </item>
    <item>
      <title>将 ML 模型从一个 Azure Databricks 工作区复制到另一个 Databricks 工作区</title>
      <link>https://stackoverflow.com/questions/79065062/copy-a-ml-model-from-one-azure-databricks-workspace-to-another-databricks-worksp</link>
      <description><![CDATA[我运行了以下代码以在基于 Azure Databricks 的 mlflow 中导出 ML 模型，但我似乎收到了此错误

MLflow 主机或令牌配置不正确

我无法找出问题所在。工作区的 URL 和 PAT 令牌都是正确的。
export_import 工具有很多错误。它需要 mlfow 库，但 Databricks ML Runtime 附带的是 mlflow-skinny。
import mlflow
import os
from mlflow_export_import.model.export_model import ModelExporter
from mlflow.tracking import MlflowClient

# 使用工作区 URL 设置 Databricks MLflow 跟踪 URI
mlflow.set_tracking_uri(&quot;https://adb-xxxyyymmmnnnyyy.1.azuredatabricks.net/&quot;)

# 设置两个令牌以实现兼容性
os.environ[&quot;DATABRICKS_TOKEN&quot;] = &quot;mnop6672ec8e20c7d219eb2A-3&quot;
os.environ[&quot;MLFLOW_TRACKING_TOKEN&quot;] = &quot;mnop6672ec8e20c7d219eb2A-3&quot;

# 初始化 MLflow 客户端（无需传递跟踪 URI，因为它是全局设置的）
mlflow_client = MlflowClient()

# 使用 MLflow 客户端初始化 ModelExporter
exporter = ModelExporter(mlflow_client)

# 导出模型
exporter.export_model(
model_name=&quot;Signature_Test&quot;,
output_dir=&quot;/tmp/mlflow_export/model&quot;,
stage=None, # 使用&quot;None&quot; 导出所有阶段，或指定&quot;Staging&quot; 或&quot;Production&quot;
export_metadata_tags=True
)
]]></description>
      <guid>https://stackoverflow.com/questions/79065062/copy-a-ml-model-from-one-azure-databricks-workspace-to-another-databricks-worksp</guid>
      <pubDate>Tue, 08 Oct 2024 08:39:33 GMT</pubDate>
    </item>
    <item>
      <title>LogisticRegression 未返回正确结果</title>
      <link>https://stackoverflow.com/questions/79057824/logisticregression-not-returning-correct-results</link>
      <description><![CDATA[我试图使用基于训练数据的逻辑回归对测试数据中的一系列点进行分类预测。
我得到了一个没有错误的输出，但我被告知结果是错误的（错误的输出将被视为错误，但运行代码时没有错误）。
训练数据是 375 个点中 4 个不同类别的集合，每个点有 3 个变量，因此绘制在 3D 图形上。我在该集合上运行了一个计数程序，发现超过 50% 的点属于第 2 类。我的初步结果是所有测试点都属于第 2 类。我尝试将训练数据排序为几个不同的集合：随机选择 125 个条目（这是测试数据的大小），找到所有类别的最小数量，然后使用每个类别中相同数量的点创建一个训练集。
无排序 = 所有第 2 类
随机排序 = 所有第 2 类
等类数排序 = 给我一个答案，其中点被归类在所有 4 个类别中，但当我将它们插入在线最终测试表格时，我的准确率得分为 26%，这与随机机会相同。所以，我没有正确处理数据，我不确定在哪里。我希望有更多回归分类经验的人能为我指明正确的方向。
在调用 LogisticRegression 之前，我是否需要重新格式化（转换）train_X、train_y 和 test_X 数组？如果需要，该怎么做？也许我只是给它提供了格式错误的数据？
# 形成表格以推动逻辑回归
train_X = []
train_y = []
for i in range(len(train_table)):
train_X.append(
[train_table.x.iloc[i], train_table.y.iloc[i], train_table.z.iloc[i]]
)
train_y.append(train_table.label.iloc[i])

test_X = []
for k in range(len(test_table)):
test_X.append([test_table.x.iloc[k], test_table.y.iloc[k], test_table.z.iloc[k]])

# 尝试使用和不使用规范化
clf = LogisticRegression().fit(normalize(train_X), train_y)
# clf = LogisticRegression().fit(train_X, train_y)

predict = clf.predict(test_X[:])
prob = clf.predict_proba(test_X[:])

results = pd.DataFrame(
sort_results(test_table, predict, prob),
columns=[&quot;&quot;, &quot;timestamp&quot;, &quot;UTC time&quot;, &quot;label&quot;, &quot;accuracy&quot;],
)
]]></description>
      <guid>https://stackoverflow.com/questions/79057824/logisticregression-not-returning-correct-results</guid>
      <pubDate>Sat, 05 Oct 2024 19:41:26 GMT</pubDate>
    </item>
    <item>
      <title>错误：所有估算器都应实现拟合和变换，或者在使用 make_column_transformer 时可以使用“drop”或“passthrough”说明符</title>
      <link>https://stackoverflow.com/questions/71566189/error-all-estimators-should-implement-fit-and-transform-or-can-be-drop-or</link>
      <description><![CDATA[我正在尝试实现一个使用 ColumnTransformer() 和 SVC() 的模型。
我的转换方法如下所示：
num_features = X_train_svm.select_dtypes(include=np.number).columns.to_list()
cat_features = X_train_svm.select_dtypes(include=[&#39;object&#39;]).columns.to_list()

transform1 = make_column_transformer([(StandardScaler(), num_features),
(OneHotEncoder(), cat_features)
])

后面跟着一个管道：
pipe = make_pipeline(transform1, svm.SVC())

然后当我尝试拟合训练和测试数据时:
pipe.fit(X_train, y_train)

我收到错误 :: TypeError：所有估算器都应实现 fit 和 transform，或者可以是“drop”或“passthrough”说明符。&#39;(StandardScaler(), [&#39;Height&#39;, &#39;Age&#39;, &#39;Weight&#39;, &#39;Quantity&#39;, &#39;Cost&#39;])&#39; (type &lt;class &#39;tuple&#39;&gt;) 不适用。
请帮我修复此错误。
我尝试将缩放器更改为 Ordinal Scaler，尝试使用 (-1,1) 重塑数据，但没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/71566189/error-all-estimators-should-implement-fit-and-transform-or-can-be-drop-or</guid>
      <pubDate>Tue, 22 Mar 2022 02:13:21 GMT</pubDate>
    </item>
    <item>
      <title>添加我自己的密集层后，vgg16模型的可训练参数发生了变化</title>
      <link>https://stackoverflow.com/questions/65651051/trainable-parameters-of-vgg16-model-get-changed-after-adding-my-own-dense-layer</link>
      <description><![CDATA[vgg16_model = tf.keras.applications.vgg16.VGG16()

model= Sequential()

for layer in vgg16_model.layers[:-1]:

model.add(layer)

model.summary() #最后一个密集层到现在为止已被移除 


for layer in model.layers:

layer.trainable=False #对于迁移学习，我已冻结了这些层

model.add(Dense(2,activation=&#39;softmax&#39;))

model.summary() #现在当我添加密集层时，模型的可训练参数会发生变化

]]></description>
      <guid>https://stackoverflow.com/questions/65651051/trainable-parameters-of-vgg16-model-get-changed-after-adding-my-own-dense-layer</guid>
      <pubDate>Sun, 10 Jan 2021 07:39:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 迁移学习的边界框回归准确率为 0%。具有 Sigmoid 激活的输出层仅输出 0 或 1</title>
      <link>https://stackoverflow.com/questions/65459399/bounding-box-regression-using-keras-transfer-learning-gives-0-accuracy-the-out</link>
      <description><![CDATA[我正在尝试创建一个对象定位模型来检测汽车图像中的车牌。我使用了 VGG16 模型并排除了顶层以添加我自己的密集层，最后一层有 4 个节点和 S 形激活以获得 (xmin、ymin、xmax、ymax)。
我使用 keras 提供的函数读取图像，并将其调整为 (224, 244, 3)，还使用 ​​preprocess_input() 函数来处理输入。我还尝试通过使用填充调整大小来手动处理图像以保持比例，并通过除以 255 对输入进行规范化。
当我训练时，似乎什么都不起作用。我的训练和测试准确率为 0%。下面是我为该模型编写的代码。
def get_custom(output_size, optimizer, loss):

vgg = VGG16(weights=&quot;imagenet&quot;, include_top=False, input_tensor=Input(shape=IMG_DIMS))

vgg.trainable = False

flatten = vgg.output
flatten = Flatten()(flatten)

bboxHead = Dense(128,activation=&quot;relu&quot;)(flatten)
bboxHead = Dense(32,activation=&quot;relu&quot;)(bboxHead)

bboxHead = Dense(output_size,activation=&quot;sigmoid&quot;)(bboxHead)

model = Model(inputs=vgg.input,outputs=bboxHead)
model.compile(loss=loss,optimizer=optimizer,metrics=[&#39;accuracy&#39;])

return模型

X 和 y 分别为 (616, 224, 224, 3) 和 (616, 4)。我将坐标除以相应边的长度，因此 y 中的每个值都在 (0,1) 范围内。
我将在下面链接我的 github 中的 python 笔记本，以便您可以看到完整的代码。我正在使用 google colab 来训练模型。
https://github.com/gauthamramesh3110/image_processing_scripts/blob/main/License_Plate_Detection.ipynb]]></description>
      <guid>https://stackoverflow.com/questions/65459399/bounding-box-regression-using-keras-transfer-learning-gives-0-accuracy-the-out</guid>
      <pubDate>Sat, 26 Dec 2020 18:24:41 GMT</pubDate>
    </item>
    <item>
      <title>Plotly：如何使用热图制作带注释的混淆矩阵？</title>
      <link>https://stackoverflow.com/questions/60860121/plotly-how-to-make-an-annotated-confusion-matrix-using-a-heatmap</link>
      <description><![CDATA[我喜欢使用 Plotly 来可视化一切，我试图通过 Plotly 可视化混淆矩阵，这是我的代码：
def plot_confusion_matrix(y_true, y_pred, class_names):
fusion_matrix = metrics.confusion_matrix(y_true, y_pred)
confusion_matrix =fusion_matrix.astype(int)

layout = {
&quot;title&quot;: &quot;混淆矩阵&quot;,
&quot;xaxis&quot;: {&quot;title&quot;: &quot;预测值&quot;},
&quot;yaxis&quot;: {&quot;title&quot;: &quot;实际值&quot;}
}

fig = go.Figure(data=go.Heatmap(z=confusion_matrix,
x=class_names,
y=class_names,
hoverongaps=False),
layout=layout)
fig.show()

结果是

我怎样才能在相应的单元格内显示数字而不是悬停，像这样]]></description>
      <guid>https://stackoverflow.com/questions/60860121/plotly-how-to-make-an-annotated-confusion-matrix-using-a-heatmap</guid>
      <pubDate>Thu, 26 Mar 2020 02:18:36 GMT</pubDate>
    </item>
    </channel>
</rss>