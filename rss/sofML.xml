<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 29 Jan 2024 12:24:01 GMT</lastBuildDate>
    <item>
      <title>通过 Docker 镜像使用时将 Faiss 索引写入文件的问题</title>
      <link>https://stackoverflow.com/questions/77899677/issue-on-writing-faiss-index-to-a-file-when-using-through-docker-image</link>
      <description><![CDATA[我有一个 FastAPI Docker 映像，在启动部分中，我从 Redis 获取 FAISS 索引的二进制版本，使用 pickle.loads 解封它，然后使用
file_path = os.path.join(folder_path, &#39;index.faiss&#39;)
faiss.write_index(faissModelFromRedis,file_path)
将其写入文件。这在本地有效，但是当使用 Docker 部署到 Azure Web App 时，它会抛出异常
文件“/app/main.py”，第 38 行，位于 loadModelAndSetRetriever 2024-01-27T14:03:27.547507225Z faiss.write_index(faiss_model,file_path) 2024-01-27T14:03:27.547510425Z 文件“ /usr/local/lib/python3.8/site-packages/faiss/swigfaiss_avx2.py”，第 10200 行，在 write_index 2024-01-27T14:03:27.547513825Z 返回 _swigfaiss_avx2.write_index(*args) 2024-01-27T14 ：03：27.547517125Z TypeError：重载函数“write_index”的参数数量或类型错误。 2024-01-27T14:03:27.547520225Z 可能的 C/C++ 原型是： 2024-01-27T14:03:27.547523325Z faiss::write_index(faiss::Index const *,char const *) 2024-01-27T14:03 :27.547526526Z faiss::write_index(faiss::Index const *,FILE *) 2024-01-27T14:03:27.547529526Z faiss::write_index(faiss::Index const *,faiss::IOWriter *) 
我们如何解决这个问题？
我尝试使用 pickle.dumps，但使用 Faiss，它无法读取已保存文件的索引。
还尝试在 Dockerfile 中添加 chmod 777 或新用户添加命令，认为这是 faiss 的写入访问问题。]]></description>
      <guid>https://stackoverflow.com/questions/77899677/issue-on-writing-faiss-index-to-a-file-when-using-through-docker-image</guid>
      <pubDate>Mon, 29 Jan 2024 12:19:42 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的语音识别</title>
      <link>https://stackoverflow.com/questions/77899035/speech-recognition-in-machine-learning</link>
      <description><![CDATA[我举办了一场关于机器学习中语音识别的研讨会，在这次研讨会中我们提供了演示、算法和演示文稿
任何人都可以帮助我进行 ML 语音识别
用于 ML 模型中的语音识别
、演示和演示]]></description>
      <guid>https://stackoverflow.com/questions/77899035/speech-recognition-in-machine-learning</guid>
      <pubDate>Mon, 29 Jan 2024 10:30:40 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：层“conv3d”的输入 0 与该层不兼容 - Keras 3DConv 模型</title>
      <link>https://stackoverflow.com/questions/77898943/valueerror-input-0-of-layer-conv3d-is-incompatible-with-the-layer-keras-3dc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77898943/valueerror-input-0-of-layer-conv3d-is-incompatible-with-the-layer-keras-3dc</guid>
      <pubDate>Mon, 29 Jan 2024 10:16:50 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI Gym 步骤功能不起作用错误需要 5 个变量才能解压</title>
      <link>https://stackoverflow.com/questions/77898872/openai-gym-step-function-doesnt-work-error-needs-5-variables-to-unpack</link>
      <description><![CDATA[导入健身房
从 nes_py.wrappers 导入 JoypadSpace
从 Contra.actions 导入 SIMPLE_MOVMENT、COMPLEX_MOVMENT、RIGHT_ONLY
SIMPLE_MOVMENT #简化环境

环境=gym.make（&#39;Contra-v0&#39;）
envi = JoypadSpace(envi, SIMPLE_MOVMENT)

重新启动=真

对于范围（1000）内的帧：
    如果重新启动：
        状态 = envi.reset()
    action = envi.action_space.sample() # 生成随机动作
    state,reward,restart,truncated,inf=envi.step(action)#采取随机生成的动作
    envi.render() # 只显示游戏
环境.close()

运行代码后，出现以下错误：
&lt;前&gt;&lt;代码&gt;---&gt; 50 观察、奖励、终止、截断、info = self.env.step(action)
     51 self._elapsed_steps += 1
     53 如果 self._elapsed_steps &gt;= self._max_episode_steps：

ValueError：没有足够的值来解压（预期为 5，实际为 4）

)
我一直在努力解决这个问题，但我不知道如何解决它。我尝试使用较旧的健身房版本，但仍然不起作用
其他人就堆栈溢出提出了同样的问题，但有一个不清楚的答案。]]></description>
      <guid>https://stackoverflow.com/questions/77898872/openai-gym-step-function-doesnt-work-error-needs-5-variables-to-unpack</guid>
      <pubDate>Mon, 29 Jan 2024 10:06:00 GMT</pubDate>
    </item>
    <item>
      <title>在colab中运行excel文件</title>
      <link>https://stackoverflow.com/questions/77898474/running-excel-file-in-colab</link>
      <description><![CDATA[尝试在 Google colab 中加载 Excel 工作表时收到错误消息。 Excel 包含 5000 多行。
我尝试过：data=pd.read_excel(&#39;/content/data_ml.xlsx&#39;)
获得错误：:---------------------------------------- -----------------------------------
BadZipFile Traceback（最近一次调用最后一次）
&lt;ipython-input-3-0935045ece15&gt;在&lt;细胞系：1&gt;()
----&gt; 1 data=pd.read_excel(&#39;/content/data_ml.xlsx&#39;)

6帧
_RealGetContents(self) 中的 /usr/lib/python3.10/zipfile.py
   第1334章
   第1335章
-&gt;第1336章
   第1337章1：
   第1338章

BadZipFile：文件不是 zip 文件
]]></description>
      <guid>https://stackoverflow.com/questions/77898474/running-excel-file-in-colab</guid>
      <pubDate>Mon, 29 Jan 2024 09:04:56 GMT</pubDate>
    </item>
    <item>
      <title>模型“利用”加权损失？</title>
      <link>https://stackoverflow.com/questions/77897842/model-exploiting-weighted-loss</link>
      <description><![CDATA[我正在创建一个进行多类分类的模型，并尝试使用加权损失来优化它。
我的数据集包含 7 个类。这是我拥有的每个数据点的数量：
梅尔：1113
内华达州：6705
密件抄送：514
AKIEC：327
吉隆坡：1099
DF：115
VASC：142
我认为我的模型正在“利用”通过仅返回“AKIEC”来减少该加权损失每次。 （不知道为什么它不是每次都返回 VASC，但 AKIEC 是我观察到的）
这是我的模型在“MEL”上给出的输出图片：
[1.9501211e-02、8.6555272e-02、7.0136093e-02、5.3331017e-02、7.6896048e-01、
1.2218184e-03、2.9415233e-04]
这是我的模型使用的加权损失权重：
{0：0.888866699950075、1：0.3305042436345482、2：0.9486769845232151、3：0.9673489765351972、4：0.8902646030953569、5：0.9885 172241637543, 6: 0.9858212680978532}
这导致分类交叉熵损失约为 0.33，二进制准确度约为 94%。
如果有必要，这是我的模型代码：
def classi(input_shape):
    输入=图层.输入（形状=输入形状）
    vgg19 = k.applications.VGG19（include_top=False，权重=“imagenet”，input_tensor=输入）
    x = vgg19（输入，训练=False）
    
    x = 层.Conv2D(64, 3, 填充=“相同”)(x)
    x = 层.Activation(“relu”)(x)
    x = 层.BatchNormalization()(x)
    #经典层
    对于 [96, 128, 256]:#, 320]:#, 512]:#, 1024, 2048] 中的过滤器：
        x = 层.Conv2D(过滤器, 3, 填充=“相同”)(x)
        x = 层.Activation(“relu”)(x)
        x = 层.BatchNormalization()(x)

        x = 层.Conv2D(过滤器, 3, 填充=“相同”)(x)
        x = 层.Activation(“relu”)(x)
        x = 层.BatchNormalization()(x)

        x = groups.MaxPool2D(3, strides=2, padding=“相同”)(x)

    ＃输出
    x = 层数.Dropout(rate=0.3)(x)
    x = 层.Flatten()(x)
    x = 层.Dense(128, 激活=“sigmoid”)(x)

    输出 = 层.Dense(7, 激活 =“softmax”)(x)

    model = k.Model(输入=输入，输出=输出，名称=“分类”)
    返回模型

我的模型接受过以下训练：
batch_size=8
steps_per_epoch = 10015 //batch_size
历元 = 60

这里的问题是因为我的加权损失，还是我的模型只是不“好”？足够的？ （训练不够，层数太少......）或者我的指标和损失是否实施不正确？
感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/77897842/model-exploiting-weighted-loss</guid>
      <pubDate>Mon, 29 Jan 2024 06:46:18 GMT</pubDate>
    </item>
    <item>
      <title>keras多类分类欠拟合</title>
      <link>https://stackoverflow.com/questions/77897827/keras-multiclass-classification-underfitting</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77897827/keras-multiclass-classification-underfitting</guid>
      <pubDate>Mon, 29 Jan 2024 06:41:23 GMT</pubDate>
    </item>
    <item>
      <title>Causal ML 提出的 Uplift 建模中的 SHAP 解释器错误</title>
      <link>https://stackoverflow.com/questions/77897614/shap-explainer-error-on-uplift-modeling-presented-by-causal-ml</link>
      <description><![CDATA[我有 2 个问题：
1。问题：
我正在尝试在我自己的电脑上实现本教程=&gt; https://causalml.readthedocs.io/en/latest/examples/causal_trees_interpretation。 html
在 TreeExplainer 部分，这里是创建 tree_explainer 对象的代码：
tree_explainer = shap.TreeExplainer(ctree)
# 治疗=0 和治疗=1 的预期值。即 Y|X,T=0 和 Y|X,T=1
tree_explainer.expected_value

我试图在我的电脑上运行此代码，但出现此错误：
&lt;块引用&gt;
&lt;单元格行：1&gt;() 中的类型错误回溯（最近一次调用最后一次）
----&gt; 1 tree_explainer = shap.Explainer(ctree) 2 #treatment=0 和treatment=1 的期望值。即 Y|X,T=0 和 Y|X,T=1 3
tree_explainer.expected_value
/data/envs/berkere/lib/python3.8/site-packages/shap/explainers/_explainer.py
在 init(自身、模型、掩码、链接、算法、output_names、
feature_names, Linearize_link, Seed, **kwargs) 169 # 如果我们到达这里
那么我们不知道如何处理其他人给我们的东西 170:
--&gt;第171章 172
173 # 构建正确的子类
TypeError：传递的模型不可调用且无法分析
直接使用给定的掩码！模型：CausalTreeRegressor()

我该如何处理？
2。问题：
在另一个实现中，我能够获得这样的 SHAP 值：
uplift_model = BaseTClassifier(XGBClassifier(n_estimators=100,
                                             最大深度=5，
                                             Learning_rate=0.1), control_name=&#39;Kontrol&#39;)

uplift_model.fit(df_train[x_names].values,
                 治疗=df_train[&#39;treatment_group_key&#39;].values,
                 y=df_train[&#39;转换&#39;].值)

model_tau = uplift_model.predict(df_test[x_names].values)

uplift_model_shap_values = uplift_model.get_shap_values(X=df_test[x_names].values, tau=model_tau, features=x_names);

然后我想通过本地观察来深入研究。我预测控制分数为 0.000219，治疗分数为 0.041069。在这种情况下，我可以说我必须对这些数据应用治疗（因为治疗得分优于控制得分，并且我可以看到Recommended_treatment 列中的数字为 1）。然后我绘制了 shap.waterfall_plot 我发现这个实例的最重要的特征总是降低 SHAP 值，无论 base_value 是什么。所以我想要一个解释，我应该如何阅读 Uplift 模型的 SHAP 图，因为我们知道 uplift 模型不像传统的 ML 模型。我非常想知道 Uplift Model 如何决定说“您应该对此数据实施处理（或 2,3，等等）”]]></description>
      <guid>https://stackoverflow.com/questions/77897614/shap-explainer-error-on-uplift-modeling-presented-by-causal-ml</guid>
      <pubDate>Mon, 29 Jan 2024 05:36:39 GMT</pubDate>
    </item>
    <item>
      <title>神经网络可以用来追踪铅笔/钢笔草图吗？</title>
      <link>https://stackoverflow.com/questions/77897604/can-neural-networks-be-used-to-trace-pencil-pen-sketches</link>
      <description><![CDATA[我正在为即将到来的学校项目挑选主题，并认为训练神经网络将铅笔/钢笔绘图的图像跟踪为干净的数字绘图会很酷。我通过让我的输入文件夹是所有铅笔/钢笔绘图，输出文件夹是所有跟踪的数字绘图，为输入和输出创建了一个图像数据集。问题是，当我测试经过训练的 U-Net 卷积神经网络时，我的结果是空白。换句话说，我训练了一个模型，然后在新的铅笔/钢笔绘图图像上使用 model.predict，结果只是一张空白图像，而不是跟踪图像。我知道这是一个相当具体的问题，但如果有人有任何训练机器学习模型以以类似方式基于训练数据生成图像的经验，任何建议将不胜感激！
正如我之前提到的，我尝试使用 U-Net 卷积神经网络来解决这个问题，但没有得到任何结果。我的层由一个输入层、两个 Conv2D 层、一个最大池化层、另外两个 Conv2D 层、一个上采样层和另一个 Conv2D 层组成。由于我对图像机器学习相对较新，因此我不确定是否需要添加额外的层才能使模型保留应根据输入和输出生成图像的理解。]]></description>
      <guid>https://stackoverflow.com/questions/77897604/can-neural-networks-be-used-to-trace-pencil-pen-sketches</guid>
      <pubDate>Mon, 29 Jan 2024 05:33:42 GMT</pubDate>
    </item>
    <item>
      <title>将 YOLOv8 集成到 Transformer 模型中</title>
      <link>https://stackoverflow.com/questions/77897573/integrating-yolov8-to-an-transformer-model</link>
      <description><![CDATA[我是机器学习领域的新手，需要添加 YOLOv8 模型方面的帮助(YOLOv8）到下面的代码，而不是使用 InceptionV3 为我的项目提取图像特征。我需要传递检测到的对象并从 YOLOv8 模型中提取特征，以使用转换器生成标题。
def CNN_Encoder_Incep():
    inception_v3 = tf.keras.applications.InceptionV3(
        include_top=假，
        权重=&#39;imagenet&#39;
    ）
    inception_v3.trainable = False

    输出= inception_v3.output
    输出 = tf.keras.layers.Reshape(
        (-1, 输出.形状[-1]))(输出)

    cnn_model = tf.keras.models.Model(inception_v3.输入，输出)
    返回cnn_model

类 ImageCaptioningModel(tf.keras.Model):

    def __init__(self, cnn_model, 编码器, 解码器, image_aug=None):
        超级().__init__()
        self.cnn_model = cnn_model
        self.encoder = 编码器
        self.decoder = 解码器
        self.image_aug = image_aug
        self.loss_tracker = tf.keras.metrics.Mean(name=&quot;loss&quot;)
        self.acc_tracker = tf.keras.metrics.Mean(name=&quot;准确度&quot;)


    defcalculate_loss(self, y_true, y_pred, mask):
        损失 = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        损失*=掩模
        返回 tf.reduce_sum(loss) / tf.reduce_sum(mask)


    defcalculate_accuracy(self, y_true, y_pred, mask):
        精度 = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        准确度 = tf.math.logic_and(掩码, 准确度)
        准确度 = tf.cast(准确度, dtype=tf.float32)
        掩码 = tf.cast(掩码, dtype=tf.float32)
        返回 tf.reduce_sum(accuracy) / tf.reduce_sum(mask)


    defcompute_loss_and_acc(self,img_embed,captions,training=True):
        编码器输出 = self.encoder(img_embed, 训练=True)
        y_input = 标题[:, :-1]
        y_true = 标题[:, 1:]
        掩码=（y_true！= 0）
        y_pred = self.解码器（
            y_输入，编码器_输出，训练=真，掩码=掩码
        ）
        损失 = self.calculate_loss(y_true, y_pred, mask)
        acc = self.calculate_accuracy(y_true, y_pred, mask)
        回波损耗，ACC


    def train_step(自身, 批次):
        imgs、字幕 = 批处理

        如果 self.image_aug：
            imgs = self.image_aug(imgs)

        img_embed = self.cnn_model(imgs)

        使用 tf.GradientTape() 作为磁带：
            损失，acc = self.compute_loss_and_acc（
                img_embed、字幕
            ）

        训练变量 = (
            self.encoder.trainable_variables + self.decoder.trainable_variables
        ）
        grads = Tape.gradient(损失, train_vars)
        self.optimizer.apply_gradients(zip(grads, train_vars))
        self.loss_tracker.update_state(损失)
        self.acc_tracker.update_state(acc)

        return {“loss”：self.loss_tracker.result()，“acc”：self.acc_tracker.result()}


    def test_step（自身，批次）：
        imgs、字幕 = 批处理

        img_embed = self.cnn_model(imgs)

        损失，acc = self.compute_loss_and_acc（
            img_embed、字幕、训练=False
        ）

        self.loss_tracker.update_state(损失)
        self.acc_tracker.update_state(acc)

        return {“loss”：self.loss_tracker.result()，“acc”：self.acc_tracker.result()}

    @财产
    定义指标（自身）：
        返回 [self.loss_tracker, self.acc_tracker]

cnn_model = CNN_Encoder_Incep()
标题模型 = ImageCaptioningModel(
    cnn_model=cnn_model，编码器=编码器，解码器=解码器，image_aug=图像增强，
）

我尝试这样做，但当我尝试将其传递给 cnn_model 变量时，我不断收到多个错误。
def CNN_Encoder(): yolov8_model = tf.keras.models.load_model(&#39;./content/yolov8n_objdet_oidv7_640x640.pt&#39;) yolov8_model.trainable = False 输出 = yolov8_model.output 输出 = tf.keras.layers.Reshape ((-1，output.shape[-1]))(输出) cnn_model = tf.keras.models.Model(yolov8_model.input，输出) cnn_model_onnx = cnn_model.export(format=&#39;onnx&#39;) return cnn_model]]></description>
      <guid>https://stackoverflow.com/questions/77897573/integrating-yolov8-to-an-transformer-model</guid>
      <pubDate>Mon, 29 Jan 2024 05:22:01 GMT</pubDate>
    </item>
    <item>
      <title>将图像移动到目录后数字不同</title>
      <link>https://stackoverflow.com/questions/77895090/different-numbers-after-moving-images-into-a-directory</link>
      <description><![CDATA[我试图将一些图像移动到包含 3 个类（石头、布和剪刀）的目录中，以分割基本目录。
我尝试过这段代码：
train_dir = os.path.join(base_dir, &#39;train&#39;)
val_dir = os.path.join(base_dir, &#39;验证&#39;)
test_dir = os.path.join(base_dir, &#39;测试&#39;)

# 创建子目录
对于 [train_dir, val_dir, test_dir] 中的目录：
    os.makedirs（目录，exist_ok=True）

# 类别列表（石头、剪刀、布）
类名 = os.listdir(base_dir)

# 迭代每个类
对于 class_names 中的 class_name：
    class_path = os.path.join(base_dir, class_name)

    如果 os.path.isdir(class_path):
        # 列出类中的所有图像
        images = [img for img in os.listdir(class_path) if img.endswith(&#39;.jpg&#39;) or img.endswith(&#39;.png&#39;) or img.endswith(&#39;.jpeg&#39;) and img != &#39;README_rpc-简历-images.txt&#39;]

        # 将图像分为训练集、验证集和测试集
        训练图像，测试图像，训练标签，测试标签=训练测试分割（
            所有图像、所有标签、test_size=0.2、random_state=42）

        train_images，val_images，train_labels，val_labels = train_test_split（
            训练图像、训练标签、测试大小=0.25、随机状态=42）
        
        对于 [os.path.join(train_dir, class_name), os.path.join(val_dir, class_name), os.path.join(test_dir, class_name)] 中的目录：
            os.makedirs（目录，exist_ok=True）

        # 将图片移动到对应的子目录
        对于 train_images 中的 img：
          Shutil.move(os.path.join(class_path, img), os.path.join(train_dir, class_name, img))
        对于 val_images 中的 img：
          Shutil.move(os.path.join(class_path, img), os.path.join(val_dir, class_name, img))
        对于 test_images 中的 img：
          Shutil.move(os.path.join(class_path, img), os.path.join(test_dir, class_name, img))

但是在我使用此代码仔细检查了 train_images 和 train_dir 中的图像数量后：
print(f“train_images 中的图像数量: {len(train_images)}”)
print(f&quot;train_dir 中的图像数量: {len(train_dir)}&quot;)

结果是：
train_images 中的图像数量：1312 和
train_dir 中的图像数量：28
我一直在寻找导致这种数字差异的代码出了什么问题。
将train_images中的图像移动到train_dir的目的是train_dir将在生成器中使用：
train_generator = train_datagen.flow_from_directory()

这需要一个目录才能执行此操作。
关于如何解决这个问题有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77895090/different-numbers-after-moving-images-into-a-directory</guid>
      <pubDate>Sun, 28 Jan 2024 14:15:59 GMT</pubDate>
    </item>
    <item>
      <title>如何阻塞 OCDNet 管道并仅在 OCRNet 上获取结果？ （NVIDIA 光学字符检测和识别解决方案/OCDR）</title>
      <link>https://stackoverflow.com/questions/77892097/how-to-block-the-ocdnet-pipeline-and-get-the-result-only-on-ocrnet-nvidia-opti</link>
      <description><![CDATA[我正在 Jetson 上本地运行 NVIDIA 光学字符检测和识别解决方案。我想阻止 OCDNet 的管道，只使用 OCRNet 进行推断。我注释掉了所有与 OCDNet 相关的代码。结果是空洞的推论。]]></description>
      <guid>https://stackoverflow.com/questions/77892097/how-to-block-the-ocdnet-pipeline-and-get-the-result-only-on-ocrnet-nvidia-opti</guid>
      <pubDate>Sat, 27 Jan 2024 17:22:01 GMT</pubDate>
    </item>
    <item>
      <title>将从 keras YOLOV8Detector 获取的模型更新到 Apple MLPackage/CoreML</title>
      <link>https://stackoverflow.com/questions/77886439/update-model-obtained-from-keras-yolov8detector-to-apple-mlpackage-coreml</link>
      <description><![CDATA[我按照 KerasCV 上的教程使用 YOLOV8 和 KerasCV 进行高效目标检测，并在 不同的数据集
一段时间后，我能够获得预测并将其可视化，如教程中所述：
&lt; /p&gt;
我想在苹果 iOS 应用程序中使用这个模型，所以我使用了 coremltools 包来转换它。然而，“输出”似乎是这样的。 kerascv 制作的产品并不完全是苹果界所期望的。
模型训练完成后，我可以要求进行预测：
 图像，y_true = next(iter(dataset.take(1)))
 y_pred = model.predict(images) // y_pred 是一个字典

y_pred 是包含这些键的字典 [&#39;boxes&#39;, &#39;confidence&#39;, &#39;classes&#39;, &#39;num_detections&#39;]
使用Netron，我可以看看苹果世界所期待的模型的形状

如何修改/重塑从 kerascv 生成的模型，这样我就可以拥有一个将置信度和坐标答案作为两个单独输出输出的模型，而不是输出字典？
我在此链接上找到了一些元素 MobileNetv2 和 SSD 但我我不确定在这种情况下如何应用这些元素]]></description>
      <guid>https://stackoverflow.com/questions/77886439/update-model-obtained-from-keras-yolov8detector-to-apple-mlpackage-coreml</guid>
      <pubDate>Fri, 26 Jan 2024 13:00:22 GMT</pubDate>
    </item>
    <item>
      <title>数据帧的 .corr() 方法不返回理想值仅返回 -1 或 1</title>
      <link>https://stackoverflow.com/questions/77214404/corr-method-for-dataframe-not-returning-ideal-values-only-returns-either-1-o</link>
      <description><![CDATA[理想情况下，它应该为每个单元格返回 -1 到 1 之间的值，但具有相同列名和行名的单元格需要具有 1 值
在执行 corr() 之前尝试用 0 替换 NaN，它返回正确的值，但这些值对于程序的目的来说是不准确的
&lt;前&gt;&lt;代码&gt;# df
            电影A 电影B 电影C 电影D 电影E
安吉 0.000000 南 -0.500000 0.500000 南
安尼维什 1.166667 -0.333333 -0.833333 NaN NaN
杰伊 1.166667 -0.333333 南 -0.833333 南
卡蒂克 0.000000 -1.500000 南 南 1.5
纳曼 南 0.250000 南 -0.250000 南

# df.T.corr()
          安吉·阿尼维什·杰伊·卡西克·纳曼
安吉 1.0 1.0 -1.0 南 南
安尼维什 1.0 1.0 1.0 1.0 NaN
杰伊 -1.0 1.0 1.0 1.0 1.0
卡蒂克 NaN 1.0 1.0 1.0 NaN
纳曼 南 南 1.0 南 1.0
]]></description>
      <guid>https://stackoverflow.com/questions/77214404/corr-method-for-dataframe-not-returning-ideal-values-only-returns-either-1-o</guid>
      <pubDate>Mon, 02 Oct 2023 09:15:04 GMT</pubDate>
    </item>
    <item>
      <title>手动实施多元线性回归的 AIC 分数</title>
      <link>https://stackoverflow.com/questions/58965317/implementing-aic-score-for-multiple-linear-regression-manually</link>
      <description><![CDATA[我已经手动实现了一个多元线性回归类，现在我正在研究度量方法。我尝试手动计算AIC和BIC分数，但结果不正确。原因是我没有使用 Log Likelihood 函数，而是使用了 SSE 方法。您能否建议我如何更改实现来计算完整的 AIC 和 BIC 分数？
这是我的方法现在的样子：
 def AIC_BIC(self, 实际 = None, pred = None):
    如果实际为 None：
      实际 = 自我响应
    如果 pred 为 None：
      pred = self.response_pred

    n = len（实际）
    k = self.num_features

    残差 = np.subtract(pred, 实际)
    RSS = np.sum(np.power(残差, 2))

    AIC = n * np.log(RSS / n) + 2 * k
    BIC = n * np.log(RSS / n) + k * np.log(n)

    退货（AIC、BIC）

请尝试为我提供手动方法，而不是图书馆调用。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/58965317/implementing-aic-score-for-multiple-linear-regression-manually</guid>
      <pubDate>Thu, 21 Nov 2019 00:11:59 GMT</pubDate>
    </item>
    </channel>
</rss>