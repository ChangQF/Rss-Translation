<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 12 Feb 2024 00:57:44 GMT</lastBuildDate>
    <item>
      <title>在 qiskit.algorithms.optimizers.ADAM 优化过程中获取中间步骤的方法</title>
      <link>https://stackoverflow.com/questions/77978874/ways-to-get-intermediate-steps-during-optimization-process-in-qiskit-algorithms</link>
      <description><![CDATA[使用 Qiskit 内部 ADAM 优化器时，有什么方法可以获取中间步骤（类似于 Tensorflow 或 Pytorch Adams 优化器中的回调函数）？
我浏览了 ADAM 的文档 Qiskit 中的优化器，但我找不到合适的方法来获取它们。
我可以使用 Pytorch Adam 优化器获取中间训练信息，但我想知道是否可以使用 Qiskit 来做到这一点。
我正在使用qiskit v0.45.2。
由于我是社区新手，如果我的问题不恰当，给您带来的不便，我深表歉意。
我非常感谢您提供的任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/77978874/ways-to-get-intermediate-steps-during-optimization-process-in-qiskit-algorithms</guid>
      <pubDate>Mon, 12 Feb 2024 00:13:43 GMT</pubDate>
    </item>
    <item>
      <title>整洁的Python基因组的计算并没有真正相加</title>
      <link>https://stackoverflow.com/questions/77978427/the-calculation-of-the-neat-python-genome-doesnt-really-add-up</link>
      <description><![CDATA[我为 XOR 制作了一个非常简单的整洁的 python。这是基因组的摘要。
最佳基因组：
钥匙：141
健身：2.993003425787766
节点：
    0 DefaultNodeGene（键= 0，偏差= -0.8080802310263379，响应= 1.0，激活= sigmoid，聚合=总和）
连接：
    DefaultConnectionGene(键=(-2, 0)，权重=1.5655941592328844，启用=True)
    DefaultConnectionGene(key=(-1, 0)，权重=1.2986799185483175，启用=True

但是当我尝试计算它时，它并没有真正意义。
以下是评估结果：
 输入 (0.0, 0.0)，预期输出 (0.0,)，得到 [0.017286340618090416]
  输入（0.0，1.0），预期输出（1.0，），得到[0.9778511014280682]
  输入（1.0，0.0），预期输出（1.0，），得到[0.920780444438713]
  输入（1.0，1.0），预期输出（0.0，），得到[0.9999657218870016]

这就是我计算输入 (1, 1) 的方法：
activation_func( sum( 输入 * 权重 ) + 偏差 )

对于这种情况：
sigmoid( 1 * 1.2986799185483175 + 1 * 1.5655941592328844 + (-0.8080802310263379) )
= 0.88657197794273698476
但是根据评估应该是0.9999657218870016
我错过了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/77978427/the-calculation-of-the-neat-python-genome-doesnt-really-add-up</guid>
      <pubDate>Sun, 11 Feb 2024 21:04:52 GMT</pubDate>
    </item>
    <item>
      <title>当图片是几个函数的绘图时的图像识别</title>
      <link>https://stackoverflow.com/questions/77978372/image-recognition-when-the-picture-is-a-plot-of-a-few-functions</link>
      <description><![CDATA[
你好：
您能给我一些建议吗？
目标是对多个对象进行监督分类。每个对象都由两个函数的图来描述。每个对象的绘图尺寸 (b - a) 和 T 大致相同，看起来它们可以标准化为 (0, 1) 和 (0, 1)。
这是一个图像分类问题吗？我这么问是因为大多数像素都是白色的。
无论如何，您会推荐什么方法来解决这个问题？
TIA]]></description>
      <guid>https://stackoverflow.com/questions/77978372/image-recognition-when-the-picture-is-a-plot-of-a-few-functions</guid>
      <pubDate>Sun, 11 Feb 2024 20:46:45 GMT</pubDate>
    </item>
    <item>
      <title>BERT 如何理解 [CLS] 令牌？</title>
      <link>https://stackoverflow.com/questions/77978283/how-does-bert-understand-the-cls-token</link>
      <description><![CDATA[我已经阅读了论坛上的很多答案和媒体上的文章，但我仍然不完全理解模型如何准确地从我们有分类任务的 [CLS] 标记中识别出来。 [CLS] 标记的处理是在代码的开头部分，还是来自编码器中处理标记的注意层？]]></description>
      <guid>https://stackoverflow.com/questions/77978283/how-does-bert-understand-the-cls-token</guid>
      <pubDate>Sun, 11 Feb 2024 20:17:52 GMT</pubDate>
    </item>
    <item>
      <title>MLTextClassifier 遇到问题</title>
      <link>https://stackoverflow.com/questions/77978258/running-into-a-problem-with-mltextclassifier</link>
      <description><![CDATA[我正在尝试学习如何通过传递训练数据来创建机器学习模型。我正在使用 MLTextCLassifier 创建文本分类器，但我在第 4 行代码中不断收到错误，告诉我 init 在 macOS13 中已被弃用，初始化时我应该使用 DataSource 而不是 MLDataTable，当它显示不匹配时会出现另一个错误调用实例方法“evaluation”。 初始代码块
我尝试使用 DataSource 而不是 MLTextClassifier，就像 Xcode 建议的那样。它消除了其他错误，然后出现了一个新错误，指出无法构造 MLTextClassifier，因为它没有可访问的初始值设定项。 在此处输入图片描述
我有什么遗漏吗？]]></description>
      <guid>https://stackoverflow.com/questions/77978258/running-into-a-problem-with-mltextclassifier</guid>
      <pubDate>Sun, 11 Feb 2024 20:11:21 GMT</pubDate>
    </item>
    <item>
      <title>排名之间的注释者间协议</title>
      <link>https://stackoverflow.com/questions/77977932/inter-annotator-agreement-between-ranking</link>
      <description><![CDATA[作为实验的一部分，我使用注释器对一组列表进行排名。每个列表包含不等、数量相对较少的项目。注释器根据每个项目的重要性对列表进行了重新排序。也就是说，重要的项目应该排名更高。这项任务是由多个注释者执行的，我想衡量他们之间的一致性。
我应该使用什么指标或如何制定数据，以便我可以使用常见的一致性指标，例如 Cohen 的 Kappa / Krippendorff Alpha？]]></description>
      <guid>https://stackoverflow.com/questions/77977932/inter-annotator-agreement-between-ranking</guid>
      <pubDate>Sun, 11 Feb 2024 18:28:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在tensorflow中的CNN最后一层配置SVM？</title>
      <link>https://stackoverflow.com/questions/77977404/how-to-configure-svm-in-the-last-layer-of-cnn-in-tensorflow</link>
      <description><![CDATA[我发现了这篇关于“深度学习”的论文使用支持向量机”。观看 此 youtube 教程后，我尝试在我的模型中实现它，即正在 FER2013 数据集上进行面部表情识别训练。
型号：
Kaggle 笔记本
模型 = keras.Sequential([
    
    图层.Reshape((48, 48, 1), input_shape=(2304,)),
    
    层.BatchNormalization(),
    层.Conv2D（过滤器= 64，kernel_size = 3，激活=&#39;relu&#39;），
    层.AveragePooling2D(pool_size=(2, 2)),
    层数.Dropout(0.5),
    
    层.BatchNormalization(),
    层.Conv2D（过滤器= 128，kernel_size = 3，激活=&#39;relu&#39;），
    层.AveragePooling2D(pool_size=(2, 2)),
    层数.Dropout(0.5),
    
    层.BatchNormalization(),
    层.Conv2D（过滤器= 128，kernel_size = 3，激活=&#39;relu&#39;），
    层.AveragePooling2D(pool_size=(2, 2)),
    层数.Dropout(0.5),
    
    层.BatchNormalization(),
    层.Conv2D（过滤器= 512，kernel_size = 3，激活=&#39;relu&#39;），
    层.AveragePooling2D(pool_size=(2, 2)),
    层数.Dropout(0.5),
    
    层.Flatten(),
    
    层.BatchNormalization(),
    层.Dense(128, 激活=&#39;relu&#39;),
    层数.Dropout(0.3),
    
    层.BatchNormalization(),
    层.Dense(256, 激活=&#39;relu&#39;),
    层数.Dropout(0.3),
    
    层.Dense(7, kernel_regularizer=tf.keras.regularizers.l2(0.01),激活
             =&#39;softmax&#39;）
]）

模型.编译(
    优化器=&#39;亚当&#39;,
    损失=&#39;squared_hinge&#39;,
    指标=[&#39;准确性&#39;],
）

但这给出了意想不到的结果：
&lt;前&gt;&lt;代码&gt;纪元 47/50
202/202 [==============================] - 4s 19ms/步 - 损失：0.3469 - 准确度：0.1308 - val_loss ：0.3508 - val_accuracy：0.1299
48/50 纪元
202/202 [================================] - 4s 19ms/步 - 损失：0.3469 - 准确度：0.1326 - val_loss ：0.3508 - val_accuracy：0.1378
49/50 纪元
202/202 [================================] - 4s 19ms/步 - 损失：0.3469 - 准确度：0.1617 - val_loss ：0.3508 - val_accuracy：0.2426
纪元 50/50
202/202 [================================] - 4s 19ms/步 - 损失：0.3469 - 准确度：0.1594 - val_loss ：0.3508 - val_accuracy：0.1291


我做错了什么？如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77977404/how-to-configure-svm-in-the-last-layer-of-cnn-in-tensorflow</guid>
      <pubDate>Sun, 11 Feb 2024 15:59:14 GMT</pubDate>
    </item>
    <item>
      <title>Python机器学习pytorch测试/训练epoch结果问题</title>
      <link>https://stackoverflow.com/questions/77977231/python-machine-learning-pytorch-test-train-epoch-results-problem</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77977231/python-machine-learning-pytorch-test-train-epoch-results-problem</guid>
      <pubDate>Sun, 11 Feb 2024 15:07:50 GMT</pubDate>
    </item>
    <item>
      <title>字符串“loss”被传递给 metric_name() 而不是指标名称</title>
      <link>https://stackoverflow.com/questions/77977110/string-loss-being-passed-to-metric-name-instead-of-metric-name</link>
      <description><![CDATA[我是 ML 新手，现在只是在学习教程，但收到错误：
ValueError：无法解释指标标识符：丢失
在：
\keras\src\metrics\__init__.py:205，在 get(identifier) 中
看起来在wrapper.py第532行metric_name(key)应该接收损失函数的名称，但它实际上接收字符串“loss”
以下是错误的相关代码：
def buildNetwork():
    分类器=顺序（）
    classificator.add（密集（单位= 20，激活=&#39;relu&#39;，kernel_initializer =&#39;random_uniform&#39;，input_shape =（30，）））
    classificator.add（密集（单位= 20，激活=&#39;relu&#39;，kernel_initializer =&#39;random_uniform&#39;））
    classificator.add（密集（单位= 1，激活=&#39;sigmoid&#39;））
    优化器= keras.optimizers.Adam(learning_rate=0.001,weight_decay=0.000001)
    classificator.compile（优化器=优化器，损失=&#39;binary_crossentropy&#39;，指标=[&#39;binary_accuracy&#39;]）
    返回分类器


分类器 = KerasClassifier(model=buildNetwork,batch_size=10,epochs=100, loss=&#39;binary_crossentropy&#39;)

分数 = cross_val_score(估计器=分类器, X=数据, y=真相, cv=10, error_score=&#39;raise&#39;)

如果在keras_metric_get中我手动设置identifier =“binary_crossentropy”它工作正常
我不知道是否是兼容性问题，但我有 Keras 3.0.8、TF 2.15.0 和 SciKeras 0.12.0]]></description>
      <guid>https://stackoverflow.com/questions/77977110/string-loss-being-passed-to-metric-name-instead-of-metric-name</guid>
      <pubDate>Sun, 11 Feb 2024 14:32:54 GMT</pubDate>
    </item>
    <item>
      <title>使用 scikit-learn 估计器作为变压器</title>
      <link>https://stackoverflow.com/questions/77976770/use-scikit-learn-estimator-as-transformer</link>
      <description><![CDATA[我想创建自己的 scikit-learn 转换器，用于对包含分类的数字特征进行编码，例如邮政编码或行业代码（NAICS、MCC 等）。在这些类型的代码中有一个结构：例如MCC 3000-3999 是“旅行和娱乐”，它进一步细分为更细粒度的类别，例如“航空公司”、“汽车租赁”等。我们不能将它们用作序数特征，但如果我们将它们视为纯分类特征（例如，通过 One -Hot-Encoding）我们需要选择在代码结构的哪个级别应用特征编码。
为了解决这个问题，我创建了自己的 scikit-learn 变压器，它是 TargetEncoder 使用决策树。代码如下所示。重要的是要认识到，在模型训练期间，应使用样本外决策树回归分数来避免过度拟合。因此，我实现了自己的 fit_transform 函数来生成这些样本外分数：
从 sklearn.tree 导入 DecisionTreeRegressor

从 sklearn.base 导入 TransformerMixin、BaseEstimator
从 sklearn.model_selection 导入 cross_val_predict

类 TaxonomyEncoder（TransformerMixin，BaseEstimator）：

def __init__(自身, n_leafs=10, cv=3):
    self.n_leafs = n_leafs
    自我简历 = 简历

def fit(self, X, y=None):
    self.tree_ = DecisionTreeRegressor(max_leaf_nodes=self.n_leafs).fit(X,y)
    返回自我

def 变换（自身，X）：
    返回 self.tree_.predict(X).reshape(-1,1)

def fit_transform(self, X, y=None):
    self.tree_ = DecisionTreeRegressor(max_leaf_nodes=self.n_leafs)
    返回 cross_val_predict(self.tree_, X, y, cv=self.cv).reshape(-1,1)

转换器工作正常，除非在 ColumnTransformer 中使用：
从 sklearn.compose 导入 ColumnTransformer

变压器 = ColumnTransformer([(&#39;分类法&#39;, TaxonomyEncoder(), [&#39;mcc&#39;])])
变压器.fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
Transformer.transform(df[[&#39;mcc&#39;]])

然后我得到决策树尚未拟合的错误：
NotFittedError：此 DecisionTreeRegressor 实例尚未安装。在使用此估计器之前，请使用适当的参数调用“fit”。

显然，scikit-learn 在导致此错误的表面下进行了一些检查。请注意，实际上没有理由需要拟合决策树，因为决策树是在 cross_val_predict 函数中重新拟合的。我该如何解决这个问题？
下面显示了重现该错误的完整工作示例：
导入 pandas 作为 pd
df = pd.DataFrame({&#39;mcc&#39;:[3000,3500,7339], &#39;y&#39;:[0,0,1]})

te = TaxonomyEncoder().fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
te.transform(df[[&#39;mcc&#39;]])

给出：
数组([[0.],
       [0.],
       [1.]])

并且 fit_transform 也给出了预期的结果：
te.fit_transform(df[[&#39;mcc&#39;]], df[&#39;y&#39;])

数组([[0.],
       [0.],
       [0.]])

但是当包装在 ColumnTransformer 中时，事情就会出错：
transformer = ColumnTransformer([(&#39;taxonomy&#39;, TaxonomyEncoder(), [&#39;mcc&#39;])])
变压器.fit(df[[&#39;mcc&#39;]], df[&#39;y&#39;])
Transformer.transform(df[[&#39;mcc&#39;]])
]]></description>
      <guid>https://stackoverflow.com/questions/77976770/use-scikit-learn-estimator-as-transformer</guid>
      <pubDate>Sun, 11 Feb 2024 12:34:35 GMT</pubDate>
    </item>
    <item>
      <title>如何在苹果 M1 Pro 芯片组上的 XGBoost 中启用 GPU</title>
      <link>https://stackoverflow.com/questions/77975756/how-to-enable-gpu-in-xgboost-on-apple-m1-pro-chipset</link>
      <description><![CDATA[我尝试在带有设备 = cuda 的 Windows 上使用 GPU 进行 XGBoost 训练，它有效并且训练时间大大减少，现在我想在我的 Mac M1 Pro 上进行此实验。
如何在 m1 pro 芯片组上启用 GPU 的 XGBoost。
我尝试查找无法找到信息的文档。]]></description>
      <guid>https://stackoverflow.com/questions/77975756/how-to-enable-gpu-in-xgboost-on-apple-m1-pro-chipset</guid>
      <pubDate>Sun, 11 Feb 2024 05:51:44 GMT</pubDate>
    </item>
    <item>
      <title>如何使用多个图像训练序列模型</title>
      <link>https://stackoverflow.com/questions/77973831/how-to-train-sequential-model-with-multiple-images</link>
      <description><![CDATA[我在传递多个图像进行训练时遇到错误。但是当只传递一张图像时就很好了。图像大小相同。
这是代码：
导入tensorflow为tf
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np
从张量流导入keras
从tensorflow.keras导入图层、数据集、模型

# 加载模板图像
template_image = tf.keras.preprocessing.image.load_img(&#39;模板.jpg&#39;)
template_array = tf.keras.preprocessing.image.img_to_array(template_image)

# 加载实际图像
实际图像 = tf.keras.preprocessing.image.load_img(&#39;实际.jpg&#39;)
实际数组 = tf.keras.preprocessing.image.img_to_array(实际图像)

# 创建模型
模型 = tf.keras.Sequential([
  层.InputLayer(input_shape=(template_array.shape)),
  层.Conv2D(16, (3, 3), 激活=&#39;relu&#39;),
  层.MaxPooling2D((2, 2)),
  层.Conv2D(32, (3, 3), 激活=&#39;relu&#39;),
  层.MaxPooling2D((2, 2)),
  层.Flatten(),
  层.Dense(64, 激活=&#39;relu&#39;),
  层.Dense(2, 激活=&#39;softmax&#39;),
]）

# 编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
模型.summary()
对于 model.layers 中的图层：
    打印（层.output_shape）

template_array = template_array.reshape((1, 549, 549, 3))
实际数组 = 实际数组.reshape((1, 549, 549, 3))
train_x = [模板数组，实际数组]
y_train = np.array([1,0])
y_train = y_train.reshape(1,2)
train_y = [y_train, y_train]

print(&quot;X 形状是：&quot;)
打印（模板_数组.形状）
print(&quot;Y 形状是：&quot;)
打印（y_train）

# 训练模型
model.fit(x=train_x, y=train_y, epochs=10)

＃ 作出预测
预测 = model.predict([actual_array])

# 检查是否有错误或缺失的部分
对于范围内的 i(len(预测))：
  如果预测[i][0]&gt;预测[i][1]：
    print(&#39;第 {} 部分丢失或不正确&#39;.format(i))


收到错误：
ValueError：层“sequential_28”预计有 1 个输入，但它收到了 2 个输入张量。收到的输入：
[&lt;tf.Tensor &#39;IteratorGetNext:0&#39; shape=(None, 549, 549, 3) dtype=float32&gt;,
]

如果我传递 x=template_array 和 y = y_train 它运行良好。但这意味着我只使用一张图像进行训练。
我是否无法使图像数组和相应的分类数组同时通过？我如何立即传递所有列车数据？]]></description>
      <guid>https://stackoverflow.com/questions/77973831/how-to-train-sequential-model-with-multiple-images</guid>
      <pubDate>Sat, 10 Feb 2024 16:28:24 GMT</pubDate>
    </item>
    <item>
      <title>R 机器学习中 Ranger 模型的错误[重复]</title>
      <link>https://stackoverflow.com/questions/77970324/an-error-in-ranger-model-in-machine-learning-in-r</link>
      <description><![CDATA[我正在运行生存模型的机器学习代码。我的 pred_prob 代码有错误。谁能帮我？先感谢您
我的错误是：
&lt;代码&gt;&gt; pred_prob &lt;- rowMeans(ranger_predict$train_data[, 1:dim(ranger_predict$train_data)[2]])
h(simpleError(msg, call)) 中的错误：
  在为函数“rowMeans”选择方法时评估参数“x”时出错：$ 运算符对于原子向量无效

我的代码是：
库(ranger) 库(生存) 库(caret) 库(dplyr) 库(pec) df2 &lt;- df2 %&gt;% mutate(status = time_15year-1) # 0 = 审查，1 = 死亡 df2 &lt; ;- na.omit(df2) df2$time_15year &lt;- Floor(df2$time_15year) 对(df2 %&gt;% dplyr::select(time_15year,BS_death), main = &quot;NCCTG 脑卒中数据&quot;) 折叠 &lt; - 2 # 用于交叉验证 cvIndex &lt;- createFolds(factor(df2$BS_death),folds, returnTrain = T)container_model &lt;-vector(“list”,length(cvIndex))container_pred &lt;-container_model for (i in 1:length(cvIndex)) { train_data &lt;- df2[cvIndex[[i]], } testing_data &lt;- df2[-cvIndex[[i]],] train_data &lt;- train_data[complete.cases(train_data) , ] 测试数据 &lt;- 测试数据[完整.案例(测试数据), ] rangermodel &lt;- ranger(Surv(time_15year, BS_death) ~ 年龄 + 性别 + 教育 + 地点 + cvahis+ mihis + bphis +heartdis + smok + Pastsmok+ 被动+活动+waterpip +cvatype，数据= train_data）图（rangermodel$unique.death.times，rangermodel$survival[1，]）ranger_predict &lt;-预测（rangermodel，数据=testing_data）pred_prob &lt;-rowMeans（ranger_predict$train_data[， 1:dim(ranger_predict$train_data)[2]]) pred_prob[pred_prob&gt;median(pred_prob)]=1 pred_prob[pred_prob&lt;=median(pred_prob)]=0 fusionMatrix(as.factor(testing_data$BS_death), as.factor (pred_prob))]]></description>
      <guid>https://stackoverflow.com/questions/77970324/an-error-in-ranger-model-in-machine-learning-in-r</guid>
      <pubDate>Fri, 09 Feb 2024 19:06:49 GMT</pubDate>
    </item>
    <item>
      <title>Word2Vec 和上下文嵌入之间的区别</title>
      <link>https://stackoverflow.com/questions/76471584/difference-between-word2vec-and-contextual-embedding</link>
      <description><![CDATA[我试图理解词嵌入和上下文嵌入之间的区别。
以下是我的理解，如有错误请补充。
词嵌入算法有一个单词的全局词汇表（字典）。当我们执行 word2vec 时，输入语料库（唯一单词）与全局字典映射，它将返回嵌入。
上下文嵌入用于通过考虑文档中所有单词的序列来学习序列级语义。
但我不明白我们在词嵌入中考虑上下文的​​位置。]]></description>
      <guid>https://stackoverflow.com/questions/76471584/difference-between-word2vec-and-contextual-embedding</guid>
      <pubDate>Wed, 14 Jun 2023 08:33:37 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在训练和测试数据中建立具有不同输入向量大小的模型吗？</title>
      <link>https://stackoverflow.com/questions/57290448/can-we-build-a-model-with-different-input-vector-size-in-training-and-testing-da</link>
      <description><![CDATA[我使用 keras 构建逻辑回归模型。我的输入训练向量的形状是 10。[var1,var2,var3,var4,var5,var6,var7,var8,var9,and var10]
由于是二元分类，所以目标标签y为0或1。
有一次，我尝试了模型，我想用大小为 6 的输入向量进行预测？因此，训练和测试或预测数据的向量大小之间存在差异。
这可能吗？有机器学习算法支持这样的功能吗？
代码如下：

&lt;前&gt;&lt;代码&gt;
classifier.add（密集（单位= 50，kernel_initializer =&#39;uniform&#39;，激活=&#39;relu&#39;，input_dim = 5））
classifier.add(Dense(units = 1, kernel_initializer = &#39;uniform&#39;,activation = &#39;sigmoid&#39;))


# 编译人工神经网络
classifier.compile（优化器=优化器，损失=&#39;binary_crossentropy&#39;，指标=[&#39;准确性&#39;]）

# 拟合数据
hisroy =classifier.fit(X_train, y_train, batch_size = 5, epochs = 100)
]]></description>
      <guid>https://stackoverflow.com/questions/57290448/can-we-build-a-model-with-different-input-vector-size-in-training-and-testing-da</guid>
      <pubDate>Wed, 31 Jul 2019 12:10:31 GMT</pubDate>
    </item>
    </channel>
</rss>