<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 23 Dec 2023 18:16:23 GMT</lastBuildDate>
    <item>
      <title>Python Tensorflow.keras LSTM：类型错误：`generator` 产生了形状为 (36, 36, 147) 的元素，而预期形状为 (36, 147) 的元素</title>
      <link>https://stackoverflow.com/questions/77708557/python-tensorflow-keras-lstm-typeerror-generator-yielded-an-element-of-shape</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77708557/python-tensorflow-keras-lstm-typeerror-generator-yielded-an-element-of-shape</guid>
      <pubDate>Sat, 23 Dec 2023 18:05:26 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 tf.GradientTape 训练训练/有效/测试集？</title>
      <link>https://stackoverflow.com/questions/77708229/how-to-train-train-valid-test-set-using-tf-gradienttape</link>
      <description><![CDATA[我想问一下如何使用train/valid/test模型来训练模型。
我主要模仿这里的代码（https://www.tensorflow.org/guide/ keras/writing_a_training_loop_from_scratch）。
这是生成数据集和基本内容的代码。
导入keras
从 keras 导入层
将 numpy 导入为 np

输入= keras.Input（形状=（784，），名称=“数字”）
x1 = 层.Dense(64, 激活=“relu”)(输入)
x2 = 层.Dense(64, 激活=“relu”)(x1)
输出=layers.Dense(10,name=“预测”)(x2)
模型= keras.Model（输入=输入，输出=输出）

# 实例化优化器。
优化器 = tf.keras.optimizers.SGD(learning_rate=1e-3)
# 实例化损失函数。
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# 准备训练数据集。
批量大小 = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

# 保留 10,000 个样本用于验证。
x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

# 准备训练数据集。
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)

# 准备验证数据集。
val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
val_dataset = val_dataset.batch(batch_size)

# 获取模型
输入= keras.Input（形状=（784，），名称=“数字”）
x = 层.Dense(64, 激活=“relu”, 名称=“dense_1”)(输入)
x = 层.Dense(64, 激活=“relu”, 名称=“dense_2”)(x)
输出=layers.Dense(10,name=“预测”)(x)
模型= keras.Model（输入=输入，输出=输出）

# 实例化优化器来训练模型。
优化器 = tf.keras.optimizers.SGD(learning_rate=1e-3)
# 实例化损失函数。
loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# 准备指标。
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

@tf.函数
def train_step(x, y):
    使用 tf.GradientTape() 作为磁带：
        logits = 模型(x, 训练=True)
        loss_value = loss_fn(y, logits)
    grads = Tape.gradient(loss_value, model.trainable_weights)
    优化器.apply_gradients(zip(grads, model.trainable_weights))
    train_acc_metric.update_state(y, logits)
    回波损耗值

@tf.函数
def test_step(x, y):
    val_logits = 模型(x, 训练=False)
    val_acc_metric.update_state(y, val_logits)

这是训练部分。
导入时间

历元 = 2
对于范围内的纪元（纪元）：
    print(&quot;\n纪元%d开始&quot; % (纪元,))
    开始时间 = 时间.time()

    # 迭代数据集的批次。
    对于枚举（train_dataset）中的步骤（x_batch_train，y_batch_train）：
        loss_value = train_step(x_batch_train, y_batch_train)

        # 每 200 个批次记录一次。
        如果步骤 % 200 == 0:
            打印（
                “第 %d 步的训练损失（一批）：%.4f”
                %（步长，浮点数（损失值））
            ）
            print(“到目前为止已看到：%d 个样本” % ((step + 1) * batch_size))

    # 显示每个时期结束时的指标。
    train_acc = train_acc_metric.result()
    print(&quot;历元训练 acc: %.4f&quot; % (float(train_acc),))

    # 在每个时期结束时重置训练指标
    train_acc_metric.reset_states()

    # 在每个时期结束时运行验证循环。
    对于 val_dataset 中的 x_batch_val、y_batch_val：
        test_step(x_batch_val, y_batch_val)

    val_acc = val_acc_metric.result()
    val_acc_metric.reset_states()
    print(&quot;验证acc: %.4f&quot; % (float(val_acc),))
    print(&quot;所用时间: %.2fs&quot; % (time.time() - start_time))


我的问题是，如果数据集分为 3 个类别：训练、测试、有效，那么每个类别的代码会有什么不同，尤其是测试和有效。
例如，
&lt;前&gt;&lt;代码&gt;x_val = x_train[-20000:]
y_val = y_train[-20000:]

x_test = x_train[-20000:-10000]
y_test = y_train[-20000:-10000]

x_train = x_train[:-10000]
y_train = y_train[:-10000]


valid 也应该使用 tf.gradienttape 吗？或训练=真？这让我很困惑。
感谢您的回复。
谢谢，
分钟]]></description>
      <guid>https://stackoverflow.com/questions/77708229/how-to-train-train-valid-test-set-using-tf-gradienttape</guid>
      <pubDate>Sat, 23 Dec 2023 16:01:22 GMT</pubDate>
    </item>
    <item>
      <title>我正在研究一个示例：简单线性回归薪资数据解释了为什么为 x 列创建新轴的步骤以及什么是 np.newaxis [关闭]</title>
      <link>https://stackoverflow.com/questions/77708064/i-was-working-on-a-examples-simple-linear-regression-salary-data-from-that-expl</link>
      <description><![CDATA[# 分割训练数据和测试数据

X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state=100)

# 为 x 列创建新轴

X_train = X_train[:,np.newaxis]
X_test = X_test[:,np.newaxis]

使用 np.newaxis 添加新轴有助于重塑数据以实现兼容性]]></description>
      <guid>https://stackoverflow.com/questions/77708064/i-was-working-on-a-examples-simple-linear-regression-salary-data-from-that-expl</guid>
      <pubDate>Sat, 23 Dec 2023 15:05:36 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Label Studio 没有找到云源 - 本地存储（不存在）</title>
      <link>https://stackoverflow.com/questions/77707858/why-my-label-studio-didnt-find-the-cloud-source-local-storage-doesnt-exist</link>
      <description><![CDATA[我已经从 git 安装了 Label-Studio git 安装，
并且想要将存储更改为云存储 - 本地存储，路径为 C:\data\media\dataset 数据集图像路径
所以我从这一步开始 https://labelstud.io/guide/storage#Local -storage 和我的命令是：
 设置 LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED=true

 设置 LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT=C:\\data\\media\\dataset

我添加了set，因为如果没有它，它会出错。
当我更改本地存储时，它不存在。它警告使用“必须以 LOCAL_FILES_DOCUMENT_ROOT 开头”
[ErrorDetail(string=&#39;路径 C:\\\\data\\\\media 必须以 LOCAL_FILES_DOCUMENT_ROOT=D:\\ 开头，并且必须是子项，例如：D:\\abc&#39;, code =&#39;无效&#39;）]

[ErrorDetail(string=&#39;路径 LOCAL_FILES_DOCUMENT_ROOT=C:\\\\data\\\\media 不存在&#39;, code=&#39;无效&#39;)]

屏幕截图
屏幕截图]]></description>
      <guid>https://stackoverflow.com/questions/77707858/why-my-label-studio-didnt-find-the-cloud-source-local-storage-doesnt-exist</guid>
      <pubDate>Sat, 23 Dec 2023 13:55:59 GMT</pubDate>
    </item>
    <item>
      <title>如何为 XGboost 性能分配月度数据的权重？</title>
      <link>https://stackoverflow.com/questions/77707285/how-to-assign-weights-to-monthly-data-to-xgboost-performance</link>
      <description><![CDATA[我们收到了用于训练的逐月数据，使用每个月末的快照。 XGboost 模型（二元分类）在训练测试中一个月表现良好，但在实时测试中表现不佳，我们添加了额外的月份数据，但召回率下降了。有没有办法影响模型，使最近的数据更加重要/权重，同时使用上个月的数据来支持学习和数量。
我尝试单独使用不同月份的数据进行训练。由于数据高度不平衡尝试了SMOTE ENN、tomek，但是使用SMOTE降低了召回率很多。]]></description>
      <guid>https://stackoverflow.com/questions/77707285/how-to-assign-weights-to-monthly-data-to-xgboost-performance</guid>
      <pubDate>Sat, 23 Dec 2023 10:10:43 GMT</pubDate>
    </item>
    <item>
      <title>Vertex AI 表格数据预测错误</title>
      <link>https://stackoverflow.com/questions/77707074/vertex-ai-tabular-data-incorrect-prediction</link>
      <description><![CDATA[我使用表格数据集在 Google Vertex AI 中创建了模型，该数据集有两个感兴趣的列，称为 ActivityName 和 WorkType。我希望模型根据 ActivityName 预测工作类型。以下是我遵循的步骤：-

上传了包含用于训练、验证和测试的 SPLIT 列的 CSV 文件，确保每个工作类型至少拥有 80% 的训练数据。
创建了一个具有 3 个节点小时的 AutoML 模型，用于使用对数损失进行训练
部署模型
尝试了不同的活动名称，但预测错误

源示例位于此处]]></description>
      <guid>https://stackoverflow.com/questions/77707074/vertex-ai-tabular-data-incorrect-prediction</guid>
      <pubDate>Sat, 23 Dec 2023 08:37:44 GMT</pubDate>
    </item>
    <item>
      <title>如何拆分其中的列和数据？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77707030/how-split-column-and-data-in-it</link>
      <description><![CDATA[如何划分列及其中的数据。我已附上供参考。
示例：水泥_水列，其值为 3002； 203.0 必须分成名为水泥和水的两列，其值分别为 302.0 和 203.0。列值具有不同的分隔符（;，_），必须进行处理，并且这些值具有字符串数据，必须使用单词到数字将其转换为数值。
train.csv -https://drive.google.com/file/d/1rXC_cgJDHgvEsly9mVXphM_rl0ZI4sO2/view?usp=drive_link
test.csv https://drive.google。 com/file/d/1jUqlt5NVLvf9Y-vNLehCZShBqBkK0_NV/view?usp=drive_link
供您参考
&lt;前&gt;&lt;代码&gt;#代码
将 pandas 导入为 pd
从 word2number 导入 w2n

df = pd.read_csv(&#39;test.csv - Sheet1.csv&#39;)
def Convert_words_to_numbers(文本):
    单词 = text.replace(&#39;_&#39;, &#39; &#39;).replace(&#39;;&#39;, &#39; &#39;).replace(&#39;,&#39;, &#39; &#39;).split()
    Converted_words = [str(w2n.word_to_num(word)) if word.isalpha() else 逐字逐字]
    返回 &#39;​​ &#39;.join(converted_words)
df[&#39;水泥水&#39;] = df[&#39;水泥水&#39;].apply(lambda x:convert_words_to_numbers(x))
df[[&#39;水泥&#39;, &#39;水&#39;]] = df[&#39;水泥水&#39;].str.split(&#39; &#39;, Expand=True)
df[[&#39;coarse_aggregate&#39;, &#39;fine_aggregate&#39;]] = df[&#39;coarse_fine_aggregate&#39;].str.split(&#39;;&#39;, Expand=True)
df = df.drop([&#39;水泥水&#39;, &#39;coarse_fine_aggregate&#39;], axis=1)
df = df.apply(pd.to_numeric, 错误=&#39;忽略&#39;)
打印（df）
]]></description>
      <guid>https://stackoverflow.com/questions/77707030/how-split-column-and-data-in-it</guid>
      <pubDate>Sat, 23 Dec 2023 08:18:33 GMT</pubDate>
    </item>
    <item>
      <title>运行时错误：形状“[8192]”对于大小 8256 的输入无效</title>
      <link>https://stackoverflow.com/questions/77706527/runtimeerror-shape-8192-is-invalid-for-input-of-size-8256</link>
      <description><![CDATA[导入火炬
将 torch.nn 导入为 nn
从 torch.nn 导入功能为 F

批量大小 = 64
块大小 = 128
最大迭代数 = 5000
评估间隔 = 200
学习率 = 3e-4
设备 = &#39;cuda&#39; 如果 torch.cuda.is_available() else &#39;cpu&#39;
评估次数 = 100

火炬.manual_seed(5452)

打开（&#39;DiscordGPT_data.txt&#39;，&#39;r&#39;，encoding=&#39;utf-8&#39;）作为f：
    文本 = f.read()

字符=排序（列表（集合（文本）））
vocab_size = len(字符)

stoi = { ch:i for i,ch in enumerate(characters) }
itos = { i:ch for i,ch in enumerate(characters) }
编码 = lambda s: [stoi[c] for c in s]
解码 = lambda l: &#39;&#39;.join(itos[i] for i in l)

数据 = torch.tensor(编码(文本), dtype=torch.long)
n = int(0.9*len(数据))
训练数据 = 数据[:n]
评估数据 = 数据[n:]

def get_batch(分割):
    数据 = train_data 如果 split == &#39;train&#39; 否则评估_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i:i+block_size+1] for i in ix])
    x, y = x.to(设备), y.to(设备)

    返回 x、y

@torch.no_grad()
defestimate_loss():
    输出 = {}
    模型.eval()
    对于 [&#39;train&#39;, &#39;val&#39;] 中的分割：
        损失 = torch.zeros(eval_iters)
        对于范围内的 k(eval_iters)：
            X, Y = get_batch(分割)
            logits，损失 = 模型（X，Y）
            损失[k] = loss.item()
        out[split] = 损失.mean()
    模型.train()
    返回

类 BigramLangModel(nn.Module):

    def __init__(自身, vocab_size):
        超级().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

    def 前进（自我，idx，目标=无）：

        logits = self.token_embedding_table(idx)

        如果目标为无：
            损失=无
        别的：
            B、T、C = logits.shape
            logits = logits.view(B*T, C)
            目标 = 目标.view(B*T)
            损失 = F.cross_entropy(logits, 目标)

        返回 logits，损失
    
    def 生成（自我，idx，max_new_tokens）：

        对于 _ 在范围内（max_new_tokens）：
            logits，损失 = self(idx)
            logits = logits[:, -1, :]
            probs = F.softmax(logits, 暗淡=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), 暗淡=1)

        返回idx

模型 = BigramLangModel(vocab_size)
m = model.to(设备)

优化器 = torch.optim.AdamW(m.parameters(), lr=learning_rate)

对于范围内的迭代器（max_iters）：

    如果 iter % eval_interval == 0:
        损失=estimate_loss()
        print(f&quot;step {iter}: train loss {losses[&#39;train&#39;]:.4f}, val loss {losses[&#39;val&#39;]:.4f}&quot;)
        
    xb, yb = get_batch(&#39;火车&#39;)

    logits，损失 = 模型（xb，yb）
    优化器.zero_grad(set_to_none = True)
    loss.backward()
    优化器.step()

上下文 = torch.zeros((1,1), dtype = torch.long, 设备 = 设备)
print(解码(m.generate(上下文, max_new_tokens=500)[0].tolist()))

很好奇哪里出了问题，即使我减少批量和块大小，仍然会出现运行时错误，调整数字并没有真正的帮助。我对 ML/AI 领域还很陌生，因此任何有关如何前进的指示都会有所帮助。这应该是 Nanogpt 的复制品，并试图自己弄清楚。]]></description>
      <guid>https://stackoverflow.com/questions/77706527/runtimeerror-shape-8192-is-invalid-for-input-of-size-8256</guid>
      <pubDate>Sat, 23 Dec 2023 03:21:19 GMT</pubDate>
    </item>
    <item>
      <title>在机器学习模型中对 NDVI 和 LST 数据使用相同的空间分辨率</title>
      <link>https://stackoverflow.com/questions/77705553/using-same-spatial-resolution-for-ndvi-and-lst-data-in-machine-learning-model</link>
      <description><![CDATA[我目前正在开发干旱预报机器学习模型，这是我第一次处理卫星数据。我的两个数据源是来自 Google Earth Engine 的用于获取 NDVI 值的 MOD13A1 数据集和用于获取 LST 值的 MOD11A1 数据集。感兴趣的区域是南加州。根据 Google Earth Engine，MOD13A1 的默认分辨率为 500m，而 MOD11A1 的默认分辨率为 1000m。
我是否需要对这两种产品使用相同的空间分辨率才能最大限度地提高模型的性能？如果是这样，那会是什么决议？
我尝试更改 NDVI 的 getRegion() 方法中的“scale”参数，当我这样做时，我得到了不同的值。这就是我提出问题的原因。]]></description>
      <guid>https://stackoverflow.com/questions/77705553/using-same-spatial-resolution-for-ndvi-and-lst-data-in-machine-learning-model</guid>
      <pubDate>Fri, 22 Dec 2023 19:46:46 GMT</pubDate>
    </item>
    <item>
      <title>我无法将数组传递给 svm 模型，它显示使用序列设置数组元素的错误</title>
      <link>https://stackoverflow.com/questions/77705514/i-am-not-able-to-pass-an-array-to-svm-model-it-is-showing-an-error-of-setting-an</link>
      <description><![CDATA[由于我正在处理音频数据集，所以首先我提取了 MFCC 特征并将其存储到列表中，然后我进行了填充和展平，我收到了此警告，但我忽略了它一段时间
我不能在这里将数据类型声明为对象，因为我必须在 Svm 模型中传递它
mfcc 功能及其数组的代码
SVM 代码：
svm 代码和错误
那么任何人都可以更正代码，以便我可以将其传递到我的 Svm 模型中，并且我可以进行音频分类吗？]]></description>
      <guid>https://stackoverflow.com/questions/77705514/i-am-not-able-to-pass-an-array-to-svm-model-it-is-showing-an-error-of-setting-an</guid>
      <pubDate>Fri, 22 Dec 2023 19:34:37 GMT</pubDate>
    </item>
    <item>
      <title>Resnet34第一层7x7或3x3</title>
      <link>https://stackoverflow.com/questions/77704426/resnet34-first-layer-7x7-or-3x3</link>
      <description><![CDATA[我一直在尝试使用 pytorch 实现 Resnet34，但在查看其他实现时，我发现其中一些具有 3x3 卷积层 + bn + relu 作为第一层。然而，架构图上却写着7x7/2的卷积层。我真的很困惑哪一个是正确的。顺便说一下，我正在 CIFAR10 上进行训练，目前使用 7x7 卷积层经过 100 个周期后获得了 0.9 的准确率。
谢谢！
架构图
self.input_layer = nn.Sequential(
nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,bias=False),
nn.BatchNorm2d(64),
ReLU(),
nn.MaxPool2d(3, 步长=2, 填充=1)
）

这是我的第一个卷积层的代码。]]></description>
      <guid>https://stackoverflow.com/questions/77704426/resnet34-first-layer-7x7-or-3x3</guid>
      <pubDate>Fri, 22 Dec 2023 15:14:50 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络不学习</title>
      <link>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</link>
      <description><![CDATA[我正在尝试在包含 1500 张图像（15 个类别）的训练集上训练用于图像识别的卷积神经网络。有人告诉我，采用这种架构和从均值为 0、标准差为 0.01 的高斯分布得出的初始权重以及初始偏差值为 0 的情况，在适当的学习率下，它的准确度应该达到 30 左右%。
但是，它根本没有学到任何东西：准确度与随机分类器相似，并且训练后的权重仍然遵循正态分布。我做错了什么？
这是神经网络
class simpleCNN(nn.Module)：
  def __init__(自身):
    super(simpleCNN,self).__init__() #初始化模型

    self.conv1=nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,stride=1) #输出图像大小为(size+2*padding-kernel)/stride --&gt;62*62
    self.relu1=nn.ReLU()
    self.maxpool1=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像62/2--&gt;31*31

    self.conv2=nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,stride=1) #输出图像为29*29
    self.relu2=nn.ReLU()
    self.maxpool2=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像为29/2--&gt;14*14（MaxPool2d近似大小与floor）

    self.conv3=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1) #输出图像为12*12
    self.relu3=nn.ReLU()

    self.fc1=nn.Linear(32*12*12,15) #16 个通道 * 16*16 图像（64*64，步幅为 2 的 2 个 maxpooling），15 个输出特征=15 个类
    self.softmax = nn.Softmax(dim=1)

  def 前向（自身，x）：
    x=self.conv1(x)
    x=self.relu1(x)
    x=self.maxpool1(x)

    x=self.conv2(x)
    x=self.relu2(x)
    x=self.maxpool2(x)

    x=self.conv3(x)
    x=self.relu3(x)

    x=x.view(-1,32*12*12)

    x=self.fc1(x)
    x=self.softmax(x)

    返回x

初始化：
def init_weights(m):
  如果 isinstance(m,nn.Conv2d) 或 isinstance(m,nn.Linear)：
    nn.init.normal_(m.weight,0,0.01)
    nn.init.zeros_(m.bias)

模型 = simpleCNN()
模型.应用（init_weights）

训练函数：
loss_function=nn.CrossEntropyLoss()
优化器=optim.SGD(model.parameters(),lr=0.1,动量=0.9)

def train_one_epoch(epoch_index,loader):
  运行损失=0

  对于 i，枚举（加载器）中的数据：

    input,labels=data #获取小批量
    输出=模型（输入）#前向传递

    loss=loss_function(outputs,labels) #计算损失
    running_loss+=loss.item() #总结到目前为止处理的小批量的损失

    Optimizer.zero_grad() #重置梯度
    loss.backward() #计算梯度
    optimizer.step() #更新权重

  return running_loss/(i+1) # 每个小批量的平均损失


培训：
&lt;前&gt;&lt;代码&gt;纪元=20

best_validation_loss=np.inf

对于范围内的纪元（EPOCHS）：
  print(&#39;纪元{}:&#39;.format(纪元+1))

  模型.train(True)
  train_loss=train_one_epoch(epoch,train_loader)

  运行验证损失=0.0

  模型.eval()

  with torch.no_grad(): # 禁用梯度计算并减少内存消耗
    对于 i，枚举中的 vdata（validation_loader）：
      vinputs,vlabels=vdata
      v输出=模型（v输入）
      vloss=loss_function(v输出,v标签)
      running_validation_loss+=vloss.item()
  验证损失=运行验证损失/(i+1)
  print(&#39;LOSS 训练：{} 验证：{}&#39;.format(train_loss,validation_loss))

  if validation_loss
使用默认初始化，它的效果会好一点，但我应该使用高斯达到 30%。
您能发现一些可能导致它无法学习的问题吗？我已经尝试过不同的学习率和动力。]]></description>
      <guid>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</guid>
      <pubDate>Fri, 22 Dec 2023 14:06:23 GMT</pubDate>
    </item>
    <item>
      <title>如何使用自定义数据集格式训练自定义 Transformer 模型</title>
      <link>https://stackoverflow.com/questions/77680618/how-to-train-a-customized-transformer-model-with-custom-dataset-formatting</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77680618/how-to-train-a-customized-transformer-model-with-custom-dataset-formatting</guid>
      <pubDate>Mon, 18 Dec 2023 16:56:58 GMT</pubDate>
    </item>
    <item>
      <title>神经先知根本不预测？</title>
      <link>https://stackoverflow.com/questions/77231544/neural-prophet-not-predicting-at-all</link>
      <description><![CDATA[我正在尝试预测进入某个海滩的顾客数量。因此，数据中的数字往往会波动，并且希望使用 Neural Prophet 来预测未来的客人。然而，根据我的神经先知模型的当前设置，该模型根本无法预测原始数据中的最终日期，尽管它确实预测了其他参数，只是不准确。
以下是预测结果（虚线）与原始结果（实线）的对比：
在此处输入图片描述
我特意要求预测未来 100 天，但根本没有显示。
这是我的模型设置：
 模型 = NeuralProphet(
        #growth=“off”, # 确定趋势类型：&#39;线性&#39;、&#39;不连续&#39;、&#39;关闭&#39;
        #changepoints=None, #可能包含变更点的日期列表（None -&gt;automatic ）
        n_changepoints=0,
        #changepoints_range=0,
        #trend_reg=0,
        # trend_reg_threshold=False,
        # #seasonality_reg=1,
        # # d_hidden = 0,
        n_lags=10,
        # # num_hidden_​​layers=0, # AR-Net 隐藏层的维度
        # # ar_reg=None, # AR 系数的稀疏性
        学习率=0.01，
        纪元=100，
        Normalize=“auto”, # 标准化类型 (&#39;minmax&#39;, &#39;standardize&#39;, &#39;soft&#39;, &#39;off&#39;)
        impute_missing=真，
        yearly_seasonality=真，
        week_seasonality=假，
        daily_seasonality=假，
        季节性_模式=“乘法”，
        loss_func=“均方误差”,
    ）

    # 将模型与训练数据进行拟合
    model.fit(数据,频率=“D”)
    未来= model.make_future_dataframe（数据，周期= 1000，n_historic_predictions = len（数据））
    预测 = model.predict(future)
]]></description>
      <guid>https://stackoverflow.com/questions/77231544/neural-prophet-not-predicting-at-all</guid>
      <pubDate>Wed, 04 Oct 2023 16:52:25 GMT</pubDate>
    </item>
    <item>
      <title>如何动态地将curl变量发送给管道工函数？</title>
      <link>https://stackoverflow.com/questions/61824959/how-to-send-curl-variables-to-plumber-function-dynamically</link>
      <description><![CDATA[我想根据任意数量的输入变量动态调用管道工 API。我需要将curl 输入映射到函数名称的输入。例如，如果函数有一个输入 hi，则 curl -s --data &#39;hi=2&#39; 意味着 hi=2 应该是作为输入参数传递给函数。这可以直接在 R 中使用 match.call() 完成，但在通过管道工 API 调用它时失败。
获取函数
&lt;前&gt;&lt;代码&gt;#&#39; @post /API
#&#39; @serializer unboxedJSON
tmp &lt;- 函数(hi) {

  输出 &lt;- 列表(hi=hi)

  out &lt;- toJSON(out, Pretty = TRUE, auto_unbox = TRUE)

  返回（出）

}

tmp(嗨=2)
输出：{嗨：2}

然后
curl -s --data &#39;hi=10&#39; http://127.0.0.1/8081/API
输出: {\n \&quot;hi\&quot;: \&quot;2\&quot;\n}

一切看起来都不错。但是，取函数
&lt;前&gt;&lt;代码&gt;#&#39; @post /API
#&#39; @serializer unboxedJSON
tmp &lt;- 函数(...) {

  out &lt;- match.call() %&gt;%
         as.list() %&gt;%
         .[2:长度(.)] # %&gt;%

  out &lt;- toJSON(out, Pretty = TRUE, auto_unbox = TRUE)

  返回（出）

}
tmp(嗨=2)
输出：{嗨：2}

然后
curl -s --data &#39;hi=10&#39; http://127.0.0.1/8081/API
out: {“错误”:“500 - 内部服务器错误”,“消息”:“错误: 没有方法 asJSON S3 类: R6\n”}

实际上，我真正想做的是加载我的 ML 模型以使用管道工 API 预测分数。例如
model &lt;- readRDS(&#39;model.rds&#39;) # 将模型加载为全局变量

预测得分 &lt;- 函数(...) {
    
    df_in &lt;- match.call() %&gt;%
        as.list() %&gt;%
        .[2:长度(.)] %&gt;%
        as.data.frame()

    json_out &lt;- 列表(
        Score_out = 预测(模型, df_in) %&gt;%
        toJSON(., 漂亮 = T, auto_unbox = T)

    返回（json_out）
}


此函数在本地运行时按预期工作，但通过 curl -s --data &#39;var1=1&amp;var2=2...etc&#39; http://listen_address 通过 API 运行&lt; /p&gt;
我收到以下错误：
&lt;块引用&gt;
{“错误”：“500 - 内部服务器错误”，“消息”：“as.data.frame.default(x[[i]]，可选 = TRUE) 中的错误：无法强制类“c(“PlumberResponse”，“R6”)”到 data.frame\n&quot;}
]]></description>
      <guid>https://stackoverflow.com/questions/61824959/how-to-send-curl-variables-to-plumber-function-dynamically</guid>
      <pubDate>Fri, 15 May 2020 17:22:08 GMT</pubDate>
    </item>
    </channel>
</rss>