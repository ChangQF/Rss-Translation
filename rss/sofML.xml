<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 12 Mar 2024 03:17:03 GMT</lastBuildDate>
    <item>
      <title>改进多模态检测模型指标[关闭]</title>
      <link>https://stackoverflow.com/questions/78143946/improving-multimodal-detection-model-metrics</link>
      <description><![CDATA[我正在尝试建立一个多模态模型来检测言语中的抑郁症。我使用 opensmile 从语音文件中提取了音频特征（这是必需的），并在文本文件上使用了 bert 嵌入。然后我将它们都输入到 LSTM-CNN 模型中
def create_fused_model(audio_input_shape, text_input_shape, dropout_rate=0.3, l2_reg= 0.01):
        # 音频输入和 LSTM 处理
    音频输入=输入（形状=（音频输入形状，1），名称=&#39;音频输入&#39;）
    lstm_audio = LSTM(32, return_sequences=True, kernel_regularizer=l2(l2_reg))(audio_input)
    lstm_audio2 = LSTM(16, return_sequences=True, kernel_regularizer=l2(l2_reg))(lstm_audio)
    lstm_audio2 = 辍学（dropout_rate）（lstm_audio2）
    lstm_audio_pooled = GlobalAveragePooling1D()(lstm_audio2)
    
    # 文本输入和CNN处理
    文本输入=输入（形状=（文本输入形状，1），名称=&#39;文本输入&#39;）
    conv_text = Conv1D(filters=32，kernel_size=3，activation=&#39;relu&#39;，strides=1，padding=&#39;same&#39;，kernel_regularizer=l2(l2_reg))(text_input)
    conv_text2 = Conv1D(filters=16，kernel_size=3，activation=&#39;relu&#39;，strides=1，padding=&#39;same&#39;，kernel_regularizer=l2(l2_reg))(conv_text)
    conv_text2 = Dropout(dropout_rate)(conv_text2)
    conv_text_pooled = GlobalAveragePooling1D()(conv_text2)
    
    # Fusion - 门控机制
    ated_layer = Multiply()([lstm_audio_pooled, conv_text_pooled])
    
    # 用于分类的密集层
    密集层=密集（64，激活=&#39;relu&#39;，kernel_regularizer=l2（l2_reg））（gate_layer）
    dropout_layer = Dropout(dropout_rate)(dense_layer)
    输出层=密集（1，激活=&#39;sigmoid&#39;，名称=&#39;输出&#39;）（dropout_layer）
    
    # 模型编译
    模型=模型（输入=[音频输入，文本输入]，输出=输出层）
    model.compile(优化器=Adam(learning_rate=0.001),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
    
    返回模型

我无法获得超过 67% 的准确率。我尝试过更改所使用的模型、超参数调整以及对 txt 文件使用不同的特征提取技术。]]></description>
      <guid>https://stackoverflow.com/questions/78143946/improving-multimodal-detection-model-metrics</guid>
      <pubDate>Mon, 11 Mar 2024 23:45:24 GMT</pubDate>
    </item>
    <item>
      <title>scikit-optimize 的 BayesSearchCV 的作者是谁？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78143727/who-is-the-author-of-bayessearchcv-from-scikit-optimize</link>
      <description><![CDATA[我在我的手稿中使用了 scikit-optimize 的 BayesSearchCV。我在网上搜索过，但没有找到合适的引用。]]></description>
      <guid>https://stackoverflow.com/questions/78143727/who-is-the-author-of-bayessearchcv-from-scikit-optimize</guid>
      <pubDate>Mon, 11 Mar 2024 22:33:09 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助理解 SHAP 瀑布图 [关闭]</title>
      <link>https://stackoverflow.com/questions/78143713/need-help-in-understanding-shap-waterfall-chart</link>
      <description><![CDATA[我正在使用一个包含大约 72 个特征的数据集。我试图预测客户是否会流失，因此我的目标变量有 2 个类别：0 - 客户和 1 - 前客户（流失）
我使用此代码生成了 SHAP 瀑布图，并注意到 f(x) = 0.1
但是，当我将 sv[:,:,1], sv.base_values[:,1] 中的 1 替换为 0 时，我得到 f(x) = 0.9
为什么该特定指数的概率会发生变化？我假设概率越高，流失的可能性就越大，但现在我很困惑。
解释器 = 解释器(rf)
sv = 解释器(Train_X)

exp = 解释(sv[:,:,1], sv.base_values[:,1],Train_X, feature_names=None)

idx = 16 # 解释的数据点
瀑布图（exp[idx]，max_display=10）

这是我的瀑布图：
]]></description>
      <guid>https://stackoverflow.com/questions/78143713/need-help-in-understanding-shap-waterfall-chart</guid>
      <pubDate>Mon, 11 Mar 2024 22:30:19 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络节点迭代更新时输入输出维度的一致性</title>
      <link>https://stackoverflow.com/questions/78143339/graph-neural-network-nodes-input-and-output-dimension-consistency-amid-iterativ</link>
      <description><![CDATA[我试图弄清楚使用图神经网络学习嵌入后节点的输入维度（n）和输出维度（m）是否可以不同，我想是的。但是，我正在努力弄清楚这是如何实现的，如下所示：
一般在迭代步骤中：
h_u^(k+1) = update(h_u^(k) + 聚合({h_v^(k) | u 的 v 邻居})
现在，h_u(0) 的维度是输入维度，因此在生成 h_u(1) 时，聚合函数和更新函数的输入是 n 的向量，这里的输出具有维度 m（主要是矩阵乘法和完成加权求和类型的聚合，并将维度调整为 m)，那么从下一次迭代开始，如何通过相同的函数或矩阵来处理维度 m（而不是 n）的向量进行更新？]]></description>
      <guid>https://stackoverflow.com/questions/78143339/graph-neural-network-nodes-input-and-output-dimension-consistency-amid-iterativ</guid>
      <pubDate>Mon, 11 Mar 2024 20:47:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在本地运行主宰模型</title>
      <link>https://stackoverflow.com/questions/78143186/how-to-run-juggernaut-model-in-local</link>
      <description><![CDATA[我想使用 python 在本地电脑上运行微调的稳定扩散模型。例如剑圣：https://huggingface.co/RunDiffusion/Juggernaut-XL-v9
这是我的代码（它适用于 stable-diffusion-xl-base-1.0）：
随机导入
从扩散器导入 DiffusionPipeline、StableDiffusionXLImg2ImgPipeline
进口火炬
导入GC
导入时间

# 用于清理内存
GC.collect()
torch.cuda.empty_cache()

开始时间 = 时间.time()

型号 =“RunDiffusion/Juggernaut-XL-v9”
管道 = DiffusionPipeline.from_pretrained(
    模型，
    torch_dtype=torch.float16,
）

管道.to(“cuda”)

提示=（“中世纪男性骑士肖像，阳刚的外观，背景中的战斗，清晰的焦点，高度详细，电影风格的灯光，阴影”）
种子 = random.randint(0, 2**32 - 1)

生成器 = torch.Generator(“cuda”).manual_seed(seed)
图像=管道（提示=提示，生成器=生成器，num_inference_steps=1）
图像=图像.图像[0]
image.save(f&quot;output_images/{seed}.png&quot;)

结束时间 = time.time()

总时间 = 结束时间 - 开始时间
分钟 = int(total_time // 60)
秒 = int(总时间 % 60)

print(f&quot;花费: {分钟} 分 {秒} 秒&quot;)
print(f&quot;保存到output_images/{seed}.png&quot;)


但我得到：
&lt;块引用&gt;
OSError：在目录中找不到名为 pytorch_model.bin、tf_model.h5、model.ckpt.index 或 flax_model.msgpack 的文件时出错

可能是因为python、cuda版本的原因。我正在删除我的库版本：
Python 3.9.0
PyTorch：2.2.0+cu118
CUDA：11.8
扩散器：0.26.3
变形金刚：4.38.1]]></description>
      <guid>https://stackoverflow.com/questions/78143186/how-to-run-juggernaut-model-in-local</guid>
      <pubDate>Mon, 11 Mar 2024 20:12:01 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 中的 Brian2 模拟器实现液体状态机用于分类任务？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78141953/how-to-implement-liquid-state-machine-for-a-classification-task-using-brian2-sim</link>
      <description><![CDATA[我是一名正在攻读本科生的学生，目前正在研究用于分类相关任务的液态机。但是，即使花了几天时间研究 LSM，我也无法找到如何实现它们进行分类的良好开端。
如果没有适当的资源，学习概念也非常困难。那么，有人可以帮助我进行研究吗？]]></description>
      <guid>https://stackoverflow.com/questions/78141953/how-to-implement-liquid-state-machine-for-a-classification-task-using-brian2-sim</guid>
      <pubDate>Mon, 11 Mar 2024 15:53:53 GMT</pubDate>
    </item>
    <item>
      <title>将 Android ML-Kit 鸟类分类器与 Python 结合使用</title>
      <link>https://stackoverflow.com/questions/78139883/using-android-ml-kit-bird-classifier-with-python</link>
      <description><![CDATA[我测试了 ML Kit 中的 Android Vision 快速入门应用程序。正如您在图片中看到的，这里可以进行对象跟踪。现在我正在使用 Python 尝试相同的模型 (bird_classifier.tflite)。这非常有效，但是我如何在这里获取边界框呢？无论我做什么，反馈都是：该模型仅包含一个张量。但为什么它可以在 Android 应用程序中运行呢？
有人可以给我看一个代码示例吗？
测试图片
image = Image.fromarray(screenshot)
image_pred = image.resize((宽度,高度), Image.ANTIALIAS)
结果=classify_image(interpreter, image_pred)
# TrackingID = results[i0][0]
# 分数 = 结果[i0][1]
# Boxes = ?]]></description>
      <guid>https://stackoverflow.com/questions/78139883/using-android-ml-kit-bird-classifier-with-python</guid>
      <pubDate>Mon, 11 Mar 2024 10:19:20 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法直接在windows下使用Nvidia Rapids？</title>
      <link>https://stackoverflow.com/questions/78136820/a-way-to-use-nvidia-rapids-in-windows-directly</link>
      <description><![CDATA[我想知道有没有办法直接在 Windows 11 中安装 Nvidia Rapids 并使用它，而不是与 wsl2 或 docker 一起使用？或者有没有办法将 jupyter lab 主机连接到 dataspell？
我尝试通过 github 将 cuMl 直接安装到 Windows，但每次都失败。我想在 jetbrains dataspell 中使用 cuml，这就是寻求帮助的原因。]]></description>
      <guid>https://stackoverflow.com/questions/78136820/a-way-to-use-nvidia-rapids-in-windows-directly</guid>
      <pubDate>Sun, 10 Mar 2024 17:30:40 GMT</pubDate>
    </item>
    <item>
      <title>如何以高精度（+ 90%）对面部特征嵌入进行分类。我可以在 svm 模型中进行哪些调整来对 20 多个类别进行分类</title>
      <link>https://stackoverflow.com/questions/78133540/how-to-classify-facials-features-embedding-with-high-accuracy-90-what-adjus</link>
      <description><![CDATA[我使用facenet提取特征并使用svm进行分类。效果很好，但 20 堂课后，准确率下降到 75%。如何在利用 GPU 的同时优化 svm。
我使用了这个 svm 类模型
scikit learn 的 svm 模型不使用 GPU，所以我使用了这个
类 SVM(nn.Module):
    def __init__(自身):
        超级（SVM，自我）.__init__()
        self.fc = nn.Linear(X.shape[1], len(ClassList))

    def 前向（自身，x）：
        返回 self.fc(x)

但是对于 20 多个类别来说，这个准确率非常低
我也尝试过使用这个：
类 SoftmaxUsed(nn.Module):
    def __init__(自身):
        超级().__init__()
        self.layers = nn.Sequential(nn.Linear(512, 1024),
                                 ReLU(),
                                 nn.Dropout(0.2),
                                 nn.线性(1024, 1024),
                                 ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(1024, len(ClassList)),
                                 nn.LogSoftmax(dim=1))
    def 前向（自身，x）：
        返回 self.layers(x)

但准确率最高仍为 86%]]></description>
      <guid>https://stackoverflow.com/questions/78133540/how-to-classify-facials-features-embedding-with-high-accuracy-90-what-adjus</guid>
      <pubDate>Sat, 09 Mar 2024 18:56:09 GMT</pubDate>
    </item>
    <item>
      <title>持久化模型如何提高准确性？</title>
      <link>https://stackoverflow.com/questions/78127879/how-does-persisting-the-model-increase-accuracy</link>
      <description><![CDATA[导入 pandas 作为 pd
从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn.model_selection 导入 train_test_split
从 sklearn.metrics 导入 precision_score, f1_score

Whitewine_data = pd.read_csv(&#39;winequality-white.csv&#39;,
分隔符=&#39;;&#39;)

变量= [&#39;alcohol_cat&#39;, &#39;酒精&#39;, &#39;硫酸盐&#39;, &#39;密度&#39;,
“总二氧化硫”、“柠檬酸”、“挥发性酸度”、
‘氯化物’]

X = Whitewine_data[变量]
y = Whitewine_data[&#39;质量&#39;]
X_train, X_test, y_train, y_test = train_test_split(X, y,
测试大小=0.2）

模型 = DecisionTreeClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
准确度=准确度_得分(y_test, y_pred)
f1 = f1_score(y_test, y_pred, 平均值=&#39;加权&#39;)

预测 = model.predict([[0.27, 0.36, 0.045, 170, 1.001,
0.45, 8.9, 0]])
print(f&#39;预测输出：{预测}&#39;)
print(f&#39;准确率: {准确率 * 100}%&#39;)
print(f&#39;F1 分数: {f1 * 100}% &#39;)

这个初始模型的准确度得分为 57%
================================================== ===============
whitewine_data = pd.read_csv(&#39;winequality-white.csv&#39;,
分隔符=&#39;;&#39;)

# 要从数据集中删除的变量 - 不是输入
变量
变量 = [&#39;固定酸度&#39;,&#39;残留糖&#39;,&#39;游离硫
二氧化硫&#39;, &#39;pH&#39;, &#39;质量&#39;, &#39;isSweet&#39;]

X = Whitewine_data.drop(变量，轴=1)
y = Whitewine_data[&#39;质量&#39;]

X_train, X_test, y_train, y_test = train_test_split(X, y,
测试大小=0.2）

模型 = DecisionTreeClassifier()
model.fit(X_train, y_train)

joblib.dump(模型, &#39;WhiteWine_Quality_Predictor.joblib&#39;)

创建保存的模型
================================================== ===============
whitewine_data = pd.read_csv(&#39;winequality-white.csv&#39;,
分隔符=&#39;;&#39;)

变量 = [&#39;挥发酸度&#39;, &#39;柠檬酸&#39;, &#39;氯化物&#39;,
&#39;二氧化硫总量&#39;、&#39;密度&#39;、&#39;硫酸盐&#39;、&#39;酒精&#39;、
&#39;酒精_猫&#39;]

X_test = Whitewine_data[变量]
y_test = Whitewine_data[&#39;质量&#39;]

模型 = joblib.load(&#39;WhiteWine_Quality_Predictor.joblib&#39;)

y_pred = model.predict(X_test)

f1 = f1_score(y_test, y_pred, 平均值=&#39;加权&#39;)
准确度=准确度_得分(y_test, y_pred)
预测 = model.predict([[0.27, 0.36, 0.045, 170, 1.001,
0.45, 10.9, 3]])

print(f&#39;F1分数: {f1 * 100}%&#39;)
print(f&#39;模型精度: {accuracy * 100}%&#39;)
print(f&#39;预测输出：{预测}&#39;)

调用保存的模型现在的准确率达到 92%
问题：调用已保存的模型如何导致增加
我看到的准确性]]></description>
      <guid>https://stackoverflow.com/questions/78127879/how-does-persisting-the-model-increase-accuracy</guid>
      <pubDate>Fri, 08 Mar 2024 13:07:01 GMT</pubDate>
    </item>
    <item>
      <title>Pytroch 分割模型(.pt) 未转换为 CoreML</title>
      <link>https://stackoverflow.com/questions/78091161/pytroch-segmentation-model-pt-not-converting-to-coreml</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78091161/pytroch-segmentation-model-pt-not-converting-to-coreml</guid>
      <pubDate>Sat, 02 Mar 2024 01:26:48 GMT</pubDate>
    </item>
    <item>
      <title>SageMaker 实验跟踪重复</title>
      <link>https://stackoverflow.com/questions/76821347/sagemaker-experiment-tracking-duplication</link>
      <description><![CDATA[我正在尝试通过 AWS SageMaker 使用脚本模式训练模型。
我想使用 AWS SageMaker Experiments 以及训练作业中的一些计算指标来跟踪此训练作业。当我开始训练作业时，会成功创建一个新的实验运行，该实验运行跟踪所有提供的超参数（例如，nesimators）。
但是，如前所述，此外，我还想跟踪自定义脚本中的其他指标（例如准确性）。在这里，我在拟合模型之前使用 load_run()，然后使用 run.log_metric() 记录指标。但是，当我这样做时，SageMaker 会在 UI 中创建一个新的单独实验条目，这意味着我的超参数和指标单独存储在两个单独的实验运行中：

我希望在一次实验运行中看到所有指标和超参数的组合。我做错了什么？
这是我用来启动训练过程的缩写代码：
&lt;前&gt;&lt;代码&gt;
exp_name = “sklearn-脚本模式-实验”

与运行（
    实验名称=实验名称，
    sagemaker_session=sess,
）运行时：

    sklearn_estimator = SKLearn(&#39;train.py&#39;,
                                    instance_type=&#39;ml.m5.large&#39;,
                                    Framework_version=&#39;1.0-1&#39;,
                                    role=“arn:aws:iam:::role/service-role/AmazonSageMaker-ExecutionRole-”,
                                    超参数={&#39;nestimators&#39;: 100},
                                    环境={“区域”：区域}）

    sklearn_estimator.fit({&#39;train&#39;: f&#39;s3://{BUCKET}/{S3_INPUT_PATH}&#39;})

这是缩写的train.py：
 #在这里解析参数...等等...


    模型 = RandomForestClassifier(n_estimators=args.nesimators,
                                   最大深度=5，
                                   随机状态=1）

    使用 load_run(sagemaker_session=sagemaker_session) 作为运行：

        模型.fit(X, y)

        run.log_metric(name = &quot;最终测试损失&quot;, value = 0.9)
]]></description>
      <guid>https://stackoverflow.com/questions/76821347/sagemaker-experiment-tracking-duplication</guid>
      <pubDate>Wed, 02 Aug 2023 15:20:27 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 numpy python 读取多个 3D 图像并将它们存储在 4D 数组中？</title>
      <link>https://stackoverflow.com/questions/72667735/how-to-read-multiple-3d-images-and-store-them-in-4d-array-using-numpy-python</link>
      <description><![CDATA[我使用下面的代码为 3 个图像创建了一个形状为 (2, 3, 365, 256, 256) (2, 365, 256, 256) 的数组，但是我需要我的形状为 (2,365, 256, 256, 3) (2, 365, 256, 256) 让我的模型运行，有什么提示吗？
def Load_function(路径):
  f_img= nib.load(路径)
  img_data= f_img.get_fdata()
返回img_data


 def __load__(self, id_name):

    image_path = os.path.join(self.path, id_name)
    ## 读取图像
    图像 = np.empty((0,365, 256, 256))

    对于 [“image2B.nii.gz”、“image1to2_nlB.nii.gz”、“diffFSL.nii.gz”] 中的 imname：
        img = Load_function(os.path.join(image_path,imname))

        img = 调整大小_数据(img)
        
        ## 标准化
        img = img/np.percentile(img,99.5)
        #images.append(img)
        #images.append(img)
    ## 存储到您在上面创建的 4D 数组中（这将保存您想要的一个主题的所有图像）
        image = np.append(img[np.newaxis, ...],image, axis=0) # 为每个图像添加一个新轴并将其附加到结果中
          
    ## 阅读面具
    mask = Load_function(os.path.join(image_path, “ground_truth.nii.gz”))
    掩码 = resize_data(掩码)
    
    返回图像，掩码
]]></description>
      <guid>https://stackoverflow.com/questions/72667735/how-to-read-multiple-3d-images-and-store-them-in-4d-array-using-numpy-python</guid>
      <pubDate>Sat, 18 Jun 2022 08:12:16 GMT</pubDate>
    </item>
    <item>
      <title>种子在随机森林中起什么作用？</title>
      <link>https://stackoverflow.com/questions/36307429/what-does-seed-do-in-random-forest</link>
      <description><![CDATA[我知道通常使用种子设置，以便我们可以重现相同的结果。但是，在随机森林部分中设置种子实际上是做什么的。它是否会更改 R 中的 randomForest() 函数的任何参数，例如 nTree 或 sampSize 。 
我每次都为随机森林模型使用不同的种子，但想知道不同的种子如何影响随机森林模型。]]></description>
      <guid>https://stackoverflow.com/questions/36307429/what-does-seed-do-in-random-forest</guid>
      <pubDate>Wed, 30 Mar 2016 11:26:58 GMT</pubDate>
    </item>
    <item>
      <title>randomForest R 包的奇怪结果</title>
      <link>https://stackoverflow.com/questions/27324066/weird-results-with-the-randomforest-r-package</link>
      <description><![CDATA[我有一个包含 10,000 行和两列的数据框、段（具有 32 个值的因子）和目标（具有两个值“是”和“否”的因子，每个值 5,000 个）。我正在尝试使用随机森林来使用分段作为特征对目标进行分类。
训练随机森林分类器后：
&lt;前&gt;&lt;代码&gt;&gt;森林 &lt;- randomForest(目标 ~ 段，数据)

混淆矩阵强烈偏向“否”：
&lt;前&gt;&lt;代码&gt;&gt;打印（森林$混乱）

      否 是 类错误
无 4872 76 0.01535974
是 5033 19 0.99623911

在 10,000 行中，不到 100 行被分类为“是”（即使原始计数为 50/50）。如果我切换标签的名称，我会得到相反的结果：
&lt;前&gt;&lt;代码&gt;&gt; data$target &lt;- as.factor(ifelse(data$target == &#39;是&#39;, &#39;否&#39;, &#39;是&#39;))
&gt;森林 &lt;- randomForest(目标 ~ 段，数据 = 数据)
&gt;打印（森林$混乱）

      否 是 类错误
无 4915 137 0.02711797
是 4810 138 0.97210994

所以这不是一个真正的信号...而且，原始的交叉表是相对平衡的：
&lt;前&gt;&lt;代码&gt;&gt;表（数据$目标，数据$段）
 
         1 10 11 12 13 14 15 16 17 18 19 2 20 21 22 23 24 25 26 27 28 29 3 30 31 32 4 5 6 7 8 9
  否 1074 113 121 86 68 165 210 70 120 127 101 132 90 108 171 122 95 95 76 72 105 71 234 58 83 72 290 162 262 192 64 139
  是 1114 105 136 120 73 201 209 78 130 124 90 145 81 104 155 128 79 85 83 70 93 78 266 70 93 76 291 160 235 194 49 137

看起来 randomForest 采用第一个标签，并且几乎总是为其分配点。澄清一下，数据框是具有更多功能的更大表格的子集 - 我刚刚发现这个特定功能以某种方式导致了这个结果，无论包含多少其他功能。我想知道我是否遗漏了随机森林分类器的一些基本知识，或者是否存在一些编码问题或其他错误导致了这个奇怪的结果。
原始数据集可在此处作为 RDS 获取：
https://www.dropbox.com/s/rjq6lmvd78d6aot /weird_random_forest.RDS?dl=0]]></description>
      <guid>https://stackoverflow.com/questions/27324066/weird-results-with-the-randomforest-r-package</guid>
      <pubDate>Fri, 05 Dec 2014 20:21:16 GMT</pubDate>
    </item>
    </channel>
</rss>