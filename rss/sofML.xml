<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Sat, 29 Mar 2025 18:22:03 GMT</lastBuildDate>
    <item>
      <title>如何在增强学习中量化估计偏差？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79543440/how-is-estimation-bias-quantified-in-reinforcement-learning</link>
      <description><![CDATA[在各种估计问题中，尤其是在RL域中，我们目前正在研究Q学习及其变体，我们经常遇到术语估计偏差，这是指估计器的预期值与真实参数的系统偏差。&gt; 。
例如，Thrun（1993）[1]提到估计器具有估计偏差，但我正在寻找一种量化它的标准方法。我知道偏见通常被定义为：
 偏见（θ̂）= e [θ̂]  - θ
 
其中 θ̂ 是估计器，θ是true参数。
但是，在实际应用中，当我们只有一个数据样本时，用于量级或量化的标准技术是什么？在机器学习，统计或计量经济学中，是否有特定的数值或计算方法在现实世界中估算它？
我的观点：
我目前正在进行有关量化 Q-学习和双Q学习算法的偏差的研究。强化学习的关键挑战之一是了解估计偏置如何在价值函数更新中传播。引入了双Q学习，以减轻标准Q学习中存在的高估偏差，但准确地测量这种偏见仍然是一个开放的问题。
根据我观察到的，大多数研究通过经验绩效评估而不是通过直接定量来分析偏见。某些技术，例如使用自举置信区间或蒙特卡洛推广，试图通过将学习的价值功能与地面真相回报进行比较来估计偏差。但是，是否有一种更标准化的方法来量化和比较不同学习算法的偏差？
此外，我在最近的神经论文[2]中遇到了 amse的概念（渐近平方误差）。 AMSE由：给出
  amse（θ̂_n）= e [（θ̂_n -θ）²]
 
其中 n 是样本尺寸。但是，本文采用了零引用方法，这意味着该引用设置为零，而不是假设已知的真实值θ，并且对此进行了所有错误测量。这实际上意味着：
  amse（θ̂_n）= e [θ̂_n²]
 
所有偏差估计是相对而不是绝对的。
这种零引用方法如何影响强化学习中估计偏差的解释？ AMSE是否可以成为量化Q学习估计量中偏差的合适度量，还是有其他方法更适合强化学习应用？
 RL中的任何参考文献或示例，尤其是在Q学习和双重学习中，都将不胜感激。 

参考：
 [1] Thrun，S。（1993）。 偏见和学习算法中稳定性的量化。卡内基·梅隆大学。 
 [2] Bartlett，P。L.，Long，P.M.，Lugosi，G。，＆amp; Tsigler，A。（2020）。线性回归中的良性过度拟合。神经。 &lt;a href =“ https://proceedings.neurips.cc/paper_files/paper/2020/file/4bfbd52f4e8466dc12aaf30b7e057b6666-paper.pdf.pdf”]]></description>
      <guid>https://stackoverflow.com/questions/79543440/how-is-estimation-bias-quantified-in-reinforcement-learning</guid>
      <pubDate>Sat, 29 Mar 2025 15:12:34 GMT</pubDate>
    </item>
    <item>
      <title>使用Sklearn的SeceentialFeaturesElector，Local（Ubuntu VM）和Databrick之间的不同特征选择结果</title>
      <link>https://stackoverflow.com/questions/79541286/different-feature-selection-results-between-local-ubuntu-vm-and-databricks-usi</link>
      <description><![CDATA[我正在使用VM上的Ubuntu在Databricks上使用Ubuntu在VS代码中运行我的机器学习管道。当我使用相同的代码测试相同的数据集时，我从SeecentionFeaturesElector获得了不同的选定功能，这会导致不同的最终模型输出。
调试，我尝试了以下内容：

圆形的X和Y到4个小数点，以检查是否有少量阅读差异。
设置全局种子（np.random.seed（seed），andand.seed（seed））以控制随机性。
明确设置Random_State = sequentialFeaturesElector中的kfold中的种子。
单独运行RIDGECV（没有特征选择，没有标准标准器（）），并确认它在这两台机器上都会产生相同的结果。
确保Python和所有库的版本相同。

 观察：

当我只运行RidgeCV时，我在这两台机器上都会获得相同的结果。
当我运行sequentialFeaturesElector时，它会在本地与数据括号上选择不同的功能集，从而导致不同的模型输出。
我怀疑我尚未考虑SFS或交叉验证中可能存在一个随机性问题。

尽管使用了相同的数据和种子，但SequentialFeaturesElector为什么在本地和数据映中会产生不同的结果？
 ＃设置全局种子
种子= 42
np.random.seed（种子）
随机种子（种子）

＃山脊回归模型
ridge_model = ridgecv（
    alpha = np.logspace（-10，2，200），＃alpha =正则化强度
    fit_intercept = true，  
    store_cv_values = false）

＃型号管道：标准化 +脊回归
model_pipeline = make_pipeline（standardscaler（），ridge_model）

＃顺序特征选择（SFS）
sfs = SeceentialFeaturesElector（
    model_pipeline，
    n_features_to_select =&#39;auto&#39;，
    方向=&#39;向前&#39;，
    评分=&#39;r2&#39;，
    cv = kfold（n_splits = 2，Random_state = seed，shuffle = true））

＃适合SFS
sfs.fit（x，y）

＃获得选定的功能
predictors = sfs.get_feature_names_out（）。tolist（）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79541286/different-feature-selection-results-between-local-ubuntu-vm-and-databricks-usi</guid>
      <pubDate>Fri, 28 Mar 2025 11:22:54 GMT</pubDate>
    </item>
    <item>
      <title>用不同的二进制滤波器培训Tesseract会影响ENG.TRAINDATA吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79539916/would-training-tesseract-with-different-binarization-filters-affect-eng-trainedd</link>
      <description><![CDATA[我正在与Tesseract OCR合作，并想尝试不同的二进制方法，例如OTSU的阈值和其他自定义过滤器，以提高文本识别精度。
但是，我担心使用这些不同的预处理技术培训可能会修改或覆盖Eng.trainedData，我想保持完整。
我的问题是：

 培训新模型是否会影响现有的Eng.trainedData文件？

 如何在不修改默认英语模型的情况下安全地使用新过滤器训练Tesseract？

 是否有推荐的方法在保持启动时训练tesseract在预处理图像上。


 我尝试的是： 
用三个样本更新了我当前的eng_new.traineddata，每个样本都应用了过滤器otsu，ottu_tresh_binary，otsu_tresh_binary_inv
在第一次1000次迭代之后，我在初始训练和目标培训之间有所区别。DATA
但受到训练的目标。达塔的结果稍差。
  lstMtraining -continue_from/home/j/tribnedcurrenteng/data/checkpoints/eng_training -trainedData/home/j/trainingcurrenteng/data/data/data/eng.traineddata-train_train_train_listfile/tristfile/home/home/home/j/j/j/j/trainingcurrentengeng gultial_listefile_list_list_listfile-ev /home/j/trainingcurrenteng/data/list.eval--model_output/home/j/trainingcurrenteng/data/checkpoints/eng_training -learning_rate_rate 0.0001 -debug_interval 10 -m -max_interations 600
 
  tesseract otsu_tresh_binary_inv.tiff output_text -l eng -tessdata -dir/home/j/j/triendingcurrenteng/data -psm 7
 
  cat output_text.txt

ABCD123
 
  tesseract otsu_tresh_binary_inv.tiff output_text_1 -l eng_trained -tessdata -dir/home/home/j/j/triendingcurrenteng/data -psm 7
 
  cat output_text_1.txt
ABC
 
如何在不干扰现有模型的情况下训练自定义模型？]]></description>
      <guid>https://stackoverflow.com/questions/79539916/would-training-tesseract-with-different-binarization-filters-affect-eng-trainedd</guid>
      <pubDate>Thu, 27 Mar 2025 19:57:58 GMT</pubDate>
    </item>
    <item>
      <title>使用分层k折后代码：我们需要指定哪个折叠？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79539911/code-after-using-stratified-k-fold-do-we-need-to-specify-which-fold</link>
      <description><![CDATA[我一直在使用简单的技术来拆分数据集。但是，我很想通过更先进的技术进行进步。例如分层的k倍数据不平衡数据。但是，一旦将数据拆分，例如我提供的代码。我们如何继续进行预处理阶段？
示例：我们无法使用整个数据集估算NAN值。我们如何编码？
 ＃假设sem_df是您的数据框，“目标”是目标列
x = sem_df.drop（&#39;target&#39;，轴= 1）
y = sem_df [&#39;target&#39;]

n_total = len（sem_df）

＃1。将数据分为80％的培训和20％的临时培训（将分为验证和测试）
x_train，x_test，y_train，y_test = train_test_split（
    x，y， 
    test_size = 0.20， 
    分层= y， 
    Random_State = 42
）

＃印刷形状具有百分比信息
print（f＆quot; train设置形状：{x_train.shape}（{x_train.shape [0] / n_total * 100：.1f}％）＆quot&#39;）
打印（f＆quot;测试集形状：{x_test.shape}（{x_test.shape [0] / n_total * 100：.1f}％）＆quot; quot&#39;）
 
这是插补问题：
 ＃注意：火车和测试的不同值
train_num_medians = x_train [num_cols] .median（）
train_cat_modes = x_train [cat_cols] .mode（）。iLoc [0]

＃注意：火车和测试的不同值
test_num_medians = x_test [num_cols] .median（）
test_cat_modes = x_test [cat_cols] .mode（）。iLoc [0]

＃使用.loc且不在内地的训练集中估算丢失值
x_train.loc [：，num_cols] = x_train.loc [：，num_cols] .fillna（train_num_medians）
x_train.loc [：，cat_cols] = x_train.loc [：，cat_cols] .fillna（train_cat_modes）

＃使用训练值值将测试集中的缺失值算
x_test.loc [：，num_cols] = x_test.loc [：，num_cols] .fillna（test_num_medians）
x_test.loc [：，cat_cols] = x_test.loc [：，cat_cols] .fillna（test_cat_modes）
 
，甚至对于标准化部分：
 ＃现在安全执行缩放：
sualer = StandardScaler（）
x_train [norm_col] = scaler.fit_transform（x_train [norm_col]）
＃x_val.loc [：，norm_col] = scaler.transform（x_val [to_norm]）
x_test [norm_col] = scaler.transform（x_test [norm_col]）
 
执行与交叉验证的数据拆分时，我们必须更改经典步骤吗？
火车 - ＆gt; fit_transform 
测试 - ＆GT;变换]]></description>
      <guid>https://stackoverflow.com/questions/79539911/code-after-using-stratified-k-fold-do-we-need-to-specify-which-fold</guid>
      <pubDate>Thu, 27 Mar 2025 19:56:19 GMT</pubDate>
    </item>
    <item>
      <title>在两个nn.modulelist上使用zip（）</title>
      <link>https://stackoverflow.com/questions/79536891/using-zip-on-two-nn-modulelist</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79536891/using-zip-on-two-nn-modulelist</guid>
      <pubDate>Wed, 26 Mar 2025 18:02:29 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用机器学习来计算此图片中的发芽（绿色）和非发芽种子（红色）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79535638/how-can-i-use-machine-learning-to-count-germinated-green-and-non-germinated-se</link>
      <description><![CDATA[这张照片中如何使用机器学习来计算发芽（绿色）和非发芽种子（红色）？ 在此处输入图像描述 
我以前尝试过使用imagej，但是辐射的颜色与背景太相似，没有合适的阈值来区分它们]]></description>
      <guid>https://stackoverflow.com/questions/79535638/how-can-i-use-machine-learning-to-count-germinated-green-and-non-germinated-se</guid>
      <pubDate>Wed, 26 Mar 2025 07:00:07 GMT</pubDate>
    </item>
    <item>
      <title>通过使用Densenet169结构进行结肠癌预测的模型进行微调模型[闭幕]</title>
      <link>https://stackoverflow.com/questions/79535469/fine-tuning-a-model-utilizing-densenet169-architecture-for-colon-cancer-predicti</link>
      <description><![CDATA[我使用Python在10,000张图像的数据集上使用Densenet169创建了一个模型，以预测结肠癌，两个子文件夹有5000张图像，每张图像5000张图像用于良性和癌组织。训练持续了8小时，训练后的F1分数为0.75，这是不可取的。我试图查看我可以做出的更改以改善指标，但是我没有任何更改的方法，并且需要帮助确定可以在哪里进行更改以提高F1分数。这是我第一次从事这样的事情，我发现没有在线材料进行深度学习可以帮助我解决这个问题
 将TensorFlow导入为TF
来自Tensorflow.keras.applications导入Densenet169
来自tensorflow.keras.layers导入密集，globalaveration -pooling2d，Randomflip，RandomRotation，辍学
来自Tensorflow.keras.models导入模型，顺序
来自Tensorflow.keras.optimizer导入Adam
从tensorflow.keras.regulinizer导入l2
来自tensorflow.keras.applications.densenet导入preprocess_input
来自sklearn.metrics导入混乱_matrix，f1_score
导入numpy作为NP
进口海洋作为SNS
导入matplotlib.pyplot作为PLT
导入操作系统

＃定义常数
img_size =（224，224）＃匹配densenet169输入大小
batch_size = 32
时代= 15
data_dir =＆quot; colon_images＆quot;
class_names = [＆quast; cancyous&#39;&#39;正常＆quot;]
split_ratio = 0.2

＃直接加载数据集在224x224
def create_dataset（子集）：
    返回tf.keras.utils.image_dataset_from_directory（
        data_dir，
        验证_split = split_ratio，
        子集=子集
        种子= 42，
        image_size = img_size，＃直接以目标大小加载
        batch_size = batch_size，
        label_mode =&#39;binary&#39;
    ）

train_ds = create_dataset（“训练”）
val_ds = create_dataset（&#39;验证＆quot;）

＃通过增强进行预处理（仅在培训期间活跃）
预处理=顺序（[
    Randomflip（“水平”，“），
    随机旋转（0.1），
    tf.keras.layers.lambda（preprocess_input）＃正确归一化
）））

＃构建模型
base_model = densenet169（
    权重=&#39;Imagenet&#39;，
    include_top = false，
    input_shape = img_size +（3，）
）

输入= tf.keras.input（shape = img_size +（3，））
X =预处理（输入）
x = base_model（x）
x = globalaveragepooling2d（）（x）
x =密集（512，激活=&#39;relu&#39;，kernel_regularizer = l2（0.01））（x）
x =辍学（0.5）（x）＃正则化
输出=密集（1，激活=&#39;Sigmoid&#39;）（x）
模型=模型（输入，输出）

＃阶段1：火车顶层
base_model.trainable = false
model.compile（
    优化器= ADAM（1E-3），
    损失=&#39;binary_crossentropy&#39;，
    量表= [&#39;fecycy&#39;，tf.keras.metrics.auc（name =&#39;auc&#39;），
             tf.keras.metrics.precision（name =&#39;precision&#39;），
             tf.keras.metrics.Recall（name =&#39;recember&#39;）]
）

＃用回调监视AUC的火车
早期_stop = tf.keras.callbacks.earlystopping（
    Monitor =&#39;Val_auc&#39;，耐心= 3，模式=&#39;max&#39;，详细= 1
）
检查点= tf.keras.callbacks.modelcheckpoint（
    &#39;best_model.h5&#39;，save_best_only = true，monitor =&#39;val_auc&#39;，mode =&#39;max&#39;
）

历史= model.fit（
    train_ds，
    验证_data = val_ds，
    时代= epochs，
    回调= [早期_STOP，检查点]
）

＃阶段2：微调整个模型
base_model.trainable = true
model.compile（
    优化器= ADAM（1E-5），＃非常低的学习率
    损失=&#39;binary_crossentropy&#39;，
    量表= [&#39;facer&#39;，tf.keras.metrics.auc（name =&#39;auc&#39;）]
）

history_fine = model.fit（fit）（
    train_ds，
    验证_data = val_ds，
    时代= epochs，
    onirome_epoch = history.epoch [-1]，
    回调= [早期_STOP，检查点]
）

＃ 评估
y_pred =（model.predict（val_ds）＆gt; 0.5）.astype（int）
y_true = np.concatenate（[y for _，y in val_ds]，axis = 0）

打印（f＆quot f1分数：{f1_score（y_true，y_pred）：。3f}＆quot;）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79535469/fine-tuning-a-model-utilizing-densenet169-architecture-for-colon-cancer-predicti</guid>
      <pubDate>Wed, 26 Mar 2025 05:27:58 GMT</pubDate>
    </item>
    <item>
      <title>在Pytorch中使用大于1的批次尺寸时的错误</title>
      <link>https://stackoverflow.com/questions/79519426/error-when-using-batch-size-greater-than-1-in-pytorch</link>
      <description><![CDATA[我正在构建一个神经网络，以预测使用VVC（多功能视频编码）在压缩过程中如何对图像进行分区。该模型从YUV420图像中获取单个Y框架作为输入，并使用包含地面真相块位置和尺寸的CSV文件进行训练。
输入和地面真相

  输入： 1-Frame YUV420 10位图像。

  地面真相：具有块位置，大小和其他分区标志的CSV文件。


示例（388016_320x480_37.yuv）
在此处输入图像描述 
示例（388016_320x480_37.csv）
在这里输入图像说明 
问题描述：
我实现了 train.py 和 dataset.py ，但是当设置 batch_size＆gt; 1 在 dataloader 中。批量大小为1，该模型正常工作，但是增加批处理大小会导致运行时错误。
代码摘要：
以下是我的 custom_collat​​e_fn 和 dataloader 设置的简化版本：
  def custom_collat​​e_fn（批次）：
    帧= [批次中的项目]＃y帧张量
    块= [批次中的项目]＃块信息
    框架= torch.stack（帧，dim = 0）＃沿批处理尺寸堆叠帧
    返回框架，块

dataloader = dataloader（
    数据集，
    batch_size = batch_size， 
    shuffle = true，
    Collat​​e_fn = custom_collat​​e_fn
）
 
观察：

 当 batch_size = 1 时， blocks_batch 在训练环中是包含一组块数据的列表。

 带有 batch_size＆gt; 1 ，它成为列表的列表，在索引时会导致错误。


  i，（frame，blocks_batch）枚举（dataLoader）：
    frame = frame.to（设备）＃形状：[batch_size，1，h，w]
    blocks = blocks_batch [0]＃与batch_size = 1一起使用，但大小较大
 
我的假设：
似乎是由处理 blocks_batch 当 batch_size＆gt; 1 。嵌套列表结构使得很难处理多批次。
问题：

 如何调整 custom_collat​​e_fn 或训练循环以有效处理大于1的批次大小？

 如果有更好的方法可以通过批处理处理可变的块数据，我将感谢任何建议。


  file＆quod＆quot＆quode＆users \ indersration \ documents \ documents \ vvc_fast \ test4 \ test4 \ train.py＆quid＆quort＆quort＆quort＆quort＆quort＆quid＆quot 91 in＆lt; module＆gt;
    loss1 =标准（out_split，target_split）
  file＆quot; c：\ programData \ miniconda3 \ envs \ enkeNv3 \ lib \ lib \ site-packages \ torch \ thnn \ nn \ modules \ module.py;
    返回self._call_impl（*args，** kwargs）
  file＆quot; c：\ programData \ miniconda3 \ envs \ newenv3 \ lib \ lib \ site-packages \ torch \ thnn \ nn \ modules \ module.py;
    返回forward_call（*args，** kwargs）
  file＆quot; c：\ programdata \ miniconda3 \ envs \ newenv3 \ lib \ lib \ site-packages \ torch \ nn \ nn \ loss.py.py＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort＆quort in 725
    返回f.binary_cross_entropy_with_logits（输入，目标，
  file＆quot c：\ programData \ miniconda3 \ envs \ newenv3 \ lib \ lib \ site-packages \ torch \ nn \ functional.py;
    提高ValueError（f＆quot“目标大小（{target.size（）}）必须与输入大小（{input.size（）}）相同
ValueError：目标大小（TORCH.SIZE（[1，1]））必须与输入大小（Torch.Size（[2，1]））相同
 ]]></description>
      <guid>https://stackoverflow.com/questions/79519426/error-when-using-batch-size-greater-than-1-in-pytorch</guid>
      <pubDate>Wed, 19 Mar 2025 07:28:29 GMT</pubDate>
    </item>
    <item>
      <title>特征的长度不等于形状值的长度</title>
      <link>https://stackoverflow.com/questions/79515542/length-of-features-is-not-equal-to-the-length-of-shap-values</link>
      <description><![CDATA[我正在运行一个随机的森林模型，并获得某些特征的重要性，并试图运行形状分析。问题是，每当我尝试绘制塑形值时，我都会遇到此错误：
  dimensionError：功能的长度不等于shap_values的长度。 
 
我不知道发生了什么。当我运行XGBoost模型时，一切似乎都很好，我可以看到数据集的形状图。它的数据集完全相同，但它不会与随机森林一起运行。它用于二进制分类。
这是我的python代码：
 来自sklearn.semble import incort fandyForestClassifier
来自sklearn.model_selection导入train_test_split，cross_val_score
来自sklearn.metrics导入精度，precision_score，recke_score，f1_score，confusion_matrix

＃从功能中删除主键列“ ID”

功能= result.drop（columns = [&#39;pq2&#39;，&#39;id&#39;]）＃删除目标和ID列
target =结果[&#39;pq2&#39;]＃目标变量

＃将数据分为80-20的培训和测试集
x_train，x_test，y_train，y_test = train_test_split（功能，target，test_size = 0.2，andural_state = 42）
 
＃初始化随机森林分类器
rf_model = RandomforestClassifier（N_Estimators = 100，Random_State = 42）

＃将模型适合培训数据
rf_model.fit（x_train，y_train）

＃做出预测
y_pred = rf_model.predict（x_test）

导入塑造

＃为随机森林模型创建树状解释器
解释器= shap.treeexplainer（rf_model）

＃计算测试集的形状值
shap_values = ruminder.shap_values（x_test）

＃绘制形状摘要图
shap.summary_plot（shap_values，x_test，feature_names = features_names）

＃绘制整体特征重要性的塑形栏图

shap.summary_plot（shap_values，x_test，feature_names = features_names，plot_type =; bar＆quot; quot;
 
测试集的形状为（829,22），但是随机森林的外形值始终返回（22,2），我不知道如何修复它。数据集已经进行了预处理，列是0-1S或数值列。]]></description>
      <guid>https://stackoverflow.com/questions/79515542/length-of-features-is-not-equal-to-the-length-of-shap-values</guid>
      <pubDate>Mon, 17 Mar 2025 19:16:45 GMT</pubDate>
    </item>
    <item>
      <title>保存到磁盘中的磁盘[封闭]时会遇到Unicode错误</title>
      <link>https://stackoverflow.com/questions/79436672/getting-unicode-error-while-saving-to-disk-in-distiset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79436672/getting-unicode-error-while-saving-to-disk-in-distiset</guid>
      <pubDate>Thu, 13 Feb 2025 15:04:10 GMT</pubDate>
    </item>
    <item>
      <title>有关培训物理知情神经网络的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79415961/issue-regarding-training-physics-informed-neural-network</link>
      <description><![CDATA[我正在通过波传播数据培训物理知情的神经网络。训练输入的形状为（193524369，3）。我目前使用的批量尺寸为2048和Epoch 300。对于培训模型，我总共给了20,000批。因此，基本上，要训练模型，我使用的是20,000批次，每个批次都有2048个数据，几乎是我整个数据集的10％。该代码正在Tesla V100-PCIE-32GB GPU上运行。
训练进度非常慢，训练模型可能需要几个月的时间。但是，NN并不那么沉重，而有那么多节点和层。我该如何使培训进度更快？
我对更快地进行培训过程有些疲惫。使培训批量更大可以使其变得更好一些，但这会导致准确性降低。我应该如何优化以使训练过程更快并保持准确性更好？]]></description>
      <guid>https://stackoverflow.com/questions/79415961/issue-regarding-training-physics-informed-neural-network</guid>
      <pubDate>Wed, 05 Feb 2025 19:21:17 GMT</pubDate>
    </item>
    <item>
      <title>Importerror：无法从“火炬”（未知位置）导入“张量”的名称</title>
      <link>https://stackoverflow.com/questions/79367182/importerror-cannot-import-name-tensor-from-torch-unknown-location</link>
      <description><![CDATA[我正在尝试从Pytorch导入 ：
 来自火炬导入张量
 
但我一直遇到此错误：
  Importerror：无法从“火炬”（未知位置）导入名称&#39;tensor&#39;
 
我尝试的是：

检查是否已安装了Pytorch（ pip show torch ），我正在使用版本 2.5.1 。。
重新安装了Pytorch：
  pip卸载火炬
PIP安装火炬
 

测试了python壳中的导入，但错误仍然存​​在。

环境：

 Python版本：3.10 
 Pytorch版本：2.5.1 
 OS：Windows 10 
虚拟环境：是

我该如何解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/79367182/importerror-cannot-import-name-tensor-from-torch-unknown-location</guid>
      <pubDate>Sat, 18 Jan 2025 13:04:35 GMT</pubDate>
    </item>
    <item>
      <title>如何使用OpenAI的API（GPT）处理PDF？</title>
      <link>https://stackoverflow.com/questions/77469097/how-can-i-process-a-pdf-using-openais-apis-gpts</link>
      <description><![CDATA[ CHATGPT的Web界面具有简单的PDF上传。 是否有Openai的API可以接收PDF？ 
我知道有第三方库可以读取PDF，但是鉴于PDF中有图像和其他重要信息，如果像GPT 4 Turbo这样的模型直接喂食实际PDF可能会更好。。
我将说明我的用例以添加更多上下文。我打算做抹布。在下面的代码中，我处理PDF和提示。通常，我会在提示结束时附加文本。如果我手动提取其内容，我仍然可以用PDF做到这一点。
以下代码从此处获取 https://platform.openai.com/doces.com/docs/assistans/assistans/tools/tools/tools/code-interterpreter-这是我应该做的吗？
 ＃＃用“助手”上传文件。目的
file = client.files.files.create（
  file = open（示例。
  目的=&#39;助手&#39;
）

＃使用文件ID创建助手
助理= client.beta.assistants.greate（
  说明=“您是个人数学老师。当被问到一个数学问题时，编写并运行代码以回答问题。
  Model =＆quort; gpt-4-1106-Preview＆quort; quot;
  工具= [{{&#39;type;：＆quod_interpreter;}]，
  file_ids = [file.id]
）
 
也有一个上传端点，但是这些端点的目的似乎是针对微调和助手的。我认为抹布用例是普通的，不一定与助手有关。]]></description>
      <guid>https://stackoverflow.com/questions/77469097/how-can-i-process-a-pdf-using-openais-apis-gpts</guid>
      <pubDate>Sun, 12 Nov 2023 13:25:44 GMT</pubDate>
    </item>
    <item>
      <title>总和和平均值在.backward（）计算损失和通过网络反向传播时的差异</title>
      <link>https://stackoverflow.com/questions/72429838/difference-between-sum-and-mean-in-backward-in-calculating-the-loss-and-backp</link>
      <description><![CDATA[我知道我们应该在向后应用之前将张量转换为标量（），但是何时和何时表示？
  some_loss_function.sum（）。backward（）
-或者-
some_loss_function.mean（）。backward（）
 ]]></description>
      <guid>https://stackoverflow.com/questions/72429838/difference-between-sum-and-mean-in-backward-in-calculating-the-loss-and-backp</guid>
      <pubDate>Mon, 30 May 2022 06:13:42 GMT</pubDate>
    </item>
    <item>
      <title>如何将稀疏的numpy数组转换为数据框架？</title>
      <link>https://stackoverflow.com/questions/64746124/how-to-convert-sparse-numpy-array-to-dataframe</link>
      <description><![CDATA[下面是代码段，
 来自sklearn.com pos import columntransformer
从sklearn.preprocessing导入onehotencoder
ct = columnTransFormer（变形金刚= [（&#39;encoder&#39;，onehotencoder（），[2,3,4]）]，剩余=&#39;PassThrough&#39;）
x = np.array（ct.fit_transform（x_data））
X.Shape
 
我得到下面的输出
 （）
 
当我尝试打印X时，我会像以下那样获得输出
  array（＆lt; 8820x35类型&#39;＆lt; class&#39;numpy.float64&#39;&#39;＆gt;&#39;&#39;的稀疏矩阵
    在压缩稀疏行格式中存储了41527个元素，dtype = object）
 
现在，当我尝试将此数组转换为DataFrame 时
  x = pd.dataframe（x）
 
我在以下错误以下
  valueerror：必须通过2-D输入
 
如何将我的numpy数组转换为dataframe？]]></description>
      <guid>https://stackoverflow.com/questions/64746124/how-to-convert-sparse-numpy-array-to-dataframe</guid>
      <pubDate>Mon, 09 Nov 2020 05:03:36 GMT</pubDate>
    </item>
    </channel>
</rss>