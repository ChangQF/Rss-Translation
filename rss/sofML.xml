<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 30 Nov 2023 03:15:10 GMT</lastBuildDate>
    <item>
      <title>修改来自google的ml教程代码没有给出预期的结果</title>
      <link>https://stackoverflow.com/questions/77575529/modifying-ml-tutorial-code-from-google-does-not-give-expected-result</link>
      <description><![CDATA[有一个很好的使用tensorflow lib的ml python代码的迷你示例。
Google 代码实验室教程
它（正确地）从线性方程预测一个数字。但仅仅制作一个小模型来训练模型并预测二次函数就会得到完全错误的结果。
导入tensorflow为tf
将 numpy 导入为 np
从张量流导入keras

模型 = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer=&#39;sgd&#39;, loss=&#39;mean_squared_error&#39;)

# 从原始教程修改 -&gt; y = 2x^2-1
xs = np.array([-3.0, -2.0, -1.0, 0.0, 2.0, 3.0, 4.0, 5.0], dtype=float)
ys = np.array([ 17.0, 7.0, 1.0, -1.0, 7.0, 17.0, 31.0, 49.0], dtype=float)

model.fit(xs, ys, epochs=5000)

打印（模型.预测（[1.0]））

给出结果：
&lt;前&gt;&lt;代码&gt;&gt;&gt;&gt;打印（模型.预测（[1.0]））
1/1 [================================] - 0s 84ms/步
[[15.999977]]
&gt;&gt;&gt;&gt;&gt;

我本来预计大约。 1.0。
不知道出了什么问题。]]></description>
      <guid>https://stackoverflow.com/questions/77575529/modifying-ml-tutorial-code-from-google-does-not-give-expected-result</guid>
      <pubDate>Thu, 30 Nov 2023 02:28:17 GMT</pubDate>
    </item>
    <item>
      <title>OpenAi 从我的应用程序中检索数据</title>
      <link>https://stackoverflow.com/questions/77575498/openai-to-retive-data-from-my-application</link>
      <description><![CDATA[我管理一个包含数千个商机、客户、联系人等的 CRM 应用程序。我正在寻求实现类似聊天的功能，允许用户提出问题并从存储的记录中检索数据。例如，他们可以查询价值超过 10,000 美元的机会。
最初，我探索使用 NLP to SQL 方法。我向 OpenAI 提供了我的表结构和用户提示，执行生成的 SQL 查询产生了准确的结果。然而，正如在各种实例中所观察到的那样，仅仅依靠 OpenAI 生成 SQL 会带来安全风险。
我正在探索替代方法。一种想法是为特定任务创建专用 API，然后使用 OpenAI 对其进行训练。这看起来是一个可行的解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/77575498/openai-to-retive-data-from-my-application</guid>
      <pubDate>Thu, 30 Nov 2023 02:19:04 GMT</pubDate>
    </item>
    <item>
      <title>遇到影响 GPT 代码存储库的问题</title>
      <link>https://stackoverflow.com/questions/77574375/running-into-issues-with-affect-gpts-code-repo</link>
      <description><![CDATA[我本质上正在阅读这篇论文，并想运行他们的代码，但目前无法，
有人可以向我提供运行其代码所需的看似简短的 google collab 脚本吗？
https://github.com/zeroQiaoba/AffectGPT
我尝试运行它，但遇到了一些问题，例如 app.py 未运行]]></description>
      <guid>https://stackoverflow.com/questions/77574375/running-into-issues-with-affect-gpts-code-repo</guid>
      <pubDate>Wed, 29 Nov 2023 20:42:13 GMT</pubDate>
    </item>
    <item>
      <title>过采样或欠采样方法在实时数据测试过程中有何帮助？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77573960/how-does-oversampling-or-undersampling-approch-is-going-to-help-during-the-testi</link>
      <description><![CDATA[我们有一个数据集，其中 A 类仅占 10%，B 类占 90%。假设我们对训练数据进行了欠采样或过采样，我们对 A 类进行了 50%，对 B 类进行了 50%。但实际上，数据分布是倾斜的。因为如果我们进行欠采样或过采样，那么我们的模型将更加重视少数群体数据。
那么过采样或欠采样在实时数据测试过程中有何帮助？]]></description>
      <guid>https://stackoverflow.com/questions/77573960/how-does-oversampling-or-undersampling-approch-is-going-to-help-during-the-testi</guid>
      <pubDate>Wed, 29 Nov 2023 19:16:16 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用哪种机器学习模型来检测奶酪中的孔洞数量？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77573916/which-ml-model-should-i-use-for-detecting-number-of-holes-in-cheese</link>
      <description><![CDATA[我有一堆奶酪图像，我需要一种算法来计算奶酪上有多少个洞。我过去做过一些愿景项目，但这肯定不是我的强项。谁能给我一些入门建议？这可能是一种监督学习，因为我应该事先知道孔的数量，但为了以防万一，请随意建议一种无监督方法。]]></description>
      <guid>https://stackoverflow.com/questions/77573916/which-ml-model-should-i-use-for-detecting-number-of-holes-in-cheese</guid>
      <pubDate>Wed, 29 Nov 2023 19:09:09 GMT</pubDate>
    </item>
    <item>
      <title>如何在 SKLearn Estimator 上使用 Sagemaker HyperparameterTuner？</title>
      <link>https://stackoverflow.com/questions/77573670/how-do-i-use-sagemaker-hyperparametertuner-on-a-sklearn-estimator</link>
      <description><![CDATA[我正在关注 Amazon Sagemaker 研讨会尝试利用 Sagemaker 的多个实用程序，而不是像我目前所做的那样在笔记本上运行所有内容。
问题是，在研讨会上，他们教您如何使用来自 AWS 的现成 XGBoost 图像来使用 HyperparameterTuner，而我的大多数管道都使用 Scikit-Learn 模型，例如 GradientBoostingClassifier 或 RandomForest，因此我实例化了一个估计器如下此示例文件：
sklearn = SKLearn(entry_point=&quot;train.py&quot;,
                  Framework_version =“1.2-1”，
                  instance_type=“ml.m5.xlarge”，
                  角色=角色，
                  超参数=fixed_hyperparameters
）

之后，我使用刚刚创建的估计器实例化一个 HyperparameterTuner 作业，其中包含我想要测试的超参数范围。
hyperparameters_ranges = {
    “n_estimators”: ContinuousParameter(100, 500),
    “学习率”：连续参数（1e-2，1e-1），
    “最大深度”：IntegerParameter(2, 5),
    “子样本”：连续参数（0.6，1），
    “max_df”：连续参数（0.4，1），
    “max_features”：IntegerParameter(5, 25),
    “use_idf”：CategoricalParameter([True, False])
}

度量=“验证：f1”

调谐器 = 超参数调谐器(
    sklearn,
    公制，
    超参数范围，
    最大作业数=2,
    最大并行作业数=2
）

我的问题是，我没有找到任何有关如何访问“train.py”内部 SKLearn 估计器中传递的超参数的信息。文件。我也没有找到最佳超参数存储在哪里，因此我可以将它们用于最终模型。有人可以告诉我这是否可能吗？或者如果有另一种更简单的方法可以提供替代方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/77573670/how-do-i-use-sagemaker-hyperparametertuner-on-a-sklearn-estimator</guid>
      <pubDate>Wed, 29 Nov 2023 18:27:13 GMT</pubDate>
    </item>
    <item>
      <title>应该支持数组（不仅仅是类似列表）输入的评分规则的分解，但会给出错误</title>
      <link>https://stackoverflow.com/questions/77573489/decomposition-of-scoring-rule-for-array-not-just-list-like-inputs-should-be-su</link>
      <description><![CDATA[我已经获得了 model_diagnostics.scoring 函数 decompose 来处理类似列表的输入，例如文档中的示例。
from model_diagnostics.scoring import SquaredError，分解
将 numpy 导入为 np
将 pandas 导入为 pd
se = 平方误差()
y_true = [0, 0, 0, 1, 1, 1]
y_pred = [0.1, 0.4, 0.3, 0.3, 0.99, 0.9]
分解（y_true，y_pred，评分函数= se）

在特定的上下文中，我们可以将这种分解视为给出与统计中感兴趣的 Brier 分数相关的各种度量。然而，Brier 分数对于比较类别数组（矩阵）和类别概率数组的矩阵（多类或多标签问题）有意义，我也对这种设置中的分解感兴趣。但是，decompose 函数并未按其应有的方式工作。
from model_diagnostics.scoring import SquaredError，分解
将 numpy 导入为 np
将 pandas 导入为 pd
np.随机.种子(2023)
se = 平方误差()
y_true = np.random.multinomial(1, np.ones(3)/3, 10)
y_pred = np.random.dirichlet(np.ones(3), 10)
分解（y_true，y_pred，评分函数= se）
分解（y_true.T，y_pred.T，scoring_function = se）
分解（y_true.T，y_pred，scoring_function = se）
分解（y_true，y_pred.T，scoring_function = se）

使用 decompose 的前两次尝试失败，并出现 具有多个元素的数组的真值不明确。使用 a.any() 或 a.all() 错误消息。 （最后两个也失败了，尽管它们只是我半绝望地尝试让它工作，但我知道会失败。）
但是，decompose 的文档提到输入可以是类似数组的，所以我似乎想做一些文档说支持的事情。是什么赋予了？如何使用数组？]]></description>
      <guid>https://stackoverflow.com/questions/77573489/decomposition-of-scoring-rule-for-array-not-just-list-like-inputs-should-be-su</guid>
      <pubDate>Wed, 29 Nov 2023 17:54:48 GMT</pubDate>
    </item>
    <item>
      <title>机器学习与传统编程有何不同？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77573240/how-does-machine-learning-differ-from-traditional-programming</link>
      <description><![CDATA[机器学习 (ML) 与传统编程的不同之处在于它获取知识和做出决策的方式。
总而言之，传统编程涉及明确指示计算机，而机器学习则侧重于训练算法以从数据中学习并做出决策。机器学习在问题复杂且定义显式规则具有挑战性或不切实际的场景中尤其强大。]]></description>
      <guid>https://stackoverflow.com/questions/77573240/how-does-machine-learning-differ-from-traditional-programming</guid>
      <pubDate>Wed, 29 Nov 2023 17:14:19 GMT</pubDate>
    </item>
    <item>
      <title>从 parquet 读取文件</title>
      <link>https://stackoverflow.com/questions/77572902/reading-file-from-parquet</link>
      <description><![CDATA[我在尝试读取镶木地板文件时遇到了挑战。最初，我怀疑该文件可能已损坏。然而，改变读取方法后，文件被成功处理。这个解决方案花了相当长的时间才确定，主要是因为我在机器学习领域相对缺乏经验。
这些方法有什么区别？为什么第一种方法给我一个损坏的文件？
正在工作
data = pd.read_parquet(&#39;data.parquet&#39;,engine=&#39;fastparquet&#39;)

无法正常工作，文件损坏并抛出异常 utf-8，我们尝试在记事本中修复
data = pq.read_table(&#39;data.parquet&#39;)
df = data.to_pandas()
]]></description>
      <guid>https://stackoverflow.com/questions/77572902/reading-file-from-parquet</guid>
      <pubDate>Wed, 29 Nov 2023 16:24:05 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中用 Transformer 替代 LSTM</title>
      <link>https://stackoverflow.com/questions/77570734/replace-lstm-with-transformer-in-neural-network</link>
      <description><![CDATA[我正在尝试在汽车行车记录仪视频中实现事故预期模型（Anticipating-Accidents）。我的目标是用 Transformer 替换原始项目中使用的 LSTM 并比较结果。使用 Tensorflow 2 可行吗？我做了一些研究，但我不确定这种变化会对代码结构和模型逻辑产生多大影响。如果有人有任何建议，我们将非常感谢任何帮助。到目前为止，我刚刚做了一些小的调整，使代码可以与 Tensorflow 2 和 Python 3 一起使用，因为原始代码已经过时了（你可以找到我的分叉存储库 此处）。]]></description>
      <guid>https://stackoverflow.com/questions/77570734/replace-lstm-with-transformer-in-neural-network</guid>
      <pubDate>Wed, 29 Nov 2023 11:26:12 GMT</pubDate>
    </item>
    <item>
      <title>xgboost 中的形状值通过 1000 个样本的值误差</title>
      <link>https://stackoverflow.com/questions/77570420/shap-values-in-xgboost-thorughing-value-error-with-1000-samples</link>
      <description><![CDATA[我使用下面的代码来生成形状图
defplot_shap(
    n_sample：int =无，
    n_features：int =无，
    model_output：字典=无，
    标题：str = 无
）：
    如果 n_sample 不是 None：
        X_sampled = model_output[“X_test”].sample(n_sample,random_state=10)
        打印（X_sampled.shape）
    别的：
        X_sampled = model_output[“X_test”]
        print(&#39;无:&#39;, X_sampled.shape)
    解释器 = shap.TreeExplainer(model_output[“模型”])
    打印（解释器）
    shap_values =explainer.shap_values(X_sampled, check_additivity=False)
    打印（形状值）
    plt.标题（标题）
    shap.summary_plot(
        形状值，
        X_采样，
        最大显示=20
    ）
    返回 shap_values

使用下面的代码调用此函数时会给出值错误
shap_values =plot_shap(
    n_样本=1000，
    模型输出=模型输出，
    标题=模型输出[&#39;模型名称&#39;]
）

&lt;块引用&gt;
ValueError：此重塑错误通常是由于将错误的数据矩阵传递给 SHAP 引起的。请参阅https://github.com/shap/shap/issues/580。&lt; /p&gt;

打印时在病房上打印（shap_values）失败
(1000, 360) for 语句 print(X_sampled.shape)

 for语句打印（解释器）
原始X_test包含13827行和360列。]]></description>
      <guid>https://stackoverflow.com/questions/77570420/shap-values-in-xgboost-thorughing-value-error-with-1000-samples</guid>
      <pubDate>Wed, 29 Nov 2023 10:41:00 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的线性回归实现不起作用？</title>
      <link>https://stackoverflow.com/questions/77569740/why-is-my-implementation-of-linear-regression-not-working</link>
      <description><![CDATA[我正在尝试在 python 中从头开始实现线性回归。
作为参考，以下是我使用过的数学公式：方程
这是我尝试过的：
类线性回归：
    
    def __init__(
    自己，
    特征：np.ndarray[np.float64]，
    目标：np.ndarray[np.float64]，
    ）-&gt;没有任何：
        self.features = np.concatenate((np.ones((features.shape[0], 1)), features), axis=1)
        self.targets = 目标
        self.params = np.random.randn(features.shape[1] + 1)
        self.num_samples = features.shape[0]
        self.num_feats = features.shape[1]
        自我成本 = []
    
    def假设（自我）-&gt; np.ndarray[np.float64]：
        返回 np.dot(self.features, self.params)
    
    def cost_function(self) -&gt;; def cost_function(self) -&gt; np.float64：
        pred_vals = self.hypothesis()
        return (1 / (2 * self.num_samples)) * np.dot((pred_vals - self.targets).T, pred_vals - self.targets)
    
    def update(self, alpha: np.float64) -&gt;;没有任何：
        self.params = self.params - (alpha / self.num_samples) * (self.features.T @ (self.hypothesis() - self.targets))
    
    defgradientDescent(self, alpha: np.float64, 阈值: np.float64, max_iter: int) -&gt;没有任何：
        收敛=假
        计数器 = 0
        未收敛时：
            计数器 += 1
            curr_cost = self.cost_function()
            self.costs.append(curr_cost)
            自我更新（阿尔法）
            new_cost = self.cost_function()
            如果abs(new_cost - curr_cost) &lt;临界点：
                收敛=真
            如果计数器&gt;最大迭代次数：
                收敛=真

我使用了这样的类：
regr = LinearRegression(features=np.linspace(0, 1000, 200, dtype=np.float64).reshape((20, 10)), 目标= np.linspace(0, 200, 20, dtype=np.float64))
regr.gradientDescent(0.1, 1e-3, 1e+3)
regr.cost_function()

但是，我收到以下错误：
RuntimeWarning：标量幂中遇到溢出
  return (1 / (2 * self.num_samples)) * (la.norm(self.hypothesis() - self.targets) ** 4)

RuntimeWarning：标量减法中遇到无效值
  如果abs(new_cost - curr_cost) &lt;临界点：

RuntimeWarning：matmul 中遇到溢出
  self.params = self.params - (alpha / self.num_samples) * (self.features.T @ (self.hypothesis() - self.targets))

究竟出了什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/77569740/why-is-my-implementation-of-linear-regression-not-working</guid>
      <pubDate>Wed, 29 Nov 2023 08:54:58 GMT</pubDate>
    </item>
    <item>
      <title>使用 VGG16 MNIST 数字进行迁移学习</title>
      <link>https://stackoverflow.com/questions/77568420/transfer-learning-using-vgg16-mnist-digits</link>
      <description><![CDATA[我正在尝试对 MNIST 数字执行迁移学习。我有兴趣获取 logits 并将其用于基于梯度的攻击。但由于某种原因，即使我的计算机是启用了 GPU 的 Apple m2max 计算机，内核仍然会死机。我也尝试使用 GPU 进行 colab，但遇到同样的问题。该数据集不太好学，我正在重用 imagenet 权重。我该如何解决这个问题？
类 VGG16TransferLearning(tf.keras.Model)：
  def __init__(自我，基本模型，模型)：
    超级（VGG16TransferLearning，自我）.__init__（）
    #基础模型
    self.base_model = 基本模型

   # 其他层
   self.flatten = tf.keras.layers.Flatten()
   self.dense1 = tf.keras.layers.Dense(512, 激活=&#39;relu&#39;)
   self.dense2 = tf.keras.layers.Dense(512, 激活=&#39;relu&#39;)
   self.dense3 = tf.keras.layers.Dense(10)
   self.layers_list = [self.flatten, self.dense1, self.dense2, self.dense3]
  
  #用其他层实例化基础模型
  self.model = models.Sequential(
    [self.base_model, *self.layers_list]
   ）

def 调用(self, *args, **kwargs):
  激活列表 = []
  输出=参数[0]
  
  对于 self.model.layers 中的图层：
    输出 = 层（输出）
    激活列表.append(out)
  如果 kwargs[&#39;训练&#39;]:
   返回
  别的：
   概率 = tf.nn.softmax(输出)
   返回，问题

这是上面类的实例化：
base_model = VGG16(weights=“imagenet”, include_top=False, input_shape=x_train[0].shape)

base_model.trainable = False
我的输入形状是(75,75,3)
这是编译和拟合方法
从tensorflow.keras导入层、模型

模型 = VGG16TransferLearning(base_model, 模型)
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
          优化器=tf.keras.optimizers.legacy.Adam(),
          指标=[&#39;准确性&#39;])

model.fit(x_train, y_train, epochs=10,validation_data=(x_test, y_test))

这是我每次调用 fit 方法时遇到的错误：
内核重启
Untitled.ipynb 的内核似乎已经死亡。它将自动重新启动
]]></description>
      <guid>https://stackoverflow.com/questions/77568420/transfer-learning-using-vgg16-mnist-digits</guid>
      <pubDate>Wed, 29 Nov 2023 03:29:36 GMT</pubDate>
    </item>
    <item>
      <title>Google AI Platform 训练 - 等待作业完成</title>
      <link>https://stackoverflow.com/questions/64806003/google-ai-platform-training-wait-for-the-job-to-finish</link>
      <description><![CDATA[我构建了一个包含大量并行进程的 AI Platform 管道。每个进程都会在 AI 平台上启动一个训练作业，如下所示：
gcloud ai-platform 作业提交培训...

然后它必须等待作业完成才能进入下一步。为此，我尝试将参数 --stream-logs 添加到上述命令中。通过这种方式，它会传输所有日志，直到作业完成。
问题是，有这么多并行进程，我用完了获取日志的请求：
超出配额指标“读取请求”和限制“每分钟读取请求”的配额
服务“logging.googleapis.com”

但我不需要实际流式传输日志，我只需要一种方法来告诉进程“等待”直到训练工作完成。有没有更聪明、更简单的方法来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/64806003/google-ai-platform-training-wait-for-the-job-to-finish</guid>
      <pubDate>Thu, 12 Nov 2020 14:39:20 GMT</pubDate>
    </item>
    <item>
      <title>训练+测试集是否必须与预测集不同（以便您需要对所有列应用时移）？ （没有时间序列！）</title>
      <link>https://stackoverflow.com/questions/59210109/does-the-trainingtesting-set-have-to-be-different-from-the-predicting-set-so-t</link>
      <description><![CDATA[由于这个问题被否决了，并且第一个答案似乎将其视为一个时间序列问题：这个问题不是关于经典的时间序列分析，而是寻求将每月的列作为特征来处理。没有目的是检查趋势或任何其他分解！我分享了这个问题，因为我在工作中正是遇到了这个挑战，最后，该模型在这种设置下运行良好，混合了非每月的数据（永恒）具有每月功能的功能。因此，这只是一个使用每月数据列作为特征的问题，模型并不关心是12月还是6月，它只关心这些特征是过去多少个月，以便它从模式中学习大约 x 个月前的数据。这些特征不是在月份之后调用的，而是在它们回溯了多少个月之后调用的，例如财富_月_1、财富_月_2 代表过去 1 或 2 个月的财富。
&lt;小时/&gt;
我知道我们应该仅在测试集上测试经过训练的分类器的一般规则。
但现在出现了问题：当我准备好经过训练和测试的分类器时，我可以将其应用到作为训练和测试集基础的同一数据集吗？&lt; /em&gt; 或者我是否必须将其应用于与训练+测试集不同的新预测集？
如果我预测时间序列的标签列怎么办（稍后编辑：我并不是想在这里创建经典的时间序列分析，而是只是从典型数据库中广泛选择列，每周、每月或随机存储的数据，我将其转换为单独的特征列，每个特征列为一周/一个月/一年...），我是否必须转移全部将训练+测试集的特征（不仅是时间序列标签列的过去列，还包括所有其他正常特征）设置回数据没有“知识”的时间点与预测集的拦截？
然后，我将根据过去 n 个月的特征来训练和测试分类器，针对未移动且最新的标签列进行评分，然后根据最近未移动的特征进行预测。移位和未移位的特征具有相同的列数，我通过将移位特征的列名称分配给未移位的特征来对齐移位和未移位的特征。
附注：
p.s.1：https://en.wikipedia.org/wiki/Dependent_and_independent_variables&lt;的一般方法/a&gt;
在数据挖掘工具（用于多元统计和机器学习）中，因变量被分配为目标变量（或在某些工具中为标签属性），而自变量可能被分配为常规变量。[ 8]为训练数据集和测试数据集提供了目标变量的已知值，但应对其他数据进行预测。
p.s.2：在这个基本教程中，我们可以看到预测集有所不同：https://scikit-learn.org/stable/tutorial/basic/tutorial.html
我们使用 [:-1] Python 语法选择训练集，它会生成一个包含所有 &gt; 的新数组。但digits.data 中的最后一项：[…] 现在您可以预测新值。在这种情况下，您将使用digits.data [-1:]中的最后一个图像进行预测。通过预测，您将从训练集中确定与最后一个图像最匹配的图像。]]></description>
      <guid>https://stackoverflow.com/questions/59210109/does-the-trainingtesting-set-have-to-be-different-from-the-predicting-set-so-t</guid>
      <pubDate>Fri, 06 Dec 2019 09:16:37 GMT</pubDate>
    </item>
    </channel>
</rss>