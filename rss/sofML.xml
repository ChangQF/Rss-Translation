<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 16 Jul 2024 15:13:44 GMT</lastBuildDate>
    <item>
      <title>使用张量流的问题（既不包含“saved_model.pb”也不包含“saved_model.pbtxt”）</title>
      <link>https://stackoverflow.com/questions/78755101/issue-on-using-tensor-flow-contains-neither-saved-model-pb-nor-saved-model</link>
      <description><![CDATA[我尝试运行代码，但出现错误。
ValueError：尝试加载不兼容/未知类型的模型。&#39;C:\Users\pm23821\AppData\Local\Temp\tfhub_modules\9616fd04ec2360621642ef9455b84f4b668e219e&#39; 既不包含 &#39;saved_model.pb&#39; 也不包含 &#39;saved_model.pbtxt&#39;

这是我第一次使用 tensorflow 和模型，所以我不知道我需要做什么才能运行我的代码。
目标是找到声音之间的差异，当检测到紧急警报器（警察、消防部门和医院）时，它需要看起来像已被识别一样。当听到家用警报器或鹦鹉的声音时，应该看起来它还没有被识别。
# 加载 YAMNet 模型
model = hub.load(&#39;https://tfhub.dev/google/yamnet/1&#39;)

# 加载 YAMNet 中的类名
def load_class_names(csv_path=&#39;yamnet_class_map.csv&#39;):
class_map = pd.read_csv(csv_path, sep=&#39;,&#39;, header=None)
class_names = class_map[1].tolist()
return class_names

class_names = load_class_names()

# 麦克风音频捕获函数
def record_audio(duration=5, fs=16000): # 录制时长为 5 秒
recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype=&#39;float32&#39;)
sd.wait()
return recording.flatten()

# 预测音频类的函数
def predict_sound(audio_data):
scores, embeddings, spectrogram = model(audio_data)
prediction = np.argmax(scores, axis=1)[0]
return class_names[prediction]

# 循环主函数
while True:
print(&quot;Escutando...&quot;)
audio_data = record_audio()
audio_data /= np.max(np.abs(audio_data))

predict_class = predict_sound(audio_data)
print(&quot;预测的类：&quot;, predict_class)

if “/m/012n7d”在 predict_class 或 “/m/03j1ly” 中在predicted_class或“/m/03kmc9”中在预测类中：
print(&quot;Sirene 已检测到！&quot;)
else:
print(&quot;Não é uma sirene.&quot;)
time.sleep(1) # 暂停 1 秒，直到下一个重击开始 
]]></description>
      <guid>https://stackoverflow.com/questions/78755101/issue-on-using-tensor-flow-contains-neither-saved-model-pb-nor-saved-model</guid>
      <pubDate>Tue, 16 Jul 2024 14:11:25 GMT</pubDate>
    </item>
    <item>
      <title>如何将每小时数据转换为每日最大值、平均值……和最小值</title>
      <link>https://stackoverflow.com/questions/78755052/how-can-i-convert-hourly-data-to-daily-max-aver-and-min</link>
      <description><![CDATA[这是每小时的 TMP 数据，因此我想将其转换为每日的最大值、平均值和最小值，我在 excel 中做过一次，但结果不合理。
此数据需要预处理，因此我只是应用了一些函数，例如查找平均值，我只是使用函数平均值来获取一天的平均数据，但结果并不好，因此我需要一些其他准确且最佳的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78755052/how-can-i-convert-hourly-data-to-daily-max-aver-and-min</guid>
      <pubDate>Tue, 16 Jul 2024 14:03:10 GMT</pubDate>
    </item>
    <item>
      <title>深度学习和跨摄像头跟踪物体</title>
      <link>https://stackoverflow.com/questions/78754641/deep-learning-and-tracking-objects-across-cameras</link>
      <description><![CDATA[我正在使用 YOLOV10 检测和深度排序来跟踪物体，当我们在第一个 CCTV 摄像机中时，它工作正常，当我们到达下一个 CCTV 摄像机时，物体来自不同的视角，类似的物体也很多，我将跟踪更改为 YOLOV10 本身，它们都无法解决在第二个摄像机中识别第一个摄像机中跟踪的物体的问题，因为第二个摄像机中的大小、视点和光照都发生了变化，现在如何给出与模型在第一个摄像机中跟踪的物体相同的 id，第二个摄像机中的 id 为 13，当物体到达它时，它应该具有相同的 id，模型应该知道视点比例和光照是否也发生了变化
这个物体应该匹配
使用这个，而那里也有相同类型的汽车
在第一个摄像头中，物体向东行走，但在第二个摄像头中，物体从它的南边过来，所以我们无法跟踪位置
from ultralytics import YOLO
import cv2

# 全局变量
selected_bbox = None
tracking_id = None

# 鼠标回调函数用于选择要跟踪的对象
def select_object(event, x, y, flags, param):
global selected_bbox, tracking_id
if event == cv2.EVENT_LBUTTONDOWN:
for result in param:
bboxes = result.boxes.xyxy.cpu().numpy() # 边界框
ids = result.boxes.id.cpu().numpy() # 跟踪 ID
for bbox, id_ in zip(bboxes, ids):
if bbox[0] &lt;= x &lt;= bbox[2] and bbox[1] &lt;= y &lt;= bbox[3]:
selected_bbox = bbox
tracking_id = id_
return

print(&quot;starting&quot;)
# 加载 YOLOv10 模型
model = YOLO(&#39;yolov10m.pt&#39;)

# 加载视频
video_path = &#39;videos/1.mp4&#39;
cap = cv2.VideoCapture(&quot;rtsp://amdin:admin@192.168.1.100:554&quot;)

cv2.namedWindow(&#39;frame&#39;)
cv2.setMouseCallback(&#39;frame&#39;, select_object, param=None)

ret = True
results = []

# 读取帧
while ret:
ret, frame = cap.read()

if ret:
# 检测和跟踪对象
results = model.track(frame, persist=True)

# 设置鼠标回调参数
cv2.setMouseCallback(&#39;frame&#39;, select_object, param=results)

if tracking_id is not None:
# 为所选对象绘制边界框
for result in results:
bboxes = result.boxes.xyxy.cpu().numpy()
ids = result.boxes.id.cpu().numpy()
for bbox, id_ in zip(bboxes, ids):
if id_ == tracking_id:
cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)
cv2.putText(frame, f&quot;ID: {int(id_)}&quot;, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 2)

# 可视化
cv2.imshow(&#39;frame&#39;, frame)
if cv2.waitKey(25) &amp; 0xFF == ord(&#39;q&#39;):
break

else:
cap = cv2.VideoCapture(&quot;rtsp://amdin:admin@192.168.1.101:554&quot;)
ret = True

cap.release()
cv2.destroyAllWindows()
`
]]></description>
      <guid>https://stackoverflow.com/questions/78754641/deep-learning-and-tracking-objects-across-cameras</guid>
      <pubDate>Tue, 16 Jul 2024 12:40:28 GMT</pubDate>
    </item>
    <item>
      <title>尝试在多 GPU 设置上训练机器翻译的 Transformer 模型</title>
      <link>https://stackoverflow.com/questions/78754435/trying-to-train-transformer-model-for-machine-translation-on-multi-gpu-setup</link>
      <description><![CDATA[我正在尝试训练机器翻译的变换模型。这是训练函数：
在单个 GPU 上训练时，我得到以下结果：
for src, tgt in train_dataloader:
try:
src = src.to(DEVICE)
tgt = tgt.to(DEVICE)
 tgt_input = tgt[:-1, :]

src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)

logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)

...
在Seq2SeqTransformer：
class Seq2SeqTransformer(nn.Module):
def forward(self,
src: Tensor,
trg: Tensor,
src_mask: Tensor,
tgt_mask: Tensor,
src_padding_mask: Tensor,
tgt_padding_mask: Tensor,
memory_key_padding_mask: Tensor):
 src_emb = self.positional_encoding(self.src_tok_emb(src))
tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,
src_padding_mask, tgt_padding_mask, memory_key_padding_mask)
return self.generator(outs)

在此行：
outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,
src_padding_mask, tgt_padding_mask, memory_key_padding_mask)
我明白了：

sec_emb 的大小为 (31,31) tgt_emb 的大小为 (37,37)

并且训练运行顺利。
现在，我正尝试使用此设置在具有 8 个 GPU 的计算机上进行训练：
transformer = nn.DataParallel(transformer)
transformer = transformer.to(DEVICE)

在调试模式下，我检查了此行中的值：
outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,
src_padding_mask, tgt_padding_mask, memory_key_padding_mask)
我明白了：

sec_emb 的大小为 (4,31) tgt_emb 的大小为 (5,37)

我认为这是因为所有东西都是并行工作的。
但是，我遇到了此错误消息：

文件
&quot;C:\Projects\MT005.venv\Lib\site-packages\torch\nn\ functional.py&quot;,
line 5382，在 multi_head_attention_forward 中
引发 RuntimeError(f&quot;2D attn_mask 的形状是 {attn_mask.shape}，但应该是 {correct_2d_size}。&quot;) RuntimeError:
2D attn_mask 的形状是 torch.Size([4, 31])，但应该是
(4, 4)。

有人可以帮助我或提供一些指导吗？]]></description>
      <guid>https://stackoverflow.com/questions/78754435/trying-to-train-transformer-model-for-machine-translation-on-multi-gpu-setup</guid>
      <pubDate>Tue, 16 Jul 2024 12:00:27 GMT</pubDate>
    </item>
    <item>
      <title>优化 pgvector 以实现多用户文档存储：索引和分区的最佳实践</title>
      <link>https://stackoverflow.com/questions/78754328/optimizing-pgvector-for-multi-user-document-storage-best-practices-for-indexing</link>
      <description><![CDATA[我使用 PostgreSQL 和 pgvector 以及 LangChain 进行文档存储和检索。我的设置：

许多用户，每个用户上传多个文档
文档具有带 userId 的元数据
需要按用户隔离用户文档并按 userId 进行过滤
使用 LangChain 进行带过滤器的检索

当前表结构：
CREATE TABLE document_vector (
id BIGSERIAL,
content TEXT,
metadata JSONB,
embedding VECTOR(1536)
);

优化可扩展性和查询性能的最佳方法是什么？

为每个用户创建一个新表？
添加 user_id 列并对表进行分区？
索引元数据 - &gt;&gt;&#39;userId&#39;？
另一种方法？

寻找一种可扩展性好且在使用用户特定过滤器进行向量相似性搜索时保持良好性能的解决方案。
感谢您的帮助！
我尝试根据存储在元数据 JSONB 字段中的 userId 在 document_vector 表上实现哈希分区，但不确定这是否是正确的方法。
CREATE TABLE document_vector (
id BIGSERIAL,
content TEXT,
metadata JSONB,
embedding VECTOR(1536)
) PARTITION BY HASH ((metadata-&gt;&gt;&gt;&#39;userId&#39;));
]]></description>
      <guid>https://stackoverflow.com/questions/78754328/optimizing-pgvector-for-multi-user-document-storage-best-practices-for-indexing</guid>
      <pubDate>Tue, 16 Jul 2024 11:33:47 GMT</pubDate>
    </item>
    <item>
      <title>t-SNE 中的不同结果</title>
      <link>https://stackoverflow.com/questions/78754318/different-result-in-t-sne</link>
      <description><![CDATA[有人能帮我回答我的问题吗？
所以我从纸上获取数据集并使用 t-SNE 进行可视化。另一方面，我尝试按照之前的数据集特征创建新的数据集。但是在 t-SNE 上绘制新数据集后，我得到了相反的结果。您可以在此图中看到

我感到困惑的是，视觉效果不应该是相同的吗？因为我提取的方式和特征是相同的？或者至少它们对于标签位置的方向是相同的。在 result-2021 中，标签 0 在左边，标签 1 在右边，但在 dataset_full 中，标签 0 在右边，但标签 1 在左边]]></description>
      <guid>https://stackoverflow.com/questions/78754318/different-result-in-t-sne</guid>
      <pubDate>Tue, 16 Jul 2024 11:31:22 GMT</pubDate>
    </item>
    <item>
      <title>为什么我要将 Python 包安装到我的 Machine_Learning 环境中，但它们却是在本地安装的？[关闭]</title>
      <link>https://stackoverflow.com/questions/78753784/why-am-i-installing-python-packages-into-my-machine-learning-environment-but-th</link>
      <description><![CDATA[在此处输入图片说明在此处输入图片说明在此处输入图片说明
如图所示，我使用
“(D:\conda_envs\Machine_Learning) C:\Users\GuYuanji&gt;python -m pip install -r &quot;C:\Users\GuYuanji\2022-Machine-Learning-Specialization-main\requirements.txt&quot;” 

要安装 Python 包，但它们的安装方式如下。
我希望在我指定的环境中安装这些包。]]></description>
      <guid>https://stackoverflow.com/questions/78753784/why-am-i-installing-python-packages-into-my-machine-learning-environment-but-th</guid>
      <pubDate>Tue, 16 Jul 2024 09:40:15 GMT</pubDate>
    </item>
    <item>
      <title>嘿，有人能帮我解答这个基本的机器学习问题吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78753768/hey-can-anyone-help-me-with-this-basic-ml-question</link>
      <description><![CDATA[ 多选题 
嗨，我想找到这个问题的答案，这是关于机器学习的一个基本问题。你能帮我吗？我希望了解机器学习中的关键概念和技术，例如算法、数据预处理、模型训练和评估。如果您有任何提示或资源可以帮助我很好地掌握这些主题并将它们应用于实际场景，那就太好了。非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/78753768/hey-can-anyone-help-me-with-this-basic-ml-question</guid>
      <pubDate>Tue, 16 Jul 2024 09:36:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyTorch 计算信息熵</title>
      <link>https://stackoverflow.com/questions/78753537/compute-information-entropy-with-pytorch</link>
      <description><![CDATA[问题：
如何利用当前模型计算信息熵。
当我尝试实现这篇论文提出的方法时。但我不知道如何让它发挥作用。
假设有一个矩阵 M，它是 m x n。
这个矩阵 M 缺少一些条目。 
该模型的任务是完成矩阵M，使用的方法类似于矩阵分解，但采用自动编码器的架构。
以下是该论文的陈述，未经修改。

在该算法中，我们使用二项分布概率来衡量每个未知延迟的不确定性。具体而言，对于每个未知延迟，我们利用当前模型计算两个潜在延迟的概率。这两个概率构成二项分布。


因此，我们将延迟值的概率设为P(x)=p，P(x_bar)=1-p，其中p等于1表示我们知道x处的延迟，p等于0表示我们不知道x_bar处的延迟。


同时，我们使用信息熵来衡量分布的不确定性。 
对于二项分布，信息熵可以计算为：H(p)=-plog(p)-(1-p)log(1-p)

这是否类似于使用 sigmoid 或 softmax 将模型的输出转换为概率
然后使用概率获取信息熵？
任何帮助都值得感激。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78753537/compute-information-entropy-with-pytorch</guid>
      <pubDate>Tue, 16 Jul 2024 08:42:59 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 模型无法训练</title>
      <link>https://stackoverflow.com/questions/78753201/lstm-model-doesnt-train</link>
      <description><![CDATA[我正在尝试使用深度学习来查找粒子的化学状态。作为输入，我有粒子在 X_train 中随时间的位置，形状为 (num_train,sequence_length)。 （我的序列长度为 100），输出是形状为 (num_train,1) 的 Y_train 中包含的转换帧（介于 1 和 100 之间）。
这是一个序列示例（https://i.sstatic.net/Ddmhjc24.jpg），转换位于第 84 帧。
所有数据都是用非常具体的算法生成的，但是该算法不会生成非常复杂的数据，我认为自己很容易找到转换，但我希望这个深度学习模型能够正常工作。
这是 LSTM 代码：
# 过滤

# 定义 LSTM 模型
model = Sequential([
LSTM(64, input_shape=(sequence_length, 1), return_sequences=False),
Dense(64,activation=&#39;relu&#39;),
Dense(1)
])

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;) # 回归的均方误差

# 显示模型摘要
model.summary()

# 训练模型
model.fit(X_train, Y_train, epochs=40, batch_size=32, validation_data=(X_test, Y_test))

# 用新数据进行预测的示例
prediction = model.predict(X_test)
print(prediction)

结果：
模型：“顺序”
_________________________________________________________________
层（类型）输出形状参数 # 
====================================================================
lstm (LSTM) (无，64) 16896 

密集 (密集) (无，64) 4160 

密集_1 (密集) (无，1) 65 

============================================================================
总参数：21121 (82.50 KB)
可训练参数： 21121 (82.50 KB)
不可训练参数：0 (0.00 字节)
_________________________________________________________________
Epoch 1/10
631/631 [==============================] - 35s 50ms/step - 损失：1043.6710 - val_loss：840.6771
Epoch 2/10
631/631 [==============================] - 30s 48ms/step - 损失：840.9444 - val_loss：839.9596
Epoch 3/10
631/631 [===============================] - 32s 50ms/步 - 损失：841.6289 - val_loss：840.7188
Epoch 4/10
631/631 [=============================] - 30s 48ms/步 - 损失：840.9946 - val_loss：840.6344
Epoch 5/10
631/631 [===============================] - 33s 52ms/步 - 损失：841.8745 - val_loss：839.9298
Epoch 6/10
631/631 [==============================] - 31s 49ms/步 - 损失：841.6499 - val_loss：839.8434
Epoch 7/10
631/631 [=============================] - 31s 49ms/步 - 损失：841.2045 - val_loss：840.0717
Epoch 8/10
631/631 [===============================] - 30s 48ms/步 - 损失：842.0576 - val_loss： 840.2137
纪元 9/10
631/631 [=============================] - 33s 52ms/步 - 损失：842.7056 - val_loss：840.5657
纪元 10/10
631/631 [=============================] - 30s 48ms/步 - 损失：841.5714 - val_loss：839.8404
70/70 [================================] - 2s 16ms/步
[[52.569366]
[52.569286]
[52.569378]
...
[52.569344]
[52.569313]
[52.56937 ]]

如您所见，当我测试训练后的模型时，无论输入是什么，输出都是相同的。 val_loss 不会随着 epoch 的数量而改善。 这就是问题所在，我不明白发生了什么。
我是深度学习的初学者，所以也许我犯了一个非常简单的错误。 但是我仔细检查了我的数据，X_train 已标准化，我尝试在我的模型上添加一些 drop out 和其他层，但没有任何变化。
也许使用 LSTM 无法做到这一点，但我认为数据非常简单。 我真的想尝试找到一种方法来使用深度学习来找到它。 我 d]]></description>
      <guid>https://stackoverflow.com/questions/78753201/lstm-model-doesnt-train</guid>
      <pubDate>Tue, 16 Jul 2024 07:31:40 GMT</pubDate>
    </item>
    <item>
      <title>在带有 Bullseye 的 Raspberry Pi 3B 上安装 Ultralytics 以运行 yolov5 时出错，“错误：无法为使用 PEP 517 的 opencv-python 构建轮子...”</title>
      <link>https://stackoverflow.com/questions/78752286/error-installing-ultralytics-to-run-yolov5-on-raspberry-pi-3b-with-bullseye-er</link>
      <description><![CDATA[我尝试使用 pip 在装有 Bullseye OS 的 Raspberry Pi 3B 上安装 Ultralytics。虽然我能够安装所有依赖项以成功运行 yolov5，但在尝试安装 Ultralytics 时，我收到错误，
&quot;CMake 安装出现问题，中止构建。CMake 可执行文件是 /tmp/pip-build-env-3didr32b/overlay/lib/python3.9/site-packages/cmake/data/bin/cmake

错误：无法为 opencv-python 构建 wheel
无法构建 opencv-python
错误：无法为使用 PEP 517 且无法直接安装的 opencv-python 构建 wheel&quot;

虽然有多种方法可以成功安装 opencv，并且对我来说很有效，但立即安装 Ultralytics 会导致再次安装 opencv，从而导致同样的失败。有人知道如何修复这个问题吗？
我尝试过单独安装 open-cv，然后安装 ultralytics，但出现了同样的错误。有人建议在安装 ultralytics 之前先安装 opencv-python-headless，但我无法成功安装此包。]]></description>
      <guid>https://stackoverflow.com/questions/78752286/error-installing-ultralytics-to-run-yolov5-on-raspberry-pi-3b-with-bullseye-er</guid>
      <pubDate>Tue, 16 Jul 2024 00:06:34 GMT</pubDate>
    </item>
    <item>
      <title>Huggingface 的 Beam RunInference 和句子转换器</title>
      <link>https://stackoverflow.com/questions/78750157/beam-runinference-and-sentence-transformers-from-huggingface</link>
      <description><![CDATA[我正在尝试将 RunInference beam Transform 与 stsb-xlm-r-multilingual 模型一起使用。
像这样：
 inferences = (
formatted_examples
| &quot;Run Inference&quot; &gt;&gt; RunInference(model_handler)
| &#39;ProcessOutput&#39; &gt;&gt; beam.ParDo(PostProcessor())
)

而 model_handler 是：
model_handler = HuggingFacePipelineModelHandler(
task=PipelineTask.FeatureExtraction,
model = &quot;sentence-transformers/stsb-xlm-r-multilingual&quot;,
load_pipeline_args={&#39;framework&#39;: &#39;pt&#39;},
inference_args={&#39;max_length&#39;: 200}
)

但是，转换返回的 predictionResult 对象包含一个嵌套数组，其中包含多个嵌入。
我可以像这样访问嵌入：
result.inference[0][0][0]。
这为我提供了一个嵌入。
我不确定所有其他嵌入是什么。我只对其中一个感兴趣，我不需要所有数据。
虽然我总是可以通过使用上面显示的内部索引来获取嵌入，但我想知道我是否做错了什么，或者是否有办法将一些参数传递给处理程序，以指示它只返回单个嵌入？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78750157/beam-runinference-and-sentence-transformers-from-huggingface</guid>
      <pubDate>Mon, 15 Jul 2024 13:36:33 GMT</pubDate>
    </item>
    <item>
      <title>如何在不使用标签编码的情况下对 RandomForest 的非序数分类变量进行编码？</title>
      <link>https://stackoverflow.com/questions/78742216/how-to-encode-non-ordinal-categorical-variables-for-randomforest-without-using-l</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78742216/how-to-encode-non-ordinal-categorical-variables-for-randomforest-without-using-l</guid>
      <pubDate>Fri, 12 Jul 2024 21:11:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 中的空间数据管理机器学习模型中的类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78733642/managing-problems-of-class-imbalance-in-machine-learning-models-using-spatial-da</guid>
      <pubDate>Thu, 11 Jul 2024 05:01:17 GMT</pubDate>
    </item>
    <item>
      <title>当我通过 docker-compose.yml 运行镜像 ollama 时，无法正确运行它</title>
      <link>https://stackoverflow.com/questions/78724837/i-cannot-run-the-image-ollama-correctly-when-i-run-it-through-docker-compose-yml</link>
      <description><![CDATA[我正在做一个项目，分析文本信息以从中提取特定数据。Python 中的正则表达式效果不佳，因为文本格式不断变化且没有一致性。因此，我决定使用语言模型来处理这些文本，如果文本包含我感兴趣的内容，则返回结果。
在开发程序时，我使用以下命令运行模型：
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
之后，我在代码中发送了一个请求，如下所示：
url = &#39;http://localhost:11434/api/generate&#39;
data = { 
&quot;model&quot;: &quot;llama3&quot;,
&quot;prompt&quot;: f&quot;{input_text}&quot;,
}

这有效（尽管响应需要一点时间才能完成）生成）。
现在，当我尝试配置我的 docker-compose.yml 文件以启动语言模型容器时，它看起来像这样：
ollama:
container_name: ollama
image: ollama/ollama
volumes:
- ollama:/root/.ollama
ports:
- &quot;11434:11434&quot;

volumes:
ollama:

但我只收到 404 错误，这意味着找不到端点。我不明白我做错了什么。有人可以帮忙吗？
此外，有人知道语言模型是否支持多线程吗？我的脚本发送文本非常快，我不确定是否要限制向语言模型发送请求的速率，或者它是否可以处理多线程和异步请求。]]></description>
      <guid>https://stackoverflow.com/questions/78724837/i-cannot-run-the-image-ollama-correctly-when-i-run-it-through-docker-compose-yml</guid>
      <pubDate>Tue, 09 Jul 2024 09:38:52 GMT</pubDate>
    </item>
    </channel>
</rss>