<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 12 Dec 2023 15:14:47 GMT</lastBuildDate>
    <item>
      <title>如何使我的模型从经过训练的数据集中预测数据？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77646376/how-to-make-my-model-predicted-a-data-from-trained-dataset</link>
      <description><![CDATA[&lt;前&gt;&lt;代码&gt;输入大小 = 8
输出大小 = 1
隐藏层大小 = 50
模型 = tf.keras.Sequential([
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense（hidden_​​layer_size，激活=&#39;relu&#39;），
    tf.keras.layers.Dense(output_size, 激活=&#39;tanh&#39;)
  ]）
model.compile(optimizer=&#39;adam&#39;,loss=&#39;mean_squared_error&#39;,metrics=[&#39;accuracy&#39;])
批量大小 = 150
最大纪元 = 100
Early_stopping = tf.keras.callbacks.EarlyStopping(耐心=10)
model.fit(train_inputs_with_longtitude,
            train_targets_with_longtitude，
            批量大小=批量大小，
            纪元 = max_epochs，
            回调=[early_stopping],
            validation_data=(validation_inputs_with_longtitude ,validation_targets_with_longtitude ), # 验证数据
            详细 = 2
            ）
model.predict(test_targets_with_longtitude)

我尝试了一些方法model.predict()但它不起作用
我应该添加什么代码才能预测数据？]]></description>
      <guid>https://stackoverflow.com/questions/77646376/how-to-make-my-model-predicted-a-data-from-trained-dataset</guid>
      <pubDate>Tue, 12 Dec 2023 13:12:28 GMT</pubDate>
    </item>
    <item>
      <title>我的训练功能仅几步之后就意外结束</title>
      <link>https://stackoverflow.com/questions/77646034/my-training-function-ends-unexpectedly-after-only-a-few-steps</link>
      <description><![CDATA[我正在尝试运行 Pix2Pix，但是我的训练功能在前 1k 步中突然停止，没有错误。我使用 PyTorch 来创建鉴别器和生成器。下面是包含 2 个负责训练的函数的代码，一个用于训练每个步骤，一个用于拟合模型。
训练步骤函数：
def train_step(input_image, 目标, 步骤):
    生成器.train()
    判别器.train()

    # 前向传递
    gen_output = 生成器（输入图像）

    Disc_real_output = 鉴别器（输入图像，目标）
    Disc_ generated_output = 鉴别器（input_image，gen_output）

    # 计算损失
    gen_total_loss, gen_gan_loss, gen_l1_loss = 生成器_loss(disc_generate_output,
        生成输出，目标）
    光盘损失 = 判别器损失（光盘真实输出，光盘生成输出）

    # 向后传递
    Generator_optimizer.zero_grad()
    discriminator_optimizer.zero_grad()

    gen_total_loss.backward(retain_graph=True)
    discriminator_optimizer.zero_grad() # 清除生成器梯度
        判别器后向传递
    Disc_loss.backward()

    # 更新权重
    生成器优化器.step()
    discriminator_optimizer.step()

    # 日志记录
    使用 torch.no_grad()：
        writer.add_scalar(&#39;gen_total_loss&#39;, gen_total_loss.item(), global_step=step // 1000)
        writer.add_scalar(&#39;gen_gan_loss&#39;, gen_gan_loss.item(), global_step=step // 1000)
        writer.add_scalar(&#39;gen_l1_loss&#39;, gen_l1_loss.item(), global_step=step // 1000)
        writer.add_scalar(&#39;disc_loss&#39;,disc_loss.item(),global_step=step // 1000)

拟合函数：
def fit(train_loader, test_loader, 步骤):
   example_target, example_input = next(iter(test_loader))
   开始 = 时间.time()

   对于枚举（train_loader）中的步骤（目标，input_image）：
    如果（步骤）% 1000 == 0：
        显示.clear_output(等待=True)

        如果步骤！= 0：
            print(f&#39;1000 步所用时间: {time.time()-start:.2f} 秒\n&#39;)

        开始 = 时间.time()

        生成图像（生成器，示例_输入，示例_目标）
        print(f&quot;步长: {step//1000}k&quot;)

    train_step（输入图像，目标，步骤）

    # 训练步骤
    如果（步长+1）% 10 == 0：
        打印（&#39;。&#39;，结束=&#39;&#39;，刷新= True）

    # 每 5k 步保存（检查点）模型
    如果（步长 + 1）% 5000 == 0：
        火炬.保存（{
            &#39;generator_state_dict&#39;：generator.state_dict(),
            &#39;discriminator_state_dict&#39;: discriminator.state_dict(),
            &#39;generator_optimizer_state_dict&#39;：generator_optimizer.state_dict(),
            &#39;discriminator_optimizer_state_dict&#39;: discriminator_optimizer.state_dict(),
        }, f&#39;检查点_{step + 1}.pt&#39;)

我是使用 GAN 的新手，我不确定这里的问题是什么。我尝试检查训练循环期间是否发生任何异常并打印它，但没有打印任何内容。]]></description>
      <guid>https://stackoverflow.com/questions/77646034/my-training-function-ends-unexpectedly-after-only-a-few-steps</guid>
      <pubDate>Tue, 12 Dec 2023 12:15:15 GMT</pubDate>
    </item>
    <item>
      <title>寻找用于构建人工智能领域推荐系统的数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/77645042/seeking-datasets-for-building-a-ai-domain-recommendation-system</link>
      <description><![CDATA[我正在创建一个用于域名推荐的人工智能系统，并正在寻找数据集来训练我的模型。我需要的关键数据包括：
有关域名注册和可用性的历史数据。
有关域服务提供商的信息，包括他们的产品和服务。
与域选择相关的用户偏好和行为（如果有）。
我的主要挑战和疑问是：
获取此类数据集的最佳来源是什么？是否有已知包含此类信息的公共存储库或数据库？
在这种情况下，在选择用于训练人工智能模型的数据集时，我应该考虑哪些因素？例如，在这个特定用例中，数据量、多样性和准确性等因素有多重要？
如果现成的数据集很稀缺，我可以采用什么策略来以合乎道德和合法的方式编译和管理这些数据？
在此处发帖之前，我进行了初步研究，但尚未找到满足这些特定要求的数据集。如果有人能向我指出可能提供此类信息的公共数据集或存储库，我将不胜感激。另外，有关如何收集此类数据的建议也会非常有帮助。
请注意，我并不是在寻找专有或机密信息，而是在寻找公开数据或有关如何以道德和合法方式汇总这些数据的建议。]]></description>
      <guid>https://stackoverflow.com/questions/77645042/seeking-datasets-for-building-a-ai-domain-recommendation-system</guid>
      <pubDate>Tue, 12 Dec 2023 09:42:31 GMT</pubDate>
    </item>
    <item>
      <title>如何解决运行时错误尝试在pytorch中的backward()函数中再次向后浏览图形</title>
      <link>https://stackoverflow.com/questions/77644164/how-to-solve-runtimeerror-trying-to-backward-through-the-graph-a-second-time-in</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77644164/how-to-solve-runtimeerror-trying-to-backward-through-the-graph-a-second-time-in</guid>
      <pubDate>Tue, 12 Dec 2023 06:52:02 GMT</pubDate>
    </item>
    <item>
      <title>EconML 和 SHAP</title>
      <link>https://stackoverflow.com/questions/77644092/econml-and-shap</link>
      <description><![CDATA[我们可以使用形状值工具（TreeExplainer）来帮助我们评估EconML中的因果森林并进行一些可视化吗？我正在阅读EconML的cookbook，似乎SHAP在因果推理中可能有用，但我正在尝试在我的程序中应用因果森林！
也许我希望有人能教我如何使用这两个工具！]]></description>
      <guid>https://stackoverflow.com/questions/77644092/econml-and-shap</guid>
      <pubDate>Tue, 12 Dec 2023 06:33:54 GMT</pubDate>
    </item>
    <item>
      <title>用于检测源代码中语法问题的 ML 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/77644081/ml-model-for-detecting-syntax-issues-in-source-code</link>
      <description><![CDATA[我编写了一个使用正则表达式来识别源代码中的语法错误的工具，该工具还提供了纠正问题的原因和建议。
同样，我需要训练一个机器学习模型，以便照顾到每个极端情况并将其推广。
需要建议来实现这一目标。（是否使用监督方法或现在哪种方法更好）
示例：避免在 src 代码中使用特定变量、以正确的推荐格式为变量赋值等。]]></description>
      <guid>https://stackoverflow.com/questions/77644081/ml-model-for-detecting-syntax-issues-in-source-code</guid>
      <pubDate>Tue, 12 Dec 2023 06:29:43 GMT</pubDate>
    </item>
    <item>
      <title>VS Code .ipynb 中的 FileUpload 小部件问题：显示“Upload(6)”，但数据长度返回 0</title>
      <link>https://stackoverflow.com/questions/77643801/fileupload-widget-issue-in-vs-code-ipynb-upload6-displayed-but-data-lengt</link>
      <description><![CDATA[我试图使用 widget.FileUpload() 上传文件并打印图像。但上传 6 个图像并尝试在新单元格中运行最后一行后，它返回错误，显示“IndexError：列表索引超出范围”
btn_upload = widgets.FileUpload()
btn_上传


img = PILImage.create(btn_upload.data[-1])

然后我尝试了这条线
print(&quot;上传文件数：&quot;, len(btn_upload.data))
期待“6”因为这是我上传的文件数，所以它返回了 0。]]></description>
      <guid>https://stackoverflow.com/questions/77643801/fileupload-widget-issue-in-vs-code-ipynb-upload6-displayed-but-data-lengt</guid>
      <pubDate>Tue, 12 Dec 2023 05:06:16 GMT</pubDate>
    </item>
    <item>
      <title>如何利用 GPU 减少 xgboost 的处理时间？</title>
      <link>https://stackoverflow.com/questions/77643788/how-can-i-reduce-processing-time-with-xgboost-by-utilizing-my-gpu</link>
      <description><![CDATA[我正在关注数据营的本教程，他们有一件事提到的是利用 GPU 来加快处理时间。他们甚至说它“速度极快”。
然而，我看到了相反的结果。对于下面的代码块，在 10k 提升的情况下，我看到在我的 params 中传递 “hist” 大约需要 30 秒，而在 ” 中传递则只需一分多钟。 gpu_hist&quot; 与我的 params 一起传递。
使用 “gpu_hist” 时，我的 GPU 的使用率上限为 40%，使用 “hist” 时，所有 24 个逻辑核心的使用率上限为 100%
params = {“objective”: “reg:squarederror”, “tree_method”: “gpu_hist”, “subsample”: 0.8,
    “colsample_bytree”：0.8}

evals = [(dtrain_reg, “训练”),(dtest_reg, “验证”)]

n = 10000


模型 = xgb.train(
   参数=参数，
   dtrain=dtrain_reg,
   num_boost_round=n,
   评估=评估，
   详细评估=50，
）

我正在尝试在 jupyter 笔记本的 VSCode 中运行它。

我已安装 CUDA 工具包和 cuDNN
我已检查它们是否已添加到路径中
我已确保安装了正确版本的 xgboost 来使用 GPU。
数据集有 53k 行 10 列，所以我不认为数据集太小
我已确认兼容性（使用 RTX 2060）

我问过 chatGPT，在网上搜索过，甚至问过我正在学习的课程中的导师，但无法诊断为什么“gpu_hist”花费了这么长时间。 vs 只是“历史”。
4 个月前，Stack Overflow 上还有另一个类似问题其响应为零。]]></description>
      <guid>https://stackoverflow.com/questions/77643788/how-can-i-reduce-processing-time-with-xgboost-by-utilizing-my-gpu</guid>
      <pubDate>Tue, 12 Dec 2023 05:02:17 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的模型能够在数据集上进行训练和测试，但在添加 SoftMax 层并要求其进行预测时出现错误？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77642982/how-come-my-model-is-able-to-train-and-test-on-a-dataset-but-gives-an-error-when</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77642982/how-come-my-model-is-able-to-train-and-test-on-a-dataset-but-gives-an-error-when</guid>
      <pubDate>Mon, 11 Dec 2023 23:58:34 GMT</pubDate>
    </item>
    <item>
      <title>不使用 OpenAI Gym 环境的近端策略优化代码 [关闭]</title>
      <link>https://stackoverflow.com/questions/77641484/proximal-policy-optimization-code-without-using-openai-gym-environments</link>
      <description><![CDATA[对于一个项目，我必须在 Python 中对使用在线物理系统收集的数据实施近端策略优化。我见过的所有示例都使用 OpenAI 的 Gym 环境。我将如何修改/设置使用来自我的收集系统的数据而不是健身房环境数据的实现？]]></description>
      <guid>https://stackoverflow.com/questions/77641484/proximal-policy-optimization-code-without-using-openai-gym-environments</guid>
      <pubDate>Mon, 11 Dec 2023 17:58:03 GMT</pubDate>
    </item>
    <item>
      <title>如何修复我的感知器来识别数字？</title>
      <link>https://stackoverflow.com/questions/77594625/how-can-i-fix-my-perceptron-to-recognize-numbers</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77594625/how-can-i-fix-my-perceptron-to-recognize-numbers</guid>
      <pubDate>Sun, 03 Dec 2023 14:03:49 GMT</pubDate>
    </item>
    <item>
      <title>如何为多重损失设置可训练的权重？</title>
      <link>https://stackoverflow.com/questions/77380874/how-to-have-trainable-weights-for-multiple-losses</link>
      <description><![CDATA[我有一个加权损失，其中包含用于训练模型的多个损失。
损失 = w1 * 损失1 + w2 * 损失2 + w3 * 损失3
loss.backward()

是否有任何可能的方法使 w1、w2 和 w3 可学习参数？我尝试在模型内使用 self.w1 = nn.Parameter(torch.tensor(0.33), require_grad=True) 来初始化它们，但在一次迭代之后，它的值变成了 nan代码&gt;.
当我们想要使权重可学习时，还需要考虑其他问题，例如，什么阻止它们趋于零甚至负数以使损失为零，我们如何确保权重不会集中于只有一次损失。
有没有办法让这些权重可以学习，而不是手动调整权重？]]></description>
      <guid>https://stackoverflow.com/questions/77380874/how-to-have-trainable-weights-for-multiple-losses</guid>
      <pubDate>Sat, 28 Oct 2023 19:14:51 GMT</pubDate>
    </item>
    <item>
      <title>CNN 架构的问题</title>
      <link>https://stackoverflow.com/questions/75060717/issue-with-cnn-architecture</link>
      <description><![CDATA[我正在尝试实现 CNN 架构，但是输出的形状存在问题。集合的形状如下：
x_train.shape、y_train.shape、x_test.shape、y_test.shape

&lt;前&gt;&lt;代码&gt;((1203, 162, 1), (1203, 7), (402, 162, 1), (402, 7))

架构设置如下：
input_x = tf.keras.layers.Input(shape = (x_train.shape[1],1))
conv_1 = tf.keras.layers.Conv1D(filters=16,kernel_size=3,padding=“相同”,activation=“relu”)(input_x)
pool_1 = tf.keras.layers.MaxPooling1D(2)(conv_1)
conv_2 = tf.keras.layers.Conv1D(filters=32,kernel_size=3,padding=“相同”,activation=“relu”)(pool_1)
pool_2 = tf.keras.layers.MaxPooling1D(2)(conv_2)

展平 = tf.keras.layers.Flatten()(pool_2)
密集= tf.keras.layers.Dense（512，激活=“relu”）（展平）
fb = tf.keras.layers.Dropout(0.4)（密集）
fb = tf.keras.layers.Dense(512，激活=“relu”)(fb)
fb = tf.keras.layers.Dropout(0.4)(fb)

输出= tf.keras.layers.Dense（8，激活=“softmax”）（fb）
model_branching_summed = tf.keras.models.Model(输入=input_x，输出=输出)
model_branching_summed.summary()
model_branching_summed.compile(optimizer=SGD(learning_rate=0.01,momentum=0.8),loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])


历史= model_branching_summed.fit（x_train，y_train，batch_size = 128，epochs = 100，validation_data =（x_test，y_test），callbacks = [rlrp]）

但是当我运行模型时，它给出以下错误：
ValueError Traceback（最近一次调用最后一次）
[192] 中的单元格，第 5 行
      1 rlrp =ReduceLROnPlateau(监视器=&#39;损失&#39;,因子=0.4,详细=0,耐心=2,min_lr=0.0001)
      2 #(min_lr=0.000001)
----&gt; 5 历史=model_branching_summed.fit（x_train，y_train，batch_size = 128，epochs = 100，validation_data =（x_test，y_test），callbacks = [rlrp]）

ValueError：形状（无，7）和（无，8）不兼容

哪里出错了？]]></description>
      <guid>https://stackoverflow.com/questions/75060717/issue-with-cnn-architecture</guid>
      <pubDate>Mon, 09 Jan 2023 17:09:09 GMT</pubDate>
    </item>
    <item>
      <title>将循环转换为双循环</title>
      <link>https://stackoverflow.com/questions/72054967/converting-recurrent-to-bi-recurrent</link>
      <description><![CDATA[我想将下面的 RNN 转换为双向 RNN，我该怎么做？
&lt;前&gt;&lt;代码&gt;模型 = RNN()
模型.summary()
model.compile(loss=&#39;categorical_crossentropy&#39;,optimizer=RMSprop(),metrics=[&#39;accuracy&#39;])
model.fit(X_train,Y_train,batch_size=10,epochs=20,
          验证分割=0.1）
]]></description>
      <guid>https://stackoverflow.com/questions/72054967/converting-recurrent-to-bi-recurrent</guid>
      <pubDate>Fri, 29 Apr 2022 08:27:10 GMT</pubDate>
    </item>
    <item>
      <title>感知器权重更新规则的直觉</title>
      <link>https://stackoverflow.com/questions/34477827/intuition-for-perceptron-weight-update-rule</link>
      <description><![CDATA[我无法理解感知器的权重更新规则：
w(t + 1) = w(t) + y(t)x(t)。
假设我们有一个线性可分离的数据集。

w 是一组权重 [w0, w1, w2, ...]，其中 w0 是偏差。
x 是一组输入参数 [x0, x1, x2, ...]，其中 x0 固定为 1 以适应偏差。

在迭代 t 时，其中 t = 0, 1, 2, ...,

w(t) 是迭代 t 时的权重集。
x(t) 是一个错误分类的训练示例。
y(t) 是 x(t) 的目标输出（-1 或 1）。


&lt;小时/&gt;

为什么这个更新规则会将边界向正确的方向移动？]]></description>
      <guid>https://stackoverflow.com/questions/34477827/intuition-for-perceptron-weight-update-rule</guid>
      <pubDate>Sun, 27 Dec 2015 05:20:30 GMT</pubDate>
    </item>
    </channel>
</rss>