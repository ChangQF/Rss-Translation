<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 27 Jan 2024 18:14:55 GMT</lastBuildDate>
    <item>
      <title>在 Flask 框架中集成 ML 模型时似乎无法解决此错误</title>
      <link>https://stackoverflow.com/questions/77892140/cant-seem-to-solve-this-error-while-integrating-a-ml-model-in-a-flask-framework</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77892140/cant-seem-to-solve-this-error-while-integrating-a-ml-model-in-a-flask-framework</guid>
      <pubDate>Sat, 27 Jan 2024 17:33:27 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：使用高斯贝叶斯和高斯朴素贝叶斯对点进行分类时，操作数无法与形状 (1,13) (14,) 一起广播</title>
      <link>https://stackoverflow.com/questions/77892117/valueerror-operands-could-not-be-broadcast-together-with-shapes-1-13-14-wh</link>
      <description><![CDATA[我正在尝试对此数据集中的数据点进行分类：https: //sharon.srworkspace.com/ml/datasets/hw1/wine.data.csv。我几乎从头开始在 Python 中使用高斯贝叶斯和高斯朴素贝叶斯分类器。因此，在模型的训练测试拆分之后，我实现了这些函数来对数据点进行分类：
将 numpy 导入为 np
从 scipy.stats 导入 multivariate_normal

def Classify_point_gaussian_bayes(x):
    类 = np.unique(y)
    可能性= []
    
    对于类中的 c：
        类数据 = 数据[y == c]
        先验 = len(class_data) / len(数据)
        平均值 = np.mean(class_data, axis=0)
        cov = np.cov(class_data.T)
                
        可能性= multivariate_normal.pdf(x_reshape,mean=mean,cov=cov,allow_singular=True)
        likelihoods.append(先验 * 可能性)
    
    返回类别[np.argmax(可能性)]

def Classify_point_gaussian_naive_bayes(x):
    类 = np.unique(y)
    可能性= []
    
    对于类中的 c：
        类数据 = 数据[y == c]
        先验 = len(class_data) / len(数据)
        平均值 = np.mean(class_data, axis=0)
        var = np.var(class_data, 轴=0)
                
        可能性= multivariate_normal.pdf(x_reshape,mean=mean,cov=np.diag(var),allow_singular=True)
        likelihoods.append(先验 * 可能性)
    
    返回类别[np.argmax(可能性)]

然后我必须查看这两种方法的测试精度，我以这种形式进行：
&lt;前&gt;&lt;代码&gt;res = []
对于 idx，枚举中的 test_point(X_test.values)：
    res.append(classify_point_gaussian_bayes(test_point) == y_test[idx])
print(f&#39;高斯贝叶斯的测试精度为 {res.count(True)/len(res)}&#39;)

分辨率=[]
对于 idx，枚举中的 test_point(X_test.values)：
    res.append(classify_point_gaussian_naive_bayes(test_point) == y_test[idx])
print(f&#39;高斯朴素贝叶斯的测试精度为 {res.count(True)/len(res)}&#39;)

但我仍然遇到同样的错误：ValueError：操作数无法与形状 (1,13) (14,) 一起广播。
&lt;小时/&gt;
更具体地说：
ValueError Traceback（最近一次调用最后一次）
单元格 In[42]，第 3 行
      1 资源 = []
      2 对于 idx，enumerate(X_test.values) 中的 test_point：
----&gt; 3 res.append(classify_point_gaussian_bayes(test_point) == y_test[idx])
      4 print(f&#39;高斯贝叶斯的测试精度为 {res.count(True)/len(res)}&#39;)
      6 资源 = []

单元格 In[41]，第 21 行
     18 # 重塑 x 使其具有与平均值相同数量的特征
     19 x_reshape = x.reshape(1, -1)
---&gt; 21 可能性 = multivariate_normal.pdf(x_reshape,mean=mean,cov=cov,allow_singular=True)
     22likelihoods.append(先验*可能性)
     24 个返回类别[np.argmax(likelihoods)]

文件c：\ Users \ User \ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ scipy \ stats \ _multivariate.py：583，在multivariate_normal_gen.pdf中（self，x，mean，cov，allow_singular）
    第581章
    第582章
--&gt;第583章
    第584章
    第585章

文件c：\ Users \ User \ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ scipy \ stats \ _multivariate.py：526，在multivariate_normal_gen._logpdf（self，x，mean，cov_object）中
    507“”“”多元正态概率密度函数的对数。
    508
    第509章 参数
   （...）
    第523章
    第524章
    第525章
--&gt; 526 dev = x - 平均值
    第527章1：
    第528章

ValueError：操作数无法与形状 (1,13) (14,) 一起广播

由于这是一个关于维度的问题，我尝试在两个函数中使用此行调整函数作为参数的 x 数据点的大小：x_reshape = x.reshape(1, -1)甚至：x_reshape = x.reshape(-1)。
但它不起作用，仍然给我带来与上面相同的错误。]]></description>
      <guid>https://stackoverflow.com/questions/77892117/valueerror-operands-could-not-be-broadcast-together-with-shapes-1-13-14-wh</guid>
      <pubDate>Sat, 27 Jan 2024 17:26:09 GMT</pubDate>
    </item>
    <item>
      <title>如何阻塞 OCDNet 管道并仅在 OCRNet 上获取结果？ （NVIDIA 光学字符检测和识别解决方案/OCDR）</title>
      <link>https://stackoverflow.com/questions/77892097/how-to-block-the-ocdnet-pipeline-and-get-the-result-only-on-ocrnet-nvidia-opti</link>
      <description><![CDATA[我正在 Jetson 上本地运行 NVIDIA 光学字符检测和识别解决方案。我想阻止 OCDNet 的管道，只使用 OCRNet 进行推断。我注释掉了所有与 OCDNet 相关的代码。结果是空洞的推论。]]></description>
      <guid>https://stackoverflow.com/questions/77892097/how-to-block-the-ocdnet-pipeline-and-get-the-result-only-on-ocrnet-nvidia-opti</guid>
      <pubDate>Sat, 27 Jan 2024 17:22:01 GMT</pubDate>
    </item>
    <item>
      <title>我无法运行二元期权机器人</title>
      <link>https://stackoverflow.com/questions/77892067/i-can-not-run-a-binary-option-bot</link>
      <description><![CDATA[我正在使用这个 git hub 存储库
https://github.com/ItamarRocha/binary-bot
当我运行 pip install -rrequirements.txt 时
我收到此错误
&lt;前&gt;&lt;代码&gt;

E:\binary-bot&gt;pip install -r requests.txt
收集 iqoptionapi@ git+git://github.com/Lu-Yi-Hsun/iqoptionapi.git@e96ba2c5
b905a139a4765167b08c5df48cf57773 来自 git+git://github.com/Lu-Yi-Hsun/iqoptionap
i.git@e96ba2c5b905a139a4765167b08c5df48cf57773（来自 -rrequirements.txt（第 1 行
））
  克隆 git://github.com/Lu-Yi-Hsun/iqoptionapi.git （修订版 e96ba2c5b905a
139a4765167b08c5df48cf57773) 到 c:\users\pars\appdata\local\temp\pip-install-fq4
i3vho\iqoptionapi
  运行命令 git clone -q git://github.com/Lu-Yi-Hsun/iqoptionapi.git &#39;C:\U
sers\pars\AppData\Local\Temp\pip-install-fq4i3vho\iqoptionapi&#39;
  致命：无法从远程存储库读取。

  请确保您拥有正确的访问权限
  并且存储库存在。
错误：命令出错，退出状态为 128：git clone -q git://github.com/L
u-Yi-Hsun/iqoptionapi.git &#39;C:\Users\pars\AppData\Local\Temp\pip-install-fq4i3vho
\iqoptionapi&#39; 检查日志以获取完整的命令输出。
警告：您正在使用 pip 版本 19.2.3，但版本 23.3.2 可用。
您应该考虑通过“python -m pip install --upgrade pip”通讯进行升级
和。

E:\binary-bot&gt;

如果您能帮助我，那就太好了。
我尝试使用 pip install -rrequirements.txt 安装该要求
但它给出了错误]]></description>
      <guid>https://stackoverflow.com/questions/77892067/i-can-not-run-a-binary-option-bot</guid>
      <pubDate>Sat, 27 Jan 2024 17:11:34 GMT</pubDate>
    </item>
    <item>
      <title>使用 Torchio 对两个图像应用完全相同的变换</title>
      <link>https://stackoverflow.com/questions/77892019/apply-the-exact-same-transformation-to-two-images-using-torchio</link>
      <description><![CDATA[我想使用 torchio 对两个图像（图像和分割数据）应用完全相同的转换。这两个图像都存储在名为 image_data 和 segmentation_data 的 numpy 数组中。
到目前为止，我添加了一些增强功能：
self.augmentations = tio.Compose([
            仿射变换，
            弹性变换，
            翻转变换，
            交换变换
        ]）

例如， elastic_transform = tio.RandomElasticDeformation 并尝试通过以下方式将它们应用到图像：
 subject_image = tio.Subject(image=tio.ScalarImage(tensor=image_data))
        subject_segmentation = tio.Subject(
            图像=tio.ScalarImage(张量=segmentation_data))
        数据集 = tio.SubjectsDataset([subject_image, subject_segmentation])
        数据集 = self.augmentations(数据集)
        image_data = 数据集[0][&#39;image&#39;].data
        分段数据 = 数据集[1][&#39;图像&#39;].data

不幸的是，这是不正确的（因为 Compose 无法与主题数据集一起使用），但我找不到任何有关如何正确执行此操作的信息。有人可以帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/77892019/apply-the-exact-same-transformation-to-two-images-using-torchio</guid>
      <pubDate>Sat, 27 Jan 2024 16:57:43 GMT</pubDate>
    </item>
    <item>
      <title>Java 使用 weka 库使用 Weka API 对一些 arff 文件运行随机森林分类</title>
      <link>https://stackoverflow.com/questions/77891962/java-using-the-weka-library-to-run-random-forest-classification-on-some-arff-fil</link>
      <description><![CDATA[我编写了一些代码来执行 10 倍交叉验证，然后最终通过 weka 运行随机森林算法。然而，我有很多文件需要通过 Weka 运行，因此一直在尝试使用 Weka API 并遵循其文档。
我正在努力遵循/创造一些按照我想要的方式工作的东西。
这是我到目前为止所做的代码：
for(int i = 0; i &lt; 阈值.length; i++)
{
    //学习集和有效集的副本
    ArrayList&gt; learnSetCopy = new ArrayList&lt;&gt;(learnSet);
    ArrayList&gt; validSetCopy = new ArrayList&lt;&gt;(validSet);
                
    //二值化化学蛋白质相互作用值
    binarizeCpiAttributes(learnSetCopy, 阈值[i]);
    //生成要通过Weka运行的Arff文件
    generateARFF(文件名 + “LearningThreshold” + 阈值[i] + “Fold” + j + “.arff”, attributeNames, learnSetCopy);

    binarizeCpiAttributes(validSetCopy, 阈值[i]);
    generateARFF(文件名 + “ValidThreshold” + 阈值[i] + “Fold” + j + “.arff”, attributeNames, validSetCopy);
                
    //创建学习和有效arff文件的实例
    实例learningInstances = DataSource.read(fileName + &quot;LearningThreshold&quot; + Threshold[i] + &quot;Fold&quot; + j + &quot;.arff&quot;);
    实例 validInstances = DataSource.read(fileName + &quot;ValidThreshold&quot; + Threshold[i] + &quot;Fold&quot; + j + &quot;.arff&quot;);
                
    //设置学习和有效集的类标签
    if(learningInstances.classIndex() == -1)
    {
        LearningInstances.setClassIndex(learningInstances.numAttributes()-1);
    }

    if(validInstances.classIndex() == -1)
    {
        validInstances.setClassIndex(validInstances.numAttributes()-1);
    }
                
    //初始化随机森林分类器
    RandomForest cls = new RandomForest();
    cls.buildClassifier(learningInstances);

    评估 eval = 新评估(learningInstances);
    eval.evaluateModel(cls​​, validInstances);


    System.out.println(“处理阈值：”+threshold[i]);
}

我想做的是：

在某个折叠上以某个阈值初始化两个arff文件，即学习的arff文件和有效的arff文件

将类标签设置为 arff 文件中的最后一个属性

通过 Weka 对学习 arff 文件运行随机森林分类，然后将该模型应用于有效的 arff 文件

输出生成的 ROC 面积值。


这里还有一张图片，展示了如果我通过 GUI 进行操作的话我会做什么（如果有帮助的话）：
Weka GUI 示例
我尝试通读下载 Weka API 的 jar 文件时提供的文档，以将其实现为 java 代码。]]></description>
      <guid>https://stackoverflow.com/questions/77891962/java-using-the-weka-library-to-run-random-forest-classification-on-some-arff-fil</guid>
      <pubDate>Sat, 27 Jan 2024 16:40:59 GMT</pubDate>
    </item>
    <item>
      <title>机器学习和人工智能[关闭]</title>
      <link>https://stackoverflow.com/questions/77891866/machine-learning-and-artificial-intelligence</link>
      <description><![CDATA[1.机器学习和人工智能，这两个概念是互补的吗？
2.上述概念我们将走向何方？
3.为什么机器学习被视为神经科学？
4.是否有任何算法可用于处理能够缓解太阳放射性射线导致的全球变暖危机的机器？
我正在进行一项研究，以寻找上述问题的解决方案]]></description>
      <guid>https://stackoverflow.com/questions/77891866/machine-learning-and-artificial-intelligence</guid>
      <pubDate>Sat, 27 Jan 2024 16:09:56 GMT</pubDate>
    </item>
    <item>
      <title>为什么随机森林无法预测模型中的高值（稀有值）？</title>
      <link>https://stackoverflow.com/questions/77891522/why-the-random-forest-can-not-predict-the-high-values-rare-values-in-my-model</link>
      <description><![CDATA[我最近开始使用随机森林，并且遇到了一个问题：模型无法预测大部分数据的值。相反，尽管模型没有表现出过度拟合的迹象，但预测值似乎保持不变。任何见解或建议将不胜感激。

我希望模型也能够预测稀有数据？]]></description>
      <guid>https://stackoverflow.com/questions/77891522/why-the-random-forest-can-not-predict-the-high-values-rare-values-in-my-model</guid>
      <pubDate>Sat, 27 Jan 2024 14:24:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 sympy 时消除格式字符串冲突</title>
      <link>https://stackoverflow.com/questions/77891386/get-rid-of-format-string-conflict-when-using-sympy</link>
      <description><![CDATA[我正在学习用于机器学习的 Python 编码。当我尝试使用 sympy 求损失函数的导数时，它会与格式字符串发生冲突。
将 numpy 导入为 np
将 sympy 导入为 sp

def 预测（X，w，b）：
    返回 np.dot(X, w) + b

def 损失(X, w, b, Y):
    返回 np.mean((预测(X, w, b) - Y) ** 2)

X, Y = np.loadtxt(“code/02_first/pizza.txt”，unpack=True，skiprows=1)

# 将 X 和 Y 转换为 sympy 符号
X, w, b, Y = sp.symbols(“X w b Y”)

def 梯度(X, w, b, Y):
    loss_expr = 损失(X, w, b, Y)
    dw_dX = sp.diff(loss_expr, w)
    db_dX = sp.diff(loss_expr, b)
    返回 dw_dX, db_dX

def train(X, Y, 迭代, lr):
    w = sp.symbols(&#39;w&#39;)
    b = sp.symbols(&#39;b&#39;)
    
    对于范围内的 i（迭代）：
        损失值 = 损失(X, w, b, Y)
        print(f&quot;迭代: {i:4d}, 损失: {loss_value:.10f}&quot;)
        dw_dX, db_dX = 梯度(X, w, b, Y)
        w -= dw_dX * lr
        b -= db_dX * lr
    返回w,b

w, b = 训练(X, Y, 迭代=20000, lr=0.001)

print(f&quot;\nw = {w:.10f}, b = {b:.10f}&quot;)
print(f&quot;预测: x = 20 ==&gt; y = {predict(20, w, b):.2f}&quot;)

类型错误：传递给 Pow.__format__ 的格式字符串不受支持

数据位于 txt 中（或通过链接此处&lt; /a&gt;):
预订披萨
13 33
2 16
14 32
23 51
13 27
1 16
18 34
10 17
26 29
3 15
3 15
21 32
7月22日
22 37
2 13
27 44
6 16
10 21
18 37
15 30
9 26
26 34
8月23日
15 39
10 27
21 37
5 17
6 18
13 25
13 23

我可以只使用numpy，但这样做我需要自己计算损失函数，这不是有效的（并且很容易用括号引发错误）。
如果您能解释该错误以及为什么 sympy 与格式字符串不兼容，我们将不胜感激。另外，如何使用 sympy 生成正确的脚本？
提前非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/77891386/get-rid-of-format-string-conflict-when-using-sympy</guid>
      <pubDate>Sat, 27 Jan 2024 13:35:43 GMT</pubDate>
    </item>
    <item>
      <title>如何将对象检测数据集转换为 Tensorflow 数据集</title>
      <link>https://stackoverflow.com/questions/77891268/how-to-convert-object-detection-dataset-into-tensorflow-dataset</link>
      <description><![CDATA[我正在尝试使用 keras 在张量流上建立一个对象检测模型，但我一直遇到困难。我自动执行了查找训练图像的边界框的任务（训练数据集是游戏的），然后将所有内容转储到包含与其相关的所有数据的 .csv 文件中：对象出现的帧、边界框的坐标、和对象的类。即使同一帧上出现多个边界框，每个边界框在数据集上都有不同的行。
我正在尝试使用此函数将我的数据集导入 Tensorflow：
def Data_Loader(annotation_file):
    数据=pd.read_csv（注释文件）
    data_groups=data.groupby(&#39;文件名&#39;)

    数据集={&#39;images&#39;:[],&#39;bounding_boxes&#39;:[]}
    ngroups=data_groups.ngroups

    对于 image_name，data_groups 中的组：

        BBoxes={&#39;类&#39;:[],&#39;盒子&#39;:[]}
        对于 _，group.iterrows() 中的行：
            BBoxes[&#39;boxes&#39;].append(Get_BBOX(行))
            BBoxes[&#39;classes&#39;].append(class_ids.index(row[&#39;class&#39;]))

       
        数据集[&#39;bounding_boxes&#39;].append(tf.data.Dataset.from_tensor_slices(BBoxes))
        图像=load_img(图像名称,(224, 224))
        数据集[&#39;images&#39;].append(tf.constant(image))
 

    数据集=tf.data.Dataset.from_tensor_slices(数据集)

    返回（数据集）


以下是边界框的加载方式：
def Get_BBOX(行):
    xmin=int(行[&#39;xmin&#39;])
    ymin=int(行[&#39;ymin&#39;])
    xmax=int(行[&#39;xmax&#39;])
    ymax=int(行[&#39;ymax&#39;])

    bbox=np.array([xmin, ymin, xmax, ymax])

    返回bbox


图像加载如下：
def load_img(文件名, target_size):
    img = tf.keras.utils.load_img(文件名, target_size=target_size)
    img = tf.keras.utils.img_to_array(img)

    返回（图片）


我正在使用 Keras 的本教程来指导自己
但是每当我到达教程中将地图应用于数据的部分时，我都会收到以下错误消息：
“_VariantDataset”对象不可下标

有人知道我可能做错了什么以及如何解决它吗？
我多次尝试进行多次类型转换，从包含列表的字典更改为字典列表和所有其他类型的内容。这些都没有产生任何结果。]]></description>
      <guid>https://stackoverflow.com/questions/77891268/how-to-convert-object-detection-dataset-into-tensorflow-dataset</guid>
      <pubDate>Sat, 27 Jan 2024 12:55:36 GMT</pubDate>
    </item>
    <item>
      <title>对 MLH 奖学金代码示例有什么建议吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77890706/any-suggestions-for-mlh-fellowship-code-sample</link>
      <description><![CDATA[我计划今年申请 MLH 奖学金，我想展示一个强大的代码示例来展示我的全栈开发技能。我正在考虑的项目是任务管理器 Web 应用程序。该应用程序将包括用户身份验证、任务创建/编辑/删除、数据库集成、响应式用户界面和部署等功能。
我希望获得有关如何有效实施该项目的建议，以及能够给评选委员会留下深刻印象的任何具体技术或框架。此外，我们将非常感谢有关确保代码质量、安全实践以及任何可以使我的项目脱颖而出的额外功能的指导。
我的目标不仅仅是满足奖学金的要求，而是提供一个全面且精心制作的代码示例，反映我作为全栈开发人员的能力。经历过类似申请流程的经验丰富的开发人员提供的任何见解、技巧或建议都会非常有帮助。
提前感谢您的指导！]]></description>
      <guid>https://stackoverflow.com/questions/77890706/any-suggestions-for-mlh-fellowship-code-sample</guid>
      <pubDate>Sat, 27 Jan 2024 09:45:26 GMT</pubDate>
    </item>
    <item>
      <title>为什么 DecisionTreeClassifier 会按照指定的标准错误地分割数据？</title>
      <link>https://stackoverflow.com/questions/77890508/why-decisiontreeclassifier-split-wrongly-the-data-with-the-specified-criterion</link>
      <description><![CDATA[在第一次使用 DecisionTreeClassifier 时，我们得到了样本数为 192 和 346 的两个子树，但是当我们使用文件 Counter 并在 Treeclassifier 决策中设置与分离相同的条件时，我们得到了样本数 171 和 367。这种差异的标志是什么？
DecisionTreeClassifier 代码块：
导入 pandas 作为 pd
从 sklearn.tree 导入 DecisionTreeClassifier
从 sklearn 导入树
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np
数据 = pd.read_csv(r“PCOS.csv”)
X = data.drop(“PCOS (Y/N)”, axis=1)
y = 数据[“PCOS (Y/N)”]
模型= DecisionTreeClassifier（max_深度= 2，标准=“基尼”）
模型.fit(X, y)

树.plot_tree(模型)
fn = 数据.列

标签 = [“0”,“1”]
图，轴 = plt.subplots()
tree.plot_tree(模型，feature_names=fn，class_names=label，filled=True)
Fig.savefig(&#39;imagenae.png&#39;)


计数器代码块：
导入 pandas 作为 pd


def 子树（数据，列）：
    第一个列表 = []
    秒列表 = []
    对于范围内的 i（len（数据））：
        如果数据[col][i] &lt;= 7.5：
            first_list.append(data.loc[i, :].values)
        别的：
            sec_l​​ist.append(data.loc[i, :].values)
    基尼系数（第一个列表）
    基尼系数（秒列表）


定义基尼系数（数据）：
    a, b= 0, 0
    对于数据中的 i：
        如果 i[-1] == 0:
            一个+= 1
        别的：
            b+=1
    print(&quot;标签 0 :&quot;, a)
    print(&quot;标签 1 :&quot;, b)


col = [&#39;皮肤变黑(Y/N)&#39;, &#39;毛发生长(Y/N)&#39;, &#39;体重增加(Y/N)&#39;, &#39;周期(R/I)&#39;, &#39;毛囊编号(R)&#39; ,
       &#39;快餐（Y/N）&#39;、&#39;卵泡编号（L）&#39;、&#39;PCOS（Y/N）&#39;]

数据 = pd.read_csv(“PCOS.csv”)[col]

X = data.drop(“PCOS (Y/N)”, axis=1)
y = 数据[[“PCOS（是/否）”]]

subtree(data, &#39;毛囊编号 (L)&#39;)

结果决策树分类器：
192 和 346
结果计数器：
171 和 367
数据库：
数据库
可视化决策树：
可视化决策树]]></description>
      <guid>https://stackoverflow.com/questions/77890508/why-decisiontreeclassifier-split-wrongly-the-data-with-the-specified-criterion</guid>
      <pubDate>Sat, 27 Jan 2024 08:24:18 GMT</pubDate>
    </item>
    <item>
      <title>两阶段推荐系统中的top-K推荐</title>
      <link>https://stackoverflow.com/questions/77890505/top-k-recommendations-in-two-stage-recommendation-system</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77890505/top-k-recommendations-in-two-stage-recommendation-system</guid>
      <pubDate>Sat, 27 Jan 2024 08:22:43 GMT</pubDate>
    </item>
    <item>
      <title>我尝试使用 (pip install pickle5) 安装 Python 的 pickle 包，但安装包失败</title>
      <link>https://stackoverflow.com/questions/77890171/i-tried-installing-the-pickle-package-for-python-using-pip-install-pickle5-and</link>
      <description><![CDATA[这是我尝试过的：
pip install pickle5

这是包含错误消息的快照。
这也是我得到的错误：
错误：无法为 pickle5 构建轮子，这是安装基于 pyproject.toml 的项目所必需的
我尝试按照其他一些帖子中的建议安装并重新安装 Microsoft Visual C++，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/77890171/i-tried-installing-the-pickle-package-for-python-using-pip-install-pickle5-and</guid>
      <pubDate>Sat, 27 Jan 2024 05:36:19 GMT</pubDate>
    </item>
    <item>
      <title>为什么在小数据集上微调 MLP 模型，仍然保持与预训练权重相同的测试精度？</title>
      <link>https://stackoverflow.com/questions/77885918/why-finetuning-mlp-model-on-a-small-dataset-still-keeps-the-test-accuracy-same</link>
      <description><![CDATA[我设计了一个简单的 MLP 模型，在 6k 数据样本上进行训练。
类 MLP(nn.Module)：
    def __init__(自身,input_dim=92,hidden_​​dim=150,num_classes=2):
        超级().__init__()
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.hidden_​​dim = 隐藏_dim
        #self.softmax = nn.Softmax(dim=1)

        self.layers = nn.Sequential(
            nn.Linear(self.input_dim, self.hidden_​​dim),
            ReLU(),
            nn.Linear(self.hidden_​​dim, self.hidden_​​dim),
            ReLU(),
            nn.Linear(self.hidden_​​dim, self.hidden_​​dim),
            ReLU(),
            nn.Linear(self.hidden_​​dim, self.num_classes),

        ）

    def 前向（自身，x）：
        x = self.layers(x)
        返回x

并且模型已实例化
model = MLP(input_dim=input_dim,hidden_​​dim=hidden_​​dim,num_classes=num_classes).to(设备)

优化器= Optimizer.Adam(model.parameters(),lr=learning_rate,weight_decay=1e-4)
标准 = nn.CrossEntropyLoss()

和超参数：
num_epoch = 300 # 200e3//len(train_loader)
学习率 = 1e-3
批量大小 = 64
设备 = torch.device(“cuda”)
种子 = 42
火炬.manual_seed(42)

我的实现主要遵循这个问题。我将模型保存为预训练权重 model_weights.pth。
测试数据集上模型的准确率为96.80%。
然后，我还有另外 50 个样本（在 finetune_loader 中），我正在尝试在这 50 个样本上微调模型：
model_finetune = MLP()
model_finetune.load_state_dict(torch.load(&#39;model_weights.pth&#39;))
model_finetune.to（设备）
model_finetune.train()
# 训练网络
对于 t in tqdm(范围(num_epoch))：
  对于 i，enumerate(finetune_loader, 0) 中的数据：
    #def 闭包():
      # 获取并准备输入
      输入、目标 = 数据
      输入，目标=输入.float(), 目标.long()
      输入，目标 = 输入.to(设备), 目标.to(设备)
      
      # 将梯度归零
      优化器.zero_grad()
      # 执行前向传递
      输出 = model_finetune(输入)
      # 计算损失
      损失=标准（输出，目标）
      # 执行向后传递
      loss.backward()
      #回波损耗
      优化器.step() # a

model_finetune.eval()
使用 torch.no_grad()：
    输出2 = model_finetune(测试数据)
    #predicted_labels =outputs.squeeze().tolist()

    _, preds = torch.max(输出2, 1)
    Prediction_test = np.array(preds.cpu())
    准确度测试微调 = 准确度得分（y_测试，预测测试）
    精度测试微调
    
    输出：0.9680851063829787

我检查过，精度与将模型微调到 50 个样本之前保持不变，并且输出概率也相同。
可能是什么原因？我在微调代码中是否犯了一些错误？]]></description>
      <guid>https://stackoverflow.com/questions/77885918/why-finetuning-mlp-model-on-a-small-dataset-still-keeps-the-test-accuracy-same</guid>
      <pubDate>Fri, 26 Jan 2024 11:32:05 GMT</pubDate>
    </item>
    </channel>
</rss>