<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 20 Jan 2024 09:13:15 GMT</lastBuildDate>
    <item>
      <title>如何做出你的第一个开源贡献？ - 我有丰富的编程知识，但我从未参加过在线活动[关闭]</title>
      <link>https://stackoverflow.com/questions/77850403/how-to-make-your-first-open-source-contribution-i-have-substantial-programmin</link>
      <description><![CDATA[我正在寻找有关如何为项目做出开源贡献的步骤，以改善我的 github 个人资料并为更大的编码社区做出贡献。
我尝试在 YouTube 上搜索要遵循的指南，但没有得到适当的步骤和指导。我期待着一个详细的分步视频，介绍什么是开源、如何参与和承认开源、如何为项目做出贡献，同时提高我的编程专业知识。]]></description>
      <guid>https://stackoverflow.com/questions/77850403/how-to-make-your-first-open-source-contribution-i-have-substantial-programmin</guid>
      <pubDate>Sat, 20 Jan 2024 09:09:37 GMT</pubDate>
    </item>
    <item>
      <title>a决策树回归和决策树分类器之间的区别</title>
      <link>https://stackoverflow.com/questions/77850372/adifference-between-decision-tree-regression-and-decision-tree-classifier-proper</link>
      <description><![CDATA[当输出变量是分类变量时使用分类树，而当输出变量是连续变量时使用回归树。我知道这是正确的，但我对这个主题还没有清楚的了解，如果您帮助我举例，请帮助我更好地获得最佳答案。]]></description>
      <guid>https://stackoverflow.com/questions/77850372/adifference-between-decision-tree-regression-and-decision-tree-classifier-proper</guid>
      <pubDate>Sat, 20 Jan 2024 09:00:05 GMT</pubDate>
    </item>
    <item>
      <title>在拥抱脸上部署机器学习模型</title>
      <link>https://stackoverflow.com/questions/77850134/deploy-machine-learning-model-on-hugging-face</link>
      <description><![CDATA[任何人都可以帮助我部署在拥抱脸上使用 Streamlit 构建的机器学习模型吗？
我上传了app.py文件、.hdf5模型和requirements.txt文件。我收到构建错误。总是显示找不到 TensorFlow 模块。我包含了我在项目中使用的所有包。
所以我希望有人指导我解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/77850134/deploy-machine-learning-model-on-hugging-face</guid>
      <pubDate>Sat, 20 Jan 2024 07:20:44 GMT</pubDate>
    </item>
    <item>
      <title>对于特征工程师来说，有哪些好主意可以为分类模型创建与目标特征的更多相关性？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77848799/what-are-good-ideas-to-feature-engineer-features-to-create-more-correlation-with</link>
      <description><![CDATA[我正在开发一个分类模型，可以对客户进行分类，判断他们是否会成功付款或未能付款。
我尝试了 Scikit Learn 分类模型，但它的准确率略高于 50%。然后，我研究了 TensorFlow 分类模型，并获得了 67% 的准确率分数。
我正在努力提高准确率，希望模型的准确率能够达到 90% 以上。
我注意到的主要问题是没有一个特征与目标特征具有高相关性得分。
这是我尝试过的：
&lt;前&gt;&lt;代码&gt;df5.corr()

结果是

我尝试了多个 TensorFlow 模型，最高准确度得分为 67%
tf.random.set_seed(42)

log_model_8 = tf.keras.Sequential([
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(50, 激活 = &#39;relu&#39;),
    tf.keras.layers.Dense(1)
]）

log_model_8.compile(loss = tf.keras.losses.BinaryCrossentropy(),
                   优化器= tf.keras.optimizers.Adam(),
                   指标 = [&#39;准确性&#39;])

log_model_8.fit(X,y, epochs = 100, verbose = 1)

log_model_8.evaluate(X,y)


在这种情况下有什么好主意，如何才能提高模型准确率达到 90% 以上？]]></description>
      <guid>https://stackoverflow.com/questions/77848799/what-are-good-ideas-to-feature-engineer-features-to-create-more-correlation-with</guid>
      <pubDate>Fri, 19 Jan 2024 21:03:11 GMT</pubDate>
    </item>
    <item>
      <title>如何将字符串转换为浮点数，dtype='numeric' 与字节/字符串数组不兼容。将数据显式转换为数值</title>
      <link>https://stackoverflow.com/questions/77848723/how-to-convert-string-to-float-dtype-numeric-is-not-compatible-with-arrays-of</link>
      <description><![CDATA[将 pandas 导入为 pd
从 sklearn.tree 导入 DecisionTreeClassifier
从sklearn导入预处理

cols = [&#39;国家&#39;, &#39;人口&#39;, &#39;中位数年龄&#39;]
col_types = {&#39;国家&#39;：str，&#39;人口&#39;：int，&#39;median_age&#39;：int}
数据库 = pd.read_csv(&#39;dataset.csv&#39;, dtype=col_types)

le = 预处理.LabelEncoder()
数据库[&#39;国家&#39;] = le.fit_transform(数据库[&#39;国家&#39;])

X = (数据库[[&#39;国家&#39;]])
y = database.drop(列=[&#39;国家&#39;])

模型 = DecisionTreeClassifier()
model.fit(X.值, y.值)
prevision = model.predict([[&#39;意大利&#39;]])
打印（预览）

我正在尝试使用机器学习编写一个程序，该程序以一个国家的名称作为唯一输入，返回中位年龄以及该国家有多少居民。问题是拟合和预测函数需要浮点数，但我的输入是字符串，因此我尝试使用标签编码器对其进行转换，但收到此错误：
dtype=&#39;numeric&#39; 与字节/字符串数组不兼容。
明确地将数据转换为数值

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77848723/how-to-convert-string-to-float-dtype-numeric-is-not-compatible-with-arrays-of</guid>
      <pubDate>Fri, 19 Jan 2024 20:44:26 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 中具有多个层的简单 RNN，用于顺序预测</title>
      <link>https://stackoverflow.com/questions/77848436/simple-rnn-with-more-than-one-layer-in-pytorch-for-squential-prediction</link>
      <description><![CDATA[我得到了连续的时间序列数据。在每个时间戳，只有一个变量可供观察（如果我的理解是正确的，这意味着特征数量 = 1）。我想训练一个具有多个层的简单 RNN 来预测下一个观察结果。
我使用滑动窗口创建了训练数据，窗口大小设置为8。为了给出具体的想法，下面是我的原始数据、训练数据和目标。
示例数据
0.40 0.82 0.14 0.01 0.98 0.53 2.5 0.49 0.53 3.37 0.49
训练数据
&lt;前&gt;&lt;代码&gt;X =
    0.40 0.82 0.14 0.01 0.98 0.53 2.5 0.49
    0.82 0.14 0.01 0.98 0.53 2.5 0.49 0.53
    0.14 0.01 0.98 0.53 2.5 0.49 0.53 3.37


对应的目标是
&lt;前&gt;&lt;代码&gt;Y =
     0.53
     3.37
     0.49

我将批量大小设置为 3。但它给了我一个错误
运行时错误：input.size(-1) 必须等于 input_size。期望 8，得到 1
导入火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
导入 torch.utils.data 作为数据
将 numpy 导入为 np

X = np.array( [ [0.40, 0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49], [0.82, 0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53], [0.14, 0.01, 0.98, 0.53, 2.5, 0.49, 0.53, 3.37] ], dtype=np.float32)

Y = np.array([[0.53], [3.37], [0.49]], dtype=np.float32)

类 RNNModel(nn.Module):
    def __init__(self, input_sz, n_layers):
        超级（RNNModel，自我）.__init__()
        self.hidden_​​dim = 3*input_sz
        self.n_layers = n_layers
        输出大小 = 1
        self.rnn = nn.RNN（input_sz，self.hidden_​​dim，num_layers = n_layers，batch_first = True）
        self.线性 = nn.Linear(self.hidden_​​dim, output_sz)

    def 前向（自身，x）：
        batch_sz = x.size(0)
        hide = torch.zeros(self.n_layers, batch_sz, self.hidden_​​dim) #初始化n_layer*batch_sz维度的隐藏状态数hidden_​​dim)
        out, 隐藏 = self.rnn(x, 隐藏)
        out = out.contigious().view(-1, self.hidden_​​dim)
        返回，隐藏

device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
模型 = RNNModel(8,2)
X = torch.tensor(X[:,:,np.newaxis])
Y = torch.tensor(Y[:,:,np.newaxis])
X = X.to(设备)
Y = Y.to(设备)
模型 = model.to(设备)
优化器 = optim.Adam(model.parameters())
loss_fn = nn.MSELoss()

加载器= data.DataLoader（data.TensorDataset（X，Y），shuffle=False，batch_size=3）

n_epoch = 10
对于范围内的历元（n_epoch）：
    模型.train()
    对于加载器中的 X_batch、Y_batch：
        Y_pred = 模型(X_batch)
        损失 = loss_fn(Y_pred,Y_batch)
        优化器.zero_grad()
        loss.backward()
        优化器.step()

    如果纪元 % 10 != 0:
        继续
        模型.eval()
        使用 torch.no_grad()：
            Y_pred = 模型(X)
            train_rmse = np.sqrt(loss_fn(Y_pred,Y))
        print(“纪元 %d: 训练 RMSE %.4f” % (纪元, train_rmse))


我做错了什么？谁能帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77848436/simple-rnn-with-more-than-one-layer-in-pytorch-for-squential-prediction</guid>
      <pubDate>Fri, 19 Jan 2024 19:36:58 GMT</pubDate>
    </item>
    <item>
      <title>如果新的交叉验证迭代出现，是否需要重新创建神经网络层？</title>
      <link>https://stackoverflow.com/questions/77847336/whether-should-be-the-neuron-network-layers-recreated-if-the-new-iteration-of-cr</link>
      <description><![CDATA[我有一个关于交叉验证的问题。
k=5，
将有 4/5 训练数据集和 1/5 验证数据集
 data = np.concatenate([self.training_data1, self.training_data2], axis=1)
        
        kf = KFold(n_splits=k, shuffle=True) # k 折叠交叉验证
        kf.get_n_splits(data) # 返回交叉验证器中的分割迭代次数
     
        损失CV = 0
        val_loss_cv = 0
    
        对于 kf.split(data) 中的 train_index、val_index：
            
            logging.info(f“train_index:{train_index.shape}”)
            logging.info(f&quot;val_index:{val_index.shape}&quot;)

            self.is_train = tf.Variable(initial_value=True, trainable=False, dtype=tf.bool, name=“is_train”)
   
            loss_cv, val_loss_cv = self.train(train_index, val_index)
            loss_cv += loss_cv
            val_loss_cv += val_loss_cv

loss_cv = loss_cv / k
val_loss_cv = val_loss_cv / k

每次迭代都有新的train_index、val_index。（例如在train_index0、val_index0之后，下一次迭代将从train_index1、val_index1开始）
这些数据集将被加载到函数 self.train(train_index, val_index) 中。
在 train() 函数中，有一个使用神经元网络层创建的自动编码器层。
当新的交叉验证索引（train_index1，val_index1）在新的迭代中出现时，是否应该使用新的初始权重和偏差重新创建新的神经元网络层？
如果我在神经元网络层中使用继承的权重和偏差，而不是创建新的神经元网络层，结果是否会导致过拟合？]]></description>
      <guid>https://stackoverflow.com/questions/77847336/whether-should-be-the-neuron-network-layers-recreated-if-the-new-iteration-of-cr</guid>
      <pubDate>Fri, 19 Jan 2024 15:56:07 GMT</pubDate>
    </item>
    <item>
      <title>使用 spaCy 模型对文本中的实体进行分类但不定位实体</title>
      <link>https://stackoverflow.com/questions/77847222/classifying-but-not-locating-entities-within-a-text-with-spacy-model</link>
      <description><![CDATA[有没有办法“强制”经过训练的 spaCy 模型对文本中的给定字符串进行分类，而不是定位？
就我而言，我想构建一个 NER 模型，将文本中的地名分类为特定类别，例如“城市”或“国家”。所以模型不需要定位地名。对于每个文本文件，它需要处理其中的地名列表、字典等作为参数，然后根据周围的文本为该地名分配最可能的类。
此外，是否有一个选项可以让 spaCy“忽略”地名的各个字符，而仅通过查看周围的文本来训练模型？
如果我尝试在不指定标签的情况下设置实体，则 print(doc.ents) 返回一个空集。
doc = nlp(“德国距离日本比莫斯科更远”)

doc.set_ents([跨度(doc, 0, 1), 跨度(doc, 4, 5), 跨度(doc, 6, 7)])

打印（文档）
对于 doc.ents 中的 ent：
    打印（ent.text，ent.label_）
]]></description>
      <guid>https://stackoverflow.com/questions/77847222/classifying-but-not-locating-entities-within-a-text-with-spacy-model</guid>
      <pubDate>Fri, 19 Jan 2024 15:36:34 GMT</pubDate>
    </item>
    <item>
      <title>Xgboost算法问题文件为空</title>
      <link>https://stackoverflow.com/questions/77843515/xgboost-algorithm-issue-file-empty</link>
      <description><![CDATA[我尝试使用 1.7-1 版本的 Xgboost 算法训练数据集。调用 Xgboost 函数时，它会抛出如下错误。
2024-01-19:02:57:27:INFO] 导入框架 sagemaker_xgboost_container.training
[2024-01-19:02:57:27:INFO] 未检测到 GPU（如果未安装 GPU，则正常）
[2024-01-19:02:57:27:INFO] 调用用户培训脚本。
[2024-01-19:02:57:27:错误] 报告培训失败
[2024-01-19:02:57:27:ERROR] 框架错误：
回溯（最近一次调用最后一次）：
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 2318 行，下一个
    tarinfo = self.tarinfo.fromtarfile(self)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1105 行，fromtarfile
    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1041 行，frombuf 中
    raise EmptyHeaderError(“空标题”)
tarfile.EmptyHeaderError：空标头
在处理上述异常的过程中，又出现了一个异常：
回溯（最近一次调用最后一次）：
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_trainer.py”，第 84 行，列车中
    入口点（）
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py”，第 102 行，在 main 中
    火车（框架.training_env（））
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py”，第 87 行，训练中
    框架.模块.run_module(
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_modules.py”，第 290 行，在 run_module 中
    _files.download_and_extract(uri, _env.code_dir)
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_files.py”，第 131 行，位于 download_and_extract 中
    使用 tarfile.open(name=dst, mode=“r:gz”) 作为 t：
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1621 行，打开
    返回 func(名称、文件模式、fileobj、**kwargs)
  gzopen 中的文件“/miniconda3/lib/python3.8/tarfile.py”，第 1674 行
    t = cls.taropen(名称、模式、fileobj、**kwargs)
  taropen 中的文件“/miniconda3/lib/python3.8/tarfile.py”，第 1651 行
    返回 cls(名称、模式、fileobj、**kwargs)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1514 行，位于 __init__ 中
    self.firstmember = self.next()
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 2333 行，在下一个
    引发 ReadError(“空文件”)
tarfile.ReadError：空文件
空的文件

我有两个具有相同结构且扩展名为 .csv 的源文件。
我不知道为什么它抱怨 tar 文件为空]]></description>
      <guid>https://stackoverflow.com/questions/77843515/xgboost-algorithm-issue-file-empty</guid>
      <pubDate>Fri, 19 Jan 2024 03:10:14 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：给定 groups=1，权重大小为 [128, 64, 4, 4]，预期输入 [1, 128, 65, 65] 有 64 个通道，但得到了 128 个通道</title>
      <link>https://stackoverflow.com/questions/77843263/runtimeerror-given-groups-1-weight-of-size-128-64-4-4-expected-input1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77843263/runtimeerror-given-groups-1-weight-of-size-128-64-4-4-expected-input1</guid>
      <pubDate>Fri, 19 Jan 2024 01:31:36 GMT</pubDate>
    </item>
    <item>
      <title>不平衡数据的隔离森林和SHAP过程</title>
      <link>https://stackoverflow.com/questions/77837871/isolation-forest-and-shap-process-for-imbalanced-data</link>
      <description><![CDATA[我想对类别为正常 99.93% 异常 0.07% 的数据使用隔离森林，并使用 SHAP 检查异常数据特征之间的相关性。
于是，我参考了下面kaggle网站上的方法继续学习。
kaggle
在这个 Kaggle 站点上，Class = 0 的数据和 Class = 1 的数据划分如下：
inliers = df[df.Class==0]
ins = inliers.drop([&#39;Class&#39;], axis=1)

离群值 = df[df.Class==1]
outs = outliers.drop([&#39;Class&#39;], axis=1)

为了查看学习中使用的特征与异常值（“Class == 1”的数据）之间的相关性，我按如下方式使用了 SHAP，并通过蜂群图检查了相关性。
&lt;前&gt;&lt;代码&gt;状态= 42
ISF = 隔离森林（random_state=状态）
ISF.fit(ins)

normal_isf = ISF.predict(ins)
欺诈_isf = ISF.predict(outs)

导入形状
解释器 = shap.TreeExplainer(ISF)
shap_values = 解释器(outs)
shap.plots.beeswarm(shap_values)

代码工作正常，但是 beeswarn 的结果与我使用 shap_values ​​=explainer(ins) 时类似，即正常数据。我犯错了吗？如果您能告诉我是否有任何需要改进的地方，我将非常感激。]]></description>
      <guid>https://stackoverflow.com/questions/77837871/isolation-forest-and-shap-process-for-imbalanced-data</guid>
      <pubDate>Thu, 18 Jan 2024 08:20:12 GMT</pubDate>
    </item>
    <item>
      <title>在小型训练数据集上训练的文本转语音模型</title>
      <link>https://stackoverflow.com/questions/77406851/text-to-speech-model-that-trains-on-small-training-dataset</link>
      <description><![CDATA[我需要一个模型，可以使用包含转录本和最多 20 个句子的 wav 文件的数据集进行训练。
我尝试在这样的情况下训练 https://github.com/coqui-ai/TTS数据集，它根本没有训练得很好。这个推论只是噪音而不是文字。
我正在研究 https ://github.com/microsoft/SpeechT5/tree/main/SpeechLM#pre-trained-and-fine-tuned-models 但他们使用的微调数据集似乎也有超过 100 小时的音频内容。
解决这个问题的最佳研究模型是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77406851/text-to-speech-model-that-trains-on-small-training-dataset</guid>
      <pubDate>Thu, 02 Nov 2023 04:00:19 GMT</pubDate>
    </item>
    <item>
      <title>面对强化学习的问题</title>
      <link>https://stackoverflow.com/questions/76695094/facing-a-problem-with-reinforcement-learning</link>
      <description><![CDATA[导入健身房
从 stable_baselines3 导入 A2C

env =gym.make(&#39;LunarLander-v2&#39;, render_mode=&quot;人类&quot;)
env.reset()

模型 = A2C(“MlpPolicy”, env, verbose=1)
model.learn(total_timesteps=1000)

集数 = 10

对于范围内的 ep（剧集）：
    obs = env.reset()
    完成=假
    虽然没有完成：
        动作、_states、_episode、_determ = model.predict(obs)
        obs、奖励、完成、info = env.step(action)
        env.render()

env.close()

我上面的代码产生以下输出：
DeprecationWarning：“np.bool8”是“np.bool_”的已弃用别名。 （已弃用 NumPy 1.24）
  如果不是 isinstance(终止, (bool, np.bool8)):
------------------------------------------------
|推出/ | |
| ep_len_mean | 89.2 | 89.2
| ep_rew_mean | -227 | -227
|时间/| |
|帧率 | 43 | 43
|迭代| 100 | 100
|已用时间 | 11 | 11
|总时间步数 | 500 | 500
|火车/ | |
|熵损失 | -1.29 | -1.29
|解释方差 | -0.0216 | -0.0216
|学习率 | 0.0007 | 0.0007
| n_更新 | 99 | 99
|政策损失 | 2.79 | 2.79
|价值损失 | 12.3 | 12.3
------------------------------------------------
------------------------------------------------
|推出/ | |
| ep_len_mean | 107 | 107
| ep_rew_mean | -209 | -209
|时间/| |
|帧率 | 45 | 45
|迭代| 200 | 200
|已用时间 | 21 | 21
|总时间步数 | 1000 | 1000
|火车/ | |
|熵损失 | -0.864 |
|解释方差 | -0.00161 | -0.00161
|学习率 | 0.0007 | 0.0007
| n_更新 | 199 | 199
|政策损失 | -16.6 | -16.6
|价值损失 | 228 | 228

随后出现此错误：
&lt;前&gt;&lt;代码&gt;------------------------------------
回溯（最近一次调用最后一次）：
  文件“c:\Appu\Courses\Fun items\Reinforcement Learning\c1.py”，第 17 行，位于  中。
    动作、_states、_episode、_determ = model.predict(obs)
                                         ^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\sarav\AppData\Local\Programs\Python\Python311\Lib\site-packages\stable_baselines3\common\base_class.py”，第 555 行，在预测中
    返回 self.policy.predict（观察、状态、episode_start、确定性）
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\sarav\AppData\Local\Programs\Python\Python311\Lib\site-packages\stable_baselines3\common\policies.py”，第 346 行，在预测中
    观察，vectorized_env = self.obs_to_tensor(观察)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  文件“C:\Users\sarav\AppData\Local\Programs\Python\Python311\Lib\site-packages\stable_baselines3\common\policies.py”，第 260 行，位于 obs_to_tensor 中
    观察 = np.array(观察)
                  ^^^^^^^^^^^^^^^^^^^^^^^
ValueError：使用序列设置数组元素。请求的数组在 1 维之后具有不均匀的形状。检测到的形状为(2,)+不均匀部分。

当我运行代码时，它会运行几个时间步，然后退出并出现上述错误。有什么解决办法吗？]]></description>
      <guid>https://stackoverflow.com/questions/76695094/facing-a-problem-with-reinforcement-learning</guid>
      <pubDate>Sat, 15 Jul 2023 17:51:40 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost：如何使用带有 scikit-learn 接口 .fit 的 DMatrix</title>
      <link>https://stackoverflow.com/questions/76502318/xgboost-how-to-use-a-dmatrix-with-scikit-learn-interface-fit</link>
      <description><![CDATA[我目前在我的项目中使用 XGBoost 的 scikit-learn 接口。但是，我有一个非常大的数据集，每次调用 .fit 时，数据都会转换为 DMatrix，这非常耗时，尤其是在使用训练相对较快的 GPU 时。我使用本机接口对每次拟合使用单个 DMatrix 进行基准测试，结果显示出显着差异（每次拟合 14 秒与每次拟合 0.9 秒）。问题是，我需要一个 scikit-learn 模型，以便它可以与我的程序的其余部分配合使用。
有没有办法在 XGBoost 中将 DMatrix 与 scikit-learn 接口结合使用，或者有任何解决方法来避免重复转换为 DMatrix，同时仍保持与 scikit-learn 的兼容性？
请参阅下面的代码以获取导致此问题的可重现方法。
from sklearn.datasets import make_classification
从 xgboost 导入 XGBClassifier
将 xgboost 导入为 xgb

# 大型综合数据集
X, y = make_classification(n_samples=500_0000, n_features=20,
                           n_informative=10，n_redundant=10，random_state=42）

# scikit 学习
t = 时间.time()
模型 = XGBClassifier(tree_method=“gpu_hist”, gpu_id=0,
                      预测器=“gpu_predictor”，max_bin=256）
模型.fit(X, y)
print(&quot;scikit-learn 接口：&quot;, time.time() - t)

# 再次进行 scikit-learn
t = 时间.time()
模型.fit(X, y)
print(&quot;scikit-learn (2nd) 接口：&quot;, time.time() - t)

打印（）

#DMatrix
dtrain = xgb.DMatrix(数据=X,标签=y)
t = 时间.time()
model = xgb.train({“tree_method”: “gpu_hist”, “gpu_id”: 0,
                  “预测器”：“gpu_predictor”}，dtrain）
print(&quot;本机接口：&quot;, time.time() - t)

#再次DMatrix
t = 时间.time()
model = xgb.train({“tree_method”: “gpu_hist”, “gpu_id”: 0,
                  “预测器”：“gpu_predictor”}，dtrain）
print(“本机（第二）接口::”, time.time() - t)

输出：
scikit-learn 接口：14.393212795257568
scikit-learn（第二）接口：14.048950433731079

本机接口：3.9494242668151855
本机（第二）接口:: 0.9888997077941895

如您所见，scikit-learn 和 Native 之间存在很大的时间差异。]]></description>
      <guid>https://stackoverflow.com/questions/76502318/xgboost-how-to-use-a-dmatrix-with-scikit-learn-interface-fit</guid>
      <pubDate>Sun, 18 Jun 2023 19:56:25 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中处理 nan/null 的分类器</title>
      <link>https://stackoverflow.com/questions/30317119/classifiers-in-scikit-learn-that-handle-nan-null</link>
      <description><![CDATA[我想知道 scikit-learn 中是否有处理 nan/null 值的分类器。我认为随机森林回归器可以处理这个问题，但当我调用 predict 时出现错误。
X_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])
y_train = np.array([1, 2])
clf = RandomForestRegressor(X_train, y_train)
X_test = np.array([7, 8, np.nan])
y_pred = clf.predict(X_test) # 失败！

我不能使用任何带有缺失值的 scikit-learn 算法来调用预测吗？
编辑。
现在我想起来，这是有道理的。这在训练期间不是问题，但是当您预测变量为空时如何分支时？也许你可以将两种方式分开并平均结果？看来只要距离函数忽略空值，k-NN 就应该可以正常工作。
编辑 2（年长且聪明的我）
一些 gbm 库（例如 xgboost）使用三叉树而不是二叉树正是为了这个目的：2 个子节点用于是/否决策，1 个子节点用于缺失决策。 sklearn 使用二叉树&lt; /a&gt;]]></description>
      <guid>https://stackoverflow.com/questions/30317119/classifiers-in-scikit-learn-that-handle-nan-null</guid>
      <pubDate>Tue, 19 May 2015 05:02:35 GMT</pubDate>
    </item>
    </channel>
</rss>