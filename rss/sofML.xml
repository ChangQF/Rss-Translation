<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 28 Aug 2024 12:31:03 GMT</lastBuildDate>
    <item>
      <title>MLP 回归器和简单的 ANN 模型有什么区别？</title>
      <link>https://stackoverflow.com/questions/78923116/what-is-the-difference-between-mlp-regressor-and-a-simple-ann-model</link>
      <description><![CDATA[简单的 ANN 模型和 MLP 回归器之间有什么区别吗？它们不一样吗？
我尝试在我的时间序列数据集中使用它们，当我使用 R2 分数来评估模型在测试集上的性能时，它们产生了不同的结果。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78923116/what-is-the-difference-between-mlp-regressor-and-a-simple-ann-model</guid>
      <pubDate>Wed, 28 Aug 2024 11:36:19 GMT</pubDate>
    </item>
    <item>
      <title>ML Endpoint 在低流量期间延迟较高</title>
      <link>https://stackoverflow.com/questions/78922460/ml-endpoint-higher-latency-during-low-traffic</link>
      <description><![CDATA[在测试新的 ML 端点时，我们遇到了一个有点令人困惑的情况，在低流量时段，延迟从 200 毫秒 飙升至 1-3 秒。我们的模型有一个预处理步骤，然后是 XGBClassifier。我们正在使用托管在 AWS Sagemaker 上的 FastAPI 容器运行，并带有一个固定配置的实例。
基础设施/托管
我们在 AWS sagemaker 的一个中型实例上运行 fastapi 容器。这是一个固定实例，没有自动扩展。启动时，我们调用 uvicorn.run(&quot;invoke:app&quot;, etc.)。
我们的模型是什么/我们的调用端点做什么？
我们的模型对象是一个 sklearn Pipeline，由以下内容组成：

通过 FunctionTransformer 完成数据类型转换

通过 ColumnTransformer 进行特征工程

XGBClassifier


在我们的调用函数中，我们还有一个 TreeExplainer，我们使用它来获取每个预测的 SHAP 值。模型管道和 TreeExplainer 均在 asynccontextmanager 包装函数中实例化，该函数传递给应用程序生命周期：app = FastAPI(lifespan=load_model)
因此，本质上，我们的调用函数被定义为异步函数，它反序列化 JSON 有效负载，使用管道的前两个步骤预处理数据，使用 XGBClassifier 进行预测，然后在预处理的数据上运行解释器的 shap_values 方法。
我们的观察结果：
在生产环境中运行时，在正常工作时间内，我们可能会在任何给定时间点看到多个并发请求，端点看起来非常健康。 CPU 利用率在 20% 以下波动，内存利用率稳定在 8%，响应时间非常稳定，约为 200 毫秒。
当进入低营业时间时，您可能会长达一小时而看不到单个请求，我们看到响应时间超过整整 1 秒，有时甚至长达 3 秒。与此同时，内存利用率下降到 4-6%，然后在请求进入时回升到 8%。
此外，我们添加了一些分析，并在一定程度上复制了在本地 docker 容器上运行。我们看到，在让容器闲置一段时间后，第一个预测也需要一些额外的时间。但只有 ~800 毫秒。我们看到这段时间几乎全部花在 dtype 转换 + 执行 ColumnTransformer 上。
对我们来说，这一切都指向我们模型的“某些”部分，在空闲一段时间后从内存中删除。我无法判断这是由 python 的 gc、sklearn 库中的一些内部机制还是某些 fastapi 相关行为造成的。我们很好奇是否有人可以更好地理解为什么会发生这种情况，因此我们可以想出一个比通过定期在一些虚拟数据上使用它们来保持模型/解释器温暖更好的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78922460/ml-endpoint-higher-latency-during-low-traffic</guid>
      <pubDate>Wed, 28 Aug 2024 09:13:31 GMT</pubDate>
    </item>
    <item>
      <title>在没有 static_vectors 的情况下，是否可以在 en_core_web_lg 上训练 NER 模型？</title>
      <link>https://stackoverflow.com/questions/78921915/is-it-possible-to-train-ner-model-on-en-core-web-lg-without-static-vectors</link>
      <description><![CDATA[我正在尝试使用自定义标记来训练 NER 模型。它与 en_core_web_sm 模型配合得很好，但我正在尝试提高准确性，所以我现在尝试使用 en_core_web_lg。无论我做什么来禁用静态向量，它总是会抱怨：
RuntimeError：[E896] 使用静态向量时出错。确保词汇的向量已正确初始化，或将“include_static_vectors”设置为 False
我决定不使用静态向量，因为我必须使用自定义标记，而且由于我是新手，我决定干脆不使用它更好。这错了吗？
这是我用于大型模型的代码：
def train_model(train_data, output_dir, n_iter=50):
# 启用 GPU
spacy.require_gpu()

# 加载包括 tok2vec 的预训练模型，但不包括向量
nlp = spacy.load(&quot;en_core_web_lg&quot;, exclude=[&quot;vectors&quot;])

# 设置自定义标记器
nlp.tokenizer = custom_tokenizer(nlp)

# 确保没有组件使用静态向量
for component in nlp.pipe_names:
if hasattr(nlp.get_pipe(component), &quot;include_static_vectors&quot;):
nlp.get_pipe(component).include_static_vectors = False

# 检查 NER 管道是否存在，如果不存在则添加
if “ner” not in nlp.pipe_names:
ner = nlp.add_pipe(&quot;ner&quot;, last=True)
else:
ner = nlp.get_pipe(&quot;ner&quot;)

# 向 NER 模型添加标签
for example in train_data:
for ent in example.reference.ents:
ner.add_label(ent.label_)

# 使用正确的组件创建优化器
optimizer = nlp.create_optimizer()

for i in range(n_iter):
random.shuffle(train_data)
loss = {}
batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))
for batch in batches:
nlp.update(batch, drop=0.5, loss=losses, sgd=optimizer)
print(f&quot;Iteration {i + 1}/{n_iter}: Losses {losses}&quot;)

# 将模型保存到输出目录
if output_dir 不为 None:
nlp.to_disk(output_dir)
print(f&quot;将模型保存到 {output_dir}&quot;)

这会导致前面提到的运行时错误]]></description>
      <guid>https://stackoverflow.com/questions/78921915/is-it-possible-to-train-ner-model-on-en-core-web-lg-without-static-vectors</guid>
      <pubDate>Wed, 28 Aug 2024 06:51:52 GMT</pubDate>
    </item>
    <item>
      <title>我没有在 YOLOV8 模型中获得预期的结果</title>
      <link>https://stackoverflow.com/questions/78921876/i-am-not-getting-expected-results-in-yolov8-model</link>
      <description><![CDATA[最近，由于某些系统原因，我正在尝试研究 yolov8 模型，我已经在其他系统中训练了该模型，并在该系统中进行了检查。我在那里给出了预期的结果。
然后，我从该设备中取出训练好的模型，并尝试在我的设备上进行测试。
但它给出了意想不到的结果。
让我在这里分享一些结果：
72.0 576.0 168.0 960.0 1.0 0.0
264.0 576.0 792.0 1008.0 1.0 0.0
360.0 528.0 888.0 1008.0 1.0 0.0
456.0 528.0 984.0 1008.0 1.0 0.0
264.0 528.0 1080.0 1008.0 1.0 0.0
888.0 480.0 984.0 1008.0 1.0 0.0
936.0 480.0 1080.0 912.0 1.0 0.0 1080.0 480.0 1224.0 912.0 1.0 0.0 1176.0 432.0 1320.0 912.0 1.0 0.0 1224.0 480.0 1368.0 1056.0 1.0 0.0 1272 0 480.0 1416.0 1056.0 1.0 0.0 1320.0 480.0 1464.0 1056.0 1.0 0.0 1368.0 480.0 1704.0 1056.0 1.0 0.0 1464.0 480.0 1992.0 1056.0 1.0 0.0 1080.0 480.0 2040.0 1056.0 1.0 0.0 1512.0 480.0 2232.0 1056.0 1.0 0.0 1656.0 480.0 2376.0 1056.0 1.0 0.0 18 00.0 432.0 2520.0 1056.0 1.0 0.0 1944.0 432.0 2664.0 1056.0 1.0 0.0 2088.0 432.0 2808.0 1056.0 1.0 0.0 2232.0 432.0 2952.0 1056.0 1.0 0.0
2376.0 432.0 3096.0 1056.0 1.0 0.0
最后 2 个是分数和 classID，为什么每帧的分数都是 1.0？？？
我想知道为什么每帧的结果都显示为 1，以及如何解决。]]></description>
      <guid>https://stackoverflow.com/questions/78921876/i-am-not-getting-expected-results-in-yolov8-model</guid>
      <pubDate>Wed, 28 Aug 2024 06:41:20 GMT</pubDate>
    </item>
    <item>
      <title>unstructured.document.html 中的 ModuleNotFound 错误</title>
      <link>https://stackoverflow.com/questions/78921827/modulenotfound-error-in-unstructured-document-html</link>
      <description><![CDATA[我正在执行此代码
from unstructured.documents.html import HTMLDocument

# 加载您的 HTML 文件
html_file_path = &#39;UBER_2019.html&#39;
doc = HTMLDocument.from_file(html_file_path)

# 提取文本
text = doc.text


我收到一个错误，它是
ModuleNotFoundError Traceback (most recent call last)
Cell In[3], line 1
----&gt; 1 from unstructured.documents.html import HTMLDocument
3 # 加载您的 HTML 文件
4 html_file_path = &#39;UBER_2019.html&#39;

ModuleNotFoundError: 没有名为 &#39;unstructured.documents.html&#39; 的模块

那么我该怎么做才能解决这个问题呢]]></description>
      <guid>https://stackoverflow.com/questions/78921827/modulenotfound-error-in-unstructured-document-html</guid>
      <pubDate>Wed, 28 Aug 2024 06:28:50 GMT</pubDate>
    </item>
    <item>
      <title>应用模型预测时出现属性错误[关闭]</title>
      <link>https://stackoverflow.com/questions/78921545/attribut-error-during-applying-predicition-for-the-model</link>
      <description><![CDATA[代码
arr=[[90,42,43,20.879744,82.002744,6.502985,202.935536]]
y_predict=app.predict(arr)
y_predict

第 2 行代码出错
y_predict=app.predict(arr)
AttributeError: &#39;tuple&#39; 对象没有属性 &#39;predict&#39;

第 2 行代码出错了什么]]></description>
      <guid>https://stackoverflow.com/questions/78921545/attribut-error-during-applying-predicition-for-the-model</guid>
      <pubDate>Wed, 28 Aug 2024 04:31:39 GMT</pubDate>
    </item>
    <item>
      <title>预言机预测未反映外部回归量的预期行为</title>
      <link>https://stackoverflow.com/questions/78921509/prophet-forecast-not-reflecting-expected-behavior-with-external-regressors</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78921509/prophet-forecast-not-reflecting-expected-behavior-with-external-regressors</guid>
      <pubDate>Wed, 28 Aug 2024 04:13:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 pytorch 训练 LSTM 时，稍微增加批量大小会导致 CPU 使用率异常</title>
      <link>https://stackoverflow.com/questions/78921011/unusual-cpu-usage-from-slightly-increasing-batch-size-for-training-an-lstm-with</link>
      <description><![CDATA[我正在尝试使用 GPU 通过 pytorch 训练一个简单的 LSTM 模型。输入大小约为 11KB（与目标大小相同）。当我以 32 的批处理大小运行训练循环时，一切似乎都运行良好。CPU 使用率低于 10%（尽管感觉有点过高），GPU 使用率约为 30%……
当我将批处理大小增加到 48 时，CPU 使用率上升到 100%，一切都变慢了（训练时间增加了一倍）。GPU 使用率几乎没有变化，约为 30%。我检查了批处理大小为 40 的情况，这似乎没问题。在 48 时，似乎发生了一些变化并触发了持续的 CPU 使用率。其他一切似乎都很好（剩余大量 RAM，几乎没有任何磁盘活动等）。
关于如何解决此问题的任何建议或想法？在批处理大小为 48 时，总输入大小应约为 528KB。不确定这是否会在某个地方触发某种阈值。任何帮助都将不胜感激。
这是在 Windows 11 上使用 NVIDIA GPU 进行的。我希望训练能够顺利进行，直到批次大小比我看到的要大得多。]]></description>
      <guid>https://stackoverflow.com/questions/78921011/unusual-cpu-usage-from-slightly-increasing-batch-size-for-training-an-lstm-with</guid>
      <pubDate>Tue, 27 Aug 2024 23:03:51 GMT</pubDate>
    </item>
    <item>
      <title>学习LSTM神经网络的问题</title>
      <link>https://stackoverflow.com/questions/78920308/the-problem-with-learning-the-lstm-neural-network</link>
      <description><![CDATA[我需要创建一个模型来预测某些信号随时间变化的误差，即解决时间序列回归问题。我为此使用 LSTM：
class MyLSTM(nn.Module):
def __init__(self, num_classes, input_size, hidden_​​size, num_layers, seq_length):
super(MyLSTM, self).__init__()
self.num_classes = num_classes 
self.num_layers = num_layers 
self.input_size = input_size 
self.hidden_​​size = hidden_​​size 
self.seq_length = seq_length 

self.lstm = nn.LSTM(input_size = input_size, hidden_​​size = hidden_​​size, num_layers = num_layers, batch_first = True) 
self.fc_1 = nn.Linear(hidden_​​size, 128) 
self.fc_2 = nn.Linear(128, num_classes) 
self.activatin_func = nn.Tanh()

def forward(self, x):
h_0 = 变量(torch.zeros(self.num_layers, x.size(0), self.hidden_​​size)) 
c_0 = 变量(torch.zeros(self.num_layers, x.size(0), self.hidden_​​size)) 
out, (hn, cn) = self.lstm(x, (h_0, c_0)) 
hn = hn.view(-1, self.hidden_​​size) 
out = self.activatin_func(hn)
out = self.fc_1(out) 
out = self.fc_2(out) 
out = self.activatin_func(out)
return out

超参数：
num_epochs = 1000 
learning_rate = 0.01 

input_size = X.shape[1] 
num_classes = y.shape[1] 

hidden_​​size = 16 
num_layers = 1 


结果，我在训练阶段得到了模型估计的相当准确的近似值，而在测试阶段得到了差异。我觉得这很奇怪，因为训练非常成功，并且评估函数在预测阶段的值没有急剧跳跃，评估曲线应该足够接近真实值，但在实践中我得到了急剧跳跃，我不明白其性质。
可能是什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/78920308/the-problem-with-learning-the-lstm-neural-network</guid>
      <pubDate>Tue, 27 Aug 2024 18:29:17 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：'DataLoader' 对象在 SuperGradients Trainer 中不可下标</title>
      <link>https://stackoverflow.com/questions/78917847/typeerror-dataloader-object-is-not-subscriptable-in-supergradients-trainer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78917847/typeerror-dataloader-object-is-not-subscriptable-in-supergradients-trainer</guid>
      <pubDate>Tue, 27 Aug 2024 08:28:38 GMT</pubDate>
    </item>
    <item>
      <title>Fairmot 重新实现 reid 过度拟合？</title>
      <link>https://stackoverflow.com/questions/78916378/fairmot-reimplementation-reid-overfitting</link>
      <description><![CDATA[我正在以非常简化的方式重新实现 Fairmot，我很难理解为什么在 mot17、prw、cuhksysu 和部分 caltech 行人上进行训练时，reid 分支似乎立即过度拟合，而训练损失似乎以正常方式减少。
我想到了一些原因：

我的数据比原始项目少，所以模型在 reid 上过度拟合

我正在对他们的工作进行简化的增强，没有旋转，没有裁剪，所以这意味着我的数据更少，或者至少我的数据集中的变化更少

我搞砸了 id 损失，但我几乎复制粘贴了你的函数，所以

我使用的模型有点不同，可能对于任务来说太简单了（一直被教导更简单-&gt;更少过度拟合，无论如何我使用双线性插值进行上采样而不是逆卷积，因为我无法消除棋盘效应）

验证代码实现得很糟糕，但我试图在训练集上进行验证，结果与训练损失一致

我搞砸了一些我无法理解的东西


我在这里发布了我正在重新实现模型的 colab 笔记本。
此外，如果我的代码是正确的，我正在模型部分加载在 crowdhuman 上的预训练，并且我已经修改了数据集以具有不重叠的 id。
这是我的 colab，Google 限制了数据集的下载次数，无论如何它应该可以正常运行。
https://colab.research.google.com/drive/1EVR3S6Qd0sFeRbhCYiZo5xtPZXYHsOvA?usp=sharing
这是我使用的 id 损失：
class IdLoss(nn.Module):
def init(self,n_id):
super(IdLoss, self).init()
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
self.id_classifier = nn.Linear(id_vect_size, int(n_id+1),device=device)
self.cross_entr_loss=nn.CrossEntropyLoss(ignore_index=-1)
self.dropout = nn.Dropout(0.3)

def forward(self, id_output, ind_target, mask_target, id_targets):
id_vectors = gather_feat(id_output, ind_target)
id_vectors = id_vectors[mask_target &gt; 0].contiguous()
id_vectors = F.normalize(id_vectors)
id_targets = id_targets[mask_target &gt; 0].contiguous()
id_logits = self.id_classifier(id_vectors)

return self.cross_entr_loss(id_logits, id_targets)

我尝试了空间 dropout、限制 reid 向量大小、添加增强、不同的数据集大小（但最多 45000 张图像）我添加了 50% 的对原始图像执行增强的可能性，我简化了模型，改变了学习率。无法使 reid 损失表现良好。]]></description>
      <guid>https://stackoverflow.com/questions/78916378/fairmot-reimplementation-reid-overfitting</guid>
      <pubDate>Mon, 26 Aug 2024 21:44:44 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow-federated 0.86.0.. AttributeError: 模块‘tensorflow_federated.python.learning’没有属性‘build_federated_averaging_process [关闭]</title>
      <link>https://stackoverflow.com/questions/78916201/tensorflow-federated-0-86-0-attributeerror-module-tensorflow-federated-pytho</link>
      <description><![CDATA[--------------------------------------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
&lt;ipython-input-21-ab950bcb167c&gt; 在 &lt;cell line: 1&gt;()
----&gt; 1 trainer = tff.learning.build_federated_averaging_process(
2 model_fn,
3 client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),
4 server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.05)#learning_rate=0.01
5 )

AttributeError: 模块“tensorflow_federated.python.learning”没有属性“build_federated_averaging_process”

我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78916201/tensorflow-federated-0-86-0-attributeerror-module-tensorflow-federated-pytho</guid>
      <pubDate>Mon, 26 Aug 2024 20:32:53 GMT</pubDate>
    </item>
    <item>
      <title>该层需要 2 个输入，但实际收到 1 个输入张量</title>
      <link>https://stackoverflow.com/questions/78916012/layer-expects-2-inputs-but-it-received-1-input-tensors</link>
      <description><![CDATA[我正在尝试构建模型来预测帖子的点赞数，该模型采用文本和内容类型，即独热编码列。
我已经创建了一个 TensorFlow 数据集，但在尝试拟合模型时出现此错误：
层“ functional_13”需要 2 个输入，但它收到了 1 个输入张量。
收到的输入：[&lt;tf.Tensor &#39;data:0&#39; shape=(None, 1000) dtype=int64&gt;]

以下是我的一些代码片段：
dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,
content,
df[&#39;likes_rate&#39;]))

dataset= dataset.cache()
dataset= dataset.shuffle(160000)
dataset= dataset.batch(16)
dataset= dataset.prefetch(8)

这是我的模型
from tensorflow.keras.layers import Input, Embedding, Concatenate,LSTM,Bidirectional
text_input=输入（形状=（1000，））
content_input=输入（形状=（3，））

text_embeddings = tf.keras.layers.Embedding（Max_Features+1，32）（text_input）# 调整嵌入 dim
lstm= Bidirectional（LSTM（32，activation=&#39;tanh&#39;））（text_embeddings）
# 连接文本嵌入和内容特征
combined_features = tf.keras.layers.Concatenate()（[lstm，content_input]）

# 隐藏层（调整数量/激活函数）
x = tf.keras.layers.Dense（256，activation=&#39;relu&#39;）（combined_features）
x = tf.keras.layers.Dropout（0.2）（x）
x = tf.keras.layers.Dense(128,activation=&#39;relu&#39;)(x)
x = tf.keras.layers.Dropout(0.1)(x)
x = tf.keras.layers.Dense(64,activation=&#39;relu&#39;)(x)
# 用于点赞预测的输出层
output = tf.keras.layers.Dense(1,activation=&#39;linear&#39;)(x)

我想为文本创建一个嵌入层，然后将其传递给 LSTM，然后将 LSTM 的输出和内容合并到 Dense 层。
当尝试拟合模型时，我遇到了上述问题。
model = tf.keras.models.Model(inputs=[text_input, content_input],输出=输出)
model.compile(loss=&#39;mse&#39;,optimizer=&#39;Adam&#39;)
model.fit(dataset,epochs=10)

如果我迭代数据集。代码工作正常。但.fit每次都会产生随机权重，因此模型没有进展。
for text_batch, content_batch, y_batch in dataset:
# 在当前批次上训练模型
model.fit(x=[text_batch, content_batch], y=y_batch)
]]></description>
      <guid>https://stackoverflow.com/questions/78916012/layer-expects-2-inputs-but-it-received-1-input-tensors</guid>
      <pubDate>Mon, 26 Aug 2024 19:24:22 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 FAISS 减少大型人脸数据库的人脸识别中的误报？</title>
      <link>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</link>
      <description><![CDATA[我正在开发一个使用人脸识别的考勤跟踪系统。
该系统的工作原理如下：

1. 人脸检测：使用 Ultra Face 检测人脸。
2. 人脸编码：使用 FaceNet 对检测到的人脸进行编码。
3. 人脸比较：将编码的人脸与现有数据库进行比较以标记出勤率
4.使用的库：OpenCV 和 FAISS。
5.来源：CCTV摄像机镜头。

考勤系统说明：
当一个人走到摄像机前时，系统使用Ultra Face检测人脸，并使用FaceNet进行编码。然后将编码的人脸与现有数据库进行比较。如果相似度（余弦相似度）小于0.25，则标记出勤。
问题：
最初，数据库中的人数少于100人，比较时间是可以接受的。随着人数的增加，比较时间明显变长。每个人在数据库中都有5张图片。为了加快比较速度，我改用FAISS库。虽然FAISS显著缩短了比较时间，但也增加了误报（错误地标记出勤）。
人脸比较的旧方法：
for db_name, db_encode in encoding_dict.items():
尝试：
dist = cosine(db_encode, f_e[1])
除 ValueError 为 e 外：
print(&quot;&gt;&gt;&gt;&gt;&gt;&gt; : &quot;,f_e[1],&quot;\n&quot;,type(f_e[1]))
继续
if dist &lt;识别_t：
name = db_name
distance = dist

cv2.rectangle(img, (f_e[0][0], f_e[0][1]), (f_e[0][2], f_e[0][3]), (0, 255, 0), 1)
cv2.putText(img, f&#39;{name}:{distance - 1:.2f}&#39;, (f_e[0][0], f_e[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

使用 FAISS 的新方法：
class StaffCustManagement：
def __init__(self, staff_n_neighbours=4, identification_t=0.80):
self.staff_db：Custom_DB = Custom_DB（db_name =“mydatabase”，col_name =“staff”）
self.staff_names，self.staff_encodings = self.staff_load_encodings（）
self.staff_n_neighbours：int = staff_n_neighbours
self.staff_ini_faiss（）
self.recognition_t：float = identification_t

def staff_load_encodings（self） -&gt; Tuple[List[str], List[np.ndarray]]:
staff_names, staff_encodings = [], []
for document in self.staff_db.find_all_data():
staff_names.append(document[&#39;_id&#39;])
staff_encodings.append(ArrayEncDec.decode_from_base64(b64_str=document[&#39;encoding&#39;]))
return staff_names, staff_encodings

def staff_ini_faiss(self):
if self.staff_names and self.staff_encodings:
Dimensions = 128
self.staff_index_faiss = faiss.IndexFlatL2(dimensions)
faiss_embeddings = np.array(self.staff_encodings, dtype=&#39;float32&#39;)
faiss.normalize_L2(faiss_embeddings)
self.staff_index_faiss.add(faiss_embeddings)

def find_staff_cust(self, current_encode: np.ndarray) -&gt; Tuple[str, float]:
name = &quot;Unknown&quot;
distance = float(&quot;inf&quot;)
if len(self.staff_names) == 0:
return name, distance
target_rep = np.expand_dims(current_encode, axis=0)
# faiss.normalize_L2(target_rep)
distances, neighbours = self.staff_index_faiss.search(target_rep, self.staff_n_neighbours)
print(&quot;Distances&quot;, distances)
print(&quot;neighbors&quot;, neighbours)
if distances[0][0] &gt;= self.recognition_t:
return self.staff_names[neighbors[0][0]].split(&#39;-&#39;)[0], distances[0][0]
return name, distance

问题：
如何在使用 FAISS 进行人脸比较时减少误报我的出勤跟踪系统如何做到这一点？虽然 FAISS 大大缩短了比较时间，但准确性却受到影响，导致出勤标记不正确。是否有任何最佳实践或替代方法可以在大型数据库中保持高精度？]]></description>
      <guid>https://stackoverflow.com/questions/78739882/how-to-reduce-false-positives-in-face-recognition-using-faiss-for-large-face-dat</guid>
      <pubDate>Fri, 12 Jul 2024 10:33:51 GMT</pubDate>
    </item>
    <item>
      <title>ConvergenceWarning：lbfgs 未能收敛（状态=1）：停止：迭代次数已达到限制</title>
      <link>https://stackoverflow.com/questions/62658215/convergencewarning-lbfgs-failed-to-converge-status-1-stop-total-no-of-iter</link>
      <description><![CDATA[我有一个由数字和分类数据组成的数据集，我想根据患者的医疗特征预测患者的不良后果。我为我的数据集定义了一个预测管道，如下所示：
X = dataset.drop(columns=[&#39;target&#39;])
y = dataset[&#39;target&#39;]

# 定义分类和数字转换器
numeric_transformer = Pipeline(steps=[
(&#39;knnImputer&#39;, KNNImputer(n_neighbors=2, weights=&quot;uniform&quot;)),
(&#39;scaler&#39;, StandardScaler())])

categorical_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;missing&#39;)),
(&#39;onehot&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))])

# 将对象列分派给 categorical_transformer，将剩余列分派给 numeric_transformer
preprocessor = ColumnTransformer(transformers=[
(&#39;num&#39;, numeric_transformer, selector(dtype_exclude=&quot;object&quot;)),
(&#39;cat&#39;, categorical_transformer, selector(dtype_include=&quot;object&quot;))
])

# 将分类器附加到预处理管道。
# 现在我们有了一个完整的预测管道。
clf = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor),
(&#39;classifier&#39;, LogisticRegression())])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf.fit(X_train, y_train)
print(&quot;model score: %.3f&quot; % clf.score(X_test, y_test))

但是，运行此代码时，我收到以下警告消息：
ConvergenceWarning: lbfgs 未能收敛 (status=1):
STOP: 迭代次数已达到限制。
增加迭代次数 (max_iter) 或按如下所示缩放数据：
https://scikit-learn.org/stable/modules/preprocessing.html
另请参阅文档以了解备选求解器选项：
https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)

模型得分：0.988

有人能向我解释一下这个警告是什么意思吗？我是机器学习的新手，所以对于如何改进预测模型有些迷茫。从 numeric_transformer 中可以看出，我通过标准化缩放了数据。我也对模型得分如此之高感到困惑，这到底是好事还是坏事。]]></description>
      <guid>https://stackoverflow.com/questions/62658215/convergencewarning-lbfgs-failed-to-converge-status-1-stop-total-no-of-iter</guid>
      <pubDate>Tue, 30 Jun 2020 13:08:20 GMT</pubDate>
    </item>
    </channel>
</rss>