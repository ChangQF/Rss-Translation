<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 10 Feb 2024 15:12:07 GMT</lastBuildDate>
    <item>
      <title>Pytorch ImageFolder 抛出错误“无效目录”，但 os.listdir() 确认目录有效</title>
      <link>https://stackoverflow.com/questions/77973551/pytorch-imagefolder-throws-error-invalid-directory-but-os-listdir-confirms-t</link>
      <description><![CDATA[因此，当我尝试使用 torchvision ImageFolder() 将图像文件夹加载到数据集中时。
我得到的错误是：
单元格 In[46]，第 299 行
    第295章
    298 print(os.listdir(“/kaggle/input/flickr-image-dataset/”))
--&gt; [第 299 章]
    第301章
    304 # 您最多可以向当前目录 (/kaggle/working/) 写入 20GB 的内容，当您使用“保存并保存”创建版本时，该目录将保留为输出运行全部”
    305 # 你也可以将临时文件写入/kaggle/temp/，但它们不会保存在当前会话之外

文件 /opt/conda/lib/python3.10/site-packages/compressai/datasets/image.py:64，在 ImageFolder.__init__(self、root、transform、split) 中
     61 splitdir = 路径（根）/ split
     63 如果不是 splitdir.is_dir():
---&gt; 64 raise RuntimeError(f&#39;无效目录“{root}”&#39;)
     66 self.samples = [f for f in splitdir.iterdir() if f.is_file()]
     68 self.transform = 变换

RuntimeError：无效目录“/kaggle/input/flickr-image-dataset/”

但是，对同一目录使用 os.listdir() 效果很好。
这是代码：
print(os.listdir(“/kaggle/input/flickr-image-dataset/”)) 
train_dataset = ImageFolder(&quot;/kaggle/input/flickr-image-dataset/&quot;, )]]></description>
      <guid>https://stackoverflow.com/questions/77973551/pytorch-imagefolder-throws-error-invalid-directory-but-os-listdir-confirms-t</guid>
      <pubDate>Sat, 10 Feb 2024 15:00:50 GMT</pubDate>
    </item>
    <item>
      <title>3 个嵌入共线性的自定义损失函数</title>
      <link>https://stackoverflow.com/questions/77973255/custom-loss-function-for-collinearity-of-3-embeddings</link>
      <description><![CDATA[我正在尝试实现一个损失函数，该函数以 3 个嵌入作为输入，并输出一个与嵌入的共线性成比例的值。这是为了塑造用于嵌入插值的卷积自动编码器的潜在空间，如本文所述：Alon Oring 等人- 2020年。
我目前有代码可以在训练期间将嵌入作为张量获取，并使用基本的 mse 损失函数。我曾多次尝试实现这一点，但没有成功，我的训练损失总是会在开始时陷入困境。
您对在 pytorch 中实现此共线性损失函数及其与其他损失的总和有什么建议吗？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77973255/custom-loss-function-for-collinearity-of-3-embeddings</guid>
      <pubDate>Sat, 10 Feb 2024 13:31:03 GMT</pubDate>
    </item>
    <item>
      <title>关于训练测试分割、SMOTE、PCA 的困惑</title>
      <link>https://stackoverflow.com/questions/77973120/confusion-about-train-test-split-smote-pca</link>
      <description><![CDATA[我有一个高度不平衡的数据集，用于使用 SMOTE 的过采样技术。在使用 SMOTE 之前，我首先在 train test 之间分割数据集。然后我在训练数据集上应用PCA。然后使用PCA降维，再次需要训练应用PCA得到的测试分割。
这里我应用了两次训练-测试分割。因为在使用 SMOTE 应用训练测试之前，该训练集再次应用 PCA 。为了减少维度，我再次分割训练数据集。我的步骤有效吗？如果没有给我一个解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/77973120/confusion-about-train-test-split-smote-pca</guid>
      <pubDate>Sat, 10 Feb 2024 12:50:18 GMT</pubDate>
    </item>
    <item>
      <title>Autogluon 在训练 bagged 模型时不使用 GPU</title>
      <link>https://stackoverflow.com/questions/77972995/autogluon-doesnt-use-gpu-when-training-bagged-models</link>
      <description><![CDATA[我正在使用 Autogluon 0.8.2。我已经在 fit 方法中给出了相应的 GPU 参数，但我意识到在使用 GPU 在堆栈的第一层训练模型之后，当模型的袋装版本为下一个堆栈层进行训练时，GPU 不会在第一次堆栈训练后使用。我使用 nvidia-smi 检查了 GPU 利用率，结果为 0% 并且没有使用内存。
Autogluon 或袋装模型是否存在这样的训练问题？]]></description>
      <guid>https://stackoverflow.com/questions/77972995/autogluon-doesnt-use-gpu-when-training-bagged-models</guid>
      <pubDate>Sat, 10 Feb 2024 12:13:47 GMT</pubDate>
    </item>
    <item>
      <title>使用时间序列数据集训练模型，如何使用数据和时间作为输入特征？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77972529/training-models-with-a-time-series-dataset-how-to-use-the-data-and-time-as-inpu</link>
      <description><![CDATA[我正在使用 房间占用估计数据集，包含 10129 个实例和 18 个特征。对于所有三个模型，预测准确率为 97-99%，我认为这是因为我删除了日期和时间列，因为它们是对象。
df.drop([&#39;日期&#39;,&#39;时间&#39;], axis = 1)
打印（df.count（））

但是，我想使用日期和时间来查看准确性是否有变化。但是，我不知道该怎么做，但我最初想到从时间中提取小时和分钟并将其用作输入特征。
df[&#39;Hour&#39;] = df[&#39;Time&#39;].dt.hour

有什么帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/77972529/training-models-with-a-time-series-dataset-how-to-use-the-data-and-time-as-inpu</guid>
      <pubDate>Sat, 10 Feb 2024 09:46:38 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 模型显示 X 有 6 个特征，但 MinMaxScaler 期望有 7 个特征作为输入</title>
      <link>https://stackoverflow.com/questions/77972253/lstm-model-shows-x-has-6-features-but-minmaxscaler-is-expecting-7-features-as-i</link>
      <description><![CDATA[我正在构建一个 LSTM 模型，该模型可以随时间分析六个变量。但是，我的代码抛出错误。您能指导我在哪里更改我的代码吗？我还有一个采用这种格式的 ARIMA 模型。这种格式对于分析风速是否正确？我的任务是用 LSTM 模型组装 ARIMA 模型。但是，我无法完成这个 LSTM 模型。
随机导入
进口警告

将 numpy 导入为 np
将 pandas 导入为 pd
从 keras.layers 导入 LSTM，密集
从 keras.models 导入顺序
从 keras.optimizers 导入 Adam
从 sklearn.preprocessing 导入 MinMaxScaler

# 从 CSV 文件加载数据集
file_path = &#39;孟加拉国天气数据 (1948 - 2013).csv&#39;
df = pd.read_csv(文件路径)

# 如果有日期列，请确保 DataFrame 按日期排序
如果 df.columns 中有“日期”：
    df[&#39;日期&#39;] = pd.to_datetime(df[&#39;日期&#39;])
    df.sort_values(&#39;日期&#39;, inplace=True)

# 预定义的特征和目标变量
selected_features = [&#39;最高温度&#39;、&#39;最低温度&#39;、&#39;降雨量&#39;、&#39;相对湿度&#39;、&#39;云量覆盖&#39;、&#39;明亮阳光&#39;]
目标=&#39;风速&#39;

# 要求用户输入所选特征的值
用户输入 = {}
对于 selected_features 中的功能：
    value = float(input(f&quot;输入 {feature} 值：&quot;))
    用户输入[特征] = 值

# 要求用户输入年、月、站的值
user_input[&#39;YEAR&#39;] = int(input(&quot;请输入年份：&quot;))
user_input[&#39;月份&#39;] = int(input(&quot;请输入月份：&quot;))
user_input[&#39;车站名称&#39;] = input(&quot;输入车站：&quot;)

# 使用占位符值将“Wind_Speed”列添加到 user_input_df
user_input_df = pd.DataFrame({**user_input, &#39;Wind_Speed&#39;: [0]})

# 禁用特定警告
warnings.simplefilter(“忽略”, UserWarning)
warnings.simplefilter(“忽略”, FutureWarning)

尝试：
    # 确保 df[target] 有合适的索引
    如果不是 isinstance(df.index, pd.RangeIndex):
        df.reset_index(drop=True, inplace=True)

    # 对每个特征使用 MinMaxScaler 进行特征缩放
    缩放器 = MinMaxScaler()

    scaled_data = scaler.fit_transform(df[selected_features + [目标]])


    # 创建用于 LSTM 训练的序列
    序列长度 = 10
    x_train, y_train = [], []

    对于范围内的 i（sequence_length，len（scaled_data））：
        x_train.append(scaled_data[i - 序列长度:i, :-1])
        y_train.append(scaled_data[i, -1])

    x_train, y_train = np.array(x_train), np.array(y_train)

    # 重塑 LSTM 的输入数据
    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))

    # 构建 LSTM 模型
    模型=顺序（）
    model.add(LSTM(单位=100, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))
    model.add(LSTM(单位=100，return_sequences=True))
    model.add(LSTM(单位=50, return_sequences=False))
    model.add(密集(单位=1))

    # 编译模型
    model.compile(优化器=Adam(learning_rate=0.001), loss=&#39;mean_squared_error&#39;)

    # 训练模型
    model.fit(x_train、y_train、epochs=5、batch_size=16、validation_split=0.1)

    # 准备用于预测的输入数据
    输入=scaled_data[-sequence_length:, :-1]
    输入 = 缩放器.transform(输入)
    输入=输入.reshape(1,sequence_length,len(selected_features))


    ＃ 作出预测
    预测 = model.predict(输入)
    预测 =scaler.inverse_transform(预测.reshape(-1, 1))


    # 引入额外的随机性
    random_perturbation = random.uniform(-2, 0.5) # 根据需要调整范围
    预测+=随机扰动

    print(f&#39;预测下一周期的{目标}：{预测[0, 0]}&#39;)

    #MAPE计算
    random_mape = random.uniform(12, 14)
    print(f&#39;估计平均绝对百分比误差: {random_mape:.2f}%&#39;)

    # 将用户输入和预测输出合并到一个新的 DataFrame 中
    output_data = pd.concat([user_input_df, pd.DataFrame({目标: [预测[0, 0]]})], axis=1)

    # 将输入数据和预测输出保存到新的 CSV 文件中
    输出文件路径 = &#39;LSTM_Predictions_Output.csv&#39;
    输出数据.to_csv（输出文件路径，索引=假）

    print(f&#39;输入数据和预测输出保存到{output_file_path}&#39;)


除了 ValueError 为 e：
    打印（f&#39;错误：{e}&#39;）

最后：
    warnings.resetwarnings()
]]></description>
      <guid>https://stackoverflow.com/questions/77972253/lstm-model-shows-x-has-6-features-but-minmaxscaler-is-expecting-7-features-as-i</guid>
      <pubDate>Sat, 10 Feb 2024 07:45:08 GMT</pubDate>
    </item>
    <item>
      <title>理想的step_per_epoch和纪元数是多少[关闭]</title>
      <link>https://stackoverflow.com/questions/77972223/what-is-the-ideal-step-per-epoch-and-no-of-epoch</link>
      <description><![CDATA[我的总数据点为 1000000 个观察值，分别分为 80000 个和 20000 个训练/测试观察值。理想的step_per_epoch是多少，epoch数和Batch_size]]></description>
      <guid>https://stackoverflow.com/questions/77972223/what-is-the-ideal-step-per-epoch-and-no-of-epoch</guid>
      <pubDate>Sat, 10 Feb 2024 07:29:51 GMT</pubDate>
    </item>
    <item>
      <title>如何在训练过程中随机裁剪图像并协调标签？</title>
      <link>https://stackoverflow.com/questions/77972180/how-to-random-crop-image-and-coordinate-label-during-training</link>
      <description><![CDATA[我的任务是头影测量地标定位。
我在此数据框中显示坐标 X1,Y1 的图像路径。

&lt;标题&gt;

文件名
X1
Y1


&lt;正文&gt;

/Images_data/binary0006.png
89
80


/Images_data/binary0008.png
37
70


/Images_data/binary0007.png
50
76


/Images_data/binary0003.png
55
92


/Images_data/binary0005.png
91
64


/Images_data/binary0004.png
100
76



训练时如何裁剪图像并坐标X1,Y1？]]></description>
      <guid>https://stackoverflow.com/questions/77972180/how-to-random-crop-image-and-coordinate-label-during-training</guid>
      <pubDate>Sat, 10 Feb 2024 07:13:06 GMT</pubDate>
    </item>
    <item>
      <title>用Python训练分类器</title>
      <link>https://stackoverflow.com/questions/77970572/training-classifier-in-python</link>
      <description><![CDATA[我用 Python (PyCharm) 编写了一个分类器。它不显示模型的训练阶段。如何在我的代码中解决这个问题？我想查看分类器的训练阶段。在 Jupiter 笔记本中工作时，所有内容都会显示（您可以在图片中看到它）。
导入 pandas 作为 pd
将 matplotlib.pyplot 导入为 plt
将张量流导入为 tf
从 sklearn.model_selection 导入 train_test_split
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从sklearn导入数据集
从 sklearn.preprocessing 导入 LabelEncoder

CSV_COLUMN_NAMES = [&#39;SEPAL_LENGTH&#39;、&#39;SEPAL_WIDTH&#39;、&#39;PETAL_LENGTH&#39;、&#39;PETAL_WIDTH&#39;、&#39;SPECIES&#39;]

数据= pd.read_csv（&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&#39;，名称= CSV_COLUMN_NAMES，标题= 0）

编码器 = LabelEncoder()
数据[&#39;SPECIES&#39;] =编码器.fit_transform(data[&#39;SPECIES&#39;])
打印（数据.head（））
X = pd.DataFrame(data, columns=data.columns.drop(&#39;SPECIES&#39;))

y = data.pop(&#39;物种&#39;)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

def input_fn（特征，标签，训练= True，batch_size = 256）：
    数据集 = tf.data.Dataset.from_tensor_slices((dict(features), labels))
    数据集 = 数据集.batch(10)
    如果训练：
        数据集 = dataset.shuffle(1000).repeat()

    返回数据集.shuffle(1000).repeat()

my_feature_columns = [] # 记录特征

对于 X_train.keys() 中的密钥：
    my_feature_columns.append(tf.feature_column.numeric_column(key=key))

分类器= tf.estimator.DNNClassifier（feature_columns = my_feature_columns，hidden_​​units =。[30,10]，n_classes = 3）


 classifier.train（input_fn = lambda：input_fn（X_train，y_train，训练= True），步骤= 5）

 classifier.evaluate(input_fn=lambda: input_fn(X_test, y_test, Training=False))
 print(&#39;\n测试集精度: {accuracy:0.3f}\n&#39;.format(**train_result))
]]></description>
      <guid>https://stackoverflow.com/questions/77970572/training-classifier-in-python</guid>
      <pubDate>Fri, 09 Feb 2024 20:04:15 GMT</pubDate>
    </item>
    <item>
      <title>形状summary_plot的子图</title>
      <link>https://stackoverflow.com/questions/77969990/subplot-for-shap-summary-plot</link>
      <description><![CDATA[假设我们有以下简化代码：
导入 pandas 作为 pd
导入形状
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.model_selection 导入 train_test_split
将 matplotlib.pyplot 导入为 plt
从 sklearn.preprocessing 导入 LabelEncoder
mylabel =LabelEncoder()
数据 =pd.read_csv(“https://raw.githubusercontent.com/krishnaik06/Multiple-Linear-Regression/master/50_Startups.csv”)
数据[&#39;状态&#39;] =mylabel.fit_transform(数据[&#39;状态&#39;])
打印（数据.head（））
模型 =RandomForestRegressor()
y =数据[&#39;利润&#39;]
X =data.drop(&#39;利润&#39;,axis=1)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=1)
model.fit(X_train,y_train)
解释器 =shap.TreeExplainer(模型)
shap_values =explainer.shap_values(X_train)
plt.figure(figsize=(30,30))
plt.子图(2,1,1)
shap.summary_plot（shap_values，X_train，feature_names = X.columns，plot_type =“bar”）
plt.子图(2,1,2)
shap.summary_plot(shap_values, X_train, feature_names=X.columns)
plt.show()

当我运行此代码时，我在不同的图形上得到两个图像：
一张图片：

和另一张图片：

我想将它们绘制在一起，正如你所看到的，我使用了子图：
plt.subplot(2,1,1)
shap.summary_plot（shap_values，X_train，feature_names = X.columns，plot_type =“bar”）
plt.子图(2,1,2)
shap.summary_plot(shap_values, X_train, feature_names=X.columns)

但它不起作用，我试图使用此代码：
fig，轴= plt.subplots（nrows = 2，ncols = 2，figsize =（10,10））
shap.dependence_plot(&#39;年龄&#39;, shap_values[1], X_train, ax=axes[0, 0], show=False)
shap.dependence_plot(&#39;收入&#39;, shap_values[1], X_train, ax=axes[0, 1], show=False)
shap.dependence_plot(&#39;分数&#39;, shap_values[1], X_train, ax=axes[1, 0], show=False)
plt.show()

但是summary_plot没有参数ax，那么我该如何使用它？]]></description>
      <guid>https://stackoverflow.com/questions/77969990/subplot-for-shap-summary-plot</guid>
      <pubDate>Fri, 09 Feb 2024 17:54:11 GMT</pubDate>
    </item>
    <item>
      <title>WEKA凯姆包</title>
      <link>https://stackoverflow.com/questions/77934889/weka-caim-package</link>
      <description><![CDATA[在网络搜索中找不到任何用于 CAIM 离散化的 WEKA 包。我需要 WEKA v3 的软件包。
在 google 上搜索 WEKA 软件包，但没有找到任何内容，但一些文档 说它存在。
谁能提供 WEKA 的 CAIM 包的工作链接吗？]]></description>
      <guid>https://stackoverflow.com/questions/77934889/weka-caim-package</guid>
      <pubDate>Sun, 04 Feb 2024 07:13:41 GMT</pubDate>
    </item>
    <item>
      <title>keras多类分类欠拟合[关闭]</title>
      <link>https://stackoverflow.com/questions/77897827/keras-multiclass-classification-underfitting</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77897827/keras-multiclass-classification-underfitting</guid>
      <pubDate>Mon, 29 Jan 2024 06:41:23 GMT</pubDate>
    </item>
    <item>
      <title>CNN 图像分类奇怪的 Sigmoid 预测问题 [已关闭]</title>
      <link>https://stackoverflow.com/questions/77893518/cnn-image-classification-weird-sigmoid-predictions-issue</link>
      <description><![CDATA[我正在使用 Tensorflow 开发机器学习神经网络模型，该模型可以识别一个人是否戴着口罩或未给出图像。请先阅读其余部分，然后再进入 github 链接（其中包含整个笔记本，以防万一模型本身以外的问题）。
我的问题是，模型训练完成后（准确度为 96%，损失为 15%），预测输出一个奇怪的 sigmoid 值，远低于阈值 0.5，并且在使用图像的测试图像之间无关紧要戴口罩的人和不戴口罩的人。
这是使用 Tensorflow 函数的神经网络模型：
模型 = 顺序([
    Conv2D(16, (3,3), 1, 激活=&#39;relu&#39;, input_shape = (256,256,3)),
    最大池化2D(),
    辍学率（0.25），

    Conv2D(32, (3,3), 1, 激活=&#39;relu&#39;),
    最大池化2D(),
    辍学率（0.25），

    Conv2D(16, (3,3), 1, 激活=&#39;relu&#39;),
    最大池化2D(),
    辍学率（0.25），

    展平（），

    密集（256，激活=&#39;relu&#39;），
    辍学率（0.5），

    密集（1，激活=&#39;sigmoid&#39;）
]）

到目前为止我做了什么：
我首先认为这是过度拟合（仍然可能是），其中我的第一直觉是降低学习率。在这次修复之前，预测的 sigmoid 值在负数中使用了科学计数法，但后来他们冷静下来，恢复了不使用科学计数法。在其他人帮助我一点之后，我还添加了 Dropout 层，这也让 sigmoid 值平静了一点。然而，它们仍然不是正确的预测，并且它们小于 0.1，这不是 sigmoid 函数所期望的。]]></description>
      <guid>https://stackoverflow.com/questions/77893518/cnn-image-classification-weird-sigmoid-predictions-issue</guid>
      <pubDate>Sun, 28 Jan 2024 02:58:25 GMT</pubDate>
    </item>
    <item>
      <title>如何提高这个回归问题的准确率？</title>
      <link>https://stackoverflow.com/questions/63036212/how-to-improve-accuracy-score-for-this-regression-problem</link>
      <description><![CDATA[我正在使用 UCI 的学生表现数据集。我想根据给定的特征预测学生的最终结果。
我首先尝试使用两个主要且高度相关的特征 G1 和 G2，它们是两次考试的成绩。我使用 LinearRegression 算法，得到的准确度为 0.4 或更低。
然后我尝试对数据框中对象的所有特征进行特征工程，但准确性仍然相同。
如何提高准确度分数？
我的代码作为 Python 笔记本&lt; /p&gt;
from matplotlib import pyplot as plt
将seaborn导入为sns
将 numpy 导入为 np
从 sklearn.model_selection 导入 train_test_split

从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.linear_model 导入 ElasticNet
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.ensemble 导入 ExtraTreesRegressor
从 sklearn.ensemble 导入 GradientBoostingRegressor
从 sklearn.svm 导入 SVR

从 sklearn.metrics 导入mean_squared_error、mean_absolute_error、median_absolute_error、accuracy_score

df = pd.read_csv(&#39;student-mat.csv&#39;,sep=&#39;;&#39;)
df2 = pd.read_csv(&#39;student-por.csv&#39;,sep=&#39;;&#39;)

df = [df,df2]
df = pd.concat(df)
df = pd.get_dummies(df)

X = df.drop(&#39;G3&#39;,轴=1)
y = df[&#39;G3&#39;]

X_train，X_test，y_train，y_test = train_test_split（X，y，test_size = 0.1，random_state = 42）

模型=线性回归()
model.fit(X_train,y_train)

y_pred = model.predict(X_test)
y_pred = [int(round(i)) for i in y_pred]

准确度分数（y_test，y_pred）
]]></description>
      <guid>https://stackoverflow.com/questions/63036212/how-to-improve-accuracy-score-for-this-regression-problem</guid>
      <pubDate>Wed, 22 Jul 2020 14:09:00 GMT</pubDate>
    </item>
    <item>
      <title>r 神经网络包——多输出</title>
      <link>https://stackoverflow.com/questions/34663573/r-neuralnet-package-multiple-output</link>
      <description><![CDATA[我目前使用神经网络的方式是它从许多输入点预测一个输出点。更具体地说，我运行以下命令。
nn &lt;- 神经网络(
as.公式(a ~ c + d),
数据 = Z，隐藏 = c（3,2），err.fct =“sse”，act.fct = 自定义，
线性.输出=真，重复= 5）

这里，如果 Z 是一个由名称为 a、b、c 的列组成的矩阵，它将根据 c 行和 d 行中的对应点预测 a 列中某一行的一个点。 （以垂直维度作为训练样本。）
假设还有一列 b。我想知道是否有办法从 c 和 d 预测 a 和 b？我已经尝试过
as.formula(a+b ~ c+d)

但这似乎不起作用。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/34663573/r-neuralnet-package-multiple-output</guid>
      <pubDate>Thu, 07 Jan 2016 19:29:54 GMT</pubDate>
    </item>
    </channel>
</rss>