<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 29 Dec 2024 18:21:01 GMT</lastBuildDate>
    <item>
      <title>如何处理和解决这个扭曲的验证码图像[关闭]</title>
      <link>https://stackoverflow.com/questions/79315607/how-to-process-and-solve-this-distorted-captcha-image</link>
      <description><![CDATA[我有一张带有扭曲文本和噪声的 CAPTCHA 图像（下面的示例图像）。
我的目标是使用 Python 以编程方式从此 CAPTCHA 中提取和识别文本。

图像包含：

带有重叠字符的扭曲文本。
线条和噪声干扰文本。
渐变背景。
]]></description>
      <guid>https://stackoverflow.com/questions/79315607/how-to-process-and-solve-this-distorted-captcha-image</guid>
      <pubDate>Sun, 29 Dec 2024 12:46:47 GMT</pubDate>
    </item>
    <item>
      <title>Blenderbot：从 json 进行训练时出现关键错误</title>
      <link>https://stackoverflow.com/questions/79315340/blenderbot-key-error-while-training-from-json</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79315340/blenderbot-key-error-while-training-from-json</guid>
      <pubDate>Sun, 29 Dec 2024 09:46:30 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyTorch CNN 处理扩展 MNIST 数据集时网络没有得到改善 [关闭]</title>
      <link>https://stackoverflow.com/questions/79315321/network-not-improving-with-pytorch-cnn-for-extended-mnist-dataset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79315321/network-not-improving-with-pytorch-cnn-for-extended-mnist-dataset</guid>
      <pubDate>Sun, 29 Dec 2024 09:30:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在目录中找到给定图像的相似图像</title>
      <link>https://stackoverflow.com/questions/79315320/how-to-find-similar-images-for-given-image-in-a-directory</link>
      <description><![CDATA[我是机器学习和 Python 的新手，我正在尝试在目录中找到与给定图像最匹配（相似）的图像。
例如，我的文件夹中有 50,000 张硬币图像，并尝试找到与用户指定图像最匹配的前 10 张图像。
这是我的示例文件 https://drive.google.com/drive/folders/1pxz4nuyWfSLcrzs0QZpvqWO77ZI3Vn6M?usp=sharing
这是示例搜索图像 https://drive.google.com/file/d/1cWTxLIjG2pKW4-1g-LYfdAbccG18sPnX/view?usp=sharing
我也用 OpenCV 修改了给定的图像，但没有任何变化，这里是裁剪版本 https://drive.google.com/file/d/1AvZbPgZJvPwSBqNxVZ9N7t-DxTI_Xzsl/view?usp=sharing
预期 18206-f.jpg 应为根据指定示例文件匹配度最高的文件。
我尝试了许多算法并提供了类似下面的解决方案，但找不到合适的解决方案。

我使用了自定义训练模型进行图像分类，如这里所述 https://www.tensorflow.org/tutorials/images/classification 但我失败了，我猜我没有针对特定示例的不同文件，我还尝试了自定义数据增强机制，但再次失败（失败 = 可能找不到正确的相关文件）

我使用了https://github.com/TechyNilesh/DeepImageSearch但失败了我想这种解决方案最适合语义搜索，我需要图像到图像的搜索，例如图像相似性，而不是关键字/内容相似性等。

我使用了 Ahash、Dhash 和 Phash 等哈希算法，但在图像处于不同条件等时失败了。

我尝试了这里提到的 KNN 搜索https://stackoverflow.com/a/56881718/6799182但失败了再次。


import cv2
import os
import numpy as np

from typing import Union

def read_img_from_dir(image_dir: str, query_shape: Union[list, tuple]) -&gt; tuple[list, np.ndarray]:
name_image = []
img_array = np.empty((0, np.prod(query_shape)), dtype=np.float32)
for image_name in os.listdir(image_dir):
name_image.append(image_name)
image = cv2.imread(os.path.join(image_dir, image_name))
if not isinstance(image, np.ndarray):
# 如果路径不是图像
continue
image = cv2.resize(image, query_shape[:2][::-1])
image = image.reshape(1, -1) / 255.
img_array = np.concatenate((img_array, image))
return name_image, img_array 

def find_by_knn(query_img: np.ndarray，list_name：list [str]，数据库：np.ndarray）-&gt;; str:
query_img = query_img.reshape(1, -1) / 255.
dists = np.sqrt(np.sum((database-query_img) ** 2,axis = 1))
idx = dists.argmin()
return list_name[idx]

if __name__==&#39;__main__&#39;:
image_query = &#39;1tl-cropped.png&#39;
image_dir = &#39;sample_flat&#39;
img = cv2.imread(image_query)

# 可选：由于查询图像大小可能很大，调整大小 
# 所有图像为较小的所需形状可以避免 OOM 问题
# 并提高计算速度

# global_shape = (320, 320)
# img = cv2.resize(img, global_shape)

shape = img.shape
name_image, img_array = read_img_from_dir(image_dir, shape)
result = find_by_knn(img, name_image, img_array)
print(result)

请为我提供该任务的适当解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/79315320/how-to-find-similar-images-for-given-image-in-a-directory</guid>
      <pubDate>Sun, 29 Dec 2024 09:30:09 GMT</pubDate>
    </item>
    <item>
      <title>计算模型的最小二乘误差[关闭]</title>
      <link>https://stackoverflow.com/questions/79315069/calculating-the-least-squares-error-for-a-model</link>
      <description><![CDATA[我做了一些自学，遇到了这个问题。我研究过 LSE 的问题，但我真的不明白这是怎么回事。有人能给我一个关于如何解决这个问题的简要想法吗？我已经花了好几天时间才弄清楚，但没有成功。
]]></description>
      <guid>https://stackoverflow.com/questions/79315069/calculating-the-least-squares-error-for-a-model</guid>
      <pubDate>Sun, 29 Dec 2024 06:25:46 GMT</pubDate>
    </item>
    <item>
      <title>如何防止 C# SoftMax 实现中出现溢出？</title>
      <link>https://stackoverflow.com/questions/79314811/how-do-i-prevent-overflows-in-my-c-sharp-softmax-implementation</link>
      <description><![CDATA[为了从“第一原理”的角度更好地理解机器学习，我正在实现自己的 ML 相关函数。目前，我正在尝试实现 SoftMax：
IEnumerable&lt;double&gt; SoftMax(IEnumerable&lt;double&gt; vector)
{
var exps = vector.Select(v =&gt; Math.Exp(v));
var sumExps = exps.Sum();
return exps.Select(exp =&gt; exp / sumExps);
}

如果我正确理解了 SoftMax，如果我将此函数的结果相加，即 SoftMax(vector).Sum()，则输出应始终为 1。
然而，在几乎所有情况下，这都会返回一个略大于或略小于 1 的值。通常，类似于 1.0568102998178908 或 0.9758570985704772。
我听说这种情况并不罕见（基本上是溢出问题），可以通过从输入到 Math.Exp() 的值中减去输入的最大元素来解决。所以我根据建议想出了这个实现：
IEnumerable&lt;double&gt; SoftMax(IEnumerable&lt;double&gt; vector)
{
var maxVal = vector.Max();
var exps = vector.Select(v =&gt; Math.Exp(v - maxVal));
var sumExps = exps.Sum();
return exps.Select(exp =&gt; exp / sumExps);
}

在测试我的实现时，我传入了一个随机的双精度数集合，如下所示：
IEnumerable&lt;double&gt; vector = Enumerable.Range(0, 100).Select(n =&gt; new Random().NextDouble());
var softMaxVec = SoftMax(vector);
Console.WriteLine(softMaxVec.Sum());

但是我改进后的实现仍然给我带来了同样的问题。我做错了什么？还是我误解了 SoftMax 的工作原理？]]></description>
      <guid>https://stackoverflow.com/questions/79314811/how-do-i-prevent-overflows-in-my-c-sharp-softmax-implementation</guid>
      <pubDate>Sun, 29 Dec 2024 01:01:45 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的代码输出“audio_features 为空。跳过 LSTM 准备。”</title>
      <link>https://stackoverflow.com/questions/79314222/why-is-my-code-outputting-audio-features-is-empty-skipping-lstm-preparation</link>
      <description><![CDATA[我尝试将 librispeech 数据集导入我的代码，然后使用它进行训练，但我一直收到：

audio_features 为空。跳过 LSTM 准备。

librispeech 文件夹包含顶部的 .txt 和位于 .txt 文件下方的 .flac 文件。
import librosa
import os
import numpy as np

def load_librispeech_dataset(directory):
audio_files = []
labels = []
for root, _, files in os.walk(directory):
for file in files:
if file.endswith(&#39;.flac&#39;):
file_path = os.path.join(root, file)
try:
audio, sr = librosa.load(file_path, sr=None)
mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)
audio_files.append(np.mean(mfccs.T, axis=0))

# 假设标签（转录）位于相应的文本文件中
label_path = file_path.replace(&#39;.flac&#39;, &#39;.txt&#39;)
with open(label_path, &#39;r&#39;) as label_file:
label = label_file.read().strip()
labels.append(label)
except Exception as e:
print(f&quot;Error processing {file_path}: {e}&quot;)

return np.array(audio_files), labels # 缩进已更正：处理所有文件后返回

dataset_directory = &#39;C:\\Users\\rowro\\Downloads\\train-clean-100\\LibriSpeech\\train-clean-100&#39;
audio_features, transcriptions = load_librispeech_dataset(dataset_directory)
import tensorflow as tf
从 tensorflow.keras.models 导入 Sequential
从 tensorflow.keras.layers 导入 Dense、LSTM、Dropout
从 tensorflow.keras.utils 导入 to_categorical # 导入 to_categorical

如果 audio_features.size == 0:
print(&quot;audio_features 为空。跳过 LSTM 准备。）
否则：

audio_features = audio_features.reshape(audio_features.shape[0], 1, audio_features.shape[1]) # 重塑 LSTM

词汇 = sorted(list(set(transcriptions)))

transcription_to_index = {transcription: index for index, transcription in enumerate(vocabulary)}

indexed_transcriptions = [transcription_to_index[transcription] for transcription in transcriptions]

one_hot_transcriptions = to_categorical(indexed_transcriptions, num_classes=len(vocabulary))

dataset = tf.data.Dataset.from_tensor_slices((audio_features, one_hot_transcriptions))

epochs = 50
]]></description>
      <guid>https://stackoverflow.com/questions/79314222/why-is-my-code-outputting-audio-features-is-empty-skipping-lstm-preparation</guid>
      <pubDate>Sat, 28 Dec 2024 17:06:32 GMT</pubDate>
    </item>
    <item>
      <title>确定在提供的视频中同一辆车被拍摄的次数</title>
      <link>https://stackoverflow.com/questions/79313854/identify-how-many-times-same-vehicle-was-captured-in-the-provided-video</link>
      <description><![CDATA[正在进行视频分析作业，我需要捕捉在给定视频中同一车辆被拍摄的次数。
到目前为止，使用 YOLO11 能够识别汽车、自行车、公共汽车和卡车等车辆。相应地，在视频帧中绘制车辆的矩形。
我不明白如何用一些识别码标记车辆。这样，当同一辆车出现在视频帧中时，我可以增加该车辆的数量。
添加我尝试过的代码
from ultralytics import YOLO
import cv2
from enum import Enum

class DetectionType(Enum):
CAR = 2
MOTORCYCLE = 3
BUS = 5
TRUCK = 6

coco_model = YOLO(&#39;yolo11n.pt&#39;)
cap = cv2.VideoCapture(&#39;testVideo.mp4&#39;)

vehicles = [
DetectionType.CAR.value, 
DetectionType.MOTORCYCLE.value, 
DetectionType.BUS.value,
DetectionType.TRUCK.value
]

ret = True

while ret:
ret, frame = cap.read()

if ret:
#detect vehicle
detections_model = coco_model(frame)[0]

for detection in detections_model.boxes.data.tolist():
x1, y1, x2, y2, score, class_id = detection

if int(class_id) in vehicles:
x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)

# 在窗口中显示帧 
cv2.imshow(&#39;video&#39;, frame)

if cv2.waitKey(33) == 27:
break

cap.release()
cv2.destroyAllWindows() 

任何建议或代码片段都会帮助我完成这项作业。]]></description>
      <guid>https://stackoverflow.com/questions/79313854/identify-how-many-times-same-vehicle-was-captured-in-the-provided-video</guid>
      <pubDate>Sat, 28 Dec 2024 13:29:54 GMT</pubDate>
    </item>
    <item>
      <title>需要 chromadb 和 transformers 一起使用，但要求有冲突，因为 chromadb 需要 0.20 版本的 tokenizers，而后者需要 0.21 版本</title>
      <link>https://stackoverflow.com/questions/79309306/need-chromadb-transformers-together-but-have-conflicting-requirements-as-chrom</link>
      <description><![CDATA[我必须在一个项目中同时使用 chromadb 和 transformers，但 chromadb 需要 &lt;=0.20.3 版本的 tokenizers，而 transformers 需要 &gt;=0.21 版本的 tokenizers，并且与 chromadb 兼容的旧版本 transformers 需要 rust 编译器，因此这也不是一种选择。
我尝试升级 transformers、tokenizers，也尝试降级 transformers，但都不起作用，而对于所有这些，我都在使用虚拟环境。]]></description>
      <guid>https://stackoverflow.com/questions/79309306/need-chromadb-transformers-together-but-have-conflicting-requirements-as-chrom</guid>
      <pubDate>Thu, 26 Dec 2024 11:03:46 GMT</pubDate>
    </item>
    <item>
      <title>“使用 YOLO 和 EasyOCR 进行车牌识别时遇到的文本识别问题”</title>
      <link>https://stackoverflow.com/questions/79291987/text-recognition-issues-in-license-plate-recognition-using-yolo-and-easyocr</link>
      <description><![CDATA[问题
我正在开发一个车牌识别系统，使用 YOLOv8 进行检测，使用 EasyOCR 进行文本识别。虽然 YOLO 可以正确检测车牌区域，但 OCR 结果对于阿拉伯语文本和数字通常不准确。
检测到的文本示例：
检测到：“اباز”，这是无关紧要的。
检测到：“الراق”，与“العراق”部分匹配。
像“٢٦٠٤٩٩”这样的数字被准确检测到。
我尝试过的
管道设置：
YOLOv8 检测车牌并提取其边界框。
EasyOCR 处理裁剪后的车牌以进行文本识别。
文本校正：
使用 difflib.get_close_matches() 将 OCR 检测到的文本与预定义单词进行匹配（例如，“العراق”、“دهوك”）。
应用置信度阈值来过滤低置信度结果。
图像预处理：
将车牌区域转换为灰度。
调整区域大小以增强 OCR 性能。
最小可重现示例
import cv2
import easyocr
from ultralytics import YOLO

def detect_plate_with_yolo(image_path, model_path=&quot;yolov8n.pt&quot;):
model = YOLO(model_path)
img = cv2.imread(image_path)
results = model(img)
detections = results[0].boxes.xyxy.cpu().numpy()
if detections:
x1, y1, x2, y2 = map(int, detections[0])
return img[y1:y2, x1:x2]
return None

def perform_ocr_on_plate(plate_img):
reader = easyocr.Reader([&#39;ar&#39;, &#39;en&#39;], gpu=False)
plate_gray = cv2.cvtColor(plate_img, cv2.COLOR_BGR2GRAY)
return reader.readtext(plate_gray, detail=1)

plate_img = detect_plate_with_yolo(&quot;path/to/image.jpg&quot;)
if plate_img is not None:
detected_text = perform_ocr_on_plate(plate_img)
print(detected_text)

预期与实际行为
预期：正确识别阿拉伯语文本和数字（例如，&quot;العراق&quot;）。
实际：部分匹配（例如，&quot;الراق&quot;）或不相关的结果（例如，&quot;اباز&quot;）。
问题
如何使用 EasyOCR 提高阿拉伯语车牌的 OCR 准确率？
有没有比 difflib.get_close_matches() 更好的文本校正替代方案？
哪些额外的预处理步骤可能有助于提高 OCR 性能？]]></description>
      <guid>https://stackoverflow.com/questions/79291987/text-recognition-issues-in-license-plate-recognition-using-yolo-and-easyocr</guid>
      <pubDate>Wed, 18 Dec 2024 17:26:40 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：
&#39;super&#39; 对象没有属性 &#39;__sklearn_tags__&#39;。

当我在 RandomizedSearchCV 对象上调用 fit 方法时会发生这种情况。我怀疑它可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我希望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>如何在语言模型中使用标记化？[关闭]</title>
      <link>https://stackoverflow.com/questions/79289981/how-to-use-tokenization-in-language-model</link>
      <description><![CDATA[我一直在训练这个 LM，使用以下超参数：
blocksiz = 128
batchsiz = 32
nemb = 256
nhead = 4
nlayers = 4
evalIters = 100
lr = 3e-4
epochs = 30

我的数据集是 93kb..，
我使用 GPT-2 作为 Tokenizer..
但是，当我使用 GPT-2 Tokenizer 时，训练时间太长了，
但是当我将模型训练为二元组时。它不需要那么多时间。
因此，使用具有 50257 个 Vocabsiz 的 GPT2 作为 tokenizer，会影响模型训练吗？
是否需要更多 GPU 资源来训练？
因此对于小型数据集。我应该使用什么作为标记器...？
我减少了超参数，上面我使用的参数是减少的结果
但模型训练时间太长了]]></description>
      <guid>https://stackoverflow.com/questions/79289981/how-to-use-tokenization-in-language-model</guid>
      <pubDate>Wed, 18 Dec 2024 04:30:58 GMT</pubDate>
    </item>
    <item>
      <title>EEG时域特征选择</title>
      <link>https://stackoverflow.com/questions/78833629/eeg-time-domain-features-selection</link>
      <description><![CDATA[我目前正在研究 EEG 数据并尝试对其进行分类。我想知道是否可以将每个时间步骤用作特征。这似乎不适合我，但我得到了不错的结果，所以我有点迷茫。
我有 16 个通道和 300 条记录，每条记录对应 4 个标签之一。我从时间域开始，因为我读到这个域可以用于分类（与其他域一起）。我做了一些预处理，比如过滤，但我在想：是否可以将每个通道用作观察值并将“EEG 时间序列”用作特征，以便每个特征都是一个时间步骤？
例如，如果我的录音时长为 2 秒，并且每 0.008 秒有一个值，那么我将有 2 / 0.008 = 250 个特征]]></description>
      <guid>https://stackoverflow.com/questions/78833629/eeg-time-domain-features-selection</guid>
      <pubDate>Mon, 05 Aug 2024 09:02:52 GMT</pubDate>
    </item>
    <item>
      <title>独热编码掩码的 resample_poly</title>
      <link>https://stackoverflow.com/questions/78827743/resample-poly-of-one-hot-encoded-masking</link>
      <description><![CDATA[我有这些张量：
X_test = X_unseen_flutter[0,0,:][None, :] # (批次大小，振幅长度) -&gt; (1, 3208)
y_true = y_unseen_flutter[0,0,:][None, :] # (批次大小，掩码长度，类别数量) -&gt; (1, 3208, 4) (独热编码)

我可以对 X_test 进行重新采样，但我不知道 y_true：
from scipy.signal import resample_poly

X_test_resampled = resample_poly(X_test, up=512, down=3208, axis=1) # (1, 512)
y_true_resampled = # ??? 我期望形状 (1, 512, 4)

除了独热编码标签外，resample_poly 的等价物是什么？
我希望有一个函数可以做到这一点，它接受 tensor, up, down, mask_axis, class_axis]]></description>
      <guid>https://stackoverflow.com/questions/78827743/resample-poly-of-one-hot-encoded-masking</guid>
      <pubDate>Sat, 03 Aug 2024 03:21:27 GMT</pubDate>
    </item>
    <item>
      <title>如何逐步训练朴素贝叶斯分类器？</title>
      <link>https://stackoverflow.com/questions/40639034/how-can-i-train-a-naivebayes-classifier-incrementally</link>
      <description><![CDATA[使用 Accord.NET，我创建了一个 NaiveBayes 分类器。它将根据 6 组左右的图像处理结果对像素进行分类。我的图像是 5MP，因此 50 张图像的训练集会创建一组非常大的训练数据。
每个像素 6 个 int 数组 * 500 万像素 * 50 张图像。
除了尝试将所有数据存储在内存中之外，有没有办法逐步训练 NaiveBayes 分类器？多次调用 Learn() 每次都会覆盖旧数据，而不是添加数据。]]></description>
      <guid>https://stackoverflow.com/questions/40639034/how-can-i-train-a-naivebayes-classifier-incrementally</guid>
      <pubDate>Wed, 16 Nov 2016 17:56:08 GMT</pubDate>
    </item>
    </channel>
</rss>