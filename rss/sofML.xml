<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 22 Jan 2024 01:04:45 GMT</lastBuildDate>
    <item>
      <title>如何提高YOLOv8自定义模型的准确率？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77855989/how-to-increase-the-accuracy-of-yolov8-custom-model</link>
      <description><![CDATA[我最近使用 YOLOv8 训练了一个用于动物物种检测的自定义数据集，但得到的结果非常糟糕。我有 70 多个类，每个类平均约有 80 张图像。我对模型进行了 200 个时期的训练，大约花费了 1.5 天。
我使用 Roboflow 来注释数据集。
我首先在 100 个 epoch 上训练模型，这给了我不好的结果，所以我再次尝试在 200 个 epoch 上训练模型，这比之前的模型得到了更好的结果。模型的准确率很低，没有达到我的预期。所以我需要一些帮助来提高模型的准确性。 我打算将类别增加到 110-115，但从当前的训练结果来看，我首先尝试使模型的准确率至少达到 75%。]]></description>
      <guid>https://stackoverflow.com/questions/77855989/how-to-increase-the-accuracy-of-yolov8-custom-model</guid>
      <pubDate>Sun, 21 Jan 2024 17:57:19 GMT</pubDate>
    </item>
    <item>
      <title>如何为 Pytorch 预处理表格数据中的地址、纬度和经度特征</title>
      <link>https://stackoverflow.com/questions/77855711/how-to-preprocess-address-latitude-and-longitude-features-in-tabular-data-for-p</link>
      <description><![CDATA[我已将数据清除到接下来的 6 列中，您可以看到，这是我的输入数据。我分割数据集，将标签放在另一个变量 Y 中。
我的主要问题：我不知道如何预处理数据以便为任何模型提供良好的输入。
我的数据集 X 如下所示：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

描述
提示
地址
地区
纬度
经度


&lt;正文&gt;

加尔彭
工业
Subdivisión de la Finca Denominada Violeta S/N
阿里卡和帕里纳科塔-阿里卡地区
-19.423411
-11.371551





desc - 字符串
tipo - 字符串
地址 - 字符串
区域 - 字符串
纬度 - 字符串
经度 - 字符串

我的数据集 Y 看起来像

&lt;表类=“s-表”&gt;
&lt;标题&gt;

首席信息官


&lt;正文&gt;

169379



我尝试过的内容
我已经遵循了这个教程，它允许我对表格数据有了一些了解，但数据完全不同，我不知道它是否也适合我。因此，我的代码将所有数据转换为 LabelEncoder，但显然这不适用于纬度和经度。
对于 df.columns 中的 col：
    如果 df.dtypes[col] == “对象”：
        df[col] = df[col].fillna(“NA”)
    别的：
        df[列] = df[列].fillna(0)
    df[col] = LabelEncoder().fit_transform(df[col])

对于 df.columns 中的 col：
    df[col] = df[col].astype(&#39;类别&#39;)

此外，作者还使用了一些分类嵌入，我不知道它们是否也适用于我的数据类型。]]></description>
      <guid>https://stackoverflow.com/questions/77855711/how-to-preprocess-address-latitude-and-longitude-features-in-tabular-data-for-p</guid>
      <pubDate>Sun, 21 Jan 2024 16:39:27 GMT</pubDate>
    </item>
    <item>
      <title>GaussianNB 准确率分数似乎不起作用</title>
      <link>https://stackoverflow.com/questions/77855644/gaussiannb-accuracy-score-seemingly-not-working</link>
      <description><![CDATA[这是我的代码：
来自 sklearn.feature_extraction.text 导入 *
从 sklearn.model_selection 导入 *
从 sklearn.metrics 导入 *

CV = CountVectorizer()

#将稀疏数组转换为密集数组#transformed_text 是单句输入
X =cv.fit_transform(df[&#39;transformed_text&#39;]).toarray()


#获取标记输出y
y =df[&#39;目标&#39;].值


#20% 的数据用于测试
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)


gnb = GaussianNB()

gnb.fit(X_train,y_train)
y_pred1=gnb.predict(X_train)

准确度分数（y_test，y_pred1）

虽然我使用了average=weighed，但我在accuracy_score(y_test, y_pred1)部分遇到错误
------------------------------------------------ ----------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-96-f91e0ea15d1f&gt;在&lt;细胞系：1&gt;()
----&gt; 1 准确度分数（y_test，y_pred1）

3帧
包装器中的 /usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py(*args, **kwargs)
    190
    191 尝试：
--&gt; 192 return func(*args，**kwargs)
    193 除了 InvalidParameterError 为 e：
    攀上漂亮女局长之后194

/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py中的accuracy_score（y_true，y_pred，标准化，sample_weight）
    219
    [第 220 章] 第 220 章
--&gt;第221章
    第222章
    223 if y_type.startswith(“multilabel”):

/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py 中的 _check_targets(y_true, y_pred)
     84 y_pred ：数组或指示矩阵
     第85章
---&gt; 86 检查一致长度（y_true，y_pred）
     87 type_true = type_of_target(y_true, input_name=“y_true”)
     88 type_pred = type_of_target(y_pred, input_name=“y_pred”)

/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py 在 check_consistent_length(*arrays)
    第395章
    [第 396 章] 1：
--&gt;第397章
    [第 398 章]
    399 % [int(l)，长度为 l]

ValueError：发现输入变量的样本数量不一致：[1034、4135]

我正在遵循的教程实现此功能没有任何问题
有什么帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/77855644/gaussiannb-accuracy-score-seemingly-not-working</guid>
      <pubDate>Sun, 21 Jan 2024 16:18:05 GMT</pubDate>
    </item>
    <item>
      <title>Flower - 联邦学习的每一轮模型精度都是相同的</title>
      <link>https://stackoverflow.com/questions/77855250/model-accuracy-is-the-same-after-every-round-with-flower-federated-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77855250/model-accuracy-is-the-same-after-every-round-with-flower-federated-learning</guid>
      <pubDate>Sun, 21 Jan 2024 14:33:27 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 输入、输出和隐藏状态混乱 [关闭]</title>
      <link>https://stackoverflow.com/questions/77855145/lstm-input-output-and-hidden-state-confusion</link>
      <description><![CDATA[我对如何准备用于训练 LSTM 的数据感到困惑。它们有一个隐藏状态（至少在 Keras 中也是输出）和一个单元状态，它们都用于下一个时间步。我看到很多人也使用一个时间步的输出作为下一时间步的输入，但根据我的理解，信息应该已经被隐藏和/或单元状态覆盖。
有时人们想要预测时间序列的下一个时间步长并使用之前的时间步长作为输入，这对我来说很有意义。但就我而言，我有多个时间序列作为输入，多个不同时间序列作为输出，我知道这些时间序列取决于输入。
在这种情况下，您是否也会使用输出作为下一个时间步的输入？
LSTM 是解决此类问题的正确选择吗？]]></description>
      <guid>https://stackoverflow.com/questions/77855145/lstm-input-output-and-hidden-state-confusion</guid>
      <pubDate>Sun, 21 Jan 2024 14:09:41 GMT</pubDate>
    </item>
    <item>
      <title>警告：ctransformers：令牌数量（6462）超过最大上下文长度（2048）[关闭]</title>
      <link>https://stackoverflow.com/questions/77854655/warningctransformersnumber-of-tokens-6462-exceeded-maximum-context-length-2</link>
      <description><![CDATA[我正在尝试对我的 json 数据使用 Mistra 7B 指令，当我使用 Ctransformer 加载我的模型时，我收到此错误。谁能帮我解决我的问题吗？
到目前为止，我尝试了不同的方法来加载模型，但它们都不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/77854655/warningctransformersnumber-of-tokens-6462-exceeded-maximum-context-length-2</guid>
      <pubDate>Sun, 21 Jan 2024 11:46:28 GMT</pubDate>
    </item>
    <item>
      <title>“Atari 基于模型的强化学习”论文中代理的输入是什么？为什么世界模型在推理时运行？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77854230/what-is-the-input-to-the-agent-in-the-paper-model-based-reinforcement-learning</link>
      <description><![CDATA[我目前正在阅读论文“Atari 基于模型的强化学习” （链接：https://arxiv.org/abs/1903.00374）。但是，他们没有指定具体使用什么作为代理的输入。
我相信它是观察空间 - 意思是堆叠的框架。在训练期间，这些将由世界模型提供，而在推理时，真实环境将提供它们 - 对吗？然而，他们还在第 4 节“随机模型”下指定了模型。这个随机模型 - 据我所知，在某种意义上是世界模型的一部分 - 在推理时运行。但只有当我们在那段时间使用世界模型的任何输出时，这才有意义，但据我所知，我们并不这样做。也许有人能为我澄清这一点。]]></description>
      <guid>https://stackoverflow.com/questions/77854230/what-is-the-input-to-the-agent-in-the-paper-model-based-reinforcement-learning</guid>
      <pubDate>Sun, 21 Jan 2024 09:16:43 GMT</pubDate>
    </item>
    <item>
      <title>检测图像中的地板并更改其颜色[关闭]</title>
      <link>https://stackoverflow.com/questions/77853519/detecting-the-floor-in-an-image-and-change-its-color</link>
      <description><![CDATA[我的目标是更改给定图像中的地板颜色。有没有我可以使用的工具、库、产品等？流程将非常简单：

选择照片
为地板选择新颜色
使用地板的新颜色更新图像
]]></description>
      <guid>https://stackoverflow.com/questions/77853519/detecting-the-floor-in-an-image-and-change-its-color</guid>
      <pubDate>Sun, 21 Jan 2024 03:26:35 GMT</pubDate>
    </item>
    <item>
      <title>如何预测带有字符串的结构化数据输入中的数字？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77853308/how-to-predict-a-number-in-a-structured-data-input-with-strings</link>
      <description><![CDATA[我在尝试使用字符串作为输入来预测数字时遇到了一些麻烦。为了具体化，我的输入程序是一组考虑建筑物地址以及与该建筑物关联的 ID 的列。我的数据如下所示：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

building_id
标题
简短描述
构造类型
地址
地区 - 州 - 城市
纬度 - 经度
首席信息官


&lt;正文&gt;

1836
Obra nueva destinada a galpón de 2 pisos de altura con superficie总量de 2.037 m2
加尔彭
工业
Subdivisión de la Finca Denominada Violeta S/N
阿里卡和帕里纳科塔-阿里卡地区
-19.423411,-11.371551
183653




所以，基本上我想预测的是 CIO。这是与整个项目关联的唯一编号，输入数据如下：

building_id - 整数
标题 - 字符串
建筑物外观的简短描述 - 字符串
一种构造类型，表示如何使用该构造 - 字符串
字符串形式的地址 - 字符串
建筑物所在的州和城市 - 字符串
纬度和经度 - 字符串

如何制作一个仅使用输入数据即可预测 CIO 编号（有点像 ID）的项目？我不知道如何使用字符串，所以我想了解如何正确地执行此操作并获得良好的准确性！]]></description>
      <guid>https://stackoverflow.com/questions/77853308/how-to-predict-a-number-in-a-structured-data-input-with-strings</guid>
      <pubDate>Sun, 21 Jan 2024 01:11:48 GMT</pubDate>
    </item>
    <item>
      <title>我无法从“typing_extensions”导入名称“TypeAliasType”</title>
      <link>https://stackoverflow.com/questions/77450322/i-cannot-import-name-typealiastype-from-typing-extensions</link>
      <description><![CDATA[我是 Python 新手，发现了以下类似错误。我非常感谢您的评论。谢谢
我尝试将 Gradio 库导入为 gr
我已经尝试了一些现有的建议，但结果都是徒劳的。我不知道该怎么办]]></description>
      <guid>https://stackoverflow.com/questions/77450322/i-cannot-import-name-typealiastype-from-typing-extensions</guid>
      <pubDate>Thu, 09 Nov 2023 03:38:10 GMT</pubDate>
    </item>
    <item>
      <title>在微调LLM模型时如何给数据集赋予权重或排名？</title>
      <link>https://stackoverflow.com/questions/76958393/how-to-give-weights-or-ranking-to-dataset-while-finetuning-the-llm-model</link>
      <description><![CDATA[我目前正在使用 Llama 配方和 LoRA 技术对 meta-llama/Llama-2-7b-chat-hf 模型进行微调。我的方法包括采用即时工程来改进模型的性能，利用以 Alpaca 格式呈现的数据：
&lt;前&gt;&lt;代码&gt;[
    {
        &quot;instruction&quot;: &quot;什么是 CubeOS？&quot;,
        “输入”：“”，
        “输出”：“CubeOS 是专门的操作系统，包含操作 Cube 所需的所有软件和驱动程序。”
    },
    {
        &quot;instruction&quot;: &quot;Myst 是什么？&quot;,
        “输入”：“”，
        “output”：“Myst 作为 Cube 的控制台界面，也是随附应用程序的指定名称。”
    },
    。
    。
    。
]

这个过程使我能够有效地微调模型并将其应用于回答与机密文档相关的问题。
我尝试过为问答对分配分数，然后按照这些分数的顺序排列它们以进行微调。然而，我遇到了挑战，因为该模型似乎没有根据分数较高的数据的重要性给出结果。
我发现了一篇标题为 https://towardsdatascience.com/how-to-fine-tune-llama2-for-python-coding-on-consumer-hardware-46942fa3cf92，作者似乎采用了类似的方法。&lt; /p&gt;
此外，我还探索了其他资源，提出了一种方法，涉及确定各个特征的相关性分数，然后对它们进行排序以进行微调。以下链接 https://pyvideo.org /pydata-warsaw-2019/learning-to-rank-with-the-transformer.html 提供了对此技术的见解。
我还尝试使用 llama-2 所需的提示结构来训练模型：
[INST] &lt;&gt;&gt; {{ system_prompt }} &lt;&lt;/SYS&gt;&gt; {{ 用户消息 }} [/INST]

但是，这种方法并没有对答案产生令人满意的强调。
鉴于我拥有 PDF 和 DOC 格式的文档，我的目标是为特定文档分配更大的权重，并确保它们优先出现在最佳答案中。
我非常感谢您指导如何通过合并权重或分数来强调某些文档的重要性来微调模型。]]></description>
      <guid>https://stackoverflow.com/questions/76958393/how-to-give-weights-or-ranking-to-dataset-while-finetuning-the-llm-model</guid>
      <pubDate>Wed, 23 Aug 2023 05:01:40 GMT</pubDate>
    </item>
    <item>
      <title>AutoModelForSeq2SeqLM 和 AutoModelForCausalLM 之间的区别</title>
      <link>https://stackoverflow.com/questions/75549632/difference-between-automodelforseq2seqlm-and-automodelforcausallm</link>
      <description><![CDATA[根据标题，Huggingface 上的这两个自动类有何不同？我尝试阅读文档，但没有找到区分信息]]></description>
      <guid>https://stackoverflow.com/questions/75549632/difference-between-automodelforseq2seqlm-and-automodelforcausallm</guid>
      <pubDate>Thu, 23 Feb 2023 19:45:50 GMT</pubDate>
    </item>
    <item>
      <title>如何对拥抱脸部模型进行批量推理？</title>
      <link>https://stackoverflow.com/questions/68058974/how-to-do-batch-inferenece-for-hugging-face-models</link>
      <description><![CDATA[我想对 MarianMT 模型进行批量推理。代码如下：
从变压器导入 MarianTokenizer
tokenizer = MarianTokenizer.from_pretrained(&#39;赫尔辛基-NLP/opus-mt-en-de&#39;)
src_texts = [“我是一只小青蛙。”,“汤姆向老师寻求建议。”]
tgt_texts = [“Ich bin ein kleiner Frosch。”,“Tom bat seinen Lehrer um Rat。”] # 可选
输入 = tokenizer(src_texts, return_tensors=“pt”, padding=True)
使用 tokenizer.as_target_tokenizer()：
    标签 = tokenizer(tgt_texts, return_tensors=“pt”, padding=True)
输入[“标签”] = 标签[“input_ids”]
输出=模型（**输入）

如何进行批量推理？]]></description>
      <guid>https://stackoverflow.com/questions/68058974/how-to-do-batch-inferenece-for-hugging-face-models</guid>
      <pubDate>Sun, 20 Jun 2021 18:35:50 GMT</pubDate>
    </item>
    <item>
      <title>Google Colaboratory：有关其 GPU 的误导性信息（仅 5% RAM 可供某些用户使用）</title>
      <link>https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available</link>
      <description><![CDATA[更新：这个问题与Google Colab的“笔记本设置：硬件加速器：GPU”有关。这个问题是在添加“TPU”选项之前写的。
阅读了多个关于 Google Colaboratory 提供免费 Tesla K80 GPU 的激动人心的公告，我尝试运行 fast.ai 课程让它永远无法完成 - 内存很快就耗尽了。我开始调查原因。
最重要的是，“免费 Tesla K80”并不是对所有人来说都是“免费”的——对于某些人来说，只有一小部分是“免费”的。 
我从加拿大西海岸连接到 Google Colab，但本应是 24GB GPU RAM 的却只有 0.5GB。其他用户可以使用 11GB GPU RAM。
显然 0.5GB GPU RAM 不足以满足大多数 ML/DL 工作的需要。
如果您不确定自己得到什么，这里是我整理的一些调试功能（仅适用于笔记本电脑的 GPU 设置）：
# 内存占用支持库/代码
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip 安装 gputil
!pip 安装 psutil
!pip 安装人性化
导入 psutil
导入人性化
导入操作系统
导入 GPUtil 作为 GPU
GPU = GPU.getGPUs()
# XXX：Colab 上只有一个 GPU，且无法保证
GPU = GPU[0]
def printm():
 进程 = psutil.Process(os.getpid())
 print(&quot;Gen RAM 空闲：&quot; + humanize.naturalsize( psutil.virtual_memory().available ), &quot; | Proc 大小：&quot; + humanize.naturalsize( process.memory_info().rss))
 print(&quot;GPU RAM 可用：{0:.0f}MB | 已用：{1:.0f}MB | Util {2:3.0f}% | 总计 {3:.0f}MB&quot;.format(gpu.memoryFree, gpu.memoryUsed、gpu.memoryUtil*100、gpu.memoryTotal))
打印（）

在运行任何其他代码之前在 jupyter 笔记本中执行它会给我：
Gen RAM 可用：11.6 GB |进程大小：666.0 MB
GPU 可用内存：566MB |已用：10873MB |利用率 95% |总计 11439MB

获得完整卡的幸运用户将看到：
Gen RAM 可用：11.6 GB |进程大小：666.0 MB
GPU 可用内存：11439MB |已用：0MB |利用率 0% |总计 11439MB

您是否发现我从 GPUtil 借用的 GPU RAM 可用性计算有任何缺陷？
您能否确认，如果您在 Google Colab 笔记本上运行此代码，您会得到类似的结果吗？
如果我的计算正确，有什么办法可以在免费盒子上获得更多 GPU RAM 吗？
更新：我不确定为什么我们中的一些人得到的只是其他用户的 1/20。例如帮助我调试这个的人来自印度，他掌握了全部内容！
注意：请不要再发送任何有关如何消除可能消耗 GPU 部分的潜在卡住/失控/并行笔记本的建议。不管你如何划分它，如果你和我在同一条船上并运行调试代码，你会发现你仍然获得总共 5% 的 GPU RAM（截至本次更新仍然如此）。]]></description>
      <guid>https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available</guid>
      <pubDate>Mon, 12 Feb 2018 15:44:14 GMT</pubDate>
    </item>
    <item>
      <title>Keras LSTM - 验证损失从 Epoch #1 开始增加</title>
      <link>https://stackoverflow.com/questions/48542473/keras-lstm-validation-loss-increasing-from-epoch-1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/48542473/keras-lstm-validation-loss-increasing-from-epoch-1</guid>
      <pubDate>Wed, 31 Jan 2018 12:40:36 GMT</pubDate>
    </item>
    </channel>
</rss>