<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 18 Jun 2024 12:28:10 GMT</lastBuildDate>
    <item>
      <title>pmdarima 中的 auto_arima 的执行时间随着 m 的增加而疯狂增加，原因是什么？</title>
      <link>https://stackoverflow.com/questions/78636875/auto-arima-from-pmdarima-has-insane-execution-times-increases-at-the-growing-of</link>
      <description><![CDATA[我正在尝试为我的时间序列拟合 SARIMA 模型，为了找到最佳模型，我正在使用 pmdarima 的 autom_arima。代码很简单：
stepwise_fit = pm.auto_arima(train_dataset[&#39;shift_hours&#39;], start_p=1, start_q=1,
max_p=3, max_q=3, m=365,
start_P=0, seasonal=True,
d=1, D=1, trace=True,
error_action=&#39;ignore&#39;, 
suppress_warnings=True, 
stepwise=True) 

其中 &#39;train_dataset[&#39;shift_hours&#39;]&#39; 由大约两年的每日值组成，这些值具有 365 天的季节性。
我遇到的问题与执行时间有关：当 m 设置为 1 时，在逐步搜索中执行一步需要不到一秒的时间，当它为 7 或 12 时，一步的执行时间在 1 到 2 秒之间，但是当我将 m 设置为 365（我需要的值），执行单个步骤需要 30 多分钟。如果我使用 girdsearch 并将 n_jobs &gt; 4，则会出现内存错误。
就我对 SARIMA 的了解而言，当我使用更大的周期时，它不应该花费更多时间来拟合一个系列，因为它用于对系列进行建模的术语数量不依赖于 m，所以我想知道那里发生了什么，以及是否发生了什么我无法理解的事情]]></description>
      <guid>https://stackoverflow.com/questions/78636875/auto-arima-from-pmdarima-has-insane-execution-times-increases-at-the-growing-of</guid>
      <pubDate>Tue, 18 Jun 2024 10:36:25 GMT</pubDate>
    </item>
    <item>
      <title>如何准确检测不同音轨中主节拍和配乐的开始？</title>
      <link>https://stackoverflow.com/questions/78636871/how-to-accurately-detect-the-start-of-the-main-beat-and-soundtracks-in-diverse-a</link>
      <description><![CDATA[我正在做一个需要编辑配乐的项目。挑战在于检测任何给定配乐的主要节拍和旋律何时得到正确发展。我确信有更好的术语来描述我的目标，但理想情况下，我想跳过“构建”并立即让歌曲从“主要部分”开始。这需要适用于不同类型的各种歌曲，这些歌曲通常具有不同的结构和开始模式，这使得简化流程变得困难。
例如：
https://www.youtube.com/watch?v=P77CNtHrnmI -&gt;我希望我的代码能够识别 0:24 处的开始
https://www.youtube.com/watch?v=OOsPCR8SyRo -&gt; 0:12 处的开始检测
https://www.youtube.com/watch?v=XKiZBlelIzc -&gt; 0:19 处的起始检测
我尝试使用 librosa 分析起始强度并检测节拍，但当前的实现要么检测到歌曲的最开始，要么无法一致地识别节拍何时完全形成。
这是我的方法；
def analyze_and_edit_audio(input_file, output_file):
y, sr = librosa.load(input_file)
tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)
beat_times = librosa.frames_to_time(beat_frames, sr=sr)
main_beat_start = beat_times[0]

我对 librosa/audio 编辑经验很少，因此如果您有任何建议，我将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78636871/how-to-accurately-detect-the-start-of-the-main-beat-and-soundtracks-in-diverse-a</guid>
      <pubDate>Tue, 18 Jun 2024 10:35:15 GMT</pubDate>
    </item>
    <item>
      <title>用于文本生成的微调 BERT 模型（填字游戏解答器）</title>
      <link>https://stackoverflow.com/questions/78636736/fine-tuning-bert-model-for-text-generation-crossword-solver</link>
      <description><![CDATA[我需要在我的 NLP 项目中提供帮助，该项目的目标是预测给定填字游戏线索的可能答案列表。这个想法是使用填字游戏线索 - 答案对的数据集微调 BERT 模型。
train.source 看起来像这样：
机场排队，一种煎蛋卷，苏萨是它的首都，后缀为洞穴......或峡谷？，九：前缀，龙的猎物，一些烟火，...
train.targets 看起来像这样：LIMOS、EGGWHITE、ELAM、OUS、ENNEA、MAIDEN、FLARES，...
目前，我有一个用于 train、test 和 val 中的数据集的加载函数。 (len(train) = 433034, len(val) = 72304, len(test) = 72940)
我从 BERT 加载了两个模型，即 tokenizer 和 model：
tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
model = BertForMaskedLM.from_pretrained(&#39;bert-base-uncased&#39;)

然后我创建了一个 BERTDataset 类：
class BERTDataset(Dataset):
def __init__(self, tokenizer, texts, target=None, max_length=512):
self.tokenizer = tokenizer
self.texts = texts
self.targets = target # 如果提供了特定目标响应，则可选使用
self.max_length = max_length

def __len__(self):
return len(self.texts)

def __getitem__(self, idx):
text = self.texts[idx]
# 将 &#39;[MASK]&#39; 附加到文本末尾
text_with_mask = text + &quot; ? [MASK].&quot;

# 使用附加的 MASK 标记对文本进行编码
encoding = self.tokenizer.encode_plus(
text_with_mask,
max_length=self.max_length,
padding=&#39;max_length&#39;,
truncation=True,
return_tensors=&#39;pt&#39;
)

input_ids = encoding[&#39;input_ids&#39;].squeeze(0) # 删除批次维度
tention_mask = encoding[&#39;attention_mask&#39;].squeeze(0)

# 如果不需要预测，则标签理想情况下应为 -100
labels = input_ids.detach().clone()
# 如果输入 ID 未被屏蔽，则将标签设置为 -100
labels[labels != self.tokenizer.mask_token_id] = -100

return {
&#39;input_ids&#39;: input_ids,
&#39;attention_mask&#39;:tention_mask,
&#39;labels&#39;: labels
}

train_dataset = BERTDataset(tokenizer, train_sources, train_targets, max_length=128)
val_dataset = BERTDataset(tokenizer, val_sources, val_targets, max_length=128)

最后启动训练：
from transformers import TrainingArguments,Trainer

# 定义训练参数
training_args = TrainingArguments(
output_dir=&#39;./results&#39;, # 输出目录
num_train_epochs=3, # 训练周期数
per_device_train_batch_size=8, # 训练批次大小
per_device_eval_batch_size=16, # 评估批次大小
warmup_steps=500, # 学习率调度程序的预热步骤数
weight_decay=0.005, # 权重衰减强度
logs_dir=&#39;./logs&#39;, # 存储日志的目录
logging_steps=10,
)

# 初始化 Trainer
trainer = Trainer(
model=model,
args=training_args,
train_dataset=train_dataset,
eval_dataset=val_dataset,

#compute_metrics=lambda eval_pred: {&quot;loss&quot; : eval_pred.loss}
)

# 开始训练
trainer.train()

我遇到了一些问题，主要问题是训练损失很快变为 0，我不明白为什么（从步骤 510 开始，训练损失为 0）：

我做错了什么吗？我真的不明白为什么模型没有正确训练。谢谢！！]]></description>
      <guid>https://stackoverflow.com/questions/78636736/fine-tuning-bert-model-for-text-generation-crossword-solver</guid>
      <pubDate>Tue, 18 Jun 2024 10:06:20 GMT</pubDate>
    </item>
    <item>
      <title>Android 中的 Movenet Singlepose 照明模型：“不支持的图像格式：1”错误</title>
      <link>https://stackoverflow.com/questions/78636622/movenets-singlepose-lighting-model-in-android-unsupported-image-format-1-e</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78636622/movenets-singlepose-lighting-model-in-android-unsupported-image-format-1-e</guid>
      <pubDate>Tue, 18 Jun 2024 09:42:32 GMT</pubDate>
    </item>
    <item>
      <title>我如何重新训练 Xgboost 回归器？</title>
      <link>https://stackoverflow.com/questions/78636365/how-can-i-retrain-a-xgboost-regressor</link>
      <description><![CDATA[我现在正在做的实习要求模型“定期更新新数据”。如何在 Xgboost 上做到这一点？我已将模型保存为 pickle 文件
在项目中，我将从后端获取新数据，并且我应该定期重新训练模型]]></description>
      <guid>https://stackoverflow.com/questions/78636365/how-can-i-retrain-a-xgboost-regressor</guid>
      <pubDate>Tue, 18 Jun 2024 08:51:09 GMT</pubDate>
    </item>
    <item>
      <title>每次运行机器学习预测都错误</title>
      <link>https://stackoverflow.com/questions/78636296/wrong-machine-learning-predictions-at-every-run</link>
      <description><![CDATA[我训练了一个机器学习模型，测试集中的几行相同的数据（1800 行中的大约 100 行）在使用不同种子的每次运行（10 次）时都会给出错误的预测。我应该对此做些什么吗？例如将其放入训练集中以将一些数据从训练集换到测试集，还是保持原样？
使用不同的种子运行模型 10 次。有些行始终得到错误的预测。]]></description>
      <guid>https://stackoverflow.com/questions/78636296/wrong-machine-learning-predictions-at-every-run</guid>
      <pubDate>Tue, 18 Jun 2024 08:37:10 GMT</pubDate>
    </item>
    <item>
      <title>根据 HistGratientBoostingClassifier 绘制决策树</title>
      <link>https://stackoverflow.com/questions/78636029/plot-a-decision-tree-from-histgratientboostingclassifier</link>
      <description><![CDATA[我有一个 HistGradientBoostingClassifier 模型，我想绘制一个或多个决策树，但我找不到原生函数来执行此操作，我可以访问 Tree 预测器对象及其节点，但为了将其绘制到 sklearn.tree.plot_tree 函数中，它需要是 DecisionTree 类型的对象
我试过这个：
from sklearn.tree import plot_tree

plot_tree(RF_90._predictors[0][0])

出现此错误：

InvalidParameterError：plot_tree 的 &#39;decision_tree&#39; 参数必须
是 &#39;sklearn.tree._classes.DecisionTreeClassifier&#39; 的实例或
&#39;sklearn.tree._classes.DecisionTreeRegressor&#39;。得到的是
&lt;sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor
对象位于 0x7f676ebf0310&gt;。

注意：RF_90 是 HistGradientBoostingClassifier 拟合模型]]></description>
      <guid>https://stackoverflow.com/questions/78636029/plot-a-decision-tree-from-histgratientboostingclassifier</guid>
      <pubDate>Tue, 18 Jun 2024 07:26:34 GMT</pubDate>
    </item>
    <item>
      <title>用于特征节点值插值的图神经网络</title>
      <link>https://stackoverflow.com/questions/78635913/graph-neural-networks-for-features-nodes-value-interpolations</link>
      <description><![CDATA[我目前正在尝试了解是否存在基于 GNN 的深度学习技术，可以重建图的部分区域。例如，我有一个包含 5 个节点和 10 条边的图，我知道 3 个节点和所有 10 条边的特征的数值，我想构建一个能够预测其他 2 个缺失节点的特征中应该出现的假设值的网络。我试图想象这种架构，但我发现很难想象基于 GCN 或消息传递的网络如何与我没有数值但想找到最佳值的节点一起运行。
我目前尝试咨询图的主要 DL 方法，但问题似乎是这种类型的任务没有得到解决。因此，我质疑它的可行性。
（https://distill.pub/2021/gnn-intro/，以及一些教程https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html）]]></description>
      <guid>https://stackoverflow.com/questions/78635913/graph-neural-networks-for-features-nodes-value-interpolations</guid>
      <pubDate>Tue, 18 Jun 2024 06:59:41 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 tfa.losses.TripletSemiHardLoss 训练具有三重损失的暹罗网络？</title>
      <link>https://stackoverflow.com/questions/78635866/how-to-train-a-siamese-network-with-triplet-loss-using-tfa-losses-tripletsemihar</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78635866/how-to-train-a-siamese-network-with-triplet-loss-using-tfa-losses-tripletsemihar</guid>
      <pubDate>Tue, 18 Jun 2024 06:47:18 GMT</pubDate>
    </item>
    <item>
      <title>运行时错误预测张量大小为 64，与 y_train 张量大小不同，张量暗淡错误</title>
      <link>https://stackoverflow.com/questions/78635713/runtime-error-pred-tensor-size-is-64-which-is-different-to-y-train-tensor-size</link>
      <description><![CDATA[我正在尝试使用 Hippo 模型来预测外汇价格（数据集有日期时间列和开盘价列）。以下是完整代码的 GitHub 链接 GitHub Repo
原始 hippo 模型可在此链接中找到：GitHub Model
我有两个问题：

在更新状态 u 中，第二个 dim 应该是批处理大小，即 [64,1]，在我的情况下，我将其设置为 1，但它仍然显示 [1, 64]
运行时错误为 pred 张量大小与 output20 中的 y_train 大小不匹配。

以下是错误详细信息
Cell In[18]，第 17 行，在 train(X_train, y_train, model, loss_fn, optimizer, batch_size, device)
14 print(&#39;pred.shpae:&#39;,pred.shape)
15 print(&#39;y_train&#39;,y_train.shape)
---&gt; 17 loss = loss_fn(pred, torch.tensor(y_train))
18 optimizer.zero_grad()
19 loss_backward()

文件 ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1511，位于 Module._wrapped_call_impl(self, *args, **kwargs)
1509 return self._compiled_call_impl(*args, **kwargs) # 类型：ignore[misc]
1510 else:
-&gt; 1511 return self._call_impl(*args, **kwargs)

File ~\anaconda3\lib\site-packages\torch\nn\modules\module.py:1520, in Module._call_impl(self, *args, **kwargs)
1515 # 如果我们没有任何钩子，我们希望跳过此函数中的其余逻辑
1516 # 并直接调用 forward。
1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
1518 or _global_backward_pre_hooks or _global_backward_hooks
1519 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1520 返回 forward_call(*args, **kwargs)
1522 尝试：
1523 结果 = None

文件 ~\anaconda3\lib\site-packages\torch\nn\modules\loss.py:535，位于 MSELoss.forward(self, input, target)
534 def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
-&gt; 535 返回 F.mse_loss(input, target, reduction=self.reduction)

文件 ~\anaconda3\lib\site-packages\torch\nn\ functional.py:3338，位于 mse_loss(input, target, size_average, reduce, reduction)
3335 如果 size_average 不是 None 或 reduce 不是 None：
3336 reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 3338 expand_input, expand_target = torch.broadcast_tensors(input, target)
3339 return torch._C._nn.mse_loss(expanded_input, expand_target, _Reduction.get_enum(reduction))

文件 ~\anaconda3\lib\site-packages\torch\ functional.py:76，在 broadcast_tensors(*tensors) 中
74 if has_torch_function(tensors):
75 return handle_torch_function(broadcast_tensors, tensors, *tensors)
---&gt; 76 return _VF.broadcast_tensors(tensors)

RuntimeError: 在非单例维度 0 处，张量 a (64) 的大小必须与张量 b (4322) 的大小匹配

这是我的“训练”方法在 loss_fn 上抛出错误
def train(X_train, y_train, model, loss_fn, optimizer, batch_size = None, device = None):
size = len(X_train)
if device is None:
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

model.to(device)
if batch_size is None:
# X_train_tensor= torch.tensor(X_train, dtype = torch.float32).to(device)
# y_train_tensor = torch.tensor(y_train, dtype = torch.float32).to(device)

model.train()
pred = model(X_train)[0] 
print(&#39;pred.shpae:&#39;,pred.shape)
print(&#39;y_train&#39;,y_train.shape)

loss = loss_fn(pred, torch.tensor(y_train))
optimizer.zero_grad()
loss_backward()
optimizer.step()
loss_value = loss.item()
print(f&#39;loss:{loss_value:&gt;7f}, [{size:&gt;5d}/{size:&gt;5d}]&#39;)
else:
dataset = TimeSeriesDataset(X_train, y_train)
dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = False)

model.train()

for batch_idx, (X_batch, y_batch )in enumerate(dataloader):
X_batch, y_batch = X_batch.to(device,dtype = torch.float32), y_batch.to(device,dtype = torch.float32)

# 计算预测误差
pred = model(X_batch)
loss_ = loss_fn(pred, y_batch)

optimizer.zero_grad()
loss.backward()
optimizer.step()

if batch%10==0:
loss_value = loss.item()
current = batch_idx * len(X_batch)
print(f&quot;loss: {loss_value:&gt;7f} [{current:&gt;5d}/{len(dataset):&gt;5d}]&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78635713/runtime-error-pred-tensor-size-is-64-which-is-different-to-y-train-tensor-size</guid>
      <pubDate>Tue, 18 Jun 2024 06:03:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 阅读阿拉伯语 pdf 书</title>
      <link>https://stackoverflow.com/questions/78631415/read-arabic-pdf-book-using-python</link>
      <description><![CDATA[我正在使用 python 阅读一本阿拉伯语书籍（pdf 是可选的，它不需要任何 OCR（光学字符识别从图像中提取文本）），所以我使用了多个库 pdfplumber、pdfminer.six 和 flitz（PyMuPdf））这是我使用的代码之一：
import pdfplumber
from bidi.algorithm import get_display
import arabic_reshaper
import re

def clean_text(text):
# 删除 NULL 字节和控制字符
cleaned_text = re.sub(r&#39;[\x00-\x1F\x7F]&#39;, &#39;&#39;, text)
return cleaned_text

def reshape_and_bidi_text(text):
# 重塑阿拉伯语文本并应用 bidi 算法
reshaped_text = arabic_reshaper.reshape(text)
bidi_text = get_display(reshaped_text)
return bidi_text

def extract_text_from_pdf(pdf_path):
text = &quot;&quot;
使用 pdfplumber.open(pdf_path) 作为 pdf:
对于 pdf.pages 中的 page:
page_text = page.extract_text()
如果 page_text:
text += page_text + &quot;\n&quot;
返回文本

def save_text_to_file(text, output_path):
with open(output_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as text_file:
text_file.write(text)

def convert_pdf_to_text(pdf_path, output_path):
# 使用 pdfplumber 从 PDF 中提取文本
extracted_text = extract_text_from_pdf(pdf_path)

# 清理提取的文本
cleaned_text = clean_text(extracted_text)

# 重塑文本并将 bidi 算法应用于文本
reshaped_bidi_text = reshape_and_bidi_text(cleaned_text)

# 将清理和重塑的文本保存到文本文件
save_text_to_file(reshaped_bidi_text, output_path)
print(f&quot;来自 {pdf_path} 的文本已保存到 {output_path}&quot;)

# 示例用法
pdf_path = r&#39;C:\Users\DELL\Desktop\Book Printed\البوليميرات العالية الأداء.pdf&#39;
text_output_path = r&quot;C:\Users\DELL\Desktop\output.txt&quot;

convert_pdf_to_text(pdf_path, text_output_path)

因此，当使用这些库时，我总是得到以下带有错误编码的输出，我不知道该用什么来修复它？对此有什么建议吗？
提前致谢
注意：附在上面https://www.noor-book.com/%D9%83%D8%AA%D8%A7%D8%A8-%D8%A7%D9%84%D8%A8%D9%88%D9%84%D9%8A%D9%85%D9%8A%D8%B1%D8%A7%D8%AA-%D8%A7%D9%84%D8%B9%D8%A7%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A3%D8%AF%D8%A7%D8%A1-pdf?next=72c6f38a363b368a7bd978a8449ea530是我尝试阅读的阿拉伯语书]]></description>
      <guid>https://stackoverflow.com/questions/78631415/read-arabic-pdf-book-using-python</guid>
      <pubDate>Mon, 17 Jun 2024 07:46:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 BARTDecoder 和 cached_property 的 Nougat OCR 中的 ImportError 和 TypeError 问题</title>
      <link>https://stackoverflow.com/questions/78594832/importerror-and-typeerror-issues-in-nougat-ocr-with-bartdecoder-and-cached-prope</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78594832/importerror-and-typeerror-issues-in-nougat-ocr-with-bartdecoder-and-cached-prope</guid>
      <pubDate>Sat, 08 Jun 2024 05:43:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中手动对某一层的输出进行反量化，并为下一层重新量化？</title>
      <link>https://stackoverflow.com/questions/78239906/how-to-manually-dequantize-the-output-of-a-layer-and-requantize-it-for-the-next</link>
      <description><![CDATA[我正在做一个学校项目，需要我对模型的每一层进行手动量化。具体来说，我想手动实现：

量化激活，结合量化权重 A - 层 A -
量化输出 - 去量化输出 - 重新量化输出，结合量化权重 B - 层 B - ...

我知道 Pytorch 已经有一个量化函数，但该函数仅限于 int8。我想执行从 bit = 16 到 bit = 2 的量化，然后比较它们的准确性。
我遇到的问题是，量化后，层的输出大了几个量级（bit = 16），我不知道如何将其去量化。我正在使用激活和权重的相同最小值和最大值执行量化。因此，这里有一个例子：
激活 = [1,2,3,4]
权重 = [5,6,7,8]
激活和权重的最小值和最大值 = 1, 8
预期的非量化输出 = 70

使用位量化 = 16
量化激活 = [-32768, -23406, -14044, -4681]
量化权重 = [4681, 14043, 23405, 32767]
量化输出 = -964159613
使用最小值 = 1、最大值 = 8 反量化输出 = -102980

这个计算对我来说很有意义，因为输出涉及激活和权重的乘积，它们的幅度增加也相乘。如果我使用原始的最小值和最大值执行一次反量化，则输出会大得多，这是合理的。
Pytorch 如何处理反量化？我试图找到 Pytorch 的量化，但找不到它。如何对输出进行反量化？]]></description>
      <guid>https://stackoverflow.com/questions/78239906/how-to-manually-dequantize-the-output-of-a-layer-and-requantize-it-for-the-next</guid>
      <pubDate>Thu, 28 Mar 2024 17:17:53 GMT</pubDate>
    </item>
    <item>
      <title>让一个非常简单的 stablebaselines3 示例发挥作用</title>
      <link>https://stackoverflow.com/questions/77766048/getting-a-very-simple-stablebaselines3-example-to-work</link>
      <description><![CDATA[我尝试模拟最简单的硬币翻转游戏，你必须预测它是否会是正面。遗憾的是它无法运行，给我：
使用 cpu 设备
回溯（最近一次调用最后一次）：
文件“/home/user/python/simplegame.py”，第 40 行，在&lt;module&gt;
model.learn(total_timesteps=10000)
文件 &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py&quot;，第 315 行，在 learn 中
return super().learn(
文件 &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py&quot;，第 264 行，在 learn 中
total_timesteps, callback = self._setup_learn(
文件 &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/common/base_class.py&quot;，第 423 行，在 _setup_learn 中
self._last_obs = self.env.reset() # 类型： ignore[assignment]
文件 &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py&quot;，第 77 行，在 reset
obs，self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
TypeError: CoinFlipEnv.reset() 获得了意外的关键字参数 &#39;seed&#39;

代码如下：
import gymnasium as gym
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

class CoinFlipEnv(gym.Env):
def __init__(self, heads_probability=0.8):
super(CoinFlipEnv, self).__init__()
self.action_space = gym.spaces.Discrete(2) # 0 表示正面，1 表示反面
self.observation_space = gym.spaces.Discrete(2) # 0 表示正面，1 表示反面
self.heads_probability = heads_probability
self.flip_result = None

def reset(self):
# 重置环境
self.flip_result = None
return self._get_observation()

def step(self, action):
# 执行操作（0 表示正面，1 表示反面）
self.flip_result = int(np.random.rand() &lt; self.heads_probability)

# 计算奖励（1 表示正确预测，-1 表示错误）
reward = 1 if self.flip_result == action else -1

# 返回观察、奖励、完成和信息
return self._get_observation(), reward, True, {}

def _get_observation(self):
# 返回当前硬币翻转结果
return self.flip_result

# 创建正面概率为 0.8 的环境
env = DummyVecEnv([lambda: CoinFlipEnv(heads_probability=0.8)])

# 创建 PPO 模型
model = PPO(&quot;MlpPolicy&quot;, env, verbose=1)

# 训练模型
model.learn(total_timesteps=10000)

# 保存模型
model.save(&quot;coin_flip_model&quot;)

# 评估模型
obs = env.reset()
for _ in range(10):
action, _states = model.predict(obs)
obs, rewards, dones, info = env.step(action)
print(f&quot;Action: {action}, Observation: {obs}, Reward: {rewards}&quot;)

我做错了什么？
这是 2.2.1 版本。]]></description>
      <guid>https://stackoverflow.com/questions/77766048/getting-a-very-simple-stablebaselines3-example-to-work</guid>
      <pubDate>Fri, 05 Jan 2024 16:47:55 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 中的平衡准确度分数</title>
      <link>https://stackoverflow.com/questions/59339531/balanced-accuracy-score-in-tensorflow</link>
      <description><![CDATA[我正在为一个高度不平衡的分类问题实现一个 CNN，我想在 TensorFlow 中实现自定义指标以使用“选择最佳模型”回调。
具体来说，我想实现平衡准确度分数，即每个类的召回率的平均值（请参阅 sklearn 实现此处），有人知道怎么做吗？]]></description>
      <guid>https://stackoverflow.com/questions/59339531/balanced-accuracy-score-in-tensorflow</guid>
      <pubDate>Sat, 14 Dec 2019 21:59:49 GMT</pubDate>
    </item>
    </channel>
</rss>