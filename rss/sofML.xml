<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 14 Jun 2024 18:20:48 GMT</lastBuildDate>
    <item>
      <title>mxnet、gluonts 和 numpy 出现奇怪的错误</title>
      <link>https://stackoverflow.com/questions/78623912/strange-error-with-mxnet-gluonts-and-numpy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78623912/strange-error-with-mxnet-gluonts-and-numpy</guid>
      <pubDate>Fri, 14 Jun 2024 15:46:13 GMT</pubDate>
    </item>
    <item>
      <title>在训练随机森林之前进行额外的引导是否有用？</title>
      <link>https://stackoverflow.com/questions/78623778/is-additional-bootstrapping-useful-before-training-a-random-forest</link>
      <description><![CDATA[我正在审查使用随机森林 (RF) 回归器的代码库，我注意到在创建每个 RF 模型之前都应用了引导程序。但是，RF 本质上使用引导来训练每个决策树 (DT)。
在创建随机森林之前使用额外的引导是否有意义并且有用？
以下是该过程的说明：

从 75 个训练观察中，创建了 100 个引导批次。
每个引导批次用于训练 RF，RF 本身由
100 个 DT 组成。
总共，我们将训练 1000 个 DT。
最后，将每个 DT 的预测汇总以形成
RF 预测。
最后，将来自所有 RF 的预测汇总以获得最终结果。

]]></description>
      <guid>https://stackoverflow.com/questions/78623778/is-additional-bootstrapping-useful-before-training-a-random-forest</guid>
      <pubDate>Fri, 14 Jun 2024 15:15:11 GMT</pubDate>
    </item>
    <item>
      <title>RobertaForSequenceClassification 的一些权重未初始化[重复]</title>
      <link>https://stackoverflow.com/questions/78623284/some-weights-of-robertaforsequenceclassification-were-not-initialized</link>
      <description><![CDATA[我训练了一个基于 Bert 的模型并想加载它。因此我想出了一个python脚本-
import torch
from transformers import RobertaForSequenceClassification, AutoTokenizer, AutoModel
RobertaForSequenceClassification
def read_code_file(file_path):
with open(file_path, &#39;r&#39;) as file:
code = file.read()
return code

code_file_path = &#39;/home/hprakash/test/code.c&#39;

input_text = read_code_file(code_file_path)

tokenizer = AutoTokenizer.from_pretrained(&quot;/home/hprakash/tokenizer&quot;, local_files_only=True)

model = RobertaForSequenceClassification.from_pretrained(&quot;/home/hprakash/Ghidra-O2-Function&quot;, local_files_only=True)

input_ids = tokenizer.encode(input_text, return_tensors=&#39;pt&#39;)

使用 torch.no_grad():
output = model(input_ids)

print(output)

我尝试运行此脚本后遇到了此错误 -
初始化 RobertaForSequenceClassification 时未使用 /home/hprakash/Ghidra-O2-Function 处的模型检查点的一些权重：[&#39;lm_head.dense.weight&#39;, &#39;lm_head2.decoder.weight&#39;, &#39;lm_head3.decoder.bias&#39;, &#39;lm_head3.dense.weight&#39;, &#39;lm_head2.dense.weight&#39;, &#39;lm_head3.decoder.weight&#39;, &#39;lm_head2.layer_norm.bias&#39;, &#39;lm_head2.bias&#39;, &#39;lm_head2.dense.bias&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.bi as&#39;, &#39;lm_head3.layer_norm.weight&#39;, &#39;lm_head3.dense.bias&#39;, &#39;lm_head3.bias&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head2.decoder.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head3.layer_norm.bias&#39;, &#39;lm_head2.layer_norm.weight&#39;]

如果您从针对另一项任务或使用另一种架构训练的模型的检查点初始化 RobertaForSequenceClassification（例如从 BertForPreTraining 模型初始化 BertForSequenceClassification 模型），则会出现这种情况。

如果您从您期望完全相同的模型的检查点初始化 RobertaForSequenceClassification（从 BertForSequenceClassification 模型初始化 BertForSequenceClassification 模型），则不会出现这种情况。

RobertaForSequenceClassification 的一些权重未从 /home/hprakash/Ghidra-O2-Function 的模型检查点初始化，并且是新初始化的：[&#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.dense.weight &#39;, &#39;classifier.out_proj.bias&#39;] 

您可能应该在下游任务上训练此模型，以便能够将其用于预测和推理。

SequenceClassifierOutput(loss=None, logits=tensor(\[\[0.2434, 0.1226\]\]), hidden_​​states=None,tentions=None)

我已经对模型进行了微调。错误日志仍然显示我需要进一步微调。
我尝试调试我的脚本，但它似乎不起作用，而且我不断收到相同的错误。]]></description>
      <guid>https://stackoverflow.com/questions/78623284/some-weights-of-robertaforsequenceclassification-were-not-initialized</guid>
      <pubDate>Fri, 14 Jun 2024 13:35:39 GMT</pubDate>
    </item>
    <item>
      <title>是否有针对不平衡数据集的 ML 算法？</title>
      <link>https://stackoverflow.com/questions/78623027/is-there-an-ml-algorithm-for-imbalanced-dataset</link>
      <description><![CDATA[我有 3 个标签，分别是正面、负面和中性。此 ML 模型的数据是流式/批处理的。假设每个批次包含 100 个样本（batch_size=100）。我可以清楚地看到这是一个在线/增量学习问题。我的批次也有可能获得不平衡/倾斜的数据样本。例如，B1-B4 可能包含所有正面数据，B5-B10 可能包含所有负面数据，B11-B15 可能包含所有中性数据样本...
在了解用例并进行一些基础研究后，我想到在 partial_fit () 上使用 SGDClassifier 可以很好地解决我的问题。
我面临的实际困难是，在完成 15 个批次的训练（每个标签 5 个批次）后，我的模型将所有推理数据预测为最新标签。在找到根本原因后，我发现模型在最近几批训练中对最近的批次（中性样本）进行了训练，因此一切都被预测为中性。
鉴于这个困难，我还有另一个困难。我无法执行数据平衡技术，如 SMOTE、欠采样、过采样等，因为我没有接触过完整的数据集。在任何给定的时间实例中，我只能访问当前批次数据（100 个样本）和在先前批次上训练的模型（partial_fit () 模型）。
start_timer = time.time()
column_classes_lists = [np.array(sgd_model[PIPELINE_MODEL_CONSTANTS.META][PIPELINE_MODEL_CONSTANTS.CLASSES].get(key)) for key in sgd_model[PIPELINE_MODEL_CONSTANTS.META][PIPELINE_MODEL_CONSTANTS.CLASSES].keys()]

if data is not None:
kf = KFold(n_splits = PREDICTOR_CONSTANTS.NUM_FOLDS)
y_predictions, y_actual = [], []
# target_df = pd.DataFrame(target)

for train_index, val_index in kf.split(data):
X_train_fold, X_val_fold = [data[idx] for idx in train_index], [data[idx] for idx in val_index]
y_train_fold = [[target[col][idx][0] for col in target.keys()] for idx in train_index]
y_val_fold = [[target[col][idx][0] for col in target.keys()] for idx in val_index]

sgd_model[PIPELINE_MODEL_CONSTANTS.MODEL].partial_fit(np.array(X_train_fold), y_train_fold, classes = column_classes_lists)

y_predictions.extend(sgd_model[PIPELINE_MODEL_CONSTANTS.MODEL].predict(X_val_fold))

y_actual.extend(y_val_fold)

Utils.calculate_prediction_metrics(sgd_model, target, y_actual, y_predictions, column_classes_lists, meta)
else:
Utils.calculate_prediction_metrics(sgd_model, target, None, None, column_classes_lists, meta)

logger.warning(f&quot;SGDModel 在 {time.time() - start_timer} 秒内成功训练了 id: {id}，得分指标为：{sgd_model[PIPELINE_MODEL_CONSTANTS.META][PIPELINE_MODEL_CONSTANTS.SCORES]}&quot;)
return sgd_model


除此之外，在我的训练过程中的任何时间点都可能出现新的标签。对于 partial_fit()，我们需要在第一次训练调用中传递所有可能的标签。因此，我猜 sklearn 的 partial_fit() 可能不太适合我的情况...
你能帮我找到一个用 Python 实现的针对这种情况的最佳解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78623027/is-there-an-ml-algorithm-for-imbalanced-dataset</guid>
      <pubDate>Fri, 14 Jun 2024 12:40:42 GMT</pubDate>
    </item>
    <item>
      <title>直接在 gpu 上加载并生成 Qwen2</title>
      <link>https://stackoverflow.com/questions/78622820/direct-loading-and-generation-qwen2-on-gpu</link>
      <description><![CDATA[我想在挖矿机上部署 LLM 模型 Qwen2-7b-instruct，但由于内存存储量低（4GB RAM）和处理能力有限（2 核奔腾）等限制，我无法做到这一点。
另一方面，我的电脑不支持 CUDA 和 ROCM 技术。
矿机规格：
4x amd rx580 8GB]]></description>
      <guid>https://stackoverflow.com/questions/78622820/direct-loading-and-generation-qwen2-on-gpu</guid>
      <pubDate>Fri, 14 Jun 2024 11:58:01 GMT</pubDate>
    </item>
    <item>
      <title>在机器学习中，什么时候对数据进行欠采样/过采样？</title>
      <link>https://stackoverflow.com/questions/78622628/at-what-point-do-you-undersample-oversample-data-in-machine-learning</link>
      <description><![CDATA[我有一个问题，关于在机器学习过程中何时应该对数据集进行欠采样或过采样。我正在处理一个包含 NaN 值的不平衡数据集（约 5% 的阳性情况）。接下来的步骤应该如何衔接？这有一般规则吗？我应该填写缺失值、重新采样数据并继续删除异常值吗？]]></description>
      <guid>https://stackoverflow.com/questions/78622628/at-what-point-do-you-undersample-oversample-data-in-machine-learning</guid>
      <pubDate>Fri, 14 Jun 2024 11:13:18 GMT</pubDate>
    </item>
    <item>
      <title>Vitas AI - AttributeError：在模型量化期间无法设置属性</title>
      <link>https://stackoverflow.com/questions/78621742/vitas-ai-attributeerror-cant-set-attribute-during-quantization-of-model</link>
      <description><![CDATA[我有一个 Python 版的 LSTM 模型，我正在尝试使用 Vitis AI (Pytorch) 将其部署到 ZCU104 板上。我在量化模型时遇到错误。我收到以下行的错误：
quantizer.export_xmodel(output_dir=&quot;quantize_result&quot;, deploy_check=True)

错误是：
[VAIQ_NOTE]: =&gt;Converting to xmodel ...

回溯（最近一次调用最后一次）：

文件“lstm_quant.py”，第 118 行，位于 &lt;module&gt;

main(args)

文件“lstm_quant.py”，第 107 行，在 main 中

quantizer.export_xmodel(output_dir=“quantize_result”，deploy_check=True)

文件“/opt/vitis_​​ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/apis.py”，第 148 行，在 export_xmodel 中

self.processor.export_xmodel(output_dir, deploy_check, dynamic_batch)

文件“/opt/vitis_​​ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/qproc/base.py”，第 368 行，在 export_xmodel 中

dump_xmodel(output_dir, deploy_check, self._lstm_app)

文件 &quot;/opt/vitis_​​ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/qproc/base.py&quot;，第 505 行，位于 dump_xmodel

deploy_graphs, _ = get_deploy_graph_list(quantizer.quant_model, quantizer.Nndctgraph)

文件 &quot;/opt/vitis_​​ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/qproc/utils.py&quot;，第 463 行，位于 get_deploy_graph_list

return _deploy_optimize(quant_model, nndct_graph, need_pa​​rtition)

文件&quot;/opt/vitis_​​ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/pytorch_nndct/qproc/utils.py&quot;，第 419 行，在 _deploy_optimize

g_optmizer = DevGraphOptimizer(nndct_graph)

文件 &quot;/opt/vitis_​​ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/nndct_shared/compile/deploy_optimizer.py&quot;，第 92 行，在 __init__

self._dev_graph.clone_from(nndct_graph)

文件 &quot;/opt/vitis_​​ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/nndct_shared/nndct_graph/base_graph.py&quot;，第133，在 clone_from 中

self._top_block.clone_from(src_graph.block, local_map, converted_nodes)

文件 &quot;/opt/vitis_​​ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/nndct_shared/nndct_graph/base_block.py&quot;，第 47 行，在 clone_from 中

self.append_node(self.owning_graph.create_node_from(node, local_map, converted_nodes))

文件 &quot;/opt/vitis_​​ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/nndct_shared/nndct_graph/base_graph.py&quot;，第 161 行，在 create_node_from 中

node.clone_from(src_node, local_map)

文件&quot;/opt/vitis_​​ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/nndct_shared/nndct_graph/base_node.py&quot;，第 120 行，在 clone_from 中

self.op.clone_from(src_node.op, local_map)

文件 &quot;/opt/vitis_​​ai/conda/envs/vitis-ai-pytorch/lib/python3.8/site-packages/nndct_shared/nndct_graph/base_operator.py&quot;，第 214 行，在 clone_from 中

setattr(self, config, new_value)

AttributeError：无法设置属性

在以下 Google 链接中附加完整错误、模型代码和量化脚本：
量化脚本：
https://docs.google.com/document/d/1jRYmPH2z70ovpc_FJIBpUQaTHUPRrTgnPVxlQJ1JLug/edit?usp=sharing
模型脚本：
https://docs.google.com/document/d/1OBZw4XhdHpVhA0gKcJn2NzR_WA7ZczQ3hL42f_sMKug/edit?usp=sharing
完整错误：
https://docs.google.com/document/d/1kI1WJqq9pp3aSsGLpNGjf22mzK6swIiZbIXwfUeTGto/edit?usp=sharing
尝试更改 base_operator.py，但没有成功。也尝试简化模型，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78621742/vitas-ai-attributeerror-cant-set-attribute-during-quantization-of-model</guid>
      <pubDate>Fri, 14 Jun 2024 08:00:33 GMT</pubDate>
    </item>
    <item>
      <title>未找到 keras.utils.PyDataset</title>
      <link>https://stackoverflow.com/questions/78621236/keras-utils-pydataset-not-found</link>
      <description><![CDATA[class BatchedDataset(tf.keras.utils.PyDataSet):
使用 keras PyDataset 时，我不断收到此错误。我尝试更新 tensorflow 并使用 tf.keras.api._v2。这些都不起作用
我正在使用 google colab
]]></description>
      <guid>https://stackoverflow.com/questions/78621236/keras-utils-pydataset-not-found</guid>
      <pubDate>Fri, 14 Jun 2024 05:43:38 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯优化中的探索与利用权衡</title>
      <link>https://stackoverflow.com/questions/78620985/exploration-and-exploitation-tradeoff-in-bayesian-optimization</link>
      <description><![CDATA[我一直在研究贝叶斯优化，但有些事情我不太明白。我知道 BO 使用获取函数来平衡探索和利用。我们可以添加一个参数（epsilon）来调整我们想要更多的探索还是利用。但参数是如何做到的？就像 PI 和 EI 获取函数一样，为什么大的 epsilon 可以使算法更具探索性，反之亦然？]]></description>
      <guid>https://stackoverflow.com/questions/78620985/exploration-and-exploitation-tradeoff-in-bayesian-optimization</guid>
      <pubDate>Fri, 14 Jun 2024 04:07:57 GMT</pubDate>
    </item>
    <item>
      <title>pytorch PascalVOC 数据集中的数据加载器错误：RuntimeError：批次列表中的每个元素应大小相同</title>
      <link>https://stackoverflow.com/questions/78620402/dataloader-error-in-pytorch-pascalvoc-dataset-runtimeerror-each-element-in-lis</link>
      <description><![CDATA[当尝试实现 yolo 算法时，其中一个步骤是将每个图像的大小调整为 (448, 448)。但即使应用了转换，Dataloader 也会抛出有关数据集大小差异的异常。
确切的错误消息：“RuntimeError：批次列表中的每个元素应大小相同”
from torchvision import datasets
from torchvision.transforms import v2, ToTensor

from torch.utils.data import DataLoader

validation_data = datasets.voc.VOCDetection(
root=&#39;.DATA/&#39;,
download=False,
image_set=&quot;val&quot;,
transform=v2.Compose([ v2.Resize(size=(448, 448)), ToTensor() ])
)

batch_size = 64

validation_dataloader = DataLoader(validation_data, batch_size=batch_size)

for X, y in validation_dataloader:
print(f&quot;Shape X [N, C, H, W] 的：{X.shape}&quot;)
break
]]></description>
      <guid>https://stackoverflow.com/questions/78620402/dataloader-error-in-pytorch-pascalvoc-dataset-runtimeerror-each-element-in-lis</guid>
      <pubDate>Thu, 13 Jun 2024 22:43:49 GMT</pubDate>
    </item>
    <item>
      <title>在不同的 Python 环境中训练的相同 XGBClassifier 模型得出的预测结果明显不同</title>
      <link>https://stackoverflow.com/questions/78619966/the-same-xgbclassifier-model-trained-in-different-python-environment-made-notice</link>
      <description><![CDATA[我尝试将在旧的 Python 环境中训练的 XGBClassifier 模型转移到新的环境中。
以下是新旧环境中关键软件包的版本信息。
旧环境

python=3.6.0
scikit-learn==0.22.2.post1
xgboost==0.90
pickleshare==0.7.5
numpy==1.18.1

新环境

python=3.11.9
scikit-learn==1.4.2
xgboost==2.0.3
pickleshare==0.7.5
numpy==1.26.4

在新旧环境中分别使用同一组超参数和相同的数据，预测的概率明显不同。
我还注意到，拟合管道对象的大小以及训练模型所需的时间发生了显着变化。
拟合管道对象的大小旧 vs. 新： 30 MB vs. 7 MB
训练时间旧 vs. 新： 4:38:46 vs. 0:06:40
对于我在旧环境中训练的模型和新环境中训练的模型之间的差异，您有什么看法？
以下是我用来训练模型的关键 Python 代码。
def create_pipeline(model_params, cat_indices):
&quot;&quot;&quot;
创建管道
:param model_params：管道中 XGBoost 分类器的模型参数
:param cat_indices：X 中分类特征的索引
&quot;&quot;&quot;

cat_transformer = Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;missing&#39;)),
(&#39;one_hot_encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))])

preprocessor = ColumnTransformer(
transformers=[(&#39;cat&#39;, cat_transformer, cat_indices)],
remainder=&#39;passthrough&#39;)

xgb = XGBClassifier(objective=&quot;binary:logistic&quot;, eval_metric=&quot;auc&quot;, missing=np.nan, use_label_encoder=False)
xgb.set_params(**model_params)

full_pipeline_model = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor),
(&#39;model&#39;, xgb)])
return full_pipeline_model

model_params = {
&#39;n_estimators&#39;: 500,
&#39;alpha&#39;: 9.73974803929248e-06,
&#39;gamma&#39;: 19,
&#39;lambda&#39;: 0.557185777864069,
&#39;learning_rate&#39;: 0.029438952461179668,
&#39;max_depth&#39;: 13,
&#39;scale_pos_weight&#39;: 5,
&#39;subsample&#39;: 0.687206238714661
}

cat_indices = [X.columns.get_loc(col) for col in cat_cols]

fitted_pipeline = create_pipeline(model_params, cat_indices).fit(X.values, y.values)
pickle.dump(fitted_pipeline, open(&quot;fitted_pipeline_final1.pkl&quot;, &quot;wb&quot;))

我预计从两个模型获得的预测概率非常相似，因为我使用了相同的超参数集和相同的数据。预测概率明显不同的原因可能是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78619966/the-same-xgbclassifier-model-trained-in-different-python-environment-made-notice</guid>
      <pubDate>Thu, 13 Jun 2024 20:07:18 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 detector2 区分灰度图像中的两种颜色并掩盖它们？</title>
      <link>https://stackoverflow.com/questions/78619402/how-would-you-use-detectron2-to-distinguish-between-two-colors-in-a-grayscale-im</link>
      <description><![CDATA[我的项目包括使用detectron2模型来区分灰度图像中的物体。该图像由形状奇怪的灰色细胞组成，而其余空间为黑色。我的任务是使用该模型并描绘出灰色细胞的形状。
示例图像：
细胞的灰度图像
我曾尝试使用预先存在的模型来解决这个问题，但它们无法识别物体的存在。解决这个问题的最佳方法是什么？
此外，我愿意使用不同的机器学习模型。我只是想找到一种区分灰色和黑色的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78619402/how-would-you-use-detectron2-to-distinguish-between-two-colors-in-a-grayscale-im</guid>
      <pubDate>Thu, 13 Jun 2024 17:36:00 GMT</pubDate>
    </item>
    <item>
      <title>如何使用多个 AMD GPU 运行 Ollama [关闭]</title>
      <link>https://stackoverflow.com/questions/78618964/how-to-run-ollama-with-multiple-amd-gpus</link>
      <description><![CDATA[我尝试在配备多个 AMD GPU 的系统上运行 Ollama，但在正确使用所有 GPU 时遇到了困难。
设置详细信息：

操作系统：RedHat
GPU：MI210 x 4
Ollama 版本：0.1.42
ROCm

面临的问题：

似乎只使用了一个 GPU，或者没有明显的性能改进。
[watch -n 0.1 /opt/rocm/bin/rocm-smi]
0 2 0x740f，30145 63.0°C 253.0W N/A，N/A，0 1700Mhz 1600Mhz 0% 自动 300.0W 60% 100%
1 3 0x740f，41677 28.0°C 41.0W N/A，N/A，0 800Mhz 1600Mhz 0% 自动 300.0W 0% 0%
2 4 0x740f，39309 31.0°C 40.0W N/A，N/A，0 800Mhz 1600Mhz 0% 自动 300.0W 0% 0%
3 5 0x740f, 50825 35.0°C 40.0W N/A, N/A, 0 800Mhz 1600Mhz 0% 自动 300.0W 0% 0%

问题：

我是否缺少在 Ollama 中启用多 GPU 支持的特定步骤？
在 Ollama 中，多 GPU 设置是否需要任何其他配置设置？

是否有任何指导或详细步骤可以正确设置和验证 Ollama 是否使用多个 AMD GPU？
我尝试过 /set 参数 num_gpu 12，但没有成功]]></description>
      <guid>https://stackoverflow.com/questions/78618964/how-to-run-ollama-with-multiple-amd-gpus</guid>
      <pubDate>Thu, 13 Jun 2024 15:56:57 GMT</pubDate>
    </item>
    <item>
      <title>尝试将 Kaggle 笔记本提交到 GitHub 存储库时出现错误</title>
      <link>https://stackoverflow.com/questions/78618684/getting-error-while-trying-to-commit-a-kaggle-notebook-to-a-github-repository</link>
      <description><![CDATA[提交内核时发生错误：ConcurrencyViolation 序列号必须匹配草稿记录：KernelId=59714315，ExpectedSequence=43，
ActualSequence=42，AuthorUserId=16388128

这是什么意思？
当我尝试将笔记本从 kaggle 提交到 github 时出现此错误。我该如何解决这个问题？
我原本以为它会直接提交到 github 而不会遇到任何问题]]></description>
      <guid>https://stackoverflow.com/questions/78618684/getting-error-while-trying-to-commit-a-kaggle-notebook-to-a-github-repository</guid>
      <pubDate>Thu, 13 Jun 2024 15:06:29 GMT</pubDate>
    </item>
    <item>
      <title>实现神经网络的岭回归方程</title>
      <link>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</link>
      <description><![CDATA[我试图在 MATLAB 中复制以下方程，以使用岭回归训练找到神经网络的最佳输出权重矩阵。
使用岭回归训练后的神经网络输出权重矩阵：

此方程来自 Mantas Lukosevicius 提供的回声状态网络指南，可在以下位置找到：https://www.researchgate.net/publication/319770153_A_practical_guide_to_applying_echo_state_networks（见第 11 页）
我的尝试如下。我认为外括号（红色）使其成为非传统的双重求和，这意味着 Voss 提出的方法（见 https://www.mathworks.com/matlabcentral/answers/1694960-nested-loops-for-double-summation）无法遵循。请注意，y_i 是一个 T x 1 向量，而 y_i_target 也是一个 T x 1 向量。Wout_i 是一个 N x 1 向量，其中 N 是神经网络中的节点数。我为每个 i^th 目标训练信号生成三个 Ny x 1 向量 Wout_i,y_i,y_i_target，其中 Ny 是训练信号的数量。Wout 的最终输出是一个 N x 1 向量，其中向量中的每个元素都是网络中每个节点的最佳权重。
N = 100; % 神经网络节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度 
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
outer_sum = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); 
Wouts{i} = Wout_i; % 针对每个第 i 个目标训练信号收集的每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号 
inner_sum = sum(((y_i&#39;-y_i_target).^2)+reg*norm(Wout_i)^2);
outer_sum(i) = inner_sum;
end
outer_sum = outer_sum.*(1/Ny);
[minval, minidx] = min(outer_sum);
Wout = cell2mat(Wouts(minidx));

我对 Wout 的最终答案是 N 乘以 1，正如它应该的那样，但我对我的答案不确定。我特别不确定我是否正确地完成了关于 Wout 操作的双重求和和 arg min。有什么方法可以验证我的答案吗？
解决方案：
我尝试了另一种方法/尝试，如下所示：
N = 100; % 神经网络中的节点数
Ny = 200; % 训练信号数
T = 50; % 每个训练信号的时间长度
X = rand(N,T); % 神经网络状态矩阵
reg = 10^-4; % 岭回归系数
MSE = zeros(Ny,1);
for i = 1:Ny
y_i_target = rand(T,1); % 训练信号
Wout_i = ((X*X&#39; + reg*eye(N)) \ (X*y_i_target)); % Luko 等人的 Eq. 9。
Wouts{i} = Wout_i; % 为每个第 i 个目标训练信号收集每个 Wout_i 的单元矩阵
y_i = Wout_i&#39;*X; % 预测信号
MSE(i) = (1/T)*sum((y_i&#39;-y_i_target).^2); % 均方误差
end
[minval, minidx] = min(MSE);
Wout = cell2mat(Wouts(minidx));

我相信这次尝试比第一次更好，但我不确定它是否仍然正确。
正如 BillBokeey 所强调的那样，所需的方程只是 Luko 等人提出的方程 9 的迭代版本。要进行训练，必须将方程 9 应用于训练数据集中的每个目标信号，并选择最小化均方误差 (MSE) 的结果 W_out。]]></description>
      <guid>https://stackoverflow.com/questions/78597100/implement-ridge-regression-equation-for-a-neural-network</guid>
      <pubDate>Sat, 08 Jun 2024 22:31:47 GMT</pubDate>
    </item>
    </channel>
</rss>