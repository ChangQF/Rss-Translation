<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 11 Apr 2024 06:18:54 GMT</lastBuildDate>
    <item>
      <title>SQL 查询数据集包含查询和执行时间/内存</title>
      <link>https://stackoverflow.com/questions/78308154/sql-queries-dataset-containing-queries-and-execution-time-memory</link>
      <description><![CDATA[我正在致力于创建 ML 模型来预测 SQL 查询的执行速度或所需的内存量。是否有任何数据集可供我使用，同时具有 SQL 查询和执行时间/内存作为特征？]]></description>
      <guid>https://stackoverflow.com/questions/78308154/sql-queries-dataset-containing-queries-and-execution-time-memory</guid>
      <pubDate>Thu, 11 Apr 2024 04:08:54 GMT</pubDate>
    </item>
    <item>
      <title>如何解决使用 cnn 和 lstm 进行机器学习时出现的错误？</title>
      <link>https://stackoverflow.com/questions/78308090/how-can-i-resolve-an-error-that-appeared-in-machine-learning-using-cnn-and-lstm</link>
      <description><![CDATA[我想创建一个回归模型，其中包含 6 个时间序列图像作为解释变量和 3 个输出层。目前，有 125 对解释变量（6 个时间序列图像）和目标变量（3 个数值）。具有时间序列关系的 6 个图像被分组在一个文件夹中。我用以下代码创建了模型，执行环境是Google colab。
导入操作系统
将 pandas 导入为 pd
将 numpy 导入为 np
导入CV2
从 sklearn.model_selection 导入 train_test_split
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入ConvLSTM2D，BatchNormalization，密集
从tensorflow.keras.optimizers导入Adam
将 matplotlib.pyplot 导入为 plt

将张量流导入为 tf


# 三种类型的数值数据
df = pd.read_excel(r“/path/to/file/hoge.xlsx”)

# 图像数据加载和预处理
image_folder = r“/路径/到/文件夹”
图片 = []
标签=[]

img_set_cnt = 125
min_img_cnt = 6

对于范围内的 i(1, img_set_cnt + 1)：
    第一=[]
    对于范围内的 j（1，min_img_cnt + 1）：
        img_path = os.path.join(image_folder, str(i), f&quot;no{i}_BW_{j}.jpg&quot;)
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        img = np.array(img) / 255.0 # 标准化（例如在浮点表示系统中）
        lst.追加（img）
    图像.append(lst)
    labels.append(df.iloc[i-1][[“column1”, “column2”, “column3”]].values)

标签 = np.array(标签)

# 将数据分为训练和测试
train_x, val_x, train_y, val_y = train_test_split(图像, 标签, test_size=0.3, random_state=71, shuffle=True)

# 仅第一层需要输入规范
模型=顺序（[
    ConvLSTM2D(filters=16, kernel_size=(18,18), return_sequences=True, data_format=“channels_last”, input_shape=(min_img_cnt, 256, 256, 1)),
    批量归一化（动量=0.8），
    ConvLSTM2D(filters=8, kernel_size=(18,18), return_sequences=True, data_format=“channels_last”),
    批量归一化（动量=0.8），
    ConvLSTM2D(filters=3，kernel_size=(6,6)，return_sequences=False，data_format=“channels_last”，activation=&#39;relu&#39;),
    Dense(units=3,activation=&#39;linear&#39;) # 假设回归的恒等函数
]）


print(len(images)) # 图像集的数量
print(len(images[0])) # 第一个图像集中的图像数量
print(images[0][0].shape) # 第一个图像集中第一个图像的形状


# 模型编译
model.compile(loss=&#39;mean_squared_error&#39;, 优化器=Adam(), 指标=[&#39;mae&#39;])

# 模型学习
历史=模型.fit(
    np.array(train_x), np.array(train_y),
    批量大小=16，
    纪元=10，
    validation_data=(np.array(val_x), np.array(val_y)),
    随机播放=真）


############执行结果############
125
6
(256, 256)
纪元 1/10
&lt;小时/&gt;
InvalidArgumentError Traceback（最近一次调用最后一次）
 在&lt;细胞系：5&gt;()
历史 = model.fit(
    np.array(train_x), np.array(train_y),
    批量大小=16，

1 帧
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py 中的 fast_execute(op_name, num_outputs, inputs, attrs, ctx, name)
&lt;前&gt;&lt;代码&gt;尝试：
    ctx.ensure_initialized()
    张量 = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
                                      输入、属性、num_outputs）

除了 core._NotOkStatusException 作为 e:
InvalidArgumentError：图形执行错误
############################################
我不知道为什么在做机器学习的时候Graph这个词是错误的。请告诉我该怎么做。]]></description>
      <guid>https://stackoverflow.com/questions/78308090/how-can-i-resolve-an-error-that-appeared-in-machine-learning-using-cnn-and-lstm</guid>
      <pubDate>Thu, 11 Apr 2024 03:40:58 GMT</pubDate>
    </item>
    <item>
      <title>使用 2 层神经网络进行 sin 近似</title>
      <link>https://stackoverflow.com/questions/78307766/sin-approximation-with-a-2-layer-neural-network</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78307766/sin-approximation-with-a-2-layer-neural-network</guid>
      <pubDate>Thu, 11 Apr 2024 00:58:29 GMT</pubDate>
    </item>
    <item>
      <title>尝试寻找相关数据模式</title>
      <link>https://stackoverflow.com/questions/78307684/attempting-to-find-relative-data-patterns</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78307684/attempting-to-find-relative-data-patterns</guid>
      <pubDate>Thu, 11 Apr 2024 00:21:56 GMT</pubDate>
    </item>
    <item>
      <title>将手写数学方程解析为字符串/数值</title>
      <link>https://stackoverflow.com/questions/78307038/parse-handwritten-math-equation-to-string-numerical-value</link>
      <description><![CDATA[我有这个图像，我需要在Python中解决。它非常像手写数字，我需要将图像解析为字符串，甚至更好，对其求值并获取该表达式或与此类似的表达式的结果。
有什么方法可以至少半可靠地做到这一点吗？阅读它需要使用大量手写字符进行机器学习，但我无法理解它是如何工作的。我也希望它能够相当快地解析图像，但我不介意用数据训练我的模型，无论它在实际识别数字之前需要训练多长时间。
感谢任何帮助，提前致谢！
我尝试过超正方体，但它只能从该图像中检测到 4 和 5，因为我认为这些图像是不整洁和草率的，即使在将其转换为黑白并增强对比度等之后也是如此。]]></description>
      <guid>https://stackoverflow.com/questions/78307038/parse-handwritten-math-equation-to-string-numerical-value</guid>
      <pubDate>Wed, 10 Apr 2024 20:44:13 GMT</pubDate>
    </item>
    <item>
      <title>识别随机森林中错误分类的样本</title>
      <link>https://stackoverflow.com/questions/78306767/identifying-misclassified-samples-in-randomforest</link>
      <description><![CDATA[我正在 RStudio 中执行随机森林分析，我可以使用下面的代码提取混淆矩阵。我可以看到有多少样本被错误分类，但是我可以使用什么代码来识别哪些特定样本被错误分类？
库（随机森林）
库（rfPermute）

rfmetrics &lt;- randomForest(x, y, ntree=ntree,重要性=T)
打印（rfmetrics）

称呼：
 randomForest(x = x, y = y, ntree = ntree, 重要性 = T)
               随机森林类型：分类
                     树木数量：1999
每次分割尝试的变量数量：25

        OOB 估计错误率：56.88%
混淆矩阵：
  1 2 3 4 类.错误
1 8 7 7 4 0.6923077
2 4 19 2 4 0.3448276
3 3 3 15 6 0.4444444
4 3 9 10 5 0.8148148
]]></description>
      <guid>https://stackoverflow.com/questions/78306767/identifying-misclassified-samples-in-randomforest</guid>
      <pubDate>Wed, 10 Apr 2024 19:44:24 GMT</pubDate>
    </item>
    <item>
      <title>生成每个 ID 具有多个记录的合成数据</title>
      <link>https://stackoverflow.com/questions/78306740/generating-synthetic-data-with-multiple-records-per-id</link>
      <description><![CDATA[我想生成一个合成数据集，其中每个 ID 有多个记录，并且每个 ID 的记录之间保持自我一致性。
例如，想象一个数据集，其中 ID 是一家杂货店，每条记录是该商店在给定日期内各种商品的销售额。换句话说，多个时间序列。您可以想象，虽然所有商店都有全球趋势，但也存在商店级别的趋势，而现实的数据集不仅必须保留全球趋势，还必须保留商店级别的趋势。也许 A 店在周末出售的薯条较多，而 B 店周末出售的薯条较少，但饼干较多。
对虚假记录 IID 进行采样的合成数据模型只能保留全球销售趋势（“周五薯条的销量通常比饼干多”）。复杂性又提高了一步的模型，例如 LSTM，可以保留在所有商店中表达的时间趋势（“如果饼干在时间 t-1 的销量少于薯片，那么它们在时间 t 的销量通常会更高”）。但是，我如何制作一个模型，同时允许这些趋势因商店而异（“在某些商店，薯片在周六的销量通常超过饼干，但在其他商店却恰恰相反”）？
我的一个想法是使用领域知识/集群将商店分配给 N 个“配置文件”之一，然后使用该配置文件作为一项功能。该模型自然能够捕获依赖性。也就是说，这种方法需要手动定义 N 并创建一组有限的配置文件。我正在寻找限制较少的东西，并且可以检测到比我手动检测到的更微妙的配置文件（如果存在）。
希望我已经很好地解释了我的问题 - 请随时要求澄清。相关论文的链接将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78306740/generating-synthetic-data-with-multiple-records-per-id</guid>
      <pubDate>Wed, 10 Apr 2024 19:39:08 GMT</pubDate>
    </item>
    <item>
      <title>克服时间序列分类中的重叠数据点：寻找有效的模型</title>
      <link>https://stackoverflow.com/questions/78306506/overcoming-overlapping-data-points-in-time-series-classification-searching-for</link>
      <description><![CDATA[我目前正在研究时间序列分类问题，并遇到了挑战。在我的时间序列数据中，某些类之间的数据点相同且重叠，使得分类任务变得困难。我尝试过使用各种分类器，例如 LSTM、CNN、SVM 和 FNN，但它们的准确率都很差，约为 60-64%。不幸的是，我无法添加新功能，因为我已经使用了所有可用的功能进行分类。此外，我无法包含更多训练数据，因为它只会添加更多相似的重叠数据点。有没有任何模型/分类器可以帮助解决这个问题？
分类准确率高。]]></description>
      <guid>https://stackoverflow.com/questions/78306506/overcoming-overlapping-data-points-in-time-series-classification-searching-for</guid>
      <pubDate>Wed, 10 Apr 2024 18:43:03 GMT</pubDate>
    </item>
    <item>
      <title>需要为我的应用程序在日志中记录机器学习模型及其版本的输入</title>
      <link>https://stackoverflow.com/questions/78306322/need-inputs-on-logging-machine-learning-models-and-their-versions-in-logs-for-my</link>
      <description><![CDATA[所以我有一个网络应用程序，它根据用户输入的主题和问题推荐电影，它还使用 NLP 和 ML 模型，如命名实体识别 (NER) 模型来提取关键字和 BERT 模型。我目前只是将登录数据记录到 sql 中的数据库表之一中。现在我还想将 ML 模型及其版本记录到表中。
我脑子里有几个捕获点，即什么时候捕获这些数据。第一个是用户登录时，第二个是用户将电影添加到列表时。我考虑实现这一点，以便提高 ML 模型的准确性并跟踪性能。如果提出任何不相关的建议，它也将帮助我发现任何类型的差异。
计划只是记录提出建议时触发的模型及其版本。基本上我还会显示与推荐电影匹配的关键字，因此也可能显示这些关键字来自哪些型号以及版本
我只是对如何实现这个感到困惑，并且希望如果有人以前做过类似的事情，我希望得到一些意见。会有帮助的。]]></description>
      <guid>https://stackoverflow.com/questions/78306322/need-inputs-on-logging-machine-learning-models-and-their-versions-in-logs-for-my</guid>
      <pubDate>Wed, 10 Apr 2024 18:05:25 GMT</pubDate>
    </item>
    <item>
      <title>尽管已安装，脚本仍不断请求安装face_recognition_models</title>
      <link>https://stackoverflow.com/questions/78300706/script-continuously-requests-face-recognition-models-installation-despite-being</link>
      <description><![CDATA[我在使用 Face_recognition 库的 Python 脚本中遇到问题。尽管已经成功安装了face_recognition_models包，但当我尝试执行脚本时，脚本反复提示我安装它。这是我收到的命令和输出：

代码：
导入操作系统
导入人脸识别
导入人脸识别模型
导入CV2


image_path = “tes.png”;

# 检查图片文件是否存在
如果不是 os.path.isfile(image_path):
    print(&quot;图像文件不存在:&quot;, image_path)
    出口（）

# 获取网络摄像头 #0 的引用（默认摄像头）
video_capture = cv2.VideoCapture(0)

# 加载您的图像并学习如何识别它。
图像=face_recognition.load_image_file(image_path)
face_encoding =face_recognition.face_encodings(图像)[0]

# 创建已知面部编码及其名称的数组
已知人脸编码 = [
    面部编码，
]
已知面孔名称 = [
        “瓦利德”
]

而真实：
        # 抓取单帧视频
        ret, 帧 = video_capture.read()

        # 将图像从 BGR 颜色（OpenCV 使用）转换为 RGB 颜色（face_recognition 使用）
        rgb_frame = 帧[:, :, ::-1]

        # 查找当前帧视频中的所有人脸
        面部位置 = 面部识别.面部位置(rgb_frame)
        face_encodings =face_recognition.face_encodings（rgb_frame，face_locations）

        # 循环遍历该视频帧中的每张脸
        对于（上，右，下，左），zip中的face_encoding（face_locations，face_encodings）：
            # 查看该面孔是否与已知面孔匹配
            匹配=face_recognition.compare_faces（known_face_encodings，face_encoding）

            名称=“未知”

            如果匹配中为真：
                first_match_index = matches.index(True)
                名称 =known_face_names[first_match_index]

            # 在脸部周围画一个方框
            cv2.rectangle(frame, (左, 上), (右, 下), (0, 0, 255), 2)

            # 在脸部下方画一个带有名字的标签
            cv2.rectangle(frame, (左, 下 - 35), (右, 下), (0, 0, 255), cv2.FILLED)
            字体= cv2.FONT_HERSHEY_DUPLEX
            cv2.putText(框架, 名称, (左 + 6, 下 - 6), 字体, 1.0, (255, 255, 255), 1)

        # 显示结果图像
        cv2.imshow(&#39;视频&#39;, 帧)

        # 按键盘上的“q”退出！
        如果 cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
            休息

# 释放网络摄像头的句柄
video_capture.release()
cv2.destroyAllWindows()


我尝试利用face_recognition 库执行Python 脚本。尽管成功安装了face_recognition_models包，但该脚本在执行时不断提示我安装它。我希望脚本能够识别已安装的包并执行而不会出现错误，但它继续请求安装face_recognition_models。]]></description>
      <guid>https://stackoverflow.com/questions/78300706/script-continuously-requests-face-recognition-models-installation-despite-being</guid>
      <pubDate>Tue, 09 Apr 2024 19:29:09 GMT</pubDate>
    </item>
    <item>
      <title>TensorBoard HParams 未显示超参数调整的准确性指标</title>
      <link>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</link>
      <description><![CDATA[我正在 TensorFlow 中进行超参数调整，并使用 TensorBoard 中的 HParams 插件设置了一个实验来记录不同的配置。我的模型正在使用 dropout 和学习率的变化进行训练，并且我正在记录这些参数以及模型的准确性。但是，当我打开 TensorBoard 并导航到 HParams 仪表板时，不会显示与每个试验相关的准确性指标。该表正确显示了超参数，但“准确性”列为空，即使我的代码使用“准确性”作为指标来编译模型并使用 hp.KerasCallback 进行日志记录。我已经验证了模型训练正确，并且标量仪表板等其他 TensorBoard 功能显示了各个时期的准确性趋势。我正在寻求帮助来理解为什么 HParams 表中没有显示准​​确性以及如何解决此问题。
图片：准确度列中缺少值
我使用 TensorBoard 的 HParams 进行超参数调整的代码：
从tensorboard.plugins.hparams导入api作为hp
将张量流导入为 tf
从tensorflow.keras.layers导入Conv2D、MaxPooling2D、Dense、Flatten、Dropout

# 定义超参数
HP_DROPOUT = hp.HParam(&#39;dropout&#39;, hp.Discrete([0.2, 0.3, 0.4]))
HP_LEARNING_RATE = hp.HParam(&#39;learning_rate&#39;, hp.Discrete([1e-2, 1e-3]))

# 设置日志记录
log_dir = &#39;./tensorboard/nn_1&#39;
使用 tf.summary.create_file_writer(log_dir).as_default()：
    hp.hparams_config(
        hparams=[HP_DROPOUT, HP_LEARNING_RATE],
        指标=[hp.Metric(&#39;准确度&#39;,display_name=&#39;准确度&#39;)]
    ）

# 训练函数
def train_test_model(hparams, session_num):
    model_name = f“model_1_session_{session_num}”
    print(f&quot;使用超参数 {hparams} 训练 {model_name}...&quot;)
    模型 = tf.keras.Sequential([
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        MaxPooling2D(pool_size=(2, 2)),
        展平（），
        密集（10，激活=&#39;softmax&#39;）
    ]）
    模型.编译(
        损失=&#39;分类交叉熵&#39;，
        优化器=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),
        指标=[&#39;准确性&#39;]
    ）

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f&#39;{log_dir}/{model_name}&#39;)
    hparams_callback = hp.KerasCallback(writer=f&#39;{log_dir}/{model_name}&#39;, hparams=hparams)

    模型.拟合(
        x_train_reshape, y_train_,
        纪元=3，
        验证数据=（x_val_reshape，y_val），
        回调=[hparams_callback，tensorboard_callback]
    ）

# 对每组超参数进行训练
会话编号 = 0
对于 HP_DROPOUT.domain.values 中的 dropout_rate：
    对于 HP_LEARNING_RATE.domain.values 中的learning_rate：
        hparams = {
            HP_DROPOUT：辍学率，
            HP_LEARNING_RATE：学习率，
        }
        train_test_model(hparams, session_num)
        会话编号 += 1

]]></description>
      <guid>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</guid>
      <pubDate>Tue, 09 Apr 2024 12:14:56 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：“GraphModule”对象不可下标（Pytorch 中 .onnx ML 模型的访问权重）</title>
      <link>https://stackoverflow.com/questions/78289901/typeerror-graphmodule-object-is-not-subscriptable-access-weights-for-onnx-m</link>
      <description><![CDATA[我有一个流行的 .onnx ML 天气预报模型，我正在尝试将其转换为 PyTorch 进行微调。我使用以下代码来转换它：
导入操作系统
将 numpy 导入为 np
导入onnx
从 onnx 导入 numpy_helper
将 onnxruntime 导入为 ort
从 onnx2torch 导入 转换

model_24 = onnx.load(&#39;pangu_weather_24.onnx&#39;)
tm = Convert(model_24) #将onnx模型转换为torch

从这里，我想访问“tm”对象中模型的权重，但我似乎无法在网上找到任何相关资源。
尝试使用 tm[0] 对其进行下标会显示以下错误：
TypeError：“GraphModule”对象不可下标

通过“tm.dict”获取该对象的字典更加令人困惑（粘贴在图像中）。
在线访问 PyTorch 权重矩阵的常规方法也显示出图形模块不可下标的相同错误]]></description>
      <guid>https://stackoverflow.com/questions/78289901/typeerror-graphmodule-object-is-not-subscriptable-access-weights-for-onnx-m</guid>
      <pubDate>Mon, 08 Apr 2024 02:13:29 GMT</pubDate>
    </item>
    <item>
      <title>为什么 PCA 图像与原始图像完全不相似？</title>
      <link>https://stackoverflow.com/questions/58976380/why-the-pca-image-doesnt-resemble-the-original-image-at-all</link>
      <description><![CDATA[我正在尝试在没有任何图像降维库的情况下实现 PCA。我尝试了 O&#39;Reilly 计算机视觉书中的代码，并在示例 lenna 图片上实现了它：
 来自 PIL 导入图像
    从 numpy 导入 *

    def pca(X):
        num_data, 暗淡 = X.shape

        Mean_X = X.mean(轴=0)
        X = X - 平均值_X

        如果暗淡&gt;数据数量：
            # PCA 紧凑技巧
            M = np.dot(X, X.T) # 协方差矩阵
            e, U = np.linalg.eigh(M) # 计算特征值和特征向量
            tmp = np.dot(X.T, U).T
            V = tmp[::-1] # 反转，因为最后一个特征向量是我们想要的
            S = np.sqrt(e)[::-1] #reverse 因为最后一个特征值是按递增顺序排列的
            对于范围内的 i(V.shape[1])：
                V[:,i] /= S
        别的：
            #普通PCA、SVD方法
            U,S,V = np.linalg.svd(X)
            V = V[:num_data] # 仅返回第一个 num_data 才有意义
        返回 V、S、mean_X
img=color.rgb2gray(io.imread(&#39;D:\lenna.png&#39;))
x,y,z=pca(img)
plt.imshow(x)



但是 PCA 的图像图看起来根本不像原始图像。
据我所知，PCA 有点减少图像尺寸，但它仍然会在某种程度上类似于原始图像，但细节较低。代码有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/58976380/why-the-pca-image-doesnt-resemble-the-original-image-at-all</guid>
      <pubDate>Thu, 21 Nov 2019 13:39:39 GMT</pubDate>
    </item>
    <item>
      <title>错误“无法将字符串转换为浮点数：'INLAND'”</title>
      <link>https://stackoverflow.com/questions/55332339/error-could-not-convert-string-to-float-inland</link>
      <description><![CDATA[我正在做一个使用机器学习进行房价预测的项目，并想将其提交给一家私营公司申请。
我正在 Jupiter 笔记本中处理这个项目，但我无法修复有关将字符串转换为数字数据的错误
from sklearn.model_selection import train_test_split
X_train,X_test, Y_train, Y_test=train_test_split(X,
                                              是，
                                             测试大小=0.2，
                                               随机状态=0）
从 sklearn.preprocessing 导入 StandardScaler
独立标量 = StandardScaler()
X_train = Independent_scalar.fit_transform(X_train) #拟合和变换
X_test = Independent_scalar.transform(X_test) # 只进行变换
打印（X_train）

我期望训练集数据是完全数字化的]]></description>
      <guid>https://stackoverflow.com/questions/55332339/error-could-not-convert-string-to-float-inland</guid>
      <pubDate>Mon, 25 Mar 2019 06:33:59 GMT</pubDate>
    </item>
    <item>
      <title>使用无标签的机器学习进行异常检测[关闭]</title>
      <link>https://stackoverflow.com/questions/44942551/anomaly-detection-with-machine-learning-without-labels</link>
      <description><![CDATA[我正在一段时间内跟踪多个信号，并将它们与时间戳相关联，如下所示：
&lt;前&gt;&lt;代码&gt;t0 1 10 2 0 1 0 ...
t1 1 10 2 0 1 0 ...
t2 3 0 9 7 1 1 ... //按下按钮更改模式
t3 3 0 9 7 1 1 ...
t4 3 0 8 7 1 1 ... // 按下按钮来调整某个特性，如温度（信号 3）

其中 t0 是时间戳，1 是信号 1 的值，10 是信号 2 的值，依此类推。
在该特定时间段内捕获的数据应被视为正常情况。现在应该可以检测到与正常情况的显着偏差。通过显着推导，我并不是指一个信号值仅更改为跟踪阶段期间未见过的值，而是指许多值发生了尚未相互关联的更改。我不想对规则进行硬编码，因为将来可能会添加或删除更多信号，并且可能会添加或删除其他“modi”信号。可以实现具有其他信号值的。
这可以通过某种机器学习算法来实现吗？如果发生小的推导，我希望算法首先将其视为对训练集的微小更改，如果将来多次发生，则应该“学习”。主要目标是检测更大的变化/异常。
我希望我能足够详细地解释我的问题。提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/44942551/anomaly-detection-with-machine-learning-without-labels</guid>
      <pubDate>Thu, 06 Jul 2017 07:36:35 GMT</pubDate>
    </item>
    </channel>
</rss>