<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 10 Jan 2025 06:25:00 GMT</lastBuildDate>
    <item>
      <title>使用“bitsandbytes”4 位量化需要最新版本的 bitsandbytes：“pip install -U bitsandbytes”</title>
      <link>https://stackoverflow.com/questions/79344565/using-bitsandbytes-4-bit-quantization-requires-the-latest-version-of-bitsandby</link>
      <description><![CDATA[加载 tokenizer 时，我收到此错误：
ImportError：使用 bitsandbytes 4 位量化需要最新版本的 bitsandbytes：
pip install -U bitsandbytes。

我在 Macbook M2 pro 上使用 Jupyter 笔记本。
以下是源代码：
quant_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_use_double_quant=True,
bnb_4bit_compute_dtype=torch.bfloat16,
bnb_4bit_quant_type=&quot;nf4&quot;

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

base_model = AutoModelForCausalLM.from_pretrained(
BASE_MODEL,
quantization_config=quant_config,
device_map=&quot;auto&quot;,
)

base_model.generation_config.pad_token_id = tokenizer.pad_token_id

有人能帮忙吗？
我按照说明更新了 bitsandbytes，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79344565/using-bitsandbytes-4-bit-quantization-requires-the-latest-version-of-bitsandby</guid>
      <pubDate>Fri, 10 Jan 2025 03:52:41 GMT</pubDate>
    </item>
    <item>
      <title>lightgbm.cv：cvbooster.best_iteration 总是返回 -1</title>
      <link>https://stackoverflow.com/questions/79344545/lightgbm-cv-cvbooster-best-iteration-always-returns-1</link>
      <description><![CDATA[我正在从 XGBoost 迁移到 LightGBM（因为我需要它精确处理交互约束），并且我很难理解 LightGBM CV 的结果。在下面的示例中，在第 125 次迭代中实现了最小对数损失，但 model[&#39;cvbooster&#39;].best_iteration 返回 -1。我原本希望它也能返回 125 - 还是我在这里误解了什么？有没有更好的方法来获得最佳迭代，还是只需要手动检查？
我看过这个讨论，但即使我检查cvbooster中的boosters（例如，model[&#39;cvbooster&#39;].boosters[0].best_iteration），它们也都返回 -1...
import lightgbm as lgb
import numpy as np
from sklearn import datasets

X, y = datasets.make_classification(n_samples=10_000, n_features=5, n_informative=3, random_state=9)

data_train_lgb = lgb.Dataset(X, label=y)

param = {&#39;objective&#39;: &#39;binary&#39;,
&#39;metric&#39;: [&#39;binary_logloss&#39;],
&#39;device_type&#39;: &#39;cuda&#39;}

model = lgb.cv(param,
data_train_lgb,
num_boost_round=1_000,
return_cvbooster=True)

opt_1 = np.argmin(model[&#39;valid binary_logloss-mean&#39;])
print(f&quot;index argmin: {opt_1}&quot;)
print(f&quot;logloss argmin: {model[&#39;valid binary_logloss-mean&#39;][opt_1]}&quot;)

opt_2 = model[&#39;cvbooster&#39;].best_iteration
print(f&quot;index best_iteration: {opt_2}&quot;)
print(f&quot;logloss best_iteration: {model[&#39;valid binary_logloss-mean&#39;][opt_2]}&quot;)

---

&gt;&gt;&gt; 索引参数最小值：125
&gt;&gt;&gt; 对数损失参数最小值：0.13245999867688793

&gt;&gt;&gt; 索引最佳迭代：-1
&gt;&gt;&gt; 对数损失最佳迭代：0.2661896445658779
]]></description>
      <guid>https://stackoverflow.com/questions/79344545/lightgbm-cv-cvbooster-best-iteration-always-returns-1</guid>
      <pubDate>Fri, 10 Jan 2025 03:40:32 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 使用高度右偏的数据</title>
      <link>https://stackoverflow.com/questions/79344322/lightgbm-using-highly-right-skewed-data</link>
      <description><![CDATA[序言：我正在处理一个倾斜的数据集，在约 7000 个数据点中，只有约 2500 个是非零值，最有趣的数据点是最右边的异常值。
我希望使用 LightGBM 根据多个特征预测此目标数据，但由于数据的倾斜性而遇到问题。我考虑过使用转换（例如 log、sqrt、box-cox），但它们并没有提高 ML 模型的性能。

我偶然发现了 imblearn.oversampling，它可能是解决我的问题的方法。它为异常值创建合成样本，以便模型有更多数据点可供训练。但是，因为我处理的是整数而不是分类，所以我需要对数据进行分类。我已将代码设置如下：
# 为 SMOTE 设置连续目标
binning = KBinsDiscretizer(n_bins=200, encode=&#39;ordinal&#39;, strategies=&#39;quantile&#39;)
targets[&#39;binned&#39;] = binning.fit_transform(new_targets.values.reshape(-1, 1)).ravel()

# 为 SMOTE 定义动态采样策略
smote_sampling_strategy = {i: 125 for i in range(1, 83)}
# 使用调整后的策略应用 SMOTE
smote = SMOTE(sampling_strategy=smote_sampling_strategy, random_state=42)
features_upsampled, binned_targets_upsampled = smote.fit_resample(features, new_targets[&#39;binned&#39;])

# 替换已分箱的具有连续值的目标
bin_centers = binning.inverse_transform(binned_targets_upsampled.to_numpy().reshape(-1, 1)).ravel()

# 重建上采样目标 DataFrame
upsampled_targets = pd.DataFrame({
&#39;new_hothr&#39;: bin_centers,
&#39;binned&#39;: binned_targets_upsampled
})


问题：
虽然我很想使用这种方法，但我对使用此方法的最佳实践有疑问，但这些疑问并不明确在 imblearn 的文档中找到，例如：如何确定将数据划分为多少个 bin？如何确定要添加多少个合成样本？如何确保此方法不会导致过度拟合？最后，即使按照上述方式进行上采样，这个精炼数据集仍然无法捕获 LightGBM 在测试/训练中预测结果的第 99 个百分位数的异常值（使用 GridSearchCV 选择超参数），因此如果有人有进一步的建议可以尝试，我洗耳恭听。 
 提前感谢大家的时间和见解！-V ]]></description>
      <guid>https://stackoverflow.com/questions/79344322/lightgbm-using-highly-right-skewed-data</guid>
      <pubDate>Thu, 09 Jan 2025 23:51:44 GMT</pubDate>
    </item>
    <item>
      <title>CycleGAN 的鉴别器损失停留在 0.0</title>
      <link>https://stackoverflow.com/questions/79344264/discriminator-loss-for-cyclegan-stuck-at-0-0</link>
      <description><![CDATA[我目前正在训练 BD-Cycle GAN，这是 Mol-Cycle GAN 的修改版本。我没有修改任何代码，但需要从 [Mol-Cycle GAN] (https://github.com/ardigen/mol-cycle-gan) 存储库下载 utils 文件夹和 environment.yml。使用默认参数运行 train.py 文件时，判别器 A 和 B 的损失都停留在 0.0，但生成器损失似乎正常。
我不明白问题的原因是什么，因为我使用的是作者提供的模型官方 repo，没有修改任何代码或超参数，但得到了这个结果。判别器的损失在每个时期都保持在 0.0，破坏了整个训练过程。]]></description>
      <guid>https://stackoverflow.com/questions/79344264/discriminator-loss-for-cyclegan-stuck-at-0-0</guid>
      <pubDate>Thu, 09 Jan 2025 23:15:36 GMT</pubDate>
    </item>
    <item>
      <title>使用什么样的规范化[关闭]</title>
      <link>https://stackoverflow.com/questions/79343281/what-kind-of-normalization-is-used</link>
      <description><![CDATA[yelp_features_1

0 1 2 3 4 ... 27 28 29 30 31
0 8 2.0000 1 0 0 ... 1.4545 1.7439 1.9363 0.000 142.0909
1 9 0.0000 0 0 0 ... 1.4545 1.7439 1.9363 0.000 142.0909
2 2 2.0000 1 0 0 ... 1.4545 1.7439 1.9363 0.000 142.0909
3 6 2.0000 1 0 0 ... 1.4545 1.7439 1.9363   0.000 142.0909
4 3 0.0000 0 0 0 ... 1.4545 1.7439 1.9363 0.000 142.0909
   .. ... .. .. .. .. ... ... ... ... ... ...
45949 3 0.3333 1 0 0 ... 0.7437 0.3631 1.8376 1.6182 170.4366
45950 1 0.6667 0 0 1 ... 0.7437 0.3631 1.8376 1.6182 170.4366
45951 3 1.0000 1 0 0 ... 0.8488 1.0167 1.7599 2.4910 133.1060
45952 2 2.0000 0 0 0 ... 0.8488 1.0167 1.7599 2.4910 133.1060
45953 1 1.0000 1 0 1 ... 0.8488 1.0167 1.7599 2.4910 133.1060

yelp_feature_2
-----------------
0 1 2 3 4 ... 27 28 29 30 31
0 0.0224 0.0705 0.4287 1.0000 1.0000 ... 0.0100 0.0149 0.5920 0.1393 0.4975
1 0.0249 1.0000 1.0000 1.0000 1.0000 ... 0.0100 0.0149 0.5920 0.1393 0.4975
2 0.0062 0.0705 0.4287 1.0000 1.0000 ... 0.0100 0.0149 0.5920 0.1393 0.4975
3 0.0174 0.0705 0.4287 1.0000 1.0000 ... 0.0100 0.0149 0.5920 0.1393 0.4975
4 0.0091 1.0000 1.0000 1.0000 1.0000 ... 0.0100 0.0149 0.5920 0.1393 0.4975
     ………………………………
45949 0.0091 0.6951 0.4287 1.0000 1.0000 ... 0.4577 0.2687 0.3682 0.3035 0.8458
45950 0.0032 0.5739 1.0000 1.0000 0.0228 ... 0.6020 0.4030 0.4826 0.8010 0.1642
45951 0.0091 0.3500 0.4287 1.0000 1.0000 ... 0.6020 0.4030 0.4826 0.8010 0.1642
45952 0.0062 0.0705 1.0000 1.0000 1.0000 ... 0.7811 0.8557 0.4428 0.4478 0.5871
45953 0.0032 0.3500 0.4287 1.0000 0.0228 ... 0.7811 0.8557 0.4428 0.4478 0.5871

yelp_features_1 被标准化为yelp_feature_2。无法弄清楚使用了什么规范化。看起来像是对数。]]></description>
      <guid>https://stackoverflow.com/questions/79343281/what-kind-of-normalization-is-used</guid>
      <pubDate>Thu, 09 Jan 2025 15:56:46 GMT</pubDate>
    </item>
    <item>
      <title>请帮我找到 KAN 模型的正确代码[关闭]</title>
      <link>https://stackoverflow.com/questions/79342506/please-help-me-with-the-correct-code-for-the-kan-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79342506/please-help-me-with-the-correct-code-for-the-kan-model</guid>
      <pubDate>Thu, 09 Jan 2025 11:59:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 lgbm 回归器进行交叉验证[关闭]</title>
      <link>https://stackoverflow.com/questions/79341444/cross-validation-with-lgbm-regressor</link>
      <description><![CDATA[当我使用 lgbm 回归器的交叉验证时，
它仅用于查找 best_num_round
我说得对吗？
与 lgbm 一起使用时，我找不到它的其他用处
我用它训练模型，除了 num boost round 之外找不到其他用处
但我们可以使用早期停止技术来代替它]]></description>
      <guid>https://stackoverflow.com/questions/79341444/cross-validation-with-lgbm-regressor</guid>
      <pubDate>Thu, 09 Jan 2025 05:23:42 GMT</pubDate>
    </item>
    <item>
      <title>Azure AutoML 图像分类作业</title>
      <link>https://stackoverflow.com/questions/79341200/azure-automl-image-classification-job</link>
      <description><![CDATA[我在尝试为 Azure ML 中的数据集创建 MLTable YAML 文件时遇到问题。
我的工作区中有一个默认数据存储，其中包含两个带有图像的文件夹（OK 和 NOK）。我的目标是读取所有图像并使用文件夹名称作为每幅图像的标签。
以下是我到目前为止尝试过的方法：
mltable_yaml = &quot;&quot;&quot;
type: mltable
paths:
- file: ./OK 
- file: ./NOK 
transformations:
- read_from_directory:
image_column: image_url 
folder_column: label 
recursive: true 
&quot;&quot;&quot;

# 创建目录并保存 MLTable
mltable_dir = &quot;image_data&quot;
os.makedirs(mltable_dir, exist_ok=True)
with open(os.path.join(mltable_dir, &quot;MLTable&quot;), &quot;w&quot;) as f:
f.write(mltable_yaml)

training_data = Input(
type=&quot;mltable&quot;,
path=mltable_dir
)

但是，当我运行实验时，我遇到了以下错误：
MLTable 输入无效。UserErrorException：
消息：从数据集获取数据时遇到用户错误。错误：UserErrorException：
消息：MLTable yaml 架构无效：
错误代码：ScriptExecution.Validation
验证错误代码：无效
验证目标：脚本
本机错误：数据流脚本错误：InvalidScriptElement(&quot;read_from_directory&quot;)
ScriptError(InvalidScriptElement(&quot;read_from_directory&quot;))
=&gt; 脚本元素&quot;read_from_directory&quot;无效
InvalidScriptElement(&quot;read_from_directory&quot;)
错误消息：Yaml 脚本无效：InvalidScriptElement(&quot;read_from_directory&quot;)。| session_id=1a30b15a-7e85-498b-b735-2348bfe0625b
InnerException None
ErrorResponse 
{
&quot;error&quot;: {
&quot;code&quot;: &quot;UserError&quot;,
&quot;message&quot;: &quot;MLTable yaml 模式无效:\n错误代码：ScriptExecution.Validation\n验证错误代码：无效\n验证目标：脚本\n本机错误：数据流脚本错误：InvalidScriptElement(\&quot;read_from_directory\&quot;)\n\tScriptError(InvalidScriptElement(\&quot;read_from_directory\&quot;))\n=&gt;无效的脚本元素 \&quot;read_from_directory\&quot;\n\tInvalidScriptElement(\&quot;read_from_directory\&quot;)\n错误消息：Yaml 脚本无效：InvalidScriptElement(\&quot;read_from_directory\&quot;)。| session_id=1a30b15a-7e85-498b-b735-2348bfe0625b&quot;
}
}
InnerException UserErrorException:
消息：MLTable yaml 架构无效：
错误代码：ScriptExecution.Validation
验证错误代码：无效
验证目标：脚本
本机错误：数据流脚本错误：InvalidScriptElement(&quot;read_from_directory&quot;)
ScriptError(InvalidScriptElement(&quot;read_from_directory&quot;))
=&gt; 无效的脚本元素 &quot;read_from_directory&quot;
InvalidScriptElement(&quot;read_from_directory&quot;)
错误消息：Yaml 脚本无效：InvalidScriptElement(&quot;read_from_directory&quot;)。| session_id=1a30b15a-7e85-498b-b735-2348bfe0625b
InnerException None
ErrorResponse 
{
&quot;error&quot;: {
&quot;code&quot;: &quot;UserError&quot;,
&quot;message&quot;: &quot;MLTable yaml 架构无效：\n错误代码：ScriptExecution.Validation\n验证错误代码：无效\n验证目标：脚本\n本机错误：数据流脚本错误：InvalidScriptElement(\&quot;read_from_directory\&quot;)\n\tScriptError(InvalidScriptElement(\&quot;read_from_directory\&quot;))\n=&gt;无效的脚本元素 \&quot;read_from_directory\&quot;\n\tInvalidScriptElement(\&quot;read_from_directory\&quot;)\n错误消息：Yaml 脚本无效：InvalidScriptElement(\&quot;read_from_directory\&quot;)。| session_id=1a30b15a-7e85-498b-b735-2348bfe0625b&quot;
}
}
ErrorResponse 
{
&quot;error&quot;: {
&quot;code&quot;: &quot;UserError&quot;,
&quot;message&quot;: &quot;从数据集获取数据时遇到用户错误。错误：UserErrorException:\n\t消息：MLTable yaml 架构无效:\n错误代码：ScriptExecution.Validation\n验证错误代码：无效\n验证目标：脚本\n本机错误：数据流脚本错误：InvalidScriptElement(\&quot;read_from_directory\&quot;)\n\tScriptError(InvalidScriptElement(\&quot;read_from_directory\&quot;))\n=&gt; 无效脚本元素 \&quot;read_from_directory\&quot;\n\tInvalidScriptElement(\&quot;read_from_directory\&quot;)\n错误 M

从错误详细信息来看，似乎无法识别 read_from_directory 元素，但我不确定如何构造 YAML 以正确将文件夹名称映射到标签。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79341200/azure-automl-image-classification-job</guid>
      <pubDate>Thu, 09 Jan 2025 02:14:54 GMT</pubDate>
    </item>
    <item>
      <title>如何从物理信息神经网络 (PINN) 获取具有初始和边界条件的 PDE 的单一解？</title>
      <link>https://stackoverflow.com/questions/79329941/how-to-get-a-single-solution-from-a-physics-informed-neural-network-pinn-for-a</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79329941/how-to-get-a-single-solution-from-a-physics-informed-neural-network-pinn-for-a</guid>
      <pubDate>Sun, 05 Jan 2025 01:01:10 GMT</pubDate>
    </item>
    <item>
      <title>关于论文“RL CQL”和“Cal-QL：经过校准的离线 RL 预训练以实现高效的在线微调”的困惑 [关闭]</title>
      <link>https://stackoverflow.com/questions/79310839/confusion-about-papers-rl-cql-and-cal-ql-calibrated-offline-rl-pre-training</link>
      <description><![CDATA[最近看了两篇论文，包括《Conservative Q-Learning for Offline Reinforcement Learning》和《Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning》。
根据CQL的论文，我认为如果一个状态-动作对存在于行为策略收集到的数据集中，那么在训练时就不会受到惩罚（正则项为零）。
因为惩罚项的两个子项抵消了。或者如果它不在数据集中，就会受到第一个子项的惩罚，如下图1所示
（正则项以蓝色突出显示）。因为如果状态-动作对不在数据集中，惩罚项的第二个子项就会为零。所以对于行为策略收集到的状态-动作对，可能由于最大化和引导训练而被高估。因为即使使用目标网络，也只能缓解高估问题。
因此，CQL的估计值与真实值之间的关系可能如图2所示。我上面说的对吗？
图1：
在此处输入图像描述
图2：
在此处输入图像描述
但在Cql-QL中，我们可以看到CQL的估计值明显低于真实值，如图3所示。
图3：
在此处输入图像描述
在论文CQL和Cql-QL中，对CQL估计值与真值关系的描述存在矛盾。
如何理解？我上面的描述对吗？
非常感谢您的回答。
如何理解？我上面的描述对吗？RL专家能给我一个答案吗？]]></description>
      <guid>https://stackoverflow.com/questions/79310839/confusion-about-papers-rl-cql-and-cal-ql-calibrated-offline-rl-pre-training</guid>
      <pubDate>Fri, 27 Dec 2024 03:20:54 GMT</pubDate>
    </item>
    <item>
      <title>为什么 RAG 比 LLM 慢？</title>
      <link>https://stackoverflow.com/questions/78432197/why-rag-is-slower-than-llm</link>
      <description><![CDATA[我使用 RAG 和 LLAMA3 来做 AI 机器人。我发现 RAG 和 chromadb 比调用 LLM 本身慢得多。
根据测试结果，仅一个约 1000 个单词的简单网页，检索就需要 2 秒以上：
检索所用时间：2.245511054992676
LLM 所用时间：2.1182022094726562

这是我的简单代码：
embeddings = OllamaEmbeddings(model=&quot;llama3&quot;)
vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
retriever = vectorstore.as_retriever()
question = &quot;什么是 COCONut？&quot;
start = time.time()
retrieved_docs = withdrawer.invoke(question)
formatted_context = Combine_docs(retrieved_docs)
end = time.time()
print(f&quot;检索所用时间：{end - start}&quot;)

start = time.time()
answer = ollama_llm(question, formatted_context)
end = time.time()
print(f&quot;LLM 所用时间：{end - start}&quot;)

我发现当我的 chromaDB 大小只有 1.4M 左右时，检索需要 20 多秒，而 LLM 仍然只需要 3 或 4 秒。我遗漏了什么吗？还是 RAG 技术本身就很慢？]]></description>
      <guid>https://stackoverflow.com/questions/78432197/why-rag-is-slower-than-llm</guid>
      <pubDate>Sun, 05 May 2024 12:28:52 GMT</pubDate>
    </item>
    <item>
      <title>predict_proba 如何与交叉验证一起工作？</title>
      <link>https://stackoverflow.com/questions/72638981/how-does-predict-proba-work-with-cross-validation</link>
      <description><![CDATA[使用 5 倍交叉验证创建模型时，会创建 5 个不同的模型。最终模型的选择可能会有所不同：

5 倍创建的模型中估计最佳（或其他标准）的模型或
在所有数据集上训练后创建的模型。

我理解交叉验证用于模型检查，而不是用于模型构建。因此，当在模型上使用 predict_proba 时，这个概率是如何定义的？您能否分享一些论文或文章，讨论如何使用交叉验证在 R 中的插入符号和 Python 中的 sklearn 中进行预测？]]></description>
      <guid>https://stackoverflow.com/questions/72638981/how-does-predict-proba-work-with-cross-validation</guid>
      <pubDate>Wed, 15 Jun 2022 23:47:57 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证的实现</title>
      <link>https://stackoverflow.com/questions/60231102/implementation-of-cross-validation</link>
      <description><![CDATA[我很困惑，因为许多人都有自己的方法来应用交叉验证。例如，有些人将它应用于整个数据集，有些人将它应用于训练集。 
我的问题是，下面的代码是否适合实现交叉验证，并在应用交叉验证的情况下从此类模型进行预测？
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import KFold

model= GradientBoostingClassifier(n_estimators= 10,max_depth = 10, random_state = 0)#sepcifying the model
cv = KFold(n_splits=5, shuffle=True)

from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import cross_val_score

#X - 整个数据集
#y - 整个数据集但仅目标属性

y_pred = cross_val_predict(model, X, y, cv=cv)
scores = cross_val_score(模型，X，y，cv=cv)
]]></description>
      <guid>https://stackoverflow.com/questions/60231102/implementation-of-cross-validation</guid>
      <pubDate>Fri, 14 Feb 2020 17:37:27 GMT</pubDate>
    </item>
    <item>
      <title>如何计算神经网络预测的置信度分数</title>
      <link>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</link>
      <description><![CDATA[我正在使用深度神经网络模型（在 keras 中实现）进行预测。类似这样的代码：
def make_model():
model = Sequential() 
model.add(Conv2D(20,(5,5),activation = &quot;relu&quot;))
model.add(MaxPooling2D(pool_size=(2,2))) 
model.add(Flatten())
model.add(Dense(20,activation = &quot;relu&quot;))
model.add(Lambda(lambda x: tf.expand_dims(x, axis=1)))
model.add(SimpleRNN(50,activation=&quot;relu&quot;))
model.add(Dense(1,activation=&quot;sigmoid&quot;)) 
model.compile(loss = &quot;binary_crossentropy&quot;,optimizer = adagrad,metrics = [&quot;accuracy&quot;])

返回模型

model = make_model()
model.fit(x_train, y_train, validation_data = (x_validation,y_validation), epochs = 25, batch_size = 25, verbose = 1)

##预测：
prediction = model.predict_classes(x)
probabilities = model.predict_proba(x) #我假设这些是被预测的类的概率

我的问题是分类（二元）问题。我希望计算每个预测的置信度分数，即我想知道 - 我的模型是否 99% 确定它是“0”或 58% 是“0”。
我找到了一些关于如何做到这一点的观点，但无法实现它们。我希望遵循的方法是：“使用分类器，当你输出时，你可以将值解释为属于每个特定类别的概率。你可以使用它们的分布作为你对观察结果属于该类别的信心的粗略衡量标准。”
我应该如何使用类似上述模型的东西进行预测，以便获得对每个预测的信心？我希望有一些实际的例子（最好是在 Keras 中）。]]></description>
      <guid>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</guid>
      <pubDate>Wed, 22 Jan 2020 02:52:32 GMT</pubDate>
    </item>
    <item>
      <title>验证集上的验证程序[关闭]</title>
      <link>https://stackoverflow.com/questions/56270679/validation-procedure-on-validation-set</link>
      <description><![CDATA[我很难理解验证步骤；当我不想使用 k 倍交叉验证而只想使用验证集时，我也想得到一些建议。我一直在阅读，但似乎无法正确掌握 k 倍交叉验证：

我是否将初始数据分成 k 倍，然后在 k-1 上进行训练并在剩下的 1 上进行测试，然后继续旋转 - 因此每个折叠都用于测试等。

或者我是否将初始数据分成训练和测试数据 - 然后将训练数据分成 k 倍并进行交叉验证，然后最后在看不见的测试数据上测试准确性？

在 k 倍交叉验证期间如何选择最佳参数？
cross_val_score 在返回分数列表后，是否会在准确率最高的验证步骤中应用最佳参数？ （代码如下）


model = svm.SVC(kernel=&#39;linear&#39;, C=1)
scores = cross_val_score(model, X, y, cv=5)

或者这一步应该手动完成（由我完成）？使用 gridsearchcv 等？

就我而言，我有一个初始数据集，其中包含 400.000 个样本（行）和大约 70 个特征（列）。对我的数据集执行 k 折交叉验证需要很长时间（据我所知，它主要用于较小的数据集），相反，我希望有 3 组数据：训练（90%）验证（5%）和测试（5%）- 对那 5% 进行验证并在该步骤中调整我的模型参数，最后检查测试集的准确性。应该怎么做？
]]></description>
      <guid>https://stackoverflow.com/questions/56270679/validation-procedure-on-validation-set</guid>
      <pubDate>Thu, 23 May 2019 08:05:08 GMT</pubDate>
    </item>
    </channel>
</rss>