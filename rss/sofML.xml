<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Thu, 10 Apr 2025 12:36:53 GMT</lastBuildDate>
    <item>
      <title>我刚刚训练，使用keras image_from_dataset训练的事实会影响我的指标</title>
      <link>https://stackoverflow.com/questions/79566385/does-the-fact-that-i-have-just-train-test-split-using-keras-image-from-dataset</link>
      <description><![CDATA[我一直在试图弄清楚为什么训练后我的指标很低。我的F1得分为75％，这根本不是我所期望的。我审查了代码，我怀疑问题是火车，验证拆分，但我正在评估验证集的模型 y_pred =（model.predict（val_ds）＆gt; 0.5）.astype（int）（int）。我阅读了KERAS文档，以查看我是否犯了错误，或者是否有办法将数据集拆分为火车，验证，测试拆分，但我找不到任何可能指出是否犯错的东西。这是我的代码：
 将TensorFlow导入为TF
来自Tensorflow.keras.applications导入Densenet169
来自tensorflow.keras.layers导入密集，globalaveration -pooling2d，Randomflip，RandomRotation，辍学
来自Tensorflow.keras.models导入模型，顺序
来自Tensorflow.keras.optimizer导入Adam
从tensorflow.keras.regulinizer导入l2
来自tensorflow.keras.applications.densenet导入preprocess_input
来自sklearn.metrics导入混淆_matrix，f1_score
导入numpy作为NP

＃定义常数
img_size =（224，224）＃匹配densenet169输入大小
batch_size = 32
时代= 15
data_dir =＆quot; colon_images＆quot;
class_names = [＆quast; cancyous&#39;&#39;正常＆quot;]
split_ratio = 0.2

＃直接加载数据集在224x224
def create_dataset（子集）：
    返回tf.keras.utils.image_dataset_from_directory（
        data_dir，
        验证_split = split_ratio，
        子集=子集
        种子= 42，
        image_size = img_size，＃直接以目标大小加载
        batch_size = batch_size，
        label_mode =&#39;binary&#39;
    ）

train_ds = create_dataset（“训练”）
val_ds = create_dataset（&#39;验证＆quot;）

＃通过增强进行预处理（仅在培训期间活跃）
预处理=顺序（[
    Randomflip（“水平”，“），
    随机旋转（0.1），
    tf.keras.layers.lambda（preprocess_input）＃正确归一化
）））

＃构建模型
base_model = densenet169（
    权重=&#39;Imagenet&#39;，
    include_top = false，
    input_shape = img_size +（3，）
）

输入= tf.keras.input（shape = img_size +（3，））
X =预处理（输入）
x = base_model（x）
x = globalaveragepooling2d（）（x）
x =密集（512，激活=&#39;relu&#39;，kernel_regularizer = l2（0.01））（x）
x =辍学（0.5）（x）＃正则化
输出=密集（1，激活=&#39;Sigmoid&#39;）（x）
模型=模型（输入，输出）

＃阶段1：火车顶层
base_model.trainable = false
model.compile（
    优化器= ADAM（1E-3），
    损失=&#39;binary_crossentropy&#39;，
    量表= [&#39;fecycy&#39;，tf.keras.metrics.auc（name =&#39;auc&#39;），
             tf.keras.metrics.precision（name =&#39;precision&#39;），
             tf.keras.metrics.Recall（name =&#39;recember&#39;）]
）

＃用回调监视AUC的火车
早期_stop = tf.keras.callbacks.earlystopping（
    Monitor =&#39;Val_auc&#39;，耐心= 3，模式=&#39;max&#39;，详细= 1
）
检查点= tf.keras.callbacks.modelcheckpoint（
    &#39;best_model.h5&#39;，save_best_only = true，monitor =&#39;val_auc&#39;，mode =&#39;max&#39;
）

历史= model.fit（
    train_ds，
    验证_data = val_ds，
    时代= epochs，
    回调= [早期_STOP，检查点]
）

＃阶段2：微调整个模型
base_model.trainable = true
model.compile（
    优化器= ADAM（1E-5），＃非常低的学习率
    损失=&#39;binary_crossentropy&#39;，
    量表= [&#39;facer&#39;，tf.keras.metrics.auc（name =&#39;auc&#39;）]
）

history_fine = model.fit（fit）（
    train_ds，
    验证_data = val_ds，
    时代= epochs，
    onirome_epoch = history.epoch [-1]，
    回调= [早期_STOP，检查点]
）

＃ 评估
y_pred =（model.predict（val_ds）＆gt; 0.5）.astype（int）
y_true = np.concatenate（[y for _，y in val_ds]，axis = 0）

打印（f＆quot f1分数：{f1_score（y_true，y_pred）：。3f}＆quot;）
 
`]]></description>
      <guid>https://stackoverflow.com/questions/79566385/does-the-fact-that-i-have-just-train-test-split-using-keras-image-from-dataset</guid>
      <pubDate>Thu, 10 Apr 2025 10:29:08 GMT</pubDate>
    </item>
    <item>
      <title>Google文档中的自定义培训版本AI突然无法部署</title>
      <link>https://stackoverflow.com/questions/79566278/custom-trained-versions-in-google-document-ai-suddenly-failing-to-deploy</link>
      <description><![CDATA[我已经成功地在Google文档AI中成功使用了经过训练的版本。但是，最近，我所有先前功能培训的版本都停止了工作，无法部署。在此问题之前，我没有进行任何更改。
所有受过训练的版本都有同样的问题。只有验证的Google版本2.0作品。
这是内部错误消息。多数民众赞成什么也没说：
  {
  ＆quot“ name”：＆quot;
  ＆quot“ down”：是的，
  “结果”：“错误”
  ＆quot“响应＆quot”：{}，，
  “元数据：{{
    ＆quot@type; type;
    ＆quot“ commonmetadata”：{
      “状态”：“失败”
      ＆quot“ covtement”：＆quot” 2025-04-10T07：27：49.628398Z＆quot;
      ＆quot&#39;updateTime＆quort＆quot&#39;2025-04-10T07：29：51.008548Z＆quot;
      ＆quot“资源”：＆quot; quots/xxxxxxx/locations/eu/processors/xxxxxxx/processorversions/xxxxxxxx;
    }
  }，，
  ＆quot“错误＆quot”：{
    ＆quot“代码＆quot”：13，
    “消息”：“遇到内部错误。”
    “详细信息”：[]
  }
}
 ]]></description>
      <guid>https://stackoverflow.com/questions/79566278/custom-trained-versions-in-google-document-ai-suddenly-failing-to-deploy</guid>
      <pubDate>Thu, 10 Apr 2025 09:41:34 GMT</pubDate>
    </item>
    <item>
      <title>Kaggle数据集主题选择[关闭]</title>
      <link>https://stackoverflow.com/questions/79565691/kaggle-dataset-topic-selection</link>
      <description><![CDATA[ 如何选择一个主题以在Kaggle上创建数据集？ 
我有兴趣为Kaggle贡献原始数据集，但是我在选择一个有价值且相关的主题方面面临困难。我想创建一个数据集，该数据集不仅为Kaggle社区增加价值，而且有可能获得良好的可见性，下载和参与度。
我的目标不仅是上传任何数据集，而且要仔细制作解决现实世界问题的东西，还填补了现有数据的空白。
我希望从经验丰富的Kagglers或数据集创建者中获得有关他们在选择主题时如何看待的指导 - 他们问自己的问题，他们做什么研究以及他们如何验证数据集的想法值得追求。一个周到且实用的答案确实可以帮助我建立正确的心态。
我期望有一个有益的，概念的和这样的答案，可以给人以正确的方向。]]></description>
      <guid>https://stackoverflow.com/questions/79565691/kaggle-dataset-topic-selection</guid>
      <pubDate>Thu, 10 Apr 2025 03:53:33 GMT</pubDate>
    </item>
    <item>
      <title>关于在GitHub上发表的预训练模型[封闭]</title>
      <link>https://stackoverflow.com/questions/79564309/regarding-the-pre-trained-model-published-on-github</link>
      <description><![CDATA[在MIT许可证下发布了一个GITHUB项目，其中包括代码和预训练模型（已上传到Google Drive）。但是，用于培训的数据集此模型符合以下条件：

遵守本协议的条款和条件，许可人在此向您授予研究使用，无特许权使用费，非排他性，不可转让的许可，但遵守以下条件：
授权材料仅用于您的研究用途，并且在需要的基础上，对于那些与您属于同一研究机构并遵守本许可条款的直接研究同事有关。。
除备份外，许可材料不会以任何形式复制或分发。
许可材料将仅用于研究目的，并且不会以任何形式使用或包含在商业应用程序中（例如原始文件，加密文件，包含提取功能的文件，在数据集中培训的模型，其他导数作品等）。）。）。）。
任何公开的工作，无论该表格如何直接或间接地基于许可材料的任何部分，都必须包括以下参考：

鉴于这些术语，是否允许将此预先培训的模型用于商业目的？？]]></description>
      <guid>https://stackoverflow.com/questions/79564309/regarding-the-pre-trained-model-published-on-github</guid>
      <pubDate>Wed, 09 Apr 2025 12:14:25 GMT</pubDate>
    </item>
    <item>
      <title>将张量传递到tensorflow.js模型时的错误：无法读取未定义的属性（读取'Backend'）</title>
      <link>https://stackoverflow.com/questions/79564195/error-when-passing-tensor-to-tensorflow-js-model-deployed-on-node-red-typeerror</link>
      <description><![CDATA[我已经在节点红色上部署了一个深度学习模型。最初是一个keras文件，后来转换为模型。
使用node-red-contrib-tf-model我部署了该模型，但是，当我尝试将张量从.npy文件转换到其上时，会发生以下错误：
&#39;TypeError：无法阅读未定义的属性（读取“后端”）
  
以下是转换我的.npy文件的函数的代码：
  
这是调试节点输出的图片：
  
我在这里真的迷失了。我尝试了很多解决方案，例如重新安装TFJS节点，重新部署我的模型，在发生故障转换时重新串起该模型等等。 ]]></description>
      <guid>https://stackoverflow.com/questions/79564195/error-when-passing-tensor-to-tensorflow-js-model-deployed-on-node-red-typeerror</guid>
      <pubDate>Wed, 09 Apr 2025 11:10:36 GMT</pubDate>
    </item>
    <item>
      <title>自然语言处理 - 泰米尔语[关闭]</title>
      <link>https://stackoverflow.com/questions/79564180/natural-language-processing-tamil</link>
      <description><![CDATA[我正在建立一个基于语音的系统，最终用户在泰米尔语中讲话，但它们可能包括英语关键字（例如，wifi wifi;
我计划使用AI4Bharat的语音到文本模型，因为它是针对印度语言训练的，并且是免费的和开源的。来自ASR模型的输出以纯泰米尔语脚本（例如“ wifi”“ wifi”。 
✅我当前的方法：
使用AI4Bharat将泰米尔语演讲转录为文本。
标记泰米尔语输出。
检查某些目标英语单词或数字。
检测到这些动作时，请更换或触发动作。
🤔我的问题：
这是可靠地检测泰米尔语中特定英语单词或数值的好方法吗？
是否有更好或更健壮的自由和长期替代方案或这种方法的增强？
提高准确性的任何建议（例如，“ wifi”
🔧约束：
它必须免费。
它应该长期可靠地工作（无付费API或许可）。
如果可能的话，应脱机或最少的依赖性工作。]]></description>
      <guid>https://stackoverflow.com/questions/79564180/natural-language-processing-tamil</guid>
      <pubDate>Wed, 09 Apr 2025 11:03:00 GMT</pubDate>
    </item>
    <item>
      <title>为什么XGBoost不处理将数据复制到GPU（直接支持Numpy阵列或Pandas而不是像Cupy这样的库）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79562678/why-does-xgboost-do-not-handle-the-data-copying-to-gpu-a-direct-support-to-nump</link>
      <description><![CDATA[我正在使用XGBoost，以使其更快地使用GPU方法，代码变得复杂，这个问题对我来说。我对这个主题的研究还不错，我是GPU的新手。]]></description>
      <guid>https://stackoverflow.com/questions/79562678/why-does-xgboost-do-not-handle-the-data-copying-to-gpu-a-direct-support-to-nump</guid>
      <pubDate>Tue, 08 Apr 2025 17:09:01 GMT</pubDate>
    </item>
    <item>
      <title>Hfhubhttperror：403禁止：无。无法访问：https：//hf.co/api/s3proxy [闭幕]</title>
      <link>https://stackoverflow.com/questions/79562246/hfhubhttperror-403-forbidden-none-cannot-access-content-at-https-hf-co-api</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79562246/hfhubhttperror-403-forbidden-none-cannot-access-content-at-https-hf-co-api</guid>
      <pubDate>Tue, 08 Apr 2025 13:52:28 GMT</pubDate>
    </item>
    <item>
      <title>如何在Pytorch中执行全球结构化修剪</title>
      <link>https://stackoverflow.com/questions/79561884/how-to-perform-global-structured-pruning-in-pytorch</link>
      <description><![CDATA[我正在培训CIFAR10以下简单CNN 
  simplecnn（nn.module）：
    def __init __（自我）：
        super（）.__ init __（）
        self.conv1 = nn.conv2d（3，32，kernel_size = 3，padding = 1）
        self.conv2 = nn.conv2d（32，64，kernel_size = 3，padding = 1）
        self.pool = nn.maxpool2d（2，2）
        self.relu = nn.relu（）

        self.fc1 = nn.linear（64 * 8 * 8，128，bias = false）
        self.fc2 = nn.linear（128，128，bias = false）
        self.fc3 = nn.linear（128，10，bias = false）

    def向前（self，x）：
        x = self.pool（self.relu（self.conv1（x）））
        x = self.pool（self.relu（self.conv2（x）））
        x = x.View（-1，64 * 8 * 8）
        x = self.relu（self.fc1（x））
        x = self.relu（self.fc2（x））
        x = self.fc3（x）
        返回x
 
我想在网络的最后部分执行全局结构化修剪，MLP部分（SO  fc1 ， fc2 ， fc3 ），截止百分比。基本上，我想根据总连接性切断 x 神经元（以及所有相对连接）的百分比。在pytorch中，有一个函数可以执行类似的作业： ln_structrud  
 导入torch.nn.utils.prune作为修剪
prune.ln_structruct（model.fc1，name =&#39;strize&#39;，量= fraction_of_neurons_to_prune）
prune.ln_structruct（model.fc2，name =&#39;strize&#39;，量= fraction_of_neurons_to_prune）
prune.ln_structruct（model.fc3，name =&#39;strige&#39;，量= fraction_of_neurons_to_prune）
 
 ，但主要问题是，此函数将逐层而不是全球将小数应用于修剪。我希望某些东西可以截断，例如，网络的MLP部分中的80％的神经元，而不是一层80％。
是否有执行我想要的功能？我自己怎么写？在Pytorch中缺乏执行此非常常见的操作的功能对我来说似乎很奇怪，但是我找不到任何东西。]]></description>
      <guid>https://stackoverflow.com/questions/79561884/how-to-perform-global-structured-pruning-in-pytorch</guid>
      <pubDate>Tue, 08 Apr 2025 11:20:16 GMT</pubDate>
    </item>
    <item>
      <title>ML河的决策树的印刷规则</title>
      <link>https://stackoverflow.com/questions/79561495/printing-rules-from-decision-tree-in-river-ml</link>
      <description><![CDATA[我正在使用河流ML库，并使用决策树模型。我想从训练有素的树木中打印或提取决策规则。但是，看来图书馆目前没有提供内置方法来执行此操作。
如何手动从河中的树模型手动打印决策规则？
  pipeline = compose.pipeline（
        self.extract_features（事件=事件），
        ordinalencoder（），
        lastClassifier（max_depth = 5，track_error = true，remove_poor_attrs = true）
    ）
metric = f1（）
    self.state.update（{&#39;pipeline&#39;：pipeline，&#39;metric&#39;：metric}）


标签= 1如果分类_label其他0

＃检索当前管道组件
pipeline = self.state.value（）[&#39;pipeline&#39;]
模型=管道[&#39;lastClassifier&#39;]
metric = self.state.value（）[&#39;metric&#39;]

＃进行预测并更新度量
predicted_label = model.predict_one（self.state.value（）[&#39;pipeline&#39;] [&#39;dict&#39;]）
吨

＃通过新事件更新模型
model.learn_one（self.state.value（）[&#39;pipeline&#39;] [&#39;dict&#39;]，标签）

＃保存更新的状态
self.state.update（{&#39;pipeline&#39;：pipeline，&#39;metric&#39;：metric}）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79561495/printing-rules-from-decision-tree-in-river-ml</guid>
      <pubDate>Tue, 08 Apr 2025 07:59:08 GMT</pubDate>
    </item>
    <item>
      <title>使用armadillo库计算邓恩的索引的错误</title>
      <link>https://stackoverflow.com/questions/77341536/error-calculating-dunns-index-in-c-using-armadillo-library</link>
      <description><![CDATA[我一直在尝试使用Armadillo库找到我正在处理的较大算法的Dunns索引。每当我运行代码时，我都会得到一个输出 dunns索引：-Nan（ind），并且错误地说我不在索引。我提供下面的代码以及我用于测试的主要功能。该代码还具有我添加的随机检查来尝试解决问题。
  #include＆lt; iostream＆gt;
#include＆lt; armadillo＆gt;

使用名称空间性std;
使用名称空间ARMA；

double dunns（int clusters_number，const mat＆amp＆amp; distm，const uvec＆amp; ind）{
    //确定唯一群集的数量
    int i = max（ind）;
    VEC分母；

    for（int i2 = 1; i2＆lt; = i; ++ i2）{
        uvec indi = find（ind == i2）;
        uvec indj = find（ind！= i2）;

        //检查Indi和Indj是否没有空
        如果（！indi.is_empty（）＆amp;＆amp;！
            垫子温度；

            //检查索引是否在范围内提取之前
            if（indi.max（）＆lt; distm.n_rows＆amp;＆amp; indj.max（）＆lt; distm.n_cols）{
                temp = distm.submat（indi，indj）;
                分母= join_cols（分母，vectorise（temp））;
            }
            别的 {
                //调试：引起错误的打印索引
                cout＆lt;＆lt; “错误：集群的界限”索引。 ＆lt;＆lt; i2＆lt;＆lt;端
            }
        }
    }

    double num = 0.0;  //将num初始化为0.0

    //在找到最低限度之前检查分母是否没有空
    如果（！denominator.is_empty（））{
        num = min（分母）;
    }

    MAT NEG_OBS = ZEROS＆lt; MAT＆gt;（distm.n_rows，distm.n_cols）;

    for（int ix = 1; ix＆lt; = i; ++ ix）{
        uvec indxs = find（ind == ix）;

        //检查INDXS是否没有空
        如果（！indxs.is_empty（））{
            //在设置元素之前检查索引是否在范围内
            if（indxs.max（）＆lt; distm.n_rows）{
                neg_obs.submat（indxs，indxs）.fill（1.0）;
            }
        }
    }

    //打印中间值
    cout＆lt;＆lt; “中间值：” ＆lt;＆lt;端
    cout＆lt;＆lt; “分母：” ＆lt;＆lt;分母＆lt;＆lt;端
    cout＆lt;＆lt; ＆quot num：; quot; ＆lt;＆lt; num＆lt;＆lt;端

    MAT DEM = neg_obs％distm;
    double max_dem = max（max（dem））;

    //打印max_dem
    cout＆lt;＆lt; ＆quot&#39;max_dem：＆quot; ＆lt;＆lt; max_dem＆lt;＆lt;端

    double di = num / max_dem;
    返回di;
}

int main（）{
    //用于测试的新输入
    int clusters_number = 2;

    //修改的差异矩阵（4x4）
    MAT DISTM（4，4）;
    distm＆lt;＆lt; 0.0＆lt;＆lt; 1.0＆lt;＆lt; 2.0＆lt;＆lt; 3.0
        ＆lt;＆lt; 1.0＆lt;＆lt; 0.0＆lt;＆lt; 1.0＆lt;＆lt; 2.0
        ＆lt;＆lt; 2.0＆lt;＆lt; 1.0＆lt;＆lt; 0.0＆lt;＆lt; 1.0
        ＆lt;＆lt; 3.0＆lt;＆lt; 2.0＆lt;＆lt; 1.0＆lt;＆lt; 0.0;

    //修改的群集指数（4x1）
    arma :: uvec ind;
    ind＆lt;＆lt; 1＆lt;＆lt; 1＆lt;＆lt; 2＆lt;＆lt; 2;

    //打印输入差异矩阵
    cout＆lt;＆lt; “差异矩阵：” ＆lt;＆lt;端
    cout＆lt;＆lt; distm＆lt;＆lt;端

    //打印群集索引
    cout＆lt;＆lt; “集群索引：” ＆lt;＆lt;端
    cout＆lt;＆lt; ind＆lt;＆lt;端

    double di = dunns（clusters_number，distm，ind）;

    cout＆lt;＆lt; ＆quot“邓恩的索引：” ＆lt;＆lt; di＆lt;＆lt;端

    返回0;
}
 
数据格式似乎正确。我正在使用Double用于差异矩阵和ARMA :: UVEC用于集群指数，这是合适的。
数据对齐：差异矩阵和群集指数中数据点的对齐似乎是正确的。矩阵中的每个数据点对应于集群指数中的条目。
似乎在差异矩阵中似乎没有任何空簇或缺少数据点。数据似乎已经完成。
鉴于数据似乎正确对齐，并且没有空的群集或丢失的数据没有明显的问题，因此我仍然遇到“界限”的“索引”很困惑。在下提取过程中的错误。]]></description>
      <guid>https://stackoverflow.com/questions/77341536/error-calculating-dunns-index-in-c-using-armadillo-library</guid>
      <pubDate>Sun, 22 Oct 2023 20:09:58 GMT</pubDate>
    </item>
    <item>
      <title>模块“ numpy”没有属性“ machar”？</title>
      <link>https://stackoverflow.com/questions/75371176/module-numpy-has-no-attribute-machar</link>
      <description><![CDATA[我有一个问题。当我从“ statsmodels.stats.outliers_influence&#39;导入variance_inflation_factor”时，我得到&#39;module&#39;numpy&#39;没有属性&#39;machar&#39;&#39;错误，原因是什么？
我曾经在项目中执行此代码，并且它没有任何问题，但是它为后续项目提供了此错误]]></description>
      <guid>https://stackoverflow.com/questions/75371176/module-numpy-has-no-attribute-machar</guid>
      <pubDate>Tue, 07 Feb 2023 09:17:47 GMT</pubDate>
    </item>
    <item>
      <title>pytorch，typeerror：object（）无参数</title>
      <link>https://stackoverflow.com/questions/43781526/pytorch-typeerror-object-takes-no-parameters</link>
      <description><![CDATA[这可能是一个初学者的问题，但是尽管如此：在使用pytorch构建图像分类器时，我会得到此错误：
  trackback（最近的最新通话）：
文件“/pytorch/kanji_torch.py​​”，第47行，in＆lt; module＆gt;
    网络=网络（）
  文件“/pytorch/kanji_torch.py​​”，第113行，in __init__
    self.conv1 = nn.conv2d（1，32，5）
  文件“/python3.5/site-packages/torch/nn/modules/conv.py”，第233行，in __init__
    false，_pair（0），群体，偏见）
  文件“/python3.5/site-packages/torch/nn/modules/conv.py”，第32行，in __init__
    out_channels，in_channels //组， *kernel_size）））
TypeError：Object（）无参数
 
我这样定义网络类：
 类网络（Torch.nn.Module）：
    def __init __（自我）：
        超级（网络，self）.__ Init __（）
        self.conv1 = nn.conv2d（1，32，5）
        self.pool = nn.maxpool2d（2，2）
        self.conv2 = nn.conv2d（32，64，5）
        self.pool2 = nn.maxpool2d（2，2）
        self.conv3 = nn.conv2d（64，64，5）
        self.pool2 = nn.maxpool2d（2，2）
        self.fc1 = nn.linear（64 * 5 * 5，512）
        self.fc2 = nn.linear（512，640）
        self.fc3 = nn.linear（640，3756）
 
可以肯定的是，我正确地导入了所有相关的Pytorch库模块。 
（导入Torch.nn为NN和
导入火炬）
关于我做错了什么的想法？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/43781526/pytorch-typeerror-object-takes-no-parameters</guid>
      <pubDate>Thu, 04 May 2017 11:24:01 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn.decomposition.pca的特征向量的简单图</title>
      <link>https://stackoverflow.com/questions/37976564/simple-plots-of-eigenvectors-for-sklearn-decomposition-pca</link>
      <description><![CDATA[我正在尝试了解主组件分析的工作方式，并且我正在 sklearn.datasets.load_iris  dataset上对其进行测试。  我了解每个步骤的工作原理（例如，使用 k 选定的尺寸，将数据，协方差，特征分类，对最高特征值进行排序，将原始数据转换为新轴）。）。
下一步是可视化这些 eigenVectors 在数据集中投射到（在 pc1 vs. pc2 vs. pc2 plot 上，对吗？）。。
如何在还原尺寸数据集的3D图上绘制[PC1，PC2，PC3]特征向量？
另外，我是否正确绘制了此2D版本？我不确定为什么我的第一个特征向量的长度较短。  我应该乘以特征值吗？

 这是我为实现这一目标所做的一些研究： 
我关注的PCA方法来自：
 https://plot.ly/ipython-notebooks/principal-component-analysis/#shortcut--pca-in-scikit-learn （尽管我不想使用 Plotly 我想坚持使用。
我一直在关注本教程，以绘制特征向量，这似乎很简单： pca for matplotlib 基本示例
我找到了这个，但是我试图做的事情似乎过于复杂，我不想创建一个 fancyarrowpatch ： 的协方差矩阵

 我试图使我的代码尽可能直接地遵循其他教程： 
 导入numpy作为NP
导入大熊猫作为pd
导入matplotlib.pyplot作为PLT
来自sklearn.datasets import load_iris
从sklearn.prepercorsing进口标准标准
来自Sklearn进口分解
进口海洋为SNS； sns.set_style（&#39;whitegrid＆quort; {&#39;axes.grid&#39;：false}）

％matplotlib内联
np.random.seed（0）

＃虹膜数据集
df_data = pd.dataframe（load_iris（）。数据， 
                       index = [＆quot; iris_％d＆quot; ％i for i在范围内（load_iris（）。data.shape [0]）]，
                       列= load_iris（）。feature_names）

se_targets = pd.Series（load_iris（）。目标， 
                       index = [＆quot; iris_％d＆quot; ％i for i在范围内（load_iris（）。data.shape [0]）]， 
                       名称=“物种”

＃缩放平均= 0，var = 1
df_standard = pd.dataframe（standardscaler（）。fit_transform（df_data）， 
                           index = df_data.index，
                           列= df_data.columns）

＃用于主要组合分析的Sklearn

＃昏暗
m = df_standard.shape [1]
k = 2

＃PCA（我倾向于设置它）
m_pca = demomposition.pca（n_components = m）
df_pca = pd.dataframe（m_pca.fit_transform（df_standard）， 
                列= [＆quot; pc％d＆quot;范围内K的％k（1，m + 1）]）。iloc [：，：k]


＃绘制特征向量
#https：//stackoverflow.com/questions/18299523/basic-example-for-pca-with-matplotlib

＃这是东西很奇怪的地方...
data = df_standard

mu = data.mean（轴= 0）
eigenVector，eigenvalues = m_pca.components_，m_pca.explained_variance_ #eigenVectors，eigenvalues，v = np.linalg.svd（data.t，fult_matrices = false）
Projected_data = df_pca＃np.dot（数据，特征向量）

sigma = Projected_data.std（axis = 0）.mean（）

图，ax = plt.subplots（figsize =（10,10））
ax.Scatter（Projected_data [＆quot; pc1;]，Projected_data [＆quot; pc2＆quot;]）
对于轴，zip中的颜色（eigenVectors [：k]，[red&#39;&#39;
＃开始，end = mu，mu + sigma *轴###导致“ valueerror：太多值无法打开（预期2）”

    ＃所以我尝试过，但我认为这是不对的
    start，end =（mu）[：k]，（Mu + Sigma * Axis）[：K] 
    ax.annotate（&#39;&#39;，xy = end，xytext = start，arrowprops = dict（faceColor = color，width = 1.0））
    
ax.set_aspect（&#39;quare&#39;）
plt.show（）
 
  &lt;img alt =“在此处输入图像描述” src =“ https://i.sstatic.net.net/t9st0.png”]]></description>
      <guid>https://stackoverflow.com/questions/37976564/simple-plots-of-eigenvectors-for-sklearn-decomposition-pca</guid>
      <pubDate>Wed, 22 Jun 2016 19:20:15 GMT</pubDate>
    </item>
    <item>
      <title>自然语言处理模型[关闭]</title>
      <link>https://stackoverflow.com/questions/7945130/natural-language-processing-model</link>
      <description><![CDATA[我正在制作一个项目来解析，并了解用户英语的输入线的意图。
这是我认为我应该做的：

使用POS标记＆amp;创建句子的文字；每个句子都有明显的意图。
创建模型说：决策树并在上述句子上训练它。
尝试用户输入上的模型：
在用户输入句子上进行基本的令牌和pos标记，并在上面的模型上对其进行测试。

这一切可能完全是错误的或愚蠢的，但我决心学习如何做。我不想使用现成的解决方案，编程语言也不关心。
你们将如何完成这项任务？选择哪种型号，为什么？通常要制作NLP解析器，完成了哪些步骤。]]></description>
      <guid>https://stackoverflow.com/questions/7945130/natural-language-processing-model</guid>
      <pubDate>Sun, 30 Oct 2011 13:20:54 GMT</pubDate>
    </item>
    </channel>
</rss>