<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 28 May 2024 09:15:48 GMT</lastBuildDate>
    <item>
      <title>截断标记LM</title>
      <link>https://stackoverflow.com/questions/78542847/truncation-marckuplm</link>
      <description><![CDATA[我在使用 marckupLM 时遇到困难，我想知道 MarkupLMProcessor 是否有办法考虑因数据本身而被截断的信息。我的问题来自于需要比我必须截断的数据更大的数据，但我需要模型考虑为标记分类而截断的信息。
我希望有一些参数可以促进不同大小的数据，但在我的研究过程中我还没有找到它。我想知道我是否真的必须手工填写，或者是否有某种方法。]]></description>
      <guid>https://stackoverflow.com/questions/78542847/truncation-marckuplm</guid>
      <pubDate>Tue, 28 May 2024 08:29:01 GMT</pubDate>
    </item>
    <item>
      <title>如何聚合 DNA 序列数据 [关闭]</title>
      <link>https://stackoverflow.com/questions/78542443/how-to-aggregate-dna-sequence-data</link>
      <description><![CDATA[数据文件：Task1-serious data.zip，包含10,000,000个样本，每个样本一行，第一个样本编号，剩余64个样本特征。数据来自DNA序列
簇数最大为1000000，如果超过1000000，超出的簇将合并到第1000000个簇中
例如在此处输入图片描述
什么聚类方案是最好的以及我们应该如何做]]></description>
      <guid>https://stackoverflow.com/questions/78542443/how-to-aggregate-dna-sequence-data</guid>
      <pubDate>Tue, 28 May 2024 07:13:48 GMT</pubDate>
    </item>
    <item>
      <title>微调模型时内存不足</title>
      <link>https://stackoverflow.com/questions/78542429/running-out-of-ram-when-finetuning-model</link>
      <description><![CDATA[我目前正在尝试从以下位置微调 Wav2Vec2 模型：https://huggingface。 co/dima806/bird_sounds_classification。但我的 RAM 使用率超过了 Google Colab 上的免费套餐。
以下是我的代码：
从 Transformers 导入 TrainingArguments、Trainer

# 使用ignore_mismatched_sizes=True加载模型
模型 = Wav2Vec2ForSequenceClassification.from_pretrained(
    “dima806/bird_sounds_classification”，
    num_labels=len(label2id),
    ignore_mismatched_sizes=True
）

# 设置梯度累积训练
batch_size = 1 # 减少批量大小以管理内存
accumulation_steps = 4 # 累积 4 个步骤的梯度

训练参数 = 训练参数（
    输出目录=“./结果”,
    evaluation_strategy=“纪元”，
    学习率=2e-5,
    per_device_train_batch_size=batch_size，
    per_device_eval_batch_size=batch_size，
    gradient_accumulation_steps=accumulation_steps, # 梯度累积
    num_train_epochs=3,
    权重衰减=0.01，
    fp16=True, # 启用混合精度训练
）

教练=教练（
    型号=型号，
    参数=训练参数，
    训练数据集=训练数据集，
    eval_dataset = val_dataset，
    分词器=特征提取器，
）

# 训练模型
训练师.train()

RAM 超过 12.7GB 的原因可能是什么？我的数据集仅包含 20 个项目。我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78542429/running-out-of-ram-when-finetuning-model</guid>
      <pubDate>Tue, 28 May 2024 07:10:21 GMT</pubDate>
    </item>
    <item>
      <title>如何构建一个中介代理来调节两个聊天机器人之间的交互？</title>
      <link>https://stackoverflow.com/questions/78542204/how-to-build-an-intermediary-agent-to-regulate-interactions-between-two-chatbots</link>
      <description><![CDATA[正文：
我正在开展一个项目，开发一个代理，该代理将管理顾问聊天机器人和医生聊天机器人之间的交互，以便为用户提供治疗会话。
聊天机器人：
顾问聊天机器人：此聊天机器人收集所有用户信息，包括聊天记录、评估和个人资料数据，以生成治疗建议和计划。
医生聊天机器人：此聊天机器人接受顾问聊天机器人的建议，并根据这些建议与用户进行治疗会话。
目标：
我想创建一个中介代理，充当这两个聊天机器人之间的中介。该代理应该：
从顾问聊天机器人的响应中获取各个要点。
指示医生聊天机器人从一个点开始，并为其分配特定的时间（例如，X 分钟）。
根据顾问聊天机器人的建议，连续引导医生聊天机器人按顺序完成每个点。
详细信息：
两个聊天机器人都是使用大型语言模型 (LLM) 实现的。
有一个用于用户交互的前端应用程序。
主要的挑战是确保医生聊天机器人不断受到中介代理的指导和监控。
问题：
如何构建这个中介代理来有效地调节和排序顾问聊天机器人和医生聊天机器人之间的交互？任何关于架构、实施方法或要使用的特定技术的建议都将不胜感激。
我尝试过的方法：
基于规则的基本方法：
我首先创建了一个简单的基于规则的中介代理，它可以解析顾问聊天机器人的建议并相应地指导医生聊天机器人。
代理使用一组预定义的规则来对治疗任务进行排序并为每个步骤分配时间。
我编写了一些初始代码来处理两个聊天机器人之间的基本交互流程。
手动监控：
我手动监控聊天机器人之间的交互，以确保医生聊天机器人遵循中介代理的指示。
这涉及在每次任务完成后手动检查医生聊天机器人的状态和进度。
我的期望：
我希望中介代理能够无缝管理治疗任务的顺序并处理时间，而无需不断进行人工干预。
我预计医生聊天机器人将能够遵循中介代理的指示并顺利执行治疗课程。
遇到的问题：
排序的复杂性：随着治疗步骤数量的增加，基于规则的方法很快变得复杂且难以管理。
实时监控：手动监控和调整治疗课程不可扩展且容易出错。
集成挑战：确保两个聊天机器人之间的顺畅通信和同步比预期的更具挑战性。]]></description>
      <guid>https://stackoverflow.com/questions/78542204/how-to-build-an-intermediary-agent-to-regulate-interactions-between-two-chatbots</guid>
      <pubDate>Tue, 28 May 2024 06:11:51 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“keras.wrappers”的模块</title>
      <link>https://stackoverflow.com/questions/78541790/modulenotfounderror-no-module-named-keras-wrappers</link>
      <description><![CDATA[我正在尝试使用 GridSearchCV 进行机器学习分类任务，但是我不断收到此行错误，有什么建议吗？
from keras.wrappers.scikit_learn import KerasClassifier

我的tensorflow版本是2.16.1，我的keras版本是3.3.3
我尝试过以下方法：
pip install scikeras

从 scikeras.wrappers 导入 KerasClassifier

并收到“导入错误：无法从“sklearn.utils.deprecation”导入名称“_deprecate_Xt_in_inverse_transform””
如有任何帮助，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78541790/modulenotfounderror-no-module-named-keras-wrappers</guid>
      <pubDate>Tue, 28 May 2024 03:17:40 GMT</pubDate>
    </item>
    <item>
      <title>如何标记像 COCOA 这样的非模态数据集？</title>
      <link>https://stackoverflow.com/questions/78541723/how-to-label-an-amodal-dataset-like-cocoa</link>
      <description><![CDATA[我们经常通过 LabelMe 使用 COCO 数据集进行实例分割，最近，模态实例分割变得很流行，因为它可以感知遮挡的对象。有一些公共的非模态数据集，例如 COCOA 和 KINS，但如何标记它们对我来说并不清楚。我要标记我自己的amodal数据集，我应该怎么做？我尝试通过LabelMe来标记它们，但是LableMe没有参数，例如occlude_rate，visible_mask，invisible_mask，
这是 COCOA 数据集 json flie
在此处输入图片说明
这是 COCO 数据集 json flie
在此处输入图片描述
我有 COCO 格式的数据集，如何将它们标记为像 COCOA 和 KINS 这样的模态数据集？]]></description>
      <guid>https://stackoverflow.com/questions/78541723/how-to-label-an-amodal-dataset-like-cocoa</guid>
      <pubDate>Tue, 28 May 2024 02:44:02 GMT</pubDate>
    </item>
    <item>
      <title>有没有可以根据文档布局自动标记 PDF 表单字段的工具？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78541705/any-tool-to-label-pdf-form-fields-automatically-based-on-document-layout</link>
      <description><![CDATA[我正在寻找一种工具，可以接受带有可填写表单字段的 PDF，然后根据文档的布局检测标签和部分。
例如，在下面的文档中，以红色突出显示的表单字段应具有标签：“投保人详细信息 -&gt;”全名”。蓝色表格字段应有标签：“驾驶或负责车辆的人员（需填写，即使停放）-&gt;”业务”。

否则，如果不存在这样的工具，我如何开始训练可以实现这一目标的模型？有没有我可以训练的基础模型？我对机器学习还很陌生。]]></description>
      <guid>https://stackoverflow.com/questions/78541705/any-tool-to-label-pdf-form-fields-automatically-based-on-document-layout</guid>
      <pubDate>Tue, 28 May 2024 02:32:31 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Kaggle 上以 .h5 格式保存深度学习模型</title>
      <link>https://stackoverflow.com/questions/78541201/unable-to-save-deep-learning-model-in-h5-format-on-kaggle</link>
      <description><![CDATA[我在尝试将深度学习模型以 .h5 格式保存在 Kaggle 上时遇到问题。尽管遵循了标准程序，但保存过程始终失败。在此处输入图片说明我已添加代码和面临的问题。
在此处输入图片说明
我将格式指定为 .keras，但模型无法保存。但是，代码在 Google Colab 上运行良好。不幸的是，Google Colab 的内存不足以有效运行我的代码。
任何解决此问题并确保在 Kaggle 平台上成功以 .h5 格式保存我的模型的见解或潜在解决方案都将对我非常有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78541201/unable-to-save-deep-learning-model-in-h5-format-on-kaggle</guid>
      <pubDate>Mon, 27 May 2024 21:49:55 GMT</pubDate>
    </item>
    <item>
      <title>即使经过 500 个 epoch，结果也没有改善 [关闭]</title>
      <link>https://stackoverflow.com/questions/78540839/results-not-improving-even-after-500-epochs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78540839/results-not-improving-even-after-500-epochs</guid>
      <pubDate>Mon, 27 May 2024 19:40:59 GMT</pubDate>
    </item>
    <item>
      <title>层“dense_4”的输入 0 与该层不兼容：预期输入形状的轴 -1 值为 1，但收到的输入形状为（无，6）</title>
      <link>https://stackoverflow.com/questions/78538382/input-0-of-layer-dense-4-is-incompatible-with-the-layer-expected-axis-1-of-i</link>
      <description><![CDATA[我正在尝试实现多元回归模型。
使用以下代码：
all_normalizer = keras.layers.Normalization(input_shape=(1, ), axis=-1)
all_normalizer.adapt(x_train_all)

nn_model = tf.keras.Sequential([
    all_正规化器，
    tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(1)
]）

nn_model.compile(keras.optimizers.Adam(learning_rate=0.001), loss=&#39;mean_squared_error&#39;)

历史记录 = nn_model.fit(
    x_train_Temp,y_train_Temp,
    验证数据=（x_val_Temp，y_val_Temp），
    详细 = 0，纪元 = 100
）

我收到以下错误：
 文件“C:\~ai.py”，第 330 行，在  中
    历史记录 = nn_model.fit(
              ^^^^^^^^^^^^^^
  文件“C:\~\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
    从 None 引发 e.with_traceback(filtered_tb)
  文件“C:\~PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\input_spec.py”，第 227 行，位于assert_input_compatibility
    引发值错误（
ValueError：调用 Sequential.call() 时遇到异常。

层“dense_4”的输入0与图层不兼容：预期输入形状的轴 -1 值为 1，但收到的输入形状为（无，6）

Sequential.call() 收到的参数：
  输入=tf.Tensor（形状=（无，1），dtype=float32）
  • 训练=真
  • 掩码=无

我应该在代码中更改/实现什么来解决此错误？
PS：我是初学者，可能不懂东西，所以请不要嫌弃。]]></description>
      <guid>https://stackoverflow.com/questions/78538382/input-0-of-layer-dense-4-is-incompatible-with-the-layer-expected-axis-1-of-i</guid>
      <pubDate>Mon, 27 May 2024 09:54:27 GMT</pubDate>
    </item>
    <item>
      <title>如何在暗网中预测多图像txt？</title>
      <link>https://stackoverflow.com/questions/78533567/how-to-predict-multi-image-txt-in-darknet</link>
      <description><![CDATA[在 Yolov7 中，我使用此代码来测试整个文件夹图像：
python test.py --save-txt --data data/coco.yaml --save-conf --conf 0.1 --weights yolov7_20240316best.pt --task test --name 0316conf01

现在我需要在yolov4中预测test.txt（包括所有图像路径）。
我尝试了这个命令，但没有成功：
暗网探测器测试数据/obj.data cfg/yolo-obj.cfg backup/yolo-obj_best.weights -thresh 0.9 -dont_show data/test.txt result.txt
]]></description>
      <guid>https://stackoverflow.com/questions/78533567/how-to-predict-multi-image-txt-in-darknet</guid>
      <pubDate>Sat, 25 May 2024 19:51:00 GMT</pubDate>
    </item>
    <item>
      <title>无法在 ML.net 中加载动态数据</title>
      <link>https://stackoverflow.com/questions/78528640/unable-to-load-dynamic-data-in-ml-net</link>
      <description><![CDATA[我的输入数据为List&gt;
我想将此数据作为 Enumerable 加载到 MLContext
var mlContext = new MLContext();

mlContext.Data.LoadFromEnumerable(inputData);

我也无法为输入数据预定义一个强静态类，因为每次数据列都会发生变化
在 dataView 中，加载时我得到的列架构为 0
在转换时，我没有发现任何列异常
我尝试过创建动态强类型类
生成的架构定义并加载数据
我想使用加载的数据转换列]]></description>
      <guid>https://stackoverflow.com/questions/78528640/unable-to-load-dynamic-data-in-ml-net</guid>
      <pubDate>Fri, 24 May 2024 12:43:35 GMT</pubDate>
    </item>
    <item>
      <title>UNET预测</title>
      <link>https://stackoverflow.com/questions/70761109/unet-prediction</link>
      <description><![CDATA[我使用unet进行图像分割，我的问题是下面的代码是什么意思
test_img_norm=test_img[:,:,0][:,:,无] 

和
prediction_other = (model.predict(test_img_other_input)[0,:,:,0] &gt; 0.2).astype(np.uint8)
]]></description>
      <guid>https://stackoverflow.com/questions/70761109/unet-prediction</guid>
      <pubDate>Tue, 18 Jan 2022 19:09:02 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程置信度与可信区间</title>
      <link>https://stackoverflow.com/questions/60560152/gaussian-process-confidence-vs-credible-intervals</link>
      <description><![CDATA[由于高斯过程返回分布而不是点估计，为什么会这样示例（实际上在 GP 的每个示例中）谈论贝叶斯统计类似物的置信区间“可信区间” ？
更新
一个建议（不是来自我）是他们将它们称为置信区间，因为他们使用最大似然法而不是使用完整的贝叶斯方法 - 我对此不相信，因为经验贝叶斯方法也可以提供可信的区间]]></description>
      <guid>https://stackoverflow.com/questions/60560152/gaussian-process-confidence-vs-credible-intervals</guid>
      <pubDate>Fri, 06 Mar 2020 08:27:27 GMT</pubDate>
    </item>
    <item>
      <title>数据挖掘方面 R 与 Matlab 的比较 [关闭]</title>
      <link>https://stackoverflow.com/questions/4811995/comparing-r-to-matlab-for-data-mining</link>
      <description><![CDATA[我最近开始学习 R，而不是开始用 Matlab 编码，主要是因为它是开源的。我目前从事数据挖掘和机器学习领域。我发现许多机器学习算法都是用 R 实现的，我仍在探索用 R 实现的不同软件包。
我有一个快速问题：您如何比较 R 和 Matlab 在数据挖掘应用方面的差异，它的受欢迎程度、优缺点、行业和学术接受度等？您会选择哪一个，为什么？
我根据各种指标对 Matlab 和 R 进行了各种比较，但我特别感兴趣的是它在数据挖掘和机器学习中的适用性。
由于这两种语言对我来说都很新，我只是想知道 R 是否是一个好的选择。
我很感激任何建议。]]></description>
      <guid>https://stackoverflow.com/questions/4811995/comparing-r-to-matlab-for-data-mining</guid>
      <pubDate>Thu, 27 Jan 2011 01:04:05 GMT</pubDate>
    </item>
    </channel>
</rss>