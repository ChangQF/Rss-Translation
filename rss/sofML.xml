<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 09 Feb 2024 21:12:19 GMT</lastBuildDate>
    <item>
      <title>LLM 的数据访问已损坏。想法？</title>
      <link>https://stackoverflow.com/questions/77970854/data-access-for-llms-is-broken-thoughts</link>
      <description><![CDATA[为了构建真正有用的 GenAI 应用程序，法学硕士需要能够从结构化和非结构化数据源访问专有数据。法学硕士很难了解检索相关数据的可用性、位置和方法。
关键问题：

法学硕士无法确定是否有必要的数据可用于回答任何数据源中的用户查询。
如果数据可用，法学硕士无法找到要从哪个数据源检索。
如果来源已知，为众多检索协议编写检索管道就会变得复杂、重复且不确定。

您在项目中遇到过这些问题吗？您发现哪些有效的变通方法或解决方案？有什么新工具或实践可以简化法学硕士与不同数据源的集成吗？
一些人建议微调或 RAG 作为潜在的解决方案，但是：

使用特定数据微调模型的成本很高，而且会随着最新数据而过时，并且存在内置的访问控制问题。持续的再培训成本高昂，而且跟不上实时数据变化的步伐。
RAG 不适用于结构化数据。大多数有用的数据都是结构化的，可以分布在多个数据源中，每个数据源都有自己的查询机制。
编写单独的检索管道是有限且复杂的，并且不能确保确定性、安全性和访问控制。
]]></description>
      <guid>https://stackoverflow.com/questions/77970854/data-access-for-llms-is-broken-thoughts</guid>
      <pubDate>Fri, 09 Feb 2024 21:01:59 GMT</pubDate>
    </item>
    <item>
      <title>R 机器学习中 Ranger 模型的错误</title>
      <link>https://stackoverflow.com/questions/77970324/an-error-in-ranger-model-in-machine-learning-in-r</link>
      <description><![CDATA[我正在运行生存模型的机器学习代码。我的 pred_prob 代码有错误。谁能帮我？先感谢您
我的错误是：
&lt;代码&gt;&gt; pred_prob &lt;- rowMeans(ranger_predict$train_data[, 1:dim(ranger_predict$train_data)[2]])
h(simpleError(msg, call)) 中的错误：
  在为函数“rowMeans”选择方法时评估参数“x”时出错：$ 运算符对于原子向量无效

我的代码是：
图书馆（护林员）# RSF
Library(survival) # 包含生存示例，处理生存对象
Library(caret) # 用于分层交叉验证
库(dplyr) # 数据操作
图书馆（佩奇）
# 加载数据集，其中Time是事件发生的时间，Event是事件发生的时间

库（readxl）
df2 &lt;- read_excel(“E:/ME/BS DATA/数据 BS.xlsx”)

# 改变状态变量的标签
df2 &lt;- df2 %&gt;% mutate(status = time_15year-1) # 0 = 审查，1 = 死亡
# 一些数据包含缺失值，为了简单起见，我省略了对 NA 的观察
df2 &lt;- na.omit(df2)
# 缩放至月
df2$time_15year &lt;- 楼层(df2$time_15year)
par(“三月”)
par(mar=c(.01,.01,.01,.01))
pairs(df2 %&gt;% dplyr::select(time_15year,BS_death), main = “NCCTG 脑中风数据”)
# 交叉验证，对状态变量进行分层，以确保每个组（此处已审查，已死亡）
# 均匀分布在交叉验证折叠上
折叠 &lt;- 2 # 表示 交叉验证
cvIndex &lt;- createFolds(factor(df2$BS_death), Folds, returnTrain = T)
#2 模型，训练
# 创建一些容器来存储结果
# （对于大模型来说不合理，对于大模型你可能需要将中间结果存储在磁盘上）
容器模型&lt;-向量（“列表”，长度（cvIndex））
容器_pred &lt;- 容器_模型

# 定义训练/测试数据
for (i in 1:length(cvIndex)) {
  train_data &lt;- df2[cvIndex[[i]], ]
  # 其余代码
}

测试数据 &lt;- df2[-cvIndex[[i]],]

train_data &lt;- train_data[complete.cases(train_data), ]
测试数据 &lt;- 测试数据[完整.案例(测试数据), ]

rangermodel &lt;- ranger(Surv(time_15year, BS_death) ~ 年龄 + 性别 + edu + 地点 + cvahis+ mihis + bphis +heartdis + smok + Pastsmok+ 被动+活动 +waterpip +cvatype, 数据 = train_data)

情节（rangermodel$unique.death.times，rangermodel$survival[1，]）
ranger_predict &lt;- 预测（rangermodel，数据=testing_data）
str(ranger_预测)
ranger_predict &lt;- unlist(ranger_predict)
pred_prob &lt;- rowMeans(ranger_predict$train_data[, 1:dim(ranger_predict$train_data)[2]])
pred_prob[pred_prob&gt;中位数(pred_prob)]=1
pred_prob[pred_prob&lt;=中位数(pred_prob)]=0
表（pred_prob）
fusionMatrix(as.factor(testing_data$BS_death), as.factor(pred_prob))
]]></description>
      <guid>https://stackoverflow.com/questions/77970324/an-error-in-ranger-model-in-machine-learning-in-r</guid>
      <pubDate>Fri, 09 Feb 2024 19:06:49 GMT</pubDate>
    </item>
    <item>
      <title>'{{nodeequential_15/conv2d_26/Conv2D}} = Conv2D[T=DT_FLOAT] 的 1 减 3 导致的负维度大小</title>
      <link>https://stackoverflow.com/questions/77969971/negative-dimension-size-caused-by-subtracting-3-from-1-for-node-sequential-15</link>
      <description><![CDATA[尝试 model.fit 后：
hist = model.fit(train, epochs=15,validation_data=val,callbacks=[tensorboard_callback])

我收到错误：
ValueError Traceback（最近一次调用最后一次）
单元格位于\[132\]，第 1 行
\----\&gt; 1 hist = model.fit（train，epochs = 15，validation_data = val，callbacks = \ [tensorboard_callback \]）#mudei epocas pra 3，antesera 20

文件 \~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\utils\traceback_utils.py :70，在filter_traceback中。\.error_handler(\*args, \*\*kwargs)
67 过滤_tb = \_process_traceback_frames(e.__traceback__)
68 # 要获取完整的堆栈跟踪，请调用：
69 # `tf.debugging.disable_traceback_filtering()`
\---\&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
71 最后：
72 删除filtered_tb

文件 \~\\AppData\\Local\\Temp\__autograph_ generated_filea6t43riq.py:15，位于outer_factory.\.inner_factory.\.tf__train_function(iterator)
13 尝试：
14 do_return =真
\---\&gt; 15 retval_ = ag_\_.converted_call(ag_\_.ld(step_function), (ag_\_.ld(self), ag_\_.ld(迭代器)), 无, fscope)
16 除外：
17 do_return = 假

ValueError：在用户代码中：

    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1338 行，位于训练函数*
        返回step_function（自身，迭代器）
    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1322 行，位于步骤函数 **
        输出 = model.distribute_strategy.run(run_step, args=(data,))
    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1303 行，位于运行步骤**
        输出 = model.train_step(数据)
    文件“C:\Users\Eenon\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\keras\src\engine\training.py”，第 1080 行，位于训练步骤
        y_pred = self(x, 训练=True)
    文件“C：\ Users \ Enenon \ AppData \ Local \ Packages \ PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0 \ LocalCache \ local-packages \ Python310 \ site-packages \ keras \ src \ utils \traceback_utils.py”，第70行，位于错误处理程序
        从 None 引发 e.with_traceback(filtered_tb)
    
    ValueError：调用层“conv2d_26”（类型 Conv2D）时遇到异常。
    
    &#39;{{nodeequential_15/conv2d_26/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=“NHWC”, dilations=[1, 1, 1, 1],explicit_paddings=[ 1 减 3 导致的负维度大小], padding=“VALID”, strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_15/Cast,equential_15/conv2d_26/Conv2D/ReadVariableOp)&#39; 输入形状：[64,64,1, 1]、[3,3,1,32]。
    
    调用层“conv2d_26”接收的参数（类型 Conv2D）：
      • 输入=tf.Tensor(形状=(64, 64, 1, 1), dtype=float32)`

我的代码：
&lt;前&gt;&lt;代码&gt;模型 = 顺序()
model.add(Conv2D(32, (3,3), 激活=&#39;relu&#39;, input_shape=(64,64,1)))
model.add(MaxPooling2D())
model.add(Conv2D(16, (3,3), 激活=&#39;relu&#39;, input_shape=(64,64,1)))
model.add(MaxPooling2D())
模型.add(压平())
model.add（密集（64，激活=&#39;relu&#39;））
model.add（密集（3，激活=&#39;sigmoid&#39;））

model.compile(&#39;adam&#39;,loss=&#39;sparse_categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
logdir=&#39;日志&#39;
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

hist = model.fit(train, epochs=15,validation_data=val,callbacks=[tensorboard_callback])

我正在使用 64,64,1 数据集。这是将张量从 64,64 修改为 64,64,1 并从张量创建数据集的代码：
x = x.reshape(-1, 64, 64, 1)
标签 = tf.constant(y)
特征 = tf.constant(x)
datax = tf.data.Dataset.from_tensor_slices(特征)
datay = tf.data.Dataset.from_tensor_slices(标签)
数据 = tf.data.Dataset.zip((datax,datay))
data_iterator = data.as_numpy_iterator()
批处理 = data_iterator.next()
]]></description>
      <guid>https://stackoverflow.com/questions/77969971/negative-dimension-size-caused-by-subtracting-3-from-1-for-node-sequential-15</guid>
      <pubDate>Fri, 09 Feb 2024 17:51:53 GMT</pubDate>
    </item>
    <item>
      <title>要使应用程序能够理解加载的非结构化文档（例如，不规则表格）中的信息，我需要了解什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77969909/what-do-i-need-to-know-to-make-an-application-that-understands-the-information-i</link>
      <description><![CDATA[例如
不规则表格
一种能够理解文档中的文本内容并可能将信息分离为键：值的技术。我还是一名大学生，我想在 NLP 领域取得进步，但我找不到从哪里开始。
我应该学习哪些 NLP 技术，路线图应该是什么样的？]]></description>
      <guid>https://stackoverflow.com/questions/77969909/what-do-i-need-to-know-to-make-an-application-that-understands-the-information-i</guid>
      <pubDate>Fri, 09 Feb 2024 17:36:06 GMT</pubDate>
    </item>
    <item>
      <title>如何创建目标变量[关闭]</title>
      <link>https://stackoverflow.com/questions/77969840/how-to-create-target-variable</link>
      <description><![CDATA[我尝试预测 ncaa 篮球疯狂游行的每个结果。
我有过去 20 场左右锦标赛的历史数据，并且有 team1_score 和 team2_score 等列。我认为创建一个目标变量很容易，只需创建一个列 team1_win 并在 true 时返回 1，否则返回 0。问题是我的数据是经过组织的，因此 team1 始终是获​​胜团队。所以我的目标变量列将只包含 1。对于二元分类来说，这似乎是一个问题。我不确定如何创建目标变量。我是否需要以某种方式重新整理我的数据，以便 team1 并不总是获胜团队？我的目标变量全为1有问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77969840/how-to-create-target-variable</guid>
      <pubDate>Fri, 09 Feb 2024 17:22:22 GMT</pubDate>
    </item>
    <item>
      <title>如何在 WSL GPU 支持下运行 Windows Python 代码？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77969677/how-to-run-windows-python-code-with-wsl-gpu-support</link>
      <description><![CDATA[我尝试在 Windows PS 上的 WSL 2 发行版中使用 TensorFlow。我的问题是，我只能在互联网上找到描述安装过程的页面，但没有人解释如何在 Windows 上使用 GPU 加速从我的 virtual-env 环境运行代码。那么有没有办法在 WSL Distrubition 中远程运行机器学习脚本呢？
我已经成功安装了WSL2，包括miniconda和tensorflow（带有CUDA）。 Tensorflow Feedback 线也发现 GPU 没有问题。]]></description>
      <guid>https://stackoverflow.com/questions/77969677/how-to-run-windows-python-code-with-wsl-gpu-support</guid>
      <pubDate>Fri, 09 Feb 2024 16:52:00 GMT</pubDate>
    </item>
    <item>
      <title>预测《Madden》的未来统计数据[关闭]</title>
      <link>https://stackoverflow.com/questions/77969495/predicting-future-stats-in-madden</link>
      <description><![CDATA[我有一堆过去 4 年中玩家的《Madden》统计数据以及与之相关的属性（速度、投掷力量等）。
我已经运行了相关性分析，并找出了哪些属性是统计数据的最佳指标，但我想使用 scikit-learn 来看看是否可以预测未来的统计数据。
我希望得到一些反馈，看看我是否以正确的方式这样做，因为我得到的一些答案并不相符。
X = all_years[[&#39;AWARENESS&#39;,&#39;THROW ACC&#39;,&#39;THROW POWER&#39;]]
y = all_years[[&#39;TD&#39;]]

X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.4,random_state=100)

X_train = X_train.值
X_test = X_test.值
y_train = y_train.值
y_test = y_test.值

模型=线性回归()
model.fit(X_train,y_train)
FLACCO = pd.DataFrame([[96,94,93]])
林德利 = pd.DataFrame([[90,89,88]])


y_pred = model.predict(X_test)
分数 = model.score(X_test, y_test)

f_predict = model.predict(FLACCO)
l_predict = model.predict(LINDLEY)
打印（f_预测）
打印（l_预测）
]]></description>
      <guid>https://stackoverflow.com/questions/77969495/predicting-future-stats-in-madden</guid>
      <pubDate>Fri, 09 Feb 2024 16:20:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在嵌入词汇中添加新项目？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77969343/how-to-add-a-new-item-in-the-embeddings-vocabulary</link>
      <description><![CDATA[假设您已经训练了一个包含嵌入层的模型。
您的模型表现良好，并且您对嵌入感到满意。
然后，突然，您想在词汇表中添加一个新项目。
换句话说，您想要计算这个新项目的嵌入。
嵌入层基本上是一个查找表，用于将正整数转换为固定大小的密集向量，现在您想要考虑训练期间不存在的新整数。
如何在不从头开始重新训练模型的情况下做到这一点？
重新启动训练冻结除用于嵌入新项目的参数之外的所有参数是否有意义（在进行一些矩阵形状调整之后）？]]></description>
      <guid>https://stackoverflow.com/questions/77969343/how-to-add-a-new-item-in-the-embeddings-vocabulary</guid>
      <pubDate>Fri, 09 Feb 2024 15:55:26 GMT</pubDate>
    </item>
    <item>
      <title>pandas.get_dummies() 的 drop_first=False 参数什么时候适合使用？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77969065/when-would-pandas-get-dummiess-parameter-of-drop-first-false-be-appropriate-t</link>
      <description><![CDATA[在进行一些使用各种机器学习模型的案例研究时，我遇到了一个预测电信行业流失的项目。我看到的jupyter笔记本有以下几行代码：
dummies1 = pd.get_dummies(df_churn, columns=cat_cols, drop_first=False) #for kNN 和决策树
dummies2 = pd.get_dummies(df_churn, columns=cat_cols, drop_first=True) #用于逻辑回归

据我所知，drop_first 用于防止所谓的“虚拟变量陷阱”。在运行 kNN 和决策树模型时，是否有理由不删除额外的虚拟变量？
无法联系到 jupyter Notebook 的作者，我在其他地方找不到答案。]]></description>
      <guid>https://stackoverflow.com/questions/77969065/when-would-pandas-get-dummiess-parameter-of-drop-first-false-be-appropriate-t</guid>
      <pubDate>Fri, 09 Feb 2024 15:07:54 GMT</pubDate>
    </item>
    <item>
      <title>“AdaBoostClassifier”对象没有属性“estimator_”</title>
      <link>https://stackoverflow.com/questions/77963903/adaboostclassifier-object-has-no-attribute-estimator</link>
      <description><![CDATA[我正在尝试对 adaboost 算法的每一轮进行计时（构建每棵附加树需要多长时间）。我 conda 安装了 scikit-learn 1.4.0（因为在他们的网站上说是这个版本）以及运行代码的所有其他要求。
这是我的代码：
Y, z = parse.getHARData() #返回我的特征 Y 和标签 z

Z_train, Z_test, j_train, j_test = train_test_split(Y, z, test_size=0.30, shuffle=True)

b_estimator = DecisionTreeClassifier(max_深度=深度)

ada = AdaBoostClassifier(估计器=b_估计器，n_估计器=NUMTREES)

经过时间 = []

对于范围内的阶段（NUMTREES）：start_time = time.time()

    # 访问并拟合当前的基本估计器
    base_estimator = ada._make_estimator(append=True, random_state=42)
    base_estimator.fit(Z_train, j_train)
    
    elapsed_time = time.time() - 开始时间
    elapsed_times.append(elapsed_time)

我期望这会启动计时器，使用森林之前的信息种植一棵树，将该树添加到集合中，停止时间，并将经过的时间附加到 elapsed_times 中。
相反，它返回此错误：
AttributeError Traceback（最近一次调用最后一次）
第 7 行 [9] 中的单元格
      4 开始时间 = time.time()
      6 # 访问并拟合当前的基本估计器
----&gt; 7 base_estimatr = ada._make_estimator(append=True, random_state=42)
      8 base_estimatr.fit(Z_train, j_train)
     10 经过时间 = time.time() - 开始时间

文件〜/anaconda3/envs/ADA/lib/python3.11/site-packages/sklearn/ensemble/_base.py:141，在BaseEnsemble._make_estimator（self，append，random_state）中
    135 def _make_estimator（自我，附加=真，随机状态=无）：
    136 &quot;&quot;&quot;制作并配置`estimator_`属性的副本。
    137
    138 警告：此方法应用于正确实例化新的
    139 名次级估算员。
    第140章 140
--&gt; 141 估计器 = 克隆（self.estimator_）
    142 estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})
    144如果random_state不是None：

AttributeError：“AdaBoostClassifier”对象没有属性“estimator_”
]]></description>
      <guid>https://stackoverflow.com/questions/77963903/adaboostclassifier-object-has-no-attribute-estimator</guid>
      <pubDate>Thu, 08 Feb 2024 18:08:39 GMT</pubDate>
    </item>
    <item>
      <title>带有 ball_tree 和度量半正矢的 DBSCAN</title>
      <link>https://stackoverflow.com/questions/77944949/dbscan-with-ball-tree-and-metric-haversine</link>
      <description><![CDATA[我有来自一本使用欧几里得度量的期刊的类代码：
 类 ST_DBSCAN():
        ”“”
        执行 ST_DBSCAN 聚类的类
        参数
        ----------
        eps1：浮点数，默认=0.5
            之间的空间密度阈值（最大空间距离）
            有两点被认为是相关的。
        eps2：浮点数，默认=10
            两个之间的时间阈值（最大时间距离）
            被认为相关的点。
        min_samples ：整数，默认=5
            一个核心点所需的样本数量。
        度量：字符串默认=&#39;euclidean&#39;
            使用的距离度量 - 更多选项是
            ‘braycurtis’，‘堪培拉’，‘切比雪夫’，‘cityblock’，‘相关性’，
            ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘jensenshannon’,
            ‘kulsinski’，‘mahalanobis’，‘匹配’，‘rogerstanimoto’，‘sqeuclidean’，
            ‘russellrao’、‘seuclidean’、‘sokalmichener’、‘sokalsneath’、‘yule’。
        n_jobs ：int 或 None，默认=-1
            启动进程数 -1 表示使用所有处理器
        属性
        ----------
        标签：数组，形状= [n_samples]
            数据的聚类标签 - 噪声定义为 -1
        参考
        ----------
        Ester, M.、H. P. Kriegel、J. Sander 和 X. Xu，《基于密度的方法》
        在带有噪声的大型空间数据库中发现簇的算法”。
        见：第二届国际知识发现会议论文集
        和数据挖掘，俄勒冈州波特兰，AAAI Press，第 226-231 页。 1996年
    ”“”
        
    
        def __init__(自我,
                     eps1=0.5，
                     每股收益2=10,
                     最小样本=5，
                     度量=&#39;欧几里得&#39;,
                     n_jobs=-1):
            self.eps1 = eps1
            自我.eps2 = eps2
            self.min_samples = min_samples
            self.metric = 公制
            self.n_jobs = n_jobs
    
        def fit(自身, X):
            
            应用ST DBSCAN算法
            ----------
            X ：二维 numpy 数组
                数组的第一个元素应该是时间
                属性为浮点数。数组中的以下位置是
                被视为空间坐标。该结构应如下所示 [[time_step1, x, y], [time_step2, x, y]..]
                例如二维数据集：
                数组([[0,0.45,0.43],
                [0,0.54,0.34],...])
            退货
            --------
            自己
            
            #检查输入是否正确
            X = 检查数组(X)
    
            如果不是 self.eps1 &gt; 0.0 或不是 self.eps2 &gt; 0.0 或不是 self.min_samples &gt; 0.0：
                raise ValueError(&#39;eps1, eps2, minPts 必须为正&#39;)
    
            n, m = X.形状
    
            
            # 使用二次内存消耗进行计算
    
            # 计算“时间”属性和空间属性的平方形式欧几里德距离矩阵
            time_dist = pdist(X[:, 0].reshape(n, 1), metric=self.metric)
            euc_dist = pdist(X[:, 1:], metric=self.metric)
    
            # 使用 time_dist 过滤 euc_dist 矩阵
            dist = np.where(time_dist &lt;= self.eps2, euc_dist, 2 * self.eps1)
    
            db = DBSCAN(eps=self.eps1,
                            min_samples=self.min_samples,
                            指标=&#39;预先计算&#39;）
            db.fit(正方形(距离))
    
            self.labels = db.labels_
    
           
    
        返回自我

这是在欧几里得度量中实现的，但是当应用于地理位置时，转换为笛卡尔坐标会出现问题。如何更改该类以处理具有度量正弦值的球树实现。]]></description>
      <guid>https://stackoverflow.com/questions/77944949/dbscan-with-ball-tree-and-metric-haversine</guid>
      <pubDate>Tue, 06 Feb 2024 02:25:00 GMT</pubDate>
    </item>
    <item>
      <title>AutoTrain 高级 CLI：错误：无法识别的参数：--fp16 --use-int4 [关闭]</title>
      <link>https://stackoverflow.com/questions/77664921/autotrain-advanced-cli-error-unrecognized-arguments-fp16-use-int4</link>
      <description><![CDATA[我目前在使用提供的自动训练工具在 Colab 笔记本中使用 LLM 模型微调数据时遇到问题。错误消息表明 autotrain 无法识别参数“--fp16”和“--use-int4”。我已经检查了文档和语法，但问题仍然存在。您能否提供解决此问题的指导或提供有关任何潜在解决方案的见解？谢谢。
/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13：
 UserWarning：无法加载图像Python扩展：&#39;/usr/local/lib/python3.10/dist-packages/torchvision/image.so：未定义符号：_ZN3c104cuda9SetDeviceEi&#39;如果您不打算使用`torchvision中的图像功能。 io`，你可以忽略这个警告。否则，您的环境可能有问题。在从源代码构建“torchvision”之前，您是否安装了“libjpeg”或“libpng”？ warn( 用法: autotrain  [] AutoTrain 高级 CLI: 错误: 无法识别的参数: --fp16 --use-int4

错误的屏幕截图
直到昨天，这段代码在这个 https://github.com/huggingface/autotrain-advanced 存储库中给出的 colab 笔记本上运行良好微调LLM，现在出现此错误。]]></description>
      <guid>https://stackoverflow.com/questions/77664921/autotrain-advanced-cli-error-unrecognized-arguments-fp16-use-int4</guid>
      <pubDate>Fri, 15 Dec 2023 07:53:31 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow 中分析 RNN、CNN、NN 结果</title>
      <link>https://stackoverflow.com/questions/61488789/analyze-of-rnn-vs-cnn-vs-nn-results-in-tensorflow</link>
      <description><![CDATA[我有大量标记数据集。每行包含 863 标记化单词。我正在尝试验证哪种类型的 NN 最适合分析此类数据集。
我准备了3个模型：
美国有线电视新闻网：
模型 = tf.keras.Sequential([
        tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32, input_length=863),
        tf.keras.layers.Conv1D(32, 5, 激活=&#39;relu&#39;,kernel_regularizer=l2(0.01),bias_regularizer=l2(0.01)),
        tf.keras.layers.GlobalMaxPooling1D(),
        tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
        tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）

简单平面神经网络：
模型 = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32,input_length=863),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(32，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
    tf.keras.layers.Dense(32，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
    tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
    tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
    tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
]）

和 RNN：
模型 = tf.keras.Sequential([
        tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32,input_length=863),
        tf.keras.layers.LSTM（32，激活=&#39;relu&#39;，return_sequences=True），
        tf.keras.layers.LSTM(32, 激活=&#39;relu&#39;, ),
        tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
        tf.keras.layers.Dense(16，激活=&#39;relu&#39;，kernel_regularizer=l2(0.01)，bias_regularizer=l2(0.01))，
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）

CNN 和 NN 给出了有希望的结果，accu 率约为 98%（可能过度拟合），而 RNN 的accu 率仅为 65% 左右。值得一提的是，RNN 的 epoch 至少在 10 分钟左右，而 CNN 和 NN 只有 1 分钟。
如何让 RNN 表现更好？]]></description>
      <guid>https://stackoverflow.com/questions/61488789/analyze-of-rnn-vs-cnn-vs-nn-results-in-tensorflow</guid>
      <pubDate>Tue, 28 Apr 2020 19:50:48 GMT</pubDate>
    </item>
    <item>
      <title>对 Dataframe 中的某些列进行估算</title>
      <link>https://stackoverflow.com/questions/52384806/imputer-on-some-columns-in-a-dataframe</link>
      <description><![CDATA[我正在尝试在名为“年龄”的单个列上使用Imputer来替换缺失值。但是，我收到错误：“预期是二维数组，却得到了一维数组：”
以下是我的代码
将 pandas 导入为 pd
将 numpy 导入为 np
从 sklearn.preprocessing 导入 Imputer

数据集 = pd.read_csv(“titanic_train.csv”)

dataset.drop(&#39;小屋&#39;, axis=1, inplace=True)
x = dataset.drop(&#39;幸存&#39;, axis=1)
y = 数据集[&#39;幸存&#39;]

imputer = Imputer（missing_values =“nan”，策略=“平均值”，轴= 1）
imputer = imputer.fit(x[&#39;年龄&#39;])
x[&#39;年龄&#39;] = imputer.transform(x[&#39;年龄&#39;])
]]></description>
      <guid>https://stackoverflow.com/questions/52384806/imputer-on-some-columns-in-a-dataframe</guid>
      <pubDate>Tue, 18 Sep 2018 10:44:49 GMT</pubDate>
    </item>
    <item>
      <title>网格上的 CNN 回归 - 卷积神经网络的局限性？</title>
      <link>https://stackoverflow.com/questions/49110140/cnn-regression-on-grid-limitation-of-convolutional-neural-networks</link>
      <description><![CDATA[我正在使用 CNN 解决（与高能物理相关的）问题。
为了理解这个问题，让我们考虑一下这里的这些示例。
左侧是 CNN 的输入，右侧是所需的输出。因此网络应该对输入进行聚类。这种聚类背后的实际算法（即我们如何获得所需的训练输出）非常复杂，我们希望 CNN 能够学习这一点。
我尝试过不同的 CNN 架构，例如类似于 U-net 架构的架构 (https:// arxiv.org/abs/1505.04597），还有各种卷积层的串联等。
输出总是非常相似（对于所有架构）。
在这里您可以看到一些 CNN 预测。
原则上，网络表现得相当好，但正如您所看到的，在大多数情况下，CNN 输出由几个直接相邻的填充像素组成，这在真实情况下永远不会（！）发生。&lt; /p&gt;
我一直在所有网络中使用均方误差作为损失函数。
您对如何避免这一问题并提高网络性能有什么建议吗？
或者这是 CNN 的一般限制，并且在实践中不可能使用 CNN 解决这样的问题？]]></description>
      <guid>https://stackoverflow.com/questions/49110140/cnn-regression-on-grid-limitation-of-convolutional-neural-networks</guid>
      <pubDate>Mon, 05 Mar 2018 12:05:42 GMT</pubDate>
    </item>
    </channel>
</rss>