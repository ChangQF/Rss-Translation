<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 07 Jan 2024 15:13:32 GMT</lastBuildDate>
    <item>
      <title>使用所有其他列作为特征来预测汽车成本（金钱）</title>
      <link>https://stackoverflow.com/questions/77773234/predict-cost-of-the-car-money-using-all-other-columns-as-features</link>
      <description><![CDATA[(https://i.stack.imgur.com/LhEWU.jpg)
我有以下数据集。任务是预测金钱栏。我面临的问题是我有很多分类特征。为了将这些分类特征放入模型中，我将使用 one-hot 编码。但是，当您查看数据集时，会发现某些特征是描述性的。此外，某些功能在对其应用一种热编码时，将产生额外的列，这将增加模型中数据集的大小。我必须使用所有功能。有关如何解决此问题的任何建议。
我不知道一种热编码是否有效，因为这会导致添加许多其他列。但是，任务规定我必须使用所有功能。一些功能（例如便利性和娱乐性）将导致有许多额外的列。]]></description>
      <guid>https://stackoverflow.com/questions/77773234/predict-cost-of-the-car-money-using-all-other-columns-as-features</guid>
      <pubDate>Sun, 07 Jan 2024 13:28:28 GMT</pubDate>
    </item>
    <item>
      <title>在时间序列数据上训练 Transformer 网络的最佳历元数？早期停止和模型选择策略[关闭]</title>
      <link>https://stackoverflow.com/questions/77772896/optimal-number-of-epochs-for-training-transformer-network-on-time-series-data-e</link>
      <description><![CDATA[我有一个经过时间序列数据训练的变压器网络。任务是预测变量在接下来的 dt 天内是否会增加一定的百分比。输入是前 90 天的数据。训练数据是截至 2022 年的数据，而验证数据是从 2022 年至今的数据。训练和验证集中没有重叠的数据。我根据训练数据训练模型，并记下验证损失最小化的纪元 E。然后，我将训练数据和验证数据连接起来，并重新训练 E 轮以获得最终模型。
然而，当连接数据时，我在生成的训练集中有更多批次，我担心过度拟合，并且纪元号 E 存在很大差异。尽管如此，我认为如果我不要对连接的数据集进行重新训练，或者如果我遗漏了最终的测试集。我使用 1,600,000 个样本进行训练，并使用 400,000 个样本进行验证。我认为由于训练模型需要时间，交叉验证是不可行的。有什么关于执行训练的稳健方法的建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77772896/optimal-number-of-epochs-for-training-transformer-network-on-time-series-data-e</guid>
      <pubDate>Sun, 07 Jan 2024 11:39:28 GMT</pubDate>
    </item>
    <item>
      <title>在本地计算机上加载LLM [关闭]</title>
      <link>https://stackoverflow.com/questions/77772302/loading-llm-on-local-machine</link>
      <description><![CDATA[我的硬件
16GB内存
4 GB 显存 Nvidia RTX 3050
i5 12 代 12500H 处理器
我可以加载 Microsoft phi 2.7B 的完整精度并在我的本地计算机或至少 QLoraed 模型上使用它吗？
如果不是，如何计算硬件要求？
我尝试加载 7B llama 模型，但我的系统卡住并出现错误]]></description>
      <guid>https://stackoverflow.com/questions/77772302/loading-llm-on-local-machine</guid>
      <pubDate>Sun, 07 Jan 2024 07:53:06 GMT</pubDate>
    </item>
    <item>
      <title>放大后的输出图像颜色与原始图像颜色不同</title>
      <link>https://stackoverflow.com/questions/77772227/upscaled-output-image-color-different-from-orginal</link>
      <description><![CDATA[我已将 Real-ESRGAN-x44-general onnx 转换为 tflite。但得到不同的颜色输出。
输入
output_image_before_conversion
输出
我尝试过的代码：
导入tensorflow.lite作为tflite
将 numpy 导入为 np
导入CV2
导入操作系统

# 定义模型和图像的路径。
model_path = “realesr-general-x4v3_float32.tflite”
input_image_path = &#39;输入.jpg&#39;
输出图像路径 = &#39;输出图像.jpg&#39;

# 检查镜像文件是否存在。
如果不是 os.path.exists(input_image_path):
    引发异常（f“错误：输入图像路径 &#39;{input_image_path}&#39; 不存在。”）

# 加载 TensorFlow Lite 模型。
解释器 = tflite.Interpreter(model_path=model_path)
解释器.allocate_tensors()

# 获取输入和输出的详细信息。
input_details =terpreter.get_input_details()
输出详细信息=解释器.get_output_details()

# 加载并预处理图像。
输入图像 = cv2.imread(输入图像路径)
input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)
input_image_resized = cv2.resize(input_image_rgb, tuple(input_details[0][&#39;shape&#39;][1:3]))
# 跳过标准化以查看是否是问题所在
input_image_normalized = input_image_resized.astype(np.float32) / 255.0

# 设置未归一化的输入张量
terpreter.set_tensor(input_details[0][&#39;index&#39;], [input_image_normalized])

# 运行推理。
解释器.invoke()

# 获取输出结果。
output_data =terpreter.get_tensor(output_details[0][&#39;index&#39;])
输出图像 = np.squeeze(输出数据)

# 假设模型输出在 [0, 1] 范围内，必要时进行反规范化。
# 如果模型输出不在这个范围内，这一步可能需要调整。
输出图像非标准化 = (输出图像 * 255.0).astype(np.uint8)

# 保存颜色转换前的图像以检查原始输出。
cv2.imwrite（&#39;output_image_before_conversion.jpg&#39;，output_image_denormalized）

# 将输出图像从RGB转换为BGR，以与OpenCV的保存功能保持一致。
output_image_bgr = cv2.cvtColor(output_image_denormalized, cv2.COLOR_RGB2BGR)

# 保存颜色转换后的图像以检查最终输出。
cv2.imwrite（输出图像路径，输出图像背景）

# 打印成功消息。
print(f“输出图像已成功保存到‘{output_image_path}’。”)

为什么会发生这种情况以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/77772227/upscaled-output-image-color-different-from-orginal</guid>
      <pubDate>Sun, 07 Jan 2024 07:18:35 GMT</pubDate>
    </item>
    <item>
      <title>如何将 JSON 数据对象引入 LLM 模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77770292/how-to-ingest-json-data-object-into-an-llm-model</link>
      <description><![CDATA[我正在寻求开发一个自定义（langchain、llama 或 openAI）LLM 模型，该模型可以摄取 JSON 数据对象格式的股票数据，如下所示：
示例数据
&lt;前&gt;&lt;代码&gt;[
    {
        “符号”：“AAPL”，
        “价格”：178.72，
        “贝塔”：1.286802，
        “平均成交量”：58405568，
        “mktCap”：2794144143933，
        “最后一个Div”：0.96，
        “范围”：“124.17-198.23”，
        “变化”：-0.13，
        “公司名称”：“苹果公司”，
        “货币”：“美元”，
        “cik”：“0000320193”，
        “isin”：“US0378331005”，
        “尖头”：“037833100”，
        “交易所”：“纳斯达克全球精选”，
        “exchangeShortName”：“纳斯达克”，
        “行业”：“消费电子产品”，
        “网站”：“https://www.apple.com”，
        “首席执行官”：“先生”蒂莫西·D·库克”，
        “部门”：“技术”，
        “国家”：“美国”，
        “全职员工”：“164000”，
        “电话”：“408 996 1010”，
        “地址”：“苹果公园路一号”，
        “城市”：“库比蒂诺”，
        “州”：“CA”，
        “邮编”：“95014”，
        “dcfDiff”：4.15176，
        “DCF”：150.082，
        “图像”：“https://financialmodelingprep.com/image-stock/AAPL.png”，
        “ipoDate”：“1980-12-12”，
        “默认图像”：假，
        “isEtf”：假，
        “isActivelyTrading”：true，
        “isAdr”：假，
        “isFund”：假
    }
]

我目前正在处理一个数据集，其中包括纽约证券交易所和纳斯达克的所有股票代码。本质上，我很好奇将这些数据输入语言模型的最佳方法。我应该使用 .txt 文件来实现此目的，还是需要与 API 交互以动态检索数据？此外，是否可以直接向 LLM 提供表格数据对象？
摘要
我的目标是建立一个本地托管模型，可以解释自然语言查询，例如“提供价格低于 40 美元且属于技术领域的所有股票行情？”这将导致根据指定的条件检索相关库存数据。]]></description>
      <guid>https://stackoverflow.com/questions/77770292/how-to-ingest-json-data-object-into-an-llm-model</guid>
      <pubDate>Sat, 06 Jan 2024 17:12:22 GMT</pubDate>
    </item>
    <item>
      <title>TF Transformer 模型永远不会过拟合，只会停滞不前：训练曲线的解读和改进建议</title>
      <link>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</link>
      <description><![CDATA[此训练曲线适用于 Transformer 模型，该模型处理 2D（不包括批次）顺序信号并使用 Adam 优化器、32 批次大小和学习率：一个自定义 LR 调度程序，它复制在“Attention is”中使用的预热调度程序所有你需要的&#39;纸。训练曲线如下所示，最终训练损失略低于验证损失，但训练损失永远不会开始回升，我将其解释为模型永远不会开始过度拟合，只是在 90 纪元后停止重新调整权重。
更好的解释和解决方案来改进这个模型？

下面是我的简短的可重现代码：
x_train = np.random.normal(size=(32, 512, 512))
批量大小 = 32
H, W = x_train.shape
行，列= np.indices（（H，W），稀疏= True）
padding_mask_init = np.zeros((H, W, W), dtype=np.bool_)
padding_mask_init[行，1：，列] = 1
padding_mask = padding_mask_init[:batch_size]
嵌入尺寸 = 512
密集_暗 = 2048
头数 = 2
形状 = (batch_size, embed_dim, 512) #(32, 512, 512)
解码器_输入=层.输入（batch_input_shape=形状，dtype=tensorflow.float16）
mha_1 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
mha_2 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
Layernorm_1 = 层.LayerNormalization()

Z = 解码器输入
Z = mha_1(查询=Z、值=Z、键=Z、use_causal_mask=True、attention_mask=padding_mask)
Z = layernorm_1(Z + 解码器输入)
Z = mha_2(查询=Z，值=解码器输入，键=解码器输入，attention_mask=padding_mask)
输出=layers.TimeDistributed（keras.layers.Dense（embed_dim，激活=“softmax”））（Z）

模型 = keras.Model(decoder_inputs, 输出)
model.compile（损失=“mean_squared_error”，optimizer=tf.keras.optimizers.Adam（learning_rate=lr_schedule（embed_dim，3000），beta_1=0.9，beta_2=0.98，epsilon=1.0e-9），metrics=[&quot; “准确度”]）

历史= model.fit（数据集，epochs = 200，validation_data = val_dataset）
]]></description>
      <guid>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</guid>
      <pubDate>Fri, 05 Jan 2024 02:47:25 GMT</pubDate>
    </item>
    <item>
      <title>在定制样本上测试 LipVoicer 模型的困难</title>
      <link>https://stackoverflow.com/questions/77746726/difficulty-testing-lipvoicer-model-on-custom-samples</link>
      <description><![CDATA[我正在使用 LipVoicer 模型，这是一个开源在线工具，旨在从无声视频中生成语音。 LipVoicer 的 GitHub 存储库位于 https://github.com/yochaiye/LipVoicer
虽然我已经在 Kaggle 笔记本上成功设置了模型环境，如下面的代码所示：
!git 克隆 https://github.com/yochaiye/LipVoicer.git
cd 唇音
!apt-get 安装 ffmpeg
!git 克隆 https://github.com/hhj1897/face_detection.git
cd 人脸检测
!apt-get 安装 git-lfs
!git lfs 拉
pip install -e 。
光盘 ..
!git 克隆 https://github.com/hhj1897/face_alignment.git
cd 面对齐
pip install -e 。
光盘 ..
!git clone --recursive https://github.com/parlance/ctcdecode.git
cd ctc解码
！点安装。

自述文件提供了模型及其设置的概述，但在指导用户完成测试阶段方面存在不足，尤其是在使用示例时。我正在寻求有关如何在我的特定样本集上有效测试 LipVoicer 模型的帮助或指导。]]></description>
      <guid>https://stackoverflow.com/questions/77746726/difficulty-testing-lipvoicer-model-on-custom-samples</guid>
      <pubDate>Tue, 02 Jan 2024 14:37:46 GMT</pubDate>
    </item>
    <item>
      <title>猜几个变量之和是一个固定值，如何预测这个固定值？</title>
      <link>https://stackoverflow.com/questions/77746513/guess-the-sum-of-several-variables-is-a-fixed-value-how-to-predict-this-fixed-v</link>
      <description><![CDATA[我知道数据框有三列（名为 A、B 和 C），现在我想预测
k1×A+k2×B+k3×C = 固定值D
A、B、C已知，k1、k2、k3、D未知（需要预测）。
如何预测这三个系数和这个固定值？
详情如下：
file_path = &#39;输入/CharacterData.xlsx&#39;
df = pd.read_excel(文件路径)
Five_star_data = df[df[&#39;star&#39;] == 5] # 选择五星级人物
X = Five_star_data[[&#39;生命值&#39;, &#39;攻击力&#39;, &#39;防御力&#39;]] # 这里我选择了关于五星级人物的三个三维度。而“生命值”、“攻击力”、“防御力”在英文中分别表示HP、ATK、DEF


我想预测：
k1×HP +k2×ATK +k3×DEF = 固定值D
肯定有误差，如何判断误差是大还是小？
另外，请原谅我的英语能力和机器学习水平不是很好。
首先，我尝试将 D 设置为固定值 1：
X = Five_star_data[[&#39;生命值&#39;, &#39;攻击力&#39;, &#39;防御力&#39;]]
y 值 = 1
y = pd.DataFrame(np.full((X.shape[0], 1), y_value))
lin_reg = 线性回归()
lin_reg.fit(X, y)
print(lin_reg.intercept_) # [1.]
打印（lin_reg.coef_）# [[0。 0.0.]]

我认为，因为X(HP,ATK,DEF)改变了，但y是固定的。所以线性回归模型认为A,B,C的系数应该为0，截距正好为1。
其次，我选择 HP(&#39;生命值&#39;) 作为 y，并将 HP 降低到 X。像这样：
X = Five_star_data[[&#39;生命值&#39;, &#39;攻击力&#39;, &#39;防御力&#39;]]
X_1 = X.drop([“生命值”], axis=1)
y = Five_star_data[&#39;生命值&#39;]
y = -y
lin_reg = 线性回归()
lin_reg.fit(X_1, y)
打印（lin_reg.intercept_）＃-9252.295197338677
打印（lin_reg.coef_）＃[5.60260844-6.41900625]

可以得到结果，但不知道是否合适。
是的，我将问题更改为：
-HP = k1×ATK + K2×EF + D。
我觉得可以改成原来的问题：
HP + k1×ATK + K2×DEF = -D。]]></description>
      <guid>https://stackoverflow.com/questions/77746513/guess-the-sum-of-several-variables-is-a-fixed-value-how-to-predict-this-fixed-v</guid>
      <pubDate>Tue, 02 Jan 2024 13:53:56 GMT</pubDate>
    </item>
    <item>
      <title>数字预测准确率 0%</title>
      <link>https://stackoverflow.com/questions/77744924/digit-prediction-give-0-accuracy</link>
      <description><![CDATA[我需要使用口袋算法制作一个数字分类器进行二元分类。因为这是一个 10 位数字的问题，所以我需要使用一对一的方法。我已经实现了基本的学习算法来找到每个分类器的权重。我对所有数字的准确率约为 98%

现在，当我运行测试数据时，我得到了 0.3% 的准确度，甚至没有 1 个正确的预测。我不太确定哪里出了问题，因为看起来每个数字的准确性有点高。
我的错误在哪里？
这是代码：
将 numpy 导入为 np
# 获取mnist数据
从 sklearn.datasets 导入 fetch_openml
从 sklearn.model_selection 导入 train_test_split

mnist = fetch_openml(&#39;mnist_784&#39;, 版本=1)
x, y = mnist[&#39;数据&#39;], mnist[&#39;目标&#39;]
x = x / 255.0 # 标准化数据


# 添加偏差的函数
def add_bias(x):
    偏差 = np.ones((x.shape[0], 1))
    返回 np.concatenate((偏差, x), 轴=1)


# 返回点积的符号。用作谓词
def 预测（权重，x）：
    返回 np.sign(np.dot(x, 权重))


def update_weights(权重, x, y):
    对于范围内的 i(len(x))：
        预测 = 预测（权重，x[i]）
        if y[i] * 预测 &lt;= 0: # 错误分类
            权重 = 权重 + y[i] * x[i]
    返回权重


# 确定权重准确性的函数
def calc_acc(权重, x, y):
    预测=预测（权重，x）
    正确 = sum(预测 == y)
    返回正确的/len(y)


def Predict_all_classifiers（分类器，样本）：
    # 将每个分类器应用于样本
    预测 = [np.dot(样本, 分类器[数字]) 对于范围(10) 中的数字]
    # 选择输出值最高的分类器
    返回 np.argmax(预测)


digital_classifier = {} # 字典来存储每个分类器的权重
y = y.astype(int) # 将数据转换为整数
对于范围 (10) 中的数字：
    # 准备数据：
    y_binary = (y == digital).astype(int) * 2 - 1 # 创建二进制目标数组
    # 将数据分割为 60K 用于训练，10K 用于测试
    X_train，X_test，y_train，y_test = train_test_split（x，y_binary，train_size = 60000，test_size = 10000，random_state = 42）
    X_train = X_train.值
    X_test = X_test.值
    y_train = y_train.值
    y_test = y_test.值

    # 初始化权重向量
    权重 = np.zeros(785) # 权重向量
    X_train_bias = add_bias(X_train) # 添加偏差值
    X_test_bias = add_bias(X_test) # 添加偏差值
    pocket_weights = np.copy(权重)

    # 使用袖珍算法训练分类器 1000 个时期
    历元 = 1000
    最佳_acc = 0.0

    对于范围内的纪元（纪元）：
        curr_weights = update_weights(np.copy(pocket_weights), X_train_bias, y_train)
        curr_acc = calc_acc(curr_weights, X_train_bias, y_train)
        如果 curr_acc &gt;最佳_ACC：
            最佳_acc = 当前_acc
            pocket_weights = np.copy(curr_weights)

    print(&quot;数字的最佳准确度：&quot; + str(digit) + &quot; 是：&quot; + str(best_acc))
    digital_classifier[digit] = pocket_weights # 将分类器添加到字典中

    # 根据测试数据进行预测和评估
    正确预测 = 0
    对于范围内的 i(len(X_test_bias))：
        # 预测数字
        Predicted_digit = Predict_all_classifiers(digit_classifier, X_test_bias[i])

        # 检查预测是否正确
        如果预测数字 == y_test[i]：
            正确预测 += 1

    # 计算准确率
    准确度 = Correct_predictions / len(X_test)
    print(f&quot;测试数据的准确度：{accuracy * 100}%&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/77744924/digit-prediction-give-0-accuracy</guid>
      <pubDate>Tue, 02 Jan 2024 08:35:38 GMT</pubDate>
    </item>
    <item>
      <title>如何用视频数据训练一些模型</title>
      <link>https://stackoverflow.com/questions/77733070/how-to-train-some-model-with-video-data</link>
      <description><![CDATA[我用这样的图像训练了我的模型：
类 ASDataset(数据集):
    def __init__(self, client_file: str, imposter_file: str, 转换=无):
        将 open(client_file, &quot;r&quot;) 作为 f：
            client_files = f.read().splitlines()
        将 open(imposter_file, &quot;r&quot;) 作为 f：
            imposter_files = f.read().splitlines()
        self.labels = torch.cat((torch.ones(len(client_files)), torch.zeros(len(imposter_files))))
        self.imgs = client_files + imposter_files
        self.transforms = 变换

    def __len__(自身):
        返回 len(self.imgs)

    def __getitem__(self, idx):
        img_name = self.imgs[idx]
        img = Image.open(img_name)
        标签 = self.labels[idx]
        如果自我变换：
            img = self.transforms(img)
        返回图片、标签

train_dataset = ASDataset(client_file=“/kaggle/input/nuaaaa/raw/client_train_raw.txt”，imposter_file=“/kaggle/input/nuaaaa/raw/imposter_train_raw.txt”，transforms=预处理)
val_dataset = ASDataset(client_file=“/kaggle/input/nuaaaa/raw/client_test_raw.txt”，imposter_file=“/kaggle/input/nuaaaa/raw/imposter_test_raw.txt”，transforms=预处理)

# 创建数据加载器
train_loader = DataLoader(train_dataset,batch_size=8,shuffle=True)
val_loader = DataLoader(val_dataset,batch_size=8,shuffle=False)


但现在我有了视频数据，据我了解，我需要更改 class ASDataset。我尝试了不同的变体。例如：
类VideoDataset（数据集）：
    def __init__(self, video_file: str, 标签: int, 转换=无):
        self.video = cv2.VideoCapture(video_file)
        self.label = 标签
        self.transforms = 变换

    def __len__(自身):
        return int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))

    def __getitem__(self, idx):
        self.video.set(cv2.CAP_PROP_POS_FRAMES, idx)
        成功，frame = self.video.read()
        如果没有成功：
            raise ValueError(“读取帧失败”)
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # 转换为RGB
        如果自我变换：
            框架 = self.transforms(框架)
        返回框架，自我标签

但是没有人给我结果。
请帮助我，如何使用视频数据进行训练？
数据示例client_train_raw.txt：
&lt;前&gt;&lt;代码&gt;/kaggle/input/dfdcdfdc/DFDCDFDC/dbnygxtwek.mp4
/kaggle/input/dfdcdfdc/DFDCDFDC/dbtbbhakdv.mp4
/kaggle/输入/dfdcdfdc/DFDCDFDC/ddepeddixj.mp4
]]></description>
      <guid>https://stackoverflow.com/questions/77733070/how-to-train-some-model-with-video-data</guid>
      <pubDate>Fri, 29 Dec 2023 16:49:09 GMT</pubDate>
    </item>
    <item>
      <title>无法从“keras.utils.layer_utils”导入名称“CallFunctionSpec”（/usr/local/lib/python3.9/dist-packages/keras/utils/layer_utils.py）</title>
      <link>https://stackoverflow.com/questions/75982832/cannot-import-name-callfunctionspec-from-keras-utils-layer-utils-usr-local</link>
      <description><![CDATA[代码昨天运行良好
代码是：
来自 sklearn 导入指标
从tensorflow.keras.layers导入密集，辍学，激活，扁平化
从tensorflow.keras.models导入顺序
从tensorflow.python.keras.optimizers导入Adam

显示的错误是：
ImportError Traceback（最近一次调用最后一次）
&lt;ipython-input-71-00e64660885b&gt;在&lt;细胞系：2&gt;()
      1 从sklearn导入指标
----&gt; 2 从tensorflow.keras.layers导入Dense、Dropout、Activation、Flatten
      3 从tensorflow.keras.models导入顺序
      4 从tensorflow.python.keras.optimizers导入Adam

7帧
/usr/local/lib/python3.9/dist-packages/keras/ saving/legacy/saved_model/utils.py 在  中
     28 从 keras.utils 导入 tf_contextlib
     29 从 keras.utils.generic_utils 导入 LazyLoader
---&gt; 30 从 keras.utils.layer_utils 导入 CallFunctionSpec
     31
     32training_lib = LazyLoader(“training_lib”, globals(), “keras.engine.training”)

ImportError：无法从“keras.utils.layer_utils”导入名称“CallFunctionSpec”（/usr/local/lib/python3.9/dist-packages/keras/utils/layer_utils.py）


它说无法从“keras.utils.layer_utils”导入“CallFunctionSpec”
但我没有在其中使用 utils
我不需要其中的实用程序，我需要的是图层，但它在实用程序中显示错误
我应该如何解决无法从“keras.utils.layer_utils”导入名称“CallFunctionSpec”
并且此错误已多次出现]]></description>
      <guid>https://stackoverflow.com/questions/75982832/cannot-import-name-callfunctionspec-from-keras-utils-layer-utils-usr-local</guid>
      <pubDate>Tue, 11 Apr 2023 06:07:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 sklearn.neighbors 时出现未来警告该怎么办？</title>
      <link>https://stackoverflow.com/questions/75521999/what-to-do-about-future-warning-when-using-sklearn-neighbors</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/75521999/what-to-do-about-future-warning-when-using-sklearn-neighbors</guid>
      <pubDate>Tue, 21 Feb 2023 14:39:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 Trial.suggest_int 从 optuna 中的给定列表中选择值，就像 Trial.suggest_categorical 一样</title>
      <link>https://stackoverflow.com/questions/75282840/use-trial-suggest-int-to-pick-values-from-given-list-in-optuna-just-like-trial</link>
      <description><![CDATA[我正在使用 optuna 对 Python 中的 ML 模型进行超参数调整。在定义用于调整深度学习模型的目标函数时，我尝试定义一个选项列表，trail.suggest_int 可以从中获取值。
例如 - 
&#39;batch_size&#39;: Trial.suggest_int(&#39;batch_size&#39;, [16, 32, 64, 128, 256])

optuna 文档建议 Trial.suggest_int 应采用以下格式
&#39;some_param&#39;: Trial.suggest_int(&#39;some_param&#39;, low, high, step)

我的代码如下所示
def 目标（试用）：
        DL_参数 = {
            &#39;learning_rate&#39;: Trial.suggest_float(&#39;learning_rate&#39;, 1e-3, 1e-1),
            &#39;optimizer&#39;: Trial.suggest_categorical(&#39;optimizer&#39;, [&quot;Adam&quot;, &quot;RMSprop&quot;, &quot;SGD&quot;]),
            &#39;h_units&#39;: Trial.suggest_int(&#39;h_units&#39;, 50, 250, 步骤 = 50),
            &#39;阿尔法&#39;: Trial.suggest_float(&#39;阿尔法&#39;, [0.001,0.01, 0.1, 0.2, 0.3]),
            &#39;batch_size&#39;: Trial.suggest_int(&#39;batch_size&#39;, [16, 32, 64, 128, 256]),
        }
        DL_模型=构建_模型（DL_参数）
        DL_model.compile(optimizer=DL_param[&#39;optimizer&#39;], loss=&#39;mean_squared_error&#39;)
        DL_model.fit（x_train，y_train，validation_split = 0.3，shuffle = True，
                                  batch_size = DL_param[&#39;batch_size&#39;], epochs = 30)
        y_pred_2 = DL_model.predict(x_test)
        返回 mse(y_test_2, y_pred_2, 平方=True)

我在定义参数&#39;alpha&#39;和&#39;batch_size&#39;的列表时遇到问题。有办法吗？类似 Trial.suggest_categorical 的东西可以从给定列表中选择字符串，如上面的代码
&#39;optimizer&#39;: Trial.suggest_categorical(&#39;optimizer&#39;, [&quot;Adam&quot;, &quot;RMSprop&quot;, &quot;SGD&quot;])

欢迎任何建议。提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/75282840/use-trial-suggest-int-to-pick-values-from-given-list-in-optuna-just-like-trial</guid>
      <pubDate>Mon, 30 Jan 2023 10:06:22 GMT</pubDate>
    </item>
    <item>
      <title>有没有其他方法（将一列的值组合到不同的组中），而不是在下面的问题中多次使用“df.replace（）”？</title>
      <link>https://stackoverflow.com/questions/70277650/is-there-any-other-way-to-combine-values-of-one-column-into-different-groups</link>
      <description><![CDATA[在：
char_df[&#39;Loan_Title&#39;].unique()

输出：
array([&#39;债务合并&#39;, &#39;信用卡再融资&#39;,
       ‘家装’、‘信用整合’、‘绿色贷款’、‘其他’、
       “搬家”、“信用卡”、“医疗费用”、
       “再融资”、“信用卡整合”、“借贷俱乐部”、
       “债务合并贷款”、“重大采购”、“休假”、
       &#39;业务&#39;, &#39;信用卡还款&#39;, &#39;信用卡&#39;,
       &#39;信用卡再融资&#39;, &#39;个人贷款&#39;, &#39;抄送再融资&#39;, &#39;合并&#39;,
       &#39;医疗&#39;、&#39;贷款1&#39;、&#39;合并&#39;、&#39;卡合并&#39;、
       “汽车融资”、“债务”、“购房”、“自由”、“合并”、
       “摆脱债务”、“合并贷款”、“部门合并”、
       &#39;个人&#39;, &#39;卡&#39;, &#39;浴室&#39;, &#39;refi&#39;, &#39;信用卡贷款&#39;,
       &#39;信用卡债务&#39;、&#39;房屋&#39;、&#39;2013 年债务合并&#39;、
       &#39;债务贷款&#39;, &#39;cc 再融资&#39;, &#39;房屋&#39;, &#39;cc 合并&#39;,
       “信用卡再融资”、“信用贷款”、“还款”、
       &#39;账单合并&#39;、&#39;信用卡还款&#39;、&#39;信用卡还款&#39;、
       &#39;免除债务&#39;, &#39;我的贷款&#39;, &#39;信用还清&#39;, &#39;我的贷款&#39;, &#39;贷款&#39;,
       &#39;账单还清&#39;、&#39;cc-再融资&#39;、&#39;债务减少&#39;、&#39;医疗贷款&#39;、
       &#39;婚礼贷款&#39;、&#39;信贷&#39;、&#39;还清账单&#39;、&#39;再融资贷款&#39;、
       “债务还清”、“汽车贷款”、“还清”、“资金池”、“信用还清”、
       &#39;信用卡再融资贷款&#39;、&#39;抄送贷款&#39;、&#39;无债务&#39;、&#39;conso&#39;、
       &#39;房屋装修贷款&#39;，&#39;贷款合并&#39;，&#39;借贷贷款&#39;，
       &#39;救济&#39;，&#39;抄送&#39;，&#39;贷款1&#39;，&#39;取得进步&#39;，&#39;住房贷款&#39;，&#39;账单&#39;]，
      数据类型=对象）

在：
char_df=char_df.replace([&#39;债务合并&#39;,&#39;债务合并贷款&#39;,&#39;部门合并&#39;,&#39;债务合并2013&#39;], &#39;dept_consolidation&#39;)
char_df = char_df.replace([&#39;个人&#39;,&#39;个人贷款&#39;],&#39;personal_loan&#39;)
char_df = char_df.replace([&#39;信用卡再融资&#39;,&#39;信用卡再融资&#39;,&#39;信用卡再融资&#39;,&#39;信用卡再融资贷款&#39;],&#39;credit_card_refinance&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/70277650/is-there-any-other-way-to-combine-values-of-one-column-into-different-groups</guid>
      <pubDate>Wed, 08 Dec 2021 15:35:37 GMT</pubDate>
    </item>
    <item>
      <title>如何在 XGBRegressor 的 MultiOutputRegressor 上使用验证集？</title>
      <link>https://stackoverflow.com/questions/66785587/how-do-i-use-validation-sets-on-multioutputregressor-for-xgbregressor</link>
      <description><![CDATA[我正在使用以下 MultiOutputRegressor：
从 xgboost 导入 XGBRegressor
从 sklearn.multioutput 导入 MultiOutputRegressor

#定义估计器
估计器 = XGBRegressor(
    目标 = &#39;reg:squarederror&#39;
    ）

# 定义模型
my_model = MultiOutputRegressor(估计器 = 估计器, n_jobs = -1).fit(X_train, y_train)

我想使用验证集来评估 XGBRegressor 的性能，但我认为 MultiOutputRegressor 不支持将 eval_set 传递给 fit 函数。
在这种情况下如何使用验证集？是否有任何解决方法可以调整 XGBRegressor 以具有多个输出？]]></description>
      <guid>https://stackoverflow.com/questions/66785587/how-do-i-use-validation-sets-on-multioutputregressor-for-xgbregressor</guid>
      <pubDate>Wed, 24 Mar 2021 16:45:21 GMT</pubDate>
    </item>
    </channel>
</rss>