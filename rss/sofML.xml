<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 17 Mar 2024 15:13:57 GMT</lastBuildDate>
    <item>
      <title>Hugging Face 的无头 GPT2 模型在保存时抛出错误 - 如何添加输入和输出层以及自定义 PositionalEmbedding</title>
      <link>https://stackoverflow.com/questions/78175539/headless-gpt2-model-from-hugging-face-throws-error-on-saving-how-to-add-input</link>
      <description><![CDATA[我想使用 GPT2 对序列数据进行回归任务，因此尝试从 Hugging Face 中找出无头 TFGPT2，代码如下：
配置 = GPT2Config(n_embd = embed_dim, n_head=num_heads)
基础模型 = TFGPT2Model（配置）
输入形状 = (1, 嵌入尺寸)
input1 = 层.Input(shape=input_shape, dtype=tf.float32)
positional_encoding = PositionalEmbedding(sequence_length, embed_dim)
解码器输入=位置编码（输入1）
Z = base_model.call(inputs_embeds=decoder_inputs)
输出=layers.TimeDistributed（keras.layers.Dense（embed_dim，激活=“relu”））（Z.last_hidden_​​state）
模型= keras.Model（输入1，输出）
model.compile(loss=“mean_squared_error”，optimizer=tf.keras.optimizers.Adam(beta_1=0.9，beta_2=0.98，epsilon=1.0e-9)，metrics=[tf.keras.metrics.RootMeanSquaredError()] ）
历史= model.fit（数据集，validation_data = val_dataset，epochs = epoch_len，verbose = 1）
tf.keras. saving. save_model(模型, r&#39;/drive/model_huggingface&#39;)

另请注意，我还使用自定义 PositionalEmbedding 类，因此可选的 input_embeds 参数传递给模型。
该模型训练并学习数据，但在尝试保存时会抛出错误：
AssertionError：尝试导出引用“未跟踪”资源的函数。由函数捕获的 TensorFlow 对象（例如 tf.Variable）必须通过将其分配给被跟踪对象的属性或直接分配给主对象的属性来“跟踪”。请参阅以下信息：
    函数名称 = b&#39;__inference_signature_wrapper_452514&#39;
    捕获的张量 = 
    可追踪引用此张量 = ;
    内部张量 = Tensor(“452144:0”, shape=(), dtype=resource)

我认为这是因为我向该模型添加了头部和自定义层。请让我知道您对我对如何实现此模型的解释的看法。]]></description>
      <guid>https://stackoverflow.com/questions/78175539/headless-gpt2-model-from-hugging-face-throws-error-on-saving-how-to-add-input</guid>
      <pubDate>Sun, 17 Mar 2024 14:03:14 GMT</pubDate>
    </item>
    <item>
      <title>在 UNet 模型的 FluxTraining.jl 中将数据从 DataLoader 传递到 Learner 时出现问题</title>
      <link>https://stackoverflow.com/questions/78175117/trouble-with-passing-data-from-dataloader-to-learner-in-fluxtraining-jl-for-unet</link>
      <description><![CDATA[我正在尝试使用 FluxTraining.jl 训练 UNet 模型 u，但在将数据从 DataLoader 正确传递到 Learner 时遇到困难。
上下文：
我有两个数据集：一个用于名为“w”的输入图像，另一个用于名为“w”的输入图像。尺寸为 256x256x3x20（20 个观察值，3 个 RGB 通道），另一个用于地面实况比较，称为“wp”尺寸为 256x256x1x20（20 个观察值，1 个灰度通道）。
我使用 DataLoader 定义数据迭代器，如下所示：
trainiter = DataLoader((w, wp), 4)

然后，我尝试使用以下代码将数据传递给学习者：
学习者 = 学习者(
    你，
    损失，
    回调 = [
        指标（准确度），
        检查点（“trainingData/modelSaves/”），
        记录器后端
    ],
    优化器=选择
）

#一个纪元出现错误
纪元！（学习者，TrainingPhase（），培训师）

问题：
运行代码时，我遇到一个错误，表明损失函数（请参阅帖子底部）正在接收尺寸为 256x256x1x20 而不是预期的 256x256x3x20 的输入数据 x。数据似乎没有从 DataLoader 正确传递到 Learner。
如何正确地将数据从 DataLoader 传递到 FluxTraining.jl 中的 Learner？
在使用 FluxTraining 之前，我能够接受相关培训
Flux.train!(loss, Flux.params(u),rep, opt, cb = () -&gt; @show(loss(w, wp)))，其中 rep =迭代器.repeated((w, wp), 100)。在所有情况下，ADAM() 都是我的优化器（opt）。
作为参考，我的损失函数是：
函数损失(x, y)
    @显示尺寸(x)
    @显示尺寸(y)
    Flux.dice_coeff_loss(u(x), y)
结尾

我尝试了对代码的各种修改，例如将 DataLoader 语法更改为 DataLoader((w,w), 4) 或 DataLoader(w, 4) ，但我仍然面临以下问题：要么将单个 Float32 而不是数组传递到模型中，要么输入数据的维度仍然不正确。
我还尝试循环遍历训练器中的所有 xs 和 ys 并调用损失函数。在这种情况下，它工作得很好，所以我认为这与我使用纪元的方式不一样！功能。]]></description>
      <guid>https://stackoverflow.com/questions/78175117/trouble-with-passing-data-from-dataloader-to-learner-in-fluxtraining-jl-for-unet</guid>
      <pubDate>Sun, 17 Mar 2024 11:50:28 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归实现 - 损失不收敛且模型结果不佳</title>
      <link>https://stackoverflow.com/questions/78175088/logistic-regression-implementation-loss-is-not-converging-and-poor-model-resul</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78175088/logistic-regression-implementation-loss-is-not-converging-and-poor-model-resul</guid>
      <pubDate>Sun, 17 Mar 2024 11:42:37 GMT</pubDate>
    </item>
    <item>
      <title>如何使用机器学习从视频中提取运动数据并将该数据存储在某种动画文件中，例如 .fbx？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78174587/how-to-extract-movement-data-from-video-and-store-that-data-in-some-kind-of-anim</link>
      <description><![CDATA[我想从视频中提取运动数据并将该数据存储在某种动画文件中，例如 .fbx。我的主要目的是训练音频输入的运动数据，然后，我可以根据给定的音频生成动画。有人可以帮助我吗？]]></description>
      <guid>https://stackoverflow.com/questions/78174587/how-to-extract-movement-data-from-video-and-store-that-data-in-some-kind-of-anim</guid>
      <pubDate>Sun, 17 Mar 2024 08:55:10 GMT</pubDate>
    </item>
    <item>
      <title>时间序列中未来批次的预测[关闭]</title>
      <link>https://stackoverflow.com/questions/78173753/predicition-of-future-batches-in-time-series</link>
      <description><![CDATA[我正在使用神经网络，我想进一步预测时间序列。我上了一门关于神经网络的课程，就遇到了这类问题。但我不太明白它是如何工作的。
def model_forecast(模型、系列、window_size):
    ds = tf.data.Dataset.from_tensor_slices(系列)
    ds = ds.window(window_size, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size))
    ds = ds.batch(32).prefetch(1)
    预测 = model.predict(ds)
    回报预测

rnn_forecast = model_forecast(模型, x_valid[:, np.newaxis], window_size)
rnn_forecast = rnn_forecast[:, 0]

图, ax = plt.subplots(figsize=(12, 6))

ax.plot(time_valid, x_valid, &#39;k-&#39;, label=&#39;验证数据&#39;)
ax.plot(time_valid[window_size-1:], rnn_forecast, &#39;r-&#39;, label=&#39;RNN 预测&#39;)
ax.plot(time_train, x_train, &#39;b-&#39;, label=&#39;训练数据&#39;)
斧头.设置（
    xlabel=&#39;日期&#39;,
    ylabel=&#39;太阳黑子数量&#39;,
    title=&#39;月平均太阳黑子数量&#39;,
）
plt.图例()

这是代码，它工作正常。我的问题是为什么预测函数中需要验证序列？这不正是我想要预测的吗？或者我是否将其解析为该系列的最后一个已知帧？我认为训练完成后，我应该只需要 test_time 并获取 x_valid 作为输出。]]></description>
      <guid>https://stackoverflow.com/questions/78173753/predicition-of-future-batches-in-time-series</guid>
      <pubDate>Sun, 17 Mar 2024 00:32:16 GMT</pubDate>
    </item>
    <item>
      <title>关于机器学习和数值训练数据的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78172418/question-about-machine-learning-and-numerical-training-data</link>
      <description><![CDATA[如果您使用 is_away 之类的内容作为数据中的数字字段，人工智能将对其进行训练以预测团队获胜的可能性。所以 1 代表 true，0 代表 false，那么是否应该将其更改为 is_home 之类的内容并翻转值，或者人工智能最终会在预测诸如获胜机会之类的内容时了解到 0 更有可能获胜？
我认为另一个很好的例子是海拔高度，而你的目标值是点。在大多数情况下，海拔越高，得分越少或赛道时间越慢。我假设人工智能理解海拔越高意味着它更有可能预测较低的目标值。在看到一些奇怪的预测后，我将玩家的高度提高了 5k，并在一场游戏中复制了所有其他字段，并且它总是预测该游戏的目标值更高。所以我对人工智能如何处理更高的数值感到困惑。
另请注意，我正在使用 relu 激活和 500k 行数据。我将随机更改单行的高度并使用相同的参数重新训练。与之前的训练数据相比，该行的预测值将从 20 变为 25，其他任何事情都不会发生变化...所以总结一下我的问题，应该反转对预测目标值产生负面影响的数值数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/78172418/question-about-machine-learning-and-numerical-training-data</guid>
      <pubDate>Sat, 16 Mar 2024 15:57:58 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：无法解释指标标识符：使用 Keras 和 Scikeras 造成损失</title>
      <link>https://stackoverflow.com/questions/78172101/valueerror-could-not-interpret-metric-identifier-loss-using-keras-and-scikeras</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78172101/valueerror-could-not-interpret-metric-identifier-loss-using-keras-and-scikeras</guid>
      <pubDate>Sat, 16 Mar 2024 14:19:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML Kit 的自定义模型返回不正确的边界框，与预览不匹配</title>
      <link>https://stackoverflow.com/questions/78171749/custom-model-with-ml-kit-returning-incorrect-bounding-boxes-mismatch-with-previ</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78171749/custom-model-with-ml-kit-returning-incorrect-bounding-boxes-mismatch-with-previ</guid>
      <pubDate>Sat, 16 Mar 2024 12:28:50 GMT</pubDate>
    </item>
    <item>
      <title>代码有问题还是我的数据有问题？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78171320/is-there-something-wrong-with-the-code-or-will-the-problem-be-in-my-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78171320/is-there-something-wrong-with-the-code-or-will-the-problem-be-in-my-data</guid>
      <pubDate>Sat, 16 Mar 2024 10:00:18 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降算法中，如何导出-2*wx</title>
      <link>https://stackoverflow.com/questions/78171263/in-gradient-descent-algorithm-how-to-induce-2wx</link>
      <description><![CDATA[梯度下降算法的一部分
this.updateWeights = function() {
 
  让wx;
  让w_deriv = 0;
  让 b_deriv = 0;

  for (让 i = 0; i &lt; this.points; i++) {
    wx = this.yArr[i] - (this.weight * this.xArr[i] + this.bias);
    w_deriv += -2 * wx * this.xArr[i];
    b_deriv += -2 * wx;
  }
  
  this.weight -= (w_deriv / this.points) * this.learnc;
  this.bias -= (b_deriv / this.points) * this.learnc;
}
            

请解释一下这部分！！
-2 * wx * this.xArr[i]

这部分是诱导出来的......？
如何通过数学公式归纳...]]></description>
      <guid>https://stackoverflow.com/questions/78171263/in-gradient-descent-algorithm-how-to-induce-2wx</guid>
      <pubDate>Sat, 16 Mar 2024 09:41:21 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中的超参数调整和模型评估 [关闭]</title>
      <link>https://stackoverflow.com/questions/78171227/hyperparameter-tuning-and-model-evaluation-in-scikit-learn</link>
      <description><![CDATA[我对如何正确使用超参数调整和模型评估感到有点困惑。
超参数调整应该在整个数据集上进行还是仅在训练集上进行？正确的操作顺序是什么？
您能否检查我的代码并建议我考虑该问题的最佳实践？
在这里，我首先对整个数据集使用超参数调整，然后仅在训练集上评估模型性能。这是对的吗？不会导致数据泄露吗？
超参数调优
numeric_features = X.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).columns
categorical_features = X.select_dtypes(include=[&#39;object&#39;, &#39;category&#39;]).columns

预处理器 = ColumnTransformer(
    变形金刚=[
        (&#39;num&#39;, StandardScaler(), numeric_features),
        (&#39;猫&#39;, OneHotEncoder(handle_unknown=&#39;忽略&#39;), categorical_features)
    ]
）

en_cv = ElasticNetCV(l1_ratio=np.arange(0, 1.1, 0.1),
                     alpha = np.arange(0, 1.1, 0.1),
                     随机状态=818，
                     职位数 = -1)

模型= make_pipeline（预处理器，en_cv）
模型.fit(X, y)

best_alpha = en_cv.alpha_
best_l1_ratio = en_cv.l1_ratio_

模型评估：
ElasticNet = make_pipeline(预处理器, ElasticNet(alpha=best_alpha, l1_ratio=l1_ratio))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=818)

ElasticNet.fit(X_train, y_train)
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mse = 均方误差(y_test, y_pred)

打印（r2，MSE）

实际上，这段代码在包含约 80000 个观察值和约 150 列的数据集上运行大约需要 18 分钟。这是否足够？]]></description>
      <guid>https://stackoverflow.com/questions/78171227/hyperparameter-tuning-and-model-evaluation-in-scikit-learn</guid>
      <pubDate>Sat, 16 Mar 2024 09:27:26 GMT</pubDate>
    </item>
    <item>
      <title>从二维输入预测多个输出的回归问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</link>
      <description><![CDATA[我有几个二维图表，每个图表都有八个独特的数字特征，可用于生成这些图表。我以大量 CSV 文件的形式获得了所有这些图表的 x 和 y 坐标及其数值特征。我想通过使用机器学习或深度学习模型来预测每个图的数值特征（通过使用图的图像或使用每个图的点的坐标）
例如，这是我的一张图表：

该图的独特数值特征为[1, 8.76e15, 8e-1, 5e-2, 5e-3, 5e-2, 9.65e-1, 2.1e-9]（I以两列 CSV 文件的形式提供该图所有点的坐标对 (x, y)，我也可以使用它们。）。
到目前为止，我已经寻找了很多预训练的模型，并在 HuggingFace 等网站上搜索了此类模型，还在 GitHub 代码中搜索了很多。我还在 Papers with Code 网站上搜索了做过同样事情的文章，但不幸的是，我仍然没有找到任何东西！我曾多次尝试自己编写一个网络，但由于这样做的复杂性以及对如何设置网络的超参数以达到预期结果的了解不够，我遇到了很多错误并且无法做到这一点！
我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78170872/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</guid>
      <pubDate>Sat, 16 Mar 2024 07:03:13 GMT</pubDate>
    </item>
    <item>
      <title>实时检测在YOLOv8中播放音频文件</title>
      <link>https://stackoverflow.com/questions/78170802/playing-audio-file-in-yolov8-in-real-time-detection</link>
      <description><![CDATA[我正在 YOLOv8 项目中工作，以检测困倦并在检测到困倦时播放警报音频文件。我面临的问题是我无法实时播放音频，因为我的检测首先存储在结果中。一旦我关闭检测窗口，它就会访问结果中存储的数据并连续播放音频。我该如何解决这个问题？
导入操作系统
从 ultralytics 导入 YOLO

进口火炬
导入 matplotlib
将 numpy 导入为 np
导入CV2
导入pygame

pygame.init()
sound_to_play = pygame.mixer.Sound(r&#39;D:\ML\同步警惕驱动程序\alarm.wav&#39;)
sound_to_play.play()

模型 = YOLO(r&#39;C:\Users\HP\Downloads\last.pt&#39;)

上限 = cv2.VideoCapture(0)
而真实：
    ret, 框架 = cap.read()

    结果 = model.predict(source=“0”,show=True)
    对于结果中的 r：
        如果 len(r.boxes.cls)&gt;0:
            dclass=r.boxes.cls[0].item()
            打印（d类）
            如果 dclass==2.0:
              sound_to_play.play()
    如果 cv2.waitKey(1) == ord(&#39;q&#39;):
        休息

pygame.quit()
cap.release()
cv2.destroyAllWindows()

问题是我的代码首先进行检测并将其存储在结果中，然后进入 for 循环。预期输出是它同时检测并检查类值。]]></description>
      <guid>https://stackoverflow.com/questions/78170802/playing-audio-file-in-yolov8-in-real-time-detection</guid>
      <pubDate>Sat, 16 Mar 2024 06:28:04 GMT</pubDate>
    </item>
    <item>
      <title>机器学习模型仅返回 0 分。我做错了什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78170278/the-machine-learning-models-are-only-returning-a-score-of-0-what-am-i-doing-wro</link>
      <description><![CDATA[在 Jupyter-Notebook 中，我创建了一个函数，可以对不同的 sklearn 机器学习模型进行拟合和评分。使用的数据集有超过 400000 行和 103 列，因此我分为两个不同的数据集：训练数据集和验证数据集。但是当我在函数中使用数据时，我想要测试的所有 4 个模型的得分均为 0。
这是我的代码：
# 分割数据集
df_val = df_tmp[df_tmp[&#39;年份&#39;] == 2012]
df_train = df_tmp[df_tmp[&#39;年份&#39;] != 2012]

# 将数据集分为训练和测试
X_train, y_train = df_train.drop(&#39;年&#39;, axis=1), df_train[&#39;年&#39;]
X_val, y_val = df_val.drop(&#39;年份&#39;, axis=1), df_val[&#39;年份&#39;]

# 将模型放入字典中
测试模型 = {
    “套索”：套索()，
    “ElasticNet”：ElasticNet()，
    “RandomForestRegressor”：RandomForestRegressor()，
    “山脊”：山脊()
}

# 创建函数来评估两个模型
def fit_and_score(test_models, X_train, X_val, y_train, y_val):
    
    # 记录模型分数的字典
    模型分数 = {}
    
    ＃ 环形
    for name, model in test_models.items(): # name, model = key, value
        # 拟合模型
        model.fit(X_train, y_train)
        # 评估模型并将其分数附加到 models_scores
        models_scores[名称] = model.score(X_val, y_val)
        
    返回模型分数

首先我想也许我没有正确编写函数，所以我单独测试了模型，它们仍然得分为 0。之后我决定测试是否我的数据有问题（我不认为这是它，bcz 我从一个旧的 Kaggle 竞赛中得到它，推土机竞赛），所以我用相同的数据训练并安装了一个模型，希望我的分数是 1，但我得到了 0.31。我真的不知道该怎么办]]></description>
      <guid>https://stackoverflow.com/questions/78170278/the-machine-learning-models-are-only-returning-a-score-of-0-what-am-i-doing-wro</guid>
      <pubDate>Sat, 16 Mar 2024 01:04:49 GMT</pubDate>
    </item>
    <item>
      <title>验证损失根本没有改变</title>
      <link>https://stackoverflow.com/questions/72446953/validation-loss-is-not-changing-at-all</link>
      <description><![CDATA[我第一次使用 PyTorch 来使用 Bert 的预训练模型来训练我的情绪分析模型。
这是我的分类器
类 SentimentClassifier2(nn.Module):

  def __init__(self, n_classes):
    super(SentimentClassifier2, self).__init__()
    D_输入、H、D_输出 = 768、200、3

    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
    self.drop = nn.Dropout(p=0.4)

    self.classifier = nn.Sequential(
            nn.Linear(D_in, H),
            ReLU(),
            nn.Linear(H, D_out)
    ）
  defforward（自身，input_ids，attention_mask）：

         _，pooled_output = self.bert（input_ids = input_ids，attention_mask = attention_mask，return_dict = False）
         输出 = self.drop(pooled_output)
         logits = self.classifier(输出)
         返回逻辑值

这是我的优化器/损失函数（我只做了 20 个周期，因为训练需要一段时间）
EPOCHS = 20

model2 = SentimentClassifier2(len(class_names))
model2= model2.to(设备)

优化器 = AdamW(model.parameters(), lr=2e-5, Correct_bias=True)

总步数 = len(train_data_loader) * EPOCHS

调度程序 = get_linear_schedule_with_warmup(
  优化器，
  num_warmup_steps=0,
  num_training_steps=total_steps
）
loss_fn = nn.CrossEntropyLoss().to(设备)

培训与培训评估代码
def train_epoch（模型，data_loader，loss_fn，优化器，设备，调度程序，n_examples）：
  模型 = model.train()
  损失=[]
  正确预测 = 0
  对于 data_loader 中的 d：
    input_ids = d[“input_ids”].to(设备)
    注意掩码 = d[“注意掩码”].to(设备)
    目标 = d[“目标”].to(设备)

    输出=模型（
      输入ID=输入ID，
      注意掩码=注意掩码
    ）

    _, preds = torch.max(输出, 暗淡=1)
    损失 = loss_fn(输出，目标)

    Correct_predictions += torch.sum(preds == 目标)
    损失.追加（损失.项目（））

    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    优化器.step()
    调度程序.step()
    优化器.zero_grad()

  返回 Correct_predictions.double() / n_examples, np.mean(losses)


def eval_model（模型，data_loader，loss_fn，设备，n_examples）：
  模型 = model.eval()

  损失=[]
  正确预测 = 0
  使用 torch.no_grad()：
    对于 data_loader 中的 d：
      input_ids = d[“input_ids”].to(设备)
      注意掩码 = d[“注意掩码”].to(设备)
      目标 = d[“目标”].to(设备)

      输出=模型（
        输入ID=输入ID，
        注意掩码=注意掩码
      ）

      _, preds = torch.max(输出, 暗淡=1)
      损失 = loss_fn(输出，目标)

      Correct_predictions += torch.sum(preds == 目标)
      损失.追加（损失.项目（））

  返回 Correct_predictions.double() / n_examples, np.mean(losses)

我的问题：验证样本的损失根本没有改变！
&lt;前&gt;&lt;代码&gt;纪元1：______________________
列车损失 1.0145157482929346 准确度 0.4185746994848311
价值损失 1.002384223589083 准确度 0.4151087371232354
纪元2：______________________
列车损失 1.015038197996413 准确度 0.41871780194619346
价值损失 1.002384223589083 准确度 0.4151087371232354
epoch3：______________________
列车损失 1.014710763787351 准确度 0.4188609044075558
价值损失 1.002384223589083 准确度 0.4151087371232354
epoch4：______________________
列车损失 1.0139196826735648 准确度 0.41909940850982635
价值损失 1.002384223589083 准确度 0.4151087371232354

我不明白问题出在哪里......]]></description>
      <guid>https://stackoverflow.com/questions/72446953/validation-loss-is-not-changing-at-all</guid>
      <pubDate>Tue, 31 May 2022 11:26:27 GMT</pubDate>
    </item>
    </channel>
</rss>