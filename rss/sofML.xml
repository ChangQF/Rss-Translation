<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 24 Aug 2024 09:14:46 GMT</lastBuildDate>
    <item>
      <title>Resnet 内存不足：torch.OutOfMemoryError：CUDA 内存不足</title>
      <link>https://stackoverflow.com/questions/78908540/resnet-out-of-memory-torch-outofmemoryerror-cuda-out-of-memory</link>
      <description><![CDATA[我正在针对视频任务训练端到端模型。我使用 Pytorch ResNet50 作为编码器，输入形状为 (1,seq_length,3,224,224)，其中 seq_length 是每个视频中的帧数。例如，如果视频 1 有 1500 帧，则输入形状为 (1,1500,3,224,224)，如果视频 2 有 2000 帧，则输入形状为 (1,2000,3,224,224)。但是，当我将输入提供给 Resnet 时，CUDA 在通过前向传递中的第一个卷积层时会耗尽内存。
我尝试过：

torch.cuda.empty_cache()
在 dataloader 中设置 pin_memory=True 和 prefetch_factor=2
设置 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
减少 seq_length。这不起作用，因为它会显著影响性能，视频是一系列数据，减少 seq_length 会破坏这个结构。

有什么技巧可以解决这个问题吗？任何帮助都非常感谢
以下是完整的错误消息
回溯（最近一次调用最后一次）：
文件“Path/E2E.py”，第 143 行，位于&lt;module&gt;
p_classes1, phase_preds = model1(long_feature)
文件“Path/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1553 行，在 _wrapped_call_impl 中
return self._call_impl(*args, **kwargs)
文件“Path/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1562 行，在 _call_impl 中
return forward_call(*args, **kwargs)
文件“Path/E2E.py”，第 47 行，在 forward 中
x = self.resnet_lstm(x)
文件“Path/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1553 行，在_wrapped_call_impl
返回 self._call_impl(*args, **kwargs)
文件“Path/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1562 行，在 _call_impl 中
返回 forward_call(*args, **kwargs)
文件“Path/train_embedding.py”，第 226 行，在 forward 中
x = self.share.forward(x)
文件“Path/lib/python3.9/site-packages/torch/nn/modules/container.py”，第 219 行，在 forward 中
输入 = module(input)
文件“Path/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1553 行，在 _wrapped_call_impl 中
返回self._call_impl(*args, **kwargs)
文件“Path/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1562 行，在 _call_impl 中
return forward_call(*args, **kwargs)
文件“Path/lib/python3.9/site-packages/torch/nn/modules/container.py”，第 219 行，在 forward 中
input = module(input)
文件“Path/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1553 行，在 _wrapped_call_impl 中
return self._call_impl(*args, **kwargs)
文件“Path/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1562 行1562，在 _call_impl 中
返回 forward_call(*args, **kwargs)
文件 &quot;Path/lib/python3.9/site-packages/torchvision/models/resnet.py&quot;，第 155 行，在 forward 中
out = self.bn3(out)
文件 &quot;Path/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;，第 1553 行，在 _wrapped_call_impl 中
返回 self._call_impl(*args, **kwargs)
文件 &quot;Path/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;，第 1562 行，在 _call_impl 中
返回 forward_call(*args, **kwargs)
文件&quot;Path/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py&quot;，第 176 行，在 forward 中
return F.batch_norm(
File &quot;Path/lib/python3.9/site-packages/torch/nn/ functional.py&quot;，第 2512 行，在 batch_norm 中
return torch.batch_norm(
torch.OutOfMemoryError: CUDA 内存不足。尝试分配 614.00 MiB。GPU 0 的总容量为 23.65 GiB，其中 353.38 MiB 是空闲的。包括非 PyTorch 内存在内，此进程使用了​​ 23.28 GiB 内存。在分配的内存中，22.84 GiB 由 PyTorch 分配，3.77 MiB 由 PyTorch 保留但未分配。如果保留但未分配的内存很大，请尝试设置 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True 以避免碎片化。请参阅内存管理文档 (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
]]></description>
      <guid>https://stackoverflow.com/questions/78908540/resnet-out-of-memory-torch-outofmemoryerror-cuda-out-of-memory</guid>
      <pubDate>Sat, 24 Aug 2024 08:53:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中创建具有点数据检索功能的交互式聚类可视化？</title>
      <link>https://stackoverflow.com/questions/78908500/how-to-create-an-interactive-clustering-visualization-with-point-data-retrieval</link>
      <description><![CDATA[我正在做一个项目，需要可视化数据点集群，并允许用户单击这些点来检索有关它们的详细信息
我已经实现了 K-means（或任何聚类算法，如 dbscan）聚类并使用 matplotlib 可视化了集群，但我不确定如何设置交互性，以便单击某个点显示其相关数据。
但我的主要问题是，我是否应该以特殊方式存储和构造数据以再次检索它们？例如，我想识别靠近集群边界的点并检索它们的信息（可以通过单击或工具提示或任何方法）]]></description>
      <guid>https://stackoverflow.com/questions/78908500/how-to-create-an-interactive-clustering-visualization-with-point-data-retrieval</guid>
      <pubDate>Sat, 24 Aug 2024 08:29:56 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何在包含其他业务逻辑的主 Web 应用程序中部署我的 ML 模型？</title>
      <link>https://stackoverflow.com/questions/78908377/how-should-i-deploy-my-ml-model-inside-main-web-app-containing-other-business-lo</link>
      <description><![CDATA[我正在为我的学期做一个小项目。我正在使用 flask。

我在这个应用程序中有一些业务逻辑，其中收集了一些数据。
我想将这些数据发送到我的 ML 模型。
那么我应该在不同的服务器上部署我的 ML 模型并使用其端点吗？
这不会增加响应时间吗？
此外，由于我将其部署在 render.com 或任何其他免费服务上的低内存免费服务器上。
我的主要业务逻辑也将占用一些内存。
此外，我应该如何分离我的 ML 模型，比如将它部署到整个其他服务器上并对其进行配置，然后将我的主要逻辑部署到其他服务器上？
那么这里应该做什么？
你的想法会对我这个项目有所帮助。
感谢您的宝贵回复
我正在创建一个名为 mainapp 的主 flask 应用程序。
在其中，我在单独的文件中编写了一些业务逻辑，并将它们作为包导入到我的 routes.py 文件中。
现在我也对 ML 模型做了同样的事情。这是一个简单的情感分析模型，我将传递句子并接收其分数。]]></description>
      <guid>https://stackoverflow.com/questions/78908377/how-should-i-deploy-my-ml-model-inside-main-web-app-containing-other-business-lo</guid>
      <pubDate>Sat, 24 Aug 2024 07:15:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 中的 RL 解决离散时间 LQR 问题</title>
      <link>https://stackoverflow.com/questions/78908049/struggling-to-solve-discrete-time-lqr-with-rl-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78908049/struggling-to-solve-discrete-time-lqr-with-rl-in-python</guid>
      <pubDate>Sat, 24 Aug 2024 03:11:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在调用 MessageGraph 期间传递多个输入？</title>
      <link>https://stackoverflow.com/questions/78907608/how-to-pass-multiple-inputs-during-invoke-to-a-messagegraph</link>
      <description><![CDATA[我们有一个用于 LLMCompiler 实现的 MessageGraph，并且正如预期的那样，我们在运行调用时将用户的问题作为 HumanMessage 对象列表传递（这些对象映射到默认的&quot;messages&quot; 键并传递给提示模板），这对于简单的用例来说工作得很好，但现在我们需要在调用时（而不是在构建图形时）传递额外的信息/上下文，我们对 React 代理做了类似的事情，并且很容易传递类似于 invoke({&quot;messages&quot;:input, &quot;context&quot;:context}) 的字典。但对于 MessageGraph，这不起作用，看起来运行 invoke(messages) 时传递的消息列表会在提示中自动映射到&quot;messages&quot;键，无法添加其他输入，我尝试传递一个字典 invoke({&quot;messages&quot;:messages, &quot;context&quot;:context})，但没有成功，错误失败：

消息字典必须包含“role”和“content”键，得到
{&quot;messages&quot;:messages,&quot;context&quot;:context
]]></description>
      <guid>https://stackoverflow.com/questions/78907608/how-to-pass-multiple-inputs-during-invoke-to-a-messagegraph</guid>
      <pubDate>Fri, 23 Aug 2024 21:30:27 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Google Colab 中运行自定义 Segment Anything 模型？</title>
      <link>https://stackoverflow.com/questions/78907597/how-can-i-run-a-custom-segment-anything-model-in-google-colab</link>
      <description><![CDATA[由于我的笔记本电脑没有必要的硬件，我试图在 Google Colab 中运行此 SAM 模型：https://github.com/htcr/sam_road，但我很迷茫。该存储库在 README 中有设置说明，但我不确定如何使用 Colab 遵循这些说明。
该存储库是公开的，但我需要令牌权限才能将存储库直接安装到 Colab 中，因此我想我可以将代码复制并粘贴到 Colab 中，但当然有很多代码需要复制。如果复制/粘贴是解决此问题的正确方法，我应该如何在 Colab 笔记本中组织代码（因为我需要复制所有存储库的代码）？或者我应该用其他方式来运行模型而不是复制/粘贴？
我尝试过进行一些复制/粘贴，但粘贴到一个代码文件中包含依赖于 repo 中其他代码文件的导入语句，但由于 Colab 笔记本似乎只有一页代码块，我不确定我是否可以每个代码块只做一个代码文件，或者是否需要任何特定的顺序。]]></description>
      <guid>https://stackoverflow.com/questions/78907597/how-can-i-run-a-custom-segment-anything-model-in-google-colab</guid>
      <pubDate>Fri, 23 Aug 2024 21:24:12 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv10 自定义训练——导入 YOLO 还是 YOLOv10？</title>
      <link>https://stackoverflow.com/questions/78907557/yolov10-custom-training-import-yolo-or-yolov10</link>
      <description><![CDATA[我希望对我拥有的数据集使用 YoloV10n 进行自定义训练（到目前为止我一直在使用 YoloV8），但我不确定是否要使用/导入 YOLO 或 YOLOv10。
我最初尝试使用 YOLOv10，并能够成功完成自定义训练和推理。但是，由于断言不一致，我无法将模型导出到 tflite。然后我切换回 YOLO（从预先训练的 yolov10.pt 模型开始），并能够将其导出到 tflite。此外，Ultralytics 的官方文档确实指导如何使用 YOLO（而不是 YOLOv10）进行 YoloV10 训练。另一方面，RoboFlow 教程确实指导如何使用 YOLOv10... 🤔
我应该使用 from ultralytics import YOLOv10 还是 from ultralytics import YOLO？这有关系吗？对于训练、推理和导出（tflite 等），答案是否相同？]]></description>
      <guid>https://stackoverflow.com/questions/78907557/yolov10-custom-training-import-yolo-or-yolov10</guid>
      <pubDate>Fri, 23 Aug 2024 21:07:49 GMT</pubDate>
    </item>
    <item>
      <title>AWS SageMaker 预测和测试数据</title>
      <link>https://stackoverflow.com/questions/78907538/aws-sagemaker-predictions-and-test-data</link>
      <description><![CDATA[import pandas as pd
import itertools
import numpy as np
import s3fs
from sagemaker.predictor import Predictor
from sagemaker.serializers import CSVSerializer

# 为您的 CSV 文件定义 S3 路径
import pandas as pd
import s3fs

# 为您的 CSV 文件定义 S3 路径
s3_path = &quot;s3://{}/{}/{}.csv&quot;

# 带有附加检查的读取文件函数
def read_and_check_csv(s3_path):
fs = s3fs.S3FileSystem()
with fs.open(s3_path) as f:
try:
# 尝试读取 CSV 文件
df = pd.read_csv(f, header=None, low_memory=False)
# 检查行长度是否一致
if not df.apply(lambda x: len(x.dropna()), axis=1).nunique() == 1:
raise ValueError(&quot;检测到不一致的行长度&quot;)
print(&quot;文件读取成功，似乎为 CSV 格式。&quot;)
return df
except Exception as e:
print(f&quot;无法读取 CSV 文件：{e}&quot;)
return None

# 读取并检查 CSV 文件
df = read_and_check_csv(s3_path)

如果 df 不为 None:
print(df.head())
否则:
print(&quot;文件无法读取或不是有效的 CSV 格式。&quot;)

# 定义用于切片数据的索引
a = [50 * i for i in range(3)]
b = [40 + i for i in range(10)]
indices = [i + j for i, j in itertools.product(a, b)]

# 准备测试数据
test_data = shape.iloc[indices[:-1]]
test_X = test_data.iloc[:, 1:]

# 确保所有行的列数相同
min_cols = test_X.shape[1]
test_X = test_X.dropna(axis=1, how=&#39;all&#39;) # 删除所有 NaN 值的列

# 验证没有具有不同值的行长度
test_X = test_X.apply(lambda x: x.dropna().reset_index(drop=True), axis=1)

# 使用 SageMaker 端点名称初始化预测器
predictor = Predictor(endpoint_name=&#39;sagemaker-xgboost-2024-08-23-19-59-10-793&#39;)

# 确保预测器使用 CSV 序列化器
predictor.serializer = CSVSerializer()

# 将 DataFrame 转换为端点所需的格式
test_X_csv = test_X.to_csv(index=False, header=False, sep=&#39;,&#39;)

# 进行预测
try:
predictions = predictor.predict(test_X_csv)
# 打印预测
print(predictions.decode(&#39;utf-8&#39;))
except Exception as e:
print(f&quot;Error making预测：{e}&quot;)


我在 aws sagemaker Jupyter 实验室中为我的 xgboost 框架使用上述脚本。在此脚本之前，我正在运行以下代码来设置端点。
predictor = estimator.deploy(
initial_instance_count=1, instance_type=&quot;ml.m5.2xlarge&quot;
)

我添加了一些错误处理，这就是我了解到我的测试文件似乎没有被正确读取的地方。我得到的实际错误是：
无法读取 CSV 文件：检测到不一致的行长度
无法读取文件或文件不是有效的 CSV 格式。
进行预测时出错：调用 InvokeEndpoint 操作时发生错误 (ModelError)：从主服务器收到客户端错误 (415)，消息为“加载 csv 数据失败，出现异常，请确保数据为 csv 格式：
&lt;class &#39;ValueError&#39;&gt;
设置带有序列的数组元素。请求的数组在 1 维之后具有非均匀形状。检测到的形状为 (29,) + 非均匀部分。”

关于如何修复此问题有任何见解吗？]]></description>
      <guid>https://stackoverflow.com/questions/78907538/aws-sagemaker-predictions-and-test-data</guid>
      <pubDate>Fri, 23 Aug 2024 20:58:39 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助验证谷歌/机器学习课程提供的数据</title>
      <link>https://stackoverflow.com/questions/78907434/need-help-verifying-data-provided-by-google-machine-learning-course</link>
      <description><![CDATA[在谷歌提供的机器学习课程简介中的梯度下降页面中，提供了特征和相应的标签、MSE 损失函数、初始数据集和结果。我很难验证他们的结果，我想知道是否有人可以帮助确认我是否犯了错误或者他们犯了错误。
我有以下内容：
import pandas as pd
import numpy as np

data = [3.5, 18], [3.69, 15], [3.44, 18], [3.43, 16], [4.34, 15], [4.42, 14], [2.37, 24]
initial_data_df = pd.DataFrame(data,columns=[&#39;pounds&#39;,&#39;mpg&#39;])

number_of_iterations = 6
weight = 0 # 初始化权重
bias = 0 # 初始化权重
weight_slope = 0
bias_slope = 0
final_results_df = pd.DataFrame()
learning_rate = 0.01

对于 i 在范围内（迭代次数）：
loss = calculate_loss（初始数据 df、权重、偏差）
final_results_df = update_results（最终结果 df、权重、偏差、损失）
weight_slope = find_weight_slope（初始数据 df、权重、偏差）
bias_slope = find_bias_slope（初始数据 df、权重、偏差）
weight = new_weight_update（权重、learning_rate、weight_slope）
bias = new_bias_update（偏差、learning_rate、bias_slope）
print（最终结果 df）

def calculate_loss（df、权重、偏差）：
loss_summation = []
对于 i 在范围内（0、len（df））：
loss_summation.append((df[&#39;mpg&#39;][i]-((weight*df[&#39;pounds&#39;][i])+bias))**2)
return (sum(loss_summation)//len(df))

def update_results(df,weight,bias,loss):
if df.empty:
df = pd.DataFrame([[weight,bias,loss]],columns=[&#39;weight&#39;,&#39;bias&#39;,&#39;loss&#39;])
else:
df = pd.concat([df,pd.DataFrame([[weight,bias,loss]],columns=df.columns)])
return df

def find_weight_slope(df,weight,bias):
weight_update_summation = []
for i in range(0,len(df)):
wx_plus_b = (weight*df[&#39;pounds&#39;][i])+bias
wx_plus_b_minus_y = wx_plus_b-df[&#39;mpg&#39;][i]
weight_update_summation.append(2*(wx_plus_b_minus_y*df[&#39;pounds&#39;][i]))
return sum(weight_update_summation)//len(df)

def find_bias_slope(df,weight,bias):
bias_update_summation = []
for i in range(0,len(df)):
wx_plus_b = (weight*df[&#39;pounds&#39;][i])+bias
wx_plus_b_minus_y = wx_plus_b-df[&#39;mpg&#39;][i]
bias_update_summation.append(2*wx_plus_b_minus_y)
total_sum = sum(bias_update_summation)
return total_sum//len(df)

def new_weight_update(old_weight,lr,slope):
return old_weight-1*lr*slope

def new_bias_update(old_bias,lr,slope):
return old_bias-1*lr*slope

得出：
iter weight bias loss
0 0.00 0.00 303.0
0 1.20 0.35 170.0
0 2.06 0.60 102.0
0 2.67 0.79 67.0
0 3.10 0.93 50.0
0 3.41 1.04 41.0

这与提供的解决方案不同在网站上：
迭代权重偏差损失（MSE）
1 0 0 303.71
2 1.2 0.34 170.67
3 2.75 0.59 67.3
4 3.17 0.72 50.63
5 3.47 0.82 42.1
6 3.68 0.9 37.74
]]></description>
      <guid>https://stackoverflow.com/questions/78907434/need-help-verifying-data-provided-by-google-machine-learning-course</guid>
      <pubDate>Fri, 23 Aug 2024 20:09:14 GMT</pubDate>
    </item>
    <item>
      <title>非正统的时间序列预处理方法[关闭]</title>
      <link>https://stackoverflow.com/questions/78903433/unorthodox-time-series-preprocessing-methods</link>
      <description><![CDATA[我正在写一篇关于时间序列预处理的学校论文。在这篇论文中，我想测试时间序列预处理的非正统方法。然后我想将数据输入 LSTM 模型并测量其在预测时间序列方面的准确性。所以我想问问社区我应该测试哪些新颖/非正统的方法。我不想测试任何经典方法，如去噪、正则化、季节性消除等。如果您有任何想法，请随时分享。]]></description>
      <guid>https://stackoverflow.com/questions/78903433/unorthodox-time-series-preprocessing-methods</guid>
      <pubDate>Thu, 22 Aug 2024 20:35:13 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能正确地连接不同的功能？</title>
      <link>https://stackoverflow.com/questions/78900219/how-can-i-properly-conctenate-the-different-features</link>
      <description><![CDATA[我有一个形状为 (100,48) 的特征图，以及其他形状为 (100,1) 的特征
我该如何正确地连接这些不同的特征？
我想这样做是因为我现在正在训练 XGboost 模型，但如果我将不同的特征直接连接在一起，机器就无法区分特征图和单个特征。
我尝试了下面的代码（来自 keras 函数 API），但我认为这不是正确的方法，每次我操作代码（没有随机种子）时，这样做的结果都不同。
input_1_layer = Input(shape=(input_1.shape[1],)) density_1 = Dense(48,activation=&#39;relu&#39;)(input_1_layer)

input_2_layer = Input(shape=(input_2.shape[1],)) density_2 = Dense(6,激活=&#39;relu&#39;)(输入层 2)

feature_extractor = Model(输入=[输入层 1, 输入层 2], 输出=merged)

new_features = feature_extractor.predict([输入层 1, 输入层 2])

你能提供任何论文或网站吗？]]></description>
      <guid>https://stackoverflow.com/questions/78900219/how-can-i-properly-conctenate-the-different-features</guid>
      <pubDate>Thu, 22 Aug 2024 07:19:22 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hugging Face Transformers 训练 GPT-2 模型时如何修复分段错误？</title>
      <link>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</guid>
      <pubDate>Tue, 06 Aug 2024 21:47:06 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 R 包“敏感性”执行 Sobol 敏感性分析？</title>
      <link>https://stackoverflow.com/questions/76444438/how-to-perform-a-sobol-sensitivity-analysis-using-r-package-sensitivity</link>
      <description><![CDATA[我想使用 R 包“sensitivity”执行 Sobol 敏感性分析。但是，我不确定如何在 sobol 函数中创建第一和第二个随机样本 (X1, X2)。我假设 X1 和 X2 都是输入数据的子集，但最终结果似乎不正确。
library(tidymodels)
library(sensitivity)

# 示例数据
set.seed(123)
x1 = runif(100)
x2 = runif(100)
x3 = runif(100)
y = 3 * x1 + 2 * x2 + x3 + rnorm(100)
data &lt;- data.frame(x1, x2, x3, y)

# 将数据拆分为训练集和测试集
set.seed(234)
data_split &lt;- initial_split(data, prop = 0.8)
train_data &lt;- training(data_split)
test_data &lt;- testing(data_split)

# 使用创建线性回归模型tidymodels
lm_spec &lt;- linear_reg() %&gt;%
set_engine(&quot;lm&quot;) %&gt;%
set_mode(&quot;regression&quot;)

lm_fit &lt;- lm_spec %&gt;%
fit(y ~ x1 + x2 + x3, data = train_data)

# 定义模型函数
model_function &lt;- function(x) {
new_data &lt;- data.frame(x1 = x[, 1], x2 = x[, 2], x3 = x[, 3])
predict(lm_fit, new_data)$`.pred`
}

# 执行 Sobol 敏感性分析
set.seed(345)
X_index1 = sample(x=1:100, size = 50, replace = FALSE)
X_index2 = c(1:length(data$x1))[-X_index1]

sobol_results &lt;- sobol(model = model_function, 
X1 = data[X_index1, -4], 
X2 = data[X_index2, -4], 
nboot = 1000, order = 2)

sobol_results

sobol_results 显示敏感度顺序为：x2&gt;x1&gt;x3。根据函数 y = 3 * x1 + 2 * x2 + x3 + rnorm(100)，“x1”应该具有更高的敏感度，因为它是 3 * x1。我应该如何修正我的代码？谢谢。
]]></description>
      <guid>https://stackoverflow.com/questions/76444438/how-to-perform-a-sobol-sensitivity-analysis-using-r-package-sensitivity</guid>
      <pubDate>Sat, 10 Jun 2023 01:54:12 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow model.evaluate 给出的结果与训练得到的结果不同</title>
      <link>https://stackoverflow.com/questions/64049608/tensorflow-model-evaluate-gives-different-result-from-that-obtained-from-trainin</link>
      <description><![CDATA[我正在使用 tensorflow 进行多类分类
我以以下方式加载训练数据集和验证数据集
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;training&quot;,
shuffle=True,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
data_dir,
validation_split=0.2,
subset=&quot;validation&quot;,
shuffle=True,
seed=123,
image_size=(img_height, img_width),
batch_size=batch_size)

然后当我使用 model.fit() 训练模型
history = model.fit(
train_ds,
validation_data=val_ds,
epochs=epochs,
shuffle=True
)

我的验证准确率约为 95%。
但是当我加载相同的验证集并使用 model.evaluate() 时
model.evaluate(val_ds)

我的准确率非常低（约为 10%）。
为什么我得到的结果如此不同？我是否错误地使用了 model.evaluate 函数？
注意：在 model.compile() 中，我指定了以下内容，
优化器 - Adam，
损失 - SparseCategoricalCrossentropy，
指标 - 准确度
Model.evaluate() 输出
41/41 [================================] - 5s 118ms/step - 损失：0.3037 - 准确度：0.1032
测试损失 - 0.3036555051803589
测试准确度 - 0.10315627604722977

最后三个时期的 Model.fit() 输出
时期8/10
41/41 [==============================] - 3s 80ms/步 - 损失：0.6094 - 准确度：0.8861 - val_loss：0.4489 - val_accuracy：0.9483
Epoch 9/10
41/41 [=============================] - 3s 80ms/步 - 损失：0.5377 - 准确度：0.8953 - val_loss：0.3868 - val_accuracy：0.9554
Epoch 10/10
41/41 [==============================] - 3s 80ms/步 - 损失：0.4663 - 准确度：0.9092 - val_loss：0.3404 - val_accuracy：0.9590
]]></description>
      <guid>https://stackoverflow.com/questions/64049608/tensorflow-model-evaluate-gives-different-result-from-that-obtained-from-trainin</guid>
      <pubDate>Thu, 24 Sep 2020 15:26:53 GMT</pubDate>
    </item>
    <item>
      <title>训练期间改变模型</title>
      <link>https://stackoverflow.com/questions/36748574/changing-model-during-training</link>
      <description><![CDATA[我正在 TensorFlow 中创建一个模型，其中所有层都具有 relu 作为激活层。但是，当批处理大小增加到 500 时，我想更改模型，使输出层的倒数第二层具有 sigmoid 激活层。
我感到困惑的是，由于我在中间替换了优化器，我是否需要重新初始化所有变量？还是保留旧变量？]]></description>
      <guid>https://stackoverflow.com/questions/36748574/changing-model-during-training</guid>
      <pubDate>Wed, 20 Apr 2016 15:30:49 GMT</pubDate>
    </item>
    </channel>
</rss>