<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 16 May 2024 01:01:42 GMT</lastBuildDate>
    <item>
      <title>Sagemaker SDK 无法使用自定义算法运行训练</title>
      <link>https://stackoverflow.com/questions/78486992/unable-to-sagemaker-sdk-to-run-training-using-a-custom-algorithm</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78486992/unable-to-sagemaker-sdk-to-run-training-using-a-custom-algorithm</guid>
      <pubDate>Wed, 15 May 2024 23:54:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么 JAX 编译时间随着 vmap 批处理大小的增加而增长？</title>
      <link>https://stackoverflow.com/questions/78486071/why-does-jax-compilation-time-grow-with-vmap-batch-size</link>
      <description><![CDATA[我正在使用 JAX 来评估批量损失梯度，其中涉及一些复杂的线性代数（包括 Cholesky 分解和解决方案等）。我的梯度损失的示意图形式是
jax.jit( jax.value_and_grad( jax.vmap(loss)(...).mean() ) )

我发现编译/首次评估时间在给定 vmap 的特定批量大小之前是恒定的（正如我通常所期望的那样），然后开始超线性增长。在 A100 上，nbatch &lt;= 64 需要 6 分钟，nbatch=128 需要 13 分钟，nbatch=256 需要 1 小时，这变得很笨拙。
这里可能发生什么？如果内存或计算单元不足，jax.vmap 是否会尝试展开批处理？]]></description>
      <guid>https://stackoverflow.com/questions/78486071/why-does-jax-compilation-time-grow-with-vmap-batch-size</guid>
      <pubDate>Wed, 15 May 2024 19:17:27 GMT</pubDate>
    </item>
    <item>
      <title>我应该在 GitHub 上分享我的自我项目吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78484980/should-i-share-my-self-projects-on-github</link>
      <description><![CDATA[我一直在考虑是否应该在 GitHub 上分享我自己的项目。这些项目主要涉及机器学习，重点关注回归和分类任务。虽然我已经付出了努力，但我不确定是否值得清理代码并上传它。展示这些小项目实际上是否有益，或者最终只是浪费时间？]]></description>
      <guid>https://stackoverflow.com/questions/78484980/should-i-share-my-self-projects-on-github</guid>
      <pubDate>Wed, 15 May 2024 15:26:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Flask 时的机器学习问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78484656/machine-learning-issue-while-using-flask</link>
      <description><![CDATA[我有一个用于房屋预测的数据库。在这个数据集中，我有很多分类变量，所以我使用了一个热编码器，现在我有大约 40 列数据。我正在尝试为我的模型构建一个 API，尽管我无法传递正确的数据来进行预测。有什么办法可以实现它吗？
我尝试使用每一个变量并获取一个请求表单，然后我将它们放在一个数组中，尽管我遇到了错误。是否可以编写代码，说明如何在 Flask 中插入大数据集，以便 Postman 发送请求？]]></description>
      <guid>https://stackoverflow.com/questions/78484656/machine-learning-issue-while-using-flask</guid>
      <pubDate>Wed, 15 May 2024 14:32:08 GMT</pubDate>
    </item>
    <item>
      <title>有什么办法可以找到音频和视频之间的同步分数吗？</title>
      <link>https://stackoverflow.com/questions/78484136/is-there-any-way-to-find-synchronization-score-between-audio-and-video</link>
      <description><![CDATA[假设我们有一个扩展名为 .mp4 的静音视频文件和一个扩展名为 .wav 的音频文件。有什么办法可以找到音频和视频之间的同步分数。预先感谢您回答问题。
我使用深度学习模型进行视频特征提取，并使用另一种模型进行音频特征提取。现在我已经使用欧几里德距离和相关性来检查音频和视频特征之间的相似性，但它不起作用。我希望如果两个功能相似或同步，那么输出应该接近 1，否则接近 0。]]></description>
      <guid>https://stackoverflow.com/questions/78484136/is-there-any-way-to-find-synchronization-score-between-audio-and-video</guid>
      <pubDate>Wed, 15 May 2024 13:00:05 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Windows 机器中安装 Rasa</title>
      <link>https://stackoverflow.com/questions/78483192/unable-to-install-rasa-in-windows-machine</link>
      <description><![CDATA[我尝试在 Windows 10 笔记本电脑中使用命令 pip install rasa 安装 Rasa。
我安装了Python 3.11版本。
但我收到以下错误：
获取构建轮的要求未成功运行。
  │ 退出代码：1
  ╰─&gt; [20行输出]
      回溯（最近一次调用最后一次）：
        文件“C:\python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，行
     353，在&lt;模块&gt;中
          主要的（）
    文件“C:\python311\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py”，第 335 行，
    在主要
          json_out[&#39;return_val&#39;] = hook(**hook_input[&#39;kwargs&#39;])
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

我错过了什么？之前，我尝试安装chatterbot模块，但也遇到了类似的错误。
此外，我也尝试使用 pip install chatterbot ，但仍然出现错误。]]></description>
      <guid>https://stackoverflow.com/questions/78483192/unable-to-install-rasa-in-windows-machine</guid>
      <pubDate>Wed, 15 May 2024 10:10:42 GMT</pubDate>
    </item>
    <item>
      <title>随机森林和交叉验证</title>
      <link>https://stackoverflow.com/questions/78483176/random-forest-and-cross-validation</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78483176/random-forest-and-cross-validation</guid>
      <pubDate>Wed, 15 May 2024 10:08:38 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 TensorFlow 提高多类分类的准确性？</title>
      <link>https://stackoverflow.com/questions/78481152/how-to-enhance-accuracy-in-multi-class-classification-with-tensorflow</link>
      <description><![CDATA[我正在使用 TensorFlow 解决多类分类问题，并在实现令人满意的准确性方面遇到了挑战。我有7节课。文件夹中的每个类包含 2000 个 .csv 文件（每个文件有两列）。当我使用二元分类方法训练模型并用另一个类测试一个类时，准确性和 val_accuracy 会很高，0.85 到 0.95，但是当我使用多类进行测试时，精度最高可达0.47。下面是包含数据抛光和模型多类的代码。
#文件夹中的 csv 类
文件夹路径 = [
    &#39;/content/drive/MyDrive/medical_chem/Aa&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Ab&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Ac&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Ba&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Bb&#39;,
    &#39;/content/drive/MyDrive/medical_chem/Cc&#39;,
    &#39;/内容/驱动器/MyDrive/medical_chem/DD&#39;
]


数据 = []
标签=[]

#加载文件夹并将文件csv存档在数据框中
对于 enumerate(folder_paths) 中的 class_index、folder_path：
    对于 os.listdir(folder_path) 中的文件：
        file_path = os.path.join(文件夹路径, 文件)
        df = pd.read_csv(文件路径)
        数据.append(df)
        标签.append(class_index)

X = 数据
y = 标签

# 找到数据框中的最小值
min_length = min(len(df) for df in X)
# 设置数据帧长度相同
truncated_dfs = [df.head(min_length) for df in X]
# 数据帧到 numpy 数组
X = np.array([df.truncated_dfs 中 df 的值])


# 分割数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# 标准化数据
X_train = 归一化(X_train, 轴=1)
X_test = 归一化(X_test, 轴=1)
y_train = to_categorical(y_train, num_classes=7)
y_test = to_categorical(y_test, num_classes=7)


X_train.shape、y_train.shape、X_test.shape、y_test.shape
# 输出 ((8943, 2906, 2), (8943, 7), (2236, 2906, 2), (2236, 7))

模型 = tf.keras.Sequential([
    tf.keras.layers.Dense(128, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(64, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(7,activation=&#39;softmax&#39;) # 7个类的输出层
]）

# 训练模型的检查点
checkpoint_path = “training_checkpoint/cp.ckpt”
checkpoint_dir = os.path.dirname(checkpoint_path)
checkpoint_callback = ModelCheckpoint(文件路径=checkpoint_path,
                                      save_weights_only=真，
                                      save_best_only=真，
                                      监视器=&#39;val_loss&#39;,
                                      详细=1)

model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;分类交叉熵&#39;，
              指标=[&#39;准确性&#39;])

#model.load_weights(检查点路径)

历史 = model.fit(X_train, y_train,
                    纪元=100，
                    验证数据=（X_测试，y_测试），
                    回调=[检查点回调])


我尝试过调整神经网络的架构，尝试不同的激活函数，并优化学习率和批量大小等超参数。但是，我仍然没有达到预期的准确性。
我确信我出错的地方是在预处理数据或模型中，因为二进制训练有很好的结果。
与二进制训练相比，准确度为 0.85 至 0.95
**多类别的预期准确率：高于 0.90
**
数据集： https://drive .google.com/drive/folders/1UAt50dPH7ABeoLu16nfa19g4oVccPeFO?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/78481152/how-to-enhance-accuracy-in-multi-class-classification-with-tensorflow</guid>
      <pubDate>Wed, 15 May 2024 00:27:22 GMT</pubDate>
    </item>
    <item>
      <title>特征工程是一种新近特征[关闭]</title>
      <link>https://stackoverflow.com/questions/78481149/feature-engineering-a-recency-feature</link>
      <description><![CDATA[我有一个客户评分问题，我正在专门研究预测转化并得出转化的概率分数（使用 xgboost 分类器 atm）。我想介绍一个功能，但我很难明确该功能的定义。
具体来说，我知道当事件 A 最近发生时（例如，客户给我们的办公室打电话），这表明客户对我们的产品感兴趣并且可能会转化。为此，我创建了一个新近度功能，基本上是：（今天 - 事件日期）以天为单位。
问题在于，这没有捕捉到旧客户记录的影响。例如，客户可能在一年前给我们打电话（事件 A 触发），并在不久后进行转换，并且使用该公式，新近度特征将相对较大。我希望模型知道低新近度值会转化为更高的概率。
有没有什么好的方法来设计功能来捕捉这种关系？]]></description>
      <guid>https://stackoverflow.com/questions/78481149/feature-engineering-a-recency-feature</guid>
      <pubDate>Wed, 15 May 2024 00:26:20 GMT</pubDate>
    </item>
    <item>
      <title>调查 TensorFlow 和 PyTorch 性能的差异</title>
      <link>https://stackoverflow.com/questions/78478574/investigating-discrepancies-in-tensorflow-and-pytorch-performance</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78478574/investigating-discrepancies-in-tensorflow-and-pytorch-performance</guid>
      <pubDate>Tue, 14 May 2024 13:54:26 GMT</pubDate>
    </item>
    <item>
      <title>如何构建交易分类的分类引擎？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78468435/how-to-build-a-classification-engine-for-transaction-categorization</link>
      <description><![CDATA[我目前正在开发这个个人项目，以便让我的生活更轻松地预算支出，并且我正在尝试添加一种方法，允许用户训练模型来对其交易进行分类（或更详细地说是分类）。
问题是，除了本大学课程中的一些术语和小型项目之外，我没有深厚的数据科学背景，我需要一些帮助来理解如何执行此操作以及我需要学习/重新学习哪些内容。从来没有做过这样的事情。
基本上，我的想法是：
允许用户上传其交易的 CSV 数据文件（带有我预定义的校准类别变量，例如交易名称、金额、日期等）。
我知道分类模型必须经过训练才能有效，因此我希望用户分类数据量的阈值是 x，然后用户才能使用模型来预测类别（正确我，如果我不需要这个...）。
一旦超过阈值，模型将被解锁，供用户用来预测其数据的类别。
使用模型时，它将返回包含预测类别的数据集，然后用户可以根据需要编辑类别。编辑后的数据集将反馈给模型以提高准确性。
我的方向正确吗？我该如何执行此操作以及我应该学习/重新学习哪些内容？我可以在 SQL 数据库中执行此操作还是应该使用 Python 脚本（我的理解是我可以将数据存储在 SQL 服务器中，然后在 Python 中执行预处理和 ML 任务）？]]></description>
      <guid>https://stackoverflow.com/questions/78468435/how-to-build-a-classification-engine-for-transaction-categorization</guid>
      <pubDate>Sun, 12 May 2024 15:41:45 GMT</pubDate>
    </item>
    <item>
      <title>尝试强制新预测器中的观察结果以在医疗补助支出数据集中执行逻辑函数</title>
      <link>https://stackoverflow.com/questions/78456621/attempting-to-coerce-observations-in-a-new-predictor-to-perform-logistic-functio</link>
      <description><![CDATA[我正在分析按药物数据划分的医疗补助支出数据字典数据集。具体来说，我想执行逻辑回归，其中 y 应该是 CAGR_Avg_Spnd_Per_Dsg_Unt_18_22。
不幸的是，根据我的代码，类和模式仍然是字符。
我对“Up”的灵感来自于和“向下”方法来自以下内容：
# 该库来自《统计学习简介：R 中的应用》

图书馆（ISLR）
附加(Smarket)
总结(Smarket)
# 期望的输出：
glm.fit=glm(方向~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 音量,
            家庭=二项式，数据=Smarket）
对比（方向）

通过使用 glm.fit，我可以执行预测、创建混淆矩阵等等。
但是，在检查时
摘要（drug.spending）

我的“向上”和“向下”是角色，而 ISLR 的“Up”的作者是角色。和“向下”看来是按数字算的。作者从未提供使用数据框的“向上”来执行此操作的代码。和“向下”观察！
这是我的代码：
库(dplyr)
图书馆（tidyr）
图书馆（心理学）
图书馆（跳跃）
设置.种子(1)

支出 &lt;- read.csv(“medicaid_spending_by_drug_data_dictionary.csv”)
药品支出 &lt;- 支出 %&gt;%
  na.omit(支出) %&gt;%
  filter(Mftr_Name == “总体”) %&gt;%
  安排(desc(Tot_Mftr)) %&gt;%
  过滤器（重复（Gnrc_Name））
drug.spending &lt;- drug.spending[!duplicate(drug.spending$Gnrc_Name),]
附加（药物.支出）



药物支出 &lt;- 药物支出 %&gt;%
  mutate(CAGR_Direction = ifelse(CAGR_Avg_Spnd_Per_Dsg_Unt_18_22 &gt; 0, &#39;向上&#39;, &#39;向下&#39;))
drug.spending$CAGR_Direction &lt;- factor(drug.spending$CAGR_Direction,levels = c(&#39;Down&#39;, &#39;Up&#39;)) # 更新 #1

摘要（药品.支出）
对比（CAGR_Direction）#给出错误

我使用了不同的强制转换，例如 as.numeric() 和 as.integer()。我不太确定我哪里出错了......]]></description>
      <guid>https://stackoverflow.com/questions/78456621/attempting-to-coerce-observations-in-a-new-predictor-to-perform-logistic-functio</guid>
      <pubDate>Thu, 09 May 2024 19:27:54 GMT</pubDate>
    </item>
    <item>
      <title>将分类加权损失函数集成到我的代码中后，准确性下降了[关闭]</title>
      <link>https://stackoverflow.com/questions/78455283/the-accuracy-decreased-after-integrating-categorical-weighted-loss-function-to-m</link>
      <description><![CDATA[我想提高准确性，并且我有不平衡数据集：akiec：229，bcc：360，bkl：769，df：81，mel：779，vasc：99。为了解决这个问题，我选择将分类加权损失机制集成到模型中。然而，尽管进行了这样的调整，我还是注意到准确性随后下降了。这个意想不到的结果让我怀疑实施过程中出现了错误。您能否帮助我识别和解决任何潜在的错误以优化模型的性能？
# 定义目录
train_dir = &#39;/content/drive/MyDrive/ikinciasamadataset/Train&#39;
test_dir = &#39;/content/drive/MyDrive/ikinciasamadataset/Test&#39;
validation_dir = &#39;/content/drive/MyDrive/ikinciasamadataset/Validation&#39;

# 确定类的数量
numClasses = len(os.listdir(train_dir))

# 定义超参数网格
参数网格 = {
    &#39;学习率&#39;：[0.001]，
    &#39;批量大小&#39;：[16]，
}

最佳准确度 = 0
最佳参数=无

# 执行网格搜索
对于 ParameterGrid(param_grid) 中的参数：
    # 为每次网格搜索迭代加载预训练的 VGG19 模型
    base_model = VGG19(权重=&#39;imagenet&#39;, include_top=False, input_shape=(224, 224, 3))
    对于 base_model.layers 中的图层：
        可训练层 = False

    # 定义函数从最后一个卷积层提取特征
    def extract_features（生成器，模型）：
        特征 = model.predict(生成器)
        返回 features.reshape((len(generator.filenames), -1))

    # 创建数据生成器
    train_datagen = 图像数据生成器(
        重新缩放=1./255，
        旋转范围=20，
        width_shift_range=0.2，
        height_shift_range=0.2，
        剪切范围=0.2，
        缩放范围=0.2，
        水平翻转=真，
        fill_mode=&#39;最近&#39;)

    validation_datagen = ImageDataGenerator（重新缩放=1./255）

    train_generator = train_datagen.flow_from_directory(
        火车目录，
        目标大小=(224, 224),
        批量大小=参数[&#39;批量大小&#39;],
        class_mode=&#39;分类&#39;
    ）

    validation_generator =validation_datagen.flow_from_directory(
        验证目录，
        目标大小=(224, 224),
        批量大小=参数[&#39;批量大小&#39;],
        class_mode=&#39;分类&#39;
    ）
&#39;&#39;&#39;

可能这里有一个错误

&#39;&#39;&#39;

    # 定义类索引
    类索引 = {
        &#39;基亚克&#39;: 0,
        “密件抄送”：1，
        &#39;bkl&#39;：2，
        “df”：3，
        “梅尔”：4，
        “血管”：5
    }

    ## 计算班级人数
    类计数 = {}
    对于 os.listdir(train_dir) 中的 class_name：
        class_counts[class_name] = len(os.listdir(os.path.join(train_dir, class_name)))

    # 计算类别权重
    类权重 = {}
    Total_samples = sum(class_counts.values())
    对于 class_name、class_count 在 class_counts.items() 中：
        class_weights[class_indices[class_name]] = 总样本数 / (class_count * len(class_counts))



    # 定义模型架构以接受提取的特征作为输入
    输入=输入(形状=(combined_data_train.shape[1],))
    x = 密集（256，激活=&#39;relu&#39;）（输入）
    预测=密集（numClasses，激活=&#39;softmax&#39;）（x）
    模型=模型（输入=输入，输出=预测）

    # 使用当前的超参数和类权重编译模型
    model.compile（优化器=SGD（learning_rate=params[&#39;learning_rate&#39;]），loss=&#39;sparse_categorical_crossentropy&#39;，metrics=[&#39;accuracy&#39;]，sample_weight_mode=&#39;temporal&#39;）

    # 定义提前停止
    Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 5，restore_best_weights = True）

    # 通过提前停止来训练模型
    num_epochs = 50 # 您可以在此处调整纪元数
    历史=模型.fit(
        x=组合数据训练，
        y=train_generator.labels,
        纪元=num_epochs，
        批量大小=参数[&#39;批量大小&#39;],
        validation_data=(combined_data_validation,validation_generator.labels),
        回调=[early_stopping],
        类权重=类权重，
        详细=1
    ）

    model.save(&#39;best_vgg19_model_with_age.h5&#39;)

    # 根据验证数据评估模型
    _，val_accuracy = model.evaluate（combined_data_validation，validation_generator.labels，详细= 0）

    # 如有必要，更新最佳精度和最佳参数
    如果 val_accuracy &gt;最佳准确度：
        最佳准确度 = 有效准确度
        最佳参数 = 参数

# 打印最佳参数和准确度
print(&#39;最佳参数：&#39;, best_params)
print(&#39;最佳验证准确度：&#39;, best_accuracy)


# 加载最佳模型
best_model = load_model(&#39;best_vgg19_model_with_age.h5&#39;)

]]></description>
      <guid>https://stackoverflow.com/questions/78455283/the-accuracy-decreased-after-integrating-categorical-weighted-loss-function-to-m</guid>
      <pubDate>Thu, 09 May 2024 14:52:54 GMT</pubDate>
    </item>
    <item>
      <title>do_one(nmeth) 中的错误：外部函数调用中的 NA/NaN/Inf (arg 1)</title>
      <link>https://stackoverflow.com/questions/36469671/error-in-do-onenmeth-na-nan-inf-in-foreign-function-call-arg-1</link>
      <description><![CDATA[我有一个数据表（“范数”），其中包含数字 - 至少就我所见 - 具有以下形式的标准化值：

当我执行时
k &lt;- kmeans(norm,center=3)

我收到以下错误：
do_one(nmeth) 中的错误：外部函数调用中的 NA/NaN/Inf (arg 1)

你能帮我吗？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/36469671/error-in-do-onenmeth-na-nan-inf-in-foreign-function-call-arg-1</guid>
      <pubDate>Thu, 07 Apr 2016 07:40:01 GMT</pubDate>
    </item>
    <item>
      <title>比 tf/idf 和余弦相似度更好的文本文档聚类？</title>
      <link>https://stackoverflow.com/questions/17537722/better-text-documents-clustering-than-tf-idf-and-cosine-similarity</link>
      <description><![CDATA[我正在尝试对 Twitter 流进行聚类。我想将每条推文放入讨论同一主题的集群中。我尝试使用具有 tf/idf 和余弦相似度的在线聚类算法对流进行聚类，但我发现结果非常糟糕。
使用 tf/idf 的主要缺点是它会聚集关键字相似的文档，因此只能识别几乎相同的文档。例如，考虑以下句子：
1- Stackoverflow 网站是一个不错的地方。
2- Stackoverflow 是一个网站。
前面的两个句子可能会以合理的阈值聚集在一起，因为它们共享很多关键字。但现在考虑以下两句话：
1- Stackoverflow 网站是一个不错的地方。
2- 我定期访问 Stackoverflow。
现在，通过使用 tf/idf，聚类算法将严重失败，因为即使它们谈论同一主题，它们也只共享一个关键字。
我的问题：是否有更好的技术来聚类文档？]]></description>
      <guid>https://stackoverflow.com/questions/17537722/better-text-documents-clustering-than-tf-idf-and-cosine-similarity</guid>
      <pubDate>Mon, 08 Jul 2013 23:40:57 GMT</pubDate>
    </item>
    </channel>
</rss>