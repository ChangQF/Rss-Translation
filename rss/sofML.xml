<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 04 Sep 2024 15:17:38 GMT</lastBuildDate>
    <item>
      <title>ImportError：无法从“gaze_tracking”导入名称“GazeTracking”[关闭]</title>
      <link>https://stackoverflow.com/questions/78948526/importerror-cannot-import-name-gazetracking-from-gaze-tracking</link>
      <description><![CDATA[每当我单击 python main.py 时，我都会卡住。以下是我无法解决的错误：
(newenv) C:\Users\Khan Mohd Arshad\OneDrive\Desktop\major\Oculus&gt;python main.py
2024-09-04 17:24:09.930311：我 tensorflow/core/util/port.cc:153] oneDNN 自定义操作已启用。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 `TF_ENABLE_ONEDNN_OPTS=0`。
2024-09-04 17:24:11.396430：我 tensorflow/core/util/port.cc:153] oneDNN 自定义操作已启用。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量“TF_ENABLE_ONEDNN_OPTS=0”。
警告：tensorflow：来自 C:\Users\Khan Mohd Arshad\OneDrive\Desktop\major\Oculus\newenv\Lib\site-packages\tf_keras\src\losses.py:2976：名称 tf.losses.sparse_softmax_cross_entropy 已弃用。请改用 tf.compat.v1.losses.sparse_softmax_cross_entropy。

回溯（最近一次调用）：
文件“C:\Users\Khan Mohd Arshad\OneDrive\Desktop\major\Oculus\main.py”，第 16 行，位于&lt;module&gt;
来自 gaze_tracking 导入 GazeTracking
ImportError：无法从“gaze_tracking”（未知位置）导入名称“GazeTracking”

我尝试运行自闭症检测项目。我下载了所有要求，然后出现错误。
这是 repo。请运行并解释步骤。https://github.com/deyRupak/oculus.git]]></description>
      <guid>https://stackoverflow.com/questions/78948526/importerror-cannot-import-name-gazetracking-from-gaze-tracking</guid>
      <pubDate>Wed, 04 Sep 2024 12:00:14 GMT</pubDate>
    </item>
    <item>
      <title>如何设置 Kubernetes GPU 集群以与位于本地网络中不同 PC 上的 GPU 协同工作？</title>
      <link>https://stackoverflow.com/questions/78947983/how-to-set-up-kubernetes-gpu-cluster-to-work-with-gpus-located-on-different-pcs</link>
      <description><![CDATA[我想在 Jupyter Notebook 中设置一个用于机器学习的 Kubernetes GPU 集群。我在一个本地网络中有 3 台配备 GPU（RTX 3060ti）的计算机，我想结合这些 GPU 的资源来运行神经网络训练和其他机器学习方法。
我的 RTX 3060ti 在解决某些神经网络训练任务时会执行太长的计算，我想用它来加速神经网络的训练。
感谢您的任何建议！
我尝试搜索有关此主题的信息，但大多数文章都涉及将一个 GPU 的资源分配给不同的机器学习任务。]]></description>
      <guid>https://stackoverflow.com/questions/78947983/how-to-set-up-kubernetes-gpu-cluster-to-work-with-gpus-located-on-different-pcs</guid>
      <pubDate>Wed, 04 Sep 2024 09:48:45 GMT</pubDate>
    </item>
    <item>
      <title>Azure ML Studio 数据资产权限被拒绝</title>
      <link>https://stackoverflow.com/questions/78947921/azure-ml-studio-permission-denied-on-data-asset</link>
      <description><![CDATA[我使用下面的代码通过 python azure API 创建了一个数据资产。模式如下：
wasbs://&lt;container-name&gt;@&lt;account name&gt;.blob.core.windows.net/&lt;folder&gt;/*.csv

其中帐户和容器与 Azure 机器学习工作区使用的相同。
 tbl = mltable.from_delimited_files(
path=[{&quot;pattern&quot;: body}],
delimiter=&quot;,&quot;,
header=MLTableHeaders.all_files_same_headers,
infer_column_types=True,
include_path_column=False,
encoding=MLTableFileEncoding.utf8,
)

my_data = Data(
path=mltable_folder, 
type=AssetTypes.MLTABLE,
name=&quot;Measurements&quot;,
)

my_data = ml_client.data.create_or_update(my_data)

数据资产已成功创建，我可以使用 UI 探索数据。此外，MLTable 文件看起来正常，具有正确的模式。
当我使用数据资产和自创计算实例启动自动化 ML 任务时，我收到以下错误：
错误消息：尝试访问流时身份验证失败。请确保您已设置正确的权限。确定（此请求无权使用此权限执行此操作。）| session_id=0e120470-1498-4fc9-8201-19783974e5df
ErrorResponse 
{
&quot;error&quot;: {
&quot;code&quot;: &quot;UserError&quot;,
&quot;message&quot;: &quot;提供的数据存储区中的数据路径无法访问。请确保您对资源拥有必要的访问权限。错误：\n错误代码：ScriptExecution.StreamAccess.Authentication\nNative 错误：数据流访问错误：ExecutionError(StreamError(PermissionDenied(Some(此请求无权使用此权限执行此操作。))))\n\tVisitError(ExecutionError(StreamError(PermissionDenied(Some(此请求无权使用此权限执行此操作。)))))\n=&gt;执行错误导致失败：从输入数据源流式传输时出错\n\tExecutionError(StreamError(PermissionDenied(Some(此请求未获授权使用此权限执行此操作。))))\n错误消息：尝试访问流时身份验证失败。请确保您已设置正确的权限。Ok(此请求未获授权使用此权限执行此操作。)| session_id=0e120470-1498-4fc9-8201-19783974e5df&quot;,
&quot;inner_error&quot;: {
&quot;code&quot;: &quot;Auth&quot;,
&quot;inner_error&quot;: {
&quot;code&quot;: &quot;Authentication&quot;,
&quot;inner_error&quot;: {
&quot;code&quot;: &quot;DataPathInaccessible&quot;
}
}
}
}
}

注意：当我使用具有相同数据和数据存储的 UI 创建数据资产时，我没有收到任何错误，并且一切正常。
我尝试了各种访问设置，但似乎无法使这个设置正常工作。还有人知道如何解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78947921/azure-ml-studio-permission-denied-on-data-asset</guid>
      <pubDate>Wed, 04 Sep 2024 09:36:02 GMT</pubDate>
    </item>
    <item>
      <title>训练准确率不断提高，但验证准确率早期停滞</title>
      <link>https://stackoverflow.com/questions/78947096/training-accuracy-keeps-increasing-but-validation-plateaus-early</link>
      <description><![CDATA[我尝试使用自己实现的 Resnet 模型对 CIFAR-100 数据集进行分类。
我尝试了多种不同的超参数配置，更改了学习率、批量大小、辍学率、数据增强和正则化，但我所做的一切都无法将验证准确率提高 40% 以上，而训练准确率可以达到 99%。
我意识到训练准确率高但验证准确率低是过度拟合的标志，但在增加正则化参数和辍学率后，我仍然没有看到任何改善。训练、验证和测试的分割由 CIFAR-100 提供，因此分别为 40000、10000、10000。
有人知道如何突破这个瓶颈，或者知道我可能哪里出错了吗？
以下是我一直在尝试的超参数类型：
learning_rates = [0.001, 0.0001]
batch_sizes = [16, 32]
dropout_rates = [0.3,0.5]
decay = [0.001, 0.0001]

以下是我一直在增强数据的方式：
transform = transforms.Compose([
transforms.RandomCrop(32, padding=4),
transforms.RandomHorizo​​ntalFlip(),
transforms.RandomRotation(10),
transforms.ToTensor(),
transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

DATA_ROOT_FOLDER = &#39;./cifar-100-python&#39; 

train_data = datasets.CIFAR100(root=f&quot;{DATA_ROOT_FOLDER}&quot;, train=True, download=True, transform=transform)
test_data = datasets.CIFAR100(root=f&quot;{DATA_ROOT_FOLDER}&quot;, train=False, download=True, transform=transform)

train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)

batch_size = 64
train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)

这些是我得到的结果。
准确度图
准确度图失落]]></description>
      <guid>https://stackoverflow.com/questions/78947096/training-accuracy-keeps-increasing-but-validation-plateaus-early</guid>
      <pubDate>Wed, 04 Sep 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>在 python 中进行向量搜索，根据上下文获取独特性分数</title>
      <link>https://stackoverflow.com/questions/78946820/vector-search-in-python-to-get-the-unqueness-score-according-to-context</link>
      <description><![CDATA[我有一篇带有标题和说明的博客文章，我想将其唯一性与 CSV 文件中的多个博客条目进行比较。CSV 包含多个博客，每个博客都有标题和元描述。
我目前使用 TF-IDF 矢量化和余弦相似度将单个博客与 CSV 文件中的所有条目进行比较。但是，这种方法仅基于确切的单词而不是上下文进行匹配。]]></description>
      <guid>https://stackoverflow.com/questions/78946820/vector-search-in-python-to-get-the-unqueness-score-according-to-context</guid>
      <pubDate>Wed, 04 Sep 2024 04:27:53 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Google Colab 中导入 KerasRegressor？</title>
      <link>https://stackoverflow.com/questions/78946669/how-do-i-import-kerasregressor-in-google-colab</link>
      <description><![CDATA[我正在使用 Google colab，在导入 KerasRegressor 时遇到问题。
我运行：
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor

并收到以下错误：
ModuleNotFoundError：没有名为“tensorflow.keras.wrappers”的模块

在一些论坛中，我发现有人解决了“scikeras”的问题，但我在 colab 上遇到了这个问题。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78946669/how-do-i-import-kerasregressor-in-google-colab</guid>
      <pubDate>Wed, 04 Sep 2024 03:10:09 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 回归器给出非常糟糕的相对误差％[关闭]</title>
      <link>https://stackoverflow.com/questions/78945802/xgboost-regressor-giving-very-bad-relative-error</link>
      <description><![CDATA[当我训练 XGBRegressor 时，对因变量进行了 boxcox 变换，以获得更高的线性度，因为它可以改善模型。对于标准化指标，我获得了 94.7% 的良好准确度，rmse 良好，相对误差 % 为 1.8%。但是当我尝试对因变量进行逆变换时，相对误差 % 为 245%。这令人担忧，因为随机森林给出的相对误差 % 为 10。随机森林和 XGBoost 几乎给出了因变量形式相同的平均绝对误差，但不知何故 XG 的相对误差 % 为 245%。这怎么可能呢？
我尝试过 hypertuning XGBoost，希望得到一些好消息，但错误率上升到了 450%。]]></description>
      <guid>https://stackoverflow.com/questions/78945802/xgboost-regressor-giving-very-bad-relative-error</guid>
      <pubDate>Tue, 03 Sep 2024 19:28:23 GMT</pubDate>
    </item>
    <item>
      <title>如何使用自动生成的代码重新训练 ML.Net 模型？</title>
      <link>https://stackoverflow.com/questions/78945171/how-do-you-retrain-an-ml-net-model-using-the-auto-generated-code</link>
      <description><![CDATA[我已经使用 Ml.net 配置接口训练了一个图像分类模型，并且它生成了自包含的 MlModel.consumption 和 MlModel.training 部分类。
它还自动生成一个程序，演示如何使用该模型，但没有演示如何重新训练它。
我曾尝试这样做
MLContext mlContext = new MLContext();

DataViewSchema modelSchema;

ITransformer trainedModel = mlContext.Model.Load(&quot;MlModel2.mlnet&quot;, out modelSchema);

var x = MLModel2.LoadImageFromFolder(mlContext, @&quot;C:\temp\Latest\Train&quot;);

var y = MLModel2.RetrainModel(mlContext, x);

mlContext.Model.Save(y, modelSchema, &quot;MlModel2.mlnet&quot;);

但它将“未创建保存器，因为图中没有要恢复的变量”写入控制台，并且在我测试时不会更改模型的行为。
自动生成的代码如下所示
// 此文件由 ML.NET Model Builder 自动生成。
使用系统;
使用 System.IO;
使用 System.Collections.Generic;
使用 System.Linq;
使用 System.Text;
使用 System.Threading.Tasks;
使用 Microsoft.ML;
使用 Microsoft.ML.Data;
使用 Microsoft.ML.Vision;

namespace NewCassetteModel
{
public partial class MLModel2
{

/// &lt;summary&gt;
/// 从文件夹路径加载 IDataView。
/// &lt;/summary&gt;
/// &lt;param name=&quot;mlContext&quot;&gt;所有 ML.NET 操作的通用上下文。&lt;/param&gt;
/// &lt;param name=&quot;folder&quot;&gt; 用于训练的图像数据文件夹。&lt;/param&gt;
public static IDataView LoadImageFromFolder(MLContext mlContext, string folder)
{
var res = new List&lt;ModelInput&gt;();
var allowedImageExtensions = new[] { &quot;.png&quot;, &quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.gif&quot; };
DirectoryInfo rootDirectoryInfo = new DirectoryInfo(folder);
DirectoryInfo[] subDirectories = rootDirectoryInfo.GetDirectories();

if (subDirectories.Length == 0)
{
throw new Exception(&quot;fail to find subdirectories&quot;);
}

foreach (DirectoryInfo directory in subDirectories)
{
var imageList = directory.EnumerateFiles().Where(f =&gt; allowedImageExtensions.Contains(f.Extension.ToLower()));
if (imageList.Count() &gt; 0)
{
res.AddRange(imageList.Select(i =&gt; new ModelInput 
{
Label = directory.Name,
ImageSource = File.ReadAllBytes(i.FullName),
}));
}
}
return mlContext.Data.LoadFromEnumerable(res);
}

/// &lt;summary&gt;
/// 使用在训练过程中生成的管道重新训练模型。
/// &lt;/summary&gt;
/// &lt;param name=&quot;mlContext&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;trainData&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
public static ITransformer RetrainModel(MLContext mlContext, IDataView trainData)
{
var pipeline = BuildPipeline(mlContext);
var model = pipeline.Fit(trainData);

return model;
}

/// &lt;summary&gt;
/// 构建模型构建器使用的管道。使用此函数重新训练模型。
/// &lt;/summary&gt;
/// &lt;param name=&quot;mlContext&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
public static IEstimator&lt;ITransformer&gt; BuildPipeline(MLContext mlContext)
{
// 带有管道数据转换的数据处理配置
var pipeline = mlContext.Transforms.Conversion.MapValueToKey(outputColumnName:@&quot;Label&quot;,inputColumnName:@&quot;Label&quot;,addKeyValueAnnotationsAsText:false) 
.Append(mlContext.MulticlassClassification.Trainers.ImageClassification(labelColumnName:@&quot;Label&quot;,scoreColumnName:@&quot;Score&quot;,featureColumnName:@&quot;ImageSource&quot;)) 
.Append(mlContext.Transforms.Conversion.MapKeyToValue(outputColumnName:@&quot;PredictedLabel&quot;,inputColumnName:@&quot;PredictedLabel&quot;));

return pipeline;
}
}
}


我需要做什么才能使其工作？]]></description>
      <guid>https://stackoverflow.com/questions/78945171/how-do-you-retrain-an-ml-net-model-using-the-auto-generated-code</guid>
      <pubDate>Tue, 03 Sep 2024 15:53:58 GMT</pubDate>
    </item>
    <item>
      <title>SAM 模型中无法检测图像 - TypeError：无法处理此数据类型</title>
      <link>https://stackoverflow.com/questions/78942834/image-not-detecting-in-sam-model-typeerror-cannot-handle-this-data-type</link>
      <description><![CDATA[每当我上传任何类型的图像时，都会出现相同的错误。它是否只处理高质量图像（或任何特定类型的图像），还是我的代码中存在一些错误？
我尝试了不同的图像，上面的代码是 chatgpt 经过一些修改后给出的。仍然没有运气。我得到一个
TypeError：无法处理此数据类型：（1, 1, 640, 3），|u1

代码：
import torch
import numpy as np
from PIL import Image
fromsegment_anything import sam_model_registry, SamPredictor
from google.colab import files

# 加载 SAM 模型

sam = sam_model_registry[&quot;vit_b&quot;](checkpoint=&quot;/content/sam_vit_b_01ec64.pth&quot;)
predictor = SamPredictor(sam)

uploaded = files.upload()
image_name = list(uploaded.keys())[0]

image = Image.open(image_name).convert(&quot;RGB&quot;)
image_np = np.array(image)

# 检查图像形状

print(f&quot;原始图像形状：{image_np.shape}&quot;)

# 如果存在额外维度，则删除它们

if len(image_np.shape) == 4 and image_np.shape[0] == 1:
image_np = image_np.squeeze(0) # 如果第一个维度的大小为 1，则删除它

# 确保图像的格式和类型正确

image_np = image_np.astype(np.uint8)

print(f&quot;处理后的图像形状：{image_np.shape}&quot;)

# 将图像设置为 SAM 预测器

predictor.set_image(image_np)

# 预测图像的蒙版

masks = predictor.predict()

# 确保您有蒙版并使用它们

if mask is not None and len(masks) &gt; 0:
mask = mask[0] # 假设第一个掩码就是您需要的

# 将掩码应用于图像
masked_image = np.where(mask[..., None], image_np, 0) # 将掩码应用于图像

# 将掩码图像转换回 PIL 图像
masked_image = Image.fromarray(masked_image)

# 显示掩码图像
masked_image.show()

else:
print(&quot;未找到给定图像的掩码。&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78942834/image-not-detecting-in-sam-model-typeerror-cannot-handle-this-data-type</guid>
      <pubDate>Tue, 03 Sep 2024 05:49:49 GMT</pubDate>
    </item>
    <item>
      <title>梯度盗贼代理的性能问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78942595/performance-issue-with-gradient-bandit-agent</link>
      <description><![CDATA[我正在阅读 Sutton&amp;Barto 的《强化学习：导论》。尝试测试梯度强盗代理（第 2.7 章）。但性能极低。我试过：

使用基线 = 平均奖励，不使用基线；
alpha = 0.1、0.2、0.3、0.4；
初始偏好 H = 0、1、10、100。

没有任何帮助。
这是我的 Python 代码，用于代理的 生命步骤 = 动作选择 + 参数更新 (self = agent)：
# 用于概率计算：
pref_exps = np.exp(self.params[&quot;preferences&quot;])
pref_exps_sum = sum(pref_exps)

# 选择强盗：
choice_dice = np.random.uniform() * pref_exps_sum
accum_pref_exp = 0
for i, pref_exp in enumerate(pref_exps):
accum_pref_exp += pref_exp
if accum_pref_exp &gt;= choice_dice:
self.chosen_bandit_i = i
break

# self.reward 在此处填充：
self.perform_bandit(self.chosen_bandit_i)

# 更新基线：
self.params[&quot;lifetime&quot;] += 1
self.params[&quot;average_reward&quot;] += 1 / self.params[&quot;lifetime&quot;] * (self.reward - self.params[&quot;average_reward&quot;])

# 更新偏好：
for i, pref_exp in enumerate(pref_exps):
probability = pref_exp / pref_exps_sum
if i == self.chosen_bandit_i:
self.params[&quot;preferences&quot;][i] += self.params[&quot;alpha&quot;] * (self.reward - self.params[&quot;average_reward&quot;]) * (1 - probability)
else:
self.params[&quot;preferences&quot;][i] -= self.params[&quot;alpha&quot;] * (self.reward - self.params[&quot;average_reward&quot;]) * probability

此代码导致性能极差（100 个代理，每个代理访问自己的 10 个 1-armed-bandits，测试超过 2000 步），我们可以从下图中看到：

我看过这篇帖子，修复错误后，它的代码似乎与我的代码相同，这也是那篇帖子的原因。但与我的代码不同，那篇帖子的代码在纠正后可以正常工作！
我不知道我在哪里犯了错误。你能帮助我正确使用Gradient-bandit 代理的全部功能吗？]]></description>
      <guid>https://stackoverflow.com/questions/78942595/performance-issue-with-gradient-bandit-agent</guid>
      <pubDate>Tue, 03 Sep 2024 03:40:50 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：在层“conv2d_7”上调用“set_weights(weights)”，权重列表长度为 2，但该层需要 1 个权重</title>
      <link>https://stackoverflow.com/questions/78836514/valueerror-called-set-weightsweights-on-layer-conv2d-7-with-a-weight-list</link>
      <description><![CDATA[我尝试在模型中的某一层上设置权重，但无济于事。
我在网上查找了类似问题的解决方案，但似乎都不起作用。变量“w”（如下面代码所示）的结构为 [numpy array, numpy array]。第一个的大小为 (3, 3, 3, 64)，第二个的形状为 (64,)。我想实现与 tf 2.X 中的“weights”kwarg 类似的功能，但似乎无法让它工作。这是我的代码：
encoder = Sequential()
encoder.add(layers.Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;,use_bias=False,input_shape=(SIZE,SIZE,3)))
w = model.layers[0].get_weights()
encoder.layers[0].set_weights([w])
encoder.add(layers.MaxPooling2D((2, 2),padding=&#39;same&#39;))
encoder.add(layers.Conv2D(32, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;,weights=model.layers[2].get_weights()))
encoder.add(layers.MaxPooling2D((2, 2), padding=&#39;same&#39;))
encoder.add(layers.Conv2D(16, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;,weights=model.layers[4].get_weights()))
encoder.add(layers.MaxPooling2D((2, 2), padding=&#39;same&#39;))
encoder.summary()

错误：
错误：ValueError：您在层“conv2d_7”上调用了`set_weights(weights)`，权重列表长度为 2，但该层需要 1 个权重。
]]></description>
      <guid>https://stackoverflow.com/questions/78836514/valueerror-called-set-weightsweights-on-layer-conv2d-7-with-a-weight-list</guid>
      <pubDate>Mon, 05 Aug 2024 21:33:29 GMT</pubDate>
    </item>
    <item>
      <title>自定义模型聚合器 TensorFlow Federated</title>
      <link>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</link>
      <description><![CDATA[我正在尝试使用 TensorFlow Federated，使用 FedAvg 算法模拟训练过程。
trainer = tff.learning.algorithms.build_weighted_fed_avg(
model_fn= tff_model,
client_optimizer_fn=client_optimizer,
server_optimizer_fn=server_optimizer
)

我想使用自定义权重来聚合客户端的更新，而不是使用它们的样本数量。我知道 tff.learning.algorithms.build_weighted_fed_avg() 有一个名为 client_weighting 的参数，但唯一接受的值来自类 tff.learning.ClientWeighting，它是一个枚举。
还有其他方法可以使用自定义权重吗？]]></description>
      <guid>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</guid>
      <pubDate>Mon, 05 Aug 2024 16:06:48 GMT</pubDate>
    </item>
    <item>
      <title>UnicodeEncodeError：'charmap'编解码器无法对位置 19-38 的字符进行编码：字符映射到 <undefined></title>
      <link>https://stackoverflow.com/questions/78367946/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-19-38-c</link>
      <description><![CDATA[我正在开发一个基于 Flask 的 Web 应用程序，用户可以上传图像以使用机器学习模型进行预测。上传的图像存储在本地目录中，并使用预先训练的模型进行预测。但是，当我点击预测按钮时
是什么导致了这个 UnicodeEncodeError？
我该如何解决这个问题，以确保我的应用程序能够正确处理图像上传和预测？
在 Flask 环境中，尤其是在 Windows 上，是否有处理字符编码的最佳实践？
==app.py====
@app.route(&#39;/uploadimage&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;])
def upload_image():

file = request.files[&#39;my_image&#39;]
# 获取预测
predict_label = predict_label(img_path)
# 使用 flash 消息返回预测标签
flash(f&quot;Prediction: {predicted_label}&quot;, &quot;success&quot;)
os.remove(img_path) # 处理后删除临时文件
return render_template(&#39;uploadimage.html&#39;) # 对于 GET 请求，呈现表单


即使我设置了环境变量“UTF-8”，我仍然收到此错误
错误
文件“C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\encodings\cp1252.py”，第 19 行，位于 encode 中
返回codecs.charmap_encode(input,self.errors,encoding_table)[0]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: &#39;charmap&#39; 编解码器无法对位置 19-38 中的字符进行编码：字符映射到未定义。
============
即使我得到了一个最简单的代码来测试编码
标题是“要测试您的控制台是否可以处理 UTF-8，请尝试输出带有特殊字符或 Unicode 字符的文本：&quot;
print(&quot;UTF-8 test: àéîöü — 中文 — العربية&quot;)


错误与此相同好吧
print(&quot;UTF-8 测试：����� � \u4e2d\u6587 � \u0627\u0644\u0639\u0631\u0628\u064a\u0629&quot;)
文件 &quot;C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\encodings\cp1252.py&quot;，第 19 行，在编码中
返回 codecs.charmap_encode(input,self.errors,encoding_table)[0]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: &#39;charmap&#39; 编解码器无法对位置中的字符进行编码20-21：字符映射到 ]]></description>
      <guid>https://stackoverflow.com/questions/78367946/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-19-38-c</guid>
      <pubDate>Mon, 22 Apr 2024 17:25:20 GMT</pubDate>
    </item>
    <item>
      <title>如何沿批次将张量连接到 keras 层（不指定批次大小）？</title>
      <link>https://stackoverflow.com/questions/68345125/how-to-concatenate-a-tensor-to-a-keras-layer-along-batch-without-specifying-bat</link>
      <description><![CDATA[我想将嵌入层的输出与自定义张量 (myarr / myconst) 连接起来。我可以使用固定的批处理大小指定所有内容，如下所示：
import numpy as np
import tensorflow as tf

BATCH_SIZE = 100
myarr = np.ones((10, 5))
myconst = tf.constant(np.tile(myarr, (BATCH_SIZE, 1, 1)))

# 模型定义
inputs = tf.keras.layers.Input((10,), batch_size=BATCH_SIZE)
x = tf.keras.layers.Embedding(10, 5)(inputs)
x = tf.keras.layers.Concatenate(axis=1)([x, myconst])
model = tf.keras.models.Model(inputs=inputs, output=x)

但是，如果我不要指定批处理大小和平铺我的数组，即仅以下...
myarr = np.ones((10, 5))
myconst = tf.constant(myarr)

# 模型定义
inputs = tf.keras.layers.Input((10,))
x = tf.keras.layers.Embedding(10, 5)(inputs)
x = tf.keras.layers.Concatenate(axis=1)([x, myconst])
model = tf.keras.models.Model(inputs=inputs, output=x)

... 我收到一个错误，指定形状 [(None, 10, 5), (10, 5)] 无法连接。有没有办法添加这个 None / batch_size 轴以避免平铺？
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/68345125/how-to-concatenate-a-tensor-to-a-keras-layer-along-batch-without-specifying-bat</guid>
      <pubDate>Mon, 12 Jul 2021 09:40:09 GMT</pubDate>
    </item>
    <item>
      <title>如何裁剪无人机拍摄的太阳能电池板？</title>
      <link>https://stackoverflow.com/questions/51414835/how-do-i-crop-the-solar-panels-captured-by-drone</link>
      <description><![CDATA[我目前正在从无人机拍摄的图像中裁剪太阳能电池板（附上示例图像）。我尝试过使用轮廓，但没有得到正确的结果。它没有检测到图像中的所有太阳能电池板，其中一些缺失了。我在这里遇到了麻烦。我该如何继续？请帮我解决这个问题。
谢谢，
示例代码：
import cv2
import numpy as np
img = cv2.imread(&#39;D:\\SolarPanel Images\\solarpanel.jpg&#39;)
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(gray,(5,5),0)
edges = cv2.Canny(blur,100,200) 
th3 = cv2.adaptiveThreshold(edges,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)

im2, contours, Hierarchy = cv2.findContours(th3, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
print(&quot;Len of contours&quot;,len(contours)
try: Hierarchy = Hierarchy[0]
except: Hierarchy = []

height, width, = Edges.shape
min_x, min_y = width, height
max_x = max_y = 0

# 计算轮廓的边界框，并将其绘制在图像上，
for contour, hier in zip(contours, Hierarchy):
area = cv2.contourArea(contour)

if area &gt; 10000 and area &lt; 250000:
(x,y,w,h) = cv2.boundingRect(contour)
min_x, max_x = min(x, min_x), max(x+w, max_x)
min_y, max_y = min(y, min_y), max(y+h, max_y)
如果 w &gt; 80 且 h &gt; 80:
cv2.rectangle(img, (x,y), (x+w,y+h), (255, 0, 0), 2)

cv2.imshow(&#39;cont imge&#39;, img)
cv2.waitKey(0)

]]></description>
      <guid>https://stackoverflow.com/questions/51414835/how-do-i-crop-the-solar-panels-captured-by-drone</guid>
      <pubDate>Thu, 19 Jul 2018 05:18:40 GMT</pubDate>
    </item>
    </channel>
</rss>