<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 30 Jun 2024 21:14:42 GMT</lastBuildDate>
    <item>
      <title>LSTM 作为双机器学习中的回归器</title>
      <link>https://stackoverflow.com/questions/78689719/lstm-as-regressor-in-double-machine-learning</link>
      <description><![CDATA[我想使用双重机器学习 (DML) 进行一些因果关系研究。为此，我使用了 doubleml 包中的 DoubleMLIRM。DML 使用两种 ML/DL/NN 方法；一种用于回归，另一种用于分类。对于分类，我使用 ExtraTreesClassifier，我想为回归器开发一个 LSTM。我有点困惑我该怎么做，因为我想将其输入为回归器。这是我目前使用 DML 和分类器的代码。我想在代码中定义 LSTM 回归器。df 是一个时间序列格式的数据框，它至少有 3 列，分别为 y_col、d_col 和 x_cols。请注意，d_cols 列是二进制的（这是处理变量）。
import doubleml as dml
from sklearn.ensemble import ExtraTreesClassifier

data_dml_base = dml.DoubleMLData(df,
y_col=y_col,
d_cols= d_cols,
x_cols=x_cols)

boost1 = ExtraTreesClassifier(n_jobs=1,
n_estimators=300, max_depth = 2)

boost2 = LSTM(...) # 回归器，我该如何写这个？

np.random.seed(1111)
dml_plr_boost = dml.DoubleMLIRM(data_dml_base,
ml_g = boost2,
ml_m = boost1,
n_folds = 10)
dml_plr_boost.fit(store_predictions=True)
dml_plr_boost.summary

我以前使用 XGBRegressor 作为回归器，但它对我的数据不太适用。代码看起来应该是这样的：
boost3 = XGBRegressor(n_jobs=1, objective = &#39;reg:squarederror&#39;,
eta=0.3, n_estimators=500, max_depth = 2)

dml_plr_boost_xgb = dml.DoubleMLIRM(data_dml_base,
ml_g = boost3,
ml_m = boost1,
n_folds = 10)
]]></description>
      <guid>https://stackoverflow.com/questions/78689719/lstm-as-regressor-in-double-machine-learning</guid>
      <pubDate>Sun, 30 Jun 2024 20:29:08 GMT</pubDate>
    </item>
    <item>
      <title>Minecraft mod 机器学习 [关闭]</title>
      <link>https://stackoverflow.com/questions/78689322/minecraft-mod-machine-learning</link>
      <description><![CDATA[我正在和朋友们一起开发一个 Minecraft 模组，我们的想法之一是让怪物在了解玩家在地图上的行进路线后跟踪玩家，以使其变得可怕。我考虑过使用强化学习和训练，记录玩家在特定时间点后的位置，但我对机器学习还不熟悉，在大学里只上过 2 门关于这个主题的课程，如果能得到任何建议，我将不胜感激。
还没有写过代码，主要是想找个起点。我考虑过使用 TensorFlow，但我唯一的经验是使用基本语言模型，而不是训练实体来执行操作]]></description>
      <guid>https://stackoverflow.com/questions/78689322/minecraft-mod-machine-learning</guid>
      <pubDate>Sun, 30 Jun 2024 17:30:40 GMT</pubDate>
    </item>
    <item>
      <title>从 YOLO 格式标记文本文件生成模型/ckpt 文件？</title>
      <link>https://stackoverflow.com/questions/78689257/generate-model-ckpt-file-from-yolo-format-labeled-text-file</link>
      <description><![CDATA[我正在尝试使用注释工具从头开始创建自己的 ckpt 文件，首先注释两个被归类为两个不同类别的图像及其宽度、高度和位置信息，如下所示：
&lt;object-class&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;

现在给定 .txt YOLO 格式的标记文本文件，我想生成一个模型/检查点文件，用于另一个程序中的图像分类。
怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78689257/generate-model-ckpt-file-from-yolo-format-labeled-text-file</guid>
      <pubDate>Sun, 30 Jun 2024 17:02:39 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 DB 扫描来聚类椭圆？</title>
      <link>https://stackoverflow.com/questions/78689202/is-it-possible-to-use-db-scan-to-cluster-ellipses</link>
      <description><![CDATA[所以我有一组椭圆形的轮廓。轮廓以特定角度倾斜，我想将这些轮廓聚类。如果两个相邻的细菌的质心相距小于距离 R，并且它们的运动方向相差小于角度 A，我们将它们定义为同一簇的成员。
我的轮廓是这样的：

所以我想问一下我们是否可以在这种情况下使用 DB-scan 算法？据我所知，我们定义一个半径为 R 的小圆，如果这些点的总数在我们之前设定的标准范围内，那么该圆中重叠的所有点都将成为核心点。从这些核心点开始，集群不断扩展，然后包含非核心点。
我不明白如何将角度的其他条件纳入其中。我是否应该首先使用 DB 扫描基于 R 查找集群，然后使用其他可以处理角度的算法过滤掉这个结果？或者有没有办法使用 DB 扫描本身来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78689202/is-it-possible-to-use-db-scan-to-cluster-ellipses</guid>
      <pubDate>Sun, 30 Jun 2024 16:41:42 GMT</pubDate>
    </item>
    <item>
      <title>使用嵌入技术从数据库中进行人脸识别</title>
      <link>https://stackoverflow.com/questions/78688976/face-recognize-from-the-database-using-embedding-technique</link>
      <description><![CDATA[我目前正在开展一个项目，旨在识别大学记录中是否存在任何个人的照片。所提出的方法涉及将每个学生照片的嵌入及其详细信息存储在矢量数据库中。当需要比较照片时，系统将生成该照片的嵌入值，然后将该值与数据库进行比较。如果该值在特定阈值内，则表明该个人存在于记录中。
我正在寻求专家建议，以确定这种方法是否可行。如果对此方法有任何疑虑，我将不胜感激最佳解决方案的建议。]]></description>
      <guid>https://stackoverflow.com/questions/78688976/face-recognize-from-the-database-using-embedding-technique</guid>
      <pubDate>Sun, 30 Jun 2024 15:09:55 GMT</pubDate>
    </item>
    <item>
      <title>大数据的 n 分割计算存在问题</title>
      <link>https://stackoverflow.com/questions/78688933/having-problem-with-n-split-computation-for-large-data</link>
      <description><![CDATA[我正在执行元特征提取的任务，但计算在某些时候似乎消耗了所有可用内存，因此 Ubuntu 不断终止进程（Killed）以过度使用内存。
我决定将计算拆分为较小的任务，然后汇总最终结果。这似乎没问题，但我确实注意到一些指标计算失败了。我在两种情况下对小数据集进行了测试：1）使用整个数据集（因为它适合内存），2）使用我实现的 n_splits 计算。
我预计场景 2 的最终结果将大致接近场景 1。但是，它也无法计算这些度量。
为了给出 MWE，我使用 iris 数据集 进行了说明，如下所示：
!pip -q install pymfe # 用于元特征计算的库。

将 numpy 导入为 np
从 sklearn.datasets 导入 load_iris
从 pymfe.mfe 导入 MFE

data = load_iris()
X, y= data.data, data.target


场景 1 计算整个数据
features_to_compute = [&#39;f1&#39;, &#39;f2&#39;, &#39;f3&#39;, &#39;t1&#39;] # 元特征
summaries = [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;, &#39;sd&#39;] # 摘要

extractor = MFE(features=features_to_compute, groups=[&quot;complexity&quot;], summary=summaries)
extractor.fit(X,y)
res = extractor.extract()

# 结果
for i in range(len(res[0])):
var_sp = res[0][i].split(&#39;.&#39;)
g_name = var_sp[0]
print(f&quot;{res[0][i]} = {res[1][i]}\n&quot;)

f1.max = 0.599217152923665

f1.mean = 0.2775641932566493

f1.min = 0.05862828094263208

f1.sd = 0.2612622587707819

f2.max = 0.01914529914529914

f2.mean = 0.0063817663817663794

f2.min = 0.0

f2.sd = 0.011053543615254369

f3.max = 0.37

f3.mean = 0.12333333333333334

f3.min = 0.0

f3.sd = 0.21361959960016152

t1 = 0.12

这看起来不错。
场景 2 将超出可用内存的大型数据集拆分为较小的任务。
# 辅助函数
def split_dataset(X, y, n_splits):
# 将数据拆分为 n_splits 个较小的数据集。
split_X = np.array_split(X, n_splits)
split_y = np.array_split(y, n_splits)
return split_X, split_y

def compute_meta_features(X, y, features, summary):
# 计算给定数据集分割的元特征。
extractor = MFE(features=features, groups=[&quot;complexity&quot;], summary=summary)
extractor.fit(X,y)
return extractor.extract()

def average_results(results):
# 对多个分割的结果取平均值。
features = results[0][0]
summary_values = np.mean([result[1] for result in results], axis=0)
return features, summary_values

n_splits = 10
split_X, split_y = split_dataset(X, y, n_splits)

results = [compute_meta_features(X_part, y_part, features=features_to_compute,
summary=summaries) for X_part, y_part in zip(split_X, split_y)]
# 此处堆栈跟踪发出了几个警告，例如
0/dist-packages/pymfe/_internal.py:731: RuntimeWarning: 无法使用摘要“mean”总结特征“f2”。将设置为“np.nan”。
warnings.warn(
/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:731: RuntimeWarning: 无法使用摘要“max”总结功能“f2”。将设置为“np.nan”。
warnings.warn(
/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:731: RuntimeWarning: 无法使用摘要“min”总结功能“f2”。将设置为“np.nan”。
warnings.warn(
/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:731: RuntimeWarning: 无法使用摘要“sd”总结功能“f2”。将设置为“np.nan”。
warnings.warn(
/usr/local/lib/python3.10/dist-packages/pymfe/_internal.py:731: RuntimeWarning: 无法使用摘要“mean”总结特征“f3”。将设置为“np.nan”。


场景 2 的结果：
final_features, final_summary = average_results(results)
for i in range(len(final_features)):
var_sp = final_features[i].split(&#39;.&#39;)
g_name = var_sp[0]
print(f&quot;{final_features[i]} = {final_summary[i]}\n&quot;)

f1.max = 0.9378374974227318

f1.mean = 0.8736795575459622

f1.min = 0.8198589466408711

f1.sd = 0.058203211724503635

f2.max = nan

f2.mean = nan

f2.min = nan

f2.sd = nan

f3.max = nan

f3.mean = nan

f3.min = nan

f3.sd = nan

t1 = nan

f1、f2、f3、... 均为 nan。使用 openml volcanoesa1 数据集获得了类似的结果。
我不明白是什么原因造成的，问题出在哪里。如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78688933/having-problem-with-n-split-computation-for-large-data</guid>
      <pubDate>Sun, 30 Jun 2024 14:57:46 GMT</pubDate>
    </item>
    <item>
      <title>如何进入 ML？[关闭]</title>
      <link>https://stackoverflow.com/questions/78688663/how-to-get-into-ml</link>
      <description><![CDATA[您好，我使用 Python 编程大约 2 个月了。可以说我处于初学者和中级水平之间。我做过一些项目、一些游戏等等，但现在我不知道该做什么或学什么。我对 ML 非常感兴趣，但我不知道如何进入这个领域，甚至不知道我是否应该尝试它，或者更确切地说，做一些更简单的事情并获得更多技能。
感谢您的所有回答。]]></description>
      <guid>https://stackoverflow.com/questions/78688663/how-to-get-into-ml</guid>
      <pubDate>Sun, 30 Jun 2024 13:05:19 GMT</pubDate>
    </item>
    <item>
      <title>编码图像缩放后的质量下降</title>
      <link>https://stackoverflow.com/questions/78688658/degradation-after-scaling-of-coded-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78688658/degradation-after-scaling-of-coded-images</guid>
      <pubDate>Sun, 30 Jun 2024 13:01:40 GMT</pubDate>
    </item>
    <item>
      <title>如何优化 PyTorch 和 Ultralytics Yolo 代码以在 Python 中利用 GPU？</title>
      <link>https://stackoverflow.com/questions/78687946/how-to-optimize-pytorch-and-ultralytics-yolo-code-to-utilize-gpu-in-python</link>
      <description><![CDATA[我正在做一个涉及对象检测和跟踪的项目。对于对象检测，我使用 yolov8，对于跟踪，我使用 SORT 跟踪器。运行以下代码后，我的 GPU 使用率始终低于 10%，而 CPU 使用率始终超过 40%。我安装了 cuda、cudnn，并使用 cuda 安装了 torch。我还编译了支持 cuda 的 opencv。我正在使用 RTX 4060 ti，但看起来它没有被使用。
有没有办法进一步优化下面的代码，以便所有工作都由 GPU 而不是 CPU 处理？
from src.sort import *
import cv2
import time
import torch
import numpy as np
from ultralytics import YOLO

device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
print(f&quot;Using device: {device}&quot;)
sort_tracker = Sort(max_age=20, min_hits=2, iou_threshold=0.05)
model = YOLO(&#39;yolov8s.pt&#39;).to(device)

cap = cv2.VideoCapture(0)

while True:
ret, frame = cap.read() 
if not ret:
print(&quot;**未收到帧**&quot;)
继续

results = model(frame)
dets_to_sort = np.empty((0, 6))
for result in results:
for obj in result.boxes:
bbox = obj.xyxy[0].cpu().numpy().astype(int)
x1, y1, x2, y2 = bbox

conf = obj.conf.item()
class_id = int(obj.cls.item())
dets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, class_id])))

tracked_dets = sort_tracker.update(dets_to_sort)
for det in tracked_dets:
x1, y1, x2, y2 = [int(i) for i in det[:4]]
track_id = int(det[8]) if det[8] 不为 None else 0
class_id = int(det[4])
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 4)
cv2.putText(frame, f&quot;{track_id}&quot;, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)

frame = cv2.resize(frame, (800, int(frame.shape[0] * 800 / frame.shape[1])), interpolation=cv2.INTER_NEAREST)
cv2.imshow(&quot;Frame&quot;, frame)
key = cv2.waitKey(1)
如果 key == ord(&quot;q&quot;):
break
如果 key == ord(&quot;p&quot;):
cv2.waitKey(-1)

cap.release()
cv2.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/78687946/how-to-optimize-pytorch-and-ultralytics-yolo-code-to-utilize-gpu-in-python</guid>
      <pubDate>Sun, 30 Jun 2024 07:43:52 GMT</pubDate>
    </item>
    <item>
      <title>对 GAN 输出大小的困惑</title>
      <link>https://stackoverflow.com/questions/78687394/confusion-about-output-sizes-of-gan</link>
      <description><![CDATA[我正在尝试理解代码，我对测试单元感到困惑。当我打印输出的形状时，它是 hidden_​​output.shape =(num_test, 20, 4, 4), test_hidden_​​block_stride(hidden_​​output).shape) == (num_test, 20, 10, 10) 和 Gen_output.shape=(num_test, 1,28,28)（对于 Mnist 数据集）。我试图理解这里的大小是如何计算的。任何帮助都将不胜感激！
class Generator(nn.Module):
def __init__(self, z_dim=10, im_chan=1, hidden_​​dim=64):
super(Generator, self).__init__()
self.z_dim = z_dim
# 构建神经网络
self.gen = nn.Sequential(
self.make_gen_block(z_dim, hidden_​​dim * 4),
self.make_gen_block(hidden_​​dim * 4, hidden_​​dim * 2, kernel_size=4, stride=1),
self.make_gen_block(hidden_​​dim * 2, hidden_​​dim),
self.make_gen_block(hidden_​​dim, im_chan, kernel_size=4, final_layer=True),
)
def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, padding=0 ,final_layer=False):
# 构建神经块
layer = []
layer.append(nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding, output_padding=padding))
if not final_layer:
layer.append(nn.BatchNorm2d(output_channels))
layer.append(nn.ReLU(True))
else:
layer.append(nn.Tanh())

return nn.Sequential(*layers)
# 测试
gen = Generator()
num_test = 100
# 测试隐藏块
test_hidden_​​noise = get_noise(num_test, gen.z_dim)
test_hidden_​​block = gen.make_gen_block(10, 20, kernel_size=4, stride=1)
test_uns_noise = gen.unsqueeze_noise(test_hidden_​​noise)
hidden_​​output = test_hidden_​​block(test_uns_noise)
# 检查它是否与其他 strides 兼容
test_hidden_​​block_stride = gen.make_gen_block(20, 20, kernel_size=4, stride=2)
test_final_noise = get_noise(num_test, gen.z_dim) * 20
test_final_block = gen.make_gen_block(10, 20, final_layer=True)
test_final_uns_noise = gen.unsqueeze_noise(test_final_noise)
final_output = test_final_block(test_final_uns_noise)
# 测试整个过程：
test_gen_noise = get_noise(num_test, gen.z_dim)
test_uns_gen_noise = gen.unsqueeze_noise(test_gen_noise)
gen_output = gen(test_uns_gen_noise)

我正在尝试手动计算公式中的大小。我只是看到不同的内核大小、步幅和填充。不确定要使用哪些值。]]></description>
      <guid>https://stackoverflow.com/questions/78687394/confusion-about-output-sizes-of-gan</guid>
      <pubDate>Sun, 30 Jun 2024 00:41:37 GMT</pubDate>
    </item>
    <item>
      <title>我该如何修复“缓存可能已过期”错误？</title>
      <link>https://stackoverflow.com/questions/78685493/how-do-i-the-fix-cache-may-be-out-of-date-error</link>
      <description><![CDATA[我使用 YOLOv5s 作为预训练模型，以十张图片作为数据集，成功创建了一个训练模型。
但是当我使用另外十张图片创建第二个模型时，我遇到了麻烦。
我的第一个和第二个代码相似，但保存的模型名称不同。
我使用 Spyder 3.11 作为我的 IDE
import torch
from PIL import Image
import os

# 步骤 1：加载 YOLOv5 模型
model = torch.hub.load(&#39;ultralytics/yolov5&#39;, &#39;yolov5s&#39;)

# 步骤 2：准备训练数据
dataset_path = &#39;C:/Users/Stk/Desktop&#39;
image_files = [&#39;eye_1.png&#39;, &#39;eye_2.png&#39;, &#39;eye_3.png&#39;, &#39;eye_4.png&#39;, &#39;eye_5.png&#39;]
images = [Image.open(os.path.join(dataset_path, f)) for f in image_files]

# 步骤 3：根据训练数据拟合模型
results = model(images)

# 步骤 4：检查结果
display(results.pandas().xyxy[0])

# 步骤 5：保存模型以供相机使用
model.eval()
save_path = os.path.join(dataset_path, &#39;yolov5s.pt&#39;)
torch.save(model.state_dict(), save_path)

我收到此错误：
异常：&#39;model&#39;。缓存可能已过期，请尝试“force_reload=True”或参阅 https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading 寻求帮助。
我尝试了 force_reload=True，但对我没有帮助。还阅读了所有错误链接网站的指南并尝试了所有步骤，但也要考虑它们的实用性。
我花了大约七天时间，目前我就像疯了一样。]]></description>
      <guid>https://stackoverflow.com/questions/78685493/how-do-i-the-fix-cache-may-be-out-of-date-error</guid>
      <pubDate>Sat, 29 Jun 2024 08:45:22 GMT</pubDate>
    </item>
    <item>
      <title>我无法使用 FastAPI 运行使用 Tensorflow 保存的模型</title>
      <link>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</guid>
      <pubDate>Wed, 26 Jun 2024 19:25:44 GMT</pubDate>
    </item>
    <item>
      <title>在深度学习中对同一数据集中的两个分类数据进行编码</title>
      <link>https://stackoverflow.com/questions/76149232/encoding-two-categorial-data-present-in-same-dataset-in-deep-learning</link>
      <description><![CDATA[我有一个数据集，其中包含列 reason 和 issue。
我想将其编码为：
enc = OneHotEncoder()
reason_no_enc = enc.fit_transform(temp[&#39;REASON NO&#39;].values.reshape(-1, 1)).toarray()
issue_enc = enc.fit_transform(temp[&#39;Issue&#39;].values.reshape(-1, 1)).toarray()

但我意识到它正在产生问题，后者 issue_enc 被认为是编码的，当我尝试反转 reason_no_enc 时，它会生成错误。
如何处理？]]></description>
      <guid>https://stackoverflow.com/questions/76149232/encoding-two-categorial-data-present-in-same-dataset-in-deep-learning</guid>
      <pubDate>Mon, 01 May 2023 18:22:08 GMT</pubDate>
    </item>
    <item>
      <title>Detectron2 在 Docker 容器中使用 layoutparser 预训练模型错误：未找到检查点</title>
      <link>https://stackoverflow.com/questions/76098441/detectron2-pre-trained-model-using-layoutparser-in-docker-container-error-check</link>
      <description><![CDATA[以下是我的 Dockerfile。
来自 python:3.9
运行 apt-get clean &amp;&amp; apt-get update
pip install --upgrade pip

运行 pip install layoutparser 

运行 pip install &quot;layoutparser[ocr]&quot; 

运行 pip install pytesseract 

运行 pip install pdf2image 

运行 pip install torch 

运行 pip install torchvision

运行 apt-get install -y poppler-utils #(pdf-image) 

运行 apt-get install -y tesseract-ocr 

运行 apt-get install git #(安装 detectron2) 

运行 pip install &quot;git+https://github.com/facebookresearch/detectron2.git&quot; #(detectron2 模型) 

运行 apt-get update &amp;&amp; apt-get install ffmpeg libsm6 libxext6 -y #(运行软件包所需)

workdir /home/jovyan/work/layout_parser

volume [&quot;/home/jovyan/work/layout_parser&quot;]

CMD [&quot;python&quot;, &quot;test_code.py&quot;]

python 代码 test_code.py:
import pdf2image
import layoutparser as lp
import pytesseract
import numpy as np
import cv2
import matplotlib.pyplot as plt

pdf_file= r&quot;/home/jovyan/work/layout_parser/test_pdf.pdf&quot;
image = np.asarray(pdf2image.convert_from_path(pdf_file)[0])

model = lp.Detectron2LayoutModel(&#39;lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config&#39;)


我收到以下错误：

我尝试了以下方法解决问题，但均未成功：

使用的 torch 和 torchvision 版本的变化
使用的 python 3.7、3.8、3.9 基础版本
预建模型的 config_path 的变化使用
手动下载配置文件中的模型 -&gt; layout_parser_modelzoo
尝试在扩展 pth 和 pkl 中手动下载模型。收到以下错误：


如何使用预先训练的模型？]]></description>
      <guid>https://stackoverflow.com/questions/76098441/detectron2-pre-trained-model-using-layoutparser-in-docker-container-error-check</guid>
      <pubDate>Tue, 25 Apr 2023 07:07:40 GMT</pubDate>
    </item>
    <item>
      <title>python中两个大数据样本的二次拟合函数进行预测</title>
      <link>https://stackoverflow.com/questions/74160767/quadratic-fit-function-of-two-large-data-samples-in-python-to-predict</link>
      <description><![CDATA[使用此 csv：
https://docs.google.com/spreadsheets/d/1QbFIUE1AcFCOgDZH5K2vPColxXeBVDV-sJhS9On2BYY/edit?usp=sharing
我正在尝试弄清楚如何编写程序来拟合二次函数，以预测全球温度的变化
（y）作为年份（x）的函数。
所讨论的函数是：
函数
该程序应创建一个图表，显示训练示例和数据的二次拟合。
这是我迄今为止为弄清楚基础知识所做的尝试。只需转换一下即可：
import numpy as np
from numpy.linalg import inv
import matplotlib.pyplot as plt

#生成200个训练样本

m = 200
x = np.random.randn(m)
y = np.random.randn(1) * x ** 2 + np.random.randn(1) * x + 
np.random.randn(1)
y = y + 0.4 * np.random.randn(m)

#二次拟合

X = np.transpose([np.ones(m), x, x ** 2])
print(np.shape(X))
print(np.shape(y))

theta = inv(np.transpose(X) @X) @ np.transpose(X) @ y

plt.plot(x, y, &#39;bo&#39;)

xp = np.arange(-5, 5, 0.1)
yp = theta[0] + theta[1] * xp + theta[2] * xp ** 2

plt.plot(xp, yp, &#39;r-&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/74160767/quadratic-fit-function-of-two-large-data-samples-in-python-to-predict</guid>
      <pubDate>Sat, 22 Oct 2022 02:53:21 GMT</pubDate>
    </item>
    </channel>
</rss>