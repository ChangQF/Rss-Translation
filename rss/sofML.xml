<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 13 Feb 2024 12:23:16 GMT</lastBuildDate>
    <item>
      <title>感知器权重更新规则证明[关闭]</title>
      <link>https://stackoverflow.com/questions/77987620/perceptron-weight-update-rule-proof</link>
      <description><![CDATA[
我想要详细的数学证明，无论它可能涉及导数等统计数据。]]></description>
      <guid>https://stackoverflow.com/questions/77987620/perceptron-weight-update-rule-proof</guid>
      <pubDate>Tue, 13 Feb 2024 11:38:05 GMT</pubDate>
    </item>
    <item>
      <title>在 ML 回归上下文中压缩测试和训练集中的数据以进行一种热编码？</title>
      <link>https://stackoverflow.com/questions/77987380/condensing-data-in-test-and-train-set-for-one-hot-encoding-in-ml-regression-cont</link>
      <description><![CDATA[我正在尝试理解特征工程的想法。我在网上看到了一些相互矛盾的信息（这可能是它取决于上下文的事实），所以也许如果我给出上下文，建议的方法可能会更清楚。
我们已经获得了一个训练集和一个测试集以及一个测试集。那么我假设我们想要对两者都进行任何数据预处理吗？ （也许是故意让事情变得更具挑战性）
所以目前我需要使用回归来预测 Spotify 上歌曲的流行度得分。我们被赋予了一堆特征——艺术家、响度、bpm 等。其中一个特征是流派和节奏。问题是：

这是分类数据
唯一值太多（100+）

所以我尝试为火车组和火车组执行此操作设法将流派特征的大小压缩到显着降低（大约 30 左右？）。不太确定我是否应该更改现有专栏或添加新的“广泛流派”功能。
但是，我在如何处理测试集方面遇到了困难。我也可以应用相同类型的特征工程，但在网上阅读一些内容后，似乎有人说我们不应该更改测试集（因为我们假设我们不知道测试集的存在）。
所以我只是有点不确定如何使用此功能，或者也许应该完全删除它。
所以我知道我需要使用一种技术（例如热编码）将分类数据转换为合理的回归格式。然而，由于流派数量众多，我知道的一种方法是特征工程。
我注意到其中许多类型：

重复但略有不同，即成人标准与成人标准
可以归为一个广泛的流派，即不同类型的摇滚乐

例如：
train_df.loc[train_df[&#39;顶级流派&#39;].str.contains(&#39;摇滚&#39;), [&#39;顶级流派&#39;]] = &#39;摇滚&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/77987380/condensing-data-in-test-and-train-set-for-one-hot-encoding-in-ml-regression-cont</guid>
      <pubDate>Tue, 13 Feb 2024 10:59:38 GMT</pubDate>
    </item>
    <item>
      <title>knn手动计算与R类包的比较</title>
      <link>https://stackoverflow.com/questions/77987372/knn-manual-calculation-vs-r-class-package</link>
      <description><![CDATA[为了更好地理解 kNN 方法，我想手动复制 R 使用 class 包的 knn 函数所做的事情。
首先，可以在此处获取数据https://github.com/NPejovicE/kNN。它是一个 csv 文件，包含 3 个类别的交通标志：pedestrian、speed 和 stop。为了解释数据集，假设标志被分为 16 块，每块中心的颜色用 r/g/b 颜色代码测量。因此，每个标志有 48 列 (16 x 3)。数据分为训练数据集和测试数据集。
signs %&gt;% filter(sample == &quot;train&quot;) -&gt; &gt;训练数据
标志%&gt;%过滤器(样本==“测试”)-&gt;测试数据

我想根据测试数据预测第 12 行。
test_data %&gt;% slice(12) %&gt;% select(4:ncol(test_data)) -&gt;我的测试

我将通过 R 中的“类包”进行 kNN 分类：
knn(train_data[4:ncol(train_data)], unlist(my_test), cl = train_data$sign_type)

行人
级别：行人限速

它说这是行人。
现在，我将尝试手动计算缩放值的欧几里得距离。我将仅使用训练和测试数据中所需的列，并从测试数据中提取第 12 行。
train_data[4:ncol(train_data)] -&gt;训练数据清理
test_data[4:ncol(test_data)] -&gt;测试数据清理

as.data.frame(scale(train_data_clean)) -&gt;;缩放训练
as.data.frame(scale(test_data_clean)) -&gt;;缩放测试

scaled_test %&gt;% 切片(12) -&gt;测试行

现在逐步计算距离：
scaled_train - unlist(test_row) -&gt;差异
# 平方差异：
diff^2 -&gt;差异2
# 行间求和：
rowSums(diff2) -&gt;;差异3
# 取平方根：
sqrt(diff3) -&gt;;差异4
# 查找具有最小值的行。
其中.min(diff4) -&gt; n

它说我的 train_data 第 72 行具有最小距离。
当我回顾我的原始列车数据时，它是：
 train_data %&gt;% slice(72) %&gt;% select(sign_type)

它是 stop 而不是类包中的 pedestrian。
我如何从类包中复制结果，我在这里做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/77987372/knn-manual-calculation-vs-r-class-package</guid>
      <pubDate>Tue, 13 Feb 2024 10:58:23 GMT</pubDate>
    </item>
    <item>
      <title>MLFlow：最后一次实验运行超出了超参数调整的值范围</title>
      <link>https://stackoverflow.com/questions/77987139/mlflow-last-experiment-runs-exceed-range-of-values-of-hyperparameter-tuning</link>
      <description><![CDATA[我目前正在开发一个使用 MLFlow Recipes 的项目。到目前为止，一切都进展顺利，但是我遇到了 MLFlow 的一些奇怪行为。在我的实验中，我使用 lgbm 分类器以及超参数调整 (hyperopt) 和提前停止。因此我定义了一些参数的可接受值的范围。这是我的 recipe.yaml 的训练部分：
火车：
  Predict_scores_for_all_classes：真
  预测前缀：“预测_”
  使用：“自定义”
  estimator_method：estimator_fn
  估计器参数：
    随机状态：42
  调整：
    启用：真
    算法：“hyperopt.rand.suggest”
    最大试验次数：100
    并行度：1
    Early_stop_fn ：早期停止
    参数：
        α：
          分布：“均匀”
          低：0.0001
          高：0.1
        学习率：
          分布：“均匀”
          低：0.0001
          高：0.1
        最大深度：
          分布：“uniformint”
          低：1
          高：3
        n_估计器：
          分布：“uniformint”
          低：1000
          高：10000

到目前为止，训练进展顺利，但查看实验运行，我发现在训练结束前的最后一次运行中，max_depth 始终记录为 -1，即使 max_depth&lt; 的范围也是如此/code&gt; 是 [1, 3]。这是某种标准行为、错误还是我做错了什么？
我还注意到，如果最后一次运行结果击败了之前的最佳模型，它就会被注册，而无需经历另一个早期停止周期（十次迭代）。也许这与它有关，也许这是另一个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77987139/mlflow-last-experiment-runs-exceed-range-of-values-of-hyperparameter-tuning</guid>
      <pubDate>Tue, 13 Feb 2024 10:21:06 GMT</pubDate>
    </item>
    <item>
      <title>尝试在 GPU 上嵌入 CatBoostClassifier 时出现“CatBoostError：尝试在打包功能编写器上调用单个功能编写器”</title>
      <link>https://stackoverflow.com/questions/77987029/getting-catboosterror-attempt-to-call-single-feature-writer-on-packed-feature</link>
      <description><![CDATA[当我尝试使用带嵌入的 pandas 数据帧在 GPU 上安装 CatBoostClassifier 时，出现此错误：
CatBoostError：尝试在打包功能编写器上调用单个功能编写器

嵌入列中的数据类型是对象。 Catboost版本==1.2.2。
当我从训练中删除嵌入列时，一切正常，所以这肯定与嵌入有关。我尝试将它存储为列表、ndarray、张量。不会影响任何事情。
我规定了 embedding_features 参数。 
类不平衡或 eval_set 的存在也不会产生任何影响。
发现 task_type=&#39;GPU&#39; 导致了此错误。即使您尝试文档使用示例中的一些简单代码，但使用 task_type=&#39;GPU&#39;：
&lt;前&gt;&lt;代码&gt;cat_features = [3]
嵌入特征=[0, 1]
训练数据 = [
    [[0.1，0.12，0.33]，[1.0，0.7]，2，“男性”]，
    [[0.0,0.8,0.2],[1.1,0.2],1,“女性”],
    [[0.2，0.31，0.1]，[0.3，0.11]，2，“女性”]，
    [[0.01,0.2,0.9],[0.62,0.12],1,“男性”]
]
训练标签 = [1, 0, 0, 1]
评估数据 = [
    [[0.2，0.1，0.3]，[1.2，0.3]，1，“女性”]，
    [[0.33，0.22，0.4]，[0.98，0.5]，2，“女性”]，
    [[0.78，0.29，0.67]，[0.76，0.34]，2，“男性”]，
]

模型= CatBoostClassifier（迭代= 2，
                           学习率=1，
                           深度=2，
                           任务类型=&#39;GPU&#39;)

model.fit(train_data, train_labels, cat_features=cat_features, embedding_features=embedding_features)

preds_class = model.predict(eval_data)

但我需要它适合 GPU，所以]]></description>
      <guid>https://stackoverflow.com/questions/77987029/getting-catboosterror-attempt-to-call-single-feature-writer-on-packed-feature</guid>
      <pubDate>Tue, 13 Feb 2024 10:00:09 GMT</pubDate>
    </item>
    <item>
      <title>删除 DCA 曲线中的“Treat All”和“Treat None”</title>
      <link>https://stackoverflow.com/questions/77986685/remove-treat-all-and-treat-none-in-dca-curve</link>
      <description><![CDATA[在此处输入图像描述
# 绘制 DCA 曲线
库（dcurves）
# dca(AS1min ~ knn + Boosting, data= data.set)
dcaoutput = dca(AS1min ~ knn + Boosting, data= data.set,
  阈值 = seq(0, 0.99, by = 0.01))

dca输出%&gt;%
  绘图（类型 = &#39;net_benefit&#39;，平滑 = TRUE，show_ggplot_code = FALSE）

我正在尝试在 R 中绘制 DCA 曲线，我的问题是如何删除“Treat All”和“不治疗”图中的变量？]]></description>
      <guid>https://stackoverflow.com/questions/77986685/remove-treat-all-and-treat-none-in-dca-curve</guid>
      <pubDate>Tue, 13 Feb 2024 09:06:37 GMT</pubDate>
    </item>
    <item>
      <title>哪种 ML 模型可以处理多个输入和一个输出</title>
      <link>https://stackoverflow.com/questions/77986009/which-ml-model-can-handle-multiple-input-and-one-output</link>
      <description><![CDATA[我有一个机器学习问题。我得到的数据如下

&lt;标题&gt;

前一天
潮湿
阴
风速
城市最近下雨
最近下雨的天数


&lt;正文&gt;

第一天
60
是
&lt; 50米/秒
城市A
3


第二天
20
没有
&gt; 50米/秒
B市
1



...可以有多行。
输出是 Z 市下雨
是的
不
也许
对于 Z 市的 N 天，我有时有 2 天前的数据。有时我有 3 天前的数据。有时是 10 天前的数据。
下面是示例
13/03/2023 - Z 市下雨 - 是

&lt;标题&gt;

前一天
潮湿
阴
风速
城市最近下雨
最近下雨的天数


&lt;正文&gt;

第一天
60
是
&lt; 50米/秒
城市A
3


第二天
20
没有
&gt; 50米/秒
B市
1



2023 年 9 月 19 日 - Z 市下雨 - 否

&lt;标题&gt;

前一天
潮湿
阴
风速
城市最近下雨
最近下雨的天数


&lt;正文&gt;

第一天
60
是
&lt; 50米/秒
城市A
3


第一天
20
没有
&gt; 50米/秒
C市
1


第四天
30
是
&gt; 20米/秒
D市
5


第六天
10
没有
&lt; 50米/秒
城市Q
3



对于当前 X 天，我可以有任意数量的行，并且我想预测是否可能下雨。我该如何做到这一点以及使用哪种模型
尝试阅读可以应用的不同机器学习模型，但无法理解一个可以接受许多输入行、自动决定权重并​​预测降雨的模型。
例如，几乎可以肯定，如果第一天 A 市下雨，Z 市也会下雨]]></description>
      <guid>https://stackoverflow.com/questions/77986009/which-ml-model-can-handle-multiple-input-and-one-output</guid>
      <pubDate>Tue, 13 Feb 2024 06:58:07 GMT</pubDate>
    </item>
    <item>
      <title>通过 3dskullstripping 无 Nifiti 生成 MRI</title>
      <link>https://stackoverflow.com/questions/77985923/no-nifiti-generation-of-mri-via-3dskullstripping</link>
      <description><![CDATA[以下代码不会生成 nifiti 剥离头骨 MRI 图像。
目录看起来像这样 /4-Resample/LGG101/LGG102flair.nii.gz
“4-Resasmple”内有两个文件夹，文件夹内有一个 nifti 文件。
头骨条纹后，它会在“Skull_strpping_folder”中创建类似的文件夹和类似的nifiti文件。但以下代码没有创建任何内容。
导入操作系统
导入子流程
从 tqdm 导入 tqdm
导入时间

# 定义输入和输出文件夹的路径
input_folder =“D：/AID/1-Anum（预处理）/Dataset-20240202T042956Z-001/Dataset/Sumaiya/4-Resampled/”
头骨_stripped_folder =“D：/AID/1-Anum（预处理）/Dataset-20240202T042956Z-001/Dataset/Sumaiya/5-Skull_Stripped/brain/”
mask_folder =“D：/AID/1-Anum（预处理）/Dataset-20240202T042956Z-001/Dataset/Sumaiya/5-Skull_Stripped/brain_mask/”

# 处理子文件夹中的 NIfTI 文件的函数
def process_subfolders(子文件夹路径):
    对于 os.walk(subfolder_path) 中的 root、_、文件：
        对于 tqdm(files) 中的文件名：
            if filename.endswith(“.nii.gz”):
                input_file = os.path.join(root, 文件名).replace(&quot;\\&quot;,&quot;/&quot;)
                打印（输入文件）
                relative_path = os.path.relpath(input_file,input_folder).replace(&quot;\\&quot;,&quot;/&quot;)
                打印（相对路径）
                头骨_stripped_output = os.path.join(skull_stripped_folder, 相对路径)
                打印（头骨剥离输出）
                mask_output = os.path.join(mask_folder, 相对路径)
# 打印（掩码输出）
                # 如果输出文件夹不存在则创建子文件夹
                os.makedirs（os.path.dirname（skull_stripped_output），exist_ok = True）
                os.makedirs（os.path.dirname（mask_output），exist_ok = True）

                # 定义 3dSkullStrip 命令
                skullstrip_cmd = f“3dSkullStrip -input {input_file} -prefix {skull_stripped_output}”

                # 运行 3dSkullStrip 命令
                subprocess.run(skullstrip_cmd, shell=True)

                # 定义 3dcalc 命令来创建二进制掩码
                calc_cmd = f“3dcalc -a {skull_stripped_output} -expr &#39;step(a)&#39; -prefix {mask_output}”`

                # 运行 3dcalc 命令
                subprocess.run(calc_cmd, shell=True)


# 处理输入文件夹子文件夹中的 NIfTI 文件
对于 os.listdir(input_folder) 中的子文件夹：
    subfolder_path = os.path.join(input_folder, subfolder).replace(&quot;\\&quot;,&quot;/&quot;)
    如果 os.path.isdir(子文件夹路径):
        进程子文件夹（子文件夹路径）
        打印（子文件夹路径）

找出代码中的错误。]]></description>
      <guid>https://stackoverflow.com/questions/77985923/no-nifiti-generation-of-mri-via-3dskullstripping</guid>
      <pubDate>Tue, 13 Feb 2024 06:35:05 GMT</pubDate>
    </item>
    <item>
      <title>如何正确准备数据集，设置 EfficientNetV2B0 模型以使用 Tensorflow 对自定义数据集进行训练 [关闭]</title>
      <link>https://stackoverflow.com/questions/77985276/how-to-properly-prepare-dataset-setting-up-efficientnetv2b0-model-for-training</link>
      <description><![CDATA[我的代码中有什么问题，结果模型没有正确分类苹果叶病？我在 4 个类的 7k 图像数据集上进行了训练，每个类都有大约 1.8k 的图像。每个类的总图像不相等，这会影响训练结果吗？或者我下面的代码有问题吗？
从 google.colab 导入驱动器
驱动器.mount(&#39;/content/gdrive&#39;)


导入压缩文件
zip_ref = zipfile.ZipFile(&#39;/content/gdrive/MyDrive/dataset/data9k.zip&#39;, &#39;r&#39;)
zip_ref.extractall(“/内容/数据集”)
zip_ref.close()


将张量流导入为 tf
从tensorflow.keras.applications.imagenet_utils导入preprocess_input
将 matplotlib.pyplot 导入为 plt


train_dataset = tf.keras.utils.image_dataset_from_directory(
&#39;/内容/数据集/数据集/火车&#39;,
批量大小=10，
图像大小=(224, 224),
标签=&#39;推断&#39;,
label_mode=&#39;分类&#39;
）


validation_dataset = tf.keras.utils.image_dataset_from_directory(
&#39;/内容/数据集/数据集/测试&#39;,
批量大小=10，
图像大小=(224, 224),
标签=&#39;推断&#39;,
label_mode=&#39;分类&#39;
）


val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = valid_dataset.take(val_batches // 5)
validation_dataset =validation_dataset.skip(val_batches // 5)


自动调谐 = tf.data.AUTOTUNE
train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset =validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)


data_augmentation = tf.keras.Sequential(
[tf.keras.layers.RandomFlip(&#39;水平&#39;),
tf.keras.layers.RandomRotation(0.2)]
）


模型 = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(
include_top=假，
权重=无，
输入张量=无，
输入形状=(224, 224, 3),
池=&#39;平均&#39;，
include_preprocessing=True
）


#model.trainable=False


Prediction_layer = tf.keras.layers.Dense(4, 激活=&#39;softmax&#39;)


输入 = tf.keras.Input(形状=(224, 224, 3))
x = 数据增强（输入）
#x = 预处理输入(x)
x = 模型(x)
x = tf.keras.layers.Dropout(0.2)(x)
输出=预测层(x)
模型= tf.keras.Model（输入，输出）


模型.编译(
优化器=tf.keras.optimizers.Adam(learning_rate=1e-4),
损失=tf.keras.losses.CategoricalCrossentropy(),
指标=[&#39;准确性&#39;]
）


模型.拟合(
训练数据集，
验证数据=验证数据集，
纪元=10
）


将 numpy 导入为 np
从tensorflow.keras.preprocessing导入图像


img_path = &#39;gdrive/MyDrive/dataset/rust.jpg&#39;
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, 轴=0)


预测 = model.predict(img_array)


class_names = [&#39;apple_scab&#39;, &#39;black_rot&#39;, &#39;cedar_apple_rust&#39;, &#39;healthy&#39;]


Predicted_index = np.argmax(预测[0])
标签=类名[预测索引]
print(&quot;预测标签：&quot;, label)
]]></description>
      <guid>https://stackoverflow.com/questions/77985276/how-to-properly-prepare-dataset-setting-up-efficientnetv2b0-model-for-training</guid>
      <pubDate>Tue, 13 Feb 2024 02:23:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 GPytorch 的多输出 GP</title>
      <link>https://stackoverflow.com/questions/77985900/multiple-output-gp-using-gpytorch</link>
      <description><![CDATA[我正在尝试按照基本教程为下面的数据训练高斯过程https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/Simple_GP_Regression.html
train_x=[4058, 12] 的张量
train_y=[4058, 140] 的张量

我在损失计算中遇到错误：
loss = -mll(输出, train_y)

表示output (model(train_x)) 和train_y 没有相同的维度。鉴于此，我尝试了 MultitaskGPModel https:// /docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html 出现非常相似的错误
运行时错误：张量 a (568120) 的大小必须与非单维 0 处张量 b (8116) 的大小匹配

显然MultitaskGPModel要求相同的条目总数相等。有没有办法培养多次入境的全科医生？]]></description>
      <guid>https://stackoverflow.com/questions/77985900/multiple-output-gp-using-gpytorch</guid>
      <pubDate>Mon, 12 Feb 2024 22:47:00 GMT</pubDate>
    </item>
    <item>
      <title>MLFlow 可以在没有“with mlflow.start_run()”块的情况下使用吗？</title>
      <link>https://stackoverflow.com/questions/77983736/can-mlflow-be-used-without-the-with-mlflow-start-run-block</link>
      <description><![CDATA[我想跟踪整个笔记本并记录训练模型之前发生的清洁步骤的参数。我想使用 mlflow 来执行此操作，但在所有文档中，您似乎必须使用此格式跟踪模型：
与 mlflow.start_run():
    ...

有没有办法使用 mlflow 跟踪整个笔记本而不使用 with 块？]]></description>
      <guid>https://stackoverflow.com/questions/77983736/can-mlflow-be-used-without-the-with-mlflow-start-run-block</guid>
      <pubDate>Mon, 12 Feb 2024 19:05:13 GMT</pubDate>
    </item>
    <item>
      <title>如何包装 keras 模型以供 scikit-learn 堆叠集成使用</title>
      <link>https://stackoverflow.com/questions/77982056/how-to-wrap-keras-models-for-scikit-learn-stacking-ensemble-usages</link>
      <description><![CDATA[我有一个已经训练过的 keras 模型列表。我想在 scikit learn 中将它们与 StackingClassifier 一起使用。由于 keras 没有 Predict_proba 方法，我创建了一个包装器。
如果使用我的为 VotingClassifier 包装的模型以及软方法和硬方法，它就可以工作。
但是当我使用堆叠模型时，第一次运行后，它会向我显示此错误。我没有找到任何相关信息。
类 KerasWrapperWithEncoder(BaseEstimator, ClassifierMixin):
    def __init__(自身，keras_model，classes_)：
        self.keras_model = keras_model
        self.encoder = OneHotEncoder(sparse_output=False)
        # L&#39;encoder OneHotEncoder 已经过去了
        self.classes_ = classes_ # 定义可分配类

    def fit(自身, X, y):
        # 模型已安装，不再适合
        y_reshape = y.reshape(-1, 1)
        self.encoder.fit(y_reshape)
        返回自我

    def 预测（自身，X）：
        # 利用 les modèles entraînés pour faire des predictions
        预测 = self.keras_model.predict(X)
        np_argmax = np.argmax(预测，轴=1)
        打印（预测）
        打印（np_argmax）
        返回 np_argmax

    def Predict_proba(自身, X):
        # 返回分类模型的类别概率
        概率 = self.keras_model.predict(X)
        print(&quot;概率形状：&quot;, probabilities.shape) # 调试
        返回概率


keras_wrapped_models_with_encoder = [
    (name.replace(&#39; &#39;, &#39;_&#39;).replace(&#39;__&#39;, &#39;_&#39;), KerasWrapperWithEncoder(model, _target_classes_))
    对于 keras_models.items() 中的名称、模型
]

vote_clf = 投票分类器(
         估计器=all_估计器，
         投票=&#39;软&#39;，
         n_职位=3，
         详细=真）
vote_clf .fit(X_train, y_train) # 完美运行

cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
keras_stacking_models_current_year = StackingClassifier(
    估计器=all_估计器，
    Final_estimator=LogisticRegression(),
    简历=简历，
    详细=3，
    # n_jobs=2
）


keras_stacking_models_current_year.fit(X_train, y_train) # 抛出错误


&lt;前&gt;&lt;代码&gt;========================================stacking_models_all_models===== =======================
12105/12105 [================================] - 25s 2ms/步
概率形状：(387348, 3)
训练折叠 (1) 中的类数与类总数 (3) 不匹配。结果可能不适合您的用例。要解决此问题，请使用交叉验证技术来产生正确分层的折叠
_enforce_prediction_order（类、预测、n_classes、方法）
   第1457章
   第1458章）
-&gt;第1459章
   第1460章 1460
   第1461章 回归预测

ValueError：形状不匹配：形状（387348,3）的值数组无法广播到形状（387348,1,3）的索引结果
]]></description>
      <guid>https://stackoverflow.com/questions/77982056/how-to-wrap-keras-models-for-scikit-learn-stacking-ensemble-usages</guid>
      <pubDate>Mon, 12 Feb 2024 14:12:27 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow ImportError：无法导入名称“dtensor”</title>
      <link>https://stackoverflow.com/questions/77828746/tensorflow-importerror-cannot-import-name-dtensor</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77828746/tensorflow-importerror-cannot-import-name-dtensor</guid>
      <pubDate>Tue, 16 Jan 2024 21:13:45 GMT</pubDate>
    </item>
    <item>
      <title>Coco注释：将RLE转换为多边形分割</title>
      <link>https://stackoverflow.com/questions/75326066/coco-annotations-convert-rle-to-polygon-segmentation</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/75326066/coco-annotations-convert-rle-to-polygon-segmentation</guid>
      <pubDate>Thu, 02 Feb 2023 16:22:04 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow中标准化函数的作用是什么？</title>
      <link>https://stackoverflow.com/questions/53786149/what-is-the-role-of-normalization-function-in-tensorflow</link>
      <description><![CDATA[我学习机器学习并尝试构建一个简单的张量流模型。当我尝试训练模型时，我的损失数约为 10。
5s 83us/步 - 损耗：9.6847 - acc：0.3971
模型代码：
模型 = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128，激活=tf.nn.relu),
    keras.layers.Dense(128，激活=tf.nn.relu),
    keras.layers.Dense(10, 激活=tf.nn.softmax)
]）

model.compile(优化器=tf.train.AdamOptimizer(),
          损失=&#39;sparse_categorical_crossentropy&#39;,
          指标=[&#39;准确性&#39;])

model.fit(x_train, y_train, epochs=3)

但后来我使用此代码标准化了数据集
x_train = keras.utils.normalize(x_train, axis=1)
然后损失降至小于 1。
问题是它是如何产生如此巨大的影响的？]]></description>
      <guid>https://stackoverflow.com/questions/53786149/what-is-the-role-of-normalization-function-in-tensorflow</guid>
      <pubDate>Fri, 14 Dec 2018 19:55:09 GMT</pubDate>
    </item>
    </channel>
</rss>