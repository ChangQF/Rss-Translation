<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 18 Jun 2024 15:15:49 GMT</lastBuildDate>
    <item>
      <title>两个单一模型还是一个多类模型？</title>
      <link>https://stackoverflow.com/questions/78638050/two-mono-models-or-one-multi-class-model</link>
      <description><![CDATA[我需要预测下一个目标事件 - 购买两个价格类别的汽车（例如，高档和中档）。训练的目标数量大致相同（假设每个价格类别有 10,000 次购买）。我在这里看到两种训练机器学习模型的方法。第一种是多类模型。第二种是两个单独的模型来预测每个细分市场。您认为应该采用哪种方法，为什么？我想以多类模型为基础，您能列出哪些优点和缺点吗？]]></description>
      <guid>https://stackoverflow.com/questions/78638050/two-mono-models-or-one-multi-class-model</guid>
      <pubDate>Tue, 18 Jun 2024 14:23:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在没有特定文档的情况下估计在大型语言模型上运行推理的硬件要求？[关闭]</title>
      <link>https://stackoverflow.com/questions/78637912/how-to-estimate-hardware-requirements-for-running-inference-on-large-language-mo</link>
      <description><![CDATA[我有兴趣在我的计算机上本地运行一个大型语言模型进行推理。我想选择一个可以在线免费访问的模型，但具体的 CPU/RAM 或 GPU/VRAM 要求通常不会在他们的 Hugging Face 或 Azure 页面上提供，而且我在网上其他地方也找不到它们。
当除了上下文长度和参数数量之外没有提供任何要求信息时，我该如何估计给定模型的硬件要求？
例如：我想在我的计算机上运行最大可能的 Phi-3 模型版本，但我在模型页面或网络上找不到任何要求信息。最大的版本之一是这个，上下文长度为 128_000，参数为 14B，我该如何推断硬件要求？]]></description>
      <guid>https://stackoverflow.com/questions/78637912/how-to-estimate-hardware-requirements-for-running-inference-on-large-language-mo</guid>
      <pubDate>Tue, 18 Jun 2024 13:56:25 GMT</pubDate>
    </item>
    <item>
      <title>pmdarima 中的 auto_arima 的执行时间随着 m 的增加而疯狂增加，原因是什么？</title>
      <link>https://stackoverflow.com/questions/78636875/auto-arima-from-pmdarima-has-insane-execution-times-increases-at-the-growing-of</link>
      <description><![CDATA[我正在尝试为我的时间序列拟合 SARIMA 模型，为了找到最佳模型，我正在使用 pmdarima 的 autom_arima。代码很简单：
stepwise_fit = pm.auto_arima(train_dataset[&#39;shift_hours&#39;], start_p=1, start_q=1,
max_p=3, max_q=3, m=365,
start_P=0, seasonal=True,
d=1, D=1, trace=True,
error_action=&#39;ignore&#39;, 
suppress_warnings=True, 
stepwise=True) 

其中 &#39;train_dataset[&#39;shift_hours&#39;]&#39; 由大约两年的每日值组成，这些值具有 365 天的季节性。
我遇到的问题与执行时间有关：当 m 设置为 1 时，在逐步搜索中执行一步需要不到一秒的时间，当它为 7 或 12 时，一步的执行时间在 1 到 2 秒之间，但是当我将 m 设置为 365（我需要的值），执行单个步骤需要 30 多分钟。如果我使用 girdsearch 并将 n_jobs &gt; 4，则会出现内存错误。
就我对 SARIMA 的了解而言，当我使用更大的周期时，它不应该花费更多时间来拟合一个系列，因为它用于对系列进行建模的术语数量不依赖于 m，所以我想知道那里发生了什么，以及是否发生了什么我无法理解的事情]]></description>
      <guid>https://stackoverflow.com/questions/78636875/auto-arima-from-pmdarima-has-insane-execution-times-increases-at-the-growing-of</guid>
      <pubDate>Tue, 18 Jun 2024 10:36:25 GMT</pubDate>
    </item>
    <item>
      <title>如何准确检测不同音轨中主节拍和配乐的开始？</title>
      <link>https://stackoverflow.com/questions/78636871/how-to-accurately-detect-the-start-of-the-main-beat-and-soundtracks-in-diverse-a</link>
      <description><![CDATA[我正在做一个需要编辑配乐的项目。挑战在于检测任何给定配乐的主要节拍和旋律何时得到正确发展。我确信有更好的术语来描述我的目标，但理想情况下，我想跳过“构建”并立即让歌曲从“主要部分”开始。这需要适用于不同类型的各种歌曲，这些歌曲通常具有不同的结构和开始模式，这使得简化流程变得困难。
例如：
https://www.youtube.com/watch?v=P77CNtHrnmI -&gt;我希望我的代码能够识别 0:24 处的开始
https://www.youtube.com/watch?v=OOsPCR8SyRo -&gt; 0:12 处的开始检测
https://www.youtube.com/watch?v=XKiZBlelIzc -&gt; 0:19 处的起始检测
我尝试使用 librosa 分析起始强度并检测节拍，但当前的实现要么检测到歌曲的最开始，要么无法一致地识别节拍何时完全形成。
这是我的方法；
def analyze_and_edit_audio(input_file, output_file):
y, sr = librosa.load(input_file)
tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)
beat_times = librosa.frames_to_time(beat_frames, sr=sr)
main_beat_start = beat_times[0]

我对 librosa/audio 编辑经验很少，因此如果您有任何建议，我将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78636871/how-to-accurately-detect-the-start-of-the-main-beat-and-soundtracks-in-diverse-a</guid>
      <pubDate>Tue, 18 Jun 2024 10:35:15 GMT</pubDate>
    </item>
    <item>
      <title>用于文本生成的微调 BERT 模型（填字游戏解答器）</title>
      <link>https://stackoverflow.com/questions/78636736/fine-tuning-bert-model-for-text-generation-crossword-solver</link>
      <description><![CDATA[我需要在我的 NLP 项目中提供帮助，该项目的目标是预测给定填字游戏线索的可能答案列表。这个想法是使用填字游戏线索 - 答案对的数据集微调 BERT 模型。
train.source 看起来像这样：
机场排队，一种煎蛋卷，苏萨是它的首都，后缀为洞穴......或峡谷？，九：前缀，龙的猎物，一些烟火，...
train.targets 看起来像这样：LIMOS、EGGWHITE、ELAM、OUS、ENNEA、MAIDEN、FLARES，...
目前，我有一个用于 train、test 和 val 中的数据集的加载函数。 (len(train) = 433034, len(val) = 72304, len(test) = 72940)
我从 BERT 加载了两个模型，即 tokenizer 和 model：
tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
model = BertForMaskedLM.from_pretrained(&#39;bert-base-uncased&#39;)

然后我创建了一个 BERTDataset 类：
class BERTDataset(Dataset):
def __init__(self, tokenizer, texts, target=None, max_length=512):
self.tokenizer = tokenizer
self.texts = texts
self.targets = target # 如果提供了特定目标响应，则可选使用
self.max_length = max_length

def __len__(self):
return len(self.texts)

def __getitem__(self, idx):
text = self.texts[idx]
# 将 &#39;[MASK]&#39; 附加到文本末尾
text_with_mask = text + &quot; ? [MASK].&quot;

# 使用附加的 MASK 标记对文本进行编码
encoding = self.tokenizer.encode_plus(
text_with_mask,
max_length=self.max_length,
padding=&#39;max_length&#39;,
truncation=True,
return_tensors=&#39;pt&#39;
)

input_ids = encoding[&#39;input_ids&#39;].squeeze(0) # 删除批次维度
tention_mask = encoding[&#39;attention_mask&#39;].squeeze(0)

# 如果不需要预测，则标签理想情况下应为 -100
labels = input_ids.detach().clone()
# 如果输入 ID 未被屏蔽，则将标签设置为 -100
labels[labels != self.tokenizer.mask_token_id] = -100

return {
&#39;input_ids&#39;: input_ids,
&#39;attention_mask&#39;:tention_mask,
&#39;labels&#39;: labels
}

train_dataset = BERTDataset(tokenizer, train_sources, train_targets, max_length=128)
val_dataset = BERTDataset(tokenizer, val_sources, val_targets, max_length=128)

最后启动训练：
from transformers import TrainingArguments,Trainer

# 定义训练参数
training_args = TrainingArguments(
output_dir=&#39;./results&#39;, # 输出目录
num_train_epochs=3, # 训练周期数
per_device_train_batch_size=8, # 训练批次大小
per_device_eval_batch_size=16, # 评估批次大小
warmup_steps=500, # 学习率调度程序的预热步骤数
weight_decay=0.005, # 权重衰减强度
logs_dir=&#39;./logs&#39;, # 存储日志的目录
logging_steps=10,
)

# 初始化 Trainer
trainer = Trainer(
model=model,
args=training_args,
train_dataset=train_dataset,
eval_dataset=val_dataset,

#compute_metrics=lambda eval_pred: {&quot;loss&quot; : eval_pred.loss}
)

# 开始训练
trainer.train()

我遇到了一些问题，主要问题是训练损失很快变为 0，我不明白为什么（从步骤 510 开始，训练损失为 0）：

我做错了什么吗？我真的不明白为什么模型没有正确训练。谢谢！！]]></description>
      <guid>https://stackoverflow.com/questions/78636736/fine-tuning-bert-model-for-text-generation-crossword-solver</guid>
      <pubDate>Tue, 18 Jun 2024 10:06:20 GMT</pubDate>
    </item>
    <item>
      <title>Android 中的 Movenet Singlepose 照明模型：“不支持的图像格式：1”错误</title>
      <link>https://stackoverflow.com/questions/78636622/movenets-singlepose-lighting-model-in-android-unsupported-image-format-1-e</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78636622/movenets-singlepose-lighting-model-in-android-unsupported-image-format-1-e</guid>
      <pubDate>Tue, 18 Jun 2024 09:42:32 GMT</pubDate>
    </item>
    <item>
      <title>我如何重新训练 Xgboost 回归器？</title>
      <link>https://stackoverflow.com/questions/78636365/how-can-i-retrain-a-xgboost-regressor</link>
      <description><![CDATA[我现在正在做的实习要求模型“定期更新新数据”。如何在 Xgboost 上做到这一点？我已将模型保存为 pickle 文件
在项目中，我将从后端获取新数据，并且应该定期重新训练模型]]></description>
      <guid>https://stackoverflow.com/questions/78636365/how-can-i-retrain-a-xgboost-regressor</guid>
      <pubDate>Tue, 18 Jun 2024 08:51:09 GMT</pubDate>
    </item>
    <item>
      <title>每次运行机器学习预测都错误</title>
      <link>https://stackoverflow.com/questions/78636296/wrong-machine-learning-predictions-at-every-run</link>
      <description><![CDATA[我训练了一个机器学习模型，测试集中的几行相同的数据（1800 行中的大约 100 行）在使用不同种子的每次运行（10 次）时都会给出错误的预测。我应该对此做些什么吗？例如将其放入训练集中以将一些数据从训练集换到测试集，还是保持原样？
使用不同的种子运行模型 10 次。有些行始终得到错误的预测。]]></description>
      <guid>https://stackoverflow.com/questions/78636296/wrong-machine-learning-predictions-at-every-run</guid>
      <pubDate>Tue, 18 Jun 2024 08:37:10 GMT</pubDate>
    </item>
    <item>
      <title>根据 HistGratientBoostingClassifier 绘制决策树</title>
      <link>https://stackoverflow.com/questions/78636029/plot-a-decision-tree-from-histgratientboostingclassifier</link>
      <description><![CDATA[我有一个 HistGradientBoostingClassifier 模型，我想绘制一个或多个决策树，但我找不到原生函数来执行此操作，我可以访问 Tree 预测器对象及其节点，但为了将其绘制到 sklearn.tree.plot_tree 函数中，它需要是 DecisionTree 类型的对象
我试过这个：
from sklearn.tree import plot_tree

plot_tree(RF_90._predictors[0][0])

出现此错误：

InvalidParameterError：plot_tree 的 &#39;decision_tree&#39; 参数必须
是 &#39;sklearn.tree._classes.DecisionTreeClassifier&#39; 的实例或
&#39;sklearn.tree._classes.DecisionTreeRegressor&#39;。得到的是
&lt;sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor
对象位于 0x7f676ebf0310&gt;。

注意：RF_90 是 HistGradientBoostingClassifier 拟合模型]]></description>
      <guid>https://stackoverflow.com/questions/78636029/plot-a-decision-tree-from-histgratientboostingclassifier</guid>
      <pubDate>Tue, 18 Jun 2024 07:26:34 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 tfa.losses.TripletSemiHardLoss 训练具有三重损失的暹罗网络？</title>
      <link>https://stackoverflow.com/questions/78635866/how-to-train-a-siamese-network-with-triplet-loss-using-tfa-losses-tripletsemihar</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78635866/how-to-train-a-siamese-network-with-triplet-loss-using-tfa-losses-tripletsemihar</guid>
      <pubDate>Tue, 18 Jun 2024 06:47:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么具有非奇异输出的鉴别器（针对修改后的 WGAN 架构）可能比传统鉴别器表现更好？</title>
      <link>https://stackoverflow.com/questions/78631846/why-might-a-discriminator-for-a-modified-wgan-architecture-with-a-non-singlula</link>
      <description><![CDATA[以下是我提出的（修改后的）WGAN 模型的生成器和鉴别器模型架构，用于将降雨数据下采样 4 倍（仅作为测试）。数据集的输入形状是 (8030, 14, 21)，我想要的输出形状是 (8030, 28, 42)。
def build_generator(input_shape):
inputs = Input(shape=input_shape, name=&#39;generator_input&#39;)

# 下采样
downsample_3 = conv_block(inputs, 128, (3, 3), &#39;downsample_3&#39;)
downsample_2 = conv_block(downsample_3, 64, (3, 3), &#39;downsample_2&#39;)
downsample_1 = conv_block(downsample_2, 32, (3, 3), &#39;downsample_1&#39;)

# 瓶颈
bottleneck_0 = conv_block(downsample_1, 16, (3, 3), &#39;bottleneck_0&#39;)
bottleneck_00 = conv_block(bottleneck_0, 16, (3, 3), &#39;bottleneck_00&#39;)

# 上采样
upsample_1 = deconv_block(bottleneck_00, 32, (3, 3), &#39;upsample_1&#39;)
upsample_2 = deconv_block(upsample_1, 64, (3, 3), &#39;upsample_2&#39;)
upsample_3 = deconv_block(upsample_2, 128, (3, 3), &#39;upsample_3&#39;)

# 最终上采样至所需形状
output = Conv2DTranspose(filters=1, kernel_size=(3, 3), kernel_initializer=he_normal(), padding=&#39;same&#39;, 
strides=(2, 2), activated=&#39;relu&#39;, name=&#39;final_upsample_conv&#39;)(upsample_3)

model =模型（输入=输入，输出=输出，名称=&#39;generator&#39;）

返回模型

def build_discriminator（输入形状）：
输入层 = 输入（形状=输入形状，名称=&#39;discriminator_input&#39;）

x = conv_block（输入层，过滤器=16，内核大小=（3，3），名称=&#39;conv1&#39;）#，使用批处理规范=False）
x = conv_block（x，过滤器=32，内核大小=（3，3），名称=&#39;conv2&#39;）#使用批处理规范=False）
x = conv_block（x，过滤器=128，内核大小=（3，3），名称=&#39;conv3&#39;）#使用批处理规范=False）

x = MaxPooling2D（）（x）
x = Dropout（0.25）（x）
输出层 = Dense（1，激活=&#39;线性&#39;）（x）

模型= Model(inputs=input_layer, output=output_layer, name=&#39;discriminator&#39;)

返回模型

根据文献中的 WGAN 实现，我使用 RMSprop 优化器代替 ADAM，学习率为 5e-5，动量为 0.5。此外，我将鉴别器权重剪裁为 -1e-2 和 1e-2 之间。传统上（据我所知），鉴别器模型输出单个值损失，表示它是否能够区分真实输出和虚假输出。在我的情况下，鉴别器输出的形状为 (None, 14, 21, 1)，它似乎效果更好，但我不明白为什么。有人知道为什么会发生这种情况吗？提前致谢！
编辑：这些是 conv_block 和 deconv_block 的架构
def conv_block(x, filters, kernel_size, name, use_batch_norm=True):
x = Conv2D(filters=filters, kernel_size=kernel_size, padding=&#39;same&#39;, kernel_initializer=he_normal(), name=name+&#39;_conv&#39;)(x)
x = LeakyReLU(alpha=0.2, name=name+&#39;_lrelu&#39;)(x)
if use_batch_norm:
x = BatchNormalization(name=name+&#39;_bn&#39;)(x)
x = Dropout(0.25, name=name+&#39;_dropout&#39;)(x)
return x

def deconv_block(x, filters, kernel_size, name, use_batch_norm=True):
x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, padding=&#39;same&#39;, kernel_initializer=he_normal(), name=name+&#39;_deconv&#39;)(x)
x = LeakyReLU(alpha=0.2, name=name+&#39;_lrelu&#39;)(x)
if use_batch_norm:
x = BatchNormalization(name=name+&#39;_bn&#39;)(x)
return x

编辑 2：以下是损失函数
def generator_loss(fake_output, real_output, penalty_weight=10):
&quot;&quot;&quot;
使用 Wasserstein 损失并添加惩罚项的生成器损失函数。

参数：
fake_output (tf.Tensor)：给定生成的图像时，判别器的输出。
real_output (tf.Tensor)：给定真实图像时，判别器的输出。
penalty_weight (float)：惩罚项的权重。

返回：
tf.Tensor：生成器损失。
“” “”
wasserstein_loss = -tf.reduce_mean(fake_output)

# 偏离实际输出的惩罚项
penalty = penalty_weight * tf.reduce_mean(tf.abs(fake_output - real_output))

return wasserstein_loss + penalty

def discriminator_loss(real_output, fake_output):
“” “”
使用 Wasserstein 损失的判别器损失函数。

参数：
real_output (tf.Tensor)：给定真实图像时判别器的输出。
fake_output (tf.Tensor)：给定生成图像时判别器的输出。

返回：
tf.Tensor：判别器损失。
“” “”
return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)
]]></description>
      <guid>https://stackoverflow.com/questions/78631846/why-might-a-discriminator-for-a-modified-wgan-architecture-with-a-non-singlula</guid>
      <pubDate>Mon, 17 Jun 2024 09:29:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 阅读阿拉伯语 pdf 书</title>
      <link>https://stackoverflow.com/questions/78631415/read-arabic-pdf-book-using-python</link>
      <description><![CDATA[我正在使用 python 阅读一本阿拉伯语书籍（pdf 是可选的，它不需要任何 OCR（光学字符识别从图像中提取文本）），所以我使用了多个库 pdfplumber、pdfminer.six 和 flitz（PyMuPdf））这是我使用的代码之一：
import pdfplumber
from bidi.algorithm import get_display
import arabic_reshaper
import re

def clean_text(text):
# 删除 NULL 字节和控制字符
cleaned_text = re.sub(r&#39;[\x00-\x1F\x7F]&#39;, &#39;&#39;, text)
return cleaned_text

def reshape_and_bidi_text(text):
# 重塑阿拉伯语文本并应用 bidi 算法
reshaped_text = arabic_reshaper.reshape(text)
bidi_text = get_display(reshaped_text)
return bidi_text

def extract_text_from_pdf(pdf_path):
text = &quot;&quot;
使用 pdfplumber.open(pdf_path) 作为 pdf:
对于 pdf.pages 中的 page:
page_text = page.extract_text()
如果 page_text:
text += page_text + &quot;\n&quot;
返回文本

def save_text_to_file(text, output_path):
with open(output_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as text_file:
text_file.write(text)

def convert_pdf_to_text(pdf_path, output_path):
# 使用 pdfplumber 从 PDF 中提取文本
extracted_text = extract_text_from_pdf(pdf_path)

# 清理提取的文本
cleaned_text = clean_text(extracted_text)

# 重塑文本并将 bidi 算法应用于文本
reshaped_bidi_text = reshape_and_bidi_text(cleaned_text)

# 将清理和重塑的文本保存到文本文件
save_text_to_file(reshaped_bidi_text, output_path)
print(f&quot;来自 {pdf_path} 的文本已保存到 {output_path}&quot;)

# 示例用法
pdf_path = r&#39;C:\Users\DELL\Desktop\Book Printed\البوليميرات العالية الأداء.pdf&#39;
text_output_path = r&quot;C:\Users\DELL\Desktop\output.txt&quot;

convert_pdf_to_text(pdf_path, text_output_path)

因此，当使用这些库时，我总是得到以下带有错误编码的输出，我不知道该用什么来修复它？对此有什么建议吗？
提前致谢
注意：附在上面https://www.noor-book.com/%D9%83%D8%AA%D8%A7%D8%A8-%D8%A7%D9%84%D8%A8%D9%88%D9%84%D9%8A%D9%85%D9%8A%D8%B1%D8%A7%D8%AA-%D8%A7%D9%84%D8%B9%D8%A7%D9%84%D9%8A%D8%A9-%D8%A7%D9%84%D8%A3%D8%AF%D8%A7%D8%A1-pdf?next=72c6f38a363b368a7bd978a8449ea530是我尝试阅读的阿拉伯语书]]></description>
      <guid>https://stackoverflow.com/questions/78631415/read-arabic-pdf-book-using-python</guid>
      <pubDate>Mon, 17 Jun 2024 07:46:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 BARTDecoder 和 cached_property 的 Nougat OCR 中的 ImportError 和 TypeError 问题</title>
      <link>https://stackoverflow.com/questions/78594832/importerror-and-typeerror-issues-in-nougat-ocr-with-bartdecoder-and-cached-prope</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78594832/importerror-and-typeerror-issues-in-nougat-ocr-with-bartdecoder-and-cached-prope</guid>
      <pubDate>Sat, 08 Jun 2024 05:43:48 GMT</pubDate>
    </item>
    <item>
      <title>让一个非常简单的 stablebaselines3 示例发挥作用</title>
      <link>https://stackoverflow.com/questions/77766048/getting-a-very-simple-stablebaselines3-example-to-work</link>
      <description><![CDATA[我尝试模拟最简单的硬币翻转游戏，你必须预测它是否会是正面。遗憾的是它无法运行，给我：
使用 cpu 设备
回溯（最近一次调用最后一次）：
文件“/home/user/python/simplegame.py”，第 40 行，在&lt;module&gt;
model.learn(total_timesteps=10000)
文件 &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py&quot;，第 315 行，在 learn 中
return super().learn(
文件 &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py&quot;，第 264 行，在 learn 中
total_timesteps, callback = self._setup_learn(
文件 &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/common/base_class.py&quot;，第 423 行，在 _setup_learn 中
self._last_obs = self.env.reset() # 类型： ignore[assignment]
文件 &quot;/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py&quot;，第 77 行，在 reset
obs，self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
TypeError: CoinFlipEnv.reset() 获得了意外的关键字参数 &#39;seed&#39;

代码如下：
import gymnasium as gym
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

class CoinFlipEnv(gym.Env):
def __init__(self, heads_probability=0.8):
super(CoinFlipEnv, self).__init__()
self.action_space = gym.spaces.Discrete(2) # 0 表示正面，1 表示反面
self.observation_space = gym.spaces.Discrete(2) # 0 表示正面，1 表示反面
self.heads_probability = heads_probability
self.flip_result = None

def reset(self):
# 重置环境
self.flip_result = None
return self._get_observation()

def step(self, action):
# 执行操作（0 表示正面，1 表示反面）
self.flip_result = int(np.random.rand() &lt; self.heads_probability)

# 计算奖励（1 表示正确预测，-1 表示错误）
reward = 1 if self.flip_result == action else -1

# 返回观察、奖励、完成和信息
return self._get_observation(), reward, True, {}

def _get_observation(self):
# 返回当前硬币翻转结果
return self.flip_result

# 创建正面概率为 0.8 的环境
env = DummyVecEnv([lambda: CoinFlipEnv(heads_probability=0.8)])

# 创建 PPO 模型
model = PPO(&quot;MlpPolicy&quot;, env, verbose=1)

# 训练模型
model.learn(total_timesteps=10000)

# 保存模型
model.save(&quot;coin_flip_model&quot;)

# 评估模型
obs = env.reset()
for _ in range(10):
action, _states = model.predict(obs)
obs, rewards, dones, info = env.step(action)
print(f&quot;Action: {action}, Observation: {obs}, Reward: {rewards}&quot;)

我做错了什么？
这是 2.2.1 版本。]]></description>
      <guid>https://stackoverflow.com/questions/77766048/getting-a-very-simple-stablebaselines3-example-to-work</guid>
      <pubDate>Fri, 05 Jan 2024 16:47:55 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 中的平衡准确度分数</title>
      <link>https://stackoverflow.com/questions/59339531/balanced-accuracy-score-in-tensorflow</link>
      <description><![CDATA[我正在为一个高度不平衡的分类问题实现一个 CNN，我想在 TensorFlow 中实现自定义指标以使用“选择最佳模型”回调。
具体来说，我想实现平衡准确度分数，即每个类的召回率的平均值（请参阅 sklearn 实现此处），有人知道怎么做吗？]]></description>
      <guid>https://stackoverflow.com/questions/59339531/balanced-accuracy-score-in-tensorflow</guid>
      <pubDate>Sat, 14 Dec 2019 21:59:49 GMT</pubDate>
    </item>
    </channel>
</rss>