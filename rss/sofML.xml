<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 15 Jun 2024 09:15:06 GMT</lastBuildDate>
    <item>
      <title>删除边界框detectron2之外的所有内容</title>
      <link>https://stackoverflow.com/questions/78626157/delete-everything-outside-of-bounding-boxes-detectron2</link>
      <description><![CDATA[我已经训练了一个具有一个类的 detectron2 模型。现在我想将不在 bbox 内的所有内容设置为白色，其余部分保持原样。一张图片上可以有多个 bbox，它们可以叠加。我阅读了 detectron2 文档 和 Cv2 文档，但找不到可以解决我的问题的内容。这是我的预测代码：
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
import cv2

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(&#39;COCO-Detection/tmp&#39;))
cfg.MODEL.WEIGHTS = &#39;tmp&#39;

predictor = DefaultPredictor(cfg)

img = cv2.imread(&#39;tmp&#39;)

out = predictor(img)

我阅读了检测和 cv2 的文档，但找不到解决问题的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78626157/delete-everything-outside-of-bounding-boxes-detectron2</guid>
      <pubDate>Sat, 15 Jun 2024 09:09:50 GMT</pubDate>
    </item>
    <item>
      <title>Python 中分类列的目标编码设置问题</title>
      <link>https://stackoverflow.com/questions/78625992/target-encoding-setup-issue-for-a-categorial-column-in-python</link>
      <description><![CDATA[我尝试对具有多个类别级别的一列进行目标编码。我首先将数据拆分为训练和测试以避免泄漏，然后尝试进行如下所示的编码：
X_train[&quot;Municipality&quot;] 和 y_train 没有 NA 值。X_train[&quot;Municipality&quot; 的类型是分类的，y_train 是浮点型
但我收到此错误，我不确定问题是什么。
当我尝试进行目标编码时，我认为它会起作用，因为列中没有 na 值，但由于某种原因，出现了与该列相关的错误。]]></description>
      <guid>https://stackoverflow.com/questions/78625992/target-encoding-setup-issue-for-a-categorial-column-in-python</guid>
      <pubDate>Sat, 15 Jun 2024 08:04:28 GMT</pubDate>
    </item>
    <item>
      <title>将模型正确预测的图像传回模型进行训练是否会对我们产生任何有用的结果？</title>
      <link>https://stackoverflow.com/questions/78625950/would-passing-correctly-predicted-images-by-a-model-back-to-the-model-for-traini</link>
      <description><![CDATA[我没有所需的技术知识来尝试测试模型的指标以比较结果和假设，因此我依靠社区的智慧。
我试图寻找答案，但没有找到任何有用的东西（也许我的问题表述不正确）。
任何有关上述主题的指导都将不胜感激。
尝试谷歌搜索，期望找到一篇文章或一些答案，但没有找到任何东西。]]></description>
      <guid>https://stackoverflow.com/questions/78625950/would-passing-correctly-predicted-images-by-a-model-back-to-the-model-for-traini</guid>
      <pubDate>Sat, 15 Jun 2024 07:41:59 GMT</pubDate>
    </item>
    <item>
      <title>在平面图图像中分割墙壁（实心黑色和条纹）</title>
      <link>https://stackoverflow.com/questions/78625835/segmenting-walls-solid-black-striped-in-floorplan-images</link>
      <description><![CDATA[数据集中的示例图像
给定这样的平面图，我想执行家具检测（对象检测）和墙壁分割。
现在，在手动注释数据集中的图像后，家具检测是可行的。我总共有 57 幅图像，很难再获得更多图像。
我将在下面列出我尝试执行的墙壁分割方法。
墙壁分割 - 如果墙壁被分割出来，我可以删除图像中的所有其他项目并应用洪水填充来分割房间。
现在，我遇到了一个堆栈溢出帖子检测/分割填充的矩形 OpenCV。这有一种检测模式的方法，这正是我们在本例中所需要的。墙壁上画有成对的平行线，角度为 45 度。问题是我编写了一个锅炉代码，但不知道参数值，似乎根本无法得到结果。
我还发现了一种无监督方法，可以从平面图中分割出任何类型的墙壁 -&gt; https://refbase.cvc.uab.es/files/HFV2013.pdf。然而，阈值再次没有在任何地方指定，也没有提供指向 github repo 的链接。
有人知道如何解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78625835/segmenting-walls-solid-black-striped-in-floorplan-images</guid>
      <pubDate>Sat, 15 Jun 2024 06:46:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 scikit-learn 在 Python 中对未标记数据实现层次聚类？</title>
      <link>https://stackoverflow.com/questions/78625589/how-to-implement-hierarchical-clustering-in-python-with-scikit-learn-for-unlabel</link>
      <description><![CDATA[我正在学习聚类，在尝试查找带有标记数据的数据库时遇到了一些问题，这对我来说是一个限制，因为我发现了非常有趣的未标记数据集。我读过各种无监督聚类技术，并想实现层次聚类。
我将数据加载到 pandas DataFrame 中，对数据进行标准化并应用层次聚类。然后我可视化了树状图，但我不确定如何解释结果或我是否使用了正确的参数。]]></description>
      <guid>https://stackoverflow.com/questions/78625589/how-to-implement-hierarchical-clustering-in-python-with-scikit-learn-for-unlabel</guid>
      <pubDate>Sat, 15 Jun 2024 04:03:59 GMT</pubDate>
    </item>
    <item>
      <title>了解 Vits 对 HiFi-GAN 的使用</title>
      <link>https://stackoverflow.com/questions/78625475/understanding-usage-of-hifi-gan-by-vits</link>
      <description><![CDATA[我正在（尝试）学习语音合成的 AI/ML，并尝试了解 Vits 如何使用 HiFi-GAN。
据我所知，Vits 会将文本输入转换为梅尔频谱图，然后由 HiFi-GAN 转换为音频波。
让我困惑的是为什么从 Vits 发送到 HiFi-GAN 的输入不是梅尔频谱图。
例如，当我测试其他模型并将以下代码添加到 HiFi-GAN 的 forward 方法时：
class Generator(torch.nn.Module):
...
def forward(self, x):
plot_spectrogram(x[0].cpu().detach().numpy(), &quot;mel_spec.png&quot;)
...
...

它保存了正确的图像，看起来像梅尔频谱图图像，但是，当我对 vits 执行相同操作时，保存的图像是纯绿色图像，当然不是梅尔频谱图的表示。
但生成的音频文件当然是有效的音频文件。
有人能向我解释一下吗？
我正在评估一些神经 tts 模型，我想要做的是保存由模型创建的梅尔频谱图，以便以后进行比较，并通过不同的声码器运行它们以进行比较。
我注意到 vits repo 中的 HiFi-GAN 代码与原始 repo 略有不同，但我不明白为什么。
有什么方法可以转换输入参数 x 转换为梅尔频谱图表示，而无需先将其转换为音频，然后再将音频转换为梅尔？]]></description>
      <guid>https://stackoverflow.com/questions/78625475/understanding-usage-of-hifi-gan-by-vits</guid>
      <pubDate>Sat, 15 Jun 2024 02:17:36 GMT</pubDate>
    </item>
    <item>
      <title>训练期间存在 EarlyStopping 回调的 keras 中的 ReduceLROnPlateau 流程图</title>
      <link>https://stackoverflow.com/questions/78624879/flowchart-of-reducelronplateau-in-keras-during-training-while-there-is-earlystop</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78624879/flowchart-of-reducelronplateau-in-keras-during-training-while-there-is-earlystop</guid>
      <pubDate>Fri, 14 Jun 2024 20:29:11 GMT</pubDate>
    </item>
    <item>
      <title>模型不适用于多类分割</title>
      <link>https://stackoverflow.com/questions/78624691/model-not-working-for-multiclass-segmentation</link>
      <description><![CDATA[我正在训练一个用于多类分割问题的模型。我有 3 个类，图像大小为 512x512 和 1 个通道。我的数据集中的类别不平衡。问题是该模型在多类分割方面表现不佳。
我尝试过交叉熵损失、Dice 损失、Jaccard 损失和损失组合（Jaccard + Focal）。
交叉熵工作正常，但结果并不令人满意。
我应该在其中进行哪些更改？
以下是模型的代码
class AxialDW(nn.Module):
def __init__(self, dim, mixer_kernel, dilation=1):
super().__init__()
h, w = mixer_kernel
self.dw_h = nn.Conv2d(dim, dim, kernel_size=(h, 1), padding=(max(h // 2, dilation), 0), groups=dim, dilation=dilation)
self.dw_w = nn.Conv2d(dim, dim, kernel_size=(1, w), padding=(0, max(w // 2, dilation)), groups=dim, dilation=dilation)

def forward(self, x):
x = x + self.dw_h(x) + self.dw_w(x)
返回 x

class EncoderBlock(nn.Module):
&quot;&quot;&quot;编码然后下采样&quot;&quot;&quot;

def __init__(self, in_c, out_c, mixer_kernel=(7, 7)):
super().__init__()
self.dw = AxialDW(in_c, mixer_kernel=(7, 7))
self.bn = nn.BatchNorm2d(in_c)
self.pw = nn.Conv2d(in_c, out_c, kernel_size=1)
self.down = nn.MaxPool2d((2, 2))
self.act = nn.GELU()

def forward(self, x):
skip = self.bn(self.dw(x))
x = self.act(self.down(self.pw(skip)))
return x, skip

class DecoderBlock(nn.Module):
&quot;&quot;&quot;上采样然后解码&quot;&quot;&quot;

def __init__(self, in_c, out_c, mixer_kernel=(7, 7)):
super().__init__()
self.up = nn.Upsample(scale_factor=2)
self.pw = nn.Conv2d(in_c + out_c, out_c, kernel_size=1)
self.bn = nn.BatchNorm2d(out_c)
self.dw = AxialDW(out_c, mixer_kernel=(7, 7))
self.act = nn.GELU()
self.pw2 = nn.Conv2d(out_c, out_c, kernel_size=1)

def forward(self, x, skip):
x = self.up(x)
x = torch.cat([x, skip], dim=1)
x = self.act(self.pw2(self.dw(self.bn(self.pw(x)))))
return x

class BottleNeckBlock(nn.Module):
&quot;&quot;&quot;轴向扩张 DW 卷积&quot;&quot;&quot;

def __init__(self, dim):
super().__init__()

gc = dim // 4
self.pw1 = nn.Conv2d(dim, gc, kernel_size=1)
self.dw1 = AxialDW(gc, mixer_kernel=(3, 3), dilation=1)
self.dw2 = AxialDW(gc, mixer_kernel=(3, 3), dilation=2)
self.dw3 = AxialDW(gc, mixer_kernel=(3, 3), dilation=3)

self.bn = nn.BatchNorm2d(4 * gc)
self.pw2 = nn.Conv2d(4 * gc, dim, kernel_size=1)
self.act = nn.GELU()

def forward(self, x):
x = self.pw1(x)
x = torch.cat([x, self.dw1(x), self.dw2(x), self.dw3(x)], 1)
x = self.act(self.pw2(self.bn(x)))
return x

class ULite(nn.Module):
def __init__(self, freeze_model, num_classes=3):
super().__init__()

&quot;&quot;&quot;Encoder&quot;&quot;&quot;
self.conv_in = nn.Conv2d(1, 16, kernel_size=7, padding=3)
self.e1 = EncoderBlock(16, 32)
self.e2 = EncoderBlock(32, 64)
self.e3 = EncoderBlock(64, 128)
self.e4 = EncoderBlock(128, 256)
self.e5 = EncoderBlock(256, 512)

“瓶颈”
self.b5 = BottleNeckBlock(512)

“解码器”
self.d5 = DecoderBlock(512, 256)
self.d4 = DecoderBlock(256, 128)
self.d3 = DecoderBlock(128, 64)
self.d2 = DecoderBlock(64, 32)
self.d1 = DecoderBlock(32, 16)
self.conv_out = nn.Conv2d(16, num_classes, kernel_size=1)

if freeze_model:
self.freeze_model()

def forward(self, x):
&quot;&quot;&quot;编码器&quot;&quot;&quot;
x = self.conv_in(x)
x, skip1 = self.e1(x)
x, skip2 = self.e2(x)
x, skip3 = self.e3(x)
x, skip4 = self.e4(x)
x, skip5 = self.e5(x)

“瓶颈” “” “”
x = self.b5(x) # (512, 8, 8)

“解码器” “” “”
x = self.d5(x, skip5)
x = self.d4(x, skip4)
x = self.d3(x, skip3)
x = self.d2(x, skip2)
x = self.d1(x, skip1)
x = self.conv_out(x)

# 应用 softmax 进行多类分类
x = F.softmax(x, dim=1)
return x

def freeze_model(self):
for name, param in self.named_pa​​rameters():
param.requires_grad = False

Jaccard + Focal Loss
Loss Image
Jaccard Loss
丢失图片]]></description>
      <guid>https://stackoverflow.com/questions/78624691/model-not-working-for-multiclass-segmentation</guid>
      <pubDate>Fri, 14 Jun 2024 19:30:25 GMT</pubDate>
    </item>
    <item>
      <title>“利用计算机视觉技术创建高效的实时人脸检测系统” [关闭]</title>
      <link>https://stackoverflow.com/questions/78624565/creating-an-efficient-real-time-face-detection-system-using-computer-vision-tec</link>
      <description><![CDATA[“如何使用现代计算机视觉技术和框架，从实时视频中创建实时人脸检测系统，确保准确性和效率？”
我期待一个结构良好的答案来消除我的疑虑，我需要任何链接供我参考。]]></description>
      <guid>https://stackoverflow.com/questions/78624565/creating-an-efficient-real-time-face-detection-system-using-computer-vision-tec</guid>
      <pubDate>Fri, 14 Jun 2024 18:51:33 GMT</pubDate>
    </item>
    <item>
      <title>ML.NET 中的 Essentia 模型无法预测</title>
      <link>https://stackoverflow.com/questions/78622030/essentia-models-in-ml-net-fail-to-predict</link>
      <description><![CDATA[我正在尝试使用 Essentia discogs_track_embeddings-effnet-bs64 模型和 ML.NET 进行预测。我尝试过使用 tensorflow 和 onnx，但当我尝试预测任何东西时，我都遇到了问题
抛出异常：Microsoft.ML.Data.dll 中的“System.InvalidOperationException”
Microsoft.ML.Data.dll 中发生了未处理的“System.InvalidOperationException”类型的异常
Splitter/consolidator 工作程序在使用源数据时遇到异常

目前，我正在使用 onnx，因此其余部分将是该尝试的堆栈跟踪和代码。
完整调用堆栈：
 在 Microsoft.ML.Data.DataViewUtils.Splitter.Batch.SetAll(OutPipe[] pipes)
在 Microsoft.ML.Data.DataViewUtils.Splitter.Cursor.MoveNextCore()
在 Microsoft.ML.Data.RootCursorBase.MoveNext()
在Microsoft.ML.Data.ColumnCursorExtensions.&lt;GetColumnArrayDirect&gt;d__4`1.MoveNext()
在 System.Collections.Generic.List`1..ctor(IEnumerable`1 collection)
在 System.Linq.Enumerable.ToList[TSource](IEnumerable`1 source)
在 Program.&lt;Main&gt;$(String[] args) 中的 Program.cs:line 122

在行上：var embeddingColumn = perceivedData.GetColumn&lt;float[]&gt;(&quot;embeddings&quot;).ToList();
onnx 加载和预测代码：
Console.WriteLine($&quot;[+] Loading Model&quot;);
var mlContext = new MLContext();

// 将 melspectrogram 数据加载到管道中
var modelPath = &quot;discogs_track_embeddings-effnet-bs64-1.onnx&quot;;
var pipeline = mlContext.Transforms.ApplyOnnxModel(
modelFile: modelPath,
fallbackToCpu: true
);
IDataView mockData = mlContext.Data.LoadFromEnumerable(new List&lt;ModelInput&gt;() { new ModelInput() });
var model = pipeline.Fit(mockData);

var schema = model.Transform(mockData).Schema;
Console.WriteLine(&quot;[*] Model Schema:&quot;);
foreach (var column in schema)
{
Console.WriteLine($&quot;Column Name: {column.Name}, Column Type: {column.Type}&quot;);
}

List&lt;ModelOutput&gt; allPredictions = new List&lt;ModelOutput&gt;();

foreach (var fragment in melSpectrogram)
{
var seg = MelSpectrogramGenerator.ConvertToFloat(segment);

var data = new ModelInput
{
Melspectrogram = seg
};
IDataView dataView = mlContext.Data.LoadFromEnumerable(new [] { data });
var formedData = model.Transform(dataView);

// 检索嵌入
var embeddingColumn = formedData.GetColumn&lt;float[]&gt;(&quot;embeddings&quot;).ToList();
foreach (var value in embeddingColumn.First())
{
Console.Write($&quot;{value} &quot;);
}
//allPredictions.Add(scoredData.);
Console.WriteLine(&quot;Wheee&quot;);
}

public class ModelInput
{
[VectorType(64, 128, 96)]
[ColumnName(&quot;melspectrogram&quot;)]
public float[,,] Melspectrogram { get; set; }
public ModelInput()
{
Melspectrogram = new float[64, 128, 96];
}
}

// 定义输出模式
public class ModelOutput
{
[VectorType(64, 512)]
[ColumnName(&quot;embeddings&quot;)]
public float[,] Embeddings { get; set; }
public ModelOutput()
{
Embeddings = new float[64, 512];
}
}

目前在 Microsoft.ML 3.0.1、Microsoft.ML.OnnxRuntime.Managed 1.18.0 上
我已检查，我的数据中没有 NaN，并且我的变量都不是 Null。我非常迷茫，不确定如何修复此问题，甚至不知道如何继续进行故障排除。]]></description>
      <guid>https://stackoverflow.com/questions/78622030/essentia-models-in-ml-net-fail-to-predict</guid>
      <pubDate>Fri, 14 Jun 2024 09:11:02 GMT</pubDate>
    </item>
    <item>
      <title>在不同的 Python 环境中训练的相同 XGBClassifier 模型得出的预测结果明显不同</title>
      <link>https://stackoverflow.com/questions/78619966/the-same-xgbclassifier-model-trained-in-different-python-environment-made-notice</link>
      <description><![CDATA[我尝试将在旧的 Python 环境中训练的 XGBClassifier 模型转移到新的环境中。
以下是新旧环境中关键软件包的版本信息。
旧环境

python=3.6.0
scikit-learn==0.22.2.post1
xgboost==0.90
pickleshare==0.7.5
numpy==1.18.1

新环境

python=3.11.9
scikit-learn==1.4.2
xgboost==2.0.3
pickleshare==0.7.5
numpy==1.26.4

在新旧环境中分别使用同一组超参数和相同的数据，预测的概率明显不同。
我还注意到，拟合管道对象的大小以及训练模型所需的时间发生了显着变化。
拟合管道对象的大小旧版与新版：30 MB vs. 7 MB
训练时间旧版与新版：4:38:46 vs. 0:06:40
对于我在旧环境中训练的模型和新环境中训练的模型之间的差异，您有什么看法？
以下是我用来训练模型的关键python代码。
def create_pipeline(model_params, cat_indices):
&quot;&quot;&quot;
创建管道
:param model_params：管道中 XGBoost 分类器的模型参数
:param cat_indices：X 中分类特征的索引
&quot;&quot;&quot;

cat_transformer = Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;missing&#39;)),
(&#39;one_hot_encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))])

preprocessor = ColumnTransformer(
transformers=[(&#39;cat&#39;, cat_transformer, cat_indices)],
remainder=&#39;passthrough&#39;)

xgb = XGBClassifier(objective=&quot;binary:logistic&quot;, eval_metric=&quot;auc&quot;, missing=np.nan, use_label_encoder=False)
xgb.set_params(**model_params)

full_pipeline_model = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor),
(&#39;model&#39;, xgb)])
return full_pipeline_model

model_params = {
&#39;n_estimators&#39;: 500,
&#39;alpha&#39;: 9.73974803929248e-06,
&#39;gamma&#39;: 19,
&#39;lambda&#39;: 0.557185777864069,
&#39;learning_rate&#39;: 0.029438952461179668,
&#39;max_depth&#39;: 13,
&#39;scale_pos_weight&#39;: 5,
&#39;subsample&#39;: 0.687206238714661
}

cat_indices = [X.columns.get_loc(col) for col in cat_cols]

fitted_pipeline = create_pipeline(model_params, cat_indices).fit(X.values, y.values)
pickle.dump(fitted_pipeline, open(&quot;fitted_pipeline_final1.pkl&quot;, &quot;wb&quot;))

我预计从两个模型获得的预测概率非常相似，因为我使用了相同的超参数集和相同的数据。预测概率明显不同的原因可能是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78619966/the-same-xgbclassifier-model-trained-in-different-python-environment-made-notice</guid>
      <pubDate>Thu, 13 Jun 2024 20:07:18 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 detector2 区分灰度图像中的两种颜色并掩盖它们？</title>
      <link>https://stackoverflow.com/questions/78619402/how-would-you-use-detectron2-to-distinguish-between-two-colors-in-a-grayscale-im</link>
      <description><![CDATA[我的项目包括使用detectron2模型来区分灰度图像中的物体。该图像由形状奇怪的灰色细胞组成，而其余空间为黑色。我的任务是使用该模型并描绘出灰色细胞的形状。
示例图像：
细胞的灰度图像
我曾尝试使用预先存在的模型来解决这个问题，但它们无法识别物体的存在。解决这个问题的最佳方法是什么？
此外，我愿意使用不同的机器学习模型。我只是想找到一种区分灰色和黑色的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78619402/how-would-you-use-detectron2-to-distinguish-between-two-colors-in-a-grayscale-im</guid>
      <pubDate>Thu, 13 Jun 2024 17:36:00 GMT</pubDate>
    </item>
    <item>
      <title>如何使用多个 AMD GPU 运行 Ollama [关闭]</title>
      <link>https://stackoverflow.com/questions/78618964/how-to-run-ollama-with-multiple-amd-gpus</link>
      <description><![CDATA[我尝试在配备多个 AMD GPU 的系统上运行 Ollama，但在正确使用所有 GPU 时遇到了困难。
设置详细信息：

操作系统：RedHat
GPU：MI210 x 4
Ollama 版本：0.1.42
ROCm

面临的问题：

似乎只使用了一个 GPU，或者没有明显的性能改进。
[watch -n 0.1 /opt/rocm/bin/rocm-smi]
0 2 0x740f，30145 63.0°C 253.0W N/A，N/A，0 1700Mhz 1600Mhz 0% 自动 300.0W 60% 100%
1 3 0x740f，41677 28.0°C 41.0W N/A，N/A，0 800Mhz 1600Mhz 0% 自动 300.0W 0% 0%
2 4 0x740f，39309 31.0°C 40.0W N/A，N/A，0 800Mhz 1600Mhz 0% 自动 300.0W 0% 0%
3 5 0x740f, 50825 35.0°C 40.0W N/A, N/A, 0 800Mhz 1600Mhz 0% 自动 300.0W 0% 0%

问题：

我是否缺少在 Ollama 中启用多 GPU 支持的特定步骤？
在 Ollama 中，多 GPU 设置是否需要任何其他配置设置？

是否有任何指导或详细步骤可以正确设置和验证 Ollama 是否使用多个 AMD GPU？
我尝试过 /set 参数 num_gpu 12，但没有成功]]></description>
      <guid>https://stackoverflow.com/questions/78618964/how-to-run-ollama-with-multiple-amd-gpus</guid>
      <pubDate>Thu, 13 Jun 2024 15:56:57 GMT</pubDate>
    </item>
    <item>
      <title>尝试将 Kaggle 笔记本提交到 GitHub 存储库时出现错误</title>
      <link>https://stackoverflow.com/questions/78618684/getting-error-while-trying-to-commit-a-kaggle-notebook-to-a-github-repository</link>
      <description><![CDATA[提交内核时发生错误：ConcurrencyViolation 序列号必须匹配草稿记录：KernelId=59714315，ExpectedSequence=43，
ActualSequence=42，AuthorUserId=16388128

这是什么意思？
当我尝试将笔记本从 kaggle 提交到 github 时出现此错误。我该如何解决这个问题？
我原本以为它会直接提交到 github 而不会遇到任何问题]]></description>
      <guid>https://stackoverflow.com/questions/78618684/getting-error-while-trying-to-commit-a-kaggle-notebook-to-a-github-repository</guid>
      <pubDate>Thu, 13 Jun 2024 15:06:29 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 将特征重要性作为列表而不是绘图</title>
      <link>https://stackoverflow.com/questions/63060367/xgboost-get-feature-importance-as-a-list-of-columns-instead-of-plot</link>
      <description><![CDATA[我想知道是否可以将特征重要性作为列表而不是图表来获取。这就是我所拥有的
xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)
import matplotlib.pyplot as plt

xgb.plot_importance(xg_reg)
plt.rcParams[&#39;figure.figsize&#39;] = [5,5]
plt.show()

这给了我这个图

我想只获取主要特征的列表，因为我有超过 800 个不同的特征。]]></description>
      <guid>https://stackoverflow.com/questions/63060367/xgboost-get-feature-importance-as-a-list-of-columns-instead-of-plot</guid>
      <pubDate>Thu, 23 Jul 2020 17:57:31 GMT</pubDate>
    </item>
    </channel>
</rss>