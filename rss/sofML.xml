<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 08 Sep 2024 12:28:44 GMT</lastBuildDate>
    <item>
      <title>如何将 tensorflow 权重文件夹（包含.data、.index 和另一个未知文件名检查点）转换为.pb 格式和.tflite？</title>
      <link>https://stackoverflow.com/questions/78962022/how-to-convert-a-tensorflow-weights-folder-containing-data-index-and-another</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78962022/how-to-convert-a-tensorflow-weights-folder-containing-data-index-and-another</guid>
      <pubDate>Sun, 08 Sep 2024 09:39:58 GMT</pubDate>
    </item>
    <item>
      <title>如何根据多项评估对推荐路线与实际路线的相似度进行分类？[关闭]</title>
      <link>https://stackoverflow.com/questions/78962013/how-to-classify-the-similarity-between-a-recommended-and-an-actual-route-based-o</link>
      <description><![CDATA[我需要对应用程序推荐的路线和司机所走的路线是否相似进行分类。
我有一个包含以下变量的数据集：

id_viaje（行程 ID）
evaluador（评估者）——有 8 个不同的评估者
evaluacion（评估）——评估者的判断：相似、不相似或不确定
ruta_real（实际路线的点集）
ruta_estimada（估计路线的点集）

我已经做了一些探索性数据分析，以下是一些观察结果：

同一个评估者有时会对同一次行程进行两次评估，并给出相互矛盾的结果（一次说路线相似，另一次说不相似）。
在某些情况下在某些情况下，他们说他们不确定，但当我可视化路线时，它们实际上是相同的。
对于同一次旅行，存在不同的评估者意见相矛盾的情况。

我对如何处理这个问题有些疑问：
对于评估者说“不确定”的情况，我计划为“相似”和“不相似”分配 0.5 的权重。（或者，我可以忽略这些，或者先建立一个初始模型对这些情况进行分类，然后使用包含所有数据的最终模型。）
由于任务似乎涉及对实际路线和拟议路线之间的相似性进行分类，而不是建立传统模型，因此似乎基于距离进行分类是最好的方法。
关于如何处理这些情况或计算路线相似度有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78962013/how-to-classify-the-similarity-between-a-recommended-and-an-actual-route-based-o</guid>
      <pubDate>Sun, 08 Sep 2024 09:35:49 GMT</pubDate>
    </item>
    <item>
      <title>什么是局部梯度错位？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78961861/what-is-local-gradient-misalignment</link>
      <description><![CDATA[我在阅读FedPRoto相关论文时，文章中提到了这个概念，但是我却无法理解它的具体定义。
但是我似乎无法从如此少的信息中找到明确的答案。]]></description>
      <guid>https://stackoverflow.com/questions/78961861/what-is-local-gradient-misalignment</guid>
      <pubDate>Sun, 08 Sep 2024 07:57:37 GMT</pubDate>
    </item>
    <item>
      <title>Python-NEAT 算法的配置</title>
      <link>https://stackoverflow.com/questions/78961771/configuration-of-python-neat-algorithm</link>
      <description><![CDATA[我正在使用 NEAT 算法根据十字路口四个方向的队列长度等输入来优化交通信号时序。目标是生成最佳信号时序作为输出。但是，我在实现中遇到了激活函数的持续问题。
我使用 Python-NEAT 进行交通信号优化，其中：
输入：交叉路口四个方向的队列长度。
输出：每个方向的信号时序。

以下是我在使用不同激活函数时遇到的问题的描述：
ReLU：不断将输出值增加到非常大的数字，然后变得无法运行。
Sigmoid：虽然它给出的值从 0 到 1，但我通过将其乘以 10 来放大它，但问题是它对所有输出都卡在 1，无论输入如何，它都会不断给出 [1.0, 1.0, 1.0, 1.0]值。
tanh：与 sigmoid 有同样的问题。
sin：与 sigmoid 有同样的问题。
Softplus：对所有信号时序输出一个常数值 12。
我最初认为运行该算法多代可能会解决这个问题。所以，我让它用 Sigmoid 之类的函数运行了一整夜，结果醒来后发现适应度值已经暴跌到负数十亿。
我正在按如下方式评估我的适应度值：
ifqueue_length_north_avg &gt; 15:
fitness_N += 队列长度_北_avg + ((队列长度_北_avg - 15) ** 2)
else:
fitness_N += 队列长度_北_avg

然后将所有方向的适应度相加并从基因组中减去，如下所示：
 fitness = fitness_N + fitness_S + fitness_E + fitness_W / 4
基因组.fitness -= fitness

我的问题是，是什么原因导致我的 NEAT 实现中的激活函数出现这些问题，我如何配置算法以产生更稳定的输出，从而实现信号时序的有意义的优化？
是否可以为所有四个方向输入不同的适应度？这会有帮助吗？
我的配置文件如下：
*
[NEAT]
fitness_criterion = max
fitness_threshold = 1
pop_size = 30
reset_on_extinction = False

[DefaultGenome]
#compatibility_weight_coefficient = 1.0
activation_default = softplus
activation_mutate_rate = 0.0
activation_options = softplus

aggregation_default = sum
aggregation_mutate_rate = 0.0
aggregation_options = sum

bias_init_mean = 0.0
bias_init_stdev = 1.0
bias_max_value = 10.0
bias_min_value = 0.0
bias_mutate_power = 0.5
bias_mutate_rate = 0.4
bias_replace_rate = 0.1

compatibility_disjoint_coefficient = 1.0
compatibility_weight_coefficient = 1.0

conn_add_prob = 0.5
conn_delete_prob = 0.5

enabled_default = True
enabled_mutate_rate = 0.01

feed_forward = True
initial_connection = full_direct

node_add_prob = 0.2
node_delete_prob = 0.2

num_hidden = 4
num_inputs = 4
num_outputs = 4

response_init_mean = 1.0
response_init_stdev = 0.1
response_max_value = 10.0
response_min_value = 0.0
response_mutate_power = 0.1
response_mutate_rate = 0.1
response_replace_rate = 0.05

weight_init_mean = 0.0
weight_init_stdev = 0.5
weight_max_value = 10
weight_min_value = 0.0
weight_mutate_power = 0.5
weight_mutate_rate = 0.4
weight_replace_rate = 0.1

[DefaultSpeciesSet]
compatibility_threshold = 3.0

[DefaultStagnation]
species_fitness_func = max
max_stagnation = 20
species_elitism = 2

[DefaultReproduction]
elitism = 2
survival_threshold = 0.3*
]]></description>
      <guid>https://stackoverflow.com/questions/78961771/configuration-of-python-neat-algorithm</guid>
      <pubDate>Sun, 08 Sep 2024 06:48:43 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 2.1 Keras 错误，度量尚未建立</title>
      <link>https://stackoverflow.com/questions/78961753/tensorflow-2-1-keras-error-the-metric-has-not-yet-been-built</link>
      <description><![CDATA[我正在堆叠 5 个预训练模型，有点像迁移学习。
我遇到了有关度量尚未构建错误的问题，我不知道如何解决它。
我正在使用 tensorflow 2.17 版本。我检查了所有模型的构建状态，并且度量是正确的。我还确保所有模型输入和输出形状都兼容。
以下是我的代码和错误消息。
来自 sklearn.datasets 导入 make_blobs
来自 sklearn.metrics 导入 accuracy_score
导入 tensorflow 作为 tf
来自 tensorflow.keras.utils 导入 plot_model
来自 tensorflow.keras.models 导入 Model, load_model
来自 tensorflow.keras.layers 导入 Input, Dense, Concatenate
来自 tensorflow.keras.utils 导入 to_categorical
来自 numpy 导入 argmax
导入 IPython

print(tf.__version__)

def define_stacked_model():
ensemble_visible = []
ensemble_outputs = []
for i in range(5):
filename = &#39;models/model_&#39; + str(i + 1) + &#39;.keras&#39;
model = load_model(filename)
for layer in model.layers:
layer.trainable = False
input = 输入(shape=(model.input_shape[-1],), name=f&#39;ensemble_input_{i+1}&#39;)
ensemble_visible.append(input)
output = model(input)
ensemble_outputs.append(output)

merge = 连接(axis=-1, name=&#39;merge_layer&#39;)(ensemble_outputs)
hidden = 密集(10, 激活=&#39;relu&#39;, name=&#39;hidden_​​layer&#39;)(合并)
output = 密集(3, 激活=&#39;softmax&#39;, name=&#39;output_layer&#39;)(隐藏)

final_model = 模型(输入=ensemble_visible, 输出=输出, name=&#39;stacked_model&#39;)
final_model.build(输入形状=ensemble_visible)
final_model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;categorical_accuracy&#39;])
plot_model(final_model, to_file=&#39;stacked_model_plot.png&#39;, show_shapes=True, show_layer_names=True, expand_nested=True)
IPython.display.display(IPython.display.Image(&quot;stacked_model_plot.png&quot;))
return final_model

stacked_model = define_stacked_model()

def fit_stacked_model(model, inputX, inputy):
X = [inputX for _ in range(len(model.input_shape))]
inputy_enc = to_categorical(inputy)
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;categorical_accuracy&#39;])
model.fit(X, inputy_enc, epochs=300, verbose=0)

def predict_stacked_model(model, inputX):
X = [inputX for _ in range(len(model.input_shape))]
return model.predict(X, verbose=0)

X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)
n_train = 100
trainX, testX = X[:n_train, :], X[n_train:, :]
trainy, testy = y[:n_train], y[n_train:]

fit_stacked_model(stacked_model, trainX, trainy)

------------------------------------------------------------------------------------------
ValueError Traceback （最近一次调用最后一次）
单元格 In[37]，第 55 行
52 trainX, testX = X[:n_train, :], X[n_train:, :]
53 trainy, testy = y[:n_train], y[n_train:]
---&gt; 55 fit_stacked_model(stacked_model, trainX, trainy)
56 yhat = predict_stacked_model(stacked_model, testX)
57 yhat = argmax(yhat, axis=1)

单元格 In[37]，第 44 行，在 fit_stacked_model(model, inputX, inputy) 中
42 inputy_enc = to_categorical(inputy)
43 model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;categorical_accuracy&#39;])
---&gt; 44 model.fit(X, inputy_enc, epochs=300, verbose=0)

文件 ~\Anaconda\envs\pytorch-gpu\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 ~\Anaconda\envs\pytorch-gpu\Lib\site-packages\keras\src\trainers\compile_utils.py:356，位于 CompileMetrics.result(self)
354 def result(self):
355 if not self.built:
--&gt; 356 引发 ValueError(
357 &quot;无法获取 result()，因为尚未构建度量。&quot;
358 )
359 results = {}
360 unique_name_counters = {}

ValueError: 无法获取 result()，因为尚未构建度量。

我尝试使其从旧版本 keras 运行到 tensorflow 2.1]]></description>
      <guid>https://stackoverflow.com/questions/78961753/tensorflow-2-1-keras-error-the-metric-has-not-yet-been-built</guid>
      <pubDate>Sun, 08 Sep 2024 06:36:05 GMT</pubDate>
    </item>
    <item>
      <title>我是否正确地实现了带有反向传播的感知器？</title>
      <link>https://stackoverflow.com/questions/78961133/am-i-implementing-my-perceptron-with-backpropagation-correctly</link>
      <description><![CDATA[我在课堂上学习了感知器以及如何使用反向传播来训练模型。我目前在实施过程中遇到了麻烦，因为它仅能为我提供 50% 的准确率，而班上大多数同学的准确率都达到 90%。我在实施过程中是否忽略了什么？这是我目前从我查看过的资料中得到的结果。
class Perceptron():
def __init__(self, num_features):
self.num_features = num_features
self.weights = np.random.rand(num_features) * 0.1 # 这将创建一个用零填充的数组，形状为 num_features
self.bias = 0.0

def forward(self, x):
linear = np.dot(x, self.weights) + self.bias
print(linear)
predictions = np.where(linear &gt; 0, 1, 0)
return predictions

def behind(self, x, y, predictions):
errors = y - predictions
self.weights += self.learning_rate * np.dot(x.T, errors)
self.bias += self.learning_rate * np.sum(errors)
return errors

def train(self, x, y, epochs, learning_rate = 0.01):
self.learning_rate = learning_rate
for e in range(epochs):
for i in range(y.shape[0]):
x_i, y_i = x[i], y[i]
prediction = self.forward(x_i)
self.backward(x_i, y_i, prediction)

def assess(self, x, y):
predictions = self.forward(x)
accuracy = np.mean(predictions == y)
return accuracy

到目前为止，我已经尝试了不同的学习率，并询问了班上的其他人，说实话，这并没有真正改变我的实施结果。我期望准确率约为 90%，但实际只有 50%。
编辑：以下是一些示例数据：
 0.77 -1.14 0
-0.33 1.44 0
0.91 -3.07 0
-0.37 -1.91 0
-1.84 -1.13 0
-1.50 0.34 0
-0.63 -1.53​​ 0
-1.08 -1.23 0
0.39 -1.99 0
-1.26 -2.90 0
-5.27 -0.78 0
-0.49 -2.74 0
1.48 -3.74 0
-1.64 -1.96 0
0.45 0.36    0 -1.48 -1.17 0 -2.94 -4.47 0 -2.19 -1.48 0 0.02 -0.02 0 -2.24 -2.12 0 -3.17 -3.69 0 -4.09 1.03 0 -2.41 -2.31 0 -3.45 -0.61 0 -3.96 -2.00 0 - 2.95 -1.16 0 -2.42 -3.35 0 -1.74 -1.10 0 -1.61 -1.28 0 -2.59 -2.21 0 -2.64 -2.20 0 -2.84 -4.12 0 -1.45 -2.26 0 -3.98   -1.05 0 -2.97 -1.63 0 -0.68 -1.52 0 -0.10 -3.43 0 -1.14 -2.66 0 -2.92 -2.51 0 -2.14 -1.62 0 -3.33 -0.44 0 -1.05 -3.85 0 0.38 0.95 0 -0.05 -1。 95 0 -3.20 -0.22 0 -2.26 0.01 0 -1.41 -0.33 0 -1.20 -0.71 0 -1.69 0.80 0 -1.52 -1.14 0 3.88 0.65 1 0.73 2.97 1
    0.83 3.94 1 1.59 1.25 1 3.92 3.48 1 3.87 2.91 1 1.14 3.91 1 1.73 2.80 1 2.95 1.84 1 2.61 2.92 1 2.38 0.90 1 2.30 3.33 1 31 1.85 1 1.56 3.85 1 2.67 2.41 1 1.23 2.54 1 1.33 2.03 1 1.36 2.68 1 2.58 1.79 1 2.40 0.91 1 0.51 2.44    1 2.17 2.64 1 4.38 2.94 1 1.09 3.12 1 0.68 1.54 1 1.93 3.71 1 1.26 1.17 1 1.90 1.34 1 3.13 0.92 1 0.85 1.56 1 1.50 3.93 1 2.95 2.09 1 0.77 2.84 1 1.00 0.46 1 3.19 2.32 1 2.92 2.32 1 2.86 1.35 1 0.97 2.68 1 1.20 1.31 1 1.54 2.02 1
1.65 0.63 1
1.36 -0.22 1
2.63 0.40 1
0.90 2.05 1
1.26 3.54 1
0.71 2.27 1
1.96 0.83 1
2.52 1.83 1
2.77 2.82 1
4.16 3.34 1

在使用感知器模型之前，此代码首先进行随机化，然后分成2部分：原始数据的2/3用于训练，另外1/3用于测试。之后，对训练和测试数据集的前 2 个特征执行 z 分数标准化。
这是我使用该类的方式：
perceptron = Perceptron(num_features = 2)
perceptron.train(combined_x_train[:, :2], combined_x_train[:, 2], epochs = 5, learning_rate=0.1)
accuracy = perceptron.evaluate(x_train, y_train)
print(f&#39;Final Accuracy: {accuracy * 100:.2f}%&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78961133/am-i-implementing-my-perceptron-with-backpropagation-correctly</guid>
      <pubDate>Sat, 07 Sep 2024 21:05:33 GMT</pubDate>
    </item>
    <item>
      <title>安装 Snap ML 时出错 | 未找到与 snapml 匹配的发行版 [关闭]</title>
      <link>https://stackoverflow.com/questions/78960743/error-on-installing-snap-ml-no-matching-distribution-found-for-snapml</link>
      <description><![CDATA[danilovpavel@Pauls-MacBook-Pro snapml-examples % pip install snapml
错误：找不到满足要求 snapml 的版本（来自版本：无）
错误：未找到与 snapml 匹配的发行版]]></description>
      <guid>https://stackoverflow.com/questions/78960743/error-on-installing-snap-ml-no-matching-distribution-found-for-snapml</guid>
      <pubDate>Sat, 07 Sep 2024 17:39:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 imageDataGenerator 对象执行 Keras model.fit 时出错[关闭]</title>
      <link>https://stackoverflow.com/questions/78960164/error-while-executing-keras-model-fit-with-imagedatagenerator-object</link>
      <description><![CDATA[在使用 Cats &amp; 时执行以下代码行时出现错误狗数据集。
EPOCHS = 100
history = model.fit(
train_data_gen,
steps_per_epoch=int(np.ceil(2000 / float(BATCH_SIZE))),
epochs=EPOCHS,
validation_data=val_data_gen,
validation_steps=int(np.ceil(1000 / float(BATCH_SIZE)))
)

错误
Epoch 1/100
/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: 您的 `PyDataset` 类应在其构造函数中调用 `super().__init__(**kwargs)`。 `**kwargs` 可以包括 `workers`、`use_multiprocessing`、`max_queue_size`。不要将这些参数传递给 `fit()`，因为它们将被忽略。
self._warn_if_super_not_called()
20/20 ━━━━━━━━━━━━━━━━━━━━━━━ 17s 311ms/step - 准确度：0.4858 - 损失：0.7865 - val_accuracy：0.5000 - val_loss：0.6925
Epoch 2/100
/usr/lib/python3.10/contextlib.py:153：UserWarning：您的输入数据不足；中断训练。确保您的数据集或生成器至少可以生成 `steps_per_epoch * epochs` 批次。构建数据集时，您可能需要使用 `.repeat()` 函数。
self.gen.throw(typ, value, traceback)
---------------------------------------------------------------------------
AttributeError Traceback (most recent call last)
&lt;ipython-input-22-f495897bdf8d&gt; in &lt;cell line: 2&gt;()
1 EPOCHS = 100
----&gt; 2 history = model.fit(
3 train_data_gen,
4 steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))),
5 epochs=EPOCHS,

1 帧
/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)
352 )
353 val_logs = {
--&gt; 354 &quot;val_&quot; + name: val for name, val in val_logs.items()
355 }
356 epoch_logs.update(val_logs)

AttributeError: &#39;NoneType&#39; 对象没有属性 &#39;items&#39;

错误提到输入用尽数据，但步骤大小对于训练和验证数据集都是正确的。
我找到了一些使用 fit_generator 的示例，但该方法在 tensorflow 2.1.0 中已弃用。
有什么解决该问题的建议吗？
使用以下代码解决问题
EPOCHS = 100
history = model.fit(
train_data_gen,
batch_size = BATCH_SIZE,
epochs=EPOCHS,
validation_data=val_data_gen
)
]]></description>
      <guid>https://stackoverflow.com/questions/78960164/error-while-executing-keras-model-fit-with-imagedatagenerator-object</guid>
      <pubDate>Sat, 07 Sep 2024 12:52:02 GMT</pubDate>
    </item>
    <item>
      <title>高效的 PyTorch 带矩阵到密集矩阵乘法</title>
      <link>https://stackoverflow.com/questions/78959447/efficient-pytorch-band-matrix-to-dense-matrix-multiplication</link>
      <description><![CDATA[问题：在我的某个程序中，我需要计算矩阵乘法 A @ B，其中两个矩阵的大小均为 N x N，但 N 相当大。我推测使用 band_matrix(A, width) @ B 来近似该乘积即可满足需求，其中 band_matrix(A, width) 表示 A 的带状矩阵部分，宽度为 width。例如，width = 0 给出对角矩阵，对角线元素取自 A，而 width = 1 给出以类似方式获取的三对角矩阵。
我的尝试：我尝试提取三对角矩阵，例如，以以下方式：
# 步骤 1：提取主对角线
main_diag = torch.diagonal(A, dim1=-2, dim2=-1) # 形状：[d1, d2, N]

# 步骤 2：提取上对角线（偏移量=1）
upper_diag = torch.diagonal(A, offset=1, dim1=-2, dim2=-1) # 形状：[d1, d2, N-1]

# 步骤 3：提取下对角线(offset=-1)
lower_diag = torch.diagonal(A, offset=-1, dim1=-2, dim2=-1) # 形状：[d1, d2, N-1]

# 步骤 4：重建三对角矩阵
# 主对角线
tridiag = torch.diag_embed(main_diag) # 形状：[d1, d2, N, N]

# 上对角线（移动值以创建第一个上对角线）
tridiag += torch.diag_embed(upper_diag, offset=1)

# 下对角线（移动值以创建第一个下对角线）
tridiag += torch.diag_embed(lower_diag, offset=-1)

但我不确定 tridiag @ B 是否比原始 A 更有效率@ B 或者只是相同的复杂性，因为 Torch 可能不知道 tridiag 的具体结构。理论上，使用三对角矩阵的计算应该快 N 倍。

任何有助于理解 PyTorch 在这种情况下的行为或实施一些替代的 GPU 优化方法的帮助都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78959447/efficient-pytorch-band-matrix-to-dense-matrix-multiplication</guid>
      <pubDate>Sat, 07 Sep 2024 05:46:17 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Flutter 中实现 flex 委托</title>
      <link>https://stackoverflow.com/questions/78959407/how-to-implement-flex-delegate-in-flutter</link>
      <description><![CDATA[当我尝试使用 python 将模型保存在 .tflite 文件中时，我使用 tf_ops，它以将模型保存在 .tflite 文件中的方式实现 flex 委托。
以下是我的示例代码供参考：
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# 仅使用受支持的 TensorFlow Lite 操作和 Flex 委托

converter.target_spec.supported_ops = \[tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\]

当我使用 tflite_flutter 包在 Flutter 中加载模型时，它会抛出错误：无法加载模型
供参考的 Flutter 代码。
import &#39;package:flutter/services.dart&#39;;
import &#39;package:tflite_flutter/tflite_flutter.dart&#39;;
import &#39;dart:convert&#39;;

class TFLiteService {
late Interpreter \_interpreter;
\_interpreter = await Interpreter.fromAsset(&#39;assets/ml_simple_rnn.tflite&#39;);
print(&#39;TFLite 模型已成功加载&#39;);
}

我尝试了 tflite_flutter_helper 包，但没有得到预期的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78959407/how-to-implement-flex-delegate-in-flutter</guid>
      <pubDate>Sat, 07 Sep 2024 05:27:57 GMT</pubDate>
    </item>
    <item>
      <title>在函数中创建 VLLM 对象时会导致内存错误，即使明确清除 GPU 缓存也是如此，只有共享引用才能使代码不会崩溃</title>
      <link>https://stackoverflow.com/questions/78959131/vllm-objects-cause-memory-errors-when-created-in-a-function-even-when-explicitly</link>
      <description><![CDATA[我在 Python 中使用 VLLM 库时遇到了问题。具体来说，当我在函数内部创建 VLLM 模型对象时，我遇到了内存问题，并且无法有效清除 GPU 内存，即使在删除对象并使用 torch.cuda.empty_cache() 之后也是如此。
当我尝试在函数内部实例化 LLM 对象时会出现问题，但如果我在父进程或全局范围内实例化该对象，则不会发生这种情况。这表明 VLLM 在函数中创建和管理对象时存在问题，从而导致内存保留和 GPU 耗尽。
以下是代码的简化版本：
import torch
import gc
from vllm import LLM

def run_vllm_eval(model_name, samples_params, path_2_eval_dataset):
# 在函数中实例化 LLM
llm = LLM(model=model_name, dtype=torch.float16, trust_remote_code=True)

# 在此处运行一些 VLLM 推理或评估（简化）
result = llm.generate([path_2_eval_dataset], samples_params)

# 推理后清理
del llm
gc.collect()
torch.cuda.empty_cache()

# 在此之后，GPU 内存不会被清除正确并导致 OOM 错误
run_vllm_eval()
run_vllm_eval()
run_vllm_eval()

但是
llm = run_vllm_eval2()
llm = run_vllm_eval2(llm)
llm = run_vllm_eval2(llm)

有效。
即使明确删除 LLM 对象并清除缓存后，GPU 内存仍未正确释放，导致在尝试加载或运行同一脚本中的另一个模型时出现内存不足 (OOM) 错误。
我尝试过的方法：

使用 del 删除 LLM 对象。
运行 gc.collect() 以触发 Python 的垃圾集合。
使用 torch.cuda.empty_cache() 清除 CUDA 内存。
确保父进程中没有实例化 VLLM 对象。

当在函数内创建 LLM 对象时，这些似乎都无法解决问题。
问题：

在函数内创建 VLLM 对象时，有人遇到过类似的内存问题吗？
是否有推荐的方法来管理或清除函数中的 VLLM 对象以防止 GPU 内存保留？
在这种情况下，是否存在与标准 Hugging Face 或 PyTorch 模型不同的特定 VLLM 处理技术？
]]></description>
      <guid>https://stackoverflow.com/questions/78959131/vllm-objects-cause-memory-errors-when-created-in-a-function-even-when-explicitly</guid>
      <pubDate>Sat, 07 Sep 2024 00:58:59 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：'DataLoader' 对象在 SuperGradients Trainer 中不可下标</title>
      <link>https://stackoverflow.com/questions/78917847/typeerror-dataloader-object-is-not-subscriptable-in-supergradients-trainer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78917847/typeerror-dataloader-object-is-not-subscriptable-in-supergradients-trainer</guid>
      <pubDate>Tue, 27 Aug 2024 08:28:38 GMT</pubDate>
    </item>
    <item>
      <title>来自segmentation_models_pytorch 的 Unet 在训练中停滞</title>
      <link>https://stackoverflow.com/questions/78798820/unet-from-segmentation-models-pytorch-stalling-in-training</link>
      <description><![CDATA[我一直在遵循一个关于在自定义数据集上训练分割模型的教程，但它拒绝在训练模型方面取得任何进展。
这是我的模型设置
import fragmentation_models_pytorch as smp
import torch

ENCODER = &#39;efficientnet-b0&#39;
ENCODER_WEIGHTS = &#39;imagenet&#39;
CLASSES = [&#39;ship&#39;]
ACTIVATION = &#39;sigmoid&#39;
DEVICE = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

model = smp.Unet(
coder_name=ENCODER, 
coder_weights=ENCODER_WEIGHTS, 
classes=len(CLASSES), 
activation=ACTIVATION,
).to(DEVICE)

from fragmentation_models_pytorch import utils as smp_utils

loss = smp_utils.losses.DiceLoss()
metrics = [
smp_utils.metrics.IoU(threshold=0.5),
]

optimizer = torch.optim.Adam([ 
dict(params=model.parameters(), lr=0.0001),
])


和 epochs 运行器
train_epoch = smp_utils.train.TrainEpoch(
model, 
loss=loss, 
metrics=metrics, 
optimizer=optimizer,
device=DEVICE,
verbose=True,
)

valid_epoch = smp_utils.train.ValidEpoch(
model, 
loss=loss, 
metrics=metrics, 
device=DEVICE,
verbose=True,
)

并且，当我运行训练，模型只是停留在第一个 epoch 上，没有任何进展
max_score = 0

for i in range(0, 40):

print(&#39;\nEpoch: {}&#39;.format(i))
train_logs = train_epoch.run(train_loader)
valid_logs = valid_epoch.run(valid_loader)

# 执行某些操作（保存模型、更改 lr 等）
if max_score &lt; valid_logs[&#39;iou_score&#39;]:
max_score = valid_logs[&#39;iou_score&#39;]
torch.save(model, &#39;./best_model.pth&#39;)
print(&#39;模型已保存！&#39;)

if i == 25:
optimizer.param_groups[0][&#39;lr&#39;] = 1e-5
print(&#39;将解码器学习率降低至 1e-5！&#39;)

结果：
Epoch：0
train：0%| | 0/3851 [00:00&lt;?, ?it/s]

我这样把它放了 3 个小时，它一点变化都没有
我在 CPU (i7-10710U) 上运行（我知道它比 GPU 慢得多，但我的 GPU (GeForce 1650mq) 不支持 cuda），内存为 32 GB，我之前运行过类似的模型，没有任何问题。
有人能帮帮我吗？也许我漏掉了什么？也许有一个更轻的模型可以在我的系统上运行？
我已经尝试了一些其他设置和模型，YOLOv8 和 YOLOv3 也拒绝训练。]]></description>
      <guid>https://stackoverflow.com/questions/78798820/unet-from-segmentation-models-pytorch-stalling-in-training</guid>
      <pubDate>Fri, 26 Jul 2024 15:14:45 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 尽管进行了子采样并且没有设置种子，仍然表现确定性吗？</title>
      <link>https://stackoverflow.com/questions/76691875/xgboost-behaving-deterministically-despite-subsampling-and-not-setting-seed</link>
      <description><![CDATA[据我所知，XGBoost（版本 1.7.4）中的子采样是随机进行的，因此应该将随机行为引入 xgboost - 随机梯度下降也应如此。但是，当我在相同的数据分割上训练/测试 xgboost 时，尽管没有在任何地方设置随机状态，但我总是得到相同的结果，这是为什么？
在定义 XGBoostRegressor 时，我尝试在有和没有 seed=42 的情况下运行代码。我为精心挑选的数据做了这件事，这样我就会知道随机行为只能源自 XGBoost 模型本身，而不是来自变量数据分割。]]></description>
      <guid>https://stackoverflow.com/questions/76691875/xgboost-behaving-deterministically-despite-subsampling-and-not-setting-seed</guid>
      <pubDate>Sat, 15 Jul 2023 01:24:32 GMT</pubDate>
    </item>
    <item>
      <title>预测黄瓜产量</title>
      <link>https://stackoverflow.com/questions/59382497/predicting-cucumber-harvest</link>
      <description><![CDATA[我正在尝试预测温室中黄瓜的收获量。我测量了有关湿度、温度、人造光、阳光和二氧化碳的数据。每天收获的黄瓜数量以公斤为单位。
由于黄瓜需要大约 14 天才能生长，因此前 14 天的测量数据会影响特定日期收获的黄瓜的实际数量。我已经通过将前 14 天的平均测量数据与给定日期的每个收获结果相关联来创建数据集，并使用该数据集训练预测模型。这已经给了我有希望的结果。
现在我想改进系统。我不想对前 14 天的数据取平均值并假设每一天的影响为 1/14，而是想找出对收获结果的实际影响（经验法则表明，收获前 1 天测量的数据对实际收获的黄瓜数量有 50% 的影响；我的目标是验证或改进该规则）。 有什么想法可以实现吗？]]></description>
      <guid>https://stackoverflow.com/questions/59382497/predicting-cucumber-harvest</guid>
      <pubDate>Tue, 17 Dec 2019 21:38:54 GMT</pubDate>
    </item>
    </channel>
</rss>