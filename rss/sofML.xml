<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 08 Dec 2024 03:36:06 GMT</lastBuildDate>
    <item>
      <title>未来训练模型的文本标记方法</title>
      <link>https://stackoverflow.com/questions/79261744/text-labeling-approach-to-train-a-model-in-the-future</link>
      <description><![CDATA[我之前在 Reddit 上没有找到我需要的答案，后来我想起 Stackoverflow 上的朋友，所以我想我可以试一试。
我最近在学习一些机器学习概念，最近真的很有趣。我正在开发一个文档处理器，它将接收一堆 PDF 并标记其中的数据。我想知道我是否有正确的方法来为稍后要创建的模型生成训练数据。
PDF 文件的路径被输入到一个使用 DocTR 提取文本的函数中。数据格式如下：
 data_list.append({
&#39;id&#39;:coord_key,
&#39;text&#39;:block_list[coord_key],
&quot;x1&quot;:float(block[&quot;geometry&quot;][0][0]),
&quot;y1&quot;:float(block[&quot;geometry&quot;][1][0]),
&quot;x2&quot;:float(block[&quot;geometry&quot;][0][1]),
&quot;y2&quot;:float(block[&quot;geometry&quot;][1][1])
});

Coord_Key 用作文本块的 ID。x1、x2、y1、y2 是文档上边界框的坐标，因此位置是已知的。文本当然就是所讨论的文本。
我的想法是获取这些数据并将其传递到 Label Studio，然后我将使用 Open AI 来帮助我应用我想要使用的特定标签。例如：address、phoneNumber、remarks 等。我当然会查看它得出的结果，并在筛选信息时进行更正。这样，我要训练的模型就有了最好的学习信息。
此时的目标是生成训练数据，以便我可以使用 Tensorflow 编写一些内容，学习如何自行进行标记。
我的问题基本上是这种方法是否正确，以及在我开始大量从我必须测试的文档中创建更多此类数据之前，我是否应该为我的模型创建训练数据采取任何其他考虑因素。
感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/79261744/text-labeling-approach-to-train-a-model-in-the-future</guid>
      <pubDate>Sun, 08 Dec 2024 02:23:07 GMT</pubDate>
    </item>
    <item>
      <title>如何正确训练 GAN 模型？</title>
      <link>https://stackoverflow.com/questions/79261315/how-to-properly-train-a-gan-model</link>
      <description><![CDATA[我尝试在我的数据库中训练 ViT-GAN 模型（来自此 repo），其中我有图像作为输入和输出。输入图像是路径规划问题的 PNG 地图。红色通道是障碍物地图，绿色和蓝色是带有起点/目标的一个像素。输出图像将是红色通道上的规划路径，这就是我试图教给模型的内容。来自 repo 的模型在所包含的示例上运行良好，这是一个比我的问题复杂得多的问题，所以我自然而然地认为这个网络可以解决我的问题而无需进一步调整。
我的问题是，即使不接触网络，只需将训练数据替换为挖掘数据也会完全搞乱输出。我尝试以多种方式调整网络，但结果始终保持不变 -&gt;要么是任何给定输入的相同结构化垃圾输出（黑色上的白色补丁），要么是黑色背景上的一些嘈杂的 rgb 补丁。我就是无法让它学习预期的输出。

左边是给定的输入，右边是预期的输出。中间是网络的输出。我读过关于训练 GAN 的文章，我发现它们很不稳定，很难找到正确的结构/参数集，但我就是不知道下一步该怎么做。我已经尝试过的方法：

通过删除一些 transformer/卷积层来缩小网络
减少过滤器的数量，这样我的问题就没那么复杂了
调整生成器/鉴别器的学习率 -&gt; 一起和单独调整
按照另一篇类似文章的建议，将鉴别器输出层的激活函数更改为 sigmoid
删除所有 ReLU 激活函数，并用 LeakyReLU 替换它们，以解决可能的梯度消失问题
将 Wasserstein 损失添加到鉴别器损失函数，以避免模型崩溃
将生成器损失函数中像素的平均差异更改为总和差异（以更多地惩罚全黑输出）
使用偏差来避免梯度消失
将过滤器的随机初始化器从 (0.0, 0.02) 更改为 -&gt; (0.0, 1.0) 使初始过滤器更加多样化
更改 lambda、批量大小、ff_dim、头数、补丁大小、嵌入暗淡、投影暗淡参数
训练 200 个时期和 20 个时期。全部针对相同的输出结构。
在数据集的较小部分（400 张图像）和较大部分（1000 张和 3000 张图像）上进行训练

我知道这个问题也可以用其他（可能更简单）网络来解决，但我想让这个 GAN 工作，并了解为什么它一开始就不起作用。我将我当前的更改状态添加到此repo。我不知道下一步该去哪里。如果有人有建议，请随时分享。如果您建议调整某些参数，请写一个具体的值。如果我忘记了描述中的任何内容，请告诉我。]]></description>
      <guid>https://stackoverflow.com/questions/79261315/how-to-properly-train-a-gan-model</guid>
      <pubDate>Sat, 07 Dec 2024 20:09:37 GMT</pubDate>
    </item>
    <item>
      <title>在新的医学图像上训练深度学习模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79261142/training-deep-learning-model-on-new-medical-images</link>
      <description><![CDATA[我有一个基于深度学习的医学图像配准模型，我已经在数据集（大脑的 MRI 图像）上对其进行了训练，该数据集在模型的 GitHub 存储库中提供。现在我想在自己的数据集（FBCT 和 CBCT 图像）上对其进行训练 - 有人可以告诉我如何执行此操作的步骤吗？
我需要更改数据集的大小以匹配其他数据集吗？图片的模态在训练等方面是否起作用？]]></description>
      <guid>https://stackoverflow.com/questions/79261142/training-deep-learning-model-on-new-medical-images</guid>
      <pubDate>Sat, 07 Dec 2024 18:27:22 GMT</pubDate>
    </item>
    <item>
      <title>每次使用 Keras 加载模型后，是否都需要调用 model.fit() 函数</title>
      <link>https://stackoverflow.com/questions/79261060/do-i-need-to-call-model-fit-function-every-time-after-loading-the-model-using</link>
      <description><![CDATA[我正在使用 Keras 创建模型：
def my_model():
model = Sequential()
model.add(Conv2D(60, (5,5), input_shape=(32,32,1),activation=&#39;relu&#39;))
model.add(Conv2D(60, (5,5),activation=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(30, (3,3),activation=&#39;relu&#39;))
model.add(Conv2D(30, (3,3),activation=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2,2)))
#model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(500,activation=&#39;relu&#39;))
model.add(Dropout(0.5))
model.add(Dense(num_classes,activation=&#39;softmax&#39;))
## 编译模型
model.compile(Adam(learning_rate = 0.001),loss = &#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
return model

现在保存模型并进行训练：
model = my_model()
print(model.summary())
model.save(&#39;my_model.h5&#39;)
history = model.fit(X_train, y_train,epochs = 10,validation_data=(X_val, y_val),batch_size = 400,verbose = 1,shuffle = 1)

#### 现在预测一些图像
pred = model.predict(img)
prediction = np.argmax(pred,axis=1)
print(&quot;predicted sign: &quot;+ str(prediction))

&#39;model.fit()&#39; 函数是最耗 CPU 的工作。在功能较弱的平台上，训练需要花费大量时间。是否可以保存训练好的模型并将其用于预测？
我知道有办法加载保存的模型：
from tensorflow.keras.models import load_model 
model = load_model(&#39;my_model.h5&#39;) 

但是我仍然必须运行耗费 CPU 的 &#39;model.fit()&#39;。有没有办法避免使用 model.fit() 并加载模型并用于预测？]]></description>
      <guid>https://stackoverflow.com/questions/79261060/do-i-need-to-call-model-fit-function-every-time-after-loading-the-model-using</guid>
      <pubDate>Sat, 07 Dec 2024 17:33:44 GMT</pubDate>
    </item>
    <item>
      <title>机器学习试图将现实世界的数据融入数学方程式[关闭]</title>
      <link>https://stackoverflow.com/questions/79260639/machine-learning-trying-to-fit-real-world-data-into-mathematical-equations</link>
      <description><![CDATA[例如，在监督学习中，我们根据现实生活中的数据训练模型，并尝试找到最合适的数学表示。我的问题是，为什么现实生活中的数据（似乎是随机的）会符合数学方程式 y=mx+c、y=c1x1+c2x2 或任何复杂形式？这是因为现实世界中的大多数事物都可以用数学来概括吗？]]></description>
      <guid>https://stackoverflow.com/questions/79260639/machine-learning-trying-to-fit-real-world-data-into-mathematical-equations</guid>
      <pubDate>Sat, 07 Dec 2024 12:50:57 GMT</pubDate>
    </item>
    <item>
      <title>Llama3 8B 在 Tesla Core GPU 上生成文本非常慢</title>
      <link>https://stackoverflow.com/questions/79260246/llama3-8b-generating-text-very-slow-on-tesla-core-gpu</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79260246/llama3-8b-generating-text-very-slow-on-tesla-core-gpu</guid>
      <pubDate>Sat, 07 Dec 2024 08:25:29 GMT</pubDate>
    </item>
    <item>
      <title>想要使用 tf-idf 或类似的矢量化器进行垃圾邮件分析 Cnn，而不是标记器 [关闭]</title>
      <link>https://stackoverflow.com/questions/79260117/want-to-use-tf-idf-or-simillar-vectorizer-for-my-spam-email-analysis-cnn-instead</link>
      <description><![CDATA[因此，当我使用 tokenizer 修改 X test 和 X train 时，我的 CNN 项目成功运行
X_train = X_train.ravel()
X_test = X_test.ravel()
#
#y_train = y_train.ravel().tolist()
#y_test =y_test.ravel().tolist()
tokenizer = text.Tokenizer(num_words=config.vocab_size)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_matrix(X_train)

X_test = tokenizer.texts_to_matrix(X_test)

X_train =sequence.pad_sequences(X_train,maxlen=config.max_len)
X_test =序列。pad_sequences(X_test,maxlen=config.max_len)

但是当我尝试使用 Tf-idf 向量器时，由于某种原因它无法工作
X_train = X_train.ravel()
X_test = X_test.ravel()
#
#y_train = y_train.ravel().tolist()
#y_test =y_test.ravel().tolist()

X_train = tfidf.fit_transform(X_train)

X_test = tfidf.fit_transform(X_test)

X_train = 序列。pad_sequences(X_train,maxlen=config.max_len)
X_test = 序列。pad_sequences(X_test,maxlen=config.max_len)

有没有可能解决这个问题？我之所以这样想，是因为使用该模型预测电子邮件的输出网站使用 Tf-idf 矢量化器来分析输入文本]]></description>
      <guid>https://stackoverflow.com/questions/79260117/want-to-use-tf-idf-or-simillar-vectorizer-for-my-spam-email-analysis-cnn-instead</guid>
      <pubDate>Sat, 07 Dec 2024 07:02:00 GMT</pubDate>
    </item>
    <item>
      <title>如何在多项任务上训练 LSTM 模型并使用它根据初始条件预测完整轨迹？</title>
      <link>https://stackoverflow.com/questions/79259872/how-to-train-an-lstm-model-on-multiple-missions-and-use-it-to-predict-full-traje</link>
      <description><![CDATA[我正在为机器人应用训练 LSTM 模型（我一直在测试不同的 ML）。我有来自多个“任务”的数据，每个任务包含 900 个时间步骤。每个时间步骤包括：
输入 (theta_vector)：具有 4 个值 [theta_0、theta_cmd、theta_dot_0、thetadot_cmd] 的状态向量。
目标 (f_value)：与时间步骤相对应的单个控制输入值。
训练：
训练 LSTM 模型以学习每个任务中的时间依赖性（即，theta_vector 如何演变并与 900 个时间步骤中的 f_value 相对应）。
通过对所有任务的数据进行训练，将模型推广到各个任务。
在训练期间，我提供了所有任务中所有时间步骤的数据（例如，每个时间步骤的 theta_vector 和相应的 f_value）。
推理：
在推理时，仅提供初始条件（第一个时间步骤的 theta_vector）。
预测所有 900 个时间步骤的完整控制输入轨迹（f_value）。
模型应该生成轨迹而不需要显式动力学建模（时间关系应该从训练数据中学习）。
问题：
我不确定如何做到这一点，也不知道如何在 PyTorch 中正确构建训练和推理管道。具体来说：
推理：
给定初始条件，如何设置 LSTM 以生成 900 个时间步骤的完整控制输入轨迹？我是否应该将预测的 f_value 迭代地反馈到模型中以供下一个时间步骤使用？
这有可能实现吗？即以时间步骤的方式进行训练，然后输入初始条件并接收结果（根据一些论文，Transformers 可以做到这一点，但我没有找到有关如何实现它的任何详细信息）？
我们是否也将使用 Transformers 来代替 LSTM 来查看它们的性能差异？以 transfermer 的方式放置它是否更容易？
我尝试过的方法：
我创建了一个 PyTorch DataLoader 来批量处理任务，并以 [f_value] 为目标，在 [theta_vector] 序列上训练 LSTM。
为了进行推理，我尝试将 initial_condition 作为输入，并使用模型迭代生成轨迹。但是，我在维护正确的输入/输出形状和更新后续时间步骤的隐藏状态方面遇到了问题。]]></description>
      <guid>https://stackoverflow.com/questions/79259872/how-to-train-an-lstm-model-on-multiple-missions-and-use-it-to-predict-full-traje</guid>
      <pubDate>Sat, 07 Dec 2024 02:56:41 GMT</pubDate>
    </item>
    <item>
      <title>使用共生矩阵时如何可视化 Kmeans 聚类？</title>
      <link>https://stackoverflow.com/questions/79259076/how-to-visualise-kmeans-clusters-when-using-cooccurrence-matrices</link>
      <description><![CDATA[我正在尝试使用 Kmeans 对单词进行聚类。我有一个大型文档，我首先使用 NLTK RegexpTokenizer，然后根据字数、长度进行过滤并删除停用词。接下来，我构建一个共现矩阵并使用它来训练 Kmeans 模型。最后，我使用轮廓分数测试它的性能。
我想可视化这些集群。这通常似乎是使用散点图和标签来完成的。当它目前是一个 N x N 矩阵（N 是唯一单词的数量）时，我如何将这个共现矩阵简化为 x 和 y？我尝试过使用 PCA：
plt.figure()
pca_2d = PCA(n_components=2)
reduced = pca_2d.fit_transform(mat)
newKm = KMeans(n_clusters=3)
labels = newKm.fit_predict(reduced)
plt.scatter(reduced[:, 0], Reduced[:, 1], c=labels) # 选择第 0 列（所有行）作为 x 坐标，第 1 列（所有行）作为 y 坐标。然后对标签进行聚类。
plt.title(&quot;K-means Clustering&quot;)
plt.show()

但不确定这是最佳或正确的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79259076/how-to-visualise-kmeans-clusters-when-using-cooccurrence-matrices</guid>
      <pubDate>Fri, 06 Dec 2024 18:55:00 GMT</pubDate>
    </item>
    <item>
      <title>NLTK 和 Spacy 中的标记化列表</title>
      <link>https://stackoverflow.com/questions/79250131/list-of-tokenization-in-nltk-and-spacy</link>
      <description><![CDATA[我开始研究 NLP 基础是标记化，我看到了两样东西，即 NLTK 和 spcy
你能说出像 sent、word 等标记化吗？像这样总体来说，NLTK 和 Spacy 中有多少个我需要完整的标记化东西。]]></description>
      <guid>https://stackoverflow.com/questions/79250131/list-of-tokenization-in-nltk-and-spacy</guid>
      <pubDate>Wed, 04 Dec 2024 06:54:22 GMT</pubDate>
    </item>
    <item>
      <title>如何从非结构化文本中提取特定实体</title>
      <link>https://stackoverflow.com/questions/79227390/how-to-extract-specific-entities-from-unstructured-text</link>
      <description><![CDATA[给定一个通用文本句子（在特定上下文中），如何使用 python 和任何 NLP 库提取属于特定“类别”的感兴趣的单词/实体？
例如，给定烹饪食谱的步骤 将洋葱添加到一碗胡萝卜 作为输入文本，我想检索 洋葱 和 胡萝卜，而给定 撒上辣椒粉。 应该返回 辣椒粉。
但这也适用于不包含任何食物实体的句子，如 搅拌均匀，再煮一分钟。。
到目前为止，我能够实现的是使用 spacy 库来训练 NER 模块来解析句子。 NER 管道的问题在于它是一种基于规则的解析，它经过训练，提供了一组句子和实体/匹配/标签供学习，它在与训练期间使用的句子类似的句子上工作正常，但在新的和不同的句子上表现不佳：
nlp = spacy.load(&#39;trained_model&#39;)

document = nlp(&#39;Add powder, mustard, and salt&#39;)
[(ent.text, ent.label_) for ent in document.ents]
# &gt;&gt; [(&#39;添加面粉&#39;, &#39;食物&#39;), (&#39;芥末&#39;, &#39;食物&#39;), (&#39;盐&#39;, &#39;食物&#39;)]
# (相当) 正确的输出

document = nlp(&#39;周末我拍了一座建筑、一辆车和一只松鼠&#39;)
[(ent.text, ent.label_) for ent in document.ents]
# &gt;&gt; [(&#39;建筑&#39;, &#39;食物&#39;), (&#39;汽车&#39;, &#39;食物&#39;), (&#39;松鼠&#39;, &#39;食物&#39;)]
# 错误输出

document = nlp(&#39;搅拌均匀，再煮一分钟。&#39;)
[(ent.text, ent.label_) for ent in document.ents]
# &gt;&gt; [(&#39;stir well&#39;, &#39;FOOD&#39;), (&#39;cook&#39;, &#39;FOOD&#39;), (&#39;additionalminute.&#39;, &#39;FOOD&#39;)]
# 错误输出

我知道有几个类似的问题和帖子，但我发现只有适用于“半结构化”文本的解决方案，即成分列表为 1 茶匙糖、1 杯牛奶、...，可以使用以前的基于规则的方法轻松解决。此外，nltk 和词性 (POS) 也是一种选择，但我更喜欢替代解决方案，而不是将每个名词与详尽的食物列表进行比较。
我正在寻找的是一种提取特定实体的方法，或者至少使用基本解析之外的其他类别对通用文本中的单词进行分类。
我应该使用/查看哪些方法来实现这一点？]]></description>
      <guid>https://stackoverflow.com/questions/79227390/how-to-extract-specific-entities-from-unstructured-text</guid>
      <pubDate>Tue, 26 Nov 2024 15:46:48 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“anomalib.engine”的模块</title>
      <link>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</link>
      <description><![CDATA[# 导入所需模块

从 anomalib.data 导入 MVTec
从 anomalib.models 导入 Patchcore
从 anomalib.engine 导入 Engine

错误：
ModuleNotFoundError：没有名为“anomalib.engine”的模块

我正在尝试运行它......已按照库安装并看到
https://anomalib.readthedocs.io/en/latest/markdown/get_started/anomalib.html
我认为这要么是因为引擎已被修改，要么已被库删除......
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</guid>
      <pubDate>Sat, 03 Feb 2024 05:25:02 GMT</pubDate>
    </item>
    <item>
      <title>如何为项目确定正确的 NLP 方法</title>
      <link>https://stackoverflow.com/questions/77410116/how-to-decide-correct-nlp-approach-for-a-project</link>
      <description><![CDATA[我正在从事一个 NLP 项目。我的任务是从对话本身确定土耳其呼叫中心对话的类别和情感分数。我们使用 Python 作为编程语言。但是，我在项目的初始阶段陷入了数百种备选解决方案的困境。我有 300,000 行客户代表和客户文本数据，我已经通过预处理阶段很好地清理了它们。所有数据目前都以句子标记形式呈现，并已通过其他标准预处理阶段。客户代表和客户对话已准备好放在 ~600 MB csv 的不同列中。在决定建模算法之前，我的经理希望我准备好训练数据集。数据集应该包含以下信息，我们稍后会决定哪些是必要的，哪些是不必要的，然后我们才会最终确定数据集：

应该提取词向量
应该提取句子向量
应该提取对话的摘要，并提取这些摘要的句子和词向量
应该提取NER计数
应该提取POS计数
应该提取形态分析，并将词缀、连词等以数字形式显示
应该从主观性和极性两个方面提取情感得分
应该提取关键词及其向量
应该进行主题建模，并将每个对话最接近的主题添加到数据集中
应该对摘要和主要对话之间的相似度得分进行提取
应提取对话中提到的单词的稀有度比率，并逐句取平均值（我们认为这将给出句子含义丰富的程度）

我面临的问题如下：

土耳其语最好的词向量提取库是什么？有 Word2vec、NGram、GloVe 等技术。还有相对较新的技术，如 Fasttext。BERT 也是一个选择。我应该选择哪一个？我将如何比较它们的表现？我应该训练自己的模型，还是应该选择预先训练的模型？（例如，Fasttext 有针对土耳其语的训练模型）哪一个比哪一个更优秀或更新颖？如果它们都使用了不同的技术，我应该以哪篇文章或研究为依据？

Gensim 似乎是一个带有工具的库，可以为许多 NLP 问题提供解决方案。这个库太大了，我甚至无法掌握它的全部功能。我应该继续使用 Gensim，还是应该一起使用不同的工具？它会满足我的所有需求吗？我怎么知道？

有很多工具可以进行词形还原，这些工具也是矢量化的，因为我也会进行词形还原，我应该使用它们的矢量化功能，还是应该从上面提到的最多的矢量化工具中受益？哪一个能给出最好的结果？我读过很多比较文章，它们都谈到了不同的结果。

有针对土耳其语训练的 SBERT 模型用于提取句子向量。当我使用其他工具提取词向量时，使用 SBERT 获得的向量会变得毫无意义吗？毕竟，我会用不同的方法提取这些，而且它们会在同一个数据集中。


由于这些替代解决方案彼此之间的优越性存在模糊性，我感到很困惑。
NLP 项目应该如何正确开展？]]></description>
      <guid>https://stackoverflow.com/questions/77410116/how-to-decide-correct-nlp-approach-for-a-project</guid>
      <pubDate>Thu, 02 Nov 2023 13:45:26 GMT</pubDate>
    </item>
    <item>
      <title>关系提取中表面形式是什么意思？</title>
      <link>https://stackoverflow.com/questions/76495143/what-does-surface-form-mean-in-relation-extraction</link>
      <description><![CDATA[大多数关系抽取论文中都会反复提到“实体表面形式”这个术语。它是什么意思呢？
例如，在REBEL论文中，作者提到“只有正确提取出头和尾实体表面形式，关系才被认为是正确的。”
“头”和“尾”是什么意思？]]></description>
      <guid>https://stackoverflow.com/questions/76495143/what-does-surface-form-mean-in-relation-extraction</guid>
      <pubDate>Sat, 17 Jun 2023 07:15:12 GMT</pubDate>
    </item>
    <item>
      <title>用于大数据集分类的 NLP 软件</title>
      <link>https://stackoverflow.com/questions/7248372/nlp-software-for-classification-of-large-datasets</link>
      <description><![CDATA[多年来，我一直在使用自己的贝叶斯类方法，基于大量且不断更新的训练数据集对来自外部来源的新项目进行分类。
每个项目有三种类型的分类：

30 个类别，其中每个项目必须属于一个类别，最多两个类别。
10 个其他类别，其中每个项目只有在强匹配的情况下才与一个类别相关联，并且每个项目可以属于尽可能多的匹配类别。
4 个其他类别，其中每个项目必须只属于一个类别，如果没有强匹配，则该项目被分配到默认类别。

每个项目由大约 2,000 个字符的英文文本组成。我的训练数据集中有大约 265,000 个项目，其中包含粗略估计的 10,000,000 个特征（独特的三个词短语）。
我的自制方法相当成功，但肯定还有改进的空间。我读过 NLTK 书中的“学习对文本进行分类”一章，这本书很棒，让我对 NLP 分类技术有了很好的概述。我希望能够尝试不同的方法和参数，直到获得对我的数据来说最好的分类结果。
问题
有哪些现成的 NLP 工具可以有效地对如此大的数据集进行分类？
到目前为止，我尝试过的工具有：

NLTK
TIMBL

我尝试使用一个数据集来训练它们，该数据集包含不到 1% 的可用训练数据：1,700 个项目，375,000 个特征。对于 NLTK，我使用了稀疏二进制格式，对于 TIMBL，我使用了类似的紧凑格式。
两者似乎都依赖于在内存中执行所有操作，并且很快消耗了所有系统内存。我可以让它们处理微小的数据集，但不能处理大数据集。我怀疑如果我尝试逐步添加训练数据，那么同样的问题会在当时或进行实际分类时发生。
我查看了 Google 的预测 API，它似乎可以完成我的大部分工作，但不是全部。如果可能的话，我也想避免依赖外部服务。
关于特征的选择：多年来，在我使用自制方法进行测试时，三个词组产生了迄今为止最好的结果。虽然我可以通过添加单词或两个词组来减少特征的数量，但这很可能会产生较差的结果，并且仍然会是大量的特征。]]></description>
      <guid>https://stackoverflow.com/questions/7248372/nlp-software-for-classification-of-large-datasets</guid>
      <pubDate>Tue, 30 Aug 2011 19:00:10 GMT</pubDate>
    </item>
    </channel>
</rss>