<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 30 May 2024 09:16:33 GMT</lastBuildDate>
    <item>
      <title>特征提取 + 逻辑回归 与 特征提取 + softmax 密集层的区别</title>
      <link>https://stackoverflow.com/questions/78553888/differences-on-features-extraction-logistic-regression-vs-features-extraction</link>
      <description><![CDATA[我有一个用于分类问题的小型图像数据库，因此我选择迁移学习方法。我从 Tensorflow 中的经典方法开始：

在 ImageNet 数据库上预训练的 ResNet50
GlobalMaxPooling2D 层
具有 softmax 激活函数的类数的密集层
使用 Adam 编译器、交叉熵损失和准确率作为指标进行编译。

然后我需要优化超参数，例如批量大小和时期。
一段代码是
将 tensorflow 导入为 tf
数据 = 加载并预处理您的数据
输入 = tf.keras.Input(shape=(None, None, 3))
x= base_resnet(x)
输出 = tf.keras.layers.GlobalMaxPooling2D()(x)
extractor = tf.keras.Model(inputs, output)
x = tf.keras.layers.Dense(nClasses)(x)
outputs = tf.keras.layers.Activation(activation=&quot;softmax&quot;, dtype = &#39;float32&#39;)(x)
model = tf.keras.Model(inputs, output)
model.compile(optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;], loss = &quot;categorical_crossentropy&quot;)

超参数优化之后我会做的
model.fit(...)
model.evaluate(...)

也许是哥伦布蛋，但我已经尝试了提取特征方法加上逻辑回归scikit-learn。
代码片段如下：
from sklearn.linear_model import LogisticRegression

x= base_resnet(x)
outputs = tf.keras.layers.GlobalMaxPooling2D()(x)
extractor = tf.keras.Model(inputs, output)
features = extractor.predict(data) # features extractor

# logistic 回归
log_reg = LogisticRegression(max_iter=500)
log_reg.fit(features, y)

log_reg.predict(features_test)

我在这两种方法中都获得了几乎相同的良好性能（测试集上的准确率接近 91%），但第二种方法快了 10 倍。我遗漏了什么吗？我的方法正确吗？]]></description>
      <guid>https://stackoverflow.com/questions/78553888/differences-on-features-extraction-logistic-regression-vs-features-extraction</guid>
      <pubDate>Thu, 30 May 2024 08:47:43 GMT</pubDate>
    </item>
    <item>
      <title>在经过训练的 LSTM 模型中，使用较大的窗口大小无法获得任何预测</title>
      <link>https://stackoverflow.com/questions/78553585/not-getting-any-predictions-with-higher-window-size-in-trained-lstm-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78553585/not-getting-any-predictions-with-higher-window-size-in-trained-lstm-model</guid>
      <pubDate>Thu, 30 May 2024 07:50:34 GMT</pubDate>
    </item>
    <item>
      <title>平均准确率</title>
      <link>https://stackoverflow.com/questions/78553088/average-precision-score</link>
      <description><![CDATA[GridSearchCV 给出的每次折叠的平均精度得分与我使用相同分割使用 for 循环计算平均精度得分获得的得分不同。
平均精度得分
它们是否以 Sklearn 指南中提到的两种不同方式计算？
在此处输入图像描述
我尝试使用 GridSearchCV 中使用的相同分割来计算平均精度得分，但 GridSearchCv.cv_results_ 的结果更高。]]></description>
      <guid>https://stackoverflow.com/questions/78553088/average-precision-score</guid>
      <pubDate>Thu, 30 May 2024 05:49:52 GMT</pubDate>
    </item>
    <item>
      <title>单项赛冠军如何排序？</title>
      <link>https://stackoverflow.com/questions/78552850/how-to-classify-winner-of-individual-races</link>
      <description><![CDATA[随着奥运会的临近，我认为这将是一个有趣的机会来提高一些机器学习方法的技能。我读过各种试图预测赛马、足球等比赛结果的例子，但我缺少一些关于我的用例的细节。
让我们以游泳为例。我有一堆运动员在不同距离和项目上的历史表现数据。我想训练一个分类模型，以便能够预测特定比赛的获胜者。
我已经研究出如何使用 GroupKFold 来确保我的训练集和测试集中的数据沿着我的 race_id 的边界干净地分割，这样就不会有一半的比赛同时出现在两场比赛中。然后，我根据 is_winner 的值针对分类/结果的各种特征训练模型。
该模型需要尽最大努力查看测试集并预测每场比赛的获胜者。我并不是想查看整个数据集并说“对所有可能成为赢家的行进行分类”。这对于决赛之类的比赛毫无帮助，决赛由大约 8 名游泳运动员组成，他们在国内比赛和大多数国际比赛中都可能保持不败，而每年只有一两次比赛，与地球上比他们更快的其他六个人比赛。
这就是为什么我试图找出如何在单个比赛中进行分类和预测的原因。到目前为止，我想到的最好的解决方案是手动循环测试集中的每一场比赛，只对这大约 8 名参赛者测试预测，重复，保持结果的累计以计算精度/准确度/召回率/等。但事实证明这非常慢。
有没有更好的方法可以探索？]]></description>
      <guid>https://stackoverflow.com/questions/78552850/how-to-classify-winner-of-individual-races</guid>
      <pubDate>Thu, 30 May 2024 04:25:33 GMT</pubDate>
    </item>
    <item>
      <title>由于 decision_function，使用 roc_auc 度量的 KNeighborsClassifier 的 GridSearchCV 和 cross_val_score 返回错误</title>
      <link>https://stackoverflow.com/questions/78552800/gridsearchcv-and-cross-val-score-with-kneighborsclassifier-using-roc-auc-metric</link>
      <description><![CDATA[我正在研究二元分类问题。
类别分布为位置：30% - 负面：70%。因此，我决定使用 roc_auc 作为度量标准
然后，我在 KNeighborsClassifier 上运行超参数调整，但出现错误，我不知道如何解决
我使用的 scikit-learn 版本是 &#39;1.2.2&#39;
这是代码
param_grid = [ 
{
&#39;knn__n_neighbors&#39;: np.arange(2, 30, 1),
&#39;knn__weights&#39; : [&#39;uniform&#39;, &#39;distance&#39;],
&#39;knn__algorithm&#39; : [&#39;auto&#39;, &#39;ball_tree&#39;, &#39;kd_tree&#39;],
&#39;knn__leaf_size&#39;: np.arange(30, 1000, 20)
}
]

knn = Pipeline([
(&quot;preprocessing&quot;, preprocessing),
(&quot;knn&quot;, KNeighborsClassifier())
])

grid_search = GridSearchCV(
knn,
param_grid,
cv=cv,
scoring=&quot;roc_auc&quot;,
verbose=0
)
grid_search.fit(x_train, y_train)
grid_search.best_params_

错误是
/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:778: UserWarning: 评分失败。此训练测试分区中这些参数的分数将设置为 nan。详细信息：
回溯（最近一次调用）：
文件“/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py”，第 373 行，在 _score 中
y_pred = method_caller(clf, “decision_function”, X)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py”，第 73 行，在 _cached_call 中
返回 getattr(estimator, method)(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件&quot;/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_available_if.py&quot;, line 32, in __get__
if not self.check(obj):
^^^^^^^^^^^^^^^
File &quot;/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py&quot;, line 46, in check
getattr(self._final_estimator, attr)
AttributeError: &#39;KNeighborsClassifier&#39; object has no attribute &#39;decision_function&#39;

在处理上述异常时，发生了另一个异常：

它说 KNeighborsClassifier 没有属性 decision_function
经过一些阅读，我明白了 decision_function 在度量标准为roc_auc
现在，即使存在此问题，如何运行超参数调整？
此外，即使在使用 KNeighborsClassifier 和 roc_auc 作为指标运行时，cross_val_score 也会返回 nan
knn = Pipeline([
(&quot;preprocessing&quot;, preprocessing),
(&quot;knn&quot;, KNeighborsClassifier(
algorithm=&#39;auto&#39;,
leaf_size=30,
metric=&#39;minkowski&#39;,
n_neighbors=28,
weights=&#39;uniform&#39;
))
])
scores = cross_val_score(knn, x_train, y_train, cv=cv)
scores

错误是
/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:794: UserWarning: 评分失败。此训练测试分区中这些参数的分数将设置为 nan。详细信息：
回溯（最近一次调用）：
文件“/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py”，第 117 行，在 __call__ 中
score = scorer(estimator, *args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py”，第 444 行，在 _passthrough_scorer 中
返回 estimator.score(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件&quot;/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py&quot;，第 722 行，在分数中
返回 self.steps[-1][1].score(Xt, y, **score_params)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py&quot;，第 668 行，在分数中
返回 accuracy_score(y, self.predict(X), sample_weight=sample_weight)
^^^^^^^^^^^^^^^^^
文件 &quot;/opt/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_classification.py&quot;，第234，在预测中
neigh_ind = self.kneighbors(X, return_distance=False)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;/opt/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_base.py&quot;，第 824 行，在 kneighbors 中
results = ArgKmin.compute(

现在由 predict() 完成]]></description>
      <guid>https://stackoverflow.com/questions/78552800/gridsearchcv-and-cross-val-score-with-kneighborsclassifier-using-roc-auc-metric</guid>
      <pubDate>Thu, 30 May 2024 04:00:25 GMT</pubDate>
    </item>
    <item>
      <title>理解环境中的形状</title>
      <link>https://stackoverflow.com/questions/78552788/understanding-shapes-in-enviroment</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78552788/understanding-shapes-in-enviroment</guid>
      <pubDate>Thu, 30 May 2024 03:52:57 GMT</pubDate>
    </item>
    <item>
      <title>GCP 上的 AI 平台：持续出现“当前尝试发生内部错误。”错误</title>
      <link>https://stackoverflow.com/questions/78552412/ai-platform-on-gcp-persistent-internal-error-occurred-for-the-current-attempt</link>
      <description><![CDATA[大约一周前，我每次向 Google 的 AI 平台提交作业时都会不断收到此错误：当前尝试发生内部错误。。在作业终止之前，每个作业都会重复此错误 3 次。我尝试在使用时更改区域
gcloud ai-platform jobs submit training

命令，但无济于事。同时，当我运行
gcloud computeregionslist

每个区域的状态都是“UP”。就目前而言，AI 平台对我来说完全无法使用。我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/78552412/ai-platform-on-gcp-persistent-internal-error-occurred-for-the-current-attempt</guid>
      <pubDate>Thu, 30 May 2024 00:35:03 GMT</pubDate>
    </item>
    <item>
      <title>可并行化的 C++ 代码数据库或数据集</title>
      <link>https://stackoverflow.com/questions/78552019/parallelizable-c-code-database-or-dataset</link>
      <description><![CDATA[是否有任何数据库或数据集包含可以和不能并行化的标记代码示例？
我正在构建的项目需要这样的数据集，而创建自己的数据集似乎不是一个非常可靠的选择。]]></description>
      <guid>https://stackoverflow.com/questions/78552019/parallelizable-c-code-database-or-dataset</guid>
      <pubDate>Wed, 29 May 2024 21:30:37 GMT</pubDate>
    </item>
    <item>
      <title>为什么模型训练需要填充数据？[关闭]</title>
      <link>https://stackoverflow.com/questions/78551888/why-is-padding-data-for-model-training-necessary</link>
      <description><![CDATA[为什么需要填充？有人能解释一下吗？如果你的张量大小不同，它们可以堆叠，但问题是什么？有人能用一个现实生活中的例子来解释一下吗？（我问过 gpt4，但它没有给出令人满意的答案）
我正在尝试制作一个 AI 机器人，它告诉我要填充我的输入，但我不明白为什么？]]></description>
      <guid>https://stackoverflow.com/questions/78551888/why-is-padding-data-for-model-training-necessary</guid>
      <pubDate>Wed, 29 May 2024 20:47:59 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 CLI 创建 Azure 机器学习工作区</title>
      <link>https://stackoverflow.com/questions/78550892/unable-to-create-an-azure-machine-learning-workspace-using-the-cli</link>
      <description><![CDATA[我目前正在尝试使用 CLI 创建一个 Azure 机器学习工作区。我使用了以下代码：
az ml working create --name &quot;rg-dp100-labs&quot; --resource-group &quot;rg-dp100-labs&quot;

我收到以下错误。
代码：ValidationError
消息：工作区 json 中缺少依赖资源
目标：工作区
异常详细信息：（无效）工作区 json 中缺少依赖资源
代码：无效
消息：工作区 json 中缺少依赖资源
目标：工作区

仅创建了 KeyVault 和存储帐户。参考Microsoft Learn 上的链接，我发现其他人也遇到了同样的问题。甚至谷歌搜索也给了我与上面相同的代码。lease，有人遇到过同样的问题并解决了吗？我认为这是最近出现的问题。]]></description>
      <guid>https://stackoverflow.com/questions/78550892/unable-to-create-an-azure-machine-learning-workspace-using-the-cli</guid>
      <pubDate>Wed, 29 May 2024 16:44:20 GMT</pubDate>
    </item>
    <item>
      <title>如何追踪多标签MLP的损失？</title>
      <link>https://stackoverflow.com/questions/78550784/how-to-track-loss-of-multi-label-mlp</link>
      <description><![CDATA[我获得了维度为 5,000 的二进制数据点。我被要求执行机器学习，预测长度为 1k 的二进制向量，其中输出的每个位置都是一个类。这些类别不互斥。
我对类别分布的了解：

索引较小的位置更常见
一个样本可以属于多个类别
每个样本仅满足少数类别要求（即输出是“稀疏的”）

如何跟踪我的 ML 模型中的损失？我使用了多层感知器（pytorch）和交叉熵损失（CE 损失），但我发现很难解释结果。我假设当您有多个类别但一次只选择一个（多类分类）时使用 CE 损失。
此外，我的预测导致向量设置了大约一半的位，而我预计设置了 20 到 50 位，不会更多。
# 一个“示例”数据点：
point = [1, 0, 0, 1, 0, 0, 1, 1, ...,1, 1, 0, 0, 1, 0, 1] # 长度 5000
label = [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...,0] # 长度 1000
# 标签仅设置了 20-50 位
]]></description>
      <guid>https://stackoverflow.com/questions/78550784/how-to-track-loss-of-multi-label-mlp</guid>
      <pubDate>Wed, 29 May 2024 16:17:36 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 SHAP 值来分组特征重要性？</title>
      <link>https://stackoverflow.com/questions/78547686/how-to-use-shap-values-for-grouped-feature-importance</link>
      <description><![CDATA[我做什么：
我借助不同的机器学习算法和不同的预处理步骤等分析来自 EEG 数据的不同生物标志物。这为每种预处理步骤和算法的组合产生了多个模型。
每个模型都使用 StratifiedGroupKFold 进行训练，总共 6 个折叠。
每个折叠都保存为作业库，即 .joblib
生物标志物：
EEG 信号的每个波段都有许多生物标志物。这些生物标志物又由来自 EEG 所有电极的所有信号组成。因此，生物标志物由多个特征组成，这些特征不能分开（每个生物标志物必须包含所有电极数据）。
我想做什么：
在我的第一种方法中，我用所有生物标志物训练了每个模型。现在我想使用特征重要性来确定是否可以省略其中一些。
为此，我想研究每个预处理步骤和每个模型。
有人向我推荐 SHAP，但我的问题是我不知道如何总结每个生物标志物的通道。
编辑：
在这篇论文的帮助下，我终于总结了折叠。但我仍然不明白如何总结每个生物标志物的通道。
新代码：
for r, fold_file in enumerate(fold_files):
model = joblib.load(fold_file)

fold_splits = list(sgkf.split(X, y, groups))

for train_index, test_index in fold_splits:
X_train, X_test = X.iloc[train_index], X.iloc[test_index]
y_train, y_test = y.iloc[train_index], y.iloc[test_index]

explainer = shap.Explainer(model, X_train)
train_shap_values = explainer(X_train)
test_shap_values = explainer(X_test)

for i in range(len(train_index)):
train_folds_shap_values[train_index[i]] += train_shap_values.values[i] / (len(fold_splits) - 1)
for i in range(len(test_index)):
test_folds_shap_values[test_index[i]] += test_shap_values.values[i]

average_train_folds_shap_values = train_folds_shap_values / R
average_test_folds_shap_values = test_folds_shap_values / R

train_shap_df = pd.DataFrame(average_train_folds_shap_values, columns=columns)
test_shap_df = pd.DataFrame(average_test_folds_shap_values, columns=columns)


我第一次尝试它就像这个：
grouped_features = group_features(columns, biomarker_names, bands)

defaggregate_shap_values(shap_df, grouped_features):
aggregated_shap_values = pd.DataFrame()
for group, features in grouped_features.items():
aggregated_shap_values[group] = shap_df[features].sum(axis=1)
returnaggregated_shap_values

train_aggregated_shap_df =aggregate_shap_values(train_shap_df, grouped_features)

test_aggregated_shap_df =aggregate_shap_values(test_shap_df, grouped_features)

shap.summary_plot(train_aggregated_shap_df.values, feature_names=train_aggregated_shap_df.columns.tolist())

但它看起来……不对。我忽略了重要性之间的区别。
分组：

未分组：

提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/78547686/how-to-use-shap-values-for-grouped-feature-importance</guid>
      <pubDate>Wed, 29 May 2024 06:28:02 GMT</pubDate>
    </item>
    <item>
      <title>YoloV8 结果中没有 'box'、'max' 属性</title>
      <link>https://stackoverflow.com/questions/78547320/yolov8-results-have-no-box-max-properties-in-it</link>
      <description><![CDATA[我已经训练了一个 YOLOV8 模型来识别十字路口的物体（即汽车、道路等）。
它工作正常，我可以将输出作为带有感兴趣分割对象的图像。
但是，我需要做的是捕获原始几何图形（多边形），以便稍后将它们保存在 txt 文件中。
我尝试了在文档中找到的内容（https://docs.ultralytics.com/modes/predict/#key-features-of-predict-mode）但返回的对象与文档所述不同。
实际上，结果是 TensorFlow 数字列表：

这是我的代码：
import argparse
import cv2
import numpy as np
from pathlib import Path
from ultralytics.yolo.engine.model import YOLO 

# 解析命令行参数
parser = argparse.ArgumentParser()
parser.add_argument(&#39;--source&#39;, type=str, required=True, help=&#39;源图像目录或文件&#39;)
parser.add_argument(&#39;--output&#39;, type=str, default=&#39;output&#39;, help=&#39;输出目录&#39;)
args = parser.parse_args()

# 如果不存在则创建输出目录
Path(args.output).mkdir(parents=True, exist_ok=True)

# 模型路径
model_path = r&#39;C:\\_Projects\\best_100img.pt&#39;

# 直接加载模型
model = YOLO(model_path)
model.fuse()

# 加载图像
if Path(args.source).is_dir():
image_paths = list(Path(args.source).rglob(&#39;*.tiff&#39;))
else:
image_paths = [args.source]

# 处理每幅图像
for image_path in image_paths:
img = cv2.imread(str(image_path))
if img is None:
continue

# 执行推理
predictions = model.predict(image_path, save=True, save_txt=True)

print(&quot;处理完成。&quot;)

问题在于：返回对象（预测变量）没有框、掩码、关键点和等等。
我想我的问题是：

为什么结果与文档如此不同？
是否有转换步骤？
]]></description>
      <guid>https://stackoverflow.com/questions/78547320/yolov8-results-have-no-box-max-properties-in-it</guid>
      <pubDate>Wed, 29 May 2024 04:32:01 GMT</pubDate>
    </item>
    <item>
      <title>即使按照示例 3 的文档代码操作，PyKan 代码仍然不起作用：分类</title>
      <link>https://stackoverflow.com/questions/78451382/pykan-code-not-working-even-after-following-the-documentation-code-for-example-3</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78451382/pykan-code-not-working-even-after-following-the-documentation-code-for-example-3</guid>
      <pubDate>Wed, 08 May 2024 22:17:49 GMT</pubDate>
    </item>
    <item>
      <title>是否可以加载没有 config.json 文件的 huggingface 模型？</title>
      <link>https://stackoverflow.com/questions/75626974/is-it-possible-to-load-huggingface-model-which-does-not-have-config-json-file</link>
      <description><![CDATA[我尝试使用以下代码从 HF 加载此语义分割模型：
from transformers import pipeline

model = pipeline(&quot;image-segmentation&quot;, model=&quot;Carve/u2net-universal&quot;, device=&quot;cpu&quot;)

但我收到以下错误：
OSError：tamnvcc/isnet-general-use 似乎没有名为 config.json 的文件。查看“https://huggingface.co/tamnvcc/isnet-general-use/main”以获取可用文件。

即使没有提供 config.json 文件，是否也可以从 HuggingFace 加载模型？
我也尝试通过以下方式加载模型：
id2label = {0: &quot;background&quot;, 1: &quot;target&quot;}
label2id = {&quot;background&quot;: 0, &quot;target&quot;: 1}
image_processor = AutoImageProcessor.from_pretrained(&quot;Carve/u2net-universal&quot;)
model = AutoModelForSemanticSegmentation(&quot;Carve/u2net-universal&quot;, id2label=id2label, label2id=label2id)

但出现了同样的错误。]]></description>
      <guid>https://stackoverflow.com/questions/75626974/is-it-possible-to-load-huggingface-model-which-does-not-have-config-json-file</guid>
      <pubDate>Fri, 03 Mar 2023 12:16:42 GMT</pubDate>
    </item>
    </channel>
</rss>