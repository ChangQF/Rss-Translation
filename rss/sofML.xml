<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 08 Oct 2024 12:31:50 GMT</lastBuildDate>
    <item>
      <title>ValueError：调用层“bert_preprocess”（类型 KerasLayer）时遇到异常</title>
      <link>https://stackoverflow.com/questions/79065272/valueerror-exception-encountered-when-calling-layer-bert-preprocess-type-ker</link>
      <description><![CDATA[# Bert 层
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=&#39;text&#39;)
preprocessed_text = bert_preprocess(text_input)
outputs = bert_encoder(preprocessed_text)

# 神经网络层
l = tf.keras.layers.Dropout(0.1, name=&quot;dropout&quot;)(outputs[&#39;pooled_output&#39;])
l = tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;, name=&quot;output&quot;)(l)

# 使用输入和输出构建最终模型
model = tf.keras.Model(inputs=[text_input],outputs = [l])


ValueError：调用时遇到异常层“bert_preprocess”（类型 KerasLayer）。KerasTensor 是符号：它是形状和 dtype 的占位符。它没有任何实际数值。您无法将其转换为 NumPy 数组。调用层“bert_preprocess”（类型 KerasLayer）接收的参数：• 输入=&lt;KerasTensor shape=(None,), dtype=string, sparse=None, name=text&gt; • 训练=None
]]></description>
      <guid>https://stackoverflow.com/questions/79065272/valueerror-exception-encountered-when-calling-layer-bert-preprocess-type-ker</guid>
      <pubDate>Tue, 08 Oct 2024 09:30:47 GMT</pubDate>
    </item>
    <item>
      <title>将 ML 模型从一个 Azure Databricks 工作区复制到另一个 Databricks 工作区</title>
      <link>https://stackoverflow.com/questions/79065062/copy-a-ml-model-from-one-azure-databricks-workspace-to-another-databricks-worksp</link>
      <description><![CDATA[我运行了以下代码以在基于 Azure Databricks 的 mlflow 中导出 ML 模型，但我似乎收到了此错误
MLflow 主机或令牌配置不正确

我无法弄清楚问题是什么。工作区的 URL 和 PAT 令牌都是正确的。
export_import 工具有很多错误。它需要 mlfow 库，但 Databricks ML Runtime 附带的是 mlflow-skinny。
import mlflow
import os
from mlflow_export_import.model.export_model import ModelExporter
from mlflow.tracking import MlflowClient

# 使用工作区 URL 设置 Databricks MLflow 跟踪 URI
mlflow.set_tracking_uri(&quot;https://adb-xxxyyymmmnnnyyy.1.azuredatabricks.net/&quot;)

# 设置两个令牌以实现兼容性
os.environ[&quot;DATABRICKS_TOKEN&quot;] = &quot;mnop6672ec8e20c7d219eb2A-3&quot;
os.environ[&quot;MLFLOW_TRACKING_TOKEN&quot;] = &quot;mnop6672ec8e20c7d219eb2A-3&quot;

# 初始化 MLflow 客户端（无需传递跟踪 URI，因为它是全局设置的）
mlflow_client = MlflowClient()

# 使用 MLflow 客户端初始化 ModelExporter
exporter = ModelExporter(mlflow_client)

# 导出模型
exporter.export_model(
model_name=&quot;Signature_Test&quot;,
output_dir=&quot;/tmp/mlflow_export/model&quot;,
stage=None, # 使用&quot;None&quot; 导出所有阶段，或指定&quot;Staging&quot; 或&quot;Production&quot;
export_metadata_tags=True
)
]]></description>
      <guid>https://stackoverflow.com/questions/79065062/copy-a-ml-model-from-one-azure-databricks-workspace-to-another-databricks-worksp</guid>
      <pubDate>Tue, 08 Oct 2024 08:39:33 GMT</pubDate>
    </item>
    <item>
      <title>Azure 数据集问题</title>
      <link>https://stackoverflow.com/questions/79064369/azure-dataset-issues</link>
      <description><![CDATA[问题是 Azure 在数字后面添加了逗号，因此我无法将其更改为整数。
我的 CSV：

G 列（平均价格）是我遇到问题的那一列。
Azure 数据预览和设置：

然后 Azure 添加了逗号，从而导致以下问题：

如何解决此问题？
从中删除了逗号csv
尝试更改分隔符，但数据无法正确显示]]></description>
      <guid>https://stackoverflow.com/questions/79064369/azure-dataset-issues</guid>
      <pubDate>Tue, 08 Oct 2024 05:04:33 GMT</pubDate>
    </item>
    <item>
      <title>用于增量学习的 Python 非线性回归器</title>
      <link>https://stackoverflow.com/questions/79063665/python-non-linear-regressor-for-incremental-learning</link>
      <description><![CDATA[我想知道 scikit-learn 中是否有一个非线性回归程序，允许增量学习，即通过 partial_fit 调用。我发现 SGDRegressor 和 PassiveAggressiveRegressor 都允许 partial_fit，但它们是线性的，而我的数据显然是非线性的，因此拟合效果并不理想。]]></description>
      <guid>https://stackoverflow.com/questions/79063665/python-non-linear-regressor-for-incremental-learning</guid>
      <pubDate>Mon, 07 Oct 2024 21:18:59 GMT</pubDate>
    </item>
    <item>
      <title>3D加工零件特征识别（点云、网格）</title>
      <link>https://stackoverflow.com/questions/79062004/3d-machining-part-feature-recognition-point-cloud-mesh</link>
      <description><![CDATA[我有一个加工部件 (.STL)，想要识别（并提取）它的加工特征。有些特征很简单，但有些更复杂，这就是为什么我认为机器学习方法会很合适，因为我无法用数学方式描述该特征。
有一个 FeatureNet，它基本上可以完成这项工作，但它无法识别多个特征，并且代码无法按预期工作。
我还知道 AAGNet，它可以完成我想要的工作，但它使用 .STEP 作为输入，但我有一个网格（如果我转换它，则是点云）。
由于有更多的点云存储库，我认为我可以使用它们来解决我的问题。像 FPFH 这样的东西是正确的方向吗，还是我走错了路？
如果我使用机器学习方法，我可以轻松创建标记数据集。]]></description>
      <guid>https://stackoverflow.com/questions/79062004/3d-machining-part-feature-recognition-point-cloud-mesh</guid>
      <pubDate>Mon, 07 Oct 2024 12:41:25 GMT</pubDate>
    </item>
    <item>
      <title>验证多元线性回归模型的 AUC 计算</title>
      <link>https://stackoverflow.com/questions/79061528/validating-auc-calculation-for-a-multiple-linear-regression-model</link>
      <description><![CDATA[我正在尝试计算多元线性回归模型（两个变量）的 AUC 值，因此我正在处理以下代码并想与您确认。
我有两个变量，分别名为 Sa_T1 和 Sa_07，x 轴和 y 轴，z 轴上有 log_EDPs，我确实使用多元线性回归来确定以下关系：
log_EDPs = coef_log_Sa_T1 * log_Sa_T1 + coef_log_Sa_07 * log_Sa_07 + 截距
结果分别为以下值：0.3364、0.6530、-7.1452。现在我想计算 AUC，我根据我之前的单变量案例代码开发了以下代码：
# 系数
coef_log_Sa_T1 = 0.3364
coef_log_Sa_07 = 0.6530
intercept = -7.1452

# 使用回归模型预测的 log(edp)
predicted_log_edp = coef_log_Sa_T1 * log_Sa_T1 + coef_log_Sa_07 * log_Sa_07 + 截距

# 基于阈值的 log(edp)
threshold_SA = np.log(np.median(log_EDPs))
y_true = (log_EDPs &gt;= Threshold_SA).astype(int) 

y_pred_binary = (predicted_log_edp &gt;= Threshold_SA).astype(int)

y_pred_prob = (predicted_log_edp - predicted_log_edp.min()) / (predicted_log_edp.max() - predict_log_edp.min()) 

# 根据预测概率计算 AUC
auc_prob = roc_auc_score(y_true, y_pred_prob)
print(f&quot;AUC (probabilities): {auc_prob}&quot;)

结果 AUC 值为 0.9802。
代码是否进行了正确的计算？]]></description>
      <guid>https://stackoverflow.com/questions/79061528/validating-auc-calculation-for-a-multiple-linear-regression-model</guid>
      <pubDate>Mon, 07 Oct 2024 10:12:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 DDP（分布式数据并行）时，在多 GPU 上获得相同的损失，但梯度不同</title>
      <link>https://stackoverflow.com/questions/79061290/get-same-loss-but-different-grad-on-multi-gpus-when-using-ddp-distributeddatapa</link>
      <description><![CDATA[当将 torch.nn.parallel.DistributedDataParallel 添加到单 GPU 训练代码时，我遇到一个问题，即在不同的 GPU 上得到相同的损失但不同的梯度。与之前的单 GPU 训练代码相比，我确信损失是正确的，但在 loss.backward() 之后，我观察了各层的权重和偏差的梯度，发现在 all_gather 之前它们在不同的 GPU 上是不同的，在 all_gather 和计算损失之间的各层的梯度在不同的 GPU 上是相同的。

这是一个对比学习代码，所以我 all_gather 所有 GPU 上的张量来计算共同的最终损失。

以下是该模型的部分代码：
import torch.nn as nn
import torch
from config.base_config import Config
from modules.transformer import Transformer
from modules.stochastic_module import StochasticText
from modules.basic_utils import AllGather
allgather = AllGather.apply
from modules.tokenization_clip 导入 SimpleTokenizer

class CLIPStochastic(nn.Module):
def __init__(self, config: Config):
super(CLIPStochastic, self).__init__()
self.config = config

从 transformers 导入 CLIPModel
如果 config.clip_arch == &#39;ViT-B/32&#39;:
self.clip = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
elif config.clip_arch == &#39;ViT-B/16&#39;:
self.clip = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch16&quot;)
else:
引发 ValueError

self.task_config = config
config.pooling_type = &#39;transformer&#39;
self.pool_frames = Transformer(config)
self.stochastic = StochasticText(config)

def forward(self, data, return_all_frames=False, is_train=True):
batch_size = data[&#39;video&#39;].shape[0]
text_data = data[&#39;text&#39;] # text_data[&quot;input_ids&quot;].shape = torch.Size([16, 17])
video_data = data[&#39;video&#39;] # [16, 12, 3, 224, 224]
video_data = video_data.reshape(-1, 3, self.config.input_res, self.config.input_res) # [192, 3, 224, 224]

if is_train:

text_features = self.clip.get_text_features(**text_data)
video_features = self.clip.get_image_features(video_data)

video_features = video_features.reshape(batch_size, self.config.num_frames, -1) # [bs, #F, 512]

text_features = allgather(text_features,self.task_config)
video_features = allgather(video_features,self.task_config)
torch.distributed.barrier()

video_features_pooled = self.pool_frames(text_features, video_features)

# @WJM：执行随机文本
text_features_stochstic, text_mean, log_var = self.stochastic(text_features, video_features)

if return_all_frames:
return text_features, video_features, video_features_pooled, text_features_stochstic, text_mean, log_var

return text_features, video_features_pooled, text_features_stochstic, text_mean, log_var

else:

text_features = self.clip.get_text_features(**text_data)
video_features = self.clip.get_image_features(video_data)

video_features = video_features.reshape(batch_size, self.config.num_frames, -1)
video_features_pooled = self.pool_frames(text_features, video_features)

# @WJM：文本的重新参数化（独立于文本条件池化）
text_features_stochstic, _, _ = self.stochastic(text_features, video_features)

if return_all_frames:
return text_features, video_features, video_features_pooled, text_features_stochstic

return text_features, video_features_pooled, text_features_stochstic


allgather 函数如下：
class AllGather(torch.autograd.Function):
&quot;&quot;&quot;对张量执行 allgather 的 autograd 函数。&quot;&quot;&quot;

@staticmethod
def forward(ctx, tensor, args):
output = [torch.empty_like(tensor) for _ in range(args.world_size)]
torch.distributed.all_gather(output, tensor)
ctx.rank = local_rank
ctx.batch_size = tensor.shape[0]
return torch.cat(output, dim=0)

@staticmethod
def behind(ctx, grad_output):
local_grad = grad_output[ctx.batch_size * ctx.rank : ctx.batch_size * (ctx.rank + 1)]
return local_grad, None

我尝试在 AllGather 类中向后添加 all_reduce，代码如下，但似乎不起作用，可能是因为 DDP 自带了同步梯度函数？
def behind(ctx, *grads):
all_gradients = torch.stack(grads)
torch.distributed.all_reduce(all_gradients)
return all_gradients[torch.distributed.get_rank()]
]]></description>
      <guid>https://stackoverflow.com/questions/79061290/get-same-loss-but-different-grad-on-multi-gpus-when-using-ddp-distributeddatapa</guid>
      <pubDate>Mon, 07 Oct 2024 09:03:30 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 TensorFlow 训练中的这个问题？未知图像文件格式。需要 JPEG、PNG、GIF、BMP 之一</title>
      <link>https://stackoverflow.com/questions/79060211/how-do-i-fix-this-problem-with-my-tensorflow-training-unknown-image-file-format</link>
      <description><![CDATA[我正在构建一个 U-Net 模型来检测乳腺癌，我从这里获取了数据集：https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset
尽管所有图像都是 png 格式，但在尝试训练我的模型时，会出现错误，指出我的图像格式不正确。
错误如下：
---------------------------------------------------------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
Cell In[51]，第 7 行
5 train_dataset = image_ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
6 print(image_ds.element_spec)
----&gt; 7 model_history = unet.fit(train_dataset, epochs=EPOCHS)

文件 ~\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 最后：
124 delfiltered_tb

文件 ~\anaconda3\Lib\site-packages\tensorflow\python\eager\execute.py:53，在 quick_execute(op_name, num_outputs, input, attrs, ctx, name) 中
51 尝试：
52 ctx.ensure_initialized()
---&gt; 53 张量 = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
54 输入、属性、输出)
55 除 core._NotOkStatusException 外，因为 e:
56 如果名称不为 None:

InvalidArgumentError：图形执行错误：

在节点处检测到，decode_image/DecodeImage 定义在（最近一次调用最后一次）：
&lt;堆栈跟踪不可用&gt;
传递给 MapDataset:3 转换的用户定义函数中的错误，迭代器：Iterator::Root::Prefetch::BatchV2::Shuffle::MemoryCacheImpl::Filter::ParallelMapV2：未知图像文件格式。需要 JPEG、PNG、GIF、BMP 之一。
[[{{node decrypt_image/DecodeImage}}]]
[[IteratorGetNext]] [Op:__inference_one_step_on_iterator_9520]

错误仅在尝试训练模型时发生，它引用此函数：
def preprocess_image(image, mask, target_size=(256, 256)):
try:
# 安全地解码图像和掩码
image = tf.io.decode_image(image, channels=3, expand_animations=False)
mask = tf.io.decode_image(mask, channels=1, expand_animations=False)

# 检查未定义或零维度
if image.shape is None or image.shape[0] == 0 or image.shape[1] == 0:
print(f&quot;Error: Image has undefined or zero Dimensions: {image.shape}&quot;)
return None, None

if mask.shape is None or mask.shape[0] == 0 or mask.shape[1] == 0:
print(f&quot;Error: Mask 具有未定义或零维度：{mask.shape}&quot;)
return None, None

# 确保图像恰好有 3 个通道 (RGB)
if image.shape[-1] != 3:
print(f&quot;Error: Image does not have 3 channels (found {image.shape[-1]}).&quot;)
return None, None

# 将图像标准化为范围 [0, 1]
image = tf.image.convert_image_dtype(image, tf.float32)

# 将图像和 mask 的大小调整为目标尺寸 (256*256)
image = tf.image.resize(image, target_size, method=&#39;nearest&#39;)
mask = tf.image.resize(mask, target_size, method=&#39;nearest&#39;)

#将 mask 转换为二进制（0 或 1）格式以用于分类任务
mask = tf.cast(tf.math.reduce_max(mask, axis=-1, keepdims=True) &gt; 0, tf.float32) # 确保二进制 mask

return image, mask

except Exception as e:
print(f&quot;Error during preprocessing: {str(e)}&quot;)
return None, None

# 将预处理函数应用于数据集
image_ds = dataset.map(preprocess_image)

# 过滤掉 preprocess_image 返回的 None 值
image_ds = image_ds.filter(lambda img, mask: img is not None and mask is not None)

我尝试了多种方法尝试使用 chatgpt 修复此问题，但似乎没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/79060211/how-do-i-fix-this-problem-with-my-tensorflow-training-unknown-image-file-format</guid>
      <pubDate>Sun, 06 Oct 2024 22:45:18 GMT</pubDate>
    </item>
    <item>
      <title>LogisticRegression 未返回正确结果</title>
      <link>https://stackoverflow.com/questions/79057824/logisticregression-not-returning-correct-results</link>
      <description><![CDATA[我试图使用基于训练数据的逻辑回归对测试数据中的一系列点进行分类预测。
我得到了一个没有错误的输出，但我被告知结果是错误的（错误的输出将被视为错误，但运行代码时没有错误）。
训练数据是 375 个点中 4 个不同类别的集合，每个点有 3 个变量，因此绘制在 3D 图形上。我在该集合上运行了一个计数程序，发现超过 50% 的点属于第 2 类。我的初步结果是所有测试点都属于第 2 类。我尝试将训练数据排序为几个不同的集合：随机选择 125 个条目（这是测试数据的大小），找到所有类别的最小数量，然后使用每个类别中相同数量的点创建一个训练集。
无排序 = 所有第 2 类
随机排序 = 所有第 2 类
等类数排序 = 给我一个答案，其中点被归类在所有 4 个类别中，但当我将它们插入在线最终测试表格时，我的准确率得分为 26%，这与随机机会相同。所以，我没有正确处理数据，我不确定在哪里。我希望有更多回归分类经验的人能为我指明正确的方向。
编辑：在调用 LogisticRegression 之前，我是否需要重新格式化（转换）train_X、train_y 和 test_X 数组？如果需要，该怎么做？也许我只是给它提供了格式错误的数据。
# 形成表格以推动逻辑回归
train_X = []
train_y = []
for i in range(len(train_table)):
train_X.append(
[train_table.x.iloc[i], train_table.y.iloc[i], train_table.z.iloc[i]]
)
train_y.append(train_table.label.iloc[i])

test_X = []
for k in range(len(test_table)):
test_X.append([test_table.x.iloc[k], test_table.y.iloc[k], test_table.z.iloc[k]])

# 尝试使用和不使用规范化
clf = LogisticRegression().fit(normalize(train_X), train_y)
# clf = LogisticRegression().fit(train_X, train_y)

predict = clf.predict(test_X[:])
prob = clf.predict_proba(test_X[:])

results = pd.DataFrame(
sort_results(test_table, predict, prob),
columns=[&quot;&quot;, &quot;timestamp&quot;, &quot;UTC time&quot;, &quot;label&quot;, &quot;accuracy&quot;],
)

编辑：已解决
因此，我编写了一个比较 LogisticRegression 和 RandomForestClassifier 的过程，并将我的数据推送给它。使用 RandomForestClassifier 的准确性要好得多。
事实证明，LogisticRegression 返回所有 2 并不是错误，只是结果准确度较低。我使用 RandomForestClassifier 重写了程序，并添加了 RandomizedSearchCV，它可以创建多棵树，选择准确率最高的树，并使用它来预测分类。
新的返回值仍然主要是 2，但准确率大大提高。
param_dist = {&#39;n_estimators&#39;: randint(100, 375),
&#39;max_depth&#39;: randint(5, 20)}

rf = RandomForestClassifier()

rand_search = RandomizedSearchCV(rf, param_distributions = param_dist, n_iter=10, cv=5)

rand_search.fit(X, y)

best_rf = rand_search.best_estimator_

print(&#39;Best hyperparameters:&#39;, rand_search.best_params_)

predictions = pd.Series(best_rf.predict(to_pred_covariates))
]]></description>
      <guid>https://stackoverflow.com/questions/79057824/logisticregression-not-returning-correct-results</guid>
      <pubDate>Sat, 05 Oct 2024 19:41:26 GMT</pubDate>
    </item>
    <item>
      <title>Keras MultiHeadAttention 和填充掩码警告</title>
      <link>https://stackoverflow.com/questions/79053957/keras-multiheadattention-and-padding-mask-warnings</link>
      <description><![CDATA[我正在使用 tensorflow/keras 创建转换器模型。第一次使用 tensorflow。
我正在使用 MultiHeadAttention 层。我的输入是零填充的，并且有一个掩码。我收到有关掩码被丢弃的警告。但看起来确实使用了掩码。
这是一段简单的代码来重现它：
import tensorflow as tf

values = tf.expand_dims([1, 2, 3, 0, 0, 0], 0)
x = tf.keras.layers.Embedding(64, 512, mask_zero=True)(values)
attn = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=512)
output,tention_scores = attn(query=x, key=x, value=x, return_attention_scores=True)

print(attention_scores)

输出：
tf.Tensor(
[[[[0.3333333 0.3333318 0.33333483 0. 0. 0. ]
[0.33333272 0.3333335 0.33333382 0. 0. 0. ]
[0.33333284 0.3333336 0.33333352 0. 0. 0. ]
[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]
[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]
[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]]]], shape=(1, 1, 6, 6), dtype=float32)

UserWarning：向层“query”（类型为 EinsumDense）传递了一个带有掩码的输入。但是，此层不支持掩码，因此会破坏掩码信息。下游层将看不到掩码。
UserWarning：向层“key”（类型为 EinsumDense）传递了一个附加了掩码的输入。但是，此层不支持掩码，因此将破坏掩码信息。下游层将看不到掩码。
UserWarning：向层“value”（类型为 EinsumDense）传递了一个附加了掩码的输入。但是，此层不支持掩码，因此将破坏掩码信息。下游层将看不到掩码。

来自 TF 2.10.0 发行说明：
改进了对 tf.keras.layers.MultiHeadAttention 的掩码支持。

查询、键和值输入的隐式掩码将自动用于计算层的正确注意掩码。这些填充掩码将与调用层时直接传入的任何注意掩码相结合。这可以与 mask_zero=True 的 tf.keras.layers.Embedding 一起使用，以自动推断正确的填充掩码。
向层添加了 use_causal_mask 调用时间参数。传递 use_causal_mask=True 将计算因果注意力掩码，并可选择将其与调用层时直接传入的任何 Attention_mask 相结合。

所以我的理解是 tf.keras.layers.MultiHeadAttention 支持此掩码，看起来确实如此。但我仍然收到这些警告。忽略警告安全吗？如果是这样，有没有好的方法可以抑制这些警告？]]></description>
      <guid>https://stackoverflow.com/questions/79053957/keras-multiheadattention-and-padding-mask-warnings</guid>
      <pubDate>Fri, 04 Oct 2024 11:01:16 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中多元时间序列预测的 LSTM 模型中的验证损失和早期停止</title>
      <link>https://stackoverflow.com/questions/76345515/validation-loss-and-early-stopping-in-an-lstm-model-for-multivariate-time-series</link>
      <description><![CDATA[我正在尝试训练一个 LSTM 模型来预测油价，并遵循一些教程。
我的数据集：



日期
美元指数
油价




2019 年 10 月 12 日
50
66


2019 年 10 月 13 日
51
60



其中油价是目标列。
序列大小 = 7，输出 = 1。
我无法添加验证数据并打印除训练和测试损失之外的验证损失。
这是我的代码和尝试：
 #split 为训练、验证和测试（数据集大小为 2380。因此 150 用于测试，100 用于验证，其余用于训练）
X_train = X_seq[:-150]
y_train = y_seq[:-150]
X_test = X_seq[-150:]
y_test = y_seq[-150:] 
X_val = X_train[-100:]
y_val = y_train [-100:]
X_train= X_train [:-100]
y_train = y_train[:-100]

LSTM 模型
class LSTM(nn.Module):
def __init__(self, num_classes, input_size, hidden_​​size, num_layers):
super().__init__()
self.num_classes = num_classes # 输出大小
self.num_layers = num_layers # lstm 中的循环层数量
self.input_size = input_size # 输入大小
self.hidden_​​size = hidden_​​size # 每个 lstm 层中的神经元
# LSTM 模型
self.lstm = nn.LSTM(input_size=input_size, hidden_​​size=hidden_​​size, 
num_layers=num_layers, batch_first=True, dropout=0.2) # lstm
self.fc_1 = nn.Linear(hidden_​​size, 128) # 完全连接
self.fc_2 = nn.Linear(128, num_classes) # 完全连接最后一层
self.relu = nn.ReLU()

def forward(self,x):
# 隐藏状态
h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_​​size))
# 单元状态
c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_​​size))
# 通过 LSTM 传播输入
output, (hn, cn) = self.lstm(x, (h_0, c_0)) # (输入、隐藏和内部状态)
hn = hn.view(-1, self.hidden_​​size) # 重塑密集层的数据
out = self.relu(hn)
out = self.fc_1(out) # 第一个密集
out = self.relu(out) # relu
out = self.fc_2(out) # 最终输出
return out

这是循环：
def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test,
X_val , y_val,):
for epoch in range(n_epochs):
lstm.train()
output = lstm.forward(X_train) # 前向传递
optimiser.zero_grad() # 计算梯度，手动设置为 0
# 获取损失函数
loss = loss_fn(outputs, y_train)
#val_loss = loss_fn(y_val, y_test).item()
#####
###
loss.backward() # 计算损失函数的损失
optimiser.step() # 从损失改进，即反向传播测试损失
lstm.eval()
test_preds = lstm(X_test) 
test_loss = loss_fn(test_preds, y_test)
if epoch % 100 == 0:
print(&quot;Epoch: %d, 训练损失: %1.5f, 测试损失: %1.5f&quot; % (epoch, 
loss.item(), 
test_loss.item()))

这是我对模型的调用方式：
n_epochs = 1000 
learning_rate = 0.001 

input_size = 3 # 特征数量
hidden_​​size = 2 # 隐藏状态的特征数量
num_layers = 1 # 堆叠的 lstm 层数
num_classes = 1 # 输出类数

lstm = LSTM(num_classes, input_size, hidden_​​size, num_layers)

loss_fn = torch.nn.MSELoss() # 回归的均方误差
optimiser = torch.optim.Adam(lstm.parameters(), lr=learning_rate)

training_loop(n_epochs=n_epochs,lstm=lstm, optimiser=optimiser, loss_fn=loss_fn, X_train=X_train_tensors,y_train=y_train_tensors, X_test=X_test_tensors, y_test=y_test_tensors, X_val=X_val_tensors,y_val=y_val_tensors) 


如何通过训练时要考虑的验证集并计算验证损失并在此基础上进行提前停止？]]></description>
      <guid>https://stackoverflow.com/questions/76345515/validation-loss-and-early-stopping-in-an-lstm-model-for-multivariate-time-series</guid>
      <pubDate>Sat, 27 May 2023 05:51:12 GMT</pubDate>
    </item>
    <item>
      <title>如何格式化时间序列数据以用于 PyTorch LSTM 分类？[关闭]</title>
      <link>https://stackoverflow.com/questions/76321333/how-can-i-format-my-time-series-data-for-pytorch-lstm-classification</link>
      <description><![CDATA[如何预处理时间序列数据以解决分类问题并将其输入 PyTorch LSTM 模型？我有如下图所示的数据集。

此处，event_type 是目标列，这是一个二元分类问题。我想使用 LSTM 训练此数据集。]]></description>
      <guid>https://stackoverflow.com/questions/76321333/how-can-i-format-my-time-series-data-for-pytorch-lstm-classification</guid>
      <pubDate>Wed, 24 May 2023 08:05:02 GMT</pubDate>
    </item>
    <item>
      <title>如何将 Stanza 导出为 ONNX 格式？</title>
      <link>https://stackoverflow.com/questions/70205743/how-to-export-stanza-to-onnx-format</link>
      <description><![CDATA[如何将 Stanza 导出为 ONNX 格式？
似乎不可能只是简单地训练模型。]]></description>
      <guid>https://stackoverflow.com/questions/70205743/how-to-export-stanza-to-onnx-format</guid>
      <pubDate>Thu, 02 Dec 2021 19:59:27 GMT</pubDate>
    </item>
    <item>
      <title>从稀疏的三维点云中检测静态和动态行人</title>
      <link>https://stackoverflow.com/questions/69600975/detection-of-static-and-dynamic-pedestrians-from-sparse-3d-point-clouds</link>
      <description><![CDATA[我想使用点云来检测静态和动态的人。但是，我使用VLP-16，很明显点云在垂直方向上会非常稀疏。
我研究过相关文献，例如使用深度学习（PointNet，PointPillar，SECOND，PointRCNN ...）或机器学习（SVM），但大多数方法都不符合我的需求。
是否有一些方法可以满足我的需求（仅使用激光雷达检测静态和动态的人）？
如果有，您能告诉我如何实现吗？]]></description>
      <guid>https://stackoverflow.com/questions/69600975/detection-of-static-and-dynamic-pedestrians-from-sparse-3d-point-clouds</guid>
      <pubDate>Sun, 17 Oct 2021 02:51:17 GMT</pubDate>
    </item>
    <item>
      <title>如何建立随机森林和粒子群优化器的混合模型来寻找产品的最优折扣？</title>
      <link>https://stackoverflow.com/questions/63413064/how-to-build-hybrid-model-of-random-forest-and-particle-swarm-optimizer-to-find</link>
      <description><![CDATA[我需要找到每种产品（例如 A、B、C）的最佳折扣，以便最大化总销售额。我为每种产品都建立了随机森林模型，将折扣和季节与销售额进行映射。我如何组合这些模型并将它们提供给优化器以找到每个产品的最佳折扣？
选择模型的原因：

RF：它能够提供更好的（相对于线性模型）预测因子和响应（sales_uplift_norm）之间的关系。
PSO：在许多白皮书中都有建议（可在researchgate/IEEE 获得），也可以在此处和此处的python 包中找到。

输入数据：样本数据用于在产品级别构建模型。数据一览如下：

我的想法/步骤：

针对每个产品构建 RF 模型
 # 预处理数据
products_pre_processed_data = {key:pre_process_data(df, key) for key, df in df_basepack_dict.items()}
# rf 模型
products_rf_model = {key:rf_fit(df) for key, df in products_pre_processed_data .items()}




将模型传递给优化器

目标函数：最大化sales_uplift_norm（RF 模型的响应变量）
约束：

总支出（A + B + C 的支出&lt;= 20），支出 = 产品总销售量 * 折扣百分比 * 产品 mrp_of_products
产品下限（A、B、C）：[0.0, 0.0, 0.0] # 折扣百分比下限
产品上限（A、B、C）：[0.3, 0.4, 0.4] # 折扣百分比上限




sudo/sample code # 因为我无法找到将 product_models 传递到优化器。
从 pyswarm 导入 pso
def obj(x):
model1 = products_rf_model.get(&#39;A&#39;)
model2 = products_rf_model.get(&#39;B&#39;)
model3 = products_rf_model.get(&#39;C&#39;)
return -(model1 + model2 + model3) # -ve 符号表示最大化

def con(x):
x1 = x[0]
x2 = x[1]
x3 = x[2]
return np.sum(units_A*x*mrp_A + unit_B*x*mrp_B + unit_C* x *spend_C)-20 # 支出预算

lb = [0.0, 0.0, 0.0]
ub = [0.3, 0.4, 0.4]

xopt, fopt = pso(obj, lb, ub, f_ieqcons=con)

如何将 PSO 优化器（如果我没有遵循正确的优化器，可以使用任何其他优化器）与 RF 一起使用？
添加用于模型的函数：
def pre_process_data(df,product):
data = df.copy().reset_index()
# print(data)
bp = product
print(&quot;-------product: {}-------&quot;.format(bp))
# 预处理步骤
print(&quot;pre process df.shape {}&quot;.format(df.shape))
#1. 响应变量转换
response = data.sales_uplift_norm # 已转换

#2.预测器数值变量转换 
numeric_vars = [&#39;discount_percentage&#39;] # 可能包括 mrp、深度
df_numeric = data[numeric_vars]
df_norm = df_numeric.apply(lambda x: scale(x), axis = 0) # 中心和尺度

#3. char 字段 dummification
#选择类别字段
cat_cols = data.select_dtypes(&#39;category&#39;).columns
#选择字符串字段
str_to_cat_cols = data.drop([&#39;product&#39;], axis = 1).select_dtypes(&#39;object&#39;).astype(&#39;category&#39;).columns
# 合并所有分类字段
all_cat_cols = [*cat_cols,*str_to_cat_cols]
# print(all_cat_cols)

#将 cat 转换为 dummies
df_dummies = pd.get_dummies(data[all_cat_cols])

#4.将 num 和 char df 组合在一起
df_combined = pd.concat([df_dummies.reset_index(drop=True), df_norm.reset_index(drop=True)], axis=1)

df_combined[&#39;sales_uplift_norm&#39;] = response
df_processed = df_combined.copy()
print(&quot;post process df.shape {}&quot;.format(df_processed.shape))
# print(&quot;model fields: {}&quot;.format(df_processed.columns))
return(df_processed)

def rf_fit(df, random_state = 12):

train_features = df.drop(&#39;sales_uplift_norm&#39;, axis = 1)
train_labels = df[&#39;sales_uplift_norm&#39;]

#随机森林回归器
rf = RandomForestRegressor(n_estimators = 500,
random_state = random_state,
bootstrap = True,
oob_score=True)
# RF 模型
rf_fit = rf.fit(train_features, train_labels)

return(rf_fit)
]]></description>
      <guid>https://stackoverflow.com/questions/63413064/how-to-build-hybrid-model-of-random-forest-and-particle-swarm-optimizer-to-find</guid>
      <pubDate>Fri, 14 Aug 2020 12:47:25 GMT</pubDate>
    </item>
    </channel>
</rss>