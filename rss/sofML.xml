<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 15 Nov 2024 03:29:22 GMT</lastBuildDate>
    <item>
      <title>如何使用 HMM 模型对某些动作进行门预测？</title>
      <link>https://stackoverflow.com/questions/79190691/how-to-make-gate-predictions-for-certain-movements-using-hmm-model</link>
      <description><![CDATA[我正在开展一个项目，我想使用 IMU 传感器来预测用户接下来要进入哪个门（用于行走、跑步、跳跃等）。根据我的研究，HMM 似乎是状态预测的最佳机器学习模型。但我在网上看到的所有模型都是基于预先记录的静态数据，而不是正在记录的实时数据。我应该如何实现这一点？
我已经实现了一个滚动窗口，它使用捕获的最后 30 条数据记录进行下一次预测，并对它们进行了规范化。
我已经确定了我想要识别的每个动作的门，并且我已经设置了一个分类算法，该算法遍历每个窗口并使用它来对当前动作进行分类。
我打算用它们来验证我将要做出的预测（或者甚至在需要时将其用作输入来预测人的下一个动作）。]]></description>
      <guid>https://stackoverflow.com/questions/79190691/how-to-make-gate-predictions-for-certain-movements-using-hmm-model</guid>
      <pubDate>Thu, 14 Nov 2024 23:15:03 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 提前停止轮次</title>
      <link>https://stackoverflow.com/questions/79189607/xgboost-early-stopping-rounds</link>
      <description><![CDATA[下面的代码一直在崩溃，我不知道发生了什么
import optuna
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 假设 `X` 和 `y` 是你的特征矩阵和目标数组
X_train, X_valid, y_train, y_valid = train_test_split(df_combined, y, test_size=0.2, random_state=42)

# 为 Optuna 定义目标函数
def objective(trial):
# 为超参数建议值
params = {
&quot;objective&quot;: &quot;reg:squarederror&quot;,
&quot;eval_metric&quot;: &quot;rmse&quot;,
&quot;tree_method&quot;: &quot;hist&quot;, # 使用 hist 方法
&quot;device&quot;: &quot;cuda&quot;, # 指定使用 GPU
&quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 0.01, 0.3, log=True),
&quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 3, 10),
&quot;min_child_weight&quot;: trial.suggest_float(&quot;min_child_weight&quot;, 1, 10),
&quot;gamma&quot;: trial.suggest_float(&quot;gamma&quot;, 0, 1),
&quot;subsample&quot;: trial.suggest_float(&quot;subsample&quot;, 0.5, 1.0),
&quot;colsample_bytree&quot;: trial.suggest_float(&quot;colsample_bytree&quot;, 0.5, 1.0),
&quot;lambda&quot;: trial.suggest_float(&quot;lambda&quot;, 1e-3, 10.0, log=True),
&quot;alpha&quot;: trial.suggest_float(&quot;alpha&quot;, 1e-3, 10.0, log=True),
&quot;n_estimators&quot;: 1000 # 在模型初始化中定义 n_estimators
}

# 初始化模型
model = xgb.XGBRegressor(**params)

# 使用早期停止回调训练模型
model.fit(
X_train,
y_train,
eval_set=[(X_valid, y_valid)],
verbose=False,
early_stopping_rounds=50 # 如果之后没有改进则停止50 轮
)

# 预测并计算验证集的 RMSE
preds = model.predict(X_valid)
rmse = mean_squared_error(y_valid, preds, squared=False)

return rmse # Optuna 将其最小化

# 设置 Optuna 研究
study = optuna.create_study(direction=&quot;minimize&quot;)

# 优化超参数
study.optimize(objective, n_trials=100, n_jobs=40) # 100 次试验，40 次并行作业

# 显示最佳试验
print(&quot;最佳试验：&quot;)
trial = study.best_trial
print(f&quot;值 (RMSE)：{trial.value}&quot;)
print(&quot; Params: &quot;)
for key, value in trial.params.items():
print(f&quot; {key}: {value}&quot;)

我得到
TypeError: XGBModel.fit() 得到一个意外的关键字参数 &#39;early_stopping_rounds&#39;

我已更新所有内容以确保我拥有所有更新的库。
提前停止轮次是正确的（我认为），但由于某种原因，它只是爆炸了。]]></description>
      <guid>https://stackoverflow.com/questions/79189607/xgboost-early-stopping-rounds</guid>
      <pubDate>Thu, 14 Nov 2024 16:14:40 GMT</pubDate>
    </item>
    <item>
      <title>我在 SVR 建模中做错了什么？</title>
      <link>https://stackoverflow.com/questions/79188969/what-have-i-done-wrong-in-my-svr-modeling</link>
      <description><![CDATA[我正在尝试使用 SVR 模型预测风速，因为我的数据集都是连续数字。我按照惯例对数据进行训练测试分割，并使用 minmax 缩放器来规范化我的数据集。
# 训练 - 测试分割
X_train , X_test , y_train , y_test = train_test_split (X , y , test_size =0.3 , random_state =42)

# 规范化数据 - MinMax 缩放器
scaler = MinMaxScaler ()
df_normalized = scaler.fit_transform(df)
df_normalized = pd.DataFrame(df_normalized, columns=df.columns)
X_train = scaler.fit_transform ( X_train )
X_test = scaler.transform ( X_test )

# 支持向量回归 (SVR) 模型
svr_model = SVR(kernel=&#39;rbf&#39;)
svr_model.fit(X_train, y_train)

# 预测和评估 SVR 结果
y_pred = svr_model.predict(X_test)

# 可视化 SVR 结果 
plt.scatter(X_train, y_train, color = &#39;magenta&#39;, label = &#39;实际数据&#39;)
plt.plot(X_test, y_pred, color = &#39;blue&#39;, label = &#39;SVR 预测&#39;)
plt.title(&quot;SVR - 风速预测&quot;)
plt.xlabel(&#39;位置级别&#39;)
plt.ylabel(&#39;风速&#39;)
plt.legend()
plt.show()

plt.scatter(X_train, y_train, color = &#39;magenta&#39;, label = &#39;实际数据&#39;)
引发 ValueError(&quot;x 和 y 必须大小相同&quot;)
ValueError：x 和 y 必须大小相同

我尝试使用 X_grid 和 Y_grid，但问题相同。有人能帮我解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79188969/what-have-i-done-wrong-in-my-svr-modeling</guid>
      <pubDate>Thu, 14 Nov 2024 13:30:38 GMT</pubDate>
    </item>
    <item>
      <title>分割任务中神经网络中编码器层和解码器层的不一致</title>
      <link>https://stackoverflow.com/questions/79188395/inconsistency-of-encoder-and-decoder-layers-in-a-neural-network-for-a-segmentati</link>
      <description><![CDATA[在我的神经网络中，瓶颈层之后，它拒绝接受该值并给出错误：
无效的 output_size &#39;torch.Size(\[16, 16\])&#39;（dim 0 必须介于 16 和 18 之间）。

这是我的模型的代码：
class SegNet(nn.Module):
def __init__(self):
super().__init__()

self.enc_conv0 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=2)
self.enc_conv1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=2)
self.enc_conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=2)
self.enc_conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=2)

self.pool0 = nn.MaxPool2d(kernel_size=2, stride=1, return_indices=True) # 256 -&gt; 128
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1, return_indices=True) # 128 -&gt; 64
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=1, return_indices=True) # 64 -&gt; 32
self.pool3 = nn.MaxPool2d(kernel_size=2, stride=1, return_indices=True) # 32 -&gt; 16

# 瓶颈
self.bottleneck_conv = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, stride=1)

# 解码器（上采样）

self.dec_conv0 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=2)
self.dec_conv1 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=2)
self.dec_conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=2)
self.dec_conv3 = nn.Conv2d(in_channels=32, out_channels=2, kernel_size=3, stride=2, padding=2)

self.upsample0 = nn.MaxUnpool2d(kernel_size=2, stride=1) # 16 -&gt; 32
self.upsample1 = nn.MaxUnpool2d(kernel_size=2, stride=1) # 32 -&gt; 64
self.upsample2 = nn.MaxUnpool2d(kernel_size=2, stride=1) # 64 -&gt; 128
self.upsample3 = nn.MaxUnpool2d(kernel_size=2, stride=1) # 128 -&gt; 256

def forward(self, x):
# 编码器
print(f&#39;x = {x.size()}&#39;)
e0, indices0 = self.pool0(F.relu(self.enc_conv0(x)))
print(f&#39;e0 = {e0.size()}&#39;)
e1, indices1 = self.pool1(F.relu(self.enc_conv1(e0)))
print(f&#39;e1 = {e1.size()}&#39;)
e2, indices2 = self.pool2(F.relu(self.enc_conv2(e1)))
print(f&#39;e2 = {e2.size()}&#39;)
e3, indices3 = self.pool3(F.relu(self.enc_conv3(e2)))
print(f&#39;e3 = {e3.size()}&#39;)

# 瓶颈
b = self.bottleneck_conv(e3)
print(b.size())

# 解码器
d0 = F.relu(self.dec_conv0(self.upsample0(b, indices3, output_size=e3.size()))) #self.upsample0(F.relu(self.dec_conv0(b)), indices3)
print(f&#39;d0 = {d0.size()}&#39;)
d1 = F.relu(self.dec_conv1(self.upsample1(d0, indices2, output_size=e2.size())))
print(f&#39;d1 = {d1.size()}&#39;)
d2 = F.relu(self.dec_conv2(self.upsample2(d1, indices1, output_size=e1.size())))
print(f&#39;d2 = {d2.size()}&#39;)
d3 = self.dec_conv3(self.upsample3(d2, indices0, output_size=e0.size())) # 无激活
print(f&#39;d3 = {d3.size()}&#39;)
return d3

当值不合适时，我已经解决过许多类似的问题，但我不知道如何处理这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79188395/inconsistency-of-encoder-and-decoder-layers-in-a-neural-network-for-a-segmentati</guid>
      <pubDate>Thu, 14 Nov 2024 10:42:35 GMT</pubDate>
    </item>
    <item>
      <title>无法训练我的 UNET 多类别细分模型</title>
      <link>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</link>
      <description><![CDATA[我尝试使用 pytorch 从头开始​​制作 UNET。我的模型输出只有黑色蒙版。我需要分割汽车上的损坏，所以我实现了一个彩色图。我确信 70% 的数据集有问题，而这个彩色图恰恰就是其中的原因。任务是多类预测，所以我使用交叉熵损失函数。我将提供我的数据集和训练文件的代码。
# dataset.py
import os
from PIL import Image
from torch.utils.data import Dataset
import numpy as np
import torch

class Segm_Dataset(Dataset):
def __init__(self, image_dir, mask_dir, color_map):
self.image_dir = image_dir
self.mask_dir = mask_dir
self.image_files = os.listdir(self.image_dir)
self.mask_files = os.listdir(self.mask_dir)
self.color_map = color_map

def __len__(self):
return len(self.image_files)

def __getitem__(self, idx):
image_path = os.path.join(self.image_dir, self.image_files[idx])
mask_path = os.path.join(self.mask_dir, self.mask_files[idx])
image = np.array(Image.open(image_path).convert(&#39;RGB&#39;))
mask = np.array(Image.open(mask_path).convert(&#39;RGB&#39;), dtype=np.float32)
label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int64)

for color, label in self.color_map.items():
color_array = np.array(color, dtype=np.float32)
mask_area = np.all(mask == color_array, axis=-1)
label_mask[mask_area] = label

image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)
label_mask = torch.tensor(label_mask, dtype=torch.long)

返回图像，label_mask

# train.py
从模型导入 UNET
从 tqdm 导入 tqdm
从数据集导入 Segm_Dataset
导入 torch
从 torch.utils.data 导入 DataLoader
导入 torch.nn 作为 nn
导入 torch.optim 作为 optim
导入 os

LEARNING_RATE = 1e-4
BATCH_SIZE = 5
NUM_EPOCHS = 10
NUM_WORKERS = 2
IMAGE_HEIGHT = 180
IMAGE_WIDTH = 180
PIN_MEMORY = True
LOAD_MODEL =错误
TRAIN_IMG_DIR = r&#39;data\train\images&#39;
TRAIN_MASK_DIR = r&#39;data\train\masks&#39;
VAL_IMG_DIR = r&#39;data\val\images&#39;
VAL_MASK_DIR = r&#39;data\val\masks&#39;
SAVED_MODELS_PATH = r&#39;saved_models&#39;

color_map = {
(19, 164, 201): 0, # 缺失部分：#13A4C9
(166, 255, 71): 1, # 破损部分：#A6FF47
(180, 45, 56): 2, # 划痕：#B42D38
(225, 150, 96): 3, # 破裂：#E19660
(144, 60, 89): 4, # 凹痕： #903C59
(167, 116, 27): 5, # 剥落: #A7741B
(180, 14, 19): 6, # 油漆剥落: #B40E13
(115, 194, 206): 7, # 腐蚀: #73C2CE
}

train_dataset = Segm_Dataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, color_map)
train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)

val_dataset = Segm_Dataset(VAL_IMG_DIR, VAL_MASK_DIR, color_map)
val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)

model = UNET(in_channels=3, out_channels=len(color_map))
model = model.cuda() if torch.cuda.is_available() else model

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

for epoch in range(NUM_EPOCHS):
train_loop = tqdm(enumerate(train_loader), total=len(train_loader))

for batch_index, (data, target) in train_loop: 
#前向传递
scores = model(data)
train_loss = criterion(scores, target)

#后向传递
optimizer.zero_grad()
train_loss.backward()

#梯度下降或优化器步骤
optimizer.step()

if batch_index % 10 == 0:
current_batch = batch_index
val_loss = 0
with torch.no_grad():
for val_data, val_targets in val_loader:
val_scores = model(val_data)
val_loss = criterion(val_scores, val_targets)

#更新进度条
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

else:
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

checkpoint = {
&#39;epoch&#39;: epoch + 1,
&#39;model_state_dict&#39;: model.state_dict(),
&#39;optimizer_state_dict&#39;: optimizer.state_dict(),
&#39;train_loss&#39;: train_loss.item(),
&#39;val_loss&#39;: val_loss.item()
}

torch.save(checkpoint, os.path.join(SAVED_MODELS_PATH, f&#39;unet_epoch_{epoch}.pth&#39;))

一些训练 epoches:
Epoch: [9/10]: 100%|████████████████| 888/888 [34:24&lt;00:00, 2.32s/it, train_loss=0.000271, val_batch=880, val_loss=0.000278]

Epoch：[10/10]：100%|███████████████| 888/888 [34:29&lt;00:00, 2.33s/it, train_loss=0.000163, val_batch=880, val_loss=0.000167]
]]></description>
      <guid>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</guid>
      <pubDate>Thu, 14 Nov 2024 09:17:27 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：预期隐藏[0]大小（2，50，1024），在PyTorch中得到[1，50，1024]</title>
      <link>https://stackoverflow.com/questions/79187818/runtimeerror-expected-hidden0-size-2-50-1024-got-1-50-1024-in-pytorc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79187818/runtimeerror-expected-hidden0-size-2-50-1024-got-1-50-1024-in-pytorc</guid>
      <pubDate>Thu, 14 Nov 2024 07:55:37 GMT</pubDate>
    </item>
    <item>
      <title>寻求顶点项目的建议：本地 Raspberry Pi 与使用深度学习的多物联网系统的基于云的解决方案</title>
      <link>https://stackoverflow.com/questions/79186425/seeking-advice-for-capstone-project-local-raspberry-pi-vs-cloud-based-solution</link>
      <description><![CDATA[我们的项目重点是开发一种系统，通过使用传感器、电机、摄像头以及最重要的微处理器控制器，根据参观者的脸部高度调整博物馆中绘画的高度，从而提高行动不便人士的可访问性。我们的目标是升级系统，以便您可以同时管理多幅画作。每幅画都有自己的组件，此外还有一个微处理器控制器，可以识别我们在某个通信协议中与哪个控制器对话。
我们的系统需要深度学习软件来识别坐在轮椅上的人
深度学习软件会很重，需要强大的资源，所以我们正在讨论选择哪种方法，并寻求您的专业建议。
选项是：

在本地运行软件：每个绘画系统将使用 Raspberry Pi 5 ai 套件，该套件的价格高达很多，能够运行深度学习软件。

优点：易于实现，特别是如果您正在处理一幅画。
缺点：当有多个绘图时很复杂，需要在每个控制器中重新安装，维护复杂且成本高。

使用强大的云服务器：用更便宜的控制器替换 Raspberry Pi （esp32），并建立控制器与云之间的通信。

优点：更易于维护、经济、可扩展。
缺点：系统更复杂。
问题：
例如：当 4 个人坐在轮椅上，每个人同时接近博物馆中的不同画作并需要识别时，云是否能承受资源负载？
后续问题，是否需要同步编程，以便每个对云的请求（来自每个绘图的不同访问者的请求）都等待前一个请求的完成？这意味着为每个绘画系统通信的 ML 程序托管 1 个云？
TL;DR：争论使用 Raspberry Pi 为每个绘图提供本地解决方案，这更昂贵但简单，以及使用它的基于云的解决方案，这便宜但复杂。]]></description>
      <guid>https://stackoverflow.com/questions/79186425/seeking-advice-for-capstone-project-local-raspberry-pi-vs-cloud-based-solution</guid>
      <pubDate>Wed, 13 Nov 2024 19:43:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 Apache Spark ML 进行预测</title>
      <link>https://stackoverflow.com/questions/79186067/prediction-with-apache-spark-ml</link>
      <description><![CDATA[我是 Apache Spark ML 的新手。
我想预测按年龄和国家/地区划分的余额。作为输入，我有一个以下格式的 CSV 文件：
RowNumber,Age,Country,Balance

模型已构建，也可以针对测试数据进行训练。到目前为止一切正常。
我现在的问题是当我想对新客户记录进行预测时
Dataset&lt;Row&gt; newCustomer = spark.createDataFrame(Collections.singletonList(
new Customer(28, ‘Germany’)), Customer.class);
Dataset&lt;Row&gt; newCustomerPrediction = model.transform(newCustomer);

我收到以下错误消息：
java.lang.IllegalArgumentException：CountryIndex 不存在。可用：年龄、国家。

如何获取新数据集的预测？
public static void main(String[] args) {

SparkSession spark = SparkSession
.builder()
.master(&quot;local[*]&quot;) 
.appName(&quot;JavaGeneralizedLinearRegressionExample&quot;)
.getOrCreate();

Dataset&lt;Row&gt; data = spark.read()
.option(&quot;header&quot;, &quot;true&quot;)
.option(&quot;inferSchema&quot;, &quot;true&quot;)
.option(&quot;delimiter&quot;, &quot;,&quot;) // oder &quot;,&quot;您可以使用 Dataiformat
.csv(&quot;/data/testdaten_v4.csv&quot;);

StringIndexer countryIndexer = new StringIndexer()
.setInputCol(&quot;Country&quot;)
.setOutputCol(&quot;CountryIndex&quot;)
.setHandleInvalid(&quot;skip&quot;);
OneHotEncoder countryEncoder = new OneHotEncoder()
.setInputCol(&quot;CountryIndex&quot;)
.setOutputCol(&quot;CountryVec&quot;);

VectorAssembler assembler = new VectorAssembler()
.setInputCols(new String[]{&quot;Age&quot;, &quot;CountryVec&quot;}) // 可以添加其他 Features
.setOutputCol(&quot;features&quot;);

StandardScaler scaler = new StandardScaler()
.setInputCol(&quot;features&quot;)
.setOutputCol(&quot;scaledFeatures&quot;);

LinearRegression lr = new LinearRegression()
.setLabelCol(&quot;Balance&quot;)
.setFeaturesCol(&quot;scaledFeatures&quot;)
.setMaxIter(100)
.setRegParam(0.3)
.setElasticNetParam(0.8);

Pipeline pipeline = new Pipeline()
.setStages(new PipelineStage[]{countryIndexer, countryEncoder, assembler, scaler, lr});

PipelineModel model = pipeline.fit(data);

Dataset&lt;Row&gt;[] splits = data.randomSplit(new double[]{0.8, 0.2}, 42);
数据集&lt;Row&gt; trainData = splits[0];
数据集&lt;Row&gt; testData = splits[1];

数据集&lt;Row&gt; predictions = model.transform(testData);
predictions.select(&quot;Age&quot;, &quot;Country&quot;, &quot;Balance&quot;, &quot;prediction&quot;).show();

RegressionEvaluator evaluator = new RegressionEvaluator()
.setLabelCol(&quot;Balance&quot;)
.setPredictionCol(&quot;prediction&quot;)
.setMetricName(&quot;rmse&quot;);
double rmse = evaluator.evaluate(predictions);

数据集&lt;Row&gt; newCustomer = spark.createDataFrame(Collections.singletonList(
new Customer(28, &quot;Germany&quot;)), Customer.class);
Dataset&lt;Row&gt; newCustomerPrediction = model.transform(newCustomer);
newCustomerPrediction.select(&quot;prediction&quot;).show();

spark.stop();
}

public static class Customer {
private int Age;
private String Country;

public Customer(int age, String country) {
this.Age = age;
this.Country = country;
}

public int getAge() { return Age; }
public String getCountry() { return Country; } 
}
]]></description>
      <guid>https://stackoverflow.com/questions/79186067/prediction-with-apache-spark-ml</guid>
      <pubDate>Wed, 13 Nov 2024 17:42:53 GMT</pubDate>
    </item>
    <item>
      <title>使用《Python 深度学习》中的 Keras 进行多类分类的准确率与教科书上的准确率相差甚远</title>
      <link>https://stackoverflow.com/questions/79185545/multiclass-classifier-using-keras-from-deep-learning-with-python-yields-very-d</link>
      <description><![CDATA[以下是 François Chollet 所著《使用 Python 进行深度学习》第 4 章中多类分类器的代码。教科书提到此代码将产生&gt;95% 的训练准确率，但我的环境似乎与教科书相比产生了非常低的准确率&lt;50%。
Keras 版本 - 3.6
Tensorflow - 2.18
硬件 - Apple M1 Pro
import keras
from tensorflow.keras.datasets import reuters
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import layer
import matplotlib.pyplot as plt
import numpy as np

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

def vectorize_sequences(sequences, dimension=10000):
results = np.zeros((len(sequences), dimension))
for i, serial in enumerate(sequences):
for j in series:
results[i, j] = 1.
return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

y_train = to_categorical(train_labels)
y_test = to_categorical(test_labels)

model = keras.Sequential([
layer.Dense(64,activation=&quot;relu&quot;),
layer.Dense(64,activation=&quot;relu&quot;),
layer.Dense(46,activation=&quot;softmax&quot;)
])

model.compile(
optimizer=&quot;rmsprop&quot;,
loss=&quot;categorical_crossentropy&quot;,
metrics=[&quot;accuracy&quot;]
)

# 留出验证集
x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = y_train[:1000]
partial_y_train = y_train[1000:]

# 训练模型

history = model.fit(
partial_x_train,
partial_y_train,
epochs=10,
batch_size=512,
validation_data=(x_val, y_val)
)

# 绘制训练图 &amp;验证准确率
history_dict = history.history
loss_values = history_dict[&quot;loss&quot;]
val_loss_values = history_dict[&quot;val_loss&quot;]
epochs = range(1, len(loss_values) + 1)
acc = history_dict[&quot;accuracy&quot;]
val_acc = history_dict[&quot;val_accuracy&quot;]
plt.plot(epochs, acc, &quot;bo&quot;, label=&quot;Training acc&quot;)
plt.plot(epochs, val_acc, &quot;b&quot;, label=&quot;Validation acc&quot;)
plt.xlabel(&quot;Epochs&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.legend()
plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/79185545/multiclass-classifier-using-keras-from-deep-learning-with-python-yields-very-d</guid>
      <pubDate>Wed, 13 Nov 2024 15:19:50 GMT</pubDate>
    </item>
    <item>
      <title>模型严重偏向少数群体，但准确率、召回率和精确率得分都非常好[关闭]</title>
      <link>https://stackoverflow.com/questions/79184102/model-extremely-biased-towards-the-minority-class-but-the-accuracy-recall-and-pr</link>
      <description><![CDATA[我正在使用 xgboost 分类器来预测下一次事件发生的时间。这是一个二元分类，0:1 的比例是 1.6:1
验证集和验证预测中的类分布相似，性能指标确实很好，但是混淆矩阵显示几乎所有样本都被归入类 1（少数类），而不是 0（多数类），你们知道这是为什么吗？
我尝试处理 scale_pos_weight：

我首先将 scale_pos_weight 提高到 1.6，但它给了我更多类 1 偏斜的结果。
我将 scale_pos_weight 降低到 0.25。对于类别 0，我得到了真阳性结果，并且假阳性明显较少，但对于类别 1，我得到了相等的真阳性和假阳性。它们加起来占样本的约 55%，这意味着模型仍然偏向类别 0。
]]></description>
      <guid>https://stackoverflow.com/questions/79184102/model-extremely-biased-towards-the-minority-class-but-the-accuracy-recall-and-pr</guid>
      <pubDate>Wed, 13 Nov 2024 09:01:14 GMT</pubDate>
    </item>
    <item>
      <title>VSCode 安装 hugginface relik 库时出错</title>
      <link>https://stackoverflow.com/questions/79182549/vscode-install-error-for-the-hugginface-relik-library</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79182549/vscode-install-error-for-the-hugginface-relik-library</guid>
      <pubDate>Tue, 12 Nov 2024 19:53:20 GMT</pubDate>
    </item>
    <item>
      <title>Keras 模型中的自定义编码器和解码器层显示为未构建</title>
      <link>https://stackoverflow.com/questions/79034907/custom-encoder-and-decoder-layers-within-keras-model-show-as-unbuilt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79034907/custom-encoder-and-decoder-layers-within-keras-model-show-as-unbuilt</guid>
      <pubDate>Sat, 28 Sep 2024 18:27:22 GMT</pubDate>
    </item>
    <item>
      <title>尝试保存自定义 Keras 模型时出现“TypeError：不支持的整数大小 (0)”</title>
      <link>https://stackoverflow.com/questions/79032646/typeerror-unsupported-integer-size-0-when-attempted-to-save-custom-keras-mo</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79032646/typeerror-unsupported-integer-size-0-when-attempted-to-save-custom-keras-mo</guid>
      <pubDate>Fri, 27 Sep 2024 19:14:51 GMT</pubDate>
    </item>
    <item>
      <title>如何将多个观测值拟合到单个高斯过程</title>
      <link>https://stackoverflow.com/questions/78554891/how-to-fit-a-multiple-observations-to-single-gaussian-process</link>
      <description><![CDATA[我试图将多个观测值拟合到单个高斯过程。
我尝试像这样拟合两个观测值 (Y) 的数据：
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C

# 示例数据

# 输入数据 X 
X = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]])

# 输出数据 Y 
Y = np.array([[1.5, 2.5], [2.5, 3.5], [3.5, 4.5], [4.5, 5.5], [5.5, 6.5]])
kernel = C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)

# 拟合模型
gp.fit(X, Y)

mean_prediction, cov_prediction = gp.predict(X, return_cov=True)

我得到了两个 mean_prediction 数组和两个 cov_prediction 矩阵。但我想要一个与单个拟合 GP 对应的观测值相同维度的单个均值和协方差矩阵。我该如何实现？]]></description>
      <guid>https://stackoverflow.com/questions/78554891/how-to-fit-a-multiple-observations-to-single-gaussian-process</guid>
      <pubDate>Thu, 30 May 2024 12:16:10 GMT</pubDate>
    </item>
    <item>
      <title>随机森林的蜂群图与 SHAP 值</title>
      <link>https://stackoverflow.com/questions/78037608/bee-swarm-plot-with-shap-values-for-random-forest</link>
      <description><![CDATA[在下面提供的代码中，我将可视化随机森林模型的 SHAP 值结果。
代码在 R 中，如下所示：
# 加载必要的库
library(randomForest)
library(DALEX)
library(beeswarm)

data &lt;- my_database

# 将数据拆分为特征和目标
features &lt;- data[, -which(names(data) %in% &quot;Clus.1&quot;)]
target &lt;- data$Clus.1

# 训练随机森林模型
rf_model &lt;- randomForest(features, target)

# 创建解释器对象
explainer &lt;- DALEX::explain(rf_model, data = features, y = target)

# 计算 SHAP 值
shapley_values &lt;- DALEX::predict_parts(explainer, new_observation = features)

# 绘制蜜蜂群
beeswarm(shapley_values$shap_1)

我尝试使用 beeswarm 包
最后出现了这个错误：
beeswarm(shapley_values$shap_1)
rep(nms, sapply(x, length)) 中的错误：无效的“times”参数

您能否建议我 beeswarm 或其他类似包有什么问题？
我尝试执行的操作的输出
如果我使用 plot(shapley_values)，则这是我得到的输出]]></description>
      <guid>https://stackoverflow.com/questions/78037608/bee-swarm-plot-with-shap-values-for-random-forest</guid>
      <pubDate>Wed, 21 Feb 2024 23:15:30 GMT</pubDate>
    </item>
    </channel>
</rss>