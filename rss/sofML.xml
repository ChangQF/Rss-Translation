<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 21 May 2024 15:15:48 GMT</lastBuildDate>
    <item>
      <title>我应该选择哪种编码方法 OneHotEncoding 或 Ordinal Encoding 或 LabelEncoding [关闭]</title>
      <link>https://stackoverflow.com/questions/78511767/which-encoding-method-should-i-choose-onehotencoding-or-ordinal-encoding-or-labe</link>
      <description><![CDATA[我正在处理 25 个分类列（特征）以将它们转换为数字。对于在 OneHotEncoding 或 OrdinalENcoding 之间选择适当的编码技术感到困惑。选择 OneHotEncoding 将为列中的每个唯一值创建新列并将其添加到 DataFrame 中，并可能导致多重共线性或模型过度拟合问题。
我尝试将 OneHotEncoding 应用于具有 &lt;= 2 个唯一值（二进制类别）的列，并将 OrdinalEncoding 应用于包含 2 个以上唯一值的列。这将有助于在一定程度上保持较低的维度。]]></description>
      <guid>https://stackoverflow.com/questions/78511767/which-encoding-method-should-i-choose-onehotencoding-or-ordinal-encoding-or-labe</guid>
      <pubDate>Tue, 21 May 2024 12:12:26 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 RTX 4090 训练 Mask RCNN？</title>
      <link>https://stackoverflow.com/questions/78511541/is-it-possible-to-train-mask-rcnn-with-a-rtx-4090</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78511541/is-it-possible-to-train-mask-rcnn-with-a-rtx-4090</guid>
      <pubDate>Tue, 21 May 2024 11:27:34 GMT</pubDate>
    </item>
    <item>
      <title>如何从 Hugging 脸上的 Mbart 微调模型中获取 GGUF 文件？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78511521/how-do-you-get-the-gguf-file-from-a-mbart-fine-tuned-model-on-hugging-face</link>
      <description><![CDATA[我已经对 Mbart 进行了微调，以根据英语新闻生成印地语内容，它工作正常，但我想要它是 gguf 文件，我尝试了各种方法来做到这一点，但似乎没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/78511521/how-do-you-get-the-gguf-file-from-a-mbart-fine-tuned-model-on-hugging-face</guid>
      <pubDate>Tue, 21 May 2024 11:22:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 YOLO-v8 TFLite 模型比 Pytorch 模型慢？</title>
      <link>https://stackoverflow.com/questions/78510654/why-is-my-yolo-v8-tflite-model-slower-than-my-pytorch-model</link>
      <description><![CDATA[我最初有一个经过训练的 Pytorch YOLO-v8 nano 模型，用于视频中的多对象检测（10 个类别 -“自行车”、“椅子”、“盒子”、“桌子”、“塑料袋”） ;、“花盆”、“行李箱​​包”、“雨伞”、“购物车”、“人”）。
我使用 ultralytics 库的导出功能将其转换为 TFLite 模型。然而，当我在视频流上运行这两个模型时，我的 TFLite 模型的运行速度（FPS 约为 8）比我的 Pytorch 模型（FPS 约为 20）慢得多。为什么会这样？
TFLite 和 Pytorch 模型均位于：https://drive .google.com/drive/folders/1A2XUD5sV332nXv-Z756Di_QUIZV3ObFv?usp=sharing
从经过训练的 Pytorch 模型到 tflite 模型的转换。
从 ultralytics 导入 YOLO

模型 = YOLO(“yolov8n_trained.pt”)
路径 = model.export(format=&quot;tflite&quot;)

在模型上运行视频流：
从 ultralytics 导入 YOLO
导入CV2
从导入时间开始

# 启动网络摄像头
Stream_url = “视频流路径”

cap = cv2.VideoCapture(stream_url) # 使用 0 表示网络摄像头
上限.设置(3, 640)
上限设置(4, 640)

# 加载重新训练的 YOLOv8 模型
model = YOLO(“yolov8n_trained.tflite”) # 测试 tflite 模型时使用它
# model = YOLO(&quot;yolov8n_trained.pt&quot;) # 测试 pytorch 模型时使用它

# 自定义类名
classNames = [“自行车”,“椅子”,“盒子”,“桌子”,“塑料袋”,“花盆”,
              “行李箱包”、“雨伞”、“购物车”、“人”]

# 初始化变量来计算帧速率
上一个时间 = 0
帧率 = 0

而真实：
    成功，img = cap.read()
    如果没有成功：
        休息

    #计算处理帧所花费的时间
    curr_time = 时间()
    fps = 1 / (当前时间 - 上一个时间)
    上一个时间 = 当前时间

    #在图像上显示帧速率
    cv2.putText(img, f&quot;FPS: {fps:.2f}&quot;, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

    # 在图像上运行 YOLO 模型
    结果=模型（img，流= True）

    # 处理检测结果
    对于结果中的 r：
        对于 r.boxes 中的框：
            # 提取边界框坐标和置信度
            x1, y1, x2, y2 = map(int, box.xyxy[0]) # 转换为整数
            confidence = box.conf[0] # 置信度得分
            class_id = box.cls[0] # 类 ID

            # 绘制边界框
            color = (0, 255, 0) # 边界框的绿色
            cv2.矩形（img，（x1，y1），（x2，y2），颜色，2）

            # 绘制带有类名和置信度的标签
            label = f“{classNames[int(class_id)]}：{置信度：.2f}”
            label_size, base_line = cv2.getTextSize(标签, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            y1 = max(y1, label_size[1])
            cv2.矩形(img, (x1, y1 - label_size[1]), (x1 + label_size[0], y1 + base_line), (0, 255, 0), cv2.FILLED)
            cv2.putText(img, 标签, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)

    # 显示网络摄像头源
    cv2.imshow(&#39;网络摄像头&#39;, img)
    如果 cv2.waitKey(1) == ord(&#39;q&#39;):
        休息

# 释放资源
cap.release()
cv2.destroyAllWindows()

]]></description>
      <guid>https://stackoverflow.com/questions/78510654/why-is-my-yolo-v8-tflite-model-slower-than-my-pytorch-model</guid>
      <pubDate>Tue, 21 May 2024 08:49:32 GMT</pubDate>
    </item>
    <item>
      <title>DecisionTreeRegressor 算法如何工作？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78510567/how-does-the-decisiontreeregressor-algorithm-work</link>
      <description><![CDATA[我是人工智能新手，老实说，我不明白决策树回归算法为何有效。
我尝试研究其背后的算法，但找不到令我满意的答案。
# 导入数组和东西的 numpy 包
将 numpy 导入为 np

# 导入 matplotlib.pyplot 来绘制我们的结果
将 matplotlib.pyplot 导入为 plt

# import pandas 用于导入 csv 文件
将 pandas 导入为 pd

# 导入数据集
# 数据集 = pd.read_csv(&#39;Data.csv&#39;)
# 或者打开.csv文件来读取数据

数据集 = np.array(
[[&#39;资产翻转&#39;, 100, 1000],
[&#39;基于文本&#39;, 500, 3000],
[&#39;视觉小说&#39;, 1500, 5000],
[&#39;2D 像素艺术&#39;, 3500, 8000],
[&#39;2D 矢量艺术&#39;, 5000, 6500],
[&#39;策略&#39;, 6000, 7000],
[&#39;第一人称射击游戏&#39;, 8000, 15000],
[&#39;模拟器&#39;, 9500, 20000],
[&#39;赛车&#39;, 12000, 21000],
[&#39;角色扮演&#39;, 14000, 25000],
[&#39;沙盒&#39;, 15500, 27000],
[&#39;开放世界&#39;, 16500, 30000],
[&#39;MMOFPS&#39;, 25000, 52000],
[&#39;MMORPG&#39;, 30000, 80000]
]）

# 打印数据集
打印（数据集）

# 按 : 选择所有行和第 1 列
# 按1:2表示特征
X = 数据集[:, 1:2].astype(int)

# 打印X
打印（X）

# 按 : 选择所有行和第 2 列
# 由2到Y代表标签
y = 数据集[:, 2].astype(int)

# 打印y
打印（y）

# 导入回归器
从 sklearn.tree 导入 DecisionTreeRegressor

# 创建一个回归器对象
回归器 = DecisionTreeRegressor(random_state = 0)

# 用 X 和 Y 数据拟合回归器
回归器.fit(X, y)

# 预测一个新值

# 通过更改值来测试输出，例如 3750
y_pred = regressor.predict([[3750]]) #输出：8000

# 打印预测价格
print(&quot;预测价格：% d\n&quot;% y_pred)

为什么输出是8000？
我的意思是，我们如何知道这些价格预测函数背后运行的是什么？
它背后的数学公式是什么，我想手动完成，而不使用任何内置函数。
我尝试研究其背后的算法，但找不到令我满意的答案。
我希望得到满意的答复。]]></description>
      <guid>https://stackoverflow.com/questions/78510567/how-does-the-decisiontreeregressor-algorithm-work</guid>
      <pubDate>Tue, 21 May 2024 08:34:32 GMT</pubDate>
    </item>
    <item>
      <title>无法将自参数添加到 catboost 中的自定义指标。无法优化方法“evaluate”，因为使用了 self 参数</title>
      <link>https://stackoverflow.com/questions/78510490/cannot-add-self-arguments-to-custom-metrics-in-catboost-cant-optimze-method-e</link>
      <description><![CDATA[我使用此处的示例。
该示例工作正常，但是，当我尝试通过在其中使用 self 来使自定义指标类变得更加灵活时，我遇到了 UserWarning: Can&#39;t optimze method &quot;evaluate&quot;因为使用了 self 参数
复制问题的代码（如果您希望问题消失，请注释掉 LoglossMetric 的评估方法中的 self.foo = 5 行）
from catboost import CatBoostClassifier、CatBoostRegressor、MultiTargetCustomMetric、MultiTargetCustomObjective
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification、make_regression、make_multilabel_classification
从 sklearn.model_selection 导入 train_test_split
LoglossMetric 类（对象）：
    def get_final_error(自身, 错误, 权重):
        返回误差/(重量+1e-38)

    def is_max_optimal(自身):
        返回错误

    def评估（自我，近似值，目标，重量）：
        自我.foo = 5
        断言 len(大约) == 1
        断言 len(目标) == len(大约[0])

        大约 = 大约[0]

        错误总和 = 0.0
        权重总和 = 0.0

        对于范围内的 i（len（大约））：
            e = np.exp(大约[i])
            p = e / (1 + e)
            w = 1.0 如果权重为 None else 权重[i]
            权重总和 += w
            error_sum += -w * (目标[i] * np.log(p) + (1 - 目标[i]) * np.log(1 - p))

        返回error_sum、weight_sum
X, y = make_classification(n_classes=2, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
model2 = CatBoostClassifier(迭代=10, loss_function=“Logloss”, eval_metric=LoglossMetric(),
                            Learning_rate=0.03，bootstrap_type=&#39;贝叶斯&#39;，boost_from_average=False，
                            leaf_estimation_iterations=1, leaf_estimation_method=&#39;梯度&#39;)
model2.fit(X_train, y_train, eval_set=(X_test, y_test))

如果我在评估方法中使用 self ，也会发生同样的事情

catboost 未能完成哪些优化？原因是什么？
]]></description>
      <guid>https://stackoverflow.com/questions/78510490/cannot-add-self-arguments-to-custom-metrics-in-catboost-cant-optimze-method-e</guid>
      <pubDate>Tue, 21 May 2024 08:17:53 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow PPO 模型未给出模型输出</title>
      <link>https://stackoverflow.com/questions/78510077/tensorflow-ppo-model-not-giving-model-output</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78510077/tensorflow-ppo-model-not-giving-model-output</guid>
      <pubDate>Tue, 21 May 2024 06:49:17 GMT</pubDate>
    </item>
    <item>
      <title>如何修复“ValueError：模型没有从输入中返回损失”？</title>
      <link>https://stackoverflow.com/questions/78510000/how-do-i-fix-valueerror-the-model-did-not-return-a-loss-from-the-inputs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78510000/how-do-i-fix-valueerror-the-model-did-not-return-a-loss-from-the-inputs</guid>
      <pubDate>Tue, 21 May 2024 06:32:28 GMT</pubDate>
    </item>
    <item>
      <title>为什么大型语言模型无法执行基本算术运算的理论？</title>
      <link>https://stackoverflow.com/questions/78509871/theories-for-why-large-language-models-cannot-perform-basic-arithmetic-operation</link>
      <description><![CDATA[我试图理解有哪些理论可以解释为什么基于 Transformer 的语言模型会答错这个问题：
 2571 
77130

171400

251601
|
这个数字不正确，正确答案是 251101
该模型有足够的训练数据来正确排列所有数字（尽管我怀疑这是为了让读者受益）。该模型已经通过具有挑战性的乘法正确计算出了所有这些数字。该模型甚至正确执行了后续进位。
那么为什么它会在那个 token 位置吐出 6 呢？
许多这样的例子出现在一组随机抽样的数字相乘或相加中。语言模型为什么会这样做（当它们在其他语言任务中表现出色时）的主流理论是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78509871/theories-for-why-large-language-models-cannot-perform-basic-arithmetic-operation</guid>
      <pubDate>Tue, 21 May 2024 05:57:14 GMT</pubDate>
    </item>
    <item>
      <title>如何在Unity中使用Python训练的机器学习模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78509446/how-to-using-machine-learning-model-trained-in-python-within-unity</link>
      <description><![CDATA[我有一个任务需要在 Unity 中使用用 Python 训练的机器学习模型。我仍在研究其可行性，并发现可以使用 IronPython，但网上关于此类任务的信息很少，所以我预计我的方法不是主流的。
有人对主流方法有任何经验或其他建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78509446/how-to-using-machine-learning-model-trained-in-python-within-unity</guid>
      <pubDate>Tue, 21 May 2024 02:36:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 预测手机销量 [关闭]</title>
      <link>https://stackoverflow.com/questions/78509245/predicting-phone-sales-with-python</link>
      <description><![CDATA[我正在尝试预测手机的销量，有几个因素。

发布日的销售额可能达到数月至 6 个月的销售额

季节性起着巨大的作用，我有大约 5 年的数据

有时销售会因为手机缺货而停止（但除此之外还有需求）

特定模型的寿命通常较短，因此数据不多，主要使用 2-6 个月的数据


我正在尝试找到一个包含所有 4 点的模型。
你有什么建议？]]></description>
      <guid>https://stackoverflow.com/questions/78509245/predicting-phone-sales-with-python</guid>
      <pubDate>Tue, 21 May 2024 00:45:07 GMT</pubDate>
    </item>
    <item>
      <title>keras 指标和 sklearn 指标之间的差异</title>
      <link>https://stackoverflow.com/questions/78509105/discrepancy-between-keras-metrics-and-sklearn-metrics</link>
      <description><![CDATA[我使用 InceptionV3 训练了一个 cnn 网络模型，对图像进行分类，以检测胸部 X 光片中的肺结核。
问题在于，在训练时，指标似乎进展顺利。但在评估时却发现了很大的差异。
首先使用此方法使用验证数据生成器评估模型。
## 模型评估
损失、准确度、精确度、召回率、auc = model.evaluate(valid_generator)
print(f&#39;损失: {loss}, 准确度: {accuracy}, 精度: { precision}, 召回率: {recall}, AUC: {auc}&#39;)

它给了我以下结果：
38/38 [================================] - 5s 128ms/步 - 损耗：0.1684 - 准确度：0.9339 - 精确度：0.9978 - 召回率：0.8515 - auc：0.9952
损失：0.16840478777885437，准确度：0.9339389204978943，精度：0.9977973699569702，召回率：0.8515037298202515，AUC：0.9952080845832825

还可以通过以下代码使用 sklearn.metrics 评估模型：
从sklearn.metrics导入classification_report，confusion_matrix
将 numpy 导入为 np
导入 sklearn.metrics

##获取验证数据集的预测
valid_generator.reset()
Y_pred = model.predict（valid_generator，steps=len（valid_generator），verbose=1）
Y_pred = np.round(Y_pred)

##将真实标签转换为数组格式
Y_true = valid_generator.classes

print(&quot;分类报告：\n&quot;,classification_report(Y_true, Y_pred))

# 混淆矩阵
conf_mat = 混淆矩阵(Y_true, Y_pred)
print(&quot;混淆矩阵:\n&quot;, conf_mat)

它给了我这个结果：
38/38 [================================] - 5s 130ms/步
分类报告：
               精确召回率 f1-score 支持

           0 0.57 0.64 0.60 679
           1 0.46 0.39 0.42 532

    精度 0.53 1211
   宏观平均 0.51 0.51 0.51 1211
加权平均 0.52 0.53 0.52 1211

混淆矩阵：
 [[432247]
 [325207]]

显然这些不同的评估方法的结果存在差异，哪一个是正确的？
按如下方式编译和训练我的模型：
&lt;前&gt;&lt;代码&gt;#InceptionV3
base_model = InceptionV3(权重=&#39;imagenet&#39;, include_top=False, input_shape=(299, 299, 3))
x = 基础模型.输出
x = GlobalAveragePooling2D()(x)
x = 密集（1024，激活=&#39;relu&#39;）（x）
预测=密集（1，激活=&#39;sigmoid&#39;）（x）

模型 = 模型（输入=base_model.输入，输出=预测）

对于 base_model.layers 中的图层：
    可训练层 = False

model.compile(优化器=Adam(learning_rate=0.0001),
              损失=&#39;binary_crossentropy&#39;,
              指标=[&#39;准确率&#39;, tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])

历史=模型.fit(
    火车发电机，
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    验证数据=有效生成器，
    valid_steps=valid_generator.samples // valid_generator.batch_size,
    纪元=50
）

应该记住，我只对 2 个类别进行分类。

希望您能帮我解决这个疑惑。提前致谢。
能够知道要使用哪些评估指标以及哪些是正确的。]]></description>
      <guid>https://stackoverflow.com/questions/78509105/discrepancy-between-keras-metrics-and-sklearn-metrics</guid>
      <pubDate>Mon, 20 May 2024 23:24:26 GMT</pubDate>
    </item>
    <item>
      <title>有什么方法可以近似特殊条件下的softmax概率吗？</title>
      <link>https://stackoverflow.com/questions/62190052/is-any-method-to-approximate-the-softmax-probability-under-special-conditions</link>
      <description><![CDATA[我正在尝试找到不使用 exp() 来计算 softmax 概率的方法。
假设：
目标：计算 f(x1, x2, x3) = exp(x1)/[exp(x1)+exp(x2)+exp(x3)]

状况：

    1.-64＜ x1,x2,x3＜ 64

    2.结果只保留3位小数。

有没有办法找到一个多项式来近似表示这种条件下的结果？]]></description>
      <guid>https://stackoverflow.com/questions/62190052/is-any-method-to-approximate-the-softmax-probability-under-special-conditions</guid>
      <pubDate>Thu, 04 Jun 2020 08:25:38 GMT</pubDate>
    </item>
    <item>
      <title>loss.backward() 与模型的适当参数有何关系？</title>
      <link>https://stackoverflow.com/questions/58844168/how-does-loss-backward-relate-to-the-appropriate-parameters-of-the-model</link>
      <description><![CDATA[我是 PyTorch 的新手，我无法理解 loss 如何知道通过 loss.backward() 计算梯度？ 
当然，我知道参数需要有 requires_grad=True 并且我知道它将 x.grad 设置为适当的梯度，仅供优化器稍后执行梯度更新。 
优化器在实例化时链接到模型参数，但损失永远不会链接到模型。 
我一直在经历这个帖子，但我认为没有人清楚地回答它，并且发起该帖子的人似乎与我有同样的问题。 
当我有两个不同的网络、两个不同的损失函数和两个不同的优化器时会发生什么。我可以轻松地将优化器链接到每个网络，但是如果我从不将它们链接在一起，损失函数如何知道如何计算每个适当网络的梯度？]]></description>
      <guid>https://stackoverflow.com/questions/58844168/how-does-loss-backward-relate-to-the-appropriate-parameters-of-the-model</guid>
      <pubDate>Wed, 13 Nov 2019 19:23:44 GMT</pubDate>
    </item>
    <item>
      <title>k-mean python 图像分离</title>
      <link>https://stackoverflow.com/questions/51245877/image-segregation-by-k-mean-python</link>
      <description><![CDATA[我是机器学习的新手，我正在学习用于图像分离的 k 均值，但我无法理解它的代码：
from matplotlib.image import imread
image = imread(os.path.join(&quot;images&quot;,&quot;unsupervised_learning&quot;,&quot;ladybug.png&quot;))
图像.形状
X = 图像.reshape(-1, 3)
kmeans = KMeans(n_clusters=8, random_state=42).fit(X)
segmented_img = kmeans.cluster_centers_[kmeans.labels_]
Segmented_img = Segmented_img.reshape(image.shape)
分段的imgs = []
n_颜色 = (10, 8, 6, 4, 2)
对于 n_colors 中的 n_clusters：
    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X)
    segmented_img = kmeans.cluster_centers_[kmeans.labels_]
    segmented_imgs.append(segmented_img.reshape(image.shape))
plt.figure(figsize=(10,5))
plt.subplots_调整（wspace = 0.05，hspace = 0.1）
plt.子图(231)
plt.imshow(图像)
plt.title(&quot;原图&quot;)
plt.axis(&#39;关闭&#39;)
对于 idx，枚举中的 n_clusters(n_colors)：
   plt.子图（232 + idx）
   plt.imshow(segmented_imgs[idx])
   plt.title(&quot;{} 颜色&quot;.format(n_clusters))
   plt.axis(&#39;关闭&#39;)
plt.show()

使用的图像：

输出图

特别是，这段代码的含义
segmented_img = kmeans.cluster_centers_[kmeans.labels_]
]]></description>
      <guid>https://stackoverflow.com/questions/51245877/image-segregation-by-k-mean-python</guid>
      <pubDate>Mon, 09 Jul 2018 12:42:30 GMT</pubDate>
    </item>
    </channel>
</rss>