<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 29 Nov 2023 15:14:55 GMT</lastBuildDate>
    <item>
      <title>提高深度学习模型准确性的建议</title>
      <link>https://stackoverflow.com/questions/77571577/suggestions-to-improve-deep-learning-models-accuracy</link>
      <description><![CDATA[目前我正在研究一个利用深度学习的现实问题。在这个问题中，我有一个总共 74 个物种的各种植物图像的大型数据集，并且我已经将数据集分割成 80:10:10 的大小，用于训练、测试和验证套件。我为此目的使用张量流，但无法提高模型的准确性。
链接
这是代码]]></description>
      <guid>https://stackoverflow.com/questions/77571577/suggestions-to-improve-deep-learning-models-accuracy</guid>
      <pubDate>Wed, 29 Nov 2023 13:31:12 GMT</pubDate>
    </item>
    <item>
      <title>为模型创建输入时，pandas DataFrame 和 keras Concatenate 有什么区别</title>
      <link>https://stackoverflow.com/questions/77571304/what-is-the-difference-between-pandas-dataframe-and-concatenate-of-keras-while-c</link>
      <description><![CDATA[在为模型创建输入时，pandas DataFrame 和 keras 的 Concatenate 有什么区别？
我们可以用两种方法训练模型：
方法 1：
 train_df = pd.DataFrame({ &#39;total_rooms&#39; : train_df[&quot;total_rooms&quot;], &#39;median_venue&#39; :
  train_df[“中位数收入”]，“中位数房屋值”：train_df[“中位数房屋值”]})
  model.compile（优化器=tf.keras.optimizers.experimental.RMSprop（learning_rate=my_learning_rate），损失=tf.keras.losses.BinaryCrossentropy（），
                指标=指标）

历史 = model.fit(x=train_df,
                      y=train_df[“中位数房屋值”],
                      # 详细=&#39;0&#39;,
                      批量大小=批量大小，
                      纪元=纪元，
                      回调=weight_callback，
                      #validation_split=validation_split
                   ）

方法 2：
 # 使用连接层将输入层连接成单个张量。
  # 作为密集层的输入。例如：[input_1[0][0]、input_2[0][0]]
  concatenated_inputs = tf.keras.layers.Concatenate()(my_inputs.values())
  密集=层.密集（单位= 1，名称=&#39;dense_layer&#39;，激活= tf.sigmoid）
  密集输出 = 密集（连接输入）
  “”“创建并编译一个简单的分类模型。”“”
  我的输出 = {
    &#39;密集&#39;：密集输出，
  }
  模型= tf.keras.Model（输入= my_inputs，输出= my_outputs）
  历史= model.fit(x=特征，y=标签，batch_size=batch_size，
                      纪元=纪元，洗牌=洗牌）
]]></description>
      <guid>https://stackoverflow.com/questions/77571304/what-is-the-difference-between-pandas-dataframe-and-concatenate-of-keras-while-c</guid>
      <pubDate>Wed, 29 Nov 2023 12:51:28 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试创建一个视觉问题回答模型，当我尝试进行问题编码时，出现此错误。有谁知道如何解决</title>
      <link>https://stackoverflow.com/questions/77571289/i-am-trying-to-create-a-visual-qustion-anwering-model-and-when-i-try-to-do-the-q</link>
      <description><![CDATA[我的代码
def textokenizer(文本):
  最大长度 = 24
  tok = 分词器()
  tok.fit_on_texts(x_train[&#39;ques&#39;])
  vocab_size = len(tok.word_index) + 1
  #print(&#39;x_train 中的唯一单词总数&#39;,vocab_size)
  编码文本 = tok.texts_to_sequences(文本)
  padded_text = pad_sequences(encoded_text, maxlen=max_length) #在每个问题的开头填充零，以便每个序列具有相同的长度
  #print（填充文本.形状）
  返回填充文本，tok

with open(&#39;glove_vectors&#39;, &#39;rb&#39;) as f:
    模型 = pickle.load(f)
    glove_words = 设置(model.keys())
    
# 火车
_,tok = textokenizer(x_train[&#39;ques&#39;])
vocab_size = len(tok.word_index) + 1
embedding_matrix_train = np.zeros((vocab_size, 300))
对于单词，i 在 tok.word_index.items() 中：
    如果 glove_words 中的单词：
        嵌入向量=模型[词]
        embedding_matrix_train[i] = embedding_vector

错误
UnpicklingError Traceback（最近一次调用最后一次）
&lt;ipython-input-68-e88f73604095&gt;在&lt;细胞系：12&gt;()
     11 #每个 token 使用 300-dim 向量使用预训练的 GloVe 表示来表示。
     12 以 open(&#39;glove_vectors&#39;, &#39;rb&#39;) 作为 f：
---&gt; 13 模型 = pickle.load(f)
     14 glove_words = 设置（model.keys（））
     15

UnpicklingError：无效的加载键，&#39;%&#39;。
]]></description>
      <guid>https://stackoverflow.com/questions/77571289/i-am-trying-to-create-a-visual-qustion-anwering-model-and-when-i-try-to-do-the-q</guid>
      <pubDate>Wed, 29 Nov 2023 12:48:54 GMT</pubDate>
    </item>
    <item>
      <title>与从头开始训练的模型一起使用时，GradCam+ 会给出不正确的结果</title>
      <link>https://stackoverflow.com/questions/77571210/gradcam-gives-incorrect-results-when-using-with-from-scratch-trained-model</link>
      <description><![CDATA[我正在尝试将 grad_cam_plus 的实现用于我的二元分类模型，但没有得到任何结果。我注意到 conv_second_grad conv_third_grad 为零张量，所以我认为这是一个问题。我也尝试过 vgg 但它工作正常。我不确定我做错了什么。
def grad_cam_plus(模型, img,
              layer_name=“conv2d_8”，label_name=无，
              类别 ID=无）：
”“”通过 Grad-CAM++ 获取热图。

参数：
    model：模型对象，从 tf.keras 2.X 构建。
    img：图像 ndarray。
    layer_name：一个字符串，模型中的图层名称。
    label_name：列表或无，
        通过分配此参数来显示标签名称，
        它应该是所有标签名称的列表。
    category_id：一个整数，类别的索引。
        默认是预测中得分最高的类别。

返回：
    热图 ndarray（无颜色）。
”“”
img_tensor = np.expand_dims(img, 轴=0)

conv_layer = model.get_layer(layer_name)
heatmap_model = Model([model.inputs], [conv_layer.output, model.output])

将 tf.GradientTape() 用作 gtape1：
    将 tf.GradientTape() 用作 gtape2：
        将 tf.GradientTape() 用作 gtape3：
            conv_output，预测= heatmap_model（img_tensor）
            如果category_id为None：
                Category_id = np.argmax(预测[0])
            如果 label_name 不是 None：
                打印（标签名称[类别id]）
            输出=预测[:,category_id]
            conv_first_grad = gtape3.gradient(输出, conv_output)
        conv_second_grad = gtape2.gradient(conv_first_grad, conv_output)
        打印（conv_second_grad）
    conv_third_grad = gtape1.gradient(conv_second_grad, conv_output)

global_sum = np.sum(conv_output, axis=(0, 1, 2))

alpha_num = conv_second_grad[0]
alpha_denom = conv_second_grad[0]*2.0 + conv_third_grad[0]*global_sum
alpha_denom = np.where(alpha_denom != 0.0, alpha_denom, 1e-10)

alpha = alpha_num/alpha_denom
alpha_normalization_constant = np.sum(alphas, axis=(0,1))
alphas /= alpha_normalization_constant

权重 = np.maximum(conv_first_grad[0], 0.0)

deep_线性化_权重 = np.sum(权重*alphas, 轴=(0,1))
grad_cam_map = np.sum(deep_线性化_权重*conv_output[0], 轴=2)

热图 = np.maximum(grad_cam_map, 0)
max_heat = np.max(热图)
如果最大热量== 0：
    最大热量 = 1e-10
热图 /= max_heat

返回热图


模型=顺序（[
  图层.重新缩放(1./255, input_shape=(img_height, img_width, 3)),
  层.Conv2D(16, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;),
  层.MaxPooling2D(),
  层.Conv2D(32, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;),
  层.MaxPooling2D(),
  层.Conv2D(64, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;),
  层.MaxPooling2D(),
  层.Flatten(),
  层.Dense(128, 激活=&#39;relu&#39;),
  层数.Dense(num_classes)
]）
]]></description>
      <guid>https://stackoverflow.com/questions/77571210/gradcam-gives-incorrect-results-when-using-with-from-scratch-trained-model</guid>
      <pubDate>Wed, 29 Nov 2023 12:35:34 GMT</pubDate>
    </item>
    <item>
      <title>Llama-2、Q4-量化模型在不同CPU上的响应时间</title>
      <link>https://stackoverflow.com/questions/77570944/llama-2-q4-quantized-models-response-time-on-different-cpus</link>
      <description><![CDATA[我正在此处运行量化的 llama-2 模型。我使用两台不同的机器。

第 11 代英特尔(R) 酷睿(TM) i7-1165G7 @ 2.80GHz 2.80 GHz
16.0 GB（15.8 GB 可用）

这台机器上的推理时间非常好。我在 3-4 分钟内得到了我想要的答复

Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz 2.20 GHz（2 个处理器）
224GB

这台机器上的推理时间很长。大约需要半个小时才能给出不满意的答复。它甚至还有 Nvidia 2080-Ti GPU。 （但不使用它来加载模型的权重。
为什么会出现这种行为？ CPU如何影响性能？
我正在使用 llama_cpp python 包来加载模型。]]></description>
      <guid>https://stackoverflow.com/questions/77570944/llama-2-q4-quantized-models-response-time-on-different-cpus</guid>
      <pubDate>Wed, 29 Nov 2023 11:56:01 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中用 Transformer 替代 LSTM</title>
      <link>https://stackoverflow.com/questions/77570734/replace-lstm-with-transformer-in-neural-network</link>
      <description><![CDATA[我对 Tensorflow 的经验很少，但我正在尝试开发一个现有项目，该项目在汽车行车记录仪视频中实现了事故预测模型（预期事故）。我的目标是用 Transformer 替换原始项目中使用的 LSTM 并比较结果。使用 Tensorflow 2 可行吗？我做了一些研究，但我不确定这种变化会对代码结构和模型逻辑产生多大影响。如果有人有任何建议，我们将非常感谢任何帮助。到目前为止，我刚刚做了一些小的调整，使代码可以与 Tensorflow 2 和 Python 3 一起使用，因为原始代码已经过时了（你可以找到我的分叉存储库 此处）。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77570734/replace-lstm-with-transformer-in-neural-network</guid>
      <pubDate>Wed, 29 Nov 2023 11:26:12 GMT</pubDate>
    </item>
    <item>
      <title>如何根据特征集子集的值不可用来选择性地训练深度模型</title>
      <link>https://stackoverflow.com/questions/77570428/how-to-selectively-train-a-deep-model-based-on-the-unavailability-of-values-for</link>
      <description><![CDATA[我正在创建一个深度学习二元分类模型。数据集中的每个样本都包含两个互斥的特征集X和Y。
特征集 X 存在于所有样本中；然而，总样本中约有 45% 的特征集 Y 的值不可用。 Y 中的特征值本质上是二进制的。
我希望我的模型在推理过程中如果测试样本不包含特征集 Y 的值，则应仅对特征集 X 进行推理。如果 Y 的值可用，则推理应基于 X 和 Y。
我正在使用 PyTorch Lightning 框架进行模型架构设计和开发。
根据我的理解，我可以遵循的一种方法是“填写” Y 中的特征的一些默认值（以防它们不存在）并训练模型。但是，要填充什么值呢？应该是 0 或 1 还是任何其他类似 -1 等
另一种方法可能是创建一个名为 isYPresent 的二进制功能（例如）。如果 Y 不存在则为 0，如果存在则为 1。是否存在一种技术可以根据这个新功能的值有条件地在模型训练期间（以及随后的推理期间）处理这种情况？]]></description>
      <guid>https://stackoverflow.com/questions/77570428/how-to-selectively-train-a-deep-model-based-on-the-unavailability-of-values-for</guid>
      <pubDate>Wed, 29 Nov 2023 10:41:41 GMT</pubDate>
    </item>
    <item>
      <title>xgboost 中的形状值通过 1000 个样本的值误差</title>
      <link>https://stackoverflow.com/questions/77570420/shap-values-in-xgboost-thorughing-value-error-with-1000-samples</link>
      <description><![CDATA[您好，我正在使用下面的代码来生成形状图
defplot_shap(
    n_sample：int =无，
    n_features：int =无，
    model_output：字典=无，
    标题：str = 无
）：
    如果 n_sample 不是 None：
        X_sampled = model_output[“X_test”].sample(n_sample,random_state=10)
        打印（X_sampled.shape）
    别的：
        X_sampled = model_output[“X_test”]
        print(&#39;无:&#39;, X_sampled.shape)
    解释器 = shap.TreeExplainer(model_output[“模型”])
    打印（解释器）
    shap_values =explainer.shap_values(X_sampled, check_additivity=False)
    打印（形状值）
    plt.标题（标题）
    shap.summary_plot(
        形状值，
        X_采样，
        最大显示=20
    ）
    返回 shap_values

使用下面的代码调用此函数时，它的值错误
shap_values =plot_shap(
    n_样本=1000，
    模型输出=模型输出，
    标题=模型输出[&#39;模型名称&#39;]
）

ValueError：此重塑错误通常是由于将错误的数据矩阵传递给 SHAP 引起的。请参阅https://github.com/shap/shap/issues/580。&lt; /p&gt;
打印时在病房上打印（shap_values）失败
(1000, 360) for 语句 print(X_sampled.shape)
 for语句打印（解释器）
原始X_test包含13827行和360列。
情况紧急，请帮忙]]></description>
      <guid>https://stackoverflow.com/questions/77570420/shap-values-in-xgboost-thorughing-value-error-with-1000-samples</guid>
      <pubDate>Wed, 29 Nov 2023 10:41:00 GMT</pubDate>
    </item>
    <item>
      <title>在 kfold 交叉验证中选择适当的分割</title>
      <link>https://stackoverflow.com/questions/77570349/selecting-appropriate-splits-in-kfold-cross-validation</link>
      <description><![CDATA[我正在调整基于自行车租赁数据训练的 DecisionTreeRegressor。我的目的是学习超参数调整。
我的问题是为什么 kfold 中不同数量的分割会产生如此不同的结果，哪个结果更好以及如何决定适当的分割/折叠数量。
这是我创建的函数：
defune(param_grid, reg=DecisionTreeRegressor(random_state=2), cv=5):
    网格 = skm.GridSearchCV(reg, cv=cv, 评分=“neg_mean_squared_error”, n_jobs=-1, param_grid=param_grid)
    grid.fit(X_train, y_train)
    print(&quot;最佳参数：&quot;, grid.best_params_)

    print(&quot;CV RMSE:&quot;, np.sqrt(-grid.best_score_))

    y_pred = grid.predict(X_train)
    print(&quot;训练 RMSE:&quot;, np.sqrt(skmt.mean_squared_error(y_train, y_pred)))

    y_pred = grid.predict(X_test)
    print(&quot;测试 RMSE:&quot;, np.sqrt(skmt.mean_squared_error(y_test, y_pred)))

使用 cv=5：
tune({“min_samples_leaf”: [1,2,3,4,6,8,10,20],
      “最大深度”：[2,3,4,6,8,10,20,无]}，cv=5)

输出：
最佳参数：{&#39;max_depth&#39;: 6, &#39;min_samples_leaf&#39;: 2}
CV RMSE：870.3962060281716
训练均方根误差：537.6546032752227
测试均方根误差：912.9995795416623

使用 cv=10：
tune({“min_samples_leaf”: [1,2,3,4,6,8,10,20],
      “最大深度”：[2,3,4,6,8,10,20,无]}，cv=10)

输出：
最佳参数：{&#39;max_depth&#39;: 20, &#39;min_samples_leaf&#39;: 4}
CV RMSE：838.0643420317033
训练均方根误差：449.2062167577294
测试均方根误差：881.9551697123571
]]></description>
      <guid>https://stackoverflow.com/questions/77570349/selecting-appropriate-splits-in-kfold-cross-validation</guid>
      <pubDate>Wed, 29 Nov 2023 10:29:55 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的线性回归实现不起作用？</title>
      <link>https://stackoverflow.com/questions/77569740/why-is-my-implementation-of-linear-regression-not-working</link>
      <description><![CDATA[我正在尝试在 python 中从头开始实现线性回归。
作为参考，以下是我使用过的数学公式：方程
这是我尝试过的：
类线性回归：
    
    def __init__(
    自己，
    特征：np.ndarray[np.float64]，
    目标：np.ndarray[np.float64]，
    ）-&gt;没有任何：
        self.features = np.concatenate((np.ones((features.shape[0], 1)), features), axis=1)
        self.targets = 目标
        self.params = np.random.randn(features.shape[1] + 1)
        self.num_samples = features.shape[0]
        self.num_feats = features.shape[1]
        自我成本 = []
    
    def假设（自我）-&gt; np.ndarray[np.float64]：
        返回 np.dot(self.features, self.params)
    
    def cost_function(self) -&gt;; def cost_function(self) -&gt; np.float64：
        pred_vals = self.hypothesis()
        return (1 / (2 * self.num_samples)) * np.dot((pred_vals - self.targets).T, pred_vals - self.targets)
    
    def update(self, alpha: np.float64) -&gt;;没有任何：
        self.params = self.params - (alpha / self.num_samples) * (self.features.T @ (self.hypothesis() - self.targets))
    
    defgradientDescent(self, alpha: np.float64, 阈值: np.float64, max_iter: int) -&gt;没有任何：
        收敛=假
        计数器 = 0
        未收敛时：
            计数器 += 1
            curr_cost = self.cost_function()
            self.costs.append(curr_cost)
            自我更新（阿尔法）
            new_cost = self.cost_function()
            如果abs(new_cost - curr_cost) &lt;临界点：
                收敛=真
            如果计数器&gt;最大迭代次数：
                收敛=真

我使用了这样的类：
regr = LinearRegression(features=np.linspace(0, 1000, 200, dtype=np.float64).reshape((20, 10)), 目标= np.linspace(0, 200, 20, dtype=np.float64))
regr.gradientDescent(0.1, 1e-3, 1e+3)
regr.cost_function()

但是，我收到以下错误：
RuntimeWarning：标量幂中遇到溢出
  return (1 / (2 * self.num_samples)) * (la.norm(self.hypothesis() - self.targets) ** 4)

RuntimeWarning：标量减法中遇到无效值
  如果abs(new_cost - curr_cost) &lt;临界点：

RuntimeWarning：matmul 中遇到溢出
  self.params = self.params - (alpha / self.num_samples) * (self.features.T @ (self.hypothesis() - self.targets))

任何人都可以帮助我了解到底出了什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77569740/why-is-my-implementation-of-linear-regression-not-working</guid>
      <pubDate>Wed, 29 Nov 2023 08:54:58 GMT</pubDate>
    </item>
    <item>
      <title>tf2onnx.convert 没有 from_saved_model 属性</title>
      <link>https://stackoverflow.com/questions/77569503/tf2onnx-convert-has-no-attribute-from-saved-model</link>
      <description><![CDATA[我正在尝试将我的自定义对象检测模型 .pb 转换为 .onnx 模型
这是代码片段
&lt;前&gt;&lt;代码&gt;导入 tf2onnx

# 替换为你的路径
已保存模型路径 = &#39;/path/to/saved_model&#39;
onnx_export_path = &#39;/path/to/model.onnx&#39;

# 将 SavedModel 转换为 ONNX
tf2onnx.convert.from_saved_model（saved_model_path，output_path = onnx_export_path）

系统信息
操作系统平台和发行版：WINDOW
TensorFlow版本：2.15.1
Python版本：3.9.12
ONNX 版本（如果适用，例如 1.1.）：1.15.1
ONNXRuntime 版本（如果适用，例如 1.11）：1.16.2
重现
我读了库 tf2onnx.convert 文件，没有名为 from_saved_model 以及 from_tensorflow 的模块/函数
其他上下文
我现有的模型是使用 SSD EfficientNet-D2 和 BiFPN 进行训练
我试图将带有 BiFPN .pb 模型的自定义对象检测 EfficientNet-D2 转换为 .onnx 模型]]></description>
      <guid>https://stackoverflow.com/questions/77569503/tf2onnx-convert-has-no-attribute-from-saved-model</guid>
      <pubDate>Wed, 29 Nov 2023 08:17:46 GMT</pubDate>
    </item>
    <item>
      <title>如何预测2050年家庭用水量</title>
      <link>https://stackoverflow.com/questions/77534533/how-to-predict-the-household-water-consumption-in-2050</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77534533/how-to-predict-the-household-water-consumption-in-2050</guid>
      <pubDate>Thu, 23 Nov 2023 05:31:32 GMT</pubDate>
    </item>
    <item>
      <title>将 MONAI 变换统一应用于图像序列</title>
      <link>https://stackoverflow.com/questions/77505170/applying-monai-transforms-to-sequences-of-images-uniformly</link>
      <description><![CDATA[我正在处理一个医学数据集，其中每个数据点都是 10 个图像的序列。我想将 MONAI 变换（Rand2DElastic、RandRotate、RandZoom、RandGaussianNoise）应用于这些序列以进行增强。这些变换应随机应用于每个序列，但出于一致性目的，给定序列中的每个图像应具有完全相同的变换。是否已有功能可以做到这一点？如果没有的话有什么好的方法吗？
数据存储为形状为 (n, 10, 3, 128, 128) 的 np 数组
谢谢！
images = [Image.open(os.path.join(self.root_dir, image_path)) for image_path inequence_path]
images = [np.transpose(np.array(image), (2,0,1)) 图像中的图像]

        如果自我增强：
            增强变换=撰写（[
                Rand2DElastic(概率=0.6, 间距=(30, 30), 幅度_范围=(0.1, 0.3)),
                RandRotate(range_x=np.pi / 60, prob=0.2, keep_size=True),
                RandZoom(min_zoom=0.8, max_zoom=1.5, prob=0.6),
                RandGaussianNoise（概率=0.5，平均值=0，标准=0.01）
            ]）
            图像 = Augment_transforms({“图像”: np.array(图像)})[“图像”]
            打印（图像）

这是我到目前为止所尝试的，但没有给出我需要的结果]]></description>
      <guid>https://stackoverflow.com/questions/77505170/applying-monai-transforms-to-sequences-of-images-uniformly</guid>
      <pubDate>Fri, 17 Nov 2023 23:59:40 GMT</pubDate>
    </item>
    <item>
      <title>在 Node.js 中使用 TensorFlow.js 时出现错误No backend found inregistry 错误</title>
      <link>https://stackoverflow.com/questions/75906320/errorno-backend-found-in-registry-error-while-using-tensorflow-js-in-node-js</link>
      <description><![CDATA[我尝试在 Node.js 中使用 TensorFlow.js，通过 MobileNet 对图像进行分类。但是，我收到以下错误：
未捕获（承诺中）错误：在注册表中找不到后端。 
我知道此错误与缺少注册后端有关，但我不确定如何解决它。这是我正在使用的代码：
这是完整的错误
 未捕获（承诺中）错误：在注册表中找不到后端。在 w.getSortedBackends (engine.js:267:19) 在 w.initializeBackendsAndReturnBest (engine.js:276:37) 在 get backend [as backend] (engine.js:108:46) 在 w.makeTensor (engine.js) :612:35) 在 (tensor_ops_util.js:57:12) 在 n (tensor.js:49:12) 在 Module.d (io_utils.js:214:25) 在 d.loadSync (graph_model.js:141) :35) 在 graph_model.js:116:54 在异步 u (graph_model.js:417:5)

我应该做什么来修复这个错误。先感谢您。非常感谢任何指导。
我也尝试使用 tf cdn，但我不断收到相同的错误。它表明我没有注册后端。]]></description>
      <guid>https://stackoverflow.com/questions/75906320/errorno-backend-found-in-registry-error-while-using-tensorflow-js-in-node-js</guid>
      <pubDate>Sat, 01 Apr 2023 12:57:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 Vertex AI 批量预​​测的自定义模型返回置信度分数</title>
      <link>https://stackoverflow.com/questions/70070421/return-confidence-score-with-custom-model-for-vertex-ai-batch-predictions</link>
      <description><![CDATA[我将预训练的 scikit learn 分类模型上传到 Vertex AI，并对 5 个样本进行批量预测。它只是返回了一个没有置信度分数的错误预测列表。我在 SDK 文档或 GCP 控制台中没有看到如何获取批量预测以包含置信度分数。 Vertex AI 可以做到这一点吗？
我的目的是使用以下代码自动执行批量预测管道。
batch_prediction_job = model.batch_predict(
    作业显示名称 = 作业显示名称,
    gcs_source = 输入路径，
    实例格式=“”，#“jsonl”，“csv”，“bigquery”，
    gcs_destination_prefix = 输出路径，
    起始副本计数 = 1,
    最大副本数 = 10,
    同步=真，
）

batch_prediction_job.wait()
]]></description>
      <guid>https://stackoverflow.com/questions/70070421/return-confidence-score-with-custom-model-for-vertex-ai-batch-predictions</guid>
      <pubDate>Mon, 22 Nov 2021 18:04:35 GMT</pubDate>
    </item>
    </channel>
</rss>