<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 26 Dec 2024 12:32:34 GMT</lastBuildDate>
    <item>
      <title>需要 chromadb 和 transformers 一起使用，但要求有冲突，因为 chromadb 需要 0.20 版本的 tokenizers，而后者需要 0.21 版本</title>
      <link>https://stackoverflow.com/questions/79309306/need-chromadb-transformers-together-but-have-conflicting-requirements-as-chrom</link>
      <description><![CDATA[我必须在一个项目中同时使用 chromadb 和 transformers，但 chromadb 需要 &lt;=0.20.3 版本的 tokenizers，而 transformers 需要 &gt;=0.21 版本的 tokenizers，并且与 chromadb 兼容的旧版本 transformers 需要 rust 编译器，因此这也不是一种选择。
我尝试升级 transformers、tokenizers，也尝试降级 transformers，但都不起作用，而对于所有这些，我都在使用虚拟环境。]]></description>
      <guid>https://stackoverflow.com/questions/79309306/need-chromadb-transformers-together-but-have-conflicting-requirements-as-chrom</guid>
      <pubDate>Thu, 26 Dec 2024 11:03:46 GMT</pubDate>
    </item>
    <item>
      <title>将多个模型指标运行记录到 MLFlow 中的同一个图中</title>
      <link>https://stackoverflow.com/questions/79308237/logging-multiple-model-metrics-runs-to-the-same-plot-in-mlflow</link>
      <description><![CDATA[我正在对模型参数进行网格搜索优化，并使用将损失记录到 MLFlow
Mlflow.log_metric(f“{run_number}_Loss”, returns, iteration)

但对于每次新运行，我在 MLFlow UI 中都会得到不同的图。
有没有办法多次记录到同一个图，也许是不同的颜色，并添加图例以便能够轻松比较不同的运行？]]></description>
      <guid>https://stackoverflow.com/questions/79308237/logging-multiple-model-metrics-runs-to-the-same-plot-in-mlflow</guid>
      <pubDate>Wed, 25 Dec 2024 19:50:23 GMT</pubDate>
    </item>
    <item>
      <title>在 MNIST 数据集上使用 Gumbel softmax 进行 VAE</title>
      <link>https://stackoverflow.com/questions/79307976/vae-with-gumbel-softmax-on-mnist-dataset</link>
      <description><![CDATA[kl 损失变为 0 可能是什么问题？重建损失很小，但每幅图像都相同，并且不代表任何数字。
这是我使用的编码器/解码器架构，我认为该架构足够复杂，可以有效地捕获和创建新的数字。
import tensorflow as tf
from keras import layer
import gumbel_softmax

class VariationalAutoEncoder(tf.keras.Model):
def __init__(self, latent_dim, categorical_dim):
super(VariationalAutoEncoder, self).__init__()
self.latent_dim = latent_dim
self.categorical_dim = categorical_dim
self.z_dim = latent_dim * categorical_dim

self.encoder = tf.keras.Sequential([
layer.InputLayer(input_shape=(28, 28, 1)),
层。Conv2D（16，（3，3），激活=&#39;relu&#39;，步幅=2，填充=&#39;相同&#39;），
层。Conv2D（32，（3，3），激活=&#39;relu&#39;，步幅=2，填充=&#39;相同&#39;），
层。Conv2D（64，（3，3），激活=&#39;relu&#39;，步幅=2，填充=&#39;相同&#39;），
层。BatchNormalization（），
层。Flatten（），
层。Dense（128，激活=&#39;relu&#39;），
层。Dense（self.z_dim），
层。Reshape（（latent_dim，categorical_dim）），
]）

self.decoder = tf.keras.Sequential（[
层。InputLayer（input_shape=（latent_dim，categorical_dim）），
层。Flatten（），
层。Dense（7 * 7 * 64，激活=&#39;relu&#39;），
layers.Reshape（（7，7，64）），
layers.Conv2DTranspose（64，（3，3），激活=&#39;relu&#39;，strides= 2，padding=&#39;same&#39;），
layers.Conv2DTranspose（32，（3，3），激活=&#39;relu&#39;，strides = 2，padding=&#39;same&#39;），
layers.Conv2DTranspose（1，（3，3），激活=&#39;sigmoid&#39;，padding=&#39;same&#39;），
]）

def call（self，x，temperature，hard）：
logits = self.encoder（x）
z = gumbel_softmax.gumbel_softmax（logits，temperature，hard=hard）
reconstructed = self.decoder（z）
return tf.reshape（reconstructed，（reconstructed.shape[0]，28，28））， tf.nn.softmax(logits, axis=-1)


import tensorflow as tf

def sample_gumbel(shape, eps=1e-15): 
&quot;&quot;&quot;从 Gumbel(0, 1) 分布中采样。&quot;&quot;&quot;
U = tf.random.uniform(shape, minval=0, maxval=1)
return -tf.math.log(-tf.math.log(U + eps) + eps)

def gumbel_softmax_sample(logits,temperature): 
&quot;&quot;&quot;从 Gumbel-Softmax 分布中采样。&quot;&quot;&quot;
y = logits + sample_gumbel(tf.shape(logits))
return tf.nn.softmax(y /temperature)

def gumbel_softmax(logits,temperature,hard=False):
&quot;&quot;&quot;从 Gumbel-Softmax 分布中采样并可选择离散化。

参数：
logits：[batch_size, n_class] 未归一化的对数概率
temperature：非负标量
hard：如果为 True，则取 argmax，但对软样本 y 进行区分

返回：
[batch_size, n_class] 来自 Gumbel-Softmax 分布的样本。
如果 hard=True，则返回的样本将是独热样本，否则它将是跨类别总和为 1 的概率分布。
&quot;&quot;&quot;
y = gumbel_softmax_sample(logits,temperature) 
if hard: 
y_hard = tf.one_hot(tf.argmax(y, axis=-1), tf.shape(logits)[-1])
y = tf.stop_gradient(y_hard - y) + y 

return y


以下是我使用的参数，我尝试了很多次，但没有成功。
LR_RATE = 5e-3

BATCH_SIZE=64
NUM_ITERS=900
tau0 = 1
ANNEAL_RATE=5e-5
MIN_TEMP=0.1
EPOCHS = 30

CATEGORICAL_DIM = 10
LATENT_DIM = 32

我计算了 kl 损失和重建损失的总和batch_size
def compute_loss(model, x,temperature,hard,beta):
重建，logits = model(x,temperature =temperature,hard =hard)

mse = keras.losses.BinaryCrossentropy(reduction=&quot;sum_over_batch_size&quot;)

rebuilding_loss = mse(x,reconstructed)
kl_loss = tf.reduce_mean(
tf.reduce_sum(
logits * (tf.math.log(logits + 1e-8) - tf.math.log(1.0 / config.CATEGORICAL_DIM)),
axis=[1, 2]
)
)
total_loss = rebuilding_loss + beta * kl_loss
return total_loss, rebuilding_loss, beta * kl_loss, reconstructed
]]></description>
      <guid>https://stackoverflow.com/questions/79307976/vae-with-gumbel-softmax-on-mnist-dataset</guid>
      <pubDate>Wed, 25 Dec 2024 16:28:11 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用哪些“领域特定”功能来预测糖尿病？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79307392/what-are-some-domain-specific-features-i-can-use-for-diabetes-prediction</link>
      <description><![CDATA[我需要在 MLOps 周期中为经典糖尿病 ML 数据集添加“领域特定”特征和特征工程的 Python 代码实现？
我正在向高中生教授机器学习。因此，我们使用经典 ML 糖尿病数据集。我需要使用 pandas、numpy 和 scikit-learn 使其保持简单。
我目前所做的工作：
我的特征工程实践想法是对性别进行分类、根据出生日期计算年龄（我已将日期添加到原始数据集）并根据 年龄 * BMI 计算风险百分比，但这些想法更侧重于从现有特征和特征交互中得出新变量。我真的需要一个想法/代码来实际演示“创建特定于域的功能”：
我拥有的代码需要基于特定于域的示例进行构建：
import pandas as pd
#数据以 CSV 格式导入，学生对此进行了一些基本的处理
data_frame = pd.read_csv(&quot;2.2.1.wrangled_data.csv&quot;)
data_frame[&#39;SEX&#39;] = data_frame[&#39;SEX&#39;].apply(lambda gender: -1 if gender.lower() == &#39;male&#39; else 1 if gender.lower() == &#39;female&#39; else None)
data_frame[&#39;Age&#39;] = ((data_frame[&#39;DoTest&#39;] - data_frame[&#39;DoB&#39;]).dt.days / 365.25).round().astype(int)
data_frame[&#39;Risk&#39;] = data_frame[&#39;BMI&#39;] * data_frame[&#39;Age&#39;]
data_frame[&#39;RiskPercentage&#39;] = ((data_frame[&#39;Risk&#39;] / data_frame[&#39;Risk&#39;].max()) * 100).round(2)

有任何关于特定领域示例的帮助想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79307392/what-are-some-domain-specific-features-i-can-use-for-diabetes-prediction</guid>
      <pubDate>Wed, 25 Dec 2024 10:36:46 GMT</pubDate>
    </item>
    <item>
      <title>分离图像内的盲文字符</title>
      <link>https://stackoverflow.com/questions/79306951/separation-of-braille-characters-inside-of-an-image</link>
      <description><![CDATA[我正在做一个将盲文转换为文本的项目。我已经编写了从图像中识别盲文点的代码，但我不知道如何将盲文分割成单元格。
这部分是识别图像中的斑点（较小的低质量图像目前不起作用）
import cv2
import numpy as np
from sklearn.cluster import KMeans

# 加载图像
image_path = &quot;braille.jpg&quot;
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 设置 SimpleBlobDetector
params = cv2.SimpleBlobDetector_Params()

# 按区域过滤（斑点大小）
params.filterByArea = True
params.minArea = 100 # 根据点大小进行调整
params.maxArea = 1000

# 按圆度过滤
params.filterByCircularity = True
params.minCircularity = 0.9 # 调整点的形状

# 按凸度过滤
params.filterByConvexity = False
params.minConvexity = 0.7

# 按惯性过滤（圆度）
params.filterByInertia = True
params.minInertiaRatio = 0.95

# 使用参数创建检测器
detector = cv2.SimpleBlobDetector_create(params)

# 检测斑点
keypoints = detector.detect(image)

# 将检测到的斑点绘制为红色圆圈
output_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
output_image = cv2.drawKeypoints(output_image, keypoints, np.array([]),
(0, 0, 255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

print(&quot;输出图像&quot;)
cv2.imshow(&quot;输出图像&quot;,output_image)
cv2.waitKey(0)
cv2.destroyAllWindows()

print(f&quot;检测到的斑点数量：{len(keypoints)}&quot;)

以下代码将 blob 的坐标放在图形上（认为这种方式可能更容易操作）
#将图像转换为图形

import matplotlib.pyplot as plt
import numpy

blob_coords = np.array([kp.pt for kp in keypoints]) #blob 的坐标
rounded_coords = np.round(blob_coords).astype(int) #四舍五入的坐标

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

# 基于邻近度的分组
# 如果 X 距离小于最小距离
# 如果 Y 距离小于最小距离
# 存储 X 和 Y 坐标

# 计算最小 x 和 y差异（尝试基于接近度）
minx = 10000
miny = 10000
for i in x_coords:
for j in x_coords:
if abs(i - j) &lt;= minx and (15 &lt; abs(i - j)): # 单元格宽度阈值
minx = abs(i - j)

for i in y_coords:
for j in y_coords:
if abs(i - j) &lt;= miny and (15 &lt; abs(i - j)): # 单元格高度阈值
miny = abs(i - j)

print(f&quot;Smallest x difference: {minx}, Smallest y difference: {miny}&quot;,)

# 绘图
fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;) # 绘制斑点
ax.invert_yaxis()
plt.title(&quot;Braille Cell Detection&quot;)
plt.show()

尝试通过接近度将它们分开（位于我尝试将距离很近的物体分组到一起（我将距离很近的物体分组到一起），但我无法理解其中的逻辑。我也尝试了组聚类 (Kmeans)，但它不是很准确，并且不适用于具有不同字符数的图像，因为它需要不断知道要形成多少个簇。
# 尝试 kmeans 聚类方法
# kmeans 不起作用（无法从图像中找出簇的数量）
# 如果可以找出 nclusters，则可以工作

导入数学
从 sklearn.cluster 导入 KMeans

blob_coords = np.array([kp.pt for kp in keypoints]) # 提取 blob 的 (x, y) 位置
rounded_coords = np.round(blob_coords).astype(int) # 为简单起见，对坐标进行四舍五入

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;) # 绘制斑点

ax.invert_yaxis() # 反转 Y 轴以获得类似图像的坐标
plt.title(&quot;盲文单元检测&quot;)
plt.show()

inertias = []

# 2
kmeans = KMeans(n_clusters=26)
kmeans.fit(rounded_coords)

plt.scatter(x_coords,y_coords, c=kmeans.labels_)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79306951/separation-of-braille-characters-inside-of-an-image</guid>
      <pubDate>Wed, 25 Dec 2024 05:54:00 GMT</pubDate>
    </item>
    <item>
      <title>比较图像并返回相似度百分比（针对徽标）[关闭]</title>
      <link>https://stackoverflow.com/questions/79305487/compare-images-and-return-similarity-percentage-for-logos</link>
      <description><![CDATA[假设我有 2 张图片


这有相同的徽标，因此结果应该超过 90%
这里是另外 2 个徽标在此处输入图片说明
现在我们又有 2 张照片了


这也是相同的图片，所以结果一定是正面的。
我遇到的问题是，在交换图片并比较“奥迪”和“奥运会”的标志时，尽管图像完全不同，相似度得分却超过 75%。我尝试过边缘检测等方法来解决这种差异，但这些方法都被证明是无效的。您能建议一种合适的方法来解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79305487/compare-images-and-return-similarity-percentage-for-logos</guid>
      <pubDate>Tue, 24 Dec 2024 11:41:18 GMT</pubDate>
    </item>
    <item>
      <title>同一样本的预测在训练和测试中有所不同</title>
      <link>https://stackoverflow.com/questions/79303693/prediction-on-the-same-sample-differs-from-training-to-testing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79303693/prediction-on-the-same-sample-differs-from-training-to-testing</guid>
      <pubDate>Mon, 23 Dec 2024 16:37:41 GMT</pubDate>
    </item>
    <item>
      <title>训练 transformer 模型时出现 OOM 错误</title>
      <link>https://stackoverflow.com/questions/79302713/oom-error-when-training-transformer-model</link>
      <description><![CDATA[我正在研究 HMER（手写数学表达式识别）问题，并尝试使用 CNN-Transformer 架构。但是，当我尝试训练我的模型时，我遇到了此错误：
DefaultCPUAllocator：内存不足：您尝试分配 69271363584 字节。
我认为我的位置编码存在一些问题，但我真的不知道如何调试它或这里真正的问题是什么（我在这方面还只是初学者）
我当前的模型如下所示
class PositionalEncoding(nn.Module):
def __init__(self, d_model, max_len=5000):
super(PositionalEncoding, self).__init__()
self.encoding = torch.zeros(max_len, d_model)
position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
self.encoding[:, 0::2] = torch.sin(position * div_term)
self.encoding[:, 1::2] = torch.cos(position * div_term)
self.encoding = self.encoding.unsqueeze(0) # 添加批次维度

def forward(self, x):
seq_len = x.size(1)
return x + self.encoding[:, :seq_len, :].to(x.device)

class TransformerDecoder(nn.Module):
def __init__(self, vocab_size, d_model, num_heads, num_layers, max_seq_length):
super(TransformerDecoder, self).__init__()
self.embedding = nn.Embedding(vocab_size, d_model)
self.positional_encoding = PositionalEncoding(d_model, max_seq_length)
decrypt_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads)
self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
self.fc_out = nn.Linear(d_model, vocab_size)

def forward(self, target_seqs, memory, target_mask):
# 嵌入目标序列
embedded = self.embedding(target_seqs) # (batch, seq_len, d_model)
embedded = checkpoint(self.positional_encoding, embedded) # 添加位置编码
# 转置以兼容 nn.TransformerDecoder (seq_len, batch, d_model)
embedded = embedded.permute(1, 0, 2)
memory = memory.permute(1, 0, 2)
# 解码
coded = checkpoint(self.transformer_decoder, embedded, memory, target_mask) # (seq_len, batch, d_model)
# 转置回 (batch, seq_len, d_model)
coded = decrypted.permute(1, 0, 2)
# 最终输出层
logits = checkpoint(self.fc_out,coded) # (batch, seq_len, vocab_size)
return logits


我的代码有什么问题吗？如果没有，是什么原因导致我遇到这个问题的]]></description>
      <guid>https://stackoverflow.com/questions/79302713/oom-error-when-training-transformer-model</guid>
      <pubDate>Mon, 23 Dec 2024 09:44:33 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：
&#39;super&#39; 对象没有属性 &#39;\_\_sklearn_tags__&#39;。

当我在 RandomizedSearchCV 对象上调用 fit 方法时会发生这种情况。我怀疑它可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我希望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Hugging Face Trainer 或 SFT Trainer 中记录第零步的训练损失？</title>
      <link>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</link>
      <description><![CDATA[我正在使用 Hugging Face Trainer（或 SFTTrainer）进行微调，我想在步骤 0（在执行任何训练步骤之前）记录训练损失。我知道有一个用于评估的 eval_on_start 选项，但我找不到在训练开始时记录训练损失的直接等效方法。
是否有办法使用 Trainer 或 SFTTrainer 在步骤 0（在任何更新之前）记录初始训练损失？理想情况下，我希望使用类似于 eval_on_start 的方法。
以下是我迄今为止尝试过的方法：
解决方案 1：自定义回调
我实现了自定义回调，以在训练开始时记录训练损失：
from transformers import TrainerCallback

class TrainOnStartCallback(TrainerCallback):
def on_train_begin(self, args, state, control, logs=None, **kwargs):
# 在第 0 步记录训练损失
logs = logs or {}
logs[&quot;train/loss&quot;] = None # 如果可用，用初始值替换 None
logs[&quot;train/global_step&quot;] = 0
self.log(logs)

def log(self, logs):
print(f&quot;Logging at start: {logs}&quot;)
wandb.log(logs)

# 将回调添加到 Trainer
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
train_dataset=train_dataset,
eval_dataset=eval_dataset,
args=training_args,
optimizers=(optimizer, scheduler),
callbacks=[TrainOnStartCallback()],
)

这有效，但感觉有点过头了。它会在训练开始时记录任何步骤之前的指标。
解决方案 2：手动记录
或者，我在开始训练之前手动记录训练损失：
wandb.log({&quot;train/loss&quot;: None, &quot;train/global_step&quot;: 0})
trainer.train()

问题：
Trainer 或 SFTTrainer 中是否有任何内置功能可以在第 0 步记录训练损失？或者自定义回调或手动记录是这里的最佳解决方案吗？如果是这样，是否有更好的方法来实现此功能？类似于 eval_on_start 但 train_on_start？
交叉：

discuss.huggingface
github/huggingface
]]></description>
      <guid>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</guid>
      <pubDate>Thu, 28 Nov 2024 00:23:35 GMT</pubDate>
    </item>
    <item>
      <title>无法将标准 torchvision ResNet50 模型导出到 ONNX 文件</title>
      <link>https://stackoverflow.com/questions/78853571/cant-export-standard-torchvision-resnet50-model-into-onnx-file</link>
      <description><![CDATA[我制作了一个非常简单的 Python 脚本，它加载 torchvision ResNet50 模型并尝试以两种方式导出到 onnx 文件（torch.onnx.export 和 torch.onnx.dynamo_export）
import torch
import torch.onnx

import torchvision

torch_model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2( weights=&#39;DEFAULT&#39;)
torch_model.eval()
torch_input = torch.randn(1, 3, 32, 32)

is_dynamo_export = False

if (is_dynamo_export):
onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)
onnx_program.save(&quot;onnx_dynamo_export_ResNET50.onnx&quot;) 
else:
torch.onnx.export(torch_model, # 正在运行的模型
torch_input, # 模型输入（或多个输入的元组）
&quot;onnx_export_ResNET50.onnx&quot;, # 模型的保存位置（可以是文件或类似文件的对象）
export_params=True, # 将训练后的参数权重存储在模型文件中
opset_version=10, # 将模型导出到的 ONNX 版本
do_constant_folding=True, # 是否执行常量折叠以进行优化
input_names = [&#39;input&#39;], # 模型的输入名称
output_names = [&#39;output&#39;], # 模型的输出名称
dynamic_axes={&#39;input&#39; : {0 : &#39;batch_size&#39;}, # 可变长度轴
&#39;output&#39; : {0 : &#39;batch_size&#39;}}) 

出现错误：
文件“C:\tools\Python311\Lib\site-packages\torch\onnx\_internal\exporter.py”，第 1439 行，位于 dynamo_export
raise OnnxExporterError(

torch.onnx.OnnxExporterError：无法将模型导出到 ONNX。在“report_dynamo_export.sarif”处生成 SARIF 报告。SARIF 是静态分析工具输出的标准格式。SARIF 日志可以在 VS Code SARIF 查看器扩展或 SARIF Web 查看器（https://microsoft.github.io/sarif-web-component/）中加载。请在 PyTorch Github 上报告错误：https://github.com/pytorch/pytorch/issues

torch.onnx.errors.SymbolicValueError：不支持：ONNX 导出 opset 9 中的 Pad。填充的大小必须是恒定的。请尝试 opset 版本 11。[由 (%535 : int[] = prim::ListConstruct(%405, %534, %405, %533, %405, %532) 中定义的值 &#39;535 引起，范围：torchvision.models.detection.faster_rcnn.FasterRCNN::

这两种方法都适用于极其简单的模型，例如
class MyModel(nn.Module):

def __init__(self):
super(MyModel, self).__init__()
self.conv1 = nn.Conv2d(1, 6, 5)
self.conv2 = nn.Conv2d(6, 16, 5)
self.fc1 = nn.Linear(16 * 5 * 5, 120)
self.fc2 = nn.Linear(120, 84)
self.fc3 = nn.Linear(84, 10)

def forward(self, x):
x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
x = F.max_pool2d(F.relu(self.conv2(x)), 2)
x = torch.flatten(x, 1)
x = F.relu(self.fc1(x))
x = F.relu(self.fc2(x))
x = self.fc3(x)
return x
]]></description>
      <guid>https://stackoverflow.com/questions/78853571/cant-export-standard-torchvision-resnet50-model-into-onnx-file</guid>
      <pubDate>Fri, 09 Aug 2024 15:25:36 GMT</pubDate>
    </item>
    <item>
      <title>特征名称应与 fit 期间传递的特征名称相匹配</title>
      <link>https://stackoverflow.com/questions/77748547/the-feature-names-should-match-those-that-were-passed-during-fit</link>
      <description><![CDATA[我尝试在使用 sklearn 线性回归创建模型后计算 r 平方值。
我只是

导入 csv 数据集
过滤有趣的列
在训练和测试中拆分数据集
创建模型
对测试进行预测
计算 r 平方以查看模型与测试数据集的拟合程度

数据集取自https://www.kaggle.com/datasets/jeremylarcher/american-house-prices-and-demographics-of-top-cities
代码如下
&#39;&#39;&#39; 让我们验证价格和浴室床位数量之间是否存在相关性&#39;&#39;&#39;

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

df = pd.read_csv(&#39;data/American_Housing_Data_20231209.csv&#39;)

df_interesting_columns = df[[&#39;Beds&#39;, &#39;Baths&#39;, &#39;Price&#39;]]

independent_variables = df_interesting_columns[[&#39;Beds&#39;, &#39;Baths&#39;]]
dependent_variable = df_interesting_columns[[&#39;Price&#39;]]

X_train, X_test, y_train, y_test = train_test_split(independent_variables,dependent_variable, test_size=0.2)

model = LinearRegression()
model.fit(X_train, y_train)

prediction = model.predict(X_test)

print(model.score(y_test, prediction))

但我得到了错误
ValueError：特征名称应与拟合期间传递的特征名称相匹配。
拟合时未看到的特征名称：

价格
拟合时看到的特征名称，但现在缺失：
浴室
床

我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/77748547/the-feature-names-should-match-those-that-were-passed-during-fit</guid>
      <pubDate>Tue, 02 Jan 2024 21:01:29 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在仅具有边缘特征的图上使用 GNN 吗？</title>
      <link>https://stackoverflow.com/questions/77258901/can-we-use-gnn-on-graphs-with-only-edge-features</link>
      <description><![CDATA[我正在尝试使用 GNN 对系统发育数据进行分类（完全二分、单向树）。我将 R 中的系统发育树格式转换为 PyTorch 数据集。以其中一棵树为例：

Data(x=[83, 1], edge_index=[2, 82], edge_attr=[82, 1], y=[1], num_nodes=83)

它有 83 个节点（内部节点 + 提示节点，x=[83, 1]），我为所有节点分配了 0，因此每个节点都有一个特征值 0。我构建了一个 82 X 1 矩阵，其中包含节点之间所有有向边的长度（edge_attr=[82, 1]），我打算使用 edge_attr 表示边长度并将其用作权重。每棵树都有一个用于分类的标签（y=[1]，值在 {0, 1, 2} 中）。
如您所见，节点特征在我的例子中并不重要，唯一重要的是边缘特征（边缘长度）。
以下是我用于建模和训练的代码实现：
tree_dataset = TreeData(root=None, data_list=all_graphs)

class GCN(torch.nn.Module):
def __init__(self, hidden_​​size=32):
super(GCN, self).__init__()
self.conv1 = GCNConv(tree_dataset.num_node_features, hidden_​​size)
self.conv2 = GCNConv(hidden_​​size, hidden_​​size)
self.linear = Linear(hidden_​​size, tree_dataset.num_classes)

def forward(self, x, edge_index, edge_attr, batch):
# 1. 获取节点嵌入
x = self.conv1(x, edge_index, edge_attr)
x = x.relu()
x = self.conv2(x, edge_index, edge_attr)

# 2. 读出层
x = global_mean_pool(x, batch) # [batch_size, hidden_​​channels]

# 3. 应用最终分类器
x = F.dropout(x, p=0.5, training=self.training)
x = self.linear(x)

return x

model = GCN(hidden_​​size=32)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
train_loader = DataLoader(tree_dataset, batch_size=64, shuffle=True)
print(model)

def train():
model.train()

lost_all = 0
for data in train_loader:
optimizer.zero_grad() # 清除梯度。
out = model(data.x, data.edge_index, data.edge_attr, data.batch) # 执行一次前向传递。
loss = criterion(out, data.y) # 计算损失。
loss.backward() # 得出梯度。
lost_all += loss.item() * data.num_graphs
optimizer.step() # 根据梯度更新参数。

return lost_all / len(train_loader.dataset)

def test(loader):
model.eval()

correct = 0
for data in loader: # 在训练/测试数据集上分批迭代。
out = model(data.x, data.edge_index, data.edge_attr, data.batch)
pred = out.argmax(dim=1) # 使用概率最高的类。
correct += int((pred == data.y).sum()) # 对照真实标签进行检查。
return correct / len(loader.dataset) # 得出正确预测的比例。

for epoch in range(1, 20):
loss = train()
train_acc = test(train_loader)
# test_acc = test(test_loader)
print(f&#39;Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Loss: {loss:.4f}&#39;)

看来我的代码根本不起作用：
......
Epoch: 015, Train Acc: 0.3333, Loss: 1.0988
Epoch: 016, Train Acc: 0.3333, Loss: 1.0979
Epoch: 017, Train Acc: 0.3333, Loss: 1.0938
Epoch: 018, Train Acc: 0.3333, Loss: 1.1044
Epoch: 019，训练精度：0.3333，损失：1.1012
......
Epoch：199，训练精度：0.3333，损失：1.0965

是不是因为没有有意义的节点特征就不能使用GNN？还是我的实现有问题？]]></description>
      <guid>https://stackoverflow.com/questions/77258901/can-we-use-gnn-on-graphs-with-only-edge-features</guid>
      <pubDate>Mon, 09 Oct 2023 12:36:47 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ML 模型和 FastAPI 处理来自多个用户的请求？</title>
      <link>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</link>
      <description><![CDATA[我正在研究通过FastAPI分发人工智能模块的过程。
我创建了一个FastAPI应用，使用预先学习的机器学习模型来回答问题。
这种情况下，一个用户使用是没有问题的，但是多个用户同时使用的时候，响应可能会太慢。
那么，当多个用户输入一个问题的时候，有没有办法一次性复制模型并加载进去？
class sentencebert_ai():
def __init__(self) -&gt;无：
super().__init__()

def ask_query(self,query, topN):
startt = time.time()

ask_result = []
score = []
result_value = [] 
embedder = torch.load(model_path)
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)
query_embedding = embedder.encode(query, convert_to_tensor=True)
cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0] #torch.Size([121])121 表示该数据集为 10 ... cos_scores = cos_scores.cpu()

        top_results = np.argpartition(-cos_scores, range(topN))[0:topN]

        对于 top_results[0:topN] 中的 idx：        
            Ask_result.append(corpusid[idx].item())
            #.item()으로 접근하는 유는 张量(5)에서 해당 숫자에 접근하기 위한 방식다.
            score.append(round(cos_scores[idx].item(),3))

# 生成 json 数组并返回结果集
for i,e in zip(ask_result,score):
result_value.append({&quot;pred_id&quot;:i,&quot;pred_weight&quot;:e})
endd = time.time()
print(&#39;结果集&#39;,endd-startt)
return result_value
# return &#39;,&#39;.join(str(e) for e in ask_result),&#39;,&#39;.join(str(e) for e in score)

class Item_inference(BaseModel):
text : str
topN : Optional[int] = 1

@app.post(&quot;/retrieval&quot;, tags=[&quot;knowledge referral&quot;])
async def Knowledge_recommendation(item: Item_inference):

# db.append(item.dict())
item.dict()
results = _ai.ask_query(item.text, item.topN)

return results

if __name__ == &quot;__main__&quot;:
parser = argparse.ArgumentParser()
parser.add_argument(&quot;--port&quot;, default=&#39;9003&#39;, type=int)
# parser.add_argument(&quot;--mode&quot;, default=&#39;cpu&#39;, type=str, help=&#39;cpu for CPU mode, gpu for GPU mode&#39;)
args = parser.parse_args()

_ai = sentencebert_ai()
uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=args.port,workers=4)

更正版本
@app.post(&quot;/aaa&quot;) def your_endpoint(request: Request, item:Item_inference): start = time.time() model = request.app.state.model item.dict() # 测试结果 _ai = sentencebert_ai() results = _ai.ask_query(item.text, item.topN,model) end = time.time() print(end-start) return results ``` 
]]></description>
      <guid>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</guid>
      <pubDate>Fri, 25 Mar 2022 07:13:32 GMT</pubDate>
    </item>
    <item>
      <title>模型的敏感性和特异性</title>
      <link>https://stackoverflow.com/questions/65421010/sensitivity-and-specificity-of-model</link>
      <description><![CDATA[如果我有一个包含两个类别的图像数据集：正常和异常，除了准确度指标之外，我还想添加敏感度和特异性标准。
那么，我该如何引入这两个指标来计算我的模型的性能呢？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/65421010/sensitivity-and-specificity-of-model</guid>
      <pubDate>Wed, 23 Dec 2020 08:15:24 GMT</pubDate>
    </item>
    </channel>
</rss>