<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 05 Nov 2024 12:31:36 GMT</lastBuildDate>
    <item>
      <title>为什么我们需要 keras model.fit() 中的 y 变量</title>
      <link>https://stackoverflow.com/questions/79159003/why-do-we-need-a-y-varibale-in-keras-model-fit</link>
      <description><![CDATA[我正在处理手写数字数据集。数据加载如下：
(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

这是为对数字进行分类而创建的神经网络的代码：
model = keras.Sequential([
keras.layers.Dense(10, input_shape=(784,),activation=&#39;sigmoid&#39;)
])

model.compile(
optimizer=&#39;adam&#39;, 
loss = &#39;sparse_categorical_crossentropy&#39;,
metrics = [&#39;accuracy&#39;]
)
model.fit(X_train_flattened, y_train, epochs=5)

问题是，model.fit() 中的 y_train 的作用是什么。这似乎是一个分类问题，网络只需要输入（x_train_flattened）即可进行训练。]]></description>
      <guid>https://stackoverflow.com/questions/79159003/why-do-we-need-a-y-varibale-in-keras-model-fit</guid>
      <pubDate>Tue, 05 Nov 2024 11:56:46 GMT</pubDate>
    </item>
    <item>
      <title>JAX：训练模型时跟踪测试/验证损失</title>
      <link>https://stackoverflow.com/questions/79158791/jax-tracking-test-val-loss-when-training-a-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79158791/jax-tracking-test-val-loss-when-training-a-model</guid>
      <pubDate>Tue, 05 Nov 2024 10:58:38 GMT</pubDate>
    </item>
    <item>
      <title>拟合 lightgbm 回归模型时日期时间中的对象类型错误</title>
      <link>https://stackoverflow.com/questions/79158044/object-type-error-in-datetime-while-fitting-lightgbm-regressor-model</link>
      <description><![CDATA[我想进行库存预测，使用正则表达式提取后的数据如下所示。在此处输入图片说明
更重要的是，我花了太长时间执行这个模型（数据集大小为 4132338x10）并得到两个响应
提高值错误...
在此处输入图片说明]]></description>
      <guid>https://stackoverflow.com/questions/79158044/object-type-error-in-datetime-while-fitting-lightgbm-regressor-model</guid>
      <pubDate>Tue, 05 Nov 2024 07:31:26 GMT</pubDate>
    </item>
    <item>
      <title>Weka RandomForest m_Classifiers 为空且仅在构建 Spring Boot 项目后才出现 NullPointerException 和无法找到允许的类错误</title>
      <link>https://stackoverflow.com/questions/79157997/nullpointerexception-with-weka-randomforest-m-classifiers-is-null-and-cant-find</link>
      <description><![CDATA[我正在开发一个 Spring Boot 项目，我使用 Weka 的 RandomForest 模型并使用从 PostgreSQL 数据库检索的数据对其进行训练。当我正常启动项目（IDE 中的运行命令）时，项目运行完美，所有预测都正常工作。但是，当我构建项目然后尝试运行它时，我遇到了以下两个错误：
错误 1
错误 2
我想知道为什么这些错误只在构建项目后出现。可能是什么原因造成的？我该如何解决？
问题：
我通过调用 buildClassifier 来训练 RandomForest 模型，它在常规运行中训练成功。
但是，在构建项目后，我在 classifyInstance 方法调用期间收到 m_Classifiers 为空错误，这表明分类器数组未初始化。此外，在构建环境中找不到 RandomTree，尽管它在正常运行期间可用。
我尝试过的解决方案：
验证 trainingSet 和 testSet 是否包含实例（它们不为空）。
确认在常规运行中成功使用 buildClassifier 训练模型。
使用 values[2] = Double.NaN 设置一个空的（缺失的）acc 类值。
通过 Maven 在 pom.xml 中添加了 Weka 依赖项（weka-stable，版本 3.8.6）。
问题：
为什么这些错误只在构建项目后出现？如何解决从构建运行时 m_Classifiers 为空且找不到 RandomTree 的问题？
其他信息：

Weka 版本：3.8.6
Java 版本：1.8
使用 Spring Tool Suite (STS) 并通过 Maven 添加 Weka 依赖项。
]]></description>
      <guid>https://stackoverflow.com/questions/79157997/nullpointerexception-with-weka-randomforest-m-classifiers-is-null-and-cant-find</guid>
      <pubDate>Tue, 05 Nov 2024 07:17:06 GMT</pubDate>
    </item>
    <item>
      <title>GAN 图像生成给出疯狂的输出</title>
      <link>https://stackoverflow.com/questions/79157642/gan-image-generation-gives-crazy-output</link>
      <description><![CDATA[我正在尝试使用 GAN 创建一个生成器网络，该网络可以将 32x32 图像转换为 128x128 图像。我最初训练了一个 CNN，并取得了一些成功：

为了获得更准确的结果，我使用了一种 GAN 架构，该架构为生成器网络实现了更低的损失分数。然而，当我尝试进行预测时，我收到的图像如下所示：

似乎生成器网络正在寻找某种方法来打破鉴别器网络。我该如何解决这个问题，并找到一种让生成器真正生成高清图像的方法？
这是我的 GAN 的样子：
generator = UpsamplingCNN().to(device)
discriminator = Discriminator().to(device)

criterion = nn.BCELoss()

## 不同的学习率
optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))

num_generator_updates = 5
lambda_gp = 10
num_epochs = 50 
## 标签平滑
real_label = 0.9
fake_label = 0.0

for epoch in range(num_epochs):
generator.train()
discriminator.train()

for i, (low_res, high_res) in enumerate(tqdm(train_loader)):
# 将数据移动到正确的设备
low_res = low_res.to(device)
high_res = high_res.to(device)

# 为真实图像和虚假图像添加噪声
noise_std_dev = 0.05
noisy_real_images = high_res + noise_std_dev * torch.randn_like(high_res).to(device)
noisy_real_images = torch.clamp(noisy_real_images, 0.0, 1.0)

fake_images = generator(low_res)
noisy_fake_images = fake_images + noise_std_dev * torch.randn_like(fake_images).to(device)
noisy_fake_images = torch.clamp(noisy_fake_images, 0.0, 1.0)

# 真标签和假标签
output_real = discriminator(noisy_real_images).view(-1)
labels_real = torch.full((output_real.size(0),), real_label, dtype=torch.float, device=device)
loss_real = criterion(output_real, labels_real)

# 使用嘈杂的假图像进行训练
output_fake = discriminator(noisy_fake_images.detach()).view(-1)
labels_fake = torch.full((output_fake.size(0),), fake_label, dtype=torch.float, device=device)
loss_fake = criterion(output_fake, labels_fake)

# 计算梯度惩罚
gradient_penalty = compute_gradient_penalty(discriminator, high_res, fake_images, device)

# 带梯度惩罚的鉴别器总损失
loss_D = loss_real + loss_fake + lambda_gp * gradient_penalty
loss_D.backward()

optimizer_D.step()

## 多个生成器更新
for _ in range(num_generator_updates):
optimizer_G.zero_grad()

# 生成假图像并计算生成器的损失
fake_images = generator(low_res)
output_fake_for_G = discriminator(fake_images).view(-1)
labels_fake_for_G = torch.full((output_fake_for_G.size(0),),real_label, dtype=torch.float, device=device)
loss_G = criterion(output_fake_for_G, labels_fake_for_G)
loss_G.backward()

optimizer_G.step()

if i % 10 == 0:
print(f&#39;Epoch [{epoch+1}/{num_epochs}], Batch [{i}/{len(train_loader)}], &#39;
f&#39;Loss_D: {loss_real.item() + loss_fake.item():.4f}, Loss_G: {loss_G.item():.4f}&#39;)

我还对判别器使用了梯度惩罚，如下所示：
def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):
alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)
interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)
d_interpolates = discriminator(interpolates)
fake = torch.ones(d_interpolates.size(), device=device, require_grad=False)

gradients = torch.autograd.grad(
output=d_interpolates,
input=interpolates,
grad_outputs=fake,
create_graph=True,
retain_graph=True,
only_inputs=True
)[0]

gradients = gradients.view(gradients.size(0), -1)
gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
return gradient_penalty
]]></description>
      <guid>https://stackoverflow.com/questions/79157642/gan-image-generation-gives-crazy-output</guid>
      <pubDate>Tue, 05 Nov 2024 03:55:19 GMT</pubDate>
    </item>
    <item>
      <title>我们可以使用 SAM2 来追踪类似的物体吗？</title>
      <link>https://stackoverflow.com/questions/79157637/can-we-use-sam2-for-tracking-similar-objects</link>
      <description><![CDATA[有没有什么方法（或资源）可以使用 SAM2 来跟踪视频中的统一对象？
我发现 SAM2 可以更快地进行分割，我想用它来检测/分割和跟踪我的数据集上只有 1 个类（自定义类：0，统一对象）的对象。
可用的 SOTA 跟踪器（如 sort、botsort、bytetrack 和 deepsort）无法跨帧跟踪对象（但检测效果良好）。]]></description>
      <guid>https://stackoverflow.com/questions/79157637/can-we-use-sam2-for-tracking-similar-objects</guid>
      <pubDate>Tue, 05 Nov 2024 03:50:39 GMT</pubDate>
    </item>
    <item>
      <title>将位图转换为黑白以用于 Google MLKit 文本识别是否更好[关闭]</title>
      <link>https://stackoverflow.com/questions/79157550/is-it-better-to-convert-a-bitmap-to-black-and-white-for-for-google-mlkit-text-re</link>
      <description><![CDATA[我正在使用 Google MLKit 进行文本识别，效果很好，但似乎很难识别彩色图像中的文本。我想知道是否最好将图像转换为灰度，或者在使用 ML 阅读器扫描之前我可以进行任何其他优化。似乎 Google 的 Google 镜头比普通的 ML 套件阅读器效果更好，所以他们一定在做其他事情。]]></description>
      <guid>https://stackoverflow.com/questions/79157550/is-it-better-to-convert-a-bitmap-to-black-and-white-for-for-google-mlkit-text-re</guid>
      <pubDate>Tue, 05 Nov 2024 02:58:20 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中如何处理重叠数据</title>
      <link>https://stackoverflow.com/questions/79157457/how-to-deal-with-overlapping-data-in-machine-learning</link>
      <description><![CDATA[我正在创建一个机器学习模型来确定用户是否是机器人，我使用 seaborn 绘制了配对图并意识到大多数数据是重叠的。下面是我为标准化、拆分和部署模型编写的代码。该图显示了该模型在超过 40000 个样本的情况下的表现。如您所见，模型正在猜测，我正在尝试找出原因。

X = new_df[[&#39;Retweet Count&#39;, &#39;Mention Count&#39;, &#39;Follower Count&#39;, &#39;Tweet&#39;, &#39;Hashtags&#39;, &#39;Verified&#39;, &#39;Created At&#39;]]
y = new_df[[&#39;Bot Label&#39;]].values

y = y.ravel() # 确保 y 是一维数组而不是二维数组

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

Scaler = StandardScaler()
X_train_scaled = Scaler.fit_transform(X_train)
X_test_scaled = Scaler.transform(X_test)

从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.datasets 导入 make_classification
从 sklearn.metrics 导入 Confusion_matrix

rfc = RandomForestClassifier(n_estimators = 1000)
rfc.fit(X_train_scaled, y_train)
y_pred = rfc.predict(X_test_scaled)
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

]]></description>
      <guid>https://stackoverflow.com/questions/79157457/how-to-deal-with-overlapping-data-in-machine-learning</guid>
      <pubDate>Tue, 05 Nov 2024 01:41:31 GMT</pubDate>
    </item>
    <item>
      <title>行业 RAG 技术 [关闭]</title>
      <link>https://stackoverflow.com/questions/79157397/industries-rag-technology</link>
      <description><![CDATA[RAG 技术如何通过检索相关医学研究并生成量身定制的治疗建议来增强患者护理？
RAG 技术如何通过检索相关医学研究并生成量身定制的治疗建议来增强患者护理？在临床环境中可以实施哪些具体用例来改善决策？]]></description>
      <guid>https://stackoverflow.com/questions/79157397/industries-rag-technology</guid>
      <pubDate>Tue, 05 Nov 2024 00:54:23 GMT</pubDate>
    </item>
    <item>
      <title>对图像进行分类的正确逻辑是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79157264/what-is-the-correct-line-of-logic-for-classifying-images</link>
      <description><![CDATA[我读过这里和其他地方的许多文章，咨询过 ChatGPT 和 Gemini，甚至询问过真人，但我也想听听这个社区的意见：
我正在尝试制作一个照片分类器。
我的想法是，我可以使用一个模型，或者训练我自己的模型，给它几千张各种主题的图像，它会将它们归档到各自的分类文件夹中。
这些图像主要是各种活动的；户外、室内、舞台、观众、音乐会、演讲者、颁奖典礼、红地毯、活动前活动、活动后交流等等。我收集了 10 到 20 位摄影师拍摄的照片，并手动按类别归档，例如“房屋左侧远景”、“中央特写”、“颁奖”、“观众反应”、“活动前”等。并非所有事件都具有相同的类别，而且并非所有类别都易于区分，即使对于人类来说也是如此。
我尝试了以下方法：

对图像进行聚类，然后使用 CLIP 进行零拍摄，然后进行排序
训练我自己的“顶层” CLIP 的 10000 张图像，我已经手动分类了
使用其他模型，例如 alexnet、densenet、efficientnet、
inception_v3、mobilenet、vgg
尝试微调这些模型中的每一个
不同的聚类 thechiques
然后我甚至尝试使用 Florence 自动添加字幕，然后使用
MiniLM-L6-v2 将字幕提炼为单个标签，然后对它们进行排序

还有更多...
到目前为止，我的最佳准确率约为 90%，使用 2 种类型的聚类和使用我自己的数据在我的计算机上训练 CLIP 约一个月，但我不相信今天的技术不能做得更好。
所以我的问题是：
这样的任务的最佳逻辑是什么？对这些类型的图像进行分类的最佳方法是什么？这不像区分摩托车和香蕉那么容易。这需要“看到”“房子右侧远景”和“房子左侧远景”之间的区别，“活动后娱乐”和“活动前音乐会”之间的区别，“主持人 A”和“主持人 B”等。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79157264/what-is-the-correct-line-of-logic-for-classifying-images</guid>
      <pubDate>Mon, 04 Nov 2024 23:17:03 GMT</pubDate>
    </item>
    <item>
      <title>我的自定义 layernorm 函数有什么问题？</title>
      <link>https://stackoverflow.com/questions/79157138/whats-the-problems-in-my-custom-layernorm-function</link>
      <description><![CDATA[import numpy as np
import torch
import torch.nn. functional as F

def layer_norm(x, weight, bias, eps=1e-6):
# x 形状：[bs, h, w, c]
# 计算空间维度（高度、宽度）的平均值和方差
mean = np.mean(x, axis=(1, 2), keepdims=True) # 形状：(batch_size, 1, 1, channels)
var = np.var(x, axis=(1, 2), keepdims=True, ddof=0) # 使用 ddof=0 表示有偏方差

# 标准化
x_normalized = (x - mean) / np.sqrt(var + eps)

# 应用权重和偏差
out = weight[None, None, None, :] * x_normalized + bias[None, None, None, :]
return out

def test1(x):
x = np.transpose(x, (0, 2, 3, 1)) # 转置为 [bs, h, w, c]
weight = np.ones(channels)
bias = np.zeros(channels)

normalized_output = layer_norm(x, weight, bias)
return normalized_output

def test2(x):
global channels
x = np.transpose(x, (0, 2, 3, 1)) # 转置为 [bs, h, w, c]
x_tensor = torch.tensor(x, dtype=torch.float32)
weight = torch.ones(channels)
bias = torch.zeros(channels)

# 使用 PyTorch 的层规范，在最后一个维度（通道）上进行规范化
normalized_output = F.layer_norm(x_tensor, normalized_shape=(channels,), weight=weight, bias=bias)
return normalized_output.detach().numpy()

# 测试
batch, channels, height, width = 4, 3, 8, 8
# 生成随机输入
x = np.random.randint(-10, 10, (batch, channels, height, width))

# 计算两种实现的输出
layernorm1 = test1(x)
layernorm2 = test2(x)

# 检查输出是否接近
are_close = np.allclose(layernorm1, layernorm2, atol=1e-4)
print(&quot;Outputs are close:&quot;, are_close) # 如果它们足够接近，则应输出 True

var = np.var(x, axis=(1, 2), keepdims=True, ddof=0) # 使用 ddof=0对于有偏方差
var = np.var(x, axis=(1, 2), keepdims=True)

我的期望是 are_close==True,，这意味着 layernorm1 和 layernorm2 的距离非常小。由于 layernorm1 和 layernorm2 的形状很大，所以我会显示 layernorm1 和 layernorm2 的部分结果。 layernorm1[0,0,0:3,0:4] 数组([[ 0.35208505, 1.06448374, -0.52827179], [-1.6216472 , -1.7376534 , -1.07653225], [-1.12821414, 0.88935017, 1 .84752351]]) layernorm2[0,0,0:3,0:4] 数组([[ 0.07412489, 1.1859984 , -1.2601235 ], [-1.0690411 , -0.2672601 , 1.336302 ], [-1.3920445 , 0.4800153 , 0.9120291 ]], dtype=float32)
我尝试了带或不带 ddof=0 的 variacne 方法，打印语句中全部为 False。
我想知道如何实现类似于 Pytorch 内置 layernorm 函数的自定义 layernorm。
从代码角度看 layernorm 步骤是什么？
关于计算机视觉，layernorm 对特征图有什么作用？]]></description>
      <guid>https://stackoverflow.com/questions/79157138/whats-the-problems-in-my-custom-layernorm-function</guid>
      <pubDate>Mon, 04 Nov 2024 22:07:24 GMT</pubDate>
    </item>
    <item>
      <title>验证数据的输入形状无效</title>
      <link>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</link>
      <description><![CDATA[我正在使用 Tensorflow 在 Python 中开发一个简单的 ML 模型。代码如下：
import tensorflow as tf
import pandas as pd

# 加载 CSV 数据
def load_data(filename):
data = pd.read_csv(filename)
X = data[[&#39;X0&#39;,&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;]]
Y = data[[&#39;Y0&#39;,&#39;Y1&#39;]]
return tf.data.Dataset.from_tensor_slices((X.values, Y.values))

training_data = load_data(&quot;binarydatatraining.csv&quot;)
print(training_data)

# 构建一个简单的神经网络模型
model = tf.keras.models.Sequential([
tf.keras.layers.Dense(4,activation=&#39;relu&#39;),
tf.keras.layers.Dense(2)
])
# 编译模型
model.compile(optimizer=&#39;adam&#39;,
loss=&#39;mean_squared_error&#39;)

# 加载验证数据
validation_data = load_data(&quot;binarydatavalidation.csv&quot;)
print(validation_data)

# 训练模型
model.summary()
model.fit(training_data.batch(9), epochs=5)
model.summary()
model.fit(training_data.batch(9), epochs=1, validation_data = validation_data, validation_steps = 2)

一切都运行正常，直到我开始包含验证数据，该数据具有与训练数据相同数量的参数。然后我收到错误
ValueError：调用 Sequential.call() 时遇到异常。

[1m输入 Tensor(&quot;sequence_1/Cast:0&quot;, shape=(4,), dtype=float32) 的输入形状无效。预期形状 (None, 4)，但输入具有不兼容的形状 (4,)[0m

Sequential.call() 收到的参数：
• 输入=tf.Tensor(shape=(4,), dtype=int64)
• 训练=False
• 掩码=None

打印验证和训练数据集显示它们具有相同的维度，并且运行 print(training_data) 和 print(validation_data) 都给出
&lt;_TensorSliceDataset element_spec=(TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(2,), dtype=tf.int64, name=None))&gt;

如何正确设置验证数据以与 model.fit 内联运行？]]></description>
      <guid>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</guid>
      <pubDate>Tue, 29 Oct 2024 21:59:29 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“datachain.lib”的模块；“datachain”不是一个包</title>
      <link>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</link>
      <description><![CDATA[
为什么我会遇到 datachain.lib 模块的 ModuleNotFoundError？
我需要采取其他步骤才能在项目中正确使用 datachain 包吗？

我正在开发一个 Python 项目，在尝试导入模块时遇到以下错误：
import os
os.environ[&quot;PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION&quot;] = &quot;python&quot;
import tensorflow as tf
import numpy as np
from PIL import Image
from datachain.lib.dc import Column, DataChain

错误消息：
ModuleNotFoundError：没有名为“datachain.lib”的模块； &#39;datachain&#39; 不是包

详细信息：

我已使用 pip 安装了 datachain：pip install datachain。
通过运行 pip list 可看到 datachain 的安装版本为 0.2.18。
我已验证包已正确安装并位于我的 Python 环境中。
]]></description>
      <guid>https://stackoverflow.com/questions/78843004/modulenotfounderror-no-module-named-datachain-lib-datachain-is-not-a-packa</guid>
      <pubDate>Wed, 07 Aug 2024 09:53:07 GMT</pubDate>
    </item>
    <item>
      <title>layoutlmv3：尽管已完成推理，但 postprocess 方法仍未返回超过 512 个标记的数据</title>
      <link>https://stackoverflow.com/questions/76899333/layoutlmv3-issue-with-postprocess-method-not-returning-data-beyond-512-tokens-d</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76899333/layoutlmv3-issue-with-postprocess-method-not-returning-data-beyond-512-tokens-d</guid>
      <pubDate>Mon, 14 Aug 2023 13:24:26 GMT</pubDate>
    </item>
    <item>
      <title>如何创建用于回归的神经网络？</title>
      <link>https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression</link>
      <description><![CDATA[我正在尝试使用 Keras 来构建神经网络。我使用的数据是 https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics。我的代码如下：
import numpy as np
from keras.layers import Dense, Activation
from keras.models import Sequential
from sklearn.model_selection import train_test_split

data = np.genfromtxt(r&quot;&quot;&quot;file location&quot;&quot;&quot;, delimiter=&#39;,&#39;)

model = Sequential()
model.add(Dense(32,activation =&#39;relu&#39;,input_dim = 6))
model.add(Dense(1,))
model.compile(optimizer=&#39;adam&#39;,loss=&#39;mean_squared_error&#39;,metrics =[&#39;accuracy&#39;])

Y = data[:,-1]
X = data[:,:-1]

从这里我尝试使用model.fit(X,Y)，但模型的准确性似乎保持在 0。我是 Keras 的新手，所以这可能是一个简单的解决方案，提前致歉。
我的问题是，向模型添加回归以提高准确性的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/49008074/how-to-create-a-neural-network-for-regression</guid>
      <pubDate>Tue, 27 Feb 2018 11:53:38 GMT</pubDate>
    </item>
    </channel>
</rss>