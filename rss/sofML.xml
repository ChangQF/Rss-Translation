<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 07 Jun 2024 21:15:48 GMT</lastBuildDate>
    <item>
      <title>优化评分指标的多变量线性回归系数</title>
      <link>https://stackoverflow.com/questions/78593935/optimize-coefficients-for-multi-variable-linear-regression-of-scoring-metric</link>
      <description><![CDATA[我有一个电子商务网站，我尝试优化搜索结果，为用户提供最相关的结果。
为了提供最相关的搜索结果，我制定了一个评级指标。评级指标基于产品参数和每个参数的权重构建。
评级分数从 0-1 标准化
fn 函数返回 0-1 的分数
rating = b0*f0(X0) + b0*f1(X1) + bn*fn(Xn)

例如：rating = 0.4*f(seller_score) + 0.25*f(customers_stars) + 0.35*f(return_rate) = 0.683
当有人搜索“厨房椅子”时我根据评分对返回的结果进行排序。
SELECT 
* 
FROM 
db 
WHERE 
category = &quot;kitchen chair&quot; 
ORDER BY ratings DESC 
LIMIT 50

用户点击我记录的一些结果。在最佳情况下，用户将选择第一个结果之一，因为评分效果良好，并且用户可能得到了他想要的东西。但如果用户从列表中选择了第 24 个产品，那么我的评分可能没有得到优化。
我的目标是优化我的评分指标的系数，使其尽可能匹配用户实际点击的内容，这样他们就能首先获得最佳结果。
我有一系列 n 个搜索词，以及它们的结果和用户点击的内容。
我可以使用什么算法来找到 b0、b1、..bn 的最佳系数，这样点击的结果实际上就会位于顶部。现在，我希望找到适合所有搜索词的结果，而不是优化每个搜索词。]]></description>
      <guid>https://stackoverflow.com/questions/78593935/optimize-coefficients-for-multi-variable-linear-regression-of-scoring-metric</guid>
      <pubDate>Fri, 07 Jun 2024 20:43:38 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow Lite 尺寸错误？同样的模型，如果不使用 tensorflow lite，模型可以完美运行。为什么？</title>
      <link>https://stackoverflow.com/questions/78592790/tensorflow-lite-dimmension-error-with-the-same-model-if-not-using-tensorflow</link>
      <description><![CDATA[我将模型运行到 tflite 中。
然后收到错误：
ValueError：无法设置张量：维度不匹配。得到 56 但输入 0 的维度 0 应为 1
我将使用的输入是一个名为 user_place_array 的数组
这是我的代码：
model = tf.lite.Interpreter(model_path=&quot;/content/recommender_model.tflite&quot;)

input_details = model.get_input_details()
output_details = model.get_output_details()
input_shape = input_details[0][&#39;shape&#39;]

print(f&quot;user_place_array shape = &quot;,user_place_array.shape)
print(f&quot;model input = &quot;,input_shape)


返回值为：
user_place_array shape = (56, 2)
模型输入= [1 2]

然后，我检查 user_place_array，输出为：
array([[253, 0],
[253, 1],
[253, 2],
[253, 3],
# 直到 56


然后我使用 tflite 运行模型：
model.set_tensor(input_details[0][&#39;index&#39;], user_place_array)
这就是问题所在。我在这里得到了 tflite 错误
ValueError: 无法设置张量：维度不匹配。得到 56 但输入 0 的维度 0 应为 1。
当我不使用 tflite 时，模型运行完美。我使用：
model.predict(user_place_array).flatten()
该模型按预期提供建议。我不知道为什么它在 Tflite 上不起作用。
有人能帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/78592790/tensorflow-lite-dimmension-error-with-the-same-model-if-not-using-tensorflow</guid>
      <pubDate>Fri, 07 Jun 2024 15:22:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 提取多个实体 [关闭]</title>
      <link>https://stackoverflow.com/questions/78592545/extract-multiple-entities-with-llms</link>
      <description><![CDATA[我正在开展一个项目，该项目涉及从相同类型的 PDF 文档中提取实体（40-100 个），每个文档包含 8-20 页。这些文档包含表格、键值对和文本。我正在尝试寻找一种高效且经济的方法，以使用大型语言模型 (LLM) 实现高精度和高速度的实体提取。
我尝试过 RAG（检索增强生成），但它似乎很昂贵，因为它需要获取相关块并为每个实体生成 JSON 输出。因此，我正在寻找其他方法来完成此任务。
我考虑的一种方法是使用滑动窗口技术，其中我将连续的文档部分提供给 LLM 并采用提示工程来提取该特定部分中定义的实体。但是，这种方法引入了复杂性，例如处理某些实体的重复条目。
如果您能提供任何建议或最佳实践，以便以更高效、更经济的方式使用 LLM 从多页 PDF 中提取实体，我将不胜感激。
提前感谢您的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78592545/extract-multiple-entities-with-llms</guid>
      <pubDate>Fri, 07 Jun 2024 14:33:37 GMT</pubDate>
    </item>
    <item>
      <title>scikit 中的 gbrt_minimize 如何决定尝试多少个参数分割</title>
      <link>https://stackoverflow.com/questions/78592454/how-does-gbrt-minimize-from-scikit-decide-how-many-parameter-splits-to-try</link>
      <description><![CDATA[根据我对https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html和梯度提升决策树的理解，我假设对于 N 个参数，回归器会沿着每个参数选择一组分割，计算出应用此分割如何对数据进行分区，然后决定选择哪个分割以最大程度地减少损失（对于特定分位数）。
我的问题是，如果您的参数是实数，您如何决定在哪些参数值处进行分割？我原本希望找到某种参数来确定要进行多少次“等距”分割，但我只看到一个参数可以确定分割两侧所需的数据值数量以使其有效。这是否意味着它以某种方式反向运作？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78592454/how-does-gbrt-minimize-from-scikit-decide-how-many-parameter-splits-to-try</guid>
      <pubDate>Fri, 07 Jun 2024 14:14:02 GMT</pubDate>
    </item>
    <item>
      <title>线性回归和时间序列分析预测完成一项工作的时间</title>
      <link>https://stackoverflow.com/questions/78592088/linear-regression-and-time-series-analysis-in-predict-the-time-to-complete-a-wor</link>
      <description><![CDATA[我对线性回归和时间序列分析等预测技术还不熟悉。我的问题如下。我的应用有点像待办事项应用。它有很多项目。一个项目有很多部分。一项任务有优先级和任务数量。假设工作没有限制。我想根据同一项目的历史数据预测完成一项工作的时间。对于线性回归，我有以下内容：

y：完成一项工作的时间
x1：一项工作的优先级（类别变量）
x2：一项工作中的任务数量。

虽然我知道这个模型不够准确，但我仍然想给出关于预期时间的建议，因为我无法收集足够的数据来影响 y。这样可以吗？我可以使用时间序列分析吗？]]></description>
      <guid>https://stackoverflow.com/questions/78592088/linear-regression-and-time-series-analysis-in-predict-the-time-to-complete-a-wor</guid>
      <pubDate>Fri, 07 Jun 2024 13:02:03 GMT</pubDate>
    </item>
    <item>
      <title>Python 中事件/事件关系的关联规则</title>
      <link>https://stackoverflow.com/questions/78591986/association-rule-on-incident-event-relations-in-python</link>
      <description><![CDATA[我有两组数据：一个事件列表和一个已解决事件列表，其中包含导致每个事件的事件。我如何训练模型来预测哪个事件导致新事件发生？
示例：触发事件后，模型预测哪个事件最有可能导致事件发生。]]></description>
      <guid>https://stackoverflow.com/questions/78591986/association-rule-on-incident-event-relations-in-python</guid>
      <pubDate>Fri, 07 Jun 2024 12:38:09 GMT</pubDate>
    </item>
    <item>
      <title>我可以训练一个逻辑回归模型来将 ML 模型组合起来形成一个集成模型吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78591369/can-i-train-a-logistic-regression-model-for-combining-ml-models-to-form-an-ensem</link>
      <description><![CDATA[我有 3 个经过训练的 ML 模型，用于对数据集进行分类。我想将它们组合成一个集成模型。我知道有多种方法可以做到这一点 - 投票分类器、堆叠、装袋、提升等。
但我想组合概率而不是类别，我认为这可以通过加权总和来实现。为了将权重分配给每个模型的概率，而不是尝试多种权重组合，我得到了一个建议，即使用逻辑回归，将概率值作为特征。这是一种从我的每个模型中获取概率值的权重或系数的可接受方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78591369/can-i-train-a-logistic-regression-model-for-combining-ml-models-to-form-an-ensem</guid>
      <pubDate>Fri, 07 Jun 2024 10:38:34 GMT</pubDate>
    </item>
    <item>
      <title>寻找快速大规模图像嵌入比较数据库结构</title>
      <link>https://stackoverflow.com/questions/78591334/looking-for-fast-large-scale-image-embeddings-comparison-database-structure</link>
      <description><![CDATA[我计划将 100-200 万个实体存储到数据库中。每个实体将链接到 10-100 个图像嵌入。目标是使用可能属于也可能不属于数据库的外部图像，并将其与每个存储的嵌入进行比较，以找到它可能属于的最相似的实体。
我想问一下，哪种数据库最适合这项任务，为什么？与 Milvus 或 FAISS 等更专业的数据库相比，带有 pgvector 的 PostgreSQL 会慢多少？]]></description>
      <guid>https://stackoverflow.com/questions/78591334/looking-for-fast-large-scale-image-embeddings-comparison-database-structure</guid>
      <pubDate>Fri, 07 Jun 2024 10:31:33 GMT</pubDate>
    </item>
    <item>
      <title>我的 UNet 图像重建模型无法学习</title>
      <link>https://stackoverflow.com/questions/78591272/my-unet-image-reconstruction-model-wont-learn</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78591272/my-unet-image-reconstruction-model-wont-learn</guid>
      <pubDate>Fri, 07 Jun 2024 10:13:54 GMT</pubDate>
    </item>
    <item>
      <title>如果训练数据集中的正样本多于负样本，XGBoost 的 scale_pos_weight 是否可以正确平衡正样本？</title>
      <link>https://stackoverflow.com/questions/78587301/does-xgboosts-scale-pos-weight-correctly-balance-the-positive-samples-if-the-tr</link>
      <description><![CDATA[经过研究，我意识到 scale_pos_weight 通常计算为训练数据中负样本数量与正样本数量的比率。我的数据集有 840 个负样本和 2650 个正样本，因此比率为 0.32。如果我的样本反过来，我相信 scale_pos_weight 会是一种更好的方法。
是否可以安全地假设，由于比率小于 1，它仍将正确平衡类别？特异性在我的研究中很重要，但我们的主要目标集中在召回率、精确度和 F1 分数上。此设置是否会通过最大程度地影响特异性而导致更多的假阳性？]]></description>
      <guid>https://stackoverflow.com/questions/78587301/does-xgboosts-scale-pos-weight-correctly-balance-the-positive-samples-if-the-tr</guid>
      <pubDate>Thu, 06 Jun 2024 14:27:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 logistf 包时，Firth 的模型在 R 中卡住了（出现未收敛警告，CPU 使用率高达 99%）</title>
      <link>https://stackoverflow.com/questions/78579401/firths-model-stuck-with-non-converge-warning-and-cpu-usage-99-in-r-using-the</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78579401/firths-model-stuck-with-non-converge-warning-and-cpu-usage-99-in-r-using-the</guid>
      <pubDate>Wed, 05 Jun 2024 07:34:40 GMT</pubDate>
    </item>
    <item>
      <title>通过单一数字指标训练 XGBoost</title>
      <link>https://stackoverflow.com/questions/78558863/training-xgboost-over-a-single-number-metric</link>
      <description><![CDATA[假设我正在用 Python（xgboost 版本 2.0.3）构建一个 XGBoost 模型（这里的回归或分类完全不重要）来预测股票市场时间序列分析中的目标变量。
例如，目标可能是：时间序列中的下一个值或二进制变量，如果下一个值高于前一个值，则设置为 1，否则设置为 0。
为了训练模型，是否可以使用回归问题中的 MSE 或分类问题中的“二元逻辑”。
训练后，可以根据测试集中模型的输出对策略进行回测并计算总体回报。
我的问题是：使用 xgboost scikit-learn 接口，是否可以根据用于回测策略的性能指标来训练模型？
例如：按照策略最大化训练集中的总体回报规则。
在xgboost库网站上，展示了如何使用自定义损失函数来训练模型：
def softprob_obj(labels: np.ndarray, predt: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:
rows = labels.shape[0]
classes = predt.shape[1]
grad = np.zeros((rows, classes), dtype=float)
hess = np.zeros((rows, classes), dtype=float)
eps = 1e-6
for r in range(predt.shape[0]):
target = labels[r]
p = softmax(predt[r, :])
for c in range(predt.shape[1]):
g = p[c] - 1.0 if c == target else p[c]
h = max((2.0 * p[c] * (1.0 - p[c])).item(), eps)
grad[r, c] = g
hess[r, c] = h

grad = grad.reshape((rows * classes, 1))
hess = hess.reshape((rows * classes, 1))
return grad, hess

clf = xgb.XGBClassifier(tree_method=&quot;hist&quot;, objective=softprob_obj)

目标函数需要计算梯度和 hessian。
假设函数定义如下：
def maximum_performance_metric(y_true: np.ndarray, y_pred: np.ndarray):
# 指标计算（例如：使用 y_pred 计算总体回报
overall_return = get_overall_return(y_pred, real_prices, ...) #overall_return 是浮点数
return grad, hess


是否可以根据总体回报计算梯度和 hessian，然后使用此自定义损失训练模型函数？
函数 maximize_performance_metric() 如何访问包含 real_prices 的变量（需要整体回报计算）？]]></description>
      <guid>https://stackoverflow.com/questions/78558863/training-xgboost-over-a-single-number-metric</guid>
      <pubDate>Fri, 31 May 2024 08:14:07 GMT</pubDate>
    </item>
    <item>
      <title>如何绘制多类别分类中所有类别的 SHAP 摘要图</title>
      <link>https://stackoverflow.com/questions/78396068/how-to-plot-shap-summary-plots-for-all-classes-in-multiclass-classification</link>
      <description><![CDATA[我正在使用 XGBoost 和 SHAP 来分析多类分类问题中的特征重要性，并需要帮助同时绘制所有类的 SHAP 摘要图。目前，我一次只能生成一个类的图。
SHAP 版本：0.45.0
Python 版本：3.10.12

这是我的代码：
import xgboost as xgb
import shap
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score

# 生成合成数据
X, y = make_classification(n_samples=500, n_features=20, n_informative=4, n_classes=6, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 训练 XGBoost 模型进行多类分类
model = xgb.XGBClassifier(objective=&quot;multi:softprob&quot;, random_state=42)
model.fit(X_train, y_train)

然后我尝试绘制形状值：
# 创建 SHAP TreeExplainer
explainer = shap.TreeExplainer(model)

# 计算测试集的 SHAP 值
shap_values = explainer.shap_values(X_test)

# 尝试绘制所有类的摘要
shap.summary_plot(shap_values, X_test, plot_type=&quot;bar&quot;)

我得到了这个交互图而是：

我在这篇文章的帮助下解决了这个问题：
shap.summary_plot(shap_values[:,:,0], X_test, plot_type=&quot;bar&quot;)

哪个给出类别 0 的正常条形图：

然后我可以对类别 1、2、3 等执行相同操作。
问题是，如何为所有类别制作摘要图？即，单个图显示特征对每个类别的贡献？]]></description>
      <guid>https://stackoverflow.com/questions/78396068/how-to-plot-shap-summary-plots-for-all-classes-in-multiclass-classification</guid>
      <pubDate>Sat, 27 Apr 2024 19:02:16 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 模型拟合与 train_on_batch 之间的区别</title>
      <link>https://stackoverflow.com/questions/62629997/difference-between-tensorflow-model-fit-and-train-on-batch</link>
      <description><![CDATA[我正在构建一个 vanilla DQN 模型来玩 OpenAI gym Cartpole 游戏。
但是，在训练步骤中，我将状态作为输入，将目标 Q 值作为标签，如果我使用 model.fit(x=states, y=target_q)，它会正常工作，并且代理最终可以很好地玩游戏，但如果我使用 model.train_on_batch(x=states, y=target_q)，损失不会减少，并且模型在游戏中的表现不会比随机策略更好。
我想知道 fit 和 train_on_batch 之间有什么区别？据我了解，fit 在后台调用批处理大小为 32 的 train_on_batch，这应该没有什么区别，因为将批处理大小指定为等于我输入的实际数据大小并没有区别。
如果需要更多上下文信息来回答这个问题，完整的代码在这里：https://github.com/ultronify/cartpole-tf]]></description>
      <guid>https://stackoverflow.com/questions/62629997/difference-between-tensorflow-model-fit-and-train-on-batch</guid>
      <pubDate>Mon, 29 Jun 2020 01:30:09 GMT</pubDate>
    </item>
    <item>
      <title>使用单类 SVM 计算异常检测的异常分数</title>
      <link>https://stackoverflow.com/questions/53956538/calculating-anomaly-score-for-anomaly-detection-using-one-class-svm</link>
      <description><![CDATA[我对使用单类 SVM 计算异常检测的异常分数有疑问。我的问题是：如何使用 decision_function(X) 计算它，就像我在隔离森林中计算异常分数一样？
非常感谢，]]></description>
      <guid>https://stackoverflow.com/questions/53956538/calculating-anomaly-score-for-anomaly-detection-using-one-class-svm</guid>
      <pubDate>Fri, 28 Dec 2018 09:50:20 GMT</pubDate>
    </item>
    </channel>
</rss>