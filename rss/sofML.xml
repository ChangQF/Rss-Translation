<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 06 Dec 2023 12:26:42 GMT</lastBuildDate>
    <item>
      <title>RNN 模型的 RMSE 值极高</title>
      <link>https://stackoverflow.com/questions/77612881/rmse-value-extremly-high-on-rnn-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77612881/rmse-value-extremly-high-on-rnn-model</guid>
      <pubDate>Wed, 06 Dec 2023 11:59:25 GMT</pubDate>
    </item>
    <item>
      <title>我想知道我应该为 Btech AI & ML 的职业生涯学习什么语言？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77612647/i-want-to-know-what-language-should-i-learn-for-my-career-in-btech-ai-ml</link>
      <description><![CDATA[我在 Btech 中选择了 AI 和 ML 作为辅修，但在 12 标准中，我没有学习任何编码语言；因此，我目前无法决定应该开始什么以及从哪里开始。我正在学习Python，因为他们在大学里教书，但我不能很好地理解它，因为我没有编码背景。现在很想学但是不知道从哪里学。我预计到今年年底就能精通 Python 和其他语言。]]></description>
      <guid>https://stackoverflow.com/questions/77612647/i-want-to-know-what-language-should-i-learn-for-my-career-in-btech-ai-ml</guid>
      <pubDate>Wed, 06 Dec 2023 11:24:13 GMT</pubDate>
    </item>
    <item>
      <title>从头开始学习 ai-ml 的最佳网站或 YouTube 频道是哪个</title>
      <link>https://stackoverflow.com/questions/77612624/which-is-the-best-website-or-youtube-channel-for-learning-ai-ml-from-scratch</link>
      <description><![CDATA[几个月前我开始编程，并且了解一点 Python 和前端 Web 开发。
并从头开始学习 ai-ml，因此请推荐最佳的学习资源。
我尝试了几个 YouTube 频道，但对此并不满意。]]></description>
      <guid>https://stackoverflow.com/questions/77612624/which-is-the-best-website-or-youtube-channel-for-learning-ai-ml-from-scratch</guid>
      <pubDate>Wed, 06 Dec 2023 11:20:42 GMT</pubDate>
    </item>
    <item>
      <title>如何解决尝试安装模块时出现的此错误</title>
      <link>https://stackoverflow.com/questions/77612617/how-can-i-solve-this-error-that-is-appearing-when-i-am-trying-to-install-a-modul</link>
      <description><![CDATA[我正在使用 yolo-NAS 进行对象检测，但为此我需要安装一个名为 super-gradients 的模块。我正在 Conda 环境中构建我的项目，该环境是我现在专门为该项目创建的。但是在安装超级梯度模块时，它在中间抛出了一个错误。我已经尝试了所有我能做的方法。这是出现错误的图像在此处输入图像描述
我尝试更新 Visual C++，如上所述，当我在 google 中搜索解决方案时，我也尝试安装一些模块。如果有人给我一个解决方案，那将是一个很大的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/77612617/how-can-i-solve-this-error-that-is-appearing-when-i-am-trying-to-install-a-modul</guid>
      <pubDate>Wed, 06 Dec 2023 11:18:36 GMT</pubDate>
    </item>
    <item>
      <title>我正在构建一个允许动态训练数据的面部识别应用程序。有可能吗>]？</title>
      <link>https://stackoverflow.com/questions/77611753/i-am-building-a-facial-recognition-application-that-allows-dynamic-training-of-d</link>
      <description><![CDATA[我正在做一个学校项目，我需要允许学生（用户）在我的应用程序上注册他们的面孔。稍后，当学生完成注册后，他们可以登录应用程序并验证他们的面部。是否可以制作这种模型，让我可以为每个学生动态训练模型，而不需要使用已经获得的数据集来训练模型。我希望我的应用程序是动态的，因为学生每年都会加入学校或大学。我想节省收集学生面部数据所需的时间。
到目前为止我还没有尝试过任何事情。我需要一些建议]]></description>
      <guid>https://stackoverflow.com/questions/77611753/i-am-building-a-facial-recognition-application-that-allows-dynamic-training-of-d</guid>
      <pubDate>Wed, 06 Dec 2023 09:08:56 GMT</pubDate>
    </item>
    <item>
      <title>SARSA模型减少交通拥堵的奖励计算</title>
      <link>https://stackoverflow.com/questions/77610911/reward-calculation-for-a-sarsa-model-to-reduce-traffic-congestion</link>
      <description><![CDATA[我正在尝试实施奖励系统，SARSA 模型可以使用该系统来做出更好的决策，以缓解十字路口所有车道的交通。这就是我的奖励函数的样子：
defcalculate_reward(self,old_dti,new_dti):

    阿尔法 = 0.5
    贝塔 = 0.3
    伽马= 0.2

    duction_in_total_congestion = sum(old_dti.values()) - sum(new_dti.values())
    extra_vehicles = [max(0, count - self.vehicle_threshold) for count in self.vehicle_parameters[“vehicle_count”].values()]
    avg_congestion_above_threshold = sum(excess_vehicles) / 4

    如果 self.action_changed，则 action_cost = 1 否则 0

    奖励 = alpha * 总拥塞减少量 - beta * avg_congestion_above_threshold - gamma * action_cost

    返回奖励

dti（Delay Time Indicator）：是车道上所有车辆等待时间的总和
示例：old_dti = {“北”：4334，“南”：83，“东”：2332，“西”：432}
vehicle_threshold：一条车道可以拥有的最大车辆数量。我已将其设置为 12
self.vehicle_parameters[“vehicle_count”]：是每个车道上等待红灯的车辆数量。
示例：{“北”：12，“南”：0，“东”：2，“西”：2}
action_cost：如果SARSA模型做了一个决策，如果和之前不是同一个决策，则成本为1。如果做出了相同的决策，则成本为0
我为上述参数添加了权重以表示它们的重要性。 DTI 的重要性最高，因为一条 DTI 较低的车道上可以有 10 辆车，而另一条 DTI 较高的车道上可以有 5 辆车。在这种情况下，DTI 的优先级高于vehicle_count。
我之前的奖励计算函数：
&lt;前&gt;&lt;代码&gt;@staticmethod
defcalculate_reward(old_dti,new_dti,vehicle_count):
    最大奖励 = 10
    最大惩罚 = -10

    延迟之前 = sum(old_dti.values())
    延迟后 = sum(new_dti.values())

    如果delay_before == 0：
        如果delay_after&gt; 0:
            # 在没有延迟的情况下引入延迟应该受到惩罚
            返回最大惩罚值
        别的：
            # 保持不拥堵可能是一个中性或稍微积极的结果
            return 1 # 或一些小的正值
    别的：
        改进 = 延迟之前 - 延迟之后
        如果改善&gt; 0:
            # 根据改进百分比调整奖励
            奖励 = (改进 / 之前延迟) * max_reward
        elif改进&lt; 0:
            # 根据恶化的百分比调整惩罚范围
            惩罚率 = 绝对值（改进） / 之前延迟
            奖励 = 惩罚比例 * 最大惩罚
        别的：
            # 延迟没有变化
            奖励=0

    返回奖励

在此实现中，我仅根据 DTI 计算奖励。但经过 20 代后，奖励并没有显着变化，模型还没有正确学习。
我计算奖励的新方法是否能更好地缓解每条车道的拥堵？还有，我的SARSA应该根据什么来做出下一步的决定呢？截至目前，SARSA 每 0.5 秒就会做出一次决定。我正在考虑实施vehicle_threshold，如果车道的车辆阈值超过预设限制，SARSA 应该做出决定]]></description>
      <guid>https://stackoverflow.com/questions/77610911/reward-calculation-for-a-sarsa-model-to-reduce-traffic-congestion</guid>
      <pubDate>Wed, 06 Dec 2023 06:16:13 GMT</pubDate>
    </item>
    <item>
      <title>我不理解分类中的 .pred_class （使用逻辑回归）</title>
      <link>https://stackoverflow.com/questions/77610242/im-not-understanding-pred-class-in-classification-using-logistic-regression</link>
      <description><![CDATA[我有一个非常简单的问题，我的结果是二元的，我正在尝试使用逻辑回归（使用 tidymodels）根据一些预测变量（其中一些是众所周知的良好预测变量）进行分类。
我将因子结果编码为 0 和 1（1=正，这是我最感兴趣的）。
当我使用两种 types=“class” 运行预测函数时且 types=“prob”我得到名为：.pred_class、.pred_0 和 .pred_1 的列。
然后，例如，在绘制 ROC 曲线时，我想知道是否应该使用
roc1 &lt;- roc_curve(data_test_pred, 结果, .pred_1)

或
roc1 &lt;- roc_curve(data_test_pred, 结果, .pred_0)。

第一个（我认为是正确的）在对角线下方给出了一个糟糕的 ROC 曲线，第二个给出了一个不错的 ROC 曲线。
所以，我只是不明白这里发生了什么，也不知道如何继续。]]></description>
      <guid>https://stackoverflow.com/questions/77610242/im-not-understanding-pred-class-in-classification-using-logistic-regression</guid>
      <pubDate>Wed, 06 Dec 2023 02:26:33 GMT</pubDate>
    </item>
    <item>
      <title>如何训练笔迹模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77610232/how-to-train-handwriting-model</link>
      <description><![CDATA[我看到了 sjvasquez 的一个名为手写合成的项目。我真的很想训练自己的模型来匹配我的笔迹，但我不确定如何开始。
基本上，我只是不确定如何输入数据并训练它。]]></description>
      <guid>https://stackoverflow.com/questions/77610232/how-to-train-handwriting-model</guid>
      <pubDate>Wed, 06 Dec 2023 02:20:08 GMT</pubDate>
    </item>
    <item>
      <title>无法让我的逻辑回归算法显示任何图[关闭]</title>
      <link>https://stackoverflow.com/questions/77609888/cant-get-my-logistic-regression-algorithm-to-display-any-of-the-plots</link>
      <description><![CDATA[这是我的逻辑回归项目的代码。我没有错误并且程序运行，但是一旦运行完成，它只显示进程完成退出代码0。它应该显示诸如我的成本函数和 thetas 以及所有常见的机器学习信息之类的内容，但我一生都无法让它显示。
导入 matplotlib.pyplot 作为 plt
将 numpy 导入为 np
将 pandas 导入为 pd
从 numpy 导入日志、点、exp、形状
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.model_selection 导入 train_test_split


def标准化（x_tr）：
    对于范围内的 i(shape(x_tr)[1])：
        x_tr[:, i] = (x_tr[:, i] - np.mean(x_tr[:, i])) / np.std(x_tr[:, i])
    返回x_tr


定义 sigmoid(z):
    sig = 1 / (1 + exp(-z))
    返回信号


定义成本（theta，x，y）：
    z = 点(x, θ)
    cost0 = y.T.dot(log(sigmoid(z)))
    成本1 = (1 - y).T.dot(log(1 - sigmoid(z)))
    成本 = -(成本1 + 成本0) / len(y)
    退货成本


def 初始化（x，y）：
    thetas = np.zeros((shape(x)[1] + 1, len(np.unique(y))))
    x = np.c_[np.ones((形状(x)[0], 1)), x]
    返回 θ，x


def fit(x, y, alpha=0.001, 迭代=400):
    θ，x = 初始化（x，y）
    cost_list = np.zeros(迭代, )

    对于范围内的 i（迭代）：
        对于范围内的 c(len(np.unique(y)))：
            y_temp = np.where(y == c, 1, 0)
            thetas[:, c] = thetas[:, c] - alpha * dot(x.T, (sigmoid(dot(x, thetas[:, c])) - y_temp))
            cost_list[i] += cost(thetas[:, c], x, y_temp)

    返回cost_list，thetas


def 预测（x，thetas）：
    x = np.c_[np.ones((形状(x)[0], 1)), x]
    z = 点(x, θ)
    sig = sigmoid(z)
    返回 np.argmax(sig, axis=1)


def 比较（y_test，y_pred，y_pred1）：
    正确分类 = np.sum(y_test == y_pred)
    正确分类1 = np.sum(y_test == y_pred1)

    print(“我们的模型测试集的准确度：”, (rightly_classified / len(y_test)) * 100)
    print(“sklearn 模型测试集的准确度：”, (rightly_classified1 / len(y_test)) * 100)


def main():
    df = pd.read_csv(&#39;Iris.csv&#39;) # 使用正确的文件名更新文件名
    x = df.iloc[:, :-1].values
    y = df.iloc[:, -1].值

    # 将分类值转换为数值
    df[&#39;物种&#39;].replace(&#39;Iris-setosa&#39;, 0, inplace=True)
    df[&#39;物种&#39;].replace(&#39;杂色鸢尾&#39;, 1, inplace=True)
    df[&#39;物种&#39;].replace(&#39;维吉尼亚鸢尾&#39;, 2, inplace=True)

    x_tr, x_te, y_tr, y_te = train_test_split(x, y, test_size=0.3, random_state=0)
    x_tr = 标准化(x_tr)

    cost_list, thetas = fit(x_tr, y_tr)
    plt.scatter(range(len(cost_list)), cost_list, c=“蓝色”)
    plt.show()

    y_pred = 预测（x_te，thetas）
    模型1 = 逻辑回归()
    model1.fit(x_tr, y_tr)
    y_pred1 = model1.预测(x_te)

    比较（y_te，y_pred，y_pred1）

    主要的（）
]]></description>
      <guid>https://stackoverflow.com/questions/77609888/cant-get-my-logistic-regression-algorithm-to-display-any-of-the-plots</guid>
      <pubDate>Wed, 06 Dec 2023 00:03:09 GMT</pubDate>
    </item>
    <item>
      <title>HMM R 包 if (d < delta) { 中的错误：缺少 TRUE/FALSE 需要的值</title>
      <link>https://stackoverflow.com/questions/77609409/hmm-r-package-error-in-if-d-delta-missing-value-where-true-false-needed</link>
      <description><![CDATA[我正在尝试在 R 中使用 HMM 包。
我想要 4 个隐藏状态，我的观察值范围为 2 到 15。
我可以毫无问题地初始化隐藏模型：
if (!require(HMM, 悄悄地 = TRUE)) {
  install.packages(“HMM”)
  图书馆（隐马尔可夫模型）
} 别的 {
  图书馆（隐马尔可夫模型）
}

# 加载数据
url &lt;-“https://raw.githubusercontent.com/luancvieira/HMM/main/ottawa_2010-2012.csv”
df &lt;- read.csv(url)

观察到的数据 &lt;- df$AvgTemperature

# 定义状态的数量和名称
n_states &lt;- 4
state_names &lt;-paste0(“州”, 1:n_states)

# 对符号进行排序
symbol_names &lt;- as.character(sort(unique(observed_data)))
observed_data &lt;- as.character(observed_data)

# 用随机概率初始化HMM模型
start_probs &lt;- runif(n_states)
trans_probs &lt;- 矩阵(runif(n_states * n_states),
               nrow = n_states, ncol = n_states)
emission_probs &lt;- 矩阵(runif(n_states * length(symbol_names)),
                  nrow = n_states, ncol = 长度(symbol_names))

# 标准化行以确保概率总和为 1
start_probs &lt;- start_probs / sum(start_probs)
trans_probs &lt;- trans_probs / rowSums(trans_probs)
Emission_probs &lt;- Emission_probs / rowSums(emission_probs)

# 初始化HMM模型
hmm_model &lt;- initHMM(States = state_names,
                     符号 = 符号名称，
                     起始概率=起始概率，
                     反式概率=反式概率，
                     排放概率 = 排放概率）

# 打印初始化的模型
打印（嗯_模型）


但是，当我尝试运行它时：
bw = baumWelch(hmm = hmm_model,观察=observed_data,
               最大迭代次数 = 100，增量 = 0.001)

我明白了
if (d &lt; delta) { 中的错误：缺少 TRUE/FALSE 需要的值。

如果我将迭代次数减少到 10 次，它运行时不会出现问题，但 10 次迭代不足以收敛。 delta 是算法的停止标准，如下所示： https:// /cran.r-project.org/web/packages/HMM/HMM.pdf。 delta 的默认值为 1e-9，因此即使没有指定 delta，在运行更多次迭代时仍然会返回错误。]]></description>
      <guid>https://stackoverflow.com/questions/77609409/hmm-r-package-error-in-if-d-delta-missing-value-where-true-false-needed</guid>
      <pubDate>Tue, 05 Dec 2023 21:45:12 GMT</pubDate>
    </item>
    <item>
      <title>拆分苹果ML模型可以获得更好的准确性？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77607078/splitting-the-apple-ml-model-gets-better-accuracy</link>
      <description><![CDATA[我的目标是使用 WLASL 数据集创建一个将手语转换为文本的模型。现在，从一开始就从 Kaggle 下载这个模型，虽然数据集看起来相当全面，但每个类别的视频数量从 5-13 个不等，这显然需要训练的内容相当少。我决定尝试 Apple Create ML，而不是像 Tensorflow 或更复杂的深度学习框架，因为这样会简单得多。由于数据集在每个类别的视频方面非常有限，因此我在“手部动作分类器”中使用了所有 6 个数据增强。 （水平翻转、旋转、平移、缩放、插帧、丢帧）。虽然我知道这无法保存模型，但它肯定会大大提高准确性。请注意，我没有使用数据集中的所有 2000 个类（单词），而是仅使用了 300 个类的子集。在所有增强的情况下，我获得了 16% 的验证准确度和 90% 的训练准确度。所以我对 25 个类进行了同样的尝试，这次我获得了 42% 的验证准确率，以及 100% 的训练准确率。我进入实时预览，几乎我尝试的每个迹象都被预测为错误。
现在，我决定使用“模型源”在侧边栏中。我不太确定它们的用途，但这是我尝试过的：
我将数据子集分成 2 个单独的模型源（16 个类，但数量仍然很高），分别获得了 83% 的验证准确率和 90% 的验证准确率。这两个模型源都使用所有数据增强。两个来源都有 100% 的训练准确度，但将其分成两个模型显然提高了我的准确度，当我在“实时预览”中测试这一点时，我自己做的每个 ASL 标志，它都能够准确地猜测每个单词置信度超过 90%。
所以我的问题是，即使我的数据有限（虽然增强确实增加了很多，但显然性能差异不应该这么大），我的模型如何表现得这么好？此外，将一个模型拆分为单独的模型源是否可行？我不确定“模型来源”有什么用？甚至是，所以我尝试了这个，不知怎的，我得到了更好的结果。如果可行，我如何将它们实现到一个快速应用程序中。我现在有点困惑，所以希望有人能告诉我发生了什么事。如果这不是一个可行的解决方案，有人可以提供另一个解决方案来说明我如何使用这个数据集吗？事先了解它会非常有帮助，但即使你不知道，你能帮助我吗？
Kaggle链接：https://www.kaggle.com/datasets/risangbaskoro/ wlasl 处理
原始论文github页面：https://github.com/dxli94/WLASL]]></description>
      <guid>https://stackoverflow.com/questions/77607078/splitting-the-apple-ml-model-gets-better-accuracy</guid>
      <pubDate>Tue, 05 Dec 2023 14:43:09 GMT</pubDate>
    </item>
    <item>
      <title>spaCy 值错误：[E1041] 需要字符串、文档或字节作为输入，但得到：<class 'float'></title>
      <link>https://stackoverflow.com/questions/77596731/spacy-value-error-e1041-expected-a-string-doc-or-bytes-as-input-but-got</link>
      <description><![CDATA[我正在尝试使用 spaCy 对中文输入进行矢量化。
我的代码如下：


nlp = spacy.load(&#39;zh_core_web_md&#39;)

def tokenize_and_vectorize_textZH(文本):
    clean_tokensZH = []
    对于 nlp(text) 中的标记：
        if (不是 token.is_stop) &amp; (token.lemma_ != &#39;-PRON-&#39;) &amp; （不是 token.is_punct）：
          # -PRON- 是一个特殊的全包“引理” spaCy 用于任何代词，我们要排除这些
            if (len(token.vector) != 300):
              打印（令牌）
            clean_tokensZH.append(token.vector)
    返回 np.array(clean_tokensZH)
    
    
all_summmed_vecsZH = []

def sum_vecsZH(输入):
  tokenized_vectorsZH = input.apply(tokenize_and_vectorize_textZH)
  tokenized_vectorZH = tokenized_vectorsZH.to_numpy()

  打印（len（tokenized_vectorsZH））
  #print(类型(标记化向量))

  对于 tokenized_vectorsZH 中的行：

    #打印（行）

    summed_vecZH = [0]*300 # 从 300 个零的列表开始

    for vec in row: # 循环遍历与行中每个标记对应的每个向量
      #if (len(vec) != 300):
        #打印（向量）
      summed_vecZH += vec

    all_summmed_vecs.append(summed_vecZH)

  #print(tokenized_vectors[0][0].向量)
  
  
#@title 应用矢量化
sum_vecsZH(X_trainZH)
打印（all_summmed_vecs）

sum_vecsZH(y_trainZH)
打印（all_summmed_vecs）

sum_vecsZH(X_testZH)
打印（all_summmed_vecs）

sum_vecsZH(y_testZH)
打印（all_summmed_vecs）



最后 8 行的预期输出应与此类似：
33384
33384
14308
14308
这是我的数据集的照片：https://i.stack。 imgur.com/lJVJo.png
这个错误的原因是什么？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77596731/spacy-value-error-e1041-expected-a-string-doc-or-bytes-as-input-but-got</guid>
      <pubDate>Mon, 04 Dec 2023 00:59:14 GMT</pubDate>
    </item>
    <item>
      <title>在 LightGBM 中使用不同 boosting 类型的数据采样方法</title>
      <link>https://stackoverflow.com/questions/77578111/use-of-data-sample-methods-with-different-boosting-types-in-lightgbm</link>
      <description><![CDATA[我的问题
我不太清楚所有参数的用法以及它们如何相互交互（或应该使用）。
我所知道的
据我了解，LightGBM中有3种算法：

GBDT，默认的，使用 boosting
DART 是一种带有 dropout 的 boosting 算法
随机森林，不使用增强（确实如此，但仅在一次迭代中）

并且有两种数据采样策略：

Bagging，默认设置，用于集成学习
GOSS 选择更多对误差梯度贡献最大的数据（我们的想法是，我们需要对远离基线的数据进行更多训练），而对“弱”数据进行更少的训练。数据点（对误差梯度贡献较小的数据点）。

问题
所以我的问题如下：

为什么 Bagging 和 GOSS 不兼容？它们似乎不会影响同一件事。
LightGBM 的主要创新似乎是 GOSS，但它并不是默认选择，这样做的动机是什么？
最后，我们能够将 boosting_type=goss 作为参数传递。当我们这样做时会发生什么？算法会是GBDT，而数据样本策略是goss吗？

非常感谢您抽出时间。
祝你有美好的一天。]]></description>
      <guid>https://stackoverflow.com/questions/77578111/use-of-data-sample-methods-with-different-boosting-types-in-lightgbm</guid>
      <pubDate>Thu, 30 Nov 2023 11:34:21 GMT</pubDate>
    </item>
    <item>
      <title>执行手动交叉验证时 Sklearn 的 precision_score 与 cross_val_score 的行为不一致</title>
      <link>https://stackoverflow.com/questions/77269168/inconsistent-behavior-of-sklearns-precision-score-when-manual-cross-validation</link>
      <description><![CDATA[我尝试将  precision_score  与  np.nan  一起用于  Zero_division 。它不适用于 cross_val_score，但当我使用相同的对进行手动交叉验证时，它可以工作。
这是要重现的数据文件：
sklearn_data.pkl.zip
# 加载数据
将 open(“sklearn_data.pkl”, “rb”) 作为 f：
    对象 = pickle.load(f)


#&gt;对象.keys()
# dict_keys([&#39;估计器&#39;, &#39;X&#39;, &#39;y&#39;, &#39;评分&#39;, &#39;cv&#39;, &#39;n_jobs&#39;])

估计器=对象[“估计器”]
X = 对象[“X”]
y = 对象[“y”]
评分=对象[“评分”]
简历 = 对象[“简历”]
n_jobs = 对象[“n_jobs”]

#&gt;得分
# make_scorer( precision_score, pos_label=Case_0, Zero_division=nan)

#&gt; y.unique()
# [&#39;控制&#39;, &#39;Case_0&#39;]
# 类别（2，对象）：[&#39;Case_0&#39;, &#39;Control&#39;]

# 首先我检查以确保所有训练和验证对中都存在两个类
pos_label =“案例_0”
control_label = “控制”
对于简历中的index_training、index_validation：
    断言 y.iloc[index_training].nunique() == 2
    断言 y.iloc[index_validation].nunique() == 2
    在 y.values 中断言 pos_label
    在 y.values 中断言 control_label

# 如果我手动运行：
分数 = 列表()
对于简历中的index_training、index_validation：
    estimator.fit(X.iloc[index_training], y.iloc[index_training])
    y_hat = estimator.predict(X.iloc[index_validation])
    分数 = precision_score(y_true = y.iloc[index_validation], y_pred=y_hat, pos_label=pos_label)
    分数.append(分数)
#&gt; print(np.mean(分数))
# 0.501156937317928

# 如果我使用 cross_val_score:
cross_val_score（估计器=估计器，X=X，y=y，cv=cv，评分=评分，n_jobs=n_jobs）
# /Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:839：UserWarning：评分失败。这些参数的训练测试分区的分数将设置为 nan。细节：
# 回溯（最近一次调用最后一次）：
# 文件“/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/python3.9/site-packages/sklearn/metrics/_scorer.py”，第 136 行，在 __call__ 中
# 分数 = Scorer._score(
# 文件“/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/python3.9/site-packages/sklearn/metrics/_scorer.py”，第 355 行，在 _score 中
# 返回 self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
# 文件“/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/python3.9/site-packages/sklearn/utils/_param_validation.py”，第 201 行，包装器中
# 验证参数约束（
# 文件“/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/python3.9/site-packages/sklearn/utils/_param_validation.py”，第 95 行，位于 validate_parameter_constraints
# 引发无效参数错误(
# sklearn.utils._param_validation.InvalidParameterError: precision_score 的 &#39;zero_division&#39; 参数必须是 {0.0, 1.0, nan} 中的 float 或 {&#39;warn&#39;} 中的 str。取而代之的是南。

这是我的版本：
&lt;前&gt;&lt;代码&gt;系统：
    蟒蛇: 3.9.16 |由 conda-forge 打包 | （主要，2023 年 2 月 1 日，21:42:20）[Clang 14.0.6 ]
可执行文件：/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/bin/python
   机器：macOS-13.4.1-x86_64-i386-64位

Python 依赖项：
      sklearn：1.3.1
          点：22.0.3
   安装工具：60.7.1
        numpy：1.24.4
        scipy：1.8.0
       赛通：0.29.27
       熊猫：1.4.0
   matplotlib：3.7.1
       作业库：1.3.2
线程池控制：3.1.0

使用 OpenMP 构建：正确

线程池控制信息：
       user_api: 布拉斯
   内部API：openblas
         前缀：libopenblas
       文件路径：/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/libopenblasp-r0.3.18.dylib
        版本：0.3.18
线程层：openmp
   架构：哈斯韦尔
    线程数：16

       用户API：openmp
   内部API：openmp
         前缀：libomp
       文件路径：/Users/jespinoz/anaconda3/envs/soothsayer_py3.9_env2/lib/libomp.dylib
        版本：无
    线程数：16

]]></description>
      <guid>https://stackoverflow.com/questions/77269168/inconsistent-behavior-of-sklearns-precision-score-when-manual-cross-validation</guid>
      <pubDate>Tue, 10 Oct 2023 21:56:09 GMT</pubDate>
    </item>
    <item>
      <title>SHAP中的Explainer和Kernelexplainer有什么区别？</title>
      <link>https://stackoverflow.com/questions/74251331/what-is-difference-between-explainer-and-kernelexplainer-in-shap</link>
      <description><![CDATA[我对可解释的人工智能很陌生。我开始研究 SHAP。当我查看代码时，我很困惑。该网站是 https:// /towardsdatascience.com/using-shap-v​​alues-to-explain-how-your-machine-learning-model-works-732b3f40e137
您可以看到下面的代码：
# 适合解释器
解释器 = shap.Explainer(model.predict, X_test)
# 计算 SHAP 值 - 需要一些时间
shap_values = 解释器(X_test)

另一个网站是 https://snyk.io/advisor/python /shap/functions/shap.KernelExplainer
您可以看到下面的代码：
 # 使用 Kernel SHAP 解释测试集预测
    解释器 = shap.KernelExplainer(svm.predict_proba, X_train, nsamples=100, link=“logit”)
    shap_values = 解释器.shap_values(X_test)

有什么区别？哪一个是真的？在第一个代码中，X_test 用于解释器。在第二个代码中，X_train 用于 kernelexplainer。为什么？]]></description>
      <guid>https://stackoverflow.com/questions/74251331/what-is-difference-between-explainer-and-kernelexplainer-in-shap</guid>
      <pubDate>Sun, 30 Oct 2022 07:48:03 GMT</pubDate>
    </item>
    </channel>
</rss>