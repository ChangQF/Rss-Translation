<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 24 Jan 2025 12:31:48 GMT</lastBuildDate>
    <item>
      <title>Bert 模型未使用 JAX 进行学习。结果没有改变</title>
      <link>https://stackoverflow.com/questions/79383687/bert-model-not-learning-using-jax-results-dont-change</link>
      <description><![CDATA[我正在使用 TPU 上的 JAX 训练垃圾邮件分类的 BERT 模型。我的模型没有学习，其结果也没有改变。
Epoch 0：训练损失 = 2.7961559295654297：训练准确度：0.30608975887298584 评估损失 = 3.6600053310394287：评估准确度 = 0.0
Epoch 1：训练损失 = 2.7961559295654297：训练准确度：0.30608975887298584 评估损失 = 3.6600053310394287：评估准确度 = 0.0
Epoch 2：训练损失 = 2.7961559295654297：训练准确度： 0.30608975887298584 Eval Loss = 3.6600053310394287: Eval Accuracy = 0.0

训练代码：
@jax.pmap
def train_step(state, batch, labels):
def loss_fn(params):

# 将批次中的所有内容放入模型并传递模型参数
logits = model(**batch, params = state.params).logits
loss = compute_loss(logits, labels) # 计算损失

return loss, logits

# 将损失函数转换为梯度微分函数
grad_fn = jax.value_and_grad(loss_fn, has_aux = True) # has_aux 允许返回 logits
# 从批次中获取损失和梯度grad_fn
(loss, logits), grads = grad_fn(state.params)
# 使用产生的梯度更新模型状态
new_state = state.apply_gradients(grads = grads)

return loss, logits, new_state

for epoch in range(epochs):
epoch_losses, epoch_accuracies = [], []
for batch in train_dataset:

batch[&quot;input_ids&quot;] = jnp.array(batch[&quot;input_ids&quot;])
batch[&quot;attention_mask&quot;] = jnp.array(batch[&quot;attention_mask&quot;])
batch[&quot;token_type_ids&quot;] = jnp.array(batch[&quot;token_type_ids&quot;])

# 我们将在多个数据集上复制该值设备 (tpus)
batch_inputs = {k: jax.device_put_replicated(v, jax.devices()) for k, v in batch.items() if k != &quot;Category&quot;}
batch_labels = jax.device_put_replicated(batch[&quot;Category&quot;], jax.devices()) # 在设备之间复制标签

# 从数据中删除 none
batch_labels = safe_convert_to_jax_array(jnp.array(batch_labels))
batch_labels = batch_labels.transpose(1, 0)

loss, logits, state = train_step(state, batch_inputs, batch_labels)

cls_logits = logits[:, :, 0, :] 
分类_logits = cls_logits[:, :, :2]

prediction_labels = jnp.argmax(classification_logits, axis = -1)
accuracy = compute_accuracy(predicted_labels, batch_labels)

初始化状态的代码：
class TrainState(train_state.TrainState):
pass

# 我们的模型参数
params = model.params
# 为我们的训练创建初始状态
state = TrainState.create(apply_fn = model.__call__, params = params, tx = optimizer)

def safe_convert_to_jax_array(input_data, default_value = 0):
# 用 default_value 替换 None 值
return jnp.array([default_value if x is None else x for x in input_data])

# 在 tpu 上复制状态
state = jax.device_put_replicated(state, jax.devices())

要查看完整代码：https://www.kaggle.com/code/yousefr/bert-spam-classification-using-jax-and-tpus
此外，我尝试调整学习率，但没有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/79383687/bert-model-not-learning-using-jax-results-dont-change</guid>
      <pubDate>Fri, 24 Jan 2025 08:33:23 GMT</pubDate>
    </item>
    <item>
      <title>社区连接器管道的 Apps 脚本中未检测到 BigQuery ML 模型训练完成</title>
      <link>https://stackoverflow.com/questions/79382851/bigquery-ml-model-training-completion-not-detected-in-apps-script-for-community</link>
      <description><![CDATA[我正在 lookerstudio 社区连接器中构建 Google Apps Script 管道，该管道创建 BigQuery ML 模型，然后将其用于异常检测。尽管实现了模型存在性检查和等待函数，但我仍然收到此错误：

错误：异常检测失败：{ 
&quot;error&quot;：{ 
&quot;code&quot;：400，
&quot;message&quot;：&quot;无效的表值函数 ML.DETECT_ANOMALIES\n该模型尚不可用...&quot;，
&quot;status&quot;：&quot;INVALID_ARGUMENT&quot; 
}
}


这是我的工作流程：

创建每日汇总表
创建 ARIMA 模型
等待模型训练
创建异常结果表

相关代码片段：
function modelExists(projectId, datasetId, modelId, accessToken) {
var url = &#39;https://bigquery.googleapis.com/bigquery/v2/projects/&#39; + 
projectId + &#39;/datasets/&#39; + datasetId + &#39;/models/&#39; + modelId;
var options = {
method: &#39;get&#39;,
headers: { Authorization: &#39;Bearer &#39; + accessToken },
muteHttpExceptions: true
};

尝试 {
var response = UrlFetchApp.fetch(url, options);
if (response.getResponseCode() === 200) {
var modelInfo = JSON.parse(response.getContentText());
// 检查 ACTIVE 状态或 creationTime 是否存在
return (modelInfo.state === &#39;ACTIVE&#39; || !!modelInfo.creationTime);
}
return false;
} catch (e) {
return false;
}
}

function waitForModel(projectId, datasetId, modelId, accessToken, timeout, interval) {
timeout = timeout || 480000; // 8 分钟
interval = interval || 5000; // 5 秒

var startTime = Date.now();
while (Date.now() - startTime &lt; timeout) {
var exist = modelExists(projectId, datasetId, modelId, accessToken);
if (存在) {
Logger.log(&#39;模型 &#39; + modelId + &#39; 已准备就绪&#39;);
return true;
}
Logger.log(&#39;正在等待模型 &#39; + modelId + &#39;... 当前状态： &#39; + getModelState(projectId, datasetId, modelId, accessToken));
Utilities.sleep(interval);
}
throw new Error(&#39;等待模型超时 &#39; + modelId);
}

function getModelState(projectId, datasetId, modelId, accessToken) {
var url = &#39;https://bigquery.googleapis.com/bigquery/v2/projects/&#39; + 
projectId + &#39;/datasets/&#39; + datasetId + &#39;/models/&#39; + modelId;
var options = {
method: &#39;get&#39;,
headers: { Authorization: &#39;Bearer &#39; + accessToken },
muteHttpExceptions: true
};

try {
var response = UrlFetchApp.fetch(url, options);
if (response.getResponseCode() === 200) {
var modelInfo = JSON.parse(response.getContentText());
return modelInfo.state || &#39;UNKNOWN&#39;;
}
return &#39;NOT_FOUND&#39;;
} catch (e) {
return &#39;ERROR&#39;;
}
}

管道执行：
const createModelQuery = `CREATE MODEL ...`;
fetchBigQuery(createModelQuery); 

// 2. 等待模型
waitForModel(projectId, datasetId, modelName, accessToken);

// 3. 创建结果表 &lt;- 此处失败
const detectQuery = `SELECT * FROM ML.DETECT_ANOMALIES(...)`;
fetchBigQuery(detectQuery);

问题：
即使 waitForModel 成功完成，后续的 ML.DETECT_ANOMALIES 调用仍会失败，并显示“模型不可用”。当我在 BigQuery 控制台中手动检查时，模型最终会在几分钟后可用。
问题：
为什么我的 waitForModel 函数无法正确检测模型训练完成情况，我如何确保管道等到模型真正准备就绪？]]></description>
      <guid>https://stackoverflow.com/questions/79382851/bigquery-ml-model-training-completion-not-detected-in-apps-script-for-community</guid>
      <pubDate>Thu, 23 Jan 2025 23:17:36 GMT</pubDate>
    </item>
    <item>
      <title>有人有使用 kluster.ai 进行 DeepSeek-R1 托管的经验吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79382626/does-anyone-have-experience-using-kluster-ai-for-deepseek-r1-hosting</link>
      <description><![CDATA[寻找一个使用 DeepSeek-R1 且价格不贵的地方。我正在为一家初创公司做一个项目，他们的预算有限。
我只想得到推理结果而不是推理过程，因为我不需要这些信息。我该怎么做？https://kluster.ai]]></description>
      <guid>https://stackoverflow.com/questions/79382626/does-anyone-have-experience-using-kluster-ai-for-deepseek-r1-hosting</guid>
      <pubDate>Thu, 23 Jan 2025 21:28:38 GMT</pubDate>
    </item>
    <item>
      <title>使用Yolov3进行光学音乐字符识别</title>
      <link>https://stackoverflow.com/questions/79382146/optical-music-characters-recognition-using-yolov3</link>
      <description><![CDATA[我正在尝试编写一个模型（Yolov3）来检测乐谱上的各种音乐符号。但所有适合此目的的数据集都只建立在印刷乐谱上。有没有办法以某种方式将模型适应手写字符？预训练 darknet-53 会对此有所帮助吗？如果我训练 darknet-53 识别手写和印刷字符，这会产生什么影响？
Yolov3 架构：Yolov3]]></description>
      <guid>https://stackoverflow.com/questions/79382146/optical-music-characters-recognition-using-yolov3</guid>
      <pubDate>Thu, 23 Jan 2025 18:12:00 GMT</pubDate>
    </item>
    <item>
      <title>java.lang.AssertionError：Android Studio 中“不支持数据类型 INT32”</title>
      <link>https://stackoverflow.com/questions/79380176/java-lang-assertionerror-does-not-support-data-type-int32-in-android-studio</link>
      <description><![CDATA[我正在使用 TensorFlow Lite 模型在 Android Studio 中开发应用程序。运行应用程序时，我遇到以下错误：
java.lang.AssertionError：TensorFlow Lite 不支持数据类型 INT32

以下是我的代码的相关部分：
// 准备输入张量
val inputFeature0 = TensorBuffer.createFixedSize(inputShape, DataType.FLOAT32)
inputFeature0.loadArray(flatArray)

// 运行推理
val output = model?.process(inputFeature0)
val rawOutputBuffer = output?.outputFeature0AsTensorBuffer

// 根据数据类型将原始数据提取为 IntArray 或 FloatArray
val outputArray = when (rawOutputBuffer?.dataType) {
DataType.INT32 -&gt; rawOutputBuffer.intArray // 直接访问 INT32 数据
DataType.FLOAT32 -&gt; rawOutputBuffer.floatArray.map { it.toInt() }.toIntArray() // 将 FloatArray 转换为 IntArray
else -&gt;抛出 IllegalArgumentException(&quot;不支持的输出张量数据类型：${rawOutputBuffer?.dataType}&quot;)
}


输入张量的类型为 FLOAT32，并且使用 TensorBuffer.createFixedSize() 和 loadArray() 正确加载输入数据。

在处理模型的输出张量 (outputFeature0AsTensorBuffer) 时，我添加了检查以处理 FLOAT32 和 INT32 输出。

尽管如此，应用程序崩溃并显示错误，表明 TensorFlow Lite 不支持 INT32。


我有什么已尝试：

确保输入张量使用 FLOAT32。
验证 TensorFlow Lite 模型是否与 FLOAT32 数据类型兼容。
检查输出张量数据类型并添加对 FLOAT32 和 INT32 的处理。

我预计模型推理可以顺利运行，因为我已经处理了 FLOAT32 和 INT32 输出情况。
问题：

为什么即使输入张量使用 FLOAT32，TensorFlow Lite 也会抛出此错误？
该错误是否与 TensorFlow Lite 内部管理张量维度或元数据等数据类型的方式有关？
如何解决此错误并确保 TensorFlow Lite 成功运行推理？
]]></description>
      <guid>https://stackoverflow.com/questions/79380176/java-lang-assertionerror-does-not-support-data-type-int32-in-android-studio</guid>
      <pubDate>Thu, 23 Jan 2025 07:30:52 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试使用 GNN-LSTM 预测尼日利亚某州未来确诊的脑膜炎病例数</title>
      <link>https://stackoverflow.com/questions/79377500/i-am-trying-to-predict-the-future-number-of-confirmed-cases-of-meningitis-for-a</link>
      <description><![CDATA[以下是代码：
 # 初始化模型
# 初始化模型
model = GNN_LSTM(input_dim=window_size, gcn_hidden_​​dim=16, lstm_hidden_​​dim=32, predict_steps=20)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.MSELoss()
print(f&quot;Shape of time_series_features: {time_series_features.shape}&quot;)

# 训练循环
for epoch in range(200):
model.train()
optimizer.zero_grad()

# 前向传递
out = model(data, time_series_features) # 无需解压，应符合模型输入预期
zamfara_predictions = out[target_idx] # 提取Zamfara 的预测

# 使用 train_mask 计算损失
adapted_train_mask = train_mask[-predict_steps:]

# 打印形状以供调试
print(f&quot;Shape of zamfara_predictions: {zamfara_predictions.shape}&quot;)
print(f&quot;Shape of zamfara_features[-predict_steps:]: {zamfara_features[-predict_steps:].shape}&quot;)
print(f&quot;Shape of adapted_train_mask: {adjusted_train_mask.shape}&quot;)

# 使用调整后的掩码计算损失
loss = criterion(
zamfara_predictions.squeeze()[adjusted_train_mask],
zamfara_features[-predict_steps:][adjusted_train_mask]
)

loss.backward()
optimizer.step()

# 验证
if (epoch + 1) % 20 == 0:
model.eval()
with torch.no_grad():
print(f&quot;zamfara_predictions (squeezed) 的形状：{zamfara_predictions.squeeze().shape}&quot;)
print(f&quot;zamfara_features 的形状：{zamfara_features.shape}&quot;)
print(f&quot;val_mask 的形状：{val_mask.shape}，类型：{val_mask.dtype}&quot;)
print(f&quot;test_mask 的形状：{test_mask.shape}，类型：{test_mask.dtype}&quot;)

# 验证损失计算
val_loss = criterion(
zamfara_predictions.squeeze()[val_mask],
zamfara_features[val_mask]
)
print(f&quot;Epoch {epoch+1}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}&quot;)

# 测试
model.eval()
with torch.no_grad():
test_loss = criterion(zamfara_predictions.squeeze()[test_mask], zamfara_features[test_mask])
print(f&quot;Test Loss: {test_loss.item()}&quot;)

# 指标计算
true_values = zamfara_features[test_mask].numpy()
predictions = zamfara_predictions.squeeze()[test_mask].numpy()

mse = mean_squared_error(true_values, predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(true_values, predictions)
print(f&quot;Test MSE: {mse:.4f}&quot;)
print(f&quot;Test RMSE: {rmse:.4f}&quot;)
print(f&quot;Test MAE: {mae:.4f}&quot;)

# Zamfara 的预测案例

我正在使用 Google Colab 编译该程序。编译后，代码中突出显示了此行：loss = criterion( zamfara_predict，并出现以下错误：
TypeError：&#39;int&#39; 对象不可调用。

代码由 meningitis_​​cases_graph.graphml 组成，它是尼日利亚 37 个州确诊脑膜炎病例的图表表示，edges.csv 是尼日利亚这些州的形状文件。我们想要预测尼日利亚某个州“赞法拉”未来 20 天的确诊病例数。如何解决此错误？]]></description>
      <guid>https://stackoverflow.com/questions/79377500/i-am-trying-to-predict-the-future-number-of-confirmed-cases-of-meningitis-for-a</guid>
      <pubDate>Wed, 22 Jan 2025 11:15:34 GMT</pubDate>
    </item>
    <item>
      <title>当我使用 knn 时收到错误：-215：断言失败）test_samples.type() == CV_32F && test_samples.cols == samples.cols 在函数“findNearest”中</title>
      <link>https://stackoverflow.com/questions/79375305/receiving-error-when-i-use-knn-215assertion-failed-test-samples-type-cv</link>
      <description><![CDATA[我正在开展一个项目，通过 OCR 从图像中读取手写数字。我使用这个 OpenCV 源代码来尝试它是否有效：https://docs.opencv.org/4.x/d8/d4b/tutorial_py_knn_opencv.html。
这是到目前为止有效的代码：
import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt

img = cv.imread(&#39;digits.png&#39;)
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)

# 现在我们将图像拆分为 5000 个单元格，每个单元格大小为 20x20
cells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]

# 将其转换为 Numpy 数组：其大小为 (50,100,20,20)
x = np.array(cells)

# 现在我们准备训练数据和测试数据
train = x[:,:50].reshape(-1,400).astype(np.float32) # Size = (2500,400)
test = x[:,50:100].reshape(-1,400).astype(np.float32) # Size = (2500,400)

# 为训练和测试数据创建标签
k = np.arange(10)
train_labels = np.repeat(k,250)[:,np.newaxis]
test_labels = train_labels.copy()

#启动 kNN，在训练数据上进行训练，然后使用 k=1 的测试数据进行测试
knn = cv.ml.KNearest_create()
knn.train(train, cv.ml.ROW_SAMPLE, train_labels)
ret,result,neighbours,dist = knn.findNearest(test,k=5)

# 现在我们检查分类的准确性
# 为此，将结果与 test_labels 进行比较，并检查哪些是错误的
matches = result==test_labels
correct = np.count_nonzero(matches)
accuracy = correct*100.0/result.size
print( accuracy )

# 保存数据
np.savez(&#39;knn_data.npz&#39;,train=train, train_labels=train_labels)

# 现在保存 kin_data.npz 后加载数据
使用np.load(&#39;knn_data.npz&#39;) 作为数据：
print( data.files )
train = data[&#39;train&#39;]
train_labels = data[&#39;train_labels&#39;]

现在让我们继续尝试手写数字：
在此处输入图像描述
但是我收到一条错误消息：错误：OpenCV(4.10.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/ml/src/knearest.cpp:313: 错误：(-215：断言失败) test_samples.type() == CV_32F &amp;&amp; test_samples.cols == samples.cols 在函数“findNearest”中
此消息的原因是什么？我该如何修复它？]]></description>
      <guid>https://stackoverflow.com/questions/79375305/receiving-error-when-i-use-knn-215assertion-failed-test-samples-type-cv</guid>
      <pubDate>Tue, 21 Jan 2025 17:12:30 GMT</pubDate>
    </item>
    <item>
      <title>使用 nb 方法进行交叉验证</title>
      <link>https://stackoverflow.com/questions/79338629/cross-validation-with-nb-method</link>
      <description><![CDATA[我尝试在 WESBROOK 数据集上使用 k 倍交叉验证。它使用 caret 包中的 train 函数来执行此操作。到目前为止，此函数已与 svm、knn 和 rpart 等方法配合使用，但使用 nb（朴素贝叶斯）方法时，我收到以下错误：
错误 { : 
任务 1 失败 - “未在 newdata 中找到对象中使用的所有变量名称”

我的 train 函数如下所示：
k_folds &lt;- 5
train_control &lt;- trainControl(method = &quot;cv&quot;, number = k_folds, classProbs = TRUE, summaryFunction = twoClassSummary)

nb_model &lt;- train(
TOTLGIVE ~ ., data = train_data,
method = &quot;nb&quot;,
trControl = train_control
)

我检查了一下，没有缺失数据，训练集和测试集中的列名及其类型相同。]]></description>
      <guid>https://stackoverflow.com/questions/79338629/cross-validation-with-nb-method</guid>
      <pubDate>Wed, 08 Jan 2025 09:50:09 GMT</pubDate>
    </item>
    <item>
      <title>如何在处理 EOS 代币时计算拥抱人脸模型的教师强制准确度 (TFA)？</title>
      <link>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</guid>
      <pubDate>Thu, 21 Nov 2024 00:25:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在微调过程中正确设置 pad token（而不是 eos）以避免模型无法预测 EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>pyspark 实现的 ALS 是如何处理每个用户-项目组合的多个评级的？</title>
      <link>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</link>
      <description><![CDATA[我观察到 ALS 的输入数据不需要每个用户-项目组合都有唯一的评分。
这是一个可重现的示例。
# 示例数据框
df = spark.createDataFrame([(0, 0, 4.0),(0, 1, 2.0), 
(1, 1, 3.0), (1, 2, 4.0), 
(2, 1, 1.0), (2, 2, 5.0)],[&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;])

df.show(50,0)
+----+----+------+
|user|item|rating|
+----+----+------+
|0 |0 |4.0 |
|0 |1 |2.0 |
|1 |1 |3.0 |
|1 |2 |4.0 |
|2 |1 |1.0 |
|2 |2 |5.0 |
+----+----+------+

可以看到，每个用户-商品组合只有一个评分（理想情况）。
如果我们将这个数据框传递到 ALS，它将为您提供如下预测：
# 拟合 ALS
from pyspark.ml.recommendation import ALS
als = ALS(rank=5, 
maxIter=5, 
seed=0,
regParam = 0.1,
userCol=&#39;user&#39;,
itemCol=&#39;item&#39;,
ratingsCol=&#39;rating&#39;,
nonnegative=True)
model = als.fit(df)

# 来自 als 的预测
all_comb = df.select(&#39;user&#39;).distinct().join(broadcast(df.select(&#39;item&#39;).distinct()))
predictions = model.transform(all_comb)

predictions.show(20,0)
+----+----+----------+
|user|item|prediction|
+----+----+----------+
|0 |0 |3.9169915 |
|0 |1 |2.031506 |
|0 |2 |2.3546133 |
|1 |0 |4.9588947 |
|1 |1 |2.8347554 |
|1 |2 |4.003007 |
|2 |0 |0.9958025 |
|2 |1 |1.0896711 |
|2 |2 |4.895194 |
+----+----+----------+

到目前为止，一切对我来说都是有意义的。但是如果我们有一个包含多个用户-项目评分组合的数据框，如下所示 -
# 示例数据框
df = spark.createDataFrame([(0, 0, 4.0), (0, 0, 3.5),
(0, 0, 4.1),(0, 1, 2.0),
(0, 1, 1.9),(0, 1, 2.1),
(1, 1, 3.0), (1, 1, 2.8),
(1, 2, 4.0),(1, 2, 3.6),
(2, 1, 1.0), (2, 1, 0.9),
(2, 2, 5.0),(2, 2, 4.9)],
[&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;])
df.show(100,0)
+----+----+------+
|user|item|rating|
+----+----+------+
|0 |0 |4.0 |
|0 |0 |3.5 |
|0 |0 |4.1 |
|0 |1 |2.0 |
|0 |1 |1.9 |
|0 |1 |2.1 |
|1 |1 |3.0 |
|1 |1 |2.8 |
|1 |2 |4.0 |
|1 |2 |3.6 |
|2 |1 |1.0 |
|2 |1 |0.9 |
|2 |2 |5.0 |
|2 |2 |4.9 |
+----+----+------+

如您在上面的数据框中看到的那样，一个用户-项目组合有多条记录。例如 - 用户“0”多次对项目“0”进行评分，即分别为 4.0、3.5 和 4.1。
如果我将此输入数据框传递给 ALS 会怎样？这会起作用吗？
我最初认为它不应该起作用，因为 ALS 应该根据用户-项目组合获得唯一评级，但当我运行它时，它起作用了，让我感到惊讶！
# 拟合 ALS
als = ALS(rank=5, 
maxIter=5, 
seed=0,
regParam = 0.1,
userCol=&#39;user&#39;,
itemCol=&#39;item&#39;,
ratingsCol=&#39;rating&#39;,
nonnegative=True)
model = als.fit(df)

# 来自 als 的预测
all_comb = df.select(&#39;user&#39;).distinct().join(broadcast(df.select(&#39;item&#39;).distinct()))
predictions = model.transform(all_comb)

predictions.show(20,0)
+----+----+----------+
|user|item|prediction|
+----+----+----------+
|0 |0 |3.7877638 |
|0 |1 |2.020348 |
|0 |2 |2.4364853 |
|1 |0 |4.9624424 |
|1 |1 |2.7311888 |
|1 |2 |3.8018093 |
|2 |0 |1.2490809 |
|2 |1 |1.0351425 |
|2 |2 |4.8451777 |
+----+----+----------+

为什么它会起作用？我以为它会失败，但它没有，而且还给了我预测。
我尝试查看研究论文、ALS 的有限源代码和互联网上可用的信息，但找不到任何有用的东西。
是取这些不同评分的平均值然后将其传递给 ALS 还是其他什么？
有人遇到过类似的事情吗？或者知道 ALS 内部如何处理此类数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</guid>
      <pubDate>Tue, 26 Apr 2022 10:37:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中创建混淆矩阵的图像</title>
      <link>https://stackoverflow.com/questions/65317685/how-to-create-image-of-confusion-matrix-in-python</link>
      <description><![CDATA[我是 Python 和机器学习的新手。我正在研究多类分类（3 个类）。我想将混淆矩阵保存为图像。现在，sklearn.metrics.confusion_matrix() 帮助我找到混淆矩阵，例如：
array([[35, 0, 6],
[0, 0, 3],
[5, 50, 1]])

接下来，我想知道如何将这个混淆矩阵转换为图像并保存为 png。]]></description>
      <guid>https://stackoverflow.com/questions/65317685/how-to-create-image-of-confusion-matrix-in-python</guid>
      <pubDate>Wed, 16 Dec 2020 05:11:00 GMT</pubDate>
    </item>
    <item>
      <title>加载 pickle NotFittedError：TfidfVectorizer - 词汇表不适合</title>
      <link>https://stackoverflow.com/questions/57213165/loading-pickle-notfittederror-tfidfvectorizer-vocabulary-wasnt-fitted</link>
      <description><![CDATA[多标签分类
我正在尝试使用 scikit-learn/pandas/OneVsRestClassifier/logistic 回归预测多标签分类。构建和评估模型有效，但尝试对新样本文本进行分类无效。
场景 1：
一旦我构建了一个模型，就使用名称 (sample.pkl) 保存该模型并重新启动我的内核，但是当我在对样本文本进行预测期间加载已保存的模型 (sample.pkl) 时，它给出了错误：
 NotFittedError：TfidfVectorizer - 词汇表不适合。

我构建了模型并评估了模型，然后我使用名称 sample.pkl 保存了该模型。我重启了内核，然后加载模型对样本文本进行预测 NotFittedError: TfidfVectorizer - 词汇表不适合
推理
import pickle,os
import collections
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import Counter
from nltk.corpus import stopwords
import json, nltk, re, csv, pickle
from sklearn.metrics import f1_score # 性能矩阵
from sklearn.multiclass import OneVsRestClassifier # 二元相关性
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split 
from sklearn.feature_extraction.text import TfidfVectorizer 
来自 sklearn.feature_extraction.text 导入 CountVectorizer
来自 sklearn.preprocessing 导入 MultiLabelBinarizer
来自 sklearn.model_selection 导入 train_test_split
来自 sklearn.linear_model 导入 LogisticRegression
stop_words = set(stopwords.words(&#39;english&#39;))

def cleanHtml(sentence):
&#39;&#39;&#39;&#39; 删除标签 &#39;&#39;&#39;
cleanr = re.compile(&#39;&lt;.*?&gt;&#39;)
cleantext = re.sub(cleanr, &#39; &#39;, str(sentence))
return cleantext

def cleanPunc(sentence): 
&#39;&#39;&#39; 函数用于清除单词中的任何标点符号或特殊字符 &#39;&#39;&#39;
cleaned = re.sub(r&#39;[?|!|\&#39;|&quot;|#]&#39;,r&#39;&#39;,sentence)
cleaned = re.sub(r&#39;[.|,|)|(|\|/]&#39;,r&#39; &#39;,cleaned)
cleaned = cleaned.strip()
cleaned = cleaned.replace(&quot;\n&quot;,&quot; &quot;)
return cleaned

def keepAlpha(sentence):
&quot;&quot;&quot; 保留 alpha 句子 &quot;&quot;&quot;
alpha_sent = &quot;&quot;
for word in sentence.split():
alpha_word = re.sub(&#39;[^a-z A-Z]+&#39;, &#39; &#39;, word)
alpha_sent += alpha_word
alpha_sent += &quot; &quot;
alpha_sent = alpha_sent.strip()
return alpha_sent

def remove_stopwords(text):
&quot;&quot;&quot; 删除停用词 &quot;&quot;&quot;
no_stopword_text = [w for w in text.split() if not w in stop_words]
return &#39; &#39;.join(no_stopword_text)

test1 = pd.read_csv(&quot;C:\\Users\\abc\\Downloads\\test1.csv&quot;)
test1.columns

test1.head()
siNo plot movie_name gender_new
1 故事以 Hannah 开始... 唱歌 [戏剧,青少年]
2 Debbie 最喜欢的乐队是 Dream... 最忠实的粉丝 [戏剧]
3 这个祖鲁家庭的故事是... 回来，非洲 [戏剧,纪录片]

出现错误
当我对示例文本进行推理时，我在这里出现错误
def infer_tags(q):
q = cleanHtml(q)
q = cleanPunc(q)
q = keepAlpha(q)
q = remove_stopwords(q)
multilabel_binarizer = MultiLabelBinarizer()
tfidf_vectorizer = TfidfVectorizer()
q_vec = tfidf_vectorizer.transform([q])
q_pred = clf.predict(q_vec)
return multilabel_binarizer.inverse_transform(q_pred)

for i in range(5):
print(i)
k = test1.sample(1).index[0] 
print(&quot;电影：&quot;, test1[&#39;movie_name&#39;][k], &quot;\n预测类型：&quot;, infer_tags(test1[&#39;plot&#39;][k])), print(&quot;实际类型：&quot;,test1[&#39;genre_new&#39;][k], &quot;\n&quot;)


已解决
我解决了将 tfidf 和 multibiniraze 保存到 pickle 模型中的问题
从 sklearn.externals 导入 joblib
pickle.dump(tfidf_vectorizer, open(&quot;tfidf_vectorizer.pickle&quot;, &quot;wb&quot;))
pickle.dump(multilabel_binarizer, open(&quot;multibinirizer_vectorizer.pickle&quot;, &quot;wb&quot;))
vectorizer = joblib.load(&#39;/abc/downloads/tfidf_vectorizer.pickle&#39;)
multilabel_binarizer = joblib.load(&#39;/abc/downloads/multibinirizer_vectorizer.pickle&#39;)

def infer_tags(q):
q = cleanHtml(q)
q = cleanPunc(q)
q = keepAlpha(q) 
q = remove_stopwords(q)
q_vec = vectorizer .transform([q])
q_pred = rf_model.predict(q_vec)
return multilabel_binarizer.inverse_transform(q_pred)

我通过以下链接找到了解决方案
,https://stackoverflow.com/questions/32764991/how-do-i-store-a-tfidfvectorizer-for-future-use-in-scikit-learn&gt;]]></description>
      <guid>https://stackoverflow.com/questions/57213165/loading-pickle-notfittederror-tfidfvectorizer-vocabulary-wasnt-fitted</guid>
      <pubDate>Fri, 26 Jul 2019 04:21:05 GMT</pubDate>
    </item>
    <item>
      <title>Keras 进度条中的准确度是什么意思？</title>
      <link>https://stackoverflow.com/questions/52559086/what-does-the-accuracy-mean-in-the-keras-progress-bar</link>
      <description><![CDATA[在 Keras 中，您将获得类似以下内容：
Epoch 1/1
60000/60000 [==============================] - 297s 5ms/step - 损失：0.7048 - acc：0.7669

60000/60000 [==============================] - 179s 3ms/step
训练集：
acc：94.60%

10000/10000 [================================] - 30s 3ms/step
测试集：
acc： 95.10%

但我是这样拟合的：

model.fit(X_train, oh_y_train,
batch_size=512,
epochs=1,
verbose=1)

.fit() 方法中没有验证数据，它从第 1 个时期测量准确率的是什么？
最终准确率差别很大。]]></description>
      <guid>https://stackoverflow.com/questions/52559086/what-does-the-accuracy-mean-in-the-keras-progress-bar</guid>
      <pubDate>Fri, 28 Sep 2018 16:07:45 GMT</pubDate>
    </item>
    <item>
      <title>ALS（交替最小二乘）算法对用户的多个排名</title>
      <link>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</link>
      <description><![CDATA[嗨，经过大量研究，我们决定使用 Google Cloud 基础架构，并在我们的产品推荐系统中使用 ALS 算法（一种协同过滤方法 - https://cloud.google.com/solutions/recommendations-using-machine-learning-on-compute-engine#Training-the-models ），详细说明如下：
我们有两种类型的客户。第一类是附近销售产品的公司，第二类是打算从这些公司购买产品的消费者

每个消费者都可以搜索附近的公司或按行业搜索公司（例如杂货店、干洗店、肉店等）
当消费者找到一家公司时，他/她可以执行以下操作（他可以一次执行多项操作）
2.1. 仅查看公司简介
2.2. 将公司添加到收藏夹
2.3. 开始与公司聊天
2.4. 从公司下订单
2.5.给公司评分和评论

所以我不明白的是：上面描述的每件商品都被确定为我们数据库中的某些评分列，例如：
查看公司简介：10 分
从公司下订单：20 分
给公司打星或评论：20 分
因此，对于同一用户，每件商品都是单独的评分。
在我们的数据库中，对于用户-公司对，可能会有超过 1 行
例如：
第 1 行：user18-company18-10pts（查看过一次个人资料）
第 2 行：user18-company18-20pts（从公司下订单）
第 3 行：user18-company19-10pts
我不确定这个算法，它是计算该用户对同一家公司的所有评分的总和（我到底想要什么）还是只是寻找单个用户对单个公司的评分的单行？（我想要的是这个 ALS 算法来总结该用户-公司对的第 1 行和第 2 行）
有人知道吗？这对我们的推荐系统非常重要。因为我正在寻找的算法需要计算用户所有评分的总和，以便推荐另一家公司。因为我们的商业模式与电影评分系统不同
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</guid>
      <pubDate>Tue, 01 May 2018 09:53:59 GMT</pubDate>
    </item>
    </channel>
</rss>