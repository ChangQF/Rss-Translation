<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 30 Apr 2024 15:12:30 GMT</lastBuildDate>
    <item>
      <title>如何应用神经网络的总势能原理来解决结构力学问题？</title>
      <link>https://stackoverflow.com/questions/78409373/how-implement-a-total-potential-energy-principle-for-a-neural-network-to-solve-s</link>
      <description><![CDATA[我需要创建一个神经网络来解决负载下的板的弹性问题，并且我想实现一个基于物理的神经网络，通过最小化基于总势能原理的损失函数来解决问题。
我使用总势能原理作为损失函数，该原理指出结构的平衡配置是最小化系统总能量的配置。所以我规定损失函数=最小势能，我进行了积分，然后，为了最小化它，我规定总势能本身相对于我想要恢复的变量的导数等于 0。我得到了一个 A*x=b 类型的方程，其中 x 是神经网络的权重，但它不起作用。也许积分过程或损失函数构造有问题。]]></description>
      <guid>https://stackoverflow.com/questions/78409373/how-implement-a-total-potential-energy-principle-for-a-neural-network-to-solve-s</guid>
      <pubDate>Tue, 30 Apr 2024 14:50:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么学习率需要这么低？[关闭]</title>
      <link>https://stackoverflow.com/questions/78409111/why-does-learning-rate-need-to-be-so-low</link>
      <description><![CDATA[我是机器学习新手，所以我尝试在多元线性回归中使梯度下降。函数很简单，就是z = x + y。但由于某种原因，学习率需要为 1e-16 才能发挥作用。有什么想法吗？
这是代码
将 numpy 导入为 np

case_x = np.array([[1,1], [2,3], [5,6], [1,5], [2,2], [100,100], [101,102], [500,500], [ 1000,1000],[100000000,100000000]])
case_y = np.array([2,5,11,6,4,200,203,1000,2000,200000000])


cur_w = np.array([0,0])
当前_b = 0
def f(x,w,b)：
    返回 np.dot(w,x) + b
    
定义 J(w,b)：
    总和_ = 0
    对于范围 (0,10) 内的 i：
        sum_ += pow(f(cases_x[i], w,b) - Cases_y[i],2)
    总和_ /= 20
    返回总和_
    

def derivitive_w(w):
    d = np.array([])
    对于范围 (0,2) 内的 i：
        电流_d = 0
        对于范围 (0,10) 内的 j：
            cur_d += (f(cases_x[j], cur_w, cur_b) - Cases_y[j]) * Cases_x[j][i]
            
        cur_d /= 10
        #print(cur_d)
        d = np.append(d,cur_d)
    #打印（d）
    返回d
def derivitive_b(b):
    tmp_b = 0
    对于范围 (0,10) 内的 i：
        tmp_b += f(cases_x[i], cur_w, cur_b) - Cases_y[i]
    tmp_b /= 10
    返回tmp_b
对于范围 (0,100) 内的 i：
    tmp_w = derivitive_w(cur_w)
    tmp_b = derivitive_b(cur_b)
    
    cur_b = cur_b - (0.0000000000000001 * tmp_b)
    cur_w = cur_w - (0.0000000000000001 * tmp_w)
inpt = 列表(map(int,input().split()))
arr = np.array([inpt[0],inpt[1]])
打印（int（f（arr，cur_w，cur_b）））
]]></description>
      <guid>https://stackoverflow.com/questions/78409111/why-does-learning-rate-need-to-be-so-low</guid>
      <pubDate>Tue, 30 Apr 2024 14:11:05 GMT</pubDate>
    </item>
    <item>
      <title>RNN实现方程问题的简单BPTT</title>
      <link>https://stackoverflow.com/questions/78408682/simple-bptt-for-rnn-implementation-equation-question</link>
      <description><![CDATA[我试图从头开始实现简单的 RNN，以了解每个实现背后的计算步骤。前向传播似乎很简单，但后向传播似乎非常困难。特别是维度不匹配……
def rnn_forward(self, e) :
for t in range(self.T_x - 1) :
# single (n_x, n_m) m 是时间上的训练集大小
xt = self.cache[&#39;X&#39;][:, :, t]
# 上一步的隐藏状态
a_prev = self.cache[&#39;A&#39;][:, :, t]
# 下一步的隐藏状态通过以下权重和偏差计算。
a_next = tanh(np.dot(self.parameters[&#39;W_aa&#39;], a_prev) + np.dot(self.parameters[&#39;W_ax&#39;], xt) + self.parameters[&#39;b_a&#39;])
# 使用 softmax 作为最终激活
yt_pred = softmax(np.dot(self.parameters[&#39;W_ya&#39;], a_next) + self.parameters[&#39;b_y&#39;])
self.cache[&#39;A&#39;][:, :, (t + 1)] = a_next
self.cache[&#39;Y_pred&#39;][:, :, t] = yt_pred

def rnn_backward(self, e):
# 成本函数导数
self.cache[&#39;dY_pred&#39;] = - (self.cache[&#39;Y&#39;] / self.cache[&#39;Y_pred&#39;])
# 初始化 da
da_next = np.zeros((self.n_a, self.m))
for t in reversed(range(self.T_x - 1)) :
# 维度似乎为 (n_y, n_m)
print(self.cache[&#39;dY_pred&#39;][:, :, t].shape)
# 维度似乎也是 (n_y, n_m)
print(softmax_backward(np.dot(self.parameters[&#39;W_ya&#39;], self.cache[&#39;A&#39;][:, :, t]) + self.parameters[&#39;b_y&#39;]).shape)
# 以下计算会有问题吗？ da_prev 预计有 dim (n_a, n_m)，但 (n_y, n_m) 和 (n_y, n_m) 的点不给我 (n_a, n_m)
da_prev = np.dot(self.cache[&#39;dY_pred&#39;][:, :, t], np.dot(self.parameters[&#39;W_ya&#39;].T, softmax_backward(np.dot(self.parameters[&#39;W_ya&#39;], self.cache[&#39;A&#39;][:, :, t]) + self.parameters[&#39;b_y&#39;]))) + da_next
self.cache[&#39;dW_ya&#39;] += np.dot(self.cache[&#39;dY_pred&#39;][:, :, t], np.dot(softmax_backward(np.dot(self.parameters[&#39;W_ya&#39;], self.cache[&#39;A&#39;][:, :, t]) + self.parameters[&#39;b_y&#39;]), self.cache[&#39;A&#39;][:, :, t].T))
self.cache[&#39;db_y&#39;] += np.dot(self.cache[&#39;dY_pred&#39;][:, :, t], softmax_backward(np.dot(self.parameters[&#39;Wya&#39;], self.cache[&#39;A&#39;][:, :, t]) + self.parameters[&#39;b_y&#39;]))
self.cache[&#39;dW_aa&#39;] += np.dot(da_prev, np.dot(tanh_backward(np.dot(self.parameters[&#39;W_aa&#39;], self.cache[&#39;A&#39;][:, :, (t - 1)]) + np.dot(self.parameters[&#39;W_ax&#39;], self.cache[&#39;X&#39;][:, :, t]) + self.parameters[&#39;b_a&#39;]), self.cache[&#39;A&#39;][:, :, (t - 1)].T))
self.cache[&#39;dW_ax&#39;] += np.dot(da_prev, np.dot(tanh_backward(np.dot(self.parameters[&#39;W_aa&#39;], self.cache[&#39;A&#39;][:, :, (t - 1)]) + np.dot(self.parameters[&#39;W_ax&#39;], self.cache[&#39;X&#39;][:, :, t]) + self.parameters[&#39;b_a&#39;]), self.cache[&#39;X&#39;][:, :, t].T))
self.cache[&#39;db_a&#39;] += np.dot(da_prev, tanh_backward(np.dot(self.parameters[&#39;W_aa&#39;], self.cache[&#39;A&#39;][:, :, (t - 1)]) + np.dot(self.parameters[&#39;W_ax&#39;], self.cache[&#39;X&#39;][:, :, t]) + self.parameters[&#39;b_a&#39;]))
da_next = np.dot(da_prev, np.dot(self.parameters[&#39;W_aa&#39;].T, tanh_backward(np.dot(self.parameters[&#39;W_aa&#39;], self.cache[&#39;A&#39;][:, :, (t - 1)]) + np.dot(self.parameters[&#39;W_ax&#39;], self.cache[&#39;X&#39;][:, :, t]) + self.parameters[&#39;b_a&#39;])))

正如代码中的注释，我似乎无法正确实现反向传播。 da_prev 的维度不匹配，无法正确得出 (n_a, n_m)... 我是不是漏掉了什么？
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78408682/simple-bptt-for-rnn-implementation-equation-question</guid>
      <pubDate>Tue, 30 Apr 2024 12:58:33 GMT</pubDate>
    </item>
    <item>
      <title>迁移学习：图像分类精度高，但 val_loss 非常高。为什么？</title>
      <link>https://stackoverflow.com/questions/78408313/transfer-learning-image-classification-high-accuracy-but-very-high-val-loss-wh</link>
      <description><![CDATA[我正在使用 Tensorflow 和 Keras 使用 Resnet_50 进行迁移学习。我遇到的问题是，我的模型似乎在准确性方面表现良好，但我的 val_loss 非常高，并且当我尝试进行预测时，准确性非常低。
以下是代码的相关部分：
# 创建一个带有数据增强的 ImageDataGenerator 用于训练
data_generator = keras.preprocessing.image.ImageDataGenerator(
    重新缩放=1./255，
    验证分割=0.5，
    rotation_range=30, # 随机旋转
    width_shift_range=0.2, # 水平移动
    height_shift_range=0.2, # 垂直位移
    sher_range=0.2, # 剪切变换
    Zoom_range=0.2, # 缩放
    Horizo​​ntal_flip=True, # 水平翻转
）

# 加载并预处理训练数据
train_data_flow = data_generator.flow_from_directory(
    数据集_路径，
    target_size=(224, 224), # 将图像大小调整为 224x224
    批量大小=32，
    class_mode=&#39;分类&#39;,
    subset=&#39;training&#39; # 使用训练子集
）

# 加载并预处理验证数据
val_data_flow = data_generator.flow_from_directory(
    数据集_路径，
    target_size=(224, 224), # 将图像大小调整为 224x224
    批量大小=32，
    class_mode=&#39;分类&#39;,
    subset=&#39;validation&#39; # 使用验证子集
）

加载模型
# 从 TensorFlow Hub 加载模型
model_url =“https://tfhub.dev/tensorflow/resnet_50/feature_vector/1”
hub_layer = hub.KerasLayer(model_url, input_shape=(224, 224, 3) , trainable=False)

# 创建一个带有 dropout 和批量归一化的序列模型
模型 = keras.Sequential([
    集线器层，
    Layers.Dropout(0.2), # 降低辍学率
    层.Dense(256, 激活=&#39;relu&#39;),
    Layers.BatchNormalization(), # 批量归一化
    图层.Dropout(0.5), # 丢弃
    层.Dense(9, 激活=&#39;softmax&#39;)
]）

# 构建顺序模型
模型.build((无, 224, 224, 3))

# 模型总结
模型.summary()

然后我编译：
# 编译模型
模型.编译(
    优化器=&#39;亚当&#39;,
    损失=&#39;分类交叉熵&#39;，
    指标=[&#39;准确性&#39;]
）

最后：
# 通过提前停止来拟合模型
历史=模型.fit(
    训练数据流，
    验证数据=val_data_flow，
    epochs=15, # 纪元数,
）

以下是纪元：
纪元 1/15
81/81 [================================] - 489s 6s/步 - 损失：0.3773 - 准确度：0.8832 - val_loss：0.7994 - val_accuracy：0.7476
纪元 2/15
81/81 [================================] - 489s 6s/步 - 损失：0.3316 - 准确度：0.8980 - val_loss ：0.8229 - val_accuracy：0.7378
纪元 3/15
81/81 [================================] - 488s 6s/步 - 损失：0.3468 - 准确度：0.8879 - val_loss ：0.8221 - val_accuracy：0.7362
纪元 4/15
81/81 [================================] - 489s 6s/步 - 损失：0.3148 - 准确度：0.9011 - val_loss ：0.8380 - val_accuracy：0.7362
纪元 5/15
81/81 [================================] - 488s 6s/步 - 损失：0.3250 - 准确度：0.8972 - val_loss ：0.7680 - val_accuracy：0.7409
纪元 6/15
81/81 [================================] - 491s 6s/步 - 损失：0.3100 - 准确度：0.9026 - val_loss ：0.7220 - val_accuracy：0.7616
纪元 7/15
81/81 [================================] - 491s 6s/步 - 损失：0.2844 - 准确度：0.9120 - val_loss ：0.7259 - val_accuracy：0.7651
纪元 8/15
81/81 [================================] - 490s 6s/步 - 损失：0.2811 - 准确度：0.9007 - val_loss ：0.7722 - val_accuracy：0.7511
纪元 9/15
81/81 [================================] - 490s 6s/步 - 损失：0.2689 - 准确度：0.9167 - val_loss ：0.7943 - val_accuracy：0.7433
纪元 10/15
81/81 [================================] - 预计到达时间：0 秒 - 损失：0.2569 - 准确度：0.9182

预测：
测试集上的模型准确度：0.2021069059695669
]]></description>
      <guid>https://stackoverflow.com/questions/78408313/transfer-learning-image-classification-high-accuracy-but-very-high-val-loss-wh</guid>
      <pubDate>Tue, 30 Apr 2024 11:50:46 GMT</pubDate>
    </item>
    <item>
      <title>如何输出估计的工作量结果=数据集长度[关闭]</title>
      <link>https://stackoverflow.com/questions/78407801/how-to-output-the-estimated-effort-result-dataset-length</link>
      <description><![CDATA[如何输出估计工作量结果 = 数据集长度？
在此处输入图片描述
用于计算工作量估计，可使用平均值、中位数或 IRWA（逆秩加权平均值）方法调整类比适应。所使用的类比适应的选择由程序中预先设置的参数决定。计算估计工作量的过程针对数据集中的每个数据进行。在每次迭代中，程序都会检查类比适应参数（self.adaptationAnalogy）的值。如果值为 1，则将使用 mean_k 方法计算该数据的估计工作量。如果值为 2，则将使用 median_k 方法，如果值为 3，则将使用 irwa_k 方法。]]></description>
      <guid>https://stackoverflow.com/questions/78407801/how-to-output-the-estimated-effort-result-dataset-length</guid>
      <pubDate>Tue, 30 Apr 2024 10:12:29 GMT</pubDate>
    </item>
    <item>
      <title>转换深度学习 esrgan 模型时输入张量形状不匹配</title>
      <link>https://stackoverflow.com/questions/78407762/input-tensor-shape-mismatch-when-converting-deep-learning-esrgan-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78407762/input-tensor-shape-mismatch-when-converting-deep-learning-esrgan-model</guid>
      <pubDate>Tue, 30 Apr 2024 10:05:56 GMT</pubDate>
    </item>
    <item>
      <title>我无法将数据插入/更新到 postgres 数据库表中[关闭]</title>
      <link>https://stackoverflow.com/questions/78407703/i-am-unable-to-insert-update-data-into-postgres-database-table</link>
      <description><![CDATA[我已经在 postgres 数据库中创建了一个表。我正在尝试使用 api 更新或插入它。正确编写插入查询。
可能是什么原因？]]></description>
      <guid>https://stackoverflow.com/questions/78407703/i-am-unable-to-insert-update-data-into-postgres-database-table</guid>
      <pubDate>Tue, 30 Apr 2024 09:56:37 GMT</pubDate>
    </item>
    <item>
      <title>我什么时候应该进行特征选择？</title>
      <link>https://stackoverflow.com/questions/78407668/when-should-i-do-characteristic-selection</link>
      <description><![CDATA[我正在运行一些机器学习算法来训练模型。
到目前为止，我一直在制作相关矩阵，以便选择与目标变量相关性最高的特征。
我在网上读到，除非我运行逻辑回归，否则不需要进行此选择。这是真的吗？
我运行的算法是逻辑回归、决策树、SVM、KNN 和朴素贝叶斯。
我是否应该使用具有除 Logistic 回归之外的所有算法的所有特征的训练集以及仅包含 Logistic 回归最相关变量的另一个版本？]]></description>
      <guid>https://stackoverflow.com/questions/78407668/when-should-i-do-characteristic-selection</guid>
      <pubDate>Tue, 30 Apr 2024 09:50:21 GMT</pubDate>
    </item>
    <item>
      <title>如何使 XGBoost 在分层分类器中工作</title>
      <link>https://stackoverflow.com/questions/78407603/how-to-make-xgboost-work-in-a-hierarchical-classifier</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78407603/how-to-make-xgboost-work-in-a-hierarchical-classifier</guid>
      <pubDate>Tue, 30 Apr 2024 09:37:37 GMT</pubDate>
    </item>
    <item>
      <title>当时间序列元数据变化时如何构建多元时间序列</title>
      <link>https://stackoverflow.com/questions/78406347/how-to-structure-multivariate-timeseries-when-timeseries-metadata-varies</link>
      <description><![CDATA[我根据多元、地理、时间序列数据进行预测。
我的数据包括每种产品的历史价格和生产数量，不同的地方生产和销售不同的产品组。例如：

&lt;标题&gt;

旧金山
第 1 个月
第 2 个月
第 3 个月


&lt;正文&gt;

小麦





美元价格
3
4
3


生产数量T
200
100
150


苹果





美元价格
1
2
0.8


生产数量T
20
10
50




&lt;标题&gt;

布里斯班
第 1 个月
第 2 个月
第 3 个月


&lt;正文&gt;

米饭





美元价格
3
4
3


生产数量T
200
100
150


香蕉





美元价格
5
4
3


生产数量T
200
300
450



每个地点的产品列表都不同（日期范围和长度也不同，但这些问题的解决方案都有详细记录）。我正在寻找异常情况并预测消费类别。我应该如何构建它来进行训练和预测？
&lt;小时/&gt;
我的猜测（您可能可以停止阅读这里，因为这是新手的猜测，但建议包括迄今为止完成的工作）：
我可以使用的一种方法是标准化并平均所有价格。 （收集每个区域最重要产品的价格。）但这会消除很多细节。我可以根据种植的数量来猜测每种产品的重要性，但这通常会产生误导。价格和产量之间的关系和紧张关系很重要。
我想蛮力方法是在每个示例中包含每个产品，并为未在某个区域交易的任何产品的时间序列使用一个空令牌。但这将是一个非常稀疏且庞大的数据集。
我可以说product1=“大米w2v嵌入”，product2=“香蕉w2v嵌入”，price_timeseries1=[大米价格]，price_timeseries2=[香蕉价格]（加上产量，以及在另一个例子中，小麦和苹果也是如此）？有任何模型能够解释这一点吗？
我最好的猜测是将产品分类，例如，将大米和小麦分类为“碳水化合物”，然后标准化，然后对每个类别的产品进行平均。 （我确实有一个层次结构，但是有没有办法可以导出或学习分类？）这在统计上有效吗？
源地理数据以行政单位（城镇边界多边形）为单位。我正在考虑选择 (a) 将所有内容转换为 100km2 网格，或 (b) cos(lat).cos(lon), cos(lat).sin(lon), sin(lat) 形状质心 lon/lat，获取 3D 中的（缩放）坐标，加上面积。选项（b）更容易，除了管理单位随着时间的推移而变化，所以我必须将旧的数字重新分配到最新的边界。
（我还将添加位置元数据，例如国家/地区、人口、季节时间序列等）
有什么建议/预感吗？或者只是从单变量开始并构建？
我（也许天真地）正在考虑 ARIMA、XGBoost、LSTM、timeseries变压器，也许Mamba，如果相关的话。鉴于我完全缺乏经验，我怀疑 XGBoost 可能是复杂性/功能和新手超参数调整技能的最佳点，尽管我会尝试更复杂的架构，因为我知道树会错过生产和价格之间的相互作用，这在这里很重要。&lt; /p&gt;
非常感谢您提供任何提示。]]></description>
      <guid>https://stackoverflow.com/questions/78406347/how-to-structure-multivariate-timeseries-when-timeseries-metadata-varies</guid>
      <pubDate>Tue, 30 Apr 2024 05:08:43 GMT</pubDate>
    </item>
    <item>
      <title>NefTune 在 Transformers 上获得 0 训练损失</title>
      <link>https://stackoverflow.com/questions/78404768/neftune-receiving-0-training-loss-on-transformers</link>
      <description><![CDATA[我基本上是在尝试使用 Neftune 微调我的模型。模型基于土耳其语言。但在那里我的训练损失为零。我尝试过另一种模型，例如 Turkish-GPT2 没有问题一切都好。我认为模型可能有问题。我不知道如何处理这个问题。
加载模型：
从变压器导入 AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(“asafaya/kanarya-750m”)
模型 = AutoModelForCausalLM.from_pretrained(“asafaya/kanarya-750m”)

v3_prompt =“”“Aşağıda，daha fazla bağlam sağlayan bir girdiyle eşleştirilmiş，bir görevi açıklayan bir talimat bulunmaktadır。请注意，请确保您的设备正常工作。

＃＃＃ 输入：
{}

＃＃＃ 指示：
{}

＃＃＃ 回复：
{}
”“”

更改格式提示：
EOS_TOKEN = tokenizer.eos_token # 必须添加EOS_TOKEN
defformatting_prompts_func（示例）：
    输入=示例[“输入”]
    说明 = 示例[&#39;说明&#39;]
    输出=示例[“响应”]
    文本=[]
    对于 zip 中的输入、指令、输出（输入、指令、输出）：
        # 必须添加EOS_TOKEN，否则你的一代将永远延续下去！
        text = v3_prompt.format(输入、指令、输出) + EOS_TOKEN
        文本.append(文本)
    返回 {“文本”； ：文本，}
经过

从数据集导入数据集

数据集 = Dataset.from_pandas(数据[:40000])
数据集= dataset.map(formatting_prompts_func,batched=True)

Neftune：
from trl import SFTTrainer
从 Transformers 导入 TrainingArguments

trainer_2 = SFTTrainer(
    型号=型号，
    train_dataset=数据集，
    dataset_text_field=&quot;文本&quot;,
    最大序列长度=512，
    neftune_noise_alpha=5,
    包装=假，
    args = 训练参数(
        per_device_train_batch_size = 1, # 批量大小
        gradient_accumulation_steps = 2, # 梯度累积步数
        热身步骤 = 5,
        最大步数 = 80,
        学习率 = 2e-4,
        fp16 = 假，
        bf16 = torch.cuda.is_bf16_supported(),
        日志记录步骤 = 1,
        优化=“adamw_8bit”，
        权重衰减 = 0.01,
        lr_scheduler_type =“线性”，
        种子=3407，
        输出目录=“输出”，
    ),
）
trainer_2.train()

输出：
&lt;前&gt;&lt;代码&gt; [80/80 00:25，纪元 0/1]
步数训练损失
1 0.000000
2 0.000000
3 0.000000
4 0.000000
5 0.000000
6 0.000000
7 0.000000
8 0.000000
9 0.000000
10 0.000000
11 0.000000
12 0.000000
13 0.000000
14 0.000000
15 0.000000
16 0.000000
17 0.000000
18 0.000000
19 0.000000
20 0.000000
21 0.000000
22 0.000000
23 0.000000
24 0.000000
25 0.000000
26 0.000000
27 0.000000
28 0.000000
29 0.000000
30 0.000000
31 0.000000
32 0.000000
33 0.000000
34 0.000000
35 0.000000
36 0.000000
37 0.000000
38 0.000000
39 0.000000
40 0.000000
41 0.000000
42 0.000000
43 0.000000
44 0.000000
45 0.000000
46 0.000000
47 0.000000
48 0.000000
49 0.000000
50 0.000000
51 0.000000
52 0.000000
53 0.000000
54 0.000000
55 0.000000
56 0.000000
57 0.000000
58 0.000000
59 0.000000
60 0.000000
61 0.000000
62 0.000000
63 0.000000
64 0.000000
65 0.000000
66 0.000000
67 0.000000
68 0.000000
69 0.000000
70 0.000000
71 0.000000
72 0.000000
73 0.000000
74 0.000000
75 0.000000
76 0.000000
77 0.000000
78 0.000000
79 0.000000
80 0.000000
TrainOutput（global_step = 80，training_loss = 0.0，metrics = {&#39;train_runtime&#39;：25.3789，&#39;train_samples_per_second&#39;：6.304，&#39;train_steps_per_second&#39;：3.152，&#39;total_flos&#39;：117937578909696.0，&#39;train_loss&#39;：0.0，&#39;epoch&#39;： 0.004})
]]></description>
      <guid>https://stackoverflow.com/questions/78404768/neftune-receiving-0-training-loss-on-transformers</guid>
      <pubDate>Mon, 29 Apr 2024 19:23:04 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法在 Designer 中使用在 Azure AutoML 中创建的 ML 模型？</title>
      <link>https://stackoverflow.com/questions/78403537/is-there-a-way-to-use-a-ml-model-created-in-azure-automl-within-designer</link>
      <description><![CDATA[我知道我可以在 Azure Designer 中创建自定义代码模块，但是有没有办法连接我在 AutoML 中本机创建的 ML 模型？
AutoML 模型正在使用 XGBoost，这似乎不是 Designer 的 ML 组件功能下的选项。我的目标是创建一个低代码解决方案，因此我不想使用自定义 Python 代码组件。
有什么想法吗？
使用 AutoML 构建模型，需要连接到 Designer 中的现有数据管道]]></description>
      <guid>https://stackoverflow.com/questions/78403537/is-there-a-way-to-use-a-ml-model-created-in-azure-automl-within-designer</guid>
      <pubDate>Mon, 29 Apr 2024 14:52:21 GMT</pubDate>
    </item>
    <item>
      <title>通过 Keras 训练同时检查不同类型的数据</title>
      <link>https://stackoverflow.com/questions/78348894/simultaneously-going-over-different-kinds-of-data-with-keras-training</link>
      <description><![CDATA[在回归任务中，我得到以下数据：

具有已知标签的输入向量。 MSE损失应该用在预测和标签之间。
没有标签的输入向量对，已知模型应给出相似的结果。应在两个预测之间使用 MSE 损失。

同时将这两种数据拟合 Keras 模型的正确方法是什么？
理想情况下，我希望火车循环以交错的方式迭代这两种类型 - 一个有监督的（1）批次，然后是一个自我监督的（2）批次，然后再次监督，等等。
如果重要的话，我正在使用 Jax 后端。 Keras 版本 3.2.1。]]></description>
      <guid>https://stackoverflow.com/questions/78348894/simultaneously-going-over-different-kinds-of-data-with-keras-training</guid>
      <pubDate>Thu, 18 Apr 2024 16:05:11 GMT</pubDate>
    </item>
    <item>
      <title>模块“keras.layers”没有属性“实验性”</title>
      <link>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</link>
      <description><![CDATA[你好，我试图调整我的数据集的大小和比例，如下所示，但我遇到了这个错误：
AttributeError：模块“keras.layers”没有属性“experimental”
&lt;前&gt;&lt;代码&gt;
resize_and_rescale= tf.keras.Sequential([
    图层.实验.预处理.调整大小(IMAGE_SIZE,IMAGE_SIZE),
    层.实验.预处理.重新缩放(1.0/255)
]）

]]></description>
      <guid>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</guid>
      <pubDate>Wed, 14 Dec 2022 00:43:49 GMT</pubDate>
    </item>
    <item>
      <title>识别图像中钢筋上的锈迹</title>
      <link>https://stackoverflow.com/questions/59048488/identifying-rust-on-steel-bars-in-images</link>
      <description><![CDATA[我正在探索在类似于此处链接的图像中识别钢制品上生锈的方法：生锈钢筋。 
目前，我只是对整个图像应用简单的图像处理技术，以拾取特定色调的红色/棕色/橙色来识别锈迹。我遇到的问题是，这也将背景区域（任何非钢的东西）识别为生锈，这是可以预料的。因此，我需要消除这种噪音，以使我的结果更加可靠。
我研究过的一个想法是分割图像中的钢结构，并将简单的图像处理技术仅应用于钢，这将消除背景中的任何噪音。然而，我是计算机视觉的新手，我不知道从哪里开始这样的事情。
所以，我的问题是，对于此类任务，您是否会推荐任何特定的图像分割技术？有没有我可以使用的资源来学习如何应用此类技术？
我的一个想法是在 keras 中训练图像分割模型，但我以前没有这样做过，因此需要大量时间来学习如何做到这一点并准备数据。
另外，回到最初的问题，是否有其他方法可以在不接收背景噪音的情况下隔离钢结构上的锈迹？&lt;​​/p&gt;

谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/59048488/identifying-rust-on-steel-bars-in-images</guid>
      <pubDate>Tue, 26 Nov 2019 10:12:25 GMT</pubDate>
    </item>
    </channel>
</rss>