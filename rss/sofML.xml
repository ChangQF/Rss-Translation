<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 20 May 2024 12:28:58 GMT</lastBuildDate>
    <item>
      <title>使用 Minirocket 分类的自定义数据集的问题</title>
      <link>https://stackoverflow.com/questions/78506562/problems-with-custom-dataset-using-minirocket-classification</link>
      <description><![CDATA[我正在做一个更大的学校项目，尝试使用 Minirocket/Rocket 对时间序列测量进行分类。我的训练数据由一个包含测量值的 1D 矩阵和一个包含相应标签 (0/1) 的单独 1D 矩阵组成。矩阵长度相等，因此每个测量值都有一个标签。我尝试运行该程序，该程序与其他样本数据兼容，但我总是收到以下错误：“ValueError：发现样本数量不一致的输入变量：[1, 321408]”。我在这里做错了什么？我是否必须以不同的方式格式化训练数据，或者是否还有其他 MiniRocket 设置需要我调整？
错误由以下代码行触发：classifier.fit(train_x_transform, train_y)
这是我迄今为止尝试运行的完整代码：
import pandas as pd
from sklearn.metrics import classes_report
from sktime.transformations.panel.rocket import MiniRocket
from sklearn.linear_model import RidgeClassifierCV
import numpy as np

# 加载数据
def load_data(file_path):
data = pd.read_csv(file_path)
X = data[&#39;sensor_values_final&#39;].values
y = data[&#39;labels&#39;].values
assert len(X) == len(y), &quot;Length of X and y do not match&quot;
返回 X、y

train_x、train_y = load_data(r&#39;csv\TrainingData_2024-05-18_18-21-46_train.csv&#39;)
test_x、test_y = load_data(r&#39;csv\TrainingData_2024-05-18_18-21-46_test.csv&#39;)

# 转换数据
minirocket = MiniRocket(10_000) # 默认情况下，MiniRocket 使用 ~10,000 个内核
minirocket.fit(train_x)
train_x_transform = minirocket.transform(train_x)
test_x_transform = minirocket.transform(test_x)

# 训练模型
classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))
classifier.fit(train_x_transform, train_y)

#评估模型
train_y_pred = classifier.predict(train_x_transform)
test_y_pred = classifier.predict(test_x_transform)

print(&quot;测试集性能：&quot;)
print(classification_report(test_y, test_y_pred))
]]></description>
      <guid>https://stackoverflow.com/questions/78506562/problems-with-custom-dataset-using-minirocket-classification</guid>
      <pubDate>Mon, 20 May 2024 12:21:28 GMT</pubDate>
    </item>
    <item>
      <title>电子邮件数据的多目标分类</title>
      <link>https://stackoverflow.com/questions/78506372/multi-target-classification-on-email-data</link>
      <description><![CDATA[我有电子邮件数据，其中有以下几列：

&lt;标题&gt;

文本
类别
电子邮件类型


&lt;正文&gt;

您的 Zomato 在线订单收据退款优惠...
catergoty_1
email_type_13


您的 Myntra 在线订单成功......
category_2
email_type_32



类别和电子邮件类型的分布如下：
类别计数
类别_3 32000
类别_1 6000
类别_2 22

df[df[&#39;Category&#39;]==&#39;category_1&#39;][&#39;EmailType&#39;].unique() 给出
&lt;前&gt;&lt;代码&gt;[&#39;email_type_13&#39;]

df[df[&#39;Category&#39;]==&#39;category_2&#39;][&#39;EmailType&#39;].unique() 给出
&lt;前&gt;&lt;代码&gt;[&#39;email_type_13&#39;]

df[df[&#39;Category&#39;]==&#39;category_3&#39;][&#39;EmailType&#39;].unique() 给出
[&#39;email_type_13&#39;, &#39;email_type_21&#39;, &#39;email_type_29&#39;, ... &#39;email_type_4&#39;]

“category_3”共有 143 种电子邮件类型，计数范围从 1 到 11,000+
我想训练一个模型，在给定文本的情况下预测类别和电子邮件类型。
我探索了 BERT 和 T5 模型。如何训练 BERT 来预测类别和电子邮件类型。
请问还有什么更好的办法吗？如何处理“category_1”和“category_2”的类别不平衡 EmailType 只有一个“email_type_13”。训练时，类别和电子邮件类型中的哪一个应被视为类别和标签？
我尝试对 EmailType 类进行分类，计数  82（通过分位数选择）作为“其他”来管理类别不平衡。另外，我尝试使用 EmailType 作为类，使用 Category 作为 BERT 的标签。]]></description>
      <guid>https://stackoverflow.com/questions/78506372/multi-target-classification-on-email-data</guid>
      <pubDate>Mon, 20 May 2024 11:34:02 GMT</pubDate>
    </item>
    <item>
      <title>k-最近分类概率估计问题</title>
      <link>https://stackoverflow.com/questions/78506193/k-nearest-classification-probability-estimation-problem</link>
      <description><![CDATA[已知邻居连接方法对噪声敏感。我们将考虑训练样本的一个属性和两个对象的二元分类模型问题：（x_1 = 0.2），（x_2 = 0.7）。第一个对象属于第一类，第二个对象属于第二类。
让我们向对象添加一个新的噪声特征，均匀分布在段 ([0, 1]) 上。现在每个对象都由两个侧面来描述。需要使用具有欧几里德度量的最近邻方法对该空间中的新对象（ u = (0, 0) ）进行分类。
添加第二个噪声对象后，它比第一个更接近对象（u）的概率是多少？
如果可能的话我想得到如何解决的方法和解决方案数值]]></description>
      <guid>https://stackoverflow.com/questions/78506193/k-nearest-classification-probability-estimation-problem</guid>
      <pubDate>Mon, 20 May 2024 10:54:59 GMT</pubDate>
    </item>
    <item>
      <title>SpaCy transformer NER 训练——transformer 上零损失，未经训练</title>
      <link>https://stackoverflow.com/questions/78506114/spacy-transformer-ner-training-zero-loss-on-transformer-not-trained</link>
      <description><![CDATA[我正在使用 [&#39;transformer&#39;, &#39;ner&#39;] 组件训练 SpaCy 管道，ner 训练得很好，但 Transformer 的损失为 0，并且我假设它没有进行训练。 
这是我的配置：
&lt;代码&gt;[路径]
矢量=“en_core_web_trf”
init_tok2vec = null
火车=“/home/sxdadmin/spacy/input/train.spacy”
dev =“/home/sxdadmin/spacy/input/dev.spacy”

[系统]
gpu_allocator = “pytorch”;
种子 = 0

[自然语言处理]
lang =“en”；
pipeline = [“变压器”, “ner”]
批量大小 = 512
禁用 = []
创建之前 = null
创建后=空
after_pipeline_creation = null
tokenizer = {“@tokenizers”：“spacy.Tokenizer.v1”}
向量 = {“@vectors”：“spacy.Vectors.v1”}

#################################################### ####################
[成分]
#################################################### ####################

[组件.变压器]
工厂=“变压器”
最大批次项 = 4096

[组件.变压器.模型]
@architectures = “spacy-transformers.TransformerModel.v1”
name = “bert-base-cased”；
tokenizer_config = {“use_fast”：true}

[组件.transformer.model.get_spans]
@span_getters = “spacy-transformers.doc_spans.v1”

[components.transformer.set_extra_annotations]
@annotation_setters = “spacy-transformers.null_annotation_setter.v1”

#################################################### ####################

[组件.ner]
工厂=“ner”
不正确的跨度键 = null
移动=空
记分器 = {“@scorers”：“spacy.ner_scorer.v1”}
update_with_oracle_cut_size = 100

[组件.ner.模型]
@architectures = “spacy.TransitionBasedParser.v2”
state_type =“ner”；
extra_state_tokens = false
隐藏宽度 = 64
最大输出件数 = 2
use_upper = true
nO = 空

#################################################### ####################
[语料库]
#################################################### ####################

[语料库.train]
@readers =“spacy.Corpus.v1”
路径 = ${paths.train}
最大长度 = 3000
gold_preproc = false
限制 = 0
增强器 = null

[语料库.dev]
@readers =“spacy.Corpus.v1”
路径 = ${paths.dev}
最大长度 = 3000
gold_preproc = false
限制 = 0
增强器 = null

#################################################### ####################
[训练]
#################################################### ####################

dev_corpus = “corpora.dev”;
train_corpus = “语料库.train”;
种子 = 0
gpu_allocator =“pytorch”；
辍学率 = 0.1
累积梯度= 1
耐心=1600
最大纪元 = 0
最大步数 = 20000
评估频率 = 200
冻结组件 = []
注释组件 = []
before_to_disk = null
更新前=空

#################################################### ####################

[训练.batcher]
@batchers = “spacy.batch_by_words.v1”
丢弃尺寸过大= false
公差 = 0.2
获取长度=空

[训练.batcher.大小]
@schedules =“compounding.v1”；
开始 = 64
停止= 512
化合物 = 1.001
t = 0.0

#################################################### ####################

[训练记录器]
@loggers = “spacy.ConsoleLogger.v1”
进度条=假

[训练.优化器]
@optimizers =“Adam.v1”；
贝塔1 = 0.9
贝塔2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
梯度剪辑 = 1.0
use_averages = false
每股收益 = 0.00000001
学习率 = 0.001

[训练.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

#################################################### ####################
[预训练]
#################################################### ####################

[初始化]
矢量=“en_core_web_lg”
init_tok2vec = null
词汇数据=空
查找=空
before_init = null
after_init = null

[初始化.组件]
[初始化.组件.变压器]
[初始化.tokenizer]

和输出：

所有警告都得到满足，著名的Bert的max_length 512个tokens就是通过文本分割实现的。数据之前已在 [tok2vec, ner] 设置上进行了测试。
请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78506114/spacy-transformer-ner-training-zero-loss-on-transformer-not-trained</guid>
      <pubDate>Mon, 20 May 2024 10:39:58 GMT</pubDate>
    </item>
    <item>
      <title>如何处理文本分类器中的未标记数据以进行订阅推荐</title>
      <link>https://stackoverflow.com/questions/78505967/how-to-handle-unlabeled-data-in-a-text-classifier-for-subscription-recommendatio</link>
      <description><![CDATA[我正在构建一个文本分类器，用于根据传入文本推荐订阅。场景如下：
数据集不平衡：训练数据不平衡，某些类别的样本明显多于其他类别。
未标记数据：大部分传入文本（约 80%）不需要任何订阅，因此不包含在训练数据集中。
问题：
部署文本分类器时，我想确保模型不会为训练数据集中未表示的文本提供订阅建议。我该如何处理这种大多数文本都没有标签并且不需要任何订阅的情况？
具体问题：
如何训练模型来识别不需要任何订阅的文本（即那些未包含在训练数据中的文本）？
我可以使用哪些技术来有效管理不平衡数据集？
是否有任何具体策略或最佳实践来确保分类器可以识别和处理标记类别之外的文本？
还没有尝试过任何东西，寻求建议。]]></description>
      <guid>https://stackoverflow.com/questions/78505967/how-to-handle-unlabeled-data-in-a-text-classifier-for-subscription-recommendatio</guid>
      <pubDate>Mon, 20 May 2024 10:12:46 GMT</pubDate>
    </item>
    <item>
      <title>痤疮数据集上的 Mask R-CNN [关闭]</title>
      <link>https://stackoverflow.com/questions/78505757/mask-r-cnn-on-acne-dataset</link>
      <description><![CDATA[为什么 Mask R-CNN 没有在痤疮数据集上进行训练？如果我在痤疮数据集上训练它来分类痤疮类型，那么它背后的优势是什么？
Mask R-CNN 不在痤疮数据集上进行训练的真实且坚实的原因]]></description>
      <guid>https://stackoverflow.com/questions/78505757/mask-r-cnn-on-acne-dataset</guid>
      <pubDate>Mon, 20 May 2024 09:29:38 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 决策林支持增量学习吗？</title>
      <link>https://stackoverflow.com/questions/78505673/does-tensorflow-decision-forest-support-incremental-learning</link>
      <description><![CDATA[我想预测供应链中的交货时间，其中依赖的特征是材料、供应商、价格、数量等......，这可以通过使用随机森林回归模型来完成，但随机森林回归模型不能支持增量学习，所以我正在寻找其他支持增量学习的模型以及支持GPU的模型。
我尝试过随机森林回归模型和支持向量机，在随机森林回归模型中增量学习是问题，而在 SVM 中处理分类数据是问题。]]></description>
      <guid>https://stackoverflow.com/questions/78505673/does-tensorflow-decision-forest-support-incremental-learning</guid>
      <pubDate>Mon, 20 May 2024 09:08:37 GMT</pubDate>
    </item>
    <item>
      <title>当我们升级到最新版本 17 时，PYPMML 模型在响应中返回“无”</title>
      <link>https://stackoverflow.com/questions/78505291/pypmml-model-is-returning-none-in-the-responses-when-we-upgrade-to-latest-vers</link>
      <description><![CDATA[我们在 Flask 应用程序中使用 pypmml 包。最近，我们在 commons-text-1.6.jar 中发现了一个漏洞（当我们安装 pypmml 时，该漏洞出现在站点包中）。 commons-text版本中存在的漏洞&gt;= 1.5，&lt; 1.10.0。
为了缓解此漏洞，我们将软件包版本从 0.9.12 升级到 0.9.17。 最新版本的通用文本版本是1.10。但是，当我们使用请求数据调用预测方法时，模型已开始在响应中给出“无”。旧版本的响应与预期一致。
有人可以帮忙吗？
在此处输入图片说明
我尝试将软件包降级回 0.9.12 并按预期工作，当我降级到该软件包的第一个版本 0.9.0 时，它也正常工作。我无法使用此软件包的较低版本，因为它具有常见的文本 jar 文件漏洞。]]></description>
      <guid>https://stackoverflow.com/questions/78505291/pypmml-model-is-returning-none-in-the-responses-when-we-upgrade-to-latest-vers</guid>
      <pubDate>Mon, 20 May 2024 07:40:14 GMT</pubDate>
    </item>
    <item>
      <title>GAN (Pix2Pix) 训练后生成空白白色图像作为输出</title>
      <link>https://stackoverflow.com/questions/78504974/gan-pix2pix-generating-blank-white-image-as-output-after-training</link>
      <description><![CDATA[我训练了一个 Pix2Pix GAN 模型（将卫星图像转换为地图），并想测试该模型的性能。但是，我只能看到空白/白色图像作为输出。我正在使用在训练结束时保存的预训练模型（pix2pix_15000.pth）（即具有所需的生成器和鉴别器损失）。然而，在训练过程中，我可以看到生成的图像与预期的地图（真实标签）相似。
下面是我用来测试模型并检查生成的图像的代码：
input_dim = 3 # 输入通道数
real_dim = 3 # 输出通道数
target_shape = 256 # 图像的目标形状
device = &#39;cpu&#39; # 如果 CUDA 可用，则使用 &#39;cuda&#39;

gen = UNet(input_dim, real_dim).to(设备)
loaded_state = torch.load(“pix2pix_15000.pth”,map_location=torch.device(&#39;cpu&#39;))
gen.load_state_dict(loaded_state[&quot;gen&quot;]) # 调整保存模型的路径
gen.eval() # 将模型设置为评估模式

# 定义转换以预处理输入图像
变换 = 变换.Compose([
    Transforms.Resize((target_shape, target_shape)), # 调整大小以匹配目标形状
    变换.ToTensor(),
    Transforms.Normalize((0.5,), (0.5,)) # 将图像转换为张量
]）

# 加载并预处理图像
image_path = “test/842-sat.jpg” # 卫星图像的路径
图像 = Image.open(image_path).convert(&#39;RGB&#39;)
condition = transform(image).unsqueeze(0) # 添加批量维度
条件最终 = 条件.to(设备)

# 生成输出
使用 torch.no_grad()：
    生成的图像 = gen(条件最终)
    打印（生成的图像）

# 将生成的图像张量转换为PIL图像
to_pil = 变换.ToPILImage()
generated_image = generated_image.squeeze(0).cpu() # 删除batch维度并移至CPU
generated_image_pil = to_pil(生成的图像)

# 显示图像
plt.imshow（生成的图像_pil）
plt.axis(&#39;off&#39;) # 隐藏坐标区
plt.show()

我期望输出是一个地图，对应于卫星图像。请大家提出宝贵的建议，指出我可能错的地方。谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78504974/gan-pix2pix-generating-blank-white-image-as-output-after-training</guid>
      <pubDate>Mon, 20 May 2024 06:24:13 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中如何实现数据并行优化器步骤？</title>
      <link>https://stackoverflow.com/questions/78504808/how-is-optimizer-step-implemented-for-data-parallelism-in-pytorch</link>
      <description><![CDATA[我正在尝试在 PyTorch 中从头开始实现数据并行性。为此，我实施了以下步骤：

为每台设备制作模型副本
重新批处理每个模型副本的数据
向前跑动传球
累积梯度并求平均值
优化器采用平均梯度&lt;- 你在这里

我正在尝试找出如何将 pytorch 优化器步骤和手动数据并行性结合起来。目前，我能做到这一点的唯一方法是为每个数据并行模型副本保留优化器的副本 - 这是该代码的简单简化
# 创建优化器，使其步骤更新所有模型的权重
优化器 = [torch.optim.SGD(model.parameters(), lr=0.1) 对于模型中的模型]

# 步进优化器
对于优化器中的优化器：
    优化器.step()

我怀疑这不是人们实现优化器步骤的方式，因为它看起来内存效率不高。在实践中这是如何做到的？有没有一种方法可以将一个优化器附加到多个模型副本？我想了解这样做的正确方法是什么！]]></description>
      <guid>https://stackoverflow.com/questions/78504808/how-is-optimizer-step-implemented-for-data-parallelism-in-pytorch</guid>
      <pubDate>Mon, 20 May 2024 05:32:57 GMT</pubDate>
    </item>
    <item>
      <title>主成分分析是否适用于 Hu 矩特征 [关闭]</title>
      <link>https://stackoverflow.com/questions/78504177/is-principal-component-analysis-applicable-to-hu-moments-features</link>
      <description><![CDATA[我正在尝试改进使用 Hu 矩作为特征的图像分类应用程序的结果，但当我应用 pca 时，只有一个特征可以弥补方差的 0.95。
我对原始 Hu 特征执行了以下步骤来查找投影矩阵：

标准化
计算协方差
使用 Eigen 库查找特征向量和值
选择构成方差 0.8 的特征向量并丢弃其余的。
结果是一个 1 到 X 矩阵（只有一个特征覆盖了方差的 0.98）
]]></description>
      <guid>https://stackoverflow.com/questions/78504177/is-principal-component-analysis-applicable-to-hu-moments-features</guid>
      <pubDate>Sun, 19 May 2024 23:29:42 GMT</pubDate>
    </item>
    <item>
      <title>定义数字手写识别问题的自动机[关闭]</title>
      <link>https://stackoverflow.com/questions/78504083/defining-an-automaton-for-the-digits-handwritten-recognition-problem</link>
      <description><![CDATA[大家好，你们能帮我吗，我仍然不知道如何定义我的自动机，以及它和手写识别过程之间的关系是什么，基本上我在定义转换、状态和字母方面遇到问题。
这是使用张量流和 MNIST 数据集进行手写数字识别的代码，但如何定义我的自动机？？？
`导入操作系统
导入CV2
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将张量流导入为 tf

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = tf.keras.utils.normalize(x_train, axis=1)
x_test = tf.keras.utils.normalize(x_test, axis=1)

模型 = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
model.add(tf.keras.layers.Dense(128，激活=tf.nn.relu))
model.add(tf.keras.layers.Dense(128，激活=tf.nn.relu))
model.add(tf.keras.layers.Dense(128，激活=tf.nn.relu))
model.add(tf.keras.layers.Dense(10, 激活=tf.nn.softmax))
model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;sparse_categorical_crossentropy&#39;,
              指标=[&#39;准确性&#39;])
model.fit(x_train, y_train, epochs=20)
model.save(&#39;手写.model&#39;)

model = tf.keras.models.load_model(&#39;手写.model&#39;)
损失，准确度 = model.evaluate(x_test, y_test)`
打印（丢失）
打印（准确度）

img = cv2.imread(&#39;img.5.png&#39;)[:,:,0]
img = cv2.resize(img, (28, 28))
img = np.invert(np.array([img]))
预测 = model.predict(img)
print(f&quot;该数字可能是 {np.argmax(prediction)}&quot;)
plt.imshow(img[0], cmap=plt.cm.binary)
plt.show()`
]]></description>
      <guid>https://stackoverflow.com/questions/78504083/defining-an-automaton-for-the-digits-handwritten-recognition-problem</guid>
      <pubDate>Sun, 19 May 2024 22:26:28 GMT</pubDate>
    </item>
    <item>
      <title>ValidationError：LLMChain 出现 2 个验证错误</title>
      <link>https://stackoverflow.com/questions/77842203/validationerror-2-validation-errors-for-llmchain</link>
      <description><![CDATA[这是我的完整代码：
!pip install -q Transformers einops 加速 langchain BitsandBytes Sentence_Transformers faiss-cpu pypdf Sentpiece
从 langchain 导入 HuggingFacePipeline
从 Transformer 导入 AutoTokenizer
从 langchain.embeddings 导入 HuggingFaceEmbeddings
从 langchain.document_loaders.csv_loader 导入 CSVLoader
从 langchain.vectorstores 导入 FAISS、Chroma
从 langchain.chains 导入 RetrievalQA
从 langchain.prompts 导入 PromptTemplate
从 langchain.chains 导入 ConversationalRetrievalChain
从 langchain.chains.question_answering 导入 load_qa_chain
从 langchain.memory 导入 ConversationBufferMemory
进口加速
进口变压器
进口火炬
导入文本换行
loader = CSVLoader(&#39;/kaggle/input/csvdata/chatdata.csv&#39;, 编码=“utf-8”, csv_args={&#39;分隔符&#39;: &#39;,&#39;})
数据 = 加载器.load()

嵌入 = HuggingFaceEmbeddings(model_name=&#39;sentence-transformers/all-MiniLM-L6-v2&#39;,model_kwargs={&#39;device&#39;: &#39;cpu&#39;})

db = FAISS.from_documents(数据，嵌入)


#Mistral 7B 模型 llm

进口火炬
从变压器进口（
    AutoModelForCausalLM，
    自动标记器，
    生成配置，
    文本流媒体,
    管道，
）

MODEL_NAME =“mistralai/Mistral-7B-Instruct-v0.1”

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
模型 = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME、device_map=“自动”、torch_dtype=torch.float16、load_in_8bit=True
）

Generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
Generation_config.max_new_tokens = 1024
Generation_config.温度 = 0.0001
Generation_config.do_sample = True
流光= TextStreamer（标记器，skip_prompt = True，skip_special_tokens = True）


llm = 管道(
    “文本生成”，
    型号=型号，
    分词器=分词器，
    return_full_text=真，
    Generation_config = Generation_config，
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id,
    流光=流光，
）


def format_prompt（提示，system_prompt =“”）：
    如果 system_prompt.strip():
        return f“[INST] {system_prompt} {prompt} [/INST]”
    return f“[INST] {提示} [/INST]”


SYSTEM_PROMPT = “””
您是一名临床数据科学家和数据分析师，专门从事统计数据分析和报告生成。您的使命是为医疗保健和临床研究提供准确且富有洞察力的数据驱动解决方案。当您做出回应时，请发挥临床数据科学领域经验丰富的数据专业人员所特有的专业知识和精确度。
如果您遇到没有必要信息的问题，请务必不要提供推测性或不准确的答案。
“”“”.strip()

链 = ConversationalRetrievalChain.from_llm(
    嗯，
    chain_type=“东西”，
    检索器=db.as_retriever(),
    return_source_documents=真，
    详细=真，
）

这里我面临错误：
ValidationError：LLMChain 出现 2 个验证错误
勒姆
  预期的 Runnable 实例（type=type_error.任意_type；expected_任意_type=Runnable）
勒姆
  预期的 Runnable 实例（type=type_error.任意_type；expected_任意_type=Runnable）


从 textwrap 导入填充

结果=链（输入（“临床试验平面计ChatBot ---”）
）
打印（填充（结果[“结果”].strip（），宽度= 80））

此 llm 链被编程为使用 llm、矢量数据库和提示与 csv 聊天，我在运行 ConversationalRetrievalChain 时遇到上述错误]]></description>
      <guid>https://stackoverflow.com/questions/77842203/validationerror-2-validation-errors-for-llmchain</guid>
      <pubDate>Thu, 18 Jan 2024 20:20:48 GMT</pubDate>
    </item>
    <item>
      <title>批量归一化适用于小批量吗？</title>
      <link>https://stackoverflow.com/questions/56859748/does-batch-normalisation-work-with-a-small-batch-size</link>
      <description><![CDATA[我使用批量标准化（批量大小为 10）进行人脸检测。
批量归一化适用于如此小的批量大小吗？如果没有，那么我还能用什么来标准化？]]></description>
      <guid>https://stackoverflow.com/questions/56859748/does-batch-normalisation-work-with-a-small-batch-size</guid>
      <pubDate>Tue, 02 Jul 2019 20:38:25 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：分类指标无法处理多类和多标签指标目标的混合</title>
      <link>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</link>
      <description><![CDATA[我有 2000 个不同标签的多类标记文本分类问题。使用 LSTM 和 Glove Embedding 进行分类。

目标变量的标签编码器
带有嵌入层的 LSTM 层
误差指标是 F2 分数

LabelEncoded 目标变量：
le = LabelEncoder()
le.fit(y)
train_y = le.transform(y_train)
test_y = le.transform(y_test)

LSTM 网络如下所示，带有 Glove Embeddings
np.random.seed(种子)
K.clear_session()
模型=顺序（）
model.add(嵌入(max_features, embed_dim, input_length = X_train.shape[1],
         权重=[embedding_matrix]))#,trainable=False
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add（密集（num_classes，激活=&#39;softmax&#39;））
model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;sparse_categorical_crossentropy&#39;)
打印（模型.摘要（））

我的错误指标是 F1 分数。我为错误指标构建了以下函数
类指标（回调）：
    def on_train_begin(self, 日志={}):
        self.val_f1s = []
        self.val_recalls = []
        self.val_ precisions = []
 
    def on_epoch_end(自我, 纪元, 日志={}):
        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()
        val_targ = self.validation_data[1]
        _val_f1 = f1_score(val_targ, val_predict)
        _val_recall=recall_score(val_targ, val_predict)
        _val_ precision = precision_score（val_targ，val_predict）
        self.val_f1s.append(_val_f1)
        self.val_recalls.append(_val_recall)
        self.val_ precisions.append(_val_ precision)
        print(&quot;— val_f1: %f — val_ precision: %f — val_recall %f&quot; % (_val_f1, _val_ precision, _val_recall))
        返回
 
指标=指标（）

##模型适合
model.fit（X_train，train_y，validation_data =（X_test，test_y），epochs = 10，batch_size = 64，callbacks = [指标]）

第一个纪元后出现以下错误：
ValueError：分类指标无法处理多类和连续多输出目标的混合

我的代码哪里出错了？]]></description>
      <guid>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</guid>
      <pubDate>Fri, 07 Jun 2019 14:55:37 GMT</pubDate>
    </item>
    </channel>
</rss>