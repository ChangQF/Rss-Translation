<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 08 Dec 2023 09:13:58 GMT</lastBuildDate>
    <item>
      <title>为什么我的逻辑回归模型重复预测相同的事情？</title>
      <link>https://stackoverflow.com/questions/77625369/why-is-my-logistic-regression-model-predicting-the-same-thing-repetitively</link>
      <description><![CDATA[https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset 
我使用这个 Kaggle 数据集作为我的糖尿病数据集，并尝试创建一个 LogisticRegression 模型来预测结果。
我创建了以下类：
导入 pandas 作为 pd
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.preprocessing 导入 StandardScaler、MinMaxScaler
从 sklearn.model_selection 导入 KFold
将seaborn导入为sns

从 sklearn.model_selection 导入 train_test_split
从 sklearn.metrics 导入 precision_score

糖尿病日志类别：
    df = pd.read_csv(“/Users/aahan_bagga/Desktop/diabetes_data.csv”)
    X=df.drop([“结果”], axis=1)
    Y=df[“结果”]
    预浸料 = 0
    葡萄糖 = 0
    血压=0
    皮肤厚度 = 0
    胰岛素 = 0
    体重指数 = 0
    糖尿病谱系函数 = 0
    年龄 = 0
    def __init__(self, p, g, BP, ST, I, BMI, DPF, 年龄):
        self.preg = p
        自身葡萄糖 = g
        自身.BP = BP
        self.skinThickness = ST
        自身胰岛素 = I
        自身体重指数 = BMI
        self.diabetesPedigreeFunction = DPF
        自我年龄 = 年龄

    def 预处理（自身）：
        全局 Y_train
        全局Y_测试
        #K 折交叉验证
        kf = KFold(n_splits = 9, shuffle = True, random_state = 19)

        全局 X_train、X_test、Y_train、Y_test
        对于 kf.split(self.X) 中的训练索引、测试索引：
            X_train, X_test = self.X.iloc[训练索引], self.X.iloc[测试索引]
            Y_train, Y_test = self.Y.iloc[训练索引], self.Y.iloc[测试索引]


        #标准化比标准化稍微好一些
        缩放器 = MinMaxScaler()
        全局 x_train_s、x_test_s
        x_train_s = 缩放器.fit_transform(X_train)
        x_test_s = 缩放器.transform(X_test)

    def 火车（自己）：
        全球模式
        模型 = 逻辑回归（max_iter = 2000）
        model.fit(x_train_s,Y_train)
        y_pred = model.predict(x_test_s)
        返回f“{accuracy_score(Y_test, y_pred) * 100}%”
    
        # 在此调整超参数
    
    defdiabetes_pred（自我）：
        prob = model.predict_proba([[self.preg, self.glucose, self.BP, self.skinThickness, self.insulin, self.bmi, self.diabetesPedigreeFunction, self.age]])
        打印（问题）
        如果概率[0,1]&gt; 0.5：
            返回“糖尿病”
        别的：
            返回“没有糖尿病”
    
    #def Decision_boundary_graph():
        #
    


d = 糖尿病LogReg(2,126,45,23,340,30,0.12,29)

d.预处理()
打印（d.train（））
打印（d.diabetes_pred（））

重复输出：
80.0%
[[0。 1.]]
糖尿病
已输出“糖尿病”信息它所做的所有预测的结果。我是机器学习的新手，但我知道我还没有调整我的超参数。这与数据集的长度有关吗，是不是太短了？或者也许与我的 k 折交叉验证有关？
如果有人可以看看并提供帮助，那就太棒了。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/77625369/why-is-my-logistic-regression-model-predicting-the-same-thing-repetitively</guid>
      <pubDate>Fri, 08 Dec 2023 08:47:22 GMT</pubDate>
    </item>
    <item>
      <title>如何在 jupyter Notebook 中查看模型的所有参数</title>
      <link>https://stackoverflow.com/questions/77625216/how-do-i-view-all-the-parameters-of-my-model-in-jupyter-notebook</link>
      <description><![CDATA[在此处输入图像描述
我尝试了很多方法来查看jupyter输出中的所有内容，但无法查看该模型中的所有参数
我是初学者，请帮助。
我尝试了 pd.pandas.set_option 来设置最大列数和行数，设置 max_seq_itempd.options.display.max_seq_items = 2000
到处寻找但没有找到解决办法]]></description>
      <guid>https://stackoverflow.com/questions/77625216/how-do-i-view-all-the-parameters-of-my-model-in-jupyter-notebook</guid>
      <pubDate>Fri, 08 Dec 2023 08:18:29 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的训练和验证损失曲线看起来像对数正态分布？</title>
      <link>https://stackoverflow.com/questions/77624977/why-my-training-and-validation-loss-curve-looks-like-lognormal-distribution</link>
      <description><![CDATA[我训练了 XGBoost 模型，我的训练和验证曲线看起来像这样？我在做什么奇怪的事吗？我总是看到它从高到低，或者像 U 形，以防过度拟合。
]]></description>
      <guid>https://stackoverflow.com/questions/77624977/why-my-training-and-validation-loss-curve-looks-like-lognormal-distribution</guid>
      <pubDate>Fri, 08 Dec 2023 07:30:28 GMT</pubDate>
    </item>
    <item>
      <title>当我证明概率 P(A|B)>=P(A) 时一定有问题，我找不到它</title>
      <link>https://stackoverflow.com/questions/77624901/something-must-be-wrong-when-i-proof-that-the-probability-pab-pa-i-cannot</link>
      <description><![CDATA[我得出了一个非常奇怪的结果，即 P(A|B)&gt;=P(A)，但我无法找出问题所在，请帮忙。推导过程如下所示。
&lt;前&gt;&lt;代码&gt; P(A|B)
=P(A,B)/P(B)
=(P(A)+P(B)-P(A 或 B))/P(B)
=(P(A)-P(A 或 B))/P(B) + 1

由于 0&lt;=P(B)&lt;=1，我们有：
P(A|B) &gt;= P(A)-P(A 或 B) + 1

由于 0&lt;=P(A 或 B)&lt;=1，我们有：
&lt;前&gt;&lt;代码&gt;P(A|B) &gt;= P(A) - 1 + 1 = P(A)

]]></description>
      <guid>https://stackoverflow.com/questions/77624901/something-must-be-wrong-when-i-proof-that-the-probability-pab-pa-i-cannot</guid>
      <pubDate>Fri, 08 Dec 2023 07:12:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用大型语言模型为视频添加字幕</title>
      <link>https://stackoverflow.com/questions/77624821/how-do-i-caption-videos-using-large-language-models</link>
      <description><![CDATA[我想为机器学习任务生成视频字幕。我尝试过使用 miniGPT4 或 CoCap 为视频添加字幕，但速度很慢（CPU 需要几分钟）。有没有一个好的方法来获得像样的视频字幕（最好是几秒钟）？]]></description>
      <guid>https://stackoverflow.com/questions/77624821/how-do-i-caption-videos-using-large-language-models</guid>
      <pubDate>Fri, 08 Dec 2023 06:52:55 GMT</pubDate>
    </item>
    <item>
      <title>对于边界框来获取坐标以进行对象检测，哪个更好？ AWS 地面实况还是 Azure？</title>
      <link>https://stackoverflow.com/questions/77624770/for-bounding-box-to-get-coordinates-for-object-detection-which-is-better-aws-g</link>
      <description><![CDATA[我目前正在使用 AWS Ground Truth 为我的机器学习模型标记训练图像，特别是针对带有边界框的对象检测任务。我利用 Mechanical Turk 集成来获得准确的注释。
现在，我正在探索 Azure 作为替代云提供商，并想知道 Azure 是否提供类似的服务或功能。我对一种服务或工具特别感兴趣，它允许我用边界框标记图像并检索对象检测任务的相应坐标。
如果有人有使用提供图像标签和边界框注释功能的 Azure 服务的经验，我将不胜感激任何见解或建议。此外，如果Azure生态系统中有特定工具或API可以实现此目的，请提供相关详细信息和示例。
预先感谢您的帮助！
我已经浏览了 Azure 文档和各种在线资源，以查找有关图像标记和对象检测服务的信息。然而，可用的文档有时可能很广泛，我可能忽略了相关细节。
我还检查了 Azure 机器学习服务，但我找不到与 AWS Ground Truth 直接等效的服务，用于使用边界框注释进行图像标记。]]></description>
      <guid>https://stackoverflow.com/questions/77624770/for-bounding-box-to-get-coordinates-for-object-detection-which-is-better-aws-g</guid>
      <pubDate>Fri, 08 Dec 2023 06:42:38 GMT</pubDate>
    </item>
    <item>
      <title>如何在房间布局检测中微调 vit/ convnet 模型？</title>
      <link>https://stackoverflow.com/questions/77624651/how-to-fine-tune-a-vit-convnet-model-on-room-layout-detection</link>
      <description><![CDATA[我想从微调模型中生成良好的嵌入，该模型经过微调以专注于房间的布局，而不是图像中墙壁或家具的颜色。但如何去做呢？]]></description>
      <guid>https://stackoverflow.com/questions/77624651/how-to-fine-tune-a-vit-convnet-model-on-room-layout-detection</guid>
      <pubDate>Fri, 08 Dec 2023 06:08:26 GMT</pubDate>
    </item>
    <item>
      <title>VGG 和 ResNet 输入可以是原始值而不是图像吗？</title>
      <link>https://stackoverflow.com/questions/77624129/can-vgg-and-resnet-inputs-be-raw-values-instead-of-images</link>
      <description><![CDATA[我可以使用原始 mel、mfcc、谱质心、谱通量和过零率值作为 VGG 或 ResNet 的输入来执行音频分类任务吗？
我读到的大多数研究都涉及获取频谱图并将其输入 CNN 模型。我想知道是否可以使用原始值]]></description>
      <guid>https://stackoverflow.com/questions/77624129/can-vgg-and-resnet-inputs-be-raw-values-instead-of-images</guid>
      <pubDate>Fri, 08 Dec 2023 03:07:05 GMT</pubDate>
    </item>
    <item>
      <title>是否可以向 CNN 提供原始数据而不是图像？</title>
      <link>https://stackoverflow.com/questions/77623936/is-it-possible-to-feed-cnn-with-raw-numbers-instead-of-images</link>
      <description><![CDATA[我正在尝试使用 CNN 构建乐器识别系统。我想要提取的特征是梅尔谱图、MFCC、谱质心、谱通量和过零率。我想比较哪个特征在输入 CNN 时会带来最高的性能（准确率、精确度、召回率和 F1）。我的问题是：是否可以向 CNN 提供原始值，而不是我提取的特征的频谱图？
我读到的大多数相关研究都涉及获取频谱图并将该图像直接输入 CNN。]]></description>
      <guid>https://stackoverflow.com/questions/77623936/is-it-possible-to-feed-cnn-with-raw-numbers-instead-of-images</guid>
      <pubDate>Fri, 08 Dec 2023 01:57:26 GMT</pubDate>
    </item>
    <item>
      <title>如何修复 Pyarrow.dataset.dataset.take() 花费大量时间来获取数据</title>
      <link>https://stackoverflow.com/questions/77623919/how-to-fix-pyarrow-dataset-dataset-take-spend-so-much-time-to-featch-the-data</link>
      <description><![CDATA[对于我的代码，def getitems(self,idx)的idx是一个列表，每个元素是从0-59999随机的，idx的长度是32。当我使用sample = self时.dataset.take(arrow_array) 来取数据，会花这么多时间。
类 CustomDataset(数据集):
    def __init__(self, pyarrow_dataset):
        self.record_count = 0
        自己。批次计数 = []
        对于 pyarrow_dataset.to_batches() 中的 _：
            self.record_count += len(_)
        self.dataset = pyarrow_dataset#统计当前批次的记录

    def __len__(自身):
        返回 self.record_count

    def __getitems__(self, idx):
        # Sample = self.dataset.to_batches().take([idx]) # 使用to_batches获取记录
        self.comsum_time = 0
        
        
        arrow_array = pa.array(idx,type=pa.int64())
        开始时间 = 时间.time()
        # print(dataset.take(arrow_array).to_pandas())
        Sample = self.dataset.take(arrow_array) # 使用to_batches获取记录
        end_time1 = time.time()

        # 标签 = 样本[&#39;标签&#39;].to_pandas()
        标签 = 样本[&#39;标签&#39;].to_pandas()

        # image = torch.FloatTensor(sample[&#39;image&#39;].to_pandas()).view((28,28)).unsqueeze(0)
        图像 = torch.FloatTensor(sample[&#39;image&#39;].to_pandas()).view(32,1,28,28)
        
        comsume_time = (结束时间1-开始时间)
        以 open(&#39;/home/yue21/mlndp/ML_example/result/take_time.txt&#39;, &#39;a&#39;) 作为 take_time_save：
            take_time_save.write(f&quot;take_operation_time: {comsume_time} 秒\n&quot;)
        # 图像 = torch.tensor(样本[0][&#39;图像&#39;][0])

        # batch_data = [{&#39;data&#39;: images[i], &#39;target&#39;: int(labels[i])} for i in range(len(idx))]
        batch_data = [{&#39;data&#39;: images[i], &#39;target&#39;: int(labels[i])} for i in range(len(idx))]


        返回批次数据

我想问一下这部分如何优化？]]></description>
      <guid>https://stackoverflow.com/questions/77623919/how-to-fix-pyarrow-dataset-dataset-take-spend-so-much-time-to-featch-the-data</guid>
      <pubDate>Fri, 08 Dec 2023 01:50:58 GMT</pubDate>
    </item>
    <item>
      <title>使用 RNN 确定可接受的情感分析基线</title>
      <link>https://stackoverflow.com/questions/77623881/determining-an-acceptable-baseline-for-sentiment-analysis-using-rnns</link>
      <description><![CDATA[按照我发现的教程，我一直在尝试使用循环神经网络 (RNN) 进行情感分析 此处。虽然我的模型可以正常运行，但我在准确性方面遇到了障碍，始终达到 85-90% 之间。我尝试了各种优化，但我不确定什么构成合理的基线来结束我的努力，特别是考虑到基于 Transformer 的 LSTM 模型的潜在效率。
我的主要查询并不是以进一步提高准确性为中心；相反，我寻求指导来确定何时考虑情感分析模型对于实际使用足够有效，特别是在使用 RNN 来完成此任务时。由于我对机器学习比较陌生，因此我不清楚何时确定我的目标已经实现。]]></description>
      <guid>https://stackoverflow.com/questions/77623881/determining-an-acceptable-baseline-for-sentiment-analysis-using-rnns</guid>
      <pubDate>Fri, 08 Dec 2023 01:35:34 GMT</pubDate>
    </item>
    <item>
      <title>垃圾邮件检测机器学习问题[关闭]</title>
      <link>https://stackoverflow.com/questions/77623749/email-spam-detection-machine-learning-problem</link>
      <description><![CDATA[我正在制作一个程序，可以检测某人收件箱中的垃圾邮件并为他们删除它。首先，我想通过使用机器学习来检测垃圾邮件，以确定电子邮件是否是垃圾邮件。我从网上复制了某人的垃圾邮件检测器代码，并尝试使用 Jupyter 笔记本运行它。在我复制代码的人的网站上，这个人的最后一行与我相同，当他运行它时，它显示出大约 99% 的准确率。但当我运行它时，每次运行它都能始终保持 0.5% 的准确率。
我有与他完全相同的代码，与他相同的 csv 数据集文件，但我们的结果不同。有人可以告诉我这个程序做错了什么吗？
代码如下：
导入 pandas 作为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.feature_extraction.text 导入 CountVectorizer
从 sklearn 导入 svm

spam = pd.read_csv(&#39;C:\\Users\\nethm\\Downloads\\spam.csv&#39;)
z = 垃圾邮件[&#39;电子邮件文本&#39;]
y = 垃圾邮件[“标签”]
z_train, z_test,y_train, y_test = train_test_split(z,y,test_size
= 0.2)

CV = CountVectorizer()
特征 = cv.fit_transform(z_train)

模型 = svm.SVC()
model.fit(特征,y_train)

features_test = cv.transform(z_test)
print(model.score(features_test,y_test))
]]></description>
      <guid>https://stackoverflow.com/questions/77623749/email-spam-detection-machine-learning-problem</guid>
      <pubDate>Fri, 08 Dec 2023 00:39:14 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归（数据-预处理）[关闭]</title>
      <link>https://stackoverflow.com/questions/77623608/logistic-regression-data-preprocessing</link>
      <description><![CDATA[我正在学习机器学习中的逻辑回归部分。
我了解到我们需要进行数据预处理，例如删除空值和大纲。
但我不知道我能在这里做什么......！
这是我的作业
这是我的数据。( https:// github.com/nam14d/imt574_conglomorate/blob/806fd329af1672e08827367ba044263703bcee49/Assignment3_wine_quality/quality.csv#L1)
S.No. num_words num_characters num_misspelled bin_end_qmark num_interrogative bin_start_small num_sentences num_punctuations 标签
1 10 48 2 0 0 0 2 4 B
2 8 25 0 0 0 1 1 0 B
3 20 81 0 1 19 0 1 1 B
4 9 34 1 0 1 0 1 2 B
5 18 69 3 0 1 0 1 0 B
6 7 39 1 0 0 0 1 2 B
7 10 46 4 0 2 1 2 2 B
8 14 70 5 0 0 0 2 16 B
9 0 46 0 0 0 0 1 0 B
10 31 173 26 0 0 1 3 7 B
11 22 90 1 0 1 0 2 9 B
12 8 48 0 1 1 0 1 1 B
13 9 56 5 0 0 0 2 1 B
14 9 41 0 1 0 0 1 1 B
501 79 345 4 0 1 0 9 20 G
502 4 17 2 0 1 1 1 0 G
503 169 831 17 0 1 0 13 48 G
504 25 124 4 0 1 0 4 6 G
505 9 25 0 0 0 0 1 2 G
506 22 103 1 0 0 0 2 4 G
507 44 196 4 0 1 0 6 14 G
508 7 47 0 0 1 0 1 0 G
509 34 118 0 1 1 0 2 2 G
510 17 93 1 0 1 0 1 0 G
511 76 329 3 0 3 0 8 9 G
512 13 44 0 0 1 0 1 0 G
513 10 35 0 0 1 0 1 0 G
514 52 201 1 0 1 0 1 0 克

我写了一些没有预处理的东西，但我也不确定我是否可以这样做。
导入 pandas 作为 pd
将 numpy 导入为 np
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.neighbors 导入 KNeighborsClassifier
从 sklearn.model_selection 导入 train_test_split
将 matplotlib.pyplot 导入为 plt

qual = pd.read_csv(“./quality.csv”)
## 对数回归模型 - X：前八个
qual[&#39;标签&#39;] = np.where(qual[&#39;标签&#39;] == &#39;B&#39;, 0, 1)
X = qual.drop([&#39;序号&#39;,&#39;标签&#39;], axis=1)
X_normalized = X.apply(lambda x: (x-min(x))/max(x)-min(x))
y = 质量[&#39;标签&#39;]
Xtrain，Xtest，ytrain，ytest = train_test_split（X，y，test_size = 0.2）

logmod = 逻辑回归()
logmod.fit(Xtrain, ytrain)

预测 = logmod.predict(Xtest)

打印（accuracy_score（ytest，预测））

在逻辑回归之前我应该​​处理哪些数据？
以及如何处理这个练习题？]]></description>
      <guid>https://stackoverflow.com/questions/77623608/logistic-regression-data-preprocessing</guid>
      <pubDate>Thu, 07 Dec 2023 23:46:26 GMT</pubDate>
    </item>
    <item>
      <title>模型无法正确保存或加载</title>
      <link>https://stackoverflow.com/questions/77623502/model-not-being-able-to-be-saved-or-loaded-correctly</link>
      <description><![CDATA[我正在制作一个简单的 NPL 模型，我想保存它并让另一个脚本打开该模块。这是一段极其未优化的代码。我怎样才能让它真正发挥作用，而不必每次我想发送新提示时都重新训练它？每次我保存为 .h5 时都会收到错误，每次我将其保存为 .keras 并加载时都会收到错误。我使用 new_model = load_model(&#39;your_model.h5&#39;) 或 load_model(&#39;your_model.keras&#39;)
显示模型架构
new_model.summary()
脚本 1：训练模型并保存
脚本2，单独文件：加载模型。请及时采取行动。预测提示的其余部分
导入tensorflow为tf

从tensorflow.keras.preprocessing.sequence导入pad_sequences
从tensorflow.keras.layers导入嵌入、LSTM、密集、双向
从tensorflow.keras.preprocessing.text导入Tokenizer
从tensorflow.keras.models导入顺序
从tensorflow.keras.optimizers导入Adam
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
分词器 = 分词器()

data = open(&#39;rawdata.txt&#39;).read()

语料库 = data.lower().split(“\n”)

tokenizer.fit_on_texts（语料库）
总单词数 = len(tokenizer.word_index) + 1

print(tokenizer.word_index)
打印（总字数）

输入序列 = []
对于语料库中的行：
    token_list = tokenizer.texts_to_sequences([行])[0]
    对于范围内的 i(1, len(token_list))：
        n_gram_sequence = token_list[:i+1]
        input_sequences.append(n_gram_sequence)

# 填充序列
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding=&#39;pre&#39;))

# 创建预测变量和标签
xs，标签 = input_sequences[:,:-1],input_sequences[:,-1]

ys = tf.keras.utils.to_categorical(标签, num_classes=total_words)

模型=顺序（）
model.add(嵌入(total_words, 100))
model.add(双向(LSTM(150)))
model.add（密集（total_words，激活=&#39;softmax&#39;））
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=&#39;adam&#39;, 指标=[&#39;accuracy&#39;])
历史= model.fit(xs, ys, epochs=35, verbose=1)
plt.plot(history.history[&#39;准确度&#39;])
plt.title(&#39;随时间变化的模型精度&#39;)
plt.xlabel(&#39;纪元&#39;)
plt.ylabel(&#39;准确度&#39;)
plt.show()
打印（模型）
model.save(&#39;your_model.keras&#39;)
Seed_text = &quot;在《守望先锋》中擅长源氏&quot;; #这是提示。应该是一个字符串
next_words = 25 # 显示的总字数
对于范围内的 _(next_words)：
    token_list = tokenizer.texts_to_sequences([seed_text])[0]
    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding=&#39;pre&#39;)
    预测 = np.argmax(model.predict(token_list), axis=-1)
    输出字=“”
    对于单词，在 tokenizer.word_index.items() 中索引：
        如果索引==预测：
            输出字=字
            休息
    种子文本+=“ ” + 输出字
打印（种子文本）
]]></description>
      <guid>https://stackoverflow.com/questions/77623502/model-not-being-able-to-be-saved-or-loaded-correctly</guid>
      <pubDate>Thu, 07 Dec 2023 23:05:05 GMT</pubDate>
    </item>
    <item>
      <title>不同的交叉验证技术产生相同的评估指标</title>
      <link>https://stackoverflow.com/questions/77555347/different-cross-validation-techniques-yielding-identical-evaluation-metrics</link>
      <description><![CDATA[我实现了三种 ML 算法（K 最近邻、决策树和随机森林）并使用四种不同的交叉验证技术（Hold-Out 方法、留一方法、K 折交叉验证、分层每个算法的 K 折交叉验证）。目标是评估性能指标并比较技术和算法。我的代码可以运行，但不同技术的评估指标值是相同的。这些值相同是否正常，或者我可能做错了什么？
这是我的代码的一部分：
# 初始化分类器
knn = KNeighborsClassifier(n_neighbors=5, metric=&#39;minkowski&#39;, p=2)
dtree = DecisionTreeClassifier(random_state=42)
rf = RandomForestClassifier(n_estimators=20, criteria=&#39;entropy&#39;, random_state=0)

classifiers = {&#39;KNN&#39;: knn, &#39;决策树&#39;: dtree, &#39;随机森林&#39;: rf}

# 定义交叉验证方法
厕所 = LeaveOneOut()
kf = K折叠(10)
skf = 分层KFold(n_splits=5)

cv_methods = {&#39;保留方法&#39;: (X_train, X_test, y_train, y_test),
              “留一法”：loo，
              &#39;K 折交叉验证&#39;: kf,
              “分层 K 折交叉验证”：skf}

# 对每个分类器和交叉验证方法进行分类和评估
对于 clf_name，clf 在 classifiers.items() 中：
    print(f&quot;分类器：{clf_name}&quot;)
    对于 cv_name、cv_method 在 cv_methods.items() 中：
        if cv_name == &#39;保留方法&#39;:
            X_train_cv、X_test_cv、y_train_cv、y_test_cv = cv_method
            clf.fit(X_train_cv, y_train_cv)
            y_pred = clf.predict(X_test_cv)
        别的：
            分数 = cross_val_score(clf, X, y, cv=cv_method, 评分=&#39;准确度&#39;)
            

        # 计算评价指标
        准确度=准确度_得分（y_test_cv，y_pred）
        精度 = precision_score(y_test_cv, y_pred, 平均值=&#39;加权&#39;)
        召回率=召回率（y_test_cv，y_pred，平均值=&#39;加权&#39;）
        f1 = f1_score(y_test_cv, y_pred, 平均值=&#39;加权&#39;)
        混乱=混乱_矩阵（y_test_cv，y_pred）

以下是输出，每个分类器和交叉验证方法的输出都是相同的：
分类器：KNN
KNN 的保留方法指标：
准确度：0.864620939
精度：0.8661
召回率：0.8646
F1分数：0.8652
混淆矩阵：
[[326 41]
 [34153]]

KNN 的留一法指标：
准确度：0.864620939
精度：0.8661
召回率：0.8646
F1分数：0.8652
混淆矩阵：
[[326 41]
 [34153]]

KNN 的 K 折交叉验证指标：
准确度：0.864620939
精度：0.8661
召回率：0.8646
F1分数：0.8652
混淆矩阵：
[[326 41]
 [34153]]

KNN 的分层 K 折交叉验证指标：
准确度：0.864620939
精度：0.8661
召回率：0.8646
F1分数：0.8652
混淆矩阵：
[[326 41]
 [34153]]

分类器：决策树
决策树的保留方法指标：
准确度：0.980144404
精度：0.9801
召回率：0.9801
F1分数：0.9801
混淆矩阵：
[[363 4]
 [7180]]

决策树的留一法指标：
准确度：0.980144404
精度：0.9801
召回率：0.9801
F1分数：0.9801
混淆矩阵：
[[363 4]
 [7180]]

决策树的 K 重交叉验证指标：
准确度：0.980144404
精度：0.9801
召回率：0.9801
F1分数：0.9801
混淆矩阵：
[[363 4]
 [7180]]

决策树的分层 K 重交叉验证指标：
准确度：0.980144404
精度：0.9801
召回率：0.9801
F1分数：0.9801
混淆矩阵：
[[363 4]
 [7180]]

分类器：随机森林
随机森林的保留方法指标：
准确度：0.981949458
精度：0.9820
召回率：0.9819
F1分数：0.9819
混淆矩阵：
[[364 3]
 [7180]]

随机森林的留一法指标：
准确度：0.981949458
精度：0.9820
召回率：0.9819
F1分数：0.9819
混淆矩阵：
[[364 3]
 [7180]]

随机森林的 K 重交叉验证指标：
准确度：0.981949458
精度：0.9820
召回率：0.9819
F1分数：0.9819
混淆矩阵：
[[364 3]
 [7180]]

随机森林的分层 K 重交叉验证指标：
准确度：0.981949458
精度：0.9820
召回率：0.9819
F1分数：0.9819
混淆矩阵：
[[364 3]
 [7180]]

为什么会发生这种情况？]]></description>
      <guid>https://stackoverflow.com/questions/77555347/different-cross-validation-techniques-yielding-identical-evaluation-metrics</guid>
      <pubDate>Mon, 27 Nov 2023 08:08:58 GMT</pubDate>
    </item>
    </channel>
</rss>