<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 10 Jan 2025 12:33:04 GMT</lastBuildDate>
    <item>
      <title>1000- torchrl：使用 SyncDataCollector 和自定义 pytorch dqn</title>
      <link>https://stackoverflow.com/questions/79345260/torchrl-using-syncdatacollector-with-a-custom-pytorch-dqn</link>
      <description><![CDATA[我尝试将 torchrl 的 SyncDataCollector 与我自己在 torch 中实现的 DQN 一起使用。由于 DQN 使用 Conv2d 和线性层，我必须计算第一个线性层的输入的正确大小，即以下网络中的 size 参数
class PixelDQN(nn.Module):
def __init__(self, input_shape, n_actions) -&gt;无：
super().__init__()
self.conv = nn.Sequential(
nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
nn.ReLU(),
nn.Conv2d(32, 64, kernel_size=4, stride=2),
nn.ReLU(),
nn.Conv2d(64, 64, kernel_size=3, stride=1),
nn.ReLU(),
nn.Flatten(),
)
size = self.conv(torch.zeros(1, *input_shape)).size()[-1]
self.fc_adv = nn.Sequential(
NoisyLinear(size, 256),
nn.ReLU(),
NoisyLinear(256, n_actions),
)
self.fc_val = nn.Sequential(
NoisyLinear(size, 256),
nn.ReLU(),
NoisyLinear(256, 1)
)

def forward(self, x: torch.Tensor):
print(x.shape)
conv = self.conv(x)
print(conv.shape)
adv = self.fc_adv(conv)
val = self.fc_val(conv)
outp = val + (adv - adv.mean(dim=1, keepdim=True))
return outp

负责这个。如您所见，我期望批量输入，因为我将使用重放缓冲区并从中抽样一批。
我以以下方式包装该 DQN，然后使用 SyncDataCollector：
n_obs = [4,84,84]
n_act = 6

agent = QValueActor(
module=PixelDQN(n_obs, n_act), in_keys=[&quot;pixels&quot;], spec=env.action_spec
)
policy_explore = EGreedyModule(
env.action_spec, eps_end=EPS_END, annealing_num_steps=ANNEALING_STEPS
)
agent_explore = TensorDictSequential(
agent, policy_explore
)

collector = SyncDataCollector(
env,
agent_explore,
frames_per_batch=FRAMES_PER_BATCH,
init_random_frames=INIT_RND_STEPS,
postproc=MultiStep(gamma=GAMMA, n_steps=N_STEPS)
)

但是，这失败了，因为 SyncDataCollector 在将 obs 提供给 DQN 之前没有对来自环境的 obs 进行批处理，因此 size 计算出错，并且线性层获得错误的输入维度。
RuntimeError：mat1 和 mat2 形状无法相乘（64x49 和 3136x256）
我已经尝试在 SyncDataCollector 中设置 buffer=True。我也尝试使用
agent_explore = TensorDictSequential(
UnsqueezeTransform(0, allow_positive_dim=True), agent, policy_explore
)

因为这是 ChatGPT 建议的，但似乎没有任何效果。
我也在我的 env 创建中尝试了 UnsqueezeTransform，但这也没有用，我的 env 如下所示：
def make_env(env_name: str):
return TransformedEnv(
GymEnv(env_name, from_pixels=True),
Compose(
RewardSum(),
EndOfLifeTransform(),
NoopResetEnv(noops=30),
ToTensorImage(),
Resize(84, 84),
GrayScale(),
FrameSkipTransform(frame_skip=4),
CatFrames(N=4, dim=-3),
)
)

我可以将 size 计算拉入 PixelDQN 的前向传递中，并检查输入张量的大小以调整计算，但这似乎是一件很奇怪的事情，因为这意味着我需要在每次前向传递时运行大小计算。]]></description>
      <guid>https://stackoverflow.com/questions/79345260/torchrl-using-syncdatacollector-with-a-custom-pytorch-dqn</guid>
      <pubDate>Fri, 10 Jan 2025 09:49:05 GMT</pubDate>
    </item>
    <item>
      <title>在 ubuntu 22.04 中安装 x13as arima seat 时出错</title>
      <link>https://stackoverflow.com/questions/79345160/error-while-installing-x13as-arima-seat-in-ubuntu-22-04</link>
      <description><![CDATA[我正在做时间序列项目，为了找到 p、d 和 q 值，我试图安装 x13as arima 座位，但我无法正确安装它，然后我使用“sudo apt install x13as”进行安装。然后我设置了一个路径，但它仍然给我一个错误路径设置不正确，我该怎么办？
sudo apt install x13as

我试过从文档中手动下载，还是有其他我可以找到的东西？]]></description>
      <guid>https://stackoverflow.com/questions/79345160/error-while-installing-x13as-arima-seat-in-ubuntu-22-04</guid>
      <pubDate>Fri, 10 Jan 2025 09:13:45 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的批次大小和时期？[关闭]</title>
      <link>https://stackoverflow.com/questions/79345103/batch-size-and-epochs-in-machine-learning</link>
      <description><![CDATA[我正在使用 TensorFlow 从股票图表中进行图像/模式识别，我创建了一个包含约 20,000 张图像的目录，其中包含价格大幅上涨或下跌之前的模式示例。我应该使用什么批次大小和多少个时期，以 80/20 的训练/验证比例进行分割？
我还打算将目录大小增加到约 100,000 张图像，那么我应该使用什么？]]></description>
      <guid>https://stackoverflow.com/questions/79345103/batch-size-and-epochs-in-machine-learning</guid>
      <pubDate>Fri, 10 Jan 2025 08:55:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 Anthropic Vision API 获取精确的图标坐标</title>
      <link>https://stackoverflow.com/questions/79345033/getting-accurate-icon-coordinates-using-anthropic-vision-api</link>
      <description><![CDATA[我正在使用 Anthropic Vision API 从桌面图像中提取特定图标、文件夹或元素的坐标，但遇到了障碍。以下是我正在做的事情：
我有一个桌面图像（原始尺寸：1920x1080），其中包含多个图标和文件夹。
我正在要求 Vision 模型（通过 Sonnet 3.5 模型）查找特定图标（如“Chrome”）的坐标。
根据 Anthropic Vision 文档，我按照指示将图像的大小调整为 1366x768，然后将其发送到 API。
尽管遵循了指南，但 API 返回的坐标与原始图像中图标的实际位置不匹配。
有趣的是，计算机使用模型在其环境中工作正常，但在这种情况下，我只想将图像发送到 Vision 模型并获取特定元素的精确坐标。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79345033/getting-accurate-icon-coordinates-using-anthropic-vision-api</guid>
      <pubDate>Fri, 10 Jan 2025 08:28:24 GMT</pubDate>
    </item>
    <item>
      <title>使用“bitsandbytes”4 位量化需要最新版本的 bitsandbytes：“pip install -U bitsandbytes”</title>
      <link>https://stackoverflow.com/questions/79344565/using-bitsandbytes-4-bit-quantization-requires-the-latest-version-of-bitsandby</link>
      <description><![CDATA[加载 tokenizer 时，我收到此错误：
ImportError：使用 bitsandbytes 4 位量化需要最新版本的 bitsandbytes：
pip install -U bitsandbytes。

我在 Macbook M2 pro 上使用 Jupyter 笔记本。
以下是源代码：
quant_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_use_double_quant=True,
bnb_4bit_compute_dtype=torch.bfloat16,
bnb_4bit_quant_type=&quot;nf4&quot;

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

base_model = AutoModelForCausalLM.from_pretrained(
BASE_MODEL,
quantization_config=quant_config,
device_map=&quot;auto&quot;,
)

base_model.generation_config.pad_token_id = tokenizer.pad_token_id

有人能帮忙吗？
我按照说明更新了 bitsandbytes，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79344565/using-bitsandbytes-4-bit-quantization-requires-the-latest-version-of-bitsandby</guid>
      <pubDate>Fri, 10 Jan 2025 03:52:41 GMT</pubDate>
    </item>
    <item>
      <title>lightgbm.cv：cvbooster.best_iteration 总是返回 -1</title>
      <link>https://stackoverflow.com/questions/79344545/lightgbm-cv-cvbooster-best-iteration-always-returns-1</link>
      <description><![CDATA[我正在从 XGBoost 迁移到 LightGBM（因为我需要它精确处理交互约束），并且我很难理解 LightGBM CV 的结果。在下面的示例中，在第 125 次迭代中实现了最小对数损失，但 model[&#39;cvbooster&#39;].best_iteration 返回 -1。我原本希望它也能返回 125 - 还是我在这里误解了什么？有没有更好的方法来获得最佳迭代，还是只需要手动检查？
我看过这个讨论，但即使我检查cvbooster中的boosters（例如，model[&#39;cvbooster&#39;].boosters[0].best_iteration），它们也都返回 -1...
import lightgbm as lgb
import numpy as np
from sklearn import datasets

X, y = datasets.make_classification(n_samples=10_000, n_features=5, n_informative=3, random_state=9)

data_train_lgb = lgb.Dataset(X, label=y)

param = {&#39;objective&#39;: &#39;binary&#39;,
&#39;metric&#39;: [&#39;binary_logloss&#39;],
&#39;device_type&#39;: &#39;cuda&#39;}

model = lgb.cv(param,
data_train_lgb,
num_boost_round=1_000,
return_cvbooster=True)

opt_1 = np.argmin(model[&#39;valid binary_logloss-mean&#39;])
print(f&quot;index argmin: {opt_1}&quot;)
print(f&quot;logloss argmin: {model[&#39;valid binary_logloss-mean&#39;][opt_1]}&quot;)

opt_2 = model[&#39;cvbooster&#39;].best_iteration
print(f&quot;index best_iteration: {opt_2}&quot;)
print(f&quot;logloss best_iteration: {model[&#39;valid binary_logloss-mean&#39;][opt_2]}&quot;)

---

&gt;&gt;&gt; 索引参数最小值：125
&gt;&gt;&gt; 对数损失参数最小值：0.13245999867688793

&gt;&gt;&gt; 索引最佳迭代：-1
&gt;&gt;&gt; 对数损失最佳迭代：0.2661896445658779
]]></description>
      <guid>https://stackoverflow.com/questions/79344545/lightgbm-cv-cvbooster-best-iteration-always-returns-1</guid>
      <pubDate>Fri, 10 Jan 2025 03:40:32 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 使用高度右偏数据[关闭]</title>
      <link>https://stackoverflow.com/questions/79344322/lightgbm-using-highly-right-skewed-data</link>
      <description><![CDATA[序言：我正在处理一个倾斜的数据集，在约 7000 个数据点中，只有约 2500 个是非零值，最有趣的数据点是最右边的异常值。
我希望使用 LightGBM 根据多个特征预测此目标数据，但由于数据的倾斜性而遇到问题。我考虑过使用转换（例如 log、sqrt、box-cox），但它们并没有提高 ML 模型的性能。

我偶然发现了 imblearn.oversampling，它可能是解决我的问题的方法。它为异常值创建合成样本，以便模型有更多数据点可供训练。但是，因为我处理的是整数而不是分类，所以我需要对数据进行分类。我已将代码设置如下：
# 为 SMOTE 设置连续目标
binning = KBinsDiscretizer(n_bins=200, encode=&#39;ordinal&#39;, strategies=&#39;quantile&#39;)
targets[&#39;binned&#39;] = binning.fit_transform(new_targets.values.reshape(-1, 1)).ravel()

# 为 SMOTE 定义动态采样策略
smote_sampling_strategy = {i: 125 for i in range(1, 83)}
# 使用调整后的策略应用 SMOTE
smote = SMOTE(sampling_strategy=smote_sampling_strategy, random_state=42)
features_upsampled, binned_targets_upsampled = smote.fit_resample(features, new_targets[&#39;binned&#39;])

# 替换已分箱的具有连续值的目标
bin_centers = binning.inverse_transform(binned_targets_upsampled.to_numpy().reshape(-1, 1)).ravel()

# 重建上采样目标 DataFrame
upsampled_targets = pd.DataFrame({
&#39;new_hothr&#39;: bin_centers,
&#39;binned&#39;: binned_targets_upsampled
})


问题：
虽然我很想使用这种方法，但我对使用此方法的最佳实践有疑问，但这些疑问并不明确在 imblearn 的文档中找到，例如：

如何确定将数据划分为多少个 bin？
如何确定要添加多少个合成样本？
如何确保此方法不会导致过度拟合？

最后，即使进行了上述上采样，这个精炼数据集仍然无法捕获 LightGBM 在测试/训练中预测结果的第 99 个百分位异常值（使用 GridSearchCV 选择超参数），因此如果有人有进一步的尝试途径建议，我洗耳恭听。]]></description>
      <guid>https://stackoverflow.com/questions/79344322/lightgbm-using-highly-right-skewed-data</guid>
      <pubDate>Thu, 09 Jan 2025 23:51:44 GMT</pubDate>
    </item>
    <item>
      <title>CycleGAN 的鉴别器损失停留在 0.0</title>
      <link>https://stackoverflow.com/questions/79344264/discriminator-loss-for-cyclegan-stuck-at-0-0</link>
      <description><![CDATA[我目前正在训练 BD-Cycle GAN，这是 Mol-Cycle GAN 的修改版本。我没有修改任何代码，但需要从 Mol-Cycle GAN 存储库下载 utils 文件夹和 environment.yml。当使用默认参数运行 train.py 文件时，鉴别器 A 和 B 的损失都停留在 0.0，但生成器损失似乎正常。
我不知道问题的原因是什么，因为我使用的是作者提供的模型官方存储库，没有修改任何代码或超参数，但得到了这个结果。鉴别器的损失在每个时期都保持在 0.0，从而扰乱整个训练过程。]]></description>
      <guid>https://stackoverflow.com/questions/79344264/discriminator-loss-for-cyclegan-stuck-at-0-0</guid>
      <pubDate>Thu, 09 Jan 2025 23:15:36 GMT</pubDate>
    </item>
    <item>
      <title>阿曼车牌的 OCR 预处理 - 字母识别问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/79343645/ocr-preprocessing-for-oman-license-plates-issues-with-alphabet-recognition</link>
      <description><![CDATA[我正在开发一个用于阿曼车牌的 OCR 系统，并努力提高字母识别的准确性。车牌上通常包含小而粗的字符，而我目前的预处理流程无法产生令人满意的结果。
到目前为止，我所做的是：
尝试过的 OCR 工具：
尽管进行了预处理和配置（--oem 3、--psm 6），但 PaddleOCR 和 Tesseract 仍然难以识别字母。
预处理步骤：
尝试使用 Sauvola 和 Wolf-Jolion 二值化、缩放图像（1.5 倍）并应用膨胀来增强文本。
问题：
字母仍然难以识别。
如何改进预处理以更好地对小而粗的字母进行 OCR 识别？
是否有任何 OCR 模型或自定义训练方法更适合像阿曼这样设计复杂的车牌？
样本车牌：
]]></description>
      <guid>https://stackoverflow.com/questions/79343645/ocr-preprocessing-for-oman-license-plates-issues-with-alphabet-recognition</guid>
      <pubDate>Thu, 09 Jan 2025 18:15:26 GMT</pubDate>
    </item>
    <item>
      <title>使用什么样的规范化[关闭]</title>
      <link>https://stackoverflow.com/questions/79343281/what-kind-of-normalization-is-used</link>
      <description><![CDATA[yelp_features_1

0 1 2 3 4 ... 27 28 29 30 31
0 8 2.0000 1 0 0 ... 1.4545 1.7439 1.9363 0.000 142.0909
1 9 0.0000 0 0 0 ... 1.4545 1.7439 1.9363 0.000 142.0909
2 2 2.0000 1 0 0 ... 1.4545 1.7439 1.9363 0.000 142.0909
3 6 2.0000 1 0 0 ... 1.4545 1.7439 1.9363   0.000 142.0909
4 3 0.0000 0 0 0 ... 1.4545 1.7439 1.9363 0.000 142.0909
   .. ... .. .. .. .. ... ... ... ... ... ...
45949 3 0.3333 1 0 0 ... 0.7437 0.3631 1.8376 1.6182 170.4366
45950 1 0.6667 0 0 1 ... 0.7437 0.3631 1.8376 1.6182 170.4366
45951 3 1.0000 1 0 0 ... 0.8488 1.0167 1.7599 2.4910 133.1060
45952 2 2.0000 0 0 0 ... 0.8488 1.0167 1.7599 2.4910 133.1060
45953 1 1.0000 1 0 1 ... 0.8488 1.0167 1.7599 2.4910 133.1060

yelp_feature_2
-----------------
0 1 2 3 4 ... 27 28 29 30 31
0 0.0224 0.0705 0.4287 1.0000 1.0000 ... 0.0100 0.0149 0.5920 0.1393 0.4975
1 0.0249 1.0000 1.0000 1.0000 1.0000 ... 0.0100 0.0149 0.5920 0.1393 0.4975
2 0.0062 0.0705 0.4287 1.0000 1.0000 ... 0.0100 0.0149 0.5920 0.1393 0.4975
3 0.0174 0.0705 0.4287 1.0000 1.0000 ... 0.0100 0.0149 0.5920 0.1393 0.4975
4 0.0091 1.0000 1.0000 1.0000 1.0000 ... 0.0100 0.0149 0.5920 0.1393 0.4975
     ……………………………………
45949 0.0091 0.6951 0.4287 1.0000 1.0000 ... 0.4577 0.2687 0.3682 0.3035 0.8458
45950 0.0032 0.5739 1.0000 1.0000 0.0228 ... 0.6020 0.4030 0.4826 0.8010 0.1642
45951 0.0091 0.3500 0.4287 1.0000 1.0000 ... 0.6020 0.4030 0.4826 0.8010 0.1642
45952 0.0062 0.0705 1.0000 1.0000 1.0000 ... 0.7811 0.8557 0.4428 0.4478 0.5871
45953 0.0032 0.3500 0.4287 1.0000 0.0228 ... 0.7811 0.8557 0.4428 0.4478 0.5871

yelp_features_1 被标准化为yelp_feature_2。无法弄清楚使用了什么规范化。看起来像是对数。]]></description>
      <guid>https://stackoverflow.com/questions/79343281/what-kind-of-normalization-is-used</guid>
      <pubDate>Thu, 09 Jan 2025 15:56:46 GMT</pubDate>
    </item>
    <item>
      <title>为何我无法包裹 LGBM？</title>
      <link>https://stackoverflow.com/questions/79320289/why-cant-i-wrap-lgbm</link>
      <description><![CDATA[我使用 LGBM 预测数值量的相对变化。我使用 MSLE（均方对数误差）损失函数来优化我的模型并获得正确的误差缩放比例。由于 MSLE 不是 LGBM 的原生功能，因此我必须自己实现它。但幸运的是，数学可以大大简化。这是我的实现；
class MSLELGBM(LGBMRegressor):
def __init__(self, **kwargs): 
super().__init__(**kwargs)

def predict(self, X):
return np.exp(super().predict(X))

def fit(self, X, y, eval_set=None, callbacks=None):
y_log = np.log(y.copy())
print(super().get_params()) # 这不会打印任何 kwargs
if eval_set:
eval_set = [(X_eval, np.log(y_eval.copy())) for X_eval, y_eval in eval_set]
super().fit(X, y_log, eval_set=eval_set, callbacks=callbacks)

如您所见，它非常简单。我基本上只需要对模型目标应用对数变换，并对预测取指数以返回我们自己的非对数世界。
但是，我的包装器不起作用。我使用以下命令调用该类；
model = MSLELGBM(**lgbm_params)
model.fit(data[X_cols_all], data[y_col_train]) 

我收到以下异常；

-----------------------------------------------------------------------------------------
KeyError Traceback (most recent call last)
Cell In[31], line 38
32 callbacks = [
33 lgbm.early_stopping(10, verbose=0), 
34 lgbm.log_evaluation(period=0),
35 ]
37 model = MSLELGBM(**lgbm_params)
---&gt; 38 model.fit(data[X_cols_all], data[y_col_train]) 
40 feature_importances_df = pd.DataFrame([model.booster_.feature_importance(importance_type=&#39;gain&#39;)], columns=X_cols_all).T.sort_values(by=0, accending=False)
41 feature_importances_df.iloc[:30]

单元格 In[31]，第 17 行
15 if eval_set:
16 eval_set = [(X_eval, np.log(y_eval.copy())) for X_eval, y_eval in eval_set]
---&gt; 17 super().fit(X, y_log, eval_set=eval_set, callbacks=callbacks)

文件 c:\X\.venv\lib\site-packages\lightgbm\sklearn.py:1189，在 LGBMRegressor.fit(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)
1172 def fit( # type: ignore[override]
1173 self,
1174 X: _LGBM_ScikitMatrixLike,
(...)
1186 init_model: Optional[Union[str, Path, Booster, LGBMModel]] = None,
1187 ) -&gt; &quot;LGBMRegressor&quot;:
1188 &quot;&quot;&quot;Docstring 继承自 LGBMModel。&quot;&quot;&quot;
...
--&gt; 765 if isinstance(params[&quot;random_state&quot;], np.random.RandomState):
766 params[&quot;random_state&quot;] = params[&quot;random_state&quot;].randint(np.iinfo(np.int32).max)
767 elif isinstance(params[&quot;random_state&quot;], np.random.Generator):

KeyError: &#39;random_state&#39;

我不知道 random_state 为何从 fit 方法中缺失，因为该函数甚至不需要它。我感觉这是一个复杂的软件工程问题，超出了我的理解范围。有人知道发生了什么吗？
如果有帮助的话，我尝试使用更简单的非 lgbm 结构来说明我想要的内容；

我只想将我提供给 MSLELGBM 的任何参数传递给原始 LGBM，但这样做时我遇到了很多问题。]]></description>
      <guid>https://stackoverflow.com/questions/79320289/why-cant-i-wrap-lgbm</guid>
      <pubDate>Tue, 31 Dec 2024 15:25:17 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM API与Sklearn API之间的训练区别</title>
      <link>https://stackoverflow.com/questions/75649038/training-difference-between-lightgbm-api-and-sklearn-api</link>
      <description><![CDATA[我正在尝试训练 LGBClassifier 以完成多类任务。我首先尝试直接使用 LightGBM API，并按如下方式设置模型和训练：
LightGBM API
train_data = lgb.Dataset(X_train, (y_train-1))
test_data = lgb.Dataset(X_test, (y_test-1))
params = {}
params[&#39;learning_rate&#39;] = 0.3
params[&#39;boosting_type&#39;] = &#39;gbdt&#39;
params[&#39;objective&#39;] = &#39;multiclass&#39;
params[&#39;metric&#39;] = &#39;softmax&#39;
params[&#39;max_depth&#39;] = 10
params[&#39;num_class&#39;] = 8
params[&#39;num_leaves&#39;] = 500

lgb_train = lgb.train(params, train_data, 200)

# 训练后模型

y_pred = lgb_train.predict(X_test)
y_pred_class = [np.argmax(line) for line in y_pred]
y_pred_class = np.asarray(y_pred_class) + 1

混淆矩阵如下所示：

Sklearn API
然后我尝试转到 Sklearn API 以便能够使用其他工具。这是我使用的代码：
lgb_clf = LGBMClassifier(objective=&#39;multiclass&#39;,
boosting_type=&#39;gbdt&#39;,
max_depth=10,
num_leaves=500,
learning_rate=0.3,
eval_metric=[&#39;accuracy&#39;,&#39;softmax&#39;],
num_class=8,
n_jobs=-1,
early_stopping_rounds=100,
num_iterations=500)

clf_train = lgb_clf(X_train, (y_train-1), verbose=1, eval_set=[(X_train, (y_train-1)), (X_test, (y_test-1)))])

# 训练：我可以看到过度拟合正在发生

y_pred = clf_train.predict(X_test)
y_pred = [np.argmax(line) for line in y_pred]
y_pred = np.asarray(y_pred) + 1

这是本例中的混淆矩阵：

备注

我需要从 y_train 中减去 1，因为我的类从 1 开始，LightGBM 对此有抱怨。
当我尝试 RandomSearch 或 GridSearch 时，我总是得到与上一个混淆​​矩阵相同的结果。
我在这里检查了不同的问题，但没有一个能解决这个问题问题。

问题

在 Sklearn API 中实现模型时，我遗漏了什么吗？
为什么我使用 LightGBM API 获得了良好的结果（可能过度拟合）？
如何使用这两个 API 获得相同的结果？

提前致谢。
更新这是我的错误。我以为两个 API 中的输出会相同，但似乎并非如此。我刚刚在使用 Sklearn API 进行预测时删除了 np.argmax() 行。看来这个 API 已经直接预测了类。不要删除问题，以防其他人遇到类似问题。]]></description>
      <guid>https://stackoverflow.com/questions/75649038/training-difference-between-lightgbm-api-and-sklearn-api</guid>
      <pubDate>Mon, 06 Mar 2023 09:24:30 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证的实现</title>
      <link>https://stackoverflow.com/questions/60231102/implementation-of-cross-validation</link>
      <description><![CDATA[我很困惑，因为许多人都有自己的方法来应用交叉验证。例如，有些人将它应用于整个数据集，有些人将它应用于训练集。 
我的问题是，下面的代码是否适合实现交叉验证，并在应用交叉验证的情况下从此类模型进行预测？
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import KFold

model= GradientBoostingClassifier(n_estimators= 10,max_depth = 10, random_state = 0)#sepcifying the model
cv = KFold(n_splits=5, shuffle=True)

from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import cross_val_score

#X - 整个数据集
#y - 整个数据集但仅目标属性

y_pred = cross_val_predict(model, X, y, cv=cv)
scores = cross_val_score(模型，X，y，cv=cv)
]]></description>
      <guid>https://stackoverflow.com/questions/60231102/implementation-of-cross-validation</guid>
      <pubDate>Fri, 14 Feb 2020 17:37:27 GMT</pubDate>
    </item>
    <item>
      <title>如何计算神经网络预测的置信度分数</title>
      <link>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</link>
      <description><![CDATA[我正在使用深度神经网络模型（在 keras 中实现）进行预测。类似这样的代码：
def make_model():
model = Sequential() 
model.add(Conv2D(20,(5,5),activation = &quot;relu&quot;))
model.add(MaxPooling2D(pool_size=(2,2))) 
model.add(Flatten())
model.add(Dense(20,activation = &quot;relu&quot;))
model.add(Lambda(lambda x: tf.expand_dims(x, axis=1)))
model.add(SimpleRNN(50,activation=&quot;relu&quot;))
model.add(Dense(1,activation=&quot;sigmoid&quot;)) 
model.compile(loss = &quot;binary_crossentropy&quot;,optimizer = adagrad,metrics = [&quot;accuracy&quot;])

返回模型

model = make_model()
model.fit(x_train, y_train, validation_data = (x_validation,y_validation), epochs = 25, batch_size = 25, verbose = 1)

##预测：
prediction = model.predict_classes(x)
probabilities = model.predict_proba(x) #我假设这些是被预测的类的概率

我的问题是分类（二元）问题。我希望计算每个预测的置信度分数，即我想知道 - 我的模型是否 99% 确定它是“0”或 58% 是“0”。
我找到了一些关于如何做到这一点的观点，但无法实现它们。我希望遵循的方法是：“使用分类器，当你输出时，你可以将值解释为属于每个特定类别的概率。你可以使用它们的分布作为你对观察结果属于该类别的信心的粗略衡量标准。”
我应该如何使用类似上述模型的东西进行预测，以便获得对每个预测的信心？我希望有一些实际的例子（最好是在 Keras 中）。]]></description>
      <guid>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</guid>
      <pubDate>Wed, 22 Jan 2020 02:52:32 GMT</pubDate>
    </item>
    <item>
      <title>验证集上的验证程序[关闭]</title>
      <link>https://stackoverflow.com/questions/56270679/validation-procedure-on-validation-set</link>
      <description><![CDATA[我很难理解验证步骤；当我不想使用 k 倍交叉验证而只想使用验证集时，我也想得到一些建议。我一直在阅读，但似乎无法正确掌握 k 倍交叉验证：

我是否将初始数据分成 k 倍，然后在 k-1 上进行训练并在剩下的 1 上进行测试，然后继续旋转 - 因此每个折叠都用于测试等。

或者我是否将初始数据分成训练和测试数据 - 然后将训练数据分成 k 倍并进行交叉验证，然后最后在看不见的测试数据上测试准确性？

在 k 倍交叉验证期间如何选择最佳参数？
cross_val_score 在返回分数列表后，是否会在准确率最高的验证步骤中应用最佳参数？ （代码如下）


model = svm.SVC(kernel=&#39;linear&#39;, C=1)
scores = cross_val_score(model, X, y, cv=5)

或者这一步应该手动完成（由我完成）？使用 gridsearchcv 等？

就我而言，我有一个初始数据集，其中包含 400.000 个样本（行）和大约 70 个特征（列）。对我的数据集执行 k 折交叉验证需要很长时间（据我所知，它主要用于较小的数据集），相反，我希望有 3 组数据：训练（90%）验证（5%）和测试（5%）- 对那 5% 进行验证并在该步骤中调整我的模型参数，最后检查测试集的准确性。应该怎么做？
]]></description>
      <guid>https://stackoverflow.com/questions/56270679/validation-procedure-on-validation-set</guid>
      <pubDate>Thu, 23 May 2019 08:05:08 GMT</pubDate>
    </item>
    </channel>
</rss>