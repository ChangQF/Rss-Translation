<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 23 Apr 2024 09:14:10 GMT</lastBuildDate>
    <item>
      <title>如何使用 TimeSeriesSplit 处理面板数据？</title>
      <link>https://stackoverflow.com/questions/78370489/how-to-use-timeseriessplit-for-panel-data</link>
      <description><![CDATA[我一直在尝试使用TimeSeriesSplit对于面板数据。我所说的面板数据是指人口的年度图片。我对多年来的数据分割很感兴趣。该人口正在不断变化，每年的人口规模并不相同。因此，直接使用 TimeSeriesSplit 是不可能的。
基本上我试图获得以下简历方案：

我设法使用以下代码来做到这一点：
将 pandas 导入为 pd，将 numpy 导入为 np
将seaborn导入为sns，将matplotlib.pyplot导入为plt

从 sklearn.datasets 导入 make_regression
从 sklearn.dummy 导入 DummyRegressor
从 sklearn.metrics 导入mean_squared_error
从 sklearn.model_selection 导入 TimeSeriesSplit

X_测试，y_测试 = []，[]

开始年份 = 2010
年底 = 2020 年

对于 np.arange(start_year, end_year+1) 中的年份：
    X_year, y_year = make_regression(n_samples=5+year-start_year, n_features=2, 偏差=100, 噪声=1, random_state=year)
    X_year = pd.DataFrame(X_year).rename(columns={0:&#39;X1&#39;, 1:&#39;X2&#39;})
    X_year[&#39;年份&#39;] = 年
    y_year = pd.Series(y_year)
    X_test.append(X_year)
    y_test.append(y_year)
    
X_test, y_test = pd.concat(X_test), pd.concat(y_test)

# 建模

X = X_测试
y = y_测试
年 = np.unique(X_test[&#39;year&#39;])

# 建模
模型= DummyRegressor（策略=“平均值”）
指标=均方误差
cv = TimeSeriesSplit(n_splits=len(年)-1)

年数=[]
分辨率=[]

对于 i，枚举（cv.split（years））中的（train_year，test_year）：
    
    print(f&quot;折叠 {i}:&quot;)
    print(f&quot;火车：索引={years[train_year]}&quot;)
    print(f&quot;测试：index={years[test_year]}&quot;)
    
    years_folds.append((years[train_year],years[test_year]))
    
    train_filter = X[&#39;year&#39;].isin(years[train_year])
    test_filter = X[&#39;year&#39;].isin(years[train_year])
    
    X_train, y_train = X.loc[train_filter.values], y[train_filter.values]
    X_test, y_test = X.loc[test_filter.values], y[test_filter.values]
    
    model.fit(X_train, y_train)
    分数 = 指标(model.predict(X_test), y_test)
    print(f&#39; {score=:.3}&#39;)
    res.append((年份[test_year][0], 分数))

情节_年份_折叠（年份_折叠）
    
Folds_res = pd.DataFrame(res,columns=[&#39;test_year&#39;, metric.__name__])
Folds_res.plot.scatter(x=&#39;test_year&#39;, y=metric.__name__, title=f&#39;{metric.__name__} over test_year&#39;);

注意：我使用虚拟数据集和虚拟模型是为了提供运行示例。这不是我帖子的主题。
正如您所注意到的，我必须使用一个技巧：我分割年份而不是数据。我想知道：是否有一种标准方法可以使用 sklearn cv 对象分割数据？目标是在 cross_val_score 函数中使用它。]]></description>
      <guid>https://stackoverflow.com/questions/78370489/how-to-use-timeseriessplit-for-panel-data</guid>
      <pubDate>Tue, 23 Apr 2024 07:14:30 GMT</pubDate>
    </item>
    <item>
      <title>基于兴趣的匹配数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/78370399/interest-based-matching-dataset</link>
      <description><![CDATA[寻求资源以使用机器学习构建基于兴趣的匹配约会应用程序。在哪里可以找到这方面的数据集、教程和代码示例？任何指导表示赞赏！需要数据集、教程视频和代码方面的帮助。呃]]></description>
      <guid>https://stackoverflow.com/questions/78370399/interest-based-matching-dataset</guid>
      <pubDate>Tue, 23 Apr 2024 07:00:04 GMT</pubDate>
    </item>
    <item>
      <title>多级数据的机器学习算法[关闭]</title>
      <link>https://stackoverflow.com/questions/78370395/machine-learning-algorithms-for-multilevel-data</link>
      <description><![CDATA[我正在开发一个机器学习项目，我的数据集包含 1960 年到 2022 年 218 个国家/地区的社会、人口和经济方面的变量。该数据的缺失值数量很少，其中大多数都是相关的到分类变量。目标变量是一个二元变量（是或否），表示该国家在特定年份是否至少发生过一次政变企图，因此我使用了处理多级数据的机器学习模型。
我使用了这些模型：
&lt;前&gt;&lt;代码&gt;库(glmmTMB)
逻辑 &lt;- glmmTMB(目标 ~ 年+V3+V5+V11+V14+V15+V16+V19+V27+V28+V29+V33+V34+V37+V38+
                       V41+V42+V45+V46+V49+V59+V61+V62+V63+V67+V68+V69+V72+V73+V74+V75+
                       V78+V79+V81+V105+V106 +（1 | 国家/地区），数据=下，系列=“二项式”）

库（r部分）
tree_model &lt;- rpart(Target ~ ., data = under, method = &quot;class&quot;)

库（随机森林）
rf_model &lt;- randomForest(目标 ~ ., 数据 = under, ntree = 500)

randomForest.default(m, y, ...) 中的错误：
  无法处理超过 53 个类别的分类预测变量。

公制“ROC”
控制 &lt;- trainControl(方法=“cv”，数字=10，搜索=“网格”，summaryFunction = TwoClassSummary，classProbs = TRUE)
unegrid &lt;- Expand.grid(.mtry=c(1:6))
rf_model &lt;- train(Target~., data=under, method=“rf”, metric=metric,tuneGrid=tunegrid, ntree=100, trControl=control)

所有这些模型都不起作用，例如逻辑模型，它根本不收敛，或者随机森林，其他模型都表现不佳
我使用的型号正确吗？或者还有我不知道的更好的模型？]]></description>
      <guid>https://stackoverflow.com/questions/78370395/machine-learning-algorithms-for-multilevel-data</guid>
      <pubDate>Tue, 23 Apr 2024 06:59:07 GMT</pubDate>
    </item>
    <item>
      <title>我的 DQN 是否由于参数原因而未收敛？</title>
      <link>https://stackoverflow.com/questions/78370274/is-my-dqn-not-converging-because-of-parameters</link>
      <description><![CDATA[我正在使用gym cartpole 学习强化学习，并且尝试使用 Deep Q 网络来解决它，但由于某种原因我的 DQN 没有收敛
这是我的模型
类模型（nn.Module）：

    def __init__(self, in_features=4, h1=64, h2=32, out_features=2) -&gt;没有任何：
        超级().__init__()
        self.fc1 = nn.Linear(in_features,h1)
        self.fc2 = nn.Linear(h1,h2)
        self.out = nn.Linear(h2, out_features)


    def 前向（自身，x）：
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.out(x)
        返回x


这是我的 DQN 代码
&lt;前&gt;&lt;代码&gt;学习率 = 0.01
折扣系数 = 0.99
探索概率 = 1
最小探索概率 = 0.01

历元 = 1000

重播缓冲区批量大小 = 128
最小重播缓冲区大小 = 5000
重播缓冲区 = []

目标网络=模型()
策略网络=模型()

target_network.load_state_dict(policy_network.state_dict())


优化器= torch.optim.Adam(policy_network.parameters(),learning_rate)

loss_function = nn.MSELoss()

奖励=[]

对于范围内的 i（纪元）：

    explore_prob = max(min_exploration_prob, custom_decay(exploration_prob, i))

    终端=假

    如果我％10==0：
        target_network.load_state_dict(policy_network.state_dict())

    当前状态 = env.reset()

    奖励金额 = 0

    p = 假

    虽然不是终端：

        #env.render()

        如果 np.random.rand() &lt;探索概率：
            动作 = env.action_space.sample()
        别的：
            state_tensor = torch.tensor([current_state], dtype=torch.float32)
            使用 torch.no_grad()：
                q_values = 策略网络（状态张量）
            动作 = torch.argmax(q_values).item()
        
        next_state、奖励、终端、info = env.step(action)

        奖励金额+=奖励

        replay_buffer.append((current_state, 动作, 终端, 奖励, next_state))

        if(len(replay_buffer) &gt;= min_replay_buffer_size) :

            minibatch = random.sample(replay_buffer, replay_buffer_batch_size)

            batch_states = torch.tensor([transition[0] 用于小批量中的转换], dtype=torch.float32)
            batch_actions = torch.tensor([transition[1] 用于小批量中的转换], dtype=torch.int64)
            batch_terminal = torch.tensor([transition[2] 用于小批量中的转换], dtype=torch.bool)
            batch_rewards = torch.tensor([transition[3] 用于小批量中的转换], dtype=torch.float32)
            batch_next_states = torch.tensor([transition[4] 用于小批量中的转换], dtype=torch.float32)

            使用 torch.no_grad()：
                q_values_next = target_network(batch_next_states).detach()
                max_q_values_next = q_values_next.max(1)[0]

            y = batch_rewards + (discount_factor * max_q_values_next * (~batch_terminal))

            q_values =policy_network(batch_states).gather(1,batch_actions.unsqueeze(-1)).squeeze(-1)

            损失=损失函数（y，q_值）


            优化器.zero_grad()
            
            loss.backward()

            #torch.nn.utils.clip_grad_norm_(policy_network.parameters(), 3)

            优化器.step()

        如果 i%100 == 0 而不是 p：
                打印（丢失）
                p = 真

        当前状态 = 下一个状态
        
        

    奖励.append(奖励总和)

我尝试调整参数，甚至尝试了梯度裁剪。我不确定这是否是我的模型架构的错误，或者我只是错误地完成了一个步骤。任何建议将不胜感激]]></description>
      <guid>https://stackoverflow.com/questions/78370274/is-my-dqn-not-converging-because-of-parameters</guid>
      <pubDate>Tue, 23 Apr 2024 06:32:58 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 模型加载问题</title>
      <link>https://stackoverflow.com/questions/78370261/tensorflow-model-loading-issue</link>
      <description><![CDATA[我已经下载了一个模型 .keras 文件，当尝试加载它时，我收到此错误，
ValueError: 无法识别的关键字参数传递给 ConvLSTM2D: {&#39;batch_input_shape&#39;: [None, 5, 240, 240, 3], &#39;time_major&#39;: False}
这是我的代码，因为在添加 custom_objects 之前它给出了一些其他错误，我询问了聊天 gpt 并向其中添加了一些错误。
from keras.models import load_model
从 keras.layers 导入 LeakyReLU
从 keras.initializers 导入正交
从 keras.layers 导入 ConvLSTM2D
从 keras.models 导入顺序

custom_objects = {&#39;LeakyReLU&#39;：LeakyReLU，&#39;正交&#39;：正交，&#39;ConvLSTM2D&#39;：ConvLSTM2D}

模型 = load_model(&#39;./Models/CNN_Model.keras&#39;, custom_objects=custom_objects)`

我需要加载这个模型，现在就足够了。]]></description>
      <guid>https://stackoverflow.com/questions/78370261/tensorflow-model-loading-issue</guid>
      <pubDate>Tue, 23 Apr 2024 06:30:59 GMT</pubDate>
    </item>
    <item>
      <title>学生分数所需的数据申请/建议[关闭]</title>
      <link>https://stackoverflow.com/questions/78370196/student-marks-data-application-suggestions-required</link>
      <description><![CDATA[我有学生分数和人口统计数据，我想在实用程序应用程序中使用它。到目前为止，我已经提出了“标记预测”应用程序，它使用线性回归模型来预测标记。我想制作一个实用程序，学生可以在其中进行交互，提供他们的分数历史/人口统计数据以预测未来课程的分数等。
这些是我的一些想法，我愿意接受有关此类数据的更多想法/应用的建议。还请推荐工具。我可以在这里使用预制的法学硕士来进行fintune吗？还有其他工具吗？GPT？我的数据在CSV文件中以字符串和整数的形式存在。
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78370196/student-marks-data-application-suggestions-required</guid>
      <pubDate>Tue, 23 Apr 2024 06:15:17 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中制作人工智能驱动的自学习聊天机器人 [关闭]</title>
      <link>https://stackoverflow.com/questions/78370101/how-to-make-ai-powered-self-learning-chatbot-in-python</link>
      <description><![CDATA[我一直在尝试用 Python 制作一个自学习聊天机器人，并尝试了不同的库，如 NLTK、TensorFlow、ChatBot 和 PyTorch，但所有这些库都在处理预定义的训练数据。我找不到任何选项来根据给定的输入自行训练模型并尝试不同类型的数据集。
在Python中有什么方法可以实现这一点吗？
我想获得一个基于我的数据的自学习人工智能模型。]]></description>
      <guid>https://stackoverflow.com/questions/78370101/how-to-make-ai-powered-self-learning-chatbot-in-python</guid>
      <pubDate>Tue, 23 Apr 2024 05:44:52 GMT</pubDate>
    </item>
    <item>
      <title>张量流联合错误模块“tensorflow_federated.python.learning”没有属性“build_federated_averaging_process”</title>
      <link>https://stackoverflow.com/questions/78370048/tensorflow-federated-error-module-tensorflow-federated-python-learning-has-no</link>
      <description><![CDATA[我有一个机器学习代码
# 导入所有需要的库

将 pandas 导入为 pd
将 numpy 导入为 np
从 sklearn.datasets 导入 load_iris
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler

将张量流导入为 tf
将tensorflow_federated导入为tff

虹膜 = load_iris()
df = pd.DataFrame(iris.data,列=iris.feature_names)
df[&#39;物种&#39;]=iris.target

# 将数据帧拆分为输入特征和目标变量

x = df.drop(&#39;物种&#39;,axis=1)
y = df[&#39;物种&#39;]
# 创建客户端数据集的函数（假设数据已预先分区）
def create_tf_dataset(client_data):
  “”“”根据提供的客户端数据（特征、标签）创建 tf.data.Dataset。“”“”
  特征，标签= client_data
  返回 tf.data.Dataset.from_tensor_slices((特征，标签))

# 将数据拆分为clientdatasets（模拟数据分区）
客户端数据集 = []
客户数量 = 5
对于范围内的 i（num_clients）：
  start_index = int(i * (len(x) / num_clients))
  end_index = int((i + 1) * (len(x) / num_clients))
  client_features = x[开始索引:结束索引]
  client_labels = y[开始索引:结束索引]
  client_datasets.append(create_tf_dataset((client_features, client_labels)))
  # 定义模型架构（替换为您想要的模型复杂度）
def model_fn(输入):
   features, _ = input # 我们只使用特征进行分类
   稠密1 = tf.keras.layers.Dense(10, 激活=&#39;relu&#39;)(特征)
   稠密2 = tf.keras.layers.Dense(3,activation=&#39;softmax&#39;)(dense1) # 3个鸢尾花类的3个单元
   返回 tf.keras.Model(输入=特征，输出=dense2)

# 定义客户端优化器
client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)

# 定义服务器优化器（用于服务器端聚合）
server_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
fed_learning_model = tff.learning.build_federated_averaging_process(
     模型_fn，
     client_optimizer_fn=client_optimizer,
     server_optimizer_fn=服务器优化器）

但我一直收到此错误。
&lt;小时/&gt;
AttributeError Traceback（最近一次调用最后一次）
 在&lt;细胞系：1&gt;()
----&gt; 1 fed_learning_model = tff.learning.build_federated_averaging_process(
2 模型_fn，
3 client_optimizer_fn=client_optimizer,
4 server_optimizer_fn=server_optimizer)
属性错误：模块“tensorflow_federated.python.learning”没有属性“build_federated_averaging_process”
我不知道我的版本是否适合。我的tensorflow联合版本是0.76.0
我的tensorflow版本是2.14.1，python版本是3.10.12
当我在互联网上搜索时，我发现此代码不支持 tensorflow 版本 0.21.0 及以上版本。但我不知道在最新版本中使用什么]]></description>
      <guid>https://stackoverflow.com/questions/78370048/tensorflow-federated-error-module-tensorflow-federated-python-learning-has-no</guid>
      <pubDate>Tue, 23 Apr 2024 05:30:55 GMT</pubDate>
    </item>
    <item>
      <title>如何修改 Adam 优化器以在计算中不包含零？</title>
      <link>https://stackoverflow.com/questions/78369654/how-to-modify-the-adam-optimizer-to-not-include-zeros-in-the-calculations</link>
      <description><![CDATA[我在这个SO问题中找到了Adam的实现：
类 ADAMOptimizer(torch.optim.Optimizer):
    ”“”
    作为前面的步骤，实现 ADAM 算法。
    ”“”
    def __init__(自身，参数，lr=1e-3，betas=(0.9，0.999)，eps=1e-8，weight_decay=0)：
        默认值 = dict(lr=lr, betas=betas, eps=eps,weight_decay=weight_decay)
        super(ADAMOptimizer, self).__init__(参数, 默认值)

    def 步骤（自身）：
        ”“”
        执行单个优化步骤。
        ”“”
        损失=无
        对于 self.param_groups 中的组：

            对于组 [&#39;params&#39;] 中的 p：
                grad = p.grad.data
                状态 = self.state[p]

                # 状态初始化
                如果 len(状态) == 0:
                    状态[&#39;步骤&#39;] = 0
                    # 动量（梯度的指数 MA）
                    状态[&#39;exp_avg&#39;] = torch.zeros_like(p.data)

                    # RMS Prop 组件。 （平方梯度的指数 MA）。分母。
                    状态[&#39;exp_avg_sq&#39;] = torch.zeros_like(p.data)

                exp_avg, exp_avg_sq = 状态[&#39;exp_avg&#39;], 状态[&#39;exp_avg_sq&#39;]

                b1, b2 = 组[&#39;betas&#39;]
                状态[&#39;步骤&#39;] += 1

                # 添加权重衰减（如果有）
                如果组[&#39;weight_decay&#39;] != 0:
                    grad = grad.add(group[&#39;weight_decay&#39;], p.data)

                ＃ 势头
                exp_avg = torch.mul(exp_avg, b1) + (1 - b1)*grad
                
                # 有效值
                exp_avg_sq = torch.mul(exp_avg_sq, b2) + (1-b2)*(grad*grad)

                mhat = exp_avg / (1 - b1 ** 状态[&#39;步骤&#39;])
                vhat = exp_avg_sq / (1 - b2 ** 状态[&#39;步骤&#39;])
                
                denom = torch.sqrt( vhat + group[&#39;eps&#39;] )

                p.data = p.data - group[&#39;lr&#39;] * mhat / denom
                
                # 保存状态
                状态[&#39;exp_avg&#39;], 状态[&#39;exp_avg_sq&#39;] = exp_avg, exp_avg_sq

        回波损耗

我的问题是我的很多梯度都有 0 值，这会扰乱动量和速度项。我感兴趣的是修改代码，以便在计算动量和速度项（即第一和第二矩估计）时不考虑 0 值。
不过，我不确定该怎么做。如果它是一个简单的网络，其中梯度只是简单的维度，我可以检查是否 p.grad.data=0，但由于这将是一个多维张量，我不确定如何删除计算中的零并且不会弄乱其他东西（例如，剩余的更新）。]]></description>
      <guid>https://stackoverflow.com/questions/78369654/how-to-modify-the-adam-optimizer-to-not-include-zeros-in-the-calculations</guid>
      <pubDate>Tue, 23 Apr 2024 02:41:02 GMT</pubDate>
    </item>
    <item>
      <title>Python 中用于二元分类机器学习模型的 LazyClassifier 中的 ValueError？</title>
      <link>https://stackoverflow.com/questions/78368809/valueerror-in-lazyclassifier-in-python-for-binary-classification-machine-learnin</link>
      <description><![CDATA[我尝试使用 LazyClassifier 构建具有二进制目标变量的机器学习模型。

X_train 有 114074 个观测值和 15 列
X_test 有 48890 个观测值
y_train 有 114074 个观测值和 15 列
y_test 有 48890 个观测值

我创建了如下所示的 custom_metic 函数：
def custom_metric(y_train_true, y_train_pred, y_test_true, y_test_pred):
    准确度训练 = 准确度分数(y_train_true, y_train_pred)
    准确度测试 = 准确度分数(y_test_true, y_test_pred)
    
    f1_beta1_train = f1_score(y_train_true, y_train_pred, beta=1)
    f1_beta1_test = f1_score(y_test_true, y_test_pred, beta=1)
    
    f1_beta2_train = f1_score(y_train_true, y_train_pred, beta=2)
    f1_beta2_test = f1_score(y_test_true, y_test_pred, beta=2)
    
    auc_train = roc_auc_score(y_train_true, y_train_pred)
    auc_test = roc_auc_score(y_test_true, y_test_pred)
    
    precision_train = precision_score(y_train_true, y_train_pred)
    precision_test = precision_score(y_test_true, y_test_pred)
    
    召回训练 = 召回分数(y_train_true, y_train_pred)
    召回测试 = 召回分数(y_test_true, y_test_pred)
    
    返回 {
        &#39;accuracy_train&#39;：accuracy_train，&#39;accuracy_test&#39;：accuracy_test，
        &#39;f1_beta1_train&#39;：f1_beta1_train，&#39;f1_beta1_test&#39;：f1_beta1_test，
        &#39;f1_beta2_train&#39;：f1_beta2_train，&#39;f1_beta2_test&#39;：f1_beta2_test，
        &#39;auc_train&#39;：auc_train，&#39;auc_test&#39;：auc_test，
        &#39;精度训练&#39;：精度训练，&#39;精度测试&#39;：精度测试，
        &#39;recall_train&#39;：recall_train，&#39;recall_test&#39;：recall_test
    }

然后我尝试使用 LazyClassifier：
cls = LazyClassifier(ignore_warnings=True, custom_metric=custom_metric)
模型，预测 = cls.fit(X_train, X_test, y_train, y_test)

尽管如此，我收到了如下错误：
ValueError：所有数组的长度必须相同


我该如何避免该错误？]]></description>
      <guid>https://stackoverflow.com/questions/78368809/valueerror-in-lazyclassifier-in-python-for-binary-classification-machine-learnin</guid>
      <pubDate>Mon, 22 Apr 2024 20:49:06 GMT</pubDate>
    </item>
    <item>
      <title>在使用 LR 查找器代码后，如何获得使用循环学习率方法找到的最佳 LR 的学习率 (LR) 范围（最小值和最大值）？</title>
      <link>https://stackoverflow.com/questions/78368740/how-to-get-learning-rate-lr-bounds-min-and-max-values-wrt-optimal-lr-found-t</link>
      <description><![CDATA[我使用以下代码来获取给定神经网络模型的最佳学习率 - https://github.com/beringresearch/lrfinder/blob/master/lrfinder/lrfinder.py - 最终通过 get_best_lr 函数。因此，在获得最佳学习率的值后，如何以编程方式找出使用循环学习率 (CLR) 方法找到的最佳 LR 的 LR 边界（最小值和最大值）值 (https://arxiv.org/abs/1506.01186)?
来自引用的 GitHub 存储库的代码：
导入数学

将 matplotlib.pyplot 导入为 plt
导入tensorflow.keras.backend为K
将 numpy 导入为 np

从tensorflow.keras.callbacks导入LambdaCallback


LRFinder 类：
    ”“”
    训练的循环学习率中详细介绍了学习率范围测试
    神经网络 作者：Leslie N. Smith。学习率范围测试是一个测试
    它提供了有关最佳学习率的有价值的信息。期间
    预训练运行时，学习率线性增加或
    两个边界之间呈指数关系。较低的初始学习率允许
    网络开始收敛，并且随着学习率的增加
    最终会太大并且网络会发散。
    ”“”

    def __init__(自我，模型)：
        self.model = 模型
        自我损失= []
        自我学习率 = []
        self.best_loss = 1e9

    def on_batch_end（自身，批次，日志）：
        lr = K.get_value(self.model.optimizer.lr)
        self.learning_rates.append（lr）

        损失=日志[&#39;损失&#39;]
        self.losses.append(损失)

        如果批次&gt; 5 且 (math.isnan(loss) 或 loss &gt; self.best_loss * 4)：
            self.model.stop_training = True
            返回

        如果损失&lt; self.best_loss：
            self.best_loss = 损失

        lr *= self.lr_mult
        K.set_value(self.model.optimizer.lr, lr)

    def find(自我, 数据集, start_lr, end_lr, epochs=1,
             steps_per_epoch=无，**kw_fit）：
        如果steps_per_epoch为None：
            引发异常（&#39;正确训练数据生成器，&#39;
                            “steps_per_epoch”不能为“None”。”
                            &#39;你可以将其计算为&#39;
                            &#39;`np.ceil(len(TRAINING_LIST) / BATCH)`&#39;)

        self.lr_mult = (浮点(end_lr) /
                        浮动（start_lr））**（浮动（1）/
                                             浮点数（纪元*steps_per_epoch））
        初始权重 = self.model.get_weights()

        Original_lr = K.get_value(self.model.optimizer.lr)
        K.set_value(self.model.optimizer.lr, start_lr)

        回调 = LambdaCallback(on_batch_end=lambda 批次,
                                  日志：self.on_batch_end（批次，日志））

        self.model.fit（数据集，
                       纪元=纪元，回调=[回调]，**kw_fit）
        self.model.set_weights(initial_weights)

        K.set_value(self.model.optimizer.lr,original_lr)

    def get_learning_rates(自我):
        返回（自我学习率）

    def get_losses(自身):
        返回（自我损失）

    def get_derivatives(self, sma):
        断言 sma &gt;= 1
        导数 = [0] * sma
        对于范围内的 i(sma, len(self.learning_rates))：
            衍生品.append((self.losses[i] - self.losses[i - sma]) / sma)
        回报衍生品

    def get_best_lr（自身，sma，n_skip_beginning = 10，n_skip_end = 5）：
        衍生品 = self.get_derivatives(sma)
        best_der_idx = np.argmin(导数[n_skip_beginning:-n_skip_end])
        返回 self.learning_rates[n_skip_beginning:-n_skip_end][best_der_idx]
]]></description>
      <guid>https://stackoverflow.com/questions/78368740/how-to-get-learning-rate-lr-bounds-min-and-max-values-wrt-optimal-lr-found-t</guid>
      <pubDate>Mon, 22 Apr 2024 20:33:17 GMT</pubDate>
    </item>
    <item>
      <title>关于Keras历史回调损失与控制台输出损失不匹配的调查</title>
      <link>https://stackoverflow.com/questions/78360208/investigation-about-keras-history-callback-loss-not-matching-with-console-output</link>
      <description><![CDATA[请问，有谁知道为什么这个问题中描述了这个问题（Keras历史回调损失与损失的控制台输出不匹配）会发生吗？这个问题只有一个答案，它指的是可能的 TensorFlow 版本错误，但我不相信这一点，特别是因为 OP 没有对答案发表评论。我也遇到了这种情况，使用 Keras 指南中的 LossAndErrorPrintingCallback(keras.callbacks.Callback) 类和 def function on_epoch_end(self, epoch, logs=None) 函数 &lt; a href=&quot;https://keras.io/guides/writing_your_own_callbacks/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://keras.io/guides/writing_your_own_callbacks/。我还测试了 CSVLogger Keras 回调的使用，并且得到的结果与 model.fit() 输出中显示的结果不同。我使用的是 TensorFlow 2.4.1 版本。]]></description>
      <guid>https://stackoverflow.com/questions/78360208/investigation-about-keras-history-callback-loss-not-matching-with-console-output</guid>
      <pubDate>Sun, 21 Apr 2024 02:09:48 GMT</pubDate>
    </item>
    <item>
      <title>模块“keras.layers”没有属性“实验性”</title>
      <link>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</link>
      <description><![CDATA[你好，我试图调整我的数据集的大小和比例，如下所示，但我遇到了这个错误：
AttributeError：模块“keras.layers”没有属性“experimental”
&lt;前&gt;&lt;代码&gt;
resize_and_rescale= tf.keras.Sequential([
    图层.实验.预处理.调整大小(IMAGE_SIZE,IMAGE_SIZE),
    层.实验.预处理.重新缩放(1.0/255)
]）

]]></description>
      <guid>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</guid>
      <pubDate>Wed, 14 Dec 2022 00:43:49 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法让tensorflow js从json文件而不是csv读取/训练数据？</title>
      <link>https://stackoverflow.com/questions/65316105/is-there-a-way-to-make-tensorflow-js-read-train-data-from-a-json-file-instead-of</link>
      <description><![CDATA[所以我是机器学习的新手，一直在努力根据 json 文件提供的数据来训练我的模型。我用 csv 进行训练没有问题。另外，将 json 转换为 csv 也不是一个选项，因为 json 文件并不简单。]]></description>
      <guid>https://stackoverflow.com/questions/65316105/is-there-a-way-to-make-tensorflow-js-read-train-data-from-a-json-file-instead-of</guid>
      <pubDate>Wed, 16 Dec 2020 01:38:16 GMT</pubDate>
    </item>
    <item>
      <title>最大似然估计伪代码</title>
      <link>https://stackoverflow.com/questions/7718034/maximum-likelihood-estimate-pseudocode</link>
      <description><![CDATA[我需要编写一个最大似然估计器来估计一些玩具数据的均值和方差。我有一个包含 100 个样本的向量，是使用 numpy.random.randn(100) 创建的。数据应具有零均值和单位方差高斯分布。
我检查了维基百科和一些额外的资源，但我有点困惑，因为我没有统计背景。
是否有最大似然估计器的伪代码？我得到了 MLE 的直觉，但我不知道从哪里开始编码。
Wiki 表示采用对数似然的 argmax。我的理解是：我需要使用不同的参数来计算对数似然，然后我将采用给出最大概率的参数。我不明白的是：我首先在哪里可以找到参数？如果我随机尝试不同的平均值 &amp;方差以获得高概率，我什么时候应该停止尝试？]]></description>
      <guid>https://stackoverflow.com/questions/7718034/maximum-likelihood-estimate-pseudocode</guid>
      <pubDate>Mon, 10 Oct 2011 20:05:45 GMT</pubDate>
    </item>
    </channel>
</rss>