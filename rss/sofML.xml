<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 03 Jul 2024 12:29:06 GMT</lastBuildDate>
    <item>
      <title>需要的建议：从能源效率和楼宇管理系统的初级数据科学家做起 [关闭]</title>
      <link>https://stackoverflow.com/questions/78701117/advice-needed-starting-as-a-junior-data-scientist-in-energy-efficiency-and-buil</link>
      <description><![CDATA[大家早上好，我是这个论坛的新人。我是一名初级数据科学家。9 月份，我将在一家从事商业建筑能源效率和建筑管理系统的公司开始实习。我将成为第一位加入办公室的数据科学家，但老实说，我不知道从哪里开始。我参加了几次面试，以下是我大致了解的情况：他们有现场传感器，可以记录各种建筑参数，如能耗、湿度、二氧化碳和其他特征，每 45 分钟上传一次到公司仪表板。他们的使命是减少能源消耗，从而为客户省钱。他们有一个数据分析师团队，负责执行一些基本的统计。他们希望我开始实施高级统计（我可以做到），对缺失数据进行 ML（我可以做到），对价格进行一些预测，并为我能识别的其他类型的问题实施机器学习。我有一些想法，并且正在阅读几篇关于 BMS 的论文，但我想请您就我能做什么以及您是否知道我可以在哪里找到数据库来运行一些模拟提出一些建议。只要我能带来成果，他们就给了我创作自由。谢谢。
我还没有开始实习，所以我还没有尝试任何具体的方法。但是，根据我对他们的需求和我的技能的理解，我有一个总体计划：
高级统计：我计划从分析从传感器收集的数据开始，以识别模式和相关性。
缺失数据处理：我将使用机器学习技术（如 KNN、MICE 或深度学习模型）来处理缺失数据。
能源消耗预测：我的目标是使用时间序列分析和回归技术开发预测模型来预测能源消耗和价格。（需要帮助）
机器学习应用：我打算探索 ML 在优化能源使用和改善建筑管理方面的其他潜在应用。 （需要帮助）
我希望能够明确从哪里开始，并得到关于具体技术或可能有益的项目的实用建议。我还希望找到实践的资源或数据库。因此，我正在寻求社区的指导，以改进我的方法并确保我走在正确的轨道上。]]></description>
      <guid>https://stackoverflow.com/questions/78701117/advice-needed-starting-as-a-junior-data-scientist-in-energy-efficiency-and-buil</guid>
      <pubDate>Wed, 03 Jul 2024 09:17:19 GMT</pubDate>
    </item>
    <item>
      <title>我可以申请序列 keras 模型的专利吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78700633/can-i-patent-a-sequential-keras-model</link>
      <description><![CDATA[我的问题与任何代码片段都无关，但我想问一下我是否创建了一个 keras 模型
这里是从 网站 获取的示例
model = keras.Sequential()
model.add(keras.Input(shape=(16,)))
model.add(keras.layers.Dense(8))

# 请注意，您也可以省略初始“Input”。
# 在这种情况下，模型在第一次调用训练/评估方法之前没有任何权重（因为它尚未构建）：
model = keras.Sequential()
model.add(keras.layers.Dense(8))
model.add(keras.layers.Dense(4))
# model.weights 尚未创建

# 而如果您指定“输入”，则模型会在您添加层时连续构建：
model = keras.Sequential()
model.add(keras.Input(shape=(16,)))
model.add(keras.layers.Dense(8))
len(model.weights) # 返回“2”

# 使用延迟构建模式（未指定输入形状）时，您可以
# 选择通过调用
# `build(batch_input_shape)` 来手动构建模型：
model = keras.Sequential()
model.add(keras.layers.Dense(8))
model.add(keras.layers.Dense(4))
model.build((None, 16))
len(model.weights) # 返回“4”

# 请注意，使用延迟构建模式（未指定输入形状）时，
# 第一次调用 `fit`、`eval` 或 `predict` 时，
# 或第一次对某些输入数据调用模型时，模型就会构建。
model = keras.Sequential()
model.add(keras.layers.Dense(8))
model.add(keras.layers.Dense(1))
model.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;)

现在我的模型相对来说处于类似的模式，由于 keras 是一个开源库，所以我获得了专利。]]></description>
      <guid>https://stackoverflow.com/questions/78700633/can-i-patent-a-sequential-keras-model</guid>
      <pubDate>Wed, 03 Jul 2024 07:27:39 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：传递的项目数错误 9，放置位置意味着 1</title>
      <link>https://stackoverflow.com/questions/78700237/valueerror-wrong-number-of-items-passed-9-placement-implies-1</link>
      <description><![CDATA[我正在尝试计算数据框中两列之间的余弦相似度。它的代码片段如下：
def cal_cosine_similarity(row):
vec1 = np.array(row[&#39;sup_vec&#39;])
vec2 = np.array(row[&#39;vector&#39;])
return cosine_similarity([vec1], [vec2])[0][0]
cross_join_df[&#39;cos_sim&#39;] = cross_join_df.apply(cal_cosine_similarity,axis = 1)

大多数情况下，这种方法都行得通，但有时我会收到如下错误：
Traceback（最近一次调用最后一次）：
File &quot;/usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py&quot;, line 2898, in get_loc
return self._engine.get_loc(casted_key)
文件 &quot;pandas/_libs/index.pyx&quot;, 第 70 行, 位于 pandas._libs.index.IndexEngine.get_loc
文件 &quot;pandas/_libs/index.pyx&quot;, 第 101 行, 位于 pandas._libs.index.IndexEngine.get_loc
文件 &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, 第 1675 行, 位于 pandas._libs.hashtable.PyObjectHashTable.get_item
文件 &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, 第 1683 行, 位于 pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: &#39;cos_sim&#39;
上述异常是导致以下异常的直接原因异常：
回溯（最近一次调用）：
文件“/usr/local/lib/python3.8/site-packages/pandas/core/generic.py”，第 3576 行，在 _set_item 中
loc = self._info_axis.get_loc(key)
文件“/usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py”，第 2900 行，在 get_loc 中
从 err 引发 KeyError(key)
KeyError：&#39;cos_sim&#39;
在处理上述异常期间，发生了另一个异常：
回溯（最近一次调用）：
文件“/opt/prism/src/main.py”，第 79 行，在 &lt;module&gt; 中
res = job.run()
文件 &quot;/opt/prism/src/jobs/v2/SparkJob.py&quot;，第 45 行，运行中
self.start()
文件 &quot;/opt/prism/src/jobs/v2/SparkJob.py&quot;，第 71 行，启动中
raise e
文件 &quot;/opt/prism/src/jobs/v2/SparkJob.py&quot;，第 68 行，启动中
self.execute(self.input_data, 1)
文件 &quot;/opt/prism/src/jobs/v2/DprmMappingInference.py&quot;，第 289 行，执行中
cross_join_df[&#39;cos_sim&#39;] = cross_join_df.apply(cal_cosine_similarity,axis = 1)
文件&quot;/usr/local/lib/python3.8/site-packages/pandas/core/frame.py&quot;，第 3044 行，在 __setitem__ 中
self._set_item(key, value)
文件 &quot;/usr/local/lib/python3.8/site-packages/pandas/core/frame.py&quot;，第 3121 行，在 _set_item 中
NDFrame._set_item(self, key, value)
文件 &quot;/usr/local/lib/python3.8/site-packages/pandas/core/generic.py&quot;，第 3579 行，在 _set_item 中
self._mgr.insert(len(self._info_axis), key, value)
文件 &quot;/usr/local/lib/python3.8/site-packages/pandas/core/internals/managers.py&quot;，第1198，在插入中
block = make_block(values=value, ndim=self.ndim, placement=slice(loc, loc + 1))
文件 &quot;/usr/local/lib/python3.8/site-packages/pandas/core/internals/blocks.py&quot;，第 2744 行，在 make_block 中
return klass(values, ndim=ndim, placement=placement)
文件 &quot;/usr/local/lib/python3.8/site-packages/pandas/core/internals/blocks.py&quot;，第 2400 行，在 __init__ 中
super().__init__(values, ndim=ndim, placement=placement)
文件 &quot;/usr/local/lib/python3.8/site-packages/pandas/core/internals/blocks.py&quot;，第 130 行，在 __init__ 中
raise ValueError(
ValueError：项目数错误，超过 9，位置意味着 1

我无法找到此错误。此错误是否由余弦相似度函数的某些功能引起？]]></description>
      <guid>https://stackoverflow.com/questions/78700237/valueerror-wrong-number-of-items-passed-9-placement-implies-1</guid>
      <pubDate>Wed, 03 Jul 2024 05:35:57 GMT</pubDate>
    </item>
    <item>
      <title>自动编码器中的梯度消失</title>
      <link>https://stackoverflow.com/questions/78700019/vanishing-gradient-in-autoecnoder</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78700019/vanishing-gradient-in-autoecnoder</guid>
      <pubDate>Wed, 03 Jul 2024 03:59:42 GMT</pubDate>
    </item>
    <item>
      <title>如何利用小型神经网络高效地做出大量预测？</title>
      <link>https://stackoverflow.com/questions/78699009/how-to-efficiently-make-a-lot-of-predictions-with-small-neural-network</link>
      <description><![CDATA[我需要用小型神经网络（100-150 个参数）进行大量预测。我在 TensorFlow 中实现了它，但遇到了效率问题。这是伪代码：
for my_dense_netowrk,my_lstm_netowrk in networks_list
my_dense_netowrk.paramters = 100
my_lstm_netowrk.paramters = 150
for images in data[:60]:
@tf.function
def tf_wrapper(images, state):
model_data = meta_model(images)
data_prepared = image_preparation(model_data)

results = my_dense_netowrk(data_prepared)
results.shape = (19000,1,1)

better_results, state = my_lstm_netowrk(results, state)
return better_results, state

better_results, state = tf_wrapper(images, state)

my_dense_netowrk_n2.paramters = 100
my_lstm_netowrk_n2.paramters = 100

并继续...


我使用 tensorflow 数据管道 api，实际上所有必需的数据（数据变量）都可以分配到我的内存中。
在构建和将数据作为一大堆（批处理大小为 19000）插入神经网络以并行化所有内容时，我没有为我的神经网络指定批处理大小。即使是 lstm 也不会受到序列处理的瓶颈，因为它必须一次处理 19000 个输入。但是当我将神经网络参数增加 10 倍（我不需要）时，我的代码几乎没有注意到它认为批处理大小相当大。
@tf.fcuntion 加快了一切速度。
我尝试了分析，但由于发生了太多操作，未能找到瓶颈。我发现内核启动只花费了一半的时间，因为通常 tensorflow 预计这个过程会花费大量时间，所以我猜它没有针对此类任务进行优化，因为当我将循环从 60 增加到 6000 时，每次循环的效率都会提高 10 倍！似乎需要时间进行准备。
image_preparation() 函数仅使用 tf 操作，如重塑、堆叠、平铺，我无法提前准备数据。
我使用带有 M3 Max 芯片的 macOS，使用 GPU 或 CPU 没有区别。我尝试了 python 3.8、3.9、3.10、3.11、3.12。

因此，似乎 tensorflow 并没有被我的模型所限制，这很奇怪，而且互联网上没有太多关于如何有效地从小模型中获得许多预测的讨论，每个人都将这样的库用于大型 NN。虽然我认为我的管道应该会从中受益，因为我使用了大批量，但 gpu 根本没有帮助。所以我真的很难找到一个好的解决方案来解决我的问题，并想寻求建议。也许有更好的 ml 框架可以解决我的问题（PyTorch、Jax，也许还有其他？）或者我只是不擅长分析？或者我应该尝试用汇编语言构建自己的内核吗？我不知道]]></description>
      <guid>https://stackoverflow.com/questions/78699009/how-to-efficiently-make-a-lot-of-predictions-with-small-neural-network</guid>
      <pubDate>Tue, 02 Jul 2024 19:52:03 GMT</pubDate>
    </item>
    <item>
      <title>机器过期域名 gnews [关闭]</title>
      <link>https://stackoverflow.com/questions/78698880/machine-expired-domains-gnews</link>
      <description><![CDATA[我需要创建一台机器，一个程序，通过网站 https://www.expireddomains.net/ 从过期域名列表中获取仅属于 google 新闻的域名。
我需要检测 2019 年 12 月之前属于 google 新闻的 google 新闻域名
我曾尝试手动检查过期域名以查看它们是否属于 Google 新闻，但此过程非常耗时且效率低下。我希望找到一种更自动化的方法来交叉引用过期域名及其在 2019 年 12 月之前的 Google 新闻状态。
但是，我还没有找到可以有效完成此任务的现有工具或方法。我需要一个可以自动化此过程的解决方案，从而节省我的时间并确保准确性。]]></description>
      <guid>https://stackoverflow.com/questions/78698880/machine-expired-domains-gnews</guid>
      <pubDate>Tue, 02 Jul 2024 19:15:05 GMT</pubDate>
    </item>
    <item>
      <title>我的感知器和 sklearn 感知器的区别</title>
      <link>https://stackoverflow.com/questions/78698399/difference-in-my-perceptron-and-sklearn-perceptron</link>
      <description><![CDATA[我从头开始编写感知器算法，并将训练后获得的权重与训练 sklearn 感知器模型后获得的权重进行比较。我相信 sklearn 模型将权重和偏差初始化为零向量，我选择学习率 eta0=1 来匹配我的感知器代码。 （注意：我的代码中的偏差是向量 w_b 中的最后一项）
我的代码：
def perceptron(X_train, y_train):
#将权重初始化为 0
w = np.zeros(len(X_train.columns))
b = 0
w_b = np.append(w, b)
while True:
misclassifications = 0 
for X , Y in zip(X_train.values, y_train.values):
X_i = np.append(X, 1)
if Y*(np.dot(X_i,w_b)) &lt;= 0:
w_b = w_b + Y*X_i
misclassifications += 1
if misclassifications == 0:
break
return w_b

w_b = perceptron(X_train, y_train)

结果：[-3. 6.7 -1. ]
sklearn 代码：
perceptron = Perceptron(max_iter=1000, eta0=1,random_state=42) 
perceptron.fit(X_train, y_train)

print(&quot;weights are&quot;,perceptron.coef_)
print(&quot;bias is&quot;,perceptron.intercept_)

结果：weights are [[-4.7 10.1]] bias is [-2.]
我期望权重相同，但事实并非如此。有什么线索可以解释原因吗？]]></description>
      <guid>https://stackoverflow.com/questions/78698399/difference-in-my-perceptron-and-sklearn-perceptron</guid>
      <pubDate>Tue, 02 Jul 2024 17:04:46 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中使用不同分辨率图像训练 DeepLabV3 的最佳实践</title>
      <link>https://stackoverflow.com/questions/78698316/best-practice-to-train-deeplabv3-with-different-resolution-images-in-pytorch</link>
      <description><![CDATA[我正在尝试在 COCO 2017 数据集 上训练 PyTorch 的 DeepLabV3 进行语义分割，但我不确定如何处理不同分辨率的图像。我知道 DeepLab 的架构可以毫无问题地处理它们，但由于它们的分辨率，我无法将它们分批堆叠。处理此问题的最佳做法是什么？我是否将它们调整为固定大小？我是否随机裁剪固定大小？我知道有很多解决方案可以解决此问题，但我真的不知道在语义分割训练的背景下最佳做法是什么。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78698316/best-practice-to-train-deeplabv3-with-different-resolution-images-in-pytorch</guid>
      <pubDate>Tue, 02 Jul 2024 16:39:00 GMT</pubDate>
    </item>
    <item>
      <title>chemprop：RuntimeError：在“目标”中检测到以下值：张量（[0，12]），但仅预期以下值[0，1]</title>
      <link>https://stackoverflow.com/questions/78698076/chemprop-runtimeerror-detected-the-following-values-in-target-tensor-0-1</link>
      <description><![CDATA[运行 Chemprop 脚本时出现运行时错误。
所有脚本均可在此处找到：
https://github.com/chemprop/chemprop/blob/main/examples/training.ipynb
我遇到错误的部分是：
trainer.fit(mpnn, train_loader, val_loader)
两个月前它还可以正常工作。
我已经更新了所有软件包]]></description>
      <guid>https://stackoverflow.com/questions/78698076/chemprop-runtimeerror-detected-the-following-values-in-target-tensor-0-1</guid>
      <pubDate>Tue, 02 Jul 2024 15:44:02 GMT</pubDate>
    </item>
    <item>
      <title>使用 OpenCV 和自适应阈值检测和掩盖图像中的花朵</title>
      <link>https://stackoverflow.com/questions/78697737/struggling-to-detect-and-mask-flowers-in-images-using-opencv-and-adaptive-thresh</link>
      <description><![CDATA[我正在做一个项目，需要使用 OpenCV 和自适应阈值检测和掩盖图像中的花朵。尽管我付出了努力，但结果并不一致。有些花朵被很好地掩盖了，但其他花朵要么被部分掩盖，要么根本检测不到。我在 TensorFlow 中使用 Oxford Flowers 102 数据集来完成这项任务。下面是我正在使用的代码：
(train_dataset, test_dataset, validation_dataset), ds_info = tfds.load(&#39;oxford_flowers102&#39;, split=[&#39;test&#39;, &#39;train&#39;,&#39;validation&#39;], with_info=True, as_supervised=True)

def normalize_img(image, label):
image = tf.image.resize(image, (256, 256))
return tf.cast(image, tf.float32) / 255.0, label

train_dataset = train_dataset.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
train_dataset = train_dataset.cache()
train_dataset = train_dataset.shuffle(ds_info.splits[&#39;train&#39;].num_examples)
train_dataset = train_dataset.batch(32)
train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)

def detect_flowers_and_mask(image):
image_np = (image.numpy() * 255).astype(np.uint8)
gray_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY) 
binary_image = cv2.adaptiveThreshold(gray_image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 2) 
contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL，cv2.CHAIN_APPROX_SIMPLE)

如果轮廓：
max_contour = max(contours，key=cv2.contourArea)
mask = np.zeros_like(gray_image，dtype=np.uint8)
cv2.drawContours(mask，[max_contour]，-1，(255，255，255)，厚度=cv2.FILLED)
masked_image = cv2.bitwise_and(image_np，image_np，mask=mask)
else:
masked_image = image_np

返回 masked_image

plt.figure(figsize=(15, 15)) 
for i, (image_batch, label_batch) in enumerate(train_dataset.take(20)):
for image, label in zip(image_batch, label_batch):
masked_image = detect_flowers_and_mask(image)
plt.subplot(4, 5, i+1)
plt.imshow(masked_image)
plt.title(ds_info.features[&#39;label&#39;].int2str(label.numpy())) 
plt.axis(&quot;off&quot;)
plt.tight_layout()
plt.show()

]]></description>
      <guid>https://stackoverflow.com/questions/78697737/struggling-to-detect-and-mask-flowers-in-images-using-opencv-and-adaptive-thresh</guid>
      <pubDate>Tue, 02 Jul 2024 14:34:33 GMT</pubDate>
    </item>
    <item>
      <title>如何在 macOS 10.12 上运行 Core ML 模型？</title>
      <link>https://stackoverflow.com/questions/78694076/how-can-one-run-a-core-ml-model-on-macos-10-12</link>
      <description><![CDATA[https://developer.apple.com/documentation/coreml 提到 macOS 10.13+：

如何在 macOS 10.12 上运行 Core ML 模型？

在 Ubuntu 20.04 上使用 Hugging Face 的 Exporters lib:
git clone https://github.com/huggingface/exporters.git
cd exporters
pip install -e .
python -m exporters.coreml --model=distilbert-base-uncasederated/ --quantize=float32 
]]></description>
      <guid>https://stackoverflow.com/questions/78694076/how-can-one-run-a-core-ml-model-on-macos-10-12</guid>
      <pubDate>Mon, 01 Jul 2024 20:13:24 GMT</pubDate>
    </item>
    <item>
      <title>使用嵌入技术从数据库中进行人脸识别</title>
      <link>https://stackoverflow.com/questions/78688976/face-recognize-from-the-database-using-embedding-technique</link>
      <description><![CDATA[我目前正在开展一个项目，旨在识别大学记录中是否存在任何个人的照片。所提出的方法涉及将每个学生照片的嵌入及其详细信息存储在矢量数据库中。当需要比较照片时，系统将生成该照片的嵌入值，然后将该值与数据库进行比较。如果该值在特定阈值内，则表明该个人存在于记录中。
我正在寻求专家建议，以确定这种方法是否可行。如果对此方法有任何疑虑，我将不胜感激最佳解决方案的建议。]]></description>
      <guid>https://stackoverflow.com/questions/78688976/face-recognize-from-the-database-using-embedding-technique</guid>
      <pubDate>Sun, 30 Jun 2024 15:09:55 GMT</pubDate>
    </item>
    <item>
      <title>Pycaret 设置独热编码</title>
      <link>https://stackoverflow.com/questions/74001472/pycaret-setup-for-one-hot-encoding</link>
      <description><![CDATA[我陷入了 Pycaret 中分类变量独热编码的问题。问题是，即使设置了我的分类变量，管道也会对分类变量应用规范化，我不知道我做错了什么。
首先，使用下面的代码一切正常：
from pycaret.classification import *
from pycaret.datasets import get_data
import pandas as pd
import numpy as np
import seaborn as sns
dataset = get_data(&#39;income&#39;)
dataset.dtypes

直到我开始设置和
exp_clf01 = setup( data = dataset
, target = &#39;income &gt;50K&#39;
, session_id = 123
, numeric_features = [&#39;age&#39;,&#39;education-num&#39;,&#39;capital-gain&#39;,&#39;capital-loss&#39;,&#39;hours-per-week&#39;]
, categorical_features = [&#39;workclass&#39;,&#39;education&#39;,&#39;marital-status&#39;,&#39;occupation&#39;,&#39;relationship&#39;,&#39;race&#39;,&#39;sex&#39;,&#39;native-country&#39;]
)
df_transformed = get_config(&quot;X_train&quot;)
df_transformed.head()

尝试查看数据框的头部后，它仅将独热编码应用于列 race，并将其他分类输入标准化，我不明白为什么。




age
workclass
education
education-num
marital-status
occupation
other列




46.0
0.303273
0.271186
11.0
0.101942
0.484643
...


27. 0
0.218620
0.412939
13.0
0.044165
0.484643
...


33.0
0.218557
0.568315
 14.0
0.448894
0.455449
...


60.0
0.218557
0.412673
13.0
0.448894
0.484286
&lt; td&gt;...


25.0
0.218620
0.063798
6.0
0.044165
0.229692
...




我该如何防止这种行为？]]></description>
      <guid>https://stackoverflow.com/questions/74001472/pycaret-setup-for-one-hot-encoding</guid>
      <pubDate>Sun, 09 Oct 2022 00:59:48 GMT</pubDate>
    </item>
    <item>
      <title>Keras：内核和活动正则化器之间的区别</title>
      <link>https://stackoverflow.com/questions/44495698/keras-difference-between-kernel-and-activity-regularizers</link>
      <description><![CDATA[我注意到 Keras 中不再提供 weight_regularizer，取而代之的是 activity 和 kernel 正则化器。
我想知道：

kernel 和 activity 正则化器之间的主要区别是什么？
我可以使用 activity_regularizer 代替 weight_regularizer 吗？
]]></description>
      <guid>https://stackoverflow.com/questions/44495698/keras-difference-between-kernel-and-activity-regularizers</guid>
      <pubDate>Mon, 12 Jun 2017 09:16:34 GMT</pubDate>
    </item>
    <item>
      <title>Spark MLlib 中 DataFrame 的‘rawPrediction’和‘probability’列是什么意思？</title>
      <link>https://stackoverflow.com/questions/37903288/what-do-columns-rawprediction-and-probability-of-dataframe-mean-in-spark-mll</link>
      <description><![CDATA[我训练了一个 LogisticRegressionModel 之后，用它对测试数据 DF 进行了变换，得到了预测 DF。然后当我调用 prediction.show() 时，输出的列名为：[label | features | rawPrediction | probability | prediction]。我知道 label 和 featrues 是什么意思，但是我该如何理解 rawPrediction|probability|prediction？]]></description>
      <guid>https://stackoverflow.com/questions/37903288/what-do-columns-rawprediction-and-probability-of-dataframe-mean-in-spark-mll</guid>
      <pubDate>Sun, 19 Jun 2016 02:00:47 GMT</pubDate>
    </item>
    </channel>
</rss>