<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 25 Aug 2024 03:17:14 GMT</lastBuildDate>
    <item>
      <title>在拆分之前仅对 tf.data.Dataset 进行一次混洗</title>
      <link>https://stackoverflow.com/questions/78910366/shuffle-a-tf-data-dataset-before-split-only-once</link>
      <description><![CDATA[我正在使用 tf.data.Dataset 来为我的模型的 model.fit() 方法提供数据。
整个数据集无法放入我的 RAM 中，因此我需要按批次加载它，因此我考虑使用数据集生成器 (tf.data.Dataset) 在训练期间加载它并预取批次。
我用来获取训练、验证和测试数据集的函数如下所示：
def get_all_datasets(batch_size=64):
path = &#39;a/path&#39;
img_folder = &#39;src&#39;
mask_folder = &#39;masks&#39;
im_path = os.path.join(path, img_folder)
mk_path = os.path.join(path, mask_folder)

path = &#39;another/path/&#39;
img_folder = &#39;src&#39;
mask_folder = &#39;masks&#39;
im_path2 = os.path.join(path, img_folder)
mk_path2 = os.path.join(path, mask_folder)

# 创建数据集
ds1 = create_dataset(im_path, mk_path, 256, 256)
ds2 = create_dataset(im_path2, mk_path2, 256, 256)
ds = ds1.concatenate(ds2)

# 在拆分之前对整个数据集进行一次打乱
ds = ds.shuffle(buffer_size=tf.data.experimental.cardinality(ds).numpy(),
reshuffle_each_iteration=True)

# 拆分为训练集、验证集和测试集
total_size = tf.data.experimental.cardinality(ds).numpy()
train_size = int(total_size * 0.7)
val_size = int(total_size * 0.15)
test_size = total_size - train_size - val_size

# 创建单独的数据集
train_dataset = ds.take(train_size)
remaining = ds.skip(train_size)
val_dataset = remaining.take(val_size)
test_dataset = remaining.skip(val_size)

# 缓存数据集以防止重新洗牌/重新加载
train_dataset = train_dataset.cache().shuffle(batch_size).batch(batch_size).p​​refetch(buffer_size=tf.data.AUTOTUNE)
val_dataset = val_dataset.cache().batch(batch_size).p​​refetch(buffer_size=tf.data.AUTOTUNE)
test_dataset = test_dataset.cache().batch(batch_size).p​​refetch(buffer_size=tf.data.AUTOTUNE)

return train_dataset, val_dataset, test_dataset

第一次洗牌是为了使训练、验证和测试中的“场景”比例相等。这种洗牌只能进行一次，以避免污染训练、验证和测试集。
我不知道如何避免每次都重新洗牌整个集合。也许这个数据集方法也不合适。

在训练期间，整个数据集被洗牌两次，一次用于训练，另一次用于验证。我看到 shuffle 缓冲区已填满整个数据集基数。
我还考虑过在 shuffle 一次后创建三个文件夹（train、val 和 test）以获取我的拆分，然后仅使用 shuffle 方法对训练数据进行 shuffle，但如果这是解决问题的最简单方法，我会感到惊讶...
我尝试在 shuffle 之前设置一个 if 语句来检查 shuffle 是否已经完成，但 shuffle 仍然发生。我的想法是执行图已经“安排好”，并且在惰性解释期间将全局变量设置为 False。
非常感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78910366/shuffle-a-tf-data-dataset-before-split-only-once</guid>
      <pubDate>Sun, 25 Aug 2024 03:01:31 GMT</pubDate>
    </item>
    <item>
      <title>LogisticRegression 模型 predict_proba 产生了意外结果。尽管数据偏向一侧，但所有输出均为 ~0.5</title>
      <link>https://stackoverflow.com/questions/78909975/the-logisticregression-model-predict-proba-is-producing-unexpected-results-all</link>
      <description><![CDATA[我正在使用 Google Colab，它有时会在没有任何警告的情况下更新库，并且我的代码会毫无解释地中断。
对于我的项目，我正在训练一个单独的 ML 模型进行异常检测。在这种情况下，我有被视为正常的良性数据和被视为异常的恶性数据。我从一批良性和恶性数据中生成重建错误列表。我使用 LogisticRegression 模型来查找划分正常和异常数据点的决策边界。
X = np.concatenate((np.array(benign_recon_error).reshape(-1, 1), np.array(malignant_recon_error).reshape(-1, 1)))
Y = np.concatenate((np.zeros(len(benign_recon_error)), np.ones(len(malignant_recon_error))))

# 训练逻辑回归模型
lr_clf = LogisticRegression().fit(X, Y)

x_values = np.linspace(min(X), max(X), 100)
y_values = lr_clf.predict_proba(x_values)[:, 1]
print(y_values)

decision_boundary = x_values[np.abs(y_values - 0.5).argmin()]

问题是 y_values 都在 0.5 左右徘徊。
[0.49999572 0.49999602 0.49999631 0.49999661 0.49999691 0.49999721
0.49999751 0.49999781 0.4999981 0.4999984 0.4999987 0.499999
0.4999993 0.4999996 0.49999989 0.50000019 0.50000049 0.50000079 0.50000109 0.50000139 0.50000169 0.50000198 0.50000228 0.50000258 0.50000288 0.50000318 0 .50000348 0.50000377 0.50000407 0.50000437 0.50000467 0.50000497 0.50000527 0.50000556 0.50000586 0.50000616 0.50000646 0.50000676 0.50000706 0.50000736 0.50000765 0.50000795 0.50000825 0.50000855 0.50000885 0.50000915 0.50000944 0.50000974 0 .50001004 0.50001034 0.50001064 0.50001094 0.50001123 0.50001153 0.50001183 0.50001213 0.50001243 0.50001273 0.50001303 0.50001332 0.50001362 0.50001392 0.50001422 0.50001452 0.50001482 0.50001511 0.50001541 0.50001571 0.50001601 0.50001631 0 .50001661 0.5000169 0.5000172 0.5000175 0.5000178 0.5000181 0.5000184 0.5000187 0.50001899 0.50001929 0.50001959 0.50001989 0.50002019 0.50002049
0.50002078 0.50002108 0.50002138 0.50002168 0.50002198 0.50002228
0.50002258 0.50002287 0.50002317 0.50002347 0.50002377 0.50002407
0.50002437 0.50002466 0.50002496 0.50002526]

以下是数据的可视化。请记住，y 轴上的熵标签是针对数据点本身的，仅用于将数据分散到 y 轴上以防止它们聚集在一起。
异常概率图
异常预测的准确率为 0.66。
决策边界仍然有效，但回归线无效。预期行为是逻辑回归线倾斜，表明异常数据的概率随着侦察误差的增加而增加。这表明 y_value 在某种程度上是不正确的。
以下是代码按预期工作的示例。
异常概率图（工作）
虽然这似乎表明两组之间的方差有显著改善，但这些异常预测的准确率仅为 0.68。
我只是想弄清楚如何让我的代码像以前一样工作。]]></description>
      <guid>https://stackoverflow.com/questions/78909975/the-logisticregression-model-predict-proba-is-producing-unexpected-results-all</guid>
      <pubDate>Sat, 24 Aug 2024 20:55:24 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用轮廓分析来评估在 k 均值图像分割中选择的聚类数量是否最优？[关闭]</title>
      <link>https://stackoverflow.com/questions/78909544/is-it-possible-to-use-silhouette-analysis-to-assess-how-optimal-the-number-of-cl</link>
      <description><![CDATA[正如标题所述，我正在寻找方法来评估所选簇数的最优性，自动肘形法是主观的并且容易出错。我已经决定下次尝试使用轮廓分析，但根据我浏览过的教程/论文，似乎没有人使用过它。在这种情况下使用轮廓分析是否有意义？]]></description>
      <guid>https://stackoverflow.com/questions/78909544/is-it-possible-to-use-silhouette-analysis-to-assess-how-optimal-the-number-of-cl</guid>
      <pubDate>Sat, 24 Aug 2024 17:00:34 GMT</pubDate>
    </item>
    <item>
      <title>对于时间序列分类，当 shuffle = False 时，如何获得分层训练 - 测试分割？</title>
      <link>https://stackoverflow.com/questions/78909358/how-to-get-stratified-train-test-split-while-shuffle-false-for-time-series-cla</link>
      <description><![CDATA[我正在尝试使用深度学习模型构建时间序列分类器。我有一个数据集，其中有 5 种不同活动各 10 个样本（总共 50 个样本）。就好像前 10 个样本属于第 1 种活动，接下来的 10 个样本属于第 1 种活动，最后 10 个样本属于第 5 种活动。我需要将数据分成训练集和测试集，以便每个活动也应该在测试集中。问题是我无法使用 scikit-learn 中的 train_test_split，因为它不允许分层和 shuffle = False，而这正是我的情况。
我试过了：
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, 
test_size=0.3, 
stratify=y_train_val, 
random_state=42, 
shuffle = False)

我得到了错误：
Traceback（最近一次调用）：
文件 &quot;d:\Thesis\wearable_device_based_human_activity_recognition\MARS\miscellaneous\garmin_data_analysis.py&quot;，第 130 行，位于 &lt;module&gt;
data_train, data_valid = train_test_split(data_train, test_size=0.2, random_state=42, stratify = df[&#39;Class&#39;], shuffle=False)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件 &quot;D:\DevSetupFiles\anaconda3\envs\mars\Lib\site-packages\sklearn\utils\_param_validation.py&quot;，第 213 行，在包装器中
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^
文件“D:\DevSetupFiles\anaconda3\envs\mars\Lib\site-packages\sklearn\model_selection\_split.py”，第 2784 行，位于 train_test_split
raise ValueError(
ValueError: 分层训练/测试拆分未针对 shuffle=False 实施

然后我尝试将数据集按顺序拆分为训练、验证和测试，同时对测试集进行分层。最小可重现代码如下：
train, valid, test = 0.6, 0.2, 0.2
data_points = np.arange(1, 101).reshape(100, 1) 
classes = np.repeat(np.arange(10), 10) 
class_set = np.unique(classes)

df = pd.DataFrame({
&#39;DataPoint&#39;: data_points.flatten(),
&#39;Class&#39;: classes
})

print(df.head(20))

data_train, data_valid, data_test = [], [], []

for class_i in class_set:
data_inds = np.where(classes == class_i)[0]
data_i = data_points[data_inds, ...]

N_i = len(data_inds)
N_i_train = int(N_i * train)
N_i_valid = int(N_i * valid)

data_train.append(data_i[:N_i_train])
data_valid.append(data_i[N_i_train:N_i_train + N_i_valid])
data_test.append(data_i[N_i_train + N_i_valid:])

data_train = np.concatenate(data_train, axis=0)
data_valid = np.concatenate(data_valid, axis=0)
data_test = np.concatenate(data_test, axis=0)

我的方法分割数据集正确吗？我需要在保持测试集每个活动的同时保持时间顺序。在这种情况下我应该如何解决我的问题？]]></description>
      <guid>https://stackoverflow.com/questions/78909358/how-to-get-stratified-train-test-split-while-shuffle-false-for-time-series-cla</guid>
      <pubDate>Sat, 24 Aug 2024 15:28:57 GMT</pubDate>
    </item>
    <item>
      <title>24 步时间序列预测的难题：特征相关性和时间依赖性问题</title>
      <link>https://stackoverflow.com/questions/78909318/struggling-with-24-step-ahead-time-series-forecasting-issues-with-feature-corre</link>
      <description><![CDATA[我正在努力预测 24 小时后的负载数据，尽管尝试了各种算法（统计、机器学习、深度学习），但仍然面临一些挑战。以下是主要问题：
相关性低：特征（包括天气数据）与目标变量没有很强的相关性，这使得模型很难有效学习。我尝试使用滑动窗口技术，以 48 个过去值作为输入，但性能并没有太大提高。
数据中的峰值：存在明显的峰值，它们不是异常值，但具有随机性，没有特定的幅度或时间，因此很难预测。时间特征（如日、时、月和季节）也与目标显示出较弱的相关性。
时间依赖性：我的模型（ARIMA、LSTM、CNN-LSTM-Attention）难以有效捕捉与时间相关的模式。时间序列的残差部分代表最重要的部分，模型无法准确预测。
模型性能：错误率很高，尤其是在高峰时段。我尝试过大量的预处理步骤，如差分、应用对数变换、降噪和处理自相关，但结果仍然不令人满意。
ADF 统计量：-20.032363997484328
p 值：0.0
临界值：{&#39;1%&#39;：-3.4307346775420773，&#39;5%&#39;：-2.8617100123025554，&#39;10%&#39;：-2.5668604931899694}
KPSS 统计量：1.5772813072959655
p 值：0.01
临界值：{&#39;10%&#39;：0.347，&#39;5%&#39;：0.463，&#39;2.5%&#39;： 0.574, &#39;1%&#39;: 0.739}

您发现附件中显示了执行的不同分析步骤的图像，例如季节性分解、相关矩阵、ACF、PACF、目标与其滞后特征之间的相关性测试。
寻求有关改进模型性能的建议，特别是在处理这些随机峰值方面。有没有关于特征工程、替代模型或应对这些挑战的策略的提示？
[在此处输入图片描述](https://i.sstatic.net/9QIyxLbK.png)[[[在此处输入图片描述](https://i.sstatic.net/0k5QaM6C.png)](https://i.sstatic.net/MBzsxmpB.png)](https://i.sstatic.net/pBxGQQrf.png)]]></description>
      <guid>https://stackoverflow.com/questions/78909318/struggling-with-24-step-ahead-time-series-forecasting-issues-with-feature-corre</guid>
      <pubDate>Sat, 24 Aug 2024 15:12:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 LSTM 进行 ASL 识别</title>
      <link>https://stackoverflow.com/questions/78909036/asl-recognition-using-lstm</link>
      <description><![CDATA[实时模型是否可以转换为预先录制的视频上传？
例如，我将训练一个用于手语的 LSTM 模型，以实现实时识别。现在我想将其集成到 Android Studio 中的移动应用程序中，并使其预先录制。
我想将其他模型迁移到我的项目中。
我尝试了一些技术，例如 CNN 识别，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78909036/asl-recognition-using-lstm</guid>
      <pubDate>Sat, 24 Aug 2024 13:02:58 GMT</pubDate>
    </item>
    <item>
      <title>深度学习和机器学习中“学习率高、大、低、小”是什么意思？[关闭]</title>
      <link>https://stackoverflow.com/questions/78908993/what-does-learning-rate-is-high-big-low-or-small-mean-in-deep-and-machine-le</link>
      <description><![CDATA[在深度学习和机器学习中，经常会说学习率高、大、低或小，但我不知道这是什么意思。 *问题是关于如何用英语表达学习率。
例如，我在 PyTorch 中将学习率 0.000001 和 100.0 设置为 SGD() 的 lr 参数，如下所示：
示例 A：
 # ↓ 这里 ↓
optimizer = torch.optim.SGD(params=my_model.parameters(), lr=0.000001)

示例 B：
 # ↓ 这里↓
optimizer = torch.optim.SGD(params=my_model.parameters(), lr=100.0)

现在，示例A和示例B的学习率是高、大、低还是小？而常见的学习率是0.1 ~ 0.001。我不知道如何用英语表达学习率。]]></description>
      <guid>https://stackoverflow.com/questions/78908993/what-does-learning-rate-is-high-big-low-or-small-mean-in-deep-and-machine-le</guid>
      <pubDate>Sat, 24 Aug 2024 12:39:22 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 Tessaract 进行 OCR</title>
      <link>https://stackoverflow.com/questions/78908921/unable-to-do-ocr-using-tessaract</link>
      <description><![CDATA[我正在研究牙膏卷边的 OCR。所以我用手机相机拍了这些照片。我使用了 opencv 和 tessaract。这是来自 chatgpt 的基本代码。它根本不起作用，关于此的任何建议或帮助。这是图片。

有人请向我解释我该怎么做或建议我一些容易做的事情。]]></description>
      <guid>https://stackoverflow.com/questions/78908921/unable-to-do-ocr-using-tessaract</guid>
      <pubDate>Sat, 24 Aug 2024 12:00:40 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中创建具有点数据检索的交互式聚类可视化？</title>
      <link>https://stackoverflow.com/questions/78908500/how-to-create-an-interactive-clustering-visualization-with-point-data-retrieval</link>
      <description><![CDATA[我需要可视化数据点集群，并允许用户单击这些点来检索有关它们的详细信息。
我已经实现了 K-means（或任何聚类算法，如 dbscan）聚类并使用 Matplotlib 可视化了集群，但我不确定如何设置交互性，以便单击某个点显示其相关数据。
但我的主要问题是，我是否应该以特殊方式存储和构造数据以再次检索它？例如，我想识别靠近集群边界的点并检索它们的信息（可以通过单击或工具提示或任何方法）。]]></description>
      <guid>https://stackoverflow.com/questions/78908500/how-to-create-an-interactive-clustering-visualization-with-point-data-retrieval</guid>
      <pubDate>Sat, 24 Aug 2024 08:29:56 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 中的 RL 解决离散时间 LQR 问题</title>
      <link>https://stackoverflow.com/questions/78908049/struggling-to-solve-discrete-time-lqr-with-rl-in-python</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78908049/struggling-to-solve-discrete-time-lqr-with-rl-in-python</guid>
      <pubDate>Sat, 24 Aug 2024 03:11:26 GMT</pubDate>
    </item>
    <item>
      <title>如何在 PyTorch 中检查模型处于训练模式还是评估模式？</title>
      <link>https://stackoverflow.com/questions/65344578/how-to-check-if-a-model-is-in-train-or-eval-mode-in-pytorch</link>
      <description><![CDATA[如何从模型内部检查它当前处于训练模式还是评估模式？]]></description>
      <guid>https://stackoverflow.com/questions/65344578/how-to-check-if-a-model-is-in-train-or-eval-mode-in-pytorch</guid>
      <pubDate>Thu, 17 Dec 2020 16:27:34 GMT</pubDate>
    </item>
    <item>
      <title>简单来说，损失函数是什么？</title>
      <link>https://stackoverflow.com/questions/42877989/what-is-a-loss-function-in-simple-words</link>
      <description><![CDATA[有人能用简单的语言并举几个例子解释一下机器学习/神经网络领域中的损失函数是什么吗？
这是我在学习 Tensorflow 教程时想到的：
https://www.tensorflow.org/get_started/get_started]]></description>
      <guid>https://stackoverflow.com/questions/42877989/what-is-a-loss-function-in-simple-words</guid>
      <pubDate>Sat, 18 Mar 2017 18:04:55 GMT</pubDate>
    </item>
    <item>
      <title>什么是“批量标准化”？为什么要使用它？它如何影响预测？</title>
      <link>https://stackoverflow.com/questions/41269570/what-is-batch-normalizaiton-why-using-it-how-does-it-affect-prediction</link>
      <description><![CDATA[最近，许多深度架构使用“批量标准化”进行训练。
什么是“批量标准化”？它在数学上有什么作用？它以何种方式帮助训练过程？
批量标准化在训练过程中是如何使用的？它是插入模型中的特殊层吗？我需要在每一层之前进行标准化，还是只需要标准化一次？
假设我使用批量标准化进行训练。这会影响我的测试时间模型吗？我是否应该在我的“部署”网络中用其他/等效的层/操作替换批量标准化？

这个问题关于批量标准化只涵盖了这个问题的一部分，我的目标是并希望得到更详细的答案。更具体地说，我想知道批量标准化训练如何影响测试时间预测，即“部署”网络和网络的测试阶段。]]></description>
      <guid>https://stackoverflow.com/questions/41269570/what-is-batch-normalizaiton-why-using-it-how-does-it-affect-prediction</guid>
      <pubDate>Wed, 21 Dec 2016 18:30:03 GMT</pubDate>
    </item>
    <item>
      <title>ReLU 激活函数在神经网络中如何工作？</title>
      <link>https://stackoverflow.com/questions/39092201/how-does-relu-activation-function-work-in-a-neural-network</link>
      <description><![CDATA[有人能告诉我 ReLU（整流线性单元）是如何工作的吗？
从理论上讲，我明白了，但我需要一些演示。一个简单的 2x2 矩阵就可以了。希望你能在这方面帮助我。这与我正在上的深度学习课程有关。]]></description>
      <guid>https://stackoverflow.com/questions/39092201/how-does-relu-activation-function-work-in-a-neural-network</guid>
      <pubDate>Tue, 23 Aug 2016 04:09:04 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中的前向传递和后向传递是什么？</title>
      <link>https://stackoverflow.com/questions/36740533/what-are-forward-and-backward-passes-in-neural-networks</link>
      <description><![CDATA[神经网络中的前向传递和后向传递是什么意思？
每个人在谈论反向传播和时期时都会提到这些表达方式。
我的理解是前向传递和后向传递共同构成一个时期。]]></description>
      <guid>https://stackoverflow.com/questions/36740533/what-are-forward-and-backward-passes-in-neural-networks</guid>
      <pubDate>Wed, 20 Apr 2016 10:10:19 GMT</pubDate>
    </item>
    </channel>
</rss>