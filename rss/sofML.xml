<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 07 Jan 2024 09:12:52 GMT</lastBuildDate>
    <item>
      <title>在本地计算机上加载 LLM</title>
      <link>https://stackoverflow.com/questions/77772302/loading-llm-on-local-machine</link>
      <description><![CDATA[我的硬件
16GB内存
4 GB 显存 Nvidia RTX 3050
i5 12 代 12500H 处理器
我可以加载 Microsoft phi 2.7B 的完整精度并在我的本地计算机或至少 QLoraed 模型上使用它吗？
如果不是，如何计算硬件要求？
我尝试加载 7B llama 模型，但我的系统卡住并出现错误]]></description>
      <guid>https://stackoverflow.com/questions/77772302/loading-llm-on-local-machine</guid>
      <pubDate>Sun, 07 Jan 2024 07:53:06 GMT</pubDate>
    </item>
    <item>
      <title>放大后的输出图像颜色与原始图像颜色不同</title>
      <link>https://stackoverflow.com/questions/77772227/upscaled-output-image-color-different-from-orginal</link>
      <description><![CDATA[我已将 Real-ESRGAN-x44-general onnx 转换为 tflite。但得到不同的颜色输出。
输入：在此处输入图片描述
output_image_before_conversion：在此处输入图像描述
输出：在此处输入图像描述
我尝试过的代码：
导入tensorflow.lite作为tflite
将 numpy 导入为 np
导入CV2
导入操作系统

# 定义模型和图像的路径。
model_path = “realesr-general-x4v3_float32.tflite”
input_image_path = &#39;输入.jpg&#39;
输出图像路径 = &#39;输出图像.jpg&#39;

# 检查镜像文件是否存在。
如果不是 os.path.exists(input_image_path):
    引发异常（f“错误：输入图像路径 &#39;{input_image_path}&#39; 不存在。”）

# 加载 TensorFlow Lite 模型。
解释器 = tflite.Interpreter(model_path=model_path)
解释器.allocate_tensors()

# 获取输入和输出的详细信息。
input_details =terpreter.get_input_details()
输出详细信息=解释器.get_output_details()

# 加载并预处理图像。
输入图像 = cv2.imread(输入图像路径)
input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)
input_image_resized = cv2.resize(input_image_rgb, tuple(input_details[0][&#39;shape&#39;][1:3]))
# 跳过标准化以查看是否是问题所在
input_image_normalized = input_image_resized.astype(np.float32) / 255.0

# 设置未归一化的输入张量
terpreter.set_tensor(input_details[0][&#39;index&#39;], [input_image_normalized])

# 运行推理。
解释器.invoke()

# 获取输出结果。
output_data =terpreter.get_tensor(output_details[0][&#39;index&#39;])
输出图像 = np.squeeze(输出数据)

# 假设模型输出在 [0, 1] 范围内，必要时进行反规范化。
# 如果模型输出不在这个范围内，这一步可能需要调整。
输出图像非标准化 = (输出图像 * 255.0).astype(np.uint8)

# 保存颜色转换前的图像以检查原始输出。
cv2.imwrite（&#39;output_image_before_conversion.jpg&#39;，output_image_denormalized）

# 将输出图像从RGB转换为BGR，以与OpenCV的保存功能保持一致。
output_image_bgr = cv2.cvtColor(output_image_denormalized, cv2.COLOR_RGB2BGR)

# 保存颜色转换后的图像以检查最终输出。
cv2.imwrite（输出图像路径，输出图像背景）

# 打印成功消息。
print(f“输出图像已成功保存到‘{output_image_path}’。”)
]]></description>
      <guid>https://stackoverflow.com/questions/77772227/upscaled-output-image-color-different-from-orginal</guid>
      <pubDate>Sun, 07 Jan 2024 07:18:35 GMT</pubDate>
    </item>
    <item>
      <title>如何将 JSON 数据对象引入 LLM 模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77770292/how-to-ingest-json-data-object-into-an-llm-model</link>
      <description><![CDATA[我正在寻求开发一个自定义（langchain、llama 或 openAI）LLM 模型，该模型可以摄取 JSON 数据对象格式的股票数据，如下所示：
示例数据
&lt;前&gt;&lt;代码&gt;[
    {
        “符号”：“AAPL”，
        “价格”：178.72，
        “贝塔”：1.286802，
        “平均成交量”：58405568，
        “mktCap”：2794144143933，
        “最后一个Div”：0.96，
        “范围”：“124.17-198.23”，
        “变化”：-0.13，
        “公司名称”：“苹果公司”，
        “货币”：“美元”，
        “cik”：“0000320193”，
        “isin”：“US0378331005”，
        “尖头”：“037833100”，
        “交易所”：“纳斯达克全球精选”，
        “exchangeShortName”：“纳斯达克”，
        “行业”：“消费电子产品”，
        “网站”：“https://www.apple.com”，
        “首席执行官”：“先生”蒂莫西·D·库克”，
        “部门”：“技术”，
        “国家”：“美国”，
        “全职员工”：“164000”，
        “电话”：“408 996 1010”，
        “地址”：“苹果公园路一号”，
        “城市”：“库比蒂诺”，
        “州”：“CA”，
        “邮编”：“95014”，
        “dcfDiff”：4.15176，
        “DCF”：150.082，
        “图像”：“https://financialmodelingprep.com/image-stock/AAPL.png”，
        “ipoDate”：“1980-12-12”，
        “默认图像”：假，
        “isEtf”：假，
        “isActivelyTrading”：true，
        “isAdr”：假，
        “isFund”：假
    }
]

我目前正在处理一个数据集，其中包括纽约证券交易所和纳斯达克的所有股票代码。本质上，我很好奇将这些数据输入语言模型的最佳方法。我应该使用 .txt 文件来实现此目的，还是需要与 API 交互以动态检索数据？此外，是否可以直接向 LLM 提供表格数据对象？
摘要
我的目标是建立一个本地托管模型，可以解释自然语言查询，例如“提供价格低于 40 美元且属于技术领域的所有股票行情？”这将导致根据指定的条件检索相关库存数据。]]></description>
      <guid>https://stackoverflow.com/questions/77770292/how-to-ingest-json-data-object-into-an-llm-model</guid>
      <pubDate>Sat, 06 Jan 2024 17:12:22 GMT</pubDate>
    </item>
    <item>
      <title>如何在Siamese网络中实现Triplet损失？</title>
      <link>https://stackoverflow.com/questions/77769407/how-to-implement-triplet-loss-in-siamese-network</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77769407/how-to-implement-triplet-loss-in-siamese-network</guid>
      <pubDate>Sat, 06 Jan 2024 12:38:05 GMT</pubDate>
    </item>
    <item>
      <title>RandomizedSearchCV 独立于集成中的模型</title>
      <link>https://stackoverflow.com/questions/77769033/randomizedsearchcv-independently-on-models-in-an-ensemble</link>
      <description><![CDATA[假设我构建了两个估计器的集合，其中每个估计器运行自己的参数搜索：
导入和回归数据集：
从 sklearn.ensemble 导入 VotingRegressor、StackingRegressor、RandomForestRegressor
从 sklearn.tree 导入 DecisionTreeRegressor
从 sklearn.datasets 导入 make_regression

从 sklearn.model_selection 导入 RandomizedSearchCV

X, y = make_regression()

定义两个自调整估计器，并将它们组合起来：
rf_param_dist = dict(n_estimators=[1, 2, 3, 4, 5])
rf_searcher = RandomizedSearchCV(RandomForestRegressor(), rf_param_dist, n_iter=5, cv=3)

dt_param_dist = dict(max_深度=[4, 5, 6, 7, 8])
dt_searcher = RandomizedSearchCV(DecisionTreeRegressor(), dt_param_dist, n_iter=5, cv=3)

合奏 = StackingRegressor(
    [（&#39;rf&#39;，rf_searcher），（&#39;dt&#39;，dt_searcher）]
).fit(X, y)

我的问题是关于sklearn如何处理ensemble的拟合。
Q1）我们有两个并行的未拟合估计器，并且都需要在 ensemble.predict(...) 工作之前进行拟合。但是，如果没有首先从整体中获得预测，我们就无法拟合任何估计器。 sklearn 如何处理这种循环依赖？
Q2）由于我们有两个运行独立调整的估计器，每个估计器是否会错误地假设另一个估计器的参数是固定的？因此，我们最终遇到了一个定义不明确的优化问题。
&lt;小时/&gt;
作为参考，我认为联合优化集成模型的正确方法是定义一个联合搜索所有参数的 CV，如下所示。但我的问题是关于 sklearn 如何处理前面描述的特殊情况。
#联合优化
合奏 = VotingRegressor(
    [ (&#39;rf&#39;, RandomForestRegressor()), (&#39;dt&#39;, DecisionTreeRegressor()) ]
）

jointsearch_param_dist = 字典(
    rf__n_estimators=[1, 2, 3, 4, 5],
    dt__max_深度=[4,5,6,7,8]
）

ensemble_jointsearch = RandomizedSearchCV(ensemble, jointsearch_param_dist)
]]></description>
      <guid>https://stackoverflow.com/questions/77769033/randomizedsearchcv-independently-on-models-in-an-ensemble</guid>
      <pubDate>Sat, 06 Jan 2024 10:35:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么 cartpole 奖励不收敛？</title>
      <link>https://stackoverflow.com/questions/77766359/why-is-cartpole-reward-not-converging</link>
      <description><![CDATA[通过此图像，训练损失和期望值随着时间的推移而收敛，但每个情节的回报没有收敛，即使是伟大的情节。
这是我的训练循环代码：（抱歉，如果有一些混乱的事情）
def selectAction(状态):
    全局步骤_完成
    样本 = random.random()
    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)
    步骤_完成 += 1
    如果样品&gt; eps_阈值：
        使用 torch.no_grad()：
            返回policy_net(state).max(1).indices.view(1, 1)
    别的：
        返回 torch.tensor([env.action_space.sample()], device=device, dtype=torch.long).unsqueeze(0)
            
def 学习():
    如果steps_done &lt; BATCH_SIZE：
        返回

    转换=内存.样本(BATCH_SIZE)
    批处理 = 过渡(*zip(*过渡))
    
    state_batch = torch.cat(batch.state)
    action_batch = torch.cat(batch.action)
    奖励_batch = torch.cat(batch.reward)
    
    next_state_batch = torch.cat(batch.next_state)
    state_action_values = policy_net(state_batch).gather(1, action_batch)
    
    使用 torch.no_grad()：
        argmax_action = target_net(next_state_batch).max(1)[1].view(1,BATCH_SIZE)
    预期状态动作值 = 奖励批次 + GAMMA * target_net(next_state_batch).gather(1, argmax_action)

    损失 = loss_func(state_action_values,expected_state_action_values)
    
    损失.追加（损失.项目（））
    Expected_values.extend(expected_state_action_values.detach().numpy())

    优化器.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)
    优化器.step()
    
对于范围内的剧集（500）：
    状态，信息 = env.reset()
    状态 = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
    总奖励 = 0

    对于 count() 中的 t：
        env.render()
        动作=选择动作（状态）
        观察、奖励、完成、_ = env.step(action.item())[:4]
        总奖励+=奖励
        奖励 = torch.tensor([奖励], 设备=设备)

        next_state = torch.tensor(观察, dtype=torch.float32, device=device).unsqueeze(0)
        memory.push（状态，操作，next_state，奖励）
        状态 = 下一个状态
        
        学习（）
        
        target_net_state_dict = target_net.state_dict()
        policy_net_state_dict=policy_net.state_dict()
        对于policy_net_state_dict中的参数：
            target_net_state_dict[参数]=policy_net_state_dict[参数]*TAU+policy_net_state_dict[参数]*(1-TAU)

        target_net.load_state_dict(target_net_state_dict)
        
        如果完成：
            Episode_durations.append（总奖励）
            绘图持续时间（）
            休息
    print(&#39;i_episode:&#39;, 剧集)
    打印（&#39;学习步骤：&#39;，steps_done）

all_total_reward = Episode_durations
打印（“完成”）

在此处输入图像描述
有什么方法可以帮助看到收敛吗？
我尝试了很多次，有时候奖励增加了，但还是不收敛。]]></description>
      <guid>https://stackoverflow.com/questions/77766359/why-is-cartpole-reward-not-converging</guid>
      <pubDate>Fri, 05 Jan 2024 17:51:33 GMT</pubDate>
    </item>
    <item>
      <title>AWS ElasticBean CodePipeline 部署一次又一次失败。我缺少什么？</title>
      <link>https://stackoverflow.com/questions/77766259/aws-elasticbean-codepipeline-deployment-failed-again-and-again-what-am-i-missi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77766259/aws-elasticbean-codepipeline-deployment-failed-again-and-again-what-am-i-missi</guid>
      <pubDate>Fri, 05 Jan 2024 17:28:46 GMT</pubDate>
    </item>
    <item>
      <title>TF Transformer 模型永远不会过拟合，只会停滞不前：训练曲线的解读和改进建议</title>
      <link>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</link>
      <description><![CDATA[此训练曲线适用于 Transformer 模型，该模型处理 2D（不包括批次）顺序信号并使用 Adam 优化器、32 批次大小和学习率：一个自定义 LR 调度程序，它复制在“Attention is”中使用的预热调度程序所有你需要的&#39;纸。训练曲线如下所示，最终训练损失略低于验证损失，但训练损失永远不会开始回升，我将其解释为模型永远不会开始过度拟合，只是在 90 纪元后停止重新调整权重。
更好的解释和解决方案来改进这个模型？

下面是我的简短的可重现代码：
x_train = np.random.normal(size=(32, 512, 512))
批量大小 = 32
H, W = x_train.shape
行，列= np.indices（（H，W），稀疏= True）
padding_mask_init = np.zeros((H, W, W), dtype=np.bool_)
padding_mask_init[行，1：，列] = 1
padding_mask = padding_mask_init[:batch_size]
嵌入尺寸 = 512
密集_暗 = 2048
头数 = 2
形状 = (batch_size, embed_dim, 512) #(32, 512, 512)
解码器_输入=层.输入（batch_input_shape=形状，dtype=tensorflow.float16）
mha_1 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
mha_2 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
Layernorm_1 = 层.LayerNormalization()

Z = 解码器输入
Z = mha_1(查询=Z、值=Z、键=Z、use_causal_mask=True、attention_mask=padding_mask)
Z = layernorm_1(Z + 解码器输入)
Z = mha_2(查询=Z，值=解码器输入，键=解码器输入，attention_mask=padding_mask)
输出=layers.TimeDistributed（keras.layers.Dense（embed_dim，激活=“softmax”））（Z）

模型 = keras.Model(decoder_inputs, 输出)
model.compile（损失=“mean_squared_error”，optimizer=tf.keras.optimizers.Adam（learning_rate=lr_schedule（embed_dim，3000），beta_1=0.9，beta_2=0.98，epsilon=1.0e-9），metrics=[&quot; “准确度”]）

历史= model.fit（数据集，epochs = 200，validation_data = val_dataset）
]]></description>
      <guid>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</guid>
      <pubDate>Fri, 05 Jan 2024 02:47:25 GMT</pubDate>
    </item>
    <item>
      <title>在定制样本上测试 LipVoicer 模型的困难</title>
      <link>https://stackoverflow.com/questions/77746726/difficulty-testing-lipvoicer-model-on-custom-samples</link>
      <description><![CDATA[我正在使用 LipVoicer 模型，这是一个开源在线工具，旨在从无声视频中生成语音。 LipVoicer 的 GitHub 存储库位于 https://github.com/yochaiye/LipVoicer
虽然我已经在 Kaggle 笔记本上成功设置了模型环境，如下面的代码所示：
!git 克隆 https://github.com/yochaiye/LipVoicer.git
cd 唇音
!apt-get 安装 ffmpeg
!git 克隆 https://github.com/hhj1897/face_detection.git
cd 人脸检测
!apt-get 安装 git-lfs
!git lfs 拉
pip install -e 。
光盘 ..
!git 克隆 https://github.com/hhj1897/face_alignment.git
cd 面对齐
pip install -e 。
光盘 ..
!git clone --recursive https://github.com/parlance/ctcdecode.git
cd ctc解码
！点安装。

自述文件提供了模型及其设置的概述，但在指导用户完成测试阶段方面存在不足，尤其是在使用示例时。我正在寻求有关如何在我的特定样本集上有效测试 LipVoicer 模型的帮助或指导。]]></description>
      <guid>https://stackoverflow.com/questions/77746726/difficulty-testing-lipvoicer-model-on-custom-samples</guid>
      <pubDate>Tue, 02 Jan 2024 14:37:46 GMT</pubDate>
    </item>
    <item>
      <title>猜几个变量之和是一个固定值，如何预测这个固定值？</title>
      <link>https://stackoverflow.com/questions/77746513/guess-the-sum-of-several-variables-is-a-fixed-value-how-to-predict-this-fixed-v</link>
      <description><![CDATA[我知道数据框有三列（名为 A、B 和 C），现在我想预测
k1×A+k2×B+k3×C = 固定值D
A、B、C已知，k1、k2、k3、D未知（需要预测）。
如何预测这三个系数和这个固定值？
详情如下：
file_path = &#39;输入/CharacterData.xlsx&#39;
df = pd.read_excel(文件路径)
Five_star_data = df[df[&#39;star&#39;] == 5] # 选择五星级人物
X = Five_star_data[[&#39;生命值&#39;, &#39;攻击力&#39;, &#39;防御力&#39;]] # 这里我选择了关于五星级人物的三个三维度。而“生命值”、“攻击力”、“防御力”在英文中分别表示HP、ATK、DEF


我想预测：
k1×HP +k2×ATK +k3×DEF = 固定值D
肯定有误差，如何判断误差是大还是小？
另外，请原谅我的英语能力和机器学习水平不是很好。
首先，我尝试将 D 设置为固定值 1：
X = Five_star_data[[&#39;生命值&#39;, &#39;攻击力&#39;, &#39;防御力&#39;]]
y 值 = 1
y = pd.DataFrame(np.full((X.shape[0], 1), y_value))
lin_reg = 线性回归()
lin_reg.fit(X, y)
print(lin_reg.intercept_) # [1.]
打印（lin_reg.coef_）# [[0。 0.0.]]

我认为，因为X(HP,ATK,DEF)改变了，但y是固定的。所以线性回归模型认为A,B,C的系数应该为0，截距正好为1。
其次，我选择 HP(&#39;生命值&#39;) 作为 y，并将 HP 降低到 X。像这样：
X = Five_star_data[[&#39;生命值&#39;, &#39;攻击力&#39;, &#39;防御力&#39;]]
X_1 = X.drop([“生命值”], axis=1)
y = Five_star_data[&#39;生命值&#39;]
y = -y
lin_reg = 线性回归()
lin_reg.fit(X_1, y)
打印（lin_reg.intercept_）＃-9252.295197338677
打印（lin_reg.coef_）＃[5.60260844-6.41900625]

可以得到结果，但不知道是否合适。
是的，我将问题更改为：
-HP = k1×ATK + K2×EF + D。
我觉得可以改成原来的问题：
HP + k1×ATK + K2×DEF = -D。]]></description>
      <guid>https://stackoverflow.com/questions/77746513/guess-the-sum-of-several-variables-is-a-fixed-value-how-to-predict-this-fixed-v</guid>
      <pubDate>Tue, 02 Jan 2024 13:53:56 GMT</pubDate>
    </item>
    <item>
      <title>数字预测准确率 0%</title>
      <link>https://stackoverflow.com/questions/77744924/digit-prediction-give-0-accuracy</link>
      <description><![CDATA[我需要使用口袋算法制作一个数字分类器进行二元分类。因为这是一个 10 位数字的问题，所以我需要使用一对一的方法。我已经实现了基本的学习算法来找到每个分类器的权重。我对所有数字的准确率约为 98%

现在，当我运行测试数据时，我得到了 0.3% 的准确度，甚至没有 1 个正确的预测。我不太确定哪里出了问题，因为看起来每个数字的准确性有点高。
我的错误在哪里？
这是代码：
将 numpy 导入为 np
# 获取mnist数据
从 sklearn.datasets 导入 fetch_openml
从 sklearn.model_selection 导入 train_test_split

mnist = fetch_openml(&#39;mnist_784&#39;, 版本=1)
x, y = mnist[&#39;数据&#39;], mnist[&#39;目标&#39;]
x = x / 255.0 # 标准化数据


# 添加偏差的函数
def add_bias(x):
    偏差 = np.ones((x.shape[0], 1))
    返回 np.concatenate((偏差, x), 轴=1)


# 返回点积的符号。用作谓词
def 预测（权重，x）：
    返回 np.sign(np.dot(x, 权重))


def update_weights(权重, x, y):
    对于范围内的 i(len(x))：
        预测 = 预测（权重，x[i]）
        if y[i] * 预测 &lt;= 0: # 错误分类
            权重 = 权重 + y[i] * x[i]
    返回权重


# 确定权重准确性的函数
def calc_acc(权重, x, y):
    预测=预测（权重，x）
    正确 = sum(预测 == y)
    返回正确的/len(y)


def Predict_all_classifiers（分类器，样本）：
    # 将每个分类器应用于样本
    预测 = [np.dot(样本, 分类器[数字]) 对于范围(10) 中的数字]
    # 选择输出值最高的分类器
    返回 np.argmax(预测)


digital_classifier = {} # 字典来存储每个分类器的权重
y = y.astype(int) # 将数据转换为整数
对于范围 (10) 中的数字：
    # 准备数据：
    y_binary = (y == digital).astype(int) * 2 - 1 # 创建二进制目标数组
    # 将数据分割为 60K 用于训练，10K 用于测试
    X_train，X_test，y_train，y_test = train_test_split（x，y_binary，train_size = 60000，test_size = 10000，random_state = 42）
    X_train = X_train.值
    X_test = X_test.值
    y_train = y_train.值
    y_test = y_test.值

    # 初始化权重向量
    权重 = np.zeros(785) # 权重向量
    X_train_bias = add_bias(X_train) # 添加偏差值
    X_test_bias = add_bias(X_test) # 添加偏差值
    pocket_weights = np.copy(权重)

    # 使用袖珍算法训练分类器 1000 个时期
    历元 = 1000
    最佳_acc = 0.0

    对于范围内的纪元（纪元）：
        curr_weights = update_weights(np.copy(pocket_weights), X_train_bias, y_train)
        curr_acc = calc_acc(curr_weights, X_train_bias, y_train)
        如果 curr_acc &gt;最佳_ACC：
            最佳_acc = 当前_acc
            pocket_weights = np.copy(curr_weights)

    print(&quot;数字的最佳准确度：&quot; + str(digit) + &quot; 是：&quot; + str(best_acc))
    digital_classifier[digit] = pocket_weights # 将分类器添加到字典中

    # 根据测试数据进行预测和评估
    正确预测 = 0
    对于范围内的 i(len(X_test_bias))：
        # 预测数字
        Predicted_digit = Predict_all_classifiers(digit_classifier, X_test_bias[i])

        # 检查预测是否正确
        如果预测数字 == y_test[i]：
            正确预测 += 1

    # 计算准确率
    准确度 = Correct_predictions / len(X_test)
    print(f&quot;测试数据的准确度：{accuracy * 100}%&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/77744924/digit-prediction-give-0-accuracy</guid>
      <pubDate>Tue, 02 Jan 2024 08:35:38 GMT</pubDate>
    </item>
    <item>
      <title>创建多语言聊天机器人</title>
      <link>https://stackoverflow.com/questions/77737679/create-a-multilingual-chatbot</link>
      <description><![CDATA[我使用 PyTorch 创建了一个聊天机器人，我想让它支持法语。请注意，我想训练聊天机器人，以便它能够回答技术问题。
我想到的一件事是使用翻译 API，但由于聊天机器人需要回答技术问题，翻译 API 可能会提供不准确的信息]]></description>
      <guid>https://stackoverflow.com/questions/77737679/create-a-multilingual-chatbot</guid>
      <pubDate>Sat, 30 Dec 2023 23:05:27 GMT</pubDate>
    </item>
    <item>
      <title>如何用视频数据训练一些模型</title>
      <link>https://stackoverflow.com/questions/77733070/how-to-train-some-model-with-video-data</link>
      <description><![CDATA[我用这样的图像训练了我的模型：
类 ASDataset(数据集):
    def __init__(self, client_file: str, imposter_file: str, 转换=无):
        将 open(client_file, &quot;r&quot;) 作为 f：
            client_files = f.read().splitlines()
        将 open(imposter_file, &quot;r&quot;) 作为 f：
            imposter_files = f.read().splitlines()
        self.labels = torch.cat((torch.ones(len(client_files)), torch.zeros(len(imposter_files))))
        self.imgs = client_files + imposter_files
        self.transforms = 变换

    def __len__(自身):
        返回 len(self.imgs)

    def __getitem__(self, idx):
        img_name = self.imgs[idx]
        img = Image.open(img_name)
        标签 = self.labels[idx]
        如果自我变换：
            img = self.transforms(img)
        返回图片、标签

train_dataset = ASDataset(client_file=“/kaggle/input/nuaaaa/raw/client_train_raw.txt”，imposter_file=“/kaggle/input/nuaaaa/raw/imposter_train_raw.txt”，transforms=预处理)
val_dataset = ASDataset(client_file=“/kaggle/input/nuaaaa/raw/client_test_raw.txt”，imposter_file=“/kaggle/input/nuaaaa/raw/imposter_test_raw.txt”，transforms=预处理)

# 创建数据加载器
train_loader = DataLoader(train_dataset,batch_size=8,shuffle=True)
val_loader = DataLoader(val_dataset,batch_size=8,shuffle=False)


但现在我有了视频数据，据我了解，我需要更改 class ASDataset。我尝试了不同的变体。例如：
类VideoDataset（数据集）：
    def __init__(self, video_file: str, 标签: int, 转换=无):
        self.video = cv2.VideoCapture(video_file)
        self.label = 标签
        self.transforms = 变换

    def __len__(自身):
        return int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))

    def __getitem__(self, idx):
        self.video.set(cv2.CAP_PROP_POS_FRAMES, idx)
        成功，frame = self.video.read()
        如果没有成功：
            raise ValueError(“读取帧失败”)
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # 转换为RGB
        如果自我变换：
            框架 = self.transforms(框架)
        返回框架，自我标签

但是没有人给我结果。
请帮助我，如何使用视频数据进行训练？
数据示例client_train_raw.txt：
&lt;前&gt;&lt;代码&gt;/kaggle/input/dfdcdfdc/DFDCDFDC/dbnygxtwek.mp4
/kaggle/input/dfdcdfdc/DFDCDFDC/dbtbbhakdv.mp4
/kaggle/输入/dfdcdfdc/DFDCDFDC/ddepeddixj.mp4
]]></description>
      <guid>https://stackoverflow.com/questions/77733070/how-to-train-some-model-with-video-data</guid>
      <pubDate>Fri, 29 Dec 2023 16:49:09 GMT</pubDate>
    </item>
    <item>
      <title>Llama 2 上的 PEFT QLoRA 培训</title>
      <link>https://stackoverflow.com/questions/77725437/peft-qlora-training-on-llama-2</link>
      <description><![CDATA[这是一个更具概念性的问题。我正在尝试在 Llama 2 上执行 PEFT QLoRA，特别是在 imdb 电影评论数据集上。我仅使用 650 个样本进行训练，使用 650 个样本进行测试。我使用了“meta-llama/Llama-2-7b-chat-hf”模型作为我的基本 llama 2 模型。使用 SFTTrainer 训练后，我将模型保存到目录中。如果我没有记错的话，只有适配器权重会保存到目录中，而不是整个模型权重。完成此操作后，我知道这些适配器权重可以与原始模型权重一起加载。
模型 = PeftModel.from_pretrained(
    模型，
    “./my_dir”，
）

完成此操作后，我们应该将这些适配器权重合并到原始模型
merged_model = model.merge_and_unload()

但是，当我使用此 merged_model 进行推理时，我注意到性能非常差，因为仅对 PEFT 加载的模型进行推理，即来自
模型 = PeftModel.from_pretrained(
    模型，
    “./my_dir”，
）

是理想的。这种行为是预期的吗？我正在这样进行推理
tokenizer = AutoTokenizer.from_pretrained(“meta-llama/Llama-2-7b-chat-hf”)
管道=变压器.管道(
    “文本生成”，
    型号=型号，
    分词器=分词器，
    torch_dtype=torch.float16,
    device_map=“自动”，
）

序列=管道（
    迅速的，
    do_sample=真，
    顶部_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    最大长度=500，
）
对于序列中的 seq：
    print(f&quot;结果: {seq[&#39;generate_text&#39;]}&quot;)

还有什么我可以做得更好的吗？]]></description>
      <guid>https://stackoverflow.com/questions/77725437/peft-qlora-training-on-llama-2</guid>
      <pubDate>Thu, 28 Dec 2023 07:01:41 GMT</pubDate>
    </item>
    <item>
      <title>如何为 Transformer 实现位置明智的前馈神经网络？</title>
      <link>https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers</link>
      <description><![CDATA[我很难理解 Transformer 架构中的位置明智前馈神经网络。

让我们以机器翻译任务为例，其中输入是句子。从图中我了解到，对于每个单词，不同的前馈神经网络用于自注意力子层的输出。前馈层应用类似的线性变换，但每个变换的实际权重和偏差不同，因为它们是两个不同的前馈神经网络。
参考链接，这里是类PositionWiseFeedForward神经网络
class PositionwiseFeedForward(nn.Module)：
    “实现 FFN 方程。”
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def 前向（自身，x）：
        返回 self.w_2(self.dropout(F.relu(self.w_1(x))))

我的问题是：
我没有看到任何关于此的立场。这是一个简单的两层全连接神经网络。假设 x 是句子中每个单词的嵌入列表，句子中的每个单词都由上层使用相同的权重和偏差集进行转换。（如果我错了，请纠正我）
我期望找到类似将每个单词嵌入传递到单独的Linear层之类的东西，该层将具有不同的权重和偏差，以实现与图片中所示类似的效果。]]></description>
      <guid>https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers</guid>
      <pubDate>Mon, 02 Jan 2023 05:59:25 GMT</pubDate>
    </item>
    </channel>
</rss>