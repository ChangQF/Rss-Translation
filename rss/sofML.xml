<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 17 Aug 2024 15:14:47 GMT</lastBuildDate>
    <item>
      <title>训练时间随机变化，没有明显原因</title>
      <link>https://stackoverflow.com/questions/78882116/training-time-randomly-varies-without-apparent-reason</link>
      <description><![CDATA[我正在 Nvidia RTX4090 GPU 中训练 Keras Tensorflow ResNet50 模型。我使用 Python 3.10、TF 2.10（我使用的是 Windows）、Keras 2.10、CUDA 12.5、CuDNN 8.9 和 PyCharm 作为解释器。该模型通常每轮大约需要 2 分钟。但是，我观察到有时在不更改任何超参数并使用完全相同的输入的情况下，一轮可能需要长达一小时。此外，这种情况有时会在同一次运行中发生：第一个轮需要 15 分钟，但其他轮需要 2 分钟，或者模型以正常速度运行三​​个轮，第四个轮需要半个多小时。在代码的开头，我使用了 tf.config.experimental.set_memory_growth(device, True) 和 tf.keras.backend.clear_session()，并且我检查了 GPU 使用情况，发现两种情况下的 GPU 使用情况相同。我是机器学习和 Keras 的新手，有什么我遗漏的吗？
我希望使用相同输入的相同代码的训练时间相同]]></description>
      <guid>https://stackoverflow.com/questions/78882116/training-time-randomly-varies-without-apparent-reason</guid>
      <pubDate>Sat, 17 Aug 2024 11:40:40 GMT</pubDate>
    </item>
    <item>
      <title>Azure AI 自定义图像分析模型未返回性能指标</title>
      <link>https://stackoverflow.com/questions/78881768/azure-ai-custom-image-analysis-model-not-returning-performance-metrics</link>
      <description><![CDATA[成功训练并使用自定义图像分析模型后，我没有获得任何性能指标。在 Vision Studio 的 UI 和执行代码这两个地方，我都收到以下错误：
状态：错误（code={&#39;code&#39;: &#39;InternalServerError&#39;, &#39;message&#39;: &#39;批处理未成功完成。OutputResultWriteFailed：遇到意外错误。&#39;}, message=&#39;&#39;, target=&#39;&#39;, details=[])

您有什么建议，如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78881768/azure-ai-custom-image-analysis-model-not-returning-performance-metrics</guid>
      <pubDate>Sat, 17 Aug 2024 08:44:21 GMT</pubDate>
    </item>
    <item>
      <title>如何加速随机森林回归和SVR的训练？</title>
      <link>https://stackoverflow.com/questions/78881480/how-to-speed-up-training-of-random-forest-regression-and-svr</link>
      <description><![CDATA[我目前正在尝试使用以下数据集创建一个回归模型来预测比特币的收盘价。
数据集：https://www.kaggle.com/datasets/prasoonkottarathil/btcinusd/data?select=BTC-2021min.csv
它有超过 60 万条记录，包含 15 个特征（其中一些是我创建的）。
我曾多次尝试在 google colab 和我的笔记本电脑上对其进行训练。我甚至把它放了一夜，但它花了太长时间。
有什么方法可以加快速度吗？
笔记本电脑规格：
CPU：Ryzen 7 5800H
GPU：RTX 3050
RAM：16 GB
这是训练代码。
models = {
&#39;Linear Regression&#39;：{
&#39;model&#39;：LinearRegression()，
&#39;params&#39;：{}
},
&#39;Ridge Regression&#39;：{
&#39;model&#39;：Riddom_state=42，
&#39;params&#39;：{&#39;alpha&#39;：[0.01, 0.1, 1, 5, 10, 50, 100]}
},
&#39;Lasso Regression&#39;：{
&#39;model&#39;： Lasso(random_state=42),
&#39;params&#39;: {&#39;alpha&#39;: [0.001, 0.01, 0.1, 1, 10]}
},
&#39;决策树&#39;: {
&#39;模型&#39;: DecisionTreeRegressor(random_state=42),
&#39;params&#39;: {&#39;max_depth&#39;: [None, 5, 10, 20], &#39;min_samples_split&#39;: [2, 5, 10]}
},
&#39;随机森林&#39;: {
&#39;模型&#39;: RandomForestRegressor(random_state=42),
&#39;params&#39;: {&#39;n_estimators&#39;: [50, 100, 200], &#39;max_depth&#39;: [None, 5, 10], &#39;min_samples_split&#39;: [2, 5, 10]}
},
&#39;支持向量回归&#39;: {
&#39;model&#39;: SVR(),
&#39;params&#39;: {&#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;], &#39;C&#39;: [0.1, 1, 10], &#39;epsilon&#39;: [0.01, 0.1, 1]}
}
}

results = {}

for model_name, model_data in models.items():
print(f&quot;Tuning {model_name}&quot;)
grid_search = GridSearchCV(model_data[&#39;model&#39;], model_data[&#39;params&#39;], cv=5,scoring=&#39;neg_mean_squared_error&#39;, verbose=1)
grid_search.fit(X_train, y_train)

# 获取最佳模型
best_model = grid_search.best_estimator_

# 预测
y_pred = best_model.predict(X_test)

# 性能指标
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

results[model_name] = {
&#39;MAE&#39;: mae,
&#39;MSE&#39;: mse,
&#39;RMSE&#39;: rmse,
&#39;R2&#39;: r2,
&#39;Best Model&#39;: best_model,
&#39;Best Params&#39;: grid_search.best_params_
}


感谢您的时间。]]></description>
      <guid>https://stackoverflow.com/questions/78881480/how-to-speed-up-training-of-random-forest-regression-and-svr</guid>
      <pubDate>Sat, 17 Aug 2024 05:30:15 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 模型上实际数据和预测数据之间的转换</title>
      <link>https://stackoverflow.com/questions/78881405/shift-between-real-and-predicted-data-on-lstm-model</link>
      <description><![CDATA[我不明白为什么我的预测数据（股票）仍然有差距，或者说在之前的真实数据之后发生了变化，而不是预测数据。这是图片：
X 横坐标：时间（以小时为单位）
Y 横坐标：价格（以美元为单位）
红色曲线：实际价格，
蓝色曲线：预测价格

需要知道的是，用于训练和测试的数据都是标准化的。
当然，我随机混合了我的数据。
每行代表一个系列（32 个报价）

尽管在上图中，价格并未标准化。
所以你可以看到，一开始价格预测得很好，但是随着预测的深入，有时候会出错，就好像它没有考虑到真实数据，修正后的数据。
我将提供训练文件：链接到我的 google Drive 文件]]></description>
      <guid>https://stackoverflow.com/questions/78881405/shift-between-real-and-predicted-data-on-lstm-model</guid>
      <pubDate>Sat, 17 Aug 2024 04:30:50 GMT</pubDate>
    </item>
    <item>
      <title>参数逻辑模型的 AUC 如何能比 BART 模型更好？</title>
      <link>https://stackoverflow.com/questions/78881371/how-can-a-parametric-logit-model-have-a-better-auc-than-its-bart-equivalent</link>
      <description><![CDATA[我正在研究一个由大约 20k 个观察值和 620 个回归量组成的表格数据集，其中 25 个回归量是密集的，其余的则非常稀疏。大约 5-10 个回归量是连续变量，而其余的则是二分变量。同样，我的目标变量也是二分变量。我将通过标准 maxLH 方法估计的标准参数 logit 模型与带有 logit 链接器的简单贝叶斯加性回归树 (BART) 模型（包 BART - 函数 lbart）的结果进行了比较
从 Logit 回归中，我发现大多数回归量都不显著。即使是那些理论预测显著的回归量。这是内生性问题和可能的共线性（如果我没记错的话）的预期结果，这困扰着我的观察。有趣的是，参数逻辑回归的 AUC 约为 0.7，这对于社会科学估计问题来说相对较高。
另一方面，BART 模型在理论认为重要的变量的平均治疗效果上提供了预期的强可信区间。然而，AUC 低于参数逻辑回归！大约 0.68。
我想知道，我该如何调和这种相互矛盾的结果？具有明显病理规范的 Logit 模型的预测能力远远优于 BART 模型，后者在重要变量的可信区间上显示出相当令人信服的结果。
谢谢大家。
我运行了具有稳健聚类标准误差的参数逻辑回归和标准 BART 模型。结果与预期相冲突]]></description>
      <guid>https://stackoverflow.com/questions/78881371/how-can-a-parametric-logit-model-have-a-better-auc-than-its-bart-equivalent</guid>
      <pubDate>Sat, 17 Aug 2024 04:09:13 GMT</pubDate>
    </item>
    <item>
      <title>如何创建一个可以回答与论文分析和论文评分相关的问题的模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78881222/how-do-i-create-a-model-that-can-answer-questions-related-to-essay-analysis-and</link>
      <description><![CDATA[我正在为一个项目制作一个 ReactJS 应用程序，用户可以在其中输入他们的文章/作品，当他们单击按钮时，它会显示一堆 AI 分析。此分析包括拼写检查、估计的教师分数和一个聊天机器人区域，用户可以在其中就他们的论文提问并获得反馈。我使用 Flask python 服务器和一些基于我找到的文章数据集的简单机器学习制作了拼写检查和教师分数区域。
我是 NLP 的初学者，我正在使用这个项目来学习它，所以我不知道我在做什么？哈哈，我只是在寻找一种资源，可以帮助我理解如何实现我正在做的事情，而它与前 10 个不一样。
我尝试使用我的分数来生成输入，例如（如果分数 &lt; 70，则建议更好的语法），但它似乎太静态了。我也尝试使用不同的可读性分数测量方法，例如 flesh-Kincaid，但它给出的响应并不像我希望的那样个性化。我希望它能够根据用户的文章唯一地生成。
我也尝试使用 BERT（我认为是一种语言模型），但我对微调它所需的数据感到困惑。我在网上看到的所有示例都太具体了，与我的问题无关，所以我不知道如何开始解决问题。我应该坚持使用静态文本生成还是尝试使用 BERT？我担心我没有足够的数据来正确训练它。它甚至对聊天机器人有帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/78881222/how-do-i-create-a-model-that-can-answer-questions-related-to-essay-analysis-and</guid>
      <pubDate>Sat, 17 Aug 2024 01:36:26 GMT</pubDate>
    </item>
    <item>
      <title>MatplotLib 未显示图像</title>
      <link>https://stackoverflow.com/questions/78881216/matplotlib-is-not-showing-the-image</link>
      <description><![CDATA[MatplotLib 不显示图像的原因是什么？
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
plt.figure(figsize=(10, 10))
plt.imshow(image_rgb)
plt.show()

它不显示任何图像。
更新：
以下是更新后的代码：
import matplotlib.pyplot as plt

image_path = &#39;/Users/johndoe/Desktop/machine-learning/traffic-light.png&#39;
image = cv2.imread(image_path)

plt.imshow(image)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78881216/matplotlib-is-not-showing-the-image</guid>
      <pubDate>Sat, 17 Aug 2024 01:24:40 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试训练 TensorFlow 模型时，出现了“ValueError”，我不明白为什么</title>
      <link>https://stackoverflow.com/questions/78881211/i-get-a-valueerror-when-i-try-to-train-my-tensorflow-model-and-i-dont-underst</link>
      <description><![CDATA[这是我在第一个 epoch 调用 model.fit 时遇到的错误：

&quot;发生异常：ValueError
Layer &quot; functional&quot; 需要 2 个输入，但它收到了 1 个输入张量。收到的输入：[&lt;tf.Tensor &#39;data:0&#39; shape=(None, 128) dtype=float32&gt;]
文件 &quot;D:\workspace\Machine Learning 545\PSU_classes\cs445_group_project\code\Keras Music Genres Classification\encoder_decoder_feature_extractor.py&quot;，第 177 行，位于 train_encoder_decoder_model
model.fit(x = X_train,
文件 &quot;D:\workspace\Machine Learning 545\PSU_classes\cs445_group_project\code\Keras Music Genres Classification\encoder_decoder_feature_extractor.py&quot;，第 217 行，位于 
trained_model = train_encoder_decoder_model(encoder_decoder_model, X_train, y_train, X_test, y_test)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError：层“ functional”需要 2 个输入，但收到 1 个输入张量。收到的输入：[&lt;tf.Tensor&#39;data：0&#39;shape=(None，128)dtype=float32&gt;]&quot;

这是我的模型：
def define_encoder_decoder_model(num_features):
# 定义编码器
encoder_inputs = 输入（shape=(None，num_features))
encoder_hidden1 = Dense(100, 激活=&#39;relu&#39;)(encoder_inputs)
coder_hidden2 = Dense(50, 激活=&#39;relu&#39;)(encoder_hidden1)
coder_lstm = LSTM(25, return_state=True)
coder_outputs, state_h, state_c =coder_lstm(encoder_hidden2)
coder_states = [state_h, state_c]

# 定义解码器
decoder_inputs = 输入(shape=(None, 25))
decoder_hidden1 = Dense(50, 激活=&#39;relu&#39;)(decoder_inputs)
decoder_hidden2 = Dense(100, 激活=&#39;relu&#39;)(decoder_hidden1)
decoder_lstm = LSTM(num_features, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decrypt_lstm(decoder_hidden2, initial_state=encoder_states)
decoder_dense = Dense(num_features,activation=&#39;softmax&#39;)
decoder_outputs =coder_dense(decoder_outputs)

# 定义将encoder_inputs和decoder_inputs转换为decoder_outputs的模型
model = Model([encoder_inputs,decoder_inputs],decoder_outputs)

# 编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;,&#39;precision&#39;,&#39;recall&#39;,&#39;f1_score&#39;])

# 模型摘要
model.summary()

在此处返回modeltype

这是我调用model.fit的方式：
def train_encoder_decoder_model(model,X_train, y_train, X_test, y_test):
&quot;&quot;&quot;
使用提供的数据训练编码器-解码器模型。

参数：
model：要训练的编码器-解码器模型。
X_train：输入训练数据。
y_train：目标训练数据。
X_test：输入测试数据。
y_test：目标测试数据。

返回：
训练好的编码器-解码器模型。
&quot;&quot;&quot;
print(&quot;shapes: X_train:&quot;, np.shape(X_train),&quot; y_train: &quot;, np.shape(y_train),&quot; X_test: &quot;, np.shape(X_test),&quot; y_test: &quot;, np.shape(y_test))
# 训练模型
# 将每个时期的训练日志附加到文件中
file_logger = FileLogger(&#39;training.log&#39;)
y_train_T = tf.convert_to_tensor(np.array([y_train]).T)
y_test_T = tf.convert_to_tensor(np.array([y_test]).T)
#x_train = tf.convert_to_tensor(X_train)
y_train = X_train
y_test = X_test
model.fit(x = X_train,
y= y_train,
batch_size=100,
epochs=100,
verbose=2,
validation_data=(X_test, y_test),
callbacks=[file_logger])
返回模型

这是一个编码器-解码器模型，因此 X_train 数据集等于 y_train，X_test、y_test 也一样。在第一种情况下，训练集的形状为 (799,128)，测试数据集为 (299,128)。特征表示为“float64”值。
我在 Visual Studio Code 下运行代码。我将数据预处理为标准化和缩放的数据集，然后将其分为训练数据集和测试数据集，构建我的编码器-解码器模型（参见上面的方法），然后尝试训练模型。我得到的是 model.fit “Epoch 1/100” 的输出和上面显示的错误消息。
我不明白这个错误，也不知道需要改正什么。
如有任何建议，我们将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78881211/i-get-a-valueerror-when-i-try-to-train-my-tensorflow-model-and-i-dont-underst</guid>
      <pubDate>Sat, 17 Aug 2024 01:20:22 GMT</pubDate>
    </item>
    <item>
      <title>随机森林模型的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/78881067/issue-with-random-forest-model</link>
      <description><![CDATA[我正在尝试使用 Python 创建一些模型来预测城市与公司的合作，但我一直在数据解析部分遇到问题，我尝试制作一个随机森林模型，但在读取列时遇到了问题，即使我清理了数据并将列名更改为每个文件中的完全相同，我该如何解决这个问题？
这些是我正在使用的数据集：
text
import pandas as pd
import matplotlib.pyplot as plt

# 加载数据集
cities_disclosing_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Predicting-city-collaboration-with-business/Datasets/Data/Cities/Cities Disclosing/2020_Cities_Disclosing_to_CDP.csv&quot;)
corp_climate_change_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Data/Corporations/Corporations Disclosing/Climate Change/2020_Corporates_Disclosing_to_CDP_Climate_Change.csv&quot;)

# 加载数据集
cities_disclosing_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Predicting-city-collaboration-with-business/Datasets/Data/Cities/Cities Disclosing/2020_Cities_Disclosing_to_CDP.csv&quot;)
cities_responses_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Data/Cities/Cities Responses/2020_Full_Cities_Dataset.csv&quot;)

corp_climate_change_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Data/Corporations/Corporations Disclosing/Climate Change/2020_Corporates_Disclosing_to_CDP_Climate_Change.csv&quot;)
corp_water_security_2020 = pd.read_csv(&quot;C:/Users/User/OneDrive/Documents/Data/Corporations/Corporations Disclosing/Water Security/2020_Corporates_Disclosing_to_CDP_Water_Security.csv&quot;)

# 合并数据集
merged_2020 = pd.merge(cities_disclosing_2020, corp_climate_change_2020, on=[&#39;Account_Number&#39;, &#39;Year&#39;], suffixes=(&#39;_city&#39;, &#39;_corp_climate&#39;))
merged_2020 = pd.merge(merged_2020, corp_water_security_2020, on=[&#39;Account_Number&#39;, &#39;Year&#39;], suffixes=(&#39;&#39;, &#39;_corp_water&#39;))

merged_responses_2020 = pd.merge(cities_responses_2020, corp_climate_change_2020, on=[&#39;Account_Number&#39;, &#39;Year&#39;], suffixes=(&#39;_city&#39;, &#39;_corp_climate&#39;))
merged_responses_2020 = pd.merge(merged_responses_2020, corp_water_security_2020, on=[&#39;Account_Number&#39;, &#39;Year&#39;], suffixes=(&#39;&#39;, &#39;_corp_water&#39;))

# 计算协作率
merged_2020[&#39;climate_change_collaboration&#39;] = (merged_2020[&#39;theme_city&#39;] == merged_2020[&#39;theme_corp_climate&#39;]).astype(int)
merged_2020[&#39;water_security_collaboration&#39;] = (merged_2020[&#39;theme_city&#39;] == merged_2020[&#39;theme_corp_water&#39;]).astype(int)

# 计算影响
merged_responses_2020[&#39;impact&#39;] = merged_responses_2020[&#39;Response Answer_city&#39;].apply(lambda x: len(str(x)))

# 计算协作率和平均影响
climate_change_collab_rate = merged_2020[&#39;climate_change_collaboration&#39;].mean()
water_security_collab_rate = merged_2020[&#39;water_security_collaboration&#39;].mean()
average_impact_2020 = merged_responses_2020[&#39;impact&#39;].mean()

print(f&quot;2020 年气候变化协作率：{climate_change_collab_rate}&quot;)
print(f&quot;2020 年水安全协作率：{water_security_collab_rate}&quot;)
print(f&quot;2020 年对城市的平均影响：{average_impact_2020}&quot;)

# 协作率条形图
collab_rates = {
&#39;气候变化&#39;:climate_change_collab_rate,
&#39;水安全&#39;: water_security_collab_rate
}
plt.bar(collab_rates.keys(), collab_rates.values())
plt.title(&#39;2020 年的协作率&#39;)
plt.ylabel(&#39;Rate&#39;)
for i, rate in enumerate(collab_rates.values()):
plt.text(i, rate + 0.01, f&#39;{rate:.2f}&#39;, ha=&#39;center&#39;, va=&#39;bottom&#39;)
plt.show()

# 影响分布的直方图
plt.hist(merged_responses_2020[&#39;impact&#39;], bins=20, edgecolor=&#39;black&#39;)
plt.title(&#39;2020 年的影响分布&#39;)
plt.xlabel(&#39;影响&#39;)
plt.ylabel(&#39;频率&#39;)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/78881067/issue-with-random-forest-model</guid>
      <pubDate>Fri, 16 Aug 2024 23:07:44 GMT</pubDate>
    </item>
    <item>
      <title>什么时候需要将数据居中？[关闭]</title>
      <link>https://stackoverflow.com/questions/78880910/when-is-it-necessary-to-center-the-data</link>
      <description><![CDATA[我正在尝试找出不同的缩放方法，我有以下问题。StandardScaler、RobustScaler 和类似的缩放器将数据相对于平均值或中位数居中，但 MinMaxScaler 不会这样做，它只会将数据传输到 0 到 1 的范围。什么时候应该将数据居中，什么时候不需要？
我还读到，当数据已经呈正态分布时，最好使用 StandardScaler，但如果只有一部分数据具有此属性怎么办？我应该对一半数据使用 StadardScaler，对另一半数据使用 MinMaxScaler（听起来不合逻辑）还是优先使用其中一种方法？]]></description>
      <guid>https://stackoverflow.com/questions/78880910/when-is-it-necessary-to-center-the-data</guid>
      <pubDate>Fri, 16 Aug 2024 21:39:50 GMT</pubDate>
    </item>
    <item>
      <title>Reshape RuntimeError：形状'[1, 2]'对于大小为 100 的输入无效</title>
      <link>https://stackoverflow.com/questions/78878681/reshape-runtimeerror-shape-1-2-is-invalid-for-input-of-size-100</link>
      <description><![CDATA[我正在 Pytorch 中构建一个简单的足球比分预测模型。
我的步骤：

清理和处理数据
使用球队名称作为输入，将分数作为输出
对球队名称进行独热编码
将两个球队名称传递给模型
添加非线性
（问题就在这里）接收两个分数作为输出

但是我的模型多次返回两个分数
tensor([[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
        [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5 232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4845], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4 844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844],
        [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5 232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844], [0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844],
[0.5232, 0.4844]], grad_fn=&lt;SigmoidBackward0&gt;)

这是我的 NN 的输出，但我不确定为什么当我的输出层大小为 2 个神经元时，它会返回 50x2。
我的模型：
class EstimatorModel(nn.Module):
def __init__(self,
input_neurons:int,
hidden_​​neurons:int,
output_neurons:int):

super().__init__()
self.input_neurons= nn.Linear(in_features=input_neurons, out_features=hidden_​​neurons)
self.hidden_​​neurons= nn.Linear(in_features=hidden_​​neurons, out_features=hidden_​​neurons)
self.output_neurons= nn.Linear(in_features=hidden_​​neurons, out_features=output_neurons)

# 激活函数
self.sigmoid = nn.Sigmoid()

def forward(self, x):
x = self.input_neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.hidden_​​neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.hidden_​​neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.hidden_​​neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

x = self.output_neurons(x)
#x = self.relu(x)
x = self.sigmoid(x)

return x

使用带有 Adam 优化器的 MSEloss。
训练循环：
 for e in range(epochs):
x=0
for input, output in zip(input_data, output_data):
input = stack(inputs, dim=1)
output = stack(outputs, dim=1)
#outputs = tensor(outputs

print(outputs.shape)
print(outputs)

prediction:Tensor = model(inputs)
prediction_processed = model.process_model_output(prediction)

print(prediction.shape)
print(prediction)

loss = loss_func(prediction, output.float())

optim.zero_grad()
loss.backward()
optim.step()

quit()

x+=1
# 如果您不喜欢太冗长，请注释掉此打印语句
#print(f&quot;[INFO] {x+1} 对完成。损失：{loss}&quot;)

有什么建议吗？
尝试改变输入传递的方式，例如，正常传递，作为单个张量传递，最后作为堆栈传递。没有运气。]]></description>
      <guid>https://stackoverflow.com/questions/78878681/reshape-runtimeerror-shape-1-2-is-invalid-for-input-of-size-100</guid>
      <pubDate>Fri, 16 Aug 2024 10:20:05 GMT</pubDate>
    </item>
    <item>
      <title>如何在 android kotlin 中实现输出形状为 [1, 25200, 6],[1, 2, 640, 640],[1, 2, 640, 640] 的 yolop ( 分割模型 ) [关闭]</title>
      <link>https://stackoverflow.com/questions/78878176/how-can-i-implement-yolop-segemntation-model-with-output-shapes-1-25200-6</link>
      <description><![CDATA[我正在开发一个 Android 项目，需要使用 YOLOp 模型进行分割。我已成功将模型转换为 onnx/tflite 格式，但在处理输出形状时遇到了困难。该模型包含三个输出，形状分别为 [1, 25200, 6]、[1, 2, 640, 640] 和 [1, 2, 640, 640]
如何在我的 Android 应用程序中解释和处理此输出形状？具体来说，我需要帮助来理解输出张量的结构并提取对象的边界框坐标和分割蒙版。
是否有任何与在 Android 中使用此特定输出形状实现 YOLOP TFLite 相关的代码片段、建议或资源？]]></description>
      <guid>https://stackoverflow.com/questions/78878176/how-can-i-implement-yolop-segemntation-model-with-output-shapes-1-25200-6</guid>
      <pubDate>Fri, 16 Aug 2024 08:05:49 GMT</pubDate>
    </item>
    <item>
      <title>Python mediappe手部识别方块优化[关闭]</title>
      <link>https://stackoverflow.com/questions/78872856/python-mediappe-hand-recognition-square-optimization</link>
      <description><![CDATA[所以 python 有 mediapipe lib，它提供了识别照片/视频上手的工具
我在我的项目中使用它。但我正在考虑优化 - 所以它会运行得更快。
我们知道，如果在 1 帧中有一只手 - 它在另一帧中不会太远 - 所以没有必要重复整个照片 - 手周围的区域就足够了（包括测量它的角速度和径向速度）
mediapipe 是否包含一些优化方法？或者代码只是愚蠢地检查了整张照片？
我还没有搜索过有关这方面的信息。]]></description>
      <guid>https://stackoverflow.com/questions/78872856/python-mediappe-hand-recognition-square-optimization</guid>
      <pubDate>Wed, 14 Aug 2024 20:38:41 GMT</pubDate>
    </item>
    <item>
      <title>sentence-transformers：自定义分块函数和 encode_multi_process() 的组合并行化</title>
      <link>https://stackoverflow.com/questions/78855135/sentence-transformers-combined-parallelization-for-custom-chunking-function-and</link>
      <description><![CDATA[我正在使用 Python 3.10，使用句子转换器模型来编码/嵌入文本字符串列表。我想使用句子转换器的 encode_multi_process 方法来利用我的 GPU。这是一个非常特殊的函数，它接受一个字符串或一个字符串列表，并生成一个数字向量（或向量列表）。该函数将工作分配给系统 CPU 和 GPU。
我还想并行化我的自定义分块函数 create_chunks，它将原始文本字符串拆分成足够小的块以适应模型的约束。因此，对于任何给定的文本输入，它必须先经过 create_chunks，然后再经过 encode_multi_process。我很确定使用多个 CPU 内核来并行化此步骤是可行的方法。
现在，我正在考虑使用 multiprocessing 将 create_chunks 应用于我的数据集，然后使用 encode_multi_process，但这似乎效率低下：从 create_chunks 中产生的块必须等到整个数据集完成后才能继续使用 encode_multi_process。有没有更高效的 Python 替代方案？我必须围绕 encode_multi_process 构建我的解决方案，这是主要的困难。
我希望我可以使用 Dask，但语言模型太大，无法放入 Dask 任务图中。]]></description>
      <guid>https://stackoverflow.com/questions/78855135/sentence-transformers-combined-parallelization-for-custom-chunking-function-and</guid>
      <pubDate>Sat, 10 Aug 2024 03:16:07 GMT</pubDate>
    </item>
    <item>
      <title>设置分类器参数并直接使用，无需拟合</title>
      <link>https://stackoverflow.com/questions/48252006/set-parameters-for-classifier-and-use-it-without-fitting</link>
      <description><![CDATA[我正在使用 python 和 scikit-learn 进行一些分类。
是否可以重复使用分类器学习到的参数？
例如：
from sklearn.svm import SVC

cl = SVC(...) # 使用一些超参数创建 svm 分类器
cl.fit(X_train, y_train)
params = cl.get_params()

让我们将这个 params 存储在某个字符串字典中，甚至写入 json 文件。假设，我们稍后想使用这个经过训练的分类器对一些数据进行一些预测。尝试恢复它：
params = ... # 检索以字典形式存储在某处的这些参数
data = ... # 我们想要预测的数据
cl = SVC(...)
cl.set_params(**params)
predictions = cl.predict(data)

如果我这样做，我会得到 NonFittedError 和以下堆栈跟踪：
File &quot;C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py&quot;, line 548, in predict
y = super(BaseSVC, self).predict(X)
File &quot;C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py&quot;, line 308, in predict
X = self._validate_for_predict(X)
文件“C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py”，第 437 行，位于 _validate_for_predict
check_is_fitted(self, &#39;support_&#39;)
文件“C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\utils\validation.py”，第 768 行，位于 check_is_fitted
raise NotFittedError(msg % {&#39;name&#39;: type(estimator).__name__})
sklearn.exceptions.NotFittedError：此 SVC 实例尚未拟合。使用此方法前，请使用适当的参数调用“fit”。

是否可以设置分类器的参数并在不进行拟合的情况下进行预测？我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/48252006/set-parameters-for-classifier-and-use-it-without-fitting</guid>
      <pubDate>Sun, 14 Jan 2018 17:04:03 GMT</pubDate>
    </item>
    </channel>
</rss>