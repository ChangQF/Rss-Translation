<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 25 Jul 2024 12:28:45 GMT</lastBuildDate>
    <item>
      <title>使用 CNN 进行单类物体检测获得假阳性</title>
      <link>https://stackoverflow.com/questions/78793283/single-class-object-detection-using-cnn-getting-false-positive</link>
      <description><![CDATA[在这里，我尝试使用 cnn 构建一个 Manhole 物体检测，在这个模型中，经过训练我得到了 95% 的准确率。我得到的是假阳性，例如，如果我用人孔（训练对象）测试图像进​​行检测，它将绘制边界框，而我测试没有训练对象的随机图像时，会出现一个随机边界框，这就是问题所在，在实时网络摄像头测试中也是如此，但在这里，如果对象甚至没有被检测到，就会在框架中得到一些随机边界框。这里我提供我的代码，请帮忙
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Conv2D, Input, BatchNormalization``, Flatten, MaxPool2D, Dense
from pathlib import Path

train_img = Path(&quot;DATASET/train/Manhole&quot;)
val_img = Path(&quot;DATASET\valid\Manhole&quot;)

train_csv = pd.read_csv(&#39;DATASET/train/Manhole/_annotations.csv&#39;) 
val_csv = pd.read_csv(&#39;DATASET/valid/_annotations.csv&#39;)
#print(train_csv)
train_csv[[&#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;]] = train_csv[[&#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;]].fillna(0)
train_csv[[&#39;xmin&#39;,&#39;ymin&#39;,&#39;xmax&#39;,&#39;ymax&#39;]] = train_csv[[&#39;xmin&#39;,&#39;ymin&#39;,&#39;xmax&#39;,&#39;ymax&#39;]].astype(int)
train_csv.drop_duplicates(subset=&#39;filename&#39;,inplace=True, ignore_index=True)
val_csv.drop_duplicates(subset=&#39;filename&#39;, inplace=True, ignore_index=True)

def datagenerator(df ,batch_size ,path):
while True:
images = np.zeros((batch_size,640,640,3))
bounding_box_coords = np.zeros((batch_size, 4))

for i in range(batch_size):
rand_index = np.random.randint(0, train_csv.shape[0])
row = df.loc[rand_index, :]
images[i] = cv2.imread(str(path/row.filename)) / 255.
bounding_box_coords[i] = np.array([row.xmin, row.ymin, row.xmax, row.ymax])

产生 {&#39;filename&#39;: images}, {&#39;coords&#39;: bounding_box_coords}

# example, label = next(datagenerator(batch_size=16))
# img = example[&#39;filename&#39;][0]
# bbox_coords = label[&#39;coords&#39;][0] 

# x1, y1, x2, y2 = map(int, bbox_coords)
# print(&#39;bbox cords&#39;,x1,y1,x2,y2)
# cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)
# cv2.putText(img, &#39;&#39;, (x1,y1-10),cv2.FONT_HERSHEY_DUPLEX, 0.8, (0, 0, 255), 2)
# # plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
# plt.imshow(img)
# plt.show()

input_ = 输入(shape=[640, 640, 3], name=&#39;filename&#39;)

x = input_
x = Conv2D(16, (3,3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2, 填充=&#39;same&#39;)(x)

x = Conv2D(32, (3,3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2, 填充=&#39;same&#39;)(x)

x = Conv2D(64, (3,3),激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(128，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(256，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(312，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(500，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(580，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(680，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Flatten()(x)
x = Dense(256，激活=&#39;relu&#39;)(x)
x = Dense(32, 激活=&#39;relu&#39;)(x)
输出坐标 = Dense(4, 名称=&#39;coords&#39;)(x)

模型 = tf.keras.models.Model(input_,output_coords)

模型摘要()

模型编译(loss={&#39;coords&#39;: &#39;mse&#39;},
优化器=tf.keras.optimizers.Adam(5e-5), 
指标={&#39;coords&#39;: &#39;accuracy&#39;})

检查点回调 = ModelCheckpoint(&#39;model_Checkpoint.h5&#39;, 监视器=&#39;val_loss&#39;, save_best_only=True, 模式=&#39;min&#39;)

模型拟合(数据生成器(df=train_csv,batch_size=6,path=train_img), 
epochs=80, steps_per_epoch=150,
validation_data=datagenerator(df=val_csv,batch_size=6,path=val_img), 
validation_steps=240, 
callbacks=[checkpoint_callback])

model.save(&#39;model2.h5&#39;)

我需要代码来在实时网络摄像头中正确检测训练过的对象，而不会出现任何边界框，并从 cnn 接收置信度值，这样我就可以设置检测的阈值]]></description>
      <guid>https://stackoverflow.com/questions/78793283/single-class-object-detection-using-cnn-getting-false-positive</guid>
      <pubDate>Thu, 25 Jul 2024 12:22:40 GMT</pubDate>
    </item>
    <item>
      <title>EMA 衰减与 LR 衰减之间的实际差异</title>
      <link>https://stackoverflow.com/questions/78793123/practical-difference-between-ema-decay-and-lr-decay</link>
      <description><![CDATA[如果这个问题很愚蠢，请原谅，但我无法理解 EMA 衰减和 LR 衰减在实践中的差异。
我觉得它们都以不同的方式完成了相同的事情（以下内容可能是错误的，所以我提前道歉，如果我的理解完全错误，请纠正我）：

使用 EMA，在训练期间保留模型的单独副本，并且每 N 步使用原始模型权重的平均值更新模型。
使用 LR 衰减，原始模型的权重在训练期间更新的次数总是较少，但只有一个模型得到有效训练。

现在，给定一个包含 32 个样本的数据集，我可以想象这是两次训练的进行方式：
训练 A（无 EMA）
给定以下内容超参数：

LR 为 1e-5
批次大小为 4
线性调度程序

经过 4 个步骤后，模型将看到 16 个样本，LR 将下降到最终 LR 的一半。
实际上，模型已更新 4 次。
训练 B (EMA)
给定以下超参数：

LR 为 1e-5
批次大小为 1
恒定调度程序
EMA 衰减为 0.9999
EMA 更新步骤为 4

经过 16 个步骤后，模型将看到 16 个样本， EMA 衰减将达到最终 EMA 衰减的一半。
实际上，原始模型已更新 16 次，EMA 模型已更新 4 次。
问题
最终，两个模型都更新了 4 次，我能看到的唯一区别是训练 A 直接更新了权重，而训练 B 更新了原始模型和 EMA 模型的权重。
为什么人们决定选择训练 B 而不是训练 A？]]></description>
      <guid>https://stackoverflow.com/questions/78793123/practical-difference-between-ema-decay-and-lr-decay</guid>
      <pubDate>Thu, 25 Jul 2024 11:49:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 Gratz 大学 LSM 模型预测 Lorenz 吸引子</title>
      <link>https://stackoverflow.com/questions/78792982/predicting-lorenz-attractor-using-the-gratz-university-lsm-model</link>
      <description><![CDATA[我目前正在研究 LSM，在阅读了几篇论文后，我认为我掌握了有关这种 Reservoir Computing 模型的主要信息。
但是，我很难用我用于 Lorenz Attractor 的模型获得良好的结果。
目前，我正试图仅预测 X 分量，但结果很糟糕。
我尝试了 NEST 模拟器 中的两种生成器。spike_generator 和 step_current_generator。
使用 step_current_generator 我有更好的结果，但我想坚持使用 W.Maas 模型并使用 spike_generator。
这是我所做的：
def generate_spike_times_lorenz(data, stim_times, gen_burst, scale_factor=100.0):
&quot;&quot;&quot;
根据 Lorenz X 分量生成尖峰时间。

参数：
- data：类似数组，Lorenz X 分量的值。
- stim_times：类似数组，刺激的时间。
- gen_burst：函数，在给定时间附近生成一连串尖峰。
- scale_factor：浮点数，缩放尖峰时间的因子。

返回：
- inp_spikes：数组列表，每个数组包含输入神经元的尖峰时间。
“” “”

inp_spikes = []

data = data * scale_factor

for value, t in zip(data, stim_times):
spike_count = int(value)
if spike_count &gt; 0:
spikes = np.concatenate([t + gen_burst() for _ in range(spike_count)])

# 缩放并调整尖峰时间
spikes *= 10
spikes = spikes.round() + 1.0
spikes = spikes / 10.0

spikes = np.sort(spikes)

inp_spikes.append(spikes)
else:
inp_spikes.append(np.array([]))

return inp_spikes

def injection_spikes(inp_spikes, neuron_targets):
spike_generators = nest.Create(&quot;spike_generator&quot;, len(inp_spikes))

for sg, sp in zip(spike_generators, inp_spikes):
nest.SetStatus(sg, {&#39;spike_times&#39;: sp})

C_inp = 100 # int(N_E / 20) # 每个输入神经元的传出输入突触数量

def generate_delay_normal_clipped(mu=10., sigma=20., low=3., high=200.):
delay = np.random.normal(mu, sigma)
delay = max(min(delay, high), low)
return delay

nest.Connect(spike_generators, neuron_targets,
{&#39;rule&#39;: &#39;fixed_outdegree&#39;,
&#39;outdegree&#39;: C_inp},
{&#39;synapse_model&#39;: &#39;static_synapse&#39;,
&#39;delay&#39;: generate_delay_normal_clipped(),
&#39;weight&#39;: nest.random.uniform(min=2.5 * 10 * 5.0, max=7.5 * 10 * 5.0)})

有人能提供一些见解或建议，告诉我如何改进我的模型，特别是在使用 spike_generator 来提高性能方面吗？我可能忽略了哪些特定的参数或技术？
非常感谢您抽出时间和提供帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78792982/predicting-lorenz-attractor-using-the-gratz-university-lsm-model</guid>
      <pubDate>Thu, 25 Jul 2024 11:11:09 GMT</pubDate>
    </item>
    <item>
      <title>寻找机器学习模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78791542/finding-a-machine-learning-model</link>
      <description><![CDATA[我需要帮助寻找一个机器学习模型，该模型可以查看数值，并且可以使用我可以重新利用的一组标准进行预测。
我研究过现有的模型，但我只能找到预测房价的模型。]]></description>
      <guid>https://stackoverflow.com/questions/78791542/finding-a-machine-learning-model</guid>
      <pubDate>Thu, 25 Jul 2024 05:12:34 GMT</pubDate>
    </item>
    <item>
      <title>在列联表和博彩公司知情度/标记度的背景下，DTP 是什么？</title>
      <link>https://stackoverflow.com/questions/78791154/what-is-dtp-in-the-context-of-contingency-tables-and-bookmaker-informedness-mark</link>
      <description><![CDATA[Powers 使用 dtp 概念定义博彩公司知情度/标记度：
“我们可以通过将每个表达式的顶部和底部简化为概率（除以 N2，注意原始偶然性计数总和为 N，简化后的联合概率总和为 1），进一步了解这些回归和相关系数的性质。分子是偶然性矩阵的决定因素，并且是所有三个系数的共同点，简化为 dtp，而回归系数的简化分母仅取决于基础变量的普遍性或偏差。
因此，回归系数博彩公司知情度 (B) 和标记度 (M) 可以用准确率 (Prec) 或召回率以及偏差和流行度 (Prev) 或它们的倒数 (I-) 重新表示：&quot;
全文链接：https://arxiv.org/pdf/2010.16061
博彩公司标记度 = dtp/ [偏差 · (1-偏差)]
博彩公司知情度 = dtp/ [rp·rn]

我不太明白 DTP 是什么。它似乎是列联表的行列式，但我对 DTP 微积分或其首字母缩略词的含义都不是 100% 确定。他说的“所有 3 个系数都相同”是什么意思？在计算行列式之前，我应该将整个矩阵除以 N² 吗？
我期待 DTP 含义及其微积分的正式定义]]></description>
      <guid>https://stackoverflow.com/questions/78791154/what-is-dtp-in-the-context-of-contingency-tables-and-bookmaker-informedness-mark</guid>
      <pubDate>Thu, 25 Jul 2024 01:35:37 GMT</pubDate>
    </item>
    <item>
      <title>hmmlearn 中的隐马尔可夫模型不收敛</title>
      <link>https://stackoverflow.com/questions/78791079/hidden-markov-model-in-hmmlearn-not-converging</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78791079/hidden-markov-model-in-hmmlearn-not-converging</guid>
      <pubDate>Thu, 25 Jul 2024 00:56:12 GMT</pubDate>
    </item>
    <item>
      <title>如何追踪之前图像的轮廓？</title>
      <link>https://stackoverflow.com/questions/78790401/how-do-i-track-a-contour-from-my-previous-image</link>
      <description><![CDATA[我有一个细菌细胞移动的视频，我将其转换为帧。现在我想找到每个细菌细胞的瞬时速度。为此，我感兴趣的是找出细菌细胞移动了多少，但我不知道如何告诉我的程序准确识别这种特定的细菌移动了。例如，假设我只有两张图像。对于每张图像，我都有每种细菌的 COM 坐标。现在我如何关联这些数据。我如何让我的程序准确识别这种特定细菌的 COM 变化量。我已将两张图片附上以供参考。


我想到的一个方法是给每个轮廓一个唯一的 id，并将该轮廓的特征与该唯一 id 关联起来。例如它的长轴和短轴长度。这样我就可以关联轮廓的初始和最终 COM。但是这个想法假设所有细菌细胞都是独一无二的，并且我的代码可以准确而精确地识别每个细菌细胞的轮廓，但事实并非如此。如果您感兴趣，我还附上了查找每个细菌细胞轮廓的代码。有人可以提出一些更好的想法吗？非常感谢。
import cv2 as cv
import numpy as np
from numpy.typing import NDArray
import math

def gaussian_filter_multiscale_retinex(image: NDArray, sigmas: list[float], weights: list[float]) -&gt;; NDArray:
img32 = image.astype(&#39;float32&#39;) / 255

img32_log = cv.log(img32 + 1)

msr = np.zeros(image.shape, np.float32)
对于 zip(sigmas, weights) 中的 sigma、weight:

blur = cv.GaussianBlur(img32, ksize=(0, 0), sigmaX=sigma)
blur_log = cv.log(blur + 1)
ssr = cv.subtract(img32_log, blur_log)
ssr = cv.multiply(ssr, weight)

msr = cv.add(msr, ssr)

msr = cv.divide(msr, sum(weights))

msr = cv.normalize(msr, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)
返回 msr
def calculate_ellipse_area(椭圆):

(cx, cy), (a, b), 角度 = 椭圆
半长轴 = a / 2
半短轴 = b / 2
面积 = math.pi * 半长轴 * 半短轴
返回面积，角度

def process_image(img, size_threshold):
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
rtnx = gaussian_filter_multiscale_retinex(gray, sigmas=[15, 55, 185], weights=[10, 5, 1])
阈值 = cv.adaptiveThreshold(rtnx, 255,自适应方法 = cv.ADAPTIVE_THRESH_GAUSSIAN_C，
阈值类型 = cv.THRESH_BINARY，blockSize = 7，C = -7)
nb_components，输出，统计，_ = cv.connectedComponentsWithStats（阈值，连通性 = 8）
大小 = 统计 [1：，-1]
new_img = np.zeros_like（阈值）
对于 i 在范围内（0，nb_components - 1）：
如果sizes [i]＆gt; = size_threshold：
new_img [输出 == i + 1] = 255
connected_components = cv.connectedComponentsWithStats（new_img）
（numLabels，标签，统计，质心）=connected_components

result_image = np.ones_like（img）* 255
对于 i 在范围内（1， numLabels):
componentMask = (labels == i).astype(&#39;uint8&#39;)
contours, _ = cv.findContours(componentMask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)
if len(contours) &gt; 0:
cnt = contours[0]
if len(cnt) &gt;= 5:
ellipse = cv.fitEllipse(cnt)
area, angle = calculate_ellipse_area(ellipse)
if area &lt; 250:
cv.ellipse(result_image, ellipse, (0, 0, 0), 1) # 在白色背景上绘制黑色轮廓
return result_image
img1path = &quot;/Users/yahya2/Desktop/1.png&quot;
img = cv.imread(img1path)
size_threshold = 16
result_image = process_image(img, size_threshold)

cv.imshow(&#39;轮廓&#39;, result_image)
cv.waitKey(0)
cv.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/78790401/how-do-i-track-a-contour-from-my-previous-image</guid>
      <pubDate>Wed, 24 Jul 2024 20:14:25 GMT</pubDate>
    </item>
    <item>
      <title>Keras CNN 模型对同一输入给出不同的输出预测</title>
      <link>https://stackoverflow.com/questions/78789559/keras-cnn-model-gives-different-output-predictions-for-the-same-input</link>
      <description><![CDATA[嗨，我用 keras 训练了一个 CNN 模型，类似于他们在网站上使用的示例模型，但层数略小，最后还有一个额外的 dropout 层。模型构建函数看起来有点像这样：
&quot; --------- 模型参数 ---------&quot;

epochs = 800
image_size = (384, 256)
batch_size = 128
number_of_layers = 4
drop_out = 0.25
num_dropouts = 2
learn_rate = 0.00001
layer_sizes = [64, 128, 256, 512, 728]
class_weight = {0:1, 1:3}
image_rotation = 0.1

def make_model(input_shape, num_classes, layer_num=3, drop_out=0.25, dropouts=1):
inputs = keras.Input(shape=input_shape)

# 输入块
x = layer.Rescaling(1.0 / 255)(inputs)
x = layer.Conv2D(128, 3, strides=2, padding=&quot;same&quot;)(x)
x = layer.BatchNormalization()(x)
x = layer.Activation(&quot;relu&quot;)(x)

previous_block_activation = x # 留出残差

layer_use = []
for i in range(layer_num): layer_use.append(layer_sizes[i])

for size in layer_use:
x = layer.Activation(&quot;relu&quot;)(x)
x = layer.SeparableConv2D(size, 3, padding=&quot;same&quot;)(x)
x = layer.BatchNormalization()(x)

x = layer.Activation(&quot;relu&quot;)(x)
x = layer.SeparableConv2D(size, 3, padding=&quot;same&quot;)(x)
x = layer.BatchNormalization()(x)

x = layer.MaxPooling2D(3, strides=2, padding=&quot;same&quot;)(x)

# 投影残差
residual = layer.Conv2D(size, 1, strides=2, padding=&quot;same&quot;)(
previous_block_activation
)
x = layer.add([x, residual]) # 添加回残差
previous_block_activation = x # 留出下一个残差

x = layer.SeparableConv2D(1024, 3, padding=&quot;same&quot;)(x)
x = layer.BatchNormalization()(x)
x = layer.Activation(&quot;relu&quot;)(x)

x = layer.GlobalAveragePooling2D()(x)
if num_classes == 2:
unit = 1
else:
unit = num_classes

if dropouts == 1:
x = layer.Dropout(drop_out)(x)
# 我们指定activation=None以便返回logits
outputs = layer.Dense(units,activation=None)(x)
elif dropouts == 2:
x = layer.Dropout(0.2)(x)
x = layer.Dropout(units,activation=None)(x)
outputs = layer.Dropout(drop_out)(x)

返回keras.Model(inputs,outputs)

model = make_model(input_shape=image_size + (3,),num_classes=2,layer_num=number_of_layers,
drop_out=drop_out,dropouts=num_dropouts)

model.compile(
optimizer=keras.optimizers.Adam(learning_rate=learn_rate),
loss=keras.losses.BinaryCrossentropy(from_logits=True),
metrics=[keras.metrics.BinaryAccuracy(name=&quot;acc&quot;)],
)

model.fit(
train_ds,
epochs=epochs,
callbacks=callbacks,
validation_data=val_ds,
class_weight=class_weight
)

模型训练良好 - 达到约 97% 的验证准确率。使用预测函数时，模型通常会根据我提供的输入数据给出合理的输出。我遇到的问题是，使用 predict_on_batch 函数时的输出预测并不相同，如果使用相同的输入数据重复，通常会相差 +-0.15。这是什么原因造成的，一旦模型经过训练并用于预测，对于相同的输入数据，预测难道不应该相同吗？]]></description>
      <guid>https://stackoverflow.com/questions/78789559/keras-cnn-model-gives-different-output-predictions-for-the-same-input</guid>
      <pubDate>Wed, 24 Jul 2024 16:36:55 GMT</pubDate>
    </item>
    <item>
      <title>如何在窗口上聚合 RESNET50 全局特征？</title>
      <link>https://stackoverflow.com/questions/78788243/how-to-aggregate-resnet50-global-features-over-a-window</link>
      <description><![CDATA[我必须通过时间序列凝视数据集进行人员识别。我有正在观看视频的人的凝视信息，并且对于每个时间戳，我都会从视频中获取裁剪的帧以了解人员正在看哪里，然后将其提供给 RESNET-50 预训练模型以提取其特征。从模型中删除最后一个分类层，以便我获得每个图像的 2048 维向量。
最后，我有一个带有时间戳的时间序列数据，对于每一行，凝视信息 + 2048 个来自 RESNET 模型的特征。我的目标是使用凝视特征进行人员识别，并在这些凝视特征之上，输出 RESNET 特征。
现在，我想进行特征聚合，可能在窗口大小上进行。我一直在研究像 Fischer 向量这样的局部特征聚合方法，但我拥有的是图像的全局特征。
TL;DR，有没有一种好的方法可以在窗口大小上聚合 RESNET-50 特征？
我尝试过使用常见的聚合方法，如均值和方差、最大最小值等，但不确定聚合结果是否对应有意义的信息。]]></description>
      <guid>https://stackoverflow.com/questions/78788243/how-to-aggregate-resnet50-global-features-over-a-window</guid>
      <pubDate>Wed, 24 Jul 2024 12:07:58 GMT</pubDate>
    </item>
    <item>
      <title>XGBoostError：参数详细程度的值 -1 超出界限 [0,3]</title>
      <link>https://stackoverflow.com/questions/78761783/xgboosterror-value-1-for-parameter-verbosity-exceed-bound-0-3</link>
      <description><![CDATA[错误消息如标题所示。根据下面的代码，这对我来说毫无意义：
clf = xgboost.XGBClassifier(verbosity=1)
print (clf.__class__, clf.verbosity) 
# prints &lt;class &#39;xgboost.sklearn.XGBClassifier&#39;&gt; 1
clf.fit(X=train_data_iter[features].fillna(0), y=train_data_iter[&#39;y&#39;]) # 错误在这里出现

值显然是 1，但不知何故却变成了 -1？我不明白。]]></description>
      <guid>https://stackoverflow.com/questions/78761783/xgboosterror-value-1-for-parameter-verbosity-exceed-bound-0-3</guid>
      <pubDate>Wed, 17 Jul 2024 22:12:23 GMT</pubDate>
    </item>
    <item>
      <title>由于 keras 中的 SSL 错误，无法加载 MNIST 数据集...load_data() 函数</title>
      <link>https://stackoverflow.com/questions/78668638/unable-to-load-mnist-data-set-due-to-ssl-error-in-keras-load-data-function</link>
      <description><![CDATA[import keras
(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()
...

我目前开始使用机器学习，但由于错误，我无法加载 MNIST 数据集：

异常：https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 上的 URL 获取失败：无 -- [SSL：CERTIFICATE_VERIFY_FAILED] 证书验证失败：无法获取本地颁发者证书(_ssl.c:1000)

我确保一切都是最新的（macOS 版本 14.5、pip 版本 24.1、最新的 tensorflow 和 keras 库以及最新的 safari 和 vscode 版本）。我尝试重新安装 keras 几次，确保不要从 pip3 缓存中下载最新的 pip 24.1 版本。这个错误一直存在，我还没有找到解决办法。我唯一弄清楚的是，这是由 ...load_data() 函数导致的错误。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78668638/unable-to-load-mnist-data-set-due-to-ssl-error-in-keras-load-data-function</guid>
      <pubDate>Tue, 25 Jun 2024 16:48:10 GMT</pubDate>
    </item>
    <item>
      <title>没有名为“tensorflow.keras.layers.experimental.preprocessing”的模块</title>
      <link>https://stackoverflow.com/questions/63542803/no-module-named-tensorflow-keras-layers-experimental-preprocessing</link>
      <description><![CDATA[代码下方
import numpy as np
np.random.seed(0)
from sklearn import datasets
import matplotlib.pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format =&#39;retina&#39;

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD

错误消息下方
-------------------------------------------------------------------------------
ModuleNotFoundError Traceback (most recent call last)
~\Anaconda3\lib\site-packages\keras\__init__.py in &lt;module&gt;
2 try:
----&gt; 3 from tensorflow.keras.layers.experimental.preprocessing import RandomRotation
4 except ImportError:

ModuleNotFoundError: 没有名为“tensorflow.keras.layers.experimental.preprocessing”的模块

在处理上述异常期间，发生了另一个异常：

ImportError Traceback（最近一次调用最后一次）
&lt;ipython-input-5-943507dd87a6&gt; in &lt;module&gt;
6 get_ipython().run_line_magic(&#39;config&#39;, &quot;InlineBackend.figure_format =&#39;retina&#39;&quot;)
7 
----&gt; 8 从 keras.models 导入 Sequential
9 从 keras.layers 导入 Dense
10 从 keras.optimizers 导入 SGD

~\Anaconda3\lib\site-packages\keras\__init__.py in &lt;module&gt;
4 except ImportError:
5 raise ImportError(
----&gt; 6 &#39;Keras 需要 TensorFlow 2.2 或更高版本。&#39;
7 &#39;通过 `pip install tensorflow`&#39; 安装 TensorFlow)
8 

ImportError: Keras 需要 TensorFlow 2.2 或更高版本。通过 `pip install tensorflow` 安装 TensorFlow

注意：`我认为，主要问题是 Tensorflow 版本。我使用了一些命令，如下所示，
conda create -n tf tensorflow
conda activate tf

我还使用了以下命令
conda create -n tf-gpu tensorflow-gpu
conda activate tf-gpu

但是它不起作用，请帮助解决错误。]]></description>
      <guid>https://stackoverflow.com/questions/63542803/no-module-named-tensorflow-keras-layers-experimental-preprocessing</guid>
      <pubDate>Sun, 23 Aug 2020 02:18:29 GMT</pubDate>
    </item>
    <item>
      <title>如何计算伯努利朴素贝叶斯的联合对数似然</title>
      <link>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</link>
      <description><![CDATA[对于使用 BernoulliNB 的分类问题，如何计算联合对数似然。联合似然由以下公式计算，其中 y(d) 是实际输出（不是预测值）的数组，x(d) 是特征的数据集。
我阅读了这个答案并阅读了文档，但它并没有完全满足我的目的。有人可以帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</guid>
      <pubDate>Wed, 17 Oct 2018 18:08:50 GMT</pubDate>
    </item>
    <item>
      <title>在 GridSearchCV 中为 XGBoost 评分</title>
      <link>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</link>
      <description><![CDATA[我目前正在尝试首次使用 XGBoost 分析数据。我想使用 GridsearchCV 找到最佳参数。我想最小化均方根误差，为此，我使用“rmse”作为 eval_metric。但是，网格搜索中的评分没有这样的指标。我在这个网站上发现“neg_mean_squared_error”也有同样的效果，但我发现这给出的结果与 RMSE 不同。当我计算“neg_mean_squared_error”绝对值的根时，我得到的值约为 8.9，而另一个函数给出的 RMSE 约为 4.4。
我不知道哪里出了问题，或者我如何让这两个函数一致/给出相同的值？
由于这个问题，我得到了错误的值作为“best_params_”，这给了我比我最初开始调整的一些值更高的 RMSE。
有人能解释一下如何在网格搜索中获得 RMSE 的分数，或者为什么我的代码给出了不同的值吗？ 
提前致谢。
def modelfit(alg, trainx, trainy, useTrainCV=True, cv_folds=10, early_stopping_rounds=50):
if useTrainCV:
xgb_param = alg.get_xgb_params()
xgtrain = xgb.DMatrix(trainx, label=trainy)
cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()[&#39;n_estimators&#39;], nfold=cv_folds,
metrics=&#39;rmse&#39;, early_stopping_rounds=early_stopping_rounds)
alg.set_params(n_estimators=cvresult.shape[0])

# 将算法拟合到数据上
alg.fit(trainx, trainy, eval_metric=&#39;rmse&#39;)

# 预测训练集：
dtrain_predictions = alg.predict(trainx)
# dtrain_predprob = alg.predict_proba(trainy)[:, 1]
print(dtrain_predictions)
print(np.sqrt(mean_squared_error(trainy, dtrain_predictions)))

# 打印模型报告：
print(&quot;\nModel Report&quot;)
print(&quot;RMSE : %.4g&quot; % np.sqrt(metrics.mean_squared_error(trainy, dtrain_predictions)))

param_test2 = {
&#39;max_depth&#39;:[6,7,8],
&#39;min_child_weight&#39;:[2,3,4]
}

grid2 = GridSearchCV(estimator = xgb.XGBRegressor( learning_rate =0.1, n_estimators=2000, max_depth=5,
min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,
objective= &#39;reg:linear&#39;, nthread=4, scale_pos_weight=1, random_state=4),
param_grid = param_test2,scoring=&#39;neg_mean_squared_error&#39;, n_jobs=4,iid=False, cv=10, verbose=20)
grid2.fit(X_train,y_train)
# best_estimator 的平均交叉验证分数
print(grid2.best_params_, np.sqrt(np.abs(grid2.best_score_))), print(np.sqrt(np.abs(grid2.score(X_train, y_train))))
modelfit(grid2.best_estimator_, X_train, y_train)
print(np.sqrt(np.abs(grid2.score(X_train, y_train))))
]]></description>
      <guid>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</guid>
      <pubDate>Fri, 11 May 2018 16:46:16 GMT</pubDate>
    </item>
    <item>
      <title>神经网络收敛至零输出</title>
      <link>https://stackoverflow.com/questions/44213659/neural-network-converging-to-zero-output</link>
      <description><![CDATA[我正在尝试训练这个神经网络来对一些数据进行预测。
我在一个小的数据集（大约 100 条记录）上尝试了它，它工作得很好。然后我插入了新的数据集，我发现 NN 收敛到 0 输出，误差大约收敛到正例数与总例数之比。
我的数据集由是/否特征（1.0/0.0）组成，基本事实也是是/否。
我的假设：

1) 存在输出为 0 的局部最小值（但我尝试了许多学习率和初始权重值，它似乎总是收敛到那里）

2) 我的权重更新是错误的（但在我看来很好）

3) 它只是一个输出缩放问题。我尝试缩放输出（即输出/最大值（输出）和输出/平均值（输出）），但结果并不好，如您在下面提供的代码中看到的那样。我应该以不同的方式缩放它吗？Softmax？ 
代码如下：
import pandas as pd
import numpy as np
import pickle
import random
from collections import defaultdict

alpha = 0.1
N_LAYERS = 10
N_ITER = 10
#N_FEATURES = 8
INIT_SCALE = 1.0

train = pd.read_csv(&quot;./data/prediction.csv&quot;)

y = train[&#39;y_true&#39;].as_matrix()
y = np.vstack(y).astype(float)
ytest = y[18000:]
y = y[:18000]

X = train.drop([&#39;y_true&#39;], axis = 1).as_matrix()
Xtest = X[18000:].astype(float)
X = X[:18000]

def tanh(x,deriv=False):
if(deriv==True):
return (1 - np.tanh(x)**2) * alpha
else:
return np.tanh(x)

def sigmoid(x,deriv=False):
if(deriv==True):
return x*(1-x)
else:
return 1/(1+np.exp(-x))

def relu(x,deriv=False):
if(deriv==True):
return 0.01 + 0.99*(x&gt;0)
else:
return 0.01*x + 0.99*x*(x&gt;0)

np.random.seed()

syn = defaultdict(np.array)

for i in range(N_LAYERS-1):
syn[i] = INIT_SCALE * np.random.random((len(X[0]),len(X[0]))) - INIT_SCALE/2
syn[N_LAYERS-1] = INIT_SCALE * np.random.random((len(X[0]),1)) - INIT_SCALE/2

l = defaultdict(np.array)

delta = defaultdict(np.array)

for j in xrange(N_ITER):
l[0] = X
for i in range(1,N_LAYERS+1):
l[i] = relu(np.dot(l[i-1],syn[i-1]))

error = (y - l[N_LAYERS])

e = np.mean(np.abs(error))
if (j% 1) == 0:
print &quot;\nIteration &quot; + str(j) + &quot; of &quot; + str(N_ITER)
print &quot;Error: &quot; + str(e)

delta[N_LAYERS] = error*relu(l[N_LAYERS],deriv=True) * alpha
for i in range(N_LAYERS-1,0,-1):
error = delta[i+1].dot(syn[i].T)
delta[i] = error*relu(l[i],deriv=True) * alpha

for i in range(N_LAYERS):
syn[i] += l[i].T.dot(delta[i+1])

pickle.dump(syn, open(&#39;neural_weights.pkl&#39;, &#39;wb&#39;))

# 使用 f1-measure 进行测试
# 召回率 = 真阳性 / (真阳性 + 假阴性)
# 准确率 = 真阳性 / (真阳性阳性 + 假阳性)

l[0] = Xtest
for i in range(1,N_LAYERS+1):
l[i] = relu(np.dot(l[i-1],syn[i-1]))

out = l[N_LAYERS]/max(l[N_LAYERS])

tp = float(0)
fp = float(0)
fn = float(0)
tn = float(0)

for i in l[N_LAYERS][:50]:
print i

for i in range(len(ytest)):
if out[i] &gt; 0.5 and ytest[i] == 1:
tp += 1
if out[i] &lt;= 0.5 and ytest[i] == 1:
fn += 1
if out[i] &gt; 0.5 且 ytest[i] == 0:
fp += 1
if out[i] &lt;= 0.5 且 ytest[i] == 0:
tn += 1

print &quot;tp: &quot; + str(tp)
print &quot;fp: &quot; + str(fp)
print &quot;tn: &quot; + str(tn)
print &quot;fn: &quot; + str(fn)

print &quot;\nprecision: &quot; + str(tp/(tp + fp))
print &quot;recall: &quot; + str(tp/(tp + fn))

f1 = 2 * tp /(2 * tp + fn + fp)
print &quot;\nf1-measure:&quot; + str(f1)

输出结果如下：
第 0 次迭代（共 10 次）
错误： 0.222500767998

10 次迭代中的第 1 次
错误：0.222500771157

10 次迭代中的第 2 次
错误：0.222500774321

10 次迭代中的第 3 次
错误：0.22250077749

10 次迭代中的第 4 次
错误：0.222500780663

10 次迭代中的第 5 次
错误：0.222500783841

10 次迭代中的第 6 次
错误：0.222500787024

10 次迭代中的第 7 次
错误：0.222500790212

10 次迭代中的第 8 次
错误：0.222500793405

10 次迭代中的第 9 次10
错误：0.222500796602

[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 4.62182626e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 4.62182626e-06]
[ 0.]
[ 0.]
[ 5.04501079e-10]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 5.04501079e-10]
[ 0.]
[ 0.]
[ 4.62182626e-06]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 1.31432294e-05]

tp：28.0
fp：119.0
tn： 5537.0
fn：1550.0

精度：0.190476190476
召回率：0.0177439797212

f1-measure：0.0324637681159
]]></description>
      <guid>https://stackoverflow.com/questions/44213659/neural-network-converging-to-zero-output</guid>
      <pubDate>Sat, 27 May 2017 06:21:36 GMT</pubDate>
    </item>
    </channel>
</rss>