<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 13 Feb 2025 06:24:10 GMT</lastBuildDate>
    <item>
      <title>ValueError：X 有 7 个特征，但 ColumnTransformer 需要 13 个特征</title>
      <link>https://stackoverflow.com/questions/79434756/valueerror-x-has-7-features-but-columntransformer-expects-13-features</link>
      <description><![CDATA[我有以下代码，我尝试使用泊松回归预测工具的价格。
# --- 加载和准备数据 ---
y = train[&#39;PriceToday&#39;]
X = train.drop(columns=[&#39;PriceToday&#39;])

# 定义非标准类型
non_standard_types = [&quot;Nar&quot;, &quot;Orch&quot;, &quot;Fru&quot;,&quot;Comp&quot;]

# 为非标准创建标志特征
X[&quot;Non_Standard_Flag&quot;] = X[&quot;Type_LS&quot;].isin(non_standard_types).astype(int)

# 识别数字和分类列
num_features = [&quot;AGE&quot;, &quot;POWER&quot;, &quot;Hours&quot;, &quot;Non_Standard_Flag&quot;]
cat_features = [&quot;BRAND&quot;, &quot;Country&quot;, &quot;Final_Trans&quot;]

# 定义预处理管道
preprocessor = ColumnTransformer(
transformers=[
(&#39;num&#39;, StandardScaler(), num_features),
(&#39;cat&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;), cat_features)
], remainder=&quot;drop&quot;
)

# --- 训练/测试拆分 ---
# 创建权重列
train[&quot;sample_weight&quot;] = train[&quot;Type_LS&quot;].apply(lambda x: 1 if x == &quot;Standard&quot; else 5)

train[&quot;stratify_group&quot;] = train[&quot;BRAND&quot;].astype(str)
X_train, X_val, y_train, y_val, train_weights, val_weights = train_test_split(
X, y, train[&quot;sample_weight&quot;], test_size=0.2, random_state=42, stratify=train[&quot;stratify_group&quot;]
)
# 在训练数据上对预处理器进行一次拟合
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_val_preprocessed = preprocessor.transform(X_val)

# 定义模型
models = {
&quot;Poisson&quot;: PoissonRegressor(alpha=0.01)
}

# 训练和评估模型
model_results = {}

for model_name, model in models.items():
model.fit(X_train_preprocessed, y_train, sample_weight=train_weights)

# 预测
predictions = model.predict(X_val_preprocessed)
# 计算指标
r2 = r2_score(y_val, predictions)

model_results[model_name] = {
&quot;model&quot;: model,
&quot;R2&quot;: r2
}


我有一个测试数据，我想将其价格与模型预测的价格进行比较。
我的测试数据是这样的：
# 确保新数据具有正确的格式
new_data = pd.DataFrame({
&quot;AGE&quot;: [12, 24, 36, 48, 60, 72, 84, 12, 24, 36, 48, 60, 72, 84],
&quot;Hours&quot;: [500, 1000, 1500, 2000, 2500, 3000, 3500, 500, 1000, 1500, 2000, 2500, 3000, 3500],
&quot;BRAND&quot;: [&quot;NH&quot;] * 14,
&quot;POWER&quot;: [150] * 7 + [80] * 7,
&quot;Final_Trans&quot;: [&quot;Cv&quot;] * 14,
&quot;Country&quot;: [&quot;DEU&quot;] * 14,
&quot;Type_LS&quot;: [Nar, Nar, Nar, ST, ST, ST, ST, ST, ST, ST, ST, ST, ST, ST, ST] 
&quot;Current_Pred&quot;: [105614, 96681, 88504, 81018, 74165, 67892, 62150, 42608, 39728, 37043, 34540, 32206, 30029, 28000]
})
``


我的代码是：
new_df = pd.DataFrame(new_data)
# 创建 &#39;Non_Standard_Flag&#39;
new_df[&quot;Non_Standard_Flag&quot;] = new_df[&quot;Type_LS&quot;].isin(non_standard_types).astype(int)

# 选择预处理器所需的列
X_new = new_df[[&#39;AGE&#39;, &#39;POWER&#39;, &#39;Hours&#39;, &#39;Non_Standard_Flag&#39;, &#39;BRAND&#39;, &#39;Country&#39;, &#39;Final_Trans&#39;]]

X_new_preprocessed = preprocessor.transform(X_new) 

# 从训练数据中获取独热编码后的列名
ohe = preprocessor.named_transformers_[&#39;cat&#39;]
encoded_cat_columns = ohe.get_feature_names_out(cat_features)

#为数字特征创建列名
num_columns = num_features

# 合并列名
all_columns = num_columns + list(encoded_cat_columns)

# 从预处理数据创建 DataFrame
X_new_preprocessed_df = pd.DataFrame(X_new_preprocessed, columns=all_columns)

# --- 使用泊松模型进行预测 ---
poisson_model = model_results[&quot;Poisson&quot;][&quot;model&quot;] # 访问经过训练的泊松模型
predicted_prices = poisson_model.predict(X_new_preprocessed_df)

# 比较并存储结果 ---
new_df[&#39;Predicted_Price&#39;] = predict_prices

# 计算预测价格与当前价格之间的差额
new_df[&#39;Price_Difference&#39;] = new_df[&#39;Predicted_Price&#39;] - new_df[&#39;Current_Pred&#39;]


但执行此操作后，我收到错误：X 有 7 个特征，但 ColumnTransformer 需要 13 个特征 我有相同数量的列，所以我不明白为什么会出现此错误。任何帮助都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/79434756/valueerror-x-has-7-features-but-columntransformer-expects-13-features</guid>
      <pubDate>Wed, 12 Feb 2025 23:51:03 GMT</pubDate>
    </item>
    <item>
      <title>如何使用嵌入和余弦相似度改进 Excel 文件中的列标题匹配？[关闭]</title>
      <link>https://stackoverflow.com/questions/79433790/how-to-improve-column-header-matching-in-excel-files-using-embeddings-and-cosine</link>
      <description><![CDATA[我正在构建一个处理用户上传的 Excel 文件的工具。这些文件可以有各种列标题，我的目标是将这些标题映射到一组预定义的输出列。例如：
输出列是固定的：名字、姓氏、年龄、性别、城市、地址等。
输入的 Excel 标题可以有所不同。例如，输出中的名字可能在输入文件中表示为员工名字、F_Name 或名字。
如果工具找不到列的匹配项（例如，不存在与名字等同的项），则输出列应填充为 null。
尝试的方法
我使用了基于嵌入的方法：

我使用模型（例如，来自 OpenAI 或其他 NLP 模型的 text-embedding-ada-002）为输入列标题生成嵌入。

我计算这些嵌入与预定义输出列名称的嵌入之间的余弦相似度。

我根据相似度分数确定匹配项。


面临的问题
虽然这适用于某种程度上，余弦相似度得分往往不可靠：

对于名字（输出列）：

与员工名字的相似度 = 0.90（预期）。
与从属名字的相似度 = 0.92（意外且不正确）。

对于名字和不相关的列：

与年龄的相似度 = 0.70，对于不相关的术语来说太高了。
这个问题使得很难区分相关和不相关的匹配。例如：
年龄和名字不应被视为相似，但相似度仍然很高。
员工名字和受抚养人名字应具有不同的分数，以有利于正确匹配。
要求
我需要一个确保准确映射列的解决方案，考虑到以下几点：

相似的列名（例如，名字和员工名字）应具有较高的相似度分数。

不相关的列名（例如，名字和年龄）应具有较低的相似度分数。

解决方案应处理列名的变化，例如同义词（性别 ↔ 性别）或缩写（出生日期 ↔ 出生日期）。


问题

为什么余弦相似度不相关的列对（例如，名字 ↔ 年龄）的得分如此之高？

在这种情况下，我如何提高列匹配的准确性？


尝试的潜在解决方案

手动创建常见变体的映射字典，但这不可扩展。

尝试使用余弦相似度的阈值，但仍然不一致。


我正在寻找什么

替代方法（例如，微调嵌入模型或使用特定于领域的模型）。

专门为匹配列名而设计的任何预训练模型或库。

将基于规则的方法与嵌入来提高准确性。

]]></description>
      <guid>https://stackoverflow.com/questions/79433790/how-to-improve-column-header-matching-in-excel-files-using-embeddings-and-cosine</guid>
      <pubDate>Wed, 12 Feb 2025 16:28:05 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 dart 提取 MFCC 特征[关闭]</title>
      <link>https://stackoverflow.com/questions/79433652/how-to-extract-mfcc-features-using-dart</link>
      <description><![CDATA[我正在开发一个 Flutter 应用程序，尝试使用机器学习模型来识别谁是说话者，该应用程序将离线运行，输入是 5 秒记录的 MFCC 特征数组，输出是说话者的姓名，数组的大小是 13，有没有办法使用 dart 提取这些特征。]]></description>
      <guid>https://stackoverflow.com/questions/79433652/how-to-extract-mfcc-features-using-dart</guid>
      <pubDate>Wed, 12 Feb 2025 15:40:32 GMT</pubDate>
    </item>
    <item>
      <title>lightgbm 强制变量拆分</title>
      <link>https://stackoverflow.com/questions/79433458/lightgbm-force-variables-to-be-in-splits</link>
      <description><![CDATA[我正在尝试找到一种方法来训练 lightgbm 模型，强制将某些特征放在分割中，即：“放在特征重要性中”，然后预测会受到这些变量的影响。
这是一个建模代码的示例，其中有一个无用的变量，因为它是常量，但其想法是从业务角度来看可能有一个重要的变量不在特征中
from lightgbm import LGBMRegressor
import pandas as pd
import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 生成随机回归数据集
X, y = make_regression(n_samples=1000, n_features=10, noise=0.9, random_state=42)
feature_names = [f&quot;feature_{i}&quot;对于范围内的 i(X.shape[1])]

# 将 DataFrame 转换为可读取的数据
X = pd.DataFrame(X, columns=feature_names)

# Agregar características inútiles
X[“useless_feature_1”] = 1

# 区分数据和相关内容
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 定义 LGBMRegressor 模型
模型 = LGBMRegressor(
    目标=“回归”，
    度量=“rmse”，
    随机状态=1，
    n_估计器=100
）

# 恩特雷纳埃尔modelo
model.fit(X_train, y_train, eval_set=[(X_test, y_test)])

# 预测和评估
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f&quot;Test RMSE: {rmse:.4f}&quot;)

# 特征重要性
importance = pd.DataFrame({
&quot;feature&quot;: X.columns,
&quot;importance&quot;: model.feature_importances_
}).sort_values(by=&quot;importance&quot;, accending=False)

print(&quot;\nFeature Importance:&quot;)
print(importance)

预期解决方案：应该有一些解决方法，但最有趣的是在拟合或回归方法中使用某些参数的方法。]]></description>
      <guid>https://stackoverflow.com/questions/79433458/lightgbm-force-variables-to-be-in-splits</guid>
      <pubDate>Wed, 12 Feb 2025 14:36:26 GMT</pubDate>
    </item>
    <item>
      <title>来自视频的实时对象跟踪模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79433270/real-time-object-tracking-model-from-videos</link>
      <description><![CDATA[我需要开发一个可以实时准确跟踪物体的机器学习模型。以下是我的想法：

数据收集：我计划在网上寻找大约 10 个视频，这些视频清楚地显示了物体从一个地方移动到另一个地方。
目标：使用这些视频，我的目标是构建和训练一个模型，可以实时跟踪这些物体并准确确定它们的位置

我正在寻找以下方面的建议：

如何最好地选择和预处理此类视频进行训练。
您发现哪些算法或框架对实时物体跟踪有效？
关于处理遮挡或物体速度变化等潜在挑战的任何提示。
]]></description>
      <guid>https://stackoverflow.com/questions/79433270/real-time-object-tracking-model-from-videos</guid>
      <pubDate>Wed, 12 Feb 2025 13:38:28 GMT</pubDate>
    </item>
    <item>
      <title>Chatgpt 与 AI 代理的角色文件进行聊天 [关闭]</title>
      <link>https://stackoverflow.com/questions/79432203/chatgpt-chats-to-character-file-for-ai-agents</link>
      <description><![CDATA[我正在使用 Eliza 框架构建 AI 代理。有没有办法让我利用我的 chatgpt 聊天来创建角色文件或相应地训练代理。]]></description>
      <guid>https://stackoverflow.com/questions/79432203/chatgpt-chats-to-character-file-for-ai-agents</guid>
      <pubDate>Wed, 12 Feb 2025 06:57:36 GMT</pubDate>
    </item>
    <item>
      <title>在 Colab 中对大型数据集使用 train_test_split 时 RAM 崩溃</title>
      <link>https://stackoverflow.com/questions/79430737/ram-crash-when-using-train-test-split-on-large-dataset-in-colab</link>
      <description><![CDATA[我正在开展一个情绪分析项目，该项目有一个包含 160,000 行嵌入评论的大型数据集。
当我使用 sklearn.model_selection 中的 train_test_split() 时，Google Colab 中的 RAM 会被完全占用，并且会话会崩溃。
如何减少 RAM 使用量以防止崩溃？
是否有任何替代或优化的方法来拆分如此大的数据集？]]></description>
      <guid>https://stackoverflow.com/questions/79430737/ram-crash-when-using-train-test-split-on-large-dataset-in-colab</guid>
      <pubDate>Tue, 11 Feb 2025 16:47:15 GMT</pubDate>
    </item>
    <item>
      <title>如何从 PDF 中提取表格并将其转换为结构化 HTML (<table>、<tr>、<td>)，同时保持原始布局和格式？</title>
      <link>https://stackoverflow.com/questions/79430117/how-to-extract-tables-from-a-pdf-and-convert-them-into-structured-html-table</link>
      <description><![CDATA[1]Doc1 的原始页面包含 4 个表格
1]Doc1 的输出 .html 页面无法正确检测表格，有时会将表格中的文本提取为纯文本
2]Doc2 的原始页面在表格单元格中包含图像
2]Doc2 的输出 .html 页面，无法正确处理嵌入图像的复杂表格&amp; 有时，对于简单的表格，.html 文件中的结构也不正确
我正在从 PDF 文件中提取内容并将其转换为 HTML 格式，同时保持原始结构和格式。我为此使用了 Docling 库。
我在 .html 文件中获得了与原始 PDF 文件内容流相同的输出。但是，我在输出 HTML 文件中保留表格结构时遇到了问题。
我期望发生的情况：

从 PDF 中提取具有正确行和列结构的表格。

保留 &lt;table&gt;、&lt;tr&gt; 和 &lt;td&gt; HTML 标记中的表格布局。

保持 PDF 中显示的原始格式、对齐方式和单元格内容。


实际发生的情况：

表格检测不正确 - 表格数据显示在 &lt;p&gt; 标记内，而不是正确的 &lt;table&gt;结构。

表格未对齐 - 单元格内容拆分不正确，并且表格单元格内的图像出现在表格外部。

嵌入图像的复杂表格无法正确保存。


使用的代码：
来自 docling.document_converter 导入 DocumentConverter、PdfFormatOption
来自 docling.datamodel.pipeline_options 导入 PdfPipelineOptions
来自 docling.datamodel.base_models 导入 InputFormat
来自 docling_core.types.doc 导入 ImageRefMode
来自 pathlib 导入 Path
导入日志记录

# 设置日志记录
logging.basicConfig(level=logging.INFO)
log = logs.getLogger(__name__) # 已更正：_name_ ——&gt; __name__

# 配置图像设置
IMAGE_RESOLUTION_SCALE = 2.0

# PDF 文件的路径
source = Path(r&quot;C:\Users\Downloads\Journal.pdf&quot;)
output_path = Path(r&quot;C:\Users\Desktop\output20.html&quot;)

# 配置用于图像处理的管道选项
pipeline_options = PdfPipelineOptions()
pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE
pipeline_options.generate_page_images = True
pipeline_options.generate_picture_images = True

# 使用图像选项创建转换器实例
converter = DocumentConverter(
format_options={
InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
}
)

# 将 PDF 转换为文档
result = converter.convert(source)

# 保存嵌入图像的 HTML
result.document.save_as_html(output_path, image_mode=ImageRefMode.EMBEDDED)

log.info(f&quot;嵌入图像的 HTML 文件创建于：{output_path}&quot;)

我目前尝试过的方法：

检查提取的 HTML 输出 — 表格缺失或显示不正确。

尝试了 PdfPipelineOptions() 中的不同管道选项，看看它们是否影响表格提取。

将输出与文档智能库进行比较 — 它可以更好地提取页眉/页脚，但仍难以处理复杂的表格。


关键挑战：

表格未保留在 &lt;table&gt; 标签中。

单元格拆分问题 - 数据未对齐或拆分成多个部分。

表格内的图像位置错误（出现在上方/下方，而不是表格单元格内）。


其他观察：

提取的 HTML 文件中的内容流与原始 PDF 文件**匹配，但**表格结构格式不正确。

问题：
如何正确从 PDF 中提取表格并将其转换为结构化 HTML（&lt;table&gt;、&lt;tr&gt;、 &lt;td&gt;) 同时使用 Docling 库保持原有的布局和格式？]]></description>
      <guid>https://stackoverflow.com/questions/79430117/how-to-extract-tables-from-a-pdf-and-convert-them-into-structured-html-table</guid>
      <pubDate>Tue, 11 Feb 2025 13:21:50 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost bst.predict() 输出与二进制：逻辑案例的（文本）树模型的手动计算不匹配</title>
      <link>https://stackoverflow.com/questions/79430043/xgboost-bst-predict-output-not-matching-with-manual-calculation-from-the-text</link>
      <description><![CDATA[我试图验证 XGBoost 输出 (booster.predict) 的逻辑回归，以了解我对通过构建的树进行输出计算的理解。我发现所有结果的差异约为 -1.58 倍。下面分享我用来验证的代码。我肯定遗漏了一些东西，所以请求帮助我理解它是什么。
import xgboost as xgb
import pandas as pd
import numpy as np
import math
import random
np.random.seed(1)

data = pd.DataFrame(np.arange(100*4).reshape((100,4)), columns=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;])
label = pd.DataFrame(np.random.randint(2, size=(100,1)))
data = pd.concat([data,label], ignore_index=True, axis =1)
data = pd.DataFrame(np.arange(100*4).reshape((100,4)), columns=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;])
features = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]

dtrain = xgb.DMatrix(data, label=label)
param = {&quot;max_depth&quot;:2, &quot;base_score&quot;:0.2, &#39;objective&#39;: &#39;binary:logistic&#39;}
clf1 = xgb.train(param, dtrain, 2)
clf1.dump_model(&quot;base_score1.txt&quot;)

e = math.exp(-(-0.143835619-0.123642519+0.2))
print(clf1.predict(dtrain)[0],1/(1+e))
## 0.39109966 0.7583403831446165
## 理想情况下，e 的值应该是1.5568930331924702，而这里 e 是 0.31866905973448423

这是生成的树
booster[0]:
0:[a&lt;126] yes=1,no=2,missing=1
1:[a&lt;58] yes=3,no=4,missing=3
3:leaf=0.617647052
4:leaf=0.0483870991
2:leaf=0.691919208
booster[1]:
0:leaf=0.325955093

所以我的理解是 bst.predict() 输出应用于总和的 sigmoid tree_values 和 base_score，即 1/(1+math.exp(-sum))，其中 sum = base_score+sum_of_tree_values（即有多少棵树）。
我做错了什么？
这可能相关，但不确定使用 &quot;binary:logistic&quot; 时 XGBoost 中单个树的权重计算]]></description>
      <guid>https://stackoverflow.com/questions/79430043/xgboost-bst-predict-output-not-matching-with-manual-calculation-from-the-text</guid>
      <pubDate>Tue, 11 Feb 2025 12:55:07 GMT</pubDate>
    </item>
    <item>
      <title>为什么我必须将数据从 torch.Size([50]) 解压到 torch.Size([50, 1])</title>
      <link>https://stackoverflow.com/questions/79409149/why-do-i-have-to-unsqueeze-the-data-from-torch-size50-to-torch-size50-1</link>
      <description><![CDATA[我正在学习 FreeCodeCamp 的 PyTorch 深度学习课程，疑问是：
weight = 0.7
bias = 0.3
start = 0
end = 1
step = 0.02

X = torch.arange(start, end, step).unsqueeze(dim=1)
y=weight*X + bias
X[:10], y[:10]
train_split=int(0.8*len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test=X[train_split:], y[train_split:]

为什么使用 unsqueeze 函数生成大小为 [50, 1] 的张量，而不是 [50]？导师说这会导致错误，但我不知道为什么会发生错误？
你能用数学和基本原理回答这个问题吗？
尝试训练模型后，我收到此错误：
class LinearRegressionModelv2(nn.Module):
def __init__(self):
super().__init__()
self.linear_layer = nn.Linear(in_features=1, out_features=1)

def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
return self.linear_layer(x)

torch.manual_seed(42)
model_v2 = LinearRegressionModelv2()

y_prediction = model_v2(X_train)
IndexError: 维度超出范围（预期在 [-1, 0] 范围内，但结果为 -2）]]></description>
      <guid>https://stackoverflow.com/questions/79409149/why-do-i-have-to-unsqueeze-the-data-from-torch-size50-to-torch-size50-1</guid>
      <pubDate>Mon, 03 Feb 2025 14:44:08 GMT</pubDate>
    </item>
    <item>
      <title>为什么MobileNet V2模型（mobilenet_v2_1.4_224.tflite）的概率总是相同的？</title>
      <link>https://stackoverflow.com/questions/79281349/why-are-the-probabilities-always-the-same-with-mobilenet-v2-model-mobilenet-v2</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79281349/why-are-the-probabilities-always-the-same-with-mobilenet-v2-model-mobilenet-v2</guid>
      <pubDate>Sat, 14 Dec 2024 20:23:10 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch RuntimeError：CUDA 内存不足，有大量可用内存</title>
      <link>https://stackoverflow.com/questions/71498324/pytorch-runtimeerror-cuda-out-of-memory-with-a-huge-amount-of-free-memory</link>
      <description><![CDATA[在训练模型时，我遇到了以下问题：
RuntimeError：CUDA 内存不足。尝试分配 304.00 MiB（GPU 0；总容量 8.00 GiB；已分配 142.76 MiB；6.32 GiB 可用；PyTorch 总共保留 158.00 MiB）如果保留内存是 &gt;&gt; 分配的内存，请尝试设置 max_split_size_mb 以避免碎片。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档
我们可以看到，当尝试分配 304 MiB 内存时发生错误，而 6.32 GiB 是可用的！问题是什么？我可以看到，建议的选项是设置 max_split_size_mb 以避免碎片。这有帮助吗？如何正确做到这一点？
这是我的 PyTorch 版本：
torch==1.10.2+cu113 
torchvision==0.11.3+cu113 
torchaudio===0.10.2+cu113
]]></description>
      <guid>https://stackoverflow.com/questions/71498324/pytorch-runtimeerror-cuda-out-of-memory-with-a-huge-amount-of-free-memory</guid>
      <pubDate>Wed, 16 Mar 2022 13:53:45 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：编码器要求其输入为统一的字符串或数字。得到 ['int', 'str']</title>
      <link>https://stackoverflow.com/questions/71193740/typeerror-encoders-require-their-input-to-be-uniformly-strings-or-numbers-got</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/71193740/typeerror-encoders-require-their-input-to-be-uniformly-strings-or-numbers-got</guid>
      <pubDate>Sun, 20 Feb 2022 11:04:02 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 上带预热功能的 Adam 优化器</title>
      <link>https://stackoverflow.com/questions/65343377/adam-optimizer-with-warmup-on-pytorch</link>
      <description><![CDATA[在论文Attention is all you need中，在第 5.3 节中，作者建议线性增加学习率，然后按步数的平方根倒数比例减少。

我们如何使用Adam 优化器？最好不要使用其他软件包。]]></description>
      <guid>https://stackoverflow.com/questions/65343377/adam-optimizer-with-warmup-on-pytorch</guid>
      <pubDate>Thu, 17 Dec 2020 15:12:56 GMT</pubDate>
    </item>
    <item>
      <title>错误“无法将字符串转换为浮点数：‘INLAND’”</title>
      <link>https://stackoverflow.com/questions/55332339/error-could-not-convert-string-to-float-inland</link>
      <description><![CDATA[我正在做一个使用机器学习进行房价预测的项目，想提交给一家私人公司申请。
我正在为这个项目使用 Jupiter 笔记本，但我无法修复有关将字符串转换为数值数据的错误
from sklearn.model_selection import train_test_split
X_train,X_test, Y_train, Y_test= train_test_split(X,
Y,
test_size=0.2,
random_state=0)
from sklearn.preprocessing import StandardScaler
independent_scalar = StandardScaler()
X_train = independent_scalar.fit_transform (X_train) #fit 和 transform
X_test = independent_scalar.transform (X_test) # only transform
print(X_train)

我希望训练集数据完全是数值的]]></description>
      <guid>https://stackoverflow.com/questions/55332339/error-could-not-convert-string-to-float-inland</guid>
      <pubDate>Mon, 25 Mar 2019 06:33:59 GMT</pubDate>
    </item>
    </channel>
</rss>