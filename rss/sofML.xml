<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 15 Nov 2024 12:33:59 GMT</lastBuildDate>
    <item>
      <title>对图像集的平均值进行标注</title>
      <link>https://stackoverflow.com/questions/79192357/captioning-an-average-of-image-set</link>
      <description><![CDATA[我正在寻找一种用一个句子描述一组图像的方法。或者，我需要一种方法来在概念上平均一组图像，然后再将该“概念”（可能是特征向量）提供给常规字幕模型。
为什么？
用于 Lora 训练评估。在适合整个数据集的提示上测试训练后的生成模型会很有用，而不是选择单个图像的标题或尝试找出它们之间的共同点。此外，这还允许生成单个负面提示来测试模型在范围外的提示上的表现。
我到目前为止所做的：
我已经修改了现有的 CLIP+BLIP 询问器以处理图像集（它也可以生成负片）。然而，虽然 CLIP 字幕允许在使用图像特征选择最佳字幕之前对其进行平均，但它的准确性远低于 BLIP 生成的字幕，后者仅适用于单幅图像。我需要一个可以像 CLIP 一样接收特征向量的模型，这样我就可以对它们进行预处理。]]></description>
      <guid>https://stackoverflow.com/questions/79192357/captioning-an-average-of-image-set</guid>
      <pubDate>Fri, 15 Nov 2024 12:19:27 GMT</pubDate>
    </item>
    <item>
      <title>如何从 CoreML 预测中获取置信度变量</title>
      <link>https://stackoverflow.com/questions/79192127/how-can-i-get-the-confidence-variable-from-a-coreml-prediction</link>
      <description><![CDATA[我正在使用 CreateML 工具训练文本分类器，当我使用预览功能并输入一个句子时，它会给我一个预测以及一个置信度变量
以下是我在应用程序上使用该模型的方式
import CoreML
...

func predict(phrase:String) -&gt; String {
guard let rollModel = try? Roll(configuration: MLModelConfiguration()) else {
return &quot;Failed to load the Roll Model.&quot;
}

let rollModelInput = RollInput(text: phrase)

guard let prediction = try? rollModel.prediction(input: rollModelInput, options: MLPredictionOptions()) else {
return &quot;Roll Model Prediction Failed&quot;
}

return prediction.label
}

这有效，它提供了预测。
我的数据是标准文本/标签格式
即使我将模型导出到 xcode 并在 xcode 中运行预览，置信度变量仍然存在。
当我在设备上运行预测时，我想知道置信度变量是什么，我如何获取访问权限？
]]></description>
      <guid>https://stackoverflow.com/questions/79192127/how-can-i-get-the-confidence-variable-from-a-coreml-prediction</guid>
      <pubDate>Fri, 15 Nov 2024 11:13:21 GMT</pubDate>
    </item>
    <item>
      <title>如何知道平均绝对误差（MAE）是否是根据测试数据计算出来的？</title>
      <link>https://stackoverflow.com/questions/79191223/how-to-know-mean-absolute-error-mae-was-calculated-from-test-data-or-not</link>
      <description><![CDATA[我正在尝试使用 R 中的留一法交叉验证 (LOOCV) 来计算各种类型模型的平均绝对误差 (MAE)。在此链接中，作者展示了使用 R 中的 caret 包进行 LOOCV 然后计算 MAE 来计算 MAE 的指南。
链接为：https://www.statology.org/leave-one-out-cross-validation-in-r/
结果为
library(caret)

#指定交叉验证方法
ctrl &lt;- trainControl(method = &quot;LOOCV&quot;)

#拟合回归模型并使用 LOOCV 评估性能
model &lt;- train(y ~ x1 + x2, data = df, method = &quot;lm&quot;, trControl = ctrl)

#查看 LOOCV 摘要
print(model)

线性回归

10 个样本
2 个预测器

无预处理
重采样：留一法交叉验证
样本量摘要：9、9、9、9、9、9、...

重采样结果：

RMSE Rsquared MAE 
3.619456 0.6186766 3.146155

调整参数“截距”保持不变，值为 TRUE

但是，尚不清楚 MAE 是使用训练数据还是测试数据（样本外）计算的。]]></description>
      <guid>https://stackoverflow.com/questions/79191223/how-to-know-mean-absolute-error-mae-was-calculated-from-test-data-or-not</guid>
      <pubDate>Fri, 15 Nov 2024 05:50:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 HMM 模型对某些动作进行门预测？</title>
      <link>https://stackoverflow.com/questions/79190691/how-to-make-gate-predictions-for-certain-movements-using-hmm-model</link>
      <description><![CDATA[我想使用 IMU 传感器来预测用户接下来要进入哪个门（用于行走、跑步、跳跃等）。根据我的研究，HMM 似乎是状态预测的最佳机器学习模型。但我在网上看到的所有模型都是基于预先记录的静态数据，而不是正在记录的实时数据。我应该如何实现这一点？
我已经实现了一个滚动窗口，它使用最后捕获的 30 条数据记录进行下一次预测，并对它们进行了规范化。
我已经确定了我想要识别的每个动作的门，并且我已经设置了一个分类算法，该算法遍历每个窗口并使用它来对当前动作进行分类。
我打算用它们来验证我将要做出的预测（或者甚至在需要时将其用作输入来预测人的下一个动作）。]]></description>
      <guid>https://stackoverflow.com/questions/79190691/how-to-make-gate-predictions-for-certain-movements-using-hmm-model</guid>
      <pubDate>Thu, 14 Nov 2024 23:15:03 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 提前停止轮次</title>
      <link>https://stackoverflow.com/questions/79189607/xgboost-early-stopping-rounds</link>
      <description><![CDATA[下面的代码一直在崩溃，我不知道发生了什么
import optuna
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 假设 `X` 和 `y` 是你的特征矩阵和目标数组
X_train, X_valid, y_train, y_valid = train_test_split(df_combined, y, test_size=0.2, random_state=42)

# 为 Optuna 定义目标函数
def objective(trial):
# 为超参数建议值
params = {
&quot;objective&quot;: &quot;reg:squarederror&quot;,
&quot;eval_metric&quot;: &quot;rmse&quot;,
&quot;tree_method&quot;: &quot;hist&quot;, # 使用 hist 方法
&quot;device&quot;: &quot;cuda&quot;, # 指定使用 GPU
&quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 0.01, 0.3, log=True),
&quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 3, 10),
&quot;min_child_weight&quot;: trial.suggest_float(&quot;min_child_weight&quot;, 1, 10),
&quot;gamma&quot;: trial.suggest_float(&quot;gamma&quot;, 0, 1),
&quot;subsample&quot;: trial.suggest_float(&quot;subsample&quot;, 0.5, 1.0),
&quot;colsample_bytree&quot;: trial.suggest_float(&quot;colsample_bytree&quot;, 0.5, 1.0),
&quot;lambda&quot;: trial.suggest_float(&quot;lambda&quot;, 1e-3, 10.0, log=True),
&quot;alpha&quot;: trial.suggest_float(&quot;alpha&quot;, 1e-3, 10.0, log=True),
&quot;n_estimators&quot;: 1000 # 在模型初始化中定义 n_estimators
}

# 初始化模型
model = xgb.XGBRegressor(**params)

# 使用早期停止回调训练模型
model.fit(
X_train,
y_train,
eval_set=[(X_valid, y_valid)],
verbose=False,
early_stopping_rounds=50 # 如果之后没有改进则停止50 轮
)

# 预测并计算验证集的 RMSE
preds = model.predict(X_valid)
rmse = mean_squared_error(y_valid, preds, squared=False)

return rmse # Optuna 将其最小化

# 设置 Optuna 研究
study = optuna.create_study(direction=&quot;minimize&quot;)

# 优化超参数
study.optimize(objective, n_trials=100, n_jobs=40) # 100 次试验，40 次并行作业

# 显示最佳试验
print(&quot;最佳试验：&quot;)
trial = study.best_trial
print(f&quot;值 (RMSE)：{trial.value}&quot;)
print(&quot; Params: &quot;)
for key, value in trial.params.items():
print(f&quot; {key}: {value}&quot;)

我得到
TypeError: XGBModel.fit() 得到一个意外的关键字参数 &#39;early_stopping_rounds&#39;

我已更新所有内容以确保我拥有所有更新的库。
提前停止轮次是正确的（我认为），但由于某种原因，它只是爆炸了。]]></description>
      <guid>https://stackoverflow.com/questions/79189607/xgboost-early-stopping-rounds</guid>
      <pubDate>Thu, 14 Nov 2024 16:14:40 GMT</pubDate>
    </item>
    <item>
      <title>我在 SVR 建模中做错了什么？</title>
      <link>https://stackoverflow.com/questions/79188969/what-have-i-done-wrong-in-my-svr-modeling</link>
      <description><![CDATA[我正在尝试使用 SVR 模型预测风速，因为我的数据集都是连续数字。我按照惯例对数据进行训练测试分割，并使用 minmax 缩放器来规范化我的数据集。
# 训练 - 测试分割
X_train , X_test , y_train , y_test = train_test_split (X , y , test_size =0.3 , random_state =42)

# 规范化数据 - MinMax 缩放器
scaler = MinMaxScaler ()
df_normalized = scaler.fit_transform(df)
df_normalized = pd.DataFrame(df_normalized, columns=df.columns)
X_train = scaler.fit_transform ( X_train )
X_test = scaler.transform ( X_test )

# 支持向量回归 (SVR) 模型
svr_model = SVR(kernel=&#39;rbf&#39;)
svr_model.fit(X_train, y_train)

# 预测和评估 SVR 结果
y_pred = svr_model.predict(X_test)

# 可视化 SVR 结果 
plt.scatter(X_train, y_train, color = &#39;magenta&#39;, label = &#39;实际数据&#39;)
plt.plot(X_test, y_pred, color = &#39;blue&#39;, label = &#39;SVR 预测&#39;)
plt.title(&quot;SVR - 风速预测&quot;)
plt.xlabel(&#39;位置级别&#39;)
plt.ylabel(&#39;风速&#39;)
plt.legend()
plt.show()

plt.scatter(X_train, y_train, color = &#39;magenta&#39;, label = &#39;实际数据&#39;)
引发 ValueError(&quot;x 和 y 必须大小相同&quot;)
ValueError：x 和 y 必须大小相同

我尝试使用 X_grid 和 Y_grid，但问题相同。有人能帮我解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79188969/what-have-i-done-wrong-in-my-svr-modeling</guid>
      <pubDate>Thu, 14 Nov 2024 13:30:38 GMT</pubDate>
    </item>
    <item>
      <title>分割任务中神经网络中编码器层和解码器层的不一致</title>
      <link>https://stackoverflow.com/questions/79188395/inconsistency-of-encoder-and-decoder-layers-in-a-neural-network-for-a-segmentati</link>
      <description><![CDATA[在我的神经网络中，瓶颈层之后，它拒绝接受该值并给出错误：
无效的 output_size &#39;torch.Size(\[16, 16\])&#39;（dim 0 必须介于 16 和 18 之间）。

这是我的模型的代码：
class SegNet(nn.Module):
def __init__(self):
super().__init__()

self.enc_conv0 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=2)
self.enc_conv1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=2)
self.enc_conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=2)
self.enc_conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=2)

self.pool0 = nn.MaxPool2d(kernel_size=2, stride=1, return_indices=True) # 256 -&gt; 128
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1, return_indices=True) # 128 -&gt; 64
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=1, return_indices=True) # 64 -&gt; 32
self.pool3 = nn.MaxPool2d(kernel_size=2, stride=1, return_indices=True) # 32 -&gt; 16

# 瓶颈
self.bottleneck_conv = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, stride=1)

# 解码器（上采样）

self.dec_conv0 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=2)
self.dec_conv1 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=2)
self.dec_conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=2)
self.dec_conv3 = nn.Conv2d(in_channels=32, out_channels=2, kernel_size=3, stride=2, padding=2)

self.upsample0 = nn.MaxUnpool2d(kernel_size=2, stride=1) # 16 -&gt; 32
self.upsample1 = nn.MaxUnpool2d(kernel_size=2, stride=1) # 32 -&gt; 64
self.upsample2 = nn.MaxUnpool2d(kernel_size=2, stride=1) # 64 -&gt; 128
self.upsample3 = nn.MaxUnpool2d(kernel_size=2, stride=1) # 128 -&gt; 256

def forward(self, x):
# 编码器
print(f&#39;x = {x.size()}&#39;)
e0, indices0 = self.pool0(F.relu(self.enc_conv0(x)))
print(f&#39;e0 = {e0.size()}&#39;)
e1, indices1 = self.pool1(F.relu(self.enc_conv1(e0)))
print(f&#39;e1 = {e1.size()}&#39;)
e2, indices2 = self.pool2(F.relu(self.enc_conv2(e1)))
print(f&#39;e2 = {e2.size()}&#39;)
e3, indices3 = self.pool3(F.relu(self.enc_conv3(e2)))
print(f&#39;e3 = {e3.size()}&#39;)

# 瓶颈
b = self.bottleneck_conv(e3)
print(b.size())

# 解码器
d0 = F.relu(self.dec_conv0(self.upsample0(b, indices3, output_size=e3.size()))) #self.upsample0(F.relu(self.dec_conv0(b)), indices3)
print(f&#39;d0 = {d0.size()}&#39;)
d1 = F.relu(self.dec_conv1(self.upsample1(d0, indices2, output_size=e2.size())))
print(f&#39;d1 = {d1.size()}&#39;)
d2 = F.relu(self.dec_conv2(self.upsample2(d1, indices1, output_size=e1.size())))
print(f&#39;d2 = {d2.size()}&#39;)
d3 = self.dec_conv3(self.upsample3(d2, indices0, output_size=e0.size())) # 无激活
print(f&#39;d3 = {d3.size()}&#39;)
return d3

当值不合适时，我已经解决过许多类似的问题，但我不知道如何处理这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79188395/inconsistency-of-encoder-and-decoder-layers-in-a-neural-network-for-a-segmentati</guid>
      <pubDate>Thu, 14 Nov 2024 10:42:35 GMT</pubDate>
    </item>
    <item>
      <title>无法训练我的 UNET 多类别细分模型</title>
      <link>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</link>
      <description><![CDATA[我尝试使用 pytorch 从头开始​​制作 UNET。我的模型输出只有黑色蒙版。我需要分割汽车上的损坏，所以我实现了一个彩色图。我确信 70% 的数据集有问题，而这个彩色图恰恰就是其中的原因。任务是多类预测，所以我使用交叉熵损失函数。我将提供我的数据集和训练文件的代码。
# dataset.py
import os
from PIL import Image
from torch.utils.data import Dataset
import numpy as np
import torch

class Segm_Dataset(Dataset):
def __init__(self, image_dir, mask_dir, color_map):
self.image_dir = image_dir
self.mask_dir = mask_dir
self.image_files = os.listdir(self.image_dir)
self.mask_files = os.listdir(self.mask_dir)
self.color_map = color_map

def __len__(self):
return len(self.image_files)

def __getitem__(self, idx):
image_path = os.path.join(self.image_dir, self.image_files[idx])
mask_path = os.path.join(self.mask_dir, self.mask_files[idx])
image = np.array(Image.open(image_path).convert(&#39;RGB&#39;))
mask = np.array(Image.open(mask_path).convert(&#39;RGB&#39;), dtype=np.float32)
label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int64)

for color, label in self.color_map.items():
color_array = np.array(color, dtype=np.float32)
mask_area = np.all(mask == color_array, axis=-1)
label_mask[mask_area] = label

image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)
label_mask = torch.tensor(label_mask, dtype=torch.long)

返回图像，label_mask

# train.py
从模型导入 UNET
从 tqdm 导入 tqdm
从数据集导入 Segm_Dataset
导入 torch
从 torch.utils.data 导入 DataLoader
导入 torch.nn 作为 nn
导入 torch.optim 作为 optim
导入 os

LEARNING_RATE = 1e-4
BATCH_SIZE = 5
NUM_EPOCHS = 10
NUM_WORKERS = 2
IMAGE_HEIGHT = 180
IMAGE_WIDTH = 180
PIN_MEMORY = True
LOAD_MODEL =错误
TRAIN_IMG_DIR = r&#39;data\train\images&#39;
TRAIN_MASK_DIR = r&#39;data\train\masks&#39;
VAL_IMG_DIR = r&#39;data\val\images&#39;
VAL_MASK_DIR = r&#39;data\val\masks&#39;
SAVED_MODELS_PATH = r&#39;saved_models&#39;

color_map = {
(19, 164, 201): 0, # 缺失部分：#13A4C9
(166, 255, 71): 1, # 破损部分：#A6FF47
(180, 45, 56): 2, # 划痕：#B42D38
(225, 150, 96): 3, # 破裂：#E19660
(144, 60, 89): 4, # 凹痕： #903C59
(167, 116, 27): 5, # 剥落: #A7741B
(180, 14, 19): 6, # 油漆剥落: #B40E13
(115, 194, 206): 7, # 腐蚀: #73C2CE
}

train_dataset = Segm_Dataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, color_map)
train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)

val_dataset = Segm_Dataset(VAL_IMG_DIR, VAL_MASK_DIR, color_map)
val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)

model = UNET(in_channels=3, out_channels=len(color_map))
model = model.cuda() if torch.cuda.is_available() else model

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

for epoch in range(NUM_EPOCHS):
train_loop = tqdm(enumerate(train_loader), total=len(train_loader))

for batch_index, (data, target) in train_loop: 
#前向传递
scores = model(data)
train_loss = criterion(scores, target)

#后向传递
optimizer.zero_grad()
train_loss.backward()

#梯度下降或优化器步骤
optimizer.step()

if batch_index % 10 == 0:
current_batch = batch_index
val_loss = 0
with torch.no_grad():
for val_data, val_targets in val_loader:
val_scores = model(val_data)
val_loss = criterion(val_scores, val_targets)

#更新进度条
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

else:
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

checkpoint = {
&#39;epoch&#39;: epoch + 1,
&#39;model_state_dict&#39;: model.state_dict(),
&#39;optimizer_state_dict&#39;: optimizer.state_dict(),
&#39;train_loss&#39;: train_loss.item(),
&#39;val_loss&#39;: val_loss.item()
}

torch.save(checkpoint, os.path.join(SAVED_MODELS_PATH, f&#39;unet_epoch_{epoch}.pth&#39;))

一些训练 epoches:
Epoch: [9/10]: 100%|████████████████| 888/888 [34:24&lt;00:00, 2.32s/it, train_loss=0.000271, val_batch=880, val_loss=0.000278]

Epoch：[10/10]：100%|███████████████| 888/888 [34:29&lt;00:00, 2.33s/it, train_loss=0.000163, val_batch=880, val_loss=0.000167]
]]></description>
      <guid>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</guid>
      <pubDate>Thu, 14 Nov 2024 09:17:27 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：预期隐藏[0]大小（2，50，1024），在PyTorch中得到[1，50，1024]</title>
      <link>https://stackoverflow.com/questions/79187818/runtimeerror-expected-hidden0-size-2-50-1024-got-1-50-1024-in-pytorc</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79187818/runtimeerror-expected-hidden0-size-2-50-1024-got-1-50-1024-in-pytorc</guid>
      <pubDate>Thu, 14 Nov 2024 07:55:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 Apache Spark ML 进行预测</title>
      <link>https://stackoverflow.com/questions/79186067/prediction-with-apache-spark-ml</link>
      <description><![CDATA[我是 Apache Spark ML 的新手。
我想预测按年龄和国家/地区划分的余额。作为输入，我有一个以下格式的 CSV 文件：
RowNumber,Age,Country,Balance

模型已构建，也可以针对测试数据进行训练。到目前为止一切正常。
我现在的问题是当我想对新客户记录进行预测时
Dataset&lt;Row&gt; newCustomer = spark.createDataFrame(Collections.singletonList(
new Customer(28, ‘Germany’)), Customer.class);
Dataset&lt;Row&gt; newCustomerPrediction = model.transform(newCustomer);

我收到以下错误消息：
java.lang.IllegalArgumentException：CountryIndex 不存在。可用：年龄、国家。

如何获取新数据集的预测？
public static void main(String[] args) {

SparkSession spark = SparkSession
.builder()
.master(&quot;local[*]&quot;) 
.appName(&quot;JavaGeneralizedLinearRegressionExample&quot;)
.getOrCreate();

Dataset&lt;Row&gt; data = spark.read()
.option(&quot;header&quot;, &quot;true&quot;)
.option(&quot;inferSchema&quot;, &quot;true&quot;)
.option(&quot;delimiter&quot;, &quot;,&quot;) // oder &quot;,&quot;您可以使用 Dataiformat
.csv(&quot;/data/testdaten_v4.csv&quot;);

StringIndexer countryIndexer = new StringIndexer()
.setInputCol(&quot;Country&quot;)
.setOutputCol(&quot;CountryIndex&quot;)
.setHandleInvalid(&quot;skip&quot;);
OneHotEncoder countryEncoder = new OneHotEncoder()
.setInputCol(&quot;CountryIndex&quot;)
.setOutputCol(&quot;CountryVec&quot;);

VectorAssembler assembler = new VectorAssembler()
.setInputCols(new String[]{&quot;Age&quot;, &quot;CountryVec&quot;}) // 可以添加其他 Features
.setOutputCol(&quot;features&quot;);

StandardScaler scaler = new StandardScaler()
.setInputCol(&quot;features&quot;)
.setOutputCol(&quot;scaledFeatures&quot;);

LinearRegression lr = new LinearRegression()
.setLabelCol(&quot;Balance&quot;)
.setFeaturesCol(&quot;scaledFeatures&quot;)
.setMaxIter(100)
.setRegParam(0.3)
.setElasticNetParam(0.8);

Pipeline pipeline = new Pipeline()
.setStages(new PipelineStage[]{countryIndexer, countryEncoder, assembler, scaler, lr});

PipelineModel model = pipeline.fit(data);

Dataset&lt;Row&gt;[] splits = data.randomSplit(new double[]{0.8, 0.2}, 42);
数据集&lt;Row&gt; trainData = splits[0];
数据集&lt;Row&gt; testData = splits[1];

数据集&lt;Row&gt; predictions = model.transform(testData);
predictions.select(&quot;Age&quot;, &quot;Country&quot;, &quot;Balance&quot;, &quot;prediction&quot;).show();

RegressionEvaluator evaluator = new RegressionEvaluator()
.setLabelCol(&quot;Balance&quot;)
.setPredictionCol(&quot;prediction&quot;)
.setMetricName(&quot;rmse&quot;);
double rmse = evaluator.evaluate(predictions);

数据集&lt;Row&gt; newCustomer = spark.createDataFrame(Collections.singletonList(
new Customer(28, &quot;Germany&quot;)), Customer.class);
Dataset&lt;Row&gt; newCustomerPrediction = model.transform(newCustomer);
newCustomerPrediction.select(&quot;prediction&quot;).show();

spark.stop();
}

public static class Customer {
private int Age;
private String Country;

public Customer(int age, String country) {
this.Age = age;
this.Country = country;
}

public int getAge() { return Age; }
public String getCountry() { return Country; } 
}
]]></description>
      <guid>https://stackoverflow.com/questions/79186067/prediction-with-apache-spark-ml</guid>
      <pubDate>Wed, 13 Nov 2024 17:42:53 GMT</pubDate>
    </item>
    <item>
      <title>VSCode 安装 hugginface relik 库时出错</title>
      <link>https://stackoverflow.com/questions/79182549/vscode-install-error-for-the-hugginface-relik-library</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79182549/vscode-install-error-for-the-hugginface-relik-library</guid>
      <pubDate>Tue, 12 Nov 2024 19:53:20 GMT</pubDate>
    </item>
    <item>
      <title>Keras 模型中的自定义编码器和解码器层显示为未构建</title>
      <link>https://stackoverflow.com/questions/79034907/custom-encoder-and-decoder-layers-within-keras-model-show-as-unbuilt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79034907/custom-encoder-and-decoder-layers-within-keras-model-show-as-unbuilt</guid>
      <pubDate>Sat, 28 Sep 2024 18:27:22 GMT</pubDate>
    </item>
    <item>
      <title>如何将多个观测值拟合到单个高斯过程</title>
      <link>https://stackoverflow.com/questions/78554891/how-to-fit-a-multiple-observations-to-single-gaussian-process</link>
      <description><![CDATA[我试图将多个观测值拟合到单个高斯过程。
我尝试像这样拟合两个观测值 (Y) 的数据：
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C

# 示例数据

# 输入数据 X 
X = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]])

# 输出数据 Y 
Y = np.array([[1.5, 2.5], [2.5, 3.5], [3.5, 4.5], [4.5, 5.5], [5.5, 6.5]])
kernel = C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)

# 拟合模型
gp.fit(X, Y)

mean_prediction, cov_prediction = gp.predict(X, return_cov=True)

我得到了两个 mean_prediction 数组和两个 cov_prediction 矩阵。但我想要一个与单个拟合 GP 对应的观测值相同维度的单个均值和协方差矩阵。我该如何实现？]]></description>
      <guid>https://stackoverflow.com/questions/78554891/how-to-fit-a-multiple-observations-to-single-gaussian-process</guid>
      <pubDate>Thu, 30 May 2024 12:16:10 GMT</pubDate>
    </item>
    <item>
      <title>为嵌套在多个文件夹中的数据创建训练和测试拆分</title>
      <link>https://stackoverflow.com/questions/64758066/creating-a-train-test-split-for-data-nested-in-multiple-folders</link>
      <description><![CDATA[我正在准备用于训练图像识别模型的数据。我目前有一个文件夹（数据集），其中包含多个带有标签名称的文件夹，这些文件夹中包含图像。
我想以某种方式拆分此数据集，以便我有两个具有相同子文件夹的主文件夹，但这些文件夹中的图像数量应根据首选的训练/测试拆分，例如，训练数据集中的 90% 的图像和测试数据集中的 10% 的图像。
我正在努力寻找拆分数据的最佳方法。我读过一个建议，pytorch torch.utils.Dataset 类可能是一种方法，但我似乎无法让它工作以保留文件夹层次结构。]]></description>
      <guid>https://stackoverflow.com/questions/64758066/creating-a-train-test-split-for-data-nested-in-multiple-folders</guid>
      <pubDate>Mon, 09 Nov 2020 19:27:34 GMT</pubDate>
    </item>
    <item>
      <title>什么是 x_train.reshape() 以及它的作用是什么？</title>
      <link>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</link>
      <description><![CDATA[使用 MNIST 数据集
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist

# MNIST 数据集参数
num_classes = 10 # 总类别（0-9 位数字）
num_features = 784 # 数据特征（图像形状：28*28）

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 转换为 float32
x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)

# 将图像展平为 784 个特征（28*28）的一维向量
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])

# 将图像值从 [0, 255] 标准化为 [0, 1]
x_train, x_test = x_train / 255., x_test / 255.

在这些代码的第 15 行中，
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])。我无法理解这些重塑在我们的数据集中到底起什么作用..?? 请解释一下。]]></description>
      <guid>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</guid>
      <pubDate>Sat, 02 May 2020 06:44:34 GMT</pubDate>
    </item>
    </channel>
</rss>