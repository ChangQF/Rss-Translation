<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 23 Sep 2024 09:20:01 GMT</lastBuildDate>
    <item>
      <title>我可以做些什么来提高我在 Kaggle 泰坦尼克号竞赛中的表现？</title>
      <link>https://stackoverflow.com/questions/79013898/what-can-i-do-to-improve-my-performance-on-kaggles-titanic-contest</link>
      <description><![CDATA[我尝试了所有方法，使用决策树和随机森林来预测泰坦尼克号上的幸存者。但都不起作用。
我使用决策树和随机森林模型来预测谁会在泰坦尼克号上幸存。
我选择的特征包括“Pclass”、“性别”、“年龄”、“票价”和“舱位”，对于舱位，我使用 OneHotEncoder 来转换数据，对于性别，我使用 LabelEncoder。
然后我使用决策树来训练模型。当我设置 max_depth=4 并提交时，我的模型在预测中达到了最高的准确度分数，即 0.7799。后来，无论我做什么，我都无法获得更高的分数。我使用了 train_test_split、RandomForestClassifier，我使用了 GridSearch 来测试不同的参数。这些都不起作用，我的分数总是低于 0.7799。每当我设置 max_depth=4 时，无论是决策树还是随机森林，分数始终为 0.7799
如何使用 DecisionTree 或 RandomForest 提高我的性能？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79013898/what-can-i-do-to-improve-my-performance-on-kaggles-titanic-contest</guid>
      <pubDate>Mon, 23 Sep 2024 08:58:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Deepface Deepface.represent 从 ROI 获取嵌入时出错</title>
      <link>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</link>
      <description><![CDATA[我在使用 Deepface 从 Retinaface 识别的裁剪 ROI 获取嵌入时遇到了问题。
我正尝试使用一些名人的数据集（图像）学习对象识别，并可能考虑将其用于我的个人照片库。我尝试使用 Haar Cascade 进行人脸检测，并使用 Open Cv 中的 LBPHFaceRecognize 进行人脸识别，效果很好。然后我想尝试使用 Retinafce 进行人脸检测并获得 ROI。ROI 存储在列表中，并使用 Deepface 从选定的 ROI 获取嵌入并存储在另一个列表中。我正在尝试将嵌入存储到列表中，但我一直收到
raise ValueError（
ValueError：无法在 numpy 数组中检测到人脸。请确认图片是人脸照片，或考虑将 force_detection 参数设置为 False。
虽然所有图像都有一张被清楚检测到的人脸。这是我的代码供参考：
import os
import cv2 as cv
from retinaface import RetinaFace
from deepface import DeepFace
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

artist = [&#39;50cent&#39;] # type: ignore #MJ the GOAT!! , &#39;Kanye&#39;, &#39;Eminem&#39;, &#39;MichaelJackson&#39;
ROOT_DIR = &#39;asset/Face_Recon_Dataset&#39; #图像数据集的路径
faces_roi =[]
labels = []
embeddings = []
#现在在脸部坐标上绘制一个矩形
#脸部范围有：
# x1, y1) = (28, 51) #左上角
# (x2, y2) = (61, 98) #右下角
&quot;&quot;&quot; 这定义了检测到的脸部周围的矩形边界框。
- x1 (28)：脸部左边缘
- y1 (51)：脸部上边缘
- x2 (61)：脸部右边缘
- y2 (98)：脸部下边缘&quot;&quot;&quot;

def get_roi():
for artist_name in artist:
# 获取艺术家姓名的索引
label = artist.index(artist_name)
image_folder = os.path.join(ROOT_DIR,artist_name) # 获取包含图像的实际文件夹
for artist_images in os.listdir(image_folder): # 列出该目录中的所有图像
image = os.path.join(image_folder,artist_images)
resp = RetinaFace.detect_faces(image)
# 确保人脸存在
if isinstance(resp,dict):
img = cv.imread(image)
for face_id, face_data in resp.items():
# print(face_id)
# print(&quot;x1: &quot;, face_data[&#39;facial_area&#39;][0])
# print(&quot;y1: &quot;, face_data[&#39;facial_area&#39;][1])
# print(&quot;x2: &quot;, face_data[&#39;facial_area&#39;][2])
# print(&quot;y2: &quot;, face_data[&#39;facial_area&#39;][3], &quot;\n&quot;)
# 读取图像

# 检测人脸
x1 = face_data[&#39;facial_area&#39;][0]
y1 = face_data[&#39;facial_area&#39;][1]
x2 = face_data[&#39;facial_area&#39;][2]
y2 = face_data[&#39;facial_area&#39;][3]

# 为人脸绘制边界框 
# faces_rect = cv.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
face_roi = img[y1:y2,x1:x2]

#用其名称标记裁剪后的 roi 人脸
faces_roi.append(face_roi)

labels.append(label)
print(len(faces_roi))
print(len(labels))
print(&quot;已标记和索引的图像&quot;)
print(&quot;正在初始化嵌入过程.....&quot;)
get_embeddings()

def get_embeddings():
&quot;&quot;&quot; 使用 deepface 从每个人脸 roi 中提取嵌入&quot;&quot;&quot;
print(&quot;Satarting embedding: 🚀🚀 &quot;)
for roi in faces_roi:
face_roi_resized = cv.resize(roi, (160, 160)) # 将人脸 ROI 调整为 160x160 像素
embedding = DeepFace.represent(face_roi_resized, model_name=&quot;Facenet&quot;)
print(embedding)
embeddings.append(embedding)
print(&quot;Vectors storage in list..&quot;)

get_roi()

# 是时候使用 svm 分类器测试和训练这个坏家伙了
# 将嵌入和索引标记为 numpy 数组
X = np.array(embeddings) #feature
y = np.array(labels) #label

# 将数据分成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 SVM 分类器
svm_model = SVC(kernel=&#39;linear&#39;) # 线性核是嵌入的良好默认值
svm_model.fit(X_train, y_train)

# 评估模型
y_pred = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;SVM 模型准确率：{accuracy * 100:.2f}%&quot;)


有人能帮我理解为什么即使 ROI 已被裁剪，该错误仍然持续存在吗？解决该错误的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</guid>
      <pubDate>Mon, 23 Sep 2024 08:03:12 GMT</pubDate>
    </item>
    <item>
      <title>面部皮肤健康分析 API [关闭]</title>
      <link>https://stackoverflow.com/questions/79013615/face-skin-health-analysis-api</link>
      <description><![CDATA[我想创建一个 React Native 应用来评估皮肤健康状况。我需要测量诸如光泽、斑点、皱纹、纹理、黑眼圈、眼袋、发红、油性、毛孔和水分等因素，并按 1 到 100 的等级显示每个因素的摘要。我遇到了一些用于此目的的 API，但它们非常昂贵。有没有使用 Python 和 OpenCV 的解决方案？是否有可用的模型或指南可以帮助我学习和开发项目的 API？
我尝试了 Google ML Vision 和 Microsoft API，但它们不符合我的要求。我找到了一些 API，但它们非常昂贵。现在我正在寻找自定义模型或数据集。]]></description>
      <guid>https://stackoverflow.com/questions/79013615/face-skin-health-analysis-api</guid>
      <pubDate>Mon, 23 Sep 2024 07:38:51 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用回归算法而不是分类算法？（描述中提供的数据集））[关闭]</title>
      <link>https://stackoverflow.com/questions/79013513/why-are-regression-algorithms-used-instead-of-classification-algorithms-datase</link>
      <description><![CDATA[众所周知，在 ML 中，如果依赖特征本质上是连续的，则应用回归模型。但是，如果依赖特征本质上是分类的，则使用分类算法。
正如您在这张图（https://i.sstatic.net/9Q3wfudK.png）中看到的那样，最大值为。大量数据点重复出现，表明它们正在形成类别。
那么，为什么这里使用回归？
这是数据集：（https://drive.google.com/file/d/1vTIiQ0NZKgBI-EfpGzfPKHx1VaAdEYdH/view?usp=sharing）
我和同学、老师讨论了这个问题。他们都说回归是用来预测的，但没人能解释他们是如何得出应该用回归来代替分类的结论的。]]></description>
      <guid>https://stackoverflow.com/questions/79013513/why-are-regression-algorithms-used-instead-of-classification-algorithms-datase</guid>
      <pubDate>Mon, 23 Sep 2024 07:10:30 GMT</pubDate>
    </item>
    <item>
      <title>在 nn.Transformer 中使用填充掩码时，损失返回为 Nan</title>
      <link>https://stackoverflow.com/questions/79013493/loss-is-returned-as-nan-when-using-padding-mask-in-nn-transformer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79013493/loss-is-returned-as-nan-when-using-padding-mask-in-nn-transformer</guid>
      <pubDate>Mon, 23 Sep 2024 07:04:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 从随机森林中获取观测权重</title>
      <link>https://stackoverflow.com/questions/79012909/obtaining-observation-weights-from-random-forests-using-r</link>
      <description><![CDATA[众所周知，随机森林的预测是 yi 观测值的加权平均值；即：
haty_j(x_j) = w_1(x_j)y_1 + ... + w_n(x_j)y_n, 
其中 x_j 是一个新的数据点，我想要预测其相关的独立变量 y_j。预测是 \haty_j。此外，(x_i, ~ y_i), i = 1, ..., n 是训练数据。我想要导出权重 w_i(x_j) 以执行其他分析。但是，R 只导出预测。我该如何获得这些权重？]]></description>
      <guid>https://stackoverflow.com/questions/79012909/obtaining-observation-weights-from-random-forests-using-r</guid>
      <pubDate>Mon, 23 Sep 2024 01:25:57 GMT</pubDate>
    </item>
    <item>
      <title>预处理 COCO2017 数据集时出错</title>
      <link>https://stackoverflow.com/questions/79012622/error-during-preprocessing-coco2017-dataset</link>
      <description><![CDATA[我正在使用 COCO2017 训练 mobilenetV2 来检测人。我在预处理数据集以将其更改为 Tensorflow 数据集时遇到了困难。当我设法进行更改时，它无法正确解析，导致我在执行 model.fit() 时出错。如何解决这个问题？
# 加载 COCO 数据集
(ds_train, ds_val), ds_info = tfds.load(
&#39;coco/2017&#39;,
split=[&#39;train[:80000]&#39;, &#39;validation&#39;],
with_info=True
)
IMG_SIZE = 224
NUM_CLASSES = 80 # 有效类别的数量，对于无效样本，另加一个

def preprocess(sample):
image = sample[&#39;image&#39;]

# 检查 &#39;objects&#39; 和 &#39;label&#39; 是否存在且有效
if &#39;objects&#39; in sample and &#39;label&#39; in sample[&#39;objects&#39;] and len(sample[&#39;objects&#39;][&#39;label&#39;]) &gt; 0:
label = tf.cast(sample[&#39;objects&#39;][&#39;label&#39;][0], tf.int64)
else:
# 为无效标签分配默认类（例如，额外的类 NUM_CLASSES）
label = tf.cast(NUM_CLASSES, tf.int64)

# 调整图像大小并进行预处理
image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
image = tf.keras.applications.mobilenet_v2.preprocess_input(image)

return image, label

# 应用预处理
ds_train = ds_train.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE)

我得到：
TypeError Traceback (most recent call last) Cell在[48]中，第 20 行 17 返回图像，标签 19 # 应用预处理 ---&gt; 20 ds_train = ds_train.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE) 21 ds_val = ds_val.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE) TypeError: 在用户代码中：

TypeError: outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.&lt;lambda&gt;() 需要 1 个位置参数，但给出了 2 个

我尝试了不同的预处理方法。基本上，我需要将输入图像设置为 224x224，以用于 mobilenetv2 模型。我尝试获取一个模型，以便可以在 Himax WE-I 上使用它。]]></description>
      <guid>https://stackoverflow.com/questions/79012622/error-during-preprocessing-coco2017-dataset</guid>
      <pubDate>Sun, 22 Sep 2024 21:09:52 GMT</pubDate>
    </item>
    <item>
      <title>如何使用不同的深度神经网络机器学习算法利用水稻作物图像预测甲烷排放量和用水量[关闭]</title>
      <link>https://stackoverflow.com/questions/79012121/how-to-predict-methane-emmision-and-water-usage-using-image-of-rice-crops-using</link>
      <description><![CDATA[我有水稻作物的图像，但我不知道如何使用深度神经网络的图像来预测甲烷排放量和用水量，而无需数据集的数值。我只有带土壤的水稻作物图像。
我尝试使用图像进行预测。我找不到预测甲烷排放量和用水量的正确代码。
我需要相同的代码]]></description>
      <guid>https://stackoverflow.com/questions/79012121/how-to-predict-methane-emmision-and-water-usage-using-image-of-rice-crops-using</guid>
      <pubDate>Sun, 22 Sep 2024 16:42:44 GMT</pubDate>
    </item>
    <item>
      <title>为什么即使我每次都初始化模型，训练损失也会不断减少？</title>
      <link>https://stackoverflow.com/questions/79011416/why-does-the-training-loss-keeps-decreasing-even-though-im-initialising-the-mod</link>
      <description><![CDATA[我试图重现 Neuromatch DL 课程中的这段摘录，在这里，我在 MNIST 图像数据集上训练一个多层感知器（在本例中没有隐藏层）。我注意到，每次运行整个代码时，初始损失和最终损失都在减少，在三次运行后从 (2.488, 0.0965) 减少到 (2.38,0.092)。
这是怎么发生的？由于 MLP 一次又一次地被初始化，权重不应该恢复为默认值吗？为什么模型拟合得更好？
我已将代码附在下面：
#加载数据集：
train_set, test_set = load_mnist_data(change_tensors=True) 

# 随机抽取 500 个索引的子集
subset_index = np.random.choice(len(train_set.data), 500)

# 我们将使用这些符号来表示训练数据和标签，以尽可能接近数学表达式。
X, y = train_set.data[subset_index, :], train_set.targets[subset_index]

loss_fn = F.nll_loss

cell_verbose = True # 仅用于切换是否打印损失

# 我相信，这是实际模型训练和优化开始的地方
partial_trained_model = MLP(in_dim=784, out_dim=10, hidden_​​dims=[])

if cell_verbose:
print(&#39;Init loss&#39;, loss_fn(partial_trained_model(X), y).item()) # 这与 np.log(10 = # of classes) 匹配

# 使用自适应梯度和动量调用优化器（有关此内容的更多信息，请参见第 7 节）
optimizer = optim.Adam(partial_trained_model.parameters(), lr=7e-4)
for i in range(200):
loss = loss_fn(partial_trained_model(X), y)
optimizer.zero_grad()
loss.backward()
optimizer.step()

if cell_verbose:
print(&#39;End loss&#39;, loss_fn(partial_trained_model(X), y).item()) # 这应该小于 1e-2

我添加了一个隐藏层，并且相同的特征仍然存在。]]></description>
      <guid>https://stackoverflow.com/questions/79011416/why-does-the-training-loss-keeps-decreasing-even-though-im-initialising-the-mod</guid>
      <pubDate>Sun, 22 Sep 2024 10:27:57 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用深度学习进行心脏扩大检测。是否可以使用自动编码器提取特征并将其作为集成模型的输入[关闭]</title>
      <link>https://stackoverflow.com/questions/79010975/i-am-doing-cardiomegaly-detection-using-deep-learning-is-it-possible-to-extract</link>
      <description><![CDATA[它复杂吗？结果会是什么样子？从概念上讲，是否可以使用自动编码器进行特征提取并将其作为二元分类任务中的集成模型的输入？
6.定义自动编码器模型
def build_autoencoder(input_shape):
inputs = Input(shape=input_shape)
# Encoder
x = Conv2D(32, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;)(inputs)
x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x)
x = Conv2D(64, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;)(x)
x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x)
x = Conv2D(128, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;)(x)
x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x)
x = Flatten()(x)
encoded = Dense(128,activation=&#39;relu&#39;)(x)

# 解码器
x = Dense(32 * 32 * 128,activation=&#39;relu&#39;)(encoded)
x = tf.keras.layers.Reshape((32, 32, 128))(x)
x = Conv2D(128, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
x = UpSampling2D((2, 2))(x)
x = Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
x = UpSampling2D((2, 2))(x)
x = Conv2D(32,(3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
# 调整此处的上采样以返回原始大小
x = UpSampling2D((2, 2))(x) # 从 (7,7) 更改为 (2,2)
decoded = Conv2D(3, (3, 3),activation=&#39;sigmoid&#39;,padding=&#39;same&#39;)(x)

autoencoder = Model(inputs,decoded)
encoder = Model(inputs,coded)

autoencoder.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;)

return autoencoder,encoder

/**ValueError Traceback (most recent call last)
&lt;ipython-input-17-3da45b43efb2&gt; in &lt;cell line: 37&gt;()
35 
36 # 训练自动编码器
---&gt; 37 autoencoder.fit(train_gen, epochs=10, validation_data=valid_gen)

1 帧
/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 中引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py in binary_crossentropy(target, output, from_logits)
673 for e1, e2 in zip(target.shape, output.shape):
674 如果 e1 不是 None 且 e2 不是 None 且 e1 != e2:
--&gt; 675 raise ValueError(
676 “参数 `target` 和 `output` 必须具有相同的形状。”
677 “收到：”

ValueError：参数 `target` 和 `output` 必须具有相同的形状。收到：target.shape=(None, 224, 224, 3)，output.shape=(None, 256, 256, 3)**/`在此处输入代码`
]]></description>
      <guid>https://stackoverflow.com/questions/79010975/i-am-doing-cardiomegaly-detection-using-deep-learning-is-it-possible-to-extract</guid>
      <pubDate>Sun, 22 Sep 2024 05:54:57 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：参数 clone_function 和 input_tensors 仅支持顺序模型或功能模型</title>
      <link>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</link>
      <description><![CDATA[我正在使用Quantization perceived training，参考网上的lstm代码，想把QAT放进lstm，结果遇到了ValueError。
ValueError Traceback (most recent call last)
&lt;ipython-input-11-00669bb76f9d&gt; in &lt;cell line: 6&gt;()
4 return layer
5 
----&gt; 6 annotated_model = tf.keras.models.clone_model(
7 model,
8 clone_function=apply_quantization_to_dense,

/usr/local/lib/python3.10/dist-packages/tf_keras/src/models/cloning.py in clone_model(model, input_tensors, clone_function)
544 # 自定义模型类的情况
545 if clone_function or input_tensors:
--&gt; 546 raise ValueError(
547 &quot;参数 clone_function 和 input_tensors &quot;
548 &quot;仅支持 Sequential 模型 &quot;

ValueError: 参数 clone_function 和 input_tensors 仅支持 Sequential 模型或 Functional 模型。收到类型为“Sequential”的模型，其中 clone_function=&lt;function apply_quantization_to_dense 位于0x78b727ec4040&gt; 和 input_tensors=None

这是我的代码
import keras
从 keras.layers 导入 LSTM
从 keras.layers 导入 Dense、Activation
从 keras.datasets 导入 mnist
从 keras.models 导入 Sequential
从 keras.optimizers 导入 Adam

learning_rate = 0.001
training_iters = 20
batch_size = 128
display_step = 10

n_input = 28
n_step = 28
n_hidden = 128
n_classes = 10

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(-1, n_step, n_input)
x_test = x_test.reshape(-1, n_step, n_input)
x_train = x_train.astype(&#39;float32&#39;)
x_test = x_test.astype(&#39;float32&#39;)
x_train /= 255
x_test /= 255

y_train = keras.utils.to_categorical(y_train, n_classes)
y_test = keras.utils.to_categorical(y_test, n_classes)

model = Sequential()
model.add(LSTM(n_hidden,
batch_input_shape=(None, n_step, n_input),
unroll=True))

model.add(Dense(n_classes))
model.add(Activation(&#39;softmax&#39;))

adam = Adam(lr=learning_rate)
model.summary()
model.compile(optimizer=adam,
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

model.fit(x_train, y_train,
batch_size=batch_size,
epochs=training_iters,
verbose=1,
validation_data=(x_test, y_test))

scores = model.evaluate(x_test, y_test, verbose=0)
print(&#39;LSTM 测试分数：&#39;, scores[0])
print(&#39;LSTM 测试准确率：&#39;, scores[1])

def apply_quantization_to_dense(layer):
if isinstance(layer, tf.keras.layers.LSTM):
return tfmot.quantization.keras.quantize_annotate_layer(layer)
return layer

annotated_model = tf.keras.models.clone_model(
模型，
clone_function=apply_quantization_to_dense，
)
]]></description>
      <guid>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</guid>
      <pubDate>Fri, 26 Jul 2024 03:41:57 GMT</pubDate>
    </item>
    <item>
      <title>希望使用 Google ML 套件识别面部类型</title>
      <link>https://stackoverflow.com/questions/75714844/looking-to-identify-face-types-using-google-ml-kit</link>
      <description><![CDATA[我想使用 Google ML 找出一个人的脸型，比如方形、椭圆形或圆形。我知道我需要从不同角度测量脸的长度和宽度来确定脸型。但是，API 并没有给我提供这些。任何输入都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/75714844/looking-to-identify-face-types-using-google-ml-kit</guid>
      <pubDate>Sun, 12 Mar 2023 17:03:06 GMT</pubDate>
    </item>
    <item>
      <title>Detectron2 - 提取阈值区域特征以进行物体检测</title>
      <link>https://stackoverflow.com/questions/62442039/detectron2-extract-region-features-at-a-threshold-for-object-detection</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/62442039/detectron2-extract-region-features-at-a-threshold-for-object-detection</guid>
      <pubDate>Thu, 18 Jun 2020 03:47:25 GMT</pubDate>
    </item>
    <item>
      <title>随机森林比线性回归差。这是正常的吗？原因是什么？</title>
      <link>https://stackoverflow.com/questions/48087676/random-forest-is-worse-than-linear-regression-is-it-normal-and-what-is-the-reas</link>
      <description><![CDATA[我正在尝试使用机器学习来预测数据集。这是一个具有 180 个输入特征和 1 个连续值输出的回归问题。我尝试比较神经网络、随机森林回归和线性回归。
正如我所料，3 隐藏层神经网络的表现优于其他两种方法，均方根误差 (RMSE) 为 0.1。然而，我意外地发现随机森林的表现甚至比线性回归更差（RMSE 0.29 vs. 0.27）。正如我所料，随机森林可以发现特征之间更复杂的依赖关系以减少错误。我尝试调整随机森林的参数（树数、最大特征、max_depth 等）。我也尝试了不同的 K 交叉验证，但性能仍然不如线性回归。
我在网上搜索了一下，一个答案说，如果特征对协变量具有平滑、近乎线性的依赖性，线性回归可能会表现得更好。我没有完全理解这一点，因为如果是这样的话，深度神经网络不应该带来很大的性能提升吗？
我很难给出解释。在什么情况下，随机森林比线性回归更差，但深度神经网络可以表现得更好？]]></description>
      <guid>https://stackoverflow.com/questions/48087676/random-forest-is-worse-than-linear-regression-is-it-normal-and-what-is-the-reas</guid>
      <pubDate>Thu, 04 Jan 2018 01:58:35 GMT</pubDate>
    </item>
    <item>
      <title>不同分类器的 TPR 和 FPR 曲线 - R 中的 kNN、NaiveBayes、决策树</title>
      <link>https://stackoverflow.com/questions/34335074/tpr-fpr-curve-for-different-classifiers-knn-naivebayes-decision-trees-in-r</link>
      <description><![CDATA[我正在尝试理解并绘制不同类型分类器的 TPR/FPR。我在 R 中使用 kNN、NaiveBayes 和决策树。使用 kNN，我执行以下操作：
clnum &lt;- as.vector(diabetes.trainingLabels[,1], mode = &quot;numeric&quot;)
dpknn &lt;- knn(train = diabetes.training, test = diabetes.testing, cl = clnum, k=11, prob = TRUE)
prob &lt;- attr(dpknn, &quot;prob&quot;)
tstnum &lt;- as.vector(diabetes.testingLabels[,1], mode = &quot;numeric&quot;)
pred_knn &lt;- prediction(prob, tstnum)
pred_knn &lt;- performance(pred_knn, &quot;tpr&quot;, &quot;fpr&quot;)
plot(pred_knn, avg= &quot;threshold&quot;, colorize=TRUE, lwd=3, main=&quot;Knn=11 的 ROC 曲线&quot;)

其中 diabetes.trainingLabels[,1] 是我想要预测的标签（类）向量，diabetes.training 是训练数据，diabetes.testing 是测试数据。
该图如下所示：

存储在 prob 属性中的值是一个数字向量（0 到 1 之间的小数）。我将类标签因子转换为数字，然后可以将其与 ROCR 库中的预测/性能函数一起使用。不能 100% 确定我做得对，但至少它有效。
但是对于 NaiveBayes 和决策树，在预测函数中指定 prob/raw 参数，我得到的不是单个数字向量，而是一个列表向量或矩阵，其中指定了每个类的概率（我猜），例如：
diabetes.model &lt;- naiveBayes(class ~ ., data = diabetesTrainset)
diabetes.predicted &lt;- predict(diabetes.model, diabetesTestset, type=&quot;raw&quot;)

并且 diabetes.predicted 是：
tested_negative checked_positive
[1,] 5.787252e-03 0.9942127
[2,] 8.433584e-01 0.1566416
[3,] 7.880800e-09 1.0000000
[4,] 7.568920e-01 0.2431080
[5,] 4.663958e-01 0.5336042

问题是如何使用它来绘制 ROC 曲线，以及为什么在 kNN 中我得到一个向量，而对于其他分类器，我得到两个类别的向量是分开的？]]></description>
      <guid>https://stackoverflow.com/questions/34335074/tpr-fpr-curve-for-different-classifiers-knn-naivebayes-decision-trees-in-r</guid>
      <pubDate>Thu, 17 Dec 2015 12:51:31 GMT</pubDate>
    </item>
    </channel>
</rss>