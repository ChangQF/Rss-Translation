<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 19 Sep 2024 01:13:45 GMT</lastBuildDate>
    <item>
      <title>CustomScaler 类型的对象未在 gurobi_ml 中注册/支持</title>
      <link>https://stackoverflow.com/questions/79000471/object-of-type-customscaler-is-not-registered-supported-with-gurobi-ml</link>
      <description><![CDATA[我在 sklearn.pipeline.make_pipeline 中构建了一个 CustomScaler 来缩放我的数据。它包含 transform 和 inversetransform 函数。当我使用 gurobipy 优化回归模型时，我得到了上述错误。Gurobi 文档中没有明确提到他们不接受 CustomScaler，但提到他们接受 StandardScalar 的很多地方。我是否需要使用不同的方法来制定我的问题，或者我可以在 gurobipy_ml 中使用 CustomScalar？我想绝对使用 gurobi 框架，因为它具有商业可行性和专业经验。
我采用了最简单的方法，使用 StandardScalar，但用作 CustomeScalar，并得到 NotRegistred：CustomScaler 类型的对象未在 gurobi_ml 中注册/支持。
class CustomScaler(BaseEstimator, TransformerMixin):
def __init__(self):
self.scaler = StandardScaler()

def fit(self, X, y=None):
return self.scaler.fit(X)

def transform(self, X):
return self.scaler.transform(X)

def inverse_transform(self, X):
return self.scaler.inverse_transform(X)
]]></description>
      <guid>https://stackoverflow.com/questions/79000471/object-of-type-customscaler-is-not-registered-supported-with-gurobi-ml</guid>
      <pubDate>Wed, 18 Sep 2024 23:50:16 GMT</pubDate>
    </item>
    <item>
      <title>无法从 Pytorch Dataset 的 __get_item__ 返回布尔变量</title>
      <link>https://stackoverflow.com/questions/79000230/unable-to-return-a-boolean-variable-from-pytorch-datasets-get-item</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79000230/unable-to-return-a-boolean-variable-from-pytorch-datasets-get-item</guid>
      <pubDate>Wed, 18 Sep 2024 21:33:47 GMT</pubDate>
    </item>
    <item>
      <title>Jupyter 笔记本无法通过 Anaconda Navigator 打开，尝试打开 Jupyter 笔记本时出现以下错误，请提供解决方案</title>
      <link>https://stackoverflow.com/questions/78999330/jupyter-notebook-is-not-opening-via-anaconda-navigator-and-following-error-is-th</link>
      <description><![CDATA[[W 2024-09-18 21:59:36.813 ServerApp] 在 jupyter_lsp 中未找到 _jupyter_server_extension_points 函数。相反，找到了 _jupyter_server_extension_paths 函数，目前将使用该函数。此函数名称将在 Jupyter Server 的未来版本中弃用。
[W 2024-09-18 21:59:36.996 ServerApp] 在 notebook_shim 中未找到 _jupyter_server_extension_points 函数。相反，找到了 _jupyter_server_extension_paths 函数，目前将使用该函数。此函数名称将在 Jupyter Server 的未来版本中弃用。
[I 2024-09-18 21:59:38.680 ServerApp] 扩展包 panel.io.jupyter_server_extension 导入耗时 1.6916 秒
[I 2024-09-18 21:59:38.680 ServerApp] jupyter_lsp | 扩展已成功链接。
[I 2024-09-18 21:59:38.696 ServerApp] jupyter_server_terminals | 扩展已成功链接。
[I 2024-09-18 21:59:38.710 ServerApp] jupyterlab | 扩展已成功链接。
[I 2024-09-18 21:59:38.711 ServerApp] notebook | 扩展已成功链接。
[I 2024-09-18 21:59:39.355 ServerApp] notebook_shim | 扩展已成功链接。
[I 2024-09-18 21:59:39.355 ServerApp] panel.io.jupyter_server_extension | 扩展已成功链接。
[I 2024-09-18 21:59:39.428 ServerApp] notebook_shim | 扩展已成功加载。
[I 2024-09-18 21:59:39.443 ServerApp] jupyter_lsp | 扩展已成功加载。
[I 2024-09-18 21:59:39.443 ServerApp] jupyter_server_terminals | 扩展已成功加载。
[I 2024-09-18 21:59:39.457 LabApp] JupyterLab 扩展已从 C:\ProgramData\anaconda3\Lib\site-packages\jupyterlab 加载
[I 2024-09-18 21:59:39.458 LabApp] JupyterLab 应用程序目录为 C:\ProgramData\anaconda3\share\jupyter\lab
[I 2024-09-18 21:59:39.459 LabApp] 扩展管理器为“pypi”。
[I 2024-09-18 21:59:39.461 ServerApp] jupyterlab | 扩展已成功加载。
[I 2024-09-18 21:59:39.470 ServerApp] notebook | 扩展已成功加载。
[I 2024-09-18 21:59:39.470 ServerApp] panel.io.jupyter_server_extension | 扩展已成功加载。
[I 2024-09-18 21:59:39.474 ServerApp] 从本地目录提供笔记本：C:\Users\Avi
[I 2024-09-18 21:59:39.474 ServerApp] Jupyter Server 2.14.1 正在运行：
[I 2024-09-18 21:59:39.474 ServerApp] http://localhost:8888/tree?token=435b978e976c033e2b96cb8ce465e12d2ec469809cf2ab35
[I 2024-09-18 21:59:39.474 ServerApp] http://127.0.0.1:8888/tree?token=435b978e976c033e2b96cb8ce465e12d2ec469809cf2ab35
[I 2024-09-18 21:59:39.474 ServerApp] 使用 Control-C 停止此服务器并关闭所有内核（两次以跳过确认）。
[E 2024-09-18 21:59:39.475 ServerApp] 无法将服务器信息写入 C:\Users\Avi\AppData\Roaming\jupyter\runtime\jpserver-3148.json：PermissionError(13, &#39;权限被拒绝&#39;)
回溯（最近一次调用）：
文件“C:\ProgramData\anaconda3\Scripts\jupyter-notebook-script.py”，第 10 行，位于
sys.exit(main())
^^^^^^
文件“C:\ProgramData\anaconda3\Lib\site-packages\jupyter_server\extension\application.py”，第 623 行，位于 launch_instance
serverapp.start()
文件“C:\ProgramData\anaconda3\Lib\site-packages\jupyter_server\serverapp.py”，第3119，在 start 中
self.start_app()
文件“C:\ProgramData\anaconda3\Lib\site-packages\jupyter_server\serverapp.py”，第 3023 行，在 start_app 中
self.write_browser_open_files()
文件“C:\ProgramData\anaconda3\Lib\site-packages\jupyter_server\serverapp.py”，第 2890 行，在 write_browser_open_files 中
self.write_browser_open_file()
文件“C:\ProgramData\anaconda3\Lib\site-packages\jupyter_server\serverapp.py”，第 2913 行，在 write_browser_open_file 中
使用 open(self.browser_open_file, “w”, encoding=“utf-8”) 作为f:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
PermissionError：[Errno 13] 权限被拒绝：&#39;C:\Users\Avi\AppData\Roaming\jupyter\runtime\jpserver-3148-open.html&#39;
我尝试通过 Anaconda Navigator 打开 Jupyter Notebook，但在打开应用程序时遇到问题，此外，我想让您知道，安装后 Jupyter Notebook 可以打开，但关闭 Windows 后无法打开。]]></description>
      <guid>https://stackoverflow.com/questions/78999330/jupyter-notebook-is-not-opening-via-anaconda-navigator-and-following-error-is-th</guid>
      <pubDate>Wed, 18 Sep 2024 16:41:36 GMT</pubDate>
    </item>
    <item>
      <title>我应该在 XGBoost 中对具有不平衡类别的分区模型进行集合或平均 SHAP 值吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78997484/should-i-ensemble-or-average-shap-values-across-partitioned-models-with-imbalanc</link>
      <description><![CDATA[我使用 XGBoost 构建了一个分类模型，但我的类别高度不平衡。我有大约 2,000 个正类实例和 130,000 个负类实例。我没有对负类进行欠采样以匹配正类的大小，而是决定将 130,000 个实例划分为 65 个子集以避免数据丢失。我还使用 SHAP 值和增益（即特征为其贡献的分支带来的准确度改进）来解释特征重要性。我的目标是根据这两个指标比较特征重要性。
但是，我不确定是否应该在 65 个模型中应用模型集成以获得平均性能，还是独立运行 65 个模型，然后计算平均 SHAP 和增益值。
我看到 StackOverflow 和 GitHub 上的讨论表明可以对模型之间的 SHAP 值进行平均，但前提是样本相同。由于我的分区中的负类不同，因此我不确定在模型之间比较 SHAP 值是否仍然合理。]]></description>
      <guid>https://stackoverflow.com/questions/78997484/should-i-ensemble-or-average-shap-values-across-partitioned-models-with-imbalanc</guid>
      <pubDate>Wed, 18 Sep 2024 09:27:31 GMT</pubDate>
    </item>
    <item>
      <title>损失偏差巨大原因何在？[关闭]</title>
      <link>https://stackoverflow.com/questions/78997392/what-are-the-reasons-for-the-huge-deviation-of-loss</link>
      <description><![CDATA[我的loss
我的loss函数
这是我的loss值，我觉得它看起来不稳定。
请问loss偏差这么大的原因是什么？
可能是loss函数的设计问题？还是数据集的质量问题？
非常感谢您的回答。]]></description>
      <guid>https://stackoverflow.com/questions/78997392/what-are-the-reasons-for-the-huge-deviation-of-loss</guid>
      <pubDate>Wed, 18 Sep 2024 09:05:41 GMT</pubDate>
    </item>
    <item>
      <title>无法从移动设备访问 Flask 服务器：图像分类问题</title>
      <link>https://stackoverflow.com/questions/78997111/flask-server-not-accessible-from-mobile-device-image-classification-issue</link>
      <description><![CDATA[我的本​​地机器上运行着一个 Flask 服务器，它被设置为使用预先训练的 TensorFlow 模型对图像进行分类。Flask 服务器在我的计算机上成功运行，并且可以在从同一台机器上的模拟器访问时处理请求。但是，当我尝试从移动设备访问 Flask 服务器以对图像进行分类时，我遇到了问题。
Flask 服务器代码：
if __name__ == &#39;__main__&#39;:
app.run(debug=True, host=&#39;0.0.0.0&#39;, port=5000)

在 Flutter 中，我将发送图像的 URL 设置为“http://&lt;my_computer_ip&gt;:5000/predict”。
当我尝试从移动应用程序向 Flask 服务器发送图像时，我遇到了以下问题：
连接超时：请求超时而未到达 Flask 服务器

但是，我没有在 Flask 服务器控制台上收到相关日志。Flask 服务器在 0.0.0.0 上运行，应该可以从同一网络上的其他设备访问。当我尝试将图像从移动设备发送到本地计算机上运行的 Flask 服务器时，如何解决连接超时问题？]]></description>
      <guid>https://stackoverflow.com/questions/78997111/flask-server-not-accessible-from-mobile-device-image-classification-issue</guid>
      <pubDate>Wed, 18 Sep 2024 07:55:14 GMT</pubDate>
    </item>
    <item>
      <title>面临 MLC-LLM Android 应用程序构建问题</title>
      <link>https://stackoverflow.com/questions/78996973/facing-build-issue-in-the-mlc-llm-android-app</link>
      <description><![CDATA[我尝试使用以下链接运行 MLC-LLM Android 应用程序，但由于以下屏幕截图中显示的错误，我无法运行该应用程序：
Git 存储库

我已克隆并运行 MLC-AI/MLC-LLC，但它没有运行，我得到了它，帮我解决这个问题。
有人在 android 上运行过这个吗，请帮我解决这个问题。
在 android 目录中，有 3 个文件夹：

mlc4j
MLCChat
MLCEngineExample

我们需要使用哪一个？我该如何使用它？]]></description>
      <guid>https://stackoverflow.com/questions/78996973/facing-build-issue-in-the-mlc-llm-android-app</guid>
      <pubDate>Wed, 18 Sep 2024 07:17:43 GMT</pubDate>
    </item>
    <item>
      <title>ImportError: 导入 o​​nnx_cpp2py_export 时 DLL 加载失败：动态链接库 (DLL) 初始化例程失败</title>
      <link>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</guid>
      <pubDate>Wed, 18 Sep 2024 07:08:40 GMT</pubDate>
    </item>
    <item>
      <title>如何利用 writer.add_image 将图像添加到 tensorboard 记录器？</title>
      <link>https://stackoverflow.com/questions/78996316/how-to-add-images-to-tensorboard-logger-leveraging-writer-add-image</link>
      <description><![CDATA[我有一个通道图像，想使用 writer.add_image 将它们添加到 tensorboard 记录器中，如下所示：
 generated_images_to_show = torch.cat(gtimage_cpu, predimage_cpu), dim=0)
gen_image_grid = make_grid(generated_images_to_show .unsqueeze(1), nrow=inputs.size(0), padding=6, normalize=False)
writer.add_image(&#39;Validation Gen Comparison&#39;, gen_image_grid, epoch)

我生成的图像有一个通道，我需要使用 cmap= &#39;hsv&#39; 绘制它们，但 add_image 中没有可用的 cmap 功能。在这种情况下，我该怎么办？我只想在 tensorboard 中查看结果。
一种解决方案是将图像转换为 RGB 通道，然后使用 cmap=hsv 并将转换后的图像记录到 tensorboard，还有其他（也许更有效的）解决方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/78996316/how-to-add-images-to-tensorboard-logger-leveraging-writer-add-image</guid>
      <pubDate>Wed, 18 Sep 2024 01:48:03 GMT</pubDate>
    </item>
    <item>
      <title>如何重新训练 ML 模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78996028/how-to-re-train-ml-model</link>
      <description><![CDATA[我正在使用 AWS Sagemaker，用一些带标签的数据训练了一个模型，将其部署到终端并设置 Lambda 以提供预测。
一切都很好，但我想定期重新训练我的模型，例如使用 1 周的历史数据。
但我的历史数据没有标签，这意味着它不能用于训练。我该如何标记它？
我最初认为我可以使用我的模型的预测来标记新的（未标记的）数据，但我读到这不是一个好主意，因为它只会确保我的模型的准确性，即使它可能远非准确。
那么我在哪里可以获得我的历史数据的标签？
如果历史数据不能通过模型​​标记，那么是否意味着它应该手动标记？那么，训练和使用模型的意义何在？
举个例子，我们来检测一下欺诈交易。好吧，有一些初始数据，由某个确切知道交易是否欺诈的人手动标记，因此准确率是 100%。
那么是否应该定期手动更新额外的 100% 准确事件？]]></description>
      <guid>https://stackoverflow.com/questions/78996028/how-to-re-train-ml-model</guid>
      <pubDate>Tue, 17 Sep 2024 22:44:46 GMT</pubDate>
    </item>
    <item>
      <title>生存分析的校准图</title>
      <link>https://stackoverflow.com/questions/78995759/calibration-plots-for-survival-analysis</link>
      <description><![CDATA[我无法为我的生存分析项目创建校准图。（食管癌数据集）
经过调整后，我已经完成了我的模型（AORSF）：
aorsf_fit &lt;- last_fit(
final_aorsf_wf,
split = initial_split(final_main, prop = 0.75),
metrics = survivor_metrics,
eval_time = time_points_complete,
)

然后我使用以下代码收集我的预测集：
&gt; predictions &lt;- collect_predictions(aorsf_fit)
&gt; predictions 
# A tibble: 687 × 6
.pred .pred_time id .row surv .config 
&lt;list&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;Surv&gt; &lt;chr&gt; 
1 &lt;tibble [8 × 3]&gt; 107. 训练/测试分割 3 57.8+ Preprocessor1_Model1
2 &lt;tibble [8 × 3]&gt; 96.1 训练/测试分割 7 96.8+ Preprocessor1_Model1
3 &lt;tibble [8 × 3]&gt; 94.2 训练/测试分割 11 130.0 Preprocessor1_Model1
4 &lt;tibble [8 × 3]&gt; 11.4 训练/测试分割 15 9.0 Preprocessor1_Model1
5 &lt;tibble [8 × 3]&gt; 102. 训练/测试分割 16 69.1+ Preprocessor1_Model1
6 &lt;tibble [8 × 3]&gt; 37.8 训练/测试分割 17 33.0 Preprocessor1_Model1
7 &lt;tibble [8 × 3]&gt; 103. 训练/测试分割 18 142.8 Preprocessor1_Model1
8 &lt;tibble [8 × 3]&gt; 23.4 训练/测试分割 20 13.5 Preprocessor1_Model1
9 &lt;tibble [8 × 3]&gt; 89.7 训练/测试分割 21 146.5 Preprocessor1_Model1
10 &lt;tibble [8 × 3]&gt; 107. 训练/测试分割 23 60.1+ Preprocessor1_Model1
# ℹ 677 更多行

我甚至能够获得时间相关的 ROC 曲线（感谢 roc_curve_survival() 函数）：
predictions |&gt; 
roc_curve_survival(truth = surv, .pred)|&gt; 
filter(.eval_time == 60) |&gt; 
ggplot(aes(1 - specificity,sensitive)) +
geom_line() +
theme_minimal()

但是，我无法使用“.pred”列表创建校准图。我尝试使用链接中建议的代码：使用 tidymodels 进行校准的简介
我希望能够使用“predictions”对象中的预测生存数据来构建校准图。predictions 对象中的“.pred”变量包含以下 3 行：
.eval_time
.pr​​ed_survival
.weight_censored
我确信，如果以某种方式提取，它可以用于创建校准图（如链接中所示），然后可以用来显示模型相对于观察到的生存率的运行情况！（我也观察到了我的数据集的生存率）
我只尝试了以下方法，但没有结果：
predictions |&gt; 
+ ggplot(aes(.pred_survival))
]]></description>
      <guid>https://stackoverflow.com/questions/78995759/calibration-plots-for-survival-analysis</guid>
      <pubDate>Tue, 17 Sep 2024 20:49:44 GMT</pubDate>
    </item>
    <item>
      <title>对多个相互交织的目标使用反向预测[关闭]</title>
      <link>https://stackoverflow.com/questions/78992262/using-reverse-prediction-for-multiple-intertwined-targets</link>
      <description><![CDATA[我有一个人生阶段预测模型，它根据一组规则为每个潜在的人生阶段分配点数，总点数最高的人生阶段被视为赢家。这种无监督模型在识别样本客户是青少年、单身、夫妻还是有年幼的孩子等方面效果很好。
在每次模型运行结束时，我都会总结结果，以获得每个人生阶段内的客户比例。该模型每三个月运行一次，如果不进行补救，每次的最终比例都会与目标比例相差太大。目前，我使用反复试验来重新平衡比例，通过在每个人生阶段增加或减去少量点数，将边缘客户转移到另一个人生阶段类别。
我想要的是一种自动化方法，可以指定重新校准最终比例所需的点数，以比我的手动方法更轻松（更少的迭代次数）地匹配目标比例。本质上，协调点 (recon_pts) 是模型的输入，预测每个客户的生命阶段是关键输出，同时还有相应的比例 (model_pn)。我希望模型比例最终与目标比例 (target_pn) 相匹配。
一个例子肯定有助于说明我所追求的。请注意，以下是实际观察到的结果，但我不想要使用或输出这些精确数字的工具，而是使用机器学习来更有效地重现我的反复试验方法。
假设起始位置为：

提供者：
life_stage=c(&#39;CHD&#39;, &#39;TNG&#39;, &#39;TWS&#39;, &#39;SGL&#39;, &#39;CPL&#39;, &#39;YGF&#39;, &#39;PTF&#39;, &#39;OLF&#39;, &#39;ENR&#39;),
recon_pts=c(0.215, -0.143, -0.086, 0.024, -0.049, -0.079, -0.14, -0.162, 0.083),
model_pn=c(0.012, 0.087, 0.091, 0.065, 0.113, 0.115, 0.123, 0.122, 0.273),
target_pn=c(0.014, 0.091, 0.095, 0.065, 0.114, 0.114, 0.107, 0.122, 0.277))

CHD 生命阶段比例过低 (1.2%)，低于目标 (1.4%)。在另一个极端，PTF 太高 (12.3%)，因为它高于目标 (10.7%)。
在我的手动方法下，我选择将 CHD 的 recon_pts 从 0.215 增加到 0.22，这样我就能在这个生命阶段获得更多客户。同时，我还将 PTF 的分数从 -0.14 减少到 -0.17，这样分配到这里的客户就会减少。
然后我得到了这些结果，所有比例都经过了调整。 CHD 比例更接近目标但仍然太低，PTF 现在太低，因为比例已低于目标，其他所有方面也都需要注意：

因此，在下一次迭代中，我可能会选择增加 CHD 的 recon_pts，增加 PTF 的点数并减少 YFG 的点数。然后它就以一种极其缓慢的方式继续下去了！
对我来说，这看起来像是机器学习的理想候选者，其中的过程可以学习如何调整 recon_pts，以便 model_pn 最终匹配每个生命阶段的 target_pn。
如何在 R 中有效地编码？]]></description>
      <guid>https://stackoverflow.com/questions/78992262/using-reverse-prediction-for-multiple-intertwined-targets</guid>
      <pubDate>Tue, 17 Sep 2024 01:29:30 GMT</pubDate>
    </item>
    <item>
      <title>将safetensors模型格式（LLaVA模型）转换为gguf格式</title>
      <link>https://stackoverflow.com/questions/78763327/convert-safetensors-model-formatllava-model-into-gguf-format</link>
      <description><![CDATA[我想在 ollama 中进行 LLaVA 推理，因此我需要将其转换为 gguf 文件格式。
我的模型具有文件格式 safetensors。（使用 lora 训练）
似乎 ollama 仅支持 llama，但不支持 llava，如下所示，
https://github.com/ollama/ollama/blob/main/docs/import.md
我遵循了 llama.cpp 的说明，并在此处使用了代码 convert_lora_to_gguf.py，
https://github.com/ggerganov/llama.cpp/blob/master/convert_lora_to_gguf.py
但是我收到如下错误：
ERROR:lora-to-gguf:不支持 Model LlavaLlamaForCausalLM

如果我在模型文件的 config.json 中写入 llama 模型并运行以下代码，则会收到另一个错误。
model_instance.gguf_writer.add_string(gguf.Keys.General.TYPE, gguf.GGUFType.ADAPTER)
model_instance.gguf_writer.add_string(gguf.Keys.Adapter.TYPE, &quot;lora&quot;)
model_instance.gguf_writer.add_float32(gguf.Keys.Adapter.LORA_ALPHA, float(alpha))
model_instance.gguf_writer.add_quantization_version(gguf.GGML_QUANT_VERSION)
logger.info(&quot;Exporting model...&quot;)
model_instance.write()
logger.info(f&quot;模型已成功导出至 {model_instance.fname_out}&quot;)

Traceback (most recent call last):
File &quot;C:\Users\jjjy2\OneDrive\Desktop\VLM_FastAPI\ollama\convert_lora_to_gguf.py&quot;, line 373, in &lt;module&gt;
model_instance.gguf_writer.add_string(gguf.Keys.General.FILE_TYPE, gguf.GGUFType.ADAPTER)
AttributeError: module &#39;gguf&#39; has no attribute &#39;GGUFType&#39;

似乎所有代码和 gguf 包都不支持 llava，只支持 llama。我必须将我自己训练的模型转换为 gguf。我无法使用 hugging face 的 gguf llava 模型进行推理。
有没有办法转换它？]]></description>
      <guid>https://stackoverflow.com/questions/78763327/convert-safetensors-model-formatllava-model-into-gguf-format</guid>
      <pubDate>Thu, 18 Jul 2024 08:47:53 GMT</pubDate>
    </item>
    <item>
      <title>文本分割器输出不可 JSON 序列化</title>
      <link>https://stackoverflow.com/questions/76890207/text-splitter-output-is-not-json-serializable</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76890207/text-splitter-output-is-not-json-serializable</guid>
      <pubDate>Sat, 12 Aug 2023 16:34:50 GMT</pubDate>
    </item>
    <item>
      <title>如何处理一列包含图像名称、另一列包含图像路径的 csv 文件数据集？</title>
      <link>https://stackoverflow.com/questions/72773807/how-to-handle-dataset-which-is-a-csv-file-that-contains-image-names-in-one-colum</link>
      <description><![CDATA[我是 Python 和机器学习的新手。我只是在练习模型训练和数据集。我偶然发现了这个数据集，它有测试和训练文件夹。该文件夹中有几个包含不同图像的文件（这是一个乐器数据集，因此每个乐器都按名称分类在不同的文件夹中）。csv 文件包含乐器的名称及其在文件夹中的路径，如下所示：Instrument.csv
现在我的问题是如何处理这个数据集？我应该遍历训练和测试文件夹还是使用这个 csv 文件？
如果我想选择文件夹选项，那么如何遍历每个子文件夹并访问图像？
这是数据集的链接：https://www.kaggle.com/datasets/gpiosenka/musical-instruments-image-classification
如果问题没有任何意义或太容易回答，我很抱歉。我承认我是菜鸟]]></description>
      <guid>https://stackoverflow.com/questions/72773807/how-to-handle-dataset-which-is-a-csv-file-that-contains-image-names-in-one-colum</guid>
      <pubDate>Mon, 27 Jun 2022 14:30:58 GMT</pubDate>
    </item>
    </channel>
</rss>