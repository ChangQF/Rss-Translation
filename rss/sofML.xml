<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 13 Jan 2024 12:24:24 GMT</lastBuildDate>
    <item>
      <title>人工神经网络 (ANN) 测试存在问题，所有预测都具有相同的值，没有变化</title>
      <link>https://stackoverflow.com/questions/77811384/problem-with-artificial-neural-network-ann-testing-all-predictions-have-the-s</link>
      <description><![CDATA[这是我的代码：
拟合神经网络
ANN_1 &lt;- 神经网络(as.factor(redeemer_latest_ind) ~ 总购买金额 + 购买频率 +
centre_purchase_indicator + customer_tenure +
int_pf_ct，
数据=训练集，
隐藏=c(4,2)，stepmax=1e+06，act.fct =“逻辑”，
线性.输出 = FALSE,
）
绘图(ANN_1)
获取模型预测
temp_test &lt;- 子集(test_set, select = c(“total_purchase_amount”, “purchase_Frequency”, “recent_purchase_indicator”, “customer_tenure”, “int_pf_ct”))
头（临时测试）
nn.结果 &lt;- 预测(ANN_1, temp_test)
nn.results &lt;- nn.results$net.result
当我查看 nn 结果时，我只看到所有预测的值相同。
[874] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[883] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[892] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[901] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[910] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[919] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[928] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[937] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[946] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[955] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[964] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[973] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[982] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[991] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[1000] 0.3115995
如何解决这个问题？我还想添加交叉验证，就像我测试的其他模型一样，这在我的代码中称为 cv_5。但最重要的是获得预测的一些变化，因为我需要它们来测试命中率、最高十分位数提升和基尼系数。
以下是我的火车数据库中的一些见解：
在此处输入图像描述
我已经尝试了不同的最大步长和不同的隐藏层。]]></description>
      <guid>https://stackoverflow.com/questions/77811384/problem-with-artificial-neural-network-ann-testing-all-predictions-have-the-s</guid>
      <pubDate>Sat, 13 Jan 2024 12:15:40 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归值与预期相差不大</title>
      <link>https://stackoverflow.com/questions/77811361/logistic-regression-values-dont-differ-that-much-as-expected</link>
      <description><![CDATA[我想使用 5 折交叉验证方法在数据集中执行逻辑回归（预处理后）。然后计算并打印指标 Accuracy、F1-Score、Gmean 和 Fit time。
然后使用 MinMax Normalization 重复实验，并再次使用特征标准化。
我写的这段代码正确吗？
我只是没有看到标准化方法之间的分数有那么不同，而且我不知道它们是否有效。
将 numpy 导入为 np
将 pandas 导入为 pd
从sklearn.model_selection导入train_test_split，cross_validate，KFold
从sklearn导入线性模型
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.metrics 导入 precision_score,f1_score,make_scorer
从 imblearn.metrics 导入几何平均分数

data_url =“jm1.csv”；
df = pd.read_csv(data_url, sep=&quot;,&quot;)
df.replace(&#39;?&#39;, pd.NA, inplace=True)
df.dropna(就地=True)
df[&#39;缺陷&#39;] = df[&#39;缺陷&#39;].astype(int)
X=df.drop(&#39;缺陷&#39;, axis=1)
y=df[&#39;缺陷&#39;]

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=13,stratify=y)
模型 = Linear_model.LogisticRegression(solver=&#39;liblinear&#39;,max_iter=500)
model.fit(X_train, y_train)
y_pr=模型.预测(X_test)

kfold=KFold(n_splits=5,shuffle=True,random_state=13)
Scorers={&#39;accuracy_score&#39;: make_scorer(accuracy_score),&#39;F1_score&#39;: make_scorer(f1_score,average=&#39;weighted&#39;),&#39;G-Mean&#39;: make_scorer(geometric_mean_score)}
cv_results=cross_validate(模型,X_train,y_train,cv=kfold,scoring=scorers,return_train_score=True)
print(&quot;准确率：&quot;,cv_results[&#39;test_accuracy_score&#39;].mean())
print(&quot;F1 分数：&quot;, cv_results[&#39;test_F1_score&#39;].mean())
print(&quot;G-Mean 分数：&quot;, cv_results[&#39;test_G-Mean&#39;].mean())
print(&quot;拟合时间：&quot;, cv_results[&#39;fit_time&#39;].mean())

从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.pipeline 导入管道
管道 = 管道([(&#39;scaler&#39;, MinMaxScaler()), (&#39;逻辑&#39;, LogisticRegression(solver=&#39;liblinear&#39;))])
cv_results_minmax= cross_validate(管道, X, y, cv=kfold, 评分=得分者, return_train_score=True)
print(&quot;准确率：&quot;,cv_results_minmax[&#39;test_accuracy_score&#39;].mean())
print(&quot;F1 分数：&quot;, cv_results_minmax[&#39;test_F1_score&#39;].mean())
print(&quot;G-Mean 分数：&quot;, cv_results_minmax[&#39;test_G-Mean&#39;].mean())
print(&quot;拟合时间：&quot;, cv_results_minmax[&#39;fit_time&#39;].mean())

从 sklearn.preprocessing 导入 StandardScaler
管道 = Pipeline([(&#39;scaler&#39;, StandardScaler()), (&#39;logistic&#39;, LogisticRegression(solver=&#39;liblinear&#39;))])
cv_results_stand = cross_validate（管道，X，y，cv = kfold，评分=得分者，return_train_score = True）
print(&quot;准确率：&quot;,cv_results_stand[&#39;test_accuracy_score&#39;].mean())
print(&quot;F1 分数：&quot;, cv_results_stand[&#39;test_F1_score&#39;].mean())
print(&quot;G-Mean 分数：&quot;, cv_results_stand[&#39;test_G-Mean&#39;].mean())
print(&quot;适合时间：&quot;, cv_results_stand[&#39;fit_time&#39;].mean())
]]></description>
      <guid>https://stackoverflow.com/questions/77811361/logistic-regression-values-dont-differ-that-much-as-expected</guid>
      <pubDate>Sat, 13 Jan 2024 12:05:34 GMT</pubDate>
    </item>
    <item>
      <title>神经网络的权重初始化，用于调整我的神经网络，以便进行适当的反向传播</title>
      <link>https://stackoverflow.com/questions/77811329/weights-initialization-for-neural-network-for-tunning-my-neural-networks-in-orde</link>
      <description><![CDATA[神经网络中的权重初始化方法是什么？
我想知道如何初始化神经网络中的权重，它们是通过手动还是其他方法完成的，权重对于调整神经网络非常有效，为了进行反向传播它将非常有帮助]]></description>
      <guid>https://stackoverflow.com/questions/77811329/weights-initialization-for-neural-network-for-tunning-my-neural-networks-in-orde</guid>
      <pubDate>Sat, 13 Jan 2024 11:56:45 GMT</pubDate>
    </item>
    <item>
      <title>制作一个执行拼图任务的人工智能[关闭]</title>
      <link>https://stackoverflow.com/questions/77810984/making-an-ai-that-perform-jigsaw-puzzles-tasks</link>
      <description><![CDATA[图像拼图 AI：
背景：

我刚刚完成机器学习课程并学习了一些计算机视觉技术，突然这个想法出现在我的脑海中，我尝试谷歌类似的东西但没有找到任何东西，所以我想做一个关于这。你们对如何开始以下任务有什么想法或建议吗？

数据库设置：

访问提供的图像数据库。
将每个图像分割成更小的、定义的部分（补丁）
为每个补丁分配特定位置，类似于解决拼图游戏。
随机化这些分段补丁的顺序。

&lt;小时/&gt;
目标：

尝试重新组合补丁以重建原始图像。
训练一个人工智能，在推理时间内：在不知道补丁对应位置的情况下重新组装它们

&lt;小时/&gt;]]></description>
      <guid>https://stackoverflow.com/questions/77810984/making-an-ai-that-perform-jigsaw-puzzles-tasks</guid>
      <pubDate>Sat, 13 Jan 2024 09:51:55 GMT</pubDate>
    </item>
    <item>
      <title>CNN 模型（在 Matlab 中创建）未经训练</title>
      <link>https://stackoverflow.com/questions/77810698/the-cnn-model-created-in-matlab-is-not-trained</link>
      <description><![CDATA[我正在尝试在 Matlab 中训练 CNN 模型来预测样本大小等于 10 的随机向量的平均值。但是，模型的 RMSE 变得太高。
如果有人能提供建议，我将不胜感激。
问候，
莫森
`clear;clc;关闭全部

随机数(1)
mkdir(&#39;数字&#39;)

Num_Sample=500；
N=10；
X=1:N；
百分比_训练=70；
Percent_Val=15；
Percent_Test=100-(Percent_Train+Percent_Val);

Num_Train=floor(Percent_Train/100*Num_Sample);
Num_Val=下限(Percent_Val/100*Num_Sample);
Num_Test=Num_Sample-(Num_Train+Num_Val);

Rand_Ind=randperm(Num_Sample);
Rand_Ind_Train=Rand_Ind(1,1:Num_Train);
Rand_Ind_Val=Rand_Ind(1,1+Num_Train:Num_Train+Num_Val);
Rand_Ind_Test=Rand_Ind(1,1+Num_Train+Num_Val:end);

X0_Train=[];
X0_Val=[];
X0_测试=[];

对于 i=1：Num_Sample
    Y0{i}=2*rand(1,N);
    Mean_Y(1,i)=平均值(Y0{i});

    Fig_i=图(i);
    绘图(X,Y0{i},&#39;o&#39;,&#39;线宽&#39;,2);
    xlim([1,N])
    ylim([0,2])
    清除 Y0{i}

    saveas(Fig_i,[&#39;图/Fig_&#39; num2str(i) &#39;.jpg&#39;]);
    Fig_JPG=imread([&#39;图/Fig_&#39; num2str(i) &#39;.jpg&#39;]);
    %Fig_JPG_RS=imresize(Fig_JPG,0.5);
    Fig_JPG_Gray=rgb2gray(Fig_JPG);
    Fig_JPG_Gray_Double{i}=im2double(Fig_JPG_Gray);
    关闭所有
    我
结尾
[R_Fig_JPG_Gray C_Fig_JPG_Gray]=大小(Fig_JPG_Gray);

X_Train_0=[];
对于 i=1:长度(Rand_Ind_Train)
    X_Train_0=[X_Train_0 Fig_JPG_Gray_Double{Rand_Ind_Train(1,i)}];
    清除 Fig_JPG_Gray_Double{Rand_Ind_Train(1,i)}
结尾
X_Train=reshape(X_Train_0,R_Fig_JPG_Gray,C_Fig_JPG_Gray,1,长度(Rand_Ind_Train));

X_Val_0=[];
对于 i=1:长度(Rand_Ind_Val)
    X_Val_0=[X_Val_0 Fig_JPG_Gray_Double{Rand_Ind_Val(1,i)}];
    清除 Fig_JPG_Gray_Double{Rand_Ind_Val(1,i)}
结尾
X_Val=重塑(X_Val_0,R_Fig_JPG_Gray,C_Fig_JPG_Gray,1,长度(Rand_Ind_Val));

X_Test_0=[];
对于 i=1:长度(Rand_Ind_Test)
    X_Test_0=[X_Test_0 Fig_JPG_Gray_Double{Rand_Ind_Test(1,i)}];
    清除 Fig_JPG_Gray_Double{Rand_Ind_Test(1,i)}
结尾
X_Test=reshape(X_Test_0,R_Fig_JPG_Gray,C_Fig_JPG_Gray,1,长度(Rand_Ind_Test));
 
Y_Train=[Mean_Y(1,Rand_Ind_Train)]&#39;;
Y_Val=[Mean_Y(1,Rand_Ind_Val)]&#39;;
Y_Test=[Mean_Y(1,Rand_Ind_Test)]&#39;;

图1）
直方图(Mean_Y,N)
轴紧
ylabel(&#39;计数&#39;)
xlabel(&#39;Mean_Y&#39;)

%创建网络层
层数=[
    imageInputLayer([R_Fig_JPG_Gray C_Fig_JPG_Gray 1])
    卷积2dLayer(5,12,&#39;填充&#39;,&#39;相同&#39;)
    批量归一化层
    重新定义层
    maxPooling2dLayer(2,&#39;跨步&#39;,2)
    卷积2dLayer(3,16,&#39;填充&#39;,&#39;相同&#39;)
    批量归一化层
    重新定义层
    %averagePooling2dLayer(2,&#39;跨步&#39;,2)
    %卷积2dLayer(3,32,&#39;填充&#39;,&#39;相同&#39;)
    %batch归一化层
    %relu层
    %卷积2dLayer(3,32,&#39;填充&#39;,&#39;相同&#39;)
    %batch归一化层
    %relu层
    dropout层(0.2)
    全连接层(1)
    回归层]；

% 训练网络
miniBatchSize=60；
验证频率=3； %floor(numel(Y_Train)/miniBatchSize);
选项=trainingOptions(&#39;sgdm&#39;, ...
    &#39;MiniBatchSize&#39;,miniBatchSize, ...
    &#39;最大纪元&#39;,6, ...
    &#39;初始学习率&#39;,1e-3, ...
    &#39;LearnRateSchedule&#39;,&#39;分段&#39;, ...
    &#39;学习率下降因子&#39;,0.1, ...
    &#39;学习率下降周期&#39;,2, ...
    “随机播放”、“每个时代”、...
    &#39;验证数据&#39;,{X_Val,Y_Val}, ...
    &#39;验证频率&#39;，验证频率，...
    &#39;情节&#39;，&#39;训练进度&#39;，...
    &#39;详细&#39;，假）；

% 创建网络
Net=trainNetwork(X_Train,Y_Train,层数,选项);

% 测试网络
Y_Sim_Test=预测(Net,X_Test);`

我尝试创建一个 CNN 模型来预测随机向量的平均值。]]></description>
      <guid>https://stackoverflow.com/questions/77810698/the-cnn-model-created-in-matlab-is-not-trained</guid>
      <pubDate>Sat, 13 Jan 2024 07:54:32 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 XGBoost 改进建模？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77809914/how-to-improve-modeling-with-xgboost</link>
      <description><![CDATA[我正在尝试使用 XGBoost 建模时间序列，但我觉得我的数据集非常不连续（如下图）。该数据集记录了 2018 年至 2023 年测量的可可价格价值。
数据集可以在我的 GitHub 存储库中找到：
https://raw.githubusercontent。 com/nunesisabella/Analise-Preditiv-Cacau/main/Cacau%20NY%20Futuros_2018-2023.csv
建模如下：
model_XGB = xgb.XGBRegressor(n_estimators=1000，early_stopping_rounds = 50)

model_XGB.fit(X_train, y_train,
          eval_set=[(X_train, y_train), (X_test, y_test)],
          详细=20)

是否可以通过改进超参数或类似的方法来使测试集获得更好的结果？或者我的数据集确实缺乏季节性，即使是好的建模方法也很难处理它？&lt;​​/p&gt;
]]></description>
      <guid>https://stackoverflow.com/questions/77809914/how-to-improve-modeling-with-xgboost</guid>
      <pubDate>Sat, 13 Jan 2024 00:36:40 GMT</pubDate>
    </item>
    <item>
      <title>用于比较两个向量进行回归的损失函数[关闭]</title>
      <link>https://stackoverflow.com/questions/77809542/loss-function-for-comparing-two-vectors-for-regression</link>
      <description><![CDATA[我想创建一个损失函数来训练我的模型进行回归，其中预测的 Y&#39; 应等于标签 Y。其中 Y 和 Y&#39; 是 (1,16) 二进制胜利者（1 或 0 ）的向量。 
我应该使用binary_cross_entropy还是MSE？
PS：输入和输出之间没有关系，我想通过使用模型从 X 预测 Y 来在它们之间建立关系。
示例：
&lt;前&gt;&lt;代码&gt;X=[0,1,0,1,1,0,1,1,0,1,1,0,1,0,1,0,0,0,0,1,1 ,0,0,0,1,1,0,1,0,1,0,0]
Y = [1,1,1,0,0,0,1,1,1,1,1,0,1,0,0,1]

错误预测= [1,1,1,0,0,0,1,1,1,1,1,0,0,0,0,0]（在 2 中抖动）位）。
所以我想从 X 预测 Y ...什么是最好的成本函数？]]></description>
      <guid>https://stackoverflow.com/questions/77809542/loss-function-for-comparing-two-vectors-for-regression</guid>
      <pubDate>Fri, 12 Jan 2024 22:18:52 GMT</pubDate>
    </item>
    <item>
      <title>两步 ML 预测模型（二元然后连续）：第二步是否应该仅针对正因变量进行训练？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77809477/two-step-ml-prediction-model-binary-then-continuous-should-second-step-be-tra</link>
      <description><![CDATA[我正在创建一个两步 ML 预测模型来预测库存产品的销售情况。我的训练集中有很多销售额要么为 0，要么为负。第一步是一个二元模型，用于预测产品是否应该库存，即预期销量为正 (1) 或不库存，即（销量为负或 0）(0)。
第二步是使用连续模型来预测预计将从库存中受益的商品的销售情况。在这种情况下，我应该在第二步中在整个训练集上训练我的模型，还是只训练训练集中 sales&gt;0 的那些行？
我做了后者，但不确定我是否应该在第二步中使用完整的数据集。]]></description>
      <guid>https://stackoverflow.com/questions/77809477/two-step-ml-prediction-model-binary-then-continuous-should-second-step-be-tra</guid>
      <pubDate>Fri, 12 Jan 2024 21:59:58 GMT</pubDate>
    </item>
    <item>
      <title>citeseer和cora（图链接预测），代码准确性问题[关闭]</title>
      <link>https://stackoverflow.com/questions/77808697/citeseer-and-cora-graph-link-prediciton-problem-whith-code-accuracy</link>
      <description><![CDATA[大家好，我有一个正在训练 citeseer 数据集的代码，该数据集的准确率约为 62%，我需要将其至少提高到 75%，或者使用其中的另一个数据集，该数据集的准确率超过 80%，我使用的 cora 的准确率为 82%准确性，但我需要另一个数据集，或者正如我所说，使 citeseer 75
导入networkx为nx
将 numpy 导入为 np
从 sklearn.model_selection 导入 train_test_split，GridSearchCV
从 sklearn.metrics 导入 precision_score
从 sklearn.preprocessing 导入 StandardScaler
从 sklearn.svm 导入 SVC
从 Node2Vec 导入 Node2Vec
将 pandas 导入为 pd
导入操作系统

# 选择数据集 [ Comic, citeseer, cora ]
数据集=“漫画”

os.system(“cls”)
print(&quot;正在加载数据集...&quot;)
# 加载节点和边数据集
如果数据集==“漫画”：
    G = nx.有向图()
    对于索引，pd.read_csv(&#39;comic_nodes.csv&#39;).iterrows():G.add_node(row[&#39;node&#39;], type=row[&#39;type&#39;]) 中的行
    对于索引，行 in pd.read_csv(&#39;comic_edges.csv&#39;).iterrows():G.add_edge(row[&#39;hero&#39;], row[&#39;comic&#39;])
    node_features = np.array([G.nodes[node].get(&#39;type&#39;, &#39;unknown&#39;) for Node in G.nodes()])
    node_labels = np.array([G.nodes[node].get(&#39;type&#39;, &#39;unknown&#39;) for Node in G.nodes()])
    node_index = {node_id: i for i, node_id in enumerate(G.nodes())}
别的：
    G = nx.read_edgelist(f&quot;{dataset}.cites&quot;, create_using=nx.DiGraph())
    node_data = np.loadtxt(f&quot;{dataset}.content&quot;, dtype=str)
    node_data = [node_data 中数据的数据，如果 G.nodes() 中的数据[0]]
    node_ids, node_features, node_labels = zip(*[(data[0], data[1:-1].astype(int), data[-1]) 对于node_data中的数据])
    node_index = {node_id: i for i, node_id in enumerate(node_ids)}
节点= [node_id为node_id，_排序（node_index.items（），key = lambda item：item [1]）]

os.system(“cls”)
# 使用node2vec生成游走
node2vec = Node2Vec(G，维度=256，walk_length=70，num_walks=200，工人=4)
模型 = node2vec.fit(窗口=20, min_count=0, sg=1, epochs=20)
embeddings = np.array([model.wv[node] 用于节点中的节点])

# 将数据分成训练集和测试集（20%用于测试）
X_train，X_test，y_train，y_test = train_test_split（嵌入，node_labels，test_size = 0.01，random_state = 42）
缩放 = StandardScaler()
X_train = Scale.fit_transform(X_train)
X_test = Scale.transform(X_test)

best_params = {&#39;C&#39;:8}
##### 网格搜索
# grid_search = GridSearchCV(SVC(kernel=&#39;rbf&#39;, gamma=&#39;scale&#39;, random_state=42), {&#39;C&#39;: range(1, 11)}, cv=4)
# grid_search.fit(X_train, y_train)
# best_params = grid_search.best_params_
##### print(&quot;最佳超参数：&quot;, best_params)

# 使用最佳超参数训练 SVM
eclf = SVC(内核=&#39;rbf&#39;, C=best_params[&#39;C&#39;], gamma=&#39;scale&#39;, random_state=42)
eclf = eclf.fit(X_train, y_train)

os.system(“cls”)
print(“准确度：”, precision_score(y_test, eclf.predict(X_test)))
]]></description>
      <guid>https://stackoverflow.com/questions/77808697/citeseer-and-cora-graph-link-prediciton-problem-whith-code-accuracy</guid>
      <pubDate>Fri, 12 Jan 2024 18:44:56 GMT</pubDate>
    </item>
    <item>
      <title>Filter_Value 选择 TDA R</title>
      <link>https://stackoverflow.com/questions/77807593/filter-value-selection-tda-r</link>
      <description><![CDATA[我对 R 中 TDA 库（GSSTDA、TDAMapper、Mapper 等）的 filter_value 参数感到有点困惑。
例如，给出 GSSTDA 中的 Mapper 对象：
&lt;前&gt;&lt;代码&gt;映射器(
  完整数据，
  过滤器值，
  间隔数 = 5,
  重叠百分比 = 40,
  distance_type =“cor”，
  clustering_type =“分层”，
  num_bins_when_clustering = 10,
  连接类型=“单一”，
  最优聚类模式=“”，
  na.rm = TRUE
）

&lt;块引用&gt;
参数
过滤值
对输入应用过滤函数后得到的向量
矩阵，即具有每个过滤函数值的向量
包括样本。


有人可以通过一个众所周知的数据（例如 Iris）的示例来描述如何手动选择 filter_value 吗？
https://search.r-project.org /CRAN/refmans/GSSTDA/html/mapper.html]]></description>
      <guid>https://stackoverflow.com/questions/77807593/filter-value-selection-tda-r</guid>
      <pubDate>Fri, 12 Jan 2024 15:21:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 LOOCV 进行 K 最近邻的问题</title>
      <link>https://stackoverflow.com/questions/77804296/problem-conducting-k-nearest-neighbors-using-loocv</link>
      <description><![CDATA[我有一个示例表，我想对其进行 KKNN 分类。变量 V4 是响应，我希望分类器查看新数据点是否将分类为 0 或 1（实际数据有 12 列，第 12 列是响应，但我仍然会简化示例
库(kknn)

数据 &lt;- data.frame(
  V1=c(1.2,2.5,3.1,4.8,5.2),
  V2=c(0.7, 1.8, 2.3, 3.9, 4.1),
  V3=c(2.3, 3.7, 1.8, 4.2, 5.5),
  V4= c(0, 1, 0, 1, 0)
）

现在，我想使用 for 循环通过 LOOCV 构建 kknn 分类。假设 kknn=3
for (i in 1:nrow(data)) {
  train_data &lt;- 数据[-i, 1:3]
  train_data_response &lt;- data.frame(data[-i, 4])
  colnames(train_data_response) &lt;- “响应”
  test_set &lt;- 数据[i, 3]
  模型 &lt;- kknn(公式=train_data_response ~ ., data.frame(train_data),
                data.frame(test_set)，k=3，scale=TRUE)
}

现在我收到以下错误：
model.frame.default(公式，数据=训练)中的错误：
  变量“train_data_response”的类型（列表）无效

有什么办法可以解决这个错误吗？我认为 kknn 接受矩阵或数据帧。我的训练和测试数据确实是数据框，那么什么给出了？
另外，我是否正确执行了 LOOCV？]]></description>
      <guid>https://stackoverflow.com/questions/77804296/problem-conducting-k-nearest-neighbors-using-loocv</guid>
      <pubDate>Fri, 12 Jan 2024 04:15:37 GMT</pubDate>
    </item>
    <item>
      <title>联邦学习全局聚合后准确率下降</title>
      <link>https://stackoverflow.com/questions/77798059/the-accuracy-decreased-after-global-aggregation-in-federated-learning</link>
      <description><![CDATA[我正在开展一个联合学习项目。我编写了一段代码来刺激联邦学习的过程。然而，每次迭代进行全局聚合后，全局模型的测试精度会下降很多，并且在接下来的迭代中保持不变。我使用的聚合算法是FedAvg。我尝试将我的代码分成不同的单元来找出问题所在。
对于本地训练，所选客户训练 3 轮。在这个实验中，将选择所有五个客户端进行训练和聚合，我用于本地的模型是从 torchvision 分叉的 vgg16，数据集是 MNIST，并以 i.i.d 方式分割每个客户端： 
for id, net_id in enumerate(selected):
    logging.info(“训练所选设备 %s。” % (str(net_id)))
    结果 = Userlists[net_id].train(hparams[&#39;n_local_epochs&#39;])
    logging.info(&#39;&gt;&gt; 局部模型 %d: 局部精度: %f in round %d\n&#39; % (id, result[&#39;local_test_acc&#39;], step+1))

在本地模型聚合之前，我使用全局服务器的测试数据来测试本地模型的准确性，
tesc，conf = Misc.compute_accuracy(Userlists[2].model，test_dl_global，get_confusion_matrix=True，device=hparams[&#39;device&#39;])
打印（测试）
&gt; 0.2478966346153846
tesc，conf = Misc.compute_accuracy（Userlists [3] .model，test_dl_global，get_confusion_matrix = True，device = hparams [&#39;device&#39;]）
打印（测试）
&gt; 0.14413060897435898
tesc,conf=misc.compute_accuracy(Userlists[4].model,test_dl_global,get_confusion_matrix=True,device=hparams[&#39;device&#39;])
打印（测试）
&gt; 0.17387820512820512

我使用下面的聚合代码来聚合所选客户端的权重：
&lt;前&gt;&lt;代码&gt;total_sum = 0.0
对于选定的 client_idx：
    Total_sum += 用户列表[client_idx].data_len
    
    
global_para = global_model.state_dict()
client_weights = [torch.tensor( Userlists[client_idx].data_len/total_sum, device=hparams[&#39;device&#39;]) for client_idx in selected]

使用 torch.no_grad()：
    对于顺序，枚举中的 idx（选定）：
        logging.info(f“对于客户端 {idx}”)
        net_para = Userlists[idx].model.state_dict()
        
        如果订单 == 0：
            对于 net_para.keys() 中的键：
                global_para[key] = net_para[key] * client_weights[订单]
        别的：
            对于 net_para.keys() 中的键：
                global_para[key] += net_para[key] * client_weights[订单]


global_model.load_state_dict(global_para)
tesc,conf=misc.compute_accuracy(global_model,train_dl_global,get_confusion_matrix=True,device=hparams[&#39;device&#39;])

全局测试精度下降并保持不变
&lt;前&gt;&lt;代码&gt;&gt; 0.11236666666666667

虽然我尝试增加本地训练的纪元，局部准确率提高到 40%，但全局准确率仍然落入与之前相同的值。我的聚合代码中是否有错误的地方？
测试精度应与本地精度保持在同一水平。]]></description>
      <guid>https://stackoverflow.com/questions/77798059/the-accuracy-decreased-after-global-aggregation-in-federated-learning</guid>
      <pubDate>Thu, 11 Jan 2024 06:30:41 GMT</pubDate>
    </item>
    <item>
      <title>带有我自己的预训练模型的 Sagemaker 批处理变压器</title>
      <link>https://stackoverflow.com/questions/77781734/sagemaker-batch-transformer-with-my-own-pre-trained-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77781734/sagemaker-batch-transformer-with-my-own-pre-trained-model</guid>
      <pubDate>Mon, 08 Jan 2024 15:54:18 GMT</pubDate>
    </item>
    <item>
      <title>张量流形状错误：要求输入和输出形状匹配？</title>
      <link>https://stackoverflow.com/questions/77174989/tensorflow-shape-error-asking-for-input-and-output-shape-to-match</link>
      <description><![CDATA[我是 Tensorflow 的新手，我意识到有很多关于形状问题的帖子，但我还无法完全将他们的解决方案应用于我的问题。因此，如果这是一个常见/多余的问题，请原谅。
上下文：我正在使用图像数据来预测图像的类型。更具体地说，我有许多图像的像素数据 (50x50x3)，这些图像属于 4 个类别之一 [风景、肖像、抽象、其他]。
代码细节：
我将尝试显示可能有问题的代码。当然，这个项目中还有其他代码，但为了简单起见，我将省略它（否则它将是一篇难以阅读的文章）；但是，如果没有找到解决方案，我将在后续内容中详细介绍。
我使用以下代码对标签进行了一次性编码
train_labels = to_categorical(train_labels, num_classes=NUM_CLASSES, dtype=&#39;float32&#39;)
然后，我通过提供带有图像路径列表的张量对象并在所有路径上运行 load_image 函数（未显示）来加载像素数据。
train_data = tf.data.Dataset.from_tensor_slices((train_files, train_labels))
train_data = train_data.map(load_image, num_parallel_calls=AUTOTUNE)
train_data = train_data.map(标准化, num_parallel_calls=AUTOTUNE)
train_data = train_data.shuffle(buffer_size=shuffle_buffer_size)
train_data = train_data.batch(batch_size)`

train_data 对象如下所示：
&lt;_BatchDataset element_spec=(TensorSpec(shape=(None, 50, 50, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf .float32，名称=无））&gt;
我认为这是有道理的，因为输入数据是 50x50x3（具有批处理维度），而输出维度是 4，因为 4 个类的 one-hot 编码（具有批处理维度）。整个过程和结果跟我的验证数据是一样的。
但是，当我尝试训练模型时出现形状错误：
def simple_FFNN(image_height, image_width, num_channels, num_classes):
    input_shape = \[image_height, image_width, num_channels\] # 高度、宽度、通道
    模型=顺序（）
    model.add(layers.Input(shape=input_shape))
    model.add(layers.Dense(64,activation=&#39;relu&#39;))
    model.add(layers.Dense(units=num_classes,activation=&#39;softmax&#39;))

    返回模型

学习率 = 0.01
历元 = 3
模型 = simple_FFNN(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS, NUM_CLASSES)
优化器 = 优化器.SGD(learning_rate=learning_rate)
损失=损失.categorical_crossentropy
model.compile(损失=损失,
优化器=优化器，
指标=&#39;准确性&#39;）

训练结果 = model.fit(
    训练数据，
    验证数据=验证数据，
    纪元=纪元，
    详细=1)`

错误是：
ValueError：形状 (None, 4) 和 (None, 50, 50, 4) 不兼容
第一个形状是我的输出形状，第二个形状几乎是我的输入形状，但通道数量略有偏差......几乎就像被替换的类数量一样。
无论如何，我不明白为什么输出形状和输入形状必须首先匹配。我一定是误解了这个问题。有人有什么想法吗？
谢谢
我尝试删除 train_data.batch() 中的批处理，但随后出现错误，指出 (None, 50, 50, 3) 和 (50, 50, 3) 不匹配 (这对我来说很有意义）。有趣的是，它显示 (50, 50, 3) 而不是 (50, 50, 4)，所以我不明白为什么最后一个维度被 4 交换。]]></description>
      <guid>https://stackoverflow.com/questions/77174989/tensorflow-shape-error-asking-for-input-and-output-shape-to-match</guid>
      <pubDate>Mon, 25 Sep 2023 18:29:13 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch RuntimeError：CUDA 内存不足且有大量可用内存</title>
      <link>https://stackoverflow.com/questions/71498324/pytorch-runtimeerror-cuda-out-of-memory-with-a-huge-amount-of-free-memory</link>
      <description><![CDATA[在训练模型时，我遇到了以下问题：
运行时错误：CUDA 内存不足。尝试分配 304.00 MiB（GPU 0；8.00 GiB 总容量；已分配 142.76 MiB；6.32 GiB 空闲；PyTorch 总共保留 158.00 MiB）分配的内存尝试设置 max_split_size_mb 以避免碎片。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档
正如我们所看到的，当尝试分配 304 MiB 内存时会发生错误，而 6.32 GiB 是空闲的！问题是什么？正如我所看到的，建议的选项是设置 max_split_size_mb 以避免碎片。它会有帮助吗？如何正确地做到这一点？
这是我的 PyTorch 版本：
火炬==1.10.2+cu113
火炬视觉==0.11.3+cu113
火炬音频===0.10.2+cu113]]></description>
      <guid>https://stackoverflow.com/questions/71498324/pytorch-runtimeerror-cuda-out-of-memory-with-a-huge-amount-of-free-memory</guid>
      <pubDate>Wed, 16 Mar 2022 13:53:45 GMT</pubDate>
    </item>
    </channel>
</rss>