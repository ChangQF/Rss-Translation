<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 16 Jan 2024 18:17:37 GMT</lastBuildDate>
    <item>
      <title>如何仅使用组件在 azure ml Designer 中训练和部署 ml 模型？</title>
      <link>https://stackoverflow.com/questions/77827691/how-to-train-and-deploy-ml-models-in-azure-ml-designer-just-using-components</link>
      <description><![CDATA[我在 azure ml Designer 中创建了一个训练管道。现在，我需要通过添加用于注册和部署的组件来部署此模型。我想我可以使用“执行 python 脚本”组件来执行此操作。但是我不知道如何将“训练的最佳模型”（“调整模型超参数”组件的输出）与“执行 python 脚本”组件连接起来。那么，知道如何完成这项任务吗？我将非常感谢您的帮助。
这是我的管道：
训练管道]]></description>
      <guid>https://stackoverflow.com/questions/77827691/how-to-train-and-deploy-ml-models-in-azure-ml-designer-just-using-components</guid>
      <pubDate>Tue, 16 Jan 2024 17:39:49 GMT</pubDate>
    </item>
    <item>
      <title>如何在提供 customGPT 之前处理大型 PDF 文件并构建数据</title>
      <link>https://stackoverflow.com/questions/77827516/how-to-process-large-pdf-file-and-structure-the-data-before-feeding-customgpt</link>
      <description><![CDATA[我正在学习机器学习和人工智能。我有大量 pdf、研究论文和书籍，我想将它们提供给 customGPT。我正在学习自然语言处理、标记化、词干提取等
我认为所有这些都可以通过编程方式完成，但我真正的问题是 - 在提取为“文本”后执行这些 PDF 文件吗？只有文件需要人工干预吗？我无法想象自己会浏览数千页，所以我对此表示怀疑。]]></description>
      <guid>https://stackoverflow.com/questions/77827516/how-to-process-large-pdf-file-and-structure-the-data-before-feeding-customgpt</guid>
      <pubDate>Tue, 16 Jan 2024 17:09:25 GMT</pubDate>
    </item>
    <item>
      <title>max在遗传算法中起什么作用？</title>
      <link>https://stackoverflow.com/questions/77826027/what-role-does-max-play-in-genetic-algorithms</link>
      <description><![CDATA[在网上找到了一个遗传算法代码，求解的是f(x)=2*sin(x) + cos(x)的最大值，但是我发现代码中有一个参数max_value，并且不知道有什么作用。
GA代码如下：
随机导入
导入数学
将 matplotlib.pyplot 导入为 plt


def 物种起源（种群大小，染色体长度）：
    人口=[[]]
    对于范围内的 i（population_size）：
        临时=[]
        对于范围内的 j（染色体长度）：
            临时.append(随机.randint(0, 1))
        人口.追加（临时）
    返回人口[1:]


def 翻译（人口，染色体长度）：
    临时=[]
    对于范围内的 i(len(population))：
        总计 = 0
        对于范围内的 j（染色体长度）：
            总计 = 总计 + 人口[i][j] * (math.pow(2, j))
        临时.追加（总计）
    暂时返回


def 函数（群体、染色体长度、最大值）：
    临时=[]
    函数1 = []
    临时=翻译（人口，染色体长度）
    对于范围内的 i(len(临时))：
        x = 临时[i] * max_value / (math.pow(2, chtomosome_length) - 1)
        function1.append(2 * math.sin(x) + math.cos(x))
    返回函数1

def 健身（功能1）：
    健身1 = []
    最小适应度 = mf = 0
    对于范围内的 i(len(function1))：
        如果（函数 1[i] + mf &gt; 0）：
            临时 = mf + 函数 1[i]
        别的：
            临时 = 0.0
        Fitness1.append（临时）
    返回健身1


def sum(fitness1):
    总计 = 0
    对于范围内的 i(len(fitness1))：
        总计 += 适应度1[i]
    返回总计

# https://blog.csdn.net/weixin_39068956/article/details/105121469
def cumsum(健身1):
    对于范围内的 i(len(fitness1) - 2, -1, -1)：
        总计 = 0
        j = 0
        而（j &lt;= i）：
            总计 += 适应度1[j]
            j += 1
        适应度1[i] = 总计
        健身1[len(健身1) - 1] = 1


def选择（人口，适应度1）：
    新健身=[]
    总适应度=总和（适应度1）
    对于范围内的 i(len(fitness1))：
        new_fitness.append(fitness1[i]/total_fitness)


    cumsum(new_fitness)

    毫秒 = []
    人口长度 = pop_len = len(人口)
    对于范围内的 i(pop_len)：
        ms.append(随机.随机())
    ms.sort()

    适应= 0
    纽因 = 0
    新人口 = 新人口 = 人口

    而纽因 &lt;流行长度：
        if (ms[newin] &lt; new_fitness[fitin]):
            new_pop[newin] = 人口[fitin]
            纽因 += 1
        别的：
            适合+= 1
    人口=新人口


def 交叉（人口，pcB00）：
    pop_len = len(人口)

    对于范围内的 i(pop_len - 1)：
        cpoint = random.randint(0, len(population[0]))
        临时1 = []
        临时2 = []

        临时1.extend(人口[i][0:cpoint])
        临时1.extend(population[i + 1][cpoint:len(population[i])])

        临时2.extend(人口[i + 1][0:cpoint])
        临时2.extend(population[i][cpoint:len(population[i])])

        人口[i] = 临时1
        人口[i + 1] = 临时2


def 突变（群体，pm）：
    px = len(人口)
    py = len(人口[0])

    对于范围内的 i（px）：
        if (random.random() &lt; pm):
            mpoint = random.randint(0, py - 1)
            if (人口[i][mpoint] == 1):
                人口[i][m点] = 0
            别的：
                人口[i][m点] = 1


def b2d(b, 最大值, 染色体长度):
    总计 = 0
    对于范围内的 i(len(b))：
        总计 = 总计 + b[i] * math.pow(2, i)
    总计 = 总计 * max_value / (math.pow(2, 染色体长度) - 1)
    返回总计


def best（人口，健身1）：
    px = len(人口)
    最佳个人=[]
    最佳适应度 = 适应度1[0]

    对于范围 (1, px) 内的 i：
        if (fitness1[i] &gt; bestfitness):
            最佳适应度 = 适应度1[i]
            最佳个体 = 总体[i]

    返回[最佳个体，最佳适应度]

＃ 主要的
人口规模 = 500
最大值 = 10
染色体长度 = 10
个人计算机=0.6
下午 = 0.01
结果=[[]]
健身1 = []
拟合平均值 = []

人口=流行=物种起源（人口大小，染色体长度）


对于范围内的 i（population_size）：
    函数 1 = 函数（群体、染色体长度、最大值）
    适应度1 = 适应度(函数1)
    best_individual, best_fitness = best(人口, 健身1)
    results.append([best_fitness, b2d(best_individual, max_value, 染色体长度)])

    选择（人口，适应度1）
    交叉（人口，个人电脑）
    突变（群体，pm）

结果=结果[1:]
结果.sort()
X = []
Y = []
对于范围（500）内的 i：
    X.追加(i)
    Y.append(结果[i][0])
plt.plot(X, Y)
plt.show()

我尝试调整max_value的值，发现对结果影响很大，这让我很困惑。]]></description>
      <guid>https://stackoverflow.com/questions/77826027/what-role-does-max-play-in-genetic-algorithms</guid>
      <pubDate>Tue, 16 Jan 2024 13:20:31 GMT</pubDate>
    </item>
    <item>
      <title>如何在多维复杂数据上训练模型？</title>
      <link>https://stackoverflow.com/questions/77825016/how-to-train-a-model-on-multidimensional-complex-data</link>
      <description><![CDATA[我有一个输入数据数组，它们是 5 个不同长度的数组。如何构建正确的张量和形式进行训练？
&lt;前&gt;&lt;代码&gt;[
[
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ],],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
[
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2 ], [ 1, 2 ] ],
  [ [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ], [ 1, 2, 3, 4, 5 ],],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]],
  [ [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]
],
...
],
]
]]></description>
      <guid>https://stackoverflow.com/questions/77825016/how-to-train-a-model-on-multidimensional-complex-data</guid>
      <pubDate>Tue, 16 Jan 2024 10:22:33 GMT</pubDate>
    </item>
    <item>
      <title>是否建议对经过one-hot编码的数据进行主成分分析（PCA）</title>
      <link>https://stackoverflow.com/questions/77824892/is-it-recommended-to-perform-principal-component-analysis-pca-on-data-that-has</link>
      <description><![CDATA[我正在做一个项目，虽然机器学习模型做得还不错，但我觉得它还可以更好。该模型可以很好地预测多数类别，但不能很好地预测少数类别。多数类的召回率和精度分别为 84% 和 82%，少数类的召回率和精度分别为 39% 和 52%。
我向数据中添加了更多特征，并使用 SMOTE 来平衡数据的分布，少数类别的召回率和精度分别提高到 54% 和 52%，这是一个显着的结果，但是少数类别的召回率和精度多数阶层仍分别保持在 84% 和 82%。
我希望少数类的查全率和查准率都在 70% 以上，我想尝试的一种方法是对数据进行 one-hot 编码，然后使用主成分分析 (PCA) 来减小特征空间的大小同时保留尽可能多的信息，但我不知道是否建议这样做。
那么有谁知道是否建议对经过 one-hot 编码的数据执行主成分分析（PCA）？]]></description>
      <guid>https://stackoverflow.com/questions/77824892/is-it-recommended-to-perform-principal-component-analysis-pca-on-data-that-has</guid>
      <pubDate>Tue, 16 Jan 2024 10:02:11 GMT</pubDate>
    </item>
    <item>
      <title>尝试运行 SVC 分类模型，花了一个小时但没有响应</title>
      <link>https://stackoverflow.com/questions/77822664/trying-to-run-a-classification-model-for-svc-taking-hour-and-not-responding</link>
      <description><![CDATA[我已经尝试运行 SVC 分类模型三天了，但该模型没有响应。我检查了我的数据、标准缩放器、训练测试拆分和所有必要的库，所有这些都已正确应用和工作。我运行随机森林分类器模型，该模型运行成功，但存在良好的准确度分数（F1 分数）。但是 SVC 不工作，可能是什么问题？
我尝试过 RandomForest，效果很好，但 SVC 从未起作用。可能是什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/77822664/trying-to-run-a-classification-model-for-svc-taking-hour-and-not-responding</guid>
      <pubDate>Mon, 15 Jan 2024 22:38:54 GMT</pubDate>
    </item>
    <item>
      <title>在使用pywinauto生成的多个系统中使用Wrapper_objects</title>
      <link>https://stackoverflow.com/questions/77819573/using-wrapper-objects-in-multiple-systems-generated-using-pywinauto</link>
      <description><![CDATA[我在使用 pywinauto 执行操作时得到了一个包装器。
现在我想在不同的系统上使用同一应用程序上的相同包装器来播放它。
可能吗？
我正在使用它创建包装对象：
from ctypes.wintypes import tagPOINT
导入 pywinauto
导入时间
时间.睡眠(2)
def get_ElementFromPoint(x,y):
     elem = pywinauto.uia_defines.IUIA().iuia.ElementFromPoint(tagPOINT(x, y))
     元素 = pywinauto.uia_element_info.UIAElementInfo(elem)
     包装器 = pywinauto.controls.uiawrapper.UIAWrapper(元素)
     返回包装器

创建的对象示例如下：


如何在不同的系统中使用此包装器来自动执行我的任务？]]></description>
      <guid>https://stackoverflow.com/questions/77819573/using-wrapper-objects-in-multiple-systems-generated-using-pywinauto</guid>
      <pubDate>Mon, 15 Jan 2024 11:49:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在 python 3.12.1 上安装 PyTorch</title>
      <link>https://stackoverflow.com/questions/77792551/how-to-install-pytorch-on-python-3-12-1</link>
      <description><![CDATA[我正在安装 DARTS TimeSeries 库 (https: //github.com/unit8co/darts/blob/master/INSTALL.md#enabling-Optional-dependencies），但我遇到了依赖项安装问题。在 DARTS 安装指南中，它说如果我们遇到这个问题，我们必须参考 PyTorch 的官方安装指南，然后尝试再次安装 Darts。然后，当我尝试在 python 3.12.1 上安装 torch 时，我遇到了这个错误：
&lt;块引用&gt;
错误：找不到满足火炬要求的版本（来自版本：无）
错误：找不到火炬的匹配发行版。

如何解决？
我使用 PyCharm 作为 Python 代码编辑器。
我尝试了pip install darts，但它没有安装所有软件包并遇到此错误错误：subprocess-exited-with-error
 用于安装构建依赖项的 pip 子进程未成功运行。
  │ 退出代码：1
  ╰─&gt; 【136行输出】
      正在收集setuptools&gt;=64.0
        从 https://files.pythonhosted.org/packages 获取 setuptools&gt;=64.0 的依赖信息

然后，我尝试使用 pip install torch 安装 torch 并遇到此错误
错误：找不到满足火炬要求的版本（来自版本：无）
错误：找不到火炬的匹配发行版]]></description>
      <guid>https://stackoverflow.com/questions/77792551/how-to-install-pytorch-on-python-3-12-1</guid>
      <pubDate>Wed, 10 Jan 2024 10:16:06 GMT</pubDate>
    </item>
    <item>
      <title>来自不同包的相同算法会生成完全不同的模型？</title>
      <link>https://stackoverflow.com/questions/77415335/same-algorithm-from-different-packages-generates-completely-different-model</link>
      <description><![CDATA[我正在分别使用两个包训练线性模型。
但是，我意识到这两个结果在变量系数方面存在巨大差异。
def 测试（x，y，模型）：
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=101)
    regr = Linear_model.LinearRegression()
    regr.fit(x_train, y_train)
    
    lr = sm.OLS(y_train, x_train).fit()
    打印（lr.params）
    
    打印（regr.coef_）

上面是我使用的代码。令人惊讶的是，系数差异如此之大，以至于给出了完全不同的预测。
两个模型都以相同的顺序列出变量，所以我现在真的很困惑。知道出了什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77415335/same-algorithm-from-different-packages-generates-completely-different-model</guid>
      <pubDate>Fri, 03 Nov 2023 08:58:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在海量数据上训练机器学习模型？</title>
      <link>https://stackoverflow.com/questions/74886400/how-to-train-a-machine-learning-model-on-huge-amount-of-data</link>
      <description><![CDATA[关键点：数据集太大了，我几乎无法将其存储在硬件中。 （拍字节）
假设我的数据集中有数万亿行。该数据集太大，无法存储在内存中。我想在这个数据集上训练一个机器学习模型，比如逻辑回归。我该怎么办？
现在，我知道亚马逊/谷歌在大量数据上进行机器学习。他们怎样做呢？例如点击数据集，全局每个智能设备的输入都存储在一个数据集中。
拼命寻找新想法并乐于接受修正。
我的思路：

加载部分数据到内存
执行梯度下降

这样优化就是小批量下降。
现在的问题是，在优化中，无论是SGD还是mini Batch，在最坏的情况下，当它遍历完所有数据时就会停止。遍历整个数据集是不可能的。
所以我有了提前停止的想法。提前停止保留验证集，并在错误停止下降/收敛于验证集时停止优化。但由于数据集的大小，这可能不可行。
现在我正在考虑简单地随机采样训练集和测试集，并使用可行的大小来训练模型。]]></description>
      <guid>https://stackoverflow.com/questions/74886400/how-to-train-a-machine-learning-model-on-huge-amount-of-data</guid>
      <pubDate>Thu, 22 Dec 2022 09:16:33 GMT</pubDate>
    </item>
    <item>
      <title>Ubuntu 上安装的 AWS EFS 文件系统中的 Python AWS Lambda 错误“libgomp.so.1：无法打开共享对象文件：没有此类文件或目录”</title>
      <link>https://stackoverflow.com/questions/67844901/python-aws-lambda-error-libgomp-so-1-cannot-open-shared-object-file-no-such-f</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/67844901/python-aws-lambda-error-libgomp-so-1-cannot-open-shared-object-file-no-such-f</guid>
      <pubDate>Fri, 04 Jun 2021 23:09:34 GMT</pubDate>
    </item>
    <item>
      <title>机器学习线性回归 - Sklearn</title>
      <link>https://stackoverflow.com/questions/55096288/machine-learning-liner-regression-sklearn</link>
      <description><![CDATA[我是机器学习领域的新手，对于学习回归我有一些疑问
1：在练习sklearn学习回归模型预测方法时出现以下错误。 
代码：
sklearn.linear_model.LinearRegression.predict(25)

错误：
 “ValueError：需要 2D 数组，改为标量数组：array=25。如果数据具有单个特征，则使用 array.reshape(-1, 1) 重塑数据，如果数据包含，则使用 array.reshape(1, -1) 重塑数据单个样本。”
我需要传递一个二维数组吗？检查了 sklearn 文档页面，没有找到任何版本更新的内容。
 **在 Kaggle 上运行我的代码
https://www.kaggle.com/aman9d/bikesharingdemand-upx/ &lt; /p&gt;

2：数据集的索引会影响模型的得分（权重）吗？]]></description>
      <guid>https://stackoverflow.com/questions/55096288/machine-learning-liner-regression-sklearn</guid>
      <pubDate>Mon, 11 Mar 2019 06:29:10 GMT</pubDate>
    </item>
    <item>
      <title>InvalidArgumentError：无法计算 MatMul，因为输入 #0（从零开始）预计是浮点张量，但实际上是双张量 [Op:MatMul]</title>
      <link>https://stackoverflow.com/questions/54255431/invalidargumenterror-cannot-compute-matmul-as-input-0zero-based-was-expected</link>
      <description><![CDATA[有人可以解释一下，TensorFlow 的 eager 模式是如何工作的吗？我正在尝试构建一个简单的回归，如下所示：
将张量流导入为 tf
将 numpy 导入为 np

tfe = tf.contrib.eager
tf.enable_eager_execution()

def make_model():
    net = tf.keras.Sequential()
    net.add(tf.keras.layers.Dense(4, 激活=&#39;relu&#39;))
    net.add(tf.keras.layers.Dense(1))
    回网

def计算损失（预测，实际）：
    返回 tf.reduce_mean(tf.square(tf.subtract(pred,actual)))

defcompute_gradient（模型，预测，实际）：
    “”“”用给定的噪声和输入“”“计算梯度”
    使用 tf.GradientTape() 作为磁带：
        损失=计算损失（预测，实际）
    grads = Tape.gradient(loss, model.variables)
    返回毕业生，损失

def apply_gradients（优化器，梯度，model_vars）：
    优化器.apply_gradients(zip(grads, model_vars))
    
模型 = make_model()
优化器 = tf.train.AdamOptimizer(1e-4)

x = np.linspace(0,1,1000)
y = x + np.random.normal(0,0.3,1000)
y = y.astype(&#39;float32&#39;)
train_dataset = tf.data.Dataset.from_tensor_slices((y.reshape(-1,1)))

纪元 = 2# 10
批量大小 = 25
itr = y.shape[0] # 批量大小
对于范围内的纪元（纪元）：
    对于 tf.contrib.eager.Iterator(train_dataset.batch(25)) 中的数据：
        preds = 模型（数据）
        梯度，损失=compute_gradient（模型，preds，数据）
        apply_gradients（优化器，梯度，模型.变量）
# 梯度输出：[无，无，无，无，无，无]

错误如下：
------------------------------------------------ ------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-3-a589b9123c80&gt;在&lt;模块&gt;中
     35 梯度，损失=compute_gradient（模型，preds，数据）
     36 打印（渐变）
---&gt; 37 apply_gradients（优化器，梯度，模型.变量）
     38 # 用 tf.GradientTape() 作为磁带：
     39 # 损失 = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(preds, data))))

&lt;ipython-input-3-a589b9123c80&gt;在 apply_gradients（优化器，梯度，model_vars）
     17 号
     18 def apply_gradients（优化器，梯度，model_vars）：
---&gt; 19 优化器.apply_gradients(zip(grads, model_vars))
     20
     21 模型 = make_model()

〜/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py 在 apply_gradients(self, grads_and_vars, global_step, name)
    第589章
    590 raise ValueError(“没有为任何变量提供梯度：%s。”%
--&gt;第591章
    第592章
    第593章

ValueError：没有为任何变量提供渐变：

编辑
我更新了我的代码。现在，问题出现在梯度计算中，它返回零。我检查过损失值不为零。]]></description>
      <guid>https://stackoverflow.com/questions/54255431/invalidargumenterror-cannot-compute-matmul-as-input-0zero-based-was-expected</guid>
      <pubDate>Fri, 18 Jan 2019 13:52:51 GMT</pubDate>
    </item>
    <item>
      <title>R 中奇怪的 svm 行为 (e1071)</title>
      <link>https://stackoverflow.com/questions/51852415/weird-svm-behavior-in-r-e1071</link>
      <description><![CDATA[我在 R（第一个示例）和 Python（第二个示例）中使用 SVM 运行了以下代码来执行二元分类任务。 
给定随机生成的数据(X)和响应(Y)，此代码执行离开组交叉验证 1000 次。因此，Y 的每个条目都是 CV 迭代预测的平均值。 
曲线下的计算面积应为 ~0.5，因为 X 和 Y 是完全随机的。然而，这并不是我们所看到的。曲线下面积通常显着高于 0.5。 X 的行数非常少，这显然会导致问题。 
知道这里会发生什么吗？我知道我可以增加 X 的行数或减少列数来解决问题，但我正在寻找其他问题。 
Y=as.factor(rep(c(1,2), times=14))
X=矩阵(runif(长度(Y)*100), nrow=长度(Y))

图书馆(e1071)
库（pROC）

列名(X)=1:ncol(X)
迭代=1000
ansMat=矩阵(NA,长度(Y),iter)
for(i in seq(iter)){
    #搭火车

    训练=样本(seq(长度(Y)),0.5*长度(Y))
    if(min(表(Y[train]))==0)
    下一个

    #从火车上测试
    测试=seq(长度(Y))[-train]

    #训练模型
    XX=X[火车,]
    YY=Y[火车]
    mod=svm(XX,YY,概率=FALSE)
    XXX=X[测试,]
    predVec=预测(mod,XXX)
    RFans=attr(predVec,&#39;decision.values&#39;)
    ansMat[测试，i]=as.numeric(predVec)
}

ans=rowMeans(ansMat,na.rm=TRUE)

r=roc(Y,ans)$auc
打印(r)

同样，当我在 Python 中实现相同的事情时，我得到了类似的结果。 
Y = np.array([1, 2]*14)
X = np.random.uniform(size=[len(Y), 100])
n_iter = 1000
ansMat = np.full((len(Y), n_iter), np.nan)
对于范围内的 i(n_iter)：
    # 获取训练/测试索引
    火车= np.random.choice（范围（len（Y）），大小= int（0.5 * len（Y）），替换= False，p =无）
    如果 len(np.unique(Y)) == 1:
        继续
    test = np.array([i for i in range(len(Y)) if i not in train])
    # 训练模型
    mod = SVC(概率=假)
    mod.fit(X=X[火车, :], y=Y[火车])
    # 预测并收集答案
    ansMat[测试，i] = mod.predict(X[测试，:])
ans = np.nanmean(ansMat, 轴=1)
fpr、tpr、阈值 = roc_curve(Y、ans、pos_label=1)
打印（auc（fpr，tpr））`
]]></description>
      <guid>https://stackoverflow.com/questions/51852415/weird-svm-behavior-in-r-e1071</guid>
      <pubDate>Wed, 15 Aug 2018 03:06:09 GMT</pubDate>
    </item>
    <item>
      <title>数据科学模型和培训 - 理解[关闭]</title>
      <link>https://stackoverflow.com/questions/48197227/data-science-model-and-training-understanding</link>
      <description><![CDATA[来自于编写代码、测试、部署、运行的编程背景。我试图理解数据科学中“训练模型”或“经过训练的模型”的概念，并部署该模型训练有素的模型。 
我并不真正关心部署环境、自动化等。我正在尝试了解部署单元......一个经过训练的模型。经过训练的模型在文件系统上是什么样子，它包含什么？ 
我理解训练模型并将一组数据分为训练集和测试集的概念，但是假设我有一个笔记本（python / jupyter）并且我加载了一些数据，在训练/测试之间进行划分数据，并运行算法来“训练”我的模型。我的幕后成果是什么？当我训练模型时，我认为内存中会存储一定量的数据……那么这些数据如何成为训练模型的一部分呢？它显然不能包含用于训练的所有数据；因此，例如，如果我正在训练一个聊天机器人代理（基于检索），那么在我添加/输入用户问题或“意图”的示例之后，作为该训练的一部分实际发生了什么，以及我的可部署性是什么训练有素的模型？这个经过训练的模型是否包含某种来自训练或术语数组的数据总和，它可以达到多大（可部署大小）？
虽然这个问题看起来相对简单“什么是经过训练的模型”，但我如何用简单的术语向 DevOps 技术解释它？这是一个“对数据科学感兴趣的 IT 人员，试图在与数据科学人员的讨论中理解经过训练的模型的有形单元”。 
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/48197227/data-science-model-and-training-understanding</guid>
      <pubDate>Wed, 10 Jan 2018 22:36:27 GMT</pubDate>
    </item>
    </channel>
</rss>