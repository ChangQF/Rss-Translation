<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 25 Jun 2024 15:15:38 GMT</lastBuildDate>
    <item>
      <title>Yolov9分类</title>
      <link>https://stackoverflow.com/questions/78668142/yolov9-classification</link>
      <description><![CDATA[我是初学者，所以有些东西我不太懂。是否可以只使用 yolov9 模型进行分类任务？如果可以，我该怎么做？
我学会了如何在 yolov8 模型上进行分类，但它不适用于 yolov9。我在互联网上也几乎找不到任何信息。如果有人能帮我解决这个问题，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78668142/yolov9-classification</guid>
      <pubDate>Tue, 25 Jun 2024 15:01:39 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关 [关闭]</title>
      <link>https://stackoverflow.com/questions/78667972/machine-learning-related</link>
      <description><![CDATA[我有一个数据集，其中包含对某个问题的回答，其中的选项包括：1. 从不，2. 很少，3. 有时，4. 经常，5. 总是。我想评估这个数据集，创建一个机器学习模型，并根据用户响应预测分数。这可能吗？此外，该模型能否根据聊天机器人中的用户响应预测分数？您能帮忙吗？
我已尝试以下步骤
使用独热编码将分类响应转换为数字。
将数据集拆分为训练集和测试集。
训练随机森林分类器。
评估模型的准确性。
提供一个根据用户响应预测分数的函数，该函数可以集成到聊天机器人中。]]></description>
      <guid>https://stackoverflow.com/questions/78667972/machine-learning-related</guid>
      <pubDate>Tue, 25 Jun 2024 14:30:24 GMT</pubDate>
    </item>
    <item>
      <title>Yolo V5 卡住了，在 Google Colab 中也没有显示任何错误</title>
      <link>https://stackoverflow.com/questions/78667865/yolo-v5-gets-stuck-doesnt-show-any-error-too-in-google-colab</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78667865/yolo-v5-gets-stuck-doesnt-show-any-error-too-in-google-colab</guid>
      <pubDate>Tue, 25 Jun 2024 14:08:38 GMT</pubDate>
    </item>
    <item>
      <title>如何避免 nan 损失（从第一次迭代开始）和梯度为无？</title>
      <link>https://stackoverflow.com/questions/78667734/how-to-avoid-a-nan-loss-from-the-first-iteration-and-gradients-being-none</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78667734/how-to-avoid-a-nan-loss-from-the-first-iteration-and-gradients-being-none</guid>
      <pubDate>Tue, 25 Jun 2024 13:40:11 GMT</pubDate>
    </item>
    <item>
      <title>用于放射性核素识别的 CNN：如何使用 CategoricalCrossentropy 进行损失和使用指标进行性能测试来规范化数据？</title>
      <link>https://stackoverflow.com/questions/78667148/cnn-for-radionuclide-identification-how-to-normalize-data-using-categoricalcro</link>
      <description><![CDATA[目前，在阅读了一些与此类工作相关的论文后，我已经使用伽马光谱制作了一个用于放射性核素识别的 CNN 模型。该模型应该用于多标签分类。
第一层是 CNN 层（Conv1D），它将整个伽马光谱作为输入。
下面显示了一个伽马光谱示例...

注意 - 该模型不将图像作为输入。它将每个通道的计数作为输入。
我应该使用什么方法来规范化这种类型的数据？从我读过的论文来看，似乎不对伽马光谱进行归一化会产生最佳结果，但对于之前做过 ML 的人来说，对此类数据进行归一化似乎很重要。
由于我正在制作一个进行多标签分类的模型来检测伽马光谱中存在的放射性核素，我应该使用什么损失函数？看了论文 1 后，似乎我应该使用 CategoricalCrossentropy，但在网上搜索后，似乎每个人都使用 BinaryCrossentropy。任何建议，因为目前我倾向于 CategoricalCrossentropy。
我倾向于使用 CategoricalCrossentropy 作为损失函数的原因在于论文和这样一个事实：如果模型选择了一种存在于伽马光谱中的放射性核素，那么它应该会影响它所预测的其他放射性核素。
最后，由于我正在做多标签分类，我正在考虑使用 F1 分数作为我的指标来确定我的模型在放射性核素识别方面是否表现良好，但这似乎不是最好的指标，因为我希望我的模型能够预测伽马光谱中存在的所有放射性核素。而且准确率在多标签分类中并没有真正用到。
我正在用 TensorFlow 编写代码，因为我一般遵循论文 1。
我读过的一些论文 -
(1) 使用深度学习和通道注意模块进行多放射性核素识别和视觉解释
(2) NaI γ 光谱的放射性同位素识别算法
(3) 使用深度学习进行同位素识别：解释
(4) 伽马射线光谱中放射性核素的自动实时识别：一种基于卷积神经网络的新方法，使用合成数据集训练
等等]]></description>
      <guid>https://stackoverflow.com/questions/78667148/cnn-for-radionuclide-identification-how-to-normalize-data-using-categoricalcro</guid>
      <pubDate>Tue, 25 Jun 2024 11:46:49 GMT</pubDate>
    </item>
    <item>
      <title>为给定的文本生成标签</title>
      <link>https://stackoverflow.com/questions/78667039/generate-tags-for-a-text-given</link>
      <description><![CDATA[我正在尝试使用 flask 构建一个 API，它将从给定的 URL 中提取文本并为该文本生成有效标签。例如，文本是咖喱鸡的食谱，有效标签可以是食谱、印度菜、食物等。
我试过 nltk 库、TF-IDF 矢量化器等，但它们都在分析单词的最大频率，而不是生成新单词。
有没有人能解决这个问题。
我也尝试过使用 gtp2 模块，但输出不是我期望的
import request
from bs4 import BeautifulSoup
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from flask import Flask, request, jsonify

app = Flask(__name__)

# 加载预先训练的 GPT-2 模型和 tokenizer
model_name = &#39;gpt2-medium&#39;
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

def fetch_text_from_url(url):
try:
response = request.get(url)
response.raise_for_status() # 对错误响应引发 HTTPError
soup = BeautifulSoup(response.content, &#39;html.parser&#39;)

# 查找所有段落并连接它们的文本
passages = soup.find_all(&#39;p&#39;)
text = &#39; &#39;.join([para.get_text() for para in passages])

return text
except request.exceptions.RequestException as e:
print(f&quot;Error fetching content from {url}: {str(e)}&quot;)
return None
except Exception as e:
print(f&quot;Error parsing content from {url}: {str(e)}&quot;)
return None

def generate_tags(text, max_length=20, num_return_sequences=3):
try:
# 创建提示以生成标签
prompt = &quot;Tags for this text: &quot;

# 对输入文本和提示进行标记
input_ids = tokenizer.encode(prompt + text, return_tensors=&#39;pt&#39;)

# 使用 GPT-2 模型生成标签
output = model.generate(
input_ids,
max_length=max_length + len(input_ids[0]),
num_return_sequences=num_return_sequences,
num_beams=5,
no_repeat_ngram_size=2,
early_stopping=True
)

# 解码生成的标签
tags = []
for seq in output:
coded_seq = tokenizer.decode(seq, skip_special_tokens=True).strip()
# 从生成的文本中提取标签
tags.extend([t.strip() for t incoded_seq.split() if t.startswith(&#39;#&#39;)])

return tags

except Exception as e:
print(f&quot;生成标签时出错：{str(e)}&quot;)
return None

@app.route(&#39;/generate_tags&#39;, methods=[&#39;POST&#39;])
def generate_tags_api():
data = request.get_json()
url = data.get(&#39;url&#39;)
if not url:
return jsonify({&#39;error&#39;: &#39;URL 是必需的&#39;}), 400

try:
text = fetch_text_from_url(url)
if not text:
return jsonify({&#39;error&#39;: &#39;无法从 URL 获取内容&#39;}), 500

tags = generate_tags(text)
if tags:
return jsonify({&#39;tags&#39;: tags})
else:
return jsonify({&#39;error&#39;: &#39;无法从 URL 生成标签&#39;}), 500
except Exception as e:
return jsonify({&#39;error&#39;: str(e)}), 500

如果 __name__ == &quot;__main__&quot;:
app.run(port=8000, debug=True)
]]></description>
      <guid>https://stackoverflow.com/questions/78667039/generate-tags-for-a-text-given</guid>
      <pubDate>Tue, 25 Jun 2024 11:22:18 GMT</pubDate>
    </item>
    <item>
      <title>Keras model.export() 因模型中的 NoneType 形状而失败</title>
      <link>https://stackoverflow.com/questions/78666998/keras-model-export-fails-because-of-nonetype-shapes-in-model</link>
      <description><![CDATA[我正尝试微调 keras_cv 库中的 DeepLabV3Plus 模型以用于自定义数据集，但在尝试导出为 SavedModel 格式时，我收到此错误：
文件“C:\Users\u\.pyenv\pyenv-win\versions\3.10.2\lib\site-packages\keras\src\utils\traceback_utils.py”，第 731 行，位于 error_handler *
return fn(*args, **kwargs)

TypeError：调用 UpSampling2D.call() 时遇到异常。

* 不支持的操作数类型：&#39;NoneType&#39; 和 &#39;int&#39;

UpSampling2D.call() 收到的参数：
• input=tf.Tensor(shape=(None, None, None, 256), dtype=float32)

这很令人困惑，因为我在调用 DeepLabV3Plus.from_preset 时已将模型的输入形状指定为 [224,224,3]，但模型摘要显示所有层的 None 形状（请参阅此处了解 model.summary() 输出）。但是，从我看到的笔记本来看，即使您指定了输入形状，这也是预期的行为。
至于训练脚本，这是我使用的代码：
model = keras_cv.models.DeepLabV3Plus.from_preset(
&quot;mobilenet_v3_large_imagenet&quot;,
num_classes=NUM_CLASSES,
input_shape=[224,224,3],
load_weights=True
)

layers_to_train = 1
def disable_training(x): x.trainable = False
[disable_training(layer) for layer in model.layers[:-layers_to_train]]
model.summary()

model.compile(
optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
loss=[keras.losses.CategoricalFocalCrossentropy(from_logits=False, alpha=class_weights, gamma=3)],
metrics=[keras.metrics.OneHotMeanIoU(num_classes=NUM_CLASSES), &#39;accuracy&#39;])

callback_cyclic = CyclicLR(base_lr = LEARNING_RATE, max_lr = MAX_LEARNING_RATE, step_size=STEP_SIZE, mode = CYCLIC_MODE)

history = model.fit(train_dataset, epochs=NUM_EPOCHS, batch_size=NUM_BATCH, validation_data=val_dataset, callbacks=[callback_cyclic])

model.export(savepath_dir+&quot;model.tf&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78666998/keras-model-export-fails-because-of-nonetype-shapes-in-model</guid>
      <pubDate>Tue, 25 Jun 2024 11:14:12 GMT</pubDate>
    </item>
    <item>
      <title>Megatron-LM 用于学习目的 [关闭]</title>
      <link>https://stackoverflow.com/questions/78666975/megatron-lm-for-learning-purpose</link>
      <description><![CDATA[我想在本地机器上使用 GatorTron LM 进行学习。我从 https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og 下载了文件（config.json、hparam.yml、MegtronBERT.nemo、MegatronBERT.pt、vocab.txt）。我还有 NVIDIA GPU。我应该如何向模型发送提示消息。
嗨，我想在本地机器上使用 GatorTron LM 进行学习。我从 https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og 下载了文件（config.json、hparam.yml、MegtronBERT.nemo、MegatronBERT.pt、vocab.txt）。我还有 NVIDIA GPU。我应该如何向模型发送提示消息。]]></description>
      <guid>https://stackoverflow.com/questions/78666975/megatron-lm-for-learning-purpose</guid>
      <pubDate>Tue, 25 Jun 2024 11:08:50 GMT</pubDate>
    </item>
    <item>
      <title>元特征分析：根据可用内存拆分数据进行计算</title>
      <link>https://stackoverflow.com/questions/78666961/meta-feature-analysis-split-data-for-computation-on-available-memory</link>
      <description><![CDATA[我正在使用元特征提取器包：pymfe进行复杂性分析。
例如，在小型数据集上，这不是问题。
pip install -U pymfe

from sklearn.datasets import make_classification
from sklearn.datasets import load_iris
from pymfe.mfe import MFE

data = load_iris()
X= data.data
y = data.target

extractor = MFE(features=[ &quot;t1&quot;], groups=[&quot;complexity&quot;],
summary=[&quot;min&quot;, &quot;max&quot;, &quot;mean&quot;, &quot;sd&quot;])
extractor.fit(X,y)
extractor.extract()
([&#39;t1&#39;], [0.12])

我的数据集很大 (32690, 80)，由于内存过大，此计算被终止 用法。 我在 Ubuntu 24.04 上工作，有 32GB RAM。
要重现场景：
# 生成数据集
X, y = make_classification(n_samples=20_000,n_features=80,
n_informative=60, n_classes=5, random_state=42)

extractor = MFE(features=[ &quot;t1&quot;], groups=[&quot;complexity&quot;],
summary=[&quot;min&quot;, &quot;max&quot;, &quot;mean&quot;, &quot;sd&quot;])
extractor.fit(X,y)
extractor.extract()
Killed

问题：
如何我可以将此任务拆分为数据集的小分区进行计算，并合并最终结果（平均）？]]></description>
      <guid>https://stackoverflow.com/questions/78666961/meta-feature-analysis-split-data-for-computation-on-available-memory</guid>
      <pubDate>Tue, 25 Jun 2024 11:05:09 GMT</pubDate>
    </item>
    <item>
      <title>当打开 cv 和 spyder 时如何安装 MASK RCNN？</title>
      <link>https://stackoverflow.com/questions/78666765/how-to-install-mask-rcnn-when-have-open-cv-and-spyder</link>
      <description><![CDATA[嗨，亲爱的，我正在努力通过 MASK RCNN 方法提取图像中的形状。但是当我尝试安装 maskrcnn 时，挑战就出现了，因为它与 spyder 3.11 版本不兼容。所以我去了 python 网站下载它，但适用于 maskrcnn 的 3.7 和 3.6 python 版本已经结束了。我无法下载它们来使用 mask rcnn。我该如何解决这个问题？
谢谢，我花了时间。
我使用 anaconda prompt 进行安装，但没用。每次我运行包含 maskrcnn 的代码时，我都会收到错误，提示没有找到 keras 的模块......................................................................................................................................................]]></description>
      <guid>https://stackoverflow.com/questions/78666765/how-to-install-mask-rcnn-when-have-open-cv-and-spyder</guid>
      <pubDate>Tue, 25 Jun 2024 10:27:35 GMT</pubDate>
    </item>
    <item>
      <title>无法安装 tensorflow_addons</title>
      <link>https://stackoverflow.com/questions/78666757/unable-to-install-tensorflow-addons</link>
      <description><![CDATA[我试图运行此命令- onnx-tf convert -i yolov8n.onnx -o yolov8_tf
然后我遇到了这个错误-
2024-06-25 15:20:44.150147：我 tensorflow/core/util/port.cc:113] oneDNN 自定义操作已打开。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 TF_ENABLE_ONEDNN_OPTS=0。
2024-06-25 15:20:50.272876：我 tensorflow/core/util/port.cc:113] oneDNN 自定义操作已打开。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 TF_ENABLE_ONEDNN_OPTS=0。
回溯（最近一次调用）：
文件“”，第 198 行，位于 run_module_as_main
文件“”，第 88 行，位于 run_code
文件“C:\Users\aasth\AppData\Roaming\Python\Python312\Scripts\onnx-tf.exe_main.py”，第 4 行，位于
文件“C:\Users\aasth\AppData\Roaming\Python\Python312\site-packages\onnx_tf_init.py”，第 1 行，位于 
来自 。导入后端
文件“C:\Users\aasth\AppData\Roaming\Python\Python312\site-packages\onnx_tf\backend.py”，第 29 行，在
来自 onnx_tf.common.handler_helper 导入 get_all_backend_handlers
文件“C:\Users\aasth\AppData\Roaming\Python\Python312\site-packages\onnx_tf\common\handler_helper.py”，第 3 行，在
来自 onnx_tf.handlers.backend 导入 * # noqa
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件&quot;C:\Users\aasth\AppData\Roaming\Python\Python312\site-packages\onnx_tf\handlers\backend\hardmax.py&quot;，第 3 行，在 
import tensorflow_addons as tfa
ModuleNotFoundError：没有名为“tensorflow_addons”的模块
我尝试使用 - pip install tensorflow-addons 安装模块
然后我收到此错误 - 默认为用户安装，因为正常的站点包不可写
错误：找不到满足要求 tensorflow-addons 的版本（来自版本：无）
错误：未找到与 tensorflow-addons 匹配的发行版
我也尝试使用.whl 文件进行安装，但遇到了同样的错误。
tensorflow 版本是 2.16
python 版本是 3.12]]></description>
      <guid>https://stackoverflow.com/questions/78666757/unable-to-install-tensorflow-addons</guid>
      <pubDate>Tue, 25 Jun 2024 10:26:35 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 CV/ML 识别重叠图片中的不同符号？</title>
      <link>https://stackoverflow.com/questions/78666370/how-to-recognize-different-symbols-in-a-picture-with-overlapping-using-cv-ml</link>
      <description><![CDATA[我有一张包含每个项目里程碑信息的图片。每个里程碑都用独特的符号在图表中标记。在图片中，我们有不同的三角形或正方形，颜色也不同。
我需要从图例中的符号中提取有关日期的信息。这意味着我需要知道每个符号及其图例的位置。然后在 x 轴和 y 轴上搜索临近日期。
里程碑图片
我尝试过使用 cv2 进行颜色识别。首先，我提取了 x-y 轴上的日期并保存了它们的位置。我对黄色菱形使用 cv2，效果很好。但是当涉及到红色或灰色时，因为有不同的符号，但都是红色。我无法仅使用 cv2 来区分它们。
cv2 中的传统计算机视觉方法无法处理重叠。例如有两种类型的红色三角形，一种指向左侧。面对所有这些问题，我无法找到一种好的方法来识别图例中的每个符号。你有什么建议吗？
也许训练 ML 是一个潜在的解决方案。我有大约 1000 张这样的图片。我不知道训练模型是否足够。但工作量也会很大。]]></description>
      <guid>https://stackoverflow.com/questions/78666370/how-to-recognize-different-symbols-in-a-picture-with-overlapping-using-cv-ml</guid>
      <pubDate>Tue, 25 Jun 2024 09:04:21 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法用比率来测量用于创建第三个特征的两个特征的初始 SHAP 值？</title>
      <link>https://stackoverflow.com/questions/78662828/is-there-a-way-of-measuring-the-initial-shap-value-of-two-features-that-have-bee</link>
      <description><![CDATA[在 Python 机器学习、回归或分类等背景下，我有时会计算两个特征的比例来得出第三个特征，并删除前两个特征。例如：
living_surface_ratio = (
Residential_building__total_living_surface 
/ building__footprint_surface
)

因此，在我的数据处理管道的末端，我只有 living_surface_ratio 作为一个特征。
我想知道是否有办法测量 residential_building__total_living_surface 和 building__footprint_surface 的形状值，尽管这些特征已被删除，因此不再可用。
也许：
shap(living_surface_ratio) = (
0.3 * shap(residential_building__total_living_surface) 
+ 0.7 * shap(building__footprint_surface)
)

我考虑过先用这两个特征训练模型，然后再用比例来训练，但让我困扰的是，我并没有真正提供特征比例模型的可解释性。
我研究过 SHAP 值的实现，但修改起来似乎很复杂。]]></description>
      <guid>https://stackoverflow.com/questions/78662828/is-there-a-way-of-measuring-the-initial-shap-value-of-two-features-that-have-bee</guid>
      <pubDate>Mon, 24 Jun 2024 13:36:28 GMT</pubDate>
    </item>
    <item>
      <title>删除边界框detectron2之外的所有内容</title>
      <link>https://stackoverflow.com/questions/78626157/delete-everything-outside-of-bounding-boxes-detectron2</link>
      <description><![CDATA[我已经训练了一个包含一个类的 detectron2 模型。现在我想将不在 bbox 内的所有内容设置为白色，其余部分保持原样。一张图片上可以有多个 bbox，它们可以叠加。
我阅读了 detectron2 的文档以及 cv2，但我找不到解决问题的方法。
这是我的预测代码：
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
import cv2

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(&#39;COCO-Detection/tmp&#39;))
cfg.MODEL.WEIGHTS = &#39;tmp&#39;

预测器 = DefaultPredictor(cfg)

img = cv2.imread(&#39;tmp&#39;)

out = 预测器(img)
]]></description>
      <guid>https://stackoverflow.com/questions/78626157/delete-everything-outside-of-bounding-boxes-detectron2</guid>
      <pubDate>Sat, 15 Jun 2024 09:09:50 GMT</pubDate>
    </item>
    <item>
      <title>如何将媒体管道 3D 关键点转换为骨架的欧拉角？</title>
      <link>https://stackoverflow.com/questions/78424171/how-can-i-convert-mediapipe-3d-keypoints-to-eular-angles-of-a-skeleton</link>
      <description><![CDATA[我有一个代码片段，用于将 openpose 3D 关键点转换为骨架的欧拉角。从 openpose 检测到的关键点可以在下图中看到：

将这些 3D 关键点转换为骨架的欧拉角的代码是：
def pose2euler(self, pose, header):
channel = []
quats = {}
eulers = {}
stack = [header.root]
while stack:
node = stack.pop()
joint = node.name
joint_idx = self.keypoint2index[joint]

if node.is_root:
channel.extend(pose[joint_idx])

index = self.keypoint2index
order = None
if joint == &#39;臀部&#39;：
x_dir = pose[index[&#39;左臀部&#39;]] - pose[index[&#39;右臀部&#39;]]
y_dir = 无
z_dir = pose[index[&#39;脊柱&#39;]] - pose[joint_idx]
order = &#39;zyx&#39;
elif 关节在 [&#39;右臀部&#39;，&#39;右膝&#39;]：
child_idx = self.keypoint2index[node.children[0].name]
x_dir = pose[index[&#39;臀部&#39;]] - pose[index[&#39;右臀部&#39;]]
y_dir = 无
z_dir = pose[joint_idx] - pose[child_idx]
order = &#39;zyx&#39;
elif 关节在 [&#39;左臀部&#39;，&#39;左膝&#39;]：
child_idx = self.keypoint2index[node.children[0].name]
x_dir = pose[index[&#39;左臀部&#39;]] - pose[index[&#39;臀部&#39;]]
y_dir = 无
z_dir = pose[joint_idx] - pose[child_idx]
order = &#39;zyx&#39;
elif 关节 == &#39;脊椎&#39;:
x_dir = pose[index[&#39;左髋&#39;]] - pose[index[&#39;右髋&#39;]]
y_dir = 无
z_dir = pose[index[&#39;胸部&#39;]] - pose[joint_idx]
order = &#39;zyx&#39;
elif 关节 == &#39;胸部&#39;:
x_dir = pose[index[&#39;左肩&#39;]] - \
pose[index[&#39;右肩&#39;]]
y_dir = 无
z_dir = pose[joint_idx] - pose[index[&#39;脊椎&#39;]]
order = &#39;zyx&#39;
elif 关节 == &#39;颈部&#39;:
x_dir = 无
y_dir = pose[index[&#39;胸廓&#39;]] - pose[joint_idx]
z_dir = pose[index[&#39;头端站点&#39;]] - pose[index[&#39;胸廓&#39;]]
order = &#39;zxy&#39;
elif 关节 == &#39;左肩&#39;:
x_dir = pose[index[&#39;左肘&#39;]] - pose[joint_idx]
y_dir = pose[index[&#39;左肘&#39;]] - pose[index[&#39;左腕&#39;]]
z_dir = None
order = &#39;xzy&#39;
elif 关节 == &#39;左肘&#39;:
x_dir = pose[index[&#39;左腕&#39;]] - pose[joint_idx]
y_dir = pose[joint_idx] - pose[index[&#39;左肩&#39;]]
z_dir = None
order = &#39;xzy&#39;
elif 关节 == &#39;右肩&#39;:
x_dir = pose[joint_idx] - pose[index[&#39;右肘&#39;]]
y_dir = pose[index[&#39;右肘&#39;]] - pose[index[&#39;右腕&#39;]]
z_dir = 无
order = &#39;xzy&#39;
elif joint == &#39;右肘&#39;:
x_dir = pose[joint_idx] - pose[index[&#39;右腕&#39;]]
y_dir = pose[joint_idx] - pose[index[&#39;右肩&#39;]]
z_dir = 无
order = &#39;xzy&#39;
if order:
dcm = math3d.dcm_from_axis(x_dir, y_dir, z_dir, order)
quats[joint] = math3d.dcm2quat(dcm)
else:
quats[joint] = quats[self.parent[joint]].copy()

local_quat = quats[joint].copy()
if node.parent:
local_quat = math3d.quat_divide(
q=quats[joint], r=quats[node.parent.name]
)

euler = math3d.quat2euler(
q=local_quat, order=node.rotation_order
)
euler = np.rad2deg(euler)
eulers[joint] = euler
channel.extend(euler)

for child in node.children[::-1]:
if not child.is_end_site:
stack.append(child)

return channel

现在我想实现 mediapipe 关键点的欧拉角，但我看不懂上面的代码。 x_dir、y_dir 和 z_dir 是如何计算的？为什么？有人能给我解释一下吗？我正在做一个项目，已经在这里呆了 4 周了。Mediapipe 计算出的关键点如下图所示：

仅适用于姿势关键点，不适用于面部。]]></description>
      <guid>https://stackoverflow.com/questions/78424171/how-can-i-convert-mediapipe-3d-keypoints-to-eular-angles-of-a-skeleton</guid>
      <pubDate>Fri, 03 May 2024 10:38:56 GMT</pubDate>
    </item>
    </channel>
</rss>