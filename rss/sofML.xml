<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 18 Jan 2024 21:13:01 GMT</lastBuildDate>
    <item>
      <title>ValidationError：LLMChain 出现 2 个验证错误</title>
      <link>https://stackoverflow.com/questions/77842203/validationerror-2-validation-errors-for-llmchain</link>
      <description><![CDATA[这是我的完整代码：
!pip install -q Transformers einops 加速 langchain BitsandBytes Sentence_Transformers faiss-cpu pypdf Sentpiece
从 langchain 导入 HuggingFacePipeline
从 Transformer 导入 AutoTokenizer
从 langchain.embeddings 导入 HuggingFaceEmbeddings
从 langchain.document_loaders.csv_loader 导入 CSVLoader
从 langchain.vectorstores 导入 FAISS、Chroma
从 langchain.chains 导入 RetrievalQA
从 langchain.prompts 导入 PromptTemplate
从 langchain.chains 导入 ConversationalRetrievalChain
从 langchain.chains.question_answering 导入 load_qa_chain
从 langchain.memory 导入 ConversationBufferMemory
进口加速
进口变压器
进口火炬
导入文本换行
loader = CSVLoader(&#39;/kaggle/input/csvdata/chatdata.csv&#39;, 编码=“utf-8”, csv_args={&#39;分隔符&#39;: &#39;,&#39;})
数据 = 加载器.load()

嵌入 = HuggingFaceEmbeddings(model_name=&#39;sentence-transformers/all-MiniLM-L6-v2&#39;,model_kwargs={&#39;device&#39;: &#39;cpu&#39;})

db = FAISS.from_documents(数据，嵌入)


#Mistral 7B 模型 llm

进口火炬
从变压器进口（
    AutoModelForCausalLM，
    自动标记器，
    生成配置，
    文本流媒体,
    管道，
）

MODEL_NAME =“mistralai/Mistral-7B-Instruct-v0.1”

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
模型 = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME、device_map=“自动”、torch_dtype=torch.float16、load_in_8bit=True
）

Generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
Generation_config.max_new_tokens = 1024
Generation_config.温度 = 0.0001
Generation_config.do_sample = True
流光= TextStreamer（标记器，skip_prompt = True，skip_special_tokens = True）


llm = 管道(
    “文本生成”，
    型号=型号，
    分词器=分词器，
    return_full_text=真，
    Generation_config = Generation_config，
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id,
    流光=流光，
）


def format_prompt（提示，system_prompt =“”）：
    如果 system_prompt.strip():
        return f“[INST] {system_prompt} {prompt} [/INST]”
    return f“[INST] {提示} [/INST]”


SYSTEM_PROMPT = “””
您是一名临床数据科学家和数据分析师，专门从事统计数据分析和报告生成。您的使命是为医疗保健和临床研究提供准确且富有洞察力的数据驱动解决方案。当您做出回应时，请发挥临床数据科学领域经验丰富的数据专业人员所特有的专业知识和精确度。
如果您遇到没有必要信息的问题，请务必不要提供推测性或不准确的答案。
“”“”.strip()

链 = ConversationalRetrievalChain.from_llm(
    嗯，
    chain_type=“东西”，
    检索器=db.as_retriever(),
    return_source_documents=真，
    详细=真，
）

这里我面临错误：
ValidationError：LLMChain 出现 2 个验证错误
勒姆
  预期的 Runnable 实例（type=type_error.任意_type；expected_任意_type=Runnable）
勒姆
  预期的 Runnable 实例（type=type_error.任意_type；expected_任意_type=Runnable）


从 textwrap 导入填充

结果=链（输入（“临床试验平面计ChatBot ---”）
）
打印（填充（结果[“结果”].strip（），宽度= 80））

此 llm 链被编程为使用 llm、矢量数据库和提示与 csv 聊天，我在运行 ConversationalRetrievalChain 时遇到上述错误]]></description>
      <guid>https://stackoverflow.com/questions/77842203/validationerror-2-validation-errors-for-llmchain</guid>
      <pubDate>Thu, 18 Jan 2024 20:20:48 GMT</pubDate>
    </item>
    <item>
      <title>使用网格时的机器学习[关闭]</title>
      <link>https://stackoverflow.com/questions/77842088/machine-learning-when-working-with-meshes</link>
      <description><![CDATA[假设有一个类似于 Stormworks 的编辑器，您可以在其中用零件建造车辆。教人工智能用此类车辆制作带有动画和其他参数的普通游戏网格有多现实和困难？]]></description>
      <guid>https://stackoverflow.com/questions/77842088/machine-learning-when-working-with-meshes</guid>
      <pubDate>Thu, 18 Jan 2024 19:52:27 GMT</pubDate>
    </item>
    <item>
      <title>寻找人来帮助我玩堆叠游戏[关闭]</title>
      <link>https://stackoverflow.com/questions/77841942/searching-for-someone-to-help-me-with-a-stacking-game</link>
      <description><![CDATA[我正在寻找付费某人制作一款软件，该软件可以使用相机/屏幕记录（屏幕记录更好）来击败堆叠游戏，例如所附的游戏。我可以根据需要提供其他信息。 堆栈游戏
我对编码是相对论新手，所以我什么也没做，也没有期待任何事情。]]></description>
      <guid>https://stackoverflow.com/questions/77841942/searching-for-someone-to-help-me-with-a-stacking-game</guid>
      <pubDate>Thu, 18 Jan 2024 19:22:40 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 在具有多输出分类器的不平衡数据集上如何使其正常工作？</title>
      <link>https://stackoverflow.com/questions/77841917/xgboost-on-unbalanced-dataset-with-multioutput-classifier-how-to-make-it-work-pr</link>
      <description><![CDATA[from sklearn.multioutput import MultiOutputClassifier
从 sklearn.pipeline 导入管道
从 xgboost 导入 XGBClassifier

分类器 = MultiOutputClassifier(XGBClassifier())

clf = 管道([(&#39;分类&#39;, 分类器)])

clf.fit(xgbX_train, xgby_train)
打印（clf.score（xgbX_train，xgby_train））

上面的代码无法按我的预期工作，因为我的数据集不平衡。我尝试使用sample_weights对数据集进行加权
从sklearn.utils.class_weight导入compute_sample_weight
样本权重 = 计算样本权重（
    class_weight = &#39;平衡&#39;,
    y =
）

但真的不知道如何定义 y。我的 xgby_train 是一个由 6 个元素组成的向量，元素为 0 或 1。如果它们全部为 0，则它是一类，并且取决于哪个元素是另一类。所以我有 7 节课。]]></description>
      <guid>https://stackoverflow.com/questions/77841917/xgboost-on-unbalanced-dataset-with-multioutput-classifier-how-to-make-it-work-pr</guid>
      <pubDate>Thu, 18 Jan 2024 19:16:38 GMT</pubDate>
    </item>
    <item>
      <title>当输入任何模型时，TFRecord 文件使用过多 RAM</title>
      <link>https://stackoverflow.com/questions/77841705/excessive-ram-usage-for-tfrecord-file-while-feeding-into-any-model</link>
      <description><![CDATA[问题
我有一个 *.tfrecords 文件，我想将其输入到使用 Tensorflow 创建的 ConvLSTM2D 模型中。这是模型结构。
模型 = 顺序([
    ConvLSTM2D(64, (3, 3), 激活=&#39;relu&#39;, input_shape=(20, 224, 224, 3), return_sequences=True),
    批量归一化(),
    ConvLSTM2D(64, (3, 3), 激活=&#39;relu&#39;, return_sequences=True),
    批量归一化(),
    展平（），
    密集（1，激活=&#39;sigmoid&#39;）
]）

当我尝试将数据放入模型中时，它占用了所有系统内存。
在 M1 MacBook 2020（Jupyter Notebook、Pycharm）、Google Colab 上测试。
model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
model.fit(train_input_fn(),steps_per_epoch=5,validation_data=val_input_fn(),epochs=10)

我们在做什么
我们有上海数据集，其中包含战斗和非战斗数据集。因此，我们尝试使用卷积长短期记忆来预测和分类打斗和非打斗视频。
我们有 800 个火车视频。我们以 250 毫秒的间隔捕获帧并将所有帧转换为 numpy 数组。然后我们将所有数组存储在 TFRecord 文件中。
当我们将数据集传递到模型中时，我们使用这个函数 train_input_fn() 来读取 tfrecord 文件并将数据传递到我们的模型中。
&lt;小时/&gt;
您可以在此处查看 Colab 笔记本
数据集结构如下：
数据集
    - 火车
        - Fight # 有 800 个 *.avi 文件
        - NonFight # 有 800 个 *.avi 文件
    - 值
        - Fight # 有 200 个 *.avi 文件
        - NonFight # 有 200 个 *.avi 文件


我们尝试过什么？

我们尝试将 batch_size 从 64 减少到 16。
将整个数据集从训练集中的 800 个视频减少到 200 个视频
尝试减小 ConvLSTM2D 的滤波器大小
对 *.mp4 做了所有相同的事情
减少了一层 BatchNormalization() 和 ConvLSTM2D
]]></description>
      <guid>https://stackoverflow.com/questions/77841705/excessive-ram-usage-for-tfrecord-file-while-feeding-into-any-model</guid>
      <pubDate>Thu, 18 Jan 2024 18:36:04 GMT</pubDate>
    </item>
    <item>
      <title>nn.参数在训练期间不更新[关闭]</title>
      <link>https://stackoverflow.com/questions/77841694/nn-parameter-does-not-update-during-training</link>
      <description><![CDATA[我正在尝试编写 PyGeometric MessagePassing 层，如下所示：
类 EdgeNNLayer(MessagePassing):
    def __init__(自身, d0=45/60):
        super().__init__(aggr=&#39;mean&#39;)
        self.d0 = d0
        self.beta = 参数(torch.empty(1))
        self.reset_parameters()
        self.weight_aggr = aggr.SumAggregation()
    
    def重置参数（自身）：
        print(&quot;重置参数&quot;)
        self.beta.data[0] = 1 / self.d0
    
    def 消息（自身，x_i，x_j，edge_attr）：
        beta = self.beta.clone()
        返回 x_j * torch.exp(-beta * edge_attr[:, 0:1]), torch.exp(-beta * edge_attr[:, 0:1])
    
    defforward（自身，x，edge_index，edge_attr）：
        edge_index，edge_attr = add_self_loops（edge_index，edge_attr，fill_value = 0，num_nodes = x.shape [0]）
        返回 self.propagate(edge_index, x=x, edge_attr=edge_attr)
        
    def 聚合（自我，输入，索引，ptr =无，dim_size =无）：
        值、权重 = 输入
        return self.aggr_module(vals,index,ptr=ptr,dim_size=dim_size,dim=self.node_dim)#/self.weight_aggr(权重,index,ptr,dim_size)

当我训练该层（使用该层更新 PyG 图后使用的一些其他 Linear 层）时，我看到网络的所有其他可训练元素均由 beta&lt; 更新/code&gt; 在上面的层中不是。
我尝试将上面的 __init__ 中的行更改为：
 self.beta = 参数(torch.empty(1))
        self.beta.retain_grad()
        打印(self.beta.retains_grad)
        打印（自我.beta.grad）

我得到False和None，这很奇怪。当我在每个纪元之后打印它时，我也会得到这些。
我尝试以几种不同的方式更改代码，包括更改克隆、注释掉几行、更改该层所使用的整体网络结构等。]]></description>
      <guid>https://stackoverflow.com/questions/77841694/nn-parameter-does-not-update-during-training</guid>
      <pubDate>Thu, 18 Jan 2024 18:34:13 GMT</pubDate>
    </item>
    <item>
      <title>ML .NET 训练异常 [关闭]</title>
      <link>https://stackoverflow.com/questions/77840868/ml-net-exception-on-training</link>
      <description><![CDATA[我正在使用模型生成器 UI，选择 NLP 文本分类。
目标是尝试“清洁”数据丑陋，所以我制作了一个金融交易的样本数据集。这个想法是输入 Amazon Digit*8SDFS98 amzn.com/bill WA Am 应该输出：“Amazon Digital”。
但是我在训练后收到此错误。我认为数据有问题（除了它根本不是一个大数据集，但它都是模拟数据，只是为了学习这些东西）。
在此处输入图像描述
重现步骤。

使用此数据（或喜欢它）创建 csv。
打开 VS 并添加机器学习模型
选择“文本分类”如场景所示。
选择清理后的描述作为标签（目标）
选择“描述”作为数据
点击“下一步”去火车。
开始训练。
收到错误。

有没有办法从这个错误中找出实际问题是什么？
 位于 System.RuntimeMethodHandle.InvokeMethod（对象目标、对象[] 参数、签名 sig、布尔构造函数）
   在 System.Reflection.RuntimeMethodInfo.UnsafeInvokeInternal（对象 obj，对象 [] 参数，对象 [] 参数）
   在System.Reflection.RuntimeMethodInfo.Invoke（对象obj，BindingFlags invokeAttr，Binder活页夹，Object []参数，CultureInfo文化）
   在 Microsoft.ML.Runtime.ComponentCatalog.LoadableClassInfo.CreateInstanceCore(Object[] ctorArgs)
   在 Microsoft.ML.Runtime.ComponentCatalog.TryCreateInstance[TRes]（IHostEnvironment env，类型签名类型，TRes&amp;结果，字符串名称，字符串选项，Object[] 额外）
   在 Microsoft.ML.Runtime.ComponentCatalog.TryCreateInstance[TRes,TSig](IHostEnvironment env, TRes&amp; 结果，字符串名称，字符串选项，Object[] extra)
   在 Microsoft.ML.ModelLoadContext.TryLoadModelCore[TRes,TSig](IHostEnvironment env, TRes&amp; result, Object[] extra)
   在 Microsoft.ML.ModelLoadContext.TryLoadModel[TRes,TSig](IHostEnvironment env、TRes&amp; 结果、RepositoryReader 代表、Entry ent、String dir、Object[] extra)
   在 Microsoft.ML.ModelLoadContext.LoadModel[TRes,TSig](IHostEnvironment env、TRes&amp; 结果、RepositoryReader 代表、Entry ent、String dir、Object[] extra)
   在 Microsoft.ML.ModelLoadContext.LoadModelOrNull[TRes,TSig](IHostEnvironment env、TRes&amp; 结果、RepositoryReader 代表、String dir、Object[] extra)
   在 Microsoft.ML.ModelLoadContext.LoadModel[TRes,TSig](IHostEnvironment env、TRes&amp; 结果、RepositoryReader 代表、String dir、Object[] extra)
   在 Microsoft.ML.ModelOperationsCatalog.Load（流流、DataViewSchema&amp; inputSchema）
   在 Microsoft.ML.ModelOperationsCatalog.Load(String filePath, DataViewSchema&amp; inputSchema)
   在 /_/src/Microsoft.ML.ModelBuilder.AutoMLService/ServiceFactory/CodeGeneratorService 中的 Microsoft.ML.ModelBuilder.AutoMLService.ServiceFactory.CodeGeneratorService.SetTorchRunTimeFolderAndLoadModel（ITrainingConfiguration 配置、字符串 modelPath、MLContext 和上下文、ITransformer 和模型、DataViewSchema 和 inputSchema） .cs：第 139 行
   在 /_/src/Microsoft.ML.ModelBuilder 中的 Microsoft.ML.ModelBuilder.AutoMLService.ServiceFactory.CodeGeneratorService.GenerateConsumationAsync（ITrainingConfiguration 配置、字符串trainingConfigurationFolder、字符串nameSpace、字符串className、TargetType 目标、String[] 标签、CancellationToken ct）。 AutoMLService/ServiceFactory/CodeGeneratorService.cs：StreamJsonRpc.JsonRpc 处的第 155 行。d__151`1.MoveNext()
--- 从先前抛出异常的位置开始的堆栈跟踪结束 ---
   在 System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()
   在 System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification（任务任务）
   在 System.Runtime.CompilerServices.TaskAwaiter`1.GetResult()
   在 Microsoft.ML.ModelBuilder.ViewModels.TrainViewModel.d__100.MoveNext()

我尝试清理数据（没有重复值，两列中都没有空格）。]]></description>
      <guid>https://stackoverflow.com/questions/77840868/ml-net-exception-on-training</guid>
      <pubDate>Thu, 18 Jan 2024 16:14:56 GMT</pubDate>
    </item>
    <item>
      <title>CNN 模型训练循环中的批量大小不匹配</title>
      <link>https://stackoverflow.com/questions/77840815/batch-size-in-training-loop-of-a-cnn-model-is-not-matching</link>
      <description><![CDATA[这是我的 CNN 模型，适用于 400x400 的灰度图像：
导入火炬
将 torch.nn 导入为 nn

类 MModel(nn.Module):
    def __init__(自身):
        超级（MModel，自我）.__init__()
        
        # 定义卷积层
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        
        # 计算全连接层之前展平特征图的大小
        self.fc_input_size = 64 * 200 * 200
        
        # 定义全连接层
        self.fc1 = nn.Linear(self.fc_input_size, 128)
        self.fc2 = nn.Linear(128, 18) # 根据您的要求调整输出大小
        
    def 前向（自身，x）：
        # 应用卷积层和池化层
        x = self.pool(nn.function.relu(self.conv1(x)))
        x = self.pool(nn.function.relu(self.conv2(x)))
        
        # 展平特征图
        x = x.view(-1, self.fc_input_size)
        
        # 应用全连接层
        x = nn.function.relu(self.fc1(x))
        x = self.fc2(x)
        
        返回x

# 创建 CNN 模型的实例
模型 = MModel()

这是我的训练循环：
导入火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
从 torch.utils.data 导入 DataLoader
导入 torchvision.transforms 作为变换

标准 = nn.CrossEntropyLoss()
优化器 = optim.Adam(model.parameters(), lr=0.001)

# 训练循环
纪元数 = 10

对于范围内的纪元（num_epochs）：
    运行损失 = 0.0

    对于 i，enumerate(train_DL, 0) 中的数据：
        输入，标签=数据

        # 将参数梯度归零
        优化器.zero_grad()

        # 前向传递
        输出 = 模型（输入）

        # 计算损失
        损失=标准（输出，标签）

        # 向后传递和优化
        loss.backward()
        优化器.step()

        # 打印统计数据
        running_loss += loss.item()

        if i % 10 == 9: # 每 10 个小批量打印一次
            print(f&quot;[{epoch + 1}, {i + 1}] 损失: {running_loss / 10:.3f}&quot;)
            运行损失 = 0.0

print(&quot;训练结束&quot;)

运行此代码时，我收到此错误，我的 DataLoader 的批处理大小为 32：
ValueError：预期输入batch_size (8) 与目标batch_size (32) 匹配。

我尝试将批量大小更改为 8，但这仍然给我带来了值错误。另外，在 StackOverflow 上找到的其他解决方案似乎也不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/77840815/batch-size-in-training-loop-of-a-cnn-model-is-not-matching</guid>
      <pubDate>Thu, 18 Jan 2024 16:05:59 GMT</pubDate>
    </item>
    <item>
      <title>安装适用于 Python 3.11 的 Neurolab</title>
      <link>https://stackoverflow.com/questions/77840804/install-neurolab-for-python-3-11</link>
      <description><![CDATA[我正在尝试将 neurolab 软件包安装到我的 Python 3.11 环境中。
我将 Anaconda 与 Python 版本 3.11 结合使用。在 Anaconda 页面上找到的 Neurolab 的可用版本如下：

但是这些版本都不适用于 Python 3.11 。我尝试通过 Anaconda Navigator 和 CMD（我使用 Windows 11）安装。
如何使用我的 Python 版本安装此软件包（或类似的可用版本）？]]></description>
      <guid>https://stackoverflow.com/questions/77840804/install-neurolab-for-python-3-11</guid>
      <pubDate>Thu, 18 Jan 2024 16:04:02 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 图执行错误：训练期间形状不兼容</title>
      <link>https://stackoverflow.com/questions/77840662/lstm-graph-execution-error-incompatible-shapes-during-training</link>
      <description><![CDATA[我对于使用深度网络还很陌生。我正在尝试使用 LSTM 来使用 keras 执行（我认为是）简单的二元分类。我的数据集由小说中的文本块组成，我使用这些文本进行处理：
def process_data(数据):
    文本 = data[&#39;text&#39;].tolist()
    标签 = data[&#39;label&#39;].tolist()

    tokenizer.fit_on_texts(文本)
    序列 = tokenizer.texts_to_sequences(texts)

    X_pated = pad_sequences(序列, maxlen=2000)
    X_sliding_window, y_sliding_window = create_sliding_window_data(
        X_填充，标签）

    y_train_reshape = np.expand_dims(y_sliding_window, axis=-1)

    返回 X_sliding_window, y_train_reshape

在将其输入 BiLSTM 之前：
def build_model（sequence_length，embedding_dim，batch_size）：
    输入层=输入（形状=（序列长度，
                        embedding_dim),batch_size=batch_size)
    lstm = 双向（
        LSTM(64, return_sequences=True, stateful=True))(input_layer)
    注意力=注意力（）（[lstm，lstm]）
    合并=连接（轴=-1）（[lstm，注意]）
    输出层=密集（1，激活=&#39;sigmoid&#39;）（合并）

    模型=模型（输入=输入层，输出=输出层）
    model.compile(优化器=&#39;亚当&#39;,
                  损失=&#39;binary_crossentropy&#39;，指标=[&#39;准确性&#39;]）

    返回模型

我一直在努力理解为什么会出现以下错误以及如何修复它：
图形执行错误：

在节点gradient_tape/binary_crossentropy/mul/Mul处检测到（最近一次调用最后）：

  文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py”，第 65 行，位于 error_handler 中

  文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py”，第 1807 行，适合

  文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py”，第 1401 行，在 train_function 中

  文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py”，第1384行，在step_function中

  文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py”，第 1373 行，在 run_step 中

  文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py”，第 1154 行，在 train_step 中

  文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py”，第 598 行，最小化

  文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py”，第 656 行，位于 _compute_gradients 中

  文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py”，第 532 行，在 _get_gradients 中

不兼容的形状：[26,1] 与 [64,250]
         [[{{节点gradient_tape/binary_crossentropy/mul/Mul}}]] [操作：__inference_train_function_4934]

我使用的参数是：
&lt;前&gt;&lt;代码&gt;sequence_length = 250
embedding_dim = X_train.shape[2]
批量大小 = 64

我检查了训练形状，即 y_train.shape: (218, 1) X_train.shape: (218, 250, 2000) 对我来说似乎没问题......我不知道在哪里 [26 ,1] 会来自。有人可以帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/77840662/lstm-graph-execution-error-incompatible-shapes-during-training</guid>
      <pubDate>Thu, 18 Jan 2024 15:43:11 GMT</pubDate>
    </item>
    <item>
      <title>如何使用经过训练/测试的线性回归模态进行预测[关闭]</title>
      <link>https://stackoverflow.com/questions/77840461/how-to-make-prediction-with-linear-regression-modal-that-has-been-trained-tested</link>
      <description><![CDATA[我在我的数据集上制作了线性回归模态。但我不知道如何让模态进行预测。
我的数据集的格式为年份、旅程类型、VALUE作为列标题。我想预测数据集中未包含的年份的值（因为我没有此数据）。所以我想预测 2023-2027 年的值。
线性回归的变量是
X = &#39;年份&#39;
y = &#39;VALUE&#39;
我尝试了不同的方法，但似乎都不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/77840461/how-to-make-prediction-with-linear-regression-modal-that-has-been-trained-tested</guid>
      <pubDate>Thu, 18 Jan 2024 15:14:05 GMT</pubDate>
    </item>
    <item>
      <title>模型遭受巨大损失[关闭]</title>
      <link>https://stackoverflow.com/questions/77839031/model-getting-huge-loss</link>
      <description><![CDATA[我正在尝试按标题制作文本生成器。文本最大长度为2500，词典大小为45,000字。这是我正在使用的模型。在训练过程中，损失增加，但准确率保持不变。
那么，我的模型出了什么问题？
&lt;前&gt;&lt;代码&gt;纪元 1/75
1311/1311 [================================] - 461s 347ms/步 - 损失：27230606.0000 - 准确度：0.0382
纪元 2/75
1311/1311 [================================] - 454s 346ms/步 - 损失：78650848.0000 - 准确度：0.0382


我在 anaconda 环境中使用 Tensorflow GPU 和 python 3.11。
模型 = keras.Sequential([
    嵌入（vocab_size，256，input_length = max_sequence_length），
    LSTM（单位= 256，kernel_regularizer = l2（0.01），return_sequences = True），
    LSTM（单位= 128，kernel_regularizer = l2（0.01）），
    密集（max_sequence_length，激活=&#39;softmax&#39;）
]）

亚当 = 亚当(lr=0.01)

model.compile(optimizer=adam,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
model.fit（标题_序列_填充，食谱_序列_填充，纪元= 75，详细= 1，
          回调=[ModelCheckpoint(filepath=Settings.new_model_path)])

我尝试增加单位数量、损失类型​​，但没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/77839031/model-getting-huge-loss</guid>
      <pubDate>Thu, 18 Jan 2024 11:28:10 GMT</pubDate>
    </item>
    <item>
      <title>Python dgl 库 API 更新</title>
      <link>https://stackoverflow.com/questions/77837193/python-dgl-library-api-updates</link>
      <description><![CDATA[这是我的代码：
def 标准化（自我，logits）：
    self.\_logits_name = “\_logits”
    self.\_normalizer_name = “\_norm”
    self.g.edata\[self.\_logits_name\] = logits

    self.g.update_all(fn.copy_u(self._logits_name, self._logits_name),
                     fn.sum(self._logits_name, self._normalizer_name))
    返回 self.g.edata.pop(self._logits_name), self.g.ndata.pop(self._normalizer_name)

def edge_softmax(自身):

    如果 self.l0 == 0:
        分数 = self.softmax(self.g, self.g.edata.pop(&#39;a&#39;))
    别的：
        分数，归一化器 = self.normalize(self.g.edata.pop(&#39;a&#39;))
        self.g.ndata[&#39;z&#39;] = 标准化器[:,0,:].unsqueeze(1)

    self.g.edata[&#39;a&#39;] = 分数[:,0,:].unsqueeze(1)

这是堆栈跟踪：
回溯（最近一次调用最后一次）：
文件“/datasets/\_deepnote_work/train.py”，第 211 行，位于 \ 中
主要（参数）

文件“/datasets/\_deepnote_work/train.py”，第 130 行，在 main 中
logits = 模型（特征）

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1518 行，位于 \_wrapped_call_impl
返回 self.\_call_impl(\*args, \*\*kwargs)

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1527 行，位于 \_call_impl
返回forward_call(\*args, \*\*kwargs)

文件“/datasets/\_deepnote_work/gat.py”，第 209 行，向前
h，边缘 = self.gat_layers\[0\](h，边缘)

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1518 行，位于 \_wrapped_call_impl
返回 self.\_call_impl(\*args, \*\*kwargs)

文件“/root/venv/lib/python3.9/site-packages/torch/nn/modules/module.py”，第 1527 行，位于 \_call_impl
返回forward_call(\*args, \*\*kwargs)

文件“/datasets/\_deepnote_work/gat.py”，第 105 行，向前
self.edge_softmax()

文件“/datasets/\_deepnote_work/gat.py”，第 166 行，edge_softmax
分数，归一化器 = self.normalize(self.g.edata.pop(&#39;a&#39;))

文件“/datasets/\_deepnote_work/gat.py”，第 157 行，标准化
self.g.update_all(fn.copy_u(self.\_logits_name, self.\_logits_name),

文件“/root/venv/lib/python3.9/site-packages/dgl/heterograph.py”，第 5110 行，位于 update_all
ndata = core.message_passing()
文件“/root/venv/lib/python3.9/site-packages/dgl/core.py”，第 398 行，message_passing
ndata = invoke_gspmm(g, mfunc, rfunc)

文件“/root/venv/lib/python3.9/site-packages/dgl/core.py”，第 361 行，invoke_gspmm
x = alldata\[mfunc.target\]\[mfunc.in_field\]

文件“/root/venv/lib/python3.9/site-packages/dgl/view.py”，第 80 行，在 _getitem_ 中
返回 self.\_graph.\_get_n_repr(self.\_ntid, self.\_nodes)\[key\]

文件“/root/venv/lib/python3.9/site-packages/dgl/frame.py”，第 688 行，在 _getitem_ 中
返回 self.\_columns\[name\].data
关键错误：&#39;\_logits&#39;

我查看了DGLEdgeBatch的文档，但没有找到任何解决方案
链接到文档：https://docs.dgl.ai/en/1.1.x/api/python/udf.html#edge-wise-user-defined-function
阅读 DGL 的文档并尝试了一些替代函数。但他们没有工作。
如何修复/更新代码？]]></description>
      <guid>https://stackoverflow.com/questions/77837193/python-dgl-library-api-updates</guid>
      <pubDate>Thu, 18 Jan 2024 06:02:39 GMT</pubDate>
    </item>
    <item>
      <title>为什么支持向量机的 varImp() 出现错误？</title>
      <link>https://stackoverflow.com/questions/77714417/why-am-i-getting-an-error-with-varimp-for-support-vector-machine</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77714417/why-am-i-getting-an-error-with-varimp-for-support-vector-machine</guid>
      <pubDate>Mon, 25 Dec 2023 17:08:22 GMT</pubDate>
    </item>
    <item>
      <title>即使使用 IQR 方法去除异常值。异常值仍然存在于数据中</title>
      <link>https://stackoverflow.com/questions/77662142/even-though-removing-the-outliers-using-the-iqr-method-the-outliers-are-still-p</link>
      <description><![CDATA[我使用箱线图方法找到了数据中的异常值。
在此处输入图像描述应用 IQR 方法之前的箱形图
&lt;前&gt;&lt;代码&gt;file1.shape
# (457, 11)

我已对数据应用了 IQR 方法。
q1, q2, q3 = file1[&#39;工资&#39;].quantile([0.25, 0.5, 0.75])
IQR = q3 - q1
f_data = file1[(file1[&#39;工资&#39;] &gt; lower_bound) &amp; (file1[&#39;工资&#39;] &lt; upper_bound)]

我删除了一些数据点。
&lt;前&gt;&lt;代码&gt;f_data.shape
# (420, 11)

但是，在使用箱线图查看过滤后的数据后，我仍然在数据中发现了一些异常值。
在此处输入图像描述应用 IQR 方法后的箱线图。
我现在应该做什么。
我是否必须对过滤后的数据再次执行 IQR 方法。
薪资数据是右偏数据。其偏差值约为 1.5
或者我应该减少倾斜值。就像使用 log、power 方法一样。]]></description>
      <guid>https://stackoverflow.com/questions/77662142/even-though-removing-the-outliers-using-the-iqr-method-the-outliers-are-still-p</guid>
      <pubDate>Thu, 14 Dec 2023 17:45:51 GMT</pubDate>
    </item>
    </channel>
</rss>