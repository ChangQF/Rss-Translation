<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 06 Aug 2024 09:16:57 GMT</lastBuildDate>
    <item>
      <title>调用 cublasLtMatmul 时出现 RuntimeError：CUDA 错误：CUBLAS_STATUS_EXECUTION_FAILED</title>
      <link>https://stackoverflow.com/questions/78838155/runtimeerror-cuda-error-cublas-status-execution-failed-when-calling-cublasltma</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78838155/runtimeerror-cuda-error-cublas-status-execution-failed-when-calling-cublasltma</guid>
      <pubDate>Tue, 06 Aug 2024 09:00:40 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试使用 TensorFlow 运行一些面部识别代码，但我一直收到此 FLAG 错误</title>
      <link>https://stackoverflow.com/questions/78838145/im-trying-to-run-some-facial-recognition-code-using-tensorflow-but-i-keep-on-get</link>
      <description><![CDATA[2024-08-06 14:18:52.654763：I tensorflow/core/platform/cpu_feature_guard.cc:210] 此 TensorFlow 二进制文件经过优化，可在性能关键型操作中使用可用的 CPU 指令。
要启用以下指令：AVX2 AVX_VNNI FMA，在其他操作中，使用适当的编译器标志重建 TensorFlow。
每当我尝试运行任何类型的面部识别代码时，我都会收到这种错误。即使它与面部识别无关，而只是常规的张量流，我也会出现此错误。有人能帮我吗？我是这方面的新手，想学习 TensorFlow，但由于这个错误，我被困在起跑线上]]></description>
      <guid>https://stackoverflow.com/questions/78838145/im-trying-to-run-some-facial-recognition-code-using-tensorflow-but-i-keep-on-get</guid>
      <pubDate>Tue, 06 Aug 2024 08:58:22 GMT</pubDate>
    </item>
    <item>
      <title>NLTK bleu 分数明显高于 Sacrebleu bleu 分数</title>
      <link>https://stackoverflow.com/questions/78837792/nltk-bleu-score-significantly-higher-than-sacrebleu-bleu-score</link>
      <description><![CDATA[我试图将 bleu-4 分数的结果与 sacrebleu 和 NLTK corpus bleu 包进行比较，但结果之间的差异非常显著。
对于 NLTK corpus bleu，我获得了非常高的 bleu 分数（0.47、0.39、0.33、0.28）
但对于 sacrebleu，我获得了较低的分数（19.57、10.78、7.07、5.15），sacrebleu 分数已经将它们乘以 100，而 NLTK 没有
这是我计算这些分数的实现：
def compute_and_save_metrics(all_references, all_hypotheses, dataset_type, folder_name):
# 确保长度匹配
print(f&quot;Number of references: {len(all_references)}&quot;)
print(f&quot;假设数量：{len(all_hypotheses)}&quot;)

if len(all_references) != len(all_hypotheses):
raise ValueError(&quot;参考文献和假设的数量必须匹配。&quot;)

sacrebleu_scores = corpus_bleu(all_hypotheses, [all_references]).scores

bleu_score1 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(1.0, 0.0, 0.0, 0.0))
bleu_score2 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.5, 0.5))
bleu_score3 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.33, 0.33, 0.33))
bleu_score4 = nltk_corpus_bleu([[ref] for ref in all_references], [hyp for hyp in all_hypotheses], weights=(0.25, 0.25, 0.25, 0.25))


这是我生成预测的函数：
def assess_and_save(loader, dataset_type, folder_name):
model.eval()
all_references = []
all_hypotheses = []

sample_file_path = os.path.join(folder_name, &#39;samples.txt&#39;)
with open(sample_file_path, &#39;a&#39;, encoding=&#39;utf-8&#39;) as sample_file:
with torch.no_grad():
for Skeletons, Labels in loader:
Skeletons, Labels = Skeletons.to(device), Labels.to(device)

Outputs = model(skeletons, Labels[:, :-1])
Predictions = torch.argmax(outputs, dim=-1)

for i in range(predictions.size(0)):
Reference = tokenizer.decode(labels[i], skip_special_tokens=True)
Hypothesis = tokenizer.decode(predictions[i], skip_special_tokens=True)

# 调试：打印一些样本
if i &lt; 25：# 仅打印前 5 个样本
sample_text = f&quot;样本 {i+1}:\n参考：{reference}\n假设：{hypothesis}\nNew\n&quot;
print(sample_text)
sample_file.write(sample_text)

all_references.append(reference)
all_hypotheses.append(hypothesis)

# 检查是否有空引用或假设
empty_references = [ref for ref in all_references if not ref.strip()]
empty_hypotheses = [hyp for hyp in all_hypotheses if not hyp.strip()]

print(f&quot;空引用数：{len(empty_references)}&quot;)
print(f&quot;空假设数：{len(empty_hypotheses)}&quot;)

# 过滤掉空假设和相应的引用
non_empty_indices = [i for i, hyp in enumerate(all_hypotheses) if hyp.strip()]
all_references = [all_references[i] for i in non_empty_indices]
all_hypotheses = [all_hypotheses[i] for i in non_empty_indices]

compute_and_save_metrics(all_references, all_hypotheses, dataset_type, folder_name)


有什么建议我哪里错了吗？提前谢谢您]]></description>
      <guid>https://stackoverflow.com/questions/78837792/nltk-bleu-score-significantly-higher-than-sacrebleu-bleu-score</guid>
      <pubDate>Tue, 06 Aug 2024 07:40:08 GMT</pubDate>
    </item>
    <item>
      <title>在 Jupyter Notebook 中导入自定义模块的问题</title>
      <link>https://stackoverflow.com/questions/78837738/issues-with-importing-custom-modules-in-jupyter-notebook</link>
      <description><![CDATA[假设我们有一个如下的文件结构，
#注意：不是实际的目录结构，而是类似的结构
root../
tests../
src../
__init__.py
utils../
__init__.py
data.py
pipeline.py
sqlal../
__init__.py
sql_alchm.py
test.py
process.ipynb

以下是 data.py 的示例
from abc import ABC

class DataStrategy(ABC):
pass

以下是 test.py 的示例
from utils.data import DataStrategy

class TestPipeline(DataStrategy):
pass

现在我尝试在我的jupyter notebook process.ipynb，我得到如下错误
import os
from src.test import Test Pipeline

#单元格中的其余代码
.....
.....

ModuleNotFoundError：没有名为“utils”的模块

但是如果我运行文件 test.py，我没有收到错误，这个错误是否与笔记本的范围有关...还是我遗漏了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78837738/issues-with-importing-custom-modules-in-jupyter-notebook</guid>
      <pubDate>Tue, 06 Aug 2024 07:28:05 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何开源 AI 工具或 ML 库可以用 Java 和 Selenium 实现以进行 UI 测试？[关闭]</title>
      <link>https://stackoverflow.com/questions/78837606/is-there-any-open-source-ai-tool-or-ml-library-that-can-be-implemented-with-java</link>
      <description><![CDATA[我正在寻找一个可以直接实现的开源 AI 或 ML 库，就像我们在现有项目中添加外部 jar 或依赖项并将它们与一些自定义代码更改或内置方法一起使用以执行不同的操作以及执行不同的功能。AI 或 ML 库应与 Java 和 Selenium 兼容。我还在寻找一种可以将此 AI 或 ML 库用于 Jenkins 管道集成的方法。主要目标是 AI 应该能够捕获网页中的所有 Web 元素，如果 Web 元素发生任何变化，例如：如果字段 &#39;username&#39; 更改为 &#39;login&#39;，则 AI 应该能够扫描我们现有的 java-selenium 代码并让我们知道我们有旧定位器的地方，它应该建议解决方案，例如“将值为 &#39;username&#39; 的字段更改为新值 &#39;login&#39;
......................................................................................................................
如果没有可用于此的开源 AI，请告诉我是否有其他方法可以实现这个想法。有没有什么方法可以构建和训练自定义 AI 模型并将其与我的 Java-Selenium 代码集成并在我的本地系统中运行？
我已经浏览了互联网上的一些研究论文和博客。在互联网上搜索时，我发现一些 git 存储库将 AI 和 Healenium 的简单操作模型实现到 Maven Java Selenium 项目中。我目前正在尝试研究这些 git 存储库和研究论文。
我期待一些想法或基于 AI 的解决方案，这些解决方案可以通过我现有的 selenium 代码轻松实现。
请查看我浏览过的以下链接和文档
GitHub 链接：
https://github.com/arthurandreev/MLPoweredSeleniumJavaPOC
https://github.com/vishalmysore/sam
信息链接：
https://www.geeksforgeeks.org/top-ai-testing-tools-for-test-automation/
https://healenium.io/docs/overview
研究论文：
https://www.scitepress.org/Papers/2020/98854/98854.pdf]]></description>
      <guid>https://stackoverflow.com/questions/78837606/is-there-any-open-source-ai-tool-or-ml-library-that-can-be-implemented-with-java</guid>
      <pubDate>Tue, 06 Aug 2024 06:53:54 GMT</pubDate>
    </item>
    <item>
      <title>是否有可以执行 DLSS/FSR 功能的 CPU 超级采样库？[关闭]</title>
      <link>https://stackoverflow.com/questions/78837143/is-there-cpu-super-sampling-library-that-does-what-dlss-fsr-do</link>
      <description><![CDATA[我想将基于机器学习的升级器添加到我的 CPU 光线追踪器中。是否有可以这样做的 CPU 框架？
谢谢 :)]]></description>
      <guid>https://stackoverflow.com/questions/78837143/is-there-cpu-super-sampling-library-that-does-what-dlss-fsr-do</guid>
      <pubDate>Tue, 06 Aug 2024 03:39:35 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：具有多个输出的模型，当以列表形式提供“metrics”参数时，它应该具有与模型输出一样多的条目</title>
      <link>https://stackoverflow.com/questions/78837133/valueerror-a-model-with-multiple-outputs-when-providing-the-metrics-argument</link>
      <description><![CDATA[当我的朋友使用卷积神经网络开发年龄和性别检测模型时，他遇到了这个 ValueError。他在定义模型时将“metrics”参数作为列表给出，但它表示它应该具有与模型一样多的条目。我的模型有两个输出“年龄”和“性别”。
他正在定义他的 CNN 模型，当他尝试将模型拟合到训练数据上时，它显示错误提示

ValueError：对于具有多个输出的模型，当以列表形式提供 metrics 参数时，它应该具有与模型输出一样多的条目。收到：
metrics=[&#39;accuracy&#39;]
长度为 1，而模型有 2 个输出。

这是我的完整错误：
ValueError Traceback（最近一次调用最后一次）
Cell In[47]，第 1 行
----&gt; 1 History=Model.fit(X_train,Y_train_2,batch_size=64,validation_data=(X_test,Y_test_2),epochs=10,callbacks=callback_list)

文件 c:\Users\mebub_9a7jdi8\Desktop\Age_Gender_Detection_Model\.env\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 中引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 c:\Users\mebub_9a7jdi8\Desktop\Age_Gender_Detection_Model\.env\Lib\site-packages\keras\src\trainers\compile_utils.py:250，位于 CompileMetrics._build_metrics_set(self, metrics, num_outputs, output_names, y_true, y_pred,argument_name)
248 if isinstance(metrics, (list, tuple)):
249 if len(metrics) != len(y_pred):
--&gt; 250 raise ValueError(
251 &quot;对于具有多个输出的模型，&quot;
252 f&quot;当以 &quot;
253 &quot;列表形式提供 `{argument_name}` 参数时，它应该具有与模型具有的 &quot;
254 f&quot;输出一样多的条目。收到:\n{argument_name}={metrics}\nof &quot;
255 f&quot;长度 {len(metrics)}，而模型具有 &quot;
256 f&quot;{len(y_pred)} 个输出。&quot;
257 )
258 for idx, (mls, yt, yp) in enumerate(
...
261 if not isinstance(mls, list):

ValueError: 对于具有多个输出的模型，当以列表形式提供 `metrics` 参数时，它应该具有与模型具有的一样多的条目输出。收到：
metrics=[&#39;accuracy&#39;]
长度为 1，而模型有 2 个输出。

这是我定义模型的代码：
def model(input_shape):
inputs=Input((input_shape))
conv_1=Convolution(inputs,32)
maxp_1=MaxPooling2D(pool_size=(2,2))(conv_1)
conv_2=Convolution(maxp_1,64)
maxp_2=MaxPooling2D(pool_size=(2,2))(conv_2)
conv_3=Convolution(maxp_2,128)
maxp_3=MaxPooling2D(pool_size=(2,2))(conv_3)
conv_4=Convolution(maxp_3,256)
maxp_4=MaxPooling2D(pool_size=(2,2))(conv_4)
flatten= Flatten()(maxp_4)
density_1=Dense(64,activation=&#39;relu&#39;)(flatten)
density_2=Dense(64,activation=&#39;relu&#39;)(flatten)
drop_1=Dropout(0.2)(dense_1)
drop_2=Dropout(0.2)(dense_2)
output_1=Dense(1,activation=&#39;sigmoid&#39;,name=&#39;sex_out&#39;)(drop_1)
output_2=Dense(1,activation=&#39;relu&#39;,name=&#39;age_out&#39;)(drop_2)
model=Model(inputs=[inputs],outputs=[output_1,output_2])
model.compile(loss=[&quot;binary_crossentropy&quot;,&quot;mae&quot;],optimizer=&quot;Adam&quot;,metrics=[&quot;accuracy&quot;])
返回模型
]]></description>
      <guid>https://stackoverflow.com/questions/78837133/valueerror-a-model-with-multiple-outputs-when-providing-the-metrics-argument</guid>
      <pubDate>Tue, 06 Aug 2024 03:33:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML 算法的神经网络将产品列表映射到类别</title>
      <link>https://stackoverflow.com/questions/78837098/mapping-product-listings-to-categories-using-neural-networks-of-ml-algorithm</link>
      <description><![CDATA[我正在尝试实现 NN ML 算法来将产品映射到各自的类别，但该算法没有给出一致的结果。我使用了 keras、tensorflow 的顺序模型。请提出是否有更好的方法来解决此问题。
代码片段：
 # 分成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
print(y_train)
# 定义模型
model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_len))
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(units=128,activation=&#39;relu&#39;))
model.add(Dropout(0.5))
model.add(Dense(units=y_train.shape[1],activation=&#39;softmax&#39;))

# 编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;]

)
]]></description>
      <guid>https://stackoverflow.com/questions/78837098/mapping-product-listings-to-categories-using-neural-networks-of-ml-algorithm</guid>
      <pubDate>Tue, 06 Aug 2024 03:13:17 GMT</pubDate>
    </item>
    <item>
      <title>何时使用复合损失深度学习（图像分割）？</title>
      <link>https://stackoverflow.com/questions/78837046/when-to-use-composite-losses-deep-learning-image-segmentation</link>
      <description><![CDATA[我想知道在训练图像分割算法时，复合损失（例如 Dice + Focal Loss、Dice + Cross-Entropy Loss 或 Generalized Dice + Focal Loss）何时优于使用常规 Dice/CE Loss。
在什么情况下它们可以帮助算法更好地收敛？这是一个比较普遍的问题，但是否存在一个粗略的策略来确定某些算法的良好复合损失？]]></description>
      <guid>https://stackoverflow.com/questions/78837046/when-to-use-composite-losses-deep-learning-image-segmentation</guid>
      <pubDate>Tue, 06 Aug 2024 02:43:38 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow Privacy 中的差分隐私错误</title>
      <link>https://stackoverflow.com/questions/78836989/error-in-tensorflow-privacy-for-differential-privacy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78836989/error-in-tensorflow-privacy-for-differential-privacy</guid>
      <pubDate>Tue, 06 Aug 2024 02:18:31 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：在层“conv2d_7”上调用“set_weights(weights)”，权重列表长度为 2，但该层需要 1 个权重</title>
      <link>https://stackoverflow.com/questions/78836514/valueerror-called-set-weightsweights-on-layer-conv2d-7-with-a-weight-list</link>
      <description><![CDATA[我尝试在模型中的某一层上设置权重，但无济于事。
我在网上查找了类似问题的解决方案，但似乎都不起作用。变量“w”（如下面代码所示）的结构为 [numpy array, numpy array]。第一个的大小为 (3, 3, 3, 64)，第二个的形状为 (64,)。我想实现与 tf 2.X 中的“weights”kwarg 类似的功能，但似乎无法让它工作。这是我的代码：
encoder = Sequential()
encoder.add(layers.Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;,use_bias=False,input_shape=(SIZE,SIZE,3)))
w = model.layers[0].get_weights()
encoder.layers[0].set_weights([w])
encoder.add(layers.MaxPooling2D((2, 2),padding=&#39;same&#39;))
encoder.add(layers.Conv2D(32, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;,weights=model.layers[2].get_weights()))
encoder.add(layers.MaxPooling2D((2, 2), padding=&#39;same&#39;))
encoder.add(layers.Conv2D(16, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;,weights=model.layers[4].get_weights()))
encoder.add(layers.MaxPooling2D((2, 2), padding=&#39;same&#39;))
encoder.summary()

错误：
错误：ValueError：您在层“conv2d_7”上调用了`set_weights(weights)`，权重列表长度为 2，但该层需要 1 个权重。
]]></description>
      <guid>https://stackoverflow.com/questions/78836514/valueerror-called-set-weightsweights-on-layer-conv2d-7-with-a-weight-list</guid>
      <pubDate>Mon, 05 Aug 2024 21:33:29 GMT</pubDate>
    </item>
    <item>
      <title>如何将 adaboost 与 xgboost 结合起来</title>
      <link>https://stackoverflow.com/questions/78835596/how-to-combine-adaboost-with-xgboost</link>
      <description><![CDATA[这是一个基于 5 个特征的 3 类分类问题。y 的 3 个类别（标签）分别为 0、1 和 2。我尝试使用 xgboost 模型作为基础估计器并将其放入 adabboost 中。在训练集上拟合模型后，我使用这个拟合模型来预测测试集。但是测试集上的预测值全都是0。
如何解决这个问题？
我的代码在这里
x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.3, random_state=10)
ss_x = StandardScaler()
x_train_transformed = ss_x.fit_transform(x_train)
x_test_transformed = ss_x.transform(x_test)
le_y = LabelEncoder()
y_train_transformed = le_y.fit_transform(y_train)
y_test_transformed = le_y.transform(y_test)

base_model = xgb.XGBClassifier()
model = AdaBoostClassifier(estimator=base_model)
model.fit(x_train_transformed, y_train_transformed)
model_test_sc = accuracy_score(y_test_transformed, model.predict(x_test_transformed))
conf_matrix = confused_matrix(y_test_transformed, model.predict(x_test_transformed),labels=[0,1,2])

我不确定这样将两个助推器结合起来是否合理。]]></description>
      <guid>https://stackoverflow.com/questions/78835596/how-to-combine-adaboost-with-xgboost</guid>
      <pubDate>Mon, 05 Aug 2024 17:00:45 GMT</pubDate>
    </item>
    <item>
      <title>用于聚类的机器学习模型（与 K-means 类似，但功能不同）[关闭]</title>
      <link>https://stackoverflow.com/questions/78833002/machine-learning-model-for-clustering-similar-with-k-means-but-different-funct</link>
      <description><![CDATA[当我研究几个机器学习模型时，我看到了几个聚类算法，包括K-Means。
据我所知K-Means使用欧几里德距离作为他们自己的计算方法，
我想要的不是使用欧几里德距离，而是数据的值。
例如，样本分布很广（如坐标），坐标有自己的值。
我想找到平均值较高的N个簇。
有没有其他适合这个图的算法，或者我是否只处理K-Means算法中的几个参数来做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78833002/machine-learning-model-for-clustering-similar-with-k-means-but-different-funct</guid>
      <pubDate>Mon, 05 Aug 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>DSPy 无法检索 ChromaDB 中带有文本嵌入的段落</title>
      <link>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</link>
      <description><![CDATA[我正在使用 DSPy 和 ChromaDB 为 pdf 文件开发 RAG 应用程序。
首先，我从 pdf 中获取文本并将其作为块添加到 Chromadb。还添加了块的嵌入。并尝试使用 DSPy 检索与查询相关的块。但是它出现了错误
存储数据和嵌入
def store_document_in_chromadb(text):
chunks = chunk_document(text)
ids = [f&#39;chunk_{i}&#39; for i in range(len(chunks))]
embeddings = [get_embedding(chunk).tolist() for chunk in chunks]

collection.add(ids=ids, documents=chunks, embeddings=embeddings)

我尝试像这样检索相关块，
retriever_model = ChromadbRM(&quot;contracts_collection&quot;, &#39;db/&#39;, k=2)
dspy.settings.configure(lm=llama2_model, rm=retriever_model)

class GenerateAnswer(dspy.Signature): 
“”“”根据给出的上下文回答问题。“”“”
context = dspy.InputField(desc=&quot;可能包含相关上下文&quot;)
question = dspy.InputField()
answer = dspy.OutputField(desc=&quot;通常为 5 到 10 个单词&quot;)

class RAG(dspy.Module): 
def __init__(self, num_passages=2):
super().__init__()
self.retrieve = dspy.Retrieve(k=num_passages)
self.generate_answer = dspy.ChainOfThought(GenerateAnswer)

def forward(self, question):
context = self.retrieve(question).passages
prediction = self.generate_answer(context=context, question=question)
return dspy.Prediction(context=context, answer=prediction.answer)

with dspy.context(lm=llama2_model, rm=retriever_model):
module = RAG()
response = module(&quot;总支出是多少&quot;)
print(response)

当我运行此程序时，出现此错误
InvalidDimensionException：嵌入维度 384 与集合维度 768 不匹配
但是当我从 ChromaDB 中删除嵌入时，它会正确检索相关块。
为什么使用嵌入时没有出现此错误？]]></description>
      <guid>https://stackoverflow.com/questions/78758312/dspy-cant-retrieve-passage-with-text-embeddings-in-chromadb</guid>
      <pubDate>Wed, 17 Jul 2024 08:03:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在微调过程中正确设置 pad token（而不是 eos）以避免模型无法预测 EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    </channel>
</rss>