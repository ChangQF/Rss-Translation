<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 04 Dec 2023 21:11:47 GMT</lastBuildDate>
    <item>
      <title>如何使用异步生成器/如何将数据异步加载到数据集中？</title>
      <link>https://stackoverflow.com/questions/77602360/how-to-use-an-async-generator-how-to-load-data-asynchronous-into-a-dataset</link>
      <description><![CDATA[例如，当尝试从生成器返回承诺时，我收到打字稿错误。
const generated = function* () {
    产生新的 Promise(() =&gt; {});
};

tf.data.generator(生成);

&#39;() =&gt; 类型的参数生成器，无效，未知&gt;&#39;不可分配给 &#39;() =&gt; 类型的参数迭代器 | Promise&gt;&#39;。
使用异步生成器也不起作用：
异步生成器导致类型错误
tf.data.generator(异步函数* () {})

抛出
&#39;() =&gt; 类型的参数AsyncGenerator&lt;任何、无效、未知&gt;&#39;不可分配给 &#39;() =&gt; 类型的参数迭代器 | Promise&gt;&#39;。
这不应该是一个常见的用例吗？人们需要从网络获取数据或从数据库中读取数据来学习，而数据太大而无法一次全部装入内存？]]></description>
      <guid>https://stackoverflow.com/questions/77602360/how-to-use-an-async-generator-how-to-load-data-asynchronous-into-a-dataset</guid>
      <pubDate>Mon, 04 Dec 2023 20:23:17 GMT</pubDate>
    </item>
    <item>
      <title>回归的特征选择</title>
      <link>https://stackoverflow.com/questions/77601601/feature-selection-for-regression</link>
      <description><![CDATA[我正在做一个学校项目，其中我 (1) 使用创意写作中的计算语言学 (CL) 功能寻找教师分数和算法计算分数之间的相关性，以及 (2) 尝试使用这些功能和算法来预测教师分数回归。
我有 6 位老师，他们根据 4 个标准的评分标准对 12 个故事进行评分（每个故事总共有 6x4 分）。对于评分标准中的每个标准，我选择了相应的 CL 特征，并编写了一个代码实现，使用这些特征来计算分数（每个故事总共 4 个分数）。
对于相关性分析，我将每个故事、每个标题组件的平均教师分数（因此减少到每个故事 4 分）与每个故事、每个相应功能的 CL 分数（每个故事仍然 4 分）。
现在，如果我训练回归模型，(A) 使用每个故事的单独分数（即每个故事 6x4 分数）而不是 (B) 教师平均分数（每个故事 4 分）是否有意义？我背后的原因是使用个人分数，因为这样我就有更多数据来训练我的模型，但我不确定我是否监督了某些事情。
答：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

故事
评分者
得分 c1
得分 c2
得分 c3
得分 c4
得分f1
得分f2
得分f3
得分f4


&lt;正文&gt;

故事_1
grader_1
5
5
8
10
7
5
6
8


故事_1
grader_2
9
7
8
6
7
5
6
8


故事_1
grader_3
7
7
8
7
7
5
6
8


故事_1
grader_4
7
5
6
9
7
5
6
8


故事_1
grader_5
6
6
6
8
7
5
6
8


故事_1
grader_6
8
6
6
8
7
5
6
8


...
...
...
...
...
...
...
...
...
...


故事_12
grader_1
1
6
8
7
4
6
5
6


故事_12
grader_2
4
6
7
7
4
6
5
6


故事_12
grader_3
3
5
6
7
4
6
5
6


故事_12
grader_4
3
6
4
5
4
6
5
6


故事_12
grader_5
3
7
3
5
4
6
5
6


故事_12
grader_6
3
6
2
5
4
6
5
6




B：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

故事
平均得分 c1
平均得分 c2
平均得分 c3
平均得分 c4
得分f1
得分f2
得分f3
得分f4


&lt;正文&gt;

故事_1
7
6
7
8
7
5
6
8


...
...
...
...
...
...
...
...
...


故事_12
3
6
5
6
4
6
5
6



]]></description>
      <guid>https://stackoverflow.com/questions/77601601/feature-selection-for-regression</guid>
      <pubDate>Mon, 04 Dec 2023 17:55:28 GMT</pubDate>
    </item>
    <item>
      <title>特征矩阵和变量赋值</title>
      <link>https://stackoverflow.com/questions/77601471/feature-matrix-and-variable-assignment</link>
      <description><![CDATA[我目前正在使用 Python 开发 ML 的 scikit 模块。我有一个 .csv 格式的数据集。我在选择特征矩阵 X 和变量 y 时遇到问题。它说找不到我作为代码输入的列标题。我也尝试对它们取消索引，但仍然无法做到。]]></description>
      <guid>https://stackoverflow.com/questions/77601471/feature-matrix-and-variable-assignment</guid>
      <pubDate>Mon, 04 Dec 2023 17:35:58 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow：无法将 NumPy 数组转换为 Tensor（不支持的对象类型 int）</title>
      <link>https://stackoverflow.com/questions/77600884/tensorflow-failed-to-convert-a-numpy-array-to-a-tensor-unsupported-object-type</link>
      <description><![CDATA[我收到错误“ValueError：无法将 NumPy 数组转换为张量（不支持的对象类型 int）。”在下面的 nn_model.fit 行上。
这里我的 data.csv 文件包含列：文本（对象类型）、附件（float 类型）、类别（lable、int 类型）。
# 将数据帧拆分为测试数据和训练数据
def split_data(壮举,标签):
    # 训练测试分割
    x_train, x_val, y_train, y_val = train_test_split(
        壮举，encode_labels（标签），test_size = 0.20，random_state = 42，shuffle = True）
    数据 = {“train”：{“X”：x_train，“y”：y_train}，
            “测试”：{“X”：x_val，“y”：y_val}}
    返回数据

## NN 的标签编码
def 编码标签（标签）：
    # 标签处理
    le = 标签编码器()
    le.fit(标签)
    类= le.classes_
    编码标签 = le.transform(标签)
    # 使用 Keras 将整数转换为 one-hot 编码
    编码标签 = utils.to_categorical(编码标签)
    返回编码标签

定义模型（）：
        # CNN + LSTM 模型
        模型=顺序（）
        model.add(Embedding(train_args[“max_feature”], train_args[“embed_size”], input_length=train_args[“max_len”]))
        model.add(Conv1D(filters=64，kernel_size=7，padding=&#39;same&#39;，activation=&#39;relu&#39;))
        model.add(MaxPooling1D(pool_size=2, padding=&#39;相同&#39;))
        model.add(Dropout(0.1)) # 由于严重过拟合而进行正则化
        model.add(双向(LSTM(64, recurrent_dropout=0.02,return_sequences=True)))
        model.add(GlobalMaxPool1D())
        模型.add(Dropout(0.1))
        model.add(密集(16,activation=&#39;relu&#39;))
        model.add(Dense(train_args[“total_classes”],activation=&#39;softmax&#39;))

        model.layers[0].trainable = True

        model.compile(优化器=&#39;亚当&#39;,
                    损失=&#39;分类交叉熵&#39;，
                    指标=[&#39;准确性&#39;])
        logger.info(模型.summary())
        返回模型

# 训练模型，返回模型
def train_model(数据):
    ＃ 训练
    logger.info(“开始训练神经网络”)
    nn_model = 模型()
    历史= nn_model.fit(数据[“火车”][“X”],
                数据[“火车”][“y”]，
                批量大小=64，
                纪元=10，
                validation_data=(数据[“测试”][“X”]，数据[“测试”][“y”]))
    logger.info(&quot;神经网络训练完成&quot;)
    返回 nn_model

# 评估模型的指标
def get_model_metrics（模型，数据）：
    preds = model.predict(data[&quot;test&quot;][&quot;X&quot;])
    准确度 = precision_score(np.argmax(preds, axis=1), np.argmax(data[&quot;test&quot;][&quot;y&quot;],axis=1))
    指标 = {“accuracy_score”：准确性}
    返回指标

def main():
    # 将训练数据加载为数据帧
    data_dir =“数据”；
    data_file = os.path.join(data_dir, &#39;data.csv&#39;)
    train_df = pd.read_csv(数据文件)
    标签 = train_df[“类别”]
    feat = train_df.drop(columns=[&#39;类别&#39;])

    数据 = split_data(壮举,标签)

    # 训练模型
    模型 = train_model(数据)

    # 记录模型的指标
    指标= get_model_metrics（模型，数据）
    对于metrics.items()中的(k, v)：
        打印（f“{k}：{v}”）

如果 __name__ == “__main__”：
    主要的（）

现在我想运行该程序，但不知道该怎么做才能运行它？我对 python 和 ML 不太熟悉。]]></description>
      <guid>https://stackoverflow.com/questions/77600884/tensorflow-failed-to-convert-a-numpy-array-to-a-tensor-unsupported-object-type</guid>
      <pubDate>Mon, 04 Dec 2023 16:05:15 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习的高光谱图像分类[关闭]</title>
      <link>https://stackoverflow.com/questions/77600329/hyperspectral-image-classification-using-machine-learning</link>
      <description><![CDATA[我有一个人脸高光谱图像数据集，共有42名参与者，压力类别为4-情绪压力基线、情绪压力、身体压力基线、PS1（在某个时间测量的身体压力）、PS2（在其他时间测量的身体压力）。使用这个高光谱图像数据集，我们必须将其分类为身体压力或情绪压力。该数据集包含 mat 文件。我们必须将压力分类为身体压力或情绪压力。对于分类，我们必须使用SVM。您能建议在 jupyter Notebook 中实现它的代码吗？
文字
数据集链接-文本
导入 pandas 作为 pd
将 numpy 导入为 np
导入操作系统
从 sklearn.impute 导入 SimpleImputer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.svm 导入 SVC

# CSV 文件所在的目录
目录 = &#39;驱动器/我的驱动器/StO2_mat(size513_911)/&#39;

# 初始化空列表来存储数据和文件名
数据数组 = []
文件名 = []

# 循环遍历目录下的所有CSV文件
对于 os.listdir（目录）中的文件名：
    if filename.endswith(&#39;.csv&#39;):
        file_path = os.path.join(目录, 文件名)
        df = pd.read_csv(文件路径)
        data_array = df.values.ravel()
        data_arrays.append(data_array)
        file_names.append(文件名)

# 从一维 NumPy 数组列表创建一个 DataFrame
数据 = pd.DataFrame(data_arrays)

# 添加“目标列”包含原始文件名
数据[&#39;目标列&#39;] = 文件名

# 检查是否有足够的唯一样本用于分割
if len(data[&#39;target_column&#39;].unique()) &lt;= 1:
    print(“没有足够的唯一样本用于训练-测试分割。”)
别的：
    # 分离非数字和数字数据列
    non_numeric_data = data.select_dtypes(&#39;字符串&#39;)
    numeric_data = data.select_dtypes(include=[&#39;number&#39;])

    # 估算数值数据中的缺失值
    imputer = SimpleImputer(策略=&#39;均值&#39;)
    numeric_data_impulated = imputer.fit_transform(numeric_data)
    numeric_data_impulated_df = pd.DataFrame(numeric_data_impulated)

    # 合并非数值数据和估算数值数据
    impulated_data = pd.concat([non_numeric_data, numeric_data_impulated_df], axis=1)

    # 将数据分为训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(impulated_data.drop(&#39;target_column&#39;, axis=1), impulated_data[&#39;target_column&#39;], test_size=0.2, random_state=42)

    # 在训练数据上训练模型
    clf = SVC(内核=&#39;线性&#39;)
    clf.fit(X_train, y_train)

    # 对测试数据进行预测
    y_pred = clf.predict(X_test)

    # 评估模型性能
    准确度 = np.mean(y_pred == y_test)
    print(&#39;准确度：&#39;, 准确度)


我收到这样的错误
KeyError Traceback（最近一次调用最后一次）
&lt;ipython-input-6-0e3b82a51446&gt;在&lt;细胞系：31&gt;()
     45
     46 # 将数据拆分为训练集和测试集
---&gt; 47 X_train, X_test, y_train, y_test = train_test_split(impulated_data.drop(&#39;target_column&#39;, axis=1), impulated_data[&#39;target_column&#39;], test_size=0.2, random_state=42)
     48
     49 # 在训练数据上训练模型

5帧
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py 中 drop(self, labels, error)
   第6932章
   第6933章
-&gt;第6934章
   第6935章
   第6936章

KeyError：“在轴中找不到[&#39;target_column&#39;]”

如何解决该错误？基本目标是打印精度。
在此处输入图片描述
我试图打印准确性但出现错误]]></description>
      <guid>https://stackoverflow.com/questions/77600329/hyperspectral-image-classification-using-machine-learning</guid>
      <pubDate>Mon, 04 Dec 2023 14:45:22 GMT</pubDate>
    </item>
    <item>
      <title>后端的大.pkl数据没有推送到github中</title>
      <link>https://stackoverflow.com/questions/77600252/large-pkl-data-for-backend-is-not-pushed-in-github</link>
      <description><![CDATA[我正在学习机器学习。最近，我从 tmdb 数据集制作了电影推荐模型，我使用 .pkl （二进制）文件中的模型处理数据。使用该数据制作后端，但是数据太大，无法推送到 github，我无法托管网站。
我正在尝试将已处理的数据推送到后端，但无法部署，因为它超出了文件大小的限制]]></description>
      <guid>https://stackoverflow.com/questions/77600252/large-pkl-data-for-backend-is-not-pushed-in-github</guid>
      <pubDate>Mon, 04 Dec 2023 14:33:48 GMT</pubDate>
    </item>
    <item>
      <title>张量流形状错误</title>
      <link>https://stackoverflow.com/questions/77600065/tensorflow-shape-bug</link>
      <description><![CDATA[我正在进行深度 Q 学习，当我将图像提供给我的模型（来自硒驱动程序）时，一切都很好，但是当我尝试拟合我的模型时，它给了我这个错误
ValueError：层“顺序”的输入 0与图层不兼容：预期形状=(无, 500, 400, 1)，发现形状=(无, 4, 500, 400, 1)


这是我的图像、合身度和模型的代码
 def get_image(驱动程序):
      屏幕 = driver.get_screenshot_as_png()
      img = Image.open(BytesIO(屏幕))
      返回 np.array(img)[500:900, 400:900]

    def load_model(自身):
        ”“”
        从文件加载模型。
        ”“”

        if os.path.isfile(“model.h5”):
            self.model = load_model(“model.h5”)
            print(“模型已加载。”)

        模型=顺序（）
        #32个尺寸为3x3的滤波器的卷积层，具有relu激活函数
        model.add(Conv2D(32, (3, 3), input_shape=(500, 400, 1)))
        model.add(激活(&#39;relu&#39;))
        #池化层大小为2x2
        model.add(MaxPooling2D(pool_size=(2, 2)))

        #32个尺寸为3x3的滤波器的卷积层，具有relu激活函数
        model.add(Conv2D(32, (3, 3)))
        model.add(激活(&#39;relu&#39;))
        #池化层大小为2x2
        model.add(MaxPooling2D(pool_size=(2, 2)))

        #隐藏层64个神经元
        模型.add(压平())
        model.add(密集(64))
        model.add(激活(&#39;relu&#39;))
        #输出层有 5 个神经元，每个神经元对应一个可能的动作
        model.add(密集(5))

        #编译模型
        model.compile(loss=&#39;categorical_crossentropy&#39;,
                      优化器=&#39;亚当&#39;,
                      指标=[&#39;准确性&#39;])


        返回模型

      def fit_model(自身):
        ”“”
        将模型拟合到重播内存中。
        ”“”

        #如果回放内存不够满，则不要训练模型
        如果 len(self.replay_memory) &lt; self.MIN_REPLAY_MEMORY_SIZE：
            返回

        print(&quot;拟合模型&quot;)

        #从重放内存中获取过渡的随机样本
        样本 = random.sample(self.replay_memory, self.MINIBATCH_SIZE)

        #从样本中获取当前状态
        current_states = np.array([样本中的转换的转换[0]])
        #预测当前状态的q值
        current_qs_list = self.model.predict(current_states)

        #从样本中获取未来状态
        future_states = np.array([样本中的转换的转换[3]])
        #预测未来状态的q值
        future_qs_list = self.model.predict(future_states)

        X = []
        y = []

        #对于样本中的每个转换
        对于枚举（样本）中的索引（current_state、action、reward、future_state、done）：
            #如果过渡不是样本中的最后一个
            如果没有完成：
                #计算所采取行动的新q值
                max_future_q = np.max(future_qs_list[索引])
                new_q = 奖励 + self.DISCOUNT * max_future_q
            别的：
                #如果转变是样本中的最后一个，则将新的 q 值设置为奖励
                new_q = 奖励

            #更新所采取操作的q值
            current_qs = current_qs_list[索引]
            当前_qs[操作] = 新_q

            #将当前状态和新的q值添加到训练数据中
            X.append(当前状态)
            y.append(current_qs)

        #将模型拟合到训练数据上
        self.model.fit(np.array(X)，np.array(y)，batch_size=self.MINIBATCH_SIZE，verbose=0，shuffle=False)

        #更新目标模型
        如果 self.target_update_counter &gt; self.UPDATE_TARGET_EVERY：
            self.target_model.set_weights(self.model.get_weights())
            self.target_update_counter = 0
            self.save_model()
        别的：
            self.target_update_counter += 1```


我已经尝试对我的图像进行 .shape，但它不起作用。我还尝试将我的过渡[0]和过渡[3]直接添加到列表中，并制作预测列表，但它不起作用并告诉我 current_qs[action] 超出范围。这很奇怪，因为我在创建 futur_qs_list 和 current_qs_list 时想要预测的图像似乎与我用来预测动作的图像形状不同。然而，它与我的代码中的格式完全相同，所以我不知道该怎么办。]]></description>
      <guid>https://stackoverflow.com/questions/77600065/tensorflow-shape-bug</guid>
      <pubDate>Mon, 04 Dec 2023 14:08:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在第二次运行 Optuna 中重试失败的试验？</title>
      <link>https://stackoverflow.com/questions/77599820/how-can-i-retry-fail-trials-in-optuna-in-a-second-run</link>
      <description><![CDATA[我正在使用 Optuna 进行网格搜索，但失败的试验不会在第二次运行中重复。相反，已经完成的试验被无用地重复。
这里我分别描述一下两个问题：

当试验失败（例如缺乏计算资源）时，第二次启动网格搜索（Python 文件）时不会重复。这可以使用以下独立代码进行测试，其中我通过启动异常来模拟问题。注释这些行并再次重新运行，可以看到组合 x=2 和 y=2 没有重复。

导入时间
导入奥图纳
从 optuna.storages 导入 RetryFailedTrialCallback
将 numpy 导入为 np


定义目标（试用）：
    # 获取值
    参数 = {
                &#39;x&#39;: Trial.suggest_categorical(&#39;x&#39;, [0, 1, 2, 3]),
                &#39;y&#39;: Trial.suggest_categorical(&#39;y&#39;, [0, 1, 2, 3])
            }
    # 打印它
    print(&#39;用 x=&#39; 进行测试&#39; + str(params[&#39;x&#39;]), &#39;y=&#39; + str(params[&#39;y&#39;]))

    ##########################################
    # 首次运行后评论此部分#
    ##########################################
    如果 params[&#39;x&#39;] == 2 且 params[&#39;y&#39;] == 2：
        引发 ValueError(&quot;x==2, y==2&quot;)
    ##########################################

    ＃ 返回
    返回参数[&#39;x&#39;] ** 2 - 参数[&#39;y&#39;]



def optuna_search_space():
    # 定义搜索空间
    返回 {
        &#39;x&#39;：范围（3），
        &#39;y&#39;：范围（3），
    }



def optuna_grid():
    # 定义网址
    URL = &#39;mysql://&lt;用户&gt;:&lt;密码&gt;@:&lt;端口&gt;&#39;
    # 获取搜索空间
    搜索空间 = optuna_search_space()
    # 定义存储
    存储 = optuna.storages.RDBStorage(
        url=f&quot;{URL}/prove_optuna&quot;,
        failed_trial_callback=重试失败TrialCallback(max_retry=3),
    ）
    # 定义研究
    研究 = optuna.load_study(
        研究名称=“测试1”，
        采样器 = optuna.samplers.GridSampler(search_space),
        存储=存储，
    ）
    ＃ 跑步
    研究.优化（目标）
    ＃ 打印
    打印（研究.best_Trial）



如果 __name__ == “__main__”：
    ＃ 跑步
    optuna_grid()


当我重新运行代码时，它会重复已经执行的试验（或更多）。我不希望这样，因为这是计算资源的损失。

在 Optuna 仪表板上可以看到，在多次重新运行组合 (x=2, y=2) 后，它永远不会重复（即使第一次失败），并且组合 (x= 0, y=1) 已测试多次（无用）。

如何解决这些问题？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/77599820/how-can-i-retry-fail-trials-in-optuna-in-a-second-run</guid>
      <pubDate>Mon, 04 Dec 2023 13:32:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在colab中查找数据集的某一列中有多少个不同的数据</title>
      <link>https://stackoverflow.com/questions/77599408/how-to-find-how-many-different-data-are-in-a-column-of-a-data-set-in-colab</link>
      <description><![CDATA[我有一个大约由 400000 行和 8 列组成的数据集，我只想知道一列中有多少种不同类型的数据，我该怎么做？列中的数据是字符串的形式，我需要给它们分配数字，所以我需要找出该列中有多少个不同的单词。我不知道我应该做什么]]></description>
      <guid>https://stackoverflow.com/questions/77599408/how-to-find-how-many-different-data-are-in-a-column-of-a-data-set-in-colab</guid>
      <pubDate>Mon, 04 Dec 2023 12:22:17 GMT</pubDate>
    </item>
    <item>
      <title>如何创建机器人[关闭]</title>
      <link>https://stackoverflow.com/questions/77599245/how-to-create-a-bot</link>
      <description><![CDATA[我很想知道制作 Alexa 这样的机器人的代码是什么
我尝试过云计算，但我仍然对这个过程是如何完成的有些怀疑。我对云计算非常感兴趣，并对它进行了彻底的简短研究。我注意到 C3loud 计算是为了获得客户或用户的评论而完成的。云计算有可能永远是对的吗？就像云计算总是会显示正确的输出一样吗？]]></description>
      <guid>https://stackoverflow.com/questions/77599245/how-to-create-a-bot</guid>
      <pubDate>Mon, 04 Dec 2023 11:56:20 GMT</pubDate>
    </item>
    <item>
      <title>LLM 微调和推理所需的廉价云计算平台 [关闭]</title>
      <link>https://stackoverflow.com/questions/77599001/cheap-cloud-computing-platform-needed-for-llm-fine-tuning-and-inference</link>
      <description><![CDATA[我是一名刚毕业的人工智能毕业生，现在在一家非常小的初创公司工作，探索（并尝试实施）人工智能可以在公司软件中使用的地方。公司里没有其他人做人工智能，这就是为什么我想在这里问一个问题（也是因为我在谷歌上找不到具体的答案）。
基本上，我正在尝试使用 HuggingFace 来尝试一些法学硕士，以便我可以找到适合我的想法的法学硕士。问题是我的笔记本电脑不够强大，无法在 LLM 上运行推理，因为我只有 GTX 1650。我尝试使用 Google Colab，但只成功运行了一个小型 3B 参数模型，该模型表现不佳。
我的问题是：在哪里可以找到最便宜的云计算平台，该平台仍然强大到足以运行推理并可能对中小型法学硕士进行微调？如果有帮助的话，我目前正在尝试找到一个可以进行自定义命名实体识别的模型，因此该模型可能不需要太大，我也不需要进行训练。
问题是，由于我工作的公司是一家小型初创公司，他们无法为一个人提供像 AWS 或 Azure 这样的东西（我尝试研究了这方面的成本，我认为每月大约 2500 美元） .
我非常感谢您对此的帮助！感谢您的宝贵时间:)]]></description>
      <guid>https://stackoverflow.com/questions/77599001/cheap-cloud-computing-platform-needed-for-llm-fine-tuning-and-inference</guid>
      <pubDate>Mon, 04 Dec 2023 11:17:32 GMT</pubDate>
    </item>
    <item>
      <title>机器学习预测想法[关闭]</title>
      <link>https://stackoverflow.com/questions/77598931/machine-learning-forecast-ideas</link>
      <description><![CDATA[虚拟数据
我必须预测此数据未来 3 个月的故事点，我该如何开始？
我必须预测未来 3 个月的故事点，我可以使用哪种 ML 算法
如何分析数据以及数据的趋势]]></description>
      <guid>https://stackoverflow.com/questions/77598931/machine-learning-forecast-ideas</guid>
      <pubDate>Mon, 04 Dec 2023 11:07:11 GMT</pubDate>
    </item>
    <item>
      <title>MLPClassifier 适合二元分类吗？</title>
      <link>https://stackoverflow.com/questions/77596591/is-mlpclassifier-appropriate-for-binary-classification</link>
      <description><![CDATA[我编写了一个使用 MLPClassifier 来解决二元分类问题的程序。它有点有效，但我不相信这是正确的模型。
我有 1300 个整数的十六进制数要放入两个类之一：类 0 和类 1。一个潜在的问题是，在我的训练数据中，98% 属于类 0，所以我将从 &quot; 获得 98% 的准确率“预测函数”总是返回“class 0”与输入无关。
是否有专为此类问题设计的机器学习模型？
================================================== ===============
TLDR？
我的数据如下：
X = 数组([[ 0, 11, 51, 13, 0, 9],
       [51,13,0,9,0,11],
       [ 0, 8, 0, 10, 0, 13],
       ...,
       [ 0, 11, 61, 12, 0, 8],
       [ 0, 8, 0, 0, 60, 11],
       [30, 11, 0, 6, 0, 9]])

目标是 y，一个包含 1300 个 0 和 1 的列表。我使用 MLPClassifier 并获得了 98% 的预测准确率。这时我突然想到，98% 的元组恰好属于 0 类，因此，如果我不费心进行任何机器学习，而是猜测类始终为 0，那么我将获得 98% 的准确率。
我检查了拟合度，看看它在 1 类元组上的表现如何，发现其中 82% 的预测正确，因此准确度为 98% 的 82%，即大约 80%，我想改进，但是怎么办？
除了盲目增加层的大小/数量之外，我不知道如何更改 MLPClassifier 的参数，但我突然想到，我很可能使用完全错误的模型来解决带有“是/”的学习问题没有分类。另外，六元组中的整数不是任意的，我想到这也可能与模型的选择有关。特别是，六个输入中的三个始终在 0 - 15 范围内，另外三个是两位数代码，第一位数字有三种可能，第二位数字有两种可能。
感谢您的任何想法。
代码：
m = MLPClassifier(hidden_​​layer_sizes = (256, 128, 64), max_iter=10000) # 从我在网上找到的示例粘贴:(
_ = m.fit(X, y)
yhat = m.predict(X)
cm = 混淆矩阵(y, yhat)
print( &#39;准确率 = &#39;, np.mean( y == yhat ) )
打印（厘米）

输出：
准确度 = 0.9816653934300993
[[1267 13]
 [11 18]]

(pdb) class1 = [ i for i in range(len(y)) if y[i] == 1]
(pdb) z = m.predict(np.row_stack((X[q] for q in class1)))
(pdb) z
数组([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 , 1, 1, 1, 0, 1])
(pdb) len(z), 总和(z)
29, 24
]]></description>
      <guid>https://stackoverflow.com/questions/77596591/is-mlpclassifier-appropriate-for-binary-classification</guid>
      <pubDate>Sun, 03 Dec 2023 23:53:22 GMT</pubDate>
    </item>
    <item>
      <title>使用 svm 进行高光谱图像分类</title>
      <link>https://stackoverflow.com/questions/77594411/hyperspectral-image-classification-using-svm</link>
      <description><![CDATA[将 pandas 导入为 pd
将 numpy 导入为 np
导入操作系统
从 sklearn.impute 导入 SimpleImputer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.metrics 导入 precision_score
从 imblearn.over_sampling 导入 SMOTE
从 imblearn.under_sampling 导入 RandomUnderSampler



# CSV 文件所在的目录
目录 = &#39;驱动器/我的驱动器/StO2_mat(size513_911)/&#39;

# 初始化空列表来存储数据和文件名
数据数组 = []
文件名 = []

# 循环遍历目录下的所有CSV文件
对于 os.listdir（目录）中的文件名：
    if filename.endswith(&#39;.csv&#39;):
        file_path = os.path.join(目录, 文件名)
        df = pd.read_csv(文件路径)
        data_array = df.values.ravel()
        data_arrays.append(data_array)
        file_names.append(文件名)

# 从一维 NumPy 数组列表创建一个 DataFrame
数据 = pd.DataFrame(data_arrays)

# 添加“目标列”包含原始文件名
数据[&#39;目标列&#39;] = 文件名

# 检查是否有足够的唯一样本用于分割
if len(data[&#39;target_column&#39;].unique()) &lt;= 1:
    print(“没有足够的唯一样本用于训练-测试分割。”)
别的：
    # 分离非数字和数字数据列
    non_numeric_data = data.select_dtypes(&#39;字符串&#39;)
    numeric_data = data.select_dtypes(include=[&#39;number&#39;])

    # 估算数值数据中的缺失值
    imputer = SimpleImputer(策略=&#39;均值&#39;)
    numeric_data_impulated = imputer.fit_transform(numeric_data)
    numeric_data_impulated_df = pd.DataFrame(numeric_data_impulated)

    # 合并非数值数据和估算数值数据
    impulated_data = pd.concat([non_numeric_data, numeric_data_impulated_df], axis=1)

    # 将数据分为训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(impulated_data.drop(&#39;target_column&#39;, axis=1), impulated_data[&#39;target_column&#39;], test_size=0.1, random_state=42)

   # 将 SMOTE 应用于训练数据
    smote = SMOTE(random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

    # 使用 RandomForestClassifier （如您的示例中所示）
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train_resampled, y_train_resampled)

    # 对测试数据进行预测
    y_pred = clf.predict(X_test)

    # 评估模型性能
    准确度=准确度_得分(y_test, y_pred)
    print(&#39;准确度：&#39;, 准确度)

我尝试过欠采样、不同的 ckassifiers，如 svm、knn 和随机森林分类器（对数据 imabalance 不太敏感）。仍然无法解决该错误。
错误-KeyError Traceback（最近一次调用最后一次）
 在&lt;细胞系：43&gt;()
43、如果len(imput_data)==1：
44#处理单个样品箱
---&gt; 45 X_train = impulated_data.drop(&#39;target_column&#39;, axis=1)
46 y_train = impulated_data[&#39;target_column&#39;]
47
5帧
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py 中 drop(self, labels, error)
第6932章
第6933章
-&gt;第6934章
第6935章
第6936章
KeyError：“在轴中找不到[&#39;target_column&#39;]”]]></description>
      <guid>https://stackoverflow.com/questions/77594411/hyperspectral-image-classification-using-svm</guid>
      <pubDate>Sun, 03 Dec 2023 13:04:03 GMT</pubDate>
    </item>
    <item>
      <title>如何在 SKLearn Estimator 上使用 Sagemaker HyperparameterTuner？</title>
      <link>https://stackoverflow.com/questions/77573670/how-do-i-use-sagemaker-hyperparametertuner-on-a-sklearn-estimator</link>
      <description><![CDATA[我正在关注 Amazon Sagemaker 研讨会尝试利用 Sagemaker 的多个实用程序，而不是像我目前所做的那样在笔记本上运行所有内容。
问题是，在研讨会上，他们教您如何使用来自 AWS 的现成 XGBoost 图像来使用 HyperparameterTuner，而我的大多数管道都使用 Scikit-Learn 模型，例如 GradientBoostingClassifier 或 RandomForest，因此我实例化了一个估计器如下此示例文件：
sklearn = SKLearn(entry_point=&quot;train.py&quot;,
                  Framework_version =“1.2-1”，
                  instance_type=“ml.m5.xlarge”，
                  角色=角色，
                  超参数=fixed_hyperparameters
）

之后，我使用刚刚创建的估计器实例化一个 HyperparameterTuner 作业，其中包含我想要测试的超参数范围。
hyperparameters_ranges = {
    “n_estimators”: ContinuousParameter(100, 500),
    “学习率”：连续参数（1e-2，1e-1），
    “最大深度”：IntegerParameter(2, 5),
    “子样本”：连续参数（0.6，1），
    “max_df”：连续参数（0.4，1），
    “max_features”：IntegerParameter(5, 25),
    “use_idf”：CategoricalParameter([True, False])
}

度量=“验证：f1”

调谐器 = 超参数调谐器(
    sklearn,
    公制，
    超参数范围，
    最大作业数=2,
    最大并行作业数=2
）

我的问题是，我没有找到任何有关如何访问“train.py”内部 SKLearn 估计器中传递的超参数的信息。文件。我也没有找到最佳超参数存储在哪里，因此我可以将它们用于最终模型。有人可以告诉我这是否可能吗？或者如果有另一种更简单的方法可以提供替代方案吗？]]></description>
      <guid>https://stackoverflow.com/questions/77573670/how-do-i-use-sagemaker-hyperparametertuner-on-a-sklearn-estimator</guid>
      <pubDate>Wed, 29 Nov 2023 18:27:13 GMT</pubDate>
    </item>
    </channel>
</rss>