<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 31 Oct 2024 15:18:33 GMT</lastBuildDate>
    <item>
      <title>尽管 MPS（Apple Silicon）可用且可检测到，但 PyTorch 模型仍在 CPU 上运行</title>
      <link>https://stackoverflow.com/questions/79144698/pytorch-model-running-on-cpu-despite-mps-apple-silicon-being-available-and-det</link>
      <description><![CDATA[我尝试使用 MPS（Metal Performance Shaders）在我的 Apple Silicon Mac 上运行 HuggingFace Transformers 模型，但尽管 MPS 可用且被检测到，该模型仍在 CPU 上运行，导致严重的性能问题。
这是我的模型初始化代码：
def instantiate_model(cfg: dict):
model_fp = cfg[&#39;model&#39;][&#39;model_fp&#39;]
batch_size = cfg[&#39;model&#39;][&#39;batch_size&#39;]
default_threshold = float(cfg[&#39;postprocessing&#39;][&#39;threshold&#39;])

# 清除所有缓存内存
torch.mps.empty_cache()

# 设置设备
if torch.backends.mps.is_available():
device = torch.device(&quot;mps&quot;)
print(f&quot;MPS device found: {device}&quot;)
else:
device = torch.device(&quot;cpu&quot;)
print(f&quot;MPS not available, using: {device}&quot;)

# 加载模型
model = AutoModelForTokenClassification.from_pretrained(
model_fp,
torch_dtype=torch.float32
)
model.eval()
model = model.to(device)

# 训练参数设置
args = TrainingArguments(
model_fp,
learning_rate=3e-5,
per_device_train_batch_size=batch_size,
per_device_eval_batch_size=batch_size,
weight_decay=0.01,
evaluation_strategy=IntervalStrategy.STEPS,
eval_steps=100,
max_steps=4000,
load_best_model_at_end=True,
metric_for_best_model=&#39;f1&#39;,
save_total_limit=5,
no_cuda=True,
use_mps_device=torch.backends.mps.is_available()
)

trainer = MultiLabelTrainer(model, args)
tokenizer = AutoTokenizer.from_pretrained(model_fp)

return {&#39;trainer&#39;: trainer, &#39;tokenizer&#39;: tokenizer, &#39;default_threshold&#39;: default_threshold}

代码正确检测 MPS 并打印“MPS 设备已找到：mps”，但监控我的系统显示处理仍在 CPU 上进行。
环境详细信息：
Apple Silicon (M2) 上的 macOS
Python 3.10
安装了 MPS 支持的 PyTorch
transformers 库最新版本
我尝试过的方法：
明确将设备设置为
使用 torch.backends.mps.is_available() 确认 MPS 可用性&gt;
使用 model.to(device)
在 TrainingArguments 中设置 use_mps_device=True
如何确保模型实际在 MPS 而不是 CPU 上运行？

明确将设备设置为“mps”：
python
device = torch.device(&quot;mps&quot;)
model = model.to(device)


验证 MPS 是否实际可用：
pythonCopyprint(f&quot;MPS 是否可用？ {torch.backends.mps.is_available()}&quot;)
print(f&quot;MPS 是否已构建？{torch.backends.mps.is_built()}&quot;)

输出显示两者都为 True

移动模型设备后检查它：
pythonCopyprint(f&quot;模型设备：{next(model.parameters()).device}&quot;)

输出显示“mps”


在模型执行期间监控 CPU 使用率显示 CPU 利用率为 600%（GPU 使用率几乎为 0%），这表明尽管模型已移动到 MPS 设备，但并未使用 GPU
在 TrainingArguments 中设置 use_mps_device=True
预期行为：
模型应通过 MPS 使用 Apple Silicon GPU
由于计算被卸载到 GPU，CPU 使用率应更低
与 CPU 执行相比，处理速度应明显更快
实际行为：
尽管检测到 MPS 并将模型移动到 MPS 设备
在模型执行期间，CPU 使用率仍为 100%
处理速度与仅使用 CPU 执行相同
由于 CPU 处理，系统变得过载]]></description>
      <guid>https://stackoverflow.com/questions/79144698/pytorch-model-running-on-cpu-despite-mps-apple-silicon-being-available-and-det</guid>
      <pubDate>Thu, 31 Oct 2024 12:17:17 GMT</pubDate>
    </item>
    <item>
      <title>我有一些特征，数据通常很大，但问题是其中两个特征（列）完全缺失，我们如何计算这些？</title>
      <link>https://stackoverflow.com/questions/79144583/i-have-some-feature-and-data-is-usually-large-but-the-problem-is-that-two-of-the</link>
      <description><![CDATA[我有 20 个特征，数据通常很大，但问题是其中两个特征（列）完全缺失，我们如何使用机器学习根据其他值计算这些特征？问题是我们无法删除该列，需要检索该特征？
我学习了许多处理少量缺失值的方法，但无法逻辑地检索此问题。]]></description>
      <guid>https://stackoverflow.com/questions/79144583/i-have-some-feature-and-data-is-usually-large-but-the-problem-is-that-two-of-the</guid>
      <pubDate>Thu, 31 Oct 2024 11:45:21 GMT</pubDate>
    </item>
    <item>
      <title>NeuralProphet 想要预测 Dataframe 中的 y 列</title>
      <link>https://stackoverflow.com/questions/79144504/neuralprophet-wants-y-column-in-predict-dataframe</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79144504/neuralprophet-wants-y-column-in-predict-dataframe</guid>
      <pubDate>Thu, 31 Oct 2024 11:22:41 GMT</pubDate>
    </item>
    <item>
      <title>用于对图像集平均值进行标注的工具（而不是单个图片）</title>
      <link>https://stackoverflow.com/questions/79144252/tools-for-captioning-an-average-of-image-set-instead-of-individial-pictures</link>
      <description><![CDATA[我正在寻找能够用一个句子描述一组图像的 AI 字幕模型。或者，我需要一种方法来在概念上平均一组图像，然后再将该“概念”（可能是特征向量）提供给常规字幕模型。
为什么？
用于训练数据集管理。在适合整个提示上测试训练后的生成模型会很有帮助，而不是选择单个图像的字幕或尝试找出它们之间的共同点。此外，这还允许生成单个负面提示来测试模型在范围外提示上的表现。
我到目前为止所做的：
我已经修改了现有的 CLIP+BLIP 询问器以处理图像集。然而，虽然 CLIP 字幕允许在使用图像特征选择最佳字幕之前对其进行平均，但它的准确性远低于 BLIP 生成的字幕，后者仅适用于单幅图像。我需要一个可以像 CLIP 一样接收特征向量的模型，这样我就可以对它们进行预处理。]]></description>
      <guid>https://stackoverflow.com/questions/79144252/tools-for-captioning-an-average-of-image-set-instead-of-individial-pictures</guid>
      <pubDate>Thu, 31 Oct 2024 09:57:53 GMT</pubDate>
    </item>
    <item>
      <title>寻找集群：DBSCAN 还是 Friends-of-Friends？</title>
      <link>https://stackoverflow.com/questions/79143343/finding-clusters-dbscan-or-friends-of-friends</link>
      <description><![CDATA[我使用 Friends-of-Friends 算法在宇宙学模拟中搜索了暗物质晕。
https://swift.dur.ac.uk/docs/FriendsOfFriends/algorithm_description.html
现在我使用 DBSCAN 算法在射电望远镜数据中搜索 HI 云。
https://en.wikipedia.org/wiki/DBSCAN
我发现这两种算法有一些相同之处，例如：

基于密度的聚类：DBSCAN 和 FoF 都可以被视为基于密度的方法广义上。DBSCAN 根据数据点的密度明确定义聚类，而 FoF 还可以根据点之间的连接（即朋友）密度识别聚类。
聚类识别：两种算法都通过将某种意义上彼此接近的点分组在一起来识别聚类。在 DBSCAN 中，这基于指定半径内的点的密度，而在 FoF 中，它基于连通性或共享邻居的数量。
不需要预定义的聚类数量：两种算法都不需要提前指定聚类数量。DBSCAN 根据密度参数确定聚类，而 FoF 根据连通性结构查找聚类，而无需事先设置聚类数量。
处理噪声和异常值：两种方法都可以处理噪声或异常值，尽管它们处理的方式不同。DBSCAN 明确将不适合任何聚类的点归类为噪声。在 FoF 中，与其他点连接不够的点最终可能会被孤立或不属于任何重要聚类。
聚类形状灵活性：两种算法都能够识别任意形状的聚类。DBSCAN 以其基于密度的方法而闻名，能够找到各种形状和大小的聚类。FoF 还可以根据连接模式找到形状不规则的聚类。

我想知道它们之间是否存在一些重大差异，以便我可以更好地选择在聚类时使用哪种算法。]]></description>
      <guid>https://stackoverflow.com/questions/79143343/finding-clusters-dbscan-or-friends-of-friends</guid>
      <pubDate>Thu, 31 Oct 2024 02:40:27 GMT</pubDate>
    </item>
    <item>
      <title>如何训练具有两个相互依赖的特征的 knn 分类模型？</title>
      <link>https://stackoverflow.com/questions/79142864/how-to-train-a-knn-classification-model-with-2-features-that-are-dependant-to-ea</link>
      <description><![CDATA[我收集了一系列质谱图，并尝试用它们在 sklearn 上训练一个模型。因此，特征是“m/z”和“强度”，目标是“茶叶品种”。问题是，这两个特征是不可分割的，我想以某种方式将它们连接起来，如下所示：
&#39;spectrum id&#39; &#39;m/z-intensity&#39; &#39;tea variety&#39;

1 ; [[1,2],[2,3],...] ; var1

2 ; [[1,2],[2,3],...] ; var1

但是发生了错误，指出有一个列表需要浮点数，所以我只是将所有 m/z 和强度值连接起来，并在一个单元格中为每个值创建一个数据框，这样虽然有效，但完全不准确。有没有更好的方法可以做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/79142864/how-to-train-a-knn-classification-model-with-2-features-that-are-dependant-to-ea</guid>
      <pubDate>Wed, 30 Oct 2024 21:34:24 GMT</pubDate>
    </item>
    <item>
      <title>如何根据模式从地图中提取数据</title>
      <link>https://stackoverflow.com/questions/79141905/how-to-extract-data-from-a-map-based-on-pattern</link>
      <description><![CDATA[我有一张这样的地图（jpg 图像）：

以及相应的图案：

我想根据图案创建多边形（在 .shp 文件中）。
目标是叠加一个多边形，然后查看哪些图案落在该叠加多边形中。
我尝试使用opencv 但结果并不理想。
有没有办法做类似的事情，哪怕是近似的？是在 qgis 中，还是使用纯 python 或其他软件？]]></description>
      <guid>https://stackoverflow.com/questions/79141905/how-to-extract-data-from-a-map-based-on-pattern</guid>
      <pubDate>Wed, 30 Oct 2024 15:58:26 GMT</pubDate>
    </item>
    <item>
      <title>如何计算零膨胀泊松回归和零膨胀负二项回归的平均绝对误差（MAE）？</title>
      <link>https://stackoverflow.com/questions/79139968/how-can-we-calculate-mean-absolute-error-mae-for-zero-inflated-poisson-regress</link>
      <description><![CDATA[我尝试使用 Python 在进行零膨胀泊松回归和零膨胀负二项回归时计算平均绝对误差 (MAE)。
我将数据分为训练数据和测试数据。我使用下面的代码，但它不起作用：
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
import statsmodels.formula.api as smf
import tensorflow as tf
df = pd.read_excel(&#39;....&#39;, sheet_name=&#39;Sheet1&#39;)
print(df.head())
X = df[[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;]]
y = df[&#39;g&#39;]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from statsmodels.discrete.count_model import ZeroInflatedPoisson
y_zip = y_train.values

y_zip_test = y_test.values

X_count = X_train.values # 计数部分的预测器
X_zero = X_train.values # 零膨胀部分的预测器

X_count_test = X_test.values
X_zero_test = X_test.values

# 为截距添加一个常数
X_count = sm.add_constant(X_count)
X_zero = sm.add_constant(X_zero)

# 拟合 ZIP 模型
zip_model = ZeroInflatedPoisson(endog=y_zip, exog=X_count, exog_infl=X_zero, indication=&#39;logit&#39;)
zip_model_fit = zip_model.fit()
print(zip_model_fit.summary())

# 进行预测
y_pred = zip_model_fit.predict(X_count_test)

# 计算 MAE
mae = np.mean(np.abs(y_zip_test - y_pred))
print(f&#39;平均绝对误差：{mae}&#39;)

结果如下
-------------------------------------------------------------------------------
ValueError Traceback (most recent call last)
Cell In[3], line 33
29 print(zip_model_fit.summary())
32 # 进行预测
---&gt; 33 y_pred = zip_model_fit.predict(X_count_test)
35 # 计算 MAE 测试
36 mae = np.mean(np.abs(y_zip_test - y_pred))

文件 ~\anaconda3\envs\tf\lib\site-packages\statsmodels\base\model.py:1174，位于 Results.predict(self, exog, transform, *args, **kwargs)
1127 &quot;&quot;&quot;
1128 调用 self.model.predict 并以 self.params 作为第一个参数。
1129 
(...)
1169 返回预测。
1170 &quot;&quot;&quot;
1171 exog, exog_index = self._transform_predict_exog(exog,
1172 transform=transform)
-&gt; 1174 predict_results = self.model.predict(self.params, exog, *args,
1175 **kwargs)
1177 如果 exog_index 不为 None 且不 hasattr(predict_results,
1178 &#39;predicted_values&#39;):
1179 如果 predict_results.ndim == 1:

文件 ~\anaconda3\envs\tf\lib\site-packages\statsmodels\discrete\count_model.py:453，位于 GenericZeroInflated.predict(self, params, exog, exog_infl, Exposure, Offset, which, y_values)
449 params_main = params[self.k_inflate:]
451 prob_main = 1 - self.model_infl.predict(params_infl, exog_infl)
--&gt; 453 lin_pred = np.dot(exog, params_main[:self.exog.shape[1]]) + Exposure + Offset
455 # 重构：这很不靠谱，
456 # model_main 中应该有一个合适的预测方法
457 # 这只是 prob(y=0 | model_main)
458 tmp_exog = self.model_main.exog

ValueError：形状 (21,6) 和 (7,) 未对齐：6 (1 维) != 7 (0 维)

可以使用数据框解决错误。
y_train, X_train = dmatrices(expr, recreated_train_data, return_type=&#39;dataframe&#39;)
但是，我遇到了模型无法收敛的问题，信息如下：
C:\Users\Admin\anaconda3\envs\tf\lib\site-packages\scipy\optimize\_optimize.py:1291: OptimizeWarning: 已超出最大迭代次数。
res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)
C:\Users\Admin\anaconda3\envs\tf\lib\site-packages\statsmodels\base\model.py:607: ConvergenceWarning: 最大似然优化无法收敛。检查 mle_retvals
warnings.warn(&quot;最大似然优化失败&quot;

如何解决此错误？]]></description>
      <guid>https://stackoverflow.com/questions/79139968/how-can-we-calculate-mean-absolute-error-mae-for-zero-inflated-poisson-regress</guid>
      <pubDate>Wed, 30 Oct 2024 07:04:19 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 优化器：具有权重衰减的 AdamW 和 Adam</title>
      <link>https://stackoverflow.com/questions/64621585/pytorch-optimizer-adamw-and-adam-with-weight-decay</link>
      <description><![CDATA[torch.optim.Adam(weight_decay=0.01) 和 torch.optim.AdamW(weight_decay=0.01) 之间有什么区别吗？
文档链接：torch.optim。]]></description>
      <guid>https://stackoverflow.com/questions/64621585/pytorch-optimizer-adamw-and-adam-with-weight-decay</guid>
      <pubDate>Sat, 31 Oct 2020 12:11:46 GMT</pubDate>
    </item>
    <item>
      <title>我的图像字幕模型为所有图像提供相同的字幕</title>
      <link>https://stackoverflow.com/questions/60434487/my-image-captioning-model-giving-me-same-caption-on-all-images</link>
      <description><![CDATA[我正在做一个与医学图像字幕相关的项目。我使用的代码来自此链接。
我正在使用印第安纳射线照片数据集并使用结果作为训练字幕。我训练成功，损失值为 0.75。但是我的最终模型为我检查过的所有图像提供了相同的标题（有些人也面临同样的问题。请查看此链接的评论）。
您能否建议我对代码的任何部分或其他任何内容进行任何更改，以便它开始为我将检查的每张图片提供适当的标题。]]></description>
      <guid>https://stackoverflow.com/questions/60434487/my-image-captioning-model-giving-me-same-caption-on-all-images</guid>
      <pubDate>Thu, 27 Feb 2020 13:34:38 GMT</pubDate>
    </item>
    <item>
      <title>如何清理图像以便与 MNIST 训练模型一起使用？</title>
      <link>https://stackoverflow.com/questions/57000160/how-to-clean-images-to-use-with-a-mnist-trained-model</link>
      <description><![CDATA[我正在创建一个机器学习模型，用于对数字图像进行分类。我使用内置的 tf.keras.datasets.mnist 数据集，通过 Tensorflow 和 Keras 训练了该模型。该模型与来自 mnist 数据集本身的测试图像配合得很好，但我想给它提供我自己的图像。我提供给该模型的图像是从 Captcha 中提取的，因此它们将遵循类似的模式。我在此公共 Google Drive 文件夹中包含了一些图像示例。当我输入这些图像时，我注意到模型不是很准确，我对原因有一些猜测。

图像的背景在图片中产生了太多噪音。
数字不居中。
图像不严格遵循 MNIST 训练集的颜色格式（黑底白字）。

我想问一下如何去除背景并将其居中，以便减少图像中的噪音，从而实现更好的分类。
这是我正在使用的模型：
import tensorflow as tf
from tensorflow import keras

mnist = keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

class Stopper(keras.callbacks.Callback):
def on_epoch_end(self, epoch, log={}):
if log.get(&#39;acc&#39;) &gt;= 0.99:
self.model.stop_training = True
print(&#39;\n达到 99% 准确率。停止训练...&#39;)

model = keras.Sequential([
keras.layers.Flatten(),
keras.layers.Dense(1024,activation=tf.nn.relu),
keras.layers.Dense(10,activation=tf.nn.softmax)])

model.compile(
optimizer=tf.train.AdamOptimizer(),
loss=&#39;sparse_categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

x_train, x_test = x_train / 255, x_test / 255

model.fit(x_train, y_train, epochs=10, callbacks=[Stopper()])

下面是我将图像导入 tensorflow 的方法：
from PIL import Image
img = Image.open(&quot;image_file_path&quot;).convert(&#39;L&#39;).resize((28, 28), Image.ANTIALIAS)
img = np.array(img)
model.predict(img[None,:,:])

我还从 MNIST 数据集中引入了一些示例 此处。我想要一个脚本，将我的图像尽可能接近 MNIST 数据集格式。此外，由于我必须对无限数量的图像执行此操作，如果您能提供一种完全自动化的转换方法，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/57000160/how-to-clean-images-to-use-with-a-mnist-trained-model</guid>
      <pubDate>Fri, 12 Jul 2019 04:25:47 GMT</pubDate>
    </item>
    <item>
      <title>我们如何在基于密度的算法（DBSCAN）中表示聚类的摘要？</title>
      <link>https://stackoverflow.com/questions/40895201/how-can-we-represent-a-summary-of-clusters-in-density-based-algorithms-dbscan</link>
      <description><![CDATA[在基于密度的算法中，我们如何表示聚类？换句话说，在基于原型的算法中，聚类由质心和数据点数量表示；在基于模型的算法中，聚类由质心和方差表示；在基于网格的算法中，聚类由长度、高度和每个网格中的数据点数量表示。那么，对于基于密度的算法（DBSCAN），我们如何表示输出聚类的摘要？]]></description>
      <guid>https://stackoverflow.com/questions/40895201/how-can-we-represent-a-summary-of-clusters-in-density-based-algorithms-dbscan</guid>
      <pubDate>Wed, 30 Nov 2016 18:02:40 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow - 任何输入都会给我相同的输出</title>
      <link>https://stackoverflow.com/questions/39540806/tensorflow-any-input-gives-me-same-output</link>
      <description><![CDATA[我遇到了一个非常奇怪的问题，我正在使用 tensorflow 构建 RNN 模型，然后在完成训练后使用 tf.Saver 存储模型变量（全部）。
在测试期间，我只需再次构建推理部分并将变量恢复到图中。恢复部分不会给出任何错误。
但是当我开始在评估测试上进行测试时，我总是从推理中获得相同的输出，即对于所有测试输入，我都会得到相同的输出。
我在训练期间打印了输出，我确实看到不同训练样本的输出不同，并且成本也在降低。
但是当我进行测试时，无论输入是什么，它总是给我相同的输出。
有人能帮我理解为什么会发生这种情况吗？我想发布一些最小的例子，但由于我没有收到任何错误，我不确定我应该在这里发布什么。如果能帮助解决问题，我很乐意分享更多信息。
训练和测试期间的推理图之间的一个区别是 RNN 中的时间步数。在训练期间，我在更新梯度之前对一个批次进行 n 步（n = 20 或更多）训练，而对于测试，我只使用一个步骤，因为我只想预测该输入。]]></description>
      <guid>https://stackoverflow.com/questions/39540806/tensorflow-any-input-gives-me-same-output</guid>
      <pubDate>Fri, 16 Sep 2016 22:13:32 GMT</pubDate>
    </item>
    <item>
      <title>验证 DBSCAN 集群的最佳方法</title>
      <link>https://stackoverflow.com/questions/33502455/best-way-to-validate-dbscan-clusters</link>
      <description><![CDATA[我使用 DBSCAN 的 ELKI 实现从火灾数据集中识别火灾热点集群，结果看起来相当不错。数据集是空间的，集群基于纬度、经度。基本上，DBSCAN 参数识别火灾点高度集中的热点区域（由密度定义）。这些是火灾热点区域。
我的问题是，在尝试了几个不同的参数并找到一对给出合理聚类结果的参数后，如何验证集群？
是否有适合我的用例的正式验证方法？或者这取决于应用领域，是主观的？]]></description>
      <guid>https://stackoverflow.com/questions/33502455/best-way-to-validate-dbscan-clusters</guid>
      <pubDate>Tue, 03 Nov 2015 15:07:21 GMT</pubDate>
    </item>
    <item>
      <title>以距离矩阵作为输入的基于密度的聚类库</title>
      <link>https://stackoverflow.com/questions/7385326/a-density-based-clustering-library-that-takes-distance-matrix-as-input</link>
      <description><![CDATA[需要帮助寻找一个开放/免费的基于密度的聚类库，该库以距离矩阵作为输入并返回其中每个元素与聚类中其他每个元素的最大“x”距离的聚类（基本上返回具有指定密度的聚类）。
我检查了 DBSCAN 算法，它似乎满足我的需求。您可能不知道是否有任何干净的 DBSCAN 实现，可以使用预先计算的距离矩阵并输出具有所需密度的聚类？]]></description>
      <guid>https://stackoverflow.com/questions/7385326/a-density-based-clustering-library-that-takes-distance-matrix-as-input</guid>
      <pubDate>Mon, 12 Sep 2011 08:48:40 GMT</pubDate>
    </item>
    </channel>
</rss>