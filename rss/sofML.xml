<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 11 Mar 2024 15:14:37 GMT</lastBuildDate>
    <item>
      <title>在 PyTorch Lightning 中实施 SSD</title>
      <link>https://stackoverflow.com/questions/78141575/implement-ssd-into-pytorch-lightning</link>
      <description><![CDATA[我不断收到此错误 AssertionError: Expected target boxs to be a tensor of shape [N, 4], got torch.Size([10, 100, 4]). ，这很奇怪因为我制作了形状 N,4 但由于某种原因我的训练批次 10 变成了形状？
我正在尝试训练图像和注释/地面实况框
下面是我对数据集的准备。
from pathlib 导入路径
从输入导入 Any、Callable、Dict、List、Optional、Tuple

将闪电导入为 L # noqa: N812
将 numpy 导入为 np
将 pandas 导入为 pd
进口火炬
导入 torch.nn.function 作为 F # noqa: N812
将 torchvision 导入为电视
导入 torchvision.transforms 作为 tv
从 Lightning.pytorch.utilities.types 导入 STEP_OUTPUT，OptimizerLRScheduler
从 PIL 导入图像
从 torch.utils.data 导入 DataLoader
从 torchvision.datasets 导入 VisionDataset

一个=[]

data_folder = Path().parent / “数据”

图像集 = 设置()
对于 data_folder.glob(“*.png”) 中的图像：
    image_set.add(images.stem)

盒子集 = 设置（）
对于data_folder.glob(“*txt”)中的boxes_list：
    box_set.add(boxes_list.stem)

交集= image_set &amp;盒子集

对于交叉点的茎：
    image_path = data_folder / (stem + “.png”)
    box_path = data_folder / (stem + “.txt”)

    a.append((image_path,boxes_path))

图像张量列表 = []
盒子列表 = []
自定义数据 = []

对于 a[0:10] 中的 image_path、boxes_path：
    img = Image.open(图像路径)
    变换 = tv.Compose([
        电视.调整大小((300,300)),
        tv.ToTensor()
        ]
    ）

    img_tensor = 变换(img)
    image_tensor_list.append（img_tensor）

    # 张量转换需要 dtype=float 来处理空 csv
    框= pd.read_csv(boxes_path,skiprows=[0],sep=&quot;&quot;,names=[&quot;xmin&quot;,&quot;ymin&quot;,&quot;xmax&quot;,&quot;ymax&quot;], dtype=np.float64 ）
    盒子 = torch.tensor(boxes.values)
    盒子=盒子.resize_(100,4)
    标签 = torch.zeros(size=(boxes.shape[0],), dtype=torch.int64)
    box_list.append({
        “盒子”：盒子，
        “标签”：标签
    })

数据集=列表（zip（image_tensor_list，boxs_list））

类数据集（视觉数据集）：
    def __init__(
        自己，
        根：str，
        变换：可选[可调用] = 无，
        变换：可选[可调用] = 无，
        target_transform：可选[可调用] = 无，
    ）-&gt;没有任何：
        super().__init__(根、变换、变换、target_transform,)
        self.dataset = 数据集

    def __getitem__(self, 索引: int) -&gt;元组[torch.Tensor, dict] |没有任何：
        返回 self.dataset[索引]

    def __len__(self) -&gt;; __len__(self) -&gt;整数：
        返回 len(self.dataset)

类分类器（L.LightningModule）：
    def __init__(自身):
        超级().__init__()

        self.model = tv.models.detection.ssd300_vgg16(权重=“SSD300_VGG16_Weights.DEFAULT”)
        对于 self.model.parameters() 中的参数：
            param.requires_grad = False

    定义前进（
        self，图像：List [torch.Tensor]，目标：可选[List [Dict [str，torch.Tensor]]] = None
    ）-&gt; Tuple[Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]]:
        返回 self.model(图像、目标)

    def Training_step（自身，批次，batch_idx）：
        图像、目标 = 批次
        y_hat = self(图像, [目标])
        损失 = F.l1_loss(y_hat, 目标)
        self.log(“train_loss”, loss, on_epoch=True)
        回波损耗

    def configure_optimizers(self) -&gt;;优化器LRScheduler：
        返回 torch.optim.Adam(self.parameters())

类数据模块（L.LightningDataModule）：
    def __init__(自身,batch_size=32):
        超级().__init__()
        self.batch_size = 批量大小
        self.train_dataset = 无
        self.val_dataset = 无

    #（数据 - 平均值）/std
    def 设置（自身，阶段：str）：
        data_root = Path().parent / “数据”
        变换 = tv.transforms.Compose([
            电视. 变换. 调整大小((300, 300)),
            tv.transforms.ToTensor()
    ]）

        # TODO(ehnjcio): 进行适当的训练、测试、验证分割
        self.train_dataset = VarroaDataset(根=data_root, 变换=变换)
        # self.val_dataset = VarroaDataset(根=根, 变换=变换)

    def train_dataloader(自身):
        返回DataLoader（self.train_dataset，batch_size = self.batch_size，
                          工人数=0)

模型 = 分类器()
数据 = 数据模块()
训练员 = L.Trainer()
trainer.fit(模型，数据)
]]></description>
      <guid>https://stackoverflow.com/questions/78141575/implement-ssd-into-pytorch-lightning</guid>
      <pubDate>Mon, 11 Mar 2024 14:58:08 GMT</pubDate>
    </item>
    <item>
      <title>使用稳定基线3进行作业调度</title>
      <link>https://stackoverflow.com/questions/78141506/job-scheduling-using-stable-baselines3</link>
      <description><![CDATA[我正在做一个由作业调度问题组成的项目，其中有机器、作业和配方。我在使用稳定基线实现 DQN 时遇到问题。在自定义环境中，我发现 PPO 表现良好，但 DQN 很糟糕。我已经测试过更改超参数。可能是什么问题呢？有什么我错过的吗？
感谢任何帮助。谨致问候。
我有一个遵循稳定基线3和健身房（体育馆）界面的自定义环境，但从稳定基线导入的 DQN 模型与 PPO 相比表现非常糟糕。通过将 DQN 和 PPO 与 FIFO（先进先出）和 EDD（最早到期日）等模型进行比较来完成评估。 DQN 的表现甚至比这些传统模型更差。]]></description>
      <guid>https://stackoverflow.com/questions/78141506/job-scheduling-using-stable-baselines3</guid>
      <pubDate>Mon, 11 Mar 2024 14:48:21 GMT</pubDate>
    </item>
    <item>
      <title>结合 Savee 和 Crema-d 音频数据集常见情感的 Python 代码 [关闭]</title>
      <link>https://stackoverflow.com/questions/78141428/python-code-combining-common-emotions-from-savee-and-crema-d-audio-dataset</link>
      <description><![CDATA[https://www.kaggle.com/datasets/ejlok1/cremad
https://www.kaggle.com/datasets/ejlok1/萨里视听表达情感保存
将这些由不同感官组成的数据集中的共同情感结合起来。
您能分享带有组合声音集的 rar 文件吗？
如何将这些数据集中的常见声音与机器学习结合起来？
由于我不懂机器学习，所以尝试使用Python，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78141428/python-code-combining-common-emotions-from-savee-and-crema-d-audio-dataset</guid>
      <pubDate>Mon, 11 Mar 2024 14:34:37 GMT</pubDate>
    </item>
    <item>
      <title>在张量流中重命名模型输出名称</title>
      <link>https://stackoverflow.com/questions/78141102/rename-model-output-names-in-tensorflow</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78141102/rename-model-output-names-in-tensorflow</guid>
      <pubDate>Mon, 11 Mar 2024 13:47:17 GMT</pubDate>
    </item>
    <item>
      <title>如何在 M1 MacBook Pro (Max) 上优化 Tensorflow/Keras 训练？</title>
      <link>https://stackoverflow.com/questions/78141065/how-to-optimize-tensorflow-keras-trainings-on-a-m1-macbook-pro-max</link>
      <description><![CDATA[我想知道如何优化配备 32GB RAM 的 M2 MB Pro Max 上的训练时间。有没有办法参数化TF框架？或者我可以插入 NVIDIA CUDA GPU 来加速该过程吗？
您有什么建议以及如何做？或者您只是在本地开发并在其他地方进行训练？
我想加快机器上的训练时间。我正在使用 TF 2.15.0 &amp; conda 环境中的 Python 3.11.5。]]></description>
      <guid>https://stackoverflow.com/questions/78141065/how-to-optimize-tensorflow-keras-trainings-on-a-m1-macbook-pro-max</guid>
      <pubDate>Mon, 11 Mar 2024 13:42:12 GMT</pubDate>
    </item>
    <item>
      <title>预测分位数与梯度增强回归相交</title>
      <link>https://stackoverflow.com/questions/78140825/prediction-quantiles-intersect-from-gradient-boosted-regression</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78140825/prediction-quantiles-intersect-from-gradient-boosted-regression</guid>
      <pubDate>Mon, 11 Mar 2024 13:05:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 Power Automate 将我的 Excel 输入连接到 Azure ML Studio API 端点并进行预测</title>
      <link>https://stackoverflow.com/questions/78140665/using-power-automate-to-connect-my-excel-inputs-to-azure-ml-studio-api-endpoint</link>
      <description><![CDATA[我正在研究如何使用 Power Automate 配置一个流程，该流程可以将我的 Excel 文件输入与我的 Azure ML Studio API 端点连接起来。理想情况下，当添加或修改 Excel 中的某一列值时，将触发此流程。
Excel 输入
基于 iris 数据集，我在 Azure ML Studio 中使用 API 端点创建了一个简单的机器学习模型。该模型需要 4 个输入，并根据这些输入返回预测标签。
将数据输入到测试端点
我创建了一个 Excel 文件，其中有 4 列（sepal_length、sepal_width、petal_length、petal_width），每列都包含一个值，如 Excel 输入屏幕截图中所示。
在 Power Automate 中，我能够使用“列出表中存在的行”操作连接到我的 Excel 文件。在此操作中，我可以找到我的表“Table1”。
我应该在 Power Automate 中采取哪些后续步骤来连接到 API 并能够接收预测结果？
这就是我的流程当前的样子。 当前流程
因此，我在将输入数据转换为可以作为 API 输入提供的格式时遇到了一些困难。下面的屏幕截图显示了我如何使用 Bearer 令牌和特定 URI 连接到我的 API。
API 连接]]></description>
      <guid>https://stackoverflow.com/questions/78140665/using-power-automate-to-connect-my-excel-inputs-to-azure-ml-studio-api-endpoint</guid>
      <pubDate>Mon, 11 Mar 2024 12:36:57 GMT</pubDate>
    </item>
    <item>
      <title>对模型进行 GAN 训练，根据 4 个人体测量输入（身高、腰部、胸部、臀部）生成 3D 模型</title>
      <link>https://stackoverflow.com/questions/78140368/gan-training-of-a-model-to-generate-3d-models-from-4-anthropometric-inputheight</link>
      <description><![CDATA[# 训练循环
纪元数 = 100
对于范围内的纪元（num_epochs）：
    对于 train_dataloader 中的批次：
        真实数据 = 批次
        fake_data = Generator(anthropometric_tensor) # 替换为实际输入

        # 训练鉴别器
        optim_d.zero_grad()
        real_labels = torch.ones(real_data.size(0), 1)
        fake_labels = torch.zeros(fake_data.size(0), 1)
        loss_real = 标准(鉴别器(real_data), real_labels)
        loss_fake = 标准(鉴别器(fake_data.detach()), fake_labels)
        loss_D = loss_real + loss_fake
        loss_D.backward()
        优化器_D.step()

        # 训练生成器
        Optimizer_G.zero_grad()
        loss_G = 标准（鉴别器（假数据），真实标签）
        loss_G.backward()
        优化器_G.step()

    print(f&quot;Epoch [{epoch}/{num_epochs}] Loss_D: {loss_D.item()} Loss_G: {loss_G.item()}&quot;)

我明白了
运行时错误：形状“[-1, 37500]”对于大小 64 的输入无效
]]></description>
      <guid>https://stackoverflow.com/questions/78140368/gan-training-of-a-model-to-generate-3d-models-from-4-anthropometric-inputheight</guid>
      <pubDate>Mon, 11 Mar 2024 11:43:20 GMT</pubDate>
    </item>
    <item>
      <title>如何在训练之前将 pytorch resnet50 模型的输入形状从 3, 224, 224 更改为 224, 224, 3</title>
      <link>https://stackoverflow.com/questions/78140043/how-do-i-change-the-input-shape-of-a-pytorch-resnet50-model-before-training-to-2</link>
      <description><![CDATA[在数据集上进行训练之前，如何更改 pytorch resnet50 模型的输入形状
当我将训练好的模型转换为 .tflite 格式以在 flutter 应用程序中使用时，我遇到了错误，该应用程序基本上希望我将模型的输入张量从 1, 3, 224 更改为 1, 224, 224, 3 ，224。]]></description>
      <guid>https://stackoverflow.com/questions/78140043/how-do-i-change-the-input-shape-of-a-pytorch-resnet50-model-before-training-to-2</guid>
      <pubDate>Mon, 11 Mar 2024 10:47:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 Huggingface MT5 模型中执行批量编码时会得到不同的嵌入？</title>
      <link>https://stackoverflow.com/questions/78139855/why-do-i-get-different-embeddings-when-i-perform-batch-encoding-in-huggingface-m</link>
      <description><![CDATA[我正在尝试使用 HuggingFace 的 mt5-base 模型对一些文本进行编码。我使用的模型如下所示
从转换器导入 MT5EncoderModel、AutoTokenizer

模型 = MT5EncoderModel.from_pretrained(“google/mt5-base”)
tokenizer = AutoTokenizer.from_pretrained(“google/mt5-base”)

def get_t5_embeddings(文本):
    last_hidden_​​state = model(input_ids=tokenizer(texts, return_tensors=“pt”, padding=True).input_ids).last_hidden_​​state
    pooled_sentence = torch.max(last_hidden_​​state, 暗淡=1)
    返回 pooled_sentence[0].detach().numpy()

当我注意到相同的文本与其自身的余弦相似度分数较低时，我正在做一些实验。我做了一些挖掘，意识到如果我批量进行编码，模型会返回非常不同的嵌入。为了验证这一点，我运行了一个小实验，逐步生成 Hello 的嵌入和 10 个 Hello 的列表。并检查列表中 Hello 和第一个 Hello 的嵌入（两者应该相同）。
对于范围 (1, 10) 内的 i：
    print(i, (get_t5_embeddings([“你好”])[0] == get_t5_embeddings([“你好”]*i)[0]).sum())

这将返回嵌入中相互匹配的值的数量。
结果是这样的：
&lt;前&gt;&lt;代码&gt;1 768
2 768
3 768
4 768
5 768
6 768
7 768
8 27
9 27

每次运行它时，如果批量大小超过 768，就会出现不匹配情况。
为什么我会得到不同的嵌入以及如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78139855/why-do-i-get-different-embeddings-when-i-perform-batch-encoding-in-huggingface-m</guid>
      <pubDate>Mon, 11 Mar 2024 10:14:53 GMT</pubDate>
    </item>
    <item>
      <title>Predict_proba() 给出的概率为 0 和 1，但中间值很少</title>
      <link>https://stackoverflow.com/questions/78139504/predict-proba-giving-probabilities-as-0s-and-1s-but-few-intermediate-values</link>
      <description><![CDATA[我正在研究乳腺癌检测分类问题。我已经从 Kaggle 下载了数据集： https://www.kaggle.com/数据集/yasserh/breast-cancer-dataset
我想预测：
a) 肿瘤是良性还是恶性
和
b) 肿瘤恶性的概率（0-1）是多少。
我正在实现随机森林分类器。
我面临的问题是，当我使用 rf_classifier.predict_proba() 方法时，我获得的概率包含大量 1 和 0，但中间值很少。理想情况下，我希望概率列中的所有值都是 0 到 1 之间的小数。
这种方法是实现目标的正确方法吗？如果是，如何解决这个问题？
分类器表现非常好。
这是我的代码的相关部分：
X_train、X_test、y_train、y_test = train_test_split(X、y、test_size=0.2)
定标器=标准定标器()

X_train = 缩放器.fit_transform(X_train)
X_test = 缩放器.transform(X_test)

rf_classifier = RandomForestClassifier()
rf_classifier.fit(X_train, y_train)

y_pred = rf_classifier.predict(X_test)

y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]

结果 = np.column_stack((y_test[:200], y_pred[:200], y_pred_proba[:200]))
np.set_printoptions(精度=2, 抑制=True)
print(&quot;实际|预测|概率&quot;)
打印（结果）

输出：

分类报告：
]]></description>
      <guid>https://stackoverflow.com/questions/78139504/predict-proba-giving-probabilities-as-0s-and-1s-but-few-intermediate-values</guid>
      <pubDate>Mon, 11 Mar 2024 09:13:47 GMT</pubDate>
    </item>
    <item>
      <title>Julia 和 MLJ 中的数据类型</title>
      <link>https://stackoverflow.com/questions/78139165/data-type-in-julia-and-mlj</link>
      <description><![CDATA[我是 Julia 的新手，正在尝试拟合一个简单的分类树
包导入和环境激活：
使用 Pkg
Pkg.activate(“.”)

使用 CSV
使用数据框
使用随机
使用下载
使用 ARFF 文件
使用科学类型
使用 DataFramesMeta
使用动态管道
使用MLJ
使用 MLJDecisionTreeInterface

数据：
titanic_reader = CSV.File(“/home/andrea/dev/julia/titanic.csv”; header = 1);
泰坦尼克号 = DataFrame(titanic_reader);

# 删除缺失值
泰坦尼克号 = dropmissing(泰坦尼克号);


泰坦尼克号 = @transform(泰坦尼克号,
    ：类=分类（：类），
    ：性别=分类（：性别），
    ：幸存=分类（：幸存）
    ）；

检查数据
第一（泰坦尼克号，3）

3×4 数据框
 排 │ 班级 性别 年龄 幸存
     │ 猫…猫…Float64 猫…
──────┼──────────────────────────────────
   1 │ 3 男 22.0 N
   2 │ 1 女 38.0 岁
   3 │ 3 女 26.0 岁

检查数据架构
架构（泰坦尼克号）；


┌──────────┬──────────────┬──────────────────────── ──────────────┐
│ 名称 │ scitypes │ 类型 │
├──────────┼──────────────┼────────────────────── ──────────────┤
│ 类 │ 多类{3} │ CategoricalValue{Int64, UInt32} │
│ 性别 │ 多类{2} │ CategoricalValue{String7, UInt32} │
│ 年龄 │ 连续 │ Float64 │
│ 幸存下来 │ 多类{2} │ CategoricalValue{String1, UInt32} │
└──────────┴──────────────┴────────────────────── ──────────────┘

架构对我来说似乎没问题
准备建模数据：
# 目标和功能
y, X = 解包(泰坦尼克号, ==(:幸存), rng = 123);

# 分区训练&amp;测试
(X_trn, X_tst), (y_trn, y_tst) = 分区((X, y), 0.75, multi=true, rng=123);

拟合模型：
&lt;前&gt;&lt;代码&gt;# 型号
mod = @load DecisionTreeClassifier pkg = “DecisionTree”; ;
fm = mod() ;
fm_mach = 机器(fm, X_trn, y_trn);

问题是这样的：
警告：数据参数的数量和/或类型与指定模型不匹配
│ 支持。通过指定“scitype_check_level=0”来抑制此类型检查。
│
│ 运行“@doc DecisionTree.DecisionTreeClassifier”以了解有关模型要求的更多信息。
│
│ 通常但非唯一地，监督模型是使用以下语法构建的
│ `machine(model, X, y)` 或 `machine(model, X, y, w)` 而大多数其他模型是
│ 用 `machine(model, X)` 构造。这里“X”是特征，“y”是目标，“w”
│ 样本或类别权重。
│
│ 一般来说，`machine(model, data...)`中的数据预计满足
│
│ scitype(数据) &lt;: MLJ.fit_data_scitype(模型)
│
│ 在本案中：
│
│ scitype(数据) = Tuple{Table{Union{AbstractVector{连续}, AbstractVector{Multiclass{3}}, AbstractVector{Multiclass{2}}}}, AbstractVector{Multiclass{2}}}
│
│ fit_data_scitype(model) = Tuple{Table{&lt;:Union{AbstractVector{&lt;:连续}, AbstractVector{&lt;:Count}, AbstractVector{&lt;:OrderedFactor}}}, AbstractVector{&lt;:有限}}
└ @ MLJBase ~/.julia/packages/MLJBase/eCnWm/src/machines.jl:231

显然，在拟合模型时：
适合！(fm_mach)

我收到错误
[信息：学习网络中的上游节点似乎正在提供不兼容的 scitype 数据。往上看。
错误：ArgumentError：无法使用 &lt; 测试无序 CategoricalValue 对象的顺序。使用 isless 代替，或者调用 ordered！父数组上的函数来更改此值
堆栈跟踪：

我几乎确定错误取决于数据类型规范，但是我无法找到解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78139165/data-type-in-julia-and-mlj</guid>
      <pubDate>Mon, 11 Mar 2024 08:07:08 GMT</pubDate>
    </item>
    <item>
      <title>在 MERN Stack 应用程序中集成线性回归：Python 还是 JavaScript？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78138619/integrating-linear-regression-in-mern-stack-application-python-or-javascript</link>
      <description><![CDATA[我使用 MERN 堆栈（MongoDB、Express.js、React、Node.js）开发了一个餐厅管理面板，现在正在寻求实现线性回归来预测销售或根据历史数据预测客户流量。后端完全采用 Node.js，但我正在考虑使用 Python 作为线性回归部分，因为它具有广泛的库和对数据科学的支持（如 NumPy、pandas 和 scikit-learn）。
我的困境是，是坚持使用 JavaScript/Node.js 以保持堆栈一致，还是将 Python 引入其中，以获取其卓越的机器学习功能。我担心将 Python 与现有 Node.js 后端集成的潜在复杂性以及如何管理两者之间的通信（如果这是我选择的路线）。另一方面，我也在考虑 Python 库可能为线性回归任务提供的性能和实现的简易性。
我希望了解以下方面的见解：

将 Python 进行线性回归集成到 MERN 堆栈应用程序的可行性和最佳实践。
如果我沿着这条路走下去，管理 Node.js 和 Python 之间通信的潜在挑战和解决方案。
如果留在 JS 生态系统内，可以有效处理线性回归的 JavaScript 库的建议。

最终，我正在寻找有关将线性回归合并到我的项目中的最佳路径的指导，权衡 Python 与 JavaScript 对于这个特定用例的优缺点。
我最初探索直接在 Node.js 中实现线性回归，希望保持一致的技术堆栈。我尝试了几个 JavaScript 库，例如用于基本统计操作的 simple-statistics 和用于更多面向机器学习的任务的 mljs，希望它们能够提供一种将线性回归模型应用到我的数据集的简单方法。
通过这些库，我成功地在 Node.js 中实现了基本的线性回归模型。我的期望是，这种方法不仅能够满足我所需的预测准确性，而且还可以通过避免跨语言集成来保持应用程序部署和维护的简单性。
然而，结果好坏参半。虽然我能够开发和运行线性回归模型，但我遇到了两个主要问题：

性能和可扩展性：JavaScript 解决方案适用于小型数据集，但当我尝试扩大数据大小以更接近地反映餐厅管理面板的实际使用场景时，性能未达到我的预期。处理时间比预期的要长，我开始担心这个解决方案的可扩展性。

功能集和易用性：虽然我使用的库提供了线性回归的基本功能，但我发现与我所知道的可用库相比，它们缺乏功能的广度和高级统计分析的易用性。 Python 的生态系统（例如 scikit-learn）。例如，我想要更复杂的方法来处理模型拟合、诊断和验证，以提高预测准确性，这在 Python 库中似乎更容易访问。


这些经历让我考虑将 Python 作为实现项目线性回归部分的替代方案，尽管我最初的目的是将所有内容保留在 Node.js 环境中。这里的期望不仅是实现更好的性能和可扩展性，而且还需要访问一组更丰富的数据分析和机器学习工具，以增强我的餐厅管理面板中预测的功能和准确性。]]></description>
      <guid>https://stackoverflow.com/questions/78138619/integrating-linear-regression-in-mern-stack-application-python-or-javascript</guid>
      <pubDate>Mon, 11 Mar 2024 05:49:33 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 绘制梯度下降曲线</title>
      <link>https://stackoverflow.com/questions/78137739/plotting-of-gradient-descent-curves-by-using-keras</link>
      <description><![CDATA[我在 Keras 中实现了以下代码，该代码使用加州住房数据集，试图绘制 theta 1 和 theta 2 的值，以及随机梯度下降、批量梯度或小批量的选择如何影响结果： 
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
从 keras.models 导入顺序
从 keras.layers 导入密集
从 keras.optimizers 导入 SGD
从 sklearn.datasets 导入 fetch_california_housing
从 sklearn.preprocessing 导入 StandardScaler

# 加载加州住房数据集
加州住房 = fetch_加州住房()
X, y = california_housing.data, california_housing.target

# 标准化特征
定标器=标准定标器()
X_归一化 = 缩放器.fit_transform(X)

def build_model():
    模型=顺序（[
    密集（64，激活=“relu”，input_shape=（8，）），
    密集(64，激活=“relu”)，
    密集(1)
    ]）
    #model.compile（优化器=“rmsprop”，损失=“mse”，指标=[“mae”]）
    返回模型


sgd_optimizer = SGD(lr=0.001) # 随机梯度下降
minibatch_sgd_optimizer = SGD(lr=0.001) # 小批量梯度下降
batch_sgd_optimizer = SGD(lr=0.001) # 批量梯度下降


# 编译模型
模型=build_model()
model.compile(loss=&#39;mse&#39;, 优化器=sgd_optimizer)

theta1_sgd、theta2_sgd = []、[]
theta1_minibatch_sgd、theta2_minibatch_sgd = []、[]
theta1_batch_sgd、theta2_batch_sgd = []、[]


# 执行梯度下降并存储 theta 值的函数
def Perform_gradient_descent（优化器，batch_size=None）：
    theta1_列表、theta2_列表 = []、[]
    损失历史记录 = []
    for _ in range(5): # 纪元数
        历史记录=model.fit(X_normalized, y, epochs=1,batch_size=batch_size, verbose=0)
        loss_history.append(history.history[&#39;loss&#39;][0])
        weights = model.layers[0].get_weights()[0].flatten() # 获取当前 theta 值
        theta1_list.append(权重[0])
        theta2_list.append(权重[1])
    打印（theta1_列表，“”，theta2_列表）
    返回loss_history，theta1_list，theta2_list

# 使用不同的优化器执行梯度下降
loss_sgd, theta1_sgd, theta2_sgd = Perform_gradient_descent(sgd_optimizer, batch_size=1) # 随机梯度下降
loss_minibatch_sgd, theta1_minibatch_sgd, theta2_minibatch_sgd = Perform_gradient_descent(minibatch_sgd_optimizer, batch_size=32) # 小批量梯度下降
loss_batch_sgd, theta1_batch_sgd, theta2_batch_sgd = Perform_gradient_descent(batch_sgd_optimizer, batch_size=len(X_normalized)) # 批量梯度下降

# 绘制损失与纪元数的关系图
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(loss_sgd) + 1), loss_sgd, label=&#39;随机梯度下降&#39;)
plt.plot(range(1, len(loss_minibatch_sgd) + 1), loss_minibatch_sgd, label=&#39;小批量梯度下降&#39;)
plt.plot(range(1, len(loss_batch_sgd) + 1), loss_batch_sgd, label=&#39;批量梯度下降&#39;)
plt.xlabel(&#39;历元数&#39;)
plt.ylabel(&#39;损失&#39;)
plt.title(&#39;损失与历元数&#39;)
plt.图例()
plt.网格（真）
plt.show()

# 绘制梯度下降轨迹
plt.figure(figsize=(10, 6))
plt.plot(theta1_sgd,theta2_sgd,label=&#39;随机梯度下降&#39;,marker=&#39;o&#39;)
plt.plot(theta1_minibatch_sgd, theta2_minibatch_sgd, label=&#39;小批量梯度下降&#39;,marker=&#39;s&#39;)
plt.plot(theta1_batch_sgd, theta2_batch_sgd, label=&#39;批量梯度下降&#39;,marker=&#39;x&#39;)
plt.xlabel(&#39;Theta 1&#39;)
plt.ylabel(&#39;Theta 2&#39;)
plt.title(&#39;梯度下降轨迹&#39;)
#plt.xlim(-0.08, -0.05) # 设置 Theta 1 的限制
#plt.ylim(0.02, 0.03) # 设置 Theta 2 的限制
plt.图例()
plt.网格（真）
plt.show()

但是，我发现的问题是，有时保存 theta 值的列表的值是 Nan，而在其他情况下是正常值。当 epoch 数量增加到 10 以上时，我注意到了这一点，这是为什么？]]></description>
      <guid>https://stackoverflow.com/questions/78137739/plotting-of-gradient-descent-curves-by-using-keras</guid>
      <pubDate>Sun, 10 Mar 2024 22:46:54 GMT</pubDate>
    </item>
    <item>
      <title>绘制 scikit-learn (sklearn) SVM 决策边界/曲面</title>
      <link>https://stackoverflow.com/questions/51297423/plot-scikit-learn-sklearn-svm-decision-boundary-surface</link>
      <description><![CDATA[我目前正在使用 python 的 scikit 库执行具有线性内核的多类 SVM。
样本训练数据和测试数据如下：
模型数据：

&lt;预&gt;&lt;代码&gt;x = [[20,32,45,33,32,44,0],[23,32,45,12,32,66,11],[16,32,45,12, 32,44,23],[120,2,55,62,82,14,81],[30,222,115,12,42,64,91],[220,12,55,222,82,14,181],[30,222,315, 12,222,64,111]]
y = [0,0,0,1,1,2,2]

我想绘制决策边界并可视化数据集。有人可以帮忙绘制此类数据吗？
上面给出的数据只是模拟数据，因此请随意更改值。
如果至少您能建议应遵循的步骤，那将会很有帮助。
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/51297423/plot-scikit-learn-sklearn-svm-decision-boundary-surface</guid>
      <pubDate>Thu, 12 Jul 2018 04:43:23 GMT</pubDate>
    </item>
    </channel>
</rss>