<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 05 Dec 2023 21:13:03 GMT</lastBuildDate>
    <item>
      <title>tfjs - 训练自己的手模型，在所有轴上旋转</title>
      <link>https://stackoverflow.com/questions/77609087/tfjs-train-an-own-hand-model-with-rotation-on-all-axis</link>
      <description><![CDATA[我对整个主题相当陌生，但对于我的项目来说，遗憾的是我无法使用那个很棒的 mediapipe 手部跟踪工具，因为它不跟踪具有较长比例的特殊手部假肢。我研究了很多东西，但不确定下一步该怎么做。是否可以使用另一只手的 50 - 100 张新图片来微调现有的 .tflite 模型，然后使用我新训练的模型？]]></description>
      <guid>https://stackoverflow.com/questions/77609087/tfjs-train-an-own-hand-model-with-rotation-on-all-axis</guid>
      <pubDate>Tue, 05 Dec 2023 20:32:00 GMT</pubDate>
    </item>
    <item>
      <title>一个张量流管道，输入 shape=(256, 256, 3), dtype=tf.float32 图像，使用 MTCNN() 提取人脸。我就是无法完成这件事</title>
      <link>https://stackoverflow.com/questions/77608982/a-tensorflow-pipeline-that-inputs-shape-256-256-3-dtype-tf-float32-image</link>
      <description><![CDATA[一个张量流 2.15 管道，需要两个 256X256X3、uint8 图像...“input_image”、“real_image”，来自之前的 tf.data.Dataset 类型管道，使用 MTCNN.detect_faces() 从 input_image 中提取面部，并在图像上绘制面加上 5% 额外的有界框作为相同大小的输入图像，并返回 input_image 和 real_image 而不更改 real_image
。
..
...
....
def extract_faces_from_tensors(input_image, real_image, margin_percent=5, required_size=(256, 256)):
# 将输入图像张量转换为 NumPy 数组
input_image = input_image.numpy()
# 检测人脸
检测器 = MTCNN()
faces = detector. detector_faces(input_image)

脸部图像 = []
真实图像 = 真实图像.numpy()

如果面临：
    对于面孔中的面孔：
        # 从请求的面中提取带有边距的边界框
        x, y, 宽度, 高度 = 面[&#39;box&#39;]
        边距 = int(min(宽度, 高度) * (margin_percent / 100.0))
        x1, y1 = max(x - 边距, 0), max(y - 边距, 0)
        x2, y2 = x + 宽度 + 边距, y + 高度 + 边距

        # 提取有边缘的脸
        面边界 = 输入图像[y1:y2, x1:x2]

        # 在创建 PIL 图像之前转换为 uint8
        面边界 = np.uint8(面边界)

        # 从数组创建一个 PIL 图像
        face_image = Image.fromarray(face_boundary)

        # 将像素大小调整为所需的大小
        面部图像 = 面部图像.调整大小(required_size)
        
        # 转换为 float32 并标准化为范围 [0, 1]
        face_array = tf.cast(np.array(face_image), tf.float32) / 255.0
        真实图像 = tf.cast(真实图像, tf.float32)/255.0
        face_images.append(face_array)
        输入图像=人脸图像

返回输入图像、真实图像

tr_data = tr_data.map(extract_faces_from_tensors, num_parallel_calls=tf.data.AUTOTUNE)
tr_数据
。
..
...
属性错误：在用户代码中：
文件“/tmp/ipykernel_47/4070542639.py”，第 3 行，位于 extract_faces_from_tensors *
    输入图像 = 输入图像.numpy()

AttributeError：“SymbolicTensor”对象没有属性“numpy”

过去两天我一直陷入这个问题，已经尝试了 gpt、co-pilot 一切。
请大家帮忙]]></description>
      <guid>https://stackoverflow.com/questions/77608982/a-tensorflow-pipeline-that-inputs-shape-256-256-3-dtype-tf-float32-image</guid>
      <pubDate>Tue, 05 Dec 2023 20:05:29 GMT</pubDate>
    </item>
    <item>
      <title>我有 300 个课程标题，需要将它们分组为未知数量的主题。这项作业推荐使用什么无监督学习方法[关闭]</title>
      <link>https://stackoverflow.com/questions/77608183/i-have-300-course-titles-and-need-to-group-them-into-an-unknown-number-of-topics</link>
      <description><![CDATA[此问题涉及将 300 个课程标题分类为几个总体主题，而事先不知道这些主题的数量或性质。挑战在于确定这些主题的数量和内容。在此背景下，我们正在寻找一种合适的无监督学习方法来完成这一任务。
我很困惑，不知道该怎么做]]></description>
      <guid>https://stackoverflow.com/questions/77608183/i-have-300-course-titles-and-need-to-group-them-into-an-unknown-number-of-topics</guid>
      <pubDate>Tue, 05 Dec 2023 17:31:29 GMT</pubDate>
    </item>
    <item>
      <title>缺陷的时间演变：预测剩余缺陷</title>
      <link>https://stackoverflow.com/questions/77607265/temporal-evolution-of-defects-predicting-remaining-defect</link>
      <description><![CDATA[我在一家公司的人工智能论文是预测 10 分钟或 Xtime 后纸张上剩余的缺陷

墨水印刷在金属板上
打印后直接出现一些缺陷（打印步骤后1秒拍照）
但一段时间后，一些缺陷消失了，一些缺陷仍然存在

我的目标是打印一张新纸张并拍摄即时照片后：使用机器/深度学习预测 10 分钟或 Xtime 后剩余的缺陷
说明（涂上墨水后涂抹更多并覆盖疤痕等缺陷）
我有多种资源，如相机、机器等
我愿意接受任何想法]]></description>
      <guid>https://stackoverflow.com/questions/77607265/temporal-evolution-of-defects-predicting-remaining-defect</guid>
      <pubDate>Tue, 05 Dec 2023 15:12:18 GMT</pubDate>
    </item>
    <item>
      <title>我如何开始学习数据科学和机器学习请建议一条明确的路径？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77607227/how-can-i-start-learning-data-science-and-machine-learning-please-suggest-a-well</link>
      <description><![CDATA[我希望以机器学习工程师的身份从事数据科学和机器学习的职业，但是我找不到一条有明确指导和良好布局的道路，这个领域似乎是多样化的，我只是感到困惑。
人们只是告诉要学习 python 和 numpy、pandas、用于可视化的 matplot 等库，这很模糊，我已经知道 python 我找不到合适的资源来从基础理论开始学习数据科学和机器学习基础知识，并在实践中通过建议提供帮助参考资料、书籍、讲座、课程我才刚刚开始。]]></description>
      <guid>https://stackoverflow.com/questions/77607227/how-can-i-start-learning-data-science-and-machine-learning-please-suggest-a-well</guid>
      <pubDate>Tue, 05 Dec 2023 15:04:53 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助......Apple ML 给出奇怪的结果[关闭]</title>
      <link>https://stackoverflow.com/questions/77607078/need-help-apple-ml-giving-out-weird-results</link>
      <description><![CDATA[我的目标是使用 WLASL 数据集创建一个将手语转换为文本的模型。现在，从一开始就从kaggle下载这个模型，虽然数据集看起来相当全面，但每个类别的视频数量从5到13个不等，这显然需要训练的内容相当少。我决定尝试 Apple Create ML，而不是像 Tensorflow 或更复杂的深度学习框架，因为这样会简单得多。由于数据集在每个类别的视频方面非常有限，因此我在“手部动作分类器”中使用了所有 6 个数据增强。 （水平翻转、旋转、平移、缩放、插帧、丢帧）。虽然我知道这无法保存模型，但它肯定会大大提高准确性。请注意，我没有使用数据集中的所有 2000 个类（单词），而是仅使用了 300 个类的子集。我获得了 16% 的验证准确率，以及 90% 的所有增强训练准确率，因此我的模型显然过度拟合。所以我对 25 个类进行了同样的尝试，这次我获得了 42% 的验证准确率，以及 100% 的训练准确率。再次，过度拟合。我进入实时预览，几乎我尝试的每个迹象都被预测为错误。
现在，我决定使用“模型源”在侧边栏中。我不太确定它们的用途，但这是我尝试过的：
我将数据子集分成 2 个独立的模型源（16 个类，但数量仍然很高），分别获得了 83% 的验证准确率和 90% 的验证准确率。这两个模型源都使用所有数据增强。我的模型显然过度拟合，在两个来源中都有 100% 的训练准确度，但将其分成两个模型显然提高了我的准确度，当我在“实时预览”中测试这一点时，我自己做的每个 ASL 标志都能够以超过 90% 的置信度准确猜出每个单词。
所以我的问题是，即使我的数据有限（虽然增强确实增加了很多，但显然性能差异不应该这么大），我的模型如何表现得这么好？此外，将一个模型拆分为单独的模型源是否可行？我不确定“模型来源”有什么用？甚至是，所以我尝试了这个，不知怎的，我得到了更好的结果。如果可行，我如何将它们实现到一个快速应用程序中。我现在有点困惑，所以希望有人能告诉我发生了什么事。如果这不是一个可行的解决方案，有人可以提供另一个解决方案来说明我如何使用这个数据集吗？事先了解它会非常有帮助，但即使你不知道，你能帮助我吗？
Kaggle链接：https://www.kaggle.com/datasets/risangbaskoro/ wlasl 处理
原始论文github页面：https://github.com/dxli94/WLASL]]></description>
      <guid>https://stackoverflow.com/questions/77607078/need-help-apple-ml-giving-out-weird-results</guid>
      <pubDate>Tue, 05 Dec 2023 14:43:09 GMT</pubDate>
    </item>
    <item>
      <title>运行“docker build -t file-name”时构建 docker 映像会导致错误。</title>
      <link>https://stackoverflow.com/questions/77606730/building-the-docker-image-results-in-error-while-running-docker-build-t-file-n</link>
      <description><![CDATA[我正在开发一个机器学习模型，我正在尝试使用 Docker 将其容器化，
运行命令时
docker build -t &lt;文件名&gt; &lt;位置&gt;
下面是docker文件。
&lt;前&gt;&lt;代码&gt;来自 python:3.11-alpine
复制 。 /应用程序
工作目录/应用程序
运行 pip install -rrequirements.txt
CMD Streamlit 运行 viz_app.python

下面是我收到的错误。
22.34 注意：此错误源自子进程，并且可能不是 pip 的问题。
22.34 错误：子进程退出并出现错误
22.34
22.34 × 用于安装构建依赖项的 pip 子进程未成功运行。
22.34 │ 退出代码：1
22.34 ╰─&gt;请参阅上面的输出。
22.34
22.34 注意：此错误源自子进程，并且可能不是 pip 的问题。
22.36
22.36 [通知] pip 新版本发布：23.0.1 -&gt; 23.3.1
22.36 [注意] 要更新，请运行： pip install --upgrade pip
------
Dockerfile:4
--------------------
   2 |复制 。 /应用程序
   3 |工作目录/应用程序
   4 | &gt;&gt;&gt;&gt;&gt;运行 pip install -rrequirements.txt
   5 | CMD Streamlit 运行 viz_app.python
--------------------
错误：无法解决：进程“/bin/sh -c pip install -rrequirements.txt”未成功完成：退出代码：1

我尝试使用以下方法解决特定问题。

我尝试通过提及“RUN pip install --upgrade pip”来升级 pip
还尝试安装需求文件的不同版本的软件包。

为了进一步参考，我添加了下面的requirement.txt 文件的包。
&lt;前&gt;&lt;代码&gt;numpy
熊猫
流光溢彩
scikit学习
绘图库
OpenCV-Python
张量流
分割模型
]]></description>
      <guid>https://stackoverflow.com/questions/77606730/building-the-docker-image-results-in-error-while-running-docker-build-t-file-n</guid>
      <pubDate>Tue, 05 Dec 2023 13:51:20 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow.js 散点图显示预测维护与实际维护相差甚远，我不明白为什么</title>
      <link>https://stackoverflow.com/questions/77606726/tensorflow-js-scatterplot-showing-predicted-vs-actual-maintenance-very-far-apart</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77606726/tensorflow-js-scatterplot-showing-predicted-vs-actual-maintenance-very-far-apart</guid>
      <pubDate>Tue, 05 Dec 2023 13:50:52 GMT</pubDate>
    </item>
    <item>
      <title>端到端 ML 项目上的模型训练器问题 - TypeError：__init__() 获得意外的关键字参数“trained_model_file_path”</title>
      <link>https://stackoverflow.com/questions/77606532/model-trainer-issue-on-end-to-end-ml-project-typeerror-init-got-an-unex</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77606532/model-trainer-issue-on-end-to-end-ml-project-typeerror-init-got-an-unex</guid>
      <pubDate>Tue, 05 Dec 2023 13:22:19 GMT</pubDate>
    </item>
    <item>
      <title>后端的大.pkl数据没有推送到github中</title>
      <link>https://stackoverflow.com/questions/77600252/large-pkl-data-for-backend-is-not-pushed-in-github</link>
      <description><![CDATA[我正在学习机器学习。最近，我从 tmdb 数据集制作了电影推荐模型，我使用 .pkl （二进制）文件中的模型处理数据。使用该数据制作后端，但是数据太大，无法推送到 github，我无法托管网站。
我正在尝试将已处理的数据推送到后端，但无法部署，因为它超出了文件大小的限制]]></description>
      <guid>https://stackoverflow.com/questions/77600252/large-pkl-data-for-backend-is-not-pushed-in-github</guid>
      <pubDate>Mon, 04 Dec 2023 14:33:48 GMT</pubDate>
    </item>
    <item>
      <title>如何修复我的感知器来识别数字？</title>
      <link>https://stackoverflow.com/questions/77594625/how-can-i-fix-my-perceptron-to-recognize-numbers</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77594625/how-can-i-fix-my-perceptron-to-recognize-numbers</guid>
      <pubDate>Sun, 03 Dec 2023 14:03:49 GMT</pubDate>
    </item>
    <item>
      <title>在 LightGBM 中使用不同 boosting 类型的数据采样方法</title>
      <link>https://stackoverflow.com/questions/77578111/use-of-data-sample-methods-with-different-boosting-types-in-lightgbm</link>
      <description><![CDATA[我的问题
我不太清楚所有参数的用法以及它们如何相互交互（或应该使用）。
我所知道的
据我了解，LightGBM中有3种算法：

GBDT，默认的，使用 boosting
DART 是一种带有 dropout 的 boosting 算法
随机森林，不使用增强（确实如此，但仅在一次迭代中）

并且有两种数据采样策略：

Bagging，默认设置，用于集成学习
GOSS 选择更多对误差梯度贡献最大的数据（我们的想法是，我们需要对远离基线的数据进行更多训练），而对“弱”数据进行更少的训练。数据点（对误差梯度贡献较小的数据点）。

问题
所以我的问题如下：

Bagging 和 GOSS 似乎能够协同工作，但 data_sample_method 参数阻止我们这样做，因为它是一个字符串参数。这是有意为之的行为吗？
在选择 GOSS 数据样本方法时，如果我们提供 bagging_fraction 和 bagging_freq 参数，会发生什么情况？文档没有提到这两个需要 badding 示例方法。
LightGBM的主要创新似乎是GOSS，但它并不是默认选择，这样选择的动机是什么？
最后，我们能够将 boosting_type=goss 作为参数传递。当我们这样做时会发生什么？算法会是GBDT，而数据样本策略是goss吗？

非常感谢您抽出时间。
祝你有美好的一天。]]></description>
      <guid>https://stackoverflow.com/questions/77578111/use-of-data-sample-methods-with-different-boosting-types-in-lightgbm</guid>
      <pubDate>Thu, 30 Nov 2023 11:34:21 GMT</pubDate>
    </item>
    <item>
      <title>Lightgbm 中“is_unbalance”参数的使用</title>
      <link>https://stackoverflow.com/questions/68738225/use-of-is-unbalance-parameter-in-lightgbm</link>
      <description><![CDATA[我尝试在模型训练中使用“is_unbalance”参数来解决二元分类问题，其中正类约为 3%。如果我设置参数“is_unbalance”，我会观察到二进制对数损失在第一次迭代中下降，但随后继续增加。仅当我启用此参数“is_unbalance”时，我才会注意到此行为。否则，log_loss 会持续下降。感谢您对此的帮助。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/68738225/use-of-is-unbalance-parameter-in-lightgbm</guid>
      <pubDate>Wed, 11 Aug 2021 08:05:52 GMT</pubDate>
    </item>
    <item>
      <title>训练+测试集是否必须与预测集不同（以便您需要对所有列应用时移）？ （没有时间序列！）[关闭]</title>
      <link>https://stackoverflow.com/questions/59210109/does-the-trainingtesting-set-have-to-be-different-from-the-predicting-set-so-t</link>
      <description><![CDATA[TLDR：
这个问题与经典的机器学习时间序列分析无关，而是试图将每月的列作为纯粹的特征来处理，就像任何永恒的特征一样。我分享了这个问题，因为我在工作中遇到了这个挑战，最后，该模型在这种设置下运行良好，将非每月（永恒）功能与每月功能混合在一起。因此，这只是一个使用每月数据列作为特征的问题，模型并不关心是12月还是6月，它只关心这些特征是过去多少个月，以便它从模式中学习大约 x 个月前的数据。这些特征不是按月份名称来命名的，而是按它们回溯到过去的月份来命名的，例如，reality_month_1、reality_month_2 表示回溯 1 或 2 个月的财富。
&lt;小时/&gt;
我知道我们应该仅在测试集上测试经过训练的分类器的一般规则。
但现在出现了问题：当我准备好经过训练和测试的分类器时，我可以将其应用到作为训练和测试集基础的同一数据集吗？&lt; /em&gt; 或者我是否必须将其应用于与训练+测试集不同的新预测集？
如果我预测时间序列的标签列怎么办（稍后编辑：我并不是想在这里创建经典的时间序列分析，而是只是从典型数据库中广泛选择列，每周、每月或随机存储的数据，我将其转换为单独的特征列，每个特征列为一周/一个月/一年...），我是否必须移动全部将训练+测试集的特征（不仅是时间序列标签列的过去列，还包括所有其他正常特征）设置回数据没有“知识”的时间点与预测集的拦截？
然后，我将根据过去 n 个月的特征来训练和测试分类器，针对未移动且最新的标签列进行评分，然后根据最近未移动的特征进行预测。移位和未移位的特征具有相同的列数，我通过将移位特征的列名称分配给未移位的特征来对齐移位和未移位的特征。
附注：
p.s.1：https://en.wikipedia.org/wiki/Dependent_and_independent_variables&lt; 的一般方法/a&gt;
在数据挖掘工具（用于多元统计和机器学习）中，因变量被分配为目标变量（或在某些工具中为标签属性），而自变量可能被分配为常规变量。[ 8]为训练数据集和测试数据集提供了目标变量的已知值，但应对其他数据进行预测。
p.s.2：在这个基本教程中，我们可以看到预测集有所不同：https://scikit-learn.org/stable/tutorial/basic/tutorial.html
我们使用 [:-1] Python 语法选择训练集，它会生成一个包含所有 &gt; 的新数组。但digits.data 中的最后一项：[…] 现在您可以预测新值。在这种情况下，您将使用digits.data [-1:]中的最后一个图像进行预测。通过预测，您将从训练集中确定与最后一个图像最匹配的图像。]]></description>
      <guid>https://stackoverflow.com/questions/59210109/does-the-trainingtesting-set-have-to-be-different-from-the-predicting-set-so-t</guid>
      <pubDate>Fri, 06 Dec 2019 09:16:37 GMT</pubDate>
    </item>
    <item>
      <title>正则化参数在正则化中如何工作？</title>
      <link>https://stackoverflow.com/questions/44742122/how-does-regularization-parameter-work-in-regularization</link>
      <description><![CDATA[在机器学习成本函数中，如果我们想最小化两个参数（例如 theta3 和 theta4）的影响，似乎我们必须给出一个较大的正则化参数值，如下式所示。

我不太清楚为什么更大的正则化参数会减少而不是增加影响。这个功能是如何工作的？]]></description>
      <guid>https://stackoverflow.com/questions/44742122/how-does-regularization-parameter-work-in-regularization</guid>
      <pubDate>Sun, 25 Jun 2017 00:26:41 GMT</pubDate>
    </item>
    </channel>
</rss>