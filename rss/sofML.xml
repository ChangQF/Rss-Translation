<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 21 May 2024 18:18:37 GMT</lastBuildDate>
    <item>
      <title>联合概率分布如何帮助生成事物？</title>
      <link>https://stackoverflow.com/questions/78513688/how-does-the-joint-probability-distribution-help-to-generate-things</link>
      <description><![CDATA[我试图理解判别模型和生成模型之间的区别。堆栈溢出的有用答案之一在这里：生成算法和判别算法有什么区别？
在最上面的答案中（请参阅上面的链接），有一个简单的示例，其中只有四个 (x,y) 形式的数据点。答案的作者说了以下内容：分布 p(y|x) 是将给定示例 x 分类为类 y 的自然分布code&gt;，这就是为什么直接建模的算法被称为判别算法。生成算法模型p(x,y)，可以应用贝叶斯规则将其转化为p(y|x)，然后用于分类。然而，分布p(x,y)也可以用于其他目的。例如，您可以使用p(x,y)生成可能的(x,y)对。
我不太明白如何使用p(x,y)来生成可能的(x,y)对。有人可以帮助我给出使用联合概率分布p(x,y)生成的(x,y)对的具体示例吗？另外，有人还可以告诉我为什么条件概率分布p(y|x)不能用于生成新的对吗？非常感谢。
我对此很困惑。请帮助我。]]></description>
      <guid>https://stackoverflow.com/questions/78513688/how-does-the-joint-probability-distribution-help-to-generate-things</guid>
      <pubDate>Tue, 21 May 2024 18:13:38 GMT</pubDate>
    </item>
    <item>
      <title>Model.forecast() 不显示预测下一个日期的数据，而是预测训练数据中已有的日期的数据</title>
      <link>https://stackoverflow.com/questions/78513216/model-forecast-is-not-showin-forecasting-the-data-for-the-next-date-rather-is</link>
      <description><![CDATA[Python 中的问题：model.forecast() 不显示下一个日期
我在 Python 中遇到 model.forecast() 函数的问题。具体来说，它不显示下一个日期，即我用来训练的 y_train 的最后一天的第二天。详细信息如下：
我正在从事一个时间序列预测项目。
我已将数据拆分为 y_train 和 y_test。
我正在使用 AutoReg 库 AutoReg。
这是我的代码的简化版本：
从 statsmodels.tsa.ar_model 导入 AutoReg

data = pd.DataFrame(df).set_index(“日期”)
data.index = pd.DatetimeIndex(data.index).to_period(&#39;D&#39;)
# 删除 nan 值
data.fillna(method=“ffill”, inplace=True)


series_data=data[“价格”]


截止 = int(len(series_data)*0.9)
y_train = series_data.iloc[:截止]
y_test = series_data.iloc[截止：]

# 将自回归模型拟合到历史记录中，滞后=13
模型 = AutoReg(y_train, lags=13).fit()
# 预测下一个值
预测=模型.预测()

# 显示预测
打印（预测）

输出：2011-09-08 2.799414
频率：D，数据类型：float64

y_test.head(1)
输出：日期2018-04-23 2.78
 

问题：问题：
库输出不包含 y_train 中最后一个日期之后的下一个日期。相反，它显示[描述它显示的内容，如果有的话]。
我尝试过的：

我已检查 y_train 中的日期格式正确且连续。

我尝试在 forecast() 函数中的步骤使用不同的值。

期望的结果：
我希望预测显示 y_train 中最后一天的第二天的预测值。
其他信息：

我正在使用 AutoReg 库。

print(forecast) 应该给我 Date2018-04-23 而不是 2011-09-08

任何有关如何解决此问题的意见或建议将不胜感激。
print(forecast) 应该给我 Date2018-04-23 而不是 2011-09-08]]></description>
      <guid>https://stackoverflow.com/questions/78513216/model-forecast-is-not-showin-forecasting-the-data-for-the-next-date-rather-is</guid>
      <pubDate>Tue, 21 May 2024 16:34:13 GMT</pubDate>
    </item>
    <item>
      <title>如何优化Python的多线程性能以实现实时机器学习预测？</title>
      <link>https://stackoverflow.com/questions/78512908/how-to-optimize-pythons-multithreaded-performance-for-real-time-machine-learnin</link>
      <description><![CDATA[我目前正在使用 Python 开发实时机器学习应用程序，并且面临着优化多线程性能以减少模型预测延迟的挑战。即使有高效的模型架构，线程方面似乎也是瓶颈。
这是我使用并发.futures.ThreadPoolExecutor 的 Python 代码的相关部分：
导入并发.futures
将 numpy 导入为 np

def 预测（模型，数据）：
    # 模拟预测
    返回模型.预测（数据）

def main():
    model = load_your_model() # 加载预训练模型的假设函数
    data = np.random.rand(100, 10) # 模拟输入的随机数据

    以并发.futures.ThreadPoolExecutor(max_workers=5) 作为执行器：
        futures = [executor.submit(predict, model, data[i]) for i in range(100)]
        results = [f.result() for f in future]

    print(&quot;预测结果：&quot;, results)

如果 __name__ == &#39;__main__&#39;:
    主要的（）

这种方法没有产生预期的吞吐量，我怀疑 GIL（全局解释器锁）可能会导致问题，尤其是 I/O 绑定任务与 CPU 绑定任务混合在一起时。
在这种情况下实现最大并发性和并行性的最佳实践是什么？
过渡到多处理或使用 asyncio 等库会显着提高性能吗？
是否有已知的 Python 机器学习或并发库或框架可以更有效地处理此类实时预测需求？
任何见解或替代方法建议将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78512908/how-to-optimize-pythons-multithreaded-performance-for-real-time-machine-learnin</guid>
      <pubDate>Tue, 21 May 2024 15:30:51 GMT</pubDate>
    </item>
    <item>
      <title>我应该选择哪种编码方法 OneHotEncoding 或 Ordinal Encoding 或 LabelEncoding [关闭]</title>
      <link>https://stackoverflow.com/questions/78511767/which-encoding-method-should-i-choose-onehotencoding-or-ordinal-encoding-or-labe</link>
      <description><![CDATA[我正在处理 25 个分类列（特征）以将它们转换为数字。对于在 OneHotEncoding 或 OrdinalENcoding 之间选择适当的编码技术感到困惑。选择 OneHotEncoding 将为列中的每个唯一值创建新列并将其添加到 DataFrame 中，并可能导致多重共线性或模型过度拟合问题。
我尝试将 OneHotEncoding 应用于具有 &lt;= 2 个唯一值（二进制类别）的列，并将 OrdinalEncoding 应用于包含 2 个以上唯一值的列。这将有助于在一定程度上保持较低的维度。]]></description>
      <guid>https://stackoverflow.com/questions/78511767/which-encoding-method-should-i-choose-onehotencoding-or-ordinal-encoding-or-labe</guid>
      <pubDate>Tue, 21 May 2024 12:12:26 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 RTX 4090 训练 Mask RCNN？</title>
      <link>https://stackoverflow.com/questions/78511541/is-it-possible-to-train-mask-rcnn-with-a-rtx-4090</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78511541/is-it-possible-to-train-mask-rcnn-with-a-rtx-4090</guid>
      <pubDate>Tue, 21 May 2024 11:27:34 GMT</pubDate>
    </item>
    <item>
      <title>如何从 Hugging 脸上的 Mbart 微调模型中获取 GGUF 文件？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78511521/how-do-you-get-the-gguf-file-from-a-mbart-fine-tuned-model-on-hugging-face</link>
      <description><![CDATA[我已经对 Mbart 进行了微调，以根据英语新闻生成印地语内容，它工作正常，但我想要它是 gguf 文件，我尝试了各种方法来做到这一点，但似乎没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/78511521/how-do-you-get-the-gguf-file-from-a-mbart-fine-tuned-model-on-hugging-face</guid>
      <pubDate>Tue, 21 May 2024 11:22:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 YOLO-v8 TFLite 模型比 Pytorch 模型慢？</title>
      <link>https://stackoverflow.com/questions/78510654/why-is-my-yolo-v8-tflite-model-slower-than-my-pytorch-model</link>
      <description><![CDATA[我最初有一个经过训练的 Pytorch YOLO-v8 nano 模型，用于视频中的多对象检测（10 个类别 -“自行车”、“椅子”、“盒子”、“桌子”、“塑料袋”） ;、“花盆”、“行李箱​​包”、“雨伞”、“购物车”、“人”）。
我使用 ultralytics 库的导出功能将其转换为 TFLite 模型。然而，当我在视频流上运行这两个模型时，我的 TFLite 模型的运行速度（FPS 约为 8）比我的 Pytorch 模型（FPS 约为 20）慢得多。为什么会这样？
TFLite 和 Pytorch 模型均位于：https://drive .google.com/drive/folders/1A2XUD5sV332nXv-Z756Di_QUIZV3ObFv?usp=sharing
从经过训练的 Pytorch 模型到 tflite 模型的转换。
从 ultralytics 导入 YOLO

模型 = YOLO(“yolov8n_trained.pt”)
路径 = model.export(format=&quot;tflite&quot;)

在模型上运行视频流：
从 ultralytics 导入 YOLO
导入CV2
从导入时间开始

# 启动网络摄像头
Stream_url = “视频流路径”

cap = cv2.VideoCapture(stream_url) # 使用 0 表示网络摄像头
上限设置(3, 640)
上限设置(4, 640)

# 加载重新训练的 YOLOv8 模型
model = YOLO(“yolov8n_trained.tflite”) # 测试 tflite 模型时使用它
# model = YOLO(&quot;yolov8n_trained.pt&quot;) # 测试 pytorch 模型时使用它

# 自定义类名
classNames = [“自行车”,“椅子”,“盒子”,“桌子”,“塑料袋”,“花盆”,
              “行李箱包”、“雨伞”、“购物车”、“人”]

# 初始化变量来计算帧速率
上一个时间 = 0
帧率 = 0

而真实：
    成功，img = cap.read()
    如果没有成功：
        休息

    #计算处理帧所花费的时间
    curr_time = 时间()
    fps = 1 / (当前时间 - 上一个时间)
    上一个时间 = 当前时间

    #在图像上显示帧速率
    cv2.putText(img, f&quot;FPS: {fps:.2f}&quot;, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

    # 在图像上运行 YOLO 模型
    结果=模型（img，流= True）

    # 处理检测结果
    对于结果中的 r：
        对于 r.boxes 中的框：
            # 提取边界框坐标和置信度
            x1, y1, x2, y2 = map(int, box.xyxy[0]) # 转换为整数
            confidence = box.conf[0] # 置信度得分
            class_id = box.cls[0] # 类 ID

            # 绘制边界框
            color = (0, 255, 0) # 边界框的绿色
            cv2.矩形（img，（x1，y1），（x2，y2），颜色，2）

            # 绘制带有类名和置信度的标签
            label = f“{classNames[int(class_id)]}：{置信度：.2f}”
            label_size, base_line = cv2.getTextSize(标签, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            y1 = max(y1, label_size[1])
            cv2.矩形(img, (x1, y1 - label_size[1]), (x1 + label_size[0], y1 + base_line), (0, 255, 0), cv2.FILLED)
            cv2.putText(img, 标签, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)

    # 显示网络摄像头源
    cv2.imshow(&#39;网络摄像头&#39;, img)
    如果 cv2.waitKey(1) == ord(&#39;q&#39;):
        休息

# 释放资源
cap.release()
cv2.destroyAllWindows()

]]></description>
      <guid>https://stackoverflow.com/questions/78510654/why-is-my-yolo-v8-tflite-model-slower-than-my-pytorch-model</guid>
      <pubDate>Tue, 21 May 2024 08:49:32 GMT</pubDate>
    </item>
    <item>
      <title>DecisionTreeRegressor 算法如何工作？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78510567/how-does-the-decisiontreeregressor-algorithm-work</link>
      <description><![CDATA[我是人工智能新手，老实说，我不明白决策树回归算法为何有效。
我尝试研究其背后的算法，但找不到令我满意的答案。
# 导入数组和东西的 numpy 包
将 numpy 导入为 np

# 导入 matplotlib.pyplot 来绘制我们的结果
将 matplotlib.pyplot 导入为 plt

# import pandas 用于导入 csv 文件
将 pandas 导入为 pd

# 导入数据集
# 数据集 = pd.read_csv(&#39;Data.csv&#39;)
# 或者打开.csv文件来读取数据

数据集 = np.array(
[[&#39;资产翻转&#39;, 100, 1000],
[&#39;基于文本&#39;, 500, 3000],
[&#39;视觉小说&#39;, 1500, 5000],
[&#39;2D 像素艺术&#39;, 3500, 8000],
[&#39;2D 矢量艺术&#39;, 5000, 6500],
[&#39;策略&#39;, 6000, 7000],
[&#39;第一人称射击游戏&#39;, 8000, 15000],
[&#39;模拟器&#39;, 9500, 20000],
[&#39;赛车&#39;, 12000, 21000],
[&#39;角色扮演&#39;, 14000, 25000],
[&#39;沙盒&#39;, 15500, 27000],
[&#39;开放世界&#39;, 16500, 30000],
[&#39;MMOFPS&#39;, 25000, 52000],
[&#39;MMORPG&#39;, 30000, 80000]
]）

# 打印数据集
打印（数据集）

# 按 : 选择所有行和第 1 列
# 按1:2表示特征
X = 数据集[:, 1:2].astype(int)

# 打印X
打印（X）

# 按 : 选择所有行和第 2 列
# 由2到Y代表标签
y = 数据集[:, 2].astype(int)

# 打印y
打印（y）

# 导入回归器
从 sklearn.tree 导入 DecisionTreeRegressor

# 创建一个回归器对象
回归器 = DecisionTreeRegressor(random_state = 0)

# 用 X 和 Y 数据拟合回归器
回归器.fit(X, y)

# 预测一个新值

# 通过更改值来测试输出，例如 3750
y_pred = regressor.predict([[3750]]) #输出：8000

# 打印预测价格
print(&quot;预测价格：% d\n&quot;% y_pred)

为什么输出是8000？
我的意思是，我们如何知道这些价格预测函数背后运行的是什么？
它背后的数学公式是什么，我想手动完成，而不使用任何内置函数。
我尝试研究其背后的算法，但找不到令我满意的答案。
我希望得到满意的答复。]]></description>
      <guid>https://stackoverflow.com/questions/78510567/how-does-the-decisiontreeregressor-algorithm-work</guid>
      <pubDate>Tue, 21 May 2024 08:34:32 GMT</pubDate>
    </item>
    <item>
      <title>无法将自参数添加到 catboost 中的自定义指标。无法优化方法“evaluate”，因为使用了 self 参数</title>
      <link>https://stackoverflow.com/questions/78510490/cannot-add-self-arguments-to-custom-metrics-in-catboost-cant-optimze-method-e</link>
      <description><![CDATA[我使用此处的示例。
该示例工作正常，但是，当我尝试通过在其中使用 self 来使自定义指标类变得更加灵活时，我遇到了 UserWarning: Can&#39;t optimze method &quot;evaluate&quot;因为使用了 self 参数
复制问题的代码（如果您希望问题消失，请注释掉 LoglossMetric 的评估方法中的 self.foo = 5 行）
from catboost import CatBoostClassifier、CatBoostRegressor、MultiTargetCustomMetric、MultiTargetCustomObjective
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification、make_regression、make_multilabel_classification
从 sklearn.model_selection 导入 train_test_split
LoglossMetric 类（对象）：
    def get_final_error(自身, 错误, 权重):
        返回误差/(重量+1e-38)

    def is_max_optimal(自身):
        返回错误

    def评估（自我，近似值，目标，重量）：
        自我.foo = 5
        断言 len(大约) == 1
        断言 len(目标) == len(大约[0])

        大约 = 大约[0]

        错误总和 = 0.0
        权重总和 = 0.0

        对于范围内的 i（len（大约））：
            e = np.exp(大约[i])
            p = e / (1 + e)
            w = 1.0 如果权重为 None else 权重[i]
            权重总和 += w
            error_sum += -w * (目标[i] * np.log(p) + (1 - 目标[i]) * np.log(1 - p))

        返回error_sum、weight_sum
X, y = make_classification(n_classes=2, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
model2 = CatBoostClassifier(迭代=10, loss_function=“Logloss”, eval_metric=LoglossMetric(),
                            Learning_rate=0.03，bootstrap_type=&#39;贝叶斯&#39;，boost_from_average=False，
                            leaf_estimation_iterations=1, leaf_estimation_method=&#39;梯度&#39;)
model2.fit(X_train, y_train, eval_set=(X_test, y_test))

如果我在评估方法中使用 self ，也会发生同样的事情

catboost 未能完成哪些优化？原因是什么？
]]></description>
      <guid>https://stackoverflow.com/questions/78510490/cannot-add-self-arguments-to-custom-metrics-in-catboost-cant-optimze-method-e</guid>
      <pubDate>Tue, 21 May 2024 08:17:53 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow PPO 模型未给出模型输出</title>
      <link>https://stackoverflow.com/questions/78510077/tensorflow-ppo-model-not-giving-model-output</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78510077/tensorflow-ppo-model-not-giving-model-output</guid>
      <pubDate>Tue, 21 May 2024 06:49:17 GMT</pubDate>
    </item>
    <item>
      <title>如何修复“ValueError：模型没有从输入中返回损失”？</title>
      <link>https://stackoverflow.com/questions/78510000/how-do-i-fix-valueerror-the-model-did-not-return-a-loss-from-the-inputs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78510000/how-do-i-fix-valueerror-the-model-did-not-return-a-loss-from-the-inputs</guid>
      <pubDate>Tue, 21 May 2024 06:32:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 预测手机销量 [关闭]</title>
      <link>https://stackoverflow.com/questions/78509245/predicting-phone-sales-with-python</link>
      <description><![CDATA[我正在尝试预测手机的销量，有几个因素。

发布日的销售额可能达到数月至 6 个月的销售额

季节性起着巨大的作用，我有大约 5 年的数据

有时销售会因为手机缺货而停止（但除此之外还有需求）

特定模型的寿命通常较短，因此数据不多，主要使用 2-6 个月的数据


我正在尝试找到一个包含所有 4 点的模型。
你有什么建议？]]></description>
      <guid>https://stackoverflow.com/questions/78509245/predicting-phone-sales-with-python</guid>
      <pubDate>Tue, 21 May 2024 00:45:07 GMT</pubDate>
    </item>
    <item>
      <title>我用自己的数据集训练yolo模型，但是没有测试结果</title>
      <link>https://stackoverflow.com/questions/78497575/i-train-yolo-model-with-my-own-data-set-but-there-is-no-test-result</link>
      <description><![CDATA[我正在使用 Yolov3 模型以及从 Kaggle 收到的数据集来训练模型。模型训练已完成，我将新权重添加到备份文件夹中。我运行了我训练过的一种水果进行测试，但没有发生对象检测。同一图像显示为 Prediction.jpg。训练看起来不错，但我不明白为什么它不能检测物体。请帮助我。
火车站代码：
./darknet探测器列车 /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/Desktop/Projects/Bitirmeprojesi/yolov3.weights

测试终端代码：
./darknet探测器测试 /Users/melisabagcivan/darknet/data/obj.data /Users/melisabagcivan/darknet/cfg/yolov3.cfg /Users/melisabagcivan/darknet/backup/yolov3_final.weights -thresh 0.25 -out预测.jpg

我设置并编辑了 obj.data、obj.names 和 yolov3.cfg 文件。
我有 3 个类别：苹果、香蕉和橙子。我已经根据3个类在cfg文件中正确设置了filter和class值等值。
cfg 文件
[网]
# 测试
批次=64
细分=1
＃ 训练
细分=16
宽度= 608
高度=608
频道=3
动量=0.9
衰减=0.0005
角度=0
饱和度=1.5
曝光=1.5
色调=0.3

学习率=0.001
烧入=1000
max_batches = 6000 # 类数 * 2000
政策=步骤
步骤=3600,4800 # max_batches num %80, %90
尺度=.1,.1

数据集中除了.jpg图片外，还有yolo格式的同名.txt文件。
文件图片：
&lt;img alt=&quot;文件图像&quot; src=&quot;https://i.sstatic.net/Ed1BBcZP.png ” /&gt;
包含所有图像路径的 train.txt 和 test.txt 文件也已准备就绪。
当我在终端中运行测试命令时，它可以工作，但图片看起来相同，没有检测对象的边界框。我确定我已经安装了 Opencv。我正在使用 macOS。为什么它没有检测到它？请有人帮忙。我多次通过 make clean 清理暗网，并通过 make opencv = 1 运行它，但结果没有改变。
[yolo]参数：iou损失：mse（2），iou_norm：0.75，obj_norm：1.00，cls_norm：1.00，delta_norm：1.00，scale_x_y：1.00
总 BFLOPS 137.613
平均输出 = 1052318
正在从 /Users/melisabagcivan/darknet/backup/yolov3_final.weights 加载权重...
 看过 64 个，训练过：32013 个 K 图像（500 Kilo-batches_64）
完毕！从权重文件加载 107 层
输入图像路径：/Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg
 检测层：82-类型=28
 检测层数：94-型=28
 检测层：106-类型=28
/Users/melisabagcivan/Desktop/Projects/yoloOD/dataset/test/38_Orange.jpg：预测为 6738.129000 毫秒。

我尝试了很多图像，但它没有在任何图像中绘制方框。我不明白是它无法检测到还是我在测试时犯了错误。]]></description>
      <guid>https://stackoverflow.com/questions/78497575/i-train-yolo-model-with-my-own-data-set-but-there-is-no-test-result</guid>
      <pubDate>Fri, 17 May 2024 19:21:32 GMT</pubDate>
    </item>
    <item>
      <title>loss.backward() 与模型的适当参数有何关系？</title>
      <link>https://stackoverflow.com/questions/58844168/how-does-loss-backward-relate-to-the-appropriate-parameters-of-the-model</link>
      <description><![CDATA[我是 PyTorch 的新手，我无法理解 loss 如何知道通过 loss.backward() 计算梯度？ 
当然，我知道参数需要有 requires_grad=True 并且我知道它将 x.grad 设置为适当的梯度，仅供优化器稍后执行梯度更新。 
优化器在实例化时链接到模型参数，但损失永远不会链接到模型。 
我一直在经历这个帖子，但我认为没有人清楚地回答它，并且发起该帖子的人似乎与我有同样的问题。 
当我有两个不同的网络、两个不同的损失函数和两个不同的优化器时会发生什么。我可以轻松地将优化器链接到每个网络，但是如果我从不将它们链接在一起，损失函数如何知道如何计算每个适当网络的梯度？]]></description>
      <guid>https://stackoverflow.com/questions/58844168/how-does-loss-backward-relate-to-the-appropriate-parameters-of-the-model</guid>
      <pubDate>Wed, 13 Nov 2019 19:23:44 GMT</pubDate>
    </item>
    <item>
      <title>k-mean python 图像分离</title>
      <link>https://stackoverflow.com/questions/51245877/image-segregation-by-k-mean-python</link>
      <description><![CDATA[我是机器学习的新手，我正在学习用于图像分离的 k 均值，但我无法理解它的代码：
from matplotlib.image import imread
image = imread(os.path.join(&quot;images&quot;,&quot;unsupervised_learning&quot;,&quot;ladybug.png&quot;))
图像.形状
X = 图像.reshape(-1, 3)
kmeans = KMeans(n_clusters=8, random_state=42).fit(X)
segmented_img = kmeans.cluster_centers_[kmeans.labels_]
Segmented_img = Segmented_img.reshape(image.shape)
分段的imgs = []
n_颜色 = (10, 8, 6, 4, 2)
对于 n_colors 中的 n_clusters：
    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X)
    segmented_img = kmeans.cluster_centers_[kmeans.labels_]
    segmented_imgs.append(segmented_img.reshape(image.shape))
plt.figure(figsize=(10,5))
plt.subplots_调整（wspace = 0.05，hspace = 0.1）
plt.子图(231)
plt.imshow(图像)
plt.title(&quot;原图&quot;)
plt.axis(&#39;关闭&#39;)
对于 idx，枚举中的 n_clusters(n_colors)：
   plt.子图（232 + idx）
   plt.imshow(segmented_imgs[idx])
   plt.title(&quot;{} 颜色&quot;.format(n_clusters))
   plt.axis(&#39;关闭&#39;)
plt.show()

使用的图像：

输出图

特别是，这段代码的含义
segmented_img = kmeans.cluster_centers_[kmeans.labels_]
]]></description>
      <guid>https://stackoverflow.com/questions/51245877/image-segregation-by-k-mean-python</guid>
      <pubDate>Mon, 09 Jul 2018 12:42:30 GMT</pubDate>
    </item>
    </channel>
</rss>