<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 31 Jan 2025 15:16:03 GMT</lastBuildDate>
    <item>
      <title>从 pix2pix 实现矢量化生成的线条</title>
      <link>https://stackoverflow.com/questions/79402632/vectorize-generated-lines-from-a-pix2pix-implementation</link>
      <description><![CDATA[我有以下使用 pix2pix 模型生成的图像。我希望生成的线条是直的，并且具有相同的高度（如果是水平的）和相同的宽度（如果是垂直的）。
基本上这些线是墙壁，稍后应该被渲染为房子里的墙壁。
有没有任何 python /ml 逻辑可以实现这一点？
]]></description>
      <guid>https://stackoverflow.com/questions/79402632/vectorize-generated-lines-from-a-pix2pix-implementation</guid>
      <pubDate>Fri, 31 Jan 2025 11:47:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyTorch 微调 LLM 时出现运行时错误：“张量的元素 0 不需要梯度”</title>
      <link>https://stackoverflow.com/questions/79402407/runtimeerror-with-pytorch-when-fine-tuning-llm-element-0-of-tensors-does-not-r</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79402407/runtimeerror-with-pytorch-when-fine-tuning-llm-element-0-of-tensors-does-not-r</guid>
      <pubDate>Fri, 31 Jan 2025 10:11:53 GMT</pubDate>
    </item>
    <item>
      <title>教授如何识别学习模型的闰年</title>
      <link>https://stackoverflow.com/questions/79401755/teaching-how-to-identify-leap-years-for-a-learning-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79401755/teaching-how-to-identify-leap-years-for-a-learning-model</guid>
      <pubDate>Fri, 31 Jan 2025 04:14:59 GMT</pubDate>
    </item>
    <item>
      <title>从蒙版风力涡轮机图像中提取叶片尖端坐标的想法[关闭]</title>
      <link>https://stackoverflow.com/questions/79400379/ideas-for-extracting-blade-tip-coordinates-from-masked-wind-turbine-image</link>
      <description><![CDATA[我正在寻找 Python 中的图像处理工具来获取风力涡轮机叶片尖端的坐标，在本例中是小型模型。叶片已经由 yoloV8 分割模型分割，现在我想使用该图像获取尖端的 xy 坐标。示例图像：
风能涡轮机的蒙版机翼。
有人可以推荐一些关于如何做到这一点的想法吗？转子可以旋转，因此三个尖端可以位于椭圆上的任何位置。
我已经尝试训练 yolo-pose 模型进行关键点检测，但它没有给出足够精确的结果。我将使用这些坐标来计算转子盘的偏心率，因此这些点需要相当精确。]]></description>
      <guid>https://stackoverflow.com/questions/79400379/ideas-for-extracting-blade-tip-coordinates-from-masked-wind-turbine-image</guid>
      <pubDate>Thu, 30 Jan 2025 15:27:02 GMT</pubDate>
    </item>
    <item>
      <title>模仿单个个体的合成数据[关闭]</title>
      <link>https://stackoverflow.com/questions/79400139/synthetic-data-whic-mimics-a-single-individual</link>
      <description><![CDATA[传统 GAN 生成的合成数据集与原始数据集来自同一分布。是否有任何实例可以使用整个数据集中的单个样本来创建唯一的合成数据样本？]]></description>
      <guid>https://stackoverflow.com/questions/79400139/synthetic-data-whic-mimics-a-single-individual</guid>
      <pubDate>Thu, 30 Jan 2025 13:56:09 GMT</pubDate>
    </item>
    <item>
      <title>Whisper 模型实时端点容器部署在 Azure ML 上失败</title>
      <link>https://stackoverflow.com/questions/79399452/whisper-model-real-time-endpoint-container-deployment-failed-on-azure-ml</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79399452/whisper-model-real-time-endpoint-container-deployment-failed-on-azure-ml</guid>
      <pubDate>Thu, 30 Jan 2025 09:50:51 GMT</pubDate>
    </item>
    <item>
      <title>在使用 YOLO v8 DETECT TRAIN 进行对象检测时，如何在每个时期提取验证集上的预测？</title>
      <link>https://stackoverflow.com/questions/79398928/how-to-extract-the-predictions-on-the-validation-set-at-each-epoch-when-using-y</link>
      <description><![CDATA[我使用的是 yolo 模型 yolov8l.pt 的 CLI 版本（可通过下面的 WEIGHTS 参数访问）：
!yolo detect train model={WEIGHTS} data=&#39;data/tvt3_data_v8.yaml&#39; single_cls imgsz={IMG_SIZE} batch={BATCH_SIZE} epochs={3}

当前工作原理
在训练期间，在每个时期：

计算训练损失
计算验证损失和验证召回率、准确率和 mAP

在训练结束时，选择最佳 .pt 模型并在验证集上进行评估。
我需要从脚本中获得什么
在每个epoch，我想提取在 VALIDATION 数据集上做出的预测
我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/79398928/how-to-extract-the-predictions-on-the-validation-set-at-each-epoch-when-using-y</guid>
      <pubDate>Thu, 30 Jan 2025 06:02:26 GMT</pubDate>
    </item>
    <item>
      <title>dsac_tools（使用 pytorch 计算本质矩阵）计算问题</title>
      <link>https://stackoverflow.com/questions/79398453/dsac-toolscalculate-essential-matrix-using-pytorch-computational-problem</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79398453/dsac-toolscalculate-essential-matrix-using-pytorch-computational-problem</guid>
      <pubDate>Wed, 29 Jan 2025 23:36:30 GMT</pubDate>
    </item>
    <item>
      <title>具有 40,000 个状态和 81 个动作的 Q 学习是否可行？[关闭]</title>
      <link>https://stackoverflow.com/questions/79398056/is-q-learning-feasible-with-40-000-states-and-81-actions</link>
      <description><![CDATA[我正在使用 Q-learning 解决一个问题，其中状态数为 40,000，操作数为 81，因此 Q 表大小为 3,240,000 个条目。使用 Q-learning 处理如此大的数据集是否可行，还是需要额外的技术来优化学习过程？]]></description>
      <guid>https://stackoverflow.com/questions/79398056/is-q-learning-feasible-with-40-000-states-and-81-actions</guid>
      <pubDate>Wed, 29 Jan 2025 20:01:22 GMT</pubDate>
    </item>
    <item>
      <title>在 GPU 上进行 PyWavelets 计算</title>
      <link>https://stackoverflow.com/questions/79396894/doing-pywavelets-calculation-on-gpu</link>
      <description><![CDATA[目前正在使用 PyWavelets 进行分类器工作，这是我的计算块：
class WaveletLayer(nn.Module):
def __init__(self):
super(WaveletLayer, self).__init__()

def forward(self, x):
def wavelet_transform(img):
coeffs = pywt.dwt2(img.cpu().numpy(), &quot;haar&quot;)
LL, (LH, HL, HH) = coeffs
return (
torch.from_numpy(LL).to(img.device),
torch.from_numpy(LH).to(img.device),
torch.from_numpy(HL).to(img.device),
torch.from_numpy(HH).to(img.device),
)

# 将小波变换分别应用于每个通道
LL, LH, HL, HH = zip(
*[wavelet_transform(x[:, i : i + 1]) for i in range(x.shape[1])]
)

# 连接结果
LL = torch.cat(LL, dim=1)
LH = torch.cat(LH, dim=1)
HL = torch.cat(HL, dim=1)
HH = torch.cat(HH, dim=1)

return torch.cat([LL, LH, HL, HH], dim=1)


此模块的输出将进入 resnet 块进行学习，在此过程中，我发现 CPU 堵塞，从而减慢了训练过程
我正在尝试使用 GPU 进行这些计算。]]></description>
      <guid>https://stackoverflow.com/questions/79396894/doing-pywavelets-calculation-on-gpu</guid>
      <pubDate>Wed, 29 Jan 2025 13:26:29 GMT</pubDate>
    </item>
    <item>
      <title>使用输入层作为第二个输入层的权重</title>
      <link>https://stackoverflow.com/questions/79396213/using-an-input-layer-as-a-weight-to-a-second-input-layer</link>
      <description><![CDATA[我有两个输入结构，其中第二个输入中的每个特征都使用输入 1 的值来计算每个特征。然后，第二个输入层连接到隐藏层，最后连接到单个输出层。因此，假设我在第一个输入中有 A、B、C，在第二个输入中有 G M N O，其中例如 G 被计算为总和（A 到 C），并且 G M N O 连接到隐藏层。如何使用 tensorflow keras 实现这一点？ G M N O 是连接到隐藏层的输入层。
input_elements = Input(shape=(4,), name=&quot;Elemental_Composition&quot;)
input_descriptors = Input(shape=(7,), name=&quot;Descriptors&quot;)

combined = concatenate([input_elements, input_descriptors])
hidden = Dense(64,activation=&quot;relu&quot;)(combined)
output = Dense(1,activation=&quot;linear&quot;)(hidden)
]]></description>
      <guid>https://stackoverflow.com/questions/79396213/using-an-input-layer-as-a-weight-to-a-second-input-layer</guid>
      <pubDate>Wed, 29 Jan 2025 09:17:34 GMT</pubDate>
    </item>
    <item>
      <title>自定义字母表的文本识别</title>
      <link>https://stackoverflow.com/questions/79394460/text-recogniton-for-custom-alphabet</link>
      <description><![CDATA[我有一个虚构的字母表，由大约 20 种形状和字母组成，它们与希腊字母和西里尔字母相似。
它们是作为资产生成的，我为它们每个都制作了 30 x 30 的图像。我想创建一个特殊的图像处理工具来实时翻译它们。
它们总是打印在黑色多边形上。
所以我尝试使用通用 opencv2 方法使用黑色多边形进行检测，并且成功了。
为了扫描和检测字母，我尝试了 ORB 特征提取和匹配，但没有成功。由于相似的符文形状，它在整个字母中都发现了特征。
我曾尝试使用 Yolo11 训练对象检测模型，但由于数据量少（我没有已打印的示例，我尝试生成具有不同角度的模拟图像），它成本高且表现不佳。
我没有足够的数据来训练 HOG。
是否有一个简单的 Python 模式匹配算法，可以考虑现实生活中相机的小倾斜平移和滚动，因此仍然可以检测字母？
编辑 -&gt;
以下是我拥有的一些字母示例

]]></description>
      <guid>https://stackoverflow.com/questions/79394460/text-recogniton-for-custom-alphabet</guid>
      <pubDate>Tue, 28 Jan 2025 16:02:10 GMT</pubDate>
    </item>
    <item>
      <title>EfficientNetB3模型识别脑肿瘤准确率极低及学习停滞问题</title>
      <link>https://stackoverflow.com/questions/79390644/very-low-accuracy-of-efficientnetb3-model-and-learning-plateau-on-identifying-br</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79390644/very-low-accuracy-of-efficientnetb3-model-and-learning-plateau-on-identifying-br</guid>
      <pubDate>Mon, 27 Jan 2025 11:58:17 GMT</pubDate>
    </item>
    <item>
      <title>ML-Agents 代理无法在 Unity 中完成简单的“射弹到目标”任务</title>
      <link>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</link>
      <description><![CDATA[代理在重力作用下向目标发射弹丸。代理只有一个动作 - 射击角度。发射力是恒定的。我还没有改变目标的位置。因此这应该是微不足道的，因为模型只需要学习正确的射击角度。但经过 300000 个训练步骤后，模型仍然射击不稳定。
代理：
使用 Unity.MLAgents;
使用 Unity.MLAgents.Actuators;
使用 Unity.MLAgents.Sensors;
使用 UnityEngine;

公共类 ProjectileAgent：代理
{
公共 Transform 目标; //带有 2D 碰撞器和“目标”标签的固定目标
公共 Transform launchPoint; //生成弹丸的位置
公共 GameObject projectilePrefab; //带有 Rigidbody2D 和 ProjectileCollision 脚本的预制件
公共 float fixedForce = 500f; // 对射弹施加恒定的力

private bool hasLaunched = false;

public override void OnEpisodeBegin()
{
hasLaunched = false;
RequestDecision(); // 在每个情节开始时请求一个决定
}

public override void CollectObservations(VectorSensor sensor)
{
// 观察从发射点到目标的相对位置 (x,y)
Vector2 diff = target.position - launchPoint.position;
sensor.AddObservation(diff.x);
sensor.AddObservation(diff.y);
}

public override void OnActionReceived(ActionBuffers action)
{
if (!hasLaunched)
{
// 一个连续动作 (0..1) 映射到 [0..180] 度
float angle01 = Mathf.Clamp01(actions.ContinuousActions[0]);
float angleDegrees = Mathf.Lerp(0f, 180f, angle01);

LaunchProjectile(angleDegrees);
hasLaunched = true;
}
}

private void LaunchProjectile(float angleDegrees)
{
GameObject projObj = Instantiate(projectilePrefab, launchPoint.position, Quaternion.identity);
ProjectileCollision projScript = projObj.GetComponent&lt;ProjectileCollision&gt;();
projScript.agent = this;

Rigidbody2D rb = projObj.GetComponent&lt;Rigidbody2D&gt;();
float rad = angleDegrees * Mathf.Deg2Rad;
Vector2 direction = new Vector2(Mathf.Cos(rad), Mathf.Sin(rad));
rb.AddForce(direction * fixedForce);
}

// 射弹击中目标时调用
public void OnHitTarget()
{
AddReward(1.0f);
EndEpisode();
}

// 射弹未击中目标时调用
public void OnMiss(Vector2 projectilePosition)
{
float distance = Vector2.Distance(projectilePosition, target.position);
float maxDistance = 10f; // 根据需要调整
float vicinity = 1f - (distance / maxDistance);
vicinity = Mathf.Clamp01(proximity);

// 接近目标时获得部分奖励
AddReward(proximity * 0.5f);

// 未击中时获得小额惩罚
AddReward(-0.1f);
EndEpisode();
}

// Unity 编辑器中测试的启发式方法（随机角度）
public override void Heuristic(in ActionBuffers actionOut)
{
actionOut.ContinuousActions[0] = Random.value;
}
}

Projectile:
using UnityEngine;

public class ProjectileCollision : MonoBehaviour
{
public ProjectileAgent agent;

private void Start()
{
// 短暂时间后销毁，以便我们可以记录未击中
Destroy(gameObject, lifetime);
}

private void OnCollisionEnter2D(Collision2D collision)
{
if (collision.gameObject.CompareTag(&quot;Target&quot;))
{
agent.OnHitTarget();
}
else
{
agent.OnMiss(transform.position);
}
Destroy(gameObject);
}
}


我尝试过的方法

奖励塑造：
击中目标可获得 +1 奖励，近距离击中可获得部分基于距离的奖励，未击中可获得少量负奖励。
我将击中奖励提高到 +3，降低了未击中惩罚，等等。
训练步骤：
我使用 PPO 运行了 300k+ 步。
碰撞检查：
日志确认 OnHitTarget() 和 OnMiss() 在预期时间触发。
固定力和重力：
通过硬编码角度，验证箭可以手动到达目标。
重力已设置，因此物理上可以击中。
无随机目标：
目标目前固定在一个位置以保持简单。
]]></description>
      <guid>https://stackoverflow.com/questions/79389655/ml-agents-agent-not-converging-for-simple-projectile-to-target-task-in-unity</guid>
      <pubDate>Mon, 27 Jan 2025 02:47:35 GMT</pubDate>
    </item>
    <item>
      <title>宏观 VS 微观 VS 加权 VS 样本 F1 分数</title>
      <link>https://stackoverflow.com/questions/55740220/macro-vs-micro-vs-weighted-vs-samples-f1-score</link>
      <description><![CDATA[在 sklearn.metrics.f1_score 中，f1 分数有一个名为“平均值”的参数。宏、微、加权和样本是什么意思？请详细说明，因为在文档中，没有正确解释。或者简单地回答以下问题：

为什么“样本”是多标签分类的最佳参数？
为什么微最适合不平衡的数据集？
加权和宏有什么区别？
]]></description>
      <guid>https://stackoverflow.com/questions/55740220/macro-vs-micro-vs-weighted-vs-samples-f1-score</guid>
      <pubDate>Thu, 18 Apr 2019 06:26:25 GMT</pubDate>
    </item>
    </channel>
</rss>