<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 23 Apr 2024 06:20:52 GMT</lastBuildDate>
    <item>
      <title>学生分数 所需数据申请/建议</title>
      <link>https://stackoverflow.com/questions/78370196/student-marks-data-application-suggestions-required</link>
      <description><![CDATA[我有学生分数和人口统计数据，我想在实用程序应用程序中使用它。到目前为止，我已经提出了“标记预测”应用程序，它使用线性回归模型来预测标记。我想制作一个聊天机器人实用程序，学生可以在其中进行交互，提供他们的分数历史/人口统计数据以预测未来课程的分数等。
这些是我的一些想法，我愿意接受有关此类数据的更多想法/应用的建议。还请推荐聊天机器人的工具。我可以在这里使用预制的法学硕士来进行fintune吗？还有其他工具吗？GPT？我的数据在CSV文件中以text和int的形式存在。
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78370196/student-marks-data-application-suggestions-required</guid>
      <pubDate>Tue, 23 Apr 2024 06:15:17 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python 制作人工智能驱动的自学习聊天机器人</title>
      <link>https://stackoverflow.com/questions/78370101/how-to-make-ai-powered-self-learning-chatbot-in-python</link>
      <description><![CDATA[我一直在尝试用 Python 制作一个自学习聊天机器人，并尝试了不同的库，如 NLTK、TensorFlow、ChatBot 和 PyTorch，但所有这些库都在处理预定义的训练数据。我找不到任何选项来根据给定的输入自行训练模型并尝试不同类型的数据集。
在Python中有什么方法可以实现这一点吗？
我想获得一个基于我的数据的自学习人工智能模型。]]></description>
      <guid>https://stackoverflow.com/questions/78370101/how-to-make-ai-powered-self-learning-chatbot-in-python</guid>
      <pubDate>Tue, 23 Apr 2024 05:44:52 GMT</pubDate>
    </item>
    <item>
      <title>张量流联合错误模块“tensorflow_federated.python.learning”没有属性“build_federated_averaging_process”</title>
      <link>https://stackoverflow.com/questions/78370048/tensorflow-federated-error-module-tensorflow-federated-python-learning-has-no</link>
      <description><![CDATA[我有一个机器学习代码
# 导入所有需要的库

将 pandas 导入为 pd
将 numpy 导入为 np
从 sklearn.datasets 导入 load_iris
从 sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 StandardScaler

将张量流导入为 tf
将tensorflow_federated导入为tff

虹膜 = load_iris()
df = pd.DataFrame(iris.data,列=iris.feature_names)
df[&#39;物种&#39;]=iris.target

# 将数据帧拆分为输入特征和目标变量

x = df.drop(&#39;物种&#39;,axis=1)
y = df[&#39;物种&#39;]
# 创建客户端数据集的函数（假设数据已预先分区）
def create_tf_dataset(client_data):
  “”“”根据提供的客户端数据（特征、标签）创建 tf.data.Dataset。“”“”
  特征，标签= client_data
  返回 tf.data.Dataset.from_tensor_slices((特征，标签))

# 将数据拆分为clientdatasets（模拟数据分区）
客户端数据集 = []
客户数量 = 5
对于范围内的 i（num_clients）：
  start_index = int(i * (len(x) / num_clients))
  end_index = int((i + 1) * (len(x) / num_clients))
  client_features = x[开始索引:结束索引]
  client_labels = y[开始索引:结束索引]
  client_datasets.append(create_tf_dataset((client_features, client_labels)))
  # 定义模型架构（替换为您想要的模型复杂度）
def model_fn(输入):
   features, _ = input # 我们只使用特征进行分类
   稠密1 = tf.keras.layers.Dense(10, 激活=&#39;relu&#39;)(特征)
   稠密2 = tf.keras.layers.Dense(3,activation=&#39;softmax&#39;)(dense1) # 3个鸢尾花类的3个单元
   返回 tf.keras.Model(输入=特征，输出=dense2)

# 定义客户端优化器
client_optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)

# 定义服务器优化器（用于服务器端聚合）
server_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
fed_learning_model = tff.learning.build_federated_averaging_process(
     模型_fn，
     client_optimizer_fn=client_optimizer,
     server_optimizer_fn=服务器优化器）

但我一直收到此错误。
&lt;小时/&gt;
AttributeError Traceback（最近一次调用最后一次）
 在&lt;细胞系：1&gt;()
----&gt; 1 fed_learning_model = tff.learning.build_federated_averaging_process(
2 模型_fn，
3 client_optimizer_fn=client_optimizer,
4 server_optimizer_fn=server_optimizer)
属性错误：模块“tensorflow_federated.python.learning”没有属性“build_federated_averaging_process”
我不知道我的版本是否适合。我的tensorflow联合版本是0.76.0
我的tensorflow版本是2.14.1，python版本是3.10.12
当我在互联网上搜索时，我发现此代码不支持 tensorflow 版本 0.21.0 及以上版本。但我不知道在最新版本中使用什么]]></description>
      <guid>https://stackoverflow.com/questions/78370048/tensorflow-federated-error-module-tensorflow-federated-python-learning-has-no</guid>
      <pubDate>Tue, 23 Apr 2024 05:30:55 GMT</pubDate>
    </item>
    <item>
      <title>如何修改 Adam 优化器以在计算中不包含零？</title>
      <link>https://stackoverflow.com/questions/78369654/how-to-modify-the-adam-optimizer-to-not-include-zeros-in-the-calculations</link>
      <description><![CDATA[我在这个SO问题中找到了Adam的实现：
类 ADAMOptimizer(torch.optim.Optimizer):
    ”“”
    作为前面的步骤，实现 ADAM 算法。
    ”“”
    def __init__(自身，参数，lr=1e-3，betas=(0.9，0.999)，eps=1e-8，weight_decay=0)：
        默认值 = dict(lr=lr, betas=betas, eps=eps,weight_decay=weight_decay)
        super(ADAMOptimizer, self).__init__(参数, 默认值)

    def 步骤（自身）：
        ”“”
        执行单个优化步骤。
        ”“”
        损失=无
        对于 self.param_groups 中的组：

            对于组 [&#39;params&#39;] 中的 p：
                grad = p.grad.data
                状态 = self.state[p]

                # 状态初始化
                如果 len(状态) == 0:
                    状态[&#39;步骤&#39;] = 0
                    # 动量（梯度的指数 MA）
                    状态[&#39;exp_avg&#39;] = torch.zeros_like(p.data)

                    # RMS Prop 组件。 （平方梯度的指数 MA）。分母。
                    状态[&#39;exp_avg_sq&#39;] = torch.zeros_like(p.data)

                exp_avg, exp_avg_sq = 状态[&#39;exp_avg&#39;], 状态[&#39;exp_avg_sq&#39;]

                b1, b2 = 组[&#39;betas&#39;]
                状态[&#39;步骤&#39;] += 1

                # 添加权重衰减（如果有）
                如果组[&#39;weight_decay&#39;] != 0:
                    grad = grad.add(group[&#39;weight_decay&#39;], p.data)

                ＃ 势头
                exp_avg = torch.mul(exp_avg, b1) + (1 - b1)*grad
                
                # 有效值
                exp_avg_sq = torch.mul(exp_avg_sq, b2) + (1-b2)*(grad*grad)

                mhat = exp_avg / (1 - b1 ** 状态[&#39;步骤&#39;])
                vhat = exp_avg_sq / (1 - b2 ** 状态[&#39;步骤&#39;])
                
                denom = torch.sqrt( vhat + group[&#39;eps&#39;] )

                p.data = p.data - group[&#39;lr&#39;] * mhat / denom
                
                # 保存状态
                状态[&#39;exp_avg&#39;], 状态[&#39;exp_avg_sq&#39;] = exp_avg, exp_avg_sq

        回波损耗

我的问题是我的很多梯度都有 0 值，这会扰乱动量和速度项。我感兴趣的是修改代码，以便在计算动量和速度项（即第一和第二矩估计）时不考虑 0 值。
不过，我不确定该怎么做。如果它是一个简单的网络，其中梯度只是简单的维度，我可以检查是否 p.grad.data=0，但由于这将是一个多维张量，我不确定如何删除计算中的零并且不会弄乱其他东西（例如，剩余的更新）。]]></description>
      <guid>https://stackoverflow.com/questions/78369654/how-to-modify-the-adam-optimizer-to-not-include-zeros-in-the-calculations</guid>
      <pubDate>Tue, 23 Apr 2024 02:41:02 GMT</pubDate>
    </item>
    <item>
      <title>Python 中用于二元分类机器学习模型的 LazyClassifier 中的 ValueError？</title>
      <link>https://stackoverflow.com/questions/78368809/valueerror-in-lazyclassifier-in-python-for-binary-classification-machine-learnin</link>
      <description><![CDATA[我尝试使用 LazyClassifier 构建具有二进制目标变量的机器学习模型。

X_train 有 114074 个观测值和 15 列
X_test 有 48890 个观测值
y_train 有 114074 个观测值和 15 列
y_test 有 48890 个观测值

我创建了如下所示的 custom_metic 函数：
def custom_metric(y_train_true, y_train_pred, y_test_true, y_test_pred):
    准确度训练 = 准确度分数(y_train_true, y_train_pred)
    准确度测试 = 准确度分数(y_test_true, y_test_pred)
    
    f1_beta1_train = f1_score(y_train_true, y_train_pred, beta=1)
    f1_beta1_test = f1_score(y_test_true, y_test_pred, beta=1)
    
    f1_beta2_train = f1_score(y_train_true, y_train_pred, beta=2)
    f1_beta2_test = f1_score(y_test_true, y_test_pred, beta=2)
    
    auc_train = roc_auc_score(y_train_true, y_train_pred)
    auc_test = roc_auc_score(y_test_true, y_test_pred)
    
    precision_train = precision_score(y_train_true, y_train_pred)
    precision_test = precision_score(y_test_true, y_test_pred)
    
    召回训练 = 召回分数(y_train_true, y_train_pred)
    召回测试 = 召回分数(y_test_true, y_test_pred)
    
    返回 {
        &#39;accuracy_train&#39;：accuracy_train，&#39;accuracy_test&#39;：accuracy_test，
        &#39;f1_beta1_train&#39;：f1_beta1_train，&#39;f1_beta1_test&#39;：f1_beta1_test，
        &#39;f1_beta2_train&#39;：f1_beta2_train，&#39;f1_beta2_test&#39;：f1_beta2_test，
        &#39;auc_train&#39;：auc_train，&#39;auc_test&#39;：auc_test，
        &#39;精度训练&#39;：精度训练，&#39;精度测试&#39;：精度测试，
        &#39;recall_train&#39;：recall_train，&#39;recall_test&#39;：recall_test
    }

然后我尝试使用 LazyClassifier：
cls = LazyClassifier(ignore_warnings=True, custom_metric=custom_metric)
模型，预测 = cls.fit(X_train, X_test, y_train, y_test)

尽管如此，我收到了如下错误：
ValueError：所有数组的长度必须相同


我该如何避免该错误？]]></description>
      <guid>https://stackoverflow.com/questions/78368809/valueerror-in-lazyclassifier-in-python-for-binary-classification-machine-learnin</guid>
      <pubDate>Mon, 22 Apr 2024 20:49:06 GMT</pubDate>
    </item>
    <item>
      <title>在使用 LR 查找器代码后，如何获得使用循环学习率方法找到的最佳 LR 的学习率（LR）界限（最小值和最大值）？</title>
      <link>https://stackoverflow.com/questions/78368740/how-to-get-learning-rate-lr-bounds-min-and-max-values-wrt-optimal-lr-found-t</link>
      <description><![CDATA[我使用以下代码来获取给定神经网络模型的最佳学习率 - https://github.com/beringresearch/lrfinder/blob/master/lrfinder/lrfinder.py - 最终通过 get_best_lr 函数。因此，在获得最佳学习率的值后，如何以编程方式找出使用循环学习率 (CLR) 方法找到的最佳 LR 的 LR 边界（最小值和最大值）值 (https://arxiv.org/abs/1506.01186)?
来自引用的 GitHub 存储库的代码：
导入数学

将 matplotlib.pyplot 导入为 plt
导入tensorflow.keras.backend为K
将 numpy 导入为 np

从tensorflow.keras.callbacks导入LambdaCallback


LRFinder 类：
    ”“”
    训练的循环学习率中详细介绍了学习率范围测试
    神经网络 作者：Leslie N. Smith。学习率范围测试是一个测试
    它提供了有关最佳学习率的有价值的信息。期间
    预训练运行时，学习率线性增加或
    两个边界之间呈指数关系。较低的初始学习率允许
    网络开始收敛，并且随着学习率的增加
    最终会太大并且网络会发散。
    ”“”

    def __init__(自我，模型)：
        self.model = 模型
        自我损失= []
        自我学习率 = []
        self.best_loss = 1e9

    def on_batch_end（自身，批次，日志）：
        lr = K.get_value(self.model.optimizer.lr)
        self.learning_rates.append（lr）

        损失=日志[&#39;损失&#39;]
        self.losses.append(损失)

        如果批次&gt; 5 且 (math.isnan(loss) 或 loss &gt; self.best_loss * 4)：
            self.model.stop_training = True
            返回

        如果损失&lt; self.best_loss：
            self.best_loss = 损失

        lr *= self.lr_mult
        K.set_value(self.model.optimizer.lr, lr)

    def find(自我, 数据集, start_lr, end_lr, epochs=1,
             steps_per_epoch=无，**kw_fit）：
        如果steps_per_epoch为None：
            引发异常（&#39;正确训练数据生成器，&#39;
                            “steps_per_epoch”不能为“None”。”
                            &#39;你可以将其计算为&#39;
                            &#39;`np.ceil(len(TRAINING_LIST) / BATCH)`&#39;)

        self.lr_mult = (浮点(end_lr) /
                        浮动（start_lr））**（浮动（1）/
                                             浮点数（纪元*steps_per_epoch））
        初始权重 = self.model.get_weights()

        Original_lr = K.get_value(self.model.optimizer.lr)
        K.set_value(self.model.optimizer.lr, start_lr)

        回调 = LambdaCallback(on_batch_end=lambda 批次,
                                  日志：self.on_batch_end（批次，日志））

        self.model.fit（数据集，
                       纪元=纪元，回调=[回调]，**kw_fit）
        self.model.set_weights(initial_weights)

        K.set_value(self.model.optimizer.lr,original_lr)

    def get_learning_rates(自我):
        返回（自我学习率）

    def get_losses(自身):
        返回（自我损失）

    def get_derivatives(self, sma):
        断言 sma &gt;= 1
        导数 = [0] * sma
        对于范围内的 i(sma, len(self.learning_rates))：
            衍生品.append((self.losses[i] - self.losses[i - sma]) / sma)
        回报衍生品

    def get_best_lr（自身，sma，n_skip_beginning = 10，n_skip_end = 5）：
        衍生品 = self.get_derivatives(sma)
        best_der_idx = np.argmin(导数[n_skip_beginning:-n_skip_end])
        返回 self.learning_rates[n_skip_beginning:-n_skip_end][best_der_idx]
]]></description>
      <guid>https://stackoverflow.com/questions/78368740/how-to-get-learning-rate-lr-bounds-min-and-max-values-wrt-optimal-lr-found-t</guid>
      <pubDate>Mon, 22 Apr 2024 20:33:17 GMT</pubDate>
    </item>
    <item>
      <title>语码转换/混合的情感分析[关闭]</title>
      <link>https://stackoverflow.com/questions/78368584/sentiment-analysis-of-code-switching-mixing</link>
      <description><![CDATA[Stack Overflow 社区您好，
我目前正在从事一个涉及代码切换/代码混合的情感分析的项目。我的目标是在检测和解释混合语言文本中的情感方面实现高精度。然而，我在寻找适合此任务的强大数据集方面面临着挑战。
有人对我可以用来训练代码转换或混合语言文本的情感分析的数据集和 ML/DL 模型有建议吗？此外，如果您有任何可以帮助提高此类分析准确性的提示或技术，我将非常感谢您的见解。
预先感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78368584/sentiment-analysis-of-code-switching-mixing</guid>
      <pubDate>Mon, 22 Apr 2024 19:50:03 GMT</pubDate>
    </item>
    <item>
      <title>即使添加更多层，训练损失也不会减少</title>
      <link>https://stackoverflow.com/questions/78368391/training-loss-does-not-decrease-even-when-adding-more-layers</link>
      <description><![CDATA[我正在使用 Keras 的顺序模型来训练包含约 10000 个数据点和 16 个特征的小型数据集。目标在 0 到 1 之间。理论上，当我们通过添加更多层来增加模型复杂性时，训练损失应该会减少，并且可能会过度拟合。就我而言，我使用均方误差，训练误差饱和在 0.14 左右（远未达到过拟合），这比 XGBoost、RandomForest 等误差接近 0 的其他算法要差得多。
如果您能给出一些可能的原因，我将不胜感激。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78368391/training-loss-does-not-decrease-even-when-adding-more-layers</guid>
      <pubDate>Mon, 22 Apr 2024 19:04:25 GMT</pubDate>
    </item>
    <item>
      <title>UnicodeEncodeError：“charmap”编解码器无法对位置 19-38 中的字符进行编码：字符映射到 <未定义></title>
      <link>https://stackoverflow.com/questions/78367946/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-19-38-c</link>
      <description><![CDATA[我正在开发一个基于 Flask 的 Web 应用程序，用户可以上传图像以使用机器学习模型进行预测。上传的图像存储在本地目录中，并使用预先训练的模型进行预测。然而，当我点击预测按钮时
是什么导致了这个 UnicodeEncodeError？
如何解决此问题以确保我的应用程序能够正确处理图像上传和预测？
是否有在 Flask 环境中处理字符编码的最佳实践，尤其是在 Windows 上？
==app.py====
@app.route(&#39;/uploadimage&#39;,methods=[&#39;GET&#39;, &#39;POST&#39;])
def upload_image():
    如果不是 session.get(&#39;logged_in&#39;):
        flash(&quot;您需要登录才能上传图片。&quot;, &quot;错误&quot;)
        返回重定向（url_for（&#39;登录&#39;））
    
    如果 request.method == &#39;POST&#39;:
        # 从表单中获取文件
        如果“my_image”不在 request.files 中：
            flash(“请求中没有文件部分。”,“错误”)
            返回重定向(request.url)

        文件 = request.files[&#39;my_image&#39;]

        # 检查文件是否上传
        if file.filename == &#39;&#39;:
            flash(“没有选择文件。”,“错误”)
            返回重定向(request.url)

        # 验证文件类型（假设图像文件是图像）
        如果不是 file.filename.lower().endswith((&#39;.png&#39;, &#39;.jpg&#39;, &#39;.jpeg&#39;)):
            flash(“仅允许 PNG、JPG 或 JPEG 文件。”, “错误”)
            返回重定向(request.url)

        # 将文件保存到临时位置
        img_path = os.path.join(&#39;images&#39;, file.filename) # 您可能需要创建 &#39;temp&#39; 目录
        文件.保存（img_path）

        # 获取预测结果
        预测标签 = 预测标签(img_path)

        # 返回预测的标签和一条提示信息
        flash(f&quot;预测：{predicted_label}&quot;, &quot;成功&quot;)
        os.remove(img_path) # 处理后删除临时文件

        return redirect(request.url) # 重新加载页面以避免重新提交

    return render_template(&#39;uploadimage.html&#39;) # 对于 GET 请求，渲染表单

]]></description>
      <guid>https://stackoverflow.com/questions/78367946/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-19-38-c</guid>
      <pubDate>Mon, 22 Apr 2024 17:25:20 GMT</pubDate>
    </item>
    <item>
      <title>用于回归问题的 PyTorch 模型，每个样本 4 个图像，图像之间有时间间隔</title>
      <link>https://stackoverflow.com/questions/78366460/pytorch-model-for-regression-problem-with-4-images-per-sample-with-time-gap-betw</link>
      <description><![CDATA[我正在使用一个数据集，其中每个样本对应于以已知延迟拍摄的 4 个图像，并且每组 4 个图像都有一个目标预测，该目标预测是一个数字（不是分类）。我目前已经制作了下面的模型，但它根本没有给出好的结果。有什么建议吗？
class SimpleModel(nn.Module)：
    def __init__(自身):
        超级(SimpleModel, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(8)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout1 = nn.Dropout(p=0.25)
        
        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(16)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout2 = nn.Dropout(p=0.25)
        
        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(32)
        self.pool3 = nn.MaxPool2d(kernel_size=5, stride=2)
        self.dropout3 = nn.Dropout(p=0.25)
        
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28800, 512)
        self.dropout4 = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(512, 1) # 单输出

    def 前向（自身，x）：
        x = torch.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        x = self.dropout1(x)
        
        x = torch.relu(self.bn2(self.conv2(x)))
        x = self.pool2(x)
        x = self.dropout2(x)
        
        x = torch.relu(self.bn3(self.conv3(x)))
        x = self.pool3(x)
        x = self.dropout3(x)
        
        x = self.展平(x)
        x = torch.relu(self.fc1(x))
        x = self.dropout4(x)
        
        x = self.fc2(x) # 输出层，无回归激活函数
        返回x

此外，目标预测值通常非常小，有时甚至大得多，例如从 1e-9 到 1e2 左右。我已将对数刻度应用于目标预测，以减少这种影响，以尝试改进学习，但不确定它有多大帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78366460/pytorch-model-for-regression-problem-with-4-images-per-sample-with-time-gap-betw</guid>
      <pubDate>Mon, 22 Apr 2024 13:04:42 GMT</pubDate>
    </item>
    <item>
      <title>预测值与目标值/实际值之间没有相关性[关闭]</title>
      <link>https://stackoverflow.com/questions/78365619/no-correlation-between-predicted-values-and-target-value-real-values</link>
      <description><![CDATA[我正在执行回归任务。当我绘制预测值与实际值时，我发现变量之间没有相关性。我猜这意味着模型无法拟合数据（类似于分类模型预测最常见的类别）。


我尝试了很多方法，但没有一个能够使模型适合数据：

我尝试用对数函数转换目标值：

y_train = np.log(y_train)
y_test = np.log(y_test)



我对目标变量应用了平方根函数，但它不起作用：

y_train = (y_train)**0.5
y_测试 = (y_测试)**0.5



我什至尝试标准化目标函数，但也不起作用

def preprocess_data_standard_regression(数据):
    定标器=标准定标器()
    X = data[[data.columns 中的 col 的 col
              如果不是 col.startswith(&quot;POSTOP_&quot;)
              并且 col !=“in_患者_id”
              和 col !=“in_laterity”]]
    y = 数据[“POSTOP_MAN_vault_posto”]
    y = scaler.fit_transform(y.values.reshape(-1,1)).flatten()
    缩放器 = MinMaxScaler()
    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)
    X_train = 缩放器.fit_transform(X_train)
    X_test = 缩放器.transform(X_test)
    返回 X_train、X_test、y_train、y_test


我的数据集的形状是 545 行 vs 24 列。]]></description>
      <guid>https://stackoverflow.com/questions/78365619/no-correlation-between-predicted-values-and-target-value-real-values</guid>
      <pubDate>Mon, 22 Apr 2024 10:41:06 GMT</pubDate>
    </item>
    <item>
      <title>关于Keras历史回调损失与控制台输出损失不匹配的调查</title>
      <link>https://stackoverflow.com/questions/78360208/investigation-about-keras-history-callback-loss-not-matching-with-console-output</link>
      <description><![CDATA[请问，有谁知道为什么这个问题中描述了这个问题（Keras历史回调损失与损失的控制台输出不匹配）会发生吗？这个问题只有一个答案，它指的是可能的 TensorFlow 版本错误，但我不相信这一点，特别是因为 OP 没有对答案发表评论。我也遇到了这种情况，使用 Keras 指南中的 LossAndErrorPrintingCallback(keras.callbacks.Callback) 类和 def function on_epoch_end(self, epoch, logs=None) 函数 &lt; a href=&quot;https://keras.io/guides/writing_your_own_callbacks/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://keras.io/guides/writing_your_own_callbacks/。我还测试了 CSVLogger Keras 回调的使用，并且得到的结果与 model.fit() 输出中显示的结果不同。我使用的是 TensorFlow 2.4.1 版本。]]></description>
      <guid>https://stackoverflow.com/questions/78360208/investigation-about-keras-history-callback-loss-not-matching-with-console-output</guid>
      <pubDate>Sun, 21 Apr 2024 02:09:48 GMT</pubDate>
    </item>
    <item>
      <title>多类问题的层次分类方法</title>
      <link>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</link>
      <description><![CDATA[有一个多类分类任务。我的目标是使用每父节点本地分类器 (LCPN) 方法来解决这个问题。
让我解释一下如何使用 MWE。
假设我有这个虚拟数据集：
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification
从 scipy.cluster 导入层次结构

X, y = make_classification(n_samples=1000, n_features=10, n_classes=5,
                             n_信息=4）

我想出了这些类之间的距离矩阵：
d = np.array(
[[ 0.、201.537、197.294、200.823、194.517]、
 [201.537, 0., 199.449, 202.941, 196.703],
 [197.294, 199.449, 0., 198.728, 192.354],
 [200.823, 202.941, 198.728, 0., 195.972],
[[194.517, 196.703, 192.354, 195.972, 0.]]
）

因此，我确定了类层次结构，如下所示：
hc = hierarchy.linkage(d, method=&#39;complete&#39;)

得到的树状图如下：
dendrogram = hierarchy.dendrogram(hc, labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;, &#39;D&#39;, &#39;F&#39;])
树状图


我使用hierarchy.to_tree()以树状结构进行说明：

我的问题：
如何按照 LCPN 方法在每个内部节点（包括根）处安装分类器，例如 DecisionTreeClassifier 或 SVM，以像在树中一样进行上图？]]></description>
      <guid>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</guid>
      <pubDate>Sat, 20 Apr 2024 14:08:05 GMT</pubDate>
    </item>
    <item>
      <title>为什么数组形状会出现这个错误，有其他解决方案吗？</title>
      <link>https://stackoverflow.com/questions/75301595/why-does-this-error-of-array-shape-is-there-other-solutions</link>
      <description><![CDATA[我不断收到此错误，但我通过重塑数组解决了问题：data = data.reshape(-1, 1)
我的输出：
回溯（最近一次调用最后一次）：
  文件“C:\Users\USER\Desktop\python\machine-learning\bot4.py”，第 93 行，在  中
    预测 = model.predict(data)
  文件“C:\Users\USER\Desktop\python\machine-learning\machine-learningVenv\lib\site-packages\sklearn\naive_bayes.py”，第 105 行，在预测中
    X = self._check_X(X)
  文件“C:\Users\USER\Desktop\python\machine-learning\machine-leaningVenv\lib\site-packages\sklearn\naive_bayes.py”，第 579 行，位于 _check_X 中
    返回 self._validate_data(X,accept_sparse=“csr”,reset=False)
  文件“C:\Users\USER\Desktop\python\machine-learning\machine-learningVenv\lib\site-packages\sklearn\base.py”，第 546 行，位于 _validate_data
    X = check_array(X, input_name=“X”, **check_params)
  文件“C:\Users\USER\Desktop\python\machine-learning\machine-learningVenv\lib\site-packages\sklearn\utils\validation.py”，第 902 行，在 check_array 中
    引发值错误（
ValueError：需要 2D 数组，却得到 1D 数组：
array=[&#39;猫正在阳光下睡觉。&#39; “狗对着月亮狂吠。”]。
如果数据具有单个特征，则使用 array.reshape(-1, 1) 重塑数据；如果数据包含单个样本，则使用 array.reshape(1, -1) 重塑数据。

我期待输出：
[{“猫”:“睡觉”,“狗”:“吠叫”}]]]></description>
      <guid>https://stackoverflow.com/questions/75301595/why-does-this-error-of-array-shape-is-there-other-solutions</guid>
      <pubDate>Tue, 31 Jan 2023 18:27:25 GMT</pubDate>
    </item>
    <item>
      <title>pyspark：名称错误：名称“spark”未定义</title>
      <link>https://stackoverflow.com/questions/39541204/pyspark-nameerror-name-spark-is-not-defined</link>
      <description><![CDATA[我是从官方文档网站复制pyspark.ml示例：
http://spark.apache.org /docs/latest/api/python/pyspark.ml.html#pyspark.ml.Transformer
data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),(Vectors.dense([9.0, 8.0]),) , (Vectors.dense([8.0, 9.0]),)]
df = Spark.createDataFrame(数据, [“特征”])
kmeans = KMeans(k=2, 种子=1)
模型 = kmeans.fit(df)

但是，上面的示例无法运行并给出以下错误：

&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
NameError Traceback（最近一次调用最后一次）
&lt;ipython-input-28-aaffcd1239c9&gt;在&lt;模块&gt;()中
      1 从 pyspark 导入 *
      2 数据 = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),(Vectors.dense([9.0, 8.0]),), (Vectors.dense ([8.0, 9.0]),)]
----&gt; 3 df = Spark.createDataFrame(数据, [“特征”])
      4 kmeans = KMeans(k=2, 种子=1)
      5 模型 = kmeans.fit(df)

NameError：名称“spark”未定义

需要设置哪些附加配置/变量才能运行示例？]]></description>
      <guid>https://stackoverflow.com/questions/39541204/pyspark-nameerror-name-spark-is-not-defined</guid>
      <pubDate>Fri, 16 Sep 2016 23:05:11 GMT</pubDate>
    </item>
    </channel>
</rss>