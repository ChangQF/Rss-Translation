<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 27 Jul 2024 09:15:32 GMT</lastBuildDate>
    <item>
      <title>如何将 nural 函数 1.X tensorflow 更改为 2.X tensorflow</title>
      <link>https://stackoverflow.com/questions/78800789/how-to-change-nural-function-1-x-tensorflow-to-2-x-tensorflow</link>
      <description><![CDATA[layer_1 = tf.nn.relu(tf.add(tf.matmul(x, w_1), b_1))
layer_1_b = tf.layers.batch_normalization(layer_1)
layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1_b, w_2), b_2))
layer_2_b = tf.layers.batch_normalization(layer_2)
layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2_b, w_3), b_3))
layer_3_b = tf.layers.batch_normalization(layer_3)
y = tf.nn.relu(tf.add(tf.matmul(layer_3, w_4), b_4))
g_q_action = tf.argmax(y, axis=1)

# 计算损失
g_target_q_t = tf.placeholder(tf.float32, None, name=&quot;target_value&quot;)
g_action = tf.placeholder(tf.int32, None, name=&#39;g_action&#39;)
action_one_hot = tf.one_hot(g_action, n_output, 1.0, 0.0, name=&#39;action_one_hot&#39;)
q_acted = tf.reduce_sum(y * action_one_hot, reduction_indices=1, name=&#39;q_acted&#39;)

g_loss = tf.reduce_mean(tf.square(g_target_q_t - q_acted), name=&#39;g_loss&#39;)
optim = tf.train.RMSPropOptimizer(learning_rate=0.001, motivation=0.95, epsilon=0.01).minimize(g_loss)

错误语句：
回溯（最近一次调用）：
文件“C:\Users\T\PycharmProjects\Project1.venv\main_test.py”，第 139 行，位于 
layer_1 = tf.compat.v1.nn.relu(tf.add(tf.matmul(x, w_1), b_1))
^^^^^^^^^^^^^^^^^
文件“C:\Users\T\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\ops\weak_tensor_ops.py”，第 142 行，位于包装器中
return op(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^
文件 &quot;C:\Users\T\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\util\traceback_utils.py&quot;，第 153 行，位于 error_handler
从 None 引发 e.with_traceback(filtered_tb)
文件 &quot;C:\Users\T\AppData\Local\Programs\Python\Python312\Lib\site-packages\tensorflow\python\framework\ops.py&quot;，第 1037 行，位于 _create_c_op
引发 ValueError(e.message)
将 1.x 更改为 2.x tensorflow]]></description>
      <guid>https://stackoverflow.com/questions/78800789/how-to-change-nural-function-1-x-tensorflow-to-2-x-tensorflow</guid>
      <pubDate>Sat, 27 Jul 2024 07:23:19 GMT</pubDate>
    </item>
    <item>
      <title>尽管有多个 GPU，CUDA 仍出现内存不足错误</title>
      <link>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</link>
      <description><![CDATA[尝试运行 PyTorch 模型时，我遇到了 CUDA 内存不足错误，尽管我的系统有多个 NVIDIA GPU。
# 加载 tokenizer 和模型
tokenizer = AutoTokenizer.from_pretrained(&quot;MODEL_TYPE&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;MODEL_TYPE&quot;, output_attentions=True, device_map = &#39;auto&#39;, torch_dtype=torch.float16, low_cpu_mem_usage=True)

我有 8 个 GPU，模型分布在所有 GPU 上。但是，由于我的输入是长上下文（大约 20k 个 token）。尽管其他 GPU 中有很多空间，但我还是收到 GPU0 的 CUDA 内存错误。请注意，这是对批处理大小 1 的推断。
OutOfMemoryError：CUDA 内存不足。尝试分配 20.11 GiB。GPU 0 的总容量为 22.17 GiB，其中 16.06 GiB 是空闲的。包括非 PyTorch 内存在内，此进程使用了​​ 6.10 GiB 内存。在分配的内存中，5.57 GiB 由 PyTorch 分配，308.62 MiB 由 PyTorch 保留但未分配。如果保留但未分配的内存很大，请尝试设置 max_split_size_mb 以避免碎片化。请参阅内存管理和 PYTORCH_CUDA_ALLOC_CONF 的文档

 input = tokenizer(prompt, return_tensors=&quot;pt&quot;)
torch.cuda.empty_cache()
model.generation_config.temperature = temp
model.eval()
with torch.no_grad():
output = model.generate(inputs.input_ids, max_length=25000, output_attentions=False,output_scores=False, return_dict_in_generate=True)
print(&quot;temp:&quot;,model.generation_config.temperature)
tokens = tokenizer.convert_ids_to_tokens(inputs[&#39;input_ids&#39;][0])

response = tokenizer.batch_decode(output[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]

如何有效利用可用的 GPU 进行长上下文输入以避免内存不足错误？
我尝试将输入强制到其他 GPU，但没有成功：
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda:1&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78800294/cuda-out-of-memory-error-despite-having-multiple-gpus</guid>
      <pubDate>Sat, 27 Jul 2024 01:14:45 GMT</pubDate>
    </item>
    <item>
      <title>Raspberry Pi 0 2W 上的深度音频分类模型的最佳 ML API 是什么？</title>
      <link>https://stackoverflow.com/questions/78798996/what-is-the-best-ml-api-for-a-deep-audio-classification-model-on-the-raspberry-p</link>
      <description><![CDATA[我想听听您的意见，哪种 ML API 最适合构建音频分类模型。此模型将部署在一台小型 Raspberry Pi（Raspberry Pi 0 2W；可能略有不同）计算机上，该计算机位于我正在构建的设备中，是我所从事的技术初创公司的一部分。欢迎您提出您的想法和建议。
我开始使用 tensorflow keras 进行深度频谱分析；但此时的首要任务是构建尽可能最好的模型，我知道 tf keras 有点业余。]]></description>
      <guid>https://stackoverflow.com/questions/78798996/what-is-the-best-ml-api-for-a-deep-audio-classification-model-on-the-raspberry-p</guid>
      <pubDate>Fri, 26 Jul 2024 15:58:11 GMT</pubDate>
    </item>
    <item>
      <title>来自segmentation_models_pytorch 的 Unet 在训练中停滞</title>
      <link>https://stackoverflow.com/questions/78798820/unet-from-segmentation-models-pytorch-stalling-in-training</link>
      <description><![CDATA[我一直在遵循关于在自定义数据集上训练分割模型的教程，但它拒绝在训练模型方面取得任何进展。
这是我的模型设置
import fragmentation_models_pytorch as smp
import torch

ENCODER = &#39;efficientnet-b0&#39;
ENCODER_WEIGHTS = &#39;imagenet&#39;
CLASSES = [&#39;ship&#39;]
ACTIVATION = &#39;sigmoid&#39;
DEVICE = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

model = smp.Unet(
coder_name=ENCODER, 
coder_weights=ENCODER_WEIGHTS, 
classes=len(CLASSES), 
activation=ACTIVATION,
).to(DEVICE)

from fragmentation_models_pytorch import utils as smp_utils

loss = smp_utils.losses.DiceLoss()
metrics = [
smp_utils.metrics.IoU(threshold=0.5),
]

optimizer = torch.optim.Adam([ 
dict(params=model.parameters(), lr=0.0001),
])


和 epochs 运行器
train_epoch = smp_utils.train.TrainEpoch(
model, 
loss=loss, 
metrics=metrics, 
optimizer=optimizer,
device=DEVICE,
verbose=True,
)

valid_epoch = smp_utils.train.ValidEpoch(
model, 
loss=loss, 
metrics=metrics, 
device=DEVICE,
verbose=True,
)

而且，当我运行训练时，模型只是停留在第一个 epoch 上，没有任何进展
max_score = 0

for i in range(0, 40):

print(&#39;\nEpoch: {}&#39;.format(i))
train_logs = train_epoch.run(train_loader)
valid_logs = valid_epoch.run(valid_loader)

# 执行某些操作（保存模型、更改 lr 等）
if max_score &lt; valid_logs[&#39;iou_score&#39;]:
max_score = valid_logs[&#39;iou_score&#39;]
torch.save(model, &#39;./best_model.pth&#39;)
print(&#39;模型已保存！&#39;)

if i == 25:
optimizer.param_groups[0][&#39;lr&#39;] = 1e-5
print(&#39;将解码器学习率降低至 1e-5！&#39;)

结果：
Epoch：0
train：0%| | 0/3851 [00:00&lt;?, ?it/s]

我这样把它放了 3 个小时，它一点变化都没有
我在 CPU (i7-10710U) 上运行（我知道它比 GPU 慢得多，但我的 GPU (GeForce 1650mq) 不支持 cuda），内存为 32 GB，我之前运行过类似的模型，没有任何问题。
有人能帮帮我吗？也许我漏掉了什么？也许有一个更轻的模型可以在我的系统上运行？
我已经尝试了一些其他设置和模型，YOLOv8 和 YOLOv3 也拒绝训练。]]></description>
      <guid>https://stackoverflow.com/questions/78798820/unet-from-segmentation-models-pytorch-stalling-in-training</guid>
      <pubDate>Fri, 26 Jul 2024 15:14:45 GMT</pubDate>
    </item>
    <item>
      <title>线性模型的 SHAP 值与手动计算的值不同</title>
      <link>https://stackoverflow.com/questions/78796974/shap-values-for-linear-model-different-from-those-calculated-manually</link>
      <description><![CDATA[我训练一个线性模型来预测房价，然后我手动比较 Shapley 值计算结果与 SHAP 库返回的值，发现它们略有不同。
我的理解是，对于线性模型，Shapley 值由以下公式给出：
coeff * features for obs - coeffs * mean(features in training set)

或者如 SHAP 文档中所述：coef[i] * (x[i] - X.mean(0)[i])，其中 i 是一个特征。
问题是，为什么 SHAP 返回的值与手动计算不同？
代码如下：
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import LinearRegression
from sklearn.model_selection 导入 train_test_split
从 sklearn.preprocessing 导入 MinMaxScaler
导入 shap

X, y = fetch_california_housing(return_X_y=True, as_frame=True)
X = X.drop(columns = [&quot;Latitude&quot;, &quot;Longitude&quot;, &quot;AveBedrms&quot;])

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.3, random_state=0,
)

scaler = MinMaxScaler().set_output(transform=&quot;pandas&quot;).fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

linreg = LinearRegression().fit(X_train, y_train)
coeffs = pd.Series(linreg.coef_, index=linreg.feature_names_in_)

X_test.reset_index(inplace=True, drop=True)
obs = 6188

# 手动 shapley 计算
effect = coeffs * X_test.loc[obs]
effect - coeffs * X_train.mean()

返回结果：
MedInc 0.123210
HouseAge -0.459784
AveRooms -0.128162
Population 0.032673
AveOccup -0.001993
dtype: float64

SHAP 库返回的结果略有不同：
explainer = shap.LinearExplainer(linreg, X_train)
shap_values = explainer(X_test)
shap_values[obs]

结果如下：
.values =
array([ 0.12039244, -0.47172515, -0.12767778, 0.03473923, -0.00251017])

.base_values =
2.0809714707337523

.data =
array([0.25094137, 0.01960784, 0.06056066, 0.07912217, 0.00437137])

设置为忽略交互：
explainer.feature_perturbation

返回
&#39;interventional&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/78796974/shap-values-for-linear-model-different-from-those-calculated-manually</guid>
      <pubDate>Fri, 26 Jul 2024 08:22:09 GMT</pubDate>
    </item>
    <item>
      <title>GradientBoostedClassifier() 中的 min_samples_leaf 行为怪异</title>
      <link>https://stackoverflow.com/questions/78796671/min-samples-leaf-in-gradientboostedclassifier-having-weird-behavior</link>
      <description><![CDATA[尝试在 GradientBoostedClassifer() 中调整 min_samples_leaf。我看到了偏差/方差权衡的预期结果。但是，为了测试边界，我让 min_samples_leaf &gt;训练数据集中有 n_samples，预计会出现错误或其他问题，但我仍然得到与模型调整时类似的结果：
df = df_a # 样本数 = 347
df=df.sample(frac=1) 
train_proportion = 0.8 
n = len(df)
t = int(train_proportion * n)

# 单独的训练和测试集
y = df[&#39;detected&#39;]
X = df.loc[:, ~df.columns.isin([&#39;detected&#39;])]

# 训练集中的样本
train_x = X.iloc[:t,:].reset_index().iloc[:,1:]
# 测试集中的样本
test_x = X.iloc[t:,:].reset_index().iloc[:,1:]
# 训练集中的目标
train_y = pd.Series(y[:t].reset_index().iloc[:,1:].iloc[:,0])
#测试集中的目标
test_y = pd.Series(y[t:].reset_index().iloc[:,1:].iloc[:,0])

clf = GradientBoostingClassifier(n_estimators = 100, max_depth = 10, random_state= 0, min_samples_leaf=500)
clf.fit(train_x,train_y)
print(clf.score(train_x,train_y))
print(clf.score(test_x,test_y))

输出：
0.924187725631769
0.9142857142857143

为什么会这样？我预计会出现错误或不会进行拆分。文档中似乎没有说明如果 min_samples_leaf &gt; n_samples 会发生什么。对 int 的唯一要求是范围 [1,inf]。对此也没有其他说明。
我当时想也许它会将 min_samples_leaf 重置为某个可用值，但所有子树都没有深度，也没有进行拆分：subtree]]></description>
      <guid>https://stackoverflow.com/questions/78796671/min-samples-leaf-in-gradientboostedclassifier-having-weird-behavior</guid>
      <pubDate>Fri, 26 Jul 2024 07:05:30 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：不支持 y 的稀疏多标签指标 - 如何处理具有稀疏数据的多标签分类？</title>
      <link>https://stackoverflow.com/questions/78795297/valueerror-sparse-multilabel-indicator-for-y-is-not-supported-how-to-handle-m</link>
      <description><![CDATA[我只是个初学者，我仍在学习稀疏矩阵。
这是我遇到的问题，在网上搜索后找不到合适的答案。
我使用默认参数sparse_output=True对分类标签进行了 OneHotEncoded，
当我尝试在训练测试拆分后使用transformed_X和目标y拟合 RandomForestClassifier 时，它显示了此错误。
ValueError: 不支持 y 的稀疏多标签指示器。

使用列转换器进行独热编码
#seed
np.random.seed(42)

#独热编码导入
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer as ct

#数据分割
X = f_data.drop(&#39;attended&#39;, axis = 1)
y = f_data[&#39;attended&#39;]

#选择列
cat_col = [&#39;days_before&#39;,&#39;day_of_week&#39;,&#39;time&#39;,&#39;category&#39;]

#初始化编码器
enc = OneHotEncoder()

#使用 ct 进行编码器拟合
transformer = ct([(&#39;enc&#39;,enc,cat_col)], remainder = &#39;passthrough&#39;)
transformed_X = transformer.fit_transform(X)
transformed_X

经过独热编码后的 transformed_X
&lt;1480x36 稀疏矩阵，类型为 &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;压缩稀疏行格式中存储了 10360 个元素&gt;
使用 RandomForestClassifier 拟合 transformed_X 和 y
#BaseLine 模型
np.random.seed(42)

#imports
from sklearn.model_selection import train_test_split as tts
from sklearn.ensemble import RandomForestClassifier

#splitting
X_train,Y_train,X_test,Y_test = tts(transformed_X,y, test_size = 0.2)

#model fitting
model = RandomForestClassifier()
model.fit(X_train,Y_train)

完整错误
--------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[416]，第 13 行
11 #modelling
12 model = RandomForestClassifier()
---&gt; 13 model.fit(X_train,Y_train)
15 #模型得分
16 blsc = model.score(X_test,Y_test)

文件 G:\Md Jaffer\UDEMY\Machine Learning Course ZTM\Projects\HeartDesease_Classification\env\Lib\site-packages\sklearn\base.py:1474，在 _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)
1467 estimator._validate_params()
1469 使用 config_context(
1470 skip_parameter_validation=(
1471 prefer_skip_nested_validation 或 global_skip_validation
1472 )
1473 ):
-&gt; 1474 返回 fit_method(estimator, *args, **kwargs)

文件 G:\Md Jaffer\UDEMY\Machine Learning Course ZTM\Projects\HeartDesease_Classification\env\Lib\site-packages\sklearn\ensemble\_forest.py:361，位于 BaseForest.fit(self, X, y, sample_weight)
359 # 验证或转换输入数据
360 if issparse(y):
--&gt; 361 引发 ValueError(&quot;不支持 y 的稀疏多标签指标。&quot;)
363 X, y = self._validate_data(
364 X,
365 y,
(...)
369 force_all_finite=False,
370 )
371 # _compute_missing_values_in_feature_mask 检查 X 是否有缺失值，并且
372 # 如果底层树基础估计器无法处理缺失值，则会引发错误。只需要标准来确定树是否支持
374 # 缺失值。

ValueError: 不支持 y 的稀疏多标签指标。

我尝试设置 sparse_output=False，但它给出了样本数量不一致的错误。标签编码后的实际形状为 (1480 x 36)
---------------------------------------------------------------------------
ValueError Traceback (most recent call last)
Cell In[444], line 13
11 #modelling
12 model = RandomForestClassifier()
---&gt; 13 model.fit(X_train,Y_train)
15 #model score
16 blsc = model.score(X_test,Y_test)

ValueError: 发现输入变量的样本数量不一致: [1184, 296]
]]></description>
      <guid>https://stackoverflow.com/questions/78795297/valueerror-sparse-multilabel-indicator-for-y-is-not-supported-how-to-handle-m</guid>
      <pubDate>Thu, 25 Jul 2024 20:21:08 GMT</pubDate>
    </item>
    <item>
      <title>具有 10k 行独特上下文的合成 PII 数据集 [关闭]</title>
      <link>https://stackoverflow.com/questions/78794440/synthetic-pii-dataset-with-unique-contexts-for-10k-lines</link>
      <description><![CDATA[我正在寻找一个包含 10,000 行数据的合成数据集，其中包含各种类型的个人身份信息 (PII)，用于分类问题。数据应按段落格式化，并且每个段落应具有唯一的上下文。
我需要涵盖不同类型的 PII 数据，例如
[&quot;地址&quot;,
&quot;银行 • 帐号&quot;
&quot;信用卡 • 卡号&quot;
&quot;电子邮件地址&quot;
&quot;政府 - 身份证号码&quot;
&quot;个人姓名&quot;
&quot;密码&quot;
&quot;电话号码&quot;
&quot;密钥•（又称私钥）&quot;
121]
&quot;用户 ID&quot;,
&quot;出生日期&quot;,&quot;性别&quot;]

此外，每个段落在上下文中都是不同的，这一点至关重要。我尝试过使用 Faker，但它依赖于占位符模板，例如：
templates = [
&quot;{intro} {name} 出生于 {dob}，住在 {address}。您可以通过电子邮件 {email} 或电话 {phone} 联系他们。{closing}&quot;,
&quot;{intro} {name} 的社会安全号码是 {ssn}，护照号码是 {passport}。他们的信用卡号是 {ccn}。{closing}&quot;,
&quot;{intro} {name} 在 {license_year} 年获得了驾照号码 {dl}。 {closing}&quot;,
&quot;{intro} {name} 的电子邮件地址是 {email}，家庭住址是 {address}。他们出生于 {dob}，电话号码是 {phone}。{closing}&quot;,
&quot;{intro} {name} 的全名是 {name}，出生于 {dob}。他们的联系信息包括电话号码 {phone} 和电子邮件 {email}。他们居住在 {address}。{closing}&quot;
]

问题是这些句子在模板中重复出现，导致上下文变化有限。
我也尝试过使用 Kaggel，但它的结果没有涵盖所有必需的 PII 数据类型。还检查了 Github 存储库，但没有找到任何可靠的解决方案。
我正在寻找一种生成完全随机且唯一段落的方法。有人可以建议一种以编程方式创建具有多样化和独特上下文的合成 PII 数据的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78794440/synthetic-pii-dataset-with-unique-contexts-for-10k-lines</guid>
      <pubDate>Thu, 25 Jul 2024 16:19:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 JSON 数据降低掩码质量以训练 U 网模型</title>
      <link>https://stackoverflow.com/questions/78786001/down-quality-of-mask-with-json-data-for-train-u-net-model</link>
      <description><![CDATA[我想用 json 格式屏蔽我的图像作为 u net 训练模型的数据。
我使用下面的代码来屏蔽它们：

import json
import numpy as np
import cv2
import os

# 包含 JSON 文件的文件夹路径
json_folder = os.path.expanduser(&#39;~/Desktop/jeson&#39;) # 包含 JSON 文件的文件夹
# 用于保存掩码图像的文件夹路径
mask_folder = os.path.expanduser(&#39;~/Desktop/masks&#39;) # 用于保存掩码的文件夹

# 确保用于保存掩码的文件夹存在
os.makedirs(mask_folder, exist_ok=True)

# 列出文件夹中的所有 JSON 文件
for filename in os.listdir(json_folder):
if filename.endswith(&#39;.json&#39;):
json_file = os.path.join(json_folder, filename)

# 从 JSON 文件加载数据
with open(json_file) as f:
data = json.load(f)

# 创建一个空的掩码
mask = np.zeros((data[&#39;imageHeight&#39;], data[&#39;imageWidth&#39;]), dtype=np.uint8)

# 将区域添加到掩码
for shape in data[&#39;shapes&#39;]:
points = np.array(shape[&#39;points&#39;], dtype=np.int32)
if len(points) &gt; 0:
# 用白色填充由点定义的区域
cv2.fillPoly(mask, [points], 49)

# 使用 OpenCV 将掩码保存为 PNG 图像
mask_filename = os.path.splitext(filename)[0] + &#39;_mask.png&#39;
cv2.imwrite(os.path.join(mask_folder, mask_filename), mask)

print(&quot;转换完成，掩码图像已保存在 &#39;masks&#39; 文件夹中！&quot;)


但有一个有趣的问题。它在开始时只很好地掩盖了其中的几个，但其他的只用一条细线掩盖了。当我再次尝试使用此代码时，它会用一条细线掩盖所有数据。
例如在 labelme 工具中处理之前的图像：
在 lamelme 中处理之前的图像
例如屏蔽的 jeson 数据：
jeson 被屏蔽
我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78786001/down-quality-of-mask-with-json-data-for-train-u-net-model</guid>
      <pubDate>Wed, 24 Jul 2024 00:14:38 GMT</pubDate>
    </item>
    <item>
      <title>PipeOp classif.avg (mlr3) 错误：对“prob”的断言失败：包含缺失值（元素 1）</title>
      <link>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78763091/error-with-pipeop-classif-avg-mlr3-assertion-on-prob-failed-contains-missi</guid>
      <pubDate>Thu, 18 Jul 2024 07:56:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 VAE 减少和重建 CNN 模型参数</title>
      <link>https://stackoverflow.com/questions/78457309/reducing-and-reconstruction-cnn-model-parameters-using-a-vae</link>
      <description><![CDATA[假设我有一个带有 2 个 Conv2D 层的简单 CNN 模型，我在我的图像数据集上训练了这个模型，我将把这个 CNN 模型的参数输入到 VAE（作为编码器的输入）中，首先将它们的参数减少到嵌入空间（Z 或 VAE 的潜在空间）。然后，我想使用 VAE 解码器的输出重建 CNN 参数（具有其原始尺寸）。
我不知道如何在 PyTorch 中实现这一点，并将训练好的 CNN 的参数输入到 VAE 模型的编码器输入中，最后将参数向量重建为 CNN 模型参数。
提前致谢！
这是 CNN 模型：
class Net(nn.Module):
def __init__(self):
super(Net, self).__init__()
self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
self.conv2_drop = nn.Dropout2d()
self.fc1 = nn.Linear(320, 50)
self.fc2 = nn.Linear(50, 10)

def forward(self, x):
x = F.relu(F.max_pool2d(self.conv1(x), 2))
x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
x = x.view(-1, 320)
x = F.relu(self.fc1(x))
x = F.dropout(x, training=self.training)
x = self.fc2(x)
return F.log_softmax(x)

以下代码用于 VAE：
class VAE(nn.Module):
def __init__(self, image_channels=1, h_dim=1024, z_dim=32):
super(VAE, self).__init__()
self.encoder = nn.Sequential(
nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),
nn.ReLU(),
nn.Conv2d(32, 64, kernel_size=4, stride=2),
nn.ReLU(),
nn.Conv2d(64, 128, kernel_size=4, stride=2),
nn.ReLU(),
nn.Conv2d(128, 256, kernel_size=4, stride=2),
nn.ReLU(),
Flatten()
)

self.fc1 = nn.Linear(h_dim, z_dim)
self.fc2 = nn.Linear(h_dim, z_dim)
self.fc3 = nn.Linear(z_dim, h_dim)

self.decoder = nn.Sequential(
UnFlatten(),
nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),
nn.ReLU(),
nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),
nn.ReLU(),
nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),
nn.ReLU(),
nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),
nn.Sigmoid(),
)

def reparameterize(self, mu, logvar):
std = logvar.mul(0.5).exp_()
# return torch.normal(mu, std)
esp = torch.randn(*mu.size())
z = mu + std * esp
返回 z

def bottleneck(self, h):
mu, logvar = self.fc1(h), self.fc2(h)
z = self.reparameterize(mu, logvar)
返回 z, mu, logvar

def encode(self, x):
h = self.encoder(x)
z, mu, logvar = self.bottleneck(h)
返回 z, mu, logvar

def decrypt(self, z):
z = self.fc3(z)
z = self.decoder(z)
返回 z

def forward(self, x):
z, mu, logvar = self.encode(x)
z = self.decode(z)
返回 z, mu, logvar
]]></description>
      <guid>https://stackoverflow.com/questions/78457309/reducing-and-reconstruction-cnn-model-parameters-using-a-vae</guid>
      <pubDate>Thu, 09 May 2024 22:59:50 GMT</pubDate>
    </item>
    <item>
      <title>如何让 Matrox Model Finder 在单个图像中多次查找同一模型？</title>
      <link>https://stackoverflow.com/questions/78311681/how-do-i-make-the-matrox-model-finder-look-for-the-same-model-multiple-times-in</link>
      <description><![CDATA[我是 Matrox 的新手，所以这可能是一个初学者的问题。
我有一个托盘，上面有多个项目，它们都是同一型号。当我将 ModelFinder 步骤添加到程序中时，我添加了我正在寻找的模型，但它只显示我注册的模型，我猜是因为相机的失真。我如何让 Matrox 知道还有更多项目，并且它们也是同一型号？

我添加了一个必须找到它的搜索区域，我选择了查找所有出现的选项，但它只显示一个，而不是实际存在的 3/4。
]]></description>
      <guid>https://stackoverflow.com/questions/78311681/how-do-i-make-the-matrox-model-finder-look-for-the-same-model-multiple-times-in</guid>
      <pubDate>Thu, 11 Apr 2024 16:02:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在 SKLearn Estimator 上使用 Sagemaker HyperparameterTuner？</title>
      <link>https://stackoverflow.com/questions/77573670/how-do-i-use-sagemaker-hyperparametertuner-on-a-sklearn-estimator</link>
      <description><![CDATA[我正在关注 Amazon Sagemaker 研讨会，尝试利用 Sagemaker 的几个实用程序，而不是像我目前所做的那样在 Notebook 上运行所有内容。
问题是，在研讨会上，他们教你如何使用来自 AWS 的现成 XGBoost 图像来使用 HyperparameterTuner，而我的大多数管道都在使用 Scikit-Learn 模型，例如 GradientBoostingClassifier 或 RandomForest，所以我正在实例化一个像这样的估算器 此示例文件:
sklearn = SKLearn(entry_point=&quot;train.py&quot;, 
framework_version=&quot;1.2-1&quot;, 
instance_type=&quot;ml.m5.xlarge&quot;, 
role=role,
hyperparameters=fixed_hyperparameters
)

之后，我将使用刚刚创建的估算器实例化 HyperparameterTuner 作业，其中包含我想要的超参数范围测试。
hyperparameters_ranges = {
&quot;n_estimators&quot;: ContinuousParameter(100, 500),
&quot;learning_rate&quot;: ContinuousParameter(1e-2, 1e-1),
&quot;max_depth&quot;: IntegerParameter(2, 5),
&quot;subsample&quot;: ContinuousParameter(0.6, 1),
&quot;max_df&quot;: ContinuousParameter(0.4, 1),
&quot;max_features&quot;: IntegerParameter(5, 25),
&quot;use_idf&quot;: CategoricalParameter([True, False])
}

metric = &quot;validation:f1&quot;

tuner = HyperparameterTuner(
sklearn,
metric,
hyperparameters_ranges,
max_jobs=2,
max_parallel_jobs=2
)

我的问题是，我没有找到任何关于如何访问“train.py”文件中 SKLearn 估算器中传递的超参数的信息。我也没有找到最佳超参数存储在哪里，以便我可以将它们用于最终模型。有人能告诉我这是否可行，或者提供替代方案，看看是否有其他更简单的方法可以做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/77573670/how-do-i-use-sagemaker-hyperparametertuner-on-a-sklearn-estimator</guid>
      <pubDate>Wed, 29 Nov 2023 18:27:13 GMT</pubDate>
    </item>
    <item>
      <title>视觉框架（iOS）：VNDetectFaceLandmarksRequest 和 VNDetectFaceRectanglesRequest 有何不同？</title>
      <link>https://stackoverflow.com/questions/66367963/vision-framework-ios-how-are-vndetectfacelandmarksrequest-and-vndetectfacerec</link>
      <description><![CDATA[您可以使用两种不同的请求通过 iOS Vision Framework 执行人脸检测任务：VNDetectFaceLandmarksRequest 和 VNDetectFaceRectanglesRequest。它们都返回一个 VNFaceObservation 数组，每个检测到的人脸对应一个数组。VNFaceObservation 具有多种可选属性，包括 boundingBox 和 landmarks。landmarks 对象还包括可选属性，例如 nose、innerLips、leftEye 等。
这两种不同的 Vision 请求在执行人脸检测的方式上是否有所不同？
似乎 VNDetectFaceRectanglesRequest 只能找到一个边界框（可能还有其他一些属性），但找不到任何地标。另一方面，VNDetectFaceLandmarksRequest 似乎可以同时找到边界框和地标。
是否存在一种请求类型可以找到面部而另一种则找不到的情况？ VNDetectFaceLandmarksRequest 是否 优于 VNDetectFaceRectanglesRequest，或者后者在 性能 或 可靠性 方面可能更具优势？
以下是如何使用这两个 Vision 请求的示例代码：
let faceLandmarkRequest = VNDetectFaceLandmarksRequest()
let faceRectangleRequest = VNDetectFaceRectanglesRequest()
let requestHandler = VNImageRequestHandler(ciImage: image, options: [:])
try requestHandler.perform([faceRectangleRequest, faceLandmarkRequest])
if let rectangleResults = faceRectangleRequest.results as? [VNFaceObservation] {
let boundingBox1 = rectangleResults.first?.boundingBox //这是一个可选类型
}
if let landmarkResults = faceLandmarkRequest.results as? [VNFaceObservation] {
let boundingBox2 = landmarkResults.first?.boundingBox //这是一个可选类型
let landmarks = landmarks //这是一个可选类型
}
]]></description>
      <guid>https://stackoverflow.com/questions/66367963/vision-framework-ios-how-are-vndetectfacelandmarksrequest-and-vndetectfacerec</guid>
      <pubDate>Thu, 25 Feb 2021 11:53:04 GMT</pubDate>
    </item>
    <item>
      <title>银行交易数据集</title>
      <link>https://stackoverflow.com/questions/56914395/dataset-for-bank-transaction</link>
      <description><![CDATA[我想使用银行交易数据集制作信用卡和借记卡之间的图表。借记和贷记金额，但没有得到正确的数据集。
有人能给我提供同样的数据集吗？]]></description>
      <guid>https://stackoverflow.com/questions/56914395/dataset-for-bank-transaction</guid>
      <pubDate>Sat, 06 Jul 2019 13:09:23 GMT</pubDate>
    </item>
    </channel>
</rss>