<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 04 May 2024 18:17:01 GMT</lastBuildDate>
    <item>
      <title>过采样如何影响使用分类器的后验概率？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78429622/how-does-oversampling-affects-my-posterior-probability-using-a-classifier</link>
      <description><![CDATA[我正在使用随机森林算法来区分噪声和信号。我预计 400.000 个“噪音”事件中有 40 个“信号”。
我的问题是，如果我对“信号”事件进行过采样，根据 this论文，我会将我的概率推向零。
那么最好的策略是什么？

使用现实世界人口，然后使用排名作为概率
使用权重
对信号进行过采样/对信号进行欠采样？

谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78429622/how-does-oversampling-affects-my-posterior-probability-using-a-classifier</guid>
      <pubDate>Sat, 04 May 2024 16:10:09 GMT</pubDate>
    </item>
    <item>
      <title>从头开始实现变分自动编码器的反向传播</title>
      <link>https://stackoverflow.com/questions/78429529/implementing-backpropagation-from-scratch-for-variational-autoencoder</link>
      <description><![CDATA[问题
我们正在从头开始实现 VAE，但可以使用 torch.Tensor 数据结构来实现 GPU 功能。这是一项学校作业，我们似乎难以应对。我们寻找了有关 VAE 的指南和教程，但每个指南和教程都使用 PyTorch/TensorFlow，这完全混淆了反向传播过程。我们正在寻求帮助来启动反向传播过程，因为似乎一旦启动，它就会自然地通过您的网络向后传播。
架构
编码器 - 第 4 步之后有一个分割，其中第 5 - 7 步针对 latent_mean 和 latent_log 完成

输入（419, 419）
压平
密集（419 * 419, 256）
LeakyRelu
密集(256, 100)
LeakyRelu
标准化

解码器

密集（100, 256）
LeakyRelu
标准化
密集（256, 419 * 419）
标准化
重塑

流程
# 我们当前的实现对每个样本进行前向/后向传递
# 但是，权重会在每批之后更新
Latent_mean, Latent_log = 编码器.forward(x)
Latent_vector = 重新参数化（latent_mean，latent_log）

重建=解码器.forward(latent_vector)

重建损失 = torch.mean((x - 重建) ** 2)
kl_divergence = -0.5 * torch.mean(1 + Latent_log - Latent_mean ** 2 - torch.exp(latent_log))
损失 = 重建损失 + kl_divergence

这就是我们陷入困境的地方。从课堂上来看，我们似乎应该计算损失函数相对于解码器输出神经元权重的导数。我们的课堂笔记还表明，输出层推导通常与隐藏层推导不同。然而，网上很多神经网络的例子似乎并没有做出这种区分。我知道我们正在使用一个众所周知的损失函数，但我不确定如何区分它或损失函数的名称是什么（我们在一篇随机博客文章中发现了它，并且从那以后已经多次看到它）。
我在网上看到了其他示例，通过将损失/误差乘以激活函数的导数对输出激活的应用来开始反向传播过程，或者
delta = 错误 * output_activation_function_derivative(output_activation)
如果我们能弄清楚如何计算初始增量，我认为我们可以向后跟踪网络并更新其余的权重，几乎没有问题。]]></description>
      <guid>https://stackoverflow.com/questions/78429529/implementing-backpropagation-from-scratch-for-variational-autoencoder</guid>
      <pubDate>Sat, 04 May 2024 15:37:39 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中训练朴素贝叶斯模型进行情感分析</title>
      <link>https://stackoverflow.com/questions/78429490/how-to-train-in-python-a-naive-bayes-model-for-sentiment-analysis</link>
      <description><![CDATA[我正在尝试训练朴素贝叶斯模型进行情感分析，但我是 Python 新手，因为我一直在 R 中工作。
导入 pandas 作为 pd
从 sklearn.feature_extraction.text 导入 TfidfVectorizer
从 sklearn.naive_bayes 导入 MultinomialNB
从sklearn.metrics导入accuracy_score，confusion_matrix
从 sklearn.model_selection 导入 cross_val_score
从 sklearn.pipeline 导入管道
from sklearn.preprocessing import LabelEncoder # 添加这一行

# 加载预处理后的TF-IDF矩阵
tfidf_df = pd.read_excel(&#39;/Users/anisabakiu/Downloads/tfidf_r.xlsx&#39;)

# 加载带有标签的原始DataFrame
df = pd.read_excel(&#39;/Users/anisabakiu/Downloads/all-review_label.xlsx&#39;)

# 删除 NaN 值（如果有）
merged_df = pd.merge(tfidf_df, df[[&#39;review_id&#39;, &#39;label&#39;]], on=&#39;review_id&#39;).dropna()

# 如果需要的话对标签进行编码
label_encoder = LabelEncoder() # 实例化LabelEncoder
merged_df[&#39;label&#39;] = label_encoder.fit_transform(merged_df[&#39;label&#39;]) # 对标签进行编码

# 定义特征（X）和目标变量（y）
X = merged_df.drop([&#39;label&#39;, &#39;review_id&#39;], axis=1)
y = merged_df[&#39;标签&#39;]

# 定义一个 TF-IDF 矢量器
tfidf_vectorizer = TfidfVectorizer()

# 创建朴素贝叶斯分类器
naive_bayes = MultinomialNB()

# 创建一个结合 TF-IDF 矢量器和朴素贝叶斯分类器的管道
管道=管道（[
    （&#39;tfidf&#39;，tfidf_向量化器），
    (&#39;clf&#39;, naive_bayes),
]）

# 进行10次交叉验证
cv_scores = cross_val_score(管道, X, y, cv=10)

# 在整个数据集上拟合朴素贝叶斯模型
naive_bayes_model = pipeline.fit(X, y)

# 使用经过训练的模型进行预测
y_pred = naive_bayes_model.predict(X)

# 评估模型
准确度=准确度_分数（y，y_pred）
print(&#39;准确度：&#39;, 准确度)

# 打印混淆矩阵
conf_matrix = fusion_matrix(y, y_pred)
print(&#39;混淆矩阵：&#39;)
打印（conf_matrix）

这段代码对我来说似乎是正确的，但我收到了这个错误：raise KeyError(key) from err KeyError: &#39;label&#39;，我不明白。
你能帮我看看出了什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78429490/how-to-train-in-python-a-naive-bayes-model-for-sentiment-analysis</guid>
      <pubDate>Sat, 04 May 2024 15:28:14 GMT</pubDate>
    </item>
    <item>
      <title>在 Transformer 中使用 LabelEncoding 的 ML 模型管道</title>
      <link>https://stackoverflow.com/questions/78429448/pipeline-for-ml-model-using-labelencoding-in-a-transformer</link>
      <description><![CDATA[我正在尝试将各种转换与 LightGBM 模型一起合并到 scikit-learn 管道中。该模型旨在预测二手车的价格。训练完成后，我计划将此模型集成到 HTML 页面中以供实际使用。
从 sklearn.preprocessing 导入 StandardScaler、LabelEncoder
从 sklearn.pipeline 导入管道
从 sklearn.compose 导入 ColumnTransformer
导入作业库

打印（数字特征）
`[&#39;car_year&#39;, &#39;km&#39;, &#39;horse_power&#39;, &#39;cyl_capacity&#39;]`
打印（分类特征）
`[&#39;品牌&#39;、&#39;型号&#39;、&#39;装饰级别&#39;、&#39;燃料类型&#39;、&#39;变速箱&#39;、&#39;车身类型&#39;、&#39;颜色&#39;]`

# 定义数字和分类特征的转换器
numeric_transformer = Pipeline(steps=[(&#39;scaler&#39;, StandardScaler())])
categorical_transformer = 管道(steps=[(&#39;labelencoder&#39;, LabelEncoder())])

# 使用 ColumnTransformer 组合变压器
预处理器 = ColumnTransformer(
    变形金刚=[
        (&#39;num&#39;, numeric_transformer, numeric_features),
        (&#39;猫&#39;, categorical_transformer, categorical_features)
    ]
）

# 将LightGBM模型附加到预处理管道
管道=管道（步骤=[
    （&#39;预处理器&#39;，预处理器），
    (&#39;模型&#39;, best_lgb_model)
]）

# 将管道拟合到训练数据
pipeline.fit(X_train, y_train)

训练时得到的输出是：
LabelEncoder.fit_transform() 需要 2 个位置参数，但给出了 3 个]]></description>
      <guid>https://stackoverflow.com/questions/78429448/pipeline-for-ml-model-using-labelencoding-in-a-transformer</guid>
      <pubDate>Sat, 04 May 2024 15:12:44 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：无法同步创建数据集（名称已存在）</title>
      <link>https://stackoverflow.com/questions/78429387/valueerror-unable-to-synchronously-create-dataset-name-already-exists</link>
      <description><![CDATA[当我尝试将模型另存为 h5 时
caption_model.save(“/kaggle/working/mymodel.h5”)

我发现了这个错误
ValueError Traceback（最近一次调用最后一次）
[19] 中的单元格，第 1 行
----&gt; 1 title_model.save(“/kaggle/working/mymodel.h5”)

文件/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122，位于filter_traceback..error_handler(*args, **kwargs)
    第 119 章
    120 # 要获取完整的堆栈跟踪，请调用：
    121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
    123 最后：
    124 删除filtered_tb

文件 /opt/conda/lib/python3.10/site-packages/h5py/_hl/group.py:183，在 Group.create_dataset(self, name, shape, dtype, data, **kwds)
    第180章 180
    [第 181 回]
--&gt;第183章
    184 dset = 数据集. 数据集（dsid）
    185 返回数据集

文件/opt/conda/lib/python3.10/site-packages/h5py/_hl/dataset.py:163，在make_new_dset(父、形状、dtype、数据、名称、块、压缩、洗牌、fletcher32、maxshape、compression_opts 、 fillvalue、scaleoffset、track_times、external、track_order、dcpl、dapl、efile_prefix、virtual_prefix、allow_unknown_filter、rdcc_nslots、rdcc_nbytes、rdcc_w0)
    160 其他：
    161 sid = h5s.create_simple（形状，maxshape）
--&gt;第163章
    165 if (data is not None) and (not isinstance(data, Empty)):
    166 dset_id.write（h5s.ALL，h5s.ALL，数据）

文件 h5py/_objects.pyx:54，在 h5py._objects.with_phil.wrapper() 中

文件 h5py/_objects.pyx:55，在 h5py._objects.with_phil.wrapper() 中

文件 h5py/h5d.pyx:137，在 h5py.h5d.create() 中

ValueError：无法同步创建数据集（名称已存在）````

你们中有人以前遇到过这个问题或者知道如何解决它吗？

谢谢

你们中有人以前遇到过这个问题或者知道如何解决它吗？

谢谢
]]></description>
      <guid>https://stackoverflow.com/questions/78429387/valueerror-unable-to-synchronously-create-dataset-name-already-exists</guid>
      <pubDate>Sat, 04 May 2024 14:51:33 GMT</pubDate>
    </item>
    <item>
      <title>努力解决卷积自动编码器的输入和输出形状差异</title>
      <link>https://stackoverflow.com/questions/78429359/struggling-with-input-and-output-differences-in-shapes-for-convolutional-autoenc</link>
      <description><![CDATA[拟合模型时出现以下错误：
ValueError：层“sequential_15”的输入 0与图层不兼容：预期形状=(无, 27088, 64, 1)，发现形状=(无, 27086, 64, 1)
我认为 MaxPool2D 是下限舍入，而 UpSamling2D 是上限舍入。当我查看模型摘要时，我发现地板舍入导致了不同的输入和输出形状，但是，我正在努力寻找必要的参数来充分适应该模型。
这是模型的代码块：
input_shape=BUFFER_INPUT_SHAPE_WITHCHANNELS
# n_channels = input_shape[-1]
# 编码器
模型=顺序（）
model.add(Conv2D(256, (4,4), 激活=&#39;relu&#39;, 填充=&#39;相同&#39;,
输入形状=输入形状)) # (27086, 64, 1)
model.add(MaxPool2D((2,2), padding=&#39;相同&#39;))
model.add(Conv2D(128, (4,4), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(MaxPool2D((2,2), padding=&#39;相同&#39;))
model.add(Conv2D(64, (4,4), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(MaxPool2D((2,2), padding=“相同”))
# 解码器
model.add(Conv2D(64, (4,4), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(UpSampling2D((2,2)))
model.add(Conv2D(128, (4,4), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(UpSampling2D((2,2)))
model.add(Conv2D(256, (4,4), 激活=&#39;relu&#39;, padding=&#39;相同&#39;))
model.add(UpSampling2D((2,2)))
model.add(Conv2D(1, (4,4), 激活=&#39;sigmoid&#39;,
填充=&#39;相同&#39;））

model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;,
指标=[&#39;mse&#39;])
模型.summary()
      



_________________________________________________________________

层（类型）输出形状参数#
=

conv2d_135（Conv2D）（无、27086、64、256）4352

max_pooling2d_60（最大池化（无、13543、32、256）0
g2D)

conv2d_136（Conv2D）（无、13543、32、128）524416

max_pooling2d_61（最大池化（无、6772、16、128）0
g2D)

conv2d_137（Conv2D）（无、6772、16、64）131136

max_pooling2d_62（最大池化（无、3386、8、64）0
g2D)

conv2d_138（Conv2D）（无、3386、8、64）65600

up_sampling2d_58（上采样（无、6772、16、64）0
g2D)

conv2d_139（Conv2D）（无、6772、16、128）131200

up_sampling2d_59（上采样（无、13544、32、128）0
g2D)

conv2d_140（Conv2D）（无、13544、32、256）524544

up_sampling2d_60（上采样（无、27088、64、256）0
g2D)

conv2d_141（Conv2D）（无、27088、64、1）4097

如果您发现任何其他提示或改进，请告诉我。
我尝试更改池、内核和步幅参数。我还尝试通过将原始输入修剪为可多次整除的形状来更改原始输入。但是，我不确定这种方法是否通常采用。
编辑：
我将数据修剪为形状 (27072,64,1)。但这似乎不是处理我的问题的适当方法。我不想修剪我的数据]]></description>
      <guid>https://stackoverflow.com/questions/78429359/struggling-with-input-and-output-differences-in-shapes-for-convolutional-autoenc</guid>
      <pubDate>Sat, 04 May 2024 14:40:11 GMT</pubDate>
    </item>
    <item>
      <title>错误：HuggingFaceInstructEmbeddings 初始化</title>
      <link>https://stackoverflow.com/questions/78428830/error-huggingfaceinstructembeddings-initalization</link>
      <description><![CDATA[`python
from langchain.embeddings import HuggingFaceInstructEmbeddings
instructor_embeddings = HuggingFaceInstructEmbeddings()

`
我被这个错误困住了，似乎无法解决这个问题：
&#39;&#39;&#39;
TypeError：INSTRUCTOR._load_sbert_model() 获得了一个意外的关键字参数“token”
&#39;&#39;&#39;
请帮忙！我该如何解决这个问题？
我想使用 HuggingFaceInstructEmbeddings 来矢量化我的数据集，但它不起作用]]></description>
      <guid>https://stackoverflow.com/questions/78428830/error-huggingfaceinstructembeddings-initalization</guid>
      <pubDate>Sat, 04 May 2024 11:32:32 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch：不同的 DataLoader 批量大小会产生截然不同的损失</title>
      <link>https://stackoverflow.com/questions/78428584/pytorch-different-dataloader-batch-sizes-yield-very-different-losses</link>
      <description><![CDATA[我正在使用此近似示例作为正弦波学习近似的基础。我是 PyTorch 新手……为什么不同的 BATCH_SIZE 值会显著改变结果？
批次大小 512：

批次大小 10000（所有数据点）：

我在下面发布了代码，但首先我将解释一下我所做的更改原文：

所有随机种子都是静态的，禁用了随机排序：

学习率改为 1e-4，X 大小改为 10**4，MAX_EPOCH 改为 20。

添加了一个图表来绘制 X 范围内所有值的近似值，以显示差异


完整代码如下……
谢谢！
import torch
import numpy as np
import matplotlib.pyplot as plt

from torch import nn, optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split

device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)
LR = 1e-4
MAX_EPOCH = 20
BATCH_SIZE = 10**4

class SineApproximator(nn.Module):
def __init__(self):
super(SineApproximator, self).__init__()
self.regressor = nn.Sequential(nn.Linear(1, 1024),
nn.ReLU(inplace=True),
nn.Linear(1024, 1024),
nn.ReLU(inplace=True),
nn.Linear(1024, 1))
def forward(self, x):
output = self.regressor(x)
return output

torch.manual_seed(41)
if torch.cuda.is_available():
torch.cuda.manual_seed_all(41)
np.random.seed(41)

X = np.random.rand(10**4) * 2 * np.pi
y = np.sin(X)

X_train, X_val, y_train, y_val = map(torch.tensor, train_test_split(X, y, test_size=0.2, shuffle=False, random_state=41))
train_dataloader = DataLoader(TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1)), batch_size=BATCH_SIZE,
pin_memory=True, shuffle=True)
val_dataloader = DataLoader(TensorDataset(X_val.unsqueeze(1), y_val.unsqueeze(1)), batch_size=BATCH_SIZE,
pin_memory=True, shuffle=True)

model = SineApproximator().to(device)
optimizer = optim.Adam(model.parameters(), lr=LR)
criterion = nn.MSELoss(reduction=&quot;mean&quot;)

train_loss_list = list()
val_loss_list = list()
for epoch in range(MAX_EPOCH):
print(&quot;epoch %d / %d&quot; % (epoch + 1, MAX_EPOCH))
model.train()
# 训练循环
temp_loss_list = list()
for X_train, y_train in train_dataloader:
X_train = X_train.type(torch.float32).to(device)
y_train = y_train.type(torch.float32).to(device)

optimizer.zero_grad()

score = model(X_train)
loss = criterion(input=score, target=y_train)
loss.backward()

optimizer.step()

temp_loss_list.append(loss.detach().cpu().numpy())

temp_loss_list = list()
for X_train, y_train in train_dataloader:
X_train = X_train.type(torch.float32).to(device)
y_train = y_train.type(torch.float32).to(device)

score = model(X_train)
loss = criterion(input=score, target=y_train)

temp_loss_list.append(loss.detach().cpu().numpy())

avg_loss = np.average(temp_loss_list)
train_loss_list.append(avg_loss)
print(&quot;\ttrain loss: %.5f&quot; % train_loss_list[-1])

# 构建一个 np 数组，所有 X 值位于其最小值和最大值之间，间隔为 0.01，每个值位于自己的数组中：
model.eval()
X_all = torch.tensor(np.arange(X.min(), X.max(), 0.01)).type(torch.float32).unsqueeze(1).to(device)
y_prediction = model(X_all)
y_prediction = y_prediction.detach().cpu().numpy().flatten()
original = plt.scatter(X, y, s=1)
predicted = plt.scatter(X_all.detach().cpu().numpy(), y_prediction, s=1)
plt.legend((predicted, original), (&quot;Function&quot;, &quot;Samples&quot;))
plt.waitforbuttonpress()
]]></description>
      <guid>https://stackoverflow.com/questions/78428584/pytorch-different-dataloader-batch-sizes-yield-very-different-losses</guid>
      <pubDate>Sat, 04 May 2024 10:10:58 GMT</pubDate>
    </item>
    <item>
      <title>如何从未分割的（正常）图像中提取放射学特征？</title>
      <link>https://stackoverflow.com/questions/78428542/how-to-extract-radiomic-features-from-an-unsegmented-normal-image</link>
      <description><![CDATA[我正在通过从医学图像中提取放射组学特征来利用机器学习进行肿瘤检测。我的问题是：
由于健康数据上没有肿瘤，因此图像中没有分割部分。如何从健康图像中提取放射组学特征？
“值错误：在此掩码中找不到标签（即没有任何内容被分段）！”]]></description>
      <guid>https://stackoverflow.com/questions/78428542/how-to-extract-radiomic-features-from-an-unsegmented-normal-image</guid>
      <pubDate>Sat, 04 May 2024 09:57:03 GMT</pubDate>
    </item>
    <item>
      <title>如何解释基于情感分析数据训练的朴素贝叶斯模型？</title>
      <link>https://stackoverflow.com/questions/78426520/how-to-explain-a-naive-bayes-model-trained-on-data-for-sentiment-analysis</link>
      <description><![CDATA[我正在编写此代码来训练朴素贝叶斯模型：
# 加载必要的库
库（readxl）
库（插入符号）
图书馆(e1071)

# 加载预处理后的TF-IDF矩阵
tfidf_df &lt;- read_excel(&#39;~/Downloads/tfidf_r.xlsx&#39;)

# 加载带有标签的原始DataFrame
df &lt;- read_excel(&#39;~/Downloads/all-review_label.xlsx&#39;)

# 将 TF-IDF 矩阵与标签 DataFrame 合并
merged_df &lt;- 合并(tfidf_df, df, by=&#39;review_id&#39;)

# 将数据分为训练集和测试集
  设置.种子(42)
  train_indices &lt;- createDataPartition(merged_df$review_id, p = 0.8, list = FALSE)
  train_data &lt;- merged_df[train_indices, ]
  test_data &lt;- merged_df[-train_indices, ]
  
  # 初始化并训练朴素贝叶斯分类器
  naive_bayes_model &lt;- naiveBayes(标签 ~ ., data = train_data)
  
  # 对测试集进行预测
  y_pred &lt;- 预测（naive_bayes_model，newdata = test_data）
  打印（y_pred）
  
  # 将预测值和实际值转换为相同水平的因子
  级别 &lt;- 唯一（c（级别（y_pred），级别（test_data$review）））
  y_pred &lt;- 因子(y_pred, 级别 = 级别)
  test_data$label&lt;- 因子(test_data$label, 级别 = 级别)

我想解释使用 SHAP 从模型中获得的结果，我使用了以下代码：
库（kernelshap）
图书馆（shapviz）

xvars &lt;- setdiff(colnames(merged_df), &quot;label...2&quot;)

# 如果 length(xvars) 大于 10，则使用 kernelshap()。对 bg_X 进行子采样至 100-500 行
shap_values &lt;- kernelshap(naive_bayes_model,
                          X = 合并_df,
                          bg_X = 合并_df,
                          功能名称 = xvars)

shap_values &lt;- shapviz(shap_values)
sv_importance(shap_values, kind = “bar”)

但是 R 显示“停止”图标几个小时，但没有给出任何结果。
如何修复它？]]></description>
      <guid>https://stackoverflow.com/questions/78426520/how-to-explain-a-naive-bayes-model-trained-on-data-for-sentiment-analysis</guid>
      <pubDate>Fri, 03 May 2024 18:57:59 GMT</pubDate>
    </item>
    <item>
      <title>为什么最终模型中的树木数量不是我指定的数量？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78426497/why-is-the-number-of-trees-in-the-final-model-not-the-number-that-i-specified</link>
      <description><![CDATA[我使用 quantregForest 进行分位数回归森林模型，代码如下：
opt_mdl &lt;- quantregForest(x = train[, features],
                          y = 训练[，目标]，
                          节点大小 = 5,
                          尝试= 14，
                          n树= 500，
                          nthreads = 并行::DetectCores() - 1)

其中 Train 是一个包含 1909 个数据实例和特征列的数据框，features 是要使用的 24 个特征名称的列表。这里我将ntree指定为500。但是在opt_mdl中，值ntree (opt_mdl$ntree)是 34。为什么会发生这种情况以及如何解决它？]]></description>
      <guid>https://stackoverflow.com/questions/78426497/why-is-the-number-of-trees-in-the-final-model-not-the-number-that-i-specified</guid>
      <pubDate>Fri, 03 May 2024 18:51:19 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法评估模型是否能够识别有影响的变量（使用 make_classification 生成的变量）？</title>
      <link>https://stackoverflow.com/questions/78398017/is-there-a-way-to-evaluate-whether-a-model-is-able-to-identify-the-variables-tha</link>
      <description><![CDATA[我有一个关于 scikit-learn 的 make_classification 的问题。我使用 make_classification（二元分类任务）创建了一个数据集，目的是测试不同模型区分重要特征和不太重要特征的能力。
如何设置一个实验来评估模型是否能够识别有影响的变量？
我查看了 make_classification 的文档，但不幸的是我没有进一步了解。
我设置了以下内容：
X,y = make_classification(n_samples=50000, n_features=10, n_informative=5,
                    n_redundant=2、n_repeated=0、n_classes=2、n_clusters_per_class=2、
                          类间隔=1，
                   Flip_y=0.01，权重=[0.9,0.1]，shuffle=True，random_state=42）

如何显示 - 在本例中 - 5 个信息变量？使用 make_classification 生成数据时可以确定特征的重要性吗？ make_classification 认为哪些功能很重要？然后在下一步中，我将使用一些 freature_importance 方法来验证（或不验证）模型检测“预设”特征的效果如何。特征重要性/具有影响力的变量。
谢谢您，我们非常感谢任何想法或建议。]]></description>
      <guid>https://stackoverflow.com/questions/78398017/is-there-a-way-to-evaluate-whether-a-model-is-able-to-identify-the-variables-tha</guid>
      <pubDate>Sun, 28 Apr 2024 11:37:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 获取 One vs Rest SVC() 的模型参数？</title>
      <link>https://stackoverflow.com/questions/78395647/how-to-get-the-model-parameter-for-one-vs-rest-svc-using-python</link>
      <description><![CDATA[我尝试使用decision_function_shape= ovr制作onvsrest分类模型，但是当我将其更改为decision_function_shape= ovo时，它给了我与ovr相同的结果。结果我读到 svc() 正在使用 ovo 作为基础，无论它是作为 ovr 还是 ovo 启动的。那么我怎样才能改变我的代码，以便它给我一个 ovr 结果呢？
model3 = SVC(kernel = &#39;rbf&#39;, Decision_function_shape=&#39;ovr&#39;)
model3.fit(X_train, Y_train)
model3_predictions = model3.predict(X_test)

我尝试过使用 OneVsRestClassifier() 但不知道如何给出所有这些命令的输出，它总是出错并说 OneVsRestClassifier 没有这些命令。有没有办法用 OneVsRestClassifier 获取 cm、sm、sv、beta 和截距？
cm3 = fusion_matrix(Y_test, model3_predictions, labels=[-1,0,1])
sm3 = 分类报告（Y_测试，model3_预测）
support_vector3 = model3.support_
n_sv_model3 = model3.n_support_
alpha_model3 = pd.DataFrame(model3.dual_coef_)
b_model3 = pd.DataFrame(model3.intercept_)

希望有人能帮助我，先谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78395647/how-to-get-the-model-parameter-for-one-vs-rest-svc-using-python</guid>
      <pubDate>Sat, 27 Apr 2024 16:20:07 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow.js 分词器</title>
      <link>https://stackoverflow.com/questions/51663068/tensorflow-js-tokenizer</link>
      <description><![CDATA[我是机器学习和 Tensorflow 的新手，因为我不了解 python，所以我决定使用 javascript 版本（可能更像是包装器）。 
问题是我试图构建一个处理自然语言的模型。因此，第一步是对文本进行分词，以便将数据提供给模型。我做了很多研究，但大多数都使用 python 版本的tensorflow，使用如下方法：tf.keras.preprocessing.text.Tokenizer，我在tensorflow.js中找不到类似的方法。我陷入了这一步，不知道如何将文本传输到可以输入模型的向量。请帮忙:)]]></description>
      <guid>https://stackoverflow.com/questions/51663068/tensorflow-js-tokenizer</guid>
      <pubDate>Thu, 02 Aug 2018 22:40:12 GMT</pubDate>
    </item>
    <item>
      <title>Android 机器学习库</title>
      <link>https://stackoverflow.com/questions/43649359/machine-learning-libraries-for-android</link>
      <description><![CDATA[我正在尝试为我的 Android 应用程序构建一个小型文本挖掘工具。我正在检查一个机器学习库，它可以让我进行聚类、分类等。
有适用于 Android 的机器学习库吗？我遇到了 Tensorflow，但我需要更多地访问常见的 ML 函数。]]></description>
      <guid>https://stackoverflow.com/questions/43649359/machine-learning-libraries-for-android</guid>
      <pubDate>Thu, 27 Apr 2017 05:33:52 GMT</pubDate>
    </item>
    </channel>
</rss>