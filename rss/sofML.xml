<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 04 Apr 2024 18:17:57 GMT</lastBuildDate>
    <item>
      <title>为什么在启用 from_logits 的情况下使用 BinaryCrossEntropy 生成器损失？</title>
      <link>https://stackoverflow.com/questions/78275777/why-generator-loss-using-binarycrossentropy-with-from-logits-enabled</link>
      <description><![CDATA[从简单的普通 GAN 代码中，我查看  GitHub
我看到这个生成器模型具有激活sigmoid：
&lt;前&gt;&lt;代码&gt;# 生成器
G = tf.keras.models.Sequential([
  tf.keras.layers.Dense(28*28 // 2, input_shape = (z_dim,), 激活=&#39;relu&#39;),
  tf.keras.layers.Dense(28*28, 激活=&#39;sigmoid&#39;),
  tf.keras.layers.Reshape((28, 28))])

在启用 from_logits 的情况下，G 的损失定义如下：
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)
def G_loss(D, x_fake):
  返回 cross_entropy(tf.ones_like(D(x_fake)), D(x_fake))

据我所知，from_logits=True旨在使损失函数接受范围在-infinity到&lt;之间的y_pred值代码&gt;无穷大。与 from_logits=False 相反，损失函数假设值的范围在 0 到 1 之间。
如您所见，G 模型的输出层已经具有 sigmoid 激活，其范围在 0 到 1.
但是，为什么作者仍然使用 from_logits=True？]]></description>
      <guid>https://stackoverflow.com/questions/78275777/why-generator-loss-using-binarycrossentropy-with-from-logits-enabled</guid>
      <pubDate>Thu, 04 Apr 2024 18:03:32 GMT</pubDate>
    </item>
    <item>
      <title>如何得到Adam训练时的平均学习率？</title>
      <link>https://stackoverflow.com/questions/78275586/how-to-get-the-average-learning-rate-for-adam-during-training</link>
      <description><![CDATA[Adam 优化器的每个参数都有一个自适应学习率，该学习率在训练期间会发生变化。我试图获得所有参数的平均学习率。我发现这个SO问题有一个相关的问题，并且建议使用答案之一（没有接受此问题的答案）
def get_current_lr（优化器，group_idx，parameter_idx）：
    # Adam 对每个参数都有不同的学习率。所以我们需要选择
    # 首先是组和参数。
    组=optimizer.param_groups[group_idx]
    p = 组[&#39;params&#39;][parameter_idx]

    beta1, _ = 组[&#39;betas&#39;]
    状态 = 优化器.状态[p]

    bias_ Correction1 = 1 - beta1 ** 状态[&#39;step&#39;]
    current_lr = group[&#39;lr&#39;] /bias_ Correction1 / torch.sqrt(state[&#39;exp_avg_sq&#39;] + 1e-8)
    返回当前_lr

我尝试根据我的情况调整它以获得平均值，但结果没有多大意义，因为学习率似乎高得离谱（有时超过 15）。所以我想知道这是否是正确的方法，或者我是否遗漏了一些东西。
导入火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim

火炬.manual_seed(42)

def get_current_lr(优化器):
    # Adam 对每个参数都有不同的学习率。所以我们需要选择
    # 首先是组和参数。
    劳斯莱斯 = []
    对于范围内的 group_idx(len(optimizer.param_groups))：
        组=optimizer.param_groups[group_idx]
        对于范围内的parameter_idx(len(opt.param_groups[group_idx][&#39;params&#39;]))：
            p = 组[&#39;params&#39;][parameter_idx]

            beta1, _ = 组[&#39;betas&#39;]
            状态 = 优化器.状态[p]

            bias_ Correction1 = 1 - beta1 ** 状态[&#39;step&#39;]
            current_lr = group[&#39;lr&#39;] /bias_ Correction1 / torch.sqrt(state[&#39;exp_avg_sq&#39;] + 1e-8)
            # 打印(current_lr.mean())
            lrs.append(current_lr.mean().item())

    返回总和(lrs)/len(lrs)

类模型（nn.Module）：
    def __init__(自身):
        超级（模型，自我）.__init__()
        self.fc1 = nn.Linear(10, 100)
        self.fc2 = nn.Linear(100, 1)

    def 前向（自身，x）：
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        返回x

净=模型（）
# opt = optim.SGD(net.parameters(), lr=1e-2)
opt = optim.Adam(net.parameters())

特征 = torch.rand((100,10))
x_goal = torch.tensor(100)

对于范围（100）内的纪元：
    x = 净值（特征）
    损失 = torch.square(x_goal - x).mean()
    opt.zero_grad()
    loss.backward()
    opt.step()
    如果纪元 % 5 == 0 ：
        打印（获取当前lr（选择））
&gt;&gt;&gt;&gt;&gt;
16.065366545983125
3.296213309022278
2.23316600310136
1.8703228593794847
1.7041209160006474
1.6200325103309297
1.5739126955249958
1.5481085549622549
1.5332565682870154
1.5246047580963022
1.5195341832059057
1.5165529197865908
1.5148021120267003
1.5137746352747854
1.5131711492076647
1.512817604085285
1.5126127881085267
1.5124981075282449
1.5124379168978521
1.5124109954704181
]]></description>
      <guid>https://stackoverflow.com/questions/78275586/how-to-get-the-average-learning-rate-for-adam-during-training</guid>
      <pubDate>Thu, 04 Apr 2024 17:26:49 GMT</pubDate>
    </item>
    <item>
      <title>验证集和测试集之间的性能差距（ResNet-18、k-Fold CV）</title>
      <link>https://stackoverflow.com/questions/78275584/performance-gap-between-validation-and-test-sets-resnet-18-k-fold-cv</link>
      <description><![CDATA[我正在使用 k 折交叉验证开发二值图像分类器 (ResNet-18)。训练过程中，模型的最高验证准确率达到99%。然而，在单独的测试集上，准确率下降至 92%。我还观察到损失函数有类似的趋势，其中验证损失与测试集损失相比较低。训练和测试数据中的标签分布是平衡的。
为什么验证集和测试集之间的性能存在如此显着的差异？]]></description>
      <guid>https://stackoverflow.com/questions/78275584/performance-gap-between-validation-and-test-sets-resnet-18-k-fold-cv</guid>
      <pubDate>Thu, 04 Apr 2024 17:26:41 GMT</pubDate>
    </item>
    <item>
      <title>使用 Snowflake Cortex 函数作为外部阶段</title>
      <link>https://stackoverflow.com/questions/78275339/use-snowflake-cortex-functions-for-external-stage</link>
      <description><![CDATA[我正在尝试将 Snowflake Cortex 函数用于外部阶段（Azure）。函数对于普通表运行良好，但我不知道如何在外部阶段运行它。
下面的示例适用于普通表：
选择 SNOWFLAKE.CORTEX.COMPLETE(&#39;llama2-70b-chat&#39;,concat(&#39;&#39; , &#39;- ddl: &#39;, get_ddl(&#39;table &#39;,&#39;&lt;数据库&gt;.&lt;架构&gt;.&lt;表&gt;&#39;)))

这就是我从外部阶段选择数据的方式，它工作正常：
从 @azure_stage t 中选择 $1、$2；

我不知道如何将 LLAMA2 用于外部舞台。
有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78275339/use-snowflake-cortex-functions-for-external-stage</guid>
      <pubDate>Thu, 04 Apr 2024 16:40:04 GMT</pubDate>
    </item>
    <item>
      <title>可能是什么引发了错误：ValueError：X 有 23 个特征，但 SVR 期望 24 个特征作为输入？</title>
      <link>https://stackoverflow.com/questions/78275238/what-may-be-raising-the-error-valueerror-x-has-23-features-but-svr-is-expecti</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78275238/what-may-be-raising-the-error-valueerror-x-has-23-features-but-svr-is-expecti</guid>
      <pubDate>Thu, 04 Apr 2024 16:21:48 GMT</pubDate>
    </item>
    <item>
      <title>在yolov5s中，我添加了一个带有 [-1, 1, CA, [1024, 1]] 的 CA 层，当我运行 train.py 时，我遇到了这个问题，如何解决这个问题</title>
      <link>https://stackoverflow.com/questions/78275133/in-yolov5s-i-added-a-layer-ca-with-1-1-ca-1024-1-when-i-run-train-py</link>
      <description><![CDATA[运行时错误：给定组=1，权重大小为[32, 1024, 1, 1]，预期输入[1, 512, 16, 1]有1024个通道，但得到了512个通道
在此处输入图片描述
预期输入[1, 512, 16, 1]有1024个通道，但实际有512个通道]]></description>
      <guid>https://stackoverflow.com/questions/78275133/in-yolov5s-i-added-a-layer-ca-with-1-1-ca-1024-1-when-i-run-train-py</guid>
      <pubDate>Thu, 04 Apr 2024 16:03:19 GMT</pubDate>
    </item>
    <item>
      <title>多元线性回归房价r2得分问题</title>
      <link>https://stackoverflow.com/questions/78275121/multiple-linear-regression-house-price-r2-score-problem</link>
      <description><![CDATA[我有样本房价数据和简单代码：
导入 pandas 作为 pd
从 sklearn.preprocessing 导入 LabelEncoder、StandardScaler
从 sklearn.model_selection 导入 train_test_split
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.metrics 导入 r2_score

数据 = pd.read_csv(&#39;house_price_4.csv&#39;)
df = pd.DataFrame(数据)
df[&#39;区域&#39;] = df[&#39;区域&#39;].str.replace(&#39;,&#39;, &#39;&#39;)
df = df.dropna()

# 对分类特征“地址”进行编码
df[&#39;地址&#39;] = df[&#39;地址&#39;].astype(&#39;类别&#39;).cat.codes
df[&#39;停车&#39;] = df[&#39;停车&#39;].replace({True: 1, False: 0})
df[&#39;仓库&#39;] = df[&#39;仓库&#39;].replace({True: 1, False: 0})
df[&#39;电梯&#39;] = df[&#39;电梯&#39;].replace({True: 1, False: 0})

X = df.drop(columns=[&#39;价格(美元)&#39;,&#39;价格&#39;])
y = df[&#39;价格&#39;]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

模型=线性回归()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

r_squared = r2_score(y_test, y_pred)
print(f&#39;R^2 得分: {r_squared:.4f}&#39;)

                                                                  

我的 R2 分数非常低：0.34
如何获得更高的 R2 分数？
这是我的示例数据：https://drive.google .com/file/d/14Se90XbGJivftq3_VrtgRSalkCplduVX/view?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/78275121/multiple-linear-regression-house-price-r2-score-problem</guid>
      <pubDate>Thu, 04 Apr 2024 16:01:38 GMT</pubDate>
    </item>
    <item>
      <title>ML ColumnTransformer OneHotEncoder</title>
      <link>https://stackoverflow.com/questions/78274904/ml-columntransformer-onehotencoder</link>
      <description><![CDATA[当在数据帧的第一列中转换分类数据时，我发现 ColumnTransformer 与 OneHotEncoder 出现奇怪的行为。当我向 csv 文件添加一行时，就会发生此行为。
初始数据为：
标题、每日总收入、影院、DayInYear
AC汀巴黎,307,5,257
给莫莫的一封信，307,5,257
生命的另一天,307,5,257
批准收养，307,5,257
四月与非凡的世界, 307,5,257
美女,307,5,257
鸟男孩被遗忘的孩子，307,5,257
奇科丽塔,307,5,257

运行代码时
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd

数据集 = pd.read_csv(&#39;../data/GKIDS_DayNum_test_names.csv&#39;)
数据集[&#39;标题&#39;].str.strip()
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

从 sklearn.compose 导入 ColumnTransformer
从 sklearn.preprocessing 导入 OneHotEncoder

title_column_index = dataset.columns.get_loc(&#39;标题&#39;)
print(&#39;标题索引：&#39;, title_column_index)
ct = ColumnTransformer(transformers=[(&#39;编码器&#39;, OneHotEncoder(), [title_column_index])], 剩余=&#39;passthrough&#39;)
X_Encoded = np.array(ct.fit_transform(X))
打印（X_编码）

结果是正确的：
&lt;前&gt;&lt;代码&gt;[[1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 307 5]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 307 5]]

但是，当我添加附加行时：BlueGiant,307,5,257
到文件并重新运行代码我得到奇怪的输出：
&lt;前&gt;&lt;代码&gt; (0, 0) 1.0
  (0, 9) 307.0
  (0, 10) 5.0
  (1, 1) 1.0
  (1, 9) 307.0
  (1, 10) 5.0
  (2, 2) 1.0
  (2, 9) 307.0
  (2, 10) 5.0
  (3, 3) 1.0
  (3, 9) 307.0
  (3, 10) 5.0
  (4, 4) 1.0
  (4, 9) 307.0
  (4, 10) 5.0
  (5, 5) 1.0
  (5, 9) 307.0
  (5, 10) 5.0
  (6, 6) 1.0
  (6, 9) 307.0
  (6, 10) 5.0
  (7, 8) 1.0
  (7, 9) 307.0
  (7, 10) 5.0
  (8, 7) 1.0
  (8, 9) 307.0
  (8, 10) 5.0

我不明白为什么会这样。
请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78274904/ml-columntransformer-onehotencoder</guid>
      <pubDate>Thu, 04 Apr 2024 15:25:57 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法在同一个 Azure 机器学习管道中使用经典组件和自定义组件？</title>
      <link>https://stackoverflow.com/questions/78274833/is-there-any-way-to-use-classic-and-custom-components-in-the-same-azure-machine</link>
      <description><![CDATA[我正在尝试将管道从 Microsoft 机器学习工作室经典迁移到 Azure 机器学习。我的管道实际上在 Machine Learning Studio Classic 上运行，由以下经典和自定义组件组成。输入数据有 6 个，是使用导入数据组件从 Blob 存储帐户收集的。它们主要是 csv 文件，我使用一个自定义组件（R 脚本）来清理输入数据。这是因为“执行 R 脚本”只允许 2 个输入，但我需要 6 个输入，因为我有 6 个输入文件。之后，使用称为导出数据的组件将 csv 文件写入同一 Blob 存储帐户，但写入另一个文件夹中。我想在 Azure 机器学习中构建相同的管道，我可以在同一管道中使用自定义组件和默认组件吗？
如果我尝试使用经典组件创建管道，显然我无法使用我的自定义组件（又名 R 脚本），并且当我尝试创建自定义管道时，我无法使用经典组件，显然没有办法在我的自定义组件清理输入数据后创建自定义管道时导出我的数据。]]></description>
      <guid>https://stackoverflow.com/questions/78274833/is-there-any-way-to-use-classic-and-custom-components-in-the-same-azure-machine</guid>
      <pubDate>Thu, 04 Apr 2024 15:14:48 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 Function Transformer 对目标列进行特征转换，但我无法了解如何将其传递给函数？</title>
      <link>https://stackoverflow.com/questions/78274143/i-am-doing-feature-transformation-of-my-target-column-using-function-transformer</link>
      <description><![CDATA[代码是采用我的 Target 列，即 Time_taken(min)，其值为 (min) 36、(min) 54、(min) 65 ... 等等。所以我想创建一个新列“所用时间”其值为 36,54,65....并使用函数转换器删除 Time_taken(min)。
从 sklearn.base 导入 BaseEstimator、TransformerMixin
从 sklearn.pipeline 导入管道
类 TimeTakenTransformer(BaseEstimator, TransformerMixin):
def init(self, input_column):
self.input_column = input_column
def fit(self, X, y=None):
返回自我
def 变换（自身，X）：
    X = X.copy()
    操作 = []
    对于 X[self.input_column] 中的 i：
        a = i.split()
        op.append(int(a[1]))
    X[&#39;Time_taken&#39;] = op
    X.drop([self.input_column], axis=1, inplace=True)
    返回X

Target_column = Pipeline([(&#39;替换值&#39;, TimeTakenTransformer(input_column=&quot;TARGET_COLUMN_NAME&quot;))])
Target_column = Pipeline([(&#39;替换值&#39;, TimeTakenTransformer(input_column=&quot;TARGET_COLUMN_NAME&quot;))])
TARGET_COLUMN_NAME = “所用时间（分钟）” # 假设这是正确的列名称
假设 df 是您的 DataFrame，应用转换
df_transformed = Target_column.fit_transform(df[[TARGET_COLUMN_NAME]])
用转换后的列替换原始列
df[“Time_Taken”] = df_transformed
我无法识别传递输入的正确方法。所以我请求你修改我的代码并告诉我如何解决这个问题。感谢您的理解。]]></description>
      <guid>https://stackoverflow.com/questions/78274143/i-am-doing-feature-transformation-of-my-target-column-using-function-transformer</guid>
      <pubDate>Thu, 04 Apr 2024 13:19:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 train_on_batch() 时，张量流的形状与预期不同</title>
      <link>https://stackoverflow.com/questions/78273628/tensor-flow-gets-a-different-shape-than-expected-when-using-train-on-batch</link>
      <description><![CDATA[将数据传递给 train_on_batch() 时出现错误，即使之前检查时数据的形状正确。
def train（X_train，y_train，latent_dim，epochs = 20，batch_size = 128）：
    #初始化GAN
    生成器=build_generator（latent_dim）
    判别器=build_discriminator()
    gan=gan_net(生成器,鉴别器)
    
    batch_count = X_train.shape[0] //batch_size
    half_batch = 批量大小 // 2

    对于范围（1，纪元+1）中的纪元：
        print(&quot;###### @ Epoch&quot;,epoch)
        for _ in tqdm(范围(batch_count)):
            x = randint(0, X_train.shape[0], half_batch)
            X，标签 = X_train[x]，y_train[x]
            y = np.ones((half_batch, 1))
            print(f&quot;X: {X.shape}, 标签: {labels.shape}, y: {y.shape}&quot;)
            # 这会打印出正确的形状：
            # X: (64, 28, 28, 1), 标签: (64, 6), y: (64, 1)
            d_loss = discriminator.train_on_batch([X, 标签], y)
            # 这行会抛出错误

回溯（最近一次调用）：文件“/Users/.../Downloads/Project_newest_new.py”，第 248 行，在  中train(X_train,y_train,latent_dim,epochs=20,batch_size=128) 文件“/Users/.../Downloads/Project_newest_new.py”，第 198 行，训练中 d_loss = discriminator.train_on_batch([X, labels], y) 文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py”，第 551 行，在 train_on_batch 日志 = self. train_function(data()) 文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py”，第 153 行，在 error_handler 中引发 e .with_traceback(filtered_tb) 来自无文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py”，第 118 行，位于one_step_on_iterator 输出 = self.distribute_strategy.run( 文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py”，第 106 行，在 one_step_on_data 返回 self.train_step(data) 文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py”，行57、在train_step y_pred = self(x,training=True)文件“/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py”中，第 122 行，在 error_handler 中从 None ValueError 引发 e.with_traceback(filtered_tb)：调用 Reshape.call() 时遇到异常。

无法将具有 301056 个元素的张量重塑为形状 [64,28,28,1]（50176 个元素），对于 &#39;{{node function_3_1/reshape_2_1/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](functioning_3_1/embedding_1_1/ GatherV2, function_3_1/reshape_2_1/Reshape/shape)&#39;，输入形状：[64,6,784], [4]，输入张量计算为部分形状：input[1] = [64,28,28,1]。

Reshape.call() 收到的参数：inputs=tf.Tensor(shape=(64, 6, 784), dtype=float32)

我很困惑，因为手动打印时 X、标签和 y 似乎具有正确的形状 X: (64, 28, 28, 1), labels: (64, 6), y: (64, 1) 但 train_on_batch() 似乎得到了不同的形状 shape=(64, 6, 784)。]]></description>
      <guid>https://stackoverflow.com/questions/78273628/tensor-flow-gets-a-different-shape-than-expected-when-using-train-on-batch</guid>
      <pubDate>Thu, 04 Apr 2024 11:46:43 GMT</pubDate>
    </item>
    <item>
      <title>为什么 scikit-learn SVC 的表现很差，甚至比 Logistic Regression 还差？</title>
      <link>https://stackoverflow.com/questions/78273085/why-does-the-scikit-learn-svc-performs-bad-and-even-worse-than-logistic-regressi</link>
      <description><![CDATA[我目前面临 SVC 的挑战性问题在多标签分类上下文中实现 scikit-learn。尽管严格尝试对模型进行微调，但性能仍然低得令人困惑。
一个数据实例由 98 个数字特征和 70 个单独的二进制标签（使用 MultiLabelBinarizer 编码的分类标签）（多标签数据）组成。
问题：

性能结果：SVC 在验证和测试数据集上的性能均低于标准，F1 分数（平均值 =“宏观”）为 0.3735，F1 分数（平均值 =验证数据的“加权”)为0.5677。形成鲜明对比的是，随机森林/决策树模型的 F1 分数高于 93%，甚至 Logistic 回归模型的 F1 分数也达到 77% 和 82%（宏观/加权），这对我来说毫无意义理论上的观点，因为 svc 实际上应该找到最好的类边界，因此比 LR 更好，因此也应该更好。

数据不平衡：我的数据集在标签分布方面存在严重的类别不平衡。这种不平衡是否会导致 SVC 表现不佳？我认为支持向量机对异常值更稳健，但令我困惑的是其他模型处理数据的效果很好。

get_config(&#39;random_state&#39;) 设置全局随机状态值


这是我到目前为止所做的事情：

数据缩放：在训练之前，我使用 scikit-learn 的 MinMaxScaler 缩放数据，确保所有特征都在 0 到 1 之间标准化。

数据拆分：数据集已拆分为训练集、验证集和测试集，以确保对模型性能进行稳健评估，例如


# 分为训练数据 (70%) 和剩余数据 (30%)
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=get_config(&#39;random_state&#39;), stratify=y)

# 将剩余部分细分为验证数据 (20%) 和测试数据 (10%)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=get_config(&#39;random_state&#39;), stratify=y_temp)



模型和超参数优化：我利用了 scikit-learn 的 SVC，并通过权重和网格搜索使用随机和网格搜索来执行超参数优化。偏差（WandB）以找到最佳可能的组合。以下是我尝试过的超参数：


C：[0.01、0.1、1、10、50、100、500]
内核：[线性、poly、rbf、sigmoid]
学位：[2,3,4,5]
伽玛：[0.0001, 0.001, 0.01, 0.1, 1, &#39;缩放&#39;, &#39;自动&#39;]
Class_weight：[无，“平衡”]
Max_iter：[100、200、300、500、700、1000、2500、5000、8000、-1]
概率：[对、错]


分类器包装：考虑到任务的多标签性质，我将 SVC 包装在 scikit-learn 的两个不同的集成包装器模型中（SVC 和 LR 本身不支持多标签分类），即 ClassifierChain 和 MultiOutputClassifier。以下是用于包装的代码片段：

# 创建 scikit-learn SVC 的实例
从 sklearn.svm 导入 SVC
从 sklearn.multioutput 导入 MultiOutputClassifier
从 sklearn.multioutput 导入 ClassifierChain

base_svc = SVC(C=C, kernel=kernel, Degree= Degree, gamma=gamma, coef0=coef0, 收缩=收缩,probability=概率, tol=tol, cache_size=cache_size, max_iter=max_iter,
Decision_function_shape=decision_function_shape，break_ties=break_ties，
random_state=random_state, class_weight=class_weight)

# 用 ClassifierChain 包装 SVC
self.clf = ClassifierChain(base_estimator=base_svc, random_state=get_config(&#39;random_state&#39;))

# 用 MultiOutputClassifier 包装 SVC
self.clf = MultiOutputClassifier(估计器=base_svc, n_jobs=-1)


我创建了两个相同的 SVC 模型，每个模型都有不同的包装器用于测试。令人惊讶的是，这两种模型的表现同样不尽如人意。
我将非常感谢社区的任何见解或帮助。 SVC 是否存在本质上不适合此特定任务的地方，或者我的实现或（hp）配置中是否存在潜在的疏忽，可能导致这种意外的糟糕性能？
提前谢谢您！]]></description>
      <guid>https://stackoverflow.com/questions/78273085/why-does-the-scikit-learn-svc-performs-bad-and-even-worse-than-logistic-regressi</guid>
      <pubDate>Thu, 04 Apr 2024 10:03:24 GMT</pubDate>
    </item>
    <item>
      <title>我在测试模型时遇到未知层错误</title>
      <link>https://stackoverflow.com/questions/78268793/i-am-getting-unknown-layer-error-while-testing-a-model</link>
      <description><![CDATA[错误是这样的：
我tensorflow/core/util/port.cc:113] oneDNN 自定义操作已开启。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 TF_ENABLE_ONEDNN_OPTS=0。
2024-04-03 20:51:40.389067：我tensorflow/core/util/port.cc:113] oneDNN 自定义操作已开启。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 TF_ENABLE_ONEDNN_OPTS=0。
警告：tensorflow：来自 C:\Users\DELL\AppData\Local\Programs\Python\Python312\Lib\site-packages\tf_keras\src\losses.py:2976：名称 tf.losses.sparse_softmax_cross_entropy 已弃用。请改用 tf.compat.v1.losses.sparse_softmax_cross_entropy。
2024-04-03 20:51:47.709342: I tensorflow/core/platform/cpu_feature_guard.cc:210] 此 TensorFlow 二进制文件经过优化，可以在性能关键型操作中使用可用的 CPU 指令。
要启用以下指令：AVX2 AVX512F AVX512_VNNI FMA，在其他操作中，使用适当的编译器标志重建 TensorFlow。
回溯（最近一次调用最后一次）：
文件“D:\image title\testing_caption_generator.py”，第 68 行，位于
模型 = 加载模型（
ValueError：未知层：“NotEqual”。请确保您使用的是 keras.utils.custom_object_scope 并且该对象包含在范围内。请参阅https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object 详细信息。
这是我的代码：
从 PIL 导入图像
将 matplotlib.pyplot 导入为 plt
导入argparse
进口泡菜
从 keras.models 导入 load_model
从 keras.layers 导入 Lambda
从 keras.applications.xception 导入 Xception
从 keras.preprocessing.sequence 导入 pad_sequences
从tensorflow.keras.preprocessing.text导入Tokenizer
from pickle import load # 从 pickle 模块导入 load 函数
导入tensorflow_hub作为集线器

# 定义自定义层
def NotEqual(x, y):
    将张量流导入为 tf
    返回 tf.math.not_equal(x, y)

# 定义用于提取特征、生成描述和其他必要实用程序的函数
def extract_features(文件名, 模型):
    尝试：
        图像 = Image.open(文件名)
    除了：
        print(&quot;错误：无法打开图像！请确保图像路径和扩展名正确&quot;)
    图像 = image.resize((299,299))
    图像 = np.array(图像)
    如果图像.shape[2] == 4:
        图像 = 图像[...,:3]
    图像 = np.expand_dims(图像, 轴=0)
    图像=图像/127.5
    图像 = 图像 - 1.0
    特征 = model.predict(图像)
    返回功能

def word_for_id(整数, 分词器):
    对于单词，在 tokenizer.word_index.items() 中索引：
        如果索引==整数：
            返回词
    返回无

defgenerate_desc（模型，分词器，照片，max_length）：
    in_text = &#39;开始&#39;
    对于范围内的 i（最大长度）：
        序列 = tokenizer.texts_to_sequences([in_text])[0]
        序列 = pad_sequences([序列], maxlen=max_length)
        pred = model.predict([照片, 序列], verbose=0)
        pred = np.argmax(pred)
        word = word_for_id(pred, 分词器)
        如果单词为“无”：
            休息
        in_text += &#39; &#39; + 单词
        如果单词==&#39;结束&#39;：
            休息
    返回 in_text

ap = argparse.ArgumentParser()
ap.add_argument(&#39;-i&#39;, &#39;--image&#39;, required=True, help=“图像路径”)
args = vars(ap.parse_args())
img_path = args[&#39;图像&#39;]




#path = &#39;Flicker8k_Dataset/111537222_07e56d5a30.jpg&#39;
最大长度 = 32
tokenizer = pickle.load(open(&quot;tokenizer.p&quot;,&quot;rb&quot;))
路径=&#39;模型/model_9.h5&#39;
模型 = 加载模型（
       （小路），
       custom_objects={&#39;KerasLayer&#39;:hub.KerasLayer}
）
xception_model = Xception(include_top=False, pooling=“avg”)

照片 = extract_features(img_path, xception_model)
img = Image.open(img_path)

描述 =generate_desc(模型、分词器、照片、max_length)
打印(“\n\n”)
打印（描述）
plt.imshow(img)```
]]></description>
      <guid>https://stackoverflow.com/questions/78268793/i-am-getting-unknown-layer-error-while-testing-a-model</guid>
      <pubDate>Wed, 03 Apr 2024 15:32:23 GMT</pubDate>
    </item>
    <item>
      <title>在 Azure Auto ML 上运行模型时遇到奇怪的错误</title>
      <link>https://stackoverflow.com/questions/78237575/strange-error-encountered-when-running-a-model-on-azure-auto-ml</link>
      <description><![CDATA[我已经在 Azure Auto ML 上研究分类器几天了，当我尝试禁用一些不需要的变量时，遇到了以下错误。
我以前从未遇到过此类错误。即使在仅使用我感兴趣的变量创建新数据集之后，错误仍然存​​在。我需要帮助来解决这个问题。谢谢
您在 Auto ML 上的数据集上使用的唯一 SQL 代码是 SELECT * FROM my_table，它可以工作，因为我可以在 Azure ML studio 上看到数据。另外我昨天才开始出现这个错误，我不知道为什么。
获取数据时遇到错误。
错误代码：ScriptExecution.Database.Unexpected
本机错误：数据流访问错误：ExecutionError(DatabaseError(Unknown(&quot;SQLError(Server(TokenError { code: 103010，state: 1，class: 16，message: \&quot;解析错误位于行：1，列：22：不正确&#39;stmt&#39; 附近的语法。\&quot;，服务器：\&quot;data-platform-sql-data-warehouse-server\&quot;，过程：\&quot;\&quot;，行：1 }))&quot;, Some( SQLError(Server(TokenError { 代码：103010，状态：1，类：16，消息：“行：1，列：22 处的解析错误：&#39;stmt&#39; 附近的语法不正确。”，服务器：“数据平台-sql-data-warehouse-server”，过程：“”，行：1 }))))))
    VisitError(ExecutionError(DatabaseError(Unknown(&quot;SQLError(Server(TokenError { code: 103010，state: 1，class: 16，message: \&quot;第 1 行解析错误，第 22 列：“stmt”附近语法不正确) .\&quot;，服务器：\&quot;data-platform-sql-data-warehouse-server\&quot;，过程：\&quot;\&quot;，行：1 }))&quot;，Some(SQLError(Server(TokenError) { 代码：103010，状态：1，类：16，消息：“第 1 行解析错误，第 22 列：&#39;stmt&#39; 附近的语法不正确。”，服务器：“data-platform-sql-data-仓库服务器”，过程：“”，行：1 })))))))
=&gt;失败并执行错误：执行数据库查询时发生错误。
    ExecutionError(DatabaseError(Unknown(&quot;SQLError(Server(TokenError { code: 103010，state: 1，class: 16，message: \&quot;行解析错误：1，列：22：&#39;stmt&#39;附近的语法不正确。\ ”，服务器：\“data-platform-sql-data-warehouse-server\”，过程：\“\”，行：1 }))”，Some(SQLError(Server(TokenError { code ：103010，状态：1，类：16，消息：“行：1，列：22处解析错误：‘stmt’附近的语法不正确。”，服务器：“data-platform-sql-data-warehouse-服务器”，过程：“”，行：1 }))))))
错误消息：数据库执行失败，并显示“SQLError(Server(TokenError { code: 103010，state: 1，class: 16，message: \”解析错误位于行：1，列：22：“stmt”附近的语法不正确。 \&quot;，服务器：\&quot;data-platform-sql-data-warehouse-server\&quot;，过程：\&quot;\&quot;，行：1 }))&quot;。 “Ok(SQLError(Server(TokenError { code: 103010，state: 1，class: 16，message: \”解析错误位于行：1，列：22：“stmt”附近的语法不正确。\”，server : \&quot;data-platform-sql-data-warehouse-server\&quot;，过程：\&quot;\&quot;，行：1 })))&quot;| session_id=af8ac40c-2ffe-410f-8ecb-70e45405ef78

谢谢
我刚刚关闭了模型不需要的一些变量。因此，我只需创建该数据集的新版本，其中变量较少，我可以将其用于新版本的模型，这是我过去几周一直在做的事情。
我创建了一个新数据集，仅包含我需要的变量，但出现了相同的错误。现在，无论我做什么，我似乎总是遇到同样的错误。]]></description>
      <guid>https://stackoverflow.com/questions/78237575/strange-error-encountered-when-running-a-model-on-azure-auto-ml</guid>
      <pubDate>Thu, 28 Mar 2024 10:44:51 GMT</pubDate>
    </item>
    <item>
      <title>微调 GPT2 - 注意掩码和 pad token id 错误</title>
      <link>https://stackoverflow.com/questions/74682597/fine-tuning-gpt2-attention-mask-and-pad-token-id-errors</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/74682597/fine-tuning-gpt2-attention-mask-and-pad-token-id-errors</guid>
      <pubDate>Mon, 05 Dec 2022 01:57:10 GMT</pubDate>
    </item>
    </channel>
</rss>