<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 07 May 2024 15:13:47 GMT</lastBuildDate>
    <item>
      <title>如何在保持梯度的同时提取“MaskedTensor”的值？</title>
      <link>https://stackoverflow.com/questions/78443158/how-to-extract-the-value-of-a-maskedtensor-while-maintaining-the-gradient</link>
      <description><![CDATA[我有一个 MaskedTensor（参见文档）
&lt;前&gt;&lt;代码&gt;打印（结果）
&gt;&gt;&gt;&gt;&gt;
掩蔽张量(
  [
    [ 1.7866, 2.5468, 1.6330],
    [2.2041、2.5388、2.3315]
  ]
）

我正在尝试用这个张量计算损失
loss = 5 - torch.sum(result_summed)

但我收到 TypeError: unsupported operand type(s) for -: &#39;int&#39; and &#39;MaskedTensor&#39;
问题似乎是，即使我调用 torch.sum(result_summed) 我也会得到一个“MaskedTensor”对象：
print(torch.sum(result_summed))
&gt;&gt;&gt;&gt;&gt;
掩码张量（13.0409，真）

如何提取值（此处为13.0409）来计算损失，同时保持梯度？]]></description>
      <guid>https://stackoverflow.com/questions/78443158/how-to-extract-the-value-of-a-maskedtensor-while-maintaining-the-gradient</guid>
      <pubDate>Tue, 07 May 2024 14:18:58 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型提示重复数据删除</title>
      <link>https://stackoverflow.com/questions/78443093/large-language-model-prompt-for-deduping-data</link>
      <description><![CDATA[我正在尝试使用 LLM 提示对以下数据进行重复数据删除
 产品线：DTV，销售渠道：间接，总增加量 =51
    产品线：BYOD，销售渠道：ONLINE2，总添加量=100
    产品线：BYOD，销售渠道：ONLINE1，总添加量=200
    产品线：BYOD，销售渠道：ONLINE3，总增加量=400
    产品线：BYOD，销售渠道：ONLINE4，总添加量=500
    产品线：BYOD，销售渠道：空，总添加量=300

重复数据删除标准：
1.识别所有Sales Channel=null记录并
看看它们是否重复
2.如果是相同的产品，则它们是重复的
任何其他销售渠道的总增加额与总增加额等于
空记录总添加量。
================================================== =======================
示例：
对于  产品线：BYOD，销售渠道：null，总添加数：300，
产品线：BYOD，销售渠道：空，总增加量：300 =

产品线：BYOD，销售渠道：ONLINE2，总添加量=100
+
产品线：BYOD，销售渠道：ONLINE1，总添加量=200

，其中 300==300
所以我需要将销售渠道：null，总添加= 300识别为重复
如何针对此类问题有效创建LLM提示？
我还需要从类似的数据集中识别趋势，有效的方法是什么？
我计划使用 Mistral/DBRX 模型。]]></description>
      <guid>https://stackoverflow.com/questions/78443093/large-language-model-prompt-for-deduping-data</guid>
      <pubDate>Tue, 07 May 2024 14:09:17 GMT</pubDate>
    </item>
    <item>
      <title>最适合实时季节性数据峰值检测的机器学习模型或统计指标是什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78442853/what-are-the-most-suited-ml-models-or-statistical-indicators-for-peak-detection</link>
      <description><![CDATA[我目前正在尝试创建一种实时功耗数据分析算法。目标很简单：实时检测建筑物中的功耗峰值/尖峰，并采取必要的措施来吸收峰值。
数据以功率值（kW）流的形式出现，每 10 分钟采样一次。我有一年多的功耗数据集。
数据非常具有季节性，建筑物的功耗因天气、人流量、假期等因素而有很大差异：许多未知参数。
我可以将功率峰值定义为相对值的快速激增，与建筑物的季节性趋势无关，持续时间不超过一小时（例如：超过可视为常态的异常）。
这个定义非常相对。在图表上，很容易区分峰值。有了数学规则，情况就变得更加复杂，尤其是考虑单个阈值或标准差时。
这是两天的数据图表。蓝色表示功耗。红色表示手动解释的峰值。
如您所见，定义峰值比定义常态或“季节性”更难。一月份的峰值与七月份的峰值不同。它们的值和来源都不一样。
这使得很难用标准差、平均值或阈值等基本工具进行识别。
有几种检测峰值的方法，基于统计和机器学习。我尝试了这两种方法，但有点迷茫。
第一种方法是我所说的统计方法：比较一个样本的标准差（z 分数）、高于平均值的阈值或指数平均值。 （例如：此处）

此处的问题是不可避免地引入了滞后，这使得模型对快速但季节性的功耗增长以及对“噪音”的敏感性以及所述峰值对平均值的不良影响。基本上，它很难处理建筑物的潜在行为。
机器学习对于处理趋势很有用。这让我想到了异常检测模型。乍一看，它们可以学习什么是正常行为并检测异常值 + 它们大多是无人监督的，当现实生活中对异常值的定义不明确时，这是一件好事。
自然而然，我试了一下。我使用了 PySad 库，并开始按照此 指南 试验模型。到目前为止，移动窗口上的 IForest 模型似乎在 AUROC 分数接近 1 时效果最佳。（如果值低于样本平均值，我会通过使异常分数为零来消除凹坑）。
我还尝试了 LOFP 和 KNNCAD，但结果很糟糕。
这里的问题是，模型的准确度会根据数据集而有很大差异，在不太明显的峰值上会低至 (AUROC)0,65。
我现在的问题是：您是否对适合实时准确检测峰值的 ML 模型或统计方法有任何见解或想法？（Autoencder、CWT、LOF、SVM？）或者您知道更好的方法吗？
我原本计划结合多种方法，但我觉得将方法准确性的随机性放在一起不会给我带来更好的结果。
我还必须考虑到调整 SARIMA 等高级模型的超参数将是不可能的，因为使用该算法的人不是 ML 专家或数据分析师。
这个项目是我计算机硕士学位的一部分。
谢谢，]]></description>
      <guid>https://stackoverflow.com/questions/78442853/what-are-the-most-suited-ml-models-or-statistical-indicators-for-peak-detection</guid>
      <pubDate>Tue, 07 May 2024 13:26:16 GMT</pubDate>
    </item>
    <item>
      <title>如何在 python 中为一个奇一问题创建 SPoSE 模型？</title>
      <link>https://stackoverflow.com/questions/78442722/how-can-i-create-s-spose-model-in-python-for-one-odd-one-out-problem</link>
      <description><![CDATA[我需要了解如何为三元组问题创建稀疏正相似嵌入，但我不知道如何开始
我正在考虑开始从它们创建嵌入，然后计算它们之间的距离，这是正确的方法吗？谁能建议一个代码]]></description>
      <guid>https://stackoverflow.com/questions/78442722/how-can-i-create-s-spose-model-in-python-for-one-odd-one-out-problem</guid>
      <pubDate>Tue, 07 May 2024 13:03:28 GMT</pubDate>
    </item>
    <item>
      <title>由于导入，在 Python 程序中使用 Bash 脚本运行 Python 代码时延迟较高</title>
      <link>https://stackoverflow.com/questions/78442377/high-latency-while-running-python-code-using-bash-script-in-a-python-program-due</link>
      <description><![CDATA[我有一个 python 应用程序，它使用子进程来运行 bash 脚本。 bash 脚本依次运行一个 python 文件，其中包含一些导入（视网膜面部、深层面部库）。该应用程序需要花费大量时间来运行，因为每次运行子进程时，都需要 20-30 秒来加载/导入视网膜/深脸模块。有没有办法可以加快速度？
注意：更改设置是不可能的，即我无法直接从原始 python 代码调用 python 代码。
我不确定如何解决这个问题，感谢任何帮助。谢谢。
我尝试使用系统模块的缓存版本，但这不起作用。虽然，我不确定如何正确使用它。]]></description>
      <guid>https://stackoverflow.com/questions/78442377/high-latency-while-running-python-code-using-bash-script-in-a-python-program-due</guid>
      <pubDate>Tue, 07 May 2024 12:09:10 GMT</pubDate>
    </item>
    <item>
      <title>模型预测的各种组合产生相似的地面事实</title>
      <link>https://stackoverflow.com/questions/78442079/various-combination-of-model-predictions-yields-to-similar-ground-truth</link>
      <description><![CDATA[我有一个模型（3DUnet，回归问题）可以预测值 PD 和 T1，其中 PD 和 T1 是基于输入的 qMRI 输出。根据这些预测，我使用以下公式计算 T1_Weighted_image：Weighted_images = PD (1 - exp(-1 / (T1 + epsilon)))*，其中 epsilon 很小值以防止被零除和 T1=&gt;0 。在训练期间，我用于损失计算的基本事实是 T1_Weighted_groundtruth，但我也有 PD 和 T1 的基本事实值，尽管它们不直接用于损失计算。它们用于确保 PD 和 T1 预测值的正确性。损失是使用 T1_Weighted_predict 和 T1_Weighted_groundtruth 之间的损失函数计算的。
但是，存在各种 PD 或 T1 组合，可以为 T1_Weighted 产生类似的结果。例如，我的模型可能预测 PD 的非常低的值（例如在 CSF 中作为一个明显的例子），而不是预测 T1 的高值（这是正确的答案）。有没有一种方法可以迫使我的模型预测正确的值，或者至少预测（任何）可能的组合？]]></description>
      <guid>https://stackoverflow.com/questions/78442079/various-combination-of-model-predictions-yields-to-similar-ground-truth</guid>
      <pubDate>Tue, 07 May 2024 11:19:19 GMT</pubDate>
    </item>
    <item>
      <title>如何训练我的图像识别模型，使其像奖励惩罚系统一样工作，让我可以分辨出它无法识别的人是谁？</title>
      <link>https://stackoverflow.com/questions/78441996/how-do-i-train-my-image-recognition-model-to-work-like-a-reward-punishment-syste</link>
      <description><![CDATA[我正在研究制作考勤系统的方法，教授点击几张照片（2到3张）
并上传到应用程序，大约 80 名学生会自动出勤。我的训练数据有限，这是我们需要应对的最大缺点和主要问题。我制作了一个用于训练和标记出勤率的基本模型。
我如何训练它像奖励惩罚系统一样工作，我可以手动告诉它它无法识别的人是谁，以便它在途中学习。]]></description>
      <guid>https://stackoverflow.com/questions/78441996/how-do-i-train-my-image-recognition-model-to-work-like-a-reward-punishment-syste</guid>
      <pubDate>Tue, 07 May 2024 11:03:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 3.10.0 在 PySpark 中加载 LightGBM 库时出错</title>
      <link>https://stackoverflow.com/questions/78441794/error-loading-lightgbm-library-in-pyspark-with-python-3-10-0</link>
      <description><![CDATA[描述：
在 Python 3.10.0 中使用 PySpark 时，我遇到了 LightGBM 回归器和分类器的问题。
环境：
PySpark 版本：3.2.1
Python版本：3.10.0
Py4j版本：0.10.9.5
Spark jar 包：com.microsoft.azure:synapseml_2.12:0.11.0
错误消息：
java.lang.UnsatisfiedLinkError：无法加载库：/var/folders/dz/mc23060n2kq52djyhcxl9kmh0000gp/T/mml-natives17452036633252549823/lib_lightgbm.dylib
重现步骤：

在 Python 中将 LightGBM 回归器或分类器与 PySpark DataFrame 结合使用
3.10.0。
遇到上述错误。

所做的尝试：
我按照此处提供的解决方案进行操作，并对已安装的 libomp 进行了符号链接，但问题仍然存在。
详细的错误堆栈：
py4j.protocol.Py4JJavaError：调用 o5147.fit 时发生错误。
E：java.lang.UnsatisfiedLinkError：无法加载库：/var/folders/dz/mc23060n2kq52djyhcxl9kmh0000gp/T/mml-natives17452036633252549823/lib_lightgbm.dylib
E 位于 java.base/java.lang.ClassLoader.loadLibrary（来源未知）
E 位于 java.base/java.lang.Runtime.load0（来源未知）
E 位于 java.base/java.lang.System.load（来源未知）
E 位于 com.microsoft.azure.synapse.ml.core.env.NativeLoader.loadLibraryByName(NativeLoader.java:66)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMUtils$.initializeNativeLibrary(LightGBMUtils.scala:33)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMBase.train(LightGBMBase.scala:37)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMBase.train$(LightGBMBase.scala:36)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMRegressor.train(LightGBMRegressor.scala:39)
E 位于 com.microsoft.azure.synapse.ml.lightgbm.LightGBMRegressor.train(LightGBMRegressor.scala:39)
E at org.apache.spark.ml.Predictor.fit(Predictor.scala:151)
E at org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0（本机方法）
E 位于 java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke（来源未知）
E 位于 java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke（来源未知）
E 位于 java.base/java.lang.reflect.Method.invoke（来源未知）
E 位于 py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E 位于 py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
E 位于 py4j.Gateway.invoke(Gateway.java:282)
E 位于 py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E 位于 py4j.commands.CallCommand.execute(CallCommand.java:79)
E 在 py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
E 位于 py4j.ClientServerConnection.run(ClientServerConnection.java:106)
E 位于 java.base/java.lang.Thread.run（来源未知）
附加说明：

该问题似乎与在 Python 3.10.0 中加载 LightGBM 库有关
我已检查指定路径中是​​否存在 lib_lightgbm.dylib。
如果您能提供有关解决此问题的任何见解或建议，我们将不胜感激。
]]></description>
      <guid>https://stackoverflow.com/questions/78441794/error-loading-lightgbm-library-in-pyspark-with-python-3-10-0</guid>
      <pubDate>Tue, 07 May 2024 10:23:19 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow：INVALID_ARGUMENT：logits 和标签必须具有相同的第一维，得到 logits 形状 [32,2] 和标签形状 [64]</title>
      <link>https://stackoverflow.com/questions/78441296/tensorflow-invalid-argument-logits-and-labels-must-have-the-same-first-dimensi</link>
      <description><![CDATA[我正在使用张量流来测试数据集的准确性。 您可以找到我的完整代码以及数据集 此处
我还在学习机器学习，有很多事情让我头疼。例如，为什么在这种情况下，当我使用“sparse_categorical_crossentropy”编译模型时，
# 由于标签不是单热编码的，我们使用稀疏分类交叉熵损失
model.compile(loss=“sparse_categorical_crossentropy”,
              优化器=“sgd”，
              指标=[“准确度”])

有一个错误：
INVALID_ARGUMENT：logits 和标签必须具有相同的第一个维度，得到 logits 形状 [32,2] 和标签形状 [64]
但是，在花了几个小时查看代码并无可救药地修复每一行之后。终于成功了！通过改变loss=“categorical_crossentropy”
# 由于标签不是单热编码的，我们使用稀疏分类交叉熵损失
model.compile(loss=“categorical_crossentropy”,
              优化器=“sgd”，
              指标=[“准确度”])

我现在真的需要一个解释，我将非常感激！]]></description>
      <guid>https://stackoverflow.com/questions/78441296/tensorflow-invalid-argument-logits-and-labels-must-have-the-same-first-dimensi</guid>
      <pubDate>Tue, 07 May 2024 09:02:32 GMT</pubDate>
    </item>
    <item>
      <title>安装在谷歌驱动器上的图像数据集上的 DataGradient 如何使用它作为分类图像</title>
      <link>https://stackoverflow.com/questions/78441217/datagradient-on-image-dataset-mounted-on-google-drive-how-to-use-it-its-a-classi</link>
      <description><![CDATA[我将 MRI 大脑图像安装在 google 驱动器上，我想使用 DataGradient 图像 EDA，但我无法理解如何将图像加载到此工具，请帮忙。这些图像安装在 Google 驱动器上的一个文件夹中，该文件夹包含 2 个文件夹“Training”和“Testing”，每个文件夹包含 4 个具有肿瘤类型的子文件夹。
训练和测试的数据路径如下。
DATA_PATH =“/content/drive/MyDrive/MRI_Dataset”
train_dir = os.path.join(DATA_PATH, &#39;Training&#39;) 图像位于训练文件夹子文件夹中，例如 Training Glioma，它们没有注释，类别由子文件夹名称确定
test_dir = os.path.join(DATA_PATH, &#39;测试&#39;)
链接到我尝试使用的工具：https://github.com/Deci -AI/数据梯度
我将 MRI 大脑图像安装在 google 驱动器上，我想使用 DataGradient 图像 EDA，但我无法理解如何将图像加载到此工具，请帮忙。这些图像安装在 Google 驱动器上的一个文件夹中，该文件夹包含 2 个文件夹“Training”和“Testing”，每个文件夹包含 4 个具有肿瘤类型的子文件夹。
训练和测试的数据路径如下。
DATA_PATH =“/content/drive/MyDrive/MRI_Dataset”
train_dir = os.path.join(DATA_PATH, &#39;Training&#39;) 图像位于训练文件夹子文件夹中，例如 Training Glioma，它们没有注释，类别由子文件夹名称确定
test_dir = os.path.join(DATA_PATH, &#39;测试&#39;)
链接到我尝试使用的工具：https://github.com/Deci -AI/数据梯度
SS]]></description>
      <guid>https://stackoverflow.com/questions/78441217/datagradient-on-image-dataset-mounted-on-google-drive-how-to-use-it-its-a-classi</guid>
      <pubDate>Tue, 07 May 2024 08:47:58 GMT</pubDate>
    </item>
    <item>
      <title>如何训练我的图像识别模型，使其像奖励惩罚系统一样工作，在其中我教它不认识的人是谁？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78439584/how-do-i-train-my-image-recognition-model-to-work-like-a-reward-punishment-syste</link>
      <description><![CDATA[我正在研究制作考勤系统的方法，教授点击几张照片（2到3张）
并上传到应用程序，大约 80 名学生会自动出勤。我的训练数据有限，这是我们需要应对的最大缺点和主要问题。我制作了一个用于训练和标记出勤率的基本模型。
我如何训练它像奖励惩罚系统一样工作，我可以手动告诉它它无法识别的人是谁，以便它在途中学习。]]></description>
      <guid>https://stackoverflow.com/questions/78439584/how-do-i-train-my-image-recognition-model-to-work-like-a-reward-punishment-syste</guid>
      <pubDate>Tue, 07 May 2024 00:12:44 GMT</pubDate>
    </item>
    <item>
      <title>为我的 Npy 数据集定义 ML 模型时出现问题</title>
      <link>https://stackoverflow.com/questions/78435504/problem-in-defining-a-ml-model-for-my-npy-dataset</link>
      <description><![CDATA[我需要帮助为我的数据定义火炬模型。我尝试了各种方法，但似乎没有任何效果。与输入尺寸和形状相关的错误不断出现。我该如何解决这些问题？
将 numpy 导入为 np
进口火炬
将 torch.nn 导入为 nn
导入 torch.optim 作为 optim
从 torch.utils.data 导入 DataLoader，TensorDataset
导入 torch.nn.function 作为 f

# 从 .npy 文件加载数据
data = np.load(“其他py文件/project_files/data/train/data.npy”)
print(&quot;数据形状：&quot;, data.shape) # (401, 701, 255)

数据大小 = 数据.形状[0] * 数据.形状[1] * 数据.形状[2]
print(“数据大小：”, data_size) # 71680755

# 从 .npy 文件加载标签数据
labels = np.load(“其他py文件/project_files/data/train/label.npy”)
print(&quot;标签数据形状:&quot;, labels.shape) # (401, 701, 255)

# 将 numpy 数组转换为 PyTorch 张量
data_tensor = torch.Tensor(数据)
labels_tensor = torch.Tensor(标签)


类 MyModel(nn.Module):
    def __init__(自身):
        超级（MyModel，自我）.__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.fc_input_size = data_size
        self.fc = nn.Linear(self.fc_input_size, 2)

    def 前向（自身，x）：
        x = self.pool(f.relu(self.conv1(x)))
        x = self.pool(f.relu(self.conv2(x)))
        x = x.view(-1, self.fc_input_size)
        x = self.fc(x)
        返回x

模型 = MyModel()
打印（模型）

标准 = nn.CrossEntropyLoss()
优化器 = optim.Adam(model.parameters(), lr=0.001)

数据集 = TensorDataset(data_tensor, labels_tensor)
dataloader = DataLoader(数据集,batch_size=32,shuffle=True)

纪元数 = 10
对于范围内的纪元（num_epochs）：
    运行损失 = 0.0
    对于 i，enumerate(dataloader, 0) 中的数据：
        输入，标签=数据
        优化器.zero_grad()
        outputs = model(inputs.unsqueeze(1)) # 通道维度
        损失=标准（输出，标签）
        loss.backward()
        优化器.step()

        running_loss += loss.item()
        如果我％100==99：
            print(f&quot;[{epoch + 1}, {i + 1}] 损失: {running_loss / 100}&quot;)
            运行损失 = 0.0


使用 torch.no_grad()：
    Predictions = model(data_tensor.unsqueeze(1)) # 通道维度

控制台输出：
已连接到 pydev 调试器（版本 223.8836.43）
数据形状：(401, 701, 255)
数据大小：71680755
标签数据形状：(401, 701, 255)
我的模型（
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), 步幅=(1, 1), 填充=(1, 1))
  （池）：MaxPool2d（kernel_size = 2，stride = 2，padding = 0，dilation = 1，ceil_mode = False）
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), 步幅=(1, 1), 填充=(1, 1))
  （fc）：线性（in_features = 71680755，out_features = 2，偏差= True）
）

文件“C:\Users\PC1\PycharmProjects\Project1\newmodel2.py”，第 36 行，向前
    x = x.view(-1, self.fc_input_size)
运行时错误：形状“[-1, 71680755]”对于大小 22579200 的输入无效
python-BaseException
]]></description>
      <guid>https://stackoverflow.com/questions/78435504/problem-in-defining-a-ml-model-for-my-npy-dataset</guid>
      <pubDate>Mon, 06 May 2024 08:45:53 GMT</pubDate>
    </item>
    <item>
      <title>尝试训练 GAN 模型时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/78420289/getting-valueerror-while-trying-to-train-gan-model</link>
      <description><![CDATA[我正在尝试训练 GAN 模型来检测糖尿病视网膜病变图像，但它抛出错误。请帮忙。
图像数据集不为空我已尝试查看它
错误是：-
&lt;前&gt;&lt;代码&gt;纪元 1/50
回溯（最近一次调用最后一次）：
  文件“C:\Users\asus\OneDrive\Desktop\project\DR-GAN\TrainModel.py”，第 65 行，在  中
    分类器.fit（X，Y，batch_size = 32，epochs = 50）
  文件“C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
    从 None 引发 e.with_traceback(filtered_tb)
  文件“C:\Users\asus\AppData\Roaming\Python\Python312\site-packages\keras\src\backend\tensorflow\nn.py”，第 553 行，在 categorical_crossentropy 中
    引发值错误（
ValueError：参数“target”和“output”必须具有相同的形状。收到：target.shape=(无，3)，output.shape=(无，5)

火车模型文件的代码是：
将 numpy 导入为 np
导入imutils
导入系统
导入CV2
导入操作系统
从tensorflow.keras.utils导入到_categorical
从 keras.models 导入 model_from_json
从 keras.layers 导入 MaxPooling2D
从 keras.layers 导入密集、丢弃、激活、扁平化
从 keras.layers 导入 Convolution2D
从 keras.models 导入顺序

图片 = []
图像标签 = []
目录=&#39;数据集&#39;
文件列表 = os.listdir(目录)
索引 = 0
对于 list_of_files 中的文件：
    子文件 = os.listdir(目录+&#39;/&#39;+文件)
    对于子文件中的子项：
        路径=目录+&#39;/&#39;+文件+&#39;/&#39;+子
        img = cv2.imread(路径)
        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        如果 img 为 None：
          print(&#39;路径错误：&#39;, 路径)
        别的：
         img = cv2.resize(img, (32,32))
         im2arr = np.array(img)
         im2arr = im2arr.reshape(32,32,3)
         图像.append(im2arr)
         image_labels.append(文件)
    打印（文件）

X = np.asarray(图像)
Y = np.asarray(image_labels)
Y = to_categorical(Y)
img = X[20].reshape(32,32,3)
cv2.imshow(&#39;ff&#39;,cv2.resize(img,(250,250)))
cv2.waitKey(0)
print(&quot;形状 == &quot;+str(X.shape))
print(“形状==”+str(Y.shape))
打印（Y）
X = X.astype(&#39;float32&#39;)
X = X/255

np.save(“model/img_data.txt”,X)
np.save(“model/img_label.txt”,Y)

X = np.load(&#39;model/img_data.txt.npy&#39;)
Y = np.load(&#39;model/img_label.txt.npy&#39;)
打印（Y）
img = X[20].reshape(32,32,3)
cv2.imshow(&#39;ff&#39;,cv2.resize(img,(250,250)))
cv2.waitKey(0)

classifier = Sequential() #alexnet 迁移学习代码在这里
classifier.add(Convolution2D(32, 3, 3, input_shape = (32, 32, 3), 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D((2, 2) , padding=&#39;相同&#39;))
classifier.add(Convolution2D(32, 3, 3, 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D((2, 2) , padding=&#39;相同&#39;))
分类器.add(Flatten())
classifier.add（密集（单位= 128，激活=&#39;relu&#39;））
classifier.add(Dense(单位 = 5, 激活 = &#39;softmax&#39;))
classifier.compile（优化器=&#39;adam&#39;，损失=&#39;categorical_crossentropy&#39;，指标= [&#39;准确性&#39;]）
分类器.fit（X，Y，batch_size = 32，epochs = 50）

我尝试过更改尺寸，但它不起作用，我无法理解这是版本错误还是代码错误，因此为了解决同样的问题，请提供解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78420289/getting-valueerror-while-trying-to-train-gan-model</guid>
      <pubDate>Thu, 02 May 2024 15:39:54 GMT</pubDate>
    </item>
    <item>
      <title>如何基于掩码相乘矩阵并排除元素？</title>
      <link>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78404705/how-to-multiply-matrices-and-exclude-elements-based-on-masking</guid>
      <pubDate>Mon, 29 Apr 2024 19:07:12 GMT</pubDate>
    </item>
    <item>
      <title>用最少层数训练绝对函数的神经网络</title>
      <link>https://stackoverflow.com/questions/78311513/train-neural-network-for-absolute-function-with-minimum-layers</link>
      <description><![CDATA[我正在尝试训练神经网络来学习 y = |x|功能。我们知道，绝对函数有两条不同的线在零点处相互连接。所以我尝试使用以下顺序模型：
隐藏层：
2 致密层（激活relu）
输出层：
1 致密层
训练模型后，它只拟合函数的一半边。大多数时候是右手边，有时是左手边。一旦我在隐藏层中再添加 1 层，那么我就用 3 层代替 2 层，它就完全符合该功能了。谁能解释为什么当绝对函数只有一次切割时需要额外的一层？
这是代码：
将 numpy 导入为 np


X = np.linspace(-1000,1000,400)
np.random.shuffle(X)
Y = np.abs(X)

# 重塑数据以适应模型输入
X = X.reshape(-1, 1)
Y = Y.重塑(-1, 1)

将张量流导入为 tf
将张量流导入为 tf
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

# 构建模型
模型 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(2, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(1)
]）

# 编译模型
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;,metrics=[&#39;mae&#39;])
model.fit(X, Y, epochs=1000)
# 使用模型进行预测
Y_pred = model.predict(X)

# 绘制结果
plt.scatter(X, Y, color=&#39;blue&#39;, label=&#39;实际&#39;)
plt.scatter(X, Y_pred, color=&#39;red&#39;, label=&#39;预测&#39;)
plt.title(&#39;实际与预测&#39;)
plt.xlabel(&#39;X&#39;)
plt.ylabel(&#39;Y&#39;)
plt.图例()
plt.show()

2 个密集层的绘图：

3 个密集层的绘图：
]]></description>
      <guid>https://stackoverflow.com/questions/78311513/train-neural-network-for-absolute-function-with-minimum-layers</guid>
      <pubDate>Thu, 11 Apr 2024 15:34:01 GMT</pubDate>
    </item>
    </channel>
</rss>