<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 16 Jan 2025 12:32:04 GMT</lastBuildDate>
    <item>
      <title>如何测试这些在 CICIDS2017 数据上训练的模型是否能够检测到来自 suricata 日志的攻击？</title>
      <link>https://stackoverflow.com/questions/79361293/how-to-test-these-models-trained-on-cicids2017-data-will-they-be-able-to-detect</link>
      <description><![CDATA[我应该使用哪些函数来实现这一点？模型应该实时工作并预测攻击或威胁。他们将获得来自 suricata 的日志。
selected_features = [
# 基于流的特征
&#39;流持续时间&#39;、&#39;流字节/秒&#39;、&#39;流数据包/秒&#39;、
&#39;转发数据包的总长度&#39;、&#39;回程数据包的总长度&#39;、

# 时间特征
&#39;流 IAT 平均值&#39;、&#39;流 IAT 标准&#39;、&#39;流 IAT 最大值&#39;、&#39;流 IAT 最小值&#39;、
&#39;转发 IAT 总数&#39;、&#39;回程 IAT 总数&#39;、

# 数据包特征
&#39;转发数据包长度最大值&#39;、&#39;转发数据包长度最小值&#39;、
&#39;回程数据包长度最大值&#39;、&#39;回程数据包长度最小值&#39;、
&#39;数据包长度平均值&#39;、&#39;数据包长度标准值&#39;、&#39;数据包长度方差&#39;、

# TCP 标志
&#39;SYN 标志计数&#39;、&#39;FIN 标志计数&#39;、&#39;RST 标志计数&#39;、
&#39;PSH 标志计数&#39;、&#39;ACK 标志计数&#39;、&#39;URG 标志计数&#39;、

# 附加功能
&#39;总转发数据包数&#39;、&#39;总反向数据包数&#39;、
&#39;转发报头长度&#39;、&#39;反向报头长度&#39;、
&#39;活动平均值&#39;、&#39;活动标准&#39;、&#39;空闲平均值&#39;、
&#39;Init_Win_bytes_forward&#39;、&#39;Init_Win_bytes_backward&#39;
]

我尝试使用此功能，但没有用。]]></description>
      <guid>https://stackoverflow.com/questions/79361293/how-to-test-these-models-trained-on-cicids2017-data-will-they-be-able-to-detect</guid>
      <pubDate>Thu, 16 Jan 2025 10:57:37 GMT</pubDate>
    </item>
    <item>
      <title>代码无法在 AWS sagemaker 上成功运行</title>
      <link>https://stackoverflow.com/questions/79361126/code-not-running-sucessfully-on-aws-sagemaker</link>
      <description><![CDATA[当我运行应该创建训练作业的单元时，我没有在 sagemaker 笔记本上收到任何更新？我在 AWS Sagemaker 上运行以下代码，当我运行此代码时，它显示训练作业已创建并且没有进一步的更新。当我检查训练作业时，我可以看到状态已完成，但代码仍在笔记本上运行并且没有显示任何更新。有人可以帮我为什么会这样吗？
kmeans.fit(kmeans.record_set(scaled_data)) 
]]></description>
      <guid>https://stackoverflow.com/questions/79361126/code-not-running-sucessfully-on-aws-sagemaker</guid>
      <pubDate>Thu, 16 Jan 2025 10:15:35 GMT</pubDate>
    </item>
    <item>
      <title>有效覆盖 pytorch 数据集</title>
      <link>https://stackoverflow.com/questions/79360229/override-pytorch-dataset-efficiently</link>
      <description><![CDATA[我想继承 torch.utils.data.Dataset 类来加载我的自定义图像数据集，比如说用于分类任务。这是官方 pytorch 网站的示例，位于此 链接:
import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
self.img_labels = pd.read_csv(annotations_file)
self.img_dir = img_dir
self.transform = transform
self.target_transform = target_transform

def __len__(self):
return len(self.img_labels)

def __getitem__(self, idx):
img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
image = read_image(img_path)
label = self.img_labels.iloc[idx, 1]
if self.transform:
image = self.transform(image)
if self.target_transform:
label = self.target_transform(label)
return image, label

我注意到：

在 __getitem__ 中，我们将图像从磁盘读取到内存中。这意味着如果我们训练模型几个时期，我们会多次将同一幅图像重新读入内存。据我所知，这是一个代价高昂的操作
每次从磁盘读取图像时都会应用一次变换，在我看来，这几乎是一个多余的操作。

我理解，在非常大的数据集中，我们无法将数据完全放入内存中，因此我们别无选择，只能以这种方式读取数据（因为我们必须在一个时期内迭代所有数据），我想知道，如果我的所有数据都可以放入内存中，那么在 __init__ 函数中从磁盘读取所有数据不是更好的方法吗？
通过我在计算机视觉方面的一点经验，我注意到在 变换 中将图像裁剪成固定大小的图像非常常见。那么为什么我们不应该裁剪图像一次并将其存储在磁盘上的其他地方，而在整个训练过程中只读取裁剪后的图像呢？在我看来，这似乎是一种更有效的方法。
我理解，一些用于增强而不是规范化的转换最好应用于 __getitem__ 中，以便获得随机生成的数据而不是固定的数据。
你能为我澄清一下这个主题吗？
如果我缺少的是常识，请用正确的方法指导我找到代码库。]]></description>
      <guid>https://stackoverflow.com/questions/79360229/override-pytorch-dataset-efficiently</guid>
      <pubDate>Thu, 16 Jan 2025 02:38:21 GMT</pubDate>
    </item>
    <item>
      <title>创建算法的算法[关闭]</title>
      <link>https://stackoverflow.com/questions/79360176/algorithm-that-creates-algorithms</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79360176/algorithm-that-creates-algorithms</guid>
      <pubDate>Thu, 16 Jan 2025 01:58:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 yolov5 创建的模型执行脚本时出现图像大小调整问题 [关闭]</title>
      <link>https://stackoverflow.com/questions/79360063/image-resizing-issue-while-executing-script-with-a-model-created-using-yolov5</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79360063/image-resizing-issue-while-executing-script-with-a-model-created-using-yolov5</guid>
      <pubDate>Thu, 16 Jan 2025 00:27:41 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-learn 中 .fit() 方法的用例？</title>
      <link>https://stackoverflow.com/questions/79360038/use-cases-for-the-fit-method-in-scikit-learn</link>
      <description><![CDATA[是否存在 .fit() 比使用 .fit_transform() 更实用的情况/用例？例如，当标签编码时：
encoder = LabelEncoder()
title = data[&#39;title&#39;]
encoder.fit(title)
title_encoded =coder.transform(title)

vs
encoder = LabelEncoder()
title = data[&#39;title&#39;]
title_encoded =coder.fit_transform(title)
]]></description>
      <guid>https://stackoverflow.com/questions/79360038/use-cases-for-the-fit-method-in-scikit-learn</guid>
      <pubDate>Thu, 16 Jan 2025 00:04:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 OpenCV 库训练 ML 模型时出现问题</title>
      <link>https://stackoverflow.com/questions/79359797/problems-training-the-ml-model-using-the-opencv-library</link>
      <description><![CDATA[我想创建一个 ML 模型，用于识别 MNIST 数据集中的手写数字。ML 模型是使用 OpenCV 库用 C++ 编写的。在构建模型并训练数据后，我在输出层中得到的数据总是 Nan 或 Inf。
我尝试更改训练参数，但没有任何效果。我尝试处理一个较小的 100 个单位的数据集，在该数据集中，我设法在输出层中获取一些值，但其中大多数仍然是 Nan。我就是找不到原因。以下是 ML 模型的代码：
cv::Ptr&lt;cv::ml::ANN_MLP&gt; mlp = cv::ml::ANN_MLP::create();
mlp-&gt;setActivationFunction(cv::ml::ANN_MLP::SIGMOID_SYM, 1, 1);

int inputLayerSize = imagesData[0].total();
if (inputLayerSize &gt; std::numeric_limits&lt;int&gt;::max()) {
throw std::overflow_error(&quot;inputLayerSize 超出 int 的最大值&quot;);
}
size_t hiddenLayerSize = 100;
size_t outputLayerSize = 10;

cv::Mat layer = (cv::Mat_&lt;int&gt;(3, 1)&lt;&lt;inputLayerSize, hiddenLayerSize, outputLayerSize);

mlp-&gt;setLayerSizes(layers);

int numSamples = imagesData.size();

cv::Mat trainingData(numSamples, inputLayerSize, CV_32F);
cv::Mat labelData(numSamples, outputLayerSize, CV_32F);

for (int i = 0; i &lt; numSamples; i++) {

cv::Mat image = imagesData[i].reshape(1, 1);
image.convertTo(trainingData.row(i), CV_32F);

cv::Mat label = cv::Mat::zeros(1, outputLayerSize, CV_32F);
la​​bel.at&lt;float&gt;(0, labelsData[i]) = 1.0;
label.copyTo(labelData.row(i));

}

cv::TermCriteria termCrit(cv::TermCriteria::MAX_ITER + cv::TermCriteria::EPS, 10, 0.001);
mlp-&gt;setTermCriteria(termCrit);

mlp-&gt;setTrainMethod(cv::ml::ANN_MLP::BACKPROP, 0.001, 0.1);

mlp-&gt;train(trainingData, cv::ml::ROW_SAMPLE, labelData);

我还对像素数据进行了标准化。]]></description>
      <guid>https://stackoverflow.com/questions/79359797/problems-training-the-ml-model-using-the-opencv-library</guid>
      <pubDate>Wed, 15 Jan 2025 21:50:41 GMT</pubDate>
    </item>
    <item>
      <title>F1-score、IOU 和 Dice Score 的实现</title>
      <link>https://stackoverflow.com/questions/79359767/implementation-of-f1-score-iou-and-dice-score</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79359767/implementation-of-f1-score-iou-and-dice-score</guid>
      <pubDate>Wed, 15 Jan 2025 21:33:49 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯分类器代码错误。需要修复问题 7 和问题 8 中的运行时错误 [关闭]</title>
      <link>https://stackoverflow.com/questions/79359212/bayes-classifier-code-error-need-fix-for-a-runtime-error-in-problem-7-and-prob</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79359212/bayes-classifier-code-error-need-fix-for-a-runtime-error-in-problem-7-and-prob</guid>
      <pubDate>Wed, 15 Jan 2025 17:38:08 GMT</pubDate>
    </item>
    <item>
      <title>损失函数是否返回正确的值？</title>
      <link>https://stackoverflow.com/questions/79336024/is-the-loss-function-returns-the-correct-value</link>
      <description><![CDATA[我正在研究医学数据集的语义分割问题。我使用专注于正类像素的软骰子损失，具体实现如下：https://arxiv.org/abs/2209.06078 。这是我的代码：
def soft_dice_loss(y_true, y_pred):
epsilon = 1e-8 # 添加小 epsilon 以避免除以零

# 计算分子和分母 
numerator_dice_coef= 2 * tf.reduce_sum(y_true * y_pred, axis=(2, 3)) + epsilon

den_dice_coef = (tf.reduce_sum(y_true * y_true, axis=(2, 3))) + (tf.reduce_sum(y_pred * y_pred, axis=(2, 3))) + epsilon

# 批次中每幅图像的骰子系数
dice_coef = numerator_dice_coef / den_dice_coef

# 批次中的平均骰子系数
#mean_dice_coef = tf.reduce_mean(dice_coef)

# 每个样本所有通道的平均 Dice 系数
mean_dice_coef_per_sample = tf.reduce_mean(dice_coef, axis=1) # 形状：[B]

return 1 - mean_dice_coef_per_sample

我的问题是，我返回的值是否正确？我之所以问这个问题，是因为我很困惑，我是否应该将整个批次的平均损失值返回给 Keras，还是只返回一个大小为 batch_size 的数组，其中每个元素都是每个样本的损失值？我很困惑，因为我看到一些实现只返回一个值，而另一些实现则返回一个数组。]]></description>
      <guid>https://stackoverflow.com/questions/79336024/is-the-loss-function-returns-the-correct-value</guid>
      <pubDate>Tue, 07 Jan 2025 12:37:35 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 nltk 函数</title>
      <link>https://stackoverflow.com/questions/78862426/unable-to-use-nltk-functions</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78862426/unable-to-use-nltk-functions</guid>
      <pubDate>Mon, 12 Aug 2024 15:17:29 GMT</pubDate>
    </item>
    <item>
      <title>如何理解二元分类问题的Shapley值？</title>
      <link>https://stackoverflow.com/questions/66018154/how-to-understand-shapley-value-for-binary-classification-problem</link>
      <description><![CDATA[我对 shap python 包非常陌生。我想知道我应该如何解释二元分类问题的 shapley 值？这是我到目前为止所做的。
首先，我使用 lightGBM 模型来拟合我的数据。类似于
import shap
import lightgbm as lgb

params = {&#39;object&#39;:&#39;binary, 
...}
gbm = lgb.train(params, lgb_train, num_boost_round=300)
e = shap.TreeExplainer(gbm)
shap_values = e.shap_values(X)
shap.summary_plot(shap_values[0][:, interested_feature], X[interested_feature])

由于这是一个二元分类问题。shap_values 包含两个部分。我假设一个是针对类别 0，另一个是类别 1。如果我想知道一个特征的贡献。我必须绘制两个如下图所示的图形。
针对类别 0

针对类别 1

但是我应该如何进行更好的可视化呢？结果无法帮助我理解“cold_days 会增加输出成为 1 类还是 0 类的概率？”
使用相同的数据集，如果我使用 ANN，输出就是那样的。我认为 shapley 结果清楚地告诉我“cold_days”将积极增加结果成为 1 类的概率。
我感觉 LightGBM 输出有问题，但我不知道如何修复它。我怎样才能获得更清晰的类似于 ANN 模型的可视化效果？
#编辑
我怀疑我错误地使用了 lightGBM 来获得奇怪的结果。以下是原始代码
import lightgbm as lgb
import shap

lgb_train = lgb.Dataset(x_train, y_train, free_raw_data=False)
lgb_eval = lgb.Dataset(x_val, y_val, free_raw_data=False)
params = {
&#39;boosting_type&#39;: &#39;gbdt&#39;,
&#39;objective&#39;: &#39;binary&#39;,
&#39;metric&#39;: &#39;binary_logloss&#39;,
&#39;num_leaves&#39;: 70,
&#39;learning_rate&#39;: 0.005,
&#39;feature_fraction&#39;: 0.7,
&#39;bagging_fraction&#39;: 0.7,
&#39;bagging_freq&#39;: 10,
&#39;verbose&#39;: 0,
&#39;min_data_in_leaf&#39;: 30,
&#39;max_bin&#39;: 128,
&#39;max_depth&#39;: 12,
&#39;early_stopping_round&#39;: 20,
&#39;min_split_gain&#39;: 0.096,
&#39;min_child_weight&#39;: 6,
}

gbm = lgb.train(params,
lgb_train,
num_boost_round=300,
valid_sets=lgb_eval,
)
e = shap.TreeExplainer(gbm)
shap_values = e.shap_values(X)
shap.summary_plot(shap_values[0][:, interested_feature], X[interested_feature])
]]></description>
      <guid>https://stackoverflow.com/questions/66018154/how-to-understand-shapley-value-for-binary-classification-problem</guid>
      <pubDate>Tue, 02 Feb 2021 21:52:39 GMT</pubDate>
    </item>
    <item>
      <title>训练历史与验证历史非常相似可以吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/65628074/is-it-ok-to-have-the-training-history-very-similar-to-the-validation-history</link>
      <description><![CDATA[我训练了一个模型 50 个 epoch，按以下比例分割数据集：

X_train, Y_train = 70%
X_validation, Y_validation = 20%
X_test, Y_test = 10%

所有分割均使用 train_test_split(shuffle=True) keras 函数完成：
X = np.load(....)
Y = np.load(....)

# 在训练和验证上进行分割
N_validation = int(len(X) * 0.2)
X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=N_validation)

# 再次分割训练数据以获取测试数据
N_test = int(len(X_train) * 0.1)
X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=N_test)

这是历史图。
从历史中可以看出，验证准确率/损失与训练准确率/损失非常相似。有时验证损失甚至低于训练损失。
至于最后这句话，我在这里读到，这可能导致较高的 dropout 值。可能是这种情况，因为我有一个 rate=0.3 的 dropout 层。
我不明白这是否是个问题。
在测试集上测试模型，我的准确率为91%。]]></description>
      <guid>https://stackoverflow.com/questions/65628074/is-it-ok-to-have-the-training-history-very-similar-to-the-validation-history</guid>
      <pubDate>Fri, 08 Jan 2021 11:23:50 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：分类指标无法处理多类别和多标签指标目标的混合</title>
      <link>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</link>
      <description><![CDATA[我有一个多类标记文本分类问题，有 2000 个不同的标签。使用带有 Glove Embedding 的 LSTM 进行分类。

目标变量的标签编码器
带有 Embedd 层的 LSTM 层
错误度量是 F2 分数

LabelEncoded 目标变量：
le = LabelEncoder() 
le.fit(y)
train_y = le.transform(y_train)
test_y = le.transform(y_test)

带有 Glove Embeddings 的 LSTM 网络如下所示
np.random.seed(seed)
K.clear_session()
model = Sequential()
model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1],
weights=[embedding_matrix]))#,trainable=False
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(num_classes,activation=&#39;softmax&#39;))
model.compile(optimizer=&#39;rmsprop&#39;,loss=&#39;sparse_categorical_crossentropy&#39;)
print(model.summary())

我的错误指标是 F1 分数。我为错误度量构建了以下函数
class Metrics(Callback):
def on_train_begin(self, logs={}):
self.val_f1s = []
self.val_recalls = []
self.val_precisions = []

def on_epoch_end(self, epoch, logs={}):
val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()
val_targ = self.validation_data[1]
_val_f1 = f1_score(val_targ, val_predict)
_val_recall = recall_score(val_targ, val_predict)
_val_precision = precision_score(val_targ, val_predict)
self.val_f1s.append(_val_f1)
self.val_recalls.append(_val_recall)
self.val_precisions.append(_val_precision)
print(&quot;— val_f1: %f — val_precision: %f — val_recall %f&quot; % (_val_f1, _val_precision, _val_recall))
return

metrics = Metrics()

##模型拟合是
model.fit(X_train, train_y, validation_data=(X_test, test_y),epochs=10, batch_size=64, callbacks=[metrics])

第一个 epoch 后出现以下错误：
ValueError：分类指标无法处理多类和连续多输出目标的混合

在哪里我的代码有错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</guid>
      <pubDate>Fri, 07 Jun 2019 14:55:37 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归的随机梯度下降总是返回 Inf 的成本，并且权重向量永远不会接近</title>
      <link>https://stackoverflow.com/questions/26418178/stochastic-gradient-descent-for-logistic-regression-always-returns-a-cost-of-inf</link>
      <description><![CDATA[我正在尝试在 MATLAB 中实现逻辑回归求解器，并通过随机梯度下降找到权重。我遇到了一个问题，我的数据似乎产生了无限的成本，无论发生什么，它都不会下降……
这是我的梯度下降函数：
function weightVector = logisticWeightsByGradientDescentStochastic(trueClass,features)
%% 此函数尝试收敛到逻辑回归阶数为 1 的最佳权重集
%% 输入：
% trueClass - 训练数据的真实类值向量
% features
%% 输出：
% weightVector - 大小为 n+1 的向量（n 是特征数）
% 对应于收敛权重

%% 获取数据大小
dataSize = size(features);

%% 初始选择权重向量
weightVector = zeros(dataSize(2)+1, 1) %创建一个等于特征数量加 1 的零向量

%% 选择学习率
learningRate = 0.0001;

%% 初始成本
cost = logisticCost(weightVector, features, trueClass)

%% 随机梯度下降
costThresh = 0.05 %定义成本阈值

iterCount = 0;
while(cost &gt; costThresh)
for m=1:dataSize(1) %for all samples

%% test 语句
curFeatures = transpose([1.0 features(m,:)])

%% 计算 Sigmoid 预测值 
predictClass = assessSigmoid(weightVector , [1.0 features(m,:)] )

%% test 语句
truth = trueClass(m)

%% 计算所有特征的梯度
gradient = learningRate .* (trueClass(m) - predictClass) .* transpose([1.0 features(m,:)])

%% 通过从旧权重向量中减去梯度来更新权重向量
weightVector = weightVector - gradient 

%% 使用新权重向量重新评估 Cost
cost = logisticCost(weightVector, features, trueClass)

if(cost &lt; costThresh)
break
end
iterCount = iterCount + 1

结束 %for m
结束 %while cost &gt; 0.05

weightVector
iterCount
end

这是我的成本函数：
function cost = logisticCost(weightVector, features, trueClass)
%% 计算将 weightVector 应用于所有样本的总成本
%% 对于线性回归模型，根据
%% J(theta) = -(1/m) sum[ trueClass(log(predictedClass) + (1-trueClass)log(predictedClass)]
%% 输入：
% weightVector - n+1 个权重向量，其中 n 是特征数
% 加 1
% features - 特征矩阵
% trueClass - 训练数据的真实类别
%% 输出：
% cost - 总成本

dataSize = size(features); %获取数据大小

errorSum = 0.0; %存储错误总和
for m = 1:dataSize(1) %每行
predictedClass = assessEvaluateSigmoid(weightVector, [1.0 features(m,:)]); %评估 Sigmoid 来预测样本 m 的类别
if trueClass(m) == 1
errorSum = errorSum + log(predictedClass);
else
errorSum = errorSum + log(1 - predictClass);
end
end

cost = errorSum / (-1 .* dataSize(1)); % 乘以 -(1/m) 以获得成本
结束

这两者看起来都很好，我无法想象为什么我的成本函数总是会返回无穷大。
这是我的训练数据，其中第一列是类（1 或 0），接下来的七列是我要回归的特征。]]></description>
      <guid>https://stackoverflow.com/questions/26418178/stochastic-gradient-descent-for-logistic-regression-always-returns-a-cost-of-inf</guid>
      <pubDate>Fri, 17 Oct 2014 05:00:30 GMT</pubDate>
    </item>
    </channel>
</rss>