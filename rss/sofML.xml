<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 29 Mar 2024 12:25:32 GMT</lastBuildDate>
    <item>
      <title>使用上游-下游 ML 模型，上游是 Wav2Vec 2.0 转换器，下游是 CNN。模型的准确性趋于稳定，为什么？</title>
      <link>https://stackoverflow.com/questions/78243585/using-an-upstream-downstream-ml-model-with-the-upstream-being-wav2vec-2-0-trans</link>
      <description><![CDATA[尝试将 wav2vec 2.0 与 CNN 结合使用进行语音情感识别。已定义四个类别。所有音频都已根据 wav2vec 2.0 模型的需要进行了预处理、充分截断/填充和重新采样。这就是模型的定义方式：
class SimpleNN(nn.Module)：
    def __init__(自身，输入大小，隐藏大小，输出大小)：
        超级(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(输入大小，隐藏大小)
        self.fc2 = nn.Linear(隐藏大小, 输出大小)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)

    def 前向（自身，x）：
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        返回x

tokenizer = Wav2Vec2Tokenizer.from_pretrained(“facebook/wav2vec2-base-960h”)
模型 = Wav2Vec2Model.from_pretrained(“facebook/wav2vec2-base-960h”)

# 修改最后一层以匹配输出类的数量
label_mapping = {&#39;OAF_Fear&#39;：0，&#39;OAF_angry&#39;：1，&#39;OAF_happy&#39;：2，&#39;OAF_neutral&#39;：3}
num_classes = len(label_mapping)

model.lm_head = nn.Linear（in_features=model.config.hidden_​​size，out_features=num_classes，bias=True）

对于 model.parameters() 中的参数：
    param.requires_grad = False
对于 model.lm_head.parameters() 中的参数：
    param.requires_grad = True

input_size = 768 # 从预训练模型中提取的特征的大小
隐藏大小 = 256
output_size = num_classes # 情感类别的数量
学习率 = 0.001
纪元数 = 50
批量大小 = 32
root_dir =“/content/drive/MyDrive/BTP_hanan_dataset/Dataset/TESS”

类 FullModel(nn.Module):
    def __init__(self, wav2vec_model, simple_nn_model):
        超级（FullModel，自我）.__init__()
        self.wav2vec_model = wav2vec_model
        self.simple_nn_model = simple_nn_model

    def 前向（自身，x）：
        # 从预训练模型中获取隐藏状态
        隐藏状态 = self.wav2vec_model(x)[0]
        
        # 聚合隐藏状态（例如，通过平均或最大池化）
        aggregate_hidden_​​state = torch.mean(hidden_​​states, dim=1) # 示例：求平均值
        
        # 通过简单的神经网络
        输出= self.simple_nn_model（聚合隐藏状态）
        
        返回输出
simple_nn = SimpleNN(输入大小、隐藏大小、输出大小)

对于 simple_nn.parameters() 中的参数：
    param.requires_grad = True

# 将预训练模型和简单的神经网络组合成一个模型
full_model = FullModel(模型, simple_nn)

# 损失函数和优化器
标准 = nn.CrossEntropyLoss()
优化器 = optim.Adam(full_model.parameters(), lr=learning_rate)


预训练模型的最后一层已经训练完毕，其参数被传递给简单的 CNN。该模型的准确率停滞在 35%。
在两个不同的数据集上进行了尝试，但没有任何改进。提前停止在 7 - 10 个 epoch 后触发，耐心 = 5。我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78243585/using-an-upstream-downstream-ml-model-with-the-upstream-being-wav2vec-2-0-trans</guid>
      <pubDate>Fri, 29 Mar 2024 11:48:46 GMT</pubDate>
    </item>
    <item>
      <title>如何在 TensorFlow 中对多个类进行分类</title>
      <link>https://stackoverflow.com/questions/78243492/how-to-classify-multiple-classes-in-tensorflow</link>
      <description><![CDATA[我已经关注了 youtube 上的教程，该教程向我展示了如何对 2 个数据集进行分类（咳嗽，不是咳嗽），但现在我需要添加一个额外的类，即打喷嚏，因此需要训练 3 个类上（咳嗽，打喷嚏，其他），我不知道该怎么做。请帮忙！！！
在代码中，模型在 2 个类别（咳嗽、not_cough）上进行训练并且表现相当不错，但我无法让它在多个类别（例如咳嗽、打喷嚏、其他）上工作。
导入操作系统
从 matplotlib 导入 pyplot 作为 plt
将张量流导入为 tf
将tensorflow_io导入为tfio
从tensorflow.keras.models导入顺序，load_model
从tensorflow.keras.layers导入Conv2D、Dense、Flatten、MaxPool2D、Dropout、TimeDistributed、Reshape
从tensorflow.keras.optimizers.legacy导入Adam
从 keras 导入层
从 keras.utils 导入到_categorical

def load_wav_16k_mono(文件名):
    # 加载编码后的wav文件
    file_contents = tf.io.read_file(文件名)
    # 解码 wav（按通道的张量）
    wav，sample_rate = tf.audio.decode_wav（文件内容，desired_channels = 1）
    # 删除尾随轴
    wav = tf.squeeze(wav, 轴=-1)
    样本率 = tf.cast(样本率，dtype=tf.int64)
    # 从 44100Hz 到 16000Hz - 音频信号的幅度
    wav = tfio.audio.resample(wav,rate_in=sample_rate,rate_out=16000)
    返回波形

def 预处理（文件路径，标签）：
    wav = load_wav_16k_mono(文件路径)
    wav = wav[:8000]
    Zero_padding = tf.zeros([8000] - tf.shape(wav), dtype=tf.float32)
    wav = tf.concat([zero_padding, wav],0)
    
    频谱图 = tf.signal.stft(wav,frame_length=100,frame_step=20)
    频谱图 = tf.abs(频谱图)
    频谱图= tf.expand_dims（频谱图，轴= 2）
    返回频谱图、标签


def get_CNN(input_shape):
    模型=顺序（）
    model.add(Conv2D(16, (3,3), 激活=&#39;relu&#39;, input_shape=input_shape))
    model.add(Conv2D(16, (3,3), 激活=&#39;relu&#39;))
    model.add(MaxPool2D((2,2)))
    模型.add(压平())
    model.add（密集（128，激活=&#39;relu&#39;））
    model.add（密集（1，激活=&#39;softmax&#39;））
    
    model.compile(&#39;Adam&#39;, loss=&#39;BinaryCrossentropy&#39;, 指标=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision(),&#39;accuracy&#39;])
    model.summary() # 删除一些最大池层以减少参数
    返回模型
    

def main():
    POS_COUGH = “./data/咳嗽”
    NEG_COUGH =“./data/not_cough”
  
    #POS_SPEECH =“./数据/语音”

    pos_cough = tf.data.Dataset.list_files(POS_COUGH+&#39;\*.wav&#39;)
    neg_cough = tf.data.Dataset.list_files(NEG_COUGH+&#39;\*.wav&#39;)
    
    #pos_speech = tf.data.Dataset.list_files(POS_SPEECH +&#39;\*.wav&#39;)

    咳嗽标签 = tf.data.Dataset.from_tensor_slices(tf.ones(len(pos_cough)))
    
    not_cough_labels = tf.data.Dataset.from_tensor_slices(tf.ones(len(neg_cough)))
    
    # 添加标签并合并正负样本
    咳嗽 = tf.data.Dataset.zip((pos_cough, 咳嗽_标签))
    
    not_cough = tf.data.Dataset.zip((neg_cough, not_cough_labels))
   
    阴性= not_cough
    阳性=咳嗽
    # 连接两个相同的元素
    数据=正数.连接（负数）

    ### 2. 创建 Tensorflow 数据管道
    数据 = data.map(预处理)
    数据 = data.cache()
    数据 = data.shuffle(buffer_size=1000)
    数据 = 数据.batch(16)
    数据 = 数据.预取(8)
    
    ## 3. 将数据拆分为训练数据和测试数据
    火车 = data.take(int(len(数据) * 0.7))
    test = data.skip(int(len(data) * 0.7)).take(int(len(data) - len(data) * 0.7)) #test.as_numpy_iterator().next()

    输入形状频谱图 = (396, 65,1)
    模型 = get_CNN(input_shape_spectrogram)
    hist = model.fit(train, epochs=2,validation_data=test)
]]></description>
      <guid>https://stackoverflow.com/questions/78243492/how-to-classify-multiple-classes-in-tensorflow</guid>
      <pubDate>Fri, 29 Mar 2024 11:24:58 GMT</pubDate>
    </item>
    <item>
      <title>训练和测试分开，目标类别的每个名称和比例都出现在训练和测试中</title>
      <link>https://stackoverflow.com/questions/78242480/train-and-test-split-in-such-a-way-that-each-name-and-proportion-of-tartget-clas</link>
      <description><![CDATA[我正在尝试解决一个机器学习问题，如果一个人是否会交付订单。高度不平衡的数据集。这是我的数据集的一瞥
[{&#39;order_id&#39;: &#39;1bjhtj&#39;, &#39;送货员&#39;: &#39;约翰&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1aec&#39;, &#39;送货员&#39;: &#39;约翰&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1cgfd&#39;, &#39;送货员&#39;: &#39;约翰&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bceg&#39;, &#39;送货员&#39;: &#39;汤姆&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a2fg&#39;, &#39;送货员&#39;: &#39;汤姆&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1cbsf&#39;, &#39;送货员&#39;: &#39;汤姆&#39;, &#39;目标&#39;: 1},
 {&#39;order_id&#39;: &#39;1bc5&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a22&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bzc5&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1av22&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bsc5&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 1},
 {&#39;order_id&#39;: &#39;1a2t2&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bc5b&#39;, &#39;送货员&#39;: &#39;周杰伦&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a22a&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1c5bv&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;vb2er&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1bs5s&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a22n&#39;, &#39;送货员&#39;: &#39;玛丽&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;122a&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 1},
 {&#39;order_id&#39;: &#39;1cw5bv&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;vb=er&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1b5s&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 0},
 {&#39;order_id&#39;: &#39;1a2n&#39;, &#39;送货员&#39;: &#39;詹姆斯&#39;, &#39;目标&#39;: 1}]



这是我的桌子：
&lt;前&gt;&lt;代码&gt;|订单 ID |送货员 |目标|
|----------|--------------|--------|
| 1bjhtj |约翰 | 0 |
| 1aec |约翰 | 0 |
| 1cgfd |约翰 | 0 |
| 1bceg |汤姆| 0 |
| 1a2fg | 1a2fg汤姆| 0 |
| 1cbsf |汤姆| 1 |
| 1BC5 | 1BC5 |周杰伦 | 0 |
| 1a22 | 1a22周杰伦 | 0 |
| 1bzc5 | 1bzc5周杰伦 | 0 |
| 1av22 | 1av22周杰伦 | 0 |
| 1BSC5 |周杰伦 | 1 |
| 1a2t2 | 1a2t2 |周杰伦 | 0 |
| 1bc5b | 1bc5b |周杰伦 | 0 |
| 1a22a | 1a22a玛丽| 0 |
| 1c5bv | 1c5bv玛丽| 0 |
| VB2er |玛丽| 0 |
| 1bs5s | 1bs5s |玛丽| 0 |
| 1a22n | 1a22n |玛丽| 0 |
| 122a | 122a詹姆斯 | 1 |
| 1cw5bv | 1cw5bv詹姆斯 | 0 |
| vb=呃|詹姆斯 | 0 |
| 1b5s | 1b5s |詹姆斯 | 0 |
| 1a2n |詹姆斯 | 1 |


我希望我的机器学习模型能够理解每个人的属性并预测这两个
案例：
将传送“0”和
不会传送“1”
我想以这样的方式分割我的训练和测试，使其保留几行名称和几行目标类，以便它学习所有模式。
到目前为止我已经用过这个
X = df.drop(columns = “目标”)
y = df.目标
X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.7,stratify=y)

它确实给了我每个送货员的输出，但它错过了我们可以以“1”和“1”的方式分割“James”的部分。火车上将会有另一个“1”将在测试中。
谁能帮助我以不同的方式解决这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/78242480/train-and-test-split-in-such-a-way-that-each-name-and-proportion-of-tartget-clas</guid>
      <pubDate>Fri, 29 Mar 2024 07:21:32 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：无法从“diffusers.utils”导入名称“DIFFUSERS_SLOW_IMPORT”</title>
      <link>https://stackoverflow.com/questions/78242432/importerror-cannot-import-name-diffusers-slow-import-from-diffusers-utils</link>
      <description><![CDATA[当使用像这样的扩散器时
从扩散器导入 AutoencoderKL、DDPMScheduler、UNet2DConditionModel
出现以下错误：
导入错误：无法从“diffusers.utils”导入名称“DIFFUSERS_SLOW_IMPORT”(/opt/conda/lib/python3.10/site-packages/diffusers/utils/__init__.py)
还尝试将版本降级到 0.26.3 ，但没有帮助。
扩散器版本：0.27.2
感谢任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78242432/importerror-cannot-import-name-diffusers-slow-import-from-diffusers-utils</guid>
      <pubDate>Fri, 29 Mar 2024 07:07:02 GMT</pubDate>
    </item>
    <item>
      <title>如何在 FastAPI 中处理 ML 模型 API 的多个并行请求</title>
      <link>https://stackoverflow.com/questions/78242301/how-to-handle-multiple-parallel-requests-in-fastapi-for-an-ml-model-api</link>
      <description><![CDATA[我使用 FastAPI 构建了一个 ML 模型 API，该 API 主要需要使用 GPU。我想让这个 API 至少服务一定数量的并行请求。为了实现这一点，我尝试将所有函数设置为 def 而不是 async def，以便它可以同时处理请求（如前所述 此处 ，此处和此处）。目前，我遇到的情况是，如果发出一个请求，需要 3 秒才能获得输出，如果发出 3 个并行请求，则所有用户都将在 9 秒内获得输出。在这里，所有用户都同时获得输出，但正如您所看到的，时间随着请求数量的增加而增加。然而，我真正想要的是所有用户都能在 3 秒内获得输出。
我尝试过一些方法，例如 ThreadPoolExecutor (此处)，ProcessPoolExecutor (此处），Asyncio (此处)，run_in_threadpool (这里），但这些方法都不适合我。
这是我的 api 代码使用简单 def 的样子：
from fastapi import Depends、FastAPI、文件、UploadFile、响应
进口uvicorn

类 Model_loading():
    def __init__():
        self.model = torch.load(&#39;model.pth&#39;)


应用程序 = Fastapi()
model_instance = Model_loading()

def gpu_based_processing(x):

   ---- 进行一些基于 GPU 的计算 ----

   返回结果


@app.post(&#39;/模型测试&#39;)
def my_function(文件: UploadFile = File(...)):
    
   ---- 进行一些初始预处理 ----

   输出 = gpu_based_processing(x)

   返回响应（内容=输出，media_type=“图像/jpg”）


此外，我还观察到向上述 API 发出 20 个并行请求会导致 CUDA 内存不足错误的行为。即使只有 20 个请求，它也无法处理它们。如何解决 CUDA 内存问题并管理同时处理多个并行请求？]]></description>
      <guid>https://stackoverflow.com/questions/78242301/how-to-handle-multiple-parallel-requests-in-fastapi-for-an-ml-model-api</guid>
      <pubDate>Fri, 29 Mar 2024 06:26:11 GMT</pubDate>
    </item>
    <item>
      <title>在数字分类混合数据帧上应用 RandomForestRegressor 来预测两列标签集时得分较低</title>
      <link>https://stackoverflow.com/questions/78242202/low-score-when-applying-randomforestregressor-on-a-numeric-categorical-mixed-dat</link>
      <description><![CDATA[我在 Kaggle 上使用此保险数据集 保险数据集
尝试构建一个简单的回归器来预测最后两列 [&#39;coverage_level&#39;,&#39;charges&#39;]，同时使用所有其他 10 列作为特征输入到回归器模型中。
我意识到用作特征的 10 列既是数字类型又是分类类型，因此我使用 LabelEncoder 进行了一些转换：
df2 = df.copy()
# 姜
le = 标签编码器()
le.fit(df2.gender.drop_duplicates())
df2.gender = le.transform(df2.gender)
...其余分类列如“吸烟者”、“地区”等。

然后我在转换后的数据帧上应用了最小最大缩放器：
inputs = df2[[“年龄”、“性别”、“bmi”、“儿童”、“吸烟者”、“地区”、“医疗历史”、
         “家庭医疗史”、“运动频率”、“职业”]]
目标 = df2[[“coverage_level”, “charges”]]

缩放器 = MinMaxScaler()
scaledInputs = np.array(scaler.fit_transform(输入))

X_train，X_test，y_train，y_test = train_test_split（scaledInputs，目标，test_size = 0.20，random_state = 42）

最后是训练和测试部分：
rf_model = RandomForestRegressor(n_estimators=10, random_state=42)

# 拟合训练集
rf_model.fit(X_train, y_train)
rf_outputs = rf_model.predict(X_test)

rf_mse =mean_squared_error(y_test, rf_outputs)
rf_score = rf_model.score(X_test, y_test)

但是性能非常低，得分为0.27，mse接近2615601。
我尝试了一些修复。第一个不是仅缩放输入，而是在馈送之前缩放了两个目标列 [&#39;coverage_level&#39;,&#39;charges&#39;]，但是，它根本没有帮助。第二个修复是使用one-hot编码代替标签编码，但仍然没有增益。
请提出一些解决此问题的方法。]]></description>
      <guid>https://stackoverflow.com/questions/78242202/low-score-when-applying-randomforestregressor-on-a-numeric-categorical-mixed-dat</guid>
      <pubDate>Fri, 29 Mar 2024 05:50:10 GMT</pubDate>
    </item>
    <item>
      <title>如何解决：InvalidArgumentError：图形执行错误？</title>
      <link>https://stackoverflow.com/questions/78242076/how-i-resolve-invalidargumenterror-graph-execution-error</link>
      <description><![CDATA[我是计算机视觉和分类方面的专家，我正在尝试使用 keras VGG16 模型进行迁移学习，但我不断收到此代码下面的错误，任何人都可以帮助我或至少给我一个和平的建议？
导入tensorflow为tf
从tensorflow.keras导入模型、层
将 matplotlib.pyplot 导入为 plt
从全局导入全局
导入操作系统

IMG_大小 = 224
批次大小 = 32
通道 = 3
纪元 = 30

数据集 = tf.keras.preprocessing.image_dataset_from_directory(
    “../笔记本/数据集”，
    随机播放=真，
    图像大小=（IMG_SIZE，IMG_SIZE），
    批量大小=批量大小，
    
）

类名 = 数据集.类名
类名

def preprocess_image(图像):
    图像 = tf.image.resize(图像, [IMG_SIZE, IMG_SIZE])
    image /= 255.0 # 标准化为 [0,1] 范围
    打印（“嗨”）
    打印（图像）
    返回图像

# 应用预处理和增强
数据集 = 数据集. 地图(
    lambda x, y: (preprocess_image(x), y),
    num_parallel_calls=tf.data.experimental.AUTOTUNE
）

def apply_augmentation（图像，标签）：
    # 随机水平翻转
    图像 = tf.image.random_flip_left_right(图像)
    # 随机旋转
    图像 = tf.image.rot90(图像, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))
    # 随机亮度调节
    图像 = tf.image.random_brightness(图像, max_delta=0.1)
    返回图像、标签

数据集 = 数据集. 地图(
    应用增强，
    num_parallel_calls=tf.data.experimental.AUTOTUNE
）


train_size = int(0.8 * len(数据集))
val_size = int(0.1 * len(数据集))
test_size = len(数据集) - train_size - val_size

train_dataset = dataset.take(train_size)
val_dataset = dataset.skip(train_size).take(val_size)
test_dataset = dataset.skip(train_size).skip(val_size)

vgg = tf.keras.applications.VGG16(
    输入形状=（IMG_SIZE，IMG_SIZE，3），
    权重=&#39;imagenet&#39;,
    include_top=False
）


对于 vgg.layers 中的图层：
    可训练层 = False

x = 层.Flatten()(vgg.输出)
预测 = 层.Dense(3, 激活=&#39;relu&#39;)(x)

模型 = tf.keras.models.Model(输入=vgg.输入，输出=预测)
模型.summary()

模型.编译(
    损失=&#39;sparse_categorical_crossentropy&#39;,
    优化器=&#39;亚当&#39;,
    指标=[&#39;准确性&#39;]
）

历史=模型.fit(
    训练数据集，
    验证数据=val_数据集，
    纪元=10，
    steps_per_epoch=train_size，
    验证步骤=val_size
）


当我开始训练模型时出现以下错误，
2024-03-29 09:36:18.734909：W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES 在稀疏_xent_op.cc:103 处失败：INVALID_ARGUMENT：收到超出范围的标签值 3 [0, 3) 的有效范围。标签值：2 3 1 0 1 2 2 2 1 2 2 0 0 0 3 0 0 0 2 0 3 1 0 2 2 2 2 0 3 0 2 0
2024-03-29 09:36:18.734957: W tensorflow/core/framework/local_rendezvous.cc:404] 本地集合点正在中止，状态为：INVALID_ARGUMENT：收到的标签值 3 超出了 [0, 3 的有效范围）。标签值：2 3 1 0 1 2 2 2 1 2 2 0 0 0 3 0 0 0 2 0 3 1 0 2 2 2 2 0 3 0 2 0
     [[{{function_node __inference_one_step_on_data_2794}}{{节点compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]

InvalidArgumentError Traceback（最近一次调用最后一次）
输入 In [15], in ()
----&gt; 1 历史记录 = model.fit(
      2 训练数据集，
      3validation_data=val_dataset，
      4 epoch=10，
      5steps_per_epoch=train_size，
      6 验证步骤=val_size
      7）

文件 ~/anaconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122，位于filter_traceback..error_handler(*args, **kwargs)
    第119章
    120 # 要获取完整的堆栈跟踪，请调用：
    121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
    123 最后：
    124 删除filtered_tb

文件〜/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53，在quick_execute（op_name，num_outputs，输入，attrs，ctx，名称）中
     51 尝试：
     52 ctx.ensure_initialized()
---&gt; 53 张量 = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54 个输入、属性、输出数）
     55 除了 core._NotOkStatusException 为 e：
     56 如果名称不是 None：

InvalidArgumentError：图形执行错误：

在节点compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits处检测到（最近一次调用最后）：
。
。
。

我检查了这个问题，但找不到答案，如何可以回复：InvalidArgumentError：图形执行错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78242076/how-i-resolve-invalidargumenterror-graph-execution-error</guid>
      <pubDate>Fri, 29 Mar 2024 05:01:02 GMT</pubDate>
    </item>
    <item>
      <title>Pycharm 调试不适用于 Tensorflow。我该如何解决？</title>
      <link>https://stackoverflow.com/questions/78241816/pycharm-debug-is-not-working-with-tensorflow-how-do-i-resolve-it</link>
      <description><![CDATA[我已成功安装以下内容：
tensorflow（最新版本2.16.1）
keras（最新版本3.1.1

我使用的是pycharm 2023.3.5（社区版）。我有一些导入的代码行，包括张量流：
&lt;前&gt;&lt;代码&gt;...
从tensorflow.keras导入后端为K
...

每当我调试代码时，都会收到如下错误：
回溯（最近一次调用最后一次）：
文件“C:\Program Files\JetBrains\PyCharm Community Edition 2023.3.5\plugins\python-ce\helpers\pydev\_pydevd_bundle\pydevd_xml.py”，第 177 行，在 _get_type 中
if isinstance(o, t[0]):
   ^^^^^^^^^^^^^^^^^^^^
文件“C:\Program Files\Python312\Lib\site-packages\tensorflow\python\platform\flags.py”，第 73 行，在 __getattribute__ 中
返回 self.__dict__[&#39;__wrapped&#39;].__getattribute__(name)
       ~~~~~~~~~~~~~^^^^^^^^^^^^^
关键错误：&#39;__wrapped&#39;

我想相信问题不是由张量流引起的，但我似乎无法弄清楚确切的问题。我已经上网但无济于事。我得到的最接近的解决方案是这个 问题，但是，它似乎我作为一个不同的问题。请这个崇高平台上的博学之士来帮助我。]]></description>
      <guid>https://stackoverflow.com/questions/78241816/pycharm-debug-is-not-working-with-tensorflow-how-do-i-resolve-it</guid>
      <pubDate>Fri, 29 Mar 2024 03:17:26 GMT</pubDate>
    </item>
    <item>
      <title>包含clip_by_value的LSTM冻结层导致android studio在部署时崩溃</title>
      <link>https://stackoverflow.com/questions/78241639/lstm-frozen-layer-containing-clip-by-value-causing-android-studio-to-crash-when</link>
      <description><![CDATA[我对 ML 很陌生，但我正在尝试将冻结的 LSTM 模型部署到 android studio 中，但我收到此错误，该错误说明在 lstm 层中使用了 Clipbyvalue 操作，而我没有明确声明该操作。
java.lang.IllegalArgumentException：没有注册 OpKernel 来支持具有这些属性的 Op &#39;ClipByValue&#39;。注册设备：[CPU]，注册内核：&lt;无注册内核&gt; [[节点：lstm_1/while/clip_by_value = ClipByValue[T=DT_FLOAT](lstm_1/while/add_2，lstm_1/while/Const，lstm_1/while/Const_1)]]
我目前使用的是tensorflow 1.8，并尝试使用更高版本来使用TFLite，但遇到了问题，这就是为什么我决定降级以尝试使用冻结图功能。
下面是我在android studio中的java代码
公共类 pbClassifier {


    静止的{
        System.loadLibrary(“tensorflow_inference”);
    }

    私有 TensorFlowInferenceInterface 推理接口；
    私有静态最终字符串MODEL_FILE =“model.pb”;
    私有静态最终字符串INPUT_NODE =“lstm_1_input”；
    private static Final String[] OUTPUT_NODES = {“输出/Softmax”};
    私有静态最终字符串 OUTPUT_NODE =“输出/Softmax”；
    私有静态最终长[] INPUT_SIZE = {1, 200, 6};
    私有静态最终 int OUTPUT_SIZE = 7;


    公共 pbClassifier（最终上下文上下文）{
        inferenceInterface = new TensorFlowInferenceInterface(context.getAssets(), MODEL_FILE);
    }

    公共浮点预测（浮点[]数据）{
        float[] 结果 = new float[OUTPUT_SIZE];
        inferenceInterface.feed(INPUT_NODE, 数据, INPUT_SIZE);
        inferenceInterface.run(OUTPUT_NODES);
        inferenceInterface.fetch(OUTPUT_NODE, 结果);
        返回结果[1]；
    }


}

这是用于模型训练和导出的 python 代码
&lt;前&gt;&lt;代码&gt;模型 = 顺序()
# RNN层
model.add(LSTM(128, input_shape = (200, 6), return_sequences = True, kernel_regularizer = l2(0.000001), name = &#39;lstm_1&#39;))
# 对每个时间序列单独应用密集操作
model.add(TimeDistributed(Dense(64,activation=&#39;relu&#39;), name=&#39;time_distributed&#39;))
# 压平图层
model.add(Flatten(name=&#39;flatten&#39;))
# 使用 ReLu 的密集层
model.add（密集（64，激活=&#39;relu&#39;，名称=&#39;dense_1&#39;））
# Softmax层
model.add(Dense(2, 激活 = &#39;softmax&#39;, name=&#39;输出&#39;))

# 编译模型
model.compile（损失=&#39;sparse_categorical_crossentropy&#39;，优化器= Adam（），指标= [&#39;准确性&#39;]）

从 keras.callbacks 导入 ModelCheckpoint

回调 = [ModelCheckpoint(&#39;model.h5&#39;, save_weights_only=False, save_best_only=True, verbose=1)]

历史= model.fit（X_train，y_train，epochs = 10，validation_split = 0.20，batch_size = 128，
                    详细 = 1，回调 = 回调）

从 keras 导入后端为 k
从tensorflow.python.tools导入freeze_graph，optimize_for_inference_lib

input_node_name = [&#39;lstm_1_input&#39;]
output_node_name = &#39;输出/Softmax&#39;
模型名称 = &#39;坠落模型&#39;

tf.train.write_graph(k.get_session().graph_def, &#39;models&#39;, model_name + &#39;_graph.pbtxt&#39;)
保存程序 = tf.train.Saver()
saver.save(k.get_session(), &#39;models/&#39; + model_name + &#39;.chkp&#39;)

freeze_graph.freeze_graph(&#39;models/&#39; +model_name + &#39;_graph.pbtxt&#39;, None, False, &#39;models/&#39; +model_name+&#39;.chkp&#39;,
                          output_node_name, &#39;save/restore_all&#39;, &#39;save/Const:0&#39;, &#39;models/frozen_&#39;+model_name+&#39;.pb&#39;,
                          确实，“”）

如果有任何关于如何通过阻止图层使用clip_by_value函数或可能将其更改为clip_norm函数以查看是否有效来解决此问题的建议，我将不胜感激。
这也是来自 Netron 的图像
非常感谢]]></description>
      <guid>https://stackoverflow.com/questions/78241639/lstm-frozen-layer-containing-clip-by-value-causing-android-studio-to-crash-when</guid>
      <pubDate>Fri, 29 Mar 2024 01:56:04 GMT</pubDate>
    </item>
    <item>
      <title>在 Databricks AutoML 运行的最佳试用笔记本的“加载数据”部分中访问 df_loaded 和/或 run_id</title>
      <link>https://stackoverflow.com/questions/78241331/access-df-loaded-and-or-run-id-in-load-data-section-of-best-trial-notebook-of-da</link>
      <description><![CDATA[下面的代码块是通过执行 Databricks AutoML 运行自动生成的最佳试用笔记本的一部分。
导入mlflow
导入操作系统
导入uuid
进口舒蒂尔
将 pandas 导入为 pd

# 创建临时目录以从 MLflow 下载输入数据
input_temp_dir = os.path.join(os.environ[&quot;SPARK_LOCAL_DIRS&quot;], &quot;tmp&quot;, str(uuid.uuid4())[:8])
os.makedirs(input_temp_dir)


# 下载工件并将其读入 pandas DataFrame
input_data_path = mlflow.artifacts.download_artifacts（run_id =“e2a4a93aafb24aa9956e83f6b7ab3e28”，artifact_path =“数据”，dst_path = input_temp_dir）

df_loaded = pd.read_parquet(os.path.join(input_data_path, “training_data”))
# 删除临时数据
Shutil.rmtree(input_temp_dir)

# 预览数据
df_loaded.head(5)


上面代码块中的 run_id e2a4a93aafb24aa9956e83f6b7ab3e28，我可以从运行 automl.regress 返回的 AutoMLSummary 中获取它吗？如果我使用summary.best_trial.mlflow_run_id，我会得到不同的值。那么这个 run_id 是什么以及如何获取它？

除了上面的代码块之外，还有没有办法获取已加载到 df_loaded 中的数据集？它本质上是我输入到 automl.regress 中的输入数据集，只不过它有一列指示每一行是否是训练、验证和测试子集的一部分。


我对 Databricks AutoML 相当陌生，因此不确定完成此任务的最佳方法是什么。
提前致谢。
正如我所提到的，我尝试从summary.best_trial.mlflow_run_id 中获取run_id，但值不匹配。我尝试阅读 automl 和 mlflow 的文档，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78241331/access-df-loaded-and-or-run-id-in-load-data-section-of-best-trial-notebook-of-da</guid>
      <pubDate>Thu, 28 Mar 2024 23:32:11 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中手动对某一层的输出进行反量化并为下一层重新量化？</title>
      <link>https://stackoverflow.com/questions/78239906/how-to-manually-dequantize-the-output-of-a-layer-and-requantize-it-for-the-next</link>
      <description><![CDATA[我正在开展学校项目，该项目要求我对模型的每一层执行手动量化。具体来说，我想手动实现：
&lt;块引用&gt;
量化激活，结合量化权重A-A层-
量化输出 - 反量化输出 - 重新量化输出，组合
量化权重 B - 层 B - ...

我知道Pytorch已经有量化函数，但该函数仅限于int8。我想从bit = 16到bit = 2进行量化，然后比较它们的准确性。
我遇到的问题是，量化后，一层的输出大了多个数量级（bit = 16），并且我不知道如何将其反量化回来。我正在使用相同的激活和权重的最小值和最大值来执行量化。这是一个例子：
激活 = [1,2,3,4]
权重 = [5,6,7,8]
激活和权重的最小值和最大值 = 1, 8
预期非量化输出 = 70

量化位 = 16
量化激活 = [-32768, -23406, -14044, -4681]
量化权重 = [4681, 14043, 23405, 32767]
量化输出 = -964159613
反量化输出，最小值 = 1，最大值 = 8 = -102980

这个计算对我来说很有意义，因为输出涉及激活值和权重的相乘，它们的幅度增加也相乘。如果我用原始的最小值和最大值执行一次反量化，那么有一个更大的输出是合理的。
Pytorch 如何处理反量化？我尝试定位Pytorch的量化，但是找不到。如何对输出进行反量化？]]></description>
      <guid>https://stackoverflow.com/questions/78239906/how-to-manually-dequantize-the-output-of-a-layer-and-requantize-it-for-the-next</guid>
      <pubDate>Thu, 28 Mar 2024 17:17:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 Geotiff 图像分割</title>
      <link>https://stackoverflow.com/questions/78238294/segmentation-with-geotiff-image</link>
      <description><![CDATA[我想用 python 进行 k-means 分割。我的代码适用于 jpg 图像，但是当我尝试使用 geotiff 时，它只会使图像变成黑白。我怎么解决这个问题？下面是我的代码；
导入 matplotlib 作为 mpl
将 matplotlib.pyplot 导入为 plt
将 matplotlib.image 导入为 mpimg
从 sklearn.cluster 导入 KMeans

从 PIL 导入图像
Image.MAX_IMAGE_PIXELS = 无

从 google.colab 导入驱动器
驱动器.mount（&#39;/内容/驱动器&#39;）

# Dosya yolu，Google Drive&#39;ınızda dosyanın bulunduğu yol

file_path = &#39;/content/drive/MyDrive/Colab Notebooks/ortofoto.tif&#39;

# TIFF dosyasını oku
图像 = mpimg.imread(文件路径)
[文本]([https://stackoverflow.com](https://stackoverflow.com))

# 多斯亚伊·戈斯特
plt.imshow(图像)
plt.show()

X=图像.reshape(-1, 4)
kmeans=KMeans(n_clusters=2, n_init=10).fit(X)

segmented_img=kmeans.cluster_centers_[kmeans.labels_]
splited_img=segmented_img.reshape(image.shape)
plt.imshow(segmented_img/255)
]]></description>
      <guid>https://stackoverflow.com/questions/78238294/segmentation-with-geotiff-image</guid>
      <pubDate>Thu, 28 Mar 2024 12:48:44 GMT</pubDate>
    </item>
    <item>
      <title>OCR 结果不一致：训练和测试期间的预测不同</title>
      <link>https://stackoverflow.com/questions/78202518/inconsistent-ocr-results-different-predictions-during-training-and-testing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78202518/inconsistent-ocr-results-different-predictions-during-training-and-testing</guid>
      <pubDate>Thu, 21 Mar 2024 20:02:00 GMT</pubDate>
    </item>
    <item>
      <title>CatBoostRegressor 与 loss_function='Lq'</title>
      <link>https://stackoverflow.com/questions/76498616/catboostregressor-with-loss-function-lq</link>
      <description><![CDATA[我不知道如何指定“q” “Lq”中的变量损失函数。我收到以下错误消息：
CatBoostError：/src/catboost/catboost/private/libs/options/catboost_options.cpp:82：参数 q 对于 Lq 丢失是必需的

我的代码如下：
from catboost import CatBoostRegressor
从 sklearn.datasets 导入 make_regression
从 sklearn.model_selection 导入 train_test_split
将 numpy 导入为 np

# 生成人工回归数据集
X, y = make_regression(n_samples=1000, n_features=10, random_state=42)

# 将数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建一个 CatBoostRegressor 对象
模型 = CatBoostRegressor(loss_function=&#39;Lq&#39;)

# 拟合模型
model.fit(X_train, y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/76498616/catboostregressor-with-loss-function-lq</guid>
      <pubDate>Sun, 18 Jun 2023 00:20:48 GMT</pubDate>
    </item>
    <item>
      <title>将 Detectron2 模型转换为 torchscript</title>
      <link>https://stackoverflow.com/questions/73619217/convert-detectron2-model-to-torchscript</link>
      <description><![CDATA[我想将 detectorron2 &#39;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml 模型&#39; 转换为 torchscript。
我用过托克
我的代码如下。
&lt;前&gt;&lt;代码&gt;导入cv2

将 numpy 导入为 np

进口火炬
从 detector2 导入 model_zoo
从 detector2.config 导入 get_cfg
从 detectorron2.engine 导入 DefaultPredictor
从 detector2.modeling 导入 build_model
从 detectorron2.export.flatten 导入 TracingAdapter
导入操作系统

ModelPath=&#39;/home/jayasanka/working_files/create_torchsript/model.pt&#39;
将 open(&#39;savepic.npy&#39;, &#39;rb&#39;) 作为 f：
    图像 = np.load(f)

#------------------------------------------------- ------------------------------------------------

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(“COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml”))

cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 # 你的类数 + 1

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, ModelPath)

cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.60 # 设置该模型的测试阈值

预测器 = DefaultPredictor(cfg)



我使用了 TracingAdapter 和跟踪函数。我不太了解其背后的概念是什么。
&lt;前&gt;&lt;代码&gt;# im = cv2.imread(图像)
im = torch.tensor(图像)

def inference_func（模型，图像）：
    输入= [{“图像”：图像}]
    返回 model.inference(inputs, do_postprocess=False)[0]

包装器= TracingAdapter（预测器，im，inference_func）
包装器.eval()
Traced_script_module= torch.jit.trace（包装器，（im，））
traced_script_module.save(“torchscript.pt”)

它给出了下面给出的错误。
回溯（最近一次调用最后一次）：
  文件“script.py”，第 49 行，位于  中。
    Traced_script_module= torch.jit.trace（包装器，（im，））
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/jit/_trace.py”，第 744 行，跟踪中
    _模块_类，
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/jit/_trace.py”，第 959 行，在trace_module 中
    参数名称，
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/nn/modules/module.py”，第 1051 行，在 _call_impl 中
    返回forward_call（*输入，**kwargs）
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/nn/modules/module.py”，第 1039 行，位于 _slow_forward
    结果 = self.forward(*输入, **kwargs)
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/detectron2/export/flatten.py”，第 294 行，向前
    输出 = self.inference_func(self.model, *inputs_orig_format)
  文件“script.py”，第 44 行，inference_func
    返回 model.inference(inputs, do_postprocess=False)[0]
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/yacs/config.py”，第 141 行，在 __getattr__ 中
    引发属性错误（名称）
属性错误：推理


你能帮我解决这个问题吗？
还有其他方法可以轻松做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/73619217/convert-detectron2-model-to-torchscript</guid>
      <pubDate>Tue, 06 Sep 2022 08:50:15 GMT</pubDate>
    </item>
    </channel>
</rss>