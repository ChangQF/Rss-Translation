<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 18 Jan 2025 09:15:21 GMT</lastBuildDate>
    <item>
      <title>为什么我们不屏蔽 Transformer 中除了多头注意力之外的其他层？</title>
      <link>https://stackoverflow.com/questions/79366294/why-we-dont-mask-other-layers-besides-the-multihead-attention-in-transformers</link>
      <description><![CDATA[通常在训练 NLP 任务时，我们需要将序列填充到 max_len，以便可以以批处理方式高效处理它们。但是，这些填充的标记不应影响训练（模型参数的更新），因为它们是“虚构的”。
每个人都在谈论掩盖注意力操作的必要性。这是有道理的，因为有效标记不应该关注虚拟标记。但是，我们仍然需要考虑所有其他层（例如 FF 中的线性层、规范化层等）不受填充的影响。
为了简单起见，考虑单个 Transformer-Encoder 层：

从图中可以清楚地看出，FF 和最后的规范化层将填充的标记处理为其他每个标记。为了正确屏蔽所有层，我们应该屏蔽损失吗？如果这听起来微不足道，请原谅，但我没有找到任何相关信息。
例如，假设我们想使用 Transformer 编码器对序列进行分类。我们传递一个形状为 (B, N, E) 的输入 x，其中 B 是批量大小，N 是最大序列长度，E 是嵌入维度。输出具有相同的形状，我们将使用它进行分类。我们可以通过为每个序列提取一个“全局”向量来实现，然后将其传递给特定于任务的头部，如下所示：
x =coder_layer(x)
x = x.mask_fill(mask, -torch.inf) # 在最大操作期间屏蔽填充。
x = torch.max(x, dim=1)[0] # 忽略索引，简化为形状 (B, E)。
loss = loss_fn(cls_head(x), y)

由于 max 操作对任何未选定元素返回 0 梯度，因此所有参数都不会受到填充的影响。我的理解正确吗？]]></description>
      <guid>https://stackoverflow.com/questions/79366294/why-we-dont-mask-other-layers-besides-the-multihead-attention-in-transformers</guid>
      <pubDate>Fri, 17 Jan 2025 23:05:49 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 3.11.11 中安装 tfx</title>
      <link>https://stackoverflow.com/questions/79365624/how-to-install-tfx-in-python-3-11-11</link>
      <description><![CDATA[我在 Google Colab 上安装 TFX 时遇到了问题，而其他组件如

tensorflow_model_analysis
tensorflow_data_validation
tensorflow_transform
tensorflow_transform.beam

现在可以安装了
只需安装 TFX，它还不起作用
来自 tfx.components 导入 CsvExampleGen、StatisticsGen、SchemaGen、ExampleValidator、Transform、Trainer、Tuner、Evaluator、Pusher
来自 tfx.proto 导入 example_gen_pb2
来自 tfx.orchestration.experimental.interactive.interactive_context 导入 InteractiveContext
来自 tfx.dsl.components.common.resolver 导入 Resolver
来自tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy
from tfx.types import Channel
from tfx.types.standard_artifacts import Model, ModelBlessing

安装 TFX 时出现以下错误

请帮忙，我应该修复哪些错误？]]></description>
      <guid>https://stackoverflow.com/questions/79365624/how-to-install-tfx-in-python-3-11-11</guid>
      <pubDate>Fri, 17 Jan 2025 17:28:55 GMT</pubDate>
    </item>
    <item>
      <title>如何利用多个 CPU 进行 YOLO 训练？</title>
      <link>https://stackoverflow.com/questions/79365374/how-to-utilize-multiple-cpus-for-training-of-yolo</link>
      <description><![CDATA[我可以访问一个没有 GPU 的大型 CPU 集群。通过在多个 CPU 节点之间并行化，是否可以加快 YOLO 训练速度？
文档说 device 参数指定用于训练的计算设备：单个 GPU（device=0）、多个 GPU（device=0,1）、CPU（device=cpu）或 Apple 芯片的 MPS（device=mps）。
那么多个 CPU 呢？]]></description>
      <guid>https://stackoverflow.com/questions/79365374/how-to-utilize-multiple-cpus-for-training-of-yolo</guid>
      <pubDate>Fri, 17 Jan 2025 16:05:46 GMT</pubDate>
    </item>
    <item>
      <title>将 Tensorflow 模型工件部署到 Sagemaker</title>
      <link>https://stackoverflow.com/questions/79363695/deploying-a-tensorflow-model-artifact-to-sagemaker</link>
      <description><![CDATA[我正在尝试将 TensorFlow 模型部署到 Sagemaker 端点。我在 generic_graph.pb 处有模型工件，在 labels.txt 处有模型标签。
我首先创建了一个包含以下内容的 tar 文件：
# #model 目录结构 
# #model.tar.gz
# └── &lt;model_name&gt;
# └── &lt;version_number&gt;
# ═── saved_model.pb
# └── 变量
# ═── labels.txt

我将文件上传到 S3 中的存储桶。然后，我尝试使用以下代码部署模型：
sagemaker_session = sagemaker.Session()
role = &#39;my-role&#39;

model = TensorFlowModel(model_data=&#39;s3://my-bucket/model.tar.gz&#39;,
role=role,
framework_version=&#39;2.3.0&#39;)

predictor = model.deploy(initial_instance_count=1, instance_type=&#39;ml.m5.large&#39;)

我的 cloudwatch 日志中不断出现以下错误：
ValueError: 未找到 SavedModel 包！

不确定还能尝试什么。]]></description>
      <guid>https://stackoverflow.com/questions/79363695/deploying-a-tensorflow-model-artifact-to-sagemaker</guid>
      <pubDate>Fri, 17 Jan 2025 05:19:16 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 AI 在 GPU 上的训练比 CPU 慢很多</title>
      <link>https://stackoverflow.com/questions/79362113/why-is-my-ai-training-on-gpu-is-a-lot-slower-than-cpu</link>
      <description><![CDATA[我目前正在训练我的简单预测 AI，但我的 GPU 训练速度为每 epoch 40S，而我的 CPU 训练速度为每 epoch 9S
我的 CPU 是 i7-4720HQ，我的 GPU 是 Nvidia 950m
这是我的代码
`import tensorflow as tf
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

df = pd.read_csv(&#39;Updated_Train.csv&#39;)
df = df.drop(columns=&#39;date&#39;)
df = df.drop(columns=&#39;id&#39;)
label_encoder = LabelEncoder()

df[&#39;Country&#39;] = label_encoder.fit_transform(df[&#39;Country&#39;])
df[&#39;Store&#39;] = label_encoder.fit_transform(df[&#39;Store&#39;])
df[&#39;Product&#39;] = label_encoder.fit_transform(df[&#39;Product&#39;])

x = df.iloc[:, :-1]
y = df.iloc[:, -1]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = (
train_dataset
.shuffle(buffer_size=1000) # 打乱数据
.batch(32) # 批次大小为 32
.prefetch(tf.data.AUTOTUNE) # 预取以提高管道性能
)

test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)
with tf.device(&#39;/CPU:0&#39;):
输入 = tf.keras.layers.Input(shape=(3,))
m = tf.keras.layers.Dense(32,activation=&#39;relu&#39;)(输入)
m = tf.keras.layers.Dense(16,activation=&#39;relu&#39;)(m)
m = tf.keras.layers.Dense(8,activation=&#39;relu&#39;)(m)
m = tf.keras.layers.Dense(8,activation=&#39;relu&#39;)(m)
输出 = tf.keras.layers.Dense(1)(m)

model = tf.keras.models.Model(inputs=Input, output=output)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
loss=&#39;mse&#39;)

history = model.fit(train_dataset,
epochs=100,
validation_data=test_dataset)

model.save(&#39;Sticker.keras&#39;)

plt.plot(history.history[&#39;loss&#39;], label=&#39;Training Loss&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;Validation Loss&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.title(&#39;Loss Over时间&#39;)
plt.legend()
plt.show()

`

我的数据有 3 个输入列和 1 个输出列，共有 230129 行
我询问了 gpt，他们让我改进数据管道，我照做了，但我的 GPU 每个时期仍然需要 40 秒]]></description>
      <guid>https://stackoverflow.com/questions/79362113/why-is-my-ai-training-on-gpu-is-a-lot-slower-than-cpu</guid>
      <pubDate>Thu, 16 Jan 2025 15:11:47 GMT</pubDate>
    </item>
    <item>
      <title>所有样本的前向传递</title>
      <link>https://stackoverflow.com/questions/79361940/forward-pass-with-all-samples</link>
      <description><![CDATA[import torch
import torch.nn as nn

class PINN(nn.Module):
def __init__(self, input_dim, output_dim, hidden_​​layers, neurons_per_layer):
super(PINN, self).__init__()
layer = []
layer.append(nn.Linear(input_dim, neurons_per_layer))
for _ in range(hidden_​​layers):
layer.append(nn.Linear(neurons_per_layer, neurons_per_layer))
layer.append(nn.Linear(neurons_per_layer, output_dim))
self.network = nn.Sequential(*layers)

def forward(self, x):
return self.network(x)

# 示例：生成随机输入数据
inputs = torch.rand((1000, 3)) # 3D 输入坐标

model = PINN(input_dim=3, output_dim=3, hidden_​​layers=4, neurons_per_layer=64)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 10000
for epoch in range(epochs):
optimizer.zero_grad() 
nn_output = model(inputs) # 计算 NN 预测
# 计算 nn_output 的梯度
loss.backward() 
optimizer.step() 

我想实现一个物理信息 NN，其中输入是 N 个 3d 点 (x,y,z)，NN 输出是此时的矢量值量，即输入维度和输出维度都相同。
要计算每个时期的损失，我需要有该量的值在所有点。示例：对于 N=1000 点，我需要所有 1000 个 NN 预测，然后才能继续进行损失计算。
在我的代码中，我基本上将一个 1000x3 对象提供给输入层，假设 pytorch 将每一行（1x3）分别传递给网络，最后将其再次组织为 1000x3 对象。
pytorch 是否像那样工作，还是我必须重新考虑这种方法？]]></description>
      <guid>https://stackoverflow.com/questions/79361940/forward-pass-with-all-samples</guid>
      <pubDate>Thu, 16 Jan 2025 14:23:21 GMT</pubDate>
    </item>
    <item>
      <title>通过手动汇总值来重现 LGBMRegressor 预测</title>
      <link>https://stackoverflow.com/questions/79361226/reproduce-lgbmregressor-predictions-by-manually-aggregate-the-values</link>
      <description><![CDATA[我正在尝试自己重现 LGBMRegressor 预测，因此当我成功时，我会将平均值转换为中位数。但目前看来我做不到。
这是我为检查是否可以重现结果而创建的简单脚本。
我需要 reg_y_hat 与 self_y_hat 相同。
我遗漏了什么？如果我知道训练中的哪些样本落到每个叶子上，我就可以自己汇总预测...
 import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split

# 生成一些随机回归数据
np.random.seed(42)
X = np.random.rand(100, 5)
y = 4 * X[:, 0] - 2 * X[:, 1] + np.random.rand(100) * 0.1

# 分割数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 LGBMRegressor
model = lgb.LGBMRegressor(objective=&#39;regression&#39;, n_estimators=2, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)

# 常规预测：
reg_y_hat = model.predict(X_test)

# 获取初始预测（y_train 的平均值）
init_pred = np.mean(y_train)

# 获取训练叶子值
train_leaf_indices = model.predict(X_train, pred_leaf=True)
leaf_samples = {(i, leaf_id): [] for i in range(model.n_estimators) for leaf_id in np.unique(train_leaf_indices[:, i])}

# 存储每个叶子的相应目标值
for i, row in enumerate(train_leaf_indices):
for j, leaf_id in enumerate(row):
leaf_samples[(j, leaf_id)].append(y_train[i])

#计算每个叶子的平均值：
leaf_agg = {}
for key, values in leaf_samples.items():
leaf_agg[key] = np.mean(values)

# 通过聚合平均值并添加初始预测进行预测：
preds = []
test_leaf_indices = model.predict(X_test, pred_leaf=True)
for row_indices in test_leaf_indices:
row_pred = init_pred
for i, leaf_index in enumerate(row_indices):
row_pred += model.learning_rate * (leaf_agg[(i, leaf_index)] - init_pred) # 仅初始预测后叶子的残差贡献
preds.append(row_pred)
self_y_hat = np.array(preds)

# 验证结果
print(&#39;reg_y_hat 之间的差异和 self_y_hat:&#39;, np.abs(reg_y_hat - self_y_hat).sum())
]]></description>
      <guid>https://stackoverflow.com/questions/79361226/reproduce-lgbmregressor-predictions-by-manually-aggregate-the-values</guid>
      <pubDate>Thu, 16 Jan 2025 10:39:14 GMT</pubDate>
    </item>
    <item>
      <title>stable_baselines3：为什么比较 ep_info_buffer 与评估时奖励不匹配？</title>
      <link>https://stackoverflow.com/questions/79353843/stable-baselines3-why-the-reward-does-not-match-comparing-ep-info-buffer-vs-eva</link>
      <description><![CDATA[我正在使用 stable_baselines3 库，这时我发现了一些意想不到的东西。
这里有一个简单的代码来重现这个问题：
import gymnasium as gym

from stable_baselines3 import DQN

env = gym.make(&quot;CartPole-v1&quot;)

model = DQN(&quot;MlpPolicy&quot;, env, verbose=0, stats_window_size=100_000)
model.learn(total_timesteps=100_000)

看看最后一集的奖励：
print(model.ep_info_buffer[-1])


{&#39;r&#39;: 409.0, &#39;l&#39;: 409, &#39;t&#39;: 54.87983

但是如果我使用以下代码评估模型：
obs, info = env.reset()
total_reward = 0
while True:
action, _states = model.predict(obs, deterministic=True)
obs, reward, termed, truncated, info = env.step(action)
total_reward = total_reward + reward
if termed or truncated:
obs, info = env.reset()
break

print(&quot;total_reward {}&quot;.format(total_reward))


total_reward 196.0

我得到了不同的奖励，这是我没有预料到的。
我预计会得到与 409 相同的奖励model.ep_info_buffer[-1]。
为什么会有这种差异？.ep_info_buffer 与每集奖励不同吗？]]></description>
      <guid>https://stackoverflow.com/questions/79353843/stable-baselines3-why-the-reward-does-not-match-comparing-ep-info-buffer-vs-eva</guid>
      <pubDate>Tue, 14 Jan 2025 02:14:32 GMT</pubDate>
    </item>
    <item>
      <title>使用 MAPIE 进行共形预测时，当 alpha 较大时，我会得到空的预测集</title>
      <link>https://stackoverflow.com/questions/78240714/using-mapie-for-conformal-predictions-i-get-empty-predictions-sets-when-alpha-is</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78240714/using-mapie-for-conformal-predictions-i-get-empty-predictions-sets-when-alpha-is</guid>
      <pubDate>Thu, 28 Mar 2024 20:28:12 GMT</pubDate>
    </item>
    <item>
      <title>如何为 Transformer 实现位置方向前馈神经网络？</title>
      <link>https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers</link>
      <description><![CDATA[我很难理解 transformer 架构中的位置式前馈神经网络。

让我们以机器翻译任务为例，其中输入是句子。从图中我了解到，对于每个单词，不同的前馈神经网络用于自注意子层的输出。前馈层应用了类似的线性变换，但每个变换的实际权重和偏差是不同的，因为它们是两个不同的前馈神经网络。
参考链接，这是PositionWiseFeedForward神经网络的类
class PositionwiseFeedForward(nn.Module):
“实现 FFN 方程。”
def __init__(self, d_model, d_ff, dropout=0.1):
super(PositionwiseFeedForward, self).__init__()
self.w_1 = nn.Linear(d_model, d_ff)
self.w_2 = nn.Linear(d_ff, d_model)
self.dropout = nn.Dropout(dropout)

def forward(self, x):
return self.w_2(self.dropout(F.relu(self.w_1(x))))

我的问题是：
我没有看到任何与位置相关的信息。这是一个具有两层的简单全连接神经网络。假设 x 是句子中每个单词的嵌入列表，句子中的每个单词都由上面的层使用相同的权重和偏差进行转换。（如果我错了，请纠正我）
我期望找到类似将每个单词嵌入传递到单独的 Linear 层的方法，该层将具有不同的权重和偏差，以实现与图片中所示的类似效果。]]></description>
      <guid>https://stackoverflow.com/questions/74979359/how-is-position-wise-feed-forward-neural-network-implemented-for-transformers</guid>
      <pubDate>Mon, 02 Jan 2023 05:59:25 GMT</pubDate>
    </item>
    <item>
      <title>如何获取 mlflow 记录工件的 url？</title>
      <link>https://stackoverflow.com/questions/73815787/how-to-get-url-of-mlflow-logged-artifacts</link>
      <description><![CDATA[我正在运行 ML 管道，在其末尾我使用 mlflow 记录某些信息。我主要在学习 Databricks 的官方 mlflow 跟踪教程。
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

with mlflow.start_run():
n_estimators = 100
max_depth = 6
max_features = 3
# 创建并训练模型
rf = RandomForestRegressor(n_estimators = n_estimators, max_depth = max_depth, max_features = max_features)
rf.fit(X_train, y_train)
# 进行预测
predictions = rf.predict(X_test)

# 记录参数
mlflow.log_param(&quot;num_trees&quot;, n_estimators)
mlflow.log_param(&quot;maxdepth&quot;, max_depth)
mlflow.log_param(&quot;max_feat&quot;, max_features)

# 记录模型
mlflow.sklearn.log_model(rf, &quot;random-forest-model&quot;)

# 创建指标
mse = mean_squared_error(y_test, predictions)

# 记录指标
mlflow.log_metric(&quot;mse&quot;, mse)

当我在 Databricks 笔记本中运行上述代码块时，会显示以下状态消息：
(1) MLflow 运行
在 MLflow 中记录了 1 次实验运行。了解更多

我可以通过单击 &quot;1 run&quot; 查看记录的信息。
但是，我想自动检索此链接。特别是，我需要指向存储工件的 mlflow uri 的链接。此链接的格式如下：
https://mycompany-dev.cloud.databricks.com/?o=&lt;ID_1&gt;#mlflow/experiments/&lt;ID_2&gt;/runs/&lt;ID_3&gt;

我尝试调查 URL，并通过打印以下信息来查找其中存在的各种 ID 代码：
print(&quot;Tracking URI: &quot;, mlflow.get_tracking_uri())
print(&quot;Run id:&quot;, run.info.run_id)
print(&quot;Experiment:&quot;, run.info.experiment_id)

我发现上面链接中的 &lt;ID_2&gt; 是 experiment_id，而 &lt;ID_3&gt; 是 run_id。但我不知道 &lt;ID_1&gt; 代表什么。此外，我相信应该有一个内置功能来检索已保存工件的链接，而不必手动从各个部分构建链接。但是，到目前为止，我还没有在文档中找到这样的功能。
编辑：现在我发现 &lt;ID_1&gt; 是 Databricks 工作区 ID，我可以轻松访问它。
编辑 2：
但如何以编程方式访问整个 URL 仍然是一个问题，而不必通过在代码中对 URL 的各个部分进行硬编码来逐个构建它。]]></description>
      <guid>https://stackoverflow.com/questions/73815787/how-to-get-url-of-mlflow-logged-artifacts</guid>
      <pubDate>Thu, 22 Sep 2022 13:46:21 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习进行形状检测[关闭]</title>
      <link>https://stackoverflow.com/questions/44649290/shape-detection-using-machine-learning</link>
      <description><![CDATA[我想使用机器学习技术检测形状，即圆形、正方形、矩形、三角形等。
以下是形状检测的规范，

使用卷积神经网络 (CNN)。
对于训练，数据集包含 10 种形状的每个类别中的 1000 张图像。
对于测试，数据集包含 10 种形状的每个类别中的 100 张图像。
所有图像均为 28x28 大小，具有一个通道（灰色通道）。
数据集中的所有图像都是边缘检测图像。

问题

机器学习算法是否可以区分正方形和矩形……？，正方形和菱形……？
如何改进形状检测数据集？
]]></description>
      <guid>https://stackoverflow.com/questions/44649290/shape-detection-using-machine-learning</guid>
      <pubDate>Tue, 20 Jun 2017 09:36:35 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何分类器能够非常快速地做出决策？</title>
      <link>https://stackoverflow.com/questions/34311785/is-there-any-classifier-which-is-able-to-make-decisions-very-fast</link>
      <description><![CDATA[大多数分类算法都是为了提高训练速度而开发的。然而，有没有一个分类器或算法专注于决策速度（计算复杂度低，可实现结构简单）？我可以得到足够的训练数据，并忍受较长的训练时间。]]></description>
      <guid>https://stackoverflow.com/questions/34311785/is-there-any-classifier-which-is-able-to-make-decisions-very-fast</guid>
      <pubDate>Wed, 16 Dec 2015 12:15:26 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习创建语音识别系统[关闭]</title>
      <link>https://stackoverflow.com/questions/15127461/creating-a-voice-identification-system-using-machine-learning</link>
      <description><![CDATA[作为机器学习的一个教育项目，我考虑从头开始创建一个语音识别系统。它应该能够在之前对说话者的声音进行训练后，根据说话者的声音识别出说话者。
我应该采取什么方法来应对这一挑战？具体来说，这样的系统在高水平上将如何工作？]]></description>
      <guid>https://stackoverflow.com/questions/15127461/creating-a-voice-identification-system-using-machine-learning</guid>
      <pubDate>Thu, 28 Feb 2013 04:22:56 GMT</pubDate>
    </item>
    <item>
      <title>C++ 决策树实现问题：用代码思考</title>
      <link>https://stackoverflow.com/questions/5646120/c-decision-tree-implementation-question-think-in-code</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/5646120/c-decision-tree-implementation-question-think-in-code</guid>
      <pubDate>Wed, 13 Apr 2011 08:01:29 GMT</pubDate>
    </item>
    </channel>
</rss>