<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 03 Feb 2024 18:15:09 GMT</lastBuildDate>
    <item>
      <title>用于头影测量标志检测的随机作物数据预处理</title>
      <link>https://stackoverflow.com/questions/77933062/random-crop-data-preprocessing-for-cephalometric-landmark-detection</link>
      <description><![CDATA[我的任务是头影测量地标定位。我的图像路径的坐标显示在此数据框中。

&lt;表类=“s-表”&gt;
&lt;标题&gt;

文件名
X1
Y1


&lt;正文&gt;

/Images_data/Img0006.png
89
80


/Images_data/Img0008.png
37
70


/Images_data/Img0007.png
50
76


/Images_data/Img0003.png
55
92


/Images_data/Img0005.png
91
64


/Images_data/Img0004.png
100
76




目标：我想在训练图中所示的模型之前在数据预处理步骤中使用随机裁剪。
在此处输入图片描述
我尝试使用随机裁剪来提取 10 个图像块。
从 PIL 导入图像
从随机导入 randrange

img = Image.open(r&quot;/Images_data/Img0006.png&quot;)
a, b = 图片大小

矩阵 = 250
样本=10
样本列表 = []

对于范围内的 i（样本）：
    a1 = randrange(0, a - 矩阵)
    b1 = randrange(0, b - 矩阵)
    Sample_list.append(img.crop((a1, b1, a1 + 矩阵, b1 + 矩阵)))

问题：如何将坐标为 x1,y1 的随机裁剪图像块放入深度学习模型中。]]></description>
      <guid>https://stackoverflow.com/questions/77933062/random-crop-data-preprocessing-for-cephalometric-landmark-detection</guid>
      <pubDate>Sat, 03 Feb 2024 17:24:43 GMT</pubDate>
    </item>
    <item>
      <title>对看起来像噪声的图像进行分类</title>
      <link>https://stackoverflow.com/questions/77932725/classifying-images-that-look-like-noise</link>
      <description><![CDATA[我即将构建一个系统，该系统应该评估如下所示的图像 (900 x 150)，并将其分类为五个类别之一：
看起来像噪音的图像
如果您想知道，它们是 DNA 测序仪流动池中聚类分布的快照。目前正在标记的总数据集约为 30000 张图像。
让系统对这些图像进行分类的最有效方法是什么？
我最初的想法是微调现有的视觉模型（例如，拥抱脸部模型之一），但我对这是否合适存在一些疑问；大多数视觉模型都是根据世界上人和事物的照片进行训练的，因此似乎尝试微调其中一个模型来识别看起来像噪音的图像是行不通的。
如果这个假设是正确的，那么从头开始训练基于 CNN 的模型会是更好的方法吗？也许是 CNN 的修改版本？我读了一篇有趣的研究论文＆quot;使用卷积的图像噪声类型识别具有主成分分析的神经网络”作者将 PCA 融入到他们的 CNN 中。
或者还有其他选择吗？]]></description>
      <guid>https://stackoverflow.com/questions/77932725/classifying-images-that-look-like-noise</guid>
      <pubDate>Sat, 03 Feb 2024 15:43:44 GMT</pubDate>
    </item>
    <item>
      <title>在 Rust-linfa 中加载用于预测的线性回归模型</title>
      <link>https://stackoverflow.com/questions/77932307/loading-a-linear-regression-model-back-up-for-prediction-in-rust-linfa</link>
      <description><![CDATA[我对 Rust 比较陌生，一直在研究 linfa 在 Rust 中的机器学习，特别是线性回归模型。我希望能够保存和加载经过训练的线性回归模型，但我无法找到实现此目的的方法。
到目前为止，我的方法是获取训练中涉及的主要参数，这些参数可以从 linfa 的线性回归实现中获取，并将它们存储在一个可以存储为 JSON 文件的结构中（通过 serde_json 完成）。然而，在此之后我不知道如何将其加载回来进行训练。
以上内容详情如下：
存储训练参数的结构：
struct ModelJson {
    系数：Vec f64 ，
    拦截：f64，
}

存储过程：
let model = lin_reg.fit(&amp;dataset)?;
让 model_json = ModelJson {
    系数： model.params().to_vec(),
    拦截： model.intercept(),
};

存储的数据看起来如何：
{“系数”:[-0.00017907873576254802,-0.00100659702068151,-0.0008275037845519519,0.0004613216043979551,0.00103006349345 99436]，“拦截”：50.525680622870084}

关于序列化和反序列化整个模型，我发现以下信息表明 linfa 中支持相同的操作。
加载和保存模型
这引出了我的第二种方法，其中我使用了 linfa-linear 的 serde 功能（包含 LinearRegression 模型），首先在我的 Cargo.toml 中包含以下内容：
linfa-clustering = {version=&quot;0.7.0&quot;, features=[&quot;serde&quot;]}
根据我对实现的理解，此功能为 LinearRegression 实现了以下功能：
Serde 序列化和反序列化实现 - 派生
上述实现：
&lt;前&gt;&lt;代码&gt;#[cfg_attr(
    特征=“serde”，
    派生（序列化，反序列化），
    serde(crate = “serde_crate”)
)]
/// 可用于进行预测的拟合线性回归模型。
pub struct FittedLinearRegression; {
    截距：F，
    参数：Array1,
}

发现于： linfa-线性导出实现
我的实现如下：
let model = lin_reg.fit(&amp;dataset)?;
让序列化 = serde_json::to_string(&amp;model).unwrap();

但是此方法出现以下错误：
不满足特征边界 `FittedLinearRegression: serde::ser::Serialize`
以下其他类型实现了特征 `serde::ser::Serialize`：
  布尔值
  字符
  大小
  i8
  i16
  i32
  i64
  i128
和其他 133 个rustcClick 以获取完整的编译器诊断
main.rs(82, 22)：此调用引入的绑定所需

是否有其他方法可以做到这一点，或者是否有某种方法可以使这些方法之一发挥作用？

阿莱霍
]]></description>
      <guid>https://stackoverflow.com/questions/77932307/loading-a-linear-regression-model-back-up-for-prediction-in-rust-linfa</guid>
      <pubDate>Sat, 03 Feb 2024 13:44:24 GMT</pubDate>
    </item>
    <item>
      <title>用于机器学习的数据集，至少包含 100 列和 10,000 个原始数据</title>
      <link>https://stackoverflow.com/questions/77931469/data-set-for-machine-learning-with-minimum-100-columns-and-10-000-raws</link>
      <description><![CDATA[我想要一个至少包含 100 列和 10000 行的数据集。您不能使用任何编程语言来生成它，因为它将用于机器学习实践
提前感谢您的帮助🙏]]></description>
      <guid>https://stackoverflow.com/questions/77931469/data-set-for-machine-learning-with-minimum-100-columns-and-10-000-raws</guid>
      <pubDate>Sat, 03 Feb 2024 09:03:55 GMT</pubDate>
    </item>
    <item>
      <title>有人可以解释一下在其他层但不在第一层使用 BatchNorm2d 的目的吗</title>
      <link>https://stackoverflow.com/questions/77931391/can-someone-explain-the-purpose-of-using-batchnorm2d-in-the-other-layers-but-not</link>
      <description><![CDATA[我正在设计Discriminator类，我在github上看到有人的实现，我无法向自己解释为什么batchnormalization被用在conv2，conv3中，但特别是在第一个卷积层中没有，我为您提供了代码类还有转换函数
类鉴别器（nn.Module）：
    def __init__(self, conv_dim = 32):
        super(鉴别器, self).__init__()

        self.conv_dim = conv_dim
        
        self.conv1 = conv(
            3、conv_dim、4、batch_norm = False
        ）
        self.conv2 = conv(
            转换亮度, 转换亮度 * 2, 4
        ）
        self.conv3 = conv(
            转换亮度 * 2, 转换亮度 * 4, 4
        ）
        self.fc = nn.Linear(
            卷积暗度 * 4 * 4 * 4, 1
        ）
    def 前向（自身，x）：
        leaky_relu = F.leaky_relu
        输出=leaky_relu（
            自转换1(x), 0.2
        ）
        输出=leaky_relu（
            self.conv2(输出), 0.2
        ）
        输出=leaky_relu（
            self.conv3(输出), 0.2
        ）
        输出 = out.view(-1, self.conv_dim * 4 * 4 * 4)
        输出 = self.fc(输出)
        返回

我尝试询问 ChatGPT，但它没有给出一致的答案]]></description>
      <guid>https://stackoverflow.com/questions/77931391/can-someone-explain-the-purpose-of-using-batchnorm2d-in-the-other-layers-but-not</guid>
      <pubDate>Sat, 03 Feb 2024 08:35:38 GMT</pubDate>
    </item>
    <item>
      <title>在 TensorFlow 中使用神经网络进行动物检测</title>
      <link>https://stackoverflow.com/questions/77931021/animal-detection-using-neural-network-in-tensorflow</link>
      <description><![CDATA[当我运行最后一部分来训练模型时，我无法检查图像是否有错误。正如你所看到的，我想检测我是否能够训练我的模型来检测动物，例如动物。猫和狗之间。 数据集。
将 pandas 导入为 pd
将 numpy 导入为 np
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Conv2D
从tensorflow.keras.layers导入MaxPooling2D
从tensorflow.keras.layers导入Flatten
从tensorflow.keras.layers导入Dense

# 初始化 CNN
分类器=顺序（）

＃卷积
classifier.add(Conv2D(32,(3,3), input_shape = (64, 64, 3), 激活 = &#39;relu&#39;))

# 池化
classifier.add(MaxPooling2D(pool_size = (2,2)))

# 添加第二个卷积层
classifier.add(Conv2D(32, (3,3), 激活 = &#39;relu&#39;))
classifier.add(MaxPooling2D(pool_size = (2,2)))

# 展平
分类器.add(Flatten())

# 全连接
classifier.add（密集（单位= 128，激活=&#39;relu&#39;））
classifier.add(Dense(单位 = 1, 激活 = &#39;sigmoid&#39;))

# 编译 CNN
classifier.compile（优化器=&#39;adam&#39;，损失=&#39;binary_crossentropy&#39;，指标= [&#39;准确性&#39;]）

# 将 CNN 拟合到图像上

从 keras.preprocessing.image 导入 ImageDataGenerator
train_datagen = ImageDataGenerator(重新缩放 = 1./255,
                                  剪切范围 = 0.2,
                                  缩放范围 = 0.2,
                                  水平翻转=真）

training_set = train_datagen.flow_from_directory(r&quot;D:\神经网络\神经网络完整课程-20240203T042209Z-001\神经网络完整课程-复制\神经网络\training_set&quot;,
                                                目标大小= (64,64),
                                                批量大小=32，
                                                类模式 = &#39;分类&#39;)

test_datagen = ImageDataGenerator（重新缩放= 1./255）
test_set = test_datagen.flow_from_directory(r&quot;D:\神经网络\神经网络完整课程-20240203T042209Z-001\神经网络完整课程-复制\神经网络\test_set&quot;,
                                                目标大小= (64,64),
                                                批量大小=32，
                                                类模式 = &#39;分类&#39;)

分类器.fit（训练集，
               每纪元的步数=700，
               纪元=10，
               验证数据=测试集，
               验证步骤=10)
train_datagen = ImageDataGenerator(重新缩放 = 1./255,
                                  剪切范围 = 0.2,
                                  缩放范围 = 0.2,
                                  水平翻转=真）

training_set = train_datagen.flow_from_directory(r&quot;D:\神经网络\神经网络完整课程-20240203T042209Z-001\神经网络完整课程-复制\神经网络\training_set&quot;,
                                                目标大小= (64,64),
                                                批量大小=32，
                                                类模式 = &#39;分类&#39;)

test_datagen = ImageDataGenerator（重新缩放= 1./255）
test_set = test_datagen.flow_from_directory(r&quot;D:\神经网络\神经网络完整课程-20240203T042209Z-001\神经网络完整课程-复制\神经网络\test_set&quot;,
                                                目标大小= (64,64),
                                                批量大小=32，
                                                类模式 = &#39;分类&#39;)

##### 这里我收到错误
分类器.fit（训练集，
               每纪元的步数=700，
               纪元=10，
               验证数据=测试集，
               验证步骤=10)

错误：
]]></description>
      <guid>https://stackoverflow.com/questions/77931021/animal-detection-using-neural-network-in-tensorflow</guid>
      <pubDate>Sat, 03 Feb 2024 05:55:16 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch CCN 勉强训练</title>
      <link>https://stackoverflow.com/questions/77931017/pytorch-ccn-barely-training</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77931017/pytorch-ccn-barely-training</guid>
      <pubDate>Sat, 03 Feb 2024 05:53:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么变压器可以接受不同长度的输入？</title>
      <link>https://stackoverflow.com/questions/77930859/how-come-transformers-can-accept-inputs-of-different-length</link>
      <description><![CDATA[我读了“你所需要的就是注意力”论文，描述了变压器的架构。在 Transformer 中，有一个叫做 masked multi-head Attention 的组件，仅在解码器部分使用。
问题是，解码器的输入是编码器的输出以及之前生成的标记。并且之前每次迭代生成的token数量不同，但是线性层的神经元数量是相同的。因此，我们必须使用“pad tokens”来实现。屏蔽注意力用于对这些 pad token 给予 0 注意力。
编码器也是如此。输入可以是不同的大小，所以我们还必须使用填充令牌，但在这里，我们不使用屏蔽注意力，我很好奇，为什么？
或者我们不在那里使用填充令牌，而是使用其他东西？
在我与 Chat-GPT 的第一次对话中，它告诉我我们使用 pad 令牌，而在第二次对话中，我们没有使用。我很困惑，我什至不知道该相信什么。]]></description>
      <guid>https://stackoverflow.com/questions/77930859/how-come-transformers-can-accept-inputs-of-different-length</guid>
      <pubDate>Sat, 03 Feb 2024 04:28:27 GMT</pubDate>
    </item>
    <item>
      <title>VertexAIException - 调用 Gemini-Pro API 时列表索引超出范围错误</title>
      <link>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</link>
      <description><![CDATA[我正在以连续的方式调用 Google Gemini-Pro API（大约每分钟 50 个查询）。我相信我已经正确设置了我的 VertexAI 项目和凭据。当我使用的连续查询数量低于恒定条时，查询将运行并且可以很好地收到响应。但是，一旦查询数量超过上述栏，就会出现以下错误：
&lt;块引用&gt;
索引错误 - 列表索引超出范围

请注意，查询数量“bar”是发生此错误的时间取决于每个查询的长度，并且如果查询长度在程序执行期间保持相同，则该错误是一致的。例如，在尝试将查询长度增加大约 20% 后，查询长度从大约 330 个查询下降到大约 60 个查询。
&lt;块引用&gt;
文件
“/Users/user/anaconda3/envs/chat1/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py”，
第 1315 行，文本
返回 self.candidates[0].text
~~~~~~~~~~~~~~~^^^ IndexError：列表索引超出范围

这是什么原因造成的？我已将 VertexAI 服务器位置设置为：“us-central1”，据我所知，该位置应该只有 300 个查询/分钟的配额。由于我连续执行 API 调用，但低于每分钟 60 次查询的速率，因此我认为我处于使用正常范围。我目前正在使用免费的 VertexAI 试用帐户（有 300 美元的免费信用）。
我写的Gemini Pro API调用函数是：
def gemini_response(message: str) -&gt; &gt;字符串：
    # 初始化顶点AI
    vertexai.init(project=“project-id-0123”, location=“us-central1”)

    # 加载模型
    模型 = GenerativeModel(“gemini-pro”)

    # 查询模型
    响应 = model.generate_content(消息)
    返回响应.文本
]]></description>
      <guid>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</guid>
      <pubDate>Sat, 03 Feb 2024 04:05:38 GMT</pubDate>
    </item>
    <item>
      <title>为什么 CreateML 只检测到 JSON 文件中的一个类，而实际上我有 2 个类？</title>
      <link>https://stackoverflow.com/questions/77930615/why-is-createml-only-detecting-one-class-within-my-json-file-when-i-actually-ha</link>
      <description><![CDATA[我正在开发一个对象检测机器学习模型，我决定使用 CreateML 来训练它。我希望我的模型能够区分苹果和橙子。
在开始训练之前，我必须提供我的“训练数据”到 CreateML，其中包括我的 JSON 文件（带有我的图像注释）。在我的 JSON 文件中，我希望模型能够检测到 2 个类（苹果和橙子）。
但由于某种原因，CreateML 仅检测类“apple”。 下面是我的 JSON 文件的一小部分的图像，其中包括“apple”的一个注释和“橙色”。
这是我的 JSON 注释的小图片，包括我的类“apple”和和“橙色”
此外，CreateML 表示我对标签“apple”的计数为 37。手动统计后，我发现这是不准确的；实际上，我对“apple”类的计数是 97。 下面是代表这一点的图像。
CreateML 向我展示了我的 JSON 文件计数。它只向我展示了“apple”类，但我也有一个“橙色”类。
最初，我的 JSON 文件中的一些注释具有不同的属性。例如，“坐标”可以是“坐标”。我的部分注释有时有整数值，有时有小数值。我意识到这可能会带来问题，所以我将它们全部更改为整数值。它最终没有成功。
此外，我还进行了一个小实验。因为我所有的“苹果”都是注释位于文件的最顶部，所有“橙色”都位于文件的最顶部。注释在底部，我翻转顺序看看结果。我把所有的“橙色”都放在了注释在顶部。令我惊讶的是，CreateML 仍然告诉我有一个类，但这次它说该类名为“orange！”为什么会出现这种情况？
有人听说过这个吗？我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77930615/why-is-createml-only-detecting-one-class-within-my-json-file-when-i-actually-ha</guid>
      <pubDate>Sat, 03 Feb 2024 01:48:45 GMT</pubDate>
    </item>
    <item>
      <title>如何修复一层的输出，使其与另一层兼容？</title>
      <link>https://stackoverflow.com/questions/77930107/how-do-i-fix-the-output-of-one-layer-so-it-is-compatible-to-another-layer</link>
      <description><![CDATA[我的输入是由 21 个氨基酸标签编码的序列。它是一个包含 21 个元素 (1x21) 的数组。我想通过嵌入层然后通过卷积层（等等）将其提供给它。但它不允许我添加卷积层。我明白为什么（输出和输入的尺寸不匹配），但我不知道如何修复它。
这是我的代码：
&lt;前&gt;&lt;代码&gt;embeded_vector_size = 9
最大长度 = 21
vocab_size = len(标签)
模型=顺序（）
model.add（嵌入（vocab_size，embeded_vector_size，input_length = max_length，name =“嵌入”））
model.add(Conv2D(filters=64,activation=&#39;relu&#39;, kernel_size=(10, 3), input_shape=(21, 9, 1)))

我收到此错误：
层“conv2d_17”的输入 0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、21、9）

完整回溯：
ValueError Traceback（最近一次调用最后一次）
[177] 中的单元格，第 7 行
      5 model.add(Embedding(vocab_size,embeded_vector_size,input_length=max_length, name=“embedding”))
      6 打印(模型.summary())
----&gt; 7 model.add(Conv2D(filters=3,activation=&#39;relu&#39;, kernel_size=(10, 3), input_shape=(21, 9, 1)))
      8 # 打印(模型.summary())
      9 # 模型.add(MaxPooling2D())

文件 /opt/conda/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204，在 no_automatic_dependency_tracking.._method_wrapper(self, *args, **kwargs)
    第202章
    203 尝试：
--&gt; [第 204 章]
    205 最后：
    206 self._self_setattr_tracking = previous_value # pylint：禁用=受保护访问

文件 /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70，位于filter_traceback..error_handler(*args, **kwargs)
     67、filtered_tb = _process_traceback_frames（e.__traceback__）
     68 # 要获取完整的堆栈跟踪，请调用：
     69 # `tf.debugging.disable_traceback_filtering()`
---&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
     71 最后：
     72 删除filtered_tb

文件/opt/conda/lib/python3.10/site-packages/keras/src/engine/input_spec.py:253，在assert_input_compatibility(input_spec、inputs、layer_name)中
    第251章
    [252] 第252话规范.min_ndim:
--&gt;第253章
    254 f&#39;层“{layer_name}”的输入{input_index} &#39;
    255、“与层不兼容：”
    [256] 第256话
    257 f”发现 ndim={ndim}。 ”
    258 f“接收到完整形状：{tuple(shape)}”
    第259章）
    260 # 检查数据类型。
    第261章

ValueError：层“conv2d_17”的输入 0与图层不兼容：预期 min_ndim=4，发现 ndim=3。收到完整形状：（无、21、9）

如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/77930107/how-do-i-fix-the-output-of-one-layer-so-it-is-compatible-to-another-layer</guid>
      <pubDate>Fri, 02 Feb 2024 22:14:15 GMT</pubDate>
    </item>
    <item>
      <title>RNN 中的输入编码</title>
      <link>https://stackoverflow.com/questions/77929678/input-encoding-in-rnns</link>
      <description><![CDATA[我正在开发一个循环神经网络 (RNN)，它执行以下任务：有 4 个灯，在每次试验期间，其中一个打开，然后打开另一个。目标是根据灯光的组合做出决定。下面是一个试验示例。

我想知道如何对 RNN 的输入数据进行编码：我应该使用相同的 4 个维度来表示灯打开的第一个和第二个实例，还是应该单独表示它们，即使用 8 个维度？]]></description>
      <guid>https://stackoverflow.com/questions/77929678/input-encoding-in-rnns</guid>
      <pubDate>Fri, 02 Feb 2024 20:25:15 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 CIDEr 评估指标来评估图像字幕模型。即使两个句子相同，它也会输出 0.0 [关闭]</title>
      <link>https://stackoverflow.com/questions/77927272/im-using-cider-evaluation-metric-for-image-captioning-model-it-outputs-0-0-eve</link>
      <description><![CDATA[从 pycocoevalcap.cider.cider 导入 Cider
导入 json

# 加载您的参考和候选标题
# 参考标题的格式应为：{image_id: [caption1, caption2, ...]}
# 候选标题的格式应为：[{image_id, Caption}]

Reference_file = &#39;/content/ref.json&#39;
候选文件 = &#39;/content/preds.json&#39;

将 open(reference_file, &#39;r&#39;) 作为 f：
    引用 = json.load(f)

以 open(candidate_file, &#39;r&#39;) 作为 f：
    候选人 = json.load(f)
打印（候选人）
# 创建 CIDEr 记分器
cider_scorer = 苹果酒()

# 计算 CIDEr 分数
cider_score, cider_scores = cider_scorer.compute_score(参考文献, 参考文献)

# 打印 CIDEr 分数
print(&quot;CIDEr 分数：&quot;, cider_score)


它给出输出 0.0。
ref.json 有以下数据
{&#39;1087539207_9f77ab3aaf.jpg&#39;: [&#39;三个人在背景有树木的田野里骑着全地形车&#39;]}&#39;

preds.json 具有以下数据
{&#39;1087539207_9f77ab3aaf.jpg&#39;: [&#39;三个人在背景有树木的田野里骑着全地形车&#39;]}&#39;

Pycocoevalcap 是一个 github 存储库，它实现了 CIDEr，即基于共识的图像描述评估]]></description>
      <guid>https://stackoverflow.com/questions/77927272/im-using-cider-evaluation-metric-for-image-captioning-model-it-outputs-0-0-eve</guid>
      <pubDate>Fri, 02 Feb 2024 13:18:02 GMT</pubDate>
    </item>
    <item>
      <title>MLflow 代理工件访问：无法找到凭据</title>
      <link>https://stackoverflow.com/questions/72886409/mlflow-proxied-artifact-access-unable-to-locate-credentials</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/72886409/mlflow-proxied-artifact-access-unable-to-locate-credentials</guid>
      <pubDate>Wed, 06 Jul 2022 15:40:30 GMT</pubDate>
    </item>
    <item>
      <title>重新训练 Tensorflow 模型</title>
      <link>https://stackoverflow.com/questions/52769607/retrain-tensorflow-model</link>
      <description><![CDATA[我有一个使用对象检测 SSD 移动网络训练的张量流模型。
训练现已完成，我导出了模型推理以进行测试。我的问题是，如果我想稍后使用新的图像数据集重新训练模型，我现在应该在这个阶段做什么以使权重渗透到模型中，以便我可以从那时起重新训练它。我知道有一个冻结脚本，我必须使用它吗？ 
谢谢
阿亚德]]></description>
      <guid>https://stackoverflow.com/questions/52769607/retrain-tensorflow-model</guid>
      <pubDate>Thu, 11 Oct 2018 22:12:07 GMT</pubDate>
    </item>
    </channel>
</rss>