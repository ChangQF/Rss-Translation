<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 23 Sep 2024 12:33:01 GMT</lastBuildDate>
    <item>
      <title>有没有方法可以从具有 200 行的初始数据集扩展我们的数据集。我希望从中至少获得 2000 行来应用 ML 模型</title>
      <link>https://stackoverflow.com/questions/79014580/is-there-any-method-about-how-to-expand-our-dataset-from-initial-dataset-having</link>
      <description><![CDATA[我的项目是利用 ML 技术预测抑郁程度或向孩子的父母提出一些预防措施建议。
我想应用 ML 模型根据我们的数据集预测抑郁程度或其症状。因此，我需要至少 2000 个训练数据元组来训练它们。我怎样才能在不改变属性之间的关系（相关性）的情况下实现这一点。
我的数据集包含许多属性，例如屏幕时间、抑郁程度。如何使用一些代码来解决这个问题。
我尝试使用 CTGAN，但它给了我很多错误。]]></description>
      <guid>https://stackoverflow.com/questions/79014580/is-there-any-method-about-how-to-expand-our-dataset-from-initial-dataset-having</guid>
      <pubDate>Mon, 23 Sep 2024 12:21:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 NumPy 实现基本神经网络的问题</title>
      <link>https://stackoverflow.com/questions/79014083/problem-implementing-a-basic-neural-network-with-numpy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79014083/problem-implementing-a-basic-neural-network-with-numpy</guid>
      <pubDate>Mon, 23 Sep 2024 09:49:44 GMT</pubDate>
    </item>
    <item>
      <title>构建 OCR 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/79013996/building-ocr-model</link>
      <description><![CDATA[问题：
我正在研究一个机器学习模型，其中的输入是图像和实体名称，目标是从图像中提取相应的实体值。例如，如果实体名称是“高度”，并且图像包含门的高度，则模型应该提取此值（例如 6 英尺）以及正确的单位。
输入：
图像：包含对象（例如门）和相关信息（例如高度或其他相关测量值）。
实体名称：关键字，例如“高度”或“重量”指定要从图像中提取的值。
输出：
与图像中的实体相对应的值及其单位（例如，“6 英尺”）。
挑战：
实体值可以出现在图像的不同部分，具有不同的文本格式、字体或样式。
需要识别和提取测量单位（例如米、英尺）以及值。
问题：
处理此类任务的最佳方法或模型架构是什么？
是否有任何特定技术或预训练模型可以帮助将图像和实体名称作为输入结合起来以从图像中提取相应的值？
我应该如何预处理图像和标签以训练此类任务的模型？
任何有关框架和工具的指导、参考或建议都将不胜感激！
我尝试使用带有 CTC 损失的 CNN+RNN，但我的损失接近 20 并且没有进一步减少
这里我附上了我的 google cloab 链接
笔记本链接]]></description>
      <guid>https://stackoverflow.com/questions/79013996/building-ocr-model</guid>
      <pubDate>Mon, 23 Sep 2024 09:27:35 GMT</pubDate>
    </item>
    <item>
      <title>我可以做些什么来提高我在 Kaggle 泰坦尼克号竞赛中的表现？</title>
      <link>https://stackoverflow.com/questions/79013898/what-can-i-do-to-improve-my-performance-on-kaggles-titanic-contest</link>
      <description><![CDATA[我尝试了所有方法，使用决策树和随机森林来预测泰坦尼克号上的幸存者。但都不起作用。
我使用决策树和随机森林模型来预测谁会在泰坦尼克号上幸存。
我选择的特征包括“Pclass”、“性别”、“年龄”、“票价”和“舱位”，对于舱位，我使用 OneHotEncoder 来转换数据，对于性别，我使用 LabelEncoder。
然后我使用决策树来训练模型。当我设置 max_depth=4 并提交时，我的模型在预测中达到了最高的准确度分数，即 0.7799。后来，无论我做什么，我都无法获得更高的分数。我使用了 train_test_split、RandomForestClassifier，我使用了 GridSearch 来测试不同的参数。这些都不起作用，我的分数总是低于 0.7799。每当我设置 max_depth=4 时，无论是决策树还是随机森林，分数始终为 0.7799
如何使用 DecisionTree 或 RandomForest 提高我的性能？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79013898/what-can-i-do-to-improve-my-performance-on-kaggles-titanic-contest</guid>
      <pubDate>Mon, 23 Sep 2024 08:58:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Deepface Deepface.represent 从 ROI 获取嵌入时出错</title>
      <link>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</link>
      <description><![CDATA[我在使用 Deepface 从 Retinaface 识别的裁剪 ROI 获取嵌入时遇到了问题。
我正尝试使用一些名人的数据集（图像）学习对象识别，并可能考虑将其用于我的个人照片库。我尝试使用 Haar Cascade 进行人脸检测，并使用 Open Cv 中的 LBPHFaceRecognize 进行人脸识别，效果很好。然后我想尝试使用 Retinafce 进行人脸检测并获得 ROI。ROI 存储在列表中，并使用 Deepface 从选定的 ROI 获取嵌入并存储在另一个列表中。我正在尝试将嵌入存储到列表中，但我一直得到
 raise ValueError(
ValueError: 无法在 numpy 数组中检测到人脸。请确认图片

是人脸照片或考虑将 force_detection 参数设置为 False。
虽然所有图像都有一张被清楚检测到的人脸。这是我的代码供参考：
import os
import cv2 as cv
from retinaface import RetinaFace
from deepface import DeepFace
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

artist = [&#39;50cent&#39;] # type: ignore #MJ the GOAT!! , &#39;Kanye&#39;, &#39;Eminem&#39;, &#39;MichaelJackson&#39;
ROOT_DIR = &#39;asset/Face_Recon_Dataset&#39; #图像数据集的路径
faces_roi =[]
labels = []
embeddings = []
#现在在脸部坐标上画一个矩形
#脸部范围有：
# x1, y1) = (28, 51) #左上角
# (x2, y2) = (61, 98) #右下角
&quot;&quot;&quot; 这定义了检测到的脸部周围的矩形边界框。
- x1 (28)：脸部的左边缘
- y1 (51)：脸部的上边缘
- x2 (61)：脸部的右边缘
- y2 (98)：脸部的下边缘&quot;&quot;&quot;

def get_roi():
for artist_name in artist:
# 获取艺术家姓名的索引
label = artist.index(artist_name)
image_folder = os.path.join(ROOT_DIR,artist_name) # 获取包含图像的实际文件夹
for artist_images in os.listdir(image_folder): # 列出该目录中的所有图像
image = os.path.join(image_folder,artist_images)
resp = RetinaFace.detect_faces(image)
# 确保人脸存在
if isinstance(resp,dict):
img = cv.imread(image)
for face_id, face_data in resp.items():
# print(face_id)
# print(&quot;x1: &quot;, face_data[&#39;facial_area&#39;][0])
# print(&quot;y1: &quot;, face_data[&#39;facial_area&#39;][1])
# print(&quot;x2: &quot;, face_data[&#39;facial_area&#39;][2])
# print(&quot;y2: &quot;, face_data[&#39;facial_area&#39;][3], &quot;\n&quot;)
# 读取图像

# 检测人脸
x1 = face_data[&#39;facial_area&#39;][0]
y1 = face_data[&#39;facial_area&#39;][1]
x2 = face_data[&#39;facial_area&#39;][2]
y2 = face_data[&#39;facial_area&#39;][3]

# 为人脸绘制边界框 
# faces_rect = cv.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
face_roi = img[y1:y2,x1:x2]

#用其名称标记裁剪后的 roi 人脸
faces_roi.append(face_roi)

labels.append(label)
print(len(faces_roi))
print(len(labels))
print(&quot;已标记和索引的图像&quot;)
print(&quot;正在初始化嵌入过程.....&quot;)
get_embeddings()

def get_embeddings():
&quot;&quot;&quot; 使用 deepface 从每个面部 roi 中提取嵌入&quot;&quot;&quot;
print(&quot;Satarting embedding: 🚀🚀 &quot;)
for roi in faces_roi:
face_roi_resized = cv.resize(roi, (160, 160)) # 将人脸 ROI 调整为 160x160 像素
embedding = DeepFace.represent(face_roi_resized, model_name=&quot;Facenet&quot;)
print(embedding)
embeddings.append(embedding)
print(&quot;Vectors storage in list..&quot;)

get_roi()

# 是时候使用 svm 分类器测试和训练这个坏家伙了
# 将嵌入和索引标记为 numpy 数组
X = np.array(embeddings) #feature
y = np.array(labels) #label

# 将数据分成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 SVM 分类器
svm_model = SVC(kernel=&#39;linear&#39;) # 线性核是嵌入的良好默认值
svm_model.fit(X_train, y_train)

# 评估模型
y_pred = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;SVM 模型准确率：{accuracy * 100:.2f}%&quot;)


有人能帮我理解为什么即使 ROI 已被裁剪，该错误仍然持续存在吗？解决该错误的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</guid>
      <pubDate>Mon, 23 Sep 2024 08:03:12 GMT</pubDate>
    </item>
    <item>
      <title>面部皮肤健康分析 API [关闭]</title>
      <link>https://stackoverflow.com/questions/79013615/face-skin-health-analysis-api</link>
      <description><![CDATA[我想创建一个 React Native 应用来评估皮肤健康状况。我需要测量诸如光泽、斑点、皱纹、纹理、黑眼圈、眼袋、发红、油性、毛孔和水分等因素，并按 1 到 100 的等级显示每个因素的摘要。我遇到了一些用于此目的的 API，但它们非常昂贵。有没有使用 Python 和 OpenCV 的解决方案？是否有可用的模型或指南可以帮助我学习和开发项目的 API？
我尝试了 Google ML Vision 和 Microsoft API，但它们不符合我的要求。我找到了一些 API，但它们非常昂贵。现在我正在寻找自定义模型或数据集。]]></description>
      <guid>https://stackoverflow.com/questions/79013615/face-skin-health-analysis-api</guid>
      <pubDate>Mon, 23 Sep 2024 07:38:51 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用回归算法而不是分类算法？（描述中提供的数据集））[关闭]</title>
      <link>https://stackoverflow.com/questions/79013513/why-are-regression-algorithms-used-instead-of-classification-algorithms-datase</link>
      <description><![CDATA[众所周知，在 ML 中，如果依赖特征本质上是连续的，则应用回归模型。但是，如果依赖特征本质上是分类的，则使用分类算法。
正如您在这张图（https://i.sstatic.net/9Q3wfudK.png）中看到的那样，最大值为。大量数据点重复出现，表明它们正在形成类别。
那么，为什么这里使用回归？
这是数据集：（https://drive.google.com/file/d/1vTIiQ0NZKgBI-EfpGzfPKHx1VaAdEYdH/view?usp=sharing）
我和同学、老师讨论了这个问题。他们都说回归是用来预测的，但没人能解释他们是如何得出应该用回归来代替分类的结论的。]]></description>
      <guid>https://stackoverflow.com/questions/79013513/why-are-regression-algorithms-used-instead-of-classification-algorithms-datase</guid>
      <pubDate>Mon, 23 Sep 2024 07:10:30 GMT</pubDate>
    </item>
    <item>
      <title>在 nn.Transformer 中使用填充掩码时，损失返回为 Nan</title>
      <link>https://stackoverflow.com/questions/79013493/loss-is-returned-as-nan-when-using-padding-mask-in-nn-transformer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79013493/loss-is-returned-as-nan-when-using-padding-mask-in-nn-transformer</guid>
      <pubDate>Mon, 23 Sep 2024 07:04:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 从随机森林中获取观测权重</title>
      <link>https://stackoverflow.com/questions/79012909/obtaining-observation-weights-from-random-forests-using-r</link>
      <description><![CDATA[众所周知，随机森林的预测是 yi 观测值的加权平均值；即：
haty_j(x_j) = w_1(x_j)y_1 + ... + w_n(x_j)y_n, 
其中 x_j 是一个新的数据点，我想要预测其相关的独立变量 y_j。预测是 \haty_j。此外，(x_i, ~ y_i), i = 1, ..., n 是训练数据。我想要导出权重 w_i(x_j) 以执行其他分析。但是，R 只导出预测。我该如何获得这些权重？]]></description>
      <guid>https://stackoverflow.com/questions/79012909/obtaining-observation-weights-from-random-forests-using-r</guid>
      <pubDate>Mon, 23 Sep 2024 01:25:57 GMT</pubDate>
    </item>
    <item>
      <title>预处理 COCO2017 数据集时出错</title>
      <link>https://stackoverflow.com/questions/79012622/error-during-preprocessing-coco2017-dataset</link>
      <description><![CDATA[我正在使用 COCO2017 训练 mobilenetV2 来检测人。我在预处理数据集以将其更改为 Tensorflow 数据集时遇到了困难。当我设法进行更改时，它无法正确解析，导致我在执行 model.fit() 时出错。如何解决这个问题？
# 加载 COCO 数据集
(ds_train, ds_val), ds_info = tfds.load(
&#39;coco/2017&#39;,
split=[&#39;train[:80000]&#39;, &#39;validation&#39;],
with_info=True
)
IMG_SIZE = 224
NUM_CLASSES = 80 # 有效类别的数量，对于无效样本，另加一个

def preprocess(sample):
image = sample[&#39;image&#39;]

# 检查 &#39;objects&#39; 和 &#39;label&#39; 是否存在且有效
if &#39;objects&#39; in sample and &#39;label&#39; in sample[&#39;objects&#39;] and len(sample[&#39;objects&#39;][&#39;label&#39;]) &gt; 0:
label = tf.cast(sample[&#39;objects&#39;][&#39;label&#39;][0], tf.int64)
else:
# 为无效标签分配默认类（例如，额外的类 NUM_CLASSES）
label = tf.cast(NUM_CLASSES, tf.int64)

# 调整图像大小并进行预处理
image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
image = tf.keras.applications.mobilenet_v2.preprocess_input(image)

return image, label

# 应用预处理
ds_train = ds_train.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE)

我得到：
TypeError Traceback (most recent call last) Cell在[48]中，第 20 行 17 返回图像，标签 19 # 应用预处理 ---&gt; 20 ds_train = ds_train.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE) 21 ds_val = ds_val.map(lambda sample: preprocess(sample)).batch(32).prefetch(tf.data.AUTOTUNE) TypeError: 在用户代码中：

TypeError: outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.&lt;lambda&gt;() 需要 1 个位置参数，但给出了 2 个

我尝试了不同的预处理方法。基本上，我需要将输入图像设置为 224x224，以用于 mobilenetv2 模型。我尝试获取一个模型，以便可以在 Himax WE-I 上使用它。]]></description>
      <guid>https://stackoverflow.com/questions/79012622/error-during-preprocessing-coco2017-dataset</guid>
      <pubDate>Sun, 22 Sep 2024 21:09:52 GMT</pubDate>
    </item>
    <item>
      <title>如何使用不同的深度神经网络机器学习算法利用水稻作物图像预测甲烷排放量和用水量[关闭]</title>
      <link>https://stackoverflow.com/questions/79012121/how-to-predict-methane-emmision-and-water-usage-using-image-of-rice-crops-using</link>
      <description><![CDATA[我有水稻作物的图像，但我不知道如何使用深度神经网络的图像来预测甲烷排放量和用水量，而无需数据集的数值。我只有带土壤的水稻作物图像。
我尝试使用图像进行预测。我找不到预测甲烷排放量和用水量的正确代码。
我需要相同的代码]]></description>
      <guid>https://stackoverflow.com/questions/79012121/how-to-predict-methane-emmision-and-water-usage-using-image-of-rice-crops-using</guid>
      <pubDate>Sun, 22 Sep 2024 16:42:44 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用深度学习进行心脏扩大检测。是否可以使用自动编码器提取特征并将其作为集成模型的输入[关闭]</title>
      <link>https://stackoverflow.com/questions/79010975/i-am-doing-cardiomegaly-detection-using-deep-learning-is-it-possible-to-extract</link>
      <description><![CDATA[它复杂吗？结果会是什么样子？从概念上讲，是否可以使用自动编码器进行特征提取并将其作为二元分类任务中的集成模型的输入？
6.定义自动编码器模型
def build_autoencoder(input_shape):
inputs = Input(shape=input_shape)
# Encoder
x = Conv2D(32, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;)(inputs)
x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x)
x = Conv2D(64, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;)(x)
x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x)
x = Conv2D(128, (3, 3),activation=&#39;relu&#39;, padding=&#39;same&#39;)(x)
x = MaxPooling2D((2, 2), padding=&#39;same&#39;)(x)
x = Flatten()(x)
encoded = Dense(128,activation=&#39;relu&#39;)(x)

# 解码器
x = Dense(32 * 32 * 128,activation=&#39;relu&#39;)(encoded)
x = tf.keras.layers.Reshape((32, 32, 128))(x)
x = Conv2D(128, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
x = UpSampling2D((2, 2))(x)
x = Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
x = UpSampling2D((2, 2))(x)
x = Conv2D(32,(3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;)(x)
# 调整此处的上采样以返回原始大小
x = UpSampling2D((2, 2))(x) # 从 (7,7) 更改为 (2,2)
decoded = Conv2D(3, (3, 3),activation=&#39;sigmoid&#39;,padding=&#39;same&#39;)(x)

autoencoder = Model(inputs,decoded)
encoder = Model(inputs,coded)

autoencoder.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;)

return autoencoder,encoder

/**ValueError Traceback (most recent call last)
&lt;ipython-input-17-3da45b43efb2&gt; in &lt;cell line: 37&gt;()
35 
36 # 训练自动编码器
---&gt; 37 autoencoder.fit(train_gen, epochs=10, validation_data=valid_gen)

1 帧
/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 中引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py in binary_crossentropy(target, output, from_logits)
673 for e1, e2 in zip(target.shape, output.shape):
674 如果 e1 不是 None 且 e2 不是 None 且 e1 != e2:
--&gt; 675 raise ValueError(
676 “参数 `target` 和 `output` 必须具有相同的形状。”
677 “收到：”

ValueError：参数 `target` 和 `output` 必须具有相同的形状。收到：target.shape=(None, 224, 224, 3)，output.shape=(None, 256, 256, 3)**/`在此处输入代码`
]]></description>
      <guid>https://stackoverflow.com/questions/79010975/i-am-doing-cardiomegaly-detection-using-deep-learning-is-it-possible-to-extract</guid>
      <pubDate>Sun, 22 Sep 2024 05:54:57 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：参数 clone_function 和 input_tensors 仅支持顺序模型或功能模型</title>
      <link>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</link>
      <description><![CDATA[我正在使用Quantization perceived training，参考网上的lstm代码，想把QAT放进lstm，结果遇到了ValueError。
ValueError Traceback (most recent call last)
&lt;ipython-input-11-00669bb76f9d&gt; in &lt;cell line: 6&gt;()
4 return layer
5 
----&gt; 6 annotated_model = tf.keras.models.clone_model(
7 model,
8 clone_function=apply_quantization_to_dense,

/usr/local/lib/python3.10/dist-packages/tf_keras/src/models/cloning.py in clone_model(model, input_tensors, clone_function)
544 # 自定义模型类的情况
545 if clone_function or input_tensors:
--&gt; 546 raise ValueError(
547 &quot;参数 clone_function 和 input_tensors &quot;
548 &quot;仅支持 Sequential 模型 &quot;

ValueError: 参数 clone_function 和 input_tensors 仅支持 Sequential 模型或 Functional 模型。收到类型为“Sequential”的模型，其中 clone_function=&lt;function apply_quantization_to_dense 位于0x78b727ec4040&gt; 和 input_tensors=None

这是我的代码
import keras
从 keras.layers 导入 LSTM
从 keras.layers 导入 Dense、Activation
从 keras.datasets 导入 mnist
从 keras.models 导入 Sequential
从 keras.optimizers 导入 Adam

learning_rate = 0.001
training_iters = 20
batch_size = 128
display_step = 10

n_input = 28
n_step = 28
n_hidden = 128
n_classes = 10

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(-1, n_step, n_input)
x_test = x_test.reshape(-1, n_step, n_input)
x_train = x_train.astype(&#39;float32&#39;)
x_test = x_test.astype(&#39;float32&#39;)
x_train /= 255
x_test /= 255

y_train = keras.utils.to_categorical(y_train, n_classes)
y_test = keras.utils.to_categorical(y_test, n_classes)

model = Sequential()
model.add(LSTM(n_hidden,
batch_input_shape=(None, n_step, n_input),
unroll=True))

model.add(Dense(n_classes))
model.add(Activation(&#39;softmax&#39;))

adam = Adam(lr=learning_rate)
model.summary()
model.compile(optimizer=adam,
loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])

model.fit(x_train, y_train,
batch_size=batch_size,
epochs=training_iters,
verbose=1,
validation_data=(x_test, y_test))

scores = model.evaluate(x_test, y_test, verbose=0)
print(&#39;LSTM 测试分数：&#39;, scores[0])
print(&#39;LSTM 测试准确率：&#39;, scores[1])

def apply_quantization_to_dense(layer):
if isinstance(layer, tf.keras.layers.LSTM):
return tfmot.quantization.keras.quantize_annotate_layer(layer)
return layer

annotated_model = tf.keras.models.clone_model(
模型，
clone_function=apply_quantization_to_dense，
)
]]></description>
      <guid>https://stackoverflow.com/questions/78796155/valueerror-arguments-clone-function-and-input-tensors-are-only-supported-for-se</guid>
      <pubDate>Fri, 26 Jul 2024 03:41:57 GMT</pubDate>
    </item>
    <item>
      <title>希望使用 Google ML 套件识别面部类型</title>
      <link>https://stackoverflow.com/questions/75714844/looking-to-identify-face-types-using-google-ml-kit</link>
      <description><![CDATA[我想使用 Google ML 找出一个人的脸型，比如方形、椭圆形或圆形。我知道我需要从不同角度测量脸的长度和宽度来确定脸型。但是，API 并没有给我提供这些。任何输入都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/75714844/looking-to-identify-face-types-using-google-ml-kit</guid>
      <pubDate>Sun, 12 Mar 2023 17:03:06 GMT</pubDate>
    </item>
    <item>
      <title>不同分类器的 TPR 和 FPR 曲线 - R 中的 kNN、NaiveBayes、决策树</title>
      <link>https://stackoverflow.com/questions/34335074/tpr-fpr-curve-for-different-classifiers-knn-naivebayes-decision-trees-in-r</link>
      <description><![CDATA[我正在尝试理解并绘制不同类型分类器的 TPR/FPR。我在 R 中使用 kNN、NaiveBayes 和决策树。使用 kNN，我执行以下操作：
clnum &lt;- as.vector(diabetes.trainingLabels[,1], mode = &quot;numeric&quot;)
dpknn &lt;- knn(train = diabetes.training, test = diabetes.testing, cl = clnum, k=11, prob = TRUE)
prob &lt;- attr(dpknn, &quot;prob&quot;)
tstnum &lt;- as.vector(diabetes.testingLabels[,1], mode = &quot;numeric&quot;)
pred_knn &lt;- prediction(prob, tstnum)
pred_knn &lt;- performance(pred_knn, &quot;tpr&quot;, &quot;fpr&quot;)
plot(pred_knn, avg= &quot;threshold&quot;, colorize=TRUE, lwd=3, main=&quot;Knn=11 的 ROC 曲线&quot;)

其中 diabetes.trainingLabels[,1] 是我想要预测的标签（类）向量，diabetes.training 是训练数据，diabetes.testing 是测试数据。
该图如下所示：

存储在 prob 属性中的值是一个数字向量（0 到 1 之间的小数）。我将类标签因子转换为数字，然后可以将其与 ROCR 库中的预测/性能函数一起使用。不能 100% 确定我做得对，但至少它有效。
但是对于 NaiveBayes 和决策树，在预测函数中指定 prob/raw 参数，我得到的不是单个数字向量，而是一个列表向量或矩阵，其中指定了每个类的概率（我猜），例如：
diabetes.model &lt;- naiveBayes(class ~ ., data = diabetesTrainset)
diabetes.predicted &lt;- predict(diabetes.model, diabetesTestset, type=&quot;raw&quot;)

并且 diabetes.predicted 是：
tested_negative checked_positive
[1,] 5.787252e-03 0.9942127
[2,] 8.433584e-01 0.1566416
[3,] 7.880800e-09 1.0000000
[4,] 7.568920e-01 0.2431080
[5,] 4.663958e-01 0.5336042

问题是如何使用它来绘制 ROC 曲线，以及为什么在 kNN 中我得到一个向量，而对于其他分类器，我得到两个类别的向量是分开的？]]></description>
      <guid>https://stackoverflow.com/questions/34335074/tpr-fpr-curve-for-different-classifiers-knn-naivebayes-decision-trees-in-r</guid>
      <pubDate>Thu, 17 Dec 2015 12:51:31 GMT</pubDate>
    </item>
    </channel>
</rss>