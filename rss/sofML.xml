<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Sun, 23 Feb 2025 21:16:28 GMT</lastBuildDate>
    <item>
      <title>LSTM培训是否在恢复学习后重置？</title>
      <link>https://stackoverflow.com/questions/79461981/does-lstm-training-reset-after-resuming-learning</link>
      <description><![CDATA[我有一个称为processed_data的数据库，其中包含像这样构成的单元格：
  0.980999999999767 0.945912306864893 1
1.46300000000338 0.926617136227153 1
0.51199999995169 0.868790509137634 2
1.00600000000122 0.978074194186882 1
0.995999999999185 0.884817478795566 2
1.12400000000343 0.740093883803231 2
1.3539999999936 0.418494628137842 2
0.65399999994994 0.399199457500103 2
1.00600000000122 0.438938088213894 2
0.99000000003434 0.566427539286267 2
 
第一列表示自上一行以来已通过的秒数，第二列是在给定时间（归一化）的值，第三列包含分类值1和2。前两个列是预测指标，第三列是目标。
每个细胞代表一天，我有大约215天的数据，每个数据都有不同的观测值。在培训期间，我每天都会停止学习时，当达到给定的一天的最后一批时，请停止学习，然后在第二天加载数据并恢复培训。
问题在于，当训练加载第二天的数据后训练恢复时，好像网络已经完全重置并开始从头开始学习。它反复产生相同的精度结果（不包括第一次迭代），只有损失值的略有变化。其余迭代的准确性保持不变，总是产生相同的值，就好像网络根本没有学习。这是第1天输出的示例：
  1。天
    迭代时期的时代学习训练训练训练准确
    _________ _____ ___________ ______________________________________________________
            1 1 00:00:00 0.001 0.69781 40.625
           50 1 00:00:00 0.001 0.65881 64.844
          100 1 00:00:00 0.001 0.70176 50.781
          117 1 00:00:00 0.001 0.63057 69.531
训练停止：Max Epochs完成
...
...
...
1。天
    迭代时期的时代学习训练训练训练准确
    _________ _____ ___________ ______________________________________________________
            1 1 00:00:00 0.001 0.70017 41.406
           50 1 00:00:00 0.001 0.65913 64.844
          100 1 00:00:00 0.001 0.6985 50.781
          117 1 00:00:00 0.001 0.62994 69.531
训练停止：Max Epochs完成
...
...
...
1。天
    迭代时期的时代学习训练训练训练准确
    _________ _____ ___________ ______________________________________________________
            1 1 00:00:00 0.001 0.69753 42.188
           50 1 00:00:00 0.001 0.6619 64.844
          100 1 00:00:00 0.001 0.70356 50.781
          117 1 00:00:00 0.001 0.6291 69.531
训练停止：Max Epochs完成
...
...
...
 
这是我的代码段：
  %%定义培训选项 
train_opts =训练（...
    “亚当”，...
    初始learternrate = 0.001，...
    minibatchsize = 128，...
    图=; none＆quot; ...
    详细= true，...
    maxepochs = 1，...
    Shuffle =“从不
    指标=“准确性” ...
    ）；

%%定义网络。
net = dlnetwork;
temp_net = [
    sequenceInputlayer（2，“名称”，“输入”）
    lstmlayer（256，“ name”&#39;lstm; quot&#39;ouppotemode“
    dropoutlayer（0.5，“名称”，“ dropfout”）
    完整连接的layerer（2，“名称”，“输出”）
    SoftMaxlayer];
net = addlayers（net，temp_net）;
net =初始化（net）;

％清理助手变量
清除temp_net;

%%每天加载数据并训练网络。
num_of_epochs = 30;
train_data_length = round（长度（processed_data） * 0.9）;
train_data = processed_data（1：train_data_length）;

for epoch = 1：num_of_epochs
    一天= 1：train_data_length
        如果train_opts.verbose
            disp（Day +&#39;Day&#39;Day&#39;）
        结尾
        train_x = processed_data {day}（：，1：2）;
        train_x = dlarray（train_x，“ bct”）;
        train_y =分类（processed_data {day}（：，3））;
    
        net = trainnet（train_x，train_y，net，&#39;crossentropy＆quot; train_opts）;
    结尾
结尾
 
目标是创建一种基于预测因子的LSTM算法，可以预测该值将来是否会增加（2）或减少（1）。]]></description>
      <guid>https://stackoverflow.com/questions/79461981/does-lstm-training-reset-after-resuming-learning</guid>
      <pubDate>Sun, 23 Feb 2025 21:11:38 GMT</pubDate>
    </item>
    <item>
      <title>您将如何从YouTube视频中产生一个问题回答数据集？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79461489/how-would-u-generate-a-question-answering-dataset-from-an-youtube-video</link>
      <description><![CDATA[我需要使用特定YouTube视频的成绩单微调模型。但是，微调通常需要一个提问的数据集，并且每次添加新的YouTube视频作为知识时，我都不想手动创建数据集。
这是我到目前为止尝试的：

 使用YouTube-Transcript-api等工具从YouTube视频中提取成绩单。

 手动从成绩单中手动创建一个小型Q＆amp;用于微调的数据集。


手动过程是耗时的，我正在寻找一种从成绩单中自动化提问数据集的方法。
如何从YouTube视频成绩单中自动生成Q＆amp;用于微调模型的数据集？是否有任何工具，库或技术可以帮助您解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79461489/how-would-u-generate-a-question-answering-dataset-from-an-youtube-video</guid>
      <pubDate>Sun, 23 Feb 2025 15:56:17 GMT</pubDate>
    </item>
    <item>
      <title>如何使用Docling库从DOCX文件中提取页面上的HTML内容，以检测页面断路？</title>
      <link>https://stackoverflow.com/questions/79461458/how-to-extract-page-wise-html-content-from-docx-files-using-docling-library-by-d</link>
      <description><![CDATA[我使用Docling和PYPDF2成功实现了PDF文件的页面html提取。这是我当前代码对PDF的作用：

使用PYPDF2将PDF分为单个页面
使用Docling的DocumentConverter 将每个页面转换为HTML
用嵌入式图像提取HTML内容
添加元数据（页码，文档ID，文件名）
将所有内容保存到JSON结构

重要说明：我首先将pdf分为单个页面的原因是因为docling的save_as_html（）和export_to_html（）函数在完整的文档对象（而不是在单个页面上）起作用。要获取页面html内容，我需要创建临时的单页PDF并分别转换一个。
这是我每页获得的示例JSON结构：
{
＆quot“ page”：“第1页”
＆quot“ content＆quot”：“”
“元数据：{{
＆quot“ docutsId”：; quot“ uuid; quot”
“文件名”：“ document.pdf”
&#39;page_number＆quot”：1，
＆quot“ total_pages”：总计
}
} 
现在，我需要为DOCX文件实现相同的功能。对我来说，DOCX文件包含诸如标题，页脚和页面断开之类的元素，我们可以使用这些页面断开将内容分为页面。。
我正在使用Docling库进行转换（DOCX到HTML），但是我无法识别或检测到DOCX文件中的页面中断。由于Docling的HTML转换在完整的文档上起作用，因此我需要根据页面断路首先将DOCX内容拆分，类似于我处理PDF的方式。
问题：

如何使用Docling在DOCX文件中检测页面中断？
是否有一种方法可以根据这些页面断开以创建单独的文档对象来拆分DOCX内容？
如果Docling不直接支持此内容，是否还有其他python库，我应该与文档一起使用以检测和拆分页面中断？

我尝试查看文档文档，但找不到有关DOCX文件中处理页面中断的信息。
我正在使用的相关库：

文档
 python-docx（如果需要）
]]></description>
      <guid>https://stackoverflow.com/questions/79461458/how-to-extract-page-wise-html-content-from-docx-files-using-docling-library-by-d</guid>
      <pubDate>Sun, 23 Feb 2025 15:19:41 GMT</pubDate>
    </item>
    <item>
      <title>有效的网络验证损失Stagnan不会降低，验证精度会提高，但Stagnan [封闭]</title>
      <link>https://stackoverflow.com/questions/79459417/efficientnetb0-validation-loss-stagnan-not-decreases-validation-accuracy-increa</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79459417/efficientnetb0-validation-loss-stagnan-not-decreases-validation-accuracy-increa</guid>
      <pubDate>Sat, 22 Feb 2025 10:14:34 GMT</pubDate>
    </item>
    <item>
      <title>拟合功能在时期1之后停止</title>
      <link>https://stackoverflow.com/questions/79457237/fit-function-stops-after-epoch-1</link>
      <description><![CDATA[我已经实现了此功能以适合模型
  def fit_model（型号，x_train_sequence_tensor，y_train_sequence_tensor，epochs，val_set，time_windows，sualer）：
    
    x_column_list = [val_set.columns.to_list（）中的项目中的项目，如果不在[&#39;date&#39;，&#39;deres&#39;&#39;&#39;，&#39;rank&#39;，&#39;rank_group&#39;，&#39;counts&#39;，&#39;counts&#39;，&#39;target&#39;]]中
    x_val_set = val_set [x_column_list] .Round（2）
                    
    X_VAL_SET [X_VAL_SET.COLUMNS] = SCALER.TRANSFORM（X_VAL_SET [X_VAL_SET.COLUMNS]）
    x_val_sequence = get_feature_array（x_val_set，x_column_list，time_windows）
    X_VAL_SECONCE_TENSOR = TF.CONVERT_TO_TENSOR（X_VAL_SECERESE，dtype = tf.float32）
    
    y_column_list = [&#39;target&#39;]                
    y_val_set = val_set [y_column_list] .Round（2）
    y_val_sequence = get_feature_array（y_val_set，y_column_list，time_windows）
    y_val_sequence_tensor = tf.convert_to_tensor（y_val_sequence，dtype = tf.float32）

                    
    历史= model.fit（x_train_sequence_tensor，y_train_sequence_tensor，epochs， 
                        验证_data =（x_val_secorence_tensor，y_val_sepence_tensor））
    返回模型，历史记录

 
但是当我称其为时
  fitted_model，history = fit_model（模型，x_train_secorence_tensor，y_train_secorce_tensor， 
                    epochs = 100，val_set = val_set，time_windows = 90，scaleer = scaleer）
 
它在第一个时期后停止。它不能按要求所有100个运行。
我试图在函数调用之外打电话给它。
 `＃步骤3.2：安装模型 +我们通过一些验证
                                                ＃监视验证损失和指标
                                                ＃在每个时代的末尾
                    x_val_set = val_set [x_column_list] .Round（2）
                    
                    ＃x_val_set.values = scaler.transform（x_val_set.values）
                    
                    X_VAL_SET [X_VAL_SET.COLUMNS] = SCALER.TRANSFORM（X_VAL_SET [X_VAL_SET.COLUMNS]）
                    x_val_sequence = get_feature_array（x_val_set，x_column_list，90）
                    X_VAL_SECONCE_TENSOR = TF.CONVERT_TO_TENSOR（X_VAL_SECERESE，dtype = tf.float32）
                    
                    y_val_set = val_set [y_column_list] .Round（2）
                    y_val_sequence = get_feature_array（y_val_set，y_column_list，90）
                    y_val_sequence_tensor = tf.convert_to_tensor（y_val_sequence，dtype = tf.float32）

                    
                    triench_history = cnn1d_bilstm_model.fit（x_train_sequence_tensor，y_train_sequence_tensor，epochs = 200， 
                                                            ＃我们通过一些验证
                                                            ＃监视验证损失和指标
                                                            ＃在每个时代的末尾
                                                            验证_data =（x_val_secorence_tensor，y_val_sepence_tensor））
 
我在做什么错？]]></description>
      <guid>https://stackoverflow.com/questions/79457237/fit-function-stops-after-epoch-1</guid>
      <pubDate>Fri, 21 Feb 2025 11:26:50 GMT</pubDate>
    </item>
    <item>
      <title>我面临的问题是阅读Python中的MT数据[关闭]</title>
      <link>https://stackoverflow.com/questions/79456454/i-am-facing-problem-to-read-mt-data-in-python</link>
      <description><![CDATA[错误是
  parserError                               
Trackback（最近的电话最后一次）
[9]中的单元，第1行
----＆gt; 1 data = pd.read_csv（r＆quot; c：\ users \ rosha \ oneDrive \ desktop \ vbox data \ vbox.csv＆quot;

File ~\anaconda3\Lib\site-packages\pandas\io\parsers\readers.py:912, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace， skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, FoqueChar，引用，双语，EscapeChar，评论，编码，encoding_errors，言语，on_bad_lines，delim_whitespace，low_memory，memory_map，float_map，float_precision，storage_options，storage_options，dtype_backend），dtype_backend）
    899 kwds_defaults = _refine_defaults_read（
    900方言，
    901定界符，
   （...）
    908 dtype_backend = dtype_backend，
    909）
    910 kwds.update（kwds_defaults）
 - ＆gt; 912返回_read（filepath_or_buffer，kwds）

文件〜\ anaconda3 \ lib \ site-packages \ pandas \ io \ parsers \ parsers \ readers.py：583，in _read（filepath_or_buffer，kwds）
    580返回解析器
    582与解析器：
 - ＆gt; 583返回parser.Read（nrows）

file〜 \ anaconda3 \ lib \ site-packages \ pandas \ io \ parsers \ parsers \ readers.py：1704，in textfileReader.read.read（self，nrows）
   1697 nrows = validate_integer（&#39;nrows＆quot; nrows）
   1698尝试：
   1699＃错误：“ parserbase”没有“读”属性。
   1700（
   1701索引，
   1702列，
   1703 col_dict，
 - ＆gt; 1704）= self._engine.Read（＃类型：忽略[attr-defined]
   1705 nrows
   1706）
   1707除例外：
   1708 self.close（）

file〜 \ anaconda3 \ lib \ lib \ site-packages \ pandas \ io \ parsers \ c_parser_wrapper.py：234，在cparserwrapper.read（self，nrows）中
    232尝试：
    233如果self.low_memory：
 - ＆gt; 234块= self._reader.read_low_memory（nrows）
    235＃破坏了大块
    236数据= _concatenate_chunks（块）

文件〜\ anaconda3 \ lib \ lib \ site-packages \ pandas \ _libs \ parsers.pyx：814，in pandas._libs.parsers.parsers.textreader.read_low_memory（）

文件〜\ anaconda3 \ lib \ site-packages \ pandas \ _libs \ parsers.pyx：875，在pandas._libs.parsers.parsers.textreader._read_rows（）

文件〜\ anaconda3 \ lib \ site-packages \ pandas \ _libs \ parsers.pyx：850，在pandas._libs.parsers.parsers.textreader._tokenize_rows（）

文件〜\ anaconda3 \ lib \ lib \ site-packages \ pandas \ _libs \ parsers.pyx：861，in pandas._libs.parsers.parsers.textreader._check_tokenize_tokenize_status（）

file〜 \ anaconda3 \ lib \ site-packages \ pandas \ _libs \ parsers.pyx：2029，in pandas._libs.parsers.raise_parser_parser_error（）

ParserError：错误令牌数据。 C错误：第6行中的预期2个字段，Saw 20
 
如何解决它？]]></description>
      <guid>https://stackoverflow.com/questions/79456454/i-am-facing-problem-to-read-mt-data-in-python</guid>
      <pubDate>Fri, 21 Feb 2025 06:01:03 GMT</pubDate>
    </item>
    <item>
      <title>将Python ML模型与Flutter Client进行集成[关闭]</title>
      <link>https://stackoverflow.com/questions/79452057/integrating-python-ml-models-with-flutter-client-locally</link>
      <description><![CDATA[我在工作中面临挑战，我需要在我的客户端应用程序上运行许多Python ML模型，因为使某些模型在服务器上运行。。
除了项目资产中的张量流光模型和实施Python模型的同事告诉我，他不能以Tflite模型导出某些模型。，我没有实验。
有一个软件包（OnnxRuntime）使用ONNX型号，并在我的flutter代码中使用了其中的功能，例如Dart FFI，我以前使用此软件包在我的颤音代码中运行C ++功能，并且可以很好地运行。我的牛头人说，我有同样的问题，他不能将所有模型提取到ONNX模型中，但这让我认为有一种方法可以在我的Flutter应用程序中使用Python代码，例如Dart FFI，我知道它不会一样，因为Python是一种解释语言，我无法从中脱离共享对象，所以我的问题是：是否有一种方法可以在我的客户端应用程序或Python ML模型中使用Python代码，而无需使用TFLITE或OnnxRuntime？
 ps：这是一个移动应用程序，因此像运行本地Pyhton服务器这样的解决方案无法与我合作！]]></description>
      <guid>https://stackoverflow.com/questions/79452057/integrating-python-ml-models-with-flutter-client-locally</guid>
      <pubDate>Wed, 19 Feb 2025 16:13:44 GMT</pubDate>
    </item>
    <item>
      <title>训练LSTM的时间序列不同</title>
      <link>https://stackoverflow.com/questions/79449572/train-lstm-for-time-series-with-varying-lengths</link>
      <description><![CDATA[我正在训练LSTM进行时间序列预测，其中数据来自不规则间隔的传感器。我正在使用最后5分钟的数据来预测下一个值，但是某些序列比其他序列大。
我的输入阵列的形状是（611,1200,15），其中（示例，时间段，功能）。每个样本的第二维度均未完成，因此我用NP.NAN值填充了丢失的数据。例如，示例（1，：，：）有1000个时间段和200 np.nan。
训练时，损失等于Nan。
我在做什么错？我该如何训练？
这是我尝试训练LSTM的尝试：
  def lstmfit（y，x，n_hidden = 1，n_neurons = 30，Learning_rate = 1E-2）：   
    lstm = sequention（）
    lstm.add（basking（mask_value = np.nan，input_shape =（none，x. shape [2]）））））））
        
    对于范围（n_hidden）的图层：
        lstm.add（lstm（n_neurons，， 
                      激活=“ tanh”
                      recurrent_activation =＆quot; sigmoid＆quot;
                      return_sequences = true））
        
    lstm.add（密集（1））
    
    lstm.compile（loss =; mse; optimizer =; adam＆quot;）
    
    早期_STOPPING =早期踩踏（Monitor =&#39;损失&#39;，耐心= 10，详细= 1，restore_best_weights = true）
  
    
    lstm.-fit（x，y.Reshape（-1），epochs = 100，callbacks = [arfore_stopping]）
    
    y_train_fit = lstm.predict（x）
    
    返回lstm，y_train_fit
 
模型的摘要：
  lstm.summary（）
型号：sequential_9＆quot
__________________________________________________________________________
 图层（类型）输出形状参数＃   
=============================================== ===============
 masking_7（掩模）（无，无，15）0         
                                                                 
 LSTM_6（LSTM）（无，无，30）5520      
                                                                 
 密集_10（密集）（无，无，1）31        
                                                                 
=============================================== ===============
总参数：5551（21.68 kb）
可训练的参数：5551（21.68 kb）
不可训练的参数：0（0.00字节）
__________________________________________________________________________
 
和训练的第一个时期：
 时期1/100
18/18 [=======================================
时代2/100
18/18 [========================================
时期3/100
18/18 [====================================
 ]]></description>
      <guid>https://stackoverflow.com/questions/79449572/train-lstm-for-time-series-with-varying-lengths</guid>
      <pubDate>Tue, 18 Feb 2025 20:47:34 GMT</pubDate>
    </item>
    <item>
      <title>增加自我注意力后，CNN网络的非确定性行为</title>
      <link>https://stackoverflow.com/questions/79439790/non-deterministic-behavior-of-a-cnn-network-after-adding-self-attention</link>
      <description><![CDATA[当我添加nlbloclos时，在我的网络（简单CNN）中添加了一个自发层时，网络的结果不再可重现，当我再次训练它时，结果是不同的。但是，当我删除网络中的nlblocks时，它是确定性的。
这是代码：
  os.environ [＆quot&#39;cuda_visible_devices;
os.environ [&#39;tf_cpp_min_log_level&#39;] =&#39;3&#39;
种子= 42
os.environ [&#39;tf_deterministic_ops&#39;] =&#39;1&#39;
tf.config.experiment.enable_op_determinism（）

os.environ [&#39;pythonhashseed&#39;] = str（seed）
os.environ [&#39;tf_cudnn_deterministic&#39;] =&#39;1&#39; 

随机种子（种子）
np.random.seed（种子）
tf.random.set_seed（种子）

tf.keras.backend.set_floatx（&#39;float64&#39;）



nlblock类（层）：
    def __init __（self，num_channels，** kwargs）：
        super（nlblock，self）.__ init __（** kwargs）
        self.num_channels = num_channels
        self.theta = conv1d（filters = num_channels，kernel_size = 1，步幅= 1，padding =＆quort; same＆quot;）
        self.phi = conv1d（filters = num_channels，kernel_size = 1，步幅= 1，padding =＆quort; same＆quot;）
        self.g = conv1d（filters = num_channels，kernel_size = 1，步幅= 1，padding =; same＆quort;）
        self.attention_layer =注意（）＃keras注意层

    def呼叫（self，输入）：
        ＃变换功能图
        query = self.theta（输入）＃query（q）
        key = self.phi（输入）＃key（k）
        value = self.g（输入）＃value（v）

        ＃应用注意力层
        activation_output = self.attention_layer（[查询，键，值]）

        ＃残差连接
        返回输入 +注意_Output
        
＃定义NL注意的模型
def build_model（）：
    优化器= ADAM（Learning_rate = 0.002，beta_1 = 0.89，beta_2 = 0.995）

    输入= tf.keras.input（shape =（num_time_steps，1））＃输入层
    
    ＃Conv Block 1
    x = conv1d（filters = 64，kernel_size = 3，activation =&#39;relu&#39;，padding =&#39;same&#39;）（输入）
    x = batchnormatorization（）（x）
    x = nlblock（num__channels = 64）（x）＃nl注意
    x = maxpooling1d（pool_size = 2）（x）

    ＃Conv 2 2
    x = conv1d（filters = 32，kernel_size = 3，activation =&#39;relu&#39;，padding =&#39;same&#39;）（x）
    x = batchnormatorization（）（x）
    x = nlblock（num__channels = 32）（x）＃nl注意
    x = maxpooling1d（pool_size = 2）（x）
    x =辍学（0.1）（x）

    ＃Conv Block 3
    x = conv1d（filters = 16，kernel_size = 3，activation =&#39;relu&#39;，padding =&#39;same&#39;）（x）
    x = batchnormatorization（）（x）
    x = nlblock（num__channels = 16）（x）＃nl注意
    x = maxpooling1d（pool_size = 2）（x）
    x =辍学（0.35）（x）

    ＃完全连接的图层
    x = flatten（）（x）
    x =密集（40，激活=&#39;relu&#39;）（x）
    x =辍学（0.35）（x）
    输出=致密（20）（x）＃回归的输出层
    
    ＃编译模型
    型号= tf.keras.model（输入，输出）
    model.compile（优化器=优化器，lose = root_mean_squared_error，metrics = [&#39;mean_absolute_error&#39;]）
    
    返回模型

model = build_model（）

redy_lr = reducelronplateau（monitor =&#39;val_loss&#39;，因子= 0.6，耐心= 25，min_lr = 1e-6）
早期_Stopping =早期踩踏（Monitor =&#39;Val_loss&#39;，Patience = 30，Restore_best_weights = true）
＃步骤5：训练模型
历史= model.fit（x_train_scaled，y_train，validation_data =（x_val_scaled，y_val），
                epochs = 180，batch_size = 90，callbacks = [redion_lr，ropand_stopping]，冗长= 0）

test_loss，test_mae = model.evaluate（x_test_scaled，y_test，batch_size = len（x_test_scaled），词= 0）
 
我还使用TF.Matmul（）和SoftMax使用了服装注意块，但没有任何变化。]]></description>
      <guid>https://stackoverflow.com/questions/79439790/non-deterministic-behavior-of-a-cnn-network-after-adding-self-attention</guid>
      <pubDate>Fri, 14 Feb 2025 15:08:45 GMT</pubDate>
    </item>
    <item>
      <title>比较通过转移学习训练的几种不同Yolov8S模型的重量和偏见</title>
      <link>https://stackoverflow.com/questions/76220101/comparing-the-weights-and-biases-of-several-different-yolov8s-models-trained-thr</link>
      <description><![CDATA[我有3种不同的Yolov8s模型，我想评估：

  yolov8s接受了普通模型训练。Train（）命令

  yolo8vs模型，训练有冷冻骨干

  yolov8s模型，训练有所有层冰冻


我正在使用回调功能冻结重量，请参见下文：
  def freeze_layer（训练器）：
    型号= Trainer.Model
    num_freeze = 10
    打印（f＆quot {num_freeze}层；）
    冻结= [f&#39;model。{x}。&#39;&#39;对于x范围（num_freeze）]＃冻结层
    对于k，v in Model.Named_pa​​rameters（）：
        v.requires_grad = true＃火车所有层
        如果有（x中的x in for x in freeze）：
            打印（f&#39;Freezing {k}&#39;）
            v.requires_grad = false
    打印（f＆quot {num_freeze}层被冷冻。＆quot;）

如果__name__ ==＆quot __ Main __＆quot;：
    模型= Yolo（Yolov8s.pt）
    model.add_callback（＆quot; on_train_start＆quort＆quot; freeze_layer）
    模型。
        data =&#39;coco128.yaml＆quort;
        时代= 300，
        IMGSZ = 640
        ）
 
我希望能够评估这三个神经网络之间的权重和偏见的差异。我想看看哪些神经元在转移学习后死亡，并评估所有三个神经网络之间的主要差异。是否有任何标准解决方案可用于获得对神经网络的这种见解？
我已经比较了Yolo提供的标准培训指标，例如地图和损失，一切都是预期的。冻结的层越多，表现较差。当我冻结整个网络时，性能比其他两个实例要差得多。
我几乎不知道在比较同一网络架构时从哪里开始，但训练有不同的训练。
任何帮助都非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/76220101/comparing-the-weights-and-biases-of-several-different-yolov8s-models-trained-thr</guid>
      <pubDate>Wed, 10 May 2023 15:13:36 GMT</pubDate>
    </item>
    <item>
      <title>如何标记卫星图像以进行图像分割？ [关闭]</title>
      <link>https://stackoverflow.com/questions/64553426/how-to-label-satellite-images-for-image-segmentation</link>
      <description><![CDATA[我想在卫星图像中检测地雷。最初，我构建了一个模型，每个图像具有多个标签并训练它以对图像进行分类。
但是，我想使用以下提到的图像分割技术： https://towardsdataScience.com/dstl-satellite-imagery-contest-on-kaggle-2f3ef7b8ac40  
我通过AWS S3存储桶下载了所需的图像。我想标记我从频段文件生成的多光谱图像的每个像素。
但是，我在如何标记方面面临困难。
是否有任何开源或其他工具可以执行相同的操作？
图像是12个频段多光谱卫星图像。]]></description>
      <guid>https://stackoverflow.com/questions/64553426/how-to-label-satellite-images-for-image-segmentation</guid>
      <pubDate>Tue, 27 Oct 2020 11:25:11 GMT</pubDate>
    </item>
    <item>
      <title>SSD的预测图像ID和框</title>
      <link>https://stackoverflow.com/questions/60398211/predicted-image-id-and-box-from-ssd</link>
      <description><![CDATA[如何从SSD中找到预测的图像ID和盒子，我正在使用此 github链接这是我要保存图像ID和的测试功能盒子
  def Test（加载器，NET，CRITERION，设备）：
    net.eval（）
    Running_loss = 0.0
    Running_Regression_loss = 0.0
    Running_Classification_loss = 0.0
    num = 0
    对于_，枚举（加载程序）中的数据：
        图像，盒子，标签=数据
        图像=图像TO（设备）
        box = box.to（设备）
        标签= labels.to（设备）
        num += 1
    
        使用Torch.no_grad（）：
            信心，位置= NET（图像）
            recression_loss，分类_loss = Criterion（置信度，位置，标签，盒子）
            损失= recression_loss + classification_loss
    
        running_loss += loss.item（）
        Running_Regression_loss += regression_loss.item（）
        Running_classification_loss += classification_loss.item（）
    返回Runnun_loss / num，Runnun_Regression_loss / num，Run
 ]]></description>
      <guid>https://stackoverflow.com/questions/60398211/predicted-image-id-and-box-from-ssd</guid>
      <pubDate>Tue, 25 Feb 2020 15:36:01 GMT</pubDate>
    </item>
    <item>
      <title>训练芯片和目标图像格式在Tensorflow [封闭]</title>
      <link>https://stackoverflow.com/questions/58306578/training-chip-and-target-image-format-in-tensorflow</link>
      <description><![CDATA[我正在尝试为前哨图像构建土地覆盖分类模型。我正在使用的图像通道（频段）是32位浮点。
我需要了解如何最好地格式化图像数据，包括用于培训的芯片/补丁和用于分类的目标图像。我有几个问题：

我需要将原始图像和训练芯片从32位转换为其他深度吗？
我是否需要确保训练芯片/补丁和目标都具有相同的深度（32位，16位或其他）？
我需要转售我的数据吗？我看到了一些论文，其中数据在0-1或0-255之间重新缩放？
数据深度是否影响学习和预测的表现？
]]></description>
      <guid>https://stackoverflow.com/questions/58306578/training-chip-and-target-image-format-in-tensorflow</guid>
      <pubDate>Wed, 09 Oct 2019 14:35:06 GMT</pubDate>
    </item>
    <item>
      <title>在机器学习上获得高精度的数据集的最小尺寸是多少？</title>
      <link>https://stackoverflow.com/questions/47397130/what-is-the-minimum-size-of-dataset-that-get-high-accuracy-on-machine-learning</link>
      <description><![CDATA[我正在尝试制作一个获得花图像并识别它的应用程序。
我想到了使用机器学习，但是当我查找数据集时，我只发现每花最多700张图像的数据集。
我知道这还不足以获得良好的结果。
有人知道一个更大的数据集吗？使用机器学习模型获得高精度的数据集的最小尺寸是多少？
您认为尝试图像处理而不是ML是最好的？]]></description>
      <guid>https://stackoverflow.com/questions/47397130/what-is-the-minimum-size-of-dataset-that-get-high-accuracy-on-machine-learning</guid>
      <pubDate>Mon, 20 Nov 2017 16:52:34 GMT</pubDate>
    </item>
    <item>
      <title>pytorch中的L1/L2正则化</title>
      <link>https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch</link>
      <description><![CDATA[如何在不手动计算的情况下在Pytorch中添加L1/L2正则化？]]></description>
      <guid>https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch</guid>
      <pubDate>Thu, 09 Mar 2017 19:54:19 GMT</pubDate>
    </item>
    </channel>
</rss>