<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Sun, 16 Mar 2025 01:24:51 GMT</lastBuildDate>
    <item>
      <title>训练后，我们可以从文件夹中删除某些图像吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79511077/can-we-make-a-bot-delete-certain-images-from-the-folder-after-training</link>
      <description><![CDATA[如果我们训练AI模型来识别某些类型的图像，例如为狗的示例图像。我们可以从文件夹中识别并删除此类图像吗？例如：我在狗图像上训练模型，现在我向机器人介绍一个未知文件夹，以便它可以从中删除所有狗图像。
如果是，我们如何使用python？]]></description>
      <guid>https://stackoverflow.com/questions/79511077/can-we-make-a-bot-delete-certain-images-from-the-folder-after-training</guid>
      <pubDate>Sat, 15 Mar 2025 11:52:12 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的梯度下降模型会产生常数（非常高）的正方形错误成本函数而不会收敛到最低？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79510918/why-is-my-gradient-descent-model-giving-a-constant-very-high-square-error-cost</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79510918/why-is-my-gradient-descent-model-giving-a-constant-very-high-square-error-cost</guid>
      <pubDate>Sat, 15 Mar 2025 09:53:56 GMT</pubDate>
    </item>
    <item>
      <title>我如何自动计算这张照片中的所有汽车？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79510435/how-could-i-automatically-count-all-the-cars-in-this-picture</link>
      <description><![CDATA[我要手动进行，但是认为必须有一些机器学习模型自动进行此类事情。我想在收费大门之前计算右侧（西行）车道的所有汽车。
  https://imgur.com/a/bppac8p   ]]></description>
      <guid>https://stackoverflow.com/questions/79510435/how-could-i-automatically-count-all-the-cars-in-this-picture</guid>
      <pubDate>Sat, 15 Mar 2025 00:04:18 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用遗传算法（GA）进行飞行价格优化吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79510111/can-i-use-genetic-algorithm-ga-for-flight-price-optimization</link>
      <description><![CDATA[我在Kaggle上找到了一个飞行价格预测数据集，并训练了RandomForestRegressor模型，以根据航空公司，停车，源，目的地，目的地和时间等功能来预测飞行价格。现在，我想使用遗传算法（GA）来优化并找到最便宜的飞行。
在此数据集上使用GA进行价格优化是否可行？]]></description>
      <guid>https://stackoverflow.com/questions/79510111/can-i-use-genetic-algorithm-ga-for-flight-price-optimization</guid>
      <pubDate>Fri, 14 Mar 2025 19:56:06 GMT</pubDate>
    </item>
    <item>
      <title>自动ML动作识别训练模型失败[关闭]</title>
      <link>https://stackoverflow.com/questions/79508679/auto-ml-action-recognition-training-model-fail</link>
      <description><![CDATA[我是自动ML的新手，我遵循了有关如何创建识别操作模型的文档，我创建了一个包含视频的数据集，我在GCP控制台中注释了视频，
当我进行培训时，我会收到此错误：

 keyframe 0.34没有足够的上下文。

我下载了遵循此  {; aiplatform.googleapis.com/annotation_set_name＆quot;：＆quord&#39;6595640401539891200}
 
我有错误是正常的吗？如果是这样，我该如何注释视频？在控制台中，只有每次都有一个注释，这就是为什么开始和结束时间相同]]></description>
      <guid>https://stackoverflow.com/questions/79508679/auto-ml-action-recognition-training-model-fail</guid>
      <pubDate>Fri, 14 Mar 2025 09:50:30 GMT</pubDate>
    </item>
    <item>
      <title>带有Interleave的Tensorflow Keras模型有问题</title>
      <link>https://stackoverflow.com/questions/79507456/tensorflow-keras-model-with-interleave-is-having-issues-fitting</link>
      <description><![CDATA[我正在研究一个CNN项目，在该项目中，我使用TensorFlow和Keras。此外，我将Interleave用作数据集很大（太大，无法用我们的资源加载到RAM中）。但是，由于训练的精度非常低，而且非常跳动，因此存在一个问题。此外，我得到警告：

用户保存：您的输入用完数据；中断训练。制作
确保您的数据集或发电机至少可以生成
 step_per_epoch * epochs 批次。您可能需要使用
 .repeat（）构建数据集时功能。

    数据集由txt文件组成，其中包含浮点的3D阵列，其中两个内部维度为属性集，属性集的数量（可能在文件之间有所不同）。这意味着每个TXT文件都包含多个用于培训的条目。然后将这些文件存储在标记的文件夹中。在时刻，数据集的准备看起来像下面看到的。当TXT文件包含不同数量的条目时，我尝试编写PathTodatAset（）函数以生成能够单个条目而不是批处理的数据集。
 ＃定义ProcessTXTFILE_TF功能
def processTXTFILE_TF（file_path）：
    ＃由于file_path是张量，我们需要对其进行解码
    file_path = file_path.numpy（）。解码（&#39;utf-8&#39;）
    ＃获取MFCC NP数组
    mfccarray = readtxtfile（file_path）
    ＃添加颜色频道
    mfccarray = np.expand_dims（mfccarray，axis = -1）
    ＃转换为张量
    张量= tf.convert_to_tensor（mfccarray，dtype = tf.float16）
    #print（f＆quot“张量形状：{Tensor.shape}”＆quot;）
    ＃返回张量
    返回张量

＃从文件路径创建数据集
def pathtodataset（file_paths）：
    返回 （
        tf.data.dataset.from_tensor_slices（file_paths）
        .intreleave（
            lambda文件：tf.data.dataset.from_tensor_slices（
                tf.py_function（processTXTFILE_TF，[file]，[tf.float16]）
            ），
            ＃cycle_length = min（train_len，16），
            num_parallel_calls = tf.data.autotune，
        ）
        .unbatch（）
        .map（lambda x：tf.ensure_shape（x，input_shape））＃确保形状正确
    ）

＃创建X_TRAIN_TF数据集
x_train_tf = pathtodataset（x_train）
＃创建y_train_tf数据集
y_train_tf = tf.data.dataset.from_tensor_slices（y_train）
＃将x_train_tf和y_train_tf组合到一个数据集中
train_tf = tf.data.dataset.zip（（（x_train_tf，y_train_tf）））。shuffle（buffer_size = len（x_train），reshuffle_each_iteration = true = true）.batch（batch_size）.repeat）.repeat）.repeat .repeat（repeat）。从batch（）之后

＃创建X_VAL_TF数据集
x_val_tf = pathtodataset（x_val）
＃创建y_val_tf数据集
y_val_tf = tf.data.dataset.from_tensor_slices（y_val）
＃将x_val_tf和y_val_tf组合到一个数据集中
val_tf = tf.data.dataset.zip（（（（x_val_tf，y_val_tf）））。
 
完整代码可在此处找到：我相信这个问题与警告有关，并且交织/数据库对象不应该这样做...但是问题到底是什么以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/79507456/tensorflow-keras-model-with-interleave-is-having-issues-fitting</guid>
      <pubDate>Thu, 13 Mar 2025 19:34:36 GMT</pubDate>
    </item>
    <item>
      <title>我应该在此机器学习问题（CNN层）中添加哪些修改？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79507291/what-modifications-should-i-add-in-this-machine-learning-question-cnn-layer</link>
      <description><![CDATA[如何接近和解决此练习？

下面您会找到CNN的输入，过滤和结果输出
层。这些都被绘制为灰度图像，并与
精确的像素值写出。对于像素的边缘
输出图像，假定输入图像以外的任何坐标
为0（零盖）。输出中的某些值丢失。
使用给定输入计算像素A，B和C的输出值
和卷积过滤器。
 在此处输入图像说明 
像素A，B和C的值是多少

这是我得到的最远的，但是值仍然不正确：
 导入numpy作为NP

input_image = np.array（[[
    [0、0、0、0、0、0、0、0、0]，
    [0，0，1，1，1，0，0，0，0]，
    [0、3、4、4、4、3、0、0]，
    [0、1、4、4、4、1、0、0]，
    [0，0，1，3，4，4，4，4，2]，，
    [0，0，1，2，3，4，4，3]，
    [2，4，4，4，4，4，3，3，3]，
    [1，2，3，2，0，2，2，2]，
    [0，0，0，0，0，0，0，0，0]
）））

filter = np.array（[
    [2，3，0]，
    [2，2，3]，
    [0，0，2]
）））

＃执行卷积的功能
def卷积（input_image，filter，x，y）：
    filter_size = filter.shape [0]
    结果= 0
    对于我的范围（filter_size）：
        对于范围内的J（filter_size）：
            如果x + i＆lt; input_image.shape [0]和y + j＆lt; input_image.shape [1]：
                结果 + = input_image [x + i，y + j] * filter [i，j]
    返回结果

＃计算缺失的像素值
a =卷积（input_image，filter，7，1）
b =卷积（input_image，filter，5，3）
c =卷积（input_image，filter，2，1）

打印（f＆quot; a = {a}，b = {b}，c = {c}＆quet;）
 
输出：
a = 13，b = 42，c = 46
✘像素A的值不正确。]]></description>
      <guid>https://stackoverflow.com/questions/79507291/what-modifications-should-i-add-in-this-machine-learning-question-cnn-layer</guid>
      <pubDate>Thu, 13 Mar 2025 18:16:03 GMT</pubDate>
    </item>
    <item>
      <title>使用不同的GIT回购分支进行ML模型开发，测试和部署。这是一个不好的做法吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79505557/using-different-git-repo-branches-for-ml-model-development-testing-and-deploym</link>
      <description><![CDATA[我是一名科学家，致力于开发ML模型。我通常将我的开发代码保留在单独的存储库中，然后提取模型，停靠和部署在单独的仓库中。
现在，我正在研究一个小型项目，我不希望维护2个存储库。但是，我不希望在同一分支机构中开发和部署代码。
维护3个分支的建议 -  

 ML探索：在完成模型算法之前进行实验。 （Jupyter）
 ML-DEV：开发模型的代码。 （Python）
 ml -deploy：部署代码 - 烧瓶应用程序，docker文件等

我想将部署分支作为主人，然后在集成测试后进行生产。
是否有使用GIT存储库用于使用MLOP的最佳做法？
定义分支并使用它们的特定最佳实践是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79505557/using-different-git-repo-branches-for-ml-model-development-testing-and-deploym</guid>
      <pubDate>Thu, 13 Mar 2025 06:15:34 GMT</pubDate>
    </item>
    <item>
      <title>将函数传递到张量流数据集映射方法时的奇怪行为</title>
      <link>https://stackoverflow.com/questions/72552272/strange-behaviour-when-passing-a-function-into-a-tensorflow-dataset-map-method</link>
      <description><![CDATA[今天早些时候对我来说这很好，但是当我重新启动笔记本时，它突然开始非常奇怪。
我有一个TF数据集，该数据集将Numpy文件及其相应的标签作为输入，例如SO  tf.data.dataset.from_tensor_slices（（（Specgram_files，labels，labels））。
当我使用在ds.take（1）中使用进行1个项目时：print（item）我获得了预期的输出，这是张紧器的元组，其中第一个张量包含numpy文件的名称作为字节字符串，第二个张量张量包含编码的标签。
然后，我有一个功能，可以使用 np.load（）读取文件，并产生一个numpy数组，然后返回。此函数传递到Map（）方法中，看起来像这样：
  ds = ds.map（
lambda文件，标签：元组（[tf.numpy_function（read_npy_file，[file]，[tf.float32]），label]），），），），），
num_parallel_calls = tf.data.autotune）
 
，read_npy_file看起来像这样：
  def read_npy_file（数据）：
    ＃“数据”存储numpy二进制文件的文件名，存储特定声音文件的功能
    ＃作为字节字符串。
    ＃decode（）在字节字符串上调用以将其从字节字符串解码为常规字符串
    ＃，以便它可以作为参数传递到np.load（）中
    data = np.load（data.decode（））
    返回data.stype（np.float32） 
 
您可以看到，映射应创建另一个张量的元组，其中第一个张量是numpy阵列，第二个张量是标签，未触及。这很早就起作用了，但是现在它给出了最奇怪的行为。我将打印语句放在 read_npy_file（）函数中，以查看是否传递了正确的数据。我希望它会传递一个字符串，但是当我调用 print（data&gt; data） read_npy_file（read_npy_file（） ds.take（1）：
  b&#39;./挑战e_data/log_spectRogram/2603ebb3-3cd3-43cc-98ef-0C128C515863.npy&#39;b&#39;./ challen gea_data/log_spectRogram/fab6a266-e97a-4935-a0c3-4444444426fc5.npy&#39;b&#39;./挑战e_data/log_spectrogr AM/93014682-60A2-45BD-9C9E-7F3C97B83BE9.NPY&#39;B&#39;./挑战A252-6AD3601B92D9.NPY&#39;B&#39;./挑战e_data/log_spectRogram/e757058C-91DE-4381-8181-8184-65F001C95647.NPY&#39;


b&#39;./挑战_data/log_spectRogram/38B12689-04BA-422B-A972-5856B05CA868.NPY&#39;
b&#39;./挑战_data/log_spectRogram/7C9CCCC04-A2D2-4EEC-BAFD-0C97B3658C26.NPY&#39;B&#39;./挑战



b&#39;./挑战e_data/log_spectRogram/21F6​​060A-9766-4810-BD7C-0437F47CCCB98.NPY&#39;
 
我没有修改输出的任何格式。
这是完整的代码
  def read_npy_file（数据）：
    ＃“数据”存储numpy二进制文件的文件名，存储特定声音文件的功能
    ＃作为字节字符串。
    ＃decode（）在字节字符串上调用以将其从字节字符串解码为常规字符串
    ＃，以便它可以作为参数传递到np.load（）中
    打印（数据）
    data = np.load（data.decode（））
    返回data.stype（np.float32）

specgram_ds = tf.data.dataset.from_tensor_slices（（specgram_files，labels））

specgram_ds = specgram_ds.map（
                    lambda文件，标签：元组（[tf.numpy_function（read_npy_file，[file]，[tf.float32]），label]），），），），），
                    num_parallel_calls = tf.data.autotune）

num_files = len（train_df）
num_train = int（0.8 * num_files）
num_val = int（0.1 * num_files）
num_test = int（0.1 * num_files）

specgram_ds = specgram_ds.shuffle（buffer_size = 1000）
specgram_train_ds = specgram_ds.take（num_train）
specgram_test_ds = specgram_ds.skip（num_train）
specgram_val_ds = specgram_test_ds.take（num_val）
specgram_test_ds = specgram_test_ds.skip（num_val）

＃迭代一项以触发映射功能
对于specgram_ds.take（1）中的项目：
    经过
 ]]></description>
      <guid>https://stackoverflow.com/questions/72552272/strange-behaviour-when-passing-a-function-into-a-tensorflow-dataset-map-method</guid>
      <pubDate>Wed, 08 Jun 2022 21:13:37 GMT</pubDate>
    </item>
    <item>
      <title>Python的联合平均实施</title>
      <link>https://stackoverflow.com/questions/66472157/federated-averaging-implementation-in-python</link>
      <description><![CDATA[我正在从事联合学习。我正在使用全局服务器，在该服务器上定义了基于CNN的分类器。全局服务器用超参数编制模型，并将其发送到边缘（客户端），目前我正在使用两个客户端。每个客户端使用其本地数据（目前我正在使用相同的数据，并在每个客户端上建模）。在训练模型之后，每个客户在本地模型中的准确性，精度和回忆超过95％。客户将训练有素的本地模型发送到服务器。服务器获取模型并从每个接收到的模型中获取权重，并根据此公式。以下是我编写的代码以在Python中实现此公式。当我将平均权重设置为模型并尝试预测准确性，召回和精确度低于20％。
我在实施中做错了什么？
 ＃全局模型的初始权重，设置为ZER0。  
  ave_weights = model.get_weights（）
  ave_weights = [i * 0 for I in ave_weights]
  计数= 0
＃多线程Python服务器：TCP服务器套接字线程池
def clientthread_send（conn，地址，权重）：
    ＃将模型发送给客户
    conn.send（模型）

    打印（“型号发送到：”，地址）
    打印（“等重量”
    model_recv = conn.recv（1024）
    打印（“从：“地址”收到的权重）
    全球人数
    全局ave_weights

    
    ＃来自客户的重量
    rec_weight = model.get_weights（）
    #multip，按客户端本地数据中的本地数据示例数量地量身
    rec_weight = [i * 100000 for i in rec_Weeight]
    ＃将权重除以所有参与者的样本总数
    rec_weight = [i / 200000 for i in rec_Weeight]

    ＃所有客户的权重
    ave_weights = [x + y for x，y in zip（ave_weights，rec_aweight）]
  
    计数=计数+1
    conn.close（）
如果计数== 2：
    ＃如果计数（客户次数为两个），请设置全局模型权重
    model.set_weights（ave_weights）


 而真：
     conn，地址= s.accept（）
     start_new_thread（clientthread_send，（conn，address，ave_weights））   
     
 ]]></description>
      <guid>https://stackoverflow.com/questions/66472157/federated-averaging-implementation-in-python</guid>
      <pubDate>Thu, 04 Mar 2021 09:27:37 GMT</pubDate>
    </item>
    <item>
      <title>如何在预测后解开数据？</title>
      <link>https://stackoverflow.com/questions/63380766/how-to-unscale-data-after-predictions</link>
      <description><![CDATA[我有一个具有2个功能的数据集（价格＆amp;卷）＆amp; 1预测变量（价格）并使用LTSM模型根据先前的价格预测下一个价格。
首先，我扩展数据集：
  #scale数据
sualer = minmaxscaler（feature_range =（0,1））
scaled_data = scaler.fit_transform（数据集）
 
最后我想解开它：
 ＃获取模型预测的价格值
预测= model.predict（x_test）
预测= sualer.inverse_transform（预测）
 
，但这不起作用，我得到了这个错误：
  valueerror：具有形状（400,1）的非广播输出操作数不匹配广播形状（400,2）
 ]]></description>
      <guid>https://stackoverflow.com/questions/63380766/how-to-unscale-data-after-predictions</guid>
      <pubDate>Wed, 12 Aug 2020 16:20:50 GMT</pubDate>
    </item>
    <item>
      <title>用Spacy进行其他指定实体识别所需的培训数据数量是多少？</title>
      <link>https://stackoverflow.com/questions/52120487/what-is-the-amount-of-training-data-needed-for-additional-named-entity-recogniti</link>
      <description><![CDATA[我正在使用Spacy模块查找输入文本的名称实体。我正在训练该模型以预测医学术语。我目前可以访问200万张医疗笔记，我为注释笔记提供了一个程序。
 i与约90,000条的预定列表交叉参考医疗注释，该列表用于注释任务。在当前注释的速度下，大约需要一个半小时的注释才能注释10,000个纸币。注释目前的工作方式，我最终得到了大约90％的注释没有注释的笔记（我目前正在努力获取更好的交叉引用术语列表），因此我采用约1000个注释的注释并对其进行训练。&gt; 
我已经检查过，模型对已知的注释术语做出了一种响应（例如，术语 tachycardia  在注释之前已看到，有时会在文本中显示该术语时。）。）。）。
这个背景可能与我的特定问题不太相关，但我认为我会给我当前的立场提供一小部分背景。
我想知道是否有人成功地培训了一个新实体，可以使我对他们在至少具有某种可靠的实体识别所必需的培训中的个人经验有所了解。]]></description>
      <guid>https://stackoverflow.com/questions/52120487/what-is-the-amount-of-training-data-needed-for-additional-named-entity-recogniti</guid>
      <pubDate>Fri, 31 Aug 2018 17:46:41 GMT</pubDate>
    </item>
    <item>
      <title>培训和测试数据如何分配？ [关闭]</title>
      <link>https://stackoverflow.com/questions/51006505/how-training-and-test-data-is-split</link>
      <description><![CDATA[我目前正在使用神经网络训练我的数据并使用拟合功能。
  history = model.fit（x，encoded_y，batch_size = 50，nb_epoch = 500，verialation_split = 0.2，verbose = 1）
 
现在，我已经使用验证_split 作为 20％。我了解的是，我的培训数据将为 80％，测试数据将为 20％。我很困惑这些数据在后端如何处理。它是否会像Top  80％的样品进行训练，并且低于20％的20％用于测试，或者样品是从介于两者之间随机选择的？如果我想提供单独的培训和测试数据，我将如何使用 model.fit（） ?? 进行操作。
此外，我的第二个问题是如何检查数据是否适合模型？从结果我可以看出，训练精度约为 90％，而验证精度约为 55％。这是否意味着过度安装或不合格的情况？
我的最后一个问题是评估回报是什么？文档表示它返回损失，但我已经在每个时期期间都会获得损失和准确性（作为fit的回报（）（在 history 中））。评估表演返回的精度和得分如何？如果通过评估返回 90％返回的准确性，我可以说我的数据很合适，无论每个时期的个人准确性和损失是什么？？
以下是我的代码：
 导入numpy
进口熊猫
导入matplotlib.pyplot作为PLT
来自keras.models导入顺序
来自keras.layers导入密集，辍学
来自keras.wrappers.scikit_learn导入kerasclassifier
来自sklearn.model_selection导入cross_val_score
从Sklearn.Preprocessing Import LabElenCoder
来自sklearn.model_selection导入stratifiedkfold
从sklearn.prepercorsing进口标准标准
来自Sklearn.Pipeline Import Pipeline
来自keras.utils导入np_utils
来自sklearn.model_selection导入kfold
来自sklearn.metrics导入混乱_matrix
导入Itertools

种子= 7
numpy.random.seed（种子）

dataframe = pandas.read_csv（&#39;inputfile.csv＆quort; skiprows = range（0，0））

dataset = dataframe.values
x = dataset [：，0：50] .astype（float）＃cols-1的数量
y =数据集[：，50]

encoder = labelencoder（）
coder.fit（y）
encoded_y = encoder.transform（y）

encoded_y = np_utils.to_categorical（encoded_y）
打印（&#39;encoded_y =＆quot; encoded_y） 
＃基线模型
def create_baseline（）：
    ＃创建模型
    型号=顺序（）
    model.Add（密集（5，input_dim = 5，kernel_initializer =&#39;normal&#39;，activation =&#39;relu&#39;））
    model.Add（密集（5，kernel_initializer =&#39;normal&#39;，activation =&#39;relu&#39;））
    ＃model.Add（密集（2，kernel_initializer =&#39;normal&#39;，activation =&#39;sigmoid&#39;）））））

    model.Add（密集（2，kernel_initializer =&#39;normal&#39;，activation =&#39;softmax&#39;）））））

    ＃编译模型
    model.compile（loss =&#39;binary_crossentropy&#39;，优化器=&#39;adam&#39;，metrics = [&#39;fecicy&#39;]）＃for binayr分类
        ＃model.compile（loss =&#39;cancorical_crossentropopy&#39;，importizer =&#39;adam&#39;，metrics = [&#39;cocuctiacy&#39;]）＃for Multi类
    返回模型
    

model = create_baseline（）;
历史= model.fit（x，encoded_y，batch_size = 50，nb_epoch = 500，验证_split = 0.2，冗长= 1）

print（history.history.keys（））
＃列出历史记录中的所有数据
print（history.history.keys（））
＃总结历史的准确性
plt.plot（历史学家[&#39;acc&#39;]）
plt.plot（history.history [&#39;val_acc&#39;]）
plt.title（“模型精度”）
plt.ylabel（“准确性”）
plt.xlabel（&#39;epoch&#39;）
plt.legend（[&#39;train&#39;，&#39;test&#39;]，loc =“左上”）
plt.show（）
＃总结损失的历史
plt.plot（历史学家[&#39;损失&#39;]）
plt.plot（历史学家[&#39;val_loss&#39;]）
plt.title（“模型损失”）
plt.ylabel（“损失”）
plt.xlabel（&#39;epoch&#39;）
plt.legend（[&#39;train&#39;，&#39;test&#39;]，loc =“左上”）
plt.show（）


pre_cls = model.predict_classes（x）    
cm1 = Confusion_matrix（encoder.transform（y），pre_cls）
打印（&#39;混淆矩阵：\ n&#39;）
打印（CM1）


得分，acc = model.evaluate（x，encoded_y）
打印（“测试分数：”，分数）
打印（“测试准确性：”，ACC）
 ]]></description>
      <guid>https://stackoverflow.com/questions/51006505/how-training-and-test-data-is-split</guid>
      <pubDate>Sun, 24 Jun 2018 02:28:07 GMT</pubDate>
    </item>
    <item>
      <title>如何解释几乎完美的准确性和AUC-ROC但F1得分为零，精确和回忆</title>
      <link>https://stackoverflow.com/questions/34698161/how-to-interpret-almost-perfect-accuracy-and-auc-roc-but-zero-f1-score-precisio</link>
      <description><![CDATA[我正在培训ML Logistic分类器，使用Python Scikit-Learn对两个类进行分类。它们处于极度不平衡的数据中（大约14300：1）。我的准确性几乎为100％和ROC-AUC，但精确度为0％，召回和F1得分。我了解准确性通常在非常不平衡的数据中没有用，但是为什么ROC-AUC措施也接近完美？
 来自Sklearn.metrics导入roc_curve，auc

＃获取ROC 
y_score = classifierused2.decision_function（x_test）
false_posisitive_rate，true_posive_rate，阈值= roc_curve（y__test，y_score）
roc_auc = auc（false_posistion_rate，true_posistion_rate）
打印&#39;auc  - &#39;+&#39;=&#39;，roc_auc

1 = class1
0 = class2
班级计数：
0 199979
1 21

精度：0.99992
分类报告：
             精确召回F1得分支持

          0 1.00 1.00 1.00 99993
          1 0.00 0.00 0.00 7

AVG /总计1.00 1.00 1.00 100000

混乱矩阵：
[[99992 1]
 [7 0]]
AUC = 0.977116255281
 
以上是使用逻辑回归，以下是使用决策树，决策矩阵看起来几乎相同，但是AUC却大不相同。
  1 = class1
0 = class2
班级计数：
0 199979
1 21
精度：0.99987
分类报告：
             精确召回F1得分支持

          0 1.00 1.00 1.00 99989
          1 0.00 0.00 0.00 11

AVG /总计1.00 1.00 1.00 100000

混乱矩阵：
[[99987 2]
 [11 0]]
AUC = 0.4999899989
 ]]></description>
      <guid>https://stackoverflow.com/questions/34698161/how-to-interpret-almost-perfect-accuracy-and-auc-roc-but-zero-f1-score-precisio</guid>
      <pubDate>Sat, 09 Jan 2016 19:50:50 GMT</pubDate>
    </item>
    <item>
      <title>Smote未能过度样品[关闭]</title>
      <link>https://stackoverflow.com/questions/24101802/smote-fails-to-oversample</link>
      <description><![CDATA[我刚刚使用SMOTE在数据集中进行了过采样，包括DMWR软件包中包括。
我的数据集由两个类形成。原始分布是12 vs 62。所以，我已经对此进行了编码：
  newdata＆lt;  -  smote（得分〜。，data，k = 3，perc.over = 400，perc.ull = 150）
 
现在，分布是60 vs 72。
但是，当我显示“ newdata”数据集时，我会发现Smote是如何进行过采样的，并且有一些样本重复。
例如，样本号24以24.1、24.2和24.3。出现。
这是正确的吗？这直接影响分类，因为分类器将学习一个模型，该模型将在测试中存在，因此这在分类中是不合法的。
编辑：
我想我没有正确解释我的问题：
您知道，Smote是一种过度样品的技术。它从原始样本中创建新样本，从而修改其功能的值。但是，当我显示Smote生成的新数据时，我会得到：
（这些值是功能的值）样本50：1.8787547 0.19847987 -0.0105946940 4.420207 4.660536 1.0936388 0.5312777 0.0717171645 0.008040431645 0.00804043167 
样本50.1：1.8787547 0.19847987 -0.0105946940 4.420207 4.660536 1.0936388 0.5312777 0.0717171645 
样本50属于原始数据集。样本50.1是Smote产生的“人造”样品。但是（这是我的问题），Smote创建了一个重复的样本，而不是创建一个人造一个修改“稍微”的功能值。]]></description>
      <guid>https://stackoverflow.com/questions/24101802/smote-fails-to-oversample</guid>
      <pubDate>Sun, 08 Jun 2014 00:09:22 GMT</pubDate>
    </item>
    </channel>
</rss>