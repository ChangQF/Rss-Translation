<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 13 Jun 2024 15:16:18 GMT</lastBuildDate>
    <item>
      <title>尝试将 Kaggle 笔记本提交到 GitHub 存储库时出错？如何解决此问题</title>
      <link>https://stackoverflow.com/questions/78618684/getting-error-while-trying-to-commit-a-kaggle-notebook-to-a-github-repository-h</link>
      <description><![CDATA[提交内核时发生错误：ConcurrencyViolation 序列号必须匹配草稿记录：KernelId=59714315、ExpectedSequence=43、ActualSequence=42、AuthorUserId=16388128（这是什么意思）
当我尝试将笔记本从 kaggle 提交到 github 时出现此错误。我该如何解决这个问题？
我原本以为它会直接提交到 github 而不会遇到任何问题]]></description>
      <guid>https://stackoverflow.com/questions/78618684/getting-error-while-trying-to-commit-a-kaggle-notebook-to-a-github-repository-h</guid>
      <pubDate>Thu, 13 Jun 2024 15:06:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 `tf.data.Dataset.from_generator` 训练序列模型时 Tensorflow 性能下降</title>
      <link>https://stackoverflow.com/questions/78618554/tensorflow-performance-drop-when-training-sequential-model-using-tf-data-datase</link>
      <description><![CDATA[我正在训练二元分类问题中的顺序模型。我的数据集是 HDF 格式，包含许多文件，这些文件通常太大而无法放入内存中。为了解决这个问题，我尝试使用基于 tf.data.Dataset.from_generator 的 TensorFlow 管道方法。但是，经过多次尝试（使用这篇文章等资源），我发现训练模型的性能明显下降。
我设法使用以下代码在通用数据集中复制了该问题：
import tensorflow as tf
import tensorflow.keras as keras
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

def set_seeds(seed=999):
tf.random.set_seed(seed)
np.random.seed(seed)
tf.keras.utils.set_random_seed(seed) # 设置种子base-python、numpy 和 tf

n_features = 5

# 生成合成数据集并分成训练集和测试集
X, y = make_classification(n_samples=1_000_000, n_features=n_features, n_classes=2, random_state=42)
X_train = pd.DataFrame(X)
y_train = pd.DataFrame(y)

X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# 定义一个顺序模型
def define_model(n_features=n_features):
model = keras.models.Sequential([
keras.layers.BatchNormalization(input_shape=(n_features, )),
keras.layers.Dense(10,activation=&#39;relu&#39;),
keras.layers.Dense(1,activation=&#39;sigmoid&#39;)
])
return model

def compile(model):
# 编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])


基于 DataFrame 方法的训练
当我使用 pd.DataFrame 训练模型时，我得到以下结果：
# 基于 pd.DataFrame 进行训练
set_seeds()
model = define_model()
compile(model)
history = model.fit(X_train, y_train, epochs=10, batch_size=10_000,validation_split=0.0, verbose=True)
ax = pd.DataFrame(history.history).plot()
ax.set_ylim(0, 1.0)

# 在测试样本上进行评估
model.evaluate(X_test, y_test, batch_size=10_000)


这大约需要 5 秒钟，并且准确率约为 0.96。
基于 Dataset 方法进行训练
但是，当我使用 tf.data.Dataset 进行训练时，代码如下：
class Generator:
def __call__(self):
Yield X_train.values, y_train.values

ds_train = tf.data.Dataset.from_generator(
Generator(),
output_signature=(
tf.TensorSpec(shape=(None, n_features), dtype=tf.float32),
tf.TensorSpec(shape=(None, 1), dtype=tf.int32)
)
)

set_seeds()
model = define_model()
compile(model)
history = model.fit(ds_train, epochs=10, batch_size=10_000, verbose=True)
ax = pd.DataFrame(history.history).plot()
ax.set_ylim(0, 1.0)

model.evaluate(X_test, y_test, batch_size=10_000)

我得到的准确率约为 0.6。但是，这种方法大约需要 2.5 秒。为了达到与 DataFrame 方法相同的准确度，我需要训练大约 400 个 epoch，这需要 40 秒。
为什么数据集表现更差？
我不确定我是否从根本上误解了 tf.data.Dataset 的用法，但这种行为是意料之外的。似乎 Dataset 方法中的 epoch 比使用 DataFrame 方法的 epoch 提供的信息更少。
有人对为什么会发生这种情况以及如何修复它有什么见解或建议吗？
我使用的是 TF 2.13.1
和 python 3.11.3]]></description>
      <guid>https://stackoverflow.com/questions/78618554/tensorflow-performance-drop-when-training-sequential-model-using-tf-data-datase</guid>
      <pubDate>Thu, 13 Jun 2024 14:40:53 GMT</pubDate>
    </item>
    <item>
      <title>机器学习实验工作空间建议[关闭]</title>
      <link>https://stackoverflow.com/questions/78618311/suggestion-of-workspace-for-machine-learning-experiments</link>
      <description><![CDATA[目前，我需要对机器学习模型和设置进行多项实验。
像 Kaggle、Colab 和本地机器这样的初学者环境已经不能满足我的需求了。因为这是我第一次进行大规模实验，所以我想请教一些关于我的工作空间的建议和选择。
我期待一种环境，我可以用 Jupyter 创建小规模实验，然后发送每个笔记本（或脚本）以在不同的设置下作为不同的作业运行。
请给我一些指导]]></description>
      <guid>https://stackoverflow.com/questions/78618311/suggestion-of-workspace-for-machine-learning-experiments</guid>
      <pubDate>Thu, 13 Jun 2024 13:55:50 GMT</pubDate>
    </item>
    <item>
      <title>优化数据结构以微调客户服务聊天机器人开发中的 LLM 模型</title>
      <link>https://stackoverflow.com/questions/78618258/optimizing-data-structure-for-fine-tuning-an-llm-model-in-customer-service-chatb</link>
      <description><![CDATA[在开发客户服务聊天机器人以协助制定解决客户问题的指令时，我可以访问客户电子邮件、指令列表以及客户服务人员之间交换的电子邮件。我正在寻求有关微调 LLM 模型以提供特定领域响应的理想数据结构的指导。您的见解对于提高聊天机器人的性能和改善客户服务将非常有价值。
问题：
数据表示：表示客户电子邮件和指令以微调 LLM 模型的最有效方法是什么？

上下文信息：数据集中应包含哪些其他上下文信息以提高模型对客户问题的理解？

处理指令：应如何将指令纳入数据集以确保聊天机器人正确理解和应用它们？

模型训练：在微调过程中应采用哪些策略来优化模型生成对客户查询的特定响应的能力？

评估指标：哪些指标最适合评估微调后的 LLM 模型在客户服务聊天机器人环境中的性能？

与聊天机器人集成：如何将微调后的 LLM 模型有效地集成到客户服务聊天机器人框架中以提供实时决策支持？

我读到过我可以使用 RAG 或微调来实现此目的，但我不确定 LLM 模型所需的输入数据格式。]]></description>
      <guid>https://stackoverflow.com/questions/78618258/optimizing-data-structure-for-fine-tuning-an-llm-model-in-customer-service-chatb</guid>
      <pubDate>Thu, 13 Jun 2024 13:46:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 yolov8 无法检测到图像中的物体</title>
      <link>https://stackoverflow.com/questions/78618079/object-not-detect-in-image-using-yolov8</link>
      <description><![CDATA[我使用 yolov8 训练自定义对象检测模型，在训练模型后，我检查模型并给出一张图像，但模型没有检测到任何对象，我在线搜索此错误，人们说这个解决方案
将 detect.py 文件中的变量 half 修改为 False
第 31 行：half = False
但这对我来说不起作用。我在 google colab 中运行此代码。]]></description>
      <guid>https://stackoverflow.com/questions/78618079/object-not-detect-in-image-using-yolov8</guid>
      <pubDate>Thu, 13 Jun 2024 13:12:34 GMT</pubDate>
    </item>
    <item>
      <title>通过 FLASK API 调用获取图像的面部编码时出现 tensorflow 包问题</title>
      <link>https://stackoverflow.com/questions/78617502/issue-with-tensorflow-package-while-getting-face-encodings-for-a-image-through-a</link>
      <description><![CDATA[我有一个名为 get_signatures_localmodel(image_path) 的 Python 函数，它将在图像上运行 ML 模型，并为提供的图像提供 face encodings = face_model.predict([input_pairs])，我正在使用 Flask API 服务调用此函数。当我执行此操作时，我收到以下错误和警告：
venv/lib/python3.6/site-packages/keras/engine/training.py:2470: 
UserWarning: `Model.state_updates` 将在未来的版本中删除。
此属性不应在 TensorFlow 2.0 中使用，因为 `updates` 
会自动应用。

warnings.warn(&#39;`Model.state_updates` 将在未来的版本中删除。&#39;
异常：找不到变量 conv5_1_1x1_reduce/kernel。
这可能意味着变量已被删除。
在 TF1 中，它也可能意味着变量未初始化。
调试信息：容器 = localhost，状态 = 未找到：
容器 localhost 不存在。
（找不到资源：localhost/conv5_1_1x1_reduce/kernel）
[[{{node model_1_2/conv5_1_1x1_reduce/Conv2D/ReadVariableOp}}]]

但是当我尝试将图像直接传递给此函数并像普通的 python 文件一样运行它时，它没有给出任何错误并获得预期的输出，问题仅与 API 有关调用。
目前我正在使用 tensorflow 1.x 版本，也尝试使用 tensorflow 2.x，但仍然卡在此。
我尝试将 tensorflow 版本从 1.x 更改为 2.x。
尝试加载模型并将其保存在 session() 中。
预期输出：当我们使用 Flask API 调用访问该函数时，它应该提供面部编码以及此错误和警告的原因....]]></description>
      <guid>https://stackoverflow.com/questions/78617502/issue-with-tensorflow-package-while-getting-face-encodings-for-a-image-through-a</guid>
      <pubDate>Thu, 13 Jun 2024 11:14:29 GMT</pubDate>
    </item>
    <item>
      <title>支持向量机拟合无法检测和预测训练集上的 1 [关闭]</title>
      <link>https://stackoverflow.com/questions/78616808/the-support-vector-machine-fit-isnt-able-to-detect-and-predict-the-1-on-the-tra</link>
      <description><![CDATA[我正在尝试使用 e1071 库在 R 中实现 SVM。
svm_train &lt;- svm(factor_new ~ RFS +LI+SDI+LDI+DR+DBT+FCT+FII+DITP+ADCG+ADDG+ROA+ROI+ROS+ROE,data = train01_new, kernel = &quot;linear&quot;, cost = 0.1, scale = TRUE, type = &quot;C-classification&quot;)
predict_svm_train &lt;- predict(svm_train, train01_new)
cm_svm_train &lt;- chaosMatrix(predict_svm_train, train01_new$factor_new, mode = &quot;everything&quot;, positive = &quot;1&quot;)

代码运行正常，没有错误或警告，但训练集上的预测结果确实很奇怪。似乎模型无法检测到默认值（1）。]]></description>
      <guid>https://stackoverflow.com/questions/78616808/the-support-vector-machine-fit-isnt-able-to-detect-and-predict-the-1-on-the-tra</guid>
      <pubDate>Thu, 13 Jun 2024 08:50:14 GMT</pubDate>
    </item>
    <item>
      <title>强化学习算法中的 CUDA 内存不足</title>
      <link>https://stackoverflow.com/questions/78616686/cuda-out-of-memory-on-a-reinforcement-learning-algorithm</link>
      <description><![CDATA[import os
import gym
from gym import space
import numpy as np
import torch
import torch.nn as nn
import torch.nn. functional as F
from torch.utils.data import Dataset, DataLoader
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from tqdm import tqdm
from TTset_main import load_TTset

# 加载数据集
filted_tset_path = &quot;training_path.csv&quot;
filted_vset_path = &quot;validation_path.csv&quot;
TTset_training, TTset_vali, training_data_length = load_TTset(filted_tset_path, filted_vset_path)

# 自定义数据集类
class VideoDataset(Dataset):
def __init__(self, data_list):
self.data_list = data_list
self.length = sum(len(video[0]) for video in data_list) # 总帧数

def __len__(self):
return self.length

def __getitem__(self, idx):
video_idx = 0
frame_idx = idx
# 查找对应的视频和帧
for video, labels in self.data_list:
if frame_idx &lt; len(video):
frame = video[frame_idx]
label = labels[frame_idx]
return frame, label
frame_idx -= len(video)
raise IndexError(&quot;索引超出范围&quot;)

# 创建 DataLoader
batch_size = 1 # 每次一帧
video_dataset = VideoDataset(TTset_training)
data_loader = DataLoader(video_dataset, batch_size=batch_size, shuffle=True)

# CNN 模型
class CNNModel(nn.Module):
def __init__(self, num_classes):
super(CNNModel, self).__init__()
self.conv1 = nn.Conv3d(3, 32, kernel_size=3, stride=1, padding=1)
self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)
self.fc1 = nn.Linear(64*30*115*115, 512)
self.fc2 = nn.Linear(512, num_classes)

def forward(self, x):
x = F.relu(self.conv1(x))
x = F.max_pool3d(x, 2)
x = F.relu(self.conv2(x))
x = F.max_pool3d(x, 2)
x = x.view(-1, 64*30*115*115)
x = F.relu(self.fc1(x))
x = self.fc2(x)
return x

# 训练循环
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model = CNNModel(num_classes=20).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_function = nn.CrossEntropyLoss()

# 带进度条的训练
total_epochs = 10
for epoch in range(total_epochs):
training_acu_count = 0 # 重置每个 epoch 的准确率
for i, (x, y) in tqdm(enumerate(data_loader)):
model.train()
x = x.to(device).unsqueeze(0) # 添加批次维度
y = y.to(device)
pred = model(x)
t_loss = loss_function(pred, y.unsqueeze(0)) # 匹配维度以计算损失
t_loss.backward()
optimizer.step()
optimizer.zero_grad()

pred_convert = torch.argmax(pred, 1)
training_acu_count += (pred_convert == y).sum().item()

print(f&quot;Epoch {epoch+1}/{total_epochs}, Training Accuracy: {training_acu_count/len(video_dataset):.4f}&quot;)

# 保存模型
torch.save(model.state_dict(), &quot;video_classification_model.pth&quot;)


我使用上面的代码来训练一个对视频进行分类的 RL 模型。TTset_training 是一个包含输入和目标的数据加载器对象。输入尺寸为 129x15x3x60x230x230，目标尺寸为 129x15。视频帧是尺寸为 3x60x230x230 的张量。
我在 4070Ti 上运行此代码，并收到以下错误：
torch.cuda.OutOfMemoryError：CUDA 内存不足。尝试分配 48.43 GiB。GPU

我尝试了多种调整，更改批处理大小，使用循环单独输入数据，但都没有成功。我在网上搜索解决方案，但所有解决方案都建议更改帧的大小，但这是不可能的，因为我会丢失大量数据。]]></description>
      <guid>https://stackoverflow.com/questions/78616686/cuda-out-of-memory-on-a-reinforcement-learning-algorithm</guid>
      <pubDate>Thu, 13 Jun 2024 08:27:33 GMT</pubDate>
    </item>
    <item>
      <title>从 hugging face 空间创建 API 端点</title>
      <link>https://stackoverflow.com/questions/78616285/create-api-endpoint-from-hugging-face-space</link>
      <description><![CDATA[我在 Hugging Face 上创建了一个 Space，使用 Gradio 运行我的自定义机器学习模型。它在 Web 界面中运行完美，但现在我想将此 Space 转换为可以从我的应用程序调用的 API 端点。
有人可以指导我将 Hugging Face Space 转换为 API 端点的过程吗？具体来说，我正在寻找：
从我现有的 Space 设置推理端点的步骤。
如何处理用于访问端点的身份验证和 API 密钥？
是否有任何最佳实践或技巧可用于优化端点的性能。]]></description>
      <guid>https://stackoverflow.com/questions/78616285/create-api-endpoint-from-hugging-face-space</guid>
      <pubDate>Thu, 13 Jun 2024 06:57:15 GMT</pubDate>
    </item>
    <item>
      <title>Torch.unique() 的替代方案不会破坏梯度流吗？</title>
      <link>https://stackoverflow.com/questions/78615860/torch-unique-alternatives-that-do-not-break-gradient-flow</link>
      <description><![CDATA[在 Pytorch 梯度下降算法中，函数
def TShentropy(wf):
unique_elements, counts = wf.unique(return_counts=True)
entrsum = 0
for x in counts:
p = x/len_a #计算 x 的概率
entrsum-= p*torch.log2(p) #Shannon 熵公式 
return entrsum

使用方法 torch.unique()，该方法会破坏梯度流。每当我将其切换到连续概率计算（例如 torch.softmax()）时，程序就会运行。但是，该公式需要使用离散概率质量分布，而这不适用于 softmax。
我尝试使用 torch.nn. functional.one_hot 和 torch.bincount，两者都给出了相同的错误：
RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn

这注定会失败吗？我应该尝试以某种方式插入概率函数吗？]]></description>
      <guid>https://stackoverflow.com/questions/78615860/torch-unique-alternatives-that-do-not-break-gradient-flow</guid>
      <pubDate>Thu, 13 Jun 2024 04:48:39 GMT</pubDate>
    </item>
    <item>
      <title>使用 Huggingface Trainer 进行多 GPU 训练时，如何避免内存使用不均衡？</title>
      <link>https://stackoverflow.com/questions/78608004/how-can-i-avoid-unbalanced-memory-usage-when-performing-multi-gpu-training-using</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78608004/how-can-i-avoid-unbalanced-memory-usage-when-performing-multi-gpu-training-using</guid>
      <pubDate>Tue, 11 Jun 2024 13:55:30 GMT</pubDate>
    </item>
    <item>
      <title>如何将 JSON 中的标记坐标叠加到 JPG 图像中以进行 CNN 训练？[关闭]</title>
      <link>https://stackoverflow.com/questions/78606586/how-to-overlay-labeled-coordinates-from-json-into-jpg-images-for-cnn-training</link>
      <description><![CDATA[我正在开展一个计算机视觉项目，该项目涉及检测和分割 MRI 扫描中的骨折。作为该项目的一部分，我让专家直接在图像上标记骨折区域。此过程会生成一个 JSON 文件，其中包含以下信息：

标记区域的坐标
标记区域的名称
标记图像的名称

我面临的挑战是将这些坐标从 JSON 文件转移到相应的 JPG 图像上，以准备进行 CNN 训练。
以下是我的 JSON 文件结构示例：
&quot;item&quot;: {
&quot;name&quot;: &quot;img-00003-00082.jpg&quot;,
&quot;team&quot;: {
&quot;name&quot;: &quot;Mask&quot;,
&quot;slug&quot;: &quot;mask&quot;
&quot;file_name&quot;: &quot;img-00003-00082.jpg&quot;,
&quot;annotations&quot;: [
{
&quot;bounding_box&quot;: {
&quot;h&quot;: 142.16649999999993,
&quot;w&quot;: 124.14549999999997,
&quot;x&quot;: 679.8006,
&quot;y&quot;: 425.7789
},
&quot;name&quot;: &quot;Broken&quot;,
&quot;polygon&quot;: {
&quot;paths&quot;: [
[
{
&quot;x&quot;: 695.1519,
&quot;y&quot;: 567.9454
},
{
&quot;x&quot;: 679.8006,
&quot;y&quot;: 530.5683
},


到目前为止，我已经设法从 JSON 文件中提取了必要的坐标。但是，我很难将这些坐标叠加到 JPG 图像上以生成 CNN 的训练数据。
我的问题：

如何准确地将 JSON 文件中的坐标叠加到相应的 JPG 图像上？
是否有任何推荐的 Python 库或方法专门适合此任务？
]]></description>
      <guid>https://stackoverflow.com/questions/78606586/how-to-overlay-labeled-coordinates-from-json-into-jpg-images-for-cnn-training</guid>
      <pubDate>Tue, 11 Jun 2024 09:28:24 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Huggingface Pipeline 中使用适配器变压器</title>
      <link>https://stackoverflow.com/questions/77673353/how-to-use-adapter-transformers-with-a-huggingface-pipeline</link>
      <description><![CDATA[我尝试运行模型“AdapterHub/bert-base-uncased-pf-conll2003” （此处为模型描述）用于 NLP 中的标记分类。
首先，我尝试安装适配器变压器
pip install -U adapter-transformers 

上述命令的输出是
收集适配器变压器

[... 查看跳过的行的编辑历史记录...]

安装收集的软件包：tokenizers、huggingface-hub、adapter-transformers
尝试卸载：tokenizers
找到现有安装：tokenizers 0.15.0
卸载 tokenizers-0.15.0：
成功卸载 tokenizers-0.15.0
尝试卸载：huggingface-hub
找到现有安装：huggingface-hub 0.19.4
卸载 huggingface-hub-0.19.4:
成功卸载 huggingface-hub-0.19.4
错误：pip 的依赖解析器目前未考虑已安装的所有软件包。此行为是以下依赖冲突的根源。
transformers 4.35.2 需要 huggingface-hub&lt;1.0,&gt;=0.16.4，但您拥有不兼容的 huggingface-hub 0.13.4。
transformers 4.35.2 需要 tokenizers&lt;0.19,&gt;=0.14，但您拥有不兼容的 tokenizers 0.13.3。
成功安装 adapter-transformers-3.2.1.post0 huggingface-hub-0.13.4 tokenizers-0.13.3


我尝试将模型像这样加载到管道中：
from transformers import AutoModelWithHeads
from transformers import pipeline
token_classification = pipeline(&quot;token-classification&quot;, model = &quot;AdapterHub/bert-base-uncased-pf-conll2003&quot;)
res = token_classification(&quot;从垃圾箱中取出垃圾袋并放回原处。&quot;)
print(res)

我收到了错误
EntryNotFoundError: 404 客户端错误。 （请求 ID：Root=1-657e793c-0ce0c1936aff5e5741676650）

未找到 URL 的条目：https://huggingface.co/AdapterHub/bert-base-uncased-pf-conll2003/resolve/main/config.json。

在处理上述异常期间，发生了另一个异常：

OSError Traceback（最近一次调用最后一次）
&lt;ipython-input-3-030dfe0e128d&gt; in &lt;cell line: 3&gt;()
1 from transformers import AutoModelWithHeads
2 from transformers import pipeline
----&gt; 3 token_classification = pipeline(&quot;token-classification&quot;, model = &quot;AdapterHub/bert-base-uncased-pf-conll2003&quot;)
4 res = token_classification(&quot;从垃圾箱中取出垃圾袋并放回原处。&quot;)
5 print(res)

/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)
673 hub_kwargs[&quot;_commit_hash&quot;] = config._commit_hash
674 elif config is None and isinstance(model, str):
--&gt; 675 config = AutoConfig.from_pretrained(model, _from_pipeline=task, **hub_kwargs, **model_kwargs)
676 hub_kwargs[&quot;_commit_hash&quot;] = config._commit_hash
677 

[... 查看跳过的行的编辑历史记录 ...]

/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
624 try:
625 # 从本地文件夹或缓存加载或从模型 Hub 和缓存下载
--&gt; 626 solved_config_file = cached_file(
627 pretrained_model_name_or_path,
628 configuration_file,

/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)
452 if revision is None:
453 revision = &quot;main&quot;
--&gt; 454 raise EnvironmentError(
455 f&quot;{path_or_repo_id} 似乎没有名为 {full_filename} 的文件。结帐&quot;
456 f&quot;&#39;https://huggingface.co/{path_or_repo_id}/{revision}&#39; 查找可用文件。&quot;

OSError：AdapterHub/bert-base-uncased-pf-conll2003 似乎没有名为 config.json 的文件。
查看 &#39;https://huggingface.co/AdapterHub/bert-base-uncased-pf-conll2003/main&#39; 查找可用文件。

如何正确加载此适配器模型？]]></description>
      <guid>https://stackoverflow.com/questions/77673353/how-to-use-adapter-transformers-with-a-huggingface-pipeline</guid>
      <pubDate>Sun, 17 Dec 2023 04:49:28 GMT</pubDate>
    </item>
    <item>
      <title>如何获取 keras 模型的中间输出？</title>
      <link>https://stackoverflow.com/questions/75269671/how-to-get-an-intermediate-output-of-a-keras-model</link>
      <description><![CDATA[例如，我想查看模型的中间输出：
import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4,activation=tf.nn.relu)(inputs)
x1 = tf.keras.layers.Dense(5,activation=tf.nn.softmax)(x)
outputs = tf.keras.layers.Dense(5,activation=tf.nn.softmax)(x1)
model = tf.keras.Model(inputs=inputs,outputs=outputs)
model.compile()

实际模型非常复杂，我只是提供了一个片段。有没有办法在前向传递中设置断点或类似内容来查看中间模型输出。]]></description>
      <guid>https://stackoverflow.com/questions/75269671/how-to-get-an-intermediate-output-of-a-keras-model</guid>
      <pubDate>Sat, 28 Jan 2023 17:50:47 GMT</pubDate>
    </item>
    <item>
      <title>如何保存 Python ml 模型以便可以在 C# Unity 中运行</title>
      <link>https://stackoverflow.com/questions/75246372/how-to-save-python-ml-model-so-that-in-can-be-run-in-c-sharp-unity</link>
      <description><![CDATA[我有一个用于训练 ml 模型的 python 脚本。我想导出此模型，以便可以在用 C# 编写的 Unity 游戏中加载和使用它。
我知道我无法使用 pickle 或 scikit-learn 的 joblib 序列化模型，因为它们不受 C# 支持。那么有没有办法保存模型，以便可以反序列化并在我的 Unity 游戏中使用它？]]></description>
      <guid>https://stackoverflow.com/questions/75246372/how-to-save-python-ml-model-so-that-in-can-be-run-in-c-sharp-unity</guid>
      <pubDate>Thu, 26 Jan 2023 12:50:23 GMT</pubDate>
    </item>
    </channel>
</rss>