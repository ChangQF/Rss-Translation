<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 11 Dec 2023 12:27:16 GMT</lastBuildDate>
    <item>
      <title>我应该使用什么来向我的测试数据添加 20% 的噪声</title>
      <link>https://stackoverflow.com/questions/77639239/what-should-i-use-to-add-20-noise-to-my-test-data</link>
      <description><![CDATA[我有一个关于 np.random 函数如何工作的问题。我想在测试数据中添加 20% 的噪声，但我不知道如何做到这一点，因为我不知道 np.random 函数实际上是如何工作的。
我应该使用类似的东西吗：
x_test_noise = x_test * np.random.uniform(0.8, 1.2, size=x_test.shape)

或者类似的东西：
x_test_noise=pd.DataFrame()
对于范围内的 i (len(x_test))：
    对于范围内的 j(len(x_test.columns))：
        x_test_noise=x_test*np.random.uniform(0.8,1.2)

我主要关心的是，我希望 x_test 的每个值都乘以从随机分布中提取的不同值。第一个语法是要执行此操作还是要将 x_test 的每个值与从随机分布中抽取的相同值相乘？
提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/77639239/what-should-i-use-to-add-20-noise-to-my-test-data</guid>
      <pubDate>Mon, 11 Dec 2023 11:30:32 GMT</pubDate>
    </item>
    <item>
      <title>我在尝试运行决策树模型时遇到错误</title>
      <link>https://stackoverflow.com/questions/77638235/i-have-an-error-when-trying-to-run-the-decision-tree-model</link>
      <description><![CDATA[我有一个学校机器学习小组项目，它是关于在 NSL-KDD 数据集上重现入侵检测系统研究的结果，我得到了分类部分
我试图实施决策树模型，但我得到了一个 ValueError

该错误与项目的数据准备部分有关吗？
pca部分的代码有一些错误
这是 x_train 数据的示例
]]></description>
      <guid>https://stackoverflow.com/questions/77638235/i-have-an-error-when-trying-to-run-the-decision-tree-model</guid>
      <pubDate>Mon, 11 Dec 2023 08:18:07 GMT</pubDate>
    </item>
    <item>
      <title>使用形状值分析模型时刻度标签中的字形错误</title>
      <link>https://stackoverflow.com/questions/77637695/glyph-errors-in-tick-labels-when-using-shap-values-to-analysis-my-model</link>
      <description><![CDATA[我正在 python 中使用 shap 包为我的模型重新创建一些图表。其中之一是瀑布图，来自 手册我按照使用以下代码生成的（完整代码太长，请查看手册）。
shap.waterfall_plot(shap_explainer_values[4652]) 
但是，我的图表的减号缺失，并出现警告消息“当前字体中缺少 Glyph 8722 (\N{MINUS SIGN})”。

stackoverflow上有很多与这个问题相关的问题，都可以通过来解决
plt.rcParams[&#39;axes.unicode_minus&#39;] = False

但是，我的却不能。有人可以帮助解决这个特定问题吗？非常感谢。
我也尝试了shutil.rmtree(matplotlib.get_cachedir())。]]></description>
      <guid>https://stackoverflow.com/questions/77637695/glyph-errors-in-tick-labels-when-using-shap-values-to-analysis-my-model</guid>
      <pubDate>Mon, 11 Dec 2023 05:54:34 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Python AI/ML/DL 生成专业的头像图像应用程序？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77637655/how-to-generating-a-professional-headshot-image-application-using-python-ai-ml-d</link>
      <description><![CDATA[我想使用 Python AI/ML 创建头像图像，因此有人建议我如何创建它的工作流程，并建议我使用哪种深度学习/机器学习模型
我希望上传简单的五张图像，并使用 AI/ML 输出专业头像图像。]]></description>
      <guid>https://stackoverflow.com/questions/77637655/how-to-generating-a-professional-headshot-image-application-using-python-ai-ml-d</guid>
      <pubDate>Mon, 11 Dec 2023 05:39:33 GMT</pubDate>
    </item>
    <item>
      <title>OpenCv 和 Numpy 无法兼容</title>
      <link>https://stackoverflow.com/questions/77637559/opencv-and-numpy-cannot-be-resloved</link>
      <description><![CDATA[我一直在Python中使用指纹匹配系统，并且我已经安装了所有库，但我无法导入这些库，我的pylance正在发送一条无法解析库的错误消息
在此处输入图像描述
我不想搞乱我迄今为止所做的工作]]></description>
      <guid>https://stackoverflow.com/questions/77637559/opencv-and-numpy-cannot-be-resloved</guid>
      <pubDate>Mon, 11 Dec 2023 05:03:30 GMT</pubDate>
    </item>
    <item>
      <title>在 Tensorflow 中组合图像和表格数据</title>
      <link>https://stackoverflow.com/questions/77637432/combining-image-and-tabular-data-in-tensorflow</link>
      <description><![CDATA[我一直在尝试将图像（胸部 X 光）和表格数据（年龄、性别、BMI 等）结合起来形成二元预测模型（疾病：0 或 1）。我有使用顺序的 2D-CNN，但在合并表格数据时遇到困难。我在网上探索了一些资源（例如 https://machinelearningmastery.com/keras-function -api-deep-learning/），但大多数都已经过时了 - 我还没有找到任何好的例子。 函数式 API 是最好的方法吗？您能否指导我如何针对下面的数据处理此问题？
导入tensorflow为tf
从tensorflow.keras.layers导入输入、Conv2D、MaxPooling2D、展平、密集、连接
从tensorflow.keras.models导入模型

label_encoder = LabelEncoder()
df[&#39;疾病&#39;] = label_encoder.fit_transform(df[&#39;疾病&#39;]).astype(str)

# 将 DataFrame 拆分为训练集和测试集
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df[&#39;疾病&#39;])

# 图像数据生成器
image_datagen = ImageDataGenerator（重新缩放=1/255）

# 训练图像生成器
train_image_generator = image_datagen.flow_from_dataframe(
    训练数据，
    x_col=&#39;文件名&#39;,
    y_col=&#39;疾病&#39;,
    目标大小=(224, 224),
    批量大小=32，
    class_mode=&#39;二进制&#39;,
    dtype=&#39;float32&#39;
）

# 测试图像生成器
test_image_generator = image_datagen.flow_from_dataframe(
    测试数据，
    x_col=&#39;文件名&#39;,
    y_col=&#39;疾病&#39;,
    目标大小=(224, 224),
    批量大小=32，
    class_mode=&#39;二进制&#39;,
    dtype=&#39;float32&#39;
）

# 将“性别”特征转换为数值
train_data[&#39;性别&#39;] = train_data[&#39;性别&#39;].map({&#39;F&#39;: 0, &#39;M&#39;: 1})
test_data[&#39;性别&#39;] = test_data[&#39;性别&#39;].map({&#39;F&#39;: 0, &#39;M&#39;: 1})

# 用于训练的表格特征
train_tabular_features = train_data[[&#39;年龄&#39;, &#39;性别&#39;, &#39;身高&#39;, &#39;体重&#39;]].values.astype(float)

# 用于测试的表格特征
test_tabular_features = test_data[[&#39;年龄&#39;, &#39;性别&#39;, &#39;身高&#39;, &#39;体重&#39;]].values.astype(float)

# 定义图像输入层
img_input = 输入(形状=(224, 224, 3), 名称=&#39;image_input&#39;)
x1 = Conv2D(16, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;)(img_input)
x1 = MaxPooling2D()(x1)
x1 = Conv2D(32, 3, 填充=&#39;相同&#39;, 激活=&#39;relu&#39;)(x1)
x1 = MaxPooling2D()(x1)
x1 = 展平()(x1)

# 定义表格输入层
tabular_input = 输入(形状=(4,), name=&#39;tabular_input&#39;)
x2 = 密集（16，激活=&#39;relu&#39;）（tabular_input）
x2 = 密集(32, 激活=&#39;relu&#39;)(x2)
x2 = 展平()(x2)

# 连接图像和表格分支的输出
连接=连接（[x1，x2]）

# 组合特征的公共层
x = 密集（128，激活=&#39;relu&#39;）（连接）
输出层=密集（1，激活=&#39;sigmoid&#39;，名称=&#39;输出&#39;）（x）

# 创建模型
模型 = 模型(输入=[img_input, tabular_input], 输出=output_layer)

# 编译模型
模型.编译(
    优化器=&#39;亚当&#39;,
    损失=&#39;binary_crossentropy&#39;,
    指标=[&#39;准确性&#39;]
）

历史=模型.fit(
    x={
        &#39;image_input&#39;：train_image_generator，
        &#39;表格输入&#39;：train_tabular_features
    },
    y=训练标签，
    纪元=20，
    验证数据=(
        {
            &#39;图像输入&#39;：测试图像生成器，
            &#39;tabular_input&#39;：test_tabular_features
        },
        测试标签
    ）
）


我在处理问题的方式中遇到了各种错误，主要是以下错误。我相信我的方法是错误的，因此寻找任何资源或指南。
&lt;小时/&gt;
ValueError Traceback（最近一次调用最后）
---&gt; 79 历史 = 模型.fit(
80 x={
81&#39;图像输入&#39;：train_image_generator，
82&#39;tabular_input&#39;：train_tabular_features
83}，
ValueError：无法找到可以处理输入的数据适配器：(包含{&quot;&quot;}键和{&quot;&quot;, &quot;&quot;} 值), ]]></description>
      <guid>https://stackoverflow.com/questions/77637432/combining-image-and-tabular-data-in-tensorflow</guid>
      <pubDate>Mon, 11 Dec 2023 04:05:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 CodeT5+ 或其他转换器根据方法的特征生成方法名称</title>
      <link>https://stackoverflow.com/questions/77637422/how-to-generate-method-name-by-methods-features-using-codet5-or-other-transfor</link>
      <description><![CDATA[如何使用 CodeT5+ 或其他转换器根据方法的功能生成方法名称？特征是方法的主体、访问修饰符、参数的类型和名称以及返回类型。我需要从一些 Java 代码中提取这些数据，并使用 CodeT5+ 或其他模型来预测方法的名称，然后将它们与真实的方法名称进行比较。您能给我提供解决此任务的 Python 代码吗？
我尝试了这段代码，但它产生了无意义的结果：
defgenerate_java_function_name（主体，修饰符，参数，return_type）：
    tokenizer = RobertaTokenizer.from_pretrained(&#39;Salesforce/codet5-small&#39;)
    模型 = T5ForConditionalGeneration.from_pretrained(&#39;Salesforce/codet5-small&#39;)

    input_text = f“{modifier.replace(&#39;,&#39;, &#39;&#39;)} {return_type} functionName({params}) {body}”

    input = tokenizer.encode(“函数名称：” + input_text, return_tensors=“pt”, max_length=512, truncation=True)
    输出= model.generate（输入，max_length = 50，num_beams = 5，length_penalty = 0.6，early_stopping = True）

    generated_function_name = tokenizer.decode(outputs[0],skip_special_tokens=True)
    返回生成的函数名称
]]></description>
      <guid>https://stackoverflow.com/questions/77637422/how-to-generate-method-name-by-methods-features-using-codet5-or-other-transfor</guid>
      <pubDate>Mon, 11 Dec 2023 04:01:24 GMT</pubDate>
    </item>
    <item>
      <title>针对我的 TSA 用例的自定义张量流 CNN 实现</title>
      <link>https://stackoverflow.com/questions/77637163/custom-tensorflow-cnn-implemetnation-for-my-tsa-use-case</link>
      <description><![CDATA[我想在时间序列数据集中找到某些特征和模式。我尝试过使用 HMM 聚类，但它没有产生好的结果。我想要实现类似于 RCNN 在 2D 情况下所做的事情，但在一维情况下。我可以在其中为模型提供示例，它会在序列中找到更多示例，并标记找到它的时间范围。给定一个时间序列序列 T，假设我想通过它运行 CNN，并且假设我想要对多个模式进行多类分类，每个模式可以发生多次。在我的具体实现中，我希望能够识别发生特定模式的时间帧 t，因此将卷积开始和结束的日期时间信息保留为特征图中的另一层将很有用。我想保留有关何时发现它的日期时间范围信息。我知道 python 不允许重载，但是我是否可以修改 conv1D 的代码以在特征映射中保留第二层并保留每个特征的时间戳，然后最终可能有一个 [start_time, end_time] 对于特定模式。因此，特征图将是一个 XY2 张量，并且在每次卷积操作之后，新图将包含 [oldest_start_time,newest_end_time] 的日期时间。到目前为止，我的训练是否有任何问题，我是否使用了错误的工具（也许是基于 RNN 的架构？）？在tensorflow中自定义实现通常是如何完成的，我需要编写自己的GD实现吗？
到目前为止还没有尝试过任何事情，只是处于想法阶段。尝试看看我是否使用了错误的工具，可能使用的是 RNN 类型架构。在我承诺编码之前，我需要确保我的想法是正确的。或者，我正在考虑首先进行多类分类，然后为每个命中的类运行自定义 1D-CNN 网络，然后 bin 搜索将大小缩小到时间范围内。]]></description>
      <guid>https://stackoverflow.com/questions/77637163/custom-tensorflow-cnn-implemetnation-for-my-tsa-use-case</guid>
      <pubDate>Mon, 11 Dec 2023 02:14:57 GMT</pubDate>
    </item>
    <item>
      <title>调整图像分割模型（来自 TF 教程）以进行二元掩蔽</title>
      <link>https://stackoverflow.com/questions/77635064/adjust-image-segmentaion-model-from-tf-tutorial-for-binary-masking</link>
      <description><![CDATA[我需要 Tensorflow 的图像分割模型。输入为图像和掩码（二进制、掩码或非掩码），输出为带有 0 和 1 的图像掩码。
我遵循了 https://www.tensorflow.org/tutorials/ 中的图像分割教程图像/分割
但现在我想在我的数据集上运行它的二进制掩码（没有边框类）
新数据集已准备好并输入到 model.fit 中。应该没问题吧。
如何将此模型更改为只有 2 个类（非屏蔽和屏蔽）？
base_model: keras.Model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)

# 使用这些层的激活
图层名称 = [
    &#39;block_1_expand_relu&#39;, # 64x64
    &#39;block_3_expand_relu&#39;, # 32x32
    &#39;block_6_expand_relu&#39;, # 16x16
    &#39;block_13_expand_relu&#39;, # 8x8
    &#39;block_16_project&#39;, # 4x4
]
base_model_outputs = [base_model.get_layer(name).layer_names 中名称的输出]

# 创建特征提取模型
down_stack = 模型（输入=base_model.输入，输出=base_model_outputs）

down_stack.trainable = False

上层堆栈 = [
    pix2pix.upsample(512, 3), # 4x4 -&gt; 8x8
    pix2pix.upsample(256, 3), # 8x8 -&gt; 16x16
    pix2pix.upsample(128, 3), # 16x16 -&gt; 32x32
    pix2pix.upsample(64, 3), # 32x32 -&gt; 64x64
]

def unet_model(output_channels:int):
  输入 = 层.Input(形状=[128, 128, 3])

  # 通过模型进行下采样
  跳过= down_stack（输入）
  x = 跳过[-1]
  跳过 = 反转(跳过[:-1])

  # 上采样并建立跳跃连接
  对于 up，在 zip 中跳过（up_stack，skips）：
    x = 上(x)
    concat = 层.Concatenate()
    x = concat([x, 跳过])

  # 这是模型的最后一层
  最后=层.Conv2DTranspose(
      过滤器=output_channels，kernel_size=3，步长=2，
      padding=&#39;相同&#39;) #64x64 -&gt; 128x128

  x = 最后一个(x)

  返回模型（输入=输入，输出=x​​）

输出类 = 3

模型 = unet_model(output_channels=OUTPUT_CLASSES)

model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;binary_crossentropy&#39;,
              指标=[&#39;准确性&#39;])

当我将 OUTPUT_CLASSES 更改为 2 时，出现错误：
W tensorflow/core/kernels/data/generator_dataset_op.cc:108] 完成 GeneratorDataset 迭代器时发生错误：FAILED_PRECONDITION：Python 解释器状态未初始化。该过程可以被终止。

当OUTPUT_CLASSES为1时，预测掩码为空。
也许还必须改变其他东西？我还不熟悉神经网络架构，所以我可能看不到明显的东西。
编辑：
我已将activation=&#39;sigmoid&#39;添加到输出层
 最后 = tf.keras.layers.Conv2DTranspose(
      过滤器=output_channels，kernel_size=3，步长=2，
      填充 = &#39;相同&#39;, 激活 = &#39;sigmoid&#39;) #64x64 -&gt; 128x128

  x = 最后一个(x)

和OUTPUT_CLASSES = 1
奇怪的行为是下一个：
预期的掩模是当我在一个非常小的数据集上训练它时（该数据集中包含的测试的图片和掩模，只是为了测试它如何检测所看到的图像），我在第一个时期得到了一些东西。但纪元越多，结果越差。然而，准确度约为 0.99。
预期掩码：

预测掩码纪元 0：

如果打开图像，您可能会在预期的遮罩部分看到轻微的阴影。
预测掩码纪元1：

...
纪元 4：

所以每次迭代都会变得更糟。
数据集包含不应显示任何蒙版的图像。也许这就是问题所在？ （编辑：从数据集中排除没有掩码的数据 - 没有帮助）]]></description>
      <guid>https://stackoverflow.com/questions/77635064/adjust-image-segmentaion-model-from-tf-tutorial-for-binary-masking</guid>
      <pubDate>Sun, 10 Dec 2023 13:55:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么余弦相似度总是算为1？</title>
      <link>https://stackoverflow.com/questions/77634085/why-is-cosine-similarity-always-counted-as-1</link>
      <description><![CDATA[在做学校项目时，遇到一个问题，余弦相似度总是测量为1。我无奈地质疑余弦相似度总是测量为1，因为我确认tensor1和tensor2不同。为什么余弦相似度总是测量为1？

训练代码

for i, (_image1, _label1) in enumerate(train_loader):
    image1 = _image1.to(设备)
    标签1 = _标签1[0]
    矢量1_张量 = 模型(图像1)
  
    if (i == 0): #异常情况
      图像2 = 图像1
      标签2 = 标签1
      矢量2_张量 = 矢量1_张量
   
     #问题位置

     相似度 = F.cosine_similarity(向量1_张量，向量2_张量，暗淡 = -1)
     scaled_similarity = torch.sigmoid(相似度)

    如果标签1 == 标签2：
      目标向量 = [1]
    别的 ：
      目标向量 = [0]

    target_tensor = torch.tensor(target_vector).float()
    目标张量 = 目标张量.to(设备)

    优化器.zero_grad()
    成本 = 损失（scaled_similarity，target_tensor）
    成本.向后()
    优化器.step()

    如果不是我%40：
      print (f&#39;Epoch: {epoch:03d}/{EPOCH:03d} | &#39;
            f&#39;批次 {i:03d}/{len(train_loader):03d} |&#39;
             f&#39; 成本：{成本：.4f}&#39;)

    #回收张量以减少计算量
    图像2 = 图像1.克隆()
    标签2 = 标签1
    矢量2_张量=矢量1_张量.detach()


模型定义代码

class trans_VGG(nn.Module):
    def __init__(self, base_dim):
        超级（trans_VGG，自我）.__init__（）
        self.feature = nn.Sequential(
            conv_2（3，base_dim），
            conv_2(base_dim, base_dim*2),
            conv_2(base_dim*2,base_dim*4),
            conv_3(base_dim*4,base_dim*8),
            conv_3(base_dim*8, base_dim*8)
        ）
        self.fc_layer = nn.Sequential(
            nn.Linear(base_dim*8*7*7, 4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.线性(4096, 1000),
            nn.ReLU(True),
            nn.Dropout(),
            nn.线性(1000, 800)
        ）
        对于 self.parameters() 中的参数：
            param.requires_grad = True

    def 前向（自身，x）：
        x = self.feature(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layer(x)
        返回x


我们没有使用Pytorch的余弦相似度函数，而是自己创建并计算了余弦相似度函数，发现余弦相似度仍然是1。
我们发现传感器1和传感器2的值不同。
该模型是一个VGG模型，通过嵌入经过VGG模型的两个张量来获得两个张量之间的相似度。
]]></description>
      <guid>https://stackoverflow.com/questions/77634085/why-is-cosine-similarity-always-counted-as-1</guid>
      <pubDate>Sun, 10 Dec 2023 07:59:46 GMT</pubDate>
    </item>
    <item>
      <title>Yolo NAS模块超级梯度安装错误解决办法[已关闭]</title>
      <link>https://stackoverflow.com/questions/77612617/yolo-nas-module-super-gradient-installation-error-solution</link>
      <description><![CDATA[我正在使用 yolo-NAS 进行对象检测，但为此我需要安装一个名为 super-gradients 的模块。我正在 Conda 环境中构建我的项目，该环境是我现在专门为该项目创建的。但是在安装超级梯度模块时，它在中间抛出了一个错误。我已经尝试了所有我能做的方法。这是出现错误的图像在此处输入图像描述
我尝试更新 Visual C++，如上所述，当我在 google 中搜索解决方案时，我也尝试安装一些模块。如果有人给我一个解决方案，那将是一个很大的帮助。]]></description>
      <guid>https://stackoverflow.com/questions/77612617/yolo-nas-module-super-gradient-installation-error-solution</guid>
      <pubDate>Wed, 06 Dec 2023 11:18:36 GMT</pubDate>
    </item>
    <item>
      <title>在小数据集上使用 scikit-learn 执行 k-medoids 聚类时出现内存错误</title>
      <link>https://stackoverflow.com/questions/77393501/memoryerror-while-performing-k-medoids-clustering-with-scikit-learn-on-small-dat</link>
      <description><![CDATA[我尝试使用 scikit-learn 应用 k-medoids 聚类，但尽管数据集大小为 26 MB，但存在内存错误。
错误：
MemoryError：无法为形状为 (426740, 426740) 且数据类型为 float64 的数组分配 1.33 TiB

将 numpy 导入为 np
将 pandas 导入为 pd

# 导入数据集
数据集 = pd.read_csv(&#39;dish.csv&#39;)
X = dataset.iloc[:, 5:].values
打印（X）

# 处理丢失的数据
从 sklearn.impute 导入 SimpleImputer
imputer = SimpleImputer（missing_values = np.nan，策略=&#39;平均值&#39;）
imputer.fit(X[:, 2:])
X[:, 2:] = imputer.transform(X[:, 2:])
打印（X）

# 特征缩放
从 sklearn.preprocessing 导入 StandardScaler
sc = 标准缩放器()
X[:, :2] = sc.fit_transform(X[:, :2])
打印（X）


从 sklearn_extra.cluster 导入 KMedoids

n_簇 = 25

kmedoids = KMedoids(n_clusters=n_clusters, random_state=1).fit(X[:, :])

cluster_labels = kmedoids.labels_

打印（簇标签）

cluster_centers = kmedoids.cluster_centers_

数据集[&#39;集群&#39;] = cluster_labels
dataset.to_csv(“clustered_dish.csv”,index=False)
]]></description>
      <guid>https://stackoverflow.com/questions/77393501/memoryerror-while-performing-k-medoids-clustering-with-scikit-learn-on-small-dat</guid>
      <pubDate>Tue, 31 Oct 2023 05:49:17 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 ModuleNotFoundError：没有名为“gluonts.torch.modules.distribution_output”的模块？</title>
      <link>https://stackoverflow.com/questions/75315828/how-to-resolve-modulenotfounderror-no-module-named-gluonts-torch-modules-distr</link>
      <description><![CDATA[我正在开发一个项目，并尝试运行此存储库中的代码：
https://github.com/jc-audet/WOODS
我能够通过这一步：
python3 -m woods.scripts.download_datasets {数据集} \
        --data_path /路径/到/数据/目录

然后当我尝试运行此步骤时：
python3 -m woods.scripts.main train \
        --数据集 Spurious_Fourier \
        --客观的企业风险管理\
        --test_env 0 \
        --data_path /路径/到/数据/目录

我收到以下错误：
文件“/Users/evangertis/development/UGA/Research/WOODS/woods/datasets.py”，第 1839 行，在  中
    从 gluonts.torch.modules.distribution_output 导入（
ModuleNotFoundError：没有名为“gluonts.torch.modules.distribution_output”的模块

我期望代码能够在一个测试环境下在一个数据集上使用一个目标来训练模型。我收到以下错误：
 warnings.warn(
回溯（最近一次调用最后一次）：
  文件“/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py”，第 196 行，在 _run_module_as_main 中
    返回_run_code（代码，main_globals，无，
  文件“/usr/local/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py”，第 86 行，在 _run_code 中
    执行（代码，run_globals）
  文件“/Users/evangertis/development/UGA/Research/WOODS/woods/scripts/hparams_sweep.py”，第14行，在&lt;module&gt;中。
    从树林导入实用程序
  文件“/Users/evangertis/development/UGA/Research/WOODS/woods/utils.py”，第13行，在&lt;模块&gt;中。
    从 woods.scripts 导入 hparams_sweep
  文件“/Users/evangertis/development/UGA/Research/WOODS/woods/scripts/hparams_sweep.py”，第17行，在&lt;module&gt;中。
    从树林导入数据集
  文件“/Users/evangertis/development/UGA/Research/WOODS/woods/datasets.py”，第1839行，在&lt;模块&gt;中。
    从 gluonts.torch.modules.distribution_output 导入（
ModuleNotFoundError：没有名为“gluonts.torch.modules.distribution_output”的模块
]]></description>
      <guid>https://stackoverflow.com/questions/75315828/how-to-resolve-modulenotfounderror-no-module-named-gluonts-torch-modules-distr</guid>
      <pubDate>Wed, 01 Feb 2023 20:25:31 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Optuna 甚至建议记录参数的日志？</title>
      <link>https://stackoverflow.com/questions/74978660/why-optuna-even-suggests-to-take-a-log-of-a-parameter</link>
      <description><![CDATA[在官方 Optuna 教程 有一个使用 Trial.suggest_int 的 log=True 参数的示例：
导入火炬
将 torch.nn 导入为 nn


def create_model(试验, in_size):
    n_layers = Trial.suggest_int(“n_layers”, 1, 3)

    层=[]
    对于范围内的 i（n_layers）：
        n_units = Trial.suggest_int(“n_units_l{}”.format(i), 4, 128, log=True)
        层.追加（nn.Linear（in_size，n_units））
        层.append(nn.ReLU())
        in_size = n_units
    层.append(nn.Linear(in_size, 10))

    返回 nn.Sequential(*layers)

为什么有人会取神经元数量的对数？本教程中还有其他 (IMO) 冗余使用 log=True 的实例。有人可以解释一下他们的动机吗？]]></description>
      <guid>https://stackoverflow.com/questions/74978660/why-optuna-even-suggests-to-take-a-log-of-a-parameter</guid>
      <pubDate>Mon, 02 Jan 2023 02:45:43 GMT</pubDate>
    </item>
    <item>
      <title>Python：如何从 Optuna LightGBM 研究中检索最佳模型？</title>
      <link>https://stackoverflow.com/questions/62144904/python-how-to-retrieve-the-best-model-from-optuna-lightgbm-study</link>
      <description><![CDATA[我希望获得最佳模型，以便稍后在笔记本中使用，以使用不同的测试批次进行预测。
可重现的示例（取自 Optuna Github）：
导入 lightgbm 为 lgb
将 numpy 导入为 np
导入 sklearn.datasets
导入 sklearn.metrics
从 sklearn.model_selection 导入 train_test_split

导入奥图纳


# 仅供参考：目标函数可以接受额外的参数
#（https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args）。
定义目标（试用）：
    数据，目标 = sklearn.datasets.load_breast_cancer(return_X_y=True)
    train_x、valid_x、train_y、valid_y = train_test_split（数据、目标、test_size=0.25）
    dtrain = lgb.Dataset(train_x, label=train_y)
    dvalid = lgb.Dataset(valid_x, label=valid_y)

    参数 = {
        “目标”：“二进制”，
        “公制”：“auc”，
        “详细程度”：-1，
        &quot;boosting_type&quot;: &quot;gbdt&quot;,
        &quot;lambda_l1&quot;: Trial.suggest_loguniform(&quot;lambda_l1&quot;, 1e-8, 10.0),
        &quot;lambda_l2&quot;: Trial.suggest_loguniform(&quot;lambda_l2&quot;, 1e-8, 10.0),
        &quot;num_leaves&quot;: Trial.suggest_int(&quot;num_leaves&quot;, 2, 256),
        &quot;feature_fraction&quot;: Trial.suggest_uniform(&quot;feature_fraction&quot;, 0.4, 1.0),
        &quot;bagging_fraction&quot;: Trial.suggest_uniform(&quot;bagging_fraction&quot;, 0.4, 1.0),
        &quot;bagging_freq&quot;: Trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),
        &quot;min_child_samples&quot;: Trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),
    }

    # 添加用于修剪的回调。
    pruning_callback = optuna.integration.LightGBMPruningCallback（试用版，“auc”）
    gbm = lgb.train(
        参数，dtrain，valid_sets = [dvalid]，verbose_eval = False，callbacks = [pruning_callback]
    ）

    preds = gbm.predict(valid_x)
    pred_labels = np.rint(preds)
    准确度 = sklearn.metrics.accuracy_score(valid_y, pred_labels)
    返回精度


我的理解是，下面的研究将调整准确性。我想以某种方式从研究中检索最佳模型（不仅仅是参数）而不将其保存为泡菜，我只想在笔记本中的其他地方使用该模型。 

&lt;前&gt;&lt;代码&gt;
如果 __name__ == &quot;__main__&quot;:
    研究 = optuna.create_study(
        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)，方向=“最大化”
    ）
    研究.优化（目标，n_Trials=100）

    print(&quot;最佳试用：&quot;)
    试验 = 研究.best_试验

    print(&quot; 参数: &quot;)
    对于 Trial.params.items() 中的键、值：
        print(&quot; {}: {}&quot;.format(key, value))


期望的输出是
best_model = ~上面的模型~
new_target_pred = best_model.predict(new_data_test)
指标.accuracy_score(new_target_test, new__target_pred)

]]></description>
      <guid>https://stackoverflow.com/questions/62144904/python-how-to-retrieve-the-best-model-from-optuna-lightgbm-study</guid>
      <pubDate>Tue, 02 Jun 2020 04:35:05 GMT</pubDate>
    </item>
    </channel>
</rss>