<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 19 Dec 2024 09:19:58 GMT</lastBuildDate>
    <item>
      <title>ESP32 S3 MINI 1 上的 TinyML 部署问题</title>
      <link>https://stackoverflow.com/questions/79293436/tinyml-deployment-issue-on-esp32-s3-mini-1</link>
      <description><![CDATA[我正在开展一个项目，使用 TinyML 根据 6 轴加速度计和陀螺仪数据预测马的活动。数据通过 ESP32-S3 收集，我正尝试在 ESP32-S3 上部署 TensorFlow Lite (TFLite) 模型以进行实时预测。
但是，我遇到了几个库兼容性和部署问题。尽管遵循了标准的 TinyML 部署实践，但模型与 ESP32-S3 的集成似乎存在问题，尤其是在处理 TFLite Micro 运行时时。我尝试了一些优化，但仍然无法顺利运行。
我很感激任何有关的建议：
如何在 ESP32-S3 上有效地部署 TinyML 模型。
可能更适合此用例的替代方法或工具。
任何提示、文档或类似设置的经验都会有很大帮助！
提前感谢！]]></description>
      <guid>https://stackoverflow.com/questions/79293436/tinyml-deployment-issue-on-esp32-s3-mini-1</guid>
      <pubDate>Thu, 19 Dec 2024 07:39:46 GMT</pubDate>
    </item>
    <item>
      <title>无论如何，PyTorch DeiT 模型都会持续预测一个类别</title>
      <link>https://stackoverflow.com/questions/79293139/pytorch-deit-model-keeps-predicting-one-class-no-matter-what</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79293139/pytorch-deit-model-keeps-predicting-one-class-no-matter-what</guid>
      <pubDate>Thu, 19 Dec 2024 04:52:07 GMT</pubDate>
    </item>
    <item>
      <title>如何使用神经网络为二元分类器绘制平滑的决策边界？</title>
      <link>https://stackoverflow.com/questions/79292743/how-to-plot-a-smooth-decision-boundary-for-a-binary-classifier-using-a-neural-ne</link>
      <description><![CDATA[我正在尝试实现一个简单的神经网络来对圆形数据集进行分类并绘制一个平滑的决策边界。但是，我的决策边界不平滑或不正确。我希望它看起来像此示例中显示的边界：
像这样 
当我运行代码时，决策边界是锯齿状的，或者与我的数据集的形状不匹配。我希望内部类周围有一条平滑的曲线。
这是我的代码：
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles

# 生成数据集
X, y = make_circles(n_samples=100, noise=0.1, factor=0.3, random_state=0)
X = X.T # 转置 X
y = y.reshape((1, y.shape[0])) # 将 y 重塑为行向量

# 数据集可视化
plt.scatter(X[0, :], X[1, :], c=y.flatten())
plt.title(&#39;Dataset&#39;)
plt.xlabel(&#39;Feature 1&#39;)
plt.ylabel(&#39;Feature 2&#39;)
plt.show()

def paramètres(n0=2, n1=2, n2=1):
W1 = np.random.randn(n1, n0)
b1 = np.zeros((n1, 1))
W2 = np.random.randn(n2, n1)
b2 = np.zeros((n2, 1))
参数 = {
&#39;W1&#39;: W1,
&#39;b1&#39;: b1,
&#39;W2&#39;: W2,
&#39;b2&#39;: b2
}
返回参数

def G(X, theta, b):
Z = theta.dot(X) + b
返回 Z, 1/(1+np.exp(-Z))

def forwardPropagation(X, 参数):
W1 = 参数[&#39;W1&#39;]
b1 = 参数[&#39;b1&#39;]
W2 = 参数[&#39;W2&#39;]
b2 =参数[&#39;b2&#39;]

Z1, A1 = G(X, W1, b1)
Z2, A2 = G(A1, W2, b2) 

激活 = {
&#39;A1&#39;: A1,
&#39;A2&#39;: A2
}
返回激活

def backPropagation(X, y, 参数, 激活):
A1 = 激活[&#39;A1&#39;]
A2 = 激活[&#39;A2&#39;]
W2 = 参数[&#39;W2&#39;]

m = y.shape[1]

dZ2 = A2 - y
dW2 = 1 / m * dZ2.dot(A1.T)
db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)

dZ1 = np.dot(W2.T, dZ2) * A1 * (1 - A1)
dW1 = 1 / m * dZ1.dot(X.T)
db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)

gradients = {
&#39;dW1&#39;: dW1,
&#39;db1&#39;: db1,
&#39;dW2&#39;: dW2,
&#39;db2&#39;: db2
}

return gradients

def update(gradients, parametres, learning_rate):
W1 = parametres[&#39;W1&#39;]
b1 = parametres[&#39;b1&#39;]
W2 = parametres[&#39;W2&#39;]
b2 = parametres[&#39;b2&#39;]

dW1 = gradients[&#39;dW1&#39;]
db1 = gradients[&#39;db1&#39;]
dW2 = gradients[&#39;dW2&#39;]
db2 = gradients[&#39;db2&#39;]

W1 = W1 - 学习率 * dW1
b1 = b1 - 学习率 * db1
W2 = W2 - 学习率 * dW2
b2 = b2 - 学习率 * db2

参数 = {
&#39;W1&#39;: W1,
&#39;b1&#39;: b1,
&#39;W2&#39;: W2,
&#39;b2&#39;: b2
}

返回参数

def predict(X, 参数):
激活 = forwardPropagation(X, 参数)
返回激活[&#39;A2&#39;]

def neuro_network(X, y, n1=2, learning_rate=0.1, n_iter=1000):
n0 = X.shape[0] 
n2 = y.shape[0] 
np.random.seed(0)
参数 = paramètres(n0, n1, n2)

for i in range(n_iter):
activations = forwardPropagation(X, parametres)
gradients = backPropagation(X, y, parametres,activations)
parametres = update(gradients, parametres, learning_rate)

return parametres,activations

# 训练模型
parametres,activations = neuro_network(X, y, n1=2, learning_rate=0.1, n_iter=1000)

# 绘制决策边界
x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1
y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
np.arange(y_min, y_max, 0.02))

Z = predict(np.c_[xx.ravel(), yy.ravel()].T, parametres)

Z = Z.reshape(xx.shape)

plt.figure()
plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.8, cmap=plt.cm.RdYlBu)
plt.scatter(X[0, :], X[1, :], c=y.flatten(), cmap=plt.cm.RdYlBu)
plt.title(&#39;决策边界&#39;)
plt.xlabel(&#39;特征 1&#39;)
plt.ylabel(&#39;特征 2&#39;)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79292743/how-to-plot-a-smooth-decision-boundary-for-a-binary-classifier-using-a-neural-ne</guid>
      <pubDate>Wed, 18 Dec 2024 23:11:54 GMT</pubDate>
    </item>
    <item>
      <title>Keras 模型：调试维度后输入形状不匹配问题</title>
      <link>https://stackoverflow.com/questions/79292676/keras-model-input-shape-mismatch-issue-after-debugging-dimensions</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79292676/keras-model-input-shape-mismatch-issue-after-debugging-dimensions</guid>
      <pubDate>Wed, 18 Dec 2024 22:34:35 GMT</pubDate>
    </item>
    <item>
      <title>如何以与“https://www.tensorflow.org/tfmodels/vision/object_detection”中类似的方式修改配置文件？</title>
      <link>https://stackoverflow.com/questions/79292447/how-can-i-modify-the-config-file-in-a-similar-way-used-in-https-www-tensorflo</link>
      <description><![CDATA[我是这个领域的新手，我正在寻找有关如何修改现有脚本以使用 EfficientDet D1 模型的指导。我按照教程操作，并使用默认脚本成功训练了一个自定义数据集。该脚本使用以下行来配置模型：
exp_config = exp_factory.get_exp_config(&#39;retinanet_resnetfpn_coco&#39;)

这对于默认的 RetinaNet 模型来说很好。但是，我想改用 EfficientDet D1 模型。我已经下载了 EfficientDet D1 配置文件，但不确定如何在脚本中引用它。
我尝试过的方法

检查了配置文件：我检查了 EfficientDet D1 配置文件中的参数，看它是否有任何明确的引用名称，可以与 exp_factory.get_exp_config() 一起使用。
检查了替代配置方法：我寻找了加载自定义模型配置的其他方法，但找不到任何明确的说明。

我正在寻找什么

如何修改 exp_factory.get_exp_config() 行以引用 EfficientDet D1 配置文件？
如果这种方法不可行，如何手动加载和引用配置文件？
配置文件本身是否需要进行任何特定更改才能使其工作？

如果需要，我愿意手动修改配置文件参数。并尝试通过此方法运行训练
!python model_main_tf2.py \
--model_dir=/content/trainingdemo/models/my_efficientDet_d0 \
--pipeline_config_path=/content/trainingdemo/models/my_efficientDet_d0/pipeline.config

&quot;但结果是`
2024-12-18 23:14:43.163543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] 无法注册 cuFFT 工厂：尝试注册插件 cuFFT 工厂，但已注册一个
警告：调用 absl::InitializeLog() 之前的所有日志消息都写入 STDERR
E0000 00:00:1734563683.182126 16153 cuda_dnn.cc:8310] 无法注册 cuDNN 工厂：尝试注册插件 cuDNN 工厂，但已注册一个
E0000 00:00:1734563683.187813 16153 cuda_blas.cc:1418] 无法注册 cuBLAS 工厂：尝试注册插件 cuBLAS 的工厂，但已注册一个工厂
回溯（最近一次调用）：
文件 &quot;/content/trainingdemo/model_main_tf2.py&quot;，第 32 行，位于 &lt;module&gt;
来自 object_detection 导入 model_lib_v2
文件 &quot;/usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py&quot;，第 29 行，位于 &lt;module&gt;
来自 object_detection 导入 eval_util
文件 &quot;/usr/local/lib/python3.10/dist-packages/object_detection/eval_util.py&quot;，第 35 行，位于 &lt;module&gt;
从 object_detection.metrics 导入 coco_evaluation
文件 &quot;/usr/local/lib/python3.10/dist-packages/object_detection/metrics/coco_evaluation.py&quot;，第 28 行，位于 &lt;module&gt;
从 object_detection.utils 导入 o​​bject_detection_evaluation
文件 &quot;/usr/local/lib/python3.10/dist-packages/object_detection/utils/object_detection_evaluation.py&quot;，第 46 行，位于 &lt;module&gt;
从 object_detection.utils 导入 label_map_util
文件 &quot;/usr/local/lib/python3.10/dist-packages/object_detection/utils/label_map_util.py&quot;，第 29 行，位于 &lt;module&gt;
从 object_detection.protos 导入 string_int_label_map_pb2
文件 &quot;/usr/local/lib/python3.10/dist-packages/object_detection/protos/string_int_label_map_pb2.py&quot;，第 33 行，位于 &lt;module&gt;
_descriptor.EnumValueDescriptor(
文件 &quot;/usr/local/lib/python3.10/dist-packages/google/protobuf/descriptor.py&quot;，第 789 行，位于 __new__
_message.Message._CheckCalledFromGeneratedFile()
TypeError：无法直接创建描述符。
如果此调用来自 _pb2.py 文件，则您生成的代码已过时，必须使用 protoc &gt;= 3.19.0 重新生成。
如果您无法立即重新生成您的原型，其他一些可能的解决方法是：
1. 将 protobuf 包降级到 3.20.x 或更低版本。
2. 设置 PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python（但这将使用纯 Python 解析，速度会慢得多）。

`
任何指导或代码示例将不胜感激。提前致谢！]]></description>
      <guid>https://stackoverflow.com/questions/79292447/how-can-i-modify-the-config-file-in-a-similar-way-used-in-https-www-tensorflo</guid>
      <pubDate>Wed, 18 Dec 2024 20:49:15 GMT</pubDate>
    </item>
    <item>
      <title>这种模式有哪些可能的增强？</title>
      <link>https://stackoverflow.com/questions/79292370/what-is-the-possible-enhancement-for-this-mode</link>
      <description><![CDATA[我使用 LSTM 对多标签电影类型进行分类，并使用 Word2Vec 作为特征提取；如图所示，该模型得出的指标为测试损失：0.3067，测试准确率：0.5144。

这个模型可能有哪些改进？ 
我使用下面的代码：
# 标记文本
tokenizer = Tokenizer()
tokenizer.fit_on_texts(merged_df[&#39;clean&#39;])

sequences = tokenizer.texts_to_sequences(merged_df[&#39;clean&#39;])
X = pad_sequences(sequences, maxlen=max_len, padding=&#39;post&#39;, truncating=&#39;post&#39;) # 将序列填充到最大长度

# 词汇表大小
vocab_size = len(tokenizer.word_index) + 1 # 包含 0 作为填充
print(f&quot;词汇表大小： {vocab_size}&quot;)

#vocab_size=37696
embedding_dim=300
max_len=1000

embedding_matrix = np.zeros((vocab_size, embedding_dim))

# 将单词映射到向量
for word, i in tokenizer.word_index.items():
if word in word2vec:
embedding_matrix[i] = word2vec[word]
else:
# 使用随机值初始化不在 Word2Vec 中的单词
embedding_matrix[i] = np.random.uniform(-0.25, 0.25, embedding_dim)

print(f&quot;Embedding Matrix Shape: {embedding_matrix.shape}&quot;)

from keras.layers import Input, Embedding, Bidirectional, LSTM, Attention, BatchNormalization、Dropout、Dense
来自 keras.models 导入模型

# 定义输入层 (shape = (None, max_len))
input_layer = Input(shape=(max_len,))
embedded_input = Embedding(input_dim=vocab_size, 
output_dim=embedding_dim, 
weights=[embedding_matrix], 
input_length=max_len, 
trainable=False)(input_layer)
query = Bidirectional(LSTM(128,activation=&#39;tanh&#39;, return_sequences=True))(embedded_input)
attention_output = Attention()([query, query]) # 自注意力机制
attention_output = BatchNormalization()(attention_output)
attention_output = Dropout(0.5)(attention_output)
lstm_output = Bidirectional(LSTM(64,激活=&#39;tanh&#39;，return_sequences=True))(attention_output)
lstm_output = Dropout(0.5)(lstm_output)
lstm_output2 = Bidirectional(LSTM(32，激活=&#39;tanh&#39;，return_sequences=False))(lstm_output)
lstm_output2 = Dropout(0.5)(lstm_output2)
output_layer = Dense(y.shape[1]，激活=&#39;sigmoid&#39;)(lstm_output2)
model = Model(输入=input_layer，输出=output_layer)
model.compile(优化器=&#39;adam&#39;，损失=&#39;binary_crossentropy&#39;，指标=[&#39;accuracy&#39;])

X_train，X_test，y_train，y_test = train_test_split(X，y，test_size=0.2，random_state=42)

early_stop = EarlyStopping(monitor=&#39;val_loss&#39;, waiting=5, restore_best_weights=True,mode=&#39;auto&#39;)
lr_scheduler = Red**strong text**uceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.5, waiting=3, verbose=1)

history =model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, callbacks=[early_stop, lr_scheduler])
]]></description>
      <guid>https://stackoverflow.com/questions/79292370/what-is-the-possible-enhancement-for-this-mode</guid>
      <pubDate>Wed, 18 Dec 2024 20:11:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 ssd 和 mobilenetv2 进行对象检测时“目标”和“输出形状”不匹配</title>
      <link>https://stackoverflow.com/questions/79292180/mismatch-target-and-output-shape-on-object-detection-using-ssd-and-mobilenet</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79292180/mismatch-target-and-output-shape-on-object-detection-using-ssd-and-mobilenet</guid>
      <pubDate>Wed, 18 Dec 2024 18:50:54 GMT</pubDate>
    </item>
    <item>
      <title>“使用 YOLO 和 EasyOCR 进行车牌识别时遇到的文本识别问题”</title>
      <link>https://stackoverflow.com/questions/79291987/text-recognition-issues-in-license-plate-recognition-using-yolo-and-easyocr</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79291987/text-recognition-issues-in-license-plate-recognition-using-yolo-and-easyocr</guid>
      <pubDate>Wed, 18 Dec 2024 17:26:40 GMT</pubDate>
    </item>
    <item>
      <title>我如何识别该图像上的物体？</title>
      <link>https://stackoverflow.com/questions/79291781/how-can-i-recognize-objects-on-this-image</link>
      <description><![CDATA[我有这样的图片：
示例
但所有图片都调整为 120x80。
我需要识别图片上的内容，数字（从 1 到 9）还是字母（完整英文字母）。
但是我的模型：
(block1): CNNBlock(
(conv): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)
(act): ReLU()
(maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
(block2): CNNBlock(
(conv): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)
(act): ReLU()
(maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
(block3): CNNBlock(
(conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=same)
(act): ReLU()
(maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
(block4): CNNBlock(
(conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=same)
(act): ReLU()
(maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
(conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))
(act1): ReLU()
(conv2): Conv2d(512, 1024，kernel_size=(3, 3)，stride=(1, 1))
(act2): ReLU()
(globalmaxpool): AdaptiveMaxPool2d(output_size=1)
(linear1): 线性(in_features=1024，out_features=512，bias=True)
(act3): LeakyReLU(negative_slope=0.01)
(linear2): 线性(in_features=512，out_features=256，bias=True)
(act4): LeakyReLU(negative_slope=0.01)
(linear3): 线性(in_features=256，out_features=128，bias=True)
(act5): LeakyReLU(negative_slope=0.01)
(linear4): 线性(in_features=128， out_features=64, bias=True)
(act6): LeakyReLU(negative_slope=0.01)
(linear5): Linear(in_features=64, out_features=35, bias=True)
(act7): Softmax(dim=None)
)

未学习。它只是堆叠在 ~3.6 损失（CrossEntropy 损失，35 个类）上。
然后，我尝试查看每层之后的图像，它们都相同。我的对象无法进入下一层。我尝试增加 Conv2d 的内核大小，减少过滤器数量，但不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/79291781/how-can-i-recognize-objects-on-this-image</guid>
      <pubDate>Wed, 18 Dec 2024 16:20:26 GMT</pubDate>
    </item>
    <item>
      <title>Yolov5 模型捕获除预期或期望对象之外的其他对象</title>
      <link>https://stackoverflow.com/questions/79291357/yolov5-model-capturing-other-objects-other-than-the-intended-or-desired-object</link>
      <description><![CDATA[我有一个用于数据集的 YOLO V5 m 模型，用于检测特定产品，但在对不同情况和场景以及各种光照条件下的产品图片数据集进行训练后，在 50 个 epoch 和 Yolo v5 中等架构以及批处理大小为 16 的情况下，情况没有重复。
我注意到，该模型依赖于颜色特征，这意味着检测到的对象具有黄色及其阴影。我使用了 yolo github 克隆附带的 train.py。我应该怎么做才能解决这个问题？
用于训练的数据集（大约 450 张图片）在谷歌驱动器中。
需要检测的产品：


检测到的其他产品：
]]></description>
      <guid>https://stackoverflow.com/questions/79291357/yolov5-model-capturing-other-objects-other-than-the-intended-or-desired-object</guid>
      <pubDate>Wed, 18 Dec 2024 14:01:11 GMT</pubDate>
    </item>
    <item>
      <title>随机森林分类器的修改[关闭]</title>
      <link>https://stackoverflow.com/questions/79290974/modification-of-random-forest-classifier</link>
      <description><![CDATA[我正在尝试更改随机森林分类器的功能。虽然通常每次分割都会随机选择特征，但我希望每次分割时都评估一个特定特征。我知道这会影响性能，但我想尝试一下这在非常具体的用例中是否是个好主意。因此，调整的结果应为：用于分割的特征是随机选择的（像往常一样），但始终会考虑一个特定特征（例如索引 15）（不一定使用）。据我所知，没有允许我指定该功能的函数（如果有，请告诉我）。]]></description>
      <guid>https://stackoverflow.com/questions/79290974/modification-of-random-forest-classifier</guid>
      <pubDate>Wed, 18 Dec 2024 11:48:27 GMT</pubDate>
    </item>
    <item>
      <title>如何在 OpenAI API 中仅发送提示的动态部分以优化令牌使用？</title>
      <link>https://stackoverflow.com/questions/79290775/how-to-send-only-dynamic-part-of-a-prompt-in-openai-api-to-optimize-token-usage</link>
      <description><![CDATA[我正在处理一项任务，需要与 OpenAI API 交互以处理动态祈祷请求。我想解决的问题是优化令牌使用。
这是我目前的方法：
我有一个静态系统指令，它定义了助手的行为（例如，以同理心回应祈祷请求）。
用户的祈祷请求是动态的，每次都会发生变化。
我想在每个请求中只发送动态部分（祈祷请求），同时保持静态部分（系统指令）在每个请求中保持一致。
我的目标是减少令牌使用量，同时确保助手对用户的请求做出适当的响应。
我尝试在每个请求中将静态部分作为提示的一部分发送，但我正在寻找一种更有效的方法来只发送动态部分，同时仍保持上下文。有没有更好的方法可以通过 OpenAI API 实现这一点？
此外，我想跟踪令牌使用情况，以确保尽可能优化调用。
有人能建议一种有效的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79290775/how-to-send-only-dynamic-part-of-a-prompt-in-openai-api-to-optimize-token-usage</guid>
      <pubDate>Wed, 18 Dec 2024 10:33:04 GMT</pubDate>
    </item>
    <item>
      <title>图像预处理步骤[关闭]</title>
      <link>https://stackoverflow.com/questions/79288818/image-preprocessing-steps</link>
      <description><![CDATA[我想知道 Keras 图像数据加载是否对图像数据集进行了完整的预处理，例如应用过滤器、规范化、标准化等，在该模块之后我们是否不需要进行更详细的预处理？如果答案是肯定的，那么有人可以告诉我正确和准确的预处理步骤吗？
代码如下。
malimg=tf.keras.utils.image_dataset_from_directory(
r&quot;C:\Users\PMYLS\Desktop\Malware Pycharm Project\malimg_paper_dataset_imgs&quot;,
labels=&quot;inferred&quot;,
label_mode=&quot;int&quot;,
class_names=None,
color_mode=&quot;rgb&quot;,
batch_size=64,
image_size=(256, 256),
shuffle=True,
seed=None,
validation_split=None,
subset=None,
interpolation=&quot;bilinear&quot;,
follow_links=False,
crop_to_aspect_ratio=False,
pad_to_aspect_ratio=False,
data_format=None,
verbose=True,
)
class_names = malimg.class_names # 获取类名
print(&quot;Classes:&quot;, class_names)
]]></description>
      <guid>https://stackoverflow.com/questions/79288818/image-preprocessing-steps</guid>
      <pubDate>Tue, 17 Dec 2024 17:17:21 GMT</pubDate>
    </item>
    <item>
      <title>Keras ValueError：从未调用过层顺序，因此没有定义的输出</title>
      <link>https://stackoverflow.com/questions/78722413/keras-valueerror-the-layer-sequential-has-never-been-called-and-thus-has-no-def</link>
      <description><![CDATA[我想将 Keras 的 Grad-CAM 与我自己的 CNN 模型一起使用。我已遵循此 https://keras.io/examples/vision/grad_cam/，其中的 make_gradcam_heatmap 函数也来自此。我对 CNN 还不熟悉，所以我可能忽略了一些显而易见的东西，但为什么它在我运行模型时无法识别呢？
这是我的代码：
import numpy as np
import os
import tensorflow as tf
import keras
from tensorflow.keras.models import load_model
import cv2
from tensorflow.keras.models import Model

os.environ[&quot;KERAS_BACKEND&quot;] = &quot;tensorflow&quot;

从 IPython.display 导入图像，显示
导入 matplotlib 作为 mpl
导入 matplotlib.pyplot 作为 plt

img_path = &#39;/Users/.../image_1.npy&#39;

model = load_model(&#39;/Users/.../particle_classifier_model.h5&#39;)

model_builder = keras.applications.xception.Xception
preprocess_input = keras.applications.xception.preprocess_input
decode_predictions = keras.applications.xception.decode_predictions

image = np.load(img_path)
img_size = image.shape # 应为形状为 (240, 146) 的数组

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
# 首先，我们创建一个将输入图像映射到最后一个 conv 的激活的模型层以及输出预测
grad_model = keras.models.Model(
model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]
)

# 然后，我们计算输入图像的顶部预测类的梯度
# 相对于最后一个卷积层的激活
with tf.GradientTape() as tape:
last_conv_layer_output, preds = grad_model(img_array)
if pred_index is None:
pred_index = tf.argmax(preds[0])
class_channel = preds[:, pred_index]

# 这是输出神经元的梯度（顶部预测或选择）
# 相对于最后一个卷积层的输出特征图
grads = tape.gradient(class_channel, last_conv_layer_output)

# 这是一个向量，其中每个条目都是梯度的平均强度
# 在特定特征图通道上
pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

# 我们将特征图数组中的每个通道乘以
# 相对于顶部预测类的“此通道的重要性”
# 然后将所有通道相加以获得热图类激活
last_conv_layer_output = last_conv_layer_output[0]
heatmap = last_conv_layer_output @pooled_grads[..., tf.newaxis]
heatmap = tf.squeeze(heatmap)

# 为了可视化目的，我们还将在 0 和 1 之间对热图进行标准化
heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
return heatmap.numpy()

last_conv_layer_name = “max_pooling2d_4”

image = image.reshape(1, 240, 146, 1)
preds = model.predict(image)

model.layers[-1].activation = None

heatmap = make_gradcam_heatmap(image, model, last_conv_layer_name)
plt.matshow(heatmap)
plt.show()

我的数据是维度 (240, 146) 的 numpy 数组，CNN 将其作为输入。]]></description>
      <guid>https://stackoverflow.com/questions/78722413/keras-valueerror-the-layer-sequential-has-never-been-called-and-thus-has-no-def</guid>
      <pubDate>Mon, 08 Jul 2024 18:32:19 GMT</pubDate>
    </item>
    <item>
      <title>深度学习 Nan 损失的原因</title>
      <link>https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons</link>
      <description><![CDATA[什么会导致卷积神经网络发散？
具体信息：
我正在使用 Tensorflow 的 iris_training 模型和一些我自己的数据，但一直出现

错误：tensorflow：模型发散，损失 = NaN。
回溯...
tensorflow.contrib.learn.python.learn.monitors.NanLossDuringTrainingError：训练期间出现 NaN 损失。

回溯源自以下行：
 tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
hidden_​​units=[300, 300, 300],
#optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.001, l1_regularization_strength=0.00001), 
n_classes=11,
model_dir=&quot;/tmp/iris_model&quot;)

我尝试调整优化器，使用零作为学习率，并且不使用优化器。]]></description>
      <guid>https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons</guid>
      <pubDate>Fri, 14 Oct 2016 19:07:18 GMT</pubDate>
    </item>
    </channel>
</rss>