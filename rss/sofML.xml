<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>æ ‡è®°ä¸ºæœºå™¨å­¦ä¹ çš„æ´»è·ƒé—®é¢˜ - å †æ ˆå†…å­˜æº¢å‡º</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>æ¥è‡ª stackoverflow.com çš„æœ€æ–° 30 æ¡</description>
    <lastBuildDate>Thu, 21 Nov 2024 18:23:40 GMT</lastBuildDate>
    <item>
      <title>ä¸ºä»€ä¹ˆæˆ‘çš„ ML æ¨¡å‹åœ¨ä¸åŒçš„è¿è¡Œä¸­è·å¾—ä¸åŒçš„æ€§èƒ½ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/79212471/why-do-i-get-different-performance-on-different-runs-on-my-ml-model</link>
      <description><![CDATA[å› æ­¤ï¼Œæˆ‘æ­£åœ¨ä½¿ç”¨ snowpark è®­ç»ƒ ml æ¨¡å‹ï¼ˆXgboost qnd LightGbmï¼‰ï¼Œä½†æ¯æ¬¡è¿è¡Œåæˆ‘éƒ½ä¼šå¾—åˆ°ä¸åŒçš„æŒ‡æ ‡å€¼ï¼ˆAUCã€å¹³å‡ç²¾åº¦ï¼‰ï¼Œå› æ­¤æ°¸è¿œä¸çŸ¥é“å“ªä¸ªæ˜¯æˆ‘æœ€å¥½çš„æ¨¡å‹ã€‚
æˆ‘å°è¯•åœ¨ç¬”è®°æœ¬çš„å¼€å¤´è®¾ç½®ä¸€ä¸ªå…¨å±€å˜é‡ random_seed = 42ï¼Œå¹¶å°†å…¶æ”¾åœ¨æˆ‘çš„æ¬ é‡‡æ ·å‡½æ•°å’Œæ¨¡å‹çš„åˆå§‹åŒ–ä¸­ï¼š
 if model_type == &#39;xgboost&#39;:
model = XGBClassifier(
random_state=random_seed,
input_cols=feature_cols,
label_cols=target_col,
output_cols=[&#39;PREDICTION&#39;],
passthrough_cols=[&#39;INDIVIDUAL_SK&#39;, &#39;DATE_MONTH&#39;],
**hyperparameters
)

elif model_type == &#39;lightgbm&#39;:
model = LGBMClassifierï¼ˆ
random_state=random_seedï¼Œ
input_cols=feature_colsï¼Œ
label_cols=target_colï¼Œ
output_cols=[&#39;PREDICTION&#39;]ï¼Œ
passthrough_cols=[&#39;INDIVIDUAL_SK&#39;ï¼Œ&#39;DATE_MONTH&#39;]ï¼Œ
**è¶…å‚æ•°

)

def undersample_majority_classï¼ˆdfï¼‰ï¼š

df_with_seniority = df.with_columnï¼ˆâ€œyears_sinceâ€ï¼Œï¼ˆF.colï¼ˆ&#39;TIME_SINCE_FIRST_LEAD&#39;ï¼‰/12ï¼‰ã€‚castï¼ˆ&#39;int&#39;ï¼‰ï¼‰

df_with_random = df_with_seniority.with_columnï¼ˆ&#39;random_order&#39;ï¼ŒF.randomï¼ˆseed=random_seedï¼‰ï¼‰
window_spec = Window.partition_by(&quot;INDIVIDUAL_SK&quot;).order_by(F.col(&#39;random_order&#39;).asc())
df_ranked = df_with_random.with_column(&quot;month_rank&quot;, F.row_number().over(window_spec)
)

df_majority = df_ranked.filter(F.col(&quot;CONVERSION_INDICATOR&quot;) == 0)
df_majority_sampled = df_majority.filter(((F.col(&quot;years_since&quot;) &gt; 10) &amp; (F.col(&quot;month_rank&quot;) == 1)) |
((F.col(&quot;years_since&quot;) &lt;= 10) &amp; (F.col(&quot;month_rank&quot;) &lt;= 2))
)

df_majority_sampled = df_majority_sampled.drop(&#39;years_since&#39;,&#39;month_rank&#39;,&#39;random_order&#39; )
df_minority = df.filter(F.col(&quot;CONVERSION_INDICATOR&quot;) == 1)
df_balanced = df_majority_sampled.union_all(df_minority)

return df_balanced

æˆ‘ä¸çŸ¥é“è¯¥æ€ä¹ˆåšæ‰èƒ½è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

]]></description>
      <guid>https://stackoverflow.com/questions/79212471/why-do-i-get-different-performance-on-different-runs-on-my-ml-model</guid>
      <pubDate>Thu, 21 Nov 2024 18:16:23 GMT</pubDate>
    </item>
    <item>
      <title>ANN æ¨¡å‹çš„å‡†ç¡®æ€§ [å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79212222/accuracy-of-the-ann-model</link>
      <description><![CDATA[æˆ‘æ›¾å°è¯•ä½¿ç”¨å†å²æ•°æ®æ„å»ºä¸€ä¸ª ANN æ¨¡å‹æ¥é¢„æµ‹å¤ªé˜³è¾å°„ã€‚
2016 - 2020 NSRDB æ•°æ®
ä»¥ä¸‹æ˜¯è¯„ä¼°æŒ‡æ ‡
R2 å€¼ .999
MSE .690
MAE .450
æŸå¤± .690
ä»¥ä¸‹æ˜¯æˆ‘çš„é—®é¢˜
ANN æ¨¡å‹çš„å‡†ç¡®ç‡è¾¾åˆ° .999 æ˜¯å¦æ­£å¸¸
æ˜¯å¦è¿‡åº¦æ‹Ÿåˆï¼Ÿ
æˆ‘é™„ä¸Šäº†è®­ç»ƒæŸå¤±ä¸éªŒè¯æŸå¤±å›¾]]></description>
      <guid>https://stackoverflow.com/questions/79212222/accuracy-of-the-ann-model</guid>
      <pubDate>Thu, 21 Nov 2024 16:58:23 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨å‚è€ƒå›¾åƒäº¤æ¢å›¾åƒä¸­çš„è’™ç‰ˆåŒºåŸŸ</title>
      <link>https://stackoverflow.com/questions/79211893/swap-masked-area-in-an-image-using-a-reference-image</link>
      <description><![CDATA[æˆ‘æœ‰ä¸€å¼ å›¾åƒåŠå…¶è’™ç‰ˆã€‚æˆ‘æƒ³ç”¨ç¬¬ä¸‰å¼ å›¾åƒä½œä¸ºå‚è€ƒæ¥æ›¿æ¢è’™ç‰ˆåŒºåŸŸã€‚ä¾‹å¦‚ï¼Œæˆ‘æƒ³ç”¨å‚è€ƒå›¾åƒæ›¿æ¢æˆ¿å±‹å›¾åƒä¸­çš„è’™ç‰ˆåŒºåŸŸï¼ˆé»‘è‰²åŒºåŸŸï¼‰ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œè¾“å‡ºåº”è¯¥çœ‹èµ·æ¥åƒä½¿ç”¨å‚è€ƒå›¾åƒçš„é¢œè‰²å’Œçº¹ç†çš„ç“·ç –å»ºé€ çš„æˆ¿å±‹å±‹é¡¶ã€‚ç»“æœå›¾åƒæ˜¯æˆ‘ä½¿ç”¨ç¨³å®šæ‰©æ•£èƒ½å¤Ÿè·å¾—çš„æœ€ä½³ç»“æœã€‚ä½†æ˜¯æˆ‘éœ€è¦æ›´å¥½çš„ç»“æœã€‚å¦‚æœå¯ä»¥ä½¿ç”¨å…¶ä»–æ–¹æ³•ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚
åŸå§‹æˆ¿å±‹å›¾åƒ
æˆ¿å±‹é¢å…·
å‚è€ƒå›¾åƒ
ç»“æœå›¾åƒ]]></description>
      <guid>https://stackoverflow.com/questions/79211893/swap-masked-area-in-an-image-using-a-reference-image</guid>
      <pubDate>Thu, 21 Nov 2024 15:34:53 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ pytorch æœ€å°åŒ–æ±‰æ˜è·ç¦»</title>
      <link>https://stackoverflow.com/questions/79211677/minimization-of-hamming-distance-with-pytorch</link>
      <description><![CDATA[æˆ‘æœ‰ä¸€ä¸ª mxn çŸ©é˜µï¼Œå…¶ä¸­ m&gt;n å’Œä¸€ä¸ªå‘é‡ bã€‚å®ƒä»¬ä»…ç”± 1 å’Œ 0 ç»„æˆã€‚æ­¤å¤–ï¼Œæ‰€æœ‰åœ°æ–¹çš„å’Œéƒ½æ˜¯ä»¥ 2 ä¸ºæ¨¡çš„ã€‚é€šè¿‡é«˜æ–¯æ¶ˆå…ƒæ³•ï¼Œæˆ‘å¯ä»¥å¾—åˆ° Ax=b çš„è§£ x_pã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å« 1 å’Œ 0 çš„å‘é‡ã€‚è¿™ä¸æ˜¯å”¯ä¸€çš„è§£å†³æ–¹æ¡ˆã€‚å­˜åœ¨å…¶ä»–åŒ…å«è¾ƒå°‘ 1 çš„è§£ã€‚æˆ‘æƒ³æ‰¾åˆ°åŒ…å«æœ€å°‘ 1 çš„è§£ï¼ˆæ±‰æ˜è·ç¦»ï¼‰ï¼Œå³æˆ‘æƒ³æœ€å°åŒ–æ±‰æ˜è·ç¦»ã€‚
ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘æƒ³åˆ°äº†ä½¿ç”¨ pytorch çš„æ¢¯åº¦ä¸‹é™æ³•ã€‚è¿™æ˜¯æˆ‘çš„ä»£ç ï¼š
def optimal_solution(self, max_iter=100, lr=0.0054, lambda_param=10):

matrix = self.relative_boundary()
# åœ¨ [0,1] ä¸­åˆå§‹åŒ– x å¹¶è½¬æ¢ä¸º pytorch å¼ é‡
x = self.solve()

matrix = torch.tensor(matrix, dtype=torch.float32)
b = torch.tensor(self.boundary_vector, dtype=torch.float32)
x = torch.tensor(x, dtype=torch.float32, require_grad=True)

# Adam æ˜¯æ¢¯åº¦ä¸‹é™çš„é«˜çº§ç‰ˆæœ¬ï¼Œå¯åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­è°ƒæ•´å­¦ä¹ ç‡ã€‚
optimizer = torch.optim.Adam([x], lr=lr)

# è·Ÿè¸ª x å˜åŒ–çš„æ ‡å‡†
prev_x = x.clone()

# å¾ªç¯æ‰§è¡Œæ¢¯åº¦ä¸‹é™ï¼Œæœ€å¤šè¿­ä»£ max_iter æ¬¡ï¼š
for _ in range(max_iter):
optimizer.zero_grad()

# å®šä¹‰æŸå¤±å‡½æ•°ï¼šç¨€ç–æ€§ + çº¦æŸæ»¡è¶³
loss = torch.sum(x) + lambda_param * torch.sum((matrix @ x - b) ** 2)

# è·Ÿè¸ª x çš„å˜åŒ–é‡
change_in_x = torch.norm(x - prev_x).item()

# è®¡ç®—ç›¸å¯¹äº x çš„æŸå¤±æ¢¯åº¦
loss.backward()
# æ ¹æ®æ¢¯åº¦æ›´æ–° x çš„å€¼
optimizer.step()

# åº”ç”¨è½¯æŠ•å½±ï¼ˆä¾‹å¦‚ S å‹ï¼‰ä½¿ x ä¿æŒåœ¨ [0, 1] ä¸­
x.data = torch.clamp(x.data, 0, 1)

# å¯é€‰åœ°æ‰“å°æŸå¤±ä»¥è¿›è¡Œè°ƒè¯•
print(f&quot;è¿­ä»£ {_} æ—¶çš„æŸå¤±ï¼š{loss.item()}, x çš„å˜åŒ–ï¼š{change_in_x}&quot;)

# ä»è®¡ç®—å›¾ä¸­åˆ†ç¦» PyTorch å¼ é‡å¹¶å°†å…¶è½¬æ¢ä¸º NumPy æ•°ç»„ï¼›
# åº”ç”¨ 0.5 çš„é˜ˆå€¼å°†å€¼è½¬æ¢ä¸º 0 æˆ– 1ã€‚
# ç„¶åå‡½æ•°è¿”å›äºŒè¿›åˆ¶è§£å†³æ–¹æ¡ˆå‘é‡ã€‚
x_binary = (x.detach().numpy() &gt; 0.5).astype(int)

return x_binary

ä½†æ˜¯ï¼Œé€šè¿‡å°è¯•ä¸åŒçš„ lambda_par å’Œå­¦ä¹ ç‡ï¼Œæˆ‘è¦ä¹ˆå¾—åˆ°é›¶å‘é‡ï¼Œè¦ä¹ˆå®ƒåªè¾“å‡ºä¸€ä¸ªå…·æœ‰è¾ƒå°‘ 1 çš„å‘é‡ï¼Œä½†ä¸æ»¡è¶³æ–¹ç¨‹ Ax=bã€‚æ‚¨å¯¹æœ‰æ•ˆçš„ä»£ç æœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/79211677/minimization-of-hamming-distance-with-pytorch</guid>
      <pubDate>Thu, 21 Nov 2024 14:43:15 GMT</pubDate>
    </item>
    <item>
      <title>VAE æŸå¤±å‡å°‘ï¼Œä½†é‡å»ºæ•ˆæœå¹¶æœªæ”¹å–„</title>
      <link>https://stackoverflow.com/questions/79211354/vae-loss-decreases-but-reconstruction-doesnt-improve</link>
      <description><![CDATA[æˆ‘é‡åˆ°äº†é‡å»ºå›¾åƒæ ¹æœ¬ä¸èµ·ä½œç”¨çš„é—®é¢˜ã€‚ä¸‹é¢æ˜¯æˆ‘æ‰§è¡Œå•ä¸ªæ›´æ–°æ­¥éª¤çš„æ–¹æ³•ã€‚æˆ‘çŸ¥é“å¸¸è§„ VAE å¯ä»¥å®ç°æ›´ç®€å•çš„å®ç°ï¼Œä½†ç”±äºå…¶ä»–é™åˆ¶ï¼Œæˆ‘éœ€è¦ä»¥è¿™ç§æ–¹å¼å®ç°å®ƒã€‚å› æ­¤ï¼Œæˆ‘æƒ³çŸ¥é“æ­¤å®ç°ä¸­æ˜¯å¦å­˜åœ¨ä»»ä½•é”™è¯¯ï¼Œè€Œä¸æ˜¯è¦æ±‚æ›´é«˜æ•ˆçš„å®ç°ã€‚zeros_like_batchstats å‡½æ•°è¿”å›ä¸€ä¸ªä¸è¾“å…¥å…·æœ‰ç›¸åŒç»“æ„ä½†æ‰€æœ‰å€¼éƒ½è®¾ç½®ä¸ºé›¶çš„å¯¹è±¡ã€‚
@jax.jit
def train_step(
rng: jax.random.PRNGKey,
state_enc: TrainState,
state_dec: TrainState,
imgs: jax.Array,
) -&gt; Tuple[TrainState, TrainState, Dict]:

((mean, logvar), enc_mutated_vars), vjp_fn_enc = jax.vjp(
lambda params: state_enc.apply_fn(
{&quot;params&quot;: params, &quot;batch_stats&quot;: state_enc.batch_stats},
imgs, train=True, mutable=[&quot;batch_stats&quot;]
),
state_enc.params,
)
z, vjp_fn_latents = jax.vjp(
lambda mean, logvar: sample_z(rng, mean, logvar),
mean, logvar
)

def recon_loss_fn(dec_params, latent_features):
# ä»è§£ç å™¨è·å–é‡å»ºå›¾åƒ
recon, mutated_vars = state_dec.apply_fn(
{&#39;params&#39;: dec_params, &#39;batch_stats&#39;: state_dec.batch_stats},
latent_features, train=True, mutable=[&#39;batch_stats&#39;]
)
recon_loss = jnp.mean(jnp.sum(binary_cross_entropy_fn(recon, imgs), axis=(1, 2, 3), keepdims=True))
è¿”å› recon_loss, mutated_vars

recon_loss_grads_fn = jax.value_and_grad(recon_loss_fn, argnums=(0, 1), has_aux=True)
(recon_loss, dec_mutated_vars), (grads_dec, graz_z) = recon_loss_grads_fn(state_dec.params, z)
    grads_enc_recon=vjp_fn_enc((vjp_fn_latents(graz_z), {â€œbatch_statsâ€: Zeros_like_batchstats(state_enc.batch_stats)}))[0]

    # è®¡ç®—kld_loss
    def kld_loss_fn(å¹³å‡å€¼, logvar):
        kld_loss = jnp.mean(jnp.sum(-0.5 * (1 + logvar - å¹³å‡å€¼ ** 2 - jnp.exp(logvar)), axis=1))
        è¿”å› kld_loss

    kld_loss_grads_fn = jax.value_and_grad(kld_loss_fn, argnums=(0, 1))
    kld_loss, grads_mean_and_logvar = kld_loss_grads_fn(å¹³å‡å€¼, logvar)
    grads_enc_kld = vjp_fn_enc((grads_mean_and_logvar, {&quot;batch_stats&quot;: zeros_like_batchstats(state_enc.batch_stats)}))[0]

# è®¡ç®—ç¼–ç å™¨çš„æ¢¯åº¦
grads_enc = jax.tree_util.tree_map(lambda x, y: x + y, grads_enc_recon, grads_enc_kld)

# å­˜å‚¨æ¢¯åº¦å’Œæ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯\
state_enc = state_enc.apply_gradients(grads=grads_enc, batch_stats=enc_mutated_vars[&quot;batch_stats&quot;])
state_dec = state_dec.apply_gradients(grads=grads_dec, batch_stats=dec_mutated_vars[&quot;batch_stats&quot;])

metrics = {
&quot;train/recon_loss&quot;: recon_loss,
&quot;train/kld_loss&quot;: kld_loss,
}
return state_enc, state_dec, metrics

æˆ‘å·²ç¡®è®¤å½¢çŠ¶æ²¡æœ‰é—®é¢˜ï¼Œå¹¶ä¸”æŸå¤±ä¹Ÿåœ¨å‡å°‘ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/79211354/vae-loss-decreases-but-reconstruction-doesnt-improve</guid>
      <pubDate>Thu, 21 Nov 2024 13:23:11 GMT</pubDate>
    </item>
    <item>
      <title>Unsloth ç¾Šé©¼æç¤ºæ¨¡æ¿ï¼š[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79210129/unsloth-alpaca-prompt-template</link>
      <description><![CDATA[æˆ‘æœ‰ä¸€ä¸ªä¸æ€¥æ•‘è¯´æ˜ç›¸å…³çš„æ•°æ®é›†ï¼Œå®ƒæœ‰ä¸¤åˆ—ï¼Œä¸€åˆ—æ˜¯é—®é¢˜ï¼Œå¦ä¸€åˆ—æ˜¯ç­”æ¡ˆï¼Œç°åœ¨æˆ‘çš„é—®é¢˜æ˜¯å¦‚ä½•ä¿®æ”¹ unsloth ç¬”è®°æœ¬ä¸­æä¾›çš„ç¾Šé©¼æç¤ºæ¨¡æ¿ä»¥é€‚åº”æˆ‘çš„ç”¨ä¾‹ã€‚æˆ‘å°è¯•äº†å‡ ä¸ªæ¨¡æ¿ï¼Œä½†å¯¹æˆ‘æ¥è¯´ä¸èµ·ä½œç”¨ã€‚
æˆ‘å°è¯•äº†å‡ ä¸ªæ¨¡æ¿å’Œ llama 3.1 èŠå¤©æ¨¡æ¿ï¼Œä½†ä¸èµ·ä½œç”¨ã€‚
æœ‰äººèƒ½ä¸ºè¿™äº›ä»»åŠ¡æ¨èä¸€ä¸ªæ¨¡å‹æ¥å¾®è°ƒè¿™ä¸ªæ•°æ®é›†å—ï¼Ÿ
llama æ¨¡å‹æ˜¯å¦é€‚ç”¨äºè¿™ç§ç±»å‹çš„æ•°æ®é›†ã€‚æ•°æ®é›†åŒ…å«ç›´æ¥çš„é—®é¢˜å’Œç­”æ¡ˆã€‚
https://huggingface.co/datasets/lextale/FirstAidInstructionsDataset
è¿™æ˜¯æ•°æ®é›†çš„é“¾æ¥ï¼Œå¦‚æœæœ‰äººä½¿ç”¨è¿‡æ­¤ç±»æ•°æ®é›†ï¼Œè¯·æŒ‡å¯¼æˆ‘å¦‚ä½•é’ˆå¯¹è¿™äº›ç±»å‹çš„æ•°æ®å¾®è°ƒæ¨¡å‹æˆ–é’ˆå¯¹æ­¤ç±»æ•°æ®çš„ä»»ä½•ç‰¹å®šæ¨¡å‹ã€‚
æˆ‘å°†éå¸¸æ„Ÿè°¢æœ‰å…³è¿™äº›é—®é¢˜çš„ä»»ä½•å¸®åŠ©ï¼Œæˆ‘æ­£åœ¨ä¸ºæˆ‘çš„ FYP åšè¿™ä»¶äº‹ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/79210129/unsloth-alpaca-prompt-template</guid>
      <pubDate>Thu, 21 Nov 2024 07:40:57 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ä»€ä¹ˆDLé¢„è®­ç»ƒæ¨¡å‹æˆ–æ–¹æ³•æ¥è®­ç»ƒä»¥ä¸‹å®ç°[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79209830/what-dl-pretrained-model-or-method-to-use-for-training-the-following-implementat</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å¼€å±•ä¸€ä¸ªç‰©è”ç½‘é¡¹ç›®ï¼Œè¯¥é¡¹ç›®å°†å¯¹æŠ«è¨åœ¨æ”¾å…¥çƒ¤ç®±æ—¶æ˜¯åœ¨å·¦ä¾§è¿˜æ˜¯å³ä¾§è¿›è¡Œåˆ†ç±»ã€‚å®ƒè¿˜åº”è¯´æ˜æŠ«è¨æ˜¯è¿›çƒ¤ç®±è¿˜æ˜¯å‡ºçƒ¤ç®±ã€‚ç°åœ¨æˆ‘æœ‰æ¥è‡ªé™æ€æ‘„åƒå¤´çš„è§†é¢‘æºã€‚ä¸‹é¢æ˜¯æŠ«è¨åœ¨å·¦ä¾§æˆ–å³ä¾§çš„å¸§å›¾åƒï¼Œä¾‹å¦‚ï¼šæŠ«è¨æ­£åœ¨ç§»å…¥çƒ¤ç®±çš„å·¦ä¾§å’ŒæŠ«è¨æ­£åœ¨ç§»åˆ°çƒ¤ç®±çš„å³ä¾§ã€‚
æˆ‘å°è¯•è¿‡ä½¿ç”¨ YOLO æ¨¡å‹ï¼Œä½†å®ƒä¸»è¦ç”¨äºç‰©ä½“æ£€æµ‹ã€‚å¦‚ä½•è¯†åˆ«æŠ«è¨çš„ç›¸å¯¹ä½ç½®å’Œç›¸å¯¹è¿åŠ¨ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/79209830/what-dl-pretrained-model-or-method-to-use-for-training-the-following-implementat</guid>
      <pubDate>Thu, 21 Nov 2024 05:47:51 GMT</pubDate>
    </item>
    <item>
      <title>å…³äº Talebi è®ºæ–‡ä¸­ç©ºé—´å†³ç­–æ ‘åˆ†è£‚é€»è¾‘çš„æ¾„æ¸… [å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79209559/clarification-on-splitting-logic-in-spatial-decision-trees-on-talebi-paper</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ç ”ç©¶ Talebi ç­‰äººçš„è®ºæ–‡â€œç”¨äºåœ°çƒç§‘å­¦æ•°æ®åˆ†æå’Œå»ºæ¨¡çš„çœŸæ­£ç©ºé—´éšæœºæ£®æ—ç®—æ³•â€ï¼Œæˆ‘å¯¹å›¾ 2 æ‰€ç¤ºçš„ç©ºé—´å†³ç­–æ ‘è¿‡ç¨‹æœ‰ä¸€äº›ç–‘é—®ã€‚
æ··åˆæ—‹è½¬å’Œç¼©æ”¾ï¼š
æˆ‘çš„ç†è§£æ˜¯ï¼Œå¯¹äºæ¯ä¸ªå•å…ƒæ ¼ï¼Œå¤šä¸ªå°ºåº¦å’Œæ—‹è½¬çš„ç©ºé—´æ¨¡å¼è¢«çŸ¢é‡åŒ–å¹¶è¿æ¥æˆå•ä¸ªè¾“å…¥å‘é‡ã€‚ç„¶ååœ¨æ ‘åˆ†å‰²è¿‡ç¨‹ä¸­å°†æ­¤è¾“å…¥ç”¨ä½œé¢„æµ‹å™¨ã€‚è¿™æ˜¯æ­£ç¡®çš„å—ï¼Ÿ
æ­¤å¤–ï¼Œæ¨¡å‹å¦‚ä½•ç¡®ä¿æ¥è‡ªä¸åŒæ—‹è½¬å’Œå°ºåº¦çš„æ¨¡å¼åœ¨æ··åˆæˆä¸€ä¸ªå‘é‡æ—¶ä¿ç•™å…¶ç©ºé—´ä¸Šä¸‹æ–‡ï¼Ÿ
åˆ†å‰²ä¸­ç°è‰²åŒºåŸŸçš„ç§»åŠ¨ï¼š
åœ¨å›¾ 2 ä¸­ï¼Œæˆ‘æ³¨æ„åˆ°ç°è‰²å•å…ƒæ ¼ä¼¼ä¹ä»£è¡¨æ•°æ®çš„ä¸€ä¸ªå­é›†ï¼Œåœ¨åˆ†å‰²è¿‡ç¨‹ä¸­ä»ä¸­é—´ç§»åŠ¨åˆ°è§’è½ã€‚

è¿™æ˜¯å¦è¡¨æ˜éšç€æ ‘åˆ†å‰²æˆæ›´å°çš„åŒºåŸŸï¼Œé¢„æµ‹ç©ºé—´ä¼šç¼©å°ï¼Ÿ
è¿™ç§ç§»åŠ¨æ˜¯å°†æ¨¡å¼è¿‡æ»¤æˆæ›´å‡åŒ€çš„å­é›†çš„ç»“æœï¼Œè¿˜æ˜¯ä»£è¡¨äº†æ•°æ®ä¸­ç©ºé—´ä¾èµ–æ€§çš„ç‰¹å®šå†…å®¹ï¼Ÿ

ç¬¬äºŒæ¬¡åˆ†å‰²ä¸­çš„ä¸åŒç°è‰²åŒºåŸŸï¼š

ä¸ºä»€ä¹ˆç¬¬äºŒæ¬¡åˆ†å‰²çš„å·¦åˆ†æ”¯ä¸­çš„ç°è‰²åŒºåŸŸä¿ç•™åœ¨ç¬¬ä¸€ä¸ª-ğ‘…ä¸­ï¼Œè€Œåœ¨å³åˆ†æ”¯ä¸­ï¼Œå®ƒè½¬ç§»åˆ°ç¬¬äºŒä¸ª-ğ‘…ï¼Ÿ
è¿™æ˜¯å¦è¡¨æ˜é€‰æ‹©ä¸åŒçš„é¢„æµ‹å› å­æ¥åˆ†å‰²å·¦åˆ†æ”¯å’Œå³åˆ†æ”¯ï¼Ÿ
å¦‚æœæ˜¯è¿™æ ·ï¼Œè¿™æ˜¯å¦çªå‡ºäº†æ•°æ®é›†ä¸­çš„ç©ºé—´å¼‚è´¨æ€§ï¼Ÿ

ä»»ä½•å…³äºè¿™åœ¨ç©ºé—´å†³ç­–æ ‘ä¸­å¦‚ä½•å·¥ä½œçš„è¯´æ˜æˆ–ç¤ºä¾‹éƒ½å°†éå¸¸æœ‰å¸®åŠ©ã€‚
æˆ‘å°è¯•äº†ä»€ä¹ˆï¼š
æˆ‘å›é¡¾äº†å›¾ 2 çš„æè¿°å’Œè®ºæ–‡â€œç”¨äºåœ°çƒç§‘å­¦æ•°æ®åˆ†æå’Œå»ºæ¨¡çš„çœŸæ­£ç©ºé—´éšæœºæ£®æ—ç®—æ³•â€ä¸­çš„æ–¹æ³•ã€‚æˆ‘è¯•å›¾äº†è§£ç°è‰²åŒºåŸŸå¦‚ä½•å¯¹åº”äºç©ºé—´æ¨¡å¼å’Œç”¨äºåˆ†å‰²çš„é¢„æµ‹å› å­ã€‚
æˆ‘é¢„æœŸä¼šå‘ç”Ÿä»€ä¹ˆï¼š
æˆ‘é¢„æœŸç°è‰²åŒºåŸŸä»£è¡¨é¢„æµ‹å› å­ç©ºé—´çš„ä¸€è‡´åˆ’åˆ†ï¼Œå…¶ä¸­æ¯ä¸ªåˆ†å‰²å¯¹åº”äºæ‰€æœ‰åˆ†æ”¯ä¸­çš„ç›¸åŒé¢„æµ‹å› å­æˆ–ç©ºé—´æ¨¡å¼å­é›†ã€‚
å®é™…å‘ç”Ÿäº†ä»€ä¹ˆï¼š
æˆ‘è§‚å¯Ÿåˆ°åœ¨ç¬¬äºŒæ¬¡åˆ†å‰²ä¸­ï¼Œå·¦åˆ†æ”¯å’Œå³åˆ†æ”¯ä¹‹é—´çš„ç°è‰²åŒºåŸŸä¸åŒã€‚åœ¨å·¦ä¾§ï¼Œç°è‰²åŒºåŸŸå¯¹åº”äºç¬¬ä¸€ä¸ªé¢„æµ‹å› å­ï¼Œè€Œåœ¨å³ä¾§ï¼Œå®ƒè½¬ç§»åˆ°ç¬¬äºŒä¸ªé¢„æµ‹å› å­ã€‚æˆ‘ä¸ç¡®å®šè¿™ç§å·®å¼‚æ˜¯ç”±äºç©ºé—´å¼‚è´¨æ€§ã€ç‹¬ç«‹é¢„æµ‹å› å­é€‰æ‹©è¿˜æ˜¯å…¶ä»–åŸå› é€ æˆçš„ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/79209559/clarification-on-splitting-logic-in-spatial-decision-trees-on-talebi-paper</guid>
      <pubDate>Thu, 21 Nov 2024 03:11:43 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•åœ¨å¤„ç† EOS ä»£å¸æ—¶è®¡ç®—æ‹¥æŠ±äººè„¸æ¨¡å‹çš„æ•™å¸ˆå¼ºåˆ¶å‡†ç¡®åº¦ (TFA)ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl</guid>
      <pubDate>Thu, 21 Nov 2024 00:25:48 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•æ„å»ºæ›´é«˜æ•ˆçš„ DataLoader æ¥åŠ è½½å¤§å‹å›¾åƒæ•°æ®é›†ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/79208825/how-to-build-a-more-efficient-dataloader-to-load-large-image-datasets</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°è¯•åœ¨ä¸€ä¸ªéå¸¸å¤§çš„å›¾åƒæ•°æ®é›†ä¸Šè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æ¨¡å‹è¾“å…¥éœ€è¦ä¸€å¯¹å›¾åƒï¼ˆA å’Œ Bï¼‰ã€‚ç”±äºæˆ‘çš„å›¾åƒå°ºå¯¸éå¸¸å¤§ï¼Œæˆ‘å·²å°†æ¯ä¸ªå›¾åƒè°ƒæ•´ä¸ºå½¢çŠ¶ä¸º (3x224x224) çš„ torch.Tensorï¼Œå¹¶å°†æ¯å¯¹å›¾åƒä½œä¸ºå•ç‹¬çš„æ–‡ä»¶å­˜å‚¨åœ¨æˆ‘çš„ç£ç›˜ä¸Šã€‚ç›¸åŒçš„å¯¹å…±äº«ç›¸åŒçš„ç´¢å¼•ã€‚
ä½†æ˜¯ï¼Œå½“ä½¿ç”¨æ•°æ®é›†å’Œ DataLoader å°†è¿™äº›æ–‡ä»¶åŠ è½½â€‹â€‹åˆ°å†…å­˜ä¸­æ—¶ï¼Œæˆ‘é‡åˆ°äº†ä»¥ä¸‹é—®é¢˜ï¼š

CPU å†…å­˜é—®é¢˜ï¼šå°†å·¥ä½œå™¨æ•°é‡è®¾ç½®ä¸º 12 æ—¶ï¼Œ200GB å†…å­˜å¾ˆå¿«å°±ä¼šè€—å°½ã€‚æˆ‘å°è¯•è®¾ç½® prefetch_factor=1ï¼Œä½†æ²¡æœ‰å¸®åŠ©ã€‚
åˆå§‹åŒ–ç¼“æ…¢ï¼šåœ¨è®­ç»ƒå¼€å§‹ä¹‹å‰ï¼Œæ¯ä¸ª epoch ä¹‹å‰éƒ½éœ€è¦å¾ˆé•¿æ—¶é—´è¿›è¡Œåˆå§‹åŒ–ã€‚æˆ‘åœ¨ä¹‹å‰çš„å¸–å­ä¸­çœ‹åˆ°ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºåˆå§‹åŒ–çš„å¼€é”€é€ æˆçš„ã€‚æˆ‘è®¾ç½®äº† persistent_workers=Trueï¼Œä½†ä¹Ÿæ²¡æœ‰å¸®åŠ©ã€‚
GPU å’Œæ‰¹æ¬¡å¤§å°ï¼šæˆ‘ä½¿ç”¨ 4 ä¸ª GPU è¿›è¡Œ DDP è®­ç»ƒï¼Œå½“å‰æ‰¹æ¬¡å¤§å°ä¸º 1024ã€‚

æœ‰æ²¡æœ‰å…³äºå¦‚ä½•æé«˜æ•°æ®é›†æˆ– DataLoader æ•ˆç‡çš„å»ºè®®ï¼Ÿ

normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], 
std=[0.229, 0.224, 0.225])

augmentation = transforms.Compose([
transforms.RandomApply([transforms.ColorJitter(0.2, 0.2, 0.2, 0.05)], p=0.8),
transforms.RandomApply([transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))], p=0.5),
transforms.RandomGrayscale(p=0.1),
transforms.RandomVerticalFlip(p=0.5),
normalize,
])

class ImagePairDataset(Dataset):
def __init__(self, data_save_folder, dataset_name, num_samples, transform=None):
&quot;&quot;&quot;
Args:
data_save_folder (str)ï¼šåŒ…å«æ•°æ®æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
dataset_name (str)ï¼šè¡¨ç¤ºæ•°æ®é›†æ‹†åˆ†çš„â€œtrainâ€ã€â€œvalâ€æˆ–â€œtestâ€ä¹‹ä¸€ã€‚
num_samples (int)ï¼šæ•°æ®é›†æ‹†åˆ†ä¸­çš„æ ·æœ¬æ•°ã€‚ (train: 3000000, val: 10000, test: 10000)
transform (å¯è°ƒç”¨ï¼Œå¯é€‰)ï¼šåº”ç”¨äºå›¾åƒå¼ é‡çš„å¯é€‰å˜æ¢ã€‚
&quot;&quot;&quot;
self.data_save_folder = data_save_folder
self.dataset_name = dataset_name
self.num_samples = num_samples
self.transform = transform

def __len__(self):
return self.num_samples

def __getitem__(self, idx):

# æ ¹æ® idx æ„å»ºæ–‡ä»¶è·¯å¾„
A_image_path = f&quot;{self.data_save_folder}/{self.dataset_name}_A_images_{idx}.pt&quot;
B_image_path = f&quot;{self.data_save_folder}/{self.dataset_name}_B_images_{idx}.pt&quot;
label_path = f&quot;{self.data_save_folder}/{self.dataset_name}_labels_{idx}.pt&quot;

# ä»æ–‡ä»¶è·¯å¾„åŠ è½½å¼ é‡
A_image = torch.load(A_image_path)
B_image = torch.load(B_image_path)
label = torch.load(label_path)

# å¦‚æœå¯ç”¨ï¼Œåˆ™åº”ç”¨è½¬æ¢
if self.transform:
A_image = self.transform(A_image)
B_image = self.transform(B_image)

return A_image, B_image, label

class ImagePairDataModule(pl.LightningDataModule):

def __init__(self, data_save_folder, train_samples, val_samples, test_samples, batch_size=32, num_workers=4):
super().__init__()
self.data_save_folder = data_save_folder
self.train_samples = train_samples
self.val_samples = val_samples
self.test_samples = test_samples
self.batch_size = batch_size
self.num_workers = num_workers
self.train_transform = augmentation
self.eval_transform = normalize # ä»…å¯¹éªŒè¯å’Œæµ‹è¯•è¿›è¡Œæ ‡å‡†åŒ–

def setup(self, stage=None):

self.train_dataset = ImagePairDataset(self.data_save_folder, &#39;train&#39;, self.train_samples, transform=self.train_transform)
self.val_dataset = ImagePairDataset(self.data_save_folder, &#39;val&#39;, self.val_samples, transform=self.eval_transform)
self.test_dataset = ImagePairDataset(self.data_save_folder, &#39;test&#39;, self.test_samples, transform=self.eval_transform)

def train_dataloader(self): #prefetch_factor=1, , persistent_workers=True
return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers) 

def val_dataloader(self):
return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)

# åˆå§‹åŒ– DataModule
data_module = ImagePairDataModule(
data_save_folder=args.data_save_folder,
train_samples=train_samples,
val_samples=val_samples,
test_samples=test_samples,
batch_size=args.batch_size,
num_workers=12,
)
]]></description>
      <guid>https://stackoverflow.com/questions/79208825/how-to-build-a-more-efficient-dataloader-to-load-large-image-datasets</guid>
      <pubDate>Wed, 20 Nov 2024 20:12:35 GMT</pubDate>
    </item>
    <item>
      <title>ä¸ºä»€ä¹ˆ gamma=0 çš„äºŒå…ƒç„¦ç‚¹äº¤å‰ç†µæ€»æ˜¯ä¼šäº§ç”Ÿ nan æŸå¤±ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/79207979/why-does-binary-focal-cross-entropy-with-gamma-0-always-make-nan-loss</link>
      <description><![CDATA[æˆ‘æ­£åœ¨è®­ç»ƒä¸€ä¸ª U-Net æ¥å¯¹æˆ‘ä»¬çš„å®éªŒå›¾åƒè¿›è¡ŒäºŒå€¼åŒ–ã€‚ä½†å‰æ™¯é€šå¸¸æ²¡æœ‰å¾—åˆ°å¾ˆå¥½çš„ä½“ç°ï¼Œæ¢å¥è¯è¯´ï¼Œæˆ‘æœ‰ç±»åˆ«ä¸å¹³è¡¡ï¼Œç½‘ç»œå­¦ä¹ å¾—ä¸å¥½ã€‚æˆ‘ä¸€ç›´åœ¨ä½¿ç”¨ BinaryCrossEntropy ä½œä¸ºæŸå¤±å‡½æ•°ã€‚æ‰€ä»¥ï¼Œæˆ‘æ˜ç™½è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ä¸ªç®€å•æ–¹æ³•æ˜¯å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰çš„æŸå¤±å‡½æ•°ï¼Œä¸ºæ¯ä¸ªç±»åˆ«èµ‹äºˆæƒé‡ã€‚ä½†æˆ‘åœ¨è¿™æ ·åšæ—¶é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œæ‰€ä»¥æ”¾å¼ƒäº†è¿™ä¸ªå°è¯•ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œä½¿ç”¨ BinaryFocalCrossEntropy ä¼¼ä¹æ›´ç®€å•ï¼Œå®ƒçš„è¡¨è¾¾å¼ä¸ºï¼ˆå¦‚æœæˆ‘ç†è§£å¾—å¥½çš„è¯ï¼‰

æ‰€ä»¥ï¼Œæˆ‘çš„è®¡åˆ’æ˜¯ä½¿ç”¨ gamma=0ï¼Œè¿™æ ·æˆ‘å°±å¯ä»¥é€šè¿‡è°ƒæ•´ alpha å€¼æ¥ç»™å‡ºç±»åˆ«æƒé‡ã€‚ä½†æ˜¯ï¼Œæˆ‘ä¸æ–­å¾—åˆ° nan æŸå¤±ã€‚å®ƒå‘ç”Ÿåœ¨å‡ ä¸ªæ‰¹æ¬¡ä¹‹åçš„ç¬¬ä¸€ä¸ªæ—¶æœŸå†…ï¼šï¼ˆè¿™é‡Œæˆ‘ä½¿ç”¨ \alpha = 0.75ï¼‰

åœ¨è¿™é‡Œæˆ‘ä½¿ç”¨äº† adam ä¼˜åŒ–å™¨å’Œ Learning_rate 1e-3ã€‚æˆ‘æ³¨æ„åˆ°ï¼Œå¦‚æœæˆ‘æ”¹ç”¨ 1e-4ï¼Œå³ä½¿ nan ä»ç„¶å‡ºç°ï¼Œå®ƒä¹Ÿä¼šå†å‡ºç°å‡ ä¸ªæ‰¹æ¬¡ã€‚ä½ èƒ½å¸®æˆ‘æ‰¾å‡ºè¿™æ˜¯æ€ä¹ˆå›äº‹ï¼Œæˆ‘è¯¥å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜å—ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/79207979/why-does-binary-focal-cross-entropy-with-gamma-0-always-make-nan-loss</guid>
      <pubDate>Wed, 20 Nov 2024 15:49:20 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ä¸åŒçš„æŸå¤±æ¥è®­ç»ƒä¸åŒé˜¶æ®µçš„æ¨¡å‹</title>
      <link>https://stackoverflow.com/questions/79205991/training-different-stage-of-model-with-different-loss</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°è¯•ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è®­ç»ƒä¸€ä¸ªä¸¤é˜¶æ®µæ¨¡å‹ã€‚ä½†æ˜¯ï¼Œæˆ‘æƒ³ç”¨ä¸åŒçš„æŸå¤±æ›´æ–°æ¨¡å‹çš„ä¸åŒé˜¶æ®µã€‚ä¾‹å¦‚ï¼Œå‡è®¾ç«¯åˆ°ç«¯æ¨¡å‹ç”±ä¸¤ä¸ªæ¨¡å‹ç»„æˆï¼šmodel1 å’Œ model2ã€‚è¾“å‡ºæ˜¯é€šè¿‡è¿è¡Œè®¡ç®—çš„
features = model1(inputs)
output = model2(features)

æˆ‘æƒ³ç”¨ loss1 æ›´æ–° model1 çš„å‚æ•°ï¼ŒåŒæ—¶ä¿æŒ model2 çš„å‚æ•°ä¸å˜ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘æƒ³ç”¨ loss2 æ›´æ–° model2 çš„å‚æ•°ï¼ŒåŒæ—¶ä¿æŒ model1 çš„å‚æ•°ä¸å˜ã€‚æˆ‘çš„å®Œæ•´å®ç°å¦‚ä¸‹ï¼š
import torch
import torch.nn as nn

# å®šä¹‰ç¬¬ä¸€ä¸ªæ¨¡å‹
class Net(nn.Module):
def __init__(self):
super(Net, self).__init__()
self.conv1 = nn.Linear(20, 10)
self.conv2 = nn.Linear(10, 5)

def forward(self, x):
x = self.conv1(x)
x = self.conv2(x)
return x

# å®šä¹‰ç¬¬äºŒä¸ªæ¨¡å‹
class Net1(nn.Module):
def __init__(self):
super(Net1, self).__init__()
self.conv1 = nn.Linear(5, 1)

def forward(self, x):
x = self.conv1(x)
return x

# åˆå§‹åŒ–æ¨¡å‹
model1 = Net()
model2 = Net1()

# åˆå§‹åŒ–å•ç‹¬çš„æ¯ä¸ªæ¨¡å‹çš„ä¼˜åŒ–å™¨
optimizer = torch.optim.SGD(model1.parameters(), lr=0.1)
optimizer1 = torch.optim.SGD(model2.parameters(), lr=0.1)

optimizer.zero_grad() 
optimizer1.zero_grad()

criterion = nn.CrossEntropyLoss()

# æ ·æœ¬è¾“å…¥å’Œæ ‡ç­¾
inputs = torch.randn(2, 20)
labels = torch.randn(2,1)

features = model1(inputs) 
outputs_model = model2(features) 

loss1 = criterion(outputs_model[0], labels[0]) 
loss2 = criterion(outputs_model, labels) 

loss1.backward(retain_graph=True) 
optimizer.step() 
optimizer.zero_grad()
optimizer1.zero_grad() 

loss2.backward() 

ä½†æ˜¯ï¼Œè¿™å°†è¿”å›
å›æº¯ï¼ˆæœ€è¿‘ä¸€æ¬¡è°ƒç”¨æœ€åä¸€æ¬¡ï¼‰ï¼š
æ–‡ä»¶ï¼Œç¬¬ 55 è¡Œï¼Œåœ¨ &lt;module&gt;
loss2.backward() 
^^^^^^^^^^^^^^^^^
æ–‡ä»¶ &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_tensor.py&quot;, ç¬¬ 521 è¡Œ, åœ¨åå‘ä¼ æ’­ä¸­
torch.autograd.backward(
æ–‡ä»¶ &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py&quot;, ç¬¬ 289 è¡Œ, åœ¨åå‘ä¼ æ’­ä¸­
_engine_run_backward(
æ–‡ä»¶ &quot;/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py&quot;, ç¬¬ 769 è¡Œ, åœ¨ _engine_run_backward ä¸­
return Variable._execution_engine.run_backward( # è°ƒç”¨ C++ å¼•æ“è¿è¡Œåå‘ä¼ æ’­
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeErrorï¼šæ¢¯åº¦è®¡ç®—æ‰€éœ€çš„å˜é‡ä¹‹ä¸€å·²è¢«å°±åœ°æ“ä½œä¿®æ”¹ï¼š[torch.FloatTensor [10, 5]]ï¼ˆAsStridedBackward0 çš„è¾“å‡º 0ï¼‰å¤„äºç‰ˆæœ¬ 2ï¼›é¢„æœŸä¸ºç‰ˆæœ¬ 1ã€‚æç¤ºï¼šå¯ç”¨å¼‚å¸¸æ£€æµ‹ä»¥æŸ¥æ‰¾æ— æ³•è®¡ç®—æ¢¯åº¦çš„æ“ä½œï¼Œä½¿ç”¨ torch.autograd.set_detect_anomaly(True)ã€‚

æˆ‘æœ‰ç‚¹æ˜ç™½ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿè¿™ç§æƒ…å†µï¼Œä½†æœ‰åŠæ³•è§£å†³è¿™ä¸ªé—®é¢˜å—ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/79205991/training-different-stage-of-model-with-different-loss</guid>
      <pubDate>Wed, 20 Nov 2024 06:10:33 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ­£ç¡®è®¾ç½® pad tokenï¼ˆè€Œä¸æ˜¯ eosï¼‰ä»¥é¿å…æ¨¡å‹æ— æ³•é¢„æµ‹ EOSï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>ä¸ºä»€ä¹ˆä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹é¢„æµ‹è‚¡ç¥¨ä»·æ ¼æ—¶å‡†ç¡®ç‡èƒ½è¾¾åˆ° 100%ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/66154986/why-am-i-getting-100-accuracy-when-using-a-linear-regression-model-to-predict-s</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°è¯•ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹åœ¨ Python ä¸­é¢„æµ‹è‚¡ç¥¨ä»·æ ¼ã€‚æˆ‘ä½¿ç”¨ train_test_split åˆ†å‰²æ•°æ®ï¼Œå› æ­¤æ®æˆ‘æ‰€çŸ¥ï¼Œæˆ‘çš„æµ‹è¯•æ•°æ®ä¸åº”è¯¥åœ¨æˆ‘çš„è®­ç»ƒæ•°æ®ä¸­ï¼Œæ‰€ä»¥æˆ‘ä¸æ˜ç™½ä¸ºä»€ä¹ˆæ¨¡å‹çš„å‡†ç¡®ç‡æ˜¯ 100%ã€‚
è¿™æ˜¯æˆ‘çš„ä»£ç ï¼š
X = RMV.drop(&#39;Close&#39;, axis=1)
y = RMV[&#39;Close&#39;]`

æ¥è‡ª sklearn.model_selection å¯¼å…¥ train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

æ¥è‡ª sklearn.linear_model å¯¼å…¥ LinearRegression
reg = LinearRegression()
reg.fit(X_train, y_train)

reg_preds = reg.predict(X_test)

å½“æˆ‘ä½¿ç”¨æ­¤ä»£ç è¿è¡Œäº¤å‰éªŒè¯ä»¥æµ‹è¯•å‡†ç¡®æ€§æ—¶ï¼Œæˆ‘å¾—åˆ°çš„å€¼ä¸º1.00ã€‚
scores = model_selection.cross_val_score(reg, X_test, y_test, cv=10)
print (&quot;Accuracy: %0.2f (+/- %0.2f)&quot; % (scores.mean(), scores.std() / 2)) 

ä½œä¸ºå‚è€ƒï¼Œä¸‹é¢æ˜¯æˆ‘ä½¿ç”¨çš„æ•°æ®æ ·æœ¬ï¼š
 æ”¶ç›˜ä»· SMA EMA MACD ä¸Šè½¨ ä¸­è½¨ ä¸‹è½¨ RSI
æ—¥æœŸ 
2010-02-18 60.900002 57.335715 57.419887 2.099073 64.842238 55.4075 45.972762   60.517959
2010-02-19 61.000000 57.967857 57.897236 2.215288 65.422290 55.9000 46.377710 60.672590
2010-02-22 62.099998 58.560714 58.457604 2.368843 66.047128 56.4675 46.887872 62.416318
2010-02-23 61.200001 59.117857 58.823257 2.390360 66.386746 57.0000 47.613254 60.069541
2010-02-24 60.900002 58.539286 59.100156 2.356046 66.504379 57.5425 48.580621 59.269579

æˆ‘å“ªé‡Œé”™äº†ï¼Ÿ
æ›´æ–°ï¼šå‡†ç¡®åº¦ä¼¼ä¹æ˜¯é”™è¯¯çš„æŒ‡æ ‡ï¼Œå› æ­¤æˆ‘å·²æŒ‰ç…§å›å¤çš„å»ºè®®æ”¹ç”¨ MSEï¼š
print(&#39;å‡æ–¹è¯¯å·®ï¼š&#39;, metrics.mean_squared_error(y_true=y_test, y_pred=lm_preds))
print(&#39;åˆ¤å®šç³»æ•°ï¼š%.2f&#39; % metrics.r2_score(y_true=y_test, y_pred=lm_preds))

æ ¹æ®è¿è¡Œæƒ…å†µï¼Œè¿™ç»™äº†æˆ‘å¤§çº¦ MSE = 13-15ï¼ŒR2 = 0.999ï¼Œè¿™ä»ç„¶éå¸¸é«˜ã€‚ç”±äºå¹³å‡è‚¡ä»·åœ¨ 600 å·¦å³ï¼ŒMSE å®é™…ä¸Šå¹¶æ²¡æœ‰çœ‹èµ·æ¥é‚£ä¹ˆé«˜ã€‚è¯¥æ¨¡å‹ä¼¼ä¹ä»ç„¶è¡¨ç°å¾—å¤ªå¥½äº†ã€‚
æˆ‘ä½¿ç”¨çš„æ˜¯ 2010-2020 å¹´çš„ Rightmove è‚¡ç¥¨æ•°æ®ã€‚æˆ‘åˆšåˆšåˆ‡æ¢åˆ°ä½¿ç”¨ 2010-2020 å¹´å’Œ 2019-2020 å¹´æ³¢åŠ¨æ€§æ›´å¤§çš„è‚¡ç¥¨ (PMO.L)ï¼Œå¹¶ä¸”æˆ‘è¿˜åˆ é™¤äº†æˆ‘ä½¿ç”¨çš„ 5/7 ä¸ªæŒ‡æ ‡ã€‚
å¯¹äº 2010-2020 å¹´ï¼Œè¯¥æ¨¡å‹ç»™å‡ºçš„ MSE ä¸º 69ï¼ˆä¸è‚¡ä»·ç›¸æ¯”ç›¸å¯¹è¾ƒä½ï¼‰å’Œ 0.999 R2ã€‚ç„¶è€Œï¼Œå¯¹äº 2019-2020 å¹´ï¼Œè¯¥æ¨¡å‹ç¡®å®ä¼¼ä¹æœ‰ç‚¹å·®ï¼ŒMSE ä¸º 15.5ï¼ŒR2 ä¸º 0.82ï¼Œæ˜æ˜¾ä½äºä»¥å‰ã€‚ç„¶è€Œï¼Œè€ƒè™‘åˆ°è¿™åªæ˜¯ä¸€å¹´çš„æ•°æ®ï¼Œå®ƒçš„è¡¨ç°ä¼¼ä¹ä»ç„¶å¤ªå¥½äº†ã€‚
ä»¥ä¸‹æ˜¯ç”¨äºè®­ç»ƒæ–°è‚¡ç¥¨æ¨¡å‹çš„ç‰¹å¾æ•°æ®æ ·æœ¬ï¼š
2010-2020ï¼š
 SMA EMA
æ—¥æœŸ
2010-02-18 266.214286 266.857731
2010-02-19 266.910714 268.110034
2010-02-22 267.303571 269.428696
2010-02-23 267.589286 269.838203
2010-02-24 264.660714 270.659776

2011-2020:
 SMA EMA
æ—¥æœŸ
2019-02-18 73.425000 73.791397
2019-02-19 73.632143 74.052544
2019-02-20 73.785715 74.325538
2019-02-21 73.953572 74.335466
2019-02-22 73.928572 74.330738
]]></description>
      <guid>https://stackoverflow.com/questions/66154986/why-am-i-getting-100-accuracy-when-using-a-linear-regression-model-to-predict-s</guid>
      <pubDate>Thu, 11 Feb 2021 12:40:43 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•æ­£ç¡®åœ°å°†ä¸å¹³è¡¡çš„æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/57229775/how-can-i-properly-split-imbalanced-dataset-to-train-and-test-set</link>
      <description><![CDATA[æˆ‘æœ‰ä¸€ä¸ªèˆªç­å»¶è¯¯æ•°æ®é›†ï¼Œåœ¨é‡‡æ ·ä¹‹å‰å°è¯•å°†è¯¥æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚å‡†æ—¶æƒ…å†µçº¦å æ€»æ•°æ®çš„ 80%ï¼Œå»¶è¯¯æƒ…å†µçº¦å  20%ã€‚
é€šå¸¸ï¼Œæœºå™¨å­¦ä¹ ä¸­è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å¤§å°æ¯”ä¾‹ä¸º 8:2ã€‚ä½†æ•°æ®å¤ªä¸å¹³è¡¡äº†ã€‚å› æ­¤ï¼Œè€ƒè™‘åˆ°æç«¯æƒ…å†µï¼Œå¤§å¤šæ•°è®­ç»ƒæ•°æ®éƒ½æ˜¯å‡†æ—¶æƒ…å†µï¼Œè€Œå¤§å¤šæ•°æµ‹è¯•æ•°æ®éƒ½æ˜¯å»¶è¯¯æƒ…å†µï¼Œå‡†ç¡®ç‡ä¼šå¾ˆå·®ã€‚
æ‰€ä»¥æˆ‘çš„é—®é¢˜æ˜¯å¦‚ä½•æ­£ç¡®å°†ä¸å¹³è¡¡çš„æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/57229775/how-can-i-properly-split-imbalanced-dataset-to-train-and-test-set</guid>
      <pubDate>Sat, 27 Jul 2019 06:34:52 GMT</pubDate>
    </item>
    </channel>
</rss>