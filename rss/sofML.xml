<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 22 Sep 2024 09:15:56 GMT</lastBuildDate>
    <item>
      <title>训练过程因“RuntimeError：没有用于已保存叶子的梯度累加器！”而崩溃。</title>
      <link>https://stackoverflow.com/questions/79011167/the-training-processes-were-crashed-by-runtimeerror-no-grad-accumulator-for-a</link>
      <description><![CDATA[在训练简单模型时，loss.backward() 中的 RuntimeError: No grad accumulator for a saved leaf! 导致进程崩溃，但我确保所有需要计算梯度的数据都放在 GPU 上。
def train_epoch(args, epoch, model, loss_fn, optim, dataloader, lr_scheduler=None, warmup_scheduler=None):
model.train()
dataloader.sampler.set_epoch(epoch)

mae_m, loss_m = AverageMeter(), AverageMeter()
calc_m, read_m = AverageMeter(), AverageMeter()
timer = Timer()
log_step = len(dataloader) // 11
if args.local_rank == 0:
args.writer.add_scalar(&#39;lr&#39;, optim.param_groups[0][&#39;lr&#39;], epoch)

mae_list, pred_list = [], []

for step, sample in enumerate(dataloader):
data, label = sample[&#39;data&#39;].cuda().requires_grad_(), sample[&#39;label&#39;].cuda()
read_m.add(timer.tiktok())

optim.zero_grad()
# (output, deep_output), attn = model(data)
output = model(data)
output = output.reshape(label.shape)
# loss = loss_fn(output, label) + loss_fn(deep_output, label)
loss = loss_fn(output, label)
loss.backward(retain_graph=True)

上面列出了训练的代码。其中一个流程的错误如下图所示：
错误
我向 GPT 寻求帮助，按照它的建议为 data 添加 .requires_grad_()（我认为这是不必要的）以确保它将计算梯度，并为 loss.backward() 添加 retain_graph=True。但仍然不起作用。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79011167/the-training-processes-were-crashed-by-runtimeerror-no-grad-accumulator-for-a</guid>
      <pubDate>Sun, 22 Sep 2024 08:08:01 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用深度学习进行心脏扩大检测。是否可以使用自动编码器提取特征并将其作为集成模型的输入</title>
      <link>https://stackoverflow.com/questions/79010975/i-am-doing-cardiomegaly-detection-using-deep-learning-is-it-possible-to-extract</link>
      <description><![CDATA[我正在使用深度学习进行心脏扩大检测。是否可以使用自动编码器提取特征并将其作为集成模型的输入。这很复杂吗？结果会是什么样子？
通常我的数据集很小，我尝试了一些来自 chatgpt 的代码，它显示了 artibutte 错误。可以做什么？]]></description>
      <guid>https://stackoverflow.com/questions/79010975/i-am-doing-cardiomegaly-detection-using-deep-learning-is-it-possible-to-extract</guid>
      <pubDate>Sun, 22 Sep 2024 05:54:57 GMT</pubDate>
    </item>
    <item>
      <title>如何实时高效地检测图表和行为模式？</title>
      <link>https://stackoverflow.com/questions/79010946/how-to-detect-chart-and-behavioral-pattern-efficiently-in-real-time</link>
      <description><![CDATA[如何使用 Python 或任何其他脚本语言实时高效地检测图表模式？
我想检测类似这种逻辑的各种模式
 1. 多个拒绝区域（多次拒绝区域）
2. 摆动高低区域
3. 拒绝预测（蜡烛的灯芯区域）
4. 流动性狩猎（通过突破高/低来获取流动性）

这种类型的模式以图表形式出现，但方式并不统一，这就是为什么我很难用手动条件进行编码。我该怎么做并创建一个模型来寻找进入机会？ 是否可以使用 DL/生成式 AI 来实现，或者如何实现这些参数来寻找机会？

这里，S&amp;D 是多重拒绝区；R 表示拒绝预测；两条红色水平线是两个摆动点
注意：由于我的代码很长，我正在共享整个文件。 我的尝试在这里，5 分钟的数据集在这里。任何建议都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/79010946/how-to-detect-chart-and-behavioral-pattern-efficiently-in-real-time</guid>
      <pubDate>Sun, 22 Sep 2024 05:33:00 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 CycleGAN 实现用于图像转换的 Web 应用程序？</title>
      <link>https://stackoverflow.com/questions/79010870/how-to-implement-a-web-app-with-cyclegan-for-image-conversion</link>
      <description><![CDATA[我正在尝试创建一个使用 CycleGAN 模型进行图像转换的 Web 应用程序。该应用程序应该有一个用户友好的前端，用户可以在其中上传图像，以及一个后端，使用预先训练的 CycleGAN 模型处理图像，并将转换后的图像返回给用户。
一个有效的 Web 应用程序，用户可以通过前端上传图像。
后端应该接收图像，通过 CycleGAN 模型处理它，并返回转换后的图像。
前端应该向用户显示转换后的图像。]]></description>
      <guid>https://stackoverflow.com/questions/79010870/how-to-implement-a-web-app-with-cyclegan-for-image-conversion</guid>
      <pubDate>Sun, 22 Sep 2024 04:22:01 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的训练损失总是为零？！我在浪费时间吗？</title>
      <link>https://stackoverflow.com/questions/79010666/why-the-heck-is-my-training-loss-always-zero-am-i-wasting-my-time</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79010666/why-the-heck-is-my-training-loss-always-zero-am-i-wasting-my-time</guid>
      <pubDate>Sun, 22 Sep 2024 00:25:31 GMT</pubDate>
    </item>
    <item>
      <title>进行迁移学习后加载 Keras 模型时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/79010662/valueerror-while-loading-my-keras-model-after-doing-transfer-learning</link>
      <description><![CDATA[我开发了一种手语识别模型，使用 ResNet50 架构作为识别乌尔都语手语的基础模型。模型架构定义如下：
base_model = ResNet50(include_top=False, weights=&#39;imagenet&#39;, input_shape=(256, 256, 3))

model = tf.keras.Sequential([
tf.keras.layers.Input(shape=(256,256,3)),
tf.keras.layers.Lambda(preprocess_input_resnet),
base_model,
tf.keras.layers.GlobalAveragePooling2D(),
tf.keras.layers.Dense(128,activation=&#39;relu&#39;),
tf.keras.layers.Dense(37,activation=&#39;softmax&#39;)
])

在对模型进行几个 epoch 的训练后，我通过设置某些层对其进行了微调trainable:
base_learning_rate = 0.0001

base_model.trainable = True
fine_tune_at = 100

# 在微调之前冻结层
for layer in base_model.layers[:fine_tune_at]:
layer.trainable = False

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate / 10),
metrics=[&#39;accuracy&#39;])

history_fine = model.fit(train_dataset, epochs=total_epochs, validation_data=validation_dataset)

问题：
当我尝试使用 load_model(&#39;model.h5&#39;) 加载模型时，我遇到以下问题ValueError：

ValueError：层“dense”需要 1 个输入，但收到了 2 个输入张量。收到的输入：[&lt;KerasTensor shape=(None, 8, 8, 2048), dtype=float32, sparse=False, name=keras_tensor_564&gt;, &lt;KerasTensor shape=(None, 8, 8, 2048), dtype=float32, sparse=False, name=keras_tensor_565&gt;]

该错误表明 Dense 层正在接收两个输入张量，而不是一个。
我已通过运行 model.summary() 检查了模型架构，并确认各层的结构符合预期。但是，我对第二个张量（keras_tensor_565）的来源感到困惑。似乎架构中的某个地方可能存在意外的连接或重复。
我也尝试过以 .h5 和 .keras 格式保存和加载模型。但在加载模型时，两种格式都存在相同的错误。
model1.save(&quot;model.h5&quot;)

model = load_model(&#39;model.h5&#39;, custom_objects={&#39;preprocess_input&#39;: preprocess_input})


为什么 Dense 层会接收两个张量？
如何解决此问题以成功加载我的模型而不会出现错误？
如何确保我的模型架构在保存和加载过程中保持一致？
]]></description>
      <guid>https://stackoverflow.com/questions/79010662/valueerror-while-loading-my-keras-model-after-doing-transfer-learning</guid>
      <pubDate>Sun, 22 Sep 2024 00:24:16 GMT</pubDate>
    </item>
    <item>
      <title>在 R 中运行随机森林生存模型 (rfsrc) 时 R 会话中止</title>
      <link>https://stackoverflow.com/questions/79010492/r-session-aborted-when-running-random-forest-survival-model-rfsrc-in-r</link>
      <description><![CDATA[我正在使用 R 中的“randomForestSRC”包进行生存分析项目。不幸的是，每次我尝试运行随机森林生存模型 (rfsrc) 时，我的 R 会话都会崩溃，并显示以下消息：“R 遇到致命错误。会话已终止。”
这是我到目前为止所做的：
在此处输入图像描述

数据：数据集已清理，没有缺失值，由生存时间 (time_month) 和事件状态 (死亡) 组成。我已经成功地在这个数据集上运行了其他生存模型（例如，bnnsurvival、coxph），没有任何问题。

模型设置：

我为随机森林模型创建了公式，如下所示：



r
 formula_rfsrc &lt;- as.formula(paste(&quot;Surv(time_month, death) ~&quot;, paste(pred_vars, collapse = &quot; + &quot;)))


然后我尝试拟合模型：

 fit &lt;- rfsrc(formula_rfsrc, data = df_train, ntree = 50)


尝试修复：

检查公式：我已验证公式正确，对生存对象使用 Surv()。
减少数据大小：我尝试在较小的数据子集上运行模型。
限制树深度：我使用了 nodesize、nodedepth 等参数，并减少了树的数量。
内存管理：我确保在模型拟合之前调用垃圾收集 (gc())，认为问题可能与内存有关。



尽管付出了这些努力，但每当我运行随机森林模型时，R 会话都会崩溃。
其他详细信息：
数据集有大约 30 个预测变量，我根据需要将其转换为因子或数字。
我在装有 R 4.4.1 的机器上运行此程序。
有人遇到过类似的 randomForestSRC 问题吗？有没有关于如何解决这个问题或在 R 中运行生存随机森林的替代方法的想法？
提前感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/79010492/r-session-aborted-when-running-random-forest-survival-model-rfsrc-in-r</guid>
      <pubDate>Sat, 21 Sep 2024 21:33:20 GMT</pubDate>
    </item>
    <item>
      <title>AutoModelForSequenceClassification 损失没有减少</title>
      <link>https://stackoverflow.com/questions/79010018/automodelforsequenceclassification-loss-not-decrease</link>
      <description><![CDATA[从数据集导入 load_dataset
从 torch.utils.data 导入 DataLoader
从 transformers 导入 AutoTokenizer、AutoModelForSequenceClassification
导入 torch
从 tqdm 导入 tqdm

def train_one_epoch(model、dataloader、optimizer):
model.train()
loss_list = []
for batch in tqdm(dataloader):
batch_data = {
&#39;input_ids&#39;: batch[&#39;input_ids&#39;],
&#39;attention_mask&#39;: batch[&#39;attention_mask&#39;],
&#39;labels&#39;: batch[&#39;labels&#39;]
}
loss = model(**batch_data).loss
loss.backward()
optimizer.step()
optimizer.zero_grad()

loss_list.append(loss.detach().item())
avg_loss = sum(loss_list) / len(loss_list)
print(&#39;avg loss在 epoch:&#39;, avg_loss)

def assess(model, dataloader):
model.eval()
all_labels = []
all_predictions = []
for batch in dataloader:
with torch.no_grad():
batch_data = {
&#39;input_ids&#39;: batch[&#39;input_ids&#39;],
&#39;attention_mask&#39;: batch[&#39;attention_mask&#39;]
}
logits = model(**batch_data).logits
predictions = torch.argmax(logits, dim=-1)
labels = batch[&#39;labels&#39;]
all_labels.extend(labels)
all_predictions.extend(predictions)
accuracy = compute_accuracy(all_predictions, all_labels)
print(&quot;Accuracy&quot;, accuracy)
return accuracy

def compute_accuracy(predictions, labels):
correct = 0
for pred，zip(predictions, labels) 中的标签：
if pred == label:
correct += 1
返回正确 / len(labels)

def my_collat​​e_fn(batched_samples):
texts = [example[&#39;text&#39;] 例如 batched_samples]
labels = [example[&#39;label&#39;] 例如 batched_samples]
text_encoding = tokenizer(texts, max_length=128, truncation=True, padding=True, return_tensors=&#39;pt&#39;)
labels = torch.LongTensor(labels)
return {
&#39;input_ids&#39;: text_encoding[&#39;input_ids&#39;].cuda(),
&#39;attention_mask&#39;: text_encoding[&#39;attention_mask&#39;].cuda(),
&#39;labels&#39;: labels.cuda()
}

torch.manual_seed(64)
batch_size = 16
学习率 = 5e-5
训练次数 = 10
模型名称 = “roberta-base”

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

model = model.cuda()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate, eps=1e-8)

datasets = load_dataset(&quot;gpt3mix/sst2&quot;)

train_dataloader = DataLoader(
datasets[&#39;train&#39;],
batch_size=8,
shuffle=True,
collat​​e_fn=my_collat​​e_fn,
num_workers=0
)

validation_dataloader = DataLoader(
datasets[&#39;validation&#39;],
batch_size=8,
shuffle=False,
collat​​e_fn=my_collat​​e_fn,
num_workers=0
)

best_acc = 0.0
for周期范围（1，num_epochs + 1）：
train_one_epoch（模型，train_dataloader，优化器）
valid_acc = 评估（模型，validation_dataloader）


100%|██████████| 865/865 [01:27&lt;00:00，9.89it/s]

周期内平均损失：0.6746856869559068

准确率 0.4908256880733945

100%|██████████| 865/865 [01:25&lt;00:00, 10.09it/s]

epoch 中的平均损失：0.6922555248516833

准确率 0.4908256880733945

100%|██████████| 865/865 [01:27&lt;00:00, 9.89it/s]

epoch 中的平均损失：0.6976809655310791

准确率 0.5091743119266054

更改学习率也不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/79010018/automodelforsequenceclassification-loss-not-decrease</guid>
      <pubDate>Sat, 21 Sep 2024 16:24:50 GMT</pubDate>
    </item>
    <item>
      <title>CNN-KAN 模型的训练尚未开始</title>
      <link>https://stackoverflow.com/questions/79009899/the-training-of-a-cnn-kan-model-is-not-starting</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79009899/the-training-of-a-cnn-kan-model-is-not-starting</guid>
      <pubDate>Sat, 21 Sep 2024 15:25:05 GMT</pubDate>
    </item>
    <item>
      <title>X 有 8 个特征，但 RandomForestRegressor 需要 2924 个特征作为输入</title>
      <link>https://stackoverflow.com/questions/79009698/x-has-8-features-but-randomforestregressor-is-expecting-2924-features-as-input</link>
      <description><![CDATA[我正在使用 Kaggle 数据集和 RandomForestRegressor 为我的城市构建餐厅推荐器。
我构建了模型，现在希望模型在给定 4 个参数时推荐一家好餐厅：位置、大致费用、餐厅类型和投票数。但是，它返回了一个值错误：
X 有 8 个特征，但 RandomForestRegressor 需要 2924 个特征作为输入。

这是我尝试运行的：
import joblib
import numpy as np
from sklearn.preprocessing import StandardScaler

model = joblib.load(&#39;my_model.pkl&#39;)
scaler = joblib.load(&#39;scaler.pkl&#39;)

def preprocess_input(location, type_, cost, votes):
one_hot_location = [1 if loc == location else 0 for loc in [&#39;Whitefield&#39;, &#39;Koramangala&#39;, &#39;Indiranagar&#39;]]
one_hot_type = [1 if t == type_ else 0 for t in [&#39;Casual Dining&#39;, &#39;Quick Bites&#39;, &#39;Cafe&#39;]]

scaled_features = scaler.transform([[cost, votes]])

return np.array(one_hot_location + one_hot_type + list(scaled_features[0])).reshape(1, -1)

input_data = preprocess_input(&#39;Whitefield&#39;, &#39;Casual Dining&#39;, 1000, 500)

prediction = model.predict(input_data)

print(f&quot;预测的餐厅：{prediction}&quot;)

训练数据的形状：
X_train.shape = (41373, 2924)
y_train.shape = (41373,)
这是我的数据集的样子]]></description>
      <guid>https://stackoverflow.com/questions/79009698/x-has-8-features-but-randomforestregressor-is-expecting-2924-features-as-input</guid>
      <pubDate>Sat, 21 Sep 2024 13:43:40 GMT</pubDate>
    </item>
    <item>
      <title>我的非序列 keras 模型的一个输入出现了难以理解的形状错误</title>
      <link>https://stackoverflow.com/questions/79009687/incomprehensible-shape-error-with-one-of-the-inputs-of-my-non-sequential-keras-m</link>
      <description><![CDATA[我编写了以下 keras 模型
input_A = 输入(shape=[5], name=&quot;wide_input&quot;)
hidden_​​layer_1 = Dense(10,activation=&quot;relu&quot;, name=&#39;h_wide_layer&#39;)(input_A)
input_B = 输入(shape=[6], name=&quot;deep_input&quot;)
hidden_​​layer_2 = Dense(30,activation=&quot;relu&quot;, name=&#39;h_deep_layer_1&#39;)(input_B)
hidden_​​layer_3 = Dense(30,activation=&quot;relu&quot;, name=&#39;h_deep_layer_2&#39;)(hidden_​​layer_2)
concat = Concatenate()([hidden_​​layer_1, hidden_​​layer_3])
output = Dense(1, name=&quot;output&quot;)(concat)
complex_model_2_1 = keras.Model(inputs=[input_A, input_B], output=[output])

#编译第二个复杂模型
complex_model_2_1.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;sgd&quot;)

#训练
X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]
X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]
X_test_A, X_test_B = X_test_full[:, :5], X_test_full[:, 2:]
X_new_A, X_new_B = X_new[:, :5], X_new[:, 2:]
print(f&quot;X_train_A 的形状：{X_train_A.shape}&quot;)
print(f&quot;X_train_B 的形状：{X_train_B.shape}&quot;)
history = complex_model_2_1.fit({&quot;wide_input&quot;: X_train_A, &quot;deep_input&quot;: X_train_B},
y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))

X_train 有 8 个特征，因此 X_train_A 和 X_train_B 实际上分别有 5 个和 6 个特征。但是，我不明白为什么我会得到下面的hidden_​​layer_2 不兼容形状错误:
ValueError Traceback (most recent call last)
&lt;ipython-input-42-86b1419058f7&gt; in &lt;cell line: 11&gt;()
9 print(f&quot;Shape of X_train_A: {X_train_A.shape}&quot;)
10 print(f&quot;Shape of X_train_B: {X_train_B.shape}&quot;)
---&gt; 11 history = complex_model_2_1.fit({&quot;wide_input&quot;: X_train_A, &quot;deep_input&quot;: X_train_B},
12 y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))
13 

1 帧
/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py in assert_input_compatibility(input_spec, input, layer_name)
225 无,
226 }:
--&gt; 227 raise ValueError(
228 f&#39;层“{layer_name}”的输入 {input_index} 是 &#39;
229 f“与层不兼容：预期轴 {axis}”

ValueError：调用 Functional.call() 时遇到异常。

层“h_deep_layer_1”的输入 0 与层不兼容：预期输入形状的轴 -1 具有值 6，但收到的输入形状为 (None, 5)

Functional.call() 收到的参数：
• 输入={&#39;wide_input&#39;: &#39;tf.Tensor(shape=(None, 5), dtype=float32)&#39;, &#39;deep_input&#39;: &#39;tf.Tensor(shape=(None, 6), dtype=float32)&#39;}
• 训练=True
• 掩码={&#39;wide_input&#39;: &#39;None&#39;, &#39;deep_input&#39;: &#39;None&#39;}

如何修复？
PS：Google colab 中的 Gemini 无法解释该问题，并向我建议 X_train_B = X_train[:, 5:]，这是不正确的（形状为 (_, 3)]]></description>
      <guid>https://stackoverflow.com/questions/79009687/incomprehensible-shape-error-with-one-of-the-inputs-of-my-non-sequential-keras-m</guid>
      <pubDate>Sat, 21 Sep 2024 13:38:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 Scikit-learn、XGBoost 和 Prophet 时，保存训练模型的最佳文件格式是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79008634/what-is-the-best-file-format-to-save-trained-model-when-using-scikit-learn-xgbo</link>
      <description><![CDATA[我正在使用 Scikit-learn 开展 ML 项目。根据我的研究，人们建议使用 .joblib 保存经过训练的 Scikit-learn 模型。
这就是我将模型保存到 .joblib 的方式&gt;
import os
from joblib import dump

model_path = os.path.join(script_dir, &quot;../models/trained_model.joblib&quot;)
dump(model, model_path)
print(f&quot;Model saved at {model_path}&quot;)

我还想使用 XGBoost 和 Prophet 测试此模型，只是为了尝试不同的库。

什么是实现此目标的最佳文件格式？我在搜索过程中多次看到 ONNX，但它似乎与 Prophet 不兼容。

有没有办法将我的模型同时保存为 joblib 和 onnx，或者我是否需要将 jobllib 转换为 onnx 文件？

]]></description>
      <guid>https://stackoverflow.com/questions/79008634/what-is-the-best-file-format-to-save-trained-model-when-using-scikit-learn-xgbo</guid>
      <pubDate>Sat, 21 Sep 2024 01:49:12 GMT</pubDate>
    </item>
    <item>
      <title>无监督图像聚类：无法获得正确的结果[关闭]</title>
      <link>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</link>
      <description><![CDATA[我正在开展一个个人项目，该项目采用一组图像（金属螺母）并确定是否存在缺陷（着色、划痕、弯曲、翻转和良好）。
我使用 VGG16 模型提取特征，使用 PCA 降低维数，然后将降维后的特征输入到简单的 k 均值算法（k=5）中以识别聚类。
我遇到的问题归结为：从模型中提取的特征对于解决手头的问题并不是很有效。
更具体地说，如果我想识别特定的“翻转”金属螺母（只是制造时齿朝向错误的螺母），提取的特征确实很有效。因此，集群最终是 4 个随机集，然后是 1 组刚翻转的螺母。
我的问题是，我可以做些什么来修改我的模型/提取的特征，使它们更适合我的问题（识别所有 5 个类别的金属螺母）？我甚至很高兴能够从“有缺陷”中识别出“好”的螺母。
我尝试过的事情：

在“好”图像的训练集上训练模型（即只是普通的金属螺母）
从模型的较早层（第 10 层）而不是倒数第二层获取输出
使用不同的模型（我最初使用的是 ResNet18）
]]></description>
      <guid>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</guid>
      <pubDate>Wed, 11 Sep 2024 19:16:38 GMT</pubDate>
    </item>
    <item>
      <title>Detectron2 检查点未找到</title>
      <link>https://stackoverflow.com/questions/65327162/detectron2-checkpoint-not-found</link>
      <description><![CDATA[从昨晚开始就一直出现这样的错误，我训练了5个模型，都没有问题，然后就出现了这样的问题，怎么解决呢？
AssertionError Traceback (most recent call last)
&lt;ipython-input-9-08522bc16525&gt; in &lt;module&gt;()
34 os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
35 trainer = CocoTrainer(cfg)
---&gt; 36 trainer.resume_or_load(resume=False)
37 trainer.train()

2 帧
/usr/local/lib/python3.6/dist-packages/fvcore/common/checkpoint.py in load(self, path, checkpointables)
118 if not os.path.isfile(path):
119 path = self.path_manager.get_local_path(path)
--&gt; 120 断言 os.path.isfile(path)，“未找到检查点 {}！”。格式（路径）
121 
122 checkpoint = self._load_file(path)

AssertionError：未找到检查点 https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x/139173657/model_final_68b088.pkl！
]]></description>
      <guid>https://stackoverflow.com/questions/65327162/detectron2-checkpoint-not-found</guid>
      <pubDate>Wed, 16 Dec 2020 16:22:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 K-Means 在 LAB 颜色空间中按颜色对图像进行聚类</title>
      <link>https://stackoverflow.com/questions/30368942/cluster-image-by-colors-in-lab-color-space-using-k-means</link>
      <description><![CDATA[我尝试了以下代码。
he = imread(&#39;hestain.png&#39;);
imshow(he), title(&#39;H&amp;E image&#39;);
cform = makecform(&#39;srgb2lab&#39;);
la​​b_he = applycform(he,cform);
ab = double(lab_he(:,:,2:3));
nrows = size(ab,1); %n 行
ncols = size(ab,2); %p 列
ab = reshape(ab,nrows*ncols,2);

nColors = 3;

[cluster_idx, cluster_center] = kmeans(ab,nColors); 

它给我错误 

reshape 无法从 n*1 数组创建 n*p 矩阵。

这很有道理，但它在这里有效。
我在 octave 中尝试了相同类型的代码
ed=edge(de,&quot;canny&quot;);
imshow(ed);
ed=double(ed);
nrows=size(ed,1);
ncols=size(ed,2);
ed=reshape(ed,nrows*ncols,2)
[cluster_idx, cluster_center]=kmeans(ed,3);
pixel_labels = reshape(cluster_idx,nrows,ncols);
imshow(pixel_labels,[]), title(&#39;image labeled by cluster index&#39;);

其中 de 是一些图像。
当我运行时，我收到此错误。

错误：重塑：无法将 181x181 数组重塑为 32761x2 数组

感谢帮助]]></description>
      <guid>https://stackoverflow.com/questions/30368942/cluster-image-by-colors-in-lab-color-space-using-k-means</guid>
      <pubDate>Thu, 21 May 2015 08:55:18 GMT</pubDate>
    </item>
    </channel>
</rss>