<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 25 Apr 2024 12:21:06 GMT</lastBuildDate>
    <item>
      <title>R 中 Caret、Yardstick 和 MLeval 之间关于精确召回的差异</title>
      <link>https://stackoverflow.com/questions/78384468/discrepancy-between-caret-yardstick-and-mleval-in-r-regarding-precision-recall</link>
      <description><![CDATA[我试图绘制精确度-召回率曲线并测量插入符号交叉验证训练对象的曲线下面积。只需调用对象名称即可得出精确度-召回率曲线下面积的一些值，如下所示：
&gt; rf

随机森林

807 个样本

11 个预测器

2 个类别：&#39;X0&#39;、&#39;X1&#39;

无预处理

重采样：交叉验证（10 倍）

样本大小摘要：727、726、727、726、727、726、...

跨调整参数的重采样结果：

mtry splitrule AUC 精度召回率 F

2 gini 0.8179379 0.8618888 0.6713675 ​​0.7494214

2 extratrees 0.8061601 0.8960233 0.5725071 0.6901257

7 gini 0.7798593 0.8775955 0.8037037 0.8360293
7 extratrees 0.8004585 0.8587664 0.7696581 0.8090205
12 gini 0.7659204 0.8578710 0.8229345 0.8364962
12 extratrees 0.7840497 0.8498209 0.7925926 0.8167108

调整参数“min.node.size”保持不变，值为 1
AUC 用于选择使用最大值的最佳模型。
该模型使用的最终值为 mtry = 2、splitrule = gini 和 min.node.size = 1。

但是，当我尝试使用 yardstick 绘制实际曲线时，我得到了完全不同的结果。
prRf &lt;- pr_curve(rf$pred, X0, truth = obs)

ggplot() +
geom_path(aes(x = recall, y = precision), colour = &quot;blue&quot;, linetype = 1, data = prRf) +
xlab(&quot;Recall&quot;) +
ylab(&quot;Precision&quot;) +
theme_minimal() +
ylim(0,1)

pr_auc(rf$pred, X0, truth = obs)

在这里，曲线看起来“好多了”并且 AUPR 比 caret 给出的内部 AUPR 更高（0.878 vs. 0.817）。对于 MLeval 的简单运行也是如此，它给出了类似的“更好”的结果。
evalm(rf)

所有这些都让我很困惑，我觉得我可能以某种方式在样本内测试，但我不确定如何在不事先拆分数据的情况下正确地进行测试。任何帮助都值得感激。]]></description>
      <guid>https://stackoverflow.com/questions/78384468/discrepancy-between-caret-yardstick-and-mleval-in-r-regarding-precision-recall</guid>
      <pubDate>Thu, 25 Apr 2024 11:51:30 GMT</pubDate>
    </item>
    <item>
      <title>在数据分割过程中保留数据的空间分布</title>
      <link>https://stackoverflow.com/questions/78383883/preserving-spatial-distribution-of-data-during-data-splitting</link>
      <description><![CDATA[我正在尝试使用随机森林模型来模拟德国巴伐利亚河流中的硝酸盐浓度。我使用 Python，主要使用 sklearn。我有 490 个水质站的数据。我遵循 LongzhuQ.Shen 等人论文中的方法，该论文可以在这里找到：https://www.nature.com/articles/s41597-020-0478-7
我想将数据集分成训练集和测试集，以便两个集中数据的空间分布相同。这个想法是，如果数据分割忽略空间分布，则训练集可能最终会集中来自人口稠密区域的点，而忽略稀疏区域。这可能会扭曲模型的学习过程，使其在整个感兴趣领域的准确性或概括性降低。 sklearn train_test_split只是将数据随机划分为训练集和测试集，并且不考虑数据中的空间模式。
我上面提到的论文遵循了这种方法：“我们将完整的数据集分为两个子数据集，分别是训练和测试。为了考虑监测站空间分布的异质性，我们在数据分割步骤中采用了空间密度估计技术，通过使用带宽为 50 km 的高斯核（使用 GRASS GIS33 中可用的 v.kernel）构建密度表面来计算每个物种和季节。所得密度表面的像素值用作权重因子，将数据分成具有相同空间分布的训练和测试子集。”
我想遵循相同的方法，但我不使用草地 GIS，而是自己用 Python 构建密度表面。我还提取了概率密度值和站点的权重。 （附图）
现在我面临的唯一问题是如何使用这些权重将数据分成训练集和测试集？我检查了sklearn train_test_split函数中没有可以考虑权重的关键字。我也与GPT 4聊天来回，但它也无法给我一个明确的答案。我在互联网上也没有找到任何关于此的具体信息。也许我错过了一些东西。
还有其他函数可以用来执行此操作吗？或者我必须编写自己的算法来进行分割？如果是后者，您能否建议我一种方法，以便我自己编写代码？
在附图中您可以看到站点的位置以及使用核密度估计方法（使用高斯核）生成的概率密度面。
还附上我的数据框的屏幕截图，让您了解数据结构。 （经度（‘lon’）列之后的所有列都用作特征。NO3 列用作目标变量。）
如果有任何答案，我将不胜感激。
请查找附件图片以供参考。
使用高斯核的核密度估计方法生成的概率密度曲面。&lt; /p&gt;
我用来模拟硝酸盐浓度的数据集]]></description>
      <guid>https://stackoverflow.com/questions/78383883/preserving-spatial-distribution-of-data-during-data-splitting</guid>
      <pubDate>Thu, 25 Apr 2024 10:10:26 GMT</pubDate>
    </item>
    <item>
      <title>在 lightgbm 中，当数据集构建中已经存在时，为什么 train 和 cv API 接受 categorical_feature 参数</title>
      <link>https://stackoverflow.com/questions/78383840/in-lightgbm-why-do-the-train-and-the-cv-apis-accept-categorical-feature-argument</link>
      <description><![CDATA[以下是.cv lightgbm的API
&lt;块引用&gt;
lightgbm.cv（params，train_set，num_boost_round = 100，folds = None，nfold = 5，stratified = True，shuffle = True，metrics = None，feval = None，init_model = None，feature_name =&#39;auto&#39;，categorical_feature =&#39;auto&#39;，fpreproc=None，seed=0，callbacks=None，eval_train_metric=False，return_cvbooster=False）

有一个参数cateogrical_feature
&lt;块引用&gt;
分类特征。如果是 int 列表，则解释为索引。如果是 str 列表，则解释为功能名称（还需要指定 feature_name）。

现在是 .train API 
&lt;块引用&gt;
lightgbm.train(params, train_set, num_boost_round=100, valid_sets=None, valid_names=None, feval=None, init_model=None, feature_name=&#39;auto&#39;, categorical_feature=&#39;auto&#39;, keep_training_booster=False, 回调=None ）

这里还有一个categorical_feature参数。这方面的文档与上面相同
现在，您注意到这两个 API 都使用 lightgbm 数据集 本身带有一个categorical_feature 参数。文档完全一样
问题：

如果两者都指定，哪一个优先？
建议在哪一个位置指定 categorical_feature？
这两种选择在 lightgbm 管道的工作内部有什么不同吗？
]]></description>
      <guid>https://stackoverflow.com/questions/78383840/in-lightgbm-why-do-the-train-and-the-cv-apis-accept-categorical-feature-argument</guid>
      <pubDate>Thu, 25 Apr 2024 10:03:27 GMT</pubDate>
    </item>
    <item>
      <title>培训法学硕士执行职能</title>
      <link>https://stackoverflow.com/questions/78383583/training-a-llm-to-execute-functions</link>
      <description><![CDATA[我想开发一个能够在给定智能家居环境中执行操作并回答问题的聊天机器人。
我很好奇如何通过法学硕士来做到这一点。如何定制/训练模型来执行代码？
举一个简单的例子：当我告诉聊天机器人“打开客厅的灯”时它应该回答“我将打开客厅的灯”并同时在后台打开它（假设我有一个可以调用的 API/代码）。
您可以分享一些资源甚至示例来了解该流程吗？
我了解一些关于自定义模型的知识，例如添加系统消息或调整模型的温度，并且我之前还培训了法学硕士来生成软件需求。但我不知道如何训练模型来执行打开或关闭智能设备等操作。
我目前正在使用 Ollama 来管理和自定义模型。]]></description>
      <guid>https://stackoverflow.com/questions/78383583/training-a-llm-to-execute-functions</guid>
      <pubDate>Thu, 25 Apr 2024 09:23:07 GMT</pubDate>
    </item>
    <item>
      <title>Randforest，关于“对于每个节点，选择不替换的特征”的问题</title>
      <link>https://stackoverflow.com/questions/78383228/randforest-question-on-for-each-node-select-features-without-replacement</link>
      <description><![CDATA[从书籍（与chatGPT仔细检查）中，随机森林的步骤可以总结如下：
&lt;块引用&gt;

绘制大小为 n 的随机引导样本（随机选择 n 个示例
来自带有替换的训练数据集）。
从引导样本中生成决策树。在每个节点：

&lt;块引用&gt;
a)。随机选择d个特征，不进行替换。
b).使用提供最佳分割的功能来分割节点
例如，最大化信息增益的目标函数。
3. 重复步骤 1-2 k 次。
...


我的问题来自粗体字：既然特征是无替换选择的，如果选择了所有特征，树会停止生长吗（因为没有可供选择的特征）？ ——看来这不是真的。但这一步怎么理解呢？]]></description>
      <guid>https://stackoverflow.com/questions/78383228/randforest-question-on-for-each-node-select-features-without-replacement</guid>
      <pubDate>Thu, 25 Apr 2024 08:20:17 GMT</pubDate>
    </item>
    <item>
      <title>如何向 DBSCAN 添加自定义参数</title>
      <link>https://stackoverflow.com/questions/78383085/how-to-add-custom-parameter-to-dbscan</link>
      <description><![CDATA[我一直在努力创建一个自定义 DBSCAN，在其中我可以创建一个自定义参数来根据出租车 ID 过滤距离，以查看哪些出租车造成了交通堵塞。我添加了时间和距离矩阵，但我不知道如何创建一个新的 eps 来根据 ID 过滤出租车。如果 eps 值较高，则应对不同的 Taix ID 进行聚类，如果较低，则应对同一出租车进行聚类。有关如何进行的任何提示？谢谢
time_dist = pdist(X[:, 0].reshape(n, 1), metric=self.metric)
euc_dist = pdist(X[:, 1:], metric=self.metric)


# 使用 time_dist 过滤 euc_dist 矩阵
dist = np.where(time_dist &lt;= self.eps2, euc_dist, 2 * self.eps1)

db = DBSCAN(eps=self.eps1,
            min_samples=self.min_samples,
            指标=&#39;预先计算&#39;）
db.fit(正方形(距离))

self.labels = db.labels_
]]></description>
      <guid>https://stackoverflow.com/questions/78383085/how-to-add-custom-parameter-to-dbscan</guid>
      <pubDate>Thu, 25 Apr 2024 07:54:41 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：“dense”层的输入 0 与该层不兼容</title>
      <link>https://stackoverflow.com/questions/78383058/valueerror-input-0-of-layer-dense-is-incompatible-with-the-layer</link>
      <description><![CDATA[我正在尝试使用超模型编写一个分类器，有 4 个类。 X的尺寸为14935×2
这是我的代码的一部分：
def build_model(hp):
    模型=顺序（）
    activation_choice = hp.Choice(&#39;activation&#39;, value=[&#39;relu&#39;, &#39;sigmoid&#39;, &#39;tanh&#39;, &#39;elu&#39;, &#39;selu&#39;])
    model.add(密集(单位=hp.Int(&#39;units_input&#39;,
                                   最小值=512，
                                   最大值=1024，
                                   步骤=32),
                    input_dim=784，
                    激活=activation_choice））
    model.add(密集(单位=hp.Int(&#39;units_hidden&#39;,
                                   最小值=128，
                                   最大值=600，
                                   步骤=32),
                    激活=activation_choice））
    model.add（密集（4，激活=&#39;softmax&#39;））
    模型.编译(
        优化器=hp.Choice(&#39;优化器&#39;, 值=[&#39;adam&#39;,&#39;rmsprop&#39;,&#39;SGD&#39;]),
        损失=&#39;分类交叉熵&#39;，
        指标=[&#39;准确性&#39;])
    返回模型
调谐器=随机搜索(
    构建模型，
    目标=&#39;val_accuracy&#39;,
                                 
    最大试验次数=80，
    目录=&#39;测试目录&#39;
    ）
x_train, x_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)
x_train = x_train.reshape(2987, 8)
x_test = x_test.reshape(2987, 2)
x_train = x_train / 255
x_测试 = x_测试 / 255
y_train = utils.to_categorical(y_train, 10)
y_test = utils.to_categorical(y_test, 10)
调谐器.搜索(x_train,
             y_火车，
             批量大小=256，
             纪元=20，
             验证分割=0.2，
             ）

“tunner.search”报错：
层“密集”的输入 0与图层不兼容：输入形状的预期轴 -1 的值为 784，但收到的输入形状为（无，8）

我试图找到问题的解决方案，但没有任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78383058/valueerror-input-0-of-layer-dense-is-incompatible-with-the-layer</guid>
      <pubDate>Thu, 25 Apr 2024 07:49:53 GMT</pubDate>
    </item>
    <item>
      <title>我可以做些什么来优化 C++ 中的 CatBoost（或其他）模型？</title>
      <link>https://stackoverflow.com/questions/78382887/what-can-i-do-to-optimizing-catboost-or-other-model-in-c</link>
      <description><![CDATA[我的 C++ 程序从 .cbm 文件加载 CatBoost 模型并进行预测。但该模型对我来说花费了太多时间，我想减少它的延迟。
我想知道我可以在 C++ 中做什么来优化此类模型的延迟。
#include “wrapped_calcer.h”

结构模型{

    静态内联 ModelCalcerWrapper 模型 = ModelCalcerWrapper(&quot;./model/model.cbm&quot;);

    std::向量 xdataArray；

    无效 updateXData() {
        // 更新xdataArray中的数据
        // 我将计算一些值并将其放入模型中
    }

    双预测（）{
        // 我认为这一步对我来说花费了太多时间
        返回 model.Calc(xdataArray, {});
    }

};
]]></description>
      <guid>https://stackoverflow.com/questions/78382887/what-can-i-do-to-optimizing-catboost-or-other-model-in-c</guid>
      <pubDate>Thu, 25 Apr 2024 07:17:43 GMT</pubDate>
    </item>
    <item>
      <title>加载已保存的深度学习模型时出现问题</title>
      <link>https://stackoverflow.com/questions/78382495/issues-while-loading-saved-deep-learning-model</link>
      <description><![CDATA[在 VS code 中，我有 2 个笔记本，分别为 Final.ipynb 和 main.ipynb。在 Final.ipynb 中，我定义了模型架构，使用 Adam 对其进行编译并加载权重，并将模型保存为 main.keras。在 main.ipynb 中加载 main.keras 时，我收到这样的错误
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ValueError Traceback（最近一次调用最后一次）
[20] 中的单元格，第 1 行
----&gt; 1 模型 = keras.models.load_model(&#39;main4.keras&#39;)

文件c：\ Users \ kanch \ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ keras \ src \ saving \ saving_api.py：176，在load_model（文件路径，custom_objects，编译，安全模式）
    第173章
    第175章
--&gt; [第 176 章]
    177 文件路径，
    第178章
    179 编译=编译，
    180 安全模式=安全模式，
    181）
    182 if str(文件路径).endswith((“.h5”,“.hdf5”)):
    [第 183 章]
    184 文件路径，custom_objects=custom_objects，compile=编译
    185）

文件c：\ Users \ kanch \ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ keras \ src \ saving \ saving_lib.py：152，在load_model（文件路径，custom_objects，编译，safe_mode）中
    147 引发值错误（
    148 &#39;无效的文件名：需要 `.keras` 扩展名。 ”
    149 f“已接收：文件路径={文件路径}”
    150）
    151 用 open(filepath, “rb”) 作为 f：
--&gt; [第 152 章]
    153 f、custom_objects、编译、安全模式
    154）

文件c：\ Users \ kanch \ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ keras \ src \ saving \ saving_lib.py：207，在_load_model_from_fileobj（fileobj，custom_objects，compile，safe_mode）中
    204 asset_store.close（）
    206如果failed_trackables：
--&gt; 207 _raise_loading_failure（错误消息）
    208回归模型

文件c：\ Users \ kanch \ AppData \ Local \ Programs \ Python \ Python312 \ Lib \ site-packages \ keras \ src \ saving \ saving_lib.py：295，在_raise_loading_failure（error_msgs， warn_only）
    293 警告.warn(msg)
    294 其他：
--&gt;第295章

ValueError：总共无法加载 2 个对象。对象  的示例错误消息：

层“lstm_cell”需要 3 个变量，但在加载期间收到 0 个变量。预期：[&#39;kernel&#39;, &#39;recurrent_kernel&#39;, &#39;bias&#39;]

无法加载的对象列表：
[,]

这是我的 Final.ipynb 代码
&lt;前&gt;&lt;代码&gt;模型 = 顺序()
model.add(Conv3D(128, 3, input_shape=(75,46,140,​​1), padding=&#39;相同&#39;))
model.add(激活(&#39;relu&#39;))
model.add(MaxPool3D((1,2,2)))

model.add(Conv3D(256, 3, padding=&#39;相同&#39;))
model.add(激活(&#39;relu&#39;))
model.add(MaxPool3D((1,2,2)))

model.add(Conv3D(75, 3, padding=&#39;相同&#39;))
model.add(激活(&#39;relu&#39;))
model.add(MaxPool3D((1,2,2)))

model.add(TimeDistributed(Flatten()))

model.add(双向(LSTM(128, return_sequences=True)))
模型.add(Dropout(.5))

model.add(双向(LSTM(128, return_sequences=True)))
模型.add(Dropout(.5))

model.add(Dense(char_to_num.vocabulary_size()+1, kernel_initializer=&#39;he_normal&#39;, 激活=&#39;softmax&#39;))

model.compile(优化器=Adam(learning_rate=0.0001), 损失=CTCLoss)
model.load_weights(&#39;model_weights.weights.h5&#39;)
model.save(&#39;main4.keras&#39;)

这是我的 main.ipynb 代码
&lt;前&gt;&lt;代码&gt;@register_keras_serialized()
def CTCLoss(y_true, y_pred):
    batch_len = tf.cast(tf.shape(y_true)[0], dtype=“int64”)
    input_length = tf.cast(tf.shape(y_pred)[1], dtype=“int64”)
    label_length = tf.cast(tf.shape(y_true)[1], dtype=“int64”)

    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=“int64”)
    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=“int64”)

    损失 = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)
    回波损耗

模型 = keras.models.load_model(&#39;main4.keras&#39;)

我尝试阅读重新启动内核并从头开始运行的文档，删除模型并再次保存它，但对我来说没有任何作用。我希望我的模型能够加载到新笔记本中并给出预测]]></description>
      <guid>https://stackoverflow.com/questions/78382495/issues-while-loading-saved-deep-learning-model</guid>
      <pubDate>Thu, 25 Apr 2024 05:56:57 GMT</pubDate>
    </item>
    <item>
      <title>使用 softmax 回归批量梯度下降 - 卡住</title>
      <link>https://stackoverflow.com/questions/78382301/batch-gradient-descent-using-softmax-regression-stuck</link>
      <description><![CDATA[我已经用 softmax 回归编写了批量梯度下降，但是，结果总是 0 或 6。
我想我错过了一些东西，但不知道在哪里！
将 numpy 导入为 np

def softmax(z):
    总计 = sum(np.exp(x) for x in z)
    返回 np.array([(np.exp(k)/total) for k in z])


def one_hot_encoding(ys):
    如果 len(ys) == 0:
        返回 np.array([])
    k = np.max(ys)
    数组=[]
    对于 y 中的 y：
        行 = np.zeros(k + 1, dtype=int)
        行[y] = 1
        数组.追加（行）
    返回 np.array(array).reshape(len(ys),k+1)
        

def softmax_regression(xs, ys, 学习率, 迭代次数):
   
    tau = one_hot_encoding(ys)
    # 特征数量 = xs.shape[1]，类数量 = tau.shape[1]
    theta = np.zeros((xs.shape[1], tau.shape[1]))
    偏差 = np.zeros(tau.shape[1])

    对于 _ 在范围内（num_iterations）：
        总平均误差 = 0
        总梯度 = 0

        对于范围内的 i(xs.shape[0])：
            z = np.dot(xs[i], theta) + 偏差
            o = softmax(z)
            错误 = o - tau[i]
            Total_mean_error += np.mean(误差)
            总梯度 += (误差 * xs[i])
            
        θ += (学习率 * 总梯度) + (θ * 学习率)
        偏差 += 学习率 * 总平均误差

    打印（θ，偏差）
    def 模型（xs，theta=theta，偏差=偏差）：
        z = (theta.T * xs) + 偏差
        软=softmax(z)
        返回 soft.argmax()
    
    返回模型

            

＃测试

训练数据 = np.array([
    (0.17, 0),
    (0.79, 0),
    (2.66, 2),
    (2.81, 2),
    (1.58, 1),
    (1.86, 1),
    (2.97, 2),
    (2.70, 2),
    (1.64, 1),
    (1.68, 1)
]）

xs = Training_data[:,0].reshape((-1, 1)) # 一个 2D n×1 数组
ys = Training_data[:,1].astype(int) # 长度为 n 的一维数组

h = softmax_regression(xs, ys, 0.05, 750)

测试输入 = [(1.30, 1), (2.25, 2), (0.97, 0), (1.07, 1), (1.51, 1)]
print(f&quot;{&#39;预测&#39;:^10}{&#39;true&#39;:^10}&quot;)
对于 test_inputs 中的 x、y：
    print(f&quot;{h(x):^10}{y:^10}&quot;)
# 预测正确
#1 1
#2 2
# 0 0
#1 1
#1 1

将 theta 和偏差更新从加法切换为减法会导致结果始终为 6，反之亦然。
我已经通读了课程笔记和所有其他文档，但真的看不出它在哪里偏离]]></description>
      <guid>https://stackoverflow.com/questions/78382301/batch-gradient-descent-using-softmax-regression-stuck</guid>
      <pubDate>Thu, 25 Apr 2024 04:48:29 GMT</pubDate>
    </item>
    <item>
      <title>为什么批量求和和单次运行计算的杰卡德分数不同？</title>
      <link>https://stackoverflow.com/questions/78378954/why-does-jaccard-score-differ-between-batch-summation-and-single-run-calculation</link>
      <description><![CDATA[我在使用 jaccard_score 函数 (jaccard_score(ground_true, inference, average=&quot;micro&quot;, zero_division=0)) 时遇到了问题。
我没有足够的内存来存储我验证的所有 ground_truth 和推断。因此，我所做的是将每个批次的 Jaccard 分数相加到一个变量中，并在验证结束时，将此变量除以批次数。通过这种方法，我得到的结果为 0.8084487056909495。
但是，当我使用具有更大容量的计算机来处理数据并一次处理所有 ground_truth 和掩码时，我得到的值类似于 0.7716579100568796。有人能解释一下为什么会发生这种情况吗？]]></description>
      <guid>https://stackoverflow.com/questions/78378954/why-does-jaccard-score-differ-between-batch-summation-and-single-run-calculation</guid>
      <pubDate>Wed, 24 Apr 2024 13:49:22 GMT</pubDate>
    </item>
    <item>
      <title>为什么 scikit-learn 的 QDA 警告我“变量共线”——我该怎么办？</title>
      <link>https://stackoverflow.com/questions/78378679/why-is-qda-of-scikit-learn-warning-me-variables-are-collinear-what-can-i-do</link>
      <description><![CDATA[在训练中运行QuadraticDiscriminationAnalysis (QDA)和.fit，然后调用.predict来预测多项分类I我收到警告：
&lt;块引用&gt;
discriminant_analysis.py:935: UserWarning: 变量共线 warnings.warn(“变量共线”)

此后，程序不会崩溃，但会导致分类效果非常差，通常为 20%，而来自 scikit-learn 的许多其他分类器对同一数据集的准确率为 80% 到 90% ，包括LDA。
从本论坛之前对问题的回答中，我意识到，当它认为 X 矩阵是线性相关的时，即至少其中一个向量可以由其他向量的线性组合生成时，就会发生这样的警告。在这种情况下，算法的矩阵求逆所产生的误差很大 - 我假设这就是分类精度如此差的原因。
但是，我知道 X 矩阵不是线性相关的。所以我假设它失败可能是因为 X 矩阵接近线性相关，并且在这些条件下矩阵的求逆并不精确。
假设这就是问题所在，有没有更好的方法来得到X矩阵求逆，从而得到的误差更小，分类精度更高？
（而且由于 LDA 从相同的数据集（即 X 矩阵）中生成了良好的精度，因此此例程中的矩阵求逆很好 - 那么为什么它在 QDA 中失败？）
我正在使用 scikit-learn 版本 1.3.0、Python 3.8 和  Anaconda 包管理器。更改为 scikit-learn 1.2.0 会生成相同的警告。
我多次尝试运行该程序，但总是得到这个结果。]]></description>
      <guid>https://stackoverflow.com/questions/78378679/why-is-qda-of-scikit-learn-warning-me-variables-are-collinear-what-can-i-do</guid>
      <pubDate>Wed, 24 Apr 2024 13:04:56 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn import 无法在 python 中导入名称“METRIC_MAPPING64”</title>
      <link>https://stackoverflow.com/questions/78327535/scikit-learn-import-cannot-import-name-metric-mapping64-in-python</link>
      <description><![CDATA[我试图将 scikit-learn 中的线性模型导入到 vscode 中的 python 代码中，但收到意外的错误消息。
导入sklearn
从sklearn导入线性模型

错误：
无法从“sklearn.metrics._dist_metrics”导入名称“METRIC_MAPPING64”

我不想导入这些指标，如何解决这个问题？
使用的scikit-learn版本是1.1.3。]]></description>
      <guid>https://stackoverflow.com/questions/78327535/scikit-learn-import-cannot-import-name-metric-mapping64-in-python</guid>
      <pubDate>Mon, 15 Apr 2024 09:54:33 GMT</pubDate>
    </item>
    <item>
      <title>如何防止 Keras 在训练期间计算指标</title>
      <link>https://stackoverflow.com/questions/71412499/how-to-prevent-keras-from-computing-metrics-during-training</link>
      <description><![CDATA[我正在使用 Tensorflow/Keras 2.4.1，并且我有一个（无监督的）自定义指标，它将我的多个模型输入作为参数，例如：
model = build_model() # 返回一个 tf.keras.Model 对象
my_metric = custom_metric(model.output, model.input[0], model.input[1])
模型.add_metric(my_metric)
[...]
model.fit([...]) # 使用 fit 进行训练

但是，custom_metric 非常昂贵，因此我希望仅在验证期间计算它。我找到了这个答案，但我几乎不明白如何使解决方案适应我的指标，该指标使用多个模型输入作为参数，因为update_state 方法似乎不太灵活。
在我的上下文中，除了编写自己的训练循环之外，是否有办法避免在训练期间计算我的指标？
另外，我很惊讶我们无法本机指定 Tensorflow 某些指标只能在验证时计算，这有什么原因吗？
此外，由于模型经过训练来优化损失，并且训练数据集不应用于评估模型，我什至不明白为什么默认情况下 Tensorflow 在训练期间计算指标。]]></description>
      <guid>https://stackoverflow.com/questions/71412499/how-to-prevent-keras-from-computing-metrics-during-training</guid>
      <pubDate>Wed, 09 Mar 2022 16:11:26 GMT</pubDate>
    </item>
    <item>
      <title>TimeDistributed 层在 Keras 中的作用是什么？</title>
      <link>https://stackoverflow.com/questions/47305618/what-is-the-role-of-timedistributed-layer-in-keras</link>
      <description><![CDATA[我试图了解 TimeDistributed 包装器在 Keras 中的作用。
我知道 TimeDistributed“将一个层应用于输入的每个时间切片。”
但是我做了一些实验并得到了我无法理解的结果。
简而言之，对于 LSTM 层，TimeDistributed 和 Dense 层具有相同的结果。

&lt;前&gt;&lt;代码&gt;模型 = 顺序()
model.add(LSTM(5, input_shape = (10, 20), return_sequences = True))
model.add(TimeDistributed(密集(1)))
打印（模型.output_shape）

模型=顺序（）
model.add(LSTM(5, input_shape = (10, 20), return_sequences = True))
model.add((密集(1)))
打印（模型.output_shape）

对于这两个模型，我得到的输出形状为(None, 10, 1)。
谁能解释一下 RNN 层之后的 TimeDistributed 层和 Dense 层之间的区别吗？]]></description>
      <guid>https://stackoverflow.com/questions/47305618/what-is-the-role-of-timedistributed-layer-in-keras</guid>
      <pubDate>Wed, 15 Nov 2017 10:57:45 GMT</pubDate>
    </item>
    </channel>
</rss>