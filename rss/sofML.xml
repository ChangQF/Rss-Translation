<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 10 Jul 2024 15:18:29 GMT</lastBuildDate>
    <item>
      <title>Microsoft ML：预测代码始终预测 0，但在“评估”选项卡中尝试时，它始终有效（0 以外的值）</title>
      <link>https://stackoverflow.com/questions/78731177/microsoft-ml-the-prediction-code-always-predicts-0-but-when-tried-in-the-evalua</link>
      <description><![CDATA[系统信息：

模型构建器版本 17.15.0.2337001
Visual Studio 版本 2022

我创建了一个机器学习模型（值预测），一切都很好

通过这张图片，你可以看到模型正常工作
但是当我使用模型构建器的代码时：
使用 ConsoleApp2;

//加载样本数据
var sampleData = new MLModel1.ModelInput()
{
Device = 1F,
Temperature = 20F,
Weather = 1F,
Time = 2F,
};

//加载模型并预测输出
var result = MLModel1.Predict(sampleData);
Console.WriteLine(result.Predict);

输出始终为 0 或预测始终为 0：
]]></description>
      <guid>https://stackoverflow.com/questions/78731177/microsoft-ml-the-prediction-code-always-predicts-0-but-when-tried-in-the-evalua</guid>
      <pubDate>Wed, 10 Jul 2024 14:28:45 GMT</pubDate>
    </item>
    <item>
      <title>无法部署基于 Flask 的深度学习应用程序</title>
      <link>https://stackoverflow.com/questions/78731060/unable-to-deploy-my-deep-learning-app-based-on-flask</link>
      <description><![CDATA[我的 ml 应用程序基于 flask，它以 pdf 作为输入，让用户提问并利用 BERT 模型来回答，
到目前为止，它在本地运行良好，但在部署中没有运气，我尝试了 vercel、pythonanywhere，但没有任何效果，欢迎您为部署做出贡献
github repo https://github.com/MohdSiddiq12/Natural-Language-Question-Answering-System]]></description>
      <guid>https://stackoverflow.com/questions/78731060/unable-to-deploy-my-deep-learning-app-based-on-flask</guid>
      <pubDate>Wed, 10 Jul 2024 14:04:16 GMT</pubDate>
    </item>
    <item>
      <title>我被困在项目的这个阶段，我已经安装了 catboost 模块，但它仍然显示错误</title>
      <link>https://stackoverflow.com/questions/78730862/i-am-stuck-in-this-stage-of-my-project-where-i-already-install-the-catboost-modu</link>
      <description><![CDATA[
ValueError Traceback（最近一次调用最后一次）
Cell In[65]，第 2 行
1 # catboost 分类器模型
----&gt; 2 from catboost import CatBoostClassifier
4 # 实例化模型
5 cat = CatBoostClassifier(learning_rate = 0.1, verbose=0)
File c:\Users\Asus\AppData\Local\Programs\Python\Python312\Lib\site-packages\catboost_init_.py:1
----&gt; 1 从 .core 导入 (
2 FeaturesData、EFstrType、EShapCalcType、EFeaturesSelectionAlgorithm、EFeaturesSelectionGrouping、
3 Pool、CatBoost、CatBoostClassifier、CatBoostRegressor、CatBoostRanker、CatBoostError、cv、sample_gaussian_process、train、
4 sum_models、_have_equal_features、to_regressor、to_classifier、to_ranker、MultiRegressionCustomMetric、
5 MultiRegressionCustomObjective、MultiTargetCustomMetric、MultiTargetCustomObjective
6 ) # noqa
7 从 .version 导入 VERSION 作为 version # noqa
8 all = [
9 &#39;FeaturesData&#39;、&#39;EFstrType&#39;、&#39;EShapCalcType&#39;、&#39;EFeaturesSelectionAlgorithm&#39;， &#39;EFeaturesSelectionGrouping&#39;,
10 &#39;Pool&#39;, &#39;CatBoost&#39;, &#39;CatBoostClassifier&#39;, &#39;CatBoostRegressor&#39;, &#39;CatBoostRanker&#39;, &#39;CatboostError&#39;,
(...)
13 &#39;MultiTargetCustomMetric&#39;, &#39;MultiTargetCustomObjective&#39;
14 ]
文件 c:\Users\Asus\AppData\Local\Programs\Python\Python312\Lib\site-packages\catboost\core.py:45
40 pass
...
9 def try_plot_offline(figs):
文件 _catboost.pyx:1，在 init _catboost() 中
ValueError：numpy.dtype 大小已更改，可能表示二进制不兼容。预期 C 标头为 96，PyObject 为 88
输出被截断。以可滚动元素形式查看或在文本编辑器中打开。调整单元格输出设置...
catboost 分类器模型
来自 catboost 导入 CatBoostClassifier
实例化模型
cat = CatBoostClassifier(learning_rate = 0.1, verbose=0)
拟合模型
cat.fit(X_train,y_train)]]></description>
      <guid>https://stackoverflow.com/questions/78730862/i-am-stuck-in-this-stage-of-my-project-where-i-already-install-the-catboost-modu</guid>
      <pubDate>Wed, 10 Jul 2024 13:27:16 GMT</pubDate>
    </item>
    <item>
      <title>训练 PINN 来反演未知参数</title>
      <link>https://stackoverflow.com/questions/78730829/train-a-pinn-to-invert-for-unknown-parameters</link>
      <description><![CDATA[我使用 PINN 求解阻尼振荡器微分方程，同时以阻尼振荡器的噪声观测作为输入，找到后者的摩擦参数。我使用自定义训练程序在 Tensorflow 中编写了代码。问题是我定义的可训练参数没有接近我从噪声观测中知道的正确值。最终，PINN 的解决方案完全不正确。但是，我的代码运行得很好，不需要寻找可训练参数，也就是这里的摩擦参数。
# NN 振荡器系统的实现
def rocks_system_data_loss(t, net, func, params, mu, bc, t_data, u_data, lambda1):
t = t.reshape(-1,1)
t = tf.constant(t, dtype = tf.float32)
t_0 = tf.zeros((1,1))

# 2nd 导数的嵌套循环
with tf.GradientTape() as outer_tape:
outer_tape.watch(t)

with tf.GradientTape() as inner_tape:
inner_tape.watch(t)
x = net(t)

dx_dt = inner_tape.gradient(x, t) # 1st导数

d2x_dt2 = outer_tape.gradient(dx_dt, t) # 二阶导数

# 边界损失
bc_loss_1 = tf.square(net(t_0) - bc[0])
bc_loss_2 = tf.square(dx_dt[0] - bc[1])

# 可学习参数 mu 传递给 ODE
ode_loss = d2x_dt2 - func(x, dx_dt, params[0], mu, params[2])

# 超参数 lambda1 的数据损失
data_loss = u_data - net(t_data)

square_loss = tf.square(ode_loss) + lambda1*tf.square(data_loss) + bc_loss_1 + bc_loss_2
total_loss = tf.reduce_mean(square_loss)

return total_loss, mu

# 带有数据丢失的训练程序
def train_NN_data_loss(epochs, optm, NN, func, bc, lambda1, train_t, train_u, data_t, data_u,
data_u_noised, test_t_plot, true_u_plot, testing_t):
train_loss_record = []
loss_tracker = plotting_points(epochs)

mu = tf.Variable(initial_value=tf.ones((1,1)), trainable=True, dtype=tf.float32)
mu_list = []

waiting = 200
best = float(&#39;inf&#39;)

early_stop = 0

for itr in range(epochs):
with tf.GradientTape() as tape:
#tape.watch(mu)
train_loss, mu = rocksor_system_data_loss(train_t, NN，func，params，mu，bc，data_t，data_u_noised，lambda1)
train_loss_record.append(train_loss)

grad_w = tape.gradient(train_loss，NN.trainable_variables + [mu])
optm.apply_gradients(zip(grad_w，NN.trainable_variables + [mu]))

if itr in loss_tracker:
print(train_loss.numpy())
print(mu.numpy())
plot_epochs_with_noise(train_t，train_u，data_t，data_u_noised，test_t_plot，true_u_plot，testing_t，itr，NN)

mu_list.append(mu.numpy()) 

# 提前停止
# wait += 1
# if train_loss.numpy() &lt; best:
# best = train_loss.numpy()
# wait = 0
# if wait &gt;= waiting:
# print(f&quot;在迭代 {itr} 时停止，损失为 {train_loss_record[itr]}。&quot;)
# early_stop = itr
# break

return train_loss_record, mu_list, early_stop

# 用于 ODE 损失计算/最小化
NN_osc_func = lambda x, dx_dt, k, d, m: -k/m*x - d/m*dx_dt

您可以在此处看到 6000 个 epoch 后的结果。神经网络正在收敛到一条水平线，误差为 5.76，参数估计为 0.84，尽管正确值为 4。这是我的阻尼振荡器设置：
k = 400
d = 4
m = 1
y0 = np.array([1.0, 0.0])

错误结果。
相应损失。
不幸的是，此时我不知道问题可能是什么。我尝试更改 NN_osc_func，并在两个函数中使用了 tape.gradient()。如能得到任何帮助，我将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78730829/train-a-pinn-to-invert-for-unknown-parameters</guid>
      <pubDate>Wed, 10 Jul 2024 13:22:04 GMT</pubDate>
    </item>
    <item>
      <title>在 Android Studio 中集成已训练的模型（Java）</title>
      <link>https://stackoverflow.com/questions/78730286/integration-of-a-trained-model-in-android-studio-java</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78730286/integration-of-a-trained-model-in-android-studio-java</guid>
      <pubDate>Wed, 10 Jul 2024 11:31:14 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 分类器，我可以将模型传递给自定义损失吗？</title>
      <link>https://stackoverflow.com/questions/78729235/xgboost-classifier-can-i-pass-the-model-to-the-custom-loss</link>
      <description><![CDATA[我有一个特殊的二元分类用例，根据模型的决策，下一个评估数据会发生变化。
示例：
[[x0,y0], [x1,y1], [x2,y2]...]

如果模型预测 x0 为 1，则下一个评估点为 [x1,y1]。

如果模型预测 x0 为 0，则下一个评估点为 [x2,y2]。

起初，我以为我会在所有点上训练模型，而且在评估取决于先前预测的最终场景中，它会很好，但事实并非如此。
我开发了一个函数，以我解释的相互依赖方式计算评估函数，并发现当整个点集的损失函数改进时，它并没有改进。
即使我在同一组数据上进行训练和验证，它也没有改善。
因此，我想修改损失函数，使其使用模型选择用于计算损失的训练点子集，具体取决于模型的当前状态（在每个提升步骤中）。
我相信这在理论上应该是可能的。我的问题是：
你也认为这是可能的吗？
如果可能，我该如何将模型传递给自定义损失？
提前谢谢您！]]></description>
      <guid>https://stackoverflow.com/questions/78729235/xgboost-classifier-can-i-pass-the-model-to-the-custom-loss</guid>
      <pubDate>Wed, 10 Jul 2024 07:49:45 GMT</pubDate>
    </item>
    <item>
      <title>加载在自定义数据集上训练的 Yolov9 模型：AttributeError：“str”对象没有属性“shape”</title>
      <link>https://stackoverflow.com/questions/78728869/loading-yolov9-model-trained-on-custom-dataset-attributeerror-str-object-has</link>
      <description><![CDATA[我已经在自定义数据集上训练了一个 yolov9 模型，用于实例分割，现在我想在分割后获得分割区域。
输出如下图所示，但针对图像中分割的每个对象。

from pathlib import Path
import numpy as np
import torch
import cv2

model = torch.hub.load(&#39;.&#39;, &#39;custom&#39;, path=&#39;yolov9-inst/runs/train-seg/gelan-c-seg15/weights/best.pt&#39;, source=&#39;local&#39;) 
# Image
img = &#39;WALL-INSTANCEE-2/test/images/5a243513a69b150001f56c31_emptyroom6_jpeg_jpg.rf.7aa8f6a9aefbb1c76adc60a7b392dcd6.jpg&#39;
# 推理
res = model(img)

# 迭代检测结果（对多张图片有帮助）
for r in res:
img = np.copy(r.orig_img)
img_name = Path(r.path).stem # 源图片 base-name

# 迭代每个对象轮廓（多次检测）
for ci, c in enumerate(r):
# 获取检测类名
label = c.names[c.boxes.cls.tolist().pop()]

# 创建二进制掩码
b_mask = np.zeros(img.shape[:2], np.uint8)

# 提取轮廓结果
contour = c.masks.xy.pop()
# 更改类型
contour = contour.astype(np.int32)
# 重塑
contour = contour.reshape(-1, 1, 2)

# 将轮廓绘制到掩码上
_ = cv2.drawContours(b_mask, [contour], -1, (255, 255, 255), cv2.FILLED)

但我在仅查找 res 时收到此错误。
YOLO 🚀 v0.1-104-g5b1ea9a Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (NVIDIA RTX A5000, 24248MiB)

融合层...
gelan-c-seg-custom 摘要：414 层、27364441 个参数、0 个梯度、144.2 GFLOP
警告 ⚠️ YOLO SegmentationModel 尚不兼容 AutoShape。您将无法使用此模型运行推理。
------------------------------------------------------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
Cell In[84]，第 6 行
4 img = &#39;WALL-INSTANCEE-2/test/images/5a243513a69b150001f56c31_emptyroom6_jpeg_jpg.rf.7aa8f6a9aefbb1c76adc60a7b392dcd6.jpg&#39;
5 # 推理
----&gt; 6 结果 = model(img)

文件 /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518，位于 Module._wrapped_call_impl(self, *args, **kwargs)
1516 返回 self._compiled_call_impl(*args, **kwargs) # 类型：ignore[misc]
1517 else:
-&gt; 1518 return self._call_impl(*args, **kwargs)

File /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
1522 # 如果我们没有任何钩子，我们希望跳过此函数中的其余逻辑
1523 # 并只调用 forward。
1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
1525 or _global_backward_pre_hooks or _global_backward_hooks
1526 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1527 返回 forward_call(*args, **kwargs)
1529 尝试：
1530 结果 = 无

文件 /workspace/yolov9-inst/./models/common.py:868，在 DetectMultiBackend.forward(self, im, augment, visualize) 中
866 def forward(self, im, augment=False, visualize=False):
867 # YOLO MultiBackend 推理
--&gt; 868 b, ch, h, w = im.shape # 批次、通道、高度、宽度
869 if self.fp16 and im.dtype != torch.float16:
870 im = im.half() # 到 FP16

AttributeError: &#39;str&#39; 对象没有属性 &#39;shape&#39;

请问有人能帮我解决这个问题吗]]></description>
      <guid>https://stackoverflow.com/questions/78728869/loading-yolov9-model-trained-on-custom-dataset-attributeerror-str-object-has</guid>
      <pubDate>Wed, 10 Jul 2024 06:10:32 GMT</pubDate>
    </item>
    <item>
      <title>使用 yolov8 在组织内通过多个摄像头对员工进行 reiID 和跟踪 [关闭]</title>
      <link>https://stackoverflow.com/questions/78728683/reiid-and-track-employees-across-multiple-cameras-within-the-organization-with-y</link>
      <description><![CDATA[识别、重新识别并跟踪人员在组织摄像头中的路线。
是否可以使用 Yolo 重新识别并绘制任何在组织摄像头中看到的人的路线？在官方文件中，我找不到任何合适的答案。
PS：这些摄像头很旧，分辨率只有 4 兆像素，所以我不想通过他们的脸来跟踪他们，而是通过他们的西装来跟踪他们。]]></description>
      <guid>https://stackoverflow.com/questions/78728683/reiid-and-track-employees-across-multiple-cameras-within-the-organization-with-y</guid>
      <pubDate>Wed, 10 Jul 2024 05:05:10 GMT</pubDate>
    </item>
    <item>
      <title>从示例数据集重新创建文本嵌入</title>
      <link>https://stackoverflow.com/questions/78728307/recreating-text-embeddings-from-an-example-dataset</link>
      <description><![CDATA[我现在的情况是，我有一个句子列表，以及一个 25 维向量上的理想嵌入列表。我试图使用神经网络来生成新的编码，但很挣扎。虽然模型运行良好，但其输出毫无意义，甚至无法准确复制训练数据！
import numpy as np
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 标记化
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentence_list)
sequences = tokenizer.texts_to_sequences(sentence_list)

# 假设您的向量是 25 维
input_dim = 25

# 定义编码器
input_vec = Input(shape=(max_sequence_length,))
encoded = Dense(25,activation=&#39;tanh&#39;)(input_vec) # 缩减至 16 维的示例
encoder = Model(input_vec,coded)

# 定义解码器
decoded = Dense(input_dim,activation=&#39;sigmoid&#39;)(encoded)
autoencoder = Model(input_vec,coded)

# 编译模型
autoencoder.compile(optimizer=Adam(),loss=&#39;mse&#39;)

# 训练模型
autoencoder.fit(padded_sequences, combined_vectors_clean,
epochs=10,
batch_size=32,
shuffle=True,validation_split= 0.2)


据我所知，我的输入和标签没有任何问题，那么我遗漏了什么？非常感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78728307/recreating-text-embeddings-from-an-example-dataset</guid>
      <pubDate>Wed, 10 Jul 2024 01:39:37 GMT</pubDate>
    </item>
    <item>
      <title>Azure Custom Vision 从网络摄像头获取图像，然后返回图像</title>
      <link>https://stackoverflow.com/questions/78727554/azure-custom-vision-to-get-an-image-from-webcam-and-then-return-an-image</link>
      <description><![CDATA[我希望创建一项服务，将网络摄像头中的图像发送到 Azure Custom Vision，并让其返回它认为匹配的图像。这可以用于纸牌游戏，因此如果您在网络摄像头上显示黑桃 A，它将能够返回黑桃 A 的图像。这是否适合 Azure Custom Vision？我创建了一个项目并上传和标记了图像，但我还没有看到这样的用例。]]></description>
      <guid>https://stackoverflow.com/questions/78727554/azure-custom-vision-to-get-an-image-from-webcam-and-then-return-an-image</guid>
      <pubDate>Tue, 09 Jul 2024 19:57:34 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：未在未知的 TensorShape 上定义 as_list()。图像和掩码形状看起来正确</title>
      <link>https://stackoverflow.com/questions/78727412/valueerror-as-list-is-not-defined-on-an-unknown-tensorshape-image-and-mask-s</link>
      <description><![CDATA[我正尝试调整 Tensorflow 的示例 UNet 以达到我的目的。主要区别在于，此 UNet 采用 128x128 图像和掩码，而我的图像为 512x512，掩码为 100x100。
尝试运行 model.fit 时出现此错误：
-------------------------------------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
Cell In[137]，第 2 行
1 # %%wandb
----&gt; 2 model_history = model.fit(train_dataset, epochs=EPOCHS,
3 steps_per_epoch=STEPS_PER_EPOCH,
4 validation_steps=VALIDATION_STEPS,
5 validation_data=test_dataset,
6 callbacks = [ShapeLoggingCallback()])
7 #callbacks=[CustomCallback()]) # original: callbacks=[DisplayCallback()])

文件 /opt/conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

文件 /opt/conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122，在 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 中引发 e.with_traceback(filtered_tb)
123 finally:
124 delfiltered_tb

ValueError：as_list() 未在未知的 TensorShape 上定义。

但是，我可以毫无问题地运行 model.predict，它会生成我期望的未经训练的模型的预测。
这是我用来制作和训练模型的代码：
base_model = tf.keras.applications.MobileNetV2(input_shape=[512, 512, 3], include_top=False)

# 使用这些层的激活
layer_names = [
&#39;block_1_expand_relu&#39;, # 64x64
&#39;block_3_expand_relu&#39;, # 32x32
&#39;block_6_expand_relu&#39;, # 16x16
&#39;block_13_expand_relu&#39;, # 8x8
&#39;block_16_project&#39;, # 4x4
]
base_model_outputs = [base_model.get_layer(name).output for name in layer_names]

# 创建特征提取模型
down_stack = tf.keras.Model(inputs=base_model.input, output=base_model_outputs)

down_stack.trainable = False

up_stack = [
pix2pix.upsample(512, 3), # 4x4 -&gt; 8x8
pix2pix.upsample(256, 3), # 8x8 -&gt; 16x16
pix2pix.upsample(128, 3), # 16x16 -&gt; 32x32
pix2pix.upsample(64, 3), # 32x32 -&gt; 64x64
]

def unet_model(output_channels:int):
input = tf.keras.layers.Input(shape=[512, 512, 3])

# 通过模型进行下采样
skips = down_stack(inputs)
x = skips[-1]
skips = reversed(skips[:-1])

# 上采样并建立 skip 连接
for up, skip in zip(up_stack, skips):
x = up(x)
concat = tf.keras.layers.Concatenate()
x = concat([x, skip])

# 这是模型的最后一层
last = tf.keras.layers.Conv2DTranspose(
filters=output_channels, kernel_size=3, strides=2,
padding=&#39;same&#39;) #64x64 -&gt; 128x128

x = last(x)

return tf.keras.Model(inputs=inputs, output=x)

OUTPUT_CLASSES = 5

model = unet_model(output_channels=OUTPUT_CLASSES)
model.compile(optimizer=&#39;adam&#39;,
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
metrics=[&#39;accuracy&#39;])

model_history = model.fit(train_dataset, epochs=EPOCHS,
validation_data=test_dataset)

我尝试检查每个批次的图像形状和掩码形状。除了最后一个批次（只有一个图像）外，每个批次的图像形状为 (16, 512, 512, 3)，掩码形状为 (16, 100, 100, 1)。
我尝试将此代码放入我的 process_paths（教程中称之为）函数中：
image = tf.reshape(image, [512, 512, 3])

...
mask = tf.reshape(mask, [100, 100, 1])

我尝试对 up_stack 部分中的数字进行一些调整，但最终一无所获，因为我不理解那部分。我的假设是，既然我已经更改了输入大小，我必须更改模型层的输出大小，但我真的不知道该怎么做。另外，我很困惑，如果是这样的话，为什么我仍然可以运行 model.predict。
我的 tensorflow 版本是 2.16.1]]></description>
      <guid>https://stackoverflow.com/questions/78727412/valueerror-as-list-is-not-defined-on-an-unknown-tensorshape-image-and-mask-s</guid>
      <pubDate>Tue, 09 Jul 2024 19:17:45 GMT</pubDate>
    </item>
    <item>
      <title>如何使用我的代码可视化预测样本需要更多答案</title>
      <link>https://stackoverflow.com/questions/78719068/how-to-visualize-predicted-samples-using-my-code-need-more-answers</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78719068/how-to-visualize-predicted-samples-using-my-code-need-more-answers</guid>
      <pubDate>Mon, 08 Jul 2024 04:07:24 GMT</pubDate>
    </item>
    <item>
      <title>使用嵌入技术从数据库中进行人脸识别</title>
      <link>https://stackoverflow.com/questions/78688976/face-recognize-from-the-database-using-embedding-technique</link>
      <description><![CDATA[我正在开展一个项目，旨在识别大学记录中是否存在任何个人的照片。所提出的方法涉及将每个学生照片的嵌入及其详细信息存储在矢量数据库中。当需要比较照片时，系统将生成该照片的嵌入值，然后将该值与数据库进行比较。如果该值在特定阈值内，则表明该个人存在于记录中。
我正在寻求专家建议，以确定这种方法是否可行。如果对此方法有任何疑虑，我将不胜感激最佳解决方案的建议。]]></description>
      <guid>https://stackoverflow.com/questions/78688976/face-recognize-from-the-database-using-embedding-technique</guid>
      <pubDate>Sun, 30 Jun 2024 15:09:55 GMT</pubDate>
    </item>
    <item>
      <title>在 scikit learn 中实现自定义损失函数</title>
      <link>https://stackoverflow.com/questions/54267745/implementing-custom-loss-function-in-scikit-learn</link>
      <description><![CDATA[我想在 scikit learn 中实现自定义损失函数。我使用以下代码片段：
def my_custom_loss_func(y_true,y_pred):
diff3=max((abs(y_true-y_pred))*y_true)
return diff3

score=make_scorer(my_custom_loss_func,greater_ is_better=False)
clf=RandomForestRegressor()
mnn= GridSearchCV(clf,score)
knn = mnn.fit(feam,labm) 

传递给 my_custom_loss_func 的参数应该是什么？我的标签矩阵称为 labm。我想计算实际输出与预测输出（由模型）乘以真实输出之间的差值。如果我使用 labm 代替 y_true，那么我应该使用什么代替 y_pred？]]></description>
      <guid>https://stackoverflow.com/questions/54267745/implementing-custom-loss-function-in-scikit-learn</guid>
      <pubDate>Sat, 19 Jan 2019 13:47:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么 CPU 上的 Keras LSTM 比 GPU 快三倍？</title>
      <link>https://stackoverflow.com/questions/52481006/why-is-keras-lstm-on-cpu-three-times-faster-than-gpu</link>
      <description><![CDATA[我使用Kaggle 的这个笔记本来运行 LSTM 神经网络。
我已经开始训练神经网络，我发现它太慢了。它比 CPU 训练慢了近三倍。

CPU 性能：每轮 8 分钟；
GPU 性能：每轮 26 分钟。

之后我决定在 Stackoverflow 上的这个问题 中寻找答案，并且我应用了 CuDNNLSTM （仅在 GPU 上运行） 而不是 LSTM。 
因此，GPU 性能变为每 epoch 仅 1 分钟，模型准确率下降 3%。
问题： 
1) 有人知道为什么 GPU 在经典 LSTM 层中比 CPU 运行得慢吗？我不明白为什么会发生这种情况。
2) 为什么当我使用 CuDNNLSTM 而不是 LSTM 时，训练变得更快，模型准确率下降？
附注：
我的 CPU： Intel Core i7-7700 处理器（8M 缓存，最高 4.20 GHz）
我的 GPU： nVidia GeForce GTX 1050 Ti（4 GB）]]></description>
      <guid>https://stackoverflow.com/questions/52481006/why-is-keras-lstm-on-cpu-three-times-faster-than-gpu</guid>
      <pubDate>Mon, 24 Sep 2018 13:56:09 GMT</pubDate>
    </item>
    </channel>
</rss>