<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 29 Jun 2024 21:14:06 GMT</lastBuildDate>
    <item>
      <title>使用类似数据集 (slack) 的消息回复对 llama3 进行微调</title>
      <link>https://stackoverflow.com/questions/78687005/fine-tune-llama3-with-message-replies-like-dataset-slack</link>
      <description><![CDATA[我想在一个数据集上微调 llama3，其中数据结构是考虑以下规则的消息列表：

有频道。
每个频道都有来自各种用户的消息。
每条消息可能都有与其上下文相对应的回复。

我已经有了抓取所有数据的逻辑，但我对数据集结构应该是什么样子有点困惑。
我读过 llama3 文档，看起来应该应用下面的模板（示例取自：https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/):
&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;

您是一位乐于助人的 AI 助手，可提供旅行提示和建议&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;

您能帮我什么忙？&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;

假设每个单独的消息/重放如下所示：
&lt;timestamp&gt; - &lt;user&gt;: &lt;message&gt;

数据的最终结果应该是什么样的？
它是一个字典列表吗？如果是，那么回复应该如何放置？
我敢于问 GPT4o，它给了我以下示例：
prompt_example_1 = [
{&quot;role&quot;: &quot;system&quot;, &quot;message&quot;: &quot;Channel: general&quot;},
{&quot;role&quot;: &quot;user&quot;, &quot;message&quot;: &quot;U12345678 [2023-06-01 12:00:00]: 频道中的主要消息&quot;},
{&quot;role&quot;: &quot;assistant&quot;, &quot;message&quot;: &quot;U87654321 [2023-06-01 12:05:00]: 回复主要消息消息”},
{“role”: “assistant”, “message”: “U23456789 [2023-06-01 12:10:00]: 对主消息的另一条回复”},
{“role”: “user”, “message”: “U23456789 [2023-06-01 12:15:00]: 另一条主消息”},
{“role”: “assistant”, “message”: “U34567890 [2023-06-01 12:20:00]: 对第二条主消息的回复”}
]

出于某种原因我觉得不对劲。
如果我打乱数据集会发生什么？所有回复都将失去与其父消息的关联。
我很高兴得到一些澄清。]]></description>
      <guid>https://stackoverflow.com/questions/78687005/fine-tune-llama3-with-message-replies-like-dataset-slack</guid>
      <pubDate>Sat, 29 Jun 2024 20:35:49 GMT</pubDate>
    </item>
    <item>
      <title>根据数据序列标记评分标准的正确机器学习方法是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/78686788/whats-the-right-machine-learning-approach-to-mark-rubrics-based-on-sequences-of</link>
      <description><![CDATA[我是一名教师，我正在做一个业余项目，帮助简化一些针对学生的评估工作流程。其中一个工作流程是以如下所示的评分标准的形式收集学生进度数据：

行是我们正在涵盖的特定结果（在本例中，读时钟和使用时间单位），列是学生能够完成的问题/任务（简单、中等和具有挑战性的任务）。在特定单元中，我会记录学生每次尝试问题/任务以及他们的表现（如果他们答对了就打勾，如果他们和小组一起答对了就打 G，如果他们需要帮助就打 A，如果他们答错了就打 X，等等）。在单元结束时，我会查看所有行并选择他们能够回答的最高级别的问题，这将转化为他们的成绩。换句话说，对于每一行，我根据每个单元格中的数据选择一列。序列中后面的数据具有更高的优先级，因此早期的一堆错误答案不一定比单元后面的正确答案更重要。
我想使用某种 ML 模型来根据每列中存在的数据预测每行将选择哪一列。行是相互独立评估的。我正在使用 Swift 在 iOS 和 macOS 上开发此应用程序，但我对 ML 世界还很陌生。我无法找到让 Create ML 做我想做的事情的方法，但任何能为我指明正确方向的想法都将不胜感激！我还没有完全掌握 Swift，所以如果我需要使用 python 或其他语言来创建模型，那也没问题，只要它可以集成到 swift 应用程序中即可。我的训练数据是来自许多评分标准的一组行，其中行中的每个单元格都有一个字母与我在评分标准上使用的符号相关联，并且有一列表示基于数据的正确单元格。]]></description>
      <guid>https://stackoverflow.com/questions/78686788/whats-the-right-machine-learning-approach-to-mark-rubrics-based-on-sequences-of</guid>
      <pubDate>Sat, 29 Jun 2024 18:33:17 GMT</pubDate>
    </item>
    <item>
      <title>如何调整 keras 模型的输出</title>
      <link>https://stackoverflow.com/questions/78686718/how-to-adapt-output-of-a-keras-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78686718/how-to-adapt-output-of-a-keras-model</guid>
      <pubDate>Sat, 29 Jun 2024 17:56:20 GMT</pubDate>
    </item>
    <item>
      <title>如何缩小不同机器学习模型的训练和测试分数之间的差距？</title>
      <link>https://stackoverflow.com/questions/78686637/how-to-reduce-gap-between-train-and-test-scores-for-different-machine-learning-m</link>
      <description><![CDATA[我正在使用多个机器学习模型进行 AQI 预测。数据为每日格式，共有 1850 条记录。我的训练 R2 得分约为 99，测试得分约为 91。这个差距可以接受吗？如果没有，我该如何提高我的测试分数？
X = data[[&#39;Year&#39;, &#39;Month&#39;, &#39;Day&#39;, &#39;Raw Conc.&#39;, &#39;NowCast Conc.&#39;]]
y = data[&#39;AQI&#39;]

# 使用时间序列分割将数据拆分为训练集和测试集
tscv = TimeSeriesSplit(n_splits=2) 

for train_index, test_index in tscv.split(X):
X_train, X_test = X.iloc[train_index], X.iloc[test_index]
y_train, y_test = y.iloc[train_index], y.iloc[test_index]

# 标准化特征
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 定义参数每个模型的网格
param_grids = {
“决策树”：{&#39;max_depth&#39;：[3, 5, 7, 10]},
“随机森林”：{&#39;n_estimators&#39;：[50, 100, 200], &#39;max_depth&#39;：[3, 5, 7, 10]},
“梯度提升”：{&#39;n_estimators&#39;：[50, 100, 200], &#39;learning_rate&#39;：[0.01, 0.1, 0.2], &#39;max_depth&#39;：[3, 5, 7]},
“AdaBoost”：{&#39;n_estimators&#39;：[50, 100, 200], &#39;learning_rate&#39;：[0.01, 0.1, 0.5]},
“XGBoost”: {&#39;n_estimators&#39;: [50, 100, 200], &#39;learning_rate&#39;: [0.01, 0.1, 0.2], &#39;max_depth&#39;: [3, 5, 7]},
“CatBoost”: {&#39;iterations&#39;: [50, 100, 200], &#39;learning_rate&#39;: [0.01, 0.1, 0.2], &#39;depth&#39;: [3, 5, 7]}, 0.7]},
}

# 要评估的模型列表
models = [
(“决策树”, DecisionTreeRegressor(random_state=42)),
(“随机森林”, RandomForestRegressor(random_state=42)),
(&quot;Gradient Boosting&quot;, GradientBoostingRegressor(random_state=42)),
(&quot;AdaBoost&quot;, AdaBoostRegressor(random_state=42)),
(&quot;XGBoost&quot;, XGBRegressor(random_state=42)),
(&quot;CatBoost&quot;, CatBoostRegressor(verbose=0)),
]

#用于存储模型性能和特征重要性的字典
model_performance = {}
feature_importance_dict = {}
predictions = {}

for name, model in models:
param_grid = param_grids[name]

if param_grid:
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3,scoring=&#39;neg_mean_squared_error&#39;)
grid_search.fit(X_train_scaled, y_train)
best_model = grid_search.best_estimator_
else:
best_model = model
best_model.fit(X_train_scaled, y_train)

# 计算预测
y_train_pred = best_model.predict(X_train_scaled)
y_test_pred = best_model.predict(X_test_scaled)

# 存储预测
predictions[name] = {&#39;model_name&#39;: name, &#39;y_test_pred&#39;: y_test_pred}

# 计算训练集的评估指标
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
train_r2 = r2_score(y_train, y_train_pred)
train_mae = mean_absolute_error(y_train, y_train_pred)

# 计算测试集的评估指标
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_r2 = r2_score(y_test, y_test_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)

# 存储模型性能指标
model_performance[name] = {
&quot;Train_RMSE&quot;: train_rmse, 
&quot;Train_R2&quot;: train_r2, 
&quot;Train_MAE&quot;: train_mae,
&quot;Test_RMSE&quot;: test_rmse,
&quot;Test_R2&quot;: test_r2,
&quot;Test_MAE&quot;: test_mae
}

# 对于所有模型，尝试提取特征重要性
if hasattr(best_model, &#39;feature_importances_&#39;) or hasattr(best_model, &#39;coef_&#39;):
feature_importances = best_model.feature_importances_ if hasattr(best_model, &#39;feature_importances_&#39;) else best_model.coef_

# 获取特征名称
if isinstance(best_model, (LinearRegression, Ridge, Lasso)): # 对于线性模型
feature_names = [&#39;Raw Conc.&#39;, &#39;NowCast Conc.&#39;]
else: # 对于其他模型，从原始 DataFrame 获取特征名称
feature_names = [&#39;Raw Conc.&#39;, &#39;NowCast Conc.&#39;] # 将其替换为实际特征名称

# 使用特征名称存储特征重要性
feature_importance_dict[name] = {feature_names[i]: feature_importances[i] for i in range(min(len(feature_importances), len(feature_names)))}

# 将模型性能字典转换为 DataFrame
model_performance_df = pd.DataFrame.from_dict(model_performance, orient=&#39;index&#39;)

# 打印模型性能
print(model_performance_df)

我在这里减少了分割 (tscv = TimeSeriesSplit(n_splits=2) )，我的测试分数从 91 提高到了 94。我还能做什么？]]></description>
      <guid>https://stackoverflow.com/questions/78686637/how-to-reduce-gap-between-train-and-test-scores-for-different-machine-learning-m</guid>
      <pubDate>Sat, 29 Jun 2024 17:10:14 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用免费资源在本地机器上的大数据集上训练 gpt2 [关闭]</title>
      <link>https://stackoverflow.com/questions/78686632/trying-to-train-gpt2-on-large-dataset-in-local-machine-with-free-resources</link>
      <description><![CDATA[是否可以在 colab、jupyter 或 kaggle 上对 1.5m 个数据点进行 gpt2 训练？
到目前为止，我尝试在 colab 中进行此操作，但在标记化过程中会耗尽存储空间，这是可以理解的。我也尝试了批处理技术。后来我尝试在 kaggle 上运行相同的算法，但目前它在加载转换器时显示错误。仍在尝试运行它。我只是想知道是否可以做到这一点！]]></description>
      <guid>https://stackoverflow.com/questions/78686632/trying-to-train-gpt2-on-large-dataset-in-local-machine-with-free-resources</guid>
      <pubDate>Sat, 29 Jun 2024 17:07:55 GMT</pubDate>
    </item>
    <item>
      <title>为什么循环中classification_report和precision_recall_fscore_support之间的敏感度（召回率）值不同？</title>
      <link>https://stackoverflow.com/questions/78686328/why-do-the-sensitivity-recall-values-differ-between-classification-report-and</link>
      <description><![CDATA[标题：
为什么循环中 classification_report 和 precision_recall_fscore_support 之间的敏感度（召回率）值不同？
问题：
我正在使用一个合成数据集，该数据集使用 sklearn.datasets 中的 make_classification 生成，包含 5 个类别。我已经在此数据上训练了一个 RandomForestClassifier，并使用两种不同的方法评估其性能。但是，我观察到这两种方法的敏感度（召回率）值存在差异。
这是我使用的代码：
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classes_report, precision_recall_fscore_support
import numpy as np
import pandas as pd

# 生成包含 5 个类别的合成数据集
X, y = make_classification(n_samples=1000, n_classes=5, n_informative=10, n_clusters_per_class=1, random_state=42)

# 分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练分类器
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = clf.predict(X_test)

# 方法 1：classification_report
print(&quot;分类报告&quot;)
print(classification_report(y_test, y_pred))

# 方法 2：使用 precision_recall_fscore_support 循环
res = []

for l in range(5):
prec, recall, _, _ = precision_recall_fscore_support(np.array(y_test) == l,
np.array(y_pred) == l,
pos_label=True, average=None)
res.append([l, recall[0], recall[1]])

df = pd.DataFrame(res, columns=[&#39;class&#39;, &#39;sensitivity&#39;, &#39;specificity&#39;])
print(&quot;\nSensitivity and Specificity&quot;)
print(df)

输出：
分类报告
准确率 召回率 f1 分数 支持率

0 0.76 0.71 0.74 35
1 0.72 0.93 0.81 30
2 0.72 0.81 0.76 32
3 0.85 0.86 0.86 59
4 0.88 0.64 0.74 44

准确率 0.79 200
宏平均值 0.78 0.79 0.78 200
加权平均值 0.80 0.79 0.79 200

敏感度和特异性
类别敏感度特异性
0 0 0.951515 0.714286
1 1 0.935294 0.933333
2 2 0.940476 0.812500
3 3 0.936170 0.864407
4 4 0.974359 0.636364

问题：
为什么 classification_report 和使用 precision_recall_fscore_support 的循环之间的敏感度（召回率）值不同？具体来说，为什么 classification_report 报告的召回率值与循环方法中计算出的敏感度值之间存在差异？如果可能，您能否用一个简单的示例（手动解决）来展示它
您尝试了什么，您期望什么？
我使用了两种方法来评估我的 RandomForestClassifier 的性能。首先，我使用 classification_report 来获取每个类别的精确度、召回率和 F1 分数。然后，我使用带有 precision_recall_fscore_support 的循环计算每个类别的敏感度和特异性。
我期望循环方法中计算出的敏感度值与 classification_report 中的召回率值相匹配，因为敏感度和召回率在分类任务中通常被视为同义词。但是，我发现两组值之间存在差异。
实际上结果如何？
classification_report 中的召回率值与循环方法中计算的敏感度值不同。classification_report 在多类上下文中为每个类提供召回率值，而循环方法将每个类视为二元分类问题，从而导致不同的敏感度和特异性值。]]></description>
      <guid>https://stackoverflow.com/questions/78686328/why-do-the-sensitivity-recall-values-differ-between-classification-report-and</guid>
      <pubDate>Sat, 29 Jun 2024 14:57:35 GMT</pubDate>
    </item>
    <item>
      <title>不断变化的类别的文档分类[关闭]</title>
      <link>https://stackoverflow.com/questions/78686139/document-classification-with-changing-classes</link>
      <description><![CDATA[我想用 Java 为我的客户构建一个文档分类工具，并满足以下特殊条件：

客户使用我的软件 (SaaS) 来管理项目。一个客户
通常每年有 1-10 个项目。
在每个项目中，客户需要对 50-100 个文档进行分类。
类别因项目而异，由客户定义。
即使类别各不相同，但不同项目和不同客户的类别之间往往存在很强的相似性。


示例：

项目 A 有 3 个类别：

A1 音频编辑
A2 视频编辑
A3 管理和簿记


项目 B 有 4 个类别：

B1 音频和视频编辑
B2 管理
B3簿记
B4 电信


项目 C 有 4 个类别：

C1 音频编辑
C2 管理费用
C3 电信
C4 差旅费



文件：

电话账单将进入 A3、B4 和 C3
音频编辑专家的发票将进入 A1、B1 和 C1。
簿记软件的账单将进入 A3、B3 和 C2。
回形针的账单将进入 A3、B2 和 C2。


我预计会有数百名客户，他们有数千项目。就其本身而言，为每个项目创建和训练一个单独的分类系统显然是徒劳的。
我想我可以利用两个事实来解决问题：

这些类别随着时间的推移不断重复，要么完全相同，要么名称和含义略有变化。
客户定义的类名与文档内容之间存在关联。例如，音频编辑的发票通常包含“音频”、“声学”或类似的词，而这些词很少出现在其他类别的文​​档中。
客户可以手动更正分类。在某个时候，他们会完成项目。此时，可以假设客户已经对文档分类做出了最终决定。这种手动检查的最终分类可以用作输入，以随着时间的推移自动改进模型。

感觉我不是第一个有这种要求的人，但我在网上找不到关于这个主题的任何信息。也许我只是不知道用什么术语来描述这类问题。任何关于良好流程或一些现有算法/库/文献/产品的建议都将不胜感激，最好是基于 Java 的。]]></description>
      <guid>https://stackoverflow.com/questions/78686139/document-classification-with-changing-classes</guid>
      <pubDate>Sat, 29 Jun 2024 13:46:46 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Android Studio 上使用可教机器分析机器学习模型（使用 Kotlin）</title>
      <link>https://stackoverflow.com/questions/78685944/failed-to-analyze-machine-learning-models-with-teachable-machine-on-android-stud</link>
      <description><![CDATA[我在分析照片时遇到了问题，到现在还是卡住了。我有 3 个模型，分别是癌症、非癌症和未知。当我尝试分析时，结果总是“未知” 有人知道该如何解决吗？我尝试更改代码并询问人工智能，但仍然卡住了。
您好，我在分析照片时遇到了问题，到现在还是卡住了。我有 3 个模型，分别是癌症、非癌症和未知。当我尝试分析时，结果总是“未知” 有人知道该如何解决吗？我尝试更改我的代码并询问人工智能，但仍然卡在这里是我的代码
ImageClassifierHelper

class ImageClassifierHelper(
private var Threshold: Float = 0.5f, // Tingkatkan Threshold
private var maxResult: Int = 3,
private val modelName: String = &quot;model_unquant.tflite&quot;,
val context: Context,
val classifierListener: ClassifierListener?
) {
interface ClassifierListener {
fun onError(error: String)
fun onResults(result: MutableList&lt;Category&gt;?)
}

private var imageClassifier: ModelUnquant? = null

init {
setupImageClassifier()
}

private fun setupImageClassifier() {
try {
imageClassifier = ModelUnquant.newInstance(context)
} catch (e: IOException) {
classifierListener?.onError(context.getString(R.string.failed))
Log.e(TAG, e.message.toString())
}
}

伴随对象 {
private const val TAG = &quot;ImageClassifierHelper&quot;
}

// 按照您的模型添加标签列表
private val labels = listOf(&quot;Non-Cancer&quot;, &quot;Cancer&quot;, &quot;Unknown&quot;)

fun classifyStaticImage(imageUri: Uri) {
if (imageClassifier == null) {
setupImageClassifier()
}

// 将 Uri 转换为 Bitmap
val bitmap = uriToBitmap(context.contentResolver, imageUri)
?: run {
classifierListener?.onError(&quot;Failed to decrypt image&quot;)
return
}
if (bitmap == null) {
classifierListener?.onError(&quot;Failed to decrypt image&quot;)
return
}

val resizedBitmap = Bitmap.createScaledBitmap(bitmap, 224, 224, true)

//将 Bitmap 转换为 TensorImage
val tensorImage = TensorImage(DataType.FLOAT32)
tensorImage.load(resizedBitmap)

// 为输入创建 TensorBuffer
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 224, 224, 3), DataType.FLOAT32)
inputFeature0.loadBuffer(tensorImage.buffer)

// 调用推理模型并计算结果
val output = imageClassifier?.process(inputFeature0)
val outputFeature0 = output?.outputFeature0AsTensorBuffer

// 将 Bitmap 转换为类别列表
val probability = outputFeature0?.let { tensorBuffer -&gt;
tensorBuffer.floatArray.mapIndexed { 索引，值 -&gt;
Category(labels.getOrElse(index) { &quot;Unknown&quot; }, value)
}.filter { it.score &gt;= Threshold } // 根据阈值过滤
.toMutableList()
}

classifierListener?.onResults(probability)
}

private fun uriToBitmap(contentResolver: ContentResolver, uri: Uri): Bitmap? {
return try {
val inputStream = contentResolver.openInputStream(uri)
BitmapFactory.decodeStream(inputStream)
} catch (e: IOException) {
e.printStackTrace()
null
}
}
}

]]></description>
      <guid>https://stackoverflow.com/questions/78685944/failed-to-analyze-machine-learning-models-with-teachable-machine-on-android-stud</guid>
      <pubDate>Sat, 29 Jun 2024 12:13:59 GMT</pubDate>
    </item>
    <item>
      <title>使用 DPO 优化 LLM：评估期间排除 nan 损失值故障</title>
      <link>https://stackoverflow.com/questions/78685861/optimizing-an-llm-using-dpo-troubleshooting-nan-loss-values-during-evaluation</link>
      <description><![CDATA[我想基于 DPO 优化一个 LLM。当我尝试训练和评估模型时，评估结果中出现了 nan 值。


import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import Dataset
from trl import DPOTrainer, DPOConfig
from datasets import load_dataset

model_name = &quot;EleutherAI/pythia-14m&quot;
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

def preprocess_data(item):
return {
&#39;prompt&#39;: &#39;指示：&#39; + item[&#39;prompt&#39;] + &#39;\n&#39;,
&#39;chosen&#39;: &#39;输出：&#39; + item[&#39;chosen&#39;],
&#39;rejected&#39;: &#39;输出：&#39; + item[&#39;rejected&#39;]
}

dataset = load_dataset(&#39;jondurbin/truthy-dpo-v0.1&#39;, split=&quot;train&quot;)
dataset = dataset.map(preprocess_data)
split_dataset = dataset.train_test_split(test_size=0.1) # 根据需要调整 test_size
train_dataset = split_dataset[&#39;train&#39;]
val_dataset = split_dataset[&#39;test&#39;]

print(f&quot;训练数据的长度：{len(train_dataset)}&quot;)

print(f&quot;验证数据的长度：{len(val_dataset)}&quot;)

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.unk_token

# 要微调的模型
model = AutoModelForCausalLM.from_pretrained(
model_name,
low_cpu_mem_usage=True,
torch_dtype=torch.float16
).to(device)

model_ref = AutoModelForCausalLM.from_pretrained(
model_name,
low_cpu_mem_usage=True,
torch_dtype=torch.float16
).to(device)

# 配置
training_args = DPOConfig(
output_dir=&quot;./output&quot;,
beta=0.1,
max_length=512,
max_prompt_length=128,
remove_unused_columns=False,
)

# 加载训练器
dpo_trainer = DPOTrainer(
model,
model_ref,
args=training_args,
train_dataset=train_dataset,
eval_dataset=val_dataset, 
tokenizer=tokenizer,
)

# 训练
dpo_trainer.train()

# 评估
evaluation_results = dpo_trainer.evaluate()
print(&quot;评估结果：&quot;, evaluation_results)

这是用于训练简单“pythia-14m”模型的代码。下面是结果。
评估结果：{&#39;eval_loss&#39;: nan, &#39;eval_runtime&#39;: 0.5616, &#39;eval_samples_per_second&#39;: 181.61, &#39;eval_steps_per_second&#39;: 12.463, &#39;eval_rewards/chosen&#39;: nan, &#39;eval_rewards/rejected&#39;: nan, &#39;eval_rewards/accuracies&#39;: 0.0, &#39;eval_rewards/margins&#39;: nan, &#39;eval_logps/rejected&#39;: nan, &#39;eval_logps/chosen&#39;: nan, &#39;eval_logits/rejected&#39;: nan, &#39;eval_logits/chosen&#39;: nan, &#39;epoch&#39;: 3.0}

知道为什么评估期间会出现 nan 值吗? 代码中有什么错误吗？]]></description>
      <guid>https://stackoverflow.com/questions/78685861/optimizing-an-llm-using-dpo-troubleshooting-nan-loss-values-during-evaluation</guid>
      <pubDate>Sat, 29 Jun 2024 11:37:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我尝试从 (.fif) 文件进行可视化时，会在 mne-python 中收到此运行时警告？</title>
      <link>https://stackoverflow.com/questions/78679466/why-am-i-getting-this-runtime-warning-in-mne-python-while-trying-to-visualize-fr</link>
      <description><![CDATA[我正在努力使用 mne-python 进行预处理的 eeg 通道可视化部分。这些 (&#39;.fif&#39;) 文件是从 (&#39;.mat&#39;) 文件预处理的。这是我在 kaggle 笔记本中使用的代码：
import mne
import os
import numpy as np

# 定义存储 .fif 文件的目录
data_dir = &#39;/kaggle/input/preproccesed-dataset/128-channel-resting(2.0)/kaggle/working/preprocessed_mat_output_directory&#39;

# 列出目录中的所有 .fif 文件
fif_files = [f for f in os.listdir(data_dir) if f.endswith(&#39;-epo.fif&#39;)]

# 加载每个 .fif 文件
epochs_list = [mne.read_epochs(os.path.join(data_dir, fif_file)) for fif_file in fif_files]

# 连接所有将 epochs 合并为单个 Epochs 对象
all_epochs = mne.concatenate_epochs(epochs_list)

# 预处理：应用高通滤波器
all_epochs.filter(l_freq=1.0, h_freq=40.0)

# 执行 ICA
ica = mne.preprocessing.ICA(n_components=20, random_state=97)
ica.fit(all_epochs)

# 根据检查或自动标准手动排除组件
ica.exclude = [0, 1] # 根据已识别的工件组件进行调整

# 将 ICA 应用于 epochs
all_epochs = ica.apply(all_epochs)

# 绘制诱发反应
evoked = all_epochs.average()
evoked.plot()

# 绘制 PSD
fig_psd = all_epochs.plot_psd(fmin=1.0, fmax=40.0, average=True, spatial_colors=False)

# 绘制地形图
fig_topo = evoked.plot_topomap(times=[0.1, 0.2, 0.3], ch_type=&#39;eeg&#39;, average=0.05)

# 计算并绘制 TFR
from mne.time_frequency import tfr_morlet
freqs = np.arange(6, 30, 3)
n_cycles = freqs / 2
power = tfr_morlet(all_epochs, freqs=freqs, n_cycles=n_cycles, return_itc=False, average=True)
fig_tfr = power.plot([0])

# 确保保存目录存在
save_dir = &#39;/kaggle/working/mat_visuals/&#39;
os.makedirs(save_dir, exist_ok=True) # 如果目录不存在，则创建目录

# 保存处理后的数据
all_epochs.save(os.path.join(save_dir, &#39;processed-epochs.fif&#39;), overwrite=True)
evoked.save(os.path.join(save_dir, &#39;evoked-ave.fif&#39;), overwrite=True)

# 如果需要，保存图形
fig_psd.savefig(os.path.join(save_dir, &#39;psd_plot.png&#39;))
fig_topo.savefig(os.path.join(save_dir, &#39;topomap_plot.png&#39;))
fig_tfr.savefig(os.path.join(save_dir, &#39;tfr_plot.png&#39;))

这是我收到的错误：
[Runtime-ERROR]
/tmp/ipykernel 34/2693702433.py:35：FutureWarning：在 MNE 1.8.0 中，amplitude=&#39;auto&#39; 的值将被删除，新的默认值也将是 amplitude。
fig psd all epochs.plot psd(fmin=1.8, fmax=40.6, average=True, spatial colors=False)
RuntimeError
Traceback (most recent call last)
Cell In[1]. Line 38
35 fig psd all epochs.plot psd(fmin-1.0, fmax 40.0, average True, spatial colors=False)
37 绘制地形图
38 fig topo evoked。 plot topomap(times-10.1, 0.2, 6.31, ch type=&#39;eeg, average-0.05)
40 计算并绘制 TFR
41 from mne.time_frequency import tfr morlet
我尝试更改这两行代码：
# 绘制 PSD
fig_psd = all_epochs.plot_psd(fmin=1.0, fmax=40.0, average=False, spatial_colors=False)

# 绘制地形图
fig_topo = evoked.plot_topomap(times=False, ch_type=&#39;eeg&#39;, average=0.05)

我还尝试阅读以下 2 个文档：
neurotechedu , mne-python
但我仍然找不到任何解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78679466/why-am-i-getting-this-runtime-warning-in-mne-python-while-trying-to-visualize-fr</guid>
      <pubDate>Thu, 27 Jun 2024 19:16:05 GMT</pubDate>
    </item>
    <item>
      <title>我无法使用 FastAPI 运行使用 Tensorflow 保存的模型</title>
      <link>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</link>
      <description><![CDATA[从 fastapi 导入 FastAPI、File、UploadFile、HTTPException # 1
从 fastapi.middleware.cors 导入 CORSMiddleware # 2
导入 uvicorn # 3
导入 numpy 作为 np # 4
从 io 导入 BytesIO # 5
从 PIL 导入 Image # 6
导入 tensorflow 作为 tf # 7

app = FastAPI() # 8
print(&quot;1&quot;)

# 允许向此 API 发出请求的来源列表
origins = [ # 9
&quot;http://localhost&quot;, # 10
&quot;http://localhost:3000&quot;, # 在此处添加您的前端 URL # 11
] # 12
print(&quot;2&quot;)

app.add_middleware( # 13
CORSMiddleware, # 14
allow_origins=origins, # 15
allow_credentials=True，# 16
allow_methods=[&quot;*&quot;]，# 允许所有方法（GET、POST 等）# 17
allow_headers=[&quot;*&quot;]，# 允许所有标头# 18
)# 19
print(&quot;3&quot;)

# 加载模型
MODEL = tf.saved_model.load(r&quot;D:\Derin_Ogrenme\doma\saved_models\models\model2&quot;)# 20
print(&quot;4&quot;)

CLASS_NAMES = [&quot;Early Blight&quot;, &quot;Late Blight&quot;, &quot;Healthy&quot;]# 21
print(&quot;5&quot;)

@app.get(&quot;/&quot;)# 22
async def ping():# 23
print(&quot;6&quot;)
return &quot;你好，我还活着&quot; # 24

def read_file_as_image(data) -&gt; np.ndarray: # 25
print(&quot;7&quot;)
image = Image.open(BytesIO(data)) # 26
image = image.resize((224, 224)) # 将图像大小调整为预期的输入形状 # 27
image = np.array(image) # 28
print(&quot;8&quot;)
return image # 29

@app.post(&quot;/predict&quot;) # 30
async def predict(file: UploadFile = File(...)): # 31
print(&quot;9&quot;)
image = read_file_as_image(await file.read()) # 32
img_batch = np.expand_dims(image, 0) # 33
print(&quot;10&quot;)

# 执行预测
try: # 34
predictions = MODEL.predict(img_batch) # 35
print(&quot;11&quot;)
predict_class = CLASS_NAMES[np.argmax(predictions[0])] # 36
confidence = float(np.max(predictions[0])) # 37
print(&quot;12&quot;)
return { # 38
&#39;class&#39;: predict_class, # 39
&#39;confidence&#39;: confidence # 40
} # 41
except Exception as e: # 42
print(&quot;13&quot;)
raise HTTPException(status_code=500, detail=&quot;Internal Server Error&quot;) # 43

if __name__ == &quot;__main__&quot;: # 44
uvicorn.run(app, host=&quot;localhost&quot;, port=8001) # 45
print(&quot;14&quot;)


编辑：我应该提到我对此很陌生，所以如果您希望我修复某些问题，我将不胜感激，如果您能简单解释一下。当我运行上面看到的代码时，服务器启动，当我转到 localhost:8000 &quot;detail&quot;: &quot;Not Found&quot; 时，我会收到此警告。
**这是终端错误**
1
2
3
2024-06-29 16:17:00.434027: I tensorflow/core/platform/cpu_feature_guard.cc:210] 此 TensorFlow 二进制文件经过优化，可在性能关键型操作中使用可用的 CPU 指令。
要启用以下指令：AVX2 AVX_VNNI FMA，在其他操作中，使用适当的编译器标志重建 TensorFlow。
4
5
INFO：已启动服务器进程 [5760]
INFO：正在等待应用程序启动。
INFO：应用程序启动完成。
INFO：Uvicorn 在 http://localhost:8001 上运行（按 CTRL+C 退出）
6
INFO：::1:59291 - “GET / HTTP/1.1” 200 OK
INFO：::1:59297 - “GET /predict HTTP/1.1” 405 方法不允许
INFO：::1:59300 - “GET /docs HTTP/1.1” 200 OK
INFO：::1:59300 - “GET /openapi.json HTTP/1.1” 200 OK
9
7
8
10
发生错误：“_UserObject”对象没有属性“predict”
INFO: ::1:59302 - “POST /predict HTTP/1.1” 500 内部服务器错误

我正在尝试运行此代码，但这行不起作用
#MODEL = tf.keras.models.load_model(r&quot;D:\\Derin_Ogrenme\\doma\\saved_models\\models\\2&quot;)

我向 chatgpt 询问了这个问题，他给了我这行代码。
MODEL = TFSMLayer(r&quot;D:\\Derin_Ogrenme\\doma\\saved_models\\models\\2&quot;, call_endpoint=&#39;serving_default&#39;)

当我使用 FastApi 加载植物生长时，它应该做出预测（它必须对叶子图片进行分类。但正如您在照片中看到的那样，它导致了错误。
]]></description>
      <guid>https://stackoverflow.com/questions/78674303/i-cannot-run-my-model-that-i-saved-with-tensorflow-with-fastapi</guid>
      <pubDate>Wed, 26 Jun 2024 19:25:44 GMT</pubDate>
    </item>
    <item>
      <title>绘制预测掩码的问题</title>
      <link>https://stackoverflow.com/questions/78669554/issue-with-plotting-predicted-masks</link>
      <description><![CDATA[我目前正在进行一个深度学习项目“叶病分割”。我已经训练了一个模型超过 50 个时期，并获得了以下准确度和损失指标：
训练损失：19.4736，训练准确度：0.9395
验证损失：19.6197，验证准确度：0.9100
测试损失：19.6148，测试准确度：0.9123
但是，当我绘制预测的蒙版时，它们看起来不准确。我的绘图代码有问题吗？
def plot_predictions(model, images, mask, num_samples=5):
predictions = model.predict(images[:num_samples])
for i in range(num_samples):
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.title(&#39;真实图像&#39;)
plt.imshow(images[i])
plt.subplot(1, 3, 2)
plt.title(&#39;地面真相面具&#39;)
plt.imshow(masks[i], cmap=&#39;gray&#39;) # 假设面具已经是二进制的
plt.subplot(1, 3, 3)
plt.title(&#39;预测面具&#39;)
plt.imshow(predictions[i][:, :, 0], cmap=&#39;gray&#39;) # 转换预测面具转换为二进制
plt.show()

plot_predictions(model, test_images.numpy(), test_masks_L, num_samples=5)

原始图像-蒙版-预测蒙版
请检查我的代码并帮助找出可能导致此问题的任何错误？]]></description>
      <guid>https://stackoverflow.com/questions/78669554/issue-with-plotting-predicted-masks</guid>
      <pubDate>Tue, 25 Jun 2024 21:18:52 GMT</pubDate>
    </item>
    <item>
      <title>mlflow 在记录图像时不会自动记录工件</title>
      <link>https://stackoverflow.com/questions/78663805/mlflow-doesnt-autolog-artifacts-while-logging-images</link>
      <description><![CDATA[我对 mlflow 还很陌生。我偶然发现了一些奇怪的行为。当我运行一个简单的 Keras 模型时，使用 MLFlow Autolog 进行拟合，如下所示：
mlflow.set_tracking_uri(&quot;sqlite:///mlflow.db&quot;)
mlflow.set_experiment(&quot;keras_log&quot;)
mlflow.tensorflow.autolog()

# 定义参数。
num_epochs = 10
batch_size = 256

# 训练模型。
history = model.fit(X_train,
y_train,
epochs=num_epochs,
batch_size=batch_size,
validation_data=(X_test,y_test))
mlflow.end_run()

这会产生预期的行为。我可以在工件和指标中看到模型。

但是，当我使用包含训练准确率和损失的自定义图形进行自动记录时，工件仅包含自定义图像。好像自动记录根本不起作用。
mlflow.set_tracking_uri(&quot;sqlite:///mlflow.db&quot;)
mlflow.set_experiment(&quot;keras_log&quot;)
mlflow.tensorflow.autolog()

# 定义参数。
num_epochs = 10
batch_size = 256

# 训练模型。
history = model.fit(X_train,
y_train,
epochs=num_epochs,
batch_size=batch_size,
validation_data=(X_test, y_test))

##_________ 有问题的部分
fig, ax = plt.subplots(1,2,figsize=(10,4))
ax[0].plot(history.history[&#39;accuracy&#39;], label=&#39;Accuracy&#39; )
ax[0].plot(history.history[&#39;val_accuracy&#39;], label=&#39;Val Accuracy&#39; )
ax[0].set_title(&#39;Accuracy&#39;)
ax[0].legend(loc=&#39;best&#39;)
ax[1].plot(history.history[&#39;loss&#39;], label=&#39;Loss&#39; )
ax[1].plot(history.history[&#39;val_loss&#39;], label=&#39;Val Loss&#39; )
ax[1].set_title(&#39;Loss&#39;)
ax[1].legend(loc=&#39;best&#39;)
mlflow.log_figure(fig,&#39;training_history.png&#39;)
# _________

mlflow.end_run()

模型工件不存在。指标也没有记录。
我是否遗漏了一些简单的东西？

请帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78663805/mlflow-doesnt-autolog-artifacts-while-logging-images</guid>
      <pubDate>Mon, 24 Jun 2024 17:12:13 GMT</pubDate>
    </item>
    <item>
      <title>模块“keras.layers”没有属性“experimental”</title>
      <link>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</link>
      <description><![CDATA[你好，我试图调整数据集的大小和比例，如下所示，但遇到了此错误：
AttributeError：模块“keras.layers”没有属性“experimental”

resize_and_rescale= tf.keras.Sequential([
layers.experimental.preprocessing.Resizing(IMAGE_SIZE,IMAGE_SIZE),
layers.experimental.preprocessing.Rescaling(1.0/255)
])

]]></description>
      <guid>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</guid>
      <pubDate>Wed, 14 Dec 2022 00:43:49 GMT</pubDate>
    </item>
    <item>
      <title>我如何进行分层下采样？</title>
      <link>https://stackoverflow.com/questions/67290933/how-can-i-do-a-stratified-downsampling</link>
      <description><![CDATA[我需要使用机器学习技术构建蛋白质序列分类模型。每个观察值都可以归类为 0 或 1。但是，我注意到我的训练集总共包含 170 000 个观察值，其中只有 5000 个被标记为 1。因此，我希望将标记为 0 的观察值数量下调到 5000。
我目前在模型中使用的特征之一是序列的长度。我如何才能对 0 类的数据进行下采样，同时确保 length_sequence 的分布与 1 类的分布相似？
以下是 1 类的 length_sequence 直方图：

以下是 0 类的 length_sequence 直方图：

您可以看到，在这两种情况下，长度都从 2 到 255字符。但是，类别 0 有更多的观察值，而且它们也往往比类别 0 中的观察值长得多。
我如何对类别 0 进行下采样，并使新的直方图看起来类似于类别 1 中的直方图？
我正在尝试使用 scikit-learn 进行分层下采样，但我遇到了困难。]]></description>
      <guid>https://stackoverflow.com/questions/67290933/how-can-i-do-a-stratified-downsampling</guid>
      <pubDate>Tue, 27 Apr 2021 21:31:25 GMT</pubDate>
    </item>
    </channel>
</rss>