<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 23 Dec 2023 15:12:51 GMT</lastBuildDate>
    <item>
      <title>我正在研究一个示例：简单线性回归薪资数据，其中解释了为什么为 x 列创建新轴以及 np.newaxis 是什么的步骤</title>
      <link>https://stackoverflow.com/questions/77708064/i-was-working-on-a-examples-simple-linear-regression-salary-data-from-that-expl</link>
      <description><![CDATA[拆分训练数据和测试数据
X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state=100)
为 x 列创建新轴
X_train = X_train[:,np.newaxis]
X_test = X_test[:,np.newaxis]
使用 np.newaxis 添加新轴有助于重塑数据以实现兼容性]]></description>
      <guid>https://stackoverflow.com/questions/77708064/i-was-working-on-a-examples-simple-linear-regression-salary-data-from-that-expl</guid>
      <pubDate>Sat, 23 Dec 2023 15:05:36 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Label Studio 没有找到云源 - 本地存储（不存在）</title>
      <link>https://stackoverflow.com/questions/77707858/why-my-label-studio-didnt-find-the-cloud-source-local-storage-doesnt-exist</link>
      <description><![CDATA[我已经从 git 安装了 Label-Studio git 安装，
并且想要将存储更改为云存储 - 本地存储，路径为 C:\data\media\dataset 数据集图像路径
所以我从这一步开始 https://labelstud.io/guide/storage#Local -storage 和我的命令是：
 设置 LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED=true

 设置 LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT=C:\\data\\media\\dataset

我添加了set，因为如果没有它，它会出错。
当我更改本地存储时，它不存在。它警告使用“必须以 LOCAL_FILES_DOCUMENT_ROOT 开头”
[ErrorDetail(string=&#39;路径 C:\\\\data\\\\media 必须以 LOCAL_FILES_DOCUMENT_ROOT=D:\\ 开头，并且必须是子项，例如：D:\\abc&#39;, code =&#39;无效&#39;）]

[ErrorDetail(string=&#39;路径 LOCAL_FILES_DOCUMENT_ROOT=C:\\\\data\\\\media 不存在&#39;, code=&#39;无效&#39;)]

屏幕截图
屏幕截图]]></description>
      <guid>https://stackoverflow.com/questions/77707858/why-my-label-studio-didnt-find-the-cloud-source-local-storage-doesnt-exist</guid>
      <pubDate>Sat, 23 Dec 2023 13:55:59 GMT</pubDate>
    </item>
    <item>
      <title>如何为 XGboost 性能分配月度数据的权重？</title>
      <link>https://stackoverflow.com/questions/77707285/how-to-assign-weights-to-monthly-data-to-xgboost-performance</link>
      <description><![CDATA[我们收到了用于训练的逐月数据，使用每个月末的快照。 XGboost 模型（二元分类）在训练测试中一个月表现良好，但在实时测试中表现不佳，我们添加了额外的月份数据，但召回率下降了。有没有办法影响模型，使最近的数据更加重要/权重，同时使用上个月的数据来支持学习和数量。
我尝试单独使用不同月份的数据进行训练。由于数据高度不平衡尝试了SMOTE ENN、tomek，但是使用SMOTE降低了召回率很多。]]></description>
      <guid>https://stackoverflow.com/questions/77707285/how-to-assign-weights-to-monthly-data-to-xgboost-performance</guid>
      <pubDate>Sat, 23 Dec 2023 10:10:43 GMT</pubDate>
    </item>
    <item>
      <title>Vertex AI 表格数据预测错误</title>
      <link>https://stackoverflow.com/questions/77707074/vertex-ai-tabular-data-incorrect-prediction</link>
      <description><![CDATA[我使用表格数据集在 Google Vertex AI 中创建了模型，该数据集有两个感兴趣的列，称为 ActivityName 和 WorkType。我希望模型根据 ActivityName 预测工作类型。以下是我遵循的步骤：-

上传了包含用于训练、验证和测试的 SPLIT 列的 CSV 文件，确保每个工作类型至少拥有 80% 的训练数据。
创建了一个具有 3 个节点小时的 AutoML 模型，用于使用对数损失进行训练
部署模型
尝试了不同的活动名称，但预测错误

源示例位于此处]]></description>
      <guid>https://stackoverflow.com/questions/77707074/vertex-ai-tabular-data-incorrect-prediction</guid>
      <pubDate>Sat, 23 Dec 2023 08:37:44 GMT</pubDate>
    </item>
    <item>
      <title>如何拆分其中的列和数据？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77707030/how-split-column-and-data-in-it</link>
      <description><![CDATA[如何划分列及其中的数据。我已附上供参考。
示例：水泥_水列，其值为 3002； 203.0 必须分成名为水泥和水的两列，其值分别为 302.0 和 203.0。列值具有不同的分隔符（;，_），必须进行处理，并且这些值具有字符串数据，必须使用单词到数字将其转换为数值。
train.csv -https://drive.google.com/file/d/1rXC_cgJDHgvEsly9mVXphM_rl0ZI4sO2/view?usp=drive_link
test.csv https://drive.google。 com/file/d/1jUqlt5NVLvf9Y-vNLehCZShBqBkK0_NV/view?usp=drive_link
供您参考
&lt;前&gt;&lt;代码&gt;#代码
将 pandas 导入为 pd
从 word2number 导入 w2n

df = pd.read_csv(&#39;test.csv - Sheet1.csv&#39;)
def Convert_words_to_numbers(文本):
    单词 = text.replace(&#39;_&#39;, &#39; &#39;).replace(&#39;;&#39;, &#39; &#39;).replace(&#39;,&#39;, &#39; &#39;).split()
    Converted_words = [str(w2n.word_to_num(word)) if word.isalpha() else 逐字逐字]
    返回 &#39;​​ &#39;.join(converted_words)
df[&#39;水泥水&#39;] = df[&#39;水泥水&#39;].apply(lambda x:convert_words_to_numbers(x))
df[[&#39;水泥&#39;, &#39;水&#39;]] = df[&#39;水泥水&#39;].str.split(&#39; &#39;, Expand=True)
df[[&#39;coarse_aggregate&#39;, &#39;fine_aggregate&#39;]] = df[&#39;coarse_fine_aggregate&#39;].str.split(&#39;;&#39;, Expand=True)
df = df.drop([&#39;水泥水&#39;, &#39;coarse_fine_aggregate&#39;], axis=1)
df = df.apply(pd.to_numeric, 错误=&#39;忽略&#39;)
打印（df）
]]></description>
      <guid>https://stackoverflow.com/questions/77707030/how-split-column-and-data-in-it</guid>
      <pubDate>Sat, 23 Dec 2023 08:18:33 GMT</pubDate>
    </item>
    <item>
      <title>运行时错误：形状“[8192]”对于大小 8256 的输入无效</title>
      <link>https://stackoverflow.com/questions/77706527/runtimeerror-shape-8192-is-invalid-for-input-of-size-8256</link>
      <description><![CDATA[导入火炬
将 torch.nn 导入为 nn
从 torch.nn 导入功能为 F

批量大小 = 64
块大小 = 128
最大迭代数 = 5000
评估间隔 = 200
学习率 = 3e-4
设备 = &#39;cuda&#39; 如果 torch.cuda.is_available() else &#39;cpu&#39;
评估次数 = 100

火炬.manual_seed(5452)

打开（&#39;DiscordGPT_data.txt&#39;，&#39;r&#39;，encoding=&#39;utf-8&#39;）作为f：
    文本 = f.read()

字符=排序（列表（集合（文本）））
vocab_size = len(字符)

stoi = { ch:i for i,ch in enumerate(characters) }
itos = { i:ch for i,ch in enumerate(characters) }
编码 = lambda s: [stoi[c] for c in s]
解码 = lambda l: &#39;&#39;.join(itos[i] for i in l)

数据 = torch.tensor(编码(文本), dtype=torch.long)
n = int(0.9*len(数据))
训练数据 = 数据[:n]
评估数据 = 数据[n:]

def get_batch(分割):
    数据 = train_data 如果 split == &#39;train&#39; 否则评估_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i:i+block_size+1] for i in ix])
    x, y = x.to(设备), y.to(设备)

    返回 x、y

@torch.no_grad()
defestimate_loss():
    输出 = {}
    模型.eval()
    对于 [&#39;train&#39;, &#39;val&#39;] 中的分割：
        损失 = torch.zeros(eval_iters)
        对于范围内的 k(eval_iters)：
            X, Y = get_batch(分割)
            logits，损失 = 模型（X，Y）
            损失[k] = loss.item()
        out[split] = 损失.mean()
    模型.train()
    返回

类 BigramLangModel(nn.Module):

    def __init__(自身, vocab_size):
        超级().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

    def 前进（自我，idx，目标=无）：

        logits = self.token_embedding_table(idx)

        如果目标为无：
            损失=无
        别的：
            B、T、C = logits.shape
            logits = logits.view(B*T, C)
            目标 = 目标.view(B*T)
            损失 = F.cross_entropy(logits, 目标)

        返回 logits，损失
    
    def 生成（自我，idx，max_new_tokens）：

        对于 _ 在范围内（max_new_tokens）：
            logits，损失 = self(idx)
            logits = logits[:, -1, :]
            probs = F.softmax(logits, 暗淡=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), 暗淡=1)

        返回idx

模型 = BigramLangModel(vocab_size)
m = model.to(设备)

优化器 = torch.optim.AdamW(m.parameters(), lr=learning_rate)

对于范围内的迭代器（max_iters）：

    如果 iter % eval_interval == 0:
        损失=estimate_loss()
        print(f&quot;step {iter}: train loss {losses[&#39;train&#39;]:.4f}, val loss {losses[&#39;val&#39;]:.4f}&quot;)
        
    xb, yb = get_batch(&#39;火车&#39;)

    logits，损失 = 模型（xb，yb）
    优化器.zero_grad(set_to_none = True)
    loss.backward()
    优化器.step()

上下文 = torch.zeros((1,1), dtype = torch.long, 设备 = 设备)
print(解码(m.generate(上下文, max_new_tokens=500)[0].tolist()))

很好奇哪里出了问题，即使我减少批量和块大小，仍然会出现运行时错误，调整数字并没有真正的帮助。我对 ML/AI 领域还很陌生，因此任何有关如何前进的指示都会有所帮助。这应该是 Nanogpt 的复制品，并试图自己弄清楚。]]></description>
      <guid>https://stackoverflow.com/questions/77706527/runtimeerror-shape-8192-is-invalid-for-input-of-size-8256</guid>
      <pubDate>Sat, 23 Dec 2023 03:21:19 GMT</pubDate>
    </item>
    <item>
      <title>在机器学习模型中对 NDVI 和 LST 数据使用相同的空间分辨率</title>
      <link>https://stackoverflow.com/questions/77705553/using-same-spatial-resolution-for-ndvi-and-lst-data-in-machine-learning-model</link>
      <description><![CDATA[我目前正在开发干旱预报机器学习模型，这是我第一次处理卫星数据。我的两个数据源是来自 Google Earth Engine 的用于获取 NDVI 值的 MOD13A1 数据集和用于获取 LST 值的 MOD11A1 数据集。感兴趣的区域是南加州。根据 Google Earth Engine，MOD13A1 的默认分辨率为 500m，而 MOD11A1 的默认分辨率为 1000m。
我是否需要对这两种产品使用相同的空间分辨率才能最大限度地提高模型的性能？如果是这样，那会是什么决议？
我尝试更改 NDVI 的 getRegion() 方法中的“scale”参数，当我这样做时，我得到了不同的值。这就是我提出问题的原因。]]></description>
      <guid>https://stackoverflow.com/questions/77705553/using-same-spatial-resolution-for-ndvi-and-lst-data-in-machine-learning-model</guid>
      <pubDate>Fri, 22 Dec 2023 19:46:46 GMT</pubDate>
    </item>
    <item>
      <title>我无法将数组传递给 svm 模型，它显示使用序列设置数组元素的错误</title>
      <link>https://stackoverflow.com/questions/77705514/i-am-not-able-to-pass-an-array-to-svm-model-it-is-showing-an-error-of-setting-an</link>
      <description><![CDATA[由于我正在处理音频数据集，所以首先我提取了 MFCC 特征并将其存储到列表中，然后我进行了填充和展平，我收到了此警告，但我忽略了它一段时间
我不能在这里将数据类型声明为对象，因为我必须在 Svm 模型中传递它
mfcc 功能及其数组的代码
SVM 代码：
svm 代码和错误
那么任何人都可以更正代码，以便我可以将其传递到我的 Svm 模型中，并且我可以进行音频分类吗？]]></description>
      <guid>https://stackoverflow.com/questions/77705514/i-am-not-able-to-pass-an-array-to-svm-model-it-is-showing-an-error-of-setting-an</guid>
      <pubDate>Fri, 22 Dec 2023 19:34:37 GMT</pubDate>
    </item>
    <item>
      <title>如何解决机器学习中的值错误？</title>
      <link>https://stackoverflow.com/questions/77705470/how-do-i-solve-a-value-error-in-machine-learning</link>
      <description><![CDATA[我正在尝试使用朴素贝叶斯选择和训练我的模型，并且正在处理糖尿病数据集，但我不断收到如下值错误：
ValueError Traceback（最近一次调用最后一次）
〜\ AppData \ Local \ Temp \ ipykernel_15552 \ 3536100043.py 在？（）
      1 模型 = GaussianNB()
----&gt; 2 model.fit(X_train, y_train)

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ sklearn \ naive_bayes.py 在？（自我，X，y，sample_weight）
    263 返回实例本身。
    第264章
    第265章
    266 y = self._validate_data(y=y)
--&gt; [第 267 章]
    第268章
    第269章）

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ sklearn \ naive_bayes.py 在？（自我，X，y，类，_refit，sample_weight）
    第424章
    第425章
    第426章
    第427章
--&gt;第428章
    第429章
    第430章
    第431章

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ sklearn \ base.py 在？（自我，X，y，重置，validate_separately，** check_params）
    [580] 第580章不在 check_y_params 中：
    第581章
    第582章
    第583章：
--&gt;第584章
    第585章
    第586章
    第587章

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ sklearn \ utils \ validation.py 在？（X，y，accept_sparse，accept_large_sparse，dtype，order，copy，force_all_finite，ensure_2d，allow_nd，multi_output， Ensure_min_samples、ensure_min_features、y_numeric、估计器）
   第1102章
   第1103章 1103
   第1104章）
   1105
-&gt;第1106章
   第1107章
   第1108章
   第1109章

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ sklearn \ utils \ validation.py 在？（数组，accept_sparse，accept_large_sparse，dtype，order，copy，force_all_finite，ensure_2d，allow_nd，ensure_min_samples，ensure_min_features，估计器，输入名称）
    第876章）
    第877章
    第878章：
    第879章
--&gt;第880章
    第881章
    第882章
    第883章）

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ sklearn \ utils \ _array_api.py 在？（数组，dtype，顺序，复制，xp）
    第181章
    182 xp，_ = get_namespace（数组）
    183 if xp.__name__ in {“numpy”,“numpy.array_api”}：
    [第 184 章] 第 184 章
--&gt; array[185] = numpy.asarray(数组, order=order, dtype=dtype)
    186 return xp.asarray(数组，复制=复制)
    187 其他：
    [第 188 章]

〜\ AppData \ Local \ Programs \ Python \ Python311 \ Lib \ site-packages \ pandas \ core \ generic.py？（自我，dtype）
   1996 def __array__(self, dtype: npt.DTypeLike | None = None) -&gt; np.ndarray：
   1997 值 = self._values
-&gt; 1998 arr = np.asarray（值，dtype = dtype）
   1999 如果 (
   2000 astype_is_view（值.dtype，arr.dtype）
   2001 和 using_copy_on_write()

ValueError：无法将字符串转换为浮点数：&#39;F&#39;

我尝试再次运行所有代码单元，因为我使用的是 Jupyter Notebook，但结果与我不断得到的结果相同。]]></description>
      <guid>https://stackoverflow.com/questions/77705470/how-do-i-solve-a-value-error-in-machine-learning</guid>
      <pubDate>Fri, 22 Dec 2023 19:21:17 GMT</pubDate>
    </item>
    <item>
      <title>输入数据耗尽，中断张量流训练</title>
      <link>https://stackoverflow.com/questions/77705086/input-ran-out-of-data-interrupting-training-in-tensorflow</link>
      <description><![CDATA[我正在从事肿瘤分割工作，图像是 nifti 格式的 3D（MRI 图像）。我创建了一个数据生成器，因为如果我将完整数据集上传到 RAM，它会因 3D 图像而崩溃。该数据集由611张图像组成，尺寸为（240,240,160），计算时的块数为61000和11375
这是我的数据管道：
def load_nifti_image(文件路径, patch_size=(48, 48, 32), step_size=(48, 48, 32)):
    nifti = nib.load(文件路径)
    体积 = nifti.get_fdata()

    # 从卷创建补丁
    补丁 = patchify(体积, patch_size, 步骤=step_size)

    # 重塑 patch 相乘 (5, 5, 5) 并添加通道维度（1 表示灰度）
    补丁 = patch.reshape(-1, *patches.shape[-3:])
    补丁= np.expand_dims（补丁，轴=-1）

    返回补丁

＃  -  -  -  -  -  -  -  -  -  -  - -火车 -  -  -  -  -  -  -  -  -  -  - -
nifti_files = [os.path.join(“/content/drive/MyDrive/Interpolated/train/images”, f) for f in os.listdir(“/content/drive/MyDrive/Interpolated/train/images”) if f.endswith(&#39;.nii.gz&#39;)]
mask_files = [os.path.join(“/content/drive/MyDrive/Interpolated/train/masks”, f) for f in os.listdir(“/content/drive/MyDrive/Interpolated/train/masks”) if f.endswith(&#39;.nii.gz&#39;)]

 ＃  -  -  -  -  -  -  -  -  -  -  - -验证 -  -  -  -  -  -  -  -  -  -  - -
nifti_files_val = [os.path.join(&quot;/content/drive/MyDrive/Interpolated/validation/images&quot;, f) for f in os.listdir(&quot;/content/drive/MyDrive/Interpolated/validation/images&quot;) if f.endswith(&#39;.nii.gz&#39;)]
mask_files_val = [os.path.join(&quot;/content/drive/MyDrive/Interpolated/validation/masks&quot;, f) for f in os.listdir(&quot;/content/drive/MyDrive/Interpolated/validation/masks&quot;) if f.endswith(&#39;.nii.gz&#39;)]

defcalculate_patches(文件路径, patch_size=(48, 48, 32), step_size=(48, 48, 32)):
    nifti = nib.load(文件路径)
    体积 = nifti.get_fdata()

    # 计算补丁数量
    patch_shape = [((i - p) // s) + 1 for i, p, s in zip(volume.shape, patch_size, step_size)]
    num_patches = np.prod(patches_shape)

    返回 num_patches

num_train_patches = sum(calculate_patches(f) for i, f in enumerate(nifti_files) if print(f“处理文件 {i}...”) 为 None)
num_val_patches = sum(calculate_patches(f) for i, f in enumerate(nifti_files_val) if print(f“处理文件 {i}...”) 为 None)

def data_generator(image_files, mask_files):
    对于 zip(image_files, mask_files) 中的 img_file、mask_file：
        image_patches = load_nifti_image(img_file)
        mask_patches = load_nifti_image(mask_file)

        对于 zip(image_patches, mask_patches) 中的 img_patch、mask_patch：
            产量 img_patch, mask_patch

train_generator = data_generator(nifti_files, mask_files)
val_generator = data_generator(nifti_files_val, mask_files_val)

输出签名 = (
    tf.TensorSpec(形状=(48, 48, 32, 1), dtype=tf.float64),
    tf.TensorSpec(形状=(48, 48, 32, 1), dtype=tf.float64)
）

数据集= tf.data.Dataset.from_generator（lambda：train_generator，output_signature=output_signature）.repeat（）
dataset_val = tf.data.Dataset.from_generator(lambda: val_generator, output_signature=output_signature).repeat()

数据集=数据集.batch(32)
dataset_val = dataset_val.batch(32)

test_model.fit（数据集，validation_data=dataset_val，epochs=100，steps_per_epoch=num_train_patches//32，validation_steps=num_val_patches//32）

我添加了 .repeat() 希望它能有所帮助，但事实并非如此。
我还尝试计算补丁的数量并手动插入它们，但它也不起作用。
这是完整的回溯：
纪元 1/100
1906/1906 [================================] - 1963s 1s/步 - 损失：0.6447 - dice_coefficient：0.3553 - val_loss ：0.9113 - val_dice_系数：0.0887
纪元 2/100
   1/1906 [................................] - 预计到达时间：10:17 - 损失：1.0000 - dice_coefficient：1.7961e -05

警告：tensorflow：您的输入数据不足；中断训练。确保您的数据集或生成器可以生成至少“steps_per_epoch * epochs”批次（在本例中为 190600 个批次）。构建数据集时，您可能需要使用 Repeat() 函数。
警告：tensorflow：您的输入数据不足；中断训练。确保您的数据集或生成器可以生成至少“steps_per_epoch * epochs”批次（在本例中为 355 个批次）。构建数据集时，您可能需要使用 Repeat() 函数。

1906/1906 [================================] - 0s 31us/步 - 损失：1.0000 - dice_coefficient：1.7961e- 05


]]></description>
      <guid>https://stackoverflow.com/questions/77705086/input-ran-out-of-data-interrupting-training-in-tensorflow</guid>
      <pubDate>Fri, 22 Dec 2023 17:43:32 GMT</pubDate>
    </item>
    <item>
      <title>Resnet34第一层7x7或3x3</title>
      <link>https://stackoverflow.com/questions/77704426/resnet34-first-layer-7x7-or-3x3</link>
      <description><![CDATA[我一直在尝试使用 pytorch 实现 Resnet34，但在查看其他实现时，我发现其中一些具有 3x3 卷积层 + bn + relu 作为第一层。然而，架构图上却写着7x7/2的卷积层。我真的很困惑哪一个是正确的。顺便说一下，我正在 CIFAR10 上进行训练，目前使用 7x7 卷积层经过 100 个周期后获得了 0.9 的准确率。
谢谢！
架构图
self.input_layer = nn.Sequential(
nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,bias=False),
nn.BatchNorm2d(64),
ReLU(),
nn.MaxPool2d(3, 步长=2, 填充=1)
）

这是我的第一个卷积层的代码。]]></description>
      <guid>https://stackoverflow.com/questions/77704426/resnet34-first-layer-7x7-or-3x3</guid>
      <pubDate>Fri, 22 Dec 2023 15:14:50 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络不学习</title>
      <link>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</link>
      <description><![CDATA[我正在尝试在包含 1500 张图像（15 个类别）的训练集上训练用于图像识别的卷积神经网络。有人告诉我，采用这种架构和从均值为 0、标准差为 0.01 的高斯分布得出的初始权重以及初始偏差值为 0 的情况，在适当的学习率下，它的准确度应该达到 30 左右%。
但是，它根本没有学到任何东西：准确度与随机分类器相似，并且训练后的权重仍然遵循正态分布。我做错了什么？
这是神经网络
class simpleCNN(nn.Module)：
  def __init__(自身):
    super(simpleCNN,self).__init__() #初始化模型

    self.conv1=nn.Conv2d(in_channels=1,out_channels=8,kernel_size=3,stride=1) #输出图像大小为(size+2*padding-kernel)/stride --&gt;62*62
    self.relu1=nn.ReLU()
    self.maxpool1=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像62/2--&gt;31*31

    self.conv2=nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,stride=1) #输出图像为29*29
    self.relu2=nn.ReLU()
    self.maxpool2=nn.MaxPool2d(kernel_size=2,stride=2) #输出图像为29/2--&gt;14*14（MaxPool2d近似大小与floor）

    self.conv3=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1) #输出图像为12*12
    self.relu3=nn.ReLU()

    self.fc1=nn.Linear(32*12*12,15) #16 个通道 * 16*16 图像（64*64，步幅为 2 的 2 个 maxpooling），15 个输出特征=15 个类
    self.softmax = nn.Softmax(dim=1)

  def 前向（自身，x）：
    x=self.conv1(x)
    x=self.relu1(x)
    x=self.maxpool1(x)

    x=self.conv2(x)
    x=self.relu2(x)
    x=self.maxpool2(x)

    x=self.conv3(x)
    x=self.relu3(x)

    x=x.view(-1,32*12*12)

    x=self.fc1(x)
    x=self.softmax(x)

    返回x

初始化：
def init_weights(m):
  如果 isinstance(m,nn.Conv2d) 或 isinstance(m,nn.Linear)：
    nn.init.normal_(m.weight,0,0.01)
    nn.init.zeros_(m.bias)

模型 = simpleCNN()
模型.应用（init_weights）

训练函数：
loss_function=nn.CrossEntropyLoss()
优化器=optim.SGD(model.parameters(),lr=0.1,动量=0.9)

def train_one_epoch(epoch_index,loader):
  运行损失=0

  对于 i，枚举（加载器）中的数据：

    input,labels=data #获取小批量
    输出=模型（输入）#前向传递

    loss=loss_function(outputs,labels) #计算损失
    running_loss+=loss.item() #总结到目前为止处理的小批量的损失

    Optimizer.zero_grad() #重置梯度
    loss.backward() #计算梯度
    optimizer.step() #更新权重

  return running_loss/(i+1) # 每个小批量的平均损失


培训：
&lt;前&gt;&lt;代码&gt;纪元=20

best_validation_loss=np.inf

对于范围内的纪元（EPOCHS）：
  print(&#39;纪元{}:&#39;.format(纪元+1))

  模型.train(True)
  train_loss=train_one_epoch(epoch,train_loader)

  运行验证损失=0.0

  模型.eval()

  with torch.no_grad(): # 禁用梯度计算并减少内存消耗
    对于 i，枚举中的 vdata（validation_loader）：
      vinputs,vlabels=vdata
      v输出=模型（v输入）
      vloss=loss_function(v输出,v标签)
      running_validation_loss+=vloss.item()
  验证损失=运行验证损失/(i+1)
  print(&#39;LOSS 训练：{} 验证：{}&#39;.format(train_loss,validation_loss))

  if validation_loss
使用默认初始化，它的效果会好一点，但我应该使用高斯达到 30%。
您能发现一些可能导致它无法学习的问题吗？我已经尝试过不同的学习率和动力。]]></description>
      <guid>https://stackoverflow.com/questions/77704108/convolutional-neural-network-not-learning</guid>
      <pubDate>Fri, 22 Dec 2023 14:06:23 GMT</pubDate>
    </item>
    <item>
      <title>如何使用自定义数据集格式训练自定义 Transformer 模型</title>
      <link>https://stackoverflow.com/questions/77680618/how-to-train-a-customized-transformer-model-with-custom-dataset-formatting</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77680618/how-to-train-a-customized-transformer-model-with-custom-dataset-formatting</guid>
      <pubDate>Mon, 18 Dec 2023 16:56:58 GMT</pubDate>
    </item>
    <item>
      <title>神经先知根本不预测？</title>
      <link>https://stackoverflow.com/questions/77231544/neural-prophet-not-predicting-at-all</link>
      <description><![CDATA[我正在尝试预测进入某个海滩的顾客数量。因此，数据中的数字往往会波动，并且希望使用 Neural Prophet 来预测未来的客人。然而，根据我的神经先知模型的当前设置，该模型根本无法预测原始数据中的最终日期，尽管它确实预测了其他参数，只是不准确。
以下是预测结果（虚线）与原始结果（实线）的对比：
在此处输入图片描述
我特意要求预测未来 100 天，但根本没有显示。
这是我的模型设置：
 模型 = NeuralProphet(
        #growth=“off”, # 确定趋势类型：&#39;线性&#39;、&#39;不连续&#39;、&#39;关闭&#39;
        #changepoints=None, #可能包含变更点的日期列表（None -&gt;automatic ）
        n_changepoints=0,
        #changepoints_range=0,
        #trend_reg=0,
        # trend_reg_threshold=False,
        # #seasonality_reg=1,
        # # d_hidden = 0,
        n_lags=10,
        # # num_hidden_​​layers=0, # AR-Net 隐藏层的维度
        # # ar_reg=None, # AR 系数的稀疏性
        学习率=0.01，
        纪元=100，
        Normalize=“auto”, # 标准化类型 (&#39;minmax&#39;, &#39;standardize&#39;, &#39;soft&#39;, &#39;off&#39;)
        impute_missing=真，
        yearly_seasonality=真，
        week_seasonality=假，
        daily_seasonality=假，
        季节性_模式=“乘法”，
        loss_func=“均方误差”,
    ）

    # 将模型与训练数据进行拟合
    model.fit(数据,频率=“D”)
    未来= model.make_future_dataframe（数据，周期= 1000，n_historic_predictions = len（数据））
    预测 = model.predict(future)
]]></description>
      <guid>https://stackoverflow.com/questions/77231544/neural-prophet-not-predicting-at-all</guid>
      <pubDate>Wed, 04 Oct 2023 16:52:25 GMT</pubDate>
    </item>
    <item>
      <title>如何动态地将curl变量发送给管道工函数？</title>
      <link>https://stackoverflow.com/questions/61824959/how-to-send-curl-variables-to-plumber-function-dynamically</link>
      <description><![CDATA[我想根据任意数量的输入变量动态调用管道工 API。我需要将curl 输入映射到函数名称的输入。例如，如果函数有一个输入 hi，则 curl -s --data &#39;hi=2&#39; 意味着 hi=2 应该是作为输入参数传递给函数。这可以直接在 R 中使用 match.call() 完成，但在通过管道工 API 调用它时失败。
获取函数
&lt;前&gt;&lt;代码&gt;#&#39; @post /API
#&#39; @serializer unboxedJSON
tmp &lt;- 函数(hi) {

  输出 &lt;- 列表(hi=hi)

  out &lt;- toJSON(out, Pretty = TRUE, auto_unbox = TRUE)

  返回（出）

}

tmp(嗨=2)
输出：{嗨：2}

然后
curl -s --data &#39;hi=10&#39; http://127.0.0.1/8081/API
输出: {\n \&quot;hi\&quot;: \&quot;2\&quot;\n}

一切看起来都不错。但是，取函数
&lt;前&gt;&lt;代码&gt;#&#39; @post /API
#&#39; @serializer unboxedJSON
tmp &lt;- 函数(...) {

  out &lt;- match.call() %&gt;%
         as.list() %&gt;%
         .[2:长度(.)] # %&gt;%

  out &lt;- toJSON(out, Pretty = TRUE, auto_unbox = TRUE)

  返回（出）

}
tmp(嗨=2)
输出：{嗨：2}

然后
curl -s --data &#39;hi=10&#39; http://127.0.0.1/8081/API
out: {“错误”:“500 - 内部服务器错误”,“消息”:“错误: 没有方法 asJSON S3 类: R6\n”}

实际上，我真正想做的是加载我的 ML 模型以使用管道工 API 预测分数。例如
model &lt;- readRDS(&#39;model.rds&#39;) # 将模型加载为全局变量

预测得分 &lt;- 函数(...) {
    
    df_in &lt;- match.call() %&gt;%
        as.list() %&gt;%
        .[2:长度(.)] %&gt;%
        as.data.frame()

    json_out &lt;- 列表(
        Score_out = 预测(模型, df_in) %&gt;%
        toJSON(., 漂亮 = T, auto_unbox = T)

    返回（json_out）
}


此函数在本地运行时按预期工作，但通过 curl -s --data &#39;var1=1&amp;var2=2...etc&#39; http://listen_address 通过 API 运行&lt; /p&gt;
我收到以下错误：
&lt;块引用&gt;
{“错误”：“500 - 内部服务器错误”，“消息”：“as.data.frame.default(x[[i]]，可选 = TRUE) 中的错误：无法强制类“c(“PlumberResponse”，“R6”)”到 data.frame\n&quot;}
]]></description>
      <guid>https://stackoverflow.com/questions/61824959/how-to-send-curl-variables-to-plumber-function-dynamically</guid>
      <pubDate>Fri, 15 May 2020 17:22:08 GMT</pubDate>
    </item>
    </channel>
</rss>