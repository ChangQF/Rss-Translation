<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 10 Feb 2024 09:13:17 GMT</lastBuildDate>
    <item>
      <title>LSTM 模型显示 X 有 6 个特征，但 MinMaxScaler 期望有 7 个特征作为输入</title>
      <link>https://stackoverflow.com/questions/77972253/lstm-model-shows-x-has-6-features-but-minmaxscaler-is-expecting-7-features-as-i</link>
      <description><![CDATA[我正在构建一个 LSTM 模型，该模型可以随时间分析六个变量。但是，我的代码抛出错误。您能指导我在哪里更改我的代码吗？我还有一个采用这种格式的 ARIMA 模型。这种格式对于分析风速是否正确？我是数据分析新手，我有一个任务将 ARIMA 模型与 LSTM 模型组装起来。但是，我无法完成这个 LSTM 模型。
随机导入
进口警告

将 numpy 导入为 np
将 pandas 导入为 pd
从 keras.layers 导入 LSTM，密集
从 keras.models 导入顺序
从 keras.optimizers 导入 Adam
从 sklearn.preprocessing 导入 MinMaxScaler

# 从 CSV 文件加载数据集
file_path = &#39;孟加拉国天气数据 (1948 - 2013).csv&#39;
df = pd.read_csv(文件路径)

# 如果有日期列，请确保 DataFrame 按日期排序
如果 df.columns 中有“日期”：
    df[&#39;日期&#39;] = pd.to_datetime(df[&#39;日期&#39;])
    df.sort_values(&#39;日期&#39;, inplace=True)

# 预定义的特征和目标变量
selected_features = [&#39;最高温度&#39;、&#39;最低温度&#39;、&#39;降雨量&#39;、&#39;相对湿度&#39;、&#39;云量覆盖&#39;、&#39;明亮阳光&#39;]
目标=&#39;风速&#39;

# 要求用户输入所选特征的值
用户输入 = {}
对于 selected_features 中的功能：
    value = float(input(f&quot;输入 {feature} 值：&quot;))
    用户输入[特征] = 值

# 要求用户输入年、月、站的值
user_input[&#39;YEAR&#39;] = int(input(&quot;请输入年份：&quot;))
user_input[&#39;月份&#39;] = int(input(&quot;请输入月份：&quot;))
user_input[&#39;车站名称&#39;] = input(&quot;输入车站：&quot;)

# 使用占位符值将“Wind_Speed”列添加到 user_input_df
user_input_df = pd.DataFrame({**user_input, &#39;Wind_Speed&#39;: [0]})

# 禁用特定警告
warnings.simplefilter(“忽略”, UserWarning)
warnings.simplefilter(“忽略”, FutureWarning)

尝试：
    # 确保 df[target] 有合适的索引
    如果不是 isinstance(df.index, pd.RangeIndex):
        df.reset_index(drop=True, inplace=True)

    # 对每个特征使用 MinMaxScaler 进行特征缩放
    缩放器 = MinMaxScaler()

    scaled_data = scaler.fit_transform(df[selected_features + [目标]])


    # 创建用于 LSTM 训练的序列
    序列长度 = 10
    x_train, y_train = [], []

    对于范围内的 i（sequence_length，len（scaled_data））：
        x_train.append(scaled_data[i - 序列长度:i, :-1])
        y_train.append(scaled_data[i, -1])

    x_train, y_train = np.array(x_train), np.array(y_train)

    # 重塑 LSTM 的输入数据
    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))

    # 构建 LSTM 模型
    模型=顺序（）
    model.add(LSTM(单位=100, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))
    model.add(LSTM(单位=100，return_sequences=True))
    model.add(LSTM(单位=50, return_sequences=False))
    model.add(密集(单位=1))

    # 编译模型
    model.compile(优化器=Adam(learning_rate=0.001), loss=&#39;mean_squared_error&#39;)

    # 训练模型
    model.fit(x_train、y_train、epochs=5、batch_size=16、validation_split=0.1)

    # 准备用于预测的输入数据
    输入=scaled_data[-sequence_length:, :-1]
    输入 = 缩放器.transform(输入)
    输入=输入.reshape(1,sequence_length,len(selected_features))


    ＃ 作出预测
    预测 = model.predict(输入)
    预测 =scaler.inverse_transform(预测.reshape(-1, 1))


    # 引入额外的随机性
    random_perturbation = random.uniform(-2, 0.5) # 根据需要调整范围
    预测+=随机扰动

    print(f&#39;预测下一个周期的{目标}：{预测[0, 0]}&#39;)

    #MAPE计算
    random_mape = random.uniform(12, 14)
    print(f&#39;估计平均绝对百分比误差: {random_mape:.2f}%&#39;)

    # 将用户输入和预测输出合并到一个新的 DataFrame 中
    output_data = pd.concat([user_input_df, pd.DataFrame({目标: [预测[0, 0]]})], axis=1)

    # 将输入数据和预测输出保存到新的 CSV 文件中
    输出文件路径 = &#39;LSTM_Predictions_Output.csv&#39;
    输出数据.to_csv（输出文件路径，索引=False）

    print(f&#39;输入数据和预测输出保存到{output_file_path}&#39;)


除了 ValueError 为 e：
    打印（f&#39;错误：{e}&#39;）

最后：
    warnings.resetwarnings()
]]></description>
      <guid>https://stackoverflow.com/questions/77972253/lstm-model-shows-x-has-6-features-but-minmaxscaler-is-expecting-7-features-as-i</guid>
      <pubDate>Sat, 10 Feb 2024 07:45:08 GMT</pubDate>
    </item>
    <item>
      <title>如何在训练过程中随机裁剪图像并协调标签？</title>
      <link>https://stackoverflow.com/questions/77972180/how-to-random-crop-image-and-coordinate-label-during-training</link>
      <description><![CDATA[我的任务是头影测量地标定位。
我在此数据框中显示坐标 X1,Y1 的图像路径。

&lt;标题&gt;

文件名
X1
Y1


&lt;正文&gt;

/Images_data/binary0006.png
89
80


/Images_data/binary0008.png
37
70


/Images_data/binary0007.png
50
76


/Images_data/binary0003.png
55
92


/Images_data/binary0005.png
91
64


/Images_data/binary0004.png
100
76



训练时如何裁剪图像并坐标X1,Y1？]]></description>
      <guid>https://stackoverflow.com/questions/77972180/how-to-random-crop-image-and-coordinate-label-during-training</guid>
      <pubDate>Sat, 10 Feb 2024 07:13:06 GMT</pubDate>
    </item>
    <item>
      <title>CTkLabel.__init__() 缺少 1 个必需的位置参数：'master'</title>
      <link>https://stackoverflow.com/questions/77972082/ctklabel-init-missing-1-required-positional-argument-master</link>
      <description><![CDATA[这是我的代码：-
我正在尝试运行此代码，但收到了不应出现的错误，因为我没有指定任何主参数，请帮我解决它
将 tkinter 导入为 tk
将 customtkinter 导入为 ctk

进口火炬
将 numpy 导入为 np

导入CV2
从 PIL 导入图像、ImageTk
导入VLC
随机导入


应用程序 = tk.Tk()
app.geometry(“600x600”)
app.title(“昏昏欲睡的 Boi 4.0”)
ctk.set_appearance_mode(“深色”)

vidFrame = tk.Frame(高度=480，宽度=600)
vidFrame.pack()
vid = ctk.CTkLabel(vidFrame)
视频.pack()

计数器 = 0
counterLabel = ctk.CTkLabel(text=counter, height=40, width=120, text_font=(“Arial”, 20), text_color=“white”, fg_color=“teal”)
counterLabel.pack(pady=10)

def Reset_counter():
    全局计数器
    计数器 = 0
resetButton = ctk.CTkButton(text=“重置计数器”, command=reset_counter, height=40, width=120, text_font=(“Arial”, 20), text_color=“白色”, fg_color=“青色” ）

错误：
counterLabel = ctk.CTkLabel(text=counter, height=40, width=120, text_font=(“Arial”, 20), text_color=“white”, fg_color=“teal”)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^
类型错误：CTkLabel.__init__() 缺少 1 个必需的位置参数：&#39;master&#39;

我希望它能够正常运行！
由于没有名为 master 的位置参数，如果我错了，请告诉我要添加什么参数以及可能的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/77972082/ctklabel-init-missing-1-required-positional-argument-master</guid>
      <pubDate>Sat, 10 Feb 2024 06:30:19 GMT</pubDate>
    </item>
    <item>
      <title>是否可以训练神经网络对包含特定形状的像素进行分组？</title>
      <link>https://stackoverflow.com/questions/77971748/is-it-possible-to-train-a-neural-network-to-group-pixels-containing-a-particular</link>
      <description><![CDATA[我想训练一个模型，可以训练该模型对形状物体（例如图像中的牙齿）进行分组。如果它接收到图像作为输入，它应该输出一个包含子列表的列表，每个子列表包含检测到的牙齿的像素。因此，如果输入图像在处理前被展平，则输出列表应如下所示：
[（像素1，像素2，像素3，...），（像素4，像素5，像素6，...），...]，
其中pixel1、pixel2等表示属于牙齿的像素的索引（假设输入图像首先被展平）。
没有牙齿的图像应该有一个空列表作为输出。
每个训练数据样本都应该有一个图像和一个牙齿列表。
我不太确定如何构建一个神经网络来按照我希望的方式执行此任务（如果可能的话）。
我最初的方法是训练一个单独的模型，将一个小像素网格（如 24x24）分类为“牙齿”或“非牙齿”，然后扫描更大的输入图像（可能是 200x200）并对以下部分进行分组归类为牙齿。
有更好的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/77971748/is-it-possible-to-train-a-neural-network-to-group-pixels-containing-a-particular</guid>
      <pubDate>Sat, 10 Feb 2024 02:50:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 XGboost 进行时间序列异常检测</title>
      <link>https://stackoverflow.com/questions/77971354/anomaly-detection-on-time-series-by-using-xgboost</link>
      <description><![CDATA[我需要找到时间序列中的异常情况，对于这项任务我选择了机器学习方法，即 XGboost。
有一个数据集，包含 2参数：秒和系列值（分贝）。
问题：从PrepareData()函数获得的y_test由于某种原因为空，我不明白为什么会发生这种情况。您有解决问题的想法吗？
代码：
def code_mean(data, cat_feature, real_feature):
     ”“”
     返回一个字典，其中键是 cat_feature 的唯一类别，
     这些值是基于 real_feature 的平均值
     ”“”
    返回 dict(data.groupby(cat_feature)[real_feature].mean())

进行聚合
df1 = pd.read_csv(&quot;data.csv&quot;, sep=&#39;,&#39;)
df1.columns = [&#39;时间,秒&#39;,&#39;y&#39;]

df1[&#39;时间，秒&#39;] = df1[&#39;时间，秒&#39;]*10000
df1[&#39;时间，秒&#39;] = pd.to_datetime(df1[&#39;时间，秒&#39;],unit=&#39;s&#39;)
df1.set_index(&#39;时间,秒&#39;, inplace=True)
aggregate_df1 = df1.resample(&#39;H&#39;).mean()

数据 = pd.DataFrame(aggregate_df1)
data.index = pd.to_datetime(data.index)
数据[“小时”] = data.index.小时
数据[“工作日”] = data.index.工作日
数据[&#39;is_weekend&#39;] = data.weekday.isin([5,6])*1
数据.head()

该函数将立即返回分为训练和测试的数据集和目标变量。我猜这就是所有问题所在：
def prepareData(数据, lag_start=5, lag_end=20, test_size=0.15):

    数据 = pd.DataFrame(data.copy())
    数据.列 = [“y”]

    test_index = int(len(数据)*(1-test_size))

    对于范围内的 i（lag_start，lag_end）：
        data[“lag_{}”.format(i)] = data.y.shift(i)

    data.index = pd.to_datetime(data.index)
    数据[“小时”] = data.index.小时
    数据[“工作日”] = data.index.工作日
    数据[&#39;is_weekend&#39;] = data.weekday.isin([5,6])*1

    数据[&#39;weekday_average&#39;] = data[&#39;weekday&#39;].apply(lambda x: code_mean(data[:test_index], &#39;weekday&#39;, &quot;y&quot;).get(x))
    data[&quot;hour_average&quot;] = data[&#39;hour&#39;].apply(lambda x: code_mean(data[:test_index], &#39;hour&#39;, &quot;y&quot;).get(x))

    data.drop([“小时”,“工作日”], axis=1, inplace=True)

    数据 = data.dropna()
    数据 = data.reset_index(drop=True)

    # разбиваем весь датасет на тренировочную и тестовую выборку
    X_train = data.loc[:test_index].drop([“y”], axis=1)
    y_train = data.loc[:test_index][“y”]
    X_test = data.loc[test_index:].drop([“y”], axis=1)
    y_test = data.loc[test_index:][“y”]
    
    返回X_train，X_test，y_train，y_test

最后是预测的主要函数：
def XGB_forecast（数据，lag_start=5，lag_end=20，test_size=0.15，scale=1.96）：

    X_train、X_test、y_train、y_test = 准备数据（aggregate_df1、lag_start、lag_end、test_size）
    dtrain = xgb.DMatrix(X_train, 标签=y_train)
    dtest = xgb.DMatrix(X_test)

    参数 = {
        &#39;目标&#39;: &#39;reg:线性&#39;,
        &#39;助推器&#39;：&#39;gb线性&#39;
    }
    树 = 1000

    cv = xgb.cv(params, dtrain, 指标 = (&#39;rmse&#39;), verbose_eval=False, nfold=10, show_stdv=False, num_boost_round=trees)

    bst = xgb.train(params, dtrain, num_boost_round=cv[&#39;test-rmse-mean&#39;].argmin())

    #cv.plot(y=[&#39;test-mae-mean&#39;, &#39;train-mae-mean&#39;])

    偏差 = cv.loc[cv[&#39;test-rmse-mean&#39;].argmin()][&quot;test-rmse-mean&quot;]

    Prediction_train = bst.predict(dtrain)
    plt.figure(figsize=(15, 5))
    plt.plot(预测训练)
    plt.plot(y_train)
    plt.axis(&#39;紧&#39;)
    plt.网格（真）

    Prediction_test = bst.predict(dtest)
    下=预测测试规模*偏差
    上=预测_测试+尺度*偏差

    如果 len(y_test) &gt; 0:
        异常 = np.array([np.NaN]*len(y_test))
        异常[y_test&lt;下限] = y_test[y_test&lt;下限]
    别的：
        异常 = np.array([])

    plt.figure(figsize=(15, 5))
    plt.plot(prediction_test, label=&quot;预测&quot;)
    plt.plot(lower, &quot;r--&quot;, label=&quot;上键/下键&quot;)
    plt.plot(上，“r--”)
    plt.plot(列表(y_test), label=“y_test”)
    plt.plot（异常，“ro”，markersize=10）
    plt.legend(loc=&quot;最佳&quot;)
    plt.axis(&#39;紧&#39;)
    plt.title(“XGBoost 平均绝对误差{}用户”.format(round(mean_absolute_error(prediction_test, y_test))))
    plt.网格（真）
    plt.图例()

函数调用：XGB_forecast(aggreerated_df1, test_size=0.2, lag_start=5, lag_end=30)]]></description>
      <guid>https://stackoverflow.com/questions/77971354/anomaly-detection-on-time-series-by-using-xgboost</guid>
      <pubDate>Fri, 09 Feb 2024 23:30:21 GMT</pubDate>
    </item>
    <item>
      <title>C# 中的机器学习 [关闭]</title>
      <link>https://stackoverflow.com/questions/77971147/machine-learning-in-c-sharp</link>
      <description><![CDATA[我应该构建一个机器学习模型来充当文档过滤器。在手头的数据集中，我有 3 种不同类型的文档，它们不应通过过滤器。该模型总共将处理 20 种不同类型的文档，最终目标是减轻手动处理这三种类型的负担。
我刚刚开始这个项目，我想在我自己探索该领域的同时，向社区寻求潜在方法的建议。训练数据相当小，在标记类型之间分布为 500、430、80。
该项目最好用 C# 编写。
到目前为止，我只从数据库中收集了文档并验证了它们是否被正确标记，并探索了各种方法，例如支持向量机、朴素拜耳、无监督学习方法和文本数据的各种预处理步骤。
微调预训练模型似乎很流行（迁移学习），也是一种潜在的方法，尤其是在数据集有限的情况下。但有些文档相对广泛，三种类型中的一种包含大约 3000 个单词，据我所知，这对于可用的预训练模型可能会出现问题，并且在 .net 中实现的可能性有限。
因此，目前我正着手构建一个传统模型，并征求您的意见和意见。]]></description>
      <guid>https://stackoverflow.com/questions/77971147/machine-learning-in-c-sharp</guid>
      <pubDate>Fri, 09 Feb 2024 22:21:26 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中使用注意力机制的双向 RNN/LSTM/GRU</title>
      <link>https://stackoverflow.com/questions/77971041/bidirectional-rnn-lstm-gru-using-attention-in-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77971041/bidirectional-rnn-lstm-gru-using-attention-in-pytorch</guid>
      <pubDate>Fri, 09 Feb 2024 21:52:40 GMT</pubDate>
    </item>
    <item>
      <title>训练随机森林分类器</title>
      <link>https://stackoverflow.com/questions/77970974/train-a-random-forest-classifier</link>
      <description><![CDATA[我正在尝试在一组图像上训练随机森林分类器，旨在对 10 种不同的颜色类别进行分类。我有一个带有子目录的 \dataset 目录，每个子目录都以一种颜色（黄色、红色、白色等）命名。我采用 HSV 直方图并将其用作我的特征（它比 RGB 更好吗？）
这是我的代码。我在代码中添加了 print(image_path) 进行调试。但运行时，它只打印第一个文件，不打印其他任何内容。
有什么问题吗？如何解决？
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.feature_extraction.image 导入 extract_patches_2d
从 sklearn.model_selection 导入 train_test_split
从 skimage.color 导入 rgb2hsv，rgb2gray
从 skimage.feature 导入graycomatrix，graycoprops
导入操作系统
将 numpy 导入为 np
从 PIL 导入图像
导入作业库


# 定义路径和参数
data_dir =“.\数据集” # 替换为你的实际目录路径
num_trees = 100 # 森林中树木的数量
patch_size = (32, 32) # 用于特征提取的图像块的大小

# 定义特征提取函数
def extract_color_histogram(图像):
    # 将 PIL 图像转换为 NumPy 数组
    img_array = np.array(图像)
    # 调整图像大小
    img_resized = Image.fromarray(img_array).resize((640, 640))
    # 转换为 HSV
    img_hsv = img_resized.convert(“HSV”)
    hist, _ = np.histogram(img_hsv, bins=(8, 8, 8), 范围=((0, 255), (0, 255), (0, 255)))
    hist = hist.flatten()
    返回历史记录

def extract_texture_features(图像):
    # 转换为灰度并提取纹理特征
    灰色 = rgb2gray(图像)
    gray_uint = (gray * 255).astype(np.uint8) # 将灰度转换为无符号整数
    glcm = Graycomatrix(gray_uint，距离=[5]，角度=[0]，级别=256，对称=True，normed=True)
    特征 = Graycoprops(glcm, &#39;对比度&#39;)[0]
    返回特征

# 加载数据并提取特征
X、y = []、[]
对于 os.listdir(data_dir) 中的颜色：
    对于 os.listdir(os.path.join(data_dir, color)) 中的 image_file：
        image_path = os.path.join(data_dir, 颜色, image_file)
        打印（图像路径）
        img = Image.open(图像路径)
        img_array = np.array(img)
        补丁 = extract_patches_2d(img_array, patch_size=patch_size)
        对于补丁中的补丁：
            # 从每个补丁中提取特征
            color_hist = extract_color_histogram(补丁)
            纹理特征=提取纹理特征（补丁）
            特征 = np.concatenate((color_hist,texture_features))
            X.append（功能）
            y.追加（颜色）

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练随机森林模型
print(&quot;现在训练...&quot;)
模型 = RandomForestClassifier(n_estimators=num_trees, random_state=42)
model.fit(X_train, y_train)

# 评估模型性能
准确度 = model.score(X_test, y_test)
print(f&quot;模型精度: {accuracy:.2f}&quot;)

# 保存模型以供将来使用
joblib.dump(模型,“color_model.pkl”)
]]></description>
      <guid>https://stackoverflow.com/questions/77970974/train-a-random-forest-classifier</guid>
      <pubDate>Fri, 09 Feb 2024 21:33:17 GMT</pubDate>
    </item>
    <item>
      <title>LLM 的数据访问已损坏。想法？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77970854/data-access-for-llms-is-broken-thoughts</link>
      <description><![CDATA[为了构建真正有用的 GenAI 应用程序，法学硕士需要能够从结构化和非结构化数据源访问专有数据。法学硕士很难了解检索相关数据的可用性、位置和方法。
关键问题：

法学硕士无法确定是否有必要的数据可用于回答任何数据源中的用户查询。
如果数据可用，法学硕士无法找到要从哪个数据源检索。
如果来源已知，为众多检索协议编写检索管道就会变得复杂、重复且不确定。

您在项目中遇到过这些问题吗？您发现哪些有效的变通方法或解决方案？有什么新工具或实践可以简化法学硕士与不同数据源的集成吗？
一些人建议微调或 RAG 作为潜在的解决方案，但是：

使用特定数据微调模型的成本很高，而且会随着最新数据而过时，并且存在内置的访问控制问题。持续的再培训成本高昂，而且跟不上实时数据变化的步伐。
RAG 不适用于结构化数据。大多数有用的数据都是结构化的，可以分布在多个数据源中，每个数据源都有自己的查询机制。
编写单独的检索管道是有限且复杂的，并且不能确保确定性、安全性和访问控制。
]]></description>
      <guid>https://stackoverflow.com/questions/77970854/data-access-for-llms-is-broken-thoughts</guid>
      <pubDate>Fri, 09 Feb 2024 21:01:59 GMT</pubDate>
    </item>
    <item>
      <title>如何创建目标变量[关闭]</title>
      <link>https://stackoverflow.com/questions/77969840/how-to-create-target-variable</link>
      <description><![CDATA[我尝试预测 ncaa 篮球疯狂游行的每个结果。
我有过去 20 场左右锦标赛的历史数据，并且有 team1_score 和 team2_score 等列。我认为创建一个目标变量很容易，只需创建一个列 team1_win 并在 true 时返回 1，否则返回 0。问题是我的数据是经过组织的，因此 team1 始终是获​​胜团队。所以我的目标变量列将只包含 1。对于二元分类来说，这似乎是一个问题。我不确定如何创建目标变量。我是否需要以某种方式重新整理我的数据，以便 team1 并不总是获胜团队？我的目标变量全为1有问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77969840/how-to-create-target-variable</guid>
      <pubDate>Fri, 09 Feb 2024 17:22:22 GMT</pubDate>
    </item>
    <item>
      <title>如何在 WSL GPU 支持下运行 Windows Python 代码？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77969677/how-to-run-windows-python-code-with-wsl-gpu-support</link>
      <description><![CDATA[我尝试在 Windows PS 上的 WSL 2 发行版中使用 TensorFlow。我的问题是，我只能在互联网上找到描述安装过程的页面，但没有人解释如何在 Windows 上使用 GPU 加速从我的 virtual-env 环境运行代码。那么有没有办法在 WSL Distrubition 中远程运行机器学习脚本呢？
我已经成功安装了WSL2，包括miniconda和tensorflow（带有CUDA）。 Tensorflow Feedback 线也发现 GPU 没有问题。]]></description>
      <guid>https://stackoverflow.com/questions/77969677/how-to-run-windows-python-code-with-wsl-gpu-support</guid>
      <pubDate>Fri, 09 Feb 2024 16:52:00 GMT</pubDate>
    </item>
    <item>
      <title>UserWarning：X 没有有效的功能名称，但 KNeighborsClassifier 配备了功能名称 warnings.warn</title>
      <link>https://stackoverflow.com/questions/77804804/userwarning-x-does-not-have-valid-feature-names-but-kneighborsclassifier-was-f</link>
      <description><![CDATA[ ID 曾经_已婚 毕业 性别 职业 支出_分数细分 家庭_身材 年龄 工作_经历
0 462809 0 0 1 5 2 3 3 4 1
1 462643 1 1 0 2 0 0 2 18 15
2 466315 1 1 0 2 2 1 0 44 1
3 461735 1 1 1 7 1 1 1 44 0
4 462669 1 1 0 3 1 0 5 20 15
……………………………………
8063 464018 0 0 1 9 2 3 6 4 0
8064 464685 0 0 1 4 2 3 3 15 3
8065 465406 0 1 0 5 2 3 0 14 1
8066 467299 0 1 0 5 2 1 3 8 1
8067 461879 1 1 1 4 0 1 2 17 0
8068行×10列

data1=data.drop([&quot;ID&quot;,&quot;分段&quot;],axis=1)

从 sklearn.model_selection 导入 train_test_split
     x_train,x_test,y_train,y_test=train_test_split(data1,data.Segmentation,test_size=0.20,random_state=50)

 从 sklearn.neighbors 导入 KNeighborsClassifier
 knn=KNeighborsClassifier(n_neighbors=17)
 knn.fit(x_train,y_train)
 tahmin=knn.predict(x_test)

 knn.score(x_test,y_test)
 #0.4838909541511772
 knn.predict([[1,1,0,2,0,2,18,15]])

 UserWarning：X 没有有效的功能名称，但 KNeighborsClassifier 已安装了功能名称
  #警告.警告(
数组([1])

当我做出预测时，我并没有预料到这个警告。]]></description>
      <guid>https://stackoverflow.com/questions/77804804/userwarning-x-does-not-have-valid-feature-names-but-kneighborsclassifier-was-f</guid>
      <pubDate>Fri, 12 Jan 2024 06:45:16 GMT</pubDate>
    </item>
    <item>
      <title>基于机器学习的边缘检测器</title>
      <link>https://stackoverflow.com/questions/43299083/machine-learning-based-edge-detector</link>
      <description><![CDATA[我已阅读以下内容关于使用机器学习进行边缘检测的博客。他们

&lt;块引用&gt;
  使用了基于现代机器学习的算法。该算法是在图像上进行训练的，其中人类注释了最重要的边缘和对象边界。给定这个标记数据集，训练机器学习模型来预测图像中每个像素属于对象边界的概率。

我想使用 opencv 来实现这项技术。
有人知道或知道如何使用 Opencv 实现/开发此方法吗？
我们如何注释最重要的边缘和对象边界以供机器学习算法使用？]]></description>
      <guid>https://stackoverflow.com/questions/43299083/machine-learning-based-edge-detector</guid>
      <pubDate>Sat, 08 Apr 2017 18:51:27 GMT</pubDate>
    </item>
    <item>
      <title>r 神经网络包——多输出</title>
      <link>https://stackoverflow.com/questions/34663573/r-neuralnet-package-multiple-output</link>
      <description><![CDATA[我目前使用神经网络的方式是它从许多输入点预测一个输出点。更具体地说，我运行以下命令。
nn &lt;- 神经网络(
as.公式(a ~ c + d),
数据 = Z，隐藏 = c（3,2），err.fct =“sse”，act.fct = 自定义，
线性.输出=真，重复= 5）

这里，如果 Z 是一个由名称为 a、b、c 的列组成的矩阵，它将根据 c 行和 d 行中的对应点预测 a 列中某一行的一个点。 （以垂直维度作为训练样本。）
假设还有一列 b。我想知道是否有办法从 c 和 d 预测 a 和 b？我已经尝试过
as.formula(a+b ~ c+d)

但这似乎不起作用。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/34663573/r-neuralnet-package-multiple-output</guid>
      <pubDate>Thu, 07 Jan 2016 19:29:54 GMT</pubDate>
    </item>
    <item>
      <title>监督学习，(ii) 无监督学习，(iii) 强化学习</title>
      <link>https://stackoverflow.com/questions/15782956/supervised-learning-ii-unsupervised-learning-iii-reinforcement-learn</link>
      <description><![CDATA[在阅读有关监督学习、无监督学习、强化学习的内容时，我遇到了以下问题并感到困惑。请帮助我识别以下三个中哪一个是监督学习、无监督学习、强化学习。
什么类型的学习（如果有）最能描述以下三种场景：
(i) 为自动售货机创建硬币分类系统。为此，
开发商从美国造币厂获取准确的硬币规格并得出
尺寸、重量和面额的统计模型，自动售货机
然后机器对其硬币进行分类。
(ii) 算法不是调用美国造币厂来获取硬币信息，而是
赠送一大套贴有标签的硬币。该算法使用该数据来
推断自动售货机随后用来对其进行分类的决策边界
硬币。
(iii) 计算机通过重复玩井字游戏制定策略
并通过惩罚最终导致失败的举动来调整策略。]]></description>
      <guid>https://stackoverflow.com/questions/15782956/supervised-learning-ii-unsupervised-learning-iii-reinforcement-learn</guid>
      <pubDate>Wed, 03 Apr 2013 09:00:48 GMT</pubDate>
    </item>
    </channel>
</rss>