<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 23 Jan 2024 01:03:19 GMT</lastBuildDate>
    <item>
      <title>使用多维训练集 MATLAB 通过 RNN 进行预测</title>
      <link>https://stackoverflow.com/questions/77863462/make-predictions-with-an-rnn-using-a-multi-dimensional-training-set-matlab</link>
      <description><![CDATA[我有一个训练数据的二维矩阵TD，它是N个非线性信号的集合，这些信号是时间的函数（因此训练的规范设置在这篇文章的标题中）。让M表示每个信号的时间步数。每个信号共享相同的M。因此，TD 是一个 MxN 矩阵。我想在整个训练集上训练我的神经网络，以计算我的输出权重矩阵 Wout。 然后我想对单独的非线性信号 yTest 进行预测code&gt;，这不是使用训练后的 Wout 的 TD 中已有的 N 信号之一。然后，我想将尺寸为 1xM 的预测输出 yNN 与尺寸为 1xM 的 yTest 进行比较&gt; 查看预测的准确度。
我的问题是我当前的 yTest 和 yNN 的尺寸均为 NxM 而不是 1xM。我的目标是在所有 N 训练集信号上训练神经网络，并使用考虑所有 Wout 进行单一 1xM 预测&gt;N 非线性信号。附带说明一下，Wout 的维度为 DxN，其中 D 是我的神经网络中的节点数。
我不确定我所问的是否可能。 MATLAB 中有没有办法在计算 WoutTD 从 MxN 矩阵展平或重塑为 Mx1 矩阵&gt;？这样，Wout 将是一个 Dx1 矩阵，预测输出 yNN 将是一个 1xM 矩阵，就像我想要的那样成为？]]></description>
      <guid>https://stackoverflow.com/questions/77863462/make-predictions-with-an-rnn-using-a-multi-dimensional-training-set-matlab</guid>
      <pubDate>Tue, 23 Jan 2024 00:25:21 GMT</pubDate>
    </item>
    <item>
      <title>为一次性样本创建二元分类器[关闭]</title>
      <link>https://stackoverflow.com/questions/77863144/creating-a-binary-classifier-for-one-shot-samples</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77863144/creating-a-binary-classifier-for-one-shot-samples</guid>
      <pubDate>Mon, 22 Jan 2024 22:33:25 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：调用层“volvingal_attention_5”时遇到异常（类型 ConvolutionalAttention）</title>
      <link>https://stackoverflow.com/questions/77863006/valueerror-exception-encountered-when-calling-layer-convolutional-attention-5</link>
      <description><![CDATA[我正在尝试将卷积注意力模块合并到 ResNet-lstm 中以进行视网膜疾病分类。注意力网络是为了增强模型的可解释性。在使用注意力模块之前，模型运行得很好，但现在出现了错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-49-e65909934d0b&gt;在&lt;细胞系：2&gt;()
      1 # 构建并编译双眼模型
----&gt; 2 model_left = build_resnet_attention_model()
      3 model_right = build_resnet_attention_model()
      4 model_left.compile(optimizer=“adam”,loss=“categorical_crossentropy”,metrics=[“accuracy”,“Recall”])
      5 model_right.compile(optimizer=“adam”,loss=“categorical_crossentropy”,metrics=[“accuracy”,“Recall”])

2帧
tf__call(self, x) 中的 /tmp/__autograph_ generated_fileohrwq7eu.py
     11 e = ag__.converted_call(ag__.ld(K).tanh, (ag__.converted_call(ag__.ld(K).dot, (ag__.ld(reshape_input), ag__.ld(self).W), 无, fscope) + ag__.ld(self).b,), 无, fscope)
     12 e = ag__.converted_call(ag__.ld(K).softmax, (ag__.ld(e),), dict(axis=1), fscope)
---&gt; 13 e = ag__.converted_call(ag__.ld(K).reshape, (ag__.ld(e), (-1, ag__.converted_call(ag__.ld(K).int_shape, (ag__.ld(x),) , 无, fscope)[1], ag__.converted_call(ag__.ld(K).int_shape, (ag__.ld(x),), 无, fscope)[2], 1)), 无, fscope)
     14 Weighted_input = ag__.converted_call(ag__.ld(multiply), ([ag__.ld(x), ag__.ld(e)],), 无, fscope)
     15 尝试：

ValueError：调用层“卷积_attention_5”时遇到异常（类型卷积注意力）。

在用户代码中：

    文件“”，第 20 行，调用 *
        e = K.reshape(e, (-1, K.int_shape(x)[1], K.int_shape(x)[2], 1)) # 重塑以匹配 (None, 7, 7, 1)
    文件“/usr/local/lib/python3.10/dist-packages/keras/src/backend.py”，第 3611 行，重塑
        返回 tf.reshape(x, 形状)

    ValueError：维度大小必须能被 49 整除，但对于具有输入形状的 &#39;{{node convolutional_attention_5/Reshape_1}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](convolutional_attention_5/Softmax, convolutional_attention_5/Reshape_1/shape)&#39; 来说，维度大小为 7： [7,1]、[4] 以及计算为部分形状的输入张量：input[1] = [?,7,7,1]。


调用层“卷积_注意力_5”接收的参数（类型卷积注意力）：
  x=tf.Tensor(形状=(无, 7, 7, 2048), dtype=float32)
]]></description>
      <guid>https://stackoverflow.com/questions/77863006/valueerror-exception-encountered-when-calling-layer-convolutional-attention-5</guid>
      <pubDate>Mon, 22 Jan 2024 21:54:54 GMT</pubDate>
    </item>
    <item>
      <title>了解并使用 catboost 的增量回归</title>
      <link>https://stackoverflow.com/questions/77862605/understanding-and-using-incremental-regression-with-catboost</link>
      <description><![CDATA[我看到了这个例子（Catboost训练模型用于使用 catboost 进行分类的大数据（~22GB）和多个块），并尝试使其适应增量多元回归，但我一直在旋转，它产生了不同的不清楚的错误。
from catboost import CatBoostRegressor
从 catboost 导入池
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split

clf = CatBoostRegressor(task_type=&quot;CPU&quot;,
                     迭代=1000，
                     loss_function=&#39;MultiRMSE&#39;,
                     学习率=0.5，
                     最大深度=7)
chunk=pd.read_csv(&#39;./train2/DataTableTrain.tsv&#39;,sep=&#39;\t&#39;,chunksize=10000000)
对于枚举（块）中的 i，ds：
    X = Pool(ds, column_description=&#39;./train-7-7.cd&#39;, delimiter=&#39;\t&#39;);
    如果我==0：
        clf.fit(X,column_description=&#39;./train-7-7.cd&#39;)
    别的：
        clf.fit(X, init_model=&#39;model.bin&#39;,
                column_description=&#39;./train-7-7.cd&#39;)
    clf.save_model(&#39;model.bin&#39;) # 保存模型以便在下一步中加载
    德尔X

列描述文件如下所示：
&lt;前&gt;&lt;代码&gt;0 标签
1 标签
2 标签
3 标签
4 标签
5 标签
6 标签
7 号
8 号
9 号
10 号
11 号
12 号
13 号

数据文件是每行数字的 14 个制表符分隔的字符串表示形式。
目前它抱怨“如果指定了column_description参数，数据应该是字符串或pathlib.Path类型。”有人可以指导我了解该软件想要什么吗？或者提供一个可以增量处理文件以进行多重回归的示例 python 应用程序。
另外，您能解释一下增量在培训中的含义吗？对于我已经成功尝试过的示例，该模型似乎只是在每个块的末尾附加了一个新模型]]></description>
      <guid>https://stackoverflow.com/questions/77862605/understanding-and-using-incremental-regression-with-catboost</guid>
      <pubDate>Mon, 22 Jan 2024 20:35:20 GMT</pubDate>
    </item>
    <item>
      <title>如何使用变压器的管道处理大文件</title>
      <link>https://stackoverflow.com/questions/77862602/how-to-process-large-file-using-pipeline-from-transformers</link>
      <description><![CDATA[我使用管道从 Hugging Face 加载模型：
设备 =“cuda:0” if torch.cuda.is_available() else “cpu”
torch_dtype = torch.float16 如果 torch.cuda.is_available() else torch.float32

model_id =“openai/whisper-large-v3”；

模型 = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id、torch_dtype=torch_dtype、low_cpu_mem_usage=True、use_safetensors=True
）
模型.to（设备）

处理器 = AutoProcessor.from_pretrained(model_id)

管道 = 管道（
    “自动语音识别”，
    型号=型号，
    tokenizer=处理器.tokenizer,
    feature_extractor=处理器.feature_extractor,
    最大新令牌=128，
    块长度_s = 20，
    批量大小=16，
    return_timestamps=真，
    torch_dtype=torch_dtype,
    设备=设备，
）

我想处理大音频文件
结果=管道(“文件名3.mp3”)

但是谷歌协作说我的内存不足。是否可以将输出通过管道传输到文件而不是变量？]]></description>
      <guid>https://stackoverflow.com/questions/77862602/how-to-process-large-file-using-pipeline-from-transformers</guid>
      <pubDate>Mon, 22 Jan 2024 20:35:09 GMT</pubDate>
    </item>
    <item>
      <title>支持向量机如何与 AdaBoost 结合使用？</title>
      <link>https://stackoverflow.com/questions/77862576/how-can-support-vector-machine-be-used-with-adaboost</link>
      <description><![CDATA[AdaBoost 是一种通过使用一个简单模型（例如称为弱分类器）创建多个模型的想法。弱分类器可以是决策树中的树桩，甚至可以是支持向量机形式的线性支持向量机方程y = w*x + b 其中 w 是一个向量。
假设我们选择线性支持向量机作为弱分类器。假设我们从这些数据开始
% 创建数据
x = [-5 + randn(90, 2); 5 + 随机数(10, 2); 10 + 随机数(90, 2); randn(10, 2)];

% 创建标签
y = [linspace(-1, -1, 100)&#39;; linspace(1, 1, 100)&#39;];

％ 阴谋
分散(x(y == -1, 1), x(y == -1, 2));
坚持，稍等
散点(x(y == 1, 1), x(y == 1, 2));

然后我们找到第一条分类边界线
% 调整参数
C = 1.0;
拉姆达 = 1;

% 执行线性 SVM
[w, b, 精度, 解] = mi.lsvm(x, y, C, lambda);

% 这条线应该有多长
min_value_column_1 = min(x(:,1));
max_value_column_1 = max(x(:,1));

% 创建分隔线 y = k*x + m
x1 = linspace(min_value_column_1, max_value_column_1);
x2 = (-w(1)*x1 - b) / w(2);

（正在使用的软件是MataveID）
然后我们绘制数据边界线
% 绘制分隔线
绘图（x1，x2，&#39;k&#39;，&#39;线宽&#39;，2）；
legend(&#39;A类&#39;, &#39;B类&#39;, &#39;分离&#39;, &#39;位置&#39;, &#39;西北&#39;)


我们可以看到大的红色和蓝色散点被正确分离，但小的散点却没有。
想法：
我想使用 AdaBoost 和 SVM 来分离小散点。我有个主意！
如果我整理出正确分类的点，并且仅使用未正确分类的点重新训练 svm 模型。
% 评估
correect_classified = y == 符号(x*w&#39; + b);

% 创建仅包含未正确分类的点的新 x
x = x(correect_classified == 0, :);

% 与 y 相同
y = y(correect_classified == 0);

% 执行线性 SVM
[w, b, 精度, 解] = mi.lsvm(x, y, C, lambda);

% 这条线应该有多长
min_value_column_1 = min(x(:,1));
max_value_column_1 = max(x(:,1));

% 创建分隔线 y = k*x + m
x1 = linspace(min_value_column_1, max_value_column_1);
x2 = (-w(1)*x1 - b) / w(2);

% 绘制分隔线
绘图（x1，x2，&#39;k&#39;，&#39;线宽&#39;，2）；
legend(&#39;A类&#39;, &#39;B类&#39;, &#39;分离&#39;, &#39;位置&#39;, &#39;西北&#39;)

并且创建了另一条边界线。
问题：
这就是 SVM 与 AdaBoost 一起使用的方式吗？
]]></description>
      <guid>https://stackoverflow.com/questions/77862576/how-can-support-vector-machine-be-used-with-adaboost</guid>
      <pubDate>Mon, 22 Jan 2024 20:29:00 GMT</pubDate>
    </item>
    <item>
      <title>用于预测 DST 索引的 LSTM [关闭]</title>
      <link>https://stackoverflow.com/questions/77862458/lstm-for-predict-dst-index</link>
      <description><![CDATA[我无法用我的神经网络实现这一点：
测试集 2017 年 1 月 1 日至 2020 年 12 月 31 日期间的 100 个随机小时 对于每个随机小时 T，给出 T 和之前 359 小时的 DST 和太阳风属性值。 ( (100,360,8) 张量)
预测 100 小时中每个小时 T+1h、T+6h、T+12h 的 DST（(100,3) 张量）。
具体来说，我很难预测 T 的值，因为我无法有效地使用训练集。
我有两个数据集：trainset.csv（有 8 个值，最后一个是 DST 索引）和 testset.npy (100,360, 8)
我的 LSTM 架构：
在此处输入图像描述
有人可以以任何方式帮助我吗？]]></description>
      <guid>https://stackoverflow.com/questions/77862458/lstm-for-predict-dst-index</guid>
      <pubDate>Mon, 22 Jan 2024 19:59:45 GMT</pubDate>
    </item>
    <item>
      <title>LogisitcLoss 类中的梯度函数[关闭]</title>
      <link>https://stackoverflow.com/questions/77861514/gradient-function-in-logisitcloss-class</link>
      <description><![CDATA[我正在从头开始编写 XGBoost 代码，我指的是这个存储库 此处
对数损失函数由下式给出
对数损失函数
关于 y_pred 区分上述函数（指上面存储库中 LogisitcLoss() 下的确切变量），我得到的梯度为 (y - p) 但代码显示 -(y - p) .
我从损失函数中导出了梯度，看起来答案的符号与我得到的结果相反。我想知道我的损失函数是否正确。如果我的损失函数正确，我还想知道找到梯度的分步解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/77861514/gradient-function-in-logisitcloss-class</guid>
      <pubDate>Mon, 22 Jan 2024 17:03:32 GMT</pubDate>
    </item>
    <item>
      <title>极端类不平衡的稳健机器学习模型的数学和编码[关闭]</title>
      <link>https://stackoverflow.com/questions/77861478/mathematics-and-coding-for-robust-machine-learning-models-for-extreme-class-imba</link>
      <description><![CDATA[我的任务是解决二元分类问题中的极端类别不平衡问题，其中正类仅占总样本的 0.05%。假设您有一个包含 N 个样本和 D 个特征的数据集。
数学（50 分）：推导明确考虑类别不平衡的成本函数的数学公式。加入正则化术语并解释它如何有助于防止过度拟合。为成本函数中的每一项提供逐步推导和基本原理。
算法设计（30 分）：提出一种新颖的算法或对现有算法进行修改，专门针对极端类别不平衡的情况。提供伪代码或关于您的方法如何工作的详细算法描述。突出显示所有关键超参数及其重要性。
编码实现（20 分）：使用您选择的编程语言（例如 Python）实现您提出的算法。将算法应用于提供的数据集，并确保您的代码有完整的文档记录。使用适当的指标和可视化技术评估模型性能。讨论结果和任何观察到的权衡。
数学：
我试图推导出一个成本函数来解释极端的类别不平衡，并结合一个正则化项来防止过度拟合。我期望对成本函数中的每个术语进行全面、逐步的解释，展示对机器学习数学基础的深刻理解。
算法设计：
我提出了一种新颖的算法或对现有算法的修改，专门解决极端的类别不平衡问题。我期望提供清晰详细的算法描述，包括伪代码，并强调关键超参数的重要性。我的目的是展示创造性思维和对算法设计原理的深入理解。
编码实现：
我使用我选择的编程语言实现了所提出的算法，确保代码有良好的文档记录。我将该算法应用于提供的数据集，使用适当的指标和可视化技术评估模型性能。我希望展示强大的编码技能，展示所提出的算法的应用，并讨论观察到的结果和任何权衡。]]></description>
      <guid>https://stackoverflow.com/questions/77861478/mathematics-and-coding-for-robust-machine-learning-models-for-extreme-class-imba</guid>
      <pubDate>Mon, 22 Jan 2024 16:55:06 GMT</pubDate>
    </item>
    <item>
      <title>python 中没有库的 LSTM 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/77861406/lstm-model-with-no-libraries-in-python</link>
      <description><![CDATA[我想使用 numpy 和 pandas 在 python 中创建一个 LSTM 机器学习模型，该模型用于音乐推荐引擎，该引擎采用 Spotify 音乐功能接收特定长度的流媒体历史记录。在 python 中使用 OOP 解决这个问题最有效的方法是什么，我想将它与预训练的模型进行比较，看看它在专门为此训练时是否表现得更好。我在这个领域相对较新，这是一个学校项目。
# 导入库
将 numpy 导入为 np
将 pandas 导入为 pd


班级网络：
    def __init__(自身):
        ”“”
        用于机器学习算法形成建议的神经网络
        ”“”
        # 导入训练数据
        self.data = np.array(pd.read_csv(&#39;train_clean.csv&#39;))
        # 获取数据数组的大小
        self.m, self.n = self.data.shape
        # 随机化训练数据的顺序
        np.random.shuffle(self.data)


网络=网络（）

这是我开始使用的类布局，任何有关如何解决此问题的想法将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/77861406/lstm-model-with-no-libraries-in-python</guid>
      <pubDate>Mon, 22 Jan 2024 16:42:56 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 R 中的 iml 包在经过训练的随机森林模型上创建 2D 部分依赖图？</title>
      <link>https://stackoverflow.com/questions/77860945/how-to-create-a-2d-partial-dependence-plot-on-a-trained-random-forest-model-usin</link>
      <description><![CDATA[我使用 R 中的 mlr3 包训练了 randomForest 模型，并成功使用 iml 包创建了部分依赖 (PDP) 和累积局部效应 (ALE) 的一维图来解释训练后的 ML 模型。我已经看到可以使用《可解释机器学习》一书中的 iml 包创建 2D 绘图（这里）其中写到其中使用了iml包（不幸的是，似乎没有提供代码）。但是，我真的找不到如何创建 2D 绘图（例如要使用哪个函数/参数），也找不到任何 2D 代码示例。
这是我创建一维图的示例代码（主要来自 iml 手册）。
库(“randomForest”)
库（“iml”）

# 在波士顿数据集上训练随机森林回归：
数据（“波士顿”，包=“MASS”）
rf &lt;- randomForest(medv ~ ., 数据 = 波士顿, ntree = 50)
mod &lt;- Predictor$new(rf, 数据 = 波士顿)

# 等离子显示
eff &lt;- FeatureEffects$new(mod, method=“pdp”)
eff$plot(features = c(“lstat”,“rm”))

# 啤酒
eff &lt;- FeatureEffects$new(mod, method=“ale”)
eff$plot(features = c(“lstat”,“rm”))
]]></description>
      <guid>https://stackoverflow.com/questions/77860945/how-to-create-a-2d-partial-dependence-plot-on-a-trained-random-forest-model-usin</guid>
      <pubDate>Mon, 22 Jan 2024 15:25:34 GMT</pubDate>
    </item>
    <item>
      <title>在本地部署 Azure autoML 模型</title>
      <link>https://stackoverflow.com/questions/77859735/deploying-azure-automl-model-locally</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77859735/deploying-azure-automl-model-locally</guid>
      <pubDate>Mon, 22 Jan 2024 12:07:02 GMT</pubDate>
    </item>
    <item>
      <title>wandb：如何在高级图例部分显示分组平均值</title>
      <link>https://stackoverflow.com/questions/77858891/wandb-how-to-show-grouped-avarage-in-the-advanced-legend-section</link>
      <description><![CDATA[我正在努力记录我的种子机器学习训练的等待和偏差，我可以看到我所做的事情的良好可视化。
在高级图例部分我可以看到。
[[ ${mean} σ ${stddev} (${min}, ${max}) ]] ${run:displayName}
其中 [[ ${mean} σ ${stddev} (${min}, ${max}) ]] 部分在我悬停时可见。
如何将此值附加到如下所示的文本
 [[ ${mean} σ ${stddev} (${min}, ${max}) ]] ${run:displayName} - ${mean} σ ${stddev} 
感谢您的帮助]]></description>
      <guid>https://stackoverflow.com/questions/77858891/wandb-how-to-show-grouped-avarage-in-the-advanced-legend-section</guid>
      <pubDate>Mon, 22 Jan 2024 09:41:42 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Pandas 对同一 column_id 进行多行 One-hot 编码？</title>
      <link>https://stackoverflow.com/questions/45556281/how-to-do-one-hot-encoding-with-multiple-rows-for-same-column-id-using-pandas</link>
      <description><![CDATA[我有一个 df 格式：
 User_id 技能
0 1 蟒蛇
1 1 爪哇
2 4 爪哇

正在做
df=pd.concat([df,pd.get_dummies(df[&#39;skill&#39;],prefix=&#39;skill&#39;)],axis=1)
 df

输出：
 User_id Skill_python Skill_java
0 1 1 0
1 1 0 1
2 4 0 1

我想获得以下格式的输出：
 User_id Skill_python Skill_java
0 1 1 1
1 4 0 1

如何使用 pandas 做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/45556281/how-to-do-one-hot-encoding-with-multiple-rows-for-same-column-id-using-pandas</guid>
      <pubDate>Mon, 07 Aug 2017 21:54:32 GMT</pubDate>
    </item>
    <item>
      <title>参加 Microsoft Research 句子完成挑战 [已关闭]</title>
      <link>https://stackoverflow.com/questions/44373470/get-the-microsoft-research-sentence-completion-challenge</link>
      <description><![CDATA[我正在研究用于学术目的的自然语言处理，我想获取 Microsoft Research Sentence Completion Challenge 数据集。
不幸的是，它似乎不再可用在微软网站：当我点击两个链接中的任何一个来获取训练或测试数据时，我会被重定向到微软研究院的主页。我尝试联系微软的技术支持，但他们没有回复我，而且我在其他网站上也找不到该数据集。
在哪里可以找到这个数据集（我主要对测试集感兴趣）？]]></description>
      <guid>https://stackoverflow.com/questions/44373470/get-the-microsoft-research-sentence-completion-challenge</guid>
      <pubDate>Mon, 05 Jun 2017 16:30:07 GMT</pubDate>
    </item>
    </channel>
</rss>