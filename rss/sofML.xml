<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 03 Aug 2024 09:16:44 GMT</lastBuildDate>
    <item>
      <title>如何预处理和准备用于细菌分割的微观水样图像？[关闭]</title>
      <link>https://stackoverflow.com/questions/78828010/how-to-preprocess-and-prepare-microscopic-water-sample-images-for-bacterial-segm</link>
      <description><![CDATA[我正在开展一个机器学习项目，旨在使用显微图像测试水的纯度。该项目的目标是：
对样本图像中存在的各种细菌进行分割。
识别不同类型的细菌。
根据识别出的细菌数量和类型评估水的纯度。
我很难找到并准备一个包含不同类型细菌的水样显微图像的合适数据集。
鉴于我的任务的特殊性，我应该采取什么方法来微调像 SAM（任何分割模型）这样的预训练模型来进行细菌分割？任何关于超参数、数据增强或其他训练策略的提示都会有所帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78828010/how-to-preprocess-and-prepare-microscopic-water-sample-images-for-bacterial-segm</guid>
      <pubDate>Sat, 03 Aug 2024 06:34:35 GMT</pubDate>
    </item>
    <item>
      <title>“AttributeError:‘NoneType’对象没有属性‘cget_managed_ptr’”是什么意思？</title>
      <link>https://stackoverflow.com/questions/78827974/what-does-the-attributeerror-nonetype-object-has-no-attribute-cget-managed</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78827974/what-does-the-attributeerror-nonetype-object-has-no-attribute-cget-managed</guid>
      <pubDate>Sat, 03 Aug 2024 06:12:06 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中有效地将大型 .txt 文件拆分为训练集和测试集？</title>
      <link>https://stackoverflow.com/questions/78827762/how-can-i-efficiently-split-a-large-txt-file-into-training-and-test-sets-in-pyt</link>
      <description><![CDATA[我有一个非常大的 .txt 文件（几 GB），我需要将其拆分为机器学习项目的训练集和测试集。由于内存限制，将整个文件读入内存然后拆分的常用方法不可行。我正在寻找一种高效拆分文件而不使内存过载的方法。
我尝试使用 scikit-learn 进行拆分，但它会将整个文件加载到内存中，这会导致性能问题，不适合我的大型数据集。]]></description>
      <guid>https://stackoverflow.com/questions/78827762/how-can-i-efficiently-split-a-large-txt-file-into-training-and-test-sets-in-pyt</guid>
      <pubDate>Sat, 03 Aug 2024 03:36:13 GMT</pubDate>
    </item>
    <item>
      <title>独热编码掩码的 resample_poly</title>
      <link>https://stackoverflow.com/questions/78827743/resample-poly-of-one-hot-encoded-masking</link>
      <description><![CDATA[我有这些张量：
X_test = X_unseen_flutter[0,0,:][None, :] # (批次大小，振幅长度) -&gt; (1, 3208)
y_true = y_unseen_flutter[0,0,:][None, :] # (批次大小，掩码长度，类别数量) -&gt; (1, 3208, 4) (独热编码)

我可以对 X_test 进行重新采样，但我不知道 y_true：
from scipy.signal import resample_poly

X_test_resampled = resample_poly(X_test, up=512, down=3208, axis=1) # (1, 512)
y_true_resampled = # ??? 我期望形状 (1, 512, 4)

除了独热编码标签外，resample_poly 的等价物是什么？
我希望有一个函数可以做到这一点，它接受 tensor, up, down, mask_axis, class_axis]]></description>
      <guid>https://stackoverflow.com/questions/78827743/resample-poly-of-one-hot-encoded-masking</guid>
      <pubDate>Sat, 03 Aug 2024 03:21:27 GMT</pubDate>
    </item>
    <item>
      <title>CSV 与 Pandas Dataframe 之间的转换不正确</title>
      <link>https://stackoverflow.com/questions/78827542/csv-to-from-pandas-dataframe-not-transforming-correctly</link>
      <description><![CDATA[我有一个 csv 文件，其中包含标题、文本和 url 列的新闻文章。当我将文件导入 pandas df 时，长度似乎与 csv 文件中的行数不同。经过检查，我注意到一些文章被分成第二行，并进一步分成许多额外的列，所有 url 都位于单个文章的“第二”行的某个较远的列中。Pandas 正确地解释了这个问题，并合并了文本并将 url 放在正确的列下。
虽然 Pandas 正确地解释了这一点，但我无法将 df（清理后）保存到新的 csv，因为新的清理后的 csv 文件不会反映相同的问题，这会破坏清理后的特征工程和 NLP 任务。这些“额外”的行完全搞砸了诸如计算单词/动词/名词以及获取每个句子的被动语态和音节之类的事情。我尝试过检测/删除换行符（\n 和 \r\n），但没有用。我在读取 csv 时尝试过不同的编码和引用值（见下文），但没有用。我尝试过合并行，但做不到，因为 pandas 看不到 csv 中显示的“第二”行。
我遗漏了什么吗？知道发生了什么吗？
df = pd.read_csv(&#39;filepath.csv&#39;, encoding=&#39;utf-8&#39;, engine=&#39;python&#39;, on_bad_lines=&#39;skip&#39;, quoting=1)

df= df.rename(columns={&#39;Headline&#39;: &#39;title&#39;, &#39;Article text&#39;: &#39;text&#39;, &#39;Url&#39;: &#39;url&#39;})

df= df[[&#39;title&#39;, &#39;text&#39;, &#39;url&#39;]]

df[&#39;text&#39;] = df[&#39;text&#39;].str.replace(&#39;\r\n&#39;, &#39; &#39;, regex=True)
df.to_csv(&#39;df_investigation.csv&#39;, index=False) 
]]></description>
      <guid>https://stackoverflow.com/questions/78827542/csv-to-from-pandas-dataframe-not-transforming-correctly</guid>
      <pubDate>Fri, 02 Aug 2024 23:42:35 GMT</pubDate>
    </item>
    <item>
      <title>无法抑制来自 transformers/src/transformers/modeling_utils.py 的警告</title>
      <link>https://stackoverflow.com/questions/78827482/cant-suppress-warning-from-transformers-src-transformers-modeling-utils-py</link>
      <description><![CDATA[我对 AutoModel AutoTokenizer 类的实现相当简单：
from transformers import AutoModel, AutoTokenizer
import numpy as np
from rank_bm25 import BM25Okapi
from sklearn.neighbors import NearestNeighbors

class EmbeddingModels:

def bert(self, model_name, text):
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
input = tokenizer(text, return_tensors=&quot;pt&quot;, truncation=True, padding=True)
output = model(**inputs)
embeddings = output.last_hidden_​​state.mean(dim=1).detach().numpy()
return embeddings

def create_chunks(self, text, chunk_size):
return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

但我无法让这个警告消失：
包含“beta”的参数名称将在内部重命名为“bias”。
请使用其他名称来抑制此警告。
包含“gamma”的参数名称将在内部重命名为“weight”。
请使用其他名称来抑制此警告。

我的存储库中没有任何地方提到 beta 或 gamma 这个词。
更新软件包，使用 import warnings 抑制警告]]></description>
      <guid>https://stackoverflow.com/questions/78827482/cant-suppress-warning-from-transformers-src-transformers-modeling-utils-py</guid>
      <pubDate>Fri, 02 Aug 2024 23:04:25 GMT</pubDate>
    </item>
    <item>
      <title>VAE：潜在分布。后部塌陷，多个潜在 [关闭]</title>
      <link>https://stackoverflow.com/questions/78827397/vae-latent-distribution-posterior-collapse-multiple-latents</link>
      <description><![CDATA[在探索 VAE 一段时间后，我有两个问题。在标准 VAE 设置中，我们假设 1 个形状为 (BHWD) 的潜在变量：mu 和 var，以及先验 N(0, I)。

潜在分布：我阅读了一些关于卡方分布的资料，想知道潜在变量的 L2 范数 (B,) 是否是潜在变量呈高斯分布的良好指标。在标准 VAE 训练中，我发现它的值稳定在 (D-1)**0.5 附近，这符合中心卡方分布的描述。接下来的问题是，如果 L2 范数不等于预期值，我们可以说潜在分布比高斯分布更复杂吗？

后验崩溃：（1）后验崩溃的症状是什么？它是否必须严格为：mu~0、var~1 和 KL~0？（2）如何解释 var 的平均值。我观察到它也受到 D 的影响。此外，如果极小的 var 表示模型对输入非常确定，爆炸的 var 会告诉我们编码器缺乏能力？一般来说，我们更喜欢较小的 var 还是存在一个理想值。

给定一张图像，我尝试将其转换为 Y/UV 通道并分别学习两组潜在值。具体来说，我对两者应用了标准流程：使用一个/两个编码器为 Y 和 UV 输入生成 mu 和 var，分别计算两个相对于正常 proir 的 KL，对两者进行后验采样，在解码之前将它们连接在一起。解码器的工作是重建原始 RGB 图像。 我想，如果 VAE 能够为图像构建可插值/平滑的潜在空间，它也应该能够处理 Y/UV 潜在值并将来自同一图像的潜在值对齐。 不幸的是，我在实验中没有观察到它。重建很好，但潜在的统计数据（为方便起见，我将它们标记为 1 和 2）非常混乱。 (1) 很难从 mu1 和 mu2 读取，因为它们的值非常接近 0。但是，我总是能发现 var2 爆炸（请参阅我在第 2 点中提出的问题），可能高达 200。 (2) 通常，var1 看起来更像高斯，因为它的 L2 范数在 (d-1)**0.5 处收敛，但 var2 的值稍大一些（请参阅我在第 1 点中提出的问题）。 (3) 我还计算了 mu1 和 mu2 之间的 cos 相似度 和 L2 距离。它们大多是正交的，这与高维向量自然彼此正交的说法相符。并且 L2(mu1, mu2) 和 L2(mu2, origin) 具有相似的值，大于 L1(mu1, origin)。消化这些统计数据的正确方法是什么？或者一般来说，VAE 框架不适合学习两个独立但相关的高斯类潜在变量？


进行了大量实验并尝试更好地理解高维潜在空间。]]></description>
      <guid>https://stackoverflow.com/questions/78827397/vae-latent-distribution-posterior-collapse-multiple-latents</guid>
      <pubDate>Fri, 02 Aug 2024 22:17:58 GMT</pubDate>
    </item>
    <item>
      <title>除非 label_mode=None，否则图像目录无法打开</title>
      <link>https://stackoverflow.com/questions/78827338/image-directory-failing-to-open-unless-label-mode-none</link>
      <description><![CDATA[除非 image_dataset_from_directory 的 label_mode 参数为 None，否则我的图像目录无法打开。
我尝试将类型切换为“int”、“categorical”和“binary”，但无济于事。我还尝试为该类制作一个包装器，然后稍后切换属性（事后看来，这根本行不通），但这也行不通。我需要以某种方式将其保持在 None 之外，因为我稍后会评估生成器的准确性，而 model.evaluate() 无法处理 None 值。这是我目前的代码：
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import tensorflow
from tensorflow import keras
from keras import layer, preprocessing, Sequential
from sklearn.neighbors import KernelDensity
import glob

class CustomImageDataset:
def __init__(self, directory, image_size, batch_size, label_mode):
self.dataset = tensorflow.keras.preprocessing.image_dataset_from_directory(
directory,
image_size=image_size,
batch_size=batch_size,
label_mode=label_mode
)
self.label_mode = label_mode

def __iter__(self):
return iter(self.dataset)

def __len__(self):
return len(self.dataset)

def map(self, *args, **kwargs):
返回 self.dataset.map(*args, **kwargs)

def batch(self, *args, **kwargs):
返回 self.dataset.batch(*args, **kwargs)

def prefetch(self, *args, **kwargs):
返回 self.dataset.prefetch(*args, **kwargs)

SIZE = 8
batch_size = 64

train_generator = preprocessing.image_dataset_from_directory(
r&#39;C:\Users\{}\Downloads\archive (1)\noncloud_train&#39;, 
image_size=(SIZE, SIZE),
batch_size=batch_size,
label_mode=None
)

validation_generator = preprocessing.image_dataset_from_directory(
r&#39;C:\Users\{}\Downloads\archive (1)\noncloud_test&#39;,
image_size=(SIZE, SIZE),
batch_size=batch_size,
label_mode=None
)

anomaly_generator = CustomImageDataset(
r&#39;C:\Users\{}\Downloads\archive (1)\cloud&#39;,
image_size=(SIZE, SIZE),
batch_size=batch_size,
label_mode=None
)

rescaling_layer = layer.Rescaling(1./255)

def change_inputs(images):
x = tensorflow.image.resize(rescaling_layer(images),[SIZE, SIZE], method=tensorflow.image.ResizeMethod.NEAREST_NEIGHBOR)
return x, x

# 对数据集应用预处理
train_dataset = train_generator.map(change_inputs)
validation_dataset = validation_generator.map(change_inputs)
anomaly_dataset = anomaly_generator.map(change_inputs)

test = anomaly_generator.label_mode
def check_none_in_dataset(dataset):
for batch in dataset:
images, labels = batch
if images is None or labels is None:
print(&quot;Found None in dataset&quot;)
return True
print(&quot;No None values in dataset&quot;)
return False

# 检查验证数据集
print(&quot;正在检查验证数据集中是否有 None 值：&quot;)
c = check_none_in_dataset(validation_dataset)
print(c)

def print_labels_from_dataset(dataset, num_batches=1):
for images, labels in dataset.take(num_batches):
print(&quot;标签（应与images):&quot;)
print(labels.numpy()) # 打印标签以检查它们是否是预期值（不是 None）
print(labels.numpy() == images.numpy())

print(&quot;验证数据集标签：&quot;)
bat = print_labels_from_dataset(validation_dataset)

print(&quot;异常数据集标签：&quot;)
cow = print_labels_from_dataset(anomaly_dataset)

model = Sequential()
# 编码器
model.add(layers.Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;,input_shape=(SIZE, SIZE, 3)))
model.add(layers.MaxPooling2D((2, 2),padding=&#39;same&#39;)) 
model.add(layers.Conv2D(32, (3, 3),激活=&#39;relu&#39;，padding=&#39;same&#39;))
model.add(layers.MaxPooling2D((2, 2)，padding=&#39;same&#39;))
model.add(layers.Conv2D(16, (3, 3)，激活=&#39;relu&#39;，padding=&#39;same&#39;))
model.add(layers.MaxPooling2D((2, 2)，padding=&#39;same&#39;))

# Deconder
model.add(layers.Conv2D(16, (3, 3)，激活=&#39;relu&#39;，padding=&#39;same&#39;))
model.add(layers.UpSampling2D((2, 2)))
model.add(layers.Conv2D(32, (3, 3)，激活=&#39;relu&#39;，padding=&#39;same&#39;))
model.add(layers.UpSampling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;))
model.add(layers.UpSampling2D((2, 2)))

model.add(layers.Conv2D(3, (3, 3), 激活=&#39;sigmoid&#39;, 填充=&#39;same&#39;))

model.compile(optimizer=&#39;adam&#39;, 损失=&#39;mean_squared_error&#39;, 指标=[&#39;mse&#39;])
model.summary()

history = model.fit(
train_dataset,
steps_per_epoch = 1500 // batch_size,
epochs = 1000,
validation_data = validation_dataset,
validation_steps = 225 // batch_size,
shuffle = True
)

# 检查侦察。 val 数据和异常图像之间的错误
validation_error = model.evaluate(validation_generator)
anomaly_error = model.evaluate(anomaly_generator)
]]></description>
      <guid>https://stackoverflow.com/questions/78827338/image-directory-failing-to-open-unless-label-mode-none</guid>
      <pubDate>Fri, 02 Aug 2024 21:47:44 GMT</pubDate>
    </item>
    <item>
      <title>是否有关于所使用的 LLM 模型及其用于创意写作 AI 的架构的信息？（例如 Sudowrite、NovelAI Storyteller）[关闭]</title>
      <link>https://stackoverflow.com/questions/78827337/is-there-any-information-on-the-llm-models-used-and-their-architecture-for-creat</link>
      <description><![CDATA[我找不到任何有关创意写作的 LLM 模型（如 Sudowrite）的信息。由于它们专门用于创意写作，因此它们比 ChatGPT 效果好得多。我想了解如何构建像 Sudowrite 这样的 LLM，因为它们特别擅长帮助创意写作。
我研究了与不同创意写作 ais 相关的模型架构，但找不到太多信息。或者可能收集的数据。]]></description>
      <guid>https://stackoverflow.com/questions/78827337/is-there-any-information-on-the-llm-models-used-and-their-architecture-for-creat</guid>
      <pubDate>Fri, 02 Aug 2024 21:47:19 GMT</pubDate>
    </item>
    <item>
      <title>如何针对 JavaScript 框架微调轻量级 LLM？[关闭]</title>
      <link>https://stackoverflow.com/questions/78827245/how-can-i-fine-tune-a-lightweight-llm-specifically-for-javascript-frameworks</link>
      <description><![CDATA[我需要专门针对 JavaScript 堆栈微调一个模型。目前，所有大型 LLM 都是通用的，但我想要一个仅针对我每天使用的 JavaScript 框架进行训练的自定义模型。我的目标是拥有一个小型、高效的模型，它可以在我的 MacBook Pro 上运行并严格协助编码任务。以下是我正在寻找的具体功能：

代码完成
代码解释
代码文档（可选）
代码审查和优化建议（基于最佳实践）
错误查找和修复
特定于框架的响应（例如，指定时响应仅限于 Express.js）
针对编码问题的交互式问答
了解我的代码库并提供上下文答案

我的目标是将其打造成一个开源项目，而不依赖主要提供商的昂贵解决方案。以下是我向社区提出的具体问题：

基础模型选择：我应该选择哪种基础模型？该模型应该是轻量级的、能够进行微调的，并且针对编码任务进行了优化，没有不必要的功能。
指导与基础模型：如果我想与它进行对话式交互，我应该选择基础模型还是指导就绪模型？
微调方法：根据我的需求微调模型的最佳方法是什么？
训练资源：训练模型的最佳资源是什么？
数据集：我应该使用哪些数据集进行训练，包括最佳实践的文档？我找到了 SRI Lab 的 150k JavaScript 数据集，但我不确定它的质量。有人可以提供见解或推荐其他数据集吗？
硬件要求：我有一台 MacBook Pro M2，内存为 32 GB。这足以进行训练吗，还是我需要付费解决方案？如果是，您会推荐哪些经济实惠的解决方案？
其他注意事项：在此过程中我还应该考虑其他因素吗？
]]></description>
      <guid>https://stackoverflow.com/questions/78827245/how-can-i-fine-tune-a-lightweight-llm-specifically-for-javascript-frameworks</guid>
      <pubDate>Fri, 02 Aug 2024 21:08:50 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中影响年度数据的多个月份模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/78827204/model-for-multiple-months-that-impact-an-annual-number-in-machine-learning</link>
      <description><![CDATA[我正在寻找一种机器学习模型的建议，该模型可以解决我即将概述的问题。我最熟悉的两个模型是线性回归和逻辑回归，但似乎不是合适的模型。
我试图预测每月天气数据如何影响年度苹果产量。每个月都有自己的一组天气变量（降雨量、低温、平均温度和高温），一年中月份的组合会影响年产量。我有 X 年的月度数据，年份目前也是我数据中的一个变量。
然而，在设置数据时，使用线性回归将相同数量的年度苹果分配给给定年份的每个月是行不通的，因为模型认为变量的波动不会影响产量，因为它们都被分配了相同的生产值（澄清一下，这是因为每个月的年份都相同）。
有没有什么办法可以解决这个问题，或者我应该使用什么模型？我本质上希望有可能有一个字典数据框，其中模型接收 12 个字典输入（我知道这实际上没有什么意义），以便将每个月对全年苹果产量的影响考虑在内。
（我尝试了线性回归，它基本上将所有类别的权重视为具有完全相同的影响，具有负的 r 平方值，并预测我输入的月份和天气条件下的苹果产量为负数！这也让我回想起，我怎样才能输入所有 12 个月，而不是在预测全年时只选择一个？）]]></description>
      <guid>https://stackoverflow.com/questions/78827204/model-for-multiple-months-that-impact-an-annual-number-in-machine-learning</guid>
      <pubDate>Fri, 02 Aug 2024 20:52:41 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中使用 WordNet 查找或训练分层数据的 Mamba 模型</title>
      <link>https://stackoverflow.com/questions/78826929/how-to-find-or-train-a-mamba-model-for-hierarchical-data-using-wordnet-in-python</link>
      <description><![CDATA[我正在用 Python 开发一个机器学习项目，涉及表示来自 WordNet 的分层数据。我对此目的感兴趣，因为它适用于分层结构，因此我有兴趣使用 Mamba 模型。
我需要帮助：
我正在寻找一个预先训练好的 Mamba 模型，该模型专门针对 WordNet 数据进行训练。有人知道是否有这样的模型，或者如果没有，我该如何训练它？
我尝试过的方法：

我在 Hugging Face 和其他 ML 模型存储库中搜索过，但没有找到在 WordNet 上训练的明确提及为“Mamba”的模型。
我尝试在 WordNet 的一个子集上训练一个基本的图形神经网络模型，但我不确定我是否走在正确的轨道上。
]]></description>
      <guid>https://stackoverflow.com/questions/78826929/how-to-find-or-train-a-mamba-model-for-hierarchical-data-using-wordnet-in-python</guid>
      <pubDate>Fri, 02 Aug 2024 19:14:40 GMT</pubDate>
    </item>
    <item>
      <title>优化大数据集上的 Pandas 性能</title>
      <link>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</link>
      <description><![CDATA[我正在使用 pandas 处理一个大型数据集（约 1000 万行和 50 列），在数据操作和分析过程中遇到了严重的性能问题。这些操作包括过滤、合并和聚合数据，目前执行时间太长。
我读过几种优化技术，但不确定哪种技术最有效且适用于我的情况。以下是有关我的工作流程的一些细节：
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台具有 16GB RAM 的机器上运行分析。
社区能否分享优化 pandas 在大型数据集上的性能的最佳实践？
1.内存管理技术。
2.执行 groupby 和 apply 的有效方法。
3.处理大型数据集的 pandas 替代方案。
4. 有没有关于并行处理或有效利用多核的技巧。
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台有 16GB RAM 的机器上运行分析。]]></description>
      <guid>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</guid>
      <pubDate>Tue, 16 Jul 2024 02:24:48 GMT</pubDate>
    </item>
    <item>
      <title>LangChain 与 AmzonBedrock</title>
      <link>https://stackoverflow.com/questions/78705377/langchain-with-amzonbedrock</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78705377/langchain-with-amzonbedrock</guid>
      <pubDate>Thu, 04 Jul 2024 06:30:23 GMT</pubDate>
    </item>
    <item>
      <title>NotFittedError：估计器不适合，在利用模型之前调用“fit”</title>
      <link>https://stackoverflow.com/questions/40937543/notfittederror-estimator-not-fitted-call-fit-before-exploiting-the-model</link>
      <description><![CDATA[我在 Macbook OSX 10.2.1 (Sierra) 上运行 Python 3.5.2。
尝试运行 Kaggle 的 Titanic 数据集的一些代码时，我不断收到以下错误：


NotFittedError Traceback (most recent call
last) in ()
6 
7 # 使用测试集进行预测并打印它们。
----&gt; 8 my_prediction = my_tree_one.predict(test_features)
9 print(my_prediction)
10 
/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/tree/tree.py
在 _validate_X_predict(self, X, check_input) 中
429 &quot;&quot;&quot;
430 
--&gt; 431 X = self._validate_X_predict(X, check_input)
432 proba = self.tree_.predict(X)
433 n_samples = X.shape[0]
/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/tree/tree.py
在 _validate_X_predict(self, X, check_input)
386 “”“每当有人试图预测、应用、predict_proba 时验证 X”””
387 if self.tree_ is None:
--&gt; 388 raise NotFittedError(“估算器未安装，&quot;
389 “在利用模型之前调用 fit。”)
390 
NotFittedError：估算器未安装，在利用
模型之前调用 fit。

有问题的代码似乎是这样的：
# 用中位数估算缺失值
test.Fare[152] = test.Fare.median()

# 从测试集中提取特征：Pclass、Sex、Age 和 Fare。
test_features = test[[&quot;Pclass&quot;, &quot;Sex&quot;, &quot;Age&quot;, &quot;Fare&quot;]].values

# 使用测试集进行预测并打印。
my_prediction = my_tree_one.predict(test_features)
print(my_prediction)

# 创建一个包含两列的数据框：PassengerId 和 Survived。 Survived 包含您的预测
PassengerId =np.array(test[&quot;PassengerId&quot;]).astype(int)
my_solution = pd.DataFrame(my_prediction, PassengerId, columns = [&quot;Survived&quot;])
print(my_solution)

# 检查您的数据框是否有 418 个条目
print(my_solution.shape)

# 将您的解决方案写入名为 my_solution.csv 的 csv 文件
my_solution.to_csv(&quot;my_solution_one.csv&quot;, index_label = [&quot;PassengerId&quot;])

以下是其余部分的链接 代码。
由于我已经调用了“fit”函数，我无法理解此错误消息。我哪里做错了？感谢您的时间。
编辑：
结果发现该问题继承自上一个代码块。
# 拟合您的第一个决策树：my_tree_one
my_tree_one = tree.DecisionTreeClassifier()
my_tree_one = my_tree_one.fit(features_one, target)

# 查看所包含特征的重要性和分数
print(my_tree_one.feature_importances_)
print(my_tree_one.score(features_one, target))

使用以下行：
my_tree_one = my_tree_one.fit(features_one, target)
生成错误：

ValueError：输入包含 NaN、无穷大或对于
dtype(&#39;float32&#39;)。
]]></description>
      <guid>https://stackoverflow.com/questions/40937543/notfittederror-estimator-not-fitted-call-fit-before-exploiting-the-model</guid>
      <pubDate>Fri, 02 Dec 2016 17:10:22 GMT</pubDate>
    </item>
    </channel>
</rss>