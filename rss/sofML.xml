<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Wed, 19 Feb 2025 06:26:27 GMT</lastBuildDate>
    <item>
      <title>为什么我的验证损失低于我的训练损失，因为训练vae在时间序列数据上时？</title>
      <link>https://stackoverflow.com/questions/79450306/why-is-my-validation-loss-lower-than-my-training-loss-when-training-a-vae-on-tim</link>
      <description><![CDATA[我正在训练一个变异自动编码器（VAE）从顺序时间序列数据中提取潜在变量表示。我的模型体系结构相对简单：它在编码器中使用1个LSTM层（在32至128个单位之间），而解码器中使用了1个LSTM层。重建损失计算为原始数据及其重建之间的平方平方误差（MSE），总结在序列维度上，我还计算了KL差异。这两个损失合并为我的模型优化的总损失。
这里有一些有关我的设置的详细信息：
 数据集： 

训练集：通过滚动窗口生成的13K样品。
火车，验证和测试集首先分开，它们之间没有重叠。分裂后，将一个滚动窗口独立地应用于每组
为了保留系列的时间顺序，数据集没有被改组
验证和测试集：每个样本每个样本

 模型体系结构： 

编码器和解码器中的单个LSTM层
由于层和单元数量少，模型容量是适度的

 正则化 

使用0.2的辍学率来正规化模型

令人困惑的部分是，在模型的许多变化中，我一直观察到验证损失低于训练损失。这与通常的期望相反，即训练损失较低（由于模型在培训数据上直接优化），并且验证损失较高（由于概括错误）。 
    
我的问题：

 是什么导致验证损失低于培训损失？

 我应该如何解释这些训练和验证损失曲线？

 我可能缺少任何可能有助于诊断此问题的最佳实践或检查吗？

]]></description>
      <guid>https://stackoverflow.com/questions/79450306/why-is-my-validation-loss-lower-than-my-training-loss-when-training-a-vae-on-tim</guid>
      <pubDate>Wed, 19 Feb 2025 05:49:19 GMT</pubDate>
    </item>
    <item>
      <title>姿势估计和校正</title>
      <link>https://stackoverflow.com/questions/79450203/pose-estimation-and-correction</link>
      <description><![CDATA[我想建立一个系统来分析一个人在进行一定的练习时，如果不正确，请纠正其表格。我如何实施系统以提出建议以改善姿势。
用于检测，我使用了MediaPipe并构建了一个模型来使用几组视频来识别该动作。]]></description>
      <guid>https://stackoverflow.com/questions/79450203/pose-estimation-and-correction</guid>
      <pubDate>Wed, 19 Feb 2025 04:47:13 GMT</pubDate>
    </item>
    <item>
      <title>用于文档分类的AI驱动解决方案[关闭]</title>
      <link>https://stackoverflow.com/questions/79449606/ai-powered-solutions-for-document-classification</link>
      <description><![CDATA[分类法律文件的解决方案是什么（在5个左右的类别中）？
美洲驼可以这样做吗？还有其他类似产品吗？
我正在寻找API，何时可以加载文档和类别并获取该文档所属的类别。甚至给定内容的类别的权重。
如果没有API解决方案，则可能有一个优选的Python库。]]></description>
      <guid>https://stackoverflow.com/questions/79449606/ai-powered-solutions-for-document-classification</guid>
      <pubDate>Tue, 18 Feb 2025 21:00:01 GMT</pubDate>
    </item>
    <item>
      <title>训练LSTM的时间序列不同</title>
      <link>https://stackoverflow.com/questions/79449572/train-lstm-for-time-series-with-varying-lengths</link>
      <description><![CDATA[我正在训练LSTM进行时间序列预测，其中数据来自不规则间隔的传感器。我正在使用最后5分钟的数据来预测下一个值，但是某些序列比其他序列大。
我的输入阵列的形状是（611,1200,15），其中（示例，时间段，功能）。每个样本的第二维度均未完成，因此我用NP.NAN值填充了丢失的数据。例如，示例（1，：，：）有1000个时间段和200 np.nan。
训练时，损失等于Nan。
我在做什么错？我该如何训练？
这是我尝试训练LSTM的尝试：
  def lstmfit（y，x，n_hidden = 1，n_neurons = 30，Learning_rate = 1E-2）：   
    lstm = sequention（）
    lstm.add（basking（mask_value = np.nan，input_shape =（none，x. shape [2]）））））））
        
    对于范围（n_hidden）的图层：
        lstm.add（lstm（n_neurons，， 
                      激活=“ tanh”
                      recurrent_activation =＆quot; sigmoid＆quot;
                      return_sequences = true））
        
    lstm.add（密集（1））
    
    lstm.compile（loss =; mse; optimizer =; adam＆quot;）
    
    早期_STOPPING =早期踩踏（Monitor =&#39;损失&#39;，耐心= 10，详细= 1，restore_best_weights = true）
  
    
    lstm.-fit（x，y.Reshape（-1），epochs = 100，callbacks = [arfore_stopping]）
    
    y_train_fit = lstm.predict（x）
    
    返回lstm，y_train_fit
 
模型的摘要：
  lstm.summary（）
型号：sequential_9＆quot
__________________________________________________________________________
 图层（类型）输出形状参数＃   
=============================================== ===============
 masking_7（掩模）（无，无，15）0         
                                                                 
 LSTM_6（LSTM）（无，无，30）5520      
                                                                 
 密集_10（密集）（无，无，1）31        
                                                                 
=============================================== ===============
总参数：5551（21.68 kb）
可训练的参数：5551（21.68 kb）
不可训练的参数：0（0.00字节）
__________________________________________________________________________
 
和训练的第一个时期：
 时期1/100
18/18 [=======================================
时代2/100
18/18 [========================================
时期3/100
18/18 [====================================
 ]]></description>
      <guid>https://stackoverflow.com/questions/79449572/train-lstm-for-time-series-with-varying-lengths</guid>
      <pubDate>Tue, 18 Feb 2025 20:47:34 GMT</pubDate>
    </item>
    <item>
      <title>推荐系统中的可伸缩性问题</title>
      <link>https://stackoverflow.com/questions/79449194/scalability-issue-in-recommender-system</link>
      <description><![CDATA[我是推荐系统的新手，目前我正在建立一个基于协作过滤的建议系统。在我的数据集中，当前有600个用户和9000个项目具有不同的评分。我已经创建了一个用户项目交互矩阵，并且正在使用Numpy进行所有操作。我正在使用Pearson相关系数作为与每个用户相对的顶级K相似用户的方法。我当前找到有关每个目标用户最相似的用户的当前代码具有O（m^2  n）的时间复杂性和O（m  n）的空间复杂性，其中m是用户和n是项目的数量。
考虑到这段时间的复杂性，对于大量用户来说，这是不可行的。在研究后，我发现降低尺寸可能是一种解决方案。但是我担心的是，如果我减少用户数量，那么整个系统将无法为建议部分做出公正的态度，因为我想为每个用户推荐。
那么，优化的不同方法是什么，以便在缩放缩放的情况下有助于？]]></description>
      <guid>https://stackoverflow.com/questions/79449194/scalability-issue-in-recommender-system</guid>
      <pubDate>Tue, 18 Feb 2025 17:52:29 GMT</pubDate>
    </item>
    <item>
      <title>minibatchkmeans bertopic不返回一半数据的主题</title>
      <link>https://stackoverflow.com/questions/79449168/minibatchkmeans-bertopic-not-returning-topics-for-half-of-data</link>
      <description><![CDATA[我正在尝试将推文数据集主题。我有大约5000万条推文。不幸的是，由于嵌入，如此大的数据集将不适合RAM（甚至128GB）。因此，我一直在努力根据 docs  docs  &gt; 
因此：
 来自bertopic.vectorizer inlinecountvectorizer inlinecountvectorizer
从bertopic.Dectorizer导入ClasStFidFtransFormer
来自Sklearn.Cluster Import Minibatchkmeans
导入numpy作为NP


class safeincrementalpca（regementalpca）：
    def partial_fit（self，x，y = none）：
        ＃确保输入是连续的，并且在float64中
        x = np.sascontiguularray（x，dtype = np.float64）
        返回super（）。partial_fit（x，y）
    
    def变换（self，x）：
        结果= super（）。变换（x）
        ＃强制输出为float64并连续
        返回np.sascontiguularray（结果，dtype = np.float64）


vectorizer_model = onlinecountVectorizer（stop_words =;英语）
ctfidf_model = classtfidftransformer（redy_frequent_words = true，bm25_weighting = true）
umap_model = safeincrementalpca（n_components = 100）
cluster_model = minibatchkmeans（n_clusters = 1000，andural_state = 0）

来自伯托进口的伯托

topic_model = bertopic（umap_model = umap_model，
                       hdbscan_model = cluster_model，

对于docs_delayed，emb_delayed in tqdm（zip（docs_partitions，embeddings_partitions），total = len（docs_partitions））：

    docs_pdf = docs_delayed.compute（）
    emb_pdf = emb_delayed.compute（）

    docs = docs_pdf [&#39;text;]。tolist（）
    embeddings = np.vstack（emb_pdf [&#39;embeddings&#39;]。tolist（））
    
    ＃部分适合您的模型（确保您的模型像许多Scikit-Learn估计器一样支持Partial_fit）
    topic_model.partial_fit（文档，嵌入）

 
然后将数据集转换为SQL数据库：
 
对于docs_delayed，emb_delayed in tqdm（zip（docs_partitions，embeddings_partitions），total = len（docs_partitions））：

    docs_pdf = docs_delayed.compute（）
    emb_pdf = emb_delayed.compute（）
    docs = docs_pdf [&#39;text;]。tolist（）
    embeddings = np.vstack（emb_pdf [&#39;embeddings&#39;]。tolist（））

    ＃3）在此碎片上涂抹伯托
    主题，probs = topic_model.transform（文档，嵌入）

    ＃将主题保存到数据框
    df_topics = pd.dataframe（{{
        ＆quot&#39;tweet_id＆quot;：docs_pdf [;
        主题“：主题，
        概率＆quot：概率
    }））

    ## Merge＆amp;存储在DB中
    docs_pdf [主题;] = df_topics [tope;
    docs_pdf [概率＆quot＆quort＆quort＆quotisy = df_topics [＆quot&#39;概率;]
    docs_pdf.to_sql（“ tweets”;引擎，发动机，if_exists =＆quot&#39;append＆quort; quot; index = false）
 
我已经尝试这样做了一段时间，这是我得到的最接近的示例。唯一的问题是，数据集的一半在末尾数据库中具有零主题。从我对理论的了解来看，Minibatchkmeans不应有任何异常值，因此所有推文应至少分配给至少一个主题，对吗？我已经检查了有关的未分类推文，他们的文档中没有任何内容表明很难对其进行分类（相对于其他分类）。
我很高兴听到有关可能出了什么问题以及如何解决此问题的任何建议！
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79449168/minibatchkmeans-bertopic-not-returning-topics-for-half-of-data</guid>
      <pubDate>Tue, 18 Feb 2025 17:42:59 GMT</pubDate>
    </item>
    <item>
      <title>为什么在训练LSTM模型时面对“ CUDA错误：设备端断言触发”？</title>
      <link>https://stackoverflow.com/questions/79448910/why-facing-cuda-error-device-side-assert-triggered-while-training-lstm-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79448910/why-facing-cuda-error-device-side-assert-triggered-while-training-lstm-model</guid>
      <pubDate>Tue, 18 Feb 2025 16:05:55 GMT</pubDate>
    </item>
    <item>
      <title>Qiskit Importerror</title>
      <link>https://stackoverflow.com/questions/79448915/qiskit-importerror</link>
      <description><![CDATA[我试图使用以下内容导入量子级：
 来自qiskit_machine_learning.kernels导入量子kernel
 
但是我遇到了这个错误：
 来自qiskit_machine_learning.kernels导入量子kernel
Importerror：无法从&#39;qiskit_machine_learning.kernels&#39;导入名称&#39;量子kernel&#39; 
（c：\ user \ pshre \ appdata \ local \ program \ python \ python \ python310 \ lib \ site-packages \ qiskit_machine_learning \ kernels \ kernels \ __ init__ init__.py）
 
 qiskit版本：0.8.2 
我已经更新了模块：
  pip安装 - 升级qiskit-machine学习
 ]]></description>
      <guid>https://stackoverflow.com/questions/79448915/qiskit-importerror</guid>
      <pubDate>Tue, 18 Feb 2025 16:05:55 GMT</pubDate>
    </item>
    <item>
      <title>OPENCV：从图像分割/提取打印机标签</title>
      <link>https://stackoverflow.com/questions/79448585/opencv-segmenting-extracting-printer-labels-from-image</link>
      <description><![CDATA[我有来自打印机的标签的镜头。
我想从框架中提取单个标签（即检测每个单独标签的边界），然后在mm中找到印刷矩形和标签的顶部边缘之间的距离。。
 这是录像中的框架 
我最初尝试与一些形态学操作一起尝试轮廓检测，但是标签内的印刷内容（矩形和数字）正在干扰边缘检测，因此很难仅隔离标签边框。 
有人解决了类似问题吗？哪些预处理技术或替代方法最适合仅可靠地分割标签边缘？]]></description>
      <guid>https://stackoverflow.com/questions/79448585/opencv-segmenting-extracting-printer-labels-from-image</guid>
      <pubDate>Tue, 18 Feb 2025 14:26:06 GMT</pubDate>
    </item>
    <item>
      <title>视觉Mamba实施</title>
      <link>https://stackoverflow.com/questions/79448241/vision-mamba-implementation</link>
      <description><![CDATA[我是Mamba模型的新用户，我读了一些论文，说它在图像分割任务上具有出色的性能。如果有人以前已经实施了它，是否有有关Mamba块的正确设置或输入图像补丁和尺寸的指导，可以导致最佳结果？
到目前为止，我的实施尚未显示出将Mamba块添加到我的代码中的任何优势，这是我实施的一个小片段：
  x = torch.rand（1，16，256，256）
norm = rmsnorm（16 ** 2）
mamba = mamba（16 ** 2）
_，c，h，_ = X.Shape
x =重新安排（x，&#39;b c（p1 pH）（p2 pw） - ＆gt; b（c p1 p2）（pH pw）&#39;，pH = 16，pw = 16）
x = mamba（norm（x）） + x
x =重新安排（x，&#39;b（c p）d  - ＆gt; b c p d&#39;，c = c）
x =重新安排（x，&#39;b c（p1 p2）（pH PW） - ＆gt; b c（p1 pH）（p2 pw）&#39;，p1 = h // 16，pH = 16）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79448241/vision-mamba-implementation</guid>
      <pubDate>Tue, 18 Feb 2025 12:23:34 GMT</pubDate>
    </item>
    <item>
      <title>这两个实现洛拉（低级适应）之间有什么区别吗？</title>
      <link>https://stackoverflow.com/questions/79447495/is-there-any-difference-between-these-two-implementations-of-lora-low-rank-adap</link>
      <description><![CDATA[我们都知道洛拉是一种低级适应方法，可以表达如下：x = w_0 * x +（a @ b） * x。我有两个不同的代码实现。它们之间有什么区别吗？
代码1：
  def向前（self，x）：
    x = x @ self.lora_a
    x = x @ self.lora_b
    x = self.scaling * x
    返回x
 
代码2：
  def向前（self，x）：
    x = x @（self.lora_a @ self.lora_b）
    x = self.scaling * x
    返回x
 
从数学角度来看，两者均似乎是等效的。但是，当我在玩具数据集上运行两个实现时，我观察到它们的性能有很小的差异 - 编码2的性能稍好。
为什么会发生这种轻微的差异？是否有基本的计算或优化细微差别可以解释这一点？
我不完全确定两个实现是否正确。我经常在GitHub存储库中看到代码1，但是我注意到代码2的性能稍好一些。为什么可能是这种情况？]]></description>
      <guid>https://stackoverflow.com/questions/79447495/is-there-any-difference-between-these-two-implementations-of-lora-low-rank-adap</guid>
      <pubDate>Tue, 18 Feb 2025 07:56:56 GMT</pubDate>
    </item>
    <item>
      <title>寻求2到3D超声重建的开源数据集[封闭]</title>
      <link>https://stackoverflow.com/questions/79446055/seeking-open-source-datasets-for-2d-to-3d-ultrasound-reconstruction</link>
      <description><![CDATA[我目前正在研究一个专注于将2D超声图像转换为3D型号的项目。要培训我的AI模型，我正在寻找专门为2到3D超声重建设计的开源数据集。
您知道可用于此目的的任何公开可用数据集吗？如果没有，我将非常感谢有关工具，资源或方法的任何建议，这些建议可以帮助我为此任务创建自己的数据集。
我检查了此ResearchGate链接，但我找不到下载数据集的方法。]]></description>
      <guid>https://stackoverflow.com/questions/79446055/seeking-open-source-datasets-for-2d-to-3d-ultrasound-reconstruction</guid>
      <pubDate>Mon, 17 Feb 2025 16:37:20 GMT</pubDate>
    </item>
    <item>
      <title>如何对混合VAR-LSTM模型执行样本外预测？</title>
      <link>https://stackoverflow.com/questions/79445942/how-to-perform-out-of-sample-forecast-for-a-hybrid-var-lstm-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79445942/how-to-perform-out-of-sample-forecast-for-a-hybrid-var-lstm-model</guid>
      <pubDate>Mon, 17 Feb 2025 15:56:31 GMT</pubDate>
    </item>
    <item>
      <title>培训LLM在图数据库中用于查询生成的LLM</title>
      <link>https://stackoverflow.com/questions/77613507/training-llm-for-query-generation-in-a-graph-database</link>
      <description><![CDATA[如果我已经开发了一个具有自己的查询语言的图形数据库。我必须找到一种方法来馈送图形，然后LLM应该能够生成我们数据库的查询。
我在Langchain中发现了类似的东西，我们可以将其喂入RDF文件，然后将生成Sparql查询。
所以我对此有很多疑问，因为我非常陌生：
是否可以像我们的数据库那样培训LLM上的全新技术。如果可能的话，那么如何。
我知道我们必须向LLM提供培训数据。因此，在这种情况下，将是我们数据库查询的数据集。如果是，那么我们必须在数据集中提供多少查询。
对不起，如果没有详细详细介绍，这只是我第二次在这里问。]]></description>
      <guid>https://stackoverflow.com/questions/77613507/training-llm-for-query-generation-in-a-graph-database</guid>
      <pubDate>Wed, 06 Dec 2023 13:34:06 GMT</pubDate>
    </item>
    <item>
      <title>Azure机器学习</title>
      <link>https://stackoverflow.com/questions/70853882/azure-machine-learning</link>
      <description><![CDATA[我可以在Azure中创建机器学习工作区，因为我不能选择一个区域。
你能帮我吗？
 错误  ]]></description>
      <guid>https://stackoverflow.com/questions/70853882/azure-machine-learning</guid>
      <pubDate>Tue, 25 Jan 2022 18:32:29 GMT</pubDate>
    </item>
    </channel>
</rss>