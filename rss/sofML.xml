<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 23 Sep 2024 18:22:31 GMT</lastBuildDate>
    <item>
      <title>我们如何在 R 中计算一个数据集中的一个记录与第二个数据集中的所有记录之间的 Gower 距离？</title>
      <link>https://stackoverflow.com/questions/79015729/how-do-we-calculate-the-gower-distance-between-one-record-in-one-dataset-and-all</link>
      <description><![CDATA[我想计算数据集 1 的一条记录与数据集 2 的所有记录之间的 Gower 距离。第一种方法如下
library(gower)

data(iris)
dat1 &lt;- iris[1:10,]
dat2 &lt;- iris[11:30,]

# 第一种方法
gower::gower_dist(dat1[1,], dat2)

这给了我长度为 20 的结果。
0.09079365 0.09873016 0.16142857 0.29476190 0.20920635 0.33079365 
0.21936508 0.05000000 0.23952381 0.11507937 0.12095238 0.15079365 
0.16984127 0.24523810 0.16539683 0.12920635 0.17206349 0.03555556 
0.02761905 0.14063492

我可以将第一个值解释为 dat1[1,] 和 dat2[1,] 之间的 gower 距离，将第二个值解释为 dat1[1,] 和 dat2[2,] 之间的 gower 距离，依此类推吗？
让我感到困惑的是，如果我计算
gower::gower_dist(dat1[1,],dat2[1,])

这给了我
0.75

这与 0.09079365 不同。最终，我想计算 dat1 中每个观测值与 dat2 中每个观测值的 Gower 距离。如果我使用第二种方法，我将需要在 dat2 中的所有观测值上添加一个 for 循环，如下所示。
# 第二种方法
for(i in 1:nrow(dat2)) {
print(gower::gower_dist(dat1[1,],dat2[i,]))
}

由于这两种方法给出的结果不同，我应该使用哪一种方法来实现目的？]]></description>
      <guid>https://stackoverflow.com/questions/79015729/how-do-we-calculate-the-gower-distance-between-one-record-in-one-dataset-and-all</guid>
      <pubDate>Mon, 23 Sep 2024 17:37:36 GMT</pubDate>
    </item>
    <item>
      <title>有人知道如何修复库错误吗？</title>
      <link>https://stackoverflow.com/questions/79015388/does-anyone-know-how-to-fix-library-error</link>
      <description><![CDATA[早上好，我遇到了这个错误，尝试了所有方法，但还是无法解决，如果有人知道 crewapi 库是什么，那将非常有帮助。
我正在尝试最优解决方案以继续
这是代码和错误
ModuleNotFoundError Traceback（最近一次调用最后一次）
&lt;ipython-input-13-6c7180fd4822&gt; in &lt;cell line: 1&gt;()
----&gt; 1 from crewapi_module import CrewAPI # 用正确的导入路径替换
2 
3 crew_api = CrewAPI(api_key=&quot;your_crewai_api_key&quot;)
4 
5 # 现在您可以与 API 交互，例如：

ModuleNotFoundError：没有名为“crewapi_module”的模块
]]></description>
      <guid>https://stackoverflow.com/questions/79015388/does-anyone-know-how-to-fix-library-error</guid>
      <pubDate>Mon, 23 Sep 2024 15:52:13 GMT</pubDate>
    </item>
    <item>
      <title>受不同聚类大小约束的 KMeans</title>
      <link>https://stackoverflow.com/questions/79015120/kmeans-constrained-with-different-cluster-size</link>
      <description><![CDATA[我有一个包含商店坐标的数据框，我想根据供应商应该访问该商店的日期将它们划分为簇。例如，假设供应商应该访问 180 家商店。他应该在周一到周五访问 30-34 家商店，周六，他应该访问其他日子的 60%。
你们知道我该怎么做吗？使用 kmeans-constrained，我只能将它们划分为大小相等的簇。也许我需要使用某种解算器或在集群之间移动点以达到我想要的数字，但我不知道如何做到这一点。
以下是将它们均等划分的代码：
# 循环遍历供应商集群
for vendor in df[&quot;vendor&quot;].unique():

# 每个供应商的商店数量
n_shops = df.loc[df[&quot;vendor&quot;] == vendor][&quot;cod_shop&quot;].count()

# 索引
idx = df.loc[df[&quot;vendor&quot;] == vendor].index

# 一周中各天的集群数量
num_clusters = 5 # 星期一至星期五

# 集群的平均大小
avg_size = n_shops / (num_clusters + 0.6)

# 定义限制
min_shops = round(avg_size - n_shops * pct, 0)
max_shops = math.ceil(avg_size + n_shops * pct)

# 模型
kmeans = KMeansConstrained(n_clusters=num_clusters, size_min=min_shops, size_max=max_shops, random_state=42)
labels = kmeans.fit_predict(df.loc[df[&quot;vendor&quot;] == vendor][[&quot;latitude&quot;, &quot;longitude&quot;]])

# 向数据框添加标签
df.loc[idx, &quot;visit_day&quot;] = labels
]]></description>
      <guid>https://stackoverflow.com/questions/79015120/kmeans-constrained-with-different-cluster-size</guid>
      <pubDate>Mon, 23 Sep 2024 14:42:09 GMT</pubDate>
    </item>
    <item>
      <title>图像拼接中的泊松混合导致图像模糊、鬼影重重</title>
      <link>https://stackoverflow.com/questions/79014990/poisson-blending-in-image-stitching-results-in-blurred-ghostly-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79014990/poisson-blending-in-image-stitching-results-in-blurred-ghostly-images</guid>
      <pubDate>Mon, 23 Sep 2024 14:11:23 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 coremltools 将 TensorFlow ConcreteFunction 或 AutoTrackable 对象转换为 Core ML？</title>
      <link>https://stackoverflow.com/questions/79014855/how-to-convert-a-tensorflow-concretefunction-or-autotrackable-object-to-core-ml</link>
      <description><![CDATA[我正在尝试使用 coremltools 将 TensorFlow 对象检测模型 (ssd_mobilenet_v1_coco) 转换为 Core ML 格式。该模型采用 SavedModel 格式，但我在尝试转换时遇到了各种问题。以下是我到目前为止采取的步骤以及我得到的错误：
我到目前为止所做的：

加载了 TensorFlow SavedModel：
import tensorflow as tf

model = tf.saved_model.load(&quot;ssd_mobilenet_v1_coco_2017_11_17/saved_model&quot;)
concrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]`



尝试使用 coremltools 将模型转换为 Core ML：


mlmodel = ct.convert(concrete_func, source=&quot;tensorflow&quot;)
mlmodel.save(&quot;ssd_mobilenet_v1_coco.mlmodel&quot;) 

尝试转换 ConcreteFunction 时，我不断收到以下错误：
NotImplementedError：预期模型格式：[SavedModel | concrete_function | tf.keras.Model | .h5 | GraphDef]，得到了 ConcreteFunction

我尝试将模型导出为 SavedModel 并将该路径直接传递给 coremltools：
tf.saved_model.save(model, &quot;exported_saved_model&quot;)
mlmodel = ct.convert(&quot;exported_saved_model&quot;, source=&quot;tensorflow&quot;)

如何使用 coremltools 成功将此 TensorFlow 模型（或 ConcreteFunction）转换为 Core ML？是否有特定的方法来处理 AutoTrackable 或 ConcreteFunction 对象？在此转换过程中我遗漏了什么或做错了什么？
其他信息：
• TensorFlow 版本：X.X.X（例如 2.10.0）
• coremltools 版本：X.X.X（例如 5.0b3）
• Python 版本：X.X.X（例如 3.10）
]]></description>
      <guid>https://stackoverflow.com/questions/79014855/how-to-convert-a-tensorflow-concretefunction-or-autotrackable-object-to-core-ml</guid>
      <pubDate>Mon, 23 Sep 2024 13:36:52 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何方法可以扩展我们的数据集，从具有 200 行的初始数据集开始。我希望从中至少获得 2000 行来应用 ML 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/79014580/is-there-any-method-about-how-to-expand-our-dataset-from-initial-dataset-having</link>
      <description><![CDATA[我的项目是利用 ML 技术预测抑郁程度或向孩子的父母提出一些预防措施建议。
我想应用 ML 模型根据我们的数据集预测抑郁程度或其症状。因此，我需要至少 2000 个训练数据元组来训练它们。我怎样才能在不改变属性之间的关系（相关性）的情况下实现这一点。
我的数据集包含许多属性，例如屏幕时间、抑郁程度。如何使用一些代码来解决这个问题。
我尝试使用 CTGAN，但它给了我很多错误。]]></description>
      <guid>https://stackoverflow.com/questions/79014580/is-there-any-method-about-how-to-expand-our-dataset-from-initial-dataset-having</guid>
      <pubDate>Mon, 23 Sep 2024 12:21:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 NumPy 实现基本神经网络的问题</title>
      <link>https://stackoverflow.com/questions/79014083/problem-implementing-a-basic-neural-network-with-numpy</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79014083/problem-implementing-a-basic-neural-network-with-numpy</guid>
      <pubDate>Mon, 23 Sep 2024 09:49:44 GMT</pubDate>
    </item>
    <item>
      <title>构建 OCR 模型 [关闭]</title>
      <link>https://stackoverflow.com/questions/79013996/building-ocr-model</link>
      <description><![CDATA[问题：
我正在研究一个机器学习模型，其中的输入是图像和实体名称，目标是从图像中提取相应的实体值。例如，如果实体名称是“高度”，并且图像包含门的高度，则模型应该提取此值（例如 6 英尺）以及正确的单位。
输入：
图像：包含对象（例如门）和相关信息（例如高度或其他相关测量值）。
实体名称：关键字，例如“高度”或“重量”指定要从图像中提取的值。
输出：
与图像中的实体相对应的值及其单位（例如，“6 英尺”）。
挑战：
实体值可以出现在图像的不同部分，具有不同的文本格式、字体或样式。
需要识别和提取测量单位（例如米、英尺）以及值。
问题：
处理此类任务的最佳方法或模型架构是什么？
是否有任何特定技术或预训练模型可以帮助将图像和实体名称作为输入结合起来以从图像中提取相应的值？
我应该如何预处理图像和标签以训练此类任务的模型？
任何有关框架和工具的指导、参考或建议都将不胜感激！
我尝试使用带有 CTC 损失的 CNN+RNN，但我的损失接近 20 并且没有进一步减少
这里我附上了我的 google cloab 链接
笔记本链接]]></description>
      <guid>https://stackoverflow.com/questions/79013996/building-ocr-model</guid>
      <pubDate>Mon, 23 Sep 2024 09:27:35 GMT</pubDate>
    </item>
    <item>
      <title>我可以做些什么来提高我在 Kaggle 泰坦尼克号竞赛中的表现？[关闭]</title>
      <link>https://stackoverflow.com/questions/79013898/what-can-i-do-to-improve-my-performance-on-kaggles-titanic-contest</link>
      <description><![CDATA[我尝试了所有方法，使用决策树和随机森林来预测泰坦尼克号上的幸存者。但都不起作用。
我使用决策树和随机森林模型来预测谁会在泰坦尼克号上幸存。
我选择的特征包括“Pclass”、“性别”、“年龄”、“票价”和“舱位”，对于舱位，我使用 OneHotEncoder 来转换数据，对于性别，我使用 LabelEncoder。
然后我使用决策树来训练模型。当我设置 max_depth=4 并提交时，我的模型在预测中达到了最高的准确度分数，即 0.7799。后来，无论我做什么，我都无法获得更高的分数。我使用了 train_test_split、RandomForestClassifier，我使用了 GridSearch 来测试不同的参数。这些都不起作用，我的分数总是低于 0.7799。每当我设置 max_depth=4 时，无论是决策树还是随机森林，分数始终为 0.7799
如何使用 DecisionTree 或 RandomForest 提高我的性能？谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79013898/what-can-i-do-to-improve-my-performance-on-kaggles-titanic-contest</guid>
      <pubDate>Mon, 23 Sep 2024 08:58:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Deepface Deepface.represent 从 ROI 获取嵌入时出错</title>
      <link>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</link>
      <description><![CDATA[我在使用 Deepface 从 Retinaface 识别的裁剪 ROI 获取嵌入时遇到了问题。
我正尝试使用一些名人的数据集（图像）学习对象识别，并可能考虑将其用于我的个人照片库。我尝试使用 Haar Cascade 进行人脸检测，并使用 Open Cv 中的 LBPHFaceRecognize 进行人脸识别，效果很好。然后我想尝试使用 Retinafce 进行人脸检测并获得 ROI。ROI 存储在列表中，并使用 Deepface 从选定的 ROI 获取嵌入并存储在另一个列表中。我正在尝试将嵌入存储到列表中，但我一直得到
 raise ValueError(
ValueError: 无法在 numpy 数组中检测到人脸。请确认图片

是人脸照片或考虑将 force_detection 参数设置为 False。
虽然所有图像都有一张被清楚检测到的人脸。这是我的代码供参考：
import os
import cv2 as cv
from retinaface import RetinaFace
from deepface import DeepFace
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

artist = [&#39;50cent&#39;] # type: ignore #MJ the GOAT!! , &#39;Kanye&#39;, &#39;Eminem&#39;, &#39;MichaelJackson&#39;
ROOT_DIR = &#39;asset/Face_Recon_Dataset&#39; #图像数据集的路径
faces_roi =[]
labels = []
embeddings = []
#现在在脸部坐标上画一个矩形
#脸部范围有：
# x1, y1) = (28, 51) #左上角
# (x2, y2) = (61, 98) #右下角
&quot;&quot;&quot; 这定义了检测到的脸部周围的矩形边界框。
- x1 (28)：脸部的左边缘
- y1 (51)：脸部的上边缘
- x2 (61)：脸部的右边缘
- y2 (98)：脸部的下边缘&quot;&quot;&quot;

def get_roi():
for artist_name in artist:
# 获取艺术家姓名的索引
label = artist.index(artist_name)
image_folder = os.path.join(ROOT_DIR,artist_name) # 获取包含图像的实际文件夹
for artist_images in os.listdir(image_folder): # 列出该目录中的所有图像
image = os.path.join(image_folder,artist_images)
resp = RetinaFace.detect_faces(image)
# 确保人脸存在
if isinstance(resp,dict):
img = cv.imread(image)
for face_id, face_data in resp.items():
# print(face_id)
# print(&quot;x1: &quot;, face_data[&#39;facial_area&#39;][0])
# print(&quot;y1: &quot;, face_data[&#39;facial_area&#39;][1])
# print(&quot;x2: &quot;, face_data[&#39;facial_area&#39;][2])
# print(&quot;y2: &quot;, face_data[&#39;facial_area&#39;][3], &quot;\n&quot;)
# 读取图像

# 检测人脸
x1 = face_data[&#39;facial_area&#39;][0]
y1 = face_data[&#39;facial_area&#39;][1]
x2 = face_data[&#39;facial_area&#39;][2]
y2 = face_data[&#39;facial_area&#39;][3]

# 为人脸绘制边界框 
# faces_rect = cv.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
face_roi = img[y1:y2,x1:x2]

#用其名称标记裁剪后的 roi 人脸
faces_roi.append(face_roi)

labels.append(label)
print(len(faces_roi))
print(len(labels))
print(&quot;已标记和索引的图像&quot;)
print(&quot;正在初始化嵌入过程.....&quot;)
get_embeddings()

def get_embeddings():
&quot;&quot;&quot; 使用 deepface 从每个面部 roi 中提取嵌入&quot;&quot;&quot;
print(&quot;Satarting embedding: 🚀🚀 &quot;)
for roi in faces_roi:
face_roi_resized = cv.resize(roi, (160, 160)) # 将人脸 ROI 调整为 160x160 像素
embedding = DeepFace.represent(face_roi_resized, model_name=&quot;Facenet&quot;)
print(embedding)
embeddings.append(embedding)
print(&quot;Vectors storage in list..&quot;)

get_roi()

# 是时候使用 svm 分类器测试和训练这个坏家伙了
# 将嵌入和索引标记为 numpy 数组
X = np.array(embeddings) #feature
y = np.array(labels) #label

# 将数据分成训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练 SVM 分类器
svm_model = SVC(kernel=&#39;linear&#39;) # 线性核是嵌入的良好默认值
svm_model.fit(X_train, y_train)

# 评估模型
y_pred = svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;SVM 模型准确率：{accuracy * 100:.2f}%&quot;)


有人能帮我理解为什么即使 ROI 已被裁剪，该错误仍然持续存在吗？解决该错误的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79013712/error-getting-embeddings-from-a-roi-using-deepface-deepface-represent</guid>
      <pubDate>Mon, 23 Sep 2024 08:03:12 GMT</pubDate>
    </item>
    <item>
      <title>面部皮肤健康分析 API [关闭]</title>
      <link>https://stackoverflow.com/questions/79013615/face-skin-health-analysis-api</link>
      <description><![CDATA[我想创建一个 React Native 应用来评估皮肤健康状况。我需要测量诸如光泽、斑点、皱纹、纹理、黑眼圈、眼袋、发红、油性、毛孔和水分等因素，并按 1 到 100 的等级显示每个因素的摘要。我遇到了一些用于此目的的 API，但它们非常昂贵。有没有使用 Python 和 OpenCV 的解决方案？是否有可用的模型或指南可以帮助我学习和开发项目的 API？
我尝试了 Google ML Vision 和 Microsoft API，但它们不符合我的要求。我找到了一些 API，但它们非常昂贵。现在我正在寻找自定义模型或数据集。]]></description>
      <guid>https://stackoverflow.com/questions/79013615/face-skin-health-analysis-api</guid>
      <pubDate>Mon, 23 Sep 2024 07:38:51 GMT</pubDate>
    </item>
    <item>
      <title>在 nn.Transformer 中使用填充掩码时，损失返回为 Nan</title>
      <link>https://stackoverflow.com/questions/79013493/loss-is-returned-as-nan-when-using-padding-mask-in-nn-transformer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79013493/loss-is-returned-as-nan-when-using-padding-mask-in-nn-transformer</guid>
      <pubDate>Mon, 23 Sep 2024 07:04:53 GMT</pubDate>
    </item>
    <item>
      <title>gym_super_mario_bros 的 DummyVecEnv 构造函数存在问题</title>
      <link>https://stackoverflow.com/questions/78085766/trouble-with-dummyvecenv-constructor-with-gym-super-mario-bros</link>
      <description><![CDATA[摘要：我想要做的就是使用 DummyVecEnv 构造函数来包装我的 gym 环境，即
env = DummyVecEnv([lambda: env])，但执行此操作时我不断收到错误。当前使用 https://pypi.org/project/gym-super-mario-bros/ 作为 Super Mario 的 gym 包装器。我感觉我误解了我应该为构造函数提供哪些参数，但我是 Python 新手，很难理解我遗漏了什么。
我在从 gym_super_mario_bros.make() 返回的环境中使用 DummyVecEnv 构造函数时遇到困难。我一直收到错误“您尝试创建多个环境，但创建它们的函数返回了相同的实例，而不是创建不同的对象”。
我最初尝试将我的环境包装在 DummyVecEnv 中，就像我在许多论坛上看到的那样：
env = gym_super_mario_bros.make(&quot;SuperMarioBros-v0&quot;) 
env = JoypadSpace(env, SIMPLE_MOVEMENT) 
env = GrayScaleObservation(env, keep_dim=True) 
env = DummyVecEnv([lambda: env]) #This line 错误

但我收到此错误：
文件c:\Users\truem\AppData\Local\Programs\Python\Python311\Lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py:30，在 DummyVecEnv.init(self, env_fns) 中
29 def init(self, env_fns: List[Callable[[], gym.Env]]):
---&gt; 30 self.envs = [_patch_env(fn()) for fn in env_fns]
31 if len(set([id(env.unwrapped) for env in self.envs])) != len(self.envs):
32 raise ValueError(
33 &quot;您尝试创建多个环境，但创建它们的函数返回了同一个实例&quot;
34 &quot;而不是创建不同的对象。&quot;(...)
39 &quot;请阅读 https://github.com/DLR-RM/stable-baselines3/issues/1151 了解更多信息。&quot;
40 )

因此，我尝试按照 github 链接中列出的修复程序以及文档中提供的示例进行操作
https://stable-baselines.readthedocs.io/en/master/guide/examples.html
我继续创建了一个新的辅助函数 create_default_environment，它应该创建新的 env 实例并返回它们，但尽管如此，它仍然不起作用。
这是我的代码现在的样子：
#将我的 env 创建包装在函数中
def create_default_environment():
newEnv = gym_super_mario_bros.make(&quot;SuperMarioBros-v0&quot;) 
newEnv = JoypadSpace(newEnv, SIMPLE_MOVEMENT) 
return GrayScaleObservation(newEnv, keep_dim=True)
#在调用辅助函数的列表中创建 lambda
env = DummyVecEnv([lambda: create_default_environment()]) #仍然失败

我继续将我的初始 env 构造包装在辅助函数 create_default_environment() 中，我已经验证该函数每次调用都会返回一个新的 env 实例。然后我使用该包装器插入 DummyVecEnv 构造函数：
env = DummyVecEnv([lambda: create_default_environment()])
但编译器仍然抱怨我传入的函数列表没有返回新实例。
我一直在尝试模拟我在 baselines 提供的示例代码中看到的内容：
from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines import PPO2

env = DummyVecEnv(\[lambda: gym.make(&quot;HalfCheetahBulletEnv-v0&quot;)]) #为什么这个可以工作而我的不行？

但我不确定我做错了什么。我是 python 新手，所以如果我遗漏了什么明显的东西，请原谅我。]]></description>
      <guid>https://stackoverflow.com/questions/78085766/trouble-with-dummyvecenv-constructor-with-gym-super-mario-bros</guid>
      <pubDate>Fri, 01 Mar 2024 05:53:20 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：多标签指示器不支持混淆矩阵</title>
      <link>https://stackoverflow.com/questions/76635503/valueerror-multilabel-indicator-is-not-supported-confusion-matrix</link>
      <description><![CDATA[我尝试运行时收到的错误消息是“multilabel-indicator 不受支持”：
您能给我任何解决方案或提示吗？
import seaborn as sns
sns.heatmap(confusion_matrix(y_test, y_pred), annot = True)

ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-21-ee6823d584e9&gt; in &lt;cell line: 1&gt;()
----&gt; 1 sns.heatmap(confusion_matrix(y_test, y_pred), annot = True)

/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py infusion_matrix(y_true, y_pred, labels, sample_weight, normalize)
317 y_type, y_true, y_pred = _check_targets(y_true, y_pred)
318 if y_type not in (&quot;binary&quot;, &quot;multiclass&quot;):
--&gt; 319 raise ValueError(&quot;%s is not supports&quot; % y_type)
320 
321 if labels is None:

ValueError: multilabel-indicator is not supports
]]></description>
      <guid>https://stackoverflow.com/questions/76635503/valueerror-multilabel-indicator-is-not-supported-confusion-matrix</guid>
      <pubDate>Fri, 07 Jul 2023 09:06:01 GMT</pubDate>
    </item>
    <item>
      <title>开源神经网络库 [关闭]</title>
      <link>https://stackoverflow.com/questions/11477145/open-source-neural-network-library</link>
      <description><![CDATA[我正在寻找一个开源神经网络库。到目前为止，我已经研究过 FANN、WEKA 和 OpenNN。我还应该看看其他的吗？当然，标准是文档、示例和易用性。]]></description>
      <guid>https://stackoverflow.com/questions/11477145/open-source-neural-network-library</guid>
      <pubDate>Fri, 13 Jul 2012 19:32:11 GMT</pubDate>
    </item>
    </channel>
</rss>