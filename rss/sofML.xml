<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 14 Jan 2024 03:15:24 GMT</lastBuildDate>
    <item>
      <title>如何将 python 代码构建为 .tflite 模型，以便可以在我的 Flutter 应用程序中使用它</title>
      <link>https://stackoverflow.com/questions/77813406/how-can-i-build-python-code-to-a-tflite-model-so-it-can-be-used-in-my-flutter-a</link>
      <description><![CDATA[我正在开发一个文本摘要应用程序，我编写了以下代码：
导入spacy
导入 pytextrank
从变压器进口管道

#定义典型的spacy管道

nlp = spacy.load(“en_core_web_sm”)

nlp.add_pipe(“textrank”)

当我运行它时：
example_text=“”“深度学习。

深度学习是一类机器学习算法，[9]: 199–200 使用多个层从原始输入中逐步提取更高级别的特征。例如，在图像处理中，较低层可以识别边缘，而较高层可以识别与人类相关的概念，例如数字、字母或面部。

从另一个角度看深度学习，深度学习指的是“计算机模拟”的学习。或“自动化”人类的学习过程是从源（例如狗的图像）到学习对象（狗）。因此，创造了“更深”的概念。学习或“最深的”学习[10]是有道理的。最深度的学习是指从源到最终学习对象的全自动学习。因此，深度学习是指混合学习过程：从源到学习的半对象的人类学习过程，然后是从人类学习的半对象到最终学习对象的计算机学习过程。
概述

大多数现代深度学习模型都基于多层人工神经网络，例如卷积神经网络和变压器，尽管它们也可以包括在深度生成模型中分层组织的命题公式或潜在变量，例如深度信念网络和深度神经网络中的节点。玻尔兹曼机。[11]

在深度学习中，每个级别都会学习将其输入数据转换为稍微更抽象和复合的表示形式。在图像识别应用中，原始输入可能是像素矩阵；第一表示层可以抽象像素并对边缘进行编码；第二层可以对边缘的排列进行组合和编码；第三层可以编码鼻子和眼睛；第四层可以识别出图像中包含人脸。重要的是，深度学习过程可以自行学习将哪些特征最佳地放置在哪个级别。这并不能消除手动调整的需要；例如，不同的层数和层大小可以提供不同程度的抽象。[12][13]

“深”字在“深度学习”中指数据转换的层数。更准确地说，深度学习系统具有相当大的学分分配路径（CAP）深度。 CAP 是从输入到输出的转换链。 CAP 描述了输入和输出之间的潜在因果关系。对于前馈神经网络，CAP 的深度就是网络的深度，并且是隐藏层的数量加一（因为输出层也是参数化的）。对于循环神经网络，信号可能多次传播通过一层，CAP 深度可能是无限的。 [14]没有普遍认可的深度阈值来区分浅层学习和深度学习，但大多数研究人员都认为深度学习涉及的 CAP 深度高于
2. 深度为 2 的 CAP 已被证明是通用逼近器，因为它可以模拟任何函数。 [15]除此之外，更多的层并不会增加网络的函数逼近能力。深层模型（CAP &gt; 2）能够比浅层模型提取更好的特征，因此，
额外的层有助于有效地学习特征。 ”“”


doc=nlp(示例文本)
在 doc._.textrank.summary(limit_phrases=2,limit_sentences=2) 中发送：
    打印（已发送）

输出：
深度学习是一类机器学习算法，[9]: 199–200  使用多个层从原始输入中逐步提取更高级别的特征。

这对我来说很好，我现在的问题是，如何将此代码构建到 ,tflite 模型中，以便我可以在我的 Flutter 应用程序中使用它]]></description>
      <guid>https://stackoverflow.com/questions/77813406/how-can-i-build-python-code-to-a-tflite-model-so-it-can-be-used-in-my-flutter-a</guid>
      <pubDate>Sat, 13 Jan 2024 23:02:05 GMT</pubDate>
    </item>
    <item>
      <title>使用随机森林进行 R Tidymodels 分类：预测目标变量时出错</title>
      <link>https://stackoverflow.com/questions/77813309/r-tidymodels-classification-with-random-forest-error-while-predicting-target-va</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77813309/r-tidymodels-classification-with-random-forest-error-while-predicting-target-va</guid>
      <pubDate>Sat, 13 Jan 2024 22:25:19 GMT</pubDate>
    </item>
    <item>
      <title>如何处理图神经网络中损失被限制的情况？</title>
      <link>https://stackoverflow.com/questions/77813001/how-to-deal-with-loss-gettting-clamped-in-graph-neural-networks</link>
      <description><![CDATA[我使用 pytorch 在具有相同边缘索引的图上训练了以下模型（任务是电子健康记录上的图分类，其中每个图代表患者数据，并且节点向量是从组合知识图导出的）
class mdl(torch.nn.Module):
    def _init_（自身，输入大小，隐藏大小，输出大小，丢失率）：
        超级（GCNClassifier，自我）。_init_（）
        self.conv1 = GCNConv(输入大小, 隐藏大小)
        self.conv2 = GCNConv(隐藏大小, 输出大小)
        self.dropout = torch.nn.Dropout(dropout_rate)


    defforward（自身，x，edge_index）：
        x = self.conv1(x, 边缘索引)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, 边缘索引)
        x = torch.mean(x, dim=0, keepdim=True)
        返回x

问题是损失被限制在一个特定的值
我尝试了各种学习率值，并尝试了各种技术，例如动量和学习率调度，但损失仍然保持不变
我尝试使用以下循环训练上述模型
#training (graphVec) 800 个图（每个图的形状为 [5,20]）
#y_train是形状为[800,1]的0和1的张量，用于二元分类

纪元数 = 100
对于范围内的纪元（num_epochs）：
    模型.train()
    

    for i in range(len(graphVec)): # 在每次迭代中通过模型传递每个图
        输出 = 模型（graphVec[i]，edge_index）
        损失 = 标准(输出, y_train[i])
        loss.backward()
        优化器.step()
        优化器.zero_grad()
    # StepLR 调度步骤
    
    调度程序.step()
    打印（输出）
    # 打印每个epoch的损失和学习率
    current_lr = optimizer.param_groups[0][&#39;lr&#39;]
    print(f&#39;Epoch [{epoch + 1}/{num_epochs}], 损失: {loss.item()}, 学习率: {current_lr}&#39;)

但是我的损失被严重限制了（损失并没有随着时代的推移而减少）
我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/77813001/how-to-deal-with-loss-gettting-clamped-in-graph-neural-networks</guid>
      <pubDate>Sat, 13 Jan 2024 20:33:22 GMT</pubDate>
    </item>
    <item>
      <title>上采样，然后进行 Conv2d</title>
      <link>https://stackoverflow.com/questions/77812921/upsample-followed-by-conv2d</link>
      <description><![CDATA[在UNet架构的解码器部分，Upsampling层后面通常跟着Conv2d。
这是一个例子：
类 UpConv(nn.Module):
    def __init__(自身, in_chans, out_chans):
        超级().__init__()
        self.up = nn.Sequential(
            nn.Upsample（scale_factor = 2，mode =“双线性”，align_corners = True），
            nn.Conv2d(in_chans, out_chans, kernel_size = 1),
        ）
        self.conv = DoubleConv(out_chans*2, out_chans)
...

有人可以解释一下为什么我们需要 Conv2d 吗？
我读到 Conv2d 层允许学习上采样特征图中的空间层次结构。但我不明白为什么它是相关的，因为在编码过程中应用了 2D 卷积，这意味着已经学习了空间信息。
我的猜测是Conv2d是用来调整输出通道数的。这是正确的吗？]]></description>
      <guid>https://stackoverflow.com/questions/77812921/upsample-followed-by-conv2d</guid>
      <pubDate>Sat, 13 Jan 2024 20:05:19 GMT</pubDate>
    </item>
    <item>
      <title>数据加载减慢模型训练[关闭]</title>
      <link>https://stackoverflow.com/questions/77812244/data-loading-slowing-down-model-training</link>
      <description><![CDATA[当我开始训练模型时，我注意到第一个时期非常慢，但接下来的时期很快。当使用通常的数据加载方法时，它不需要第一个纪元，所以我怀疑错误出在我的加载器中。
请看看我的加载器：
def load_image(文件名, target_shape=(300,300,3)):
    image_string = tf.io.read_file(文件名)
    图像 = tf.image.decode_jpeg(image_string, 通道=3)
    图像 = tf.keras.applications.efficientnet_v2.preprocess_input(图像)

    # 调整图像大小，使其较大尺寸与目标尺寸匹配
    形状 = tf.shape(图像)
    比率 = tf.cast(target_shape[0], tf.float32) / tf.cast(tf.maximum(shape[0], shape[1]), tf.float32)
    new_shape = tf.cast(tf.cast(shape[:2], tf.float32) * 比率, tf.int32)
    图像 = tf.image.resize(图像, new_shape)

    # 将图像填充到目标形状
    pad_height = target_shape[0] - new_shape[0]
    pad_width = target_shape[1] - new_shape[1]
    pad_top = pad_height // 2
    pad_bottom = pad_height - pad_top
    pad_left = pad_width // 2
    pad_right = pad_width - pad_left
    图像 = tf.pad(图像, [[pad_top, pad_bottom], [pad_left, pad_right], [0, 0]], Constant_values=0)

    返回图像

def preprocess_triplets(锚点、正值、负值):
    返回 （
        加载图像（锚点），
        加载图像（正），
        加载图像（负），
    ）

我的问题是，是否有一些更有效的方法可以在数据集中手动创建批次，并且仍然受益于 tf.data 的速度。因为我现在实施的数据加载正在减慢训练速度。另外，我需要对每个批次内的数据进行洗牌，因为在洗牌整个数据集时，批次会混合。正如我所说，只有第一个纪元是缓慢的。我怀疑它然后缓存在我的内存中，但我知道整个数据集不适合我的内存。第一个 epoch 花了 25 分钟，接下来的 epoch 花了 7 分钟。]]></description>
      <guid>https://stackoverflow.com/questions/77812244/data-loading-slowing-down-model-training</guid>
      <pubDate>Sat, 13 Jan 2024 16:40:08 GMT</pubDate>
    </item>
    <item>
      <title>Q8 蛋白质二级结构预测中使用的准确性评估度量数学形式[关闭]</title>
      <link>https://stackoverflow.com/questions/77812004/q8-accuracy-evaluation-metric-mathematical-form-used-in-protein-secondary-struct</link>
      <description><![CDATA[我正在研究使用递归神经网络 (GRU) 模型的蛋白质二级结构预测。我遇到了几个已经解决类​​似问题的开源项目。
他们都使用 Q8 准确度作为评估指标，下面给出了他们用于计算 Q8 准确度的自定义代码。
def 准确率(y_true, y_pred):
y = tf.argmax(y_true, 轴 =- 1)
y_ = tf.argmax(y_pred, 轴 =- 1)
掩码 = tf.greater(y, 0)
返回 K.cast(K.equal(tf.boolean_mask(y, mask), tf.boolean_mask(y_, mask)), K.floatx())

我想知道上面代码片段对应的Q8精度的数学表达式。
以上代码源码工程参考
https://github.com/idrori/cu-ssp]]></description>
      <guid>https://stackoverflow.com/questions/77812004/q8-accuracy-evaluation-metric-mathematical-form-used-in-protein-secondary-struct</guid>
      <pubDate>Sat, 13 Jan 2024 15:33:55 GMT</pubDate>
    </item>
    <item>
      <title>神经网络预测具有相同的值，没有变化[关闭]</title>
      <link>https://stackoverflow.com/questions/77811384/neural-network-predictions-have-the-same-values-no-variation</link>
      <description><![CDATA[这是我的代码：
# 拟合神经网络
ANN_1 &lt;- 神经网络(as.factor(redeemer_latest_ind) ~ 总购买金额 + 购买频率 +
                     centre_purchase_indicator + customer_tenure +
                     int_pf_ct，
                   数据=训练集，
                   隐藏=c(4,2)，stepmax=1e+06，act.fct =“逻辑”，
                   线性.输出 = FALSE,
                   ）

情节（ANN_1）

# 获取模型预测
temp_test &lt;- 子集(test_set, select = c(“total_purchase_amount”, “purchase_Frequency”, “recent_purchase_indicator”, “customer_tenure”, “int_pf_ct”))
头（临时测试）
nn.结果 &lt;- 预测(ANN_1, temp_test)
nn.results &lt;- nn.results$net.result

当我查看 nn 结果时，我只看到所有预测的值相同。
&lt;前&gt;&lt;代码&gt; [874] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [883] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [892] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [901] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [910] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [919] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [928] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [937] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [946] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [955] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [964] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [973] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [982] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
 [991] 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995 0.3115995
[1000] 0.3115995

如何解决这个问题？我还想添加交叉验证，就像我测试的其他模型一样，这在我的代码中称为 cv_5。但最重要的是获得预测的一些变化，因为我需要它们来测试命中率、最高十分位数提升和基尼系数。
以下是我的火车数据库中的一些见解：
在此处输入图像描述
我已经尝试了不同的最大步长和不同的隐藏层。]]></description>
      <guid>https://stackoverflow.com/questions/77811384/neural-network-predictions-have-the-same-values-no-variation</guid>
      <pubDate>Sat, 13 Jan 2024 12:15:40 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归值与预期相差不大</title>
      <link>https://stackoverflow.com/questions/77811361/logistic-regression-values-dont-differ-that-much-as-expected</link>
      <description><![CDATA[我想使用 5 折交叉验证方法在数据集中执行逻辑回归（预处理后）。然后计算并打印指标 Accuracy、F1-Score、Gmean 和 Fit time。
然后使用 MinMax Normalization 重复实验，并再次使用特征标准化。
我写的这段代码正确吗？
我只是没有看到标准化方法之间的分数有那么不同，而且我不知道它们是否有效。
将 numpy 导入为 np
将 pandas 导入为 pd
从sklearn.model_selection导入train_test_split，cross_validate，KFold
从sklearn导入线性模型
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.metrics 导入 precision_score,f1_score,make_scorer
从 imblearn.metrics 导入几何平均分数

data_url =“jm1.csv”；
df = pd.read_csv(data_url, sep=&quot;,&quot;)
df.replace(&#39;?&#39;, pd.NA, inplace=True)
df.dropna(就地=True)
df[&#39;缺陷&#39;] = df[&#39;缺陷&#39;].astype(int)
X=df.drop(&#39;缺陷&#39;, axis=1)
y=df[&#39;缺陷&#39;]

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=13,stratify=y)
模型 = Linear_model.LogisticRegression(solver=&#39;liblinear&#39;,max_iter=500)
model.fit(X_train, y_train)
y_pr=模型.预测(X_test)

kfold=KFold(n_splits=5,shuffle=True,random_state=13)
Scorers={&#39;accuracy_score&#39;: make_scorer(accuracy_score),&#39;F1_score&#39;: make_scorer(f1_score,average=&#39;weighted&#39;),&#39;G-Mean&#39;: make_scorer(geometric_mean_score)}
cv_results=cross_validate(模型,X_train,y_train,cv=kfold,scoring=scorers,return_train_score=True)
print(&quot;准确率：&quot;,cv_results[&#39;test_accuracy_score&#39;].mean())
print(&quot;F1 分数：&quot;, cv_results[&#39;test_F1_score&#39;].mean())
print(&quot;G-Mean 分数：&quot;, cv_results[&#39;test_G-Mean&#39;].mean())
print(&quot;拟合时间：&quot;, cv_results[&#39;fit_time&#39;].mean())

从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.pipeline 导入管道
管道 = 管道([(&#39;scaler&#39;, MinMaxScaler()), (&#39;逻辑&#39;, LogisticRegression(solver=&#39;liblinear&#39;))])
cv_results_minmax= cross_validate(管道, X, y, cv=kfold, 评分=得分者, return_train_score=True)
print(&quot;准确率：&quot;,cv_results_minmax[&#39;test_accuracy_score&#39;].mean())
print(&quot;F1 分数：&quot;, cv_results_minmax[&#39;test_F1_score&#39;].mean())
print(&quot;G-Mean 分数：&quot;, cv_results_minmax[&#39;test_G-Mean&#39;].mean())
print(&quot;拟合时间：&quot;, cv_results_minmax[&#39;fit_time&#39;].mean())

从 sklearn.preprocessing 导入 StandardScaler
管道 = Pipeline([(&#39;scaler&#39;, StandardScaler()), (&#39;logistic&#39;, LogisticRegression(solver=&#39;liblinear&#39;))])
cv_results_stand = cross_validate（管道，X，y，cv = kfold，评分=得分者，return_train_score = True）
print(&quot;准确率：&quot;,cv_results_stand[&#39;test_accuracy_score&#39;].mean())
print(“F1 分数：”, cv_results_stand[&#39;test_F1_score&#39;].mean())
print(&quot;G-Mean 分数：&quot;, cv_results_stand[&#39;test_G-Mean&#39;].mean())
print(&quot;适合时间：&quot;, cv_results_stand[&#39;fit_time&#39;].mean())
]]></description>
      <guid>https://stackoverflow.com/questions/77811361/logistic-regression-values-dont-differ-that-much-as-expected</guid>
      <pubDate>Sat, 13 Jan 2024 12:05:34 GMT</pubDate>
    </item>
    <item>
      <title>神经网络的权重初始化，用于调整我的神经网络，以便进行适当的反向传播[关闭]</title>
      <link>https://stackoverflow.com/questions/77811329/weights-initialization-for-neural-network-for-tunning-my-neural-networks-in-orde</link>
      <description><![CDATA[神经网络中的权重初始化方法是什么？
我想知道如何初始化神经网络中的权重，它们是通过手动还是其他方法完成的，权重对于调整神经网络非常有效，为了进行反向传播它将非常有帮助]]></description>
      <guid>https://stackoverflow.com/questions/77811329/weights-initialization-for-neural-network-for-tunning-my-neural-networks-in-orde</guid>
      <pubDate>Sat, 13 Jan 2024 11:56:45 GMT</pubDate>
    </item>
    <item>
      <title>CNN 模型未经训练 [关闭]</title>
      <link>https://stackoverflow.com/questions/77810698/the-cnn-model-is-not-trained</link>
      <description><![CDATA[我正在尝试在 Matlab 中训练 CNN 模型来预测样本大小等于 10 的随机向量的平均值。但是，模型的 RMSE 变得太高。
有什么建议吗？
clear;clc;关闭全部

随机数(1)
mkdir(&#39;数字&#39;)

Num_Sample=500；
N=10；
X=1:N；
百分比_训练=70；
Percent_Val=15；
Percent_Test=100-(Percent_Train+Percent_Val);

Num_Train=floor(Percent_Train/100*Num_Sample);
Num_Val=下限(Percent_Val/100*Num_Sample);
Num_Test=Num_Sample-(Num_Train+Num_Val);

Rand_Ind=randperm(Num_Sample);
Rand_Ind_Train=Rand_Ind(1,1:Num_Train);
Rand_Ind_Val=Rand_Ind(1,1+Num_Train:Num_Train+Num_Val);
Rand_Ind_Test=Rand_Ind(1,1+Num_Train+Num_Val:end);

X0_Train=[];
X0_Val=[];
X0_测试=[];

对于 i=1：Num_Sample
    Y0{i}=2*rand(1,N);
    Mean_Y(1,i)=平均值(Y0{i});

    Fig_i=图(i);
    绘图(X,Y0{i},&#39;o&#39;,&#39;线宽&#39;,2);
    xlim([1,N])
    ylim([0,2])
    清除 Y0{i}

    saveas(Fig_i,[&#39;图/Fig_&#39; num2str(i) &#39;.jpg&#39;]);
    Fig_JPG=imread([&#39;图/Fig_&#39; num2str(i) &#39;.jpg&#39;]);
    %Fig_JPG_RS=imresize(Fig_JPG,0.5);
    Fig_JPG_Gray=rgb2gray(Fig_JPG);
    Fig_JPG_Gray_Double{i}=im2double(Fig_JPG_Gray);
    关闭所有
    我
结尾
[R_Fig_JPG_Gray C_Fig_JPG_Gray]=大小(Fig_JPG_Gray);

X_Train_0=[];
对于 i=1:长度(Rand_Ind_Train)
    X_Train_0=[X_Train_0 Fig_JPG_Gray_Double{Rand_Ind_Train(1,i)}];
    清除 Fig_JPG_Gray_Double{Rand_Ind_Train(1,i)}
结尾
X_Train=reshape(X_Train_0,R_Fig_JPG_Gray,C_Fig_JPG_Gray,1,长度(Rand_Ind_Train));

X_Val_0=[];
对于 i=1:长度(Rand_Ind_Val)
    X_Val_0=[X_Val_0 Fig_JPG_Gray_Double{Rand_Ind_Val(1,i)}];
    清除 Fig_JPG_Gray_Double{Rand_Ind_Val(1,i)}
结尾
X_Val=重塑(X_Val_0,R_Fig_JPG_Gray,C_Fig_JPG_Gray,1,长度(Rand_Ind_Val));

X_Test_0=[];
对于 i=1:长度(Rand_Ind_Test)
    X_Test_0=[X_Test_0 Fig_JPG_Gray_Double{Rand_Ind_Test(1,i)}];
    清除 Fig_JPG_Gray_Double{Rand_Ind_Test(1,i)}
结尾
X_Test=reshape(X_Test_0,R_Fig_JPG_Gray,C_Fig_JPG_Gray,1,长度(Rand_Ind_Test));
 
Y_Train=[Mean_Y(1,Rand_Ind_Train)]&#39;;
Y_Val=[Mean_Y(1,Rand_Ind_Val)]&#39;;
Y_Test=[Mean_Y(1,Rand_Ind_Test)]&#39;;

图1）
直方图(Mean_Y,N)
轴紧
ylabel(&#39;计数&#39;)
xlabel(&#39;Mean_Y&#39;)

%创建网络层
层数=[
    imageInputLayer([R_Fig_JPG_Gray C_Fig_JPG_Gray 1])
    卷积2dLayer(5,12,&#39;填充&#39;,&#39;相同&#39;)
    批量归一化层
    重新定义层
    maxPooling2dLayer(2,&#39;跨步&#39;,2)
    卷积2dLayer(3,16,&#39;填充&#39;,&#39;相同&#39;)
    批量归一化层
    重新定义层
    %averagePooling2dLayer(2,&#39;跨步&#39;,2)
    %卷积2dLayer(3,32,&#39;填充&#39;,&#39;相同&#39;)
    %batch归一化层
    %relu层
    %卷积2dLayer(3,32,&#39;填充&#39;,&#39;相同&#39;)
    %batch归一化层
    %relu层
    dropout层(0.2)
    全连接层(1)
    回归层]；

% 训练网络
miniBatchSize=60；
验证频率=3； %floor(numel(Y_Train)/miniBatchSize);
选项=trainingOptions(&#39;sgdm&#39;, ...
    &#39;MiniBatchSize&#39;,miniBatchSize, ...
    &#39;最大纪元&#39;,6, ...
    &#39;初始学习率&#39;,1e-3, ...
    &#39;LearnRateSchedule&#39;,&#39;分段&#39;, ...
    &#39;学习率下降因子&#39;,0.1, ...
    &#39;学习率下降周期&#39;,2, ...
    “随机播放”、“每个时代”、...
    &#39;验证数据&#39;,{X_Val,Y_Val}, ...
    &#39;验证频率&#39;，验证频率，...
    &#39;情节&#39;，&#39;训练进度&#39;，...
    &#39;详细&#39;，假）；

% 创建网络
Net=trainNetwork(X_Train,Y_Train,层数,选项);

% 测试网络
Y_Sim_Test=预测(Net,X_Test);

我尝试创建一个 CNN 模型来预测随机向量的平均值。]]></description>
      <guid>https://stackoverflow.com/questions/77810698/the-cnn-model-is-not-trained</guid>
      <pubDate>Sat, 13 Jan 2024 07:54:32 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Roberta 计算单词和句子嵌入？</title>
      <link>https://stackoverflow.com/questions/77805776/how-to-calculate-word-and-sentence-embedding-using-roberta</link>
      <description><![CDATA[我正在尝试使用 Roberta 计算单词和句子嵌入，对于单词嵌入，我从 RobertaModel 类中提取最后一个隐藏状态 outputs[0]，但是我不确定这是否是正确的计算方法。
至于句子嵌入，我不知道如何计算它们，这是我尝试过的代码：
从 Transformers 导入 RobertaModel、RobertaTokenizer
进口火炬

模型 = RobertaModel.from_pretrained(&#39;roberta-base&#39;)
tokenizer = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)
Captions = [“示例标题”、“lorem ipsum”、“这只鸟是黄色的，有红色翅膀”、“嗨”、“示例”]

encoded_captions = [tokenizer.encode(caption) 用于字幕中的字幕]

# 用 0 将序列填充到相同的长度
max_len = max(len(seq) 用于编码字幕中的 seq)
padded_captions = [seq + [0] * (max_len - len(seq)) 对于encoded_captions中的seq]

# 转换为批量大小为 5 的 PyTorch 张量
input_ids = torch.tensor(padded_captions)

输出=模型(input_ids)
word_embedding = 输出[0].连续()
句子嵌入 = ??????

如何使用 Roberta 计算单词和句子嵌入？]]></description>
      <guid>https://stackoverflow.com/questions/77805776/how-to-calculate-word-and-sentence-embedding-using-roberta</guid>
      <pubDate>Fri, 12 Jan 2024 10:05:01 GMT</pubDate>
    </item>
    <item>
      <title>将 XGBoost Shapely 值转换为“SHAP”的解释对象</title>
      <link>https://stackoverflow.com/questions/77800583/converting-xgboost-shapely-values-to-shaps-explanation-object</link>
      <description><![CDATA[我正在尝试将 XGBoost 形状值转换为 SHAP 解释器对象。将[此处][1]的示例与内置 SHAP 库一起使用需要几天的时间（即使在二次采样数据集上），而 XGBoost 库则需要几分钟。然而。我想输出一个与[此处][2]示例中显示的类似的蜂群图。
我的想法是，我可以使用 XGBoost 库来恢复形状值，然后使用 SHAP 库绘制它们，但蜂群图需要一个解释器对象。如何将我的 XGBoost 助推器对象转换为解释器对象？
这是我尝试过的：
导入形状
助推器 = model.get_booster()
d_test = xgboost.DMatrix(X_test[0:100], y_test[0:100])
shap_values = booster.predict(d_test, pred_contribs=True)
shap.plots.beeswarm(shap_values)

返回结果：
类型错误：蜂群图需要一个“Explanation”对象作为“shap_values”参数。

为了澄清，如果可能的话，我想用 xgboost 内置库生成的值创建解释器对象。避免 shap.explainer 或 shap.TreeExplainer 函数调用是一个优先事项，因为它们需要更长的时间（几天）而不是几分钟才能返回。
[1]: https: //shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Python%20Version%20of%20Tree%20SHAP.html
[2]:  https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html#A-simple-beeswarm-summary-plot]]></description>
      <guid>https://stackoverflow.com/questions/77800583/converting-xgboost-shapely-values-to-shaps-explanation-object</guid>
      <pubDate>Thu, 11 Jan 2024 13:55:27 GMT</pubDate>
    </item>
    <item>
      <title>如何在 BART 模型中使用自定义嵌入？以及如何生成可由 BART 模型使用的位置嵌入？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77792221/how-to-use-custom-embedding-in-bart-model-and-how-to-generate-positinal-embeddi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77792221/how-to-use-custom-embedding-in-bart-model-and-how-to-generate-positinal-embeddi</guid>
      <pubDate>Wed, 10 Jan 2024 09:28:11 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 2：获取“警告：tensorflow：最近 9 次对 <function> 的调用中的 9 次触发了 tf.function 重新跟踪。跟踪成本高昂”</title>
      <link>https://stackoverflow.com/questions/61647404/tensorflow-2-getting-warningtensorflow9-out-of-the-last-9-calls-to-function</link>
      <description><![CDATA[我认为这个错误是由形状问题引起的，但我不知道在哪里。完整的错误消息建议执行以下操作：

&lt;块引用&gt;
  此外，tf.function 有experimental_relax_shapes=True 选项，可以放宽参数形状，从而避免不必要的回溯。 

当我在函数装饰器中输入这个参数时，它确实起作用了。 
@tf.function(experimental_relax_shapes=True)

可能是什么原因？完整代码如下：
导入操作系统
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39;
将张量流导入为 tf
print(f&#39;Tensorflow 版本 {tf.__version__}&#39;)
从张量流导入keras
从tensorflow.keras.layers导入密集，Conv1D，GlobalAveragePooling1D，嵌入
将tensorflow_datasets导入为tfds
从tensorflow.keras.models导入模型

(train_data, test_data), info = tfds.load(&#39;imdb_reviews/subwords8k&#39;,
                                          split=[tfds.Split.TRAIN, tfds.Split.TEST],
                                          as_supervised=True, with_info=True)

padded_shapes = ([无], ())

train_dataset = train_data.shuffle(25000).\
    padded_batch（padded_shapes = padd_shapes，batch_size = 16）
test_dataset = test_data.shuffle(25000).\
    padded_batch（padded_shapes = padd_shapes，batch_size = 16）

n_words = info.features[&#39;text&#39;].encoder.vocab_size


类ConvModel（模型）：
    def __init__(自身):
        超级（ConvModel，自我）.__init__()
        self.embe = 嵌入（n_words，output_dim=16）
        self.conv = Conv1D(32, kernel_size=6, 激活=&#39;elu&#39;)
        self.glob = GlobalAveragePooling1D()
        self.dens = 密集(2)

    def 调用（自身，x，训练=无，掩码=无）：
        x = self.embe(x)
        x = self.conv(x)
        x = self.glob(x)
        x = self.dens(x)
        返回x


转换 = 转换模型()

转换（下一个（iter（train_dataset））[0]）

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

train_loss = tf.keras.metrics.Mean()
test_loss = tf.keras.metrics.Mean()

train_acc = tf.keras.metrics.CategoricalAccuracy()
test_acc = tf.keras.metrics.CategoricalAccuracy()

优化器 = tf.keras.optimizers.Adam(learning_rate=1e-3)


@tf.函数
def train_step（输入，标签）：
    使用 tf.GradientTape() 作为磁带：
        logits = conv(输入，训练=True)
        损失 = loss_object(标签, logits)
        训练损失（损失）
        train_acc(logits, 标签)

    梯度 = Tape.gradient(loss, conv.trainable_variables)
    optimizer.apply_gradients(zip(梯度, conv.trainable_variables))


@tf.函数
def test_step（输入，标签）：
    logits = conv(输入，训练=False)
    损失 = loss_object(标签, logits)
    测试损失（损失）
    test_acc（日志，标签）


def 学习():
    train_loss.reset_states()
    test_loss.reset_states()
    train_acc.reset_states()
    test_acc.reset_states()

    对于文本，train_dataset 中的目标：
        train_step（输入=文本，标签=目标）

    对于文本，目标在 test_dataset 中：
        test_step（输入=文本，标签=目标）


def main(epochs=2):
    对于 tf.range(1, epochs + 1) 中的纪元：
        学习（）
        template = &#39;训练损失{:&gt;5.3f}训练ACC {:.2f}测试损失{:&gt;5.3f}测试ACC {:.2f}&#39;

        打印（模板.格式（
            train_loss.result(),
            train_acc.result(),
            test_loss.result(),
            test_acc.result()
        ））

如果 __name__ == &#39;__main__&#39;:
    主要（纪元=1）
]]></description>
      <guid>https://stackoverflow.com/questions/61647404/tensorflow-2-getting-warningtensorflow9-out-of-the-last-9-calls-to-function</guid>
      <pubDate>Wed, 06 May 2020 23:47:47 GMT</pubDate>
    </item>
    <item>
      <title>pandas 中的列连接不正确</title>
      <link>https://stackoverflow.com/questions/22139189/joining-columns-in-pandas-incorrectly</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/22139189/joining-columns-in-pandas-incorrectly</guid>
      <pubDate>Mon, 03 Mar 2014 05:19:40 GMT</pubDate>
    </item>
    </channel>
</rss>