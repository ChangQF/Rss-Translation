<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 28 Aug 2024 18:20:40 GMT</lastBuildDate>
    <item>
      <title>寻求用于最后一年项目的在线机器学习的资源和用例</title>
      <link>https://stackoverflow.com/questions/78924087/seeking-resources-and-use-cases-for-online-machine-learning-for-final-year-proje</link>
      <description><![CDATA[我是一名计算机科学专业大四学生，正在从事一个以在线机器学习为重点的项目（在线机器学习是一种机器学习方法，其中模型从实时数据点流中逐步学习。）。我正在寻找资源来加深我对这个主题的理解，以及在线机器学习已得到有效应用的用例。
具体来说，我感兴趣的是：
学习资源：您是否推荐任何全面的书籍、在线课程或教程来学习在线机器学习？
用例：在线机器学习的一些实际应用是什么？我很想听听任何成功实施在线 ML 技术的实际问题或系统。
研究论文：您能给我指出任何探讨在线机器学习的应用或实现的研究论文或学术文章吗？我特别想看看这种方法是如何应用于不同领域的。
任何见解或建议都将不胜感激]]></description>
      <guid>https://stackoverflow.com/questions/78924087/seeking-resources-and-use-cases-for-online-machine-learning-for-final-year-proje</guid>
      <pubDate>Wed, 28 Aug 2024 15:16:11 GMT</pubDate>
    </item>
    <item>
      <title>ML.NET 异常检测：如何获取有关异常的大小/持续时间信息？</title>
      <link>https://stackoverflow.com/questions/78923751/ml-net-anomaly-detection-how-to-get-size-duration-information-about-anomalies</link>
      <description><![CDATA[假设我有类似于销售异常检测教程中的此图像的时间序列数据，具体来说，波动的基线和峰值看起来像左侧两个黄色条之间的峰值。

虽然检测峰值和变化点相对简单，但我想获得有关每个异常的更多详细信息，例如它持续多长时间才能回到基线，以及尖峰相对于基线有多大。由于尖峰可以重叠并且应该分开，这进一步复杂化了这一点。
我试图通过检查当前值和下一个值的增量是否超过某个阈值，或者值是否返回到接近它开始时的基线来获取持续时间。这基本上可以正常工作。
我很难确定尖峰的大小。我试图获取尖峰两侧的值，取这些值的平均值并从尖峰中的所有值中减去它。这在常规尖峰上工作正常，但当尖峰重叠时会出现问题。我所有缓解这种情况的尝试都以失败告终。
使用尖峰/变化点检测的输出执行此操作的好方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78923751/ml-net-anomaly-detection-how-to-get-size-duration-information-about-anomalies</guid>
      <pubDate>Wed, 28 Aug 2024 14:03:49 GMT</pubDate>
    </item>
    <item>
      <title>Yolov9 C++ 推理输出的 x、y、宽度和高度不符合预期</title>
      <link>https://stackoverflow.com/questions/78923447/yolov9-c-inference-outputs-for-x-y-width-and-height-not-as-expected</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78923447/yolov9-c-inference-outputs-for-x-y-width-and-height-not-as-expected</guid>
      <pubDate>Wed, 28 Aug 2024 12:54:32 GMT</pubDate>
    </item>
    <item>
      <title>创建一个模型来预测供应商数据中每条记录的正确结果。是否将其链接到现有公司或创建新的</title>
      <link>https://stackoverflow.com/questions/78923422/create-a-model-to-predict-the-correct-outcome-for-each-record-in-the-vendor-data</link>
      <description><![CDATA[构建一个智能模型，能够预测供应商数据集中每条记录的正确结果 — 是将其链接到现有公司还是创建新条目。此外，对于与现有公司匹配的记录，您的模型应推荐适当的 companyID。
train_set_records.csv：包含记录以及用于训练数据的供应商数据集的属性
train_set_labels.csv：包含任务结果（“添加新”或“添加现有”）和适当的公司 ID（如果存在链接）。对于添加新案例，linkedCompany 将为 None
test_set_records.csv：包含记录以及用于评估模型的供应商数据集的属性
searchResults.parquet：每个供应商记录都有一些从 CIQ 领域检索到的相关公司。任务的结果应基于相关公司集合中共享的相关公司
注意：
如上所述，每个供应商记录只能有 2 个可能的结果。如果预测的结果是“添加存在”，则实际链接将指向单个 companyID。
有关供应商属性的定义，请使用指定的业务代表
&#39;&#39;&#39;
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classes_report
from sklearn.preprocessing import LabelEncoder
import numpy as np

# 加载数据集
train_records = pd.read_csv(&#39;Data/train_set_records.csv&#39;)
train_labels = pd.read_csv(&#39;Data/train_set_labels.csv&#39;)
test_records = pd.read_csv(&#39;Data/test_set_records.csv&#39;)
search_results = pd.read_parquet(&#39;Data/searchResults.parquet&#39;)

# 将训练记录与标签合并
train_data = train_records.merge(train_labels, on=&#39;DUNS_NUMBER&#39;)

# 特征工程（例如：编码分类变量）
label_encoders = {}
for column in train_data.select_dtypes(include=[&#39;object&#39;]).columns:
le = LabelEncoder()
train_data[column] = le.fit_transform(train_data[column].astype(str))
label_encoders[column] = le

# 拆分特征和目标
X = train_data.drop([&#39;y_hat&#39;, &#39;linkedCompany&#39;], axis=1) # 特征
y = train_data[&#39;y_hat&#39;] # 目标

# 训练-测试拆分
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# 模型训练
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 验证
y_val_pred = model.predict(X_val)
print(classification_report(y_val, y_val_pred))

# 准备测试数据
test_data = test_records.copy()
for column in test_data.select_dtypes(include=[&#39;object&#39;]).columns:
test_data[column] = label_encoders[column].transform(test_data[column].astype(str))

# 进行预测
test_predictions = model.predict(test_data)

# 创建提交 DataFrame
submission = pd.DataFrame({
&#39;DUNS_NUMBER&#39;: test_records[&#39;DUNS_NUMBER&#39;],
&#39;y_hat&#39;: [None if pred == &#39;添加新内容&#39; else search_results.loc[search_results[&#39;DUNS_NUMBER&#39;] == duns, &#39;companyID&#39;].tolist() for duns, pred in zip(test_records[&#39;DUNS_NUMBER&#39;], test_predictions)]
})

# 为“添加新内容”案例填充 None 值
submission[&#39;y_hat&#39;] = submission[&#39;y_hat&#39;].apply(lambda x: [None]*3 if x is None else x)

# 保存到 CSV
submission.to_csv(&#39;final_submission.csv&#39;, index=False)

&#39;&#39;&#39;

我们的输出应采用以下格式：
&#39;&#39;&#39;
DUNS_NUMBER y_hat
0 123456788 [None, None, None]
1 546656567 [637022184, None, None]
2 567656667 [None, None, None]
3 676878344 [36575686345, 543657546436, 5343564363, 678143566, 243565356, 5436635235]
4 3546655778 [1232335463, None,无]
&#39;&#39;&#39;
![我得到的不是链接的 comp id，而是 True 和 false。似乎是编码问题。我们需要使用它吗？][如果您有任何其他建议，请告诉我使用其他模型来完成我的任务。]]]></description>
      <guid>https://stackoverflow.com/questions/78923422/create-a-model-to-predict-the-correct-outcome-for-each-record-in-the-vendor-data</guid>
      <pubDate>Wed, 28 Aug 2024 12:48:55 GMT</pubDate>
    </item>
    <item>
      <title>如何通过 API 获取用户输入并返回预测值[关闭]</title>
      <link>https://stackoverflow.com/questions/78923414/how-to-get-an-input-through-user-via-apis-and-return-the-predicted-value</link>
      <description><![CDATA[我创建了一个机器学习模型，该模型根据输入字符串预测数值。此输入将来自外部网站，我想为此使用 API。（我现在正在使用 Jupyter，但稍后可以切换到控制台。）
我已经加载了数据表并通过 joblib 加载了模型。我还采取了必要的措施将字符串转换为整数以供进一步分析。
但是，我不确定如何最终通过 API 从用户那里获取值，我应该为我的模型创建 API 还是使用外部网站上现有的 API？
如果有人可以为我提供任一选项的资源/代码片段，那将非常有帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78923414/how-to-get-an-input-through-user-via-apis-and-return-the-predicted-value</guid>
      <pubDate>Wed, 28 Aug 2024 12:47:31 GMT</pubDate>
    </item>
    <item>
      <title>补丁创建和 HSI 分类的 PCA 导致 Colab GPU 上的系统 RAM 崩溃。有什么解决方案吗？</title>
      <link>https://stackoverflow.com/questions/78923134/patch-creation-and-pca-for-hsi-classification-causing-the-system-ram-on-colab-gp</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78923134/patch-creation-and-pca-for-hsi-classification-causing-the-system-ram-on-colab-gp</guid>
      <pubDate>Wed, 28 Aug 2024 11:39:52 GMT</pubDate>
    </item>
    <item>
      <title>MLP 回归器和简单的 ANN 模型有什么区别？</title>
      <link>https://stackoverflow.com/questions/78923116/what-is-the-difference-between-mlp-regressor-and-a-simple-ann-model</link>
      <description><![CDATA[简单的 ANN 模型和 MLP 回归器之间有什么区别吗？它们不一样吗？
我尝试在我的时间序列数据集中使用它们，当我使用 R2 分数来评估模型在测试集上的性能时，它们产生了不同的结果。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78923116/what-is-the-difference-between-mlp-regressor-and-a-simple-ann-model</guid>
      <pubDate>Wed, 28 Aug 2024 11:36:19 GMT</pubDate>
    </item>
    <item>
      <title>如何解决多标签 BigEarthNet 数据集中 LULC 分类的类别不平衡问题？</title>
      <link>https://stackoverflow.com/questions/78922709/how-to-address-class-imbalance-in-multi-label-bigearthnet-dataset-for-lulc-class</link>
      <description><![CDATA[主要问题是数据集在各个类别之间存在严重不平衡，有些类别的代表性严重不足。
在正常的多类别数据集下，这个问题可以通过过采样或欠采样轻松解决，但考虑到数据集的多标签性质，标准平衡技术并不是那么容易应用。
我考虑过标准的过采样和欠采样方法，但我不确定如何在多标签环境中有效地实现它们。我也研究过数据增强，但我不确定它会如何影响多标签分类。
我的问题如下：

是否有任何推荐的做法或技术专门用于平衡多标签数据集（如 BigEarthNet）？
是否有人使用过 BigEarthNet 或类似数据集并找到了处理类别不平衡的有效方法？
是否有任何工具、库或代码示例可以帮助平衡此上下文中的数据集？
]]></description>
      <guid>https://stackoverflow.com/questions/78922709/how-to-address-class-imbalance-in-multi-label-bigearthnet-dataset-for-lulc-class</guid>
      <pubDate>Wed, 28 Aug 2024 10:07:59 GMT</pubDate>
    </item>
    <item>
      <title>ML.Net 预测始终显示 0 值</title>
      <link>https://stackoverflow.com/questions/78921852/ml-net-prediction-showing-0-value-always</link>
      <description><![CDATA[我的输入和输出模型
public class MLInput
{

public float[] 输入 { get; set; } // 处理任意数量的标签
public float 值 { get; set; } // 目标值
}

public class MLOutPut
{
public float 预测 { get; set; } // 预测值
}

示例 ML 输入对象

[
{&quot;输入&quot;:[480.291077,535.2749,8.185375,5.358807],&quot;值&quot;:187.421143},
{&quot;输入&quot;:[551.999756,548.0591,8.9576,4.947536],&quot;值&quot;:241.330811},
{&quot;I输入”:[513.7134,562.307739,9.582937,4.62406254],“值”:173.898956},
{“输入”:[495.53476,491.688538,9.700299,4.87886333],“值”:257.493927},
{“Inp uts&quot;：[514.7245,489.179443,8.354404,5.158723]，&quot;值&quot;：133.447586}，
{&quot;输入&quot;：[490.7611,450.309448,9.798013,4.991706]，&quot;值&quot;：243.573013}，
{&quot;输入&quot; ot;:[526.8138,441.718353,9.158744,5.07973528],&quot;Value&quot;:297.033051},
{&quot;Inputs&quot;:[551.9653,531.5339,8.307111,5.23646641],&quot;Value&quot;:162.436508},
]

我的代码是这样的


 MLContext mlContext = new MLContext();
var schemaDef = Microsoft.ML.Data.SchemaDefinition.Create(typeof(MLInput));
schemaDef[&quot;Inputs&quot;].ColumnType = new Microsoft.ML.Data.VectorDataViewType(Microsoft.ML.Data.NumberDataViewType.Single, inputCount);

IDataView dataView = mlContext.Data.LoadFromEnumerable(inputData,schemaDef);
var trainTestData = mlContext.Data.TrainTestSplit(dataView, testFraction: 0.2); // 此处应使用分割分数
var trainingDataView = trainTestData.TrainSet;
var testingDataView = trainTestData.TestSet;

var pipeline = mlContext.Transforms.Concatenate(&quot;Features&quot;, nameof(MLInput.Inputs)) // 连接所有输入特征
.Append(mlContext.Transforms.CopyColumns(&quot;Label&quot;, &quot;Value&quot;)) // 将目标值复制到 Label 列 
.Append(mlContext.Regression.Trainers.Sdca(labelColumnName: &quot;Label&quot;, featureColumnName: &quot;Features&quot;, maximumNumberOfIterations: maxIterations)); // 使用回归算法

var pre = dataView.Preview();
var dta = trainingDataView.Preview();
// 训练模型
var model = pipeline.Fit(trainingDataView);

// 评估模型
var predictions = model.Transform(testingDataView);
var metrics = mlContext.Regression.Evaluate(predictions, labelColumnName: &quot;Label&quot;);

// 输出评估指标
Console.WriteLine($&quot;R^2: {metrics.RSquared}&quot;);
Console.WriteLine($&quot;平均绝对误差: {metrics.MeanAbsoluteError}&quot;);

//使用模型进行预测

var predictionEngine = mlContext.Model.CreatePredictionEngine&lt;MLInput, MLOutPut&gt;(mo​​del, inputSchemaDefinition: schemaDef);

var newData = new MLInput { Inputs = new float[] { 530.0f, 510.0f, 8.5f, 5.0f } };
var prediction = predictionEngine.Predict(newData);

Console.WriteLine($&quot;预测值：{prediction.Prediction}&quot;);

我的预测值始终为 0，有什么问题吗，请帮帮我，我是 ML.Net 新手，我刚刚开始使用样本。]]></description>
      <guid>https://stackoverflow.com/questions/78921852/ml-net-prediction-showing-0-value-always</guid>
      <pubDate>Wed, 28 Aug 2024 06:34:11 GMT</pubDate>
    </item>
    <item>
      <title>unstructured.document.html 中的 ModuleNotFound 错误</title>
      <link>https://stackoverflow.com/questions/78921827/modulenotfound-error-in-unstructured-document-html</link>
      <description><![CDATA[我正在执行此代码
from unstructured.documents.html import HTMLDocument

# 加载您的 HTML 文件
html_file_path = &#39;UBER_2019.html&#39;
doc = HTMLDocument.from_file(html_file_path)

# 提取文本
text = doc.text


我收到一个错误，它是
ModuleNotFoundError Traceback (most recent call last)
Cell In[3], line 1
----&gt; 1 from unstructured.documents.html import HTMLDocument
3 # 加载您的 HTML 文件
4 html_file_path = &#39;UBER_2019.html&#39;

ModuleNotFoundError: 没有名为 &#39;unstructured.documents.html&#39; 的模块

那么我该怎么做才能解决这个问题呢]]></description>
      <guid>https://stackoverflow.com/questions/78921827/modulenotfound-error-in-unstructured-document-html</guid>
      <pubDate>Wed, 28 Aug 2024 06:28:50 GMT</pubDate>
    </item>
    <item>
      <title>该层需要 2 个输入，但实际收到 1 个输入张量</title>
      <link>https://stackoverflow.com/questions/78916012/layer-expects-2-inputs-but-it-received-1-input-tensors</link>
      <description><![CDATA[我正在尝试构建模型来预测帖子的点赞数，该模型采用文本和内容类型，即独热编码列。
我已经创建了一个 TensorFlow 数据集，但在尝试拟合模型时出现此错误：
层“ functional_13”需要 2 个输入，但它收到了 1 个输入张量。
收到的输入：[&lt;tf.Tensor &#39;data:0&#39; shape=(None, 1000) dtype=int64&gt;]

以下是我的一些代码片段：
dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,
content,
df[&#39;likes_rate&#39;]))

dataset= dataset.cache()
dataset= dataset.shuffle(160000)
dataset= dataset.batch(16)
dataset= dataset.prefetch(8)

这是我的模型
from tensorflow.keras.layers import Input, Embedding, Concatenate,LSTM,Bidirectional
text_input=输入（形状=（1000，））
content_input=输入（形状=（3，））

text_embeddings = tf.keras.layers.Embedding（Max_Features+1，32）（text_input）# 调整嵌入 dim
lstm= Bidirectional（LSTM（32，activation=&#39;tanh&#39;））（text_embeddings）
# 连接文本嵌入和内容特征
combined_features = tf.keras.layers.Concatenate()（[lstm，content_input]）

# 隐藏层（调整数量/激活函数）
x = tf.keras.layers.Dense（256，activation=&#39;relu&#39;）（combined_features）
x = tf.keras.layers.Dropout（0.2）（x）
x = tf.keras.layers.Dense(128,activation=&#39;relu&#39;)(x)
x = tf.keras.layers.Dropout(0.1)(x)
x = tf.keras.layers.Dense(64,activation=&#39;relu&#39;)(x)
# 用于点赞预测的输出层
output = tf.keras.layers.Dense(1,activation=&#39;linear&#39;)(x)

我想为文本创建一个嵌入层，然后将其传递给 LSTM，然后将 LSTM 的输出和内容合并到 Dense 层。
当尝试拟合模型时，我遇到了上述问题。
model = tf.keras.models.Model(inputs=[text_input, content_input],输出=输出)
model.compile(loss=&#39;mse&#39;,optimizer=&#39;Adam&#39;)
model.fit(dataset,epochs=10)

如果我迭代数据集。代码工作正常。但.fit每次都会产生随机权重，因此模型没有进展。
for text_batch, content_batch, y_batch in dataset:
# 在当前批次上训练模型
model.fit(x=[text_batch, content_batch], y=y_batch)
]]></description>
      <guid>https://stackoverflow.com/questions/78916012/layer-expects-2-inputs-but-it-received-1-input-tensors</guid>
      <pubDate>Mon, 26 Aug 2024 19:24:22 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hugging Face Transformers 训练 GPT-2 模型时如何修复分段错误？</title>
      <link>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78841125/how-to-fix-segmentation-fault-when-training-gpt-2-model-using-hugging-face-trans</guid>
      <pubDate>Tue, 06 Aug 2024 21:47:06 GMT</pubDate>
    </item>
    <item>
      <title>MediaPipe Holistic 与 Flutter 结合用于移动应用程序开发</title>
      <link>https://stackoverflow.com/questions/75697296/mediapipe-holistic-with-flutter-for-mobile-app-development</link>
      <description><![CDATA[我正在寻找在 Flutter 中使用 MediaPipe Holistic 开发移动应用程序的方法。我知道 MediaPipe 提供以下解决方案 API：

Python
Javascript

我还看到通过 ML Kit 提供了多个 MediaPipe 解决方案/模型，包括：

人脸检测
姿势检测
人脸网格检测
.... 等等

这些模型实际上是由 Flutter 插件 提供的，但该列表尚未包含整体模型。
我想构建一个移动（Android）应用程序，其中视频图像由模型。我的问题是，是否有人对如何在 Flutter 中实现这一点有什么建议？]]></description>
      <guid>https://stackoverflow.com/questions/75697296/mediapipe-holistic-with-flutter-for-mobile-app-development</guid>
      <pubDate>Fri, 10 Mar 2023 14:22:30 GMT</pubDate>
    </item>
    <item>
      <title>使用 Conda + Poetry 有意义吗？</title>
      <link>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</link>
      <description><![CDATA[在机器学习项目中使用 Conda + Poetry 是否有意义？请允许我分享我的（新手）理解，请纠正或启发我：
据我所知，Conda 和 Poetry 有不同的用途，但在很大程度上是多余的：

Conda 主要是一个环境管理器（实际上不一定是 Python），但它也可以管理包和依赖项。
Poetry 主要是一个 Python 包管理器（例如，pip 的升级），但它也可以创建和管理 Python 环境（例如，Pyenv 的升级）。

我的想法是同时使用两者并划分它们的角色：让 Conda 成为环境管理器，让 Poetry 成为包管理器。我的理由是（听起来）Conda 最适合管理环境，可用于编译和安装非 Python 包，尤其是 CUDA 驱动程序（用于 GPU 功能），而 Poetry 作为 Python 包管理器比 Conda 更强大。
我已经设法通过在 Conda 环境中使用 Poetry 相当轻松地完成这项工作。诀窍是不使用 Poetry 来管理 Python 环境：我没有使用 poetry shell 或 poetry run 之类的命令，只使用 poetry init、poetry install 等（在激活 Conda 环境后）。
为了全面披露，我的 environment.yml 文件（用于 Conda）如下所示：
name: N

channels:
- defaults
- conda-forge

dependencies:
- python=3.9
- cudatoolkit
- cudnn

我的 poetry.toml 文件如下所示：
[tool.poetry]
name = &quot;N&quot;
authors = [&quot;B&quot;]

[tool.poetry.dependencies]
python = &quot;3.9&quot;
torch = &quot;^1.10.1&quot;

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

说实话，我这样做的原因之一是，在没有 Conda 的情况下，我很难安装 CUDA（用于 GPU 支持）。
这个项目设计对你来说合理吗？]]></description>
      <guid>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</guid>
      <pubDate>Tue, 25 Jan 2022 15:09:43 GMT</pubDate>
    </item>
    <item>
      <title>如何在Python中实现Softmax函数？</title>
      <link>https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python</link>
      <description><![CDATA[从 Udacity 的深度学习课程 中，y_i 的 softmax 只是指数除以整个 Y 向量的指数和：

其中 S(y_i) 是 y_i 的 softmax 函数，e 是指数和j 是输入向量 Y 中的列数。
我尝试了以下操作：
import numpy as np

def softmax(x):
&quot;&quot;&quot;计算 x 中每组分数的 softmax 值。&quot;&quot;&quot;
e_x = np.exp(x - np.max(x))
return e_x / e_x.sum()

scores = [3.0, 1.0, 0.2]
print(softmax(scores))

返回：
[ 0.8360188 0.11314284 0.05083836]

但建议的解决方案是：
def softmax(x):
&quot;&quot;&quot;计算 x 中每组分数的 softmax 值。&quot;&quot;&quot;
返回 np.exp(x) / np.sum(np.exp(x), axis=0)

它产生与第一个实现相同的输出，即使第一个实现明确取每列与最大值的差值，然后除以总和。
有人能从数学上说明原因吗？一个是正确的，另一个是错误的吗？
在代码和时间复杂度方面，实现是否相似？哪个更有效率？]]></description>
      <guid>https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python</guid>
      <pubDate>Sat, 23 Jan 2016 20:52:50 GMT</pubDate>
    </item>
    </channel>
</rss>