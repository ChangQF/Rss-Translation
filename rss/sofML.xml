<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 14 Sep 2024 01:11:29 GMT</lastBuildDate>
    <item>
      <title>注意力图逐渐消失 - 这是正常的吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78983457/the-attention-map-fades-is-this-normal</link>
      <description><![CDATA[我目前正在构建 Vision Transformer (ViT)，到目前为止，一切似乎进展顺利 - 低损失值、高准确率。然而，当我可视化注意力图时，我注意到它们随着时间的推移逐渐消失并变得统一。我预期的情况恰恰相反 - 随着模型的学习，注意力图将被 Transformer 用于识别哪些补丁对决策的影响更大，哪些补丁的影响较小。最初，情况确实如此，但随着模型的不断学习，注意力图变得越来越统一。
似乎我的模型出了问题，或者 Transformer 在决策过程中停止关注补丁之间的关系。我很好奇是否有其他人遇到过这种行为并可以帮助我解释发生了什么。
至于模型本身，它表现良好并显示出有希望的结果，所以我倾向于认为它没有问题。然而，很难明确地说什么是“正确的”或“错误的”在这种情况下。
简而言之，如果您能帮助我解释这些结果，我将不胜感激——我不明白为什么注意力图变得如此统一，而且它们的值可以忽略不计，这表明转换器在决策过程中可能没有考虑补丁之间的关系。
我使用了超过 10k 个样本。在第 5 个时期，我得到了这些值
Epoch 5/20
1179/1179 [===============================] - 1197s 1s/step - 损失：0.1253 - 稀疏分类准确度：0.9950 - val_loss：0.0607 - val_sparse_categorical_accuracy：1.0000
谢谢你的帮助。




]]></description>
      <guid>https://stackoverflow.com/questions/78983457/the-attention-map-fades-is-this-normal</guid>
      <pubDate>Fri, 13 Sep 2024 19:05:30 GMT</pubDate>
    </item>
    <item>
      <title>ML 模型置信度问题</title>
      <link>https://stackoverflow.com/questions/78983303/ml-model-confidence-issue</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78983303/ml-model-confidence-issue</guid>
      <pubDate>Fri, 13 Sep 2024 18:07:05 GMT</pubDate>
    </item>
    <item>
      <title>无法捕捉不同的隐藏状态</title>
      <link>https://stackoverflow.com/questions/78983228/failure-to-capture-different-hidden-states</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78983228/failure-to-capture-different-hidden-states</guid>
      <pubDate>Fri, 13 Sep 2024 17:40:19 GMT</pubDate>
    </item>
    <item>
      <title>哪些机器学习技术可以结合文本和数字/分类数据？[关闭]</title>
      <link>https://stackoverflow.com/questions/78981863/what-machine-learning-techniques-can-combine-text-and-numerical-categorical-data</link>
      <description><![CDATA[问题描述：我正在开展一个项目，需要结合文本数据（例如，客户评论）和数字/分类数据（如年龄、产品类别）来构建预测模型。虽然这不是专门针对客户评论的，但我仍处于研究的早期阶段。我的数据集中有大约 150 万行，但只有大约 1200 行具有真实的 Y 值。
我正在寻找机器学习技术来处理这个问题，但我现在想避免深度学习，特别是因为我不确定我的数据集大小是否足以支持深度学习模型。
我的问题：
我可以使用哪些机器学习技术将文本数据与数字和分类数据相结合，而无需使用深度学习？我已经研究过堆叠，但我想知道还有哪些其他选项可以有效地组合这些类型的数据。
数据集详细信息：
大约 150 万行，其中只有 1200 行具有真实 Y 值。
文本特征：类似于客户评论（仅作为示例）。
数字/分类特征：年龄、产品类别等。
目标：
我正在寻找有关技术或工作流程的建议，以帮助我有效地组合这些数据类型而无需深度学习。堆叠是我最好的选择，还是我应该考虑其他方法？]]></description>
      <guid>https://stackoverflow.com/questions/78981863/what-machine-learning-techniques-can-combine-text-and-numerical-categorical-data</guid>
      <pubDate>Fri, 13 Sep 2024 11:00:29 GMT</pubDate>
    </item>
    <item>
      <title>带有随机森林的分类链：为什么即使 Base Estimator 可以处理 np.nan 却不支持它？</title>
      <link>https://stackoverflow.com/questions/78981288/classifierchain-with-random-forest-why-is-np-nan-not-supported-even-though-base</link>
      <description><![CDATA[我正在研究一个多标签分类问题，使用ClassifierChain方法，以RandomForestClassifier作为基础估计器。我遇到了一个问题，我的输入矩阵X包含np.nan值。当单独使用RandomForestClassifier时，它可以毫无问题地处理NaN值，因为它通过其内部树分割机制原生支持缺失值。
这让我很困惑，因为基础估计器（RandomForestClassifier）确实可以正确处理NaN值。我不明白为什么 ClassifierChain（它只是一个包装器）会在底层分类器没有 NaN 问题的情况下引发此错误。
当我训练一个简单的 RandomClassifier 时，它确实会处理 np.nan：
from sklearn.ensemble import RandomForestClassifier
import numpy as np

X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)
y_single_label = [0, 0, 1, 1]

tree = RandomForestClassifier(random_state=0)
tree.fit(X, y_single_label)
X_test = np.array([np.nan]).reshape(-1, 1)
tree.predict(X_test)

即使我使用 MultiOutputClassifier 而不是ClassifierChain（不模拟标签之间的依赖关系），训练进行时没有任何错误，即使输入中有 NaN - 正如预期的那样。
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import ClassifierChain , MultiOutputClassifier

X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)

# 用于多标签分类的两个标签列
y = np.array([[0, 1], [0, 0], [1, 0], [1, 1]])

# 基础分类器
base_clf = RandomForestClassifier()

# MultiOutputClassifier（二元相关性）与基础分类器
clf_BR = MultiOutputClassifier(base_clf)

# 拟合模型
clf_BR.fit(X, y)

但是，当我切换到 ClassifierChain 方法时：
# 带有基础分类器的分类器链
clf_chain = ClassifierChain(base_clf)

# 拟合模型
clf_chain.fit(X, y)

我在超参数调整期间收到以下错误：

试验 0 失败，参数为：{&#39;n_estimators&#39;: 30, &#39;max_depth&#39;: 16, &#39;max_samples&#39;: 0.4497444900238575, &#39;max_features&#39;: 550, &#39;order_type&#39;: &#39;random&#39;}，错误原因如下：ValueError(&#39;输入 X 包含 NaN.\nClassifierChain 不接受编码为 NaN 的缺失值原生。对于监督学习，您可能需要考虑 sklearn.ensemble.HistGradientBoostingClassifier 和 Regressor，它们原生接受编码为 NaN 的缺失值。或者，可以预处理数据，例如通过在管道中使用 imputer 转换器或删除具有缺失值的样本。请参阅https://scikit-learn.org/stable/modules/impute.html 您可以在以下页面找到处理 NaN 值的所有估算器的列表：https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values&#39;)

由于保持缺失值原样而不是对其进行插补或删除非常重要，我想知道是否有办法让 ClassifierChain 处理缺失值。是否有任何解决方法或我遗漏了什么？
以下是我的环境详细信息：

Python 版本：3.12.5（由 conda-forge 打包）
scikit-learn 版本：1.5.1
]]></description>
      <guid>https://stackoverflow.com/questions/78981288/classifierchain-with-random-forest-why-is-np-nan-not-supported-even-though-base</guid>
      <pubDate>Fri, 13 Sep 2024 08:25:24 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：'Dense' 对象在 TensorFlow/Keras 的 ann_visualizer 中没有属性 'output_shape'</title>
      <link>https://stackoverflow.com/questions/78980852/attributeerror-dense-object-has-no-attribute-output-shape-in-ann-visualizer</link>
      <description><![CDATA[我遇到了这个错误：
Epoch 1/2
1/1 ━━━━━━━━━━━━━━━━━━━━━━ 0s 391ms/step - 损失：6272135168.0000
Epoch 2/2
1/1 ━━━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - 损失：6272133632.0000
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step
均方误差：7431671762.639743
回溯（最近一次调用）：

文件 ~\anaconda3\Lib\site-packages\spyder_kernels\py3compat.py:356 in compat_exec
exec(code, globals, locals)

文件 c:\users\mouli\.spyder-py3\temp.py:27
ann_viz(model,title=&#39;线性回归&#39;)

文件 ~\anaconda3\Lib\site-packages\ann_visualizer\visualize.py:42 in ann_viz
input_layer = int(str(layer.input_shape).split(&quot;,&quot;)[1][1:-1]);

AttributeError: &#39;Dense&#39; 对象没有属性 &#39;input_shape

为此：
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import pandas as pd`
`# 导入数据集
dataset = pd.read_csv(r&quot;Salary_Data (1).csv&quot;)
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = keras.Sequential([
keras.layers.Input(shape=(1,)),
keras.layers.Dense`(1)
])`
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)
model.fit(`你的文本`X_train, y_train, epochs=2, batch_size=32, verbose=1)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f&quot;均方误差：{mse}&quot;)`

# 可视化数据和回归线

# 可视化神经网络
from ann_visualizer.visualize import ann_viz
from graphviz `你的文本`import Source
ann_viz(model,title=&#39;线性回归&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78980852/attributeerror-dense-object-has-no-attribute-output-shape-in-ann-visualizer</guid>
      <pubDate>Fri, 13 Sep 2024 06:14:36 GMT</pubDate>
    </item>
    <item>
      <title>我如何创建一个模型来根据过去的选择预测会选择哪个选项？</title>
      <link>https://stackoverflow.com/questions/78980646/how-do-i-create-a-model-to-predict-which-option-will-be-chosen-based-on-past-cho</link>
      <description><![CDATA[我有一个数据集，其中包含一系列涉及用户选择的购买。每个数据集包括：

1 个或多个可供购买的商品（商品 + 价格 + 每个商品的详细信息）
用户从可用商品中选择了哪个商品

通常，只有一件商品可供购买，但有时会有多个。对于有多个商品的情况，我希望建立一个可以预测将购买哪些商品的模型。我对预测型算法（和机器学习）还不熟悉，因此欢迎提出建议。]]></description>
      <guid>https://stackoverflow.com/questions/78980646/how-do-i-create-a-model-to-predict-which-option-will-be-chosen-based-on-past-cho</guid>
      <pubDate>Fri, 13 Sep 2024 04:40:07 GMT</pubDate>
    </item>
    <item>
      <title>线性模型中的规范化[关闭]</title>
      <link>https://stackoverflow.com/questions/78979243/normalization-in-linear-models</link>
      <description><![CDATA[我知道对于线性模型来说，有必要对所有数据进行归一化。但是如果我们得到一个预测，它也将被归一化。我怎样才能在原始尺度上得到这个预测？
例如，如果我进行标准化（减去平均值并除以方差），我认为当我得到模型预测（假设为 X）时，我可以执行 X_original = X * 方差 + 平均值。但是有没有内置的方法可以做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78979243/normalization-in-linear-models</guid>
      <pubDate>Thu, 12 Sep 2024 17:03:00 GMT</pubDate>
    </item>
    <item>
      <title>FFN 模型在预测总和方面实现了 100% 的准确率</title>
      <link>https://stackoverflow.com/questions/78975293/ffn-model-achieving-100-accuracy-in-predicting-sums</link>
      <description><![CDATA[我有一个模型，可以对 -10 到正 10 之间的数字进行加法运算，但使用神经网络通过两个数字相加的数据集来预测结果。然而，在获得训练准确度时，它只是打印出很多 100% 的准确度。我不确定模型是否只是快速训练，或者是否存在问题并且没有正确学习。有人能提供一些见解吗？
这是我的代码
import torch
import torch.nn as nn
import torch.nn. functional as F
from torch.utils.data import DataLoader,TensorDataset
from sklearn.model_selection import train_test_split

import numpy as np

import matplotlib.pyplot as plt
import matplotlib_inline.backend_inline
matplotlib_inline.backend_inline.set_matplotlib_formats(&#39;svg&#39;)

data = []
labels = []

datasetAmount = 2000

for i in range(datasetAmount):
x = np.random.randint(-10, 10)
y = np.random.randint(-10,10)
bothNumber = [x,y]
data.append(bothNumber)
labels.append(x+y)

data_np = np.array(data)
labels_np = np.array(labels).reshape(-1,1)

train_data, test_data, train_labels, test_labels = train_test_split(data_np, labels_np, train_size =.9)

train_data = TensorDataset(torch.tensor(train_data),torch.tensor(train_labels))
test_data = TensorDataset(torch.tensor(test_data),torch.tensor(test_labels))

batchsize = 20

train_loader = DataLoader(train_data, batch_size = batchsize, shuffle = True, drop_last = True)
test_loader = DataLoader(test_data, batch_size = test_data.tensors[0].shape[0])

def createModel():
class myModel(nn.Module):
def __init__(self):
super().__init__()

self.input = nn.Linear(2,8)
self.fc1 = nn.Linear(8,8)
self.output = nn.Linear(8,1)

def forward(self,x):
x = F.relu( self.input(x) )
x = F.relu( self.fc1(x) )
return self.output(x)

net = myModel()
lossfun = nn.MSELoss()
optimizer = torch.optim.SGD(net.parameters(),lr=.001)

return net,lossfun,optimizer

def trainModel():

numepochs = 100
net,lossfun,optimizer = createModel()
loss = torch.zeros(numepochs)
trainacc = []
testacc = []

for epochi in range(numepochs):
batchLoss = []

for X,y in train_loader:
X = X.float()
y = y.float()
yHat = net(X)

loss = lossfun(yHat,y)
batchLoss.append(loss.item())

optimizer.zero_grad()
loss.backward()
optimizer.step()

loss[epochi] = np.mean(batchLoss)

with torch.no_grad():
train_predictions = []
train_labels = []
for x_train, y_train in train_loader:
x_train = x_train.float()
y_train = y_train.float()
train_pred = net(x_train)
train_predictions.append(train_pred)
train_labels.append(y_train)

train_predictions = torch.cat(train_predictions)
train_labels = torch.cat(train_labels)

train_acc = 100 * torch.mean((np.abs(train_predictions - train_labels) &lt; 1).float())
trainacc.append(train_acc.item())

X,y = next(iter(test_data))
X = X.float() # 将 X 转换为浮点数用于测试数据
y = y.float() # 将 y 转换为浮点数用于测试数据
with torch.no_grad():
yHat = net(X)

testacc= 100*torch.mean((np.abs(yHat-y)&lt; 1).float())

return trainacc,testacc,losses,net

trainAcc, testAcc, loss , net = trainModel()


模型有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78975293/ffn-model-achieving-100-accuracy-in-predicting-sums</guid>
      <pubDate>Wed, 11 Sep 2024 18:43:48 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenCV 改进这种图像自然背景扩展方法？</title>
      <link>https://stackoverflow.com/questions/78969286/how-can-i-improve-this-approach-for-natural-background-extension-in-an-image-usi</link>
      <description><![CDATA[我正在使用 Python 中的 OpenCV 扩展图像的背景。我目前的方法是复制边框并对扩展区域应用高斯模糊以将它们混合到原始图像中。目标是使背景扩展看起来更自然，尤其是对于具有一致纹理的图像。
这是我当前使用的代码：
import cv2
import numpy as np

def expand_image_with_smart_blend(image_path, top=50, bottom=50, left=50, right=50):
img = cv2.imread(image_path)
original_h, original_w = img.shape[:2]

expanded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_REPLICATE)

blured_img = expand_img.copy()

if top &gt; 0:
blured_img[0:top, :] = cv2.GaussianBlur(expanded_img[0:top, :], (51, 51), 0)

如果底部 &gt; 0:
blured_img[original_h + top:original_h + top + bottom, :] = cv2.GaussianBlur(expanded_img[original_h + top:original_h + top + bottom, :], (51, 51), 0)

如果左侧 &gt; 0:
blured_img[:, 0:left] = cv2.GaussianBlur(expanded_img[:, 0:left], (51, 51), 0)

如果右侧 &gt; 0:
blured_img[:, original_w + left:original_w + left + right] = cv2.GaussianBlur(expanded_img[:, original_w + left:original_w + left + right], (51, 51), 0)

cv2.namedWindow(&quot;智能混合扩展图像&quot;, cv2.WINDOW_NORMAL)
cv2.namedWindow(&quot;原始图像&quot;, cv2.WINDOW_NORMAL)
cv2.imwrite(&#39;expanded_smart_blended_image.jpg&#39;, blured_img)
cv2.imshow(&#39;智能混合扩展图像&#39;, blured_img)
cv2.imshow(&quot;原始图像&quot;, img)
cv2.waitKey(0)
cv2.destroyAllWindows()

expand_image_with_smart_blend(&#39;test_img.jpg&#39;, top=100, bottom=100, left=100, right=100)

我尝试过的方法：
cv2.BORDER_REPLICATE：我使用它将原始图像的边缘复制到新扩展的区域中。
高斯模糊：应用于扩展区域以柔化原始图像和新区域之间的过渡。
问题：
结果在某种程度上是可以接受的，但过渡仍然看起来不像我想要的那样自然。特别是：
某些区域的过度模糊使背景看起来不真实。
对于纹理更复杂的图像，边缘复制并不总是有效。
原始图像 结果图像
问题：
在 OpenCV 或其他库中，是否有更复杂的方法来扩展图像的背景，从而产生更自然、无缝的结果？我愿意接受涉及高级图像处理技术或机器学习的方法。任何使用扩散模型的方法都可以。]]></description>
      <guid>https://stackoverflow.com/questions/78969286/how-can-i-improve-this-approach-for-natural-background-extension-in-an-image-usi</guid>
      <pubDate>Tue, 10 Sep 2024 11:32:09 GMT</pubDate>
    </item>
    <item>
      <title>自定义模型聚合器 TensorFlow Federated</title>
      <link>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</link>
      <description><![CDATA[我正在尝试使用 TensorFlow Federated，使用 FedAvg 算法模拟训练过程。
def model_fn():
# 包装 Keras 模型以用于 TensorFlow Federated
keras_model = get_uncompiled_model()

# 对于联合过程，模型必须是未编译的
return tff.learning.models. functional_model_from_keras(
keras_model,
loss_fn=tf.keras.losses.BinaryCrossentropy(),
input_spec=(
tf.TensorSpec(shape=[None, X_train.shape[1]], dtype=tf.float32),
tf.TensorSpec(shape=[None], dtype=tf.int32)
),
metrics_constructor=collections.OrderedDict(
accuracy=tf.keras.metrics.BinaryAccuracy,
precision=tf.keras.metrics.Precision,
recall=tf.keras.metrics.Recall,
false_positives=tf.keras.metrics.FalsePositives,
false_negatives=tf.keras.metrics.FalseNegatives,
true_positives=tf.keras.metrics.TruePositives,
true_negatives=tf.keras.metrics.TrueNegatives
)
)

trainer = tff.learning.algorithms.build_weighted_fed_avg(
model_fn= model_fn(),
client_optimizer_fn=client_optimizer,
server_optimizer_fn=server_optimizer
)

我想使用自定义权重来聚合客户端的更新，而不是使用它们的样本。我知道 tff.learning.algorithms.build_weighted_fed_avg() 有一个名为 client_weighting 的参数，但唯一接受的值来自类 tff.learning.ClientWeighting，它是一个枚举。
因此，唯一的方法似乎是编写自定义 WeightedAggregator。我尝试按照本教程进行操作，该教程解释了如何编写无加权聚合器，但我无法将其转换为加权聚合器。
这是我尝试做的：
@tff.tensorflow.computation
def custom_weighted_aggregate(values, weights):
# 规范化客户端权重
total_weight = tf.reduce_sum(weights)
normalized_weights = weights / total_weight

# 计算客户端更新的加权总和
weighted_sum = tf.nest.map_structure(
lambda v: tf.reduce_sum(normalized_weights * v, axis=0),
values
)

return weighted_sum

class CustomWeightedAggregator(tff.aggregators.WeightedAggregationFactory):
def __init__(self):
pass

def create(self, value_type, weight_type):
@tff.federated_computation
def initialize():
return tff.federated_value(0.0, tff.SERVER)

@tff.federated_computation(
initialize.type_signature.result,
tff.FederatedType(value_type, tff.CLIENTS),
tff.FederatedType(weight_type, tff.CLIENTS)
)
def next(state, value, weight):
aggregate_value = tff.federated_map(custom_weighted_aggregate, (value, weight))
return tff.templates.MeasuredProcessOutput(
state,aggregate_value,tff.federated_value((),tff.SERVER)
)

return tff.templates.AggregationProcess(initialize,next)

@property
def is_weighted(self):
return True

但是我得到了以下错误：
AggregationPlacementError：next_fn 返回类型的“result”属性必须放置在 SERVER 中，但发现 {&lt;float32[7],float32,float32[1],float32&gt;}@CLIENTS。]]></description>
      <guid>https://stackoverflow.com/questions/78835380/custom-model-aggregator-tensorflow-federated</guid>
      <pubDate>Mon, 05 Aug 2024 16:06:48 GMT</pubDate>
    </item>
    <item>
      <title>无法更改嵌入维度以将其传递给 gpt2</title>
      <link>https://stackoverflow.com/questions/74996908/cant-change-embedding-dimension-to-pass-it-through-gpt2</link>
      <description><![CDATA[我正在练习图像字幕，在张量的不同维度方面遇到了一些问题。所以我的图像嵌入又名大小 [1, 512]，但我用于字幕生成的 GPT2 需要大小 [n, 768]，其中 n 是字幕开头的标记数。我不知道如何更改图像嵌入的维度以使其通过 GPT2。
我认为用零填充图像嵌入是个好主意，因此大小将是 [1, 768]，但我认为这会对结果字幕产生负面影响。
谢谢你的帮助！
我曾尝试用零填充图像嵌入，使其大小为 [1, 768]，但我认为这不会有太大帮助]]></description>
      <guid>https://stackoverflow.com/questions/74996908/cant-change-embedding-dimension-to-pass-it-through-gpt2</guid>
      <pubDate>Tue, 03 Jan 2023 17:50:56 GMT</pubDate>
    </item>
    <item>
      <title>批次内的数据是否应该平衡？</title>
      <link>https://stackoverflow.com/questions/48200136/should-the-data-in-batch-be-balanced</link>
      <description><![CDATA[我正在训练一个深度学习模型，通过输入推文内容来预测三种情绪（快乐、悲伤、愤怒）。
我遇到的一个问题是，我的模型在悲伤、快乐方面学习得很好，但在快乐方面学习得很糟糕。

我认为原因是我的训练数据集不平衡。
快乐中的数据大小：196952，悲伤：29407，愤怒：42420
因此，在训练模型时，批量大小包含太多快乐数据集，这使得模型只能猜测答案是快乐而不是其他。
我想通过平衡每个数据集中的数据来解决这个问题批次。
也就是说批次大小为 128，我们随机选择相同数量的三种情绪数据。防止模型被快乐数据所主导。
问题是：批次中的数据应该平衡吗？
另一个问题是，我随机选择了数据集，这是否违反了 epoch 的定义？
因为 epoch 意味着读取所有训练数据集。当随机选择时，可能某些数据集在某些 epoch 中不会被选择。或者只需训练更多 epoch 即可解决此问题？]]></description>
      <guid>https://stackoverflow.com/questions/48200136/should-the-data-in-batch-be-balanced</guid>
      <pubDate>Thu, 11 Jan 2018 05:09:56 GMT</pubDate>
    </item>
    <item>
      <title>推荐相关项目的推荐算法</title>
      <link>https://stackoverflow.com/questions/43029589/recommender-algorithm-to-recommend-associated-items</link>
      <description><![CDATA[我有一个场景，我想提供建议。为简单起见，假设我有包含购买和购买项目记录的数据库表。每个购买项目都描述了应用程序向用户呈现的每件商品的数量、描述（鞋子、裤子、衬衫）以及总价。通常，与特定购买相关的购买项目或一系列购买项目会自动触发添加其他相关购买项目以进行类似类型的购买（也许他们想添加帽子或手套）。
需要注意的是，没有为这些类型的项目分配类别。它们可能完全不相关，除非它们定期一起应用于购买（衬衫、食物和相机不相关，但可能经常一起购买）。也就是说，这是用户的购买习惯，而不是（推荐帽子，因为它们是一种像衬衫一样的服装）。
我尝试了推荐算法，但不完全了解如何在这种情况下应用它。这是我应该关注的正确算法吗？]]></description>
      <guid>https://stackoverflow.com/questions/43029589/recommender-algorithm-to-recommend-associated-items</guid>
      <pubDate>Sun, 26 Mar 2017 13:52:13 GMT</pubDate>
    </item>
    <item>
      <title>多步预测神经网络</title>
      <link>https://stackoverflow.com/questions/10327260/multi-step-prediction-neural-networks</link>
      <description><![CDATA[我一直在使用 matlab 神经网络工具包。这里我使用的是 NARX 网络。我有一个数据集，其中包含某个物品的价格以及在一段时间内购买的物品数量。本质上，这个网络执行一步预测，其数学定义如下：
y(t)= f (y(t −1),y(t −2),...,y(t −ny),x(t −1),x(t −2),...,x(t −nx))
这里 y(t) 是时间 t 时的价格，x 是金额。所以我使用的输入特征是价格和金额，目标是时间 t+1 时的价格。假设我有 100 条此类交易的记录，每笔交易都包含价格和金额。那么我的神经网络基本上可以预测第 101 笔交易的价格。这对于一步预测来说效果很好。但是，如果我想进行多步预测，比如说我想预测未来 10 笔交易（第 110 笔交易），那么我假设我对价格进行一步预测，然后将其反馈给神经网络。我一直这样做，直到达到第 110 次预测。但是，在这种情况下，在我预测第 101 个价格后，我可以将此价格输入神经网络以预测第 102 个价格，但是，我不知道第 101 笔交易的物品数量。我该怎么做？我正在考虑将我的目标设置为比当前交易提前 10 笔交易的交易价格，这样当我预测第 101 笔交易时，我基本上就是在预测第 110 笔交易的价格。这是一个可行的解决方案吗，还是我完全错误地处理了这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/10327260/multi-step-prediction-neural-networks</guid>
      <pubDate>Thu, 26 Apr 2012 04:28:45 GMT</pubDate>
    </item>
    </channel>
</rss>