<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 07 Apr 2025 09:21:43 GMT</lastBuildDate>
    <item>
      <title>在预测Feed向前神经网络时，如何处理可变Lenght的输出？</title>
      <link>https://stackoverflow.com/questions/79559377/how-to-handle-outputs-of-variable-lenght-when-predicting-with-a-feed-forward-neu</link>
      <description><![CDATA[我正在处理一个回归问题，在给定固定大小的输入x时，输出y可以是可变长度值的序列。
输入和输出都是归一化的浮点值。因此，我们正在谈论回归任务。
问题以y数组样本的可变大小（最大长度46）。
平均而言，只有前30-35个值（在46中）有效。
因此，要训练网络，我正在尝试的解决方案是：

将每个Y阵列样品的非valid值（0.0）
火车和预测
&#39;undad＆quot＆quot＆quot输出阵列通过删除所有尾随零（或非常小的值）。

问题是：

实际上，小值在0.0左右也可以是有效值，从而产生歧义。也许最好使用像-100这样的可笑的怪异数字？
看来，网络从未真正从有效的谷（例如：2.3）跳到0.0，但是它正在产生平稳的过渡到0.0，从而产生了非常糟糕的输出。

解决这个问题的好解决方案是什么？
如果数组值是整数，则可以使用特殊的int作为令牌。但是拥有浮子会使一切变得更加棘手。]]></description>
      <guid>https://stackoverflow.com/questions/79559377/how-to-handle-outputs-of-variable-lenght-when-predicting-with-a-feed-forward-neu</guid>
      <pubDate>Mon, 07 Apr 2025 08:13:54 GMT</pubDate>
    </item>
    <item>
      <title>tflite-runtime valueError：找不到opcode'firms'connected的'12'的op op。可能支持此内置的较旧版本</title>
      <link>https://stackoverflow.com/questions/79559242/tflite-runtime-valueerror-didnt-find-op-for-builtin-opcode-fully-connected-v</link>
      <description><![CDATA[我已经在 tensorflow == 2.19.0 上训练和量化了我的模型，而不是以前的版本，现在正在使用定量模型来推断我的Raspberry Pi（Raspbian GNU/Linux 11 Bullseye）面临麻烦。最新版本的 tflite-runtime 似乎是 2.13.0 ，当我运行时，我会遇到以下错误：
  interneter = tflite.interpreter（model_content = tflite_model）
 
  value eRror：找不到内置opcode的op op op opcode&#39;firmon_connected&#39;版本&#39;12&#39;。可以支持此内置的较旧版本。您是否正在使用具有较新型号的旧Tflite二进制文件？
 
我确实知道，如果我将张力流降低到2.13并训练+量化我的型号，但是我有30个型号需要大约2周的时间来培训和量化我的资源。，我确实可以解决此问题。
我可以在Raspberry Pi上使用最新的 tflite-Interpreter 吗？在这方面的任何帮助都很好！
 PC ]]></description>
      <guid>https://stackoverflow.com/questions/79559242/tflite-runtime-valueerror-didnt-find-op-for-builtin-opcode-fully-connected-v</guid>
      <pubDate>Mon, 07 Apr 2025 06:55:44 GMT</pubDate>
    </item>
    <item>
      <title>如何反向标签编码值以比较结果，（产品代码说明）</title>
      <link>https://stackoverflow.com/questions/79558994/how-to-revers-label-encoded-values-to-compare-the-results-product-code-descri</link>
      <description><![CDATA[ i标记了数据集中的所有列，在运行了随机孔后，我想找到“每个产品代码描述”中有多少个记录。得到正确的结果，我该怎么做？
我想通过“产品代码描述” 查看结果]]></description>
      <guid>https://stackoverflow.com/questions/79558994/how-to-revers-label-encoded-values-to-compare-the-results-product-code-descri</guid>
      <pubDate>Mon, 07 Apr 2025 02:50:50 GMT</pubDate>
    </item>
    <item>
      <title>如何消除功能生成期间自定义某些服务的需求[关闭]</title>
      <link>https://stackoverflow.com/questions/79558725/how-to-remove-the-need-for-custom-deployment-of-some-services-during-feature-gen</link>
      <description><![CDATA[每当我必须运行功能生成器时，我都需要切换到另一个分支并在代码中进行一些更改，例如在什么日期范围内运行的DATE范围，并且在部署中进行了一些更改。yaml，service.yaml文件等。然后，我部署了 userAttribute  and strong&gt;和   useractivity   Service。  如何消除自定义部署这些服务的需求？
这些是我遵循的步骤：
生成ML功能
鉴于Zentari的大小，因此在使用Prod群集的Zentari运行功能生成器中现有问题，我们遵循以下步骤为Zentari生成ML功能。
🚨caution：非常仔细地按照步骤
产卵自定义BT实例
🚨深吸一口气。仔细记下名称。验证在任何最终操作之前提到的所有名称 /配置（创建 /修复 /删除）&lt; / p&gt;
您将需要从PAM中扮演Boogtable Admin角色。
备份NOVEXA-DATA实例。选择群集NOVEXA-DATA-JP-C1从中备份。用格式Zentari-FG-Backup-给备份一个有意义的名称。 [yyyymmdd]选择备份到期为1天。
创建一个具有以下配置的新大表实例
名称和ID：Zentari-Training-Data-Instance 
存储类型：SSD 
群集ID：Zentari-training-data-insta-c1 
区域：亚洲 - 东北1（东京）
区域：任何
节点缩放模式：自动化（最小节点：5，最大节点：8）
转到novexa-data实例→备份。选择您创建的备份。单击还原。从实例下拉列表中选择实例zentari-training-data-instance。将表ID作为HB-prod-data-backup。单击创建。
进行更改以准备要功能生成器
将PR中的分支拉动，然后对Master的最新更改进行更新。请注意，请仔细注意文件feature generatorPipeline.kt，它仅在处理时间戳时具有生成功能的流动。 
更新文件：
 userActivity/eventtransfer/featuregenerator/src/src/ain/kotlin/io/novexa/userActivity/eventtransfer/eventtransfer/featuregenerator/options.kt 
在Snapshottimestamp中更新日期到最后一个日期 + 1，您想生成功能。 （例如：今天-2D + 1）
末日更新到您要生成的SnapShotTimestamp的天数。
部署功能生成器＆amp; prod上的用户属性批处理服务。这应该在工作负载中提出实例zentari-eventtransfer-featureGenerator。
 ./ deploy/dev/decloy.sh jp-prod：userAttribute：service：debolyall 
 ./ deploy/dev/decloy.sh JP-prod：userActivity：EventTransfer：featuregenerator：Decloperall 
开始工作并监视
作业完成后，删除自定义的bigtable实例zentari-training-data-instance和backup zentari-fg-backup-您从BigTable创建了为此实例创建的。
还删除了名称：
 zentari-userattribute-batch-service-deployment ]]></description>
      <guid>https://stackoverflow.com/questions/79558725/how-to-remove-the-need-for-custom-deployment-of-some-services-during-feature-gen</guid>
      <pubDate>Sun, 06 Apr 2025 20:47:13 GMT</pubDate>
    </item>
    <item>
      <title>检测到重复的GPU：CUDA设备上的等级为0和等级1 40</title>
      <link>https://stackoverflow.com/questions/79557948/duplicate-gpu-detected-rank-0-and-rank-1-both-on-cuda-device-40</link>
      <description><![CDATA[我试图使用2x T4 GPU在Kaggle上进行Qlora+FSDP2，这是我的培训脚本
  def train_fsdp（等级，大小）：
    TORCH.CUDA.EMPTY_CACHE（）
    base_model =＆quot; meta-llama/llama-3.1-8b; quot
    bnb_config = bitsandbytesconfig（-------）
    型号= automodelforcausallm.from_pretaining（
        base_model，
        attn_implementation =＆quot; sdpa＆quot;
        ventalization_config = bnb_config，
        TORCH_DTYPE = TORCH.BFLOAT16，
    ）
    tokenizer = autotokenizer.from_pretaining（
        base_model，
        padding_side =; left＆quot;
        add_eos_token = true，
        add_bos_token = false，
        use_fast = true
    ）
    lora_config = loraconfig（-----）
    model.add_adapter（lora_config）
    fsdp_model = fsdp（
        模型，
        sharding_strategy = shardingstrategy.hybrid_shard，
        auto_wrap_policy = my_auto_wrap_policy，
        device_id =等级，
        混合_precision = myd_precision_policy，
        backward_prefetch = backwardprefetch.backward_pre，
        forward_prefetch = true，
    ）
    triending_arguments = sftConfig（
        ........
    ）
    培训师= sfttrainer（
        型号= fsdp_model，
        train_dataset = dataset_train_pre，
        processing_class = tokenizer，
        args = triending_arguments，
    ）
    对于名称，triber.model.named_modules（）中的模块：
        如果“规范”名称：
            模块=模块。
    Trainer.Train（）
    dist.destroy_process_group（）
    
DEF INIT_PROCESS（等级，大小，FN）：
    os.environ [&#39;master_addr&#39;] =&#39;127.0.0.1&#39;
    OS.Environ [&#39;Master_port&#39;] =&#39;29501&#39;
    os.environ [&#39;nccl_debug&#39;] =&#39;info&#39;
    os.environ [&#39;nccl_ib_disable&#39;] =&#39;1&#39;
    os.environ [&#39;nccl_socket_ifname&#39;] =&#39;lo&#39;
    os.environ [＆quot; pytorch_cuda_alloc_conf;]
    ＆quot&#39;roundup_power2_divisions：[32：256,64：128,256：64，＆gt;：32]＆quot;
    dist.init_process_group（
        后端=&#39;nccl&#39;，
        等级=等级，
        world_size =大小，
        超时= timedelta（分钟= 5）
    ）
    FN（等级，大小）
如果__name__ ==＆quot __ Main __＆quot;：
    world_size = 2
    过程= []
    mp.get_context（“ Spawn＆quort”）
    对于范围的排名（world_size）：
        p = mp.process（target = init_process，args =（rank，world_size，train_fsdp））
        p.start（）
        process.append（p） 
    对于P流程的P：
        P.Join（）
 
但是我一直在遇到上述错误（标题）我想问题是我在不同等级上初始化模型的方式，但我不完全确定。
详细的错误日志：
  TORCH.OUTOFMEMORYERROR：CUDA失败。试图分配1.97吉布。 GPU 0的总容量为14.74 GIB，其中1.91 GIB是免费的。 Process 4629使用了5.43 GIB内存。 Process 4627使用了7.40个GIB内存。在分配的内存4.76 GIB中，由Pytorch分配，498.26 MIB由Pytorch保留，但未分配。如果保留但未分配的内存是大的，请尝试设置pytorch_cuda_alloc_conf = Expandable_segments：true以避免碎片。  请参阅文档以获取内存管理  
 ../torch/csrc/distributed/c10d/ncclutils.hpp:317，无效用法（使用nccl_debug = WARN进行详细信息），NCCL版本2.21.5
NCCLINVALIDUSAGE：这通常反映了NCCL库的无效用法。
最后错误：
检测到重复的GPU：CUDA设备上的等级为0和等级1 40
 
我也尝试过，但也没有得到结果，我也尝试过设置型号。
fsdp_model = FSDP( model, sharding_strategy=ShardingStrategy.HYBRID_SHARD, auto_wrap_policy=my_auto_wrap_policy, device_id=rank, mixed_precision=mixed_precision_policy, backward_prefetch=BackwardPrefetch.BACKWARD_PRE, forward_prefetch=True, ） ]]></description>
      <guid>https://stackoverflow.com/questions/79557948/duplicate-gpu-detected-rank-0-and-rank-1-both-on-cuda-device-40</guid>
      <pubDate>Sun, 06 Apr 2025 08:10:36 GMT</pubDate>
    </item>
    <item>
      <title>CNN模型的问题以预测黑白图像的颜色[封闭]</title>
      <link>https://stackoverflow.com/questions/79557643/problem-with-cnn-model-for-prediction-of-color-from-an-black-and-white-image</link>
      <description><![CDATA[我已经创建了一个CNN模型，该模型作为输入13x13图像刻度刻度实验室L通道。我用12000张图像培训了它，并获得了7个MSE错误。
要使用该模型，我需要将图像切成13x13的小图像，然后通过网络进行馈送。
我从原始小图像中将 l通道从一个图像中放在一个像素上，并将其放在一个像素中，以将其放在一个像素上，以供带有颜色的预测图像。之后，我将所有这些预测的像素放在一起并形成预测的图像。
这是创建这些预测像素的功能：
  def create_pxl_from_preds（input_image，预测）：
    prediction_rescaled = torch.mul（预测，128）
    a，b = prediction_rescaled [0]
    a = a.detach（）。numpy（）
    b = b.detach（）。numpy（）

    l_channel = input_image [6，6]

    a_channel = np.full（（1，1），a，dtype = np.float32）
    b_channel = np.full（（1，1），b，dtype = np.float32）
    l_channel = np.full（（1，1），l_channel，dtype = np.float32）

    l_channel =（l_channel / 255 * 100）.astype（np.float32）
    a_channel = a_channel.astype（np.float32）
    b_channel = b_channel.astype（np.float32）

    image_pred = cv2.merge（[l_channel，a_channel，b_channel]）
    返回image_pred
 
之后，我有一个函数来构建图像行和一个函数，可以将这些行放在一起并形成整个图像。另外，总有487个图像，也有487行一起
  def rebuild_image_pxl_row（
        start_calc：int，
        end_calc：int，
        num_sections_per_row = 487，
） - ＆gt;没有任何：
    tensor_rows = torch.arange（start_calc，end_calc）
    list_rows = tensor_rows.tolist（）
    add_index = 0
    对于我在list_rows中：
        idx = i

        image_files = [f in os.listdir（temp_folder_images）如果f.lower（）。endswith（（（&#39;。
        image_files.sort（key = extract_numbers）

        start_idx = idx * num_sections_per_row
        start_idx += add_index

        end_idx = start_idx + num_sections_per_row
        row_sections = image_files [start_idx：end_idx]
        print（f＆quot&#39;start idx：{start_idx}，end IDX：{end_idx}＆quort;）

        row_images = []

        对于索引，枚举中的section_file（row_sections）：
            section_path = os.path.join（temp_folder_images，section_file）
            section_image = cv2.imread（section_path，cv2.imread_grayscale）
            section_tensor = torch.from_numpy（section_image）.float（）。unsqueeze（0）.unsqueeze（0）
            pection_pred = conv_model（extract_tensor）
            section_reconstructed = create_pxl_from_preds（extart_image，section_pred）
            row_images.append（np.Round（cv2.cvtcolor（extife_reconstructed，cv2.color_lab2bgr）*255.0，0））

        行= np.hstack（row_images）
        row_path = os.path.join（temp_folder_rows，f＆quot; row_ {idx} .png; quert;）
        cv2.imwrite（row_path，行）
        add_index += 1
        打印（f＆quort&#39;row {idx}重建并保存为{row_path}。
 
  def rebuild_image_pxl（row_ordner，target_height = 487）：
    row_files = [f in os.listdir（row_ordner）如果f.lower（）。endswith（（（&#39;。png&#39;，&#39;.jpg&#39;，&#39;.jpeg&#39;，&#39;.bmp&#39;））]]]]]]]]]
    row_files.sort（key = extract_numbers）
    打印（row_files [：20]）

    row_files = row_files [：target_height]

    all_rows = []

    对于索引，ROW_FILE在枚举中（row_files）：
        row_path = os.path.join（row_ordner，row_file）
        row_image = cv2.imread（row_path）
        all_rows.append（row_image）

    final_image = cv2.vconcat（all_rows）
    final_image = np.fliplr（final_image）
    final_image = cv2.Rotate（final_image，cv2.rotate_90_counterclockwise）
    cv2.imwrite（&#39;image.png; final_image）
    show_image（&#39;image.png;）
 
我将OpenCV用作图像处理库。
但是，当我尝试运行程序时，不会发生任何错误，但是预测图像的颜色不正确，甚至不接近原始图像。检测到L通道的边缘和细微差别，因此该数据的复制有效。但是颜色预测并非如此，我认为这是OpenCV和不同颜色空间BGR，RGB和LAB的东西。由于OpenCV将图像打开并保存在BGR色彩空间中，因此我只是将图像值从实验室转换为BGR。
我需要弄清楚错误在哪里的帮助。
预测的图像看起来像这样：
在此处输入图像描述 ]]></description>
      <guid>https://stackoverflow.com/questions/79557643/problem-with-cnn-model-for-prediction-of-color-from-an-black-and-white-image</guid>
      <pubDate>Sat, 05 Apr 2025 23:37:53 GMT</pubDate>
    </item>
    <item>
      <title>jinaai/jina-embeddings-v3嵌入模型没有注意力输出</title>
      <link>https://stackoverflow.com/questions/79557313/no-attention-output-in-jinaai-jina-embeddings-v3-embedding-model</link>
      <description><![CDATA[当我使用此模型时 -  
 来自变形金刚的导入汽车，自动驱动器

model_id =＆quot; jinaai/jina-embeddings-v3＆quot
tokenizer = autotokenizer.from_pretaining（model_id，trust_remote_code = true）
model = automodel.from_pretrataining（model_id，trust_remote_code = true）

输入= tokenizer（[
    “今天的天气很愉快。
    “外面太晴天！＆quot”
    “他开车去体育场。”
]，return_tensors =; pt; padding = true，truncation = true）

输出=模型（**输入，output_attentions = true）

注意力=输出
 
我得到了这个警告，这似乎是矛盾的 -  
 未安装flash_attn。使用Pytorch本地注意力实施。
Flash注意力实现不支持Kwargs：output_attentions
 
注意力是没有
我尝试了其他型号，并且可以按预期工作。]]></description>
      <guid>https://stackoverflow.com/questions/79557313/no-attention-output-in-jinaai-jina-embeddings-v3-embedding-model</guid>
      <pubDate>Sat, 05 Apr 2025 17:29:15 GMT</pubDate>
    </item>
    <item>
      <title>使用KERAS自定义数据生成器表现不佳的模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79557177/model-performing-poorly-with-keras-custom-data-generator</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79557177/model-performing-poorly-with-keras-custom-data-generator</guid>
      <pubDate>Sat, 05 Apr 2025 15:30:58 GMT</pubDate>
    </item>
    <item>
      <title>如何提高Vittracker的性能？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79556795/how-can-i-improve-the-performance-of-vittracker</link>
      <description><![CDATA[我将VitTracker与OpenCV一起使用，这实际上是一个对象跟踪器。但是它无法处理一些边缘案例。例如，如果有包含相似对象的图像帧，则Bbox在它们之间振荡。当对象消失时，它会失去目标，但继续搜索其他对象。另一件事是，在实际用例中，意外的相机移动会导致对象突然移动。
这些边缘情况有什么解决方案？
您知道，Vittracker使用模板和基于自我发项机制的搜索。核心原理是将存储目标特征的模板令牌与搜索令牌匹配，该图令牌代表了使用自我注意的机制来代表跟踪器的搜索区域。为了解决突然的相机移动引起的边缘案例，我试图增加搜索区域的大小。]]></description>
      <guid>https://stackoverflow.com/questions/79556795/how-can-i-improve-the-performance-of-vittracker</guid>
      <pubDate>Sat, 05 Apr 2025 09:51:00 GMT</pubDate>
    </item>
    <item>
      <title>attributeError：'aattn'对象在yolov12上运行推理时没有属性'qkv'</title>
      <link>https://stackoverflow.com/questions/79556669/attributeerror-aattn-object-has-no-attribute-qkv-when-running-inference-on</link>
      <description><![CDATA[我正在尝试使用我的yolov12型号进行推理，但是我遇到了以下错误：
  attributeError：&#39;aattn&#39;对象没有属性&#39;qkv&#39;
 
这是我的代码：
 从超级物质导入YOLO

型号= yolo（r＆quot; d：\ a \ b \ model \ newdatasetversion \ 12 \ m \ m \ best.pt＆quort;

img = r＆quot d：\ a \ br \ program \ prograpping \ frames_kopo \ kopo0_20250228_111522_frame_1.jpg＆quot;
结果=模型（img，save = false）
 
我不完全确定是什么原因引起的。这是一些上下文：

我正在使用yolov12（定制训练）。
错误在推理期间发生，而不是训练。

我尝试加载Yolov12模型并在图像上运行推断。我希望该模型像往常一样返回预测（边界框，类等）。但是，我没有得到输出，而是得到了这个错误。
我没有手动修改模型体系结构，只是使用Yolo12m.pt训练了我的自定义数据集，所以我不确定为什么会发生这种情况。我期望推断能像以前一样正常运行。
可能导致此问题或我该如何修复？]]></description>
      <guid>https://stackoverflow.com/questions/79556669/attributeerror-aattn-object-has-no-attribute-qkv-when-running-inference-on</guid>
      <pubDate>Sat, 05 Apr 2025 07:17:45 GMT</pubDate>
    </item>
    <item>
      <title>为什么拥抱面提供的DeepSeek代码会导致“未知量化类型”错误？</title>
      <link>https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t</link>
      <description><![CDATA[我正在使用huggingface的此代码：
此代码直接从 deepseek上的huggingface网站页面上的页面

 来自变形金刚导入管道

消息= [
{&#39;&#39;：＆quot“ user quot”内容“：;
这是给出的
pipe =管道（＆quot&#39;text-generation＆quot; deepseek-ai/deepseek-r1＆quort; trust_remote_code = true）
管道（消息）
 

，但我无法加载模型。当我这样做时，我会得到这个问题：
  file＆quot＆lt; ...＆gt;/site-packages/transformers/quantizers/auto.py＆quot;，第97行，在from_dict

提高价值Error（

ValueError：未知量化类型，获得FP8-支持类型为： 
[&#39;awq&#39;，&#39;bitsandbytes_4bit&#39;，&#39;bitsandbytes_8bit&#39;，&#39;gptq&#39;，&#39;aqlm&#39;，&#39;quanto&#39;，&#39;eetq&#39;，&#39;eetq&#39;， 
&#39;HQQ&#39;，“压缩张量”，“ fbgemm_fp8&#39;，&#39;torchao&#39;，&#39;bitnet&#39;]
 
我尝试了不同的代码：
 导入火炬
generate_text = pipeline（model =; deepSeek-ai/deepSeek-r1; torch_dtype = torch.bfloat16，trust_remote_code = true，device_map =; auto;
generate_text（消息）
 
这给出以下错误：

raise ValueError( ValueError: Unknown quantization type, got fp8 - supported types are: [&#39;awq&#39;, &#39;bitsandbytes_4bit&#39;, &#39;bitsandbytes_8bit&#39;, &#39;gptq&#39;, &#39;aqlm&#39;, &#39;quanto&#39;, &#39;eetq&#39;, &#39;higgs&#39;, &#39;hqq&#39;, &#39;compressed-tensors&#39;, &#39;fbgemm_fp8&#39;, &#39;torchao&#39;，&#39;bitnet&#39;，&#39;vptq&#39;] 

我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t</guid>
      <pubDate>Sun, 09 Feb 2025 03:05:30 GMT</pubDate>
    </item>
    <item>
      <title>在Kaggle中使用thundersvm</title>
      <link>https://stackoverflow.com/questions/79243091/using-thundersvm-in-kaggle</link>
      <description><![CDATA[当我想使用！PIP安装Thundersvm 安装thundersvm时，我遇到了此错误：
在
Oserror Trackback（最近的最新电话）
[6]中的单元，第3行
      1 get_ipython（）。run_line_magic（&#39;pip&#39;，&#39;安装thundersvm&#39;）
      2 get_ipython（）。run_line_magic（&#39;pip&#39;，&#39;安装keras_tuner&#39;）
----＆gt; 3来自Thundersvm进口SVC
      4来自Sklearn.Perprecorsing Import StandardardScaler
      5来自Sklearn.metrics Import Classification_Report

file/opt/conda/lib/python3.10/site-packages/thundersvm/__init__.py:10
      3“”
      4 *名称：__init__.py
      5 *作者：locke＆lt; luojiahuan001@gmail.com&gt;
      6 *版本：0.0.1
      7 *描述：
      8“”
      9名=“ Thundersvm”
---＆gt; 10来自.thundersvm进口 *

file/opt/conda/lib/python3.10/site-packages/thundersvm/thundersvm.py:39
     36 lib_path =路径。
     38如果path.exists（lib_path）：
---＆gt; 39 thundersvm = cdll（lib_path）
     40其他：
     41＃尝试构建目录
     42如果平台==“ Linux”或platform ==＆quot＆quot2＆quot;：

file/opt/conda/lib/python3.10/ctypes/__init__.py:374，在cdll .__ Init __（self，name，name，mode，hander，hander，use_errno，use_last_error，winmode）
    371 self._funcptr = _funcptr
    373如果没有手柄：
 - ＆gt; 374 self._handle = _dlopen（self._name，模式）
    375其他：
    376 self._handle =句柄

Oserror：libcusparse.so.9.0：无法打开共享对象文件：没有此类文件或目录
 
为了解决此问题，我尝试了此操作：
 ＃下载并安装cuda 9.0
wget https://developer.nvidia.com/compute/cuda/9.0/prod/local_installers/cuda_9.0.176_384.81_linux-run
sudo sh cuda_9.0.176_384.81_linux-run
 
也无效。我该如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/79243091/using-thundersvm-in-kaggle</guid>
      <pubDate>Mon, 02 Dec 2024 06:08:54 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用RuleFit进行二进制分类；如何解释规则？</title>
      <link>https://stackoverflow.com/questions/75089762/i-am-using-rulefit-for-binary-classification-how-do-i-interpret-the-rules</link>
      <description><![CDATA[我正在将规则fit与渐变boostingClassifier一起使用，以生成二进制分类问题的规则（Kaggle上的Health-Dataset）。  当我用rulefit.get_rules（）打印规则时，它显示了规则，类型，COEF，支持和重要性。  但这并未显示哪个类（0或1）是规则的目标。  例如：exang＆lt; = 0.5描述了0或1类？
摘要：我怎么知道给定规则正在描述哪个目标？]]></description>
      <guid>https://stackoverflow.com/questions/75089762/i-am-using-rulefit-for-binary-classification-how-do-i-interpret-the-rules</guid>
      <pubDate>Wed, 11 Jan 2023 22:42:33 GMT</pubDate>
    </item>
    <item>
      <title>从python中的Sklearlen线性回归获得置信区间</title>
      <link>https://stackoverflow.com/questions/61292464/get-confidence-interval-from-sklearn-linear-regression-in-python</link>
      <description><![CDATA[我想获得线性回归结果的置信区间。我正在与波士顿房屋价格数据集合作。
我发现了这个问题：
如何计算Python中线性回归模型的99％置信区间？
但是，这并不完全回答我的问题。
这是我的代码：
 导入numpy作为np
导入matplotlib.pyplot作为PLT
来自数学导入pi

导入大熊猫作为pd
进口海洋作为SNS
来自sklearn.datasets import load_boston
来自sklearn.model_selection导入train_test_split
来自sklearn.linear_model导入linearrecress
来自sklearn.metrics导入均值_squared_error，r2_score

＃导入数据
boston_dataset = load_boston（）

波士顿= pd.dataframe（boston_dataset.data，columns = boston_dataset.feature_names）
波士顿[&#39;medv​​&#39;] = boston_dataset.target

x = pd.dataframe（np.c_ [boston [&#39;lstat&#39;]，波士顿[&#39;rm&#39;]]，columns = [&#39;lstat&#39;，&#39;rm&#39;]）
Y =波士顿[&#39;MEDV&#39;]

＃将培训和测试数据集分为80％：20％
＃将Random_State分配给任何值。这确保一致性。
x_train，x_test，y_train，y_test = train_test_split（x，y，test_size = 0.2，andural_state = 5）

lin_model = linearregression（）
lin_model.fit（x_train，y_train）

＃培训集的模型评估

y_train_predict = lin_model.predict（x_train）
rmse =（np.sqrt（mean_squared_error（y_train，y_train_predict））））））
r2 = r2_score（y_train，y_train_predict）

＃测试集的模型评估

y_test_predict = lin_model.predict（x_test）
＃模型的根平方错误
rmse =（np.sqrt（mean_squared_error（y_test，y_test_predict））））））））

＃模型的R平方分数
r2 = r2_score（y_test，y_test_predict）

plt. -scatter（y_test，y_test_predict）
plt.show（）
 
例如，我如何从中获得95％或99％的置信区间？是否有某种内置的功能或代码？]]></description>
      <guid>https://stackoverflow.com/questions/61292464/get-confidence-interval-from-sklearn-linear-regression-in-python</guid>
      <pubDate>Sat, 18 Apr 2020 16:22:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么imagedatagenerator（）表现不佳？</title>
      <link>https://stackoverflow.com/questions/58562089/why-is-imagedatagenerator-performing-poorly</link>
      <description><![CDATA[我正在尝试使用Imagedatagenerator（）构建图像分类模型。 
看来该模型训练并表现不佳。训练损失停留在15左右，精度仅为10％，验证差异大致相同。
只是为了看看会发生什么，我尝试培训而不使用Imagedatagenerator（）并以类似的方式设置数据。它在培训，验证和测试方面的表现要好得多。训练损失为0.71，精度为75％，验证损失为0.8，精度为72％。 
我需要使用数据生成器来弄清楚此模型，因为我将继续使用较大的数据集，它将不适合内存。
所以，我想我的问题是我对成像的Atageatagener（）的表现如此出色，我该如何改善结果？
设置文件（在所有火车，测试，验证文件夹中）时，有具有自己的文件夹的类，在这些文件夹中是图像所在的位置。 
这是代码：
 将TensorFlow导入为TF
来自tensorflow.keras.preprocessing.image导入成像的Atagenerator
进口泡菜
来自tensorflow.keras.models导入顺序
来自tensorflow.keras.layers导入密集，激活，扁平，conv2d，maxpooling2d，辍学

data_gen = imagedatagenerator（）
img_size = 100
train_it = data_gen.flow_from_directory（&#39;d：/.../ train/&#39;，class_mode =&#39;sparse&#39;，
                                       target_size =（img_size，img_size），color_mode =&#39;grayscale&#39;，shuffle = true，batch_size = 32）
val_it = data_gen.flow_from_directory（&#39;d：/.../验证/&#39;，class_mode =&#39;sparse&#39;，
                                     target_size =（img_size，img_size），color_mode =&#39;grayscale&#39;，shuffle = true，batch_size = 32）

image_size = [100，100]

型号=顺序（）
model.Add（conv2d（32，（3,3），input_shape = [*image_size，1]））））
model.Add（激活（&#39;relu&#39;））
model.Add（maxpooling2d（pool_size =（2,2）））

ADD（辍学（0.5））

Add（Conv2d（32，（3,3）））
model.Add（激活（&#39;relu&#39;））
model.Add（maxpooling2d（pool_size =（2,2）））

ADD（辍学（0.5））

Add（Conv2d（32，（3,3）））
model.Add（激活（&#39;relu&#39;））
model.Add（maxpooling2d（pool_size =（2,2）））

ADD（辍学（0.5））

模型add（Flatten（））
model.Add（len（len（train_it.class_indices），激活=&#39;softmax&#39;））

model.compile（loss =&#39;Sparse_categorical_crossentropy&#39;，Optimizer =&#39;Adam&#39;，Metrics = [&#39;准确性&#39;]）
model.fit_generator（train_it，epochs = 20，验证_data = val_it）
 
这是我的代码，没有Imagedatagenerator（）：
使用OpenCV 设置数据

  datadir =&#39;d：\ ... \ train&#39;
类别= pickle.load（open（“ categories.p”，“ rb”））））
印刷（Len（类别））
img_size = 100
triench_data = []

def create_training_data（）：
    类别类别：
        路径= os.path.join（datadir，类别）
        class_num = categories.index（类别）
        对于os.listdir（路径）中的IMG：
            尝试：
                img_array = cv2.imread（os.path.join（路径，img），cv2.imread_grayscale）
                new_array = cv2.resize（img_array，（img_size，img_size））
                triench_data.append（[new_array，class_num]）
            除了：
                打印（类别）
                打印（IMG）

create_training_data（）

随机。

x = []
y = []
对于功能，请在triench_data中标记：
    X.Append（功能）
    Y.Append（标签）

x = np.array（x）.Reshape（-1，img_size，img_size，1）
X = X/255.0
 
模型设置：
  model = sequention（）
model.Add（conv2d（32，（3,3），input_shape = [*image_size，1]））））
model.Add（激活（&#39;relu&#39;））
model.Add（maxpooling2d（pool_size =（2,2）））

ADD（辍学（0.5））

Add（Conv2d（32，（3,3）））
model.Add（激活（&#39;relu&#39;））
model.Add（maxpooling2d（pool_size =（2,2）））

ADD（辍学（0.5））

Add（Conv2d（32，（3,3）））
model.Add（激活（&#39;relu&#39;））
model.Add（maxpooling2d（pool_size =（2,2）））

ADD（辍学（0.5））

模型add（Flatten（））
model.Add（密集（Len（类别），激活=&#39;SoftMax&#39;））

model.compile（loss =&#39;Sparse_categorical_crossentropy&#39;，Optimizer =&#39;Adam&#39;，Metrics = [&#39;准确性&#39;]）
model.fit（x，y，epochs = 20，batch_size = 32，验证_split = 0.1）
 ]]></description>
      <guid>https://stackoverflow.com/questions/58562089/why-is-imagedatagenerator-performing-poorly</guid>
      <pubDate>Fri, 25 Oct 2019 16:05:15 GMT</pubDate>
    </item>
    </channel>
</rss>