<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>æ ‡è®°ä¸ºæœºå™¨å­¦ä¹ çš„æ´»è·ƒé—®é¢˜ - å †æ ˆå†…å­˜æº¢å‡º</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>æ¥è‡ª stackoverflow.com çš„æœ€æ–° 30 æ¡</description>
    <lastBuildDate>Wed, 25 Dec 2024 06:24:03 GMT</lastBuildDate>
    <item>
      <title>åˆ†ç¦»å›¾åƒå†…çš„ç›²æ–‡å­—ç¬¦</title>
      <link>https://stackoverflow.com/questions/79306951/separation-of-braille-characters-inside-of-an-image</link>
      <description><![CDATA[æˆ‘æ­£åœ¨åšä¸€ä¸ªå°†ç›²æ–‡è½¬æ¢ä¸ºæ–‡æœ¬çš„é¡¹ç›®ã€‚æˆ‘å·²ç»ç¼–å†™äº†ä»å›¾åƒä¸­è¯†åˆ«ç›²æ–‡ç‚¹çš„ä»£ç ï¼Œä½†æˆ‘ä¸çŸ¥é“å¦‚ä½•å°†ç›²æ–‡åˆ†å‰²æˆå•å…ƒæ ¼ã€‚
è¿™éƒ¨åˆ†æ˜¯è¯†åˆ«å›¾åƒä¸­çš„æ–‘ç‚¹ï¼ˆè¾ƒå°çš„ä½è´¨é‡å›¾åƒç›®å‰ä¸èµ·ä½œç”¨ï¼‰
import cv2
import numpy as np
from sklearn.cluster import KMeans

# åŠ è½½å›¾åƒ
image_path = &quot;braille.jpg&quot;
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# è®¾ç½® SimpleBlobDetector
params = cv2.SimpleBlobDetector_Params()

# æŒ‰åŒºåŸŸè¿‡æ»¤ï¼ˆæ–‘ç‚¹å¤§å°ï¼‰
params.filterByArea = True
params.minArea = 100 # æ ¹æ®ç‚¹å¤§å°è¿›è¡Œè°ƒæ•´
params.maxArea = 1000

# æŒ‰åœ†åº¦è¿‡æ»¤
params.filterByCircularity = True
params.minCircularity = 0.9 # è°ƒæ•´ç‚¹çš„å½¢çŠ¶

# æŒ‰å‡¸åº¦è¿‡æ»¤
params.filterByConvexity = False
params.minConvexity = 0.7

# æŒ‰æƒ¯æ€§è¿‡æ»¤ï¼ˆåœ†åº¦ï¼‰
params.filterByInertia = True
params.minInertiaRatio = 0.95

# ä½¿ç”¨å‚æ•°åˆ›å»ºæ£€æµ‹å™¨
detector = cv2.SimpleBlobDetector_create(params)

# æ£€æµ‹æ–‘ç‚¹
keypoints = detector.detect(image)

# å°†æ£€æµ‹åˆ°çš„æ–‘ç‚¹ç»˜åˆ¶ä¸ºçº¢è‰²åœ†åœˆ
output_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
output_image = cv2.drawKeypoints(output_image, keypoints, np.array([]),
(0, 0, 255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

print(&quot;è¾“å‡ºå›¾åƒ&quot;)
cv2.imshow(&quot;è¾“å‡ºå›¾åƒ&quot;,output_image)
cv2.waitKey(0)
cv2.destroyAllWindows()

print(f&quot;æ£€æµ‹åˆ°çš„æ–‘ç‚¹æ•°é‡ï¼š{len(keypoints)}&quot;)

ä»¥ä¸‹ä»£ç å°† blob çš„åæ ‡æ”¾åœ¨å›¾å½¢ä¸Šï¼ˆè®¤ä¸ºè¿™ç§æ–¹å¼å¯èƒ½æ›´å®¹æ˜“æ“ä½œï¼‰
#å°†å›¾åƒè½¬æ¢ä¸ºå›¾å½¢

import matplotlib.pyplot as plt
import numpy

blob_coords = np.array([kp.pt for kp in keypoints]) #blob çš„åæ ‡
rounded_coords = np.round(blob_coords).astype(int) #å››èˆäº”å…¥çš„åæ ‡

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

# åŸºäºé‚»è¿‘åº¦çš„åˆ†ç»„
# å¦‚æœ X è·ç¦»å°äºæœ€å°è·ç¦»
# å¦‚æœ Y è·ç¦»å°äºæœ€å°è·ç¦»
# å­˜å‚¨ X å’Œ Y åæ ‡

# è®¡ç®—æœ€å° x å’Œ yå·®å¼‚ï¼ˆå°è¯•åŸºäºæ¥è¿‘åº¦ï¼‰
minx = 10000
miny = 10000
for i in x_coords:
for j in x_coords:
if abs(i - j) &lt;= minx and (15 &lt; abs(i - j)): # å•å…ƒæ ¼å®½åº¦é˜ˆå€¼
minx = abs(i - j)

for i in y_coords:
for j in y_coords:
if abs(i - j) &lt;= miny and (15 &lt; abs(i - j)): # å•å…ƒæ ¼é«˜åº¦é˜ˆå€¼
miny = abs(i - j)

print(f&quot;Smallest x difference: {minx}, Smallest y difference: {miny}&quot;,)

# ç»˜å›¾
fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;) # ç»˜åˆ¶æ–‘ç‚¹
ax.invert_yaxis()
plt.title(&quot;Braille Cell Detection&quot;)
plt.show()

å°è¯•é€šè¿‡æ¥è¿‘åº¦å°†å®ƒä»¬åˆ†å¼€ï¼ˆä½äºæˆ‘å°è¯•å°†è·ç¦»å¾ˆè¿‘çš„ç‰©ä½“åˆ†ç»„åˆ°ä¸€èµ·ï¼ˆæˆ‘å°†è·ç¦»å¾ˆè¿‘çš„ç‰©ä½“åˆ†ç»„åˆ°ä¸€èµ·ï¼‰ï¼Œä½†æˆ‘æ— æ³•ç†è§£å…¶ä¸­çš„é€»è¾‘ã€‚æˆ‘ä¹Ÿå°è¯•äº†ç»„èšç±» (Kmeans)ï¼Œä½†å®ƒä¸æ˜¯å¾ˆå‡†ç¡®ï¼Œå¹¶ä¸”ä¸é€‚ç”¨äºå…·æœ‰ä¸åŒå­—ç¬¦æ•°çš„å›¾åƒï¼Œå› ä¸ºå®ƒéœ€è¦ä¸æ–­çŸ¥é“è¦å½¢æˆå¤šå°‘ä¸ªç°‡ã€‚
# å°è¯• kmeans èšç±»æ–¹æ³•
# kmeans ä¸èµ·ä½œç”¨ï¼ˆæ— æ³•ä»å›¾åƒä¸­æ‰¾å‡ºç°‡çš„æ•°é‡ï¼‰
# å¦‚æœå¯ä»¥æ‰¾å‡º nclustersï¼Œåˆ™å¯ä»¥å·¥ä½œ

å¯¼å…¥æ•°å­¦
ä» sklearn.cluster å¯¼å…¥ KMeans

blob_coords = np.array([kp.pt for kp in keypoints]) # æå– blob çš„ (x, y) ä½ç½®
rounded_coords = np.round(blob_coords).astype(int) # ä¸ºç®€å•èµ·è§ï¼Œå¯¹åæ ‡è¿›è¡Œå››èˆäº”å…¥

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;) # ç»˜åˆ¶æ–‘ç‚¹

ax.invert_yaxis() # åè½¬ Y è½´ä»¥è·å¾—ç±»ä¼¼å›¾åƒçš„åæ ‡
plt.title(&quot;ç›²æ–‡å•å…ƒæ£€æµ‹&quot;)
plt.show()

inertias = []

# 2
kmeans = KMeans(n_clusters=26)
kmeans.fit(rounded_coords)

plt.scatter(x_coords,y_coords, c=kmeans.labels_)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79306951/separation-of-braille-characters-inside-of-an-image</guid>
      <pubDate>Wed, 25 Dec 2024 05:54:00 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ YOLO å°†æ— ç•Œè¾“å…¥å¯¼å‡ºåˆ° mlpackage/mlmodel æ–‡ä»¶</title>
      <link>https://stackoverflow.com/questions/79305588/use-yolo-with-unbounded-input-exported-to-an-mlpackage-mlmodel-file</link>
      <description><![CDATA[æˆ‘æƒ³åˆ›å»ºä¸€ä¸ª .mlpackage æˆ– .mlmodel æ–‡ä»¶ï¼Œå¯ä»¥å°†å…¶å¯¼å…¥ Xcode è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚ä¸ºæ­¤ï¼Œæˆ‘æƒ³ä½¿ç”¨ YOLO ä¸­çš„åˆ†å‰²åŒ…æ¥æ£€æŸ¥å®ƒæ˜¯å¦ç¬¦åˆæˆ‘çš„éœ€æ±‚ã€‚
ç°åœ¨çš„é—®é¢˜æ˜¯ï¼Œæ­¤è„šæœ¬åˆ›å»ºçš„ .mlpackage æ–‡ä»¶ä»…æ¥å—å›ºå®šå¤§å°ï¼ˆ640x640ï¼‰çš„å›¾åƒï¼š
from ultralytics import YOLO

model = YOLO(&quot;yolo11n-seg.pt&quot;)

model.export(format=&quot;coreml&quot;)

æˆ‘æƒ³åœ¨è¿™é‡Œè¿›è¡Œä¸€äº›æ›´æ”¹ï¼Œå¯èƒ½ä½¿ç”¨ coremltoolsï¼Œä»¥å¤„ç†æ— ç•ŒèŒƒå›´ï¼ˆæˆ‘æƒ³å¤„ç†ä»»æ„å¤§å°çš„å›¾åƒï¼‰ã€‚è¿™é‡Œæœ‰ä¸€äº›æè¿°ï¼šhttps://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html#enable-unbounded-rangesï¼Œä½†æˆ‘ä¸æ˜ç™½å¦‚ä½•ç”¨æˆ‘çš„è„šæœ¬å®ç°å®ƒã€‚]]></description>
      <guid>https://stackoverflow.com/questions/79305588/use-yolo-with-unbounded-input-exported-to-an-mlpackage-mlmodel-file</guid>
      <pubDate>Tue, 24 Dec 2024 12:23:06 GMT</pubDate>
    </item>
    <item>
      <title>æ¯”è¾ƒå›¾åƒå¹¶è¿”å›ç›¸ä¼¼åº¦ç™¾åˆ†æ¯”ï¼ˆé’ˆå¯¹å¾½æ ‡ï¼‰[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79305487/compare-images-and-return-similarity-percentage-for-logos</link>
      <description><![CDATA[å‡è®¾æˆ‘æœ‰ 2 å¼ å›¾ç‰‡


è¿™æœ‰ç›¸åŒçš„å¾½æ ‡ï¼Œå› æ­¤ç»“æœåº”è¯¥è¶…è¿‡ 90%
è¿™é‡Œæ˜¯å¦å¤– 2 ä¸ªå¾½æ ‡åœ¨æ­¤å¤„è¾“å…¥å›¾ç‰‡è¯´æ˜
ç°åœ¨æˆ‘ä»¬åˆæœ‰ 2 å¼ ç…§ç‰‡äº†


è¿™ä¹Ÿæ˜¯ç›¸åŒçš„å›¾ç‰‡ï¼Œæ‰€ä»¥ç»“æœä¸€å®šæ˜¯æ­£é¢çš„ã€‚
æˆ‘é‡åˆ°çš„é—®é¢˜æ˜¯ï¼Œåœ¨äº¤æ¢å›¾ç‰‡å¹¶æ¯”è¾ƒâ€œå¥¥è¿ªâ€å’Œâ€œå¥¥è¿ä¼šâ€çš„æ ‡å¿—æ—¶ï¼Œå°½ç®¡å›¾åƒå®Œå…¨ä¸åŒï¼Œç›¸ä¼¼åº¦å¾—åˆ†å´è¶…è¿‡ 75%ã€‚æˆ‘å°è¯•è¿‡è¾¹ç¼˜æ£€æµ‹ç­‰æ–¹æ³•æ¥è§£å†³è¿™ç§å·®å¼‚ï¼Œä½†è¿™äº›æ–¹æ³•éƒ½è¢«è¯æ˜æ˜¯æ— æ•ˆçš„ã€‚æ‚¨èƒ½å»ºè®®ä¸€ç§åˆé€‚çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜å—ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/79305487/compare-images-and-return-similarity-percentage-for-logos</guid>
      <pubDate>Tue, 24 Dec 2024 11:41:18 GMT</pubDate>
    </item>
    <item>
      <title>åŒä¸€æ ·æœ¬çš„é¢„æµ‹åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­æœ‰æ‰€ä¸åŒ</title>
      <link>https://stackoverflow.com/questions/79303693/prediction-on-the-same-sample-differs-from-training-to-testing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79303693/prediction-on-the-same-sample-differs-from-training-to-testing</guid>
      <pubDate>Mon, 23 Dec 2024 16:37:41 GMT</pubDate>
    </item>
    <item>
      <title>SHAP å€¼åœ¨äºŒå…ƒåˆ†ç±»ä¸­è¢«åè½¬æˆ–è§£é‡Šä¸ä½³ [å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79302806/shap-values-inverted-or-not-well-interpreted-in-binary-classification</link>
      <description><![CDATA[æˆ‘æ­£åœ¨åšä¸€ä¸ªé¡¹ç›®ç ”ç©¶ï¼Œé¢„æµ‹è‡ªè¡Œè½¦éª‘è¡Œè€…å’Œæ¯”èµ›æ•°æ®é›†çš„ top_20 æ ‡ç­¾ï¼Œå½“è‡ªè¡Œè½¦éª‘è¡Œè€…è¿›å…¥å‰ 20 åæ—¶ä¸º 1ï¼Œå¦åˆ™ä¸º 0ã€‚æ•°æ®é›†ä¸¥é‡ä¸å¹³è¡¡ï¼ˆ92000 ä¸ª 1 ç±»å®ä¾‹å’Œå¤§çº¦ 400000 ä¸ª 0 ç±»å®ä¾‹ï¼‰ã€‚æˆ‘è¿è¡Œä¸€ä¸ªå…·æœ‰ç®€å•æ¶æ„çš„åŸºæœ¬ç¥ç»ç½‘ç»œã€‚æˆ‘ä¹Ÿåœ¨ä½¿ç”¨ç±»æƒé‡ã€‚æˆ‘çš„ NN è¡¨ç°ä¸ä½³ï¼Œä½†è¿™ä¸æ˜¯é‡ç‚¹ï¼Œå› ä¸ºæˆ‘ä»é¡¹ç›®ç›®çš„ä¸ŠçŸ¥é“æ•°æ®ä¸æ˜¯å¾ˆå¥½ã€‚f1 åˆ†æ•°å¯¹äº 0 ç±»ï¼ˆå¤šæ•°ï¼‰æ¥è¯´å¾ˆå¥½ï¼Œçº¦ä¸º 0.87ã€‚å¯¹äº 1 ç±»ï¼ˆå°‘æ•°ï¼‰ï¼Œçº¦ä¸º 0.55ã€‚è¿™å¾ˆå¥½ï¼Œä½†æ˜¯å½“å°è¯•ä½¿ç”¨ SHAP è§£é‡Šç»“æœæ—¶ä¼šå‡ºç°é—®é¢˜ã€‚
ä½¿ç”¨ SHAPï¼Œæˆ‘ä¸ºæ‰€æœ‰å®ä¾‹è®¾ç½®äº†åŸºå€¼ 1ï¼ˆæœ‰æ—¶ä¸º 0.98ï¼‰ã€‚æ‘˜è¦å›¾ä¼¼ä¹åˆç†ï¼Œä½†åŠ›å’Œç€‘å¸ƒå›¾ä¸åˆç†ã€‚å¦‚æœæ¨¡å‹è¿‡äºè‡ªä¿¡åœ°é¢„æµ‹ç±» 0ï¼ŒåŸºå€¼å¦‚ä½•å¯¼è‡´ 1ï¼Ÿ æˆ‘è¿˜è®¤ä¸ºæ ‡ç­¾åœ¨ shap å€¼ä¸­æ˜¯åè½¬çš„ï¼Œå®é™…ä¸Šé¢„æœŸå€¼ 1 åæ˜ äº†ç±» 0ã€‚ä½†æˆ‘å¯¹æˆ‘çš„ shap å€¼å¯¹è±¡çš„ç»“æ„æ„Ÿåˆ°å›°æƒ‘ï¼Œå› ä¸ºæˆ‘æ— æ³•é€‰æ‹©ä¸€ä¸ªç±»æ¥åˆ†æï¼Œä½†å®ƒè¢«å®ä¾‹åˆ’åˆ†ã€‚ä¾‹å¦‚ shap_value[1] æŒ‡çš„æ˜¯å®ä¾‹ç¼–å· 1ï¼Œè€Œä¸æ˜¯ç±» 1ã€‚ç­‰ç­‰ã€‚ä»£ç å¦‚ä¸‹ï¼š
def build_model(optimizer=&#39;adam&#39;, dropout_rate=0.5, num_units_1=128, num_units_2=64):
model = Sequential([
Dense(num_units_1,activation=&#39;relu&#39;,input_shape=(X_train.shape[1],)), # è¾“å…¥å±‚
BatchNormalization(),
Dropout(dropout_rate), # Dropout å±‚
Dense(num_units_2,activation=&#39;relu&#39;,kernel_regularizer=l2(0.01)), # éšè—å±‚
#Dropout(dropout_rate), # Dropout å±‚
Dense(1,activation=&#39;sigmoid&#39;) # ç”¨äºäºŒåˆ†ç±»çš„è¾“å‡ºå±‚
])

# ç”¨äºæ”¶æ•›
lr_schedule = ExponentialDecay(
initial_learning_rate=0.001,
decay_steps=10000,
decay_rate=0.9
)

# ç¼–è¯‘æ¨¡å‹
optimizer_instance = {
&#39;adam&#39;: Adam(learning_rate=lr_schedule),
&#39;rmsprop&#39;: RMSprop(learning_rate=lr_schedule),
&#39;sgd&#39;: SGD(learning_rate=lr_schedule)
}[optimizer]

model.compile(optimizer=optimizer_instance,
loss=&#39;binary_crossentropy&#39;,
metrics=[&#39;accuracy&#39;])
return model

å¯¹äº SHAPï¼š
background = X_train_df.sample(200) 
test_sample = X_test_df.sample(100) 

# è§£é‡Šå™¨
explainer = shap.Explainer(best_model.predict,èƒŒæ™¯)
shap_values = explainer(test_sample)

# å¯è§†åŒ–æ¯ä¸ªé¢„æµ‹çš„ SHAP å€¼
shap.summary_plot(shap_values, test_sample)

è¯¥å›¾çªå‡ºæ˜¾ç¤ºäº†åœ¨æˆ‘çœ‹æ¥åˆç†çš„å†…å®¹ï¼šè¾ƒä½çš„ delta å€¼å¯ä»¥åæ˜ å‡ºéª‘è½¦äººæ¥è¿‘ç¬¬ä¸€ä¸ªä½ç½®çš„äº‹å®ï¼ˆdelta æ˜¯ç›¸å¯¹äºç¬¬ä¸€ä¸ªä½ç½®çš„æ—¶é—´å·®ï¼‰ï¼Œå› æ­¤å®ƒæ¨åˆ° 1
ä½†ä¾‹å¦‚ç€‘å¸ƒ/åŠ›å›¾å¯¹æˆ‘æ¥è¯´ä¼¼ä¹å®Œå…¨ä¸ä¸€è‡´ï¼š
æœ€åï¼Œæˆ‘çš„ shap å€¼çš„ç»“æ„å¦‚ä¸‹ï¼š
shap_values[2]
.values =
array([-5.55111512e-17, 1.00000000e-02, 1.73472348e-17, 3.12250226e-17,
2.42861287e-17, -3.46944695e-18, 1.04083409e-17, 0.00000000e+00])

.base_values =
0.99

.data =
array([1.20000000e+02, 1.57000000e+05, 1.55100000e+03, 3.28000000e+02,
1.71790754e-02, 3.83373426e-06, 2.40317094e+01, 1.54471545e-01])
]]></description>
      <guid>https://stackoverflow.com/questions/79302806/shap-values-inverted-or-not-well-interpreted-in-binary-classification</guid>
      <pubDate>Mon, 23 Dec 2024 10:29:12 GMT</pubDate>
    </item>
    <item>
      <title>è®­ç»ƒ transformer æ¨¡å‹æ—¶å‡ºç° OOM é”™è¯¯</title>
      <link>https://stackoverflow.com/questions/79302713/oom-error-when-training-transformer-model</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ç ”ç©¶ HMERï¼ˆæ‰‹å†™æ•°å­¦è¡¨è¾¾å¼è¯†åˆ«ï¼‰é—®é¢˜ï¼Œå¹¶å°è¯•ä½¿ç”¨ CNN-Transformer æ¶æ„ã€‚ä½†æ˜¯ï¼Œå½“æˆ‘å°è¯•è®­ç»ƒæˆ‘çš„æ¨¡å‹æ—¶ï¼Œæˆ‘é‡åˆ°äº†æ­¤é”™è¯¯ï¼š
DefaultCPUAllocatorï¼šå†…å­˜ä¸è¶³ï¼šæ‚¨å°è¯•åˆ†é… 69271363584 å­—èŠ‚ã€‚
æˆ‘è®¤ä¸ºæˆ‘çš„ä½ç½®ç¼–ç å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œä½†æˆ‘çœŸçš„ä¸çŸ¥é“å¦‚ä½•è°ƒè¯•å®ƒæˆ–è¿™é‡ŒçœŸæ­£çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼ˆæˆ‘åœ¨è¿™æ–¹é¢è¿˜åªæ˜¯åˆå­¦è€…ï¼‰
æˆ‘å½“å‰çš„æ¨¡å‹å¦‚ä¸‹æ‰€ç¤º
class PositionalEncoding(nn.Module):
def __init__(self, d_model, max_len=5000):
super(PositionalEncoding, self).__init__()
self.encoding = torch.zeros(max_len, d_model)
position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
self.encoding[:, 0::2] = torch.sin(position * div_term)
self.encoding[:, 1::2] = torch.cos(position * div_term)
self.encoding = self.encoding.unsqueeze(0) # æ·»åŠ æ‰¹æ¬¡ç»´åº¦

def forward(self, x):
seq_len = x.size(1)
return x + self.encoding[:, :seq_len, :].to(x.device)

class TransformerDecoder(nn.Module):
def __init__(self, vocab_size, d_model, num_heads, num_layers, max_seq_length):
super(TransformerDecoder, self).__init__()
self.embedding = nn.Embedding(vocab_size, d_model)
self.positional_encoding = PositionalEncoding(d_model, max_seq_length)
decrypt_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads)
self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
self.fc_out = nn.Linear(d_model, vocab_size)

def forward(self, target_seqs, memory, target_mask):
# åµŒå…¥ç›®æ ‡åºåˆ—
embedded = self.embedding(target_seqs) # (batch, seq_len, d_model)
embedded = checkpoint(self.positional_encoding, embedded) # æ·»åŠ ä½ç½®ç¼–ç 
# è½¬ç½®ä»¥å…¼å®¹ nn.TransformerDecoder (seq_len, batch, d_model)
embedded = embedded.permute(1, 0, 2)
memory = memory.permute(1, 0, 2)
# è§£ç 
coded = checkpoint(self.transformer_decoder, embedded, memory, target_mask) # (seq_len, batch, d_model)
# è½¬ç½®å› (batch, seq_len, d_model)
coded = decrypted.permute(1, 0, 2)
# æœ€ç»ˆè¾“å‡ºå±‚
logits = checkpoint(self.fc_out,coded) # (batch, seq_len, vocab_size)
return logits


æˆ‘çš„ä»£ç æœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿå¦‚æœæ²¡æœ‰ï¼Œæ˜¯ä»€ä¹ˆåŸå› å¯¼è‡´æˆ‘é‡åˆ°è¿™ä¸ªé—®é¢˜çš„]]></description>
      <guid>https://stackoverflow.com/questions/79302713/oom-error-when-training-transformer-model</guid>
      <pubDate>Mon, 23 Dec 2024 09:44:33 GMT</pubDate>
    </item>
    <item>
      <title>å¹¶é›†å’Œäº¤é›†çš„ VC ç»´æ•°çš„ä¸Šé™[å…³é—­]</title>
      <link>https://stackoverflow.com/questions/79302585/upper-bound-on-vc-dimension-of-union-and-intersection</link>
      <description><![CDATA[é—®é¢˜ï¼šç±» C çš„ VC ç»´åº¦ä¸º dã€‚ç±» Câ€™ åŒ…æ‹¬ç”± C ä¸­ s ä¸ªå¯¹è±¡çš„äº¤é›†å’Œå¹¶é›†ï¼ˆä»¥ä»»ä½•é¡ºåºï¼‰å½¢æˆçš„æ‰€æœ‰å¯¹è±¡ã€‚ç»™å‡º Câ€™ çš„ VC ç»´åº¦çš„ä¸Šé™ã€‚
å°è¯•
æˆ‘çŸ¥é“ï¼Œå¦‚æœ H æ˜¯å¤§å°ä¸º s ä¸” VC ç»´åº¦ä¸º d çš„å‡è®¾ç±»ã€‚ä»¥ä¸‹ä¸ºçœŸï¼š
å¦‚æœ H&#39; æ˜¯ç”±æ¥è‡ª H çš„ s ä¸ªå‡è®¾çš„æ‰€æœ‰å¹¶é›†å½¢æˆçš„ç±»ï¼Œå¹¶ä¸” s â‰¥ 1ã€‚åˆ™ VC ç»´åº¦ (ğ»&#39;) â‰¤ $2ğ‘‘ğ‘ log{_2}â¡(3ğ‘ )$ã€‚
å¦‚æœ H&#39; æ˜¯ç”±æ‰€æœ‰äº¤é›†å½¢æˆçš„ç±»ï¼Œåˆ™åŒæ ·ä¸ºçœŸã€‚
ä»è¿™é‡Œå¼€å§‹æ˜¯æˆ‘çš„å°è¯•ï¼š
ä¸ºäº†å½¢æˆ Câ€™ï¼Œæˆ‘ä»¬å…è®¸äº¤é›†å’Œå¹¶é›†çš„åµŒå¥—ç»„åˆã€‚è¿™æ„å‘³ç€ $â‹ƒ{{i=1}}â‹‚{{j=1}}c{_{ij}}$ï¼Œå…¶ä¸­ câˆˆCã€‚
å¦‚æœ ğ¶${_â‹ƒ}$ å’Œ ğ¶${_â‹‚}$ æ˜¯ä¸¤ä¸ªå‡è®¾ç±»ï¼Œåˆ™å®ƒä»¬åœ¨å¹¶é›†æˆ–äº¤é›†ä¸­çš„ç»„åˆç±»çš„ VC ç»´æ•°æœ€å¤šæ˜¯å®ƒä»¬çš„ VC ç»´æ•°ä¹‹å’Œã€‚
æ¯ä¸ªé¢å¤–çš„å¹¶é›†æˆ–äº¤é›†å±‚æœ€å¤šå¯¹å­é›†çš„ç»„åˆå¢é•¿è´¡çŒ® $log{_2}â¡(3ğ‘ )$ã€‚å› æ­¤ï¼ŒCâ€™ çš„ VC ç»´æ•°å—ä»¥ä¸‹é™åˆ¶ï¼šVC(Câ€™) â‰¤ $4ğ‘‘ğ‘ log{_2}â¡(3ğ‘ )$ã€‚
æˆ‘å¾ˆæƒ³å¬å¬ä½ å¯¹è¿™æ˜¯å¦æ­£ç¡®çš„çœ‹æ³•ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/79302585/upper-bound-on-vc-dimension-of-union-and-intersection</guid>
      <pubDate>Mon, 23 Dec 2024 08:40:39 GMT</pubDate>
    </item>
    <item>
      <title>äºŒè¿›åˆ¶äº¤å‰ç†µçš„å®ç°ç»™å‡ºä¸æ­£å¸¸çš„ç»“æœ</title>
      <link>https://stackoverflow.com/questions/79302179/implementation-of-binary-cross-entropy-gives-not-normal-result</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°è¯•æ„å»ºä¸€ä¸ª NNï¼Œä½†åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘å¾—åˆ°çš„æŸå¤±å‡½æ•°å€¼å°±å¼‚å¸¸äº†ã€‚è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ
å“¦ï¼Œæˆ‘åªèƒ½ä½¿ç”¨ NumPyã€‚
è¿™å°±æ˜¯æ•°æ®çœ‹èµ·æ¥ç›¸ä¼¼çš„æ–¹å¼ï¼š
print(X_train.shape) #(784,800)
print(X_test.shape) #(784,200)
print(Y_train.shape) #(800,1)
print(Y_test.shape) #(200,1)

æŸå¤±å‡½æ•° - BCE
def log_loss(y_hat, y):
m = y.shape[0]
epsilon = 1e-15 
y_hat = np.clip(y_hat, epsilon, 1 - epsilon) 

# loss = -1/m * (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))
æŸå¤± = -1/m * (np.dot(y.T,np.log(y_hat)) + np.dot((1-y).T, np.log(1-y_hat)))
å›æŠ¥æŸå¤±

è®­ç»ƒé˜¶æ®µ - å‰å‘ä¼ æ’­ + åå‘ä¼ æ’­
input_layer = X_train.shape[0]
hidden_â€‹â€‹layer = 128
learning_rate = 0.01
epochs = 10

W1 = np.random.randn(hidden_â€‹â€‹layer, input_layer)
b1 = np.zeros((hidden_â€‹â€‹layer, 1))
W2 = np.random.randn(1, hidden_â€‹â€‹layer)
b2 = np.zeros((1, 1))

X = X_train
Y = Y_train
loss_list = []
epoch_list = []
num_of_examples = X.shape[1]

for i in range(epochs):
avg_epoch_loss = 0
for j in range(num_of_examples):

Z1 = np.matmul(W1,X[:,j].reshape(-1,1)) + b1 # ä¸è¦å¿˜è®°æ·»åŠ åå·®
A1 = sigmoid(Z1)
Z2 = np.matmul(W2,A1) + b2
A2 = sigmoid(Z2)
Yout = Y[j]

loss = log_loss( A2, Yout)
avg_epoch_loss = avg_epoch_loss + loss

dZ2 = (A2-Yout)
dW2 = np.matmul(dZ2,A1.T)
db2 = dZ2

dA1 = np.matmul(W2.T,dZ2)
dZ1 = dA1 * A1 * (1 - A1)
dW1 = np.matmul(dZ1,X[:,j].reshape(-1,1).T)
db1 = dZ1

W2 = W2 - å­¦ä¹ ç‡ * dW2
b2 = b2 - å­¦ä¹ ç‡ * db2
W1 = W1 - å­¦ä¹ ç‡ * dW1
b1 = b1 - å­¦ä¹ ç‡ * db1

avg_epoch_loss = avg_epoch_loss/num_of_examples
loss_list.append(avg_epoch_loss)
epoch_list.append(i)
print(&quot;Epoch&quot;, i,&quot;æŸå¤±ï¼š&quot;ï¼Œavg_epoch_loss)

è¿™äº›æ˜¯ä¸æ­£å¸¸çš„å€¼ - å½“ç„¶æ˜¯å¯»æ‰¾ 0-1 ä¹‹é—´çš„å€¼ï¼š
Epoch 0 æŸå¤±ï¼š[-18.37821485]
Epoch 1 æŸå¤±ï¼š[-18.82406892]
Epoch 2 æŸå¤±ï¼š[-18.82406892]
Epoch 3 æŸå¤±ï¼š[-19.99316345]
Epoch 4 æŸå¤±ï¼š[-29.91647793]
Epoch 5 æŸå¤±ï¼š[-32.32075724]
Epoch 6 æŸå¤±ï¼š[-32.89639034]
Epoch 7 æŸå¤±ï¼š[-32.34691639]
Epoch 8 æŸå¤±ï¼š [-32.749394]
ç¬¬ 9 é˜¶æ®µæŸå¤±ï¼š[-33.61631871]
]]></description>
      <guid>https://stackoverflow.com/questions/79302179/implementation-of-binary-cross-entropy-gives-not-normal-result</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:53 GMT</pubDate>
    </item>
    <item>
      <title>â€˜superâ€™ å¯¹è±¡æ²¡æœ‰å±æ€§â€˜__sklearn_tags__â€™</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[æˆ‘åœ¨ä½¿ç”¨ Scikit-learn ä¸­çš„ RandomizedSearchCV æ‹Ÿåˆ XGBRegressor æ—¶é‡åˆ°äº† AttributeErrorã€‚é”™è¯¯æ¶ˆæ¯æŒ‡å‡ºï¼š
&#39;super&#39; å¯¹è±¡æ²¡æœ‰å±æ€§ &#39;\_\_sklearn_tags__&#39;ã€‚

å½“æˆ‘åœ¨ RandomizedSearchCV å¯¹è±¡ä¸Šè°ƒç”¨ fit æ–¹æ³•æ—¶ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚æˆ‘æ€€ç–‘å®ƒå¯èƒ½ä¸ Scikit-learn å’Œ XGBoost æˆ– Python ç‰ˆæœ¬ä¹‹é—´çš„å…¼å®¹æ€§é—®é¢˜æœ‰å…³ã€‚æˆ‘ä½¿ç”¨çš„æ˜¯ Python 3.12ï¼Œå¹¶ä¸” Scikit-learn å’Œ XGBoost éƒ½å®‰è£…äº†æœ€æ–°ç‰ˆæœ¬ã€‚
æˆ‘å°è¯•ä½¿ç”¨ Scikit-learn ä¸­çš„ RandomizedSearchCV è°ƒæ•´ XGBRegressor çš„è¶…å‚æ•°ã€‚æˆ‘å¸Œæœ›æ¨¡å‹èƒ½å¤Ÿæ¯«æ— é—®é¢˜åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œå¹¶åœ¨äº¤å‰éªŒè¯åæä¾›æœ€ä½³å‚æ•°ã€‚
æˆ‘è¿˜æ£€æŸ¥äº†å…¼å®¹æ€§é—®é¢˜ï¼Œç¡®ä¿åº“æ˜¯æœ€æ–°çš„ï¼Œå¹¶é‡æ–°å®‰è£…äº† Scikit-learn å’Œ XGBoostï¼Œä½†é”™è¯¯ä»ç„¶å­˜â€‹â€‹åœ¨ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•åœ¨ Hugging Face Trainer æˆ– SFT Trainer ä¸­è®°å½•ç¬¬é›¶æ­¥çš„è®­ç»ƒæŸå¤±ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ä½¿ç”¨ Hugging Face Trainerï¼ˆæˆ– SFTTrainerï¼‰è¿›è¡Œå¾®è°ƒï¼Œæˆ‘æƒ³åœ¨æ­¥éª¤ 0ï¼ˆåœ¨æ‰§è¡Œä»»ä½•è®­ç»ƒæ­¥éª¤ä¹‹å‰ï¼‰è®°å½•è®­ç»ƒæŸå¤±ã€‚æˆ‘çŸ¥é“æœ‰ä¸€ä¸ªç”¨äºè¯„ä¼°çš„ eval_on_start é€‰é¡¹ï¼Œä½†æˆ‘æ‰¾ä¸åˆ°åœ¨è®­ç»ƒå¼€å§‹æ—¶è®°å½•è®­ç»ƒæŸå¤±çš„ç›´æ¥ç­‰æ•ˆæ–¹æ³•ã€‚
æ˜¯å¦æœ‰åŠæ³•ä½¿ç”¨ Trainer æˆ– SFTTrainer åœ¨æ­¥éª¤ 0ï¼ˆåœ¨ä»»ä½•æ›´æ–°ä¹‹å‰ï¼‰è®°å½•åˆå§‹è®­ç»ƒæŸå¤±ï¼Ÿç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘å¸Œæœ›ä½¿ç”¨ç±»ä¼¼äº eval_on_start çš„æ–¹æ³•ã€‚
ä»¥ä¸‹æ˜¯æˆ‘è¿„ä»Šä¸ºæ­¢å°è¯•è¿‡çš„æ–¹æ³•ï¼š
è§£å†³æ–¹æ¡ˆ 1ï¼šè‡ªå®šä¹‰å›è°ƒ
æˆ‘å®ç°äº†è‡ªå®šä¹‰å›è°ƒï¼Œä»¥åœ¨è®­ç»ƒå¼€å§‹æ—¶è®°å½•è®­ç»ƒæŸå¤±ï¼š
from transformers import TrainerCallback

class TrainOnStartCallback(TrainerCallback):
def on_train_begin(self, args, state, control, logs=None, **kwargs):
# åœ¨ç¬¬ 0 æ­¥è®°å½•è®­ç»ƒæŸå¤±
logs = logs or {}
logs[&quot;train/loss&quot;] = None # å¦‚æœå¯ç”¨ï¼Œç”¨åˆå§‹å€¼æ›¿æ¢ None
logs[&quot;train/global_step&quot;] = 0
self.log(logs)

def log(self, logs):
print(f&quot;Logging at start: {logs}&quot;)
wandb.log(logs)

# å°†å›è°ƒæ·»åŠ åˆ° Trainer
trainer = SFTTrainer(
model=model,
tokenizer=tokenizer,
train_dataset=train_dataset,
eval_dataset=eval_dataset,
args=training_args,
optimizers=(optimizer, scheduler),
callbacks=[TrainOnStartCallback()],
)

è¿™æœ‰æ•ˆï¼Œä½†æ„Ÿè§‰æœ‰ç‚¹è¿‡å¤´äº†ã€‚å®ƒä¼šåœ¨è®­ç»ƒå¼€å§‹æ—¶è®°å½•ä»»ä½•æ­¥éª¤ä¹‹å‰çš„æŒ‡æ ‡ã€‚
è§£å†³æ–¹æ¡ˆ 2ï¼šæ‰‹åŠ¨è®°å½•
æˆ–è€…ï¼Œæˆ‘åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰æ‰‹åŠ¨è®°å½•è®­ç»ƒæŸå¤±ï¼š
wandb.log({&quot;train/loss&quot;: None, &quot;train/global_step&quot;: 0})
trainer.train()

é—®é¢˜ï¼š
Trainer æˆ– SFTTrainer ä¸­æ˜¯å¦æœ‰ä»»ä½•å†…ç½®åŠŸèƒ½å¯ä»¥åœ¨ç¬¬ 0 æ­¥è®°å½•è®­ç»ƒæŸå¤±ï¼Ÿæˆ–è€…è‡ªå®šä¹‰å›è°ƒæˆ–æ‰‹åŠ¨è®°å½•æ˜¯è¿™é‡Œçš„æœ€ä½³è§£å†³æ–¹æ¡ˆå—ï¼Ÿå¦‚æœæ˜¯è¿™æ ·ï¼Œæ˜¯å¦æœ‰æ›´å¥½çš„æ–¹æ³•æ¥å®ç°æ­¤åŠŸèƒ½ï¼Ÿç±»ä¼¼äº eval_on_start ä½† train_on_startï¼Ÿ
äº¤å‰ï¼š

discuss.huggingface
github/huggingface
]]></description>
      <guid>https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer</guid>
      <pubDate>Thu, 28 Nov 2024 00:23:35 GMT</pubDate>
    </item>
    <item>
      <title>èŠ‚ç‚¹ç‰¹å¾å¯¹èŠ‚ç‚¹åˆ†ç±»çš„ GNN çš„å½±å“</title>
      <link>https://stackoverflow.com/questions/79094576/impact-of-node-features-on-gnns-for-node-classification</link>
      <description><![CDATA[æˆ‘æ­£åœ¨æ¢ç´¢å„ç§èŠ‚ç‚¹ç‰¹å¾å¯¹å›¾ç¥ç»ç½‘ç»œ (GNN) èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚æˆ‘é‡åˆ°è¿‡ PageRankã€HITS å’Œ åŸºäºç¤¾åŒºçš„å±æ€§ ç­‰ç‰¹å¾ï¼Œå®ƒä»¬ä¼¼ä¹é€šè¿‡æä¾›é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æé«˜åˆ†ç±»å‡†ç¡®æ€§ã€‚
æˆ‘å¾ˆæƒ³å¬å¬æ‚¨å¯¹ä»¥ä¸‹é—®é¢˜çš„çœ‹æ³•ï¼š

æ‚¨å¦‚ä½•å°† PageRank æˆ– HITS ç­‰ç‰¹å¾é›†æˆåˆ°æ‚¨çš„ GNN æ¨¡å‹ä¸­ï¼Œæ‚¨è§‚å¯Ÿåˆ°å®ƒä»¬å¯¹èŠ‚ç‚¹åˆ†ç±»æ€§èƒ½æœ‰ä½•å½±å“ï¼Ÿ
æ‚¨æ˜¯å¦æ¨èä½¿ç”¨ç‰¹å®šæ–¹æ³•æˆ–æ¡†æ¶æ¥æœ‰æ•ˆåœ°å°†åŸºäºç¤¾åŒºçš„ç‰¹å¾æ•´åˆåˆ° GNN ä¸­ï¼Ÿ
æ‚¨æ˜¯å¦é‡åˆ°è¿‡ä»»ä½•æä¾›å…³äºæ­¤ä¸»é¢˜çš„è§è§£æˆ–æ¡ˆä¾‹ç ”ç©¶çš„ç ”ç©¶è®ºæ–‡æˆ–èµ„æºï¼Ÿ
]]></description>
      <guid>https://stackoverflow.com/questions/79094576/impact-of-node-features-on-gnns-for-node-classification</guid>
      <pubDate>Wed, 16 Oct 2024 14:40:25 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œè¿›è¡Œåˆ†ç±»</title>
      <link>https://stackoverflow.com/questions/78145824/classification-using-graph-neural-network</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ä½¿ç”¨ GNN å¼€å±•æ¬ºè¯ˆæ£€æµ‹é¡¹ç›®ã€‚æˆ‘çš„å›¾è¡¨ä»¥é“¶è¡Œä»£ç ï¼ˆSWIFT BIC ä»£ç ï¼‰ä½œä¸ºèŠ‚ç‚¹ï¼Œè¾¹è¡¨ç¤ºäº¤æ˜“ã€‚
ä»¥ä¸‹æ˜¯æˆ‘çš„å¼ é‡çš„å½¢çŠ¶ï¼š

èŠ‚ç‚¹ç‰¹å¾å¼ é‡å½¢çŠ¶ï¼štorch.Size([210, 6])
è¾¹ç¼˜ç‰¹å¾å¼ é‡å½¢çŠ¶ï¼štorch.Size([200, 4])
é‚»æ¥çŸ©é˜µå¼ é‡å½¢çŠ¶ï¼štorch.Size([210, 210])
æ ‡ç­¾å¼ é‡å½¢çŠ¶ï¼štorch.Size([200, 1])

æˆ‘å°è¯•äº†å¾ˆå¤šæ¬¡ï¼Œä½†ç›®å‰æ­£åœ¨éµå¾ªæœ¬æ•™ç¨‹ï¼šhttps://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html
ä»¥ä¸‹æ˜¯æˆ‘çš„ GNN ä»£ç ï¼š
class GCNLayer(nn.Module):

def __init__(self, c_in, c_out):
super().__init__()
self.projection = nn.Linear(c_in, c_out)

def forward(self, node_feats, adj_matrix):
# Num neighbours = ä¼ å…¥è¾¹çš„æ•°é‡
num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)
node_feats = self.projection(node_feats)
print(&quot;node_feats &quot;,node_feats)
node_feats = torch.bmm(adj_matrix, node_feats)
node_feats = node_feats / num_neighbours
è¿”å›node_feats

layer = GCNLayer(c_in=6, c_out=210)
layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])
layer.projection.bias.data = torch.Tensor([0., 0.])

ä½¿ç”¨ torch.no_grad():
out_feats = layer(node_features_tensor, adjacency_matrix_tensor)

print(&quot;é‚»æ¥çŸ©é˜µ&quot;, adjacency_matrix_tensor)
print(&quot;è¾“å…¥ç‰¹å¾&quot;, node_features_tensor)
print(&quot;è¾“å‡ºç‰¹å¾&quot;, out_feats)

ä½†æ— è®ºæˆ‘æ€ä¹ˆå°è¯•ï¼Œä¹˜æ³•è¿‡ç¨‹ä¸­æ€»æ˜¯å‡ºç°ç»´åº¦é”™è¯¯ï¼šâ€œRuntimeErrorï¼šmat1 å’Œ mat2 å½¢çŠ¶æ— æ³•ç›¸ä¹˜ï¼ˆ210x6 å’Œ 2x2ï¼‰â€ã€‚
æˆ‘çŸ¥é“æˆ‘ä»¬æ­£åœ¨å°è¯•å°† node_Features_tensor (210,6) ä¹˜ä»¥ adjacency_matrix_tensor (210,210)ï¼Œä½†æˆ‘å·²ç»ä¸ºæ­¤å›°æ‰°äº†å¥½å‡ å¤©ï¼
æˆ‘å°è¯•äº† GNN/GCN çš„å¤šç§å®ç°ã€‚æˆ‘å¸Œæœ›èƒ½å¤Ÿè®­ç»ƒæˆ‘çš„æ¨¡å‹ã€‚]]></description>
      <guid>https://stackoverflow.com/questions/78145824/classification-using-graph-neural-network</guid>
      <pubDate>Tue, 12 Mar 2024 09:08:05 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘ä»¬å¯ä»¥åœ¨ä»…å…·æœ‰è¾¹ç¼˜ç‰¹å¾çš„å›¾ä¸Šä½¿ç”¨ GNN å—ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/77258901/can-we-use-gnn-on-graphs-with-only-edge-features</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°è¯•ä½¿ç”¨ GNN å¯¹ç³»ç»Ÿå‘è‚²æ•°æ®è¿›è¡Œåˆ†ç±»ï¼ˆå®Œå…¨äºŒåˆ†ã€å•å‘æ ‘ï¼‰ã€‚æˆ‘å°† R ä¸­çš„ç³»ç»Ÿå‘è‚²æ ‘æ ¼å¼è½¬æ¢ä¸º PyTorch æ•°æ®é›†ã€‚ä»¥å…¶ä¸­ä¸€æ£µæ ‘ä¸ºä¾‹ï¼š

Data(x=[83, 1], edge_index=[2, 82], edge_attr=[82, 1], y=[1], num_nodes=83)

å®ƒæœ‰ 83 ä¸ªèŠ‚ç‚¹ï¼ˆå†…éƒ¨èŠ‚ç‚¹ + æç¤ºèŠ‚ç‚¹ï¼Œx=[83, 1]ï¼‰ï¼Œæˆ‘ä¸ºæ‰€æœ‰èŠ‚ç‚¹åˆ†é…äº† 0ï¼Œå› æ­¤æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªç‰¹å¾å€¼ 0ã€‚æˆ‘æ„å»ºäº†ä¸€ä¸ª 82 X 1 çŸ©é˜µï¼Œå…¶ä¸­åŒ…å«èŠ‚ç‚¹ä¹‹é—´æ‰€æœ‰æœ‰å‘è¾¹çš„é•¿åº¦ï¼ˆedge_attr=[82, 1]ï¼‰ï¼Œæˆ‘æ‰“ç®—ä½¿ç”¨ edge_attr è¡¨ç¤ºè¾¹é•¿åº¦å¹¶å°†å…¶ç”¨ä½œæƒé‡ã€‚æ¯æ£µæ ‘éƒ½æœ‰ä¸€ä¸ªç”¨äºåˆ†ç±»çš„æ ‡ç­¾ï¼ˆy=[1]ï¼Œå€¼åœ¨ {0, 1, 2} ä¸­ï¼‰ã€‚
å¦‚æ‚¨æ‰€è§ï¼ŒèŠ‚ç‚¹ç‰¹å¾åœ¨æˆ‘çš„ä¾‹å­ä¸­å¹¶ä¸é‡è¦ï¼Œå”¯ä¸€é‡è¦çš„æ˜¯è¾¹ç¼˜ç‰¹å¾ï¼ˆè¾¹ç¼˜é•¿åº¦ï¼‰ã€‚
ä»¥ä¸‹æ˜¯æˆ‘ç”¨äºå»ºæ¨¡å’Œè®­ç»ƒçš„ä»£ç å®ç°ï¼š
tree_dataset = TreeData(root=None, data_list=all_graphs)

class GCN(torch.nn.Module):
def __init__(self, hidden_â€‹â€‹size=32):
super(GCN, self).__init__()
self.conv1 = GCNConv(tree_dataset.num_node_features, hidden_â€‹â€‹size)
self.conv2 = GCNConv(hidden_â€‹â€‹size, hidden_â€‹â€‹size)
self.linear = Linear(hidden_â€‹â€‹size, tree_dataset.num_classes)

def forward(self, x, edge_index, edge_attr, batch):
# 1. è·å–èŠ‚ç‚¹åµŒå…¥
x = self.conv1(x, edge_index, edge_attr)
x = x.relu()
x = self.conv2(x, edge_index, edge_attr)

# 2. è¯»å‡ºå±‚
x = global_mean_pool(x, batch) # [batch_size, hidden_â€‹â€‹channels]

# 3. åº”ç”¨æœ€ç»ˆåˆ†ç±»å™¨
x = F.dropout(x, p=0.5, training=self.training)
x = self.linear(x)

return x

model = GCN(hidden_â€‹â€‹size=32)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
train_loader = DataLoader(tree_dataset, batch_size=64, shuffle=True)
print(model)

def train():
model.train()

lost_all = 0
for data in train_loader:
optimizer.zero_grad() # æ¸…é™¤æ¢¯åº¦ã€‚
out = model(data.x, data.edge_index, data.edge_attr, data.batch) # æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ é€’ã€‚
loss = criterion(out, data.y) # è®¡ç®—æŸå¤±ã€‚
loss.backward() # å¾—å‡ºæ¢¯åº¦ã€‚
lost_all += loss.item() * data.num_graphs
optimizer.step() # æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°ã€‚

return lost_all / len(train_loader.dataset)

def test(loader):
model.eval()

correct = 0
for data in loader: # åœ¨è®­ç»ƒ/æµ‹è¯•æ•°æ®é›†ä¸Šåˆ†æ‰¹è¿­ä»£ã€‚
out = model(data.x, data.edge_index, data.edge_attr, data.batch)
pred = out.argmax(dim=1) # ä½¿ç”¨æ¦‚ç‡æœ€é«˜çš„ç±»ã€‚
correct += int((pred == data.y).sum()) # å¯¹ç…§çœŸå®æ ‡ç­¾è¿›è¡Œæ£€æŸ¥ã€‚
return correct / len(loader.dataset) # å¾—å‡ºæ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ã€‚

for epoch in range(1, 20):
loss = train()
train_acc = test(train_loader)
# test_acc = test(test_loader)
print(f&#39;Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Loss: {loss:.4f}&#39;)

çœ‹æ¥æˆ‘çš„ä»£ç æ ¹æœ¬ä¸èµ·ä½œç”¨ï¼š
......
Epoch: 015, Train Acc: 0.3333, Loss: 1.0988
Epoch: 016, Train Acc: 0.3333, Loss: 1.0979
Epoch: 017, Train Acc: 0.3333, Loss: 1.0938
Epoch: 018, Train Acc: 0.3333, Loss: 1.1044
Epoch: 019ï¼Œè®­ç»ƒç²¾åº¦ï¼š0.3333ï¼ŒæŸå¤±ï¼š1.1012
......
Epochï¼š199ï¼Œè®­ç»ƒç²¾åº¦ï¼š0.3333ï¼ŒæŸå¤±ï¼š1.0965

æ˜¯ä¸æ˜¯å› ä¸ºæ²¡æœ‰æœ‰æ„ä¹‰çš„èŠ‚ç‚¹ç‰¹å¾å°±ä¸èƒ½ä½¿ç”¨GNNï¼Ÿè¿˜æ˜¯æˆ‘çš„å®ç°æœ‰é—®é¢˜ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/77258901/can-we-use-gnn-on-graphs-with-only-edge-features</guid>
      <pubDate>Mon, 09 Oct 2023 12:36:47 GMT</pubDate>
    </item>
    <item>
      <title>æ¨¡å‹çš„æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§</title>
      <link>https://stackoverflow.com/questions/65421010/sensitivity-and-specificity-of-model</link>
      <description><![CDATA[å¦‚æœæˆ‘æœ‰ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªç±»åˆ«çš„å›¾åƒæ•°æ®é›†ï¼šæ­£å¸¸å’Œå¼‚å¸¸ï¼Œé™¤äº†å‡†ç¡®åº¦æŒ‡æ ‡ä¹‹å¤–ï¼Œæˆ‘è¿˜æƒ³æ·»åŠ æ•æ„Ÿåº¦å’Œç‰¹å¼‚æ€§æ ‡å‡†ã€‚
é‚£ä¹ˆï¼Œæˆ‘è¯¥å¦‚ä½•å¼•å…¥è¿™ä¸¤ä¸ªæŒ‡æ ‡æ¥è®¡ç®—æˆ‘çš„æ¨¡å‹çš„æ€§èƒ½å‘¢ï¼Ÿ
è°¢è°¢]]></description>
      <guid>https://stackoverflow.com/questions/65421010/sensitivity-and-specificity-of-model</guid>
      <pubDate>Wed, 23 Dec 2020 08:15:24 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•åœ¨ PyTorch ä¸­åˆå§‹åŒ–æƒé‡ï¼Ÿ</title>
      <link>https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch</link>
      <description><![CDATA[å¦‚ä½•åˆå§‹åŒ–ç½‘ç»œçš„æƒé‡å’Œåå·®ï¼ˆä¾‹å¦‚é€šè¿‡ He æˆ– Xavier åˆå§‹åŒ–ï¼‰ï¼Ÿ]]></description>
      <guid>https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch</guid>
      <pubDate>Thu, 22 Mar 2018 16:34:42 GMT</pubDate>
    </item>
    </channel>
</rss>