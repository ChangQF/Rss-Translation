<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sat, 30 Mar 2024 12:21:41 GMT</lastBuildDate>
    <item>
      <title>在 M2 GPU 上进行 PyTorch 训练比 Colab CPU 慢</title>
      <link>https://stackoverflow.com/questions/78247818/pytorch-training-on-m2-gpu-slower-than-colab-cpu</link>
      <description><![CDATA[我对深度学习/机器学习相当陌生，我一直在尝试使用我的 Apple M2 在 Jupyter 笔记本中加速我的 CNN 训练，但是，我发现尽管使用“mps”，但每个周期的执行速度要慢得多（ 1m 30-40 秒）比 Google Colab CPU 每个周期花费 40 秒。
我不确定为什么会出现这种情况，并且想知道是否有人可以帮助我理解为什么会这样或者我可能做错了什么。
我正在检查可用性和 PyTorch 版本并相应地使用它：
# 检查 PyTorch 是否可以访问 MPS（Metal Performance Shader，Apple 的 GPU 架构）
print(f“MPS（金属性能着色器）构建了吗？{torch.backends.mps.is_built()}”)
print(f&quot;MPS 可用吗？{torch.backends.mps.is_available()}&quot;)

# 设置设备
设备=“mps”； if torch.backends.mps.is_available() else “cpu”
print(f“使用设备：{device}”)

print(&quot;版本：&quot;, torch.__version__)
=======================================
MPS（金属性能着色器）是否已构建？真的
MPS 可用吗？真的
使用设备：mps
版本：2.2.2
]]></description>
      <guid>https://stackoverflow.com/questions/78247818/pytorch-training-on-m2-gpu-slower-than-colab-cpu</guid>
      <pubDate>Sat, 30 Mar 2024 11:57:27 GMT</pubDate>
    </item>
    <item>
      <title>无法解决 Pandas 中的 KeyError</title>
      <link>https://stackoverflow.com/questions/78247772/cant-resolve-keyerror-in-pandas</link>
      <description><![CDATA[我编写了此代码来在从 HuggingFace 导入的数据集上微调 gpt2：
从 Transformers 导入 Trainer、TrainingArguments、GPT2LMHeadModel、GPT2Tokenizer
model_name = “gpt2”;
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
模型 = GPT2LMHeadModel.from_pretrained(model_name)

训练参数 = 训练参数（
输出目录=“./输出”,
num_train_epochs=3,
per_device_train_batch_size=8，
logging_dir=&quot;./logs&quot;,
）

教练=教练（
型号=型号，
参数=训练参数，
train_dataset=火车，
分词器=分词器，
）
训练师.train()`

当我运行此命令时，我收到 KeyError:9198，尽管 9198 包含在我的数据集的数据框中（我检查了两次）
这是我遇到的错误：
KeyError Traceback（最近一次调用最后一次）
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method,lerance)
第3801章 试试：
-&gt;第3802章
第3803章
11帧
pandas._libs.hashtable.PyObjectHashTable.get_item() 中的 pandas/_libs/hashtable_class_helper.pxi
pandas/_libs/hashtable_class_helper.pxi 在 pandas._libs.hashtable.PyObjectHashTable.get_item()
密钥错误：9198
上述异常是导致以下异常的直接原因：
KeyError Traceback（最近一次调用最后一次）
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method,lerance)
第3802章
第3803章
-&gt;第3804章
第3805章
第3806章
密钥错误：9198]]></description>
      <guid>https://stackoverflow.com/questions/78247772/cant-resolve-keyerror-in-pandas</guid>
      <pubDate>Sat, 30 Mar 2024 11:39:34 GMT</pubDate>
    </item>
    <item>
      <title>我应该用一组图片作为一个输入数据来训练我的模型，还是需要使用 Pytorch 裁剪成小图片</title>
      <link>https://stackoverflow.com/questions/78247685/should-i-train-my-model-with-a-set-of-pictures-as-one-input-data-or-i-need-to-cr</link>
      <description><![CDATA[我看到两种训练模型的方法：

基于一张图片进行训练以及我对决策的真实预测
基于包含一组图片的一张大图片进行训练，并根据我对决策的真实预测进行类似的训练

预测输入可能是一张图片或一组图片
如果您有建议，很高兴听到您的最佳实践
如何正确提供培训输入
如何正确给出预测输入
现在，我继续使用输入作为集合和大的分离图像来训练我的模型。
我收到基于一张图片或一组预测输入的答案。]]></description>
      <guid>https://stackoverflow.com/questions/78247685/should-i-train-my-model-with-a-set-of-pictures-as-one-input-data-or-i-need-to-cr</guid>
      <pubDate>Sat, 30 Mar 2024 11:11:48 GMT</pubDate>
    </item>
    <item>
      <title>无法打开 shape_predictor_68_face_landmarks.dat</title>
      <link>https://stackoverflow.com/questions/78247670/unable-to-open-shape-predictor-68-face-landmarks-dat</link>
      <description><![CDATA[我正在尝试使用 dlib 裁剪图像来检测人脸，但我不断收到此错误，错误
我已将 .dat 文件下载到存储 .ipynb 文件的同一目录中，但问题仍然存在。我从 dlib.net 下载了该文件。]]></description>
      <guid>https://stackoverflow.com/questions/78247670/unable-to-open-shape-predictor-68-face-landmarks-dat</guid>
      <pubDate>Sat, 30 Mar 2024 11:04:21 GMT</pubDate>
    </item>
    <item>
      <title>R 中没有为任何变量提供梯度</title>
      <link>https://stackoverflow.com/questions/78247273/no-gradients-provided-for-any-variable-in-r</link>
      <description><![CDATA[我正在使用 R 和 MNIST 数据集进行深度学习。
我编写了这段代码来存储训练和测试数据，并定义和拟合模型：
库（keras）

#获取数据
mnist &lt;- dataset_mnist()
train_data &lt;- mnist$train$x
train_labels &lt;- mnist$train$y
test_data &lt;- mnist$test$x
test_labels &lt;- mnist$test$y

#重塑&amp;正常化
train_data &lt;- array_reshape(train_data,c(nrow(train_data), 784))
训练数据 &lt;- 训练数据 / 255
test_data &lt;- array_reshape(test_data,c(nrow(test_data), 784))
测试数据 &lt;- 测试数据 / 255

#一个热编码 train_labels &lt;- to_categorical(train_labels, 10)
测试标签 &lt;- to_categorical(测试标签, 10)

＃模型
模型 &lt;- keras_model_sequential()
模型％&gt;％layer_dense（单位= 128，激活=“relu”，input_shape = c（784））％&gt;％
            Layer_dropout(率=0.3) %&gt;%
            Layer_dense(单位=64，激活=“relu”)%&gt;%
            Layer_dropout(率=0.2) %&gt;%
            Layer_dense（单位= 10，激活=“softmax”）

#编译
模型%&gt;%编译(loss=“categorical_crossentropy”,
                    优化器=“rmsprop”，
                    指标=“准确性”）
    
＃火车
历史 &lt;- 模型 %&gt;% fit(train_data,
                        火车标签，
                        纪元=10，
                        批量大小=784，
                        验证分割=0.2，
                        详细=2)
    
#评估与预测
模型 %&gt;% 评估(test_data, test_labels)
pred &lt;- 模型 %&gt;% 预测(test_data)
打印（表（预测= pred，实际= test_labels））

在R studio中运行时，出现以下错误：
ValueError：没有为任何变量提供渐变：([&#39;dense_124/kernel:0&#39;, &#39;dense_124/bias:0&#39;, &#39;dense_123/kernel:0&#39;, &#39;dense_123/bias:0&#39;, &#39;密集_122 /内核：0&#39;，&#39;密集_122 /偏差：0&#39;]，）。假设 `grads_and_vars` 为 ((None, ), (None, ), (无, ), (无, ), (无, ), (无, ))。

我认为问题可能在于输入数据和输入的形状冲突，但不知道如何解决这个问题。
感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78247273/no-gradients-provided-for-any-variable-in-r</guid>
      <pubDate>Sat, 30 Mar 2024 08:30:30 GMT</pubDate>
    </item>
    <item>
      <title>从二维输入预测多个输出的回归问题</title>
      <link>https://stackoverflow.com/questions/78247231/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</link>
      <description><![CDATA[我有几个二维图表，每个图表都有七个独特的数字特征，可用于生成这些图表。我以大量 CSV 文件的形式获得了所有这些图表的 x 和 y 坐标及其数值特征。我想通过使用机器学习或深度学习模型来预测每个图的数值特征（通过使用图的图像或使用每个图的点的坐标）
例如，这是我的一张图表：

该图的独特数字特征是 [8.76e15, 8e-1, 5e-2, 5e-3, 5e-2, 9.65e-1, 2.1e-9] （我有该图所有点的坐标对 (x, y) 以两列 CSV 文件的形式存在，我也可以使用它们）。
到目前为止，我已经寻找了很多预训练的模型，并在 HuggingFace 等网站上搜索了此类模型，还在 GitHub 代码中搜索了很多。我还在 Papers with Code 网站上搜索了做过同样事情的文章，但不幸的是，我仍然没有找到任何东西！我曾多次尝试自己编写一个网络，但由于这样做的复杂性以及对于如何设置网络的超参数以达到预期结果的知识不够，我遇到了很多错误并且无法做到这一点！
例如，我编写了以下代码：
X = []
y = []
目录=“数据”；
对于 os.listdir（目录）中的 csv_file：
    data = pd.read_csv(f&quot;{目录}/{csv_file}&quot;)
    X.append(data.iloc[1:, :2].astype(float).values)
    y.append(data.iloc[0, 2:].astype(float).values)
X = np.array(X, dtype=np.float64) # X.shape: (50000, 253, 2)
y = np.array(y, dtype=np.float64) # y.shape: (50000, 7)

X_train = X[:40000,:,:]
X_val = X[40000:, :, :]
y_train = y[:40000,:]
y_val = y[40000:, :]

定标器=标准定标器()
X_train_scaled = 缩放器.fit_transform(X_train)
X_val_scaled = 缩放器.fit_transform(X_val)

输入 = keras.layers.Input(shape=(X.shape[1], X.shape[2]))
lstm_out = keras.layers.LSTM(32)(输入)
输出 = keras.layers.Dense(7)(lstm_out)

模型= keras.Model（输入=输入，输出=输出）
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=“mse”)
模型.summary()

历史=模型.fit(
    x=X_train,
    y = y_train，
    纪元=10，
）

损失非常高，而且一点也不好。
我该怎么做？]]></description>
      <guid>https://stackoverflow.com/questions/78247231/the-regression-problem-of-predicting-multiple-outputs-from-two-dimensional-input</guid>
      <pubDate>Sat, 30 Mar 2024 08:11:00 GMT</pubDate>
    </item>
    <item>
      <title>处理用于骨折检测的 Flask 应用程序中的不相关上传</title>
      <link>https://stackoverflow.com/questions/78246942/handling-irrelevant-uploads-in-flask-application-for-bone-fracture-detection</link>
      <description><![CDATA[我正在开发一个用于骨折检测的 Flask 应用程序，用户可以在其中上传 X 射线图像，该应用程序会预测是否存在骨折。但是，我在处理不相关的上传时遇到问题，例如肘部、手和肩膀以外的身体部位的图像。
以下是我的 Flask 应用程序代码的概述：
Flask 应用代码
导入操作系统
从烧瓶导入烧瓶，请求，jsonify
从 werkzeug.utils 导入 secure_filename
从预测导入预测
从flask_cors导入CORS

应用程序=烧瓶（__名称__）
CORS(app) # 为所有路由启用 CORS
app.config[&#39;UPLOAD_FOLDER&#39;] = &#39;上传&#39;
app.config[&#39;ALLOWED_EXTENSIONS&#39;] = {&#39;png&#39;, &#39;jpg&#39;, &#39;jpeg&#39;}

def allowed_file(文件名):
    返回 &#39;​​。&#39;在 filename 和 filename.rsplit(&#39;.&#39;, 1)[1].lower() 中 app.config[&#39;ALLOWED_EXTENSIONS&#39;]

@app.route(&#39;/predict&#39;,methods=[&#39;POST&#39;])
def Predict_bone_fracture():
    如果“文件”不在 request.files 中：
        return jsonify({&#39;error&#39;: &#39;没有文件部分&#39;})

    文件 = request.files[&#39;文件&#39;]

    if file.filename == &#39;&#39;:
        return jsonify({&#39;error&#39;: &#39;没有选择文件&#39;})

    如果文件和 allowed_file(file.filename):
        文件名 = secure_filename(文件.文件名)
        filepath = os.path.join(app.config[&#39;UPLOAD_FOLDER&#39;], 文件名)
        文件.保存（文件路径）

        # 使用 Predictions.py 中的预测函数执行预测
        骨骼类型结果 = 预测（文件路径）
        结果=预测（文件路径，bone_type_result）

        # 您可以根据您的要求自定义响应
        返回jsonify（{&#39;bone_type&#39;：bone_type_result，&#39;结果&#39;：结果}）

    return jsonify({&#39;error&#39;: &#39;文件格式无效&#39;})

如果 __name__ == “__main__”：
    应用程序.run()


predict_bone_fracture() 函数接收上传的图像，将其保存到指定文件夹，然后使用外部模块 (predictions.py) 中的 Predict() 函数执行预测。如果上传的文件不是图像或格式不受支持，则返回错误响应。
我主要关心的是如何处理用户上传与指定身体部位（即肘部、手部、肩膀）不对应的图像的情况。例如，如果用户上传眼睛图像而不是骨骼图像，则应用程序应拒绝上传并提供适当的错误消息。
我相信我需要采用一种机制来检测上传图像中的相关身体部位，并验证它们是否与预期的预测身体部位（即肘部、手部、肩膀）相匹配。但是，我不确定实现此目的的最佳方法。
您能否提供有关如何解决此问题的建议或想法？具体来说，我正在寻找以下方面的指导：
实施一种机制来检测上传图像中的相关身体部位。
检查检测到的身体部位是否与预期的预测身体部位相匹配。
为不相关的上传提供适当的错误处理和消息。
任何见解或代码示例将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78246942/handling-irrelevant-uploads-in-flask-application-for-bone-fracture-detection</guid>
      <pubDate>Sat, 30 Mar 2024 05:41:52 GMT</pubDate>
    </item>
    <item>
      <title>在进行二值图像分类时，设置为二值的类模式错误地标记了图像，但它在分类上是否正确</title>
      <link>https://stackoverflow.com/questions/78246763/while-working-on-binary-image-classification-the-class-mode-set-to-binary-incor</link>
      <description><![CDATA[我目前正在研究二值图像分类。我的问题是，当我使用数据增强时，当它设置为二进制时，它会错误地标记图像。
我尝试过的事情：

在扩充我的数据之前，在图像上查找错误标记的类。不过这并没有什么问题。

将课程模式更改为分类模式。这是可行的，但是我不从事多类分类。

尝试使用图像数据生成器。没有任何效果。

寻找分类不平衡的情况。也没什么问题。


我还可以尝试什么？我应该在生成器上使用分类类模式吗？]]></description>
      <guid>https://stackoverflow.com/questions/78246763/while-working-on-binary-image-classification-the-class-mode-set-to-binary-incor</guid>
      <pubDate>Sat, 30 Mar 2024 03:44:32 GMT</pubDate>
    </item>
    <item>
      <title>计算explained_variance_score，手动方法和函数调用结果不同</title>
      <link>https://stackoverflow.com/questions/78246746/calculating-explained-variance-score-result-are-different-between-manual-method</link>
      <description><![CDATA[根据官方页面的公式
https://scikit-learn.org/stable/modules/ model_evaluation.html#explained-variance-score，计算数据集的以下 EVS：
y_true = [1, 2, 3, 4, 5] y_pred = [6, 7, 8, 9, 10]
手动：evs = 1 - var(y_true - y_pred)/var(y_true) = -11.5
使用代码：evs = 1
从 sklearn.metrics 导入解释_方差_分数

y_true = [1, 2, 3, 4, 5]
y_pred = [6, 7, 8, 9, 10]

解释的方差 = 解释的方差_分数(y_true, y_pred)

为什么结果不同？]]></description>
      <guid>https://stackoverflow.com/questions/78246746/calculating-explained-variance-score-result-are-different-between-manual-method</guid>
      <pubDate>Sat, 30 Mar 2024 03:26:49 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-Learn 排列和更新 Polars DataFrame</title>
      <link>https://stackoverflow.com/questions/78246736/scikit-learn-permutating-and-updating-polars-dataframe</link>
      <description><![CDATA[我正在尝试重写scikit-learn 排列重要性要实现的：

与 Polar 的兼容性
与功能集群的兼容性

将极坐标导入为 pl
将 Polars.selectors 导入为 cs
将 numpy 导入为 np

从 sklearn.datasets 导入 make_classification
从 sklearn.model_selection 导入 train_test_split

X, y = make_classification(
    n_样本=1000，
    n_特征=10，
    n_信息=3，
    n_冗余=0，
    n_重复=0，
    n_classes=2,
    随机状态=42，
    随机播放=假，
）
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)
feature_names = [f&quot;feature_{i}&quot;;对于范围内的 i(X.shape[1])]

X_train_polars = pl.DataFrame(X_train, schema=feature_names)
X_test_polars = pl.DataFrame(X_test, schema=feature_names)
y_train_polars = pl.Series(y_train, schema=[“目标”])
y_test_polars = pl.Series(y_test, schema=[“目标”])

为了获得一组特征的未来重要性，我们需要同时排列一组特征，然后传递给评分器以与基线分数进行比较。
但是，在检查特征簇时，我正在努力替换多个极坐标数据框列：
from sklearn.utils import check_random_state
随机状态=检查随机状态(42)
random_seed = random_state.randint(np.iinfo(np.int32).max + 1)

X_train_permuted = X_train_polars.clone()
shuffle_arr = np.array(X_train_permuted[:, [“feature_0”, “feature_1”]])

random_state.shuffle(shuffle_arr)
X_train_permuted.replace_column( # 这个操作到位
                0,
                pl.Series(name=“feature_0”,values=shuffle_arr))

通常，shuffle_arr 的形状为 (n_samples,)，可以使用 polars.DataFrame.replace_column() 轻松替换 Polars 数据帧中的相关列。在这种情况下，shuffle_arr 的多维形状为（簇中的 n_samples，n_features）。替换相关列的有效方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78246736/scikit-learn-permutating-and-updating-polars-dataframe</guid>
      <pubDate>Sat, 30 Mar 2024 03:20:39 GMT</pubDate>
    </item>
    <item>
      <title>神经网络距离与性能之间的关系</title>
      <link>https://stackoverflow.com/questions/78246568/relationship-between-neural-network-distances-and-performance</link>
      <description><![CDATA[我一直想知道“距离”与“距离”之间是否存在相关性。神经网络权重及其性能之间的关系。
为了详细说明，请考虑以下场景：
我们有三个模型：M1、M2 和 M3，它们都具有相同的结构，每个模型都在其各自的数据集 D1、D2 和 D3 上进行训练。训练后，让我们说一下“距离”。 M1 和 M2 之间的距离是 5，而 M1 和 M3 之间的距离是 20。本质上，M1 和 M2 “空间上”更接近。
我想说的是，如果我们在 D2 和 D3 上评估 M1，它在 D2 上的性能应该更高，因为 M1 更接近 M2，并且 M2 是在 D2 上训练的。然而，一些实验与这一假设相矛盾。
我故意将“距离”括起来用引号引起来，因为我不确定在这种情况下采用的适当指标。
我找到了一些关于该主题的论文，但它们似乎不能满足我的需求。
谁能帮助我更好地理解“距离”和“距离”之间是否存在关系？和性能？
非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/78246568/relationship-between-neural-network-distances-and-performance</guid>
      <pubDate>Sat, 30 Mar 2024 01:27:55 GMT</pubDate>
    </item>
    <item>
      <title>运行排列重要性时，我们是否会排列测试集中的列？</title>
      <link>https://stackoverflow.com/questions/78243995/do-we-permute-columns-in-the-test-set-when-running-permutation-importance</link>
      <description><![CDATA[我一直在查看有关排列重要性的文档和相关教程，但似乎没有人清楚地了解他们实际排列的内容。
为了澄清，分步过程如下：

将数据集拆分为 X_train、X_val 和 X_test

在 X_train 上训练数据，使用 X_val 例如找到最佳纪元

在 X_test 上运行经过训练的模型，记下我们正在测量的指标

排列 X_test 中的特征，并在此排列后的 X_test 数据集上运行相同的模型

记下相同的指标并比较两者

对每个变量重复此操作，而不更改模型。


旁白问题：是否值得重复运行此排列过程，其中 X_train、X_val 和 X_test 随着每次重复而变化。我知道最终的模型会有所不同，但我想广泛了解通用模型（具有固定超参数）在不同数据集上训练时的表现，因为保持 X_test 固定可能会扭曲某些特征的感知重要性。]]></description>
      <guid>https://stackoverflow.com/questions/78243995/do-we-permute-columns-in-the-test-set-when-running-permutation-importance</guid>
      <pubDate>Fri, 29 Mar 2024 13:25:13 GMT</pubDate>
    </item>
    <item>
      <title>用于确定 TRL（技术准备水平）的问答模型</title>
      <link>https://stackoverflow.com/questions/78243266/question-answering-model-for-determine-trltechnology-readiness-levels</link>
      <description><![CDATA[我们想要创建一个问答模型。如何创建像 chatgpt 这样的问答模型，根据对该模型提供的文章、专利或任何产品描述来确定技术准备程度（从 1 到 9）？
我们考虑使用 robeta 模型作为现成的模型，但是我们如何根据来自 API 的文章格式（标题、描述、作者等）集成掩蔽、微调和预训练阶段到这种格式并从这种格式中获取信息并确定文章在一定水平上的技术就绪程度？
我考虑过给每个部分，即标题部分中决定成熟度级别的单词赋予系数2，为描述部分中表示成熟度级别的单词赋予系数1.5，以及然后给潜文本中显示成熟度的单词赋予系数1，但是模型将如何在该部分学习这一点呢？我被困住了。如果每次冻结不同的结果，它就不会一致。您能帮我详细解释一下如何完成这些阶段吗？]]></description>
      <guid>https://stackoverflow.com/questions/78243266/question-answering-model-for-determine-trltechnology-readiness-levels</guid>
      <pubDate>Fri, 29 Mar 2024 10:34:46 GMT</pubDate>
    </item>
    <item>
      <title>在 Databricks AutoML 运行的最佳试用笔记本的“加载数据”部分中访问 df_loaded 和/或 run_id</title>
      <link>https://stackoverflow.com/questions/78241331/access-df-loaded-and-or-run-id-in-load-data-section-of-best-trial-notebook-of-da</link>
      <description><![CDATA[下面的代码块是通过执行 Databricks AutoML 运行自动生成的最佳试用笔记本的一部分。
导入mlflow
导入操作系统
导入uuid
进口舒蒂尔
将 pandas 导入为 pd

# 创建临时目录以从 MLflow 下载输入数据
input_temp_dir = os.path.join(os.environ[&quot;SPARK_LOCAL_DIRS&quot;], &quot;tmp&quot;, str(uuid.uuid4())[:8])
os.makedirs(input_temp_dir)


# 下载工件并将其读入 pandas DataFrame
input_data_path = mlflow.artifacts.download_artifacts（run_id =“e2a4a93aafb24aa9956e83f6b7ab3e28”，artifact_path =“数据”，dst_path = input_temp_dir）

df_loaded = pd.read_parquet(os.path.join(input_data_path, “training_data”))
# 删除临时数据
Shutil.rmtree(input_temp_dir)

# 预览数据
df_loaded.head(5)


上面代码块中的 run_id e2a4a93aafb24aa9956e83f6b7ab3e28，我可以从运行 automl.regress 返回的 AutoMLSummary 中获取它吗？如果我使用summary.best_trial.mlflow_run_id，我会得到不同的值。那么这个 run_id 是什么以及如何获取它？

除了上面的代码块之外，还有没有办法获取已加载到 df_loaded 中的数据集？它本质上是我输入到 automl.regress 中的输入数据集，只不过它有一列指示每一行是否是训练、验证和测试子集的一部分。


我对 Databricks AutoML 相当陌生，因此不确定完成此任务的最佳方法是什么。
提前致谢。
正如我所提到的，我尝试从summary.best_trial.mlflow_run_id 中获取run_id，但值不匹配。我尝试阅读 automl 和 mlflow 的文档，但没有成功。]]></description>
      <guid>https://stackoverflow.com/questions/78241331/access-df-loaded-and-or-run-id-in-load-data-section-of-best-trial-notebook-of-da</guid>
      <pubDate>Thu, 28 Mar 2024 23:32:11 GMT</pubDate>
    </item>
    <item>
      <title>将 Detectron2 模型转换为 torchscript</title>
      <link>https://stackoverflow.com/questions/73619217/convert-detectron2-model-to-torchscript</link>
      <description><![CDATA[我想将 detectorron2 &#39;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml 模型&#39; 转换为 torchscript。
我用过托克
我的代码如下。
&lt;前&gt;&lt;代码&gt;导入cv2

将 numpy 导入为 np

进口火炬
从 detector2 导入 model_zoo
从 detector2.config 导入 get_cfg
从 detectorron2.engine 导入 DefaultPredictor
从 detector2.modeling 导入 build_model
从 detectorron2.export.flatten 导入 TracingAdapter
导入操作系统

ModelPath=&#39;/home/jayasanka/working_files/create_torchsript/model.pt&#39;
将 open(&#39;savepic.npy&#39;, &#39;rb&#39;) 作为 f：
    图像 = np.load(f)

#------------------------------------------------- ------------------------------------------------

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file(“COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml”))

cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 # 你的类数 + 1

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, ModelPath)

cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.60 # 设置该模型的测试阈值

预测器 = DefaultPredictor(cfg)



我使用了 TracingAdapter 和跟踪函数。我不太了解其背后的概念是什么。
&lt;前&gt;&lt;代码&gt;# im = cv2.imread(图像)
im = torch.tensor(图像)

def inference_func（模型，图像）：
    输入= [{“图像”：图像}]
    返回 model.inference(inputs, do_postprocess=False)[0]

包装器= TracingAdapter（预测器，im，inference_func）
包装器.eval()
Traced_script_module= torch.jit.trace（包装器，（im，））
traced_script_module.save(“torchscript.pt”)

它给出了下面给出的错误。
回溯（最近一次调用最后一次）：
  文件“script.py”，第 49 行，位于  中。
    Traced_script_module= torch.jit.trace（包装器，（im，））
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/jit/_trace.py”，第 744 行，跟踪中
    _模块_类，
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/jit/_trace.py”，第 959 行，在trace_module 中
    参数名称，
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/nn/modules/module.py”，第 1051 行，在 _call_impl 中
    返回forward_call（*输入，**kwargs）
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/torch/nn/modules/module.py”，第 1039 行，位于 _slow_forward
    结果 = self.forward(*输入, **kwargs)
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/detectron2/export/flatten.py”，第 294 行，向前
    输出 = self.inference_func(self.model, *inputs_orig_format)
  文件“script.py”，第 44 行，inference_func
    返回 model.inference(inputs, do_postprocess=False)[0]
  文件“/home/jayasanka/anaconda3/envs/vha/lib/python3.7/site-packages/yacs/config.py”，第 141 行，在 __getattr__ 中
    引发属性错误（名称）
属性错误：推理


你能帮我解决这个问题吗？
还有其他方法可以轻松做到这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/73619217/convert-detectron2-model-to-torchscript</guid>
      <pubDate>Tue, 06 Sep 2022 08:50:15 GMT</pubDate>
    </item>
    </channel>
</rss>