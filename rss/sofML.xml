<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 01 Dec 2023 09:14:23 GMT</lastBuildDate>
    <item>
      <title>InvalidArgumentError：double 的 attr 'Tindices' 的值不在允许值列表中：int16、int32、int64</title>
      <link>https://stackoverflow.com/questions/77584003/invalidargumenterror-value-for-attr-tindices-of-double-is-not-in-the-list-of</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77584003/invalidargumenterror-value-for-attr-tindices-of-double-is-not-in-the-list-of</guid>
      <pubDate>Fri, 01 Dec 2023 08:53:25 GMT</pubDate>
    </item>
    <item>
      <title>这是学习人工智能和机器学习的良好路线图吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77583919/is-this-a-good-roadmap-to-learn-ai-and-machine-learning</link>
      <description><![CDATA[AI 工程师路线图
我在 roadmap.sh 上看到了这个。我是计算机科学专业的二年级学生，我想学习人工智能和机器学习。这对我来说是一个好的路线图吗？他们还提到了所需的课程链接
这是路线图的链接https://roadmap.sh/ai-data-scientist 
请帮助我提供您的建议和提示]]></description>
      <guid>https://stackoverflow.com/questions/77583919/is-this-a-good-roadmap-to-learn-ai-and-machine-learning</guid>
      <pubDate>Fri, 01 Dec 2023 08:36:32 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 分类，XGBClassifier 与 xgb.train 的 AUC 分数不同，即使在舍入预测概率后也是如此</title>
      <link>https://stackoverflow.com/questions/77583899/xgboost-classification-different-auc-score-for-xgbclassifier-vs-xgb-train-even</link>
      <description><![CDATA[看起来 XGBClassifier 是 xgb.train 的包装器。我试图使用 xgb.train 和 &quot;objective&quot;: &quot;binary:logistic&quot; 训练二元分类器。
似乎使用 xgb.train 后跟 .predict() 返回预测概率，我们可以将其舍入为 0 或 1 以进行分类。
但是，即使在训练数据集上与 XGBClassifier 的 AUC 分数进行比较时，xgb.train 方法也明显较差......
使用xgb.train作为分类器的正确方法是什么？
X = df[X_colnames]
y = df[&#39;BP命中&#39;].astype(&#39;类别&#39;)
dtrain = xgb.DMatrix(X, 标签=y)
param = {“目标”：“二进制：逻辑”，}
模型 = xgb.train(param,dtrain)
y_hat = model.predict(xgb.DMatrix(X))
打印（y_帽子）
print(sklearn.metrics.roc_auc_score(y,[round(y) for y in y_hat]))


模型= XGBClassifier（目标=&#39;二进制：逻辑&#39;）
# 定义训练数据
X = df[X_列名]
y = df[&#39;BP命中&#39;].astype(&#39;类别&#39;)
模型.fit(X,y)
打印（模型.预测（X））
print(sklearn.metrics.roc_auc_score(y,[model.predict(X) 中 y 的 round(y)]))

输出：
&lt;预&gt;&lt;代码&gt;[0.02280498 0.02280498 0.02280498 ... 0.02280498 0.02280498 0.02280498]
0.9427811205601163
[0 0 0 ... 0 0 0]
1.0
]]></description>
      <guid>https://stackoverflow.com/questions/77583899/xgboost-classification-different-auc-score-for-xgbclassifier-vs-xgb-train-even</guid>
      <pubDate>Fri, 01 Dec 2023 08:32:40 GMT</pubDate>
    </item>
    <item>
      <title>如何使用openvino优化模型和YOLO将目标检测应用程序转换为exe</title>
      <link>https://stackoverflow.com/questions/77583764/how-to-convert-object-detection-application-in-exe-by-using-openvino-optimized</link>
      <description><![CDATA[当我使用这个命令pyinstaller main.py --onefile -w 将项目转换为exe文件时，我遇到了这个问题：
错误：[Errno 2] 没有这样的文件或目录：&#39;&#39;C:\Users\sachin\Appdata\Local\Temp\_MEI177322\ultralytics\yolo\cfg\default.yaml。
]]></description>
      <guid>https://stackoverflow.com/questions/77583764/how-to-convert-object-detection-application-in-exe-by-using-openvino-optimized</guid>
      <pubDate>Fri, 01 Dec 2023 07:59:57 GMT</pubDate>
    </item>
    <item>
      <title>寻找特定领域的数据集来训练机器学习模型[关闭]</title>
      <link>https://stackoverflow.com/questions/77583636/seeking-domain-specific-datasets-for-training-machine-learning-models</link>
      <description><![CDATA[我目前正在开发机器学习模型，需要特定领域的数据集。我的目标是训练一个专门理解以领域为中心的内容并与之交互的模型，这些内容的范围可以从 IT 和工程等技术领域到医疗保健或金融等专业领域。
详细信息：
数据类型：我正在寻找包含特定领域术语、行话、工作流程以及与该领域专家相关的任何相关知识的数据集。
用途：目的是训练用于特定领域问答、内容生成和数据分析等任务的模型。
数据特异性：数据在领域覆盖范围方面越全面、越多样化越好。
我尝试过的：
我在 GitHub、Kaggle 和 Google 数据集搜索等平台上搜索了开源数据集。
我研究了学术数据库中的研究论文和特定领域的相关数据集。
我探索了与目标领域的行业合作伙伴的数据共享协议。
我正在联系社区中的任何人，看看是否有人有建议或知道可以公开或购买此类特定于域的数据集的存储库。]]></description>
      <guid>https://stackoverflow.com/questions/77583636/seeking-domain-specific-datasets-for-training-machine-learning-models</guid>
      <pubDate>Fri, 01 Dec 2023 07:31:33 GMT</pubDate>
    </item>
    <item>
      <title>对视频中的帧进行分类，查看它是否包含一组幻灯片中的一张</title>
      <link>https://stackoverflow.com/questions/77583613/categorise-a-frame-in-a-video-to-see-if-it-contains-one-of-a-set-of-slides</link>
      <description><![CDATA[我想要处理视频，其中讲师站在白板前，上面有多张幻灯片。他们可能会四处走动、挡住黑板或在幻灯片上书写。我已经收集了所有幻灯片，但我无法实时训练任何模型，即解决方案应该只是比较两个图像（视频帧和幻灯片图像）之间的“相似性”。我觉得问题是图像相似性和单一分类同时存在。
我的想法是每隔 30 秒左右抓取几帧（假设任何重要幻灯片的使用时间超过 30 秒），并将其与我准备的幻灯片进行比较（一场讲座通常有大约 9-10 张幻灯片）。这有多可行？
我不是在寻找代码，我来这里是为了了解总体思路。我想尽快完成这项工作，因此任何能够完成繁重工作的库都是（非常）可取的，但如果没有其他选择，我确实了解一点 PyTorch。如果唯一的选择是训练我自己的模型，什么架构/模型最适合这个？如果有人可以分享与此相关的任何资源，我也希望如此。
编辑：伙计们至少在这样做之前评论一下为什么你投反对票，这有什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77583613/categorise-a-frame-in-a-video-to-see-if-it-contains-one-of-a-set-of-slides</guid>
      <pubDate>Fri, 01 Dec 2023 07:27:55 GMT</pubDate>
    </item>
    <item>
      <title>在启用反向传播的情况下，有效模拟具有异质空间和时间感受野的神经元？</title>
      <link>https://stackoverflow.com/questions/77582849/efficient-simulation-of-neurons-with-heterogeneous-spatial-and-temporal-receptiv</link>
      <description><![CDATA[我想找到一种方法来对具有异构空间和时间感受野的神经元进行高效模拟，并且理想情况下启用反向传播，以便我可以微调感受野的权重。每个神经元的计算简单，但神经元数量较多（1e5 ~ 1e6）。主要困难在于每个神经元的空间和时间感受野的大小不同（从动物实验数据获得）并且变化很大。在GPU上进行矢量化运算有点困难。
模拟如下：
# 初始化
# 每个神经元的k和l的值是不同的。
获取输入图像的形状（在整个模拟过程中固定）。
对于每个神经元：
    (1).获取其空间感受野下的像素索引，存储为 [k x 2] 数组，因为空间感受野可能不是正方形。
    （2）。初始化空间感受野中的权重，存储为 [k x 1] 数组。
    （3）。获取时间感受野的长度l（时间步数）。
    （4）。初始化时间感受野中的权重，存储为 [l x 1] 数组。
    （5）。初始化一个空的 [l x 1] 数组缓冲区，用零填充。
将所有神经元的参数存储在列表中。

＃ 模拟
对于每个输入图像：
    对于每个神经元：
        (1).根据图像的 [k x 2] 像素索引获得展平的补丁，存储为 [k x 1] 数组。
        （2）。计算图像块与空间感受野权重的内积，输出 [1 x 1] 标量。
        （3）。删除缓冲区中的第一个元素，将其余元素与 [1 x 1] 标量（最后位置的标量）连接，输出 [l x 1] 更新的缓冲区。
        （4）。计算更新后的缓冲区与时间感受野权重的内积，输出 [1 x 1] 标量。
        （5）。将标量传递给 ReLU 函数，将输出设置为神经元的输出。

我已经在 Python JAX 中实现了上述过程。实现与上面的描述几乎完全相同，包括仿真阶段的双for循环。但CPU和GPU上的性能都很慢。
目前，我想找到一种方法来优化模拟过程中的每个神经元循环（因为在测试时，可以从相机实时获取输入图像）。初始化阶段的性能已经不错了。我想要实现的是：

在 CPU 上运行时使用所有可用的 CPU 内核。默认情况下，JAX 仅使用一个 CPU 核心。
在 GPU 上运行时对计算进行向量化，同时保持内存效率。每个神经元的空间和时间感受野的大小都不同。如果我们简单地将小感受野补零到最大尺寸，它可能很容易消耗整个 GPU 内存。
理想情况下，将来可以通过反向传播重复使用相同的模拟代码进行训练。

有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/77582849/efficient-simulation-of-neurons-with-heterogeneous-spatial-and-temporal-receptiv</guid>
      <pubDate>Fri, 01 Dec 2023 03:26:31 GMT</pubDate>
    </item>
    <item>
      <title>unserialize(socklist[[n]]) 中的错误：在插入符中执行 RF 模型时从连接读取错误</title>
      <link>https://stackoverflow.com/questions/77582605/error-in-unserializesocklistn-error-reading-from-connection-while-execut</link>
      <description><![CDATA[我正在尝试运行随机森林代码，以使用 R studio 中的插入符包对卫星图像进行分类。在训练模型时，我总是收到错误“unserialize(socklist[[n]]) 中的错误：从连接读取错误”。尝试关闭集群时“unserialize(node$con) 中的错误：从连接读取时出错”这个错误来了。我正在使用 caret 包并使用 doparallel 包进行并行计算
&quot;cl &lt;- makeCluster(3/4 * detectorCores())
registerDoParallel(cl)”
我使用的系统具有 i9（第 13 代）、128 GB 内存，并尝试使用 28、24、16 核。所有的尝试都以这个错误告终。我重新安装了 R 和 R studio，将版本更改为旧版本，在终端中运行脚本，但出现了同样的错误。相同的代码在 i5（第 9 代）笔记本电脑上成功运行； 16 GB 内存仅使用 3 个核心，但输出需要 12 小时。我该如何解决这个问题。这是代码、R studio 还是我正在使用的系统的问题？]]></description>
      <guid>https://stackoverflow.com/questions/77582605/error-in-unserializesocklistn-error-reading-from-connection-while-execut</guid>
      <pubDate>Fri, 01 Dec 2023 01:53:00 GMT</pubDate>
    </item>
    <item>
      <title>有人可以向我解释这个错误吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77582582/guys-can-some-one-explain-this-error-to-me</link>
      <description><![CDATA[文件“e:\\Projects\\Ai\\NumReco\\train.py”，第 16 行，位于 __init__ 中
   尝试：self.qtr，self.atr，self.qte，self.ate=self.train，self.test=self.mnist=self.getqa（路径）
                                           ^^^^^^^^^^^^^^^^^^^^^^
ValueError：需要解压的值太多（预期为 2）

有人可以告诉我为什么有“太多的值需要解包（预期为 2 个）”，但我却得到了两个需要解包的值？]]></description>
      <guid>https://stackoverflow.com/questions/77582582/guys-can-some-one-explain-this-error-to-me</guid>
      <pubDate>Fri, 01 Dec 2023 01:39:01 GMT</pubDate>
    </item>
    <item>
      <title>将时间序列高分辨率数据集存储为一个更简单的事务集，其中包括其元数据[关闭]</title>
      <link>https://stackoverflow.com/questions/77582450/store-time-series-high-resolution-data-sets-each-computed-summarized-into-a-simp</link>
      <description><![CDATA[我正在测试时间序列高分辨率数据集，每个数据集都计算/汇总为一个更简单的事务集（包括其元数据）。这些存储在本地服务器中。我很少遇到连接表的需求；但是，我不断使用时间序列数据来创建其他指标来构建机器学习模型。
CSV 中的每个时间序列数据集最大可达 2.6MB，而交易数据则为 kB 大小。我有超过 20,000 组数据。
什么是具有最佳性能的数据库选项/架构？
关系型还是非关系型？
每个选项的供应商有哪些？
如何确定是否应该迁移到云服务器？
推荐的供应商有哪些？]]></description>
      <guid>https://stackoverflow.com/questions/77582450/store-time-series-high-resolution-data-sets-each-computed-summarized-into-a-simp</guid>
      <pubDate>Fri, 01 Dec 2023 00:47:12 GMT</pubDate>
    </item>
    <item>
      <title>如何按照官方方式将 Hugging Face LLaMA v2 模型的权重重新初始化为原始模型？</title>
      <link>https://stackoverflow.com/questions/77499162/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-offic</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77499162/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-offic</guid>
      <pubDate>Fri, 17 Nov 2023 03:15:56 GMT</pubDate>
    </item>
    <item>
      <title>如何防止 Keras 在训练期间计算指标</title>
      <link>https://stackoverflow.com/questions/71412499/how-to-prevent-keras-from-computing-metrics-during-training</link>
      <description><![CDATA[我正在使用 Tensorflow/Keras 2.4.1，并且我有一个（无监督的）自定义指标，它将我的多个模型输入作为参数，例如：
model = build_model() # 返回一个 tf.keras.Model 对象
my_metric = custom_metric(model.output, model.input[0], model.input[1])
模型.add_metric(my_metric)
[...]
model.fit([...]) # 使用 fit 进行训练

但是，custom_metric 非常昂贵，因此我希望仅在验证期间计算它。我找到了这个答案，但我几乎不明白如何使解决方案适应我的指标，该指标使用多个模型输入作为参数，因为update_state 方法似乎不太灵活。
在我的上下文中，除了编写自己的训练循环之外，是否有办法避免在训练期间计算我的指标？
另外，我很惊讶我们无法本机指定 Tensorflow 某些指标只能在验证时计算，这有什么原因吗？
此外，由于模型经过训练来优化损失，并且训练数据集不应用于评估模型，我什至不明白为什么默认情况下 Tensorflow 在训练期间计算指标。]]></description>
      <guid>https://stackoverflow.com/questions/71412499/how-to-prevent-keras-from-computing-metrics-during-training</guid>
      <pubDate>Wed, 09 Mar 2022 16:11:26 GMT</pubDate>
    </item>
    <item>
      <title>Keras 何时以及如何计算每批样本的指标？</title>
      <link>https://stackoverflow.com/questions/66311611/when-and-how-keras-calculate-metrics-for-each-batch-of-samples</link>
      <description><![CDATA[我看到 Keras 自定义指标如何工作，并且指标函数中的 tf.print 与 model.fit 的回调打印之间的计算不匹配。
导入张量流为 tf # tf2.4.1
将 numpy 导入为 np
模型 = tf.keras.models.Sequential(
    tf.keras.layers.Dense(1, input_shape=(1,))
）
def my_metric_fn(y_true, y_pred):
    squared_difference = tf.square(y_true - y_pred)
    损失 = tf.reduce_mean(squared_difference, axis=-1)
    tf.print(y_true.shape, y_pred.shape, 损失, tf.reduce_mean(squared_difference))
    回波损耗
model.compile（优化器=&#39;adam&#39;，损失=&#39;mean_squared_error&#39;，指标=[my_metric_fn]）
x = np.random.rand(4,1)
y = x ** 2
历史= model.fit（x = x，y = y，batch_size = 2，epochs = 2）
打印（历史.历史）

输出（格式化以提高可读性）
纪元 1/2
TensorShape([2, 1]) TensorShape([2, 1]) [9.79962078e-06 0.0534314588] 0.02672063
1/2 [==============&gt;........................] - ETA：0秒 - 损失：0.0267 - my_metric_fn：0.0267
TensorShape([2, 1]) TensorShape([2, 1]) [0.0397406667 0.179955378] 0.109848022
2/2 [================================] - 0s 7ms/步 - 损耗：0.0544 - my_metric_fn：0.0544

纪元2/2
TensorShape([2, 1]) TensorShape([2, 1]) [0.0392204635 0.0521505736] 0.0456855185
1/2 [==============&gt;........................] - ETA：0秒 - 损失：0.0457 - my_metric_fn：0.0457
TensorShape([2, 1]) TensorShape([2, 1]) [0.177408844 2.45939535e-08] 0.088704437
2/2 [================================] - 0s 5ms/步 - 损耗：0.0600 - my_metric_fn：0.0600
{&#39;损失&#39;：[0.06828432530164719，0.06719497591257095]，&#39;my_metric_fn&#39;：[0.06828432530164719，0.06719497591257095]}

在上面的输出中查看批次的打印损失。
纪元 1/2 1/2 tf.print：0.02672063，model.fit：0.0267。好的。
Epoch 1/2 2/2 tf.print：0.109848022，但 model.fit：0.0544。不行。
如何理解这些匹配和不匹配？ 0.0544 从哪里来？]]></description>
      <guid>https://stackoverflow.com/questions/66311611/when-and-how-keras-calculate-metrics-for-each-batch-of-samples</guid>
      <pubDate>Mon, 22 Feb 2021 07:31:49 GMT</pubDate>
    </item>
    <item>
      <title>使用 keras 计算每个时期的 Fscore（不是批量）</title>
      <link>https://stackoverflow.com/questions/61683829/calculating-fscore-for-each-epoch-using-keras-not-batch-wise</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/61683829/calculating-fscore-for-each-epoch-using-keras-not-batch-wise</guid>
      <pubDate>Fri, 08 May 2020 16:41:15 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：feature_names 不匹配：在 xgboost 中的 Predict() 函数中</title>
      <link>https://stackoverflow.com/questions/42338972/valueerror-feature-names-mismatch-in-xgboost-in-the-predict-function</link>
      <description><![CDATA[我训练了一个 XGBoostRegressor 模型。当我必须使用这个经过训练的模型来预测新输入时，predict() 函数会抛出 feature_names 不匹配错误，尽管输入特征向量与训练数据具有相同的结构。
此外，为了以与训练数据相同的结构构建特征向量，我做了很多低效的处理，例如添加新的空列（如果数据不存在），然后重新排列数据列，以便它与训练结构相匹配。是否有更好、更简洁的方式来格式化输入以使其与训练结构相匹配？]]></description>
      <guid>https://stackoverflow.com/questions/42338972/valueerror-feature-names-mismatch-in-xgboost-in-the-predict-function</guid>
      <pubDate>Mon, 20 Feb 2017 07:43:24 GMT</pubDate>
    </item>
    </channel>
</rss>