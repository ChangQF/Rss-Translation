<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 05 Apr 2024 18:16:42 GMT</lastBuildDate>
    <item>
      <title>如何提高基于 Keras 的 CNN 眼底图像分类的训练和测试准确性？</title>
      <link>https://stackoverflow.com/questions/78281500/how-can-i-improve-my-keras-based-cnns-training-and-testing-accuracy-for-fundus</link>
      <description><![CDATA[``我一直致力于使用包含 400 张图像的眼底图像集（来自 MESSIDOR）创建糖尿病视网膜病变分类模型，分级范围为 0-3（4 个类别）。我尝试过使用增强、dropout 和正则化器来更改模型架构、复杂性、输出特征，但我的模型训练和验证准确度似乎无法超过 50%。
这是我的代码：&#39;`
#将数据拆分为训练集和验证集X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, stratify=y)
#定义模型结构
&lt;前&gt;&lt;代码&gt;模型 = 顺序()

#转换层
model.add(Conv2D(64, kernel_size=(3, 3), activate=&#39;relu&#39;, input_shape=(250, 250, 3), kernel_regularizer=regularizers.l2(0.0001)))#输出滤波器为 32)
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (3, 3), 激活=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.001)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(256, (3, 3), 激活=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.001)))
model.add(MaxPooling2D(pool_size=(2, 2)))
模型.add(压平())
#密集层
model.add（密集（64，激活=&#39;relu&#39;））
模型.add(Dropout(0.45))
model.add（密集（4，激活=&#39;softmax&#39;））

`**# 编译模型
**`opt = keras.optimizers.Adam(learning_rate=0.00001)
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=opt, 指标=[&#39;accuracy&#39;])

打印（模型.摘要（））

`***# 数据增强
**`数据生成=图像数据生成器（
    旋转范围=30，
    宽度偏移范围=0.2，
    height_shift_range=0.2，
    水平翻转=真，
    fill_mode=&#39;最近&#39;
）

历史= model.fit（datagen.flow（X_train，y_train，batch_size = 2），epochs = 30，validation_data
（X_测试，y_测试），随机播放=真）

model.evaluate(X_test, y_test)



所附图像的准确性和丢失结果[在此处输入图像描述](https://i.stack.imgur.com/dgbal.png&lt; /a&gt;)]]></description>
      <guid>https://stackoverflow.com/questions/78281500/how-can-i-improve-my-keras-based-cnns-training-and-testing-accuracy-for-fundus</guid>
      <pubDate>Fri, 05 Apr 2024 17:25:53 GMT</pubDate>
    </item>
    <item>
      <title>如何解释神经网络中的分布式表示（隐藏神经元的输出）？</title>
      <link>https://stackoverflow.com/questions/78281488/how-to-interpret-distributed-representationsoutputs-of-the-hidden-neurons-in-a</link>
      <description><![CDATA[训练具有 1 个隐藏层（由 2 个神经元组成）的 FNN：
模型 = train1([2])
绘制每个隐藏神经元的拟合以及输出：
plot1(X1, y1, label=&quot;train&quot;)
图1（X1测试，y1测试，标签=“测试”）
plot1fit(torch.linspace(0, 13, 500).unsqueeze(1), 模型, 隐藏=True, 比例=False)

输出如下：

当使用 3 个隐藏神经元进行训练时：

如何解释图表和每个隐藏神经元的输出的拟合情况？将上述视为分布式表示/嵌入，它真的很直观吗？]]></description>
      <guid>https://stackoverflow.com/questions/78281488/how-to-interpret-distributed-representationsoutputs-of-the-hidden-neurons-in-a</guid>
      <pubDate>Fri, 05 Apr 2024 17:23:10 GMT</pubDate>
    </item>
    <item>
      <title>在神经网络中创建并加载数据集</title>
      <link>https://stackoverflow.com/questions/78281439/creating-and-loading-the-dataset-in-neural-network</link>
      <description><![CDATA[我是深度学习的初学者...我想做图像分类，我的文件夹中有很多图像...
将 numpy 导入为 np
从张量流导入keras
将 matplotlib.pyplot 导入为 plt
导入keras
数据=keras.datasets.fashion_mnist
(train_images,train_labels),(test_images,test_labels)=data.load_data()

火车图像=火车图像/255.0
测试图像=测试图像/255.0


plt.imshow(train_images[0],cmap=plt.cm.binary)
plt.show()

名称=[&#39;T恤&#39;，&#39;裤子&#39;，&#39;套头衫&#39;，&#39;连衣裙&#39;，&#39;外套&#39;，&#39;凉鞋&#39;，&#39;衬衫&#39;，&#39;运动鞋&#39;，&#39;包&#39;，&#39;踝靴&#39;]
模型=keras.Sequential([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(10,activation=“softmax”)
]）
model.compile(optimizer=“adam”,loss=“sparse_categorical_crossentropy”,metrics=[“accuracy”])

model.fit(train_images,train_labels,epochs=5)

预测=模型.预测(test_images)

对于范围 (4) 内的 i：
    plt.网格（假）
    plt.imshow(test_images[i],cmap=plt.cm.binary)
    plt.xlabel(“实际：”+names[test_labels[i]])
    plt.title(“预测：”+names[np.argmax(预测[i])])
    plt.show()


现在上面的程序基本上是从 MNIST 加载数据集...现在我创建了一个文件夹并将图像放入该文件夹中...现在如何加载该数据集并对其进行预处理？
附：我会给出类名。请有人帮助我。
我尝试了几种方法，但我没有得到它..请有人帮助我]]></description>
      <guid>https://stackoverflow.com/questions/78281439/creating-and-loading-the-dataset-in-neural-network</guid>
      <pubDate>Fri, 05 Apr 2024 17:10:33 GMT</pubDate>
    </item>
    <item>
      <title>实时提取 UNSW-NB15 数据集特征的最佳且最高效的方法是什么？</title>
      <link>https://stackoverflow.com/questions/78281420/what-is-the-best-and-most-performance-efficient-way-to-extract-the-features-of-u</link>
      <description><![CDATA[所以我一直在尝试找到一种方法，从网络数据包中实时提取 UNSW-NB15 数据集的特征，并将其提供给机器学习模型
我不确定哪种工具最适合此目的，我计划使用 zeek(bro)，但对于大多数功能，它需要脚本编写，我担心这会导致更多延迟并降低性能]]></description>
      <guid>https://stackoverflow.com/questions/78281420/what-is-the-best-and-most-performance-efficient-way-to-extract-the-features-of-u</guid>
      <pubDate>Fri, 05 Apr 2024 17:06:27 GMT</pubDate>
    </item>
    <item>
      <title>由于内存违规或花费太长时间而导致测试用例错误</title>
      <link>https://stackoverflow.com/questions/78281410/error-in-the-test-case-due-to-memory-violation-or-it-took-too-long</link>
      <description><![CDATA[我正在做一个程序，结果是正确的，但对于一个案例测试它不起作用，我不知道到底为什么。
原因是由于内存违规或输出结果花费的时间太长。
我的程序是关于机器学习的，我需要根据学生的行为，使用点之间的距离来查看是否获得批准。
#include ;
#include ;
#include ; // 使用malloc
#定义 MAX_SAMPLES 300000
#定义 MAX_STUDENTS 300000

类型定义结构{
    浮动学习时间；
    浮动平均成绩；
    int pass_or_fail;
    浮动距离；
} 样品；

类型定义结构{
    浮动学习时间；
    浮动平均成绩；
} 评估；

// 调整最大堆的函数
void heapify(样本sample_array[], int n, int i) {
    int 最大 = i; // 将最大的初始化为root
    int 左 = 2 * i + 1; // 左孩子的索引
    int 右 = 2 * i + 2; // 右子节点的索引

    // 如果左孩子大于根
    if (left &lt; n &amp;&amp; 样本数组[左].距离&gt; 样本数组[最大].距离)
        最大=左；

    // 如果右孩子大于迄今为止最大的孩子
    if (右 &lt; n &amp;&amp; 样本数组[右].距离 &gt; 样本数组[最大].距离)
        最大=右；

    // 如果最大的不是根
    如果（最大！=我）{
        // 将最大的与根交换
        样本温度=样本数组[i]；
        样本数组[i] = 样本数组[最大];
        样本数组[最大] = 临时；

        // 递归调整受影响的堆
        heapify(sample_array, n, 最大);
    }
}

// 堆排序的主要函数
void heapSort(样本sample_array[], int n) {
    // 构建最大堆
    for (int i = n / 2 - 1; i &gt;= 0; i--)
        heapify（样本数组，n，i）；

    // 从堆中逐个取出元素
    for (int i = n - 1; i &gt; 0; i--) {
        // 将当前根移动到末尾
        样本温度=样本数组[0]；
        样本数组[0] = 样本数组[i];
        样本数组[i] = 临时；

        // 在缩减堆上调用 max heapify
        heapify(sample_array, i, 0);
    }
}

int main() {
    int n_样本；
    int n_students_be_evaluated;
    整数 k；
    int pass_count；
    int 失败计数；

    scanf(“%d %d %d”, &amp;n_samples, &amp;n_students_to_be_evaluated, &amp;k);

   样本sample_array[MAX_SAMPLES]；
   评估的valued_array[MAX_STUDENTS]；

    // 读取样本
    for (int j = 0; j &lt; n_samples; j++) {
        scanf(“%f %f %d”, &amp;sample_array[j].average_grade, &amp;sample_array[j].study_hours, &amp;sample_array[j].pass_or_fail);
    }

    // 读取待评价的学生
    for (int i = 0; i &lt; n_students_to_be_evaluated; i++) {
        scanf(“%f %f”, &amp;evaluated_array[i].average_grade, &amp;evaluated_array[i].study_hours);
    }

    // 加工
    for (int i = 0; i &lt; n_students_to_be_evaluated; i++) {
        通行数=0；
        失败计数=0；

        // 计算距离并使用堆排序进行排序
        for (int j = 0; j &lt; n_samples; j++) {
            样本数组[j].距离 = sqrt(((evaluated_array[i].study_hours - 样本_array[j].study_hours)*(evaluated_array[i].study_hours - 样本_array[j].study_hours)) +
                                              ((evaluated_array[i].average_grade -sample_array[j].average_grade)*(evaluated_array[i].average_grade -sample_array[j].average_grade)));
        }

        heapSort(sample_array, n_samples);

        // 统计最近的k个中通过和失败的数量
        for (int g = 0; g &lt; k; g++) {
            if (sample_array[g].pass_or_fail == 1)
                通过计数++；
            别的
                失败计数++；
        }

        // 检查学生是否通过
        如果（通过计数&gt;失败计数）{
            printf(“学生%d：(%.2f，%.2f) = 通过\n”，i，evaluated_array[i].average_grade，evaluated_array[i].study_hours);
        } 别的 {
            printf(“学生%d：(%.2f，%.2f) = 失败\n”，i，evaluated_array[i].average_grade，evaluated_array[i].study_hours);
        }
    }


    返回0；
}


我尝试动态分配并使用合并排序。还尝试以静态形式增加数组的大小]]></description>
      <guid>https://stackoverflow.com/questions/78281410/error-in-the-test-case-due-to-memory-violation-or-it-took-too-long</guid>
      <pubDate>Fri, 05 Apr 2024 17:04:20 GMT</pubDate>
    </item>
    <item>
      <title>NeuralProphet 中的多变量预测</title>
      <link>https://stackoverflow.com/questions/78281016/multivariate-forecast-in-neuralprophet</link>
      <description><![CDATA[我正在尝试构建一个全局模型来同时预测两个时间序列。下面的代码运行没有错误。但预测数据帧的所有 NaN 都对应于两个 ID 之一（即有两个时间序列）。 yhat 值也全部为 NaN。我做错了什么或遗漏了什么吗？
m = NeuralProphet(
    yearly_seasonality=真，
    week_seasonality=真，
    daily_seasonality=假，
    分位数=分位数，
    n_lags=60,
    纪元=100，
    n_预测=30，
    loss_func=&#39;胡贝尔&#39;,
）
m.set_plotting_backend(&#39;绘图&#39;)
m.highlight_nth_step_ahead_of_each_forecast(step_number=10)

指标 = m.fit(train_df[[&#39;ds&#39;, &#39;y&#39;, &#39;ID&#39;]])

df_future = m.make_future_dataframe(
    火车_df，
    n_historic_predictions=真，
）

预测 = m.predict(df_future)
]]></description>
      <guid>https://stackoverflow.com/questions/78281016/multivariate-forecast-in-neuralprophet</guid>
      <pubDate>Fri, 05 Apr 2024 15:44:18 GMT</pubDate>
    </item>
    <item>
      <title>如何解释 DNN 中验证错误和测试错误之间的巨大差异</title>
      <link>https://stackoverflow.com/questions/78280101/how-can-i-explain-the-huge-difference-between-validation-and-test-errors-in-dnn</link>
      <description><![CDATA[我是 DNN 新手，并尝试了解它们如何在 cifar10 数据集上工作（不使用卷积层）。我使用两种不同的架构：
1.
def create_model(n_layers=5, n_neurons=100, shape=[32, 32, 3]):
    模型 = tf.keras.models.Sequential()

    model.add(tf.keras.layers.Flatten(input_shape=shape))
    对于 _ 在范围内（n_layers）：
        model.add(tf.keras.layers.Dense(n_neurons,
                                 激活＝“selu”，
                                 kernel_initializer=“lecun_normal”））
    model.add（tf.keras.layers.Dense（10，激活=“softmax”））

    优化器 = tf.keras.optimizers.Nadam()
    
    model.compile(loss=“sparse_categorical_crossentropy”,
              优化器=优化器，
              指标=[“准确度”])
    返回模型




model_bn = tf.keras.models.Sequential()

   model_bn.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))
   model_bn.add(tf.keras.layers.BatchNormalization())

   对于范围（5）内的 _：
       model_bn.add(tf.keras.layers.Dense(100,
                                 kernel_initializer=“he_normal”,
                                 kernel_constraint=tf.keras.constraints.max_norm(1.)))
       model_bn.add(tf.keras.layers.BatchNormalization())
       model_bn.add(tf.keras.layers.Activation(“elu”))
   model_bn.add(tf.keras.layers.Dense(10,激活=“softmax”))

   优化器 = tf.keras.optimizers.Nadam(learning_rate=1e-3)
   model_bn.compile(loss=“sparse_categorical_crossentropy”,
              优化器=优化器，
              指标=[“准确度”])

这些模型使用相同的回调来防止过度拟合和长时间训练：
early_stopping_cb = tf.keras.callbacks.EarlyStopping（耐心=6，监视器=“val_loss”）
   lr_scheduler_cb = tf.keras.callbacks.ReduceLROnPlateau（因子=0.5，耐心=3）
   checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(“my_cifar10_model_v1.keras”, save_best_only=True)
   回调 = [early_stopping_cb、checkpoint_cb、tensorboard_cb、lr_scheduler_cb]

但是在拟合结束时，我得到了训练和验证误差之间的巨大差异（大约 10-18%），我只能通过过度拟合来解释这一点，但是 checkpoint_cb 认为这是最好的模型，无论事实如何过度拟合。我是否坚持这些结果，或者我需要获得训练和验证精度差异较小的模型，从而降低最终验证精度。如果是这样，当训练准确性提高时，是否有回调停止，而验证错误保持大致相同？
我尝试设置不同的学习率和耐心参数，但似乎没有任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78280101/how-can-i-explain-the-huge-difference-between-validation-and-test-errors-in-dnn</guid>
      <pubDate>Fri, 05 Apr 2024 13:06:20 GMT</pubDate>
    </item>
    <item>
      <title>我需要帮助：聚类 JSON 数据以进行事件分析</title>
      <link>https://stackoverflow.com/questions/78280095/i-needed-help-clustering-json-data-for-event-analysis</link>
      <description><![CDATA[我目前正在从事一个涉及对 JSON 数据进行聚类以进行事件分析的项目，我面临着一些挑战。我有 JSON 数据，其中包含具有不同属性（例如用户 ID、时间戳和事件类型）的各种事件。
我需要根据事件的属性对这些事件进行聚类，以识别模式和见解。具体来说，我想对登录、注销、购买等事件进行聚类，并且每种事件类型都需要选择不同的属性进行聚类。
例如，对于登录事件，我需要考虑用户 ID 和登录时间，而对于购买事件，我可能需要包含产品 ID 和购买金额。
我不确定如何有效地完成此任务以及哪种机器学习算法最适合此场景。此外，我不确定如何预处理数据并为每种事件类型选择正确的功能。
这是我的 JSON 数据结构的简化版本：
{
“id”：“1”，
“年龄组”：“120”，
“client_id”：“1234567890123”，
“设备”：“788”，
“金额”：2000，
“受益人账户”：“12345678901234567890”，
“原因”：“测试原因”，
“发送日期”：1710457200000，
“交易类型”：“U”，
“状态”：“已解决”，
“状态代码”：3，
“beneficiary_name”：“测试受益人”，
“附加详细信息”：[]
}
有人可以提供有关如何预处理数据、为每种事件类型选择正确的特征以及选择适当的机器学习算法进行聚类的指导吗？
任何见解、建议或代码示例将不胜感激。预先感谢您的帮助！
我想要对登录、注销、购买等事件进行聚类，每种事件类型都需要选择不同的属性进行聚类]]></description>
      <guid>https://stackoverflow.com/questions/78280095/i-needed-help-clustering-json-data-for-event-analysis</guid>
      <pubDate>Fri, 05 Apr 2024 13:05:16 GMT</pubDate>
    </item>
    <item>
      <title>使用 RNN 未能提高情感项目的准确性</title>
      <link>https://stackoverflow.com/questions/78279557/failure-to-improve-accuracy-in-the-sentiment-project-with-rnn</link>
      <description><![CDATA[希望你一切都好。我正在开发一个名为情感分析的人工智能项目，该项目适用于波斯语数据集。
我一直致力于加载数据，将它们转换为嵌入，然后将它们输入到由 LSTM 组成的神经网络中。然而，在训练过程中，第 1 轮之后准确率并没有提高，并且陷入了困境。
我的网络代码部分是：
https://colab.research.google.com/drive/1Pz20d5r1iZPLvWjHKC2oOfNsNXY1R- TC?usp=共享
纪元[1/20]，损失：1.1605366468429565，准确率：23.5%
Epoch [2/20]，损失：1.0860859155654907，准确率：37.1%
Epoch [3/20]，损失：1.0465837717056274，准确率：48.0%
Epoch [4/20]，损失：1.02091646194458，准确率：51.2%
Epoch [5/20]，损失：1.003448486328125，准确度：52.300000000000004%
Epoch [6/20]，损失：0.9991855621337891，准确率：53.6%
Epoch [7/20]，损失：0.9968012571334839，准确率：52.800000000000004%
Epoch [8/20]，损失：0.9954250454902649，准确率：52.900000000000006%
Epoch [9/20]，损失：0.9897969365119934，准确率：53.1%
Epoch [10/20]，损失：0.9899587631225586，准确率：53.7%
Epoch [11/20]，损失：0.992097020149231，准确率：53.0%
Epoch [12/20]，损失：0.9817440509796143，准确率：53.400000000000006%
]]></description>
      <guid>https://stackoverflow.com/questions/78279557/failure-to-improve-accuracy-in-the-sentiment-project-with-rnn</guid>
      <pubDate>Fri, 05 Apr 2024 11:19:48 GMT</pubDate>
    </item>
    <item>
      <title>可能是什么引发了错误：ValueError：X 有 23 个特征，但 SVR 期望 24 个特征作为输入？</title>
      <link>https://stackoverflow.com/questions/78275238/what-may-be-raising-the-error-valueerror-x-has-23-features-but-svr-is-expecti</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78275238/what-may-be-raising-the-error-valueerror-x-has-23-features-but-svr-is-expecti</guid>
      <pubDate>Thu, 04 Apr 2024 16:21:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 MAPIE 进行保形预测，当 alpha 很大时，我得到空的预测集</title>
      <link>https://stackoverflow.com/questions/78240714/using-mapie-for-conformal-predictions-i-get-empty-predictions-sets-when-alpha-is</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78240714/using-mapie-for-conformal-predictions-i-get-empty-predictions-sets-when-alpha-is</guid>
      <pubDate>Thu, 28 Mar 2024 20:28:12 GMT</pubDate>
    </item>
    <item>
      <title>解决从 Jupyter Notebook 到 .py 文件的自定义管道类转换中的 OneHotEncoder 问题</title>
      <link>https://stackoverflow.com/questions/78219825/troubleshooting-onehotencoder-issue-in-custom-pipeline-class-conversion-from-jup</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78219825/troubleshooting-onehotencoder-issue-in-custom-pipeline-class-conversion-from-jup</guid>
      <pubDate>Mon, 25 Mar 2024 14:32:03 GMT</pubDate>
    </item>
    <item>
      <title>如何将极坐标数据框与 scikit-learn 一起使用？</title>
      <link>https://stackoverflow.com/questions/74398563/how-to-use-polars-dataframes-with-scikit-learn</link>
      <description><![CDATA[我无法将极坐标数据帧与 scikitlearn 一起使用进行机器学习训练。
目前，我正在极坐标中进行所有数据帧预处理，在模型训练期间，我将其转换为 pandas 数据帧以使其正常工作。
是否有任何方法可以直接使用 Polars 数据帧进行 ML 训练而不将其更改为 pandas？]]></description>
      <guid>https://stackoverflow.com/questions/74398563/how-to-use-polars-dataframes-with-scikit-learn</guid>
      <pubDate>Fri, 11 Nov 2022 05:59:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 Conda + Poetry 有意义吗？</title>
      <link>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</link>
      <description><![CDATA[在机器学习项目中使用 Conda + Poetry 有意义吗？让我分享一下我（新手）的理解，请指正或赐教：
据我了解，Conda 和 Poetry 有不同的目的，但很大程度上是多余的：

Conda 主要是一个环境管理器（实际上不一定是 Python），但它也可以管理包和依赖项。
Poetry 主要是一个 Python 包管理器（例如，pip 的升级版），但它也可以创建和管理 Python 环境（例如，Pyenv 的升级版） .

我的想法是同时使用两者并划分它们的角色：让 Conda 担任环境管理器，让 Poetry 担任包管理器。我的推理是（听起来）Conda 最适合管理环境，可用于编译和安装非 python 包，尤其是 CUDA 驱动程序（用于 GPU 功能），而 Poetry 作为 Python 包管理器比 Conda 更强大。 
通过在 Conda 环境中使用 Poetry，我成功地相当轻松地完成了这项工作。诀窍是不使用 Poetry 来管理 Python 环境：我没有使用诸如 poetry shell 或 poetry run 这样的命令，只使用 poetry init 、poetry install 等（激活Conda环境后）。
为了充分披露，我的 environment.yml 文件（针对 Conda）如下所示：
&lt;前&gt;&lt;代码&gt;名称：N

渠道：
  - 默认值
  - 康达锻造

依赖项：
  - 蟒蛇=3.9
  -cuda工具包
  - 库德恩

我的poetry.toml文件看起来像这样：
&lt;前&gt;&lt;代码&gt;[工具.诗歌]
名称=“N”
作者 = [“B”]

[工具.诗歌.依赖项]
蟒蛇=“3.9”
火炬 =“^1.10.1”

[构建系统]
需要= [“诗歌核心&gt;=1.0.0”]
构建后端=“poetry.core.masonry.api”

说实话，我这样做的原因之一是我在没有 Conda 的情况下很难安装 CUDA（用于 GPU 支持）。
您认为这个项目设计合理吗？]]></description>
      <guid>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</guid>
      <pubDate>Tue, 25 Jan 2022 15:09:43 GMT</pubDate>
    </item>
    <item>
      <title>Keras 损失：0.0000e+00 并且精度保持不变</title>
      <link>https://stackoverflow.com/questions/70589997/keras-loss-0-0000e00-and-accuracy-stays-constant</link>
      <description><![CDATA[我有 101 个文件夹（从 0 到 100），其中包含合成训练图像。
这是我的代码：
数据集 = tf.keras.utils.image_dataset_from_directory(
&#39;图片/synthdataset5&#39;，labels=&#39;推断&#39;，label_mode=&#39;int&#39;，class_names=None，color_mode=&#39;rgb&#39;，batch_size=32，image_size=(128,128)，shuffle=True，seed=None，validation_split=None，子集=无，插值=&#39;双线性&#39;，follow_links=False，crop_to_aspect_ratio=False
）

从 keras.models 导入顺序
从 keras.layers 导入 Dense、Conv2D、Flatten

模型=顺序（）

model.add(Conv2D(32, kernel_size=5, 激活=&#39;relu&#39;, input_shape=(128,128,3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=5, 激活=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128，kernel_size=3，激活=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(256，kernel_size=3，激活=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
模型.add(压平())
model.add（密集（1，激活=&#39;sigmoid&#39;））

model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

model.fit(数据集,epochs=75)

每个时期我总是得到相同的结果：
&lt;前&gt;&lt;代码&gt;纪元 1/75
469/469 [================================] - 632s 1s/步 - 损耗：0.0000e+00 - 精度： 0.0098

怎么了？？？]]></description>
      <guid>https://stackoverflow.com/questions/70589997/keras-loss-0-0000e00-and-accuracy-stays-constant</guid>
      <pubDate>Wed, 05 Jan 2022 08:46:51 GMT</pubDate>
    </item>
    </channel>
</rss>