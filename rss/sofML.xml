<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 28 Jun 2024 09:16:04 GMT</lastBuildDate>
    <item>
      <title>MATLAB 中“trainbr”训练函数的 Python 等效项是什么？</title>
      <link>https://stackoverflow.com/questions/78680950/what-is-the-python-equivalent-of-trainbr-training-function-in-matlab</link>
      <description><![CDATA[我正在尝试将以下用 MATLAB 编写的 ANN 模型代码转换为 Python。我只是想知道如何将 trainbr（贝叶斯正则化反向传播）算法转换为 Python。
net=newff(INPTRN,TARTRN,hidden,{&#39;logsig&#39;,&#39;purelin&#39;},&#39;trainbr&#39;);
net.divideFcn=&#39;&#39;;
net.performFcn=&#39;msereg&#39;;
net.trainParam.show=10;
net.trainParam.epochs=50000;
net.trainParam.goal=0.0001;
rand(&#39;state&#39;,0);
net=init(net);
]]></description>
      <guid>https://stackoverflow.com/questions/78680950/what-is-the-python-equivalent-of-trainbr-training-function-in-matlab</guid>
      <pubDate>Fri, 28 Jun 2024 06:34:07 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 逆向工程 - 根据 Y 推荐 X 值</title>
      <link>https://stackoverflow.com/questions/78680915/reverse-engineering-of-lightgbm-recommending-x-values-based-on-y</link>
      <description><![CDATA[我正在尝试构建一个 LightGBM 回归模型，其中我有大约 15-20 个输入特征，而我的目标变量在 20-40 的范围内。
我使用了 SHAP 蜂群图来了解每个特征的重要性（由于数据的敏感性，特征名称被隐藏）

现在，用户希望我基于此 LightGBM 模型创建一个优化模型，其中将输入 Y 变量的预期范围，并且模型将返回每个输入变量的理想范围（X，15-20 个特征）以实现相同的目标。问题更像是输入一个 Y 变量，然后返回每个 X 变量的范围。
然而，这里的挑战是，需要 15-20 个特征的组合才能达到 Y 变量的预期范围，因此模型推荐也需要注意这一点。
是否有任何 scikit-learn 库可用于解决这个问题？或者在这种情况下我如何实现解决方案？
我遇到过许多类似的问题，但没有一个是我想要的。]]></description>
      <guid>https://stackoverflow.com/questions/78680915/reverse-engineering-of-lightgbm-recommending-x-values-based-on-y</guid>
      <pubDate>Fri, 28 Jun 2024 06:21:25 GMT</pubDate>
    </item>
    <item>
      <title>如何查看 YOLOV6 中的评估指标？</title>
      <link>https://stackoverflow.com/questions/78680846/how-can-i-see-evaluation-metrics-in-yolov6</link>
      <description><![CDATA[我有这种输出，但我无法弄清楚如何评估，因为没有 F1 分数或 conf 矩阵。

平均召回率 (AR) @[ IoU=0.50:0.95 | 面积= 小 |maxDets=100] = -1.000


平均召回率 (AR) @[ IoU=0.50:0.95 | 面积= 中 | maxDets=100 ] = 0.250


平均召回率 (AR) @[ IoU=0.50:0.95 | 面积= 大 | maxDets=100 ] = 0.410


20/499 0.001595 0.6697 0 1.393: 100%|██████████| 12/12 [00:


21/499 0.001594 0.6417 0 1.353: 100%|██████████| 12/12 [00:


22/499 0.001594 0.6727 0 1.431: 100%|██████████| 12/12 [00:

我也看不到 mAP。这不是完整输出。我训练了 400 个 epoch，所以这只是其中的一小部分。
有没有办法获得评估指标？]]></description>
      <guid>https://stackoverflow.com/questions/78680846/how-can-i-see-evaluation-metrics-in-yolov6</guid>
      <pubDate>Fri, 28 Jun 2024 05:55:12 GMT</pubDate>
    </item>
    <item>
      <title>预期频率的计算和卡方实现</title>
      <link>https://stackoverflow.com/questions/78680802/calculation-of-expected-frequency-and-chi-square-implementation</link>
      <description><![CDATA[我的数据在一个名为“Data.csv”的 CSV 文件中，具体关注“空气温度”列。我尝试使用两个分布来拟合数据：“gamma”和“对数正态”。现在，我不确定我是否正确计算了预期频率，以及我对卡方检验的实施是否准确。
以下是我面临的具体问题：
预期频率计算：在将“gamma”和“对数正态”分布拟合到我的数据时，我需要澄清计算卡方检验预期频率的正确方法。
卡方检验实施：我不确定我对卡方检验的实施是否正确。我将非常感激有关正确步骤和任何必要更正的指导。
如果可能的话，有人可以提供计算预期频率的正确方法以及针对我的情况正确实施卡方检验的方法吗？
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from scipy.stats import gamma, lognorm, kstest, chisquare, chi2_contingency
import warnings

warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning)

df = pd.read_csv(f&#39;Data.csv&#39;)

# 提取数据
temperature = df[&#39;Air_Temperature(AT)&#39;]

def cs(n, y):
return chisquare(n, np.sum(n) / np.sum(y) * y)

def explain_goodness_of_fit(chi2_p, ks_p, alpha=0.05):
if chi2_p &gt; alpha or ks_p &gt; alpha:
return &quot;良好拟合&quot;
else:
return &quot;不合适&quot;

# 计算箱大小
number_of_bins = int(1 + np.log2(len(temperature)))
print(&quot;箱数：&quot;, number_of_bins)

# 拟合伽马分布
gamma_params = gamma.fit(temperature)
print(&quot;伽马分布参数：&quot;, gamma_params)

# 拟合对数正态分布
lognorm_params = lognorm.fit(temperature)
print(&quot;对数正态分布参数：&quot;, lognorm_params)
print(&quot;---------------&quot;)

# 生成拟合的伽马分布
x = np.linspace(min(temperature), max(temperature), 100)
gamma_pdf_fitted = gamma.pdf(x, *gamma_params)

# 生成拟合的对数正态分布
lognorm_pdf_fitted = lognorm.pdf(x, *lognorm_params)

# 使用 KDE 和拟合分布绘制直方图
plt.figure(figsize=(10, 6))
sns.histplot(df, x=&quot;Air_Temperature(AT)&quot;, bins=number_of_bins, kde=True, stat=&#39;density&#39;, label=&#39;Data with KDE&#39;)
plt.plot(x, gamma_pdf_fitted, &#39;r-&#39;, label=&#39;Fitted Gamma Distribution&#39;)
plt.plot(x, lognorm_pdf_fitted, &#39;g-&#39;, label=&#39;Fitted Log-normal Distribution&#39;)
plt.xlabel(&#39;Air Temperature&#39;)
plt.ylabel(&#39;Density&#39;)
plt.title(f&quot;Density Plot&quot;)
plt.legend()
plt.show()

# 卡方检验
observed_freq, bins = np.histogram(temperature, bins=number_of_bins, density=False)

expected_freq_gamma = len(temperature) * gamma.cdf(bins[1:], *gamma_params) - len(temperature) * gamma.cdf(bins[:-1], *gamma_params)

expected_freq_lognorm = len(temperature) * lognorm.cdf(bins[1:], *lognorm_params) - len(temperature) * lognorm.cdf(bins[:-1], *lognorm_params)

result_goodness_of_fit_gamma = cs(observed_freq, expected_freq_gamma)
result_goodness_of_fit_log_normal = cs(observed_freq, expected_freq_lognorm)
print(f&quot;结果优度Gamma 拟合优度结果为 {result_goodness_of_fit_gamma}&quot;)
print(f&quot;对数正态拟合优度结果为 {result_goodness_of_fit_log_normal}&quot;)
print(&quot;---------------&quot;)

# KS-Test
ks_stat_gamma, ks_p_gamma = kstest(temperature, &#39;gamma&#39;, args=gamma_params)
ks_stat_lognorm, ks_p_lognorm = kstest(temperature, &#39;lognorm&#39;, args=lognorm_params)

print(f&quot;Gamma KS 统计量：{ks_stat_gamma}, p 值：{ks_p_gamma}&quot;)
print(f&quot;对数正态 KS 统计量：{ks_stat_lognorm}, p 值： {ks_p_lognorm}&quot;)
print(&quot;------------&quot;)

chi2_stat_gamma, chi2_p_gamma = cs(observed_freq, expected_freq_gamma)
chi2_stat_lognorm, chi2_p_lognorm = cs(observed_freq, expected_freq_lognorm)

gamma_fit = explain_goodness_of_fit(chi2_p_gamma, ks_p_gamma)
lognorm_fit = explain_goodness_of_fit(chi2_p_lognorm, ks_p_lognorm)

print(f&quot;Gamma 分布拟合优度：{gamma_fit}&quot;)
print(f&quot;对数正态分布拟合优度：{lognorm_fit}&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78680802/calculation-of-expected-frequency-and-chi-square-implementation</guid>
      <pubDate>Fri, 28 Jun 2024 05:36:04 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft Fabric 数据科学 - 如何使用应用模型向导在 Delta 表中保存概率和预测？</title>
      <link>https://stackoverflow.com/questions/78680642/microsoft-fabric-data-science-how-to-save-probabilities-along-with-predictions</link>
      <description><![CDATA[我对 Fabric 和 ML 还不是很熟悉。我曾为二元分类任务创建了一个逻辑回归模型。该模型在预测方面表现良好，我得到了我想要的结果。但随着业务需求的变化，我也想获得每个类别的概率。我使用 predict_proba 方法来获取 2 个类别的概率。这适用于测试数据。我得到了预测及其概率。我将实验保存为 ML 模型（使用 ML 向导中的应用此模型）并按照以下步骤操作：

选择用于评分的源数据
将数据正确映射到我的 ML 模型的输入
指定我的模型输出的目标
创建一个使用 PREDICT 生成预测结果并将其作为增量表存储到 Lakehouse 的笔记本

但是，我只在我的增量表中获得了预测，而没有概率。有没有办法我也可以获得概率？
PS：我按照此链接上的说明进行操作：链接
我注意到的另一件事是在实验和模型的输出模式上，数据类型是 int 32，这对于预测来说是正确的，但对于概率来说应该是大小为 [-1,2] 的数组。
为了以防万一，我还将链接附加到我的笔记本上：笔记本。

谢谢，]]></description>
      <guid>https://stackoverflow.com/questions/78680642/microsoft-fabric-data-science-how-to-save-probabilities-along-with-predictions</guid>
      <pubDate>Fri, 28 Jun 2024 04:22:59 GMT</pubDate>
    </item>
    <item>
      <title>在 Python 中自动选择感兴趣的区域[关闭]</title>
      <link>https://stackoverflow.com/questions/78679950/automatically-select-the-region-of-interest-in-python</link>
      <description><![CDATA[我正在从事一个光学项目，我需要根据一组照片提取感兴趣区域 (ROI)，该组照片由 32 张旋转正方形的照片组成。我找到了一种通过计算整组照片的方差来找到感兴趣区域的方法，如下所示

但现在我需要一种方法来实际提取它并仅针对该区域计算该区域中像素的平均强度。如果有一种方法可以告诉 Python“只获取由这条较暗的线界定的区域内像素的所有值”那就太好了，但我一直找不到如何做到这一点。我发现了一些使用 OpenCV 的实现，但它需要用户交互，我最想避免这部分。还请注意，这不是一个完美的圆形，所以我可以毫无问题地使用较小的圆形区域 - 由程序自动选择 - 但如果也有办法避免这种情况，那就太好了。
你有什么建议吗？文档、文章、示例、一些库、一些我自己实现的算法、一些机器学习方法/算法（这会非常好）来识别和提取区域？根据您的经验，可以完成这项工作的东西。]]></description>
      <guid>https://stackoverflow.com/questions/78679950/automatically-select-the-region-of-interest-in-python</guid>
      <pubDate>Thu, 27 Jun 2024 21:58:28 GMT</pubDate>
    </item>
    <item>
      <title>如何在本地对自定义数据运行 llama3</title>
      <link>https://stackoverflow.com/questions/78679786/how-to-run-llama3-on-custom-data-locally</link>
      <description><![CDATA[我已经使用 Ollama 在我的 PC 上本地设置了 llama3，我有一个包含 aet if 定律的文件，我希望 llama 读取该文件，以便根据其中的定律回答问题。关于如何做到这一点，有什么想法吗？？？]]></description>
      <guid>https://stackoverflow.com/questions/78679786/how-to-run-llama3-on-custom-data-locally</guid>
      <pubDate>Thu, 27 Jun 2024 20:58:36 GMT</pubDate>
    </item>
    <item>
      <title>制作 ML 模型：函数或类 [关闭]</title>
      <link>https://stackoverflow.com/questions/78679773/making-ml-models-functions-or-classes</link>
      <description><![CDATA[我正在构建一个模型，我想获得一些反馈。将模型创建为函数（例如 def model_1）还是类（class Model1:）更好？一种方法是否优于另一种方法？
我创建了几个模型，但它们都是类而不是函数]]></description>
      <guid>https://stackoverflow.com/questions/78679773/making-ml-models-functions-or-classes</guid>
      <pubDate>Thu, 27 Jun 2024 20:54:12 GMT</pubDate>
    </item>
    <item>
      <title>Oracle 机器学习（OML）df_datetime 给出“未选择任何列”错误</title>
      <link>https://stackoverflow.com/questions/78678402/oracle-machine-learning-oml-df-datetime-gives-no-columns-are-selected-error</link>
      <description><![CDATA[如果有人能帮忙，我遇到了一些编码问题！我试图从 oml.Dataframe df 中获取 Datetime 类型。我试过这个代码：
 df = oml.sync(query=QUERY)
df_datetime = df.select_types(include=[&#39;oml.Datetime&#39;])

但我收到一个错误，提示没有选择任何列。我是否错误地使用了此功能？
我找到了一种解决方法
 df = oml.sync(query=QUERY)
df_datetime = []
for col, dtype in df.dtypes.items():
if dtype.__name__ == &#39;Datetime&#39;:
df_datetime.append(col)

这确实返回了 Datetime 对象，所以我知道它们存在。如果可以的话，我更愿意使用 select_types 方法，如果有人能向我解释我做错了什么。]]></description>
      <guid>https://stackoverflow.com/questions/78678402/oracle-machine-learning-oml-df-datetime-gives-no-columns-are-selected-error</guid>
      <pubDate>Thu, 27 Jun 2024 14:52:10 GMT</pubDate>
    </item>
    <item>
      <title>无法运行 xgboost 导入 XGBRegressor [关闭]</title>
      <link>https://stackoverflow.com/questions/78672076/unable-to-run-xgboost-import-xgbregressor</link>
      <description><![CDATA[当我运行以下代码时，
from xgboost import XGBRegressor

我收到错误：
&gt; --------------------------------------------------------------------------- XGBoostError Traceback (most recent call
&gt; last) Cell In[64], line 19
&gt; 17 from sklearn.metrics import mean_squared_error
&gt; 18 from sklearn import metrics
&gt; ---&gt; 19 from xgboost import XGBRegressor File /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/__init__.py:6
&gt; 1 &quot;&quot;&quot;XGBoost: eXtreme Gradient Boosting library.
&gt; 2 
&gt; 3 贡献者：https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md
&gt; 4 &quot;&quot;&quot;
&gt; ----&gt; 6 来自 . import tracker # noqa
&gt; 7 来自 . import collective, dask
&gt; 8 来自 .core import (
&gt; 9 Booster,

我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78672076/unable-to-run-xgboost-import-xgbregressor</guid>
      <pubDate>Wed, 26 Jun 2024 11:17:02 GMT</pubDate>
    </item>
    <item>
      <title>如何在使用 colsample_bytree 超参数时始终保留 XGBoost 中的某个特征</title>
      <link>https://stackoverflow.com/questions/78665326/how-to-always-keep-a-feature-in-xgboost-while-using-colsample-bytree-hyperparame</link>
      <description><![CDATA[我使用 XGBoost 来训练 ML 模型。如果我使用 colsample_bytree 超参数，XGBoost 会平等对待所有特征。我需要始终包含一个二进制特征，因为该特征决定了观察结果属于哪个子样本。其他特征应在训练期间随机包含，由 colsample_bytree 超参数决定。如何做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/78665326/how-to-always-keep-a-feature-in-xgboost-while-using-colsample-bytree-hyperparame</guid>
      <pubDate>Tue, 25 Jun 2024 03:52:51 GMT</pubDate>
    </item>
    <item>
      <title>如何将 JSON 中的标记坐标叠加到 JPG 图像中以进行 CNN 训练？[关闭]</title>
      <link>https://stackoverflow.com/questions/78661583/how-to-overlay-labeled-coordinates-from-json-into-jpg-images-for-cnn-training</link>
      <description><![CDATA[我正在开展一个计算机视觉项目，该项目涉及检测和分割 MRI 扫描中的骨折。作为该项目的一部分，我让专家直接在图像上标记骨折区域。此过程会生成一个 JSON 文件，其中包含以下信息：

标记区域的坐标
标记区域的名称
标记图像的名称

我面临的挑战是将这些坐标从 JSON 文件转移到相应的 JPG 图像上，以准备进行 CNN 训练。
以下是我的 JSON 文件结构示例：
&quot;item&quot;: {
&quot;name&quot;: &quot;img-00003-00082.jpg&quot;,
&quot;team&quot;: {
&quot;name&quot;: &quot;Mask&quot;,
&quot;slug&quot;: &quot;mask&quot;
&quot;file_name&quot;: &quot;img-00003-00082.jpg&quot;,
&quot;annotations&quot;: [
{
&quot;bounding_box&quot;: {
&quot;h&quot;: 142.16649999999993,
&quot;w&quot;: 124.14549999999997,
&quot;x&quot;: 679.8006,
&quot;y&quot;: 425.7789
},
&quot;name&quot;: &quot;Broken&quot;,
&quot;polygon&quot;: {
&quot;paths&quot;: [
[
{
&quot;x&quot;: 695.1519,
&quot;y&quot;: 567.9454
},
{
&quot;x&quot;: 679.8006,
&quot;y&quot;: 530.5683
},


到目前为止，我已经设法从 JSON 文件中提取了必要的坐标。但是，我很难将这些坐标叠加到 JPG 图像上以生成 CNN 的训练数据。
我的问题：

如何准确地将 JSON 文件中的坐标叠加到相应的 JPG 图像上？
是否有任何推荐的 Python 库或方法专门适合此任务？
]]></description>
      <guid>https://stackoverflow.com/questions/78661583/how-to-overlay-labeled-coordinates-from-json-into-jpg-images-for-cnn-training</guid>
      <pubDate>Mon, 24 Jun 2024 09:29:08 GMT</pubDate>
    </item>
    <item>
      <title>如何修复 Huggingface 训练器的学习率？</title>
      <link>https://stackoverflow.com/questions/77792137/how-to-fix-the-learning-rate-for-huggingface%c2%b4s-trainer</link>
      <description><![CDATA[我正在使用以下参数训练模型：
Seq2SeqTrainingArguments(
output_dir = &quot;./out&quot;, 
overwrite_output_dir = True,
do_train = True,
do_eval = True,

per_device_train_batch_size = 2, 
gradient_accumulation_steps = 4,
per_device_eval_batch_size = 8, 

learning_rate = 1.25e-5,
warmup_steps = 1,

save_total_limit = 1,

evaluation_strategy = &quot;epoch&quot;,
save_strategy = &quot;epoch&quot;,
logs_strategy = &quot;epoch&quot;, 
num_train_epochs = 5, 

gradient_checkpointing = True,
fp16 = True, 

predict_with_generate = True,
generation_max_length = 225,

report_to = [&quot;tensorboard&quot;],
load_best_model_at_end = True,
metric_for_best_model = &quot;wer&quot;,
greater_is_better = False,
push_to_hub = False,
)

我假设 warmup_steps=1 固定了学习率。
但是，训练结束后，我查看文件 trainer_state.json，发现学习率似乎没有固定。
以下是 learning_rate 和 step 的值：
learning_rate，steps
1.0006 e-05 1033
7.5062 e-06 2066
5.0058 e-06 3099
2.5053 e-06 4132
7.2618 e-09 5165

学习率似乎没有固定在 1.25e-5（步骤 1 之后）。我遗漏了什么？如何修复学习率。]]></description>
      <guid>https://stackoverflow.com/questions/77792137/how-to-fix-the-learning-rate-for-huggingface%c2%b4s-trainer</guid>
      <pubDate>Wed, 10 Jan 2024 09:14:26 GMT</pubDate>
    </item>
    <item>
      <title>计算“torch.tensor”中条目之间的成对距离</title>
      <link>https://stackoverflow.com/questions/75309052/calculating-pairwise-distances-between-entries-in-a-torch-tensor</link>
      <description><![CDATA[我正在尝试实现流形对齐类型的损失，如此处所示。
给定一个表示一批形状为 (L,N) 的嵌入的张量，例如 L=256：
tensor([[ 0.0178, 0.0004, -0.0217, ..., -0.0724, 0.0698, -0.0180],
[ 0.0160, 0.0002, -0.0217, ..., -0.0725, 0.0655, -0.0207],
[ 0.0155, -0.0010, -0.0153, ..., -0.0750, 0.0688, -0.0253],
...,
[ 0.0130, -0.0113, -0.0078, ..., -0.0805, 0.0634, -0.0241],
[ 0.0120, -0.0047, -0.0135, ..., -0.0846, 0.0722, -0.0230],
[ 0.0120, -0.0048, -0.0142, ..., -0.0843, 0.0734, -0.0246]],
grad_fn=&lt;AddmmBackward0&gt;)

我想计算所有成对距离行条目。产生 (L, L) 形状的输出。
我尝试使用 torch.nn.PairwiseDistance，但我不清楚它是否有用，是否符合我的要求。]]></description>
      <guid>https://stackoverflow.com/questions/75309052/calculating-pairwise-distances-between-entries-in-a-torch-tensor</guid>
      <pubDate>Wed, 01 Feb 2023 10:47:38 GMT</pubDate>
    </item>
    <item>
      <title>初始化权重后，scikit 学习分类器的准确率降低</title>
      <link>https://stackoverflow.com/questions/41804937/decreasing-accuracy-of-scikit-learn-classifier-after-initializing-weight</link>
      <description><![CDATA[我想基于 sklearn 分类器实现 adaboost 分类器，在算法分类器的第一步中我应该将权重初始化为&quot; 1 / # 训练数据&quot;
但这会降低分类器的准确率，我不知道为什么？ （我为所有数据点设置了相同的权重）
我的代码：
svm_weight = SVC()
svm_non_weight = SVC()

w = np.ones(len(target_train))
w.fill(float(1)/float(len(target_train)))
svm_weight.fit(data_train_feature_scaled_pca,
target_train,
sample_weight= w)

svm_non_weight.fit(data_train_feature_scaled_pca,
target_train)

print &quot;score weight : &quot;,svm_weight.score(data_test_feature_scaled_pca,target_train)

print &quot;score non weight : &quot;,svm_non_weight.score(data_test_feature_scaled_pca,target_train)

输出：
得分权重：0.503592561285
得分非权重：0.729289940828

实现的 adaboost：
类 adaboost_classifier：
def __init__(self,train,target,classifier,n_estimator)：
#准备数据集
self.N_classes = np.unique(target)
self.n_estimator = n_estimator
self.N_data = len(train)
self.trained_classifier = [[classifier,float(0),float(0), True ] for i in range(n_estimator)]
indice = []
train = np.array(train)
target = np.array(target)
dataset = np.concatenate((train,target),axis=1)
#连接训练和目标以进行提升

for i in range(len(dataset[0])-1):
indice.append(i)

self.weights = np.zeros([n_estimator,self.N_data])

#初始化权重的 1/n 值
self.weights.fill(1/float(self.N_data))
#进行采样
new_dataset = dataset
self.N_data = len(new_dataset)
#开始训练子分类器
for i in range(n_estimator):
self.loss = np.zeros(self.N_data)
#分离训练和目标数据
new_train = new_dataset[:,indice]
new_target = new_dataset[:,(len(dataset[0])-1)]
#训练分类器：使用数据权重学习 f(X)
self.trained_classifier[i][0].fit(new_train,new_target,sample_weight=self.weights[i])
#计算加权误差，存储在 trained_classifier[i][1] 中
for point in range(self.N_data) :
if(self.trained_classifier[i][0].predict([new_train[point]]) != new_target[point]):
self.loss[point] = 1
self.trained_classifier[i][1] += self.weights[i][point]

#计算分类器 i 的系数，存储在 trained_classifier[i][2] 中
self.trained_classifier[i][2] = 0.5 * np.log((1-self.trained_classifier[i][1])/self.trained_classifier[i][1])
#重新计算权重
for j in range(self.N_data):
if(self.loss[j] == 1):
self.weights[i][j] *= np.exp(self.trained_classifier[i][2])
else:
self.weights[i][j] *= np.exp(-self.trained_classifier[i][2])

#规范化权重
self.trained_classifier[i][1] = self.trained_classifier[i][1] / self.weights[i].sum()
]]></description>
      <guid>https://stackoverflow.com/questions/41804937/decreasing-accuracy-of-scikit-learn-classifier-after-initializing-weight</guid>
      <pubDate>Mon, 23 Jan 2017 11:09:21 GMT</pubDate>
    </item>
    </channel>
</rss>