<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 28 Nov 2023 15:15:07 GMT</lastBuildDate>
    <item>
      <title>修改 tidytext get_sentiments() 中某些单词的情感</title>
      <link>https://stackoverflow.com/questions/77563423/modifying-the-sentiment-of-certain-words-in-tidytext-get-sentiments</link>
      <description><![CDATA[我正在尝试修改 df 中一些特定单词的情绪，使它们更适合我的上下文，这些单词在我的上下文中使用时带有负面含义，但已被归类为具有积极情绪。这两个字就是“人才”。和“更喜欢”。
这是我的代码：
#加载包
图书馆（dplyr）
库（ggplot2）
需要（读xl）
图书馆（整洁的文本）
需要（writexl）

数据示例：
dput(sentiment_words[1:20,c(7,8,9)])

数据输出：
struction(list(word = c(“天赋”, “更喜欢”, “谎言”, “困难”, “更糟”,
“瘾君子”、“令人讨厌的”、“难以忍受的”、“令人作呕的”、“令人恼火的”、
“奇怪”、“不体贴”、“奇怪”、“压倒性”、“问题”、“投诉”、
“受限”、“爱”、“受限”、“白痴”)、情绪 = c(“积极”、
“阳性”、“阴性”、“阴性”、“阴性”、“阴性”、“阴性”、
“阴性”、“阴性”、“阴性”、“阴性”、“阴性”、“阴性”、
“阴性”、“阴性”、“阴性”、“阴性”、“阳性”、“阴性”、
“负”)，计数＝c(79L，3L，53L，316L，2L，2L，3L，2L，2L，
7L、24L、2L、24L、2L、198L、21L、4L、52L、4L、19L))，类别=c(“grouped_df”，
“tbl_df”、“tbl”、“data.frame”)、row.names = c(NA，-20L)、groups = 结构(列表(
    word = c(“瘾君子”, “抱怨”, “禁闭”, “ftw”, “困难”,
    “白痴”、“不体贴”、“令人恼火”、“问题”、“谎言”、
    “迷失”、“爱”、“讨厌”、“压倒性”、“令人作呕”、
    “难以忍受”、“奇怪”、“更糟”), .rows = Structure(list(
        6L、16L、C(17L、19L)、2L、4L、20L、12L、10L、15L、3L、
        1L、18L、7L、14L、9L、8L、c(11L、13L)、5L)，ptype = 整数(0)，类 = c(“vctrs_list_of”，
    “vctrs_vctr”，“列表”)))，class = c(“tbl_df”，“tbl”，“data.frame”)
), row.names = c(NA, -18L), .drop = TRUE))

 ###### Word 情感分析 ######
## 使用“TIDYTEXT”情感词典
情感词&lt;- df |&gt;
  tidytext::unnest_tokens(输出=“单词”，输入=“帖子”) |&gt;
  dplyr::anti_join(tidytext::stop_words)|&gt;
  dplyr::inner_join(tidytext::get_sentiments(“bing”))

情感词%&gt;%
  计数（单词，排序= TRUE）

# 检查最常见的正面和负面词
情感词&lt;-
情感词 %&gt;% group_by(word) %&gt;% mutate(count = n())
 
bing_word_counts &lt;-情感词 %&gt;%
  dplyr::inner_join(tidytext::get_sentiments(“bing”) %&gt;%
  计数（单词、情感、排序 = TRUE））
]]></description>
      <guid>https://stackoverflow.com/questions/77563423/modifying-the-sentiment-of-certain-words-in-tidytext-get-sentiments</guid>
      <pubDate>Tue, 28 Nov 2023 11:13:39 GMT</pubDate>
    </item>
    <item>
      <title>通过 ImageDataGenerator 在 Tensorflow 中保存每个时期的批量图像</title>
      <link>https://stackoverflow.com/questions/77563371/save-batches-of-images-for-each-epochs-in-tensorflow-via-imagedatagenerator</link>
      <description><![CDATA[我使用 Tensorflow 中的 ImageDataGenerator 函数创建批量图像。 
我知道根据 ImageDataGenerator 随机应用于我的数据集的不同转换，每个时期的每批图像都略有不同。
datagen_full_data_aug = ImageDataGenerator(**data_full_aug)

train_generator_images=datagen_full_data_aug.flow_from_dataframe(
    数据框=火车，
    目录=文件夹图像，
    x_col=&#39;文件名&#39;,
    类模式=无，
    随机播放=真，
    种子=种子，
    批量大小=批量大小，
    目标大小=（图像大小，图像大小））

这就是我训练/拟合模型的方式。
history=model.fit(train_generator,
                  步骤_per_epoch=步骤_per_epoch，
                  纪元=纪元，
                  详细=2，
                  验证数据=验证生成器，
                  valid_steps=val_steps_per_epoch,
                  回调=[检查点],
                  批次大小 = 批次大小）

在特定时期的训练过程中，我的准确性有时会出现一些变化。 
IoU-准确率波动
我想将每个时期的单独批次图像保存在不同的文件夹中（如 epoch_1、epoch_2...等），以便我可以分析哪些图像可能导致准确性波动如此之大。 而且我将能够得出哪些转变可能影响了特定时期的模型。 
如果每个图像都可以重复使用我的数据集中给出的名称来对 puproses 进行排序，那就太好了。 
我应该如何进行？
我尝试了保存功能：
 save_to_dir=无，
    save_prefix=&#39;&#39;,
    save_format=&#39;png&#39;,

但这会将所有历元的所有图像保存到一个文件夹中。我们无法区分第 1 纪元和第 2 纪元的图像......等等。]]></description>
      <guid>https://stackoverflow.com/questions/77563371/save-batches-of-images-for-each-epochs-in-tensorflow-via-imagedatagenerator</guid>
      <pubDate>Tue, 28 Nov 2023 11:05:17 GMT</pubDate>
    </item>
    <item>
      <title>多类问题的形状力图</title>
      <link>https://stackoverflow.com/questions/77563309/shap-force-plots-for-multiclass-problems</link>
      <description><![CDATA[我试图显示给定测试示例的力图，以便在多类分类问题的情况下全部显示在同一个图中。
我的最佳尝试：
explainer = shap.TreeExplainer(model)

shap_test = 解释器.shap_values(x_test)

无花果, 轴 = plt.subplots(6, 1, 无花果大小=(10, 5 * 6))

对于范围 (6) 内的 i：
    axs[i].set_title(f“瀑布图 - {i} 类”)

    Decision_plot = shap.force_plot(
        解释器.expected_value[i]，shap_test[0][i]，x_test.columns，show=False
    ）
plt.show()

不幸的是，这仍然失败了：

有没有人能解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77563309/shap-force-plots-for-multiclass-problems</guid>
      <pubDate>Tue, 28 Nov 2023 10:56:02 GMT</pubDate>
    </item>
    <item>
      <title>如何实现基于内容的语音搜索过滤？</title>
      <link>https://stackoverflow.com/questions/77562907/how-to-implement-content-based-filtering-for-voice-search</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77562907/how-to-implement-content-based-filtering-for-voice-search</guid>
      <pubDate>Tue, 28 Nov 2023 10:01:16 GMT</pubDate>
    </item>
    <item>
      <title>累积时间序列数据集的插补</title>
      <link>https://stackoverflow.com/questions/77562836/imputation-for-cumulative-times-series-dataset</link>
      <description><![CDATA[我想估算一个累积时间序列数据集，如下所示：
累积数据集
每列是降雨量的累计总和。
我尝试过 GAIN 方法来估算该数据集；然而，结果并不像我的预期。我得到的损失非常低，但是插补后的数据集有一些错误，如下所示：
插补后的错误
我想询问处理此类数据集的一些方法。]]></description>
      <guid>https://stackoverflow.com/questions/77562836/imputation-for-cumulative-times-series-dataset</guid>
      <pubDate>Tue, 28 Nov 2023 09:51:33 GMT</pubDate>
    </item>
    <item>
      <title>如何使用tensorflow训练用于面部比较的ML模型并在java中使用它？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77561597/how-to-train-a-ml-model-for-face-comparison-using-tensorflow-and-use-it-in-java</link>
      <description><![CDATA[我想比较两张脸，无论它们是否是同一个人。为此，我使用预训练模型 (FaceNet) 来获取面部嵌入并比较两个面部。
为了使用 FaceNet 模型，我使用了这个 github 链接。但我无法这样做，因为它是 5 年前的代码，并且给我带来了折旧错误。
我有 LFW 数据集（成对的图像），我想自己训练一个模型，并想用它在 java 中比较人脸（使用人脸嵌入）。
如何使用tensorflow训练模型并在java中使用它进行人脸比较？]]></description>
      <guid>https://stackoverflow.com/questions/77561597/how-to-train-a-ml-model-for-face-comparison-using-tensorflow-and-use-it-in-java</guid>
      <pubDate>Tue, 28 Nov 2023 05:46:10 GMT</pubDate>
    </item>
    <item>
      <title>用于线性回归的随机梯度下降算法的意外输出</title>
      <link>https://stackoverflow.com/questions/77560377/unexpected-output-with-stochastic-gradient-descent-algorithm-for-linear-regressi</link>
      <description><![CDATA[在为我的 ML 作业实现 SGD 算法时，我得到了意外的输出。
这是我的训练数据的一部分，通常有 320 行：

我的数据集：https://github.com/Jangrae/csv/ blob/master/carseats.csv
我首先做了一些数据预处理：
导入 pandas 作为 pd
从 sklearn.preprocessing 导入 StandardScaler
将 numpy 导入为 np

train_data = pd.read_csv(&#39;carseats_train.csv&#39;)
train_data.replace({&#39;是&#39;: 1, &#39;否&#39;: 0}, inplace=True)
onehot_tr = pd.get_dummies(train_data[&#39;ShelveLoc&#39;], dtype=int, prefix_sep=&#39;_&#39;, prefix=&#39;ShelveLoc&#39;)
train_data = train_data.drop(&#39;ShelveLoc&#39;, axis=1)
train_data = train_data.join(onehot_tr)


train_data_Y = train_data.iloc[:, 0]
train_data_X = train_data.drop(&#39;销售额&#39;, axis=1)


然后实现这样的算法：
&lt;前&gt;&lt;代码&gt;学习率 = 0.01
epoch_num = 50
初始w = 0.1
截距 = 0.1
w_matrix = np.ones((12, 1)) * 初始w

对于范围内的 e（epoch_num）：
    对于范围内的 i(len(train_data_X))：

        x_i = train_data_X.iloc[i].to_numpy()
        y_i = train_data_Y.iloc[i]
        
        y_估计 = np.dot(x_i, w_matrix) + 截距
        
        grad_w = x_i.reshape(-1, 1) * (y_i - y_估计)
    
        grad_intercept = (y_i - y_估计)
        
       
        w_matrix = w_matrix - 2 * 学习率 * grad_w
        截距 = 截距 - 2 * 学习率 * 梯度截距
        
        

print(&quot;最终权重：\n&quot;, w_matrix)
print(&quot;最终拦截：&quot;,拦截)

但是输出是
最终权重：
 [[南]
 [南]
 [南]
 [南]
 [南]
 [南]
 [南]
 [南]
 [南]
 [南]
 [南]
 [楠]]
最终截距：[nan]

我以不同的学习率运行它，我也尝试了收敛阈值，但仍然得到相同的结果..我无法找出为什么我的代码给我nans..
有人能看到这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/77560377/unexpected-output-with-stochastic-gradient-descent-algorithm-for-linear-regressi</guid>
      <pubDate>Mon, 27 Nov 2023 22:46:46 GMT</pubDate>
    </item>
    <item>
      <title>如何提取给定文档集的顶级分类器特征</title>
      <link>https://stackoverflow.com/questions/77560320/how-to-extract-top-classifier-features-for-a-given-set-of-documents</link>
      <description><![CDATA[我有一个经过二元分类任务训练的逻辑回归分类器。我想提取 X 中给定文档集（不是全部 X，而是其中的一个子集）的顶级分类器特征（信息最丰富的系数）。这些文档的索引存储在名为 idx_list 的列表中。
我尝试使用以下代码提取 X 中所有文档的主要特征：
 def most_informative_feature_for_binary_classification（分类器，向量化器，n=20）：
        类标签 = 分类器.classes_
        feature_names = vectorizer.get_feature_names_out()
        topn_class1 = 排序(zip(classifier.coef_[0], feature_names))[:n]
        topn_class2 = 排序(zip(classifier.coef_[0], feature_names))[-n:]
        print(&#39;0 类主要功能： ----------------------&#39;)
        class0_feat =[]
        对于 coef，topn_class1 中的壮举：
            #print (class_labels[0], coef, feat)
            打印（壮举）
            class0_feat.append(feat )
    
        class0_feat = [str(x) for x in class0_feat]
        使用 open(&#39;../../classification/result/class0_top_features_top_&#39;+str(top_features_nb)+&#39;_&#39;+network+&#39;.txt&#39;,&#39;w&#39;) 作为 f：
            f.write(&#39;\n&#39;.join(class0_feat))
        
        print(&#39;1 类主要功能： ----------------------&#39;)
        class1_feat = []
        对于 coef，相反的壮举（topn_class2）：
            #print (class_labels[1], coef, feat)
            打印（壮举）
            class1_feat.append(壮举)

此代码适用于提取 X 中所有文档的主要特征，但我想提取 idx_list 定义的一组特定文档的主要特征。
使用 Sklearn 对文本文档进行分类：
向量化器 = TfidfVectorizer(input=&#39;文件名&#39;, min_df=mindf, max_df = maxdf)
        X = 矢量化器.fit_transform(friend_files)
        
        print(&quot;X 形状：&quot;,X.shape)

        y = list(username_labels.values()) # 0 或 1

        clf = 逻辑回归()

        clf.fit(X, y)
        most_informative_feature_for_binary_classification3（clf，矢量化器，n=10）

如何修改代码以提取 idx_list 指定文档的顶级特征？]]></description>
      <guid>https://stackoverflow.com/questions/77560320/how-to-extract-top-classifier-features-for-a-given-set-of-documents</guid>
      <pubDate>Mon, 27 Nov 2023 22:32:30 GMT</pubDate>
    </item>
    <item>
      <title>神经网络意外预测</title>
      <link>https://stackoverflow.com/questions/77560144/neuralnet-unexpected-prediction</link>
      <description><![CDATA[我试图了解神经网络包是如何工作的。
我使用的是 mnist 数据集，其中包含对应于不同图片的 60.000 行和代表图片每个像素的 785 列（除了第一个像素）
与图片标签对应的列）。
initial_data &lt;- read.csv(file = &#39;train.csv&#39;, header = TRUE)

数据如下所示：
 标签 Pixel1 Pixel2 Pixel3 Pixel4 Pixel5 Pixel6 ...
1 5 0 0 3 0 1 0 ...
2 3 0 0 0 7 0 0 ...
ETC

首先，我删除方差等于 0 的像素。因为它们无法提供评估图片中写入的数字的信息。
filtered_data &lt;-initial_data %&gt;%
  select_if(函数(列) var(列) != 0)

# 显示新过滤数据的维度
暗淡（过滤数据）

然后我对数据进行标准化，以确保每个功能的贡献相同
到模型中，算法不受较大尺度特征的影响。
filtered_data &lt;- as.data.frame(scale(filtered_data[-1]))

现在我进行数据分区（80% 训练和 20% 测试）。
filtered_data$label &lt;-initial_data$label
filtered_data &lt;-filtered_data %&gt;% select(标签, everything())
索引 &lt;- createDataPartition(filtered_data$label, p = 0.8, list = FALSE)

# 创建训练集和验证集
训练数据&lt;-过滤数据[索引，]
validation_data &lt;-filtered_data[-index, ]

# 通过预测变量和标签分隔
训练数据X &lt;- 训练数据[-1]
训练数据Y &lt;- 训练数据[1]
validation_data_X &lt;-validation_data[-1]
validation_data_Y &lt;-validation_data[1]

现在我生成一个非常简单的神经网络并进行预测
input_variables &lt;- 粘贴（名称（training_data_X），collapse =＆quot; +＆quot;）
输出变量 &lt;- 名称(training_data_Y)[1]
content_formula &lt;- 粘贴（输出变量，“~”，输入变量）

simple_nn_model &lt;- 神经网络（内容公式，数据 = 训练数据，隐藏 = 1，
                             act.fct =“逻辑”，线性输出= FALSE）

Predictions_simple_model &lt;- 预测（simple_nn_model，newdata =validation_data_X）

问题：我希望对象predictions_simple_model包含10列（每列代表0到9之间的一个数字），并且它们的值范围应该从0到1（取决于预测者所做的预测）模型）。但是，相反，我获得了一列，并且它们的所有值都等于 1。
&lt;前&gt;&lt;代码&gt;&gt;预测简单模型
           [,1]
137 1.0000000
171 1.0000000
213 1.0000000
225 1.0000000
236 1.0000000
420 1.0000000
第576章 1.0000000
615 1.0000000
899 1.0000000
ETC

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77560144/neuralnet-unexpected-prediction</guid>
      <pubDate>Mon, 27 Nov 2023 21:52:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 TF-IDF 方法出现大量误报[关闭]</title>
      <link>https://stackoverflow.com/questions/77559931/high-number-of-false-positives-using-tf-idf-method</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77559931/high-number-of-false-positives-using-tf-idf-method</guid>
      <pubDate>Mon, 27 Nov 2023 21:02:40 GMT</pubDate>
    </item>
    <item>
      <title>如何防止过度拟合并获得更高的准确率？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77559565/how-do-i-prevent-overfitting-and-get-a-higher-accuracy</link>
      <description><![CDATA[我正在进行基于方面的情感分析，目前我的模型在拟合时的验证损失和准确性高于训练，但随后它发生翻转，训练表现要好得多。不过，运行 epoch 后，我得到的准确率约为 52%，比我的训练准确率低 10%。这是我的模型：
&lt;前&gt;&lt;代码&gt;vocab_size = 6643
嵌入尺寸 = 300
lstm_单位 = 256

模型=顺序（[
    嵌入（vocab_size，embedding_dim，权重= [embedding_matrix_vocab]，input_length = max_seq_length，可训练= False），
    双向（LSTM（lstm_units，return_sequences = False）），
    辍学（0.1），
    批量归一化(),
    密集（128，激活=&#39;relu&#39;，activity_regularizer=tf.keras.regularizers.L2（0.0001）），
    辍学（0.1），
    批量归一化(),
    密集（128，激活=&#39;relu&#39;，activity_regularizer=tf.keras.regularizers.L2（0.0001）），
    辍学（0.1），
    批量归一化(),
    密集（128，激活=&#39;relu&#39;），
    密集（3，激活=&#39;softmax&#39;）
]）

我正在使用稀疏分类交叉熵。]]></description>
      <guid>https://stackoverflow.com/questions/77559565/how-do-i-prevent-overfitting-and-get-a-higher-accuracy</guid>
      <pubDate>Mon, 27 Nov 2023 19:50:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 BERT 预测推文用户参与度</title>
      <link>https://stackoverflow.com/questions/77555258/predicting-tweet-user-engagement-with-bert</link>
      <description><![CDATA[根据推文内容和发布时间，我可以预测用户参与度。我最初的方法是在推文内容的 BERT 嵌入上使用回归模型。然而，该推文的发布时间也包含有关该推文的有价值的信息。我如何也包括“时间”训练回归模型的信息？]]></description>
      <guid>https://stackoverflow.com/questions/77555258/predicting-tweet-user-engagement-with-bert</guid>
      <pubDate>Mon, 27 Nov 2023 07:50:23 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Hugging Face 转换器进行文本匿名化？</title>
      <link>https://stackoverflow.com/questions/77553670/how-to-do-text-anonymization-using-hugging-face-transformers</link>
      <description><![CDATA[我刚刚遵循了本指南https://medium .com/@luccailliau/text-anonymization-using-hugging-face-transformers-75b5d7392833 但代码不起作用并返回
TypeError：“BatchEncoding”对象不是迭代器

上一个主题中给出的答案不充分，因为匿名功能已被删除。我的最终目标是获得像这样的修改后的文本输出，例如：
“彼得在米兰工作” ---&gt; “PER 在 LOC 中起作用”。

如何解决这个问题？
完整代码在这里：
导入火炬
从转换器导入 AutoTokenizer、AutoModelForTokenClassification
从 Transformers.pipelines.token_classification 导入 TokenClassificationPipeline

model_checkpoint = “Davlan/bert-base-multilingual-cased-ner-hrl”

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
模型 = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

类 TokenClassificationChunkPipeline(TokenClassificationPipeline):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

def 预处理（自身，句子，offset_mapping=None）：
    model_inputs = self.tokenizer(
        句子，
        return_tensors =“pt”，
        截断=真，
        return_special_tokens_mask=真，
        return_offsets_mapping=真，
        return_overflowing_tokens=True, # 返回多个块
        max_length=self.tokenizer.model_max_length,
        填充=真
    ）
    如果偏移映射：
        model_inputs[“offset_mapping”] = offset_mapping

    model_inputs[“句子”] = 句子

    返回模型输入

def _forward（自身，模型输入）：
    Special_tokens_mask = model_inputs.pop(“special_tokens_mask”)
    offset_mapping = model_inputs.pop(“offset_mapping”, None)
    句子 = model_inputs.pop(“句子”)
    Overflow_to_sample_mapping = model_inputs.pop(“overflow_to_sample_mapping”)

    all_logits = torch.Tensor()
    num_chunks = len(model_inputs[“input_ids”])

    # 一次将一个块传递给模型并连接结果
    对于范围内的 i（num_chunks）：
        model_input = {k: torch.unsqueeze(v[i], dim=0) for k, v in model_inputs.items()}
        logits = 模型(**model_input)[0]
        all_logits = torch.cat((all_logits, logits), dim=1)

    模型输出 = {
        “logits”：all_logits，
        “special_tokens_mask”：special_tokens_mask，
        “offset_mapping”：offset_mapping，
        “句子”：句子，
        “overflow_to_sample_mapping”：overflow_to_sample_mapping，
        **模型_输入，
    }

    # 我们重塑输出以适应后处理输入
    model_outputs[“input_ids”] = torch.reshape(model_outputs[“input_ids”], (1, -1))
    model_outputs[“token_type_ids”] = torch.reshape(model_outputs[“token_type_ids”], (1, -1))
    model_outputs[“attention_mask”] = torch.reshape(model_outputs[“attention_mask”], (1, -1))
    model_outputs[“special_tokens_mask”] = torch.reshape(model_outputs[“special_tokens_mask”], (1, -1))
    model_outputs[“offset_mapping”] = torch.reshape(model_outputs[“offset_mapping”], (1, -1, 2))
    返回模型输出

管道 = TokenClassificationChunkPipeline(model=model, tokenizer=tokenizer,aggregation_strategy=“简单”)

# 替换实体
def 匿名（文本）：
    ents = 管道（文本）
    split_text = 列表（文本）
    对于 ent 中的 ent：
        split_text[ent[&#39;start&#39;]] = f&quot;[{ent[&#39;entity_group&#39;]}]&quot;&quot;
        对于范围内的 i(ent[&#39;start&#39;] + 1, ent[&#39;end&#39;]):
            split_text[i] = &quot;&quot;;

    返回“”.join(split_text)


text =“伯纳德在巴黎的法国巴黎银行工作。”
anonymized_text = 匿名化（文本）
打印（匿名文本）
]]></description>
      <guid>https://stackoverflow.com/questions/77553670/how-to-do-text-anonymization-using-hugging-face-transformers</guid>
      <pubDate>Sun, 26 Nov 2023 22:11:26 GMT</pubDate>
    </item>
    <item>
      <title>X 有 95812 个特征，但 RandomForestClassifier 期望有 178341 个特征作为输入 [重复]</title>
      <link>https://stackoverflow.com/questions/77553577/x-has-95812-features-but-randomforestclassifier-is-expecting-178341-features-as</link>
      <description><![CDATA[我有一个使用文本数据的随机森林模型。但是，当我在新数据（测试集）上尝试该模型时，训练集和测试集之间的特征数量不兼容。另外，测试集上的转换有时会给我带来错误：
sklearn.exceptions.NotFittedError：此 ColumnTransformer 实例尚未安装。在使用此估计器之前，请使用适当的参数调用“fit”。

这是我的代码：
featurizer = ColumnTransformer(
    变压器=[(“矢量化标题”, TfidfVectorizer(), “过滤标题”),
                  (“vectorized_author”,TfidfVectorizer(),“filtered_author”),
                  (“vectorized_abstract”，TfidfVectorizer()，“filtered_abstract”)，
                  (“encoded_publisher”,OneHotEncoder(),[“发布者”]),
                  (“encoded_entrytype”,OneHotEncoder(),[“ENTRYTYPE”]),
                  ],
    余数=&#39;丢弃&#39;）

使用 open(&#39;featurizer.pkl&#39;, &#39;wb&#39;) 作为 featurizer_file：
    pickle.dump（特征器，特征器文件）

X_transformed = featurizer.fit_transform(new_df)

使用 open(&#39;featurizer.pkl&#39;, &#39;rb&#39;) 作为 featurizer_file：
    load_featurizer = pickle.load(featurizer_file)

X_test_transformed = returned_featurizer.transform(test_df)

y_pred = rf_model.predict(X_test_transformed)

我尝试将 transform 部分更改为 fit_transform （我知道这对于测试集来说是不正确的）。两个数据集中的列数和列顺序相同。训练集和测试集的特征名称不同。]]></description>
      <guid>https://stackoverflow.com/questions/77553577/x-has-95812-features-but-randomforestclassifier-is-expecting-178341-features-as</guid>
      <pubDate>Sun, 26 Nov 2023 21:40:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML.NET 的动态训练/测试课程</title>
      <link>https://stackoverflow.com/questions/52822696/dynamic-training-test-classes-with-ml-net</link>
      <description><![CDATA[这是此处问题的后续内容
动态类/对象 ML.net 的 PredictionMoadel&lt; ;T输入，T输出&gt;火车（）
我的系统无法在编译时使用预定义的类，因此我尝试将动态类提供给 ML.NET，如下所示
 // 字段数据类型
    公开课领域
    {
        公共字符串字段名 { get;放; }
        公共类型 FieldType { 获取；放; }
    }

    // 动态类助手
    公共类 DynamicClass ：DynamicObject
    {
        私有只读字典&lt;字符串，KeyValuePair&lt;类型，对象&gt;&gt; _字段；

        公共 DynamicClass(List 字段)
        {
            _fields = new Dictionary&lt;字符串, KeyValuePair&lt;类型, 对象&gt;&gt;();
            fields.ForEach(x =&gt; _fields.Add(x.FieldName,
                new KeyValuePair&lt;类型，对象&gt;(x.FieldType, null)));
        }

        public override bool TrySetMember(SetMemberBinder活页夹，对象值)
        {
            if (_fields.ContainsKey(binder.Name))
            {
                var type = _fields[binder.Name].Key;
                if (value.GetType() == 类型)
                {
                    _fields[binder.Name] = new KeyValuePair&lt;类型，对象&gt;(类型，值);
                    返回真；
                }
                else throw new Exception(&quot;Value &quot; + value + &quot; 不是类型 &quot; + type.Name);
            }
            返回假；
        }

        公共覆盖布尔TryGetMember（GetMemberBinder活页夹，输出对象结果）
        {
            结果 = _fields[binder.Name].Value;
            返回真；
        }
    }

    私有静态无效主（字符串[] args）
    {
        var fields = new List&lt;字段&gt;;
        {
            新字段 {FieldName = &quot;名称&quot;, FieldType = typeof(string)},
            新字段 {FieldName = &quot;收入&quot;, FieldType = typeof(float)}
        };

        动态 obj1 = new DynamicClass(字段);
        obj1.Name = &quot;约翰&quot;;
        obj1.收入 = 100f;

        动态 obj2 = new DynamicClass(字段);
        obj2.Name = &quot;爱丽丝&quot;;
        obj2.收入 = 200f;

        var TrainingData = new List&lt;动态&gt;; {对象1，对象2}；

        var env = new LocalEnvironment();
        var schemaDef = SchemaDefinition.Create(typeof(DynamicClass));
        schemaDef.Add(new SchemaDefinition.Column(null, &quot;Name&quot;, TextType.Instance));
        schemaDef.Add(new SchemaDefinition.Column(null, &quot;收入&quot;, NumberType.R4));
        var trainDataView = env.CreateStreamingDataView(trainingData, schemaDef);

        var pipeline = new CategoricalEstimator(env, &quot;名称&quot;)
            .Append(new ConcatEstimator(env, &quot;功能&quot;, &quot;名称&quot;))
            .Append(new FastTreeRegressionTrainer(env, &quot;收入&quot;, &quot;特征&quot;));

        var model = pipeline.Fit(trainDataView);
    }

并收到错误：“&#39;在类型&#39;System.Object&#39;中找不到名称为&#39;Name&#39;的字段或属性”。我尝试使用反射生成类，但遇到了同样的问题。
有解决办法吗？谢谢]]></description>
      <guid>https://stackoverflow.com/questions/52822696/dynamic-training-test-classes-with-ml-net</guid>
      <pubDate>Mon, 15 Oct 2018 18:24:15 GMT</pubDate>
    </item>
    </channel>
</rss>