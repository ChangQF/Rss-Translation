<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 03 Sep 2024 21:14:59 GMT</lastBuildDate>
    <item>
      <title>假基站 [关闭]</title>
      <link>https://stackoverflow.com/questions/78945925/fake-base-station</link>
      <description><![CDATA[我正在研究如何使用机器学习找出假基站。我捕获了一些特征，例如“数据包编号”、“大小（B）”、“层”、“数据包信息摘要”、“时间差异（s）”。您能否建议我使用更多特征来识别假基站？
我是一名博士研究员，试图获取更多特征名称来训练机器学习模型。]]></description>
      <guid>https://stackoverflow.com/questions/78945925/fake-base-station</guid>
      <pubDate>Tue, 03 Sep 2024 20:13:23 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 回归器给出非常糟糕的相对误差 %</title>
      <link>https://stackoverflow.com/questions/78945802/xgboost-regressor-giving-very-bad-relative-error</link>
      <description><![CDATA[当我训练 XGBRegressor 时，对因变量进行了 boxcox 变换，以获得更高的线性度，因为它可以改善模型。对于标准化指标，我获得了 94.7% 的良好准确度，rmse 良好，相对误差 % 为 1.8%。但是当我尝试对因变量进行逆变换时，相对误差 % 为 245%。这令人担忧，因为随机森林给出的相对误差 % 为 10。随机森林和 XGBoost 几乎给出了因变量形式相同的平均绝对误差，但不知何故 XG 的相对误差 % 为 245%。这怎么可能呢？
我尝试过 hypertuning XGBoost，希望得到一些好消息，但错误率上升到了 450%。]]></description>
      <guid>https://stackoverflow.com/questions/78945802/xgboost-regressor-giving-very-bad-relative-error</guid>
      <pubDate>Tue, 03 Sep 2024 19:28:23 GMT</pubDate>
    </item>
    <item>
      <title>深度 CFR 实现</title>
      <link>https://stackoverflow.com/questions/78945640/deep-cfr-implementation</link>
      <description><![CDATA[我从原始 Deep CFR 文章中获取了代码
论文
我现在正在为我的游戏实现 Deep CFR 算法，当我编写完整算法时，我遇到了很多错误。所以我的问题是，本文末尾的代码是否完全正确，错误是否在我的代码中？如果有人已经为他们自己的任务实现了 Deep CFR 并且可以分享提示/代码，那就太好了
如果有人愿意提供帮助，我可以上传我的实现。它需要最终确定，我没有足够的经验来了解如何正确实现它。
这是我的代码：

nn.py
memory.py
deep_cfr.py
game_tree.py
utils.py

对于这个游戏，我从这个库中获取了 texasholdem。]]></description>
      <guid>https://stackoverflow.com/questions/78945640/deep-cfr-implementation</guid>
      <pubDate>Tue, 03 Sep 2024 18:26:16 GMT</pubDate>
    </item>
    <item>
      <title>llama 3.1 使用 AWS Jumpstart 断言 max_prompt_len <= params.max_seq_len</title>
      <link>https://stackoverflow.com/questions/78945419/llama-3-1-assert-max-prompt-len-params-max-seq-len-with-aws-jumpstart</link>
      <description><![CDATA[我们已经使用 JumpStart 将 llama 3.1 8b 指令模型部署到 AWS Sagemaker 端点，它适用于小提示，但对于稍大的提示，它会失败（这违背了使用此模型的目的，因为我们选择最大的上下文窗口）：
assert max_prompt_len &lt;= params.max_seq_len

我认为消息非常清楚，解决方案很简单：更新这些参数，但我找不到任何关于如何在通过 jumpstart 部署模型时更新这些参数的文档（我已经在预测时测试了推理参数，但没有起作用，所以我猜这些应该在部署期间配置）。
关于如何传递这些参数有什么提示吗？它们通常是如何配置的（一般来说，不是专门针对 jumpstart）？]]></description>
      <guid>https://stackoverflow.com/questions/78945419/llama-3-1-assert-max-prompt-len-params-max-seq-len-with-aws-jumpstart</guid>
      <pubDate>Tue, 03 Sep 2024 17:15:06 GMT</pubDate>
    </item>
    <item>
      <title>如何使用自动生成的代码重新训练 ML.Net 模型？</title>
      <link>https://stackoverflow.com/questions/78945171/how-do-you-retrain-an-ml-net-model-using-the-auto-generated-code</link>
      <description><![CDATA[我已经使用 Ml.net 配置接口训练了一个图像分类模型，并且它生成了自包含的 MlModel.consumption 和 MlModel.training 部分类。
它还自动生成一个程序，演示如何使用该模型，但没有演示如何重新训练它。
我曾尝试这样做
MLContext mlContext = new MLContext();

DataViewSchema modelSchema;

ITransformer trainedModel = mlContext.Model.Load(&quot;MlModel2.mlnet&quot;, out modelSchema);

var x = MLModel2.LoadImageFromFolder(mlContext, @&quot;C:\temp\Latest\Train&quot;);

MLModel2.RetrainModel(mlContext, x);

mlContext.Model.Save(trainedModel, modelSchema, &quot;MlModel2.mlnet&quot;);

但它将“未创建保存器，因为图中没有要恢复的变量”写入控制台，然后什么也不做。
自动生成的代码如下所示
// 此文件由 ML.NET Model Builder 自动生成。
使用 System;
使用 System.IO;
使用 System.Collections.Generic;
使用 System.Linq;
使用 System.Text;
使用 System.Threading.Tasks;
使用 Microsoft.ML;
使用 Microsoft.ML.Data;
使用 Microsoft.ML.Vision;

namespace NewCassetteModel
{
public partial class MLModel2
{

/// &lt;summary&gt;
/// 从文件夹路径加载 IDataView。
/// &lt;/summary&gt;
/// &lt;param name=&quot;mlContext&quot;&gt;所有 ML.NET 操作的通用上下文。&lt;/param&gt;
/// &lt;param name=&quot;folder&quot;&gt; 用于训练的图像数据文件夹。&lt;/param&gt;
public static IDataView LoadImageFromFolder(MLContext mlContext, string folder)
{
var res = new List&lt;ModelInput&gt;();
var allowedImageExtensions = new[] { &quot;.png&quot;, &quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.gif&quot; };
DirectoryInfo rootDirectoryInfo = new DirectoryInfo(folder);
DirectoryInfo[] subDirectories = rootDirectoryInfo.GetDirectories();

if (subDirectories.Length == 0)
{
throw new Exception(&quot;fail to find subdirectories&quot;);
}

foreach (DirectoryInfo directory in subDirectories)
{
var imageList = directory.EnumerateFiles().Where(f =&gt; allowedImageExtensions.Contains(f.Extension.ToLower()));
if (imageList.Count() &gt; 0)
{
res.AddRange(imageList.Select(i =&gt; new ModelInput 
{
Label = directory.Name,
ImageSource = File.ReadAllBytes(i.FullName),
}));
}
}
return mlContext.Data.LoadFromEnumerable(res);
}

/// &lt;summary&gt;
/// 使用在训练过程中生成的管道重新训练模型。
/// &lt;/summary&gt;
/// &lt;param name=&quot;mlContext&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;trainData&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
public static ITransformer RetrainModel(MLContext mlContext, IDataView trainData)
{
var pipeline = BuildPipeline(mlContext);
var model = pipeline.Fit(trainData);

return model;
}

/// &lt;summary&gt;
/// 构建模型构建器使用的管道。使用此函数重新训练模型。
/// &lt;/summary&gt;
/// &lt;param name=&quot;mlContext&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
public static IEstimator&lt;ITransformer&gt; BuildPipeline(MLContext mlContext)
{
// 带有管道数据转换的数据处理配置
var pipeline = mlContext.Transforms.Conversion.MapValueToKey(outputColumnName:@&quot;Label&quot;,inputColumnName:@&quot;Label&quot;,addKeyValueAnnotationsAsText:false) 
.Append(mlContext.MulticlassClassification.Trainers.ImageClassification(labelColumnName:@&quot;Label&quot;,scoreColumnName:@&quot;Score&quot;,featureColumnName:@&quot;ImageSource&quot;)) 
.Append(mlContext.Transforms.Conversion.MapKeyToValue(outputColumnName:@&quot;PredictedLabel&quot;,inputColumnName:@&quot;PredictedLabel&quot;));

return pipeline;
}
}
}


我需要做什么才能使其工作？]]></description>
      <guid>https://stackoverflow.com/questions/78945171/how-do-you-retrain-an-ml-net-model-using-the-auto-generated-code</guid>
      <pubDate>Tue, 03 Sep 2024 15:53:58 GMT</pubDate>
    </item>
    <item>
      <title>机器学习的均方误差成本函数如何具有与 y-hat（预测）相对应的 y（目标）？[关闭]</title>
      <link>https://stackoverflow.com/questions/78945062/how-does-the-mean-squared-error-cost-function-from-machine-learning-have-a-y-ta</link>
      <description><![CDATA[这个问题与 Coursera 上的 Andrew Ng 机器学习专业课程有关。
在平方误差成本函数公式中，我们从 y-hat（预测）中减去 y（目标）并对其求平方。但 y-hat 来自一个新的输入，我们已经在函数（直线）上找到了它的预测；如果 x 输入是一个在训练集上未见过的全新值，它如何有相应的 y（目标）来减去？
我可能误解了这里的东西。
这是一个学习问题，所以我认为我误解了一些东西，或者课程只是让我理解一些东西——这是课程的第一周。]]></description>
      <guid>https://stackoverflow.com/questions/78945062/how-does-the-mean-squared-error-cost-function-from-machine-learning-have-a-y-ta</guid>
      <pubDate>Tue, 03 Sep 2024 15:27:40 GMT</pubDate>
    </item>
    <item>
      <title>要求法学硕士 (LLM) 取一个值并从可接受的值列表中选择最适用的标签。我该如何预防幻觉？[关闭]</title>
      <link>https://stackoverflow.com/questions/78944999/asking-an-llm-to-take-a-value-and-choose-the-most-applicable-tag-from-an-accepte</link>
      <description><![CDATA[我有一个工作流，其中有大约 17,000 个值，这些值代表给定公司的行业价值（例如金融科技、医疗保健等）。其中许多被认为过于具体，因此我构建了一个提示，要求它获取每个值并从 250 个已批准的行业标签列表中选择最合适的值。我的问题是，尽管提示中明确说明，但 LLM 仍然以相当高的频率产生幻觉（900 个不同的值）。这是我的提示：
您将获得两个列表，一个包含公司所在行业的已批准值（行业价值），另一个包含广泛的不同行业（行业标签）。您的任务是将第二个列表中的每个行业标签与第一个列表中最合适的行业值进行匹配。风险投资公司将使用这些标签来识别他们可能投资的公司，因此需要从这个角度对其进行评估。只返回一个值至关重要。请勿返回不在批准的行业值列表中的值。如果没有相关匹配，则返回“无匹配”。

仅返回每个行业标签的批准列表中匹配的行业值，

格式为 YAML，不包含任何超出此格式的附加文本、说明或内容。 不返回任何额外文本至关重要。
在输入和输出之间保留行业顺序也至关重要，仅反映基于所提供信息的必要分类。 请不要返回 YAML 输出之外的任何其他文本。 输出格式与提供的示例输出相同至关重要。

每个行业标签应具有以下列表中的一个行业值：

3D 打印
配件
...

示例输入：
- 行业标签：农业加工
- 行业标签：金融科技
- 行业标签：胡言乱语
...

Yaml 输出：
- 行业标签：
农业加工：
行业价值：农业

- 行业标签：
金融科技：
行业价值：金融科技

- 行业标签：
胡言乱语：
行业价值：不匹配
...

以下是行业价值：
- 行业标签：多元化材料
- 行业标签：政府和公共服务
- 行业标签：医疗管理咨询
...

此处使用省略号缩短提示显示。尽管我已要求它不要生成批准列表之外的值，但它仍然会生成，我需要一些关于如何解决此问题的提示。]]></description>
      <guid>https://stackoverflow.com/questions/78944999/asking-an-llm-to-take-a-value-and-choose-the-most-applicable-tag-from-an-accepte</guid>
      <pubDate>Tue, 03 Sep 2024 15:11:06 GMT</pubDate>
    </item>
    <item>
      <title>如何从雪花阶段注册模型？</title>
      <link>https://stackoverflow.com/questions/78944290/how-to-register-a-model-from-a-snowflake-stage</link>
      <description><![CDATA[我在 Snowflake 阶段存储了模型，该模型可以预测给定数据集的收入。
现在我想将 Snowflake 中的模型注册为端点函数，该函数可以接受输入数据并返回预测值？
我查看了
snowflake.snowpark.functions.sproc

但它没有明确说明将模型注册为函数/存储过程端点？
那么在 Snowflake 中从现有阶段注册模型的最佳方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78944290/how-to-register-a-model-from-a-snowflake-stage</guid>
      <pubDate>Tue, 03 Sep 2024 12:14:31 GMT</pubDate>
    </item>
    <item>
      <title>如何理解 NN 输入和输出</title>
      <link>https://stackoverflow.com/questions/78944213/how-to-understand-nn-input-and-output</link>
      <description><![CDATA[我有一个项目想要对图像进行特征提取。我有这个 CNN 层：
model = keras.Sequential(
[
layers.Input((2*imsize,imsize,3)), 
layers.Reshape((2,imsize,imsize,3)), # 将图像拆分为 2 个 3 维图像 
layers.LayerNormalization(axis=[-1,-2,-3]), 

# CNN 层 
layers.SeparableConv2D(32, (3, 3),activation=&#39;relu&#39;),
layers.MaxPooling2D(pool_size=(2, 2)),
layers.SeparableConv2D(32, (3, 3),activation=&#39;relu&#39;),
layers.MaxPooling2D(pool_size=(2, 2)),
layers.SeparableConv2D(64, (3, 3),activation=&#39;relu&#39;),
layer.MaxPooling2D(pool_size=(2, 2)),

layer.Flatten(), 
layer.Dropout(rate=0.7), 
layer.Dense(16,activation=&#39;relu&#39;), 
layer.Dense(2,activation=&#39;softmax&#39;) 
])

虽然我得到了这个错误
内核形状必须与输入具有相同的长度，但接收到的内核形状为 (3, 3, 3, 32)，输入形状为 (None, 2, 64, 64, 3)。 SeparableConv2D.call() 接收的参数：
• args=(&#39;&lt;KerasTensor shape=(None, 2, 64, 64, 3), dtype=float32, sparse=False, 
name=keras_tensor_141&gt;&#39;,)
• kwargs=&lt;class &#39;inspect._empty&#39;&gt;

我无法理解如何将 LayerNormalization 层放入 SeparableConv2D 中。有人能帮我解决我做错的地方吗？]]></description>
      <guid>https://stackoverflow.com/questions/78944213/how-to-understand-nn-input-and-output</guid>
      <pubDate>Tue, 03 Sep 2024 11:55:04 GMT</pubDate>
    </item>
    <item>
      <title>我的海康威视摄像机 FPS 低且延迟严重</title>
      <link>https://stackoverflow.com/questions/78943962/low-fps-and-lot-of-delay-with-my-hikvision-cam</link>
      <description><![CDATA[我有一个任务，使用 OpenCV 处理来自 Hikvision IP 摄像机的流媒体视频。最初，视频的 FPS 约为 20-25，延迟为 2-3 秒。然而，随着代码运行时间的延长，FPS 迅速下降，延迟增加。最终，FPS 下降到 2-3，视频冻结。
class VideoLoader:
@staticmethod
def load_video(video_path):
cap = cv2.VideoCapture(video_path)
cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter.fourcc(&#39;M&#39;, &#39;J&#39;, &#39;P&#39;, &#39;G&#39;))
cap.set(cv2.CAP_PROP_FPS, 25)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) 
assert cap.isOpened(), &quot;Video dosyası okunurken hata oluştu&quot;
return cap

我尝试过使用 GPU、采用多线程和降低分辨率等方法，但这些方法效果都不太好。您认为问题是什么，我该如何解决它
def frame_reader(cap, frame_queue):
while cap.isOpened():
success, frame = cap.read()
if not success:
break
if not frame_queue.full(): # 检查队列是否有空间
frame_queue.put(frame)
cap.release()
frame_queue.put(None) # 流结束信号

def video_processor(frame_queue, output_queue, model, class_names, playing_sounds):
Threshold = 0.5 # 测试结果
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model.to(device)

while True:
frame = frame_queue.get()
if frame is None: # 流结束信号
break
start_time = time.time()
img = cv2.resize(frame, (640, 480))
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float().unsqueeze(0).to(device)
img_tensor /= 255.0 # 标准化

def video_detection(video_source):
cap = VideoLoader.load_video(video_source)
model = YOLO(&quot;YOLO-Weights/ppe.pt&quot;)
class_names = [&#39;safety-glasses&#39;, &#39;gloves&#39;, &#39;orange-vest&#39;, &#39;yellow-vest&#39;,]
playing_sounds = set()

frame_queue = Queue(maxsize=5)
output_queue =队列（最大大小=5）

reader_thread = threading.Thread（目标=frame_reader，参数=（cap，frame_queue），守护进程=True）
processor_thread = threading.Thread（目标=video_processor，参数=（frame_queue，output_queue，model，class_names，played_sounds），守护进程=True）

reader_thread.start()
processor_thread.start()
]]></description>
      <guid>https://stackoverflow.com/questions/78943962/low-fps-and-lot-of-delay-with-my-hikvision-cam</guid>
      <pubDate>Tue, 03 Sep 2024 10:50:27 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML 和深度学习对发票进行 OCR，不同供应商的发票格式各异。我需要提取详细信息并将其存储在 excel 中 [关闭]</title>
      <link>https://stackoverflow.com/questions/78943747/ocr-for-invoice-using-ml-and-deep-learning-invoice-is-of-varied-format-from-dif</link>
      <description><![CDATA[我正在尝试制作用于发票扫描的 OCR。我有来自不同供应商的许多不同格式的发票。我想扫描该发票并提取一些详细信息，例如发票号、供应商名称、交货条款等，我有 5 到 6 个详细信息，所有这些都在不同的地方，甚至其名称也发生了变化，例如有些有发票号，而有些账单有买家的订单号。
我想提取数据并将其存储在 excel 表中。
我曾考虑过使用 LayoutLMV3、Pytorch 和 OpenCv 等
我应该如何处理这个问题陈述。]]></description>
      <guid>https://stackoverflow.com/questions/78943747/ocr-for-invoice-using-ml-and-deep-learning-invoice-is-of-varied-format-from-dif</guid>
      <pubDate>Tue, 03 Sep 2024 10:00:07 GMT</pubDate>
    </item>
    <item>
      <title>如何实现自适应负荷和光伏预测模型的强化学习？[关闭]</title>
      <link>https://stackoverflow.com/questions/78942987/how-to-implement-reinforcement-learning-for-adaptive-load-and-pv-forecasting-mod</link>
      <description><![CDATA[我正在开展一个项目，使用时间序列数据构建负载和光伏 (PV) 预测模型。目前，我已经使用随机森林和 LSTM 实现了模型，虽然它们表现相当不错，但由于趋势多变，我遇到了挑战。
我面临的问题是，负载和 PV 输出由于外部因素而高度可变。例如，负载因不同的消费模式而变化，PV 输出因气候变化而波动。我需要一个可以动态适应这些变化的模型。
我正在考虑使用强化学习 (RL) 来开发一个可以实时适应这些环境变化的模型。我的目标是实施一种在线学习方法，让模型不断学习并根据新数据进行自我更新。但是，我发现重新训练随机森林和 LSTM 等模型在计算上非常昂贵。
我的问题是：

如何有效地实施强化学习以实现这种自适应预测？
是否有特定的 RL 算法或技术非常适合动态变化环境中的时间序列预测？
在在线学习场景中，我可以使用哪些策略来平衡模型性能和计算成本之间的权衡？
有任何指导或资源吗？

我已经实施了随机森林和 LSTM 模型来进行负载和 PV 预测。这些模型对于初始预测效果很好，但很难适应数据随时间变化的趋势。我探索了在线学习方法，希望它们能提供实时适应的解决方案。但是，我发现不断重新训练这些模型在计算上非常昂贵，并且不适合我的用例。
我希望开发一种可以实时适应负载和 PV 趋势变化的模型，而无需不断重新训练。我的目标是找到一种可以动态高效地处理数据变化的方法。
到目前为止，我尝试过的模型要么需要频繁重新训练，这很昂贵，要么它们无法足够快地适应数据的变化。这导致预测准确性随着时间的推移而下降。]]></description>
      <guid>https://stackoverflow.com/questions/78942987/how-to-implement-reinforcement-learning-for-adaptive-load-and-pv-forecasting-mod</guid>
      <pubDate>Tue, 03 Sep 2024 06:46:11 GMT</pubDate>
    </item>
    <item>
      <title>SAM 模型中无法检测图像 - TypeError：无法处理此数据类型</title>
      <link>https://stackoverflow.com/questions/78942834/image-not-detecting-in-sam-model-typeerror-cannot-handle-this-data-type</link>
      <description><![CDATA[每当我上传任何类型的图像时，都会出现相同的错误。它是否只处理高质量图像（或任何特定类型的图像），还是我的代码中存在一些错误？
我尝试了不同的图像，上面的代码是 chatgpt 经过一些修改后给出的。仍然没有运气。我得到一个
TypeError：无法处理此数据类型：（1, 1, 640, 3），|u1

代码：
import torch
import numpy as np
from PIL import Image
fromsegment_anything import sam_model_registry, SamPredictor
from google.colab import files

# 加载 SAM 模型

sam = sam_model_registry[&quot;vit_b&quot;](checkpoint=&quot;/content/sam_vit_b_01ec64.pth&quot;)
predictor = SamPredictor(sam)

uploaded = files.upload()
image_name = list(uploaded.keys())[0]

image = Image.open(image_name).convert(&quot;RGB&quot;)
image_np = np.array(image)

# 检查图像形状

print(f&quot;原始图像形状：{image_np.shape}&quot;)

# 如果存在额外维度，则删除它们

if len(image_np.shape) == 4 and image_np.shape[0] == 1:
image_np = image_np.squeeze(0) # 如果第一个维度的大小为 1，则删除它

# 确保图像的格式和类型正确

image_np = image_np.astype(np.uint8)

print(f&quot;处理后的图像形状：{image_np.shape}&quot;)

# 将图像设置为 SAM 预测器

predictor.set_image(image_np)

# 预测图像的蒙版

masks = predictor.predict()

# 确保您有蒙版并使用它们

if mask is not None and len(masks) &gt; 0:
mask = mask[0] # 假设第一个掩码就是您需要的

# 将掩码应用于图像
masked_image = np.where(mask[..., None], image_np, 0) # 将掩码应用于图像

# 将掩码图像转换回 PIL 图像
masked_image = Image.fromarray(masked_image)

# 显示掩码图像
masked_image.show()

else:
print(&quot;未找到给定图像的掩码。&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/78942834/image-not-detecting-in-sam-model-typeerror-cannot-handle-this-data-type</guid>
      <pubDate>Tue, 03 Sep 2024 05:49:49 GMT</pubDate>
    </item>
    <item>
      <title>梯度盗取代理</title>
      <link>https://stackoverflow.com/questions/78942595/gradient-bandit-agent</link>
      <description><![CDATA[我正在阅读 Sutton&amp;Barto 的《强化学习：导论》。尝试测试梯度强盗代理（第 2.7 章）。但性能极低。我试过：

使用基线 = 平均奖励，不使用基线；
alpha = 0.1、0.2、0.3、0.4；
初始偏好 H = 0、1、10、100。

没有任何帮助。
这是我的 Python 代码，用于代理的 生命步骤 = 动作选择 + 参数更新 (self = agent)：
# 用于概率计算：
pref_exps = np.exp(self.params[&quot;preferences&quot;])
pref_exps_sum = sum(pref_exps)

# 选择强盗：
choice_dice = np.random.uniform() * pref_exps_sum
accum_pref_exp = 0
for i, pref_exp in enumerate(pref_exps):
accum_pref_exp += pref_exp
if accum_pref_exp &gt;= choice_dice:
self.chosen_bandit_i = i
break

# self.reward 在此处填充：
self.perform_bandit(self.chosen_bandit_i)

# 更新基线：
self.params[&quot;lifetime&quot;] += 1
self.params[&quot;average_reward&quot;] += 1 / self.params[&quot;lifetime&quot;] * (self.reward - self.params[&quot;average_reward&quot;])

# 更新偏好：
for i, pref_exp in enumerate(pref_exps):
probability = pref_exp / pref_exps_sum
if i == self.chosen_bandit_i:
self.params[&quot;preferences&quot;][i] += self.params[&quot;alpha&quot;] * (self.reward - self.params[&quot;average_reward&quot;]) * (1 - probability)
else:
self.params[&quot;preferences&quot;][i] -= self.params[&quot;alpha&quot;] * (self.reward - self.params[&quot;average_reward&quot;]) * probability

此代码导致性能极差（100 个代理，每个代理访问自己的 10 个 1-armed-bandits，测试超过 2000 步），我们可以从下图中看到：

我看过这篇帖子，修复错误后，它的代码似乎与我的代码相同，这也是我写这篇帖子的原因。但与我的代码不同，那篇帖子的代码在纠正后可以正常工作！
我不知道自己在哪里犯了错误。你们能帮我正确使用Gradient-bandit agent的全部功能吗？]]></description>
      <guid>https://stackoverflow.com/questions/78942595/gradient-bandit-agent</guid>
      <pubDate>Tue, 03 Sep 2024 03:40:50 GMT</pubDate>
    </item>
    <item>
      <title>subprocess.CalledProcessError [关闭]</title>
      <link>https://stackoverflow.com/questions/78940345/subprocess-calledprocesserror</link>
      <description><![CDATA[我正在使用 Unsloth 库来加载我的微调模型，我能够成功加载我的模型，但是当我想要生成推理时，我收到此错误：
subprocess.CalledProcessError：命令&#39;[&#39;/usr/bin/gcc&#39;, &#39;/tmp/tmpepjvuftj/main.c&#39;, &#39;-O3&#39;, &#39;-shared&#39;, &#39;-fPIC&#39;, &#39;-o&#39;, &#39;/tmp/tmpepjvuftj/cuda_utils.cpython-311-x86_64-linux-gnu.so&#39;, &#39;-lcuda&#39;, &#39;-L/usr/local/lib/python3.11/dist-packages/triton/backends/nvidia/lib&#39;, &#39;-L/lib/x86_64-linux-gnu&#39;, &#39;-I/usr/local/lib/python3.11/dist-packages/triton/backends/nvidia/include&#39;, &#39;-I/tmp/tmpepjvuftj&#39;, &#39;-I/usr/include/python3.11&#39;]&#39; 返回非零退出状态 1。
这是我的代码：
def load_model()：
max_seq_length = 2048 # 选择任意！我们自动内部支持 RoPE 缩放！
dtype = None # 自动检测为 None。Float16 适用于 Tesla T4、V100，Bfloat16 适用于 Ampere+
load_in_4bit = True # 使用 4 位量化来减少内存使用量。可以为 False。
如果为 True:
model, tokenizer = FastLanguageModel.from_pretrained(
model_name = &quot;/root/natural-matchmaking-ai/models/text_to_nosql&quot;, # 您用于训练的模型
max_seq_length = max_seq_length,
dtype = dtype,
load_in_4bit = load_in_4bit,
)
FastLanguageModel.for_inference(model) 
print(&quot; 模型已成功加载&quot;)
return model,tokenizer

def generate_response(message_content,tokenizer,model):
try:
messages = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message_content}]
 # 从模板生成输入 ID
input_ids = tokenizer.apply_chat_template(
messages,
add_generation_prompt=True,
return_tensors=&quot;pt&quot;
).to(device)
print(&quot;输入 ID 设备：&quot;, input_ids.device)
print(&quot;模型设备：&quot;, next(model.parameters()).device)
# 初始化文本流器
text_streamer = TextStreamer(tokenizer, skip_prompt=True)
print(&quot;从文本流器继续&quot;)
# 生成响应
generated_outputs = model.generate(
input_ids,
streamer=text_streamer,
max_new_tokens=128,
pad_token_id=tokenizer.eos_token_id
)

# 解码生成的文本
generated_text = tokenizer.decode(generated_outputs[0], skip_special_tokens=True)

# 按行拆分响应并获取最后一行
lines = generated_text.strip().split(&#39;\n&#39;)
if not lines:
return {&quot;error&quot;: &quot;Response is empty or isn&#39;t be split into lines&quot;}

last_line = lines[-1].strip()

# 调试：打印最后一行
print(&quot;Last Line:&quot;, last_line)

return last_line
except Exception as error:
print(&quot;Error duringgeneration:&quot;,error)
traceback.print_exc()

model,tokenizer=load_model()
res=generate_response(&quot;latest products&quot;,tokenizer,model)]]></description>
      <guid>https://stackoverflow.com/questions/78940345/subprocess-calledprocesserror</guid>
      <pubDate>Mon, 02 Sep 2024 11:58:30 GMT</pubDate>
    </item>
    </channel>
</rss>