<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 24 Feb 2025 21:15:46 GMT</lastBuildDate>
    <item>
      <title>多标签分类任务的自动编码器</title>
      <link>https://stackoverflow.com/questions/79463813/autoencoder-for-multi-label-classification-task</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79463813/autoencoder-for-multi-label-classification-task</guid>
      <pubDate>Mon, 24 Feb 2025 14:17:53 GMT</pubDate>
    </item>
    <item>
      <title>LLM是否在令牌集中包含很少使用的单词或字符？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79463243/do-llms-include-very-rarely-used-words-or-characters-in-the-token-set</link>
      <description><![CDATA[我看到LLM几乎用所有语言给出答案，而且我很少看到使用英语词汇以及很少使用的汉字（我本人作为本地语说话的人甚至都不使用该角色）。
我的问题是：
当模型预测接下来的令牌时，它会计算概率分布。但这是多少个令牌的分布？
该概率分布的维度是多少？
如果它包含许多语言中的所有可能的单词或字符，则数组的长度太大。
如果他们使用相对较小的令牌集，那么这些稀有单词和汉字如何在答案中弹出？从这个意义上讲，考虑到许多语言的词汇和字符的数量，即使是令牌的大小也很小。。
他们用来解决此问题的技术方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/79463243/do-llms-include-very-rarely-used-words-or-characters-in-the-token-set</guid>
      <pubDate>Mon, 24 Feb 2025 10:45:19 GMT</pubDate>
    </item>
    <item>
      <title>候选算法算法实施无法正常工作</title>
      <link>https://stackoverflow.com/questions/79463186/candidate-elimination-algorithm-implementation-not-working-as-expected</link>
      <description><![CDATA[我正在尝试实现候选算法，以从数据集中学习概念。但是，我的代码没有产生预期的结果。以下是我的实现和我正在使用的数据集。
 我的代码： 
 将大熊猫作为pd导入
导入numpy作为NP
导入OS，系统

df = pd.read_csv（&#39;chumma.csv; quot;）

val_dict = {
    ＆quot” 0：[[一些，“许多”，“]，”，
    ＆quot“ 1＆quot”：[&#39;small&#39;，&#39;big&#39;，&#39;Medive&#39;]，
    &#39;2&#39;：[&#39;no&#39;]，
    &#39;3&#39;：[“负担得起”，“昂贵”]，
    &#39;4&#39;：[&#39;一个&#39;，&#39;许多&#39;，&#39;少数&#39;]
}

x = df.iloc [：，： -  1]。值
y = df.iloc [：， -  1]。值

s = [[&#39;$&#39;]*5]
对于我的范围（len（y））：
    如果y [i] ==&#39;是&#39;：
        s = x [i] .copy（）
        休息
    
g = [[[＆quot;？]*len（s）]

def is_consistent（x_data，y_data，g_hyp）：
    out_hyp = []
    对于g_hyp中的催眠：
        is_valid = true
        对于我的范围（len（x_data））：
            matches_hyp = true
            对于J范围（Len（Len）（Hyp））：
                如果用演[J]！=;？＆quot;和hyp [j]！= x_data [i] [j]：
                    matches_hyp = false
                    休息
            if（y_data [i] ==&#39;是&#39;而不是匹配的_hyp）或（y_data [i] ==&#39;no&#39;和matches_hyp）：
                is_valid = false
                休息
            
        如果IS_VALID和HYP不在out_hyp中：
            out_hyp.append（hyp）
            
    返回out_hyp
    
    
对于我，实例在枚举（x）中：
    如果y [i] ==&#39;是&#39;：
        对于J范围（LEN（S））的J：
            如果s [j]！=实例[J]：
                s [j] =;？？＆quot;
    别的：
        g_hyp = []
        对于G中的G：
            对于J范围（Len（g））的J：
                如果g [j] ==;
                    d_vals = val_dict [str（j）]
                    对于范围内的k（0，len（d_vals））：
                        如果d_va​​ls [k]！=实例[J]：
                            new_g = g.copy（）
                            new_g [j] = d_vals [k]
                            打印（new_g）
                            g_hyp.append（new_g）
            打印（）
            休息
        g = is_consistent（x [：i+1]，y [：i+1]，g_hyp）
        

打印（“最终特定假设：”，S）
打印（“最终的一般假设：”，G）
 
 数据集（chumma.csv）： 
 引用，尺寸，无盲，价格，版本，购买
有些，小，不，负担得起，一个，没有
许多，大，不，昂贵，很多，是的
许多，中等，不昂贵，很少，是的
有些，小，不，负担得起，一个，没有
许多，大，不，昂贵，很多，是的
许多，中等，不昂贵，很少，是的
 
 预期输出
该算法应输出特定的假设和一般假设（G），该假设代表从数据集中学到的概念
 问题
该代码似乎无法正确更新特定的假设（S）和一般假设（G）。具体：

 s尚未正确地为积极的例子正确概括。
 g无法正确专门用于负面示例。

我实施候选算法是什么问题，我该如何修复代码以正确更新S和G？]]></description>
      <guid>https://stackoverflow.com/questions/79463186/candidate-elimination-algorithm-implementation-not-working-as-expected</guid>
      <pubDate>Mon, 24 Feb 2025 10:26:17 GMT</pubDate>
    </item>
    <item>
      <title>语言模型的问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79463184/probl%c3%a8me-avec-un-mod%c3%a8le-de-langage</link>
      <description><![CDATA[ valueerror：太多值无法打开包装（预期2）
追溯：
file＆quot＆quot c：\ user \ chawk \ pycharmprojects \ pfe.venv \ lib \ lib \ site-packages \ runlit \ runtime \ runtime \ scriptrunner \ exec_code.code.code.py.py＆quote＆quort＆quote＆quort＆quort of exec_func_with_error_error_error_error_error_error_error_error_error_error_error_error_error_error_error_error_handling in
结果= func（）]]></description>
      <guid>https://stackoverflow.com/questions/79463184/probl%c3%a8me-avec-un-mod%c3%a8le-de-langage</guid>
      <pubDate>Mon, 24 Feb 2025 10:24:56 GMT</pubDate>
    </item>
    <item>
      <title>grpotrainer不支持iterabledataset</title>
      <link>https://stackoverflow.com/questions/79462501/iterabledataset-not-supported-on-grpotrainer</link>
      <description><![CDATA[执行时以下程序崩溃
 从数据集导入iterabledataset，数据集
来自TRL Import grpoconfig，grpotrainer

提示= [hi＆quot;
def data_generator（）：
    而真：
        在提示中s：
            产生{提示; ：S}
dataset = iterabledataset.from_generator（data_generator）


triending_args = grpoconfig（
    output_dir =＆quot; tmp＆quort;
    max_steps = 1000，
）

培训师= grpotrainer（
    型号=“ Facebook/opt-350m”
    Reward_funcs = Lambda提示，完成，** Kwargs：[1]*8，
    train_dataset =数据集，
    args =训练_args，
）

Trainer.Train（）
 
导致以下迹线：
  trackback（最近的最新通话）：
  file＆quot＆quort＆quot＆quode/code/code/cs234/starter_code/trl_testing.py&quot;，第24行，in＆lt; module＆gt;
    Trainer.Train（）
  file＆quot＆quort＆quort＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/transformers/trainer.py&amp;py&quot;，第2241号线
    return innion_training_loop（
  file＆quot＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/transformers/trainer.py&quot; line 2500，in _inner_training_training_loop in
    batch_samples，num_items_in_batch = self.get_batch_samples（epoch_iterator，num_batches）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/transformers/trainer.py&amp;py&quot; line 5180，在get_batch_samples中
    batch_samples += [next（epoch_iterator）]
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    next_batch，next_batch_info = self._fetch_batches（main_iterator）
  file＆quot＆quort＆quot＆quot＆quot conda/envs/cs234_3/lib/python3.9/site-packages/accelerate/data_loader.py＆quot＆quot; line 812，in _fetch_batches
    批处理=连接（批次，dim = 0）
  file＆quot＆quort＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot＆quot congatenate in Compatenate in Compatenate
    返回honador_type（data [0]，（condenenate（数据中的d [d [i]），dim = dim = dim）for range（len（data [0]））））））））））））））））
  file＆quot＆quort＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回类型（OBJ）（生成器）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回honador_type（data [0]，（condenenate（数据中的d [d [i]），dim = dim = dim）for range（len（data [0]））））））））））））））））
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回类型（data [0]）（{k：condenate（数据中的d [d [k]），data [0] .keys（）}的k中的k = dim = dim）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    返回类型（data [0]）（{k：condenate（数据中的d [d [k]），data [0] .keys（）}的k中的k = dim = dim）
  file＆quot＆quot＆quot＆quot＆quot＆quot＆quot;
    提高typeerror（f＆quot“只能连接张量，但得到{type（data [0]）};）
typeError：只能连接张量，但可以得到＆lt; class&#39;Str&#39;＆gt;
 
但是，用类似的 dataset 替换 iterabledataset 解决了问题：
 从数据集导入iterabledataset，数据集
来自TRL Import grpoconfig，grpotrainer

提示= [hi＆quot;
dataset = dataset.from_dict（{提示＆quot;：提示}）

triending_args = grpoconfig（
    output_dir =＆quot; tmp＆quort;
    max_steps = 1000，
）

培训师= grpotrainer（
    型号=“ Facebook/opt-350m”
    Reward_funcs = Lambda提示，完成，** Kwargs：[1]*8，
    train_dataset =数据集，
    args =训练_args，
）

Trainer.Train（）
 
这已经在2个截然不同的系统上复制了，因此这不太可能是原因。
我想念什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/79462501/iterabledataset-not-supported-on-grpotrainer</guid>
      <pubDate>Mon, 24 Feb 2025 05:08:12 GMT</pubDate>
    </item>
    <item>
      <title>如何使用FBProphet使用多个回归剂预测值？</title>
      <link>https://stackoverflow.com/questions/79456368/how-to-forecast-values-with-multiple-regressors-using-fbprophet</link>
      <description><![CDATA[我想使用FBProphet模型预测小时温度值。到目前为止，我已经对DS和Y变量进行了培训，这给了我良好的结果。但是现在我想添加额外的回归器，然后执行预测。
将模型与额外的回归器拟合后，我在测试数据集上对其进行了测试，这使我准确。但是主要问题是如何预测未来（我的测试集超出我的测试集）
这是我到目前为止所做的。
 ＃数据准备和功能工程

temp = df [[[＆quot; weverip; quot; quot; quot;]]。应用（kelvintodegc）.copy（）
temp [&#39;hourlylag＆quot;] = temp [温度＆quot＆quot。shift（1）.bfill（）
temp [&#39;dailylag&#39;&#39;] = temp [温度＆quot＆quot。shift（24）.bfill（）
temp [＆quot; weeklylag＆quot;] = temp [温度＆quot＆quot＆quot。shift（24*7）.bfill（）
temp [movmean＆quot;] = temp [温度＆quot＆quot＆quot＆quot;
temp [mstd＆quot;] = temp [温度＆quot＆quot＆quot＆quot;
temp [ub＆quot; quot&#39;] = temp [movmean＆quot;] +（1.6 * temp [mstd;]）
temp [lb＆quot;] = temp [movmean;]  - （1.6 * temp [mstd;]）
temp [＆quot; devfromean＆quot; quot; temp [movemean＆quord; temp [温度＆quort; quort&#39;&#39;]
temp [＆quot; devfromub＆quot;] = temp [ub quot; quot; temp [temp;
temp [devfromlb; quot; quot; quot temp [lb＆quot&#39;]  - 温度；
temp [小时;
temp [&#39;Dayofyear＆quort;] = temp.index.day
temp [; quot; quot; quot＆quot temp.index.month
temp = temp.Reset_index（）
temp.rename（columns = {; date;：＆quord ds; quot; quot; quot; quot; quot; quot; quot; y y y}

模型=先知（）
model.Add_regressor（“ hourlylag”）
model.Add_regressor（“ Dailylag”）
model.Add_regressor（“每周”
model.add_regressor（“ movmean;）
Model.Add_Regressor（“ MSTD”）
model.Add_regressor（&#39;ub＆quot;）
model.add_regressor（&#39;lb＆quot;）
model.add_regressor（“ Devfromean＆quot”）
Model.Add_Regressor（“ DevFromub”）
Model.Add_regressor（&#39;Devfromlb＆quort;）
model.add_regressor（“小时”）
model.Add_regressor（“ Dayofyear”）
model.add_regressor（“月”）

型号（火车）

这些是MAE和MAPE分数
MAE：0.00
Mape：0.17％

现在未来= model.make_future_dataframe（周期= 24 * 365 * 3，freq =; h＆quot;）

我有这个错误

ValueError Trackback（最近的最新电话）
[68]中的单元，第2行
      1未来= model.make_future_dataframe（周期= 8760，freq =; h＆quot;）
----＆gt; 2预测=模型。预定（未来）

文件C：\ USER \ 5923imtiaz \ AppData \ local \ local \ anaconda3 \ envs \ ai \ ai \ lib \ lib \ site-packages \ prophet \ forecaster.py.py.py.py：1270，in Prophet.prophet.predt.predict.predict.predict.predict.predict（self，df，df，vectorized，vectorized）
   1268如果DF.Shape [0] == 0：
   1269提高价值Error（“数据框架没有行。”）
 - ＆gt; 1270 df = self.setup_dataframe（df.copy（））
   1272 DF [&#39;趋势&#39;] = self.predict_trend（df）
   1273 sipersal_components = self.predict_seasonal_components（df）

文件C：\ USER \ 5923imtiaz \ AppData \ local \ local \ anaconda3 \ envs \ ai \ ai \ lib \ lib \ site-packages \ prophet \ forecaster.py.py.py：297，in PropHet.set.setup.dataframe（self，ddf，diredize_scales）
    295在self.extra_regressor中名称：
    296如果不在DF中的名字：
 - ＆gt; 297提高价值Error（
    298&#39;回归器{name！r} dataframe中缺少
    299 .format（名称=名称）
    300）
    301 df [name] = pd.to_numeric（df [name]）
    302如果DF [name] .isnull（）。任何（）：

valueerror：dataFrame中缺少回归器“小时lag”
 ]]></description>
      <guid>https://stackoverflow.com/questions/79456368/how-to-forecast-values-with-multiple-regressors-using-fbprophet</guid>
      <pubDate>Fri, 21 Feb 2025 04:48:53 GMT</pubDate>
    </item>
    <item>
      <title>为什么拥抱面提供的DeepSeek代码会导致“未知量化类型”错误？</title>
      <link>https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t</link>
      <description><![CDATA[我正在使用huggingface的此代码：
此代码直接从 deepseek上的huggingface网站页面上要插件代码：

 来自变形金刚导入管道

消息= [
{&#39;&#39;：＆quot“ user quot”内容“：;
这是给出的
pipe =管道（＆quot&#39;text-generation＆quot; deepseek-ai/deepseek-r1＆quort; trust_remote_code = true）
管道（消息）
 

，但我无法加载模型。当我这样做时，我会得到这个问题：

 file＆quot＆lt; ...＆gt;/site-packages/transformers/quantizers/auto.py&quot;，第97行，in_dict 
 提高ValueError（

ValueError：未知量化类型，获得FP8-支持类型为： 
[&#39;awq&#39;，&#39;bitsandbytes_4bit&#39;，&#39;bitsandbytes_8bit&#39;，&#39;gptq&#39;，&#39;aqlm&#39;，&#39;quanto&#39;，&#39;eetq&#39;，&#39;eetq&#39;， 
&#39;HQQ&#39;，“压缩张量”，“ fbgemm_fp8&#39;，&#39;torchao&#39;，&#39;bitnet&#39;]
 

我尝试了不同的代码：
 导入火炬
generate_text = pipeline（model =; deepSeek-ai/deepSeek-r1; torch_dtype = torch.bfloat16，trust_remote_code = true，device_map =; auto;
generate_text（消息）
 
这给出以下错误：

提高ValueError（valueError：未知量化类型，获得FP8-支持类型为：[&#39;awq&#39;，&#39;bitsandbytes_4bit&#39;，&#39;bitsandbytes_8bit&#39;，gptq&#39;，&#39;gptq&#39;，&#39;aqlm&#39;&#39;aqlm&#39;&#39;，&#39;aqlm&#39;，&#39; &#39;，&#39;hqq&#39;，&#39;compressed Tensors&#39;，&#39;fbgemm_fp8&#39;， &#39;torchao&#39;，&#39;bitnet&#39;，&#39;vptq&#39;] 

我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t</guid>
      <pubDate>Sun, 09 Feb 2025 03:05:30 GMT</pubDate>
    </item>
    <item>
      <title>我如何成功设置和检索元数据信息以在Huggingface Hub上的HuggingFacedatAset？</title>
      <link>https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat</link>
      <description><![CDATA[我有许多数据集，我是从诸如此类的字典中创建的：
  info = datasetinfo（
        Description =&#39;我的快乐LIL数据集
        版本=; 0.0.1＆quot;
        homepage =＆quot; https：//www.myhomepage.co.uk＆quot;
    ）
train_dataset = dataset.from_dict（prepary_data（data [＆quot; train;]），info = info）
test_dataset = dataset.from_dict（prepary_data（数据[test; test;]），info = info）
验证_DATASET = DATASET.FROM_DICT（prepary_data（data [data [＆quot; quartation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quotation; quote = info = info）
 
 i然后将它们集合到数据集中。
 ＃创建一个datasetDict
dataset = datasetDict（
    {train＆quort＆quot; train_dataset，&#39;test;：test_dataset，;
）
 
到目前为止，一切都很好。如果我访问 dataset [&#39;train&#39;]。info.description 我看到“我的快乐lil dataset”的预期结果。
所以我推到轮毂上，就像：
  dataset.push_to_hub（f＆quot {agrompome}/{repo_name}＆quits＆quits_message =＆quort; some some commin
 
这也成功了。
但是，当我来将数据集从集线器中拉回并访问与之关联的信息时，而不是获取数据集的描述时，我只会得到一个空字符串；喜欢：
  pulled_data = full = load_dataset（＆quot; f {agrommy}/{repo_name}＆quort＆quort; use_auth_token = true）

＃我希望以下内容打印出来“我的快乐LIL数据集”。
print（pulled_data [&#39;train;]。info.Description）
＃但是，它返回&#39;&#39;
 
我是否错误地从集线器加载数据？我是只推出数据集而不是以某种方式推出信息吗？
我觉得我缺少一些明显的东西，但我真的不确定。]]></description>
      <guid>https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat</guid>
      <pubDate>Wed, 17 Jul 2024 13:23:04 GMT</pubDate>
    </item>
    <item>
      <title>如何在NLTK中下载Punkt Tokenizer？</title>
      <link>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</link>
      <description><![CDATA[我使用安装了NLTK库
  PIP安装NLTK
 
使用lib 
 来自nltk.tokenize导入send_tokenize 
send_tokenize（文本）
 
我遇到此错误
  lookuperror： 
****************************************************** ********************
  找不到资源朋克。
  请使用NLTK下载器获取资源：

  ＆gt;＆gt;＆gt;导入NLTK
  ＆gt;＆gt;＆gt; nltk.download（&#39;punkt&#39;）
  
  有关更多信息，请参见：https：//www.nltk.org/data.html

  尝试加载dokenizers/punkt/English.pickle

  搜索：
     - &#39;c：\\用户\\ adars/nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ share \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\ local \\ program \\ python \\ python310 \\ lib lib \\ nltk_data&#39;
     - &#39;c：\\用户\\ adars \\ appdata \\漫游\\ nltk_data&#39;
     - &#39;c：\\ nltk_data&#39;
     - &#39;d：\\ nltk_data&#39;
     - &#39;e：\\ nltk_data&#39;
     - &#39;&#39;&#39;
 
因此，为了解决此错误，我尝试了
 导入NLTK
nltk.download（&#39;punkt&#39;）
 
但是我无法下载此软件包，因为每次运行时，我都会收到错误的错误
  [nltk_data]错误加载punkt：＆lt; urlopen错误[WinError 10060] a
[nltk_data]连接尝试失败，因为连接的聚会
[nltk_data]一段时间后没有正确响应，或者
[nltk_data]建立的连接失败，因为连接的主机
[nltk_data]未能响应＆gt;
 
请在这里帮助我]]></description>
      <guid>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</guid>
      <pubDate>Tue, 19 Sep 2023 04:36:59 GMT</pubDate>
    </item>
    <item>
      <title>在微调过程中，如何正确设置垫子令牌（不是EOS），以避免模型不预测EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch Runtimeerror：Mat1＆Mat2形状无法乘以</title>
      <link>https://stackoverflow.com/questions/75693007/pytorch-runtimeerror-mat1-mat2-shapes-cannot-be-multiplied</link>
      <description><![CDATA[我正在Pytorch上建立CNN并收到以下错误消息：

 RuntimeError：MAT1和MAT2形状无法乘以（32x32768和
512x256）

我已经构建了以下模型：
  def classifier_block（输入，输出，kernel_size，stride，last_layer = false）：
  如果不是last_layer：
    x = nn。
        nn.conv2d（输入，输出，kernel_size，大步，填充= 3），
        nn.batchnorm2d（输出），，
        nn.leakyrelu（0.2，intplophe = true）
    ）
  别的：
    x = nn。
        nn.conv2d（输入，输出，kernel_size，大步），
        nn.maxpool2d（kernel_size = 3，步幅= 2，填充= 1）
    ）
  返回x

类分类器（nn.module）：
  def __init __（self，input_dim，输出）：
    超级（分类器，self）.__ init __（）
    self.classifier = nn。
        classifier_block（input_dim，64、7、2），
        classifier_block（64、64、3、2），
        classifier_block（64、128、3、2），
        classifier_block（128，256，3，2），
        classifier_block（256，512，3，2，true）
    ）
    打印（&#39;clf：&#39;，self.classifier）
    
    self.linear = nn.Sequepention（
        nn.linear（512，256），
        nn.relu（inplace = true），
        nn.linear（256，128），
        nn.relu（inplace = true），
        nn.linear（128，64），
        nn.relu（inplace = true），
        nn.linear（64，输出）
    ）
    打印（&#39;linear：&#39;，self.linear）
  
  向前（自我，图像）：
    打印（&#39;img：&#39;，image.shape）
    x = self.classifier（图像）
    打印（&#39;X：&#39;，X.Shape）
    返回self.linear（x.View（len（x），-1））
 
输入图像是大小 512x512 。这是我的训练障碍：
  loss_train = []
loss_val = []

对于范围（时期）的时期：
  print（&#39;epoch：{}/{}&#39;。格式（epoch，epochs））
  total_train = 0
  CRORCE_TRAIN = 0
  cumloss_train = 0
  classifier.train（）
  对于枚举（x，y）的批次（train_loader）：
    x = x.to（设备）
    打印（X.Shape）
    打印（y.形）
    输出=分类器（x）
    损失=标准（输出，y.to（设备））
    优化器.zero_grad（）
    loss.backward（）
    优化器.step（）

    打印（&#39;损失：{}&#39;。格式（损失））
 
任何建议都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/75693007/pytorch-runtimeerror-mat1-mat2-shapes-cannot-be-multiplied</guid>
      <pubDate>Fri, 10 Mar 2023 06:48:16 GMT</pubDate>
    </item>
    <item>
      <title>如何在虚线文本验证码中找到轮廓图像</title>
      <link>https://stackoverflow.com/questions/71261999/how-to-find-contours-in-dotted-text-captcha-image</link>
      <description><![CDATA[我是OpenCV的新手。我正在尝试找到验证码图像的轮廓。仅当我的验证码包含虚线文本时，它不起作用。
我已经完成了以下代码：
 导入numpy作为NP
导入CV2作为CV
导入imgaug.augmenters为IAA

im = cv.imread（&#39;dataset/1.jpg&#39;）
imgray = cv.cvtcolor（im，cv.color_bgr2gray）

imgray = cv.threshold（Imgray，127，255，0）[1]

dst = cv.canny（imgray，0,150）
bluret = cv.blur（dst，（5,5），0）
img_thresh = cv.AdaptivEthreshold（Blud，255，cv.Adaptive_thresh_gaussian_c，cv.thresh_binary_inv，11，2）

内核= cv.getStructuringElement（cv.morph_rect，（3,3））
阈值= cv.morphologyex（img_thresh，cv.morph_close，kernel）

轮廓，层次结构= cv.findcontours（dst，cv.retr_tree，cv.chain_approx_simple）
打印（Len（Contours））
＃cv.drawContours（IM，轮廓，-1，（0，255，0），3）

cv.imshow（“ img_thresh”，img_thresh）
cv.imshow（dst&#39;dst）
cv.imshow（“阈值”，阈值）
CV.Waitkey（0）
cv.destroyallwindows（）
 
有人可以帮忙吗？有什么方法可以在此图像中找到轮廓？
  ]]></description>
      <guid>https://stackoverflow.com/questions/71261999/how-to-find-contours-in-dotted-text-captcha-image</guid>
      <pubDate>Fri, 25 Feb 2022 06:39:32 GMT</pubDate>
    </item>
    <item>
      <title>如何找到稀疏矢量的最近邻居</title>
      <link>https://stackoverflow.com/questions/34611337/how-to-find-the-nearest-neighbor-of-a-sparse-vector</link>
      <description><![CDATA[我有大约500个向量，每个向量是1500维矢量，
几乎每个向量都很稀疏 - 我的意思是，矢量的30-70维度不是0。
现在，问题在于，这里是一个给定的向量，也是1500个维度，我需要将其与500个向量进行比较，以查找500个最接近的矢量。（在Euclidean距离中）。。
毫无疑问，蛮力方法是一种解决方案，但是我需要计算500次的距离，这需要很长时间。
昨天，我读了一篇文章“用大词汇和快速的空间匹配”的文章，它说使用倒置索引会有所帮助，它说：
   
但是，在我的测试之后，几乎没有任何意义，想象一个1500矢量，其中50个尺寸并不为零，当涉及另一个尺寸时，它们可能总是具有相同的尺寸，而不是零。换句话说，这种算法只能排除一个小矢量，我仍然需要与剩下的许多向量进行比较。
我的问题：

 此算法是否有意义？

 还有其他方法可以做我想做的事吗？例如Flann或KD-Tree？
但是我想要精确的准确的最近邻居，大约是一个不够的

]]></description>
      <guid>https://stackoverflow.com/questions/34611337/how-to-find-the-nearest-neighbor-of-a-sparse-vector</guid>
      <pubDate>Tue, 05 Jan 2016 12:07:01 GMT</pubDate>
    </item>
    <item>
      <title>数据归一化[关闭]</title>
      <link>https://stackoverflow.com/questions/21554301/data-normalization</link>
      <description><![CDATA[当我想分类“好”时或“最佳”然后，我可以使用Facebook的计数或Twitter转发计数的计数。
但是有些社区的用户群很大，因此他们的链接获得了更多的喜欢或转发。我该如何“归一化”这些巨大的社区喜欢例如，像count这样的小得多的社区的类似新闻项目链接之类的链接？
这被称为正常化吗？我可以在哪种书籍中学习有关“质量”的这类算法。 （例如，在这种情况下）？无论如何，我想做什么？]]></description>
      <guid>https://stackoverflow.com/questions/21554301/data-normalization</guid>
      <pubDate>Tue, 04 Feb 2014 13:47:45 GMT</pubDate>
    </item>
    <item>
      <title>数据归一化的参考文献[关闭]</title>
      <link>https://stackoverflow.com/questions/5652357/references-for-data-normalization</link>
      <description><![CDATA[对于NNS和其他机器学习算法，将数据标准化（不确定是否正确）的最佳实践是什么？  我的意思是您如何表示NN/Algo的数据。
例如，您如何表示商店代码？  商店555不大于或小于554，它只是一个分类。 NNS/ALGO模型只是单独过滤出来，还是您需要使它们进行分类而不是数学上的区别？
 编辑：感谢大家的答案。  我一直在挖掘很多数据挖掘书，尽管我发现了一些在预处理的数据主题上花了一两章的时间，但我对最掩饰的效果最大感到有些惊讶。  再次感谢。]]></description>
      <guid>https://stackoverflow.com/questions/5652357/references-for-data-normalization</guid>
      <pubDate>Wed, 13 Apr 2011 16:17:41 GMT</pubDate>
    </item>
    </channel>
</rss>