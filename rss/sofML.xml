<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 13 Aug 2024 12:30:33 GMT</lastBuildDate>
    <item>
      <title>我在尝试使用 Hampel 进行 ECG 信号分析时，在下面的代码中出现了 IndexError</title>
      <link>https://stackoverflow.com/questions/78865609/i-am-getting-indexerror-in-below-code-where-i-am-trying-to-use-hampel-for-ecg-si</link>
      <description><![CDATA[代码：
signal = signal - np.mean(signal)
# 从结果对象中提取过滤后的数据
outlier_indices = hampel(pd.Series(signal), window_size=100).filtered_data 
peaks = [list(map(itemgetter(1), g)) for k, g in groupby(enumerate(outlier_indices), lambda x: x[0] - x[1])]

# 在使用索引之前，先将其转换为整数
peaks_max_vals = [peaks[i][np.argmax(abs(signal[np.round(peaks[i]).astype(int)]))] for i in range(len(peaks))] 

peaks_sign = np.sign(signal[peaks_max_vals])

diffs_max = np.where(np.diff(peaks_max_vals) &lt; 40)[0]

zipped_list = list(zip(signal[peaks_max_vals], peaks_sign, peaks_max_vals))

to_remove = []

for i in diffs_max:
a = i + 1
if zipped_list[i][0] &gt; zipped_list[a][0] and zipped_list[i][1] == zipped_list[a][1]:
to_remove.append(a)
elif zipped_list[i][0] &lt; zipped_list[a][0] 和 zipped_list[i][1] == zipped_list[a][1]:
to_remove.append(i)

for index in sorted(to_remove, reverse=True):
peaks_max_vals.pop(index)

错误：

IndexError Traceback (most recent call
last) in &lt;cell line: 9&gt;()
7 peaks_max_vals = [peaks[i][np.argmax(abs(signal[np.round(peaks[i]).astype(int)]))] for
i in range(len(peaks))]
8
----&gt; 9 peaks_sign = np.sign(signal[peaks_max_vals])
10
11 diffs_max = np.where(np.diff(peaks_max_vals) &lt; 40)[0]
IndexError：只有整数、切片 (:)、省略号 (...)、numpy.newaxis (None) 和整数或布尔数组才是有效索引
]]></description>
      <guid>https://stackoverflow.com/questions/78865609/i-am-getting-indexerror-in-below-code-where-i-am-trying-to-use-hampel-for-ecg-si</guid>
      <pubDate>Tue, 13 Aug 2024 10:01:02 GMT</pubDate>
    </item>
    <item>
      <title>如何使用文本数据集训练模型</title>
      <link>https://stackoverflow.com/questions/78865593/how-to-train-a-model-using-a-text-dataset</link>
      <description><![CDATA[我想创建一个生成文本的 AI 模型。具体来说，BDD Gherkin 黄瓜场景和步骤定义基于用户故事的输入。
带有 BDD Gherkin 黄瓜示例的用户故事
例如。
用户故事（输入）：我想在电子商务网站上将产品添加到我的购物篮中进行购买。
输出：自动创建测试用例场景和步骤定义
测试用例场景：

场景 1：验证用户是否可以将一个商品添加到购物车
场景 2：验证用户是否可以从购物车中移除一个商品

测试用例场景 1：

假设用户使用和启动并登录电子商务应用程序
然后用户导航到商品页面。
然后用户选择并单击。
然后用户单击“添加到购物车”按钮。
然后用户应导航到购物车页面。
然后用户应验证购物车页面已成功添加。

测试用例场景 2：

假设用户使用 启动并登录电子商务应用程序 
然后用户应导航到购物车页面。
然后用户在购物车中找到并单击“从购物车中移除”按钮。
然后用户应验证购物车已成功移除。

我创建了一个示例数据集，其中包含映射到场景和步骤定义的用户故事。
数据集
就我目前的理解，逻辑是：我想基于现有用户故事和场景的数据集训练一个模型。在模型训练完成后，我想输入一个用户故事，模型应该提出一个带有步骤定义的合适场景。
我是机器学习的新手，只做过某种形式的监督学习、回归。从一些研究中，我需要使用一些 NLP 技术来处理数据集。从那时起，我就很迷茫。我看到一些人谈论使用 ChatGPT 来训练数据集之类的东西。
做这个项目的好方法是什么。
本质上，我想找出如何使用文本训练模型，以便模型可以接收文本并输出文本。]]></description>
      <guid>https://stackoverflow.com/questions/78865593/how-to-train-a-model-using-a-text-dataset</guid>
      <pubDate>Tue, 13 Aug 2024 09:59:03 GMT</pubDate>
    </item>
    <item>
      <title>HuggingFace 运行评估时加速设备错误</title>
      <link>https://stackoverflow.com/questions/78865570/huggingface-accelerate-device-error-when-running-evaluation</link>
      <description><![CDATA[我正在多 GPU 集群上运行一些实验，并且正在使用加速。我试图在训练数据加载器中每次批量迭代后计算一些指标。虽然训练代码似乎使用加速运行良好（它利用多个 GPU），但在尝试计算上述指标时我遇到了错误。似乎在执行前向传递后，评估输出张量时将其放在与输入张量不同的设备上。给出错误的代码如下：
def calculatePerplexity(sentence, model, tokenizer, accelerater):
&quot;&quot;&quot;
exp(loss)
&quot;&quot;&quot;
... input_ids = torch.tensor(sentence).unsqueeze(0)
print(f&quot;输入 ids 设备：{input_ids.device}&quot;)
model.eval()
with torch.no_grad():
output = model(input_ids, labels=input_ids)
output = accelerater.gather_for_metrics(outputs)
loss, logits = output[:2]
loss, logits = accelerater.prepare(loss, logits)
print(f&quot;损失设备：{loss.device}&quot;)
print(f&#39;模型设备：{model.device}&#39;)
print(f&#39;Logits 设备：{logits.device}&#39;)

probabilities = torch.nn. functional.softmax(logits, dim=-1)
all_prob = []
input_ids_processed = input_ids[0][1:]
for i, token_id in enumerate(input_ids_processed):
probability = probabilities[0, i, token_id].item()
all_prob.append(probability)

# 用于度量计算的内容
probs = torch.nn. functional.softmax(logits[0, :-1], dim=-1)
log_probs = torch.nn. functional.log_softmax(logits[0, :-1], dim=-1)
token_log_probs = log_probs.gather(dim=-1, index=input_ids_processed.unsqueeze(-1)).squeeze(-1)
mu = (probs * log_probs).sum(-1)
sigma = (probs * torch.square(log_probs)).sum(-1) - torch.square(mu)
mink_plus = (token_log_probs - mu) / sigma.sqrt()

调试语句的输出如下：
输入 ids 设备：cpu
丢失设备：cuda：0
模型设备：cpu
Logits 设备：cuda：0

对于我使用的 4 个不同的 GPU，情况都一样，这会导致以下错误：
token_log_probs = log_probs.gather(dim=-1, index=input_ids_processed.unsqueeze(-1)).squeeze(-1)
[rank2]: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: RuntimeError：预期所有张量都在同一设备上，但发​​现至少两个设备，cuda:0 和 cpu！（在方法 wrapper_CUDA_gather 中检查参数索引的参数时）

我不确定我在这里做错了什么，我认为使用 gather_for_metrics 方法或在损失和对数上调用 accelerator.prepare 会有所帮助，但事实并非如此（删除这些语句时我收到相同的错误）。任何建议都将不胜感激！
为了完整起见，以下是我在计算指标时使用的其余（相关）代码：
# 训练循环
for i, (batch_inputs, batch_labels) in tqdm(enumerate(dataloader)):
all_labels += batch_labels

unlearned_model, tokenizer = load_base_model(self.experiment_args.model_dir_prefix, self.experiment_args.model)
torch.cuda.empty_cache()

optimizer = torch.optim.Adam(unlearned_model.parameters(), lr=self.unlearning_args.lr)
unlearned_model, optimizer, batch_inputs = accelerater.prepare(unlearned_model, optimizer, batch_inputs)
# 取消学习数据并计算 PPL 值
for i in range(self.unlearning_args.steps):
unlearned_model = unlearn_dataslice(unlearned_model, optimizer, batch_inputs, self.unlearning_args, accelerater)

torch.cuda.empty_cache()
UL_PPL_vals += calculate_PPL_values(unlearned_model, tokenizer, batch_inputs, accelerater)

def unlearn_dataslice(model, optimizer, sentences, args, accelerater):
learning_rate = args.lr
model.train()

optimizer.zero_grad()
input_data = sentences.clone().detach()

output = model(input_data)
# 添加一个减号，使梯度上升而不是下降
loss = -output[0][&#39;logits&#39;]
accelerater.backward(loss.mean())
torch.cuda.empty_cache()
optimizer.step()
del optimizer
torch.cuda.empty_cache()
返回模型

def calculate_PPL_values(model, tokenizer, text_batch, accelerater):
PPL_values = []
for text in text_batch:
PPL = calculatePerplexity(text, model, tokenizer, accelerater)[0]
PPL_values.append(PPL)
返回 PPL_values

在此代码中，我删除了许多调试语句，这些语句检查了 unlearned_model 和 batch_inputs 的设备在整个训练循环中是否位于同一设备 cpu 上，因此我很确定那里没有不一致之处。]]></description>
      <guid>https://stackoverflow.com/questions/78865570/huggingface-accelerate-device-error-when-running-evaluation</guid>
      <pubDate>Tue, 13 Aug 2024 09:54:19 GMT</pubDate>
    </item>
    <item>
      <title>如何在 CPU 上使用 Meta 的 MusicGen 加速音乐生成？初学者最实惠的选择是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/78864824/how-can-i-speed-up-music-generation-with-metas-musicgen-on-a-cpu-best-affordab</link>
      <description><![CDATA[我目前有一个微服务，它使用 Audiocraft 的 MusicGen (Meta) 根据文本输入生成音乐。当我在没有 GPU 的本地机器上运行该服务时，生成音乐大约需要 9 分钟。我研究过使用带 GPU 的 AWS EC2 实例、支持 NVIDIA GPU 的 Docker 等选项，但我不确定对于不想在这个副业上花太多钱的初学者来说哪个是最佳选择。关于如何经济高效地加快这一过程，有什么建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78864824/how-can-i-speed-up-music-generation-with-metas-musicgen-on-a-cpu-best-affordab</guid>
      <pubDate>Tue, 13 Aug 2024 07:02:47 GMT</pubDate>
    </item>
    <item>
      <title>如何更新已经微调的 GPT2 模型</title>
      <link>https://stackoverflow.com/questions/78864545/how-to-update-already-finetuned-model-of-gpt2</link>
      <description><![CDATA[我已经使用 Transformers GPT-2 针对我的数据集训练了我的模型。我已经训练过并给出了正确的输出。现在我想在保留之前训练过的模型的同时，在更多数据上训练我的模型。我不想创建新的模型，只想使用更新的数据训练之前的模型。
我尝试在训练中使用“overwrite = False”，但我不知道它是否会起作用。]]></description>
      <guid>https://stackoverflow.com/questions/78864545/how-to-update-already-finetuned-model-of-gpt2</guid>
      <pubDate>Tue, 13 Aug 2024 05:44:48 GMT</pubDate>
    </item>
    <item>
      <title>我可以在哪里开始在机器人技术中实施机器学习？[关闭]</title>
      <link>https://stackoverflow.com/questions/78864202/where-can-i-get-started-to-implement-machine-learning-in-robotics</link>
      <description><![CDATA[最近我学习了机器学习，解决了一堆分类和回归问题。现在我想运用我的知识来构建一个真正的物理机器人，它能够自行行驶并使用超声波传感器避开障碍物。我可以从哪里开始，或者我到底需要知道什么才能使用传感器值训练我的模型并通过机器学习模型控制我的机器人。
我曾尝试利用超声波传感器值和训练模型，但不知道该如何做，然后使用这个训练过的模型来真正让机器人移动。]]></description>
      <guid>https://stackoverflow.com/questions/78864202/where-can-i-get-started-to-implement-machine-learning-in-robotics</guid>
      <pubDate>Tue, 13 Aug 2024 02:50:52 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Scikit-Learn 为分类变量选择参考水平？</title>
      <link>https://stackoverflow.com/questions/78864148/how-to-select-a-reference-level-for-categorical-variables-using-scikit-learn</link>
      <description><![CDATA[我正在尝试将代码从 SAS 转换为训练 GLM 的 Python。为此，我使用带有 CLASS 字的 hpgenselect 来处理分类变量。在 SAS 中，我可以选择模型所需的任何分类变量的参考级别。参考级别是将接收值 0 的类别，并且将成为其他变量进行比较的参考。
我搜索过，但没有在 Scikit-Learn 中找到类似的方法。有没有办法在 scikit-learn 或其他 Python 库中选择参考？]]></description>
      <guid>https://stackoverflow.com/questions/78864148/how-to-select-a-reference-level-for-categorical-variables-using-scikit-learn</guid>
      <pubDate>Tue, 13 Aug 2024 02:19:55 GMT</pubDate>
    </item>
    <item>
      <title>我的 ML 二元分类模型的最终预测准确率非常低</title>
      <link>https://stackoverflow.com/questions/78863903/final-predictions-accuracy-of-my-ml-binary-classification-model-is-horrible</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78863903/final-predictions-accuracy-of-my-ml-binary-classification-model-is-horrible</guid>
      <pubDate>Mon, 12 Aug 2024 23:32:18 GMT</pubDate>
    </item>
    <item>
      <title>我们如何运行 Apple 雪貂模型？</title>
      <link>https://stackoverflow.com/questions/78863794/how-do-we-run-the-apple-ferret-model</link>
      <description><![CDATA[我已经安装了苹果雪貂模型所需的所有权重和检查点，除了运行 3 个终端并测试演示之外，我如何在本地运行模型来为图像文件夹添加标题？
谢谢！
我尝试了https://vivekupadhyay1.medium.com/how-to-use-ferret-apples-open-source-multimodal-llm-for-your-next-project-c561f0087a5d，但无法找到 github repo 中提到的 eval.py 或脚本。]]></description>
      <guid>https://stackoverflow.com/questions/78863794/how-do-we-run-the-apple-ferret-model</guid>
      <pubDate>Mon, 12 Aug 2024 22:36:25 GMT</pubDate>
    </item>
    <item>
      <title>为什么我不能用 C++ 构建一个没有依赖关系的神经网络，即使它可以在 Numpy 中运行？</title>
      <link>https://stackoverflow.com/questions/78862784/why-cant-i-build-a-neural-network-in-c-with-no-dependencies-even-though-it-w</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78862784/why-cant-i-build-a-neural-network-in-c-with-no-dependencies-even-though-it-w</guid>
      <pubDate>Mon, 12 Aug 2024 16:55:51 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 nltk 函数</title>
      <link>https://stackoverflow.com/questions/78862426/unable-to-use-nltk-functions</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78862426/unable-to-use-nltk-functions</guid>
      <pubDate>Mon, 12 Aug 2024 15:17:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在 HuggingFace 中从头开始重新初始化 GPT2 XL？</title>
      <link>https://stackoverflow.com/questions/78859343/how-to-reinitialize-from-scratch-gpt2-xl-in-huggingface</link>
      <description><![CDATA[我试图确认我的 GPT-2 模型是从头开始训练的，而不是使用任何预先存在的预训练权重。这是我的方法：

加载预训练的 GPT-2 XL 模型：我使用 AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;) 加载预训练的 GPT-2 XL 模型，并计算此模型权重的总 L2 范数。
从头开始初始化新的 GPT-2 模型：然后我使用 GPT2Config 从头开始​​使用自定义配置初始化新的 GPT-2 模型。
比较 L2 范数：我计算预训练模型和新初始化模型的权重的 L2 范数。我的假设是，如果临时模型确实是从随机权重初始化的，那么临时模型的 L2 范数应该比预训练模型小得多。

这是代码片段：
import torch
from transformers import GPT2LMHeadModel, GPT2Config, AutoModelForCausalLM

# 步骤 1：加载预训练的 GPT-2 XL 模型
pretrained_model = AutoModelForCausalLM.from_pretrained(&quot;gpt2-xl&quot;)

# 步骤 2：计算预训练模型权重的 L2 范数
pretrained_weight_norm = 0.0
for param in pretrained_model.parameters():
pretrained_weight_norm += torch.norm(param, p=2).item()

print(f&quot;Total L2预训练模型权重的范数：{pretrained_weight_norm:.2f}&quot;)

# 步骤 3：使用自定义配置从头开始初始化新的 GPT-2 模型
config = GPT2Config(
vocab_size=52000, # 确保这与 tokenizer 的词汇量相匹配
n_ctx=1024, # 上下文窗口大小（模型一次可以看到的 token 数量）
bos_token_id=0, # 序列开始 token
eos_token_id=1, # 序列结束 token
)
model = GPT2LMHeadModel(config)

# 步骤 4：计算刚初始化的模型权重的 L2 范数
scratch_weight_norm = 0.0
for param in model.parameters():
scratch_weight_norm += torch.norm(param, p=2).item()

print(f&quot;从头开始初始化的模型的总 L2 范数：{scratch_weight_norm:.2f}&quot;)

这种方法是否是确认模型是从头开始训练的有效方法？是否存在任何潜在问题或更好的方法来验证模型没有预先存在的学习权重？
看起来正确
~/beyond-scale-language-data-diversity$ /opt/conda/envs/beyond_scale_div_coeff/bin/python /home/ubuntu/beyond-scale-language-data-diversity/playground/test_gpt2_pt_vs_reinit_scratch.py​​
config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 689/689 [00:00&lt;00:00，8.05MB/s]
model.safetensors：100%|██████████████████████████████████████████████████████████████████████████| 6.43G/6.43G [00:29&lt;00:00，221MB/s]
generation_config.json：100%|██████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00&lt;00:00，1.03MB/s]
预训练模型权重的总 L2 范数：24542.74
从头初始化模型的总 L2 范数：1637.31
（beyond_scale_div_coeff）

cross: https://discuss.huggingface.co/t/how-to-reinitialize-from-scratch-gpt-xl-in-hugging-face-hf/101905
ref: https://github.com/alycialee/beyond-scale-language-data-diversity/issues/18]]></description>
      <guid>https://stackoverflow.com/questions/78859343/how-to-reinitialize-from-scratch-gpt2-xl-in-huggingface</guid>
      <pubDate>Sun, 11 Aug 2024 20:27:07 GMT</pubDate>
    </item>
    <item>
      <title>机器学习（GAN）生成图像</title>
      <link>https://stackoverflow.com/questions/78859294/machine-learning-gan-to-generate-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78859294/machine-learning-gan-to-generate-images</guid>
      <pubDate>Sun, 11 Aug 2024 20:01:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv8 进行大量错误检测</title>
      <link>https://stackoverflow.com/questions/78820748/alot-of-incorrect-detection-using-yolov8</link>
      <description><![CDATA[我尝试使用 Visual Code Studio 运行 YOLOv8。安装了 ultralytics 并在 vs code 终端上运行了 yolo predict model=yolov8n.pt source=&#39;https://ultralytics.com/images/bus.jpg&#39;。
但是我收到的输出是
2 个人、1 辆自行车、5 辆汽车、10 辆摩托车、73 艘船、3 个停车标志、1 只狗、10 匹马、10 头牛、32 只熊、1 只长颈鹿、63 把雨伞、6 个手提包、9 个飞盘、15 块滑雪板、5 块冲浪板、12 把刀、5 张床、37 张餐桌

这些显然不是这张图片的一部分。

当我第一次安装 ultralytics 并尝试运行 torch 时，出现了缺少依赖项的错误。fbgemm.ddl 丢失。后来，当我安装 vs_BuildTools 时，这个问题得到了解决。然后我继续在虚拟环境中运行代码，其中使用 torch 的程序运行没有任何错误。然后我继续输入此代码片段并遇到此问题。我也尝试使用命令提示符和 jupyter 笔记本运行，但同样的问题仍然存在。
我也检查了版本是否兼容，结果是兼容的。我还没有安装 cuda，是因为这个原因还是还有其他我不知道的问题？请有人帮忙。]]></description>
      <guid>https://stackoverflow.com/questions/78820748/alot-of-incorrect-detection-using-yolov8</guid>
      <pubDate>Thu, 01 Aug 2024 11:33:58 GMT</pubDate>
    </item>
    <item>
      <title>优化大数据集上的 Pandas 性能</title>
      <link>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</link>
      <description><![CDATA[我正在使用 pandas 处理一个大型数据集（约 1000 万行和 50 列），在数据操作和分析过程中遇到了严重的性能问题。这些操作包括过滤、合并和聚合数据，目前执行时间太长。
我读过几种优化技术，但不确定哪种技术最有效且适用于我的情况。以下是有关我的工作流程的一些细节：
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台具有 16GB RAM 的机器上运行分析。
社区能否分享优化 pandas 在大型数据集上的性能的最佳实践？
1.内存管理技术。
2.执行 groupby 和 apply 的有效方法。
3.处理大型数据集的 pandas 替代方案。
4. 有没有关于并行处理或有效利用多核的技巧。
我主要使用 pandas 进行数据清理、转换和分析。
我的操作包括多个 groupby 和 apply 函数。
我在一台有 16GB RAM 的机器上运行分析。]]></description>
      <guid>https://stackoverflow.com/questions/78752483/optimizing-pandas-performance-on-large-datasets</guid>
      <pubDate>Tue, 16 Jul 2024 02:24:48 GMT</pubDate>
    </item>
    </channel>
</rss>