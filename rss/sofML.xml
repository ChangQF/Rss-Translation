<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 06 Feb 2024 18:16:05 GMT</lastBuildDate>
    <item>
      <title>从头开始机器学习的反向传播方法神经网络中的错误</title>
      <link>https://stackoverflow.com/questions/77949922/error-in-backpropagation-method-neural-network-from-scratch-machine-learning</link>
      <description><![CDATA[我一直在做这个项目，试图制作一个预测加密货币价格的人工智能，现在我在反向传播方法中遇到了这个持续存在的错误（特别是关于 np.dot 语句中数组的规模）用于计算新的权重），我认为这可能是由于函数造成的，但我不知道如何纠正它。
`类 Layer_Dense：
def init(self, n_neurons, 权重, 偏差):
self.n_neurons = np.array(n_neurons)
self.weights = np.array(权重)
self.biases = np.array(偏差)
def 前向（自身，输入）：
self.inputs = np.array(输入)
self.output = np.dot(self.weights, 输入) + self.biases
def 反向传播（自身，梯度，学习率 = 0.01）：
梯度 = np.array(梯度)
# 计算相对于权重和偏差的梯度
weights_gradient = np.dot(梯度, self.inputs.T) / len(self.inputs)
biases_gradient = np.sum(梯度) / len(self.inputs)
 # 使用某种优化算法（例如梯度下降）更新权重和偏差
    self.weights -= 学习率 * 权重梯度
    self.biases -= 学习率 *biases_gradient
    
    self.weights = np.array(self.weights)

    # 返回相对于下一层输入的梯度
    返回 np.dot(self.weights.T, 梯度)

[...]
神经网络层 = [
Layer_Dense（neural_network_layers_and_its_neurons[1]，weights_list_of_matrises_for_continuation[0]，biases_matrix_continuation[0]），
Layer_Dense（neural_network_layers_and_its_neurons[2]，weights_list_of_matrises_for_continuation[1]，biases_matrix_continuation[1]），
Layer_Dense（neural_network_layers_and_its_neurons[3]，weights_list_of_matrises_for_continuation[2]，biases_matrix_continuation[2]），
Layer_Dense（neural_network_layers_and_its_neurons[4]，weights_list_of_matrises_for_continuation[3]，biases_matrix_continuation[3]），
Layer_Dense(neural_network_layers_and_its_neurons[5]、weights_list_of_matrises_for_continuation[4]、biases_matrix_continuation[4])
]
[...]
对于范围内的 i（0，Batch_size）：
&lt;前&gt;&lt;代码&gt; real_outputs = []

    对于 X 中的 i：
        
        当前输入=我

        # 前向传递各层
        对于 enumerate(neural_network_layers) 中的 a 层：
            层.forward（当前输入）
            如果 a != (len(neural_network_layers)-1):
                当前输入 = 向前（层.输出）
            如果 a == (len(neural_network_layers)-1):
                当前输入 = 层.输出
                当前输入 = 当前输入[0]

        real_outputs.append（当前输入）

    loss_a、accuracy_a = F.accuracy_and_or_loss_in_one_output_NN(expected_outputs, real_outputs, 0)

    准确度.append(accuracy_a)
    损失.追加（loss_a）
    

#这是我之前讨论过的函数
 loss_gradient = F.gradient_of_loss(expected_outputs, real_outputs)
    
    如果 a12 == 1：
        损失_b = 损失_a + 1e10
    
    当前梯度 = 损失梯度
    对于反向层（neural_network_layers）：
        current_gradient = layer.backpropagation(current_gradient,learning_rate)

defgradient_of_loss（真实值，预测值）：
true_values = np.array(true_values_)
预测值 = np.array(预测值_)
# 计算真实值和预测值之间的差异
梯度 = 预测值 - 真实值

返回梯度

`
我尝试过重塑数组，我尝试过转置它们，我已经改变了损失函数的梯度大约10次，我已经观看了3个有关偏导数的数学youtube视频，但没有任何效果]]></description>
      <guid>https://stackoverflow.com/questions/77949922/error-in-backpropagation-method-neural-network-from-scratch-machine-learning</guid>
      <pubDate>Tue, 06 Feb 2024 18:02:18 GMT</pubDate>
    </item>
    <item>
      <title>Faiss GPU索引传递给拥抱面部训练器时无法序列化</title>
      <link>https://stackoverflow.com/questions/77949462/faiss-gpu-index-cannot-be-serialised-when-passed-to-hugging-face-trainer</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77949462/faiss-gpu-index-cannot-be-serialised-when-passed-to-hugging-face-trainer</guid>
      <pubDate>Tue, 06 Feb 2024 16:45:32 GMT</pubDate>
    </item>
    <item>
      <title>Prophet 1.1.5 模型在通过 model_to_json 和 model_from_json 保存和加载后损坏</title>
      <link>https://stackoverflow.com/questions/77949186/prophet-1-1-5-model-is-corrupted-after-being-saved-and-loaded-via-model-to-json</link>
      <description><![CDATA[当模型拟合然后通过 model_to_json 和 model_from_json 保存和加载时，它不会提供准确的预测。如果跳过保存和加载过程并在拟合后立即使用模型进行预测，则不会出现问题。
加载 Prophet 模型后，显示 1970 年的所有数据相隔 1 秒，而不是 2023/24 相隔 15 分钟/1 小时。预测数据最终也以 1970 年代相隔 1 秒 (make_future_dataframe)。此外，预测数据往往不太准确。
当拟合和预测结合在一起时，绕过保存和加载阶段，Prophet 的行为符合预期。仅当我将模型保存到云存储桶或文件系统中时，才会出现此问题。
我比较了plot命令的输出。在保存和加载之前，其输出是准确的并且符合预期。
我怎样才能保存和保存？加载模型而不损坏模型？建议采取哪些故障排除步骤？]]></description>
      <guid>https://stackoverflow.com/questions/77949186/prophet-1-1-5-model-is-corrupted-after-being-saved-and-loaded-via-model-to-json</guid>
      <pubDate>Tue, 06 Feb 2024 16:06:33 GMT</pubDate>
    </item>
    <item>
      <title>对来自简单多元高斯的数据进行 VAE 训练会导致崩溃的重构分布</title>
      <link>https://stackoverflow.com/questions/77949137/training-vae-on-data-from-simple-multivariate-gaussian-leads-to-collapsed-recons</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77949137/training-vae-on-data-from-simple-multivariate-gaussian-leads-to-collapsed-recons</guid>
      <pubDate>Tue, 06 Feb 2024 16:00:22 GMT</pubDate>
    </item>
    <item>
      <title>梯度与损失不匹配</title>
      <link>https://stackoverflow.com/questions/77949060/gradient-does-not-match-loss</link>
      <description><![CDATA[我有一个综合实验，我在数据上拟合了一个模型，分布如下：

类别 0 - ~N(-1, 1)
1 级 - ~N(+1, 1)

该模型基本上将每个正数分类为 1，将负数分类为 0。到目前为止一切顺利。
在测试集中，我通过添加 b=3 更改了这两个类，所以现在：

类别 0 - ~ N(2, 1)
1 级 - ~ N(4, 1)

这是片段：
pbar = tqdm(cs)
对于 pbar 中的 c：
    偏差=变量(torch.tensor([c]).type(dtype),requires_grad=True)
    样本 = np.array(test_df.sample(frac=0.05).x)
    张量_x = torch.张量（样本，requires_grad=False）

    probs = calc_p(tensor_x, 偏差)
    ents = calc_ents(概率)
    
    使用 torch.no_grad()：
        protected_ents, 保护信息 = 保护器.protect(ents)

    损失 = mse_loss(ents, protected_ents.cpu())
    loss.backward()
    bias.grad.data.zero_()

现在我绘制了 X 数据分布的梯度和损失：

由于某种原因，损失看起来是正确的，但梯度却不然，当损失最大化或最小化时，它们应该交叉 0。
如果我改变线路：
loss = mse_loss(ents, protected_ents.cpu())

至：
loss = mse_loss(ents, torch.zeros_like(ents).type(torch.float64).cpu())

然后它按预期工作：

有谁知道是什么原因吗？]]></description>
      <guid>https://stackoverflow.com/questions/77949060/gradient-does-not-match-loss</guid>
      <pubDate>Tue, 06 Feb 2024 15:50:28 GMT</pubDate>
    </item>
    <item>
      <title>如何获得更好的 AUC 分数？ （和累积提升）</title>
      <link>https://stackoverflow.com/questions/77948795/how-to-yield-a-better-auc-score-and-cumulative-lift</link>
      <description><![CDATA[我有一个包含 200k 条记录和 173 个专注于二元分类的特征的数据集。班级比例约为 98.7:1.3（1.3% 目标=1）。
目前，我正在尝试提高模型的性能，该模型的 AUC 为 73%。此外，我对前 2% 的累积提升是 10.41，对前 5% 的累积提升是 5.92。由于我只会针对正面预测分数的前 2-5%，因此我并不特别关心混淆矩阵阈值或改进矩阵值（FP、FN）。
我通过转换（交互，^2）和手动数学计算执行了特征工程。
尽管如此，在没有工程化特征的情况下训练模型后，AUC 分数大致相同，在没有工程化特征的模型中，累积提升略高。我使用了一个自动功能选择工具，该工具使用 RFE 和 XGBoost 来指示所选功能。
我应该注意到，我训练了模型，该模型具有 3 个周期的下采样数据集（3 个周期中每个周期 40k），分类比为 93.5:6.5（6.5% 目标=1），并使用常规的第 4 个周期验证数据集上的数据（原始 1.3% tareget=1 率）。我使用 H20 来训练我的模型（选择 XGBoost）。
如何提高模型得分和模型质量？我知道模型训练涉及插补，但我应该在预处理/清理阶段尝试使用 SimpleImputer、IterativeImputer 或/和 KNNImputer 吗？这会改善我的模型吗？
我尝试使用或不使用我的工程特征重新训练多个模型，并返回到第 1 步并创建更多变量（工程）以尝试帮助我的 AUC 和提升分数。]]></description>
      <guid>https://stackoverflow.com/questions/77948795/how-to-yield-a-better-auc-score-and-cumulative-lift</guid>
      <pubDate>Tue, 06 Feb 2024 15:11:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 对大多数字符串数据集进行分类或聚类的解决方案</title>
      <link>https://stackoverflow.com/questions/77948741/a-solution-for-categorizing-or-clustering-mostly-string-dataset-using-tensorflow</link>
      <description><![CDATA[我想使用机器学习对主要字符串数据集进行分类/聚类。我不确定实现此目的的最佳方法是什么。我想使用 Tensorflow 来完成这个任务。也许有人有开发这样的解决方案的经验并且可以推荐一些东西。
至于我的数据集主要包含字符串值的问题，据我所知，我可以使用 pandas 类别将此数据转换为算法可以理解的内容。如果有更好的解决方案请务必推荐。
到目前为止，我一直在学习 Tensorflow 并分析我拥有的数据集（清理数据、理解结构）。]]></description>
      <guid>https://stackoverflow.com/questions/77948741/a-solution-for-categorizing-or-clustering-mostly-string-dataset-using-tensorflow</guid>
      <pubDate>Tue, 06 Feb 2024 15:03:49 GMT</pubDate>
    </item>
    <item>
      <title>在嘈杂的视频流中识别对象的更好方法是什么？</title>
      <link>https://stackoverflow.com/questions/77948465/what-is-a-better-way-to-recognize-objects-in-a-noisy-video-stream</link>
      <description><![CDATA[据我所知，即使我们使用视频流，我们也需要将其分割成帧来运行模型来检测对象。我们正在处理的视频可能非常嘈杂，以至于当一个人看着屏幕时，只有存在一系列图像（小物体正在移动或物体是静态的，但每一帧的噪声都有些不同）。因此，我担心单帧可能不足以以足够的准确度对对象进行分类（我刚刚开始讨论这个主题，所以到目前为止还没有 PoC）
如何提高准确度（也许使用图像序列，但任何其他方式也可以接受）？]]></description>
      <guid>https://stackoverflow.com/questions/77948465/what-is-a-better-way-to-recognize-objects-in-a-noisy-video-stream</guid>
      <pubDate>Tue, 06 Feb 2024 14:27:55 GMT</pubDate>
    </item>
    <item>
      <title>模型的预测始终为 0</title>
      <link>https://stackoverflow.com/questions/77947679/models-predictions-always-0</link>
      <description><![CDATA[我有一个形状为 (1280, 100, 20, 4096) 的训练集，我将其提供给基于变压器的模型进行二元分类（标签为 0 或 1）。这导致我很难处理大量的特征（我尝试将其批量提供给模型，但我不确定最好的方法。现在我只是将其减少到（450， 100, 20, 4096），但任何建议都值得赞赏），但我目前的问题是，无论我训练模型多少个时期，准确率始终为 67.5%（即 0 的百分比） -测试集中的标记特征），测试集上的精度和召回率将始终为 0%。我尝试在将数据输入模型之前对其进行标准化：
 缩放器 = StandardScaler()
    train_data = scaler.fit_transform(train_data.reshape(-1, train_data.shape[-1])).reshape(train_data.shape)
    test_data = scaler.transform(test_data.reshape(-1, test_data.shape[-1])).reshape(test_data.shape)

但这并没有带来任何改进。我使用的模型基于仅编码器变压器：
层（类型）输出形状参数#
=================================================== ===============
input_1 (输入层) [(无, 100, 20, 4096)] 0
_________________________________________________________________
框架位置嵌入（Po（无、100、20、4096）8192000
_________________________________________________________________
Transformer_layer（编码器）（无、100、20、4096）134299652
_________________________________________________________________
global_max_pooling（GlobalMa（无，4096）0
_________________________________________________________________
辍学（Dropout）（无，4096）0
_________________________________________________________________
输出（密集）（无，1）4097
=================================================== ===============
总参数：142,495,749
可训练参数：142,495,749
不可训练参数：0
_________________________________________________________________

在训练期间，我可以看到损失、准确度、精确度和召回率达到了不错的水平，但是当我在测试集上评估模型时，所有这些值都如我之前所述：
纪元 100/100
29/29 [================================] - 90s 3s/step - 损失：0.0839 - 准确度：0.9610 - 召回率：0.9316 - 精度：0.9589
2024-02-06 12:38:38.815759: W tensorflow/core/framework/cpu_allocator_impl.cc:80] 9175040000 的分配超过可用系统内存的 10%。
9/9 [================================] - 21s 2s/步 - 损失：9.4117 - 准确度：0.6750 - 召回率：0.0000e+00 - 精度：0.0000e+00
测试准确率：67.5%
测试召回率：0.0%
测试精度：0.0%

模型优化器是adam，损失是二元交叉熵。激活是S形的。
我正在努力寻找模型的适当调整，甚至理解其当前的行为。此外，我不清楚将批量数据集输入缩放器和拟合函数是否会改变模型的实际训练。]]></description>
      <guid>https://stackoverflow.com/questions/77947679/models-predictions-always-0</guid>
      <pubDate>Tue, 06 Feb 2024 12:26:30 GMT</pubDate>
    </item>
    <item>
      <title>keras.LSTM 如何将 3D 输入转换为 2D 输出？</title>
      <link>https://stackoverflow.com/questions/77946209/how-keras-lstm-converts-3d-input-to-2d-output</link>
      <description><![CDATA[根据 keras 的 LSTM 文档，输入应该是具有形状（批量、时间步长、特征）的 3D 张量
输出将为（批次，单位），其中单位是我们想要从 LSTM 单元获得的数字特征。
据我所知，lstm 的单个单元格将隐藏状态、单元格状态和单个数字作为时间戳 t 的输入，并将其输出以 c(t+1) 和 h(t+1) 的形式传递到下一个单元格。但从文档代码来看，它正在生成 2D 形式的输出？
输入 = np.random.random((32, 10, 8))
lstm = keras.layers.LSTM(4)
输出 = lstm(输入)
输出形状
(32, 4)

问题 1：向量表示如何传递给 LSTM？ （在每个时间戳处，它传递 8 个特征。如果有 8 个 lstm 单元并行运行，则输出大小也应为 8）
问题2：最终输出的大小如何为4。（如果我们忽略批量大小）]]></description>
      <guid>https://stackoverflow.com/questions/77946209/how-keras-lstm-converts-3d-input-to-2d-output</guid>
      <pubDate>Tue, 06 Feb 2024 08:25:41 GMT</pubDate>
    </item>
    <item>
      <title>关联矩阵热图中的问题[重复]</title>
      <link>https://stackoverflow.com/questions/77946123/issue-in-coorelation-matrix-heat-map</link>
      <description><![CDATA[
我在我的热图中遇到了这个问题，它只显示热图第一行中的值..
我编写了以下代码：
&lt;前&gt;&lt;代码&gt;
    将seaborn导入为sns
    
    将 matplotlib.pyplot 导入为 plt
    将 pandas 导入为 pd
    
    # 选择数字列
    numeric_columns = df.select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]).columns
    
    # 计算数字列的相关矩阵
    相关矩阵 = df[numeric_columns].corr()
    
    # 添加列名作为相关矩阵的第二行
    相关矩阵.列 = 相关矩阵.列.值
    相关矩阵.索引 = 相关矩阵.列.值
    
    # 设置 matplotlib 图形
    plt.figure(figsize=(12, 10))
    
    # 绘制正确显示值的热图
    sns.heatmap（correlation_matrix，annot=True，cmap=&#39;coolwarm&#39;，fmt=“.2f”，linewidths=.5）
    
    # 调整布局以获得更好的可视化效果
    plt.title(“相关矩阵”)
    plt.show()


我需要有人能帮我解决这个问题..]]></description>
      <guid>https://stackoverflow.com/questions/77946123/issue-in-coorelation-matrix-heat-map</guid>
      <pubDate>Tue, 06 Feb 2024 08:08:48 GMT</pubDate>
    </item>
    <item>
      <title>Keras 难以模拟简单的代数函数</title>
      <link>https://stackoverflow.com/questions/77944886/keras-struggles-to-model-simple-algebraic-function</link>
      <description><![CDATA[我最初的问题是为相当大的数据集构建一个神经网络，但我遇到了一个问题，在训练之后，模型始终为任何输入生成完全相同的输出，因此为了调试，我已经转向更简单的问题。&lt; /p&gt;
我能够成功地训练基本逻辑运算符 - AND、OR、XOR 等。因此，我继续尝试对一个简单的代数函数 y = x^2 + 3x - 7 进行建模（我任意选择了这个）。问题是，即使对于这个非常简单的函数，收敛也非常不稳定，需要大量的时期，并且通常会出现相同的“恒定输出”。问题再次出现。
例如，下面的代码：
 from tensorflow.keras.models import Sequential
 从tensorflow.keras.layers导入Dense
 从tensorflow.keras.optimizers导入Adam

# 创建一个顺序模型
    模型=顺序（）

    # 定义训练数据
    X_train = np.array([[-10], [-8], [-6], [-4], [-2], [0], [2], [4], [6], [8 ], [10]])
    y_train = np.array([[63], [33], [11], [-3], [-9], [-7], [3], [21], [47], [81], [123]]）

    # 创建一个简单的神经网络模型
    模型=顺序（）
    model.add（密集（单位= 6，input_dim = 1，激活=&#39;relu&#39;））
    model.add（密集（单位= 12，激活=&#39;relu&#39;））
    model.add（密集（单位= 6，激活=&#39;relu&#39;））
    model.add（密集（1，激活=&#39;线性&#39;））
    
    # 编译模型
    优化器类 = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizerClass,loss=&#39;mean_squared_error&#39;,metrics=[&#39;mae&#39;])

    # 训练模型
    model.fit（X_train，y_train，epochs = 5000，batch_size = 1000）

    # 测试训练好的模型
    X_test = np.array([[-9], [-7], [-5], [-3], [-1], [1], [3], [5], [7], [9 ]]）
    y_test = np.array([[47], [21], [3], [-7], [-9], [-3], [11], [33], [63], [101]] ）
    预测 = model.predict(X_test)
    
    损失，mae = model.evaluate(X_test, y_test)
    print(f&#39;总体测试损失：{loss}，MAE：{mae}&#39;)

运行 5 次会产生 3.6 到 19.0 之间的测试损失。对我来说，在这样一个简单的问题上测试损失会如此变化，这已经很奇怪了，但是看看每个测试 x 的单独输出，它似乎产生了独特的输出，而且总的来说是相当准确的。但是如果我向模型添加另外两层，如下所示
# 创建顺序模型
    模型=顺序（）

    # 定义训练数据
    X_train = np.array([[-10], [-8], [-6], [-4], [-2], [0], [2], [4], [6], [8 ], [10]])
    y_train = np.array([[63], [33], [11], [-3], [-9], [-7], [3], [21], [47], [81], [123]]）

    # 创建一个简单的神经网络模型
    模型=顺序（）
    model.add（密集（单位= 3，input_dim = 1，激活=&#39;relu&#39;））
    model.add(Dense(units=6,activation=&#39;relu&#39;)) #额外层
    model.add（密集（单位= 12，激活=&#39;relu&#39;））
    model.add（密集（单位= 6，激活=&#39;relu&#39;））
    model.add(Dense(units=3,activation=&#39;relu&#39;)) #额外层
    model.add（密集（1，激活=&#39;线性&#39;））
    
    # 编译模型
    优化器类 = Adam(learning_rate=0.001)
    model.compile(optimizer=optimizerClass,loss=&#39;mean_squared_error&#39;,metrics=[&#39;mae&#39;])

    # 训练模型
    model.fit（X_train，y_train，epochs = 5000，batch_size = 1000）

    # 测试训练好的模型
    X_test = np.array([[-9], [-7], [-5], [-3], [-1], [1], [3], [5], [7], [9 ]]）
    y_test = np.array([[47], [21], [3], [-7], [-9], [-3], [11], [33], [63], [101]] ）
    预测 = model.predict(X_test)
    
    损失，mae = model.evaluate(X_test, y_test)
    print(f&#39;总体测试损失：{loss}，MAE：{mae}&#39;)

现在性能完全崩溃了。运行 5 次会产生 10.8 到 1600 之间的损失值，并且除了最好的情况外，大量测试数据都会产生相同的输出。
仅使用四层 3、6、3、1 来简化模型，会产生与更复杂的模型相同的行为 - 高度可变的收敛，并且大部分训练数据具有统一的输出。
我尝试过使用批量大小和学习率，更不用说一堆不同的网络配置 - 比上面更简单和更复杂 - 但总的来说，无论我做什么，性能都非常不稳定，并且容易出现产生无用的结果。
事实上，对于这样一个简单的问题，这种情况如此持续地发生，让我怀疑我在设置中遗漏了一些简单的东西，但对于我的生活，我无法弄清楚]]></description>
      <guid>https://stackoverflow.com/questions/77944886/keras-struggles-to-model-simple-algebraic-function</guid>
      <pubDate>Tue, 06 Feb 2024 02:02:25 GMT</pubDate>
    </item>
    <item>
      <title>比较不同模型的 F1 分数的概率阈值图 [关闭]</title>
      <link>https://stackoverflow.com/questions/77935679/comparing-probability-threshold-graphs-for-f1-score-for-different-models</link>
      <description><![CDATA[下面是两个并排的图，针对不平衡的数据集。

我们有一个非常大的不平衡数据集，我们正在以不同的方式处理/转换。每次转换后，我们都会对其运行 xgboost 估计器。
左侧是三个 xgboost 模型在三个不同转换数据集上的 PR 曲线。从左图可以看出，3条PR曲线全部重叠；事实上，其中两个（红色和绿色）曲线下的面积是相同的。
右侧是来自相同三个模型但在不同概率阈值下的 F1 分数（根据测试数据计算）的图。左右图中模型的颜色匹配。红色和绿色模型在不同概率阈值下的峰值 F1 分数大致相同。蓝色模型的峰值 F1 分数略低于其他两个模型的峰值 F1 分数。我的问题是：
A。我可以说，绿色模型比红色模型“远”好，因为它的 F1 分数在很大的概率阈值范围内相当稳定，而红色模型的 F1 分数随着概率阈值的微小变化而迅速下降。概率阈值。
b.红色和蓝色这两种型号中，哪一种更好，为什么？
如果您能给出合理的答复，我将不胜感激，因为它可能对我的工作有所帮助。顺便说一句，我已经进行了大量关于 F1 分数、AUC 和 PR 曲线的讨论，包括这个。
简单地说，这个问题涉及如何解释不同模型的 F1 分数阈值图，因为 PR 曲线没有得出结论。]]></description>
      <guid>https://stackoverflow.com/questions/77935679/comparing-probability-threshold-graphs-for-f1-score-for-different-models</guid>
      <pubDate>Sun, 04 Feb 2024 11:59:48 GMT</pubDate>
    </item>
    <item>
      <title>VertexAIException - 调用 Gemini-Pro API 时列表索引超出范围错误</title>
      <link>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</link>
      <description><![CDATA[我正在以连续的方式调用 Google Gemini-Pro API（大约每分钟 50 个查询）。我相信我已经正确设置了我的 VertexAI 项目和凭据。当我使用的连续查询数量低于恒定条时，查询将运行并且可以很好地收到响应。但是，一旦查询数量超过上述栏，就会出现以下错误：
&lt;块引用&gt;
索引错误 - 列表索引超出范围

请注意，查询数量“bar”是发生此错误的时间取决于每个查询的长度，并且如果查询长度在程序执行期间保持相同，则该错误是一致的。例如，在尝试将查询长度增加大约 20% 后，查询长度从大约 330 个查询下降到大约 60 个查询。
&lt;块引用&gt;
文件
“/Users/user/anaconda3/envs/chat1/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py”，
第 1315 行，文本
返回 self.candidates[0].text
~~~~~~~~~~~~~~~^^^ IndexError：列表索引超出范围

这是什么原因造成的？我已将 VertexAI 服务器位置设置为：“us-central1”，据我所知，该位置应该只有 300 个查询/分钟的配额。由于我连续执行 API 调用，但低于每分钟 60 次查询的速率，因此我认为我处于使用正常范围。我目前正在使用免费的 VertexAI 试用帐户（有 300 美元的免费信用）。
我写的Gemini Pro API调用函数是：
def gemini_response(message: str) -&gt; &gt;字符串：
    # 初始化顶点AI
    vertexai.init(project=“project-id-0123”, location=“us-central1”)

    # 加载模型
    模型 = GenerativeModel(“gemini-pro”)

    # 查询模型
    响应 = model.generate_content(消息)
    返回响应.文本

在调试 candidates 变量的问题时，变量检查结果如下所示：
&lt;前&gt;&lt;代码&gt;&gt;自己
&gt;提示_反馈{block_reason：其他}
&gt;使用元数据{prompt_token_count：505total_token_count：505}

&gt;自我候选人
&gt; []

&gt; self._raw_response
&gt;提示_反馈{block_reason：其他}
&gt;使用元数据{prompt_token_count：505total_token_count：505}
]]></description>
      <guid>https://stackoverflow.com/questions/77930819/vertexaiexception-list-index-out-of-range-error-when-calling-gemini-pro-api</guid>
      <pubDate>Sat, 03 Feb 2024 04:05:38 GMT</pubDate>
    </item>
    <item>
      <title>BUFFER_SIZE 在 Tensorflow 数据集改组中起什么作用？</title>
      <link>https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling</link>
      <description><![CDATA[所以我一直在玩这个代码：https://www.tensorflow。 org/tutorials/generative/dcgan 并几乎对其功能有了一个很好的了解。但是，我不太清楚 BUFFER_SIZE 变量的用途是什么。我怀疑它可能用于创建大小为 BUFFER_SIZE 的数据库子集，然后从该子集中获取批次，但我没有看到这一点，也找不到人解释它。
所以，如果有人能解释一下 BUFFER_SIZE 的作用，我将不胜感激❤]]></description>
      <guid>https://stackoverflow.com/questions/64372390/what-does-buffer-size-do-in-tensorflow-dataset-shuffling</guid>
      <pubDate>Thu, 15 Oct 2020 13:14:54 GMT</pubDate>
    </item>
    </channel>
</rss>