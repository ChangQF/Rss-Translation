<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Wed, 26 Mar 2025 12:35:43 GMT</lastBuildDate>
    <item>
      <title>如何同时使用ROS收集实时数据并应用机器学习算法？</title>
      <link>https://stackoverflow.com/questions/79535669/how-to-collect-real-time-data-and-apply-a-machine-learning-algorithm-using-ros-a</link>
      <description><![CDATA[我正在使用Turtlebot3从事一个研究项目。该机器人配备了Raspberry Pi，OpenCR和LIDAR传感器。我需要收集实时数据（WiFi RSSI值以及X，Y坐标，然后将机器学习算法应用于实时占用检测。
我正在使用ROS（机器人操作系统）来管理机器人，我计划使用数据实时检测占用。
我感谢有关：的指导
如何构造我的ROS以进行实时数据收集和ML处理。
在机器人收集数据时，如何并行运行机器学习模型。
有效执行此操作的推荐工具，软件包或架构是什么？
我已经写了一个ROS节点来收集WiFi RSSI值，以及机器人的X和Y坐标以及时间戳。我基于时间戳并创建无线电环境图合并了这些数据。
我现在想扩展此设置，以使用机器学习算法执行实时占用检测。我不确定如何执行系统，以便机器人可以同时收集数据并运行ML模型。
我已经尝试研究与ROS节点一起运行Python ML脚本，但是我对从哪里开始以及如何实时整合所有内容感到困惑。]]></description>
      <guid>https://stackoverflow.com/questions/79535669/how-to-collect-real-time-data-and-apply-a-machine-learning-algorithm-using-ros-a</guid>
      <pubDate>Wed, 26 Mar 2025 07:14:16 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用Python在这张图片中区分（绿色）之间的发芽和非发芽种子（红色）？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79535638/how-can-i-use-python-to-differentiate-green-between-germinated-and-non-germina</link>
      <description><![CDATA[我如何编写python脚本以区分发芽的种子（绿色）和非发芽种子（红色）或使用ImageJ区分它们的步骤
我不知道哪种是在24孔板中区分这些种子的最佳方法。我之前曾尝试过ImageJ]]></description>
      <guid>https://stackoverflow.com/questions/79535638/how-can-i-use-python-to-differentiate-green-between-germinated-and-non-germina</guid>
      <pubDate>Wed, 26 Mar 2025 07:00:07 GMT</pubDate>
    </item>
    <item>
      <title>如何在没有自动差异的神经网络的情况下实施反向传播？</title>
      <link>https://stackoverflow.com/questions/79535519/how-to-implement-backpropagation-without-auto-differentiation-for-a-feedforward</link>
      <description><![CDATA[我正在研究一个深度学习任务，该任务需要使用仅使用 numpy （没有Tensorflow，Pytorch或其他自动分辨率工具）实现 feedforward神经网络（FNN）。该网络具有三层（2048、512、10个神经元），使用 relu和SoftMax激活，并用 mini-Batch随机渐变（SGD）进行了优化。
分配需要手动实施向前传播，反向传播和重量更新，以确保我使用 BackPropagation AlgorithM  确保我正确计算每个层的梯度。。
但是，我正在努力正确地计算每个层的梯度，尤其是当处理 relu激活的衍生物和 softmax 时。我想确认我的梯度计算和重量更新是正确的。
 导入numpy作为NP

def relu（x）：
    返回np.maximum（0，x）

def relu_derivative（x）：
    返回（x＆gt; 0）.astype（float）＃relu derivative（1如果x＆gt; 0，else 0）

def softmax（x）：
    exp_x = np.exp（x -np.max（x，axis = 1，keepdims = true））＃稳定技巧
    返回exp_x / np.sum（exp_x，axis = 1，keepdims = true）

def cross_entropy_loss（y_pred，y_true）：
    返回-np.mean（np.sum（y_true * np.log（y_pred + 1e -9），轴= 1））＃防止log（0）

def softmax_cross_entropy_grad（y_pred，y_true）：
    返回y_pred -y_true＃softmax +跨渗透衍生物

def gradient_check（）：
    np.random.seed（42）＃确保可重复性
    
    ＃假输入（batch_size = 3，input_dim = 5）
    x = np.random.randn（3，5）
    w1 = np.random.randn（5，4）
    b1 = np.zeros（（1，4））

    ＃假式单壁编码标签（batch_size = 3，num_classes = 4）
    y = np.Array（[[[0，1，0，0]， 
                  [1，0，0，0]， 
                  [0，0，1，0]]）

    z1 = np.dot（x，w1） + b1
    a1 = relu（z1）
    y_pred = softmax（a1）
    损失= cross_entropy_loss（y_pred，y）

    dl_da1 = softmax_cross_entropy_grad（y_pred，y）＃渐变W.R.T. SoftMax输出
    dl_dz1 = dl_da1 * relu_derivative（z1）＃与relu的链条规则

    打印（y_pred（SoftMax输出）：\ n＆quort y_pred）
    打印（“损失：; quot”损失）
    打印（&#39;梯度W.R.T. SoftMax输出（DL/DA1）：\ n＆quort; dl_da1）
    打印（``relu derivative&#39;&#39;之后的渐变（dl/dz1）：\ n＆quort; dl_dz1）

gradient_check（）
 
我仅使用 numpy 并手动计算的反向传播实现了 feedforward神经网络（FNN）。我期望损失会降低和的准确性，但相反，损失波动，准确性保持 low ，有时梯度 爆炸或成为nan 。我怀疑 relu的导数和 softmax +交叉渗透梯度计算的问题，需要验证我的反向传播是否正确。]]></description>
      <guid>https://stackoverflow.com/questions/79535519/how-to-implement-backpropagation-without-auto-differentiation-for-a-feedforward</guid>
      <pubDate>Wed, 26 Mar 2025 05:59:43 GMT</pubDate>
    </item>
    <item>
      <title>我使用densenet-169创建了使用组织病理学图像预测结肠癌的模型，我该怎么做才能将F1分数从75％提高。</title>
      <link>https://stackoverflow.com/questions/79535469/i-used-densenet-169-in-creating-a-model-for-predicting-colon-cancer-using-histop</link>
      <description><![CDATA[我使用Python在10,000张图像的数据集上使用Densenet169创建了一个模型，以预测结肠癌，两个子文件夹有5000张图像，每张图像5000张图像用于良性和癌组织。训练持续了8小时，训练后的F1分数为0.75，这是不可取的。我试图查看我可以做出的更改以改善指标，但是我没有任何更改的方法，并且需要帮助确定可以在哪里进行更改以提高F1分数。这是我第一次从事这样的事情，我发现没有在线材料进行深度学习可以帮助我解决这个问题
 将TensorFlow导入为TF
来自Tensorflow.keras.applications导入Densenet169
来自tensorflow.keras.layers导入密集，globalaveration -pooling2d，Randomflip，RandomRotation，辍学
来自Tensorflow.keras.models导入模型，顺序
来自Tensorflow.keras.optimizer导入Adam
从tensorflow.keras.regulinizer导入l2
来自tensorflow.keras.applications.densenet导入preprocess_input
来自sklearn.metrics导入混乱_matrix，f1_score
导入numpy作为NP
进口海洋作为SNS
导入matplotlib.pyplot作为PLT
导入操作系统

＃定义常数
img_size =（224，224）＃匹配densenet169输入大小
batch_size = 32
时代= 15
data_dir =＆quot; colon_images＆quot;
class_names = [＆quast; cancyous&#39;&#39;正常＆quot;]
split_ratio = 0.2

＃直接加载数据集在224x224
def create_dataset（子集）：
    返回tf.keras.utils.image_dataset_from_directory（
        data_dir，
        验证_split = split_ratio，
        子集=子集
        种子= 42，
        image_size = img_size，＃直接以目标大小加载
        batch_size = batch_size，
        label_mode =&#39;binary&#39;
    ）

train_ds = create_dataset（“训练”）
val_ds = create_dataset（&#39;验证＆quot;）

＃通过增强进行预处理（仅在培训期间活跃）
预处理=顺序（[
    Randomflip（“水平”，“），
    随机旋转（0.1），
    tf.keras.layers.lambda（preprocess_input）＃正确归一化
）））

＃构建模型
base_model = densenet169（
    权重=&#39;Imagenet&#39;，
    include_top = false，
    input_shape = img_size +（3，）
）

输入= tf.keras.input（shape = img_size +（3，））
X =预处理（输入）
x = base_model（x）
x = globalaveragepooling2d（）（x）
x =密集（512，激活=&#39;relu&#39;，kernel_regularizer = l2（0.01））（x）
x =辍学（0.5）（x）＃正则化
输出=密集（1，激活=&#39;Sigmoid&#39;）（x）
模型=模型（输入，输出）

＃阶段1：火车顶层
base_model.trainable = false
model.compile（
    优化器= ADAM（1E-3），
    损失=&#39;binary_crossentropy&#39;，
    量表= [&#39;fecycy&#39;，tf.keras.metrics.auc（name =&#39;auc&#39;），
             tf.keras.metrics.precision（name =&#39;precision&#39;），
             tf.keras.metrics.Recall（name =&#39;recember&#39;）]
）

＃用回调监视AUC的火车
早期_stop = tf.keras.callbacks.earlystopping（
    Monitor =&#39;Val_auc&#39;，耐心= 3，模式=&#39;max&#39;，详细= 1
）
检查点= tf.keras.callbacks.modelcheckpoint（
    &#39;best_model.h5&#39;，save_best_only = true，monitor =&#39;val_auc&#39;，mode =&#39;max&#39;
）

历史= model.fit（
    train_ds，
    验证_data = val_ds，
    时代= epochs，
    回调= [早期_STOP，检查点]
）

＃阶段2：微调整个模型
base_model.trainable = true
model.compile（
    优化器= ADAM（1E-5），＃非常低的学习率
    损失=&#39;binary_crossentropy&#39;，
    量表= [&#39;facer&#39;，tf.keras.metrics.auc（name =&#39;auc&#39;）]
）

history_fine = model.fit（fit）（
    train_ds，
    验证_data = val_ds，
    时代= epochs，
    onirome_epoch = history.epoch [-1]，
    回调= [早期_STOP，检查点]
）

＃ 评估
y_pred =（model.predict（val_ds）＆gt; 0.5）.astype（int）
y_true = np.concatenate（[y for _，y in val_ds]，axis = 0）

打印（f＆quot f1分数：{f1_score（y_true，y_pred）：。3f}＆quot;）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79535469/i-used-densenet-169-in-creating-a-model-for-predicting-colon-cancer-using-histop</guid>
      <pubDate>Wed, 26 Mar 2025 05:27:58 GMT</pubDate>
    </item>
    <item>
      <title>我的加权动态时间扭曲算法中是否存在错误</title>
      <link>https://stackoverflow.com/questions/79535435/is-there-an-error-in-my-weighted-dynamic-time-warping-algorithm</link>
      <description><![CDATA[使用平均归一化，并为算法中的输入进行修改的逻辑权重函数。不确定我的代码或我的数据是否弄乱了，使用了两个不同人的步态数据数据集，而我的距离最终是：0.06495056266749273 
我将以下公式用于WDTW和修改后的逻辑权重函数：
      def stripfunction（i_j，g，w_max）：#calculates修改后的逻辑权重函数
  尺寸= len（x）
  m_c = size // 2

  返回w_max /（1 + np.exp（-g*（i_j -m_c）））

def euclidean_distances（x，y）：
      返回np.sqrt（np.sum（（（x -y）** 2）））

def witeeddtw（x，y，vector = true）：
  w_max = 3

  n，m = X.Shape [0]，y.Shape [0]
  成本= np r.zeros（（（n，m））

  成本[0,0] = euclidean_distances（x [0]，y [0]）

  对于我在范围（1，n）中：
    成本[i，0] =成本[I-1，0] + Euclidean_distances（x [i]，y [0]）

  对于J范围（1，m）的J：
    成本[0，j] =成本[0，j-1] + euclidean_distances（x [0]，y [j]）


  对于我在范围（1，n）中：
    对于J范围（1，m）的J：
      i_j = euclidean_distances（x [i]，y [j]）
      w =重量函数（I_J，0.4，W_MAX）
      min_value = min（[成本[I-1，J-1]，成本[I-1，J]，成本[I，J-1]]）
      成本[i，j] = w * i_j + min_value

    max_dist = w_max *（n + m -2）
    norm_dist = cop [n-1，m-1] / max_dist


  退货成本[N-1，M-1]，NORM_DIST

x_norm =（x -np.mean（x，axis = 0）） /（np.max（x，axis = 0）-np.min（x，axis = 0））

y_norm =（y -np.mean（y，axis = 0）） /（np.max（y，axis = 0）-np.min（y，axis = 0））


＃计算WDTW
距离，归一化=加权dtw（x_norm [：，0]，y_norm [：，0]）

打印（f＆quot&#39;wdtw距离：{demand}＆quot;）
打印（f＆quot“归一化wdtw距离：{normolized_distance}＆quort”）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79535435/is-there-an-error-in-my-weighted-dynamic-time-warping-algorithm</guid>
      <pubDate>Wed, 26 Mar 2025 04:58:34 GMT</pubDate>
    </item>
    <item>
      <title>GOCV和Python OpenCV之间的推断结果不同</title>
      <link>https://stackoverflow.com/questions/79533406/different-inference-results-between-gocv-and-python-opencv</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79533406/different-inference-results-between-gocv-and-python-opencv</guid>
      <pubDate>Tue, 25 Mar 2025 10:48:10 GMT</pubDate>
    </item>
    <item>
      <title>在HuggingFace Seq2Seqtrainer中使用培训，验证和测试集[闭合]</title>
      <link>https://stackoverflow.com/questions/79532570/use-of-training-validation-and-test-set-in-huggingface-seq2seqtrainer</link>
      <description><![CDATA[ i具有以下数据集，该数据集具有3个拆分（ Train ，验证和 test ）。数据是2种语言的平行语料库。
  datasetDict（{{
    火车：数据集（{
        功能：[&#39;translation&#39;]，
        num_rows：109942
    }））
    验证：数据集（{
        功能：[&#39;translation&#39;]，
        num_rows：6545
    }））
    测试：数据集（{
        功能：[&#39;translation&#39;]，
        num_rows：13743
    }））
}））
 
对于我的 seq2seqtrainer ，我提供数据集如下：
  trainer = seq2seqtrainer（
    模型=模型，
    args =训练_args，
    train_dataset = tokenized_dataset [&#39;train&#39;]，
    eval_dataset = tokenized_dataset [&#39;验证&#39;]，
    tokenizer = tokenizer，
    data_collat​​or = data_collat​​or，
    compute_metrics = compute_metrics，
）
 
将验证拆分 ever_dataset 中的拆分是正确的吗？在 documentation&gt; documentation 

用于评估的数据集。如果是数据集，则自动删除 model.forward（）方法未接受的列。如果是字典，它将在每个数据集上进行评估。

或者我应该将测试分配在 eval_dataset 中？无论哪种方式，都没有使用其中一个分裂？]]></description>
      <guid>https://stackoverflow.com/questions/79532570/use-of-training-validation-and-test-set-in-huggingface-seq2seqtrainer</guid>
      <pubDate>Tue, 25 Mar 2025 02:22:21 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的代码不导致张量的错误“元素0”不需要毕业，也不需要grad_fn'</title>
      <link>https://stackoverflow.com/questions/79524465/why-does-not-my-code-cause-the-error-element-0-of-tensors-does-not-require-grad</link>
      <description><![CDATA[给定下面的代码片段，我使用 model.fc1.requires_grad_（false） and  model.fc2.requires_grad_（false）冻结神经网络的权重。
如果我使用损失=标准（输出，y_batch）来计算损失，则训练很好。为什么它不会导致错误 RuntimeError：张量的元素0不需要grad，也不需要Grad_fn ？
我认为这应该导致错误，因为神经网络的所有层都已冻结。
 导入火炬
导入Torch.nn作为nn
导入Torch.optim作为最佳
来自Torch.utils.data导入数据载体，TensordataSet

＃定义一个简单的神经网络
类SimpleNet（nn.Module）：
    def __init __（self，input_size，hidden_​​size，output_size）：
        超级（SimpleNet，Self）.__ INIT __（）
        self.fc1 = nn.linear（input_size，hidden_​​size）
        self.fc2 = nn.linear（hidden_​​size，output_size）

    def向前（self，x）：
        x = self.fc1（x）
        x = self.fc2（x）
        返回x

input_size = 3
hidden_​​size = 3
output_size = 3
batch_size = 16
num_epochs = 1
Learning_rate = 0.01
num_samples = 1000 
x_train = torch.randn（num_samples，input_size，quirenes_grad = true）  
y_train = torch.ones（（num_samples，output_size））
dataset = tensordataset（x_train，y_train）
dataloader = dataloader（数据集，batch_size = batch_size，shuffle = true）

model = simpleNet（input_size，hidden_​​size，output_size）


标准= nn.Crossentropyloss（）
优化器= Optim.SGD（model.parameters（），lr = Learning_rate）

对于范围（num_epochs）的时代：
    total_loss = 0
    对于emumerate（dataloader）中的batch_idx（x_batch，y_batch）：
        优化器.zero_grad（）
        输出=模型（x_batch）
        损失=标准（输出，y_batch）
        打印（损失。_grad_fn）
        loss.backward（） 
        优化器.step（） 
                
        if（batch_idx + 1）％10 == 0：
            print（f&#39;batch [{batch_idx + 1}/{len（dataloader）}]，损失：{loss.item（）：。4f}&#39;）
            model.fc1.requires_grad_（false）
            model.fc2.requires_grad_（false）
                        
打印（“训练完成！”）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79524465/why-does-not-my-code-cause-the-error-element-0-of-tensors-does-not-require-grad</guid>
      <pubDate>Fri, 21 Mar 2025 02:40:41 GMT</pubDate>
    </item>
    <item>
      <title>DL4J自动编码器用于异常检测：意外结果[封闭]</title>
      <link>https://stackoverflow.com/questions/79523631/dl4j-autoencoder-for-anomaly-detection-unexpected-results</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79523631/dl4j-autoencoder-for-anomaly-detection-unexpected-results</guid>
      <pubDate>Thu, 20 Mar 2025 16:59:01 GMT</pubDate>
    </item>
    <item>
      <title>节点/边缘归因的有向图嵌入</title>
      <link>https://stackoverflow.com/questions/79508535/node-edge-attributed-directed-graph-embedding</link>
      <description><![CDATA[  graph2vec  gl2vec  in  karateclub 需要什么？它确实提到应该没有字符串功能，但是无论有没有它，我都会遇到以下代码的错误：
 导入NetworkX为Nx
导入Karateclub为KC
导入matplotlib.pyplot作为PLT

g = nx.digraph（）

g.add_node（0，label =; a;
g.add_node（1，label =; b＆quot; feature = [1.2]）
g.add_node（2，label =; c＆quot;特征= [0.8]）

g.add_edge（0，1）
g.add_edge（0，2）
g.add_edge（1，2）

nx.draw_networkx（g，with_labels = true）
plt.show（）

图= [g]

型号= kc.graph_embedding.graph2vec（）
型号（图）
嵌入= model.get_embedding（）
打印（嵌入）
 
错误： RuntimeError：您必须在训练模型之前先构建词汇 
我在 word2vec 中看到了一个选项，但是如何为 graph2vec ？执行此操作。
是否有任何替代软件包，最好使用 KarateClub 等更简单的实现，我可以用来生成定向节点/edge属性列表 network&gt; networkx  Graphs 图形而无需定义训练/测试集吗？]]></description>
      <guid>https://stackoverflow.com/questions/79508535/node-edge-attributed-directed-graph-embedding</guid>
      <pubDate>Fri, 14 Mar 2025 08:52:01 GMT</pubDate>
    </item>
    <item>
      <title>神经网络精确度低</title>
      <link>https://stackoverflow.com/questions/73780627/neural-network-low-accuracy</link>
      <description><![CDATA[该模型的精度确实很低。这是我第一次编写神经网络，所以我真的不知道如何使它变得更好
 将TensorFlow导入为TF
导入matplotlib.pyplot作为PLT
    
#data设置
data = tf.keras.datasets.cifar10


（x_train，y_train），（x_test，y_test）= data.load_data（）
plt.imshow（x_train [0]，cmap = plt.cm.binary）

#normalize数据
x_train = tf.keras.utils.normalize（x_train，axis = 1）
x_test = tf.keras.utils.normalize（x_test，axis = 1）

#building AI模型

型号= tf.keras.models.sequeential（）
Model.Add（tf.keras.layers.flatten（））
model.Add（tf.keras.layers.dense（128，激活= tf.nn.relu））
model.Add（tf.keras.layers.dense（128，激活= tf.nn.relu））
model.Add（tf.keras.layers.dense（10，激活= tf.nn.softmax）））



#compile模型
model.compile（优化器=&#39;adam&#39;，
             损失=&#39;Sparse_categorical_crossentropy&#39;，
             指标= [&#39;准确性&#39;]）

plt.show（）
#Train AI模型
model.fit（x_train，y_train，epochs = 3）
 ]]></description>
      <guid>https://stackoverflow.com/questions/73780627/neural-network-low-accuracy</guid>
      <pubDate>Tue, 20 Sep 2022 00:59:44 GMT</pubDate>
    </item>
    <item>
      <title>通过神经网络获得非常低的精度[关闭]</title>
      <link>https://stackoverflow.com/questions/65777704/getting-a-very-low-accuracy-with-neural-network</link>
      <description><![CDATA[我正在尝试使用keras在CIFAR-10数据集上实现ANN
但是由于某种原因，我不知道我只能获得10％的准确性？
我分别使用了5个隐藏层IWTH 8,16,32,64,128神经元。
 这是jupyter笔记本 的链接
  model = sequention（）
model.Add（密集（单位= 8，activation =&#39;sigmoid&#39;，input_dim = x.shape [1]）））））
model.Add（密集（单位= 16，activation =&#39;Sigmoid&#39;））））
model.Add（密集（单位= 32，activation =&#39;Sigmoid&#39;））
model.Add（密集（单位= 64，activation =&#39;sigmoid&#39;））
model.Add（密集（单位= 128，activation =&#39;Sigmoid&#39;））
model.Add（密度（单位= 10，activation =&#39;softmax&#39;））

model.compile（loss =&#39;accorical_crossentropy&#39;，importizer =&#39;adam&#39;，量表= [&#39;fecicy&#39;]）

model.fit（x_train，y_train，epochs = 1000，batch_size = 500）
 ]]></description>
      <guid>https://stackoverflow.com/questions/65777704/getting-a-very-low-accuracy-with-neural-network</guid>
      <pubDate>Mon, 18 Jan 2021 15:44:46 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch中的数据增强</title>
      <link>https://stackoverflow.com/questions/51677788/data-augmentation-in-pytorch</link>
      <description><![CDATA[我对Pytorch中执行的数据增加有些困惑。现在，据我所知，当我们执行数据增强时，我们将保留原始数据集，然后添加其他版本（翻转，裁剪等）。但这似乎并没有发生在Pytorch中。据我从参考文献中理解时，当我们在pytorch中使用 data.transforms 时，它将它们一一应用它们。因此：
  data_transforms = {
    &#39;train&#39;：transforms.compose（[
        transforms.randomresizedcrop（224），
        transforms.randomhorizo​​ntalflip（），
        transforms.totensor（），
        transforms.normize（[0.485，0.456，0.406]，[0.229，0.224，0.225]）
    ]），，
    &#39;val&#39;：transforms.compose（[
        转换式Resize（256），
        transforms.centercrop（224），
        transforms.totensor（），
        transforms.normize（[0.485，0.456，0.406]，[0.229，0.224，0.225]）
    ]），，
}
 
在这里，对于培训，我们首先是随机裁剪图像并将其调整为Shape （224,224）。然后，我们将这些（224,224）图像进行水平翻转。因此，我们的数据集现在仅包含水平翻转的图像，因此在这种情况下，我们的原始图像丢失了。
我对吗？这理解是正确的吗？如果不是，那么我们在上面的此代码中（取自官方文档）将Pytorch告诉原始图像并将其调整到预期形状（224,224）？]]></description>
      <guid>https://stackoverflow.com/questions/51677788/data-augmentation-in-pytorch</guid>
      <pubDate>Fri, 03 Aug 2018 17:51:49 GMT</pubDate>
    </item>
    <item>
      <title>在GridSearchCV中得分XGBoost</title>
      <link>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</link>
      <description><![CDATA[我正在尝试使用XGBoost首次分析数据。我想使用GridSearchCV找到最佳参数。我想最大程度地减少均方根错误，为此，我使用了“ rmse”。作为eval_metric。但是，网格搜索的评分没有这样的度量。我在此网站上发现“ neg_mean_squared_error”这样做一样，但是我发现这给我与RMSE不同的结果。当我计算“ neg_mean_squared_error”的绝对值的根目录时，我的值约为8.9，而不同的功能使我的RMSE约为4.4。
我不知道怎么了，或者如何获得这两个功能同意/给出相同的值？
由于这个问题，我遇到了错误的值，因为 best_params _ ，它比我最初开始调音的某些值更高的RMSE。
如何在网格搜索中获得RMSE的分数以及为什么我的代码给出不同的值？
  def modelfit（alg，trainx，trainy，usetraincv = true，cv_folds = 10，armond_stopping_rounds = 50）：
    如果用UsetrainCV：
        xgb_param = alg.get_xgb_params（）
        xgtrain = xgb.dmatrix（trainx，label = Trainy）
        cvresult = xgb.cv（xgb_param，xgtrain，num_boost_round = alg.get_params（）[&#39;n_estimators&#39;]，nfold = cv_folds，
                          指标=&#39;rmse&#39;，ropard_stopping_rounds = armon_stopping_rounds）
        alg.set_params（n_estimators = cvresult.shape [0]）

    ＃将算法适合数据
    alg.fit（Trainx，Trainy，eval_metric =&#39;rmse&#39;）

    ＃预测训练集：
    dtrain_predictions = alg.predict（trainx）
    ＃dtrain_predprob = alg.predict_proba（Trainy）[：，1]
    打印（dtrain_predictions）
    print（np.sqrt（mean_squared_error（trainy，dtrain_predictions））））））））

    ＃打印模型报告：
    打印（“ \ nmodel报告”）
    打印（&#39;rmse：％.4G;％np.sqrt（量学

 param_test2 = {
 &#39;max_depth&#39;：[6,7,8]，
 &#39;min_child_weight&#39;：[2,3,4]
}

grid2 = gridSearchCV（估算= xgb.xgb.xgbregressor（Learning_rate = 0.1，n_estimators = 2000，max_depth = 5，
 min_child_weight = 2，gamma = 0，subsampe = 0.8，colsample_bytree = 0.8，
 objective =&#39;reg：linear&#39;，nthread = 4，scale_pos_weight = 1，andury_state = 4），
 param_grid = param_test2，评分=&#39;neg_mean_squared_error&#39;，n_jobs = 4，iid = false，cv = 10，冗长= 20）
grid2.fit（x_train，y_train）
＃best_estimator的平均交叉验证得分
print（grid2.best_params_，np.sqrt（np.abs（grid2.best_score_）））），print（np.sqrt（np.abs（grid2.score2.score（x_train，y_train，y_train））））
modelfit（grid2.best_estimator_，x_train，y_train）
print（np.sqrt（np.abs（grid2.score）（x_train，y_train）））））））））））
 ]]></description>
      <guid>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</guid>
      <pubDate>Fri, 11 May 2018 16:46:16 GMT</pubDate>
    </item>
    <item>
      <title>训练后如何用分布的时间来替换嵌入层？</title>
      <link>https://stackoverflow.com/questions/39532572/how-to-replace-an-embedding-layer-with-a-time-distributed-dense-after-training</link>
      <description><![CDATA[我有以下问题：

 我想使用LSTM网络进行文本分类。为了加快训练的速度并使代码更加清楚，我想沿沿 keras.tokenizer 嵌入层以训练我的模型。 
 一旦我训练了我的模型 - 我想计算输出W.R.T.的显着性图。输入。为此，我决定将嵌入层替换为 timeDistributedDense 。 

您知道什么是最好的方法。对于一个简单的模型，我可以简单地使用已知权重的模型来重建模型 - 但我想使其尽可能通用 - 例如替换模型结构的未来并使我的框架尽可能不可知。]]></description>
      <guid>https://stackoverflow.com/questions/39532572/how-to-replace-an-embedding-layer-with-a-time-distributed-dense-after-training</guid>
      <pubDate>Fri, 16 Sep 2016 13:21:25 GMT</pubDate>
    </item>
    </channel>
</rss>