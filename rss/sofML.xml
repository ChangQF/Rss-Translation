<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 02 Jul 2024 03:17:05 GMT</lastBuildDate>
    <item>
      <title>精确而强大的角点检测（噪声图像、脏污物体）</title>
      <link>https://stackoverflow.com/questions/78694749/precise-and-robust-corner-detection-noisy-image-dirty-object</link>
      <description><![CDATA[鉴于一定的质量控制要求，我们实施了一个自动化系统来测量钢板生产线的某些尺寸。问题是，有时系统不够强大，系统选择的像素不能反映我们人类推理认为的真实角落。该图像大约为 17 MPixels-
此简化的代码片段应代表我们的测量过程：
defcontrast_stretch(image, multiplier=1.0):
min_val = np.min(image)
max_val = np.max(image)
stretched = (image - min_val) * (255 / (max_val - min_val) * multiplier)
stretched = np.clip(stretched, 0, 255).astype(np.uint8)
returnstretched

def distance(pt1, pt2):
return math.sqrt((pt2[0] - pt1[0]) ** 2 + (pt2[1] - pt1[1]) ** 2)

defmeasure_diagonals(image, contours, px_to_mm):
refined_corners = []
for cnt in轮廓：
rect = cv2.minAreaRect(cnt)
box = cv2.boxPoints(rect)
box = np.int0(box)
corners = cv2.cornerSubPix(image, np.float32(box), (5, 5), (-1, -1), (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.1))
refined_corners.append(corners)
如果 len(refined_corners) &gt;= 2:
d1 = distance(refined_corners[0][0], refined_corners[0][2])
d2 = distance(refined_corners[1][0], refined_corners[1][2])
m1 = d1 * px_to_mm
m2 = d2 * px_to_mm
diff = abs(m2 - m1)
返回 m1, m2, diff
返回 None, None, None

# 加载校准数据
calibration_data = load_calibration_data(calibration_data_path)
mtx = calibration_data[&quot;mtx&quot;]
dist = calibration_data[&quot;dist&quot;]

# 加载图像
frame = cv2.imread(img_path)

# 不失真图像
frame_undistorted = cv2.undistort(frame, mtx, dist, None, mtx)

# 转换为灰度
gray = cv2.cvtColor(frame_undistorted, cv2.COLOR_BGR2GRAY)

# 应用双边滤波器
filtered_image = cv2.bilateralFilter(gray, 9, 125, 25)

# 增强对比度
enhanced_image =对比度拉伸（过滤图像）

# 检测轮廓
_, edge = cv2.threshold(enhanced_image, 140, 255, cv2.THRESH_BINARY)
contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 测量对角线
px_to_mm = 0.1 # 示例转换因子
m1, m2, diff = measure_diagonals(enhanced_image, contours, px_to_mm)

这是正确选择角的示例：
ROI 生成阈值（白点），然后是圆角子像素（黑色点）
这是一个错误选择角落的例子：
红色部分是我们知道的真正角落
我知道我们应该改善照明。我们正在测量一个大面积（&gt;4 米），并且要有一个能够生成明亮、均匀图像的照明系统极具挑战性，因此我们应用了大量软件校正，例如双边滤波器、增益和对比度增强器。]]></description>
      <guid>https://stackoverflow.com/questions/78694749/precise-and-robust-corner-detection-noisy-image-dirty-object</guid>
      <pubDate>Tue, 02 Jul 2024 01:35:14 GMT</pubDate>
    </item>
    <item>
      <title>HTML 文件输出未生成</title>
      <link>https://stackoverflow.com/questions/78694578/html-file-output-not-generating</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78694578/html-file-output-not-generating</guid>
      <pubDate>Mon, 01 Jul 2024 23:43:32 GMT</pubDate>
    </item>
    <item>
      <title>我尝试使用 hugginsface 中的 convert_graph_to_onnx.py，但出现此错误：“转换模型时出错：未安装模块 onnx！”</title>
      <link>https://stackoverflow.com/questions/78694557/i-tried-to-use-convert-graph-to-onnx-py-from-hugginsface-but-i-got-the-this-err</link>
      <description><![CDATA[我尝试使用此 [page][1] 中的示例使用导出模型功能
python convert_graph_to_onnx.py --framework pt --model bert-base-cased bert-base-cased.onnx

我收到一个错误，我需要使用 ONNX opset 版本 14，因此我使用了这个：
python convert_graph_to_onnx.py --framework pt --opset 14 --model bert-base-cased bert-base-cased.onnx

我收到此错误：

====== 将模型转换为 ONNX ====== convert_graph_to_onnx.py:361: FutureWarning: &#39;transformers.convert_graph_to_onnx` 包已弃用并将在 Transformers 版本 5 中删除
warnings.warn( ONNX opset 版本设置为：14 加载管道（模型：bert-base-cased，tokenizer：bert-base-cased）使用框架 PyTorch：2.3.1+cpu 发现输入 input_ids 形状：{0：&#39;batch&#39;，1：&#39;sequence&#39;} 发现输入 token_type_ids 形状：{0：&#39;batch&#39;，1：
&#39;sequence&#39;} 发现输入tention_mask 形状：{0：&#39;batch&#39;，1：
&#39;sequence&#39;} 发现输出 output_0 形状：{0：&#39;batch&#39;，1：
&#39;sequence&#39;} 发现输出 output_1 形状：{0：&#39;batch&#39;} 确保输入的顺序正确 position_ids 不存在于生成的输入列表中。生成的输入顺序：[&#39;input_ids&#39;,
&#39;attention_mask&#39;, &#39;token_type_ids&#39;] 转换模型时出错：
未安装模块 onnx！

我安装了 onnx==1.16.1
有人能帮我吗？]]></description>
      <guid>https://stackoverflow.com/questions/78694557/i-tried-to-use-convert-graph-to-onnx-py-from-hugginsface-but-i-got-the-this-err</guid>
      <pubDate>Mon, 01 Jul 2024 23:34:48 GMT</pubDate>
    </item>
    <item>
      <title>在 cross_val_score 中获取 mac-avg f1-score</title>
      <link>https://stackoverflow.com/questions/78694297/get-mac-avg-f1-score-in-cross-val-score</link>
      <description><![CDATA[我有一个简单的二元分类实验。我正在尝试执行。
这是代码：
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.svm 导入 LinearSVC
从 sklearn.model_selection 导入 cross_val_score
导入 xgboost 作为 xgb
models = [
RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0),
LinearSVC(),
MultinomialNB(),
LogisticRegression(random_state=0),
xgb.XGBClassifier(max_depth=3, 
objective=&#39;binary:logistic&#39;, 
n_estimators=100, 
num_classes=2, 
n_jobs = -1)
]
CV = 5
cv_df = pd.DataFrame(index=range(CV * len(models)))
entries = []
for model in models:
model_name = model.__class__.__name__
f1_score = cross_val_score(model, X_prep, labels,scoring = make_scorer(f1_score, average=&#39;weighted&#39;, labels=[2]), cv=CV)
for fold_idx, f1score in enumerate(f1_score):
entrys.append((model_name, fold_idx, f1score))

我收到以下错误消息：
-------------------------------------------------------------------------------
InvalidParameterError Traceback (most recent call last)
Cell In[34], line 22
20 for model in models:
21 model_name = model.__class__.__name__
---&gt; 22 f1_score = cross_val_score(model, X_prep, labels,scoring = make_scorer(f1_score, average=&#39;weighted&#39;, labels=[2]), cv=CV)
23 for fold_idx, f1score in enumerate(f1_score):
24 entities.append((model_name, fold_idx, f1score))

文件 ~/anaconda3/envs/gpt-ds/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:203，在validate_params.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)
200 to_ignore += [&quot;self&quot;, &quot;cls&quot;]
201 params = {k: v for k, v 在 params.arguments.items() 中，如果 k 不在 to_ignore 中}
--&gt; 203 验证参数约束 (
204 参数约束，参数，调用者名称 = 函数。__qualname__
205 )
207 尝试：
208 使用 config_context (
209 跳过参数验证 = (
210 首选跳过嵌套验证或全局跳过验证
211 )
212 ):

文件 ~/anaconda3/envs/gpt-ds/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:95，在验证参数约束 (参数约束，参数，调用者名称) 中
89 其他：
90 约束字符串 = (
91 f&quot;{&#39;, &#39;.join([str(c) for c in约束[:-1]])} 或&quot;
92 f&quot; {约束[-1]}&quot;
93 )
---&gt; 95 引发 InvalidParameterError(
96 f&quot;{caller_name} 的 {param_name!r} 参数必须是&quot;
97 f&quot; {约束_str}。得到的是 {param_val!r}。&quot;
98 )

InvalidParameterError: make_scorer 的 &#39;score_func&#39; 参数必须是可调用的。得到的是数组([0., 0., 0., 0., 0.])。

如何获取 cross_val_score 的 mac_avg f1 分数？]]></description>
      <guid>https://stackoverflow.com/questions/78694297/get-mac-avg-f1-score-in-cross-val-score</guid>
      <pubDate>Mon, 01 Jul 2024 21:32:42 GMT</pubDate>
    </item>
    <item>
      <title>如何在 macOS 10.12 上运行 Core ML 模型？</title>
      <link>https://stackoverflow.com/questions/78694076/how-can-one-run-a-core-ml-model-on-macos-10-12</link>
      <description><![CDATA[https://developer.apple.com/documentation/coreml 提到 macOS 10.13+：

如何在 macOS 10.12 上运行 Core ML 模型？

在 Ubuntu 20.04 上创建的 Core ML 模型示例（使用 Python 3.10 和 torch 2.3.1 测试）：
git clone https://github.com/huggingface/exporters.git
cd exporters
pip install -e .
python -m exporters.coreml --model=distilbert-base-uncasederated/ --quantize=float32 
]]></description>
      <guid>https://stackoverflow.com/questions/78694076/how-can-one-run-a-core-ml-model-on-macos-10-12</guid>
      <pubDate>Mon, 01 Jul 2024 20:13:24 GMT</pubDate>
    </item>
    <item>
      <title>本地机器上的实时语音分割 [无 GPU/外部 API] [关闭]</title>
      <link>https://stackoverflow.com/questions/78693474/realtime-speech-diarization-on-local-machine-no-gpu-external-apis</link>
      <description><![CDATA[我正在寻找一些在本地机器上不使用任何 GPU 的实时语音分割解决方案。目前有类似的东西吗？
我知道分割对于 CPU 来说是一项复杂的任务，老实说，我的任务甚至不需要分割。我想要实现的任务是麦克风的音频将继续流式传输，任何随机的人都可以对着麦克风说话，但当第一个人已经在说话时，每当另一个人要说话时，代码就会指出检测到了第二个人。就是这样！分割其实是不需要的，但除了分割之外，我想不出更好的解决方案来实现我想要的。
目前有没有这样的解决方案可以用于我的任务？
我能找到的只有 pyannote 解决方案、Nvidia 的 NeMo 和一些其他解决方案，但它们都必须加载需要高 GPU RAM 的重型模型。我想要一些可以在本地 CPU 上运行的简单方法。而且我绝对不允许使用付费的外部 API，例如 Assembly AI/Deepgram。]]></description>
      <guid>https://stackoverflow.com/questions/78693474/realtime-speech-diarization-on-local-machine-no-gpu-external-apis</guid>
      <pubDate>Mon, 01 Jul 2024 17:09:07 GMT</pubDate>
    </item>
    <item>
      <title>如何解决VAE训练中的梯度爆炸问题？</title>
      <link>https://stackoverflow.com/questions/78693456/how-to-solve-exploding-gradient-problem-in-vae-training</link>
      <description><![CDATA[我尝试在 CelebA 数据集上实现 VAE，灵感来自 MNIST 的 Tensorflow 实现。我尝试过改变批处理大小，但似乎没有效果。形成的图像大部分都是灰色的。理想情况下，我们希望 KL 散度和重建损失都接近于零，但在我的例子中，两者都呈指数增长。
这是我得到的损失曲线。
这是损失函数定义块：
optimizer = tf.keras.optimizers.Adam(1e-4)
def log_normal_pdf(sample, mean, logvar, raxis=1):
log2pi = tf.math.log(2. * np.pi)
return tf.reduce_sum(
-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),
axis=raxis)

def compute_loss(model, x):
mean, logvar = model.encode(x)
z = model.reparameterize(mean, logvar)
x_logit = model.decode(z)
cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)
#logpx_z = tf.reduce_mean(tf.square(x - x_logit), axis=[1, 2, 3])
logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])
logpz = log_normal_pdf(z, 0., 0.)
logqz_x = log_normal_pdf(z, mean, logvar)
return -tf.reduce_mean(logpx_z + logpz - logqz_x), logpx_z, logqz_x-logpz

我的潜在维度是 16，批量大小为 500。另外，我的输入只有 500 张图片。
我已经尝试更改输入的大小，但似乎没有影响。
以下是模型定义：
class CVAE(tf.keras.Model):
def __init__(self, latent_dim):
super(CVAE, self).__init__()
self.latent_dim = latent_dim
self.encoder = tf.keras.Sequential(
[
tf.keras.layers.InputLayer(input_shape=(64, 64, 3)),
tf.keras.layers.Conv2D(
filters=32, kernel_size=3, strides=(2, 2),activation=&#39;relu&#39;),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Conv2D(
filters=64, kernel_size=3, strides=(2, 2),activation=&#39;relu&#39;),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Conv2D(
filters=128, kernel_size=3, strides=(2, 2), activity=&#39;relu&#39;),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Conv2D(
filters=256, kernel_size=3, strides=(2, 2), activity=&#39;relu&#39;),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Conv2D(
filters=512, kernel_size=3, strides=(2, 2), activity=&#39;relu&#39;),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.Flatten(),
# 无激活
tf.keras.layers.Dense(latent_dim + latent_dim),
]
)

self.decoder = tf.keras.Sequential(
[
tf.keras.layers.InputLayer(input_shape=(latent_dim,)),
tf.keras.layers.Dense(units=4*4*256,activation=tf.nn.relu),
tf.keras.layers.Reshape(target_shape=(4, 4, 256)),
tf.keras.layers.Conv2DTranspose(
filters=128, kernel_size=3, strides=2, padding=&#39;same&#39;,
activation=&#39;relu&#39;),
tf.keras.layers.Conv2DTranspose(
filters=64, kernel_size=3, strides=2, padding=&#39;same&#39;,
激活=&#39;relu&#39;),
tf.keras.layers.Conv2DTranspose(
过滤器=32，kernel_size=3，strides=2，padding=&#39;same&#39;,
激活=&#39;relu&#39;),
tf.keras.layers.Conv2DTranspose(
过滤器=3，kernel_size=3，strides=2，padding=&#39;same&#39;),
]
)
@tf.function
def sample(self，eps=None):
如果 eps 为 None:
eps = tf.random.normal(shape=(100，self.latent_dim))
返回 self.decode(eps，apply_sigmoid=True)

def encode(self，x):
平均值，logvar = tf.split(self.encoder(x)，num_or_size_splits=2，axis=1)
返回平均值，logvar

def reparameterize(self，平均值， logvar):
eps = tf.random.normal(shape=mean.shape)
return eps * tf.exp(logvar * .5) + mean

def decrypt(self, z, apply_sigmoid=False):
logits = self.decoder(z)
if apply_sigmoid:
probs = tf.sigmoid(logits)
return probs
return logits

这是 colab 笔记本的链接]]></description>
      <guid>https://stackoverflow.com/questions/78693456/how-to-solve-exploding-gradient-problem-in-vae-training</guid>
      <pubDate>Mon, 01 Jul 2024 17:03:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 Detectron2 进行多任务问题分割和关键点检测</title>
      <link>https://stackoverflow.com/questions/78692534/multitask-issue-segmentation-keypoint-detection-with-detectron2</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78692534/multitask-issue-segmentation-keypoint-detection-with-detectron2</guid>
      <pubDate>Mon, 01 Jul 2024 13:32:45 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 Pytorch 提取视觉转换器的倒数第二层输出</title>
      <link>https://stackoverflow.com/questions/78691616/cannot-extract-the-penultimate-layer-output-of-a-vision-transformer-with-a-pytor</link>
      <description><![CDATA[我有以下模型，该模型使用我自己的 DataParallel 训练的数据集进行了调整：
model = timm.create_model(&#39;vit_base_patch16_224&#39;, pretrained=False)
model.head = nn.Sequential(nn.Linear(768, 512),nn.ReLU(),nn.BatchNorm1d(512),nn.Dropout(p=0.2),nn.Linear(512, 141))
checkpoint = torch.load(&#39;vit_b_16v3.pth&#39;)
checkpoint = {k.partition(&#39;module.&#39;)[2]: v for k, v in checkpoint.items()}
# 加载参数
model.load_state_dict(checkpoint)

但是，我不知道如何获取这种视觉转换器的倒数第二层输出。我尝试了本教程，但不起作用。我只想输入一张图片，并有一个 512 维向量来描述它。使用 Tensorflow 做这件事很容易，但在 Pytorch 中我却很挣扎。
我最后的几层如下：
(norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(fc_norm): Identity()
(head_drop): Dropout(p=0.0, inplace=False)
(head): Sequential(
(0): Linear(in_features=768, out_features=512, bias=True)
(1): ReLU()
(2): BatchNorm1d(512, eps=1e-05, motivation=0.1, affine=True, track_running_stats=True)
(3): Dropout(p=0.2, inplace=False)
(4): Linear(in_features=512, out_features=141, bias=True)
)
)
]]></description>
      <guid>https://stackoverflow.com/questions/78691616/cannot-extract-the-penultimate-layer-output-of-a-vision-transformer-with-a-pytor</guid>
      <pubDate>Mon, 01 Jul 2024 10:16:39 GMT</pubDate>
    </item>
    <item>
      <title>Roboflow Vs. Darknet 用于生成权重文件和创建模型</title>
      <link>https://stackoverflow.com/questions/78691574/roboflow-vs-darknet-for-generating-weight-file-and-creating-the-model</link>
      <description><![CDATA[我有一个 YoloV8 数据文件格式，它是手动完成的数据（图像）注释。
生成模型并因此产生权重文件的最有效和最直接的方法是什么？是通过以下命令使用 darknet 吗：
darknet.exe detector train data/obj.data yolo-obj.cfg backup\yolo-obj_2000.weights

然后使用类似下面的命令生成关联模型：
python tools/model_converter/convert.py cfg/yolov3.cfg weights/yolov3.weights weights/yolov3.h5

或者通过以下命令使用 Roboflow：
version.deploy(model_type=&quot;yolov8&quot;, model_path=f”{HOME}/runs/detect/train/&quot;)

在我看来，darknet 更难安装。]]></description>
      <guid>https://stackoverflow.com/questions/78691574/roboflow-vs-darknet-for-generating-weight-file-and-creating-the-model</guid>
      <pubDate>Mon, 01 Jul 2024 10:07:36 GMT</pubDate>
    </item>
    <item>
      <title>学习 Python 的最佳书籍 [关闭]</title>
      <link>https://stackoverflow.com/questions/78690958/best-book-to-learn-python</link>
      <description><![CDATA[寻求具有最新更新的最佳 Python 书籍推荐
我渴望从头开始学习 Python，并及时了解该语言的最新发展。随着 Python 的快速发展，我想确保自己学习的是最新的功能、最佳实践和行业标准。
您能否推荐一本全面且适合初学者的书，涵盖 Python 3.x（最好是最新版本 Python 3.10 或 3.11），并包含以下主题：

核心 Python 概念：变量、数据类型、控制结构、函数、面向对象编程等
数据分析和可视化：NumPy、Pandas、Matplotlib 和 Seaborn
Web 开发：Flask 或 Django、HTML、CSS 和 JavaScript 基础知识
机器学习和人工智能：scikit-learn、TensorFlow 和 Keras
最佳实践和编码标准：代码组织、调试和测试
]]></description>
      <guid>https://stackoverflow.com/questions/78690958/best-book-to-learn-python</guid>
      <pubDate>Mon, 01 Jul 2024 07:35:55 GMT</pubDate>
    </item>
    <item>
      <title>在 shap.Explainer() 中，我应该输入 classifier 还是 classifier.predict？那么，如何获取 shap 值？[关闭]</title>
      <link>https://stackoverflow.com/questions/78690391/in-shap-explainer-should-i-input-classifier-or-classifier-predict-then-how</link>
      <description><![CDATA[我正在使用 SHAP（Shapley Additive Explanations）。我理解工作流程必须是：将我的数据拆分为训练和测试，训练我的模型，运行 SHAP 解释器，获取 SHAP 值。
当然，我见过使用不同方法做同样事情的代码（有点像 Perl 哲学，但很好）。我搞不懂它们之间的区别。我已阅读 SHAP 文档，但未能理解正确的方法。
我已看到两者：

explainer = shap.Explainer(clf)
explainer = shap.Explainer (clf.predict, X train)

后来，为了获取 shap 值，我看到了：

explainer(X)
explainer(X_test)
explainer.shap_values(X_test) - 这是我唯一理解差异的。它返回一个 numpy 数组，而不是 shap 解释对象。

下面，我复制了一些我见过的例子。
在Towards Data Science中：
X_train, X_test, y_train, y_test = train_test_split(X, y)
clf.fit(X_train, y_train)

explainer = shap.Explainer(clf.predict, X_test)
shap_values = explainer(X_test)

在 SHAP 官方GitHub 页面（不是他们的文档）
explainer = shap.Explainer(clf)
shap_values = explainer(X)

在geeks for geeks和datacamp
X_train, X_test, y_train, y_test = train_test_split(X, y)
clf.fit(X_train, y_train)

解释器 = shap.Explainer(clf)
shap_values = explainer(X_test)
]]></description>
      <guid>https://stackoverflow.com/questions/78690391/in-shap-explainer-should-i-input-classifier-or-classifier-predict-then-how</guid>
      <pubDate>Mon, 01 Jul 2024 03:49:45 GMT</pubDate>
    </item>
    <item>
      <title>将 ONNX 模型转换为 Tensorflow Lite - 不支持 pytorch_half_pixel</title>
      <link>https://stackoverflow.com/questions/78218890/converting-onnx-model-to-tensorflow-lite-pytorch-half-pixel-not-supported</link>
      <description><![CDATA[我正在尝试将 ONNX 模型转换为 Tensorflow Lite 格式。代码简单，但出现此错误。我更新了我的 onnx 版本，但没有成功
import onnx
import tensorflow as tf
import onnx_tf
#
#
# README：此文件将 onnx 模型转换为 tflite
#
#
#

onnx_model_path = &#39;/home/sfrye/segmentation/segmentation_checkpoints/efficientnet/modified-new.onnx&#39;

onnx_model = onnx.load(onnx_model_path)

tf_model = onnx_tf.backend.prepare(onnx_model)
tf_model.export_graph(&quot;tflite_model.tf&quot;)

错误如下

RuntimeError：在用户代码中：

文件&quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/backend_tf_module.py&quot;，第 99 行，在 __call__ *
output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,
File &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/backend.py&quot;，第 347 行，在 _onnx_node_to_tensorflow_op *
return handler.handle(node, tensor_dict=tensor_dict, strict=strict)
File &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/handlers/handler.py&quot;，第 58 行，在 handle *
cls.args_check(node, **kwargs)
文件 &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/handlers/backend/resize.py&quot;，第 125 行，在 args_check *
exception.OP_UNSUPPORTED_EXCEPT(
文件 &quot;/home/sfrye/miniconda3/envs/mars_env/lib/python3.8/site-packages/onnx_tf/common/exception.py&quot;，第 50 行，在 __call__ *
raise self._func(self.get_message(op, framework))

RuntimeError: Tensorflow 不支持调整 coordinate_transformation_mode=pytorch_half_pixel 的大小。


我尝试更新我的 onnx，因为这解决了此错误代码的某些问题]]></description>
      <guid>https://stackoverflow.com/questions/78218890/converting-onnx-model-to-tensorflow-lite-pytorch-half-pixel-not-supported</guid>
      <pubDate>Mon, 25 Mar 2024 11:55:44 GMT</pubDate>
    </item>
    <item>
      <title>Pycaret 设置独热编码</title>
      <link>https://stackoverflow.com/questions/74001472/pycaret-setup-for-one-hot-encoding</link>
      <description><![CDATA[我陷入了 Pycaret 中分类变量独热编码的问题。问题是，即使设置了我的分类变量，管道也会对分类变量应用规范化，我不知道我做错了什么。
首先，使用下面的代码一切正常：
from pycaret.classification import *
from pycaret.datasets import get_data
import pandas as pd
import numpy as np
import seaborn as sns
dataset = get_data(&#39;income&#39;)
dataset.dtypes

直到我开始设置和
exp_clf01 = setup( data = dataset
, target = &#39;income &gt;50K&#39;
, session_id = 123
, numeric_features = [&#39;age&#39;,&#39;education-num&#39;,&#39;capital-gain&#39;,&#39;capital-loss&#39;,&#39;hours-per-week&#39;]
, categorical_features = [&#39;workclass&#39;,&#39;education&#39;,&#39;marital-status&#39;,&#39;occupation&#39;,&#39;relationship&#39;,&#39;race&#39;,&#39;sex&#39;,&#39;native-country&#39;]
)
df_transformed = get_config(&quot;X_train&quot;)
df_transformed.head()

尝试查看数据框的头部后，它仅将独热编码应用于列 race，并将其他分类输入标准化，我不明白为什么。




age
workclass
education
education-num
marital-status
occupation
other列




46.0
0.303273
0.271186
11.0
0.101942
0.484643
...


27. 0
0.218620
0.412939
13.0
0.044165
0.484643
...


33.0
0.218557
0.568315
 14.0
0.448894
0.455449
...


60.0
0.218557
0.412673
13.0
0.448894
0.484286
&lt; td&gt;...


25.0
0.218620
0.063798
6.0
0.044165
0.229692
...




我该如何防止这种行为？]]></description>
      <guid>https://stackoverflow.com/questions/74001472/pycaret-setup-for-one-hot-encoding</guid>
      <pubDate>Sun, 09 Oct 2022 00:59:48 GMT</pubDate>
    </item>
    <item>
      <title>当setCar设置为true时，如何显示前提和后果？</title>
      <link>https://stackoverflow.com/questions/39066421/how-to-display-the-premise-and-consequence-when-the-setcar-is-set-to-true</link>
      <description><![CDATA[我想在 Weka 3.8.0 中运行 apriori 算法后，获取生成规则的每一行的前提和后果。
 apriori.setNumRules(NUMBER_OF_RULES);
apriori.setMinMetric(MINIMUM_CONFIDENCE);
apriori.setLowerBoundMinSupport(MINIMUM_SUPPORT);

apriori.setCar(true);

apriori.buildAssociations(instances);

我尝试使用下面的代码来获取规则，但它给出了一个异常
（weka.associations.ItemSet 无法转换为 weka.associations.AprioriItemSet）：
 AssociationRules arules = apriori.getAssociationRules();

此外，我尝试使用 getAllTheRules() 方法，但它给出了不同的结果。
 ArrayList&lt;Object&gt;[] arules = apriori.getAllTheRules();
System.out.println(((ItemSet)arules[0].get(1)).getRevision()); //12014
System.out.println(((ItemSet)arules[0].get(2)).getRevision()); //12014
System.out.println(((ItemSet)arules[0].get(5)).getRevision()); //12014
]]></description>
      <guid>https://stackoverflow.com/questions/39066421/how-to-display-the-premise-and-consequence-when-the-setcar-is-set-to-true</guid>
      <pubDate>Sun, 21 Aug 2016 16:30:51 GMT</pubDate>
    </item>
    </channel>
</rss>