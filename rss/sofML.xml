<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Mon, 22 Jan 2024 09:15:58 GMT</lastBuildDate>
    <item>
      <title>如何将 Spacy Model .pkl 文件转换为 .pt/.pth pytorch 支持的格式</title>
      <link>https://stackoverflow.com/questions/77858297/how-to-convert-spacy-model-pkl-file-into-pt-pth-pytorch-supported-format</link>
      <description><![CDATA[我有 spacy 模型，用于以 .pkl 格式进行推理。 .pkl 文件的数据类型是。我想让推理脚本在 GPU 上运行。我尝试使用 spacy gpu、numba 等不同的方法。
导入spacy
spacy.prefer_gpu() # 或 spacy.require_gpu()
nlp = spacy.load(“内容/路径”)

它不起作用，我认为通过将 .pkl 转换为 .pt 将通过将 pt 文件加载到“cuda”设备来工作。请提出处理这种情况的方法。]]></description>
      <guid>https://stackoverflow.com/questions/77858297/how-to-convert-spacy-model-pkl-file-into-pt-pth-pytorch-supported-format</guid>
      <pubDate>Mon, 22 Jan 2024 07:43:08 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Google Colab 上安装和运行 Docker Compose？</title>
      <link>https://stackoverflow.com/questions/77858250/how-to-install-and-run-docker-compose-on-google-colab</link>
      <description><![CDATA[有人可以告诉我如何在 Google Colab 上运行 docker compose。
我已经尝试过很多次了。大多数时候我在 google colab 中都会遇到错误。
$ docker-compose up -d

我正在尝试使用 docker-compose 部署我的 Spring Boot Web 服务。在我的 docker-compose.yml 中
https://github.com/milvus-io /bootcamp/tree/e69ee26188cf24c4994dfd9eecf00ef3950fcd44/applications/image/reverse_image_search
在 Google Colab 上运行 Docker]]></description>
      <guid>https://stackoverflow.com/questions/77858250/how-to-install-and-run-docker-compose-on-google-colab</guid>
      <pubDate>Mon, 22 Jan 2024 07:30:12 GMT</pubDate>
    </item>
    <item>
      <title>多种产品的预测模型</title>
      <link>https://stackoverflow.com/questions/77857775/forecasting-model-for-multiple-products</link>
      <description><![CDATA[嗨，我是数据科学的一个相对较新的人，我有一个时间序列问题，我必须预测 100 多种产品的销售，并且所有产品都有不同的模式，而且新产品会不断添加，这很困难要单独建模它们，我还必须设置再训练流程，如何简化这个过程，有没有办法概括模型选择、验证和再训练，而不必每次都单独建模？
到目前为止，我已经尝试对它们进行单独建模，使用了 SARIMA，但它并不适用于我的所有产品。我的数据由日期和体积组成，而且在大多数情况下，我在 acf 和 pcf 图中也没有观察到任何重要点。]]></description>
      <guid>https://stackoverflow.com/questions/77857775/forecasting-model-for-multiple-products</guid>
      <pubDate>Mon, 22 Jan 2024 05:19:57 GMT</pubDate>
    </item>
    <item>
      <title>如何提高YOLOv8自定义模型的准确率？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77855989/how-to-increase-the-accuracy-of-yolov8-custom-model</link>
      <description><![CDATA[我最近使用 YOLOv8 训练了一个用于动物物种检测的自定义数据集，但得到的结果非常糟糕。我有 70 多个类，每个类平均约有 80 张图像。我对模型进行了 200 个时期的训练，大约花费了 1.5 天。
我使用 Roboflow 来注释数据集。
我首先在 100 个 epoch 上训练模型，这给了我不好的结果，所以我再次尝试在 200 个 epoch 上训练模型，这比之前的模型得到了更好的结果。模型的准确率很低，没有达到我的预期。所以我需要一些帮助来提高模型的准确性。 我打算将类别增加到 110-115，但从当前的训练结果来看，我首先尝试使模型的准确率至少达到 75%。]]></description>
      <guid>https://stackoverflow.com/questions/77855989/how-to-increase-the-accuracy-of-yolov8-custom-model</guid>
      <pubDate>Sun, 21 Jan 2024 17:57:19 GMT</pubDate>
    </item>
    <item>
      <title>如何为 Pytorch 预处理表格数据中的地址、纬度和经度特征</title>
      <link>https://stackoverflow.com/questions/77855711/how-to-preprocess-address-latitude-and-longitude-features-in-tabular-data-for-p</link>
      <description><![CDATA[我已将数据清除到接下来的 6 列中，您可以看到，这是我的输入数据。我分割数据集，将标签放在另一个变量 Y 中。
我的主要问题：我不知道如何预处理数据以便为任何模型提供良好的输入。
我的数据集 X 如下所示：

&lt;表类=“s-表”&gt;
&lt;标题&gt;

描述
提示
地址
地区
纬度
经度


&lt;正文&gt;

加尔彭
工业
Subdivisión de la Finca Denominada Violeta S/N
阿里卡和帕里纳科塔-阿里卡地区
-19.423411
-11.371551





desc - 字符串
tipo - 字符串
地址 - 字符串
区域 - 字符串
纬度 - 字符串
经度 - 字符串

我的数据集 Y 看起来像

&lt;表类=“s-表”&gt;
&lt;标题&gt;

首席信息官


&lt;正文&gt;

169379



我尝试过的内容
我已经遵循了这个教程，它允许我对表格数据有了一些了解，但数据完全不同，我不知道它是否也适合我。因此，我的代码将所有数据转换为 LabelEncoder，但显然这不适用于纬度和经度。
对于 df.columns 中的 col：
    如果 df.dtypes[col] == “对象”：
        df[col] = df[col].fillna(“NA”)
    别的：
        df[列] = df[列].fillna(0)
    df[col] = LabelEncoder().fit_transform(df[col])

对于 df.columns 中的 col：
    df[col] = df[col].astype(&#39;类别&#39;)

此外，作者还使用了一些分类嵌入，我不知道它们是否也适用于我的数据类型。]]></description>
      <guid>https://stackoverflow.com/questions/77855711/how-to-preprocess-address-latitude-and-longitude-features-in-tabular-data-for-p</guid>
      <pubDate>Sun, 21 Jan 2024 16:39:27 GMT</pubDate>
    </item>
    <item>
      <title>GaussianNB 准确率分数似乎不起作用[关闭]</title>
      <link>https://stackoverflow.com/questions/77855644/gaussiannb-accuracy-score-seemingly-not-working</link>
      <description><![CDATA[这是我的代码：
来自 sklearn.feature_extraction.text 导入 *
从 sklearn.model_selection 导入 *
从 sklearn.metrics 导入 *

CV = CountVectorizer()

#将稀疏数组转换为密集数组#transformed_text 是单句输入
X =cv.fit_transform(df[&#39;transformed_text&#39;]).toarray()


#获取标记输出y
y =df[&#39;目标&#39;].值


#20% 的数据用于测试
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)


gnb = GaussianNB()

gnb.fit(X_train,y_train)
y_pred1=gnb.predict(X_train)

准确度分数（y_test，y_pred1）

虽然我使用了average=weighed，但我在accuracy_score(y_test, y_pred1)部分遇到错误
------------------------------------------------ ----------------------------------------------------
ValueError Traceback（最近一次调用最后一次）
&lt;ipython-input-96-f91e0ea15d1f&gt;在&lt;细胞系：1&gt;()
----&gt; 1 准确度分数（y_test，y_pred1）

3帧
包装器中的 /usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py(*args, **kwargs)
    190
    191 尝试：
--&gt; 192 return func(*args，**kwargs)
    193 除了 InvalidParameterError 为 e：
    攀上漂亮女局长之后194

/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py中的accuracy_score（y_true，y_pred，标准化，sample_weight）
    219
    [第 220 章] 第 220 章
--&gt;第221章
    第222章
    223 if y_type.startswith(“multilabel”):

/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py 中的 _check_targets(y_true, y_pred)
     84 y_pred ：数组或指示矩阵
     第85章
---&gt; 86 检查一致长度（y_true，y_pred）
     87 type_true = type_of_target(y_true, input_name=“y_true”)
     88 type_pred = type_of_target(y_pred, input_name=“y_pred”)

/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py 在 check_consistent_length(*arrays)
    第395章
    [第 396 章] 1：
--&gt;第397章
    [第 398 章]
    399 % [int(l)，l 的长度]

ValueError：发现输入变量的样本数量不一致：[1034, 4135]

我正在遵循的教程实现此功能没有任何问题
有什么帮助吗？]]></description>
      <guid>https://stackoverflow.com/questions/77855644/gaussiannb-accuracy-score-seemingly-not-working</guid>
      <pubDate>Sun, 21 Jan 2024 16:18:05 GMT</pubDate>
    </item>
    <item>
      <title>Flower - 联邦学习的每一轮模型精度都是相同的</title>
      <link>https://stackoverflow.com/questions/77855250/model-accuracy-is-the-same-after-every-round-with-flower-federated-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77855250/model-accuracy-is-the-same-after-every-round-with-flower-federated-learning</guid>
      <pubDate>Sun, 21 Jan 2024 14:33:27 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 输入、输出和隐藏状态混乱 [关闭]</title>
      <link>https://stackoverflow.com/questions/77855145/lstm-input-output-and-hidden-state-confusion</link>
      <description><![CDATA[我对如何准备用于训练 LSTM 的数据感到困惑。它们有一个隐藏状态（至少在 Keras 中也是输出）和一个单元状态，它们都用于下一个时间步。我看到很多人也使用一个时间步的输出作为下一时间步的输入，但根据我的理解，信息应该已经被隐藏和/或单元状态覆盖。
有时人们想要预测时间序列的下一个时间步长并使用之前的时间步长作为输入，这对我来说很有意义。但就我而言，我有多个时间序列作为输入，多个不同时间序列作为输出，我知道这些时间序列取决于输入。
在这种情况下，您是否也会使用输出作为下一个时间步的输入？
LSTM 是解决此类问题的正确选择吗？]]></description>
      <guid>https://stackoverflow.com/questions/77855145/lstm-input-output-and-hidden-state-confusion</guid>
      <pubDate>Sun, 21 Jan 2024 14:09:41 GMT</pubDate>
    </item>
    <item>
      <title>在微调LLM模型时如何给数据集赋予权重或排名？</title>
      <link>https://stackoverflow.com/questions/76958393/how-to-give-weights-or-ranking-to-dataset-while-finetuning-the-llm-model</link>
      <description><![CDATA[我目前正在使用 Llama 配方和 LoRA 技术对 meta-llama/Llama-2-7b-chat-hf 模型进行微调。我的方法包括采用即时工程来改进模型的性能，利用以 Alpaca 格式呈现的数据：
&lt;前&gt;&lt;代码&gt;[
    {
        &quot;instruction&quot;: &quot;什么是 CubeOS？&quot;,
        “输入”：“”，
        “输出”：“CubeOS 是专门的操作系统，包含操作 Cube 所需的所有软件和驱动程序。”
    },
    {
        &quot;instruction&quot;: &quot;Myst 是什么？&quot;,
        “输入”：“”，
        “output”：“Myst 作为 Cube 的控制台界面，也是随附应用程序的指定名称。”
    },
    。
    。
    。
]

这个过程使我能够有效地微调模型并将其应用于回答与机密文档相关的问题。
我尝试过为问答对分配分数，然后按照这些分数的顺序排列它们以进行微调。然而，我遇到了挑战，因为该模型似乎没有根据分数较高的数据的重要性给出结果。
我发现了一篇标题为 https://towardsdatascience.com/how-to-fine-tune-llama2-for-python-coding-on-consumer-hardware-46942fa3cf92，作者似乎采用了类似的方法。&lt; /p&gt;
此外，我还探索了其他资源，提出了一种方法，涉及确定各个特征的相关性分数，然后对它们进行排序以进行微调。以下链接 https://pyvideo.org /pydata-warsaw-2019/learning-to-rank-with-the-transformer.html 提供了对此技术的见解。
我还尝试使用 llama-2 所需的提示结构来训练模型：
[INST] &lt;&gt;&gt; {{ system_prompt }} &lt;&lt;/SYS&gt;&gt; {{ 用户消息 }} [/INST]

但是，这种方法并没有对答案产生令人满意的强调。
鉴于我拥有 PDF 和 DOC 格式的文档，我的目标是为特定文档分配更大的权重，并确保它们优先出现在最佳答案中。
我非常感谢您指导如何通过合并权重或分数来强调某些文档的重要性来微调模型。]]></description>
      <guid>https://stackoverflow.com/questions/76958393/how-to-give-weights-or-ranking-to-dataset-while-finetuning-the-llm-model</guid>
      <pubDate>Wed, 23 Aug 2023 05:01:40 GMT</pubDate>
    </item>
    <item>
      <title>AutoModelForSeq2SeqLM 和 AutoModelForCausalLM 之间的区别</title>
      <link>https://stackoverflow.com/questions/75549632/difference-between-automodelforseq2seqlm-and-automodelforcausallm</link>
      <description><![CDATA[根据标题，Huggingface 上的这两个自动类有何不同？我尝试阅读文档，但没有找到区分信息]]></description>
      <guid>https://stackoverflow.com/questions/75549632/difference-between-automodelforseq2seqlm-and-automodelforcausallm</guid>
      <pubDate>Thu, 23 Feb 2023 19:45:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在海量数据上训练机器学习模型？</title>
      <link>https://stackoverflow.com/questions/74886400/how-to-train-a-machine-learning-model-on-huge-amount-of-data</link>
      <description><![CDATA[关键点：数据集太大了，我几乎无法将其存储在硬件中。 （拍字节）
假设我的数据集中有数万亿行。该数据集太大，无法存储在内存中。我想在这个数据集上训练一个机器学习模型，比如逻辑回归。我该怎么办？
现在，我知道亚马逊/谷歌在大量数据上进行机器学习。他们怎样做呢？例如点击数据集，全局每个智能设备的输入都存储在一个数据集中。
拼命寻找新想法并乐于接受修正。
我的思路：

加载部分数据到内存
执行梯度下降

这样优化就是小批量下降。
现在的问题是，在优化中，无论是SGD还是mini Batch，在最坏的情况下，当它遍历完所有数据时就会停止。遍历整个数据集是不可能的。
所以我有了提前停止的想法。提前停止保留验证集，并在错误停止下降/收敛于验证集时停止优化。但由于数据集的大小，这可能不可行。
现在我正在考虑简单地随机采样训练集和测试集，并使用可行的大小来训练模型。]]></description>
      <guid>https://stackoverflow.com/questions/74886400/how-to-train-a-machine-learning-model-on-huge-amount-of-data</guid>
      <pubDate>Thu, 22 Dec 2022 09:16:33 GMT</pubDate>
    </item>
    <item>
      <title>如何对拥抱脸部模型进行批量推理？</title>
      <link>https://stackoverflow.com/questions/68058974/how-to-do-batch-inferenece-for-hugging-face-models</link>
      <description><![CDATA[我想对 MarianMT 模型进行批量推理。代码如下：
从变压器导入 MarianTokenizer
tokenizer = MarianTokenizer.from_pretrained(&#39;赫尔辛基-NLP/opus-mt-en-de&#39;)
src_texts = [“我是一只小青蛙。”,“汤姆向老师寻求建议。”]
tgt_texts = [“Ich bin ein kleiner Frosch。”,“Tom bat seinen Lehrer um Rat。”] # 可选
输入 = tokenizer(src_texts, return_tensors=“pt”, padding=True)
使用 tokenizer.as_target_tokenizer()：
    标签 = tokenizer(tgt_texts, return_tensors=“pt”, padding=True)
输入[“标签”] = 标签[“input_ids”]
输出=模型（**输入）

如何进行批量推理？]]></description>
      <guid>https://stackoverflow.com/questions/68058974/how-to-do-batch-inferenece-for-hugging-face-models</guid>
      <pubDate>Sun, 20 Jun 2021 18:35:50 GMT</pubDate>
    </item>
    <item>
      <title>Google Colaboratory：有关其 GPU 的误导性信息（仅 5% RAM 可供某些用户使用）</title>
      <link>https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available</link>
      <description><![CDATA[更新：这个问题与Google Colab的“笔记本设置：硬件加速器：GPU”有关。这个问题是在添加“TPU”选项之前写的。
阅读了多个关于 Google Colaboratory 提供免费 Tesla K80 GPU 的激动人心的公告，我尝试运行 fast.ai 课程让它永远无法完成 - 内存很快就耗尽了。我开始调查原因。
最重要的是，“免费 Tesla K80”并不是对所有人来说都是“免费”的——对于某些人来说，只有一小部分是“免费”的。 
我从加拿大西海岸连接到 Google Colab，但本应是 24GB GPU RAM 的却只有 0.5GB。其他用户可以使用 11GB GPU RAM。
显然 0.5GB GPU RAM 不足以满足大多数 ML/DL 工作的需要。
如果您不确定自己得到什么，这里是我整理的一些调试功能（仅适用于笔记本电脑的 GPU 设置）：
# 内存占用支持库/代码
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip 安装 gputil
!pip 安装 psutil
!pip 安装人性化
导入 psutil
导入人性化
导入操作系统
导入 GPUtil 作为 GPU
GPU = GPU.getGPUs()
# XXX：Colab 上只有一个 GPU，且无法保证
GPU = GPU[0]
def printm():
 进程 = psutil.Process(os.getpid())
 print(&quot;Gen RAM 空闲：&quot; + humanize.naturalsize( psutil.virtual_memory().available ), &quot; | Proc 大小：&quot; + humanize.naturalsize( process.memory_info().rss))
 print(&quot;GPU RAM 可用：{0:.0f}MB | 已用：{1:.0f}MB | Util {2:3.0f}% | 总计 {3:.0f}MB&quot;.format(gpu.memoryFree, gpu.memoryUsed、gpu.memoryUtil*100、gpu.memoryTotal))
打印（）

在运行任何其他代码之前在 jupyter 笔记本中执行它会给我：
Gen RAM 可用：11.6 GB |进程大小：666.0 MB
GPU 可用内存：566MB |已用：10873MB |利用率 95% |总计 11439MB

获得完整卡的幸运用户将看到：
Gen RAM 可用：11.6 GB |进程大小：666.0 MB
GPU 可用内存：11439MB |已用：0MB |利用率 0% |总计 11439MB

您是否发现我从 GPUtil 借用的 GPU RAM 可用性计算有任何缺陷？
您能否确认，如果您在 Google Colab 笔记本上运行此代码，您会得到类似的结果吗？
如果我的计算正确，有什么办法可以在免费盒子上获得更多 GPU RAM 吗？
更新：我不确定为什么我们中的一些人得到的只是其他用户的 1/20。例如帮助我调试这个的人来自印度，他掌握了全部内容！
注意：请不要再发送任何有关如何消除可能消耗 GPU 部分的潜在卡住/失控/并行笔记本的建议。不管你如何划分它，如果你和我在同一条船上并运行调试代码，你会发现你仍然获得总共 5% 的 GPU RAM（截至本次更新仍然如此）。]]></description>
      <guid>https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available</guid>
      <pubDate>Mon, 12 Feb 2018 15:44:14 GMT</pubDate>
    </item>
    <item>
      <title>Keras LSTM - 验证损失从 Epoch #1 开始增加</title>
      <link>https://stackoverflow.com/questions/48542473/keras-lstm-validation-loss-increasing-from-epoch-1</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/48542473/keras-lstm-validation-loss-increasing-from-epoch-1</guid>
      <pubDate>Wed, 31 Jan 2018 12:40:36 GMT</pubDate>
    </item>
    <item>
      <title>如何在keras中将词嵌入和softmax权重结合起来？</title>
      <link>https://stackoverflow.com/questions/47095673/how-to-tie-word-embedding-and-softmax-weights-in-keras</link>
      <description><![CDATA[对于 NLP 和视觉语言问题中的各种神经网络架构来说，将初始词嵌入层的权重与输出 softmax 的权重联系起来是很常见的。通常这会提高句子生成的质量。 （参见示例此处）
在 Keras 中，通常使用 Embedding 类嵌入单词嵌入层，但是似乎没有简单的方法将该层的权重与输出 softmax 联系起来。有人知道如何实现这一点吗？]]></description>
      <guid>https://stackoverflow.com/questions/47095673/how-to-tie-word-embedding-and-softmax-weights-in-keras</guid>
      <pubDate>Fri, 03 Nov 2017 12:20:38 GMT</pubDate>
    </item>
    </channel>
</rss>