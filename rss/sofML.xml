<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 23 May 2024 09:16:57 GMT</lastBuildDate>
    <item>
      <title>从点云中提取手的最快方法是什么？</title>
      <link>https://stackoverflow.com/questions/78521999/what-is-the-fastest-method-to-extract-a-hand-from-a-point-cloud</link>
      <description><![CDATA[我有包含人手和背景的点云数据集。但我想以最快的方式删除所有点，只保留手牌。
我尝试过使用阈值方法从点云中提取手。虽然这些方法速度快并且提供了不错的结果，但当点云中没有手时，它们缺乏准确性。有没有一种方法可以准确地从点云中提取手？我的目标是在保持速度的同时实现高精度。]]></description>
      <guid>https://stackoverflow.com/questions/78521999/what-is-the-fastest-method-to-extract-a-hand-from-a-point-cloud</guid>
      <pubDate>Thu, 23 May 2024 09:03:42 GMT</pubDate>
    </item>
    <item>
      <title>如何在不执行代码的情况下计算代码的递归深度/循环深度？</title>
      <link>https://stackoverflow.com/questions/78521967/how-to-calculate-recursion-depth-loop-depth-of-a-code-without-executing-it</link>
      <description><![CDATA[我正在尝试构建一个模型，该模型能够从静态变量（例如行数、函数数量、复杂性等）推断出动态指标，例如内存分配、GPU 消耗和 CPU 消耗。 
我想到的一个问题是如何将输入作为静态特征包含在这个模型中（我认为这是一个非常相关的变量）
以阶乘函数为例：
&lt;前&gt;&lt;代码&gt;结果 = 1
  对于范围 (1, 8 + 1) 内的 i：
     结果*=我
  返回结果

结果 = 1
  对于范围 (1, 80000 + 1) 内的 i：
     结果*=我
  返回结果

在此示例中，第二个代码的执行时间比第一个代码长得多。这种差异对于人眼来说是显而易见的，但如何才能静态地使模型变得明显呢？我指的是一个可以理解输入参数影响的通用模型。这可以在不执行代码的情况下实现吗？]]></description>
      <guid>https://stackoverflow.com/questions/78521967/how-to-calculate-recursion-depth-loop-depth-of-a-code-without-executing-it</guid>
      <pubDate>Thu, 23 May 2024 08:58:11 GMT</pubDate>
    </item>
    <item>
      <title>实施机器学习模型来评估成绩并根据教育流数据库选择继续教育路径的最简单方法是什么？</title>
      <link>https://stackoverflow.com/questions/78521362/what-is-the-easiest-way-to-implement-a-ml-model-to-evaluate-grades-and-choose-th</link>
      <description><![CDATA[要实施用于评估学生成绩和推荐教育途径的机器学习模型，首先要收集和预处理数据，包括成绩、课外活动和学生兴趣。对数据进行标准化和编码，然后将其分为训练集和测试集。选择并训练适当的模型，例如决策树或随机森林，并使用准确性和 F1 分数等指标评估其性能。使用用户友好的界面部署模型，以提供实时建议并确保遵守数据隐私法。利用新数据和反馈不断改进模型，解决任何偏差以保持公平性和准确性。
期望它能够基于全面的数据分析，提供准确、公正的指导。目标是创建一个用户友好的实时推荐系统，通过新数据和反馈不断改进，同时确保数据隐私并解决潜在偏见。]]></description>
      <guid>https://stackoverflow.com/questions/78521362/what-is-the-easiest-way-to-implement-a-ml-model-to-evaluate-grades-and-choose-th</guid>
      <pubDate>Thu, 23 May 2024 06:46:07 GMT</pubDate>
    </item>
    <item>
      <title>寻找特定于移动设备的二进制数据集</title>
      <link>https://stackoverflow.com/questions/78521260/seeking-dataset-of-mobile-specific-binaries</link>
      <description><![CDATA[我目前正在训练机器学习模型，并需要特定于移动设备的二进制文件的数据集。尽管我付出了努力，但我仍然无法找到大量的数据集。
向我建议的另一种选择是从 AOSP 批量下载二进制文件，但我不确定如何开始此过程。]]></description>
      <guid>https://stackoverflow.com/questions/78521260/seeking-dataset-of-mobile-specific-binaries</guid>
      <pubDate>Thu, 23 May 2024 06:19:48 GMT</pubDate>
    </item>
    <item>
      <title>如何使用权重和偏差 wandb 扫描实现多处理以实现最大并行化，特别是 count var 在此设置中如何工作？</title>
      <link>https://stackoverflow.com/questions/78521104/how-to-implement-multiprocessing-with-weights-and-biases-wandb-sweeps-for-maximu</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78521104/how-to-implement-multiprocessing-with-weights-and-biases-wandb-sweeps-for-maximu</guid>
      <pubDate>Thu, 23 May 2024 05:25:30 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 if 条件将两个 scikit-learn 子模型组合成一个整体并将其保存到 pickle 文件中？</title>
      <link>https://stackoverflow.com/questions/78520659/how-to-combine-two-scikit-learn-sub-models-into-an-ensemble-using-an-if-conditio</link>
      <description><![CDATA[我使用 IF 条件训练了两个 scikit-learn 模型（根据 X1 功能定义的标准生成了两个训练集）。如何将这个 if 条件与这两个经过训练的模型集成到一个单个整体模型中并将其保存到 pickle 文件中？]]></description>
      <guid>https://stackoverflow.com/questions/78520659/how-to-combine-two-scikit-learn-sub-models-into-an-ensemble-using-an-if-conditio</guid>
      <pubDate>Thu, 23 May 2024 02:12:38 GMT</pubDate>
    </item>
    <item>
      <title>matplotlib 未在 macOS Sonoma VS code 上正确安装 [已关闭]</title>
      <link>https://stackoverflow.com/questions/78519268/matplotlib-not-installing-correctly-on-macos-sonoma-vs-code</link>
      <description><![CDATA[在此处输入图片描述
我安装的所有其他模块导入正常，但 matplotlib 返回警告。我用了
pip3 install matplotlib 以便安装它，感谢您的帮助！
这是错误导入“matplotlib”无法从源代码解决]]></description>
      <guid>https://stackoverflow.com/questions/78519268/matplotlib-not-installing-correctly-on-macos-sonoma-vs-code</guid>
      <pubDate>Wed, 22 May 2024 17:58:55 GMT</pubDate>
    </item>
    <item>
      <title>我如何学习如何在 React 和 JS 中创建搜索+机器学习推荐算法？</title>
      <link>https://stackoverflow.com/questions/78519224/how-can-i-learn-how-to-create-a-searching-machine-learning-recommender-algorit</link>
      <description><![CDATA[我目前正在启动我的 A level 计算机科学 NEA 项目，并希望基本上构建 supercook 的克隆 在用户登录的地方，输入他们拥有的成分，应用程序将向他们显示从网上找到的食谱。然后我想实现一个机器学习方面，用户可以根据他们喜欢的程度对食谱进行评分，并且算法将调整向用户提供的推荐食谱。我要到四月才能完成这个项目（我仍然会在学习所有其他科目的同时完成这个项目，所以没有无限的空闲时间）。
我该如何学习使用 React + javascript 以及如何学习如何创建显示菜谱的算法以及机器学习方面的内容？我对这个项目有所有的想法，以及我希望它如何运作，我只是希望有人指导我去哪里学习。那么您认为在 Brain.js 或 Tensorflow.js 之类的东西中从头开始进行所有机器学习是否可行？或者尝试将预先存在的推荐算法适应我的应用程序会更好吗？如果是这样，我将如何解决这个问题？请记住，我的 javascript + React 知识非常有限，因此我正在从头开始学习这些知识，并且几乎没有机器学习知识。
我一直在对这些主题进行大量搜索，但由于我对这些类型的算法、推荐系统和机器学习的了解有限，我不太清楚我在做什么，所以需要一个清晰的解释和一些指导关于去哪里和做什么将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78519224/how-can-i-learn-how-to-create-a-searching-machine-learning-recommender-algorit</guid>
      <pubDate>Wed, 22 May 2024 17:47:47 GMT</pubDate>
    </item>
    <item>
      <title>建议用于 Stable_Diffuson webui 的服务器 GPU [关闭]</title>
      <link>https://stackoverflow.com/questions/78518811/suggest-server-gpu-for-stable-diffuson-webui</link>
      <description><![CDATA[我需要使用 controlnet 部署稳定的扩散 webui API，建议一些好的服务器 GPU 以便轻松部署，与 img2img 和 controlnet 配合良好，我需要最大速度 20 秒，在线服务器 GPUsSources
我在本地 GPU 上尝试过，但速度很低]]></description>
      <guid>https://stackoverflow.com/questions/78518811/suggest-server-gpu-for-stable-diffuson-webui</guid>
      <pubDate>Wed, 22 May 2024 16:12:37 GMT</pubDate>
    </item>
    <item>
      <title>在 Pytorch 中正确实现 F-Beta 分数作为损失</title>
      <link>https://stackoverflow.com/questions/78518778/correct-implementation-of-f-beta-score-as-a-loss-in-pytorch</link>
      <description><![CDATA[我正在尝试将 F-Beta 分数视为损失。据我所知，我不能对概率使用 &gt;= 或 argmax() 函数，因为它是不可微分的。如果我错了，请纠正我。
我能想到的最接近的一个是这个 Kaggle笔记本，但它适用于多标签和 Keras。我正在 PyTorch 中尝试进行二进制分类。
这是到目前为止我的代码：
导入火炬
导入 torch.nn.function 作为 F

ApproxFBetaLoss 类（torch.nn.Module）：
   
    def __init__(self, beta:float = 1, class_idx:int = 0, eps:float = 1e-12) -&gt;没有任何：
        ”“”
        想法来自：
        1. https://www.kaggle.com/code/rejpalcz/best-loss-function-for-f1-score-metric
        2. https://arxiv.org/pdf/2104.01459（替代F Beta损失）
        
        参数：
            beta：Beta 控制损失。 0＜贝塔&lt; 1 有利于精度且 beta &gt; 1 有利于召回，beta == 1 是 F-1 分数
            class_idx：哪个类是好还是坏。默认情况下，我们称其为 0 类概率
        ”“”
        超级().__init__()
        自我.beta = beta
        self.class_idx = class_idx
        self.eps = eps # 除以零的问题


    defforward(self, logits: torch.Tensor, labels: torch.Tensor):
        ”“”
        logits：[批量，二维] Logits
        标签：[批次]
        ”“”
        probabilities = F.softmax(logits, dim=1) # 我们也可以使用 1 类的 Sigmoid 来实现相同的效果

        # 预测 = torch.argmax(概率, dim=1).float() # argmax 可微吗？我不这么认为，因为它会给出 True/False 和中断
        预测 = 概率[:,self.class_idx] # 0 类概率 IS_BAD = CLASS_0

        # 我认为这个计算可能存在一些问题。这在多大程度上是正确的？
        true_positives = (预测 * labels).sum().to(device = logits.device, dtype = logits.dtype)
        false_positives = ((1 - 标签) * 预测).sum().to(device = logits.device, dtype = logits.dtype)
        false_negatives= (标签 * (1 - 预测)).sum().to(device = logits.device, dtype = logits.dtype)

        精度 = 真阳性 / (真阳性 + 假阳性 + self.eps)
        召回率=真阳性/（真阳性+假阴性+ self.eps）

        f_beta_score = (1 + self.beta**2) * (精度 * 召回率) / (self.beta**2 * 精度 + 召回率 + self.eps)

        return 1 - f_beta_score # 损失为 1 - F_BETA_SCORE
    


logits = torch.tensor([[2, 10], [7, 3], [1, 4], [6, 5], [10, 3], [3, 10], [10, 3], [ 3, 10]]).to(设备 = &quot;cuda:0&quot;, dtype = torch.bfloat16)
标签 = torch.tensor([0, 1, 1, 0, 1, 1, 0, 1]).to(device = &quot;cuda:0&quot;, dtype = torch.bfloat16)

ApproxFbetaLoss（beta = 1，class_idx = 1）（logits，标签）

这给我带来了 0.3594 的损失，并查看 sklearn 的 F-1 分数，Class-1 的 F-1 分数为  &gt;0.6666 这使得它的损失为 1-0.66 = 0.3333
看起来我可能很接近，但有人可以确认或更正吗？]]></description>
      <guid>https://stackoverflow.com/questions/78518778/correct-implementation-of-f-beta-score-as-a-loss-in-pytorch</guid>
      <pubDate>Wed, 22 May 2024 16:06:48 GMT</pubDate>
    </item>
    <item>
      <title>如何确定FastText模型在文本分类中的准确性？</title>
      <link>https://stackoverflow.com/questions/78518695/how-to-find-accuracy-of-fasttext-model-in-text-classification</link>
      <description><![CDATA[在机器学习中，所有模型都有准确度方程，而在 FastText 模型中，我们没有请支持。]]></description>
      <guid>https://stackoverflow.com/questions/78518695/how-to-find-accuracy-of-fasttext-model-in-text-classification</guid>
      <pubDate>Wed, 22 May 2024 15:50:57 GMT</pubDate>
    </item>
    <item>
      <title>ML AI 模型检测 SQL 数据库中的异常</title>
      <link>https://stackoverflow.com/questions/78516610/ml-ai-model-to-detect-anomalies-in-sql-database</link>
      <description><![CDATA[构建一个机器学习模型来检测 SQL 数据库中的异常，我应该采取什么方法
我使用了隔离森林模型，我希望当我将模型连接到数据库时，在命名表以检查异常后，模型返回所有​​可能的异常，并列出所有异常]]></description>
      <guid>https://stackoverflow.com/questions/78516610/ml-ai-model-to-detect-anomalies-in-sql-database</guid>
      <pubDate>Wed, 22 May 2024 09:42:44 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的分段似乎没有保存？关于totalsegmentator</title>
      <link>https://stackoverflow.com/questions/78516029/why-my-segmentations-dont-seem-to-be-saved-about-totalsegmentator</link>
      <description><![CDATA[我一步步按照你的教程操作，但得到的结果是“分段未保存”
这是我输入的语句和得到的结果：
(d:\totalsegmentotar.conda) D:\totalsegmentotar&gt;TotalSegmentator -i hip_left.nii.gz -o 分段 -ta hip_implant

如果您使用此工具，请引用：https://pubs.rsna.org/doi/10.1148/ryai.230024

未检测到 GPU。在CPU上运行。这可能会非常慢。 &#39;--fast&#39; 或 --roi_subset 选项可以帮助减少运行时间。
生成粗糙的身体分割...
重新采样...
1.93 秒内重新采样
预测...
d:\totalsegmentotar.conda\Lib\site-packages\nnunetv2\utilities\plans_handling\plans_handler.py:37: UserWarning: 检测到旧的 nnU-Net 计划格式。尝试重构网络架构参数。如果失败，请为您的数据集重新运行 nnUNetv2_plan_experiment。如果您使用自定义架构，请将 nnU-Net 降级到您实现的版本或更新您的实现+计划。
warnings.warn(“检测到旧的 nnU-Net 计划格式。尝试重建网络架构”
100%|███████████████████████████████████████████████ ███████████████████████████████████████████████████ ███████████████████████████████████████████████████ ██| 1/1 [00:00&lt;00:00, 1.12it/s]
预测12.95秒后
重新采样...
警告：无法裁剪，因为未检测到前景
从 (333, 333, 539) 裁剪到 (333, 333, 539)
预测...
d:\totalsegmentotar.conda\Lib\site-packages\nnunetv2\utilities\plans_handling\plans_handler.py:37: UserWarning: 检测到旧的 nnU-Net 计划格式。尝试重构网络架构参数。如果失败，请为您的数据集重新运行 nnUNetv2_plan_experiment。如果您使用自定义架构，请将 nnU-Net 降级到您实现的版本或更新您的实现+计划。
warnings.warn(“检测到旧的 nnU-Net 计划格式。尝试重建网络架构”
100%|███████████████████████████████████████████████ ███████████████████████████████████████████████████ ███████████████████████████████████████████████████ | 64/64 [04:27&lt;00:00, 4.18s/it]
预测 288.96 秒
保存分段...
0%| | 0/1 [00:00
可以看到分割没有保存，我用切片器软件看确实没有预测结果，什么也没有显示。
当我使用`-tatotal时，分割器进度条发生变化，但不幸的是它似乎没有保存分割的结果。这是我的输出，以及在切片器 5.6.2 中打开的输出文件夹和图像，但没有显示任何内容。
抱歉，我是新手，还不能显示图片，如果您看到了，请打开链接。
这是我的 powershell 输出
她是我的输出文件夹，并且是在切片器 5.6.2 中打开的图像]]></description>
      <guid>https://stackoverflow.com/questions/78516029/why-my-segmentations-dont-seem-to-be-saved-about-totalsegmentator</guid>
      <pubDate>Wed, 22 May 2024 07:52:18 GMT</pubDate>
    </item>
    <item>
      <title>对数核和指数激活的非线性关系模型精度无法达到 100%</title>
      <link>https://stackoverflow.com/questions/78514097/model-accuracy-for-non-linear-relationship-with-logarithm-kernel-and-exponential</link>
      <description><![CDATA[我正在开展一个项目，需要使用神经网络对非线性关系进行建模。关系为 ( y = 3x_1^2x_2^3 )。网络设置如下：

预处理：输入的自然对数
网络设计：单层，一个神经元
激活函数：指数
损失函数：MAE（平均绝对误差）
优化器： Adam
纪元：50
批量大小：32

输入和预期输出：

输入：([x1, x2])
正确权重：([2, 3])
正确的偏差：(\ln 3)

尽管有这些设置，我仍无法达到 100% 的准确度。我尝试过随机初始化权重和偏差以及使用特定值。
这是代码：
将 numpy 导入为 np
将张量流导入为 tf
从tensorflow.keras.models导入顺序
从tensorflow.keras.layers导入Dense
从tensorflow.keras.optimizers导入Adam

# 生成数据
x1 = np.random.randint(1, 21, 大小=(1000, 1))
x2 = np.random.randint(1, 21, 大小=(1000, 1))
y = 3 * (x1 ** 2) * (x2 ** 3)

# 预处理数据
log_x1 = np.log(x1)
log_x2 = np.log(x2)
log_inputs = np.hstack((log_x1, log_x2))

# 定义模型
模型=顺序（）
model.add（密集（1，input_dim = 2，激活=&#39;指数&#39;，kernel_initializer =&#39;ones&#39;，bias_initializer =&#39;zeros&#39;））

# 编译模型
model.compile(优化器=Adam(learning_rate=0.01), loss=&#39;mae&#39;)

# 训练模型
model.fit（log_inputs，np.log（y），epochs = 50，batch_size = 32）

# 评估模型
test_x1 = np.array([[2], [4], [5]])
test_x2 = np.array([[3], [7], [19]])
test_inputs = np.hstack((np.log(test_x1), np.log(test_x2)))
预测 = model.predict(test_inputs)
print(np.exp(预测))

有人对如何提高该模型的准确性有建议吗？]]></description>
      <guid>https://stackoverflow.com/questions/78514097/model-accuracy-for-non-linear-relationship-with-logarithm-kernel-and-exponential</guid>
      <pubDate>Tue, 21 May 2024 19:54:48 GMT</pubDate>
    </item>
    <item>
      <title>如何将极坐标数据框与 scikit-learn 一起使用？</title>
      <link>https://stackoverflow.com/questions/74398563/how-to-use-polars-dataframes-with-scikit-learn</link>
      <description><![CDATA[我无法将极坐标数据帧与 scikit-learn 一起使用进行机器学习训练。
目前，我正在预处理 Polars 中的所有数据帧，并将它们转换为 pandas 进行模型训练，以使其正常工作。
是否有任何方法可以通过 scikit-learn API 直接使用极坐标数据帧（无需先转换为 pandas）？]]></description>
      <guid>https://stackoverflow.com/questions/74398563/how-to-use-polars-dataframes-with-scikit-learn</guid>
      <pubDate>Fri, 11 Nov 2022 05:59:55 GMT</pubDate>
    </item>
    </channel>
</rss>