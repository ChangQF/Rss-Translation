<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 09 Jan 2025 06:23:44 GMT</lastBuildDate>
    <item>
      <title>使用 lgbm 回归器进行交叉验证</title>
      <link>https://stackoverflow.com/questions/79341444/cross-validation-with-lgbm-regressor</link>
      <description><![CDATA[当我使用 lgbm 回归器的交叉验证时，
它仅用于查找 best_num_round
我说得对吗？
与 lgbm 一起使用时，我找不到它的其他用处
我用它训练模型，除了 num boost round 之外找不到其他用处
但我们可以使用早期停止技术来代替它]]></description>
      <guid>https://stackoverflow.com/questions/79341444/cross-validation-with-lgbm-regressor</guid>
      <pubDate>Thu, 09 Jan 2025 05:23:42 GMT</pubDate>
    </item>
    <item>
      <title>Azure AutoML 图像分类作业</title>
      <link>https://stackoverflow.com/questions/79341200/azure-automl-image-classification-job</link>
      <description><![CDATA[我在尝试为 Azure ML 中的数据集创建 MLTable YAML 文件时遇到问题。
我的工作区中有一个默认数据存储，其中包含两个带有图像的文件夹（OK 和 NOK）。我的目标是读取所有图像并使用文件夹名称作为每幅图像的标签。
以下是我到目前为止尝试过的方法：
mltable_yaml = &quot;&quot;&quot;
type: mltable
paths:
- file: ./OK 
- file: ./NOK 
transformations:
- read_from_directory:
image_column: image_url 
folder_column: label 
recursive: true 
&quot;&quot;&quot;

# 创建目录并保存 MLTable
mltable_dir = &quot;image_data&quot;
os.makedirs(mltable_dir, exist_ok=True)
with open(os.path.join(mltable_dir, &quot;MLTable&quot;), &quot;w&quot;) as f:
f.write(mltable_yaml)

training_data = Input(
type=&quot;mltable&quot;,
path=mltable_dir
)

但是，当我运行实验时，我遇到了以下错误：
MLTable 输入无效。UserErrorException：
消息：从数据集获取数据时遇到用户错误。错误：UserErrorException：
消息：MLTable yaml 架构无效：
错误代码：ScriptExecution.Validation
验证错误代码：无效
验证目标：脚本
本机错误：数据流脚本错误：InvalidScriptElement(&quot;read_from_directory&quot;)
ScriptError(InvalidScriptElement(&quot;read_from_directory&quot;))
=&gt; 脚本元素&quot;read_from_directory&quot;无效
InvalidScriptElement(&quot;read_from_directory&quot;)
错误消息：Yaml 脚本无效：InvalidScriptElement(&quot;read_from_directory&quot;)。| session_id=1a30b15a-7e85-498b-b735-2348bfe0625b
InnerException None
ErrorResponse 
{
&quot;error&quot;: {
&quot;code&quot;: &quot;UserError&quot;,
&quot;message&quot;: &quot;MLTable yaml 模式无效:\n错误代码：ScriptExecution.Validation\n验证错误代码：无效\n验证目标：脚本\n本机错误：数据流脚本错误：InvalidScriptElement(\&quot;read_from_directory\&quot;)\n\tScriptError(InvalidScriptElement(\&quot;read_from_directory\&quot;))\n=&gt;无效的脚本元素 \&quot;read_from_directory\&quot;\n\tInvalidScriptElement(\&quot;read_from_directory\&quot;)\n错误消息：Yaml 脚本无效：InvalidScriptElement(\&quot;read_from_directory\&quot;)。| session_id=1a30b15a-7e85-498b-b735-2348bfe0625b&quot;
}
}
InnerException UserErrorException:
消息：MLTable yaml 架构无效：
错误代码：ScriptExecution.Validation
验证错误代码：无效
验证目标：脚本
本机错误：数据流脚本错误：InvalidScriptElement(&quot;read_from_directory&quot;)
ScriptError(InvalidScriptElement(&quot;read_from_directory&quot;))
=&gt; 无效的脚本元素 &quot;read_from_directory&quot;
InvalidScriptElement(&quot;read_from_directory&quot;)
错误消息：Yaml 脚本无效：InvalidScriptElement(&quot;read_from_directory&quot;)。| session_id=1a30b15a-7e85-498b-b735-2348bfe0625b
InnerException None
ErrorResponse 
{
&quot;error&quot;: {
&quot;code&quot;: &quot;UserError&quot;,
&quot;message&quot;: &quot;MLTable yaml 架构无效：\n错误代码：ScriptExecution.Validation\n验证错误代码：无效\n验证目标：脚本\n本机错误：数据流脚本错误：InvalidScriptElement(\&quot;read_from_directory\&quot;)\n\tScriptError(InvalidScriptElement(\&quot;read_from_directory\&quot;))\n=&gt;无效的脚本元素 \&quot;read_from_directory\&quot;\n\tInvalidScriptElement(\&quot;read_from_directory\&quot;)\n错误消息：Yaml 脚本无效：InvalidScriptElement(\&quot;read_from_directory\&quot;)。| session_id=1a30b15a-7e85-498b-b735-2348bfe0625b&quot;
}
}
ErrorResponse 
{
&quot;error&quot;: {
&quot;code&quot;: &quot;UserError&quot;,
&quot;message&quot;: &quot;从数据集获取数据时遇到用户错误。错误：UserErrorException:\n\t消息：MLTable yaml 架构无效:\n错误代码：ScriptExecution.Validation\n验证错误代码：无效\n验证目标：脚本\n本机错误：数据流脚本错误：InvalidScriptElement(\&quot;read_from_directory\&quot;)\n\tScriptError(InvalidScriptElement(\&quot;read_from_directory\&quot;))\n=&gt; 无效脚本元素 \&quot;read_from_directory\&quot;\n\tInvalidScriptElement(\&quot;read_from_directory\&quot;)\n错误 M

从错误详细信息来看，似乎无法识别 read_from_directory 元素，但我不确定如何构造 YAML 以正确将文件夹名称映射到标签。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/79341200/azure-automl-image-classification-job</guid>
      <pubDate>Thu, 09 Jan 2025 02:14:54 GMT</pubDate>
    </item>
    <item>
      <title>生物信息学中的分类方法[关闭]</title>
      <link>https://stackoverflow.com/questions/79340781/classification-methods-in-bioinformatic</link>
      <description><![CDATA[生物信息学中序列分类的最佳分类方法是什么：SVM、kNN 还是其他方法？
以下是有关我的任务的一些详细信息：

输入数据由以字母字符串表示的生物序列组成。
我有一个带标签的数据集，其中包含大量样本（约 1200 个序列），但数据集的类别之间可能存在一些不平衡。
最终目标是将序列分类为预定义的类别，例如进化枝。
我可以使用计算资源，但我想平衡性能和计算成本。

我的问题是：

在 SVM 和 kNN 之间，哪一个在生物信息学的序列分类任务中表现更好，在什么条件下？
还有其他方法吗（例如决策树、随机森林或深度学习模型）可能为此类数据提供更好的准确性？
特征表示对这些方法有多重要，我应该优先考虑哪些特定的特征工程或预处理步骤？
]]></description>
      <guid>https://stackoverflow.com/questions/79340781/classification-methods-in-bioinformatic</guid>
      <pubDate>Wed, 08 Jan 2025 21:22:37 GMT</pubDate>
    </item>
    <item>
      <title>如何优化神经网络进行二元分类？[关闭]</title>
      <link>https://stackoverflow.com/questions/79340583/how-do-i-optimize-neural-network-for-binary-classification</link>
      <description><![CDATA[我正在尝试解决二元分类问题。由于这是欺诈检测，原始数据集非常不平衡，比如 90% 10%，所以我使用 Smote 将其调整为 70% 30%，然后使用 class_weights。数据集大约为 50000x16。
我使用了如下所示的浅层神经网络
# 定义模型
model = Sequential([
tf.keras.layers.Input(shape=(16,)),
Dense(128,activation=&#39;relu&#39;),
Dense(64,activation=&#39;relu&#39;),
Dense(32,activation=&#39;relu&#39;),
Dense(1,activation=&#39;sigmoid&#39;)
])

model.compile(optimizer=Adam(learning_rate=0.001),
loss=&#39;binary_crossentropy&#39;,
metrics=[&#39;F1Score&#39;])

early_stop = EarlyStopping(monitor=&#39;val_loss&#39;,patient=20,restore_best_weights=True)

history = model.fit(
x_resampled,y_resampled,
validation_data=(x_valid_norm, y_valid),
epochs=200,
batch_size=256,
class_weight=class_weights,
callbacks=[early_stop]
)


这运行了大约 30-50 个 epoch，然后提前停止。我得到了糟糕的结果，比随机森林或 XGBoost 差得多
我尝试了不同的激活函数、不同的学习率，我添加了 1 个层，删除了 1 个层，添加/删除了神经元，似乎没有什么能改善结果。
训练集中的 F1 分数：0.9068713401925431
验证集中的 F1 分数：0.3754874651810585
]]></description>
      <guid>https://stackoverflow.com/questions/79340583/how-do-i-optimize-neural-network-for-binary-classification</guid>
      <pubDate>Wed, 08 Jan 2025 19:45:08 GMT</pubDate>
    </item>
    <item>
      <title>我在 Python 中加载 pickle 文件时遇到问题[关闭]</title>
      <link>https://stackoverflow.com/questions/79339020/im-having-a-problem-while-loading-a-pickle-file-in-python</link>
      <description><![CDATA[我试图在另一个 Python 文件中加载此 pkl 文件：
with open(&quot;preprocessor.pkl&quot;,&quot;rb&quot;) as file:
processor=pickle.load(file)

错误：AttributeError：无法在 &lt;module &#39;__main__&#39;&gt; 上获取属性 &#39;ModifiedLabelEncoder&#39;]]></description>
      <guid>https://stackoverflow.com/questions/79339020/im-having-a-problem-while-loading-a-pickle-file-in-python</guid>
      <pubDate>Wed, 08 Jan 2025 11:47:57 GMT</pubDate>
    </item>
    <item>
      <title>如何从 fit_resamples 和超参数调整中获取训练误差？</title>
      <link>https://stackoverflow.com/questions/79338394/how-to-get-the-training-error-from-fit-resamples-and-hyperparameter-tuning</link>
      <description><![CDATA[在交叉验证期间，fit_resamples 返回验证集中度量的平均值。
lr_model &lt;-
linear_reg() |&gt;
set_engine(&#39;lm&#39;)

lr_wf &lt;-
working() |&gt;
add_recipe(basic_recipe) |&gt;
add_model(lr_model)

lr_cv &lt;-
lr_wf |&gt;
fit_resamples(
folds,
metrics = metric_set(rmse),
control = control
)

# 让我们从 CV 中提取结果。这将有助于我们将其与其他模型进行比较
lr_cv |&gt;
collect_metrics()
# 这是 RMSE 验证错误
# .metric .estimator mean n std_err .config
# &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; 
# rmse standard 0.161 10 0.000370 Preprocessor1_Model1

我遇到的问题是如何获取训练误差。
在调整超参数后也会出现同样的问题。
例如，在调整 KNN 以找到最佳邻居数时，collect_metrics 和 show_best 会返回来自交叉验证的验​​证集指标的平均值，而我们都知道，最佳邻居数是当训练误差减少而验证误差开始增加时。
不幸的是，autoplot 函数不显示训练误差，只显示验证误差。
在这种情况下，例如
tree_grid &lt;-
grid_regular(
cost_complexity(),
tree_depth(),
min_n(),
levels = c(3, 5, 10)
)

tree_wf &lt;-
working() %&gt;%
add_model(tree_model) %&gt;%
add_recipe(basic_recipe)

tree_res &lt;- 
tree_wf %&gt;%
tune_grid(
resamples = folds,
grid = tree_grid,
metrics = metric_set(rmse),
control = control
)

如何提取每对超参数/折叠的训练误差？]]></description>
      <guid>https://stackoverflow.com/questions/79338394/how-to-get-the-training-error-from-fit-resamples-and-hyperparameter-tuning</guid>
      <pubDate>Wed, 08 Jan 2025 08:23:55 GMT</pubDate>
    </item>
    <item>
      <title>在网格搜索中使用 sklearn 特征选择器来评估所有特征的有用性的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/79337434/whats-the-best-way-to-use-a-sklearn-feature-selector-in-a-grid-search-to-evalu</link>
      <description><![CDATA[我正在训练一个 sklearn 分类器，并在管道中插入一个特征选择步骤。通过网格搜索，我想确定允许我最大化性能的特征数量。不过，我想在网格搜索中探索没有特征选择，只有&quot;passthrough&quot; 的可能性步骤，是最大化性能的最佳选择。
这是一个可重现的示例：
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

# 加载泰坦尼克号数据集
titanic = sns.load_dataset(&#39;titanic&#39;)

# 选择特征和目标
features = [&#39;age&#39;, &#39;fare&#39;, &#39;sex&#39;]
X = titanic[features]
y = titanic[&#39;survived&#39;]

# 预处理管道数字和分类特征
numeric_features = [&#39;age&#39;, &#39;fare&#39;]
numeric_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;)),
(&#39;scaler&#39;, StandardScaler())
])

categorical_features = [&#39;sex&#39;]
categorical_transformer = Pipeline(steps=[
(&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;)),
(&#39;onehot&#39;, OneHotEncoder(drop=&#39;first&#39;))
])

# 合并预处理步骤
preprocessor = ColumnTransformer(transformers=[
(&#39;num&#39;, numeric_transformer, numeric_features),
(&#39;cat&#39;, categorical_transformer, categorical_features)
])

# 初始化分类器和特征选择器
clf = LogisticRegression(max_iter=1000,solver=&#39;liblinear&#39;)
sfs = SequentialFeatureSelector(clf, direction=&#39;forward&#39;)

# 创建一个包含预处理、特征选择和分类的管道
pipeline = Pipeline(steps=[
(&#39;preprocessor&#39;, preprocessor),
(&#39;feature_selection&#39;, sfs),
(&#39;classifier&#39;, clf)
])

# 定义要搜索的参数网格
param_grid = {
&#39;feature_selection__n_features_to_select&#39;: [2],
&#39;classifier__C&#39;: [0.1, 1.0, 10.0], # 正则化强度
}

# 创建并运行网格搜索
grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X, y)

# 输出最佳参数和分数
print(&quot;找到最佳参数：&quot;, grid_search.best_params_)
print(&quot;最佳交叉验证分数：&quot;, grid_search.best_score_)

X 此处有三个特征（即使在 预处理器 步骤之后），但上面的网格搜索代码不允许探索使用所有 3 个特征的模型，因为设置
feature_selection__n_features_to_select: [2,3]

将给出 ValueError: n_features_to_select 必须是 &lt; n_features。
这里的障碍是 SequentialFeatureSelector 不将所有特征的选择（又名直通选择器）视为有效的特征选择。
换句话说，我想运行网格搜索，同时考虑
(&#39;feature_selection&#39;, &#39;passthrough&#39;)

在可能的管道配置空间中的设置。有没有一种惯用的/好的方法可以做到这一点？]]></description>
      <guid>https://stackoverflow.com/questions/79337434/whats-the-best-way-to-use-a-sklearn-feature-selector-in-a-grid-search-to-evalu</guid>
      <pubDate>Tue, 07 Jan 2025 21:39:17 GMT</pubDate>
    </item>
    <item>
      <title>AdaBoostClassifier：test_size=0.25 的指标完美，但其他值的样本误差不一致</title>
      <link>https://stackoverflow.com/questions/79337075/adaboostclassifier-perfect-metrics-with-test-size-0-25-but-inconsistent-sample</link>
      <description><![CDATA[我正在使用 AdaBoostClassifier 和弱学习器 (DecisionTreeClassifier) 对数据集进行分类。数据集有 7857 个样本：
X.shape
# 输出：(7857, 5)

y.shape
# 输出：(7857,)

以下是拆分数据集和训练模型的代码：
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.25, random_state=28
)

weak_learner = DecisionTreeClassifier(max_depth=1)

adb = AdaBoostClassifier(estimator=weak_learner, n_estimators=50, random_state=42)
adb_model = adb.fit(X_train, y_train)

y_pred = adb_model.predict(X_test)
print(classification_report(y_test, y_pred))

当我使用 test_size=0.25 运行此代码时，所有类别的分类指标输出均为 100%：
 precision recall f1-score support

Cheap 1.00 1.00 1.00 496
Expensive 1.00 1.00 1.00 506
Reasonable 1.00 1.00 1.00 963

accuracy 1.00 1965
macro avg 1.00 1.00 1.00 1965
weighted avg 1.00 1.00 1.00 1965

这不可能是真的，因为我的数据点并不是完全可分离的。 （我用图表检查过）
但是，当我将 test_size 更改为任何其他值（例如 0.3、0.2）时，我收到以下错误：
ValueError：发现输入变量的样本数不一致


我已检查的内容：

确保 X 和 y 具有相同数量的样本。
确认 X 或 y 中没有缺失值。


问题：

为什么 test_size=0.25产生完美的指标，但其他 test_size 值会导致错误？
如何解决此问题以使用不同的 test_size 值？
]]></description>
      <guid>https://stackoverflow.com/questions/79337075/adaboostclassifier-perfect-metrics-with-test-size-0-25-but-inconsistent-sample</guid>
      <pubDate>Tue, 07 Jan 2025 19:00:22 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Turing.jl 中使用通过核密度估计 (KDE) 估计的分布作为先验？</title>
      <link>https://stackoverflow.com/questions/79336637/how-to-use-a-distribution-estimated-via-kernel-density-estimation-kde-as-a-pri</link>
      <description><![CDATA[背景和目标
我有一组离散样本（例如，来自一些现有数据集或后验）。我使用核密度估计 (KDE) 方法（例如通过 KernelDensity.jl）从这些样本中估计出连续分布。现在，我想将基于 KDE 的分布用作 Turing.jl 中实现的贝叶斯模型中某些参数的先验。
我已查看了 Turing 的高级用法官方文档 (https://turing.ml/dev/docs/using-turing/advanced)，但我并不完全清楚如何将自定义的基于 KDE 的分布作为先验，尤其是关于 Bijectors.jl 可能需要某些转换才能与 Turing.jl 的采样算法（如 NUTS）集成。
问题

如何获取经验或离散数据集，通过 KDE 估计分布，然后直接在 Turing.jl 中使用该基于 KDE 的分布作为先验？
具体实施步骤是什么（即 pdf、logpdf、rand 以及可能的自定义双射器的定义）是否需要确保 Turing.jl 的 MCMC（特别是 NUTS）能够无错误地处理这个基于 KDE 的自定义先验？
]]></description>
      <guid>https://stackoverflow.com/questions/79336637/how-to-use-a-distribution-estimated-via-kernel-density-estimation-kde-as-a-pri</guid>
      <pubDate>Tue, 07 Jan 2025 16:14:31 GMT</pubDate>
    </item>
    <item>
      <title>如何从物理信息神经网络 (PINN) 获取具有初始和边界条件的 PDE 的单一解？</title>
      <link>https://stackoverflow.com/questions/79329941/how-to-get-a-single-solution-from-a-physics-informed-neural-network-pinn-for-a</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79329941/how-to-get-a-single-solution-from-a-physics-informed-neural-network-pinn-for-a</guid>
      <pubDate>Sun, 05 Jan 2025 01:01:10 GMT</pubDate>
    </item>
    <item>
      <title>关于论文“RL CQL”和“Cal-QL：经过校准的离线 RL 预训练以实现高效的在线微调”的困惑 [关闭]</title>
      <link>https://stackoverflow.com/questions/79310839/confusion-about-papers-rl-cql-and-cal-ql-calibrated-offline-rl-pre-training</link>
      <description><![CDATA[最近看了两篇论文，包括《Conservative Q-Learning for Offline Reinforcement Learning》和《Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning》。
根据CQL的论文，我认为如果一个状态-动作对存在于行为策略收集到的数据集中，那么在训练时就不会受到惩罚（正则项为零）。
因为惩罚项的两个子项抵消了。或者如果它不在数据集中，就会受到第一个子项的惩罚，如下图1所示
（正则项以蓝色突出显示）。因为如果状态-动作对不在数据集中，惩罚项的第二个子项就会为零。所以对于行为策略收集到的状态-动作对，可能由于最大化和引导训练而被高估。因为即使使用目标网络，也只能缓解高估问题。
因此，CQL的估计值与真实值之间的关系可能如图2所示。我上面说的对吗？
图1：
在此处输入图像描述
图2：
在此处输入图像描述
但在Cql-QL中，我们可以看到CQL的估计值明显低于真实值，如图3所示。
图3：
在此处输入图像描述
在论文CQL和Cql-QL中，对CQL估计值与真值关系的描述存在矛盾。
如何理解？我上面的描述对吗？
非常感谢您的回答。
如何理解？我上面的描述对吗？RL专家能给我一个答案吗？]]></description>
      <guid>https://stackoverflow.com/questions/79310839/confusion-about-papers-rl-cql-and-cal-ql-calibrated-offline-rl-pre-training</guid>
      <pubDate>Fri, 27 Dec 2024 03:20:54 GMT</pubDate>
    </item>
    <item>
      <title>如何在调用 MessageGraph 期间传递多个输入？</title>
      <link>https://stackoverflow.com/questions/78907608/how-to-pass-multiple-inputs-during-invoke-to-a-messagegraph</link>
      <description><![CDATA[我们有一个用于 LLMCompiler 实现的 MessageGraph，并且正如预期的那样，我们在运行调用时将用户的问题作为 HumanMessage 对象列表传递（这些对象映射到默认的&quot;messages&quot; 键并传递给提示模板），这对于简单的用例来说工作得很好，但现在我们需要在调用时（而不是在构建图形时）传递额外的信息/上下文，我们对 React 代理做了类似的事情，并且传递类似于 invoke({&quot;messages&quot;:input, &quot;context&quot;:context}) 的字典非常容易。但对于 MessageGraph，这不起作用，看起来运行 invoke(messages) 时传递的消息列表会在提示中自动映射到&quot;messages&quot;键，无法添加其他输入，我尝试传递一个字典 invoke({&quot;messages&quot;:messages, &quot;context&quot;:context})，但没有成功，错误失败：

消息字典必须包含“role”和“content”键，得到
{&quot;messages&quot;:messages,&quot;context&quot;:context
]]></description>
      <guid>https://stackoverflow.com/questions/78907608/how-to-pass-multiple-inputs-during-invoke-to-a-messagegraph</guid>
      <pubDate>Fri, 23 Aug 2024 21:30:27 GMT</pubDate>
    </item>
    <item>
      <title>对包含文本和图像的 PDF 进行大型语言模型微调</title>
      <link>https://stackoverflow.com/questions/78251401/fine-tuning-large-language-model-on-pdfs-containing-text-and-images</link>
      <description><![CDATA[我需要对自定义数据集上的 LLM 进行微调，该数据集包含从 PDF 中提取的文本和图像。
对于文本部分，我已成功提取整个文本数据并使用 OpenAI API 生成 JSON/CSV 格式的问题和答案。这种方法对于基于文本的微调非常有效。
但是，我不确定如何处理图像。有人可以建议一种方法或库来帮助我处理图像并将其纳入微调过程吗？
然后稍后，使用微调后的模型进行问答。此外，我不知道该使用哪种模型来完成这项任务。
任何指导、资源或见解都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78251401/fine-tuning-large-language-model-on-pdfs-containing-text-and-images</guid>
      <pubDate>Sun, 31 Mar 2024 13:04:38 GMT</pubDate>
    </item>
    <item>
      <title>如何计算神经网络预测的置信度分数</title>
      <link>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</link>
      <description><![CDATA[我正在使用深度神经网络模型（在 keras 中实现）进行预测。类似这样的内容：
def make_model():
model = Sequential() 
model.add(Conv2D(20,(5,5),activation = &quot;relu&quot;))
model.add(MaxPooling2D(pool_size=(2,2))) 
model.add(Flatten())
model.add(Dense(20,activation = &quot;relu&quot;))
model.add(Lambda(lambda x: tf.expand_dims(x, axis=1)))
model.add(SimpleRNN(50,activation=&quot;relu&quot;))
model.add(Dense(1,activation=&quot;sigmoid&quot;)) 
model.compile(loss = &quot;binary_crossentropy&quot;,optimizer = adagrad,metrics = [&quot;accuracy&quot;])

返回模型

model = make_model()
model.fit(x_train,y_train,validation_data = (x_validation,y_validation), epochs = 25, batch_size = 25, verbose = 1)

##预测：
prediction = model.predict_classes(x)
probabilities = model.predict_proba(x) #我假设这些是被预测的类的概率

我的问题是分类（二元）问题。我希望计算每个预测的置信度分数，即我想知道 - 我的模型是否 99% 确定它是“0”，或者 58% 确定它是“0”。
我找到了一些关于如何做到这一点的观点，但无法实现它们。我希望遵循的方法是：“使用分类器，当你输出时，你可以将值解释为属于每个特定类别的概率。你可以使用它们的分布作为你对观察结果属于该类别的信心的粗略衡量标准。”
我应该如何使用类似上述模型的东西进行预测，以便获得对每个预测的信心？我希望有一些实际的例子（最好是在 Keras 中）。]]></description>
      <guid>https://stackoverflow.com/questions/59851961/how-to-calculate-confidence-score-of-a-neural-network-prediction</guid>
      <pubDate>Wed, 22 Jan 2020 02:52:32 GMT</pubDate>
    </item>
    <item>
      <title>keras：model.predict 和 model.predict_proba 之间有什么区别</title>
      <link>https://stackoverflow.com/questions/40747679/keras-what-is-the-difference-between-model-predict-and-model-predict-proba</link>
      <description><![CDATA[我发现 model.predict 和 model.predict_proba 都给出了相同的二维矩阵，表示每行每个类别的概率。
这两个函数有什么区别？]]></description>
      <guid>https://stackoverflow.com/questions/40747679/keras-what-is-the-difference-between-model-predict-and-model-predict-proba</guid>
      <pubDate>Tue, 22 Nov 2016 17:06:16 GMT</pubDate>
    </item>
    </channel>
</rss>