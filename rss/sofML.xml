<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 26 Aug 2024 21:14:49 GMT</lastBuildDate>
    <item>
      <title>层“ functional_5”需要 2 个输入，但实际收到 1 个输入张量</title>
      <link>https://stackoverflow.com/questions/78916012/layer-functional-5-expects-2-inputs-but-it-received-1-input-tensors</link>
      <description><![CDATA[我正在尝试构建模型来预测 Facebook 帖子的点赞数，该模型采用文本和内容类型（img、vid、other），这是一个热编码列。
我已经创建了一个 TensorFlow 数据集，但在尝试拟合模型时出现此错误：
层“ functional_13”需要 2 个输入，但它收到了 1 个输入张量。
收到的输入：[&lt;tf.Tensor &#39;data:0&#39; shape=(None, 1000) dtype=int64&gt;]

以下是我的一些代码片段：
dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,
content,
df[&#39;likes_rate&#39;]))

dataset= dataset.cache()
dataset= dataset.shuffle(160000)
dataset= dataset.batch(16)
dataset= dataset.prefetch(8)

其中内容是 3 列的 OneHotEncoded 列。
这是我的模型
from tensorflow.keras.layers import Input, Embedding, Concatenate,LSTM,Bidirectional
text_input=输入（形状=（1000，））
content_input=输入（形状=（3，））

text_embeddings = tf.keras.layers.Embedding（Max_Features+1，32）（text_input）# 调整嵌入 dim
lstm= Bidirectional（LSTM（32，activation=&#39;tanh&#39;））（text_embeddings）
# 连接文本嵌入和内容特征
combined_features = tf.keras.layers.Concatenate()（[lstm，content_input]）

# 隐藏层（调整数量/激活函数）
x = tf.keras.layers.Dense（256，activation=&#39;relu&#39;）（combined_features）
x = tf.keras.layers.Dropout（0.2）（x）
x = tf.keras.layers.Dense(128,activation=&#39;relu&#39;)(x)
x = tf.keras.layers.Dropout(0.1)(x)
x = tf.keras.layers.Dense(64,activation=&#39;relu&#39;)(x)
# 用于点赞预测的输出层
output = tf.keras.layers.Dense(1,activation=&#39;linear&#39;)(x)

我想为文本创建一个嵌入层，然后将其传递给 LSTM，然后将 LSTM 的输出和内容合并到 Dense 层。
一切都很好，但是当尝试拟合模型时，我遇到了上述问题。
model = tf.keras.models.Model(inputs=[text_input, content_input],输出=输出)
model.compile(loss=&#39;mse&#39;,optimizer=&#39;Adam&#39;)
model.fit(dataset,epochs=10)

但如果这样做，代码可以正常工作。但 .fit 每次都会产生随机权重，因此模型不会取得任何进展。
for text_batch, content_batch, y_batch in dataset:
# 在当前批次上训练模型
model.fit(x=[text_batch, content_batch], y=y_batch)
]]></description>
      <guid>https://stackoverflow.com/questions/78916012/layer-functional-5-expects-2-inputs-but-it-received-1-input-tensors</guid>
      <pubDate>Mon, 26 Aug 2024 19:24:22 GMT</pubDate>
    </item>
    <item>
      <title>我们应该在视觉编码器解码器中增加多少训练数据？[关闭]</title>
      <link>https://stackoverflow.com/questions/78915357/how-much-should-we-augment-our-training-data-in-a-vision-encoder-decoder</link>
      <description><![CDATA[我正在训练一个从图像生成文本的模型，并且正在将文本增强应用到我的数据集以尝试提高模型的性能。我想知道原始文本和增强文本之间的最佳平衡应该是什么。具体来说，我应该使用原始文本与增强文本的百分比是多少（例如，40％原始/60％增强，50％原始/50％增强，60％原始/40％增强，70％原始/30％增强等）？
我很感激您对此的意见，如果您知道任何讨论此主题的论文，请告诉我！
作为上下文，我的原始数据集中有大约150k个文本。
所以我的模型有两个阶段的训练，我得到了这个结果：
实验****************阶段1/阶段2
exp5：60/40****************0.593 /-----
exp6：70/30****************0.591 /-----
exp7： 80/20****************0.592/-----
exp8：90/10****************0.584/-----
到目前为止，似乎在阶段 1 中 60original/40augmented 是最好的，但阶段 2 需要一周以上的时间来训练，这就是为什么我想知道是否存在最佳平衡以节省 GPU 时间。]]></description>
      <guid>https://stackoverflow.com/questions/78915357/how-much-should-we-augment-our-training-data-in-a-vision-encoder-decoder</guid>
      <pubDate>Mon, 26 Aug 2024 16:00:32 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：使用判别损失函数训练 ENet 模型时无法重塑张量</title>
      <link>https://stackoverflow.com/questions/78915099/valueerror-cannot-reshape-tensor-when-training-enet-model-with-discriminative-l</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78915099/valueerror-cannot-reshape-tensor-when-training-enet-model-with-discriminative-l</guid>
      <pubDate>Mon, 26 Aug 2024 14:59:15 GMT</pubDate>
    </item>
    <item>
      <title>我想要一个包含文本和盲文的数据集[关闭]</title>
      <link>https://stackoverflow.com/questions/78915000/i-want-a-dataset-contains-text-and-braille-language</link>
      <description><![CDATA[我正在做一个将语音转换为文本的机器学习模型，然后我正在制作另一个将文本转换为盲文的模型，但我找不到同时包含文本和盲文的数据集。
我在 Kaggle 中查找过，但找不到任何数据集，有人可以看看链接或帮我找一个开源数据集吗？]]></description>
      <guid>https://stackoverflow.com/questions/78915000/i-want-a-dataset-contains-text-and-braille-language</guid>
      <pubDate>Mon, 26 Aug 2024 14:38:17 GMT</pubDate>
    </item>
    <item>
      <title>模型训练时的输入形状问题</title>
      <link>https://stackoverflow.com/questions/78914400/input-shape-problem-during-model-training</link>
      <description><![CDATA[我的代码中存在这个问题，您能帮助我吗？
ValueError: 输入层“ functional”的 0与层不兼容：
预期形状=（None，4096），发现形状=（None，64，4096）

代码：
def generator_output_signature()：
return (
(
tf.TensorSpec(shape=[batch_size, 4096], dtype=tf.float32), # X1 具有很多维度
tf.TensorSpec(shape=[batch_size, max_length], dtype=tf.int32) # X2 具有很多维度
),
tf.TensorSpec(shape=[batch_size, vocab_size], dtype=tf.float32) # y 具有很多维度
)
train_dataset = tf.data.Dataset.from_generator(
lambda: data_generator(train_ids, mining, features、tokenizer、max_length、vocab_size、batch_size)，
output_signature=generator_output_signature()
)

train_dataset = train_dataset.batch(batch_size).p​​refetch(tf.data.AUTOTUNE)

image_input = Input(shape=(4096,))
caption_input = Input(shape=(max_length,))

x = Embedding(vocab_size, 256)(caption_input) # 输出形状：(None, max_length, 256)
x = LSTM(256)(x) # 输出形状：(None, 256)

image_features = Dense(256,activation=&#39;relu&#39;)(image_input) # 输出形状：(None, 256)

x = Add()([x, image_features])

x = Dense(vocab_size,激活=&#39;softmax&#39;)(x) # 输出形状：(无，vocab_size)

model = Model(inputs=[image_input, caption_input], output=x)
model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;)

print(model.summary())

我试图删除额外的维度，但这是不可能的，所以我无法训练我的模型。]]></description>
      <guid>https://stackoverflow.com/questions/78914400/input-shape-problem-during-model-training</guid>
      <pubDate>Mon, 26 Aug 2024 12:28:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Stable Baselines3 中改变 TD3 模型的输出激活函数？</title>
      <link>https://stackoverflow.com/questions/78914358/how-to-change-the-output-activation-function-of-the-td3-model-in-stable-baseline</link>
      <description><![CDATA[我正在使用 Stable Baselines3 库进行强化学习，并想修改 TD3 模型。具体来说，我想更改 TD3 模型输出层中使用的激活函数。
我该如何实现？
我尝试修改 Stable Baselines3 库中 TD3 模型的输出激活函数。具体来说，我想用不同的激活函数替换默认的 tanh 激活函数。]]></description>
      <guid>https://stackoverflow.com/questions/78914358/how-to-change-the-output-activation-function-of-the-td3-model-in-stable-baseline</guid>
      <pubDate>Mon, 26 Aug 2024 12:16:40 GMT</pubDate>
    </item>
    <item>
      <title>由于尺寸不相等，处理化学分子原子时出现批处理错误</title>
      <link>https://stackoverflow.com/questions/78913966/batching-error-when-processing-chemical-molecule-atoms-because-sizes-not-equal</link>
      <description><![CDATA[我正在设计一个可以根据节点特征、边缘特征预测属性的神经网络。但是，当我尝试将批处理大小设置为大于 1 时，事情就出错了。分子中的原子数不同，因此在将多个分子批处理在一起时会导致错误。更具体地说，将出现以下错误：
RuntimeError：堆栈期望每个张量大小相等，但在条目 0 处得到 [6, 25] 
而在条目 1 处得到 [8, 25]

。6 和 8 表示该分子中有 6/8 个原子，25 表示每个原子有 25 个特征。有没有比根据另一个类似问题添加零来调整大小更好的解决方案，因为分子的大小可能高达 30 或更大。]]></description>
      <guid>https://stackoverflow.com/questions/78913966/batching-error-when-processing-chemical-molecule-atoms-because-sizes-not-equal</guid>
      <pubDate>Mon, 26 Aug 2024 10:39:01 GMT</pubDate>
    </item>
    <item>
      <title>使用 CLIP Vision Encoder 创建自定义对象检测模型</title>
      <link>https://stackoverflow.com/questions/78913273/create-a-custom-object-detection-model-with-clip-vision-encoder</link>
      <description><![CDATA[是否可以仅使用剪辑图像编码器创建我自己的自定义对象检测模型？
我曾尝试从 CLIP 图像编码器中提取图像的嵌入。我想尝试将其提供给一些现有的对象检测模型，例如 YOLOv5 等，其中 CLIP 图像编码器是特征提取器。然而，我无法理解，我该如何继续这个想法。这可能吗？如果是，建议如何继续？]]></description>
      <guid>https://stackoverflow.com/questions/78913273/create-a-custom-object-detection-model-with-clip-vision-encoder</guid>
      <pubDate>Mon, 26 Aug 2024 07:30:04 GMT</pubDate>
    </item>
    <item>
      <title>Dask 在 GridSearchCV 和 RandomizedSearchCV 上犯了错误</title>
      <link>https://stackoverflow.com/questions/78912576/dask-erring-on-gridsearchcv-and-randomizedsearchcv</link>
      <description><![CDATA[我正在尝试使用 dask 训练 xgboost 模型。我已经转换了数据并准备了如下数据：
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)

我可以像这样对数据进行简单的模型拟合：
model = dxgb.DaskXGBRegressor()
model.fit(X_train, y_train)
model.score(X_test, y_test)

但是当我尝试任何更复杂的事情时。就像这样使用 dask 的 gridsearchcv：
param_grid = {
&#39;max_depth&#39;: [3, 8],
&#39;learning_rate&#39;: [0.01, 0.1]}

grid_search = GridSearchCV(
model, param_grid)

grid_search.fit(X_train, y_train)

我收到以下警告：
警告：dask_ml.model_selection._search:(&#39;daskxgbregressor-fit-score-f6830e79aeb606b3eed291ac24184a8c&#39;, 1, 1) 失败...正在重试

任务图中的所有内容都因错误而变黑。
有人知道如何解决吗这个？]]></description>
      <guid>https://stackoverflow.com/questions/78912576/dask-erring-on-gridsearchcv-and-randomizedsearchcv</guid>
      <pubDate>Mon, 26 Aug 2024 01:04:55 GMT</pubDate>
    </item>
    <item>
      <title>AWS SageMaker 预测和测试数据</title>
      <link>https://stackoverflow.com/questions/78907538/aws-sagemaker-predictions-and-test-data</link>
      <description><![CDATA[import pandas as pd
import itertools
import numpy as np
import s3fs
from sagemaker.predictor import Predictor
from sagemaker.serializers import CSVSerializer

# 为您的 CSV 文件定义 S3 路径
import pandas as pd
import s3fs

# 为您的 CSV 文件定义 S3 路径
s3_path = &quot;s3://{}/{}/{}.csv&quot;

# 带有附加检查的读取文件函数
def read_and_check_csv(s3_path):
fs = s3fs.S3FileSystem()
with fs.open(s3_path) as f:
try:
# 尝试读取 CSV 文件
df = pd.read_csv(f, header=None, low_memory=False)
# 检查行长度是否一致
if not df.apply(lambda x: len(x.dropna()), axis=1).nunique() == 1:
raise ValueError(&quot;检测到不一致的行长度&quot;)
print(&quot;文件读取成功，似乎为 CSV 格式。&quot;)
return df
except Exception as e:
print(f&quot;无法读取 CSV 文件：{e}&quot;)
return None

# 读取并检查 CSV 文件
df = read_and_check_csv(s3_path)

如果 df 不为 None:
print(df.head())
否则:
print(&quot;文件无法读取或不是有效的 CSV 格式。&quot;)

# 定义用于切片数据的索引
a = [50 * i for i in range(3)]
b = [40 + i for i in range(10)]
indices = [i + j for i, j in itertools.product(a, b)]

# 准备测试数据
test_data = shape.iloc[indices[:-1]]
test_X = test_data.iloc[:, 1:]

# 确保所有行的列数相同
min_cols = test_X.shape[1]
test_X = test_X.dropna(axis=1, how=&#39;all&#39;) # 删除所有 NaN 值的列

# 验证没有具有不同值的行长度
test_X = test_X.apply(lambda x: x.dropna().reset_index(drop=True), axis=1)

# 使用 SageMaker 端点名称初始化预测器
predictor = Predictor(endpoint_name=&#39;endpoint&#39;)

# 确保预测器使用 CSV 序列化器
predictor.serializer = CSVSerializer()

# 将 DataFrame 转换为端点所需的格式
test_X_csv = test_X.to_csv(index=False, header=False, sep=&#39;,&#39;)

# 进行预测
try:
predictions = predictor.predict(test_X_csv)
# 打印预测
print(predictions.decode(&#39;utf-8&#39;))
except Exception as e:
print(f&quot;Error making predictions: {e}&quot;)


我正在使用上面的这个脚本aws sagemaker Jupyter 实验室中的 xgboost 框架。在此脚本之前，我正在运行以下代码来设置端点。
predictor = estimator.deploy(
initial_instance_count=1, instance_type=&quot;ml.m5.2xlarge&quot;
)

我添加了一些错误处理，这就是我了解到我的测试文件似乎无法正确读取的地方。我得到的实际错误是：
无法读取 CSV 文件：检测到不一致的行长度
无法读取文件或文件不是有效的 CSV 格式。
进行预测时出错：调用 InvokeEndpoint 操作时发生错误（ModelError）：从主服务器收到客户端错误（415），消息为“加载 csv 数据失败，出现异常，请确保数据为 csv 格式：
&lt;class &#39;ValueError&#39;&gt;
设置带有序列的数组元素。请求的数组在 1 维之后具有非均匀形状。检测到的形状为 (29,) + 非均匀部分。&quot;

关于如何修复此问题有任何见解吗？]]></description>
      <guid>https://stackoverflow.com/questions/78907538/aws-sagemaker-predictions-and-test-data</guid>
      <pubDate>Fri, 23 Aug 2024 20:58:39 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv8 进行大量错误检测</title>
      <link>https://stackoverflow.com/questions/78820748/a-lot-of-incorrect-detection-using-yolov8</link>
      <description><![CDATA[我尝试使用 Visual Code Studio 运行 YOLOv8。安装了 ultralytics 并在 vs code 终端上运行了 yolo predict model=yolov8n.pt source=&#39;https://ultralytics.com/images/bus.jpg&#39;。
但是我收到的输出是
2 个人、1 辆自行车、5 辆汽车、10 辆摩托车、73 艘船、3 个停车标志、1 只狗、10 匹马、10 头牛、32 只熊、1 只长颈鹿、63 把雨伞、6 个手提包、9 个飞盘、15 块滑雪板、5 块冲浪板、12 把刀、5 张床、37 张餐桌

这些显然不是这张图片的一部分。

当我第一次安装 ultralytics 并尝试运行 torch 时，出现了缺少依赖项的错误。fbgemm.ddl 丢失。后来，当我安装 vs_BuildTools 时，这个问题得到了解决。然后我继续在虚拟环境中运行代码，其中使用 torch 的程序运行没有任何错误。然后我继续输入此代码片段并遇到此问题。我也尝试使用命令提示符和 jupyter 笔记本运行，但同样的问题仍然存在。
我也检查了版本是否兼容，结果是兼容的。我还没有安装 cuda，是因为这个原因还是还有其他我不知道的问题？]]></description>
      <guid>https://stackoverflow.com/questions/78820748/a-lot-of-incorrect-detection-using-yolov8</guid>
      <pubDate>Thu, 01 Aug 2024 11:33:58 GMT</pubDate>
    </item>
    <item>
      <title>ValidationError：LLMChain 的 2 个验证错误</title>
      <link>https://stackoverflow.com/questions/77842203/validationerror-2-validation-errors-for-llmchain</link>
      <description><![CDATA[这是我的完整代码：
!pip install -q transformers einops accelerate langchain bitsandbytes sentence_transformers faiss-cpu pypdf sentencepiece 
from langchain import HuggingFacePipeline 
from transformers import AutoTokenizer 
from langchain.embeddings import HuggingFaceEmbeddings 
from langchain.document_loaders.csv_loader import CSVLoader 
from langchain.vectorstores import FAISS, Chroma
from langchain.chains import RetrievalQA 
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain.chains.question_answering import load_qa_chain
from langchain.memory import ConversationBufferMemory
import accelerate
import transformers 
import torch 
import textwrap 
loader = CSVLoader(&#39;/kaggle/input/csvdata/chatdata.csv&#39;, encoding=&quot;utf-8&quot;, csv_args={&#39;delimiter&#39;: &#39;,&#39;}) 
data = loader.load() 

embeddings = HuggingFaceEmbeddings(model_name=&#39;sentence-transformers/all-MiniLM-L6-v2&#39;,model_kwargs={&#39;device&#39;: &#39;cpu&#39;}) 

db = FAISS.from_documents(data, embeddings)

#Mistral 7B 模型 llm

import torch
from transformers import (
AutoModelForCausalLM,
AutoTokenizer,
GenerationConfig,
TextStreamer,
pipeline,
)

MODEL_NAME = &quot;mistralai/Mistral-7B-Instruct-v0.1&quot;

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
MODEL_NAME, device_map=&quot;auto&quot;, torch_dtype=torch.float16, load_in_8bit=True
)

generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
generation_config.max_new_tokens = 1024
generation_config.temperature = 0.0001
generation_config.do_sample = True
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

llm = pipeline(
&quot;text-generation&quot;,
model=model,
tokenizer=tokenizer,
return_full_text=True,
generation_config=generation_config,
num_return_sequences=1,
eos_token_id=tokenizer.eos_token_id,
pad_token_id=tokenizer.eos_token_id,
streamer=streamer,
)

def format_prompt(prompt, system_prompt=&quot;&quot;):
if system_prompt.strip():
return f&quot;[INST] {system_prompt} {prompt} [/INST]&quot;
return f&quot;[INST] {prompt} [/INST]&quot;

SYSTEM_PROMPT = &quot;&quot;&quot;

您是临床数据科学家和数据分析师，专门从事统计数据分析和报告生成。您的使命是为医疗保健和临床研究提供准确且富有洞察力的数据驱动解决方案。在回答问题时，请发挥临床数据科学领域经验丰富的数据专业人士的专业知识和精准度。
如果您遇到没有必要信息的问题，请务必避免提供推测性或不准确的答案。
&quot;&quot;&quot;.strip()

chain = ConversationalRetrievalChain.from_llm(
llm,
chain_type=&quot;stuff&quot;,
trieser=db.as_retriever(),
return_source_documents=True,
verbose=True,
)

这里我遇到了错误：
ValidationError: LLMChain 的 2 个验证错误
llm
预期 Runnable 实例 (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)
llm
预期 Runnable 实例 (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)

from textwrap import fill

result = chain(input(&quot;ClinicalTrial Planimeter ChatBot ---&quot;)
)
print(fill(result[&quot;result&quot;].strip(), width=80))

此 llm 链使用 llm、矢量数据库和提示符进行编程以与 csv 聊天，我在运行 ConversationalRetrievalChain 时遇到上述错误]]></description>
      <guid>https://stackoverflow.com/questions/77842203/validationerror-2-validation-errors-for-llmchain</guid>
      <pubDate>Thu, 18 Jan 2024 20:20:48 GMT</pubDate>
    </item>
    <item>
      <title>建议对 LLM 的个人提示/完成响应进行计时的方法？除了“时间”之外还有其他选项吗？</title>
      <link>https://stackoverflow.com/questions/77332293/suggested-methods-of-timing-individual-prompt-completion-response-from-llm-an</link>
      <description><![CDATA[我通过 jupyter notebook 在我的 ubuntu 机器上提示我的本地 LLM，得到响应，一切正常。但现在我想计算从提交提示到得到最终响应需要多长时间。不确定这是如何测量的，每秒推理次数？
我使用过 Linux 的“time”命令，它的输出很酷，但还有其他方法吗？
这是我目前使用的示例片段：
%%time
result = qa_chain(&quot;my question to the LLM?&quot;)

输出：
LLM 响应...

CPU 时间：用户 1.85 秒，系统：84 毫秒，总计：1.93 秒
挂钟时间：1.93 秒

我想要一些简单的、特定于 LLM 的输出，例如：

推理时间
GPU 时间

有人有或见过类似这样的有用的东西吗？]]></description>
      <guid>https://stackoverflow.com/questions/77332293/suggested-methods-of-timing-individual-prompt-completion-response-from-llm-an</guid>
      <pubDate>Fri, 20 Oct 2023 15:54:50 GMT</pubDate>
    </item>
    <item>
      <title>Keras 模型无法预测测试集中的值</title>
      <link>https://stackoverflow.com/questions/62838589/keras-model-not-predicting-values-in-the-test-set</link>
      <description><![CDATA[我正在构建一个 Keras 模型来预测用户是否会选择某个产品（二元分类）。
模型似乎在训练时保留的验证集上取得了进展，但当涉及到测试集时，模型的预测全为 0。
我的数据集看起来像这样：
train_dataset

customer_id id target customer_num_id
0 TCHWPBT 4 0 1
1 TCHWPBT 13 0 1
2 TCHWPBT 20 0 1
3 TCHWPBT 23 0 1
4 TCHWPBT 28 0 1
... ... ... ... ...
1631695 D4Q7TMM 849 0 7417
1631696 D4Q7TMM 855 0 7417
1631697 D4Q7TMM 856 0 7417
1631698 D4Q7TMM 858 0 7417
1631699 D4Q7TMM 907 0 7417

我使用以下命令将其拆分为 Train/Val 集：
from sklearn.model_selection import train_test_split

Train, Val = train_test_split(train_dataset, test_size=0.1, random_state=42, shuffle=False)

拆分数据集后，我选择在训练和验证模型时使用的特征：
train_customer_id = Train[&#39;customer_num_id&#39;]
train_vendor_id = Train[&#39;id&#39;]
train_target = Train[&#39;target&#39;]

val_customer_id = Val[&#39;customer_num_id&#39;]
val_vendor_id = Val[&#39;id&#39;]
val_target = Val[&#39;target&#39;]

... 并运行模型：
epochs = 2

for e in range(epochs):
print(&#39;EPOCH: &#39;, e)
model.fit([train_customer_id, train_vendor_id], train_target, epochs=1, verbose=1, batch_size=384)

prediction = model.predict(x=[train_customer_id, train_vendor_id], verbose=1, batch_size=384)
train_f1 = f1_score(y_true=train_target.astype(&#39;float32&#39;), y_pred=prediction.round())
print(&#39;TRAIN F1: &#39;, train_f1)

val_prediction = model.predict(x=[val_customer_id, val_vendor_id], verbose=1, batch_size=384)
val_f1 = f1_score(y_true=val_target.astype(&#39;float32&#39;), y_pred=val_prediction.round())
print(&#39;VAL F1: &#39;, val_f1)

EPOCH: 0
1468530/1468530 [==============================] - 19s 13us/step - 损失: 0.0891
TRAIN F1: 0.1537511577647422
VAL F1: 0.09745762711864409
EPOCH: 1
1468530/1468530 [==============================] - 19s 13us/step - 损失：0.0691
TRAIN F1：0.308748569645272
VAL F1：0.2076433121019108

验证准确率似乎随着时间的推移而提高，模型预测 1 和 0：
 prediction = model.predict(x=[val_customer_id, val_vendor_id], verbose=1, batch_size=384)
np.unique(prediction.round())

array([0., 1.], dtype=float32)

但是当我尝试预测测试集时，模型预测所有值都是 0：
prediction = model.predict(x=[test_dataset[&#39;customer_num_id&#39;], test_dataset[&#39;id&#39;]], verbose=1, batch_size=384)
np.unique(prediction.round())

array([0.], dtype=float32)

测试数据集看起来与训练集和验证集相似，并且它在训练过程中与验证集一样被忽略，但模型无法输出除 0 以外的值。
测试数据集如下所示：
 test_dataset

customer_id id customer_num_id
0 Z59FTQD 243 7418
1 0JP29SK 243 7419
... ... ... ...
1671995 L9G4OFV 907 17414
1671996 L9G4OFV 907 17414
1671997 FDZFYBA 907 17415

这里可能存在什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/62838589/keras-model-not-predicting-values-in-the-test-set</guid>
      <pubDate>Fri, 10 Jul 2020 16:30:15 GMT</pubDate>
    </item>
    <item>
      <title>避免深度神经网络中的梯度消失</title>
      <link>https://stackoverflow.com/questions/46270122/avoiding-vanishing-gradient-in-deep-neural-networks</link>
      <description><![CDATA[我正在研究 Keras，试图深入研究深度学习。
据我所知，由于梯度消失问题，仅堆叠几个密集层就可以有效地阻止反向传播工作。
我发现有一个预先训练好的 VGG-16 神经网络，您可以下载并在其基础上构建。
这个网络有 16 层，所以我想，这就是您遇到梯度消失问题的地方。
假设我想在 Keras 中自己训练网络。我应该怎么做？我应该将各层分成簇并将它们作为自动编码器进行独立训练，然后在其上堆叠一个分类器并进行训练吗？ Keras 中是否有内置的机制？]]></description>
      <guid>https://stackoverflow.com/questions/46270122/avoiding-vanishing-gradient-in-deep-neural-networks</guid>
      <pubDate>Mon, 18 Sep 2017 00:39:47 GMT</pubDate>
    </item>
    </channel>
</rss>