<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Thu, 20 Mar 2025 15:19:41 GMT</lastBuildDate>
    <item>
      <title>如何使用Codebert嵌入识别类似的代码零件？</title>
      <link>https://stackoverflow.com/questions/79523261/how-to-identify-similar-code-parts-using-codebert-embeddings</link>
      <description><![CDATA[我正在使用Codebert比较两个代码的相似性。例如：
 ＃代码1
def calculate_area（半径）：
返回3.14 *半径 *半径
 
 ＃代码2
def Compute_circle_area（R）：
返回3.14159 * r * r
 
 Codebert创建“嵌入”就像对代码的详细描述为数字。然后，我比较这些数值描述，以查看代码的相似之处。这对于告诉我多少代码是相似的。
但是，我无法分辨Codebert认为哪些部分相似。因为“嵌入”很复杂，我无法轻易看到Codebert的重点。比较逐字代码在这里不起作用。
我的问题是：我如何找出两个代码段的哪些特定部分Codebert认为相似，而不仅仅是获得一般相似性得分？
我尝试了简单的DIFF方法，但这违反了纯粹使用Codebert的目的。
我想知道是否可以单独使用Codebert。]]></description>
      <guid>https://stackoverflow.com/questions/79523261/how-to-identify-similar-code-parts-using-codebert-embeddings</guid>
      <pubDate>Thu, 20 Mar 2025 14:30:35 GMT</pubDate>
    </item>
    <item>
      <title>用剪贴图像嵌入[闭合]重建图像</title>
      <link>https://stackoverflow.com/questions/79523091/reconstruct-images-with-clip-image-embedding</link>
      <description><![CDATA[ i最近开始从事一个仅使用图像嵌入语义知识的项目，该项目是从基于夹的模型（例如siglip）编码的图像嵌入的，以重建语义上相似的图像。
为此，我使用了基于MLP的投影仪将夹具嵌入将夹具嵌入到图像编码器的潜在空间中，从扩散模型中，我学会了MSE损失以使预测的潜在向量对齐。然后，我尝试使用从扩散模型管道中的VAE解码器进行解码。但是，图像的输出很模糊，并且丢失了图像的许多细节。
到目前为止，我尝试了以下解决方案，但它们都没有起作用：

拥有一个更深的投影仪，带有更大的隐藏型昏暗以涵盖信息。
尝试最大平均差异（MMD）损失
尝试感知损失
尝试使用更高的图像质量（更高图像解决方案）
尝试使用余弦相似性损失（比较真实/合成图像之间）
尝试使用其他图像编码器/解码器（例如VQ-GAN）

我目前坚持这个重建步骤，有人可以从中分享一些见解吗？
例子：
   ]]></description>
      <guid>https://stackoverflow.com/questions/79523091/reconstruct-images-with-clip-image-embedding</guid>
      <pubDate>Thu, 20 Mar 2025 13:25:13 GMT</pubDate>
    </item>
    <item>
      <title>添加时间序列记录到张板</title>
      <link>https://stackoverflow.com/questions/79522949/adding-time-series-logging-to-tensorboard</link>
      <description><![CDATA[ Tensorboard提供访问绘图“ nofollow noreferrer”&gt;标量，直方图和图像。我试图将模型预测作为简单的向量添加到张板中。目前，我正在通过添加Matplotlib图像来实现这一目标，该图像非常适合视觉检查，但是在模型训练后不可能推断预测跟踪。
如建议在这里” ，有一些方法可以将痕迹表示为张量板中的标量。慢。
在张板中，基于向量的日志记录还有其他方法吗？并且可以推荐哪些其他方法出于此目的？
我最好的选择是简单地将它们保存为 .npy 数组，但是当读取运行计算时，除了 .tsevents 文件外，我最终还会有多个文件。所以我只是好奇人们可以推荐什么。]]></description>
      <guid>https://stackoverflow.com/questions/79522949/adding-time-series-logging-to-tensorboard</guid>
      <pubDate>Thu, 20 Mar 2025 12:33:41 GMT</pubDate>
    </item>
    <item>
      <title>torch_mlir.com当前是官方Pytorch还是Torch-Mlir API的一部分？</title>
      <link>https://stackoverflow.com/questions/79522928/is-torch-mlir-compile-currently-part-of-the-official-pytorch-or-torch-mlir-api</link>
      <description><![CDATA[我已经看到了 torch_mlir.compile 来自几种AI工具的引用，尤其是来自chatgpt，deepseek等的引用。我还根据方案附加了示例代码。
 导入火炬
导入TORCH_MLIR

＃定义一个简单的pytorch模型
类SimpleModel（Torch.nn.Module）：
    def __init __（自我）：
        超级（SimpleModel，self）.__ Init __（）
        self.linear = torch.nn.linear（3，2）

    def向前（self，x）：
        返回self.linear（x）

＃创建模型的实例
model = SimpleModel（）
model.eval（）＃将模型设置为评估模式

＃创建示例输入张量
example_input = torch.randn（1，3）

＃使用火炬 - 摩尔编译模型
mlir_module = torch_mlir.compile（型号，（example_input，），output_type =; torch; quot;）

＃打印MLIR输出
打印（mlir_module）
 
是  torch_mlir.compile 在Pytorch或Torch-Mlir的最新版本中的正式支持功能，还是过时或更名？
我感谢基于API的当前状态的明确答案。]]></description>
      <guid>https://stackoverflow.com/questions/79522928/is-torch-mlir-compile-currently-part-of-the-official-pytorch-or-torch-mlir-api</guid>
      <pubDate>Thu, 20 Mar 2025 12:25:00 GMT</pubDate>
    </item>
    <item>
      <title>tensorflow联合（TFF）中聚合的客户端选择</title>
      <link>https://stackoverflow.com/questions/79522886/client-selection-for-aggregation-in-tensorflow-federated-tff</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79522886/client-selection-for-aggregation-in-tensorflow-federated-tff</guid>
      <pubDate>Thu, 20 Mar 2025 12:12:07 GMT</pubDate>
    </item>
    <item>
      <title>深XDE模型拟合</title>
      <link>https://stackoverflow.com/questions/79522731/deep-xde-model-fit</link>
      <description><![CDATA[我正在尝试使用DEEPXDE在合成数据上训练物理信息的神经网络（PINN）。我的目标是使用简单的葡萄糖模型生成数据，然后在此数据上训练PINN，看看它是否可以恢复模型中使用的参数。
但是，当我训练模型时，拟合很糟糕。这些预测与数据不太匹配，并且损失不如预期。
这是我到目前为止尝试的：

我使用已知的参数生成了合成数据。
我在deepxde中定义了管理方程。
我同时使用了数据丢失和物理损失来训练Pinn。

在使用deepxde为Pinns时是否有人遇到过类似的问题？是否有特定的超参数或培训策略可以有助于改善合适？
 导入DeepXde为DDE
导入numpy作为NP
导入matplotlib.pyplot作为PLT
来自Scipy.Comtegrate Import Odeint
来自Sklearn.metrics导入R2_Score

def单元格（y，t，p）：
    dn = p [0]  -  p [1] *（1 +（y [1] / p [2]）** p [3]） * y [0]
    dc = p [1] *（1 +（y [1] / p [2]）** p [3]） * y [0] -p [4] * y [1]
    返回[DN，DC]

＃仿真参数
times = np.Arange（0，15，0.01）
V，K1，K，N，K2 = 14，1，2，3，4

＃生成合成数据
y = odeint（cell，t = times，y0 = [0，0]，args =（（v，k1，k，n，k2），rtol = 1e-8）

def glycolysy_ode（x，y）：
    y1，y2 = y [：，0：1]，y [：，1：]
    dy1_t = dde.grad.jacobian（y，x，i = 0）
    dy2_t = dde.grad.jacobian（y，x，i = 1）
    
    dn = v -k1 *（1 +（y2 / k）** n） * y1
    dc = k1 *（1 +（y2 / k）** n） * y1 -k2 * y2
    
    返回[DY1_T -DN，DY2_T -DC]

def边界（_，on_initial）：
    返回on_initial

def true_solution（x）：
    idx = np.searchsorted（times，x.flatten（））
    idx = np.clip（idx，0，len（times）-1）＃确保指数保持在范围内
    返回np.hstack（（y [idx，0：1]，y [idx，1：]））

＃直接使用综合数据而无需噪声
noisy_y = y＃没有噪音

geom = dde.Deometry.timedomain（0，15）
ic1 = dde.icbc.ic（Geom，Lambda X：0，边界，组件= 0）
IC2 = dde.icbc.ic（Geom，Lambda X：0，边界，组件= 1）
data = dde.data.pde（Geom，Glycolysis_ode，[IC1，IC2]，35，2，solution = true_solution，num_test = 1500）

layer_size = [1] + [50] * 3 + [2]
激活=“ tanh”
initializer =; glorot制服
net = dde.nn.fnn（layer_size，activation，initializer）

型号= dde.model（数据，网络）
model.compile（&#39;adam＆quort＆quort＆lr = 0.001，量表= [l2相对错误＆quot;]）
损失史，train_state = model.train（迭代= 20000）

＃计算模型的R²得分没有噪音
y_pred = model.predict（times.reshape（-1，1））
r2_y1 = r2_score（noisy_y [：，0]，y_pred [：，0]）
r2_y2 = r2_score（noisy_y [：，1]，y_pred [：，1]）

打印（f6p的f＆quot&#39;r²得分：{r2_y1：.4f}＆quot;）
打印（f16bp的f＆quot&#39;r²得分：{r2_y2：.4f}＆quot;）

＃绘制F6P和F16BP的预测与真实数据
plt.figure（无花果=（10，6））
plt.plot（times，noisy_y [：，0]，&#39;r&#39;，label =&#39;观察到$ f6p（t）$&#39;，lineWidth = 4.0）
plt.plot（times，y_pred [：，0]，&#39;k-&#39;，label =&#39;pinn型号$ f6p（t）$&#39;）
plt.plot（times，noisy_y [：，1]，&#39;b&#39;，label =&#39;观察到$ f16bp（t）$&#39;，lineWidth = 4.0）
plt.plot（times，y_pred [：，1]，&#39;k-&#39;，label =&#39;pinn型号$ f16bp（t）$&#39;）
plt.xlabel（“时间”）
plt.ylabel（“集中度”）
plt.legend（）
plt.show（）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79522731/deep-xde-model-fit</guid>
      <pubDate>Thu, 20 Mar 2025 11:07:49 GMT</pubDate>
    </item>
    <item>
      <title>图像分类器的培训CNN模型[闭合]</title>
      <link>https://stackoverflow.com/questions/79522416/training-cnn-model-for-image-classifier</link>
      <description><![CDATA[我在创建一个模型，可以将艺术分为三类，无论是绘画，数字艺术，雕塑。
我有一个带有2k图像的数据集，现在我被卡住了如何构建好模型
任何人都可以告诉我怎么了，我做错了什么。
我检查了诸如精确度和召回之类的指标，它们超过90％，但准确性滞后（仅24％）。
可能的原因是什么？

我的数据很干净
我有两个目录（培训和验证）
两者都有进一步的子目录（对应 DigitalArt ，绘画和 sculpture 带有等量的样本）

  model = keras。 
    layers.conv2d（32，（3，3），激活=&#39;relu&#39;，input_shape =（256，256，3）），＃这是第一个卷积层具有32个过滤器，使用relu引入非线性性
                             ＃我们以后在所有图层上在第一层中定义所有这些东西，都会自动继承这些道具 
    layers.maxpooling2d（），＃这会降低尺寸，例如图像hieght和width
    layers.conv2d（64，（3，3），激活=&#39;relu&#39;），＃第二层
    layers.maxpooling2d（），
    layers.conv2d（128，（3，3），激活=&#39;relu&#39;），＃第三
    layers.maxpooling2d（），
    layers.flatten（），＃将2D特征地图转换为一个1D，向男性完全连接的层
                             
    layers.dense（128，激活=&#39;relu&#39;），＃完全连接的层 
    层。密度（3，激活=&#39;softmax&#39;）＃3类：绘画，雕塑，数字艺术三神经元作为三个输出 
）））

＃编译模型
model.compile（优化器=&#39;adam&#39;，
              损失=&#39;Sparse_categorical_crossentropy&#39;，
              指标= [&#39;准确性&#39;]）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79522416/training-cnn-model-for-image-classifier</guid>
      <pubDate>Thu, 20 Mar 2025 09:14:52 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法显示训练管道中ML.NET FIT（）功能的训练日志输出？</title>
      <link>https://stackoverflow.com/questions/79521737/is-there-a-way-to-show-the-training-log-output-from-the-ml-net-fit-function-in</link>
      <description><![CDATA[我正在使用Microsoft.ml库中的3.0.1版，我的C＃Winforms对象检测程序。
当我将机器学习模型添加到我的Visual Studio项目中时，在MLModel1.MBConfig页面上训练它。
但是，当我创建培训管道并在程序中调用FIT（）功能时，我希望能够在窗口中看到训练日志，因为培训可能很耗时，我希望能够在进步时看到结果。在进行培训时，如何访问此输出日志？]]></description>
      <guid>https://stackoverflow.com/questions/79521737/is-there-a-way-to-show-the-training-log-output-from-the-ml-net-fit-function-in</guid>
      <pubDate>Thu, 20 Mar 2025 02:07:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么tf.keras不让我通过多个样本权重的字典？</title>
      <link>https://stackoverflow.com/questions/79521352/why-wont-tf-keras-let-me-pass-a-dictionary-of-multiple-sample-weights</link>
      <description><![CDATA[我正在尝试通过单个np。示例的阵列对我的keras模型的两个输出进行了示例阵列，其中一个是对二进制值的置信度度量，其中一个是连续的输出。但是，根据Trackback，我会收到 keyError：0 作为TF.Keras试图使用 Object = Object = Object = Object [_Path] 读取它。代码段中提供了导入，以确保
  def train_model（型号，x_ts_train，x_item_train，y_train_conf，y_train_pct，epochs = 50，batch_size = 32）：
    导入numpy作为NP
    来自sklearn.utils.class_weight导入compute_class_weight

    ＃----确保y_train_conf是整数（0或1）----＃
    y_train_conf = np.asarray（y_train_conf）.astype（int）

    ＃----计算二进制分类的每类权重----＃
    unique_classes = np.unique（y_train_conf.ravel（））
    class_weight_dict = {0：1.0，1：1.0}＃默认权重
    如果len（unique_classes）== 2：＃确保存在0和1
        class_weights = compute_class_weight（class_weight =&#39;balanced&#39;，class = unique_classes，y = y__train_conf.ravel（））
        class_weight_dict = {int（unique_classes [i]）：class_weights [i] for in range（len（simolor_classes））}}

    ＃----将类权重转换为按样本权重（匹配y_train_conf形状）----＃
   sample_weights_conf = np.array（[class_weight_dict [label] for y_train_conf.ravel（））
   sample_weights_conf = sample_weights_conf.reshape（y_train_conf.shape）＃现在形状为（84，5）

   ＃----计算连续尖峰百分比的按样本重量----＃
   y_train_pct = np.Asarray（y_train_pct）
   sample_weights_pct = np.ones_like（y_train_pct）＃默认权重= 1

   nonzero_mask = y_train_pct＆gt; 0
   如果np.any（nonzero_mask）：
       scaling_factor = np.sum（nonzero_mask） / y_train_pct.size
       sample_weights_pct [nonzero_mask] = 1 / max（scaling_factor，1e-6）
   print（sample_weights_conf.shape，sample_weights_pct.shape，y_train_conf.shape，y_train_pct.shape.shape，flush = true）

   ＃sample_weights_binary = np.mean（sample_weights_conf，axis = 1）
   ＃sample_weights_continous = np.mean（sample_weights_pct，axis = 1）

   ＃print（sample_weights_continous.shape，sample_weights_binary.shape，flush = true）
   ＃----火车模型----＃
   历史= model.fit（
       {ts_input＆quot＆quot; x_ts_train，＆quot; item_input＆quot＆quot; x_item_train}，
       {&#39;output_binary＆quot;：y_train_conf，＆quort&#39;output_continouul＆quot;：y_train_pct}，
       时代= epochs，
       batch_size = batch_size，
       验证_split = 0.1，
       详细= 2，
       sample_weight = {&#39;output_binary&#39;：sample_weights_conf，&#39;output_continuul&#39;：sample_weights_pct}＃单独通过
   ）

   返回历史
 
我希望该模型可以毫无问题地将样品权重采用，并试图将它们作为列表传递，将它们作为一个阵列，这是两者的平均值，并试图给出y_train，x_train和sample_weaights作为阵列，所有这些阵列都给了我多种错误，但仍然没有给出一个错误的结果。我的模型的输出定义如下：
  output_binary = dense（num_binary_targets，activation =&#39;sigmoid&#39;，name =＆quort; output_binary＆quot;）（dense_out）     
output_continouul = dense（num_continuun_targets，activation =&#39;linear&#39;，name =＆quot; output_continuul＆quort;）（dense_out）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79521352/why-wont-tf-keras-let-me-pass-a-dictionary-of-multiple-sample-weights</guid>
      <pubDate>Wed, 19 Mar 2025 20:47:01 GMT</pubDate>
    </item>
    <item>
      <title>需要在流失预测模型上帮助[封闭]</title>
      <link>https://stackoverflow.com/questions/79521346/need-help-on-churn-prediction-model</link>
      <description><![CDATA[我是一个小组，为公司做一个ML项目。我们正在进行的项目是服务的流失预测模型，现在该公司将其定义为Churn，整个数据集中只有大约2％的流失。 200万行。这些是我们迄今为止最佳模型的结果：
分类报告 
我们已经尝试了所有常规的监督学习算法和Pytorch神经网络算法，并且我们一直在使用Smote Overplating和Smotetomek，但是对于流失案例，我们的结果不足。截至目前，使用梯度提升，我们在上传图片中看到的分数获得了最佳效果。有什么建议吗？另外，如果我们要通过每行计算搅动或进行窗户的ChurnF.X。如果我们要以3个月的基础计算终止服务的数量，并且如果他们删除了证书百分比，那么它将被视为流失，或者我们是否应该对独特的客户进行分组并将数据集转换为每个客户的一行，而不是出现48个不同时间？数据集中有30列以上的列是捕获太多噪声的模型吗？
任何建议都会有所帮助！
我们尝试了所有常规监督的学习算法和pytorch神经网络。]]></description>
      <guid>https://stackoverflow.com/questions/79521346/need-help-on-churn-prediction-model</guid>
      <pubDate>Wed, 19 Mar 2025 20:42:11 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的Llama 3.1模型在Automodelforcausallm和Llamaforcausallm之间的作用有所不同？</title>
      <link>https://stackoverflow.com/questions/79494100/why-does-my-llama-3-1-model-act-differently-between-automodelforcausallm-and-lla</link>
      <description><![CDATA[我有一组权重，一个令牌，相同的提示和相同的生成参数。然而，某种程度上，当我使用AutoModelForCausAllm加载模型时，我将获得一个输出，当我使用LlamaForCausAllm和同一config和state_dict手动构造它时，我完全得到了另一个输出。&gt; 
此代码可以显示A6000和A100的区别。
 导入火炬
从变形金刚导入（
    自动传动器，
    AutomodelForCausAllm，
    Llamaforcausallm，
    Llamaconfig
）

＃1）根据需要调整这些
model_name =＆quot; meta-llama/llama-3.1-8b; quot
提示=&#39;Llama 3.1的Hello！告诉我一些有趣的东西。”
dtype = Torch.float16＃或Torch.float32如果需要

＃2）获取令牌
tokenizer = autotokenizer.from_pretaining（model_name，use_fast = false）

＃准备输入
inputs = tokenizer（提示，return_tensors =; pt;）。

##################################
＃a）加载AutomodelForCausAllm
##################################

打印（===加载AutomodelforCausAllm ===;）

model_auto = automodelforcausallm.from_pretaining（
    model_name，
    attn_implementation =;急切＃＃匹配您的用法
    TORCH_DTYPE = dtype
）.cuda（）
model_auto.eval（）＃关闭辍学
config = model_auto.config
使用Torch.no_grad（）：
    out_auto = model_auto（**输入）
logits_auto = out_auto.logits＃shape：[batch_size，seq_len，vocab_size]

del model_auto
TORCH.CUDA.EMPTY_CACHE（）

##################################
＃b）带有Llamaforcausallm +配置加载
##################################

打印（＆quot; ===加载llamaforcausallm + config ====;）

＃从同一检查点获取配置
＃直接构建骆驼模型
model_llama = llamaforcausallm（config）.cuda（）（）
model_llama.eval（）

＃加载与使用AutomodelForCausAllm使用的相同权重
model_auto_temp = automodelforcausallm.from_pretrateing（model_name，torch_dtype = dtype）
model_llama.load_state_dict（model_auto_temp.state_dict（））
del model_auto_temp
TORCH.CUDA.EMPTY_CACHE（）

使用Torch.no_grad（）：
    out_llama = model_llama（**输入）
logits_llama = out_llama.logits

##################################
＃c）比较逻辑
##################################

＃计算最大绝对差异
max_diff =（logits_auto -logits_llama）.abs（）。max（）
print（f＆quot \ nmax logits之间的绝对差异：{max_diff.item（）}; quot;）

如果max_diff＆lt; 1E-7：
    打印（“→逻辑有效相同）（在浮点精度内）。
别的：
    print（“→logits存在非平凡的差异！”）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79494100/why-does-my-llama-3-1-model-act-differently-between-automodelforcausallm-and-lla</guid>
      <pubDate>Sat, 08 Mar 2025 08:24:12 GMT</pubDate>
    </item>
    <item>
      <title>稳定的扩散模型的运行模型提示从MPOS上拥抱脸部的稳定扩散模型</title>
      <link>https://stackoverflow.com/questions/76939164/running-model-prompt-on-stable-diffusion-model-from-hugging-face-on-mps-macos</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76939164/running-model-prompt-on-stable-diffusion-model-from-hugging-face-on-mps-macos</guid>
      <pubDate>Sun, 20 Aug 2023 11:27:44 GMT</pubDate>
    </item>
    <item>
      <title>反馈循环：预期发电机的“ CPU”设备类型，但找到了“ MPS”</title>
      <link>https://stackoverflow.com/questions/76817578/feedback-loop-expected-a-cpu-device-type-for-generator-but-found-mps</link>
      <description><![CDATA[我试图在MacOS M2上捕获拥抱面模型。但是，MPS尚不存在火炬操作员“ Aten :: Random_”。因此，我用运行了该程序
  pytorch_enable_mps_fallback = 1 
 
然后，我收到了以下消息，“预期“ CPU”发电机设备类型，但找到了“ MPS”。我试图通过指定 generator = torch.generator（device =&#39;mps&#39;）在数据载加载程序中解决此问题。然而，这扭曲了信息，并产生“预期A MPS：0”生成器设备，但发现了“ MPS”。因此，看来我陷入了循环。
这是我正在使用的完整代码
 从数据集导入load_dataset
从变形金刚导入自动源
导入火炬
来自torch.utils.data导入数据加载程序

dataset = load_dataset（&#39;yelp_review_flull&#39;）
tokenizer = autotokenizer.from_pretaining（“ bert-base cased;）
def tokenize_function（示例）：
    返回tokenizer（示例[&#39;text;]，padding =; max_length＆quort＆quot; truncation = true）

tokenize_datasets = dataset.map（tokenize_function，batched = true）

tokenized_datasets = tokenized_datasets.remove_columns（[text;]）
tokenized_datasets = tokenized_datasets.rename_column（“ label; quot”;
tokenized_datasets.set_format（“ Torch”）
small_train_dataset = tokenized_datasets [&#39;train;]。shuffle（seed = 42）。选择（range（1000））
small_eval_dataset = tokenized_datasets [&#39;test&#39;]。shuffle（seed = 42）。选择（range（range（1000）））


train_dataloader = dataloader（small_train_dataset，shuffle = true，batch_size = 8，generator = torch.generator（device =&#39;cpu&#39;）
eval_dataloader = dataloader（small_eval_dataset，batch_size = 8，generator = torch.generator（device =&#39;cpu&#39;））
对于train_dataloader中的批次：
    打印（批次）
    ＃当然，我想在这里做其他事情，但是打开循环已经产生错误。 
 
我正在使用火炬2.0.1，数据集2.14.0与变压器4.31.0 一起使用]]></description>
      <guid>https://stackoverflow.com/questions/76817578/feedback-loop-expected-a-cpu-device-type-for-generator-but-found-mps</guid>
      <pubDate>Wed, 02 Aug 2023 07:34:31 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch MPS：“ MPS.Scatter_nd” OP无效输入张量形状</title>
      <link>https://stackoverflow.com/questions/76196734/pytorch-mps-mps-scatter-nd-op-invalid-input-tensor-shape</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76196734/pytorch-mps-mps-scatter-nd-op-invalid-input-tensor-shape</guid>
      <pubDate>Sun, 07 May 2023 23:44:53 GMT</pubDate>
    </item>
    <item>
      <title>支持向量机。精度和/或准确性？</title>
      <link>https://stackoverflow.com/questions/36846795/support-vector-machine-precision-and-or-accuracy</link>
      <description><![CDATA[我正在尝试弄清我使用的代码是计算精度还是准确性或两者兼而有之。由于我只有少量的统计背景（用另一种语言），所以我真的不理解 wikipedia&#39;&gt; wikipedia文章涵盖该主题。
具体地我使用以下python代码：
 来自Sklearn Import SVM，Cross_validation
clf = svm.svc（内核=内核，c = c）
scores = cross_validation.cross_val_score（clf，featurematrix，np.squeeze（labelmatrix），cv = d_inds）
 
  scikit-learn 函数可以在此处找到：

     sklearn.svc.svm.svc.svc.svc       
 ]]></description>
      <guid>https://stackoverflow.com/questions/36846795/support-vector-machine-precision-and-or-accuracy</guid>
      <pubDate>Mon, 25 Apr 2016 17:03:37 GMT</pubDate>
    </item>
    </channel>
</rss>