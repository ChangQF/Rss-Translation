<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 26 Mar 2024 21:13:30 GMT</lastBuildDate>
    <item>
      <title>谁能推荐一些很棒的机器学习项目来添加到简历中？对于任何初学者[关闭]</title>
      <link>https://stackoverflow.com/questions/78227961/can-anyone-suggest-some-great-projects-to-work-on-the-machine-learning-to-add-on</link>
      <description><![CDATA[我是机器学习的新手，希望构建一个强大的应用程序来确保数据分析或数据科学方面的任何实习或工作。任何建议或指导将不胜感激
任何可参考的资源或视频或任何基于项目的课程（如果您知道）请推荐]]></description>
      <guid>https://stackoverflow.com/questions/78227961/can-anyone-suggest-some-great-projects-to-work-on-the-machine-learning-to-add-on</guid>
      <pubDate>Tue, 26 Mar 2024 20:18:10 GMT</pubDate>
    </item>
    <item>
      <title>解析决策树的法学硕士？</title>
      <link>https://stackoverflow.com/questions/78227894/llm-for-parsing-decision-trees</link>
      <description><![CDATA[决策树图像
我正在寻找一种开源法学硕士，它能够接受与上述类似的决策树查询。这是否意味着找到最好的开源 ViT 模型？或者是否有更好的方法，例如以某种方式将树预处理为文本并向法学硕士询问？考虑到我仅对开源模型的限制，什么性能会更好？
我已经尝试过 ChatGPT4，它似乎表现得很好，但我希望在本地运行所有内容。]]></description>
      <guid>https://stackoverflow.com/questions/78227894/llm-for-parsing-decision-trees</guid>
      <pubDate>Tue, 26 Mar 2024 20:03:23 GMT</pubDate>
    </item>
    <item>
      <title>未来 14 天以 5 分钟为间隔的时间序列预测数据</title>
      <link>https://stackoverflow.com/questions/78227784/time-series-forecasting-data-with-5-minute-intervals-for-the-next-14-days</link>
      <description><![CDATA[我有一项任务，以 5 分钟为间隔预测未来 14 天内的呼叫数量。我们有过去 4 年的数据，间隔为 5 分钟，只有 1 个月的间隔。我计划使用神经网络模型来解决这个任务。但是，我不确定最好的方法：
我是否应该首先预测第二天的呼叫数量，然后使用这些预测来预测接下来 14 天的间隔呼叫数量？
或者直接预测未来 14 天间隔内的调用次数是否更明智？]]></description>
      <guid>https://stackoverflow.com/questions/78227784/time-series-forecasting-data-with-5-minute-intervals-for-the-next-14-days</guid>
      <pubDate>Tue, 26 Mar 2024 19:40:34 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 python 制作一个可以理解我的问题然后回答它的聊天机器人</title>
      <link>https://stackoverflow.com/questions/78227761/how-to-make-a-chatbot-using-python-which-can-understand-my-question-and-then-ans</link>
      <description><![CDATA[我想知道，如果我使用 python 制作一个聊天机器人，并且在代码中我说如果我说你好吗？那么它应该说我很好，但我希望他理解我的问题，就像我说他什么一样那么他应该明白这意味着你好吗，然后回答我的问题。
我试图制作一个可以理解我的问题的聊天机器人，但是当我制作它时，它只能回答我在代码中编写的问题。我希望聊天机器人在理解问题的含义后应该回答问题。]]></description>
      <guid>https://stackoverflow.com/questions/78227761/how-to-make-a-chatbot-using-python-which-can-understand-my-question-and-then-ans</guid>
      <pubDate>Tue, 26 Mar 2024 19:35:02 GMT</pubDate>
    </item>
    <item>
      <title>Bert Istantiation TypeError：“NoneType”对象不可调用 Tensorflow</title>
      <link>https://stackoverflow.com/questions/78227490/bert-istantiation-typeerror-nonetype-object-is-not-callable-tensorflow</link>
      <description><![CDATA[我正在尝试使用 TensorFlow 实例化 BERT 模型。这段代码直到几天前都运行正常，但现在我收到了这个错误。我已经使用相同的代码在其他笔记本中的 Kaggle 上实例化 BERT 模型，并且运行良好。谁能给我提示吗？我应该注意，我正在使用 TensorFlow 版本 4.31.0 来修复另一个错误。我使用的代码是：
# Creazione del modello BERT per BYTECODE
BC_input_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“input_ids”)
BC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
BC_pooler_output = BC_bert_model(BC_input_layer)[1]

BC_dropout_layer = Dropout(dropout_rate)(BC_pooler_output)
BC_output_layer = 密集（6，激活=&#39;sigmoid&#39;）（BC_dropout_layer）

BC_model = 模型（输入=BC_input_layer，输出=BC_output_layer）

错误是
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
TypeError Traceback（最近一次调用最后一次）
[10] 中的单元格，第 19 行
     17 SC_input_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“input_ids”)
     18 SC_mask_layer = 输入(shape=(max_seq_length,), dtype=tf.int32, name=“attention_mask”)
---&gt; 19 SC_bert_model = TFBertModel.from_pretrained(“bert-base-uncased”)
     20 SC_pooler_output = SC_bert_model(SC_input_layer, Attention_mask=SC_mask_layer)[1] # 第二个输出，che è il pooler_output
     22 # 辍学层的Aggiungi

文件 /opt/conda/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:2894，在 TFPreTrainedModel.from_pretrained(cls、pretrained_model_name_or_path、config、cache_dir、ignore_mismatched_sizes、force_download、local_files_only、token、revision、*model_args、 **夸格斯）
   第2892章
   第2893章
-&gt;第2894章
   第2896章
   第2897章

文件 /opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:224，位于 Layer.__new__..build_wrapper(*args, **kwargs)
    第221章
    222 def build_wrapper（*args，**kwargs）：
    223 with backend.name_scope(obj.name, caller=obj)：
--&gt;第224章
    225 # 记录构建配置。
    [第 226 章]

文件 /opt/conda/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1131，在 TFPreTrainedModel.build(self, input_shape) 中
   攀上漂亮女局长之后1129
   第1130章
-&gt;攀上漂亮女局长之后1131
   第1132章
   第1133章

类型错误：“NoneType”对象不可调用
]]></description>
      <guid>https://stackoverflow.com/questions/78227490/bert-istantiation-typeerror-nonetype-object-is-not-callable-tensorflow</guid>
      <pubDate>Tue, 26 Mar 2024 18:29:11 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何 API 或模型可用于构建代码审查系统？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78226916/is-there-any-api-or-model-which-i-can-use-to-build-a-code-review-system</link>
      <description><![CDATA[我正在构建一个微服务，我想在其中为 github 存储库分配一个分数，或者您可以根据其代码质量将其视为存储库的 zip 文件。
请帮助我是否有任何免费服务或可用的 API 或任何我可以使用的 AI 工具？
或者只是指导我如何自己开发这个代码审查系统，我对此任务没有任何想法。
我将使用此服务来评估存储库的代码质量。如果我需要开发和使用，请指导我为此目的训练我自己的机器学习模型。任何帮助将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78226916/is-there-any-api-or-model-which-i-can-use-to-build-a-code-review-system</guid>
      <pubDate>Tue, 26 Mar 2024 16:43:17 GMT</pubDate>
    </item>
    <item>
      <title>ARIMA 模型返回 KeyError</title>
      <link>https://stackoverflow.com/questions/78226759/arima-model-returning-a-keyerror</link>
      <description><![CDATA[拟合训练数据集后，我尝试使用拟合的 ARIMA 模型进行预测，但收到此错误消息 ==&gt; start 参数无法与与数据索引相关的位置匹配。&#39;
pred = model_fit.predict(start=test_oilPrice.index.min(), end=test_oilPrice.index.max())[[在此处输入图像描述](https:/ /i.stack.imgur.com/rwvM6.png)](https:/ /i.stack.imgur.com/MsdK9.png)]]></description>
      <guid>https://stackoverflow.com/questions/78226759/arima-model-returning-a-keyerror</guid>
      <pubDate>Tue, 26 Mar 2024 16:16:01 GMT</pubDate>
    </item>
    <item>
      <title>如何量化一系列预测的一致性，包括。预测置信度，使用 sklearn 或类似库的标准函数[关闭]</title>
      <link>https://stackoverflow.com/questions/78226139/how-to-quantify-the-consistency-of-a-sequence-of-predictions-incl-prediction-c</link>
      <description><![CDATA[假设我让分类模型在不同的情况下多次对单个对象进行分类。理想情况下，它应该一次又一次地预测同一类。但实际上，其类别预测可能会有所不同。
因此，给定单个对象的类预测序列，我想测量该序列的一致性。需要明确的是，这并不是将预测与某些基本事实进行比较。这是关于预测序列本身的一致性。

例如，像 class_a, class_a, class_a, class_a 这样完全一致的预测序列应该获得满分。
不太一致的序列，例如class_a、class_b、class_a、class_c
应该得到较低的分数。
以及完全不一致的序列，例如
class_a、class_b、class_c、class_d 应获得最低分数
有可能。

目标是找出我们可能需要哪些对象来继续训练分类模型。如果分类模型对某个对象的预测不太一致，那么我们可能需要将该对象添加到数据集中进行进一步训练。
最好它适用于任意数量的可能类别，并且还考虑预测置信度。序列 class_a (0.9)、class_b (0.9)、class_a (0.9)、class_c (0.9) 的得分应低于 class_a (0.9)、class_b (0.2)、class_a (0.8) ), class_c (0.3)，因为当预测与高置信度不一致时，它就不好。
我可以自己构建一些东西，但我想知道是否有标准的 sklearn 或 scipy （或类似）函数？提前致谢！
对这个问题的评论建议Spearman 相关系数 或 坎德尔相关系数。我也会研究一下。]]></description>
      <guid>https://stackoverflow.com/questions/78226139/how-to-quantify-the-consistency-of-a-sequence-of-predictions-incl-prediction-c</guid>
      <pubDate>Tue, 26 Mar 2024 14:41:53 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Flower 和 Tensorflow 向联邦学习中的服务器发送额外参数？</title>
      <link>https://stackoverflow.com/questions/78221905/how-to-send-extra-parameters-to-server-in-federated-learning-with-flower-and-ten</link>
      <description><![CDATA[我想将带有模型更新的额外参数发送到服务器，然后将服务器中的这些额外参数用于其他目的。我在这个项目中使用 Flower 和 Tensorflow。在发送额外参数之前，我的模型运行良好。目前我有这些代码客户端模型 server.py。
如何在服务器中成功发送额外参数或值并接收它？
感谢您的帮助。
我尝试在 get_parameter 方法中发送附加参数，并使用 FedAvg 策略接收它。但我一次又一次地遇到这个错误。 错误]]></description>
      <guid>https://stackoverflow.com/questions/78221905/how-to-send-extra-parameters-to-server-in-federated-learning-with-flower-and-ten</guid>
      <pubDate>Mon, 25 Mar 2024 21:38:40 GMT</pubDate>
    </item>
    <item>
      <title>“MENACE”井字棋电脑需要多少场比赛才能训练</title>
      <link>https://stackoverflow.com/questions/78219696/how-many-games-will-a-menace-tic-tac-toe-computer-take-to-train</link>
      <description><![CDATA[我最近读到了唐纳德·米奇 (Donald Michie) 设计的用火柴盒建造的“计算机”，它可以自学如何玩井字游戏。这是关于它的维基百科文章：
https://en.m.wikipedia.org/wiki/Matchbox_Educable_Noughts_and_Crosses_Engine 
我觉得它看起来很有趣，所以我决定用 Python 制作一个数字版本，以供娱乐和练习。它在对抗随机走棋时效果很好（我刚刚根据约 10,000 场比赛生成的数据再次运行了 5353 场比赛，它赢得了 5353 场比赛中的 4757 场），但它仍然经常输给我。
以下是完美答案应解决的一些问题：

需要玩多少场游戏才能让“火柴盒电脑”与 Michie 设计的电脑完全一样，才能完美地开始玩游戏？

带有实际火柴盒的原始计算机是否达到了完美状态
玩吗？

如果仅与计算机进行训练，计算机能否达到完美的发挥
随机移动？


编辑：
这个问题并不是寻求代码方面的帮助，但下面的评论表明包含代码可能会有所帮助。以下是我创建的 GitHub 存储库的链接，以便我可以在此处共享：
https://github.com/ACertainArchangel/ Recreation-Of-MENACE-Tic-Tac-Toe..git
抱歉，我知道这不太好并且不遵守约定；我只写了几个月的代码:)]]></description>
      <guid>https://stackoverflow.com/questions/78219696/how-many-games-will-a-menace-tic-tac-toe-computer-take-to-train</guid>
      <pubDate>Mon, 25 Mar 2024 14:12:37 GMT</pubDate>
    </item>
    <item>
      <title>用于多标签分类的堆叠集成学习</title>
      <link>https://stackoverflow.com/questions/78214688/stacking-ensamble-learning-for-multilabelclassification</link>
      <description><![CDATA[我有两个 BERT 模型来实现代码中漏洞检测的多标签分类。一名接受过源代码培训，另一名接受过编译代码培训。他们实现的任务是多标签分类，因此两个模型的单个输出都是一个包含 6 个元素的数组，每个元素可以是 0 或 1，指示漏洞是否存在。
我想在这两个模型之上构建一个经典的 ML 分类器（如随机森林、SVM 或逻辑回归等），实现称为 Stacking 的集成技术。知道我正在处理多标签分类，我该如何实现这一点？
我的主要疑问是如何堆叠输出（这将成为最终分类器的输入）。]]></description>
      <guid>https://stackoverflow.com/questions/78214688/stacking-ensamble-learning-for-multilabelclassification</guid>
      <pubDate>Sun, 24 Mar 2024 13:28:23 GMT</pubDate>
    </item>
    <item>
      <title>对训练集和测试集进行数据预处理？</title>
      <link>https://stackoverflow.com/questions/77493779/data-preprocessing-to-both-train-and-test-sets</link>
      <description><![CDATA[在构建预测模型时需要帮助理解数据预处理的步骤。
相信是这样的场景：
我构建了决策树，
I 合并多个 csv 中的文件；执行了一些步骤，例如为目标变量创建多个类别，还将数值变量转换为分类变量并添加一些新的计算列，对输入变量进行插补
然后划分 70 个训练和 30 个测试
然后我加入了 knime 中决策树学习器节点的 70% 分区测试，并将分区节点的 30% 输出加入到决策树预测器节点，我得到了 100% 的连接准确度。
经过挖掘，我发现我所做的所有数据预处理（添加额外的计算列、插补、从数值转换分类列）都必须在数据分区之前完成，以获得训练集和测试集。
我的疑问是，如果我在拆分后执行此操作，预测器节点如何知道我的更改，因为我根据数据集中的数字列创建了新的目标变量列。当我对测试数据执行此操作时，我什至如何使用预测器节点中不存在的目标变量。
并且还在某处读到我之前所做的所有步骤，我也必须在测试数据上执行。真的是这样吗？我觉得这对于在测试数据上复制每个数据清理步骤来说太乏味了。
即使在分割后进行清洁，又有什么区别。
我是一个绝对的初学者，请帮助我，如果您可以提供特定于 knime 平台的答案，那将会非常有帮助。提前致谢
描述问题中的所有内容。]]></description>
      <guid>https://stackoverflow.com/questions/77493779/data-preprocessing-to-both-train-and-test-sets</guid>
      <pubDate>Thu, 16 Nov 2023 09:50:20 GMT</pubDate>
    </item>
    <item>
      <title>如何使用不同的数据集并使用相同的模型进行训练</title>
      <link>https://stackoverflow.com/questions/77484650/how-to-use-to-different-dataset-and-train-it-with-same-model</link>
      <description><![CDATA[我有两种不同类型 ddos​​ 攻击的两个数据集，如何将此数据集合并到一个大集合中进行使用？我正在尝试在 cic ddos​​ 2019 数据集上训练深度学习模型，该数据集包含不同 ddos​​ 方法的 7 个不同的 CSV 文件
我尝试将它们合并在一起，但没有
如何基于这 7 个不同的数据集来为 ids 加载模型，请建议任何解决方案]]></description>
      <guid>https://stackoverflow.com/questions/77484650/how-to-use-to-different-dataset-and-train-it-with-same-model</guid>
      <pubDate>Wed, 15 Nov 2023 00:59:45 GMT</pubDate>
    </item>
    <item>
      <title>与张量流的顺序密集的混淆</title>
      <link>https://stackoverflow.com/questions/77475341/confusion-with-tensorflows-sequential-dense</link>
      <description><![CDATA[我正在使用 Tensorflow 进行回归探针，并创建了两个模型，它们的第一个密集层略有不同。
模型
# 创建一些回归数据
X_regression = tf.range(0, 1000, 5)
y_regression = tf.range(100, 1100, 5) # Y = X+ 100

# 将回归数据拆分为训练集和测试集
X_reg_train = X_regression[:150]
X_reg_test = X_regression[150:]
y_reg_train = y_regression[:150]
y_reg_test = y_regression[150:]

模型1
# 设置随机种子
tf.random.set_seed(42)


model_1_reg = tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
]）


model_1_reg.compile(loss=tf.keras.losses.mae,
                    优化器=tf.keras.optimizers.Adam(),
                    指标=[&#39;mae&#39;])

model_1_reg.fit(tf.expand_dims(X_reg_train, axis=-1), y_reg_train, epochs=100)

模型 2
# 设置随机种子
tf.random.set_seed(42)


model_2_reg = tf.keras.Sequential([
    tf.keras.layers.Dense(100, input_shape=(无, 1)),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
]）


model_2_reg.compile(loss=tf.keras.losses.mae,
                    优化器=tf.keras.optimizers.Adam(),
                    指标=[&#39;mae&#39;])

model_2_reg.fit(tf.expand_dims(X_reg_train, axis=-1), y_reg_train, epochs=100)

我对是否应该添加 input_shape 感到困惑。 模型 1 的输入形状变为 (None, 1)，模型 2 的输入变为 (None, None, 1) .
它们都运行，但性能不同。
模型 2 是有意义的，因为我们输入的是一个数组，但如果我考虑一下，这是否意味着我在输入层中只有一个节点？因为我给它一个完整的 ndarray 而不是它本身的实例。 模型 1 也很有意义，因为我想将每个数字放入其中。
那么，哪一个更有意义呢？或者我应该在什么情况下使用每个模型？
另外，对于模型 2 的适合度，为什么这样做
tf.expand_dims(X_reg_train, axis=-1)

对于X
model_2_reg.fit(tf.expand_dims(X_reg_train, axis=-1), y_reg_train, epochs=100

工作？我认为我们应该将其作为批处理或数据数组放入，因此它应该位于 ndarray 内？]]></description>
      <guid>https://stackoverflow.com/questions/77475341/confusion-with-tensorflows-sequential-dense</guid>
      <pubDate>Mon, 13 Nov 2023 16:22:50 GMT</pubDate>
    </item>
    <item>
      <title>SMOTE在分类问题中的正确使用方法</title>
      <link>https://stackoverflow.com/questions/77446462/the-right-way-of-using-smote-in-classification-problems</link>
      <description><![CDATA[在分类建模过程中实现 SMOTE() 的正确方法是什么？我对如何应用 SMOTE() 感到非常困惑&gt; 那里。假设我将数据集分为训练和测试，如下所示：
从 sklearn.pipeline 导入管道
从 imblearn.over_sampling 导入 SMOTE
从 imblearn.pipeline 将管道导入为 imbpipeline
从 sklearn.model_selection 导入 GridSearchCV，train_test_split

# 一些数据集初始化
X = df.drop([&#39;事物&#39;], 轴 = 1)
y = df[&#39;事物&#39;]

# 训练测试分割
X_train，X_test，y_train，y_test = train_test_split（X，y，test_size = 0.2，random_state = 42）

# 训练数据集上的 SMOTE()：
X_train_smote, y_train_smote = SMOTE().fit_resample(X_train, y_train, random_state=42)

在上述分类问题的训练数据集上应用 SMOTE() 后，我的问题是：

像这样分割上面的数据集后，我应该在管道内应用 SMOTE() 内部吗？：

# 用于缩放和初始化模型的管道
管道 = imbpipeline(步骤 = [(&#39;scale&#39;, StandardScaler()),
                                (&#39;结束&#39;, SMOTE(random_state = 42)),
                                (&#39;模型&#39;, LogisticRegression(random_state = 42))])

# 然后用Repeated Stratified KFold进行模型评估，
# 然后进行网格搜索进行超参数调整
# 然后用看不见的 X_test 进行实际的模型测试（像这样）：

cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 42)

params = {&#39;model__penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;],
          &#39;模型__C&#39;:[0.001, 0.01, 0.1, 5, 10, 100]}
    
网格 = GridSearchCV(估计器 = 管道,
                    param_grid = 参数，
                    评分 = &#39;roc_auc&#39;,
                    简历=简历，
                    职位数 = -1)

grid.fit(X_train_smote, y_train_smote)
    

cv_score = grid.best_score_
test_score = grid.score(X_test, y_test)

print(f&quot;交叉验证分数: {cv_score} \n 测试分数: {test_score}&quot;)


或者，我应该应用管道而不像这样调用SMOTE()吗？

# 用于缩放和初始化模型的管道
管道 = imbpipeline(步骤 = [(&#39;scale&#39;, StandardScaler()),
                                (&#39;模型&#39;, LogisticRegression(random_state = 42))])

# 与上述建模、评估等过程相同...


或者，我应该像这样使用 SMOTE() 而不使用这样的 SMOTE 数据：

# 训练测试分割
X_train，X_test，y_train，y_test = train_test_split（X，y，test_size = 0.2，random_state = 42）

# 用于缩放和初始化模型的管道
管道 = imbpipeline(步骤 = [(&#39;scale&#39;, StandardScaler()),
                                (&#39;结束&#39;, SMOTE(random_state = 42)),
                                (&#39;模型&#39;, LogisticRegression(random_state = 42))])

# 与上面的建模、评估等过程相同...

# 但是！，当拟合 grid.fit() 时，我们这样做？：
grid.fit(X_train, y_train)


或者像这样在 Sklearn Pipeline 中使用 SMOTE() 训练数据？：

X_train_smote, y_train_smote = SMOTE().fit_resample(X_train, y_train, random_state=42)

管道 = 管道(步骤 = [(&#39;scale&#39;, StandardScaler()),
                             (&#39;模型&#39;, LogisticRegression(random_state = 42))])


# 与上面的建模、评估等过程相同...

# 但是！，当拟合 grid.fit() 时，我们这样做？：
grid.fit(X_train_smote, y_train_smote)

]]></description>
      <guid>https://stackoverflow.com/questions/77446462/the-right-way-of-using-smote-in-classification-problems</guid>
      <pubDate>Wed, 08 Nov 2023 14:31:44 GMT</pubDate>
    </item>
    </channel>
</rss>