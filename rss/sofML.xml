<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 22 Jul 2024 09:19:57 GMT</lastBuildDate>
    <item>
      <title>如何可视化和理解 Numpy 维度？</title>
      <link>https://stackoverflow.com/questions/78777795/how-to-visualize-and-understand-numpy-dimensions</link>
      <description><![CDATA[例如
1D = np.array([100, 200, 300, 400]) 
2D = np.array([[100, 200, 300, 400]]) 
3D = np.array([[[100, 200, 300, 400]]]])

那么，1D、2D、3D 维度有什么区别，但所有维度的值都相同，请帮我形象化这个概念。
我刚刚学会了这个概念，但无法理解这个维度，为什么如果我们添加额外的括号，它们的维度就会不同。]]></description>
      <guid>https://stackoverflow.com/questions/78777795/how-to-visualize-and-understand-numpy-dimensions</guid>
      <pubDate>Mon, 22 Jul 2024 09:02:03 GMT</pubDate>
    </item>
    <item>
      <title>多个数据集的集成方法</title>
      <link>https://stackoverflow.com/questions/78777745/ensemble-methods-for-multiple-datasets</link>
      <description><![CDATA[我有 5 个数据集，我想用它们来预测一个特征。我想将 5 个数据集的预测组合起来，做出一个最终预测。考虑到不同数据集的性质，将数据集组合成一个数据集似乎不是一个可行的选择。据我所知，大多数现有的集成方法都需要使用相同的数据集来组合预测，有没有办法用多个数据集来做到这一点？
我尝试过对结果取平均值，并尝试使用 5 组预测作为输入来制作元模型，但我觉得应该有更好的方法来做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/78777745/ensemble-methods-for-multiple-datasets</guid>
      <pubDate>Mon, 22 Jul 2024 08:50:13 GMT</pubDate>
    </item>
    <item>
      <title>多类别分类选择机器学习模型</title>
      <link>https://stackoverflow.com/questions/78777697/multi-class-classification-selecting-machine-learning-model</link>
      <description><![CDATA[在多类分类问题中，使用神经网络通常比使用 svm 模型更好吗？
我对 ML 了解不多，这是我用我的数据制作的图。
计算 PCA 并使用 SVM 进行线性分类对我来说会更好吗？scatterplot3d
pca visuslization
所以当我得到这个准确度指标时。我保留代码可以吗？准确度]]></description>
      <guid>https://stackoverflow.com/questions/78777697/multi-class-classification-selecting-machine-learning-model</guid>
      <pubDate>Mon, 22 Jul 2024 08:40:05 GMT</pubDate>
    </item>
    <item>
      <title>GPy 回归（高斯过程）中的输出 Gram 矩阵</title>
      <link>https://stackoverflow.com/questions/78776963/output-gram-matrix-in-gpy-regression-gaussian-process</link>
      <description><![CDATA[因为我需要在大量点上训练我的 GP，所以我不仅想保存优化的超参数
ker = GPy.kern.Matern32(nc, ARD=True) 
m = GPy.models.GPRegression(xTrain, np.reshape(yTrain, (-1, 1)), ker, noise_var=1e-8)
m.Mat32.lengthscale.constrain_bounded(0.01, 5e0)
m.Mat32.variance.constrain_bounded(1e-4, 1e+10)
m.Gaussian_noise.variance.constrain_fixed(1e-8)

m.optimize_restarts(messages=True, num_restarts=1, max_f_eval=10000)
np.save(f, m.param_array)

yPrd, yVar = m.predict_noiseless(xTest)
yStd = np.sqrt(yVar)
yPred = np.squeeze(yPrd)

但也要保存 Gram 矩阵与观察到的 值 K(s,s)^{-1} f 的乘积
后验的平均值 = K(s^*,s)K(s,s)^{-1} f
因此加载 块 K(s,s)^{-1} f 并仅与相应的测试 点 K(s^*,s) 执行一次乘法。我如何访问/输出这些矩阵/保存它们？]]></description>
      <guid>https://stackoverflow.com/questions/78776963/output-gram-matrix-in-gpy-regression-gaussian-process</guid>
      <pubDate>Mon, 22 Jul 2024 04:33:20 GMT</pubDate>
    </item>
    <item>
      <title>使用经过 Gamma 风险暴露数据训练的机器学习模型预测期权到期效应 [关闭]</title>
      <link>https://stackoverflow.com/questions/78776893/predicting-the-options-expiration-effect-using-machine-learning-models-trained-w</link>
      <description><![CDATA[我正在尝试实现一篇论文。它适用于机器学习和股票市场数据，我已经收集了有关研究论文的训练所需的所有数据。如果有人知道如何训练有关这篇论文的模型，请重播。
研究论文
我尝试过模型，但我对机器学习不是很熟悉，我不知道我做的是否正确]]></description>
      <guid>https://stackoverflow.com/questions/78776893/predicting-the-options-expiration-effect-using-machine-learning-models-trained-w</guid>
      <pubDate>Mon, 22 Jul 2024 03:52:53 GMT</pubDate>
    </item>
    <item>
      <title>模型需要很长时间才能加载 [tf.keras.models.load_model]</title>
      <link>https://stackoverflow.com/questions/78776878/model-taking-forever-to-load-tf-keras-models-load-model</link>
      <description><![CDATA[我尝试在 Visual Studio Code 上本地加载模型，但当我运行脚本时，它卡在加载模型行。我已在 Google Collab 上使用 CPU 成功运行该脚本，因此我不认为这是计算能力不足造成的，并且在运行脚本时，我的 CPU 容量只有 25%。有人知道为什么会发生这种情况吗？
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras.applications.resnet import preprocess_input
import pickle

# 列出可用的 GPU
gpus = tf.config.list_physical_devices(&#39;CPU&#39;)

if gpus:
print(&quot;CPU found:&quot;, gpus)
else:
print(&quot;No CPU found.&quot;)
...
model = tf.keras.models.load_model(&#39;multi_output_model.h5&#39;)
print(&quot;model loaded&quot;)
...

VSC 终端：
(classificationenv) C:\Users\Lenovo\Documents\classification&gt;python certification.py
2024-07-22 12:29:17.362739：我 tensorflow/core/util/port.cc:153] oneDNN 自定义操作已开启。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 `TF_ENABLE_ONEDNN_OPTS=0`。
2024-07-22 12:29:19.621902：我 tensorflow/core/util/port.cc:153] oneDNN 自定义操作已开启。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 `TF_ENABLE_ONEDNN_OPTS=0`。
找到 CPU：[PhysicalDevice(name=&#39;/physical_device:CPU:0&#39;, device_type=&#39;CPU&#39;)]
2024-07-22 12:29:26.002626：I tensorflow/core/platform/cpu_feature_guard.cc:210] 此 TensorFlow 二进制文件经过优化，可在性能关键型操作中使用可用的 CPU 指令。
要启用以下指令：AVX2 AVX512F AVX512_VNNI FMA，在其他操作中，请使用适当的编译器标志重建 TensorFlow。

模型来源：https://www.kaggle.com/code/stdntlfe/fashion-styles-final/notebook
]]></description>
      <guid>https://stackoverflow.com/questions/78776878/model-taking-forever-to-load-tf-keras-models-load-model</guid>
      <pubDate>Mon, 22 Jul 2024 03:43:36 GMT</pubDate>
    </item>
    <item>
      <title>使用python图像去噪没有得到想要的重建图像</title>
      <link>https://stackoverflow.com/questions/78776540/not-getting-desired-reconstructed-image-with-python-image-denoising</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78776540/not-getting-desired-reconstructed-image-with-python-image-denoising</guid>
      <pubDate>Sun, 21 Jul 2024 23:19:13 GMT</pubDate>
    </item>
    <item>
      <title>目标文件的对称拟合问题</title>
      <link>https://stackoverflow.com/questions/78776322/symmetry-fitting-problem-with-target-file</link>
      <description><![CDATA[我正在构建一个有 6 列和 6700 行的数据集。该数据是从不同研究的各种蒙特卡罗模拟中提取的光子剂量转换系数获得的。数据中的列包括能量、器官名称、器官质量、器官密度、AP 剂量、PA 剂量和横向剂量。能量行表示从 1keV 到 20 MeV 的每个能量箱计算出的剂量，分为 20 个箱。每个箱重复器官名称、器官质量和器官密度。
如果我们排除器官名称，数据可用于数值拟合。但是，如果我们使用独热编码器对器官名称进行编码，则可以更有效地使用数据，并且交叉验证比仅有数值数据显示出更好的结果。不同器官的剂量取自不同的幻像，其中一些幻像比其他幻像拥有更多的器官。使用独热编码器后，合并的器官总数为 32。
当尝试从新的模型中预测器官时，问题就出现了，因为模型显示输入和目标文件之间存在对称性错误。我的问题是，我们如何解决这个问题？
如果我使用一个带有 1000 个箱子的模型来表示 20 个器官，并预测另一个带有 1000 个箱子的模型，其中 20 个器官完全相同，但器官质量和密度不同，则该文件有效
如何使用分类整体数据（来自 32 个器官的 6700 个箱子来预测（32 个器官的 1000 个箱子）？]]></description>
      <guid>https://stackoverflow.com/questions/78776322/symmetry-fitting-problem-with-target-file</guid>
      <pubDate>Sun, 21 Jul 2024 20:47:32 GMT</pubDate>
    </item>
    <item>
      <title>我想使用基于 PCA 的人脸识别技术对 Yaledatabase 进行识别，并使用留一交叉验证法测量识别率</title>
      <link>https://stackoverflow.com/questions/78775565/i-want-to-use-pca-based-face-recognition-for-yaledatabase-and-measure-the-recogn</link>
      <description><![CDATA[我不确定增加特征向量的数量却没有看到识别率发生明显变化有什么问题。我使用的 yaledatabase 总共有 15 组，我通过对每组留一然后除以总数据集来获得识别率，但我不确定哪里出了问题……
import numpy as np
import cv2
import os
from numpy import linalg as LA
def read_images(path):
images = []
filenames = []
for root, dirs, files in os.walk(path):
for index, file in enumerate(files):
if(file.endswith(&#39;.gif&#39;)):
img_path = os.path.join(root, file)
img = loadImageFromPath(img_path)
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img_resized = cv2.resize(img_gray, (20,20))
img_normalized = img_resized / 255.0 
images.append((img_normalized, index))
filenames.append(file)
else:
img_path = os.path.join(root, file)
img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
if img_gray 不为 None:
img_resized = cv2.resize(img_gray, (20,20))
img_normalized = img_resized / 255.0 
images.append((img_normalized, index))
filenames.append(file)
返回图像、文件名
def loadImageFromPath(imgPath):
try:
if str(imgPath).lower().endswith(&#39;.gif&#39;):
gif = cv2.VideoCapture(imgPath)
ret, frame = gif.read() # 如果找到帧则 ret=True，否则为 False。
if ret:
返回帧
else:
返回 cv2.imread(imgPath,cv2.IMREAD_GRAYSCALE)
除 Exception 外，因为 e:
print(e)
返回 None

def image_as_row(x):
返回 x.flatten()

def covariance(m):
返回 np.cov(m, rowvar=False) 

def eigenvector(m, k):
w, v = np.linalg.eig(m)
idx = np.argsort(w)[::-1][:k] 
返回 v[:, idx] 

def im_map(image, mean, mv):
new = image_as_row(image)
new = np.subtract(new, mean)
返回 np.dot(new, mv)

def Euclidean_distance(v1, v2):
返回np.sqrt(np.sum(np.power((v1 - v2), 2)))

def find_similar(image, tagged):
distances = [Euclidean_distance(image, m) for m in tagged]
return np.argmin(distances)

def leave_one_out(images, k):
if len(images) == 0:
print(&quot;Error: 没有要处理的图像在 leave_one_out&quot;)
return 0
correct_predictions = 0
n = len(images)

for leave_out_index in range(n):
test_image, test_image_index = images[leave_out_index]
train_images = images[:leave_out_index] + images[leave_out_index + 1:]

vector = np.array([image_as_row(image[0]) for image in train_images])
mean_train = vector.mean(axis=0)
diff = np.subtract(vector, mean_train)
cov = covariance(diff) / len(train_images)
mv = eigenvector(cov, k)

mapped_train = np.dot(diff, mv)
new_image = im_map(test_image, mean_train, mv)

index = find_similar(new_image,mapped_train)
print(f&quot;测试图像 {test_image_index} 与训练图像 {index} 最相似&quot;)

if leave_out_index == index:
correct_predictions += 1

recognition_rate = correct_predictions / n
return identification_rate

def main():
base_path = &#39;Yaledatabase_full/data&#39;
num_sets = 10
images_per_set = 10
k = 10 # Number主成分

all_recognition_rates = []
for set_index in range(1, num_sets + 1):
set_path = os.path.join(base_path, f&quot;{set_index:01d}&quot;)
images, filenames = read_images(set_path)

print(f&quot;Images in set {set_index}:&quot;)
for idx, filename in enumerate(filenames):
print(f&quot;Index: {idx}, Filename: {filename}&quot;)

recognition_rate = leave_one_out(images, k)
all_recognition_rates.append(recognition_rate)
print(f&#39;Recognition rate for set {set_index}: {recognition_rate * 100:.2f}%&#39;)
print(&#39;---------------------------------------------&#39;)

overall_recognition_rate = np.mean(all_recognition_rates)
print(&#39;---------------------------------------------&#39;)
print(f&#39;总体识别率为 {overall_recognition_rate * 100:.2f}%&#39;)

if __name__ == &quot;__main__&quot;:
main()

我想知道，当有 100 个特征向量时，基于 PCA 技术的人脸识别的识别率是否约为 40%，如各种论文所示。]]></description>
      <guid>https://stackoverflow.com/questions/78775565/i-want-to-use-pca-based-face-recognition-for-yaledatabase-and-measure-the-recogn</guid>
      <pubDate>Sun, 21 Jul 2024 15:13:11 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习方法检测玩具车中的未知缺陷</title>
      <link>https://stackoverflow.com/questions/78773870/detecting-unknown-defects-in-toy-car-using-deep-learning-methods</link>
      <description><![CDATA[我们需要从玩具车图片中判断汽车是否有缺陷。我们没有缺陷汽车的图片，我们无法提前知道这些汽车可能存在哪些缺陷。
我们可以使用各种技巧拍摄数千张完好无损的汽车照片。此外，我们还可以自动拍摄数千/数万张汽车 CAD 图像的快照。
我尝试过的方法：

在传统方法中，我尝试过 OpenCV，但它对所有事物都过于敏感，并且只能应用于以完全相同方式拍摄的两张照片。 :(

Siamese Network 和 Sentence Transformers（余弦相似度等...）也不适合，因为它们对摄影产生的差异很敏感。 :(

Autoencoder 有两种不同的方法：

3.1. 我用基于 CAD 的快照训练了一个自动编码器，然后在真实的有缺陷和无缺陷的图像上对其进行了测试，根据这些重建误差的分布从两个方向切断异常值。这个想法来自这里：https://github.com/sohamk10/Image-reconstruction-and-Anomaly-detection。不是很好解决方案。

3.2. 我用原始照片（大约 10,000 张照片，仅从一个视角拍摄）创建了一个自动编码器模型，其中解码器部分是从 ResNet50 拍摄（并冻结）的，我只训练了编码器部分。我也尝试了其他模型（MobileNetV3、EfficientNet-B3）。结果非常令人满意：它根据重建误差独立于照片环境识别训练过的对象，但它也将那些有轻微缺陷的对象识别为好对象，而它不应该这样做。



YOLO v7：产生与上一点（3.2.）中提到的自动编码器类似的结果。因此，它识别了物体，但不幸的是，它也会识别有缺陷的物体。


如何使用人工智能甚至不使用人工智能来解决这个业余项目问题？
如何修改第 3.2 点中提到的自动编码器。在训练期间提供较低的 loss 和 val_loss 值（目前约为 0.6，但在 MNIST 数据集上为 0.02...）？]]></description>
      <guid>https://stackoverflow.com/questions/78773870/detecting-unknown-defects-in-toy-car-using-deep-learning-methods</guid>
      <pubDate>Sat, 20 Jul 2024 20:36:56 GMT</pubDate>
    </item>
    <item>
      <title>为什么我通过 Mediapipe Model Maker 训练制作的自定义模型会将人检测为我的对象？[关闭]</title>
      <link>https://stackoverflow.com/questions/78773679/why-does-my-custom-model-made-by-training-through-mediapipe-model-maker-detect-p</link>
      <description><![CDATA[我有一个想要检测的软玩具。我使用 Mediapipe Model Maker 对大约 100 张图像进行了训练。结果还不错。该模型大多数时候都能从正面识别我的玩具。但问题是，它似乎认为任何人（人的侧面或正面视图）都是我的玩具，而且比例很高（例如 &gt;90%）。
我不明白为什么会发生这种情况，我该怎么做才能微调我的模型，以便它不会将人检测为我的玩具。我已将我的玩具和检测示例的图片附在下面。


共享使用的代码 -
我的所有图片都包括不同背景和灯光下的 shiva 玩具。对于代码，我使用了其官方页面上提供的 MediaPipe 模型制作器代码示例，以下是链接和代码

https://ai.google.dev/edge/mediapipe/solutions/customization/object_detector?utm_source=youtube&amp;utm_medium=video&amp;utm_campaign=MPRP
在此处输入图片描述

]]></description>
      <guid>https://stackoverflow.com/questions/78773679/why-does-my-custom-model-made-by-training-through-mediapipe-model-maker-detect-p</guid>
      <pubDate>Sat, 20 Jul 2024 19:04:20 GMT</pubDate>
    </item>
    <item>
      <title>如何保存这个 RNN 模型以及如何使用它来构建翻译 api？</title>
      <link>https://stackoverflow.com/questions/78773642/how-to-save-this-rnn-model-and-how-use-this-to-build-api-to-translation</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78773642/how-to-save-this-rnn-model-and-how-use-this-to-build-api-to-translation</guid>
      <pubDate>Sat, 20 Jul 2024 18:50:20 GMT</pubDate>
    </item>
    <item>
      <title>将三个经过训练的二分类模型组合成 Keras 中的单个多分类模型</title>
      <link>https://stackoverflow.com/questions/78773489/combine-three-trained-binary-classification-models-into-single-multiclassificati</link>
      <description><![CDATA[我有三个经过训练的二分类模型，它们在输出层使用 Sigmoid 激活函数进行训练。

第一个模型返回从 0 到 1 的概率标量，以检查图像是否为数字 ZERO。
第二个模型返回从 0 到 1 的概率标量，以检查图像是否为数字 ONE。
第三个模型返回从 0 到 1 的概率标量，以检查图像是否为数字 TWO。


我知道我可以使用 softmax 训练它们，在输出层构建三个神经元的模型。但假设我遇到一种情况，由于模型复杂，训练它们的权重确实需要很长时间，我只有它们各自的二分类模型。或者，我想提取它们在隐藏层的隐藏表示特征，例如 model_0（二分类检查图像是否为零）。
那么，如何将它们连接/组合/合并为单个模型？
我的代码目前卡在了这一点：
model_0 = init_binary_classification_model((28,28))
model_0.load_weights(&#39;trained_weight_of_binary_classification_to_check_whether_image_is_zero.h5&#39;)

model_1 = init_binary_classification_model((28,28))
model_1.load_weights(&#39;trained_weight_of_binary_classification_to_check_whether_image_is_one.h5&#39;)

model_2 = init_binary_classification_model((28,28))
model_2.load_weights(&#39;trained_weight_of_binary_classification_to_check_whether_image_is_two.h5&#39;)

其中：
def init_binary_classification_model(input_shape=(28,28)):
input_layer = Input(shape=input_shape)
tensor = Flatten()(input_layer)
tensor = Dense(16,activation=&#39;relu&#39;)(tensor)
tensor = Dense(8,activation=&#39;relu&#39;)(tensor)
output_layer = Dense(1,activation=&#39;sigmoid&#39;)(tensor)

return Model(inputs=input_layer,outputs=output_layer)

我期望多分类模型具有相同的输入形状(28,28)和不同的输出形状(3)，并且我不需要重新训练模型（如果可能的话）。
完整代码可在https://colab.research.google.com/drive/1y1mvAzebIFU_cuEQo8Q60L1I6uT8i2Ce?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/78773489/combine-three-trained-binary-classification-models-into-single-multiclassificati</guid>
      <pubDate>Sat, 20 Jul 2024 17:44:05 GMT</pubDate>
    </item>
    <item>
      <title>yolov8 在训练后不会启动冻结：扫描</title>
      <link>https://stackoverflow.com/questions/77893385/yolov8-doesn%c2%b4t-initiate-freezes-after-train-scanning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77893385/yolov8-doesn%c2%b4t-initiate-freezes-after-train-scanning</guid>
      <pubDate>Sun, 28 Jan 2024 01:33:57 GMT</pubDate>
    </item>
    <item>
      <title>是否可以将 ML 模型从 R 转换为 ONNX 格式</title>
      <link>https://stackoverflow.com/questions/77522002/is-it-possible-to-transfer-a-ml-model-from-r-into-onnx-format</link>
      <description><![CDATA[我目前正在 R 中训练 ML 模型（具体来说是使用 mlr3 框架 - 如果不行，我也愿意使用其他软件包）。稍后我想将模型应用到生产中，但为此它需要采用 ONNX 格式。我的在线研究并没有找到任何可能的解决方案，可以将任何在 R 中训练的 ML 模型转换为 ONNX 格式。这可能吗？]]></description>
      <guid>https://stackoverflow.com/questions/77522002/is-it-possible-to-transfer-a-ml-model-from-r-into-onnx-format</guid>
      <pubDate>Tue, 21 Nov 2023 10:18:59 GMT</pubDate>
    </item>
    </channel>
</rss>