<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Mon, 14 Apr 2025 21:17:27 GMT</lastBuildDate>
    <item>
      <title>不能在Colab上运行Noussalk</title>
      <link>https://stackoverflow.com/questions/79573765/cant-run-mustalk-on-colab</link>
      <description><![CDATA[ i试图在Colab上运行musetalk，然后下载了gunirments.txt文件中指定的要求，但是当我在开始时调用运行地狱函数时，我会收到一个版本不匹配错误：
 提高RuntimeError（RuntimeError：无法导入dibfusers.models.autoencoders.autoencoder_kl 
由于有以下错误（查找以查看其追溯）： 
由于以下错误，无法导入扩散器。loaders.unet 
（查找以查看其追溯）：无法从中导入名称&#39; 
“变形金刚”（/usr/local/lib/python3.11/dist-packages/transformers/init.py）
 
有人之前是否有人在Colab上编译了Musetalk，或者知道我该如何修复并找到所有兼容版本？
这是我的代码：
 ！git克隆https://github.com/tmelyralab/musetalk
％CD Musetalk

！

！ -fps 25
 
即使我尝试更改变压器和扩散器版本以传递此问题，另一个软件包会发生冲突，依此类推！]]></description>
      <guid>https://stackoverflow.com/questions/79573765/cant-run-mustalk-on-colab</guid>
      <pubDate>Mon, 14 Apr 2025 17:59:05 GMT</pubDate>
    </item>
    <item>
      <title>SVM培训和测试</title>
      <link>https://stackoverflow.com/questions/79573524/svm-training-and-test</link>
      <description><![CDATA[要将测试集应用于SVM，我使用单独的测试集。我想在没有培训的情况下将测试集应用于SVM。我写了一个简单的代码，例如遵循一个。当我将集合应用于SVM时，准确性值得，但是当我使用train_test_split函数时，精度就可以了。我的代码怎么了？
  #training数据集
f_hed = np.array（np.abs（zz_1））
f_nonhed = np.array（np.abs（zz_n1））
y1 = np.ones（f_hed.shape [0]）#label 1 for Hed
y2 = np.zeros（f_nonhed.shape [0]）#label 0 for nonthed


#combining火车套装
x = np.vstack（（f_hed，f_nonhed））
y = np.hstack（（y1，y2））


＃测试数据集  
x_test1 = np.array（np.abs（zz_x））
x_test2 = np.array（np.abs（zz_nx））
y_test1 = np.ones（x_test1.shape [0]）#label 1由于被hed
y_test2 = np.zeros（x_test2.shape [0]）#Label 0由于被hed


#combining测试集
x_test = np.vstack（（x_test2，x_test2））
y_test = np.hstack（（（y_test2，y_test2）））

#Normorization
sualer = StandardScaler（）
x_train_s = scaler.fit_transform（x）
x_test_s = scaler.transform（x_test）

#Classifier
clf = svc（kernel =&#39;rbf&#39;，c = 10）
clf.fit（x_train_s，y）

#predict部分
y_pred = clf.predict（x_test_s）


TN，FP，FN，TP = Confusion_Matrix（y_test，y_pred，labels = [0，1]）。ravel（）
打印（混乱矩阵：“）
print（f＆quot; tn：{tn}，fp：{fp}，fn：{fn}，tp：{tp}＆quort;）
 
这是我的代码的输出，我期望至少FP和FN小于5，但是，在该输出中，TP的价值为零。实际上，它应该接近22：
 混乱矩阵：
TN：24，FP：12，FN：0，TP：0
 ]]></description>
      <guid>https://stackoverflow.com/questions/79573524/svm-training-and-test</guid>
      <pubDate>Mon, 14 Apr 2025 15:40:19 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow或Pytorch可以使用这种格式的JSON时间序列数据工作吗？</title>
      <link>https://stackoverflow.com/questions/79573345/can-tensorflow-or-pytorch-work-with-json-time-series-data-in-this-format</link>
      <description><![CDATA[我正在尝试制作LSTM模型以使用这种JSON文件格式预测手语句子。该JSON文件包含每个帧和每个手的21个手动地标关节的坐标信息。（左手，右手）
这是我的JSON时间序列数据的偷偷摸摸的峰值。
  {

        &#39;frame＆quot&#39;：123，
        ＆quot“：[
            {
                ＆quot“&#39;＆quot”：&#39;左
                “地标”：[[[
                    {
                        &#39;x＆quot：0.4636201858520508，
                        ＆quot&#39;＆quot：0.3758980929851532，
                        ＆quot z＆quot;：7.529240519943414E-08，
                        ＆quot“ body_part”：“腕带”
                    }，，

   ...

                    {
                        x＆quot;：0.4639311134815216，
                        ＆quot&#39;＆quot：0.2574789524078369，
                        ＆quot z＆quot;：-0.013109659776091576，
                        ＆quot“ body_part”：“ pinky_tip”
                    }
                这是给出的
            }，，

            {
                ＆quot“：; quot”正确＆quot＆quot
                “地标”：[[[
                    {
                        x＆quot;：0.5393109321594238，
                        ＆quot&#39;＆quot;：0.6190552711486816，
                        ＆quot z＆quot;：1.0587137921902467E-07，
                        ＆quot“ body_part”：“腕带”
                    }，，
...

                    {
                        ＆quot x＆quot：0.4721616506576538，
                        ＆quot&#39;＆quot;：0.5990280508995056，
                        ＆quot z＆quot;：-0.006831812672317028，
                        ＆quot“ body_part”：“ pinky_tip”
                    }
                这是给出的
            }
        这是给出的
    }，，
 
为每个帧重复具有不同位置的坐标信息。
我仍在纠正这些JSON时间序列数据。
因此，我还没有开始为LSTM制作代码。但是，我担心我是否可以使用此时间序列的json数据。]]></description>
      <guid>https://stackoverflow.com/questions/79573345/can-tensorflow-or-pytorch-work-with-json-time-series-data-in-this-format</guid>
      <pubDate>Mon, 14 Apr 2025 14:11:22 GMT</pubDate>
    </item>
    <item>
      <title>删除“日期”列时丢失的数据，使用模型创建预测，然后读取“日期”列</title>
      <link>https://stackoverflow.com/questions/79573034/data-lost-when-removing-date-column-creating-predictions-using-a-model-and-t</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79573034/data-lost-when-removing-date-column-creating-predictions-using-a-model-and-t</guid>
      <pubDate>Mon, 14 Apr 2025 11:28:06 GMT</pubDate>
    </item>
    <item>
      <title>如何根据历史任务序列使用机器学习 / AI链接任务？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79572620/how-can-i-link-tasks-using-machine-learning-ai-based-on-historical-task-sequen</link>
      <description><![CDATA[我正在制定AI模型，以根据历史项目数据来预测工业计划任务之间的依赖关系。我有两个桌子：
任务表：
TaskID，TaskName，EquigmentType
（我正在考虑添加StartDate和端代码）
依赖项表：
predexpsortAskID，ScunstAskID，linkType 
 目标：
给出了一个新的任务列表（通常由设备类型过滤），我希望该模型暗示它们之间可能的依赖性（最终是链接类型） - 从现有数据中的历史模式中学到了。中的历史模式。
 我尝试的是： 
决策树
基本神经网络
 遇到的问题： 
随机或无关的链接
模型预测所有任务之间的依赖性
缺乏从历史数据中学到的逻辑流
我很确定我不会正确处理数据，因为我没有如何处理任务名称以识别“ ”的任务名称。
 我的问题：
将其作为图形问题并使用图形神经网络（GNN）是否有意义？还是在这种情况下进行建模和预测任务之间的依赖性的ML或统计方法更好？
我愿意就可能提高性能的模型体系结构或数据预处理策略的建议。]]></description>
      <guid>https://stackoverflow.com/questions/79572620/how-can-i-link-tasks-using-machine-learning-ai-based-on-historical-task-sequen</guid>
      <pubDate>Mon, 14 Apr 2025 07:51:11 GMT</pubDate>
    </item>
    <item>
      <title>我的神经网络试图找到懒惰的解决方案，而不是找到卑鄙的最佳解决方案，我不知道为什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79572183/my-neural-network-tries-to-find-a-lazy-solution-instead-of-the-optimal-solution</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79572183/my-neural-network-tries-to-find-a-lazy-solution-instead-of-the-optimal-solution</guid>
      <pubDate>Mon, 14 Apr 2025 00:00:34 GMT</pubDate>
    </item>
    <item>
      <title>参数“目标”和“输出”必须具有相同的等级（NDIM）。接收：target.shape =（none，），output.shape =（无，9）</title>
      <link>https://stackoverflow.com/questions/79572055/arguments-target-and-output-must-have-the-same-rank-ndim-received-target</link>
      <description><![CDATA[我一直在尝试创建一个可以识别图像的神经网络，但是当我尝试适合我的模型时，我会收到以下错误：
  value error trackback（最近的最新通话最后）
＆lt; ipython-Input-100-98095d26e04d＆gt;在＆lt;单元线：0＆gt;（）
      1＃训练模型
----＆gt; 2历史= model.fit（训练，batch_size = batch_size，epochs = epoch，verialation_data =测试）

1帧
/USR/local/lib/python3.11/dist-packages/keras/src/backend/backend/tensorflow/nn.py in cancorical_crossentropy（target，output，output，from_logits，axis）
    651）
    652如果Len（target.hape）！= len（output.hape）：
 - ＆gt; 653提高价值Error（
    654“参数”和“输出”必须具有相同的等级”。
    655“（NDIM）。收到：＆quot;

ValueError：参数“目标”和“输出”必须具有相同的等级（NDIM）。接收：target.shape =（none，），output.shape =（无，9）
 
我正在通过合作构建它，我正在使用的代码如下：
 导入numpy作为NP
导入matplotlib.pyplot作为PLT
导入TensorFlow作为TF

从Google.Colab Import Drive
从google.colab.patches导入cv2_imshow
drive.mount（&#39;/content/drive&#39;）


img_height = 180
img_width = 180
batch_size = 32
时期= 20
 
  normalization_layer = tf.keras.layers.Rescaling（1./255）
normalized_ds = triending.map（lambda x，y ：（ a rustaranization_layer（x），y​​））
image_batch，labels_batch = next（iter（normalized_ds））
first_image = image_batch [0]
打印（np.min（first_image），np.max（first_image））
 
  autotune = tf.data.autotune

训练=训练。
testing = testing.cache（）。prefetch（buffer_size = autotune）
 
 来自Tensorflow.keras.models导入顺序
来自tensorflow.keras.layers导入conv2d，maxpooling2d，扁平，密集
来自Tensorflow.keras.utils导入到_categorical

num_classes = 9

模型=顺序（[
  conv2d（32，（3，3），激活=&#39;relu&#39;），，
  maxpooling2d（2，2），
  conv2d（64，（3，3），激活=&#39;relu&#39;），，
  maxpooling2d（2，2），
  conv2d（128，（3，3），激活=&#39;relu&#39;），，
  maxpooling2d（2，2），
  flatten（），
  密集（256，激活=&#39;relu&#39;），
 密集（num_classes，激活=&#39;softmax&#39;）
）））
model.build（input_shape =（batch_size，img_height，img_width，3））
 
 ＃compilaçãodomodero
model.compile（优化器=&#39;adam&#39;， 
              损失=&#39;apcorical_crossentropy&#39;， 
              指标= [&#39;准确性&#39;]）

 
  history = model.fit（训练，batch_size = batch_size，epochs = epochs，verialation_data = testing）
 
训练和测试变量是使用创建的预取介台
  tf.keras.utils.image_dataset_from_directory
 
我如何解决此错误。]]></description>
      <guid>https://stackoverflow.com/questions/79572055/arguments-target-and-output-must-have-the-same-rank-ndim-received-target</guid>
      <pubDate>Sun, 13 Apr 2025 21:09:26 GMT</pubDate>
    </item>
    <item>
      <title>ML中的损失函数之间的差异[关闭]</title>
      <link>https://stackoverflow.com/questions/79571518/difference-between-loss-function-in-ml</link>
      <description><![CDATA[ Sparse_Categorical_CrossentRopy和centorical_crossentropy有什么区别？什么时候应该使用一个损失而不是另一个损失？例如，这些损失适合线性回归吗？]]></description>
      <guid>https://stackoverflow.com/questions/79571518/difference-between-loss-function-in-ml</guid>
      <pubDate>Sun, 13 Apr 2025 12:06:58 GMT</pubDate>
    </item>
    <item>
      <title>从本地模型[封闭]创建一个全局模型</title>
      <link>https://stackoverflow.com/questions/79571154/create-a-global-model-from-local-models</link>
      <description><![CDATA[ 当前场景： 
所以我有一项任务。我有一个具有时间戳，org_id，no_of_calls_on_premise，no_of_of_calls_cloud，bw_savings的数据。这是每天汇总的数据
（我也将时间戳分开，而不是直接将其用于诸如Day，Day_of_week，is_weekend，Quarter等各个领域））
现在，我已经训练了XGBoost，Random Forest和Sarimax模型等不同模型，它们具有不错的测试准确性。这项培训是对2024年的数据进行的。
训练后，我使用每个模型使用上述变量从所有3个模型中的所有模型中预测BW_SAVINGS，用于1月，2月和3月。现在，由于我已经是BW_SAVINGS，所以我可以从所有3个中获得最佳预测值，但是我做了很多组织的R2分数小于0。
现在，我也必须部署此模型，这里的数据将用于没有现有bw_savings的新组织，因此无法确认我的模型是否在几个月内表现出色。目前，我从上述模型中获取3个预测值的中位数。
 问题： 
因此，这里缺少的真正的是，这些模型正在同时培训组织的所有数据，这导致缺失趋势和不同的组织具有不同的趋势。即使我们离开趋势，每个组织的呼叫规模也会有所不同。这似乎是一个问题，因为R2得分为负。提高我的算法的任何建议。类似它为每个组织训练一个一个org，并将单个培训结合到全球模型中，以便它可以掌握每个组织中不同尺度的概念 ]]></description>
      <guid>https://stackoverflow.com/questions/79571154/create-a-global-model-from-local-models</guid>
      <pubDate>Sun, 13 Apr 2025 02:45:29 GMT</pubDate>
    </item>
    <item>
      <title>简单线性回归模型的剩余分析</title>
      <link>https://stackoverflow.com/questions/79571067/residual-analysis-for-simple-linear-regression-model</link>
      <description><![CDATA[我正在尝试进行简单线性回归的残差分析。我需要证明残差遵循近似正态分布。
我正在使用的CSV文件具有10年级分数百分比的值，而学生的薪水是
运行以下代码后，我的情节看起来像这样：
  
书中的情节看起来像这样：
 我期望我的情节像书一样出现，因为数据是相同的。我已经进行了仔细检查以确保我不会丢失任何数据等。
 数据如下：

薪水百分比
62 270000
76.33 200000
72 240000
60 250000
61 180000
55 300000
70 260000
68 235000
82.8 425000
59 240000
58 250000
60 180000
66 428000
83 450000
68 300000
37.33 240000
79 252000
68.4 280000
70 231000
59 224000
63 120000
50 260000
69 300000
52 120000
49 120000
64.6 250000
50 180000
74 218000
58 360000
67 150000
75 250000
60 200000
55 300000
78 330000
50.08 265000
56 340000
68 177600
52 236000
54 265000
52 200000
76 393000
64.8 360000
74.4 300000
74.5 250000
73.5 360000
57.58 180000
68 180000
69 270000
66 240000
60.8 300000
 
代码如下：
 ＃导入所有必需的库以构建回归模型
导入熊猫作为pd导入numpy作为np
导入statsmodels.api作为sm
来自sklearn.model_selection导入train_test_split
导入matplotlib.pyplot作为PLT

＃将数据集加载到数据框中
mba_salary_df = pd.read_csv（&#39;MBA Salary.csv&#39;）

＃将1的常数项添加到数据集
x = sm.add_constant（MBA_SALARY_DF [&#39;10年级的百分比]）
Y = MBA_SALARY_DF [&#39;SALERARY&#39;]

＃分别将数据集分别为火车和测试设置分别为80:20
train_x，test_x，train_y，test_y = train_test_split（x，y，train_size = 0.8，andural_state = 100）

＃适合回归模型
mba_salary_lm = sm.ols（train_y，train_x）.fit（）

mba_salary_resid = mba_salary_lm.resid 

probplot = sm.probplot（mba_salary_resid） 

plt.figure（无花果=（8，6）） 

probplot.ppplot（line = &#39;45&#39;） 

plt.title（回归标准化残差的正常p-p图；） 

plt.show（）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79571067/residual-analysis-for-simple-linear-regression-model</guid>
      <pubDate>Sun, 13 Apr 2025 00:11:50 GMT</pubDate>
    </item>
    <item>
      <title>如何从传感器数据中正确实现滑动窗口以实时活动识别？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79570885/how-to-properly-implement-sliding-windows-for-real-time-activity-recognition-fro</link>
      <description><![CDATA[问题摘要
我们使用可穿戴设备（加速度计 +陀螺仪）的实时数据部署了训练有素的ML模型，以供秋季检测。该模型在20秒的数据窗口上进行了训练，该数据以80 Hz采样（每个窗口1600个样本）。在部署中，我们的传感器将80个样本/秒发送到firebase，而ML代码：

每秒民意调查火箱以获取最新的80样品批次。
将样品附加到1600尺寸的Deque（滑动缓冲区）。
触发缓冲区已满时的预测，而step_interval（5秒）已通过。

尽管如此，我们的预测通常是不准确的。我们怀疑我们处理缓冲和窗户的方式可能与模型的培训期望不符。
主要问题
当传感器数据以固定的间隔流传输时，实现实时滑动窗口的正确方法是什么？

我们应该使用带有固定步幅的固定尺寸窗口（例如一次滑动80个样本）？
在此中使用step_interval基于时间的逻辑是一种不良练习
上下文？
使用20秒窗口（1600个样本）实时时间太长
检测，一个较短的窗口（例如5或10秒）可以提供更快或更实际的预测而不牺牲准确性？

这是一个简化的片段，说明了我们实施的相关部分：
  buffer_size = 1600＃20 sec窗口80 Hz
step_interval = 5＃每5秒预测一次
data_buffer = deque（maxlen = buffer_size）
last_prediction_time = time.time（）

而真：
    样本= fetch_from_firebase（）＃获取新数据批次（每秒80个样本）

    对于样本中的样本：
        ＃归一化并将每个传感器样品附加到缓冲区
        读取= np.array（[
            示例[&#39;ax&#39;]，样本[&#39;ay&#39;]，样本[&#39;az&#39;]，
            样本[&#39;gx&#39;]，样本[&#39;gy&#39;]，样品[&#39;gz&#39;]
        ]） / 10000.0
        data_buffer.append（读取）

    ＃预测逻辑（缓冲区填充后每个step_interval秒）
    current_time = time.time（）
    如果len（data_buffer）== buffer_size和（current_time -last_prediction_time）＆gt; = step_interval：
        窗口= np.array（data_buffer）.RESHAPE（1，-1）
        window_scaled = sualer.transform（窗口）
        预测= model.predict（window_scaled）[0]

        last_prediction_time = current_time
        handing_prediction（预测）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79570885/how-to-properly-implement-sliding-windows-for-real-time-activity-recognition-fro</guid>
      <pubDate>Sat, 12 Apr 2025 19:59:33 GMT</pubDate>
    </item>
    <item>
      <title>CNN功能提取器的机器学习[关闭]</title>
      <link>https://stackoverflow.com/questions/79566803/cnn-feature-extractor-to-machine-learning</link>
      <description><![CDATA[我设计了一个简单的CNN模型来从图像中提取功能，然后在后来使用要在机器学习中训练的功能。我很困惑有两件事：是冻结模型还是提取直到致密（128 ...）。我想就最好的方法寻求建议。此外，是否建议使用多个折叠，或者仅在您执行机器学习时才应用？谢谢。
  def create_cnn_model（img_width = 256，img_height = 256）：
输入=输入（shape =（img_width，img_height，3））
x = layers.Rescaling（1./255）（输入）
x = conv2d（64，（3，3），填充=&#39;same&#39;，activation =&#39;relu&#39;）（x）
x = maxpooling2d（（3，3），填充=&#39;same&#39;）（x）
x = conv2d（32，（3，3），填充=&#39;same&#39;，activation =&#39;relu&#39;）（x）
x = maxpooling2d（（3，3），填充=&#39;same&#39;）（x）
x = flatten（）（x）
x =密集（128，激活=&#39;relu&#39;）（x）
x =辍学（0.5）（x）
x =密集（1，激活=&#39;sigmoid&#39;）（x）

模型=模型（输入=输入，输出= x）
返回模型

feature_extractor = model（inputs = model.input，
                          输出= model.layers [-2] .output）
 ]]></description>
      <guid>https://stackoverflow.com/questions/79566803/cnn-feature-extractor-to-machine-learning</guid>
      <pubDate>Thu, 10 Apr 2025 13:36:42 GMT</pubDate>
    </item>
    <item>
      <title>模块“ keras.layers”没有属性“实验”</title>
      <link>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</link>
      <description><![CDATA[您好，我试图调整大小和重新列出我的数据集，如下所示，但我遇到了此错误：
 attributeError：模块&#39;keras.layers&#39;没有属性&#39;实验&#39;
 
resize_and_rescale = tf.keras.Sequeential（[
    layers.experiment.preprocessing.resizing（image_size，image_size），
    layers.expermentim.preprocessing.Rescaling（1.0/255）
）））

 ]]></description>
      <guid>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</guid>
      <pubDate>Wed, 14 Dec 2022 00:43:49 GMT</pubDate>
    </item>
    <item>
      <title>如何改善Levenberg-Marquardt的多项式曲线拟合方法？</title>
      <link>https://stackoverflow.com/questions/62231658/how-to-improve-levenberg-marquardts-method-for-polynomial-curve-fitting</link>
      <description><![CDATA[几周前，我开始从Matlab的Scratch中编码Levenberg-Marquardt算法。我对数据的多项式拟合感兴趣，但是我无法达到我想要的准确性水平。尝试其他多项式后，我正在使用第五阶多项式，这似乎是最好的选择。无论我尝试实现哪些改进，该算法始终会收敛到相同的功能最小化。到目前为止，我未能添加以下功能：

测量加速度作为二阶校正
延迟满足阻尼参数
增益因素以靠近高斯 - 纽顿方向或
最陡峭的下降方向取决于迭代。
有限差异方法的主要差异和正向差异

我没有非线性最小二乘的经验，所以我不知道是否有一种方法可以最大程度地减少剩余的方法，或者使用这种方法没有更多的改进空间。我将在最后一次迭代的多项式行为的图像下附加。如果我运行代码以进行更多迭代，则曲线最终不会从迭代变为迭代。正如观察到的那样，从时间= 0到时间= 12。
  ]]></description>
      <guid>https://stackoverflow.com/questions/62231658/how-to-improve-levenberg-marquardts-method-for-polynomial-curve-fitting</guid>
      <pubDate>Sat, 06 Jun 2020 12:22:40 GMT</pubDate>
    </item>
    <item>
      <title>批归一化，是还是否？ [关闭]</title>
      <link>https://stackoverflow.com/questions/58612783/batch-normalization-yes-or-no</link>
      <description><![CDATA[我使用TensorFlow 1.14.0和Keras 2.2.4。以下代码实现了一个简单的神经网络：
 导入numpy作为NP
np.random.seed（1）
导入随机
随机。种子（2）
导入TensorFlow作为TF
tf.set_random_seed（3）

来自Tensorflow.keras.models导入模型，顺序
来自tensorflow.keras.layers导入输入，密集，激活


x_train = np.random.normal（0,1，（100,12））

型号=顺序（）
ADD（密集（8，Input_Shape =（12，））））
＃model.add（tf.keras.layers.batchnormalization（））
ADD（激活（&#39;Linear&#39;））
型号（密集（12））
ADD（激活（&#39;Linear&#39;））
model.compile（优化器=&#39;adam&#39;，loss =&#39;mean_squared_error&#39;）
model.fit（x_train，x_train，epochs = 20，验证_split = 0.1，shuffle = false，derbose = 2）
 
 20个时期后的最终val_loss为0.7751。当我删除添加批处理标准化层的唯一评论行时，val_loss更改为1.1230。
我的主要问题更为复杂，但是发生同样的事情。由于我的激活是线性的，因此是否在激活之后或之前将批处理归一化都无关紧要。 
 问题：为什么批处理归一化无济于事？我有什么可以更改的，以便批处理归一化可以改善结果而不更改激活功能？
 发表评论后更新： 
一个带有一个隐藏层和线性激活的NN有点像PCA。有很多论文。对我来说，此设置在隐藏层和输出的激活函数的所有组合中给出了最小的MSE。
一些陈述线性激活的资源是指PCA：
  https://arxiv.org/pdf/pdf/1702.07800.pdf    
   https：////www.quora.com/www.quora.com/www.quora.com/www.quora/]]></description>
      <guid>https://stackoverflow.com/questions/58612783/batch-normalization-yes-or-no</guid>
      <pubDate>Tue, 29 Oct 2019 17:38:38 GMT</pubDate>
    </item>
    </channel>
</rss>