<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 10 Apr 2024 09:15:58 GMT</lastBuildDate>
    <item>
      <title>解决这个问题的最佳方法是什么？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78302669/what-would-be-the-best-approach-to-this-problem</link>
      <description><![CDATA[我有一些财务数据。它是很长一段时间内记录的人们账户和其他财务详细信息的列表。目前有一条规定，两年后，如果账户为空，加上一些其他条件，债务就会被消除。
为了节省系统负载，我们正在尝试找到一个最佳时间来缩短该时间段，同时将损失降至最低。
例如：
- 这家伙欠债了
- 两年过去了，那家伙几乎什么也没归还
- 将 Guy 从系统中删除以节省处理成本

我正在尝试找出最佳取消时间的方法，同时不损失潜在的资金（因为我们假设在 X 时间后无论如何都不会偿还债务）
我考虑了不同的机器学习算法，但似乎我最终总是进行特征提取而不是某种分析预测。谁能帮忙推荐一个适合这个问题的机器学习算法？
我尝试了随机森林算法，xgboost。
这更多的是特征提取和数据分析的结果。]]></description>
      <guid>https://stackoverflow.com/questions/78302669/what-would-be-the-best-approach-to-this-problem</guid>
      <pubDate>Wed, 10 Apr 2024 07:10:42 GMT</pubDate>
    </item>
    <item>
      <title>在Python中计算候选句子和参考句子之间的BLEU分数</title>
      <link>https://stackoverflow.com/questions/78302444/calculating-bleu-score-between-candidate-and-reference-sentences-in-python</link>
      <description><![CDATA[我正在计算 2 个句子之间的 BLEU 分数，这两个句子看起来与我非常相似，但我得到的 BLEU 分数非常低。这应该发生吗？
预测=“我是ABC。”
参考=“我是ABC。”

从nltk.translate.bleu_score导入sent_bleu，corpus_bleu
从 nltk.translate.bleu_score 导入 SmoothingFunction
# 对句子进行标记
Prediction_tokens = Prediction.split()
Reference_tokens = Reference.split()
   
# 计算 BLEU 分数
bleu_score=sentence_bleu([reference_tokens],prediction_tokens,smoothing_function=SmoothingFunction().method4)

# 打印 BLEU 分数
print(f&quot;BLEU 分数: {bleu_score:.4f}&quot;)

输出为 0.0725
]]></description>
      <guid>https://stackoverflow.com/questions/78302444/calculating-bleu-score-between-candidate-and-reference-sentences-in-python</guid>
      <pubDate>Wed, 10 Apr 2024 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>如何在具有连续标签的数据集上评估聚类算法的性能并优化超参数？</title>
      <link>https://stackoverflow.com/questions/78302382/how-to-evaluate-performance-and-optimize-hyperparameters-for-clustering-algorith</link>
      <description><![CDATA[我正在研究一个聚类问题，其中我的数据集的标签是连续的数值，而不是离散的类别。我使用 t-SNE 和 UMAP 将数据集特征的维数降低为二维，然后在散点图上可视化点，并根据其标签值对点进行着色。
我的目标是查看特征表示是否准确反映标签值的变化，即具有相似标签值的点是否在 2D 空间中聚集在一起。或者，特定的数据点不需要聚集在一起。只需要观察某些趋势，即数据点的分布随着标签值的增加或减少而按规则模式移动。
我正在调整 t-SNE 和 UMAP 的各种超参数来实现这一目标。到目前为止，我一直通过目视检查散点图是否存在任何模式或趋势来评估结果。这是我生成的绘图示例：
UMAP聚类结果图示例，供参考
有人可以提出方法或指标来定量评估 t-SNE 和 UMAP 执行的聚类反映标签连续性的程度吗？任何有关如何客观地确定这些方法的最佳超参数设置的建议将不胜感激。
如果您需要更多信息，请告诉我，谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78302382/how-to-evaluate-performance-and-optimize-hyperparameters-for-clustering-algorith</guid>
      <pubDate>Wed, 10 Apr 2024 06:03:29 GMT</pubDate>
    </item>
    <item>
      <title>聚类解释</title>
      <link>https://stackoverflow.com/questions/78302316/clustering-explaination</link>
      <description><![CDATA[我想以描述的形式总结我创建的集群，根据每个集群与其他集群的不同之处来定义每个集群。
输出应该是摘要的形式]]></description>
      <guid>https://stackoverflow.com/questions/78302316/clustering-explaination</guid>
      <pubDate>Wed, 10 Apr 2024 05:45:21 GMT</pubDate>
    </item>
    <item>
      <title>本文的正确比例是多少：梯度下降学习单隐藏层 CNN</title>
      <link>https://stackoverflow.com/questions/78302104/what-is-the-correct-ratio-of-this-paper-gradient-descent-learns-one-hidden-laye</link>
      <description><![CDATA[我正在尝试重新创建这篇论文中的实验。我能够完成初始化条件和所有内容，但是，我陷入了表 1（附有屏幕截图（ https://i.stack.imgur.com/wNuFH.png）。配给术语似乎令人困惑，因为它们有时使用 a，有时使用 a*。看看定理 4.3。定理4.3。我不确定哪个比例是正确的。只是看看理论我的理解是 $ $\frac{{(1^T a^)^2}}{{|a^|_2^2}}$$
鉴于表中的概念和定理是正确的，如何获得满足此条件的向量 a*
我试图找到一个满足 $$\frac{{(1^T a^)^2}}{{|a^|_2^ 等条件的向量 a* 2}}=4$$]]></description>
      <guid>https://stackoverflow.com/questions/78302104/what-is-the-correct-ratio-of-this-paper-gradient-descent-learns-one-hidden-laye</guid>
      <pubDate>Wed, 10 Apr 2024 04:21:24 GMT</pubDate>
    </item>
    <item>
      <title>用于大规模服务器管理的人工智能自动化[关闭]</title>
      <link>https://stackoverflow.com/questions/78301638/artificial-intelligence-enabled-automation-for-large-scale-server-management</link>
      <description><![CDATA[简介：在当今世界，有效管理企业的服务器基础设施可能既复杂又耗时。跟踪和管理数千台服务器在不同团队中的分布情况以及负责人通常会给管理员带来一项具有挑战性的任务。然而，人工智能 (AI) 和机器学习 (ML) 技术的发展为克服这些挑战提供了新的创新解决方案。
人工智能服务器管理：人工智能服务器管理可以作为有效管理大规模服务器基础设施的强大工具。这种方法涉及利用人工智能模型来确定哪些团队使用服务器以及谁负责它们。例如，可以创建包含服务器的 IP 地址、位置、功能和其他相关详细信息等信息的数据集。
通过机器学习自动化服务器部署：机器学习算法可以利用此数据集来预测哪些团队使用服务器。例如，可以开发基于特定服务器特征的分类模型。利用该模型可以预测服务器所属的团队，为管理员优化服务器部署提供指导。
安全和数据隐私：在支持人工智能的服务器管理应用程序中，安全和数据隐私是至关重要的问题。因此，必须实施安全措施来保护用户身份和密码等敏感信息。例如，服务器访问信息应安全存储，并且只有授权用户才能访问。
结论和建议：支持人工智能的服务器管理可以成为有效管理大规模服务器基础设施的有效工具。机器学习模型的利用可以在确定使用服务器的团队和优化服务器部署方面发挥重要作用。但是，始终考虑安全和数据隐私问题并实施适当的安全措施至关重要。
此时，在人工智能服务器管理领域需要进一步的研究和开发。然而，利用现有技术和方法，高效、安全地管理大规模服务器基础设施是可以实现的。
**综上所述，考虑到上述几点，我如何通过人工智能确定我的服务器（Red Hat、Solaris、Ubuntu、Debian 等）分配给哪个团队？注意：所有服务器都有可用的 root 用户和密码信息，我可以通过 SSH 连接。 **]]></description>
      <guid>https://stackoverflow.com/questions/78301638/artificial-intelligence-enabled-automation-for-large-scale-server-management</guid>
      <pubDate>Wed, 10 Apr 2024 00:21:07 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：“模型”层的输入 0 与该层不兼容：预期形状=（无，64，32，1），发现形状=（32，32，1）</title>
      <link>https://stackoverflow.com/questions/78301623/valueerror-input-0-of-layer-model-is-incompatible-with-the-layer-expected-sh</link>
      <description><![CDATA[我尝试调试并添加打印语句，令我惊讶的是它的形状是正确的，但程序说它的形状不正确
形状：（无、64、32、1）
型号：“型号”
&lt;小时/&gt;
层（类型）输出形状参数#
图像（输入层）[(无, 64, 32, 1)] 0
Conv1（Conv2D）（无、64、32、32）320
pool1 (MaxPooling2D)（无、32、16、32）0
batch_normalization (BatchN (无, 32, 16, 32) 128
正规化）
重塑（重塑）（无、32、512）0
dense2（密集）（无、32、16）8208
batch_normalization_1（Batc（无、32、16）64
h归一化）
双向（Bidirectiona（无、32、256）148480
l)
dense3（密集）（无、32、42）10794
================================================== =================
总参数：167,994
可训练参数：167,898
不可训练参数：96
&lt;小时/&gt;
无
输入形状：(64,32,1)
回溯（最近一次调用最后一次）：
文件“D:\Arabic-Handwriting-OCR\Arabic-Handwriting-OCR\inference.py”，第 156 行，位于
preds = Prediction_model.predict(batch_images)
文件“C:\Users\User\miniconda3\envs\tf\lib\site-packages\keras\utils\traceback_utils.py”，第 70 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
文件“C:\Users\User\AppData\Local\Temp_autograph_ generated_file42woagrz.py”，第 15 行，位于 tf__predict_function 中
retval = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
ValueError：在用户代码中：ValueError：层“模型”的输入 0与图层不兼容：预期形状=(无, 64, 32, 1)，发现形状=(32, 32, 1)
我尝试检查模型输入的形状，它的字面意思是 (64, 32, 1)，但回溯显示找到的形状=(32,32,1)，为什么呢？我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78301623/valueerror-input-0-of-layer-model-is-incompatible-with-the-layer-expected-sh</guid>
      <pubDate>Wed, 10 Apr 2024 00:15:22 GMT</pubDate>
    </item>
    <item>
      <title>努力开发使用带有 ARFRegressor 算法的 River 库的在线学习代码</title>
      <link>https://stackoverflow.com/questions/78298486/struggling-with-developing-code-for-an-online-learning-using-river-library-with</link>
      <description><![CDATA[我已经分割了数据，但正在努力编写用于训练模型的代码，并使用 pickle 将模型保存在特定目录中以供将来使用。关于如何继续的任何建议？
# 将数据拆分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

从河流进口评估
从河流进口森林
来自河流的进口指标
来自河流的进口预处理

型号=（
    预处理.StandardScaler() |
    森林.ARFRegressor(种子=42)
）
指标 = 指标.MAE()

# 在测试集上评估模型
mae = evaluate.progressive_val_score(X_test, y_test, 模型, 指标)
print(f&#39;测试集上的 MAE: {mae}&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/78298486/struggling-with-developing-code-for-an-online-learning-using-river-library-with</guid>
      <pubDate>Tue, 09 Apr 2024 12:35:37 GMT</pubDate>
    </item>
    <item>
      <title>TensorBoard HParams 未显示超参数调整的准确性指标</title>
      <link>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</link>
      <description><![CDATA[我正在 TensorFlow 中进行超参数调整，并使用 TensorBoard 中的 HParams 插件设置了一个实验来记录不同的配置。我的模型正在使用 dropout 和学习率的变化进行训练，并且我正在记录这些参数以及模型的准确性。但是，当我打开 TensorBoard 并导航到 HParams 仪表板时，不会显示与每个试验相关的准确性指标。该表正确显示了超参数，但“准确性”列为空，即使我的代码使用“准确性”作为指标来编译模型并使用 hp.KerasCallback 进行日志记录。我已经验证模型训练是否正确，并且标量仪表板等其他 TensorBoard 功能显示了各个时期的准确性趋势。我正在寻求帮助来理解为什么 HParams 表中没有显示准​​确性以及如何解决此问题。
我使用 TensorBoard 的 HParams 进行超参数调整的代码：
从tensorboard.plugins.hparams导入api作为hp
将张量流导入为 tf
从tensorflow.keras.layers导入Conv2D、MaxPooling2D、Dense、Flatten、Dropout

# 定义超参数
HP_DROPOUT = hp.HParam(&#39;dropout&#39;, hp.Discrete([0.2, 0.3, 0.4]))
HP_LEARNING_RATE = hp.HParam(&#39;learning_rate&#39;, hp.Discrete([1e-2, 1e-3]))

# 设置日志记录
log_dir = &#39;./tensorboard/nn_1&#39;
使用 tf.summary.create_file_writer(log_dir).as_default()：
    hp.hparams_config(
        hparams=[HP_DROPOUT, HP_LEARNING_RATE],
        指标=[hp.Metric(&#39;准确度&#39;,display_name=&#39;准确度&#39;)]
    ）

# 训练函数
def train_test_model(hparams, session_num):
    model_name = f“model_1_session_{session_num}”
    print(f&quot;使用超参数 {hparams} 训练 {model_name}...&quot;)
    模型 = tf.keras.Sequential([
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        Conv2D(32, kernel_size=(3, 3), 激活=&#39;elu&#39;),
        辍学（hparams [HP_DROPOUT]），
        MaxPooling2D(pool_size=(2, 2)),
        展平（），
        密集（10，激活=&#39;softmax&#39;）
    ]）
    模型.编译(
        损失=&#39;分类交叉熵&#39;，
        优化器=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),
        指标=[&#39;准确性&#39;]
    ）

    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f&#39;{log_dir}/{model_name}&#39;)
    hparams_callback = hp.KerasCallback(writer=f&#39;{log_dir}/{model_name}&#39;, hparams=hparams)

    模型.拟合(
        x_train_reshape, y_train_,
        纪元=3，
        验证数据=（x_val_reshape，y_val），
        回调=[hparams_callback，tensorboard_callback]
    ）

# 对每组超参数进行训练
会话编号 = 0
对于 HP_DROPOUT.domain.values 中的 dropout_rate：
    对于 HP_LEARNING_RATE.domain.values 中的learning_rate：
        hparams = {
            HP_DROPOUT：辍学率，
            HP_LEARNING_RATE：学习率，
        }
        train_test_model(hparams, session_num)
        会话编号 += 1

]]></description>
      <guid>https://stackoverflow.com/questions/78298357/tensorboard-hparams-not-showing-accuracy-metrics-for-hyperparameter-tuning</guid>
      <pubDate>Tue, 09 Apr 2024 12:14:56 GMT</pubDate>
    </item>
    <item>
      <title>标签未包含在我的张量数据集中</title>
      <link>https://stackoverflow.com/questions/78297824/label-not-included-inside-my-tensor-dataset</link>
      <description><![CDATA[我是机器学习新手，我想使用 BERT 模型中的预训练模型。
我面临以下问题：标签输出未插入张量类型数据集。
有人有解决办法吗？
from sklearn.model_selection import train_test_split
X = 特征[&#39;clean_text&#39;]
y = 特征[&#39;标签&#39;]
X_train、X_test、y_train、y_test=train_test_split(X、y、test_size = 0.3、random_state = 42)
X_train = tokenizer(X_train.tolist(), 填充 = True, 截断 = True)
X_test = tokenizer(X_test.tolist(), 填充 = True, 截断 = True)
X_train = 字典(X_train)
X_test = 字典(X_test)
y_train = y_train.tolist()
y_test = y_test.tolist()
df_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))
df_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))
输入，输出=下一个（iter（df_train））
打印出）

输出如下：tf.Tensor(0, shape=(), dtype=int32)]]></description>
      <guid>https://stackoverflow.com/questions/78297824/label-not-included-inside-my-tensor-dataset</guid>
      <pubDate>Tue, 09 Apr 2024 10:36:28 GMT</pubDate>
    </item>
    <item>
      <title>如何在各个图表上绘制多个线性回归特征与预测结果</title>
      <link>https://stackoverflow.com/questions/78296899/how-to-plot-multiple-linear-regression-features-vs-predicted-results-on-individu</link>
      <description><![CDATA[我正在研究一个电视广告数据集，该数据集具有 3 个特征（TV、radio、newspaper）和 1 个因变量 (销售）。通过使用多元线性回归，我估算了销售额并将其与 y_test 数据集上的实际值进行了比较。我已经确认我的模型获得了很高的 r2 分数。
我知道在多元线性回归中，多个特征可以在更高维度的图上可视化。但是，我希望在单独的图表上查看每个单独的特征与预测结果。
导入 pandas
导入numpy
从 sklearn.model_selection 导入 train_test_split
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.metrics 导入 r2_score
将 statsmodels.api 导入为 sm
将 matplotlib.pyplot 导入为 plt

#导入数据集并分配给X和y
df = pandas.read_csv(“广告.csv”)
df = df.drop(df.columns[[0]], 轴=1)
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].值

#将数据集分割成X_train, X_test, y_train, y_test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#初始化回归器并使用 X_train 和 y_train 对其进行训练
回归器=线性回归()
回归器.fit(X_train, y_train)

#预测测试结果并用实际值绘制它们
y_pred = 回归器.预测(X_test)
print(&quot;预测值/实际值&quot;)
print(numpy.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), axis=1))

#评估预测结果的模型性能
r_squared_线性 = r2_score(y_test, y_pred)
print(f“多元线性回归模型性能为{r_squared_线性}”)

#查看OLS回归结果
mod = sm.OLS(y.reshape(-1, 1), X)
res = mod.fit()
打印（res.summary（））

#在各个图表上绘制数据集的每个特征与预测结果

这是我的代码，它可以正常工作，没有任何问题。我只想用 matplotlib 完成绘图部分。我想绘制每个单独的特征（TV、radio、newspaper）与我的模型具有的 y_pred 值预测，包括每个图上的线性回归线。
如果有人能向我展示如何使用 for 循环（以确保它适用于不同数据集上的任何特征计数），我将不胜感激。
另外，为了获得额外的知识，如何在同一图中绘制所有特征以及 y_pred 结果？]]></description>
      <guid>https://stackoverflow.com/questions/78296899/how-to-plot-multiple-linear-regression-features-vs-predicted-results-on-individu</guid>
      <pubDate>Tue, 09 Apr 2024 07:56:36 GMT</pubDate>
    </item>
    <item>
      <title>CNN 教程模型根本拒绝训练</title>
      <link>https://stackoverflow.com/questions/78293057/cnn-tutorial-models-refusing-to-train-at-all</link>
      <description><![CDATA[我遇到的问题涉及在一些众所周知的数据集上创建和使用 CNN 模型。问题始于我的一项家庭作业，我们应该创建一个 CNN 并在 CIFAR10 数据集上运行它。然而这个问题似乎更加严重，我怀疑可能出了什么问题。
我注意到，在运行我自己的模型时，无论我制作什么模型或调整什么超参数，性能都与随机猜测一致。
为了更好地了解什么是好的模型，我在 CIFAR10 和 MNIST 上在线下载了一些教程。教程页面、网站等上的这些模型都报告了不错的准确度，范围在 60-80% 之间。
然而事情就变得奇怪了。当我使用相同的数据预处理等运行这些完全相同的模型（我下载了文件，因此没有复制/粘贴错误）时，我得到了与我自己的模型相同的结果，没有学习发生，准确性保持在 10%。这种行为与我在网上找到的至少四个不同的教程示例是一致的。
我尝试创建一个新的 conda 环境，以便我可以全新安装 tensorflow-gpu，并确保尽可能使用与教程相同的版本，但无论出于何种原因，模型似乎拒绝训练为我。下面是一个最小的可重现示例，我只是从 tensorflow.org 上的 CNN 教程示例和我的结果中复制/粘贴了该示例。
从tensorflow.keras导入数据集、图层、模型

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# 将像素值标准化为 0 到 1 之间
训练图像，测试图像 = 训练图像 / 255.0，测试图像 / 255.0


模型 = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), 激活=&#39;relu&#39;, input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), 激活=&#39;relu&#39;))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), 激活=&#39;relu&#39;))


model.add(layers.Flatten())
model.add(layers.Dense(64,activation=&#39;relu&#39;))
model.add(layers.Dense(10))

模型.编译(
    优化器=&#39;亚当&#39;,
    损失=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    指标=[&#39;准确性&#39;])

历史= model.fit(train_images, train_labels, epochs=10,
                    验证数据=（测试图像，测试标签））


结果
1563/1563 [================================] - 250s 11ms/步 - 损耗：2.3027 - 准确度：0.0977 - val_loss：2.3026 - val_accuracy：0.1000
纪元 2/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3028 - 精度：0.0999 - val_loss ：2.3027 - val_accuracy：0.1000
纪元 3/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3027 - 准确度：0.1011 - val_loss ：2.3027 - val_accuracy：0.1000
纪元 4/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3028 - 准确度：0.0993 - val_loss ：2.3027 - val_accuracy：0.1000
纪元 5/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3027 - 准确度：0.0957 - val_loss ：2.3026 - val_accuracy：0.1000
纪元 6/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3028 - 准确度：0.0979 - val_loss ：2.3026 - val_accuracy：0.1000
纪元 7/10
1563/1563 [================================] - 11s 7ms/步 - 损失：2.3027 - 准确度：0.0997 - val_loss ：2.3026 - val_accuracy：0.1000
]]></description>
      <guid>https://stackoverflow.com/questions/78293057/cnn-tutorial-models-refusing-to-train-at-all</guid>
      <pubDate>Mon, 08 Apr 2024 13:58:28 GMT</pubDate>
    </item>
    <item>
      <title>如何创建 CNN-LSTM 架构？</title>
      <link>https://stackoverflow.com/questions/78288542/how-to-create-cnn-lstm-architecture</link>
      <description><![CDATA[我尝试创建混合 CNN 和 LSTM 模型。我遇到了与架构形状相关的问题。这导致epoch无法跑完数据200次。
我的数据大小是（96,2）
错误：
纪元 1/200
    178/未知 9s 34ms/步 - 损耗：1.2366 - mse：5.4560
-------------------------------------------------- ------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
第 4 行 [40] 中的单元格
      2 is_train = True
      3 如果是_train：
----&gt; 4 model_create.fit（train_dataset，epochs = 200，batch_size = 128）

无法将张量添加到批次中：元素数量不匹配。形状为：[张量]：[78,2]，[批次]：[96,2]
     [[{{node IteratorGetNext}}]] [操作：__inference_one_step_on_iterator_23678]

CNN-LSTM 模型：
def create_model_architecture():
    model_cnn = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;,
                               输入形状=输入数据形状），
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;),
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.LSTM(32, return_sequences=True),
        tf.keras.layers.LSTM(16),
        tf.keras.layers.Reshape((-1,16)),
        #tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）
    返回 model_cnn


编译模型
def create_model():
    tf.random.set_seed(51)

    model_create = create_model_architecture()
    #model_create = create_LSTM_model()
    model_create.compile(loss=tf.keras.losses.Huber(),
                  优化器=tf.keras.optimizers.Adam(learning_rate=0.001),
                  指标=[“mse”])
    返回模型_创建

模型创建 = 创建模型()

model_create.summary()

model_create.fit（train_dataset，epochs = 200，batch_size = 128）


我曾尝试在 flatten() 函数之前添加 reshape 来改变形状。我还减小了批量大小和纪元大小。这些都不起作用。如何将我的模型与 train_data 相匹配？]]></description>
      <guid>https://stackoverflow.com/questions/78288542/how-to-create-cnn-lstm-architecture</guid>
      <pubDate>Sun, 07 Apr 2024 16:34:57 GMT</pubDate>
    </item>
    <item>
      <title>我如何提取边界框的 x 和 y 坐标</title>
      <link>https://stackoverflow.com/questions/78266928/how-do-i-extract-the-x-and-y-coordinates-of-the-bounding-boxes</link>
      <description><![CDATA[我的代码定义了一个 ROS 节点，用于使用 YOLO 算法在图像中进行对象检测，特别是使用从指定文件路径加载的 YOLO 模型。该节点订阅了广播图像的 ROS 主题 (/camera/color/image_raw)。收到图像后，会触发回调函数，使用 CvBridge 将 ROS 图像消息转换为 OpenCV 图像格式。然后该图像由 YOLO 模型处理以检测对象。结果（包括检测到的对象框）将打印到控制台。为了防止连续处理和重新处理图像，在处理第一个图像后设置一个标志（image_processed）。此外，处理后的图像将保存到指定的文件夹（临时）。该节点保持活动状态，侦听新图像，直到手动关闭。
#!/usr/bin/python3
导入罗斯比
从sensor_msgs.msg导入图像
从 cv_bridge 导入 CvBridge
导入CV2
导入操作系统
从 ultralytics 导入 YOLO

类节点（对象）：
    def __init__(自身):
        self.yolo_model = YOLO(&#39;/home/user/catkin_ws/src/run_folder/content/runs/obb/train/weights/best.pt&#39;)
        self.br = CvBridge()
        self.image_processed = False # 指示图像是否已处理的标志

        # 订阅发布图像的ROS主题
        rospy.Subscriber(“/camera/color/image_raw”, Image, self.callback)

    def 回调（自身，消息）：
        if not self.image_processed: # 仅当尚未处理图像时才处理
            image = self.br.imgmsg_to_cv2(msg) # 将ROS图像消息转换为OpenCV图像

            # 使用YOLO进行物体检测
            结果= self.yolo_model.predict（图像，显示= True）
            对于结果 [0] 中的 r：
                打印（“---------------------------”）
                打印（r.boxes）

            # 第一次处理后保存图像
            self.save_image(图像, &#39;临时&#39;)
            self.image_processed = True # 将标志设置为 True 以避免重新处理

    def save_image(自身, 图像, 文件夹名称):
        如果不是 os.path.exists(folder_name):
            os.makedirs(文件夹名称)
        
        file_path = os.path.join(folder_name, &#39;image.jpg&#39;)
        cv2.imwrite（文件路径，图像）
        print(f“图像保存在 {file_path}”)

如果 __name__ == &#39;__main__&#39;:
    rospy.init_node（“image_processor_with_yolo”，匿名= True）
    节点 = 节点()
    rospy.spin() # 保持节点运行直到关闭


当我尝试提取边界框时，我得到“无”在输出中。]]></description>
      <guid>https://stackoverflow.com/questions/78266928/how-do-i-extract-the-x-and-y-coordinates-of-the-bounding-boxes</guid>
      <pubDate>Wed, 03 Apr 2024 10:11:55 GMT</pubDate>
    </item>
    <item>
      <title>ImportError：使用 `bitsandbytes` 8 位量化需要加速：`pip install Accelerate` 和最新版本的 Bitsandbytes：`pip install</title>
      <link>https://stackoverflow.com/questions/78254344/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78254344/importerror-using-bitsandbytes-8-bit-quantization-requires-accelerate-pip-i</guid>
      <pubDate>Mon, 01 Apr 2024 08:15:53 GMT</pubDate>
    </item>
    </channel>
</rss>