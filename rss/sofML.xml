<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 05 Apr 2024 15:14:41 GMT</lastBuildDate>
    <item>
      <title>如何解释 DNN 中验证错误和测试错误之间的巨大差异</title>
      <link>https://stackoverflow.com/questions/78280101/how-can-i-explain-the-huge-difference-between-validation-and-test-errors-in-dnn</link>
      <description><![CDATA[我是 DNN 新手，并尝试了解它们如何在 cifar10 数据集上工作（不使用卷积层）。我使用两种不同的架构：
1.
def create_model(n_layers=5, n_neurons=100, shape=[32, 32, 3]):
    模型 = tf.keras.models.Sequential()

    model.add(tf.keras.layers.Flatten(input_shape=shape))
    对于 _ 在范围内（n_layers）：
        model.add(tf.keras.layers.Dense(n_neurons,
                                 激活＝“selu”，
                                 kernel_initializer=“lecun_normal”））
    model.add（tf.keras.layers.Dense（10，激活=“softmax”））

    优化器 = tf.keras.optimizers.Nadam()
    
    model.compile(loss=“sparse_categorical_crossentropy”,
              优化器=优化器，
              指标=[“准确度”])
    返回模型




model_bn = tf.keras.models.Sequential()

   model_bn.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))
   model_bn.add(tf.keras.layers.BatchNormalization())

   对于范围（5）内的 _：
       model_bn.add(tf.keras.layers.Dense(100,
                                 kernel_initializer=“he_normal”,
                                 kernel_constraint=tf.keras.constraints.max_norm(1.)))
       model_bn.add(tf.keras.layers.BatchNormalization())
       model_bn.add(tf.keras.layers.Activation(“elu”))
   model_bn.add(tf.keras.layers.Dense(10,激活=“softmax”))

   优化器 = tf.keras.optimizers.Nadam(learning_rate=1e-3)
   model_bn.compile(loss=“sparse_categorical_crossentropy”,
              优化器=优化器，
              指标=[“准确度”])

这些模型使用相同的回调来防止过度拟合和长时间训练：
early_stopping_cb = tf.keras.callbacks.EarlyStopping（耐心=6，监视器=“val_loss”）
   lr_scheduler_cb = tf.keras.callbacks.ReduceLROnPlateau（因子=0.5，耐心=3）
   checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(“my_cifar10_model_v1.keras”, save_best_only=True)
   回调 = [early_stopping_cb、checkpoint_cb、tensorboard_cb、lr_scheduler_cb]

但是在拟合结束时，我得到了训练和验证误差之间的巨大差异（大约 10-18%），我只能通过过度拟合来解释这一点，但是 checkpoint_cb 认为这是最好的模型，无论事实如何过度拟合。我是否坚持这些结果，或者我需要获得训练和验证精度差异较小的模型，从而降低最终验证精度。如果是这样，当训练准确性提高时，是否有回调停止，而验证错误保持大致相同？
我尝试设置不同的学习率和耐心参数，但似乎没有任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78280101/how-can-i-explain-the-huge-difference-between-validation-and-test-errors-in-dnn</guid>
      <pubDate>Fri, 05 Apr 2024 13:06:20 GMT</pubDate>
    </item>
    <item>
      <title>我需要帮助：聚类 JSON 数据以进行事件分析</title>
      <link>https://stackoverflow.com/questions/78280095/i-needed-help-clustering-json-data-for-event-analysis</link>
      <description><![CDATA[我目前正在从事一个涉及对 JSON 数据进行聚类以进行事件分析的项目，我面临着一些挑战。我有 JSON 数据，其中包含具有不同属性（例如用户 ID、时间戳和事件类型）的各种事件。
我需要根据事件的属性对这些事件进行聚类，以识别模式和见解。具体来说，我想对登录、注销、购买等事件进行聚类，并且每种事件类型都需要选择不同的属性进行聚类。
例如，对于登录事件，我需要考虑用户 ID 和登录时间，而对于购买事件，我可能需要包含产品 ID 和购买金额。
我不确定如何有效地完成此任务以及哪种机器学习算法最适合此场景。此外，我不确定如何预处理数据并为每种事件类型选择正确的功能。
这是我的 JSON 数据结构的简化版本：
{
“id”：“1”，
“年龄组”：“120”，
“client_id”：“1234567890123”，
“设备”：“788”，
“金额”：2000，
“受益人账户”：“12345678901234567890”，
“原因”：“测试原因”，
“发送日期”：1710457200000，
“交易类型”：“U”，
“状态”：“已解决”，
“状态代码”：3，
“beneficiary_name”：“测试受益人”，
“附加详细信息”：[]
}
有人可以提供有关如何预处理数据、为每种事件类型选择正确的特征以及选择适当的机器学习算法进行聚类的指导吗？
任何见解、建议或代码示例将不胜感激。预先感谢您的帮助！
我想要对登录、注销、购买等事件进行聚类，并且每种事件类型都需要选择不同的属性进行聚类]]></description>
      <guid>https://stackoverflow.com/questions/78280095/i-needed-help-clustering-json-data-for-event-analysis</guid>
      <pubDate>Fri, 05 Apr 2024 13:05:16 GMT</pubDate>
    </item>
    <item>
      <title>tensorflowjs 模型预测给出 NaN</title>
      <link>https://stackoverflow.com/questions/78279643/tensorflowjs-model-prediction-gives-nan</link>
      <description><![CDATA[我在tensorflowjs中创建了一个模型，它需要一个带有坐标数据的csv文件，例如-0.5到0.5之间的值。它有 99 个特征和 2 类打孔和无打孔。
这是创建它的代码，我能够从浏览器下载 2 个文件，model.json 和 model.weights.bin：
document.getElementById(&#39;csvFileInput&#39;).addEventListener(&#39;change&#39;,handleFileSelect);

异步函数handleFileSelect（事件）{
    const 文件 = event.target.files[0];
    const 数据 = 等待 parseCSV(文件);
    训练模型（数据）；
}

异步函数 parseCSV(file) {
    返回新的 Promise((解决, 拒绝) =&gt; {
        常量数据=[]；
        const reader = new FileReader();
        reader.onload = () =&gt;; {
            constlines = reader.result.split(&#39;\n&#39;);
            for (const line oflines) {
                const 值 = line.trim().split(&#39;,&#39;);
                const 标签 = value.shift();
                const 坐标 = value.map(parseFloat);
                data.push({ 标签, 坐标 });
            }
            解决（数据）；
        };
        reader.onerror = 拒绝；
        reader.readAsText(文件);
    });
}

异步函数trainModel（数据）{
    const features = data.map(entry =&gt;entry.coordinates);
    const labels = data.map(entry =&gt; entry.label === &#39;打孔&#39; ? 0 : 1);

    const 模型 = tf.sequential({
        图层：[
            tf.layers.dense({ inputShape: [99], 单位: 64, 激活: &#39;relu&#39; }),
            tf.layers.dense({ 单位: 2, 激活: &#39;softmax&#39; })
        ]
    });

    模型.编译({
        优化器：&#39;亚当&#39;，
        损失：&#39;稀疏分类交叉熵&#39;，
        指标：[&#39;准确性&#39;]
    });

    等待 model.fit(tf.tensor2d(features), tf.tensor1d(labels), { epochs: 10 });

    console.log(&#39;模型训练成功。&#39;);

    // 将模型保存到models目录下
    等待 model.save(&#39;下载://model&#39;);

    console.log(&#39;模型保存成功。&#39;);
}



然后我尝试使用测试数据来测试 model.json 文件，如下所示
异步函数 testModel() {
    const model = wait tf.loadLayersModel(&#39;./models/model.json&#39;);

    const inputTensor = tf.tensor2d(new Array(99).fill(0.5), [1, 99]);
    console.log(“输入张量：”, inputTensor)

    console.log(“输入张量形状：”, inputTensor.shape);

    console.log(“Inputtensor datasync:”,inputTensor.dataSync());

    const 预测 = model.predict(inputTensor);

    预测.print();
}

在打印完所有内容后，在 Chrome 的控制台中我得到了这个：
输入张量：e {kept：false，isDisposeInternal：false，形状：Array（2），dtype：&#39;float32&#39;，大小：99，...}
model_test.js:7 输入张量形状：(2) [1, 99]
model_test.js:9 输入张量数据同步：Float32Array(99) [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5 , 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5 , 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5 , 0.5、0.5、0.5、0.5、0.5、0.5、0.5，缓冲区：ArrayBuffer（396），字节长度：396，字节偏移：0，长度：99，符号（Symbol.toStringTag）：&#39;Float32Array&#39;]
tfjs@最新：17
张量
[[NaN，NaN]，]
我不知道为什么它在预测概率中给出 NaN。]]></description>
      <guid>https://stackoverflow.com/questions/78279643/tensorflowjs-model-prediction-gives-nan</guid>
      <pubDate>Fri, 05 Apr 2024 11:33:07 GMT</pubDate>
    </item>
    <item>
      <title>未能使用 RNN 提高情感项目的准确性</title>
      <link>https://stackoverflow.com/questions/78279557/failure-to-improve-accuracy-in-the-sentiment-project-with-rnn</link>
      <description><![CDATA[希望你一切都好。我正在开发一个名为情感分析的人工智能项目，该项目适用于波斯语数据集。
我一直致力于加载数据，将它们转换为嵌入，然后将它们输入到由 LSTM 组成的神经网络中。然而，在训练过程中，第 1 轮之后准确率并没有提高，并且陷入了困境。
我的网络代码部分是：
https://colab.research.google.com/drive/1Pz20d5r1iZPLvWjHKC2oOfNsNXY1R- TC?usp=共享
纪元[1/20]，损失：1.1605366468429565，准确率：23.5%
Epoch [2/20]，损失：1.0860859155654907，准确率：37.1%
Epoch [3/20]，损失：1.0465837717056274，准确率：48.0%
Epoch [4/20]，损失：1.02091646194458，准确率：51.2%
Epoch [5/20]，损失：1.003448486328125，准确度：52.300000000000004%
Epoch [6/20]，损失：0.9991855621337891，准确率：53.6%
Epoch [7/20]，损失：0.9968012571334839，准确率：52.800000000000004%
Epoch [8/20]，损失：0.9954250454902649，准确率：52.900000000000006%
Epoch [9/20]，损失：0.9897969365119934，准确率：53.1%
Epoch [10/20]，损失：0.9899587631225586，准确率：53.7%
Epoch [11/20]，损失：0.992097020149231，准确率：53.0%
Epoch [12/20]，损失：0.9817440509796143，准确率：53.400000000000006%
]]></description>
      <guid>https://stackoverflow.com/questions/78279557/failure-to-improve-accuracy-in-the-sentiment-project-with-rnn</guid>
      <pubDate>Fri, 05 Apr 2024 11:19:48 GMT</pubDate>
    </item>
    <item>
      <title>Pdf分析仪型号</title>
      <link>https://stackoverflow.com/questions/78279375/pdf-analyzer-model</link>
      <description><![CDATA[这是我第一次使用朋友介绍的 stackflow。
我在对冲基金部门的一家私人银行工作，并被分配了一个特殊项目，该项目要求我创建一个模型，我们可以从对冲基金（HF）提供的风险报告中提取风险指标数据。鉴于该投资组合由 2,000 多个 HF 组成，并且每月报告风险数据，我计划首先创建一个模型来训练它阅读和理解 pdf 文件（风险报告）并根据请求提取风险指标。
鉴于我对学习模型的理解有限或处于初级水平。我请求这个超级有帮助的社区指导我学习模型/包，我可以研究和学习启动这个过程。
谢谢！
我尝试了 openai 的 langchain 模型，但考虑到我的银行有紧密的网络，他们不允许我任何 api 连接，他们告诉我从零开始构建模型]]></description>
      <guid>https://stackoverflow.com/questions/78279375/pdf-analyzer-model</guid>
      <pubDate>Fri, 05 Apr 2024 10:46:26 GMT</pubDate>
    </item>
    <item>
      <title>在 Kaggle Notebook 中降级 Tensorflow 版本时遇到问题；我应该怎么办</title>
      <link>https://stackoverflow.com/questions/78279273/facing-problem-while-downgrading-tensorflow-version-in-kaggle-notebook-what-sho</link>
      <description><![CDATA[我在tensorflow versino 2.11.0中编写了代码，但最近我的代码无法运行，发现当前的tensorflow版本2.15.0是主要问题；所以我使用代码降级我的版本！pip install tensorflow-gpu==2.11.0
但是我的笔记本没有找到任何 GPU，尽管我像以前一样在 Kaggle 笔记本中启用了 GPU P100 加速器。我还在代码中检查 GPU
将张量流导入为 tf
如果 tf.test.gpu_device_name():
print(&#39;默认 GPU 设备：{}&#39;.format(tf.test.gpu_device_name()))
其他：
print(&quot;请安装GPU版本的TF&quot;)
得到了
请安装GPU版本的TF

请在这方面帮助我；我的项目截止日期非常接近
在kaggle笔记本中降级tensorflow版本时遇到问题；请帮忙]]></description>
      <guid>https://stackoverflow.com/questions/78279273/facing-problem-while-downgrading-tensorflow-version-in-kaggle-notebook-what-sho</guid>
      <pubDate>Fri, 05 Apr 2024 10:28:53 GMT</pubDate>
    </item>
    <item>
      <title>变压器评估指标</title>
      <link>https://stackoverflow.com/questions/78279033/transformer-evaluation-metrics</link>
      <description><![CDATA[我试图了解如何为 Transformer 等序列到序列模型计算字符错误率 (cer) 等评估指标。我知道这是一个编辑距离，但我的问题是标记是否是自回归生成的。
训练 Transformer 时，如果所有先前的标记都是正确的，则会预测每个标记。人们可以对测试数据做同样的事情，并获得像 cer 这样的准确性。人们还可以使用变压器一次生成一个令牌（自回归）的输出，然后将输出与真实情况进行比较。这看起来更实用，因为错误预测的标记会影响下一个预测，就像实际推理一样。
像 cer 和 wer 这样的指标是根据自回归生成的标记序列计算的还是像训练中那样生成的？这是在某处定义的吗？]]></description>
      <guid>https://stackoverflow.com/questions/78279033/transformer-evaluation-metrics</guid>
      <pubDate>Fri, 05 Apr 2024 09:45:16 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow.js：无法在nodejs中使用gpt2等模型找到用于文本生成的内容</title>
      <link>https://stackoverflow.com/questions/78278979/tensorflow-js-unable-to-find-content-for-text-generation-using-models-like-gpt</link>
      <description><![CDATA[我是机器学习或人工智能新手，刚刚开始学习
所以我的任务是使用 gpt2 或互联网上存在的任何其他模型（在我们系统本地的数据隐私模型）来制作聊天机器人，然后通过我们自己的数据集对它们进行微调。所以它可以按照我们的要求执行。在 NODEJS 或 javascript 中（无 PYTHON）。
所以，到目前为止，我已经浏览了tensorflowJS文档，因此我已经在系统中本地下载了LLM模型，例如gpt2（.h5格式），然后将其转换为TF.JS格式（.json和.bin文件），现在想要执行文本生成，但无法找到相同的任何代码或内容。
现在，我的第一个问题：-是否可以从本地模型执行文本生成或制作聊天机器人，然后使用tensorflow.js在nodejs或javascript（无python）中进行微调
如果可能的话，请告诉我接下来的步骤或除tensorflowJS之外的任何方式或任何资源或任何库来完成这项工作......
或者使用 Python 是唯一的解决方案？？？...]]></description>
      <guid>https://stackoverflow.com/questions/78278979/tensorflow-js-unable-to-find-content-for-text-generation-using-models-like-gpt</guid>
      <pubDate>Fri, 05 Apr 2024 09:35:47 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 TF-IDF 功能和微调嵌入模型复制 PECOS XR-Linear 性能</title>
      <link>https://stackoverflow.com/questions/78278617/cannot-replicate-pecos-xr-linear-performance-with-tf-idf-features-and-a-fine-tun</link>
      <description><![CDATA[我正在尝试使用 TFIDF 和 BGE 模型中的预训练嵌入来复制 XR-Linear。
parsed_result = Preprocessor.load_data_from_file(input_text_path, output_text_path)
Y = parsed_result[“label_matrix”]
语料库 = parsed_result[“语料库”]

预处理器 = Preprocessor.train(corpus, {&quot;type&quot;: &quot;tfidf&quot;})
tfidf_X = 预处理器.预测(语料库)

从句子转换器导入句子转换器
模型 = SentenceTransformer(&#39;BAAI/bge-small-en-v1.5&#39;)
嵌入= model.encode（data.query_string.values，show_progress_bar = True，batch_size = 2048）
打印（嵌入.形状）

我将 TFIDF 特征与 BGE 嵌入水平连接起来，如下所示：
X = scipy.sparse.csr_matrix(scipy.sparse.hstack((tfidf_X,embeddings)))

模型训练：
label_feat = LabelEmbeddingFactory.create(Y, X, method=“pifa”)
cluster_chain = Indexer.gen(label_feat, nr_splits=4)
xlinear_model = XLinearModel.train(X, Y, C=cluster_chain,negative_sampling_scheme=“tfn”)

预测：
def process_query_and_predict（查询，use_cpu_threads）：
    tfidf_vector = 预处理器.predict(查询)
    bge_embedding = model.encode(查询)
    pred_X = scipy.sparse.csr_matrix(scipy.sparse.hstack((tfidf_vector,bge_embedding)))
    Y_pred = xlinear_model.predict(pred_X)
    返回 smat_util.sorted_csr(Y_pred)

但是，令人惊讶的是，与在独立 TF-IDF 向量上训练的模型相比，模型的表现相当差。我看到了 PECOS XR-Linear。我试图复制所执行的过程。我想通过 BGE 模型引入语义功能，而不是 AttnXML。
参考部分的屏幕截图：
有人可以在这里分享一些见解，我可能会出错的地方..以及如何将 tf-idf 功能与 BGE 嵌入合并。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78278617/cannot-replicate-pecos-xr-linear-performance-with-tf-idf-features-and-a-fine-tun</guid>
      <pubDate>Fri, 05 Apr 2024 08:22:32 GMT</pubDate>
    </item>
    <item>
      <title>我们如何使用Python中的OpenAI获得Power BI数据的精确聚合？</title>
      <link>https://stackoverflow.com/questions/78278556/how-can-we-get-the-exact-aggregation-of-power-bi-data-using-openai-in-python</link>
      <description><![CDATA[我们拥有大约 200 万条 Power BI 格式（结构化数据）记录，其中包含期间、时间、市场、区域、子区域、区域组、产品、BU（业务单位）、子 BU、细分等字段、实际收益（收入）、预测、预算等。
数据被结构化并在上述字段内创建不同的组合。
数据示例
用户将使用 Python 中的自然语言进行询问，一旦我们将输入发送到 OpenAI，它应该按照汇总方式根据查询进行聚合响应。
例如：（根据数据图像）
实验 1. 阿莫西林的收入是多少？ [假设我们采用期间默认值 AP12 和时间 YTD]
答案应该是准确的聚合编号：4290081
实验2. Amoxile在中国的前景如何？ [假设我们采用期间默认值 AP12 和 YTD 时间，如果中国包含在市场、区域、次区域中，我们将打印市场]
答：1430027是Amoxile在中国市场的预测
由于我们有 200 万条记录，因此我们还必须关心 OpenAI 的代币限制和其他因素。
请分享您针对这种情况的处理方法，理论上的解释也对我有帮助。
#Python #OpenAI #GenAI #PromptEngineer
我们尝试了对话式 LLM、矢量搜索，但无法获取单个查询的所有记录行。如果我们搜索任何查询“阿莫西林在中国”因此，该组合大约有 3K 到 50k 条记录，而其与其他组合的独立记录则更多。
因此，我无法找到将记录传递给 OpenAI 并获得准确响应的方法。
注意：它适用于最多 100 或 1000 条的较小样本数据记录。]]></description>
      <guid>https://stackoverflow.com/questions/78278556/how-can-we-get-the-exact-aggregation-of-power-bi-data-using-openai-in-python</guid>
      <pubDate>Fri, 05 Apr 2024 08:07:21 GMT</pubDate>
    </item>
    <item>
      <title>在 VS Code 中使用 Tensorflow 和 Keras 加载模型时出错</title>
      <link>https://stackoverflow.com/questions/78278127/error-while-loading-model-using-tensorflow-and-keras-in-vs-code</link>
      <description><![CDATA[我在 Jupyter 笔记本中训练了一个模型，然后使用 model.save(modelName.h5) 保存它，然后我尝试在 VS Code 中加载该模型以创建前端和后端，但在 VS Code 中加载它时，我我收到此错误：
ValueError：未知层：请确保您使用的是 kerbs.utils.custom_object_scope 并且该对象包含在范围内。
如果有人知道如何修复它，请帮忙。
提前致谢。
我在 Jupyter 笔记本中训练了一个模型，然后使用 model.save(modelName.h5) 保存它，然后我尝试在 VS Code 中加载该模型以创建前端和后端，但在 VS Code 中加载它时，我我收到错误。]]></description>
      <guid>https://stackoverflow.com/questions/78278127/error-while-loading-model-using-tensorflow-and-keras-in-vs-code</guid>
      <pubDate>Fri, 05 Apr 2024 06:32:26 GMT</pubDate>
    </item>
    <item>
      <title>如何找到不同公司竞争对手产品矩阵与特定品牌数据集的相关性？有机器学习来预测适合度吗？</title>
      <link>https://stackoverflow.com/questions/78277282/how-can-i-find-the-correlation-of-a-matrix-of-competitor-products-of-different-c</link>
      <description><![CDATA[我非常感谢有关此挑战的任何建议，我正在尝试找到数据集（矩阵）的相关性，该数据集（矩阵）包含行上的客户和他们按列拥有的竞争对手产品（拥有=&#39;是&#39;，不拥有= “否”）以及拥有我们品牌的客户的另一个数据集（拥有=1，不拥有=0）。我可以使用任何机器学习模型或算法来根据客户拥有的产品来预测客户的适合度吗？请记住，我们品牌数据集中的所有零值都是潜在客户。
我尝试了随机森林，但我们对拥有我们品牌的客户的所有价值观都是积极的，我如何根据所有积极的价值观来预测适合度？]]></description>
      <guid>https://stackoverflow.com/questions/78277282/how-can-i-find-the-correlation-of-a-matrix-of-competitor-products-of-different-c</guid>
      <pubDate>Fri, 05 Apr 2024 00:53:35 GMT</pubDate>
    </item>
    <item>
      <title>可能是什么引发了错误：ValueError：X 有 23 个特征，但 SVR 期望 24 个特征作为输入？</title>
      <link>https://stackoverflow.com/questions/78275238/what-may-be-raising-the-error-valueerror-x-has-23-features-but-svr-is-expecti</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78275238/what-may-be-raising-the-error-valueerror-x-has-23-features-but-svr-is-expecti</guid>
      <pubDate>Thu, 04 Apr 2024 16:21:48 GMT</pubDate>
    </item>
    <item>
      <title>无法通过机器学习工作室使用 write_deltalake 创建增量表</title>
      <link>https://stackoverflow.com/questions/78272862/unable-to-create-delta-table-using-write-deltalake-via-machine-learning-studio</link>
      <description><![CDATA[尝试将 pandas 数据帧转换为 delta 表，标题中出现错误。生成一个表，但 delta_log 为空，无法读取数据。
尝试创建增量表，通过数据集使用 MLS 上传到 blob。
https://delta-io .github.io/delta-rs/python/usage.html#writing-delta-tables
导入 pandas 作为 pd
导入操作系统
从 azureml.core 导入数据存储区、数据集、工作区
从 azureml.data.datapath 导入 DataPath
从达美进口*

Training_data = pd.read_csv(&#39;Training.csv&#39;)

如果不是 os.path.isdir(&#39;output&#39;):
    os.makedirs(&#39;输出&#39;)
 
导入操作系统
从 deltalake 导入 DeltaTable
从 deltalake.writer 导入 write_deltalake
 
write_deltalake(“输出/some_delta_lake”，training_data)

错误：OSError：通用 DeltaLocalObjectStore 错误：不支持操作（操作系统错误 95）]]></description>
      <guid>https://stackoverflow.com/questions/78272862/unable-to-create-delta-table-using-write-deltalake-via-machine-learning-studio</guid>
      <pubDate>Thu, 04 Apr 2024 09:21:24 GMT</pubDate>
    </item>
    <item>
      <title>（误）-使用 open.ai Whisper 进行文本到文本的翻译</title>
      <link>https://stackoverflow.com/questions/74667955/mis-using-open-ai-whisper-for-text-to-text-translation</link>
      <description><![CDATA[我注意到使用 openai whisper 语音转文本库 转录多种语言的语音有时可以准确识别以另一种语言插入并提供预期输出，例如：八十多个人与八十几个人相同。所以多和几可以互换，都可以表示几个。
然而，不同通道上的相同音频输入（使用相同的模型或更小/更大的模型）会间歇性地导致整个句子被翻译的故障，而不是翻译转录。 IE。片段将被翻译成音频中出现的第一语言或第二语言。对于上面的示例输入，要么整个句子是英语（中文部分翻译成英语），要么整个句子是中文（英语部分翻译成中文）。 重要：在这两种情况下都没有指定输入语言，也没有传递任务类型（这意味着默认的 --task transcribe）。
耳语文档提到将英语翻译为唯一可用的目标语言（使用选项--tasktranslate 在命令行版本中），但没有提到翻译成其他目标语言。然而，上述行为表明这些模型也能够翻译成其他语言。
问题是是否有一种已知的方法来配置模型以进行文本到文本的翻译？或者这种行为只是某种故障，不能被“利用”或在较低级别上进行配置，从而允许使用模型在任何受支持的语言之间进行文本翻译？]]></description>
      <guid>https://stackoverflow.com/questions/74667955/mis-using-open-ai-whisper-for-text-to-text-translation</guid>
      <pubDate>Sat, 03 Dec 2022 15:12:21 GMT</pubDate>
    </item>
    </channel>
</rss>