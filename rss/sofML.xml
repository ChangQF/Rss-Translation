<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 18 Dec 2024 12:34:28 GMT</lastBuildDate>
    <item>
      <title>随机森林分类器的适应性</title>
      <link>https://stackoverflow.com/questions/79290974/adaptation-of-random-forest-classifier</link>
      <description><![CDATA[我正在尝试更改随机森林分类器的功能。虽然通常每次分割都会随机选择特征，但我希望每次分割时都评估一个特定特征。我知道这会影响性能，但我想尝试一下这在非常具体的用例中是否是个好主意。因此，调整的结果应为：用于分割的特征是随机选择的（与往常一样），但始终会考虑一个特定特征（例如索引 15）（不一定使用）。据我所知，没有函数允许我指定这一点（如果有，请告诉我），所以我正在尝试更改分类器函数的源代码。
我目前的状态是我选择了 R 中的 randomForest 包。我创建了一个本地版本（通过从 cran 下载 .tar.gz）并更改了 c 脚本中的某些内容（特别是“findbestsplit”函数中的以下部分：
for (i = 0, i &lt; mtry; ++i) {
/* 样本 mtry 变量，不替换。 */
j = (int) (unif_rand() * (last + 1));
mvar = mIndex[j];
swapInt(mIndex[j], mIndex[last]);
la​​st--;

if (i == mtry - 1) { /* 在最后一次迭代中更改某些内容 */
mvar = additionalVarIndex; 
/* 专门设置mvars 最后一个值到所需特征 */
}

此处，additionalVarIndex 是我通过 c 脚本中的函数传递给 findbestsplit 函数（似乎有效）的索引。我的 c 知识非常有限，所以我认为这还不够，但我也无法测试它，因为我的更改似乎不会影响 r 应用程序。当我加载调整后的包（通过以下方式将其转回 .tar.gz）时
&amp; &quot;C:\Program Files\R\R-4.4.1\bin\x64\R.exe&quot; CMD build .

通过
install.packages(&quot;C:/Users/niklas.jacobs/AppData/Local/R/win- 
library/randomForest/randomForest_4.7-1.2.tar.gz&quot;, repos = NULL, type = &quot;source&quot;)

randomForest r 函数的行为没有发生任何变化（即使我删除了 c 代码的随机部分，我这样做只是为了了解我是否可以更改内容）。我仍然确信我的更改会产生一些影响，因为如果我删除大部分 c 代码，该函数将停止工作。我目前的怀疑是c 脚本的 fortran 调用以某种方式绕过了我的更改（据我所知，使用“findbestsplit”的“buildtree”函数不是在 c 脚本中定义的，而是在 fortran 中定义的），但我不知道如何访问 fortran。
简而言之：有人对我接下来的行动有什么建议吗（或者你有更好的想法来实现我的目标吗？）。提前致谢。]]></description>
      <guid>https://stackoverflow.com/questions/79290974/adaptation-of-random-forest-classifier</guid>
      <pubDate>Wed, 18 Dec 2024 11:48:27 GMT</pubDate>
    </item>
    <item>
      <title>‘super’ 对象没有属性‘__sklearn_tags__’</title>
      <link>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</link>
      <description><![CDATA[我在使用 Scikit-learn 中的 RandomizedSearchCV 拟合 XGBRegressor 时遇到了 AttributeError。错误消息指出：
&#39;super&#39; 对象没有属性 &#39;sklearn_tags&#39;。
当我在 RandomizedSearchCV 对象上调用 .fit() 方法时会发生这种情况。我怀疑这可能与 Scikit-learn 和 XGBoost 或 Python 版本之间的兼容性问题有关。我使用的是 Python 3.12，并且 Scikit-learn 和 XGBoost 都安装了最新版本。
我尝试使用 Scikit-learn 中的 RandomizedSearchCV 调整 XGBRegressor 的超参数。我期望模型能够毫无问题地拟合训练数据，并在交叉验证后提供最佳参数。
相反，当我在 RandomizedSearchCV 对象上调用 .fit() 方法时，它引发了错误：
AttributeError：&#39;super&#39; 对象没有属性 &#39;sklearn_tags&#39;。
我还检查了兼容性问题，确保库是最新的，并重新安装了 Scikit-learn 和 XGBoost，但错误仍然存​​在。]]></description>
      <guid>https://stackoverflow.com/questions/79290968/super-object-has-no-attribute-sklearn-tags</guid>
      <pubDate>Wed, 18 Dec 2024 11:45:52 GMT</pubDate>
    </item>
    <item>
      <title>我如何确定谁将赢得罗马尼亚下届总统选举？[关闭]</title>
      <link>https://stackoverflow.com/questions/79290841/how-can-i-determine-who-will-win-the-next-presedentials-elections-in-romania</link>
      <description><![CDATA[我想确定谁将赢得罗马尼亚的下一届选举，这样我就可以赚点钱。我尝试了不同的机器学习算法，但都失败了。任何帮助都非常感谢！]]></description>
      <guid>https://stackoverflow.com/questions/79290841/how-can-i-determine-who-will-win-the-next-presedentials-elections-in-romania</guid>
      <pubDate>Wed, 18 Dec 2024 11:01:28 GMT</pubDate>
    </item>
    <item>
      <title>如何从信息流中删除“助手”一词？</title>
      <link>https://stackoverflow.com/questions/79290138/how-do-i-remove-the-word-assistant-from-a-stream</link>
      <description><![CDATA[我一直试图从流中删除单词“assistant”，因为它总是以这个词开头。无论我在后端使用和实现什么逻辑，它都不起作用，我想从后端处理它。
def generate_streamed_response(prompt: str, behavior: str):
try:
text_pipeline = load_pipeline()

# 准备具有系统和用户角色的输入消息
messages = [
{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: behavior},
{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}
]

# 准备用于文本生成的流式传输器
streamer = TextIteratorStreamer(text_pipeline.tokenizer, skip_prompt=True, skip_special_tokens=True)

# 在单独的线程中启动文本生成
thread = threading.Thread(
target=text_pipeline.model.generate,
kwargs={
&quot;inputs&quot;: text_pipeline.tokenizer.apply_chat_template(
messages, return_tensors=&quot;pt&quot;
).to(device),
&quot;streamer&quot;: streamer,
&quot;max_new_tokens&quot;: settings.max_tokens
}
)
thread.start()

# 第一个块标志
first_chunk = True

# 生成令牌时产生令牌
for chunk in streamer:
if first_chunk:
# 处理 &quot;assistant&quot;在第一个块中
chunk = chunk.strip()
if chunk.lower().startswith(&quot;assistant&quot;):
chunk = chunk[len(&quot;assistant&quot;):].strip()
first_chunk = False
yield chunk

except Exception as e:
logger.error(f&quot;流式响应生成期间出错：{e}&quot;)
raise HTTPException(status_code=500,detail=&quot;生成流式响应时出错&quot;)

]]></description>
      <guid>https://stackoverflow.com/questions/79290138/how-do-i-remove-the-word-assistant-from-a-stream</guid>
      <pubDate>Wed, 18 Dec 2024 06:14:21 GMT</pubDate>
    </item>
    <item>
      <title>ADTK 模型消耗高 CPU [Python]</title>
      <link>https://stackoverflow.com/questions/79290008/adtk-model-consuming-high-cpu-python</link>
      <description><![CDATA[我们从去年开始在 Python 应用程序中使用 ADTK 模型来检测异常。
自上周以来，我们观察到 CPU 利用率大幅上升，原因已确定为 Python 进程。
经过故障排除，我们发现 ADTK 模型是导致该问题的原因。我们正在使用其他 ML 模型，如 IForest、ECOD、CBLOF 等。但只有 ADTK 负责此峰值。
请查找当前模型代码以供参考：
data=df.copy() 
minutes = 300

df[&#39;predict_dt&#39;]=pd.to_datetime(df[&#39;predict_dt&#39;])
df = df.set_index([&#39;predict_dt&#39;]).sort_index() 

seasonal_vol = SeasonalAD(c=1.6,side=&#39;negative&#39;,trend=True) 
seasonal_vol.fit(df[&#39;predict_count&#39;])
df[&#39;anomalies&#39;]=seasonal_vol.predict(df[&#39;predict_count&#39;]) 

df2 = pd.DataFrame(columns=[&#39;predict_dt&#39;, &#39;hour&#39;, &#39;min&#39;,&#39;predict_count&#39;,&#39;label&#39;]) 
final = datetime.datetime.now(pytz.timezone(&#39;America/Los_Angeles&#39;)).replace(tzinfo=None)-timedelta(minutes=minutes)

for i in range(len(df)): 
if(data[&#39;predict_dt&#39;][i] &gt;= final):
if str(df[&#39;anomalies&#39;][i]).lower() == &quot;true&quot;:
#调用 SP 进行进一步操作

我们不确定哪个部分导致了问题。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79290008/adtk-model-consuming-high-cpu-python</guid>
      <pubDate>Wed, 18 Dec 2024 04:50:01 GMT</pubDate>
    </item>
    <item>
      <title>如何在 LM 中使用标记化</title>
      <link>https://stackoverflow.com/questions/79289981/how-to-use-tokenizeation-in-lm</link>
      <description><![CDATA[所以我一直在用这些超参数训练这个 LM..
blocksiz = 128
batchsiz = 32
nemb = 256
nhead = 4
nlayers = 4
evalIters = 100
lr = 3e-4
epochs = 30

我的数据集也是 93kb..，
我使用 GPT-2 作为 Tokenizer..
然而，当我使用 GPT-2 Tokenizer 时，训练时间太长了，
但是当我将模型训练为二元组时。它不需要那么多。
所以使用具有 50257 个 Vocabsiz 的 GPT2 作为 tokenizer，会影响模型训练吗？
训练是否需要更多 GPU 资源？
所以对于小数据集。我应该使用什么作为标记器...？
我减少了超参数，上面我使用的参数是减少的结果
但模型训练时间太长了]]></description>
      <guid>https://stackoverflow.com/questions/79289981/how-to-use-tokenizeation-in-lm</guid>
      <pubDate>Wed, 18 Dec 2024 04:30:58 GMT</pubDate>
    </item>
    <item>
      <title>如果原始训练数据是离散的，是否可以将数据添加到连续的 keras 模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/79289159/is-it-possible-to-add-data-to-a-keras-model-that-is-continous-if-the-original-tr</link>
      <description><![CDATA[我有一个模型 keras 模型，该模型是在离散数据（基于生物体中基因的存在而得出的是/否数据）上进行训练的，这可以预测离散响应（生物体是否执行某种功能而得出的是/否）。
我们想要添加连续的新数据（生物体中蛋白质的丰度），现在我们想要预测连续响应（某种功能的活动水平）。
这有可能做到吗？我已按照此链接中的步骤进行操作，但似乎假设您使用的是相同类型的数据。我该如何添加不同类型的数据？]]></description>
      <guid>https://stackoverflow.com/questions/79289159/is-it-possible-to-add-data-to-a-keras-model-that-is-continous-if-the-original-tr</guid>
      <pubDate>Tue, 17 Dec 2024 19:32:07 GMT</pubDate>
    </item>
    <item>
      <title>图像预处理步骤[关闭]</title>
      <link>https://stackoverflow.com/questions/79288818/image-preprocessing-steps</link>
      <description><![CDATA[我想知道 Keras 图像数据加载是否对图像数据集进行了完整的预处理，例如应用过滤器、规范化、标准化等，之后该模块不需要进行更详细的预处理还是怎样？
代码如下。
malimg=tf.keras.utils.image_dataset_from_directory(
r&quot;C:\Users\PMYLS\Desktop\Malware Pycharm Project\malimg_paper_dataset_imgs&quot;,
labels=&quot;inferred&quot;,
label_mode=&quot;int&quot;,
class_names=None,
color_mode=&quot;rgb&quot;,
batch_size=64,
image_size=(256, 256),
shuffle=True,
seed=None,
validation_split=None,
subset=None,
interpolation=&quot;bilinear&quot;,
follow_links=False,
crop_to_aspect_ratio=False,
pad_to_aspect_ratio=False,
data_format=None,
verbose=True,
)
class_names = malimg.class_names # 获取类名
print(&quot;Classes:&quot;, class_names)
]]></description>
      <guid>https://stackoverflow.com/questions/79288818/image-preprocessing-steps</guid>
      <pubDate>Tue, 17 Dec 2024 17:17:21 GMT</pubDate>
    </item>
    <item>
      <title>Databricks MLFlow 和 MetaFlow 集成</title>
      <link>https://stackoverflow.com/questions/79287981/databricks-mlflow-and-metaflow-integration</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79287981/databricks-mlflow-and-metaflow-integration</guid>
      <pubDate>Tue, 17 Dec 2024 13:02:01 GMT</pubDate>
    </item>
    <item>
      <title>LSTM RNN Tensorflow 语言预测模型卡住了吗？</title>
      <link>https://stackoverflow.com/questions/79273048/lstm-rnn-tensorflow-language-prediction-model-stuck</link>
      <description><![CDATA[我正在尝试开发一个模型，从不同艺术家的歌词中学习，以确定是谁写的。我的 Jupyter 笔记本和 tsv 文件保存在此文件夹中：文件夹链接。
我尝试过不同的策略，但结果很差。要么是低损失/低准确度，要么是高精度/高损失。想知道是否有人能发现我意识之外的明显问题。
Jupyter 笔记本代码在这里：
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras import regularizers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks 导入 EarlyStopping
来自 tensorflow.keras.layers 导入 BatchNormalization
导入 torch
导入 torch.nn 作为 nn

# 用于操作目录路径
导入 os

# 用于 python 的科学和矢量计算
导入 numpy 作为 np

# 绘图库
来自 matplotlib 导入 pyplot 作为 plt

# scipy 中的优化模块
来自 scipy 导入 o​​ptimize

# 将用于加载 MATLAB mat 数据文件格式
来自 scipy.io 导入 loadmat

# 告诉 matplotlib 在笔记本中嵌入绘图
%matplotlib inline

导入 pandas 作为 pd

来自 sklearn.metrics 导入 confused_matrix、ConfusionMatrixDisplay
来自 sklearn.utils.class_weight 导入 compute_class_weight
来自 sklearn.utils 导入 class_weight

导入 seaborn 作为 sns

df = pd.read_csv(&#39;shuffled_verses.tsv&#39;, sep=&#39;\t&#39;)
X = np.asarray(df.values[:3975, 6]).astype(&#39;str&#39;)
X_cv = np.asarray(df.values[3975:5300, 6]).astype(&#39;str&#39;)
X_test = np.asarray(df.values[5300:, 6]).astype(&#39;str&#39;)

# Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
X_sequences = tokenizer.texts_to_sequences(X)
X_cv_sequences = tokenizer.texts_to_sequences(X_cv)
X_test_sequences = tokenizer.texts_to_sequences(X_test)

# 添加填充
X_padded = pad_sequences(X_sequences, padding=&#39;post&#39;)
X_cv_padded = pad_sequences(X_cv_sequences, padding=&#39;post&#39;)
X_test_padded = pad_sequences(X_test_sequences, padding=&#39;post&#39;)

y = np.asarray(df.values[:3975, 1]).astype(&#39;float32&#39;)
y_cv = np.asarray(df.values[3975:5300, 1]).astype(&#39;float32&#39;)
y_test = np.asarray(df.values[5300:, 1]).astype(&#39;float32&#39;)

df = pd.read_csv(&#39;my_artists.tsv&#39;, sep=&#39;\t&#39;)
X_encoding = np.asarray(df.values[:, 0]).astype(&#39;str&#39;)
y_encoding = np.asarray(df.values[:, 1]).astype(&#39;float32&#39;)

# 超参数
embedding_dim = 100 # 从 100 更改为 200
rnn_units = 128 # 从 128 更改为 256
max_sequence_length = X_padded.shape[1] # 填充序列的长度
vocab_size = len(tokenizer.word_index) + 1 # 词汇表大小（为填充标记添加 1）
class_weights = class_weight.compute_class_weight(&#39;balanced&#39;, classes=np.unique(y_encoding), y=y)
class_weights = dict(enumerate(class_weights))
total_weight = sum(class_weights.values())
normalized_class_weights = {k: v / total_weight for k, v in class_weights.items()}

# 构建模型
model = Sequential()

# 嵌入层
model.add(Embedding(input_dim=vocab_size, # 词汇表的大小
output_dim=embedding_dim,
input_length=max_sequence_length,)) # 输入序列的长度

# RNN
model.add(Bidirectional(LSTM(rnn_units, return_sequences=True, kernel_regularizer=l2(0.006))))
model.add(Dense(70,activation=&#39;relu&#39;, kernel_regularizer=l2(0.006)))
model.add(Dropout(0.2))
model.add(Dense(45,activation=&#39;relu&#39;, kernel_regularizer=l2(0.006)))
model.add(Dropout(0.2))
model.add(Dense(num_classes,activation=&#39;softmax&#39;, kernel_regularizer=l2(0.001)))

# 编译
model.compile(optimizer=Adam(learning_rate=0.0001), 
loss=&#39;sparse_categorical_crossentropy&#39;, # 使用 &#39;categorical_crossentropy&#39; 进行多分类
metrics=[&#39;accuracy&#39;])

# EarlyStopping
early_stopping = EarlyStopping(
monitor=&#39;val_loss&#39;, # 监控验证损失
patience=3, # 如果连续 3 个时期没有改善，则停止
restore_best_weights=True # 恢复最佳模型权重
)

model.summary()

# 训练
history = model.fit(X_padded, y,
epochs=300,
batch_size=32,
validation_data=(X_cv_padded, y_cv),
class_weight=normalized_class_weights)
]]></description>
      <guid>https://stackoverflow.com/questions/79273048/lstm-rnn-tensorflow-language-prediction-model-stuck</guid>
      <pubDate>Wed, 11 Dec 2024 20:02:40 GMT</pubDate>
    </item>
    <item>
      <title>处理 Llama 3.2：3b-Instruct 模型中的令牌限制问题（最多 2048 个令牌）[关闭]</title>
      <link>https://stackoverflow.com/questions/79267003/handling-token-limit-issues-in-llama-3-23b-instruct-model-2048-tokens-max</link>
      <description><![CDATA[我正在使用 Llama 3.2:3b-instruct 模型并遇到以下错误：
此模型的最大上下文长度为 2048 个令牌。但是，您请求了 
2049 个令牌（消息中 1681 个，完成中 368 个）。

我理解这是由于超出令牌限制造成的，但我想知道：

是否有任何最佳实践或技术可以减少令牌使用量，而不会丢失消息或完成中的关键上下文？
]]></description>
      <guid>https://stackoverflow.com/questions/79267003/handling-token-limit-issues-in-llama-3-23b-instruct-model-2048-tokens-max</guid>
      <pubDate>Tue, 10 Dec 2024 04:19:17 GMT</pubDate>
    </item>
    <item>
      <title>时间序列运动捕捉数据的 PCA 图聚类问题</title>
      <link>https://stackoverflow.com/questions/79263104/pca-plot-clustering-issue-with-time-series-motion-capture-data</link>
      <description><![CDATA[我正在使用 PCA 对手部动作捕捉数据的时间序列数据集进行降维，并遇到了意外的聚类行为。以下是我的过程和我面临的问题的详细信息。

上下文：
我有一个使用智能手套捕捉的手部动作记录数据集，每个手指上有 4 个传感器。每个传感器提供：

位置值：X、Y、Z
旋转值：X、Y、Z、W

数据记录在单独的文件中，每个文件包含 10 次手势重复。这些重复随后被分割成单独的文件（每个文件 1 个重复），贴上标签，并合并成一个大型数据集。
在对数据进行标签编码和缩放后，我使用以下代码应用 PCA 进行降维：
# PCA 用于降维
pca_components = 3
pca = PCA(n_components=pca_components)
x_train_pca = pca.fit_transform(x_train_scaled)

为了可视化结果，我创建了一个 PCA 摘要图，其中每个数据段都表示为一个点。目标是查看每个手势的 10 个点的聚类。以下是总结 PCA 结果的代码：
# 将 PCA 结果转换为 DataFrame 以关联标签
x_train_pca_df = pd.DataFrame(x_train_pca, columns=[&#39;PC1&#39;, &#39;PC2&#39;, &#39;PC3&#39;])
x_train_pca_df[&#39;label&#39;] = y_train_data
x_train_pca_df[&#39;segment&#39;] = train_data[&#39;segment&#39;].values.ravel()

# 计算每个数据集的 PC1 和 PC2 的平均值（将每个数据集总结为一个点）
summary_train_points = x_train_pca_df.groupby([&#39;label&#39;, &#39;segment&#39;]).mean().reset_index()

以下是我用来绘制总结 PCA 的方法数据：
def plot_3d_pca_matplotlib(summary_points, title=&#39;3D PCA Plot&#39;):
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111,projection=&#39;3d&#39;)

# 对于每个唯一数据集（标签），分散点并分配图例条目
unique_labels = summary_points[&#39;label&#39;].unique()

for label in unique_labels:
# 过滤当前标签的数据
filtered_data = summary_points[summary_points[&#39;label&#39;] == label]

# 当前标签点的散点图
ax.scatter(filtered_data[&#39;PC1&#39;],
filtered_data[&#39;PC2&#39;],
filtered_data[&#39;PC3&#39;],
label=label)

ax.set_xlabel(&#39;PCA 组件 1&#39;)
ax.set_ylabel(&#39;PCA 组件 2&#39;)
ax.set_zlabel(&#39;PCA 组件 3&#39;)
ax.set_title(title)
ax.legend(title=&quot;Labels&quot;, loc=&quot;center left&quot;, bbox_to_anchor=(1.05, 0.7))

plt.subplots_adjust(left=0.05, right=0.75)
plt.show()


问题：
当我为相同手势记录一组新数据并绘制 PCA 结果时，我希望看到每个手势有 20 个点（10 个来自原始数据的点 + 10 个来自新数据的点）的聚类。
相反，PCA 图显示每个手势有两个独立的 10 点簇。这表明，即使手势相同，新数据也未与 PCA 空间中的原始数据对齐。

问题：
什么原因导致相同手势被分离为不同的簇？
可能与以下情​​况有关：

缩放过程？
传感器校准不一致？
在应用 PCA 之前是否需要对齐或对数据进行额外的预处理？
]]></description>
      <guid>https://stackoverflow.com/questions/79263104/pca-plot-clustering-issue-with-time-series-motion-capture-data</guid>
      <pubDate>Sun, 08 Dec 2024 18:31:23 GMT</pubDate>
    </item>
    <item>
      <title>将请求上下文从 FastAPI 传递到用于 OpenAI 集成的 Microsoft Semantic Kernel 插件</title>
      <link>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</link>
      <description><![CDATA[我正在 FastAPI 应用程序中将 Microsoft Semantic Kernel 与 OpenAI 集成。我有一个聊天/端点，我从请求中收到一个 session_id，我需要将此 session_id 与 openai_client 一起传递给插件。但是，我不确定如何在内核的执行过程中将 FastAPI 请求中的 session_id 正确传递给插件。
以下是设置内核和插件的相关代码：
# 内核和服务设置
kernel = Kernel()

execution_settings = AzureChatPromptExecutionSettings(tool_choice=&quot;auto&quot;)
execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={})

openai_client = OpenAI(api_key=api_key)
chat_completion_service = OpenAIChatCompletion(
ai_model_id=model_id, 
api_key=api_key, 
service_id=service_id 
)

# 添加服务和插件
kernel.add_service(chat_completion_service)
kernel.add_plugin(MovesPlugin(openai_client), plugin_name=&#39;MovesPlugin&#39;)

在我的 FastAPI 端点内，我想在调用内核进行聊天响应时将 session_id 传递给插件：
# 在 FastAPI 端点内
@app.post(&quot;/chat/&quot;)
async def chat(request: Request):
session_id = await request.json().get(&#39;session_id&#39;)

# 获取聊天完成服务
_chat_completion_service = kernel.get_service(type=ChatCompletionClientBase)

# 获取聊天完成响应
response = await _chat_completion_service.get_chat_message_content(
chat_history=chat_history,
kernel=kernel,
settings=execution_settings
)

return响应

如何将请求上下文 (session_id) 从 FastAPI 请求传递到 MovesPlugin，并确保它与语义内核执行中的 openai_client 一起正确使用？
如能得到任何指导或建议，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/79226790/passing-request-context-from-fastapi-to-microsoft-semantic-kernel-plugin-for-ope</guid>
      <pubDate>Tue, 26 Nov 2024 12:51:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在 nltk 中下载 punkt tokenizer？</title>
      <link>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</link>
      <description><![CDATA[我使用 pip install nltk 安装了 NLTK 库
pip install nltk

在使用库时
from nltk.tokenize import sent_tokenize 
sent_tokenize(text)

我收到此错误
LookupError: 
**************************************************************************
未找到资源 punkt。
请使用 NLTK 下载器获取资源：

&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.download(&#39;punkt&#39;)

有关更多信息，请参阅：https://www.nltk.org/data.html

尝试加载 tokenizers/punkt/english.pickle

搜索位置：
- &#39;C:\\Users\\adars/nltk_data&#39;
- &#39;C:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python310\\nltk_data&#39;
- &#39;C:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python310\\share\\nltk_data&#39;
- &#39;C:\\Users\\adars\\AppData\\Local\\Programs\\Python\\Python310\\lib\\nltk_data&#39;
- &#39;C:\\Users\\adars\\AppData\\Roaming\\nltk_data&#39;
- &#39;C:\\nltk_data&#39;
- &#39;D:\\nltk_data&#39;
- &#39;E:\\nltk_data&#39;
- &#39;&#39;

因此，为了解决此错误，我尝试了
import nltk
nltk.download(&#39;punkt&#39;)

但是我无法下载此包，因为每次运行此包时都会出现错误，提示
[nltk_data] 加载 punkt 时出错：&lt;urlopen 错误 [WinError 10060] A
[nltk_data] 连接尝试失败，因为连接方
[nltk_data] 在一段时间后未正确响应，或者
[nltk_data] 建立连接失败，因为连接的主机
[nltk_data] 未响应&gt;

请帮帮我]]></description>
      <guid>https://stackoverflow.com/questions/77131746/how-to-download-punkt-tokenizer-in-nltk</guid>
      <pubDate>Tue, 19 Sep 2023 04:36:59 GMT</pubDate>
    </item>
    <item>
      <title>收到错误“未创建保护程序，因为图中没有要恢复的变量”</title>
      <link>https://stackoverflow.com/questions/64182533/getting-the-error-saver-not-created-because-there-are-no-variables-in-the-graph</link>
      <description><![CDATA[我正在为我的项目使用此 tf OpenPose 模型实现。
从 models/graph/mobilenet_thin 目录，我尝试使用 graph.pb 文件，方法是使用
tf.saved_model.load() 函数将其加载到 tensorflow 中，但在加载模型时，我收到此警告：
未创建保存器，因为图中没有要恢复的变量 

因此，我无法在代码中稍后使用 model.predict() 函数。
如何解决此问题？
PS：我只有 .pb文件，而不是 vaiables 文件夹]]></description>
      <guid>https://stackoverflow.com/questions/64182533/getting-the-error-saver-not-created-because-there-are-no-variables-in-the-graph</guid>
      <pubDate>Sat, 03 Oct 2020 09:08:25 GMT</pubDate>
    </item>
    </channel>
</rss>