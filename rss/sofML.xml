<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 03 Oct 2024 12:33:18 GMT</lastBuildDate>
    <item>
      <title>GNU Octave 是多线程的吗？</title>
      <link>https://stackoverflow.com/questions/79050512/is-gnu-octave-multi-threaded</link>
      <description><![CDATA[根据这个老问题的答案，GNU Octave 似乎是一个单线程应用程序。
但是，我正在试验一个名为nnet的旧 Octave 神经网络包，并惊讶地发现我的 Octave 程序使用了笔记本电脑的所有 4 个核心。自从我链接的问题提出以来，情况有变化吗？GNU Octave 现在是多线程的吗？据我所知，我没有看到 nnet 内部有任何并行实现。
有关我的安装的一些信息：

我的操作系统是 Linux Mint 20
我的机器有 4 个处理单元（这是 nproc 在我的终端中显示的内容）
我的 Octave 版本是 5.2.0（如果这有区别的话，我正在使用 GUI）

我的代码相当简单，只导入了 nnet 包，没有其他内容。当我查看运行程序时的资源时，我看到所有核心都已使用（下面是 htop 屏幕截图）

这是我正在做的事情：
pkg load nnet

starttime = clock();

# 取自 http://matlab.izmiran.ru/help/toolbox/nnet/newff.html
Pr = -1:0.00005:1;
Tr = 3*sin(pi*Pr)-cos(pi*Pr);
Prmin = min(Pr);
Prmax = max(Pr);
net = newff([Prmin Prmax],[3 2 1],{&#39;tansig&#39;,&#39;logsig&#39;,&#39;purelin&#39;},&#39;trainlm&#39;);
[net] = train(net,Pr,Tr,[],[],[]);
[netoutput] = sim(net,Pr);

etime(clock(),starttime)

% 测试结果 
plot(Pr,Tr,&#39;b+&#39;);
hold on; 
plot(Pr,netoutput,&#39;r-&#39;);
hold off;
]]></description>
      <guid>https://stackoverflow.com/questions/79050512/is-gnu-octave-multi-threaded</guid>
      <pubDate>Thu, 03 Oct 2024 12:19:13 GMT</pubDate>
    </item>
    <item>
      <title>预测具有不规则时间序列的药品需求的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/79050144/best-approach-for-forecasting-pharmacy-item-demand-with-irregular-time-series</link>
      <description><![CDATA[我正在开展一个机器学习项目，旨在预测药品未来需求，从而优化库存管理。我的目标是利用历史销售数据预测某一特定商品未来一天、下一周和下个月的需求。以下是我的数据集和问题设置的详细信息：
数据集描述

开始日期：2023-05-02
结束日期：2024-09-30
行数：135,528
唯一项目名称：2,036

关键列：

date：交易时间戳（不规则间隔，不是每天或每小时都可用）。
item_name：售出商品的名称。
quantity：退货后的最终售出数量（目标变量）。
其他功能包括cost、rate、expiry_date和更多。

我每天都会收到新的销售数据，时间戳不规则，因此我计划将数据重新采样为每日间隔以进行预测。
我正在考虑使用 LSTM 模型，因为它能够捕获时间序列数据中的时间依赖性。但是，我不确定处理我的特定要求的最佳方法，包括不规则的时间戳和多个预测范围（每日、每周、每月）。
关键注意事项：

不规则的时间戳：数据没有一致的每日条目，我计划对其进行重新采样。这是否会引起偏差或影响模型的性能？
预测范围：我需要预测特定商品未来一天、下一周和下个月的需求。
模型效率：我正在处理一个相对较大的数据集，因此我需要一个能够平衡性能和准确性的模型。

问题：

根据我的数据集，LSTM 是否是预测特定商品未来需求（下一天、下一周和下个月）的最佳选择？
我应该如何处理时间序列数据中的不规则时间间隔？重新采样为每日间隔是否合适，或者有更好的技术吗？
在这种情况下，是否有其他模型或技术可能更适合预测多个时间范围？
]]></description>
      <guid>https://stackoverflow.com/questions/79050144/best-approach-for-forecasting-pharmacy-item-demand-with-irregular-time-series</guid>
      <pubDate>Thu, 03 Oct 2024 10:35:58 GMT</pubDate>
    </item>
    <item>
      <title>需要关于组合多个分类器预测的指导[关闭]</title>
      <link>https://stackoverflow.com/questions/79049883/need-guidance-on-combining-predictions-from-multiple-classifiers</link>
      <description><![CDATA[我目前正在开展一个涉及多个分类器的项目，每个分类器都针对一个类别子集进行训练。这些分类器旨在处理分类任务的不同方面，但在将它们的输出聚合为单个预测时，我面临着挑战。
例如，如果一个分类器负责区分类别 0 和 1，另一个分类器负责处理类别 2 和 3，当正确答案属于类别 1 时，我们如何有效地组合它们的结果？我们最初的方法是使用“其他”类别来指示输入不属于分类器分配的类别的情况，但这并没有产生预期的结果。
我们现在正在探索实施额外头部以检测分布外类别的可能性，但我们正在寻找更高效、更简化的解决方案。有没有人遇到过类似的问题，或者对有效聚合多个分类器的输出有什么建议？]]></description>
      <guid>https://stackoverflow.com/questions/79049883/need-guidance-on-combining-predictions-from-multiple-classifiers</guid>
      <pubDate>Thu, 03 Oct 2024 09:22:41 GMT</pubDate>
    </item>
    <item>
      <title>数据集类别对模型性能的分布影响[关闭]</title>
      <link>https://stackoverflow.com/questions/79049355/dataset-class-distribution-effect-for-model-perf</link>
      <description><![CDATA[数据集的类别分布是否直接影响模型的性能？例如，图 1 和图 2 中的数据集内容相同，但当我将类别组合在一起时，6、7、8 变为 4，2、4、5 变为 2。实际上，最合乎逻辑的做法是尝试看看，但我想问一下是否有针对此的论文式研究。在此处输入图片说明a在此处输入图片说明b
我认为一个类别过多会导致模型过度学习该类别，而不会学习其他类别。]]></description>
      <guid>https://stackoverflow.com/questions/79049355/dataset-class-distribution-effect-for-model-perf</guid>
      <pubDate>Thu, 03 Oct 2024 06:26:16 GMT</pubDate>
    </item>
    <item>
      <title>使用分层数据的无监督学习[关闭]</title>
      <link>https://stackoverflow.com/questions/79048804/unsupervised-learning-with-hierarchical-data</link>
      <description><![CDATA[假设有一组元素 A，并对 A 中的元素进行聚类/分析，是否可以在一组集合之间比较和聚类元素？例如，输入集合 A 后，输入集合 B、集合 C、D 等，然后找出元素 B3 是否与元素 A2 相似，或元素 C6 是否与 D7 相似。
让我用两个例子来说明。
示例 1：比较一组运动员的统计数据，看看在个人层面上哪些运动员最相似。
示例 2：比较一组团队，看看这些团队中的哪些运动员在团队中扮演着类似的角色。运动员 A 与 X 队的关系，就像运动员 B 与 Y 队的关系一样。如果运动员 A 提供更多传球，并且与 X 队中的其他球员相比覆盖范围更大，那么他很可能是一名组织核心。 Z 中的 B 也是如此。
因此，您要根据这些元素对各自集合的“意义”对不同集合中的单个元素进行聚类。]]></description>
      <guid>https://stackoverflow.com/questions/79048804/unsupervised-learning-with-hierarchical-data</guid>
      <pubDate>Thu, 03 Oct 2024 00:22:03 GMT</pubDate>
    </item>
    <item>
      <title>R 中 cox 模型的 c 指数[关闭]</title>
      <link>https://stackoverflow.com/questions/79048475/c-index-in-r-for-cox-model</link>
      <description><![CDATA[我最近在机器学习中为 Cox 模型运行了我的 R 代码。我已经在我的模型中进行了测试和训练。我想运行 c 索引一次进行测试，一次进行训练。我有代码，但根据 Cox 的 AUC，值太小了。我该如何解决这个问题？
这些是我的代码：
train_indices1 &lt;- sample(1:nrow(df6), 0.8 * nrow(df6))

training_data1 &lt;- df6[train_indices1, ]

testing_data1 &lt;- df6[-train_indices1, ]

res.cox11 &lt;- coxph(Surv(times,eventHFF) ~ AgeCategori + Gender + shoghl+ education + sokonat +taahol + BMIcategori+Hypertension+ DiabetesMellitus+ CAD +

HyperLipidemia+Sm​​oking+CKDDialysis +AtrialFibrillation+ StrokeTIA+ CTD+ ChemotherapyRadiotherapy +恶性肿瘤+急性心衰类型+HF类型+SBP+DBP+脾气猫+

心率猫+SPO2+NYHA等级+AF需要治疗+急性透析超滤+WRF，数据 = training_data1)

summary(res.cox11)

predicted_status1 &lt;- predict(res.cox11, newdata = testing_data1, type = &quot;risk&quot;)

predicted_status_binary1&lt;- ifelse(predicted_status1 &gt; 0.5, 1, 0)

confusion1 &lt;- chaosMatrix(factor(predicted_status_binary1), factor(testing_data1$eventHFF))

print(confusion1)

COX_MODEL1 &lt;- roc(testing_data1$eventHFF ~predicted_status1, plot = TRUE, print.auc = TRUE, main = &quot;ROC - COX 模型&quot;)

train_cindex &lt;- concordance(Surv(training_data1$times, training_data1$eventHFF) ~ predict(res.cox11, newdata = training_data1))$concordance

test_cindex &lt;- concordance(Surv(testing_data1$times, testing_data1$eventHFF) ~ predict(res.cox11, newdata = testing_data1))$concordance

cat(&quot;C-index for training data:&quot;, train_cindex, &quot;\n&quot;)

cat(&quot;C-index for testing data:&quot;, test_cindex, &quot;\n&quot;)

我该如何解决问题？
&gt; ci1 &lt;- ci.auc(COX_MODEL1)
95% CI：0.5169-0.7843 (DeLong)
&gt; cat(&quot;训练数据的 C 指数（重新拟合）：&quot;, train_cindex_refit, &quot;\n&quot;)
训练数据的 C 指数（重新拟合）：0.3102263 
&gt; cat(&quot;测试数据的 C 指数（重新拟合）：&quot;, test_cindex_refit, &quot;\n&quot;)
测试数据的 C 指数（重新拟合）：0.3152355 
]]></description>
      <guid>https://stackoverflow.com/questions/79048475/c-index-in-r-for-cox-model</guid>
      <pubDate>Wed, 02 Oct 2024 21:06:52 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Tesseract OCR 读取俄罗斯车牌？[关闭]</title>
      <link>https://stackoverflow.com/questions/79048374/how-can-i-use-tesseract-ocr-to-read-a-russian-license-plate</link>
      <description><![CDATA[我正在研究读取俄罗斯车辆的车牌。
Tesseract OCR 无法正确检测车牌。
我尝试了很多设置，例如（噪声过滤器、内核更改）

尝试了不同的内核，拉普拉斯、自定义、高斯
尝试了旋转、降低噪音
应用了一组字母/数字，但算法上将 2 检测为 Z

有什么建议可以尝试吗？
import cv2
import pytesseract
import numpy as np
from google.colab.patches import cv2_imshow

# 可选：如果 Tesseract 可执行文件路径尚未位于系统 PATH 中，请设置该路径
# pytesseract.pytesseract.tesseract_cmd = r&#39;C:\Program Files\Tesseract-OCR\tesseract.exe&#39;

# 函数用于围绕图像中心旋转图像
def rotate_image(image, angle):
(h, w) = image.shape[:2]
center = (w // 2, h // 2)
M = cv2.getRotationMatrix2D(center, angle, 1.0)
rotated = cv2.warpAffine(image, M, (w, h))
return rotated

# 函数用于根据图像矩计算倾斜角度
def compute_skew_angle(image):
coords = np.column_stack(np.where(image &gt; 0))
angle = cv2.minAreaRect(coords)[-1]
if angle &lt; -45:
angle = -(90 + angle)
else:
angle = -angle
return angle

# 加载 Haar 级联以检测车牌
plate_cascade = cv2.CascadeClassifier( &#39;/content/haarcascade_russian_plate_number.xml&#39;)

# 加载图像
img = cv2.imread(&#39;/content/vehicle1.png&#39;)
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
cv2_imshow(gray)

# 检测车牌
plates = plate_cascade.detectMultiScale(gray, 1.1, 4)

for (x, y, w, h) in boards:
# 在检测到的车牌周围绘制一个矩形
cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)

# 提取车牌区域
plate_img = gray[y:y + h, x:x + w]
cv2_imshow(plate_img)

# 将车牌图像调整为标准尺寸（例如 400x100）
scaled_plate = cv2.resize(plate_img, (4000, 1000))
cv2_imshow(scaled_plate)
# 应用高斯模糊以降低噪音
denoised_plate = cv2.GaussianBlur(scaled_plate, (5, 5), 0)
cv2_imshow(denoised_plate)
# 应用自适应阈值以获得更好的 OCR 性能
#adaptive_thresh_plate = cv2.adaptiveThreshold(denoised_plate, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
# cv2.THRESH_BINARY, 11, 2)
#cv2_imshow(adaptive_thresh_plate)
# 计算车牌倾斜角度
#skew_angle = compute_skew_angle(adaptive_thresh_plate)
#skew_angle = compute_skew_angle(denoised_plate)
# 旋转图像以校正倾斜
#rotated_plate = rotate_image(denoised_plate, skew_angle)
#cv2_imshow(rotated_plate)

# 对旋转和处理后的车牌执行 OCR
#plate_text = pytesseract.image_to_string(denoised_plate, config=&#39;--psm 6&#39;)
#enchancedimage 
kernel = cv2.getGaussianKernel(ksize=5, sigma=1.5) 
kernel = kernel @ kernel.T

enchanced_img = cv2.filter2D(denoised_plate, -1, kernel)
cv2_imshow(enchanced_img)
rotated_plate = rotate_image(denoised_plate, -2.5)
cv2_imshow(rotated_plate)
for i in range(3,14):
print(f&#39;PSM: {i}&#39;)
#plate_text =pytesseract.image_to_string(enchanced_img, 
#config = f&#39;--psm {i} --oem 3 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;)
plate_text =pytesseract.image_to_string(rotated_plate, 
config = f&#39;--psm {i} --oem 3 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;)

#plate_text =pytesseract.image_to_string(enchanced_img,config = f&#39;--psm {i}&#39;)
print(&quot;检测到的车牌号：&quot;, plate_text)

# 显示带检测框的原始图像
#cv2_imshow(img)
cv2.waitKey(0)
cv2.destroyAllWindows()


]]></description>
      <guid>https://stackoverflow.com/questions/79048374/how-can-i-use-tesseract-ocr-to-read-a-russian-license-plate</guid>
      <pubDate>Wed, 02 Oct 2024 20:30:40 GMT</pubDate>
    </item>
    <item>
      <title>在使用 TensorFlow 计算 SINR 时遇到问题，当涉及到复数时，TF 的数值稳定性如何？</title>
      <link>https://stackoverflow.com/questions/79048361/having-issue-with-calculating-sinr-using-tensorflow-how-numerically-stable-is-t</link>
      <description><![CDATA[我有用于训练神经网络的代码，对于具有多组 (G) 和多用户 (K) 的系统，损失为负 SINR。然而，当我计算 sinr 时，发生了一些奇怪的事情。
SINR公式是这样的：

这是我的代码：
def find_sinr_over_group(H, W):

    西格玛2 = 1
    # 计算 W 的 Hermitian 转置（共轭转置）
    W_H = tf.transpose(tf.math.conj(W), perm=[0, 2, 1]) # 形状：(batch_size, G, M)

    # 计算每组中每个用户的信号功率
    信号功率 = []
    for g in range(W.shape[-1]): # 循环每个组 g
        w_h_g = W_H[:, g, :] # 形状: (batch_size, M)
        h_g = H[:, :, :, g] # 形状: (batch_size, M, K)

        # 矩阵乘法计算信号功率
        s = tf.matmul(w_h_g[:, tf.newaxis, :], h_g) # 形状: (batch_size, 1, K)
        s = tf.squeeze(s, axis=1) # 形状: (batch_size, K)
        signal_power.append(s)

    signal_power = tf.stack(signal_power, axis=-1) # 形状：(batch_size, K, G)
    signal_power = tf.math.real(tf.math.multiply(signal_power, tf.math.conj(signal_power)))
    # signal_power = tf.math.abs(signal_power) ** 2 # 取绝对平方

    # 计算每组中每个用户的总功率
    总功率=[]
    for g in range(W.shape[-1]): # 循环每个组 g
        w_h = W_H[:, :, :] # 形状：(batch_size, G, M)
        h_g = H[:, :, :, g] # 形状: (batch_size, M, K)

        # 矩阵乘法计算总功率
        t = tf.matmul(w_h, h_g) # 形状：(batch_size, G, K)
        t = tf.math.real(tf.math.multiply(t, tf.math.conj(t)))
        # t = tf.math.abs(t) ** 2 # 形状：(batch_size, G, K)
        Total_power.append(tf.reduce_sum(t, axis=1)) # 对 G 求和得到 (batch_size, K)

    Total_power = tf.stack(total_power, axis=-1) # 形状: (batch_size, K, G)

    # 通过从总功率中减去信号功率来隔离干扰功率
    干扰功率 = 总功率 - 信号功率 # 形状：(batch_size, K, G)

    #添加噪声功率
    Interference_plus_noise_power = Interference_power + sigma2 # 添加噪声

    # 计算SINR
    sinr = signal_power / Interference_plus_noise_power # 形状：(batch_size, K, G)

    返回正弦值

但问题是，在某些情况下干扰功率是负的。这在数字上是不可能的，因为信号功率只是总功率的一个特例。有人知道为什么会发生这种情况以及我应该如何解决它吗？
正如你所看到的，一开始我使用了 tf.abd 函数，然后增加了 2 的幂。我认为这可能是 abs 的问题，所以我尝试通过共轭进行 myltiping 来获取信号的功率。但我仍然有这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79048361/having-issue-with-calculating-sinr-using-tensorflow-how-numerically-stable-is-t</guid>
      <pubDate>Wed, 02 Oct 2024 20:23:26 GMT</pubDate>
    </item>
    <item>
      <title>为什么 opencv.dnn.blobFromImage() 输出转换回 rgb 图像后包含 9 张灰度图像？</title>
      <link>https://stackoverflow.com/questions/79048116/why-does-opencv-dnn-blobfromimage-output-converted-back-to-rgb-image-contain-g</link>
      <description><![CDATA[据我所知，blobFromImage 将 img 形状：（宽度、高度、通道）转换为 4d 数组（n、通道、宽度、高度）。
因此，如果您传递 1/255 的 scale_factor。| 大小（640,640）据我所知，每个元素应计算为 RGB =&gt; R = R/255。| G = G/255。|...
值 = (U8 - 平均值) * scale_factor

基本上 minmax 在 0 到 1 之间标准化。
所以在 py。
我尝试将输出 blob/ndarray * 255 相乘。并重新整形为（640, 640, 3），看起来输出图像是一张包含 3 行 3 列灰度和略有不同的饱和度的 9 个图像的图像？这是我尝试过的，与上面的 255 示例输出相同。
 test = cv2.dnn.blobFromImage(img, 1.0/127.5, (640, 640), (127.5, 127.5, 127.5), swapRB=True)
t1 = test * 127.5
t2 = t1 + 127.5
cv2.imwrite(&quot;./test_output.jpg&quot;, t2.reshape((640, 640, 3)))

我一直在查看他们的 opencv repo
 subtract(images[i], mean, images[i]);
multiply(images[i], scalefactor, images[i]);

说实话，看起来在 OpenCV lib 中实现的方式相同，但想请教一下大家的意见。
另一个问题是，如果输入的是完整的 u8 rgb 值，为什么会变成灰度？
我尝试通过应用类似的公式将 4d ndarray 转换为匹配 blobFromImage 的输出。但输出并不相同。
我希望转换为 (1, 3, w, h) 的 ndarray 的图像减去平均值并乘以比例因子，当您转换回 (width, height, channel) 时，与 blobFromImage 的输出相同。
输入
输出]]></description>
      <guid>https://stackoverflow.com/questions/79048116/why-does-opencv-dnn-blobfromimage-output-converted-back-to-rgb-image-contain-g</guid>
      <pubDate>Wed, 02 Oct 2024 18:53:23 GMT</pubDate>
    </item>
    <item>
      <title>为什么 tf.keras.models.load_model 要重新构建模型？</title>
      <link>https://stackoverflow.com/questions/79047845/why-is-the-tf-keras-models-load-model-building-the-model-again</link>
      <description><![CDATA[ValueError: 顺序模型“sequential_2”已配置为使用
输入形状（None、224、224、3）。您无法使用 input_shape [None、224、224、3] 构建它

def load_model(model_path):
&quot;&quot;&quot;
从指定路径加载已保存的模型。
&quot;&quot;&quot;

tf.keras.config.enable_unsafe_deserialization()
print(f&quot;正在加载已保存的模型：{model_path}&quot;)
model = tf.keras.models.load_model(model_path)
return model

loaded_1000_image_model = load_model(&#39;/content/drive/MyDrive/Dog Vision/models/20241002-16491727887796-1000-images-mobilenetv2-Adam.h5&#39;)

我原本以为它会加载模型而不会出现任何错误，但由于某种原因，它给出了一个值错误，尽管我没有尝试重建或再次给出任何输入形状。]]></description>
      <guid>https://stackoverflow.com/questions/79047845/why-is-the-tf-keras-models-load-model-building-the-model-again</guid>
      <pubDate>Wed, 02 Oct 2024 17:18:38 GMT</pubDate>
    </item>
    <item>
      <title>关于我可以采取的方法来构建一个可以查找图像重复项和近似重复项的应用程序的建议[关闭]</title>
      <link>https://stackoverflow.com/questions/79046818/recommendation-on-the-approach-i-can-take-to-build-an-app-that-finds-image-dupli</link>
      <description><![CDATA[我正在制作一个应用程序，可以从您的图片库中查找重复图片和近似重复图片。
对于我当前的实现，我可以拍摄 2 张​​图片，从中提取嵌入（使用 CLIP 库），比较它的余弦相似度（对于精确重复）和欧几里得距离（对于压缩重复，如从聊天中下载的图片或图片的屏幕截图）。
对于我可以采取的下一步措施，有什么建议吗？我当前的实现如下所示：
main.py
从 PIL 导入图像
导入 torch
导入 numpy 作为 np
导入 cv2 作为 cv
导入 clip
从 sklearn.cluster 导入 KMeans

设备 = “cpu”
图像路径 = “assets/photos/aayush.jpeg”
image_path2 = &quot;assets/photos/aayushResized.png&quot;

# 加载 CLIP 模型和预处理函数
model, preprocess = clip.load(&quot;ViT-L/14&quot;, device, jit=False)

# 预处理图像并添加批处理维度
image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
image2 = preprocess(Image.open(image_path2)).unsqueeze(0).to(device)

print(&quot;处理后的图像数据类型为：&quot;, type(image))

def calculate_distance(embedding1, embedding2):
&quot;&quot;&quot; 计算两幅图像之间的欧几里得距离 &quot;&quot;&quot;
dist = torch.nn. functional.pairwise_distance(embedding1, embedding2)
return (dist)

def calculate_cosine_similarity(embedding1, embedding2):
&quot;&quot;&quot; 计算 2 个图像嵌入之间的余弦相似度 &quot;&quot;&quot;
return torch.nn. functional.cosine_similarity(embedding1, embedding2)

def resize_with_aspect_ratio(raw_image_path, new_width):
# 读取原始图像
original_image = cv.imread(raw_image_path)

# 计算纵横比
aspects_ratio = original_image.shape[1] / original_image.shape[0]

# 根据所需宽度确定新高度
determined_height = int(new_width / aspects_ratio)

# 调整图像大小
resized_image = cv.resize(original_image, (new_width, determined_height))
print(&quot;Newly Resized images shape: &quot;, resized_image.shape)
cv.imshow(&quot;Screen&quot;, resized_image)
cv.imwrite(&quot;aayushResized.png&quot;, resized_image)
cv.waitKey(0)
cv.destroyAllWindows()

return resized_image

def normalize(image):
# 计算每个通道的标准差
channel_stds = np.std(image, axis=(0, 1))
print(channel_stds)
# 通过将图像除以通道标准差来进行归一化
normalized_image = image / channel_stds
return (normalized_image)

def clutster(image):
kmeans = KMeans(n_clusters=2, random_state=0, n_init=&quot;auto&quot;).fit(X)

# 从模型中获取图像嵌入

with torch.no_grad():
emb1 = model.encode_image(image)
emb2 = model.encode_image(image2)

print(&quot;嵌入的数据类型为：&quot;, type(emb1), &quot;And &quot;, type(emb2))
# print(emb1, &quot;\n &quot;, emb2)

# 计算嵌入之间的余弦相似度
similarity = calculate_cosine_similarity(emb1, emb2)
distance = calculate_distance(emb1, emb2)

print(f&quot;余弦相似度：{similarity.item()} {type(similarity)}&quot;)
print(f&quot;欧几里得距离：{distance.item()} {type(similarity)}&quot;)


cluster.py
# 使用聚类算法 [k means] 将张量聚类在一起：
import os
import torch
import clip
from PIL import Image
import numpy as np
from sklearn.cluster import KMeans

device =“CPU”
model, preprocess = clip.load(&quot;ViT-L/14&quot;, device, jit=False)
# labels = [&quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;]
name_list = []
np_array = []

# 循环遍历图像文件夹
for photos in os.listdir(&quot;assets/photos&quot;):
image_path = os.path.join(&quot;assets/photos&quot;, photos)
# 预处理图像并获取其张量
image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
name_list.append(photos)
# 将张量的高维展平为类似2dim
numpyImage = np.array(image).flatten()
np_array.append(numpyImage)

k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(np_array)

# 获取 int 类标签（由 KMeans 自动生成）
cluster_labels = kmeans.labels_
image_cluster_map = {}
# 循环遍历图像并为其赋予标签
for i, labels in enumerate(cluster_labels):
# print(f&quot;Image {i} 属于集群 {labels}&quot;)
image_cluster_map[f&#39;image_{name_list[i]}&#39;] = labels

# 查看我们的最终集群
for i, (key, value) in enumerate(image_cluster_map.items()):
print(key, &quot;标记为：“，值）

]]></description>
      <guid>https://stackoverflow.com/questions/79046818/recommendation-on-the-approach-i-can-take-to-build-an-app-that-finds-image-dupli</guid>
      <pubDate>Wed, 02 Oct 2024 12:49:21 GMT</pubDate>
    </item>
    <item>
      <title>如何增强自定义视频录制 React.js 网站上的虚拟背景质量和背景分割</title>
      <link>https://stackoverflow.com/questions/78308293/how-to-enhance-virtual-background-quality-and-background-segmentation-on-a-custo</link>
      <description><![CDATA[我们目前正在开发一个视频录制平台。为了方便视频拍摄，我们已将 MediaRecorder API 集成到我们的系统中。
作为我们平台功能集的一部分，用户可以使用虚拟背景和背景模糊等功能增强他们的视频。
这些功能依靠 Google MediaPipe 的 API 进行分割，使我们能够准确地隔离框架内的主体。
然而，在将我们的输出与 Google Meet 和 Microsoft Teams 等成熟平台进行比较后，我们发现质量存在明显差距。尽管我们努力实施先进的分割技术，但结果仍未达到行业标准，特别是在视觉保真度和准确性方面。
我们探索了各种途径并尝试了不同的技术。
其中包括TensorFlow（一种开源机器学习框架）、ML Kit（由 Google 提供的机器学习 SDK）和Video-SDK（一种全面的视频处理工具包）。尽管我们付出了这些努力，但我们的输出质量和一致性仍未达到预期水平。
如果您有任何关于增强虚拟背景的建议或关于用于虚拟背景集成的替代开源或付费工具的建议，我们将不胜感激？]]></description>
      <guid>https://stackoverflow.com/questions/78308293/how-to-enhance-virtual-background-quality-and-background-segmentation-on-a-custo</guid>
      <pubDate>Thu, 11 Apr 2024 05:10:10 GMT</pubDate>
    </item>
    <item>
      <title>虚拟背景渲染不正确：寻求有关 WebRTC 实施的指导</title>
      <link>https://stackoverflow.com/questions/78087416/virtual-backgrounds-not-rendering-properly-seeking-guidance-on-webrtc-implement</link>
      <description><![CDATA[我正在使用 RobustVideoMatting 库 (GitHub 存储库) 将虚拟背景支持集成到视频通话应用程序中。前景分割成功，但在 WebRTC 流式传输期间，我面临着背景渲染的挑战，我无法将背景图像应用于分割的视频。
尝试的方法：
方法 1 - CSS 样式：
const bg = `url(&#39;${this.bg}&#39;) center center / cover`;
this.canvas.style.background = bg;

方法 2 - HTMLImageElement：
const backgroundImage = new Image();
backgroundImage.src = &quot;https://d..........................jpg&quot;;
backgroundImage.onload = () =&gt; {
canvasRef.current.getContext(&quot;2d&quot;).drawImage(backgroundImage, 0, 0, 640, 480);
}

请求协助：
我正在寻求解决此问题的指导。如果有人有虚拟背景和 WebRTC 方面的经验，或者在使用 RobustVideoMatting 库时有具体注意事项，我们将不胜感激。
GitHub 存储库： PeterL1n/RobustVideoMatting
CodePen 链接 CodePen 参考
需要将背景图像或视频应用于分段视频帧。]]></description>
      <guid>https://stackoverflow.com/questions/78087416/virtual-backgrounds-not-rendering-properly-seeking-guidance-on-webrtc-implement</guid>
      <pubDate>Fri, 01 Mar 2024 11:28:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 google colab 和 google drive 时出现输入/输出错误</title>
      <link>https://stackoverflow.com/questions/54973331/input-output-error-while-using-google-colab-with-google-drive</link>
      <description><![CDATA[当我使用 google colab 时，我会多次随机收到此错误，有时它可以工作，有时则不行 
OSError: [Errno 5] 输入/输出错误

当我与 google drive 交互时是否会发生此错误？
是否有针对此错误的解决方案 ]]></description>
      <guid>https://stackoverflow.com/questions/54973331/input-output-error-while-using-google-colab-with-google-drive</guid>
      <pubDate>Sun, 03 Mar 2019 20:18:56 GMT</pubDate>
    </item>
    <item>
      <title>文件名中的键值对是否有标准的文件命名约定？[关闭]</title>
      <link>https://stackoverflow.com/questions/10087079/is-there-a-standard-file-naming-convention-for-key-value-pairs-in-filename</link>
      <description><![CDATA[我有多个以它们所包含的内容命名的数据文件。例如
machine-testM_pid-1234_key1-value1.log

键和值由 - 和 _ 分隔。有没有更好的语法？有没有自动读取这些文件/文件名的解析器？
这里的想法是文件名是人类和机器可读的。]]></description>
      <guid>https://stackoverflow.com/questions/10087079/is-there-a-standard-file-naming-convention-for-key-value-pairs-in-filename</guid>
      <pubDate>Tue, 10 Apr 2012 10:32:32 GMT</pubDate>
    </item>
    </channel>
</rss>