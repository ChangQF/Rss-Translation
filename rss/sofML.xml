<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 07 Dec 2024 21:15:50 GMT</lastBuildDate>
    <item>
      <title>如何正确训练 GAN 模型？</title>
      <link>https://stackoverflow.com/questions/79261315/how-to-properly-train-a-gan-model</link>
      <description><![CDATA[我尝试在我的数据库中训练 ViT-GAN 模型（来自此 repo），其中我有图像作为输入和输出。输入图像是路径规划问题的 PNG 地图。红色通道是障碍物地图，绿色和蓝色是带有起点/目标的一个像素。输出图像将是红色通道上的规划路径，这就是我试图教给模型的内容。来自 repo 的模型在所包含的示例上运行良好，这是一个比我的问题复杂得多的问题，所以我自然而然地认为这个网络可以解决我的问题而无需进一步调整。
我的问题是，即使不接触网络，只需将训练数据替换为挖掘数据也会完全搞乱输出。我尝试以多种方式调整网络，但结果始终保持不变 -&gt;要么是任何给定输入的相同结构化垃圾输出（黑色上的白色补丁），要么是黑色背景上的一些嘈杂的 rgb 补丁。我就是无法让它学习预期的输出。

左边是给定的输入，右边是预期的输出。中间是网络的输出。我读过关于训练 GAN 的文章，我发现它们很不稳定，很难找到正确的结构/参数集，但我就是不知道下一步该怎么做。我已经尝试过的方法：

通过删除一些 transformer/卷积层来缩小网络
减少过滤器的数量，这样我的问题就没那么复杂了
调整生成器/鉴别器的学习率 -&gt; 一起和单独调整
按照另一篇类似文章的建议，将鉴别器输出层的激活函数更改为 sigmoid
删除所有 ReLU 激活函数，并用 LeakyReLU 替换它们，以解决可能的梯度消失问题
将 Wasserstein 损失添加到鉴别器损失函数，以避免模型崩溃
将生成器损失函数中像素的平均差异更改为总和差异（以更多地惩罚全黑输出）
使用偏差来避免梯度消失
将过滤器的随机初始化器从 (0.0, 0.02) 更改为 -&gt; (0.0, 1.0) 使初始过滤器更加多样化
更改 lambda、批量大小、ff_dim、头数、补丁大小、嵌入暗淡、投影暗淡参数
训练 200 个时期和 20 个时期。全部针对相同的输出结构。
在数据集的较小部分（400 张图像）和较大部分（1000 张和 3000 张图像）上进行训练

我知道这个问题也可以用其他（可能更简单）网络来解决，但我想让这个 GAN 工作，并了解为什么它一开始就不起作用。我将我当前的更改状态添加到此repo。我不知道下一步该去哪里。如果有人有建议，请随时分享。如果您建议调整某些参数，请写一个具体的值。如果我忘记了描述中的任何内容，请告诉我。]]></description>
      <guid>https://stackoverflow.com/questions/79261315/how-to-properly-train-a-gan-model</guid>
      <pubDate>Sat, 07 Dec 2024 20:09:37 GMT</pubDate>
    </item>
    <item>
      <title>每次使用 Keras 加载模型后，是否都需要调用 model.fit() 函数</title>
      <link>https://stackoverflow.com/questions/79261060/do-i-need-to-call-model-fit-function-every-time-after-loading-the-model-using</link>
      <description><![CDATA[我正在使用 Keras 创建模型：
def my_model():
model = Sequential()
model.add(Conv2D(60, (5,5), input_shape=(32,32,1),activation=&#39;relu&#39;))
model.add(Conv2D(60, (5,5),activation=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(30, (3,3),activation=&#39;relu&#39;))
model.add(Conv2D(30, (3,3),activation=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2,2)))
#model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(500,activation=&#39;relu&#39;))
model.add(Dropout(0.5))
model.add(Dense(num_classes,activation=&#39;softmax&#39;))
## 编译模型
model.compile(Adam(learning_rate = 0.001),loss = &#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
return model

现在保存模型并进行训练：
model = my_model()
print(model.summary())
model.save(&#39;my_model.h5&#39;)
history = model.fit(X_train, y_train,epochs = 10,validation_data=(X_val, y_val),batch_size = 400,verbose = 1,shuffle = 1)

#### 现在预测一些图像
pred = model.predict(img)
prediction = np.argmax(pred,axis=1)
print(&quot;predicted sign: &quot;+ str(prediction))

&#39;model.fit()&#39; 函数是最耗 CPU 的工作。在功能较弱的平台上，训练需要大量时间。是否可以保存训练好的模型并将其用于预测？
我知道有办法加载保存的模型：
from tensorflow.keras.models import load_model 
model = load_model(&#39;my_model.h5&#39;) 

但是我仍然必须运行耗费 CPU 的 &#39;model.fit()&#39;。有没有办法避免使用 model.fit() 并加载模型并用于预测？]]></description>
      <guid>https://stackoverflow.com/questions/79261060/do-i-need-to-call-model-fit-function-every-time-after-loading-the-model-using</guid>
      <pubDate>Sat, 07 Dec 2024 17:33:44 GMT</pubDate>
    </item>
    <item>
      <title>机器学习试图将现实世界的数据融入数学方程式[关闭]</title>
      <link>https://stackoverflow.com/questions/79260639/machine-learning-trying-to-fit-real-world-data-into-mathematical-equations</link>
      <description><![CDATA[例如，在监督学习中，我们根据现实生活中的数据训练模型，并尝试找到最合适的数学表示。我的问题是，为什么现实生活中的数据（似乎是随机的）会符合数学方程式 y=mx+c、y=c1x1+c2x2 或任何复杂形式？这是因为现实世界中的大多数事物都可以用数学来概括吗？]]></description>
      <guid>https://stackoverflow.com/questions/79260639/machine-learning-trying-to-fit-real-world-data-into-mathematical-equations</guid>
      <pubDate>Sat, 07 Dec 2024 12:50:57 GMT</pubDate>
    </item>
    <item>
      <title>Llama3 8B 在 Tesla Core GPU 上生成文本非常慢</title>
      <link>https://stackoverflow.com/questions/79260246/llama3-8b-generating-text-very-slow-on-tesla-core-gpu</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79260246/llama3-8b-generating-text-very-slow-on-tesla-core-gpu</guid>
      <pubDate>Sat, 07 Dec 2024 08:25:29 GMT</pubDate>
    </item>
    <item>
      <title>想要使用 tf-idf 或类似的矢量化器进行垃圾邮件分析 Cnn，而不是标记器 [关闭]</title>
      <link>https://stackoverflow.com/questions/79260117/want-to-use-tf-idf-or-simillar-vectorizer-for-my-spam-email-analysis-cnn-instead</link>
      <description><![CDATA[因此，当我使用 tokenizer 修改 X test 和 X train 时，我的 CNN 项目成功运行
X_train = X_train.ravel()
X_test = X_test.ravel()
#
#y_train = y_train.ravel().tolist()
#y_test =y_test.ravel().tolist()
tokenizer = text.Tokenizer(num_words=config.vocab_size)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_matrix(X_train)

X_test = tokenizer.texts_to_matrix(X_test)

X_train =sequence.pad_sequences(X_train,maxlen=config.max_len)
X_test =序列。pad_sequences(X_test,maxlen=config.max_len)

但是当我尝试使用 Tf-idf 向量器时，由于某种原因它无法工作
X_train = X_train.ravel()
X_test = X_test.ravel()
#
#y_train = y_train.ravel().tolist()
#y_test =y_test.ravel().tolist()

X_train = tfidf.fit_transform(X_train)

X_test = tfidf.fit_transform(X_test)

X_train = 序列。pad_sequences(X_train,maxlen=config.max_len)
X_test = 序列。pad_sequences(X_test,maxlen=config.max_len)

有没有可能解决这个问题？我之所以这样想，是因为使用该模型预测电子邮件的输出网站使用 Tf-idf 矢量化器来分析输入文本]]></description>
      <guid>https://stackoverflow.com/questions/79260117/want-to-use-tf-idf-or-simillar-vectorizer-for-my-spam-email-analysis-cnn-instead</guid>
      <pubDate>Sat, 07 Dec 2024 07:02:00 GMT</pubDate>
    </item>
    <item>
      <title>如何在多项任务上训练 LSTM 模型并使用它根据初始条件预测完整轨迹？</title>
      <link>https://stackoverflow.com/questions/79259872/how-to-train-an-lstm-model-on-multiple-missions-and-use-it-to-predict-full-traje</link>
      <description><![CDATA[我正在为机器人应用训练 LSTM 模型（我一直在测试不同的 ML）。我有来自多个“任务”的数据，每个任务包含 900 个时间步骤。每个时间步骤包括：
输入 (theta_vector)：具有 4 个值 [theta_0、theta_cmd、theta_dot_0、thetadot_cmd] 的状态向量。
目标 (f_value)：与时间步骤相对应的单个控制输入值。
训练：
训练 LSTM 模型以学习每个任务中的时间依赖性（即，theta_vector 如何演变并与 900 个时间步骤中的 f_value 相对应）。
通过对所有任务的数据进行训练，将模型推广到各个任务。
在训练期间，我提供了所有任务中所有时间步骤的数据（例如，每个时间步骤的 theta_vector 和相应的 f_value）。
推理：
在推理时，仅提供初始条件（第一个时间步骤的 theta_vector）。
预测所有 900 个时间步骤的完整控制输入轨迹（f_value）。
模型应该生成轨迹而不需要显式动力学建模（时间关系应该从训练数据中学习）。
问题：
我不确定如何做到这一点，也不知道如何在 PyTorch 中正确构建训练和推理管道。具体来说：
推理：
给定初始条件，如何设置 LSTM 以生成 900 个时间步骤的完整控制输入轨迹？我是否应该将预测的 f_value 迭代地反馈到模型中以供下一个时间步骤使用？
这有可能实现吗？即以时间步骤的方式进行训练，然后输入初始条件并接收结果（根据一些论文，Transformers 可以做到这一点，但我没有找到有关如何实现它的任何详细信息）？
我们是否也将使用 Transformers 来代替 LSTM 来查看它们的性能差异？以 transfermer 的方式放置它是否更容易？
我尝试过的方法：
我创建了一个 PyTorch DataLoader 来批量处理任务，并以 [f_value] 为目标，在 [theta_vector] 序列上训练 LSTM。
为了进行推理，我尝试将 initial_condition 作为输入，并使用模型迭代生成轨迹。但是，我在维护正确的输入/输出形状和更新后续时间步骤的隐藏状态方面遇到了问题。]]></description>
      <guid>https://stackoverflow.com/questions/79259872/how-to-train-an-lstm-model-on-multiple-missions-and-use-it-to-predict-full-traje</guid>
      <pubDate>Sat, 07 Dec 2024 02:56:41 GMT</pubDate>
    </item>
    <item>
      <title>在每个表上训练单独的 ML 模型与训练一个大型模型</title>
      <link>https://stackoverflow.com/questions/79259614/training-a-separate-ml-model-on-each-table-versus-training-one-large-model</link>
      <description><![CDATA[我有四组数据：每组都与不同的指纹向量相关。我正尝试通过使用 ML 来检测机器人。我使用 pandas、sklearn 和 sqlite 数据库来实现这一点，目前使用回归模型对用户进行 0 到 1 的评分。
我知道这不是最佳实践，但我可以在每个数据表上独立训练一个模型吗？在 python 中并行模型是否可行？有人告诉我 python 不能很好地支持并行性
到目前为止，我一直在合并的数据集上训练一个模型，但这导致大量数据缺失。很多时候，用户只获得其中一个向量的数据。即使我使用像 Catboost 这样的算法作为我的 ml 算法，结果仍然很差。]]></description>
      <guid>https://stackoverflow.com/questions/79259614/training-a-separate-ml-model-on-each-table-versus-training-one-large-model</guid>
      <pubDate>Fri, 06 Dec 2024 23:10:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow 进行硬币分类</title>
      <link>https://stackoverflow.com/questions/79259493/struggling-with-coin-classification-using-tensorflow</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79259493/struggling-with-coin-classification-using-tensorflow</guid>
      <pubDate>Fri, 06 Dec 2024 22:02:22 GMT</pubDate>
    </item>
    <item>
      <title>使用共生矩阵时如何可视化 Kmeans 聚类？</title>
      <link>https://stackoverflow.com/questions/79259076/how-to-visualise-kmeans-clusters-when-using-cooccurrence-matrices</link>
      <description><![CDATA[我正在尝试使用 Kmeans 对单词进行聚类。我有一个大型文档，我首先使用 NLTK RegexpTokenizer，然后根据字数、长度进行过滤并删除停用词。接下来，我构建一个共现矩阵并使用它来训练 Kmeans 模型。最后，我使用轮廓分数测试它的性能。
我想可视化这些集群。这通常似乎是使用散点图和标签来完成的。当它目前是一个 N x N 矩阵（N 是唯一单词的数量）时，我如何将这个共现矩阵简化为 x 和 y？我尝试过使用 PCA：
plt.figure()
pca_2d = PCA(n_components=2)
reduced = pca_2d.fit_transform(mat)
newKm = KMeans(n_clusters=3)
labels = newKm.fit_predict(reduced)
plt.scatter(reduced[:, 0], Reduced[:, 1], c=labels) # 选择第 0 列（所有行）作为 x 坐标，第 1 列（所有行）作为 y 坐标。然后对标签进行聚类。
plt.title(&quot;K-means Clustering&quot;)
plt.show()

但不确定这是最佳或正确的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/79259076/how-to-visualise-kmeans-clusters-when-using-cooccurrence-matrices</guid>
      <pubDate>Fri, 06 Dec 2024 18:55:00 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 Pytorch DDP 中获取空闲端口的问题？</title>
      <link>https://stackoverflow.com/questions/79259010/how-to-solve-the-issue-with-getting-free-ports-in-pytorch-ddp</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79259010/how-to-solve-the-issue-with-getting-free-ports-in-pytorch-ddp</guid>
      <pubDate>Fri, 06 Dec 2024 18:30:52 GMT</pubDate>
    </item>
    <item>
      <title>下载模型时可教机器不准确[关闭]</title>
      <link>https://stackoverflow.com/questions/79258040/teachable-machine-is-inaccurate-when-model-is-downloaded</link>
      <description><![CDATA[我正在为我的论文做一个应用程序，我们决定使用 Teachable Machine 进行图像识别，我对其进行了测试并查看了一些相关内容，它看起来不错，所以我们决定这样做。现在，当我训练我的一些模型时，我在网络上尝试它非常准确，但是当我将其导出到 tensorflow lite（浮点）并在 Android Studio 中编写代码时，它会错误分类图片，这些图片也在训练集中。现在我的问题是，有没有办法解决这个问题？如果没有，有没有简单的方法来训练我自己的模型？
public void classifyImage(Bitmap image) {
try {
ModelUnquant model = ModelUnquant.newInstance(getApplicationContext());

TensorBuffer inputFeature0 = TensorBuffer.createFixedSize(new int[]{1, 224, 224, 3}, DataType.FLOAT32);
ByteBuffer byteBuffer = ByteBuffer.allocateDirect(4 * imageSize * imageSize * 3);
byteBuffer.order(ByteOrder.nativeOrder());

int[] intValues = new int[imageSize * imageSize];
image.getPixels(intValues, 0, image.getWidth(), 0, 0, image.getWidth(), image.getHeight());

int pixel = 0;
for (int i = 0; i &lt; imageSize; i++) {
for (int j = 0; j &lt; imageSize; j++) {
int val = intValues[pixel++];
byteBuffer.putFloat(((pixel &gt;&gt; 16) &amp; 0xFF) / 255.0f); // 红色
byteBuffer.putFloat(((pixel &gt;&gt; 8) &amp; 0xFF) / 255.0f); // 绿色
byteBuffer.putFloat((pixel &amp; 0xFF) / 255.0f); // 蓝色
}
}

inputFeature0.loadBuffer(byteBuffer);

ModelUnquant.Outputs 输出 = model.process(inputFeature0);
TensorBuffer 输出Feature0 = 输出.getOutputFeature0AsTensorBuffer();

float[] 置信度 = outputFeature0.getFloatArray();
int maxPos = 0;
float maxConfidence = 0;
for (int i = 0; i &lt; 置信度.length; i++) {
if (置信度[i] &gt; maxConfidence) {
maxConfidence = 置信度[i];
maxPos = i;
}
}

String[] classes = {&quot;食物&quot;, &lt;塑料瓶&quot;, &lt;面罩&quot;, &lt;塑料餐具&quot;, &lt;注射器&quot;};
StringdetectedObject = classes[maxPos];

// 检查置信度是否低于 35%
if (maxConfidence &lt;= 0.35f) {
// 如果置信度低，则显示&quot;无法分类&quot;消息
detectedObject = &quot;未知&quot;;
}

showBottomSheet(detectedObject, maxConfidence);

String s = &quot;&quot;;
for (int i = 0; i &lt; classes.length; i++) {
s += String.format(&quot;%s: %.1f%%\n&quot;, classes[i],confidences[i] * 100);
}

// 将结果打印到控制台日志
Log.d(&quot;ClassificationResult&quot;, s);

model.close();
} catch (IOException e) {
e.printStackTrace();
}
}

上面的代码用于对图像进行分类，如果低于 75%，我会生成一个错误，但它总是低于，所以我把它设为 35%。该模型分别由 12、98、100、70、90 组成，epoch 为 68，默认批量大小 (64)，默认学习率为 0.001。我尝试将其从低 epoch 调整为高，学习率为 0.0005，下载时不准确度仍然相同。]]></description>
      <guid>https://stackoverflow.com/questions/79258040/teachable-machine-is-inaccurate-when-model-is-downloaded</guid>
      <pubDate>Fri, 06 Dec 2024 12:46:19 GMT</pubDate>
    </item>
    <item>
      <title>机器学习预测项目：在 Localhost 上的 Flask 应用程序中提交预测表单后出现“ValueError”</title>
      <link>https://stackoverflow.com/questions/79257966/machine-learning-prediction-project-getting-valueerror-after-submitting-the-p</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79257966/machine-learning-prediction-project-getting-valueerror-after-submitting-the-p</guid>
      <pubDate>Fri, 06 Dec 2024 12:21:15 GMT</pubDate>
    </item>
    <item>
      <title>ModuleNotFoundError：没有名为“anomalib.engine”的模块</title>
      <link>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</link>
      <description><![CDATA[# 导入所需模块

从 anomalib.data 导入 MVTec
从 anomalib.models 导入 Patchcore
从 anomalib.engine 导入 Engine

错误：
ModuleNotFoundError：没有名为“anomalib.engine”的模块

我正在尝试运行它......已按照库安装并看到
https://anomalib.readthedocs.io/en/latest/markdown/get_started/anomalib.html
我认为这要么是因为引擎已被修改，要么已被库删除......
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77930973/modulenotfounderror-no-module-named-anomalib-engine</guid>
      <pubDate>Sat, 03 Feb 2024 05:25:02 GMT</pubDate>
    </item>
    <item>
      <title>如何找到张量的前 $n$ 个最大值的索引？</title>
      <link>https://stackoverflow.com/questions/77919632/how-to-find-the-indexes-of-the-first-n-maximum-values-of-a-tensor</link>
      <description><![CDATA[我知道 torch.argmax(x, dim = 0) 返回 x 中沿维度 0 的第一个最大值的索引。但是有没有一种有效的方法来返回前 n 个最大值的索引？如果有重复的值，我也想要 n 个索引中的那些值的索引。
举一个具体的例子，比如 x=torch.tensor([2, 1, 4, 1, 4, 2, 1, 1])。我想要一个函数
generalized_argmax(xI torch.tensor, n: int)

这样
generalized_argmax(x, 4)
在此示例中返回 [0, 2, 4, 5]。]]></description>
      <guid>https://stackoverflow.com/questions/77919632/how-to-find-the-indexes-of-the-first-n-maximum-values-of-a-tensor</guid>
      <pubDate>Thu, 01 Feb 2024 11:05:23 GMT</pubDate>
    </item>
    <item>
      <title>joblib.load __main__ 属性错误</title>
      <link>https://stackoverflow.com/questions/49621169/joblib-load-main-attributeerror</link>
      <description><![CDATA[我开始深入研究使用 Flask 将预测模型部署到 Web 应用程序，但不幸的是，我卡在了起跑线上。 
我做了什么：
我在 model.py 程序中腌制了我的模型：
import numpy as np
from sklearn.externals import joblib

class NeuralNetwork():
&quot;&quot;&quot;
两层（隐藏）神经网络模型。
第一层和第二层包含相同数量的隐藏单元
&quot;&quot;&quot;
def __init__(self, input_dim, units, std=0.0001):
self.params = {}
self.input_dim = input_dim

self.params[&#39;W1&#39;] = np.random.rand(self.input_dim, units)
self.params[&#39;W1&#39;] *= std
self.params[&#39;b1&#39;] = np.zeros((units))

self.params[&#39;W2&#39;] = np.random.rand(units, units)
self.params[&#39;W2&#39;] *= std * 10 # 补偿消失梯度
self.params[&#39;b2&#39;] = np.zeros((units))

self.params[&#39;W3&#39;] = np.random.rand(units, 1)
self.params[&#39;b3&#39;] = np.zeros((1,))

model = NeuralNetwork(input_dim=12, units=64)

##### 就在这里 ##############
joblib.dump(model, &#39;demo_model.pkl&#39;)

然后我按照本教程在与我的 demo_model.pkl 相同的目录中创建了一个 api.py 文件 (https://blog.hyperiondev.com/index.php/2018/02/01/deploy-machine-learning-models-flask-api/):
import flask
from flask import Flask, render_template, request
from sklearn.externals import joblib

app = Flask(__name__)

@app.route(&quot;/&quot;)
@app.route(&quot;/index&quot;)
def index():
return flask.render_template(&#39;index.html&#39;)

# 创建预测端点（HTTP POST 请求）
@app.route(&#39;/predict&#39;, methods=[&#39;POST&#39;])
def make_prediction():
if request.method == &#39;POST&#39;:
return render_template(&#39;index.html&#39;, label=&#39;3&#39;)

if __name__ == &#39;__main__&#39;:
# APP 运行时加载模型 ####
model = joblib.load(&#39;demo_model.pkl&#39;)
app.run(host=&#39;0.0.0.0&#39;, port=8000, debug=True)

我还在同一目录中创建了一个 templates/index.html 文件，其中包含以下信息：
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;NN 模型作为 Flask API&lt;/title&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;波士顿房价预测器&lt;/h1&gt;
&lt;form action=&quot;/predict&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt;
&lt;input type=&quot;file&quot; name=&quot;image&quot; value=&quot;Upload&quot;&gt;
&lt;input type=&quot;submit&quot; value=&quot;Predict&quot;&gt; {% if label %} {{ label }} {% endif %}
&lt;/form&gt;
&lt;/body&gt;

&lt;/html&gt;

running:
&gt;&gt; python api.py

使用 pickler 时出现错误：
回溯（最近一次调用）：
文件“api.py”，第 22 行，位于 &lt;module&gt;
model = joblib.load(&#39;model.pkl&#39;)
文件“C:\Users\joshu\Anaconda3\lib\site-packages\sklearn\externals\joblib\numpy_pickle.py”，第 578 行，在 load 中
obj = _unpickle(fobj, filename, mmap_mode)
文件“C:\Users\joshu\Anaconda3\lib\site-packages\sklearn\externals\joblib\numpy_pickle.py”，第 508 行，在 _unpickle 中
obj = unpickler.load()
文件“C:\Users\joshu\Anaconda3\lib\pickle.py”，第 1043 行，在 load 中
dispatch[key[0]](self)
文件“C:\Users\joshu\Anaconda3\lib\pickle.py”，第 1342 行，在 load_global 中
klass = self.find_class(module, name)
文件“C:\Users\joshu\Anaconda3\lib\pickle.py”，第 1396 行，在 find_class 中
return getattr(sys.modules[module], name)
AttributeError: 模块“__main__”没有属性“NeuralNetwork”

为什么程序的主模块与我的 NeuralNetwork 模型有关？我现在很困惑……任何建议都将不胜感激。
更新：
向我的 api.py 程序添加类定义 class NeuralNetwork(object): pass 修复了该错误。 
import flask
from flask import Flask, render_template, request
from sklearn.externals import joblib

class NeuralNetwork(object):
pass

app = Flask(__name__)

如果有人愿意向我解释发生了什么，我将不胜感激！ ]]></description>
      <guid>https://stackoverflow.com/questions/49621169/joblib-load-main-attributeerror</guid>
      <pubDate>Tue, 03 Apr 2018 02:02:56 GMT</pubDate>
    </item>
    </channel>
</rss>