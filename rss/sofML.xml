<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 22 Sep 2024 03:23:12 GMT</lastBuildDate>
    <item>
      <title>为什么我的训练损失总是为零？！我在浪费时间吗？</title>
      <link>https://stackoverflow.com/questions/79010666/why-the-heck-is-my-training-loss-always-zero-am-i-wasting-my-time</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79010666/why-the-heck-is-my-training-loss-always-zero-am-i-wasting-my-time</guid>
      <pubDate>Sun, 22 Sep 2024 00:25:31 GMT</pubDate>
    </item>
    <item>
      <title>进行迁移学习后加载 Keras 模型时出现 ValueError</title>
      <link>https://stackoverflow.com/questions/79010662/valueerror-while-loading-my-keras-model-after-doing-transfer-learning</link>
      <description><![CDATA[我开发了一种手语识别模型，使用 ResNet50 架构作为识别乌尔都语手语的基础模型。模型架构定义如下：
base_model = ResNet50(include_top=False, weights=&#39;imagenet&#39;, input_shape=(256, 256, 3))

model = tf.keras.Sequential([
tf.keras.layers.Input(shape=(256,256,3)),
tf.keras.layers.Lambda(preprocess_input_resnet),
base_model,
tf.keras.layers.GlobalAveragePooling2D(),
tf.keras.layers.Dense(128,activation=&#39;relu&#39;),
tf.keras.layers.Dense(37,activation=&#39;softmax&#39;)
])

在对模型进行几个 epoch 的训练后，我通过设置某些层对其进行了微调trainable:
base_learning_rate = 0.0001

base_model.trainable = True
fine_tune_at = 100

# 在微调之前冻结层
for layer in base_model.layers[:fine_tune_at]:
layer.trainable = False

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate / 10),
metrics=[&#39;accuracy&#39;])

history_fine = model.fit(train_dataset, epochs=total_epochs, validation_data=validation_dataset)

问题：
当我尝试使用 load_model(&#39;model.h5&#39;) 加载模型时，我遇到以下问题ValueError：

ValueError：层“dense”需要 1 个输入，但收到了 2 个输入张量。收到的输入：[&lt;KerasTensor shape=(None, 8, 8, 2048), dtype=float32, sparse=False, name=keras_tensor_564&gt;, &lt;KerasTensor shape=(None, 8, 8, 2048), dtype=float32, sparse=False, name=keras_tensor_565&gt;]

该错误表明 Dense 层正在接收两个输入张量，而不是一个。
我已通过运行 model.summary() 检查了模型架构，并确认各层的结构符合预期。但是，我对第二个张量（keras_tensor_565）的来源感到困惑。似乎架构中的某个地方可能存在意外的连接或重复。
我也尝试过以 .h5 和 .keras 格式保存和加载模型。但在加载模型时，两种格式都存在相同的错误。
model1.save(&quot;model.h5&quot;)

model = load_model(&#39;model.h5&#39;, custom_objects={&#39;preprocess_input&#39;: preprocess_input})


为什么 Dense 层会接收两个张量？
如何解决此问题以成功加载我的模型而不会出现错误？
如何确保我的模型架构在保存和加载过程中保持一致？
]]></description>
      <guid>https://stackoverflow.com/questions/79010662/valueerror-while-loading-my-keras-model-after-doing-transfer-learning</guid>
      <pubDate>Sun, 22 Sep 2024 00:24:16 GMT</pubDate>
    </item>
    <item>
      <title>在 R 中运行随机森林生存模型 (rfsrc) 时 R 会话中止</title>
      <link>https://stackoverflow.com/questions/79010492/r-session-aborted-when-running-random-forest-survival-model-rfsrc-in-r</link>
      <description><![CDATA[我正在使用 R 中的“randomForestSRC”包进行生存分析项目。不幸的是，每次我尝试运行随机森林生存模型 (rfsrc) 时，我的 R 会话都会崩溃，并显示以下消息：“R 遇到致命错误。会话已终止。”
这是我到目前为止所做的：
在此处输入图像描述

数据：数据集已清理，没有缺失值，由生存时间 (time_month) 和事件状态 (死亡) 组成。我已经成功地在这个数据集上运行了其他生存模型（例如，bnnsurvival、coxph），没有任何问题。

模型设置：

我为随机森林模型创建了公式，如下所示：



r
 formula_rfsrc &lt;- as.formula(paste(&quot;Surv(time_month, death) ~&quot;, paste(pred_vars, collapse = &quot; + &quot;)))


然后我尝试拟合模型：

 fit &lt;- rfsrc(formula_rfsrc, data = df_train, ntree = 50)


尝试修复：

检查公式：我已验证公式正确，对生存对象使用 Surv()。
减少数据大小：我尝试在较小的数据子集上运行模型。
限制树深度：我使用了 nodesize、nodedepth 等参数，并减少了树的数量。
内存管理：我确保在模型拟合之前调用垃圾收集 (gc())，认为问题可能与内存有关。



尽管付出了这些努力，但每当我运行随机森林模型时，R 会话都会崩溃。
其他详细信息：
数据集有大约 30 个预测变量，我根据需要将其转换为因子或数字。
我在装有 R 4.4.1 的机器上运行此程序。
有人遇到过类似的 randomForestSRC 问题吗？有没有关于如何解决这个问题的想法或在 R 中运行生存随机森林的替代方法？
提前感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/79010492/r-session-aborted-when-running-random-forest-survival-model-rfsrc-in-r</guid>
      <pubDate>Sat, 21 Sep 2024 21:33:20 GMT</pubDate>
    </item>
    <item>
      <title>AutoModelForSequenceClassification 损失没有减少</title>
      <link>https://stackoverflow.com/questions/79010018/automodelforsequenceclassification-loss-not-decrease</link>
      <description><![CDATA[从数据集导入 load_dataset
从 torch.utils.data 导入 DataLoader
从 transformers 导入 AutoTokenizer、AutoModelForSequenceClassification
导入 torch
从 tqdm 导入 tqdm

def train_one_epoch(model、dataloader、optimizer):
model.train()
loss_list = []
for batch in tqdm(dataloader):
batch_data = {
&#39;input_ids&#39;: batch[&#39;input_ids&#39;],
&#39;attention_mask&#39;: batch[&#39;attention_mask&#39;],
&#39;labels&#39;: batch[&#39;labels&#39;]
}
loss = model(**batch_data).loss
loss.backward()
optimizer.step()
optimizer.zero_grad()

loss_list.append(loss.detach().item())
avg_loss = sum(loss_list) / len(loss_list)
print(&#39;avg loss在 epoch:&#39;, avg_loss)

def assess(model, dataloader):
model.eval()
all_labels = []
all_predictions = []
for batch in dataloader:
with torch.no_grad():
batch_data = {
&#39;input_ids&#39;: batch[&#39;input_ids&#39;],
&#39;attention_mask&#39;: batch[&#39;attention_mask&#39;]
}
logits = model(**batch_data).logits
predictions = torch.argmax(logits, dim=-1)
labels = batch[&#39;labels&#39;]
all_labels.extend(labels)
all_predictions.extend(predictions)
accuracy = compute_accuracy(all_predictions, all_labels)
print(&quot;Accuracy&quot;, accuracy)
return accuracy

def compute_accuracy(predictions, labels):
correct = 0
for pred，zip(predictions, labels) 中的标签：
if pred == label:
correct += 1
返回正确 / len(labels)

def my_collat​​e_fn(batched_samples):
texts = [example[&#39;text&#39;] 例如 batched_samples]
labels = [example[&#39;label&#39;] 例如 batched_samples]
text_encoding = tokenizer(texts, max_length=128, truncation=True, padding=True, return_tensors=&#39;pt&#39;)
labels = torch.LongTensor(labels)
return {
&#39;input_ids&#39;: text_encoding[&#39;input_ids&#39;].cuda(),
&#39;attention_mask&#39;: text_encoding[&#39;attention_mask&#39;].cuda(),
&#39;labels&#39;: labels.cuda()
}

torch.manual_seed(64)
batch_size = 16
学习率 = 5e-5
训练次数 = 10
模型名称 = “roberta-base”

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

model = model.cuda()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate, eps=1e-8)

datasets = load_dataset(&quot;gpt3mix/sst2&quot;)

train_dataloader = DataLoader(
datasets[&#39;train&#39;],
batch_size=8,
shuffle=True,
collat​​e_fn=my_collat​​e_fn,
num_workers=0
)

validation_dataloader = DataLoader(
datasets[&#39;validation&#39;],
batch_size=8,
shuffle=False,
collat​​e_fn=my_collat​​e_fn,
num_workers=0
)

best_acc = 0.0
for周期范围（1，num_epochs + 1）：
train_one_epoch（模型，train_dataloader，优化器）
valid_acc = 评估（模型，validation_dataloader）


100%|██████████| 865/865 [01:27&lt;00:00，9.89it/s]

周期内平均损失：0.6746856869559068

准确率 0.4908256880733945

100%|██████████| 865/865 [01:25&lt;00:00, 10.09it/s]

epoch 中的平均损失：0.6922555248516833

准确率 0.4908256880733945

100%|██████████| 865/865 [01:27&lt;00:00, 9.89it/s]

epoch 中的平均损失：0.6976809655310791

准确率 0.5091743119266054

更改学习率也不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/79010018/automodelforsequenceclassification-loss-not-decrease</guid>
      <pubDate>Sat, 21 Sep 2024 16:24:50 GMT</pubDate>
    </item>
    <item>
      <title>CNN-KAN 模型的训练尚未开始</title>
      <link>https://stackoverflow.com/questions/79009899/the-training-of-a-cnn-kan-model-is-not-starting</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79009899/the-training-of-a-cnn-kan-model-is-not-starting</guid>
      <pubDate>Sat, 21 Sep 2024 15:25:05 GMT</pubDate>
    </item>
    <item>
      <title>X 有 8 个特征，但 RandomForestRegressor 需要 2924 个特征作为输入</title>
      <link>https://stackoverflow.com/questions/79009698/x-has-8-features-but-randomforestregressor-is-expecting-2924-features-as-input</link>
      <description><![CDATA[我正在使用 Kaggle 数据集和 RandomForestRegressor 为我的城市构建餐厅推荐器。
我构建了模型，现在希望模型在给定 4 个参数时推荐一家好餐厅：位置、大致费用、餐厅类型和投票数。但是，它返回了一个值错误：
X 有 8 个特征，但 RandomForestRegressor 需要 2924 个特征作为输入。

这是我尝试运行的：
import joblib
import numpy as np
from sklearn.preprocessing import StandardScaler

model = joblib.load(&#39;my_model.pkl&#39;)
scaler = joblib.load(&#39;scaler.pkl&#39;)

def preprocess_input(location, type_, cost, votes):
one_hot_location = [1 if loc == location else 0 for loc in [&#39;Whitefield&#39;, &#39;Koramangala&#39;, &#39;Indiranagar&#39;]]
one_hot_type = [1 if t == type_ else 0 for t in [&#39;Casual Dining&#39;, &#39;Quick Bites&#39;, &#39;Cafe&#39;]]

scaled_features = scaler.transform([[cost, votes]])

return np.array(one_hot_location + one_hot_type + list(scaled_features[0])).reshape(1, -1)

input_data = preprocess_input(&#39;Whitefield&#39;, &#39;Casual Dining&#39;, 1000, 500)

prediction = model.predict(input_data)

print(f&quot;预测的餐厅：{prediction}&quot;)

训练数据的形状：
X_train.shape = (41373, 2924)
y_train.shape = (41373,)
这是我的数据集的样子]]></description>
      <guid>https://stackoverflow.com/questions/79009698/x-has-8-features-but-randomforestregressor-is-expecting-2924-features-as-input</guid>
      <pubDate>Sat, 21 Sep 2024 13:43:40 GMT</pubDate>
    </item>
    <item>
      <title>我的非序列 keras 模型的一个输入出现了难以理解的形状错误</title>
      <link>https://stackoverflow.com/questions/79009687/incomprehensible-shape-error-with-one-of-the-inputs-of-my-non-sequential-keras-m</link>
      <description><![CDATA[我编写了以下 keras 模型
input_A = 输入(shape=[5], name=&quot;wide_input&quot;)
hidden_​​layer_1 = Dense(10,activation=&quot;relu&quot;, name=&#39;h_wide_layer&#39;)(input_A)
input_B = 输入(shape=[6], name=&quot;deep_input&quot;)
hidden_​​layer_2 = Dense(30,activation=&quot;relu&quot;, name=&#39;h_deep_layer_1&#39;)(input_B)
hidden_​​layer_3 = Dense(30,activation=&quot;relu&quot;, name=&#39;h_deep_layer_2&#39;)(hidden_​​layer_2)
concat = Concatenate()([hidden_​​layer_1, hidden_​​layer_3])
output = Dense(1, name=&quot;output&quot;)(concat)
complex_model_2_1 = keras.Model(inputs=[input_A, input_B], output=[output])

#编译第二个复杂模型
complex_model_2_1.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;sgd&quot;)

#训练
X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]
X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]
X_test_A, X_test_B = X_test_full[:, :5], X_test_full[:, 2:]
X_new_A, X_new_B = X_new[:, :5], X_new[:, 2:]
print(f&quot;X_train_A 的形状：{X_train_A.shape}&quot;)
print(f&quot;X_train_B 的形状：{X_train_B.shape}&quot;)
history = complex_model_2_1.fit({&quot;wide_input&quot;: X_train_A, &quot;deep_input&quot;: X_train_B},
y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))

X_train 有 8 个特征，因此 X_train_A 和 X_train_B 实际上分别有 5 个和 6 个特征。但是，我不明白为什么我会得到下面的hidden_​​layer_2 不兼容形状错误:
ValueError Traceback (most recent call last)
&lt;ipython-input-42-86b1419058f7&gt; in &lt;cell line: 11&gt;()
9 print(f&quot;Shape of X_train_A: {X_train_A.shape}&quot;)
10 print(f&quot;Shape of X_train_B: {X_train_B.shape}&quot;)
---&gt; 11 history = complex_model_2_1.fit({&quot;wide_input&quot;: X_train_A, &quot;deep_input&quot;: X_train_B},
12 y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))
13 

1 帧
/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py in assert_input_compatibility(input_spec, input, layer_name)
225 无,
226 }:
--&gt; 227 raise ValueError(
228 f&#39;层“{layer_name}”的输入 {input_index} 是 &#39;
229 f“与层不兼容：预期轴 {axis}”

ValueError：调用 Functional.call() 时遇到异常。

层“h_deep_layer_1”的输入 0 与层不兼容：预期输入形状的轴 -1 具有值 6，但收到的输入形状为 (None, 5)

Functional.call() 收到的参数：
• 输入={&#39;wide_input&#39;: &#39;tf.Tensor(shape=(None, 5), dtype=float32)&#39;, &#39;deep_input&#39;: &#39;tf.Tensor(shape=(None, 6), dtype=float32)&#39;}
• 训练=True
• 掩码={&#39;wide_input&#39;: &#39;None&#39;, &#39;deep_input&#39;: &#39;None&#39;}

如何修复？
PS：Google colab 中的 Gemini 无法解释该问题，并向我建议 X_train_B = X_train[:, 5:]，这是不正确的（形状为 (_, 3)]]></description>
      <guid>https://stackoverflow.com/questions/79009687/incomprehensible-shape-error-with-one-of-the-inputs-of-my-non-sequential-keras-m</guid>
      <pubDate>Sat, 21 Sep 2024 13:38:38 GMT</pubDate>
    </item>
    <item>
      <title>对 GPT 架构感到失望 [关闭]</title>
      <link>https://stackoverflow.com/questions/79009482/disappoint-at-gpt-architecture</link>
      <description><![CDATA[我是NLP的新手，读过一些该领域的经典论文。读完GPT-2后，我感到很失望，因为它只是将Decoder堆叠了多层，并使用大量数据进行训练。
说实话，Transformer架构给我留下了深刻的印象。GPT-2释放了一个信号，它只需要在更大的模型上训练更多的数据，就可以获得良好的性能。
我还没有读过关于最近模型的论文，你们能给我一些提示吗，现在的LLM是否只是一场由资源控制的游戏？有什么有趣的论文可以阅读吗？
我会阅读更多关于这个主题的论文并有一个整体的了解。]]></description>
      <guid>https://stackoverflow.com/questions/79009482/disappoint-at-gpt-architecture</guid>
      <pubDate>Sat, 21 Sep 2024 11:45:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 Scikit-learn、XGBoost 和 Prophet 时，保存训练模型的最佳文件格式是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79008634/what-is-the-best-file-format-to-save-trained-model-when-using-scikit-learn-xgbo</link>
      <description><![CDATA[我正在使用 Scikit-learn 开展 ML 项目。根据我的研究，人们建议使用 .joblib 保存经过训练的 Scikit-learn 模型。
这就是我将模型保存到 .joblib 的方式&gt;
import os
from joblib import dump

model_path = os.path.join(script_dir, &quot;../models/trained_model.joblib&quot;)
dump(model, model_path)
print(f&quot;Model saved at {model_path}&quot;)

我还想使用 XGBoost 和 Prophet 测试此模型，只是为了尝试不同的库。

什么是实现此目标的最佳文件格式？我在搜索过程中多次看到 ONNX，但它似乎与 Prophet 不兼容。

有没有办法将我的模型同时保存为 joblib 和 onnx，或者我是否需要将 jobllib 转换为 onnx 文件？

]]></description>
      <guid>https://stackoverflow.com/questions/79008634/what-is-the-best-file-format-to-save-trained-model-when-using-scikit-learn-xgbo</guid>
      <pubDate>Sat, 21 Sep 2024 01:49:12 GMT</pubDate>
    </item>
    <item>
      <title>无监督图像聚类：无法获得正确的结果[关闭]</title>
      <link>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</link>
      <description><![CDATA[我正在开展一个个人项目，该项目采用一组图像（金属螺母）并确定是否存在缺陷（着色、划痕、弯曲、翻转和良好）。
我使用 VGG16 模型提取特征，使用 PCA 降低维数，然后将降维后的特征输入到简单的 k 均值算法（k=5）中以识别聚类。
我遇到的问题归结为：从模型中提取的特征对于解决手头的问题并不是很有效。
更具体地说，如果我想识别特定的“翻转”金属螺母（只是制造时齿朝向错误的螺母），提取的特征确实很有效。因此，集群最终是 4 个随机集，然后是 1 组刚翻转的螺母。
我的问题是，我可以做些什么来修改我的模型/提取的特征，使它们更适合我的问题（识别所有 5 个类别的金属螺母）？我甚至很高兴能够从“有缺陷”中识别出“好”的螺母。
我尝试过的事情：

在“好”图像的训练集上训练模型（即只是普通的金属螺母）
从模型的较早层（第 10 层）而不是倒数第二层获取输出
使用不同的模型（我最初使用的是 ResNet18）
]]></description>
      <guid>https://stackoverflow.com/questions/78975401/unsupervised-image-clustering-cant-get-the-right-results</guid>
      <pubDate>Wed, 11 Sep 2024 19:16:38 GMT</pubDate>
    </item>
    <item>
      <title>Optuna Hyperband 算法不遵循预期的模型训练方案</title>
      <link>https://stackoverflow.com/questions/78251318/optuna-hyperband-algorithm-not-following-expected-model-training-scheme</link>
      <description><![CDATA[我在 Optuna 中使用 Hyperband 算法时发现了一个问题。根据 Hyperband 算法，当 min_resources = 5、ma​​x_resources = 20 和 reduction_factor = 2 时，搜索应从 1 组别的 4 个模型的初始空间开始，每个模型在第一轮中接收 5 个 epoch。随后，模型数量在每一轮中减少 2 倍，搜索空间也应在下一组的 2 倍减少，即组别 2 的初始搜索空间为 2 个模型，其余模型的 epoch 数量在随后的每一轮中加倍。因此预计总模型数应为 11，但需要训练大量模型。
文章链接：- https://arxiv.org/pdf/1603.06560.pdf
import optuna
import numpy as np
import pandas as pd 
from tensorflow.keras.layers import Dense,Flatten,Dropout
import tensorflow as tf
from tensorflow.keras.models import Sequential

# 玩具数据集生成
def generate_toy_dataset():
np.random.seed(0)
X_train = np.random.rand(100, 10)
y_train = np.random.randint(0, 2, size=(100,))
X_val = np.random.rand(20, 10)
y_val = np.random.randint(0, 2, size=(20,))
return X_train, y_train, X_val, y_val

X_train, y_train, X_val, y_val = generate_toy_dataset()

# 模型构建函数
def build_model(trial):
model = Sequential()
model.add(Dense(units=trial.suggest_int(&#39;unit_input&#39;, 20, 30),
activation=&#39;selu&#39;,
input_shape=(X_train.shape[1],)))

num_layers = trial.suggest_int(&#39;num_layers&#39;, 2, 3)
for i in range(num_layers):
units = trial.suggest_int(f&#39;num_layer_{i}&#39;, 20, 30)
activation = trial.suggest_categorical(f&#39;activation_layer_{i}&#39;, [&#39;relu&#39;, &#39;selu&#39;, &#39;tanh&#39;])
model.add(Dense(units=units,activation=activation))
如果 trial.suggest_categorical(f&#39;dropout_layer_{i}&#39;, [True, False]):
model.add(Dropout(rate=0.5))

model.add(Dense(1,activation=&#39;sigmoid&#39;))

optimizer_name = trial.suggest_categorical(&#39;optimizer&#39;, [&#39;adam&#39;, &#39;rmsprop&#39;])
如果 optimizer_name == &#39;adam&#39;:
optimizer = tf.keras.optimizers.Adam()
否则:
optimizer = tf.keras.optimizers.RMSprop()

model.compile(optimizer=optimizer, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;, tf.keras.metrics.AUC(name=&#39;val_auc&#39;)])

return model

def objective(trial):
model = build_model(trial)
# 假设您已准备好数据
# 修改拟合方法以包含 AUC 指标
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), verbose=1)

# 检查是否记录了“val_auc”
auc_key = None
for key in history.history.keys():
if key.startswith(&#39;val_auc&#39;):
auc_key = key
print(f&quot;auc_key is {auc_key}&quot;)
break

if auc_key is None:
raise ValueError(&quot;历史记录中未找到AUC指标。确保在训练期间记录它。&quot;)

# 报告每个模型的验证 AUC

if auc_key ==&quot;val_auc&quot;:
step=0
else:
step = int(auc_key.split(&#39;_&#39;)[-1])

auc_value=history.history[auc_key][0]
trial.report(auc_value, step=step)
print(f&quot;prune or not:-{trial.should_prune()}&quot;)
if trial.should_prune():
raise optuna.TrialPruned()

return history.history[auc_key]

# Optuna 研究创建
study = optuna.create_study(
direction=&#39;maximize&#39;,
pruner=optuna.pruners.HyperbandPruner(
min_resource=5,
max_resource=20,
reduction_factor=2
)
)

# 开始优化
study.optimize(objective)

]]></description>
      <guid>https://stackoverflow.com/questions/78251318/optuna-hyperband-algorithm-not-following-expected-model-training-scheme</guid>
      <pubDate>Sun, 31 Mar 2024 12:38:07 GMT</pubDate>
    </item>
    <item>
      <title>Python：GridSearchCV 需要很长时间才能完成运行</title>
      <link>https://stackoverflow.com/questions/72101295/python-gridsearchcv-taking-too-long-to-finish-running</link>
      <description><![CDATA[我正尝试使用网格搜索来优化我的模型，但执行时间太长了。我的总数据集只有大约 15,000 个观测值，大约有 30-40 个变量。我成功地通过网格搜索运行了一个随机森林，大约花了一个半小时，但现在我已经切换到 SVC，它已经运行了 9 个多小时，但仍然没有完成。下面是我用于交叉验证的代码示例：
from sklearn.model_selection import GridSearchCV
from sklearn import svm
from sklearn.svm import SVC

SVM_Classifier= SVC(random_state=7)

param_grid = {&#39;C&#39;: [0.1, 1, 10, 100],
&#39;gamma&#39;: [1,0.1,0.01,0.001],
&#39;kernel&#39;: [&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;],
&#39;degree&#39; : [0, 1, 2, 3, 4, 5, 6]}

grid_obj = GridSearchCV(SVM_Classifier,

return_train_score=True,
param_grid=param_grid,
评分=&#39;roc_auc&#39;,
cv=3,
n_jobs = -1)

grid_fit = grid_obj.fit(X_train, y_train)
SVMC_opt = grid_fit.best_estimator_

print(&#39;=&#39;*20)
print(&quot;best params: &quot; + str(grid_obj.best_estimator_))
print(&quot;best params: &quot; + str(grid_obj.best_params_))
print(&#39;best score:&#39;, grid_obj.best_score_)
print(&#39;=&#39;*20)


我已经将交叉验证从 10 减少到 3，并且我使用 n_jobs=-1，因此我使用了所有核心。我还有什么遗漏的，可以在这里做些事情来加快进程吗？]]></description>
      <guid>https://stackoverflow.com/questions/72101295/python-gridsearchcv-taking-too-long-to-finish-running</guid>
      <pubDate>Tue, 03 May 2022 14:51:20 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ML 模型和 FastAPI 处理来自多个用户的请求？</title>
      <link>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</link>
      <description><![CDATA[我正在研究通过FastAPI分发人工智能模块的过程。
我创建了一个FastAPI应用，使用预先学习的机器学习模型来回答问题。
这种情况下，一个用户使用是没有问题的，但是多个用户同时使用时，响应可能会太慢。
那么，当多个用户输入一个问题时，有没有办法一次性复制模型并加载进去？
class sentencebert_ai():
def __init__(self) -&gt;无：
super().__init__()

def ask_query(self,query, topN):
startt = time.time()

ask_result = []
score = []
result_value = [] 
embedder = torch.load(model_path)
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)
query_embedding = embedder.encode(query, convert_to_tensor=True)
cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0] #torch.Size([121])121 表示该数据集为 10 ... cos_scores = cos_scores.cpu() top_results = np.argpartition(-cos_scores, range(topN))[0:topN] for idx in top_results[0:topN]: Ask_result.append(corpusid[idx].item()) #.item()으로 5）에서해당숫자에접근하기위한방식다。
            score.append(round(cos_scores[idx].item(),3))

# 生成 json 数组并返回结果集
for i,e in zip(ask_result,score):
result_value.append({&quot;pred_id&quot;:i,&quot;pred_weight&quot;:e})
endd = time.time()
print(&#39;结果集&#39;,endd-startt)
return result_value
# return &#39;,&#39;.join(str(e) for e in ask_result),&#39;,&#39;.join(str(e) for e in score)

class Item_inference(BaseModel):
text : str
topN : Optional[int] = 1

@app.post(&quot;/retrieval&quot;, tags=[&quot;knowledge referral&quot;])
async def Knowledge_recommendation(item: Item_inference):

# db.append(item.dict())
item.dict()
results = _ai.ask_query(item.text, item.topN)

return results

if __name__ == &quot;__main__&quot;:
parser = argparse.ArgumentParser()
parser.add_argument(&quot;--port&quot;, default=&#39;9003&#39;, type=int)
# parser.add_argument(&quot;--mode&quot;, default=&#39;cpu&#39;, type=str, help=&#39;cpu for CPU mode, gpu for GPU mode&#39;)
args = parser.parse_args()

_ai = sentencebert_ai()
uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=args.port,workers=4)

更正版本
@app.post(&quot;/aaa&quot;) def your_endpoint(request: Request, item:Item_inference): start = time.time() model = request.app.state.model item.dict() # 测试结果 _ai = sentencebert_ai() results = _ai.ask_query(item.text, item.topN,model) end = time.time() print(end-start) return results ``` 
]]></description>
      <guid>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</guid>
      <pubDate>Fri, 25 Mar 2022 07:13:32 GMT</pubDate>
    </item>
    <item>
      <title>Detectron2 检查点未找到</title>
      <link>https://stackoverflow.com/questions/65327162/detectron2-checkpoint-not-found</link>
      <description><![CDATA[从昨晚开始就一直出现这样的错误，我训练了5个模型，都没有问题，然后就出现了这样的问题，怎么解决呢？
AssertionError Traceback (most recent call last)
&lt;ipython-input-9-08522bc16525&gt; in &lt;module&gt;()
34 os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
35 trainer = CocoTrainer(cfg)
---&gt; 36 trainer.resume_or_load(resume=False)
37 trainer.train()

2 帧
/usr/local/lib/python3.6/dist-packages/fvcore/common/checkpoint.py in load(self, path, checkpointables)
118 if not os.path.isfile(path):
119 path = self.path_manager.get_local_path(path)
--&gt; 120 断言 os.path.isfile(path)，“未找到检查点 {}！”。格式（路径）
121 
122 checkpoint = self._load_file(path)

AssertionError：未找到检查点 https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x/139173657/model_final_68b088.pkl！
]]></description>
      <guid>https://stackoverflow.com/questions/65327162/detectron2-checkpoint-not-found</guid>
      <pubDate>Wed, 16 Dec 2020 16:22:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 K-Means 在 LAB 颜色空间中按颜色对图像进行聚类</title>
      <link>https://stackoverflow.com/questions/30368942/cluster-image-by-colors-in-lab-color-space-using-k-means</link>
      <description><![CDATA[我尝试了以下代码。
he = imread(&#39;hestain.png&#39;);
imshow(he), title(&#39;H&amp;E image&#39;);
cform = makecform(&#39;srgb2lab&#39;);
la​​b_he = applycform(he,cform);
ab = double(lab_he(:,:,2:3));
nrows = size(ab,1); %n 行
ncols = size(ab,2); %p 列
ab = reshape(ab,nrows*ncols,2);

nColors = 3;

[cluster_idx, cluster_center] = kmeans(ab,nColors); 

它给我错误 

reshape 无法从 n*1 数组创建 n*p 矩阵。

这很有道理，但它在这里有效。
我在 octave 中尝试了相同类型的代码
ed=edge(de,&quot;canny&quot;);
imshow(ed);
ed=double(ed);
nrows=size(ed,1);
ncols=size(ed,2);
ed=reshape(ed,nrows*ncols,2)
[cluster_idx, cluster_center]=kmeans(ed,3);
pixel_labels = reshape(cluster_idx,nrows,ncols);
imshow(pixel_labels,[]), title(&#39;image labeled by cluster index&#39;);

其中 de 是一些图像。
当我运行时，我收到此错误。

错误：重塑：无法将 181x181 数组重塑为 32761x2 数组

感谢帮助]]></description>
      <guid>https://stackoverflow.com/questions/30368942/cluster-image-by-colors-in-lab-color-space-using-k-means</guid>
      <pubDate>Thu, 21 May 2015 08:55:18 GMT</pubDate>
    </item>
    </channel>
</rss>