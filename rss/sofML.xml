<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 27 Mar 2024 12:24:50 GMT</lastBuildDate>
    <item>
      <title>有关 GRU 单元中前向传播计算的冲突信息 [已关闭]</title>
      <link>https://stackoverflow.com/questions/78231326/conflicting-information-on-the-calculation-of-the-forward-propagation-in-gru-uni</link>
      <description><![CDATA[当尝试从头开始编写 GRU 单元时，我发现两种相互冲突的方法来计算下一个时间步的 h 值：
&lt;前&gt;&lt;代码&gt;1。 h = z * h + (1 - z) * h_hat

和
&lt;前&gt;&lt;代码&gt;2。 h = (1 - z) * h + z * h_hat

我现在的问题是这些方法中的一种是否错误/更糟糕，或者两者是否都可以使用。
（如果两者都可以使用，那么代码中还必须更改其他内容，大概是在反向传播中。）
我找到的真实代码示例使用了1.计算：
https://github.com/erikvdplas/gru-rnn /blob/master/main.py（第 72 和 153 行）
实现有状态基于经过训练的 Keras 模型的纯 Numpy 中的 GRU（Yu-Yang 的回答）
但是我发现的所有原理图都指向使用 2. 变体：
将 z 与 h_hat 相乘以及 1-z 与 h 相乘的 GRU 原理图
然后这篇博客文章使用了 2.1，但 Nico Lynn 的评论表明这可能是一个错误。
https://medium.com/swlh/only-numpy-deriving-forward-feed-and-back-propagation-in-erated-recurrent-neural-networks-gru-8b6810f91bad
&lt;块引用&gt;
我认为图 2 中存在错误。
在 S_t 的方程中，第一项应乘以 hat(S_(t))，而第二项应乘以 S_(t-1)。
有人可以确认或拒绝这一点吗？

很抱歉，如果这是一个糟糕的问题，但我已经不知道了。
我尝试将其他计算切换到工作代码中，但这显然没有产生任何好处，因此两者之间至少必须存在某种差异。]]></description>
      <guid>https://stackoverflow.com/questions/78231326/conflicting-information-on-the-calculation-of-the-forward-propagation-in-gru-uni</guid>
      <pubDate>Wed, 27 Mar 2024 11:25:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在 stylegan2 中提示文本</title>
      <link>https://stackoverflow.com/questions/78231191/how-to-prompt-a-text-in-stylegan2</link>
      <description><![CDATA[我想在stylegan2中输入文本提示。
到目前为止，我已经能够使用加载生成器 (G)
G,D,Gs = load_networks(&#39;gdrive:networks/stylegan2-ffhq-config-a.pkl&#39;)

它调用 pretrained_networks.py 文件中定义的函数。
现在，我不知道如何按照我想要的方式使用生成器。
具体来说，给定文本提示，我想我应该：

对 numpy 数组中的文本进行编码
使用此编码提供生成器

但是我该怎么做呢？]]></description>
      <guid>https://stackoverflow.com/questions/78231191/how-to-prompt-a-text-in-stylegan2</guid>
      <pubDate>Wed, 27 Mar 2024 11:04:39 GMT</pubDate>
    </item>
    <item>
      <title>尽管 PyTorch 的学习率降低，FISTA 优化并未减少损失</title>
      <link>https://stackoverflow.com/questions/78231057/fista-optimization-not-reducing-loss-despite-decreasing-learning-rate-in-pytorch</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78231057/fista-optimization-not-reducing-loss-despite-decreasing-learning-rate-in-pytorch</guid>
      <pubDate>Wed, 27 Mar 2024 10:44:19 GMT</pubDate>
    </item>
    <item>
      <title>集成梯度下降和 ISTA 通过正则化最小化自定义损失函数</title>
      <link>https://stackoverflow.com/questions/78230490/integrating-gradient-descent-and-ista-for-minimizing-a-custom-loss-function-with</link>
      <description><![CDATA[我正在研究一个问题，目标是最小化损失函数，该损失函数是标准损失度量和正则化项的组合。具体来说，我的目标函数的形式为：L(y, m_t(x))+ l* P(t)
地点：

L 代表损失度量。
y 是实际结果。
x 表示输入数据。
t 是模型的参数向量。
m_t 是由 t 参数化的模型。
P(t) 是参数的惩罚函数，以鼓励某些属性（例如稀疏性）。
λ 是正则化系数。

我的问题围绕优化策略。我正在考虑首先使用梯度下降（GD）来最小化函数，直到它停止取得重大进展。在 GD 达到这个平台后，我正在考虑切换到迭代收缩阈值算法（ISTA）来进一步细化解决方案，特别是有效地合并正则化部分。
但是，我不确定结合这两种优化技术的最佳方法。具体来说：
我是否应该将 GD 专门应用于损失项 L(y,m_t(x))​，然后使用 ISTA 处理包括正则化项在内的整个成本函数？
优化过程中是否存在在 GD 和 ISTA 之间转换的推荐方法或最佳实践？
在针对此类问题集成这些优化方法时，我是否应该注意任何潜在的陷阱或注意事项？
任何见解、对类似实现的参考或有关如何解决此问题的指导将不胜感激。
预先感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78230490/integrating-gradient-descent-and-ista-for-minimizing-a-custom-loss-function-with</guid>
      <pubDate>Wed, 27 Mar 2024 09:11:49 GMT</pubDate>
    </item>
    <item>
      <title>内核重新​​启动 Untitled2.ipynb 的内核似乎已死亡。存储tflite模型时会自动重启</title>
      <link>https://stackoverflow.com/questions/78230023/kernel-restarting-the-kernel-for-untitled2-ipynb-appears-to-have-died-it-will-r</link>
      <description><![CDATA[我正在使用 ml.g4dn.xlarge 实例在 sagemaker 实例上运行笔记本。
我正在尝试创建如下所示的 tflite 模型
保存模型
model.save(os.path.join(model_dir, &#39;model.h5&#39;))
# 将模型转换并保存为 TensorFlow Lite
转换器 = tf.lite.TFLiteConverter.from_keras_model(模型)
converter.experimental_new_converter = True # 启用基于 MLIR 的转换流程
converter.debug_info = True # 启用调试.mlir文件的生成
tflite_model =转换器.convert()

打开（os.path.join（model_dir，&#39;model.tflite&#39;），&#39;wb&#39;）作为f：
    f.write(tflite_model)

我收到错误
内核重启
Untitled2.ipynb 的内核似乎已经死亡。它会自动重新启动。

我检查了一些要求升级实例类型的链接。我将其升级到 ml.g4dn.xlarge 但仍然给出相同的错误。有什么建议可能是什么原因吗？]]></description>
      <guid>https://stackoverflow.com/questions/78230023/kernel-restarting-the-kernel-for-untitled2-ipynb-appears-to-have-died-it-will-r</guid>
      <pubDate>Wed, 27 Mar 2024 07:42:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么我无法在函数调用中访问对象键</title>
      <link>https://stackoverflow.com/questions/78229600/why-cant-i-access-an-object-key-within-a-function-call</link>
      <description><![CDATA[我试图通过在此函数 graphics.drawPoint(ctx,pixelLoc, utils.样式[标签].color);
来自 Radu 的代码：无黑盒机器学习
utils.styles={
    汽车：{颜色：&#39;灰色&#39;，文本：&#39;🚓&#39;}，
    鱼：{颜色：&#39;红色&#39;，文本：&#39;🐠&#39;}，
    房子: { color:&#39;yellow&#39;, text:&#39;🏚&#39; },
    树：{颜色：&#39;绿色&#39;，文本：&#39;🌳&#39;}，
    自行车: { color:&#39;cyan&#39;, text:&#39;🚲&#39; },
    guiter: { color:&#39;blue&#39;, text:&#39;🎸&#39; },
    铅笔：{颜色：&#39;洋红色&#39;，文本：&#39;✏&#39;}，
    时钟：{ 颜色：&#39;lightgray&#39;，文本：&#39;⏲&#39; }
}

for（样本的常量样本）{
  const {点，标签}=样本；


sample 是以标签作为键的对象集合[&#39;car&#39;,&#39;fish&#39;,&#39;house...&#39;clock&#39;] 与 uitils.styles 相同&gt;
graphics.drawPoint(utils.styles[label].color);

这会产生一条错误消息：Uncaught TypeError:
&lt;块引用&gt;
无法读取未定义的属性（读取“颜色”）
]]></description>
      <guid>https://stackoverflow.com/questions/78229600/why-cant-i-access-an-object-key-within-a-function-call</guid>
      <pubDate>Wed, 27 Mar 2024 05:56:17 GMT</pubDate>
    </item>
    <item>
      <title>使用张量流构建神经网络时如何解决值错误</title>
      <link>https://stackoverflow.com/questions/78229525/how-to-resolve-value-error-when-building-neural-network-with-tensorflow</link>
      <description><![CDATA[我正在构建一个神经网络，此时，我想知道我的网络是什么样的。但模型摘要函数引发错误
&lt;前&gt;&lt;代码&gt;代码：
打印（模型.摘要（））

错误：
raise ValueError(f“无法将 &#39;{shape}&#39; 转换为形状。”)
ValueError：无法将“784”转换为形状。

编写下面的代码后，我期望对我的神经网络结构进行一些总结，但我得到了一个错误？
导入操作系统
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39;
将张量流导入为 tf
从张量流导入keras
从tensorflow.keras导入层、模型
从tensorflow.keras.datasets导入mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(-1, 28*28).astype(“float32”) / 255.0
x_test = x_test.reshape(-1, 28*28).astype(“float32”) / 255.0

# 一个输入到一个输出的顺序模型
模型 = keras.Sequential(
    [
        keras.输入(形状=(28*28)),
        层.密集（512，激活=&#39;relu&#39;），
        层.Dense(256, 激活=&#39;relu&#39;),
        层数.密集(10),
    ]
）


打印（模型.摘要（）），
导入系统
sys.exit()


模型.编译(
    损失=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    优化器=keras.optimizers.Adam(learning_rate=0.001),
    指标=[“准确度”],
）

model.fit(x_train、y_train、batch_size=32、epochs=5、verbose=2)
model.evaluate(x_test, y_test, batch_size=32, verbose=2)

]]></description>
      <guid>https://stackoverflow.com/questions/78229525/how-to-resolve-value-error-when-building-neural-network-with-tensorflow</guid>
      <pubDate>Wed, 27 Mar 2024 05:34:06 GMT</pubDate>
    </item>
    <item>
      <title>我如何培训法学硕士（例如米斯特拉尔）特定领域知识。任何文档/博客/建议。谢谢！ :,) [关闭]</title>
      <link>https://stackoverflow.com/questions/78229317/how-can-i-train-a-llm-such-as-mistral-on-domain-specific-knowledge-any-documen</link>
      <description><![CDATA[（我是 ML 新手，由于事情进展太快，我被告知要攻读 LLM :,），与团队中的高级工程师相比，我没有机会。我感到不知所措，想到在 Stack 上问这个问题）。
如果您能提供有关如何学习 LLM、如何对他们进行私人信息调整和培训的任何建议，我将不胜感激，出于隐私原因，我不允许使用任何 API。
我尝试浏览一些 YouTube 教程，但找不到任何不使用 API 或密钥的教程。我可以在本地运行预训练的模型，但无法弄清楚如何在私有数据上训练它们。]]></description>
      <guid>https://stackoverflow.com/questions/78229317/how-can-i-train-a-llm-such-as-mistral-on-domain-specific-knowledge-any-documen</guid>
      <pubDate>Wed, 27 Mar 2024 04:21:08 GMT</pubDate>
    </item>
    <item>
      <title>对多行数据进行分组以用于 scikit-learn 随机森林机器学习模型</title>
      <link>https://stackoverflow.com/questions/78229141/grouping-multiple-rows-of-data-for-use-in-scikit-learn-random-forest-machine-lea</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78229141/grouping-multiple-rows-of-data-for-use-in-scikit-learn-random-forest-machine-lea</guid>
      <pubDate>Wed, 27 Mar 2024 03:07:34 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：“NoneType”对象不可迭代，部署机器学习模型</title>
      <link>https://stackoverflow.com/questions/78228918/typeerror-nonetype-object-is-not-iterable-deploying-ml-model</link>
      <description><![CDATA[我试图在 HTML 页面上的 29 个字段中获取整数输入值，并将其传递给 ML 模型，如上面的 python 代码所示。我收到错误“TypeError: &#39;NoneType&#39; object is not iterable”，这可能是由于值未从 HTML 正确传递到 python 代码。下面是 python 和 html 代码。提前致谢！
fromflask导入Flask，render_template，request
进口泡菜
将 numpy 导入为 np
从 bs4 导入 BeautifulSoup
导入cgi

应用程序=烧瓶（__名称__）
cv = pickle.load(open(&quot;model/bc_model.pkl&quot;,&quot;rb&quot;))

@app.route(“/”)
def home():
    返回 render_template(“index.html”)

@app.route(“/predict”,methods=[“POST”])
def 预测（）：
    形式 = cgi.FieldStorage()
    值 = form.getvalue(&#39;值[]&#39;)
    数组=[]
    对于值中的值：
        数组.append([值])
    预测=cv.预测（数组）
    return render_template(“index.html”,prediction=prediction)

如果 __name__ == “__main__”：
    app.run（主机=“0.0.0.0”，端口=8080，调试=True）

乳腺癌预测因子
    &lt;表单方法=“post”动作=“/预测”&gt;
        &lt;输入类型=“数字”名称=“值[]”&gt;&lt;br&gt;
        &lt;输入类型=“数字”名称=“值[]”&gt;&lt;br&gt;
        &lt;输入类型=“数字”名称=“值[]”&gt;&lt;br&gt;
         ……
         ……
        &lt;输入类型=“数字”名称=“值[]”&gt;&lt;br&gt;
        &lt;输入类型＝“提交” value=“提交”&gt;
    &lt;/表格&gt;

    &lt;p id=“显示”&gt;{{预测}}&lt;/p&gt;

]]></description>
      <guid>https://stackoverflow.com/questions/78228918/typeerror-nonetype-object-is-not-iterable-deploying-ml-model</guid>
      <pubDate>Wed, 27 Mar 2024 01:34:25 GMT</pubDate>
    </item>
    <item>
      <title>用于解析（决策树）绘制图表的法学硕士？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78227894/llm-for-parsing-drawn-diagrams-of-decision-trees</link>
      <description><![CDATA[
我正在寻找一种开源法学硕士，它能够接受与上述类似的决策树查询。这是否意味着找到最好的开源 ViT 模型？或者是否有更好的方法，例如以某种方式将树预处理为文本并向法学硕士询问？考虑到我仅对开源模型的限制，什么性能会更好？
我已经尝试过 ChatGPT4，它似乎表现得很好，但我希望在本地运行所有内容。]]></description>
      <guid>https://stackoverflow.com/questions/78227894/llm-for-parsing-drawn-diagrams-of-decision-trees</guid>
      <pubDate>Tue, 26 Mar 2024 20:03:23 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Flask 将我的机器学习模型集成到我的 React-native 应用程序中 [关闭]</title>
      <link>https://stackoverflow.com/questions/78207843/how-can-i-integrate-my-ml-model-in-python-in-my-react-native-app-using-flask</link>
      <description><![CDATA[我想使用flask将xgboost模型集成到react-native应用程序中。
如果可能的话，我期待相同的过程和代码。]]></description>
      <guid>https://stackoverflow.com/questions/78207843/how-can-i-integrate-my-ml-model-in-python-in-my-react-native-app-using-flask</guid>
      <pubDate>Fri, 22 Mar 2024 17:10:01 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：层equential_40的输入0与该层不兼容：预期min_ndim = 3，发现ndim = 2。收到完整形状：（无，58）</title>
      <link>https://stackoverflow.com/questions/72743778/valueerror-input-0-of-layer-sequential-40-is-incompatible-with-the-layer-expec</link>
      <description><![CDATA[我正在研究有关学生在课程中表现的数据集，我想根据学生上一年的成绩来预测学生的水平（低、中、高）。我正在使用 CNN 来实现此目的，但是当我构建并拟合模型时，我收到此错误：
ValueError：层equential_40的输入0与层不兼容：：预期min_ndim = 3，发现ndim = 2。收到完整形状：（无，58）

这是代码：
#重塑数据
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1]))

#检查重塑后的形状
打印（X_train.shape）
打印（X_test.shape）

#标准化像素值
X_train=X_train/255
X_测试=X_测试/255

#定义模型
模型=顺序()

#添加卷积层
model.add(Conv1D(32,3, 激活=&#39;relu&#39;,input_shape=(28,1)))

#添加池化层
model.add(MaxPool1D(pool_size=2))

#添加全连接层
模型.add(压平())
model.add(密集(100,activation=&#39;relu&#39;))

#添加输出层
model.add(密集(10,activation=&#39;softmax&#39;))

#编译模型
model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;])

模型.summary()

#拟合模型
model.fit(X_train,y_train,epochs=10)

这是输出：
型号：“sequential_40”
_________________________________________________________________
层（类型）输出形状参数#
=================================================== ===============
conv1d_23（Conv1D）（无、9、32）128
_________________________________________________________________
max_pooling1d_19（最大池化（无、4、32）0
_________________________________________________________________
flatten_15（压平）（无，128）0
_________________________________________________________________
密集_30（密集）（无，100）12900
_________________________________________________________________
密集_31（密集）（无，10）1010
=================================================== ===============
总参数：14,038
可训练参数：14,038
不可训练参数：0
]]></description>
      <guid>https://stackoverflow.com/questions/72743778/valueerror-input-0-of-layer-sequential-40-is-incompatible-with-the-layer-expec</guid>
      <pubDate>Fri, 24 Jun 2022 12:01:17 GMT</pubDate>
    </item>
    <item>
      <title>神经网络收敛到零输出</title>
      <link>https://stackoverflow.com/questions/44213659/neural-network-converging-to-zero-output</link>
      <description><![CDATA[我正在尝试训练这个神经网络来对某些数据进行预测。
我在一个小数据集（大约 100 条记录）上进行了尝试，效果非常好。然后我插入新的数据集，发现神经网络收敛到 0 输出，并且误差近似收敛于正例数与示例总数之间的比率。
我的数据集由是/否特征 (1.0/0.0) 组成，基本事实也是是/否。
我的假设：
1）有一个输出为0的局部最小值（但我尝试了学习率和初始权重的许多值，它似乎总是收敛在那里）
2）我的体重更新是错误的（但对我来说看起来不错）
3）这只是一个输出缩放问题。我尝试缩放输出（即输出/最大（输出）和输出/平均值（输出）），但结果并不好，正如您在下面提供的代码中看到的那样。我应该以不同的方式缩放它吗？软最大？ 
这是代码：
导入 pandas 作为 pd
将 numpy 导入为 np
进口泡菜
随机导入
从集合导入defaultdict

阿尔法 = 0.1
N_层 = 10
N_ITER = 10
#N_功能 = 8
初始化规模 = 1.0

train = pd.read_csv(&quot;./data/prediction.csv&quot;)

y = train[&#39;y_true&#39;].as_matrix()
y = np.vstack(y).astype(浮点数)
y测试 = y[18000:]
y = y[:18000]

X = train.drop([&#39;y_true&#39;], axis = 1).as_matrix()
Xtest = X[18000:].astype(float)
X = X[:18000]

def tanh(x,deriv=False):
    如果（导数==真）：
        返回 (1 - np.tanh(x)**2) * alpha
    别的：
        返回 np.tanh(x)

def sigmoid(x,deriv=False):
    如果（导数==真）：
        返回 x*(1-x)
    别的：
        返回 1/(1+np.exp(-x))

def relu(x,deriv=False):
    如果（导数==真）：
        返回 0.01 + 0.99*(x&gt;0)
    别的：
        返回 0.01*x + 0.99*x*(x&gt;0)

np.random.seed()

syn = defaultdict(np.array)

对于范围内的 i (N_LAYERS-1)：
    syn[i] = INIT_SCALE * np.random.random((len(X[0]),len(X[0]))) - INIT_SCALE/2
syn[N_LAYERS-1] = INIT_SCALE * np.random.random((len(X[0]),1)) - INIT_SCALE/2

l = defaultdict(np.array)
delta = defaultdict(np.array)

对于 xrange(N_ITER) 中的 j：
    l[0] = X
    对于范围 (1,N_LAYERS+1) 内的 i：
        l[i] = relu(np.dot(l[i-1],syn[i-1]))

    错误 = (y - l[N_LAYERS])

    e = np.mean(np.abs(误差))
    如果（j％1）== 0：
        print &quot;\n迭代 &quot; + str(j) + &quot; of &quot; + str(N_ITER)
        打印“错误：” + str(e)

    delta[N_LAYERS] = error*relu(l[N_LAYERS],deriv=True) * alpha
    对于范围内的 i(N_LAYERS-1,0,-1)：
        错误 = delta[i+1].dot(syn[i].T)
        delta[i] = error*relu(l[i],deriv=True) * alpha

    对于范围内的 i（N_LAYERS）：
        syn[i] += l[i].T.dot(delta[i+1])



pickle.dump(syn, open(&#39;neural_weights.pkl&#39;, &#39;wb&#39;))

# 使用 f1-measure 进行测试
# 召回率 = 真阳性 / (真阳性 + 假阴性)
# 精度 = 真阳性 /（真阳性 + 假阳性）

l[0] = X测试
对于范围 (1,N_LAYERS+1) 内的 i：
    l[i] = relu(np.dot(l[i-1],syn[i-1]))

输出 = l[N_LAYERS]/max(l[N_LAYERS])

tp = 浮点数(0)
fp = 浮点数(0)
fn = 浮点数(0)
tn = 浮点数(0)

对于 l[N_LAYERS][:50] 中的 i：
    打印我

对于范围内的 i(len(ytest))：
    如果输出[i]&gt; 0.5 且 ytest[i] == 1：
        tp += 1
    如果 out[i] &lt;= 0.5 且 ytest[i] == 1：
        fn += 1
    如果输出[i]&gt; 0.5 且 ytest[i] == 0：
        焦距 += 1
    如果 out[i] &lt;= 0.5 且 ytest[i] == 0：
        tn += 1

打印“tp：” + str（tp）
打印“fp：” + str（fp）
打印“tn:”+str(tn)
打印“fn:”+str(fn)

print &quot;\n精度: &quot; + str(tp/(tp + fp))
print &quot;回忆：&quot; + str(tp/(tp + fn))

f1 = 2 * tp /(2 * tp + fn + fp)
print &quot;\nf1-测量:&quot; + str(f1)

这是输出：
第 0 次迭代（共 10 次）
错误：0.222500767998

第 1 次迭代（共 10 次）
错误：0.222500771157

第 2 次迭代（共 10 次）
错误：0.222500774321

第 3 次迭代（共 10 次）
错误：0.22250077749

第 4 次迭代（共 10 次）
错误：0.222500780663

第 5 次迭代（共 10 次）
错误：0.222500783841

第 6 次迭代（共 10 次）
错误：0.222500787024

第 7 次迭代（共 10 次）
错误：0.222500790212

第 8 次迭代（共 10 次）
错误：0.222500793405

迭代第 9 次（共 10 次）
错误：0.222500796602


[0。]
[0。]
[5.58610895e-06]
[0。]
[0。]
[0。]
[0。]
[0。]
[4.62182626e-06]
[0。]
[0。]
[0。]
[0。]
[5.58610895e-06]
[0。]
[0。]
[0。]
[0。]
[4.62182626e-06]
[0。]
[0。]
[5.04501079e-10]
[5.58610895e-06]
[0。]
[0。]
[0。]
[0。]
[0。]
[0。]
[0。]
[0。]
[0。]
[0。]
[0。]
[5.04501079e-10]
[0。]
[0。]
[4.62182626e-06]
[0。]
[5.58610895e-06]
[0。]
[0。]
[0。]
[5.58610895e-06]
[0。]
[0。]
[0。]
[5.58610895e-06]
[0。]
[1.31432294e-05]

目标点：28.0
焦距：119.0
电话号码：5537.0
号码：1550.0

精度：0.190476190476
召回率：0.0177439797212

f1-测量：0.0324637681159
]]></description>
      <guid>https://stackoverflow.com/questions/44213659/neural-network-converging-to-zero-output</guid>
      <pubDate>Sat, 27 May 2017 06:21:36 GMT</pubDate>
    </item>
    <item>
      <title>如何计算线性回归中的正则化参数</title>
      <link>https://stackoverflow.com/questions/12182063/how-to-calculate-the-regularization-parameter-in-linear-regression</link>
      <description><![CDATA[当我们有一个高次线性多项式用于拟合线性回归设置中的一组点时，为了防止过度拟合，我们使用正则化，并在成本函数中包含 lambda 参数。然后使用该 lambda 更新梯度下降算法中的 theta 参数。
我的问题是我们如何计算这个 lambda 正则化参数？]]></description>
      <guid>https://stackoverflow.com/questions/12182063/how-to-calculate-the-regularization-parameter-in-linear-regression</guid>
      <pubDate>Wed, 29 Aug 2012 16:04:04 GMT</pubDate>
    </item>
    </channel>
</rss>