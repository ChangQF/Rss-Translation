<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 25 Jan 2024 03:15:54 GMT</lastBuildDate>
    <item>
      <title>使用 Top-N 特征方法去除特征的随机森林分类器</title>
      <link>https://stackoverflow.com/questions/77877253/random-forest-classifier-removing-features-using-top-n-features-method</link>
      <description><![CDATA[我是数据科学和机器学习技术和流程的新手。我正在开展一个个人项目，该项目使用随机森林分类器预测 NBA 比赛的获胜者。我试图删除和修改我的功能列表，以便提高准确性并减少噪音。
我实现了此处找到的解决方案：https:// datascience.stackexchange.com/questions/57697/decision-trees-should-we-discard-low-importance-features，其中我将循环遍历前 N 个最重要的特征并绘制出最终的准确性。在我的所有功能都经过该循环之后，我留下了一个如下所示的图：

正如您所看到的，生成的图表有点乱。我是否要删除具有负斜率的要素？或者说删除特征的门槛是多少？有没有更好的方法来计算噪声？鉴于我有如此多的特征，并且对训练数据上的模型准确性产生如此多的影响，我如何获得最准确的模型？]]></description>
      <guid>https://stackoverflow.com/questions/77877253/random-forest-classifier-removing-features-using-top-n-features-method</guid>
      <pubDate>Thu, 25 Jan 2024 02:32:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Flux `withgradient` 计算的损失与我计算的不匹配？</title>
      <link>https://stackoverflow.com/questions/77876955/why-doesnt-the-loss-calculated-by-flux-withgradient-match-what-i-have-calcula</link>
      <description><![CDATA[我正在尝试使用 Flux 训练一个简单的 CNN，但遇到了一个奇怪的问题...在训练过程中，损失似乎下降了（表明它正在工作），但尽管损失曲线表明“训练过的”模型仍然有效，模型输出非常糟糕，当我手动计算损失时，我注意到它与训练表明的结果不同（它表现得好像根本没有经过训练）。
然后我开始计算梯度内部与外部返回的损失，经过大量挖掘，我认为问题与 BatchNorm 层有关。考虑以下最小示例：
使用 Flux
x = rand(100,100,1,1) #假设一个灰度图像 100x100，具有 1 个通道（灰度）和 1 个批次
y = @。 5*x + 3 #输出图像，与输入值的一些关系（与此无关）
m = Chain(BatchNorm(1),Conv((1,1),1=&gt;1)) #非常简单的模型（实际上没有做任何事情，但说明了问题）
l_init = Flux.mse(m(x),y) #模型创建后的初始损失
l_grad, grad = Flux.withgradient(m -&gt; Flux.mse(m(x),y), m) #梯度计算的loss
l_final = Flux.mse(m(x),y) #使用模型再次计算损失（没有更新参数）
println(&quot;初始损失：$l_init&quot;)
println(&quot;用梯度计算的损失：$l_grad&quot;)
println(&quot;最终损失：$l_final&quot;)

上面所有的损失都会有所不同，有时会非常显着（刚才运行时我得到了 22.6、30.7 和 23.0），而我认为它们应该是相同的？
有趣的是，如果我删除 BatchNorm 层，输出都是相同的，即运行：
使用 Flux
x = rand(100,100,1,1) #假设一个灰度图像 100x100，具有 1 个通道（灰度）和 1 个批次
y = @。 5*x + 3 #输出图像
m = 链(Conv((1,1),1=&gt;1))
l_init = Flux.mse(m(x),y) #模型创建后的初始损失
l_grad, grad = Flux.withgradient(m -&gt; Flux.mse(m(x),y), m)
l_final = Flux.mse(m(x),y)
println(&quot;初始损失：$l_init&quot;)
println(&quot;用梯度计算的损失：$l_grad&quot;)
println(&quot;最终损失：$l_final&quot;)

每次损失计算都会产生相同的数字。
为什么包含 BatchNorm 层会像这样改变损失值？
我（有限）的理解是，这只是为了标准化输入值，我知道这可能会影响非标准化和标准化情况之间的损失，但我不明白为什么它会产生不同的损失值同一模型上的相同输入值，而没有更新该模型的任何参数？]]></description>
      <guid>https://stackoverflow.com/questions/77876955/why-doesnt-the-loss-calculated-by-flux-withgradient-match-what-i-have-calcula</guid>
      <pubDate>Thu, 25 Jan 2024 00:30:43 GMT</pubDate>
    </item>
    <item>
      <title>Transformer架构的输入大小问题</title>
      <link>https://stackoverflow.com/questions/77876582/transformer-architectures-input-size-problem</link>
      <description><![CDATA[所以，最近，我读到了《Attention is all you need》（注意力就是你所需要的）。论文，描述了变压器的架构。在 Transformer 中，有一个叫做 masked multi-head Attention 的组件，仅在解码器部分使用。
问题是，解码器的输入是编码器的输出以及之前生成的标记。并且之前每次迭代生成的token数量不同，但是线性层的神经元数量是相同的。因此，我们必须使用“pad tokens”来实现。屏蔽注意力用于对这些 pad token 给予 0 注意力。
编码器也是如此。输入可以是不同的大小，所以我们还必须使用填充令牌，但在这里，我们不使用屏蔽注意力，我很好奇，为什么？
或者我们不在那里使用填充令牌，而是使用其他东西？
我尝试向 chat-gpt 询问此事，但它没有给我合理的答案，所以我来到这里。]]></description>
      <guid>https://stackoverflow.com/questions/77876582/transformer-architectures-input-size-problem</guid>
      <pubDate>Wed, 24 Jan 2024 22:28:51 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow MobilenetV2 对象检测 Web 模型格式导出问题</title>
      <link>https://stackoverflow.com/questions/77876211/tensorflow-mobilenetv2-object-detection-web-model-format-export-problem</link>
      <description><![CDATA[我想微调 MobileNet v2 并导出为 json + 二进制格式。不幸的是，出口出现了问题。我将在最基本的示例中展示它 - 没有微调的普通 MobileNet v2：

下载并解压SSD MobileNet v2.
使用tensorflowjs_converter我可以将saved_model/导出为Web格式。它按预期工作：

tensorflowjs_converter --control_flow_v2=True --input_format=tf_saved_model --metadata= --saved_model_tags=serve --signature_name=serving_default --strip_debug_ops=True --weight_shard_size_bytes=4194304 saving_model/ web_model/


可以在浏览器中导入模型。只需加载 tfjs：

]]></description>
      <guid>https://stackoverflow.com/questions/77876211/tensorflow-mobilenetv2-object-detection-web-model-format-export-problem</guid>
      <pubDate>Wed, 24 Jan 2024 21:05:03 GMT</pubDate>
    </item>
    <item>
      <title>配置 Kaggle 以在两个 T4 GPU 之间进行分布式训练和内存共享</title>
      <link>https://stackoverflow.com/questions/77875716/configuring-kaggle-for-distributed-training-and-memory-sharing-across-two-t4-gpu</link>
      <description><![CDATA[我正在尝试在 Kaggle 上使用以下命令来训练 Dreambooth：
！加速启动 --num_cpu_threads_per_process=2 “./sdxl_train.py” \
  --pretrained_model_name_or_path=“stabilityai/stable-diffusion-xl-base-1.0” \
  --train_data_dir=“数据集/img” \
  --reg_data_dir=“数据集/reg” \
  --output_dir=“输出” \
  --output_name=“SDXLDreambooth” \
  --save_model_as=“安全张量” \
  --train_batch_size=1 \
  --max_train_steps=8000 \
  --save_every_n_steps=4001 \
  --optimizer_type=“adafactor” \
  --optimizer_argsscale_parameter=Falserelative_step=Falsewarmup_init=False\
  --xformers \
  --lr_scheduler=“constant_with_warmup” \
  --lr_warmup_steps=100 \
  --learning_rate=2.5e-6 \
  --max_grad_norm=0.0 \
  --分辨率=“1024,1024” \
  --save_ precision =“fp16” \
  --save_n_epoch_ratio=1 \
  --max_data_loader_n_workers=1 \
  --persistent_data_loader_workers \
  --mixed_ precision =“fp16” \
  --full_fp16 \
  --logging_dir=&quot;日志&quot; \
  --log_prefix=“最后” \
  --gradient_checkpointing \
  --caption_extension=“.txt” \
  --no_half_vae \
  --缓存潜伏

并添加 --train_text_encoder 会在具有 16 GB VRAM 的 P100 GPU 上出现内存不足错误，即使启用了所有优化。我已经测试过在 Modal 上具有 24 GB VRAM 的 L4 GPU 上运行相同的命令，并且它成功运行最大 VRAM 利用率约为 18 GB。
但是，我注意到 Kaggle 还提供了使用两个 T4 GPU（每个 GPU 具有 16 GB VRAM）的选项，这让我想知道是否可以更改那里的环境配置（包括例如更改脚本， DeepSpeed 配置、加速配置等）以允许在 2 个 GPU 之间共享内存，其总组合内存 (32 GB) 应允许命令运行（需要 18 GB 内存）。
Kaggle 设置的默认行为似乎是在 2 个 GPU 之间复制配置并并行处理训练，因此使用 --train_text_encoder 选项，脚本将需要每个 GPU 18 GB ，导致内存不足错误。
我应该如何配置环境，以便允许两个 GPU 之间共享内存，并避免收到内存不足错误？]]></description>
      <guid>https://stackoverflow.com/questions/77875716/configuring-kaggle-for-distributed-training-and-memory-sharing-across-two-t4-gpu</guid>
      <pubDate>Wed, 24 Jan 2024 19:22:44 GMT</pubDate>
    </item>
    <item>
      <title>我如何开始使用 Python 进行 IA？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77875714/ia-with-python-how-i-start</link>
      <description><![CDATA[我已经学习了 Python 语言的基础知识，并想开始编程自动化和人工智能。我从哪里开始以及如何开始？我了解这些库，但我应该阅读文档并自己尝试吗？还是有专门的课程？
我希望您能给我一些有用的提示。]]></description>
      <guid>https://stackoverflow.com/questions/77875714/ia-with-python-how-i-start</guid>
      <pubDate>Wed, 24 Jan 2024 19:22:33 GMT</pubDate>
    </item>
    <item>
      <title>Aws SageMaker 实时随机砍伐森林</title>
      <link>https://stackoverflow.com/questions/77869995/aws-sagemaker-random-cut-forest-real-time</link>
      <description><![CDATA[我正在尝试了解 Sagemaker 的工作原理。我的 OpenSearch 中有一些数据需要识别异常情况。
我知道我可以执行以下逻辑：

将我的 OpenSearch 数据导入为 CSV 或使用 SDK；
使用随机森林砍伐 (RCF) 算法；
在 SageMaker 中生成端点；

但是，我的 OpenSearch 数据是实时的，我希望实时预测有一个（实时）仪表板，我们可以在其中观察异常行为并可能生成某种警报。
当 OpenSearch 收到新数据时，是否有可能在 SageMaker 中自动运行查询并将结果显示在像 Grafana 这样的仪表板上？]]></description>
      <guid>https://stackoverflow.com/questions/77869995/aws-sagemaker-random-cut-forest-real-time</guid>
      <pubDate>Wed, 24 Jan 2024 00:43:59 GMT</pubDate>
    </item>
    <item>
      <title>使用 sagemaker 使用开放搜索索引</title>
      <link>https://stackoverflow.com/questions/77833977/consume-opensearch-indices-with-sagemaker</link>
      <description><![CDATA[我需要从 AWS OpenSearch 中的索引中提取数据以进行聚合并分析异常/变化。
PS：我认为逻辑是有效的......
我正在考虑使用 Sagemaker + OpenSearch。逻辑如下：Sagemaker 连接到 Opensearch，从索引中获取实时数据，对其进行处理并通过 Sagemaker 端点使其可用，但出现了以下问题：
使用“OpenSearch 服务 AI 连接器&quot;并使用下面的代码？
连接示例：
from opensearchpy import OpenSearch、RequestsHttpConnection、AWSV4SignerAuth
导入boto3

凭证 = boto3.Session().get_credentials()
主机 = OPENSEARCH_HOSTNAME[“*.es.amazon.com”]

auth = AWSV4SignerAuth(凭证, &#39;us-east-1&#39;, &#39;es&#39;)
# 使用opensearch Python客户端连接opensearch集群。

os_client = OpenSearch(hosts=[{&#39;主机&#39;: 主机, &#39;端口&#39;: 443}],
 http_auth = 身份验证，
 use_ssl=真，
 verify_certs=真，
 connection_class=RequestsHttpConnection,
 超时=30
 ）

打印（os_client）

打印（os_client.ping（））
]]></description>
      <guid>https://stackoverflow.com/questions/77833977/consume-opensearch-indices-with-sagemaker</guid>
      <pubDate>Wed, 17 Jan 2024 16:26:06 GMT</pubDate>
    </item>
    <item>
      <title>SetFit 训练未完成评估步骤</title>
      <link>https://stackoverflow.com/questions/77760620/setfit-training-not-finishing-evaluation-step</link>
      <description><![CDATA[我正在尝试使用 SetFit 训练简单的二元分类，但我的库有问题。我使用 Huggingface 来管理我的数据集。数据集由文本和标签列组成。如果我打印我的数据集，它看起来像这样：
dataset = load_dataset(“&lt;我的数据集&gt;”)
打印（数据集）

输出：
DatasetDict({
    火车：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：20
    })
    评估：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：10
    })
    测试：数据集（{
        特征：[&#39;文本&#39;，&#39;标签&#39;]，
        行数：135
    })
})

这是我的培训代码：
# 使用预训练模型初始化 SetFit 模型并定义标签名称
模型 = SetFitModel.from_pretrained(
    “释义-多语言-mpnet-base-v2”，
    标签=[“阴性”,“阳性”],
）

# 定义训练参数
args = 训练参数(
    批量大小=32，
    num_epochs=8,
    evaluation_strategy=“纪元”，
    save_strategy=“纪元”,
    load_best_model_at_end=True
）

# 初始化训练器
教练=教练（
    型号=型号，
    参数=参数，
    train_dataset=数据集[“火车”],
    eval_dataset=数据集[“eval”],
    度量=“准确度”，
    column_mapping={&quot;text&quot;: &quot;text&quot;, &quot;label&quot;: &quot;label&quot;} # 将数据集列映射到训练器期望的文本/标签
）

# 训练模型
训练师.train()

但是我现在遇到的问题是训练行为非常奇怪。我没有受到任何培训或验证损失，评估步骤也没有完成。我不知道问题出在哪里。

另请注意，我稍微更改了参数以提高训练速度。它通常有更多的步骤等等。使用正常参数时它仍然表现得很奇怪。我还使用 SetFit 1.0.1 版本。我在 GitHub 存储库中没有发现任何与此相关的问题。]]></description>
      <guid>https://stackoverflow.com/questions/77760620/setfit-training-not-finishing-evaluation-step</guid>
      <pubDate>Thu, 04 Jan 2024 19:02:49 GMT</pubDate>
    </item>
    <item>
      <title>在docker中运行doctr ml模型永远挂起</title>
      <link>https://stackoverflow.com/questions/77552150/running-doctr-ml-model-in-docker-hangs-forever</link>
      <description><![CDATA[我有一个 fastapi 应用程序，可以在代码中加载 doctr 模型，它只需要一个图像路径并将其转换为文本
这是代码
from doctr.io 导入 DocumentFile
从 doctr.models 导入 o​​cr_predictor
__版本__ =“0.1.1”
模型= ocr_predictor（预训练= True）



def process_image(image_path):
    文档 = DocumentFile.from_images(image_path)
    结果=模型（文档）
    json_response = 结果.export()
    返回 json_response

我只想通过 API 公开模型。
main.py 文件，我的 api 代码集
&lt;前&gt;&lt;代码&gt;

从 app.model.model 导入 __version__ 作为 model_version
从 app.model.model 导入 process_image
从 fastapi 导入 FastAPI、HTTPException、UploadFile

应用程序 = FastAPI()

@app.get(“/”)
def home():
    return {“health_check”：“确定”，“model_version”：“model_version”}

当我使用此代码运行 main.py 时
uvicorn app.main:app --reload
一切正常
但是当我尝试对其进行 dockerize 时，它​​就永远挂起，这是我的 Dockerfile
来自 tiangolo/uvicorn-gunicorn-fastapi:python3.9

运行 apt-get update

运行 apt install -y libgl1-mesa-glx

复制 ./requirements.txt /app/requirements.txt

运行 pip install --no-cache-dir --upgrade -r /app/requirements.txt

复制 ./app /app/app

注意：我有一台 m1 mac
我尝试了一切使其工作，包括将导入移动到函数内
__version__ = “0.1.1”

def process_image(image_path):
    从 doctr.io 导入文档文件
    从 doctr.models 导入 o​​cr_predictor
    模型= ocr_predictor（预训练= True）
    文档 = DocumentFile.from_images(image_path)
    结果=模型（文档）
    json_response = 结果.export()
    返回 json_response

但是当 api 到达导入时，一切都不起作用，它只会永远挂起，而且只有在使用 docker 时才会出现这种情况]]></description>
      <guid>https://stackoverflow.com/questions/77552150/running-doctr-ml-model-in-docker-hangs-forever</guid>
      <pubDate>Sun, 26 Nov 2023 14:46:54 GMT</pubDate>
    </item>
    <item>
      <title>为什么使用 AI 进行图像分割在每次循环迭代时的时间更长？</title>
      <link>https://stackoverflow.com/questions/77454517/why-is-image-segmentation-using-ai-longer-at-each-iteration-of-a-loop</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77454517/why-is-image-segmentation-using-ai-longer-at-each-iteration-of-a-loop</guid>
      <pubDate>Thu, 09 Nov 2023 15:43:46 GMT</pubDate>
    </item>
    <item>
      <title>在没有 GPU 的情况下运行 Vicuna 的 FastChat 模型时出错</title>
      <link>https://stackoverflow.com/questions/76627456/error-when-running-vicunas-fastchat-model-without-gpu</link>
      <description><![CDATA[我希望使用 Vicuna 开源模型来训练我的数据集。
我的计算机中没有 GPU，所以我想使用他们的 RESTful API 服务器。我使用 Windows PowerShell 执行以下命令。
根据他们的解释（https://github. com/lm-sys/FastChat/blob/main/docs/openai_api.md)
首先，我启动了命令
&lt;前&gt;&lt;代码&gt;&gt; python3 -m fastchat.serve.controller

然后，它为我打开了一个本地主机。我在浏览器中打开它，它显示以下消息：
&lt;前&gt;&lt;代码&gt;&gt; {“详细信息”：“未找到”}。

接下来，我打开一个新的 PowerShell 窗口并运行第二个命令：
&lt;前&gt;&lt;代码&gt;&gt; python3 -m fastchat.serve.model_worker --模型路径 lmsys/vicuna-7b-v1.3

但是，我遇到了以下错误：
断言错误：未在启用 CUDA 的情况下编译 Torch

出现此错误是因为我的计算机中没有 GPU 吗？]]></description>
      <guid>https://stackoverflow.com/questions/76627456/error-when-running-vicunas-fastchat-model-without-gpu</guid>
      <pubDate>Thu, 06 Jul 2023 09:36:27 GMT</pubDate>
    </item>
    <item>
      <title>如何在海量数据上训练机器学习模型？</title>
      <link>https://stackoverflow.com/questions/74886400/how-to-train-a-machine-learning-model-on-huge-amount-of-data</link>
      <description><![CDATA[关键点：数据集太大了，我几乎无法将其存储在硬件中。 （拍字节）
假设我的数据集中有数万亿行。该数据集太大，无法存储在内存中。我想在这个数据集上训练一个机器学习模型，比如逻辑回归。我该怎么办？
现在，我知道亚马逊/谷歌在大量数据上进行机器学习。他们怎样做呢？例如点击数据集，全局每个智能设备的输入都存储在一个数据集中。
拼命寻找新想法并乐于接受修正。
我的思路：

加载部分数据到内存
执行梯度下降

这样优化就是小批量下降。
现在的问题是，在优化中，无论是SGD还是mini Batch，在最坏的情况下，当它遍历完所有数据时就会停止。遍历整个数据集是不可能的。
所以我有了提前停止的想法。提前停止保留验证集，并在错误停止下降/收敛于验证集时停止优化。但由于数据集的大小，这可能不可行。
现在我正在考虑简单地随机采样训练集和测试集，并使用可行的大小来训练模型。]]></description>
      <guid>https://stackoverflow.com/questions/74886400/how-to-train-a-machine-learning-model-on-huge-amount-of-data</guid>
      <pubDate>Thu, 22 Dec 2022 09:16:33 GMT</pubDate>
    </item>
    <item>
      <title>如何运行 Huggingface Helsinki-NLP 模型</title>
      <link>https://stackoverflow.com/questions/70043467/how-to-run-huggingface-helsinki-nlp-models</link>
      <description><![CDATA[我正在尝试使用 huggingface 中的赫尔辛基-NLP 模型，但是
我找不到任何有关如何操作的说明。
自述文件是计算机生成的，不包含解释。
有人可以给我指出一份入门指南，或者展示如何运行像 opus-mt-en-es 这样的模型的示例吗？]]></description>
      <guid>https://stackoverflow.com/questions/70043467/how-to-run-huggingface-helsinki-nlp-models</guid>
      <pubDate>Sat, 20 Nov 2021 05:27:06 GMT</pubDate>
    </item>
    <item>
      <title>R - S 和 F 计数中的 bestglm 问题不能 <0</title>
      <link>https://stackoverflow.com/questions/37964435/issue-with-bestglm-in-r-s-and-f-counts-can-not-be-0</link>
      <description><![CDATA[我尝试使用bestglm进行子集回归，但是当我执行它时，出现以下错误
bestglm 中的错误（Xy=H.bestglm2，family = 二项式，IC =“AC”，方法=“详尽”：
二项式非逻辑回归：S 和 F 计数不能 &lt;0

我不知道这意味着什么，也找不到有关此问题的任何信息。
谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/37964435/issue-with-bestglm-in-r-s-and-f-counts-can-not-be-0</guid>
      <pubDate>Wed, 22 Jun 2016 09:50:28 GMT</pubDate>
    </item>
    </channel>
</rss>