<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 02 Oct 2024 21:18:39 GMT</lastBuildDate>
    <item>
      <title>R 中 cox 模型的 c 指数</title>
      <link>https://stackoverflow.com/questions/79048475/c-index-in-r-for-cox-model</link>
      <description><![CDATA[我最近在机器学习中为 Cox 模型运行了我的 R 代码。我已经在模型中进行了测试和训练。我想运行 c 索引一次进行测试，一次进行训练。我有代码，但根据 Cox 的 AUC，值太小了。我该如何解决这个问题？非常感谢
这些是我的代码：
 train_indices1 &lt;- sample(1:nrow(df6), 0.8 * nrow(df6))

training_data1 &lt;- df6[train_indices1, ]

testing_data1 &lt;- df6[-train_indices1, ]

res.cox11 &lt;- coxph(Surv(times,eventHFF) ~ AgeCategori + Gender + shoghl+ education + sokonat +taahol + BMIcategori+Hypertension+ DiabetesMellitus+ CAD +

HyperLipidemia+Sm​​oking+CKDDialysis +AtrialFibrillation+ StrokeTIA+ CTD+ ChemotherapyRadiotherapy +恶性肿瘤+急性心衰类型+HF类型+SBP+DBP+脾气猫+

心率猫+SPO2+NYHA等级+AF需要治疗+急性透析超滤+WRF，数据 = training_data1)

summary(res.cox11)

predicted_status1 &lt;- predict(res.cox11, newdata = testing_data1, type = &quot;risk&quot;)

predicted_status_binary1&lt;- ifelse(predicted_status1 &gt; 0.5, 1, 0)

confusion1 &lt;-fusionMatrix(factor(predicted_status_binary1), factor(testing_data1$eventHFF))

print(confusion1)

COX_MODEL1 &lt;- roc(testing_data1$eventHFF ~ predict_status1, plot = TRUE，print.auc = TRUE，main = &quot;ROC - COX 模型&quot;)

train_cindex &lt;- concordance(Surv(training_data1$times, training_data1$eventHFF) ~ predict(res.cox11, newdata = training_data1))$concordance

test_cindex &lt;- concordance(Surv(testing_data1$times, testing_data1$eventHFF) ~ predict(res.cox11, newdata = testing_data1))$concordance

cat(&quot;C-index for training data:&quot;, train_cindex, &quot;\n&quot;)

cat(&quot;C-index for testing data:&quot;, test_cindex, &quot;\n&quot;)

我该如何解决这个问题？
&gt; ci1 &lt;- ci.auc(COX_MODEL1)
95% CI: 0.5169-0.7843 (DeLong)
&gt; cat(&quot;训练数据的 C 指数 (重新拟合)：&quot;, train_cindex_refit, &quot;\n&quot;)
训练数据的 C 指数 (重新拟合)：0.3102263 
&gt; cat(&quot;测试数据的 C 指数 (重新拟合)：&quot;, test_cindex_refit, &quot;\n&quot;)
测试数据的 C 指数 (重新拟合)：0.3152355 
]]></description>
      <guid>https://stackoverflow.com/questions/79048475/c-index-in-r-for-cox-model</guid>
      <pubDate>Wed, 02 Oct 2024 21:06:52 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Tesseract OCR 读取俄罗斯车牌？</title>
      <link>https://stackoverflow.com/questions/79048374/how-can-i-use-tesseract-ocr-to-read-a-russian-license-plate</link>
      <description><![CDATA[我正在研究读取俄罗斯车辆的车牌。
Tesseract OCR 无法正确检测车牌。
我尝试了很多设置，例如（噪声过滤器、内核更改）

尝试了不同的内核，拉普拉斯、自定义、高斯
尝试了旋转、降低噪音
应用了一组字母/数字，但算法上将 2 检测为 Z

有什么建议可以尝试吗？
import cv2
import pytesseract
import numpy as np
from google.colab.patches import cv2_imshow

# 可选：如果 Tesseract 可执行文件路径尚未位于系统 PATH 中，请设置该路径
# pytesseract.pytesseract.tesseract_cmd = r&#39;C:\Program Files\Tesseract-OCR\tesseract.exe&#39;

# 函数用于围绕图像中心旋转图像
def rotate_image(image, angle):
(h, w) = image.shape[:2]
center = (w // 2, h // 2)
M = cv2.getRotationMatrix2D(center, angle, 1.0)
rotated = cv2.warpAffine(image, M, (w, h))
return rotated

# 函数用于根据图像矩计算倾斜角度
def compute_skew_angle(image):
coords = np.column_stack(np.where(image &gt; 0))
angle = cv2.minAreaRect(coords)[-1]
if angle &lt; -45:
angle = -(90 + angle)
else:
angle = -angle
return angle

# 加载 Haar 级联以检测车牌
plate_cascade = cv2.CascadeClassifier( &#39;/content/haarcascade_russian_plate_number.xml&#39;)

# 加载图像
img = cv2.imread(&#39;/content/vehicle1.png&#39;)
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
cv2_imshow(gray)

# 检测车牌
plates = plate_cascade.detectMultiScale(gray, 1.1, 4)

for (x, y, w, h) in boards:
# 在检测到的车牌周围绘制一个矩形
cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)

# 提取车牌区域
plate_img = gray[y:y + h, x:x + w]
cv2_imshow(plate_img)

# 将车牌图像调整为标准尺寸（例如 400x100）
scaled_plate = cv2.resize(plate_img, (4000, 1000))
cv2_imshow(scaled_plate)
# 应用高斯模糊以降低噪音
denoised_plate = cv2.GaussianBlur(scaled_plate, (5, 5), 0)
cv2_imshow(denoised_plate)
# 应用自适应阈值以获得更好的 OCR 性能
#adaptive_thresh_plate = cv2.adaptiveThreshold(denoised_plate, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
# cv2.THRESH_BINARY, 11, 2)
#cv2_imshow(adaptive_thresh_plate)
# 计算车牌倾斜角度
#skew_angle = compute_skew_angle(adaptive_thresh_plate)
#skew_angle = compute_skew_angle(denoised_plate)
# 旋转图像以校正倾斜
#rotated_plate = rotate_image(denoised_plate, skew_angle)
#cv2_imshow(rotated_plate)

# 对旋转和处理后的车牌执行 OCR
#plate_text = pytesseract.image_to_string(denoised_plate, config=&#39;--psm 6&#39;)
#enchancedimage 
kernel = cv2.getGaussianKernel(ksize=5, sigma=1.5) 
kernel = kernel @ kernel.T

enchanced_img = cv2.filter2D(denoised_plate, -1, kernel)
cv2_imshow(enchanced_img)
rotated_plate = rotate_image(denoised_plate, -2.5)
cv2_imshow(rotated_plate)
for i in range(3,14):
print(f&#39;PSM: {i}&#39;)
#plate_text =pytesseract.image_to_string(enchanced_img, 
#config = f&#39;--psm {i} --oem 3 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;)
plate_text =pytesseract.image_to_string(rotated_plate, 
config = f&#39;--psm {i} --oem 3 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;)

#plate_text =pytesseract.image_to_string(enchanced_img,config = f&#39;--psm {i}&#39;)
print(&quot;检测到的车牌号：&quot;, plate_text)

# 显示带检测框的原始图像
#cv2_imshow(img)
cv2.waitKey(0)
cv2.destroyAllWindows()


]]></description>
      <guid>https://stackoverflow.com/questions/79048374/how-can-i-use-tesseract-ocr-to-read-a-russian-license-plate</guid>
      <pubDate>Wed, 02 Oct 2024 20:30:40 GMT</pubDate>
    </item>
    <item>
      <title>在使用 TensorFlow 计算 SINR 时遇到问题，当涉及到复数时，TF 的数值稳定性如何？</title>
      <link>https://stackoverflow.com/questions/79048361/having-issue-with-calculating-sinr-using-tensorflow-how-numerical-stable-is-tf</link>
      <description><![CDATA[因此，我有用于训练神经网络的代码，对于具有多组 (G) 和多用户 (K) 的系统，损失为负 SINR。然而，当我计算 sinr 时，发生了一些奇怪的事情。
所以SINR公式是这样的：

这是我的代码：
def find_sinr_over_group(H, W):

    西格玛2 = 1
    # 计算 W 的 Hermitian 转置（共轭转置）
    W_H = tf.transpose(tf.math.conj(W), perm=[0, 2, 1]) # 形状：(batch_size, G, M)

    # 计算每组中每个用户的信号功率
    信号功率 = []
    for g in range(W.shape[-1]): # 循环每个组 g
        w_h_g = W_H[:, g, :] # 形状: (batch_size, M)
        h_g = H[:, :, :, g] # 形状: (batch_size, M, K)

        # 矩阵乘法计算信号功率
        s = tf.matmul(w_h_g[:, tf.newaxis, :], h_g) # 形状: (batch_size, 1, K)
        s = tf.squeeze(s, axis=1) # 形状: (batch_size, K)
        signal_power.append(s)

    signal_power = tf.stack(signal_power, axis=-1) # 形状：(batch_size, K, G)
    signal_power = tf.math.real(tf.math.multiply(signal_power, tf.math.conj(signal_power)))
    # signal_power = tf.math.abs(signal_power) ** 2 # 取绝对平方

    # 计算每组中每个用户的总功率
    总功率=[]
    for g in range(W.shape[-1]): # 循环每个组 g
        w_h = W_H[:, :, :] # 形状：(batch_size, G, M)
        h_g = H[:, :, :, g] # 形状: (batch_size, M, K)

        # 矩阵乘法计算总功率
        t = tf.matmul(w_h, h_g) # 形状：(batch_size, G, K)
        t = tf.math.real(tf.math.multiply(t, tf.math.conj(t)))
        # t = tf.math.abs(t) ** 2 # 形状：(batch_size, G, K)
        Total_power.append(tf.reduce_sum(t, axis=1)) # 对 G 求和得到 (batch_size, K)

    Total_power = tf.stack(total_power, axis=-1) # 形状: (batch_size, K, G)

    # 通过从总功率中减去信号功率来隔离干扰功率
    干扰功率 = 总功率 - 信号功率 # 形状：(batch_size, K, G)

    #添加噪声功率
    Interference_plus_noise_power = Interference_power + sigma2 # 添加噪声

    # 计算SINR
    sinr = signal_power / Interference_plus_noise_power # 形状：(batch_size, K, G)

    返回正弦值

但问题是，在某些情况下干扰功率是负的。这在数字上是不可能的，因为信号功率只是总功率的一个特例。有人知道为什么会发生这种情况以及我应该如何解决它吗？
正如你所看到的，一开始我使用了 tf.abd 函数，然后以 2 的幂进行提升。我认为这可能是 abs 的问题，所以我尝试通过共轭进行 myltiping 来获取信号的功率。但我仍然有这个问题。]]></description>
      <guid>https://stackoverflow.com/questions/79048361/having-issue-with-calculating-sinr-using-tensorflow-how-numerical-stable-is-tf</guid>
      <pubDate>Wed, 02 Oct 2024 20:23:26 GMT</pubDate>
    </item>
    <item>
      <title>为什么 opencv.dnn.blobFromImage() 输出转换回 rgb 图像后包含 9 张灰度图像？</title>
      <link>https://stackoverflow.com/questions/79048116/why-does-opencv-dnn-blobfromimage-output-converted-back-to-rgb-image-contain-g</link>
      <description><![CDATA[大家好，我想问你们一个问题。
据我所知，blobFromImage 将 img 形状：（宽度、高度、通道）转换为 4d 数组（n、通道、宽度、高度）。
因此，如果您传递 1/255 的 scale_factor。| 大小（640,640）据我所知，每个元素应计算为 RGB =&gt; R = R/255。| G = G/255。|...
值 = (U8 - 平均值) * scale_factor

基本上 minmax 在 0 到 1 之间标准化。
所以在 py 上。
我尝试将输出 blob/ndarray * 255 相乘。并重新整形为（640, 640, 3），看起来输出图像是一张包含 3 行 3 列灰度和略有不同的饱和度的 9 张图像的图像？这是我尝试过的，与上面的 255 示例输出相同。
 test = cv2.dnn.blobFromImage(img, 1.0/127.5, (640, 640), (127.5, 127.5, 127.5), swapRB=True)
t1 = test * 127.5
t2 = t1 + 127.5
cv2.imwrite(&quot;./test_output.jpg&quot;, t2.reshape((640, 640, 3)))

我一直在查看他们的 opencv repo
 subtract(images[i], mean, images[i]);
multiply(images[i], scalefactor, images[i]);

说实话，看起来在 opencv lib 中实现的方式相同，但想请教一下你们的意见。
另一个问题是，如果输入的是完整的 u8 rgb 值，为什么会变成灰度？
我尝试通过应用类似的公式转换 4d ndarray 以匹配 blobFromImage 的输出。但输出不一样。
我希望转换为 (1, 3, w, h) 的 ndarray 的图像减去平均值并乘以比例因子，当您转换回 (width, height, channel) 时，与 blobFromImage 的输出相同。]]></description>
      <guid>https://stackoverflow.com/questions/79048116/why-does-opencv-dnn-blobfromimage-output-converted-back-to-rgb-image-contain-g</guid>
      <pubDate>Wed, 02 Oct 2024 18:53:23 GMT</pubDate>
    </item>
    <item>
      <title>为什么 tf.keras.models.load_model 要重新构建模型？</title>
      <link>https://stackoverflow.com/questions/79047845/why-is-the-tf-keras-models-load-model-building-the-model-again</link>
      <description><![CDATA[ValueError: 顺序模型“sequential_2”已配置为使用
输入形状（None、224、224、3）。您无法使用 input_shape [None、224、224、3] 构建它

def load_model(model_path):
&quot;&quot;&quot;
从指定路径加载已保存的模型。
&quot;&quot;&quot;

tf.keras.config.enable_unsafe_deserialization()
print(f&quot;正在加载已保存的模型：{model_path}&quot;)
model = tf.keras.models.load_model(model_path)
return model

loaded_1000_image_model = load_model(&#39;/content/drive/MyDrive/Dog Vision/models/20241002-16491727887796-1000-images-mobilenetv2-Adam.h5&#39;)

我原本以为它会加载模型而不会出现任何错误，但由于某种原因，它给出了一个值错误，尽管我没有尝试重建或再次给出任何输入形状。]]></description>
      <guid>https://stackoverflow.com/questions/79047845/why-is-the-tf-keras-models-load-model-building-the-model-again</guid>
      <pubDate>Wed, 02 Oct 2024 17:18:38 GMT</pubDate>
    </item>
    <item>
      <title>我的模型目前存在多少过度拟合问题？[关闭]</title>
      <link>https://stackoverflow.com/questions/79047127/how-much-of-an-overfitting-issue-do-my-models-currently-have</link>
      <description><![CDATA[我正在使用三个机器学习模型 - 逻辑回归、梯度提升和深度神经网络 - 使用相对简单的数据集和使用“sklearn”的机器学习模型模板。我获得的性能指标似乎很强，但我的老师提到了过度拟合的可能性，因为测试准确率略低于训练准确率。此外，它是一种二元分类。
我进行了 5 倍交叉验证，交叉验证分数非常接近我的模型准确率。但是，我仍然不确定这是否表示过度拟合，或者我的模型是否按预期运行。
以下是每个模型的结果：
逻辑回归：
测试准确率：97.69%
训练准确率：98.15%
平均交叉验证得分：97.97%
梯度提升：
测试准确率：97.98%
训练准确率：98.79%
平均交叉验证得分：97.97%
深度神经网络：
测试准确率：97.49%
训练准确率： 98.23%
平均交叉验证得分：97.92%
我是否可以得出结论：尽管可能存在轻微的过度拟合，但模型对于分类仍然有效，并且所使用的特征可能对于对目标进行分类非常有用？]]></description>
      <guid>https://stackoverflow.com/questions/79047127/how-much-of-an-overfitting-issue-do-my-models-currently-have</guid>
      <pubDate>Wed, 02 Oct 2024 14:05:29 GMT</pubDate>
    </item>
    <item>
      <title>构建电子邮件解析、数据提取和数据验证的系统[关闭]</title>
      <link>https://stackoverflow.com/questions/79047083/build-a-system-for-email-parsing-and-data-extraction-and-data-validation</link>
      <description><![CDATA[我正在为 B2B 匹配平台开发 MVP。核心思想是根据客户的需求和能力自动将客户与服务提供商配对。以下是我想要实现的目标：

从收到的电子邮件中提取数据（客户要求和服务提供商详细信息）：非结构化数据（客户和服务提供商之间的沟通没有标准）
以可用的格式构造这些数据
将这些结构化数据用于智能匹配系统

目前，我正在使用 OpenAI 的 API（gpt4-* 模型）来处理电子邮件解析和数据提取。虽然它在大多数情况下都能正常工作，但我担心它在生产环境中的稳健性和可靠性：有时，我会出现幻觉。
我的问题是：对于这种任务，有没有比这种方法更强大的替代方案？我正在寻找能够实现以下功能的东西：

可靠地解析电子邮件并提取相关信息
处理大量电子邮件
提供一致且易于使用的结构化输出

我只接受开源解决方案。
我尝试过的方法：

使用 OpenAI API 模型从非结构化数据中提取关键信息，但我仍然有幻觉。

我还需要一种实时构造数据的方法。]]></description>
      <guid>https://stackoverflow.com/questions/79047083/build-a-system-for-email-parsing-and-data-extraction-and-data-validation</guid>
      <pubDate>Wed, 02 Oct 2024 13:55:12 GMT</pubDate>
    </item>
    <item>
      <title>我可以采取下一步措施来制作一个好的图像重复和接近重复查找器[关闭]</title>
      <link>https://stackoverflow.com/questions/79046818/next-steps-i-can-take-to-make-a-good-image-duplicate-and-near-duplicate-finder</link>
      <description><![CDATA[我正在尝试制作一个可以查找图像重复和近似图像重复的应用程序，以便从您的图像库中查找相似的照片。我有一个 NAS 设置，里面有大约 30-40gb 的图像，其中很多图像都有我和我的家人拍摄的多张类似图像。
对于我当前的实现，我可以拍摄 2 幅图像，从中提取嵌入（使用 CLIP 库），比较它的余弦相似度（对于精确重复）和欧几里得距离（对于压缩重复，如从聊天中下载的图像或图像的屏幕截图）。
我如何进一步推进这个项目并制作更好的应用程序？我目前的实现如下所示：
main.py
从 PIL 导入图像
导入 torch
导入 numpy 作为 np
导入 cv2 作为 cv
导入 clip
从 sklearn.cluster 导入 KMeans

设备 = &quot;cpu&quot;
image_path = &quot;assets/photos/aayush.jpeg&quot;
image_path2 = &quot;assets/photos/aayushResized.png&quot;

# 加载 CLIP 模型和预处理函数
model, preprocess = clip.load(&quot;ViT-L/14&quot;, device, jit=False)

# 预处理图像并添加批处理维度
image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
image2 = preprocess(Image.open(image_path2)).unsqueeze(0).to(device)

print(&quot;处理后的图像数据类型为：&quot;, type(image))

def calculate_distance(embedding1, embedding2):
&quot;&quot;&quot; 计算两幅图像之间的欧几里得距离 &quot;&quot;&quot;
dist = torch.nn. functional.pairwise_distance(embedding1, embedding2)
return (dist)

def calculate_cosine_similarity(embedding1, embedding2):
&quot;&quot;&quot; 计算 2 个图像嵌入之间的余弦相似度 &quot;&quot;&quot;
return torch.nn. functional.cosine_similarity(embedding1, embedding2)

def resize_with_aspect_ratio(raw_image_path, new_width):
# 读取原始图像
original_image = cv.imread(raw_image_path)

# 计算纵横比
aspects_ratio = original_image.shape[1] / original_image.shape[0]

# 根据所需宽度确定新高度
determined_height = int(new_width / aspects_ratio)

# 调整图像大小
resized_image = cv.resize(original_image, (new_width, determined_height))
print(&quot;Newly Resized images shape: &quot;, resized_image.shape)
cv.imshow(&quot;Screen&quot;, resized_image)
cv.imwrite(&quot;aayushResized.png&quot;, resized_image)
cv.waitKey(0)
cv.destroyAllWindows()

return resized_image

def normalize(image):
# 计算每个通道的标准差
channel_stds = np.std(image, axis=(0, 1))
print(channel_stds)
# 通过将图像除以通道标准差来进行归一化
normalized_image = image / channel_stds
return (normalized_image)

def clutster(image):
kmeans = KMeans(n_clusters=2, random_state=0, n_init=&quot;auto&quot;).fit(X)

# 从模型中获取图像嵌入

with torch.no_grad():
emb1 = model.encode_image(image)
emb2 = model.encode_image(image2)

print(&quot;嵌入的数据类型为：&quot;, type(emb1), &quot;And &quot;, type(emb2))
# print(emb1, &quot;\n &quot;, emb2)

# 计算嵌入之间的余弦相似度
similarity = calculate_cosine_similarity(emb1, emb2)
distance = calculate_distance(emb1, emb2)

print(f&quot;余弦相似度：{similarity.item()} {type(similarity)}&quot;)
print(f&quot;欧几里得距离：{distance.item()} {type(similarity)}&quot;)


cluster.py
# 使用聚类算法 [k means] 将张量聚类在一起：
import os
import torch
import clip
from PIL import Image
import numpy as np
from sklearn.cluster import KMeans

device =“CPU”
model, preprocess = clip.load(&quot;ViT-L/14&quot;, device, jit=False)
# labels = [&quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;]
name_list = []
np_array = []

# 循环遍历图像文件夹
for photos in os.listdir(&quot;assets/photos&quot;):
image_path = os.path.join(&quot;assets/photos&quot;, photos)
# 预处理图像并获取其张量
image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
name_list.append(photos)
# 将张量的高维展平为类似2dim
numpyImage = np.array(image).flatten()
np_array.append(numpyImage)

k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(np_array)

# 获取 int 类标签（由 KMeans 自动生成）
cluster_labels = kmeans.labels_
image_cluster_map = {}
# 循环遍历图像并为其赋予标签
for i, labels in enumerate(cluster_labels):
# print(f&quot;Image {i} 属于群集 {labels}&quot;)
image_cluster_map[f&#39;image_{name_list[i]}&#39;] = labels

# 查看我们的最终群集
for i, (key, value) in enumerate(image_cluster_map.items()):
print(key, &quot;标记为：“，值）

]]></description>
      <guid>https://stackoverflow.com/questions/79046818/next-steps-i-can-take-to-make-a-good-image-duplicate-and-near-duplicate-finder</guid>
      <pubDate>Wed, 02 Oct 2024 12:49:21 GMT</pubDate>
    </item>
    <item>
      <title>是不是只有我一个人觉得 Python 模块的设置简直就是一场噩梦？[关闭]</title>
      <link>https://stackoverflow.com/questions/79044839/is-it-just-me-or-are-python-modules-a-nightmare-to-setup</link>
      <description><![CDATA[我最近改用 Python 来探索 ML 和 AI。我使用 Linux Mint 和 Jupyter 笔记本来处理 Python。所以我安装了 Tensorflow、TFlearn 和 six.moves，以及感觉像二十或两个其他模块。我尝试运行以下命令，
from __future__ import absolute_import, division, print_function

import os
import pickle
from six.moves import urllib

import tflearn
from tflearn.data_utils import *

我得到
AttributeError：模块“PIL.Image”没有属性“ANTIALIAS”

当我运行它时。所以我试着研究它，你知道在我问问题之前试着做一个好孩子，似乎枕头模块不再受支持或该功能已弃用。我怎样才能让 tflearn 工作？
所以我尝试了不同的方法，我尝试使用 langchain 在本地运行 huggingface。我安装了模块并运行代码，结果出现有关未安装模块的错误。所以我会安装该模块并再次运行代码，结果却出现有关未安装模块的另一个错误。所以我安装了那个模块，重新运行代码，结果却出现另一个错误。最后我让这些行运行起来
from langchain.llms import HuggingFacePipeline
from langchain import PromptTemplate, LLMChain

在让上述代码最终运行后，我尝试了以下操作。这都是我尝试遵循的教程中的内容。
model_id = &quot;lmsys/fastchat-t5-3b-v1.0&quot;
llm = HuggingFacePipeline.from_model_id(
model_id=model_id,
task=&quot;text2text-generation&quot;,
model_kwargs={&quot;temperature&quot;: 0, &quot;max_length&quot;: 1000},
)

我遇到了大量的属性错误和异常。T 尝试了另一种方式来使用本地 LLM 并安装了 Dalai Llama 下载了 13b 羊驼模型，然后通过端口将其打开到浏览器中。然后我问了一些问题，但没有得到任何回应。所以现在我想知道为什么我还要继续使用 Python。尽管 Python 被宣传为“适合初学者”，但它一直非常令人沮丧。我可以仅使用文本编辑器和 Linux 上的 Bash 以及运行 docker 镜像来在 C++ 中运行 SDL2 程序，因此我并不是一个完全的新手，但到目前为止，我使用 python 的编程经验只是调试环境，仅此而已。
似乎我尝试了一切。我只是希望它能工作。如果模块无法使用更新的代码安装它自己的必备组件，那它有什么用呢？
*编辑
所以我想问题的重点是，我是唯一一个在使用 tflearn 设置 python 环境时遇到问题的人吗？如果我必须进入我安装的代码并更改某些内容才能使其真正工作，那么 Tflearn 实际上就坏了。我不得不找到一个疯狂的解决方法来修复我的 c++ 编译器，但一旦完成，就完成了。我在 c++ 之前尝试过 Python，但对模块地狱感到沮丧。我玩了几年 C++，作为一种爱好，但当我想更深入地了解神经网络和机器学习时，我听说了 tensorflow 和 tflearn，结果又回到了模块地狱。也许我应该忘记 tflearn，尝试 torch，但我担心我会再次找到模块。]]></description>
      <guid>https://stackoverflow.com/questions/79044839/is-it-just-me-or-are-python-modules-a-nightmare-to-setup</guid>
      <pubDate>Tue, 01 Oct 2024 21:49:51 GMT</pubDate>
    </item>
    <item>
      <title>创建 PartitionedDatasets 的 Kedro PartitionedDataset</title>
      <link>https://stackoverflow.com/questions/79044783/create-kedro-partitioneddataset-of-partitioneddatasets</link>
      <description><![CDATA[我正在做一个 kedro 项目，我想自动标记数千个音频文件，对它们进行转换，然后将它们存储在一个文件夹中，每个子文件夹对应一个标签。我希望该文件夹成为我的 yml 文件的目录条目
我遵循此 Kedro 教程并创建了我自己的自定义数据集，用于在 kedro 目录中保存/加载 .wav 文件。我还能够在 catalog.yml 中创建 PartitionedDataset  目录条目，例如
audio_folder:
type:partitions.PartitionedDataset
dataset:my_kedro_project.datasets.audio_dataset.SoundDataset
path:data/output/audios/
filename_suffix:&quot;.WAV&quot;

用于在 Kedro 目录中保存/加载 .WAV 文件的文件夹。
我需要的下一个抽象级别是能够创建一个与包含文件夹（例如上面的 audio_folder）相对应的目录条目。我不想通过动态创建目录条目来实现这一点，而是通过扩展 PartitionedDataset 类来实现。这是因为我希望文件夹的文件夹成为我的 catalog.yml 的一部分。我的问题是

这可能吗？你们有人尝试过这样的事情吗？
如果可能的话，我的自定义类应该只包含 _load、_save 和 _describe 方法，就像我在自定义 AbstractDataset 时一样？
]]></description>
      <guid>https://stackoverflow.com/questions/79044783/create-kedro-partitioneddataset-of-partitioneddatasets</guid>
      <pubDate>Tue, 01 Oct 2024 21:22:40 GMT</pubDate>
    </item>
    <item>
      <title>神经网络输入数据中的特征应该是行还是列？</title>
      <link>https://stackoverflow.com/questions/79041608/should-features-be-rows-or-columns-in-input-data-for-neural-networks</link>
      <description><![CDATA[我已经实现了一个神经网络，并且对处理输入矩阵的数据形状的正确方法有疑问。具体来说，我想知道输入数据 X 是否应该在行上有示例而在列上有特征，或者反过来（行上有特征而在列上有示例）。
目前，我已经实现了它以采用形状为 (num_features, num_examples) 的 X。但是，我发现相互矛盾的来源表明，许多库和框架中的常态恰恰相反。例如，当我加载 MNIST 数据集时，它会以 (num_examples, num_features) 的形式出现，这要求我在将数据作为输入提供给神经网络之前对其进行转置
鉴于此，如果常见的做法确实是使用 (num_examples, num_features)：

我是否应该接受原始形状的 X 并在内部对其进行转置？
我是否应该更改实现以直接使用其他维度的 X？
我是否应该简单地在网络文档中记录预期形状？
]]></description>
      <guid>https://stackoverflow.com/questions/79041608/should-features-be-rows-or-columns-in-input-data-for-neural-networks</guid>
      <pubDate>Tue, 01 Oct 2024 04:07:49 GMT</pubDate>
    </item>
    <item>
      <title>XFormersMetadata.__init__() 收到意外的关键字参数“is_prompt”</title>
      <link>https://stackoverflow.com/questions/79036452/xformersmetadata-init-got-an-unexpected-keyword-argument-is-prompt</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79036452/xformersmetadata-init-got-an-unexpected-keyword-argument-is-prompt</guid>
      <pubDate>Sun, 29 Sep 2024 13:03:20 GMT</pubDate>
    </item>
    <item>
      <title>给定的梯度下降代码是按顺序还是同时更新参数？</title>
      <link>https://stackoverflow.com/questions/78582076/is-the-given-code-for-gradient-descent-updating-the-parameters-sequentially-or-s</link>
      <description><![CDATA[我是机器学习的新手，一直在学习梯度下降算法。我相信此代码使用同时更新，即使它看起来像顺序更新。由于偏导数的值是在更新 w 或 b 之前计算的，即从原始 w 和 b 开始，因此应用于单个 w、b 的算法是从原始值开始应用的。我错了吗？

dj_dw=((w*x[i]+b-y[i])*x[i])/m
dj_db=(w*x[i]+b-y[i])/m
w=w-a*dj_dw
b=b-a*dj_db

语言是 python3。
x 和 y 是训练集。
w 和 b 是应用算法的参数。
我正在使用梯度下降算法进行线性回归。
dj_dw 是均方误差成本函数对 w 的偏微分。dj_db 也是如此。
如有错误，敬请原谅，我是新手。
我尝试使用 gemini 和 chatgpt 进行交叉检查，他们说这是连续的，因此造成混淆]]></description>
      <guid>https://stackoverflow.com/questions/78582076/is-the-given-code-for-gradient-descent-updating-the-parameters-sequentially-or-s</guid>
      <pubDate>Wed, 05 Jun 2024 15:28:51 GMT</pubDate>
    </item>
    <item>
      <title>如何增强自定义视频录制 React.js 网站上的虚拟背景质量和背景分割</title>
      <link>https://stackoverflow.com/questions/78308293/how-to-enhance-virtual-background-quality-and-background-segmentation-on-a-custo</link>
      <description><![CDATA[我们目前正在开发一个视频录制平台。为了方便视频拍摄，我们已将 MediaRecorder API 集成到我们的系统中。
作为我们平台功能集的一部分，用户可以使用虚拟背景和背景模糊等功能增强他们的视频。
这些功能依靠 Google MediaPipe 的 API 进行分割，使我们能够准确地隔离框架内的主体。
然而，在将我们的输出与 Google Meet 和 Microsoft Teams 等成熟平台进行比较后，我们发现质量存在明显差距。尽管我们努力实施先进的分割技术，但结果仍未达到行业标准，特别是在视觉保真度和准确性方面。
我们探索了各种途径并尝试了不同的技术。
其中包括TensorFlow（一种开源机器学习框架）、ML Kit（由 Google 提供的机器学习 SDK）和Video-SDK（一种全面的视频处理工具包）。尽管我们付出了这些努力，但我们的输出质量和一致性仍未达到预期水平。
如果您有任何关于增强虚拟背景的建议或关于用于虚拟背景集成的替代开源或付费工具的建议，我们将不胜感激？]]></description>
      <guid>https://stackoverflow.com/questions/78308293/how-to-enhance-virtual-background-quality-and-background-segmentation-on-a-custo</guid>
      <pubDate>Thu, 11 Apr 2024 05:10:10 GMT</pubDate>
    </item>
    <item>
      <title>虚拟背景渲染不正确：寻求有关 WebRTC 实施的指导</title>
      <link>https://stackoverflow.com/questions/78087416/virtual-backgrounds-not-rendering-properly-seeking-guidance-on-webrtc-implement</link>
      <description><![CDATA[我正在使用 RobustVideoMatting 库 (GitHub 存储库) 将虚拟背景支持集成到视频通话应用程序中。前景分割成功，但在 WebRTC 流式传输期间，我面临着背景渲染的挑战，我无法将背景图像应用于分割的视频。
尝试的方法：
方法 1 - CSS 样式：
const bg = `url(&#39;${this.bg}&#39;) center center / cover`;
this.canvas.style.background = bg;

方法 2 - HTMLImageElement：
const backgroundImage = new Image();
backgroundImage.src = &quot;https://d..........................jpg&quot;;
backgroundImage.onload = () =&gt; {
canvasRef.current.getContext(&quot;2d&quot;).drawImage(backgroundImage, 0, 0, 640, 480);
}

请求协助：
我正在寻求解决此问题的指导。如果有人有虚拟背景和 WebRTC 方面的经验，或者在使用 RobustVideoMatting 库时有具体注意事项，我们将不胜感激。
GitHub 存储库： PeterL1n/RobustVideoMatting
CodePen 链接 CodePen 参考
需要将背景图像或视频应用于分段视频帧。]]></description>
      <guid>https://stackoverflow.com/questions/78087416/virtual-backgrounds-not-rendering-properly-seeking-guidance-on-webrtc-implement</guid>
      <pubDate>Fri, 01 Mar 2024 11:28:35 GMT</pubDate>
    </item>
    </channel>
</rss>