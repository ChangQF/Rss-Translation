<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 20 Jun 2024 12:27:57 GMT</lastBuildDate>
    <item>
      <title>时间序列预测，其中历史值也会由于滞后而更新</title>
      <link>https://stackoverflow.com/questions/78646921/time-series-forecasting-where-historical-values-also-gets-updated-due-to-lag</link>
      <description><![CDATA[我正在对未来 4 周的 covid 病例进行时间序列预测。
传入数据频率：每周
要预测的周数：4

数据的主要问题是，数据滞后约 8 周，这意味着特定周的值将在接下来的 8 周内更新 8 周，同时添加下周的值数据。

特定周（Epiweek 1）的值如下所示：

Load_Date 值
1-Jan-24 为 10 ，
8-Jan-24 为 11 ，
15-Jan-24 为 14 ，
22-Jan-24 为 15 ，
29-Jan-24 为 16 ，
6-Feb-24 为 18 ，
13-Feb-24 为 23 ，
20-Feb-24 为 26 ，
27-Feb-24 为 26 ，
6-Mar-24 为 26 ，

在这种情况下：
未修订值为 10
修订值为 26

code
如您在以上数据中看到的那样 - 数据在第 9 周稳定下来。同样，我们也有其他一周的数值。
当我应用 Auto_Arima、Garch 模型时，当我将我的预测与未修订的数据进行比较时，我可以看到良好的结果，但是当我将它们与修订值（在建模时不可用）进行比较时，我看到更多的 MAPE。
考虑到历史值也会在 7-8 周内更新，您能否建议我如何改进结果。
提前致谢！
]]></description>
      <guid>https://stackoverflow.com/questions/78646921/time-series-forecasting-where-historical-values-also-gets-updated-due-to-lag</guid>
      <pubDate>Thu, 20 Jun 2024 10:31:39 GMT</pubDate>
    </item>
    <item>
      <title>Mobilenet 与 resnet</title>
      <link>https://stackoverflow.com/questions/78646834/mobilenet-vs-resnet</link>
      <description><![CDATA[Q1-为什么我们不像在 mobile-net v2 中那样在 resnet50 中添加 skip connection 后移除 relu 以获得更好的性能？
Q2-为什么我们没有在 skip connection 中使用卷积层来匹配 mobile-net v2 中层尺寸变化时的维度，就像我们在 resnet 中那样，当层尺寸变化时匹配输出通道？
我尝试在 web 和 chatgpt 上搜索，但答案并不令人满意。它们都像“架构是以那种方式提出的”。]]></description>
      <guid>https://stackoverflow.com/questions/78646834/mobilenet-vs-resnet</guid>
      <pubDate>Thu, 20 Jun 2024 10:13:24 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 - 年龄和性别检测 [关闭]</title>
      <link>https://stackoverflow.com/questions/78646157/machine-learning-age-and-gender-detection</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78646157/machine-learning-age-and-gender-detection</guid>
      <pubDate>Thu, 20 Jun 2024 07:54:19 GMT</pubDate>
    </item>
    <item>
      <title>所有时期的损失和准确率相同</title>
      <link>https://stackoverflow.com/questions/78645720/same-loss-and-accuracy-for-all-epochs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78645720/same-loss-and-accuracy-for-all-epochs</guid>
      <pubDate>Thu, 20 Jun 2024 06:01:17 GMT</pubDate>
    </item>
    <item>
      <title>CUDAError：使用 Gymnasium 的 RL 环境内存不足</title>
      <link>https://stackoverflow.com/questions/78645476/cudaerror-not-enough-memory-for-an-rl-environment-using-gymnasium</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78645476/cudaerror-not-enough-memory-for-an-rl-environment-using-gymnasium</guid>
      <pubDate>Thu, 20 Jun 2024 04:14:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 DARTS 对多个时间序列进行全局模型训练导致 NaN 损失</title>
      <link>https://stackoverflow.com/questions/78645163/training-global-model-on-multiple-time-series-with-darts-results-in-nan-loss</link>
      <description><![CDATA[我尝试使用 DARTS 库在多个时间序列上训练全局模型，但我遇到了训练和验证损失的 NaN 值。在单个时间序列上进行训练时，它工作正常。我总共有 6 个时间序列。
我怀疑一个可能的问题是每个时间序列的开始和结束时间戳都不同。这会导致多个时间戳，其中只有时间序列子集的数据可用。例如，在某些时间戳，6 个时间序列中可能只有 2 个有数据可用。
这是我的代码：
from darts import TimeSeries, Scaler
from darts.models import NBEATSModel

train_series = []
val_series = []
scalers = []

for idx in train_df[&#39;region_city_item_encoded&#39;].unique():
sub_train_df = train_df[train_df[&#39;region_city_item_encoded&#39;] == idx].sort_values(&#39;timestamp&#39;).reset_index(drop=True)
sub_val_df = val_df[val_df[&#39;region_city_item_encoded&#39;] == idx].sort_values(&#39;timestamp&#39;).reset_index(drop=True)

cur_time_train_series = TimeSeries.from_dataframe(sub_train_df, time_col=&#39;timestamp&#39;, value_cols=&#39;demand&#39;, fill_missing_dates=True)
train_static_covariates = sub_train_df[[&#39;region_city_item_encoded&#39;]].drop_duplicates().reset_index(drop=True)
cur_time_train_series = cur_time_train_series.with_static_covariates(train_static_covariates)

cur_val_series = TimeSeries.from_dataframe(sub_val_df, time_col=&#39;timestamp&#39;, value_cols=&#39;demand&#39;, fill_missing_dates=True)
val_static_covariates = sub_val_df[[&#39;region_city_item_encoded&#39;]].drop_duplicates().reset_index(drop=True)
cur_val_series = cur_val_series.with_static_covariates(val_static_covariates)

scaler = Scaler()
cur_time_train_series = scaler.fit_transform(cur_time_train_series)
cur_val_series = scaler.transform(cur_val_series)

train_series.append(cur_time_train_series)
val_series.append(cur_val_series)
scalers.append(scaler)

model = NBEATSModel(
input_chunk_length=24,
output_chunk_length=12,
n_epochs=100,
random_state=0
)
model.fit(series=train_series, val_series=val_series)

我尝试了以下方法：

确保使用以下方法填充缺失的日期fill_missing_dates=True。
对每个时间序列分别应用缩放。

训练了 DART 上可用的所有模型

通过添加静态协变量和不使用静态协变量进行训练

]]></description>
      <guid>https://stackoverflow.com/questions/78645163/training-global-model-on-multiple-time-series-with-darts-results-in-nan-loss</guid>
      <pubDate>Thu, 20 Jun 2024 01:28:38 GMT</pubDate>
    </item>
    <item>
      <title>如何评估鸢尾花数据集分类问题的不确定性？</title>
      <link>https://stackoverflow.com/questions/78645152/how-to-estimate-the-uncertainty-of-the-classification-problem-for-the-iris-datas</link>
      <description><![CDATA[对于 dropout 中神经网络的不确定性估计方法，作为深度学习中表示模型不确定性的贝叶斯近似，我想将其应用于鸢尾花数据集。计算回归任务上 T 正向传播的标准差足以作为模型预测的不确定性，但我应该如何获得鸢尾花数据集上模型预测的不确定性？
下面是我的回归任务的代码和结果：
import torch
import numpy as np
import matplotlib.pyplot as plt

# 如果可用，则将设备设置为 GPU，否则设置为 CPU
device = torch.device(&quot;cuda:0&quot;)

# 定义一个带 dropout 的简单神经网络模型
class SimpleModel(torch.nn.Module):
def __init__(self, dropout_rate, decay):
super(SimpleModel, self).__init__()
self.dropout_rate = dropout_rate
self.decay = decay
self.f = torch.nn.Sequential(
torch.nn.Linear(1, 20),
torch.nn.ReLU(),
torch.nn.Dropout(p=self.dropout_rate),
torch.nn.Linear(20, 20),
torch.nn.ReLU(),
torch.nn.Dropout(p=self.dropout_rate),
torch.nn.Linear(20, 1)
)

def forward(self, X): 
return self.f(X)

# 用于估计模型预测中的不确定性的函数
defunctity_estimate(x, model, num_samples, l2):
# 从模型中获取多个预测以估计不确定性
outputs = np.hstack([model(x).cpu().detach().numpy() for i in range(num_samples)]) 
y_mean = output.mean(axis=1)
y_variance = output.var(axis=1)
tau = l2 * (1. - model.dropout_rate) / (2. * N * model.decay)
y_variance += (1. / tau)
y_std = np.sqrt(y_variance)
return y_mean, y_std

# 生成合成数据
N = 200 
min_value = -10
max_value = 10

x_obs = np.linspace(min_value, max_value, N)
noise = np.random.normal(loc = 10, scale = 80, size = N)
y_obs = x_obs**3 + noise

x_test = np.linspace(min_value - 10, max_value + 10, N)
y_test = x_test**3 + noise

# 标准化数据
x_mean, x_std = x_obs.mean(), x_obs.std()
y_mean, y_std = y_obs.mean(), y_obs.std()
x_obs = (x_obs - x_mean) / x_std
y_obs = (y_obs - y_mean) / y_std
x_test = (x_test - x_mean) / x_std
y_test = (y_test - y_mean) / y_std

# 实例化模型、损失函数和优化器
model = SimpleModel(dropout_rate=0.5, decay=1e-6).to(device)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=model.decay)

# 训练模型
for iter in range(20000):
y_pred = model(torch.Tensor(x_obs).view(-1,1).to(device))
y_true = torch.Tensor(y_obs).view(-1,1).to(device)
optimizer.zero_grad()
loss = criterion(y_pred, y_true)
loss.backward()
optimizer.step()

if iter % 2000 == 0:
print(&quot;Iter: {}, Loss: {:.4f}&quot;.format(iter, loss.item()))

# 估计模型预测中的不确定性
iters_uncertainty = 200
lengthscale = 0.01
n_std = 2 # 要绘制的标准差数
y_mean, y_std = Certainty_estimate(torch.Tensor(x_test).view(-1,1).to(device), model, iters_uncertainty, lengthscale)

# 绘制结果
plt.figure(figsize=(12,6))
plt.plot(x_obs, y_obs，ls=“none”，marker=“o”，color=“0.1”，alpha=0.8，label=“observed”）
plt.plot（x_test，y_mean，ls=“-”，color=“b”，label=“mean”）
plt.plot（x_test，y_test，ls=&#39;--&#39;，color=&#39;r&#39;，label=&#39;true&#39;）
for i in range(n_std):
plt.fill_between（ 
x_test，
y_mean - y_std * ((i+1.)),
y_mean + y_std * ((i+1.)),
color=“b”，
alpha=0.1
)
plt.legend()
plt.grid()
plt.show()



对于鸢尾花分类问题，我认为就是计算T前向传播的softmax输出的均值和标准差，在均值中找出输出最大值的索引作为预测，在标准差中找出该索引对应的标准差作为不确定性，不知道对不对。]]></description>
      <guid>https://stackoverflow.com/questions/78645152/how-to-estimate-the-uncertainty-of-the-classification-problem-for-the-iris-datas</guid>
      <pubDate>Thu, 20 Jun 2024 01:24:05 GMT</pubDate>
    </item>
    <item>
      <title>对训练数据集使用决策树模型后仅生成一个节点</title>
      <link>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</link>
      <description><![CDATA[1我正在尝试构建一个决策树模型，该模型基于预测变量预测结果变量（名为：结果）。实际上，我已经对一些&quot;&gt;2 级&quot;变量应用了独热编码，以便稍微扩展预测变量的 n [我的数据]。
我首先探索了数据，然后将其拆分为 80/20 拆分并运行模型，但在训练数据集上运行的模型最终只有一个节点，没有分支。查看类似的帖子，我发现我的数据不平衡，因为通过检查类分配的 prop.table（结果变量），大多数是负面的，而不是正面的。关于在此数据上创建正确树的任何建议
这是我的代码：
将数据拆分为测试和训练数据（80％训练和20％测试数据）
set.seed(1234)
pd &lt;- sample(2, nrow(data_hum_mod), replace = TRUE, prob = c(0.8,0.2))
data_hum_train &lt;- data_hum_mod[pd==1,]
data_hum_test&lt;- data_hum_mod[pd==2,]

拆分后的数据探索
检查数据维度
dim(data_hum_train); dim(data_hum_test)
#确保分离后的数据中每个结果类别的 n 值是平衡的（即阳性/阴性 toxo）
prop.table(table(data_hum_train$Results)) * 100
prop.table(table(data_hum_test$Results)) *100

检查缺失值
anyNA(data_hum_mod)
#确保所有变量的方差均不为零或接近零。
nzv(data_hum_mod)
构建模型（使用 party 包）
install.packages(&#39;party&#39;)
library(party)

data_human_train_tree&lt;- ctree(Results ~., data = data_hum_train,
controls = ctree_control(mincriterion = 0.1))
data_human_train_tree
plot(data_human_train_tree)

使用此代码，我获得了此图
使用其他包（如 C50 和 rpart）也得到了相同的结果
您能对此提出建议吗？我读到了关于多数类的子采样（这里是负面结果），如何在 R 中实现这一点？
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78645119/only-one-node-generated-after-using-decision-tree-model-on-training-data-set</guid>
      <pubDate>Thu, 20 Jun 2024 01:04:59 GMT</pubDate>
    </item>
    <item>
      <title>Excel 中的逻辑回归</title>
      <link>https://stackoverflow.com/questions/78644668/logistic-regression-in-excel</link>
      <description><![CDATA[我有两个优化模型：
LR-P1：

LR-P2：

我期望两个模型都得到相同的最优值，但我无法计算模型 LR-P1。我正在进行所有计算，但 excel 求解器无法找到最优值。当我将所有系数设为 0.1 时，求解器只会说找到了最佳值，但不会更改决策变量。
我的问题是，我正在进行所有计算，但 excel 给出了 NUM 错误，而模型 LR-P2 没有。这是因为目标函数对于 LR-P1 来说太小，以至于 excel 求解器无法对其进行交换，从而导致数值问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78644668/logistic-regression-in-excel</guid>
      <pubDate>Wed, 19 Jun 2024 21:13:58 GMT</pubDate>
    </item>
    <item>
      <title>控制 Azure ML 命令源代码的上传位置</title>
      <link>https://stackoverflow.com/questions/78643575/control-where-source-code-for-azure-ml-command-gets-uploaded</link>
      <description><![CDATA[我正在 Azure 机器学习工作室的笔记本中工作，并使用以下代码块通过 命令函数 实例化作业。
来自 azure.ai.ml 导入命令、输入、输出
来自 azure.ai.ml.entities 导入数据
来自 azure.ai.ml.constants 导入 AssetTypes

subscription_id = &quot;&lt;subscription_id&gt;&quot;
resource_group = &quot;&lt;resource_group&gt;&quot;
working = &quot;&lt;workspace&gt;&quot;
storage_account = &quot;&lt;storage_account&gt;&quot;
输入路径 = &quot;&lt;输入路径&gt;&quot;
输出路径 = &quot;&lt;输出路径&gt;&quot;

input_dict = {
&quot;input_data_object&quot;: 输入(
type=AssetTypes.URI_FILE, 
path=f&quot;azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{storage_account}/paths/{input_path}&quot;
)
}

output_dict = {
&quot;output_folder_object&quot;: 输出(
type=AssetTypes.URI_FOLDER,
path=f&quot;azureml://subscriptions/{subscription_id}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{storage_account}/paths/{output_path}&quot;,
)
}

job = command(
code=&quot;./src&quot;, 
command=&quot;python 01_read_write_data.py -v --input_data=${{inputs.input_data_object}} --output_folder=${{outputs.output_folder_object}}&quot;,
inputs=input_dict,
outputs=output_dict,
environment=&quot;&lt;asset_env&gt;&quot;,
compute=&quot;&lt;compute_cluster&gt;&quot;,
)

returned_job = ml_client.create_or_update(job)

此操作成功运行，但每次运行时，如果存储在 ./src 目录中的代码发生变化，则会将新副本上传到默认的 blob 存储帐户。我不介意这一点，但每次运行时，代码都会上传到我的 blob 存储帐户根目录下的新容器中。因此，我的默认存储帐户会因容器而变得杂乱无章。我已阅读使用 command() 函数实例化 command 对象的文档，但我没有看到可用于控制 ./src 代码上传位置的参数。有什么方法可以控制吗？]]></description>
      <guid>https://stackoverflow.com/questions/78643575/control-where-source-code-for-azure-ml-command-gets-uploaded</guid>
      <pubDate>Wed, 19 Jun 2024 16:06:28 GMT</pubDate>
    </item>
    <item>
      <title>理解 Transformers 的结果，通过梯度下降进行情境学习</title>
      <link>https://stackoverflow.com/questions/78639577/understanding-the-results-of-transformers-learn-in-context-with-gradient-descent</link>
      <description><![CDATA[我正在尝试实现这篇论文：
https://arxiv.org/pdf/2212.07677
（这是他们的代码）：
https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd
我正在努力匹配他们的实验结果。具体来说，在他们最简单的 GD 模型（单层、单头、无 softmax）上，他们在测试数据上获得了大约 0.20 的恒定低损失。从概念上讲，我不太明白为什么会这样。
据我所知，这个模型只对数据进行了一次梯度下降迭代，那么为什么它会达到如此低的损失？为什么损失在训练步骤中会保持恒定/接近恒定？我们不是在 GD 模型中训练学习率吗？]]></description>
      <guid>https://stackoverflow.com/questions/78639577/understanding-the-results-of-transformers-learn-in-context-with-gradient-descent</guid>
      <pubDate>Tue, 18 Jun 2024 20:43:45 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 tfa.losses.TripletSemiHardLoss 训练具有三重损失的暹罗网络？</title>
      <link>https://stackoverflow.com/questions/78635866/how-to-train-a-siamese-network-with-triplet-loss-using-tfa-losses-tripletsemihar</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78635866/how-to-train-a-siamese-network-with-triplet-loss-using-tfa-losses-tripletsemihar</guid>
      <pubDate>Tue, 18 Jun 2024 06:47:18 GMT</pubDate>
    </item>
    <item>
      <title>将图像置于中心并在导出时添加背景</title>
      <link>https://stackoverflow.com/questions/78581619/center-an-image-and-adding-a-background-at-export</link>
      <description><![CDATA[我想自动完成所有这些操作：

选择图像中的对象
在此对象上裁剪我的图像
裁剪为 1:1 的宽高比，在此对象周围留出一点空隙
以 800x800px 的 JPG 格式导出我的图像，我的对象位于图像中心，背景为白色。

我在 win11 64 位上
我做了什么：

安装 Python 并创建环境
安装opencv-python-headless、pillow、numpy、Pytorch以用于 CUDA 11.8
克隆存储库 segment-anything.git 并使用 PIP 安装它
下载sam_vit_b_01ec64.pth

像这样对 py 文件进行编码：
import os
import cv2
import numpy as np
from PIL import Image
from fragment_anything import sam_model_registry, SamAutomaticMaskGenerator

def load_image(image_path):
return cv2.imread(image_path)

def save_image(image, path):
cv2.imwrite(path + &#39;.jpg&#39;, image)

def select_object(image):
sam = sam_model_registry[&quot;vit_b&quot;](checkpoint=&quot;sam_vit_b_01ec64.pth&quot;)
mask_generator = SamAutomaticMaskGenerator(sam)
mask = mask_generator.generate(image)
largest_mask = max(masks, key=lambda x: x[&#39;area&#39;])
返回 largest_mask[&#39;segmentation&#39;]

def crop_to_object(image, mask):
x, y, w, h = cv2.boundingRect(mask.astype(np.uint8))
padding = 5
x = max(0, x - padding)
y = max(0, y - padding)
w = min(image.shape[1] - x, w + 2 * padding)
h = min(image.shape[0] - y, h + 2 * padding)

cropped_image = image[y:y+h, x:x+w]
返回 cropped_image

def resize_to_square(image, size=800):
h, w = image.shape[:2]
scale = size / max(h, w)
new_h, new_w = int(h * scale), int(w * scale)
resized_image = cv2.resize(image, (new_w, new_h), 插值=cv2.INTER_LANCZOS4)

new_image = np.ones((size, size, 3), dtype=np.uint8) * 255

top = (size - new_h) // 2
left = (size - new_w) // 2
bottom = top + new_h
right = left + new_w

new_image[top:top+new_h, left:left+new_w] = resized_image

return new_image

def process_image(image_path, output_path):

image = load_image(image_path)
mask = select_object(image)
cropped_image = crop_to_object(image, mask)
final_image = resize_to_square(cropped_image, 800)
save_image(final_image, output_path + &#39;.jpg&#39;)

def process_folder(input_folder, output_folder):

如果 os.path.exists(output_folder):
os.makedirs(output_folder)

对于 root, _, files in os.walk(input_folder):
对于 filename in files:
如果 filename.lower().endswith((&#39;.png&#39;, &#39;.jpg&#39;, &#39;.jpeg&#39;, &#39;.bmp&#39;, &#39;.tiff&#39;)):
input_path = os.path.join(root, filename)

relative_path = os.path.relpath(input_path, input_folder)
output_path = os.path.join(output_folder,relative_path)

output_dir = os.path.dirname(output_path)
如果 os.path.exists(output_dir):
os.makedirs(output_dir)

尝试:
process_image(input_path, output_path)
print(f&quot;已处理 {input_path}&quot;)
except Exception as e:
print(f&quot;无法处理 {input_path}：{e}&quot;)

if __name__ == &quot;__main__&quot;:
input_folder = &quot;&quot;
output_folder = &quot;&quot;
process_folder(input_folder, output_folder)

发生了什么：
我导入了基本图像，我想要预期结果，并且我获得了结果
我得到了一些不同的基本结果：

基本白色背景 -&gt; 结果
Base-nobg -&gt; 结果

有人能帮我理解我错过了什么吗？
提前谢谢，
Cyril]]></description>
      <guid>https://stackoverflow.com/questions/78581619/center-an-image-and-adding-a-background-at-export</guid>
      <pubDate>Wed, 05 Jun 2024 14:13:43 GMT</pubDate>
    </item>
    <item>
      <title>如果数据只有一个特征，则使用 array.reshape(-1, 1) 重塑数据；如果数据只有一个样本，则使用 array.reshape(1, -1)</title>
      <link>https://stackoverflow.com/questions/58663739/reshape-your-data-either-using-array-reshape-1-1-if-your-data-has-a-single-fe</link>
      <description><![CDATA[当我从我的数据中预测一个样本时，它给出了重塑错误，但我的模型具有相同的行数。这是我的代码：
import pandas as pd
from sklearn.linear_model import LinearRegression
import numpy as np
x = np.array([2.0 , 2.4, 1.5, 3.5, 3.5, 3.5, 3.5, 3.7, 3.7])
y = np.array([196, 221, 136, 255, 244, 230, 232, 255, 267])

lr = LinearRegression()
lr.fit(x,y)

print(lr.predict(2.4))

错误是
如果它包含单个样本。格式（数组））
ValueError：预期为 2D 数组，但得到的是标量数组：
array=2.4。
如果您的数据只有一个特征，则使用 array.reshape(-1, 1) 重塑数据；如果它包含一个样本，则使用 array.reshape(1, -1)。
]]></description>
      <guid>https://stackoverflow.com/questions/58663739/reshape-your-data-either-using-array-reshape-1-1-if-your-data-has-a-single-fe</guid>
      <pubDate>Fri, 01 Nov 2019 17:56:16 GMT</pubDate>
    </item>
    <item>
      <title>Python 中更快的 kNN 分类算法</title>
      <link>https://stackoverflow.com/questions/51688568/faster-knn-classification-algorithm-in-python</link>
      <description><![CDATA[我想从头开始编写自己的 kNN 算法，原因是我需要对特征进行加权。问题是，尽管删除了 for 循环并使用了内置的 numpy 功能，我的程序仍然很慢。
有人能建议一种加快速度的方法吗？我没有使用 np.sqrt 来计算 L2 距离，因为它没有必要，而且实际上会减慢整个过程。
class GlobalWeightedKNN:
&quot;&quot;&quot;
具有特征权重的 k-NN 分类器

返回：k-NN 的预测。
&quot;&quot;&quot;

def __init__(self):
self.X_train = None
self.y_train = None
self.k = None
self.weights = None
self.predictions = list()

def fit(self, X_train, y_train, k, weights): 
self.X_train = X_train
self.y_train = y_train
self.k = k
self.weights = weights

def predict(self, testing_data):
&quot;&quot;&quot;
获取查询案例的 2d 数组。

返回 k-NN 分类器的预测列表
&quot;&quot;&quot;

np.fromiter((self.__helper(qc) for qc in testing_data), float) 
return self.predictions

def __helper(self, qc):
neighbours = np.fromiter((self.__weighted_euclidean(qc, x) for x in self.X_train), float)
neighbours = np.array([neighbours]).T 
indexes = np.array([range(len(self.X_train))]).T
neighbours = np.append(indexes, neighbours, axis=1)

# 按第二列排序 - 距离
neighbours = neighbours[neighbours[:,1].argsort()] 
k_cases = neighbours[ :self.k]
indexes = [x[0] for x in k_cases]

y_answers = [self.y_train[int(x)] for x in indexes]
answer = max(set(y_answers), key=y_answers.count) # 获取最常见的值
self.predictions.append(answer)

def __weighted_euclidean(self, qc, other):
&quot;&quot;&quot;
自定义加权欧几里得距离

返回：浮点数
&quot;&quot;&quot;

return np.sum( ((qc - other)**2) * self.weights )
]]></description>
      <guid>https://stackoverflow.com/questions/51688568/faster-knn-classification-algorithm-in-python</guid>
      <pubDate>Sat, 04 Aug 2018 18:39:24 GMT</pubDate>
    </item>
    </channel>
</rss>