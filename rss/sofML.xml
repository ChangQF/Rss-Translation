<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 03 Nov 2024 09:17:43 GMT</lastBuildDate>
    <item>
      <title>使用机器学习的人脸识别项目</title>
      <link>https://stackoverflow.com/questions/79152234/face-recognition-project-using-ml</link>
      <description><![CDATA[做什么 代码显示错误
此代码使用 OpenCV 和 LBPH 算法实现了人脸识别系统。它由三个主要组件组成。
数据收集（生成数据集）：
初始化 harr Cascade 分类器以检测人脸。
定义辅助函数 face cropped，将帧转换为灰度，检测人脸并裁剪它们。
从网络摄像头捕获视频并连续读取帧。
如果检测到人脸，它会调整大小并将裁剪的人脸转换为灰度，使用唯一 ID 保存它，然后显示图像。
在 200 张图像后或按下 Enter 键时，该过程停止。
训练分类器（训练分类器）：
从指定目录读取图像并准备进行训练。
将图像转换为灰度并提取其 ID。
初始化 LBPH 人脸识别器，使用人脸数据对其进行训练，并将模型保存为 classifier.xml。
实时人脸识别：
从网络摄像头捕获视频帧并使用 haar Cascade 检测人脸。
调用绘制边界函数在检测到的人脸周围绘制矩形，并使用训练有素的分类器预测其身份。
根据置信度显示识别的名称或“未知”，实现实时人脸检测和识别。]]></description>
      <guid>https://stackoverflow.com/questions/79152234/face-recognition-project-using-ml</guid>
      <pubDate>Sun, 03 Nov 2024 07:21:47 GMT</pubDate>
    </item>
    <item>
      <title>LCD 7 段数字无法正确识别（CNN/MNIST）</title>
      <link>https://stackoverflow.com/questions/79151393/lcd-7-segment-digits-not-recognized-correctly-cnn-mnist</link>
      <description><![CDATA[我是计算机视觉领域的初学者，我选择读取我的热系统 LCD 7 段显示器作为学习 CNN 的第一项任务。
我能够正确读取大多数数字，但数字 6 大多数情况下被检测为 5。
有人可以建议我，如果我使用 MNIST 数据集执行该任务的方法是正确的，我应该找到更好的超参数来使其按预期工作吗？
这是我的代码，包含更多上下文：https://github.com/tkdcpl/cnn-lcd-digits
我将不胜感激任何指导 :)]]></description>
      <guid>https://stackoverflow.com/questions/79151393/lcd-7-segment-digits-not-recognized-correctly-cnn-mnist</guid>
      <pubDate>Sat, 02 Nov 2024 18:48:18 GMT</pubDate>
    </item>
    <item>
      <title>ImageDataGenerator 在预处理函数中发送图像数组，而不是文件路径</title>
      <link>https://stackoverflow.com/questions/79151089/imagedatagenerator-sending-array-of-image-instead-of-file-path-in-preprocessing</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79151089/imagedatagenerator-sending-array-of-image-instead-of-file-path-in-preprocessing</guid>
      <pubDate>Sat, 02 Nov 2024 16:29:00 GMT</pubDate>
    </item>
    <item>
      <title>为什么在Apple gpu上训练的模型比在Apple cpu（M2）上训练的性能更差？</title>
      <link>https://stackoverflow.com/questions/79150206/why-does-the-model-trained-on-apple-gpu-performs-worse-than-when-it-is-trained-o</link>
      <description><![CDATA[在苹果CPU和苹果GPU上训练了一个简单的CNN模型，并在测试数据集上评估了两个模型的性能，在苹果CPU上训练的模型准确率为98%，而在GPU上训练的模型准确率为10%。
两个模型是一样的，下面是用于训练和测试模型的代码。使用 PyTorch 创建模型。
CPU
for epoch in trange(3): 
for images, labels in tqdm(train_loader):

optimizer.zero_grad()

x = images 
y = model(x)
loss = criterion(y, labels)

loss.backward()
optimizer.step() #更新权重

GPU
for epoch in trange(3): 
for images, labels in tqdm(train_loader):
images, labels = images.to(device).float(), labels.to(device).float() 
optimizer.zero_grad()

x = images 
y = model_gpu(x).to(device)
loss = criterion(y, labels)

loss.backward()
optimizer.step()

模型测试
GPU
m = model_gpu.to(&quot;cpu&quot;) #将模型传输到 cpu

correct = 0
total = len(mnist_test)

with torch.no_grad():
for images, labels in tqdm(test_loader):
images, labels = images, labels

x = images 
y = m(x)

predictions = torch.argmax(y, dim=1)
correct += torch.sum((predictions == labels).float())

print(&#39;测试准确率：{}&#39;.format(correct/total)) 

也尝试在 GPU 上测试，但结果相同。
CPU
correct = 0
total = len(mnist_test)

使用 torch.no_grad():
for images, labels in tqdm(test_loader):
# 前向传递
x = images 
y = model(x)

predictions = torch.argmax(y, dim=1)
correct += torch.sum((predictions == labels).float())

print(f&#39;测试准确率：&#39;,{correct/total})
]]></description>
      <guid>https://stackoverflow.com/questions/79150206/why-does-the-model-trained-on-apple-gpu-performs-worse-than-when-it-is-trained-o</guid>
      <pubDate>Sat, 02 Nov 2024 07:59:33 GMT</pubDate>
    </item>
    <item>
      <title>尝试构建步骤但遇到一些问题 [重复]</title>
      <link>https://stackoverflow.com/questions/79149862/trying-to-build-the-step-but-meet-some-problems</link>
      <description><![CDATA[我正在尝试构建简单的管道：
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
import torch
from torch.utils.data import DataLoader, TensorDataset

class CGANDataAugmenter(BaseEstimator, TransformerMixin):
def __init__(self, device, opt):
self.generator = Generator(opt).to(device)
self.discriminator = Discriminator(opt).to(device)
self.device = device
self.opt = opt
self.n_samples=opt.n_samples
self.sampler=None

def fit_transform(self, X, y):
feature_name = X.columns
label_name = y.name
self.sampler = train_CGAN(X,y,self.generator,self.discriminator,self.device,self.opt)
generated_data = sample(self.sampler,self.n_samples, feature_name, label_name,self.opt)
original_data=pd.concat([X,y],axis=1)

Combine_data=pd.concat([original_data,generated_data],axis=0)

X_combined = combined_data.drop(columns=[label_name])
Y_combined = combined_data[label_name]

return X_combined, Y_combined

来自 imblearn.pipeline 导入 Pipeline
来自 sklearn.tree 导入 DecisionTreeClassifier

steps = [
(&#39;sampler&#39;, CGANDataAugmenter(device,opt)),
(&#39;model&#39;,DecisionTreeClassifier())
]

我得到了错误：

TypeError：Pipeline 的最后一步应该实现 fit 或为字符串“passthrough”。&#39;CGANDataAugmenter(device=device(type=&#39;cpu&#39;),
opt=Namespace(lr=0.0002, b1=0.5, b2=0.999, num_classes=2, latent_dim=8, n_epochs=100, batch_size=64, n_samples=100, origin_size=1))&#39; (type ) 不存在

哪里出了问题？我该如何修复？]]></description>
      <guid>https://stackoverflow.com/questions/79149862/trying-to-build-the-step-but-meet-some-problems</guid>
      <pubDate>Sat, 02 Nov 2024 02:46:19 GMT</pubDate>
    </item>
    <item>
      <title>实时 resnet 预测</title>
      <link>https://stackoverflow.com/questions/79149427/real-time-resnet-prediction</link>
      <description><![CDATA[我训练了一个 resnet50 模型，使用 0 到 5 的数字手势，并尝试通过笔记本电脑的网络摄像头部署它来预测实时类别。
虽然该模型的准确率为 98%，而且我很确定错误不是因为模型训练不当而发生的，但实时值被困在 5 个类别中的 1 个或 2 个类别中，它们总是预测数字 0 和数字 2。
这是代码：
import torch
import torch.nn as nn
import cv2
import numpy as np
from torchvision import models, transforms
from PIL import Image # 导入 PIL 进行图像转换

# 定义模型架构并加载权重
class ResNet50Modified(nn.Module):
def __init__(self, num_classes=6):
super(ResNet50Modified, self).__init__()
self.model = models.resnet50(pretrained=True) # 使用 pretrained=True 以获得更好的性能
self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)

def forward(self, X):
return self.model(X)

# 加载训练好的模型
model = ResNet50Modified(num_classes=6)
# 为 CPU 加载模型的 state_dict
model.load_state_dict(torch.load(&quot;resnet50_modified1.pth&quot;, map_location=torch.device(&#39;cpu&#39;)))
model.eval()

# 定义转换以匹配训练预处理
preprocess = transforms.Compose([
transforms.Resize((64, 64)), # 调整为模型的输入大小
transforms.ToTensor(), # 转换为张量
transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 根据 ResNet 标准进行标准化
])

# 标志的标签
class_names = [&#39;Class_0&#39;, &#39;Class_1&#39;, &#39;Class_2&#39;, &#39;Class_3&#39;, &#39;Class_4&#39;, &#39;Class_5&#39;] # 替换为实际标志名称

# 打开网络摄像头进行实时预测
cap = cv2.VideoCapture(0)

if not cap.isOpened():
print(&quot;Error: Could not open webcam.&quot;)
exit()

while True:
ret, frame = cap.read()
if not ret:
print(&quot;Error: Could not read frame.&quot;)
break

# 将帧从 BGR（OpenCV）转换为RGB (PIL)
frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

# 将 NumPy 数组转换为 PIL 图像
pil_image = Image.fromarray(frame_rgb)

# 预处理帧
input_image = preprocess(pil_image) # 使用 PIL 图像进行预处理
input_image = input_image.unsqueeze(0) # 添加批次维度

# 使用模型进行预测
with torch.no_grad():
output = model(input_image)

# 应用 softmax 获取概率
probabilities = torch.softmax(outputs, dim=1)

# 获取预测的类别和置信度
_, predict = torch.max(probabilities, 1)
confidence = probabilities[0][predicted].item() * 100 # 转换为百分比
label = class_names[predicted.item()]

# 显示结果和置信度
cv2.putText(frame, f&quot;Predicted: {label}, Confidence: {confidence:.2f}%&quot;, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
cv2.imshow(&quot;Sign Detection&quot;, frame)

# 按“q”退出
if cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
break

cap.release()
cv2.destroyAllWindows()

置信度始终很高，但由于标签错误，即使我在网络摄像头上显示 5 个手指，它仍然停留在零。
我觉得问题出在帧处理上，有人对此有任何见解吗？]]></description>
      <guid>https://stackoverflow.com/questions/79149427/real-time-resnet-prediction</guid>
      <pubDate>Fri, 01 Nov 2024 21:06:43 GMT</pubDate>
    </item>
    <item>
      <title>如何向模型指定连续/分类特征？</title>
      <link>https://stackoverflow.com/questions/79149194/how-do-i-specify-continuous-categorical-features-to-the-model</link>
      <description><![CDATA[我有一个包含 90 个分类特征和 10 个连续特征的数据集。所有列都是整数，我使用 VectorAssembler 作为唯一的预处理步骤 - 这是一个已保存的中间体，不需要 StringIndexer、StandardScaler 等进一步处理。当我尝试拟合 DecisionTreeClassifier 时，我收到以下消息：
IllegalArgumentException：要求失败：DecisionTree 要求 maxBins (= 32) 至少与每个分类特征中的值数一样大，但分类特征 91 有 100 个值。考虑删除此特征和其他具有大量值的分类特征，或添加更多训练示例。

我如何向模型或 VectorAssembler 指定哪些字段是连续的，哪些是分类的？]]></description>
      <guid>https://stackoverflow.com/questions/79149194/how-do-i-specify-continuous-categorical-features-to-the-model</guid>
      <pubDate>Fri, 01 Nov 2024 19:29:51 GMT</pubDate>
    </item>
    <item>
      <title>需要从图像中分别分割出每个数字</title>
      <link>https://stackoverflow.com/questions/79147122/need-to-segment-each-number-from-the-image-separately</link>
      <description><![CDATA[我使用 MNIST 数据集创建了一个 CNN 模型。我想对图像中存在的数字序列进行预测。该技术涉及分割每张图像并将其输入到模型中，但我在从图像中分割数字时遇到了困难，因为存在两种不同类型的图像。我需要一种强大的技术来消除图像中存在的所有噪音和阴影并分别分割每个数字。
我也在这里分享这些图片。
我正在寻找强大的技术和代码。


我尝试了此代码和技术，但它对附加的图像不起作用
def fragment_and_display_digits(image_path):# Read imageimg = cv2.imread(image_path)gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
# 获取图像尺寸
height, width = gray.shape
total_area = height * width

# 应用自适应阈值
thresh = cv2.adaptiveThreshold(
gray,
255,
cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
cv2.THRESH_BINARY_INV,
21, # 块大小
10 # C 常数
)

# 查找轮廓
contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 根据面积过滤轮廓
valid_contours = []
min_area = total_area * 0.001 # 图像面积的最小 0.1%
max_area = total_area * 0.5 # 图像面积的最大 50%

for cnt in contours:
area = cv2.contourArea(cnt)
if min_area &lt; area &lt; max_area:
x, y, w, h = cv2.boundingRect(cnt)
aspects_ratio = w / float(h)
# 检查数字的纵横比是否合理（不要太宽或太高）
if 0.2 &lt; aspects_ratio &lt; 2：
valid_contours.append(cnt)

# 从左到右对轮廓进行排序
valid_contours = sorted(valid_contours, key=lambda x: cv2.boundingRect(x)[0])

# 提取并显示数字
digits = []
padding = int(min(height, width) * 0.02) # 根据图像大小进行自适应填充

for cnt in valid_contours:
x, y, w, h = cv2.boundingRect(cnt)
# 添加填充，同时保持在图像范围内
x1 = max(0, x - padding)
y1 = max(0, y - padding)
x2 = min(width, x + w + padding)
y2 = min(height, y + h + padding)
digit = img[y1:y2, x1:x2]
digits.append(digit)

# 显示结果
if digits:
# 创建带有检测到的数字的原始图像的可视化
img_with_boxes = img.copy()
for cnt in valid_contours:
x, y, w, h = cv2.boundingRect(cnt)
cv2.rectangle(img_with_boxes, (x, y), (x+w, y+h), (0, 255, 0), 2)

# 绘制带有方框和分割数字的原始图像
plt.figure(figsize=(15, 5))

# 带有方框的原始图像
plt.subplot(2, 1, 1)
plt.imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))
plt.title(&#39;Detected Digits&#39;)
plt.axis(&#39;off&#39;)

#分割的数字
plt.subplot(2, 1, 2)
for i, digit in enumerate(digits):
plt.subplot(2, len(digits), len(digits) + i + 1)
plt.imshow(cv2.cvtColor(digit, cv2.COLOR_BGR2RGB))
plt.axis(&#39;off&#39;)
plt.title(f&#39;Digit {i+1}&#39;)

plt.tight_layout()
plt.show()
else:
print(&quot;图像中未找到数字&quot;)
]]></description>
      <guid>https://stackoverflow.com/questions/79147122/need-to-segment-each-number-from-the-image-separately</guid>
      <pubDate>Fri, 01 Nov 2024 06:27:46 GMT</pubDate>
    </item>
    <item>
      <title>如何对数据帧进行分组以获取代表更大集合的全部范围的子集</title>
      <link>https://stackoverflow.com/questions/79145581/how-to-group-dataframes-to-get-a-subset-that-represents-the-full-range-of-the-la</link>
      <description><![CDATA[这是我拥有的一组数据框的两个示例：



天
p1
p2
p3




4
2.1
3.4
4.5


15
2.2
3.6
2.8


39
2.5
2.1
0.4



和这个：



天
p1
p2
p3




4
2.1
3.4
4.5


18
8.2&lt; /td&gt;
2.2
5.8


22
6.4
3.6
1.4


29
2.4
4.1
2.3



我有大约 100 万个这样的数据框（列相同，长度不同），我想输出大约 50000 个子集，以公平地代表所有存在的不同数据框。基本上，数据框应该是一个有效的表示，因此在完整的 100 万或 50k 子集上训练 ML 模型应该会给 ML 模型带来几乎相同的行为。
天数很重要，因为 2 个具有相同参数 (p) 值但天数列差异很大的数据框是不相等的
我的方法是通过每个级别的变量将数据框分组在一起。然后从底层的每个组中取出 1 个数据框。
组级别 1 (GL1)：按行数对数据框进行分组。
组级别 2 (GL2)：对于 GL1 中的每个数据框，使用聚类分析 (DBSCAN 聚类？) 对具有相似天数列的数据框进行分组
组级别 3 (GL3)：对于 GL2 中的每个数据框，使用聚类分析 (DBSCAN 聚类？) 将具有相似参数值的数据框分组在一起
从每个 GL3 组中取出 1 个数据框来表示该组数据框。
它可能无法获得每个参数的完整最大值和最小值，但这种方法似乎相当全面。这是一个好主意还是您有更好的想法？]]></description>
      <guid>https://stackoverflow.com/questions/79145581/how-to-group-dataframes-to-get-a-subset-that-represents-the-full-range-of-the-la</guid>
      <pubDate>Thu, 31 Oct 2024 16:36:13 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用 Candle 从 NV-Embed 获取嵌入？</title>
      <link>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</link>
      <description><![CDATA[我想要做的是一个输出任意输入的嵌入的 CLI 程序。
为此，我想使用嵌入模型进行推理，我选择了 NV-Embed-v2。我选择的框架是 Candle，但我也查看了 Mistral-RS。
基本上，我想做的是这个代码片段：
https://huggingface.co/nvidia/NV-Embed-v2
但使用 Rust 和 Candle。
我尝试从 Mistral Candle 的示例，因为 NV-Embed 的 HF 页面显示：模型详细信息/仅基础解码器 LLM：Mistral-7B-v0.1。
我将原始代码中的模型 ID 替换为 nvidia/NV-Embed-v2，并能够从 Hugging Face 下载权重，但在加载配置时，我得到了这个：
错误：缺少第 101 行第 1 列的字段“vocab_size”

然后我将从 HF 加载的 JSON 配置中的值硬编码到新创建的 candle_transformers::models::mistral::Config 实例中。之后，Mistral::new(&amp;config, vb) 失败，并显示：
错误：找不到张量 model.embed_tokens.weight

有没有办法解决这个问题 — 也许还有其他一些基于 Candle 的开源作品可以作为我的灵感？或者，也许这是一个很容易诊断的常见错误？]]></description>
      <guid>https://stackoverflow.com/questions/79145419/is-it-possible-to-get-embeddings-from-nv-embed-using-candle</guid>
      <pubDate>Thu, 31 Oct 2024 15:55:49 GMT</pubDate>
    </item>
    <item>
      <title>如何在不导入 Keras 的情况下在 Python 桌面应用程序中加载 Keras 模型？</title>
      <link>https://stackoverflow.com/questions/79143377/how-to-load-a-keras-model-without-importing-keras-in-a-python-desktop-app</link>
      <description><![CDATA[我有一个经过训练的 Keras 模型，保存为 model.h5，我通常会使用 keras.models.load_model(&quot;model.h5&quot;) 加载它。我目前正在开发一个带有 GUI 的 Python 桌面应用程序，需要将其打包为独立的 .exe 文件。
问题是导入 keras.models 会显著增加 .exe 文件的大小，超出我的大小限制。我正在寻找一种方法来加载和运行 model.h5 文件，而无需在最终包中包含整个 Keras 库。
有没有办法加载 .h5 模型文件而不导入 keras.models 模块？
或者，是否有一种工具或方法可以在打包应用程序时选择性地仅包含 Keras 的必要部分，而不是捆绑整个库？
环境：
Python 版本：3.11.7
Keras 和 TensorFlow 版本：Keras 2.15.0、TensorFlow 2.15
打包工具：PyInstaller]]></description>
      <guid>https://stackoverflow.com/questions/79143377/how-to-load-a-keras-model-without-importing-keras-in-a-python-desktop-app</guid>
      <pubDate>Thu, 31 Oct 2024 03:07:54 GMT</pubDate>
    </item>
    <item>
      <title>DQN 性能波动</title>
      <link>https://stackoverflow.com/questions/79141566/dqn-performance-swinging</link>
      <description><![CDATA[我使用 DDQN 和经验重放，就像本教程中一样 https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
除了我通过模糊 x_dot 和 theta_t（推车速度和杆的角速度）使问题变得更难一些。然后，我根据当前状态计算前一个 x_dot、theta_dot、x_dot_dot 和 theta_dot_dot，并使用此状态空间进行学习过程：(x, prev_x_dot, prev_prev_x_dot_dot, theta, prev_theta_dot, prev_prev_theta_dot_dot)。
无论如何，我的主要问题是，通过使用上面链接的教程中描述的 DQN 算法，算法不会收敛。如果最后 100 集的平均长度 &gt; 450，我认为学习成功。执行时，我可能会看到 50-60 个连续的 500 集长集，但随后集长随机波动并下降到甚至 20!?!? 我需要从某个范围内（对于每个 x、theta）的任意起始位置开始，使问题更加棘手，但到目前为止，结果并不乐观。
对于像 DQN 这样的算法来说，这是一种正常行为吗？我知道，作为基于先前执行计算的策略，损失函数可能存在一些收敛问题，但这是否包括性能的严重波动？
我使用的网络有 3 个非线性层，线性层尺寸为 256x256。]]></description>
      <guid>https://stackoverflow.com/questions/79141566/dqn-performance-swinging</guid>
      <pubDate>Wed, 30 Oct 2024 14:36:28 GMT</pubDate>
    </item>
    <item>
      <title>验证数据的输入形状无效</title>
      <link>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</link>
      <description><![CDATA[我正在使用 Tensorflow 在 Python 中开发一个简单的 ML 模型。代码如下：
import tensorflow as tf
import pandas as pd

# 加载 CSV 数据
def load_data(filename):
data = pd.read_csv(filename)
X = data[[&#39;X0&#39;,&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;]]
Y = data[[&#39;Y0&#39;,&#39;Y1&#39;]]
return tf.data.Dataset.from_tensor_slices((X.values, Y.values))

training_data = load_data(&quot;binarydatatraining.csv&quot;)
print(training_data)

# 构建一个简单的神经网络模型
model = tf.keras.models.Sequential([
tf.keras.layers.Dense(4,activation=&#39;relu&#39;),
tf.keras.layers.Dense(2)
])
# 编译模型
model.compile(optimizer=&#39;adam&#39;,
loss=&#39;mean_squared_error&#39;)

# 加载验证数据
validation_data = load_data(&quot;binarydatavalidation.csv&quot;)
print(validation_data)

# 训练模型
model.summary()
model.fit(training_data.batch(9), epochs=5)
model.summary()
model.fit(training_data.batch(9), epochs=1, validation_data = validation_data, validation_steps = 2)

一切都运行正常，直到我开始包含验证数据，该数据具有与训练数据相同数量的参数。然后我收到错误
ValueError：调用 Sequential.call() 时遇到异常。

[1m输入 Tensor(&quot;sequence_1/Cast:0&quot;, shape=(4,), dtype=float32) 的输入形状无效。预期形状 (None, 4)，但输入具有不兼容的形状 (4,)[0m

Sequential.call() 收到的参数：
• 输入=tf.Tensor(shape=(4,), dtype=int64)
• 训练=False
• 掩码=None

打印验证和训练数据集显示它们具有相同的维度，并且运行 print(training_data) 和 print(validation_data) 都给出
&lt;_TensorSliceDataset element_spec=(TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(2,), dtype=tf.int64, name=None))&gt;

如何正确设置验证数据以与 model.fit 内联运行？]]></description>
      <guid>https://stackoverflow.com/questions/79139121/invalid-input-shape-for-validation-data</guid>
      <pubDate>Tue, 29 Oct 2024 21:59:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在 ML.NET 中使用 CenterFace？模型预期形状为 (10, 3, 32, 32)</title>
      <link>https://stackoverflow.com/questions/79122749/how-to-use-centerface-in-ml-net-model-expects-shape-10-3-32-32</link>
      <description><![CDATA[我尝试在 ML.NET 中使用 CenterFace ONNX，但一直出现各种错误，主要是关于输入大小的错误。
CenterFace 元数据指出，它应该有一个 10, 3, 32, 32 的输入，这对于图像检测来说已经毫无意义了 - 为算法提供 10 个批次（每个批次 32x32 像素）有什么意义？
这是我的主要代码：
 string modelPath = &quot;centerface.onnx&quot;;
var mlContext = new MLContext();

string imagePath = &quot;photo1.jpg&quot;;

var img = Image.FromFile(imagePath);
var DH = (int)(Math.Ceiling((float)img.Height / 32) * 32);
var DW = (int)(Math.Ceiling((float)img.Width / 32) * 32);

var inputData = new[] { new ModelInput { ImagePath = imagePath } };
IDataView imageData = mlContext.Data.LoadFromEnumerable(inputData);

var pipeline = mlContext.Transforms.LoadImages(outputColumnName: &quot;input.1&quot;, imageFolder: &quot;&quot;, inputColumnName: nameof(ModelInput.ImagePath))
.Append(mlContext.Transforms.ResizeImages(outputColumnName: &quot;input.1&quot;, imageWidth: DW, imageHeight: DH))
.Append(mlContext.Transforms.ExtractPixels(outputColumnName: &quot;input.1&quot;))
.Append(mlContext.Transforms.ApplyOnnxModel(
outputColumnNames: [&quot;537&quot;, &quot;538&quot;, &quot;539&quot;, &quot;540&quot;],
inputColumnNames: [&quot;input.1&quot;],
modelFile: modelPath
));

var model = pipeline.Fit(imageData);
var predictionEngine = mlContext.Model.CreatePredictionEngine&lt;ModelInput, ModelOutput&gt;(mo​​del);
var prediction = predictionEngine.Predict(new ModelInput { ImagePath = imagePath });

使用我的 2 个模型类：
 public class ModelInput
{
public string ImagePath { get; set; }
}

public class ModelOutput
{
[ColumnName(&quot;537&quot;)] 
public float[] HeatMap { get; set; }

[ColumnName(&quot;538&quot;)]
public float[] Scale { get; set; }

[ColumnName(&quot;539&quot;)]
public float[] Offset { get; set; }

[ColumnName(&quot;540&quot;)]
public float[] Landmarks { get; set; }
}

但我确实一直收到有关输入大小的错误：

System.ArgumentException：“内存长度（3686400）必须与尺寸乘积（30720）匹配。”

30720 显然是 10x3x32x32。但同样，这有什么意义呢？
我认为我的 ONNX 坏了，但我确实有一个使用 OpenCVSharp 的工作实现：
// 这是计算机 DW 和 DH，与 ML.NET 示例中的方式相同
CenterFaceParams p = new(image, resizedSize.Width, resizedSize.Height, scoreThreshold, nmsThreshold);
Size size = new(p.DW, p.DH);

使用 Mat input = new();
Cv2.Resize(image, input, size);

使用 Mat blobInput = CvDnn.BlobFromImage(input, 1.0, size, new Scalar(0, 0, 0), true, false);
_net.SetInput(blobInput, &quot;input.1&quot;);

使用 (Mat heatMap = new())
使用 (Mat scale = new())
使用 (Mat offset = new())
使用 (Mat skylines = new())
{
_net.Forward([heatMap, scale, offset, skylines], [&quot;537&quot;, &quot;538&quot;, &quot;539&quot;, &quot;540&quot;]);

CenterFaceDecodercoder = new(heatMap, scale, offset, skylines, p);
returncoder.GetOutput();
}

这个实现给了我所有 4 个层，建模后我得到了我想要的值。]]></description>
      <guid>https://stackoverflow.com/questions/79122749/how-to-use-centerface-in-ml-net-model-expects-shape-10-3-32-32</guid>
      <pubDate>Thu, 24 Oct 2024 15:52:45 GMT</pubDate>
    </item>
    <item>
      <title>更改 Keras train_on_batch() 的详细程度？</title>
      <link>https://stackoverflow.com/questions/75401761/change-verbosity-of-keras-train-on-batch</link>
      <description><![CDATA[我正在使用 Keras 的 train_on_batch() 命令训练 GAN。这与 Keras 的 fit() 非常相似。但是，在 fit() 的文档中，有一个 verbose 参数，它会更改将进度条打印到控制台的频率。
我的模型有很多批次，因此它会将大量进度条打印到命令行。不幸的是，train_on_batch() 没有 verbose 参数。有没有解决方法？是否有我可以设置的 Keras 全局变量/环境变量？我不想禁止我的程序打印到控制台，我只想更改特定的 train_on_batch() 的详细程度。
需要澄清的是，我直接从 Keras 包中使用 Keras，而不是使用 tf.keras。]]></description>
      <guid>https://stackoverflow.com/questions/75401761/change-verbosity-of-keras-train-on-batch</guid>
      <pubDate>Thu, 09 Feb 2023 16:41:49 GMT</pubDate>
    </item>
    </channel>
</rss>