<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 01 Feb 2024 06:17:49 GMT</lastBuildDate>
    <item>
      <title>对交叉熵损失真的很困惑</title>
      <link>https://stackoverflow.com/questions/77917816/really-confused-about-cross-entropy-loss</link>
      <description><![CDATA[我遇到了我的项目面临的关于交叉熵损失的三个详细实现。顺便说一下，needle 是我要构建的东西，所以你可以把它看作像 pytorch 一样的东西。
那么谁能解释一下为什么？
第一个版本：
def softmax_loss(Z, y_one_hot):
    ”“”返回softmax损失。请注意，出于本次作业的目的，
    你不需要担心“很好”缩放数值属性
    log-sum-exp 计算的一部分，但可以直接计算它。

    参数：
        Z (ndl.Tensor[np.float32]): 形状的 2D 张量
            (batch_size, num_classes)，包含 logit 预测
            每堂课。
        y (ndl.Tensor[np.int8]): 形状的 2D 张量 (batch_size, num_classes)
            每个示例的真实标签索引处包含 1，并且
            其他地方为零。

    返回：
        样本上的平均 softmax 损失。 (ndl.张量[np.float32])
    ”“”
    ### 开始你的解决方案
    # 假设二元选择，之前 y 表示真实标签，例如
    # [0, 1, 0, 1, 0]
    # 返回 np.mean(np.log(np.sum(np.exp(Z), axis=1)) - Z[np.arange(y.size), y])
    #
    # 然而，这个问题应用了 ndl 并使用 one-hot y 作为张量，例如
    # [[1, 0],
    # [0, 1],
    # [1, 0],
    # [0, 1],
    # [1, 0]]
    # 这种情况下，直接执行矩阵乘法。
    #
    # 顺便说一下，如果这里设置axes=1的话，是不会通过测试的。这是因为轴应该是
    # 这里是一个可迭代对象，如果没有优化， int 绝不是可迭代的，因此我们应用
    # 元组轴=(1,)。
    lhs = ndl.log(ndl.exp(Z).sum(axes=(1,))).sum() # 求和到 (B, 1)，然后是标量
    rhs = (y_one_hot * Z).sum() # EW (B, k) 然后是标量
    
    return (lhs - rhs) / Z.shape[0] # 除以批量大小
    ### 结束你的解决方案

第二个版本：
def softmax_loss(Z, y):
    ”“”返回softmax损失。请注意，出于本次作业的目的，
    你不需要担心“很好”缩放数值属性
    log-sum-exp 计算的一部分，但可以直接计算它。

    参数：
        Z (np.ndarray[np.float32]): 形状的 2D numpy 数组
            (batch_size, num_classes)，包含 logit 预测
            每堂课。
        y (np.ndarray[np.uint8]): 形状为 (batch_size, ) 的一维 numpy 数组
            包含每个示例的真实标签。

    返回：
        样本上的平均 softmax 损失。
    ”“”
    ### 开始你的代码
    # 例如，假设 exp_logits 和 y 是
    # [[0.3, 0.2, 0.5] [2, 1, 1, 1]
    # [0.1, 0.6, 0.3]
    # [0.4, 0.3, 0.3]
    # [0.1, 0.7, 0.2]]
    # 高级索引从行[0, 1, 2, 3]和列[2, 1, 1, 1]获取元素，
    # 表示真实分类的预测点。
    lhs = np.log(np.sum(np.exp(Z), axis=1)) # 求和为形状 (B, 1)
    rhs = Z[np.arange(y.size), y] # 形状 (B, 1)
    # 这是预测点相减然后求平均值；
    # 如果先求均值再减，就减一。
    avg_loss = np.mean(左轴 - 右轴)
    
    返回平均损失
    ### 结束你的代码

第三个版本：
&lt;前&gt;&lt;代码&gt;
/* 由于矩阵在 C++ 中表示为数组，因此它将指向每次迭代的开始。 */
const float *X_batch = &amp;X[iter * batch * n];

/* 要计算 exp_logits，首先初始化形状为 (B, k) = (B, n) * (n, k) 的数组。 */
float *exp_logits = new float[batch * k];
mat_mul(X_batch, theta, exp_logits, 批次, n, k); // 实际上在这里记录
/* 在数组的 from 中对 (B, k) exp_logits 进行一一指数运算。 */
for (size_t i = 0; i &lt; 批 * k; i++) exp_logits[i] = exp(exp_logits[i]);

/* 对 axis=1 的矩阵求和，即在 &#39;k&#39; 的维度上。 */
for (size_t i = 0; i &lt; 批次; i++) {
    浮点总和=0；
    for (size_t j = 0; j &lt; k; j++) sum += exp_logits[i * k + j];
    for (size_t j = 0; j &lt; k; j++) exp_logits[i * k + j] /= sum; // grad 实际上在这里
}
/* exp_logits 总是在迭代中更新，但是我们需要预先为 y 添加 iter*batch。
    * 例如，假设 exp_logits 和 y 是
    * [[0.3, 0.2, 0.5] [[2]
    * [0.1, 0.6, 0.3] [1]
    * [0.4, 0.3, 0.3] [1]
    * [0.1, 0.7, 0.2]] [1]]
    * 减去后，损失将如下所示
    * [[0.3, 0.2, -0.5]
    * [0.1,-0.4,0.3]
    * [0.4，-0.7，0.3]
    * [0.1,-0.3,0.2]]
    */
for (size_t i = 0; i &lt; 批次; i++) exp_logits[i * k + y[iter * 批次 + i]] -= 1;

我希望有人能好心地提示我一下。]]></description>
      <guid>https://stackoverflow.com/questions/77917816/really-confused-about-cross-entropy-loss</guid>
      <pubDate>Thu, 01 Feb 2024 05:04:35 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv1是否需要在每个单元格中训练样本？或者它可以概括来自其他细胞的训练样本吗？</title>
      <link>https://stackoverflow.com/questions/77917722/does-yolov1-need-training-examples-in-every-cell-or-can-it-generalize-training</link>
      <description><![CDATA[由于每个特定单元的输出权重不在其他单元之间共享，因此我的理解是 YOLO 网络需要每个单元中每个类的训练示例。这感觉效率很低，尤其是对于拥有很多很多小区的网络。
当每个班级没有太多训练示例时，人们如何解决这个问题？也许通过使用大量翻译图像来增强训练集？
YOLO 论文似乎没有提及数据增强。也许他们的训练集非常完整，并且包含每个单元中每个类别的示例。]]></description>
      <guid>https://stackoverflow.com/questions/77917722/does-yolov1-need-training-examples-in-every-cell-or-can-it-generalize-training</guid>
      <pubDate>Thu, 01 Feb 2024 04:25:01 GMT</pubDate>
    </item>
    <item>
      <title>对于独特的时间序列多类分类问题，我应该选择哪种神经网络模型？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77916586/which-neural-network-model-should-i-choose-for-a-unique-timeseries-multiclass-cl</link>
      <description><![CDATA[我已经研究一个特定的时间序列多类分类问题超过 10 个月了。
遇到了很多问题，也克服了很多问题。但现在我已经被困了快一个月了。由于我已经研究这个问题很长时间了，所以想不出任何新的东西。需要新的视角和想法。
这是我的问题定义！！
我有一组独特的数据集（我有多个数据集）。
每个数据集可以分为两部分：-

静态配置
时间序列数据

示例数据集
这是我拥有的数据集的示例。这只是一个例子，我的数据集有 100 多个配置列和 100 多个时间序列列。
正如您所看到的，静态列在给定的数据集中保持不变（但随数据集而变化）。原始数据集也有很多 0（即很多时间序列列有 0 值，但有些确实有时间序列值）
数据集标有不同的类别，例如正常、故障 1、故障 2 等
数据的标记方式如下：-

使用给定的数据，执行复杂的计算来派生一组新的列。
如果新列集中的数据不断变化，则将其标记为正常
如果新列集中的数据高于某个数字 k(k=100) 并且在 n(n=3) 行中保持不变，则开始将其标记为失败 1，直到其发生变化
如果新列集中的数据低于某个数字 x(x=10) 并且在 n(n=3) 行中保持该值，则开始将其标记为故障 2，直到其发生变化

带有派生列和标签的示例数据集
（这只是一个简单的例子，实际上会发生更复杂的计算）
标记时间序列数据的整个过程目前由复杂的代码执行
我想用神经网络替换这个复杂的代码，它可以做同样的事情，而无需创建所有这些派生列等。我希望模型能够学习这些复杂的计算并自行执行
注意事项：

模型应该适用于所有经过训练的数据集和所有未来未见过的数据集
对所有数据集执行相同的计算
数据稀疏
并非所有配置列和时间序列列都会用于计算
如有任何进一步说明，请务必询问。

我尝试过：-

使用 LSTM 模型
使用截断的 SVD 来减少维度。
用于类别平衡的过采样技术
使用数据清理和预处理技术

假设我有 20 个数据集
我使用 15 个数据集进行模型训练/测试，其余 5 个数据集作为“看不见的未来数据集”进行测试
我将所有 15 个数据集合并为 1 个大数据集。进行 70:30 的训练测试分割，并在组合的 15 个数据集上训练 LSTM 模型。
模型在 70% 的组合数据上进行训练，并在其余 30% 的数据上进行测试。
在实时数据上进行测试时，该模型对于 15 个数据集效果非常好。
然而，在其余 5 个未见过的数据集上进行测试时，它的表现很差。
这告诉我该模型没有学习底层的复杂计算。
我希望模型在我的所有数据集和未见过的数据集上表现良好。我希望它能够学习底层的计算。]]></description>
      <guid>https://stackoverflow.com/questions/77916586/which-neural-network-model-should-i-choose-for-a-unique-timeseries-multiclass-cl</guid>
      <pubDate>Wed, 31 Jan 2024 21:43:50 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow edit_distance 文本预处理</title>
      <link>https://stackoverflow.com/questions/77916416/tensorflow-edit-distance-text-preprocessing</link>
      <description><![CDATA[我正在尝试在 TensorFlow 中构建一个模型，该模型采用两个字符串，通过计算两个字符串之间的编辑距离来预处理字符串，然后使用结果数字作为模型的输入。这个想法是，在部署中，模型将接受这两个字符串作为输入，处理它们，然后进行评估。下面是我想要做的伪代码。
将张量流导入为 tf
从 nltk.metrics.distance 导入 edit_distance

# 我正在尝试做的事情：
string1 = “约翰·J·多伊”
string2 = “约翰·詹姆斯·多伊”

dist = edit_distance(字符串1,字符串2)
打印（分布）
# 6（这将作为输入传递给模型）

我已经尝试过类似的方法，但是得到一个 ValueError: Shape (1, None, 1) must haverank 1 并且我似乎无法得到任何结果。
&lt;代码&gt;
string1 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)
string2 = tf.keras.layers.Input(shape=(1,), dtype=tf.string)

类 EditDistanceLayer(tf.keras.layers.Layer):
    def 调用（自身，输入）：
        字符串 1、字符串 2 = 输入

        假设 = tf.sparse.SparseTensor(indices=[[0, 0]],values=[string1],dense_shape=[1, 1])
        真值 = tf.sparse.SparseTensor(indices=[[0, 0]]，values=[string2]，dense_shape=[1, 1])

        edit_distance = tf.edit_distance（假设，真相，归一化= True）

        返回编辑距离

# 这里发生错误
edit_distance_output = EditDistanceLayer()([string1_input, string2_input])

我尝试构建一个自定义预处理层来计算两个字符串之间的编辑距离。]]></description>
      <guid>https://stackoverflow.com/questions/77916416/tensorflow-edit-distance-text-preprocessing</guid>
      <pubDate>Wed, 31 Jan 2024 21:04:46 GMT</pubDate>
    </item>
    <item>
      <title>MONAI DiceMetric</title>
      <link>https://stackoverflow.com/questions/77916384/monai-dicemetric</link>
      <description><![CDATA[我正在训练 MONAI model =SegResNet( out_channels=2) 来执行分割任务。我有两个类，前景和背景。真实分割/标签是 1 通道图像。
我使用 MONAI DiceLoss(softmax=True, include_background=False, to_onehot_y=True) ，它似乎有效，它减少了，并且预测看起来不错。 （include_background 是 False，因为背景比前景大得多。）但是我似乎不知道如何使用 DiceMetric(include_background=False,duction=“mean” ;, get_not_nans=False).
它要么给出错误，要么给出 1 或 0 或大于 1 的数字。
我阅读了我能找到的所有教程，并尝试复制这些教程，但没有成功......
我这样使用损失：
 vloss = loss_fn(voutputs, vlabels)
在教程“AsDiscreted”中这是一个常见的步骤。我不想使用 MONAI 转换，所以这就是我尝试过的：
voutputs_bin=voutputs
voutputs_bin=torch.nn.function.softmax(voutputs_bin,dim=1)
#voutputs_bin=torch.argmax(voutputs_bin,dim=1)
voutputs_bin = torch.nn.function.one_hot(voutputs_bin.to(torch.int64), num_classes=-1)
#voutputs_bin=voutputs_bin[:,1:,:,:]
#voutputs_bin=torch.nn.function.threshold(voutputs_bin,0.5,1)
vlabels_bin=torch.nn.function.one_hot(vlabels.to(torch.int64), num_classes=-1)

指标（y_pred=voutputs_bin，y=vlabels_bin）

我包含了注释行，因为这是我尝试过的以及它的不同组合。
在验证循环之后我会这样做：
vmetric=metric.aggregate().item()
print(f&#39; 骰子指标: {vmetric}&#39;)
指标.reset()

您能告诉我并解释一下，我应该如何正确使用 DiceMetric？]]></description>
      <guid>https://stackoverflow.com/questions/77916384/monai-dicemetric</guid>
      <pubDate>Wed, 31 Jan 2024 20:59:41 GMT</pubDate>
    </item>
    <item>
      <title>生物系统中 ANN 模型预测的 SHAP 分析令人困惑，需要帮助[重复]</title>
      <link>https://stackoverflow.com/questions/77915752/confusing-shap-analysis-of-ann-model-prediction-in-systems-bio-need-assistance</link>
      <description><![CDATA[我开发了一个 ANN 模型来根据 Elisa 数据预测蛋白质翻译后修饰模式。为了简单起见，如何、可行性和参数对于我的问题并不重要，并且省略了一些细节。
对于给定的蛋白质，我将其称为蛋白质 X，它具有泛素作为修饰，但没有磷酸化模式。
我用各种翻译后修饰模式训练了人工神经网络，但有一个关键信息：我的训练数据不包含任何泛素模式（假设有一个原因）
因此，当我使用一组抗体进行 ELISA 时，抗体 a 特异性针对泛素模式，抗体 b 特异性针对磷酸化模式。当我使用抗体 a、抗体 b（和其他抗体）预测蛋白质 x 修饰模式时，我们获得了相当好的准确性。
为了解释模型的工作原理，我运行了 SHAP 分析和二分图来显示特征重要性（抗体）和修改，但得到了令人困惑的结果
对于抗体 a，除泛素外，其他修饰模式都有正值和负值 SHAP 值，泛素是其特异性的
对于抗体 b，我们还发现除磷酸化之外的修饰模式的正 SHAP 值和负 SHAP 值，而蛋白质 x 并不真正具有磷酸化。
那么我如何解释为什么 SHAP 产生这种模式：1）抗体 a 与其目标泛素没有任何 SHAP 相关性，但对其其他目标有任何 SHAP 相关性，2）抗体 b 也与其目标没有任何 SHAP 相关性，而是与其他目标相关。 
这又是令人困惑的，因为我预计抗体 a 与泛素有 SHAP 相关性，但与其他蛋白没有 SHAP 相关性，然后抗体 b 不应该有任何 SHAP 相关性，因为它的目标是磷酸化，但蛋白 x 没有磷酸化。
我不太相信或无法将 SHAP 的一些限制联系起来，因为它显示了模型的隐藏模式/关系，而不是我们在“现实生活”中所期望的
有人可以对这个观察到的 SHAP 分析提供更细致的见解吗？]]></description>
      <guid>https://stackoverflow.com/questions/77915752/confusing-shap-analysis-of-ann-model-prediction-in-systems-bio-need-assistance</guid>
      <pubDate>Wed, 31 Jan 2024 18:44:06 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch RuntimeError：函数“NativeBatchNormBackward0”在其第 0 个输出中返回了 nan 值[关闭]</title>
      <link>https://stackoverflow.com/questions/77914164/pytorch-runtimeerror-function-nativebatchnormbackward0-returned-nan-values-in</link>
      <description><![CDATA[我尝试从一篇提供 Tensorflow 代码的论文中实现一个卷积神经网络，并将其转换为 Pytorch。
使用 torch.autograd.detect_anomaly() 我收到错误：
RuntimeError：函数“NativeBatchNormBackward0”在第 0 个输出中返回了 nan 值。
第一次调用loss.backward()期间。
我无法找出此错误背后的原因，因为我在网络上找不到任何有关它的信息。
所讨论的架构是在 conv1d() 期间 -&gt; BatchNorm1d() -&gt;; ReLU() -&gt;; MaxPool1d() 顺序。如果我注释掉 BatchNorm1d() 层，则不会发生错误。
我运行一个自定义损失函数，根据三个特征的两个 BCELosses 和一个 MSELoss 计算加权损失。
损失本身以数字形式返回，并且表现符合预期。
这是一个使用第一个卷积层和自定义损失重现代码的示例：
导入火炬
将 torch.nn 导入为 nn
将 numpy 导入为 np


类 CustomLoss(torch.nn.Module):
    def __init__(自身):
        超级().__init__()

    def 前向（自我，y_pred，y_true，n_splits，weight_prob = 1.0，weight_loc = 1.0，weight_area = 1.0）：
        y_true = torch.Tensor(y_true)
        y_pred = torch.Tensor(y_pred)

        pred_prob、pred_loc、pred_area = torch.tensor_split(y_pred、n_splits、dim=1)
        true_prob、true_loc、true_area = torch.tensor_split(y_true、n_splits、dim=1)

        掩码 = true_prob.eq(1.)

        prob_loss = torch.nn.BCELoss()(true_prob, pred_prob)
        loc_loss = torch.nn.BCELoss()(
            torch.masked_select(true_loc, mask), torch.masked_select(pred_loc, mask))
        area_loss = torch.nn.MSELoss()(
            torch.masked_select(true_area, mask), torch.masked_select(pred_area, mask))

        返回 （
                概率损失 * 权重概率 +
                loc_loss * 权重_loc +
                面积损失 * 重量面积
        ）


类 PeakDetection(nn.Module):
    def __init__(自身):
        超级().__init__()
        self.n_splits = 3
        self.conv_block1 = nn.Sequential(
            nn.Conv1d(in_channels=1,
                      输出通道=3，
                      内核大小=9，
                      步幅=2，
                      填充=4),
            nn.BatchNorm1d(3),
            ReLU(),
            nn.MaxPool1d（内核大小=16）
        ）

    def 前向（自身，x）：
        输出 = self.conv_block1(x)
        输出 = self.CustomActivation(输出)
        返回输出

    def CustomActivation（自身，输入）：
        pred, loc, 区域 = torch.tensor_split(输入, self.n_splits, dim=1)
        pred = torch.sigmoid(pred)
        loc = torch.sigmoid(loc)
        return torch.concat([pred,loc,area],dim=1)


torch.autograd.detect_anomaly(True)
# torch.manual_seed(42)
模型 = PeakDetection()

优化器 = torch.optim.Adam(params=model.parameters(), lr=0.01)

X = np.ones((32, 1, 8192))
y = np.ones((32, 3, 256))

使用 torch.autograd.detect_anomaly()：
    y_pred = 模型(火炬.张量(X))
    损失 = CustomLoss()(y_pred, torch.Tensor(y), n_splits=3)
    优化器.zero_grad()
    loss.backward()
    优化器.step()

manual_seed(42) 总是会产生相关的 NativeBatchNormBackward0 错误。]]></description>
      <guid>https://stackoverflow.com/questions/77914164/pytorch-runtimeerror-function-nativebatchnormbackward0-returned-nan-values-in</guid>
      <pubDate>Wed, 31 Jan 2024 14:33:58 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 PyTorch 编译产生的跨步断言错误？</title>
      <link>https://stackoverflow.com/questions/77913463/how-to-resolve-the-stride-assert-error-produced-by-the-pytorch-compile</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77913463/how-to-resolve-the-stride-assert-error-produced-by-the-pytorch-compile</guid>
      <pubDate>Wed, 31 Jan 2024 12:52:11 GMT</pubDate>
    </item>
    <item>
      <title>在Python中查找特征列的哪些过滤集导致最大目标列</title>
      <link>https://stackoverflow.com/questions/77910177/find-which-filter-sets-of-feature-columns-leads-to-maximum-target-column-in-pyth</link>
      <description><![CDATA[我无法找到可以解决我的问题的机器学习模型或分类类型。我本以为这可能相当简单，但也许不是。
假设我有 10 个特征列和一个二进制目标列。目标列的数据集中应该有大致相等数量的 0 和 1。整组数据并不是强相关的，所以线性回归、逻辑回归、朴素贝叶斯等都没有得出强相关的模型。然而，我所寻找的是哪个数据系列导致目标列的最大平均值。
例如。对于特征集 A 到 J 如果我按（C = True、D = false、J = true）过滤数据集，则目标 X 的平均值现在为 56%。我正在寻找一种算法，可以找到导致最大目标列均值的方程。
我觉得这可以通过蛮力来完成（循环遍历所有可能的组合），但我希望有一种方法可以在现有的众多数据科学库之一中做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/77910177/find-which-filter-sets-of-feature-columns-leads-to-maximum-target-column-in-pyth</guid>
      <pubDate>Wed, 31 Jan 2024 00:30:49 GMT</pubDate>
    </item>
    <item>
      <title>ValidationError：StuffDocumentsChain __root__ 出现 1 个验证错误</title>
      <link>https://stackoverflow.com/questions/76776695/validationerror-1-validation-error-for-stuffdocumentschain-root</link>
      <description><![CDATA[我收到此错误ValidationError：在 llm_chain input_variables 中找不到 StuffDocumentsChain __root__ document_variable_name 上下文的 1 个验证错误：[&#39;chat_history&#39;、&#39;user_query&#39;、&#39;relevant_context&#39;] (type=value_error)
在使用 load_qa_chain 时，我搜索了此错误，但没有找到与此相关的任何内容。谁能告诉我这里缺少什么。
代码：
template = &quot;&quot;&quot;您是一个正在与人类对话的聊天机器人。

给定长文档和问题的以下提取部分，创建最终答案。

{相关上下文}

{聊天记录}
人类：{user_query}
聊天机器人：“”“”

提示=提示模板(
input_variables=[“chat_history”, “user_query”, “relevant_context”],
模板=模板
）

内存= ConversationBufferMemory（memory_key =“聊天历史记录”，input_key =“用户查询”）

llm = OpenAI()
llm_chain = LLMChain(
    llm=llm,
    提示=提示，
    内存=内存，
）

链 = load_qa_chain(
    llm，chain_type =“东西”，内存=内存，提示=提示
）
]]></description>
      <guid>https://stackoverflow.com/questions/76776695/validationerror-1-validation-error-for-stuffdocumentschain-root</guid>
      <pubDate>Thu, 27 Jul 2023 05:20:25 GMT</pubDate>
    </item>
    <item>
      <title>将经过训练的机器学习模型与 React Native 应用程序集成</title>
      <link>https://stackoverflow.com/questions/74961856/to-integrate-a-trained-machine-learning-model-with-react-native-app</link>
      <description><![CDATA[我有一个 FYP 项目（Instagram 等社交媒体应用程序），需要我创建一个简单的推荐系统。我已经使用 Python 对余弦相似度数据集进行了训练，但我不知道下一步该做什么。如何将经过训练的机器学习模型集成到 React Native 中，或者是否有更好、更简单的方法来制作推荐系统？
我尝试阅读文档和观看视频。但我似乎仍然无法掌握一些困难的概念。如果您能在训练我的模型后向我提供有关学习内容的说明或步骤，我将不胜感激。或者，如果我必须使用一些库或软件包等。[不确定这是否是适合此查询的论坛]]]></description>
      <guid>https://stackoverflow.com/questions/74961856/to-integrate-a-trained-machine-learning-model-with-react-native-app</guid>
      <pubDate>Fri, 30 Dec 2022 13:02:29 GMT</pubDate>
    </item>
    <item>
      <title>部署时，SageMaker 无法提取容器的模型数据存档 tar.gz</title>
      <link>https://stackoverflow.com/questions/65881699/sagemaker-failed-to-extract-model-data-archive-tar-gz-for-container-when-deployi</link>
      <description><![CDATA[我正在尝试在 Amazon Sagemaker 中部署现有的 Scikit-Learn 模型。所以这个模型不是在 SageMaker 上训练的，而是在我的机器上本地训练的。
在我的本地（Windows）计算机上，我已将模型保存为 model.joblib 并将模型压缩为 model.tar.gz。
接下来，我已将此模型上传到我的 S3 存储桶 (&#39;my_bucket&#39;)，路径为 s3://my_bucket/models/model.tar.gz。我可以在 S3 中看到 tar 文件。
但是当我尝试部署模型时，它不断给出错误消息“无法提取模型数据存档”。
.tar.gz 是通过在 powershell 命令窗口中运行“tar -czf model.tar.gz model.joblib”在我的本地计算机上生成的。
上传到S3的代码
&lt;前&gt;&lt;代码&gt;导入boto3
s3 = boto3.client(“s3”,
              Region_name=&#39;eu-central-1&#39;,
              aws_access_key_id=AWS_KEY_ID,
              aws_secret_access_key=AWS_SECRET)
s3.upload_file(文件名=&#39;model.tar.gz&#39;, Bucket=my_bucket, Key=&#39;models/model.tar.gz&#39;)

用于创建估计器和部署的代码：
&lt;前&gt;&lt;代码&gt;导入boto3
从 sagemaker.sklearn.estimator 导入 SKLearnModel

...

model_data = &#39;s3://my_bucket/models/model.tar.gz&#39;
sklearn_model = SKLearnModel(model_data=model_data,
                             角色=角色，
                             Entry_point =“my-script.py”，
                             Framework_version =“0.23-1”）
预测器= sklearn_model.deploy（instance_type =“ml.t2.medium”，initial_instance_count = 1）

错误信息：
&lt;块引用&gt;
错误消息：UnexpectedStatusException：托管端点错误
sagemaker-scikit-learn-2021-01-24-17-24-42-204：失败。原因：失败
提取容器“container_1”的模型数据档案来自网址
“s3://my_bucket/models/model.tar.gz”。请确保对象
位于 URL 处的是有效的 tar.gz 存档

有没有办法查看存档无效的原因？]]></description>
      <guid>https://stackoverflow.com/questions/65881699/sagemaker-failed-to-extract-model-data-archive-tar-gz-for-container-when-deployi</guid>
      <pubDate>Mon, 25 Jan 2021 09:03:31 GMT</pubDate>
    </item>
    <item>
      <title>重新训练 Tensorflow 模型</title>
      <link>https://stackoverflow.com/questions/52769607/retrain-tensorflow-model</link>
      <description><![CDATA[我有一个使用对象检测 SSD 移动网络训练的张量流模型。
训练现已完成，我导出了模型推理以进行测试。我的问题是，如果我想稍后使用新的图像数据集重新训练模型，我现在应该在这个阶段做什么以使权重渗透到模型中，以便我可以从那时起重新训练它。我知道有一个冻结脚本，我必须使用它吗？ 
谢谢
阿亚德]]></description>
      <guid>https://stackoverflow.com/questions/52769607/retrain-tensorflow-model</guid>
      <pubDate>Thu, 11 Oct 2018 22:12:07 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用具有流输入和输出的机器学习库？ [关闭]</title>
      <link>https://stackoverflow.com/questions/50850497/is-it-possible-to-use-a-machine-learning-library-with-streaming-inputs-and-outpu</link>
      <description><![CDATA[我想将机器学习纳入我一直在从事的项目中，但我还没有看到任何关于我的预期用例的信息。看起来旧的潘多拉魔盒项目做了类似的事情，但是有文本输入和输出。
我想实时训练一个模型并使用它（然后当它运行良好时将其从测试切换到实时 api 端点。）
但我发现的每个库的工作方式都类似于“输入数据块，得到答案”
我希望能够将数据传输到其中：
而不是给它“5,4,3,4,3,2,3,4,5”，它说“1”或“-1”或“0”
我想给它“5”，然后“4”，然后“3”，然后“4”等等，每次它响应时。
我什至不确定“流媒体”是否是正确的词。请帮忙！]]></description>
      <guid>https://stackoverflow.com/questions/50850497/is-it-possible-to-use-a-machine-learning-library-with-streaming-inputs-and-outpu</guid>
      <pubDate>Thu, 14 Jun 2018 05:54:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中实现多元线性回归？</title>
      <link>https://stackoverflow.com/questions/48257144/how-do-i-implement-multiple-linear-regression-in-python</link>
      <description><![CDATA[我正在尝试从头开始编写一个多元线性回归模型来预测影响 Facebook 上歌曲观看次数的关键因素。关于每首歌曲，我们收集这些信息，即我正在使用的变量：

&lt;前&gt;&lt;代码&gt;df.dtypes
单击 int64
Listened_5s int64 已听
Listened_20s int64 已听
视图 int64
已听百分比 float64
反应总数 int64
共享歌曲 int64
评论 int64
平均听时间 int64
歌曲长度 int64
喜欢 int64
已收听_稍后 int64

我使用视图数作为因变量，并将数据集中的所有其他变量作为独立变量。该模型发布如下：

&lt;前&gt;&lt;代码&gt; #df_x --&gt;自变量的新数据框
  df_x = df.drop([&#39;视图&#39;], 1)

  #df_y --&gt;因变量视图的新数据框
  df_y = df.ix[:, [&#39;视图&#39;]]

  名称 = [i 代表列表中的 i(df_x)]

  regr = Linear_model.LinearRegression()
  x_train，x_test，y_train，y_test = train_test_split（df_x，df_y，test_size = 0.2）

   #将模型拟合到训练数据集
   regr.fit(x_train,y_train)
   regr.intercept_
   print(&#39;系数: \n&#39;, regr.coef_)
   print(&quot;均方误差(MSE): %.2f&quot;
         % np.mean((regr.predict(x_test) - y_test) ** 2))
   print(&#39;方差分数: %.2f&#39; % regr.score(x_test, y_test))
   regr.coef_[0].tolist()

此处输出：
 regr.intercept_
 数组([-1173904.20950487])
 微信：19722838329246.82
 方差得分：0.99

看起来出了什么严重错误。
尝试 OLS 模型：
 import statsmodels.api as sm
   从 statsmodels.sandbox.regression.predstd 导入 wls_prediction_std
   模型=sm.OLS(y_train,x_train)
   结果=模型.fit()
   打印(结果.summary())

输出：

&lt;前&gt;&lt;代码&gt; R 平方：0.992
     F 统计量：6121。

                      coef std err t P&gt;|t| [95.0% 浓度国际]


点击 0.3333 0.012 28.257 0.000 0.310 0.356
Listened_5s -0.4516 0.115 -3.944 0.000 -0.677 -0.227
已听 20 秒 1.9015 0.138 13.819 0.000 1.631 2.172
已听百分比 7693.2520 1.44e+04 0.534 0.594 -2.06e+04 3.6e+04
反应总数 8.6680 3.561 2.434 0.015 1.672 15.664
共享歌曲 -36.6376 3.688 -9.934 0.000 -43.884 -29.392
评论 34.9031 5.921 5.895 0.000 23.270 46.536
平均听时间 1.702e+05 4.22e+04 4.032 0.000 8.72e+04 2.53e+05
歌曲长度 -6309.8021 5425.543 -1.163 0.245 -1.7e+04 4349.413
喜欢 4.8448 4.194 1.155 0.249 -3.395 13.085
稍后收听 -2.3761 0.160 -14.831 0.000 -2.691 -2.061


综合巴士：233.399 杜宾-沃森：
1.983
概率（综合）： 0.000 Jarque-Bera (JB)：
2859.005
偏差：1.621 概率（JB）：
0.00
峰度：14.020 条件。不。
2.73e+07

警告：
[1] 标准误差假设误差的协方差矩阵已正确指定。
[2] 条件数很大，2.73e+07。这可能表明存在很强的多重共线性或其他数值问题。

仅通过查看此输出就可以看出出现了严重错误。
我认为训练/测试集和创建两个不同的数据框 x 和 y 出了问题，但无法弄清楚是什么。这个问题必须可以通过使用多元回归来解决。难道不是线性的吗？您能帮我找出问题所在吗？]]></description>
      <guid>https://stackoverflow.com/questions/48257144/how-do-i-implement-multiple-linear-regression-in-python</guid>
      <pubDate>Mon, 15 Jan 2018 04:58:03 GMT</pubDate>
    </item>
    </channel>
</rss>