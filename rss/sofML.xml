<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 21 Oct 2024 03:26:18 GMT</lastBuildDate>
    <item>
      <title>从处理后的图像中提取文本</title>
      <link>https://stackoverflow.com/questions/79108131/extract-text-from-processed-image</link>
      <description><![CDATA[我正在尝试从任何欧盟车牌的裁剪图像中提取文本。我尝试使用 easyocr，但结果对我来说太不准确了。我已经训练了一个 YOLOV8 模型来检测车牌，然后我裁剪了图像并对其进行了一些处理，如灰度和阈值处理。有人能帮我完成最后一步，从裁剪和处理过的车牌中提取文本吗？我的代码目前看起来像这样
from ultralytics import YOLO
import cv2
from inference import get_model
import easyocr

reader = easyocr.Reader([&#39;en&#39;])

plate_model = get_model(model_id=&quot;plate-recogniser/3&quot;, api_key=&lt;API_KEY&gt;)

# 加载图像
image_file = r&quot;C:\Users\46723\Desktop\Plate Recognition\samples\swedish.jpg&quot;
image = cv2.imread(image_file)

# 执行推理以检测车牌
results = plate_model.infer(image)[0]

for prediction in results.predictions:
x_center = prediction.x
y_center = prediction.y
width = prediction.width
height = prediction.height

# 计算边界框坐标
x1 = int(x_center - width / 2)
y1 = int(y_center - height / 2)
x2 = int(x_center + width / 2)
y2 = int(y_center + height / 2)

# 裁剪检测到的车牌区域
license_plate_crop = image[y1:y2, x1:x2]

# 转换为灰度
license_plate_crop_gray = cv2.cvtColor(license_plate_crop, cv2.COLOR_BGR2GRAY)

# 自适应阈值化
license_plate_crop_thresh = cv2.adaptiveThreshold(
license_plate_crop_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
cv2.THRESH_BINARY_INV, 11, 2
)

text_results = reader.readtext(license_plate_crop_thresh)

for (bbox, text, prob) in text_results:
print(f&quot;检测到的车牌文本：{text}，置信度：{prob:.2f}&quot;)

cv2.imshow(&quot;裁剪后的车牌&quot;, license_plate_crop)
cv2.waitKey(0)

cv2.destroyAllWindows()


我还想补充一点，我对 python 还不太熟悉。]]></description>
      <guid>https://stackoverflow.com/questions/79108131/extract-text-from-processed-image</guid>
      <pubDate>Sun, 20 Oct 2024 21:56:49 GMT</pubDate>
    </item>
    <item>
      <title>如何有效实施人工智能（AI）和机器学习（ML）系统的测试自动化策略？[关闭]</title>
      <link>https://stackoverflow.com/questions/79107998/how-can-i-effectively-implement-test-automation-strategies-for-artificial-intell</link>
      <description><![CDATA[我的任务是自动测试一个复杂的 AI/ML 系统，该系统涉及随着时间推移从数据中学习的各种算法和模型。鉴于 AI/ML 输出的不确定性以及模型验证所涉及的复杂性，我正在努力创建一个强大的测试自动化框架，以确保功能和性能。
我尝试过的方法：到目前为止，我已经探索了以下技术：

为单个算法和函数编写单元测试。
使用统计方法根据预期结果验证模型输出。
进行性能测试以评估不同数据负载下的模型响应时间。

但是，我面临着几个挑战，包括：

难以为基于训练数据进行调整的模型定义预期结果。
管理和验证训练和测试所需的大型数据集。
考虑到模型预测的固有可变性，确保测试的可靠性和可重复性。

预期结果：我正在寻求先进的策略和实施 AI/ML 系统测试自动化的最佳实践，重点关注：
有效测试 AI 模型功能和性能的方法：

以自动化方式验证模型准确性和稳健性的技术。
在不影响模型性能的情况下将测试集成到 ML 管道 (CI/CD) 中的策略。
]]></description>
      <guid>https://stackoverflow.com/questions/79107998/how-can-i-effectively-implement-test-automation-strategies-for-artificial-intell</guid>
      <pubDate>Sun, 20 Oct 2024 20:23:55 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：'LlamaForCausalLM' 对象没有属性 'invoke'</title>
      <link>https://stackoverflow.com/questions/79107923/attributeerror-llamaforcausallm-object-has-no-attribute-invoke</link>
      <description><![CDATA[我正在尝试对块进行一些总结。尝试使用“meta-llama/Meta-Llama-3.1-8B-Instruct”。代码如下
%%capture
from huggingface_hub import login

login(token=&quot;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&quot;)
quantization_config = BitsAndBytesConfig(load_in_4bit=True,
llm_int4_enable_fp32_cpu_offload=True)

llm_llama_model = AutoModelForCausalLM.from_pretrained(
&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;,
torch_dtype=torch.float32,
temperature =0,
device_map=&#39;auto&#39;,
quantization_config=quantization_config
)

llm_llama_tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;)

然后调用它
prompt = PromptTemplate(
template = &quot;&quot;&quot;写一个简明的总结。总结应该是项目要点的列表。总结不能超过 5 个项目要点。文本是：
{text}
简明总结：&quot;&quot;&quot;,
input_variables=[&quot;text&quot;]
)

chunkSummaries = []

for split in splits:
response = llm_llama_model.invoke(prompt.format(text=split.page_content))
chunkSummaries.append(response.content)

我看到了 AttributeError。尝试谷歌搜索，我找到的唯一答案是使用 unsloth。尝试使用它，但有太多其他错误。非常感谢任何解决 attributeerror 的见解]]></description>
      <guid>https://stackoverflow.com/questions/79107923/attributeerror-llamaforcausallm-object-has-no-attribute-invoke</guid>
      <pubDate>Sun, 20 Oct 2024 19:39:28 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：使用 MSE 时无法将字符串转换为浮点数：'？'</title>
      <link>https://stackoverflow.com/questions/79107349/valueerror-could-not-convert-string-to-float-while-working-with-mse</link>
      <description><![CDATA[我正在使用 auto-mpg 数据集。我在下面给出了数据集的链接：
https://www.kaggle.com/datasets/uciml/autompg-dataset
我在下面给出了代码：
df = pd.read_csv(&#39;data/auto-mpg.csv&#39;)

df.head()
df = df.drop(&#39;car name&#39;, axis=1)
X = df

X.head()
y = df[&#39;mpg&#39;]

y.head()
SEED = 1
# 将数据分成 70% 训练和 30% 测试
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)

# 实例化 DecisionTreeRegressor dt
dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)

# 计算包含 10 倍 CV MSE 的数组
MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv = 10,
scoring = &#39;neg_mean_squared_error&#39;, n_jobs = 1)

RMSE_CV = (MSE_CV_scores.mean())**(1/2)

#Error
ValueError:
所有 10 次拟合均失败。
很可能是您的模型配置错误。
您可以尝试通过设置 error_score=&#39;raise&#39; 来调试错误。
以下是有关失败的更多详细信息：
--------------------------------------------------------------------------------
10 次拟合失败，错误如下：

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError：无法将字符串转换为浮点数：&#39;？&#39;
]]></description>
      <guid>https://stackoverflow.com/questions/79107349/valueerror-could-not-convert-string-to-float-while-working-with-mse</guid>
      <pubDate>Sun, 20 Oct 2024 14:49:44 GMT</pubDate>
    </item>
    <item>
      <title>模型在训练和交叉验证集上表现良好，但在测试集上表现不佳[关闭]</title>
      <link>https://stackoverflow.com/questions/79107324/model-performs-well-on-train-and-cross-validation-sets-but-not-on-test-set</link>
      <description><![CDATA[我一直在研究 CNN 二元分类模型，该模型在训练集和交叉验证集上的表现都相当不错（两者的准确率实际上都为 1.0）。但是，我还持有一个独特的测试集，用于在最后设置我的模型。
我这样做是因为我使用基于交叉验证准确率的超参数调整。我认为这可能会导致模型架构根据 cv 集过度拟合。
如何确保在超参数调整后，我的模型在测试集上表现良好，而该测试集在权重训练和 hp 调整期间均未使用？
我将在此处附上模型结果的图像（最底线是测试集上的准确率）：
]]></description>
      <guid>https://stackoverflow.com/questions/79107324/model-performs-well-on-train-and-cross-validation-sets-but-not-on-test-set</guid>
      <pubDate>Sun, 20 Oct 2024 14:37:33 GMT</pubDate>
    </item>
    <item>
      <title>Light Gradient Boosting Machine 无法使用 GPU</title>
      <link>https://stackoverflow.com/questions/79107131/light-gradient-boosting-machine-can-not-use-the-gpu</link>
      <description><![CDATA[我正在尝试使用 Pycaret 进行一些练习。当我尝试仅使用 CPU 使用 setup 时，比较模型和调整模型需要很长时间。因此，我在 setup 函数中使用了 use_gpu=True。这样我就可以继续进行 pycaret 操作了。
但是我得到了错误
[LightGBM] [Fatal] 此版本中未启用 CUDA Tree Learner
请使用 CMake 选项 -DUSE_CUDA=1 重新编译

然后使用 lightgbm 进行调整变得非常慢。似乎使用例如 catboostm 创建其他模型仍然很快。
可能发生了什么，如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/79107131/light-gradient-boosting-machine-can-not-use-the-gpu</guid>
      <pubDate>Sun, 20 Oct 2024 13:17:01 GMT</pubDate>
    </item>
    <item>
      <title>.pth 到 .onnx 的转换破坏了模型 u2net</title>
      <link>https://stackoverflow.com/questions/79105371/pth-to-onnx-conversion-break-the-model-u2net</link>
      <description><![CDATA[任务如下：网站需要添加从汽车磁盘图像中删除背景的功能。
我决定使用 rembg 库作为基础：https://github.com/danielgatis/rembg
这个库又在 u2net 的基础上工作：https://github.com/xuebinqin/U-2-Net
但是，标准 u2net 模型会从外部删除所有背景，而磁盘内部的空间保持不变 - 在辐条、孔等之间。
在谷歌搜索了一下之后，我得出结论，我可以进一步训练 u2net 模型我的特定需求。
操作算法如下：

进一步训练模型
将其加载到 rembg 中
使用自定义模型进行裁剪

我设法训练了标准 u2net 模型，它完美地按照我的需要裁剪出黑白蒙版。
但是，当将模型从 .pth 转换为 .onnx 格式（这是在 rembg 中工作所必需的）时，它开始工作不佳。
蒙版模糊且有肥皂味。我尝试转换标准的未训练 u2net 模型
并在 rembg 中使用它 - 结果是一样的，蒙版模糊，背景裁剪不起作用。
因此，结论是训练成功了。
问题在于转换。
因此，以下是我训练过的模型的掩码示例。
原始图像
我训练过的模型生成的掩码
我训练过的模型转换为 .onnx 格式后生成的掩码
要在 u2net 中生成掩码，我使用：
python3 u2net_test.py

要在 rembg 中生成掩码，我使用命令：
rembg i -om -m u2net_custom -x &#39;{&quot;model_path&quot;: &quot;~/.u2net/u2net_custom.onnx&quot;}&#39; 55.jpg 55.png

我尝试转换完成的模型。以下是转换代码：
import torch
import torch.onnx
from model.u2net import U2NET

def load_model(model_path, model_class):
checkpoint = torch.load(model_path, map_location=&#39;cpu&#39;)
if isinstance(checkpoint, dict) and &#39;state_dict&#39; in checkpoint:
model = model_class()
model.load_state_dict(checkpoint[&#39;state_dict&#39;])
else:
model = model_class()
model.eval()
return model

def convert_to_onnx(model, output_path):
dummy_input = torch.randn(1, 3, 320, 320)
torch.onnx.export(model, dummy_input, output_path, opset_version=12,
dynamic_axes={&#39;input&#39;: {0: &#39;batch_size&#39;, 2: &#39;height&#39;, 3: &#39;width&#39;},
&#39;output&#39;: {0: &#39;batch_size&#39;, 2: &#39;height&#39;, 3: &#39;width&#39;}})
print(f&quot;success {output_path}&quot;)

if __name__ == &quot;__main__&quot;:
import argparse

parser = argparse.ArgumentParser(description=&quot;conversion PyTorch to ONNX&quot;)
parser.add_argument(&#39;--model-path&#39;, type=str, required=True, help=&#39;path to .pth file&#39;)
parser.add_argument(&#39;--output-path&#39;, type=str, required=True, help=&#39;save ONNX file&#39;)

args = parser.parse_args()
model = load_model(args.model_path, U2NET)
convert_to_onnx(model, args.output_path)

并尝试在训练过程中保存模型：
 if ite_num % save_frq == 0:
timestamp = int(time.time())
filePath = model_dir + model_name+&quot;_%d_%d.&quot; % (ite_num, timestamp)

torch.save(net.state_dict(), filePath + &#39;pth&#39;)

dummy_input = torch.randn(1, 3, 320, 320)
net.eval()
torch.onnx.export(net, dummy_input, filePath + &#39;onnx&#39;, opset_version=12)

running_loss = 0.0
running_tar_loss = 0.0
net.train() # 恢复训练
ite_num4val = 0

我尝试更改设置、更改库版本、更改 opset_version 以及 chatGpt 建议的所有其他操作。
结果总是一样的。
转换后模型停止工作。
我犯了什么错误？]]></description>
      <guid>https://stackoverflow.com/questions/79105371/pth-to-onnx-conversion-break-the-model-u2net</guid>
      <pubDate>Sat, 19 Oct 2024 16:07:45 GMT</pubDate>
    </item>
    <item>
      <title>如何估计 CoreML 模型的参数数量？</title>
      <link>https://stackoverflow.com/questions/79103126/how-to-estimate-the-number-of-parameters-for-coreml-models</link>
      <description><![CDATA[我正在比较修剪对 CoreML 模型的影响。
虽然我可以轻松测量文件大小的变化（以 kB 为单位），但我很难估计模型参数数量的变化，因为 CoreML 没有提供像 PyTorch 的 model.parameters() 这样的直接方法。我如何估计或计算修剪后的 CoreML 模型中的参数数量？]]></description>
      <guid>https://stackoverflow.com/questions/79103126/how-to-estimate-the-number-of-parameters-for-coreml-models</guid>
      <pubDate>Fri, 18 Oct 2024 17:35:25 GMT</pubDate>
    </item>
    <item>
      <title>Python 在 XGBoost 预测模型中更新未来预测的傅里叶项和滞后[关闭]</title>
      <link>https://stackoverflow.com/questions/79102211/python-updating-fourier-terms-and-lags-for-future-predictions-in-xgboost-forecas</link>
      <description><![CDATA[我在 Python 中构建了一个 XGBoost 预测模型，该模型结合了滞后特征、日期时间特征和傅立叶项。我的目标是对每个新的工作日进行预测。该模型使用 5 倍交叉验证进行训练，我从调整过程中保存了最佳超参数。之后，我使用这些最佳参数重新训练整个模型。
我面临的挑战是使用预测日的正确特征更新未来数据框：
对于日期时间特征，我可以轻松更新它们。
但是，更新傅立叶项 (FT) 和滞后是我遇到的难题。
这是我尝试过的方法：
我将历史数据框与新的预测数据框结合起来，将 Y 列重命名为 pred。
对于第一个预测日，我使用最后已知的实际值。
对于后续的预测日，我需要使用历史数据和新预测值的组合来更新傅立叶项和滞后。
尽管尝试了几次，我还是无法让更新过程正常工作。傅立叶项没有按预期对齐，我很难根据历史数据和预测数据的组合调整滞后。
问题：如何使用历史预测和新预测的组合正确更新未来几天的傅立叶项和滞后？以下是数据框。 pred 列是实际历史数据的 Y 变量（国家计数）的副本。

# 使用 NaN 初始化预测列
future_df_all_countries[&#39;pred&#39;] = np.nan

# 使用已经训练过的 `best_model` 对未来数据进行预测
# 循环遍历 future_df_all_countries 中的每一行
for i in range(len(future_df_all_countries)):
if not future_df_all_countries[&#39;is_actual&#39;].iloc[i]: # 仅预测是否为未来数据
if i == 0:
# 对于第一个预测日，使用最后一个实际计数
last_actual_count = future_df_all_countries.loc[future_df_all_countries[&#39;is_actual&#39;] == True, &#39;Country count&#39;].iloc[-1]
future_df_all_countries.at[future_df_all_countries.index[i], &#39;pred&#39;] = last_actual_count
else:
# 使用已经训练好的模型对当天进行预测
future_df_all_countries.at[future_df_all_countries.index[i], &#39;pred&#39;] = best_model.predict(future_df_all_countries[FEATURES].iloc[[i]])[0]

# 计算傅里叶变换的函数
def calculate_fourier_transform(group, components):
data_FT = group[[&#39;pred&#39;]] # 使用 &#39;pred&#39; 列进行 FT 计算
if data_FT[&#39;pred&#39;].isnull().all():
return pd.DataFrame(index=group.index) # 如果全部为 NaN，则返回空 DF
country_count_fft = np.fft.fft(np.asarray(data_FT[&#39;pred&#39;].tolist()))
ifft_results = pd.DataFrame(index=group.index)

# 使用指定的列名进行更新
for i, num_ in enumerate(components):
fft_list = np.copy(country_count_fft)
fft_list[num_:-num_] = 0
ifft_results[f&#39;ifft_{num_}_components&#39;] = np.fft.ifft(fft_list).real

return ifft_results

# 用于傅里叶变换的组件列表
component_list = [70, 80, 90] # 使用指定的组件

#使用“pred”列计算所有行的 FT
fourier_results = calculate_fourier_transform(future_df_all_countries, component_list)

# 使用 FT 结果更新 future_df_all_countries
for col in fourier_results.columns:
future_df_all_countries[col] = fourier_results[col]

# 根据“pred”创建滞后特征
def create_lag_features(df, target_col=&#39;pred&#39;, group_col=&#39;SHIP_TO_COUNTRY&#39;, lags=[5, 10, 15, 30]):
for lag in lags:
df[f&#39;lag_{lag}_day&#39;] = df.groupby(group_col)[target_col].shift(lag)
return df

# 应用滞后特征创建
future_df_all_countries = create_lag_features(future_df_all_countries)
]]></description>
      <guid>https://stackoverflow.com/questions/79102211/python-updating-fourier-terms-and-lags-for-future-predictions-in-xgboost-forecas</guid>
      <pubDate>Fri, 18 Oct 2024 13:07:36 GMT</pubDate>
    </item>
    <item>
      <title>为什么我对二元分类的多元二维卷积 LSTM 模型的超参数调整是错误的？[关闭]</title>
      <link>https://stackoverflow.com/questions/79097667/why-is-my-hyperparameter-tuning-for-a-multivariate-2d-convolutional-lstm-model-f</link>
      <description><![CDATA[我尝试编写一个 2D 卷积 LSTM 模型。
我有 7514 个样本。
每个样本包含 180 分钟的数据。
有 5 个特征。
一个样本由连续的 120 分钟周期组成，其中每分钟的目标值为 0，然后是 60 分钟周期，每分钟的目标值为 1。
120 分钟周期表示正常活动，而 60 分钟周期表示异常活动。
顺序为 Conv2D - 批量标准化 - 激活 - LSTM - 全连接。
我不完全了解 Conv2D 的时间序列参数。
我不确定我的输出形状是否正确。从我在线阅读的示例中可以看出，对于使用多元时间序列数据的二元分类，conv2d 的使用尚不明确。这些是输出形状： │ conv2d_2 (Conv2D) │ (None, 1, 5, 180) │97,380 │ │ batch_normalization_2 │ (None, 1, 5, 180) │720 │ │activation_2 (Activation) │ (None, 1, 5, 180) │0 │ │ time_distributed_2 (TimeDistributed) │ (None, 1, 900) │0 │ │ lstm_2 (LSTM) │ (None, 1, 120) │490,080 │ │density_2 (Dense) │ (None, 1, 1) │121 │
此外，从第 2 到第 5 个时期的结果始终如下：loss：10.6280 - precision_13： 0.3333 - recall_13: 1.0000 - val_loss: 10.6276 - val_precision_13: 0.3334 - val_recall_13: 1.0000 我做错了什么？
import pandas as pd
import io
import tensorflow as tf
import keras
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import LSTM, Dense, BatchNormalization, Flatten, Conv2D, Reshape, TimeDistributed
import numpy as np
from sklearn.preprocessing import LabelEncoder

colnames=(&#39;Bx&#39;, &#39;By&#39;, &#39;Bz&#39;, &#39;Vx&#39;, &#39;Density&#39;, &#39;Labels&#39;, &#39;ID&#39;)
df = pd.read_csv(&#39;200304periodnanreplacedwithID&#39;, sep=&#39;\s+&#39;, names=colnames)
dfc=df.drop([&#39;Labels&#39;, &#39;ID&#39;], axis=1)
rmv=dfc[dfc.apply(sum, axis = 1) == 0].index
df1=df.drop(rmv)
counts = df1[&#39;ID&#39;].value_counts()
df1=df1[df1[&#39;ID&#39;].isin(counts.index[counts == 180])]
df1=df1.drop([&#39;ID&#39;], axis=1)

Xcols=[x for x in df1.columns if x!= &#39;Labels&#39;]
features=len(Xcols)
model= Sequential()
X=df1[Xcols]
X=np.resize(X, (X.shape[0], 1, X.shape[1]))
y=df1[&#39;Labels&#39;]

def basic_conv2D(n_filters=7514, fsize=5, window_size=180, n_features=5):
new_model = keras.Sequential()
new_model.add(tf.keras.layers.Conv2D(180, (3, fsize), padding=&#39;same&#39;, input_shape=(window_size, n_features, 1)))
new_model.add(BatchNormalization())
new_model.add(tf.keras.layers.Activation(&#39;relu&#39;))
new_model.add(TimeDistributed(Flatten()))
new_model.add(tf.keras.layers.LSTM(120, return_sequences=True))
new_model.add(tf.keras.layers.Dense(1))
adm = keras.optimizers.Adam(learning_rate=0.01)
new_model.compile(optimizer=adm, loss=&#39;binary_crossentropy&#39;, metrics=[keras.metrics.Recall(), keras.metrics.Precision()])
返回 new_model

m2 = basic_conv2D(n_filters=7514, fsize=5, window_size=1, n_features=5)
m2.summary()

m2_hist = m2.fit(X, y, batch_size=180, shuffle=False, validation_split=0.3, epochs=5)
]]></description>
      <guid>https://stackoverflow.com/questions/79097667/why-is-my-hyperparameter-tuning-for-a-multivariate-2d-convolutional-lstm-model-f</guid>
      <pubDate>Thu, 17 Oct 2024 10:32:43 GMT</pubDate>
    </item>
    <item>
      <title>ImportError: 导入 o​​nnx_cpp2py_export 时 DLL 加载失败：动态链接库 (DLL) 初始化例程失败</title>
      <link>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</guid>
      <pubDate>Wed, 18 Sep 2024 07:08:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 tf.keras.metrics.R2Score 导致 Tensorflow 出现错误</title>
      <link>https://stackoverflow.com/questions/78056806/using-tf-keras-metrics-r2score-results-in-an-error-in-tensorflow</link>
      <description><![CDATA[我正在使用 Tensorflow 制作回归模型，但是当我使用 tf.keras.metrics.R2Score() 作为指标时，它在第一个 epoch 之后失败，并出现 ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.0&gt;。 （但在此之前工作正常）但是，如果我使用不同的指标（tf.keras.metrics.RootMeanSquaredError()），它工作正常。
import pandas as pd

weather_states = pd.read_sql(&quot;SELECT stations.id, stations.capacity_kw, start, wind_speed_10m, wind_direction_10m, wind_speed_80m, wind_direction_80m, wind_speed_180m, wind_direction_180m FROM stations INNER JOIN weather_states ON stations.id = weather_states.station WHERE weather_states.source = &#39;openmeteo_forecast/history/best&#39; AND stations.source = &#39;wind&#39;&quot;, db_client)

grid_states = pd.read_sql(&quot;SELECT start, wind来自 grid_states&quot;, db_client)

def create_x_y(df: tuple[Any, pd.DataFrame]):
start = df[1][&quot;start&quot;].iloc[0]
res = df[1].sort_values(&quot;id&quot;).drop([&quot;id&quot;, &quot;start&quot;], axis=1)
temp_wind = grid_states.loc[grid_states[&quot;start&quot;] == start][&quot;wind&quot;].to_list()
wind_kw = temp_wind if len(temp_wind) &gt;= 1 else None
res_flat_df = pd.DataFrame(res.to_numpy().reshape((1, -1)))
res_flat_df[&quot;wind_kw&quot;] = wind_kw
返回res_flat_df

data = pd.concat(map(create_x_y, weather_states.groupby(&quot;start&quot;))).dropna()
来自 sklearn.model_selection 导入 train_test_split

data = data.astype(&quot;float32&quot;)
train, test = train, test = train_test_split(data.dropna(), test_size=0.2)

train_y = train.pop(&quot;wind_kw&quot;)
train_x = train

test_y = test.pop(&quot;wind_kw&quot;)
test_x = test

norm = tf.keras.layers.Normalization()
norm.adapt(train_x)

model = tf.keras.Sequential([
norm,
tf.keras.layers.Dense(16,activation=&quot;linear&quot;),
tf.keras.layers.Dropout(0.3),
tf.keras.layers.Dense(1, 激活=&quot;线性&quot;),
])

model.compile(
优化器=tf.keras.optimizers.legacy.Adam(0.001),
指标=[tf.keras.metrics.R2Score(dtype=tf.float32)],
损失=tf.keras.losses.MeanSquaredError(),
)

model.fit(train_x, train_y, epochs=7, batch_size=2)

tf.keras.models.save_model(model, &#39;wind.keras&#39;)

print(data.describe())
 0 1 2 3 4 ... 241 242 243 244 wind_kw
计数 1896.0 1896.000000 1896.000000 1896.000000 1896.000000 ... 1896.000000 1896.000000 1896.000000 1896.000000 1896.000000
平均值 144000.0 4.315717 189.610759 5.791377 193.830169 ... 3.881292 145.420359 4.572205 143.642405 1292.576958
标准差 0.0 2.482439 113.178764 2.926497 113.685887 ... 2.612259 93.293471 2.775681 94.721086 611.333721
最小值 144000.0 0.100000 1.000000 0.100000 1.000000 ... 0.100000 2.000000 0.000000 1.000000 34.263000
25% 144000.0 2.110000 88.000000 3.487500 90.000000 ... 1.900000 67.000000 2.500000 63.000000 793.109500
50% 144000.0 4.110000 199.000000 5.500000 231.000000 ... 3.075000 137.000000 3.940000 135.000000 1251.590000
75% 144000.0 6.220000 291.000000 7.882500 294.000000 ... 5.502500 205.000000 6.082500 205.000000 1761.926750
最大144000.0 11.670000 360.000000 15.210000 360.000000 ... 14.460000 360.000000 16.980000 360.000000 3008.125000

print(type(data))
#&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
print(data.dtypes)
#0 float32
#1 float32
#2 float32
#3 float32
#4 float32
# ... 
#241 float32
#242 float32
#243 float32
#244 float32
#wind_kw float32
#Length: 246, dtype: object
print(data.shape)
#(1896, 246)

我似乎无法在网上找到有关使用 R2Score 时出现此错误的任何信息 - 您对问题可能是什么有任何想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78056806/using-tf-keras-metrics-r2score-results-in-an-error-in-tensorflow</guid>
      <pubDate>Sun, 25 Feb 2024 16:40:38 GMT</pubDate>
    </item>
    <item>
      <title>如何将分类变量添加到百分比堆积条形图？</title>
      <link>https://stackoverflow.com/questions/77068899/how-to-add-categorical-variables-to-a-percentage-stacked-bar-chart</link>
      <description><![CDATA[第一次在这里发帖，如果我遗漏了通常包含的任何细节，请告诉我。
我正在使用 ggplot2 和 ggdendro 制作带有分层聚类树的堆叠条形百分比图，其中每个节点都与我的一个条形图相关联。

如您所见，我或多或少已经弄清楚了这一点（请注意，这只是我的数据的一个子集。我现在想将一个分类变量与我的每个条形图关联起来，其中每个变量都用一种颜色表示（在我的情况下，这是 HIV+ 或 HIV-，每个条形图代表给定类别中细胞的百分比）。此外，我想弄清楚如何将样本名称添加到每个树状图节点，但这个问题不那么紧迫。
以下是我正在使用的代码块。
library(ggplot2)
library(ggdendro)

# 加载表型图数据
TotalPercentage &lt;- read.csv(&quot;~/TotalPercentage.csv&quot;, header=TRUE)

#生成树
tree &lt;- hclust(dist(TotalPercentage))
tree &lt;- dendro_data(tree)

数据 &lt;- cbind(TotalPercentage, x = match(rownames(TotalPercentage), tree$labels$label))

# 在堆积条形图下方绘制，位于 &quot;data = tidyr::pivot_longer(data, c(2...&quot;包括
## 所有列（集群），但排除列 1，因为该值是我们的样本 ID

scale &lt;- .5
p &lt;- ggplot() +
geom_col(
data = tidyr::pivot_longer(data, c(2, 3 , 4, 5, 6, 7, 8)),
aes(x = x,
y = value, fill = factor(name)),
) +
labs(title=&quot;无监督聚类表型图输出&quot;,
x =&quot;集群表示 (%)&quot;, y = &quot;参与者样本&quot;
) +
geom_segment(
data = tree$segments,
aes(x = x, y = -y * scale, xend = xend, yend = -yend * scale)
)

p

这是一个样本数据集，包含较少行以便简单起见
data.frame(
`参与者 ID` = c(&quot;123&quot;, &quot;456&quot;, &quot;789&quot;),
`1` = c(.1933, .1721, 34.26),
`2` = c(20.95, 4.97, 2.212),
`3` = c(11.31, 35.34, .027),
`4` = c(35.55, 15.03, 0),
`5` = c(.26, .87, 7.58),
`6` = c(12.85, 33.44, .033),
`7` = c(2.04, 3.77, 4.32)
)

患者一和三感染 HIV，但患者二感染 HIV 阴性
最后，这是我最终尝试制作的一个例子
(https://i.sstatic.net/uAWxR.png)
我已经到处查看如何执行此操作，但我对 R 还不熟悉，所以我有点不知所措，不知道下一步该怎么做。提前感谢任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/77068899/how-to-add-categorical-variables-to-a-percentage-stacked-bar-chart</guid>
      <pubDate>Fri, 08 Sep 2023 17:36:00 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何正确使用 torch.compile？</title>
      <link>https://stackoverflow.com/questions/75886125/how-should-i-use-torch-compile-properly</link>
      <description><![CDATA[我目前正在尝试使用 pytorch 2.0 来提升我的项目的训练性能。我听说 torch.compile 可能会提升一些模型的性能。
所以我的问题（目前）很简单；我应该如何使用带有大型模型的 torch.compile？
例如，我应该像这样使用 torch.model 吗？
class BigModel(nn.Module):
def __init__(self, ...):
super(BigModel, self).__init__()
self.model = nn.Sequential(
SmallBlock(), 
SmallBlock(), 
SmallBlock(), 
...
)
...

class SmallBlock(nn.Module):
def __init__(self, ...):
super(SmallBlock, self).__init__()
self.model = nn.Sequential(
...some small model...
)

model = BigModel()
model_opt = torch.compile(model)

，或者像这样？
class BigModel(nn.Module):
def __init__(self, ...):
super(BigModel, self).__init__()
self.model = nn.Sequential(
SmallBlock(), 
SmallBlock(), 
SmallBlock(), 
...
)
...

class SmallBlock(nn.Module):
def __init__(self, ...):
super(SmallBlock, self).__init__()
self.model = nn.Sequential(
...一些小模型...
)
self.model = torch.compile(self.model)

model = BigModel()
model_opt = torch.compile(model)

总结一下，

应该编译每一层吗？或者 torch.compile 会自动执行此操作？
有没有什么关于正确使用 torch.compile 的技巧？

说实话，我都试过了，但没有什么区别。
而且，它并没有显著加速，我只是检查了我的模型的加速率大约为 5 ~ 10%。]]></description>
      <guid>https://stackoverflow.com/questions/75886125/how-should-i-use-torch-compile-properly</guid>
      <pubDate>Thu, 30 Mar 2023 08:59:07 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习来删除重复数据</title>
      <link>https://stackoverflow.com/questions/16381133/using-machine-learning-to-de-duplicate-data</link>
      <description><![CDATA[我遇到了以下问题，并且认为我可以使用机器学习，但我不完全确定它是否适合我的用例。
我有一个包含客户数据（包括姓名、地址、电子邮件、电话等）的数据集，我想找到一种方法来清理这些客户数据并识别数据集中的可能重复项。
大多数数据都是使用外部系统手动输入的，没有经过验证，因此我们的许多客户最终在我们的数据库中拥有多个配置文件，有时每条记录中的数据都不同。
例如，我们可能有 5 个不同的条目用于客户 John Doe，每个条目都有不同的联系方式。
我们还遇到过代表不同客户的多条记录在电子邮件等关键字段上匹配的情况。例如，当客户没有电子邮件地址但数据输入系统需要时，我们的顾问将使用随机电子邮件地址，从而导致许多不同的客户资料使用相同的电子邮件地址，电话、地址等也是如此。
我们所有的数据都在 Elasticsearch 中编入索引并存储在 SQL Server 数据库中。我的第一个想法是使用 Mahout 作为机器学习平台（因为这是一家 Java 商店），也许使用 H-base 来存储我们的数据（只是因为它适合 Hadoop 生态系统，不确定它是否有任何实际价值），但我读得越多，我就越困惑它在我的情况下会如何工作，首先我不确定我可以使用哪种算法，因为我不确定这个问题属于哪一类，我可以使用聚类算法或分类算法吗？当然，必须使用某些规则来确定什么构成了个人资料的唯一性，即哪些字段。
我们的想法是最初将其部署为客户个人资料重复数据删除服务，我们的数据输入系统可以使用它来验证和检测输入新客户个人资料时可能的重复项，将来可能会将其开发为分析平台，以收集有关我们客户的见解。
任何反馈都将不胜感激 :)
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/16381133/using-machine-learning-to-de-duplicate-data</guid>
      <pubDate>Sun, 05 May 2013 03:36:00 GMT</pubDate>
    </item>
    </channel>
</rss>