<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 23 Apr 2024 21:12:39 GMT</lastBuildDate>
    <item>
      <title>使用 python 的神经网络 NarX 模型</title>
      <link>https://stackoverflow.com/questions/78374854/neural-network-narx-model-with-python</link>
      <description><![CDATA[我正在尝试使用 Python 进行 NarX 模型。当我运行以下代码时，出现以下错误。我该如何解决这个错误？这个错误是什么意思？
net.learn(epochs= 30, show_epoch_results=True, random_testing=False)

]]></description>
      <guid>https://stackoverflow.com/questions/78374854/neural-network-narx-model-with-python</guid>
      <pubDate>Tue, 23 Apr 2024 20:31:49 GMT</pubDate>
    </item>
    <item>
      <title>无法过度拟合多项式回归？</title>
      <link>https://stackoverflow.com/questions/78374435/failing-to-overfit-polynomial-regression</link>
      <description><![CDATA[我正在尝试将多项式回归过拟合到正弦曲线。据我所知，当有 N 数据样本和多项式次数 N-1 时，曲线应该穿过所有数据点，但是，在我的例如这不会发生。
我的代码如下：
从 sklearn.linear_model 导入 LinearRegression
从 sklearn.preprocessing 导入多项式特征

数 = 50
度 = 49

X = X = np.linspace(0, 2 * np.pi, N).reshape(-1, 1)
X = np.sort(X, 轴=0)
y = np.sin(X) + np.random.randn(N, 1) * 0.2

poly_features = 多项式特征（度=deg，include_bias=False）

X_poly = poly_features.fit_transform(X)

reg = 线性回归()
reg.fit(X_poly, y)

y_vals = reg.predict(X_poly)

plt.scatter(X, y)
plt.plot(X, y_vals, color=&#39;r&#39;)
plt.show()


你能解释一下我在这里的误解吗？]]></description>
      <guid>https://stackoverflow.com/questions/78374435/failing-to-overfit-polynomial-regression</guid>
      <pubDate>Tue, 23 Apr 2024 18:44:44 GMT</pubDate>
    </item>
    <item>
      <title>提高我正在尝试的这个 GitHub 项目的准确性的方法是什么</title>
      <link>https://stackoverflow.com/questions/78374110/what-is-the-way-to-increase-the-accuracy-of-this-github-project-im-trying</link>
      <description><![CDATA[我正在尝试运行此 GitHub 项目在我的电脑中。这是唇读模型。
该项目的模型不太准确，但正如使用说明中提到的那样，可以完美运行：
&lt;块引用&gt;
注意：predict_live.py 中实现的当前模型是一个不太准确的模型。我无法
由于文件太大，请上传 /training/3DCNN.ipynb 中显示的模型的权重。

我很困惑如何将其转变为更准确的模型，可能需要更多的数据集。目前，它非常不准确，因为它无法正确标记，请阅读下文。
以下是我在尝试运行 Jupyter Notebook“3DCNN.ipynb”时遇到的问题，其中：
dir_path = “./collected_data”
高度、宽度、通道 = 80、112、3

视频=[]
标签=[]
计数器 = 0

对于 os.walk(dir_path) 中的 root、dirs、文件：
    

    对于文件中的文件：
        如果文件==“data.txt”：

            # 从目录名中提取标签
            标签 = root.split(&quot;/&quot;)[-1]
            标签 = label.split(“_”)[0]
            #如果标签不在wanted_words中：
            ＃    继续
            计数器 += 1
            打印（计数器，结束=“”）

            将 open(os.path.join(root, file), &#39;r&#39;) 作为 f：
                data_str = f.read()

            # 将文本文件的内容作为 Python 表达式进行计算
            数据列表 = eval(data_str)
            
            # 将列表转换为 numpy 数组
            data_array = np.array(data_list)
            #print(data_array.shape)

            # 将数据重塑为 4D 形状数组（num_frames、高度、宽度、通道）
            num_frames = len(数据列表)
            帧 = data_array.reshape((num_frames, 高度, 宽度, 通道))
            # 将帧和标签附加到视频和标签数组中
            视频.append(帧)
            labels.append(标签)
打印（标签）

# 将视频和标签数组转换为 NumPy 数组
视频 = np.array(视频)
标签 = np.array(标签)

# 将视频和标签保存为单独的 .npy 文件
np.save(“videosCorrect.npy”, 视频)
np.save(“labelsCorrect.npy”, 标签)

以上部分结果如下：
1 2 3 4 5 6 7 8 9 10 11 ... 685 [&#39;已收集&#39;]....[&#39;已收集&#39;]

我原以为这工作正常，因为它确实对所有 685 个数据集文件进行了编号=&gt; print(counter) 然后开始发送垃圾邮件 [&#39;collected&#39;]，即 print(labels)。
我认为它还正确地分割了训练数据：
编码器 = LabelEncoder()
编码器.fit(标签)
编码标签 = 编码器.transform(标签)
标签 = 编码标签
label_dict = {6: &#39;你好&#39;, 5: &#39;狗&#39;, 10: &#39;我的&#39;, 12: &#39;你&#39;, 9: &#39;嘴唇&#39;, 3: &#39;猫&#39;, 11: &#39;读&#39;, 0: &#39;a&#39; , 4: &#39;演示&#39;, 7: &#39;这里&#39;, 8: &#39;是&#39;, 1: &#39;再见&#39;, 2: &#39;可以&#39;}

# 将数据分为训练集和验证集

X_train, X_test, y_train, y_test = train_test_split(视频, 标签, test_size=0.2, random_state=42)

print(&quot;训练集形状：&quot;, X_train.shape, y_train.shape)
print(&quot;测试集形状：&quot;, X_test.shape, y_test.shape)

删除视频

结果：
训练集形状：(548, 22, 80, 112, 3) (548,)
测试集形状：(137,22,80,112,3)(137,)

现在，当我运行 print(labels) 时，以下代码会导致另一个 0 的垃圾邮件，因此在循环运行时它不会正确分隔 label_dict： 
# 获取唯一的类
类= np.unique(标签)
打印（标签）
# 统计每个类出现的次数
计数 = np.bincount(标签, minlength=len(类))
打印（计数）
# 找到类名的最大长度以进行对齐
max_len = max([len(label_dict[i]) for i in range(len(label_dict))])
打印（最大长度）
打印（长度（label_dict））
# 打印类的分布
对于 i，计数 enumerate(counts)：
    类名 = label_dict[i].ljust(max_len)
    print(&quot;{} {} 计数&quot;.format(class_name, count))

结果：
[0 0 0 0 0 0 0 0 0 0... 0] #删除 0 的数量以使该帖子减少垃圾邮件... print(labels)
[685]#print（计数）
5 #打印（最大长度）
13 #print(len(label_dict))
a 685 counts #Here，它应该循环遍历 label_dict 的所有索引，但由于它没有正确标记，因此无法正确计数...

我不知道如何让 label_dict 正确计数。该数据集由 685 个标记为 a_1、a_2 等的文件夹组成。它对某人来说工作正常吗？]]></description>
      <guid>https://stackoverflow.com/questions/78374110/what-is-the-way-to-increase-the-accuracy-of-this-github-project-im-trying</guid>
      <pubDate>Tue, 23 Apr 2024 17:23:48 GMT</pubDate>
    </item>
    <item>
      <title>如何开发react和python之间的数据传输以进行联邦学习[关闭]</title>
      <link>https://stackoverflow.com/questions/78373881/how-to-develop-data-transfer-between-react-and-python-for-federated-learning</link>
      <description><![CDATA[我们的团队目前正在致力于创建联合学习应用程序。在我们的团队中，一组专注于算法和其他技术方面的开发，而其余成员则致力于构建客户端界面。
考虑到我们项目的性质（围绕联邦学习），我们正在探索通过 API 直接通信的替代方案。相反，我们正在考虑在应用程序本身内实现数据传输功能。
我们的计划涉及通过 API 与服务器和模型安全地共享加密数据以供一般使用。不过，我们仍在考虑如何构建和构建该应用程序以使其正式发布的具体细节。
请与我分享您的想法，谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78373881/how-to-develop-data-transfer-between-react-and-python-for-federated-learning</guid>
      <pubDate>Tue, 23 Apr 2024 16:36:00 GMT</pubDate>
    </item>
    <item>
      <title>在时间序列预测中根据不规则采样的训练数据进行预测，无需使用 Python 进行插值</title>
      <link>https://stackoverflow.com/questions/78373739/forecasting-from-irregularly-sampled-training-data-in-time-series-forecasting-wi</link>
      <description><![CDATA[有没有一种方法可以从不规则采样的数据中获得规则间隔的预测，如下所示：样本数据
我希望从上述测试数据中收集预测，该数据每隔 1 秒间隔有一个数据点。任何形式的帮助将不胜感激:)
我尝试在 LSTM 模型中插入“日期和时间”作为第二个参数，但这导致预测存在间隙并且间隔不规则。我还尝试对数据进行插值，但这导致我的预测非常不准确。]]></description>
      <guid>https://stackoverflow.com/questions/78373739/forecasting-from-irregularly-sampled-training-data-in-time-series-forecasting-wi</guid>
      <pubDate>Tue, 23 Apr 2024 16:07:43 GMT</pubDate>
    </item>
    <item>
      <title>训练 spacy 模型时生成并添加新的词向量</title>
      <link>https://stackoverflow.com/questions/78373465/generate-and-add-new-word-vectors-when-training-spacy-model</link>
      <description><![CDATA[我正在使用 spacy 训练自定义 textcat_multilabel 分类模型（使用 pytorch GPU 初始值设定项在 GPU 精度模式下），将一些文本分类为一组 17 个类。
我正在生成文本多标签分类的配置：
python -m spacy init config /path/to/config.cfg --pipeline textcat_multilabel --优化精度 --force --gpu

模型经过以下训练：
python -m spacy train /path/to/config.cfg --paths.train /path/to/data/train.spacy --paths.dev /path/to/data/test.spacy - -输出/路径/到/输出--gpu-id 0

我遇到的一个问题是我尝试分类的文本有时可能包含非英语单词，例如“arraigo”用于训练的静态向量中没有向量。因此，如果我有一个使用“arraigo”的句子或类似的这些不能很好地分类，因为我相信词向量全为零。
我发现您可以在训练时通过添加 --paths.vectors 和向量路径来使用自定义词向量，但是我找不到任何有关如何生成自定义向量的信息非常具体的词，例如“arraigo”。有什么推荐吗？]]></description>
      <guid>https://stackoverflow.com/questions/78373465/generate-and-add-new-word-vectors-when-training-spacy-model</guid>
      <pubDate>Tue, 23 Apr 2024 15:19:43 GMT</pubDate>
    </item>
    <item>
      <title>商业提案生成器[关闭]</title>
      <link>https://stackoverflow.com/questions/78373183/businuss-proposal-generator</link>
      <description><![CDATA[我想微调或者使用嵌入来训练 gpt 为各种客户制定业务提案。我希望将招标或合同以及来自企业的信息（如名称、描述等）作为输入，并让它以特定格式输出提案。我该怎么办？
不知道如何开始]]></description>
      <guid>https://stackoverflow.com/questions/78373183/businuss-proposal-generator</guid>
      <pubDate>Tue, 23 Apr 2024 14:34:55 GMT</pubDate>
    </item>
    <item>
      <title>多级数据的机器学习分类算法</title>
      <link>https://stackoverflow.com/questions/78373040/machine-learning-classification-algorithms-for-multi-level-data</link>
      <description><![CDATA[我正在开发一个机器学习项目，我的数据集包含 1960 年到 2022 年 218 个国家/地区的社会、人口和经济方面的变量。目标变量是一个二元变量（是或否），表示如果该国在某一年至少发生过一次政变企图。
我的问题是：多级数据的最佳分类模型是什么？
通过咨询不同的来源，我写下了这些模型（没有特定的顺序）：

随机森林
XGBoost
物流分类
决策树

他们是不是错了？还有更多我不知道的型号吗？
如果没有，您知道我可以使用哪些资源在 R 中实现这些模型吗？]]></description>
      <guid>https://stackoverflow.com/questions/78373040/machine-learning-classification-algorithms-for-multi-level-data</guid>
      <pubDate>Tue, 23 Apr 2024 14:14:28 GMT</pubDate>
    </item>
    <item>
      <title>Autodistill GroundedSAM 卡在标记第一张图像上？</title>
      <link>https://stackoverflow.com/questions/78370149/autodistill-groundedsam-stuck-on-labeling-first-image</link>
      <description><![CDATA[我正在尝试标记包含 200,000 张飞机图像的数据集。我正在使用自动蒸馏和 GroundedSAM 来尝试品尝。它已经卡在 0/219670 有一段时间了，速度为 0it/s
这是我的代码：
from autodistill_grounded_sam import GroundedSAM
从 autodistill.detection 导入 CaptionOntology
从 autodistill_yolov8 导入 YOLOv8

image_dir = r&#39;C:\Users\Colter\Desktop\plane_ detector\dataset\all_images&#39;
base_model = GroundedSAM(ontology=CaptionOntology({“飞机”: “飞机”}))
base_model.label(image_dir, 扩展名=“.jpg”)

我的输出在所附图像中： 
我希望进度条能慢慢增加，并希望程序能够标记我的所有图像以及图片中飞机的位置，但它卡在 0 上。]]></description>
      <guid>https://stackoverflow.com/questions/78370149/autodistill-groundedsam-stuck-on-labeling-first-image</guid>
      <pubDate>Tue, 23 Apr 2024 05:58:47 GMT</pubDate>
    </item>
    <item>
      <title>UnicodeEncodeError：“charmap”编解码器无法对位置 19-38 中的字符进行编码：字符映射到 <未定义></title>
      <link>https://stackoverflow.com/questions/78367946/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-19-38-c</link>
      <description><![CDATA[我正在开发一个基于 Flask 的 Web 应用程序，用户可以上传图像以使用机器学习模型进行预测。上传的图像存储在本地目录中，并使用预先训练的模型进行预测。然而，当我点击预测按钮时
是什么导致了这个 UnicodeEncodeError？
如何解决此问题以确保我的应用程序能够正确处理图像上传和预测？
是否有在 Flask 环境中处理字符编码的最佳实践，尤其是在 Windows 上？
==app.py====
@app.route(&#39;/uploadimage&#39;,methods=[&#39;GET&#39;, &#39;POST&#39;])
def upload_image():

        文件 = request.files[&#39;my_image&#39;]
        # 获取预测结果
        预测标签 = 预测标签(img_path)
        # 返回预测的标签和一条提示信息
        flash(f&quot;预测：{predicted_label}&quot;, &quot;成功&quot;)
        os.remove(img_path) # 处理后删除临时文件
    return render_template(&#39;uploadimage.html&#39;) # 对于 GET 请求，渲染表单


即使我设置了环境变量“UTF-8”，我仍然收到此错误
错误
文件“C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
从 None 引发 e.with_traceback(filtered_tb)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
文件“C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\encodings\cp1252.py”，第 19 行，编码
返回 codecs.charmap_encode(输入,self.errors,encoding_table)[0]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^
UnicodeEncodeError：“charmap”编解码器无法对位置 19-38 中的字符进行编码：字符映射为未定义。
============
即使我有一个最简单的代码来测试编码
标题是“要测试您的控制台是否可以处理 UTF-8，请尝试输出带有特殊字符或 Unicode 字符的文本：”
print(&quot;UTF-8 测试: àéîöü — 中文 — 阿拉伯语&quot;)


错误也相同
print(&quot;UTF-8 测试：����� � \u4e2d\u6587 � \u0627\u0644\u0639\u0631\u0628\u064a\u0629&quot;)
文件“C:\Users\Subha\AppData\Local\Programs\Python\Python311\Lib\encodings\cp1252.py”，第 19 行，编码
返回 codecs.charmap_encode(输入,self.errors,encoding_table)[0]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^
UnicodeEncodeError：“charmap”编解码器无法对位置 20-21 中的字符进行编码：字符映射到 ]]></description>
      <guid>https://stackoverflow.com/questions/78367946/unicodeencodeerror-charmap-codec-cant-encode-characters-in-position-19-38-c</guid>
      <pubDate>Mon, 22 Apr 2024 17:25:20 GMT</pubDate>
    </item>
    <item>
      <title>关于Keras历史回调损失与控制台输出损失不匹配的调查</title>
      <link>https://stackoverflow.com/questions/78360208/investigation-about-keras-history-callback-loss-not-matching-with-console-output</link>
      <description><![CDATA[请问，有谁知道为什么这个问题中描述了这个问题（Keras历史回调损失与损失的控制台输出不匹配）会发生吗？这个问题只有一个答案，它指的是可能的 TensorFlow 版本错误，但我不相信这一点，特别是因为 OP 没有对答案发表评论。我也遇到了这种情况，使用 Keras 指南中的 LossAndErrorPrintingCallback(keras.callbacks.Callback) 类和 def function on_epoch_end(self, epoch, logs=None) 函数 &lt; a href=&quot;https://keras.io/guides/writing_your_own_callbacks/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://keras.io/guides/writing_your_own_callbacks/。我还测试了 CSVLogger Keras 回调的使用，并且得到的结果与 model.fit() 输出中显示的结果不同。我使用的是 TensorFlow 2.4.1 版本。]]></description>
      <guid>https://stackoverflow.com/questions/78360208/investigation-about-keras-history-callback-loss-not-matching-with-console-output</guid>
      <pubDate>Sun, 21 Apr 2024 02:09:48 GMT</pubDate>
    </item>
    <item>
      <title>多类问题的层次分类方法</title>
      <link>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</link>
      <description><![CDATA[有一个多类分类任务。我的目标是使用每父节点本地分类器 (LCPN) 方法来解决这个问题。
让我解释一下如何使用 MWE。
假设我有这个虚拟数据集：
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification
从 scipy.cluster 导入层次结构

X, y = make_classification(n_samples=1000, n_features=10, n_classes=5,
                             n_信息=4）

我想出了这些类之间的距离矩阵：
d = np.array(
[[ 0.、201.537、197.294、200.823、194.517]、
 [201.537, 0., 199.449, 202.941, 196.703],
 [197.294, 199.449, 0., 198.728, 192.354],
 [200.823, 202.941, 198.728, 0., 195.972],
[[194.517, 196.703, 192.354, 195.972, 0.]]
）

因此，我确定了类层次结构，如下所示：
hc = hierarchy.linkage(d, method=&#39;complete&#39;)

得到的树状图如下：
dendrogram = hierarchy.dendrogram(hc, labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;, &#39;D&#39;, &#39;F&#39;])
树状图


我使用hierarchy.to_tree()以树状结构进行说明：

我的问题：
如何按照 LCPN 方法在每个内部节点（包括根）处安装分类器，例如 DecisionTreeClassifier 或 SVM，以像在树中一样进行上图？]]></description>
      <guid>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</guid>
      <pubDate>Sat, 20 Apr 2024 14:08:05 GMT</pubDate>
    </item>
    <item>
      <title>从 torchensemble 中的基本模型获取嵌入</title>
      <link>https://stackoverflow.com/questions/78355585/getting-embeddings-from-the-base-model-in-torchensemble</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78355585/getting-embeddings-from-the-base-model-in-torchensemble</guid>
      <pubDate>Fri, 19 Apr 2024 18:25:20 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的有效张量乘法</title>
      <link>https://stackoverflow.com/questions/78330216/effective-tensor-multiplication-in-pytorch</link>
      <description><![CDATA[有谁知道我如何有效地计算两个张量乘法 - 例如，我有两个形状为 (15, 256) 和 (112, 256) 的张量，它们的乘积为形状为 (15, 112) 的张量可以是以7微秒计算。但是，如果我有像 A - (15, 100, 256) 和 B - (112, 2000, 256) 这样的张量，并且我会做出像 C = (A.reshape(-1, 256) @ B.reshape(256, - 1).reshape(15, 100, 112, 2000).permute(0, 2, 1, 3).max(-1).values.sum(-1) 得到形状为(15, 112)的张量，需要 1000 倍的时间才能计算得更快吗？
我知道第二个计算应该比第一个计算大得多，但也许它需要的时间比我的实现要少]]></description>
      <guid>https://stackoverflow.com/questions/78330216/effective-tensor-multiplication-in-pytorch</guid>
      <pubDate>Mon, 15 Apr 2024 17:49:57 GMT</pubDate>
    </item>
    <item>
      <title>ConnectionAbortedError：W&B 超参数搜索期间 [WinError 10053]</title>
      <link>https://stackoverflow.com/questions/76143500/connectionabortederror-winerror-10053-during-wb-hyperparameter-search</link>
      <description><![CDATA[我正在尝试使用权重和参数进行超参数搜索。偏见。尝试从文档运行此代码 https://笔记本中的 /docs.wandb.ai/guides/sweeps/add-w-and-b-to-your-code：
导入wandb
将 numpy 导入为 np
随机导入

# 定义扫描配置
扫描配置= {
    &#39;方法&#39;：&#39;随机&#39;，
    &#39;名称&#39;: &#39;扫一扫&#39;,
    &#39;metric&#39;: {&#39;goal&#39;: &#39;最大化&#39;, &#39;name&#39;: &#39;val_acc&#39;},
    &#39;参数&#39;：
    {
        &#39;batch_size&#39;: {&#39;values&#39;: [16, 32, 64]},
        &#39;纪元&#39;: {&#39;值&#39;: [5, 10, 15]},
        &#39;lr&#39;: {&#39;最大&#39;: 0.1, &#39;最小&#39;: 0.0001}
    }
 }

 # 通过传入config来初始化sweep。
 # （可选）提供项目名称。
 scan_id = wandb.sweep(
 扫描=扫描配置，
     项目=&#39;我的第一次扫描&#39;
 ）

 # 定义接受超参数的训练函数
 # 来自 `wandb.config` 的值并使用它们来训练
 # 模型和返回指标
 def train_one_epoch(epoch, lr, bs):
     acc = 0.25 + ((epoch/30) + (random.random()/10))
     损失 = 0.2 + (1 - ((epoch-1)/10 + random.random()/5))
     返回 acc, 损失

 defvaluate_one_epoch（纪元）：
     acc = 0.1 + ((epoch/20) + (random.random()/10))
     损失 = 0.25 + (1 - ((epoch-1)/10 + random.random()/6))
     返回 acc, 损失

 def main():
     运行=wandb.init()

     # 请注意，我们从 `wandb.config` 定义值
     # 而不是定义硬值
     lr = wandb.config.lr
     bs = wandb.config.batch_size
     纪元 = wandb.config.epochs

     对于 np.arange(1, epochs) 中的纪元：
         train_acc, train_loss = train_one_epoch(epoch, lr, bs)
         val_acc, val_loss = evaluate_one_epoch(epoch)

     wandb.log({
         &#39;纪元&#39;：纪元，
         &#39;train_acc&#39;：train_acc，
         &#39;火车损失&#39;：火车损失，
         &#39;val_acc&#39;: val_acc,
         &#39;val_loss&#39;：val_loss
     })

# 开始清扫工作。
wandb.agent(sweep_id, function=main, count=4)

出现错误ConnectionAbortedError：[WinError 10053]已建立的连接被主机中的软件中止。如何修复？当我在 Colab 中运行它时，一切正常。]]></description>
      <guid>https://stackoverflow.com/questions/76143500/connectionabortederror-winerror-10053-during-wb-hyperparameter-search</guid>
      <pubDate>Sun, 30 Apr 2023 20:54:46 GMT</pubDate>
    </item>
    </channel>
</rss>