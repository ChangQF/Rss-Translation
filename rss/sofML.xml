<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 22 Aug 2024 12:30:07 GMT</lastBuildDate>
    <item>
      <title>在 C# 中按相似性对文本文档进行分组</title>
      <link>https://stackoverflow.com/questions/78901288/grouping-text-documents-by-similarity-in-c-sharp</link>
      <description><![CDATA[我试图根据内容的相似程度对一些相对较短的文本文档（通常每个文档最多 500 个单词）进行分组。相似的文档通常在措辞上相当接近，但并不完全相同，因此我正在寻找一种机器学习方法来提供比简单地查找重复文本（即基于相似度的百分比进行分组）更宽松的匹配算法。但是，我对 ML 的经验有限，所以我正在寻找有关至少从哪里开始解决此类问题的信息。
当我使用 C# 工作时，我正在使用 ML.NET 寻找解决方案。我熟悉 K-Means，并且找到了一些关于使用它执行相似性匹配的信息，以及有关 ML.NET 的句子相似性 API的信息，但我不确定这些是否是正确的方向，因为 ML.NET 对这些的实现似乎需要一定数量的监督训练数据，而不是能够完全不受监督地对一组数据进行工作。
我理解，为了让代码理解什么是“相似文本”，可能需要一些预先的数据实际上看起来像，但我不清楚实际实现此类事情的最佳方法，也很难找到相关信息。我知道余弦相似度与此类问题有关，但 ML.NET 是应用它的正确库吗？有没有更简单的解决方案，特别是考虑到文档在文本和主题上已经相对接近？]]></description>
      <guid>https://stackoverflow.com/questions/78901288/grouping-text-documents-by-similarity-in-c-sharp</guid>
      <pubDate>Thu, 22 Aug 2024 11:21:52 GMT</pubDate>
    </item>
    <item>
      <title>使用弹性网络进行特征选择</title>
      <link>https://stackoverflow.com/questions/78901259/feature-selection-with-elastic-net</link>
      <description><![CDATA[我对机器学习中的特征选择还比较陌生。在了解到弹性网络往往比 Lasso 和 Ridge 回归表现更好（特别是因为它解决了多重共线性问题）后，我决定使用它进行特征选择。
我编写了一个代码，在嵌套交叉验证设置（具有 5 个内部折叠和 20 个外部折叠）中从最佳弹性网络模型（具体而言，具有实现最高 AUC 的 alpha 和 lambda 值的模型）中选择最佳特征。然后，我使用这些选定的特征进行进一步的模型训练。
但是，我不确定我是否正确实施了嵌套交叉验证。此外，当我尝试检索 alpha 和 lambda 值以报告它们的可重复性时，我似乎无法在结果中找到它们。
代码如下：
set.seed(1)
train_elastic_net &lt;- function(X_train, y_train, alphas, lambdas) {
best_auc &lt;- 0
best_model &lt;- NULL
best_features &lt;- NULL

for (alpha_value in alphas) {
# 设置交叉验证的训练控制
train_control &lt;- trainControl(method = &quot;cv&quot;, 
number = 5, 
classProbs = TRUE, 
summaryFunction = twoClassSummary)

# 定义调整网格
tune_grid &lt;- expand.grid(alpha = alpha_value, lambda = lambdas)

#使用 caret 训练模型
model &lt;- train(X_train, y_train,
method = &quot;glmnet&quot;,
trControl = train_control,
tuneGrid = tune_grid,
metric = &quot;ROC&quot;,
family = &quot;binomial&quot;)

# 获取最佳模型
best_lambda &lt;- model$bestTune$lambda
elastic_net_model &lt;- glmnet(as.matrix(X_train), y_train, alpha = alpha_value, lambda = best_lambda, family = &quot;binomial&quot;)

# 获取最佳模型的 AUC
auc &lt;- max(model$results$ROC)

# 如果当前模型更好，则更新最佳模型
if (auc &gt; best_auc) {
best_auc &lt;- auc
best_model &lt;- elastic_net_model
coef_elastic_net &lt;- coef(best_model, s = best_lambda)
selected_features &lt;- which(coef_elastic_net != 0) - 1 # 获取非零系数的索引
selected_features &lt;- selected_features[selected_features != 0] # 删除截距索引
best_features &lt;- selected_features
}
}

list(model = best_model, features = best_features)
}

y_train_factor &lt;- as.factor(y_train_total)
X_train_matrix &lt;- as.matrix(X_train_total)

# 定义要搜索的 alpha 和 lambda 值
alpha_values &lt;- seq(0.1, 1, by = 0.1)
lambda_values &lt;- 10^seq(-4, 1, length = 100)

# 为每个分割执行 20 次重复
n_repeats &lt;- 20
selected_features_list &lt;- list()

for (i in 1:n_repeats) {

# 使用弹性网络执行变量选择
result &lt;- train_elastic_net(X_train_matrix, y_train_factor, alpha_values, lambda_values)
best_model &lt;- result$model
best_features &lt;- result$features

# 存储此重复的选定特征
selected_features_list[[i]] &lt;- best_features
}
selected_features_list
# 聚合所有重复的选定特征
selected_features_final &lt;- Reduce(intersect, selected_features_list)
selected_features_final
# 使用最终选定的特征对训练数据进行子集化
X_train_total &lt;- X_train_total[, selected_features_final]
]]></description>
      <guid>https://stackoverflow.com/questions/78901259/feature-selection-with-elastic-net</guid>
      <pubDate>Thu, 22 Aug 2024 11:14:15 GMT</pubDate>
    </item>
    <item>
      <title>您能否建议最佳的 AI 模型来为数据库创建分类器</title>
      <link>https://stackoverflow.com/questions/78901133/can-you-suggest-best-ai-model-to-create-a-classifier-for-a-database</link>
      <description><![CDATA[为数据库创建分类器的最佳解决方案是什么。以下示例：
表 1：

|狗|猫|日期|时间|
|德国|波斯|12-09-2019|12:00:00|
|贵宾犬|布娃娃|10-08-2022|01:09:11|
|斗牛犬|缅甸猫|09-01-1999|03:00:09|
模型的输入应为“德国”，预测应为“狗”
我尝试了 NER，但我也需要概率。示例：
输入：德语
预期输出：狗：98%
猫：10%
日期：2%
时间：1%]]></description>
      <guid>https://stackoverflow.com/questions/78901133/can-you-suggest-best-ai-model-to-create-a-classifier-for-a-database</guid>
      <pubDate>Thu, 22 Aug 2024 10:47:45 GMT</pubDate>
    </item>
    <item>
      <title>使用定制的头部来微调语言模型</title>
      <link>https://stackoverflow.com/questions/78901109/finetuning-a-language-model-with-a-customized-head</link>
      <description><![CDATA[我正在微调 BERT 模型来执行一项相当复杂的任务。它应该识别输入字符串中的目标，对每个目标执行回归并输出

输入中每个已识别目标的开始/结束索引
每个目标的回归值

采用 JSON 格式。
据我所知，训练本身非常简单，但要以所需的格式获得输出（JSON 格式的多个输出），我需要自定义模型的头部。
所以，我并不是真的在这里寻找解决方案，而是寻找有关如何做到这一点的资源（教程、论文、视频）。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78901109/finetuning-a-language-model-with-a-customized-head</guid>
      <pubDate>Thu, 22 Aug 2024 10:41:28 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Azure 上自动对使用 ModelBuilder 创建的机器学习模型进行模型训练</title>
      <link>https://stackoverflow.com/questions/78900913/how-to-automate-model-training-on-azure-for-machine-learning-models-created-with</link>
      <description><![CDATA[我们使用 Visual Studio Model Builder 创建了多个机器学习模型，因为它非常易于使用。
某些类型的模型（例如对象检测）只能在 Azure 上进行训练才能产生可​​用的结果。
当有新数据可用时，我想自动完成模型训练过程。
ModelBuilder 使用方法“RetrainModel”为我生成一个训练文件。但是，我不知道如何将必要的配置传递给此方法以在 Azure 上运行此作业。
如何从我的应用程序中自动完成模型训练过程？我希望执行与单击“开始训练”按钮时 Model Builder 基本相同的操作。
我知道，当我在 Azure 上进行训练时，会自动创建一个 Azure 机器学习作业，然后输出所需的模型。当我使用 Azure 机器学习和 Python 从头开始​​训练模型时，创建的模型类型为“MLFlow”，但我需要类型为“MLNet”或“ONNX”的模型供 ML.NET 本地使用。有人能指出连接这两个世界的缺失部分吗？]]></description>
      <guid>https://stackoverflow.com/questions/78900913/how-to-automate-model-training-on-azure-for-machine-learning-models-created-with</guid>
      <pubDate>Thu, 22 Aug 2024 09:55:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在 jupyter 笔记本中摆脱“PineconeApiException：（400）”和“ValueError：在您的 Pinecone 项目中未找到索引‘None’”？</title>
      <link>https://stackoverflow.com/questions/78900691/how-to-get-rid-of-pineconeapiexception-400-and-valueerror-index-none-no</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78900691/how-to-get-rid-of-pineconeapiexception-400-and-valueerror-index-none-no</guid>
      <pubDate>Thu, 22 Aug 2024 09:07:42 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能正确地连接不同的功能？</title>
      <link>https://stackoverflow.com/questions/78900219/how-can-i-properly-conctenate-the-different-features</link>
      <description><![CDATA[我有一个特征图，形状为 (100,48)，其他特征的形状为 (100,1)
我该如何正确地连接这些不同的特征？
我想这样做是因为我现在正在训练 XGboost 模型，但如果我将不同的特征直接连接在一起，机器就无法区分特征图和单个特征。
我尝试了下面的代码，但我认为这不是正确的方法，每次我操作代码（没有随机种子）时，这样做的结果都不同。
input_1_layer = Input(shape=(input_1.shape[1],)) density_1 = Dense(48,activation=&#39;relu&#39;)(input_1_layer)

input_2_layer = Input(shape=(input_2.shape[1],)) density_2 = Dense(6,激活=&#39;relu&#39;)(输入层 2)

feature_extractor = 模型(输入=[输入层 1, 输入层 2], 输出=合并)

new_features = feature_extractor.predict([输入层 1, 输入层 2])

如果您找到任何论文，都可以提供给我。非常感谢。]]></description>
      <guid>https://stackoverflow.com/questions/78900219/how-can-i-properly-conctenate-the-different-features</guid>
      <pubDate>Thu, 22 Aug 2024 07:19:22 GMT</pubDate>
    </item>
    <item>
      <title>用于手语的 LSTM [关闭]</title>
      <link>https://stackoverflow.com/questions/78899773/lstm-for-sign-language</link>
      <description><![CDATA[实时模型是否可以转换为预先录制的视频上传？
例如，我将训练一个用于手语的 LSTM 模型，以实现实时识别。现在我想将其集成到移动应用程序中，这样我就可以随身携带手机，因此预先录制的视频会更好。]]></description>
      <guid>https://stackoverflow.com/questions/78899773/lstm-for-sign-language</guid>
      <pubDate>Thu, 22 Aug 2024 04:16:56 GMT</pubDate>
    </item>
    <item>
      <title>即使 training=False，Tensorflow 模型仍可进行训练</title>
      <link>https://stackoverflow.com/questions/78898892/tensorflow-model-still-trains-even-with-training-false</link>
      <description><![CDATA[以下是简单的重现代码
import tensorflow as tf
import tensorflow.keras as keras

inp = keras.Input((3, 3))
layer = keras.layers.Dense(1)
tar = layer(inp)

tar2 = layer(inp, training=False)
model2 = keras.Model(inp, tar2)
model2.compile(loss=&#39;mse&#39;, optimizer=keras.optimizers.Adam(0.01))

# fit
a = tf.random.normal((1, 3, 3))
b = tf.random.normal((1, 3, 1))

model2.fit(a, b)

如果您在之前/之后检查 model2.trainable_variables训练过程中，您可以轻松检查 model2 的参数是否已更改，这意味着它已完成训练。
我该怎么做才能在训练期间不在特定时间更新特定层？
我需要这样做的原因是，我的方案是重用我之前制作的一些层，并且我不希望在仍然更新模型中的其他层时再次训练这些层。如下所示：
inp1 = keras.Input((2, 3))
inp2 = keras.Input((4, 3))

layer1 = keras.layers.Dense(1)
intermediate_output1 = layer1(inp1)
intermediate_output2 = layer1(inp2, training=False) # 我不想为 inp2 再次训练该层。

已添加 = tf.concat([intermediate_output1, middle_output2], axis=1)
layer2 = keras.layers.Dense(2)
final_output = layer2(已添加)
final_output.shape # [无, 6, 4]
]]></description>
      <guid>https://stackoverflow.com/questions/78898892/tensorflow-model-still-trains-even-with-training-false</guid>
      <pubDate>Wed, 21 Aug 2024 20:28:32 GMT</pubDate>
    </item>
    <item>
      <title>尝试加载 hydra 的配置时出现问题</title>
      <link>https://stackoverflow.com/questions/78896800/problem-when-trying-to-load-the-config-of-hydra</link>
      <description><![CDATA[我正在 google collab 中运行 python 脚本。当我执行此操作时，我收到配置错误，但我不确定如何修复它
代码片段：
import hydra
from pathlib import Path # 导入文件路径处理的路径
import sys # 导入 sys 模块

@hydra.main(config_path=&quot;cfgs&quot;, config_name=&quot;config.yaml&quot;)
def main(cfg):
print(cfg) # 打印解析的配置
from train import Workspace as W
root_dir = Path.cwd()

working = W(cfg)

snap = root_dir / &#39;snapshot.pt&#39;

if snap.exists():
print(f&#39;resuming: {snapshot}&#39;)
working.load_snapshot()

working.train()

if __name__ == &#39;__main__&#39;:
main() 

输出我得到的是：
usage: colab_kernel_launcher.py [--help] [--hydra-help] [-- 
version] [--cfg {job,hydra,all}]
[--resolve] [--package PACKAGE] [-- 
run] [--multirun]
[--shell-completion] [--config-path 
CONFIG_PATH]
[--config-name CONFIG_NAME] [--config- 
dir CONFIG_DIR]
[--info 
[{all,config,defaults,defaults-tree,plugins,searchpath}]]
[overrides ...]
colab_kernel_launcher.py：错误：无法识别的参数：-f
发生异常，使用 %tb 查看完整回溯。
SystemExit：2
]]></description>
      <guid>https://stackoverflow.com/questions/78896800/problem-when-trying-to-load-the-config-of-hydra</guid>
      <pubDate>Wed, 21 Aug 2024 11:54:28 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow load_model()'charmap'编解码器无法对位置 18-37 的字符进行编码：字符映射到 <undefined> 错误 [重复]</title>
      <link>https://stackoverflow.com/questions/78894108/tensorflow-load-model-charmap-codec-cant-encode-characters-in-position-18-3</link>
      <description><![CDATA[我正在尝试使用 django 连接一个 ml 模型。在这里我已加载模型和必要的编码器。在这里我已使用 tensorflow 加载模型。但是当尝试预测输出时，它会抛出此错误
import joblib
import os
#from keras.model import load_model
from keras.src.saving.saving_api import load_model
from django.conf import settings
import numpy as np

def load_keras_model():
# 定义模型文件的路径
model_path = os.path.join(settings.BASE_DIR, &#39;Ml_Models&#39;, &#39;football_prediction_model.h5&#39;)
print(&quot;Keras model path:&quot;, model_path)

try:
# 加载模型
model1 = load_model(model_path)
# 通过打印其摘要来验证模型加载
print(&quot;模型已成功加载。&quot;)
print(&quot;模型摘要：&quot;)
model1.summary()
return model1

except Exception as e:
# 处理异常并打印错误消息
print(f&quot;加载模型时出错：{str(e)}&quot;)
return None

def load_encoder(filename):
coder_path = os.path.join(settings.BASE_DIR, &#39;Ml_Models&#39;, filename)
print(f&quot;{filename} path:&quot;,coder_path) # 调试路径
return joblib.load(encoder_path)

# 加载所有必要的模型和编码器
model = load_keras_model()
team_label_encoder = load_encoder(&#39;team_label_encoder.pkl&#39;)
outcome_label_encoder = load_encoder(&#39;outcome_label_encoder.pkl&#39;)
scaler = load_encoder(&#39;scaler.pkl&#39;)

def predict_outcome(home_team,away_team,year,month,day,temperature):
try:
print(f&quot;主队： {home_team}&quot;)
print(f&quot;客队：{away_team}&quot;)
print(f&quot;年份：{year}, 月份：{month}, 日：{day}, 温度：{temperature}&quot;)
# 对输入数据进行编码和缩放
home_team_encoded = team_label_encoder.transform([home_team])[0]
away_team_encoded = team_label_encoder.transform([away_team])[0]
temperature_scaled = scaler.transform([[temperature]])[0][0]

print(f&quot;编码的主队：{home_team_encoded}&quot;)
print(f&quot;编码的客队：{away_team_encoded}&quot;)
print(f&quot;缩放的温度：{temperature_scaled}&quot;)

# 为模型准备输入
input_data = np.array([[home_team_encoded, away_team_encoded, year, month, day,temperature_scaled]])
print(f&quot;输入日期：{input_data}&quot;)
input_data = input_data.reshape((1, 1, 6))
print(f&quot;输入更新日期：{input_data}&quot;)

# 进行预测
prediction = model.predict(input_data)
print(f&quot;预测：{prediction}&quot;)
consequence_index = np.argmax(prediction)
print(f&quot;结果索引：{outcome_index}&quot;)

# 将预测映射回原始结果标签
consequence_label = consequence_label_encoder.inverse_transform([outcome_index])
print(f&quot;输出标签：{outcome_label}&quot;)

return consequence_label[0]

except ValueError as e:
return f&quot;Error: {str(e)}&quot;

home_team = &#39;Scotland&#39;
away_team = &#39;England&#39;
year = 2024
month = 8
day = 20
temperature = 25

predicted_outcome = predict_outcome(home_team, away_team, year, month, day,temperature)
print(f&quot;Predicted Outcome: {predicted_outcome}&quot;)

对于上述代码，以下是输出。请注意，我在控制台中包含了部分输出。
主队：苏格兰
客队：英格兰
年份：2024，月份：8，日期：20，温度：25
D:\My Projects\FootBall-Match-Win-Prediction\BackEnd\venv\Lib\site-packages\sklearn\base.py:465：UserWarning：X 没有有效的特征名称，但 MinMaxScaler 配备了特征名称
warnings.warn(
编码的主队：3
编码的客队：1
缩放温度：0.75
输入日期：[[3.000e+00 1.000e+00 2.024e+03 8.000e+00 2.000e+01 7.500e-01]]
输入更新日期：[[[3.000e+00 1.000e+00 2.024e+03 8.000e+00 2.000e+01 7.500e-01]]]

预测结果：错误：“charmap”编解码器无法对位置 18-37 的字符进行编码：
字符映射到 &lt;undefined&gt;

系统检查未发现任何问题（0 静音）。
2024 年 8 月 21 日 - 00:30:52
Django 版本 5.1，使用设置“BackEnd.settings”
在 http://localhost:8000/ 启动开发服务器
使用 CTRL-BREAK 退出服务器。

对于它打印的 predicted_outcome 变量
错误：“charmap”编解码器无法对位置的字符进行编码18-37：
字符映射到 &lt;undefined&gt;。

如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78894108/tensorflow-load-model-charmap-codec-cant-encode-characters-in-position-18-3</guid>
      <pubDate>Tue, 20 Aug 2024 19:16:51 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试训练 TensorFlow 模型时，出现“ValueError”</title>
      <link>https://stackoverflow.com/questions/78881211/i-get-a-valueerror-when-i-try-to-train-my-tensorflow-model</link>
      <description><![CDATA[这是我在第一个 epoch 调用 model.fit 时遇到的错误：
发生异常：ValueError
层“ functional”需要 2 个输入，但它收到了 1 个输入张量。收到的输入：[&lt;tf.Tensor &#39;data:0&#39; shape=(None, 128) dtype=float32&gt;]
文件 &quot;D:\workspace\Machine Learning 545\PSU_classes\cs445_group_project\code\Keras Music Genres Classification\encoder_decoder_feature_extractor.py&quot;，第 177 行，位于 train_encoder_decoder_model
model.fit(x = X_train,
文件 &quot;D:\workspace\Machine Learning 545\PSU_classes\cs445_group_project\code\Keras Music Genres Classification\encoder_decoder_feature_extractor.py&quot;，第 217 行，位于 &lt;module&gt;
train_encoder_decoder_model = train_encoder_decoder_model(encoder_decoder_model, X_train, y_train, X_test, y_test)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: 层“ functional”需要 2 个输入，但收到 1 个输入张量。收到的输入：[&lt;tf.Tensor &#39;data:0&#39; shape=(None, 128) dtype=float32&gt;]&quot;

这是我的模型：
def define_encoder_decoder_model(num_features):
# 定义编码器
coder_inputs = 输入（shape=(None, num_features))
coder_hidden1 = Dense(100, 激活=&#39;relu&#39;)(encoder_inputs)
coder_hidden2 = Dense(50, 激活=&#39;relu&#39;)(encoder_hidden1)
coder_lstm = LSTM(25, return_state=True)
coder_outputs, state_h, state_c =coder_lstm(encoder_hidden2)
coder_states = [state_h, state_c]

# 定义解码器
decoder_inputs = 输入(shape=(None, 25))
decoder_hidden1 = Dense(50, 激活=&#39;relu&#39;)(decoder_inputs)
decoder_hidden2 = Dense(100, 激活=&#39;relu&#39;)(decoder_hidden1)
decoder_lstm = LSTM(num_features, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decrypt_lstm(decoder_hidden2, initial_state=encoder_states)
decoder_dense = Dense(num_features,activation=&#39;softmax&#39;)
decoder_outputs =coder_dense(decoder_outputs)

# 定义将encoder_inputs和decoder_inputs转换为decoder_outputs的模型
model = Model([encoder_inputs,decoder_inputs],decoder_outputs)

# 编译模型
model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,
metrics=[&#39;accuracy&#39;,&#39;precision&#39;,&#39;recall&#39;,&#39;f1_score&#39;])

# 模型摘要
model.summary()

在此处返回modeltype

这是我调用model.fit的方式：
def train_encoder_decoder_model(model,X_train, y_train, X_test, y_test):
&quot;&quot;&quot;
使用提供的数据训练编码器-解码器模型。

参数：
model：要训练的编码器-解码器模型。
X_train：输入训练数据。
y_train：目标训练数据。
X_test：输入测试数据。
y_test：目标测试数据。

返回：
训练好的编码器-解码器模型。
&quot;&quot;&quot;
print(&quot;shapes: X_train:&quot;, np.shape(X_train),&quot; y_train: &quot;, np.shape(y_train),&quot; X_test: &quot;, np.shape(X_test),&quot; y_test: &quot;, np.shape(y_test))
# 训练模型
# 将每个时期的训练日志附加到文件中
file_logger = FileLogger(&#39;training.log&#39;)
y_train_T = tf.convert_to_tensor(np.array([y_train]).T)
y_test_T = tf.convert_to_tensor(np.array([y_test]).T)
#x_train = tf.convert_to_tensor(X_train)
y_train = X_train
y_test = X_test
model.fit(x = X_train,
y= y_train,
batch_size=100,
epochs=100,
verbose=2,
validation_data=(X_test, y_test),
callbacks=[file_logger])
返回模型

这是一个编码器-解码器模型，因此 X_train 数据集等于 y_train，X_test、y_test 也一样。在第一种情况下，训练集的形状为 (799,128)，测试数据集为 (299,128)。特征表示为“float64”值。
我在 Visual Studio Code 下运行代码。我将数据预处理为标准化和缩放的数据集，然后将其分为训练数据集和测试数据集，构建我的编码器-解码器模型（参见上面的方法），然后尝试训练模型。我得到的是 model.fit 的这个输出“Epoch 1/100”和上面显示的错误消息。
这个错误是什么以及如何解决？]]></description>
      <guid>https://stackoverflow.com/questions/78881211/i-get-a-valueerror-when-i-try-to-train-my-tensorflow-model</guid>
      <pubDate>Sat, 17 Aug 2024 01:20:22 GMT</pubDate>
    </item>
    <item>
      <title>检测群体中某个实例的行为变化（但不是整个群体）</title>
      <link>https://stackoverflow.com/questions/74706455/detecting-a-change-in-behavior-in-one-instance-of-a-group-but-not-the-group-as</link>
      <description><![CDATA[我一直在阅读有关时间序列数据中的异常检测的文章，并了解如何使用它来跟踪一段时间内的指标。
例如，假设我们想要跟踪一个人每天使用网站的次数（例如 John）。我们可以使用异常检测来检测 John 的数据何时大幅上升或下降。我们将使用的指标是“John 的网站每日点击量”和日期。
但是，假设我想对许多用户进行同样的检查，但他们都是独立的。该算法并不是试图找到用户活动之间的相关性，而只是在组中的一个用户的活动发生显著变化时提醒我们。所以假设 John 的活动在某一天异常高，我们会收到异常警报。
另一个例子是监控大量设备并检测一台设备何时每分钟发送异常高水平的请求。再次强调，目的不是检测所有发送更多请求的设备之间的相关性，而是提醒我们一个设备的行为与其正常模式不同。
我不确定这是否是正常的异常检测，因为看起来我必须为第一个示例中的每个用户构建一个模型来检测变化。对于少数用户来说，这可能是可行的，但似乎很难扩展到大量用户。
所以我想知道异常检测是否是正确的方法，或者是否存在我不知道的其他 AI 监控解决方案/工具？]]></description>
      <guid>https://stackoverflow.com/questions/74706455/detecting-a-change-in-behavior-in-one-instance-of-a-group-but-not-the-group-as</guid>
      <pubDate>Tue, 06 Dec 2022 17:06:14 GMT</pubDate>
    </item>
    <item>
      <title>如何减少图像异常检测中的假阴性？</title>
      <link>https://stackoverflow.com/questions/71160069/how-to-reduce-false-negatives-in-image-anomaly-detection</link>
      <description><![CDATA[我正在从事一个质量检查项目，我需要开发一个可以检测不规则部件的程序。我面临的问题是我没有很多不规则样本（3,000 多个常规样本中只有 7 个）。我尝试使用 CNN，但由于样本数量不平衡，模型将所有样本检测为常规样本，因此我正在探索的方法是使用异常检测算法。我也尝试使用自动编码器，但由于常规和不规则之间的差异很小，我无法获得任何好的结果。到目前为止，给我带来最佳结果的方法是将局部离群值因子与特征提取器 (HOG) 结合使用。这种方法唯一的问题是，即使在调整算法的参数后，它仍然会给我误报（正常样本被标记为不规则），这对于此应用来说是不可接受的。
我可以在流程中添加什么来消除误报吗？或者您可以推荐我其他方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/71160069/how-to-reduce-false-negatives-in-image-anomaly-detection</guid>
      <pubDate>Thu, 17 Feb 2022 14:32:38 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的 `stack()` 与 `cat()`</title>
      <link>https://stackoverflow.com/questions/54307225/stack-vs-cat-in-pytorch</link>
      <description><![CDATA[OpenAI 的强化学习 REINFORCE 和 actor-critic 示例有以下代码：
REINFORCE：
policy_loss = torch.cat(policy_loss).sum()

actor-critic：
loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()

一个使用torch.cat，另一个使用torch.stack，用于类似的用例。
据我所知，文档没有明确区分它们。
我很高兴知道这些函数之间的区别。]]></description>
      <guid>https://stackoverflow.com/questions/54307225/stack-vs-cat-in-pytorch</guid>
      <pubDate>Tue, 22 Jan 2019 11:24:47 GMT</pubDate>
    </item>
    </channel>
</rss>