<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 21 Dec 2023 09:13:18 GMT</lastBuildDate>
    <item>
      <title>如何从具有一对多关系的数据集开始创建用于机器学习的数据集？</title>
      <link>https://stackoverflow.com/questions/77696639/how-do-i-create-a-dataset-for-machine-learning-starting-from-datasets-that-have</link>
      <description><![CDATA[我有一个数据集（我们称之为 dataset1，它是一个 3000x6 数据集），结构如下：
X1,X2,X3,X4,X5,观察索引
D,3,3,0.12,0.3,0
B,2,3,0.2,0.27,1
B,4,5,0.2,0.18,2
A,3,5,0.28,0.24,3
B,3,5,0.17,0.29,4
列“ObservationIndex”包含一个扩展的整数序列，用作包含许多行（准确地说是 191 行）的 CSV 文件的索引。
因此，对于 dataset1 中的每一行，都存在一个对应的包含 191 行的 CSV 文件。
每个对应的 191 行 csv 的结构如下：
head_id,tail_id,initial_condition,目标变量
152331933,152432928,假,假
152331933,152432917,假,假
152331933,152331936,假,假
152331936,152943327,假,假
目的是预测目标变量，可以是 1 或 0。
如何创建可输入机器学习模型的数据集？
我的想法围绕着需要以某种方式创建新特征以形成正确的数据集。但是这个目标变量应该是什么？我已经排除了将所有 CSV 文件合并在一起的可能性，因为这会生成巨大的数据集。但是，我不确定在使用这些新功能训练模型后，如何恢复到对原始目标变量进行分类的原始问题。
感谢所有阅读本文的人]]></description>
      <guid>https://stackoverflow.com/questions/77696639/how-do-i-create-a-dataset-for-machine-learning-starting-from-datasets-that-have</guid>
      <pubDate>Thu, 21 Dec 2023 08:31:21 GMT</pubDate>
    </item>
    <item>
      <title>Sagemaker 端点 - 创建 ML 指标和仪表板</title>
      <link>https://stackoverflow.com/questions/77696324/sagemaker-endpoints-creating-ml-metrics-and-dashboards</link>
      <description><![CDATA[我部署了一个 sagemaker 端点。该模型是我创建的自定义随机森林模型。
日志被推送到 Cloudwatch，我手动添加了一个日志，显示提取的特征和模型得分。
我的最终目标是创建一个仪表板，我可以在其中评估模型的稳定性并查找数据或模型漂移。
关于如何做到这一点有什么建议吗？
现在我有一个 jupyter 笔记本，它查询写入分数（仅分数）的 mongoDB，并让我了解正在发生的事情。
这是一个手动过程，因为我们有多个端点写入不同类型的数据库，所以最好的办法是将 cloudwatch 作为源。]]></description>
      <guid>https://stackoverflow.com/questions/77696324/sagemaker-endpoints-creating-ml-metrics-and-dashboards</guid>
      <pubDate>Thu, 21 Dec 2023 07:16:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 tf.GradientTape() 中的 stop_recording() 跳过某些数据的某些元素的反向传播</title>
      <link>https://stackoverflow.com/questions/77696138/skipping-backpropagation-for-certain-element-of-certain-data-with-stop-recording</link>
      <description><![CDATA[假设我有(4, 2)格式的数据，我的目的是预测三个0或1。（简单的sigmoid问题）
因为我知道这些数据应该用 CNN 处理，所以我将使用 keras.layers.Conv1D 等。
这是一个最小的示例。
&lt;前&gt;&lt;代码&gt;BATCH_SIZE = 2
Sample_inputs = keras.Input((4, 2), batch_size=BATCH_SIZE) # 形状: (BATCH_SIZE, 4, 2)
cnn_layer = keras.layers.Conv1D(3, kernel_size=2, use_bias=False) # 对于较少的可训练变量没有偏差。
cnn_outputs = cnn_layer(sample_inputs) # 形状: (BATCH_SIZE, 3, 3)
密集层 = keras.layers.Dense(1, use_bias=False)
最终输出=密集层（cnn_输出）#形状：（BATCH_SIZE，3，1）

dummy_model = keras.Model(sample_inputs, Final_outputs)

但我的问题是，我知道y_true的某些部分是错误的！
让我进一步说明，
train_x = tf.random.normal((BATCH_SIZE*5, 4, 2)) # 假设我们有 10 个数据
train_y = tf.reshape(tf.random.categorical(tf.math.log([[0.5, 0.5]]),
    num_samples=BATCH_SIZE*5*3*1, dtype=tf.int32), (BATCH_SIZE*5, 3, 1)) # 使值变为 0 或 1。

假设 train_y[2:3, :, :] 是一个张量 shape=(1, 3, 1), dtype=int32, array([[[1], [ 0], [0]]]).但我确信这个张量的第一个元素被污染了。因此，我不希望针对该元素反向传播我的梯度，但我仍然希望针对第二个和第三个元素训练 dummy_model 的 trainable_variables，因为每个数据都非常有价值。
问题1。我这样做在逻辑上正确吗？我仍然可以使用受污染模型的相应模型来计算 y_pred。我还可以跳过特定数据的特定元素的反向传播，当它不可避免时，对吧？
（也许这可能会导致一些偏斜或在某种意义上挑选数据，我知道）
问题2。张量流实现。
经过长时间的研究，我的策略是使用 tf.GradientTape() 的 stop_recording() 方法。
我会将受污染的部分标记（或编辑）为整数 100，并且每当循环注意到 y_batch 中存在 100 时，使用 &lt; code&gt;tf.where(y_batch == 100)，设置cond=True，让tf.GradientTape()照常做他所做的事情，直到然后，将 tf.UnconnectedGradients.ZERO 替换为受污染的，...
我知道这听起来很疯狂，甚至不确定它是否值得。但至少，我的想法是对的吗？]]></description>
      <guid>https://stackoverflow.com/questions/77696138/skipping-backpropagation-for-certain-element-of-certain-data-with-stop-recording</guid>
      <pubDate>Thu, 21 Dec 2023 06:31:10 GMT</pubDate>
    </item>
    <item>
      <title>随着时间的推移预测比例</title>
      <link>https://stackoverflow.com/questions/77695627/forecast-proportions-through-time</link>
      <description><![CDATA[我想预测 1-3-6 个月内阳性（样本检测呈阳性）的比例。我的数据有很多负样本测试，因此，它是按月汇总的。假设我有这些数据，它与我所拥有的数据类似。
导入 pandas 作为 pd
将 numpy 导入为 np

日期 = [&#39;2021-07&#39;, &#39;2014-10&#39;, &#39;2014-07&#39;, &#39;2015-04&#39;, &#39;2023-07&#39;, &#39;2006-08&#39;, &#39;2014-04&#39;, &#39;2022-02&#39; , &#39;2019-03&#39;, &#39;2016-04&#39;, &#39;2013-09&#39;, &#39;2013-05&#39;, &#39;2015-01&#39;, &#39;2014-03&#39;, &#39;2012-05&#39;, &#39;2021-08&#39;, &#39; 2016-11&#39;, &#39;2013-06&#39;, &#39;2020-05&#39;, &#39;2006-05&#39;, &#39;2019-06&#39;, &#39;2008-07&#39;, &#39;2017-11&#39;, &#39;2016-06&#39;, &#39;2005- 10&#39;, &#39;2009-04&#39;, &#39;2018-04&#39;, &#39;2005-12&#39;, &#39;2023-02&#39;, &#39;2013-04&#39;, &#39;2013-07&#39;, &#39;2015-09&#39;, &#39;2017-08&#39; , &#39;2012-02&#39;, &#39;2023-06&#39;, &#39;2020-07&#39;, &#39;2008-06&#39;, &#39;2007-09&#39;, &#39;2018-09&#39;, &#39;2015-07&#39;, &#39;2011-10&#39;, &#39; 2014-09&#39;, &#39;2021-09&#39;, &#39;2008-02&#39;, &#39;2009-10&#39;, &#39;2007-10&#39;, &#39;2016-05&#39;, &#39;2022-06&#39;, &#39;2008-09&#39;, &#39;2005- 09&#39;]
正数 = [2, 10, 13, 16, 7, 21, 9, 0, 10, 18, 11, 2, 2, 3, 3, 18, 14, 3, 20, 17, 18, 14, 9, 1 , 4, 10, 22, 11, 8, 11, 2, 19, 16, 0, 22, 0, 6, 19, 14, 10, 19, 8, 13, 2, 3, 2, 11, 13, 16 , 8]
总计 = [122, 69, 145, 122, 117, 111, 64, 146, 54, 189, 136, 171, 159, 125, 66, 199, 160, 75, 171, 168, 167, 133, 154, 171 , 120, 81, 63, 121, 129, 91, 68, 90, 61, 161, 143, 179, 168, 94, 175, 74, 117, 53, 85, 93, 82, 61, 154, 188, 175 ，161]
国家= [&#39;西班牙&#39;，&#39;巴西&#39;，&#39;墨西哥&#39;，&#39;英国&#39;，&#39;意大利&#39;，&#39;德国&#39;，&#39;墨西哥&#39;，&#39;阿根廷&#39;，&#39;德国&#39;，&#39;墨西哥&#39;，&#39;墨西哥&#39;，&#39;意大利&#39;, &#39;英国&#39;, &#39;英国&#39;, &#39;英国&#39;, &#39;意大利&#39;, &#39;墨西哥&#39;, &#39;巴西&#39;, &#39;墨西哥&#39;, &#39;英国&#39;, &#39;意大利&#39;, &#39;意大利&#39;, &#39;德国&#39;, &#39;加拿大&#39;、&#39;巴西&#39;、&#39;巴西&#39;、&#39;西班牙&#39;、&#39;加拿大&#39;、&#39;德国&#39;、&#39;西班牙&#39;、&#39;阿根廷&#39;、&#39;美国&#39;、&#39;巴西&#39;、&#39;德国&#39;、&#39;德国&#39;、&#39;美国”、“西班牙”、“加拿大”、“加拿大”、“美国”、“墨西哥”、“西班牙”、“西班牙”、“阿根廷”、“阿根廷”、“美国”、“英国” 、“墨西哥”、“意大利”、“墨西哥”]
food_groups = [&#39;香肠&#39;, &#39;即食&#39;, &#39;即食&#39;, &#39;香肠&#39;, &#39;即食&#39;, &#39;婴儿食品&#39;, &#39;香肠&#39;, &#39;婴儿食品&#39;, &#39;婴儿食品”、“香肠”、“意大利面”、“意大利面”、“即食食品”、“香肠”、“婴儿食品”、“意大利面”、“意大利面”、“即食食品”、“即食”、“即食”、“即食”、“香肠”、“肉类”、“面食”、“即食”、“婴儿食品”、“面食” &#39;, &#39;即食&#39;, &#39;即食&#39;, &#39;香肠&#39;, &#39;肉类&#39;, &#39;肉类&#39;, &#39;香肠&#39;, &#39;婴儿食品&#39;, &#39;肉类&#39;, &#39;婴儿食品&#39;, &#39;面食&#39;, &#39;意大利面&#39;, &#39;肉&#39;, &#39;肉&#39;, &#39;意大利面&#39;, &#39;意大利面&#39;, &#39;肉&#39;, &#39;意大利面&#39;, &#39;香肠&#39;, &#39;即食&#39;, &#39;即食&#39;, ‘香肠’、‘即食’、‘意大利面’]

df = pd.DataFrame({
    “日期”：日期，
    “积极”：积极的一面，
    “总计”：总计，
    “国家”：国家，
    &#39;食物组&#39;：食物组
})
df[“p”] = df[“正”] / df[“总计”]

df


我可以使用什么机器学习模型？我的数据高度不平衡，有什么想法可以解决这个问题吗？
感谢您的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/77695627/forecast-proportions-through-time</guid>
      <pubDate>Thu, 21 Dec 2023 03:41:39 GMT</pubDate>
    </item>
    <item>
      <title>在二元分类 ML 任务中处理日期列的最佳方法？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77695074/best-approaches-for-handling-date-columns-in-a-binary-classification-ml-task</link>
      <description><![CDATA[我有一个数据集，其中有多个数字、分类和日期列作为特征，以及一个只能有 2 个值的目标（是/否）。目标表示某个日期列的值是否发生事件。
据我所知，可以从日期中提取一些特征：

日（可以循环编码为 sin 和 cos 分量）
一周中的某一天（可以进行 one-hot 编码）
月份（可以循环编码为 sin 和 cos 分量）
年份（可以跳过，因为没有太多预测能力）
周末（是/否）
假期（当年特定国家/地区 - 是/否）
周数（可以循环编码为 sin 和 cos 分量）

是否有任何其他方法或功能可用于此类任务的日期列？另外，上述理解/方法是正确的还是我做错了什么？
此外，是否需要在标准化/归一化过程中包含正弦或余弦变换分量？]]></description>
      <guid>https://stackoverflow.com/questions/77695074/best-approaches-for-handling-date-columns-in-a-binary-classification-ml-task</guid>
      <pubDate>Thu, 21 Dec 2023 00:00:14 GMT</pubDate>
    </item>
    <item>
      <title>“4,240,240,160] 并在 /job:localhost/replica:0/task:0/device:GPU:0 上通过分配器 GPU_0_bfc 键入 float”，同时训练深度学习模型</title>
      <link>https://stackoverflow.com/questions/77694995/4-240-240-160-and-type-float-on-joblocalhost-replica0-task0-devicegpu0-b</link>
      <description><![CDATA[我正在使用 3D-Unet 架构、51GB RAM 和 16GB RAM 作为 GPU 的 Nvdia V100 对模型进行编程。这些是在 google colab 环境下的。
构成数据集的图像是nifti格式的MRI图像，大小为(1, 240, 240, 160, 1)。它们是灰度的，我使用 1 作为批量大小来查看它是否解决了问题（它没有）
这是我的代码：
def load_nifti_image(文件路径):
    nifti = nib.load(文件路径)
    体积 = nifti.get_fdata()
    返回量

＃  -  -  -  -  -  -  -  -  -  -  - -火车 -  -  -  -  -  -  -  -  -  -  - -
nifti_files = [os.path.join(“/content/drive/MyDrive/Interpolated/train/images”, f) for f in os.listdir(“/content/drive/MyDrive/Interpolated/train/images”) if f.endswith(&#39;.nii.gz&#39;)]
mask_files = [os.path.join(“/content/drive/MyDrive/Interpolated/train/masks”, f) for f in os.listdir(“/content/drive/MyDrive/Interpolated/train/masks”) if f.endswith(&#39;.nii.gz&#39;)]

nifti_images = [load_nifti_image(f) for f in nifti_files]
nifti_masks = [load_nifti_image(f) for f in mask_files]

Final_nifti_images = [np.expand_dims(image, axis=-1) 对于 nifti_images 中的图像]
Final_nifti_masks = [np.expand_dims(image, axis=-1) 对于 nifti_masks 中的图像]

数据集 = tf.data.Dataset.from_tensor_slices((final_nifti_images, Final_nifti_masks))

＃  -  -  -  -  -  -  -  -  -  -  - -验证 -  -  -  -  -  -  -  -  -  -  - -
nifti_files_val = [os.path.join(&quot;/content/drive/MyDrive/Interpolated/validation/images&quot;, f) for f in os.listdir(&quot;/content/drive/MyDrive/Interpolated/validation/images&quot;) if f.endswith(&#39;.nii.gz&#39;)]
mask_files_val = [os.path.join(“/content/drive/MyDrive/Interpolated/validation/masks”, f) for f in os.listdir(“/content/drive/MyDrive/Interpolated/validation/masks”) if f.endswith(&#39;.nii.gz&#39;)]

nifti_images_val = [load_nifti_image(f) for f in nifti_files_val]
nifti_masks_val = [load_nifti_image(f) for f in mask_files_val]

Final_nifti_images_val = [np.expand_dims(image, axis=-1) 对于 nifti_images_val 中的图像]
Final_nifti_masks_val = [np.expand_dims(image, axis=-1) for image in nifti_masks_val]

dataset_val = tf.data.Dataset.from_tensor_slices((final_nifti_images_val, Final_nifti_masks_val))

数据集 = 数据集.batch(1)
dataset_val = dataset_val.batch(1)

test_model.fit（数据集，validation_data=dataset_val，epochs=100）

我尝试将 GPU 的 RAM 限制为 14GB，也不起作用，并且还减小了批处理大小。请注意，我无法更改模型，因为我使用的是预定义的 3D-Unet 模型，我也会将其粘贴到此处，也许它会有所帮助：
&lt;前&gt;&lt;代码&gt;
# 卷积块
def conv_block(输入, num_filters):
    x = Conv3D(num_filters, (3, 3, 3), padding = “相同”)(输入)
    x = BatchNormalization()(x)
    x = 激活(“relu”)(x)

    x = Conv3D(num_filters, (3, 3, 3), 填充 = “相同”)(x)
    x = BatchNormalization()(x)
    x = 激活(“relu”)(x)

    返回x

# 编码器块
def编码器_块（输入，num_filters）：
    x = conv_block(输入, num_filters)
    p = MaxPool3D((2, 2, 2), 填充=“相同”)(x)
    返回 x, p

# 解码器块
def解码器_块（输入，跳过，num_filters）：
    x = Conv3DTranspose(num_filters, (2, 2, 2), strides=2, padding=“相同”)(输入)
    x = 连接()([x, 跳过])
    x = conv_block(x, num_filters)
    返回x

# 大学网络

def unet(输入形状):
    输入 = 输入（输入形状）

    “----编码器----”
    s1, p1 = 编码器_块(输入, 64)
    s2, p2 = 编码器_块(p1, 128)
    s3, p3 = 编码器_块(p2, 256)
    s4, p4 = 编码器_块(p3, 512)

    “----桥---”
    b1 = conv_block(p4, 1024)

    “----解码器----”
    d1 = 解码器_块(b1, s4, 512)
    d2 = 解码器_块(d1, s3, 256)
    d3 = 解码器块(d2, s2, 128)
    d4 = 解码器块(d3, s1, 64)

    输出 = Conv3D(1, 1, 填充 =“相同”, 激活 =“sigmoid”)(d4)

    模型=模型（输入，输出，名称=“UNET”）
    返回模型

输入形状 = (240, 240, 160, 1)

测试模型=unet(输入形状)
优化器 = Adam(learning_rate=0.0001)
test_model.compile（优化器=优化器，损失=dice_coefficient_loss，指标=[dice_coefficient]）

]]></description>
      <guid>https://stackoverflow.com/questions/77694995/4-240-240-160-and-type-float-on-joblocalhost-replica0-task0-devicegpu0-b</guid>
      <pubDate>Wed, 20 Dec 2023 23:31:18 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法让多元线性回归（和其他机器学习模型）预测 GeoPandas 多边形？</title>
      <link>https://stackoverflow.com/questions/77694882/is-there-a-way-to-have-a-multiple-linear-regression-and-other-machine-learning</link>
      <description><![CDATA[我有一个包含多边形的地理数据库，这些多边形代表（大致）1900 年至 2013 年之间加利福尼亚州野火的蔓延情况。我正在尝试创建一个线性回归模型，该模型可以采用前一年记录的温度（浮动）并预测给定这些温度下火灾将蔓延到的形状的多多边形。地理数据库在我的代码中存储为 GeoPandas GeoDataFrame。
我无法找到任何允许您训练模型来预测多边形的库或包。
我尝试过 Pysal：
train_X = fire_X.iloc[train].values
train_y = fire_y.iloc[train].values
test_X = fire_X.iloc[测试].values
test_y = fire_y.iloc[测试].values

模型 = spreg.OLS(y=train_y, x=train_X)

其中 fire_X 具有温度数据，fire_y 只是地理数据库中的 geometry 列，这导致
类型错误：+ 不支持的操作数类型：“MultiPolygon”和“MultiPolygon”
sklearn 回归也不起作用。这可能吗？]]></description>
      <guid>https://stackoverflow.com/questions/77694882/is-there-a-way-to-have-a-multiple-linear-regression-and-other-machine-learning</guid>
      <pubDate>Wed, 20 Dec 2023 22:55:17 GMT</pubDate>
    </item>
    <item>
      <title>在 Kaggle 上进行模型训练时如何使用多个 GPU</title>
      <link>https://stackoverflow.com/questions/77694839/how-can-i-use-multiple-gpus-during-model-training-on-kaggle</link>
      <description><![CDATA[在 Kaggle 上，我有 2 个 GPU T4，但我不明白如何在 Pytorch 中使用它们或调整代码以在 2 个 GPU 上进行训练
2 个 GPU 的图片
我的训练代码：
对于范围（2）中的纪元：

    运行损失 = 0.0
    对于 tqdm（数据集）中的数据：
        输入，标签=数据
        优化器.zero_grad()
        输出 = 模型（输入）
        损失=标准（输出，标签）
        loss.backward()
        优化器.step()

        running_loss += loss.item()
]]></description>
      <guid>https://stackoverflow.com/questions/77694839/how-can-i-use-multiple-gpus-during-model-training-on-kaggle</guid>
      <pubDate>Wed, 20 Dec 2023 22:41:19 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试找出如何检测聊天机器人用例的意图</title>
      <link>https://stackoverflow.com/questions/77693754/i-am-trying-to-figure-out-how-to-detect-intents-for-chatbot-use-cases</link>
      <description><![CDATA[我想构建一个聊天机器人，我可以在其中预定义一些意图以及 5-10 个可能的问题。聊天机器人应该能够检测给定用户查询的意图。
当我在网上查看时，大多数解决方案都指向针对意图分类任务微调预训练的语言模型。我不想在每次创建聊天机器人时都对模型进行微调，因为它很昂贵。有没有更简单的解决方案？]]></description>
      <guid>https://stackoverflow.com/questions/77693754/i-am-trying-to-figure-out-how-to-detect-intents-for-chatbot-use-cases</guid>
      <pubDate>Wed, 20 Dec 2023 18:29:57 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch中的动态量化量化后开始随机训练</title>
      <link>https://stackoverflow.com/questions/77692089/dynamic-quantization-in-pytorch-starts-random-training-after-quantization</link>
      <description><![CDATA[当我运行以下动态量化代码时，它开始使用一些随机自然图像进行 100 个时期的训练，我不想再次进行训练。我有预训练的权重，我只是想量化我的预训练的权重以减少推理时间：
从 ultralytics 导入 YOLO
进口火炬
导入火炬.量化

模型=YOLO(&#39;pre_trained_weights.pt&#39;)

model.load_state_dict(torch.load(&#39;checkpoint.pth&#39;)) #不知道这一步是否必要

qmodel = torch.quantization.quantize_dynamic(模型, dtype = torch.quint8)

我尝试了上面的代码，我希望我只是想量化我的预训练权重以减少推理时间]]></description>
      <guid>https://stackoverflow.com/questions/77692089/dynamic-quantization-in-pytorch-starts-random-training-after-quantization</guid>
      <pubDate>Wed, 20 Dec 2023 13:53:49 GMT</pubDate>
    </item>
    <item>
      <title>AutoTrain 高级 CLI：错误：无法识别的参数：--fp16 --use-int4</title>
      <link>https://stackoverflow.com/questions/77664921/autotrain-advanced-cli-error-unrecognized-arguments-fp16-use-int4</link>
      <description><![CDATA[我目前在使用提供的自动训练工具在 Colab 笔记本中使用 LLM 模型微调数据时遇到问题。错误消息表明 autotrain 无法识别参数“--fp16”和“--use-int4”。我已经检查了文档和语法，但问题仍然存在。您能否提供解决此问题的指导或提供有关任何潜在解决方案的见解？谢谢。
/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13：
 UserWarning：无法加载图像Python扩展：&#39;/usr/local/lib/python3.10/dist-packages/torchvision/image.so：未定义符号：_ZN3c104cuda9SetDeviceEi&#39;如果您不打算使用`torchvision中的图像功能。 io`，你可以忽略这个警告。否则，您的环境可能有问题。在从源代码构建“torchvision”之前，您是否安装了“libjpeg”或“libpng”？ warn( 用法: autotrain  [] AutoTrain 高级 CLI: 错误: 无法识别的参数: --fp16 --use-int4

错误的屏幕截图
直到昨天，这段代码在这个 https://github.com/huggingface/autotrain-advanced 存储库中给出的 colab 笔记本上运行良好微调LLM，现在出现此错误。]]></description>
      <guid>https://stackoverflow.com/questions/77664921/autotrain-advanced-cli-error-unrecognized-arguments-fp16-use-int4</guid>
      <pubDate>Fri, 15 Dec 2023 07:53:31 GMT</pubDate>
    </item>
    <item>
      <title>更改张量流中预训练模型的输出层</title>
      <link>https://stackoverflow.com/questions/77162671/changing-the-output-layer-of-a-pre-trained-model-in-tensorflow</link>
      <description><![CDATA[我有一个预先训练的模型，只想将输出层更改为具有更多单元的新层。在本例中，旧输出层有 18 个单元，新输出层应有 20 个单元。
我当前的实现如下所示：
old_model = load_model(model_filepath)
helper_model = 模型(输入=old_model.inputs, 输出=old_model.layers[-2].output)
new_output = Dense(单位 = old_model.layers[-1].units + 增量，激活=&#39;softmax&#39;，activity_regularizer=l1_l2(l1l2[0],
                       l1l2[1]))(helper_model.output)
new_model = 模型(输入=helper_model.input, 输出=new_output)

如果我打印 new_model.summary() 并查看最后一层，一切看起来都很好：
 密集（密集）（无，20）2580 [&#39;dropout[0][0]&#39;]

=================================================== =====================================

但是，当我尝试训练 new_model 时，出现以下错误：
ValueError：形状（无，18）和（无，20）不兼容

为什么我会收到此消息以及如何解决此问题？我是否需要重塑图层？]]></description>
      <guid>https://stackoverflow.com/questions/77162671/changing-the-output-layer-of-a-pre-trained-model-in-tensorflow</guid>
      <pubDate>Sat, 23 Sep 2023 10:42:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么 sklearn MLPClassifier 无法预测异或？</title>
      <link>https://stackoverflow.com/questions/63057691/why-cant-sklearn-mlpclassifier-predict-xor</link>
      <description><![CDATA[理论上，具有单个隐藏层和 3 个神经元的 MLP 足以正确预测异或。有时它可能无法正确收敛，但 4 个神经元是安全的选择。
这是一个示例
我尝试使用 sklearn.neural_network.MLPClassifier 重现此问题：
从sklearn导入neural_network
从 sklearn.metrics 导入准确度分数、精确度分数、召回分数
将 numpy 导入为 np


x_train = np.random.uniform(-1, 1, (10000, 2))
tmp = x_train &gt; 0
y_train = 2 * (tmp[:, 0] ^ tmp[:, 1]) - 1

模型=neural_network.MLPClassifier(
    hidden_​​layer_sizes=(3,), n_iter_no_change=100,
    Learning_rate_init=0.01，max_iter=1000
).fit(x_train, y_train)

x_test = np.random.uniform(-1, 1, (1000, 2))
tmp = x_test &gt; 0
y_test = 2 * (tmp[:, 0] ^ tmp[:, 1]) - 1

预测 = model.predict(x_test)
print(f&#39;准确率: {accuracy_score(y_pred=预测, y_true=y_test)}&#39;)
print(f&#39;recall: {recall_score(y_pred=预测, y_true=y_test)}&#39;)
print(f&#39;精度: { precision_score(y_pred=预测, y_true=y_test)}&#39;)

我只能得到大约 0.75 的准确度，而张量流游乐场模型是完美的，你知道是什么造成了差异吗？
还尝试使用张量流：
模型 = tf.keras.Sequential(layers=[
    tf.keras.layers.Input(形状=(2,)),
    tf.keras.layers.Dense(4, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(1)
]）

model.compile(loss=tf.keras.losses.binary_crossentropy)

x_train = np.random.uniform(-1, 1, (10000, 2))
tmp = x_train &gt; 0
y_train = (tmp[:, 0] ^ tmp[:, 1])

model.fit(x=x_train, y=y_train)

x_test = np.random.uniform(-1, 1, (1000, 2))
tmp = x_test &gt; 0
y_test = (tmp[:, 0] ^ tmp[:, 1])

预测 = model.predict(x_test) &gt; 0.5
print(f&#39;准确率: {accuracy_score(y_pred=预测, y_true=y_test)}&#39;)
print(f&#39;recall: {recall_score(y_pred=预测, y_true=y_test)}&#39;)
print(f&#39;精度: { precision_score(y_pred=预测, y_true=y_test)}&#39;)

通过这个模型，我得到了与 scikit-learn 模型类似的结果......所以这不仅仅是一个 scikit-learn 问题 - 我是否缺少一些重要的超参数？
编辑
好的，将损失更改为均方误差而不是交叉熵，现在我在张量流示例中获得了 0.92 的精度。我猜这是 MLPClassifier 的问题？]]></description>
      <guid>https://stackoverflow.com/questions/63057691/why-cant-sklearn-mlpclassifier-predict-xor</guid>
      <pubDate>Thu, 23 Jul 2020 15:22:48 GMT</pubDate>
    </item>
    <item>
      <title>从递归特征消除 (RFE) 中提取最佳特征</title>
      <link>https://stackoverflow.com/questions/47730328/extract-optimal-features-from-recursive-feature-elimination-rfe</link>
      <description><![CDATA[我有一个由分类数据和数值数据组成的数据集，具有 124 个特征。为了降低其维度，我想删除不相关的特征。然而，为了针对特征选择算法运行数据集，我使用 get_dummies 对其进行了热编码，这将特征数量增加到 391 个。
&lt;前&gt;&lt;代码&gt;在[16]中：
X_train.columns
输出[16]：
索引([u&#39;port_7&#39;, u&#39;port_9&#39;, u&#39;port_13&#39;, u&#39;port_17&#39;, u&#39;port_19&#39;, u&#39;port_21&#39;,
   ...
   u&#39;os_cpes.1_2&#39;，u&#39;os_cpes.1_1&#39;]，dtype =&#39;对象&#39;，长度= 391）

根据结果数据，我可以通过交叉验证运行递归特征消除，按照Scikit Learn 示例：
产生：
交叉验证分数与功能图
鉴于识别的特征的最佳数量是 8，我如何识别特征名称？我假设我可以将它们提取到一个新的 DataFrame 中以用于分类算法？
&lt;小时/&gt;
[编辑]
在 这篇文章：
def column_index(df, query_cols):
    cols = df.columns.values
    sidx = np.argsort(列)
    返回 sidx[np.searchsorted(cols, query_cols, sorter = sidx)]

特征索引 = []
特征=[]
列索引（X_dev_train，X_dev_train.columns.values）

对于num，i in enumerate(rfecv.get_support()，start=0)：
    如果我==真：
        feature_index.append(str(num))

对于 num，i 枚举（X_dev_train.columns.values，start=0）：
    如果 feature_index 中的 str(num)：
        features.append(X_dev_train.columns.values[num])

print(&quot;选择的功能：{}\n&quot;.format(len(feature_index)))
print(&quot;特征索引: \n{}\n&quot;.format(feature_index))
print(&quot;功能名称: \n{}&quot;.format(features))

产生：
选择的功能：8
特点指标：
[&#39;5&#39;、&#39;6&#39;、&#39;20&#39;、&#39;26&#39;、&#39;27&#39;、&#39;28&#39;、&#39;67&#39;、&#39;98&#39;]
特征名称：
[&#39;port_21&#39;、&#39;port_22&#39;、&#39;port_199&#39;、&#39;port_512​​&#39;、&#39;port_513&#39;、&#39;port_514&#39;、&#39;port_3306&#39;、&#39;port_32768&#39;]

鉴于一种热编码引入了多重共线性，我认为目标列选择并不理想，因为它选择的特征是非编码的连续数据特征。我尝试重新添加未编码的目标列，但 RFE 抛出以下错误，因为数据是分类的：
ValueError：无法将字符串转换为浮点数：无线接入点

我是否需要将多个单热编码特征列分组作为目标？
&lt;小时/&gt;
[编辑2]
如果我只是对目标列进行 LabelEncode，我可以将此目标用作“y”，请参阅 再次示例。但是，输出仅确定单个特征（目标列）为最佳特征。我认为这可能是因为一种热编码，我是否应该考虑生成一个密集数组，如果是的话，它可以针对 RFE 运行吗？]]></description>
      <guid>https://stackoverflow.com/questions/47730328/extract-optimal-features-from-recursive-feature-elimination-rfe</guid>
      <pubDate>Sat, 09 Dec 2017 15:54:59 GMT</pubDate>
    </item>
    <item>
      <title>如何在具有分类和数字特征的 pandas 数据帧上应用一种热编码？</title>
      <link>https://stackoverflow.com/questions/39258158/how-do-i-apply-one-hot-encoding-on-a-pandas-dataframe-with-both-categorical-and</link>
      <description><![CDATA[一些特征是数字的，例如“学校毕业率”，而其他特征是分类的，例如学校名称。我在分类特征上使用了标签编码器，将它们转换为整数。
我现在有一个包含浮点数和整数的数据框，分别表示数字特征和分类特征（使用标签编码器转换）。
我不确定如何继续学习，我是否需要使用一种热编码？如果是这样，我该怎么做？根据我目前的理解，我不能简单地将数据帧传递给 sklearn OneHotEncoder，因为存在浮点数。我是否只需将标签编码器应用于所有功能即可解决问题？
来自我的数据帧的示例数据。使用标签编码器转换 OPEID 和 opeid6]]></description>
      <guid>https://stackoverflow.com/questions/39258158/how-do-i-apply-one-hot-encoding-on-a-pandas-dataframe-with-both-categorical-and</guid>
      <pubDate>Wed, 31 Aug 2016 20:06:49 GMT</pubDate>
    </item>
    </channel>
</rss>