<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 21 Sep 2024 18:20:35 GMT</lastBuildDate>
    <item>
      <title>AutoModelForSequenceClassification 损失没有减少</title>
      <link>https://stackoverflow.com/questions/79010018/automodelforsequenceclassification-loss-not-decrease</link>
      <description><![CDATA[从数据集导入 load_dataset
从 torch.utils.data 导入 DataLoader
从 transformers 导入 AutoTokenizer、AutoModelForSequenceClassification
导入 torch
从 tqdm 导入 tqdm

def train_one_epoch(model、dataloader、optimizer):
model.train()
loss_list = []
for batch in tqdm(dataloader):
batch_data = {
&#39;input_ids&#39;: batch[&#39;input_ids&#39;],
&#39;attention_mask&#39;: batch[&#39;attention_mask&#39;],
&#39;labels&#39;: batch[&#39;labels&#39;]
}
loss = model(**batch_data).loss
loss.backward()
optimizer.step()
optimizer.zero_grad()

loss_list.append(loss.detach().item())
avg_loss = sum(loss_list) / len(loss_list)
print(&#39;avg loss在 epoch:&#39;, avg_loss)

def assess(model, dataloader):
model.eval()
all_labels = []
all_predictions = []
for batch in dataloader:
with torch.no_grad():
batch_data = {
&#39;input_ids&#39;: batch[&#39;input_ids&#39;],
&#39;attention_mask&#39;: batch[&#39;attention_mask&#39;]
}
logits = model(**batch_data).logits
predictions = torch.argmax(logits, dim=-1)
labels = batch[&#39;labels&#39;]
all_labels.extend(labels)
all_predictions.extend(predictions)
accuracy = compute_accuracy(all_predictions, all_labels)
print(&quot;Accuracy&quot;, accuracy)
return accuracy

def compute_accuracy(predictions, labels):
correct = 0
for pred，zip(predictions, labels) 中的标签：
if pred == label:
correct += 1
返回正确 / len(labels)

def my_collat​​e_fn(batched_samples):
texts = [example[&#39;text&#39;] 例如 batched_samples]
labels = [example[&#39;label&#39;] 例如 batched_samples]
text_encoding = tokenizer(texts, max_length=128, truncation=True, padding=True, return_tensors=&#39;pt&#39;)
labels = torch.LongTensor(labels)
return {
&#39;input_ids&#39;: text_encoding[&#39;input_ids&#39;].cuda(),
&#39;attention_mask&#39;: text_encoding[&#39;attention_mask&#39;].cuda(),
&#39;labels&#39;: labels.cuda()
}

torch.manual_seed(64)
batch_size = 16
学习率 = 5e-5
训练次数 = 10
模型名称 = “roberta-base”

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

model = model.cuda()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate, eps=1e-8)

datasets = load_dataset(&quot;gpt3mix/sst2&quot;)

train_dataloader = DataLoader(
datasets[&#39;train&#39;],
batch_size=8,
shuffle=True,
collat​​e_fn=my_collat​​e_fn,
num_workers=0
)

validation_dataloader = DataLoader(
datasets[&#39;validation&#39;],
batch_size=8,
shuffle=False,
collat​​e_fn=my_collat​​e_fn,
num_workers=0
)

best_acc = 0.0
for周期范围（1，num_epochs + 1）：
train_one_epoch（模型，train_dataloader，优化器）
valid_acc = 评估（模型，validation_dataloader）


100%|██████████| 865/865 [01:27&lt;00:00，9.89it/s]

周期内平均损失：0.6746856869559068

准确率 0.4908256880733945

100%|██████████| 865/865 [01:25&lt;00:00, 10.09it/s]

epoch 中的平均损失：0.6922555248516833

准确率 0.4908256880733945

100%|██████████| 865/865 [01:27&lt;00:00, 9.89it/s]

epoch 中的平均损失：0.6976809655310791

准确率 0.5091743119266054

更改学习率也不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/79010018/automodelforsequenceclassification-loss-not-decrease</guid>
      <pubDate>Sat, 21 Sep 2024 16:24:50 GMT</pubDate>
    </item>
    <item>
      <title>CNN-KAN 模型的训练尚未开始</title>
      <link>https://stackoverflow.com/questions/79009899/the-training-of-a-cnn-kan-model-is-not-starting</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79009899/the-training-of-a-cnn-kan-model-is-not-starting</guid>
      <pubDate>Sat, 21 Sep 2024 15:25:05 GMT</pubDate>
    </item>
    <item>
      <title>X 有 8 个特征，但 RandomForestRegressor 需要 2924 个特征作为输入</title>
      <link>https://stackoverflow.com/questions/79009698/x-has-8-features-but-randomforestregressor-is-expecting-2924-features-as-input</link>
      <description><![CDATA[我正在使用 Kaggle 数据集和 RandomForestRegressor 为我的城市构建餐厅推荐器。
我构建了模型，现在希望模型在给定 4 个参数时推荐一家好餐厅：位置、大致费用、餐厅类型和投票数。但是，它返回了一个值错误：
X 有 8 个特征，但 RandomForestRegressor 需要 2924 个特征作为输入。

这是我尝试运行的：
import joblib
import numpy as np
from sklearn.preprocessing import StandardScaler

model = joblib.load(&#39;my_model.pkl&#39;)
scaler = joblib.load(&#39;scaler.pkl&#39;)

def preprocess_input(location, type_, cost, votes):
one_hot_location = [1 if loc == location else 0 for loc in [&#39;Whitefield&#39;, &#39;Koramangala&#39;, &#39;Indiranagar&#39;]]
one_hot_type = [1 if t == type_ else 0 for t in [&#39;Casual Dining&#39;, &#39;Quick Bites&#39;, &#39;Cafe&#39;]]

scaled_features = scaler.transform([[cost, votes]])

return np.array(one_hot_location + one_hot_type + list(scaled_features[0])).reshape(1, -1)

input_data = preprocess_input(&#39;Whitefield&#39;, &#39;Casual Dining&#39;, 1000, 500)

prediction = model.predict(input_data)

print(f&quot;预测的餐厅：{prediction}&quot;)

训练数据的形状：
X_train.shape = (41373, 2924)
y_train.shape = (41373,)
这是我的数据集的样子]]></description>
      <guid>https://stackoverflow.com/questions/79009698/x-has-8-features-but-randomforestregressor-is-expecting-2924-features-as-input</guid>
      <pubDate>Sat, 21 Sep 2024 13:43:40 GMT</pubDate>
    </item>
    <item>
      <title>我的非序列 keras 模型的一个输入出现了难以理解的形状错误</title>
      <link>https://stackoverflow.com/questions/79009687/incomprehensible-shape-error-with-one-of-the-inputs-of-my-non-sequential-keras-m</link>
      <description><![CDATA[我编写了以下 keras 模型
input_A = 输入(shape=[5], name=&quot;wide_input&quot;)
hidden_​​layer_1 = Dense(10,activation=&quot;relu&quot;, name=&#39;h_wide_layer&#39;)(input_A)
input_B = 输入(shape=[6], name=&quot;deep_input&quot;)
hidden_​​layer_2 = Dense(30,activation=&quot;relu&quot;, name=&#39;h_deep_layer_1&#39;)(input_B)
hidden_​​layer_3 = Dense(30,activation=&quot;relu&quot;, name=&#39;h_deep_layer_2&#39;)(hidden_​​layer_2)
concat = Concatenate()([hidden_​​layer_1, hidden_​​layer_3])
output = Dense(1, name=&quot;output&quot;)(concat)
complex_model_2_1 = keras.Model(inputs=[input_A, input_B], output=[output])

#编译第二个复杂模型
complex_model_2_1.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;sgd&quot;)

#训练
X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]
X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]
X_test_A, X_test_B = X_test_full[:, :5], X_test_full[:, 2:]
X_new_A, X_new_B = X_new[:, :5], X_new[:, 2:]
print(f&quot;X_train_A 的形状：{X_train_A.shape}&quot;)
print(f&quot;X_train_B 的形状：{X_train_B.shape}&quot;)
history = complex_model_2_1.fit({&quot;wide_input&quot;: X_train_A, &quot;deep_input&quot;: X_train_B},
y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))

X_train 有 8 个特征，因此 X_train_A 和 X_train_B 实际上分别有 5 个和 6 个特征。但是，我不明白为什么我会得到下面的hidden_​​layer_2 不兼容形状错误:
ValueError Traceback (most recent call last)
&lt;ipython-input-42-86b1419058f7&gt; in &lt;cell line: 11&gt;()
9 print(f&quot;Shape of X_train_A: {X_train_A.shape}&quot;)
10 print(f&quot;Shape of X_train_B: {X_train_B.shape}&quot;)
---&gt; 11 history = complex_model_2_1.fit({&quot;wide_input&quot;: X_train_A, &quot;deep_input&quot;: X_train_B},
12 y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))
13 

1 帧
/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py in assert_input_compatibility(input_spec, input, layer_name)
225 无,
226 }:
--&gt; 227 raise ValueError(
228 f&#39;层“{layer_name}”的输入 {input_index} 是 &#39;
229 f“与层不兼容：预期轴 {axis}”

ValueError：调用 Functional.call() 时遇到异常。

层“h_deep_layer_1”的输入 0 与层不兼容：预期输入形状的轴 -1 具有值 6，但收到的输入形状为 (None, 5)

Functional.call() 收到的参数：
• 输入={&#39;wide_input&#39;: &#39;tf.Tensor(shape=(None, 5), dtype=float32)&#39;, &#39;deep_input&#39;: &#39;tf.Tensor(shape=(None, 6), dtype=float32)&#39;}
• 训练=True
• 掩码={&#39;wide_input&#39;: &#39;None&#39;, &#39;deep_input&#39;: &#39;None&#39;}

如何修复？
PS：Google colab 中的 Gemini 无法解释该问题，并向我建议 X_train_B = X_train[:, 5:]，这是不正确的（形状为 (_, 3)]]></description>
      <guid>https://stackoverflow.com/questions/79009687/incomprehensible-shape-error-with-one-of-the-inputs-of-my-non-sequential-keras-m</guid>
      <pubDate>Sat, 21 Sep 2024 13:38:38 GMT</pubDate>
    </item>
    <item>
      <title>对 GPT 架构感到失望 [关闭]</title>
      <link>https://stackoverflow.com/questions/79009482/disappoint-at-gpt-architecture</link>
      <description><![CDATA[我是NLP的新手，读过一些该领域的经典论文。读完GPT-2后，我感到很失望，因为它只是将Decoder堆叠了多层，并使用大量数据进行训练。
说实话，Transformer架构给我留下了深刻的印象。GPT-2释放了一个信号，它只需要在更大的模型上训练更多的数据，就可以获得良好的性能。
我还没有读过关于最近模型的论文，你们能给我一些提示吗，现在的LLM是否只是一场由资源控制的游戏？有什么有趣的论文可以阅读吗？
我会阅读更多关于这个主题的论文并有一个整体的了解。]]></description>
      <guid>https://stackoverflow.com/questions/79009482/disappoint-at-gpt-architecture</guid>
      <pubDate>Sat, 21 Sep 2024 11:45:47 GMT</pubDate>
    </item>
    <item>
      <title>为问题和答案生成上下文[关闭]</title>
      <link>https://stackoverflow.com/questions/79009231/generating-context-for-questions-and-answers</link>
      <description><![CDATA[一般问题概述：
我需要微调问答模型，以便从约 5 页的文档中提取问题的答案，但我只有问题和答案的对，没有上下文。
这些文档是非结构化的文本，每个文档包含约 20 个问题的答案 + 一些不重要的文本。所有文档都有非常相似的主题（它们都是数据管理计划）。问题和答案通常至少有一句话长。
问题和答案示例：

问：项目将生成/收集哪些类型和格式的数据？


答：因此，生成的数据类型和格式列表很长，包括但不限于：格式化/未格式化的文本 Mov MP4 二进制 HDF5 Xlsx Jpg VTK PDB PSF PRMTOP XTC PDF PNG EPS DICOM C3D VTK



问：数据的来源是什么？


答：数据来自许多不同的来源。非模拟数据通常用于构建模型，可以源自临床数据管理系统或 DICOM 图像存储。&quot;


查看更多问题此处（不仅仅是这些问题，但都与此类似）。
可用数据
我的数据集由问题和答案组成（大约 1 万对），我需要生成上下文来微调问答模型。
我还有大约 1000 份未标记的文档，每个文档都有自由文本，每个文本包含大约 20 个问题的答案。我的目标是训练模型，以便从此类文档中提取答案（我有一些带标签的文档，将用于评估，但不足以进行训练）。
问题：
如何通过将我的答案作为输入并用类似于我的目标文档的文本围绕它来生成上下文，同时仍然知道答案在哪里。]]></description>
      <guid>https://stackoverflow.com/questions/79009231/generating-context-for-questions-and-answers</guid>
      <pubDate>Sat, 21 Sep 2024 09:31:00 GMT</pubDate>
    </item>
    <item>
      <title>是否有一种 ML 数据处理参数 getter 和 setter 方法来包装常见的 ML 预处理转换器/块/层？[关闭]</title>
      <link>https://stackoverflow.com/questions/79008906/is-there-a-ml-data-processing-parameter-getter-and-setter-approach-to-wrap-commo</link>
      <description><![CDATA[在机器学习数据预处理管道中，管道步骤通常被序列化或保存为 pickle 或模型中的层，以便稍后可以再次加载它们以进行服务或预测，从而保留从原始训练数据中得出的每个步骤的变换/拟合参数。
为什么没有高级 Python 包装器，而是允许将处理步骤的参数或属性值作为数据返回，从而能够将其保存在数据库中而无需保存整个对象？
这将允许在预测时新创建用于服务/预测的处理步骤，并使用从数据库加载的已保存参数和属性值进行配置。
我找不到任何 Python 库或现有库的包装器（例如 TensorFlow、scikit-learn、PyTorch 等）提供用于管道步骤参数保存和设置的高级 API。
为什么它们不存在？
在我看来，它会很有用为了可移植性和从实现库中抽象出来，以及为了检查和调试处理步骤。
我研究过 sklearn get_attrs 和 set_attrs，但它们变得非常复杂，特别是在使用嵌套转换并需要进行树遍历的情况下。
我是否遗漏了基本概念或其他东西？]]></description>
      <guid>https://stackoverflow.com/questions/79008906/is-there-a-ml-data-processing-parameter-getter-and-setter-approach-to-wrap-commo</guid>
      <pubDate>Sat, 21 Sep 2024 06:31:58 GMT</pubDate>
    </item>
    <item>
      <title>使用 Scikit-learn、XGBoost 和 Prophet 时，保存训练模型的最佳文件格式是什么？[关闭]</title>
      <link>https://stackoverflow.com/questions/79008634/what-is-the-best-file-format-to-save-trained-model-when-using-scikit-learn-xgbo</link>
      <description><![CDATA[我正在使用 Scikit-learn 开展 ML 项目。根据我的研究，人们建议使用 .joblib 保存经过训练的 Scikit-learn 模型。
这就是我将模型保存到 .joblib 的方式&gt;
import os
from joblib import dump

model_path = os.path.join(script_dir, &quot;../models/trained_model.joblib&quot;)
dump(model, model_path)
print(f&quot;Model saved at {model_path}&quot;)

我还想使用 XGBoost 和 Prophet 测试此模型，只是为了尝试不同的库。

什么是实现此目标的最佳文件格式？我在搜索过程中多次看到 ONNX，但似乎它与 Prophet 不兼容。

有没有办法将我的模型同时保存为 joblib 和 onnx，或者我是否需要将 jobllib 转换为 onnx 文件？

]]></description>
      <guid>https://stackoverflow.com/questions/79008634/what-is-the-best-file-format-to-save-trained-model-when-using-scikit-learn-xgbo</guid>
      <pubDate>Sat, 21 Sep 2024 01:49:12 GMT</pubDate>
    </item>
    <item>
      <title>网页抓取问题：从网站提取干净数据 [关闭]</title>
      <link>https://stackoverflow.com/questions/79005517/issues-with-web-scraping-extracting-clean-data-from-websites</link>
      <description><![CDATA[问题：
我正在开展一个网页抓取项目，旨在从网站中提取干净、结构化的数据，以进一步丰富检索增强生成 (RAG) 模型。虽然我已经成功抓取并处理了 YouTube 转录本，但在抓取网站数据时我仍面临挑战。
我的方法：

使用的技术：

用于动态内容呈现的 Selenium
用于解析和提取 HTML 内容的 BeautifulSoup
用于过滤不需要的模式和噪音的 Regex 和 NLTK


我已采取的步骤：

删除了 &lt;script&gt;、&lt;style&gt; 等不属于主要内容的 HTML 元素。
使用正则表达式模式过滤掉不相关的数据，例如日期、电子邮件地址和 URL。
应用了 NLTK停用词来进一步清理文本。


代码片段：
这是我的 BeautifulSoup 抓取工具的一个示例：
from bs4 import BeautifulSoup
import request
import re
from nltk.corpus import stopwords

stop_words = set(stopwords.words(&#39;english&#39;))

class BeautifulSoupScraper:
@staticmethod
def extract_text_from_url(url):
response = request.get(url)
soup = BeautifulSoup(response.text, &#39;html.parser&#39;)

# 删除不需要的元素
for unwanted in soup([&#39;script&#39;, &#39;style&#39;, &#39;header&#39;, &#39;footer&#39;, &#39;nav&#39;, &#39;aside&#39;, &#39;form&#39;]):
unwanted.decompose()

段落 = soup.find_all(&#39;p&#39;)
文本 = &quot;\n&quot;.join([para.get_text() for para in 段落])
返回文本

@staticmethod
def filter_text(text):
# 删除不需要的模式（例如 URL、日期等）
unwanted_pa​​tterns = [r&#39;http[s]?://\S+&#39;, r&#39;\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b&#39;]
for pattern in unwanted_pa​​tterns:
文本 = re.sub(pattern, &#39;&#39;, text)
返回 &#39; &#39;.join([word for word in text.split() if word.lower() not in stop_words])

我还使用 Selenium 动态收集搜索结果：
from selenium import webdriver
来自 selenium.webdriver.common.by import By

class GoogleSearch:
@staticmethod
def search(keyword, num_results=5):
driver = webdriver.Chrome()
driver.get(f&quot;https://www.google.com/search?q={keyword}&quot;)
elements = driver.find_elements(By.CLASS_NAME, &quot;MjjYud&quot;)[:num_results]
links = [element.find_element(By.TAG_NAME, &#39;a&#39;).get_attribute(&#39;href&#39;) for element in elements]
driver.quit()
return links



问题：
尽管使用 BeautifulSoup 和正则表达式过滤掉不需要的数据，但提取的内容中仍然有很多噪音，尤其是来自评论部分、广告和其他不相关的内容网页的部分。我的目标是干净地提取有意义的文本（例如，博客内容、文章文本），而不产生这些噪音。
我尝试过的方法：

使用正则表达式删除日期、URL 和电子邮件地址等常见模式。
使用 NLTK 删除停用词。
按关键字过滤（例如，“订阅”、“评论”），但这仍然会留下不需要的网页部分。

我需要的方法：

改进过滤过程的最佳实践或建议，尤其是删除网页中不相关的部分。
除了正则表达式和基本停用词过滤之外，还有更有效的清理抓取数据的方法的建议。
在处理不同类型的网站时，有没有关于如何使提取过程更准确、无噪音的建议结构。
]]></description>
      <guid>https://stackoverflow.com/questions/79005517/issues-with-web-scraping-extracting-clean-data-from-websites</guid>
      <pubDate>Fri, 20 Sep 2024 06:34:55 GMT</pubDate>
    </item>
    <item>
      <title>Optuna Hyperband 算法不遵循预期的模型训练方案</title>
      <link>https://stackoverflow.com/questions/78251318/optuna-hyperband-algorithm-not-following-expected-model-training-scheme</link>
      <description><![CDATA[我在 Optuna 中使用 Hyperband 算法时发现了一个问题。根据 Hyperband 算法，当 min_resources = 5、ma​​x_resources = 20 和 reduction_factor = 2 时，搜索应从 1 组别的 4 个模型的初始空间开始，每个模型在第一轮中接收 5 个 epoch。随后，模型数量在每一轮中减少 2 倍，搜索空间也应在下一组的 2 倍减少，即组别 2 的初始搜索空间为 2 个模型，其余模型的 epoch 数量在随后的每一轮中加倍。因此预计总模型数应为 11，但需要训练大量模型。
文章链接：- https://arxiv.org/pdf/1603.06560.pdf
import optuna
import numpy as np
import pandas as pd 
from tensorflow.keras.layers import Dense,Flatten,Dropout
import tensorflow as tf
from tensorflow.keras.models import Sequential

# 玩具数据集生成
def generate_toy_dataset():
np.random.seed(0)
X_train = np.random.rand(100, 10)
y_train = np.random.randint(0, 2, size=(100,))
X_val = np.random.rand(20, 10)
y_val = np.random.randint(0, 2, size=(20,))
return X_train, y_train, X_val, y_val

X_train, y_train, X_val, y_val = generate_toy_dataset()

# 模型构建函数
def build_model(trial):
model = Sequential()
model.add(Dense(units=trial.suggest_int(&#39;unit_input&#39;, 20, 30),
activation=&#39;selu&#39;,
input_shape=(X_train.shape[1],)))

num_layers = trial.suggest_int(&#39;num_layers&#39;, 2, 3)
for i in range(num_layers):
units = trial.suggest_int(f&#39;num_layer_{i}&#39;, 20, 30)
activation = trial.suggest_categorical(f&#39;activation_layer_{i}&#39;, [&#39;relu&#39;, &#39;selu&#39;, &#39;tanh&#39;])
model.add(Dense(units=units,activation=activation))
如果 trial.suggest_categorical(f&#39;dropout_layer_{i}&#39;, [True, False]):
model.add(Dropout(rate=0.5))

model.add(Dense(1,activation=&#39;sigmoid&#39;))

optimizer_name = trial.suggest_categorical(&#39;optimizer&#39;, [&#39;adam&#39;, &#39;rmsprop&#39;])
如果 optimizer_name == &#39;adam&#39;:
optimizer = tf.keras.optimizers.Adam()
否则:
optimizer = tf.keras.optimizers.RMSprop()

model.compile(optimizer=optimizer, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;, tf.keras.metrics.AUC(name=&#39;val_auc&#39;)])

return model

def objective(trial):
model = build_model(trial)
# 假设您已准备好数据
# 修改拟合方法以包含 AUC 指标
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), verbose=1)

# 检查是否记录了“val_auc”
auc_key = None
for key in history.history.keys():
if key.startswith(&#39;val_auc&#39;):
auc_key = key
print(f&quot;auc_key is {auc_key}&quot;)
break

if auc_key is None:
raise ValueError(&quot;历史记录中未找到AUC指标。确保在训练期间记录它。&quot;)

# 报告每个模型的验证 AUC

if auc_key ==&quot;val_auc&quot;:
step=0
else:
step = int(auc_key.split(&#39;_&#39;)[-1])

auc_value=history.history[auc_key][0]
trial.report(auc_value, step=step)
print(f&quot;prune or not:-{trial.should_prune()}&quot;)
if trial.should_prune():
raise optuna.TrialPruned()

return history.history[auc_key]

# Optuna 研究创建
study = optuna.create_study(
direction=&#39;maximize&#39;,
pruner=optuna.pruners.HyperbandPruner(
min_resource=5,
max_resource=20,
reduction_factor=2
)
)

# 开始优化
study.optimize(objective)

]]></description>
      <guid>https://stackoverflow.com/questions/78251318/optuna-hyperband-algorithm-not-following-expected-model-training-scheme</guid>
      <pubDate>Sun, 31 Mar 2024 12:38:07 GMT</pubDate>
    </item>
    <item>
      <title>Python：GridSearchCV 需要很长时间才能完成运行</title>
      <link>https://stackoverflow.com/questions/72101295/python-gridsearchcv-taking-too-long-to-finish-running</link>
      <description><![CDATA[我正尝试使用网格搜索来优化我的模型，但执行时间太长了。我的总数据集只有大约 15,000 个观测值，大约有 30-40 个变量。我成功地通过网格搜索运行了一个随机森林，大约花了一个半小时，但现在我已经切换到 SVC，它已经运行了 9 个多小时，但仍然没有完成。下面是我用于交叉验证的代码示例：
from sklearn.model_selection import GridSearchCV
from sklearn import svm
from sklearn.svm import SVC

SVM_Classifier= SVC(random_state=7)

param_grid = {&#39;C&#39;: [0.1, 1, 10, 100],
&#39;gamma&#39;: [1,0.1,0.01,0.001],
&#39;kernel&#39;: [&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;],
&#39;degree&#39; : [0, 1, 2, 3, 4, 5, 6]}

grid_obj = GridSearchCV(SVM_Classifier,

return_train_score=True,
param_grid=param_grid,
评分=&#39;roc_auc&#39;,
cv=3,
n_jobs = -1)

grid_fit = grid_obj.fit(X_train, y_train)
SVMC_opt = grid_fit.best_estimator_

print(&#39;=&#39;*20)
print(&quot;best params: &quot; + str(grid_obj.best_estimator_))
print(&quot;best params: &quot; + str(grid_obj.best_params_))
print(&#39;best score:&#39;, grid_obj.best_score_)
print(&#39;=&#39;*20)


我已经将交叉验证从 10 减少到 3，并且我使用 n_jobs=-1，因此我使用了所有核心。我还有什么遗漏的，可以在这里做些事情来加快进程吗？]]></description>
      <guid>https://stackoverflow.com/questions/72101295/python-gridsearchcv-taking-too-long-to-finish-running</guid>
      <pubDate>Tue, 03 May 2022 14:51:20 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 ML 模型和 FastAPI 处理来自多个用户的请求？</title>
      <link>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</link>
      <description><![CDATA[我正在研究通过FastAPI分发人工智能模块的过程。
我创建了一个FastAPI应用，使用预先学习的机器学习模型来回答问题。
这种情况下，一个用户使用是没有问题的，但是多个用户同时使用的时候，响应可能会太慢。
那么，当多个用户输入一个问题的时候，有没有办法一次性复制模型并加载进去？
class sentencebert_ai():
def __init__(self) -&gt;无：
super().__init__()

def ask_query(self,query, topN):
startt = time.time()

ask_result = []
score = []
result_value = [] 
embedder = torch.load(model_path)
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)
query_embedding = embedder.encode(query, convert_to_tensor=True)
cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0] #torch.Size([121])121 表示该数据集为 10 ... cos_scores = cos_scores.cpu() top_results = np.argpartition(-cos_scores, range(topN))[0:topN] for idx in top_results[0:topN]: Ask_result.append(corpusid[idx].item()) #.item()으로 접근하는 Been张量( 5）에서해당숫자에접근하기위한방식다。
            score.append(round(cos_scores[idx].item(),3))

# 生成 json 数组并返回结果
for i,e in zip(ask_result,score):
result_value.append({&quot;pred_id&quot;:i,&quot;pred_weight&quot;:e})
endd = time.time()
print(&#39;结果集&#39;,endd-startt)
return result_value
# return &#39;,&#39;.join(str(e) for e in ask_result),&#39;,&#39;.join(str(e) for e in score)

class Item_inference(BaseModel):
text : str
topN : Optional[int] = 1

@app.post(&quot;/retrieval&quot;, tags=[&quot;knowledge referral&quot;])
async def Knowledge_recommendation(item: Item_inference):

# db.append(item.dict())
item.dict()
results = _ai.ask_query(item.text, item.topN)

return results

if __name__ == &quot;__main__&quot;:
parser = argparse.ArgumentParser()
parser.add_argument(&quot;--port&quot;, default=&#39;9003&#39;, type=int)
# parser.add_argument(&quot;--mode&quot;, default=&#39;cpu&#39;, type=str, help=&#39;cpu for CPU mode, gpu for GPU mode&#39;)
args = parser.parse_args()

_ai = sentencebert_ai()
uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=args.port,workers=4)

更正版本
@app.post(&quot;/aaa&quot;) def your_endpoint(request: Request, item:Item_inference): start = time.time() model = request.app.state.model item.dict() # 测试结果 _ai = sentencebert_ai() results = _ai.ask_query(item.text, item.topN,model) end = time.time() print(end-start) return results ``` 
]]></description>
      <guid>https://stackoverflow.com/questions/71613305/how-to-process-requests-from-multiiple-users-using-ml-model-and-fastapi</guid>
      <pubDate>Fri, 25 Mar 2022 07:13:32 GMT</pubDate>
    </item>
    <item>
      <title>onnxruntime 推理比 GPU 上的 pytorch 慢得多</title>
      <link>https://stackoverflow.com/questions/70740287/onnxruntime-inference-is-way-slower-than-pytorch-on-gpu</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/70740287/onnxruntime-inference-is-way-slower-than-pytorch-on-gpu</guid>
      <pubDate>Mon, 17 Jan 2022 11:03:53 GMT</pubDate>
    </item>
    <item>
      <title>Detectron2 检查点未找到</title>
      <link>https://stackoverflow.com/questions/65327162/detectron2-checkpoint-not-found</link>
      <description><![CDATA[从昨晚开始就一直出现这样的错误，我训练了5个模型，都没有问题，然后就出现了这样的问题，怎么解决呢？
AssertionError Traceback (most recent call last)
&lt;ipython-input-9-08522bc16525&gt; in &lt;module&gt;()
34 os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
35 trainer = CocoTrainer(cfg)
---&gt; 36 trainer.resume_or_load(resume=False)
37 trainer.train()

2 帧
/usr/local/lib/python3.6/dist-packages/fvcore/common/checkpoint.py in load(self, path, checkpointables)
118 if not os.path.isfile(path):
119 path = self.path_manager.get_local_path(path)
--&gt; 120 断言 os.path.isfile(path)，“未找到检查点 {}！”。格式（路径）
121 
122 checkpoint = self._load_file(path)

AssertionError：未找到检查点 https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x/139173657/model_final_68b088.pkl！
]]></description>
      <guid>https://stackoverflow.com/questions/65327162/detectron2-checkpoint-not-found</guid>
      <pubDate>Wed, 16 Dec 2020 16:22:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 K-Means 在 LAB 颜色空间中按颜色对图像进行聚类</title>
      <link>https://stackoverflow.com/questions/30368942/cluster-image-by-colors-in-lab-color-space-using-k-means</link>
      <description><![CDATA[我尝试了以下代码。
he = imread(&#39;hestain.png&#39;);
imshow(he), title(&#39;H&amp;E image&#39;);
cform = makecform(&#39;srgb2lab&#39;);
la​​b_he = applycform(he,cform);
ab = double(lab_he(:,:,2:3));
nrows = size(ab,1); %n 行
ncols = size(ab,2); %p 列
ab = reshape(ab,nrows*ncols,2);

nColors = 3;

[cluster_idx, cluster_center] = kmeans(ab,nColors); 

它给我错误 

reshape 无法从 n*1 数组创建 n*p 矩阵。

这很有道理，但它在这里有效。
我在 octave 中尝试了相同类型的代码
ed=edge(de,&quot;canny&quot;);
imshow(ed);
ed=double(ed);
nrows=size(ed,1);
ncols=size(ed,2);
ed=reshape(ed,nrows*ncols,2)
[cluster_idx, cluster_center]=kmeans(ed,3);
pixel_labels = reshape(cluster_idx,nrows,ncols);
imshow(pixel_labels,[]), title(&#39;image labeled by cluster index&#39;);

其中 de 是一些图像。
当我运行时，我收到此错误。

错误：重塑：无法将 181x181 数组重塑为 32761x2 数组

感谢帮助]]></description>
      <guid>https://stackoverflow.com/questions/30368942/cluster-image-by-colors-in-lab-color-space-using-k-means</guid>
      <pubDate>Thu, 21 May 2015 08:55:18 GMT</pubDate>
    </item>
    </channel>
</rss>