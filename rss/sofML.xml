<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 27 Mar 2024 18:18:13 GMT</lastBuildDate>
    <item>
      <title>正态贝叶斯分类</title>
      <link>https://stackoverflow.com/questions/78233586/normal-bayes-classification</link>
      <description><![CDATA[请帮助我。我是机器学习初学者。如何使用不同类型的协方差矩阵建议来训练普通贝叶斯分类器？
我有一个任务：训练普通贝叶斯分类器：

评估不同类的协方差矩阵，如果它们是 a) 相等、对角 b) 不同标量等
计算经过训练的贝叶斯分类器的分类点 a、b...
显示课程区域

我在 python 上做，但我不明白除了 GaussianNB() 之外我还能做什么。据我所知，这个函数在不检查任何类型的矩阵的情况下建立模型。请帮助我。]]></description>
      <guid>https://stackoverflow.com/questions/78233586/normal-bayes-classification</guid>
      <pubDate>Wed, 27 Mar 2024 17:33:47 GMT</pubDate>
    </item>
    <item>
      <title>VScode 抛出：ModuleNotFoundError：即使正确安装，也没有名为“keras.preprocessing.text”的模块？</title>
      <link>https://stackoverflow.com/questions/78233508/vscode-throwsmodulenotfounderror-no-module-named-keras-preprocessing-text-ev</link>
      <description><![CDATA[下面是我的完整代码。
https://github.com/mishraatharva/text_classification
每当我尝试运行我的代码时，我都会收到以下错误：
错误：
&lt;块引用&gt;
[nltk_data] 将包停用词下载到
[nltk_data] C:\Users\Naruto\AppData\Roaming\nltk_data...
[nltk_data] 包停用词已经是最新的！
2024-03-27 22:35:43.217542：我tensorflow/core/util/port.cc:113] oneDNN 自定义操作已开启。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 TF_ENABLE_ONEDNN_OPTS=0。
2024-03-27 22:35:43.760781：​​我tensorflow/core/util/port.cc:113] oneDNN 自定义操作已开启。由于不同计算顺序的浮点舍入误差，您可能会看到略有不同的数值结果。要关闭它们，请设置环境变量 TF_ENABLE_ONEDNN_OPTS=0。
回溯（最近一次调用最后一次）：
文件“C:\Users\Naruto\Desktop\generative_ai\PROJECTS\text_classification\app.py”，第 1 行，位于
从 hat.pipeline.train_pipeline 导入 TrainPipeline
文件“C:\Users\Naruto\Desktop\generative_ai\PROJECTS\text_classification\hate\pipeline\train_pipeline.py”，第 6 行，位于
从 hat.components.model_trainer 导入 ModelTrainer
文件“C:\Users\Naruto\Desktop\generative_ai\PROJECTS\text_classification\hate\components\model_trainer.py”，第 9 行，位于
从 keras.preprocessing.text 导入 Tokenizer
ModuleNotFoundError：没有名为“keras.preprocessing.text”的模块`

尽管我已经清楚地导入了所需的模块。

下面是我尝试使用 Tokenizer 的代码。
def 标记化（self，x_train）：
        尝试：
            logging.info(“对数据应用标记化”)
            分词器 = 分词器(num_words=self.model_trainer_config.MAX_WORDS)
            tokenizer.fit_on_texts(x_train)
            序列 = tokenizer.texts_to_sequences(x_train)
            logging.info(f“将文本转换为序列：{sequences}”)
            序列矩阵 = pad_sequences(序列,maxlen=self.model_trainer_config.MAX_LEN)
            logging.info(f&quot;序列矩阵为：{sequences_matrix}&quot;)
            返回sequence_matrix，分词器
        除了异常 e：
            从 e 引发 CustomException(e, sys)

我发现了下面的 stackoverflow 帖子，但没有帮助。
https://stackoverflow.com/questions/42725140/importerror-no-module-named-keras-preprocessing
提前致谢。
我希望尽快收到您的来信。]]></description>
      <guid>https://stackoverflow.com/questions/78233508/vscode-throwsmodulenotfounderror-no-module-named-keras-preprocessing-text-ev</guid>
      <pubDate>Wed, 27 Mar 2024 17:17:52 GMT</pubDate>
    </item>
    <item>
      <title>我的代码总是为每次迭代（甚至1次）给出收敛警告，请给出解决方案</title>
      <link>https://stackoverflow.com/questions/78233405/my-code-always-give-convergencewarning-for-every-iterationeven-1-please-give-a</link>
      <description><![CDATA[在此处输入图像说明在此处输入图像描述[在此处输入图像描述](https://i.stack. imgur.com/XwKBA.png)
我想检查四个内核函数的影响和稳定性
软件缺陷预测的SVM性能选择严格来讲，
我想检查非线性核函数与线性核函数的性能。我尝试使用管道解决这个问题，但是在解决这个问题时，即使 max_iter=1 也每次都会给出收敛警告。请给出一个在没有任何收敛警告的情况下运行代码的解决方案]]></description>
      <guid>https://stackoverflow.com/questions/78233405/my-code-always-give-convergencewarning-for-every-iterationeven-1-please-give-a</guid>
      <pubDate>Wed, 27 Mar 2024 17:01:16 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 多步预测</title>
      <link>https://stackoverflow.com/questions/78233177/lstm-multistep-forecast</link>
      <description><![CDATA[有没有办法使用lstm模型对多个窗口进行多步预测？
例如，我训练模型输出 5 个步骤，有没有办法，如果我想预测 10 个步骤，我可以轻松做到？或者这是模型架构固有的，我没有办法做到这一点，以便我可以在预测中拥有更大的灵活性？
我搜索了 seq2seq 编码器解码器等解决方案，但找不到答案。
谢谢
我尝试了简单的 lstm 模型，但只能有固定的输出......]]></description>
      <guid>https://stackoverflow.com/questions/78233177/lstm-multistep-forecast</guid>
      <pubDate>Wed, 27 Mar 2024 16:22:51 GMT</pubDate>
    </item>
    <item>
      <title>使用哪个模型来预测处理时间</title>
      <link>https://stackoverflow.com/questions/78233053/which-model-to-use-to-predict-processing-time</link>
      <description><![CDATA[您好，我正在尝试预测服务提供过程时间。
应用程序详细信息：它在设备上提供服务请求，这意味着用户在设备上推送一些配置。
以下是数据或 x 轴的架构。
RequestType：创建、修改、删除
服务类型：MPLS-L2、L3、ISP、...
涉及设备：Device-1、Device-2、Device-3
总处理时间。
Task-1 处理时间
任务2处理时间
当时正在进行的订单数量。

......
设备处理时间根据其使用情况而有所不同，我们没有使用信息。
我们不执行任务，我们将依赖其他系统来处理它。他们有一个队列机制，我们不知道处理它的实际时间是多少。
我们知道总体处理时间。
我们拥有数十万条此类记录。]]></description>
      <guid>https://stackoverflow.com/questions/78233053/which-model-to-use-to-predict-processing-time</guid>
      <pubDate>Wed, 27 Mar 2024 16:04:00 GMT</pubDate>
    </item>
    <item>
      <title>Yolo v9 保存每个纪元和损失</title>
      <link>https://stackoverflow.com/questions/78232885/yolo-v9-saving-each-epoch-and-loss</link>
      <description><![CDATA[你好，我有这段代码在自定义数据集上训练 yolov9 模型..但是由于我只有 T4 GPU 并且我的数据集很大，它只训练了大约 3 个时期，然后就停止了..我想训练每个时期就其本身并保存它，它是损失..我该怎么做？？
这是我正在使用的代码
%cd /content/my_drive/MyDrive/yolov9/yolov9

!python train.py \
--batch 16 --epochs 25 --img 640 --min-items 0 --close-mosaic 15 \
--data /content/my_drive/MyDrive/yolov9/yolov9/data.yaml \
--weights /content/my_drive/MyDrive/yolov9/yolov9/gelan-c.pt \
--cfg 模型/检测/gelan-c.yaml \
--hyp hyp.scratch-high.yaml
]]></description>
      <guid>https://stackoverflow.com/questions/78232885/yolo-v9-saving-each-epoch-and-loss</guid>
      <pubDate>Wed, 27 Mar 2024 15:36:49 GMT</pubDate>
    </item>
    <item>
      <title>处理不平衡数据集分类的问题</title>
      <link>https://stackoverflow.com/questions/78232803/questions-of-handling-imbalance-dataset-classification</link>
      <description><![CDATA[我正在尝试预测将终止其会员资格的会员数量。整个数据集大约有 1200 万行数据，大约 40 列。会员状态可以是“继续”、“自愿终止”或“非自愿终止”。该数据集高度不平衡，98% 的会员选择“继续”，约 1% 的会员选择“自愿终止”和“非自愿终止”。为了降低维度，我进行了相关性分析，仅选择 15 个相关性最高的特征进行建模。
以下是我面临的问题：

我的同事使用多项回归。然而，他没有应用阈值将概率转换为类别标签。相反，他总结了各个成员的所有概率，以估计预测将自愿终止或非自愿终止的成员数量。
我不确定这种方法，因为在求和个体概率而不是使用阈值后，我不太明白其含义。鉴于我们对总人数感兴趣，这种方法是否正确？另外，我们如何用这种方法衡量模型性能

我将这个问题视为分类问题。由于它是不平衡的数据集，并且我们对将停止使用的人感兴趣，因此我尝试了 SMOTE 和欠采样方法。然而，尝试使用逻辑回归、决策树和神经网络，它们对于“自愿中断”和“非自愿中断”类别的精度仍然很低。还有其他方法可以提高少数类的精度吗？

我尝试运行随机森林。但由于内存限制，未能运行。对于处理这种大型数据集有什么建议吗？

]]></description>
      <guid>https://stackoverflow.com/questions/78232803/questions-of-handling-imbalance-dataset-classification</guid>
      <pubDate>Wed, 27 Mar 2024 15:24:10 GMT</pubDate>
    </item>
    <item>
      <title>我在尝试对心电图和脑电图数据执行二元分类时需要一些帮助来解决类别不平衡问题</title>
      <link>https://stackoverflow.com/questions/78232398/i-need-some-help-in-solving-a-class-imbalance-problem-while-trying-to-perform-bi</link>
      <description><![CDATA[我有一个数据集，其中包含 23 名患者的心电图和脑电图值，每个患者有 18 个视频。这些视频与我试图预测的目标情绪相关。现在根据数据集有 8 种目标情绪，但我已将它们重新分类为 0 - 非恐惧和 1 - 恐惧。这导致了比例为 1:7 的阶级不平衡（恐惧：不是恐惧）。因此，我的准确率在 90% 范围内出现错误。我真的很感激能帮助解决这个问题。
将 numpy 导入为 np
将 pandas 导入为 pd
从 sklearn.model_selection 导入 train_test_split
从 sklearn.neighbors 导入 KNeighborsClassifier
将 matplotlib.pyplot 导入为 plt
从sklearn.metrics导入confusion_matrix
从 sklearn 导入 svm
从 sklearn.datasets 导入 make_classification
从 sklearn.metrics 导入准确度_分数、精度_分数、召回_分数、f1_分数
从 sklearn.model_selection 导入 cross_val_score
从 sklearn.linear_model 导入 LogisticRegression
从sklearn.metrics导入confusion_matrix、recall_score、 precision_score、f1_score、accuracy_score
从 sklearn.model_selection 导入 KFold
将seaborn导入为sns
从 sklearn.metrics 导入分类报告
从 imblearn.over_sampling 导入 SMOTE

defvaluate_cv_model（模型，数据，目标，kFolds）：
    a_score = cross_val_score(模型、数据、目标、cv=kFolds、评分=&#39;准确度&#39;)
    准确度 = a_score.mean()
​
    返回精度

defplot_confusionMatrix（clf，y_test，X_test）：
    
    y_pred = clf.predict(X_test)
    cm = 混淆矩阵(y_test, y_pred)
    cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis]
    sns.heatmap(cm, annot=True, fmt=&#39;.2f&#39;, cmap=“蓝调”)
    plt.ylabel(&#39;真实标签&#39;)
    plt.xlabel(&#39;预测标签&#39;)
    报告=分类报告（y_test，y_pred）
    plt.show()

    返回报告

def KNN(X_train, y_train, X_test, y_test, num_neighbors):
    
    # 创建模型
    KNN = KNeighborsClassifier(n_neighbors = num_neighbors)
    
    # 拟合模型
    KNN.fit(X_train, y_train)
    
    # 获取准确率
    test_accuracy = KNN.score(X_test, y_test)
    train_accuracy = KNN.score(X_train, y_train)
    
    # 预测值
    预测 = KNN.predict(X_test)
    
    返回 test_accuracy、train_accuracy、预测、KNN

def SVM（X_train，y_train，X_test，y_test，内核）：

    # 创建多类分类模型
    SVM = svm.SVC(kernel=kernel, C=1, Decision_function_shape=&#39;ovo&#39;)
    
    # 拟合模型
    SVM.fit(X_train, y_train)
    
    # 获取准确率
    test_accuracy = SVM.score(X_test, y_test)
    train_accuracy = SVM.score(X_train, y_train)
    
    # 预测值
    预测 = SVM.predict(X_test)
    
    返回 test_accuracy、train_accuracy、预测、SVM
def Logistic_Regression (X_train, y_train, X_test, y_test):
    
    # 创建增加 max_iter 的模型
    log_reg = LogisticRegression(multi_class=&#39;多项式&#39;, 求解器=&#39;lbfgs&#39;, max_iter=1000)

    # 拟合模型
    log_reg.fit(X_train, y_train)

    # 获取准确率
    test_accuracy = log_reg.score(X_test, y_test)
    train_accuracy = log_reg.score(X_train, y_train)

    # 预测值
    预测 = log_reg.predict(X_test)

    返回 test_accuracy、train_accuracy、预测、log_reg

ECG_data = pd.read_csv(&#39;/kaggle/input/ecgdata/binary_ECG.csv&#39;)
ECG_data.drop([&#39;未命名: 0&#39;,&#39;video_name&#39;], axis=1, inplace=True)
y_ECG = ECG_data.target
X_ECG = ECG_data.drop(&#39;目标&#39;, 轴 = 1)

# 应用 SMOTE 来处理类别不平衡
smote = SMOTE(sampling_strategy=&#39;auto&#39;, random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_ECG, y_train_ECG)

kf = KFold(n_splits=8, random_state=42 , shuffle = True)
X_train_ECG、X_test_ECG、y_train_ECG、y_test_ECG = train_test_split(X_ECG、y_ECG、test_size = 0.2、random_state = 42)
y_test_ECG = np.array(y_test_ECG)

def 评估（y_test，预测）：
    准确度=准确度_分数（y_测试，预测）
    精度 = precision_score(y_test, 预测, 平均值=&#39;加权&#39;)
    召回率=召回率（y_测试，预测，平均值=&#39;加权&#39;）
    f1 = f1_score(y_test, 预测, 平均值=&#39;加权&#39;)
    返回准确率、精确率、召回率、f1

我尝试过使用 SVM、KNN 和逻辑回归进行训练。在尝试实现 SMOTE 时，我遇到了逻辑回归的收敛错误，即使我将 max_iter 增加到 python 中的最大允许限制，简单似乎也不会消失。]]></description>
      <guid>https://stackoverflow.com/questions/78232398/i-need-some-help-in-solving-a-class-imbalance-problem-while-trying-to-perform-bi</guid>
      <pubDate>Wed, 27 Mar 2024 14:21:43 GMT</pubDate>
    </item>
    <item>
      <title>如何将 tfidfvectorizer 的功能从英语修改为西班牙语</title>
      <link>https://stackoverflow.com/questions/78232328/how-to-modify-features-of-tfidfvectorizer-from-english-to-spanish</link>
      <description><![CDATA[我有一个 tfidfvectorizer，它适合英语文本数据来预测英语通话的情绪。任务是将其转换为西班牙语。我想使用此 tfidfvectorizers 的权重，并希望将功能从英语转换为西班牙语，例如“谢谢”变成“gracias”并使用旧的权重。所以本质上我想使用相同的 tfidf 矢量器，但修改了特征名称。有人可以建议一些方法在 Python 中做到这一点吗？
带有解决方案的代码。]]></description>
      <guid>https://stackoverflow.com/questions/78232328/how-to-modify-features-of-tfidfvectorizer-from-english-to-spanish</guid>
      <pubDate>Wed, 27 Mar 2024 14:11:46 GMT</pubDate>
    </item>
    <item>
      <title>使用隔离森林进行异常检测[关闭]</title>
      <link>https://stackoverflow.com/questions/78232159/anomaly-detection-with-isolation-forest</link>
      <description><![CDATA[我有车辆数据。该数据是在会议中测量的。我在数据框中有一列显示测量会话 ID。在“时间”列中，时间每 200 毫秒累加一次。测量的块具有不同的长度。有些是 600000 毫秒长，有些是 400000 毫秒长。如果 id 发生变化，时间列会再次从 0 开始计数。我现在的问题是，我如何向隔离森林教授这一点，或者我如何准备数据和列，以便隔离森林考虑到这一点？我真的需要尽快得到一个好的答案。非常感谢
我没有任何想法。 Time 列也只是 float64 的类型，它不是日期时间对象。]]></description>
      <guid>https://stackoverflow.com/questions/78232159/anomaly-detection-with-isolation-forest</guid>
      <pubDate>Wed, 27 Mar 2024 13:45:39 GMT</pubDate>
    </item>
    <item>
      <title>“MENACE”井字棋电脑需要多少场比赛才能训练</title>
      <link>https://stackoverflow.com/questions/78219696/how-many-games-will-a-menace-tic-tac-toe-computer-take-to-train</link>
      <description><![CDATA[我最近读到了唐纳德·米奇 (Donald Michie) 设计的用火柴盒建造的“计算机”，它可以自学如何玩井字游戏。这是关于它的维基百科文章：
https://en.m.wikipedia.org/wiki/Matchbox_Educable_Noughts_and_Crosses_Engine 
我觉得它看起来很有趣，所以我决定用 Python 制作一个数字版本，以供娱乐和练习。它在对抗随机走棋时效果很好（我刚刚根据约 10,000 场比赛生成的数据再次运行了 5353 场比赛，它赢得了 5353 场比赛中的 4757 场），但它仍然经常输给我。
以下是完美答案应解决的一些问题：

需要玩多少场游戏才能让“火柴盒电脑”与 Michie 设计的电脑完全一样，才能完美地开始玩游戏？

带有实际火柴盒的原始计算机是否达到了完美状态
玩吗？

如果仅与计算机进行训练，计算机能否达到完美的发挥
随机移动？


编辑：
这个问题并不是寻求代码方面的帮助，但下面的评论表明包含代码可能会有所帮助。以下是我创建的 GitHub 存储库的链接，以便我可以在此处共享：
https://github.com/ACertainArchangel/ Recreation-Of-MENACE-Tic-Tac-Toe..git
抱歉，我知道这不太好并且不遵守约定；我只写了几个月的代码:)]]></description>
      <guid>https://stackoverflow.com/questions/78219696/how-many-games-will-a-menace-tic-tac-toe-computer-take-to-train</guid>
      <pubDate>Mon, 25 Mar 2024 14:12:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 MPI 优化 Optuna 参数</title>
      <link>https://stackoverflow.com/questions/78218072/optuna-parameter-optimisation-with-mpi</link>
      <description><![CDATA[我有一些机器学习代码，它使用 SVM（来自 scikit-learn）和预计算内核，我想使用 optuna 对其进行优化，因此代码简单地看起来有点像这样
def 目标（试用：试用，fast_check=True，target_meter=0，return_info=False）：
     #设置参数
     C = Trial.suggest_float(“C”,0.0​​1,5)
     tol = Trial.suggest_loguniform(“tol”,1e-4,1e-1)
     内核参数 = ...

     #构建火车内核
     内核训练 = ...

     #构建测试内核
     内核测试 = ...

     #火车服务
     svc = SVC(内核=“预计算”, C=C, tol=tol)
     svc.fit(kernel_train, train_labels)
     test_predict = svc.predict(kernel_test)
     test_auc = roc_auc_score(test_labels,test_predict)

     返回测试_auc

Study = optuna.create_study(direction=“最大化”,study_name=&#39;study_1&#39;)
研究.优化（目标，n_Trials=40）


但是，由于我正在计算的内核的复杂性，我使用 mpi4py 来并行计算，但同​​时使用 optuna 和 MPI 时遇到一些问题。
显然，我想要多个处理器上的内核代码，但是当我创建研究并优化它时，我不想在处理器上创建多个不同的研究，我只想对根进行优化的一项研究（我假设？）。我已经尝试了下面的方法，它有效，但是当我不使用 MPI 时，它的优化效果不佳，我认为这正在创建多项研究并优化它们，这似乎效率不高。似乎更难以收敛到最佳参数。
从 mpi4py 导入 MPI

mpi_comm = MPI.COMM_WORLD
排名 = mpi_comm.Get_rank()
n_procs = mpi_comm.Get_size()
根=0

def目标（试验：试验，fast_check = True，target_meter = 0，return_info = False）：
     #设置参数
     C = Trial.suggest_float(“C”,0.0​​1,5)
     tol = Trial.suggest_loguniform(“tol”,1e-4,1e-1)
     内核参数 = ...

     #使用 MPI 构建训练内核
     内核训练 = ...

     #使用MPI构建测试内核
     内核测试 = ...

     #火车服务
     如果排名==根：
           svc = SVC(内核=“预计算”, C=C, tol=tol)
           svc.fit(kernel_train, train_labels)
           test_predict = svc.predict(kernel_test)
           test_auc = roc_auc_score(test_labels,test_predict)
     别的：
           测试_auc = 0
     test_auc = mpi_comm.bcast(test_auc, root=0)

如果排名==根：
     Study = optuna.create_study(direction=“最大化”,study_name=&#39;study_1&#39;)
别的：
     研究 = 0
 研究= mpi_comm.bcast（研究，根= 0）

研究.优化（目标，n_Trials=40）

这是一个非常小众的问题，但只是想知道是否有人对这些软件包有任何经验，并且可以帮助建议如何运行多处理代码，同时仅优化一个处理器上的参数。如果任何术语不正确，我深表歉意，我是使用这两个软件包的新手，因此请耐心等待。 :)]]></description>
      <guid>https://stackoverflow.com/questions/78218072/optuna-parameter-optimisation-with-mpi</guid>
      <pubDate>Mon, 25 Mar 2024 09:20:08 GMT</pubDate>
    </item>
    <item>
      <title>R 混淆矩阵 - 错误：“数据”和“参考”应该是具有相同级别的因素</title>
      <link>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</link>
      <description><![CDATA[尽管还有其他针对相同错误消息的报告，但没有一个对我的情况有帮助。
我已经准备了自己的数据，分割如下，但无法获得混淆矩阵。
test_index &lt;- createDataPartition(y =workingData$PM10, times = 1, p = 0.5, list = FALSE)
train_set &lt;-工作数据[-test_index,]
test_set &lt;-工作数据[test_index,]

train_knn &lt;- train(PM10 ~. , method= &quot;knn&quot; , data = train_set)

y_hatknn &lt;- 预测(train_knn, train_set, type = “raw”)

fusionMatrix(y_hatknn, test_set$PM10)

上面最后一行给出
错误：“data”和“reference”应该是具有相同级别的因素。

我想上传数据进行复制，但可以提供基本的：
&lt;前&gt;&lt;代码&gt;str（工作数据）
“数据帧”：3653 obs。 3 个变量：
&#39; $ 日期 : 数字 2e+07 2e+07 2e+07 2e+07 2e+07 ...
&#39; $ Rain_mm: 数字 0.1 6.7 0 1.4 0.8 1.8 15.3 0 2.6 3.8 ...
&#39; $ PM10 : 数字 -1 -1 -1 -1 -1 ...

PM10 是污染 PM10 水平。
如何解决？
添加更多信息：
在原始错误之后：
&lt;块引用&gt;
confusionMatrix(y_hatknn, test_set$PM10)
错误：data 和 reference 应该是具有相同水平的因子。

我尝试将其设置为因素...
&lt;块引用&gt;
confusionMatrix(y_hatknn, as.factor(test_set$PM10))
错误：data 和 reference 应该是具有相同水平的因子。

以预测为因素...
&lt;块引用&gt;
confusionMatrix(as.factor(y_hatknn), test_set$PM10)
错误：data 和 reference 应该是具有相同水平的因子。

以两个参数为因素...
&lt;块引用&gt;
confusionMatrix(as.factor(y_hatknn), as.factor(test_set$PM10))
fusionMatrix.default(as.factor(y_hatknn), as.factor(test_set$PM10)) 中的错误：
数据的级别不能多于参考

真正需要得到的已整理出来]]></description>
      <guid>https://stackoverflow.com/questions/78205262/r-confussion-matrix-error-data-and-reference-should-be-factors-with-the-s</guid>
      <pubDate>Fri, 22 Mar 2024 09:39:08 GMT</pubDate>
    </item>
    <item>
      <title>我可以将候选数据集转换为检索 topK 张量流模型的输入吗？</title>
      <link>https://stackoverflow.com/questions/78196301/can-i-turn-candidates-dataset-to-input-on-retrieval-topk-tensorflow-model</link>
      <description><![CDATA[我有一个检索张量流训练模型，并使用 tfrs.layers.factorized_top_k.BruteForce 来预测第一个 k 的附近候选者，如下实现：
index = tfrs.layers.factorized_top_k.BruteForce(final_model.query_model)

索引.index_from_dataset(
    tf.data.Dataset.zip((parsed_topK.batch(128).map(lambda x: x[&#39;id&#39;]), parsed_topK.batch(128).map(final_model.candidate_model)))
）

并获取前 5 个结果：
结果 = 索引(input_query, k=5)

我想知道是否可以将搜索数据库（在此代码中由 parsed_topK 表示）转换为模型的输入，例如：
索引(input_query, input_candidates, k=5)

在此示例中，其中input_candidates = parsed_topK
我尝试调用final_model.predict(input_query, input_candidates)，但我需要实现一个call()方法，但我不知道这个方法需要做什么。]]></description>
      <guid>https://stackoverflow.com/questions/78196301/can-i-turn-candidates-dataset-to-input-on-retrieval-topk-tensorflow-model</guid>
      <pubDate>Wed, 20 Mar 2024 21:22:27 GMT</pubDate>
    </item>
    <item>
      <title>如何计算线性回归中的正则化参数</title>
      <link>https://stackoverflow.com/questions/12182063/how-to-calculate-the-regularization-parameter-in-linear-regression</link>
      <description><![CDATA[当我们有一个高次线性多项式用于拟合线性回归设置中的一组点时，为了防止过度拟合，我们使用正则化，并在成本函数中包含 lambda 参数。然后使用该 lambda 更新梯度下降算法中的 theta 参数。
我的问题是我们如何计算这个 lambda 正则化参数？]]></description>
      <guid>https://stackoverflow.com/questions/12182063/how-to-calculate-the-regularization-parameter-in-linear-regression</guid>
      <pubDate>Wed, 29 Aug 2012 16:04:04 GMT</pubDate>
    </item>
    </channel>
</rss>