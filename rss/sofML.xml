<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 09 Jul 2024 15:15:51 GMT</lastBuildDate>
    <item>
      <title>Google Collab Pro+ 运行时断开连接问题</title>
      <link>https://stackoverflow.com/questions/78726238/google-collab-pro-runtime-disconnect-issues</link>
      <description><![CDATA[过去 3 天，我一直在运行笔记本以进行机器学习，一切都按预期进行。突然，运行时断开连接，重新连接后，代码停止执行，这种情况已经发生过多次。
代码很好，因为它使用较小的数据集成功完成，我如何确定是什么原因导致代码停止执行，考虑到 Pro+ 还按每个计算单元收费，这现在花费了一点钱。
所有资源的利用率都远低于 100%，当时有大量计算单元。
如能提供任何帮助，我们将不胜感激]]></description>
      <guid>https://stackoverflow.com/questions/78726238/google-collab-pro-runtime-disconnect-issues</guid>
      <pubDate>Tue, 09 Jul 2024 14:36:35 GMT</pubDate>
    </item>
    <item>
      <title>我该如何解决 UnicodeDecodeError？</title>
      <link>https://stackoverflow.com/questions/78726197/how-can-i-solve-the-unicodedecodeerror</link>
      <description><![CDATA[我尝试使用我的数据集训练更快的 rcnn，但我总是收到这个错误。 &quot;UnicodeDecodeError: &#39;utf-8&#39; 编解码器无法解码位置 118 中的字节 0xfd：起始字节无效&quot;。
以下是所有回溯，
文件 &quot;C:\Users\90531\Desktop\New_tf2\models\research\object_detection\model_main_tf2.py&quot;，第 114 行，在 
tf.compat.v1.app.run()
文件 &quot;C:\Users\90531\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\platform\app.py&quot;，第 36 行，在运行中
_run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
文件&quot;C:\Users\90531\anaconda3\envs\tensorflow2\lib\site-packages\absl\app.py&quot;，第 308 行，在运行中
_run_main(main, args)
文件 &quot;C:\Users\90531\anaconda3\envs\tensorflow2\lib\site-packages\absl\app.py&quot;，第 254 行，在 _run_main
sys.exit(main(argv))
文件 &quot;C:\Users\90531\Desktop\New_tf2\models\research\object_detection\model_main_tf2.py&quot;，第 105 行，在主程序中
model_lib_v2.train_loop(
文件&quot;C:\Users\90531\anaconda3\envs\tensorflow2\lib\site-packages\object_detection\model_lib_v2.py&quot;，第 505 行，在 train_loop 中
configs = get_configs_from_pipeline_file(
文件 &quot;C:\Users\90531\anaconda3\envs\tensorflow2\lib\site-packages\object_detection\utils\config_util.py&quot;，第 138 行，在 get_configs_from_pipeline_file 中
proto_str = f.read()
文件 &quot;C:\Users\90531\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\lib\io\file_io.py&quot;，第 114 行，在 read 中
self._preread_check()
文件&quot;C:\Users\90531\anaconda3\envs\tensorflow2\lib\site-packages\tensorflow\python\lib\io\file_io.py&quot;，第 76 行，在 _preread_check
self._read_buf = _pywrap_file_io.BufferedInputStream(
UnicodeDecodeError: &#39;utf-8&#39; 编解码器无法解码位置 118 中的字节 0xfd：起始字节无效
我尝试使用此代码来训练模型（efficientdet_d0_coco17_tpu-32.tar.gz）：


python model_main_tf2.py --pipeline_config_path==ssd_efficientdet_d0_512x512_coco17_tpu-8.config --model_dir==training --alsologtostderr

]]></description>
      <guid>https://stackoverflow.com/questions/78726197/how-can-i-solve-the-unicodedecodeerror</guid>
      <pubDate>Tue, 09 Jul 2024 14:26:30 GMT</pubDate>
    </item>
    <item>
      <title>我需要改变什么参数才能满足要求？</title>
      <link>https://stackoverflow.com/questions/78725972/what-parameter-do-i-need-to-change-for-it-to-match-requirements</link>
      <description><![CDATA[我正在尝试基于修改后的 MNSIT 数据集训练模型，以便它对带有标签 10 的随机图像进行分类。我不断收到 Typeerror。
transform = transforms.Compose([
transforms.ToTensor(),
transforms.Normalize((0.1307,), (0.3081,))
])

dataset1 = datasets.MNIST(root=&#39;./data&#39;, train=True, transform = transform)
dataset2 = datasets.MNIST(root=&#39;./data&#39;, train=False, transform=transform)
num_new_images = 7000
noisy_images = torch.randn(num_new_images, 1, 28, 28)
mean = 0.1307
std = 0.3081
random_images = (noisy_images-mean)/std
noisy_labels = torch.full((num_new_images,),10, dtype=torch.long)
new_dataset = torch.utils.data.TensorDataset(noisy_images, noisy_labels)
combined_dataset = torch.utils.data.ConcatDataset([dataset1, new_dataset])
len(combined_dataset) 
num_val_images = 1000
noisy_images = torch.randn(num_val_images, 1, 28, 28)
random_val_images = (noisy_images-mean)/std
noisy_val_labels = torch.full((num_val_images,),10, dtype=torch.long)
new_val_dataset = torch.utils.data.TensorDataset(random_val_images, noisy_val_labels)
combined_val_dataset = torch.utils.data.ConcatDataset([dataset2, new_val_dataset])
batch_size = 128
train_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(combined_val_dataset, batch_size=batch_size, shuffle=False)

错误：
TypeError Traceback（最近一次调用最后一次）
Cell In[12]，第 74 行
72 # 训练神经网络
73 for epoch in range(num_epochs):
---&gt; 74 对于 train_loader 中的图像、标签：
75 输出 = 模型（图像）
76 损失 = 标准（输出、标签）

文件 ~\PycharmProjects\tensorflow_start\venv\Lib\site-packages\torch\utils\data\dataloader.py:633，在 _BaseDataLoaderIter.__next__(self) 中
630 如果 self._sampler_iter 为 None：
631 # TODO(https://github.com/pytorch/pytorch/issues/76750)
632 self._reset() # 类型：ignore[call-arg]
--&gt; 633 data = self._next_data()
634 self._num_yielded += 1
635 if self._dataset_kind == _DatasetKind.Iterable and \
636 self._IterableDataset_len_called is not None and \
637 self._num_yielded &gt; self._IterableDataset_len_called:

我已经尝试更改标签的数据类型，但没有成功]]></description>
      <guid>https://stackoverflow.com/questions/78725972/what-parameter-do-i-need-to-change-for-it-to-match-requirements</guid>
      <pubDate>Tue, 09 Jul 2024 13:43:42 GMT</pubDate>
    </item>
    <item>
      <title>机器学习 [关闭]</title>
      <link>https://stackoverflow.com/questions/78725899/machine-learning</link>
      <description><![CDATA[ final_svm_model = SVC() 
final_nb_model = GaussianNB() 
final_rf_model = RandomForestClassifier(random_state=18) 
final_svm_model.fit(X, y) 
final_nb_model.fit(X, y) 
final_rf_model.fit(X, y) 

test_data = pd.read_csv(&quot;dataset/Testing.csv&quot;).dropna(axis=1) 

test_X = test_data.iloc[:, :-1] 
test_Y =coder.transform(test_data.iloc[:, -1]) 

svm_preds = final_svm_model.predict(test_X) 
nb_preds = final_nb_model.predict(test_X) 
rf_preds = final_rf_model.predict(test_X) 
[mode([i,j,k])[0][0] for i,j,k in zip(svm_preds, nb_preds, rf_preds)] 

cf_matrix = chaos_matrix(test_Y, final_preds) 
plt.figure(figsize=(12,8)) 

sns.heatmap(cf_matrix, annot = True) 

我收到此错误
IndexError：标量变量的索引无效。

我在 mode([i,j,k})[0][0] 附近收到此错误]]></description>
      <guid>https://stackoverflow.com/questions/78725899/machine-learning</guid>
      <pubDate>Tue, 09 Jul 2024 13:27:30 GMT</pubDate>
    </item>
    <item>
      <title>当我尝试提取音频特征时出现“无法加载‘facebook/wav2vec2-large-960h-lv60-self’的特征提取器”错误</title>
      <link>https://stackoverflow.com/questions/78725798/cant-load-feature-extractor-for-facebook-wav2vec2-large-960h-lv60-self-error</link>
      <description><![CDATA[代码
processor = Wav2Vec2Processor.from_pretrained(&quot;facebook/wav2vec2-large-960h-lv60-self&quot;)
model = Wav2Vec2Model.from_pretrained(&quot;facebook/wav2vec2-large-960h-lv60-self&quot;)
model.config.output_hidden_​​states = True

错误
无法加载特征提取器&#39;facebook/wav2vec2-large-960h-lv60-self&#39;。如果您尝试从“https://huggingface.co/models”加载它，请确保您没有同名的本地目录。否则，请确保“facebook/wav2vec2-large-960h-lv60-self”是包含 preprocessor_config.json 文件的目录的正确路径

当我尝试使用 HuggingFace 提取特征时，我收到此错误。我尝试修复此问题，同时考虑 Tokenizer 错误。但我无法修复它。有人可以帮忙吗？
我需要使用此模型提取音频特征。]]></description>
      <guid>https://stackoverflow.com/questions/78725798/cant-load-feature-extractor-for-facebook-wav2vec2-large-960h-lv60-self-error</guid>
      <pubDate>Tue, 09 Jul 2024 13:07:29 GMT</pubDate>
    </item>
    <item>
      <title>如何显示图像和预测 Confution Matrics</title>
      <link>https://stackoverflow.com/questions/78725594/how-to-display-images-and-predictions-confution-matrics</link>
      <description><![CDATA[我有这样的代码，你可以访问 GitHub，整个过程是相似的，只是架构和数据集不同
https://github.com/cendekialnazalia/CaisimPestDetection/blob/main/Percobaan%20E%20-%20CNN%20add%20Models%20Xception.ipynb
有关更多信息，请参阅结果 CM

在最后一行代码“对测试集进行预测并生成混淆矩阵和分类报告”之后，我添加了类似下面的代码来找出每个测试数据的预测值
test_gen.class_indices

print(preds,preds.shape)

result_index = np.argmax(preds[])
print(result_index)

for i in range(len(preds)):
if(np.argmax(preds[i]) == 0):
print(&quot;Bercak Daun&quot;)
elif(np.argmax(preds[i]) == 1):
print(&quot;Daun Sehat&quot;)
elif(np.argmax(preds[i]) == 2):
print(&quot;Karat Merah&quot;)
else:
print(&quot;Lainya&quot;)

输出
Bercak Daun
Daun Sehat
.
.
.
最多 177
Daun Sehat

除了显示带有字符串的预测之外，我还想将其与图像一起显示。也许有更好、更高效的代码可以解决我的问题，请帮我回答，因为我还是个初学者，想学习]]></description>
      <guid>https://stackoverflow.com/questions/78725594/how-to-display-images-and-predictions-confution-matrics</guid>
      <pubDate>Tue, 09 Jul 2024 12:27:54 GMT</pubDate>
    </item>
    <item>
      <title>使用专家评分训练神经网络进行图像-文本相关性分析</title>
      <link>https://stackoverflow.com/questions/78724935/training-neural-network-for-image-text-relevance-with-expert-scores</link>
      <description><![CDATA[我有两个数据集：
第一个数据集包含由修改后的 ResNet18 模型生成的 768 维图像嵌入。删除最后的全连接层以获得特征表示而不是图像分类。输出被投影到 768 维向量空间中。

第二个数据集由 DistilBERT 生成的 768 维文本嵌入组成。

图像数据集由没有特定主题和相对中性内容的图像组成，描绘了狗在日常环境中玩耍或人们玩耍的场景。
文本数据集更加复杂。它包含图像内容的描述，每个图像都有多个描述。这些描述按从 0 到 1 的连续比例排序，反映了它们在描绘图像方面的准确性。 0 分表示描述和图像之间没有对应关系，而 1 分表示完美匹配。
目标是开发一种搜索解决方案，允许基于预定义的文本查询进行图像检索。约束是避免使用预训练的多模态模型（如 CLIP）并从头开始设计神经网络。
对我来说，这无疑是一个多模态问题。目标是在同一空间内对齐图像和文本向量。为此，我修改了 ResNet18，并打算构建一个以描述等级作为权重初始化的神经网络。但是，我不确定这种方法的正确性。
我寻求正确的指导方向，并倾向于从头开始构建解决方案以掌握底层数学概念，而不是依赖现有模型。
我无法理解的是如何在同一空间中对齐图像向量和相应的文本向量，以便可以将其用于相似性搜索……]]></description>
      <guid>https://stackoverflow.com/questions/78724935/training-neural-network-for-image-text-relevance-with-expert-scores</guid>
      <pubDate>Tue, 09 Jul 2024 10:01:33 GMT</pubDate>
    </item>
    <item>
      <title>可以将已经经过拆分数据阶段、成为训练、验证和测试数据的图像数据集保存到我的计算机存储文件夹中吗？</title>
      <link>https://stackoverflow.com/questions/78724858/can-save-an-image-dataset-that-has-gone-through-the-splitting-data-stage-becomin</link>
      <description><![CDATA[我想问一下我做的训练、验证和测试数据的分布，我可以把数据分布以文件夹的形式保存在存储中吗？可以吗？我希望可以:)
如果你想看完整的代码
https://github.com/cendekialnazalia/CaisimPestDetection/blob/main/Percobaan%20E%20-%20CNN%20add%20Models%20Xception.ipynb
我想下载测试数据部分，即&quot;test_gen&quot;或测试数据集。我希望有人能用一个代码来回答我的问题，这个代码可以将数据保存到我的电脑中，而不必从现有的数据集集合中逐个搜索图像数据
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78724858/can-save-an-image-dataset-that-has-gone-through-the-splitting-data-stage-becomin</guid>
      <pubDate>Tue, 09 Jul 2024 09:41:58 GMT</pubDate>
    </item>
    <item>
      <title>当我通过 docker-compose.yml 运行镜像 ollama 时，无法正确运行它</title>
      <link>https://stackoverflow.com/questions/78724837/i-cannot-run-the-image-ollama-correctly-when-i-run-it-through-docker-compose-yml</link>
      <description><![CDATA[我正在做一个项目，分析文本信息以从中提取特定数据。Python 中的正则表达式效果不佳，因为文本格式不断变化且没有一致性。因此，我决定使用语言模型来处理这些文本，如果文本包含我感兴趣的内容，则返回结果。
在开发程序时，我使用以下命令运行模型：
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
之后，我在代码中发送了一个请求，如下所示：
url = &#39;http://localhost:11434/api/generate&#39;
data = { 
&quot;model&quot;: &quot;llama3&quot;,
&quot;prompt&quot;: f&quot;{input_text}&quot;,
}

这有效（尽管响应需要一点时间才能完成）生成）。
现在，当我尝试配置我的 docker-compose.yml 文件以启动语言模型容器时，它看起来像这样：
ollama:
container_name: ollama
image: ollama/ollama
volumes:
- ollama:/root/.ollama
ports:
- &quot;11434:11434&quot;

volumes:
ollama:

但我只收到 404 错误，这意味着找不到端点。我不明白我做错了什么。有人可以帮忙吗？
此外，有人知道语言模型是否支持多线程吗？我的脚本发送文本非常快，我不确定是否要限制向语言模型发送请求的速率，或者它是否可以处理多线程和异步请求。]]></description>
      <guid>https://stackoverflow.com/questions/78724837/i-cannot-run-the-image-ollama-correctly-when-i-run-it-through-docker-compose-yml</guid>
      <pubDate>Tue, 09 Jul 2024 09:38:52 GMT</pubDate>
    </item>
    <item>
      <title>Keras Tensorflow load_model 函数需要很长时间才能加载模型</title>
      <link>https://stackoverflow.com/questions/78724780/keras-tensorflow-load-model-function-taking-forever-to-load-a-model</link>
      <description><![CDATA[我使用 tensorflow 训练了一个模型（用于识别面部），然后将其保存为“facetracker.h5”。但是，当我尝试加载该模型时，它只是继续加载“[*]”，如下图所示，并且实际上从未完成加载。该模型 (facetracker.h5) 只有 68 MB，所以我是否可以认为这种情况不是由于其大小而发生的？：

facetracker 如下所示：

如能提供任何帮助，我们将不胜感激。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78724780/keras-tensorflow-load-model-function-taking-forever-to-load-a-model</guid>
      <pubDate>Tue, 09 Jul 2024 09:28:15 GMT</pubDate>
    </item>
    <item>
      <title>强化学习代理没有采取现实行动</title>
      <link>https://stackoverflow.com/questions/78724550/reinforcement-learning-agent-not-taking-realistic-actions</link>
      <description><![CDATA[我在 Simulink 环境中使用 PPO 代理，但代理产生的操作似乎是离散的。具体来说，代理仅输出上限或下限。您知道为什么会发生这种情况吗？我正在使用 RL Toolbox 进行训练。
以下是有关我的设置的一些详细信息：
我正在使用带有 ode23t 求解器的可变步长 Simulink 模型。
我的 Simulink 模型使用 Simscape 热流体库并模拟简化的区域供热网络。DHN 有 2 个分支：北 (NORD) 和南 (SUD)。
我正在尝试使用 RL 代理来优化控制，最初专注于通过改变分支中的质量流量来最大限度地降低能源成本。
关于代理的超参数，我使用的是 RL Toolbox，其参数如下：
采样时间 = 3600
折扣因子 = 0.99
GPU
批量大小 = 512
学习率 = 1e-3（对于演员和评论家）
我怀疑我的模型或代理可能存在问题。我将附上 Simulink 模型（应事先加载属性表）。希望问题清楚，有人可以提供帮助！
​​提前谢谢您！
我尝试更改超参数，但没有任何变化
function reward = computeReward(EBio, EGaz, Taller,Tset, Tr,Demandes,production, penalty1,penalty2)

coutBiomass = 0.04 * EBio;
coutGas = 0.1 * EGaz;

%exp(-(Tr - minTemp) / minTemp);

tempDeviation = penalty1 * abs(Taller-Tset);

unmetDemand = penalty1 * max(0, Demandes - production);

minTemp=penalty2 * exp(-(Tr - 318) / 318);

reward = - (coutBiomass + coutGas + tempDeviation + unmetDemand+minTemp);
end

obs = rlNumericSpec([8 1]);
act = rlNumericSpec([2 1],&quot;LowerLimit&quot;,-1,&quot;UpperLimit&quot;,1);
agent=rlTD3Agent(obs,act);
env=rlSimulinkEnv(&quot;Quatrieme_Configuration_SansSolaire_RL_Training&quot;,&quot;Quatrieme_Configuration_SansSolaire_RL_Training/RL Agent&quot;,obs,act);
env.ResetFcn=@randomstart; 
env.UseFastRestart=&quot;on&quot;; 
TimeDelay=0.1; 
]]></description>
      <guid>https://stackoverflow.com/questions/78724550/reinforcement-learning-agent-not-taking-realistic-actions</guid>
      <pubDate>Tue, 09 Jul 2024 08:41:06 GMT</pubDate>
    </item>
    <item>
      <title>如何使用混淆矩阵可视化预测样本</title>
      <link>https://stackoverflow.com/questions/78719068/how-to-visualize-predicted-samples-using-a-confusion-matrix</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78719068/how-to-visualize-predicted-samples-using-a-confusion-matrix</guid>
      <pubDate>Mon, 08 Jul 2024 04:07:24 GMT</pubDate>
    </item>
    <item>
      <title>Darknet Yolov4-tiny（灰度输入）到 Tensorflow 权重，转换</title>
      <link>https://stackoverflow.com/questions/78711847/darknet-yolov4-tiny-grayscale-input-to-tensorflow-weights-conversion</link>
      <description><![CDATA[TL;DR:
1 通道 TF 模型的行为与 3 通道模型不同。两者都成功从 Darknet -&gt; TF 转换，但 1 通道模型的表现不如转换前。
手头的任务和声明：
我有两个经过训练的 yolov4-tiny darknet 权重文件 (.weights)，一个有灰度输入（1 通道），另一个有颜色输入（3 通道）。我正在将两个权重文件转换为 Tensorflow 检查点格式，使用一个通用存储库（用于此任务），该存储库位于：
https://github.com/hunglc007/tensorflow-yolov4-tflite.git
两种模型的性能都已通过 c++ opencv readNetFromDarknet() 和 Python 等效项进行了测试。这两个模型都是用灰度图像进行训练的，并且对灰度图像进行操作。3 通道模型的输入只是缩放到 3 通道的灰度图像。
Python 版本：3.10.11
TF 版本：2.10.1
问题陈述：
使用 tf.keras.Models.load_model(X) 加载时，带有颜色输入的权重文件转换良好，之后运行良好，但是当转换灰度输入权重文件时，使用 Tensorflow 加载时模型的性能急剧下降，我的意思是在最明显的情况下，检测结果很差或不存在，而带有颜色输入的模型运行完美。值得注意的是，框不会错位，这意味着当发现检测结果时，它们大致处于正确的位置，但例如宽度和高度可能会偏离。
我知道这个存储库的常见问题（硬编码内容等），并相应地更改了每次转换/模型加载的参数，并且在转换或模型加载期间不会发生任何错误。
我已经确认了输入层：

灰度：（无，640,640,1）
颜色：（无，640,640,3）

测试图像（用于性能测试）使用 opencv-python 加载，并且它们的有效性也已审查，即使将错误维度的数据插入到输入层也会出现错误。
除输入层之外的架构相同，已使用 model.summary() 确认。
我注意到，几年前我用不同的 TF 版本转换的 3 通道模型由 model.summary() 生成的架构有些不同。一些图块层似乎缺失了。此外，一些 tf 操作的名称也不同，但这可能只是 TF 版本不同。
旧颜色模型：
 tf_op_layer_Sigmoid (TensorFlo (None, 40, 40, 3, 2 0 [&#39;tf_op_layer_split_3[0][0]&#39;]
wOpLayer) )

tf_op_layer_Tile/multiples (Te (5,) 0 [&#39;tf_op_layer_strided_slice[0][0]
nsorFlowOpLayer) &#39;]

tf_op_layer_Sigmoid_3 (TensorF (None, 20, 20, 3, 2 0 [&#39;tf_op_layer_split_4[0][0]&#39;]
lowOpLayer) ) )

新灰度模型：
 tf.math.sigmoid (TFOpLambda) (无，40，40，3，2 0 [&#39;tf.split_3[0][0]&#39;]
)

---此处缺少图块层---

tf.math.sigmoid_3 (TFOpLambda) (无，20，20，3，2 0 [&#39;tf.split_4[0][0]&#39;]
)

我现在很卡。有什么帮助吗？
一些反复试验：

使用 Yolov4-tiny Head 解码块 -&gt;即使在模型能够加载的情况下也没有变化（解码时错误的尺寸会引发错误）
之前提到的较旧的 3 通道模型（几年前已转换为 Darknet -&gt; TF），当以与新模型相同的方式加载时，可以完美运行

解决方案：
我从 c++ 打印了带有形状的图层，因为模型在那里工作，我很想知道形状是否完全不同。第一个卷积层根本没有缩小。我检查了 Darknet 用于解析模型的 .cfg 文件，第一个卷积层上有一行“size=3stride=2”。大小解析正确，但步幅解析不正确，并且默认为 1，这使得第一个卷积层的形状为 (640,640,32)，而不是预期的（正确的）(320,320,32)。当我将权重加载到正确的架构时，它当然表现不佳，因为它是用另一种布局训练的。我怀疑我用没有这个错误的 .cfg 训练了 3 通道模型，这就是它完美运行的原因。]]></description>
      <guid>https://stackoverflow.com/questions/78711847/darknet-yolov4-tiny-grayscale-input-to-tensorflow-weights-conversion</guid>
      <pubDate>Fri, 05 Jul 2024 13:54:37 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Spark 中获取 spark.ml NaiveBayes 概率向量而非 [0-1] 类？</title>
      <link>https://stackoverflow.com/questions/65653286/how-to-get-spark-ml-naivebayes-probability-vector-not-0-1-class-in-spark</link>
      <description><![CDATA[我正在研究 NaiveBayes 分类器，我可以使用训练后的模型预测单个数据点的值，但我想获取概率值。
数据仅分为两类。并且预测函数返回 0 或 1。
import org.apache.log4j.{Level, Logger}
import org.apache.spark.ml.classification.{NaiveBayes, NaiveBayesModel}
import org.apache.spark.ml.feature.LabeledPoint
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.sql.SparkSession

object Test {
def main(args: Array[String]): Unit = {
Logger.getLogger(&quot;org&quot;).setLevel(Level.OFF)
Logger.getLogger(&quot;akka&quot;).setLevel(Level.OFF)
val spark = SparkSession.builder.appName(&quot;Test&quot;).master(&quot;local[4]&quot;).getOrCreate
val dataset = spark.read.option(&quot;inferSchema&quot;, &quot;true&quot;).csv(&quot;data/labelled.csv&quot;).toDF()

import spark.sqlContext.implicits._
val output = dataset.map(row =&gt; {
LabeledPoint(row.getInt(2), Vectors.dense( row.getInt(0) , row.getInt(1)))
})
val Array(training, test) = output.randomSplit(Array(0.7, 0.3),seed = 11L)
training.cache()

val model : NaiveBayesModel = new NaiveBayes().fit(training)
val speed = 110
val hour = 11
val label1 : Double = model.predict(Vectors.dense(speed,hour))
// 更新
val label = model.predictProbability(Vectors.dense(speed,hour)) // 这不起作用并引发错误[1]
}
}

[1] 使用 model.predictProbability 时引发的错误

错误：(24, 23) 类 ProbabilisticClassificationModel 中的方法 predictProbability 无法在 org.apache.spark.ml.classification.NaiveBayesModel 中访问。无法访问受保护的方法 predictProbability，因为封闭的对象 Test 不是包分类中类 ProbabilisticClassificationModel 的子类，其中定义了目标 val label = model.predictProbability(Vectors.dense(speed,hour))
]]></description>
      <guid>https://stackoverflow.com/questions/65653286/how-to-get-spark-ml-naivebayes-probability-vector-not-0-1-class-in-spark</guid>
      <pubDate>Sun, 10 Jan 2021 12:30:28 GMT</pubDate>
    </item>
    <item>
      <title>在 scikit learn 中实现自定义损失函数</title>
      <link>https://stackoverflow.com/questions/54267745/implementing-custom-loss-function-in-scikit-learn</link>
      <description><![CDATA[我想在 scikit learn 中实现自定义损失函数。我使用以下代码片段：
def my_custom_loss_func(y_true,y_pred):
diff3=max((abs(y_true-y_pred))*y_true)
return diff3

score=make_scorer(my_custom_loss_func,greater_ is_better=False)
clf=RandomForestRegressor()
mnn= GridSearchCV(clf,score)
knn = mnn.fit(feam,labm) 

传递给 my_custom_loss_func 的参数应该是什么？我的标签矩阵称为 labm。我想计算实际输出与预测输出（由模型）乘以真实输出之间的差值。如果我使用 labm 代替 y_true，那么我应该使用什么代替 y_pred？]]></description>
      <guid>https://stackoverflow.com/questions/54267745/implementing-custom-loss-function-in-scikit-learn</guid>
      <pubDate>Sat, 19 Jan 2019 13:47:47 GMT</pubDate>
    </item>
    </channel>
</rss>