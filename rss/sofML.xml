<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 07 Apr 2024 15:13:47 GMT</lastBuildDate>
    <item>
      <title>梯度下降：缩减特征集的运行时间比原始特征集更长</title>
      <link>https://stackoverflow.com/questions/78288109/gradient-descent-reduced-feature-set-has-a-longer-runtime-than-the-original-fea</link>
      <description><![CDATA[我尝试用 Python 实现梯度下降算法来解决机器学习问题。我正在使用的数据集已经过预处理，并且在比较两个数据集（一个具有原始特征，另一个数据集使用奇异值分解（SVD）减少了前一组的维度）时，我在运行时观察到了意外的行为。我始终如一观察到，与减少的数据集相比，较大的原始数据集的梯度下降算法的运行时间较低，这与我的预期相反。鉴于数据集较小，缩减后的数据集的运行时间是否应该更短？我试图理解为什么会发生这种情况。
以下是相关代码片段：
导入时间
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt

def h_theta(X1, theta1):
    # 假设函数的实现
    返回 np.dot(X1, theta1)

def j_theta(X1, y1, theta1):
    # 成本函数的实现
    返回 np.sum((h_theta(X1, theta1) - y1) ** 2) / (2 * X1.size)

def grad(X1, y1, θ):
    # 梯度计算
    梯度 = np.dot(X1.T, h_theta(X1, theta) - y1) / len(y1)
    返回梯度

def 梯度下降(X1, y1):
    theta_initial = np.zeros(X1.shape[1]) # 用零初始化 theta
    迭代次数 = 1000
    学习率 = [0.1, 0.01, 0.001]
    成本迭代 = []
    θ值 = []
    开始 = 时间.time()
    对于 Learning_rates 中的 alpha：
        theta = theta_initial.copy()
        成本历史 = []
        对于范围内的 i(num_iterations)：
            梯度 = grad(X1, y1, θ)
            theta = theta - np.dot(alpha, 梯度)
            成本 = j_theta(X1, y1, θ)
            cost_history.append(成本)
        cost_iterations.append(cost_history)
        theta_values.append(theta)
    结束 = 时间.time()
    print(f&quot;所用时间：{end - start} 秒&quot;)
    图, axs = plt.subplots(len(learning_rates), Figsize=(8, 15))
    对于 i，枚举中的 alpha（学习率）：
        axs[i].plot(范围(num_iterations), cost_iterations[i], label=f&#39;alpha = {alpha}&#39;)
        axs[i].set_title(f&#39;学习率：{alpha}&#39;)
        axs[i].set_ylabel(&#39;成本 J&#39;)
        axs[i].set_xlabel(&#39;迭代次数&#39;)
        axs[i].legend()
    plt.tight_layout()
    plt.show()

# 使用 SVD 将 X 减少到 3 个特征（列）的代码：
# 对 X 进行奇异值分解并将其减少到 3 列
U、S、Vt = np.linalg.svd(X_归一化)
# 将 X 减少到 3 列
X_reduced = np.dot(X_normalized, Vt[:3].T)

# 打印 X_reduced 的前 5 行
print(&quot;X_reduced 的前 5 行：&quot;)
# 标准化 X_reduced
X_reduced = (X_reduced - np.mean(X_reduced, axis=0)) / np.std(X_reduced, axis=0)

print(&quot;减少和归一化后 X 的均值和标准差：\n&quot; ,X_reduced.mean(axis=0), X_reduced.std(axis=0))
# 打印缩小后的 X 的形状以确认它只有 3 个特征
print(&quot;X_reduced 的形状：&quot;, X_reduced.shape)

# 将截距列添加到 X_reduced
X_reduced_with_intercept = np.hstack((intercept_column, X_reduced))


# 用法示例
# X_normalized_with_intercept 和 y_normalized 表示原始数据集
# X_reduced_with_intercept 和 y_normalized 表示缩减后的数据集

# 对原始数据集进行梯度下降
梯度下降（X_normalized_with_intercept，y_normalized）

# 对缩减后的数据集执行梯度下降
梯度下降（X_reduced_with_intercept，y_归一化）

在我的梯度下降实现中，与完整数据集相比，什么可能导致缩减数据集始终具有更长的运行时间？任何有关故障排除的见解或建议将不胜感激。
我尝试重写和审查我的实现，但似乎对于大多数学习率和迭代次数的增加，较大功能集的运行时间低于其 SVD 子集。]]></description>
      <guid>https://stackoverflow.com/questions/78288109/gradient-descent-reduced-feature-set-has-a-longer-runtime-than-the-original-fea</guid>
      <pubDate>Sun, 07 Apr 2024 14:13:17 GMT</pubDate>
    </item>
    <item>
      <title>内核形状必须与输入具有相同的长度，但接收形状为 (3, 3, (None, 7, 7, 512), 64) 的内核和形状为 [(None, 7, 7, 512)] 的输入</title>
      <link>https://stackoverflow.com/questions/78287794/kernel-shape-must-have-the-same-length-as-input-but-received-kernel-of-shape-3</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78287794/kernel-shape-must-have-the-same-length-as-input-but-received-kernel-of-shape-3</guid>
      <pubDate>Sun, 07 Apr 2024 12:27:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pytorch 进行高效的成对采样</title>
      <link>https://stackoverflow.com/questions/78287754/efficient-pair-sampling-with-pytorch</link>
      <description><![CDATA[我有一个包含大约 150k 图像和 150k 音频的数据集。每个图像，都有相应的音频。我希望我的网络能够学习使用图像数据将一个音频映射到另一个音频。
当我创建这些源目标对时，我的目标是从训练数据的子集中采样的，并且还必须满足特定的阈值标准。然后创建源目标对进行训练。
我现在这样做的方式是导出目标采样的子集并在自定义数据集的 getitem() 中应用阈值。这花费了太长的时间并在训练过程中造成了瓶颈。
我不确定如何解决这个问题并加快训练速度。我该如何解决这个问题。
def _getitem_(idx):
   sourceimg、sourceaudio = self.data[idx]
   Target_dataset = create_subset(sourceimg, sourceaudio)
   迭代次数 = 0
   虽然正确：
     targetimg, targetaudio = np.random(Target_dataset)
     如果 cal_value(target_img) &gt;= 阈值：
       返回sourceimg、sourceaudio、targetimg、targetaudio
     迭代+=1
     如果迭代次数 &gt;= 10：
       _getitem_(random.randint(0, len(self.dataset))

我在训练期间尝试过这样做。但就我拥有的数据量而言，这根本没有效率。如何在不增加数据加载器中的批处理大小或 num_worker 的情况下加快训练速度]]></description>
      <guid>https://stackoverflow.com/questions/78287754/efficient-pair-sampling-with-pytorch</guid>
      <pubDate>Sun, 07 Apr 2024 12:16:21 GMT</pubDate>
    </item>
    <item>
      <title>如何修复CNN-LSTM架构的输入形状？</title>
      <link>https://stackoverflow.com/questions/78287568/how-to-fix-input-shape-cnn-lstm-architecture</link>
      <description><![CDATA[CNN和LSTM的架构代码
我想创建 CNN-LSTM 。我在 keras 中遇到了 Flatten() 问题，形状不匹配导致模型无法运行所有数据。
数据形状 = (96,2)
def create_model_architecture():
    model_cnn = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;,
                               输入形状=输入数据形状），
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Conv1D（过滤器=64，
                               内核大小=3，
                               激活=&#39;relu&#39;),
        tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding=“相同”),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.LSTM(32, return_sequences=True),
        tf.keras.layers.LSTM(16),
        tf.keras.layers.Reshape((-1,16)),
        #tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1, 激活=&#39;sigmoid&#39;)
    ]）
    返回 model_cnn

def create_model():
    tf.random.set_seed(51)

    model_create = create_model_architecture()
    #model_create = create_LSTM_model()
    model_create.compile(loss=tf.keras.losses.Huber(),
                  优化器=tf.keras.optimizers.Adam(learning_rate=0.001),
                  指标=[“mse”])
    返回模型_创建

模型创建 = 创建模型()

model_create.summary()


model_create.fit（train_dataset，epochs=5，batch_size=128）

错误：
&lt;前&gt;&lt;代码&gt;
纪元 1/5
    178/未知 9s 40ms/步 - 损耗：3.5391 - mse：18.2760
-------------------------------------------------- ------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
[213] 单元格，第 4 行
      2 is_train = True
      3 如果是_train：
----&gt; 4 model_create.fit（train_dataset，epochs=5，batch_size=128）
      5 #其他：
      6 #model.fit(train_dataset,epochs =200,batch_size = 512)
无法将张量添加到批次中：元素数量不匹配。形状为：[张量]：[85,2]，[批次]：[96,2]
     [[{{node IteratorGetNext}}]] [操作：__inference_one_step_on_iterator_64945]


为了修复 Flatten()，我尝试使用 Reshape()，但是模型拟合无法运行 5 个时期。如何修复形状以使模型拟合运行 5 个时期？]]></description>
      <guid>https://stackoverflow.com/questions/78287568/how-to-fix-input-shape-cnn-lstm-architecture</guid>
      <pubDate>Sun, 07 Apr 2024 11:06:09 GMT</pubDate>
    </item>
    <item>
      <title>如何在不始终拥有用户 ID 的情况下根据多个标准推荐增强功能？</title>
      <link>https://stackoverflow.com/questions/78287221/how-to-recommend-enhancements-based-on-multiple-criteria-without-always-having-u</link>
      <description><![CDATA[我正在开发一个推荐系统，该系统根据按摩名称、服务长度、中心名称以及偶尔的用户详细信息向用户建议增强功能（附加服务）。但是，我遇到了一些需要帮助的挑战。
涉及的数据集结构如下：
用户数据集：包含 USER_ID、AGE、GENDER、ZIPCODE 和 BASE_CENTER。
项目数据集：包含ITEM_ID和ITEM_NAME（代表增强）。
交互数据集：包括 USER_ID、ITEM_ID、TIMESTAMP、SERVICE_LENGTH、MASSAGE_NAME、CENTER_NAME 和 EVENT_TYPE。
我的系统的一个值得注意的方面是，每个增强功能都可以与多个按摩名称和中心名称相关联。以下是我面临的具体问题：
用户 ID 缺失：在很多情况下，我没有 USER_ID。在推荐过程中处理此类情况的最佳实践是什么？我应该默认使用通用用户配置文件，还是有更复杂的方法来保持个性化而无需用户识别？
不同中心的建议不一致：我发现不同的中心有时会产生相同的增强建议，尽管我预计中心名称会影响建议的多样性。此外，用户年龄、性别和中心名称的变化似乎并没有像预期那样改变结果。
不同输入的相同结果：无论年龄、性别和中心名称输入如何变化，系统都倾向于推荐相同的增强功能。我正在寻找策略来使结果多样化并使建议对这些输入变量更加敏感。
问题：
如何改进我的推荐系统以有效处理没有用户 ID 的实例，确保一定程度的个性化？
我可以采用哪些策略或模型来使我的系统对用户详细信息（年龄、性别）和中心名称的变化更加敏感，从而提供更加多样化和相关的增强建议？
任何有关如何应对这些挑战的见解或建议将不胜感激。
尝试更改架构，但输出没有太大差异]]></description>
      <guid>https://stackoverflow.com/questions/78287221/how-to-recommend-enhancements-based-on-multiple-criteria-without-always-having-u</guid>
      <pubDate>Sun, 07 Apr 2024 08:59:41 GMT</pubDate>
    </item>
    <item>
      <title>我如何将图像数据集上传到 vs code [关闭]</title>
      <link>https://stackoverflow.com/questions/78287042/how-can-i-upload-an-image-dataset-to-vs-code</link>
      <description><![CDATA[我在 google colab 中制作了一个用于植物病害检测的机器学习模型，我想在 VS Code 中运行它，所以我想将图像数据集导入到 VS Code 中，我该怎么做
我已将代码从 colab 复制到 VS Code，现在我想导入数据集]]></description>
      <guid>https://stackoverflow.com/questions/78287042/how-can-i-upload-an-image-dataset-to-vs-code</guid>
      <pubDate>Sun, 07 Apr 2024 07:48:39 GMT</pubDate>
    </item>
    <item>
      <title>如何为 CNN 机器学习模型制作前端和后端</title>
      <link>https://stackoverflow.com/questions/78286791/how-to-make-a-frontend-and-a-backend-for-a-cnn-machine-learning-model</link>
      <description><![CDATA[我制作了一个机器学习模型来检测马铃薯植株上的疾病。现在我想为模型创建一个前端，从用户那里获取图像并预测输出，同时我还想要一个后端来存储数据。另外我将如何集成前端和后端。
我想知道如何创建一个后端来存储数据。以及如何整合前端和后端。]]></description>
      <guid>https://stackoverflow.com/questions/78286791/how-to-make-a-frontend-and-a-backend-for-a-cnn-machine-learning-model</guid>
      <pubDate>Sun, 07 Apr 2024 05:53:18 GMT</pubDate>
    </item>
    <item>
      <title>将 Python 推荐系统连接到 Laravel 应用程序</title>
      <link>https://stackoverflow.com/questions/78286410/connecting-python-recommendation-system-to-laravel-application</link>
      <description><![CDATA[我使用 Python 和余弦相似度算法开发了一个基本的推荐系统。现在，我有兴趣创建一个 Laravel 应用程序来集成这个推荐系统。但是，我不确定如何在两者之间建立联系。
任何帮助将不胜感激！
我没有找到任何可以开始的东西！！]]></description>
      <guid>https://stackoverflow.com/questions/78286410/connecting-python-recommendation-system-to-laravel-application</guid>
      <pubDate>Sun, 07 Apr 2024 01:54:56 GMT</pubDate>
    </item>
    <item>
      <title>所有模型的训练、验证集和测试集的 F1 分数、精确度和召回率均较高</title>
      <link>https://stackoverflow.com/questions/78286020/high-f1-score-precision-and-recall-on-training-validation-set-and-test-set-on</link>
      <description><![CDATA[我正在研究一个关于 Kaggle。有一些特征，例如燃料消耗、燃料等。我尝试根据该数据集进行分类任务，将排放量高于 255 定义为不可接受 (1)，低于可接受 (0)。数据不平衡，可接受类有13269个数据，不可接受类有9287个数据。我尝试使用不同的分类模型，例如随机森林分类器、决策树分类器和逻辑回归，所有这些模型在训练集、验证集甚至测试集上都实现了接近 1 的 f1 分数，这看起来很奇怪。数据没有缺失值或空值，并且在将其输入模型之前由标准定标器进行标准化。
在此处输入图像说明在此处输入图像描述
我的第一个假设是当所有模型都这样执行时数据泄漏。我多次检查了代码，甚至用函数检查了数据集，训练集和测试集之间没有重复的行。我尝试使用所有特征，然后使用随机森林发现最相关的一些特征，例如“COMB（L/100 km）”、“COMB（mpg）”、“燃油消耗”、“HWY（L/100 km）” 100 公里）”、“气缸”、“发动机尺寸”和他们玩了一下，但在所有精确度、召回率和 f1 上仍然获得了高分。我通过偶然从较大的类中删除一些数据来平衡数据集。我使用网格搜索制作模型，因此我尝试通过定义不同的参数网格以及手动定义来使模型更加复杂和简单。我还通过阈值检查了精度和召回率。
在此处输入图片说明
在此处输入图片描述]]></description>
      <guid>https://stackoverflow.com/questions/78286020/high-f1-score-precision-and-recall-on-training-validation-set-and-test-set-on</guid>
      <pubDate>Sat, 06 Apr 2024 21:57:10 GMT</pubDate>
    </item>
    <item>
      <title>我认为我的模型过度拟合？有什么建议么？</title>
      <link>https://stackoverflow.com/questions/78285324/i-think-my-model-is-overfitting-any-suggestions</link>
      <description><![CDATA[所以我正在使用来自kaggle的deepfake检测数据集，我的模型似乎过度拟合，大约有2000张图像，其中1k用于“真实”类，1k用于“假”类。所有图像都是面孔。我使用的是 VGG16，因为它众所周知适合深度伪造面部检测。
以下是过去 10 个周期的结果：
找到属于 2 个类别的 1632 个图像。
找到属于 2 个类别的 204 张图像。
找到属于 2 个类别的 205 张图像。
纪元 1/10
51/51 [==============================] - 1234s 24s/步 - 损失：0.1841 - 准确度：0.9577 - val_loss ：0.7258 - val_accuracy：0.6719
纪元 2/10
51/51 [================================] - 1168s 23s/步 - 损失：0.1397 - 准确度：0.9712 - val_loss ：0.7331 - val_accuracy：0.6406
纪元 3/10
51/51 [================================] - 1186s 23s/步 - 损失：0.1215 - 准确度：0.9743 - val_loss ：0.7938 - val_accuracy：0.6719
纪元 4/10
51/51 [==============================] - 1187s 23s/步 - 损失：0.0914 - 准确度：0.9884 - val_loss ：0.8304 - val_accuracy：0.6615
纪元 5/10
51/51 [================================] - 1188s 23s/步 - 损失：0.0705 - 准确度：0.9939 - val_loss ：0.9022 - val_accuracy：0.6562
纪元 6/10
51/51 [================================] - 1155s 23s/步 - 损失：0.0683 - 准确度：0.9896 - val_loss ：0.9072 - val_accuracy：0.6354
纪元 7/10
51/51 [==============================] - 1154s 23s/步 - 损失：0.0541 - 准确度：0.9951 - val_loss ：0.8924 - val_accuracy：0.6719
纪元 8/10
51/51 [==============================] - 1175s 23s/步 - 损失：0.0403 - 准确度：0.9975 - val_loss ：0.9353 - val_accuracy：0.6510
纪元 9/10
51/51 [==============================] - 1189s 23s/步 - 损失：0.0420 - 准确度：0.9969 - val_loss ：1.0692 - val_accuracy：0.6198
纪元 10/10
51/51 [==============================] - 1185s 23s/步 - 损失：0.0288 - 准确度：0.9988 - val_loss ：1.0250 - val_accuracy：0.6510
/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103：UserWarning：您正在通过“model.save()”将模型保存为 HDF5 文件。此文件格式被视为旧格式。我们建议使用原生 Keras 格式，例如`model.save(&#39;my_model.keras&#39;)`。
  saving_api.save_model(
6/6 [================================] - 135s 21s/步 - 损失：1.1160 - 准确度：0.6042
测试精度：0.6041666865348816，结果如下

如果有人想知道，这里是过去 10 个时期的代码：
从 google.colab 导入驱动器
导入操作系统
将张量流导入为 tf
从tensorflow.keras.preprocessing.image导入ImageDataGenerator
从tensorflow.keras.models导入load_model

train_dir = &#39;/content/drive/MyDrive/dataset/train&#39;
val_dir = &#39;/content/drive/MyDrive/dataset/val&#39;
test_dir = &#39;/content/drive/MyDrive/dataset/test&#39;

＃参数
批量大小 = 32
历元 = 5
图像形状 = (224, 224)

save_model_path = &#39;/content/drive/MyDrive/dataset/trained_model_updated.h5&#39;
模型 = load_model(保存的模型路径)

#预处理
train_datagen = ImageDataGenerator(重新缩放=1./255)
val_datagen = ImageDataGenerator(重新缩放=1./255)
test_datagen = ImageDataGenerator（重新缩放=1./255）

train_generator = train_datagen.flow_from_directory(
    火车目录，
    目标大小=图像形状，
    批量大小=批量大小，
    class_mode=&#39;二进制&#39;
）

val_generator = val_datagen.flow_from_directory(
    val_dir,
    目标大小=图像形状，
    批量大小=批量大小，
    class_mode=&#39;二进制&#39;
）

test_generator = test_datagen.flow_from_directory(
    测试目录，
    目标大小=图像形状，
    批量大小=批量大小，
    class_mode=&#39;二进制&#39;
）


model.compile(优化器=&#39;亚当&#39;,
              损失=&#39;binary_crossentropy&#39;,
              指标=[&#39;准确性&#39;])


历史=模型.fit(
    火车发电机，
    steps_per_epoch=train_generator.samples //batch_size,
    纪元=纪元，
    验证数据=val_generator，
    validation_steps=val_generator.samples //batch_size
）


model.save(&#39;/content/drive/MyDrive/dataset/trained_model_updated.h5&#39;)

test_loss, test_acc = model.evaluate(test_generator,steps=test_generator.samples //batch_size)
print(f&#39;测试准确度：{test_acc}&#39;)


我运行了该模型大约 20 个 epoch，并不断更改参数，应用数据增强。尽管该模型对训练数据产生了良好的准确性，但对验证和测试集而言仍停滞在 67% 左右。
在前 5 个 epoch 中，模型的验证准确率确实从 53% 提高到 67%，但在最后 15 个 epoch 中仍然停滞不前。
为了确认，我还尝试使用来自互联网的一些图像，但它没有正确识别这些图像，并且会错误地将一些假图像分类为“真实”。
我很确定它的过度拟合是正确的吗？
我没有计算资源来训练更大的数据集，除了增加数据集大小之外还有其他解决方案吗？任何建议，将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78285324/i-think-my-model-is-overfitting-any-suggestions</guid>
      <pubDate>Sat, 06 Apr 2024 17:38:39 GMT</pubDate>
    </item>
    <item>
      <title>神经网络对不同输入的相同预测</title>
      <link>https://stackoverflow.com/questions/78284988/neural-network-same-prediction-for-different-inputs</link>
      <description><![CDATA[我正在尝试在 Matlab 中构建一个神经网络，而不使用深度学习工具箱，其中一个隐藏层可以预测图像显示的是脑肿瘤还是健康的大脑。我使用的数据库包含 4000 张图像（2000 张脑肿瘤图像和 2000 张健康大脑图像）。
我面临的问题是准确率为 50%，并且每张图像的预测都是相同的。结果，混淆矩阵的一列始终为 0。我尝试更改学习率，尝试更改隐藏层上的神经元数量，但没有任何改变输出。我使用的隐藏层和输出层的激活函数都是 sigmoid，并且使用的优化算法是梯度下降。]]></description>
      <guid>https://stackoverflow.com/questions/78284988/neural-network-same-prediction-for-different-inputs</guid>
      <pubDate>Sat, 06 Apr 2024 15:52:17 GMT</pubDate>
    </item>
    <item>
      <title>如何解释神经网络中隐藏神经元的输出？</title>
      <link>https://stackoverflow.com/questions/78281488/how-to-interpret-the-outputs-of-the-hidden-neurons-in-a-neural-network</link>
      <description><![CDATA[我正在训练一个由 2 个神经元组成的 1 个隐藏层的 FNN：
模型 = train1([2])
绘制拟合以及每个隐藏神经元的输出（分布式表示）时：
plot1(X1, y1, label=&quot;train&quot;)
图1（X1测试，y1测试，标签=“测试”）
plot1fit(torch.linspace(0, 13, 500).unsqueeze(1), 模型, 隐藏=True, 比例=False)

输出如下：

当使用 3 个隐藏神经元进行训练时：

如何解释每个隐藏神经元的可视化和输出的拟合情况？]]></description>
      <guid>https://stackoverflow.com/questions/78281488/how-to-interpret-the-outputs-of-the-hidden-neurons-in-a-neural-network</guid>
      <pubDate>Fri, 05 Apr 2024 17:23:10 GMT</pubDate>
    </item>
    <item>
      <title>在 Kaggle Notebook 中降级 Tensorflow 版本时遇到问题；我应该怎么办</title>
      <link>https://stackoverflow.com/questions/78279273/facing-problem-while-downgrading-tensorflow-version-in-kaggle-notebook-what-sho</link>
      <description><![CDATA[我在tensorflow versino 2.11.0中编写了一个代码，但是最近我的代码无法运行，发现当前的tensorflow版本2.15.0是主要问题，所以我使用代码降级了我的版本！pip install tensorflow- GPU==2.11.0
但是我的笔记本确实找到了任何 GPU，尽管我像以前一样在我的 Kaggle 笔记本中启用了 GPU P100 加速器。我还在代码中检查 GPU。
导入tensorflow为tf

如果 tf.test.gpu_device_name():

print(&#39;默认 GPU 设备：{}&#39;.format(tf.test.gpu_device_name()))

别的：

print(&quot;请安装GPU版本的TF&quot;)

得到了
&lt;前&gt;&lt;代码&gt;
    请安装GPU版本的TF


请在这方面帮助我。我的项目截止日期非常接近
在 Kaggle 笔记本中降级 Tensorflow 版本时遇到问题。]]></description>
      <guid>https://stackoverflow.com/questions/78279273/facing-problem-while-downgrading-tensorflow-version-in-kaggle-notebook-what-sho</guid>
      <pubDate>Fri, 05 Apr 2024 10:28:53 GMT</pubDate>
    </item>
    <item>
      <title>多元线性回归房价r2得分问题</title>
      <link>https://stackoverflow.com/questions/78275121/multiple-linear-regression-house-price-r2-score-problem</link>
      <description><![CDATA[我有样本房价数据和简单代码：
导入 pandas 作为 pd
从 sklearn.preprocessing 导入 LabelEncoder、StandardScaler
从 sklearn.model_selection 导入 train_test_split
从 sklearn. Linear_model 导入 LinearRegression
从 sklearn.metrics 导入 r2_score

数据 = pd.read_csv(&#39;house_price_4.csv&#39;)
df = pd.DataFrame(数据)
df[&#39;区域&#39;] = df[&#39;区域&#39;].str.replace(&#39;,&#39;, &#39;&#39;)
df = df.dropna()

# 对分类特征“地址”进行编码
df[&#39;地址&#39;] = df[&#39;地址&#39;].astype(&#39;类别&#39;).cat.codes
df[&#39;停车&#39;] = df[&#39;停车&#39;].replace({True: 1, False: 0})
df[&#39;仓库&#39;] = df[&#39;仓库&#39;].replace({True: 1, False: 0})
df[&#39;电梯&#39;] = df[&#39;电梯&#39;].replace({True: 1, False: 0})

X = df.drop(columns=[&#39;价格(美元)&#39;,&#39;价格&#39;])
y = df[&#39;价格&#39;]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

模型=线性回归()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

r_squared = r2_score(y_test, y_pred)
print(f&#39;R^2 得分: {r_squared:.4f}&#39;)

                                                                  

我的 R2 分数非常低：0.34
如何获得更高的 R2 分数？
这是我的示例数据：https://drive.google .com/file/d/14Se90XbGJivftq3_VrtgRSalkCplduVX/view?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/78275121/multiple-linear-regression-house-price-r2-score-problem</guid>
      <pubDate>Thu, 04 Apr 2024 16:01:38 GMT</pubDate>
    </item>
    <item>
      <title>Python：GridSearchCV 花费太长时间才能完成运行</title>
      <link>https://stackoverflow.com/questions/72101295/python-gridsearchcv-taking-too-long-to-finish-running</link>
      <description><![CDATA[我正在尝试进行网格搜索来优化我的模型，但执行时间太长。我的总数据集只有大约 15,000 个观察值，大约有 30-40 个变量。我成功地通过 gridsearch 运行了一个随机森林，这花了大约一个半小时，但现在我已经切换到 SVC，它已经运行了 9 个多小时，但仍然没有完成。以下是我的交叉验证代码示例：
从 sklearn.model_selection 导入 GridSearchCV
从 sklearn 导入 svm
从 sklearn.svm 导入 SVC

SVM_Classifier= SVC(random_state=7)



param_grid = {&#39;C&#39;: [0.1, 1, 10, 100],
              ‘伽马’：[1,0.1,0.01,0.001],
              &#39;kernel&#39;: [&#39;线性&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;],
              &#39;度&#39; : [0, 1, 2, 3, 4, 5, 6]}

grid_obj = GridSearchCV(SVM_Classifier,
                        
                        return_train_score=真，
                        参数网格=参数网格，
                        评分=&#39;roc_auc&#39;,
                        简历=3，
                       职位数 = -1)

grid_fit = grid_obj.fit(X_train, y_train)
SVMC_opt = grid_fit.best_estimator_

打印（&#39;=&#39;*20）
print(&quot;最佳参数：&quot; + str(grid_obj.best_estimator_))
print(&quot;最佳参数：&quot; + str(grid_obj.best_params_))
print(&#39;最佳成绩：&#39;, grid_obj.best_score_)
打印（&#39;=&#39;*20）


我已经将交叉验证从 10 个减少到 3 个，并且我使用 n_jobs=-1，因此我正在调动所有核心。我还缺少什么可以在这里做来加快这个过程吗？]]></description>
      <guid>https://stackoverflow.com/questions/72101295/python-gridsearchcv-taking-too-long-to-finish-running</guid>
      <pubDate>Tue, 03 May 2022 14:51:20 GMT</pubDate>
    </item>
    </channel>
</rss>