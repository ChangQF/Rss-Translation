<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 05 Apr 2024 21:12:40 GMT</lastBuildDate>
    <item>
      <title>如何提高基于 Keras 的 CNN 眼底图像分类的训练和测试准确性？</title>
      <link>https://stackoverflow.com/questions/78281500/how-can-i-improve-my-keras-based-cnns-training-and-testing-accuracy-for-fundus</link>
      <description><![CDATA[``我一直致力于使用包含 400 张图像的眼底图像集（来自 MESSIDOR）创建糖尿病视网膜病变分类模型，分级范围为 0-3（4 个类别）。我尝试过使用增强、dropout 和正则化器来更改模型架构、复杂性、输出特征，但我的模型训练和验证准确度似乎无法超过 50%。
这是我的代码：&#39;`
#将数据拆分为训练集和验证集X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, stratify=y)
#定义模型结构
&lt;前&gt;&lt;代码&gt;模型 = 顺序()

#转换层
model.add(Conv2D(64, kernel_size=(3, 3), activate=&#39;relu&#39;, input_shape=(250, 250, 3), kernel_regularizer=regularizers.l2(0.0001)))#输出滤波器为 32)
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (3, 3), 激活=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.001)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(256, (3, 3), 激活=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.001)))
model.add(MaxPooling2D(pool_size=(2, 2)))
模型.add(压平())
#密集层
model.add（密集（64，激活=&#39;relu&#39;））
模型.add(Dropout(0.45))
model.add（密集（4，激活=&#39;softmax&#39;））

`**# 编译模型
**`opt = keras.optimizers.Adam(learning_rate=0.00001)
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=opt, 指标=[&#39;accuracy&#39;])

打印（模型.摘要（））

`***# 数据增强
**`数据生成=图像数据生成器（
    旋转范围=30，
    宽度偏移范围=0.2，
    height_shift_range=0.2，
    水平翻转=真，
    fill_mode=&#39;最近&#39;
）

历史= model.fit（datagen.flow（X_train，y_train，batch_size = 2），epochs = 30，validation_data
（X_测试，y_测试），随机播放=真）

model.evaluate(X_test, y_test)



所附图像的准确性和丢失结果[在此处输入图像描述](https://i.stack.imgur.com/dgbal.png&lt; /a&gt;)]]></description>
      <guid>https://stackoverflow.com/questions/78281500/how-can-i-improve-my-keras-based-cnns-training-and-testing-accuracy-for-fundus</guid>
      <pubDate>Fri, 05 Apr 2024 17:25:53 GMT</pubDate>
    </item>
    <item>
      <title>如何解释神经网络中的分布式表示（隐藏神经元的输出）？</title>
      <link>https://stackoverflow.com/questions/78281488/how-to-interpret-distributed-representationsoutputs-of-the-hidden-neurons-in-a</link>
      <description><![CDATA[训练具有 1 个隐藏层（由 2 个神经元组成）的 FNN：
模型 = train1([2])
绘制每个隐藏神经元的拟合以及输出：
plot1(X1, y1, label=&quot;train&quot;)
图1（X1测试，y1测试，标签=“测试”）
plot1fit(torch.linspace(0, 13, 500).unsqueeze(1), 模型, 隐藏=True, 比例=False)

输出如下：

当使用 3 个隐藏神经元进行训练时：

如何解释图表和每个隐藏神经元的输出的拟合情况？将上述视为分布式表示/嵌入，它真的很直观吗？]]></description>
      <guid>https://stackoverflow.com/questions/78281488/how-to-interpret-distributed-representationsoutputs-of-the-hidden-neurons-in-a</guid>
      <pubDate>Fri, 05 Apr 2024 17:23:10 GMT</pubDate>
    </item>
    <item>
      <title>在神经网络中创建并加载数据集</title>
      <link>https://stackoverflow.com/questions/78281439/creating-and-loading-the-dataset-in-neural-network</link>
      <description><![CDATA[我是深度学习的初学者...我想做图像分类，我的文件夹中有很多图像...
将 numpy 导入为 np
从张量流导入keras
将 matplotlib.pyplot 导入为 plt
导入keras
数据=keras.datasets.fashion_mnist
(train_images,train_labels),(test_images,test_labels)=data.load_data()

火车图像=火车图像/255.0
测试图像=测试图像/255.0


plt.imshow(train_images[0],cmap=plt.cm.binary)
plt.show()

名称=[&#39;T恤&#39;，&#39;裤子&#39;，&#39;套头衫&#39;，&#39;连衣裙&#39;，&#39;外套&#39;，&#39;凉鞋&#39;，&#39;衬衫&#39;，&#39;运动鞋&#39;，&#39;包&#39;，&#39;踝靴&#39;]
模型=keras.Sequential([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(10,activation=“softmax”)
]）
model.compile(optimizer=“adam”,loss=“sparse_categorical_crossentropy”,metrics=[“accuracy”])

model.fit(train_images,train_labels,epochs=5)

预测=模型.预测(test_images)

对于范围 (4) 内的 i：
    plt.网格（假）
    plt.imshow(test_images[i],cmap=plt.cm.binary)
    plt.xlabel(“实际：”+names[test_labels[i]])
    plt.title(“预测：”+names[np.argmax(预测[i])])
    plt.show()


现在上面的程序基本上是从 MNIST 加载数据集...现在我创建了一个文件夹并将图像放入该文件夹中...现在如何加载该数据集并对其进行预处理？
附：我会给出类名。请有人帮助我。
我尝试了几种方法，但我没有得到它..请有人帮助我]]></description>
      <guid>https://stackoverflow.com/questions/78281439/creating-and-loading-the-dataset-in-neural-network</guid>
      <pubDate>Fri, 05 Apr 2024 17:10:33 GMT</pubDate>
    </item>
    <item>
      <title>由于内存违规或花费太长时间而导致测试用例错误</title>
      <link>https://stackoverflow.com/questions/78281410/error-in-the-test-case-due-to-memory-violation-or-it-took-too-long</link>
      <description><![CDATA[我正在做一个程序，结果是正确的，但对于一个案例测试它不起作用，我不知道到底为什么。
原因是由于内存违规或输出结果花费的时间太长。
我的程序是关于机器学习的，我需要根据学生的行为，使用点之间的距离来查看是否获得批准。
#include ;
#include ;
#include ; // 使用malloc
#定义 MAX_SAMPLES 300000
#定义 MAX_STUDENTS 300000

类型定义结构{
    浮动学习时间；
    浮动平均成绩；
    int pass_or_fail;
    浮动距离；
} 样品；

类型定义结构{
    浮动学习时间；
    浮动平均成绩；
} 评估；

// 调整最大堆的函数
void heapify(样本sample_array[], int n, int i) {
    int 最大 = i; // 将最大的初始化为root
    int 左 = 2 * i + 1; // 左孩子的索引
    int 右 = 2 * i + 2; // 右子节点的索引

    // 如果左孩子大于根
    if (left &lt; n &amp;&amp; 样本数组[左].距离&gt; 样本数组[最大].距离)
        最大=左；

    // 如果右孩子大于迄今为止最大的孩子
    if (右 &lt; n &amp;&amp; 样本数组[右].距离 &gt; 样本数组[最大].距离)
        最大=右；

    // 如果最大的不是根
    如果（最大！=我）{
        // 将最大的与根交换
        样本温度=样本数组[i]；
        样本数组[i] = 样本数组[最大];
        样本数组[最大] = 临时；

        // 递归调整受影响的堆
        heapify(sample_array, n, 最大);
    }
}

// 堆排序的主要函数
void heapSort(样本sample_array[], int n) {
    // 构建最大堆
    for (int i = n / 2 - 1; i &gt;= 0; i--)
        heapify（样本数组，n，i）；

    // 从堆中逐个取出元素
    for (int i = n - 1; i &gt; 0; i--) {
        // 将当前根移动到末尾
        样本温度=样本数组[0]；
        样本数组[0] = 样本数组[i];
        样本数组[i] = 临时；

        // 在缩减堆上调用 max heapify
        heapify(sample_array, i, 0);
    }
}

int main() {
    int n_样本；
    int n_students_be_evaluated;
    整数 k；
    int pass_count；
    int 失败计数；

    scanf(“%d %d %d”, &amp;n_samples, &amp;n_students_to_be_evaluated, &amp;k);

   样本sample_array[MAX_SAMPLES]；
   评估的valued_array[MAX_STUDENTS]；

    // 读取样本
    for (int j = 0; j &lt; n_samples; j++) {
        scanf(“%f %f %d”, &amp;sample_array[j].average_grade, &amp;sample_array[j].study_hours, &amp;sample_array[j].pass_or_fail);
    }

    // 读取待评价的学生
    for (int i = 0; i &lt; n_students_to_be_evaluated; i++) {
        scanf(“%f %f”, &amp;evaluated_array[i].average_grade, &amp;evaluated_array[i].study_hours);
    }

    // 加工
    for (int i = 0; i &lt; n_students_to_be_evaluated; i++) {
        通行数=0；
        失败计数=0；

        // 计算距离并使用堆排序进行排序
        for (int j = 0; j &lt; n_samples; j++) {
            样本数组[j].距离 = sqrt(((evaluated_array[i].study_hours - 样本_array[j].study_hours)*(evaluated_array[i].study_hours - 样本_array[j].study_hours)) +
                                              ((evaluated_array[i].average_grade -sample_array[j].average_grade)*(evaluated_array[i].average_grade -sample_array[j].average_grade)));
        }

        heapSort(sample_array, n_samples);

        // 统计最近的k个中通过和失败的数量
        for (int g = 0; g &lt; k; g++) {
            if (sample_array[g].pass_or_fail == 1)
                通过计数++；
            别的
                失败计数++；
        }

        // 检查学生是否通过
        如果（通过计数&gt;失败计数）{
            printf(“学生%d：(%.2f，%.2f) = 通过\n”，i，evaluated_array[i].average_grade，evaluated_array[i].study_hours);
        } 别的 {
            printf(“学生%d：(%.2f，%.2f) = 失败\n”，i，evaluated_array[i].average_grade，evaluated_array[i].study_hours);
        }
    }


    返回0；
}


我尝试动态分配并使用合并排序。还尝试以静态形式增加数组的大小]]></description>
      <guid>https://stackoverflow.com/questions/78281410/error-in-the-test-case-due-to-memory-violation-or-it-took-too-long</guid>
      <pubDate>Fri, 05 Apr 2024 17:04:20 GMT</pubDate>
    </item>
    <item>
      <title>NeuralProphet 中的多变量预测</title>
      <link>https://stackoverflow.com/questions/78281016/multivariate-forecast-in-neuralprophet</link>
      <description><![CDATA[我正在尝试构建一个全局模型来同时预测两个时间序列。下面的代码运行没有错误。但预测数据帧的所有 NaN 都对应于两个 ID 之一（即有两个时间序列）。 yhat 值也全部为 NaN。我做错了什么或遗漏了什么吗？
m = NeuralProphet(
    yearly_seasonality=真，
    week_seasonality=真，
    daily_seasonality=假，
    分位数=分位数，
    n_lags=60,
    纪元=100，
    n_预测=30，
    loss_func=&#39;胡贝尔&#39;,
）
m.set_plotting_backend(&#39;绘图&#39;)
m.highlight_nth_step_ahead_of_each_forecast(step_number=10)

指标 = m.fit(train_df[[&#39;ds&#39;, &#39;y&#39;, &#39;ID&#39;]])

df_future = m.make_future_dataframe(
    火车_df，
    n_historic_predictions=真，
）

预测 = m.predict(df_future)
]]></description>
      <guid>https://stackoverflow.com/questions/78281016/multivariate-forecast-in-neuralprophet</guid>
      <pubDate>Fri, 05 Apr 2024 15:44:18 GMT</pubDate>
    </item>
    <item>
      <title>如何解释 DNN 中验证错误和测试错误之间的巨大差异</title>
      <link>https://stackoverflow.com/questions/78280101/how-can-i-explain-the-huge-difference-between-validation-and-test-errors-in-dnn</link>
      <description><![CDATA[我是 DNN 新手，并尝试了解它们如何在 cifar10 数据集上工作（不使用卷积层）。我使用两种不同的架构：
1.
def create_model(n_layers=5, n_neurons=100, shape=[32, 32, 3]):
    模型 = tf.keras.models.Sequential()

    model.add(tf.keras.layers.Flatten(input_shape=shape))
    对于 _ 在范围内（n_layers）：
        model.add(tf.keras.layers.Dense(n_neurons,
                                 激活＝“selu”，
                                 kernel_initializer=“lecun_normal”））
    model.add（tf.keras.layers.Dense（10，激活=“softmax”））

    优化器 = tf.keras.optimizers.Nadam()
    
    model.compile(loss=“sparse_categorical_crossentropy”,
              优化器=优化器，
              指标=[“准确度”])
    返回模型




model_bn = tf.keras.models.Sequential()

   model_bn.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))
   model_bn.add(tf.keras.layers.BatchNormalization())

   对于范围（5）内的 _：
       model_bn.add(tf.keras.layers.Dense(100,
                                 kernel_initializer=“he_normal”,
                                 kernel_constraint=tf.keras.constraints.max_norm(1.)))
       model_bn.add(tf.keras.layers.BatchNormalization())
       model_bn.add(tf.keras.layers.Activation(“elu”))
   model_bn.add(tf.keras.layers.Dense(10,激活=“softmax”))

   优化器 = tf.keras.optimizers.Nadam(learning_rate=1e-3)
   model_bn.compile(loss=“sparse_categorical_crossentropy”,
              优化器=优化器，
              指标=[“准确度”])

这些模型使用相同的回调来防止过度拟合和长时间训练：
early_stopping_cb = tf.keras.callbacks.EarlyStopping（耐心=6，监视器=“val_loss”）
   lr_scheduler_cb = tf.keras.callbacks.ReduceLROnPlateau（因子=0.5，耐心=3）
   checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(“my_cifar10_model_v1.keras”, save_best_only=True)
   回调 = [early_stopping_cb、checkpoint_cb、tensorboard_cb、lr_scheduler_cb]

但是在拟合结束时，我得到了训练和验证误差之间的巨大差异（大约 10-18%），我只能通过过度拟合来解释这一点，但是 checkpoint_cb 认为这是最好的模型，无论事实如何过度拟合。我是否坚持这些结果，或者我需要获得训练和验证精度差异较小的模型，从而降低最终验证精度。如果是这样，当训练准确性提高时，是否有回调停止，而验证错误保持大致相同？
我尝试设置不同的学习率和耐心参数，但似乎没有任何帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78280101/how-can-i-explain-the-huge-difference-between-validation-and-test-errors-in-dnn</guid>
      <pubDate>Fri, 05 Apr 2024 13:06:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 RNN 未能提高情感项目的准确性</title>
      <link>https://stackoverflow.com/questions/78279557/failure-to-improve-accuracy-in-the-sentiment-project-with-rnn</link>
      <description><![CDATA[希望你一切都好。我正在开发一个名为情感分析的人工智能项目，该项目适用于波斯语数据集。
我一直致力于加载数据，将它们转换为嵌入，然后将它们输入到由 LSTM 组成的神经网络中。然而，在训练过程中，第 1 轮之后准确率并没有提高，并且陷入了困境。
我的网络代码部分是：
https://colab.research.google.com/drive/1Pz20d5r1iZPLvWjHKC2oOfNsNXY1R- TC?usp=共享
纪元[1/20]，损失：1.1605366468429565，准确率：23.5%
Epoch [2/20]，损失：1.0860859155654907，准确率：37.1%
Epoch [3/20]，损失：1.0465837717056274，准确率：48.0%
Epoch [4/20]，损失：1.02091646194458，准确率：51.2%
Epoch [5/20]，损失：1.003448486328125，准确度：52.300000000000004%
Epoch [6/20]，损失：0.9991855621337891，准确率：53.6%
Epoch [7/20]，损失：0.9968012571334839，准确率：52.800000000000004%
Epoch [8/20]，损失：0.9954250454902649，准确率：52.900000000000006%
Epoch [9/20]，损失：0.9897969365119934，准确率：53.1%
Epoch [10/20]，损失：0.9899587631225586，准确率：53.7%
Epoch [11/20]，损失：0.992097020149231，准确率：53.0%
Epoch [12/20]，损失：0.9817440509796143，准确率：53.400000000000006%
]]></description>
      <guid>https://stackoverflow.com/questions/78279557/failure-to-improve-accuracy-in-the-sentiment-project-with-rnn</guid>
      <pubDate>Fri, 05 Apr 2024 11:19:48 GMT</pubDate>
    </item>
    <item>
      <title>可能是什么引发了错误：ValueError：X 有 23 个特征，但 SVR 期望 24 个特征作为输入？</title>
      <link>https://stackoverflow.com/questions/78275238/what-may-be-raising-the-error-valueerror-x-has-23-features-but-svr-is-expecti</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78275238/what-may-be-raising-the-error-valueerror-x-has-23-features-but-svr-is-expecti</guid>
      <pubDate>Thu, 04 Apr 2024 16:21:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 MAPIE 进行保形预测，当 alpha 很大时，我得到空的预测集</title>
      <link>https://stackoverflow.com/questions/78240714/using-mapie-for-conformal-predictions-i-get-empty-predictions-sets-when-alpha-is</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78240714/using-mapie-for-conformal-predictions-i-get-empty-predictions-sets-when-alpha-is</guid>
      <pubDate>Thu, 28 Mar 2024 20:28:12 GMT</pubDate>
    </item>
    <item>
      <title>解决从 Jupyter Notebook 到 .py 文件的自定义管道类转换中的 OneHotEncoder 问题</title>
      <link>https://stackoverflow.com/questions/78219825/troubleshooting-onehotencoder-issue-in-custom-pipeline-class-conversion-from-jup</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78219825/troubleshooting-onehotencoder-issue-in-custom-pipeline-class-conversion-from-jup</guid>
      <pubDate>Mon, 25 Mar 2024 14:32:03 GMT</pubDate>
    </item>
    <item>
      <title>如何为自定义变压器创建 pandas 输出？</title>
      <link>https://stackoverflow.com/questions/75026592/how-to-create-pandas-output-for-custom-transformers</link>
      <description><![CDATA[scikit-learn 1.2.0 中有很多变化，它支持所有变压器的 pandas 输出，但如何在自定义变压器中使用它？
在[1]中：这是我的自定义转换器，它是一个标准缩放器：
从 sklearn.base 导入 BaseEstimator、TransformerMixin
将 numpy 导入为 np

类 StandardScalerCustom（BaseEstimator，TransformerMixin）：
    def fit(self, X, y=None):
        self.mean = np.mean(X, 轴=0)
        self.std = np.std(X, 轴=0)
        返回自我

    def 变换（自身，X）：
        返回 (X - self.mean) / self.std

在 [2] 中：创建了特定的规模管道
scale_pipe = make_pipeline(StandardScalerCustom())

在[3]中：添加到完整的管道中，它可能与缩放器、输入器、编码器等混合。
full_pipeline = ColumnTransformer([
    (“imputer”, impute_pipe, [&#39;column_1&#39;])
    （“缩放器”，scale_pipe，[&#39;column_2&#39;]）
]）

# 来自文档
full_pipeline.set_output(transform=&quot;pandas&quot;)

出现此错误：
ValueError：无法配置 StandardScalerCustom() 的输出，因为 set_output 不可用。
&lt;小时/&gt;
有一个解决方案，它可以是：
set_config(transform_output=&quot;pandas&quot;) 
但是在具体情况的基础上，如何在 StandardScalerCustom() 类中创建一个可以修复上述错误的函数？]]></description>
      <guid>https://stackoverflow.com/questions/75026592/how-to-create-pandas-output-for-custom-transformers</guid>
      <pubDate>Fri, 06 Jan 2023 03:14:45 GMT</pubDate>
    </item>
    <item>
      <title>如何将极坐标数据框与 scikit-learn 一起使用？</title>
      <link>https://stackoverflow.com/questions/74398563/how-to-use-polars-dataframes-with-scikit-learn</link>
      <description><![CDATA[我无法将极坐标数据帧与 scikitlearn 一起使用进行机器学习训练。
目前，我正在极坐标中进行所有数据帧预处理，在模型训练期间，我将其转换为 pandas 数据帧以使其正常工作。
是否有任何方法可以直接使用 Polars 数据帧进行 ML 训练而不将其更改为 pandas？]]></description>
      <guid>https://stackoverflow.com/questions/74398563/how-to-use-polars-dataframes-with-scikit-learn</guid>
      <pubDate>Fri, 11 Nov 2022 05:59:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 Conda + Poetry 有意义吗？</title>
      <link>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</link>
      <description><![CDATA[在机器学习项目中使用 Conda + Poetry 有意义吗？让我分享一下我（新手）的理解，请指正或赐教：
据我了解，Conda 和 Poetry 有不同的目的，但很大程度上是多余的：

Conda 主要是一个环境管理器（实际上不一定是 Python），但它也可以管理包和依赖项。
Poetry 主要是一个 Python 包管理器（例如，pip 的升级版），但它也可以创建和管理 Python 环境（例如，Pyenv 的升级版） .

我的想法是同时使用两者并划分它们的角色：让 Conda 担任环境管理器，让 Poetry 担任包管理器。我的推理是（听起来）Conda 最适合管理环境，可用于编译和安装非 python 包，尤其是 CUDA 驱动程序（用于 GPU 功能），而 Poetry 作为 Python 包管理器比 Conda 更强大。 
通过在 Conda 环境中使用 Poetry，我成功地相当轻松地完成了这项工作。诀窍是不使用 Poetry 来管理 Python 环境：我没有使用诸如 poetry shell 或 poetry run 这样的命令，只使用 poetry init 、poetry install 等（激活Conda环境后）。
为了充分披露，我的 environment.yml 文件（针对 Conda）如下所示：
&lt;前&gt;&lt;代码&gt;名称：N

渠道：
  - 默认值
  - 康达锻造

依赖项：
  - 蟒蛇=3.9
  -cuda工具包
  - 库德恩

我的poetry.toml文件看起来像这样：
&lt;前&gt;&lt;代码&gt;[工具.诗歌]
名称=“N”
作者 = [“B”]

[工具.诗歌.依赖项]
蟒蛇=“3.9”
火炬 =“^1.10.1”

[构建系统]
需要= [“诗歌核心&gt;=1.0.0”]
构建后端=“poetry.core.masonry.api”

说实话，我这样做的原因之一是我在没有 Conda 的情况下很难安装 CUDA（用于 GPU 支持）。
您认为这个项目设计合理吗？]]></description>
      <guid>https://stackoverflow.com/questions/70851048/does-it-make-sense-to-use-conda-poetry</guid>
      <pubDate>Tue, 25 Jan 2022 15:09:43 GMT</pubDate>
    </item>
    <item>
      <title>Keras 损失：0.0000e+00 并且精度保持不变</title>
      <link>https://stackoverflow.com/questions/70589997/keras-loss-0-0000e00-and-accuracy-stays-constant</link>
      <description><![CDATA[我有 101 个文件夹（从 0 到 100），其中包含合成训练图像。
这是我的代码：
数据集 = tf.keras.utils.image_dataset_from_directory(
&#39;图片/synthdataset5&#39;，labels=&#39;推断&#39;，label_mode=&#39;int&#39;，class_names=None，color_mode=&#39;rgb&#39;，batch_size=32，image_size=(128,128)，shuffle=True，seed=None，validation_split=None，子集=无，插值=&#39;双线性&#39;，follow_links=False，crop_to_aspect_ratio=False
）

从 keras.models 导入顺序
从 keras.layers 导入 Dense、Conv2D、Flatten

模型=顺序（）

model.add(Conv2D(32, kernel_size=5, 激活=&#39;relu&#39;, input_shape=(128,128,3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=5, 激活=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128，kernel_size=3，激活=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(256，kernel_size=3，激活=&#39;relu&#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
模型.add(压平())
model.add（密集（1，激活=&#39;sigmoid&#39;））

model.compile(optimizer=&#39;adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])

model.fit(数据集,epochs=75)

每个时期我总是得到相同的结果：
&lt;前&gt;&lt;代码&gt;纪元 1/75
469/469 [================================] - 632s 1s/步 - 损耗：0.0000e+00 - 精度： 0.0098

怎么了？？？]]></description>
      <guid>https://stackoverflow.com/questions/70589997/keras-loss-0-0000e00-and-accuracy-stays-constant</guid>
      <pubDate>Wed, 05 Jan 2022 08:46:51 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我创建新项目时，Unity Visual Studio 无法识别“使用 MlAgents”，但可以在演示项目中识别它？</title>
      <link>https://stackoverflow.com/questions/57019163/why-unity-visual-studio-doesnt-recognise-using-mlagents-when-i-create-a-new-p</link>
      <description><![CDATA[我一直在尝试在我的系统上安装 Unity 的 MLAgents。
阅读详细指南后“https:// /github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation-Windows.md”我成功地让“3D Ball”等演示项目成功运行和训练。
我的问题是，当我创建一个新项目时，当我包含“使用 MlAgents”时，会突出显示一个错误，其中指出“找不到命名空间名称“mlagents”的类型”。
我对 Unity 没有太多经验，所以我希望这是我错过的一件愚蠢的事情，例如您可能必须导入包，但我不知道如何导入？
我发现的所有教程都已经过时了，所以这是我最后的手段。如有任何帮助或建议，我们将不胜感激。
我不明白演示项目如何在“使用 mlagents”时没有错误，但新项目却有错误。]]></description>
      <guid>https://stackoverflow.com/questions/57019163/why-unity-visual-studio-doesnt-recognise-using-mlagents-when-i-create-a-new-p</guid>
      <pubDate>Sat, 13 Jul 2019 12:27:17 GMT</pubDate>
    </item>
    </channel>
</rss>