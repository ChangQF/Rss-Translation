<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 18 Aug 2024 15:14:31 GMT</lastBuildDate>
    <item>
      <title>理解 LSTM torch</title>
      <link>https://stackoverflow.com/questions/78884735/understanding-the-lstm-torch</link>
      <description><![CDATA[大家好，我在训练包含 LSTM 层的模型时遇到了问题。
我怀疑错误可能是由于添加了数据加载器，因为当我使用完整数据集进行训练时，损失减少了。
我将非常感谢您的帮助。
配置：
num_epochs = 200
learning_rate = 1e-1
input_size = 599
hidden_​​size = 100
num_layers = 2
num_classes = 1
batch_size = 32

数据准备：
X_train_t = Variable(torch.Tensor(X_train.values))
y_train_t = Variable(torch.Tensor(y_train))

X_test_t =变量（torch.Tensor（X_test.values））
y_test_t = 变量（torch.Tensor（y_test））

X_train_t_final = torch.reshape（X_train_t，（X_train_t.shape[0]，1，X_train_t.shape[1]））
X_test_t_final = torch.reshape（X_test_t，（X_test_t.shape[0]，1，X_test_t.shape[1]））

print（“训练形状”，X​​_train_t_final.shape，y_train_t.shape）

print（“测试形状”，X​​_test_t_final.shape，y_test_t.shape）

train_dataset = TensorDataset（X_train_t_final，y_train_t）
test_dataset = TensorDataset(X_test_t_final, y_test_t)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)

输出：
训练形状 torch.Size([8000, 1, 599]) torch.Size([8000, 1])
测试形状 torch.Size([998, 1, 599]) torch.Size([998, 1])

模型：
class LSTM1(nn.Module):
def __init__(self, num_classes, input_size, hidden_​​size, num_layers):
super(LSTM1, self).__init__()
self.num_classes = num_classes
self.num_layers = num_layers
self.input_size = input_size
self.hidden_​​size = hidden_​​size

self.lstm = nn.LSTM(input_size=input_size, hidden_​​size=hidden_​​size, num_layers=num_layers, batch_first=True)
self.fc_1 = nn.Linear(hidden_​​size, 128)
self.fc = nn.Linear(128, num_classes)
self.relu = nn.ReLU()

def forward(self,x):
h_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_​​size)
c_0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_​​size)
输出，(hn, cn) = self.lstm(x, (h_0, c_0))
hn = hn[-1]
out = self.relu(hn)
out = self.fc_1(out)
out = self.relu(out)
out = self.fc(out)
返回 out

训练：
lstm1 = LSTM1(num_classes, input_size, hidden_​​size, num_layers)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate) 
scheduler = optim.lr_scheduler.MultiStepLR(optimizer,里程碑=[30, 60, 90, 120, 150, 180]，伽马=0.2)

loss_train，loss_test = []，[]
mean_loss_train，mean_loss_test = 0，0

对于 tqdm 中的 epoch(range(num_epochs))：
对于 train_loader 中的 xb、yb：
optimizer.zero_grad()
输出 = lstm1.forward(xb)
损失 = criterion(输出，yb)
损失.backward()
optimizer.step()
mean_loss_train += loss.item()
损失.append(mean_loss_train / len(train_loader))
scheduler.step()

使用 torch.no_grad()：
对于 test_loader 中的 xb、yb：
输出 = lstm1.forward(xb)
损失 = criterion(输出， yb)
mean_loss_test += loss.item()
loss_test.append(mean_loss_test / len(test_loader))

if (epoch+1) % 20 == 0:
print(f&quot;Epoch: {epoch+1}, loss_train: {round(loss_train[-1], 5)}, loss_test: {round(loss_test[-1], 5)}, lr: {round(scheduler.get_last_lr()[0], 8)}&quot;)
在此处输入代码

学习输出：
Epoch: 20, loss_train: 82.76209, loss_test: 15.35703, lr: 0.1
Epoch: 40, loss_train: 85.63364, loss_test: 18.2109，lr：0.02
纪元：60，loss_train：85.87259，loss_test：18.30187，lr：0.004
纪元：80，loss_train：85.95106，loss_test：18.39842，lr：0.004
纪元：100，loss_train：86.0281，loss_test：18.49544，lr：0.0008
纪元：120，loss_train：86.10304，loss_test：18.58921，lr：0.00016
纪元：140，loss_train：86.21459，loss_test：18.65351，lr： 0.00016
Epoch：160，loss_train：86.32599，loss_test：18.80259，lr：3.2e-05
Epoch：180，loss_train：86.43954，loss_test：19.03469，lr：6.4e-06
Epoch：200，loss_train：86.58578，loss_test：19.16584，lr：6.4e-06

]]></description>
      <guid>https://stackoverflow.com/questions/78884735/understanding-the-lstm-torch</guid>
      <pubDate>Sun, 18 Aug 2024 13:32:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 gym 导致 google colab 页面崩溃（无错误消息）</title>
      <link>https://stackoverflow.com/questions/78884681/crashing-an-google-colab-page-using-gym-no-error-message</link>
      <description><![CDATA[我尝试在 Google Colab 上训练 PPO 模型（使用 gym），但不幸的是，很快就出错了。经过几秒钟的训练，网页开始冻结，有时甚至会出错。大约 2 分钟后，屏幕变黑。这是 Google Colab 笔记本的链接：https://colab.research.google.com/drive/1uAF0_9BEiehWGsLKBtGauWCs2rGnL71-?usp=sharing。没有数据可下载。
我已经使用 Google Colab 一段时间了，但这是第一次发生这种情况。我猜想这个页面的 RAM 消耗可能存在问题，但我不确定。你能帮我改正吗？]]></description>
      <guid>https://stackoverflow.com/questions/78884681/crashing-an-google-colab-page-using-gym-no-error-message</guid>
      <pubDate>Sun, 18 Aug 2024 13:12:36 GMT</pubDate>
    </item>
    <item>
      <title>使用 ZED2 相机进行图像处理</title>
      <link>https://stackoverflow.com/questions/78884614/image-processing-with-zed2-camera</link>
      <description><![CDATA[在使用 zed2 相机处理图像时，我尝试通过 roboflow 训练图像，也使用 visual studio code 通过代码训练它们。我希望相机能够正确读取交通标志，我刚刚为特定区域拍摄了 3k 的照片。但远处的图像没有被 zed 检测到或误读。（大约 3-4 米）。我如何改进图像处理。
是否有必要拍摄更多照片？所以我已经训练了超过 10k。但结果并没有太大不同，它仍然没有按照我想要的方式读取。
这是我运行的 py 代码
from ultralytics import YOLO

model= YOLO(&quot;yolov8n.yaml&quot;)

predict = model.train(data=&quot;data.yaml&quot;, epochs=500)


我还有 yolo 代码。
如果你愿意，我可以和你分享。
我使用 ros bridge。我还试图通过 rviz2 可视化相机。
我是新来的，所以我想问你。谢谢]]></description>
      <guid>https://stackoverflow.com/questions/78884614/image-processing-with-zed2-camera</guid>
      <pubDate>Sun, 18 Aug 2024 12:43:18 GMT</pubDate>
    </item>
    <item>
      <title>在 gpu 上训练 tensorflow 模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78884301/train-tensorflow-model-on-gpu</link>
      <description><![CDATA[我的代码示例-
`xTrain=[...]
yTrain=[...]
model.fit(xTrain,yTrain)`
这花费了太多时间，因为模型很大，大约 3.4 gb，有 912,207,679 个可训练参数
告诉我将训练从 cpu 转移到 gpu 的最简单方法
我看到了许多其他问题和视频，但它们都很老了，而且 tf ha 最近发生了很大变化，所以请帮我做这个！！]]></description>
      <guid>https://stackoverflow.com/questions/78884301/train-tensorflow-model-on-gpu</guid>
      <pubDate>Sun, 18 Aug 2024 10:13:35 GMT</pubDate>
    </item>
    <item>
      <title>Spark ML：回归决策树中的用户定义杂质</title>
      <link>https://stackoverflow.com/questions/78884108/spark-ml-user-defined-impurity-in-regression-decision-trees</link>
      <description><![CDATA[我正在从 R 迁移到 PySpark。我有一个创建回归树的过程，该树目前使用 R 的 rpart 算法构建。
在 PySpark 中配置时，我无法看到指定自定义
自定义杂质函数的选项。我有一个倾斜的数据集，我不想在公式中使用均值和方差/标准差作为节点杂质的标准，而是想使用更适合我的倾斜数据的指标。
如何在 PySpark 中定义自定义杂质函数？
我查看了决策树回归的文档，并且impurity 参数的文档仅提到对 variance 的支持

impurity = Param(parent=&#39;undefined&#39;, name=&#39;impurity&#39;, doc=&#39;用于信息增益计算的标准（不区分大小写）。支持的选项：方差&#39;)

是否有任何解决方法来定义自定义杂质函数？]]></description>
      <guid>https://stackoverflow.com/questions/78884108/spark-ml-user-defined-impurity-in-regression-decision-trees</guid>
      <pubDate>Sun, 18 Aug 2024 08:38:47 GMT</pubDate>
    </item>
    <item>
      <title>Cartpole 强化学习 Python</title>
      <link>https://stackoverflow.com/questions/78883565/cartpole-reinforcement-learning-python</link>
      <description><![CDATA[我已经为 cartpole 环境和强化学习编写了代码，但我不知道从哪里开始“保存”强化学习的进度，以便我可以重新运行该程序来继续训练它，以提高程序的性能。如果有人能帮我编写代码，或者指引我去某个能帮上忙的地方，那就太好了。
我的程序：
import gym
import numpy as np
import time
env = gym.make(&quot;CartPole-v1&quot;, render_mode=&#39;human&#39;)
(state, _) = env.reset()
env.render()
env.step(0)
env.observation_space
env.observation_space.high
env.action_space
env.spec
env.spec.max_episode_steps
env.spec.reward_threshold
episodeNumber = 10000
timeSteps = 100
for episodeIndex in range(episodeNumber):
initial_state = env.reset
env.render()
appendedObservations = []
for timeIndex in range(timeSteps):
random_action = env.action_space.sample()
观察，奖励，终止，截断，信息 = env.step(random_action)
print(&quot;step&quot;, timeIndex, 观察，奖励，终止，信息)
appendedObservations. append(观察)
time.sleep(0.01)
if(terminated):
time.sleep(0.1)
break
env.close()

任何有关此事的帮助都将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78883565/cartpole-reinforcement-learning-python</guid>
      <pubDate>Sun, 18 Aug 2024 01:16:41 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn 的分类报告中的支持是否意味着原始数据集或输入模型的数据集中的出现？</title>
      <link>https://stackoverflow.com/questions/78883478/does-support-in-sklearns-classification-report-mean-occurences-within-original</link>
      <description><![CDATA[我实现了一个机器学习模型；为了获得有关模型性能的一些信息，我查看了来自 sklearn.metrics 的分类报告。
例如，这是我的分类报告：
分类报告图片
我主要有两个问题：

正类和负类旁边的支持值（56 和 3147）与底部宏和加权平均值旁边的支持值（3203 和 3203）有什么区别，我应该使用哪一个？
从此链接，支持是每个类别中有多少个样本类。这是原始数据集中的样本，还是输入到机器学习模型中的样本？我之所以问这个问题，是因为我确实进行了重新采样，因为数据集是不平衡的。换句话说，正确的支持值是基于原始（不平衡）数据集还是输入到模型中的数据集（平衡）？

对于我的第一个问题，我相信“正确”的支持值是 3203 和 3203。这与我的第二个问题类似，因为我认为支持是基于输入到模型中的数据集，所以它应该是平衡的（因为模型如何“看到”原始数据集）？
顺便说一句，一切都在管道中，因此没有数据泄漏或模型“看到”测试数据，如果这可能相关的话。
我的问题与上面链接中的问题不是重复的，因为我问的是整个分类报告，而不仅仅是其中的一部分。]]></description>
      <guid>https://stackoverflow.com/questions/78883478/does-support-in-sklearns-classification-report-mean-occurences-within-original</guid>
      <pubDate>Sat, 17 Aug 2024 23:48:10 GMT</pubDate>
    </item>
    <item>
      <title>在sklearn的ClassificationReport中，科学“使用”的度量宏是平均值还是加权平均值？</title>
      <link>https://stackoverflow.com/questions/78883328/in-sklearns-classificationreport-is-the-scientifically-used-metric-macro-ave</link>
      <description><![CDATA[在 Python 的 sklearn.metrics 中，我使用分类报告来帮助我解释不平衡数据集上的机器学习模型。例如，这是一个任意分类报告：
分类报告
在报告指标（例如精度）时，我们报告的是宏平均值（0.51）还是加权平均值（0.98）？我会假设是宏平均值，因为这会惩罚正类上较差的模型表现。我这样说对吗？
我不认为这是与text重复的问题，因为我基本上是在问使用宏还是加权平均值更有用（或在实践中使用更多）。
我曾尝试在线搜索此问题。我没有看到明确的答案，但我相信我们使用宏平均值。]]></description>
      <guid>https://stackoverflow.com/questions/78883328/in-sklearns-classificationreport-is-the-scientifically-used-metric-macro-ave</guid>
      <pubDate>Sat, 17 Aug 2024 21:45:05 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLOv10 和 RTSP 流的车牌识别系统中 RAM 和存储使用率较高</title>
      <link>https://stackoverflow.com/questions/78883152/high-ram-and-storage-usage-in-license-plate-recognition-system-with-yolov10-and</link>
      <description><![CDATA[我正在开发一个车牌识别系统，使用来自安全摄像头的 RTSP 流来识别带有阿拉伯字母/数字的埃及车牌。我的设置包括：

YOLOv10 模型 1：检测和跟踪汽车。

YOLOv10 模型 2：检测车内的车牌。

YOLOv10 模型 3：对车牌上的字符和数字进行 OCR。


我正在使用 Python 推理库，将模型导出为 .engine 格式以实现 GPU 加速。
问题：

RAM 使用情况：系统每路摄像头信号消耗高达 8 GB 的 RAM。

存储使用情况：需要 15-20 GB 的存储空间来管理软件包。

性能：尽管使用了 GPU，但系统仍然资源密集。


我预计 GPU 加速会显著降低 RAM 和存储需求，但我没有看到预期的效率。类似产品 Plate Recognizer 的运行资源要少得多（0.5 GB RAM，无 GPU）（参考链接）。
这是车牌识别器使用的软件包列表，也许有人可以帮助我了解它们如何如此高效地工作：
certifi==2024.6.2
cffi==1.16.0
charset-normalizer==3.3.2
configobj==5.0.8
cryptography==41.0.1
idna==3.7
Levenshtein==0.21.1
ntplib==0.4.0
numpy==1.24.4
opencv-python-headless==4.7.0.72
openvino==2023.3.0
openvino-telemet ry==2024.1.0
persist-queue==0.8.1
psutil==5.9.5
pycparser==2.22
python-dateutil==2.8.2
python-Levenshtein==0.21.1
rapidfuzz==3.9.3
requests==2.32.3
rollbar==0.16.3
scipy==1.10.1
six==1.16.0
urllib3==2.2.1
什么我尝试过：

模型优化：导出到 .engine 进行 GPU 加速。

流管理：使用 Python 推理库来处理 RTSP 流。


问题：

如何减少此设置中的 RAM 和存储使用量？

是否有可能更有效的替代模型或方法？

有任何提高性能的一般技巧吗？

]]></description>
      <guid>https://stackoverflow.com/questions/78883152/high-ram-and-storage-usage-in-license-plate-recognition-system-with-yolov10-and</guid>
      <pubDate>Sat, 17 Aug 2024 19:53:15 GMT</pubDate>
    </item>
    <item>
      <title>每次重复的训练时间都会发生变化[关闭]</title>
      <link>https://stackoverflow.com/questions/78882116/training-time-changes-in-each-repetition</link>
      <description><![CDATA[我正在 Nvidia RTX4090 GPU 中训练 Keras Tensorflow ResNet50 模型。我使用 Python 3.10、TF 2.10（我使用的是 Windows）、Keras 2.10、CUDA 12.5、CuDNN 8.9 和 PyCharm 作为解释器。该模型通常每轮大约需要 2 分钟。但是，我观察到有时在不更改任何超参数或代码的情况下，输入完全相同，一轮可能需要长达一小时。此外，这种情况有时会在同一次运行中发生：第一个轮需要 15 分钟，但其他轮需要 2 分钟，或者模型以正常速度运行三​​个轮，第四个轮需要半个多小时。在代码的开头，我使用了 tf.config.experimental.set_memory_growth(device, True) 和 tf.keras.backend.clear_session()，因为我读过这个问题，这可能会避免我的模型变慢。我还检查了训练期间的 GPU 使用情况，两种情况下的 GPU 使用情况相同（大约 100%）。
我希望使用相同输入的相同代码的训练时间相同。我是机器学习和 Keras 的新手，所以我可以检查其他什么来找出导致此问题的原因吗？]]></description>
      <guid>https://stackoverflow.com/questions/78882116/training-time-changes-in-each-repetition</guid>
      <pubDate>Sat, 17 Aug 2024 11:40:40 GMT</pubDate>
    </item>
    <item>
      <title>确保数据标记、数据注释的质量</title>
      <link>https://stackoverflow.com/questions/78882012/ensure-quality-of-data-labeling-data-annotation</link>
      <description><![CDATA[我有一个包含两百万数据图像、视频和文本的数据集。这些都未标记。我想雇佣来自世界各地的员工来标记它们。这是一个庞大的人数。我没有太多的数据经验。我想知道如何确保我的员工标记的数据的质量。我担心他们只是为了赚钱而浪费工作时间。
P/S：我不能使用 Scale&#39;AI 等其他公司为我标记。
对于简单的分类。我可以使用像 CAPCHA 这样的方法。它效果很好，但对于其他情况，如绘制边界框 = 或分割，我不知道如何检查标签数据的质量。]]></description>
      <guid>https://stackoverflow.com/questions/78882012/ensure-quality-of-data-labeling-data-annotation</guid>
      <pubDate>Sat, 17 Aug 2024 10:47:26 GMT</pubDate>
    </item>
    <item>
      <title>如何加速随机森林回归和SVR的训练？</title>
      <link>https://stackoverflow.com/questions/78881480/how-to-speed-up-training-of-random-forest-regression-and-svr</link>
      <description><![CDATA[我正在尝试使用以下数据集创建一个回归模型来预测比特币的收盘价：https://www.kaggle.com/datasets/prasoonkottarathil/btcinusd/data?select=BTC-2021min.csv
它有超过 60 万条记录，包含 15 个特征（其中一些是我创建的）。
我曾多次尝试在 google colab 和我的笔记本电脑上对其进行训练。我甚至把它放了一夜，但它花了太长时间。
有什么方法可以加快速度吗？
笔记本电脑规格：
CPU：Ryzen 7 5800H 
GPU：RTX 3050 
RAM：16 GB

这是训练代码：
models = {
&#39;线性回归&#39;：{
&#39;model&#39;：LinearRegression()，
&#39;params&#39;：{}
},
&#39;Ridge 回归&#39;：{
&#39;model&#39;：Riddom_state=42，
&#39;params&#39;：{&#39;alpha&#39;：[0.01, 0.1, 1, 5, 10, 50, 100]}
},
&#39;Lasso 回归&#39;：{
&#39;model&#39;： Lasso(random_state=42),
&#39;params&#39;: {&#39;alpha&#39;: [0.001, 0.01, 0.1, 1, 10]}
},
&#39;决策树&#39;: {
&#39;模型&#39;: DecisionTreeRegressor(random_state=42),
&#39;params&#39;: {&#39;max_depth&#39;: [None, 5, 10, 20], &#39;min_samples_split&#39;: [2, 5, 10]}
},
&#39;随机森林&#39;: {
&#39;模型&#39;: RandomForestRegressor(random_state=42),
&#39;params&#39;: {&#39;n_estimators&#39;: [50, 100, 200], &#39;max_depth&#39;: [None, 5, 10], &#39;min_samples_split&#39;: [2, 5, 10]}
},
&#39;支持向量回归&#39;: {
&#39;model&#39;: SVR(),
&#39;params&#39;: {&#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;], &#39;C&#39;: [0.1, 1, 10], &#39;epsilon&#39;: [0.01, 0.1, 1]}
}
}

results = {}

for model_name, model_data in models.items():
print(f&quot;Tuning {model_name}&quot;)
grid_search = GridSearchCV(model_data[&#39;model&#39;], model_data[&#39;params&#39;], cv=5,scoring=&#39;neg_mean_squared_error&#39;, verbose=1)
grid_search.fit(X_train, y_train)

# 获取最佳模型
best_model = grid_search.best_estimator_

# 预测
y_pred = best_model.predict(X_test)

# 性能指标
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

results[model_name] = {
&#39;MAE&#39;: mae,
&#39;MSE&#39;: mse,
&#39;RMSE&#39;: rmse,
&#39;R2&#39;: r2,
&#39;最佳模型&#39;: best_model,
&#39;最佳参数&#39;: grid_search.best_params_
}
]]></description>
      <guid>https://stackoverflow.com/questions/78881480/how-to-speed-up-training-of-random-forest-regression-and-svr</guid>
      <pubDate>Sat, 17 Aug 2024 05:30:15 GMT</pubDate>
    </item>
    <item>
      <title>获取 ValueError：所有数组的长度必须相同</title>
      <link>https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length</link>
      <description><![CDATA[我一直试图将字典转换为数据框，但每次我都收到 ValueError：所有数组的长度必须相同。我已经检查了每个数组的长度并确认它们相同，但我仍然收到相同的错误
def metrics_from_pipes(pipes_dict):
for name, pipeline in pipes_dict.items():

pipeline.fit(X_train, y_train)
y_pred_val = pipeline.predict(X_val)
y_pred_train = pipeline.predict(X_train)

train_metrics = {
&#39;model&#39;:list(pipes_dict.keys()),
&#39;MAE&#39;:train_mae,
&#39;MAPE&#39;:train_mape,
&#39;RMSE&#39;:train_rmse,
&#39;RSquared&#39;:train_rsquared
}

train_metrics_data = pd.DataFrame(train_metrics)
val_metrics = {
&#39;model&#39;:list(pipes_dict.keys()),
&#39;MAE&#39;:val_mae,
&#39;MAPE&#39;:val_mape,
&#39;RMSE&#39;:val_rmse,
&#39;RSquared&#39;:val_rsquared 
}

val_metrics_data = pd.DataFrame(val_metrics,)

# 合并来自训练集和测试集的指标
train_val_metrics = train_metrics_data.merge(val_metrics_data,
on = &#39;Model&#39;,
how = &#39;left&#39;,
suffixes = (&#39;_train&#39;, &#39;_val&#39;))

# 排序列 
train_val_metrics = train_val_metrics.reindex(columns = [&#39;Model&#39;,
&#39;MAE_train&#39;,
&#39;MAPE_train&#39;,
&#39;RMSE_train&#39;,
&#39;RSquared_train&#39;,
&#39;MAE_val&#39;,
&#39;MAPE_val&#39;,
&#39;RMSE_val&#39;,
&#39;RSquared_val&#39;])

return train_val_metrics.set_index(&#39;Model&#39;).transpose()

# 获取指标表
metrics_table = metrics_from_pipes(pipelines)

运行此代码会出现此错误
ValueError Traceback (most recent call last)
Cell In[45]，第 82 行
80 return train_val_metrics.set_index(&#39;Model&#39;).transpose()
81 # 获取指标表
---&gt; 82 metrics_table = metrics_from_pipes(pipelines)
83 #print(&#39;表 1：基本模型指标&#39;)
84 #metrics_table.style.background_gradient(cmap = Blues)
85 metrics_table

单元格 In[45]，第 50 行，位于 metrics_from_pipes(pipes_dict)
41 # 将性能指标列表聚合到单独的数据框中
42 train_metrics = {
43 &#39;model&#39;:list(pipes_dict.keys()),
44 &#39;MAE&#39;:train_mae,
(...)
47 &#39;RSquared&#39;:train_rsquared
48 }
---&gt; 50 train_metrics_data = pd.DataFrame(train_metrics)
51 val_metrics = {
52 &#39;model&#39;:list(pipes_dict.keys()),
53 &#39;MAE&#39;:val_mae,
(...)
56 &#39;RSquared&#39;:val_rsquared 
57 }
59 val_metrics_data = pd.DataFrame(val_metrics,)

ValueError: 所有数组的长度必须相同

当我检查 train_metrics 和 val 指标的字典结果时，我得到了这个
({&#39;model&#39;: [&#39;Linear Regression&#39;,
&#39;Random Forest Regressor&#39;,
&#39;Gradient Boost Regression&#39;,
&#39;Extra Tree Regressor&#39;],
&#39;MAE&#39;: [829.1023412412194,
288.33455697065233,
712.9637267872279,
0.0010629575741748962],
&#39;MAPE&#39;: [1.0302372135902111,
0.20937541440883897,
0.538244903316323,
6.306697580961048e-07],
&#39;RMSE&#39;: [1120.5542708017374,
416.48933196590013,
1012.399201767692,
0.05804079289490426],
&#39;RSquared&#39;: [0.5598288286601083,
0.9391916010838417,
0.6406981997919169,
0.9999999988190745]},
{&#39;model&#39;: [&#39;线性回归&#39;,
&#39;随机森林回归器&#39;,
&#39;梯度提升回归&#39;,
&#39;额外树回归器&#39;],
&#39;MAE&#39;: [855.9254413559535,
802.5902302175274,
772.3140648475379,
839.9018341377154],
&#39;MAPE&#39;: [1.0395487579496652,
0.5607987708065988,
0.5438627253681279,
0.5852285872937784],
&#39;RMSE&#39;: [1148.6549900167981,
1158.8411708570625,
1109.6145558003204,
1223.23337689915],
&#39;RSquared&#39;: [0.5876710102285392,
0.5803255834810521,
0.6152231339508221,
0.5323905190373128]})
]]></description>
      <guid>https://stackoverflow.com/questions/78858321/getting-valueerror-all-arrays-must-be-of-the-same-length</guid>
      <pubDate>Sun, 11 Aug 2024 12:27:40 GMT</pubDate>
    </item>
    <item>
      <title>如何将 transformer 编码器中的注意力掩码“Mask”矩阵与 pytorch 的 nn.TransformerEncoder 中的潜在矩阵一起发送？</title>
      <link>https://stackoverflow.com/questions/76034630/how-do-i-send-an-attention-mask-mask-matrix-in-transformer-encoder-along-with</link>
      <description><![CDATA[我在 pytorch 中初始化了一个 transformer_encoder，如下所示：
transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads), num_layers=num_layers)

现在我有一个潜在数据，其形状为 [8, 320, 512]，其中 8 是 batch_size，320 是序列长度，512 是嵌入维度。
我还有一个形状为 [320, 320] 的注意掩码。
我想将其传递到 transformer 编码器中，如下所示：
latent = torch.rand(8, 320, 512)
mask = torch.rand(320, 320)

output = transformer_encoder(latent, mask)

但它给了我错误：
“2D attn_mask 的形状是 torch.Size([320, 320])，但应该是 (8, 8)。&quot;
现在我沿着批处理维度重复矩阵，创建 8 个 (320, 320) 矩阵，使“mask”的形状为 [8, 320, 320]。当我再次运行上述代码时
latent.shape = [8, 320, 512]
mask.shape = [8, 320, 320]

output = transformer_encoder(latent, mask)

它给了我这个错误：
“3D attn_mask 的形状是 torch.Size([8, 320, 320])，但应该是 (2560, 8, 8)。&quot;
有人能解释一下这里的问题是什么吗？我该如何解决？或者正确的实现方法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/76034630/how-do-i-send-an-attention-mask-mask-matrix-in-transformer-encoder-along-with</guid>
      <pubDate>Mon, 17 Apr 2023 11:24:23 GMT</pubDate>
    </item>
    <item>
      <title>用户警告：使用的目标尺寸 (torch.Size([1])) 与输入尺寸 (torch.Size([1, 1])) 不同</title>
      <link>https://stackoverflow.com/questions/72061934/userwarning-using-a-target-size-torch-size1-that-is-different-to-the-inpu</link>
      <description><![CDATA[我有这段代码：
actual_loes_score_g = actual_loes_score_t.to(self.device, non_blocking=True)

predicted_loes_score_g = self.model(input_g)

loss_func = nn.L1Loss()
loss_g = loss_func(
predicted_loes_score_g,
actual_loes_score_g,
)

其中 predicted_loes_score_g 为 tensor([[-24.9374]], grad_fn=&lt;AddmmBackward0&gt;) 且 actual_loes_score_g 为 tensor([20.], dtype=torch.float64)。（出于调试目的，我使用批处理大小 1。）
我收到此警告：

torch/nn/modules/loss.py:96：UserWarning：使用的目标大小 (torch.Size([1])) 与输入大小 (torch.Size([1, 1])) 不同。这很可能导致由于广播而导致结果不正确。请确保它们具有相同的大小。

我如何正确确保它们具有相同的大小？
我认为这可能是答案：
predicted_loes_score = predicted_loes_score_g.detach()[0]
loss_g = loss_func(
predicted_loes_score,
actual_loes_score_g,
)

但后来我收到此错误：
torch/autograd/__init__.py&quot;，第 154 行，在 Backward
Variable._execution_engine.run_backward(
RuntimeError：张量的元素 0 不需要 grad 并且没有 grad_fn
]]></description>
      <guid>https://stackoverflow.com/questions/72061934/userwarning-using-a-target-size-torch-size1-that-is-different-to-the-inpu</guid>
      <pubDate>Fri, 29 Apr 2022 17:50:27 GMT</pubDate>
    </item>
    </channel>
</rss>