<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Sun, 14 Apr 2024 22:06:44 GMT</lastBuildDate>
    <item>
      <title>为什么混淆矩阵只有一个条目作为输出？</title>
      <link>https://stackoverflow.com/questions/78325205/why-is-there-one-entry-as-output-in-confusion-matrix</link>
      <description><![CDATA[我正在仅包含 10 个样本的简单数据集上应用简单的 SVM 和逻辑回归。我正在尝试打印混淆矩阵，但它只给出“array([[2]])”作为输出。
这是代码：
sc = StandardScaler()
X_scaled = sc.fit_transform(X)
X_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size=0.2,random_state=42)
LR.fit(X_train,y_train)
y_pred=LR.predict(X_test)
print(accuracy_score(y_test,y_pred))//1.0 输出
con= fusion_matrix(y_test,y_pred) //array([[2]]) 输出
con.shape//(1, 1)
我的数据集是一个二元分类问题。 （0 或 1）]]></description>
      <guid>https://stackoverflow.com/questions/78325205/why-is-there-one-entry-as-output-in-confusion-matrix</guid>
      <pubDate>Sun, 14 Apr 2024 20:43:28 GMT</pubDate>
    </item>
    <item>
      <title>预期类型为“MyEnv”，却得到“Env”</title>
      <link>https://stackoverflow.com/questions/78324963/expected-type-myenv-got-env-instead</link>
      <description><![CDATA[我已经从 OpenGym 创建了自定义环境，并且我在这一行收到了之前的警告：
env: MyEnv=gym.make(&#39;gym_envs/MyEnv-v0&#39;)

当我删除 MyEnv 时，我没有收到警告，但随后我收到警告“类‘MyEnv’的未解析属性引用‘action_type’”在下面一行：
agent = QLearningAgent(env.action_space, env.observation_space.n, env.action_type.n)

知道如何消除警告吗？
我尝试编写一个包装器来返回 ATMEnv 对象而不是 Env，但它没有解决问题]]></description>
      <guid>https://stackoverflow.com/questions/78324963/expected-type-myenv-got-env-instead</guid>
      <pubDate>Sun, 14 Apr 2024 19:12:45 GMT</pubDate>
    </item>
    <item>
      <title>Sklearm FeatureHasher 无法处理数据框中的单个列</title>
      <link>https://stackoverflow.com/questions/78324647/sklearm-featurehasher-not-working-on-a-single-column-in-a-dataframe</link>
      <description><![CDATA[我尝试在数据帧中的单个列上执行特征哈希器，但它不断给出错误 
ValueError：样本不能是单个字符串。输入必须是字符串可迭代对象上的可迭代对象。
from sklearn.feature_extraction import FeatureHasher

哈希向量大小 = 50
fh = FeatureHasher(n_features=hash_vector_size, input_type=&#39;string&#39;)
hashed_df = pd.DataFrame(fh.transform(X_train[“Item_Identifier”]).toarray(),
                         columns=[&#39;H&#39;+str(i) for i in range (hash_vector_size)])

我期望一个包含 50 列的数据框，其中的数据将被散列]]></description>
      <guid>https://stackoverflow.com/questions/78324647/sklearm-featurehasher-not-working-on-a-single-column-in-a-dataframe</guid>
      <pubDate>Sun, 14 Apr 2024 17:20:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 Eigen 执行步长卷积</title>
      <link>https://stackoverflow.com/questions/78324587/using-eigen-to-perform-a-convolution-with-stride</link>
      <description><![CDATA[我试图让 Eigen 以跨步执行卷积运算（对于卷积神经网络）。我知道 Eigen 有一个可以在张量上执行的卷积函数：
input.convolve（过滤器，尺寸）

没有参数可以提供步幅值。
我知道 Eigen 还有一个步幅函数，它返回应用特定步幅的张量的视图。然而，这与（我不认为）对卷积应用步幅相同。我想知道是否可以将 stride 函数应用于输入，然后调用 convolve，但我认为这不会导致与 stride 的正确卷积。
有谁知道如何应用步幅卷积（除了手动编写卷积函数而不是依赖 Eigen 的卷积函数）？
我注意到有一个“extract_image_patches”返回输入区域列表的函数 - 这是用来代替卷积函数吗？
谢谢
我查看了 Stackoverflow 和 Cross Validated，但在任何地方都看不到这个答案。 此处提出了类似的问题，但没有得到解答。]]></description>
      <guid>https://stackoverflow.com/questions/78324587/using-eigen-to-perform-a-convolution-with-stride</guid>
      <pubDate>Sun, 14 Apr 2024 16:57:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 python 创建制造缺陷检测模型</title>
      <link>https://stackoverflow.com/questions/78324269/create-model-for-manufacturing-defect-detection-using-python</link>
      <description><![CDATA[有人做过制造商缺陷检测方面的工作吗？我需要对此提供一些意见。我做了一些研究，发现了一些例子，比如 TensorFlow/Keras 和 CNN，但如果有人已经探索过，我需要一些实时经验。提前致谢。
已尝试以下

使用卷积神经网络 (CNN) 对图像进行分类
用于图像分类的 TensorFlow/Keras

如果我朝着正确的方向前进，我正在寻找输入以及一些输入如何使此类模型准备好生产错误条款]]></description>
      <guid>https://stackoverflow.com/questions/78324269/create-model-for-manufacturing-defect-detection-using-python</guid>
      <pubDate>Sun, 14 Apr 2024 15:18:31 GMT</pubDate>
    </item>
    <item>
      <title>facebook / detr-resnet-50 模型中的标签数量</title>
      <link>https://stackoverflow.com/questions/78323867/number-of-labels-in-facebook-detr-resnet-50-model</link>
      <description><![CDATA[我正准备在自定义数据集上训练 Facebook ResNet DETR 模型，以检测图像中的签名（我的数据集中只有 1 个类）。我不确定分配给模型配置中的 num_labels 参数的适当值。根据上下文，该值是否应该设置为 1（因为我只检测签名），或者我应该为没有任何签名的情况添加第二个标签？
这是代码
model = DetrForObjectDetection.from_pretrained(pretrained_model_name_or_path=CHECKPOINT,num_labels=????,ignore_mismatched_sizes=True)]]></description>
      <guid>https://stackoverflow.com/questions/78323867/number-of-labels-in-facebook-detr-resnet-50-model</guid>
      <pubDate>Sun, 14 Apr 2024 13:02:52 GMT</pubDate>
    </item>
    <item>
      <title>具有完整链接的分层凝聚聚类以对一维数据集进行聚类</title>
      <link>https://stackoverflow.com/questions/78323771/hierarchical-agglomerative-clustering-with-complete-linkage-to-cluster-a-1dimens</link>
      <description><![CDATA[我目前正在研究分层凝聚聚类，并熟悉其在表格中呈现的数据集的应用。但是，我不确定如何将此方法应用于一维数据集，特别是当仅对图形 x 轴上表示的数据点进行聚类时。谁能指导我如何在一维数据集的层次凝聚聚类中使用单一和完整的链接？此外，如果您能提供可以帮助我了解如何解决未来类似问题的示例或方法，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78323771/hierarchical-agglomerative-clustering-with-complete-linkage-to-cluster-a-1dimens</guid>
      <pubDate>Sun, 14 Apr 2024 12:19:33 GMT</pubDate>
    </item>
    <item>
      <title>Whitewine 数据集上的 K 均值聚类问题</title>
      <link>https://stackoverflow.com/questions/78323717/k-means-clustering-problem-on-whitewine-dataset</link>
      <description><![CDATA[我目前正在努力使用 R 编程语言对名为 Whitewine 的数据集进行聚类，并比较应用主成分分析 (PCA) 之前和之后的聚类效果。尽管进行了多次尝试，我还是面临着挑战，因为即使在应用 PCA 后，我的数据点也没有以最佳方式聚类。
在 PCA 之前：我执行了异常值去除并采用 z 分数标准化来预处理数据集。&lt; /a&gt;
但是，聚类结果并不理想，聚类之间有明显的重叠。
PCA之后：利用prcomp函数，我降低了数据集的维度。
为了读者的利益，我也提供了每个实例的代码。
这是在 PCA 之前
# 异常值去除前的汇总统计信息
摘要(Whitewine_v6[, 1:11])

# 计算每列的下限和上限
lower_bounds &lt;- apply(Whitewine_v6[, 1:11], 2, function(x) 分位数(x, 0.25) - 1.5 * IQR(x))
upper_bounds &lt;- apply(Whitewine_v6[, 1:11], 2, function(x) 分位数(x, 0.75) + 1.5 * IQR(x))

# 识别前 11 列中的任何值超出范围的行
异常值 &lt;- apply(Whitewine_v6[, 1:11], 1, function(row) any(row &lt; lower_bounds | row &gt; upper_bounds))

# 对数据集进行子集化以删除具有异常值的行
Whitewine_clean &lt;- Whitewine_v6[!离群值, ]

# 检查清理后的数据集的维度
暗淡（Whitewine_clean）


#k=2 的 K 均值聚类
#K-Means 聚类投资
设置.种子(123)
k_mean1&lt;-kmeans(Whitewine_scaled, 2)

#有关集群解决方案的有用信息
#集群中心
k_mean1$中心
#集群
k_mean1$簇

# 提取BSS和TSS
BSS &lt;- k_mean1$ Betweenss
TSS &lt;- k_mean1$totss

# 计算WSS
WSS &lt;- sum(k_mean1$withinss)

# 计算总 WSS
Total_WSS &lt;- k_mean1$tot.withinss

# 计算BSS/TSS比率
BSS_TSS_ratio &lt;- BSS / TSS

# 打印结果
猫（“BSS：”，BSS，“\n”）
猫（“TSS：”，TSS，“\n”）
cat(&quot;总 WSS:&quot;, WSS, &quot;\n&quot;)
cat(&quot;BSS/TSS 比率：&quot;, BSS_TSS_ratio, &quot;\n&quot;)

#说明k-means聚类
fviz_cluster(k_mean1, Whitewine_scaled, geom=“点”, ellipse.type=“凸”, ggtheme=theme_light())

cluster_silhouette&lt;-silhouette(k_mean1$cluster, dist(Whitewine_scaled))
fviz_silhouette（cluster_silhouette）


这是 PCA 之后的
#新的“转换”数据集，其中选择的主成分作为属性
Whitewine_pca&lt;-data.frame(processed_data$x[,1:7])


我所做的总结：
我清理并缩放了我的数据集 (Whitewine)，为聚类做好准备。然后，我使用主成分分析（PCA）来简化数据。即使经过这些步骤，我的簇看起来仍然不明显。我尝试了K-means（因为这是我的项目中指定的方法，我不能使用任何其他方法），但结果并没有太大改善。现在，我正在寻求有关如何使我的集群更清晰的建议，尤其是在使用 PCA 之后。正如我之前所说，我的项目只能使用 K-means 聚类方法。我使用 4 种方法（NbClust、Elbow 方法、Gap 统计、Silhouette）确定了最佳簇数。
提前致歉，无法直接提供图像。]]></description>
      <guid>https://stackoverflow.com/questions/78323717/k-means-clustering-problem-on-whitewine-dataset</guid>
      <pubDate>Sun, 14 Apr 2024 12:00:20 GMT</pubDate>
    </item>
    <item>
      <title>如何正确加载和保存tf官方模型</title>
      <link>https://stackoverflow.com/questions/78323409/how-to-correctly-load-and-save-tf-official-models</link>
      <description><![CDATA[我正在尝试保存并加载本教程中经过训练的模型&gt; https://www.tensorflow.org/hub/tutorials/movinet
这是本教程中的简要代码：
def build_classifier(batch_size、num_frames、分辨率、backbone、num_classes)：
  “”“在骨干模型之上构建分类器。”“”“
  模型 = movinet_model.MovinetClassifier(
      骨干=骨干，
      类数=类数）
  model.build([batch_size, num_frames, 分辨率, 分辨率, 3])
  返回模型

模型 = build_classifier(batch_size, num_frames, 分辨率, 主干网, 7)
纪元数 = 30
loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
优化器 = tf.keras.optimizers.Adam(learning_rate = 0.001)

model.compile（loss=loss_obj，optimizer=&#39;adam&#39;，metrics=[&#39;accuracy&#39;]）
结果 = model.fit(train_ds,
                    验证数据=val_ds，
                    纪元=num_epochs，
                    验证频率=1，
                    详细=1)

以下代码是我尝试做的：
model.save(“./saved_model1”)
模型 = tf.keras.layers.TFSMLayer(&#39;./saved_model1&#39;, call_endpoint=&#39;serving_default&#39;)
model.evaluate(test_ds, return_dict=True)

错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
AttributeError Traceback（最近一次调用最后一次）
[86] 中的单元格，第 1 行
----&gt; 1 model.evaluate(test_ds, return_dict=True)

AttributeError：“TFSMLayer”对象没有属性“evaluate”

&lt;块引用&gt;
尝试将模型另存为 .keras 、 .h5

model.save(&#39;my_model.keras&#39;)
load_model = tf.keras.models.load_model(“my_model.keras”, custom_objects={&#39;MovinetClassifier&#39;: movinet_model.MovinetClassifier})
model.evaluate(test_ds, return_dict=True)


&lt;块引用&gt;
并收到此错误：

&lt;前&gt;&lt;代码&gt;--------------------
AttributeError Traceback（最近一次调用最后一次）
单元格 In[28]，第 3 行
      1 model_path=“my_movinet_model2.keras”
      2 # 加载模型 = tf.keras.models.load_model(model_path)
----&gt; 3 load_model = tf.keras.models.load_model(model_path, custom_objects={&#39;MovinetClassifier&#39;: movinet_model.MovinetClassifier})
      4 model.evaluate(test_ds, return_dict=True)

文件/opt/conda/lib/python3.10/site-packages/keras/src/ saving/ saving_api.py:176，在load_model（文件路径，custom_objects，编译，safe_mode）中
    第173章
  
  ................................................

文件/opt/conda/lib/python3.10/site-packages/official/projects/movinet/modeling/movinet_model.py:78，在MovinetClassifier.__init__(self,backbone,num_classes,input_specs,activation,dropout_rate,kernel_initializer,kernel_regularizer 、bias_regularizer、output_states、**kwargs）
     75 self._output_states = 输出状态
     77 状态规格=无
---&gt; 78如果backbone.use_external_states：
     79 状态规格=骨干.初始状态规格（
     80 input_shape=input_specs[&#39;image&#39;].shape)
     82 个输入，输出 = self._build_network(
     83 主干，input_specs，state_specs=state_specs)

AttributeError：“dict”对象没有属性“use_external_states``
 
回溯很长，需要包含在内。我不确定如何像教程一样正确加载和评估保存的模型。
]]></description>
      <guid>https://stackoverflow.com/questions/78323409/how-to-correctly-load-and-save-tf-official-models</guid>
      <pubDate>Sun, 14 Apr 2024 10:10:52 GMT</pubDate>
    </item>
    <item>
      <title>值错误：数据不明确（由于某种原因，所有 x 值均为 1）</title>
      <link>https://stackoverflow.com/questions/78323348/value-error-data-is-ambiguous-for-some-reason-all-x-values-are-1</link>
      <description><![CDATA[所以我正在编写一个基于文本生成音乐的人工智能模型。我检查了所有的预处理功能，它们似乎工作得很好。 x 个样本是用于训练的预处理文本，由于某种原因，它们在输入模型时最终都为 1。
导入tensorflow为tf
从tensorflow.keras.preprocessing.text导入Tokenizer
从tensorflow.keras.layers导入LSTM，密集，嵌入，输入，TimeDistributed，激活
从tensorflow.keras.models导入模型
从tensorflow.keras.preprocessing.sequence导入pad_sequences
导入 csv
导入库
导入操作系统
将 numpy 导入为 np

os.system(&#39;cls&#39; if os.name == &#39;nt&#39; else &#39;clear&#39;)

# 加载音频的函数
def load_audio(文件路径):
    y, sr = librosa.load(文件路径)
    返回 y, sr

# 提取音频特征的函数
def extract_features(音频, sr):
    mfccs = librosa.feature.mfcc(y=音频, sr=sr)
    返回 mfccs.T

# 文本编码函数
defencode_text（文本，分词器）：
    序列 = tokenizer.texts_to_sequences([text])[0]
    返回序列

# 数据预处理函数
def preprocess_data(行、分词器、music_dir):
    Song_path = os.path.join(music_dir, row[&#39;歌曲&#39;])
    print(f&#39;检索到的歌曲路径：{song_path}&#39;)
    音频，sr = load_audio（歌曲路径）
    mfccs = extract_features(音频, sr)
    文本 = 行[&#39;提示&#39;]
    序列=encode_text（文本，分词器）
    padd_sequence = pad_sequences([序列], maxlen=43, padding=&#39;post&#39;)
    Expanded_pa​​dded_sequence = np.expand_dims(pagged_sequence, axis=1)
    返回 mfccs，扩展的_填充_序列

# 加载数据集并标记化提示
以 open(file=&#39;dataset.csv&#39;, mode=&#39;r&#39;,encoding=&#39;utf-8&#39;) 作为数据集：
    reader = csv.DictReader(数据集)
    all_prompts = [row[&#39;prompt&#39;] for reader 中的行]

分词器 = 分词器(num_words=5000)
tokenizer.fit_on_texts(all_prompts)

# 准备模型训练数据
预处理音频 = []
预处理文本 = []
music_dir = os.path.join(os.getcwd(), &#39;音乐&#39;)

以 open(file=&#39;dataset.csv&#39;, mode=&#39;r&#39;,encoding=&#39;utf-8&#39;) 作为数据集：
    数据集.seek(0)
    reader = csv.DictReader(数据集)
    对于读卡器中的行：
        mfccs，expanded_pa​​dded_sequence = preprocess_data（行，分词器，music_dir）
        preprocessed_audio.append(mfccs)
        preprocessed_text.append(expanded_pa​​dded_sequence)

example_mfccs = preprocessed_audio[0]
mfccs_timesteps = len(example_mfccs)
mfccs_features = len(example_mfccs[0])
最大文本长度 = 43

audio_input = 输入（形状=（mfccs_timesteps，mfccs_features），名称=&#39;audio_input&#39;）
text_input = 输入（形状=（max_text_len，），名称=&#39;text_input&#39;）

num_lstm_units = 128

text_embedding = 嵌入（input_dim=tokenizer.num_words，output_dim=64）（text_input）
text_encoder = LSTM(num_lstm_units, return_sequences=True)(text_embedding)
解码器 = LSTM(num_lstm_units, return_sequences=True)(text_encoder)
解码器 = LSTM(num_lstm_units)(解码器)
#decoder_upsampled = TimeDistributed（密集（mfccs_features））（解码器）
model_output = Activation(&#39;sigmoid&#39;, name=&#39;model_output&#39;)(解码器)
模型=模型（输入=文本输入，输出=模型输出）

model.compile(loss=&#39;mse&#39;, 优化器=&#39;adam&#39;)
打印（len（预处理文本））
打印（len（预处理音频））

# 训练模型
epochs = int(input(&#39;请输入纪元数：&#39;))
model.fit(x=预处理文本，y=预处理音频，纪元=纪元)
model.save(&#39;Ihy.tf&#39;, overwrite=True, include_optimizer=True)

错误是：
ValueError：数据基数不明确。确保所有数组包含相同数量的样本。
“x”尺寸：1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 , 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
“y”尺寸：3982、9838、7699、14171、8417、6547、9082、6763、10255、11647、6754、9491、17134、8414、10078、8303、9107、14320、 45、7339、10338、6933、9302 , 11573, 11859, 8915, 8949, 9709, 10538, 10924, 9507, 9987, 9445, 11571, 9563, 9705, 10081, 11245, 10239, 11909, 9678, , 10041

我尝试并尝试调试所有内容，并且所有文本预处理功能都工作得很好。他们返回好的数组。音频预处理正确 (y)，文本仅为 1 秒 (x)。]]></description>
      <guid>https://stackoverflow.com/questions/78323348/value-error-data-is-ambiguous-for-some-reason-all-x-values-are-1</guid>
      <pubDate>Sun, 14 Apr 2024 09:47:06 GMT</pubDate>
    </item>
    <item>
      <title>无法解释优化器标识符：<keras.src.optimizers.adam.Adam 对象位于 0x7d8646d22b00></title>
      <link>https://stackoverflow.com/questions/78323015/could-not-interpret-optimizer-identifier-keras-src-optimizers-adam-adam-object</link>
      <description><![CDATA[我正在使用这样的模型训练数据集
导入tensorflow为tf
从tensorflow.keras.optimizers导入Adam

优化器 = Adam(learning_rate=2e-5)

# 编译模型（使用优化器）
model.compile(优化器=优化器,
              损失=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              指标=[&#39;准确性&#39;])


# 训练模型
model.fit（train_inputs，train_labels，epochs=epochs，batch_size=batch_size）

它返回此错误：
ValueError：无法解释优化器标识符：
optimizer.adam 但它无法正常工作]]></description>
      <guid>https://stackoverflow.com/questions/78323015/could-not-interpret-optimizer-identifier-keras-src-optimizers-adam-adam-object</guid>
      <pubDate>Sun, 14 Apr 2024 07:12:50 GMT</pubDate>
    </item>
    <item>
      <title>将数据拆分为训练集、验证集和测试集，ID 不重叠，并且仍然平衡目标类</title>
      <link>https://stackoverflow.com/questions/78322346/splitting-data-into-training-validation-and-test-sets-without-overlapping-ids</link>
      <description><![CDATA[我需要将大型数据集拆分为一定比例的训练集、验证集和测试集，同时确保满足以下条件：

在每组中保留唯一的 ID。任何 ID 不能属于多于一组。
每次数据重组时，训练、验证和测试集中每个级别（“热”、“暖”、“冷”）都需要至少出现一次。

data &lt;- data.frame(ID = c(001, 001, 001,
                           002, 002, 002, 002,
                           003, 003, 003, 003,
                           004, 004, 004, 004, 004, 004,
                           005, 005, 005, 005, 005,
                           006, 006, 006, 006,
                           007, 007, 007,
                           008, 008,
                           009, 009,
                           010, 010, 010),
                   var1 = c(0102, 0210, 0405,
                            0318, 0629, 1201,0101,
                            0923、0702、0710、0801、
                            0203、0501、1204、0516、0112、1005、
                            1101、1125、1020、0112、0310、
                            0203、0401、0607、0811、
                            1010、1212、0707、
                            0430, 0428,
                            1030, 1008,
                            0501, 0511, 0601),
                   var2 = c(“冷”, “冷”, “冷”,
                            “温暖”、“温暖”、“温暖”、“温暖”、
                            “冷”、“冷”、“冷”、“冷”、
                            “温暖”、“温暖”、“温暖”、“温暖”、“温暖”、“温暖”、
                            “热”、“热”、“热”、“热”、“热”、
                            “冷”、“冷”、“冷”、“冷”、
                            “热”、“热”、“热”、
                            “温暖”、“温暖”、
                            “热”、“热”、
                            “冷”、“冷”、“冷”））

我尝试使用数据分割包 caret(fx = createDataPartition()) 和 splitTools (fx = partition()) 以及 dplyr 采样函数，但它们应用的分组可确保每个 ID 出现在所有集合中。 
减少数据集是可以的。以下是由现有 Stack Overflow 问题引导的众多尝试之一：
赋值 &lt;- 数据 %&gt;%
        选择（ID，var2）%&gt;%
        不同的(ID) %&gt;%
        行式() %&gt;%
        mutate(Group=sample(c(“验证”,“训练”,“测试”), 1,
                             概率 = c(0.70, 0.20, 0.10)))
数据%&gt;%
  left_join（作业，数据，by =“ID”）

这种尝试忽略了概率论点*没有设置比例。它还不能确保所有级别都出现在训练、验证和测试集中。]]></description>
      <guid>https://stackoverflow.com/questions/78322346/splitting-data-into-training-validation-and-test-sets-without-overlapping-ids</guid>
      <pubDate>Sun, 14 Apr 2024 00:19:39 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 pytorch 训练机器学习多项式回归模型，在尝试绘制预测结果时出现错误</title>
      <link>https://stackoverflow.com/questions/78321929/im-training-a-model-of-machine-learning-polynomial-regression-using-pytorch-an</link>
      <description><![CDATA[我想将数据绘制到 plt.scatter 表单中，但是当我尝试填充它时，它只是说 x 和 y 的大小不同，而且我还挤压了它们仅一维，以便更容易绘制，但仍然不起作用。
这是情节机制：
#使用 matplotlib.pyplot 中的散点图 (x,y) 可视化数据
defplot_predictions(train_features=X_train,
                     train_labels=y_train,
                     test_features=X_test,
                     测试标签=y_测试，
                     预测=无）：
    plt.figure(figsize=(10,7))

    plt.scatter(X_train, y_train, c=“g”, label=“训练数据”)

    plt.scatter(X_test, y_test, c=“b”, label=“测试数据”)

    如果预测不是 None：
        plt.scatter（test_features，预测，c =“r”，标签=“预测”）

    plt.legend(prop={“大小”: 14})

绘图预测（）

#这里检查是否存在不匹配的形状
X_train.shape、y_train.shape、y_preds.shape

#output (torch.Size([24, 1]), torch.Size([24, 1]), torch.Size([24, 1]))
#这里尝试解决问题
Predictions_reshape=y_preds.squeeze(dim=1)
labels_reshape=y_train.squeeze(dim=1)
打印（len（y_train），len（y_preds））
打印（labels_reshape.shape，predictions_reshape.shape）

＃输出
#24 24
#torch.Size([24]) torch.Size([24])
#torch.Size([6, 1]) torch.Size([6, 1])


labels_reshape=y_train.detach().numpy()
Predictions_reshape=y_preds.detach().numpy()
图_预测（标签_重塑，预测=预测_重塑）

&lt;块引用&gt;
ValueError：x 和 y 的大小必须相同

我尝试压缩张量，使它们只有一个暗淡，并且我还检查了镜头是否相同，确实如此。]]></description>
      <guid>https://stackoverflow.com/questions/78321929/im-training-a-model-of-machine-learning-polynomial-regression-using-pytorch-an</guid>
      <pubDate>Sat, 13 Apr 2024 20:23:01 GMT</pubDate>
    </item>
    <item>
      <title>我可以重新训练 AutoModelForSequenceClassification 以生成文本吗？</title>
      <link>https://stackoverflow.com/questions/78284197/can-i-retrain-an-automodelforsequenceclassification-for-text-generation</link>
      <description><![CDATA[我的目标是微调 Mistral 7b 以编写短意识流（文本完成，而不是遵循指令）。
我有一个大型数据库（100 万行），其中包含从互联网上抓取的短文本。我手动将 15k 行标记为 good (1k) 和 bad（其余 14k）示例。我的计划是训练 AutoModelForSequenceClassification在这些示例上标记其他 985k 行。
通过这种方式，我希望收集大约 20k 意识流的好例子来微调 Mistral 7b。
但仅对good示例进行微调并不会使用bad示例中的信息，这些示例的数量要多得多。因此，我正在考虑使用 Mistral 7b 作为 AutoModelForSequenceClassification 的基本模型（遵循 这篇 Medium 文章），然后重新训练生成的 AutoModelForSequenceClassification 以进行文本补全。这需要移除分类头并添加新的/重新训练的 LoRA 组件。
您认为这可行吗？这是否会削弱模型（例如，需要重新学习语法），或者这是否是将坏反例的信息合并到文本生成中的有效方法？或者至少为 LoRA 文本生成微调提供一个良好的初始化点？]]></description>
      <guid>https://stackoverflow.com/questions/78284197/can-i-retrain-an-automodelforsequenceclassification-for-text-generation</guid>
      <pubDate>Sat, 06 Apr 2024 11:32:55 GMT</pubDate>
    </item>
    <item>
      <title>模块“keras.layers”没有属性“实验性”</title>
      <link>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</link>
      <description><![CDATA[你好，我试图调整我的数据集的大小和比例，如下所示，但我遇到了这个错误：
AttributeError：模块“keras.layers”没有属性“experimental”
&lt;前&gt;&lt;代码&gt;
resize_and_rescale= tf.keras.Sequential([
    图层.实验.预处理.调整大小(IMAGE_SIZE,IMAGE_SIZE),
    层.实验.预处理.重新缩放(1.0/255)
]）

]]></description>
      <guid>https://stackoverflow.com/questions/74792455/module-keras-layers-has-no-attribute-experimental</guid>
      <pubDate>Wed, 14 Dec 2022 00:43:49 GMT</pubDate>
    </item>
    </channel>
</rss>