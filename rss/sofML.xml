<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 19 Jan 2024 09:14:34 GMT</lastBuildDate>
    <item>
      <title>多个数据帧的机器学习模型选择</title>
      <link>https://stackoverflow.com/questions/77844465/machine-learning-model-selection-for-multiple-dataframes</link>
      <description><![CDATA[我有 365 个数据帧，每个数据帧包含 30 行。每一行代表未来的一天。
数据框包含 4 个特征和目标。主要特征是未来30天每天的订单量。
目标是实际销量。其他功能与此处无关。
为此目的最好的模型是什么，其目的是预测未来的销售额，
当我们知道未来 30 天内有多少订单并且我们还有其他 4 个功能可用时。
可以通过 Randomforest/Gradboost 等以某种方式完成此操作，以便模型能够学习该行为吗？
我想这些类型的模型的数据必须位于单个数据框中？
或者这应该通过某种类型的神经网络来完成？什么类型的神经网络最适合？
欢迎任何有关如何组织输入数据以及应使用什么类型的模型的 Python 示例！
编辑！
例如，对于日期 2016-03-03 和 2016-03-04 数据帧的头部如下所示：

]]></description>
      <guid>https://stackoverflow.com/questions/77844465/machine-learning-model-selection-for-multiple-dataframes</guid>
      <pubDate>Fri, 19 Jan 2024 07:49:02 GMT</pubDate>
    </item>
    <item>
      <title>用于训练和生成遗留代码的生成式人工智能[关闭]</title>
      <link>https://stackoverflow.com/questions/77844343/generative-ai-to-train-and-generate-legacy-code</link>
      <description><![CDATA[我有一个大小约为 2 GB 的遗留代码库，主要用 C++ 编写。有许多内部库是构建在原始 C++ 之上的。例如，我们有自己的向量、队列、映射等实现。还有围绕套接字构建的库以提供 HTTP 功能等。要编写的任何新应用程序都必须使用这些内部库提供的方法。
我的问题是，AI/ML 有什么方法可以帮助训练这些用 C++（或一般任何语言）编写的内部库的 LLM 模型，以便可以使用这些遗留内部库中提供的方法自动生成应用程序功能代码？出于安全原因，我不想使用 ChatGPT，而且它还会为我提供标准 C++ 代码，而这些代码不能直接在我们的系统中使用。任何有关如何使用遗留代码库训练 LLM 的建议都将非常有帮助。
尝试搜索现有选项，但找不到任何令人满意的答案。]]></description>
      <guid>https://stackoverflow.com/questions/77844343/generative-ai-to-train-and-generate-legacy-code</guid>
      <pubDate>Fri, 19 Jan 2024 07:25:01 GMT</pubDate>
    </item>
    <item>
      <title>基于 LLM 的 DOCX 文件提取摘要解决方案</title>
      <link>https://stackoverflow.com/questions/77844145/llm-based-solutions-for-extractive-summarization-in-docx-files</link>
      <description><![CDATA[多年来，我一直维护两个主要 DOCX 文件（Sites.docx 和 Cultures.docx），每个文件大约包含 1,500 页。这些文件主要用法语撰写，重点关注欧洲前史和原始史，特别详细介绍了考古遗址（例如巨石阵）和考古文化或时期（例如新石器时代）。内容包括表格、交叉引用（Sites.docx 和 Cultures.docx 之间的超链接）、书目参考索引和特殊符号（例如，EN 表示新石器时代早期）。此外，文件按标题和副标题组织，定义：

时间分组（例如，新石器时代涵盖新石器时代早期、中期和晚期）；
空间分组（例如，新石器时代早期的大西洋、新石器时代早期的西地中海）；
类型分组（例如，涵盖支石墓、竖石碑和古墓的巨石）。

&lt;img alt=&quot;Cultures.docx 新石器时代章节的屏幕截图，包含标题层次结构、超链接以及表格、参考书目等。” src =“https://i.stack.imgur.com/1sa56.png”/&gt;
关于新石器时代的 Cultures.docx 章节的详细信息，包括标题层次结构、超链接和表格、参考书目等。
我正在通过能够管理我的档案的复杂结构的提示来寻求提取摘要。例如，我希望收到如下摘要：
&lt;代码&gt;&gt;恢复有关新石器时代早期丧葬习俗的不同章节。
但是，我观察到一些解决方案，包括 ChatGPT 3.5，往往会“扁平化”。 DOCX 和 PDF 文档在处理前转换为纯文本，导致表格、标题和链接丢失。
在灵活的文字编辑器（例如：Microsoft Word）中继续维护这些档案，同时仍然能够利用 AI/ML 文本提取摘要进行知识发现的最佳方式是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77844145/llm-based-solutions-for-extractive-summarization-in-docx-files</guid>
      <pubDate>Fri, 19 Jan 2024 06:36:46 GMT</pubDate>
    </item>
    <item>
      <title>神经网络无法正常学习</title>
      <link>https://stackoverflow.com/questions/77844067/neural-network-not-learning-properly</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77844067/neural-network-not-learning-properly</guid>
      <pubDate>Fri, 19 Jan 2024 06:15:19 GMT</pubDate>
    </item>
    <item>
      <title>用于比较两个 Android 轨迹以发现性能差异的 AI 技术</title>
      <link>https://stackoverflow.com/questions/77844043/ai-techniques-for-comparing-two-android-traces-to-spot-performance-differences</link>
      <description><![CDATA[尝试构建一个系统，该系统可以获取从我的应用程序的两个版本收集的两组 Android 跟踪，并吐出跟踪中导致延迟问题的一些有问题的方面。是否有任何可以使用的 AI/ML 技术/框架？]]></description>
      <guid>https://stackoverflow.com/questions/77844043/ai-techniques-for-comparing-two-android-traces-to-spot-performance-differences</guid>
      <pubDate>Fri, 19 Jan 2024 06:09:27 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 fitz 模块从同一文件夹中的 pdf 文件中提取多个图像？</title>
      <link>https://stackoverflow.com/questions/77843782/how-do-i-use-the-fitz-module-to-extract-multiple-images-from-pdf-files-in-the-sa</link>
      <description><![CDATA[我正在从不同的 PDF 文件中提取图像，以使用它们构建数据集。是否可以对同一文件夹中的不同 PDF 文件自动执行此操作，这样我就不必为每个 PDF 单独执行此操作。
我使用以下代码从 1 个 PDF 中提取图像
python -m fitz extract -images “文件名”

我希望能够对超过 1500 个文件执行此操作，而无需手动操作。]]></description>
      <guid>https://stackoverflow.com/questions/77843782/how-do-i-use-the-fitz-module-to-extract-multiple-images-from-pdf-files-in-the-sa</guid>
      <pubDate>Fri, 19 Jan 2024 04:51:07 GMT</pubDate>
    </item>
    <item>
      <title>为什么过滤pandas文件的输出变成NaN？</title>
      <link>https://stackoverflow.com/questions/77843589/why-the-output-of-filtering-pandas-file-becomes-nan</link>
      <description><![CDATA[我希望输出变为非 NaN 值。
这一行的问题
f_bp_max.loc[l, &#39;max&#39;] = df_frcst[df_frcst[&#39;日期时间&#39;].dt.year == k][&#39;预测&#39;].max()

当我打印这个时：
df_frcst[df_frcst[&#39;日期时间&#39;].dt.year == k][&#39;预测&#39;]

输出是：
系列（[]，名称：预测，dtype：float64）

导入 pandas 作为 pd
导入时间
导入日期时间
将 matplotlib.pyplot 导入为 plt
将 numpy 导入为 np
从 scipy.optimize 导入最小化标量

开始时间 = 时间.time()

pct_选择 = 0.98

#* 数据预测
df_frcst = pd.read_csv(&#39;lstm_forecast_results.csv&#39;)
df_frcst[&#39;日期时间&#39;] = pd.to_datetime(df_frcst[&#39;日期时间&#39;])
df_frcst_simple = df_frcst[[&#39;日期时间&#39;, &#39;预测&#39;]]
df_frcst_simple = df_frcst_simple.rename({&#39;日期时间&#39;:&#39;waktu&#39;, &#39;预测&#39;:&#39;bp&#39; }, axis = 1)



df_bp_max = pd.DataFrame()
对于范围 (2024, 2034, 1) 内的 k：
    l = k - 2024
    df_bp_max.loc[l, &#39;tahun&#39;] = str(k)
    df_bp_max.loc[l, &#39;max&#39;] = df_frcst[df_frcst[&#39;日期时间&#39;].dt.year == k][&#39;预测&#39;].max()
    df_bp_max.loc[l, &#39;min&#39;] = df_frcst[df_frcst[&#39;日期时间&#39;].dt.year == k][&#39;预测&#39;].min()
#df_bp_max[&#39;LF&#39;] = df_bp_max[&#39;min&#39;]/df_bp_max[&#39;max&#39;]
df_bp_max[&#39;tahun&#39;] = df_bp_max[&#39;tahun&#39;].apply(str)
#df_bp_max = df_bp_max.set_index(&#39;tahun&#39;)

打印（“================================================ ================》）
打印（df_bp_max）

输入文件
日期时间、实际、预测
2022-01-01 12:30:00,17809.0,17343.484
2022-01-01 13:00:00,17772.61,17382.861
2022-01-01 13:30:00,17867.8,17414.637
2022-01-01 14:00:00,17773.68,17504.357
2022-01-01 14:30:00,17869.88,17530.559
2022-01-01 15:00:00,17786.7,17592.822
2022-01-01 15:30:00,17943.11,17626.775
2022-01-01 16:00:00,18125.29,17686.678
2022-01-01 16:30:00,18463.05,17760.666
2022-01-01 17:00:00,18786.99,17892.475
2022-01-01 17:30:00,19238.97,18048.4
2022年1月1日 18
......

输出：
&lt;前&gt;&lt;代码&gt;============================================== =================
  塔洪最大最小值
0 2024 南 南
1 2025 南 南
2 2026 南 南
3 2027 南 南
4 2028 南 南
5 2029 南 南
6 2030 南 南
7 2031 南 南
8 2032 南 南
9 2033 南 南

这一行都没有 NaN 值，因为我的输入文件不为空]]></description>
      <guid>https://stackoverflow.com/questions/77843589/why-the-output-of-filtering-pandas-file-becomes-nan</guid>
      <pubDate>Fri, 19 Jan 2024 03:40:25 GMT</pubDate>
    </item>
    <item>
      <title>tf.data.dataset next(iter()) 产生相同的值</title>
      <link>https://stackoverflow.com/questions/77843560/tf-data-dataset-nextiter-yields-the-same-value</link>
      <description><![CDATA[我正在尝试构建一个数据管道来推断某些视频文件。视频数量很多，因此，我使用 tf.data.dataset 管道和 from_generator 方法来创建可管理的批次。但数据集不断再次产生相同的输出。
这是我的代码：
enteclass FrameGenerator：
def __init__(self, video_paths, n_frames, Training=False):
    ”“”
        返回 video_paths 列表中视频的一组帧

        参数：
        video_paths：视频路径列表
        n_frames：帧数
        训练：一个布尔值，用于确定是否应创建训练数据集
    ”“”

    self.video_paths = video_paths
    self.n_frames = n_frames
    自我训练=训练

def __call__(自我):
    ”“”
        被调用并产生一组帧
        每次调用类的实例时
    ”“”
    视频路径 = self.视频路径
    # 打印（类型（视频路径））
    如果自我训练：
        随机播放（视频路径）

    对于 video_paths 中的路径：
        video_frames =frames_from_video_file(路径, self.n_frames)
        文件名 = 路径.split(&#39;/&#39;)[-1]
        # print(文件名, “filfile”)
        在此处生成 video_frames, file_namer 代码

代码在这里实例化：
output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype=tf.float32), tf.TensorSpec(shape = None, dtype=tf.string))

数据集 = tf.data.Dataset.from_generator(FrameGenerator(video_paths, 20, 训练=False), output_signature=output_signature)

我的批量大小为 10
以及数据集的实例：
inference_datasets = dataset.batch(BATCH_SIZE)

但是当我这样称呼时：
sample_inference_dataset = next(iter(inference_datasets))

还有这个：
输入 codesample_inference_dataset[1]

它产生相同的文件集。]]></description>
      <guid>https://stackoverflow.com/questions/77843560/tf-data-dataset-nextiter-yields-the-same-value</guid>
      <pubDate>Fri, 19 Jan 2024 03:28:36 GMT</pubDate>
    </item>
    <item>
      <title>Xgboost算法问题文件为空</title>
      <link>https://stackoverflow.com/questions/77843515/xgboost-algorithm-issue-file-empty</link>
      <description><![CDATA[我尝试使用 1.7-1 版本的 Xgboost 算法训练数据集。调用 Xgboost 函数时，它会抛出如下错误。
2024-01-19:02:57:27:INFO] 导入框架 sagemaker_xgboost_container.training
[2024-01-19:02:57:27:INFO] 未检测到 GPU（如果未安装 GPU，则正常）
[2024-01-19:02:57:27:INFO] 调用用户培训脚本。
[2024-01-19:02:57:27:错误] 报告培训失败
[2024-01-19:02:57:27:ERROR] 框架错误：
回溯（最近一次调用最后一次）：
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 2318 行，下一个
    tarinfo = self.tarinfo.fromtarfile(self)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1105 行，fromtarfile
    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1041 行，frombuf 中
    raise EmptyHeaderError(“空标题”)
tarfile.EmptyHeaderError：空标头
在处理上述异常的过程中，又出现了一个异常：
回溯（最近一次调用最后一次）：
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_trainer.py”，第 84 行，列车中
    入口点（）
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py”，第 102 行，在 main 中
    火车（框架.training_env（））
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/training.py”，第 87 行，训练中
    框架.模块.run_module(
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_modules.py”，第 290 行，在 run_module 中
    _files.download_and_extract(uri, _env.code_dir)
  文件“/miniconda3/lib/python3.8/site-packages/sagemaker_containers/_files.py”，第 131 行，位于 download_and_extract 中
    使用 tarfile.open(name=dst, mode=“r:gz”) 作为 t：
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1621 行，打开
    返回 func(名称、文件模式、fileobj、**kwargs)
  gzopen 中的文件“/miniconda3/lib/python3.8/tarfile.py”，第 1674 行
    t = cls.taropen(名称、模式、fileobj、**kwargs)
  taropen 中的文件“/miniconda3/lib/python3.8/tarfile.py”，第 1651 行
    返回 cls(名称、模式、fileobj、**kwargs)
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 1514 行，位于 __init__ 中
    self.firstmember = self.next()
  文件“/miniconda3/lib/python3.8/tarfile.py”，第 2333 行，在下一个
    引发 ReadError(“空文件”)
tarfile.ReadError：空文件
空的文件

我有两个具有相同结构且扩展名为 .csv 的源文件。
我不知道为什么它抱怨 tar 文件为空]]></description>
      <guid>https://stackoverflow.com/questions/77843515/xgboost-algorithm-issue-file-empty</guid>
      <pubDate>Fri, 19 Jan 2024 03:10:14 GMT</pubDate>
    </item>
    <item>
      <title>需要一些关于如何区分所拍摄的图像/照片是真实世界物体而不是其他图像/照片的指示吗？</title>
      <link>https://stackoverflow.com/questions/77843452/need-some-pointers-on-how-to-differentiate-the-image-photo-taken-is-of-real-worl</link>
      <description><![CDATA[听起来可能是一个愚蠢的想法，但我有这个想法来拍摄真实世界物体、风景等的图像/照片。但不太确定我可以使用什么处理或分类技术来区分这张照片是否来自现实世界或取自其他照片、图像等。提前致谢。
我尝试在网上查找，但找不到任何相关资源，或者至少找不到我想要的方向。]]></description>
      <guid>https://stackoverflow.com/questions/77843452/need-some-pointers-on-how-to-differentiate-the-image-photo-taken-is-of-real-worl</guid>
      <pubDate>Fri, 19 Jan 2024 02:47:33 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：给定 groups=1，权重大小为 [128, 64, 4, 4]，预期输入 [1, 128, 65, 65] 有 64 个通道，但得到了 128 个通道</title>
      <link>https://stackoverflow.com/questions/77843263/runtimeerror-given-groups-1-weight-of-size-128-64-4-4-expected-input1</link>
      <description><![CDATA[运行以下代码时：
`类 NLayerDiscriminator(nn.Module):
def init(self, input_nc, ndf=64, n_layers=3,norm_layer=nn.BatchNorm2d, use_sigmoid=False):
super(NLayerDiscriminator, self).init()
self.n_layers = n_layers
&lt;前&gt;&lt;代码&gt; kw = 4
    padw = int(np.ceil((kw-1.0)/2))
    self.conv1=nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw)
    self.act=nn.LeakyReLU(0.2, True)
    
    nf=min(ndf*2,512)
    self.conv2=nn.Conv2d(ndf, nf, kernel_size=kw, stride=2, padding=padw)
    self.norm1=norm_layer(nf)
    
    ngf=min(nf*2,512)
    self.conv3=nn.Conv2d(nf, ngf, kernel_size=kw, stride=1, padding=padw)
    self.norm2=norm_layer(ngf)
    self.conv4=nn.Conv2d(ngf, 1, kernel_size=kw, stride=1, padding=padw)
    self.sig=nn.Sigmoid()
    

def 前向（自身，输入）：
    x=self.conv1(输入)
    x=self.act(x)
    
    对于范围 (1, 3) 中的 n：
        x=self.conv2(x)
        x=self.norm1(x)
        x=self.act(x)
    
    x=self.conv3(x)
    x=self.norm(x)
    x=self.act(x)
    x=self.conv4(x)
    如果使用_sigmoid：
        x=self.sig(x)
    返回x
            `

我收到以下错误
RuntimeError：给定 groups=1，权重大小为 [128, 64, 4, 4]，预期输入 [1, 128, 65, 65] 有 64 个通道，但得到了 128 个通道]]></description>
      <guid>https://stackoverflow.com/questions/77843263/runtimeerror-given-groups-1-weight-of-size-128-64-4-4-expected-input1</guid>
      <pubDate>Fri, 19 Jan 2024 01:31:36 GMT</pubDate>
    </item>
    <item>
      <title>Xgboost 算法与入口点文件问题</title>
      <link>https://stackoverflow.com/questions/77843151/xgboost-algorithm-with-entry-point-file-issue</link>
      <description><![CDATA[我正在尝试创建 awsgluespark 作业来训练其中一个数据集。我在1.3-1版本中使用xgboost算法。当我尝试运行估算器时，我遇到了问题
基础设施：awsglue 4.00 Spark shell
所有文件夹都是s3路径
代码片段。
xgb_script_mode_estimator = XGBoost(
    入口点=“训练.py”，
    超参数=超参数，
    角色=角色，
    实例计数=1，
    实例类型=实例类型，
    Framework_version =“1.3-1”，
    output_path=“s3://{}/{}/{}/output”.format(hyperparameters[&#39;bucket_nm&#39;], &#39;/output/&#39;, job_name),
   

错误：
FileNotFoundError：[Errno 2]没有这样的文件或目录：&#39;training.py&#39;
我将“粘合脚本”放置在和 Training.py 与 init.py 文件位于同一文件夹中的同一作业存储桶中。
XGBoost 函数无法识别同一文件夹中的training.py（训练文件没有名称不匹配，包括大小写）]]></description>
      <guid>https://stackoverflow.com/questions/77843151/xgboost-algorithm-with-entry-point-file-issue</guid>
      <pubDate>Fri, 19 Jan 2024 00:43:36 GMT</pubDate>
    </item>
    <item>
      <title>DeepLabV3 仅生成零类</title>
      <link>https://stackoverflow.com/questions/77842007/deeplabv3-generates-only-zero-class</link>
      <description><![CDATA[我正在研究二进制分割问题。 0 代表背景，1 代表我的建筑类别。当我训练时，我的模型损失得到更新，但准确性和并集的交集根本没有改变。
纪元 [1/10]

训练 PxAcc：0.9783，训练损失：0.0840，训练 IoU：0.4891，

Val PxAcc：0.9391，Val Loss：11.2002，Val IoU：0.4695，

纪元 [2/10]

训练 PxAcc：0.9783，训练损失：0.0827，训练 IoU：0.4891，

Val PxAcc：0.9391，Val Loss：757.0062，Val IoU：0.4695，

纪元 [3/10]

训练 PxAcc：0.9783，训练损失：0.3455，训练 IoU：0.4891，

Val PxAcc：0.9391，Val Loss：2.0062，Val IoU：0.4695，

这是我的训练和验证函数：
导入火炬
将 numpy 导入为 np
从 tqdm 导入 tqdm
从指标导入 px_acc, calc_mean_iou


def train（模型，train_loader，标准，优化器，设备）：
        模型.train()
        运行损失 = 0.0
    准确度 = 0.0
    平均iou = 0.0
    all_preds = []
    所有标签 = []

    对于输入，tqdm(train_loader, desc=&quot;Training&quot;) 中的标签：
        输入，标签=输入.to（设备），标签.to（设备）
        优化器.zero_grad()

        输出 = 模型（输入）
        输出 = 输出[&#39;输出&#39;]
        损失=标准（输出，标签）
        loss.backward()
        优化器.step()

        preds = torch.argmax(输出, 暗淡=1)
        all_preds.extend(preds.cpu().numpy()) # 在转换为 numpy 之前转移到 CPU
        all_labels.extend(labels.cpu().numpy()) # 在转换为 numpy 之前移动到 CPU
        running_loss += loss.item()
    # 计算像素精度
    准确度 = px_acc(all_labels, all_preds)
    mean_iou, iou_per_class = calc_mean_iou(all_labels, all_preds, num_classes= 2)
    返回准确率、损失、mean_iou

def validate(模型、val_loader、标准、设备)：
    模型.eval()
    运行损失 = 0.0
    准确度 = 0.0
    平均iou = 0.0
    all_preds = []
    所有标签 = []

    使用 torch.no_grad()：
        对于输入，tqdm(val_loader, desc=&quot;Validation&quot;) 中的标签：
            输入，标签=输入.to（设备），标签.to（设备）

            输出 = 模型（输入）
            输出 = 输出[&#39;输出&#39;]
            损失=标准（输出，标签）
            running_loss += loss.item()

            preds = torch.argmax(输出, 暗淡=1)
            all_preds.extend(preds.cpu().numpy()) # 在转换为 numpy 之前转移到 CPU
            all_labels.extend(labels.cpu().numpy()) # 在转换为 numpy 之前移动到 CPU

        损失 = running_loss / len(val_loader)
        准确度 = px_acc(all_labels, all_preds)
        mean_iou, iou_per_class = calc_mean_iou(all_labels, all_preds, num_classes= 2)
    返回准确率、损失、mean_iou

这是我的metrics.py，其中包括并集和像素精度函数的交集。
将 numpy 导入为 np

def px_acc(y_true_list, y_pred_list):
    # 初始化总正确预测和总像素
    总正确率 = 0
    总像素 = 0

    # 迭代列表中相应的数组对
    对于 zip(y_true_list, y_pred_list) 中的 y_true、y_pred：
        Total_ Correct += (y_true == y_pred).sum()
        总像素 += y_true.size

    # 计算像素精度
    acc = 总正确率 / 总像素数
    返回帐户

def calc_mean_iou(y_true_list, y_pred_list, num_classes):
    # 初始化混淆矩阵
    conf_matrix = np.zeros((num_classes, num_classes), dtype=np.float32)

    # 填充混淆矩阵
    对于 zip(y_true_list, y_pred_list) 中的 y_true、y_pred：
        对于范围内的 i（num_classes）：
            对于范围内的 j（num_classes）：
                conf_matrix[i, j] += np.sum((y_true == i) &amp; (y_pred == j))

    # 计算每个类的 IoU
    iou_per_class = []
    对于范围内的 i（num_classes）：
        true_positives = conf_matrix[i, i]
        假阳性 = conf_matrix[:, i].sum() - 真阳性
        假阴性 = conf_matrix[i, :].sum() - 真阳性
        denom = 真阳性 + 假阳性 + 假阴性
        iou = true_positives / denom 如果 denom &gt; 0 否则 0
        iou_per_class.append(iou)

    # 计算平均 IoU
    mean_iou = np.mean(iou_per_class)

    返回mean_iou，iou_per_class
]]></description>
      <guid>https://stackoverflow.com/questions/77842007/deeplabv3-generates-only-zero-class</guid>
      <pubDate>Thu, 18 Jan 2024 19:36:18 GMT</pubDate>
    </item>
    <item>
      <title>在 VertexAI 多线程上部署 ML 模型</title>
      <link>https://stackoverflow.com/questions/77839685/deployed-ml-model-on-vertexai-multithreading</link>
      <description><![CDATA[我需要将模型部署到 VertexAI。预测端点将被不同的 Kubernetes Pod 多次调用。
我想了解部署的模型处理并发的能力如何。托管模型的实例是否会根据每个请求将模型加载到内存中？多个请求能得到很好的处理吗？在深入了解工作流程之前，我试图了解这一点。
进行了大量的研究，但目前还没有明确的答案。我发现下面的文章中用户似乎处理并发请求。然而，我仍然很困惑。在线预测似乎是同步的，所以我认为它无法处理并发请求。 
https://medium.com/mlearning-ai/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290]]></description>
      <guid>https://stackoverflow.com/questions/77839685/deployed-ml-model-on-vertexai-multithreading</guid>
      <pubDate>Thu, 18 Jan 2024 13:15:04 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：分类指标无法处理多类和多标签指标目标的混合</title>
      <link>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</link>
      <description><![CDATA[我有 2000 个不同标签的多类标记文本分类问题。使用 LSTM 和 Glove Embedding 进行分类。

目标变量的标签编码器
带有嵌入层的 LSTM 层
误差指标是 F2 分数

LabelEncoded 目标变量：
le = LabelEncoder()
le.fit(y)
train_y = le.transform(y_train)
test_y = le.transform(y_test)

LSTM 网络如下所示，带有 Glove Embeddings
np.random.seed(种子)
K.clear_session()
模型=顺序（）
model.add(嵌入(max_features, embed_dim, input_length = X_train.shape[1],
         权重=[embedding_matrix]))#,trainable=False
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add（密集（num_classes，激活=&#39;softmax&#39;））
model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;sparse_categorical_crossentropy&#39;)
打印（模型.摘要（））

我的错误指标是 F1 分数。我为错误指标构建了以下函数
类指标（回调）：
    def on_train_begin(self, 日志={}):
        self.val_f1s = []
        self.val_recalls = []
        self.val_ precisions = []
 
    def on_epoch_end(自我, 纪元, 日志={}):
        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()
        val_targ = self.validation_data[1]
        _val_f1 = f1_score(val_targ, val_predict)
        _val_recall=recall_score(val_targ, val_predict)
        _val_ precision = precision_score(val_targ, val_predict)
        self.val_f1s.append(_val_f1)
        self.val_recalls.append(_val_recall)
        self.val_ precisions.append(_val_ precision)
        print(&quot;— val_f1: %f — val_ precision: %f — val_recall %f&quot; % (_val_f1, _val_ precision, _val_recall))
        返回
 
指标=指标（）

##模型适合
model.fit（X_train，train_y，validation_data =（X_test，test_y），epochs = 10，batch_size = 64，callbacks = [指标]）

第一个纪元后出现以下错误：
ValueError：分类指标无法处理多类和连续多输出目标的混合

我的代码哪里出错了？]]></description>
      <guid>https://stackoverflow.com/questions/56496708/valueerror-classification-metrics-cant-handle-a-mix-of-multiclass-and-multilab</guid>
      <pubDate>Fri, 07 Jun 2019 14:55:37 GMT</pubDate>
    </item>
    </channel>
</rss>