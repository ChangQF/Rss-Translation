<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 07 Jun 2024 09:16:13 GMT</lastBuildDate>
    <item>
      <title>使用 CNN 上的自定义训练模型进行实时音频分类</title>
      <link>https://stackoverflow.com/questions/78590866/real-time-audio-classification-using-a-custom-trained-model-on-cnn</link>
      <description><![CDATA[我通过构建 CNN 架构自定义训练了一个模型，用于检测机器产生的不同声音以进行状态监控。现在我想实时运行这个模型，以便它可以从麦克风接收音频输入并预处理数据，将其传递给模型 (.h5) 文件并预测音频属于哪个标签。
我没有太多的编程经验，但无论我写什么代码，都会出现形状输入错误，我无法解决它，我也不知道我写的方法是否正确。]]></description>
      <guid>https://stackoverflow.com/questions/78590866/real-time-audio-classification-using-a-custom-trained-model-on-cnn</guid>
      <pubDate>Fri, 07 Jun 2024 08:54:23 GMT</pubDate>
    </item>
    <item>
      <title>这是我在运行代码时遇到的错误，训练期间出错</title>
      <link>https://stackoverflow.com/questions/78590831/this-is-my-error-that-i-faced-it-during-run-my-code-error-during-training</link>
      <description><![CDATA[Epoch 1/100 Traceback（最近一次调用）：文件“C:\Users\noor_\Desktop\Code V1 and V2 Caps\code\Squash Code try it\Oral.py”，第 384 行，在 train(model=model, data=((x_train, y_train), (x_test, y_test)), class_names=classNames, ar
这是我在运行代码时遇到的错误，有人能解决吗]]></description>
      <guid>https://stackoverflow.com/questions/78590831/this-is-my-error-that-i-faced-it-during-run-my-code-error-during-training</guid>
      <pubDate>Fri, 07 Jun 2024 08:45:55 GMT</pubDate>
    </item>
    <item>
      <title>聚类时间太长，内存不断出错</title>
      <link>https://stackoverflow.com/questions/78590456/clustering-time-is-too-long-and-memory-keeps-causing-errors</link>
      <description><![CDATA[包含10,000,000个样本，每个样本为一行，第一个样本号，剩余64个样本特征。数据来自DNA序列 最大聚类数为1000000，如果超过1000000，则多余的聚类会合并到第1000000个聚类中。评价标准为簇内距离离合器。
from sklearn.cluster import Birch
from sklearn.metrics import pairwise_distances
branching_factor = 50
n_clusters = 1000000 # 设置一个较大的聚类数
threshold = 0.5

# 创建BIRCH聚类器
birch = Birch(n_clusters=n_clusters, Threshold=threshold, Branching_factor=branching_factor)

#训练BIRCH聚类器
birch.fit(reduced_data)

BUT MemoryError: 无法为形状为 (49604310962128,) 且数据类型为 float64 的数组分配 361. TiB
我该怎么办？还有其他可用的聚类方法或技术吗？]]></description>
      <guid>https://stackoverflow.com/questions/78590456/clustering-time-is-too-long-and-memory-keeps-causing-errors</guid>
      <pubDate>Fri, 07 Jun 2024 07:27:16 GMT</pubDate>
    </item>
    <item>
      <title>AUROC = (Sum(TP)+Sum(TN)) / P+N 是否正确？[关闭]</title>
      <link>https://stackoverflow.com/questions/78589708/is-auroc-sumtpsumtn-pn-correct</link>
      <description><![CDATA[我正在阅读一篇论文“一种用于洪水敏感性评估的新型混合人工智能方法”。该论文提到 ROC 曲线下面积 (AUROC) 的方程是：

这个方程正确吗？我认为这是一种计算 AUC 的非常简单的方法。任何想法或想法，我都想在我的报告中使用该方程。
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78589708/is-auroc-sumtpsumtn-pn-correct</guid>
      <pubDate>Fri, 07 Jun 2024 02:14:29 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 flax 0.6.1 加载 flax 0.5.3 中保存的检查点</title>
      <link>https://stackoverflow.com/questions/78589693/cannot-load-checkpoint-saved-in-flax-0-5-3-with-flax-0-6-1</link>
      <description><![CDATA[我使用 flax 0.5.3 保存了程序中的检查点。当我尝试将其加载到使用 flax 0.6.1 的程序中时，我收到一条错误消息：“列表的大小和状态字典不匹配”。唯一版本发生变化的 Python 包是 flax 及其依赖项。以下是堆栈跟踪。是否可以在 flax 0.6.1 或更高版本中加载使用 flax 0.5.3 保存的检查点？有没有办法将检查点迁移到较新版本的 flax？
 app.run(main)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/absl/app.py&quot;，第 308 行，在 run
_run_main(main, args)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/absl/app.py&quot;，第 254 行，在 _run_main
sys.exit(main(argv))
文件 &quot;/home/peng/ag4masses/alphageometry/alphageometry.py&quot;，第 739 行，在 main
run_alphageometry(
文件 &quot;/home/peng/ag4masses/alphageometry/alphageometry.py&quot;，第 652 行，在 run_alphageometry
bqsearch_init()
文件&quot;/home/peng/ag4masses/alphageometry/alphageometry.py&quot;，第 529 行，在 bqsearch_init 中
model = get_lm(_CKPT_PATH.value, _VOCAB_PATH.value)
文件 &quot;/home/peng/ag4masses/alphageometry/alphageometry.py&quot;，第 213 行，在 get_lm 中
return lm.LanguageModelInference(vocab_path, ckpt_init, mode=&#39;beam_search&#39;)
文件 &quot;/home/peng/ag4masses/alphageometry/lm_inference.py&quot;，第 62 行，在 __init__ 中
(tstate, _, imodel, prngs) = trainer.initialize_model()
文件 &quot;/home/peng/aglib/meliad/training_loop.py&quot;，第 394 行，在 initialize_model 中
tstate = self.restore_checkpoint(tstate)
文件 &quot;/home/peng/aglib/meliad/training_loop.py&quot;，第 439 行，在 restore_checkpoint 中
loaded_train_state = checkpoints.restore_checkpoint(load_dir, train_state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/training/checkpoints.py&quot;，第 752 行，在 restore_checkpoint 中
return serialization.from_state_dict(target, state_dict)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 65 行，在 from_state_dict 中
return ty_from_state_dict(target, state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/struct.py&quot;，第149，在 from_state_dict 中
updates[name] = serialization.from_state_dict(value, value_state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 65 行，在 from_state_dict 中
返回 ty_from_state_dict(target, state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/struct.py&quot;，第 149 行，在 from_state_dict 中
updates[name] = serialization.from_state_dict(value, value_state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 65 行，在 from_state_dict 中
返回 ty_from_state_dict(target, state)
文件&quot;/home/peng/pyag/lib/python3.10/site-packages/flax/struct.py&quot;，第 149 行，在 from_state_dict 中
updates[name] = serialization.from_state_dict(value, value_state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 65 行，在 from_state_dict 中
return ty_from_state_dict(target, state)
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;，第 156 行，在 &lt;lambda&gt; 中
lambda xs, state_dict: tuple(_restore_list(list(xs), state_dict)))
文件 &quot;/home/peng/pyag/lib/python3.10/site-packages/flax/serialization.py&quot;, 第 110 行, _restore_list
引发 ValueError(&#39;列表的大小和状态字典不匹配,&#39;
ValueError: 列表的大小和状态字典不匹配, 得到 6 和 1。

尝试将 flac 从 0.5.3 升级到 0.6.1，并使用 flax 0.5.3 加载代码保存的检查点。但出现上述错误。]]></description>
      <guid>https://stackoverflow.com/questions/78589693/cannot-load-checkpoint-saved-in-flax-0-5-3-with-flax-0-6-1</guid>
      <pubDate>Fri, 07 Jun 2024 02:06:11 GMT</pubDate>
    </item>
    <item>
      <title>pycharm tolist 用户问题</title>
      <link>https://stackoverflow.com/questions/78589617/issue-with-pycharm-tolist-for-user</link>
      <description><![CDATA[第一次使用 pycharm 编写代码来制作聊天分析器，但显示其他用户的聊天 wt 选择没有显示
这是代码
在此处输入图片说明
检查 blackbox ai 和 chatgpt 没有帮助在此处输入图片说明
当我尝试查看显示分析时，只有总体显示，而其他用户未显示]]></description>
      <guid>https://stackoverflow.com/questions/78589617/issue-with-pycharm-tolist-for-user</guid>
      <pubDate>Fri, 07 Jun 2024 01:28:25 GMT</pubDate>
    </item>
    <item>
      <title>可教机器使用哪个版本的 tensorFlow/Keras？</title>
      <link>https://stackoverflow.com/questions/78589351/what-version-of-tensorflow-keras-does-teachable-machine-use</link>
      <description><![CDATA[我尝试按照 YouTube 上的教程操作（https://www.youtube.com/watch?v=wa2ARoUUdU8&amp;t=2877s&amp;ab_channel=Murtaza%27sWorkshop-RoboticsandAI）。当我尝试运行 tennsorflow 时，一直出现错误。视频中，该人使用 https://teachablemachine.withgoogle.com/train/tiny_image 来训练模型。我完成了所有步骤，但是在运行时（视频的 49:32。它没有运行）错误与使用支持组参数的 TensorFlow / Keras 版本训练的模型有关，但我正在使用的当前 TensorFlow / Keras 版本无法识别它。提醒一下，我使用的是 vs 而不是 py charms 和 dowloand tensflow，我的版本是 2.16.1
这是我的代码：
import cv2
from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
import tensorflow as tf
import math
import time
import tensorflow as tf
print(tf.__version__)

cap = cv2.VideoCapture(0)
detector = HandDetector(maxHands=1)#决定要检测多少只手
classifier = Classifier(&quot;Model/keras_model.h5&quot;,&quot;Model/labels.txt&quot;)

offset = 20 #用于裁剪图像以增加尺寸
imgSize = 300

folder = “数据/C” #数据存储位置
counter = 0#知道将保存多少图像

labels = [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;]

while True:
success, img = cap.read()
hands, img = detector.findHands(img)
#裁剪图像
if hands:#还要注意，当手很大/离相机太近时，程序将停止工作
hand = hands[0]#仅适用于 1 只手
x,y,w,h = hand[&#39;bbox&#39;]#粘合框并给出尺寸

imgWhite = np.ones((imgSize,imgSize,3),np.uint8)*255#创建白色背景用于裁剪图像
imgCrop = img[y-offset:y+h+offset,x-offset:x+w+offset]#给出边界框要求

imgCropShape = imgCrop.shape

aspectRatio = h/w

ifaspectRatio&gt;1:#图像高度
k = imgSize/h #拉伸高度
wCal=math.ceil(k*w)
imgResize=cv2.resize(imgCrop,(wCal,imgSize))
imgResizeShape = imgResize.shape
wGap = math.ceil((imgSize-wCal)/2)
#在白色图像上叠加图像
imgWhite[:,wGap:wCal+wGap] = imgResize #中心图像
#发送值
#predection,index = classifier.getPrediction(img)
#print(predection,index)
else:#图像宽度
k = imgSize/w #拉伸高度
hCal=math.ceil(k*h)
imgResize=cv2.resize(imgCrop,(imgSize,hCal))
imgResizeShape = imgResize.shape
hGap = math.ceil((imgSize-hCal)/2)
#在白色图像上叠加图像
imgWhite[hGap:hCal+hGap,:] = imgResize #居中图像

cv2.imshow(&quot;ImageCrop&quot;, imgCrop)#再裁剪一次
cv2.imshow(&quot;ImageWhite&quot;, imgWhite)

cv2.imshow(&quot;Image&quot;, img)
key =cv2.waitKey(1)#1 毫秒延迟

]]></description>
      <guid>https://stackoverflow.com/questions/78589351/what-version-of-tensorflow-keras-does-teachable-machine-use</guid>
      <pubDate>Thu, 06 Jun 2024 22:56:18 GMT</pubDate>
    </item>
    <item>
      <title>训练误差持续下降，但测试误差却没有下降，即使测试数据集是训练数据集的子集</title>
      <link>https://stackoverflow.com/questions/78589173/train-error-decreases-consistently-but-test-error-does-not-even-when-test-data</link>
      <description><![CDATA[我的数据包含来自传感器的 6 个特征。我正在用这些数据训练 LSTM 网络来预测三个值。
在训练过程中，我的训练损失随着每个时期而持续减少，但测试损失在几个时期后并没有减少多少。
当训练数据和测试数据之间没有重叠时就会出现这种情况。因此，我尝试使用训练数据的子集作为测试数据。
但是，仍然是相同的行为，测试损失仍然没有减少。
以下是 LSTM 模型和训练器的代码。
class LSTMModel(nn.Module):
def __init__(self, in_dim=6, hidden_​​size=200, num_layers=1, output_size=3):
super(LSTMModel, self).__init__()
self.lstm_1 = nn.LSTM(in_dim, hidden_​​size, num_layers, batch_first=True)
self.lstm_2 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True)
self.lstm_3 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True)
self.lstm_4 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True)
self.fc = nn.Linear(hidden_​​size, output_size)

def forward(self, x):
x, _ = self.lstm_1(x)
x, _ = self.lstm_2(x)
x, _ = self.lstm_3(x)
x, _ = self.lstm_4(x)
output = self.fc(x[:, -1, :])
return output

class SimpleModelTrainer:
def __init__(self, model, train_dataset, test_dataset, batch_size=1024, epochs=100, lr=0.005): # window_size=200, do_windowing=True, waiting=5, pad_testing_data = False

self.model = model
self.optimizer = AdamW(params=self.model.parameters(), lr=lr)

self.lr = lr
self.epochs = epochs
self.batch_size = batch_size
self.loss_fn = nn.L1Loss()
self.train_data = train_dataset
self.test_data = test_dataset

def train(self):
self.train_dataloader = torch.utils.data.DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, generator=torch.Generator(device=device))
self.test_dataloader = torch.utils.data.DataLoader(self.test_data, batch_size=self.batch_size, shuffle=True, generator=torch.Generator(device=device))
total_samples = 0

for epoch in tqdm(range(self.epochs), desc=&quot;epoch&quot;):
self.model.train()
total_loss = 0

for train_data in tqdm(self.train_dataloader, desc=&quot;train&quot;):

X = train_data[0]
Y = train_data[1]

if X.shape[0] != self.batch_size: continue # 以避免 RuntimeError: 形状 &#39;[16, 1, 256]&#39; 对于大小为 3328 的输入无效

total_samples += self.batch_size

y_hat = self.model(X)

loss = self.loss_fn(y_hat, Y)
self.optimizer.zero_grad()
loss.backward()
self.optimizer.step()
total_loss += loss.item()

avg_train_loss = total_loss / total_samples
val_loss = self.test(self.test_dataloader)
print(f&quot;Epoch {epoch} - Train loss:{avg_train_loss:.10f}, Val loss:{val_loss:.10f}&quot;)

def test(self, dataloader):
self.model.eval()
with torch.no_grad():
total_loss = 0
total_samples = 0
for test_data in tqdm(dataloader, desc=&quot;test&quot;):
X = test_data[0]
Y = test_data[1]

if X.shape[0] != self.batch_size: continue # 以避免 RuntimeError: shape &#39;[Y, 200, 6]&#39; 对于大小为 Z 的输入无效

total_samples += self.batch_size 

y_hat = self.model(X)

loss = self.loss_fn(y_hat, Y)
total_loss += loss.item()

val_loss = total_loss/total_samples
return val_loss

我尝试使用随机生成的虚拟变量数据集。它给出了与上述完全相同的行为！
您可以在此 colab 笔记本中查看它。
正如您在笔记本中看到的那样，自第一个时期以来，验证损失一直停留在 0.00048。但训练损失随着每个时期持续下降，从 0.00048 下降到第 28 个时期的 0.000016。
（当我写这个问题时，它仍在训练。）测试数据集是训练数据集的子集：
train_dataset = CustomDataset(windowed_input_data, windowed_target_data)
test_dataset = CustomDataset(windowed_input_data[:20000], windowed_target_data[:20000])

因此，我相信我应该得到类似的验证损失行为，验证损失也应该达到约 0.00001。我想我在代码中犯了一些愚蠢的错误（错误的 pytorch API 调用？）我的眼睛根本无法帮助我。有人可以帮帮我吗？我在概念上错过了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78589173/train-error-decreases-consistently-but-test-error-does-not-even-when-test-data</guid>
      <pubDate>Thu, 06 Jun 2024 21:42:12 GMT</pubDate>
    </item>
    <item>
      <title>卡在恶意软件功能提取上，难以找到自动化工具[关闭]</title>
      <link>https://stackoverflow.com/questions/78585761/stuck-at-the-malware-features-extraction-difficulty-at-finding-automated-tools</link>
      <description><![CDATA[我正在尝试构建一个机器学习恶意软件检测工具；我发现了多个数据集，包括微软恶意软件分类挑战 (2015) 和 CSV 数据集。问题是我卡在了特征提取上，特别是在 Windows 上自动执行静态和动态分析的工具。
我读过关于 PE（可移植可执行文件）工具（如 pestudio）、沙箱（如 cuckoo sandbox）的文章，但我卡在了需要自动执行分析的部分（即通过 python 代码调用工具）。
我曾尝试使用 IDA 反汇编程序将二进制文件转换为 .asm 和 .bytes，但似乎我需要专业版才能自动执行转换。
如果有人可以推荐我可以用来自动执行 Windows 上的分析和特征提取的工具，我将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78585761/stuck-at-the-malware-features-extraction-difficulty-at-finding-automated-tools</guid>
      <pubDate>Thu, 06 Jun 2024 09:43:08 GMT</pubDate>
    </item>
    <item>
      <title>在 Azure Databricks 中使用 pyspark ML 库函数时出现 Py4J 安全错误</title>
      <link>https://stackoverflow.com/questions/78585509/py4j-security-error-while-using-pyspark-ml-library-functions-in-azure-databricks</link>
      <description><![CDATA[我试图在我的 Azure Databricks 工作簿中运行以下代码
import pyspark.ml.feature
from pyspark.ml.feature import Tokenizer,StopWordsRemover
tokenizer = Tokenizer()

但是我遇到了这个错误：
Py4JError：调用 
None.org.apache.spark.ml.feature.Tokenizer 时发生错误。跟踪：
py4j.security.Py4JSecurityException：构造函数 public 
org.apache.spark.ml.feature.Tokenizer(java.lang.String) 未列入白名单。

StopWordsRemover 和 pyspark.ml.feature 中的其他一些函数也会出现类似的错误
有没有办法避免此错误，以便我可以使用相同的代码？]]></description>
      <guid>https://stackoverflow.com/questions/78585509/py4j-security-error-while-using-pyspark-ml-library-functions-in-azure-databricks</guid>
      <pubDate>Thu, 06 Jun 2024 08:56:01 GMT</pubDate>
    </item>
    <item>
      <title>Azure ML-如何从 Blob 容器注册模型？</title>
      <link>https://stackoverflow.com/questions/78584772/azure-ml-how-to-register-model-from-blob-container</link>
      <description><![CDATA[我有预定义的模型并存储在 blob 存储中。现在我想注册模型并部署它，并在 Azure ML 中启用端点。我使用了以下代码
from azureml.core import Model
from azureml.core import Workspace

subscription_id = &#39;mysub
resource_group = &#39;my_resource_group&#39;
workspace_name = &#39;my_ws_name&#39;

ws = Workspace(subscription_id, resource_group, working_name)

model_path = &#39;my_model.joblib&#39;

container = &#39;mycontainer&#39;

model_name = &#39;my_model_v1&#39;

model = Model.register(workspace=ws,
model_name=model_name,
model_path=model_path,
description=&quot;Test_Model&quot;,
tags={&#39;area&#39;: &quot;emotion detection&quot;},
model_framework=Model.Framework.SCIKITLEARN,
model_framework_version=&#39;0.24.1&#39;,
resource_configuration=None,
container_registry=None,
properties=None,
sample_input_dataset=None,
sample_output_dataset=None,
datasets=None,
model_url=f&#39;https://mystorage.blob.core.windows.net/{container}/{model_path}&#39;)
print(&quot;Ws object created&quot;)

但下面的代码返回错误
TypeError: register() 得到了一个意外的关键字参数 &#39;container_registry&#39;

有没有什么解决办法或者其他方法？]]></description>
      <guid>https://stackoverflow.com/questions/78584772/azure-ml-how-to-register-model-from-blob-container</guid>
      <pubDate>Thu, 06 Jun 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 logistf 包时，Firth 的模型在 R 中卡住了（出现未收敛警告，CPU 使用率高达 99%）</title>
      <link>https://stackoverflow.com/questions/78579401/firths-model-stuck-with-non-converge-warning-and-cpu-usage-99-in-r-using-the</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78579401/firths-model-stuck-with-non-converge-warning-and-cpu-usage-99-in-r-using-the</guid>
      <pubDate>Wed, 05 Jun 2024 07:34:40 GMT</pubDate>
    </item>
    <item>
      <title>加载 pickle NotFittedError：TfidfVectorizer - 词汇表不适合</title>
      <link>https://stackoverflow.com/questions/57213165/loading-pickle-notfittederror-tfidfvectorizer-vocabulary-wasnt-fitted</link>
      <description><![CDATA[多标签分类
我正在尝试使用 scikit-learn/pandas/OneVsRestClassifier/logistic 回归预测多标签分类。构建和评估模型有效，但尝试对新样本文本进行分类无效。
场景 1：
一旦我构建了一个模型，就使用名称 (sample.pkl) 保存该模型并重新启动我的内核，但是当我在对样本文本进行预测期间加载已保存的模型 (sample.pkl) 时，它给出了错误：
 NotFittedError：TfidfVectorizer - 词汇表不适合。

我构建了模型并评估了模型，然后我使用名称 sample.pkl 保存了该模型。我重启了内核，然后加载模型对样本文本进行预测 NotFittedError: TfidfVectorizer - 词汇表不适合
推理
import pickle,os
import collections
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import Counter
from nltk.corpus import stopwords
import json, nltk, re, csv, pickle
from sklearn.metrics import f1_score # 性能矩阵
from sklearn.multiclass import OneVsRestClassifier # 二元相关性
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.feature_extraction.text 导入 CountVectorizer
从 sklearn.preprocessing 导入 MultiLabelBinarizer
从 sklearn.model_selection 导入 train_test_split
从 sklearn.linear_model 导入 LogisticRegression
stop_words = set(stopwords.words(&#39;english&#39;))

def cleanHtml(sentence):
&#39;&#39;&#39;&#39; 删除标签 &#39;&#39;&#39;
cleanr = re.compile(&#39;&lt;.*?&gt;&#39;)
cleantext = re.sub(cleanr, &#39; &#39;, str(sentence))
return cleantext

def cleanPunc(sentence): 
&#39;&#39;&#39; 函数用于清除单词中的任何标点符号或特殊字符 &#39;&#39;&#39;
cleaned = re.sub(r&#39;[?|!|\&#39;|&quot;|#]&#39;,r&#39;&#39;,sentence)
cleaned = re.sub(r&#39;[.|,|)|(|\|/]&#39;,r&#39; &#39;,cleaned)
cleaned = cleaned.strip()
cleaned = cleaned.replace(&quot;\n&quot;,&quot; &quot;)
return cleaned

def keepAlpha(sentence):
&quot;&quot;&quot; 保留 alpha 句子 &quot;&quot;&quot;
alpha_sent = &quot;&quot;
for word in sentence.split():
alpha_word = re.sub(&#39;[^a-z A-Z]+&#39;, &#39; &#39;, word)
alpha_sent += alpha_word
alpha_sent += &quot; &quot;
alpha_sent = alpha_sent.strip()
return alpha_sent

def remove_stopwords(text):
&quot;&quot;&quot; 删除停用词 &quot;&quot;&quot;
no_stopword_text = [w for w in text.split() if not w in stop_words]
return &#39; &#39;.join(no_stopword_text)

test1 = pd.read_csv(&quot;C:\\Users\\abc\\Downloads\\test1.csv&quot;)
test1.columns

test1.head()
siNo plot movie_name gender_new
1 故事以 Hannah 开始... 唱歌 [戏剧,teen]
2 Debbie 最喜欢的乐队是 Dream... 最忠实的粉丝 [戏剧]
3 这个祖鲁家庭的故事是... 回来，非洲 [戏剧,纪录片]

获取错误
当我对示例文本进行推理时，我在这里获取错误
def infer_tags(q):
q = cleanHtml(q)
q = cleanPunc(q)
q = keepAlpha(q)
q = remove_stopwords(q)
multilabel_binarizer = MultiLabelBinarizer()
tfidf_vectorizer = TfidfVectorizer()
q_vec = tfidf_vectorizer.transform([q])
q_pred = clf.predict(q_vec)
return multilabel_binarizer.inverse_transform(q_pred)

for i in range(5):
print(i)
k = test1.sample(1).index[0] 
print(&quot;电影：&quot;, test1[&#39;movie_name&#39;][k], &quot;\n预测类型：&quot;, infer_tags(test1[&#39;plot&#39;][k])), print(&quot;实际类型：&quot;,test1[&#39;genre_new&#39;][k], &quot;\n&quot;)


已解决
我解决了将 tfidf 和 multibiniraze 保存到 pickle 模型中的问题
from sklearn.externals import joblib
pickle.dump(tfidf_vectorizer, open(&quot;tfidf_vectorizer.pickle&quot;, &quot;wb&quot;))
pickle.dump(multilabel_binarizer, open(&quot;multibinirizer_vectorizer.pickle&quot;, &quot;wb&quot;))
vectorizer = joblib.load(&#39;/abc/downloads/tfidf_vectorizer.pickle&#39;)
multilabel_binarizer = joblib.load(&#39;/abc/downloads/multibinirizer_vectorizer.pickle&#39;)

def infer_tags(q):
q = cleanHtml(q)
q = cleanPunc(q)
q = keepAlpha(q) 
q = remove_stopwords(q)
q_vec = vectorizer .transform([q])
q_pred = rf_model.predict(q_vec)
return multilabel_binarizer.inverse_transform(q_pred)

我通过以下链接找到了解决方案
,https://stackoverflow.com/questions/32764991/how-do-i-store-a-tfidfvectorizer-for-future-use-in-scikit-learn&gt;]]></description>
      <guid>https://stackoverflow.com/questions/57213165/loading-pickle-notfittederror-tfidfvectorizer-vocabulary-wasnt-fitted</guid>
      <pubDate>Fri, 26 Jul 2019 04:21:05 GMT</pubDate>
    </item>
    <item>
      <title>从熊猫数据框中删除高度相关的列</title>
      <link>https://stackoverflow.com/questions/44889508/remove-highly-correlated-columns-from-a-pandas-dataframe</link>
      <description><![CDATA[我有一个名为 data 的数据框，我使用其计算了相关矩阵
corr = data.corr()

如果两列之间的相关性大于 0.75，我想从数据框 data 中删除其中一列。我尝试了一些选项
raw =corr[(corr.abs()&gt;0.75) &amp; (corr.abs() &lt; 1.0)]

但这没有帮助；我需要 raw 中值非零的列号。基本上是以下 R 命令的一些 Python 等效命令（使用函数 findCorrelation）。
{hc=findCorrelation(corr,cutoff = 0.75)

hc = sort(hc)

data &lt;- data[,-c(hc)]}

如果有人能帮助我在 Python pandas 中获取类似于上述 R 命令的命令，那将会很有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/44889508/remove-highly-correlated-columns-from-a-pandas-dataframe</guid>
      <pubDate>Mon, 03 Jul 2017 15:39:25 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的 L1/L2 正则化</title>
      <link>https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch</link>
      <description><![CDATA[如何在 PyTorch 中添加 L1/L2 正则化而无需手动计算？]]></description>
      <guid>https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch</guid>
      <pubDate>Thu, 09 Mar 2017 19:54:19 GMT</pubDate>
    </item>
    </channel>
</rss>