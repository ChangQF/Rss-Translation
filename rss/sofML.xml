<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>主动问题标记的机器学习 - 堆栈溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>最近的30个来自stackoverflow.com</description>
    <lastBuildDate>Sun, 06 Apr 2025 12:31:44 GMT</lastBuildDate>
    <item>
      <title>检测到重复的GPU：CUDA设备上的等级为0和等级1 40</title>
      <link>https://stackoverflow.com/questions/79557948/duplicate-gpu-detected-rank-0-and-rank-1-both-on-cuda-device-40</link>
      <description><![CDATA[我试图使用2x T4 GPU在Kaggle上进行Qlora+FSDP2，这是我的培训脚本
  def train_fsdp（等级，大小）：
    TORCH.CUDA.EMPTY_CACHE（）
    base_model =＆quot; meta-llama/llama-3.1-8b; quot
    bnb_config = bitsandbytesconfig（-------）
    型号= automodelforcausallm.from_pretaining（
        base_model，
        attn_implementation =＆quot; sdpa＆quot;
        ventalization_config = bnb_config，
        TORCH_DTYPE = TORCH.BFLOAT16，
    ）
    tokenizer = autotokenizer.from_pretaining（
        base_model，
        padding_side =; left＆quot;
        add_eos_token = true，
        add_bos_token = false，
        use_fast = true
    ）
    lora_config = loraconfig（-----）
    model.add_adapter（lora_config）
    fsdp_model = fsdp（
        模型，
        sharding_strategy = shardingstrategy.hybrid_shard，
        auto_wrap_policy = my_auto_wrap_policy，
        device_id =等级，
        混合_precision = myd_precision_policy，
        backward_prefetch = backwardprefetch.backward_pre，
        forward_prefetch = true，
    ）
    triending_arguments = sftConfig（
        ........
    ）
    培训师= sfttrainer（
        型号= fsdp_model，
        train_dataset = dataset_train_pre，
        processing_class = tokenizer，
        args = triending_arguments，
    ）
    对于名称，triber.model.named_modules（）中的模块：
        如果“规范”名称：
            模块=模块。
    Trainer.Train（）
    dist.destroy_process_group（）
    
DEF INIT_PROCESS（等级，大小，FN）：
    os.environ [&#39;master_addr&#39;] =&#39;127.0.0.1&#39;
    OS.Environ [&#39;Master_port&#39;] =&#39;29501&#39;
    os.environ [&#39;nccl_debug&#39;] =&#39;info&#39;
    os.environ [&#39;nccl_ib_disable&#39;] =&#39;1&#39;
    os.environ [&#39;nccl_socket_ifname&#39;] =&#39;lo&#39;
    os.environ [＆quot; pytorch_cuda_alloc_conf;]
    ＆quot&#39;roundup_power2_divisions：[32：256,64：128,256：64，＆gt;：32]＆quot;
    dist.init_process_group（
        后端=&#39;nccl&#39;，
        等级=等级，
        world_size =大小，
        超时= timedelta（分钟= 5）
    ）
    FN（等级，大小）
如果__name__ ==＆quot __ Main __＆quot;：
    world_size = 2
    过程= []
    mp.get_context（“ Spawn＆quort”）
    对于范围的排名（world_size）：
        p = mp.process（target = init_process，args =（rank，world_size，train_fsdp））
        p.start（）
        process.append（p） 
    对于P流程的P：
        P.Join（）
 
但是我一直在遇到上述错误（标题）我想问题是我在不同等级上初始化模型的方式，但我不完全确定。
详细的错误日志：
  TORCH.OUTOFMEMORYERROR：CUDA失败。试图分配1.97吉布。 GPU 0的总容量为14.74 GIB，其中1.91 GIB是免费的。 Process 4629使用了5.43 GIB内存。 Process 4627使用了7.40个GIB内存。在分配的内存4.76 GIB中，由Pytorch分配，498.26 MIB由Pytorch保留，但未分配。如果保留但未分配的内存是大的，请尝试设置pytorch_cuda_alloc_conf = Expandable_segments：true以避免碎片。  请参阅文档以获取内存管理  
 ../torch/csrc/distributed/c10d/ncclutils.hpp:317，无效用法（使用nccl_debug = WARN进行详细信息），NCCL版本2.21.5
NCCLINVALIDUSAGE：这通常反映了NCCL库的无效用法。
最后错误：
检测到重复的GPU：CUDA设备上的等级为0和等级1 40
 
我也尝试过，但也没有得到结果，我也尝试过设置型号。
fsdp_model = FSDP( model, sharding_strategy=ShardingStrategy.HYBRID_SHARD, auto_wrap_policy=my_auto_wrap_policy, device_id=rank, mixed_precision=mixed_precision_policy, backward_prefetch=BackwardPrefetch.BACKWARD_PRE, forward_prefetch=True, ） ]]></description>
      <guid>https://stackoverflow.com/questions/79557948/duplicate-gpu-detected-rank-0-and-rank-1-both-on-cuda-device-40</guid>
      <pubDate>Sun, 06 Apr 2025 08:10:36 GMT</pubDate>
    </item>
    <item>
      <title>CNN模型的问题以预测黑白图像的颜色</title>
      <link>https://stackoverflow.com/questions/79557643/problem-with-cnn-model-for-prediction-of-color-from-an-black-and-white-image</link>
      <description><![CDATA[我已经创建了一个CNN模型，该模型作为输入13x13图像刻度刻度实验室L通道。我用12000张图像培训了它，并获得了7个MSE错误。
要使用该模型，我需要将图像切成13x13的小图像，然后通过网络进行馈送。
我从原始小图像中将 l通道从一个图像中放在一个像素上，并将其放在一个像素中，以将其放在一个像素上，以供带有颜色的预测图像。之后，我将所有这些预测的像素放在一起并形成预测的图像。
这是创建这些预测像素的功能：
  def create_pxl_from_preds（input_image，预测）：
    prediction_rescaled = torch.mul（预测，128）
    a，b = prediction_rescaled [0]
    a = a.detach（）。numpy（）
    b = b.detach（）。numpy（）

    l_channel = input_image [6，6]

    a_channel = np.full（（1，1），a，dtype = np.float32）
    b_channel = np.full（（1，1），b，dtype = np.float32）
    l_channel = np.full（（1，1），l_channel，dtype = np.float32）

    l_channel =（l_channel / 255 * 100）.astype（np.float32）
    a_channel = a_channel.astype（np.float32）
    b_channel = b_channel.astype（np.float32）

    image_pred = cv2.merge（[l_channel，a_channel，b_channel]）
    返回image_pred
 
之后，我有一个函数来构建图像行和一个函数，可以将这些行放在一起并形成整个图像。另外，总有487个图像，也有487行一起
  def rebuild_image_pxl_row（
        start_calc：int，
        end_calc：int，
        num_sections_per_row = 487，
） - ＆gt;没有任何：
    tensor_rows = torch.arange（start_calc，end_calc）
    list_rows = tensor_rows.tolist（）
    add_index = 0
    对于我在list_rows中：
        idx = i

        image_files = [f in os.listdir（temp_folder_images）如果f.lower（）。endswith（（（&#39;。
        image_files.sort（key = extract_numbers）

        start_idx = idx * num_sections_per_row
        start_idx += add_index

        end_idx = start_idx + num_sections_per_row
        row_sections = image_files [start_idx：end_idx]
        print（f＆quot&#39;start idx：{start_idx}，end IDX：{end_idx}＆quort;）

        row_images = []

        对于索引，枚举中的section_file（row_sections）：
            section_path = os.path.join（temp_folder_images，section_file）
            section_image = cv2.imread（section_path，cv2.imread_grayscale）
            section_tensor = torch.from_numpy（section_image）.float（）。unsqueeze（0）.unsqueeze（0）
            pection_pred = conv_model（extract_tensor）
            section_reconstructed = create_pxl_from_preds（extart_image，section_pred）
            row_images.append（np.Round（cv2.cvtcolor（extife_reconstructed，cv2.color_lab2bgr）*255.0，0））

        行= np.hstack（row_images）
        row_path = os.path.join（temp_folder_rows，f＆quot; row_ {idx} .png; quert;）
        cv2.imwrite（row_path，行）
        add_index += 1
        打印（f＆quort&#39;row {idx}重建并保存为{row_path}。
 
  def rebuild_image_pxl（row_ordner，target_height = 487）：
    row_files = [f in os.listdir（row_ordner）如果f.lower（）。endswith（（（&#39;。png&#39;，&#39;.jpg&#39;，&#39;.jpeg&#39;，&#39;.bmp&#39;））]]]]]]]]]
    row_files.sort（key = extract_numbers）
    打印（row_files [：20]）

    row_files = row_files [：target_height]

    all_rows = []

    对于索引，ROW_FILE在枚举中（row_files）：
        row_path = os.path.join（row_ordner，row_file）
        row_image = cv2.imread（row_path）
        all_rows.append（row_image）

    final_image = cv2.vconcat（all_rows）
    final_image = np.fliplr（final_image）
    final_image = cv2.Rotate（final_image，cv2.rotate_90_counterclockwise）
    cv2.imwrite（&#39;image.png; final_image）
    show_image（&#39;image.png;）
 
我将OpenCV用作图像处理库。
但是，当我尝试运行程序时，不会发生任何错误，但是预测图像的颜色不正确，甚至不接近原始图像。检测到L通道的边缘和细微差别，因此该数据的复制有效。但是颜色预测并非如此，我认为这是OpenCV和不同颜色空间BGR，RGB和LAB的东西。由于OpenCV将图像打开并保存在BGR色彩空间中，因此我只是将图像值从实验室转换为BGR。
我需要弄清楚错误在哪里的帮助。
预测的图像看起来像这样：
在此处输入图像描述 ]]></description>
      <guid>https://stackoverflow.com/questions/79557643/problem-with-cnn-model-for-prediction-of-color-from-an-black-and-white-image</guid>
      <pubDate>Sat, 05 Apr 2025 23:37:53 GMT</pubDate>
    </item>
    <item>
      <title>jinaai/jina-embeddings-v3嵌入模型没有注意力输出</title>
      <link>https://stackoverflow.com/questions/79557313/no-attention-output-in-jinaai-jina-embeddings-v3-embedding-model</link>
      <description><![CDATA[当我使用此模型时 -  
 来自变形金刚的导入汽车，自动驱动器

model_id =＆quot; jinaai/jina-embeddings-v3＆quot
tokenizer = autotokenizer.from_pretaining（model_id，trust_remote_code = true）
model = automodel.from_pretrataining（model_id，trust_remote_code = true）

输入= tokenizer（[
    “今天的天气很愉快。
    “外面太晴天！＆quot”
    “他开车去体育场。”
]，return_tensors =; pt; padding = true，truncation = true）

输出=模型（**输入，output_attentions = true）

注意力=输出
 
我得到了这个警告，这似乎是矛盾的 -  
 未安装flash_attn。使用Pytorch本地注意力实施。
Flash注意力实现不支持Kwargs：output_attentions
 
注意力是没有
我尝试了其他型号，并且可以按预期工作。]]></description>
      <guid>https://stackoverflow.com/questions/79557313/no-attention-output-in-jinaai-jina-embeddings-v3-embedding-model</guid>
      <pubDate>Sat, 05 Apr 2025 17:29:15 GMT</pubDate>
    </item>
    <item>
      <title>有什么方法可以将具有文本文本的数据集转换为无需任何鉴定模型的语音吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79557306/is-there-any-way-to-convert-the-dataset-which-has-text-to-speech-without-any-pre</link>
      <description><![CDATA[我已经开始从事一个有关Vishing检测的项目，我更多地致力于在骗子和试图弄清楚的用户之间转换随机短信，并对他与骗子进行的对话感到困惑。有人可以提出某些ML中可用的方法并可以将文本转换为音频消息吗？]]></description>
      <guid>https://stackoverflow.com/questions/79557306/is-there-any-way-to-convert-the-dataset-which-has-text-to-speech-without-any-pre</guid>
      <pubDate>Sat, 05 Apr 2025 17:22:16 GMT</pubDate>
    </item>
    <item>
      <title>使用KERAS自定义数据生成器表现不佳的模型[关闭]</title>
      <link>https://stackoverflow.com/questions/79557177/model-performing-poorly-with-keras-custom-data-generator</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79557177/model-performing-poorly-with-keras-custom-data-generator</guid>
      <pubDate>Sat, 05 Apr 2025 15:30:58 GMT</pubDate>
    </item>
    <item>
      <title>如何提高Vittracker的性能？ [关闭]</title>
      <link>https://stackoverflow.com/questions/79556795/how-can-i-improve-the-performance-of-vittracker</link>
      <description><![CDATA[我将VitTracker与OpenCV一起使用，这实际上是一个对象跟踪器。但是它无法处理一些边缘案例。例如，如果有包含相似对象的图像帧，则Bbox在它们之间振荡。当对象消失时，它会失去目标，但继续搜索其他对象。另一件事是，在实际用例中，意外的相机移动会导致对象突然移动。
这些边缘情况有什么解决方案？
您知道，Vittracker使用模板和基于自我发项机制的搜索。核心原理是将存储目标特征的模板令牌与搜索令牌匹配，该图令牌代表了使用自我注意的机制来代表跟踪器的搜索区域。为了解决突然的相机移动引起的边缘案例，我试图增加搜索区域的大小。]]></description>
      <guid>https://stackoverflow.com/questions/79556795/how-can-i-improve-the-performance-of-vittracker</guid>
      <pubDate>Sat, 05 Apr 2025 09:51:00 GMT</pubDate>
    </item>
    <item>
      <title>attributeError：'aattn'对象在yolov12上运行推理时没有属性'qkv'</title>
      <link>https://stackoverflow.com/questions/79556669/attributeerror-aattn-object-has-no-attribute-qkv-when-running-inference-on</link>
      <description><![CDATA[我正在尝试使用我的yolov12型号进行推理，但是我遇到了以下错误：
  attributeError：&#39;aattn&#39;对象没有属性&#39;qkv&#39;
 
这是我的代码：
 从超级物质导入YOLO

型号= yolo（r＆quot; d：\ a \ b \ model \ newdatasetversion \ 12 \ m \ m \ best.pt＆quort;

img = r＆quot d：\ a \ br \ program \ prograpping \ frames_kopo \ kopo0_20250228_111522_frame_1.jpg＆quot;
结果=模型（img，save = false）
 
我不完全确定是什么原因引起的。这是一些上下文：

我正在使用yolov12（定制训练）。
错误在推理期间发生，而不是训练。

我尝试加载Yolov12模型并在图像上运行推断。我希望该模型像往常一样返回预测（边界框，类等）。但是，我没有得到输出，而是得到了这个错误。
我没有手动修改模型体系结构，只是使用Yolo12m.pt训练了我的自定义数据集，所以我不确定为什么会发生这种情况。我期望推断能像以前一样正常运行。
可能导致此问题或我该如何修复？]]></description>
      <guid>https://stackoverflow.com/questions/79556669/attributeerror-aattn-object-has-no-attribute-qkv-when-running-inference-on</guid>
      <pubDate>Sat, 05 Apr 2025 07:17:45 GMT</pubDate>
    </item>
    <item>
      <title>“+ptx85”不是此目标的公认功能（忽略功能）-NVIDIA TensorFlow容器[封闭]</title>
      <link>https://stackoverflow.com/questions/79556486/ptx85-is-not-a-recognized-feature-for-this-target-ignoring-feature-nvidia</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79556486/ptx85-is-not-a-recognized-feature-for-this-target-ignoring-feature-nvidia</guid>
      <pubDate>Sat, 05 Apr 2025 02:19:32 GMT</pubDate>
    </item>
    <item>
      <title>yolov11培训给火炬。</title>
      <link>https://stackoverflow.com/questions/79553429/yolov11-training-gives-torch-outofmemoryerror</link>
      <description><![CDATA[我正在尝试为一组图像训练Yolov11模型，但我遇到了一个问题。该模型正在训练的GPU是GeForce RTX 4070 Super。
我将yolov11与gpu一起用火炬。
  print（Torch .__版本__）
2.5.1+CU121
 
我在没有遇到此命令的问题之前几次训练了该模型：
  yolo任务=检测模式=火车设备= 0 epochs = 2000 batch = 32 data =; c：\ project \ yolo \ yolo \ data_custom.yaml; model =; c：\ project \ yolo \ yolov11m.pt; IMGSZ = 640
 
，但这并没有给我足够好的结果。因此，如果我以前理解“ imgsz”的价值来理解。到1440年（我的图像为2560 x 1440），该模型应训练剪钩质量的图像。所以我尝试运行此命令：
  yolo任务=检测模式=火车设备= 0 epochs = 2000 batch = 32 data =; c：\ project \ yolo \ yolo \ data_custom.yaml; model =; c：\ project \ yolo \ yolov11m.pt; IMGSZ = 640
 
运行此命令给我一个我不知道如何解决的错误。错误如下：
  TORCH.OUTOFMEMORYERROR：CUDA失败。试图分配1.48吉布。 GPU 0的总容量为11.99 GIB，其中0字节是免费的。在分配的内存19.12 GIB中，Pytorch分配了406.58 MIB，由Pytorch保留，但未分配。如果保留但未分配的内存是大的，请尝试设置pytorch_cuda_alloc_conf = Expandable_segments：true以避免碎片。  请参阅记忆管理（https://pytorch.org/docs/stable/notes/cuda.html#environment-variables）的文档。
 
有人如何解决这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/79553429/yolov11-training-gives-torch-outofmemoryerror</guid>
      <pubDate>Thu, 03 Apr 2025 15:54:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么拥抱面提供的DeepSeek代码会导致“未知量化类型”错误？</title>
      <link>https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t</link>
      <description><![CDATA[我正在使用huggingface的此代码：
此代码直接从 deepseek上的huggingface网站页面上的页面

 来自变形金刚导入管道

消息= [
{&#39;&#39;：＆quot“ user quot”内容“：;
这是给出的
pipe =管道（＆quot&#39;text-generation＆quot; deepseek-ai/deepseek-r1＆quort; trust_remote_code = true）
管道（消息）
 

，但我无法加载模型。当我这样做时，我会得到这个问题：
  file＆quot＆lt; ...＆gt;/site-packages/transformers/quantizers/auto.py＆quot;，第97行，在from_dict

提高价值Error（

ValueError：未知量化类型，获得FP8-支持类型为： 
[&#39;awq&#39;，&#39;bitsandbytes_4bit&#39;，&#39;bitsandbytes_8bit&#39;，&#39;gptq&#39;，&#39;aqlm&#39;，&#39;quanto&#39;，&#39;eetq&#39;，&#39;eetq&#39;， 
&#39;HQQ&#39;，“压缩张量”，“ fbgemm_fp8&#39;，&#39;torchao&#39;，&#39;bitnet&#39;]
 
我尝试了不同的代码：
 导入火炬
generate_text = pipeline（model =; deepSeek-ai/deepSeek-r1; torch_dtype = torch.bfloat16，trust_remote_code = true，device_map =; auto;
generate_text（消息）
 
这给出以下错误：

raise ValueError( ValueError: Unknown quantization type, got fp8 - supported types are: [&#39;awq&#39;, &#39;bitsandbytes_4bit&#39;, &#39;bitsandbytes_8bit&#39;, &#39;gptq&#39;, &#39;aqlm&#39;, &#39;quanto&#39;, &#39;eetq&#39;, &#39;higgs&#39;, &#39;hqq&#39;, &#39;compressed-tensors&#39;, &#39;fbgemm_fp8&#39;, &#39;torchao&#39;，&#39;bitnet&#39;，&#39;vptq&#39;] 

我该怎么办？]]></description>
      <guid>https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t</guid>
      <pubDate>Sun, 09 Feb 2025 03:05:30 GMT</pubDate>
    </item>
    <item>
      <title>Infror：导入onnx_cpp2py_export时DLL负载失败：动态链接库（DLL）初始化例程失败</title>
      <link>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78996950/importerror-dll-load-failed-while-importing-onnx-cpp2py-export-a-dynamic-link</guid>
      <pubDate>Wed, 18 Sep 2024 07:08:40 GMT</pubDate>
    </item>
    <item>
      <title>从python中的Sklearlen线性回归获得置信区间</title>
      <link>https://stackoverflow.com/questions/61292464/get-confidence-interval-from-sklearn-linear-regression-in-python</link>
      <description><![CDATA[我想获得线性回归结果的置信区间。我正在与波士顿房屋价格数据集合作。
我发现了这个问题：
如何计算Python中线性回归模型的99％置信区间？
但是，这并不完全回答我的问题。
这是我的代码：
 导入numpy作为np
导入matplotlib.pyplot作为PLT
来自数学导入pi

导入大熊猫作为pd
进口海洋作为SNS
来自sklearn.datasets import load_boston
来自sklearn.model_selection导入train_test_split
来自sklearn.linear_model导入linearrecress
来自sklearn.metrics导入均值_squared_error，r2_score

＃导入数据
boston_dataset = load_boston（）

波士顿= pd.dataframe（boston_dataset.data，columns = boston_dataset.feature_names）
波士顿[&#39;medv​​&#39;] = boston_dataset.target

x = pd.dataframe（np.c_ [boston [&#39;lstat&#39;]，波士顿[&#39;rm&#39;]]，columns = [&#39;lstat&#39;，&#39;rm&#39;]）
Y =波士顿[&#39;MEDV&#39;]

＃将培训和测试数据集分为80％：20％
＃将Random_State分配给任何值。这确保一致性。
x_train，x_test，y_train，y_test = train_test_split（x，y，test_size = 0.2，andural_state = 5）

lin_model = linearregression（）
lin_model.fit（x_train，y_train）

＃培训集的模型评估

y_train_predict = lin_model.predict（x_train）
rmse =（np.sqrt（mean_squared_error（y_train，y_train_predict））））））
r2 = r2_score（y_train，y_train_predict）

＃测试集的模型评估

y_test_predict = lin_model.predict（x_test）
＃模型的根平方错误
rmse =（np.sqrt（mean_squared_error（y_test，y_test_predict））））））））

＃模型的R平方分数
r2 = r2_score（y_test，y_test_predict）

plt. -scatter（y_test，y_test_predict）
plt.show（）
 
例如，我如何从中获得95％或99％的置信区间？是否有某种内置的功能或代码？]]></description>
      <guid>https://stackoverflow.com/questions/61292464/get-confidence-interval-from-sklearn-linear-regression-in-python</guid>
      <pubDate>Sat, 18 Apr 2020 16:22:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么imagedatagenerator（）表现不佳？</title>
      <link>https://stackoverflow.com/questions/58562089/why-is-imagedatagenerator-performing-poorly</link>
      <description><![CDATA[我正在尝试使用Imagedatagenerator（）构建图像分类模型。 
看来该模型训练并表现不佳。训练损失停留在15左右，精度仅为10％，验证差异大致相同。
只是为了看看会发生什么，我尝试培训而不使用Imagedatagenerator（）并以类似的方式设置数据。它在培训，验证和测试方面的表现要好得多。训练损失为0.71，精度为75％，验证损失为0.8，精度为72％。 
我需要使用数据生成器来弄清楚此模型，因为我将继续使用较大的数据集，它将不适合内存。
所以，我想我的问题是我对成像的Atageatagener（）的表现如此出色，我该如何改善结果？
设置文件（在所有火车，测试，验证文件夹中）时，有具有自己的文件夹的类，在这些文件夹中是图像所在的位置。 
这是代码：
 将TensorFlow导入为TF
来自tensorflow.keras.preprocessing.image导入成像的Atagenerator
进口泡菜
来自tensorflow.keras.models导入顺序
来自tensorflow.keras.layers导入密集，激活，扁平，conv2d，maxpooling2d，辍学

data_gen = imagedatagenerator（）
img_size = 100
train_it = data_gen.flow_from_directory（&#39;d：/.../ train/&#39;，class_mode =&#39;sparse&#39;，
                                       target_size =（img_size，img_size），color_mode =&#39;grayscale&#39;，shuffle = true，batch_size = 32）
val_it = data_gen.flow_from_directory（&#39;d：/.../验证/&#39;，class_mode =&#39;sparse&#39;，
                                     target_size =（img_size，img_size），color_mode =&#39;grayscale&#39;，shuffle = true，batch_size = 32）

image_size = [100，100]

型号=顺序（）
model.Add（conv2d（32，（3,3），input_shape = [*image_size，1]））））
model.Add（激活（&#39;relu&#39;））
model.Add（maxpooling2d（pool_size =（2,2）））

ADD（辍学（0.5））

Add（Conv2d（32，（3,3）））
model.Add（激活（&#39;relu&#39;））
model.Add（maxpooling2d（pool_size =（2,2）））

ADD（辍学（0.5））

Add（Conv2d（32，（3,3）））
model.Add（激活（&#39;relu&#39;））
model.Add（maxpooling2d（pool_size =（2,2）））

ADD（辍学（0.5））

模型add（Flatten（））
model.Add（len（len（train_it.class_indices），激活=&#39;softmax&#39;））

model.compile（loss =&#39;Sparse_categorical_crossentropy&#39;，Optimizer =&#39;Adam&#39;，Metrics = [&#39;准确性&#39;]）
model.fit_generator（train_it，epochs = 20，验证_data = val_it）
 
这是我的代码，没有Imagedatagenerator（）：
使用OpenCV 设置数据

  datadir =&#39;d：\ ... \ train&#39;
类别= pickle.load（open（“ categories.p”，“ rb”））））
印刷（Len（类别））
img_size = 100
triench_data = []

def create_training_data（）：
    类别类别：
        路径= os.path.join（datadir，类别）
        class_num = categories.index（类别）
        对于os.listdir（路径）中的IMG：
            尝试：
                img_array = cv2.imread（os.path.join（路径，img），cv2.imread_grayscale）
                new_array = cv2.resize（img_array，（img_size，img_size））
                triench_data.append（[new_array，class_num]）
            除了：
                打印（类别）
                打印（IMG）

create_training_data（）

随机。

x = []
y = []
对于功能，请在triench_data中标记：
    X.Append（功能）
    Y.Append（标签）

x = np.array（x）.Reshape（-1，img_size，img_size，1）
X = X/255.0
 
模型设置：
  model = sequention（）
model.Add（conv2d（32，（3,3），input_shape = [*image_size，1]））））
model.Add（激活（&#39;relu&#39;））
model.Add（maxpooling2d（pool_size =（2,2）））

ADD（辍学（0.5））

Add（Conv2d（32，（3,3）））
model.Add（激活（&#39;relu&#39;））
model.Add（maxpooling2d（pool_size =（2,2）））

ADD（辍学（0.5））

Add（Conv2d（32，（3,3）））
model.Add（激活（&#39;relu&#39;））
model.Add（maxpooling2d（pool_size =（2,2）））

ADD（辍学（0.5））

模型add（Flatten（））
model.Add（密集（Len（类别），激活=&#39;SoftMax&#39;））

model.compile（loss =&#39;Sparse_categorical_crossentropy&#39;，Optimizer =&#39;Adam&#39;，Metrics = [&#39;准确性&#39;]）
model.fit（x，y，epochs = 20，batch_size = 32，验证_split = 0.1）
 ]]></description>
      <guid>https://stackoverflow.com/questions/58562089/why-is-imagedatagenerator-performing-poorly</guid>
      <pubDate>Fri, 25 Oct 2019 16:05:15 GMT</pubDate>
    </item>
    <item>
      <title>如何找到功能对逻辑回归模型的重要性？</title>
      <link>https://stackoverflow.com/questions/34052115/how-to-find-the-importance-of-the-features-for-a-logistic-regression-model</link>
      <description><![CDATA[我有一个由逻辑回归算法训练的二进制预测模型。我想知道哪些功能（预测指标）对于正面或负面类别的决策更为重要。我知道有来自Scikit-Learn软件包的 COEF _ 参数，但我不知道它是否足以满足重要性。另一件事是我如何根据否定和正类别的重要性来评估 coef _ 值。我还阅读了有关标准化回归系数的信息，但我不知道它是什么。
可以说，有肿瘤大小，肿瘤重量等特征，可以决定恶性肿瘤或不恶性等测试案例。我想知道哪些功能对于恶性和不是恶性预测更为重要。]]></description>
      <guid>https://stackoverflow.com/questions/34052115/how-to-find-the-importance-of-the-features-for-a-logistic-regression-model</guid>
      <pubDate>Wed, 02 Dec 2015 20:11:21 GMT</pubDate>
    </item>
    <item>
      <title>r中的精确，召回和f量</title>
      <link>https://stackoverflow.com/questions/12572357/precision-recall-and-f-measure-in-r</link>
      <description><![CDATA[ i在r中有一个带有两个colums的表，第一个柱具有预测值（值为0或1），第二个值是实际值（也是0或1）。我需要找到召回，精度和f-measures，但在R中找不到一个好功能（我也读过有关ROCR，但我所能做的就是创建一些图，但是我真的不需要地块，我需要数字）。）。。
在r中找到精度，召回和F量的功能吗？
有什么不同的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/12572357/precision-recall-and-f-measure-in-r</guid>
      <pubDate>Mon, 24 Sep 2012 20:23:57 GMT</pubDate>
    </item>
    </channel>
</rss>