<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 30 Jul 2024 12:28:56 GMT</lastBuildDate>
    <item>
      <title>确定任何 HTML 元素是否位于轮播、滑块或滑动内[关闭]</title>
      <link>https://stackoverflow.com/questions/78811668/determine-if-any-html-element-lies-inside-carousel-or-slider-or-swipe</link>
      <description><![CDATA[我想创建一个可以访问我网站的 HTML DOM 和 CSS 的实用程序。该实用程序应将 DOM 中的 HTML 元素作为输入，并根据元素是否属于网站上的轮播、滑块或滑动功能返回 true 或 false。该实用程序应该非常有信心地确定元素是否属于轮播。
使用机器学习、纯启发式的解决方案：
1. 识别轮播指示器：
• 查找与轮播相关的常见类名和属性（例如，class=&quot;carousel&quot;、class=&quot;slider&quot;、data-slide、data-ride=&quot;carousel&quot;）。
• 检查具有这些指示器的父元素。
2. 检查 CSS 属性：
• 查找轮播中通常使用的 CSS 属性，例如 overflow: hidden、display: flex、transition、transform 等。
3. JavaScript 事件：
• 分析元素或其父元素上的事件监听器，查找轮播中常见的事件，例如 click、touchstart、touchmove 等。

使用机器学习的解决方案，尚待探索。
我正在寻找任何可能的解决方案，以便置信度尽可能高，错误率尽可能低。纯启发式方法可能会导致错误报告。]]></description>
      <guid>https://stackoverflow.com/questions/78811668/determine-if-any-html-element-lies-inside-carousel-or-slider-or-swipe</guid>
      <pubDate>Tue, 30 Jul 2024 12:18:01 GMT</pubDate>
    </item>
    <item>
      <title>如何判断汽车配置的相似性？</title>
      <link>https://stackoverflow.com/questions/78811479/how-to-determine-the-similarity-of-a-cars-configuration</link>
      <description><![CDATA[大家好！
这里有一个车辆配置的数据集：

ID
品牌
型号
代数
发动机类型
发动机排量
气缸数
车身类型
变速箱类型
发动机代码
制造年份

任务：
需要确定一个配置与另一个配置的相似程度。
我假设将数据集中的每个条目表示为一个向量，并计算向量之间的余弦相似度。
但是对于如何以数值形式表示值存在误解，例如，车身类型：轿车、跨界车、轿跑车，等等。
感谢您的帮助]]></description>
      <guid>https://stackoverflow.com/questions/78811479/how-to-determine-the-similarity-of-a-cars-configuration</guid>
      <pubDate>Tue, 30 Jul 2024 11:38:50 GMT</pubDate>
    </item>
    <item>
      <title>在没有相关经验的情况下训练人工智能机器学习模型的最佳方法是什么？</title>
      <link>https://stackoverflow.com/questions/78811063/whats-the-best-way-to-train-an-a-i-ml-model-without-relevant-experience</link>
      <description><![CDATA[我遇到一个问题已经 4 年多了，有人一直在愚弄我，窃取我公司的 IP 并让我破产，这几乎让我陷入困境，我有一个 12 岁的儿子，这很艰难。我相信是通过使用恶意神经网络模型与我的核心互动。因此，出于逻辑，我在这段时间里浏览了我的选择，同时试图不失去太多的睡眠{几乎不可能}。
我多次换工作/换城市，甚至换了职业。
现在，无论你和谁说话都很难找到解决办法，因为很多人都为专业破坏买单，尽管在这一点上，尽管我很想帮助别人，但我还是继续我的生活。
所以在我看来，解决办法是训练一个模型来识别它，这样这种情况就不会再发生在我身上了。到目前为止，我选择了 PINN 网络，因为围绕核心/脑波的过度拟合。获得了 NUMPY / CONDA /MATLIB /TORCH / PYTHON 库以及大量虚拟环境和 GUROBI ML 模型。
我想知道的是，在受到攻击时，如何获得正常的权重测量值？
或者我可以使用一些标准算法吗？如果可以，有没有人有 GIT CLONE 参考资料可以给我看看？此外，一旦我设置了正常权重，映射交互中的不规则变化的最佳方法是什么？我想我理解如何从那里制作模型，因为我之前已经制作了几个模型。不过任何提示都会有很大帮助。请帮忙，谢谢
我已经使用 Jupiter 笔记本运行 PINNs 模型。它似乎运行良好，只是真的不确定如何识别对识别和偏转至关重要的交互变化。它只能稳定恶意交互，而不会调整和纠正规范化。想知道使用 cifar 还是 res net 更好？]]></description>
      <guid>https://stackoverflow.com/questions/78811063/whats-the-best-way-to-train-an-a-i-ml-model-without-relevant-experience</guid>
      <pubDate>Tue, 30 Jul 2024 10:00:15 GMT</pubDate>
    </item>
    <item>
      <title>DPR 模型微调无法在自定义数据集上收敛</title>
      <link>https://stackoverflow.com/questions/78810906/dpr-model-finetuning-does-not-converge-on-custom-dataset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78810906/dpr-model-finetuning-does-not-converge-on-custom-dataset</guid>
      <pubDate>Tue, 30 Jul 2024 09:32:16 GMT</pubDate>
    </item>
    <item>
      <title>图像检测任务的图像处理</title>
      <link>https://stackoverflow.com/questions/78810715/image-processing-for-image-detections-tasks</link>
      <description><![CDATA[我正在开展一个机器学习项目，用于检测 X 射线图片中的骨折。图片中包含一个经常旋转的矩形内的手，这会在实际旋转的图片外产生不必要的白色边框和黑色背景。我想删除不必要的黑色背景，但尚未成功。以下是数据库中的一些图片：
数据库中的旋转图片
数据库中的旋转图片
数据库中的未真正旋转的图片
我尝试使用 CV2 旋转图片，然后裁剪直到白线，但结果不一致，它确实对一些图片有效，但它残酷地裁剪了其他图片，旋转错误（对于一开始没有旋转的图像）并且没有给出我期望的结果。你能帮我写一个能给出更一致结果的代码吗？
这是我尝试的代码：
import cv2
import numpy as np
import matplotlib.pyplot as plt

def straighten_and_crop(image_path, output_path):
# 加载图像
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 对图像进行阈值处理以创建二值图像
_, binary = cv2.threshold(image, 1, 255, cv2.THRESH_BINARY)

# 在二值图像中查找轮廓
contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# 查找最大轮廓
contour = max(contours, key=cv2.contourArea)

# 获取最大轮廓的旋转边界框
rect = cv2.minAreaRect(contour)
box = cv2.boxPoints(rect)
box = np.int0(box)

# 获取边界框的宽度和高度
width = int(rect[1][0])
height = int(rect[1][1])

# 获取用于拉直图像的旋转矩阵
src_pts = box.astype(&quot;float32&quot;)
dst_pts = np.array([[0, height - 1],
[0, 0],
[width - 1, 0],
[width - 1, height - 1]], dtype=&quot;float32&quot;)

M = cv2.getPerspectiveTransform(src_pts, dst_pts)

# 使用旋转矩阵拉直图像
warped = cv2.warpPerspective(image, M, (width, height))

# 通过查找非黑色区域的边界框来删除黑色边框
coords = np.column_stack(np.where(warped &gt; 0))
x, y, w, h = cv2.boundingRect(coords)

# 调整边界框以仅删除黑色边框
cropped = warped[y:y + h, x:x + w]

# 保存处理后的图像
cv2.imwrite(output_path, cropped)

# 显示结果
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title(&#39;带有边界框的原始图像&#39;)
plt.imshow(image, cmap=&#39;gray&#39;)
plt.subplot(1, 2, 2)
plt.title(&#39;拉直和裁剪后的图像&#39;)
plt.imshow(cropped, cmap=&#39;gray&#39;)
plt.show()

....
 # 示例用法
image_path = 
&quot;C:\\Users\\iker1\\Downloads\\Xray_images\\6092_0295935784_01_WRI- 
L1_M003.png&quot;
output_path = &quot;C:\\Users\\iker1\\Downloads\\processed_image5.png&quot;
straighten_and_crop(image_path, output_path)
]]></description>
      <guid>https://stackoverflow.com/questions/78810715/image-processing-for-image-detections-tasks</guid>
      <pubDate>Tue, 30 Jul 2024 08:59:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么模型中的梯度参数都是None？</title>
      <link>https://stackoverflow.com/questions/78810687/why-are-all-the-gradient-parameters-in-the-model-none</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78810687/why-are-all-the-gradient-parameters-in-the-model-none</guid>
      <pubDate>Tue, 30 Jul 2024 08:53:21 GMT</pubDate>
    </item>
    <item>
      <title>我想将一个预先训练好的 ANN 模型连接到另一个 ANN 模型，并循环一起训练它们</title>
      <link>https://stackoverflow.com/questions/78810038/i-want-to-connect-a-pre-trained-ann-model-to-another-ann-model-and-train-them-to</link>
      <description><![CDATA[我正在研究一个逆问题。我想将预训练的前向 ANN 模型与逆模型结合起来，以估计最佳设计参数（逆模型的输出）。
我知道逆模型的输出应该用作预训练的前向模型的输入，但我坚持固定前向模型的初始参数并同时训练两个模型。
我曾尝试分别设计两个模型，然后分别训练它们，这导致逆模型对设计参数的估计稀疏。]]></description>
      <guid>https://stackoverflow.com/questions/78810038/i-want-to-connect-a-pre-trained-ann-model-to-another-ann-model-and-train-them-to</guid>
      <pubDate>Tue, 30 Jul 2024 06:13:44 GMT</pubDate>
    </item>
    <item>
      <title>CVAE 合成数据分布范围过窄</title>
      <link>https://stackoverflow.com/questions/78809995/cvae-synthetic-data-distributed-too-narrowly</link>
      <description><![CDATA[我有一个包含三个特征的数据集，两个浮点特征和一个具有 33 个类别的分类特征。（此处称为 Float_A、Float_B 和 Cat_A）。
我正在尝试训练 CVAE 以生成合成数据。使用以下 sklearn 转换器转换数据：
df=df[[&quot;float_A&quot;,&quot;float_B&quot;,&quot;categorical_A&quot;]]

transformers=[(&#39;float_A&#39;,Pipeline(steps=[(&#39;imputer&#39;,SimpleImputer(strategy=&#39;mean&#39;,add_indicator=True)),
(&#39;scaler&#39;,RobustScaler(quantile_range=(5,95)))]),
[&#39;float_A&#39;]),
(&#39;float_B&#39;,
Pipeline( steps=[(&#39;imputer&#39;,SimpleImputer(strategy=&#39;mean&#39;,add_indicator=True)),
(&#39;scaler&#39;,MinMaxScaler())]),
[&#39;float_B&#39;]),
(&#39;cats&#39;,OneHotEncoder(),categorical_columns)]`

transformer=ColumnTransformer(transformers,remainder=&#39;passthrough&#39;)

transformed_df=transformer.fit_transform(df)

我的第二个浮点数有一个 S 形激活函数，声明如下：
`Def sample(self,z):
reconstructed=self.decoder(z)
# 将 S 形激活应用于浮点数特征。
reconstructed[:,self.float_B_idx]=torch.sigmoid(reconstructed[:,self.float_B_idx])
returnreconstructed

Def forward(self,x):
z_mean,z_log_var=torch.chunk(self.encoder(x),2,dim=1)
z=self.reparameterize(z_mean,z_log_var)
reconstructed=self.decoder(z)
#将 sigmoid 激活应用于浮点特征。
reconstructed[:,self.float_B_idx]=torch.sigmoid(reconstructed[:,self.float_B_idx])
return reconstructed,z_mean,z_log_var`

一旦 CVAE 经过训练（训练和验证损失似乎按应有的方式减少），我尝试使用以下方法生成随机样本：
random_latent_vectors=torch.randn(num_samples,latent_dim)

使用 torch.no_grad()：
gen_df=model.sample(random_latent_vectors).detach().cpu().numpy()


但是 gen_df 中的所有样本都非常“未展开”。
FloatA、FloatB、 Cat[0:2]…

[[0.11782782 0.286538 0.646666 0.266387 0.09747571]
[0.0963359 0.29775462 0.58443785 0.29296008 0.1101962]
[0.1300626 0.31274286 0.59086925 0.30710378 0.10169853]
[0.1232817 0.32317564 0.56470346 0.29102385 0.11446829]
[0.13240162 0.28100765 0.6230704 0.29497638 0.08924796]]

然后，当我在 gen_df 上调用 scaler.inverse_transform 时，我几乎在每一行上都得到了相同的结果。
我尝试了各种方法，我的一个类别非常占主导地位（~90%），因此使用 imblearn 进行了一些类别不平衡欠采样，使其仅占 50% 的主导地位，但仍然获得 100% 的样本。
我尝试为我的 CVAE 添加更多层和复杂性，但再次被证明是徒劳的。]]></description>
      <guid>https://stackoverflow.com/questions/78809995/cvae-synthetic-data-distributed-too-narrowly</guid>
      <pubDate>Tue, 30 Jul 2024 05:57:45 GMT</pubDate>
    </item>
    <item>
      <title>使用 Azure Vision AI 的预训练模型分析货架图像，同时检测货架中的物体和间隙</title>
      <link>https://stackoverflow.com/questions/78802566/detect-objects-gaps-in-a-shelf-while-analyzing-shelf-images-using-pretrained-m</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78802566/detect-objects-gaps-in-a-shelf-while-analyzing-shelf-images-using-pretrained-m</guid>
      <pubDate>Sat, 27 Jul 2024 22:14:06 GMT</pubDate>
    </item>
    <item>
      <title>如何追踪之前图像的轮廓？</title>
      <link>https://stackoverflow.com/questions/78790401/how-do-i-track-a-contour-from-my-previous-image</link>
      <description><![CDATA[我有一个细菌细胞移动的视频，我将其转换为帧。现在我想找到每个细菌细胞的瞬时速度。为此，我感兴趣的是找出细菌细胞移动了多少，但我不知道如何告诉我的程序准确识别这种特定的细菌移动了。例如，假设我只有两张图像。对于每张图像，我都有每种细菌的 COM 坐标。现在我如何关联这些数据。我如何让我的程序准确识别这种特定细菌的 COM 变化量。我已将两张图片附上以供参考。


我想到的一个方法是给每个轮廓一个唯一的 id，并将该轮廓的特征与该唯一 id 关联起来。例如它的长轴和短轴长度。这样我就可以关联轮廓的初始和最终 COM。但是这个想法假设所有细菌细胞都是独一无二的，并且我的代码可以准确而精确地识别每个细菌细胞的轮廓，但事实并非如此。如果您感兴趣，我还附上了查找每个细菌细胞轮廓的代码。有人可以提出一些更好的想法吗？非常感谢。
import cv2 as cv
import numpy as np
from numpy.typing import NDArray
import math

def gaussian_filter_multiscale_retinex(image: NDArray, sigmas: list[float], weights: list[float]) -&gt;; NDArray:
img32 = image.astype(&#39;float32&#39;) / 255

img32_log = cv.log(img32 + 1)

msr = np.zeros(image.shape, np.float32)
对于 zip(sigmas, weights) 中的 sigma、weight:

blur = cv.GaussianBlur(img32, ksize=(0, 0), sigmaX=sigma)
blur_log = cv.log(blur + 1)
ssr = cv.subtract(img32_log, blur_log)
ssr = cv.multiply(ssr, weight)

msr = cv.add(msr, ssr)

msr = cv.divide(msr, sum(weights))

msr = cv.normalize(msr, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)
返回 msr
def calculate_ellipse_area(椭圆):

(cx, cy), (a, b), 角度 = 椭圆
半长轴 = a / 2
半短轴 = b / 2
面积 = math.pi * 半长轴 * 半短轴
返回面积，角度

def process_image(img, size_threshold):
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
rtnx = gaussian_filter_multiscale_retinex(gray, sigmas=[15, 55, 185], weights=[10, 5, 1])
阈值 = cv.adaptiveThreshold(rtnx, 255,自适应方法 = cv.ADAPTIVE_THRESH_GAUSSIAN_C，
阈值类型 = cv.THRESH_BINARY，blockSize = 7，C = -7)
nb_components，输出，统计，_ = cv.connectedComponentsWithStats（阈值，连通性 = 8）
大小 = 统计 [1：，-1]
new_img = np.zeros_like（阈值）
对于 i 在范围内（0，nb_components - 1）：
如果sizes [i]＆gt; = size_threshold：
new_img [输出 == i + 1] = 255
connected_components = cv.connectedComponentsWithStats（new_img）
（numLabels，标签，统计，质心）=connected_components

result_image = np.ones_like（img）* 255
对于 i 在范围内（1， numLabels):
componentMask = (labels == i).astype(&#39;uint8&#39;)
contours, _ = cv.findContours(componentMask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)
if len(contours) &gt; 0:
cnt = contours[0]
if len(cnt) &gt;= 5:
ellipse = cv.fitEllipse(cnt)
area, angle = calculate_ellipse_area(ellipse)
if area &lt; 250:
cv.ellipse(result_image, ellipse, (0, 0, 0), 1) # 在白色背景上绘制黑色轮廓
return result_image
img1path = &quot;/Users/yahya2/Desktop/1.png&quot;
img = cv.imread(img1path)
size_threshold = 16
result_image = process_image(img, size_threshold)

cv.imshow(&#39;轮廓&#39;, result_image)
cv.waitKey(0)
cv.destroyAllWindows()
]]></description>
      <guid>https://stackoverflow.com/questions/78790401/how-do-i-track-a-contour-from-my-previous-image</guid>
      <pubDate>Wed, 24 Jul 2024 20:14:25 GMT</pubDate>
    </item>
    <item>
      <title>运行预先训练的 ONNX 模型 - 图像识别</title>
      <link>https://stackoverflow.com/questions/75360420/running-a-pre-trained-onnx-model-image-recognition</link>
      <description><![CDATA[我正在尝试运行一个预先训练的 ONNX 模型（在第三方标签工具上训练）来进行图像识别。该模型通过工具中的一些预定义标签进行训练。现在的下一个目标是能够在工具之外运行此模型。为此，我正在获取示例图像并尝试通过模型运行该模型以获取已识别的标签作为输出。在此过程中，我遇到了有关如何调整输入的障碍。该模型需要输入如下：

如何在以下代码中调整输入？
import cv2
import numpy as np
import onnxruntime
import pytesseract
import PyPDF2

# 加载图像
image = cv2.imread(&quot;example.jpg&quot;)

# 检查图像是否已成功加载
if image is None:
raise ValueError(&quot;Failed to load the image&quot;)

# 获取图像的形状
height, width = image.shape[:2]

# 确保高度和宽度为正数
if height &lt;= 0 or width &lt;= 0:
raise ValueError(&quot;无效图像大小&quot;)

# 设置调整大小后图像的所需大小
dsize = (640, 640)

# 使用 cv2.resize 调整图像大小
resized_image = cv2.resize(image, dsize)

# 显示调整大小后的图像
cv2.imshow(&quot;调整大小后的图像&quot;, resized_image)
cv2.waitKey(0)
cv2.destroyAllWindows()

# 加载 ONNX 模型
session = onnxruntime.InferenceSession(&quot;ic/model.onnx&quot;)

# 检查模型是否已成功加载
if session is None:
raise ValueError(&quot;加载模型失败&quot;)

# 获取模型的输入名称和形状
inputs = session.get_inputs()
for i, input_info in enumerate(inputs):
print(f&quot;Input {i}: name = {input_info.name}, shape = {input_info.shape}&quot;)

# 运行 ONNX 模型
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name
prediction = session.run([output_name], {input_name: image})[0]

# 对预测进行后处理以获取标签
labels = postprocess(prediction)

# 使用 PyTesseract 从图像中提取文本
text = pytesseract.image_to_string(image)

# 打印标签和文本
print(&quot;Labels:&quot;, labels)
print(&quot;Text:&quot;, text)

因为代码抛出了以下错误：
ValueError: Model 需要 4 个输入。输入 Feed 包含 1]]></description>
      <guid>https://stackoverflow.com/questions/75360420/running-a-pre-trained-onnx-model-image-recognition</guid>
      <pubDate>Mon, 06 Feb 2023 10:58:44 GMT</pubDate>
    </item>
    <item>
      <title>一元特征和成本量（计算机视觉）</title>
      <link>https://stackoverflow.com/questions/66014793/unary-features-and-cost-volume-computer-vision</link>
      <description><![CDATA[我读了一篇关于视差的论文，看到了以下这句话：
“我们使用深度一元特征通过形成成本量来计算立体匹配成本。”
我在文献中查找了“一元特征”和“成本量”的定义，但一无所获。有人能解释一下这些术语在计算机视觉中的含义吗？]]></description>
      <guid>https://stackoverflow.com/questions/66014793/unary-features-and-cost-volume-computer-vision</guid>
      <pubDate>Tue, 02 Feb 2021 17:36:22 GMT</pubDate>
    </item>
    <item>
      <title>单个文件夹中不同序列图像的数据加载器</title>
      <link>https://stackoverflow.com/questions/65377998/dataloader-of-different-sequence-of-images-in-a-single-folder</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/65377998/dataloader-of-different-sequence-of-images-in-a-single-folder</guid>
      <pubDate>Sun, 20 Dec 2020 07:59:20 GMT</pubDate>
    </item>
    <item>
      <title>理解 sklearn 的 KNNImputer</title>
      <link>https://stackoverflow.com/questions/61752284/understanding-sklearns-knnimputer</link>
      <description><![CDATA[我浏览了它的文档，上面写着

每个样本的缺失值都是使用在训练集中找到的
n_neighbors 最近邻居的平均值来估算的。如果两个样本都没有缺失的特征接近，则这两个样本接近。

现在，使用一个玩具数据集，即
&gt;&gt;&gt;X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]
&gt;&gt;&gt;X

[[ 1., 2., nan],
[ 3., 4., 3.],
[nan, 6., 5.],
[ 8., 8., 7.]]

我们制作一个 KNNImputer，如下所示：
imputer = KNNImputer(n_neighbors=2)

问题是，它如何填充nan，而其中两列中有 nan？例如，如果要在第一行第三列中填充 nan，由于其中一行在第一列中也有 nan，它将如何选择最接近的特征？当我执行 imputer.fit_transform(X) 时，它给了我
array([[1. , 2. , 4. ],
[3. , 4. , 3. ],
[5.5, 6. , 5. ],
[8. , 8. , 7. ]])

这意味着要在第一行中填充 nan，最近的邻居是第二行和第三行。第一行和第三行之间的欧氏距离是如何计算的？]]></description>
      <guid>https://stackoverflow.com/questions/61752284/understanding-sklearns-knnimputer</guid>
      <pubDate>Tue, 12 May 2020 12:54:49 GMT</pubDate>
    </item>
    <item>
      <title>sklearn StandardScaler，不允许直接转换，我们需要 fit_transform</title>
      <link>https://stackoverflow.com/questions/51988030/sklearn-standardscaler-doesnt-allow-direct-transform-we-need-to-fit-transform</link>
      <description><![CDATA[fit_transform 和 transform 有什么区别？
为什么直接转换不起作用？
from sklearn.preprocessing import StandardScaler

X_scaler = StandardScaler()
X_train = X_scaler.fit_transform(X_train)
X_test = X_scaler.transform(X_test)

如果直接转换，则会出现以下错误

NotFittedError：此 StandardScaler 实例尚未安装。使用此方法之前，请使用适当的参数调用“fit”。
]]></description>
      <guid>https://stackoverflow.com/questions/51988030/sklearn-standardscaler-doesnt-allow-direct-transform-we-need-to-fit-transform</guid>
      <pubDate>Thu, 23 Aug 2018 14:20:49 GMT</pubDate>
    </item>
    </channel>
</rss>