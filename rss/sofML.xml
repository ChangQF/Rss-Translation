<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 23 Jul 2024 01:07:02 GMT</lastBuildDate>
    <item>
      <title>无法在 React Native 中运行 FaceNet 或机器学习</title>
      <link>https://stackoverflow.com/questions/78781135/cannot-run-facenet-or-machine-learning-in-react-native</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78781135/cannot-run-facenet-or-machine-learning-in-react-native</guid>
      <pubDate>Tue, 23 Jul 2024 00:19:25 GMT</pubDate>
    </item>
    <item>
      <title>CNN 的可变大小输入</title>
      <link>https://stackoverflow.com/questions/78781105/variable-size-input-for-cnn</link>
      <description><![CDATA[我想创建一个可以猜测图像序列中下一张图像的 AI，我的想法是将 1、2、3、...、n-1 张图像作为输入提供给 CNN，然后通过 LSTM 运行它，输出是第 2、3、4...、第 n 张图像。
因此，我的想法是，一个 epoch 将是
步骤#1：输入：第一张图像，输出是第二张图像
步骤#2：输入：第一张和第二张图像，输出是第三张图像
步骤#3：输入：第一张、第二张和第三张图像，输出是第四张图像
.
.
.
最后一步：输入：第 1、2、3、...、第 (n-1) 张图像，输出是第 n（最后）张图像
然后我会通过将所有 n 张图像作为输入来预测第 (n+1) 张图像的输出
我以前从未使用过 LSTM，但我读到它可以处理可变的输入大小，我的问题是它需要为卷积层提供可变的输入大小，这有可能以某种方式工作吗？
我知道如何制作 CNN，但我不知道是否有可能拥有具有可变输入大小的 CNN]]></description>
      <guid>https://stackoverflow.com/questions/78781105/variable-size-input-for-cnn</guid>
      <pubDate>Mon, 22 Jul 2024 23:58:33 GMT</pubDate>
    </item>
    <item>
      <title>Apache Flink 中的依赖管理和执行环境</title>
      <link>https://stackoverflow.com/questions/78780530/dependency-management-and-execution-environment-in-apache-flink</link>
      <description><![CDATA[我们正在评估 apache flink 是否可用于部署流式机器学习应用程序。
apache flink 中如何处理依赖管理，尤其是执行环境？
想象一下，具有不同依赖关系的 Python 任务应提交给 Flink 集群。
我们只看到 Flink 任务管理器可以使用 Python 虚拟环境处理依赖管理。当每个任务都有不同的依赖关系时，我们是否应该为每个任务部署一个新的任务管理器？
从容器设置开始，我们可以在单独的 Docker 映像中部署每个任务。
使用 apache flink 时通常如何处理这个问题？我们没有看到 Flink 擅长处理需要特定依赖关系的大量任务，但希望利用流式处理器。]]></description>
      <guid>https://stackoverflow.com/questions/78780530/dependency-management-and-execution-environment-in-apache-flink</guid>
      <pubDate>Mon, 22 Jul 2024 19:46:27 GMT</pubDate>
    </item>
    <item>
      <title>尝试在 TensorFlow 中实现用于单目深度估计的 MiDas 模型，距离值超出了图表范围</title>
      <link>https://stackoverflow.com/questions/78780498/trying-to-implement-the-midas-model-for-monocular-depth-estimation-in-tensorflow</link>
      <description><![CDATA[从 Kaggle 下载了 Tflite MiDas 模型。
查看了几篇关于 MiDas 和单目深度估计的文章（基本上是使用像素找到物体与相机的距离）
这是使用边界框进行物体检测的。
我试图获取边界框的中心并从那里获取距离。并尝试修复一些错误，但
我仍然不确定我是否已完成所有正确的步骤或正确实施了它。 - 请检查我是否忘记做某事。
示例输出：（我当时就在摄像头前面）
最小深度值：88.5720443725586，最大深度值：911.3470458984375 (126, 127) 处的深度值：248.00 米 最小深度值：76.10022735595703，最大深度值：920.8251342773438 (126, 127) 处的深度值：246.00 米 最小深度值：72.52698516845703，最大深度值：916.8196411132812 (126, 127) 处的深度值：246.00米 最小深度值：68.72769165039062，最大深度值：908.2048950195312 (126, 127) 处的深度值：247.00 米 最小深度值：67.79693603515625，最大深度值：898.6953125 (126, 127) 处的深度值：248.00 米 最小深度值：69.44243621826172，最大深度值：902.6591186523438 (126, 127) 处的深度值：250.00 米 最小深度值：78.3673324584961，最大深度值：914.4412231445312 深度(126, 127) 处的值：249.00 米
以下代码：
#加载模型
interpreter = tf.lite.Interpreter(model_path=&quot;/Users/Taparia/Desktop/ObjectDetection/1.tflite&quot;)
interpreter.allocate_tensors()

input_details = interpretationer.get_input_details()
output_details = interpretationer.get_output_details()

#加载视频
cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)
cap.set(10,70)
while True:
success,img = cap.read()
if not success: break
classIds, confs, bbox = net.detect(img,confThreshold=thres)
#print(classIds,bbox)

#准备用于距离测量的图像
img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
img_rgb_resized = cv2.resize(img_rgb,(256,256))
img_tensor = np.expand_dims(img_rgb_resized.astype(np.float32)/255.0, axis=0)

#估计深度
explainer.set_tensor(input_details[0][&#39;index&#39;], img_tensor)
explainer.invoke()
depth_map = explainer.get_tensor(output_details[0][&#39;index&#39;])[0]
print(f&quot;最小深度值：{np.min(depth_map)}, 最大深度值： {np.max(depth_map)}&quot;)

# 标准化深度图以进行可视化
depth_normalized = cv2.normalize(depth_map,None,alpha=0, beta=255,norm_type=cv2.NORM_MINMAX)
depth_normalized = np.uint8(depth_normalized)

if len(classIds) != 0:
for classId, confidence, box in zip(classIds.flatten(), confs.flatten(), bbox):
class_name = classNames[classId - 1].lower()
if class_name in used_objects:
cv2.rectangle(img, box, (0, 0, 255), 1) # 在对象周围绘制方框
cv2.putText(img, classNames[classId - 1].upper(), (box[0] + 10, box[1] + 30),
cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1)
x_start, y_start, x_end, y_end = box

# 计算深度图坐标中边界框的中心
center_x = int((x_start + x_end) / 2 * 256 / img.shape[1])
center_y = int((y_start + y_end) / 2 * 256 / img.shape[0])
#需要将图像大小调整为 (256,256)
# 确保坐标在depth_map的范围内
if 0 &lt;= center_x &lt;depth_normalized.shape[1] and 0 &lt;= center_y &lt; depth_normalized.shape[0]:
depth_value =depth_normalized[center_y, center_x].item()
print(f&quot;({center_x}, {center_y}) 处的深度值：{depth_value:.2f} 米&quot;)

# 检查物体是否靠近
ifdepth_value&lt; 1.0：# 示例阈值
print(&quot;对象非常接近&lt;)
cv2.putText(img, &quot;Close Object&quot;, (box[0], box[1] - 10),
cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
else:
print(&quot;深度图坐标超出范围&lt;)

我所做的所有尝试都发现，在以下行中
if 0 &lt;= center_x &lt;depth_normalized.shape[1] and 0 &lt;= center_y &lt; depth_normalized.shape[0]:
depth_value =depth_normalized[center_y, center_x].item()
print(f&quot;({center_x}, {center_y}) 处的深度值：{depth_value:.2f} 米&quot;)

最初我执行的是depth_value =depth_map[center_y, center_x].item()，但无论我输入的是depth_map还是depth_normalized，输出结果都没有区别。这很奇怪，因为我预期会有区别，但结果却无济于事。那里出了什么问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78780498/trying-to-implement-the-midas-model-for-monocular-depth-estimation-in-tensorflow</guid>
      <pubDate>Mon, 22 Jul 2024 19:33:26 GMT</pubDate>
    </item>
    <item>
      <title>带偏移的增强回归树模型中的错误</title>
      <link>https://stackoverflow.com/questions/78780133/error-in-boosted-regression-tree-model-with-offset</link>
      <description><![CDATA[我正在使用增强回归树模型 (BRT) 来预测海豚物种出现的概率。为此，我有存在/不存在数据和几个环境变量。发生数据被分成几段，我想使用段的长度作为模型中的偏移量，以说明数据收集的工作量。
我使用 dismo 包中的 gbm.step() 函数来拟合模型。
brt&lt;-gbm.step(data=df, gbm.x=c(6,7,12,42,43,15,45,53,93,41,81,87,97), gbm.y = 4, offset = offset, family=&quot;bernoulli&quot;, tree.complexity=3, learning.rate = 0.01, bag.fraction = 0.6)

我将偏移量定义为 log(length of段）
offset&lt;-log(df$eff_length)

&gt; head(offset)
[1] 9.017928 9.171184 9.239406 9.367430 9.264165 9.233178
&gt; summary(offset)
最小值 第 1 区 中位数 平均值 第 3 区 最大值。
8.987 9.065 9.146 9.181 9.265 9.615 

当我运行模型时，它会反复出现此警告：
gbm::predict.gbm(model.list[[i]], x.data[pred.mask, , drop = FALSE], 中出现警告：
predict.gbm 不会将偏移量添加到预测值中。

随后出现此错误：
if (cv.loss.values[j] &gt; cv.loss.values[j - 1]) { 中的错误：
需要 TRUE/FALSE 的值缺失

我尝试包含 fold.vector=offset，因为看起来错误与交叉验证过程，但它给了我相同的警告和错误。
我在这里遗漏了什么？它与伯努利分布类型有关吗？]]></description>
      <guid>https://stackoverflow.com/questions/78780133/error-in-boosted-regression-tree-model-with-offset</guid>
      <pubDate>Mon, 22 Jul 2024 17:51:21 GMT</pubDate>
    </item>
    <item>
      <title>keras BackupAndRestore 恢复模型但准确率较低且损失较高</title>
      <link>https://stackoverflow.com/questions/78779866/keras-backupandrestore-resuming-a-model-but-shows-lower-accuracy-and-higher-loss</link>
      <description><![CDATA[我正在使用 google colab gpu，我需要尽可能多地保存模型，然后在新的 google colab 中重新启动训练模型，下面是我的代码，但准确率较低，损失较高，例如在 epoch 89 中我有
acc: 0.9990301728 loss:0.002603143221 
acc_val: 0.9557291865 loss_val:0.2962754667 

在 epoch 90 中，第一次在新的 google colab 中运行我有：
acc: 0.9803879261 loss:01103143221 
acc_val: 0.939127624 loss_val:0.1836656481

# 定义 5 倍交叉验证
kf = KFold(n_splits=5, shuffle=True, random_state=42)

conf_matrix_all_CNN_10s = []
fpr_matrix_all_CNN_10s = []
tpr_matrix_all_CNN_10s = []

for i, (train_val_index, test_index) in enumerate(kf.split(all_file_paths)):
print(f&quot;Fold {i+1}:&quot;)

# 将数据拆分为该折叠的训练和验证集
train_val_files = [all_file_paths[j] for j in train_val_index]
test_files = [all_file_paths[j] for j in test_index]

train_files, val_files = train_test_split(train_val_files, test_size=0.25, random_state=42)

# 创建训练和测试数据生成器
train_generator = data_generator_train(train_files, batch_size)
val_generator = data_generator_val(val_files, batch_size)
test_generator = data_generator_test(test_files, batch_size)

# 定义模型架构
model = Sequential()
input_shape = np.load(epilepsy_file_paths[0]).T.shape

model.add(Conv1D(64, 3, strides=1, input_shape=input_shape, padding=&#39;same&#39;,activation=&#39;relu&#39;))
model.add(BatchNormalization())
model.add(MaxPool1D(pool_size=(2)))
model.add(Dropout(0.5))

model.add(Conv1D(48, 3, strides=1, padding=&#39;same&#39;, activity=&#39;relu&#39;))
model.add(BatchNormalization())
model.add(MaxPool1D(pool_size=(2)))
model.add(Dropout(0.5))

model.add(Conv1D(32, 3, strides=1, padding=&#39;same&#39;, activity=&#39;relu&#39;))
model.add(BatchNormalization())
model.add(MaxPool1D(pool_size=(2)))
model.add(Dropout(0.5))

model.add(Flatten())

model.add(Dense(256, activity=&#39;relu&#39;))
model.add(Dropout(0.2))
model.add(Dense(128,激活=&#39;relu&#39;))
model.add(Dropout(0.2))
model.add(Dense(1, 激活=&#39;sigmoid&#39;))

modelOptimizer = Adam(learning_rate=0.001)
model.compile(optimizer=modelOptimizer, loss=BinaryCrossentropy(), metrics=[&#39;accuracy&#39;])

reduceLR_callback = ReduceLROnPlateau(
monitor=&quot;val_loss&quot;,
factor=0.5,
waiting=7,
mode=&quot;min&quot;,
min_lr=1e-5,
)

checkpoint_dir = &#39;/content/drive/MyDrive/Project1/Weights/10s&#39;

# 定义 CSV 记录器回调
csv_logger = CSVLogger(f&#39;{checkpoint_dir}/training_log_fold_{i + 1}_CNN_10s.csv&#39;, append=True)

backup_restore_callback = BackupAndRestore(backup_dir=f&#39;{checkpoint_dir}/backup_fold_{i + 1}&#39;)

# 使用训练数据训练模型
history = model.fit(
train_generator,
epochs=epochs,
steps_per_epoch=len(train_files) // batch_size,
validation_data=val_generator,
validation_steps=len(val_files) // batch_size,
callbacks=[reduceLR_callback, csv_logger, backup_restore_callback],
shuffle=False
)

我预计重新加载模型后准确率和损失率会接近]]></description>
      <guid>https://stackoverflow.com/questions/78779866/keras-backupandrestore-resuming-a-model-but-shows-lower-accuracy-and-higher-loss</guid>
      <pubDate>Mon, 22 Jul 2024 16:38:44 GMT</pubDate>
    </item>
    <item>
      <title>如何针对 NYU 数据集训练 Yolo 3D</title>
      <link>https://stackoverflow.com/questions/78779752/how-to-train-yolo-3d-for-nyu-dataset</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78779752/how-to-train-yolo-3d-for-nyu-dataset</guid>
      <pubDate>Mon, 22 Jul 2024 16:09:49 GMT</pubDate>
    </item>
    <item>
      <title>从 huggingface 下载数据集（仅包含图像 id 的 json 文件）</title>
      <link>https://stackoverflow.com/questions/78779713/downloading-dataset-from-huggingface-with-only-json-file-with-image-id</link>
      <description><![CDATA[我正在尝试重现LLaVA-RLHF论文。
我正在使用他们的数据集访问他们的官方huggingface repo，
https://huggingface.co/datasets/zhiqings/LLaVA-Human-Preference-10K
但我有点困惑。
提供的文件只是一个json文件，没有实际的图像，只有图像id和文件名。
（例如000000013249.jpg）
我想下载数据集的图像，但我不知道如何仅使用json文件进行下载。我很确定一定有一种方法可以下载数据集的图像，但我不知道如何下载。
有没有办法将图像与 json 文件一起下载？
我觉得我遗漏了一个非常基本的要点，但我现在有点迷茫。
我做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78779713/downloading-dataset-from-huggingface-with-only-json-file-with-image-id</guid>
      <pubDate>Mon, 22 Jul 2024 16:03:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在没有 keras 的情况下根据经过训练的 keras 模型进行预测</title>
      <link>https://stackoverflow.com/questions/78779669/how-to-make-predictions-from-trained-keras-model-without-keras</link>
      <description><![CDATA[我有一个使用 keras 创建和训练的 1D CNN，并且我将权重保存在 h5 文件中，将架构保存在 JSON 文件中。现在，我希望能够读取架构和权重，并使用它们进行新的预测，独立于 Keras 或 TensorFlow。我只想使用 numpy、scipy 等基本软件包，但如果有任何其他软件包可以以最少的依赖性完成此操作，那么我想使用它。我找到了 Konverter，但它不适用于 CNN。]]></description>
      <guid>https://stackoverflow.com/questions/78779669/how-to-make-predictions-from-trained-keras-model-without-keras</guid>
      <pubDate>Mon, 22 Jul 2024 15:53:20 GMT</pubDate>
    </item>
    <item>
      <title>哪些模型是图像形状检测最有效的模型？[关闭]</title>
      <link>https://stackoverflow.com/questions/78779365/which-are-the-most-efficient-models-for-shape-detection-in-an-image</link>
      <description><![CDATA[我想创建一个 Python 算法来检测两张照片之间出现和消失的物体。这些照片不是从同一个地方拍摄的。我的想法是使用一个预训练模型，它可以检测两张照片中所有“足够大”的物体，并检测一个或多个物体何时出现或消失。
我尝试使用 YOLO，但它似乎只能检测到他可以分类的物体。由于我的数据不是关于常见物体的，所以效果不太好。
是否有一些预训练模型可以检测几何形状/物体而无需对它们进行分类？]]></description>
      <guid>https://stackoverflow.com/questions/78779365/which-are-the-most-efficient-models-for-shape-detection-in-an-image</guid>
      <pubDate>Mon, 22 Jul 2024 14:45:32 GMT</pubDate>
    </item>
    <item>
      <title>PyBullet 的 Walker2D 的关节角度</title>
      <link>https://stackoverflow.com/questions/78778576/joint-angles-of-pybullets-walker2d</link>
      <description><![CDATA[我正在使用 pybullet_envs 中的 Walker2D 环境，并尝试获取 6 个关节角度，以便将它们用于由 Arduino 控制的实际双足机器人。
我尝试过但未能找到文档，例如 gym 中的文档，其中指定了 6 个关节角度的索引，并且它们的值由 rad 测量。我发现有人指出，在 pybullet_envs GitHub 文档中，有评论说关节位置是观察空间的偶数元素。我认为这从 Walker2D 环境的每个列的数组索引 8 开始是正确的。我的问题是我不知道这些值的单位，我不确定它们是否正确。有没有更简单的方法可以通过物理客户端 ID 和机器人 ID 通过环境访问实际值（rad 或 deg）？]]></description>
      <guid>https://stackoverflow.com/questions/78778576/joint-angles-of-pybullets-walker2d</guid>
      <pubDate>Mon, 22 Jul 2024 12:02:18 GMT</pubDate>
    </item>
    <item>
      <title>将safetensors模型格式（LLaVA模型）转换为gguf格式</title>
      <link>https://stackoverflow.com/questions/78763327/convert-safetensors-model-formatllava-model-into-gguf-format</link>
      <description><![CDATA[我想在 ollama 中进行 LLaVA 推理，因此我需要将其转换为 gguf 文件格式。
我的模型具有文件格式 safetensors。（使用 lora 训练）
似乎 ollama 仅支持 llama，但不支持 llava，如下所示，
https://github.com/ollama/ollama/blob/main/docs/import.md
我遵循了 llama.cpp 的说明，并在此处使用了代码 convert_lora_to_gguf.py，
https://github.com/ggerganov/llama.cpp/blob/master/convert_lora_to_gguf.py
但是我收到如下错误：
ERROR:lora-to-gguf:不支持 Model LlavaLlamaForCausalLM

如果我在模型文件的 config.json 中写入 llama 模型并运行以下代码，则会收到另一个错误。
model_instance.gguf_writer.add_string(gguf.Keys.General.TYPE, gguf.GGUFType.ADAPTER)
model_instance.gguf_writer.add_string(gguf.Keys.Adapter.TYPE, &quot;lora&quot;)
model_instance.gguf_writer.add_float32(gguf.Keys.Adapter.LORA_ALPHA, float(alpha))
model_instance.gguf_writer.add_quantization_version(gguf.GGML_QUANT_VERSION)
logger.info(&quot;Exporting model...&quot;)
model_instance.write()
logger.info(f&quot;模型已成功导出至 {model_instance.fname_out}&quot;)

Traceback (most recent call last):
File &quot;C:\Users\jjjy2\OneDrive\Desktop\VLM_FastAPI\ollama\convert_lora_to_gguf.py&quot;, line 373, in &lt;module&gt;
model_instance.gguf_writer.add_string(gguf.Keys.General.FILE_TYPE, gguf.GGUFType.ADAPTER)
AttributeError: module &#39;gguf&#39; has no attribute &#39;GGUFType&#39;

似乎所有代码和 gguf 包都不支持 llava，只支持 llama。我必须将我自己训练的模型转换为 gguf。我无法使用 hugging face 的 gguf llava 模型进行推理。
有没有办法转换它？]]></description>
      <guid>https://stackoverflow.com/questions/78763327/convert-safetensors-model-formatllava-model-into-gguf-format</guid>
      <pubDate>Thu, 18 Jul 2024 08:47:53 GMT</pubDate>
    </item>
    <item>
      <title>有人能帮我解决这个基本的机器学习问题吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/78753768/can-anyone-help-me-with-this-basic-ml-question</link>
      <description><![CDATA[ MCQ 问题 
我想找到这个关于 ML 的基本问题的答案。你能帮我吗？我希望了解机器学习中的关键概念和技术，例如算法、数据预处理、模型训练和评估。如果您有任何提示或资源可以帮助我很好地掌握这些主题并将它们应用于实际场景，那就太好了。]]></description>
      <guid>https://stackoverflow.com/questions/78753768/can-anyone-help-me-with-this-basic-ml-question</guid>
      <pubDate>Tue, 16 Jul 2024 09:36:19 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：X 有 1 个特征，但 LinearRegression 需要 2 个特征作为输入</title>
      <link>https://stackoverflow.com/questions/73132252/valueerror-x-has-1-features-but-linearregression-is-expecting-2-features-as-in</link>
      <description><![CDATA[我正在使用 pywebio 为我的机器学习程序创建一个小型脚本运行用户界面。当不使用小型 UI 时，运行线性回归 predict() 函数时没有任何错误。
UI 正在从用户那里检索两个数字，一个 &#39;age&#39; 和一个 &#39;salary&#39;。这两个数字被输入到一个 numpy 数组中，并且 numpy 数组已从 1D 数组重塑为 2D 数组，因为我在 numpy 数组形状上收到错误。
现在，我收到一条错误消息，提示 predict() 方法仅接收 1 个特征而不是 2 个，而 sklearn 文档指出线性回归 predict() 方法始终获取“self”和另一个特征。我该如何修复此错误？
这是我的 UI 代码：
age = int(input(&quot;输入您的年龄：&quot;, type=NUMBER))
salary = int(input(&quot;输入您的薪水：&quot;, type=NUMBER))

entry = np.array([age, salary])
reshaped_entry = entry.reshape(-1, 1)

estimate = regr.predict(reshaped_entry) 

以下是错误消息：
ValueError Traceback (most recent call last)
Input In [21], in &lt;cell line: 22&gt;()

Input In [21], in retired_ui()

文件~\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:362，位于 LinearModel.predict(self, X) 中
348 def predict(self, X):
349 &quot;&quot;&quot;
350 使用线性模型进行预测。
351 
(...)
360 返回预测值。
361 &quot;&quot;&quot;
--&gt; 362 return self._decision_function(X)

文件 ~\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:345，位于 LinearModel._decision_function(self, X) 中
342 def _decision_function(self, X):
343 check_is_fitted(self)
--&gt; 345 X = self._validate_data(X, accept_sparse=[&quot;csr&quot;, &quot;csc&quot;, &quot;coo&quot;], reset=False)
346 return safe_sparse_dot(X, self.coef_.T, density_output=True) + self.intercept_

文件 ~\anaconda3\lib\site-packages\sklearn\base.py:585, in BaseEstimator._validate_data(self, X, y, reset, valid_separately, **check_params)
582 out = X, y
584 if not no_val_X and check_params.get(&quot;ensure_2d&quot;, True):
--&gt; 585 self._check_n_features(X, reset=reset)
587 return out

File ~\anaconda3\lib\site-packages\sklearn\base.py:400, in BaseEstimator._check_n_features(self, X, reset)
397 return
399 if n_features != self.n_features_in_:
--&gt; 400 raise ValueError(
401 f&quot;X 有 {n_features} 个特征，但 {self.__class__.__name__} &quot;
402 f&quot;需要 {self.n_features_in_} 个特征作为输入。&quot;
403 )

ValueError: X 有 1 个特征，但 LinearRegression 需要 2 个特征作为输入。
​​]]></description>
      <guid>https://stackoverflow.com/questions/73132252/valueerror-x-has-1-features-but-linearregression-is-expecting-2-features-as-in</guid>
      <pubDate>Wed, 27 Jul 2022 04:29:07 GMT</pubDate>
    </item>
    <item>
      <title>面临 ValueError：目标是多类但平均值='二进制'</title>
      <link>https://stackoverflow.com/questions/52269187/facing-valueerror-target-is-multiclass-but-average-binary</link>
      <description><![CDATA[我正在尝试对我的数据集使用朴素贝叶斯算法。我能够找出准确率，但试图找出相同的精确度和召回率。但是，它抛出了以下错误：
ValueError：目标是多类，但平均值=&#39;binary&#39;。请选择其他平均设置。

有人可以建议我如何继续吗？我尝试在精确度和召回率分数中使用average =&#39;micro&#39;。它没有任何错误，但它给出了相同的准确度、精确度和召回率分数。
我的数据集：
train_data.csv：
review,label
颜色和清晰度极佳，正面
可惜图片不如我的 40 英寸三星清晰明亮，负面

test_data.csv：
review,label
图片清晰漂亮，正面
图片不清晰，负面

我的代码：
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confused_matrix

X_train, y_train = pd.read_csv(&#39;train_data.csv&#39;)
X_test, y_test = pd.read_csv(&#39;test_data.csv&#39;)

vec = CountVectorizer() 
X_train_transformed = vec.fit_transform(X_train) 
X_test_transformed = vec.transform(X_test)

clf = MultinomialNB()
clf.fit(X_train_transformed, y_train)

score = clf.score(X_test_transformed, y_test)

y_pred = clf.predict(X_test_transformed)
cm = confused_matrix(y_test, y_pred)

precision = precision_score(y_test, y_pred, pos_label=&#39;positive&#39;)
recall = recall_score(y_test, y_pred, pos_label=&#39;positive&#39;)
]]></description>
      <guid>https://stackoverflow.com/questions/52269187/facing-valueerror-target-is-multiclass-but-average-binary</guid>
      <pubDate>Tue, 11 Sep 2018 05:28:04 GMT</pubDate>
    </item>
    </channel>
</rss>