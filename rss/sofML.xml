<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 05 Jan 2024 15:14:26 GMT</lastBuildDate>
    <item>
      <title>寻求有关对多标签 NER 数据集进行分层以实现平衡训练/测试拆分的建议</title>
      <link>https://stackoverflow.com/questions/77765015/seeking-advice-on-stratifying-a-multi-label-ner-dataset-for-balanced-train-test</link>
      <description><![CDATA[我正在开展一个命名实体识别 (NER) 项目，我们拥有广泛的数据集，专为医疗保健量身定制。我们的数据集包含大约 3,000 个不同的疾病实体（实体类型），每个实体都用开头 (B-Tag) 和内部 (I-Tag) 标签的唯一编号进行注释。
以下是我们在 Huggingface 框架模型格式中使用的数据格式示例：
&lt;前&gt;&lt;代码&gt;{
    “ner_tags”：[1,2,2,4,5,0,7,8,8],
    “标记”：[“首席”、“主诉”、“：”、“糖尿病”、“糖尿病”、“和”、“慢性”、“肾脏”、“疾病” ;]
}

在此示例中，句子中的每个标记都与表示其实体类型的 NER 标记相关联。
我面临的挑战是如何正确地将这个数据集分为训练集、验证集和测试集。鉴于不同实体的数量众多，至关重要的是每组都代表整个疾病范围。这对于确保我们正在开发的 NER 模型经过良好训练并能够有效地泛化未知数据至关重要。
我正在寻求以下方面的建议：

分割大型多实体 NER 数据集的有效策略。
确保所有实体在每个子集中得到充分代表（训练、验证、测试）。
在 NER 中处理大量实体类型时的任何具体注意事项或技术。

我对能够处理数据集复杂性、确保所有实体的平衡和全面表示的方法特别感兴趣。]]></description>
      <guid>https://stackoverflow.com/questions/77765015/seeking-advice-on-stratifying-a-multi-label-ner-dataset-for-balanced-train-test</guid>
      <pubDate>Fri, 05 Jan 2024 13:47:15 GMT</pubDate>
    </item>
    <item>
      <title>我应该在代码中添加什么或者代码有什么问题？</title>
      <link>https://stackoverflow.com/questions/77764698/what-should-i-add-to-the-code-or-what-is-wrong-with-the-code</link>
      <description><![CDATA[导入 pandas 作为 pd
将 numpy 导入为 np
从 sklearn.impute 导入 SimpleImputer
从 sklearn.preprocessing 导入 MinMaxScaler
从 sklearn.neighbors 导入 KNeighborsClassifier


egitim_data = pd.read_excel(r&#39;C:\Users\memo3\OneDrive\Masaüstü\Min-MaxNormalizasyonluEgitimDatalari.xlsx&#39;)

X_train = egitim_data.drop(&#39;标签&#39;, axis=1)
y_train = egitim_data[&#39;标签&#39;]


test_data = pd.read_excel(r&#39;C:\Users\memo3\OneDrive\Masaüstü\Min-MaxNormalizasyonluTestDatalari.xlsx&#39;)

X_test = test_data.drop(&#39;标签&#39;, axis=1)
y_test = test_data[&#39;标签&#39;]


缩放器 = MinMaxScaler()
X_train_normalized = 缩放器.fit_transform(X_train)
X_test_normalized = 缩放器.transform(X_test)


imputer = SimpleImputer（missing_values = np.nan，策略=&#39;平均值&#39;）
X_train_impulated = imputer.fit_transform(X_train_normalized)
X_test_impulated = imputer.transform(X_test_normalized)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_imputed, y_train)

准确度 = knn.score(X_test_impulated, y_test)
print(&quot;模型精度：&quot;, 精度)


C:\Users\memo3\PycharmProjects\KNNModel\.venv\Scripts\python.exe C:\Users\memo3\PycharmProjects\KNNModel\main.py
模型精度：0.4527777777777778

进程已完成，退出代码为 0

我正在尝试在 PyCharm 中使用 Python 语言编写机器学习代码。但我认为该代码并未采用我拥有的 Excel 数据中的所有类特征（Excel 中有 15 列表示数据的特征），当我运行代码时，我得到的准确度分数较低，如下所示。我想要分类的目标列是“标签”列，但是不需要使用数据的所有属性来做到这一点吗？我应该在代码中添加什么或者代码中哪里有错误？]]></description>
      <guid>https://stackoverflow.com/questions/77764698/what-should-i-add-to-the-code-or-what-is-wrong-with-the-code</guid>
      <pubDate>Fri, 05 Jan 2024 12:46:17 GMT</pubDate>
    </item>
    <item>
      <title>IOPub 数据速率超过 Jupyter Notebook [重复]</title>
      <link>https://stackoverflow.com/questions/77763798/iopub-data-rate-exceeded-jupyter-notebook</link>
      <description><![CDATA[`IOPub` 数据速率超出。

‘Jupyter’服务器将暂时停止发送输出
给客户端以避免崩溃。
要更改此限制，请设置配置变量
`--ServerApp.iopub_data_rate_limit`。

当前值：
ServerApp.iopub_data_rate_limit=1000000.0（字节/秒）
ServerApp.rate_limit_window=3.0（秒）

我尝试更新整个 Jupyter 笔记本，但仍然没有解决问题，我还尝试根据我的数据集更新值，但仍然显示同样的错误。]]></description>
      <guid>https://stackoverflow.com/questions/77763798/iopub-data-rate-exceeded-jupyter-notebook</guid>
      <pubDate>Fri, 05 Jan 2024 10:04:06 GMT</pubDate>
    </item>
    <item>
      <title>实时跟踪脚本中 BYTETracker 初始化的问题</title>
      <link>https://stackoverflow.com/questions/77763541/issue-with-bytetracker-initialization-in-live-tracking-script</link>
      <description><![CDATA[尝试使用 ByteTrack 库初始化实时跟踪脚本中的 BYTETracker 时，我遇到了 TypeError。该错误具体发生在 BYTETracker 类的 __init__ 方法中。
这是我的代码的相关部分：
跟踪器 = [BYTETracker(ByteTrackArgument), BYTETracker(ByteTrackArgument), BYTETracker(ByteTrackArgument)]

我遇到的错误消息是：
TypeError：+ 不支持的操作数类型：“type”和“float”

我尝试通过创建一个 ByteTrackArgument 实例来解决这个问题，如下所示：
跟踪器 = [BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())]

但是，问题仍然存在。值得注意的是，我在脚本中使用 OpenCV 中的 FaceDetectorYN 进行人脸检测。
对于可能导致此错误的原因（尤其是与使用 FaceDetectorYN 结合使用）以及如何解决该错误有任何见解吗？
其他上下文：

我正在使用 ByteTrack 库进行实时跟踪。 链接
错误发生在 BYTETracker 类的 __init__ 方法中。
我将 ByteTrackArgument 的实例传递给 BYTETracker 构造函数。
使用 OpenCV 中的 FaceDetectorYN 执行人脸检测。 链接

这是我的完整代码：
&lt;前&gt;&lt;代码&gt;导入cv2
从 bytetracker 导入 BYTETracker
将 numpy 导入为 np

print(&quot;OpenCV 版本&quot;, cv2.__version__)

类 ByteTrackArgument：
    轨迹阈值 = 0.5
    轨道缓冲区 = 50
    匹配阈值 = 0.8
    纵横比阈值 = 10.0
    最小框面积 = 1.0
    mot20 = 假

MIN_THRESHOLD = 0.5 # 根据需要调整此阈值

# 初始化 ByteTrackArgument
byte_track_argument = ByteTrackArgument()

# 初始化BYTETracker
跟踪器 = [BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())、BYTETracker(ByteTrackArgument())]
def start_webcam_tracking():
    cap = cv2.VideoCapture(0) # 使用 0 作为默认网络摄像头，或提供网络摄像头 URL

    如果不是 cap.isOpened():
        print(“错误：无法打开相机。”)
        返回

    而真实：
        ret, 框架 = cap.read()
        如果不转：
            print(“错误：无法从相机读取帧。”)
            休息

        # 人脸检测代码
        检测器 = cv2.FaceDetectorYN.create(r&quot;C:\Users\gratu\live tracker\face_detection_yunet_2023mar.onnx&quot;, &quot;&quot;, (2200, 1200), Score_threshold=MIN_THRESHOLD)
        img_W = int(frame.shape[1])
        img_H = int(frame.shape[0])
        detector.setInputSize((img_W, img_H))

        检测= detector.detect(frame)[1]

        如果检测不是无：
            用于检测中的检测：
                x, y, 宽度, 高度 = 地图(int, 检测[:4])
                cv2.矩形(框架, (x, y), (x + 宽度, y + 高度), (0, 255, 0), 2)

                # 使用面部边界框更新跟踪器
                tracker.update(np.array([[x, y, x + 宽度, y + 高度]]), [frame.shape[0],frame.shape[1]])

        # 从 BYTETracker 获取跟踪结果
        online_targets = tracker.get_online_targets()

        如果 online_targets 不是 None：
            对于 online_targets 中的目标：
                x, y, x2, y2 = target # 根据BYTETracker的输出格式修改这部分
                cv2.矩形(框架, (x, y), (x2, y2), (255, 0, 0), 2)

        cv2.imshow(&#39;具有人脸检测和跟踪功能的网络摄像头&#39;, frame)

        如果 cv2.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;):
            休息

    cap.release()
    cv2.destroyAllWindows()

如果 __name__ == “__main__”：
    start_webcam_tracking()

这是我尝试复制的教程，他们使用 yolox 进行人物检测，我尝试对人脸检测做同样的事情
链接]]></description>
      <guid>https://stackoverflow.com/questions/77763541/issue-with-bytetracker-initialization-in-live-tracking-script</guid>
      <pubDate>Fri, 05 Jan 2024 09:10:23 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 正则化 Alpha - 权重还是叶子？</title>
      <link>https://stackoverflow.com/questions/77763321/lightgbm-regularization-alpha-weights-or-leaves</link>
      <description><![CDATA[我已经阅读了有关 XGBoost 和 LightGBM 的论文以及它们的大部分文档，但无法找到明确的声明表明除了 GOSS &amp; EFB为了更快的学习，基本算法与XGBoost相同。具体来说，XGBoost 目标函数中唯一的 L1 式正则化是 gamma*T，其中 gamma 是超参数，T 是叶子数量。 LightGBM 有 reg_alpha 参数，根据他们的文档，该参数应用 L1 正则化，该参数是否会惩罚叶子的数量，或者，在更传统的意义上，惩罚每个叶子的贡献的绝对值？
附注参考回归案例，我从未使用过该模型进行分类，因此不知道哪些部分仍然有效。]]></description>
      <guid>https://stackoverflow.com/questions/77763321/lightgbm-regularization-alpha-weights-or-leaves</guid>
      <pubDate>Fri, 05 Jan 2024 08:25:18 GMT</pubDate>
    </item>
    <item>
      <title>我的 tfidf 向量自动编码器对于不同的文本输入产生相同的输出</title>
      <link>https://stackoverflow.com/questions/77762883/my-autoencoder-for-tfidf-vectors-is-yielding-the-same-output-for-different-text</link>
      <description><![CDATA[我一直在尝试实现一个自动编码器来完成降维任务，输入到最近邻模型中，我意识到所有邻居最终彼此之间的距离为零，我意识到问题源于自动编码器，这对于不同的文本产生了类似的输出。
我已经尝试更改层数、正则化器以及输出层的激活（我使用线性，因为这应该是文本数据的最佳选择）。
我的 desc 和 req 数据帧的形状分别为 (31080, 471494) (31080, 169214)，大部分是稀疏数据，这是我在初始原始文本数据上使用 tfidf 获得的。
类 SparseDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, X, 批量大小):
        自我.X = X
        self.batch_size = 批量大小
        self.n_samples = X.shape[0]

    def __len__(自身):
        返回 int(np.ceil(self.n_samples / self.batch_size))

    def __getitem__(self, idx):
        开始 = idx * self.batch_size
        结束=分钟（开始+ self.batch_size，self.n_samples）
        batch_X = self.X[开始:结束].toarray()
        return batch_X, batch_X # 自动编码器获取与输入和目标相同的数据


组合_vecs_sparse = hstack([df_desc.sparse.to_coo(), df_req.sparse.to_coo()])


# 定义提前停止回调
Early_stopping = EarlyStopping（监视器=&#39;val_loss&#39;，耐心= 3，restore_best_weights = True）

# 分割数据
X_train, X_val = train_test_split(combined_vecs_sparse, test_size=0.2, random_state=42)
X_train = X_train.tocsr()
X_val = X_val.tocsr()
before_memory = psutil.Process().memory_info().rss
input_dim = X_train.shape[1] # 特征数量

# 定义自动编码器结构
input_layer = 输入（形状=（input_dim，））

编码=密集（128，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（input_layer）
编码=密集（64，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（编码）
编码 = Dense(32,activation=&#39;relu&#39;, kernel_regularizer=l2(0.001))(encoded) # 编码表示
解码=密集（64，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（编码）
解码=密集（128，激活=&#39;relu&#39;，kernel_regularizer=l2（0.001））（解码）
解码=密集（input_dim，激活=&#39;sigmoid&#39;）（解码）

自动编码器=模型（输入层，解码）

# 编译并训练自动编码器
autoencoder.compile（优化器=&#39;adam&#39;，损失=&#39;mse&#39;）


如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/77762883/my-autoencoder-for-tfidf-vectors-is-yielding-the-same-output-for-different-text</guid>
      <pubDate>Fri, 05 Jan 2024 06:41:35 GMT</pubDate>
    </item>
    <item>
      <title>无论输入图像如何，具有 TensorFlow Lite 模型的 Flask API 始终预测相同的类别</title>
      <link>https://stackoverflow.com/questions/77762697/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</link>
      <description><![CDATA[我正在开发 Flask API，以使用 TensorFlow Lite 模型执行推理，该模型是在阿尔茨海默氏症 5 类图像数据集上训练的，这些图像是 [“AD - 阿尔茨海默病”、“CN - 认知正常”、“EMCI - 早期轻度”认知障碍”、“LMCI - 晚期轻度认知障碍”、“MCI - 轻度认知障碍”]。
该模型在我的训练环境中运行良好，但当我将其部署到 Flask API 中时，出现了问题。 API 一致地为每张图像预测相同的类别（“MCI - 轻度认知障碍”），而在我的 Colab 笔记本中训练的模型则准确地预测各种类别。该 API 稍后将与 React Native App 集成。
使用不同的数据集训练模型两次，但问题仍然存在。我现在已经走进了死胡同，不知道如何解决它。
TFLite 模型代码：
https://colab.research.google.com/drive/1xxW8v5ZBKLvlGrofL2fBy9WYk_Fn5Dj_?usp=分享
FlaskAPI 代码：
fromflask导入Flask，request，jsonify
将张量流导入为 tf
导入CV2
将 numpy 导入为 np
从 PIL 导入图像
导入io

应用程序=烧瓶（__名称__）

解释器 = tf.lite.Interpreter(model_path=“latest_model.tflite”)
解释器.allocate_tensors()

class_names = [“CN-认知正常”、“AD-阿尔茨海默病”、“EMCI-早期轻度认知障碍”、“MCI-轻度认知障碍”、“LMCI-晚期轻度认知障碍”]

def preprocess_image(图像):
    图像 = cv2.resize(图像, (150, 150))
    图像 = image.astype(&#39;float32&#39;) / 255.0
    图像 = np.expand_dims(图像, 轴=0)
    返回图像


@app.route(&#39;/predict&#39;,methods=[&#39;POST&#39;])
def 预测（）：
    尝试：
        文件 = request.files[&#39;文件&#39;]
        image_file = Image.open(io.BytesIO(file.read()))
        图像 = cv2.cvtColor(np.array(image_file), cv2.COLOR_RGB2BGR)

    
        如果不是（image.shape[0] &gt;= 150 且 image.shape[1] &gt;= 150 且 image.shape[2] == 3）：
        return jsonify({&quot;error&quot;: &quot;无效的图像形状&quot;})


        图像 = image.astype(&#39;float32&#39;) / 255.0

        预处理图像 = 预处理图像（图像）

        terpreter.set_tensor(interpreter.get_input_details()[0][&#39;index&#39;], preprocessed_image)
        解释器.invoke()

        output_tensor =terpreter.get_tensor(interpreter.get_output_details()[0][&#39;index&#39;])
        Predicted_class_index = np.argmax(output_tensor, axis=1)[0]
        预测类名称 = 类名称[预测类索引]

        结果 = {“预测”：预测类名称，“输出张量”：output_tensor.tolist()}
        返回 jsonify(结果)
    除了异常 e：
       返回 jsonify({“错误”: str(e)})

如果 __name__ == &#39;__main__&#39;:
应用程序运行（调试=真）

尝试记录输出张量，但这是我从 Flask API 获得的输出。知道输出张量表明偏向于 MCI 类，但如果是这种情况，为什么它在 colab 环境中完美运行，而不是在 Flask API 中运行？
此外，除了使用 Flask API 来将模型与我的 React Native 应用程序集成之外，您还建议我使用其他更好的方法吗？
&lt;前&gt;&lt;代码&gt;{

“输出张量”：[

[

0.0004518234636634588,

0.0004140451201237738,

0.002781340153887868,

0.7277416586875916,

0.2686111330986023

]

],

“预测”：“MCI-轻度认知障碍”

}
]]></description>
      <guid>https://stackoverflow.com/questions/77762697/flask-api-with-tensorflow-lite-model-always-predicts-the-same-class-regardless</guid>
      <pubDate>Fri, 05 Jan 2024 05:43:52 GMT</pubDate>
    </item>
    <item>
      <title>TF Transformer 模型永远不会过拟合，只会停滞不前：训练曲线的解读和改进建议</title>
      <link>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</link>
      <description><![CDATA[此训练曲线适用于处理 2D（不包括批次）顺序信号并使用 Adam 优化器、32 批次大小和学习率的 Transformer 模型：一个自定义 LR 调度程序，它复制在“注意是”中使用的预热调度程序所有你需要的&#39;纸。训练曲线如下所示，最终训练损失略低于验证损失，但训练损失永远不会开始回升，我将其解释为模型永远不会开始过度拟合，只是在 90 纪元后停止重新调整权重。
更好的解释和解决方案来改进这个模型？

下面是我的简短的可重现代码：
x_train = np.random.normal(size=(32, 512, 512))
批量大小 = 32
H, W = x_train.shape
行，列= np.indices（（H，W），稀疏= True）
padding_mask_init = np.zeros((H, W, W), dtype=np.bool_)
padding_mask_init[行，1：，列] = 1
padding_mask = padding_mask_init[:batch_size]
嵌入尺寸 = 512
密集_暗 = 2048
头数 = 2
形状 = (batch_size, embed_dim, 512) #(32, 512, 512)
解码器_输入=层.输入（batch_input_shape=形状，dtype=tensorflow.float16）
mha_1 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
mha_2 = 层.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
Layernorm_1 = 层.LayerNormalization()

Z = 解码器输入
Z = mha_1(查询=Z、值=Z、键=Z、use_causal_mask=True、attention_mask=padding_mask)
Z = layernorm_1(Z + 解码器输入)
Z = mha_2(查询=Z，值=解码器输入，键=解码器输入，attention_mask=padding_mask)
输出=layers.TimeDistributed（keras.layers.Dense（embed_dim，激活=“softmax”））（Z）

模型 = keras.Model(decoder_inputs, 输出)
model.compile（损失=“mean_squared_error”，optimizer=tf.keras.optimizers.Adam（learning_rate=lr_schedule（embed_dim，3000），beta_1=0.9，beta_2=0.98，epsilon=1.0e-9），metrics=[&quot; “准确度”]）

历史= model.fit（数据集，epochs = 200，validation_data = val_dataset）
]]></description>
      <guid>https://stackoverflow.com/questions/77762264/tf-transformer-model-never-overfits-and-just-plateaus-interpretation-of-this-tr</guid>
      <pubDate>Fri, 05 Jan 2024 02:47:25 GMT</pubDate>
    </item>
    <item>
      <title>多元数据排序算法设计[关闭]</title>
      <link>https://stackoverflow.com/questions/77756234/multivariate-data-ranking-algorithm-design</link>
      <description><![CDATA[目前我已经收集了很多学校的相关数据。每个学校都有教师人数、学生人数、硬件设施等十个评价指标，这些指标的程度是用数字来表示的，比如教师数7.5，学生数10.5，我没有了解各个指标的权重。现在我们想用机器学习算法对高校的综合实力进行自动排名，而不参考任何国际知名排名，比如QS。有什么算法可以让计算机根据高校的综合实力自动排名基于多维度指标的实力？]]></description>
      <guid>https://stackoverflow.com/questions/77756234/multivariate-data-ranking-algorithm-design</guid>
      <pubDate>Thu, 04 Jan 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>当我在 jupyter 笔记本上运行简单的 cnn 模型时，CPU 使用率较低</title>
      <link>https://stackoverflow.com/questions/77730719/low-cpu-usage-when-i-run-a-simple-cnn-model-on-jupyter-notebook</link>
      <description><![CDATA[我在 Jupyter 笔记本上运行了一个非常简单的 CNN 模型，但过程非常慢。我在我的旧笔记本电脑（核心 i7U 10gen）上运行相同的程序。只花了一分半钟，但在我的新笔记本电脑（酷睿 i9 13900hx 和 rtx4060）上花了 30 分钟！它们都是在 CPU 上运行的，但在我的旧电脑上，CPU 使用率为 100%，在我的新电脑上，大约为 20%。然后，我在 PyCharm 中运行相同的程序，一切正常！这让我很困惑，我尝试了很多方法但都不起作用。我想知道真正的问题出在哪里？我的 Jupyter 笔记本还是其他东西？
我尝试在不同的 PC、不同的 IDE 平台上运行相同的程序。我想知道真正的问题出在哪里。]]></description>
      <guid>https://stackoverflow.com/questions/77730719/low-cpu-usage-when-i-run-a-simple-cnn-model-on-jupyter-notebook</guid>
      <pubDate>Fri, 29 Dec 2023 07:23:45 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow 自定义学习率调度程序给出意外的 EagerTensor 类型错误</title>
      <link>https://stackoverflow.com/questions/76511182/tensorflow-custom-learning-rate-scheduler-gives-unexpected-eagertensor-type-erro</link>
      <description><![CDATA[下面是我的自定义LR调度程序，它是tensorflow.keras.optimizers.schedules.LearningRateSchedule的子类，出现错误TypeError：无法将-0.5转换为dtype int64的EagerTensor。真的很困惑为什么 Eagertensor 与此自定义类的返回调用的简单平方反比计算相关。
class lr_schedule(tensorflow.keras.optimizers.schedules.LearningRateSchedule)：
    def __init__(自身、dim_embed、warmup_steps):
        self.dim_embed = dim_embed
        self.warmup_steps = 预热步骤
    def __call__(自我，步骤)：
        返回（self.dim_embed ** -0.5）* min（（步骤** -0.5），步骤*（self.warmup_steps ** -1.5））

与此错误没有特别相关，但这是一个自定义 LR 调度程序，它复制“Attention is All You Need”论文中使用的预热调度程序。]]></description>
      <guid>https://stackoverflow.com/questions/76511182/tensorflow-custom-learning-rate-scheduler-gives-unexpected-eagertensor-type-erro</guid>
      <pubDate>Tue, 20 Jun 2023 03:08:43 GMT</pubDate>
    </item>
    <item>
      <title>如何修复 KerasTensor 传递给 TF API 时出现的错误？</title>
      <link>https://stackoverflow.com/questions/71808327/how-to-fix-error-where-a-kerastensor-is-passed-to-a-tf-api</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/71808327/how-to-fix-error-where-a-kerastensor-is-passed-to-a-tf-api</guid>
      <pubDate>Sat, 09 Apr 2022 13:03:40 GMT</pubDate>
    </item>
    <item>
      <title>NEAT 的类型错误</title>
      <link>https://stackoverflow.com/questions/68248850/typeerror-with-neat</link>
      <description><![CDATA[我在尝试制作蛇 AI 时遇到 NEAT 类型错误：
node_inputs.append(self.values[i] * w)
类型错误：无法将序列乘以“float”类型的非 int

代码
类 SnakeGame(对象):
    def __init__(自身、基因组、配置):
    self.genome = 基因组
        self.nets = []

        对于 id，self.genomes 中的 g：
            净=整洁.nn.FeedForwardNetwork.create（g，配置）
            self.nets.append(net)
            g.适应度 = 0
 

在同一个类的另一个函数中的代码
def 游戏（自身）：
    而真实：
        对于 pg.event.get() 中的事件：
            如果 event.type == pg.QUIT：
                pg.quit()
        数据 = self.nets[0].activate(self.getData())
        输出 = data.index(max(data))

函数 getData 是什么样的
def getData(self):
    数据 = [self.x_position, self.y_position, self.food_x, self.food_y, self.snakeLength]
    返回数据

config-feedforward.txt 的部分代码
&lt;前&gt;&lt;代码&gt;[整洁]
健身标准 = 最大值
健身阈值 = 1000
弹出大小 = 2
灭绝时重置=真
]]></description>
      <guid>https://stackoverflow.com/questions/68248850/typeerror-with-neat</guid>
      <pubDate>Sun, 04 Jul 2021 21:19:36 GMT</pubDate>
    </item>
    <item>
      <title>如何在 BigQuery 中使用外部回归器训练 Arima_PLUS 模型？</title>
      <link>https://stackoverflow.com/questions/67624116/how-to-use-external-regressors-for-training-arima-plus-model-in-bigquery</link>
      <description><![CDATA[我在大查询上创建了一个模型，是否可以包含额外的列作为外部回归量？
例如，我想包含日期、用户、每个会话的页面、跳出率等以预测用户。
创建或替换模型 bqml_tutorial.create_model
选项
(model_type=&#39;ARIMA_PLUS&#39;,
time_series_timestamp_col=&#39;日期&#39;,
time_series_data_col=&#39;用户&#39;,
auto_arima=真，
数据频率 = &#39;自动频率&#39;,
decompose_time_series=真）
作为
从“bqml_tutorial.cvrate”中选择日期、简历作为用户 ORDER BY Date
]]></description>
      <guid>https://stackoverflow.com/questions/67624116/how-to-use-external-regressors-for-training-arima-plus-model-in-bigquery</guid>
      <pubDate>Thu, 20 May 2021 16:07:01 GMT</pubDate>
    </item>
    <item>
      <title>用于机器学习算法的 csv 流</title>
      <link>https://stackoverflow.com/questions/44240145/csv-stream-for-machine-learning-algorithms</link>
      <description><![CDATA[我有一个很大的 CSV 文件（大约 5GB）。
我试图逐行读取整个文件，并尝试应用最典型的算法（SVM、朴素贝叶斯、线性回归等）。
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd
导入 csv

i_f = open(&#39;top2Mmm.csv&#39;, &#39;r&#39; )
reader = csv.reader( i_f, 分隔符 = &#39;;&#39; )
对于读卡器中的行：
print(“斐乐 -&gt;”, 行)

我刚刚成功阅读了 CSV，但我不知道如何获取每一行并构建模型。
我从一个较小的文件开始以加快该过程，但我不知道如何使该过程正常工作。
有什么线索或提示吗？]]></description>
      <guid>https://stackoverflow.com/questions/44240145/csv-stream-for-machine-learning-algorithms</guid>
      <pubDate>Mon, 29 May 2017 10:23:16 GMT</pubDate>
    </item>
    </channel>
</rss>