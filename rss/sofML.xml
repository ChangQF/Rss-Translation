<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 25 Jul 2024 15:15:59 GMT</lastBuildDate>
    <item>
      <title>在 Azure 机器学习工作室中读取 csv 文件时出现问题</title>
      <link>https://stackoverflow.com/questions/78793951/issues-reading-csv-file-in-azure-machine-learning-studio</link>
      <description><![CDATA[我是 Azure 机器学习工作室的新手，在 .ipynb 笔记本中读取 csv 文件时遇到问题。
CatBoost.ipynb 和 data-23.csv 已以我的用户名上传在此处输入图片描述。一旦计算开始，我就会导入库并尝试读取我的 csv在此处输入图像描述。
这是我收到的错误：
&#39;FileNotFoundError：[Errno 2] 没有这样的文件或目录：&#39;data-23.csv&#39;&#39;。
然后我决定检查绝对路径：
file_path = os.path.abspath(&quot;data-23.csv&quot;)
print(file_path)
返回
/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1721395016_987/container_123_abc/data-23.csv
我保存了完整路径并尝试再次：
data = pd.read_csv(file_path)
data.head()
返回原始错误
[Errno 2] 没有这样的文件或目录：&#39;/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1721395016_987/container_123_abc/data-23.csv&#39;
有人遇到过这个问题吗？]]></description>
      <guid>https://stackoverflow.com/questions/78793951/issues-reading-csv-file-in-azure-machine-learning-studio</guid>
      <pubDate>Thu, 25 Jul 2024 14:36:09 GMT</pubDate>
    </item>
    <item>
      <title>使用变换对测试集进行留一编码</title>
      <link>https://stackoverflow.com/questions/78793796/leave-one-out-encoding-on-test-set-with-transform</link>
      <description><![CDATA[上下文：使用 sklearn 预处理数据集时，您会在训练集上使用 fit_transform，并在测试集上使用 transform，以避免数据泄露。使用留一法 (LOO) 编码时，您需要目标变量值来计算特征值的编码值。在管道中使用 LOO 编码器时，您可以使用 fit_transform 函数将其应用于训练集，该函数接受特征 (X) 和目标值 (y)。
在知道 transform 不接受目标变量值作为参数的情况下，如何使用相同的管道计算测试集的 LOO 编码？我对此很困惑。transform 函数确实转换了列，但没有考虑目标的值，因为它没有该信息。]]></description>
      <guid>https://stackoverflow.com/questions/78793796/leave-one-out-encoding-on-test-set-with-transform</guid>
      <pubDate>Thu, 25 Jul 2024 14:05:44 GMT</pubDate>
    </item>
    <item>
      <title>烧瓶岭回归模型预测</title>
      <link>https://stackoverflow.com/questions/78793708/flask-ridge-regression-model-prediction</link>
      <description><![CDATA[当我运行 application.py 时，它将运行，当我将预测的数据点提供给模型，然后提交模型。然后将显示错误，如下所示
内部服务器错误
服务器遇到内部错误，无法完成您的请求。服务器超载或应用程序中出现错误。
我如何运行此应用程序并在给出值时给出预测点。我正在提供代码，请给我解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78793708/flask-ridge-regression-model-prediction</guid>
      <pubDate>Thu, 25 Jul 2024 13:48:04 GMT</pubDate>
    </item>
    <item>
      <title>在 pythonanywhere 中输入后预测页面未显示</title>
      <link>https://stackoverflow.com/questions/78793658/prediction-page-not-showing-after-giving-input-in-pythonanywhere</link>
      <description><![CDATA[我使用 pythonanywhere 实现了 flask 项目。我能够在 localhost 上实现它，但无法在 pythonanywhere 中实现它。只显示输入页面，提交后需要很长时间，并且不显示下一页。
可能的原因是什么，整个文件大小仅低于 2MB
我希望预测显示在结果页面中。索引页仍在加载.. 服务器日志中显示信号 9]]></description>
      <guid>https://stackoverflow.com/questions/78793658/prediction-page-not-showing-after-giving-input-in-pythonanywhere</guid>
      <pubDate>Thu, 25 Jul 2024 13:37:35 GMT</pubDate>
    </item>
    <item>
      <title>使用 load_model 函数加载 HDF5 模型不起作用</title>
      <link>https://stackoverflow.com/questions/78793440/loading-hdf5-model-with-load-model-function-is-not-working</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78793440/loading-hdf5-model-with-load-model-function-is-not-working</guid>
      <pubDate>Thu, 25 Jul 2024 12:55:24 GMT</pubDate>
    </item>
    <item>
      <title>使用 CNN 进行单类物体检测获得假阳性</title>
      <link>https://stackoverflow.com/questions/78793283/single-class-object-detection-using-cnn-getting-false-positive</link>
      <description><![CDATA[在这里，我尝试使用 cnn 构建一个 Manhole 物体检测，在这个模型中，经过训练我得到了 95% 的准确率。我得到的是假阳性，例如，如果我用人孔（训练对象）测试图像进​​行检测，它将绘制边界框，而我测试没有训练对象的随机图像，则会出现一个随机边界框，这就是问题所在，在实时网络摄像头测试中也是如此，但在这里，如果对象甚至没有被检测到，则会在框架中获取一些随机边界框。这里我提供我的代码，请帮助
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Conv2D, Input, BatchNormalization``, Flatten, MaxPool2D, Dense
from pathlib import Path

train_img = Path(&quot;DATASET/train/Manhole&quot;)
val_img = Path(&quot;DATASET\valid\Manhole&quot;)

train_csv = pd.read_csv(&#39;DATASET/train/Manhole/_annotations.csv&#39;) 
val_csv = pd.read_csv(&#39;DATASET/valid/_annotations.csv&#39;)
#print(train_csv)
train_csv[[&#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;]] = train_csv[[&#39;xmin&#39;, &#39;ymin&#39;, &#39;xmax&#39;, &#39;ymax&#39;]].fillna(0)
train_csv[[&#39;xmin&#39;,&#39;ymin&#39;,&#39;xmax&#39;,&#39;ymax&#39;]] = train_csv[[&#39;xmin&#39;,&#39;ymin&#39;,&#39;xmax&#39;,&#39;ymax&#39;]].astype(int)
train_csv.drop_duplicates(subset=&#39;filename&#39;,inplace=True, ignore_index=True)
val_csv.drop_duplicates(subset=&#39;filename&#39;, inplace=True, ignore_index=True)

def datagenerator(df ,batch_size ,path):
while True:
images = np.zeros((batch_size,640,640,3))
bounding_box_coords = np.zeros((batch_size, 4))

for i in range(batch_size):
rand_index = np.random.randint(0, train_csv.shape[0])
row = df.loc[rand_index, :]
images[i] = cv2.imread(str(path/row.filename)) / 255.
bounding_box_coords[i] = np.array([row.xmin, row.ymin, row.xmax, row.ymax])

产生 {&#39;filename&#39;: images}, {&#39;coords&#39;: bounding_box_coords}

# example, label = next(datagenerator(batch_size=16))
# img = example[&#39;filename&#39;][0]
# bbox_coords = label[&#39;coords&#39;][0] 

# x1, y1, x2, y2 = map(int, bbox_coords)
# print(&#39;bbox cords&#39;,x1,y1,x2,y2)
# cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)
# cv2.putText(img, &#39;&#39;, (x1,y1-10),cv2.FONT_HERSHEY_DUPLEX, 0.8, (0, 0, 255), 2)
# # plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
# plt.imshow(img)
# plt.show()

input_ = 输入(shape=[640, 640, 3], name=&#39;filename&#39;)

x = input_
x = Conv2D(16, (3,3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2, 填充=&#39;same&#39;)(x)

x = Conv2D(32, (3,3), 激活=&#39;relu&#39;, 填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2, 填充=&#39;same&#39;)(x)

x = Conv2D(64, (3,3),激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(128，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(256，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，填充=&#39;same&#39;)(x)

x = Conv2D(312，(3,3)，激活=&#39;relu&#39;，填充=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(500，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(580，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Conv2D(680，(3,3)，activation=&#39;relu&#39;，padding=&#39;same&#39;)(x)
x = BatchNormalization()(x)
x = MaxPool2D(2，padding=&#39;same&#39;)(x)

x = Flatten()(x)
x = Dense(256，激活=&#39;relu&#39;)(x)
x = Dense(32, 激活=&#39;relu&#39;)(x)
输出坐标 = Dense(4, 名称=&#39;coords&#39;)(x)

模型 = tf.keras.models.Model(input_,output_coords)

模型摘要()

模型编译(loss={&#39;coords&#39;: &#39;mse&#39;},
优化器=tf.keras.optimizers.Adam(5e-5), 
指标={&#39;coords&#39;: &#39;accuracy&#39;})

检查点回调 = ModelCheckpoint(&#39;model_Checkpoint.h5&#39;, 监视器=&#39;val_loss&#39;, save_best_only=True, 模式=&#39;min&#39;)

模型拟合(数据生成器(df=train_csv,batch_size=6,path=train_img), 
epochs=80, steps_per_epoch=150,
validation_data=datagenerator(df=val_csv,batch_size=6,path=val_img), 
validation_steps=240, 
callbacks=[checkpoint_callback])

model.save(&#39;model2.h5&#39;)

我需要代码来在实时网络摄像头中正确检测训练过的对象，而不会出现任何边界框，并从 cnn 接收置信度值，这样我就可以设置检测的阈值]]></description>
      <guid>https://stackoverflow.com/questions/78793283/single-class-object-detection-using-cnn-getting-false-positive</guid>
      <pubDate>Thu, 25 Jul 2024 12:22:40 GMT</pubDate>
    </item>
    <item>
      <title>EMA 衰减与 LR 衰减之间的实际差异</title>
      <link>https://stackoverflow.com/questions/78793123/practical-difference-between-ema-decay-and-lr-decay</link>
      <description><![CDATA[我无法理解 EMA 衰减和 LR 衰减在实践中的差异。
我觉得它们都以不同的方式完成了相同的事情（以下内容可能是错误的，所以我提前道歉，如果我的理解完全错误，请纠正我）：

使用 EMA，在训练期间保留模型的单独副本，并且每 N 步使用原始模型权重的平均值更新模型。
使用 LR 衰减，原始模型的权重在训练期间总是更新较少，但只有一个模型得到有效训练。

现在，给定一个包含 32 个样本的数据集，我可以想象这是两次训练的方式：
训练 A（无 EMA）
给定以下超参数：

LR 1e-5
批次大小为 4
线性调度程序

经过 4 个步骤后，模型将看到 16 个样本，LR 将下降到最终 LR 的一半。
实际上，模型已更新 4 次。
训练 B (EMA)
给定以下超参数：

LR 为 1e-5
批次大小为 1
恒定调度程序
EMA 衰减为 0.9999
EMA 更新步骤为 4

经过 16 个步骤后，模型将看到 16 个样本，EMA 衰减将上升到最终 EMA 的一半衰减。
实际上，原始模型已更新 16 次，EMA 模型已更新 4 次。
问题
最终，两个模型都更新了 4 次，我能看到的唯一区别是训练 A 直接更新了权重，而训练 B 更新了原始模型和 EMA 模型的权重。
为什么人们决定选择训练 B 而不是训练 A？]]></description>
      <guid>https://stackoverflow.com/questions/78793123/practical-difference-between-ema-decay-and-lr-decay</guid>
      <pubDate>Thu, 25 Jul 2024 11:49:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 Gratz 大学 LSM 模型预测 Lorenz 吸引子</title>
      <link>https://stackoverflow.com/questions/78792982/predicting-lorenz-attractor-using-the-gratz-university-lsm-model</link>
      <description><![CDATA[我目前正在研究 LSM，在阅读了几篇论文后，我认为我掌握了有关这种 Reservoir Computing 模型的主要信息。
但是，我很难用我用于 Lorenz Attractor 的模型获得良好的结果。
目前，我正试图仅预测 X 分量，但结果很糟糕。
我尝试了 NEST 模拟器 中的两种生成器。spike_generator 和 step_current_generator。
使用 step_current_generator 我有更好的结果，但我想坚持使用 W.Maas 模型并使用 spike_generator。
这是我所做的：
def generate_spike_times_lorenz(data, stim_times, gen_burst, scale_factor=100.0):
&quot;&quot;&quot;
根据 Lorenz X 分量生成尖峰时间。

参数：
- data：类似数组，Lorenz X 分量的值。
- stim_times：类似数组，刺激的时间。
- gen_burst：函数，在给定时间附近生成一连串尖峰。
- scale_factor：浮点数，缩放尖峰时间的因子。

返回：
- inp_spikes：数组列表，每个数组包含输入神经元的尖峰时间。
“” “”

inp_spikes = []

data = data * scale_factor

for value, t in zip(data, stim_times):
spike_count = int(value)
if spike_count &gt; 0:
spikes = np.concatenate([t + gen_burst() for _ in range(spike_count)])

# 缩放并调整尖峰时间
spikes *= 10
spikes = spikes.round() + 1.0
spikes = spikes / 10.0

spikes = np.sort(spikes)

inp_spikes.append(spikes)
else:
inp_spikes.append(np.array([]))

return inp_spikes

def injection_spikes(inp_spikes, neuron_targets):
spike_generators = nest.Create(&quot;spike_generator&quot;, len(inp_spikes))

for sg, sp in zip(spike_generators, inp_spikes):
nest.SetStatus(sg, {&#39;spike_times&#39;: sp})

C_inp = 100 # int(N_E / 20) # 每个输入神经元的传出输入突触数量

def generate_delay_normal_clipped(mu=10., sigma=20., low=3., high=200.):
delay = np.random.normal(mu, sigma)
delay = max(min(delay, high), low)
return delay

nest.Connect(spike_generators, neuron_targets,
{&#39;rule&#39;: &#39;fixed_outdegree&#39;,
&#39;outdegree&#39;: C_inp},
{&#39;synapse_model&#39;: &#39;static_synapse&#39;,
&#39;delay&#39;: generate_delay_normal_clipped(),
&#39;weight&#39;: nest.random.uniform(min=2.5 * 10 * 5.0, max=7.5 * 10 * 5.0)})

有人能提供一些见解或建议，告诉我如何改进我的模型，特别是在使用 spike_generator 来获得更好的性能方面吗？我可能忽略了哪些特定的参数或技术？]]></description>
      <guid>https://stackoverflow.com/questions/78792982/predicting-lorenz-attractor-using-the-gratz-university-lsm-model</guid>
      <pubDate>Thu, 25 Jul 2024 11:11:09 GMT</pubDate>
    </item>
    <item>
      <title>寻找机器学习模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78791542/finding-a-machine-learning-model</link>
      <description><![CDATA[我需要帮助寻找一个机器学习模型，该模型可以查看数值，并且可以使用我可以重新利用的一组标准进行预测。
我研究过现有的模型，但我只能找到预测房价的模型。]]></description>
      <guid>https://stackoverflow.com/questions/78791542/finding-a-machine-learning-model</guid>
      <pubDate>Thu, 25 Jul 2024 05:12:34 GMT</pubDate>
    </item>
    <item>
      <title>在列联表和博彩公司知情度/标记度的背景下，DTP 是什么？</title>
      <link>https://stackoverflow.com/questions/78791154/what-is-dtp-in-the-context-of-contingency-tables-and-bookmaker-informedness-mark</link>
      <description><![CDATA[Powers 使用 dtp 概念定义博彩公司知情度/标记度：
“我们可以通过将每个表达式的顶部和底部简化为概率（除以 N2，注意原始偶然性计数总和为 N，简化后的联合概率总和为 1），进一步了解这些回归和相关系数的性质。分子是偶然性矩阵的决定因素，并且是所有三个系数的共同点，简化为 dtp，而回归系数的简化分母仅取决于基础变量的普遍性或偏差。
因此，回归系数博彩公司知情度 (B) 和标记度 (M) 可以用准确率 (Prec) 或召回率以及偏差和流行度 (Prev) 或它们的倒数 (I-) 重新表示：&quot;
全文链接：https://arxiv.org/pdf/2010.16061
博彩公司标记度 = dtp/ [偏差 · (1-偏差)]
博彩公司知情度 = dtp/ [rp·rn]

我不太明白 DTP 是什么。它似乎是列联表的行列式，但我对 DTP 微积分或其首字母缩略词的含义都不是 100% 确定。他说的“所有 3 个系数都相同”是什么意思？在计算行列式之前，我应该将整个矩阵除以 N² 吗？
我期待 DTP 含义及其微积分的正式定义]]></description>
      <guid>https://stackoverflow.com/questions/78791154/what-is-dtp-in-the-context-of-contingency-tables-and-bookmaker-informedness-mark</guid>
      <pubDate>Thu, 25 Jul 2024 01:35:37 GMT</pubDate>
    </item>
    <item>
      <title>hmmlearn 中的隐马尔可夫模型不收敛</title>
      <link>https://stackoverflow.com/questions/78791079/hidden-markov-model-in-hmmlearn-not-converging</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78791079/hidden-markov-model-in-hmmlearn-not-converging</guid>
      <pubDate>Thu, 25 Jul 2024 00:56:12 GMT</pubDate>
    </item>
    <item>
      <title>由于 keras 中的 SSL 错误，无法加载 MNIST 数据集...load_data() 函数</title>
      <link>https://stackoverflow.com/questions/78668638/unable-to-load-mnist-data-set-due-to-ssl-error-in-keras-load-data-function</link>
      <description><![CDATA[import keras
(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()
...

我目前开始使用机器学习，但由于错误，我无法加载 MNIST 数据集：

异常：https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 上的 URL 获取失败：无 -- [SSL：CERTIFICATE_VERIFY_FAILED] 证书验证失败：无法获取本地颁发者证书(_ssl.c:1000)

我确保一切都是最新的（macOS 版本 14.5、pip 版本 24.1、最新的 tensorflow 和 keras 库以及最新的 safari 和 vscode 版本）。我尝试重新安装 keras 几次，确保不要从 pip3 缓存中下载最新的 pip 24.1 版本。这个错误一直存在，我还没有找到解决办法。我唯一弄清楚的是，这是由 ...load_data() 函数导致的错误。
如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78668638/unable-to-load-mnist-data-set-due-to-ssl-error-in-keras-load-data-function</guid>
      <pubDate>Tue, 25 Jun 2024 16:48:10 GMT</pubDate>
    </item>
    <item>
      <title>如何计算伯努利朴素贝叶斯的联合对数似然</title>
      <link>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</link>
      <description><![CDATA[对于使用 BernoulliNB 的分类问题，如何计算联合对数似然。联合似然由以下公式计算，其中 y(d) 是实际输出（不是预测值）的数组，x(d) 是特征的数据集。
我阅读了这个答案并阅读了文档，但它并没有完全满足我的目的。有人可以帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/52861129/how-to-calculate-the-joint-log-likelihood-for-bernoulli-naive-bayes</guid>
      <pubDate>Wed, 17 Oct 2018 18:08:50 GMT</pubDate>
    </item>
    <item>
      <title>在 GridSearchCV 中为 XGBoost 评分</title>
      <link>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</link>
      <description><![CDATA[我目前正在尝试首次使用 XGBoost 分析数据。我想使用 GridsearchCV 找到最佳参数。我想最小化均方根误差，为此，我使用“rmse”作为 eval_metric。但是，网格搜索中的评分没有这样的指标。我在这个网站上发现“neg_mean_squared_error”也有同样的效果，但我发现这给出的结果与 RMSE 不同。当我计算“neg_mean_squared_error”绝对值的根时，我得到的值约为 8.9，而另一个函数给出的 RMSE 约为 4.4。
我不知道哪里出了问题，或者我如何让这两个函数一致/给出相同的值？
由于这个问题，我得到了错误的值作为“best_params_”，这给了我比我最初开始调整的一些值更高的 RMSE。
有人能解释一下如何在网格搜索中获得 RMSE 的分数，或者为什么我的代码给出了不同的值吗？ 
提前致谢。
def modelfit(alg, trainx, trainy, useTrainCV=True, cv_folds=10, early_stopping_rounds=50):
if useTrainCV:
xgb_param = alg.get_xgb_params()
xgtrain = xgb.DMatrix(trainx, label=trainy)
cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()[&#39;n_estimators&#39;], nfold=cv_folds,
metrics=&#39;rmse&#39;, early_stopping_rounds=early_stopping_rounds)
alg.set_params(n_estimators=cvresult.shape[0])

# 将算法拟合到数据上
alg.fit(trainx, trainy, eval_metric=&#39;rmse&#39;)

# 预测训练集：
dtrain_predictions = alg.predict(trainx)
# dtrain_predprob = alg.predict_proba(trainy)[:, 1]
print(dtrain_predictions)
print(np.sqrt(mean_squared_error(trainy, dtrain_predictions)))

# 打印模型报告：
print(&quot;\nModel Report&quot;)
print(&quot;RMSE : %.4g&quot; % np.sqrt(metrics.mean_squared_error(trainy, dtrain_predictions)))

param_test2 = {
&#39;max_depth&#39;:[6,7,8],
&#39;min_child_weight&#39;:[2,3,4]
}

grid2 = GridSearchCV(estimator = xgb.XGBRegressor( learning_rate =0.1, n_estimators=2000, max_depth=5,
min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,
objective= &#39;reg:linear&#39;, nthread=4, scale_pos_weight=1, random_state=4),
param_grid = param_test2,scoring=&#39;neg_mean_squared_error&#39;, n_jobs=4,iid=False, cv=10, verbose=20)
grid2.fit(X_train,y_train)
# best_estimator 的平均交叉验证分数
print(grid2.best_params_, np.sqrt(np.abs(grid2.best_score_))), print(np.sqrt(np.abs(grid2.score(X_train, y_train))))
modelfit(grid2.best_estimator_, X_train, y_train)
print(np.sqrt(np.abs(grid2.score(X_train, y_train))))
]]></description>
      <guid>https://stackoverflow.com/questions/50296817/scoring-in-gridsearchcv-for-xgboost</guid>
      <pubDate>Fri, 11 May 2018 16:46:16 GMT</pubDate>
    </item>
    <item>
      <title>神经网络收敛至零输出</title>
      <link>https://stackoverflow.com/questions/44213659/neural-network-converging-to-zero-output</link>
      <description><![CDATA[我正在尝试训练这个神经网络来对一些数据进行预测。
我在一个小的数据集（大约 100 条记录）上尝试了它，它工作得很好。然后我插入了新的数据集，我发现 NN 收敛到 0 输出，误差大约收敛到正例数与总例数之比。
我的数据集由是/否特征（1.0/0.0）组成，基本事实也是是/否。
我的假设：

1) 存在输出为 0 的局部最小值（但我尝试了许多学习率和初始权重值，它似乎总是收敛到那里）

2) 我的权重更新是错误的（但在我看来很好）

3) 它只是一个输出缩放问题。我尝试缩放输出（即输出/最大值（输出）和输出/平均值（输出）），但结果并不好，如您在下面提供的代码中看到的那样。我应该以不同的方式缩放它吗？Softmax？ 
代码如下：
import pandas as pd
import numpy as np
import pickle
import random
from collections import defaultdict

alpha = 0.1
N_LAYERS = 10
N_ITER = 10
#N_FEATURES = 8
INIT_SCALE = 1.0

train = pd.read_csv(&quot;./data/prediction.csv&quot;)

y = train[&#39;y_true&#39;].as_matrix()
y = np.vstack(y).astype(float)
ytest = y[18000:]
y = y[:18000]

X = train.drop([&#39;y_true&#39;], axis = 1).as_matrix()
Xtest = X[18000:].astype(float)
X = X[:18000]

def tanh(x,deriv=False):
if(deriv==True):
return (1 - np.tanh(x)**2) * alpha
else:
return np.tanh(x)

def sigmoid(x,deriv=False):
if(deriv==True):
return x*(1-x)
else:
return 1/(1+np.exp(-x))

def relu(x,deriv=False):
if(deriv==True):
return 0.01 + 0.99*(x&gt;0)
else:
return 0.01*x + 0.99*x*(x&gt;0)

np.random.seed()

syn = defaultdict(np.array)

for i in range(N_LAYERS-1):
syn[i] = INIT_SCALE * np.random.random((len(X[0]),len(X[0]))) - INIT_SCALE/2
syn[N_LAYERS-1] = INIT_SCALE * np.random.random((len(X[0]),1)) - INIT_SCALE/2

l = defaultdict(np.array)

delta = defaultdict(np.array)

for j in xrange(N_ITER):
l[0] = X
for i in range(1,N_LAYERS+1):
l[i] = relu(np.dot(l[i-1],syn[i-1]))

error = (y - l[N_LAYERS])

e = np.mean(np.abs(error))
if (j% 1) == 0:
print &quot;\nIteration &quot; + str(j) + &quot; of &quot; + str(N_ITER)
print &quot;Error: &quot; + str(e)

delta[N_LAYERS] = error*relu(l[N_LAYERS],deriv=True) * alpha
for i in range(N_LAYERS-1,0,-1):
error = delta[i+1].dot(syn[i].T)
delta[i] = error*relu(l[i],deriv=True) * alpha

for i in range(N_LAYERS):
syn[i] += l[i].T.dot(delta[i+1])

pickle.dump(syn, open(&#39;neural_weights.pkl&#39;, &#39;wb&#39;))

# 使用 f1-measure 进行测试
# 召回率 = 真阳性 / (真阳性 + 假阴性)
# 准确率 = 真阳性 / (真阳性阳性 + 假阳性)

l[0] = Xtest
for i in range(1,N_LAYERS+1):
l[i] = relu(np.dot(l[i-1],syn[i-1]))

out = l[N_LAYERS]/max(l[N_LAYERS])

tp = float(0)
fp = float(0)
fn = float(0)
tn = float(0)

for i in l[N_LAYERS][:50]:
print i

for i in range(len(ytest)):
if out[i] &gt; 0.5 and ytest[i] == 1:
tp += 1
if out[i] &lt;= 0.5 and ytest[i] == 1:
fn += 1
if out[i] &gt; 0.5 且 ytest[i] == 0:
fp += 1
if out[i] &lt;= 0.5 且 ytest[i] == 0:
tn += 1

print &quot;tp: &quot; + str(tp)
print &quot;fp: &quot; + str(fp)
print &quot;tn: &quot; + str(tn)
print &quot;fn: &quot; + str(fn)

print &quot;\nprecision: &quot; + str(tp/(tp + fp))
print &quot;recall: &quot; + str(tp/(tp + fn))

f1 = 2 * tp /(2 * tp + fn + fp)
print &quot;\nf1-measure:&quot; + str(f1)

输出结果如下：
第 0 次迭代（共 10 次）
错误： 0.222500767998

10 次迭代中的第 1 次
错误：0.222500771157

10 次迭代中的第 2 次
错误：0.222500774321

10 次迭代中的第 3 次
错误：0.22250077749

10 次迭代中的第 4 次
错误：0.222500780663

10 次迭代中的第 5 次
错误：0.222500783841

10 次迭代中的第 6 次
错误：0.222500787024

10 次迭代中的第 7 次
错误：0.222500790212

10 次迭代中的第 8 次
错误：0.222500793405

10 次迭代中的第 9 次10
错误：0.222500796602

[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 4.62182626e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 4.62182626e-06]
[ 0.]
[ 0.]
[ 5.04501079e-10]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 0.]
[ 5.04501079e-10]
[ 0.]
[ 0.]
[ 4.62182626e-06]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 0.]
[ 0.]
[ 5.58610895e-06]
[ 0.]
[ 1.31432294e-05]

tp：28.0
fp：119.0
tn： 5537.0
fn：1550.0

精度：0.190476190476
召回率：0.0177439797212

f1-measure：0.0324637681159
]]></description>
      <guid>https://stackoverflow.com/questions/44213659/neural-network-converging-to-zero-output</guid>
      <pubDate>Sat, 27 May 2017 06:21:36 GMT</pubDate>
    </item>
    </channel>
</rss>