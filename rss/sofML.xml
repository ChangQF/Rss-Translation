<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 17 Nov 2024 01:23:38 GMT</lastBuildDate>
    <item>
      <title>XGBRegressor 模型在时间序列模型中欠拟合</title>
      <link>https://stackoverflow.com/questions/79196179/xgbregressor-model-underfitting-in-time-series-model</link>
      <description><![CDATA[我正在尝试拟合 XGBRegressor 来预测传感器的未来行为。数据具有 6 个周期的季节性。当我拟合数据时，训练的 RMSE 会降低，而测试的 RMSE 不会降低太多并开始增加。如果我将学习率更改为 1 并将最大深度更改为 3，它会在训练数据上过度拟合，但在测试上是一条直线。以下是模型的代码训练数据预测
from xgboost import XGBRegressor
model = XGBRegressor(n_estimators = 1000, early_stopping_rounds = 50,
learning_rate = 0.01, max_depth = 8)
model.fit(X_train, Y_train, 
eval_set = [(X_train, Y_train), (X_test, Y_test)],
verbose = 10)

# 对训练数据进行预测
Y_train_pred = model.predict(X_train)

供参考：X 有 3 个特征是时间（每个点为 3 秒），其他 2 个是
sin_time = 0.5 * np.sin(time) * 2 * np.sin(time) * time 
sin_2pi_time = np.sin(2 * np.pi * time)

Y 是阻力
尝试更改参数，但没有成功，即使我过度拟合模型，训练数据预测也是一条直线测试数据预测]]></description>
      <guid>https://stackoverflow.com/questions/79196179/xgbregressor-model-underfitting-in-time-series-model</guid>
      <pubDate>Sat, 16 Nov 2024 22:36:52 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的暹罗模型不适用于验证数据？</title>
      <link>https://stackoverflow.com/questions/79196054/why-does-my-siamese-model-not-work-on-verification-data</link>
      <description><![CDATA[我的模型之前运行良好，并做出了良好的预测。然而，现在我尝试使用它，它无法识别图像之间的相似性。请帮助解决此问题。
# 保存权重
siamese_model.save(&#39;siamesemodel.h5&#39;)

# 加载模型
model = tf.keras.models.load_model(
&#39;siamesemodel.h5&#39;, 
custom_objects={&#39;L1Dist&#39;: L1Dist, &#39;BinaryCrossentropy&#39;: tf.losses.BinaryCrossentropy}
)

# 验证函数
def verify(model, detection_threshold, validation_threshold):
# 构建结果数组
results = []
for image in os.listdir(os.path.join(&#39;application_data&#39;, &#39;verification_images&#39;)):
input_img = preprocess(os.path.join(&#39;application_data&#39;, &#39;input_image&#39;, &#39;input_image.jpg&#39;))
validation_img = preprocess(os.path.join(&#39;application_data&#39;, &#39;verification_images&#39;, image))

result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))
results.append(result)

# 检测阈值：高于该指标的预测被视为正值
detection = np.sum(np.array(results) &gt; detection_threshold)

 # 验证阈值：正预测的比例/总正样本
validation = detection / len(os.listdir(os.path.join(&#39;application_data&#39;, &#39;verification_images&#39;)))
verified = validation &gt; verify_threshold

返回结果，已验证

cap = cv2.VideoCapture(0)
while cap.isOpened():
ret, frame = cap.read()
frame = frame[120:120+250, 200:200+250, :]

cv2.imshow(&#39;Verification&#39;, frame)

# 验证触发器
if cv2.waitKey(10) &amp; 0xFF == ord(&#39;v&#39;):
# 将输入图像保存到 input_image 文件夹
cv2.imwrite(os.path.join(&#39;application_data&#39;, &#39;input_image&#39;, &#39;input_image.jpg&#39;), frame)
# 运行验证
results, verified = verify(model, 0.5, 0.5)
print(verified)

if cv2.waitKey(10) &amp; 0xFF == ord(&#39;q&#39;):
break
cap.release()
cv2.destroyAllWindows()

打印结果如下：
[array([[9.938484e-09]], dtype=float32),
array([[0.00011181]], dtype=float32),
array([[4.0544733e-06]], dtype=float32),
array([[3.6490118e-07]], dtype=float32),
array([[1.779369e-07]], dtype=float32),
array([[0.15224604]], dtype=float32),
array([[2.0296879e-05]], dtype=float32),
数组([[7.9831276e-05]], dtype=float32),
数组([[2.3284203e-05]], dtype=float32),
数组([[8.0619594e-07]], dtype=float32),
数组([[1.0691416e-06]], dtype=float32),
数组([[1.9231505e-08]], dtype=float32),
数组([[2.243531e-05]], dtype=float32),
数组([[6.483703e-07]], dtype=float32),
数组([[6.656185e-07]], dtype=float32),
array([[4.8954314e-07]], dtype=float32),
array([[9.550116e-08]], dtype=float32),
array([[1.305056e-07]], dtype=float32),
array([[4.187218e-09]], dtype=float32),
array([[3.8443446e-08]], dtype=float32),
array([[5.9630083e-09]], dtype=float32),
array([[1.1699244e-06]], dtype=float32),

我知道保存模型没有问题，因为当我在原始输入上测试重新加载的模型与原始模型时，它们具有相同的输出。
对于给定的输入（两次都是我的一帧），大多数结果应该远高于 0.5。我不明白到底出了什么问题。顺便说一句，这段代码主要来自 YT 教程：https://www.youtube.com/watch?v=FNHLVRJ1HU4&amp;list=PLgNJO2hghbmhHuhURAGbe6KWpiYZt0AMH&amp;index=8
如果能就此事提供任何帮助，我将不胜感激，因为我不明白哪里出了问题。谢谢]]></description>
      <guid>https://stackoverflow.com/questions/79196054/why-does-my-siamese-model-not-work-on-verification-data</guid>
      <pubDate>Sat, 16 Nov 2024 21:07:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 knn 算法的图形 ql 突变类型错误[关闭]</title>
      <link>https://stackoverflow.com/questions/79195596/type-error-on-graph-ql-mutation-using-knn-algortihm</link>
      <description><![CDATA[require(&#39;@tensorflow/tfjs-node&#39;); //在笔记本电脑 CPU 上运行 tfjs
const tf = require(&#39;@tensorflow/tfjs&#39;); //在 Tensorflow JS lib 中需要
const Users = require(&#39;../../mongodb/models/users&#39;)
const Posts = require(&#39;../../mongodb/models/posts&#39;)

function knn(features, labels, predictionPoint, k) { 
features = tf.tensor(features);
la​​bels = tf.tensor(labels);
const {mean, variance} = tf.moments(features, 0);
const scaledPrediction = predictionPoint.sub(mean).div(variance.pow(0.5)) //用于标准化的缩放预测 const
const apple = features.sub(mean)
/* 步骤 0 - 特征标准化 */
.div(variance.pow(0.5))
/* 步骤 1 - 查找特征之间的距离 &amp;预测点特征 */
.sub(scaledPrediction) //广播操作
.pow(2) //逐元素运算，对每个元素求平方
.sum(1) //沿 x(1) 轴求和
.sqrt() //逐元素运算，对每个元素取 .5 = sqrt 的幂
/* 第 2 步 - 从最小距离到最大距离排序 */
.expandDims(1) //我们在 x 轴上扩展距离张量的维度，以获得 [4,1] 的形状，与标签距离相同
.concat(labels, 1) //我们将标签连接到 x 轴上的距离，以便它们通过一个张量中的相同索引链接
.unstack() //我们将 1 个张量拆分为 1 个包含多个张量的 Vanilla JS 数组
/* 拆分张量后，我们正在处理 vanilla JS 数组，从现在开始我们只能使用 vanilla JS 方法*/
.sort((a,b) =&gt; a.get(0) &gt; b.get(0) ? 1 : -1) //按距离从小到大的顺序对函数 tp sprt 张量进行排序
/* 步骤 3 - 平均前 k 条记录的标签值 */ 
.slice(0, k)//获取前 k 条记录

return apple
}
module.exports = knn

这是我从后端 graphql 服务器发送请求的 knn 函数。
const prediction = async(args) =&gt;{
let count = []
let countt = []
const users = await Users.find({email: {$ne:`${args.email}`}}).exec();
users.map((user)=&gt;{
let name = user.username
user.likes[0].map((genre)=&gt;{
let pusht = 
{ [name]:[`${genre.adventure}`,
`${genre.action}`,
`${genre.comedy}`,
`${genre.drama}`,
`${genre.fantasy}`,
`${genre.horror}`,
`${genre.romance}`,
`${genre.sciencefiction}`,
`${genre.thriller}`,
`${genre.mystery}`,
`${genre.documentary}`,
`${genre.western}`,
`${genre.musical}`,
`${genre.anime}`,
`${genre.educational}`,
]}
count.push(pusht)
console.log(count)
})
})
const userss = await Users.find({email:args.email}).exec()
userss.map((user)=&gt;{
user.likes[0].map((genre)=&gt;{
let pushtt = 
{ [args.email]:[`${genre.adventure}`,
`${genre.action}`,
`${genre.comedy}`,
`${genre.drama}`,
`${genre.fantasy}`,
`${genre.horror}`,
`${genre.romance}`,
`${genre.sciencefiction}`,
`${genre.thriller}`,
`${genre.mystery}`,
`${genre.documentary}`,
`${genre.western}`,
`${genre.musical}`,
`${genre.anime}`,
`${genre.educational}`,
]}
​​countt.push(pushtt)
console.log(countt)
})
})
try{
count.forEach((feature, i) =&gt; {
const result = knn(tf.tensor(Object.values(feature)), tf.tensor(Object.keys(feature)), tf.tensor(Object.values(countt)), 10);
console.log(&#39;User Prediction&#39;, result); //注销用户预测
return {peopleyoumaylike: result}
})
}
catch(e){
console.log(e)
}

}

以上是我正在发送的 gql 请求。下面是我收到的 gql 错误。
{
&quot;errors&quot;: [
{
&quot;message&quot;: &quot;PeopleYouMayLike.peopleyoumaylike 的类型必须是输出类型，但得到的是：[function GraphQLObjectType]。&quot;
},
{
&quot;message&quot;: &quot;Expected GraphQL named type but got: [function GraphQLObjectType]。&quot;
}
]
}

我不知道如何处理和解决该问题。我不知道为什么会导致类型错误。控制台日志未显示在终端上。所以我看不到结果。我该怎么办。我在开发项目时错误地实现了 docker。所以我认为 vs code javascript 调试器不起作用。请帮忙。提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/79195596/type-error-on-graph-ql-mutation-using-knn-algortihm</guid>
      <pubDate>Sat, 16 Nov 2024 16:37:01 GMT</pubDate>
    </item>
    <item>
      <title>如何在 google colab 中使用从 kaggle 加载的数据（实际使用它）</title>
      <link>https://stackoverflow.com/questions/79195592/how-to-use-loaded-data-from-kaggle-in-google-colab-to-actually-work-with-it</link>
      <description><![CDATA[因此，我最近从此 https://www.kaggle.com/datasets/mostafaabla/garbage-classification 网站导入了数据集。尽管我在 google colab 中的文件中有它（已解压和所有这些东西），但我不知道如何在代码本身中实现它。就像来自 tensorflow 的 Fashion mnist 教程 https://www.tensorflow.org/tutorials/keras/classification?hl 它加载为
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
如何将数据导入/加载到代码单元并通过分成类来处理它（因为在该教程数据集中有多个类，而在我的自定义数据集中有 12 个）
请问如何操作？
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# 定义训练和验证目录的路径
train_dir = &#39;garbage-classification/train&#39;
val_dir = &#39;garbage-classification/validation&#39;

# 创建 ImageDataGenerator 进行数据增强
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

# 从目录加载图像
train_generator = train_datagen.flow_from_directory(
train_dir,
target_size=(150, 150), # 根据需要调整图像大小
batch_size=32,
class_mode=&#39;categorical&#39; # 如果有多个类，请使用 &#39;categorical&#39;
)

validation_generator = val_datagen.flow_from_directory(
val_dir,
target_size=(150, 150),
batch_size=32,
class_mode=&#39;categorical&#39;
)

我使用 perplexity 尝试解决，结果得到了这个。显然它没有起作用，所以..]]></description>
      <guid>https://stackoverflow.com/questions/79195592/how-to-use-loaded-data-from-kaggle-in-google-colab-to-actually-work-with-it</guid>
      <pubDate>Sat, 16 Nov 2024 16:34:39 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow/Keras 模型的 SHAP force_plot 中出现 InvalidArgumentError：切片索引超出范围</title>
      <link>https://stackoverflow.com/questions/79195478/invalidargumenterror-in-shap-force-plot-for-tensorflow-keras-model-slice-index</link>
      <description><![CDATA[我正在使用 TensorFlow/Keras 二元分类模型并使用 SHAP 来解释单个预测。但是，当我尝试生成力图时，我遇到了以下错误：
# 导入 SHAP
import shap

# 确保 data_for_prediction 具有正确的形状
data_for_prediction_reshaped = data_for_prediction.reshape(1, -1)

# 为 DeepExplainer 提供背景数据
background = X_train[:100] # 使用来自训练数据的 100 个样本作为背景

# 初始化 DeepExplainer
explainer = shap.DeepExplainer(model, background)

# 计算 SHAP 值
shap_values = explainer.shap_values(data_for_prediction_reshaped)

# 生成力图
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction_reshaped)


错误：
InvalidArgumentError：{{function_node _wrapped__StridedSlice_device/job:localhost/replica:0/task:0/device:CPU:0}} 切片索引 1 的维度 0 超出范围。[Op:StridedSlice] 名称：strided_slice/
其他详细信息：
1. 该模型是具有以下架构的 Keras Sequential 模型：
• 具有 ReLU 激活的多个密集层。
• 每个密集层后都有一个 Dropout 层。
• 具有用于二元分类的 S 形激活的输出层。
2. 背景数据：
• X_train[:100] 是我预处理的训练数据（NumPy 数组）的一部分。
3. 预测输入：
• data_for_prediction_reshaped 是重塑为 (1, n_features) 的单个样本。
4. 形状：
• shap_values[1].shape：SHAP 值的输出形状（针对第 1 类）。
• data_for_prediction_reshaped.shape：重塑为 (1, n_features) 的输入特征。
问题：
1. 在此上下文中，“维度 0 的切片索引 1 超出范围”错误是什么意思？
2. 我应该如何调整代码以确保 shap.force_plot 能够与 SHAP 和 TensorFlow/Keras 模型一起正常工作？
3. 对于此用例，我应该注意 SHAP 和 TensorFlow/Keras 之间是否存在特定的兼容性问题？
如能提供任何指导，我们将不胜感激。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/79195478/invalidargumenterror-in-shap-force-plot-for-tensorflow-keras-model-slice-index</guid>
      <pubDate>Sat, 16 Nov 2024 15:38:25 GMT</pubDate>
    </item>
    <item>
      <title>视觉变换器的计算复杂度[关闭]</title>
      <link>https://stackoverflow.com/questions/79195302/computation-complexity-of-vision-transformer</link>
      <description><![CDATA[我正在寻求有关 Vision Transformers (ViTs) 计算复杂度的澄清，特别是关于多头自注意力 (MSA)、MLP 块、LayerNorm (LN) 和每个块后应用的残差连接等组件。尽管查阅了大量研究论文和资源，但我发现很难掌握这些组件的计算复杂度是如何得出和计算的清晰而简单的解释。有人能提供更详细、更易于理解的解释吗？提前感谢您的帮助。
我试图理解如何计算 ViT 的计算复杂度，但我对计算它感到困惑。我期待解释 ViT 计算复杂度部分从补丁嵌入到 MLP 块。]]></description>
      <guid>https://stackoverflow.com/questions/79195302/computation-complexity-of-vision-transformer</guid>
      <pubDate>Sat, 16 Nov 2024 14:07:32 GMT</pubDate>
    </item>
    <item>
      <title>BERTopic partial_fit 占位符集群表示</title>
      <link>https://stackoverflow.com/questions/79195169/bertopic-partial-fit-placeholder-cluster-representation</link>
      <description><![CDATA[我有大约 2M 个文本文档，每个文档都很小，我想对它们进行聚类。（最终将增长到大约 500M。）虽然我愿意接受建议，但我目前正在使用 Python3 包 BERTopic 的在线技术。在对 140 个 15k 个文档块使用 partial_fit 后，我只剩下对 topic_model.get_topic_info() 的调用，它返回未完成的聚类表示。也就是说，我看到聚类名称 0____ 的表示为 [,,,,,,,,]。我看到大多数聚类都是这样的。我从 Google Gemini 得到的建议是对我的所有文档调用 topic_model.fit(all_documents)，这些文档目前在磁盘上压缩后只有 86 GB，对于 RAM 来说太多了。我该如何填写这些聚类的表示？]]></description>
      <guid>https://stackoverflow.com/questions/79195169/bertopic-partial-fit-placeholder-cluster-representation</guid>
      <pubDate>Sat, 16 Nov 2024 12:50:38 GMT</pubDate>
    </item>
    <item>
      <title>小数据集的音频微调配置</title>
      <link>https://stackoverflow.com/questions/79195104/audio-fine-tuning-configuration-for-small-data-set</link>
      <description><![CDATA[我是数据训练方面的新手，尤其是在微调方面。我想尝试使用 vits 对音频数据进行微调，数据集小于 100 个音频文件，每个音频文件小于 10 秒，问题就在这里，我已经尝试了几种情况，例如调整

Epoch
Batch Size
Learning Rate
Betas
Warm up Epochs
Mel Processing data

但不知何故它仍然没有给出我想要的结果。我读了文档，它说给出大约 600-1000 个 epoch 可以得到很好的结果，但就我而言，情况仍然不是这样。我尝试了大约 4 天来训练几种情况：
第一次：

注释：这是我第一次进行微调项目
批次大小：16
时期：200
时间：~20 分钟
结果：我听到了很多像机器一样的声音，有些声音捕捉正确，但如果不集中注意力，声音太小而无法注意到

第二次：

注释：我读了几篇文章，似乎较小的批次可以为小数据量提供更紧密的结果，至于时期，我需要确保不要过度拟合，所以我尝试在这里实现它
批次大小：8
时期： 300
时间：~30 分钟
结果：使用这种方法，我开始听到一些声音，尽管它仍然有很多类似机器的模式，但开始在这个案例上有所启发

第三：

注释：根据第二种情况的结果，我认为增加 epoch 可以得到更好的结果，因为范围很广，所以在这种情况下，我尝试遵循推荐的配置（16 批次 &amp; 10000 个 epoch），但尝试使 epoch 更小
批次大小：8
epoch：3000
时间：~8-9 小时
结果：不知何故，在这个结果中，它开始听起来不像机器那样，大约 25%，所以如果我想添加 epoch，也许增加批次大小会有所帮助

第四次：

注意：基于最后的情况，我尝试增加批次大小，因为我的规格不是那么糟糕（我将在本节下方提供其他信息）
批次大小：16
epoch：4000
时间：~7-8 小时
结果：在这种情况下，不知何故它让声音变得非常奇怪，就像减少了性能

基于此，我想问一下，如何正确计算训练的值以获得至少不错的结果？我对第一次微调有点困惑
至于我的PC 规格，这里是：

NVidia RTX 3060 12 GB DDR6 配备 64 GB RAM &amp;第 12 代英特尔 I7-12700F

我曾尝试从 GPT 获取我的设置的最佳配置，但不知何故结果仍然相同，它给出了以下参数：
 &quot;train&quot;: {
&quot;log_interval&quot;: 50,
&quot;eval_interval&quot;: 200,
&quot;seed&quot;: 1234,
&quot;epochs&quot;: 1000,
&quot;learning_rate&quot;: 1e-4,
&quot;betas&quot;: [0.9, 0.98],
&quot;eps&quot;: 1e-9,
&quot;batch_size&quot;: 4,
&quot;fp16_run&quot;: true,
&quot;lr_decay&quot;: 0.9999,
&quot;segment_size&quot;: 8192,
&quot;init_lr_ratio&quot;: 1,
&quot;warmup_epochs&quot;: 5,
&quot;c_mel&quot;: 30,
&quot;c_kl&quot;: 1.0
}

截至存储库默认配置，给出的内容如下：
 &quot;train&quot;: {
&quot;log_interval&quot;: 200,
&quot;eval_interval&quot;: 1000,
&quot;seed&quot;: 1234,
&quot;epochs&quot;: 10000,
&quot;learning_rate&quot;: 2e-5,
&quot;betas&quot;: [0.8, 0.99],
&quot;eps&quot;: 1e-9,
&quot;batch_size&quot;: 16,
&quot;fp16_run&quot;: true,
&quot;lr_decay&quot;: 0.999875,
&quot;segment_size&quot;: 8192,
&quot;init_lr_ratio&quot;: 1,
&quot;warmup_epochs&quot;: 0,
&quot;c_mel&quot;: 45,
&quot;c_kl&quot;: 1.0
}

所以这里有一些我想问的问题：
有没有针对 100 个数据集以下音频文件的微调配置建议？最大批次大小是否应始终低于样本数据总量？]]></description>
      <guid>https://stackoverflow.com/questions/79195104/audio-fine-tuning-configuration-for-small-data-set</guid>
      <pubDate>Sat, 16 Nov 2024 12:03:58 GMT</pubDate>
    </item>
    <item>
      <title>如何使用神经网络找到下图中所有交点的坐标？</title>
      <link>https://stackoverflow.com/questions/79194488/how-can-i-find-coordinate-of-all-intersection-points-in-the-following-image-with</link>
      <description><![CDATA[我想使用神经网络找到下图中的所有交点。有人知道我该如何实现吗？

目前，我使用 openCV 阈值或边缘检测，但在某些情况下效果不佳。所以，我想使用神经网络来做到这一点，但我就是不知道如何使用神经网络来做到这一点。]]></description>
      <guid>https://stackoverflow.com/questions/79194488/how-can-i-find-coordinate-of-all-intersection-points-in-the-following-image-with</guid>
      <pubDate>Sat, 16 Nov 2024 03:55:18 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost 提前停止轮次</title>
      <link>https://stackoverflow.com/questions/79189607/xgboost-early-stopping-rounds</link>
      <description><![CDATA[下面的代码一直在崩溃，我不知道发生了什么
import optuna
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 假设 `X` 和 `y` 是你的特征矩阵和目标数组
X_train, X_valid, y_train, y_valid = train_test_split(df_combined, y, test_size=0.2, random_state=42)

# 为 Optuna 定义目标函数
def objective(trial):
# 为超参数建议值
params = {
&quot;objective&quot;: &quot;reg:squarederror&quot;,
&quot;eval_metric&quot;: &quot;rmse&quot;,
&quot;tree_method&quot;: &quot;hist&quot;, # 使用 hist 方法
&quot;device&quot;: &quot;cuda&quot;, # 指定使用 GPU
&quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 0.01, 0.3, log=True),
&quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 3, 10),
&quot;min_child_weight&quot;: trial.suggest_float(&quot;min_child_weight&quot;, 1, 10),
&quot;gamma&quot;: trial.suggest_float(&quot;gamma&quot;, 0, 1),
&quot;subsample&quot;: trial.suggest_float(&quot;subsample&quot;, 0.5, 1.0),
&quot;colsample_bytree&quot;: trial.suggest_float(&quot;colsample_bytree&quot;, 0.5, 1.0),
&quot;lambda&quot;: trial.suggest_float(&quot;lambda&quot;, 1e-3, 10.0, log=True),
&quot;alpha&quot;: trial.suggest_float(&quot;alpha&quot;, 1e-3, 10.0, log=True),
&quot;n_estimators&quot;: 1000 # 在模型初始化中定义 n_estimators
}

# 初始化模型
model = xgb.XGBRegressor(**params)

# 使用早期停止回调训练模型
model.fit(
X_train,
y_train,
eval_set=[(X_valid, y_valid)],
verbose=False,
early_stopping_rounds=50 # 如果之后没有改进则停止50 轮
)

# 预测并计算验证集的 RMSE
preds = model.predict(X_valid)
rmse = mean_squared_error(y_valid, preds, squared=False)

return rmse # Optuna 将其最小化

# 设置 Optuna 研究
study = optuna.create_study(direction=&quot;minimize&quot;)

# 优化超参数
study.optimize(objective, n_trials=100, n_jobs=40) # 100 次试验，40 次并行作业

# 显示最佳试验
print(&quot;最佳试验：&quot;)
trial = study.best_trial
print(f&quot;值 (RMSE)：{trial.value}&quot;)
print(&quot; 参数：&quot;)
for key, value in trial.params.items():
print(f&quot; {key}: {value}&quot;)

我得到
TypeError：XGBModel.fit() 获得意外的关键字参数“early_stopping_rounds”

我已更新所有内容以确保我拥有所有更新的库。
提前停止轮次是正确的（我认为），但由于某种原因，它会爆炸。]]></description>
      <guid>https://stackoverflow.com/questions/79189607/xgboost-early-stopping-rounds</guid>
      <pubDate>Thu, 14 Nov 2024 16:14:40 GMT</pubDate>
    </item>
    <item>
      <title>无法训练我的 UNET 多类别细分模型</title>
      <link>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</link>
      <description><![CDATA[我尝试使用 pytorch 从头开始​​制作 UNET。我的模型输出只有黑色蒙版。我需要分割汽车上的损坏，所以我实现了一个彩色图。我确信 70% 的数据集有问题，而这个彩色图恰恰就是其中的原因。任务是多类预测，所以我使用交叉熵损失函数。我将提供我的数据集和训练文件的代码。
# dataset.py
import os
from PIL import Image
from torch.utils.data import Dataset
import numpy as np
import torch

class Segm_Dataset(Dataset):
def __init__(self, image_dir, mask_dir, color_map):
self.image_dir = image_dir
self.mask_dir = mask_dir
self.image_files = os.listdir(self.image_dir)
self.mask_files = os.listdir(self.mask_dir)
self.color_map = color_map

def __len__(self):
return len(self.image_files)

def __getitem__(self, idx):
image_path = os.path.join(self.image_dir, self.image_files[idx])
mask_path = os.path.join(self.mask_dir, self.mask_files[idx])
image = np.array(Image.open(image_path).convert(&#39;RGB&#39;))
mask = np.array(Image.open(mask_path).convert(&#39;RGB&#39;), dtype=np.float32)
label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int64)

for color, label in self.color_map.items():
color_array = np.array(color, dtype=np.float32)
mask_area = np.all(mask == color_array, axis=-1)
label_mask[mask_area] = label

image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)
label_mask = torch.tensor(label_mask, dtype=torch.long)

返回图像，label_mask

# train.py
从模型导入 UNET
从 tqdm 导入 tqdm
从数据集导入 Segm_Dataset
导入 torch
从 torch.utils.data 导入 DataLoader
导入 torch.nn 作为 nn
导入 torch.optim 作为 optim
导入 os

LEARNING_RATE = 1e-4
BATCH_SIZE = 5
NUM_EPOCHS = 10
NUM_WORKERS = 2
IMAGE_HEIGHT = 180
IMAGE_WIDTH = 180
PIN_MEMORY = True
LOAD_MODEL =错误
TRAIN_IMG_DIR = r&#39;data\train\images&#39;
TRAIN_MASK_DIR = r&#39;data\train\masks&#39;
VAL_IMG_DIR = r&#39;data\val\images&#39;
VAL_MASK_DIR = r&#39;data\val\masks&#39;
SAVED_MODELS_PATH = r&#39;saved_models&#39;

color_map = {
(19, 164, 201): 0, # 缺失部分：#13A4C9
(166, 255, 71): 1, # 破损部分：#A6FF47
(180, 45, 56): 2, # 划痕：#B42D38
(225, 150, 96): 3, # 破裂：#E19660
(144, 60, 89): 4, # 凹痕： #903C59
(167, 116, 27): 5, # 剥落: #A7741B
(180, 14, 19): 6, # 油漆剥落: #B40E13
(115, 194, 206): 7, # 腐蚀: #73C2CE
}

train_dataset = Segm_Dataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, color_map)
train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)

val_dataset = Segm_Dataset(VAL_IMG_DIR, VAL_MASK_DIR, color_map)
val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)

model = UNET(in_channels=3, out_channels=len(color_map))
model = model.cuda() if torch.cuda.is_available() else model

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

for epoch in range(NUM_EPOCHS):
train_loop = tqdm(enumerate(train_loader), total=len(train_loader))

for batch_index, (data, target) in train_loop: 
#前向传递
scores = model(data)
train_loss = criterion(scores, target)

#后向传递
optimizer.zero_grad()
train_loss.backward()

#梯度下降或优化器步骤
optimizer.step()

if batch_index % 10 == 0:
current_batch = batch_index
val_loss = 0
with torch.no_grad():
for val_data, val_targets in val_loader:
val_scores = model(val_data)
val_loss = criterion(val_scores, val_targets)

#更新进度条
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

else:
train_loop.set_description(f&#39;Epoch: [{epoch+1}/{NUM_EPOCHS}]&#39;)
train_loop.set_postfix(train_loss=train_loss.item(), val_loss=val_loss.item(), val_batch=current_batch)

checkpoint = {
&#39;epoch&#39;: epoch + 1,
&#39;model_state_dict&#39;: model.state_dict(),
&#39;optimizer_state_dict&#39;: optimizer.state_dict(),
&#39;train_loss&#39;: train_loss.item(),
&#39;val_loss&#39;: val_loss.item()
}

torch.save(checkpoint, os.path.join(SAVED_MODELS_PATH, f&#39;unet_epoch_{epoch}.pth&#39;))

一些训练 epoches:
Epoch: [9/10]: 100%|████████████████| 888/888 [34:24&lt;00:00, 2.32s/it, train_loss=0.000271, val_batch=880, val_loss=0.000278]

Epoch：[10/10]：100%|███████████████| 888/888 [34:29&lt;00:00, 2.33s/it, train_loss=0.000163, val_batch=880, val_loss=0.000167]
]]></description>
      <guid>https://stackoverflow.com/questions/79188029/cant-train-my-unet-multiclass-segmentation-model</guid>
      <pubDate>Thu, 14 Nov 2024 09:17:27 GMT</pubDate>
    </item>
    <item>
      <title>我们可以使用 FastAPI 在 model.predict() 中直接使用 Pydantic 模型（BaseModel）吗？如果不行，为什么？</title>
      <link>https://stackoverflow.com/questions/71849683/can-we-use-pydantic-models-basemodel-directly-inside-model-predict-using-fas</link>
      <description><![CDATA[我正在使用带有 FastAPI 的 Pydantic 模型 (Basemodel)，并将输入转换为 dictionary，然后将其转换为 Pandas DataFrame，以便将其传递到 model.predict() 函数中进行机器学习预测，如下所示：
from fastapi import FastAPI
import uvicorn
from pydantic import BaseModel
import pandas as pd
from typing import List

class Inputs(BaseModel):
f1: float,
f2: float,
f3: str

@app.post(&#39;/predict&#39;)
def predict(features: List[Inputs]):
output = []

# 循环输入特征列表
for data in features:
result = {}

# 将数据转换为 dict()，然后转换为 DataFrame
data = data.dict()
df = pd.DataFrame([data])

# 获取预测
prediction = classifier.predict(df)[0]

# 获取概率
probability = classifier.predict_proba(df).max()

# 分配给字典 
result[&quot;prediction&quot;] = prediction
result[&quot;probability&quot;] = probability

# 将字典附加到列表（许多输出）
output.append(result)

返回输出

它运行良好，只是我不太确定它是否优化或是否是正确的方法，因为我将输入转换两次以获得预测。此外，我不确定在输入数量巨大的情况下它是否会快速地工作。对此有什么改进吗？如果有办法（甚至除了使用 Pydantic 模型之外），我可以直接工作并避免经过转换和循环。]]></description>
      <guid>https://stackoverflow.com/questions/71849683/can-we-use-pydantic-models-basemodel-directly-inside-model-predict-using-fas</guid>
      <pubDate>Tue, 12 Apr 2022 22:11:19 GMT</pubDate>
    </item>
    <item>
      <title>如何在二维图上可视化鸢尾花数据集的不同特征组合</title>
      <link>https://stackoverflow.com/questions/64068419/how-to-visualize-the-iris-dataset-on-2d-plots-for-different-combinations-of-feat</link>
      <description><![CDATA[我想用所有六种组合（萼片宽度-萼片长度）、（花瓣宽度-萼片长度）、（萼片长度-花瓣宽度）、（花瓣长度-花瓣宽度）（花瓣长度-萼片宽度）（萼片宽度-花瓣长度）在二维中可视化鸢尾花数据集，基本上这就是我目前得到的结果：
import matplotlib
matplotlib.rcParams[&#39;figure.figsize&#39;] = (9.0, 7.0)

data = load_iris()

pairs = [(i, j) for i in range(4) for j in range(i+1, 4)]

fig, subfigs = pyplot.subplots(2, 3, tight_layout=True)
t1 = time.time()

for (f1, f2), subfig in zip(pairs, subfigs.reshape(-1)):

根据说明，我们必须根据此对生成二维图，每次列出两个度量，以 f1 和 f2 作为度量，并创建类指标和 legend() 以更好地可视化图形，我尝试了不同的散点图，但似乎都不起作用。]]></description>
      <guid>https://stackoverflow.com/questions/64068419/how-to-visualize-the-iris-dataset-on-2d-plots-for-different-combinations-of-feat</guid>
      <pubDate>Fri, 25 Sep 2020 17:00:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么训练时我的 Keras 模型的准确率始终为 0？</title>
      <link>https://stackoverflow.com/questions/45632549/why-is-the-accuracy-for-my-keras-model-always-0-when-training</link>
      <description><![CDATA[我构建了一个简单的 Keras 网络：
import numpy as np;

from keras.models import Sequential;
from keras.layers import Dense,Activation;

data= np.genfromtxt(&quot;./kerastests/mydata.csv&quot;, delimiter=&#39;;&#39;)
x_target=data[:,29]
x_training=np.delete(data,6,axis=1)
x_training=np.delete(x_training,28,axis=1)

model=Sequential()
model.add(Dense(20,activation=&#39;relu&#39;, input_dim=x_training.shape[1]))
model.add(Dense(10,activation=&#39;relu&#39;))
model.add(Dense(1));

model.compile(optimizer=&#39;adam&#39;,loss=&#39;mean_squared_error&#39;,metrics=[&#39;accuracy&#39;])
model.fit(x_training, x_target)

如您所见，从我的源数据中，我删除了 2 列。一列是带有字符串格式日期的列（在数据集中，除此之外，我还有表示日期的列、表示月份的列和表示年份的列，因此我不需要该列），另一列是我用作模型目标的列）。
当我训练这个模型时，我得到了这个输出：
32/816 [&gt;.............................] - ETA：23s - loss：13541942.0000 - acc：0.0000e+00
800/816 [===========================&gt;.] - ETA：0s - loss：11575466.0400 - acc：0.0000e+00 
816/816 [================================] - 1s - 损失：11536905.2353 - 精度：0.0000e+00 
纪元 2/10
32/816 [&gt;.............................] - ETA：0s - 损失：6794785.0000 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5381360.4314 - 精度：0.0000e+00 
纪元 3/10
32/816 [&gt;.............................] - ETA：0s - 损失： 6235184.0000 - 精度：0.0000e+00
800/816 [============================&gt;.] - ETA：0s - 损失：5199512.8700 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5192977.4216 - 精度：0.0000e+00 
纪元 4/10
32/816 [&gt;.............................] - ETA：0s - 损失：4680165.5000 - 精度： 0.0000e+00
736/816 [===========================&gt;...] - ETA：0s - 损失：5050110.3043 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5168771.5490 - 精度：0.0000e+00 
纪元 5/10
32/816 [&gt;.............................] - ETA：0s - 损失：5932391.0000 - 精度：0.0000e+00
768/816 [============================&gt;..] - ETA：0 秒 - 损失：5198882.9167 - 精度：0.0000e+00
816/816 [==============================] - 0 秒 - 损失：5159585.9020 - 精度：0.0000e+00 
纪元 6/10
32/816 [&gt;.............................] - ETA：0 秒 - 损失：4488318.0000 - 精度：0.0000e+00
768/816 [============================&gt;..] - ETA：0s - 损失：5144843.8333 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5151492.1765 - 精度：0.0000e+00 
纪元 7/10
32/816 [&gt;.............................] - ETA：0s - 损失：6920405.0000 - 精度：0.0000e+00
800/816 [=============================&gt;.] - ETA：0s - 损失：5139358.5000 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5169839.2941 - 精度：0.0000e+00 
纪元 8/10
32/816 [&gt;.............................] - ETA：0s - 损失：3973038.7500 - 精度：0.0000e+00
672/816 [==========================&gt;......] - ETA：0s - 损失：5183285.3690 - 精度：0.0000e+00
816/816 [==============================] - 0s - 损失：5141417.0000 - 精度：0.0000e+00 
Epoch 9/10
32/816 [&gt;.............................] - ETA：0s - 损失：4969548.5000 - 精度：0.0000e+00
768/816 [===========================&gt;..] - ETA：0s - 损失：5126550.1667 - 精度： 0.0000e+00
816/816 [===============================] - 0s - 损失：5136524.5098 - 精度：0.0000e+00 
纪元 10/10
32/816 [&gt;.............................] - ETA：0s - 损失：6334703.5000 - 精度：0.0000e+00
768/816 [===========================&gt;..] - ETA：0s - 损失：5197778.8229 - 精度：0.0000e+00
816/816 [===============================] - 0s - 损失：5141391.2059 - 准确率：0.0000e+00 

为什么会发生这种情况？我的数据是时间序列。我知道对于时间序列，人们通常不使用 Dense 神经元，但这只是一个测试。真正让我困惑的是准确率始终为 0。而且，在其他测试中，我甚至输了：得到一个“NAN”值。
有人能帮忙吗？]]></description>
      <guid>https://stackoverflow.com/questions/45632549/why-is-the-accuracy-for-my-keras-model-always-0-when-training</guid>
      <pubDate>Fri, 11 Aug 2017 10:08:03 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Weka 中使用 MFCC 进行音频分类？</title>
      <link>https://stackoverflow.com/questions/45224049/how-to-use-mfccs-in-weka-for-audio-classification</link>
      <description><![CDATA[我正在尝试开发一种使用 Weka 中的 MFCC 对音频进行分类的方法。我拥有的 MFCC 是用 1024 的缓冲区大小生成的，因此每个音频记录都有一系列 MFCC 系数。我想将这些系数转换为 Weka 的 ARFF 数据格式，但我不确定如何解决这个问题。
我还问了一个关于合并数据的问题，因为我觉得这可能会影响数据转换为 ARFF 格式。
我知道对于 ARFF，数据需要通过属性列出。MFCC 的每个系数应该是单独的属性还是作为单个属性的系数数组？每个数据应该代表单个 MFCC、时间窗口还是整个文件或声音？下面，我写出了我认为如果只考虑一个 MFCC 应该是什么样子，我认为这无法对整个声音进行分类。
@relation audio

@attribute mfcc1 real
@attribute mfcc2 real
@attribute mfcc3 real
@attribute mfcc4 real
@attribute mfcc5 real
@attribute mfcc6 real
@attribute mfcc7 real
@attribute mfcc8 real
@attribute mfcc9 real
@attribute mfcc10 real
@attribute mfcc11 real
@attribute mfcc12 real
@attribute mfcc13 real
@attribute class {bark, honk, talking, wind}

@data
126.347275, -9.709645, 4.2038302, -11.606304, -2.4174862, -3.703139, 12.748064, -5.297932, -1.3114156, 2.1852574, -2.1628475, -3.622149, 5.851326, bark

如能提供任何帮助，我们将不胜感激。
编辑：
我已生成一些 ARFF 文件使用 Weka 使用 openSMILE 按照此 网站中的方法，但我不确定如何使用这些数据对音频进行分类，因为每行数据都是来自同一文件的 10 毫秒音频。每行的名称属性都是“未知”，我认为这是数据将尝试分类的属性。我如何才能对整体声音（而不是 10 毫秒）进行分类并将其与其他几个整体声音进行比较？

编辑 #2：成功！
在更彻底地阅读我找到的网站后，我看到了 Accumulate 脚本以及测试和训练数据文件。accumulate 脚本将来自不同音频文件的每个 MFCC 数据集合生成的所有文件放在一个 ARFF 文件中。他们的文件由大约 200 个属性组成，其中包含 12 个 MFCC 的统计数据。虽然我无法使用 OpenSmile 检索这些统计数据，但我使用了 Python 库来执行此操作。统计数据包括最大值、最小值、峰度、范围、标准差等。我使用 Weka 中的 BayesNet 和多层感知器准确地对我的音频文件进行了分类，这两项方法都为我带来了 100% 的准确率。]]></description>
      <guid>https://stackoverflow.com/questions/45224049/how-to-use-mfccs-in-weka-for-audio-classification</guid>
      <pubDate>Thu, 20 Jul 2017 19:52:54 GMT</pubDate>
    </item>
    </channel>
</rss>