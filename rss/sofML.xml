<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Wed, 14 Aug 2024 15:16:17 GMT</lastBuildDate>
    <item>
      <title>批次不同部分多重损失的最佳实践</title>
      <link>https://stackoverflow.com/questions/78871584/best-practice-for-multiple-losses-on-different-parts-of-batch</link>
      <description><![CDATA[我有一个想要训练的姿势估计模型。一些实例有 3d 地面实况数据，一些有 2d 地面实况数据。两种类型的预测模型相同，只有损失不同。它们由不同的数据加载器加载。
我希望两者都在同一个批次中，实现此目的的（软件工程）最佳实践是什么。
将它们分别传递给模型，因此两个模型前向和后向调用，然后在累积梯度上运行优化器。
合并两个批次（跟踪每个批次中有多少个样本），将它们输入模型，然后在将其通过损失函数之前将结果分开。
使用虚拟地面实况计算两个批次的两个损失，并乘以 1 或 0 以仅将有效项目包含在总体损失中。

哪种方法最容易理解，有没有什么方法可以最好地解决这个问题？
交叉发布：https://discuss.pytorch.org/t/best-practice-for-multiple-losses-on-different-parts-of-batch/208071]]></description>
      <guid>https://stackoverflow.com/questions/78871584/best-practice-for-multiple-losses-on-different-parts-of-batch</guid>
      <pubDate>Wed, 14 Aug 2024 14:45:23 GMT</pubDate>
    </item>
    <item>
      <title>在梯度带中使用时，训练 =False 不会更新模型参数吗？[重复]</title>
      <link>https://stackoverflow.com/questions/78870866/does-the-training-false-not-update-model-parameters-while-being-used-inside-of</link>
      <description><![CDATA[def train_step(self, batch):
# 获取数据 
real_images = batch
fake_images = self.generator(tf.random.normal((128, 128, 1)), training=False)

# 训练鉴别器
with tf.GradientTape() as d_tape: 

yhat_real = self.discriminator(real_images, training=True) 
yhat_fake = self.discriminator(fake_images, training=True)
yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)

y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)

# 为 TRUE 输出添加一些噪声
noise_real = 0.15*tf.random.uniform(tf.shape(yhat_real))
* noise_fake = -0.15*tf.random.uniform(tf.shape(yhat_fake))
y_realfake += tf.concat([noise_real, noise_fake], axis=0)

total_d_loss = self.d_loss(y_realfake, yhat_realfake)

dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) 
self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))

使用 tf.GradientTape() 作为 g_tape: 

gen_images = self.generator(tf.random.normal((128,128,1)), training=True)

predicted_labels = self.discriminator(gen_images, training=False)

total_g_loss = self.g_loss(tf.zeros_like(predicted_labels),predicted_labels) 

ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)
self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))

return {&quot;d_loss&quot;:total_d_loss, &quot;g_loss&quot;:total_g_loss}

我正在构建一个 GAN。如上面的代码所示，在计算生成器的梯度时，鉴别器设置为 training =False。这是否意味着在计算梯度时，我们不想计算相对于鉴别器参数的损失梯度，也不想将梯度应用于鉴别器？]]></description>
      <guid>https://stackoverflow.com/questions/78870866/does-the-training-false-not-update-model-parameters-while-being-used-inside-of</guid>
      <pubDate>Wed, 14 Aug 2024 12:18:37 GMT</pubDate>
    </item>
    <item>
      <title>有没有人使用图像分类来自动检测网页是否正常显示？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78870242/is-there-anyone-use-image-classification-to-automatically-detect-if-a-web-page-i</link>
      <description><![CDATA[在自动化测试中，有些情况需要手动检查截图，判断页面在窗口大小改变时是否存在缺页、文字溢出等异常情况。
我想训练一个图像分类的模型，用于检查网页，但是我没有截断或重叠或页面丢失等的截图……也许有人有同样的想法或已经完成了这个模型？]]></description>
      <guid>https://stackoverflow.com/questions/78870242/is-there-anyone-use-image-classification-to-automatically-detect-if-a-web-page-i</guid>
      <pubDate>Wed, 14 Aug 2024 09:47:18 GMT</pubDate>
    </item>
    <item>
      <title>建模时我需要标准化 Y 变量吗？</title>
      <link>https://stackoverflow.com/questions/78870125/do-i-need-to-standardize-the-y-variable-when-modelling</link>
      <description><![CDATA[我正在构建机器学习回归模型。y 变量是每只股票所有月份的超额收益。
我对股票进行了 1% 和 99% 的缩尾处理。 y 变量的图：

y_trn.hist(bins=50)
训练样本图

y_vld.hist(bins=50)
有效样本图

y_tst.hist(bins=50)
测试样本图


因为现在我的模型不够稳健，测试样本的 r2 远大于训练样本的 r2。
这是我的另一个问题：
前馈网络回归预测几乎相同的值
我将所有 X 变量标准化为 [-1,1]，我需要标准化 y 变量吗？也许我不应该对 y 变量进行缩尾？]]></description>
      <guid>https://stackoverflow.com/questions/78870125/do-i-need-to-standardize-the-y-variable-when-modelling</guid>
      <pubDate>Wed, 14 Aug 2024 09:20:20 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 `transformers` 中的 `TFRobertaForSequenceClassification` 和 `tensorflow.keras.Model`</title>
      <link>https://stackoverflow.com/questions/78870106/how-to-use-tfrobertaforsequenceclassification-from-transformers-with-tensor</link>
      <description><![CDATA[我已经按照此处所示为 TFRobertaForSequenceClassification 定义了带有两个标签的自定义分类器头，以便能够针对我的下游任务对其进行微调，即将句子分类为来自一组有限的独立标签。
from transformers import TFRobertaForSequenceClassification
roberta_model = TFRobertaForSequenceClassification.from_pretrained(pretrained_model_path, from_pt=True, num_labels=2)

我想将 roberta_model 成为 tensorflow.keras.Model。以下是我现在使用 tensorflow.keras.layers.Identity 作为最终/输出层的结果，因为 roberta 的分类主管已经处理好了它。
import numpy as np
import tensorflow as tf

MAX_SEQUENCE_LENGTH = 256

# 定义输入 ID、注意掩码和输入类型 ID
input_word_ids = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name=&#39;input_word_ids&#39;)
input_mask = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name=&#39;input_mask&#39;)
input_type_ids = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name=&#39;input_type_ids&#39;)

# 启动预训练模型
roberta_model = TFRobertaForSequenceClassification.from_pretrained(pretrained_model_path, from_pt=True, num_labels=2)
x = roberta_model(input_ids=input_word_ids,tention_mask=input_mask, token_type_ids=input_type_ids, labels=np.array([0, 1]))

# `x` 具有损失并将 logits 作为键。因此，将损失和对数传递到前面。
# 添加任务所需的最终层
out = tf.keras.layers.Identity(activation=&#39;softmax&#39;)(x)

# 构建模型
model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], output=out)

# 编译模型
model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=tf.keras.optimizers.Adam(lr=1e-5), metrics=[&#39;accuracy&#39;, tf.keras.metrics.F1Score()])

我需要将模型放在 Keras 中，因为我有处理历史记录的下游代码tf.keras.Model.fit 返回。
我发现我现有方法的问题是，如源代码所示 (https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1210-L1232)，末尾已经附加了一个损失，因此，我添加了一个 softmax 损失，只是为了使其成为 tensorflow.keras.Model 并且有两个损失函数看起来不正确。此外，我在最终的身份层中不能有任何激活吗？
我在其他地方做错了什么吗？]]></description>
      <guid>https://stackoverflow.com/questions/78870106/how-to-use-tfrobertaforsequenceclassification-from-transformers-with-tensor</guid>
      <pubDate>Wed, 14 Aug 2024 09:18:14 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML 算法对图像进行分类时，如何修复“找到具有 dim 4 的数组”错误</title>
      <link>https://stackoverflow.com/questions/78869863/how-fix-found-array-with-dim-4error-when-using-ml-algorthims-to-classify-image</link>
      <description><![CDATA[我有一个简单的 ML 分类问题。我有 8 个文件夹，每个文件夹代表一个类，因此我首先从文件夹中加载这些图像并分配标签，然后将其保存为 csv 文件（代码如下）
def load_images_from_folder(root_folder):`
image_paths = []
images = []
labels = []
for label in os.listdir(root_folder):
label_path = os.path.join(root_folder, label)
if os.path.isdir(label_path):
for filename in os.listdir(label_path):
img_path = os.path.join(label_path, filename)
if os.path.isfile(img_path) and (filename.endswith(&quot;.jpg&quot;):
img = Image.open(img_path)
img = img.resize((128, 128))
img_array = np.array(img)
image_paths.append(img_path)
images.append(img_array)
labels.append(label)
return image_paths, images, labels
if __name__ == &quot;__main__&quot;:
root_folder_path = &quot;./Datasets_1&quot;
image_paths, images, labels = load_images_from_folder(root_folder_path)

然后我将图像和标签转换为 DataFrame 并加载它
data = {&quot;Images&quot;: image_paths, &quot;Labels&quot;: labels}
df = pd.DataFrame(data)
df.to_csv(&quot;original_data.csv&quot;, index=False)
csv_file = &quot;original_data.csv&quot;
df = pd.read_csv(csv_file)

我还将向 DataFrame 添加一个带有编码标签的新列“Encoded_Labels”，并将“Encoded_Labels”列转换为整数
df[&#39;Encoded_Labels&#39;] =coded_labels
df[&#39;Encoded_Labels&#39;] = df[&#39;Encoded_Labels&#39;].astype(int)

最后，我将数据集拆分为训练集和测试集，并对训练图像进行预处理
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
def load_and_preprocess_images(file_paths, target_size=(128, 128)):
images = []
for file_path in file_paths:
img = Image.open(file_path)
img = img.resize(target_size)
img_array = np.array(img) / 255.0 # 标准化像素值
images.append(img_array)
return np.array(images)

X_train = load_and_preprocess_images(train_df[&#39;Images&#39;].values)
y_train = train_df[&#39;Encoded_Labels&#39;].values
X_test = load_and_preprocess_images(test_df[&#39;Images&#39;].values)
y_test = test_df[&#39;Encoded_Labels&#39;].values**your text**

X_train 的输出形状是
(20624, 128, 128, 3)`

对于这一点我没有问题，我可以使用它与 DL 模型一起使用没有问题，但是当尝试使用 ML 模型（例如 KNN、SVM、DT 等）时。示例代码如下
from sklearn.svm import SVC
svc = SVC(kernel=&#39;linear&#39;,gamma=&#39;auto&#39;)
svc.fit(X_train, y_train)`

或
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train)
y_pred = knn_clf.predict(X_test)
accuracy = metrics.accuracy_score(y_test, y_pred)
print(&quot;Accuracy of KNN Classifier : %.2f&quot; % (准确率*100))

我收到此错误
“ValueError：找到 dim 为 4 的数组。SVC 预期 &lt;= 2。”
如何修复此错误？
使用 ML 训练模型]]></description>
      <guid>https://stackoverflow.com/questions/78869863/how-fix-found-array-with-dim-4error-when-using-ml-algorthims-to-classify-image</guid>
      <pubDate>Wed, 14 Aug 2024 08:26:27 GMT</pubDate>
    </item>
    <item>
      <title>xgboost 是否使用有放回抽样、无放回抽样或者其他完全不同的抽样方法？</title>
      <link>https://stackoverflow.com/questions/78869855/does-xgboost-use-sampling-with-replacement-sampling-without-replacement-or-some</link>
      <description><![CDATA[在Coursera 上学习这门课程，据说它像传统的集成树一样使用替换采样。我知道 xgboost 在第一次迭代后会为错误分类的示例赋予更多权重，但是第一次迭代呢？即使在网上，我也得到了不同的信息。
课程片段
尝试了 GPT、Gemini 和在线资源。]]></description>
      <guid>https://stackoverflow.com/questions/78869855/does-xgboost-use-sampling-with-replacement-sampling-without-replacement-or-some</guid>
      <pubDate>Wed, 14 Aug 2024 08:23:38 GMT</pubDate>
    </item>
    <item>
      <title>在 Keras 3 中从“.keras”文件加载模型时出现反序列化错误，密集层可能存在问题</title>
      <link>https://stackoverflow.com/questions/78869745/deserializing-error-when-loading-models-from-keras-files-in-keras-3-possible</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78869745/deserializing-error-when-loading-models-from-keras-files-in-keras-3-possible</guid>
      <pubDate>Wed, 14 Aug 2024 07:58:35 GMT</pubDate>
    </item>
    <item>
      <title>Scikit-learn 版本不匹配问题。我不知道应该安装哪个版本</title>
      <link>https://stackoverflow.com/questions/78869112/scikit-learn-version-mismatch-problem-and-i-dont-know-which-version-should-be</link>
      <description><![CDATA[我正在努力解决涉及 Scikit-learn 的版本不匹配问题，事实证明这非常成问题。每当我尝试安装不同版本的 Scikit-learn 时，我都会遇到一系列错误，这些错误似乎因我尝试的每个版本而异。

问题的核心似乎是 Scikit-learn 与其依赖项（例如 NumPy 和 SciPy）之间的不兼容性。这些依赖项对于 Scikit-learn 正常运行至关重要，找到可以协同工作的正确版本已成为一项艰巨的任务。尽管我付出了努力，但我还是无法找到一个可以解决错误并与我现有设置很好地集成的 Scikit-learn 版本。反复试验的过程只会导致越来越多的挫败感，因为每个新版本都会带来一系列问题，而不是解决核心问题。
这个版本不匹配严重影响了我在项目中有效使用 Scikit-learn 的能力。缺乏关于将 Scikit-learn 与其依赖项的兼容版本对齐的明确指导增加了我的困难，使我很难继续工作并实现预期结果。]]></description>
      <guid>https://stackoverflow.com/questions/78869112/scikit-learn-version-mismatch-problem-and-i-dont-know-which-version-should-be</guid>
      <pubDate>Wed, 14 Aug 2024 04:15:49 GMT</pubDate>
    </item>
    <item>
      <title>设计基于距离/速度/加速度的运动启动/停止检测器</title>
      <link>https://stackoverflow.com/questions/78867452/designing-a-motion-start-stop-detector-based-on-distance-velocity-acceleration</link>
      <description><![CDATA[我训练了 yolov8 nano 来检测培养皿中游动的鱼胚胎。培养皿中任何时候都只有一个胚胎，所以这是一项相当简单的任务，模型表现良好（mAP50=0.994）。我的项目的最终目标是拥有一个以视频为输入的软件，并让其输出指标（每帧的 x、y 坐标、游动距离、游动速度等），仅针对视频中胚胎游动的帧。例如，视频可能有 200 帧，前 40 帧左右胚胎尚未游动，然后是 140 帧游动，然后是 20 帧不游动（鱼已停止游动）。因此，对于此视频，我希望有一个函数，它从包含视频中所有帧信息的 csv 文件中提取仅 140 个相关帧。
使用硬编码算法执行此操作的主要问题是数据嘈杂，使得胚胎游泳模式的结束难以检测。例如，最小每帧速度数字（假设胚胎可以游动 1 个像素）通常约为 10mm/s。然而，即使鱼静止不动，模型预测中的随机变化也会将边界框的中心移动几个像素，因此噪声约为 10-20mm/s。为此，我对速度列应用了简单指数平滑，以尝试降低噪声：
def simple_exponential_smoothing(data, alpha):
&quot;&quot;&quot;
对数据应用简单指数平滑。

参数：
data (array-like)：输入的时间序列数据。
alpha (float)：平滑因子 (0 &lt; alpha &lt;= 1)。

返回：
np.ndarray：平滑的时间序列数据。
&quot;&quot;&quot;
result = [data[0]] # 第一个值与序列相同
for n in range(1, len(data)):
result.append(alpha * data[n] + (1 - alpha) * result[n-1])
return np.array(result)


我最初的方法是使用一个 csv 文件（包含一个视频的预测，每帧一个），并在其上运行一个“检测器”函数。我尝试使用以下函数提取起始和结束帧，以便我可以将数据修剪为仅相关帧以进行进一步计算：
def find_start_end_rows(df, velocity_column,filtered_velocity_column, frame_rate):
&quot;&quot;&quot;
根据更精确的方法查找起始和结束行索引。

参数：
df (pd.DataFrame)：要分析的数据框。
velocity_column (str)：要搜索的速度列的名称。
adopted_velocity_column (str)：过滤后的速度列的名称。

返回：
tuple：包含起始行索引和结束行索引的元组。
&quot;&quot;&quot;
start_row = None
end_row = None
velocity_threshold = 20 # 开始游泳的最小速度
filtered_velocity_threshold = 10 # 考虑运动的最小过滤速度
consistent_low_velocity_frames = 5 # 检测结束的连续低速帧数

# 查找起始行
for i in range(len(df)):
if df.loc[i, velocity_column] &gt;= velocity_threshold:
start_row = i - 1
break

# 如果 start_row 仍为 None，则表示未找到值 &gt;= 20
if start_row is None:
return (-1,-1) # -1 表示函数失败

# 通过检查起始行后的一致低速来查找结束行
low_velocity_count = 0
for i in range(start_row + 2, len(df)):
if df.loc[i,过滤后的低速帧：
low_velocity_column] &lt; 过滤后的低速帧：
low_velocity_count += 1
如果低速帧：
= 一致低速帧：
end_row = i - 一致低速帧：
break
否则：
low_velocity_count = 0

# 如果 end_row 仍为 None，则表示未找到一致的低速帧
如果 end_row 为 None：
end_row = len(df) - 1

返回 start_row, end_row

但是，正如我们在下图中看到的那样，该函数的性能并不理想。该图展示了使用此函数进行起始帧预测和结束帧预测的误差（将函数的输出与这些视频中的真实起始/结束帧进行比较）。对于该项目来说，至关重要的是，我们在预测开始/结束帧时看到的变异性最多为 2-3 帧。
显示 find_start_end_frames 错误的图表
哪种方法可能是检测视频中的开始/结束帧的最佳方法？通过算法解决这个问题会很棒，而不必为这项任务训练整个其他 ML 模型，但我愿意接受任何人们认为可行的解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78867452/designing-a-motion-start-stop-detector-based-on-distance-velocity-acceleration</guid>
      <pubDate>Tue, 13 Aug 2024 16:46:36 GMT</pubDate>
    </item>
    <item>
      <title>具有大数据集的 Adaboost：初始权重接近于零</title>
      <link>https://stackoverflow.com/questions/78867381/adaboost-with-large-dataset-initial-weight-are-near-zero</link>
      <description><![CDATA[我正在尝试实现 adaboost。我选择的数据集是 mlpack 包中的 Covertype。问题在于计算数据集的初始权重为 1/n_elem，其中 n_elem (406709) 是一个非常大的数字，因此除法接近于 0。此外，类别非常不平衡：

标签 0 出现 148378 次。
标签 1 出现 198219 次。
标签 2 出现 25086 次。
标签 3 出现 1935 次
标签 4 出现 6656 次。
标签 5 出现 12181 次。
标签 6 出现 14254 次。

因此，当我规范化权重时，会发生分段错误

我想到的唯一想法是对数据集进行分区，但我不知道是否要保留这种类别分布或平衡它？这可能是个好主意吗？还有其他可能性吗？
提前感谢你的帮助！]]></description>
      <guid>https://stackoverflow.com/questions/78867381/adaboost-with-large-dataset-initial-weight-are-near-zero</guid>
      <pubDate>Tue, 13 Aug 2024 16:29:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 创建聊天机器人，使用 AWS Bedrock 中的模型与 Excel 或 PDF 文件进行交互 [关闭]</title>
      <link>https://stackoverflow.com/questions/78866843/create-a-chatbot-using-llm-for-interacting-with-excel-or-pdf-files-using-models</link>
      <description><![CDATA[我想创建一个聊天机器人，它可以根据我输入的 excel 或 PDF 文件回答问题。我有 AWS 基本访问权限，可以在其中使用多个模型。我的 excel 文件有 20k 行和 10 列。有人可以建议我应该使用哪个最好的 LLM 模型吗？或者如果有人可以分享一些相关资源，那将非常有帮助。
我尝试了一些模型，但要么我得到了 token 错误，要么输出不准确。]]></description>
      <guid>https://stackoverflow.com/questions/78866843/create-a-chatbot-using-llm-for-interacting-with-excel-or-pdf-files-using-models</guid>
      <pubDate>Tue, 13 Aug 2024 14:26:01 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的 Sums 包无法进行标记</title>
      <link>https://stackoverflow.com/questions/78862511/sums-package-failing-to-tokenize-in-python</link>
      <description><![CDATA[我使用以下代码在 Python 中总结我的文本。代码正在 Jupyter Notebook 中运行。我已经使用 pip 命令安装了 sumy。
pip install sumy nltk
python -m nltk.downloader punkt

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from io import StringIO

# 定义要总结的文本
text = &quot;&quot;&quot;
自然语言处理 (NLP) 是人工智能的一个领域，专注于通过自然语言实现计算机与人类之间的互动。NLP 的最终目标是使计算机能够以有价值和有意义的方式理解、解释和响应人类语言。
NLP 用于应用算法来识别和提取自然语言规则，从而将非结构化语言数据转换为计算机可以理解的形式。当提供文本时，计算机可以采用多种不同的方法来处理它。算法可以是基于规则的方法，也可以是基于机器学习的方法。
“”“”

# 使用 StringIO 模拟文件类对象
text_io = StringIO(text)

# 解析文本
parser = PlaintextParser.from_file(text_io, Tokenizer(&quot;english&quot;))

# 初始化 LSA 摘要器
summarizer = LsaSummarizer()

# 生成摘要（您可以调整句子数量）
summary = summaryr(parser.document, sentences_count=2)

# 打印摘要
for sentence in summary:
print(sentence) 

当我运行程序时，我收到以下错误：
ame)
662 def find_class(self, module, name):
663 # 禁止每个函数
--&gt; 664 引发 pickle.UnpicklingError(f&quot;全局 &#39;{module}.{name}&#39; 被禁止&quot;)

UnpicklingError: 全局 &#39;copy_reg._reconstructor&#39; 被禁止 

有什么想法！]]></description>
      <guid>https://stackoverflow.com/questions/78862511/sums-package-failing-to-tokenize-in-python</guid>
      <pubDate>Mon, 12 Aug 2024 15:37:15 GMT</pubDate>
    </item>
    <item>
      <title>我无法从“typing_extensions”导入名称“TypeAliasType”</title>
      <link>https://stackoverflow.com/questions/77450322/i-cannot-import-name-typealiastype-from-typing-extensions</link>
      <description><![CDATA[我是 Python 新手，发现了以下这样的错误。非常感谢您的评论。谢谢
我尝试将 Gradio 库导入为 gr
我尝试了几个现有的建议，但结果都是徒劳的。我不知道该怎么办]]></description>
      <guid>https://stackoverflow.com/questions/77450322/i-cannot-import-name-typealiastype-from-typing-extensions</guid>
      <pubDate>Thu, 09 Nov 2023 03:38:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使 RandomForestClassifier 更快？</title>
      <link>https://stackoverflow.com/questions/43640546/how-to-make-randomforestclassifier-faster</link>
      <description><![CDATA[我正在尝试使用大约有 1M 原始数据的 Twitter 情绪数据从 kaggle 网站实现词袋模型。我已经清理了它，但在最后一部分，当我将特征向量和情绪应用于随机森林分类器时，它花费了太多时间。这是我的代码...
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 100,verbose=3)
forest = forest.fit( train_data_features, train[&quot;Sentiment&quot;] )

train_data_features 是 1048575x5000 稀疏矩阵。我试图将其转换为数组，但执行时显示内存错误。
我哪里做错了？有人可以建议我一些来源或其他更快的方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/43640546/how-to-make-randomforestclassifier-faster</guid>
      <pubDate>Wed, 26 Apr 2017 17:09:55 GMT</pubDate>
    </item>
    </channel>
</rss>