<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Wed, 24 Apr 2024 15:15:22 GMT</lastBuildDate>
    <item>
      <title>LightGBM 排名指标</title>
      <link>https://stackoverflow.com/questions/78379350/lightgbm-ranking-metrics</link>
      <description><![CDATA[我使用了 LightGBM LambdaRank，但不明白如何根据获得的值进一步计算指标
数据框
将 numpy 导入为 np
将 pandas 导入为 pd
将 lightgbm 导入为 lgb

df = pd.DataFrame({
    “query_id”：[i for i in range(100) for j in range(10)],
    “var1”：np.random.random(size=(1000,)),
    “var2”：np.random.random(size=(1000,)),
    “var3”：np.random.random(size=(1000,)),
    “相关性”:list(np.random.permutation([1,2,3,4,5, 6,7,8,9,10]))*100
})


这是数据框：
query_id var1 var2 var3 相关性
0 0 0.905357 0.894079 0.375130 5
1 0 0.547075 0.377121 0.754090 3
2 0 0.160593 0.397771 0.034981 10
3 0 0.295548 0.162948 0.549913 9
4 0 0.833037 0.233751 0.096317 7
……………………
995 99 0.188557 0.258408 0.090342 6
996 99 0.178921 0.881938 0.467198 2
997 99 0.175884 0.897310 0.992994 8
998 99 0.466874 0.400800 0.561379 1
999 99 0.035098 0.232043 0.982138 4
1000行×5列


我将数据集分为训练集、验证集和测试集
&lt;前&gt;&lt;代码&gt;
train_df = df.iloc[:600]
test_df = df.iloc[600:800]
val_df = df.iloc[800:]

qids_train = train_df.groupby(“query_id”)[“query_id”].count().to_numpy()
X_train = train_df.drop([“query_id”, “相关性”], axis=1)
y_train = train_df[“相关性”]

qids_validation =validation_df.groupby(“query_id”)[“query_id”].count().to_numpy()
X_validation =validation_df.drop([“query_id”,“相关性”], axis=1)
y_validation =validation_df[“相关性”]

qids_test = test_df.groupby(“query_id”)[“query_id”].count().to_numpy()
X_test = test_df.drop([“query_id”,“相关性”], axis=1)
y_test = test_df[“相关性”]

创建 lgb 数据集
lgb_train = lgb.Dataset(X_train, label=y_train, group=qids_train)
lgb_valid = lgb.Dataset(X_validation, label=y_validation, group=qids_validation)
lgb_test = lgb.Dataset(X_test, label=y_test, group=qids_test)


火车模型
param_ranking = {
    “目标”：“lambdarank”，
    “label_gain”：[int(i) for i in range(int(max(y_train.max(), y_validation.max())) + 1)],
    “公制”：[“ndcg”]，
    “评估时间”：5，
    “随机状态”：1，
    “冗长”：-1，
    # &#39;num_threads&#39;: 16,
    “学习率”：0.1，
}
回调 = [
    lgb.early_stopping(20),
    lgb.log_evaluation（周期=10）
]
model_gbm = lgb.train(
    参数排名，
    LGB_火车，
    100,
    valid_sets=[lgb_train, lgb_valid],
    回调=回调，
）

训练直到验证分数在 20 轮内没有提高
[10]训练的ndcg@5：0.880126 valid_1的ndcg@5：1
[20] 训练的 ndcg@5: 0.906826 valid_1 的 ndcg@5: 1
提前停止，最佳迭代是：
[1] 训练的 ndcg@5: 0.789559 valid_1 的 ndcg@5: 1


y_pred = model_gbm.predict(X_test)
X_test[“预测排名”] = y_pred
X_test.sort_values(“预测排名”, 升序=False)


我现在陷入困境，无法计算 map@k、hit@k、mrr、ndcg@k 等指标。如果有人可以帮忙，请解释如何解释 lgbm 模型预测，以便进一步计算测试数据集上的指标
def hit_rate(y_true, y_pred, k=5):
    点击数 = 0
    对于 true，pred 在 zip(y_true, y_pred) 中：
        top_indices = np.argsort(pred)[::-1][:k]
        如果 top_indices 为 true：
            命中数 += 1
    返回命中数 / len(y_true)
hit_rate(y_test, y_pred )
0.0

我尝试计算命中率，但得到了 0]]></description>
      <guid>https://stackoverflow.com/questions/78379350/lightgbm-ranking-metrics</guid>
      <pubDate>Wed, 24 Apr 2024 14:48:25 GMT</pubDate>
    </item>
    <item>
      <title>关于 Sklearn 中的 jaccard_score</title>
      <link>https://stackoverflow.com/questions/78378954/about-jaccard-score-in-sklearn</link>
      <description><![CDATA[我遇到了 jaccard_score 函数的问题 (jaccard_score(ground_true, inference,average=“micro”, Zero_division=0))。我没有足够的内存来存储验证的所有 ground_truth 和推论。因此，我所做的是将每个批次的 Jaccard 分数求和到一个变量，并在验证结束时将该变量除以批次数。通过这种方法，我获得的结果为 0.8084487056909495。然而，当我使用容量更大的计算机来处理数据，并一次处理所有ground_truth和mask时，我得到一个像0.7716579100568796这样的值。有人能解释一下为什么会发生这种情况吗？]]></description>
      <guid>https://stackoverflow.com/questions/78378954/about-jaccard-score-in-sklearn</guid>
      <pubDate>Wed, 24 Apr 2024 13:49:22 GMT</pubDate>
    </item>
    <item>
      <title>为什么 scikit-learn 的 QDA 警告我“变量共线”——我该怎么办？</title>
      <link>https://stackoverflow.com/questions/78378679/why-is-qda-of-scikit-learn-warning-me-variables-are-collinear-what-can-i-do</link>
      <description><![CDATA[在训练中运行 QuadraticDiscriminationAnalysis (QDA) 和 .fit，然后调用 .predict 来预测多项分类时，我收到警告：
“discriminant_analysis.py:935：UserWarning：变量共线 warnings.warn(“变量共线”)”
此后，程序不会崩溃，但会导致分类非常差，通常为 20%，而 scikit-learn 中的许多其他分类器对同一数据集（包括 LDA）的准确率为 80% 到 90%。&lt; /p&gt;
从本论坛之前对问题的回答中，我意识到，当它认为 X 矩阵是线性相关的时，即至少其中一个向量可以由其他向量的线性组合生成时，就会发生这样的警告。在这种情况下，算法的矩阵求逆所产生的误差很大 - 我假设这就是分类精度如此差的原因。
但是，我知道 X 矩阵不是线性相关的。所以我假设它失败可能是因为 X 矩阵接近线性相关，并且在这些条件下矩阵的求逆并不精确。
假设这就是问题所在，有没有更好的方法来得到X矩阵求逆，从而得到的误差更小，分类精度更高？
（同样，由于 LDA 从相同的数据集（即 X 矩阵）中生成了良好的精度，因此此例程中的矩阵求逆很好 - 那么为什么它在 QDA 中失败？）
我正在使用 scikit-learn 版本 1.3.0、Python 3.8 和 Anaconda 包管理器。更改为 scikit-learn 1.2.0 会生成相同的警告。
我多次尝试运行该程序，但总是得到这个结果。
非常感谢，N。]]></description>
      <guid>https://stackoverflow.com/questions/78378679/why-is-qda-of-scikit-learn-warning-me-variables-are-collinear-what-can-i-do</guid>
      <pubDate>Wed, 24 Apr 2024 13:04:56 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：输入 X 包含 NaN。 SVR 不接受原生编码为 NaN 的缺失值</title>
      <link>https://stackoverflow.com/questions/78378560/valueerror-input-x-contains-nan-svr-does-not-accept-missing-values-encoded-as</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78378560/valueerror-input-x-contains-nan-svr-does-not-accept-missing-values-encoded-as</guid>
      <pubDate>Wed, 24 Apr 2024 12:44:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么在用平均值或中位数替换数据集中的 NaN 值后会得到“inf”值？</title>
      <link>https://stackoverflow.com/questions/78378193/why-do-i-get-inf-values-after-replacing-nan-values-in-my-dataset-with-the-mean</link>
      <description><![CDATA[我正在使用 Python，并且有一个包含 NaN 值的数据集。为了清理这些数据，我使用 pandas 的 fillna() 函数将 NaN 值替换为每列的平均值或中位数。然而，在此操作之后，我的数据集中的一些值变成了“inf”。我不明白为什么会发生这种情况以及如何解决此问题。]]></description>
      <guid>https://stackoverflow.com/questions/78378193/why-do-i-get-inf-values-after-replacing-nan-values-in-my-dataset-with-the-mean</guid>
      <pubDate>Wed, 24 Apr 2024 11:45:41 GMT</pubDate>
    </item>
    <item>
      <title>无法使用数据数组 1*10201 编写机器学习模型</title>
      <link>https://stackoverflow.com/questions/78377490/can-t-coding-machine-learning-model-with-data-array-110201</link>
      <description><![CDATA[我已经编写了拓扑优化代码，并希望使用从拓扑优化循环收集的数据作为数据帧来添加机器学习，如图所示，J_old 每列都有一个值，但 lamda 和敏感度值​​是 1*10201 数组，灵敏度是我希望该模型预测的输出目标
我用如图所示的数据编码机器学习模型
数据
而且，我希望 x_data 成为“J_old”和“lamda”列中的数据，我该怎么做？？
x_data = df[&#39;J_old&#39;]
y_data = df[&#39;灵敏度&#39;]

x_train, x_test, y_train, y_test = train_test_split(x_data1, y_data1, test_size=0.2, random_state=42)

模型=顺序（）

input_data = tf.placeholder(tf.float32, (None, 40, 40, 2), name=&#39;input_data&#39;) #输入大小=40
model_dens = model.add(Dense(units=64,activation=&#39;softmax&#39;)) #dense 中的单位 = 64, softmax = 输出为数组
output_true = tf.placeholder(tf.float32, (None, 40, 40, 1), name=&#39;output_true&#39;) #输入大小=40
Learning_rate = tf.placeholder(tf.float32, [], name=&#39;learning_rate&#39;)

model.compile（优化器=&#39;adam&#39;，损失=&#39;mean_squared_error&#39;，指标=[&#39;mae&#39;]）
#model.fit(x=trainX, y=trainY,validation_data=(testX, testY),epochs=200)
#model.fit(x_train, y_train, epochs=5)
model.fit(x_train, y_train, epochs=5,validation_data=(x_test, y_test))
# 打印模型摘要
模型.summary()

&lt;前&gt;&lt;代码&gt;
-------------------------------------------------- ------------------------
TypeError Traceback（最近一次调用最后一次）
[160] 中的单元格，第 19 行
     16 model.compile（优化器=&#39;adam&#39;，损失=&#39;mean_squared_error&#39;，指标=[&#39;mae&#39;]）
     17 #model.fit(x=trainX, y=trainY,validation_data=(testX, testY),epochs=200)
     18 #model.fit(x_train, y_train, epochs=5)
---&gt; 19 model.fit(x_train, y_train, epochs=5,validation_data=(x_test, y_test))
     20 # 打印模型摘要
     21 模型.summary()

文件 /usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:122，位于filter_traceback..error_handler(*args, **kwargs)
    第119章
    120 # 要获取完整的堆栈跟踪，请调用：
    121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
    123 最后：
    124 删除filtered_tb

文件/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/compat.py:81，在as_bytes（bytes_or_text，编码）中
     79 返回字节或文本
     80 其他：
---&gt; 81 raise TypeError(&#39;需要二进制或unicode字符串，得到%r&#39; %
     82（字节或文本，））

类型错误：需要二进制或 unicode 字符串，得到数组([-0.24753326+0.j, -0.24753326+0.j, -0.2500359 +0.j, ...,
       -0.34861478+0.j,-0.34856497+0.j,-0.34856497+0.j])

我该如何修复这个错误？]]></description>
      <guid>https://stackoverflow.com/questions/78377490/can-t-coding-machine-learning-model-with-data-array-110201</guid>
      <pubDate>Wed, 24 Apr 2024 09:52:12 GMT</pubDate>
    </item>
    <item>
      <title>尽管 GPU 可用，但 CUDA 设置失败</title>
      <link>https://stackoverflow.com/questions/78376600/cuda-setup-failed-despite-gpu-being-available</link>
      <description><![CDATA[我需要使用bitsandbytes包来运行使用Falcon7B模型的代码。我已经安装了 CUDA，并且我的系统具有 NVIDIA RTX A6000 GPU。我的系统有 Windows 11 操作系统。
这是代码，它只是导入部分：
导入火炬
从数据集导入load_dataset
从变压器导入 AutoModelForCausalLM、AutoTokenizer、BitsAndBytesConfig、TrainingArguments、GenerationConfig
从peft导入LoraConfig，get_peft_model，PeftConfig，PeftModel，prepare_model_for_kbit_training
从 trl 导入 SFTTrainer
进口警告
warnings.filterwarnings(“忽略”)

这是错误：
运行时错误：
        尽管 GPU 可用，但 CUDA 安装失败。请运行以下命令来获取更多信息：

        python -m 位和字节

        检查命令的输出并查看是否可以找到 CUDA 库。您可能需要添加它们
        到您的 LD_LIBRARY_PATH。如果您怀疑存在错误，请从 python -m bitsandbytes 获取信息
        并在以下位置提出问题：https://github.com/TimDettmers/bitsandbytes/issues



RuntimeError：由于以下错误而无法导入transformers.training_args（查找其回溯）：

        尽管 GPU 可用，但 CUDA 安装失败。请运行以下命令来获取更多信息：

        python -m 位和字节

        检查命令的输出并查看是否可以找到 CUDA 库。您可能需要添加它们
        到您的 LD_LIBRARY_PATH。如果您怀疑存在错误，请从 python -m bitsandbytes 获取信息
        并在以下位置提出问题：https://github.com/TimDettmers/bitsandbytes/issues

有时不会出现此错误，并且代码可以正常工作。但大多数时候我都会遇到此错误，并且无法找到准确的修复方法。
当系统中未安装 CUDA 时，首次出现此错误。安装后没有报错，但是第二天再次运行时，又出现了同样的错误。
接下来我尝试将 python 版本降级到 3.11.1 以下，之后代码再次运行。但今天我再次面临同样的错误。
这是我的 CUDA 版本：
&lt;前&gt;&lt;代码&gt;nvcc --版本
nvcc：NVIDIA (R) Cuda 编译器驱动程序
版权所有 (c) 2005-2023 NVIDIA 公司
建于 Wed_Feb__8_05:53:42_Cooperative_Universal_Time_2023
Cuda 编译工具，版本 12.1，V12.1.66
构建cuda_12.1.r12.1/compiler.32415258_0
]]></description>
      <guid>https://stackoverflow.com/questions/78376600/cuda-setup-failed-despite-gpu-being-available</guid>
      <pubDate>Wed, 24 Apr 2024 07:18:58 GMT</pubDate>
    </item>
    <item>
      <title>未能过度拟合多项式回归？</title>
      <link>https://stackoverflow.com/questions/78374435/failing-to-overfit-polynomial-regression</link>
      <description><![CDATA[我正在尝试将多项式回归过拟合到正弦曲线。据我所知，当有 N 数据样本和多项式次数 N-1 时，曲线应该穿过所有数据点，但是，在我的例如这不会发生。
我的代码如下：
from sklearn. Linear_model 导入 LinearRegression
从 sklearn.preprocessing 导入多项式特征

数 = 50
度 = 49

X = X = np.linspace(0, 2 * np.pi, N).reshape(-1, 1)
X = np.sort(X, 轴=0)
y = np.sin(X) + np.random.randn(N, 1) * 0.2

poly_features = 多项式特征（度=deg，include_bias=False）

X_poly = poly_features.fit_transform(X)

reg = 线性回归()
reg.fit(X_poly, y)

y_vals = reg.predict(X_poly)

plt.scatter(X, y)
plt.plot(X, y_vals, color=&#39;r&#39;)
plt.show()


你能解释一下我在这里的误解吗？]]></description>
      <guid>https://stackoverflow.com/questions/78374435/failing-to-overfit-polynomial-regression</guid>
      <pubDate>Tue, 23 Apr 2024 18:44:44 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 TimeSeriesSplit 处理面板数据？</title>
      <link>https://stackoverflow.com/questions/78370489/how-to-use-timeseriessplit-for-panel-data</link>
      <description><![CDATA[我一直在尝试使用TimeSeriesSplit对于面板数据。我所说的面板数据是指人口的年度图片。我对多年来的数据分割很感兴趣。该人口正在不断变化，每年的人口规模并不相同。因此，直接使用 TimeSeriesSplit 是不可能的。
基本上我试图获得以下简历方案：

我设法使用以下代码来做到这一点：
将 pandas 导入为 pd，将 numpy 导入为 np
将seaborn导入为sns，将matplotlib.pyplot导入为plt

从 sklearn.datasets 导入 make_regression
从 sklearn.dummy 导入 DummyRegressor
从 sklearn.metrics 导入mean_squared_error
从 sklearn.model_selection 导入 TimeSeriesSplit

X_测试，y_测试 = []，[]

开始年份 = 2010
年底 = 2020 年

对于 np.arange(start_year, end_year+1) 中的年份：
    X_year, y_year = make_regression(n_samples=5+year-start_year, n_features=2, 偏差=100, 噪声=1, random_state=year)
    X_year = pd.DataFrame(X_year).rename(columns={0:&#39;X1&#39;, 1:&#39;X2&#39;})
    X_year[&#39;年份&#39;] = 年
    y_year = pd.Series(y_year)
    X_test.append(X_year)
    y_test.append(y_year)
    
X_test, y_test = pd.concat(X_test), pd.concat(y_test)

# 建模

X = X_测试
y = y_测试
年 = np.unique(X_test[&#39;year&#39;])

# 建模
模型= DummyRegressor（策略=“平均值”）
指标=均方误差
cv = TimeSeriesSplit(n_splits=len(年)-1)

年数=[]
分辨率=[]

对于 i，枚举（cv.split（years））中的（train_year，test_year）：
    
    print(f&quot;折叠 {i}:&quot;)
    print(f&quot;火车：索引={years[train_year]}&quot;)
    print(f&quot;测试：index={years[test_year]}&quot;)
    
    years_folds.append((years[train_year],years[test_year]))
    
    train_filter = X[&#39;year&#39;].isin(years[train_year])
    test_filter = X[&#39;year&#39;].isin(years[test_year])
    
    X_train, y_train = X.loc[train_filter.values], y[train_filter.values]
    X_test, y_test = X.loc[test_filter.values], y[test_filter.values]
    
    model.fit(X_train, y_train)
    分数 = 指标(model.predict(X_test), y_test)
    print(f&#39; {score=:.3}&#39;)
    res.append((年份[test_year][0], 分数))

情节_年份_折叠（年份_折叠）
    
Folds_res = pd.DataFrame(res,columns=[&#39;test_year&#39;, metric.__name__])
Folds_res.plot.scatter(x=&#39;test_year&#39;, y=metric.__name__, title=f&#39;{metric.__name__} over test_year&#39;);

注意：我使用虚拟数据集和虚拟模型是为了提供运行示例。这不是我帖子的主题。
正如您所注意到的，我必须使用一个技巧：我分割年份而不是数据。我想知道：是否有一种标准方法可以使用 sklearn cv 对象分割数据？目标是在 cross_val_score 函数中使用它。]]></description>
      <guid>https://stackoverflow.com/questions/78370489/how-to-use-timeseriessplit-for-panel-data</guid>
      <pubDate>Tue, 23 Apr 2024 07:14:30 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中制作人工智能驱动的自学习聊天机器人 [关闭]</title>
      <link>https://stackoverflow.com/questions/78370101/how-to-make-ai-powered-self-learning-chatbot-in-python</link>
      <description><![CDATA[我一直在尝试用 Python 制作一个自学习聊天机器人，并尝试了不同的库，如 NLTK、TensorFlow、ChatBot 和 PyTorch，但所有这些库都在处理预定义的训练数据。我找不到任何选项来根据给定的输入自行训练模型并尝试不同类型的数据集。
在Python中有什么方法可以实现这一点吗？
我可以看到我们可以使用已经训练好的模型并对其进行微调，或者使用 DialogFlow 和 Rasa 来建立对话模型。然而，我正在寻找一种方法，可以用我们自己的数据来训练它，并且它可以从给定的数据中自我学习。]]></description>
      <guid>https://stackoverflow.com/questions/78370101/how-to-make-ai-powered-self-learning-chatbot-in-python</guid>
      <pubDate>Tue, 23 Apr 2024 05:44:52 GMT</pubDate>
    </item>
    <item>
      <title>最后一个维度的有效乘法</title>
      <link>https://stackoverflow.com/questions/78344508/effective-multiplication-over-last-dimension</link>
      <description><![CDATA[我有两个火炬张量 - A 形状为 (15, 100, 256) 和 B 形状为 (120, 2010, 256)。如何对最后一个维度进行有效乘法并获得形状 (15, 100, 120, 2010) 的张量。我尝试了类似 torch.einsum(&#39;ijk, mnk -&gt; ijmn&#39;, A, B) 的方法，但据我了解，这种方法隐式创建中间张量并需要大量内存和时间。我还尝试了 opt_einsum 库，但与 torch.einsum 相比，在时间和内存使用方面没有看到很大的差异
当然，我可以在循环中完成它，但我想要得到在时间和内存使用方面都有效的解决方案。预先感谢您]]></description>
      <guid>https://stackoverflow.com/questions/78344508/effective-multiplication-over-last-dimension</guid>
      <pubDate>Thu, 18 Apr 2024 02:13:36 GMT</pubDate>
    </item>
    <item>
      <title>迭代重新加权最小二乘法</title>
      <link>https://stackoverflow.com/questions/62116804/iterative-reweighted-least-squares</link>
      <description><![CDATA[我正在尝试手动实现irls逻辑回归（Bishop - 模式识别和机器学习）。 
为了更新权重，我使用 
然而，我没有得到令人满意的结果，而且我的权重在每次迭代中都无限增长。
到目前为止我已经编写了这段代码：
def y(X, w):
    返回 sigmoid(X.dot(w))

定义 R(y):
    R = np.identity(y.size)
    R = R*(y*(1-y))
    返回R

def irls(X, t):
    w = np.ones(X.shape[1])
    w = w.reshape(w.size, 1)
    t = np.array(list(map(lambda x: 1 if x else 0, t)))
    t = t.reshape(t.size, 1)

    #3次迭代后矩阵是奇异的
    对于范围 (3) 内的 i：
        y_ = y(X,w)
        w = w - np.linalg.inv(X.T.dot(R(y_)).dot(X)).dot((X.T).dot(y_-t))
    返回w

其中 X 是我的设计矩阵（64 个特征和 74 个样本），t 是由布尔值组成的目标向量（数据来自 https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression)。
感谢任何帮助指出我出错的地方。]]></description>
      <guid>https://stackoverflow.com/questions/62116804/iterative-reweighted-least-squares</guid>
      <pubDate>Sun, 31 May 2020 13:15:36 GMT</pubDate>
    </item>
    <item>
      <title>使用 OneHotEncoder 时出现错误“预期为 2D 数组，改为 1D 数组”</title>
      <link>https://stackoverflow.com/questions/47957151/error-expected-2d-array-got-1d-array-instead-using-onehotencoder</link>
      <description><![CDATA[我是机器学习的新手，正在尝试解决使用 OneHotEncoder 类遇到的错误。错误是：“预期是二维数组，却得到了一维数组”。因此，当我想到一维数组时，它类似于： [1,4,5,6] ，而二维数组则为 [[2,3], [3,4], [ 5,6]]，但我仍然无法弄清楚为什么会失败。这条线失败了：
X[:, 0] = onehotencoder1.fit_transform(X[:, 0]).toarray()

这是我的完整代码：
# 导入库
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
将 pandas 导入为 pd

# 导入数据集
数据集 = pd.read_csv(&#39;Data2.csv&#39;)
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 5].values
df_X = pd.DataFrame(X)
df_y = pd.DataFrame(y)

# 替换缺失值
从 sklearn.preprocessing 导入 Imputer
imputer = Imputer（missing_values =&#39;NaN&#39;，策略=&#39;平均值&#39;，轴= 0）
imputer = imputer.fit(X[:, 3:5 ])
X[:, 3:5] = imputer.transform(X[:, 3:5])


# 对分类数据“名称”进行编码
从 sklearn.preprocessing 导入 LabelEncoder、OneHotEncoder
labelencoder_x = LabelEncoder()
X[:, 0] = labelencoder_x.fit_transform(X[:, 0])

# 转化为矩阵
onehotencoder1 = OneHotEncoder(categorical_features = [0])
X[:, 0] = onehotencoder1.fit_transform(X[:, 0]).toarray()

# 编码分类数据“大学”
从 sklearn.preprocessing 导入 LabelEncoder
labelencoder_x1 = LabelEncoder()
X[:, 1] = labelencoder_x1.fit_transform(X[:, 1])

我确信您可以通过这段代码看出我有两列是标签。我使用标签编码器将这些列转换为数字。我想使用 OneHotEncoder 更进一步，将它们转换为一个矩阵，这样每一行都会有这样的内容： 

&lt;前&gt;&lt;代码&gt;0 1 0
1 0 1

我唯一想到的是我如何对标签进行编码。我一项一项地做，而不是一次全部做。不确定这就是问题所在。
我希望做这样的事情：
# 编码分类数据“名称”
从 sklearn.preprocessing 导入 LabelEncoder、OneHotEncoder
labelencoder_x = LabelEncoder()
X[:, 0] = labelencoder_x.fit_transform(X[:, 0])

# 转化为矩阵
onehotencoder1 = OneHotEncoder(categorical_features = [0])
X[:, 0] = onehotencoder1.fit_transform(X[:, 0]).toarray()

# 编码分类数据“大学”
从 sklearn.preprocessing 导入 LabelEncoder、OneHotEncoder
labelencoder_x1 = LabelEncoder()
X[:, 1] = labelencoder_x1.fit_transform(X[:, 1])

# 转化为矩阵
onehotencoder2 = OneHotEncoder(categorical_features = [1])
X[:, 1] = onehotencoder1.fit_transform(X[:, 1]).toarray()

下面你会发现我的整个错误：
文件“/Users/jim/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py”，第 441 行，在 check_array 中
    “如果它包含单个样本。”.format(array))

ValueError：需要 2D 数组，却得到 1D 数组：
数组=[ 2.1.3.2.3.5.5.0.4.0.]。
如果数据具有单个特征，则使用 array.reshape(-1, 1) 重塑数据；如果数据包含单个样本，则使用 array.reshape(1, -1) 重塑数据。

任何朝着正确方向的帮助都会很棒。]]></description>
      <guid>https://stackoverflow.com/questions/47957151/error-expected-2d-array-got-1d-array-instead-using-onehotencoder</guid>
      <pubDate>Sun, 24 Dec 2017 00:27:08 GMT</pubDate>
    </item>
    <item>
      <title>AttributeError：模块“tensorflow.python.pywrap_tensorflow”没有属性“TFE_Py_RegisterExceptionClass”</title>
      <link>https://stackoverflow.com/questions/46010571/attributeerror-module-tensorflow-python-pywrap-tensorflow-has-no-attribute-t</link>
      <description><![CDATA[我正在尝试使用最新的可用资源来开发一些时间序列序列预测。为此，我确实检查了 TensorFlow 时间序列中的示例代码，但收到此错误：
AttributeError：模块“tensorflow.python.pywrap_tensorflow”没有属性“TFE_Py_RegisterExceptionClass”

我正在使用 Anaconda。当前环境是Python 3.5和TensorFlow 1.2.1。也尝试过 TensorFlow 1.3，但没有任何改变。
这是我的代码尝试运行。我在谷歌上没有找到与该问题相关的任何有用信息。有什么解决办法吗？]]></description>
      <guid>https://stackoverflow.com/questions/46010571/attributeerror-module-tensorflow-python-pywrap-tensorflow-has-no-attribute-t</guid>
      <pubDate>Sat, 02 Sep 2017 04:48:51 GMT</pubDate>
    </item>
    <item>
      <title>最大似然估计伪代码</title>
      <link>https://stackoverflow.com/questions/7718034/maximum-likelihood-estimate-pseudocode</link>
      <description><![CDATA[我需要编写一个最大似然估计器来估计一些玩具数据的均值和方差。我有一个包含 100 个样本的向量，是使用 numpy.random.randn(100) 创建的。数据应具有零均值和单位方差高斯分布。
我检查了维基百科和一些额外的资源，但我有点困惑，因为我没有统计背景。
是否有最大似然估计器的伪代码？我得到了 MLE 的直觉，但我不知道从哪里开始编码。
Wiki 表示采用对数似然的 argmax。我的理解是：我需要使用不同的参数来计算对数似然，然后我将采用给出最大概率的参数。我不明白的是：我首先在哪里可以找到参数？如果我随机尝试不同的平均值 &amp;方差以获得高概率，我什么时候应该停止尝试？]]></description>
      <guid>https://stackoverflow.com/questions/7718034/maximum-likelihood-estimate-pseudocode</guid>
      <pubDate>Mon, 10 Oct 2011 20:05:45 GMT</pubDate>
    </item>
    </channel>
</rss>