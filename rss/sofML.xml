<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Mon, 29 Apr 2024 06:20:03 GMT</lastBuildDate>
    <item>
      <title>ValueError：在 LSTM 时间序列预测中发现输入变量样本数量不一致</title>
      <link>https://stackoverflow.com/questions/78400794/valueerror-found-input-variables-with-inconsistent-numbers-of-samples-in-lstm-t</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78400794/valueerror-found-input-variables-with-inconsistent-numbers-of-samples-in-lstm-t</guid>
      <pubDate>Mon, 29 Apr 2024 06:16:48 GMT</pubDate>
    </item>
    <item>
      <title>移植到 TensorFlow 的扩散模型无法学习</title>
      <link>https://stackoverflow.com/questions/78400562/diffusion-model-ported-to-tensorflow-doesnt-learn</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78400562/diffusion-model-ported-to-tensorflow-doesnt-learn</guid>
      <pubDate>Mon, 29 Apr 2024 05:03:46 GMT</pubDate>
    </item>
    <item>
      <title>Keras TextVectorization 似乎区分大小写，即使词汇量没有反映这一点？</title>
      <link>https://stackoverflow.com/questions/78400462/keras-textvectorization-seems-to-be-case-sensitive-even-though-vocabulary-doesn</link>
      <description><![CDATA[我有以下代码
导入 keras

v = {
    “甲板”：[&#39;a&#39;，&#39;B&#39;，&#39;C&#39;，&#39;D&#39;，&#39;E&#39;，&#39;F&#39;，&#39;G&#39;，&#39;H&#39;，&#39;I&#39;，&#39;J&#39;，&#39;K&#39;， &#39;L&#39;]
}

打印（len（v[“甲板”]））

l = keras.layers.TextVectorization(
    max_tokens=len(v[“甲板”])+2,
    词汇=v[“甲板”],
    输出模式=&#39;计数&#39;,
    名称=“甲板”）

打印（l.vocabulary_size（））
打印（l.get_vocabulary（））

print(l(&#39;a A b B&#39;))


输出是：
&lt;前&gt;&lt;代码&gt;12
13
[&#39;[UNK]&#39;、&#39;a&#39;、&#39;B&#39;、&#39;C&#39;、&#39;D&#39;、&#39;E&#39;、&#39;F&#39;、&#39;G&#39;、&#39;H&#39;、&#39;I&#39;、&#39;J&#39;、&#39;K&#39; ，&#39;L&#39;]
tf.Tensor([2.2.0.0.0.0.0.0.0.0.0.0.0.],形状=(13,),dtype=float32)

我希望至少有一个 b 能够被计算在内。
如果我使用l.adapt(v[“deck”])，事情似乎会相应地工作，但词汇都是小写的。
像这样：
导入 keras

v = {
    “甲板”：[&#39;a&#39;，&#39;B&#39;，&#39;C&#39;，&#39;D&#39;，&#39;E&#39;，&#39;F&#39;，&#39;G&#39;，&#39;H&#39;，&#39;I&#39;，&#39;J&#39;，&#39;K&#39;， &#39;L&#39;]
}

打印（len（v[“甲板”]））

l = keras.layers.TextVectorization(
    max_tokens=len(v[“甲板”])+2,
    # 词汇=v[“甲板”],
    输出模式=&#39;计数&#39;,
    名称=“甲板”）

l.adapt(v[&#39;甲板&#39;])

打印（l.vocabulary_size（））
打印（l.get_vocabulary（））

print(l(&#39;a A b B&#39;))

和输出：
&lt;前&gt;&lt;代码&gt;12
13
[&#39;[UNK]&#39;、&#39;l&#39;、&#39;k&#39;、&#39;j&#39;、&#39;i&#39;、&#39;h&#39;、&#39;g&#39;、&#39;f&#39;、&#39;e&#39;、&#39;d&#39;、&#39;c&#39;、&#39;b&#39; ， &#39;A&#39;]
tf.Tensor([0.0.0.0.0.0.0.0.0.0.0.2.2.]，形状=(13,)，dtype=float32)

如何正确使用词汇参数？]]></description>
      <guid>https://stackoverflow.com/questions/78400462/keras-textvectorization-seems-to-be-case-sensitive-even-though-vocabulary-doesn</guid>
      <pubDate>Mon, 29 Apr 2024 04:20:18 GMT</pubDate>
    </item>
    <item>
      <title>屏蔽 pytorch 张量时减少 TPU RAM 使用量</title>
      <link>https://stackoverflow.com/questions/78400256/reduce-tpu-ram-usage-when-masking-pytorch-tensors</link>
      <description><![CDATA[目前正在致力于合并 LLM 并在 Tensor 中计算其任务向量。 （通过获取前 80% 的向量）但是每次当我尝试从张量获取值时，它都会超过 Colab TPU 的 RAM 使用量。张量很大，但只需要大约 60GB 的 RAM。现阶段有什么办法可以减少TPU RAM的使用吗？
def mask（张量）：
    使用 torch.no_grad()：
        d=张量.形状[1]
        掩码=int(d*0.8)
        张量_abs=张量.abs()
    
        #RAM 使用情况良好，直到下一行
        
        kth_values，_=tensor_abs.kthvalue（掩码，dim = 1，keepdim = True）
        mask=tensor_abs&gt;=kth_values
        返回张量*掩码

我尝试过使用 torch.no_grad。有什么办法可以将张量分成不同的批次并分别处理吗？]]></description>
      <guid>https://stackoverflow.com/questions/78400256/reduce-tpu-ram-usage-when-masking-pytorch-tensors</guid>
      <pubDate>Mon, 29 Apr 2024 02:40:02 GMT</pubDate>
    </item>
    <item>
      <title>SVM 训练耗时过长</title>
      <link>https://stackoverflow.com/questions/78400254/svm-training-taking-too-long</link>
      <description><![CDATA[我有一个包含 41 个特征的数据集，其中 4 个是文本特征。我得到了“词袋”这四个特征的 numpy 数组 (npz)，我将其与其他数值特征结合起来训练 SVM 模型。总共有 100000 条记录和 41 个特征，其中 4 个特征如前所述进行了矢量化。
该模型现已训练 45 分钟:)。有没有办法减少训练时间？我预处理数据集的方式（特别是结合 npz 和现有的数值特征）有什么问题吗？我还可以探索其他选择吗？
title_feature = load_npz(&#39;train_title_bow.npz&#39;)
Overview_feature = load_npz(&#39;train_overview_bow.npz&#39;)
tagline_feature = load_npz(&#39;train_tagline_bow.npz&#39;)
Production_companies_feature = load_npz(&#39;train_product_companies_bow.npz&#39;)

numeric_features = df_train[df_train.columns.difference([&#39;标题&#39;, &#39;概述&#39;, &#39;标语&#39;, &#39;生产公司&#39;, &#39;rate_category&#39;, &#39;average_rate&#39;, &#39;original_language&#39;])]
text_features = np.hstack([title_feature.toarray()、overview_feature.toarray()、tagline_feature.toarray()、生产_companies_feature.toarray()])
svm_X_train = np.hstack([数字特征, 文本特征])
svm_y_train = df_train[&#39;rate_category&#39;]

svm_classifier = SVC(kernel=&#39;linear&#39;) # 使用线性核，也可以选择其他核
svm_classifier.fit(svm_X_train, svm_y_train)
]]></description>
      <guid>https://stackoverflow.com/questions/78400254/svm-training-taking-too-long</guid>
      <pubDate>Mon, 29 Apr 2024 02:39:09 GMT</pubDate>
    </item>
    <item>
      <title>GKE 上的 GPU 时间共享</title>
      <link>https://stackoverflow.com/questions/78400223/gpu-time-sharing-on-gke</link>
      <description><![CDATA[我正在尝试使用 说明中的 GPU 时间共享此处，但是我的工作负载不会在启用分时的节点上运行。
我有一个具有 GPU 配置的节点池，启用了策略分时的 GPU 共享以及“每个 GPU 的最大共享客户端数”。如 48 所示。节点运行良好，但我无法使用记录的 nodeSelector 配置为我的工作负载运行工作负载，例如
节点选择器：
  cloud.google.com/gke-accelerator：“nvidia-tesla-t4”
  cloud.google.com/gke-max-shared-clients-per-gpu：“48”
  cloud.google.com/gke-gpu-sharing-strategy：分时

这样，我的 Pod 就会陷入挂起状态，并显示消息xnodes did not match Pod&#39;s nodeaffinity/selector。如果我删除 gke-max-shared-clients-per-gpu 和 gke-gpu-sharing-strategy 密钥对，pod 会正常调度并运行。
当我检查 GPU 分时节点池中节点上的 kubernetes 标签时，它们不包含这些标签，并且我无法手动添加它们，因为 GCP 阻止了它。
如有任何建议，我们将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78400223/gpu-time-sharing-on-gke</guid>
      <pubDate>Mon, 29 Apr 2024 02:20:41 GMT</pubDate>
    </item>
    <item>
      <title>是否仍然建议手动使用 `del` 或 `torch.cuda.empty_cache()` ？</title>
      <link>https://stackoverflow.com/questions/78399631/is-it-still-recommended-to-use-del-or-torch-cuda-empty-cache-manually</link>
      <description><![CDATA[许多在线 ML 代码仍在使用此功能。但截至 2024 年，是否仍建议使用它？如果不建议，是否有任何特定情况建议使用它？（例如调试？）
我希望更多地了解为什么这些以前如此常见以及它们是否仍然有必要]]></description>
      <guid>https://stackoverflow.com/questions/78399631/is-it-still-recommended-to-use-del-or-torch-cuda-empty-cache-manually</guid>
      <pubDate>Sun, 28 Apr 2024 20:56:20 GMT</pubDate>
    </item>
    <item>
      <title>java中的机器学习是否可行？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78399466/machine-learning-in-java-is-possible-or-not</link>
      <description><![CDATA[
我们可以使用 Java 进行机器学习吗？
如何使用它？
Java 中有哪些可用的机器学习库？
Java 中的机器学习比 Python 更高效吗？
如果java有机器学习功能，它比python更有价值吗？

我在谷歌上搜索了上述问题并得到了几个答案，但我必须从工作专业人士那里得到答案，这样我才能消除我的疑虑......]]></description>
      <guid>https://stackoverflow.com/questions/78399466/machine-learning-in-java-is-possible-or-not</guid>
      <pubDate>Sun, 28 Apr 2024 19:45:18 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：基数为 10 的 int() 的文字无效：Q-learning 中的“”</title>
      <link>https://stackoverflow.com/questions/78399063/valueerror-invalid-literal-for-int-with-base-10-in-q-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78399063/valueerror-invalid-literal-for-int-with-base-10-in-q-learning</guid>
      <pubDate>Sun, 28 Apr 2024 17:29:11 GMT</pubDate>
    </item>
    <item>
      <title>我正在寻找这些列类型的什么类型的机器学习？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78398611/what-type-of-machine-learning-am-i-looking-for-with-these-column-types</link>
      <description><![CDATA[我一直在学习一些关于机器学习的知识，并使用了一些模型类型（xgboost、LogisticRegression）和一些测试数据。我使用这些模型的次数越多，我就越意识到它们处理的是一种特定类型的数据，即可以转换为数字的列。甚至像汽车的品牌/型号之类的东西也可以转化为数字，因为它们是有限的并且在数据集中重复。
我真正想要使用的数据集包含名字和姓氏、公司名称、电子邮件地址等唯一的字符串。这是一个例子

&lt;标题&gt;

名字和姓氏
公司名称
电子邮件地址
是欺诈


&lt;正文&gt;

全食 CVS 评估
全食/CVS 评估
laime.barry9989@gmail.com
正确


全食店
全食店
laimeb.a.r.ry9989@gmail.com
正确


蒂娜·罗森
最佳商品鞋
tina.rosen@gmail.com
错误


乔约翰
全食品市场调查
wholefoodsmark.et.l.inc@gmail.com
正确


史黛西帕克特
S Parket 奥特莱斯
sales@parkeroutlet.com
错误


迈克尔·费兰
克罗格
b.ill.h.o.rt2.2@gmail.com
正确



这是我拥有的一小部分数据，但您可以看到它不适合我所了解和使用的模型的正常数据集。我尝试过诸如 OneHotEncoder 和 LabelEncoder 之类的东西，但它们将它们转换为实际上没有任何意义的整数，因为它们不重复。
我知道看到该示例很容易想到“哦，只需自己编写验证器来查找电子邮件中的多个句点、名称中的特定单词等”即可。但有数千个重复的欺诈帐户不适合。
所以我的问题是，是否有一种机器学习模型可以接收这些电子邮件地址/名称等内容并了解欺诈电子邮件地址/名称的样子？]]></description>
      <guid>https://stackoverflow.com/questions/78398611/what-type-of-machine-learning-am-i-looking-for-with-these-column-types</guid>
      <pubDate>Sun, 28 Apr 2024 15:01:24 GMT</pubDate>
    </item>
    <item>
      <title>我们如何以优雅的方式捕获使用optimizer.step()完成的更新？</title>
      <link>https://stackoverflow.com/questions/78392429/how-can-we-capture-update-done-with-optimizer-step-in-an-elegant-way</link>
      <description><![CDATA[我想实现一种方法，按照 Karpathy 视频中提到的想法，在使用 PyTorch 训练期间在 Tensorboard 中监控更新数据比率。我已经提出了一个解决方案，但我正在寻找一种更优雅且可配置的方法。
当前的实现直接修改训练循环如下：
对于步骤，在 data_loader 中进行批处理：
    x, y = 批次
    优化器.zero_grad()
    对于名称，model.named_pa​​rameters() 中的参数：
        如果 param.requires_grad 和“weight”是名称：
            param.data_before_step = param.data.clone()
    输出=模型(x)
    损失 = loss_fn(输出, y)
    loss.backward()
    优化器.step()
    lr_scheduler.step()
    对于名称，model.named_pa​​rameters() 中的参数：
        if hasattr(param, “data_before_step”):
            更新 = param.data - param.data_before_step
            update_to_data = (update.std() / param.data_before_step.std()).log10().item()
            summary_writer.add_scalar(f“更新：数据比率 {name}”，update_to_data，epoch * len(data_loader) + 步骤)
            param.data_before_step = param.data.clone()

但是，这种方法直接在训练循环中添加代码，这可能会使代码变得混乱，如果我们想要使其可配置，则需要 if-else 语句，这会使代码更加混乱。
我还探索过使用 PyTorch hooks 来实现这一点。我已经成功实现了一个钩子来跟踪梯度：
类 GradToDataRatioHook：
    def __init__(自身、名称、参数、start_step、summary_writer):
        self.name = 名字
        self.param = 参数
        self.summary_writer = 摘要_writer
        自我.毕业生 = []
        self.grads_to_data = []
        self.param.update_step = start_step

    def __call__(自我，毕业生)：
        self.grads.append(grad.std().item())
        self.grads_to_data.append((grad.std() / (self.param.data.std() + 1e-5)).log10().item())
        self.summary_writer.add_scalar(f&quot;Grad {self.name}&quot;, self.grads[-1], self.param.update_step)
        self.summary_writer.add_scalar(f&quot;梯度:数据比例{self.name}&quot;, self.grads_to_data[-1], self.param.update_step)
        self.param.update_step += 1

但是，实现类似的钩子来捕获更新似乎很棘手。据我了解， param.register_hook(...) 注册了钩子，该钩子在计算梯度时调用，即在 optimizer.step() 之前调用叫。虽然梯度和学习率为标准 SGD 提供了更新的直接值，但像 Adam 这样的现代优化器使更新过程变得更加复杂。我正在寻找一种以与优化器无关的方式捕获更新的解决方案，最好使用 PyTorch 挂钩。但是，任何建议或替代方法也将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78392429/how-can-we-capture-update-done-with-optimizer-step-in-an-elegant-way</guid>
      <pubDate>Fri, 26 Apr 2024 18:32:22 GMT</pubDate>
    </item>
    <item>
      <title>为什么 50 个预测中只有 45 个，这里预测的是哪一列？</title>
      <link>https://stackoverflow.com/questions/78388371/why-is-there-only-45-predictions-out-of-50-and-which-column-is-predicted-here</link>
      <description><![CDATA[我在为自己的数据编码时使用了张量流教程。代码如下：
训练、验证、测试 = 数据[:420]、数据[420:450]、数据[450:]

类窗口生成器（）：
  def __init__(自我，输入宽度，标签宽度，移位，
             训练、验证、测试）：
    self.train = 火车
    self.val = val
    自测=测试
    self.input_width = input_width
    self.label_width = label_width
    self.shift = 移位
    self.total_window_size = input_width + 移位
    self.input_slice = 切片(0,input_width)
    self.input_indices = np.arange(self.total_window_size)[self.input_slice]
    self.label_start = self.total_window_size-self.label_width
    self.labels_slice = slice(self.label_start,无)
    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

  def __repr__(自我):
    返回 &#39;​​\n&#39;.join([
        f&#39;总window_size: {self.total_window_size}&#39;,
        f&#39;输入索引：{self.input_indices}&#39;,
        f&#39;标签索引：{self.label_indices}&#39;])

  def split_window(自身,特征):
    输入=特征[:,self.input_slice,:]
    标签 = 特征[:,self.labels_slice,:]
    input.set_shape([无,self.input_width,无])
    labels.set_shape([无,self.label_width,无])
    返回输入、标签

  def make_dataset(自身,数据):
    数据 = np.array(数据,dtype=np.float32)
    ds = tf.keras.utils.timeseries_dataset_from_array(数据=数据，
                                                    目标=无，
                                                    序列长度 = self.total_window_size,
                                                    序列步幅 = 1,
                                                    随机播放=真，
                                                    批量大小 = 32,)
    ds = ds.map(self.split_window)
    返回数据

  @财产
  def train_(自身):
    返回 self.make_dataset(self.train)
  @财产
  def val_(自身):
    返回 self.make_dataset(self.val)
  @财产
  def test_(自我):
    返回 self.make_dataset(self.test)
最大纪元 = 100
defcompile_and_fit(模型,窗口,耐心=30):
  Early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;,
                               耐心=耐心，
                               模式=&#39;分钟&#39;,
                              详细 =1)
  reduce_lr =ReduceLROnPlateau(监视器=&#39;val_loss&#39;,因子=0.1,耐心=10,min_lr=1e-6,详细=1)
  model.compile(loss=MeanSquaredError(), 优化器 = Adam(), 指标=[MeanAbsoluteError()])
  历史= model.fit(window.train_,epochs=MAX_EPOCHS,validation_data=window.val_,callbacks=[early_stopping,reduce_lr])
  返回历史记录
Wide_window = WindowGenerator(input_width = 5,
                          标签宽度=1，
                          移位= 1，
                          火车=火车，
                          值=值，
                          测试=测试）
conv_model = 顺序（[输入（形状=（5,2），名称=&#39;编码器输入&#39;），
                     Conv1D(filters=32,kernel_size=5,activation=&#39;relu&#39;,name=&#39;conv1D&#39;),
                    密集（32，激活=&#39;relu&#39;，名称=&#39;dense1&#39;），
                    密集(1,name=&#39;dense2&#39;)])
历史=compile_and_fit(conv_model,wide_window)
y_pred = conv_model.predict(wide_window.test_)`

现在，输出是 (45,1,1)。输出的大小不应该是 50，因为测试大小是 50，因为以下命令的输出是 50？
&lt;前&gt;&lt;代码&gt;打印(len(测试))

当我将密集2单位保持为1时，这里预测的是哪一个？
我尝试阅读 timeseries_dataset_from_array 的文档，但仍然无法找出问题或解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78388371/why-is-there-only-45-predictions-out-of-50-and-which-column-is-predicted-here</guid>
      <pubDate>Fri, 26 Apr 2024 05:17:53 GMT</pubDate>
    </item>
    <item>
      <title>多类问题的层次分类方法</title>
      <link>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</link>
      <description><![CDATA[有一个多类分类任务。我的目标是使用每父节点本地分类器 (LCPN) 方法来解决这个问题。
让我解释一下如何使用 MWE。
假设我有这个虚拟数据集：
将 numpy 导入为 np
从 sklearn.datasets 导入 make_classification
从 scipy.cluster 导入层次结构

X, y = make_classification(n_samples=1000, n_features=10, n_classes=5,
                             n_信息=4）

我想出了这些类之间的距离矩阵：
d = np.array(
[[ 0.、201.537、197.294、200.823、194.517]、
 [201.537, 0., 199.449, 202.941, 196.703],
 [197.294, 199.449, 0., 198.728, 192.354],
 [200.823, 202.941, 198.728, 0., 195.972],
[[194.517, 196.703, 192.354, 195.972, 0.]]
）

因此，我确定了类层次结构，如下所示：
hc = hierarchy.linkage(d, method=&#39;complete&#39;)

得到的树状图如下：
dendrogram = hierarchy.dendrogram(hc, labels=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;, &#39;D&#39;, &#39;F&#39;])
树状图


我使用hierarchy.to_tree()以树状结构进行说明：

我的问题：
如何按照 LCPN 方法在每个内部节点（包括根）处安装分类器，例如 DecisionTreeClassifier 或 SVM，以像在树中一样进行上图？]]></description>
      <guid>https://stackoverflow.com/questions/78358516/hierarchical-classification-approach-to-a-multiclass-problem</guid>
      <pubDate>Sat, 20 Apr 2024 14:08:05 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中获取多类分类问题中每个类的 SHAP 值</title>
      <link>https://stackoverflow.com/questions/71753428/how-to-get-shap-values-for-each-class-on-a-multiclass-classification-problem-in</link>
      <description><![CDATA[我有以下数据框：
import pandas as pd
import random

import xgboost
import shap

foo = pd.DataFrame({&#39;id&#39;:[1,2,3,4,5,6,7,8,9,10],
&#39;var1&#39;:random.sample(range(1, 100), 10),
&#39;var2&#39;:random.sample(range(1, 100), 10),
&#39;var3&#39;:random.sample(range(1, 100), 10),
&#39;class&#39;: [&#39;a&#39;,&#39;a&#39;,&#39;a&#39;,&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;b&#39;,&#39;c&#39;,&#39;c&#39;,&#39;c&#39;]})

我想运行分类算法来预测这 3 个类别。
因此，我将数据集拆分为训练集和测试集，然后运行xgboost 分类
cl_cols = foo.filter(regex=&#39;var&#39;).columns
X_train, X_test, y_train, y_test = train_test_split(foo[cl_cols],
foo[[&#39;class&#39;]],
test_size=0.33, random_state=42)

model = xgboost.XGBClassifier(objective=&quot;binary:logistic&quot;)
model.fit(X_train, y_train)

现在我想获取每个类的平均 SHAP 值，而不是从此代码生成的绝对 SHAP 值中获取平均值：
shap_values = shap.TreeExplainer(model).shap_values(X_test)
shap.summary_plot(shap_values, X_test)


此外，该图将类标记为 0,1,2。我如何知道 0,1 &amp; 属于哪个类？ 2 与原始对应吗？
因为这段代码：
shap.summary_plot(shap_values, X_test,
class_names= [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])

给出

还有这段代码：
shap.summary_plot(shap_values, X_test,
class_names= [&#39;b&#39;, &#39;c&#39;, &#39;a&#39;])

给出

所以我对这个传说不再确定了。
有什么想法吗？]]></description>
      <guid>https://stackoverflow.com/questions/71753428/how-to-get-shap-values-for-each-class-on-a-multiclass-classification-problem-in</guid>
      <pubDate>Tue, 05 Apr 2022 14:21:03 GMT</pubDate>
    </item>
    <item>
      <title>为什么 GAN 中 Generator 的训练标签应该始终为 True？</title>
      <link>https://stackoverflow.com/questions/47499404/why-should-the-training-label-for-generator-in-gan-be-always-true</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/47499404/why-should-the-training-label-for-generator-in-gan-be-always-true</guid>
      <pubDate>Sun, 26 Nov 2017 18:12:05 GMT</pubDate>
    </item>
    </channel>
</rss>