<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 15 Mar 2024 12:23:32 GMT</lastBuildDate>
    <item>
      <title>如何将多个视频输入组合成 (2+1)D ResNet</title>
      <link>https://stackoverflow.com/questions/78166723/how-do-i-combine-multiple-video-inputs-into-a-21d-resnet</link>
      <description><![CDATA[我开发了 (2+1)D ResNet 的 v1，它将每帧的像素数据作为输入，并用于预测该视频中最多 8 个对象的边界框坐标。我当前输入的形状是：
（batch_size、n_frames、高度、宽度、通道）
我的输出是这样的：
&lt;代码&gt;（n_frames，32）
我使用交并集（IoU）作为损失，并且看到一些相对较差的结果。我想通过增加模型中的特征数量来增加这一点（数据集很小，但将来会增加）。我从视频中提取的特征是：

边缘
运动向量
颜色直方图
光流
纹理

如何利用这些功能从我的模型中获得更好的预测？
我的第一步是将像素数据、单个特征和标签放入列表中。然后我创建了训练、测试和验证分组。使用帧生成器类将它们转换为数据集。
然后我创建了以下架构：
input_shape =（无、无、高度、宽度、4）
frames_input=layers.Input(形状=(无，高度，宽度，3))
Edges_input=layers.Input(形状=(无，高度，宽度，1))
merged_input=layers.concatenate(\[frames_input, Edges_input\], axis=-1)

# 重塑输入张量以包含不同长度的时间维度

x = 图层.Reshape((-1, 高度, 宽度, 4))(merged_input)

x = Conv2Plus1D(filters=FILTERS, kernel_size=KERNAL_SIZE, padding=&#39;same&#39;)(x)
x = 层.BatchNormalization()(x)
x = 层.ReLU()(x)
x = 调整视频大小(高度 // 2, 宽度 // 2)(x)

# 区块 1

x = add_residual_block(x, 16, (3, 3, 3))
x = 调整视频大小(高度 // 4, 宽度 // 4)(x)

# 区块 2

x = add_residual_block(x, 32, (3, 3, 3))
x = 调整视频大小(高度 // 8, 宽度 // 8)(x)

# 区块 3

x = add_residual_block(x, 64, (3, 3, 3))
x = 调整视频大小(高度 // 16, 宽度 // 16)(x)

# 区块 4

x = add_residual_block(x, 128, (3, 3, 3))

# 应用 TimeDistributed 密集层来输出每帧的边界框坐标

x = TimeDistributed(layers.GlobalAveragePooling2D())(x) # 将空间维度转换为单维度
x = TimeDistributed(layers.Dense(32))(x)

BoundingBoxV2_model = keras.Model（[frames_input，edges_input]，x）

并像这样构建模型：
sampled_frames，sampled_edges，sampled_labels = next(iter(train_ds)) BoundingBoxV2_model.build([sampled_frames，sampled_edges])
当我尝试拟合我的模型时，出现以下错误：
&lt;前&gt;&lt;代码&gt;-------------------------------------------------------- -------------------------------------------
ValueError Traceback（最近一次调用最后一次）
单元格位于\[149\]，第 1 行
\----\&gt; 1 历史 = BoundingBoxV2_model.fit(x = train_ds,
2 epoch = EPOCHS,
3 验证数据 = val_ds)

文件c:\\Users\\Rpiku\\miniconda3\\envs\\rally_stream\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70，位于filter_traceback中。\ .error_handler(\*args, \*\*kwargs)
67 过滤_tb = \_process_traceback_frames(e.__traceback__)
68 # 要获取完整的堆栈跟踪，请调用：
69 # `tf.debugging.disable_traceback_filtering()`
\---\&gt; 70 从 None 引发 e.with_traceback(filtered_tb)
71 最后：
72 删除filtered_tb

文件 \~\\AppData\\Local\\Temp\__autograph_ generated_file3rk3lb1s.py:15，位于outer_factory.\.inner_factory.\.tf__train_function(iterator)
13 尝试：
14 do_return =真
\---\&gt; 15 retval_ = ag_\_.converted_call(ag_\_.ld(step_function), (ag_\_.ld(self), ag_\_.ld(迭代器)), 无, fscope)
16 除外：
17 do_return = 假

ValueError：在用户代码中：

    文件“c:\Users\Rpiku\miniconda3\envs\rally_stream\lib\site-packages\keras\engine\training.py”，第 1160 行，在 train_function *

...
文件“c:\\Users\\Rpiku\\miniconda3\\envs\\rally_stream\\lib\\site-packages\\keras\\engine\\input_spec.py”，第216行，位于assert_input_compatibility中
引发值错误（

    ValueError：层“model_8”期望 2 个输入，但它收到 1 个输入张量。收到的输入：[]`
]]></description>
      <guid>https://stackoverflow.com/questions/78166723/how-do-i-combine-multiple-video-inputs-into-a-21d-resnet</guid>
      <pubDate>Fri, 15 Mar 2024 11:54:13 GMT</pubDate>
    </item>
    <item>
      <title>无限循环运行并且即使在 40 分钟后也没有获取输出 [关闭]</title>
      <link>https://stackoverflow.com/questions/78166475/endless-loop-running-and-not-fetching-the-output-even-after-40-minutes</link>
      <description><![CDATA[代码：

错误：

我尝试通过中断终端来运行，但是超过 20 的评级（这是需要的）不可用。即使跑了一个多小时他们也没有停下来。
终端运行了更长的时间，没有结束，但是当中断时，它在第 6 行显示一个错误，这完全没问题。
我想知道这是否是由于我使用的 json 文件大小以及我的系统 RAM 效率低下所致。]]></description>
      <guid>https://stackoverflow.com/questions/78166475/endless-loop-running-and-not-fetching-the-output-even-after-40-minutes</guid>
      <pubDate>Fri, 15 Mar 2024 11:10:11 GMT</pubDate>
    </item>
    <item>
      <title>自动编码器整形问题</title>
      <link>https://stackoverflow.com/questions/78165698/autoencoder-shaping-issue</link>
      <description><![CDATA[我的自动编码器出现问题，因为我错误地调整了输出。目前自动编码器的编码与此类似。
我收到此错误：
&lt;块引用&gt;
ValueError：尺寸必须相等，但为 2000 和 3750
&#39;{{节点mean_absolute_error/sub}} =
Sub[T=DT_FLOAT](sequential_8/sequential_7/conv1d_transpose_14/BiasAdd,
IteratorGetNext:1)&#39;，输入形状：[?,2000,3], [?,3750,3]。

如果可能的话，有人可以帮助调整架构吗？我似乎忘记了最初为此调整所做的原始修改。
导入tensorflow为tf
从tensorflow.keras.models导入模型
从tensorflow.keras.layers导入输入，Conv1D，MaxPooling1D，UpSampling1D，连接
从tensorflow.keras.callbacks导入EarlyStopping

# 提供的编码器
编码器 = tf.keras.models.Sequential([
    tf.keras.layers.Reshape([3750, 3], input_shape=[3750, 3]),
    tf.keras.layers.Conv1D(32，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(64，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(128，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(256，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Conv1D(512，kernel_size=5，padding=“相同”，激活=“relu”)，
    tf.keras.layers.MaxPool1D(pool_size=2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512)
]）

#潜在空间

解码器 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(512 * 125, input_shape=[512]),
    tf.keras.layers.Reshape([125, 512]),
    tf.keras.layers.Conv1DTranspose（512，kernel_size = 5，strides = 1，padding =“相同”，激活=“relu”），
    tf.keras.layers.UpSampling1D（大小=2），
    tf.keras.layers.Conv1DTranspose（256，kernel_size = 5，strides = 1，padding =“相同”，激活=“relu”），
    tf.keras.layers.UpSampling1D（大小=2），
    tf.keras.layers.Conv1DTranspose（128，kernel_size = 5，strides = 1，padding =“相同”，激活=“relu”），
    tf.keras.layers.UpSampling1D（大小=2），
    tf.keras.layers.Conv1DTranspose（64，kernel_size = 5，strides = 1，padding =“相同”，激活=“relu”），
    tf.keras.layers.UpSampling1D（大小=2），
    # 调整内核大小和填充以匹配输入形状
    tf.keras.layers.Conv1DTranspose(3，kernel_size=5，strides=1，padding=“相同”，激活=“线性”)
]）

# 向编码器和解码器添加更多具有更大内核大小的层。
ae = tf.keras.models.Sequential([编码器，解码器])

ae.编译(
    损失=“均方误差”，
    优化器=tf.keras.optimizers.Adam(learning_rate=0.00001)
）
# 定义早期停止标准
Early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, 耐心=30, mode=&#39;min&#39;)

历史= ae.fit（X_train，X_train，batch_size = 8，epochs = 150，validation_data =（X_val，X_val），callbacks = [early_stopping]）```
]]></description>
      <guid>https://stackoverflow.com/questions/78165698/autoencoder-shaping-issue</guid>
      <pubDate>Fri, 15 Mar 2024 08:56:02 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 中线性回归的内部工作原理</title>
      <link>https://stackoverflow.com/questions/78165544/internal-working-of-linear-regression-in-scikit-learn</link>
      <description><![CDATA[我试图了解 Scikit-learn 中线性回归模型的内部工作原理。
这是我的数据集

这是我执行 one-hot-encoding 后的数据集。

这是执行线性回归后的系数和截距值。

销售价格是从属列，其余列是特征。
这些是在这种情况下工作正常的预测值。

我注意到系数的数量比特征的数量多 1。这就是我生成特征矩阵的方式：
feature_matrix = dataFrame.drop([&#39;售价($)&#39;], axis = &#39;列&#39;).to_numpy()

# 要添加为列的数组
bias_column = np.array([[1] for i in range(len(feature_matrix))])

# 使用append()方法将列添加到数组
feature_matrix = np.concatenate([bias_column, feature_matrix], axis = 1) # axis = 1表示列，0表示行

结果

我想知道的是 Scikit-learn 如何使用这些系数和截距来预测值。
这是我尝试过的。
我还注意到，通过进行此计算得到的值实际上等于每种情况下的里程数。但这不是这里的依赖功能。那么这是怎么回事？]]></description>
      <guid>https://stackoverflow.com/questions/78165544/internal-working-of-linear-regression-in-scikit-learn</guid>
      <pubDate>Fri, 15 Mar 2024 08:26:45 GMT</pubDate>
    </item>
    <item>
      <title>普罗米修斯与人工智能的集成[关闭]</title>
      <link>https://stackoverflow.com/questions/78165538/prometheus-integration-with-ai</link>
      <description><![CDATA[我们的多租户项目集成了普罗米修斯，我们每天都会收到很多警报。我们需要将这些指标与人工智能模型集成，以便我们可以尝试分析它们并在此基础上采取一些行动。这只是这个项目的开始，我什至没有任何基本的想法。
我需要一些关于如何开始、学习什么的指导？]]></description>
      <guid>https://stackoverflow.com/questions/78165538/prometheus-integration-with-ai</guid>
      <pubDate>Fri, 15 Mar 2024 08:24:54 GMT</pubDate>
    </item>
    <item>
      <title>如何在Android Studio中实现实时tflite模型？</title>
      <link>https://stackoverflow.com/questions/78165517/how-to-implement-realtime-tflite-model-in-android-studio</link>
      <description><![CDATA[我正在尝试在移动应用程序上进行实时模型实现。我在 Teachable Machine 中训练了模型并将其导出为 model_unquanted.tflite。当我将其导入 Android Studio 时，它会提供以下 Kotlin 代码来实现它：
val model = ModelUnquant.newInstance(context)

// 创建输入以供参考。
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 224, 224, 3), DataType.FLOAT32)
inputFeature0.loadBuffer(byteBuffer)

// 运行模型推理并获取结果。
val 输出 = model.process(inputFeature0)
valoutputFeature0=outputs.outputFeature0AsTensorBuffer

// 如果不再使用则释放模型资源。
模型.close()

以下是我对实时更新的实现：
 覆盖 fun onSurfaceTextureUpdated(p0: SurfaceTexture) {
            位图=textureView.bitmap！！
            val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 224, 224, 3), DataType.FLOAT32)

            val byteBuffer: ByteBuffer = ByteBuffer.allocate(224* 224* 3)
            byteBuffer.rewind()

            inputFeature0.loadBuffer(byteBuffer)

            val 输出 = model.process(inputFeature0)
            valoutputFeature0=outputs.outputFeature0AsTensorBuffer

            var 可变 = bitmap.copy(Bitmap.Config.ARGB_8888, true)
            val canvas = android.graphics.Canvas(可变)

            val h = mutable.height
            val w = mutable.width

            val xPosition = 10 // 根据需要调整该值
            val yPosition = 30 // 根据需要调整该值

            canvas.drawText（outputFeature0.toString（），xPosition.toFloat（），yPosition.toFloat（），绘画）

            imageView.setImageBitmap(可变)

        }

logcat 错误：
 致命异常：main
    进程：com.example.tflite_realtime，PID：14671
    java.lang.IllegalArgumentException：字节缓冲区的大小和形状不匹配。
]]></description>
      <guid>https://stackoverflow.com/questions/78165517/how-to-implement-realtime-tflite-model-in-android-studio</guid>
      <pubDate>Fri, 15 Mar 2024 08:22:05 GMT</pubDate>
    </item>
    <item>
      <title>加载共享库时出错：libonnxruntime.so.1.7.0：无法打开共享对象文件：没有这样的文件或目录</title>
      <link>https://stackoverflow.com/questions/78165433/error-while-loading-shared-libraries-libonnxruntime-so-1-7-0-cannot-open-share</link>
      <description><![CDATA[当我在终端中运行它时，如下所示，出现此错误
nizhar@nizhar-desktop:~/Documents$ g++ -std=c++11 tespredict.cpp -o Predictx -I/usr/local/include/onnxruntime/include -L/usr/local/lib -lonnx运行时
nizhar@nizhar-desktop:~/文档$ ./predictx
./predictx：加载共享库时出错：libonnxruntime.so.1.7.0：无法打开共享对象文件：没有这样的文件或目录


我的 onnxrune-time 库路径位于
&lt;前&gt;&lt;代码&gt;/usr/local/lib
/usr/local/include/onnxruntime

我还在.bashrc中添加了
export ONNXRUNTIME_DIR=“/usr/local/include/onnxruntime/include”
导出 LD_LIBRARY_PATH=“/usr/local/lib:$LD_LIBRARY_PATH”

并且已经这样做了
源 ~/.bashrc

如何解决这个问题？请]]></description>
      <guid>https://stackoverflow.com/questions/78165433/error-while-loading-shared-libraries-libonnxruntime-so-1-7-0-cannot-open-share</guid>
      <pubDate>Fri, 15 Mar 2024 08:04:05 GMT</pubDate>
    </item>
    <item>
      <title>部署机器学习时出错 - Flask 项目</title>
      <link>https://stackoverflow.com/questions/78165242/error-while-deploying-machine-learning-flask-project</link>
      <description><![CDATA[我正在尝试使用 LSTM 构建手语识别模型。我是烧瓶新手，找不到问题所在。当我运行该文件时，它会打开相机但不会检测到该操作。此外，一旦相机打开，应用程序就会卡住。请帮我找出错误，代码如下：
来自flask导入Flask，render_template，Response
导入CV2
进口泡菜
导入 pyttsx3
将 numpy 导入为 np
将 mediapipe 导入为 mp
导入线程

应用程序=烧瓶（__名称__）

从tensorflow.keras.models导入load_model
model = load_model(&#39;action.h5&#39;)

mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

actions = np.array([&#39;你好&#39;,&#39;我是&#39;,&#39;阿凡&#39;,&#39;谢谢&#39;,&#39;我爱你&#39;,&#39;发烧&#39;,&#39;再见&#39;,&#39;上帝&#39;])

def mediapipe_detection（图像，模型）：
    图像 = cv2.cvtColor(图像, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    结果 = model.process(图像)
    image.flags.writeable = True
    图像 = cv2.cvtColor(图像, cv2.COLOR_RGB2BGR)
    返回图像、结果

def extract_keypoints(结果):
    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    返回 np.concatenate([lh, rh])

def draw_styled_landmarks（图像，结果）：
    mp_drawing.draw_landmarks(图像, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                             mp_drawing.DrawingSpec(颜色=(100, 100, 100), 厚度=2, 圆半径=4),
                             mp_drawing.DrawingSpec(颜色=(100, 100, 100), 厚度=2, 圆半径=2)
                             ）
    mp_drawing.draw_landmarks(图像, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                             mp_drawing.DrawingSpec(颜色=(200, 200,200), 厚度=2, 圆半径=4),
                             mp_drawing.DrawingSpec(颜色=(200, 200, 200), 厚度=2, 圆半径=2)
                             ）

序列=[]
句子=[]
预测=[]
阈值 = 0.5

上限 = cv2.VideoCapture(0)

defgenerate_frames():
    sequence = [] # 初始化序列变量
    Sentence = [] # 初始化Sentence变量
    而真实：
        ret, 框架 = cap.read()
        如果不转：
            休息

        图像，结果= mediapipe_detection（框架，整体）
        draw_styled_landmarks（图像，结果）
        关键点 = extract_keypoints(结果)
        序列.append(关键点)
        序列 = 序列[-30:]

        如果长度（序列）== 30：
            res = model.predict(np.expand_dims(序列，轴=0))[0]
            预测.append(np.argmax(res))
            
            if np.unique(预测[-10:])[0] == np.argmax(res):
                如果 res[np.argmax(res)] &gt;临界点：
                    if len(句子) &gt; 0:
                        if actions[np.argmax(res)] !=句子[-1]:
                            句子.append(actions[np.argmax(res)])
                            new_word = 动作[np.argmax(res)]
                            t2s.say(new_word)
                            t2s.runAndWait()
                    别的：
                        句子.append(actions[np.argmax(res)])
                        new_word = 动作[np.argmax(res)]
                        t2s.say(new_word)
                        t2s.runAndWait()

            if len(句子) &gt; 5：
                句子 = 句子[-5:]

        ret, buffer = cv2.imencode(&#39;.jpg&#39;, 图片)
        帧 = buffer.tobytes()
        产量（b&#39;--帧\r\n&#39;
                b&#39;内容类型：image/jpeg\r\n\r\n&#39; + 帧 + b&#39;\r\n&#39;)

    cap.release()


@app.route(&#39;/&#39;)
定义索引（）：
    返回 render_template(&#39;index.html&#39;)

@app.route(&#39;/video_feed&#39;)
def video_feed():
    返回响应（generate_frames（），mimetype =&#39;multipart / x-mixed-replace；边界=框架&#39;）

如果 __name__ == “__main__”：
    t2s = pyttsx3.init()
    整体 = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)
    应用程序运行（调试=真）
  


我尝试过更改模型和修改代码，但不起作用。最初相机馈送未显示，但现在可以正常工作]]></description>
      <guid>https://stackoverflow.com/questions/78165242/error-while-deploying-machine-learning-flask-project</guid>
      <pubDate>Fri, 15 Mar 2024 07:20:28 GMT</pubDate>
    </item>
    <item>
      <title>手动执行回归</title>
      <link>https://stackoverflow.com/questions/78164553/regressions-performed-by-hand</link>
      <description><![CDATA[有谁知道我在哪里可以找到手动执行的线性回归（简单和多重）示例、多项式和逻辑回归以及在哪里显示如何手动执行它们？我想练习它，以便更好地学习其背后的数学。
有人推荐任何书籍、文章、YouTube 视频吗？
我已经看过几个示例，但我很容易感到困惑，因为大多数情况下它们会在方程中添加错误，或者示例与机器学习无关。]]></description>
      <guid>https://stackoverflow.com/questions/78164553/regressions-performed-by-hand</guid>
      <pubDate>Fri, 15 Mar 2024 03:43:30 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习预测未来股票价格[关闭]</title>
      <link>https://stackoverflow.com/questions/78163298/predicting-future-stock-prices-using-machine-learning</link>
      <description><![CDATA[我正在尝试用 python 训练机器学习模型（xgb）来预测股票价格。我首先获取股票价格，然后计算技术指标以用作特征。然后，我将数据拆分为训练集和测试集，其中特征（modeling_df）为 X，收盘价（closes）为 Y。
我的问题是，我不确定该模型实际上是根据过去的价格来预测价格，但该模型是通过查看当前时间的特征来预测价格。我还想确保它使用“滚动窗口”，因此，如果有 30 个值，则值 11-20 应基于 0-10，而 21-30 应基于 0- 20.
如果有人知道神经网络的解决方案是否不同，那么我们也将不胜感激！
scaled_features = scaler.fit_transform(modeling_df)

X = 缩放特征
Y = 关闭

x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.05,shuffle=False)

然后我使用：
final_model = xgb.XGBRegressor(**best_params)
Final_model.fit(x_train,y_train, )

预测 = Final_model.predict(x_test)

然后我使用 matplotlib 显示它。
我尝试将整个数据集移动 10（和其他值），以尝试预测未来的 10 个数据点，但我不确定如何验证它是否确实有效。
这是我的图表（数据集没有被移动）。
烛台 + 蓝线 = 实际价格
紫色线 = 预测价格
图表：

这里有一些数据：
特点和价格（无变化）：
|时间 |奥格价格|相对强弱指数 |
|:---- |:------:| -----:|
| 11:02| 3.61 | 3.61 51.07 | 51.07
|11:03 | 3.62 | 3.62 57.89|
|11:04|3.62|60.28|
|11:05|3.61|62.92|
预测与原价：
|时间 |奥格价格|预测|
|:---- |:------:| -----:|
| 11:02| 3.6 | 3.61 | 3.61
|11:03 | 3.61 |3.62 |
|11:04|3.61|3.62|
|11:05|3.61|3.61|
本质上，我想确保我的模型没有使用 11:02-05 的 RSI 来预测 11:05 的值。我希望它使用 11:02-04 的值来预测 11:05 的值，以便它预测未来。]]></description>
      <guid>https://stackoverflow.com/questions/78163298/predicting-future-stock-prices-using-machine-learning</guid>
      <pubDate>Thu, 14 Mar 2024 20:30:48 GMT</pubDate>
    </item>
    <item>
      <title>TypeError: ' ' SLiMPerformer 对象不可迭代 [关闭]</title>
      <link>https://stackoverflow.com/questions/78161397/typeerror-slimperformer-object-is-not-iterable</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78161397/typeerror-slimperformer-object-is-not-iterable</guid>
      <pubDate>Thu, 14 Mar 2024 14:49:35 GMT</pubDate>
    </item>
    <item>
      <title>在预训练和微调之间使用略有不同的架构</title>
      <link>https://stackoverflow.com/questions/78157500/using-slightly-different-architecture-between-pretraining-and-fine-tuning</link>
      <description><![CDATA[如果我的以下描述不够全面，请提前致歉。
假设我已经训练了一个简单的 Resnet 类模型，其基本块结构如下：
self.layer1 = self._make_layer(self.inplanes, outplanes[0], num_blocks = 3, stride=1)
self.layer2 = self._make_layer(self.inplanes, outplanes[1], num_blocks = 3, 步长=2)
self.layer3 = self._make_layer(self.inplanes, outplanes[2], num_blocks = 3, 步长=2)
self.layer4 = self._make_layer(inplanes = outplanes[2],planes=outplanes[3],num_blocks = 3,stride=2)

现在我想对这个模型进行微调。但我不想重复相同形状的块 3 次，而是想将它们压缩成单层并加载到我的微调模型中：
self.layer1 = self._make_layer(self.inplanes, outplanes[0], num_blocks = 1, stride=1)
self.layer2 = self._make_layer(self.inplanes, outplanes[1], num_blocks = 1, stride=2)
self.layer3 = self._make_layer(self.inplanes, outplanes[2], num_blocks = 1, stride=2)
self.layer4 = self._make_layer(inplanes = outplanes[2],planes=outplanes[3],num_blocks = 1,stride=2)

本质上，这是相同的模型，但我通过将学习到的权重压缩为一个来减少重复次数。需要注意的是，重复的块都具有相同的形状。
有什么方法可以实现这个或者它在数学或理论上不正确吗？我认为这有点类似于知识蒸馏——概念上？
事实上，我在想是否也可以类似地修改内核大小。]]></description>
      <guid>https://stackoverflow.com/questions/78157500/using-slightly-different-architecture-between-pretraining-and-fine-tuning</guid>
      <pubDate>Thu, 14 Mar 2024 00:25:09 GMT</pubDate>
    </item>
    <item>
      <title>更改源代码后如何运行YOLOv8？</title>
      <link>https://stackoverflow.com/questions/78153468/how-do-i-run-yolov8-after-changing-the-source-code</link>
      <description><![CDATA[我需要将 YOLOv8 模型中的默认损失函数（CIOU）更改为 DIOU（或者可能添加自定义函数）。更改后我需要运行模型。
如何使用更改后的代码运行模型？
我找到了需要更改代码的必要文件。现在我不清楚运行模型。
我正在使用 ultralytics 库运行模型，但我认为更改代码后，我无法像那样运行模型。
我对此很陌生，因此我们将不胜感激。]]></description>
      <guid>https://stackoverflow.com/questions/78153468/how-do-i-run-yolov8-after-changing-the-source-code</guid>
      <pubDate>Wed, 13 Mar 2024 11:37:16 GMT</pubDate>
    </item>
    <item>
      <title>嵌入后无法将序列数据转换为 3D 张量</title>
      <link>https://stackoverflow.com/questions/78153453/failed-to-convert-sequence-data-to-a-3d-tensor-after-the-embedding</link>
      <description><![CDATA[我正在使用序列数据（RNA序列）进行分类任务，所以我想尝试CNN，我只是发现在数据编码（我使用序数编码）之后我必须将输入数据编码转换为3D 张量，我仍然不明白如何做到这一点，因此输入形状将是（batch_size，sequence_lenght，num_features）
这是我的输入数据代码：
&lt;块引用&gt;
使用提供的字符“”填充右侧的序列。至 11420 nt
pad_list = df[&#39;Seq&#39;].str.ljust(11420,&#39;&#39;)
减少太长的序列长度
pad_list = pad_list.map(lambda x: x[0:11420])
使用序数编码将核苷酸编码为整数
类别=[“A”、“C”、“G”、“U” 、“T”、“N”、“R”、“K”、“S”、“Y”、“M”、“W”、“D”、“_” ]
ordi = OrdinalEncoder(handle_unknown=“use_encoded_value”,unknown_value=15)
ordi.fit(np.array(列表(类别)).reshape(-1, 1))
pad_list= pad_list.map(lambda seq: ordi.transform(np.array(list(seq)).reshape(-1, 1)))
#将序列转换为整数列表列表而不是矩阵
sequence_input= np.array(pad_list.to_list()).reshape((len(pad_list), len(pad_list[0])))`

我真的尝试使用 cnn 模型查找此类数据的代码，但没有找到好的代码，而且我是这个领域的新手，所以我真的很想了解如何在 cnn 中使用序列数据]]></description>
      <guid>https://stackoverflow.com/questions/78153453/failed-to-convert-sequence-data-to-a-3d-tensor-after-the-embedding</guid>
      <pubDate>Wed, 13 Mar 2024 11:35:10 GMT</pubDate>
    </item>
    <item>
      <title>什么是 x_train.reshape() 及其作用？</title>
      <link>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</link>
      <description><![CDATA[使用 MNIST 数据集
将 numpy 导入为 np
将张量流导入为 tf
从tensorflow.keras.datasets导入mnist

# MNIST 数据集参数
num_classes = 10 # 总类别（0-9 位数）
num_features = 784 # 数据特征（img 形状：28*28）

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 转换为float32
x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)

# 将图像展平为 784 个特征的一维向量 (28*28)
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])

# 将图像值从 [0, 255] 标准化为 [0, 1]
x_train, x_test = x_train / 255., x_test / 255.

在这些代码的第 15 行，即，
x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])。我无法理解这些重塑在我们的数据集中到底做了什么......？请解释一下。]]></description>
      <guid>https://stackoverflow.com/questions/61555486/what-is-x-train-reshape-and-what-it-does</guid>
      <pubDate>Sat, 02 May 2020 06:44:34 GMT</pubDate>
    </item>
    </channel>
</rss>