<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sun, 21 Jul 2024 18:18:38 GMT</lastBuildDate>
    <item>
      <title>如何通过解决问题的能力来提升问题分析和设计的能力？</title>
      <link>https://stackoverflow.com/questions/78775745/how-can-we-improve-problem-analysis-and-design-skills-through-problem-solving-ab</link>
      <description><![CDATA[我希望提高我的问题分析和设计技能，并了解提高我的问题解决能力是这一过程的关键部分。作为一名在 Python、Java、机器学习和数据科学方面有经验的程序员，我想知道：

在开始编码之前，系统地分析问题的最佳实践或方法是什么？
如何有效地将复杂问题分解为可管理的部分？
是否有特定的练习或项目类型可以帮助提高我的问题解决和设计技能？
如何将计算机科学的概念（如算法、数据结构和设计模式）应用于现实世界的问题解决场景？
推荐哪些资源（书籍、课程、在线平台）来提高这些技能？

我尝试解决问题，但没有得到预期的结果……]]></description>
      <guid>https://stackoverflow.com/questions/78775745/how-can-we-improve-problem-analysis-and-design-skills-through-problem-solving-ab</guid>
      <pubDate>Sun, 21 Jul 2024 16:28:07 GMT</pubDate>
    </item>
    <item>
      <title>如何优化 Python 中的递归多步预测以提高性能</title>
      <link>https://stackoverflow.com/questions/78775642/how-to-optimize-recursive-multi-step-forecasting-in-python-to-improve-performanc</link>
      <description><![CDATA[我正在开发一个用于每小时销售预测的机器学习项目。我需要提前 7 天进行销售预测。我的模型 (XGBoost) 使用 24 小时前的滞后销售作为输入之一，为了能够在生产中使用它，我必须使用预测值作为第二天预测的滞后特征。我目前使用的递归实现非常慢，需要几分钟才能完成，这是不可行的，因为我每天需要为数百个模型运行它。
当前实现
下面是我正在使用的代码：
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def predict_7_days_ahead(model, X_test, y_test, transform_target_func):
y_pred_df = pd.DataFrame()
input_forecasts_df = pd.DataFrame()

concatenated_df = pd.concat([X_test, y_test], axis=1)

for hour in concatenated_df.index.hour.unique():
print(f&quot;Processing hour: {hour}&quot;)
df = concatenated_df[concatenated_df.index.hour == hour]

for index, row in df.iterrows():
data = df.copy(deep=True)
predictions = []

for steps_ahead in range(8): # 0 到 7
datetime = index + pd.Timedelta(days=steps_ahead)

if datetime in data.index:
row = data.loc[datetime]
X = row.drop(&#39;sales&#39;).values.reshape(1, -1)
y_pred = model.predict(X)
y_pred = transform_target_func(y_pred, method=&#39;log&#39;, how=&#39;inverse&#39;)

col_name = &#39;sales(t)&#39; if steps_ahead == 0 else f&#39;sales(t+{steps_ahead})&#39;
y_pred_df.loc[index, col_name] = y_pred[0]

predictions.append(y_pred[0])

# 更新第二天的滞后特征
next_day_index = datetime + pd.Timedelta(days=1)
if next_day_index in data.index:
for lag in range(1, min(len(forecasts) + 1, 8)):
data.loc[next_day_index, f&#39;sales_lag_{lag}&#39;] = Forecasts[-lag]

input_forecasts_df = pd.concat([input_forecasts_df, row.to_frame().T])

return y_pred_df, input_forecasts_df

def convert_y_test_to_multi_steps_ahead(y_test, steps_ahead=7):
sales = y_test.copy(deep=True)
sales.index = pd.to_datetime(sales.index)
sales = sales.to_frame(name=&#39;sales(t)&#39;)

for i in range(1, steps_ahead+1):
sales[f&#39;sales(t+{i})&#39;] = sales[&#39;sales(t)&#39;].shift(-15*i) # 从 24 更改为 15

return sales

def calculate_and_plot_accuracy(actual_df, Forecast_df, column_prefix=&#39;sales(t&#39;, Threshold=80):
columns = [col for col in Forecast_df.columns if col.startswith(column_prefix)]
columns.sort(key=lambda x: int(x.split(&#39;+&#39;)[-1][:-1]) if &#39;+&#39; in x else 0)

accuracies = []

for column in columns:
if column not in actual_df.columns or column not in Forecast_df.columns:
print(f&quot;在一个或两个数据框中未找到列 {column}。&quot;)
continue

actual = actual_df[column]
Forecast = Forecast_df[column]

accuracy = ((100 - np.abs(actual - Forecast) / actual * 100) &gt;= Threshold).mean() * 100
accuracies.append(accuracy)

# 绘图
plt.figure(figsize=(12, 6))
plt.plot(columns, accuracies, marker=&#39;o&#39;)
plt.title(f&quot;每个步骤的预测准确度 (阈值：{threshold}%)&quot;)
plt.xlabel(&quot;预测步骤&quot;)
plt.ylabel(&quot;准确度 (%)&quot;)
plt.ylim(0, 100)
plt.xticks(rotation=45, ha=&#39;right&#39;)

for i, (col, accuracy) in enumerate(zip(columns, accuracies)):
plt.annotate(f&#39;{accuracy:.2f}%&#39;, (col, accuracy), textcoords=&quot;offset points&quot;, xytext=(0,10), ha=&#39;center&#39;)

plt.tight_layout()
plt.show()

y_pred_df, input_forecasts_df = predict_7_days_ahead(model, X_test, y_test, transform_target)
y_test_multi = convert_y_test_to_multi_steps_ahead(y_test, steps_ahead=7)
calculate_and_plot_accuracy(y_test_multi, y_pred_df, column_prefix=&#39;sales(t&#39;,阈值=80)


是否有任何最佳实践或技术可以优化此递归预测过程？
如何在保持预测准确性的同时提高此函数的性能？
在这种情况下，矢量化操作或使用特定库（例如 Dask、Joblib）是否有帮助？
任何建议或建议都将不胜感激！
提前谢谢您！]]></description>
      <guid>https://stackoverflow.com/questions/78775642/how-to-optimize-recursive-multi-step-forecasting-in-python-to-improve-performanc</guid>
      <pubDate>Sun, 21 Jul 2024 15:45:41 GMT</pubDate>
    </item>
    <item>
      <title>我想使用基于 PCA 的人脸识别技术对 Yaledatabase 进行识别，并使用留一交叉验证法测量识别率</title>
      <link>https://stackoverflow.com/questions/78775565/i-want-to-use-pca-based-face-recognition-for-yaledatabase-and-measure-the-recogn</link>
      <description><![CDATA[作为自学机器学习的人，我正在阅读论文并运行代码，但我不确定增加特征向量的数量却没有看到识别率的明显变化有什么问题。我使用的 yaledatabase 总共有 15 组，我通过对每组留一然后除以总数据集来获得识别率，但我不确定哪里出了问题……
import numpy as np
import cv2
import os
from numpy import linalg as LA
def read_images(path):
images = []
filenames = []
for root, dirs, files in os.walk(path):
for index, file in enumerate(files):
if(file.endswith(&#39;.gif&#39;)):
img_path = os.path.join(root, file)
img = loadImageFromPath(img_path)
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img_resized = cv2.resize(img_gray, (20,20))
img_normalized = img_resized / 255.0 
images.append((img_normalized, index))
filenames.append(file)
else:
img_path = os.path.join(root, file)
img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
if img_gray 不为 None:
img_resized = cv2.resize(img_gray, (20,20))
img_normalized = img_resized / 255.0 
images.append((img_normalized, index))
filenames.append(file)
return images, filenames
def loadImageFromPath(imgPath):
try:
# gif 图像
if str(imgPath).lower().endswith(&#39;.gif&#39;):
gif = cv2.VideoCapture(imgPath)
ret, frame = gif.read() # 如果找到帧则 ret=True，否则为 False。
if ret:
返回帧
else:
返回 cv2.imread(imgPath,cv2.IMREAD_GRAYSCALE)
除 Exception 外，因为 e:
print(e)
返回 None

def image_as_row(x):
返回 x.flatten()

def covariance(m):
返回 np.cov(m, rowvar=False) 

def eigenvector(m, k):
w, v = np.linalg.eig(m)
idx = np.argsort(w)[::-1][:k] 
返回 v[:, idx] 

def im_map(image, mean, mv):
new = image_as_row(image)
new = np.subtract(new, mean)
返回 np.dot(new, mv)

def Euclidean_distance(v1, v2):
返回np.sqrt(np.sum(np.power((v1 - v2), 2)))

def find_similar(image, tagged):
distances = [Euclidean_distance(image, m) for m in tagged]
return np.argmin(distances)

def leave_one_out(images, k):
if len(images) == 0:
print(&quot;Error: 没有要处理的图像在 leave_one_out&quot;)
return 0
correct_predictions = 0
n = len(images)

for leave_out_index in range(n):
test_image, test_image_index = images[leave_out_index]
train_images = images[:leave_out_index] + images[leave_out_index + 1:]

vector = np.array([image_as_row(image[0]) for image in train_images])
mean_train = vector.mean(axis=0)
diff = np.subtract(vector, mean_train)
cov = covariance(diff) / len(train_images)
mv = eigenvector(cov, k)

mapped_train = np.dot(diff, mv)
new_image = im_map(test_image, mean_train, mv)

index = find_similar(new_image,mapped_train)
print(f&quot;测试图像 {test_image_index} 与训练图像 {index} 最相似&quot;)

if leave_out_index == index:
correct_predictions += 1

recognition_rate = correct_predictions / n
return identification_rate

def main():
base_path = &#39;Yaledatabase_full/data&#39;
num_sets = 10
images_per_set = 10
k = 10 # Number主成分

all_recognition_rates = []
for set_index in range(1, num_sets + 1):
set_path = os.path.join(base_path, f&quot;{set_index:01d}&quot;)
images, filenames = read_images(set_path)

print(f&quot;Images in set {set_index}:&quot;)
for idx, filename in enumerate(filenames):
print(f&quot;Index: {idx}, Filename: {filename}&quot;)

recognition_rate = leave_one_out(images, k)
all_recognition_rates.append(recognition_rate)
print(f&#39;Recognition rate for set {set_index}: {recognition_rate * 100:.2f}%&#39;)
print(&#39;---------------------------------------------&#39;)

overall_recognition_rate = np.mean(all_recognition_rates)
print(&#39;---------------------------------------------&#39;)
print(f&#39;总体识别率为 {overall_recognition_rate * 100:.2f}%&#39;)

if __name__ == &quot;__main__&quot;:
main()

我想知道，当有 100 个特征向量时，基于 PCA 技术的人脸识别的识别率是否约为 40%，如各种论文所示。]]></description>
      <guid>https://stackoverflow.com/questions/78775565/i-want-to-use-pca-based-face-recognition-for-yaledatabase-and-measure-the-recogn</guid>
      <pubDate>Sun, 21 Jul 2024 15:13:11 GMT</pubDate>
    </item>
    <item>
      <title>计算计算机断层扫描的内轮廓</title>
      <link>https://stackoverflow.com/questions/78775537/calculate-the-inner-contour-of-a-computed-tomography</link>
      <description><![CDATA[我正在做一个计算胸部畸形程度的项目。
为此，我需要从断层扫描中找到胸部的内轮廓，但我不知道如何选择内轮廓。
首先，我将图像转换为二进制并应用阈值。
import cv2
import numpy as np
from PIL import Image

import matplotlib.pyplot as plt

def process_and_plot(image_path):
def process(image_path):
image = cv2.imread(image_path)
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
blur_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
_, binary_image = cv2.threshold(blurred_image, 127, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
return binary_image

fig,axes = plt.subplots(1, 2,figsize=(10, 5))

img = Image.open(image_path)
axes[0].imshow(img,cmap=&#39;gray&#39;)
axes[0].set_title(&#39;Original Image&#39;)

processed_image = process(image_path)
axes[1].imshow(processed_image,cmap=&#39;gray&#39;) 
axes[1].set_title(&#39;Processed Image&#39;)

plt.show()

process_and_plot(&#39;test.jpeg&#39;)



我期望得到红色轮廓。
]]></description>
      <guid>https://stackoverflow.com/questions/78775537/calculate-the-inner-contour-of-a-computed-tomography</guid>
      <pubDate>Sun, 21 Jul 2024 15:02:37 GMT</pubDate>
    </item>
    <item>
      <title>哪里可以最好地学习 MLX 进行深度学习？[关闭]</title>
      <link>https://stackoverflow.com/questions/78775410/where-to-best-learn-mlx-for-deep-learning</link>
      <description><![CDATA[我仍在参加在线培训课程，以学习深度学习，正如你们所知，我还有很多其他东西要学。
但作为 MacOS 的粉丝，我希望能够使用 Apple 的 MLX，我相信这是一个非常好的模型模拟框架，许多人都说这是深度学习工程和研究中非常重要的一步。
有人能给我指出一个好的资源来学习 MLX 框架吗？我在网上找到了非常基础且不太详细的训练营，但 MLX 似乎比任何这些资源愿意分享的都要完整得多。
PyTorch、Keras 和 TensorFlow 在网上都面临相同的基本介绍，但由于它们更受欢迎，因此也不难找到比大多数课程更深入的在线培训课程。
我希望有人可以帮助我，如果您有任何意见或建议，我会非常乐意听到并向大家学习。
谢谢，
Alexandre。
我试图在网上找到好的、深入的培训课程，通过Coursera 或 Udemy 以及 Medium 和 YouTube 等网站，但每门课程都只向学习者介绍非常基础的深度学习和机器学习 MLX 功能。]]></description>
      <guid>https://stackoverflow.com/questions/78775410/where-to-best-learn-mlx-for-deep-learning</guid>
      <pubDate>Sun, 21 Jul 2024 14:06:48 GMT</pubDate>
    </item>
    <item>
      <title>时间序列模型中的极端 RMSE 验证分数</title>
      <link>https://stackoverflow.com/questions/78775154/extreme-rmse-validation-score-in-time-series-model</link>
      <description><![CDATA[我有一个包含每小时数据的数据集，有 4000 行和 3 个数字列。它没有明显的趋势或季节性。我在下面分享了 stl 分解图以及一个目标列的原始线图。
这是一列目标的图表
我使用此代码进行训练：
tscv = TimeSeriesSplit(n_splits=5,gap=24)
train_rmse_scores = []
val_rmse_scores = []

for fold, (train_index, test_index) in enumerate(tscv.split(melen_forecast_df), start=1):
train_df, test_df = melen_forecast_df.iloc[train_index], melen_forecast_df.iloc[test_index]

# 仅使用训练集创建特征
X_train = create_all_features(train_df)
y_train = train_df[targets]

# 为测试集创建特征
X_test = create_all_features(test_df)
y_test = test_df[targets]

X_train = X_train.iloc[4:]
y_train = y_train.iloc[4:]
X_test = X_test.iloc[4:]
y_test = y_test.iloc[4:]

print(f&quot;Fold {fold} - 训练数据形状：{X_train.shape}, 验证数据形状：{X_test.shape}&quot;)

model = MultiOutputRegressor(xgb.XGBRegressor(objective=&#39;reg:squarederror&#39;,n_estimators=500,learning_rate=0.015))
# 训练模型
model.fit(X_train, y_train)

# 预测并计算训练集的 RMSE
y_train_pred = model.predict(X_train)
train_rmse_per_target = [np.sqrt(mean_squared_error(y_train.iloc[:, i], y_train_pred[:, i])) 
for i in range(y_train.shape[1])]
train_rmse_scores.append(train_rmse_per_target)

for i, rmse in enumerate(train_rmse_per_target, start=1):
print(f&quot;Fold {fold} - Training Target {i} RMSE: {rmse:.4f}&quot;)

# 预测并计算验证集的 RMSE
y_pred = model.predict(X_test)
val_rmse_per_target = [np.sqrt(mean_squared_error(y_test.iloc[:, i], y_pred[:, i])) 
for i in range(y_test.shape[1])]
val_rmse_scores.append(val_rmse_per_target)

for i, rmse in enumerate(val_rmse_per_target, start=1):
print(f&quot;Fold {fold} - Validation Target {i} RMSE: {rmse:.4f}&quot;)

# 计算并打印所有折叠的平均 RMSE
avg_train_rmse = np.mean(train_rmse_scores, axis=0)
avg_val_rmse = np.mean(val_rmse_scores, axis=0)

我正在尝试使用 MultiOutputRegressor 训练这 3 个目标列，包装XGBoostRegressor。我的问题是 RMSE 分数对于训练和验证来说都太高了。即使使用 optuna 调整超参数后，效果似乎也不好。我甚至看到验证 RMSE 等于 7000。此时我无法确定我的模型是过度拟合还是欠拟合，因为如果我开始使用基本特征进行训练，我的验证分数会从非常高的验证 RMSE 开始，然后继续下去。我想我在某个时候犯了严重的错误。]]></description>
      <guid>https://stackoverflow.com/questions/78775154/extreme-rmse-validation-score-in-time-series-model</guid>
      <pubDate>Sun, 21 Jul 2024 12:14:40 GMT</pubDate>
    </item>
    <item>
      <title>我正在进行多类分类，但是在如何通过参数调整进行特征和模型选择方面遇到了困难</title>
      <link>https://stackoverflow.com/questions/78775112/i-am-doing-multiclass-classification-but-i-am-having-hard-time-with-how-to-do-t</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78775112/i-am-doing-multiclass-classification-but-i-am-having-hard-time-with-how-to-do-t</guid>
      <pubDate>Sun, 21 Jul 2024 11:54:53 GMT</pubDate>
    </item>
    <item>
      <title>尽管训练准确率很高，Transformer 模型在推理过程中仍重复相同的密码子</title>
      <link>https://stackoverflow.com/questions/78775009/transformer-model-repeating-same-codon-during-inference-despite-high-training-ac</link>
      <description><![CDATA[我正在开发一个基于转换器的模型，将氨基酸转化为密码子。在训练和验证过程中，我的模型的准确率达到了 95-98%。然而，在推理过程中，我遇到了一个问题，即输出序列由一遍又一遍重复的相同密码子组成，如下所示：
gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc ...
这是我的推理代码：
定义推理函数
def infer(model, path_infer, start_token_id=2, output_level=&#39;DNA&#39;): 
if output_level == &#39;DNA&#39;:
tokenizer = Tokenizer.from_file(tokenizer_DNAfile)
else:
tokenizer = Tokenizer.from_file(tokenizer_RNAfile)
model.eval()
infer_data = s.fasta_to_list(path_infer, seq_to_codon=False, Separate_aa=True, sos_eos=False)
fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_AAfile)
src = fast_tokenizer.encode(infer_data[0], padding=&#39;max_length&#39;, return_tensors=&quot;pt&quot;, max_length=config[&#39;input_length&#39;])
max_length = torch.count_nonzero(src).item()

使用 torch.no_grad():
src_embed = model.src_embedding(src)
memory = model.encoder(src_embed)
tgt_input = torch.LongTensor([[start_token_id]])
output_sequence = [start_token_id]

for _ in range(max_length):
tgt_embed = model.tgt_embedding(tgt_input)
coder_output = model.decoder(memory, tgt_embed)
output_logits = model.fc_out(decoder_output[:, -1, :])
next_token = output_logits.argmax(dim=-1).item()
output_sequence.append(next_token)
tgt_input = torch.cat((tgt_input, torch.LongTensor([[next_token]])), dim=1)

return tokenizer.decode(output_sequence[1:], skip_special_tokens=True)

loading_path = os.path.join(checkpoint_dir, config[&#39;model_name&#39;])
model.load_checkpoint(loading_path, model, optimizer)
z = infer(model, path_infer)
print(z)

详细信息：

框架： PyTorch

模型：Transformer

Tokenizer：用于氨基酸的 PreTrainedTokenizerFast，用于 DNA/RNA 的自定义 tokenizer

训练准确率：95-98%

推理输出：重复序列（例如，“gcc gcc gcc...”）


我已采取的步骤：

检查训练和验证阶段：该模型表现良好，准确率高。

检查推理代码：确保它遵循标准 Transformer 推理实践。


问题：
在推理过程中，模型输出重复的密码子，这是在训练和验证期间的高精度下不期望的。
问题：

什么可能导致模型在推理过程中生成重复序列？

Transformer 推理中是否存在可能导致此行为的常见陷阱？

我如何修改推理函数以解决此问题？


其他信息：

使用 argmax 进行下一个标记选择。

模型已正确加载并设置为评估模式。


任何见解或建议都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78775009/transformer-model-repeating-same-codon-during-inference-despite-high-training-ac</guid>
      <pubDate>Sun, 21 Jul 2024 11:06:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在 8 个 GPU 上并行化机器翻译的 Transformer 模型？</title>
      <link>https://stackoverflow.com/questions/78774602/how-to-parallelize-transformer-model-for-machine-translation-on-8-gpus</link>
      <description><![CDATA[我正尝试使用 transformer 模型以与原始文章几乎相同的方式执行机器翻译。虽然该模型运行良好，但它需要更大的计算资源。为了解决这个问题，我在一台有 8 个 GPU 处理器的计算机上运行了该模型，但我缺乏这方面的经验。
我尝试对并行化进行必要的调整：
transformer = nn.DataParallel(transformer)
transformer = transformer.to(DEVICE)

然而，由于我缺乏经验，事情进展不顺利。具体来说，我在以下错误消息上停留了很长时间：
File &quot;C:\Projects\MT005\.venv\Lib\site-packages\torch\nn\ functional.py&quot;, line 5382, 
in multi_head_attention_forward raise RuntimeError(f&quot;2D attn_mask 的形状是 {attn_mask.shape}，但应该是 {correct_2d_size}。&quot;) 
RuntimeError: 2D attn_mask 的形状是 torch.Size([8, 64])，但应该是 (4, 4)。

有人能帮我解决这个问题并让模型在所有 8 个 GPU 上运行吗？]]></description>
      <guid>https://stackoverflow.com/questions/78774602/how-to-parallelize-transformer-model-for-machine-translation-on-8-gpus</guid>
      <pubDate>Sun, 21 Jul 2024 07:14:42 GMT</pubDate>
    </item>
    <item>
      <title>如何在评估商业项目时实施 NLP 进行文本分析？</title>
      <link>https://stackoverflow.com/questions/78773575/how-to-implement-nlp-for-text-analysis-in-evaluating-business-projects</link>
      <description><![CDATA[我需要根据特定标准评估业务活动（项目）的资格。我们通过采访利益相关者来收集数据，获取项目名称、描述、不确定性和结果等详细信息。然后根据这些叙述评估每个项目的资格。
我正在考虑将数据科学融入该项目的几种方法，并希望得到有关最佳方法的建议。具体来说，我有兴趣实施 NLP 进行文本分析：

如何分析项目描述以确定共同主题和关键术语？

推荐使用哪些工具和库来使用主题建模来发现共同主题和文本分类来对项目进行分类？


我正在考虑的其他方法包括预测建模、聚类分析和情绪分析。如果您对这些方法有任何建议或资源，那也会很有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78773575/how-to-implement-nlp-for-text-analysis-in-evaluating-business-projects</guid>
      <pubDate>Sat, 20 Jul 2024 18:23:40 GMT</pubDate>
    </item>
    <item>
      <title>如何将职位名称与职位空缺名称或职位空缺描述进行匹配？</title>
      <link>https://stackoverflow.com/questions/78772979/how-to-match-job-title-with-vacancies-name-or-vacancy-descriptions</link>
      <description><![CDATA[如何将 400 个职业与 10,000 个职位空缺进行匹配？我有两个文件：一个文件包含职业名称及其所属部门，第二个文件是来自 hh.kz 的 10,000 个职位空缺，包含职位名称及其描述。我需要将 400 个职业分配到适当的职位空缺，例如，将“高级前端开发人员”与“Web 开发人员”、“UI/UX 设计师”与“Web 设计师”等进行匹配。我已经清理和规范化了数据，使用了词嵌入，但效果不佳。我还能尝试什么？
我尝试使用关键字，但对我来说不起作用]]></description>
      <guid>https://stackoverflow.com/questions/78772979/how-to-match-job-title-with-vacancies-name-or-vacancy-descriptions</guid>
      <pubDate>Sat, 20 Jul 2024 14:06:15 GMT</pubDate>
    </item>
    <item>
      <title>通过向 CNN 输入添加位置和字符信息来增强文档布局分析</title>
      <link>https://stackoverflow.com/questions/78739816/enhancing-document-layout-analysis-by-adding-positional-and-character-informatio</link>
      <description><![CDATA[我正在研究文档布局分析，并一直在探索 CNN 和基于 Transformer 的网络来完成这项任务。通常，图像作为 3 通道 RGB 输入传递给这些网络。但是，我的数据源是 PDF 格式，我可以直接从中提取准确的位置和字符信息。
我担心将这些 PDF 数据转换为图像进行分析会导致宝贵的位置和字符信息丢失。我的想法是将 CNN 的输入维度从标准的 3 RGB 通道修改为包含这些额外位置和字符信息的更高维度输入。
我了解 CNN 的工作原理，并高度怀疑这种方法可能行不通，但我很感谢社区的任何反馈或建议。有没有人尝试过以这种方式增强输入通道，或者有没有人对将位置和字符数据直接集成到 CNN 中有什么见解？]]></description>
      <guid>https://stackoverflow.com/questions/78739816/enhancing-document-layout-analysis-by-adding-positional-and-character-informatio</guid>
      <pubDate>Fri, 12 Jul 2024 10:17:29 GMT</pubDate>
    </item>
    <item>
      <title>Python机器学习pytorch测试/训练epoch结果问题</title>
      <link>https://stackoverflow.com/questions/77977231/python-machine-learning-pytorch-test-train-epoch-results-problem</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77977231/python-machine-learning-pytorch-test-train-epoch-results-problem</guid>
      <pubDate>Sun, 11 Feb 2024 15:07:50 GMT</pubDate>
    </item>
    <item>
      <title>LightGBM 用于特征选择</title>
      <link>https://stackoverflow.com/questions/62771535/lightgbm-for-feature-selection</link>
      <description><![CDATA[我正在研究二元分类问题，我的训练数据有数百万条记录和约 2000 个变量。我正在运行 lightGBM 进行特征选择，并使用从 lightGBM 中选择的特征来运行神经网络（使用 Keras）模型进行预测。我对所采用的方法有几个问题。

在使用 lightGBM 进行特征选择时，我正在执行超参数调整。这是基于我的理解，即随着超参数的变化，所选特征也会有所不同。我使用“goss”算法和“gain”作为特征重要性类型。我看过几篇文章，其中他们使用 lightGBM 进行特征选择，但我没有看到任何进行超参数调整的文章，他们只是使用默认设置。这是正确的方法吗？
使用 lightGBM 进行特征选择，使用神经网络根据从 lightGBM 中选择的特征构建预测模型可以吗？

非常感谢您的帮助。谢谢]]></description>
      <guid>https://stackoverflow.com/questions/62771535/lightgbm-for-feature-selection</guid>
      <pubDate>Tue, 07 Jul 2020 08:56:03 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn 聚类：预测（X）与 fit_predict（X）</title>
      <link>https://stackoverflow.com/questions/37106983/scikit-learn-clustering-predictx-vs-fit-predictx</link>
      <description><![CDATA[在 scikit-learn 中，一些聚类算法同时具有 predict(X) 和 fit_predict(X) 方法，例如 KMeans 和 MeanShift，而其他算法仅具有后者，例如 SpectralClustering。根据文档：
fit_predict(X[, y]): 对 X 执行聚类并返回聚类标签。
predict(X): 预测 X 中每个样本所属的最接近聚类。

我不太明白这两者之间的区别，在我看来它们似乎是等价的。]]></description>
      <guid>https://stackoverflow.com/questions/37106983/scikit-learn-clustering-predictx-vs-fit-predictx</guid>
      <pubDate>Mon, 09 May 2016 02:25:29 GMT</pubDate>
    </item>
    </channel>
</rss>