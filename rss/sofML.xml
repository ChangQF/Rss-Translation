<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 19 Dec 2023 18:13:57 GMT</lastBuildDate>
    <item>
      <title>python中的矩阵乘法得到成本函数</title>
      <link>https://stackoverflow.com/questions/77686666/matrix-multiplication-in-python-to-get-cost-function</link>
      <description><![CDATA[谁能告诉我为什么我的代码给出了错误的输出？
评论的是我的。
# 分级函数：cofi_cost_func
#UNQ_C1

def cofi_cost_func(X, W, b, Y, R, lambda_):
    ”“”
    返回基于内容的过滤的成本
    参数：
      X (ndarray (num_movies,num_features))：项目特征矩阵
      W (ndarray (num_users,num_features)) ：用户参数矩阵
      b (ndarray (1, num_users) ：用户参数向量
      Y (ndarray (num_movies,num_users) ：电影用户评分矩阵
      R (ndarray (num_movies,num_users) ：矩阵，其中 R(i, j) = 1 如果第 i 个电影由第 j 个用户评分
      lambda_ (float): 正则化参数
    返回：
      J（浮点数）：成本
    ”“”
    nm, nu = Y.shape
    J = 0
    ### 从这里开始代码 ###
    
# J+= np.sum(np.square((R*(X@(W.T)+b)-Y)))/2
# J+= (lambda_/2)*np.sum(np.square(W))
# J+= (lambda_/2)*np.sum(np.square(X))
        
    对于 j 在范围内（nu）：
        w = W[j,:]
        b_j = b[0,j]
        对于范围内的 i（nm）：
            x = X[i,:]
            y = Y[i,j]
            r = R[i,j]
            J += np.square(r * (np.dot(w,x) + b_j - y ) )
    J = J/2
    J += (lambda_/2) * (np.sum(np.square(W)) + np.sum(np.square(X)))
            
            
            
    
    
    ### 在此结束代码 ###

    返回J

我不知道为什么使用我的成本函数代码测试会失败，我只是尝试在不使用循环的情况下做同样的事情。
 J+= np.sum(np.square((R*(X@(W.T)+b)-Y)))/2
    J+= (lambda_/2)*np.sum(np.square(W))
    J+= (lambda_/2)*np.sum(np.square(X))
]]></description>
      <guid>https://stackoverflow.com/questions/77686666/matrix-multiplication-in-python-to-get-cost-function</guid>
      <pubDate>Tue, 19 Dec 2023 16:34:22 GMT</pubDate>
    </item>
    <item>
      <title>机器学习后指标结果相同的问题</title>
      <link>https://stackoverflow.com/questions/77686328/problem-with-identical-metrics-results-after-machine-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77686328/problem-with-identical-metrics-results-after-machine-learning</guid>
      <pubDate>Tue, 19 Dec 2023 15:36:36 GMT</pubDate>
    </item>
    <item>
      <title>有什么想法可以让 scikit 学得更快吗？它不断使内核崩溃</title>
      <link>https://stackoverflow.com/questions/77686255/any-ideas-how-to-make-scikit-learn-faster-it-keeps-crashing-the-kernel</link>
      <description><![CDATA[我的内核在这个单元格中不断死亡：
hyper_params = {&#39;outlier_neighbors&#39;:3, &#39;base_lambda&#39;:1e9, &#39;base_p&#39;:0.0005, &#39;smoothing_window&#39;:2,&#39;smoothing_order&#39;:1,
                &#39;diff_ma_window&#39;：2，&#39;diff_rhl_window&#39;：2，&#39;eps&#39;：0.001，&#39;ms&#39;：2，&#39;regress_portion&#39;：（0,1）}
#parameters 我建议采样频率为 1 分钟的数据

outlier_neighbors = hyper_params[&#39;outlier_neighbors&#39;]
base_lambda = hyper_params[&#39;base_lambda&#39;]
base_p = hyper_params[&#39;base_p&#39;]
smoothing_window = hyper_params[&#39;smoothing_window&#39;]
smoothing_order = hyper_params[&#39;smoothing_order&#39;]
diff_ma_window = hyper_params[&#39;diff_ma_window&#39;]
diff_rhl_window = hyper_params[&#39;diff_rhl_window&#39;]

污染物 = &#39;CO2&#39; #此处指定浓度时间序列的列名称
date_time = &#39;日期/时间&#39; #and 用于您的时间戳列

有没有办法让 scikit learn 库运行得更快或更有效，这样就不会杀死内核？]]></description>
      <guid>https://stackoverflow.com/questions/77686255/any-ideas-how-to-make-scikit-learn-faster-it-keeps-crashing-the-kernel</guid>
      <pubDate>Tue, 19 Dec 2023 15:22:52 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Matlab 中从线性 SVM 导出权重和偏移量</title>
      <link>https://stackoverflow.com/questions/77685684/how-to-derive-weights-and-offsets-from-linear-svm-in-matlab</link>
      <description><![CDATA[Matlab训练的线性SVM模型如下：
阿尔法
测试版
偏差
穆
西格玛
支持向量
支持向量标签
请问如何使用上述参数来计算超平面的权重和偏移？
超平面：f(x) = 符号((w,x) + b)]]></description>
      <guid>https://stackoverflow.com/questions/77685684/how-to-derive-weights-and-offsets-from-linear-svm-in-matlab</guid>
      <pubDate>Tue, 19 Dec 2023 13:52:53 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Hubert 模型获得音频嵌入</title>
      <link>https://stackoverflow.com/questions/77685045/how-to-get-audio-embeddings-using-hubert-model</link>
      <description><![CDATA[示例代码
进口火炬
从变压器导入 Wav2Vec2Processor、HubertForCTC
从数据集导入load_dataset

处理器 = Wav2Vec2Processor.from_pretrained(“facebook/hubert-large-ls960-ft”)
模型 = HubertForCTC.from_pretrained(“facebook/hubert-large-ls960-ft”)
input_values = 处理器(&#39;来自音频文件的数组。, return_tensors=“pt”).input_values


之后如何获得嵌入？模型中没有最后一个隐藏状态。
尝试了我提到的代码块，此外，尝试创建没有最后一层的模型，然后将其提供给输入。我的另一个问题是，假设我有不同时间维度的剪辑，那么如何创建固定嵌入。是沿着时间轴平均还是需要不同的方法。]]></description>
      <guid>https://stackoverflow.com/questions/77685045/how-to-get-audio-embeddings-using-hubert-model</guid>
      <pubDate>Tue, 19 Dec 2023 12:04:19 GMT</pubDate>
    </item>
    <item>
      <title>我实现了一个 lenet CNN 模型来对人类情感进行分类，但它只正确分类“悲伤”组 [关闭]</title>
      <link>https://stackoverflow.com/questions/77682900/i-implemented-a-lenet-cnn-model-to-classify-human-emotion-but-it-only-classifie</link>
      <description><![CDATA[我开发了一个 Lenet 模型来在 TensorFlow 中对人脸情感进行分类。我想可视化我的数据、真实标签和预测标签，但我面临各种错误。为了方便起见，我附上了所有代码。 jupyter 笔记本代码的最后一个块是我想要应用可视化的地方。
链接到我的代码：
https://github.com/mirpouya/TensorFlow -Tutorial/blob/main/Human_Emotion_Detection(可视化%20问题).ipynb
我尝试了各种具有不同参数的 Lenet 模型。]]></description>
      <guid>https://stackoverflow.com/questions/77682900/i-implemented-a-lenet-cnn-model-to-classify-human-emotion-but-it-only-classifie</guid>
      <pubDate>Tue, 19 Dec 2023 04:20:34 GMT</pubDate>
    </item>
    <item>
      <title>LSTM多类蛋白质分类模型（keras）模型疯狂的低精度[关闭]</title>
      <link>https://stackoverflow.com/questions/77682312/lstm-multiclass-protien-classification-model-keras-model-crazy-low-accuracy</link>
      <description><![CDATA[我正在做一项蛋白质分类作业。我得到了一个包含 5 个类别的不平衡数据集。每个条目都有一个总科名称和相应的序列。
我一直在遵循我的大学教学，了解使用 keras 和 sklearn 模块的正确方法和编码实践。然而，经过多次参数调整、添加和删除图层。我的模型始终预测精度较低（val_accuracy 0.2）。我依次使用 2 个双向 LSTM，然后是 relu 密集层、dropout 和带有 softmax 激活的最终密集层。
请参阅下面的代码：
导入 pandas 作为 pd
从 sklearn 导入预处理，model_selection
导入keras
将 numpy 导入为 np
将张量流导入为 tf
将tensorflow_addons导入为tfa
从 sklearn.utils 导入 class_weight
从 keras.layers 导入嵌入、Conv1D、MaxPooling1D
从 keras.layers 导入 Dropout、Flatten、Dense
从 keras.layers 导入 LSTM、双向、GRU、SimpleRNN、MaxPooling1D、SpatialDropout1D
从 scikeras.wrappers 导入 KerasClassifier
从 sklearn.model_selection 导入 GridSearchCV、StratifiedKFold
从 imblearn.over_sampling 导入 RandomOverSampler

种子 = 123

# 读入数据并删除索引列
数据= pd.read_csv(“./results/processedData.csv”,index_col=False)

# 对 seq 进行标记化，以便 CNN 可以使用
tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)

打印（数据.描述（））

y = 数据.名称

# 将标记化值设置为 x
x = tokenizer.texts_to_sequences(data.seq)
最大长度 = 580
x = keras.preprocessing.sequence.pad_sequences(
    x，MAX_LENGTH，截断=“前”）

# 设置模型参数
令牌 = len(tokenizer.word_index) + 1
课程 = 5
单位 = 64
丢包率 = 0.2


def buildLSTM():
    模型 = keras.Sequential([
        嵌入（TOKENS，32，input_length = MAX_LENGTH，mask_zero = True），

        双向（LSTM（单位，return_sequences = True）），
        双向（LSTM（UNITS // 2）），


        密集（UNITS // 2，激活=“relu”），
        辍学率（DROPOUT_RATE），

        密集（类，激活=“softmax”）
    ]）
    model.compile(loss=“categorical_crossentropy”,
                  优化器=keras.optimizers.Adam(1e-4)，指标=[tf.keras.metrics.CategoricalAccuracy()])
    返回模型


# # 过采样
过采样 = RandomOverSampler(sampling_strategy=&#39;少数&#39;)

skf = StratifiedKFold(n_splits=5, shuffle=True)

对于 skf.split(x, y) 中的 trainI、testI：
    训练X，训练Y = x [训练I]，y [训练I]
    测试X，测试Y = x [测试I]，y [测试I]

    # 将输出设置为分类器
    labelBinarize = 预处理.LabelBinarizer()
    labelBinarize.fit(trainY)
    trainY = labelBinarize.transform(trainY)
    testY = labelBinarize.transform(testY)

    # 过采样训练集
    trainX, trainY = oversample.fit_resample(trainX, trainY)

    # 构建模型并拟合数据
    模型 = buildLSTM()
    model.fit(trainX,trainY,validation_data=(
        测试X，测试Y），纪元= 5，batch_size = 64）

    # 评估分数
    分数 = model.evaluate(testX, testY, verbose=0)
    准确度 = 分数[1] * 100
    print(f&#39;准确度：{准确度}&#39;)

# 打印摘要
打印（模型.摘要）


请参阅下面的纪元示例：
39/39 [================================] - 48s 944ms/步 - 损耗：1.6088 - 分类准确度：0.2923 - 验证损失：1.6090 - 验证分类准确度：0.0761
纪元 2/5
39/39 [================================] - 32s 825ms/步 - 损失：1.6075 - categorical_accuracy：0.2987 - val_loss ：1.6085 - val_categorical_accuracy：0.0761
纪元 3/5
39/39 [================================] - 33s 838ms/步 - 损失：1.6062 - categorical_accuracy：0.2987 - val_loss ：1.6081 - val_categorical_accuracy：0.0761
纪元 4/5
39/39 [==============================] - 34s 877ms/步 - 损失：1.6050 - categorical_accuracy：0.2987 - val_loss ：1.6077 - val_categorical_accuracy：0.0761
纪元 5/5
39/39 [================================] - 33s 845ms/步 - 损失：1.6037 - categorical_accuracy：0.2987 - val_loss ：1.6073 - val_categorical_accuracy：0.0761

如上面的代码所示，我尝试将过采样与 stratifiedkfold 一起使用来尝试纠正这些数据不平衡。此外，我尝试了不同范围的时期、批量大小和学习率。]]></description>
      <guid>https://stackoverflow.com/questions/77682312/lstm-multiclass-protien-classification-model-keras-model-crazy-low-accuracy</guid>
      <pubDate>Mon, 18 Dec 2023 23:56:24 GMT</pubDate>
    </item>
    <item>
      <title>查找文本的所有标记概率[关闭]</title>
      <link>https://stackoverflow.com/questions/77682073/finding-all-token-probabilities-of-text</link>
      <description><![CDATA[我正在玩 ROBERTA，试图找到输入文本中不可能的部分。例如，我想潜在地检测奇怪的单词选择、不正确的语法等。
我当前的方法是循环序列中的所有标记，将它们交换为掩码标记，然后运行模型。然后我可以获得 logits 的 softmax，并查找实际标记的概率：
导入火炬
从变压器导入 RobertaTokenizer、RobertaForMaskedLM

def get_token_probabilities(句子、模型、分词器):

    token_ids = tokenizer.encode(sentence, return_tensors=&#39;pt&#39;)
    结果=[]

    for i in range(1, token_ids.size(1) - 1): # 排除开始和结束标记
        masked_token_ids = token_ids.clone()
        masked_token_ids[0, i] = tokenizer.mask_token_id

        使用 torch.no_grad()：
            输出=模型（masked_token_ids）

        softmax = torch.softmax(outputs.logits[0, i], dim=0)

        原始令牌 ID = 令牌 ID[0, i]
        token_prob = softmax[original_token_id].item()

        结果.追加({
            “令牌”：tokenizer.decode([original_token_id]),
            “概率”：token_prob，
        })

    返回结果


def main():
    model_name = &#39;罗伯塔基地&#39;
    tokenizer = RobertaTokenizer.from_pretrained(model_name)
    模型 = RobertaForMaskedLM.from_pretrained(model_name)

    例子= [
        “安和保罗正在准备考试。”,
        “安和保罗正在准备考试。”,
        “安和保罗正在为考试而学习。”,
    ]

    对于示例中的句子：
        结果 = get_token_probabilities(句子、模型、分词器)
        对于结果中的条目：
            print(f&#39;{entry[&quot;token&quot;]}: {entry[&quot;probability&quot;]:.6f}&#39;)
        打印（）

如果 __name__ == &#39;__main__&#39;:
    主要的（）

结果：
&lt;前&gt;&lt;代码&gt;安：0.007430
 和：0.982790
 保罗：0.012045
 研究：0.002692
 为：0.973533
 他们的：0.077359
 考试：0.008554
.: 0.691542

安：0.005499
 和：0.871503
 保罗：0.006089
 研究：0.006135
 为：0.954329
 那里：0.000004
 考试：0.008875
.: 0.545155

安：0.003608
 和：0.965940
 保罗：0.004815
 研究：0.000095
 为：0.009223
 他们：0.013043
是：0.000293
 考试：0.000020
.: 0.220154

问题是这种方法效率不高，因为它必须针对输入中的每个标记运行。有没有更好的方法来做到这一点，也许只运行模型一次？]]></description>
      <guid>https://stackoverflow.com/questions/77682073/finding-all-token-probabilities-of-text</guid>
      <pubDate>Mon, 18 Dec 2023 22:30:18 GMT</pubDate>
    </item>
    <item>
      <title>是否可以直接在张量流上运行使用 mediapipe 重新训练的对象检测模型，而不是使用 mediapipe？</title>
      <link>https://stackoverflow.com/questions/77666162/is-it-possible-to-run-object-detection-models-retrained-with-mediapipe-on-tensor</link>
      <description><![CDATA[我正在使用此 mediapipe 指南来重新训练对象检测模型，并将其导出到 tflite 模型。我想在反应原生中使用该模型。不幸的是，mediapipe 没有直接的 React-Native 实现，但我有一个可以在 RN 中运行任何 .tflite 模型的库。
起初我以为我只需要使用 mediapipe 来重新训练我的模型，但现在我在示例中意识到我还需要 mediapipe 来进行检测部分。所以我想知道是否也可以直接在张量流中运行使用 mediapipe 创建的模型？
第一次测试后，我得到了以下形状的对象检测模型的“位置”输出：和“分数”：
&lt;前&gt;&lt;代码&gt;((1, 19125, 4), (1, 19125, 4))

位置的形状对我来说很有意义，但是我应该如何解释“分数”？数据？或者在没有媒体管道的情况下运行模型没有意义吗？]]></description>
      <guid>https://stackoverflow.com/questions/77666162/is-it-possible-to-run-object-detection-models-retrained-with-mediapipe-on-tensor</guid>
      <pubDate>Fri, 15 Dec 2023 12:08:03 GMT</pubDate>
    </item>
    <item>
      <title>如何为此笔记本创建 Sagemaker 端点？</title>
      <link>https://stackoverflow.com/questions/77660996/how-can-i-create-a-sagemaker-endpoint-for-this-notebook</link>
      <description><![CDATA[我创建了一个 VectorDB (FAISS) 并将 PDF 输入到其中。然后我使用 AWS Bedrock 的 Langchain 包装器来调用它。我知道现在存在 Kowledge Base，但至少在 SageMaker 笔记本中，我有更多的控制权。该模型在 SageMaker Notebook 中完美运行，当我提出问题时，它会返回答案。
我想做的是创建一个小网页（并通过 HTTP/REST API），只需在文本字段中提交问题并在文本字段中接收答案。我猜如果链中某个地方没有 Lambda 函数，这很难做到，或者也许不是？
当我查看 Sagemaker 控制台的推理选项卡下时，没有模型或没有端点，或者没有&lt; /strong&gt; 端点配置（因为我没有从 Sagemaker 选择模型，所以我只是在 Python 笔记本中使用 langchain LLM 和 Bedrock，如下所示）。
&lt;前&gt;&lt;代码&gt;导入boto3
导入 json

bedrock = boto3.client(service_name=&quot;bedrock&quot;)
bedrock_runtime = boto3.client(service_name=“bedrock-runtime”)



从 langchain.llms.bedrock 导入 Bedrock
从 langchain.chains 导入 RetrievalQA
从 langchain.prompts 导入 PromptTemplate

嵌入 = BedrockEmbeddings(model_id=“amazon.titan-embed-text-v1”,
                               客户端=bedrock_runtime）

最终我将文档嵌入到 FAISS Vector 数据库中，我查询的就是这个数据库
db = FAISS.from_documents（文档，嵌入）


模型泰坦 = {
    “最大令牌计数”：512，
    “停止序列”：[]，
    “温度”：0.0，
    “顶部P”：0.5
}

# 亚马逊泰坦模型
llm = 基岩(
    model_id=&quot;amazon.titan-text-express-v1&quot;,
    客户端=bedrock_runtime，
    model_kwargs=model_titan,
）

然后定义一个提示......
提示 = 提示模板(
    template=prompt_template, input_variables=[“上下文”, “问题”]
）

并查询数据库：
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=“东西”，
    检索器=db.as_retriever(
        search_type=“相似度”，
    ),
    return_source_documents=真，
    chain_type_kwargs={“提示”: 提示},
）



query =“未来的技术是什么样的？”

结果 = qa({“查询”: 查询})

print(f&#39;查询: {结果[“查询”]}\n&#39;)
print(f&#39;结果: {结果[“结果”]}\n&#39;)
print(f&#39;上下文文档：&#39;)
对于结果 [“source_documents”] 中的 srcdoc：
      打印（f&#39;{srcdoc}\n&#39;）

这恰好返回了我在 Sagemaker 中需要的内容，我只需要从外部查询数据库即可。
我不想让 lambda 函数每次都重建链。我考虑的是效率，我需要的只是在 lambda 函数中传递查询并返回结果。]]></description>
      <guid>https://stackoverflow.com/questions/77660996/how-can-i-create-a-sagemaker-endpoint-for-this-notebook</guid>
      <pubDate>Thu, 14 Dec 2023 14:49:20 GMT</pubDate>
    </item>
    <item>
      <title>Optuna 分数与 Cross_val_score？</title>
      <link>https://stackoverflow.com/questions/71778333/optuna-score-vs-cross-val-score</link>
      <description><![CDATA[optuna 的准确度分数和 cross_val_score 的分数不同。为什么会发生这种情况以及我应该选择哪个分数？
我在 cross_val_score 中使用了 optuna 中获得的超参数。
def Objective_lgb（试用版）：
    num_leaves = Trial.suggest_int(“num_leaves”, 2, 1000)
    最大深度 = Trial.suggest_int(“最大深度”, 2, 100)
    学习率 = Trial.suggest_float(&#39;学习率&#39;, 0.001, 1)
    n_estimators = Trial.suggest_int(&#39;n_estimators&#39;, 100, 2000)
    min_child_samples = Trial.suggest_int(&#39;min_child_samples&#39;, 3, 1000)
    子样本 = Trial.suggest_float(&#39;子样本&#39;, 0.000001, 1)
    colsample_bytree = Trial.suggest_float(&#39;colsample_bytree&#39;, 0.00000001, 1)
    reg_alpha = Trial.suggest_float(&#39;reg_alpha&#39;, 0, 400)
    reg_lambda = Trial.suggest_float(“reg_lambda”, 0, 400)
    important_type = Trial.suggest_categorical(&#39;importance_type&#39;, [&quot;split&quot;, &quot;gain&quot;])

    lgb_clf = lgb.LGBMClassifier(random_state=1,
                         目标=“多类”，
                         类数 = 3,
                         重要性类型=重要性类型，
                         num_leaves=num_leaves,
                         最大深度=最大深度，
                         学习率=学习率，
                         n_估计器=n_估计器，
                         min_child_samples=min_child_samples,
                         子样本=子样本，
                         colsample_bytree=colsample_bytree,
                         reg_alpha=reg_alpha,
                         reg_lambda=reg_lambda
                         ）
    分数 = cross_val_score(lgb_clf, train_x, train_y, n_jobs=-1, cv=KFold(n_splits=10, shuffle=True, random_state=1), 评分=&#39;准确度&#39;)
    mean_score = 分数.mean()
    返回平均值
lgb_study = optuna.create_study(方向=“最大化”)
lgb_study.optimize(objective_lgb, n_Trials=1500)

lgb_试验 = lgb_study.best_试验
print(&quot;准确度：&quot;, lgb_Trial.value)
打印（）
print(&quot;最佳参数：&quot;, lgb_Trial.params)
=================================================== =======
def light_check(x,params):
     模型 = lgb.LGBMClassifier()
     分数 = cross_val_score(模型,x,y,cv=KFold(n_splits=10, shuffle=True, random_state=1),n_jobs=-1)
     平均值 = 分数.mean()
     返回分数，平均值
light_check(x,{&#39;num_leaves&#39;: 230, &#39;max_depth&#39;: 53, &#39;learning_rate&#39;: 0.04037430031226232, &#39;n_estimators&#39;: 1143, &#39;min_child_samples&#39;: 381, &#39;子样本&#39;: 0.12985990464862135, &#39;colsample_bytree&#39;: 0.8914118949904919, &#39;reg_alpha&#39; : 31.869348047391053, &#39;reg_lambda&#39;: 17.45653692887209, &#39;importance_type&#39;: &#39;分割&#39;})
]]></description>
      <guid>https://stackoverflow.com/questions/71778333/optuna-score-vs-cross-val-score</guid>
      <pubDate>Thu, 07 Apr 2022 07:56:04 GMT</pubDate>
    </item>
    <item>
      <title>如何修复“TypeError：无法将值转换为 TensorFlow DType”？</title>
      <link>https://stackoverflow.com/questions/67134610/how-to-fix-typeerror-cannot-convert-the-value-to-a-tensorflow-dtype</link>
      <description><![CDATA[我尝试为手写数字数据集构建 GAN（生成对抗网络），但遇到与张量数据类型错误相关的问题。

第 78 行，I 使用“tf.cast”将所有图像转换为 float32。
对于生成器和判别器损失，我使用了“tf.losses.BinaryCrossentropy”。

我不知道错误发生的位置。
这是完整的代码
导入tensorflow为tf
将 matplotlib.pyplot 导入为 plt
将 scipy.io 导入为 sio
从 sklearn.model_selection 导入 train_test_split
将 numpy 导入为 np

（火车，标签），（测试，label_test）= tf.keras.datasets.mnist.load_data（）

train_images = train.reshape(train.shape[0], 28, 28, 1)
训练图像 = (训练图像-127.5)/127.5

BUFFER_SIZE = train.shape[0]
批量大小 = 100
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

def 鉴别器():
    模型 = tf.keras.Sequential()
    model.add(tf.keras.layers.Conv2D(7, (3,3), padding=&#39;相同&#39;, input_shape=(28, 28, 1)))
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.LeakyReLU(0.1))
    model.add（tf.keras.layers.Dense（128，激活=&#39;relu&#39;））
    model.add(tf.keras.layers.Dense(1))
    返回模型

dis_model = 判别器()
disk_opt = tf.optimizers.Adam()

def discriminator_loss(y_pred_real, y_pred_fake):
    真实 = tf.sigmoid(y_pred_real)
    假 = tf.sigmoid(y_pred_fake)
    fake_loss = tf.losses.binary_crossentropy(tf.ones_like(y_pred_real), y_pred_real)
    real_loss = tf.losses.binary_crossentropy(tf.zeros_like(y_pred_fake), y_pred_fake)
    返回假损失+真实损失

def 生成器():
    模型 = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(7*7*256, input_shape=(100,)))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Reshape((7, 7, 256)))
    model.add(tf.keras.layers.Conv2DTranspose(128, (3, 3), padding=&#39;相同&#39;))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Conv2DTranspose(64, (3,3), strides=(2,2), padding=&#39;相同&#39;))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Conv2DTranspose(1, (3,3), strides=(2,2), padding=&#39;相同&#39;))
    返回模型

gen_model = 生成器()
gen_opt = tf.optimizers.Adam()

def生成器损失（fake_pred）：
    损失 = tf.sigmoid(fake_pred)
    fake_loss = tf.losses.BinaryCrossentropy(tf.zeros_like(fake_pred), fake_pred)
    返回假损失

def train_steps(图像):
    fake_noise = np.random.randn(BATCH_SIZE, 100).astype(&#39;float32&#39;)
    将 tf.GradientTape() 作为 gen_tape，tf.GradientTape() 作为 disk_tape：
        生成的图像 = gen_model(假噪声)

        fake_output = dis_model(生成的图像)
        real_output = dis_model(图像)

        gen_loss = 生成器损失（fake_output）
        Disc_loss = discriminator_loss(real_output, fake_output)

        gen_gradient = gen_tape.gradient(gen_loss, gen_model.trainable_variables)
        Disc_gradient = Disc_tape.gradient(disc_loss, dis_model.trainable_variables)

        gen_opt.apply_gradients(zip(gen_gradient, gen_model.trainable_variables))
        disk_opt.apply_gradients(zip(disc_gradient, dis_model.trainable_variables))

        print(&#39;disc_loss: &#39;, np.mean(disc_loss))
        print(&#39;gen_loss: &#39;, np.mean(gen_loss))

def train（数据集，纪元）：
    对于范围（纪元）内的 j：
        对于数据集中的图像：
            图像 = tf.cast(图像, tf.dtypes.float32)
            train_steps（图像）


火车（火车数据集，2）

发生错误
回溯（最近一次调用最后一次）：
  文件“D:/GANs_handwriting/model.py”，第 81 行，位于  中。
    火车（火车数据集，2）
  文件“D:/GANs_handwriting/model.py”，第 78 行，训练中
    train_steps（图像）
  文件“D:/GANs_handwriting/model.py”，第 66 行，train_steps 中
    gen_gradient = gen_tape.gradient（gen_loss，gene.trainable_variables）
  文件“C:\Users\Devanshu\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\eager\backprop.py”，第 1047 行，渐变
    如果不是 backprop_util.IsTrainable(t)：
  文件“C:\Users\Devanshu\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\eager\backprop_util.py”，第 30 行，在 IsTrainable 中
    dtype = dtypes.as_dtype(dtype)
  文件“C:\Users\Devanshu\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\framework\dtypes.py”，第 649 行，在 as_dtype 中
    raise TypeError(“无法将值 %r 转换为 TensorFlow DType。” %
类型错误：无法转换值 到 TensorFlow DType。
]]></description>
      <guid>https://stackoverflow.com/questions/67134610/how-to-fix-typeerror-cannot-convert-the-value-to-a-tensorflow-dtype</guid>
      <pubDate>Sat, 17 Apr 2021 04:16:49 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn cross_val_score 给出的数字与 model.score 明显不同？</title>
      <link>https://stackoverflow.com/questions/61937500/sklearn-cross-val-score-gives-significantly-different-number-than-model-score</link>
      <description><![CDATA[我有一个二元分类问题
首先，我将数据训练测试分割为：
X_train、X_test、y_train、y_test = train_test_split(X、y、random_state=42)

我检查了 y_train，它基本上有两个类 (1,0) 的 50/50 分割，这就是数据集的方式
当我尝试基本模型时，例如：
模型 = RandomForestClassifier()
model.fit(X_train, y_train)
model.score(X_train, y_train)

输出为 0.98 或 1% 的差异，具体取决于训练测试分割的随机状态。 
但是，当我尝试 cross_val_score 时，例如：
cross_val_score(model, X_train, y_train, cv=StratifiedKFold(shuffle=True), rating=&#39;accuracy&#39;)

输出为
数组([0.65, 0.78333333, 0.78333333, 0.66666667, 0.76666667])

数组中没有一个分数接近 0.98？
当我尝试得分 = &#39;r2&#39; 时，我得到了
&gt;&gt;&gt;&gt;cross_val_score(model, X_train, y_train, cv=StratifiedKFold(shuffle=True), rating=&#39;r2&#39;)
数组([-0.20133482,-0.00111235,-0.2,-0.2,-0.13333333])

有谁知道为什么会这样吗？我尝试过 Shuffle = True 和 False 但没有帮助。
提前致谢]]></description>
      <guid>https://stackoverflow.com/questions/61937500/sklearn-cross-val-score-gives-significantly-different-number-than-model-score</guid>
      <pubDate>Thu, 21 May 2020 15:04:38 GMT</pubDate>
    </item>
    <item>
      <title>我是给 cross_val_score() 整个数据集还是只提供训练集？</title>
      <link>https://stackoverflow.com/questions/52249158/do-i-give-cross-val-score-the-entire-dataset-or-just-the-training-set</link>
      <description><![CDATA[由于该类的文档不是很清楚。我不明白我赋予它什么价值。

&lt;块引用&gt;
  cross_val_score（估计器，X，y=无）

这是我的代码：
clf = LinearSVC(random_state=seed, **params)
cvscore = cross_val_score(clf, 特征, 标签)

我不确定这是否正确，或者我是否需要提供 X_train 和 y_train 而不是特征和标签。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/52249158/do-i-give-cross-val-score-the-entire-dataset-or-just-the-training-set</guid>
      <pubDate>Sun, 09 Sep 2018 22:39:32 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中参数、特征和类之间的区别</title>
      <link>https://stackoverflow.com/questions/35819869/difference-between-parameters-features-and-class-in-machine-learning</link>
      <description><![CDATA[我是机器学习和自然语言处理方面的新手。
我总是对这三个术语感到困惑？
据我了解：
class：我们的模型输出的各种类别。给出一个人的名字，确定他/她是男性还是女性？
假设我正在使用朴素贝叶斯分类器。
我的功能和参数是什么？
此外，上述单词的一些可互换使用的别名是什么。
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/35819869/difference-between-parameters-features-and-class-in-machine-learning</guid>
      <pubDate>Sat, 05 Mar 2016 21:02:05 GMT</pubDate>
    </item>
    </channel>
</rss>