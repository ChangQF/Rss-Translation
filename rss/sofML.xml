<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Thu, 05 Sep 2024 09:16:23 GMT</lastBuildDate>
    <item>
      <title>分类模型仅预测单个类别</title>
      <link>https://stackoverflow.com/questions/78952023/classification-model-only-predicts-single-class</link>
      <description><![CDATA[我有一个逻辑回归模型，我正在尝试在我的 NLP 项目中做出健康的预测。我做了一些事情，下面是我的分类报告：
 准确率 召回率 f1 分数 支持率

0 0.68 0.83 0.75 23
1 0.78 0.78 0.78 23
2 0.87 0.87 0.87 30
3 0.76 0.62 0.68 26

准确率 0.77 102
宏平均值 0.77 0.77 0.77 102
加权平均值 0.78 0.77 0.77 102

我认为它实际上没有看起来那么糟糕，但即使我以不同的方式设置参数，大多数时候它也会返回“3”，这是我的一个类。我的意思是，它看起来总是专注于一个单一的类别，即使我的分类报告不支持这种行为。
总之，我的模型看起来很平衡，但表现却不是那样。我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78952023/classification-model-only-predicts-single-class</guid>
      <pubDate>Thu, 05 Sep 2024 08:30:57 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用 Google Teachable 机器模型作为对象检测模型吗？</title>
      <link>https://stackoverflow.com/questions/78951811/can-i-use-google-teachable-machine-model-as-object-detection-model</link>
      <description><![CDATA[我正在为我的论文项目工作，这是一个带有移动对象检测功能的自动收银机应用程序。我面临的问题是，自定义项目的变体太多了（大约 250 个类别）。如果我要微调以前的模型（如 MobileNet 或 YOLO），这将花费太多时间，因为我仍然需要创建 POS、数据库和集成热敏打印机。
那么，我是否可以只使用 Google Teachable Machine 作为我的对象检测数据集（假设背景相同），而不是微调以前的模型？
应用程序的工作方式是，收银员会在白色背景上拍摄买家想要购买的商品的照片，然后应用程序会自动检测出哪些商品在框架内（使用 Google Teachable Machine .tflite 模型）。
Teachable Machine .tflite 模型是否可以替换下面这段代码中的 modelPath？提前致谢。
private fun runObjectDetection(bitmap: Bitmap) {
// 步骤 1：创建 TFLite 的 TensorImage 对象
val image = TensorImage.fromBitmap(bitmap)

// 步骤 2：初始化检测器对象
val options = ObjectDetector.ObjectDetectorOptions.builder()
.setMaxResults(5)
.setScoreThreshold(0.5f)
.build()
val detector = ObjectDetector.createFromFileAndOptions(
this, // 应用程序上下文
**&quot;model.tflite&quot;, **
options
)
// 步骤 3：将给定的图像提供给模型并打印检测结果
val results = detector.detect(image)

// 步骤 4：解析检测结果并显示
debugPrint(results)

val resultToDisplay = results.map {
// 获取 top-1 类别并制作显示文本
val category = it.categories.first()
val text = &quot;${category.label}, ${category.score.times(100).toInt()}%&quot;

// 创建数据对象，用于显示检测结果
DetectionResult(it.boundingBox, text)
}

// 将检测结果绘制到位图上并显示。
val imgWithResult = drawDetectionResult(bitmap, resultToDisplay)
runOnUiThread {
inputImageView.setImageBitmap(imgWithResult)
}
}
]]></description>
      <guid>https://stackoverflow.com/questions/78951811/can-i-use-google-teachable-machine-model-as-object-detection-model</guid>
      <pubDate>Thu, 05 Sep 2024 07:38:34 GMT</pubDate>
    </item>
    <item>
      <title>级联分段-通道设置是否正确？</title>
      <link>https://stackoverflow.com/questions/78951423/cascade-segmentation-are-the-channels-set-up-correctly</link>
      <description><![CDATA[我想训练一个机器学习模型，用于处理 2D DICOM 图像中的精细蒙版细节。我有 500 张图像准备进行标记/注释。我可以使用这种技术吗？还是我理解错了？
鱼 + 脊椎的注释

我用 1 个类别注释了 500 张图像：鱼。然后我训练一个 model1.pth，将鱼与背景区分开来。该模型有 2 个 out_channels：鱼和背景。

我再次注释了相同的 500 张图像，但现在有 2 个类别：鱼 + 脊椎。我加载 model1.pth，并创建一个具有 2 个输入通道 和 3 个输出通道 的模型：脊柱、鱼和背景，并将模型保存为 model2.pth

最后，我再次注释了 500 张图像，但现在我包括了变形。如果我有 3 种类型的变形，则每种变形都有自己的类别。我加载 model2.pth，创建一个具有 3 个输入通道 和 6 个输出通道 的模型：背景、鱼、脊柱、变形 1、变形 2、变形 3，并将模型保存为 model3.pth。

现在模型可以直接在新图像上使用。是这样吗？


背景和细节。我尝试过什么
目标是找到鱼脊椎的变形。到目前为止，我已尝试通过使用 MONAI 的 UNet 模型 来分割 3 个类别 + 背景。图像是转换为 NifTi 格式 (.dcm.nii.gz) 的 2D DICOM 图像，典型尺寸为 2000x900 像素。我使用 3Dslicer 进行注释。到目前为止的类别：

背景

鱼

脊椎

变形


到目前为止，我已经在（仅）12 张训练图像上进行了测试，只是为了让它运行，我得到了所有 3 个类别的结果，但我猜模型训练过度了。此外，我猜这种技术使得在训练结束后进行微小更改变得更加困难。例如，我想要多种不同类型的变形。 
我的结果：红线左侧：来自 tensorboard，红线右侧：在新图像上测试模型
在开始注释 500 张图像之前，我想验证我是否走在正确的道路上。我希望通过使用级联技术，我可以获得一个可以轻松分割鱼和脊椎的模型，并且我可以随后尝试不同的变形注释。]]></description>
      <guid>https://stackoverflow.com/questions/78951423/cascade-segmentation-are-the-channels-set-up-correctly</guid>
      <pubDate>Thu, 05 Sep 2024 05:34:37 GMT</pubDate>
    </item>
    <item>
      <title>用于简单图回归任务的图神经网络——增加节点数时表现不佳</title>
      <link>https://stackoverflow.com/questions/78950808/graph-neural-network-for-simple-graph-regression-task-not-performing-good-when</link>
      <description><![CDATA[我正在编写一个 gnn 来处理一个简单的任务，即为 GNN 提供一个 v 形图（在 2D 空间中），并要求 gnn 预测 v 形的顶点位置（V 的底部）。当我尝试增加节点数时，性能变差，当我增加到 200 时，它变得不可接受。（1000 * 1000 厘米图中的 2 厘米误差）
我尝试使用 ASAPooling，但性能不佳。
我目前使用的模型是
GNN_DUNE(
(convs): ModuleList(
(0): SAGEConv(3, 28, aggr=mean)
(1-4): 4 x SAGEConv(28, 28, aggr=mean)
)
(dropouts): ModuleList(
(0-5): 6 x Dropout(p=0.058092624169980844, inplace=False)
)
(jump): JumpingKnowledge(cat)
(linears): ModuleList(
(0): Linear(in_features=140, out_features=38, bias=True)
(1): Linear(in_features=38, out_features=38, bias=True)
(2): Linear(in_features=38, out_features=2, bias=True)
)
)
]]></description>
      <guid>https://stackoverflow.com/questions/78950808/graph-neural-network-for-simple-graph-regression-task-not-performing-good-when</guid>
      <pubDate>Wed, 04 Sep 2024 22:51:06 GMT</pubDate>
    </item>
    <item>
      <title>使用神经网络将输入分成几组[关闭]</title>
      <link>https://stackoverflow.com/questions/78950639/using-a-neural-network-to-separate-inputs-into-groups</link>
      <description><![CDATA[原始问题如下：
许多粒子撞击某些探测器，我们从这些事件中获得的信息是每次激活的坐标。探测器层层相继，我们可以为每个粒子绘制一些轨迹，只知道它经过的几个有探测器的坐标。
现在神经网络的问题是向它提供一段时间内发生的所有撞击坐标，并让它返回哪些撞击属于一个粒子，哪些属于另一个粒子
我对输出的唯一想法是给遵循相同轨迹的每个撞击系列编号，编号相同，最后得到一个向量“命名”每个输入坐标。我怀疑神经网络是否可以“即兴”这样的标签，因为理论上它应该迭代的每个数字不一定与任何可能的输入有联系，唯一的条件是当它们不属于同一个粒子时，它们彼此不同。
有没有更好的方法可以做到这一点？这是否可以作为解决问题的合理方法？]]></description>
      <guid>https://stackoverflow.com/questions/78950639/using-a-neural-network-to-separate-inputs-into-groups</guid>
      <pubDate>Wed, 04 Sep 2024 21:41:04 GMT</pubDate>
    </item>
    <item>
      <title>具有共享权重的嵌套模块是否应为 nn.Module 对象参数？</title>
      <link>https://stackoverflow.com/questions/78950394/should-nested-modules-with-shared-weights-be-an-nn-module-object-parameter-or-no</link>
      <description><![CDATA[我希望两个 torch.nn.Module 类共享其部分架构和权重，如下例所示：
from torch import nn

class SharedBlock(nn.Module):
def __init__(self, *args, **kwargs):
super().__init__()

self.block = nn.Sequential(
nn.Linear(...)
# 在此处定义一些块架构...
)

def forward(self, x):
return self.block(x)

class MyNestedModule(nn.Module):
def __init__(self, shared_block: nn.Module, *args, **kwargs):
super().__init__()

self.linear = nn.Linear(...)
self.shared_block = shared_block

def forward(self, x):
return self.shared_block(self.linear(x))

class MyModule(nn.Module):
def __init__(self, *args, **kwargs):
super().__init__()

# 应该是：
shared_block = SharedBlock(*args, **kwargs)
# 或者：
self.shared_block = SharedBlock(*args, **kwargs) # 注意：self。
# ...如果有区别，区别是什么？

self.nested1 = MyNestedModule(shared_block, *args, **kwargs)
self.nested2 = MyNestedModule(shared_block, *args, **kwargs)

def forward(self, x):
x_1, x_2 = torch.split(x, x.shape[0] // 2, dim=0)
y_1 = self.nested1(x_1)
y_2 = self.nested2(y_2)
return y_1, y_2

我想知道 shared_block 是否应该是 MyModule 的对象参数。我认为不是，因为它在 MyNestedModule 类对象中都被设置为对象参数，所以它应该在 torch grad 中注册，但如果我确实在 MyModule 中将它创建为对象参数，会发生什么？]]></description>
      <guid>https://stackoverflow.com/questions/78950394/should-nested-modules-with-shared-weights-be-an-nn-module-object-parameter-or-no</guid>
      <pubDate>Wed, 04 Sep 2024 20:06:25 GMT</pubDate>
    </item>
    <item>
      <title>如何计算 Conv2D 层的尺寸？</title>
      <link>https://stackoverflow.com/questions/78949615/how-can-the-dimensions-of-a-conv2d-layer-be-calculated</link>
      <description><![CDATA[我试图了解我的 GAN 生成器输出的尺寸。每层之后的结果尺寸如下：
开始：torch.Size([128, 74, 1, 1]) 
block1 之后：torch.Size([128, 256, 3, 3]) 
block2 之后：torch.Size([128, 128, 6, 6]) 
block3 之后：torch.Size([128, 64, 13, 13]) 
block4 之后：torch.Size([128, 1, 28, 28])

生成器代码如下。此处 z_dim 为 74，但最初为 64。它附加了 10 个类标签，如下所示。
fake_noise = get_noise(cur_batch_size, z_dim, device=device) 
noise_and_labels = Combine_vectors(fake_noise, one_hot_labels)
fake = gen(noise_and_labels)

class Generator(nn.Module):
&#39;&#39;&#39;
Generator Class
值：
z_dim：噪声向量的维度，标量
im_chan：输出图像的通道数，标量
（MNIST 是黑白的，因此 1 个通道是您的默认值）
hidden_​​dim：内部维度，标量
&#39;&#39;&#39;
def __init__(self, z_dim=10, im_chan=1, hidden_​​dim=64):
super(Generator, self).__init__()
self.z_dim = z_dim
# 构建神经网络
self.block1 = self.make_gen_block(z_dim, hidden_​​dim * 4)
self.block2 = self.make_gen_block(hidden_​​dim * 4, hidden_​​dim * 2, kernel_size=4, stride=1)
self.block3 = self.make_gen_block(hidden_​​dim * 2, hidden_​​dim)
self.block4 = self.make_gen_block(hidden_​​dim, im_chan, kernel_size=4, final_layer=True)

def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, padding=1, final_layer=False):
&#39;&#39;&#39;
函数返回对应于生成器块的操作序列DCGAN；
转置卷积、批处理规范（最后一层除外）和激活。
参数：
input_channels：输入特征表示有多少个通道
output_channels：输出特征表示应该有多少个通道
kernel_size：每个卷积滤波器的大小，相当于 (kernel_size, kernel_size)
stride：卷积的步幅
final_layer：布尔值，如果是最后一层则为 true，否则为 false
（影响激活和 batchnorm）
&#39;&#39;&#39;
如果不是 final_layer：
return nn.Sequential(
nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),
nn.BatchNorm2d(output_channels),
nn.LeakyReLU(0.2, inplace=True),
)
else：
return nn.Sequential(
nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),
nn.Tanh(),
)

def forward(self, noise):
&#39;&#39;&#39;
完成生成器前向传递的函数：给定一个噪声张量，
返回生成的图像。
参数：
noise：具有维度 (n_samples, input_dim) 的噪声张量
&#39;&#39;&#39;
x = noise.view(len(noise), self.z_dim, 1, 1)
print(f&#39;Gen: {x.shape}&#39;)
x = self.block1(x)
print(f&#39;After block1: {x.shape}&#39;)
x = self.block2(x)
print(f&#39;After block2: {x.shape}&#39;)
x = self.block3(x)
print(f&#39;After block3: {x.shape}&#39;)
x = self.block4(x)
print(f&#39;After block4: {x.shape}&#39;)
return x

def get_noise(n_samples, z_dim, device=&#39;cpu&#39;):
&#39;&#39;&#39;
用于创建噪声向量的函数：给定维度 (n_samples, z_dim)
创建该形状的张量，其中填充了来自正态分布的随机数。
参数：
n_samples：要生成的样本数，标量
z_dim：噪声向量的维度，标量
device：设备类型
&#39;&#39;&#39;
return torch.randn(n_samples, z_dim, device=device)

根据此处的公式，第一个块之后的结果将是(1 + 2x0 -1x(3-1) -1)/2 +1 = 0，但它显示 3x3。我在这里做错了什么？]]></description>
      <guid>https://stackoverflow.com/questions/78949615/how-can-the-dimensions-of-a-conv2d-layer-be-calculated</guid>
      <pubDate>Wed, 04 Sep 2024 16:09:58 GMT</pubDate>
    </item>
    <item>
      <title>通过索引为张量赋值后，值不匹配</title>
      <link>https://stackoverflow.com/questions/78949501/mismatch-of-values-after-assigning-values-to-a-tensor-by-index</link>
      <description><![CDATA[我正在编写一个 PyTorch 训练代码，它构建了一个算法类。其中有一个步骤需要为内部张量分配一些值。但是，即使代码只有两行，也有一个错误。我发现分配的张量的值与分配的值不同。
这是该类的代码：
class PRODEN(Algorithm):
&quot;&quot;&quot;
PRODEN
参考：部分标签学习的真实标签的渐进式识别，ICML 2020。
&quot;&quot;&quot;

def __init__(self, input_shape, train_givenY, hparams):
super(PRODEN, self).__init__(input_shape, train_givenY, hparams)
self.featurizer = networks.Featurizer(input_shape, self.hparams)
self.classifier = networks.Classifier(
self.featurizer.n_outputs,
self.num_classes)

self.network = nn.Sequential(self.featurizer, self.classifier)
self.optimizer = torch.optim.Adam(
self.network.parameters(),
lr=self.hparams[&quot;lr&quot;],
weight_decay=self.hparams[&#39;weight_decay&#39;]
)
train_givenY = torch.from_numpy(train_givenY)
tempY = train_givenY.sum(dim=1).unsqueeze(1).repeat(1, train_givenY.shape[1])
label_confidence = train_givenY.float()/tempY
self.label_confidence = label_confidence

def update(self, minibatches):
_, x, strong_x, partial_y, _, index = minibatches
loss = self.rc_loss(self.predict(x), index)
self.optimizer.zero_grad()
loss.backward()
self.optimizer.step()
self.confidence_update(x, partial_y, index)
return {&#39;loss&#39;: loss.item()}

def rc_loss(self, output, index):
device = &quot;cuda&quot; if index.is_cuda else &quot;cpu&quot;
self.label_confidence = self.label_confidence.to(device)
logsm_outputs = F.log_softmax(outputs, dim=1)
#print(self.label_confidence.is_cuda)
final_outputs = logsm_outputs * self.label_confidence[index, :]
average_loss = - ((final_outputs).sum(dim=1)).mean()
return average_loss

def predict(self, x):
return self.network(x)

def confidence_update(self, batchX, batchY, batch_index):
with torch.no_grad():
batch_outputs = self.predict(batchX)
temp_un_conf = F.softmax(batch_outputs, dim=1)
&#39;&#39;&#39;有问题的代码开始了&#39;&#39;&#39;
self.label_confidence[batch_index, :] = temp_un_conf * batchY # un_confidence 存储每个示例的权重
&#39;&#39;&#39;问题代码结束&#39;&#39;&#39;
base_value = self.label_confidence.sum(dim=1).unsqueeze(1).repeat(1, self.label_confidence.shape[1])
self.label_confidence = self.label_confidence / base_value

问题出在 confidence_update 上。我发现
self.label_confidence[batch_index, :]

的值与
temp_un_conf * batchY

在此分配之后
self.label_confidence[batch_index, :] = temp_un_conf * batchY

仅适用于少数示例，但适用于大多数示例。例如，对于 1024 的批次大小，第一次迭代时大约有 4 个示例，之后会变得更大。我对这个问题非常沮丧，尝试了很多方法：

这个问题只存在于 CIFAR10，但其他数据集不存在。

所有张量的数据类型都是 Float32。

所有张量都在 gpu 上。


我的代码有什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/78949501/mismatch-of-values-after-assigning-values-to-a-tensor-by-index</guid>
      <pubDate>Wed, 04 Sep 2024 15:41:44 GMT</pubDate>
    </item>
    <item>
      <title>从一个巨大的文本文件和一行提示生成文章[关闭]</title>
      <link>https://stackoverflow.com/questions/78949493/generate-article-from-a-huge-text-file-and-a-one-line-prompt</link>
      <description><![CDATA[我想从一个大文本文件和一个一行提示中生成一篇大约 100 行的文章
第 1 行：敏捷的棕色狐狸跳过了懒狗。

第 2 行：困难中蕴藏着机遇。

第 3 行：生活就是当你忙于制定其他计划时发生的事情。

第 4 行：在这个不断试图让你成为其他东西的世界中做你自己是最大的成就。

第 5 行：成功不是终点，失败也不是致命的：继续前进的勇气才是最重要的。

第 6 行：敏捷的棕色狐狸吃肉。

提示：狐狸做了什么？

输出文章：敏捷的棕色狐狸跳过了懒狗。敏捷的棕色狐狸吃肉。

这只是给出的一个小例子。
你能改进我的代码或给我一个更好的解决方案吗？
我所说的改进是指我想对我的文本文件进行过度拟合。
# prepare_dataset.py

from transformers import GPT2Tokenizer
from datasets import Dataset
import pandas as pd

def load_and_prepare_dataset(file_path):
&quot;&quot;&quot;从文本文件加载并准备数据集。&quot;&quot;&quot;
使用 open(file_path, &#39;r&#39;) 作为文件：
text = file.read()

lines = text.split(&#39;.&#39;)

data = {&#39;text&#39;: lines} 
df = pd.DataFrame(data)

tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)

tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
return tokenizer(examples[&#39;text&#39;], padding=&#39;max_length&#39;, truncation=True, max_length=512)

dataset = Dataset.from_pandas(df)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.rename_column(&quot;input_ids&quot;, &quot;labels&quot;)

return tokenized_datasets

如果__name__ == &quot;__main__&quot;:
dataset = load_and_prepare_dataset(&#39;data.txt&#39;) 
dataset.save_to_disk(&#39;./dataset&#39;)

# train_model.py

from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
from datasets import load_from_disk

def fine_tune_model(train_dataset):
&quot;&quot;&quot;在给定的数据集上微调 GPT-2 模型。&quot;&quot;&quot;
tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)
model = GPT2LMHeadModel.from_pretrained(&#39;gpt2&#39;)

training_args = TrainingArguments(
output_dir=&#39;./results&#39;, # 输出目录
evaluation_strategy=&#39;epoch&#39;, # 要使用的评估策略
learning_rate=2e-5, # 学习率
per_device_train_batch_size=2, # 用于训练的批次大小
per_device_eval_batch_size=2, # 用于评估的批次大小
num_train_epochs=1, # 训练周期数
weight_decay=0.01, # 权重衰减强度
logs_dir=&#39;./logs&#39;, # 用于存储日志的目录
logs_steps=10, # 日志记录之间的步骤数
)

trainer = Trainer(
model=model, # 要训练的模型
args=training_args, # 参数用于训练
train_dataset=train_dataset, # 训练数据集
)

trainer.train()

def main():
dataset = load_from_disk(&#39;./dataset&#39;)
print(dataset.column_names)
print(dataset[0]) 

fine_tune_model(dataset)

if __name__ == &quot;__main__&quot;:
main()
]]></description>
      <guid>https://stackoverflow.com/questions/78949493/generate-article-from-a-huge-text-file-and-a-one-line-prompt</guid>
      <pubDate>Wed, 04 Sep 2024 15:40:27 GMT</pubDate>
    </item>
    <item>
      <title>empty() 在 Cu-net 模型中接收到无效的参数组合</title>
      <link>https://stackoverflow.com/questions/78949410/empty-received-an-invalid-combination-of-arguments-in-cu-net-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78949410/empty-received-an-invalid-combination-of-arguments-in-cu-net-model</guid>
      <pubDate>Wed, 04 Sep 2024 15:23:33 GMT</pubDate>
    </item>
    <item>
      <title>如何设置 Kubernetes GPU 集群以与本地网络中不同 PC 上的 GPU 协同工作？[关闭]</title>
      <link>https://stackoverflow.com/questions/78947983/how-to-set-up-kubernetes-gpu-cluster-to-work-with-gpus-located-on-different-pcs</link>
      <description><![CDATA[我想在 Jupyter Notebook 中设置一个用于机器学习的 Kubernetes GPU 集群。我在一个本地网络中有 3 台配备 GPU（RTX 3060ti）的计算机，我想结合这些 GPU 的资源来运行神经网络训练和其他机器学习方法。
我的 RTX 3060ti 在解决某些神经网络训练任务时会执行太长的计算，我想用它来加速神经网络的训练。
我尝试搜索有关此主题的信息，但大多数文章都涉及将一个 GPU 的资源分配给不同的机器学习任务。]]></description>
      <guid>https://stackoverflow.com/questions/78947983/how-to-set-up-kubernetes-gpu-cluster-to-work-with-gpus-located-on-different-pcs</guid>
      <pubDate>Wed, 04 Sep 2024 09:48:45 GMT</pubDate>
    </item>
    <item>
      <title>基于上下文的向量搜索获取唯一性分数</title>
      <link>https://stackoverflow.com/questions/78946820/vector-search-to-get-a-uniqueness-score-based-on-context</link>
      <description><![CDATA[我有一篇带有标题和说明的博客文章，我想将其唯一性与 CSV 文件中的多个博客条目进行比较。CSV 包含多个博客，每个博客都有标题和元描述。
我目前使用 TF-IDF 矢量化和余弦相似度将单个博客与 CSV 文件中的所有条目进行比较。但是，这种方法仅基于确切的单词进行匹配，而不是基于上下文。]]></description>
      <guid>https://stackoverflow.com/questions/78946820/vector-search-to-get-a-uniqueness-score-based-on-context</guid>
      <pubDate>Wed, 04 Sep 2024 04:27:53 GMT</pubDate>
    </item>
    <item>
      <title>代码错误：模块“torch.nn”没有属性“ConvTranspose2D”</title>
      <link>https://stackoverflow.com/questions/77366473/errror-in-code-module-torch-nn-has-no-attribute-convtranspose2d</link>
      <description><![CDATA[def make_gen_block(self,input_channels,output_channels,kernel_size=3,stride=2,final_layer = False):
10 if not final_layer :
---&gt; 11 return nn.Sequential(nn.ConvTranspose2D(input_layer,output_layer,kernel_size,stride),
12 nn.BatchNorm2d(output_channels),
13 nn.ReLU(inplace = True),)

AttributeError: module &#39;torch.nn&#39; has no attribute &#39;ConvTranspose2D&#39;

解决方案是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77366473/errror-in-code-module-torch-nn-has-no-attribute-convtranspose2d</guid>
      <pubDate>Thu, 26 Oct 2023 11:11:39 GMT</pubDate>
    </item>
    <item>
      <title>Hugginface Trainer 中的 KeyError: 'eval_loss'</title>
      <link>https://stackoverflow.com/questions/74239556/keyerror-eval-loss-in-hugginface-trainer</link>
      <description><![CDATA[我正在尝试使用 Hugginface 框架构建问答管道，但遇到了 KeyError: &#39;eval_loss&#39; 错误。我的目标是最后训练并保存最佳模型，并在加载的模型上评估验证测试。我的训练器配置如下所示：
args = TrainingArguments(f&#39;model_training&#39;,
evaluation_strategy=&quot;epoch&quot;,
label_names = [&quot;start_positions&quot;, &quot;end_positions&quot;],
logs_steps = 1,
learning_rate=2e-5,
num_train_epochs=epochs,
save_total_limit = 2,
load_best_model_at_end=True,
save_strategy=&quot;epoch&quot;,
logs_strategy=&quot;epoch&quot;,
report_to=&quot;none&quot;,
weight_decay=0.01,
fp16=True,
push_to_hub=False)

训练时出现此错误：
Traceback（最近一次调用last):
文件“qa_pipe.py”，第 286 行，位于 &lt;module&gt;
pipe.training(train_d, val_d, epochs = 2)
文件“qa_pipe.py”，第 263 行，正在训练
self.trainer.train()
文件“/home/admin/qa/lib/python3.7/site-packages/transformers/trainer.py”，第 1505 行，正在训练
ignore_keys_for_eval=ignore_keys_for_eval,
文件“/home/admin/qa/lib/python3.7/site-packages/transformers/trainer.py”，第 1838 行，在 _inner_training_loop 中
self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
文件&quot;/home/admin/qa/lib/python3.7/site-packages/transformers/trainer.py&quot;, 第 2090 行, 在 _maybe_log_save_evaluate
self._save_checkpoint(model, trial, metrics=metrics)
文件 &quot;/home/admin/qa/lib/python3.7/site-packages/transformers/trainer.py&quot;, 第 2193 行, 在 _save_checkpoint
metric_value = metrics[metric_to_check]
KeyError: &#39;eval_loss&#39;

在 colab 上提供了最小工作示例
如何避免这个错误并最终保存最佳模型？]]></description>
      <guid>https://stackoverflow.com/questions/74239556/keyerror-eval-loss-in-hugginface-trainer</guid>
      <pubDate>Fri, 28 Oct 2022 18:30:28 GMT</pubDate>
    </item>
    <item>
      <title>FileNotFoundError：[WinError 3] 系统找不到指定的路径：</title>
      <link>https://stackoverflow.com/questions/51007476/filenotfounderror-winerror-3-the-system-cannot-find-the-path-specified</link>
      <description><![CDATA[我尝试运行此教程中的代码。我已将代码和数据集放在同一目录中，但仍然出现以下错误。
FileNotFoundError Traceback (most recent call last)
&lt;ipython-input-6-5f5284db0527&gt; in &lt;module&gt;()
39 # 从所有图像中提取特征
40 directory = &#39;Flicker8k&#39;
---&gt; 41 features = extract_features(directory)
42 print(&#39;Extracted Features: %d&#39; % len(features))
43 # 保存到文件

&lt;ipython-input-6-5f5284db0527&gt;在 extract_features(directory) 中
18 # 从每张照片中提取特征
19 features = dict()
---&gt; 20 for name in listdir(directory):
21 # 从文件加载图像
22 filename = directory + &#39;/&#39; + name

**FileNotFoundError: [WinError 3] 系统找不到指定的路径：&#39;Flicker8k&#39;**
]]></description>
      <guid>https://stackoverflow.com/questions/51007476/filenotfounderror-winerror-3-the-system-cannot-find-the-path-specified</guid>
      <pubDate>Sun, 24 Jun 2018 06:24:04 GMT</pubDate>
    </item>
    </channel>
</rss>