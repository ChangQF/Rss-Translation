<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 26 Apr 2024 06:20:03 GMT</lastBuildDate>
    <item>
      <title>寻找最佳路由解决方案</title>
      <link>https://stackoverflow.com/questions/78388418/looking-for-best-routing-solution</link>
      <description><![CDATA[所以，我正在研究呼叫路由问题陈述。这里的呼叫路由意味着，当客户呼叫到达客户服务时，自定义将根据客户选择的菜单选项路由到代理。
我遇到的问题比这更复杂。
客户信息将包括位置、所选菜单选项、世界各地的语言。理想的代理应该是具有相同语言能力、能够解决所选菜单选项的人。
但智能体也并不复杂，智能体可能拥有不止一种语言技能，并且更擅长解决多个问题。如果来自美国的人打电话（即英语），则代理可能具有英语、俄语技能。因此，如果位置是俄罗斯，也应该根据代理的可用性，将此人作为目标，否则会转到语言为俄语的其他代理。
遇到这样的问题我该怎么解决呢？很多公司都使用这个，但我真的不知道应该如何解决它。
感谢任何帮助或资源。谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78388418/looking-for-best-routing-solution</guid>
      <pubDate>Fri, 26 Apr 2024 05:32:19 GMT</pubDate>
    </item>
    <item>
      <title>为什么 50 个预测中只有 45 个，这里预测的是哪一列？</title>
      <link>https://stackoverflow.com/questions/78388371/why-is-there-only-45-predictions-out-of-50-and-which-column-is-predicted-here</link>
      <description><![CDATA[我是一名初学者，我在为自己的数据编码时使用了张量流教程。代码如下：
`
训练、验证、测试 = 数据[:420]、数据[420:450]、数据[450:]

类窗口生成器（）：
  def __init__(自我，输入宽度，标签宽度，移位，
             训练、验证、测试）：
    self.train = 火车
    self.val = val
    自测=测试
    self.input_width = input_width
    self.label_width = 标签宽度
    self.shift = 移位
    self.total_window_size = input_width + 移位
    self.input_slice = 切片(0,input_width)
    self.input_indices = np.arange(self.total_window_size)[self.input_slice]
    self.label_start = self.total_window_size-self.label_width
    self.labels_slice = slice(self.label_start,无)
    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

  def __repr__(自我):
    返回 &#39;​​\n&#39;.join([
        f&#39;总window_size: {self.total_window_size}&#39;,
        f&#39;输入索引：{self.input_indices}&#39;,
        f&#39;标签索引：{self.label_indices}&#39;])

  def split_window(自身,特征):
    输入=特征[:,self.input_slice,:]
    标签 = 特征[:,self.labels_slice,:]
    input.set_shape([无,self.input_width,无])
    labels.set_shape([无,self.label_width,无])
    返回输入、标签

  def make_dataset(自身,数据):
    数据 = np.array(数据,dtype=np.float32)
    ds = tf.keras.utils.timeseries_dataset_from_array(数据=数据，
                                                    目标=无，
                                                    序列长度 = self.total_window_size,
                                                    序列步幅 = 1,
                                                    随机播放=真，
                                                    批量大小 = 32,)
    ds = ds.map(self.split_window)
    返回数据

  @财产
  def train_(自身):
    返回 self.make_dataset(self.train)
  @财产
  def val_(自身):
    返回 self.make_dataset(self.val)
  @财产
  def test_(自我):
    返回 self.make_dataset(self.test)
最大纪元 = 100
defcompile_and_fit（模型，窗口，耐心= 30）：
  Early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;,
                               耐心=耐心，
                               模式=&#39;分钟&#39;,
                              详细 =1)
  reduce_lr =ReduceLROnPlateau(监视器=&#39;val_loss&#39;,因子=0.1,耐心=10,min_lr=1e-6,详细=1)
  model.compile(loss=MeanSquaredError(), 优化器 = Adam(), 指标=[MeanAbsoluteError()])
  历史= model.fit(window.train_,epochs=MAX_EPOCHS,validation_data=window.val_,callbacks=[early_stopping,reduce_lr])
  返回历史记录
Wide_window = WindowGenerator(input_width = 5,
                          标签宽度=1，
                          移位= 1，
                          火车=火车，
                          值=值，
                          测试=测试）
conv_model = 顺序（[输入（形状=（5,2），名称=&#39;编码器输入&#39;），
                     Conv1D(filters=32,kernel_size=5,activation=&#39;relu&#39;,name=&#39;conv1D&#39;),
                    密集（32，激活=&#39;relu&#39;，名称=&#39;dense1&#39;），
                    密集(1,name=&#39;dense2&#39;)])
历史=compile_and_fit(conv_model,wide_window)
y_pred = conv_model.predict(wide_window.test_)`

现在，输出是 (45,1,1)。由于测试大小为 50，输出大小不应该为 50 吗？当我将密集2单位保持为1时，这里预测的是哪一个？
我尝试阅读 timeseries_dataset_from_array 的文档，但仍然无法找出问题或解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78388371/why-is-there-only-45-predictions-out-of-50-and-which-column-is-predicted-here</guid>
      <pubDate>Fri, 26 Apr 2024 05:17:53 GMT</pubDate>
    </item>
    <item>
      <title>CNN 模型、深度学习准确率达到 0%</title>
      <link>https://stackoverflow.com/questions/78388352/geeting-0-accuracy-in-cnn-model-deep-learning</link>
      <description><![CDATA[我正在尝试开发一个用于多类图像分类的顺序 CNN 模型。开发模型后，当我开始训练时，模型在第一个时期给出了准确性，但在第二个时期我没有获得任何准确性。但在第三个时期，我再次获得了准确性。为什么会发生这种情况？还有一个问题，您能否建议我如何提高模型的 f1 分数、精度和召回率。[在此处输入图像描述](https://i.sstatic.net/Jpn1fW72.png )
我尝试过ChatGPT来解决这个问题，但找不到任何解决方案。]]></description>
      <guid>https://stackoverflow.com/questions/78388352/geeting-0-accuracy-in-cnn-model-deep-learning</guid>
      <pubDate>Fri, 26 Apr 2024 05:12:05 GMT</pubDate>
    </item>
    <item>
      <title>推理时出现意外的输出图像</title>
      <link>https://stackoverflow.com/questions/78387970/unexpected-output-image-on-inference</link>
      <description><![CDATA[我正在尝试在颤动中升级图像。但我得到了意想不到的结果。
我对推理模型完全是新手。
原始图片：
原始图片
输出图像：
输出图像
LOG：I/flutter ( 592)：图像转换为标准化 Float32List
I/flutter ( 592)：图像标准化成功。
I/flutter ( 592)：输入张量创建成功。
I/颤动（592）：高2880宽1640
原图：高720宽210
这是我尝试过的代码：
 未来;推理（）异步{
    if (selectedImage == null) {
      debugPrint(&#39;未选择图像&#39;);
      返回;
    }

    var 预处理图像 =
    NormalizeImage.imageToNormalizedFloat32List(selectedImage!);

    最终形状 = [1, 3, selectedImage!.height, selectedImage!.width];

    debugPrint(&#39;图像标准化成功。&#39;);

    最终输入Ort =
    OrtValueTensor.createTensorWithDataList(preprocessedImage, shape);

    最终输入 = {&#39;input&#39;: inputOrt};

    debugPrint(&#39;输入张量创建成功。&#39;);

    最终 runOptions = OrtRunOptions();
    最终输出=等待ortSession.runAsync（runOptions，输入）；

    inputOrt.release();
    runOptions.release();


    输出？.forEach((元素) {
      最终输出值=元素？.值；
      if (outputValue 为 List&gt;&gt;) {
        img.Image generatedImage =generateImageFromOutput(outputValue);
        列表 pngBytes = img.encodePng( generatedImage);
        img.Image解码图像 = img.decodeImage(Uint8List.fromList(pngBytes))!;

        显示对话框（
          上下文：上下文，
          构建器：（BuildContext 上下文）{
            返回对话框（
              孩子：大小框（
                宽度：200，
                高度：200，
                子: Image.memory(Uint8List.fromList(img.encodePng(decodedImage))),
              ),
            ）；
          },
        ）；
      } 别的 {
        debugPrint(“输出类型未知”);
      }
      元素？.release();
    });
  }
  img.ImagegenerateImageFromOutput(List&gt;&gt;输出) {
    最终宽度=输出[0][0].长度；
    最终高度=输出[0][0][0].长度；

    debugPrint(“高度$高度宽度$宽度”);

    Float32List float32Data = flattenList(输出);
    var imgData = NormalizeImage.denormalizedFloat32ListToImage(float32Data, 宽度, 高度);
    返回图像数据；
  }

  Float32List flattenList(List&gt;&gt;&gt;nestedList) {
    列表&lt;双&gt;压扁=[]；
    for (nestedList 中的 var fourDimList) {
      for (var ThreeDimList in fourDimList) {
        for (var TwoDimList in ThreeDimList) {
          for (twoDimList 中的 var 值) {
            扁平化。添加（值）；
          }
        }
      }
    }
    返回 Float32List.fromList(展平);
  }

标准化类别
class NormalizeImage {
  静态Float32List imageToNormalizedFloat32List（图像图像）{
    最终 int 高度 = image.height;
    最终 int 宽度 = image.width;

    Float32List float32Image = Float32List(3 * 高度 * 宽度);

    for (int i = 0; i &lt; 高度; i++) {
      for (int j = 0; j &lt; 宽度; j++) {
        最终 int 像素索引 = (i * 宽度 + j) * 3;
        最终 int 像素 = image.getPixel(j, i);

        float32Image[像素索引] = getRed(像素) / 255.0;
        float32Image[像素索引 + 1] = getGreen(像素) / 255.0;
        float32Image[像素索引 + 2] = getBlue(像素) / 255.0;
      }
    }

    print(&quot;图像转换为规范化的 Float32List&quot;);
    返回 float32Image；
  }

  静态图像 denormalizedFloat32ListToImage(Float32List float32Image, int width, int height) {
    最终imgData =图像（宽度，高度）；
    for (int i = 0; i &lt; 高度; i++) {
      for (int j = 0; j &lt; 宽度; j++) {
        最终 int 像素索引 = (i * 宽度 + j) * 3;
        最终 int r = (float32Image[pixelIndex] * 255).toInt().clamp(0, 255);
        最终 int g = (float32Image[pixelIndex + 1] * 255).toInt().clamp(0, 255);
        最终 int b = (float32Image[pixelIndex + 2] * 255).toInt().clamp(0, 255);
        最终 int color = getColor(r, g, b);
        imgData.setPixel(j, i, 颜色);
      }
    }
    返回图像数据；
  }
}
]]></description>
      <guid>https://stackoverflow.com/questions/78387970/unexpected-output-image-on-inference</guid>
      <pubDate>Fri, 26 Apr 2024 02:39:43 GMT</pubDate>
    </item>
    <item>
      <title>如何定义 Y 标签来创建基于风险的规则？</title>
      <link>https://stackoverflow.com/questions/78387633/how-to-define-y-label-for-creating-risk-based-rules</link>
      <description><![CDATA[在风险中，我看到部署了数百条规则，例如，他们查看过去 30 天内的销售额 &gt; 5000且模型得分&gt; 0.5 然后“暂停帐户”等我知道他们使用决策树来创建这些规则。但他们没有提到用于决策树的 y 变量是什么。如何定义 Y 标签？有办法吗？
您可以使用“帐户暂停 = 60 天内的 Y/N”本身作为 Y 标签，然后进行预测吗？或者我是否必须查看其他指标，例如违约率&gt;？例如 x% 并预测他们是谁然后暂停他们？我知道它是特定于问题的，但是为现实世界中的决策树问题选择 Y 标签背后的想法是什么？]]></description>
      <guid>https://stackoverflow.com/questions/78387633/how-to-define-y-label-for-creating-risk-based-rules</guid>
      <pubDate>Thu, 25 Apr 2024 23:42:43 GMT</pubDate>
    </item>
    <item>
      <title>流失预测模型 - 如何定义观察？</title>
      <link>https://stackoverflow.com/questions/78387585/churn-prediction-model-how-define-an-observation</link>
      <description><![CDATA[我正在尝试建立用于流失预测的机器学习模型，但不幸的是我不知道如何开始构建数据集。您如何理解这种模型中的观察？例如，在应用记分卡中，观察是信用应用，但如何在基于 id 的模型中定义它？假设我每个月都会对未来进行预测 - 如果我构建训练数据集，我是否应该选择一天（假设今天）并计算当天的所有特征？但随着时间的推移，功能稳定性如何呢？季节性？我想我不能选择多天 - 去年每个月的第一天并选择所有客户，因为观察结果不是独立的。在这样的模型中如何处理？
我读了很多文章，但我还是不明白如何定义它。]]></description>
      <guid>https://stackoverflow.com/questions/78387585/churn-prediction-model-how-define-an-observation</guid>
      <pubDate>Thu, 25 Apr 2024 23:18:22 GMT</pubDate>
    </item>
    <item>
      <title>识别图像中红色粒子占据的像素</title>
      <link>https://stackoverflow.com/questions/78387055/identify-pixels-occupied-by-red-particles-in-image</link>
      <description><![CDATA[我有一些塑料颗粒和水波实验的图像。目标是自动识别塑料颗粒。它们有时会重叠，我不需要找到单个粒子，找到那些包含塑料的像素就足够了。
由于粒子是红色的，并且背景大多是白色或黑色，我想我可以进行简单的阈值处理，如果R &gt; &gt;，则说像素是塑料。 5*B 和 R&gt; 0.25，其中 R 和 B 是红色和蓝色通道。然而，不同实验之间的曝光差异很大，有时在实验中，当部分表面被水覆盖时，所以我的方法不能非常一致地工作，有时会错误地识别侧面的黑色裂缝。
我想知道还有什么其他选择。我对神经网络的经验有限，所以我不确定这是否可行（需要付出合理的努力）。特别是，我认为形状不会有太大帮助，因为粒子靠近在一起并且部分重叠，它们之间的对比度很差，但也许颜色就足够了？
示例图像：


]]></description>
      <guid>https://stackoverflow.com/questions/78387055/identify-pixels-occupied-by-red-particles-in-image</guid>
      <pubDate>Thu, 25 Apr 2024 20:14:01 GMT</pubDate>
    </item>
    <item>
      <title>Xgboost 自定义目标函数。如何修改权重？</title>
      <link>https://stackoverflow.com/questions/78386885/xgboost-custom-objective-function-how-to-modify-the-weights</link>
      <description><![CDATA[我有一个 xgboost 的自定义目标函数：
def custom_objective(y_true, y_pred, x = 3000):

    pred_probs = 1.0 / (1.0 + np.exp(-y_pred))

    top_x_indices = np.argsort(pred_probs)[-x:]

    #权重数组：前 x 个实例的权重较高
    权重 = np.ones_line(y_true)
    权重[top_x_indices] = 10

    梯度 = (pred_probs - y_true) / (pred_probs*(1 - pred_probs))
    
    hess = (2 * pred_probs**2 - 3 * pred_probs + 1 + y_true - 2 * y_true * pred_probs) / (pred_probs * (1 - pred_probs))**2

    返回 grad * 权重，hess * 权重`

这个想法是将前 3000 个样本的权重设置得更高，因为只有最高分数才会用于生产。
问题是它每次分配相同的权重，并且权重在构建下一棵树时不会更新。如何在训练过程中将给定简单的实际权重放入自定义目标函数中？如果我能做到这一点，那么我可以将 top3000 的权重设置得更高，例如将这些权重乘以 10。
我试图解决这个问题，但找不到权重。]]></description>
      <guid>https://stackoverflow.com/questions/78386885/xgboost-custom-objective-function-how-to-modify-the-weights</guid>
      <pubDate>Thu, 25 Apr 2024 19:32:23 GMT</pubDate>
    </item>
    <item>
      <title>呼吸信号的对数功率谱</title>
      <link>https://stackoverflow.com/questions/78386374/log-power-spectrum-for-breath-signal</link>
      <description><![CDATA[我的数据集是噪声频谱的LPS，我的标签是干净频谱的LPS，每个图片大小是（1025*1292）。我使用unet作为我的模型。
型号：
导入火炬
将 torch.nn 导入为 nn

解码器类（nn.Module）：
    def __init__(self, in_channels,out_features, kernel_size, maxpoolindex, apply_dropout,stride):
        super(解码器, self).__init__()
        self.conv = nn.Conv2d(in_channels=in_channels、out_channels=out_features、kernel_size=kernel_size、stride=stride、padding=0、bias=True)
        self.batch_norm = nn.BatchNorm2d(out_features)
        self.relu = nn.LeakyReLU(负斜率=0.2)
        self.dropout = nn.Dropout(p=0.1)
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2) 如果 maxpoolindex == 1 否则无

    def 前向（自身，x）：
        x = self.conv(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        如果 self.dropout 不是 None：
            x = self.dropout(x)
        如果 self.maxpool 不是 None：
            x = self.maxpool(x)
        返回x

编码器类（nn.Module）：
    def __init__(self,in_channels, out_features, kernel_size, apply_dropout):
        超级（编码器，自我）.__init__()
        self.conv_transpose = nn.ConvTranspose2d（in_channels = in_channels，out_channels = out_features，kernel_size = kernel_size，stride = 2，padding = 0，bias = True）
        self.batch_norm = nn.BatchNorm2d(out_features)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.1)

    def 前向（自身，x）：
        x = self.conv_transpose(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        如果 self.dropout 不是 None：
            x = self.dropout(x)
        返回x

class DenoiseUnet(nn.Module):#除到第一个奇数停止的设计
    def __init__(自身):
        超级（DenoiseUnet，自我）.__init__()
        self.down_procedure = nn.ModuleList([
            解码器(1,8,2,0,0,2),
            解码器(8,16,2,0,0,2),
            解码器(16,32,2,0,0,2),
            解码器(32,128,2,0,0,2),
            解码器(128,128,1,0,0,1)
        ]）
        self.up_procedure = nn.ModuleList([
            编码器(2​​56,32,2,0),
            编码器(64,16,2,0),
            编码器(32,8,2,0),
            编码器(16,4,2,0),
        ]）
        self.convert = nn.ConvTranspose2d(4, 1, kernel_size=1, stride=1, padding=0)


    def 前向（自身，x）：
        连接=[]
        对于 self.down_procedure 中的 down：
            x = 向下(x)
            连接.append(x)

        连接=列表（反转（连接[：-1]））
        对于 up，在 zip(self.up_procedure, connection) 中连接：


            如果 x.shape[2] &lt;连接.形状[2]：
              连接 = 连接[:, :, :x.shape[2], :]
            别的：
              x = x[:, :, :connect.shape[2], :]

            如果 x.shape[3] &lt;连接.形状[3]：
              连接 = 连接[:, :, :, :x.shape[3]]
            别的：
              x = x[:, :, :, :connect.shape[3]]



            x = torch.cat([x, 连接], 暗淡=1)
            x = 上(x)

        y = self.convert(x)
        返回y


模型 = DenoiseUnet()
打印（模型）

但是经过 10 轮训练后，我得到这样的结果：
https://i.sstatic.net/b8DxXHUr.png
https://i.sstatic.net/UDhZNKHE.png
一个是预测结果，一个是测试，任何人都可以帮我找出问题所在吗？
数据集数量不同，看起来是一样的。]]></description>
      <guid>https://stackoverflow.com/questions/78386374/log-power-spectrum-for-breath-signal</guid>
      <pubDate>Thu, 25 Apr 2024 17:43:28 GMT</pubDate>
    </item>
    <item>
      <title>无法将 NumPy 数组转换为 LSTM 张量</title>
      <link>https://stackoverflow.com/questions/78385645/failed-to-convert-a-numpy-array-to-a-tensor-for-lstm</link>
      <description><![CDATA[尝试运行 LSTM 模型，其中数据被分成 csv 中的几列，并且我正在尝试从此类 csv 中准备日期。
获取错误
ValueError：无法将 NumPy 数组转换为张量
（不支持的对象类型 numpy.ndarray）

# 加载 CSV 数据 # 3 个特征 (1, 2, 3)，1 个标签 (5) 列
# 所有数据均为浮点数，1,0 位于标签中
data = pd.read_csv(“data_1.csv”, usecols=[“Column_1”,“Column_2”,“Column_3”,“Column_5”])
data2 = pd.read_csv(“data_2.csv”, usecols=[“Column_1”,“Column_2”,“Column_3”,“Column_5”])


window_size = 1 # 对于时间步长

# 创建序列的函数
def create_sequences(数据, window_size, label_col):
  序列=[]
  标签=[]
  对于范围内的 i(len(data) - window_size + 1)：
    window = data.loc[i:i+window_size,[&quot;Column_1&quot;,&quot;Column_2&quot;,&quot;Column_3&quot;]].astype(&#39;float64&#39;)
    label = data.loc[i + window_size - 1, label_col] # 窗口末尾的标签
    序列.append(window.to_numpy())
    labels.append(标签)
  返回 np.array(序列), np.array(标签)

# 创建序列和标签
序列2，标签2 = create_sequences（数据，window_size，“Column_5”）
序列2 =序列2.reshape(-1, 1) #（样本，时间步长，特征）
print(&quot;数据1准备完成&quot;)

序列3、标签3 = create_sequences(data2、window_size、“Column_5”)
序列3 = 序列3.reshape(-1, 1) # （样本、时间步长、特征）

print(&quot;数据2准备完成&quot;)

序列 = np.concatenate((序列2, 序列3))
标签 = np.concatenate((labels2, labels3), axis=0)

# 定义LSTM模型
模型 = keras.Sequential([
  keras.layers.LSTM（64，return_sequences = True，input_shape =（window_size，sequences.shape [2]）），
  keras.layers.LSTM(32),
  keras.layers.Dense(len(np.unique(labels)),activation=&#39;softmax&#39;) # 多类输出
]）

# 编译模型
model.compile(loss=&#39;categorical_crossentropy&#39;, 优化器=&#39;adam&#39;, 指标=[&#39;accuracy&#39;])
model.fit(序列, keras.utils.to_categorical(标签), epochs=1)

我也可以通过运行代码来复制它，
sequences=tf.convert_to_tensor(sequences)

期望在 LSTM 层中正确获取数据集。]]></description>
      <guid>https://stackoverflow.com/questions/78385645/failed-to-convert-a-numpy-array-to-a-tensor-for-lstm</guid>
      <pubDate>Thu, 25 Apr 2024 15:22:49 GMT</pubDate>
    </item>
    <item>
      <title>改进长尾数据的回归模型[关闭]</title>
      <link>https://stackoverflow.com/questions/78384887/improving-a-regression-model-for-long-tailed-data</link>
      <description><![CDATA[我正在对来自信贷持有的预测收入数据进行建模。正如预期的那样，该数据是长尾数据，收入较高的数据要小得多。在下图中，它是这些收入在申报收入和预测收入之间的分布。 申报收入与预测收入
正如您所看到的，该模型将低收入向上移动，但没有捕捉到长尾。我完成了一些特征工程步骤，例如对任何货币值进行对数转换、查找不同风险因素（例如信用评分）的收入中位数、查找区域收入中位数等。
模型参数如下：调优的catboost模型
{&#39;迭代&#39;：300，&#39;学习率&#39;：0.1，&#39;深度&#39;：8，&#39;损失函数&#39;：&#39;RMSE&#39;}
我选择了 catboost，因为就 RMSE 而言，它是使用 Pycaret 时最好的模型。
任何具有长尾数据建模经验并愿意提供指导的人将不胜感激]]></description>
      <guid>https://stackoverflow.com/questions/78384887/improving-a-regression-model-for-long-tailed-data</guid>
      <pubDate>Thu, 25 Apr 2024 13:09:19 GMT</pubDate>
    </item>
    <item>
      <title>R 中 Caret、Yardstick 和 MLeval 在精确度-召回率方面的差异</title>
      <link>https://stackoverflow.com/questions/78384468/discrepancy-between-caret-yardstick-and-mleval-in-r-regarding-precision-recall</link>
      <description><![CDATA[我正在尝试绘制精确召回曲线并测量插入符交叉验证的火车对象的曲线下面积。只需调用对象名称即可生成精确召回曲线下面积的一些值，如下所示：
&lt;前&gt;&lt;代码&gt;&gt;射频

随机森林

807 个样本
 11 预测器
  2 个类别：“X0”、“X1”

无需预处理
重采样：交叉验证（10 倍）
样本量摘要：727, 726, 727, 726, 727, 726, ...
跨调整参数重新采样结果：

  mtry splitrule AUC 精确召回率 F
   2 基尼系数 0.8179379 0.8618888 0.6713675 ​​0.7494214
   2 个额外树 0.8061601 0.8960233 0.5725071 0.6901257
   7 基尼系数 0.7798593 0.8775955 0.8037037 0.8360293
   7 个额外树 0.8004585 0.8587664 0.7696581 0.8090205
  12 基尼 0.7659204 0.8578710 0.8229345 0.8364962
  12 个额外树 0.7840497 0.8498209 0.7925926 0.8167108

调整参数“min.node.size”保持恒定为 1
AUC 用于使用最大值来选择最佳模型。
该模型使用的最终值为 mtry = 2、splitrule = gini 和 min.node.size = 1。

但是，当我尝试使用尺度绘制实际曲线时，我得到了完全不同的结果。
prRf &lt;- pr_curve(rf$pred, X0, true = obs)

ggplot() +
  geom_path(aes(x = 召回率，y = 精度)，颜色 = “蓝色”，线型 = 1，数据 = prRf) +
  xlab(“召回”) +
  ylab(“精度”) +
  theme_minimal() +
  ylim(0,1)

pr_auc(rf$pred, X0, 真相 = obs)

在这里，曲线看起来“更好”了很多。与插入符给出的内部 AUPR 相比，AUPR 更高（0.878 与 0.817）。对于 MLeval 的简单运行也是如此，它同样给出了“更好”的结果。结果。
&lt;前&gt;&lt;代码&gt;evalm(rf)

所有这些都让我很困惑，我觉得我可能正在以某种方式在样本内进行测试，但我不确定如何在不事先拆分数据的情况下正确地进行测试。]]></description>
      <guid>https://stackoverflow.com/questions/78384468/discrepancy-between-caret-yardstick-and-mleval-in-r-regarding-precision-recall</guid>
      <pubDate>Thu, 25 Apr 2024 11:51:30 GMT</pubDate>
    </item>
    <item>
      <title>在数据分割过程中保留数据的空间分布</title>
      <link>https://stackoverflow.com/questions/78383883/preserving-spatial-distribution-of-data-during-data-splitting</link>
      <description><![CDATA[我正在尝试使用随机森林模型来模拟德国巴伐利亚河流中的硝酸盐浓度。我使用 Python，主要使用 sklearn。我有 490 个水质站的数据。我遵循 LongzhuQ.Shen 等人论文中的方法，该论文可以在这里找到：https://www.nature.com/articles/s41597-020-0478-7
我想将数据集分成训练集和测试集，以便两个集中数据的空间分布相同。这个想法是，如果数据分割忽略空间分布，则训练集可能最终会集中来自人口稠密区域的点，而忽略稀疏区域。这可能会扭曲模型的学习过程，使其在整个感兴趣领域的准确性或概括性降低。 sklearn train_test_split只是将数据随机划分为训练集和测试集，并且不考虑数据中的空间模式。
我上面提到的论文遵循了这种方法：“我们将完整的数据集分为两个子数据集，分别是训练和测试。为了考虑监测站空间分布的异质性，我们在数据分割步骤中采用了空间密度估计技术，通过使用带宽为 50 km 的高斯核（使用 GRASS GIS33 中可用的 v.kernel）构建密度表面来计算每个物种和季节。所得密度表面的像素值用作权重因子，将数据分成具有相同空间分布的训练和测试子集。”
我想遵循相同的方法，但我不使用草地 GIS，而是自己用 Python 构建密度表面。我还提取了概率密度值和站点的权重。 （附图）
现在我面临的唯一问题是如何使用这些权重将数据分成训练集和测试集？我检查了sklearn train_test_split函数中没有可以考虑权重的关键字。我也与GPT 4聊天来回，但它也无法给我一个明确的答案。我在互联网上也没有找到任何关于此的具体信息。也许我错过了一些东西。
还有其他函数可以用来执行此操作吗？或者我必须编写自己的算法来进行分割？如果是后者，您能否建议我一种方法，以便我自己编写代码？
在附图中您可以看到站点的位置以及使用核密度估计方法（使用高斯核）生成的概率密度面。
还附上我的数据框的屏幕截图，让您了解数据结构。 （经度（‘lon’）列之后的所有列都用作特征。NO3 列用作目标变量。）
请查找附件图片以供参考。
使用高斯核的核密度估计方法生成的概率密度曲面。&lt; /p&gt;
我用来模拟硝酸盐浓度的数据集]]></description>
      <guid>https://stackoverflow.com/questions/78383883/preserving-spatial-distribution-of-data-during-data-splitting</guid>
      <pubDate>Thu, 25 Apr 2024 10:10:26 GMT</pubDate>
    </item>
    <item>
      <title>如何向 DBSCAN 添加自定义参数</title>
      <link>https://stackoverflow.com/questions/78383085/how-to-add-custom-parameter-to-dbscan</link>
      <description><![CDATA[我一直在努力创建一个自定义 DBSCAN，在其中我可以创建一个自定义参数来根据出租车 ID 过滤距离，以查看哪些出租车造成了交通堵塞。我添加了时间和距离矩阵，但我不知道如何创建一个新的 eps 来根据 ID 过滤出租车。如果 eps 值较高，则应对不同的 Taix ID 进行聚类，如果较低，则应对同一出租车进行聚类。如何进行？
time_dist = pdist(X[:, 0].reshape(n, 1), metric=self.metric)
euc_dist = pdist(X[:, 1:], metric=self.metric)


# 使用 time_dist 过滤 euc_dist 矩阵
dist = np.where(time_dist &lt;= self.eps2, euc_dist, 2 * self.eps1)

db = DBSCAN(eps=self.eps1,
            min_samples=self.min_samples,
            指标=&#39;预先计算&#39;）
db.fit(正方形(距离))

self.labels = db.labels_
]]></description>
      <guid>https://stackoverflow.com/questions/78383085/how-to-add-custom-parameter-to-dbscan</guid>
      <pubDate>Thu, 25 Apr 2024 07:54:41 GMT</pubDate>
    </item>
    <item>
      <title>尽管 GPU 可用，但 CUDA 设置失败</title>
      <link>https://stackoverflow.com/questions/78376600/cuda-setup-failed-despite-gpu-being-available</link>
      <description><![CDATA[我需要使用bitsandbytes包来运行使用Falcon7B模型的代码。我已经安装了 CUDA，并且我的系统具有 NVIDIA RTX A6000 GPU。我的系统有 Windows 11 操作系统。
这是代码，它只是导入部分：
导入火炬
从数据集导入load_dataset
从变压器导入 AutoModelForCausalLM、AutoTokenizer、BitsAndBytesConfig、TrainingArguments、GenerationConfig
从peft导入LoraConfig，get_peft_model，PeftConfig，PeftModel，prepare_model_for_kbit_training
从 trl 导入 SFTTrainer
进口警告
warnings.filterwarnings(“忽略”)

这是错误：
运行时错误：
        尽管 GPU 可用，但 CUDA 安装失败。请运行以下命令来获取更多信息：

        python -m 位和字节

        检查命令的输出并查看是否可以找到 CUDA 库。您可能需要添加它们
        到您的 LD_LIBRARY_PATH。如果您怀疑存在错误，请从 python -m bitsandbytes 获取信息
        并在以下位置提出问题：https://github.com/TimDettmers/bitsandbytes/issues



RuntimeError：由于以下错误而无法导入transformers.training_args（查找其回溯）：

        尽管 GPU 可用，但 CUDA 安装失败。请运行以下命令来获取更多信息：

        python -m 位和字节

        检查命令的输出并查看是否可以找到 CUDA 库。您可能需要添加它们
        到您的 LD_LIBRARY_PATH。如果您怀疑存在错误，请从 python -m bitsandbytes 获取信息
        并在以下位置提出问题：https://github.com/TimDettmers/bitsandbytes/issues

有时不会出现此错误，并且代码可以正常工作。但大多数时候我都会遇到此错误，并且无法找到准确的修复方法。
当系统中未安装 CUDA 时，首次出现此错误。安装后没有报错，但是第二天再次运行时，又出现了同样的错误。
接下来我尝试将 python 版本降级到 3.11.1 以下，之后代码再次运行。但今天我再次面临同样的错误。
这是我的 CUDA 版本：
&lt;前&gt;&lt;代码&gt;nvcc --版本
nvcc：NVIDIA (R) Cuda 编译器驱动程序
版权所有 (c) 2005-2023 NVIDIA 公司
建于 Wed_Feb__8_05:53:42_Cooperative_Universal_Time_2023
Cuda 编译工具，版本 12.1，V12.1.66
构建cuda_12.1.r12.1/compiler.32415258_0
]]></description>
      <guid>https://stackoverflow.com/questions/78376600/cuda-setup-failed-despite-gpu-being-available</guid>
      <pubDate>Wed, 24 Apr 2024 07:18:58 GMT</pubDate>
    </item>
    </channel>
</rss>