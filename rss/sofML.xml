<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Fri, 01 Dec 2023 06:19:27 GMT</lastBuildDate>
    <item>
      <title>在启用反向传播的情况下，有效模拟具有异质空间和时间感受野的神经元？</title>
      <link>https://stackoverflow.com/questions/77582849/efficient-simulation-of-neurons-with-heterogeneous-spatial-and-temporal-receptiv</link>
      <description><![CDATA[我想找到一种方法来对具有异构空间和时间感受野的神经元进行高效模拟，并且理想情况下启用反向传播，以便我可以微调感受野的权重。每个神经元的计算简单，但神经元数量较多（1e5 ~ 1e6）。主要困难在于每个神经元的空间和时间感受野的大小不同（从动物实验数据获得）并且变化很大。在GPU上进行矢量化运算有点困难。
模拟如下：
# 初始化
# 每个神经元的k和l的值是不同的。
获取输入图像的形状（在整个模拟过程中固定）。
对于每个神经元：
    (1).获取其空间感受野下的像素索引，存储为 [k x 2] 数组，因为空间感受野可能不是正方形。
    （2）。初始化空间感受野中的权重，存储为 [k x 1] 数组。
    （3）。获取时间感受野的长度l（时间步数）。
    （4）。初始化时间感受野中的权重，存储为 [l x 1] 数组。
    （5）。初始化一个空的 [l x 1] 数组缓冲区，用零填充。
将所有神经元的参数存储在列表中。

＃ 模拟
对于每个输入图像：
    对于每个神经元：
        (1).根据图像的 [k x 2] 像素索引获得展平的补丁，存储为 [k x 1] 数组。
        （2）。计算图像块与空间感受野权重的内积，输出 [1 x 1] 标量。
        （3）。删除缓冲区中的第一个元素，将其余元素与 [1 x 1] 标量（最后位置的标量）连接，输出 [l x 1] 更新的缓冲区。
        （4）。计算更新后的缓冲区与时间感受野权重的内积，输出 [1 x 1] 标量。
        （5）。将标量传递给 ReLU 函数，将输出设置为神经元的输出。

我已经在 Python JAX 中实现了上述过程。实现与上面的描述几乎完全相同，包括仿真阶段的双for循环。但CPU和GPU上的性能都很慢。
目前，我想找到一种方法来优化模拟过程中的每个神经元循环（因为在测试时，可以从相机实时获取输入图像）。初始化阶段的性能已经不错了。我想要实现的是：

在 CPU 上运行时使用所有可用的 CPU 内核。默认情况下，JAX 仅使用一个 CPU 核心。
在 GPU 上运行时对计算进行向量化，同时保持内存效率。每个神经元的空间和时间感受野的大小都不同。如果我们简单地将小感受野补零到最大尺寸，它可能很容易消耗整个 GPU 内存。
理想情况下，将来可以通过反向传播重复使用相同的模拟代码进行训练。

非常欢迎任何建议！]]></description>
      <guid>https://stackoverflow.com/questions/77582849/efficient-simulation-of-neurons-with-heterogeneous-spatial-and-temporal-receptiv</guid>
      <pubDate>Fri, 01 Dec 2023 03:26:31 GMT</pubDate>
    </item>
    <item>
      <title>unserialize(socklist[[n]]) 中的错误：在插入符中执行 RF 模型时从连接读取错误</title>
      <link>https://stackoverflow.com/questions/77582605/error-in-unserializesocklistn-error-reading-from-connection-while-execut</link>
      <description><![CDATA[我正在尝试运行随机森林代码，以使用 R studio 中的插入符包对卫星图像进行分类。在训练模型时，我总是收到错误“unserialize(socklist[[n]]) 中的错误：从连接读取错误”。尝试关闭集群时“unserialize(node$con) 中的错误：从连接读取时出错”这个错误来了。我正在使用 caret 包并使用 doparallel 包进行并行计算
&quot;cl &lt;- makeCluster(3/4 * detectorCores())
registerDoParallel(cl)”
我使用的系统具有 i9（第 13 代）、128 GB 内存，并尝试使用 28、24、16 核。所有的尝试都以这个错误告终。我重新安装了 R 和 R studio，将版本更改为旧版本，在终端中运行脚本，但出现了同样的错误。相同的代码在 i5（第 9 代）笔记本电脑上成功运行； 16 GB 内存仅使用 3 个核心，但输出需要 12 小时。我该如何解决这个问题。这是代码、R studio 还是我正在使用的系统的问题？]]></description>
      <guid>https://stackoverflow.com/questions/77582605/error-in-unserializesocklistn-error-reading-from-connection-while-execut</guid>
      <pubDate>Fri, 01 Dec 2023 01:53:00 GMT</pubDate>
    </item>
    <item>
      <title>有人可以向我解释这个错误吗？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77582582/guys-can-some-one-explain-this-error-to-me</link>
      <description><![CDATA[文件“e:\\Projects\\Ai\\NumReco\\train.py”，第 16 行，位于 __init__ 中
   尝试：self.qtr，self.atr，self.qte，self.ate=self.train，self.test=self.mnist=self.getqa（路径）
                                           ^^^^^^^^^^^^^^^^^^^^^^
ValueError：需要解压的值太多（预期为 2）

有人可以告诉我为什么有“太多的值需要解包（预计有 2 个）”，但我却得到了两个需要解包的值？]]></description>
      <guid>https://stackoverflow.com/questions/77582582/guys-can-some-one-explain-this-error-to-me</guid>
      <pubDate>Fri, 01 Dec 2023 01:39:01 GMT</pubDate>
    </item>
    <item>
      <title>将每个计算/汇总的时间序列高分辨率数据集存储为更简单的事务集（包括其元数据）</title>
      <link>https://stackoverflow.com/questions/77582450/store-time-series-high-resolution-data-sets-each-computed-summarized-into-a-simp</link>
      <description><![CDATA[我正在测试时间序列高分辨率数据集，每个数据集都计算/汇总为一个更简单的事务集（包括其元数据）。这些存储在本地服务器中。我很少遇到连接表的需求；但是，我不断使用时间序列数据来创建其他指标来构建机器学习模型。
CSV 中的每个时间序列数据集最大可达 2.6MB，而交易数据则为 kB 大小。我有超过 20,000 组数据。
什么是具有最佳性能的数据库选项/架构？
关系型还是非关系型？
每个选项的供应商有哪些？
如何确定是否应该迁移到云服务器？
推荐的供应商有哪些？]]></description>
      <guid>https://stackoverflow.com/questions/77582450/store-time-series-high-resolution-data-sets-each-computed-summarized-into-a-simp</guid>
      <pubDate>Fri, 01 Dec 2023 00:47:12 GMT</pubDate>
    </item>
    <item>
      <title>如何在此示例代码中探索 AWS 分析/ML/AI 服务？ [关闭]</title>
      <link>https://stackoverflow.com/questions/77582249/how-do-i-explore-aws-analytics-ml-ai-services-in-this-example-code</link>
      <description><![CDATA[我想开始使用 AWS 的一些机器学习模型。我看到他们最近发布了 Bedrock 服务，我想设置一个环境来执行此操作。我还需要与 S3、Langchain 和 Vector 数据库（例如 Pinecone）进行交互。这里有一些关于我可能会玩的想法： 示例
我的电脑上有 VS Code，并且我看过一份指南，详细介绍了设置此功能的一些步骤：指南
我从示例链接中看到的是，我猜测使用了 Jupyter notbeook。另外（请原谅我的天真），如果我将 S3 文件放入 Vector 数据库，这些服务肯定需要启动（或设置），并由我付费。之后，只需通过 AWS Bedrock（处理许多底层内容）查询数据库即可。
所以我想我的问题是最初将数据导入矢量数据库：
我需要 Jupyter 笔记本吗？
我需要 AWS Sagemaker 吗？我在设置 SageMaker 时看到，您可以指定 EC2 或某些计算能力。
我需要 VS Code + Python + 其他东西吗？
或者此处链接的指南是否足够？
本质上，要使用我链接的示例，我的设置应该是什么？]]></description>
      <guid>https://stackoverflow.com/questions/77582249/how-do-i-explore-aws-analytics-ml-ai-services-in-this-example-code</guid>
      <pubDate>Thu, 30 Nov 2023 23:35:25 GMT</pubDate>
    </item>
    <item>
      <title>SHAP 蜂群图解读</title>
      <link>https://stackoverflow.com/questions/77581801/shap-beeswarm-plot-interpretation</link>
      <description><![CDATA[我创建了一个 SHAP 蜂群图，我发现当高特征值和低特征值以微笑随机方式聚集在一起时，很难解释变量的影响。以下是几个示例（绿色 = 高，蓝色 = 低）：
示例 1

示例 2

在示例 1 中，我至少可以说极高的特征值对模型输出有积极的影响。但是，我不明白如何处理高点和低点集群，特别是在示例 2 中。我唯一的想法是，具有相同 SHAP 值的相同数量的高点和低点特征值。
那么，对于此类簇是否有更有意义的解释？]]></description>
      <guid>https://stackoverflow.com/questions/77581801/shap-beeswarm-plot-interpretation</guid>
      <pubDate>Thu, 30 Nov 2023 21:22:32 GMT</pubDate>
    </item>
    <item>
      <title>获取 'model.fit' keras API 参数的值</title>
      <link>https://stackoverflow.com/questions/77581428/get-values-of-model-fit-keras-api-parameters</link>
      <description><![CDATA[我正在尝试使用自定义回调函数获取 Kera 顺序模型的详细信息。我需要提取 model.fit() API 中设置的参数的所有值，例如 batch_size、epochs、validation_split 等。但我无法在 Keras 的回调中访问它们。您知道如何自动获取这些值吗？
我正在使用 Python 3.10 和 Keras 2.8。]]></description>
      <guid>https://stackoverflow.com/questions/77581428/get-values-of-model-fit-keras-api-parameters</guid>
      <pubDate>Thu, 30 Nov 2023 20:13:45 GMT</pubDate>
    </item>
    <item>
      <title>我的项目应该使用 TensorFlow 还是 PyTorch？</title>
      <link>https://stackoverflow.com/questions/77581368/should-i-use-tensorflow-or-pytorch-for-my-project</link>
      <description><![CDATA[我正在开展一个导师与学员匹配项目，我的目标是使用深度学习技术和语言模型（例如 BERT 或 GPT）来分析导师和学员的文本资料，以找到合适的匹配。该项目涉及对文本配置文件进行编码、计算相似性分数、生成建议，以及可能微调预训练模型以获得更好的性能。
我正在决定使用 TensorFlow 还是 PyTorch 来实施这个项目。考虑到所涉及的任务，例如文本编码、相似性计算和潜在的微调模型，哪种框架更适合此类项目？在导师与受训者匹配任务的背景下，我应该考虑这两种框架的具体优点或局限性吗？]]></description>
      <guid>https://stackoverflow.com/questions/77581368/should-i-use-tensorflow-or-pytorch-for-my-project</guid>
      <pubDate>Thu, 30 Nov 2023 20:00:14 GMT</pubDate>
    </item>
    <item>
      <title>我们可以在一个 Colab（或 Jupyter Notebook）中使用“train_test_split”两次吗？</title>
      <link>https://stackoverflow.com/questions/77555330/can-we-use-train-test-split-in-one-single-colabor-jupyter-notebook-twice</link>
      <description><![CDATA[我必须使用决策树机器学习算法执行分类和回归。现在我已经完成了代码的回归部分。如果我继续对此进行分类任务，我应该对预处理的数据集执行train_test_split。 在代码中我必须定义 X 和 y 变量，然后执行 X_train、X_test、y_train 和 y_test 部分。回归和分类中都会重复相同的变量。通过从分类中获取相同的变量，它会考虑回归中首先给出的先前值还是会采用新给定的值？
我想清楚地知道我们是否可以在单个 colab 或 jupyter 笔记本中多次使用 train_test_split 函数。]]></description>
      <guid>https://stackoverflow.com/questions/77555330/can-we-use-train-test-split-in-one-single-colabor-jupyter-notebook-twice</guid>
      <pubDate>Mon, 27 Nov 2023 08:05:19 GMT</pubDate>
    </item>
    <item>
      <title>基于预定义的特征子集创建分类器集合</title>
      <link>https://stackoverflow.com/questions/77524700/creating-an-ensemble-of-classifiers-based-on-predefined-feature-subsets</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77524700/creating-an-ensemble-of-classifiers-based-on-predefined-feature-subsets</guid>
      <pubDate>Tue, 21 Nov 2023 17:12:25 GMT</pubDate>
    </item>
    <item>
      <title>如何按照官方方式将 Hugging Face LLaMA v2 模型的权重重新初始化为原始模型？</title>
      <link>https://stackoverflow.com/questions/77499162/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-offic</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77499162/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-offic</guid>
      <pubDate>Fri, 17 Nov 2023 03:15:56 GMT</pubDate>
    </item>
    <item>
      <title>如何防止 Keras 在训练期间计算指标</title>
      <link>https://stackoverflow.com/questions/71412499/how-to-prevent-keras-from-computing-metrics-during-training</link>
      <description><![CDATA[我正在使用 Tensorflow/Keras 2.4.1，并且我有一个（无监督的）自定义指标，它将我的多个模型输入作为参数，例如：
model = build_model() # 返回一个 tf.keras.Model 对象
my_metric = custom_metric(model.output, model.input[0], model.input[1])
模型.add_metric(my_metric)
[...]
model.fit([...]) # 使用 fit 进行训练

但是，custom_metric 非常昂贵，因此我希望仅在验证期间计算它。我找到了这个答案，但我几乎不明白如何使解决方案适应我的指标，该指标使用多个模型输入作为参数，因为update_state 方法似乎不太灵活。
在我的上下文中，除了编写自己的训练循环之外，是否有办法避免在训练期间计算我的指标？
另外，我很惊讶我们无法本机指定 Tensorflow 某些指标只能在验证时计算，这有什么原因吗？
此外，由于模型经过训练来优化损失，并且训练数据集不应用于评估模型，我什至不明白为什么默认情况下 Tensorflow 在训练期间计算指标。]]></description>
      <guid>https://stackoverflow.com/questions/71412499/how-to-prevent-keras-from-computing-metrics-during-training</guid>
      <pubDate>Wed, 09 Mar 2022 16:11:26 GMT</pubDate>
    </item>
    <item>
      <title>Keras 何时以及如何计算每批样本的指标？</title>
      <link>https://stackoverflow.com/questions/66311611/when-and-how-keras-calculate-metrics-for-each-batch-of-samples</link>
      <description><![CDATA[我看到 Keras 自定义指标如何工作，并且指标函数中的 tf.print 与 model.fit 的回调打印之间的计算不匹配。
导入张量流为 tf # tf2.4.1
将 numpy 导入为 np
模型 = tf.keras.models.Sequential(
    tf.keras.layers.Dense(1, input_shape=(1,))
）
def my_metric_fn(y_true, y_pred):
    squared_difference = tf.square(y_true - y_pred)
    损失 = tf.reduce_mean(squared_difference, axis=-1)
    tf.print(y_true.shape, y_pred.shape, 损失, tf.reduce_mean(squared_difference))
    回波损耗
model.compile（优化器=&#39;adam&#39;，损失=&#39;mean_squared_error&#39;，指标=[my_metric_fn]）
x = np.random.rand(4,1)
y = x ** 2
历史= model.fit（x = x，y = y，batch_size = 2，epochs = 2）
打印（历史.历史）

输出（格式化以提高可读性）
纪元 1/2
TensorShape([2, 1]) TensorShape([2, 1]) [9.79962078e-06 0.0534314588] 0.02672063
1/2 [==============&gt;........................] - ETA：0秒 - 损失：0.0267 - my_metric_fn：0.0267
TensorShape([2, 1]) TensorShape([2, 1]) [0.0397406667 0.179955378] 0.109848022
2/2 [================================] - 0s 7ms/步 - 损耗：0.0544 - my_metric_fn：0.0544

纪元2/2
TensorShape([2, 1]) TensorShape([2, 1]) [0.0392204635 0.0521505736] 0.0456855185
1/2 [==============&gt;........................] - ETA：0秒 - 损失：0.0457 - my_metric_fn：0.0457
TensorShape([2, 1]) TensorShape([2, 1]) [0.177408844 2.45939535e-08] 0.088704437
2/2 [================================] - 0s 5ms/步 - 损耗：0.0600 - my_metric_fn：0.0600
{&#39;损失&#39;：[0.06828432530164719，0.06719497591257095]，&#39;my_metric_fn&#39;：[0.06828432530164719，0.06719497591257095]}

在上面的输出中查看批次的打印损失。
纪元 1/2 1/2 tf.print：0.02672063，model.fit：0.0267。好的。
Epoch 1/2 2/2 tf.print：0.109848022，但 model.fit：0.0544。不行。
如何理解这些匹配和不匹配？ 0.0544 从哪里来？]]></description>
      <guid>https://stackoverflow.com/questions/66311611/when-and-how-keras-calculate-metrics-for-each-batch-of-samples</guid>
      <pubDate>Mon, 22 Feb 2021 07:31:49 GMT</pubDate>
    </item>
    <item>
      <title>使用 keras 计算每个时期的 Fscore（不是批量）</title>
      <link>https://stackoverflow.com/questions/61683829/calculating-fscore-for-each-epoch-using-keras-not-batch-wise</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/61683829/calculating-fscore-for-each-epoch-using-keras-not-batch-wise</guid>
      <pubDate>Fri, 08 May 2020 16:41:15 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：feature_names 不匹配：在 xgboost 中的 Predict() 函数中</title>
      <link>https://stackoverflow.com/questions/42338972/valueerror-feature-names-mismatch-in-xgboost-in-the-predict-function</link>
      <description><![CDATA[我训练了一个 XGBoostRegressor 模型。当我必须使用这个经过训练的模型来预测新输入时，predict() 函数会抛出 feature_names 不匹配错误，尽管输入特征向量与训练数据具有相同的结构。
此外，为了以与训练数据相同的结构构建特征向量，我做了很多低效的处理，例如添加新的空列（如果数据不存在），然后重新排列数据列，以便它与训练结构相匹配。是否有更好、更简洁的方式来格式化输入以使其与训练结构相匹配？]]></description>
      <guid>https://stackoverflow.com/questions/42338972/valueerror-feature-names-mismatch-in-xgboost-in-the-predict-function</guid>
      <pubDate>Mon, 20 Feb 2017 07:43:24 GMT</pubDate>
    </item>
    </channel>
</rss>