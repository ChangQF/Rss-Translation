<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Fri, 31 May 2024 15:17:18 GMT</lastBuildDate>
    <item>
      <title>RLHF 项目：能否在具备 ML/RL 基础知识的情况下在几周内完成，如果可以，请给出有效学习 RLHF 的步骤 [关闭]</title>
      <link>https://stackoverflow.com/questions/78559656/project-on-rlhf-can-it-be-done-in-few-weeks-with-basic-knowledge-of-ml-rl-if-s</link>
      <description><![CDATA[我是人工智能专业的一年级本科生，在基础 ML/DL 项目方面有超过 1 年的经验，几天前我开始学习 RL，有人问起我一个关于 RLHF 的项目——使用人类反馈的强化学习

该项目基于本地大型语言模型开发

我应该如何学习 RLHF，给我一些建议/主题列表来介绍
到目前为止，我在 RL 中只知道基础知识：MDP 和 Q 的基础知识]]></description>
      <guid>https://stackoverflow.com/questions/78559656/project-on-rlhf-can-it-be-done-in-few-weeks-with-basic-knowledge-of-ml-rl-if-s</guid>
      <pubDate>Fri, 31 May 2024 10:53:42 GMT</pubDate>
    </item>
    <item>
      <title>如何在 keras 中添加具有可训练参数的自定义损失函数</title>
      <link>https://stackoverflow.com/questions/78559415/how-to-add-custom-loss-function-with-trainable-parameter-in-keras</link>
      <description><![CDATA[作为一个项目，我正在训练一个 LSTM 模型来预测未来的值。为此，我想定义一个与“mse”相同的自定义损失函数。但平方差将与 e^alpha 的指数项相乘。并且这个 alpha 项应该随着训练过程而更新。
我不确定我是否朝着正确的方向前进，但我已经用 mse 训练了模型，它通过使用真实数据给出了很好的预测。但作为现实生活中的预测，当我们在一段时间内没有真实数据时，我的模型应该使用最后一个预测作为模型下一个输入的输入，就像这样。在将模型作为此任务进行测试时，预测值不断偏离最后一个真实值。当一段时间后有新的真实数据可用时，它应该与未来的值相匹配。]]></description>
      <guid>https://stackoverflow.com/questions/78559415/how-to-add-custom-loss-function-with-trainable-parameter-in-keras</guid>
      <pubDate>Fri, 31 May 2024 09:59:12 GMT</pubDate>
    </item>
    <item>
      <title>神经网络在线性回归中表现良好，但在非线性回归中表现不佳</title>
      <link>https://stackoverflow.com/questions/78559180/neural-network-performs-well-in-linear-regression-but-poorly-in-nonlinear-regres</link>
      <description><![CDATA[我目前正在训练一个用于特征选择的神经网络，它在线性回归任务中表现良好，但在特定类型的非线性回归中表现不佳。该模型由两层结构组成，并使用自定义激活函数，它是 ReLU 的简单变体，定义为 max(-1, x)。为了增强稀疏性和特征选择，该模型使用成本函数进行训练，该函数在第一层的权重和偏差上包含惩罚项。
我尝试建模的非线性关系定义为 y = \sum_{i \in S} |x_i|，其中 S 是选定特征的集合。事实证明，这种绝对值总和模型对于网络来说很难学习。

自定义激活函数或模型的浅层架构是否会限制其有效建模这种非线性关系的能力？
架构、激活函数或训练过程的变化如何改善模型在此特定非线性任务上的性能？
有没有更好的方法来处理正则化，同时仍然捕获此非线性回归所需的复杂性？
也许可以尝试学习率或权重初始化？

任何有助于提高性能的修改见解或建议都将不胜感激！]]></description>
      <guid>https://stackoverflow.com/questions/78559180/neural-network-performs-well-in-linear-regression-but-poorly-in-nonlinear-regres</guid>
      <pubDate>Fri, 31 May 2024 09:20:40 GMT</pubDate>
    </item>
    <item>
      <title>多输出分类器低分</title>
      <link>https://stackoverflow.com/questions/78559087/multioutputclassifier-low-score</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78559087/multioutputclassifier-low-score</guid>
      <pubDate>Fri, 31 May 2024 09:01:14 GMT</pubDate>
    </item>
    <item>
      <title>即使指定了某些列，Pandas 也会获取数据框的所有列</title>
      <link>https://stackoverflow.com/questions/78559070/pandas-takes-all-columns-of-a-dataframe-even-when-some-columns-are-specified</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78559070/pandas-takes-all-columns-of-a-dataframe-even-when-some-columns-are-specified</guid>
      <pubDate>Fri, 31 May 2024 08:59:38 GMT</pubDate>
    </item>
    <item>
      <title>如何在多标签分类中实现类别权重采样？</title>
      <link>https://stackoverflow.com/questions/78559061/how-to-implement-class-weight-sampling-in-multi-label-classification</link>
      <description><![CDATA[我正在研究一个多标签分类问题，需要一些使用 Scikit-Learn 计算类别权重的指导。
问题背景：
我有一个包含 9973 个训练样本的数据集。标签是独热编码的，代表 13 个不同的类别。我的训练标签的形状是 (9973, 13)。
我想使用此代码：
import numpy as np
from sklearn.utils.class_weight import compute_class_weight

y_integers = np.argmax(y, axis=1)
class_weights = compute_class_weight(&#39;balanced&#39;, np.unique(y_integers), y_integers)
d_class_weights = dict(enumerate(class_weights))

这不起作用，因为位置参数太多。我的训练样本如下所示：
 [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],

如何在多类别分类问题中实现，以便解决我的数据集不平衡问题？
编辑 1：它现在运行良好，您认为它在多标签中有效吗，因为我在某处读到，您必须使用采样权重而不是类别权重。我该如何实现呢？]]></description>
      <guid>https://stackoverflow.com/questions/78559061/how-to-implement-class-weight-sampling-in-multi-label-classification</guid>
      <pubDate>Fri, 31 May 2024 08:57:17 GMT</pubDate>
    </item>
    <item>
      <title>成本函数最小值太低</title>
      <link>https://stackoverflow.com/questions/78558974/cost-function-minimum-is-too-low</link>
      <description><![CDATA[下面是我正在尝试编写的神经网络的一段 Matlab 代码。这是我第一次尝试与机器学习相关的任何事情。我在这里跟随 Michael Nielson 的书：http://neuralnetworksanddeeplearning.com/chap2.html
我正在加载一组 60000 张 28x28 灰度手写数字图像，并尝试训练这个神经网络来识别它们。还有一个包含 10000 张图像的测试数据集。该网络有 784 个输入神经元（28^2），两个隐藏层，每个隐藏层有 16 个神经元，输出层有 10 个神经元。我将权重和偏差初始化为 -0.5 和 0.5 之间的随机值。
我将成本函数评估为 C = 0.5*(a-y).^2。它似乎取得了一定成功，因为它从 C=1.35 开始，在 C=0.46 结束，然后基本趋于平稳（大约 75 个时期）。但是，错误仍然很高，只有 12% 的时间能猜出正确的数字，这几乎是随机的。我反复检查了数学，但找不到错误。我想一定有一个我没有看到的。下面的代码是主训练循环中的所有内容，因此任何错误都应该在那里。我没有将图像分成更小的批次，而是在每个时期一次处理整个 60k 图像。由于每张图像只有 28x28 像素，因此无需将其分开就足够快了。输入神经元 a_0 是一个 784x60000 的双精度数组，值介于 0 和 1 之间。我获取了原始图像，其中每个像素都是一个 uint8，然后将其转换为双精度，然后除以 255 得到 a_0。我在代码中对层进行编号，其中第 0 层是输入层，第 1 层和第 2 层是隐藏层，第 3 层是输出层。
a_0 = training_images;
epoch = 0;
while epoch &lt; 5 || C(epoch - 1) - C(epoch) &gt; 0.001 
epoch = epoch + 1;

%向前传播
z_1 = weights_1*a_0 + biases_1;
a_1 = sigmoid(z_1);
z_2 = weights_2*a_1 + biases_2;
a_2 = sigmoid(z_2);
z_3 = weights_3*a_2 + biases_3;
a_3 = sigmoid(z_3);

%评估成本函数
C(epoch) = 0.5*mean(sum((a_3-y).^2, 1));

%向后传播
sigmoid_d1 = a_1 .* (1-a_1); %Sigmoid 导数
sigmoid_d2 = a_2 .* (1-a_2);
sigmoid_d3 = a_3 .* (1-a_3);
delta_3 = (a_3-y).*sigmoid_d3;
delta_2 = weights_3.&#39;*delta_3 .* sigmoid_d2;
delta_1 = weights_2.&#39;*delta_2 .* sigmoid_d1;

%计算梯度
for image_index = 1:num_images
dC_dw3(:, :, image_index) = delta_3(:, image_index) * a_2(:, image_index).&#39;;
dC_dw2(:, :, image_index) = delta_2(:, image_index) * a_1(:, image_index).&#39;;
dC_dw1(:, :, image_index) = delta_1(:, image_index) * a_0(:, image_index).&#39;;
end

%计算调整
training_rate = 0.1;
adjust_biases_1 = -training_rate * mean(delta_1, 2);
adjust_biases_2 = -training_rate * mean(delta_2, 2);
adjust_biases_3 = -training_rate * mean(delta_3, 2);
调整权重1 = -训练速率 * 平均值(dC_dw1, 3);
调整权重2 = -训练速率 * 平均值(dC_dw2, 3);
调整权重3 = -训练速率 * 平均值(dC_dw3, 3);
偏差1 = 偏差1 + 调整偏差1;
偏差2 = 偏差2 + 调整偏差2;
偏差3 = 偏差3 + 调整偏差3;
权重1 = 权重1 + 调整权重1;
权重2 = 权重2 + 调整权重2;
权重3 = 权重3 + 调整权重3;
]]></description>
      <guid>https://stackoverflow.com/questions/78558974/cost-function-minimum-is-too-low</guid>
      <pubDate>Fri, 31 May 2024 08:39:40 GMT</pubDate>
    </item>
    <item>
      <title>通过单一数字指标训练 XGBoost</title>
      <link>https://stackoverflow.com/questions/78558863/training-xgboost-over-a-single-number-metric</link>
      <description><![CDATA[假设我正在用 Python（xgboost 版本 2.0.3）构建一个 XGBoost 模型（这里是回归还是分类完全不重要）来预测股票市场时间序列分析中的目标变量。
例如，目标可能是：时间序列中的下一个值或二进制变量，如果下一个值高于前一个值，则设置为 1，否则设置为 0。
为了训练模型，是否可以使用回归问题中的 MSE 或分类问题中的“二元逻辑”。
训练后，可以根据测试集中模型的输出对策略进行回测并计算总体回报。
我的问题是：使用 xgboost scikit-learn 接口，是否可以根据用于回测策略的性能指标来训练模型？
例如：按照策略最大化训练集中的总体回报规则。
在xgboost库网站上，展示了如何使用自定义损失函数来训练模型：
def softprob_obj(labels: np.ndarray, predt: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:
rows = labels.shape[0]
classes = predt.shape[1]
grad = np.zeros((rows, classes), dtype=float)
hess = np.zeros((rows, classes), dtype=float)
eps = 1e-6
for r in range(predt.shape[0]):
target = labels[r]
p = softmax(predt[r, :])
for c in range(predt.shape[1]):
g = p[c] - 1.0 if c == target else p[c]
h = max((2.0 * p[c] * (1.0 - p[c])).item(), eps)
grad[r, c] = g
hess[r, c] = h

grad = grad.reshape((rows * classes, 1))
hess = hess.reshape((rows * classes, 1))
return grad, hess

clf = xgb.XGBClassifier(tree_method=&quot;hist&quot;, objective=softprob_obj)

目标函数需要计算梯度和 hessian。
假设函数定义如下：
def maximum_performance_metric(y_true: np.ndarray, y_pred: np.ndarray):
# 指标计算（例如：使用 y_pred 计算总体回报
overall_return = get_overall_return(y_pred, real_prices, ...) #overall_return 是浮点数
return grad, hess


是否可以根据总体回报计算梯度和 hessian，然后使用此自定义损失训练模型函数？
函数maximize_performance_metric如何访问包含real_prices的变量（需要整体回报计算？]]></description>
      <guid>https://stackoverflow.com/questions/78558863/training-xgboost-over-a-single-number-metric</guid>
      <pubDate>Fri, 31 May 2024 08:14:07 GMT</pubDate>
    </item>
    <item>
      <title>我如何将网络安全应用于机器学习算法[关闭]</title>
      <link>https://stackoverflow.com/questions/78558772/how-could-i-apply-network-security-to-machine-learning-algorithm</link>
      <description><![CDATA[毕业设计
大家好，我是一名计算机科学专业的学生，​​我正在做一个毕业设计，主题是基于机器学习算法的再生相关基因研究，它将分析数据并确定某些生物体中的特定基因以加速或控制再生，但是我想在项目中加入一些网络安全方面的内容，你们知道我该怎么做吗？
我想做云计算，然后封装数据供用户访问。然而，这无关紧要，因为用户将无法输入任何数据。我只需要训练算法来找到特定的基因]]></description>
      <guid>https://stackoverflow.com/questions/78558772/how-could-i-apply-network-security-to-machine-learning-algorithm</guid>
      <pubDate>Fri, 31 May 2024 07:53:15 GMT</pubDate>
    </item>
    <item>
      <title>yolo 训练精度低，map少</title>
      <link>https://stackoverflow.com/questions/78558728/yolo-training-with-low-precision-and-low-map</link>
      <description><![CDATA[我正在使用 YOLOv5 训练一个模型来识别纸牌游戏中的纸牌。我从预训练模型 yolov5s.pt 开始，我的数据集由 138 张图片组成。然而，训练期间准确率和 mAP 非常低，分别从 2.35e-05 和 2.27e-05 开始，经过 80 个 epoch 后，它们仅达到 0.0169 和 0.0547。
我不知道问题出在哪里。有人能帮我吗？
这是训练批次图像和输出表的图片。


顺便说一句，我只想识别弃牌，而不是手牌。
我试过改变批次大小等。但变化不大。]]></description>
      <guid>https://stackoverflow.com/questions/78558728/yolo-training-with-low-precision-and-low-map</guid>
      <pubDate>Fri, 31 May 2024 07:43:37 GMT</pubDate>
    </item>
    <item>
      <title>使用句子相似度 NLP 匹配相似度较高的字符串</title>
      <link>https://stackoverflow.com/questions/78558021/matching-strings-with-high-similarity-using-sentence-similarity-nlp</link>
      <description><![CDATA[因此，目前我的数据库中有一个向量列表，并且我从 API 获取数据，从该 API 我循环遍历提供的每个字符串，将其转换为向量并匹配数据库中最相似的字符串。问题是，API 提供的名称与我存储在数据库中的名称不同，尽管它们是相同的。
例如，在我的数据库中，我有两所大学，分别名为 SUNY College of Technology at Alfred 和 Alfred University。从 API 中我返回的大学名称是 Alfred State College 和 Alfred University。显然，句子相似度将为 Alfred University 提供完美的相似度，但 Alfred State College 不会与 SUNY College of Technology at Alfred 匹配，而是与 Alfred University 匹配，我明白为什么它们尚未匹配，尽管名称不同，但它们是同一所大学。我能做些什么来使系统更准确？
我尝试将大学所在州添加到向量中，然后根据大学名称和州匹配一个向量，但这两所大学都是同一个州，所以这是一条死路。我正在考虑创建一些函数，如果有多个匹配项，它将暂缓处理该数据，然后将其推送到数组中。它会继续，直到找到相似度为 1 的匹配项，然后它会区分两者，并将最不准确的结果给予相似度较低的匹配项。这会起作用吗？它会叫什么？
我能做什么？]]></description>
      <guid>https://stackoverflow.com/questions/78558021/matching-strings-with-high-similarity-using-sentence-similarity-nlp</guid>
      <pubDate>Fri, 31 May 2024 03:46:06 GMT</pubDate>
    </item>
    <item>
      <title>具有不平衡类别的 U-Net 分割的图像块提取</title>
      <link>https://stackoverflow.com/questions/78555784/image-patch-extraction-for-u-net-segmentation-with-imbalanced-classes</link>
      <description><![CDATA[我正在使用 U-Net 进行多类图像分割项目。
我的数据集的类别分布不平衡。有些类别几乎出现在每幅图像中，而其他类别则很少见。我不确定图像修补的最佳方法：
在每个补丁内做出相等的类别表示？在这种情况下，对于某些类别，我将不得不使用数据增强技术。
还是保持补丁内原始的不平衡比例？]]></description>
      <guid>https://stackoverflow.com/questions/78555784/image-patch-extraction-for-u-net-segmentation-with-imbalanced-classes</guid>
      <pubDate>Thu, 30 May 2024 14:57:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么加载 AutoTokenizer 会占用这么多的 RAM？</title>
      <link>https://stackoverflow.com/questions/78546693/why-loading-autotokenizer-takes-so-much-ram</link>
      <description><![CDATA[我测量了脚本使用的 RAM，惊讶地发现它占用了大约 300Mb 的 RAM，而 tokenizer 文件本身大约只有 9MB。这是为什么？
我试过：
from transformers import AutoTokenizer
from memory_profiler import profile

@profile
def load_tokenizer():
path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
tokenizer = AutoTokenizer.from_pretrained(path)

return tokenizer

load_tokenizer()

输出：
行 # 内存使用量 增量 发生次数 行内容
==================================================================
4 377.4 MiB 377.4 MiB 1 @profile
5 def load_tokenizer():
6 377.4 MiB 0.0 MiB 1 path = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot; 
7 676.6 MiB 299.2 MiB 1 tokenizer = AutoTokenizer.from_pretrained(path)
8 
9 
10 676.6 MiB 0.0 MiB 1 返回 tokenizer
]]></description>
      <guid>https://stackoverflow.com/questions/78546693/why-loading-autotokenizer-takes-so-much-ram</guid>
      <pubDate>Tue, 28 May 2024 22:44:10 GMT</pubDate>
    </item>
    <item>
      <title>如何在 MLflow 中管理数据集？</title>
      <link>https://stackoverflow.com/questions/77822962/how-to-manage-datasets-in-mlflow</link>
      <description><![CDATA[请考虑以下取自 MLflow 文档页面的代码片段：
import mlflow.data
import pandas as pd
from mlflow.data.pandas_dataset import PandasDataset

# 使用来自 Web URL 的鸢尾花数据构建 Pandas DataFrame
dataset_source_url = &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&quot;
df = pd.read_csv(dataset_source_url)
# 从 Pandas DataFrame 构建 MLflow PandasDataset，并指定 Web URL
# 作为源
dataset: PandasDataset = mlflow.data.from_pandas(df, source=dataset_source_url)

with mlflow.start_run():
# 将数据集记录到 MLflow Run。指定“训练” context 以表明
# 数据集用于模型训练
mlflow.log_input(dataset, context=&quot;training&quot;)

# 检索运行，包括数据集信息
run = mlflow.get_run(mlflow.last_active_run().info.run_id)
dataset_info = run.inputs.dataset_inputs[0].dataset
print(f&quot;Dataset name: {dataset_info.name}&quot;)
print(f&quot;Dataset digest: {dataset_info.digest}&quot;)
print(f&quot;Dataset profile: {dataset_info.profile}&quot;)
print(f&quot;Dataset schema: {dataset_info.schema}&quot;)

# 加载数据集的源，将内容从源 URL 下载到本地
# 文件系统
dataset_source = mlflow.data.get_source(dataset_info)
dataset_source.load()

此代码正在启动新运行并记录一个数据集输入。这是否意味着在 MLflow 中我们将数据集保存为单独的运行？如果是这样，我们如何将具有自己的运行的模型的训练与数据集关联起来？我很困惑 MLflow 如何处理/跟踪数据集！老实说，我期望数据集是不同的实体类型（而不是运行），我们可以将它们链接到用于模型训练的每次运行。]]></description>
      <guid>https://stackoverflow.com/questions/77822962/how-to-manage-datasets-in-mlflow</guid>
      <pubDate>Tue, 16 Jan 2024 00:40:36 GMT</pubDate>
    </item>
    <item>
      <title>我如何知道使用 SelectKBest 选择了哪些功能？</title>
      <link>https://stackoverflow.com/questions/50942553/how-do-i-know-which-features-are-selected-with-selectkbest</link>
      <description><![CDATA[运行 SelectKBest 后会选择一些特征，结果以数组形式返回，因此我不知道它们是什么特征，因为我的训练集有数千个特征。
我想在测试集中找到并挑选出这些特征，然后删除其余特征。有什么方便的方法吗？谢谢！
代码如下：
from sklearn.feature_selection import SelectKBest, f_regression
X_opt=SelectKBest(f_regression,k=2000)
X_new=X_opt.fit_transform(df_train_X_mm, train_y)
X_new`

结果如下：
array([[0. , 0. , 0. , ..., 0. , 0. ,
0. ],
[0. , 0. , 0.00688335, ..., 0. , 0. ,
0. ],
[0. , 0. , 0. , ..., 0. , 0. ,
0. ],
...,
[0. , 0. ，0. ，...，0. ，0. ，
0. ]，
[0. ，0. ，0. ，...，0. ，0. ，
0. ]，
[0. ，0. ，0.06257587，...，0. ，0. ，
0. ]])
]]></description>
      <guid>https://stackoverflow.com/questions/50942553/how-do-i-know-which-features-are-selected-with-selectkbest</guid>
      <pubDate>Wed, 20 Jun 2018 07:25:42 GMT</pubDate>
    </item>
    </channel>
</rss>