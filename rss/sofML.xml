<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Tue, 08 Oct 2024 06:24:30 GMT</lastBuildDate>
    <item>
      <title>Azure 数据集问题</title>
      <link>https://stackoverflow.com/questions/79064369/azure-dataset-issues</link>
      <description><![CDATA[问题是 Azure 在数字后面添加了逗号，因此我无法将其更改为整数。
我的 CSV：

G 列（平均价格）是我遇到问题的那一列。
Azure 数据预览和设置：

然后 Azure 添加了逗号，从而导致以下问题：

如果有人能帮助解决这个问题，我们将不胜感激。
从中删除了逗号csv
尝试更改分隔符，但数据无法正确显示]]></description>
      <guid>https://stackoverflow.com/questions/79064369/azure-dataset-issues</guid>
      <pubDate>Tue, 08 Oct 2024 05:04:33 GMT</pubDate>
    </item>
    <item>
      <title>用于增量学习的 Python 非线性回归器</title>
      <link>https://stackoverflow.com/questions/79063665/python-non-linear-regressor-for-incremental-learning</link>
      <description><![CDATA[我想知道 scikit-learn 中是否有一个非线性回归程序，允许增量学习，即通过 partial_fit 调用。我发现 SGDRegressor 和 PassiveAggressiveRegressor 都允许 partial_fit，但它们是线性的，而我的数据显然是非线性的，因此拟合效果并不理想。]]></description>
      <guid>https://stackoverflow.com/questions/79063665/python-non-linear-regressor-for-incremental-learning</guid>
      <pubDate>Mon, 07 Oct 2024 21:18:59 GMT</pubDate>
    </item>
    <item>
      <title>如何微调时间序列转换器超参数以超越 LSTM 性能？[关闭]</title>
      <link>https://stackoverflow.com/questions/79062287/how-to-finetune-time-series-transformer-hyper-parameters-to-beat-the-lstm-perfor</link>
      <description><![CDATA[我正在尝试在时间序列数据上训练 ML 模型。输入是 10 个时间序列，本质上是传感器数据。输出是另一组三个时间序列。我给模型输入 100 个窗口。因此，输入形状变为 (100, 10)。我想预测单个时间步长的输出时间序列值。因此，输出形状变为 (1, 3)。（如果我创建大小为 x 的小批量，输入和输出形状将变为 (x, 100, 10) 和 (x, 1, 3)）。
我的方法是首先在较少的记录上对模型进行过度拟合。看看模型是否确实在学习/能够过度拟合数据。然后添加一些正则化（主要是 dropout），然后尝试在完整数据集上训练模型。
首先，我尝试在小数据集上过度拟合 LSTM 模型并可视化结果。它表现不错。所以，我尝试在整个数据集上训练它。它表现还不错，但在某些地方仍然很挣扎。我尝试的LSTM模型如下：
class LSTMModel(nn.Module):
def __init__(self, in_dim=10, hidden_​​size=1400, num_layers=1, output_size=3):
super(LSTMModelV3, self).__init__()

self.lstm_1 = nn.LSTM(in_dim, hidden_​​size, num_layers, batch_first=True) 
self.lstm_2 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True)
self.lstm_3 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True)
self.lstm_4 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True)
self.fc = nn.Linear(hidden_​​size, output_size)

def forward(self, x):
x, _ = self.lstm_1(x)
x, _ = self.lstm_2(x)
x, _ = self.lstm_3(x)
x, _ = self.lstm_4(x)
output = self.fc(x[:, -1, :])
return output

我也尝试添加 dropouts，但没有带来任何显著的改进。因此，我尝试训练 PatchTST transformer 模型。首先，我尝试过拟合较小的模型，效果很好。事实上，当我可视化输出时，我意识到它能够比 LSTM 模型获得更紧密的过拟合。因此，我尝试在整个数据集上对其进行训练。但性能与 LSTM 完全不相上下。
我尝试的 PatchTST 初始版本如下：
config = PatchTSTConfig(
num_input_channels=10,
context_length=100,
num_targets=3,
patch_length=10,
patch_stride=5,
prediction_length=1,
num_hidden_​​layers=5,
num_attention_heads=3,
d_model=300,
)

model = PatchTSTForRegression(config)

以此为基本配置，我尝试对其进行不同的更改以进行超参数优化：

d_model = 600
d_model = 800
d_model = 600, num_hidden_​​layer = 7
d_model = 600，patch_stride = 7
d_model = 300，patch_stride = 7，num_hidden_​​layers = 8

还有一些组合。选择这些超参数组合是为了让我能够在具有 24GB 内存的 GPU 中拟合模型。但是，没有任何配置会产生与 LSTM 相当的验证损失。这些是 LSTM 与 PatchTST 的曲线：

相应的学习率曲线如下：

如果 7 个 epoch 内性能没有提高，我过去常常会降低学习率。
我这里遗漏了什么？我是否错过了任何与时间序列转换器相关的见解？
我正在使用 AdamW 优化器。
PS1：是的，基础 LR 从 0.00005 开始，然后逐步下降到 0.000005、0.0000005、0.00000005。我知道这些非常小。但是，一开始我尝试用更大的基数训练 LSTM，如 0.001、LR 0.005、0.0005 等，但根本不起作用。只有在从 0.00005 开始后，一切才开始起作用。可能是因为我的传感器值本身非常小。
PS2：似乎 LSTM val 损失已经接近 0。但这只是因为我在图中运行的验证损失更高 PatchTST。如果我删除它们并添加 LSTM 过度拟合运行，那么它看起来像这样：
]]></description>
      <guid>https://stackoverflow.com/questions/79062287/how-to-finetune-time-series-transformer-hyper-parameters-to-beat-the-lstm-perfor</guid>
      <pubDate>Mon, 07 Oct 2024 14:08:26 GMT</pubDate>
    </item>
    <item>
      <title>3D加工零件特征识别（点云、网格）</title>
      <link>https://stackoverflow.com/questions/79062004/3d-machining-part-feature-recognition-point-cloud-mesh</link>
      <description><![CDATA[我有一个加工部件 (.STL)，想要识别（并提取）它的加工特征。有些特征很简单，但有些更复杂，这就是为什么我认为机器学习方法会很合适，因为我无法用数学方式描述该特征。
有一个 FeatureNet，它基本上可以完成这项工作，但它无法识别多个特征，并且代码无法按预期工作。
我还知道 AAGNet，它可以完成我想要的工作，但它使用 .STEP 作为输入，但我有一个网格（如果我转换它，则是点云）。
由于有更多的点云存储库，我认为我可以使用它们来解决我的问题。像 FPFH 这样的东西是正确的方向吗，还是我走错了路？
如果我使用机器学习方法，我可以轻松创建标记数据集。]]></description>
      <guid>https://stackoverflow.com/questions/79062004/3d-machining-part-feature-recognition-point-cloud-mesh</guid>
      <pubDate>Mon, 07 Oct 2024 12:41:25 GMT</pubDate>
    </item>
    <item>
      <title>验证多元线性回归模型的 AUC 计算</title>
      <link>https://stackoverflow.com/questions/79061528/validating-auc-calculation-for-a-multiple-linear-regression-model</link>
      <description><![CDATA[我正在尝试计算多元线性回归模型（两个变量）的 AUC 值，因此我正在处理以下代码并想与您确认。
我有两个变量，分别名为 Sa_T1 和 Sa_07，x 轴和 y 轴，z 轴上有 log_EDPs，我确实使用多元线性回归来确定以下关系：
log_EDPs = coef_log_Sa_T1 * log_Sa_T1 + coef_log_Sa_07 * log_Sa_07 + 截距
结果分别为以下值：0.3364、0.6530、-7.1452。现在我想计算 AUC，我根据我之前的单变量案例代码开发了以下代码：
# 系数
coef_log_Sa_T1 = 0.3364
coef_log_Sa_07 = 0.6530
intercept = -7.1452

# 使用回归模型预测的 log(edp)
predicted_log_edp = coef_log_Sa_T1 * log_Sa_T1 + coef_log_Sa_07 * log_Sa_07 + 截距

# 基于阈值的 log(edp)
threshold_SA = np.log(np.median(log_EDPs))
y_true = (log_EDPs &gt;= Threshold_SA).astype(int) 

y_pred_binary = (predicted_log_edp &gt;= Threshold_SA).astype(int)

y_pred_prob = (predicted_log_edp - predicted_log_edp.min()) / (predicted_log_edp.max() - predict_log_edp.min()) 

# 根据预测概率计算 AUC
auc_prob = roc_auc_score(y_true, y_pred_prob)
print(f&quot;AUC (probabilities): {auc_prob}&quot;)

结果 AUC 值为 0.9802。
代码是否进行了正确的计算？]]></description>
      <guid>https://stackoverflow.com/questions/79061528/validating-auc-calculation-for-a-multiple-linear-regression-model</guid>
      <pubDate>Mon, 07 Oct 2024 10:12:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 DDP（分布式数据并行）时，在多 GPU 上获得相同的损失，但梯度不同</title>
      <link>https://stackoverflow.com/questions/79061290/get-same-loss-but-different-grad-on-multi-gpus-when-using-ddp-distributeddatapa</link>
      <description><![CDATA[当将 torch.nn.parallel.DistributedDataParallel 添加到单 GPU 训练代码时，我遇到一个问题，即在不同的 GPU 上得到相同的损失但不同的梯度。与之前的单 GPU 训练代码相比，我确信损失是正确的，但在 loss.backward() 之后，我观察了各层的权重和偏差的梯度，发现在 all_gather 之前它们在不同的 GPU 上是不同的，在 all_gather 和计算损失之间的各层的梯度在不同的 GPU 上是相同的。

这是一个对比学习代码，所以我 all_gather 所有 GPU 上的张量来计算共同的最终损失。

以下是该模型的部分代码：
import torch.nn as nn
import torch
from config.base_config import Config
from modules.transformer import Transformer
from modules.stochastic_module import StochasticText
from modules.basic_utils import AllGather
allgather = AllGather.apply
from modules.tokenization_clip 导入 SimpleTokenizer

class CLIPStochastic(nn.Module):
def __init__(self, config: Config):
super(CLIPStochastic, self).__init__()
self.config = config

从 transformers 导入 CLIPModel
如果 config.clip_arch == &#39;ViT-B/32&#39;:
self.clip = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
elif config.clip_arch == &#39;ViT-B/16&#39;:
self.clip = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch16&quot;)
else:
引发 ValueError

self.task_config = config
config.pooling_type = &#39;transformer&#39;
self.pool_frames = Transformer(config)
self.stochastic = StochasticText(config)

def forward(self, data, return_all_frames=False, is_train=True):
batch_size = data[&#39;video&#39;].shape[0]
text_data = data[&#39;text&#39;] # text_data[&quot;input_ids&quot;].shape = torch.Size([16, 17])
video_data = data[&#39;video&#39;] # [16, 12, 3, 224, 224]
video_data = video_data.reshape(-1, 3, self.config.input_res, self.config.input_res) # [192, 3, 224, 224]

if is_train:

text_features = self.clip.get_text_features(**text_data)
video_features = self.clip.get_image_features(video_data)

video_features = video_features.reshape(batch_size, self.config.num_frames, -1) # [bs, #F, 512]

text_features = allgather(text_features,self.task_config)
video_features = allgather(video_features,self.task_config)
torch.distributed.barrier()

video_features_pooled = self.pool_frames(text_features, video_features)

# @WJM：执行随机文本
text_features_stochstic, text_mean, log_var = self.stochastic(text_features, video_features)

if return_all_frames:
return text_features, video_features, video_features_pooled, text_features_stochstic, text_mean, log_var

return text_features, video_features_pooled, text_features_stochstic, text_mean, log_var

else:

text_features = self.clip.get_text_features(**text_data)
video_features = self.clip.get_image_features(video_data)

video_features = video_features.reshape(batch_size, self.config.num_frames, -1)
video_features_pooled = self.pool_frames(text_features, video_features)

# @WJM：文本的重新参数化（独立于文本条件池化）
text_features_stochstic, _, _ = self.stochastic(text_features, video_features)

if return_all_frames:
return text_features, video_features, video_features_pooled, text_features_stochstic

return text_features, video_features_pooled, text_features_stochstic


allgather 函数如下：
class AllGather(torch.autograd.Function):
&quot;&quot;&quot;对张量执行 allgather 的 autograd 函数。&quot;&quot;&quot;

@staticmethod
def forward(ctx, tensor, args):
output = [torch.empty_like(tensor) for _ in range(args.world_size)]
torch.distributed.all_gather(output, tensor)
ctx.rank = local_rank
ctx.batch_size = tensor.shape[0]
return torch.cat(output, dim=0)

@staticmethod
def behind(ctx, grad_output):
local_grad = grad_output[ctx.batch_size * ctx.rank : ctx.batch_size * (ctx.rank + 1)]
return local_grad, None

我尝试在 AllGather 类中向后添加 all_reduce，代码如下，但似乎不起作用，可能是因为 DDP 自带了同步梯度函数？
def behind(ctx, *grads):
all_gradients = torch.stack(grads)
torch.distributed.all_reduce(all_gradients)
return all_gradients[torch.distributed.get_rank()]
]]></description>
      <guid>https://stackoverflow.com/questions/79061290/get-same-loss-but-different-grad-on-multi-gpus-when-using-ddp-distributeddatapa</guid>
      <pubDate>Mon, 07 Oct 2024 09:03:30 GMT</pubDate>
    </item>
    <item>
      <title>如何解决 TensorFlow 训练中的这个问题？未知图像文件格式。需要 JPEG、PNG、GIF、BMP 之一</title>
      <link>https://stackoverflow.com/questions/79060211/how-do-i-fix-this-problem-with-my-tensorflow-training-unknown-image-file-format</link>
      <description><![CDATA[我正在构建一个 U-Net 模型来检测乳腺癌，我从这里获取了数据集：https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset
尽管所有图像都是 png 格式，但在尝试训练我的模型时，会出现错误，指出我的图像格式不正确。
错误如下：
---------------------------------------------------------------------------
InvalidArgumentError Traceback（最近一次调用最后一次）
Cell In[51]，第 7 行
5 train_dataset = image_ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
6 print(image_ds.element_spec)
----&gt; 7 model_history = unet.fit(train_dataset, epochs=EPOCHS)

文件 ~\anaconda3\Lib\site-packages\keras\src\utils\traceback_utils.py:122，位于 filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
119filtered_tb = _process_traceback_frames(e.__traceback__)
120 # 要获取完整的堆栈跟踪，请调用：
121 # `keras.config.disable_traceback_filtering()`
--&gt; 122 从 None 引发 e.with_traceback(filtered_tb)
123 最后：
124 delfiltered_tb

文件 ~\anaconda3\Lib\site-packages\tensorflow\python\eager\execute.py:53，在 quick_execute(op_name, num_outputs, input, attrs, ctx, name) 中
51 尝试：
52 ctx.ensure_initialized()
---&gt; 53 张量 = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
54 输入、属性、输出)
55 除 core._NotOkStatusException 外，因为 e:
56 如果名称不为 None:

InvalidArgumentError：图形执行错误：

在节点处检测到，decode_image/DecodeImage 定义在（最近一次调用最后一次）：
&lt;堆栈跟踪不可用&gt;
传递给 MapDataset:3 转换的用户定义函数中的错误，迭代器：Iterator::Root::Prefetch::BatchV2::Shuffle::MemoryCacheImpl::Filter::ParallelMapV2：未知图像文件格式。需要 JPEG、PNG、GIF、BMP 之一。
[[{{node decrypt_image/DecodeImage}}]]
[[IteratorGetNext]] [Op:__inference_one_step_on_iterator_9520]

错误仅在尝试训练模型时发生，它引用此函数：
def preprocess_image(image, mask, target_size=(256, 256)):
try:
# 安全地解码图像和掩码
image = tf.io.decode_image(image, channels=3, expand_animations=False)
mask = tf.io.decode_image(mask, channels=1, expand_animations=False)

# 检查未定义或零维度
if image.shape is None or image.shape[0] == 0 or image.shape[1] == 0:
print(f&quot;Error: Image has undefined or zero Dimensions: {image.shape}&quot;)
return None, None

if mask.shape is None or mask.shape[0] == 0 or mask.shape[1] == 0:
print(f&quot;Error: Mask 具有未定义或零维度：{mask.shape}&quot;)
return None, None

# 确保图像恰好有 3 个通道 (RGB)
if image.shape[-1] != 3:
print(f&quot;Error: Image does not have 3 channels (found {image.shape[-1]}).&quot;)
return None, None

# 将图像标准化为范围 [0, 1]
image = tf.image.convert_image_dtype(image, tf.float32)

# 将图像和 mask 的大小调整为目标尺寸 (256*256)
image = tf.image.resize(image, target_size, method=&#39;nearest&#39;)
mask = tf.image.resize(mask, target_size, method=&#39;nearest&#39;)

#将 mask 转换为二进制（0 或 1）格式以用于分类任务
mask = tf.cast(tf.math.reduce_max(mask, axis=-1, keepdims=True) &gt; 0, tf.float32) # 确保二进制 mask

return image, mask

except Exception as e:
print(f&quot;Error during preprocessing: {str(e)}&quot;)
return None, None

# 将预处理函数应用于数据集
image_ds = dataset.map(preprocess_image)

# 过滤掉 preprocess_image 返回的 None 值
image_ds = image_ds.filter(lambda img, mask: img is not None and mask is not None)

我尝试了多种方法尝试使用 chatgpt 修复此问题，但似乎没有任何效果。]]></description>
      <guid>https://stackoverflow.com/questions/79060211/how-do-i-fix-this-problem-with-my-tensorflow-training-unknown-image-file-format</guid>
      <pubDate>Sun, 06 Oct 2024 22:45:18 GMT</pubDate>
    </item>
    <item>
      <title>LogisticRegression 未返回正确结果</title>
      <link>https://stackoverflow.com/questions/79057824/logisticregression-not-returning-correct-results</link>
      <description><![CDATA[我试图使用基于训练数据的逻辑回归对测试数据中的一系列点进行分类预测。
我得到了一个没有错误的输出，但我被告知结果是错误的（错误的输出将被视为错误，但运行代码时没有错误）。
训练数据是 375 个点中 4 个不同类别的集合，每个点有 3 个变量，因此绘制在 3D 图形上。我在该集合上运行了一个计数程序，发现超过 50% 的点属于第 2 类。我的初步结果是所有测试点都属于第 2 类。我尝试将训练数据排序为几个不同的集合：随机选择 125 个条目（这是测试数据的大小），找到所有类别的最小数量，然后使用每个类别中相同数量的点创建一个训练集。
无排序 = 所有第 2 类
随机排序 = 所有第 2 类
等类数排序 = 给我一个答案，其中点被归类在所有 4 个类别中，但当我将它们插入在线最终测试表格时，我的准确率得分为 26%，这与随机机会相同。所以，我没有正确处理数据，我不确定在哪里。我希望有更多回归分类经验的人能为我指明正确的方向。
编辑：在调用 LogisticRegression 之前，我是否需要重新格式化（转换）train_X、train_y 和 test_X 数组？如果需要，该怎么做？也许我只是给它提供了格式错误的数据。
# 形成表格以推动逻辑回归
train_X = []
train_y = []
for i in range(len(train_table)):
train_X.append(
[train_table.x.iloc[i], train_table.y.iloc[i], train_table.z.iloc[i]]
)
train_y.append(train_table.label.iloc[i])

test_X = []
for k in range(len(test_table)):
test_X.append([test_table.x.iloc[k], test_table.y.iloc[k], test_table.z.iloc[k]])

# 尝试使用和不使用规范化
clf = LogisticRegression().fit(normalize(train_X), train_y)
# clf = LogisticRegression().fit(train_X, train_y)

predict = clf.predict(test_X[:])
prob = clf.predict_proba(test_X[:])

results = pd.DataFrame(
sort_results(test_table, predict, prob),
columns=[&quot;&quot;, &quot;timestamp&quot;, &quot;UTC time&quot;, &quot;label&quot;, &quot;accuracy&quot;],
)

编辑：已解决
因此，我编写了一个比较 LogisticRegression 和 RandomForestClassifier 的过程，并将我的数据推送给它。使用 RandomForestClassifier 的准确性要好得多。
事实证明，LogisticRegression 返回所有 2 并不是错误，只是结果准确度较低。我使用 RandomForestClassifier 重写了程序，并添加了 RandomizedSearchCV，它可以创建多棵树，选择准确率最高的树，并使用它来预测分类。
新的返回值仍然主要是 2，但准确率大大提高。
param_dist = {&#39;n_estimators&#39;: randint(100, 375),
&#39;max_depth&#39;: randint(5, 20)}

rf = RandomForestClassifier()

rand_search = RandomizedSearchCV(rf, param_distributions = param_dist, n_iter=10, cv=5)

rand_search.fit(X, y)

best_rf = rand_search.best_estimator_

print(&#39;Best hyperparameters:&#39;, rand_search.best_params_)

predictions = pd.Series(best_rf.predict(to_pred_covariates))
]]></description>
      <guid>https://stackoverflow.com/questions/79057824/logisticregression-not-returning-correct-results</guid>
      <pubDate>Sat, 05 Oct 2024 19:41:26 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中多元时间序列预测的 LSTM 模型中的验证损失和早期停止</title>
      <link>https://stackoverflow.com/questions/76345515/validation-loss-and-early-stopping-in-an-lstm-model-for-multivariate-time-series</link>
      <description><![CDATA[我正在尝试训练一个 LSTM 模型来预测油价，并遵循一些教程。
我的数据集：



日期
美元指数
油价




2019 年 10 月 12 日
50
66


2019 年 10 月 13 日
51
60



其中油价是目标列。
序列大小 = 7，输出 = 1。
我无法添加验证数据并打印除训练和测试损失之外的验证损失。
这是我的代码和尝试：
 #split 为训练、验证和测试（数据集大小为 2380。因此 150 用于测试，100 用于验证，其余用于训练）
X_train = X_seq[:-150]
y_train = y_seq[:-150]
X_test = X_seq[-150:]
y_test = y_seq[-150:] 
X_val = X_train[-100:]
y_val = y_train [-100:]
X_train= X_train [:-100]
y_train = y_train[:-100]

LSTM 模型
class LSTM(nn.Module):
def __init__(self, num_classes, input_size, hidden_​​size, num_layers):
super().__init__()
self.num_classes = num_classes # 输出大小
self.num_layers = num_layers # lstm 中的循环层数量
self.input_size = input_size # 输入大小
self.hidden_​​size = hidden_​​size # 每个 lstm 层中的神经元
# LSTM 模型
self.lstm = nn.LSTM(input_size=input_size, hidden_​​size=hidden_​​size, 
num_layers=num_layers, batch_first=True, dropout=0.2) # lstm
self.fc_1 = nn.Linear(hidden_​​size, 128) # 完全连接
self.fc_2 = nn.Linear(128, num_classes) # 完全连接最后一层
self.relu = nn.ReLU()

def forward(self,x):
# 隐藏状态
h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_​​size))
# 单元状态
c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_​​size))
# 通过 LSTM 传播输入
output, (hn, cn) = self.lstm(x, (h_0, c_0)) # (输入、隐藏和内部状态)
hn = hn.view(-1, self.hidden_​​size) # 重塑密集层的数据
out = self.relu(hn)
out = self.fc_1(out) # 第一个密集
out = self.relu(out) # relu
out = self.fc_2(out) # 最终输出
return out

这是循环：
def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test,
X_val , y_val,):
for epoch in range(n_epochs):
lstm.train()
output = lstm.forward(X_train) # 前向传递
optimiser.zero_grad() # 计算梯度，手动设置为 0
# 获取损失函数
loss = loss_fn(outputs, y_train)
#val_loss = loss_fn(y_val, y_test).item()
#####
###
loss.backward() # 计算损失函数的损失
optimiser.step() # 从损失改进，即反向传播测试损失
lstm.eval()
test_preds = lstm(X_test) 
test_loss = loss_fn(test_preds, y_test)
if epoch % 100 == 0:
print(&quot;Epoch: %d, 训练损失: %1.5f, 测试损失: %1.5f&quot; % (epoch, 
loss.item(), 
test_loss.item()))

这是我对模型的调用方式：
n_epochs = 1000 
learning_rate = 0.001 

input_size = 3 # 特征数量
hidden_​​size = 2 # 隐藏状态的特征数量
num_layers = 1 # 堆叠的 lstm 层数
num_classes = 1 # 输出类数

lstm = LSTM(num_classes, input_size, hidden_​​size, num_layers)

loss_fn = torch.nn.MSELoss() # 回归的均方误差
optimiser = torch.optim.Adam(lstm.parameters(), lr=learning_rate)

training_loop(n_epochs=n_epochs,lstm=lstm, optimiser=optimiser, loss_fn=loss_fn, X_train=X_train_tensors,y_train=y_train_tensors, X_test=X_test_tensors, y_test=y_test_tensors, X_val=X_val_tensors,y_val=y_val_tensors) 


如何通过训练时要考虑的验证集并计算验证损失并在此基础上进行提前停止？]]></description>
      <guid>https://stackoverflow.com/questions/76345515/validation-loss-and-early-stopping-in-an-lstm-model-for-multivariate-time-series</guid>
      <pubDate>Sat, 27 May 2023 05:51:12 GMT</pubDate>
    </item>
    <item>
      <title>如何格式化时间序列数据以用于 PyTorch LSTM 分类？[关闭]</title>
      <link>https://stackoverflow.com/questions/76321333/how-can-i-format-my-time-series-data-for-pytorch-lstm-classification</link>
      <description><![CDATA[如何预处理时间序列数据以解决分类问题并将其输入 PyTorch LSTM 模型？我有如下图所示的数据集。

此处，event_type 是目标列，这是一个二元分类问题。我想使用 LSTM 训练此数据集。]]></description>
      <guid>https://stackoverflow.com/questions/76321333/how-can-i-format-my-time-series-data-for-pytorch-lstm-classification</guid>
      <pubDate>Wed, 24 May 2023 08:05:02 GMT</pubDate>
    </item>
    <item>
      <title>如何将 Stanza 导出为 ONNX 格式？</title>
      <link>https://stackoverflow.com/questions/70205743/how-to-export-stanza-to-onnx-format</link>
      <description><![CDATA[如何将 Stanza 导出为 ONNX 格式？
似乎不可能只是简单地训练模型。]]></description>
      <guid>https://stackoverflow.com/questions/70205743/how-to-export-stanza-to-onnx-format</guid>
      <pubDate>Thu, 02 Dec 2021 19:59:27 GMT</pubDate>
    </item>
    <item>
      <title>如何建立随机森林和粒子群优化器的混合模型来寻找产品的最优折扣？</title>
      <link>https://stackoverflow.com/questions/63413064/how-to-build-hybrid-model-of-random-forest-and-particle-swarm-optimizer-to-find</link>
      <description><![CDATA[我需要找到每种产品（例如 A、B、C）的最佳折扣，以便最大化总销售额。我为每种产品都建立了随机森林模型，将折扣和季节与销售额进行映射。我如何组合这些模型并将它们提供给优化器以找到每个产品的最佳折扣？
选择模型的原因：

RF：它能够提供更好的（相对于线性模型）预测因子和响应（sales_uplift_norm）之间的关系。
PSO：在许多白皮书中都有建议（可在researchgate/IEEE 获得），也可以在此处和此处的python 包中找到。

输入数据：样本数据用于在产品级别构建模型。数据一览如下：

我的想法/步骤：

针对每个产品构建 RF 模型
 # 预处理数据
products_pre_processed_data = {key:pre_process_data(df, key) for key, df in df_basepack_dict.items()}
# rf 模型
products_rf_model = {key:rf_fit(df) for key, df in products_pre_processed_data .items()}




将模型传递给优化器

目标函数：最大化sales_uplift_norm（RF 模型的响应变量）
约束：

总支出（A + B + C 的支出&lt;= 20），支出 = 产品总销售量 * 折扣百分比 * 产品 mrp_of_products
产品下限（A、B、C）：[0.0, 0.0, 0.0] # 折扣百分比下限
产品上限（A、B、C）：[0.3, 0.4, 0.4] # 折扣百分比上限




sudo/sample code # 因为我无法找到将 product_models 传递到优化器。
从 pyswarm 导入 pso
def obj(x):
model1 = products_rf_model.get(&#39;A&#39;)
model2 = products_rf_model.get(&#39;B&#39;)
model3 = products_rf_model.get(&#39;C&#39;)
return -(model1 + model2 + model3) # -ve 符号表示最大化

def con(x):
x1 = x[0]
x2 = x[1]
x3 = x[2]
return np.sum(units_A*x*mrp_A + unit_B*x*mrp_B + unit_C* x *spend_C)-20 # 支出预算

lb = [0.0, 0.0, 0.0]
ub = [0.3, 0.4, 0.4]

xopt, fopt = pso(obj, lb, ub, f_ieqcons=con)

如何将 PSO 优化器（如果我没有遵循正确的优化器，可以使用任何其他优化器）与 RF 一起使用？
添加用于模型的函数：
def pre_process_data(df,product):
data = df.copy().reset_index()
# print(data)
bp = product
print(&quot;-------product: {}-------&quot;.format(bp))
# 预处理步骤
print(&quot;pre process df.shape {}&quot;.format(df.shape))
#1. 响应变量转换
response = data.sales_uplift_norm # 已转换

#2.预测器数值变量转换 
numeric_vars = [&#39;discount_percentage&#39;] # 可能包括 mrp、深度
df_numeric = data[numeric_vars]
df_norm = df_numeric.apply(lambda x: scale(x), axis = 0) # 中心和尺度

#3. char 字段 dummification
#选择类别字段
cat_cols = data.select_dtypes(&#39;category&#39;).columns
#选择字符串字段
str_to_cat_cols = data.drop([&#39;product&#39;], axis = 1).select_dtypes(&#39;object&#39;).astype(&#39;category&#39;).columns
# 合并所有分类字段
all_cat_cols = [*cat_cols,*str_to_cat_cols]
# print(all_cat_cols)

#将 cat 转换为 dummies
df_dummies = pd.get_dummies(data[all_cat_cols])

#4.将 num 和 char df 组合在一起
df_combined = pd.concat([df_dummies.reset_index(drop=True), df_norm.reset_index(drop=True)], axis=1)

df_combined[&#39;sales_uplift_norm&#39;] = response
df_processed = df_combined.copy()
print(&quot;post process df.shape {}&quot;.format(df_processed.shape))
# print(&quot;model fields: {}&quot;.format(df_processed.columns))
return(df_processed)

def rf_fit(df, random_state = 12):

train_features = df.drop(&#39;sales_uplift_norm&#39;, axis = 1)
train_labels = df[&#39;sales_uplift_norm&#39;]

#随机森林回归器
rf = RandomForestRegressor(n_estimators = 500,
random_state = random_state,
bootstrap = True,
oob_score=True)
# RF 模型
rf_fit = rf.fit(train_features, train_labels)

return(rf_fit)
]]></description>
      <guid>https://stackoverflow.com/questions/63413064/how-to-build-hybrid-model-of-random-forest-and-particle-swarm-optimizer-to-find</guid>
      <pubDate>Fri, 14 Aug 2020 12:47:25 GMT</pubDate>
    </item>
    <item>
      <title>是否可以将单一回归技术应用于具有不同模式的数据？</title>
      <link>https://stackoverflow.com/questions/52666845/is-it-possible-to-apply-a-single-regression-technique-to-data-that-has-different</link>
      <description><![CDATA[我想根据温度估算多种不同产品的销售量，有些产品之间存在关系。对于其中一种产品，销售额和温度之间的关系绘制出来后如下所示：

这只是一种产品，但这里有一个总体趋势，即从 10 度之后销售额会增加。对于其他产品，关系可能更线性，其他产品可能有多项式关系，而其他产品可能根本没有关系。另一个产品的例子是，其销量和温度之间没有相关性，可能是这个产品：

首先，我想从一个产品中预测一些东西，所以我使用了第一个图中的产品来尝试建模。我最终将数据拆分，这样我就得到了一个数据框，其中包含从 -5 度到 10 度的所有值，并执行了线性回归，类似地，我将 10 度拆分到 30 度以执行线性回归，如下所示：

这里的一个问题是，我正在做各种各样的事情来将我的数据仅适合一种产品。我有一个包含 1000 种产品的数据集，我希望能够根据温度估算某些产品的销量。我想以某种方式循环遍历我的所有数据集，找出哪些数据集在销售和温度之间有某种关系，然后自动应用该特定产品的最佳回归模型，在给定某个温度 X 的情况下估算该产品的销售量。
我看过很多不同的神经网络回归教程，但我根本不知道如何开始或搜索什么，或者我尝试做的事情是否可行？]]></description>
      <guid>https://stackoverflow.com/questions/52666845/is-it-possible-to-apply-a-single-regression-technique-to-data-that-has-different</guid>
      <pubDate>Fri, 05 Oct 2018 13:34:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 sklearn LogisticRegression 和 RandomForest 模型的 Predict() 总是预测少数类 (1)</title>
      <link>https://stackoverflow.com/questions/51968669/predict-with-sklearn-logisticregression-and-randomforest-models-always-predict</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/51968669/predict-with-sklearn-logisticregression-and-randomforest-models-always-predict</guid>
      <pubDate>Wed, 22 Aug 2018 14:03:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 sklearn 的 RandomForestRegressor 进行预测</title>
      <link>https://stackoverflow.com/questions/46694664/prediction-using-sklearns-randomforestregressor</link>
      <description><![CDATA[我的数据如下...
date,locale,category,site,alexa_rank,sessions,user_logins
20170110,US,1,google,1,500,5000
20170110,EU,1,google,2,400,2000
20170111,US,2,facebook,2,400,2000

... 等等。这只是我想出的一个玩具数据集，但与原始数据相似。
我正在尝试使用 sklearn 的 RandomForestRegressor 构建一个模型来预测特定网站将有多少用户登录和会话。
我做了一些常规工作，将类别编码为标签，并根据今年前八个月的数据训练了我的模型，现在我想预测第九个月的登录和会话。我创建了一个针对登录进行训练的模型，以及另一个针对会话进行训练的模型。
我的测试数据集具有相同的形式：
date,locale,category,site,alexa_rank,sessions,user_logins
20170910,US,1,google,1,500,5000
20170910,EU,1,google,2,400,2000
20170911,US,2,facebook,2,400,2000

理想情况下，我希望传入测试数据集时不包含我需要预测的列，但 RandomForestRegressor 抱怨训练集和测试集之间的维度不同。
当我以当前形式传入测试数据集时，模型会预测 sessions 中的 精确 值，并且在大多数情况下，user_logins 列为零，否则值会有微小变化。
我将测试数据中的 sessions 和 user_logins 列归零，并将其传递给模型，但模型预测几乎全部为零。

我的工作流程正确吗？我是否正确使用了 RandomForestRegressor？
当我的测试数据集确实包含实际值时，我如何如此接近实际值？测试数据中的实际值是否用于预测？
如果模型正常工作，如果我将要预测的列（sessions 和 user_logins）归零，我不应该得到相同的预测值吗？
]]></description>
      <guid>https://stackoverflow.com/questions/46694664/prediction-using-sklearns-randomforestregressor</guid>
      <pubDate>Wed, 11 Oct 2017 17:55:37 GMT</pubDate>
    </item>
    </channel>
</rss>