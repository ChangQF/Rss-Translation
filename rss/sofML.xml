<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 25 Jan 2025 01:10:48 GMT</lastBuildDate>
    <item>
      <title>二元分类不平衡——修复我对此解决方案的实现</title>
      <link>https://stackoverflow.com/questions/79385651/imbalanced-binary-classification-fix-my-implementation-of-this-solution</link>
      <description><![CDATA[我查看了以下帖子：
处理高度不平衡数据的正确方法 - 二元分类
发帖者 @skillsmuggler 说：
上述问题在处理医疗数据集和其他类型的故障检测时非常常见，其中一个类别（不良影响）总是代表性不足。
解决这个问题的最佳方法是生成折叠并应用交叉验证。折叠应以平衡每个折叠中的类别的方式生成。在您的案例中，这将创建 20 个折叠，每个折叠都具有相同的代表性不足的类别和不同比例的代表性过高的类别。
生成平衡折叠
生成平衡折叠并使用交叉验证也会产生更好的通用和稳健模型。在您的案例中，20 个折叠可能看起来太苛刻，因此您可以创建 10 个折叠，每个折叠的类别比率为 2:1。
/结束。
这对我来说没有意义。
所以他的意思是：创建 10-20 个折叠，每个折叠都有 100% 的少数类和多数类的不同随机子集。好的，完成了。我现在该做什么？如果我以传统的 CV 方式对模型进行评分（使用所有其他折叠进行训练，对一个折叠进行评分），我们将得到疯狂的目标泄漏，因为模型看到的确实是与训练时相同的观察结果。
我在这里遗漏了什么？他是否建议在这些 1:1 分割上训练 20 个弱分类器，然后使用集成评分对测试集进行评估？
作为参考，我的正类频率为 1%。我还没有成功使用任何嵌入式方法来处理不平衡（加权）。我用于评估的指标是：召回率、马修相关系数和平均精度得分。]]></description>
      <guid>https://stackoverflow.com/questions/79385651/imbalanced-binary-classification-fix-my-implementation-of-this-solution</guid>
      <pubDate>Fri, 24 Jan 2025 21:15:58 GMT</pubDate>
    </item>
    <item>
      <title>为什么 PyTorch RetinaNet ResNet50 FPN V2 在配备 T4 GPU 的 Google Colab 上训练速度如此之慢？</title>
      <link>https://stackoverflow.com/questions/79385141/why-is-pytorch-retinanet-resnet50-fpn-v2-training-so-slow-on-google-colab-with-a</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79385141/why-is-pytorch-retinanet-resnet50-fpn-v2-training-so-slow-on-google-colab-with-a</guid>
      <pubDate>Fri, 24 Jan 2025 17:19:08 GMT</pubDate>
    </item>
    <item>
      <title>Bert 模型未使用 JAX 进行学习。结果没有改变</title>
      <link>https://stackoverflow.com/questions/79383687/bert-model-not-learning-using-jax-results-dont-change</link>
      <description><![CDATA[我正在使用 TPU 上的 JAX 训练垃圾邮件分类的 BERT 模型。我的模型没有学习，其结果也没有改变。
Epoch 0：训练损失 = 2.7961559295654297：训练准确度：0.30608975887298584 评估损失 = 3.6600053310394287：评估准确度 = 0.0
Epoch 1：训练损失 = 2.7961559295654297：训练准确度：0.30608975887298584 评估损失 = 3.6600053310394287：评估准确度 = 0.0
Epoch 2：训练损失 = 2.7961559295654297：训练准确度： 0.30608975887298584 Eval Loss = 3.6600053310394287: Eval Accuracy = 0.0

训练代码：
@jax.pmap
def train_step(state, batch, labels):
def loss_fn(params):

# 将批次中的所有内容放入模型并传递模型参数
logits = model(**batch, params = state.params).logits
loss = compute_loss(logits, labels) # 计算损失

return loss, logits

# 将损失函数转换为梯度微分函数
grad_fn = jax.value_and_grad(loss_fn, has_aux = True) # has_aux 允许返回 logits
# 从批次中获取损失和梯度grad_fn
(loss, logits), grads = grad_fn(state.params)
# 使用产生的梯度更新模型状态
new_state = state.apply_gradients(grads = grads)

return loss, logits, new_state

for epoch in range(epochs):
epoch_losses, epoch_accuracies = [], []
for batch in train_dataset:

batch[&quot;input_ids&quot;] = jnp.array(batch[&quot;input_ids&quot;])
batch[&quot;attention_mask&quot;] = jnp.array(batch[&quot;attention_mask&quot;])
batch[&quot;token_type_ids&quot;] = jnp.array(batch[&quot;token_type_ids&quot;])

# 我们将在多个数据集上复制该值设备 (tpus)
batch_inputs = {k: jax.device_put_replicated(v, jax.devices()) for k, v in batch.items() if k != &quot;Category&quot;}
batch_labels = jax.device_put_replicated(batch[&quot;Category&quot;], jax.devices()) # 在设备之间复制标签

# 从数据中删除 none
batch_labels = safe_convert_to_jax_array(jnp.array(batch_labels))
batch_labels = batch_labels.transpose(1, 0)

loss, logits, state = train_step(state, batch_inputs, batch_labels)

cls_logits = logits[:, :, 0, :] 
分类_logits = cls_logits[:, :, :2]

prediction_labels = jnp.argmax(classification_logits, axis = -1)
accuracy = compute_accuracy(predicted_labels, batch_labels)

初始化状态的代码：
class TrainState(train_state.TrainState):
pass

# 我们的模型参数
params = model.params
# 为我们的训练创建初始状态
state = TrainState.create(apply_fn = model.__call__, params = params, tx = optimizer)

def safe_convert_to_jax_array(input_data, default_value = 0):
# 用 default_value 替换 None 值
return jnp.array([default_value if x is None else x for x in input_data])

# 在 tpu 上复制状态
state = jax.device_put_replicated(state, jax.devices())

要查看完整代码：https://www.kaggle.com/code/yousefr/bert-spam-classification-using-jax-and-tpus
此外，我尝试调整学习率，但没有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/79383687/bert-model-not-learning-using-jax-results-dont-change</guid>
      <pubDate>Fri, 24 Jan 2025 08:33:23 GMT</pubDate>
    </item>
    <item>
      <title>社区连接器管道的 Apps 脚本中未检测到 BigQuery ML 模型训练完成</title>
      <link>https://stackoverflow.com/questions/79382851/bigquery-ml-model-training-completion-not-detected-in-apps-script-for-community</link>
      <description><![CDATA[我正在 lookerstudio 社区连接器中构建 Google Apps Script 管道，该管道创建 BigQuery ML 模型，然后将其用于异常检测。尽管实现了模型存在性检查和等待函数，但我仍然收到此错误：

错误：异常检测失败：{ 
&quot;error&quot;：{ 
&quot;code&quot;：400，
&quot;message&quot;：&quot;无效的表值函数 ML.DETECT_ANOMALIES\n该模型尚不可用...&quot;，
&quot;status&quot;：&quot;INVALID_ARGUMENT&quot; 
}
}


这是我的工作流程：

创建每日汇总表
创建 ARIMA 模型
等待模型训练
创建异常结果表

相关代码片段：
function modelExists(projectId, datasetId, modelId, accessToken) {
var url = &#39;https://bigquery.googleapis.com/bigquery/v2/projects/&#39; + 
projectId + &#39;/datasets/&#39; + datasetId + &#39;/models/&#39; + modelId;
var options = {
method: &#39;get&#39;,
headers: { Authorization: &#39;Bearer &#39; + accessToken },
muteHttpExceptions: true
};

尝试 {
var response = UrlFetchApp.fetch(url, options);
if (response.getResponseCode() === 200) {
var modelInfo = JSON.parse(response.getContentText());
// 检查 ACTIVE 状态或 creationTime 是否存在
return (modelInfo.state === &#39;ACTIVE&#39; || !!modelInfo.creationTime);
}
return false;
} catch (e) {
return false;
}
}

function waitForModel(projectId, datasetId, modelId, accessToken, timeout, interval) {
timeout = timeout || 480000; // 8 分钟
interval = interval || 5000; // 5 秒

var startTime = Date.now();
while (Date.now() - startTime &lt; timeout) {
var exist = modelExists(projectId, datasetId, modelId, accessToken);
if (存在) {
Logger.log(&#39;模型 &#39; + modelId + &#39; 已准备就绪&#39;);
return true;
}
Logger.log(&#39;正在等待模型 &#39; + modelId + &#39;... 当前状态： &#39; + getModelState(projectId, datasetId, modelId, accessToken));
Utilities.sleep(interval);
}
throw new Error(&#39;等待模型超时 &#39; + modelId);
}

function getModelState(projectId, datasetId, modelId, accessToken) {
var url = &#39;https://bigquery.googleapis.com/bigquery/v2/projects/&#39; + 
projectId + &#39;/datasets/&#39; + datasetId + &#39;/models/&#39; + modelId;
var options = {
method: &#39;get&#39;,
headers: { Authorization: &#39;Bearer &#39; + accessToken },
muteHttpExceptions: true
};

try {
var response = UrlFetchApp.fetch(url, options);
if (response.getResponseCode() === 200) {
var modelInfo = JSON.parse(response.getContentText());
return modelInfo.state || &#39;UNKNOWN&#39;;
}
return &#39;NOT_FOUND&#39;;
} catch (e) {
return &#39;ERROR&#39;;
}
}

管道执行：
const createModelQuery = `CREATE MODEL ...`;
fetchBigQuery(createModelQuery); 

// 2. 等待模型
waitForModel(projectId, datasetId, modelName, accessToken);

// 3. 创建结果表 &lt;- 此处失败
const detectQuery = `SELECT * FROM ML.DETECT_ANOMALIES(...)`;
fetchBigQuery(detectQuery);

问题：
即使 waitForModel 成功完成，后续的 ML.DETECT_ANOMALIES 调用仍会失败，并显示“模型不可用”。当我在 BigQuery 控制台中手动检查时，模型最终会在几分钟后可用。
问题：
为什么我的 waitForModel 函数无法正确检测模型训练完成情况，我如何确保管道等到模型真正准备就绪？]]></description>
      <guid>https://stackoverflow.com/questions/79382851/bigquery-ml-model-training-completion-not-detected-in-apps-script-for-community</guid>
      <pubDate>Thu, 23 Jan 2025 23:17:36 GMT</pubDate>
    </item>
    <item>
      <title>有人有使用 kluster.ai 进行 DeepSeek-R1 托管的经验吗？[关闭]</title>
      <link>https://stackoverflow.com/questions/79382626/does-anyone-have-experience-using-kluster-ai-for-deepseek-r1-hosting</link>
      <description><![CDATA[寻找一个使用 DeepSeek-R1 且价格不贵的地方。我正在为一家初创公司做一个项目，他们的预算有限。
我只想得到推理结果而不是推理过程，因为我不需要这些信息。我该怎么做？https://kluster.ai]]></description>
      <guid>https://stackoverflow.com/questions/79382626/does-anyone-have-experience-using-kluster-ai-for-deepseek-r1-hosting</guid>
      <pubDate>Thu, 23 Jan 2025 21:28:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 Yolov3 进行光学音乐识别</title>
      <link>https://stackoverflow.com/questions/79382146/optical-music-recognition-using-yolov3</link>
      <description><![CDATA[我正在尝试编写一个模型（Yolov3）来检测乐谱上的各种音乐符号。但所有适合此目的的数据集都只建立在印刷乐谱上。有没有办法以某种方式将模型适应手写字符？预训练 darknet-53 会对此有所帮助吗？如果我训练 darknet-53 识别手写和印刷字符，这会产生什么影响？
Yolov3 架构：Yolov3]]></description>
      <guid>https://stackoverflow.com/questions/79382146/optical-music-recognition-using-yolov3</guid>
      <pubDate>Thu, 23 Jan 2025 18:12:00 GMT</pubDate>
    </item>
    <item>
      <title>java.lang.AssertionError：Android Studio 中“不支持数据类型 INT32”</title>
      <link>https://stackoverflow.com/questions/79380176/java-lang-assertionerror-does-not-support-data-type-int32-in-android-studio</link>
      <description><![CDATA[我正在使用 TensorFlow Lite 模型在 Android Studio 中开发应用程序。运行应用程序时，我遇到以下错误：
java.lang.AssertionError：TensorFlow Lite 不支持数据类型 INT32

以下是我的代码的相关部分：
// 准备输入张量
val inputFeature0 = TensorBuffer.createFixedSize(inputShape, DataType.FLOAT32)
inputFeature0.loadArray(flatArray)

// 运行推理
val output = model?.process(inputFeature0)
val rawOutputBuffer = output?.outputFeature0AsTensorBuffer

// 根据数据类型将原始数据提取为 IntArray 或 FloatArray
val outputArray = when (rawOutputBuffer?.dataType) {
DataType.INT32 -&gt; rawOutputBuffer.intArray // 直接访问 INT32 数据
DataType.FLOAT32 -&gt; rawOutputBuffer.floatArray.map { it.toInt() }.toIntArray() // 将 FloatArray 转换为 IntArray
else -&gt;抛出 IllegalArgumentException(&quot;不支持的输出张量数据类型：${rawOutputBuffer?.dataType}&quot;)
}


输入张量的类型为 FLOAT32，并且使用 TensorBuffer.createFixedSize() 和 loadArray() 正确加载输入数据。

在处理模型的输出张量 (outputFeature0AsTensorBuffer) 时，我添加了检查以处理 FLOAT32 和 INT32 输出。

尽管如此，应用程序崩溃并显示错误，表明 TensorFlow Lite 不支持 INT32。


我有什么已尝试：

确保输入张量使用 FLOAT32。
验证 TensorFlow Lite 模型是否与 FLOAT32 数据类型兼容。
检查输出张量数据类型并添加对 FLOAT32 和 INT32 的处理。

我预计模型推理可以顺利运行，因为我已经处理了 FLOAT32 和 INT32 输出情况。
问题：

为什么即使输入张量使用 FLOAT32，TensorFlow Lite 也会抛出此错误？
该错误是否与 TensorFlow Lite 内部管理张量维度或元数据等数据类型的方式有关？
如何解决此错误并确保 TensorFlow Lite 成功运行推理？
]]></description>
      <guid>https://stackoverflow.com/questions/79380176/java-lang-assertionerror-does-not-support-data-type-int32-in-android-studio</guid>
      <pubDate>Thu, 23 Jan 2025 07:30:52 GMT</pubDate>
    </item>
    <item>
      <title>如何在固定的BBOX中将YOLOv8model与Deepsort连接起来？</title>
      <link>https://stackoverflow.com/questions/79378146/how-can-i-connect-yolov8model-with-deepsort-in-a-fixed-bbox</link>
      <description><![CDATA[我正在生成一个可以检测摩托车和汽车的模型来提取各自的信息。
但在将 YOLOv8 模型（这是我自定义的模型）与 Deepsort 算法连接的过程中，我发现了几个问题。

起初，自定义模型（YOLOv8）可以检测到每辆车，提取的视频显示完美的边界框
与 Deepsort 连接后，它漏掉了几辆车，提取的视频有错误的边界框（它们太大，不适合每辆车）
我在 YOLOv8 2 Deepsort 之间找不到错误的结果。

请帮帮我
import cv2
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort

# 初始化 YOLO 模型
model_path = &quot;/content/drive/MyDrive/Capstone/best_motorcycle_detector_NIGHT8.pt&quot;
model = YOLO(model_path)
model.to(&#39;cuda&#39;) # 使用 GPU

# 初始化 DeepSORT
tracker = DeepSort(max_age=200, n_init=1, nn_budget=200)

# 帮助程序将 YOLO 结果转换为 DeepSORT 格式
def yolo_to_deepsort(yolo_results, target_classes):
detections = []
for det in yolo_results[0].boxes:
x1, y1, x2, y2 = map(float, det.xyxy[0].cpu().numpy())
confidence = float(det.conf.cpu().numpy().item())
class_id = int(det.cls.cpu().numpy())
if class_id in target_classes:
detections.append([(x1, y1, x2, y2),置信度])
返回检测

# 主处理循环
video_path = &quot;/content/drive/MyDrive/Capstone/11.15 1200-1400/1320-1400.mp4&quot;
cap = cv2.VideoCapture(video_path)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))

output_path = &quot;/content/drive/MyDrive/Capstone/Results/processed_video.avi&quot;
video_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*&#39;MJPG&#39;), fps, (frame_width, frame_height))

target_classes = [2, 3] # 汽车 (2)、摩托车 (3)

while cap.isOpened():
ret, frame = cap.read()
if not ret:
break

# 运行 YOLO 模型
results = model(frame, conf=0.3)

# 将 YOLO 结果转换为 DeepSORT 格式
detections = yolo_to_deepsort(results, target_classes)

# 更新跟踪器
tracks = tracker.update_tracks(detections, frame=frame)

# 绘制边界框
for track in tracks:
if not track.is_confirmed():
continue
x1, y1, x2, y2 = map(int, track.to_tlbr())
track_id = track.track_id
标签 = f&quot;ID {track_id}&quot;
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# 保存帧
video_writer.write(frame)

cap.release()
video_writer.release()


找出 YOLO 中的协调性
Deepsort 的输入和输出
与其他算法连接，但 Deepsort 更适合我的视频

这是视频和模型权重（YOLOv8）:)
Deepsort 是一种跟踪检测到的对象的跟踪算法。
-File : extracted_weights =&gt; 定制的 YOLOv8 模型的权重
-File : 11.15 1320-1400_detected_video.avi =&gt; 通过定制的 YOLOv8 模型检测摩托车和汽车的视频
-File : 11.15 1320-1400_deepsort_processed_video.avi =&gt;跟踪（使用深度排序算法）检测到的物体（通过定制的 YOLOv8 模型）的视频
https://drive.google.com/file/d/1-03M2L42RtP6hauVP7fKSSqETEm8LtR6/view?usp=sharing]]></description>
      <guid>https://stackoverflow.com/questions/79378146/how-can-i-connect-yolov8model-with-deepsort-in-a-fixed-bbox</guid>
      <pubDate>Wed, 22 Jan 2025 14:30:52 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试使用 GNN-LSTM 预测尼日利亚某州未来确诊的脑膜炎病例数</title>
      <link>https://stackoverflow.com/questions/79377500/i-am-trying-to-predict-the-future-number-of-confirmed-cases-of-meningitis-for-a</link>
      <description><![CDATA[以下是代码：
 # 初始化模型
# 初始化模型
model = GNN_LSTM(input_dim=window_size, gcn_hidden_​​dim=16, lstm_hidden_​​dim=32, predict_steps=20)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.MSELoss()
print(f&quot;Shape of time_series_features: {time_series_features.shape}&quot;)

# 训练循环
for epoch in range(200):
model.train()
optimizer.zero_grad()

# 前向传递
out = model(data, time_series_features) # 无需解压，应符合模型输入预期
zamfara_predictions = out[target_idx] # 提取Zamfara 的预测

# 使用 train_mask 计算损失
adapted_train_mask = train_mask[-predict_steps:]

# 打印形状以供调试
print(f&quot;Shape of zamfara_predictions: {zamfara_predictions.shape}&quot;)
print(f&quot;Shape of zamfara_features[-predict_steps:]: {zamfara_features[-predict_steps:].shape}&quot;)
print(f&quot;Shape of adapted_train_mask: {adjusted_train_mask.shape}&quot;)

# 使用调整后的掩码计算损失
loss = criterion(
zamfara_predictions.squeeze()[adjusted_train_mask],
zamfara_features[-predict_steps:][adjusted_train_mask]
)

loss.backward()
optimizer.step()

# 验证
if (epoch + 1) % 20 == 0:
model.eval()
with torch.no_grad():
print(f&quot;zamfara_predictions (squeezed) 的形状：{zamfara_predictions.squeeze().shape}&quot;)
print(f&quot;zamfara_features 的形状：{zamfara_features.shape}&quot;)
print(f&quot;val_mask 的形状：{val_mask.shape}，类型：{val_mask.dtype}&quot;)
print(f&quot;test_mask 的形状：{test_mask.shape}，类型：{test_mask.dtype}&quot;)

# 验证损失计算
val_loss = criterion(
zamfara_predictions.squeeze()[val_mask],
zamfara_features[val_mask]
)
print(f&quot;Epoch {epoch+1}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}&quot;)

# 测试
model.eval()
with torch.no_grad():
test_loss = criterion(zamfara_predictions.squeeze()[test_mask], zamfara_features[test_mask])
print(f&quot;Test Loss: {test_loss.item()}&quot;)

# 指标计算
true_values = zamfara_features[test_mask].numpy()
predictions = zamfara_predictions.squeeze()[test_mask].numpy()

mse = mean_squared_error(true_values, predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(true_values, predictions)
print(f&quot;Test MSE: {mse:.4f}&quot;)
print(f&quot;Test RMSE: {rmse:.4f}&quot;)
print(f&quot;Test MAE: {mae:.4f}&quot;)

# Zamfara 的预测案例

我正在使用 Google Colab 编译该程序。编译后，代码中突出显示了此行：loss = criterion( zamfara_predict，并出现以下错误：
TypeError：&#39;int&#39; 对象不可调用。

代码由 meningitis_​​cases_graph.graphml 组成，它是尼日利亚 37 个州确诊脑膜炎病例的图表表示，edges.csv 是尼日利亚这些州的形状文件。我们想要预测尼日利亚某个州“赞法拉”未来 20 天的确诊病例数。如何解决此错误？]]></description>
      <guid>https://stackoverflow.com/questions/79377500/i-am-trying-to-predict-the-future-number-of-confirmed-cases-of-meningitis-for-a</guid>
      <pubDate>Wed, 22 Jan 2025 11:15:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在微调期间正确设置 pad token（而不是 eos）以避免模型无法预测 EOS？</title>
      <link>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi</guid>
      <pubDate>Fri, 07 Jul 2023 01:11:24 GMT</pubDate>
    </item>
    <item>
      <title>pyspark 实现的 ALS 是如何处理每个用户-项目组合的多个评级的？</title>
      <link>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</link>
      <description><![CDATA[我观察到 ALS 的输入数据不需要每个用户-项目组合都有唯一的评分。
这是一个可重现的示例。
# 示例数据框
df = spark.createDataFrame([(0, 0, 4.0),(0, 1, 2.0), 
(1, 1, 3.0), (1, 2, 4.0), 
(2, 1, 1.0), (2, 2, 5.0)],[&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;])

df.show(50,0)
+----+----+------+
|user|item|rating|
+----+----+------+
|0 |0 |4.0 |
|0 |1 |2.0 |
|1 |1 |3.0 |
|1 |2 |4.0 |
|2 |1 |1.0 |
|2 |2 |5.0 |
+----+----+------+

可以看到，每个用户-商品组合只有一个评分（理想情况）。
如果我们将这个数据框传递到 ALS，它将为您提供如下预测：
# 拟合 ALS
from pyspark.ml.recommendation import ALS
als = ALS(rank=5, 
maxIter=5, 
seed=0,
regParam = 0.1,
userCol=&#39;user&#39;,
itemCol=&#39;item&#39;,
ratingsCol=&#39;rating&#39;,
nonnegative=True)
model = als.fit(df)

# 来自 als 的预测
all_comb = df.select(&#39;user&#39;).distinct().join(broadcast(df.select(&#39;item&#39;).distinct()))
predictions = model.transform(all_comb)

predictions.show(20,0)
+----+----+----------+
|user|item|prediction|
+----+----+----------+
|0 |0 |3.9169915 |
|0 |1 |2.031506 |
|0 |2 |2.3546133 |
|1 |0 |4.9588947 |
|1 |1 |2.8347554 |
|1 |2 |4.003007 |
|2 |0 |0.9958025 |
|2 |1 |1.0896711 |
|2 |2 |4.895194 |
+----+----+----------+

到目前为止，一切对我来说都是有意义的。但是如果我们有一个包含多个用户-项目评分组合的数据框，如下所示 -
# 示例数据框
df = spark.createDataFrame([(0, 0, 4.0), (0, 0, 3.5),
(0, 0, 4.1),(0, 1, 2.0),
(0, 1, 1.9),(0, 1, 2.1),
(1, 1, 3.0), (1, 1, 2.8),
(1, 2, 4.0),(1, 2, 3.6),
(2, 1, 1.0), (2, 1, 0.9),
(2, 2, 5.0),(2, 2, 4.9)],
[&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;])
df.show(100,0)
+----+----+------+
|user|item|rating|
+----+----+------+
|0 |0 |4.0 |
|0 |0 |3.5 |
|0 |0 |4.1 |
|0 |1 |2.0 |
|0 |1 |1.9 |
|0 |1 |2.1 |
|1 |1 |3.0 |
|1 |1 |2.8 |
|1 |2 |4.0 |
|1 |2 |3.6 |
|2 |1 |1.0 |
|2 |1 |0.9 |
|2 |2 |5.0 |
|2 |2 |4.9 |
+----+----+------+

如您在上面的数据框中看到的那样，一个用户-项目组合有多条记录。例如 - 用户“0”多次对项目“0”进行评分，即分别为 4.0、3.5 和 4.1。
如果我将此输入数据框传递给 ALS 会怎样？这会起作用吗？
我最初认为它不应该起作用，因为 ALS 应该根据用户-项目组合获得唯一评级，但当我运行它时，它起作用了，让我感到惊讶！
# 拟合 ALS
als = ALS(rank=5, 
maxIter=5, 
seed=0,
regParam = 0.1,
userCol=&#39;user&#39;,
itemCol=&#39;item&#39;,
ratingsCol=&#39;rating&#39;,
nonnegative=True)
model = als.fit(df)

# 来自 als 的预测
all_comb = df.select(&#39;user&#39;).distinct().join(broadcast(df.select(&#39;item&#39;).distinct()))
predictions = model.transform(all_comb)

predictions.show(20,0)
+----+----+----------+
|user|item|prediction|
+----+----+----------+
|0 |0 |3.7877638 |
|0 |1 |2.020348 |
|0 |2 |2.4364853 |
|1 |0 |4.9624424 |
|1 |1 |2.7311888 |
|1 |2 |3.8018093 |
|2 |0 |1.2490809 |
|2 |1 |1.0351425 |
|2 |2 |4.8451777 |
+----+----+----------+

为什么它会起作用？我以为它会失败，但它没有，而且还给了我预测。
我尝试查看研究论文、ALS 的有限源代码和互联网上可用的信息，但找不到任何有用的东西。
是取这些不同评分的平均值然后将其传递给 ALS 还是其他什么？
有人遇到过类似的事情吗？或者知道 ALS 内部如何处理此类数据吗？]]></description>
      <guid>https://stackoverflow.com/questions/72012513/how-pyspark-implementation-of-als-is-handling-multiple-ratings-per-user-item-com</guid>
      <pubDate>Tue, 26 Apr 2022 10:37:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Python 中创建混淆矩阵的图像</title>
      <link>https://stackoverflow.com/questions/65317685/how-to-create-image-of-confusion-matrix-in-python</link>
      <description><![CDATA[我是 Python 和机器学习的新手。我正在研究多类分类（3 个类）。我想将混淆矩阵保存为图像。现在，sklearn.metrics.confusion_matrix() 帮助我找到混淆矩阵，如下所示：
array([[35, 0, 6],
[0, 0, 3],
[5, 50, 1]])

接下来，我想知道如何将这个混淆矩阵转换为图像并保存为 png。]]></description>
      <guid>https://stackoverflow.com/questions/65317685/how-to-create-image-of-confusion-matrix-in-python</guid>
      <pubDate>Wed, 16 Dec 2020 05:11:00 GMT</pubDate>
    </item>
    <item>
      <title>如何解决“无法将类强制转换为data.frame？</title>
      <link>https://stackoverflow.com/questions/58870663/how-to-solve-cannot-coerce-class-to-data-frame</link>
      <description><![CDATA[问题出现在第 20 行：x3 &lt;- lm(Salary ~ ...

as.data.frame.default(data) 中的错误：无法将类‘c(&quot;train&quot;, &quot;train.formula&quot;)’强制转换为 data.frame

如何解决？
attach(Hitters)
Hitters

library(caret)
set.seed(123)
# 定义训练控制
set.seed(123) 
train.control &lt;- trainControl(method = &quot;cv&quot;, number = 10)
# 训练模型
x2 &lt;- train(Salary ~., data = x, method = &quot;lm&quot;,
trControl = train.control)
# 总结结果
print(x)
x3 &lt;- lm(Salary ~ poly(AtBat,3) + poly(Hits,3) + poly(Walks,3) + poly(CRuns,3) + poly(CWalks,3) + poly(PutOuts,3), data = x2)
summary(x3)
MSE = mean(x3$residuals^2)
print(&quot;均方误差：&quot;)
print(MSE)
]]></description>
      <guid>https://stackoverflow.com/questions/58870663/how-to-solve-cannot-coerce-class-to-data-frame</guid>
      <pubDate>Fri, 15 Nov 2019 05:09:08 GMT</pubDate>
    </item>
    <item>
      <title>Keras 进度条中的准确度是什么意思？</title>
      <link>https://stackoverflow.com/questions/52559086/what-does-the-accuracy-mean-in-the-keras-progress-bar</link>
      <description><![CDATA[在 Keras 中，您将获得类似以下内容：
Epoch 1/1
60000/60000 [==============================] - 297s 5ms/step - 损失：0.7048 - acc：0.7669

60000/60000 [==============================] - 179s 3ms/step
训练集：
acc：94.60%

10000/10000 [================================] - 30s 3ms/step
测试集：
acc： 95.10%

但我是这样拟合的：

model.fit(X_train, oh_y_train,
batch_size=512,
epochs=1,
verbose=1)

.fit() 方法中没有验证数据，它从第 1 个时期测量准确率的是什么？
最终准确率差别很大。]]></description>
      <guid>https://stackoverflow.com/questions/52559086/what-does-the-accuracy-mean-in-the-keras-progress-bar</guid>
      <pubDate>Fri, 28 Sep 2018 16:07:45 GMT</pubDate>
    </item>
    <item>
      <title>ALS（交替最小二乘）算法对同一用户进行多次排名</title>
      <link>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</link>
      <description><![CDATA[嗨，经过大量研究，我们决定使用 Google Cloud 基础架构，并在我们的产品推荐系统中使用 ALS 算法（一种协同过滤方法 - https://cloud.google.com/solutions/recommendations-using-machine-learning-on-compute-engine#Training-the-models ），详细说明如下：
我们有两种类型的客户。第一类是附近销售产品的公司，第二类是打算从这些公司购买产品的消费者

每个消费者都可以搜索附近的公司或按行业搜索公司（例如杂货店、干洗店、肉店等）
当消费者找到一家公司时，他/她可以执行以下操作（他可以一次执行多项操作）
2.1. 仅查看公司简介
2.2. 将公司添加到收藏夹
2.3. 开始与公司聊天
2.4. 从公司下订单
2.5.给公司评分和评论

所以我不明白的是：上面描述的每件商品都被确定为我们数据库中的某些评分列，例如：
查看公司简介：10 分
从公司下订单：20 分
给公司打星或评论：20 分
因此，对于同一用户，每件商品都是单独的评分。
在我们的数据库中，对于用户-公司对，可能会有超过 1 行
例如：
第 1 行：user18-company18-10pts（查看过一次个人资料）
第 2 行：user18-company18-20pts（从公司下订单）
第 3 行：user18-company19-10pts
我不确定这个算法，它是计算该用户对同一家公司的所有评分的总和（我到底想要什么）还是只是寻找单个用户对单个公司的评分的单行？（我想要的是这个 ALS 算法来总结该用户-公司对的第 1 行和第 2 行）
有人知道吗？这对我们的推荐系统非常重要。因为我正在寻找的算法需要计算用户所有评分的总和，以便推荐另一家公司。因为我们的商业模式与电影评分系统不同
谢谢]]></description>
      <guid>https://stackoverflow.com/questions/50114510/als-alternating-least-square-algorithm-in-multiple-rankings-for-a-user</guid>
      <pubDate>Tue, 01 May 2018 09:53:59 GMT</pubDate>
    </item>
    </channel>
</rss>