<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - 堆栈内存溢出</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 条</description>
    <lastBuildDate>Sat, 28 Dec 2024 01:14:08 GMT</lastBuildDate>
    <item>
      <title>线性回归模型勉强优化了截距b</title>
      <link>https://stackoverflow.com/questions/79312660/linear-regression-model-barely-optimizes-the-intercept-b</link>
      <description><![CDATA[我从头开始编写了一个线性回归模型。我使用“残差平方和”作为梯度下降的损失函数。为了进行测试，我使用线性数据 (y=x)
运行算法时，截距 b 几乎没有变化。因此斜率 m 计算不正确。
%matplotlib qt5 
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

X = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
y = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12345)

class LinearRegression():
def __init__(self):
self.X = None
self.y = None

def ssr(self, m, b):
sum = 0
for i in range(len(self.X)):
sum += (self.y[i] - (m * self.X[i] + b) ) ** 2

return sum

def ssr_gradient(self, m, b):
sum_m = 0
sum_b = 0
n = len(self.X)
for i in range(n):
error = self.y[i] - (m * self.X[i] + b)
derivative_m = -(2/n) * self.X[i] * error # 相对于 m 的导数
derivative_b = -(2/n) * error # 相对于 m 的导数b
sum_m += derived_m
sum_b += derived_b

return sum_m, sum_b

def fit(self, X, y, m, b): # 梯度下降
self.X = X
self.y = y

M, B = np.meshgrid(np.arange(-10, 10, 0.1), np.arange(-10, 10, 0.1))
SSR = np.zeros_like(M)
for i in range(M.shape[0]):
for j in range(M.shape[1]):
SSR[i, j] = self.ssr(M[i, j], B[i, j])

fig, axis = plt.subplots(1, 2, figsize=(12, 6))
gd_model = fig.add_subplot(121,投影=“3d”，computed_zorder=False)
lin_reg_model = axis[1] 

current_pos = (m, b, self.ssr(m, b))
learning_rate = 0.001
min_step_size = 0.001
max_steps = 1000
current_steps = 0

while(current_steps &lt; max_steps):
M_derivative, B_derivative = self.ssr_gradient(current_pos[0], current_pos[1])
M_step_size, B_step_size = M_derivative * learning_rate, B_derivative * learning_rate

if abs(M_step_size) &lt; min_step_size 或 abs(B_step_size) &lt; min_step_size:
break

M_new, B_new = current_pos[0] - M_step_size, current_pos[1] - B_step_size

current_pos = (M_new, B_new, self.ssr(M_new, B_new))

print(f&quot;参数：m：{current_pos[0]}; b：{current_pos[1]}; SSR：{current_pos[2]}&quot;)

current_steps += 1

x = np.arange(0, 10, 1)
y = current_pos[0] * x + current_pos[1]
lin_reg_model.scatter(X_train, y_train, label=&quot;Train&quot;, s=75, c=&quot;#1f77b4&quot;)
lin_reg_model.plot(x, y)

gd_model.plot_surface(M, B, SSR, cmap=&quot;viridis&quot;, zorder=0)
gd_model.scatter(current_pos[0], current_pos[1], current_pos[2], c=&quot;red&quot;, zorder=1)
gd_model.set_xlabel(&quot;斜率 m&quot;)
gd_model.set_ylabel(&quot;截距 b&quot;)
gd_model.set_zlabel(&quot;残差平方和&quot;)

plt.tight_layout()
plt.pause(0.001)

gd_model.clear()
lin_reg_model.clear()

self.m = current_pos[0]
self.b = current_pos[1]

def predict(self, X_test):
return self.m * X_test + self.b

lin_reg_model = LinearRegression()
lin_reg_model.fit(X_train, y_train, 1, 10)


这是初始值 m=1 和 b=10 的结果：
参数：m：-0.45129949840919587；b：9.50972664859535；SSR：145.06534359577407

显然这不是最佳的，因为我的数据是线性的。因此最佳参数应该是 m=1 和 b=0
但我在代码中找不到问题。该算法根据初始值打印不同的结果，但只要 SSR 函数恰好有一个最小值，它就应该一遍又一遍地打印相同的结果。
我尝试使用不同的学习率，但问题仍然存在。]]></description>
      <guid>https://stackoverflow.com/questions/79312660/linear-regression-model-barely-optimizes-the-intercept-b</guid>
      <pubDate>Fri, 27 Dec 2024 19:40:21 GMT</pubDate>
    </item>
    <item>
      <title>RCNN RESNET 50 模型 - 我的食物回购 TRAIN [关闭]</title>
      <link>https://stackoverflow.com/questions/79312423/rcnn-resnet-50-model-my-food-repo-train</link>
      <description><![CDATA[我可以寻求帮助吗？
我找到了这篇科学文章：
https://www.frontiersin.org/journals/nutrition/articles/10.3389/fnut.2022.875143/full
其中有一个指向存储库的链接，其中包含在 COCO 数据集上训练的模型。
我想在 MyFoodRepo 数据集上训练其中一个模型（mask r-CNN ResNet50）。我想在 Google Colab pro 中使用 GPU（我本地没有这种可能性），但库兼容性存在很大问题（Colab 有 CUDA 12.2）。我尝试了很多方法，但至今还没有找到解决办法。也许有人有主意？🙏🏼
https://gitlab.aicrowd.com/.../myfoodrepo-experiments/
mmdetetection 存在很多问题：(]]></description>
      <guid>https://stackoverflow.com/questions/79312423/rcnn-resnet-50-model-my-food-repo-train</guid>
      <pubDate>Fri, 27 Dec 2024 17:20:38 GMT</pubDate>
    </item>
    <item>
      <title>Huggingface Trainer 在完成所有 Epoch 之前停止</title>
      <link>https://stackoverflow.com/questions/79312241/huggingface-trainer-stops-before-completing-all-epochs</link>
      <description><![CDATA[我正在使用 Huggingface Trainer 进行序列分类任务，配置如下：

num_train_epochs=5
save_strategy=&quot;epoch&quot;
evaluation_strategy=&quot;epoch&quot;
load_best_model_at_end=False

但是，训练在第 4 个 epoch 之后停止，尽管我预计它会完成所有 5 个 epoch。
以下是我的训练参数和 Trainer 设置的片段：
在此处输入图片说明
outputTable
我怀疑它可能与早期停止行为、梯度累积或其他参数交互有关。
我还确保没有使用早期停止回调。
如何解决这个问题？
我尝试过的方法：
我在 TrainingArguments 中设置了 num_train_epochs=5，并使用了 Huggingface Transformers 库中的 Trainer。我的目标是训练模型恰好 5 个时期。我确认没有应用任何早期停止回调或额外的终止逻辑。
我的预期：
训练过程应该运行所有 5 个时期，记录指标并按照配置在每个时期后保存检查点。
发生了什么：
训练在第 4 个时期后停止，没有任何错误消息。尽管 num_train_epochs 明确设置为 5，但日志仍过早地表明训练过程结束。]]></description>
      <guid>https://stackoverflow.com/questions/79312241/huggingface-trainer-stops-before-completing-all-epochs</guid>
      <pubDate>Fri, 27 Dec 2024 15:54:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用多类数据集训练模型来预测工作角色？[关闭]</title>
      <link>https://stackoverflow.com/questions/79312226/how-to-train-a-model-to-predict-job-roles-with-multi-class-dataset</link>
      <description><![CDATA[我正在开展一个项目，根据包含39 个特征和33 个独特工作角色作为目标标签的数据集来预测工作角色。数据集有 20,000 行，包括数值列和分类列。
以下是数据集的摘要：

数值特征 (14)：学术科目（例如操作系统、算法）的百分比、逻辑商评分、参加的黑客马拉松等。
二进制特征 (16)：诸如“可以在系统之前长时间工作吗？”，“自学能力？”等问题。
分类特征 (8)：包括“认证”、“记忆能力”分数”、“感兴趣的职业领域”等。
目标变量：建议的工作角色（例如，“数据库开发人员”、“软件工程师”等）。

问题：
我预处理了数据集并尝试了随机森林、SVM和XGBoost等训练模型，但准确率仍然一直很低（约 3%）。我怀疑我的预处理、模型选择或超参数调整可能存在问题。
预处理管道：
以下是我预处理数据的方法：
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler

def transform_data(df):
X = df.drop(&#39;Suggested Job Role&#39;, axis=1)
y = df[&#39;Suggested Job Role&#39;]

# 特征类型
two_category_features = [&#39;can work long time before system?&#39;, &#39;self-learning capacity?&#39;, 
&#39;Extra-courses did&#39;, &#39;talenttests taken?&#39;, &#39;olympiads&#39;, &#39;Job/Higher学习？&#39;,
&#39;从年长者或长辈那里获取信息&#39;, &#39;对游戏感兴趣&#39;, &#39;期望薪资范围&#39;, 
&#39;处于恋爱关系中？&#39;, &#39;行为温和还是强硬？&#39;, &#39;管理或技术&#39;, 
&#39;薪水/工作&#39;, &#39;努力/聪明的员工&#39;, &#39;曾经在团队中工作过吗？&#39;, &#39;内向&#39;]

categorical_features = [&#39;认证&#39;, &#39;研讨会&#39;, &#39;阅读和写作技能&#39;, 
&#39;记忆能力得分&#39;, &#39;感兴趣的科目&#39;, 
&#39;感兴趣的职业领域&#39;, &#39;想要在哪家公司安顿下来？&#39;, 
&#39;感兴趣的书籍类型&#39;]

numeric_features = [&#39;操作系统中的学术百分比&#39;, &#39;算法中的百分比&#39;, 
&#39;编程概念中的百分比&#39;, &#39;软件工程中的百分比&#39;,
&#39;计算机网络占比&#39;, &#39;电子学科占比&#39;, 
&#39;计算机架构占比&#39;, &#39;数学占比&#39;, 
&#39;沟通技巧占比&#39;, &#39;逻辑商评分&#39;, 
&#39;黑客马拉松&#39;, &#39;编码技能评分&#39;, &#39;公开演讲要点&#39;, &#39;每天工作时间&#39;]

# 预处理管道
two_category_transformer = Pipeline(steps=[
(&#39;ordinal&#39;, OrdinalEncoder())
])
categorical_transformer = Pipeline(steps=[
(&#39;onehot&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))
])
numeric_transformer = Pipeline(steps=[
(&#39;minmax&#39;, MinMaxScaler())
])

# 组合转换
preprocessor = ColumnTransformer(transformers=[
(&#39;two_cat&#39;, two_category_transformer, two_category_features),
(&#39;cat&#39;, categorical_transformer, categorical_features),
(&#39;minmax&#39;, numeric_transformer, numeric_features)
])

formed_X = preprocessor.fit_transform(X)
return formed_X, y

尝试的模型：

随机森林：使用默认参数。
SVM：尝试使用 RBF 内核，默认超参数。
XGBoost：默认参数。

尽管尝试了这些模型，准确率仍然停留在 3% 左右。

问题：

为什么模型在这个数据集上表现不佳？
我应该尝试哪些特定的技术或方法（例如，超参数调整、特征选择、过采样）？
我如何更好地处理目标变量的高基数（33 个独特的工作角色）？
]]></description>
      <guid>https://stackoverflow.com/questions/79312226/how-to-train-a-model-to-predict-job-roles-with-multi-class-dataset</guid>
      <pubDate>Fri, 27 Dec 2024 15:48:34 GMT</pubDate>
    </item>
    <item>
      <title>pytorch中如何在单节点单GPU系统中开发多GPU模块？</title>
      <link>https://stackoverflow.com/questions/79311997/how-to-develop-multi-gpu-modules-in-single-node-single-gpu-system-in-pytorch</link>
      <description><![CDATA[我正在开发一个多 GPU PyTorch 应用程序。torch.distributed 中的现有方法（如 scatter/gather）不能满足我的要求，因此我需要开发前向/反向传播步骤，在 GPU 之间发送和接收梯度，同时使用内置方法 scatter/gather。我可以自己做。我的最终应用程序将在多 GPU 服务器上执行。
对于开发，预算限制将我限制在单节点单 GPU 服务器上，因为我们的组织共享大型集群服务器。我遇到的一个问题是在这个单 GPU 系统中模拟多 GPU 设置。
如何在单 GPU 系统中模拟多 GPU 设置以测试这些模块？]]></description>
      <guid>https://stackoverflow.com/questions/79311997/how-to-develop-multi-gpu-modules-in-single-node-single-gpu-system-in-pytorch</guid>
      <pubDate>Fri, 27 Dec 2024 14:10:33 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证结果与混淆矩阵指标之间的差异</title>
      <link>https://stackoverflow.com/questions/79311670/discrepancy-between-cross-validation-results-and-confusion-matrix-metrics</link>
      <description><![CDATA[我有一个问题：我使用分层 10 倍交叉验证对分类模型进行 5 次重复，并报告结果。问题是，当它绘制混淆矩阵时，我手动计算矩阵中的准确度和其他指标，它们与报告的结果不同
我尝试汇总所有结果并使用完全相同的结果绘制混淆指标，但没有成功]]></description>
      <guid>https://stackoverflow.com/questions/79311670/discrepancy-between-cross-validation-results-and-confusion-matrix-metrics</guid>
      <pubDate>Fri, 27 Dec 2024 11:23:26 GMT</pubDate>
    </item>
    <item>
      <title>更改 YOLO 片段预测图像中的片段颜色</title>
      <link>https://stackoverflow.com/questions/79309903/changing-color-of-segments-from-yolo-segment-predicted-images</link>
      <description><![CDATA[我正在开展一个对象检测项目，在一组图像中检测三种类型的对象。此图像显示了 yolo 片段模型预测模型示例 图像。
我遇到的问题是检测到的符号是白色的。这种颜色对某些人来说可能可见，但对其他人来说可能不可见。有没有办法改变这种颜色？我对 Python 很陌生。我已经预测了整个集合的图像（总计超过 100,000 张图像）。
我使用使用 LabelStudio 标记感兴趣的符号的图像训练了 YOLOv8 模型。在标记过程中，我使用了红色、蓝色和绿色等颜色。但不知何故，在获得最终的 YOLOv8 模型 (best.pt) 并运行预测后，检测到的符号的颜色与最初使用的颜色不同。我有以下问题：

如何更改检测到的物体的白色？

有没有办法确保在 labelstudio 上标记时使用的颜色保留在预测图像中？


import cv2
import numpy as np

# 定义一个函数来替换完整的矩形白色边界框的颜色
def replace_white_rectangles_with_green(image_path, output_path):
# 读取图像
image = cv2.imread(image_path, cv2.IMREAD_COLOR)

# 将图像转换为灰度以进行轮廓检测
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# 对灰度图像进行阈值处理以创建白色区域的二元掩码
_, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)

# 查找轮廓以检测形状
contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

for contour in contours:
# 近似轮廓以检查其是否形成矩形
epsilon = 0.02 * cv2.arcLength(contour, True)
approx = cv2.approxPolyDP(contour, epsilon, True)

# 检查轮廓是否有四个边（矩形或正方形）
if len(approx) == 4:
# 验证形状是否凸（以确认它是矩形/正方形）
if cv2.isContourConvex(approx):
# 将边界框的白色替换为亮绿色
cv2.drawContours(image, [contour], -1, (0, 255, 0), thicken=cv2.FILLED)

# 保存修改后的图像
cv2.imwrite(output_path, image)

# 输入和输出图像的路径
input_image_path = &quot;path_to_your_image.jpg&quot; # 替换为输入图像路径
output_image_path = &quot;path_to_save_modified_image.jpg&quot; # 替换为所需的输出路径

# 将函数应用于图像
replace_white_rectangles_with_green(input_image_path, output_image_path)

print(&quot;图像已处理，白色矩形边界框被绿色替换。&quot;)

我尝试了上述代码，但不知何故它只会随机产生绿色框或在原本为白色的原始背景上产生绿色框。]]></description>
      <guid>https://stackoverflow.com/questions/79309903/changing-color-of-segments-from-yolo-segment-predicted-images</guid>
      <pubDate>Thu, 26 Dec 2024 16:15:02 GMT</pubDate>
    </item>
    <item>
      <title>如何利用代表性模式原理提高灰度纹理分割的准确性[关闭]</title>
      <link>https://stackoverflow.com/questions/79309630/how-to-improve-the-accuracy-of-grayscale-texture-segmentation-using-the-principl</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/79309630/how-to-improve-the-accuracy-of-grayscale-texture-segmentation-using-the-principl</guid>
      <pubDate>Thu, 26 Dec 2024 13:48:07 GMT</pubDate>
    </item>
    <item>
      <title>为什么 batch() 仅返回一个批次？</title>
      <link>https://stackoverflow.com/questions/79309133/why-does-batch-return-only-one-batch</link>
      <description><![CDATA[我是图像处理方面的新手。我有两个二进制类作为子目录，总共有 496 张图像，最后一个批次中还有剩余的 13 张图像，这给我带来了问题。因此，最后一个批次中的 tf.dataset 张量不是 (32, 300, 300, 3)，而是 (16, 300, 300, 3)。实际上，我注意到：

shuffle 后它包含 13 个批次
批处理后它只产生 1 个批次（我假设它是剩余批次）
drop_remainder 时数据为空

为什么 shuffle 后只剩下 1 个批次？
image_size = (300, 300)
batch_size = 32

train_dataset = image_dataset_from_directory(
dataset_dir,
image_size=(image_size[0], image_size[1]),
batch_size=batch_size,
label_mode=&quot;binary&quot;,
validation_split=0.2,
subset=&quot;training&quot;,
seed=123,
)

train_dataset = train_dataset.shuffle(1000)
train_dataset = train_dataset.batch(
batch_size=batch_size, drop_remainder=True
).prefetch(buffer_size=AUTOTUNE)

print(train_dataset.cardinality().numpy())
]]></description>
      <guid>https://stackoverflow.com/questions/79309133/why-does-batch-return-only-one-batch</guid>
      <pubDate>Thu, 26 Dec 2024 09:23:52 GMT</pubDate>
    </item>
    <item>
      <title>分离图像内的盲文字符</title>
      <link>https://stackoverflow.com/questions/79306951/separation-of-braille-characters-inside-of-an-image</link>
      <description><![CDATA[我正在做一个将盲文转换为文本的项目。我已经编写了从图像中识别盲文点的代码，但我不知道如何将盲文分割成单元格。
这部分是识别图像中的斑点（较小的低质量图像目前不起作用）
import cv2
import numpy as np
from sklearn.cluster import KMeans

# 加载图像
image_path = &quot;braille.jpg&quot;
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 设置 SimpleBlobDetector
params = cv2.SimpleBlobDetector_Params()

# 按区域过滤（斑点大小）
params.filterByArea = True
params.minArea = 100 # 根据点大小进行调整
params.maxArea = 1000

# 按圆度过滤
params.filterByCircularity = True
params.minCircularity = 0.9 # 调整点的形状

# 按凸度过滤
params.filterByConvexity = False
params.minConvexity = 0.7

# 按惯性过滤（圆度）
params.filterByInertia = True
params.minInertiaRatio = 0.95

# 使用参数创建检测器
detector = cv2.SimpleBlobDetector_create(params)

# 检测斑点
keypoints = detector.detect(image)

# 将检测到的斑点绘制为红色圆圈
output_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
output_image = cv2.drawKeypoints(output_image, keypoints, np.array([]),
(0, 0, 255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

print(&quot;输出图像&quot;)
cv2.imshow(&quot;输出图像&quot;,output_image)
cv2.waitKey(0)
cv2.destroyAllWindows()

print(f&quot;检测到的斑点数量：{len(keypoints)}&quot;)

以下代码将 blob 的坐标放在图形上（认为这种方式可能更容易操作）
#将图像转换为图形

import matplotlib.pyplot as plt
import numpy

blob_coords = np.array([kp.pt for kp in keypoints]) #blob 的坐标
rounded_coords = np.round(blob_coords).astype(int) #四舍五入的坐标

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

# 基于邻近度的分组
# 如果 X 距离小于最小距离
# 如果 Y 距离小于最小距离
# 存储 X 和 Y 坐标

# 计算最小 x 和 y差异（尝试基于接近度）
minx = 10000
miny = 10000
for i in x_coords:
for j in x_coords:
if abs(i - j) &lt;= minx and (15 &lt; abs(i - j)): # 单元格宽度阈值
minx = abs(i - j)

for i in y_coords:
for j in y_coords:
if abs(i - j) &lt;= miny and (15 &lt; abs(i - j)): # 单元格高度阈值
miny = abs(i - j)

print(f&quot;Smallest x difference: {minx}, Smallest y difference: {miny}&quot;,)

# 绘图
fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;) # 绘制斑点
ax.invert_yaxis()
plt.title(&quot;Braille Cell Detection&quot;)
plt.show()

尝试通过接近度将它们分开（位于我尝试将距离很近的物体分组到一起（我将距离很近的物体分组到一起），但我无法理解其中的逻辑。我也尝试了组聚类 (Kmeans)，但它不是很准确，并且不适用于具有不同字符数的图像，因为它需要不断知道要形成多少个簇。
# 尝试 kmeans 聚类方法
# kmeans 不起作用（无法从图像中找出簇的数量）
# 如果可以找出 nclusters，则可以工作

导入数学
从 sklearn.cluster 导入 KMeans

blob_coords = np.array([kp.pt for kp in keypoints]) # 提取 blob 的 (x, y) 位置
rounded_coords = np.round(blob_coords).astype(int) # 为简单起见，对坐标进行四舍五入

x_coords = rounded_coords[:, 0]
y_coords = rounded_coords[:, 1]

fig, ax = plt.subplots()
ax.scatter(x_coords, y_coords, color=&quot;blue&quot;) # 绘制斑点

ax.invert_yaxis() # 反转 Y 轴以获得类似图像的坐标
plt.title(&quot;盲文单元检测&quot;)
plt.show()

inertias = []

# 2
kmeans = KMeans(n_clusters=26)
kmeans.fit(rounded_coords)

plt.scatter(x_coords,y_coords, c=kmeans.labels_)
plt.show()
]]></description>
      <guid>https://stackoverflow.com/questions/79306951/separation-of-braille-characters-inside-of-an-image</guid>
      <pubDate>Wed, 25 Dec 2024 05:54:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLO 将无界输入导出到 mlpackage/mlmodel 文件</title>
      <link>https://stackoverflow.com/questions/79305588/use-yolo-with-unbounded-input-exported-to-an-mlpackage-mlmodel-file</link>
      <description><![CDATA[我想创建一个 .mlpackage 或 .mlmodel 文件，可以将其导入 Xcode 进行图像分割。为此，我想使用 YOLO 中的分割包来检查它是否符合我的需求。
现在的问题是，此脚本创建的 .mlpackage 文件仅接受固定大小（640x640）的图像：
from ultralytics import YOLO

model = YOLO(&quot;yolo11n-seg.pt&quot;)

model.export(format=&quot;coreml&quot;)

我想在这里进行一些更改，可能使用 coremltools，以处理无界范围（我想处理任意大小的图像）。这里有一些描述：https://apple.github.io/coremltools/docs-guides/source/flexible-inputs.html#enable-unbounded-ranges，但我不明白如何用我的脚本实现它。]]></description>
      <guid>https://stackoverflow.com/questions/79305588/use-yolo-with-unbounded-input-exported-to-an-mlpackage-mlmodel-file</guid>
      <pubDate>Tue, 24 Dec 2024 12:23:06 GMT</pubDate>
    </item>
    <item>
      <title>iOS Swift 根据用户数据进行动态机器学习</title>
      <link>https://stackoverflow.com/questions/79295972/ios-swift-dynamic-machine-learning-from-user-data</link>
      <description><![CDATA[是否可以使用 Apple ML 框架动态学习应用中的用户行为？我已经使用 Create ML 应用程序训练了一个模型，然后我可以从 iOS 设备更新并重新训练吗？这就是我目前使用该模型的方式。
public func calculateMuscleRecoveryTime(_ workout: Workout) {
do {

let config = MLModelConfiguration()
let model = try MuscleRecoveryModel(configuration: config)

let allMuscleGroups = workout.exercises
.compactMap { $0.muscles } // 展平每个锻炼的肌肉数组
.reduce(Set&lt;MuscleGroup&gt;()) { $0.union($1) } // 联合以删除重复项

let uniqueMuscleGroups = Array(allMuscleGroups)

for muscleGroup in uniqueMuscleGroups {
let trainingIntensity = Int64(workout.intensity.intValue)
let lastTrainedTimestamp = workout.date
let timeAgo = timeAgoInSeconds(from: lastTrainedTimestamp)
let muscleName = muscleGroup.rawValue.lowercased()

let prediction = try model.prediction(muscle: muscleName, intense: trainingIntensity, lastTrained: timeAgo)
}
} catch let error {
print(&quot;Error: &quot;, error)
}
}
]]></description>
      <guid>https://stackoverflow.com/questions/79295972/ios-swift-dynamic-machine-learning-from-user-data</guid>
      <pubDate>Fri, 20 Dec 2024 01:10:59 GMT</pubDate>
    </item>
    <item>
      <title>在 Google Cloud Functions 中部署 Keras 模型进行预测</title>
      <link>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</link>
      <description><![CDATA[我一直在尝试将一个非常简单的 Keras 玩具模型部署到 Cloud Functions，该模型可以预测图像的类别，但由于未知原因，当执行到 predict 方法时，它会卡住，不会抛出任何错误，最终会超时。
import functions_framework
import io
import numpy as np
import tensorflow as tf

from tensorflow.keras.models import load_model
from PIL import Image

model = load_model(&quot;gs://&lt;my-bucket&gt;/cifar10_model.keras&quot;)

class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;]

def preprocess_image(image_file):
img = Image.open(io.BytesIO(image_file.read()))
img = img.resize((32, 32))
img = np.array(img)
img = img / 255.0
img = img.reshape(1, 32, 32, 3)
return img

@functions_framework.http
def predict(request):
image = preprocess_image(request.files[&#39;image_file&#39;])
print(image.shape) # 这会打印 OK
prediction = model.predict(image)
print(prediction) # 永远不会打印
predict_class = class_names[np.argmax(prediction)]
return f&quot;Predicted class: {predicted_class}&quot;

本地调试运行良好，预测速度如预期一样快（模型权重文件为 2MB）。我还在此过程中添加了几个打印（从上面的代码片段中删除），执行工作正常，直到 predict 方法。
即使最小计算配置应该可以工作，我还是尝试保留更多内存和 CPU，但没有任何效果。该模型托管在存储中，我尝试先下载它，但也没有用。我也尝试在 tf.device(&#39;/cpu:0&#39;) 上下文中进行预测，传递 step=1 参数并首先将图像数组转换为 Keras 数据集，如 ChatGPT 所建议的那样，结果相同。实际上，调用 predict 根本没有打印任何内容。调用 call 而不是 predict 没有任何效果。
我错过了什么？]]></description>
      <guid>https://stackoverflow.com/questions/79288128/deploying-keras-model-for-prediction-in-google-cloud-functions</guid>
      <pubDate>Tue, 17 Dec 2024 13:51:16 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost/XGBRanker 生成概率而不是排名分数</title>
      <link>https://stackoverflow.com/questions/79278625/xgboost-xgbranker-to-produce-probabilities-instead-of-ranking-scores</link>
      <description><![CDATA[我有一个学生考试成绩的数据集，如下所示：
班级 ID 班级规模 学生编号 智商 学习时间 分数
1 3 3 101 10 98
1 3 4 99 19 80
1 3 6 130 3 95
2 4 4 93 5 50
2 4 5 103 9 88
2 4 8 112 12 99
2 4 1 200 10 100 

我想建立一个机器学习模型，尝试使用 IQ 和 Hours_Studied 预测谁将成为班级第一名（即最高 Score），对于任何给定的 Class_ID特征。
由于这是一个排名问题，因此自然的一类学习模型是使用 XGBoost 中的 XGBRanker 或 lightgbm 中的 LGBMRanker。
这是我使用 xgboost 的代码：
from sklearn.model_selection import GroupShuffleSplit
import xgboost as xgb

gss = GroupShuffleSplit(test_size=.40, n_splits=1, random_state = 7).split(df, groups=df[&#39;Class_ID&#39;])

X_train_inds, X_test_inds = next(gss)

train_data = df.iloc[X_train_inds]
X_train = train_data.loc[:, ~train_data.columns.isin([&#39;Class_ID&#39;,&#39;Student_Number&#39;,&#39;Score&#39;])]
y_train = train_data.loc[:, train_data.columns.isin([&#39;Score&#39;])]

groups = train_data.groupby(&#39;Class_ID&#39;).size().to_frame(&#39;Class_size&#39;)[&#39;Class_size&#39;].to_numpy()

test_data = df.iloc[X_test_inds]

X_test = test_data.loc[:, ~test_data.columns.isin([&#39;Student_Number&#39;,&#39;Score&#39;])]
y_test = test_data.loc[:, test_data.columns.isin([&#39;Score&#39;])]

model = xgb.XGBRanker( 
tree_method=&#39;hist&#39;,
device=&#39;cuda&#39;,
booster=&#39;gbtree&#39;,
objective=&#39;rank:pairwise&#39;,
enable_categorical=True,
random_state=42, 
learning_rate=0.1,
colsample_bytree=0.9, 
eta=0.05, 
max_depth=6, 
n_estimators=175, 
subsample=0.75 
)

model.fit(X_train, y_train, group=groups, verbose=True)

def predict(model, df):
return model.predict(df.loc[:, ~df.columns.isin([&#39;Class_ID&#39;,&#39;Student_Number&#39;])])

predictions = (X_test.groupby(&#39;Class_ID&#39;)
.apply(lambda x: predict(model, x)))

代码运行良好，具有合理的预测能力。但是，输出是“相关性得分”列表，而不是概率列表。但似乎 XGBRanker 和 LGBMRanker 都没有属性 predict_proba，该属性返回获得班级最高分的概率。
所以我的问题是，有没有办法将 相关性得分 转换为概率，或者是否有其他自然类别的排名模型可以处理此类问题？
编辑在这个问题中，我只关心最终名列前茅的人（或者可能是前三名），所以排名并不是那么重要（例如，知道学生 4 排名第 11 位，学生 8 排名第 12 位并不那么重要），所以我想一种方法是在 xgboost 中使用分类而不是排名。但我想知道还有其他方法吗。]]></description>
      <guid>https://stackoverflow.com/questions/79278625/xgboost-xgbranker-to-produce-probabilities-instead-of-ranking-scores</guid>
      <pubDate>Fri, 13 Dec 2024 14:20:37 GMT</pubDate>
    </item>
    <item>
      <title>scikit-learn交叉验证过度拟合或欠拟合[关闭]</title>
      <link>https://stackoverflow.com/questions/20357705/scikit-learn-cross-validation-over-fitting-or-under-fitting</link>
      <description><![CDATA[我正在使用 scikit-learn cross_validation 并获得例如 0.82 平均分数 (r2_scorer)。
我如何知道使用 scikit-learn 函数时是否存在过度拟合或欠拟合？]]></description>
      <guid>https://stackoverflow.com/questions/20357705/scikit-learn-cross-validation-over-fitting-or-under-fitting</guid>
      <pubDate>Tue, 03 Dec 2013 17:25:03 GMT</pubDate>
    </item>
    </channel>
</rss>