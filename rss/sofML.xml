<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Thu, 14 Mar 2024 12:24:47 GMT</lastBuildDate>
    <item>
      <title>无法使用此代码从文章中提取信息</title>
      <link>https://stackoverflow.com/questions/78160347/fail-to-extract-information-from-articles-using-this-code</link>
      <description><![CDATA[这段代码应该从不同的文章链接中提取标题和主要文本，但由于没有找到任何标题或文本，它只是跳过了网址
# 网页抓取和数据处理
对于范围内的 i(2, ws.max_row + 1)：
    link = ws.cell(row=i,column=2).hyperlink.target if ws.cell(row=i,column=2).hyperlink else ws.cell(row=i,column=2).value
    
    print(&quot;正在处理 URL:&quot;, link) # 打印正在处理的 URL
    
    headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0&#39;}
    响应 = requests.get(链接, headers=headers)
    soup = BeautifulSoup(response.content, &#39;html.parser&#39;)

    如果不是 soup.find(&#39;h1&#39;) 或不是 soup.find(&#39;div&#39;, class_=&#39;td-post-content&#39;):
        print(“由于缺少‘h1’或‘div.td-post-content’而跳过 URL”)
        继续

    head = soup.find(&#39;h1&#39;).get_text()
    text = soup.find(&#39;div&#39;, class_=&#39;td-post-content&#39;).get_text()
]]></description>
      <guid>https://stackoverflow.com/questions/78160347/fail-to-extract-information-from-articles-using-this-code</guid>
      <pubDate>Thu, 14 Mar 2024 12:01:52 GMT</pubDate>
    </item>
    <item>
      <title>难以避开墙壁</title>
      <link>https://stackoverflow.com/questions/78160153/having-a-trouble-avoiding-the-wall</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78160153/having-a-trouble-avoiding-the-wall</guid>
      <pubDate>Thu, 14 Mar 2024 11:28:34 GMT</pubDate>
    </item>
    <item>
      <title>VAE 不是学习</title>
      <link>https://stackoverflow.com/questions/78159759/vae-is-not-learning</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78159759/vae-is-not-learning</guid>
      <pubDate>Thu, 14 Mar 2024 10:25:17 GMT</pubDate>
    </item>
    <item>
      <title>由于CPU和GPU之间频繁的数据传输，CNN中的“零填充”是否会增加推理时间？</title>
      <link>https://stackoverflow.com/questions/78158937/does-zero-padding-in-cnn-increase-the-inference-time-due-to-frequent-data-tran</link>
      <description><![CDATA[我认为在设计 DNN 加速器或 NPU 时，CNN 中的零填充是一个非常烦人的操作。所以我想知道在像 Pytorch/TF 这样的现代机器学习框架中，如果 zero-padding 是在 CPU 或 GPU 上执行的（我不擅长它们）？如果在CPU上完成，当存在需要填充的连续层时，由于CPU和GPU之间频繁的数据移动，该操作是否会大大增加总推理时间？否则，GPU如何完成低效的padding操作呢？
我想我没有找到太多这方面的信息。所以我希望有人能帮我解答这个问题。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78158937/does-zero-padding-in-cnn-increase-the-inference-time-due-to-frequent-data-tran</guid>
      <pubDate>Thu, 14 Mar 2024 08:07:04 GMT</pubDate>
    </item>
    <item>
      <title>SageMaker 预期监控计划的架构？</title>
      <link>https://stackoverflow.com/questions/78158477/sagemaker-expected-schema-for-monitoring-schedule</link>
      <description><![CDATA[我正在尝试针对模型生成的数据运行 SageMaker 监控作业，但我不断遇到架构验证问题。不幸的是，我找不到任何有关 SageMaker 所期望的确切架构的文档。
它尝试分析的数据如下所示：
[{&quot;captureData&quot;: {&quot;endpointInput&quot;: {&quot;observedContentType&quot;:&quot;application/json&quot;,&quot;mode&quot;: &quot;INPUT&quot;,&quot;data&quot;: &quot;Some输入”、“编码”：“JSON”}、“端点输出”：{“observedContentType”：“application/json”、“模式”：“输出”、“数据”：“原始”输出”,“编码”:“JSON”}},“eventMetadata”:{“eventId”:“abc12345-ca38-42f2-a57b-03b6bd701235”,“推理时间”:“2024-03 -14T02:06:27Z&quot;},&quot;事件版本&quot;:&quot;0&quot;}]

遇到的错误：
com.amazonaws.sagemaker.dataanalyzer.exception.CustomerError：我们目前仅支持平面 json。

我已经将 json 展平以模拟 jsonl 结构。每当我尝试进行调整以进一步展平 json 时，它都会抱怨缺少键。
更多背景：
DataCaptureConfig
capture_options=[“请求”,“响应”]

data_capture_config = DataCaptureConfig(
    启用_捕获=真，
    采样百分比=100，
    destination_s3_uri=s3_capture_upload_path,
    捕获选项=捕获选项
）

基线作业
baseline_job = my_default_monitor.suggest_baseline(
    Baseline_dataset=f“{baseline_data_uri}/fake_data_augmented_with_variability.csv”，
    dataset_format=sagemaker.model_monitor.DatasetFormat.csv(header=True),
    output_s3_uri=baseline_results_uri,
    作业名称=基线作业名称，
    等待=假，
    日志=假
）

监控计划
my_default_monitor.create_monitoring_schedule(
    Monitor_schedule_name=mon_schedule_name,
    端点输入=端点名称，
    post_analytics_processor_script = s3_code_postprocessor_uri，
    output_s3_uri=s3_report_path,
    统计=my_default_monitor.baseline_statistics(),
    约束=my_default_monitor.suggested_constraints(),
    Schedule_cron_expression=CronExpressionGenerator.每小时(),
    enable_cloudwatch_metrics=真，
）

端点调用
响应=runtime_client.invoke_endpoint(
   端点名称=端点名称，
   ContentType =“应用程序/json”，
   正文=body_json
）
]]></description>
      <guid>https://stackoverflow.com/questions/78158477/sagemaker-expected-schema-for-monitoring-schedule</guid>
      <pubDate>Thu, 14 Mar 2024 06:24:08 GMT</pubDate>
    </item>
    <item>
      <title>cv2.imwrite 在 OpenCV Python 中行为不正常 [关闭]</title>
      <link>https://stackoverflow.com/questions/78158327/cv2-imwrite-not-behaving-properly-in-opencv-python</link>
      <description><![CDATA[我正在开发一个使用 cv2 的项目，用于在 Mac 上处理和保存图像。但问题是保存的图像数量为 490，但循环运行了 1251 次。
这是我尝试过的：
faces_path = glob(&#39;./Data/faces/*.jpg&#39;) # 此文件夹中有 1251 张图像

这是处理图像的函数：
&lt;前&gt;&lt;代码&gt;导入cv2


def extract_faces(img_path, i):
    img = cv2.imread(img_path)
    灰色 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY, 1.3, 5)
    faces = haar.detectMultiScale(灰色, 1.5, 5)
    对于面中的 x,y,w,h：
        roi = img[y:y+h, x:x+h]
        cv2.imwrite(“./Data/cropped/face_{}.jpg”.format(i), roi)

而且，这是我使用该函数的方式：
&lt;前&gt;&lt;代码&gt;# extract_faces(路径, 1)
对于 i，枚举中的路径（faces_path）：
    尝试：
        extract_faces（路径，i）
        print(“信息：{}/{} 已处理。”.format(i, len(faces_path)))
        除了：
            print(&quot;INFO: {}/{} 未成功处理。&quot;.format(i, len(faces_path)))

循环运行了 1251 次，但保存在 cropped 文件夹中的图像为 490 可能是什么问题？]]></description>
      <guid>https://stackoverflow.com/questions/78158327/cv2-imwrite-not-behaving-properly-in-opencv-python</guid>
      <pubDate>Thu, 14 Mar 2024 05:35:03 GMT</pubDate>
    </item>
    <item>
      <title>将低秩近似应用于可学习参数</title>
      <link>https://stackoverflow.com/questions/78158096/applying-low-rank-approximation-to-learnable-parameters</link>
      <description><![CDATA[我试图了解将低秩近似应用于类中的可学习参数是否有意义。目标是减少参数数量。
我有以下自定义模块：
类 CustomPara(nn.Module):
    
    def __init__(self, num_blocks, in_planes, out_planes, kernel_size):
        super(CustomPara, self).__init__()
        self.coefficient_shape = (num_blocks,1,1,1,1)
        块 = [torch.Tensor(out_planes, in_planes, kernel_size, kernel_size) for _ in range(num_blocks)]
        对于范围内的 i(num_blocks): init.kaiming_normal_(blocks[i])
        self.blocks = nn.Parameter(torch.stack(blocks)) # 这是我们稍后将冻结的内容

    defforward（自身，系数）：
        Final_blocks = (self.blocks*系数).sum(0)
        返回final_blocks

是否可以使用 blocks 参数上的低秩自适应来减少此处可学习参数的数量？
谢谢。]]></description>
      <guid>https://stackoverflow.com/questions/78158096/applying-low-rank-approximation-to-learnable-parameters</guid>
      <pubDate>Thu, 14 Mar 2024 04:22:39 GMT</pubDate>
    </item>
    <item>
      <title>使用 Keras 3 中预先创建的批量图像数据训练分割模型</title>
      <link>https://stackoverflow.com/questions/78157765/training-a-segmentation-model-with-pre-created-batches-of-image-data-in-keras-3</link>
      <description><![CDATA[我有一个用例，我需要在将数据输入模型进行训练之前手动生成批量数据。
假设我有(256 x 100 x 100)图像，而我手动创建的批次的尺寸为(32 x 100 x 100)。如果我的训练集按 [batch_1, batch_2, ... batch_8] 的顺序排列，则得到原始形状 (256 x 100 x 100)。
当我将其提供给 model.fit() 并将批量大小指定为 32 时，模型能否正确获取每个手动批次以按顺序进行训练，不混合每个批次的数据点并创建大小为 32 的新批次？
我正在使用Keras 3.0.1。]]></description>
      <guid>https://stackoverflow.com/questions/78157765/training-a-segmentation-model-with-pre-created-batches-of-image-data-in-keras-3</guid>
      <pubDate>Thu, 14 Mar 2024 02:23:09 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 sklearns 的 cross_validate 对样本进行加权仅用于评分？</title>
      <link>https://stackoverflow.com/questions/78155034/how-to-weight-samples-with-sklearnss-cross-validate-for-scoring-only</link>
      <description><![CDATA[我正在对由真实样本和增强样本组成的数据集运行回归任务。增强样本是通过抖动真实样本生成的。我想通过与 sklearn 进行交叉验证来选择性能最佳的模型。
为此我想：

在由真实样本和增强样本组成的集合上训练模型。我不希望拟合过程考虑样本的来源（即它应该相当于运行estimator.fit(..., sample_weights = [1,1,..., 1]).
根据模型仅在真实样本上的表现对模型进行评分。为此，我考虑将增强（或真实）样本的权重设置为 0（或 1）。

如何使用sklearn的cross_validate&lt;来实现这一点/代码&gt;？
我尝试了以下方法：
来自 sklearn 导入 model_selection
从 sklearn.ensemble 导入 RandomForestRegressor
从 sklearn.metrics 导入 r2_score、mean_squared_error、make_scorer
将 numpy 导入为 np

n_smpl, n_feats = 100, 5
arr_source = np.random.random((n_smpl, n_feats))
arr_target = np.random.random((n_smpl, n_feats))
arr_weight = np.random.randint(0, 2, n_smpl) # 0 表示增强，1 表示真实

模型 = RandomForestRegressor()
kfold_splitter = model_selection.KFold(n_splits=5, random_state=7, shuffle=True)
我的得分者 = {
    “r2_weighted”：make_scorer（r2_score，sample_weight = arr_weight），
    “mse_weighted”：make_scorer（mean_squared_error，greater_is_better = False，sample_weight = arr_weight）
}

cv_results = model_selection.cross_validate（模型，arr_source，arr_target，评分= my_scorers，cv = kfold_splitter）

但这会返回ValueError：发现样本数量不一致的输入变量：[20, 20, 100]。我知道发生这种情况是因为 cross_validate 无法根据折叠分割样本权重。
有什么方法可以让它通过交叉验证吗？或者还有其他方法吗？]]></description>
      <guid>https://stackoverflow.com/questions/78155034/how-to-weight-samples-with-sklearnss-cross-validate-for-scoring-only</guid>
      <pubDate>Wed, 13 Mar 2024 15:36:03 GMT</pubDate>
    </item>
    <item>
      <title>在 SageMaker 上的 TensorFlow Recommenders 中初始化 FactorizedTopK 时出错：“无法将‘计数器’转换为形状”</title>
      <link>https://stackoverflow.com/questions/78144515/error-initializing-factorizedtopk-in-tensorflow-recommenders-on-sagemaker-cann</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78144515/error-initializing-factorizedtopk-in-tensorflow-recommenders-on-sagemaker-cann</guid>
      <pubDate>Tue, 12 Mar 2024 03:28:18 GMT</pubDate>
    </item>
    <item>
      <title>训练随机森林分类器</title>
      <link>https://stackoverflow.com/questions/77970974/train-a-random-forest-classifier</link>
      <description><![CDATA[我正在尝试在一组图像上训练随机森林分类器，旨在对 10 种不同的颜色类别进行分类。我有一个带有子目录的 \dataset 目录，每个子目录都以一种颜色（黄色、红色、白色等）命名。我采用 HSV 直方图并将其用作我的特征（它比 RGB 更好吗？）
这是我的代码。我在代码中添加了 print(image_path) 进行调试。但运行时，它只打印第一个文件，不打印其他任何内容。
有什么问题吗？如何解决？
从 sklearn.ensemble 导入 RandomForestClassifier
从 sklearn.feature_extraction.image 导入 extract_patches_2d
从 sklearn.model_selection 导入 train_test_split
从 skimage.color 导入 rgb2hsv，rgb2gray
从 skimage.feature 导入graycomatrix，graycoprops
导入操作系统
将 numpy 导入为 np
从 PIL 导入图像
导入作业库


# 定义路径和参数
data_dir =“.\数据集” # 替换为你的实际目录路径
num_trees = 100 # 森林中树木的数量
patch_size = (32, 32) # 用于特征提取的图像块的大小

# 定义特征提取函数
def extract_color_histogram(图像):
    # 将 PIL 图像转换为 NumPy 数组
    img_array = np.array(图像)
    # 调整图像大小
    img_resized = Image.fromarray(img_array).resize((640, 640))
    # 转换为 HSV
    img_hsv = img_resized.convert(“HSV”)
    hist, _ = np.histogram(img_hsv, bins=(8, 8, 8), 范围=((0, 255), (0, 255), (0, 255)))
    hist = hist.flatten()
    返回历史记录

def extract_texture_features(图像):
    # 转换为灰度并提取纹理特征
    灰色 = rgb2gray(图像)
    gray_uint = (gray * 255).astype(np.uint8) # 将灰度转换为无符号整数
    glcm = Graycomatrix(gray_uint，距离=[5]，角度=[0]，级别=256，对称=True，normed=True)
    特征 = Graycoprops(glcm, &#39;对比度&#39;)[0]
    返回特征

# 加载数据并提取特征
X、y = []、[]
对于 os.listdir(data_dir) 中的颜色：
    对于 os.listdir(os.path.join(data_dir, color)) 中的 image_file：
        image_path = os.path.join(data_dir, 颜色, image_file)
        打印（图像路径）
        img = Image.open(图像路径)
        img_array = np.array(img)
        补丁 = extract_patches_2d(img_array, patch_size=patch_size)
        对于补丁中的补丁：
            # 从每个补丁中提取特征
            color_hist = extract_color_histogram(补丁)
            纹理特征=提取纹理特征（补丁）
            特征 = np.concatenate((color_hist,texture_features))
            X.append（功能）
            y.追加（颜色）

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练随机森林模型
print(&quot;现在训练...&quot;)
模型 = RandomForestClassifier(n_estimators=num_trees, random_state=42)
model.fit(X_train, y_train)

# 评估模型性能
准确度 = model.score(X_test, y_test)
print(f&quot;模型精度: {accuracy:.2f}&quot;)

# 保存模型以供将来使用
joblib.dump(模型,“color_model.pkl”)
]]></description>
      <guid>https://stackoverflow.com/questions/77970974/train-a-random-forest-classifier</guid>
      <pubDate>Fri, 09 Feb 2024 21:33:17 GMT</pubDate>
    </item>
    <item>
      <title>如何跟踪无人机图像中的单个点（例如角落）？</title>
      <link>https://stackoverflow.com/questions/77883796/how-to-track-a-single-point-e-g-corner-in-uav-images</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/77883796/how-to-track-a-single-point-e-g-corner-in-uav-images</guid>
      <pubDate>Fri, 26 Jan 2024 01:40:21 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 PyTorch 创建点的 2D 张量，每个维度从 0 到 1？</title>
      <link>https://stackoverflow.com/questions/77038120/how-to-create-a-2d-tensor-of-points-with-pytorch-each-dimension-going-from-0-to</link>
      <description><![CDATA[我正在尝试创建一个二维张量，其中每个维度的范围从 0 到 1。
对于一维张量，我可以使用：
torch.arange(0, 1, 0.2)

这给了我：
张量([0.0, 0.2, 0.4, 0.6, 0.8])

但是，我想将其扩展到 2D 点。我想要的输出是[形状为 (25, 2)]：
&lt;前&gt;&lt;代码&gt;张量([
    [0.0, 0.0], [0.0, 0.2], [0.0, 0.4], [0.0, 0.6], [0.0, 0.8],
    [0.2, 0.0], [0.2, 0.2], [0.2, 0.4], [0.2, 0.6], [0.2, 0.8],
    [0.4, 0.0], [0.4, 0.2], [0.4, 0.4], [0.4, 0.6], [0.4, 0.8],
    [0.6, 0.0], [0.6, 0.2], [0.6, 0.4], [0.6, 0.6], [0.6, 0.8],
    [0.8, 0.0], [0.8, 0.2], [0.8, 0.4], [0.8, 0.6], [0.8, 0.8]
]）

如何使用 PyTorch 实现此目的？]]></description>
      <guid>https://stackoverflow.com/questions/77038120/how-to-create-a-2d-tensor-of-points-with-pytorch-each-dimension-going-from-0-to</guid>
      <pubDate>Mon, 04 Sep 2023 13:25:27 GMT</pubDate>
    </item>
    <item>
      <title>在恢复的对象中找不到检查点中的值：(root).optimizer.iter</title>
      <link>https://stackoverflow.com/questions/71929036/value-in-checkpoint-could-not-be-found-in-the-restored-object-root-optimizer</link>
      <description><![CDATA[所以我使用了此链接中的预训练权重：http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz
然后我将 pipeline.config 从下载链接复制到我的文件夹，因为我想将优化器更改为 adam 以在我自己的数据集上进行训练（链接中的管道正在使用momentum_optimizer）
这是我要训练的 pipeline.config 代码：
优化器{
亚当_优化器{
  学习率{
    余弦_衰减_学习率{
      学习率基础：0.01
      总步数：50000
      热身学习率：0.026666
      热身步数：1000
    }
  }
  ε：1e-8
}
use_moving_average: false

}
但是 cmd 提示给了我这个：
警告：tensorflow：在恢复的对象中找不到检查点中的值：(root).optimizer.iter
W0419 23：47：07.776149 17436 util.py：194]在恢复的对象中找不到检查点中的值：（root）.optimizer.iter
警告：tensorflow：在恢复的对象中找不到检查点中的值：(root).optimizer.decay
W0419 23：47：07.777309 17436 util.py：194]在恢复的对象中找不到检查点中的值：（root）.optimizer.decay
警告：tensorflow：在恢复的对象中找不到检查点中的值：(root).optimizer.momentum
W0419 23：47：07.779311 17436 util.py：194]在恢复的对象中找不到检查点中的值：（root）.optimizer.momentum

谁能解释一下谢谢
[1]: https://i.stack.imgur.com/BBmVA.png]]></description>
      <guid>https://stackoverflow.com/questions/71929036/value-in-checkpoint-could-not-be-found-in-the-restored-object-root-optimizer</guid>
      <pubDate>Tue, 19 Apr 2022 17:33:49 GMT</pubDate>
    </item>
    <item>
      <title>Docker for Lambda (FAST API) 中的{“无法导入模块‘main’：没有名为‘main’的模块”，“errorType”：“Runtime.ImportModuleError”}</title>
      <link>https://stackoverflow.com/questions/71305887/unable-to-import-module-main-no-module-named-main-errortype-runtime</link>
      <description><![CDATA[我已经创建了一个 Fastapi，现在尝试使用 Docker 容器将其部署到 AWS lambda。但有一个错误：
{“errorMessage”：“无法导入模块“main”：没有名为“main”的模块”，“errorType”：“Runtime.ImportModuleError”，“stackTrace”：[] }

我已经尽力了。
这是我的 main.py 文件：
from fastapi 导入 FastAPI
从 starlette.status 导入 HTTP_302_FOUND,HTTP_303_SEE_OTHER
导入spacy
从字符串导入标点符号
从曼古姆进口曼古姆
进口uvicorn
应用程序 = FastAPI()

@app.get(&#39;/&#39;)
def home():
    返回{“答案”：“你好世界”}

@app.get(&#39;/tags&#39;)
def prep_data(文本):
    标签=标记（文本，nlp）
    标签 = getdict(标签)
    返回 {
        ‘标签’：标签
    }

处理程序 = Mangum(应用程序)
如果 __name__ == “__main__”：
    # 处理程序 = Mangum(应用程序)
    uvicorn.run(&#39;main:app&#39;, host=&#39;0.0.0.0&#39;, port=8000, reload=False, root_path=”/”)

该错误表明 main.py 文件没有 main.py 文件，正如您所看到的 dockerfile：
&lt;前&gt;&lt;代码&gt;来自 public.ecr.aws/lambda/python:3.8

复制./应用程序/应用程序

复制 ./requirements.txt /app/requirements.txt

工作目录/应用程序

运行 pip install -rrequirements.txt

CMD [“main.handler”]

我的目录结构是这样的：
&lt;前&gt;&lt;代码&gt;/应用程序
    主要.py
Dockerfile
要求.txt
]]></description>
      <guid>https://stackoverflow.com/questions/71305887/unable-to-import-module-main-no-module-named-main-errortype-runtime</guid>
      <pubDate>Tue, 01 Mar 2022 08:52:48 GMT</pubDate>
    </item>
    </channel>
</rss>