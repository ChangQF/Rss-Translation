<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>标记为机器学习的活跃问题 - Thinbug</title>
    <link>https://stackoverflow.com/questions/tagged/?tagnames=machine-learning&sort=active</link>
    <description>来自 stackoverflow.com 的最新 30 个</description>
    <lastBuildDate>Tue, 28 May 2024 12:27:15 GMT</lastBuildDate>
    <item>
      <title>预测多元时间序列时 VARIMA 模型的模型漂移</title>
      <link>https://stackoverflow.com/questions/78544041/model-drift-for-varima-model-when-forecasting-multivariate-time-series</link>
      <description><![CDATA[我目前正在尝试使用有关冷却系统 5 种不同传感器测量值的多元时间序列数据来训练 VARIMA 模型。数据具有周期性，因此每 25 个数据点左右就会重复出现完全相同的模式。我有一个包含 5 个不同组件的数据集，其中每分钟都有一个数据点。我使用了 2 周的数据来训练模型，然后让它合成数据。我使用的 VARIMA 模型是从 Darts 包导入的，我这样定义模型：model_VARIMA = VARIMA(p=12, d=0, q=0, trend=“n”)。该模型使用 2 周的训练数据进行训练。当预测未来 30 个点或更长时间时，预测明显开始显示模型漂移。所有组件不再产生循环的多元时间序列数据，而是不再有价值的水平直线。我想知道是否有人对此有解释以及解决问题的方法。谢谢！]]></description>
      <guid>https://stackoverflow.com/questions/78544041/model-drift-for-varima-model-when-forecasting-multivariate-time-series</guid>
      <pubDate>Tue, 28 May 2024 12:15:20 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中如何按行处理不同数量的解释变量？</title>
      <link>https://stackoverflow.com/questions/78543787/how-should-different-numbers-of-explanatory-variables-be-handled-by-row-in-machi</link>
      <description><![CDATA[我在制作预测模型时遇到问题，所以我留下一个问题。
我正在尝试使用机器学习方法（例如随机森林、xgboost 等）创建预测模型。
此时y值为差分后的月时间序列数据，x值为差分后的日时间序列数据。
供参考，t，即时间，以美国股市交易日为时间单位。
我的模型由以下格式组成。
预测值 = y_(t+21) - y_(t)
解释值 = y(t) - y(t-1), y(t-1) - y(t-2) ... y(t-p) - y(t-p-1)
此时p为该月最后一个交易日。
这里的问题是每个月都有不同的交易天数
例如，1980年1月有23个交易日，但1981年2月有20个交易日，并且有可能假期较少的月份。
在这种情况下，在构建用于预测因变量的解释变量数据集时，可能会为逐列中的某些值生成 NaN 值。
这种情况，普遍应该如何处理？或者是否有一个术语或论文涉及这个问题？
y_(t+21) - y_(t) 有两种情况。一是区分月末值和区分月均值。因此，还没有触及任何内容。]]></description>
      <guid>https://stackoverflow.com/questions/78543787/how-should-different-numbers-of-explanatory-variables-be-handled-by-row-in-machi</guid>
      <pubDate>Tue, 28 May 2024 11:21:17 GMT</pubDate>
    </item>
    <item>
      <title>二元分类得到大于1的预测值</title>
      <link>https://stackoverflow.com/questions/78543310/binary-classification-get-predict-value-greater-than-1</link>
      <description><![CDATA[
&lt;img alt=&quot;预测代码&quot; src=&quot;https://i.sstatic.net/Lwa9ISdr.png ” /&gt;
任何人都可以帮助我，为什么我的模型返回预测值大于 1。即使我使用具有 1 个单位和 sigmoid 激活函数的密集层。我创建了一个二元分类模型。
我正在使用tensorflow和kerastuner进行超参数调整。]]></description>
      <guid>https://stackoverflow.com/questions/78543310/binary-classification-get-predict-value-greater-than-1</guid>
      <pubDate>Tue, 28 May 2024 09:53:40 GMT</pubDate>
    </item>
    <item>
      <title>提高人脸识别性能并扩大检测范围</title>
      <link>https://stackoverflow.com/questions/78543297/improving-face-recognition-performance-and-extending-detection-range</link>
      <description><![CDATA[我正在使用 Python、dlib 和 OpenCV 开发人脸识别系统，但在检测多张人脸时遇到性能问题，并且我需要扩展检测范围以检测 3 米以上以外的人脸。以下是我当前的设置和挑战的摘要：
当前设置：

使用 Logitech Brio 4K 超高清网络摄像头捕获视频流。
使用 dlib 的正面人脸检测器进行人脸检测。
利用 Dlib ResNet 模型进行人脸识别。
使用 OpenCV 处理视频流。
将已知的面部特征存储在 CSV 文件中以进行比较。

挑战：

当画面中有多个面孔时，性能会显着下降，
导致丢帧和降低 FPS。
需要检测距离大于 3 米的人脸
保持准确性。

为了加快计算速度，我应该考虑哪些硬件加速技术或优化？]]></description>
      <guid>https://stackoverflow.com/questions/78543297/improving-face-recognition-performance-and-extending-detection-range</guid>
      <pubDate>Tue, 28 May 2024 09:51:28 GMT</pubDate>
    </item>
    <item>
      <title>截断标记LM [关闭]</title>
      <link>https://stackoverflow.com/questions/78542847/truncation-marckuplm</link>
      <description><![CDATA[我在使用 marckupLM 时遇到困难，我想知道是否有办法让 MarkupLMProcessor 考虑因数据本身而被截断的信息。我的问题来自于需要比我必须截断的数据更大的数据，但我需要模型考虑为标记分类而截断的信息。
我希望有一些参数可以促进不同大小的数据，但在我的研究过程中我还没有找到它。我想知道我是否真的必须手工填写，或者是否有某种方法。]]></description>
      <guid>https://stackoverflow.com/questions/78542847/truncation-marckuplm</guid>
      <pubDate>Tue, 28 May 2024 08:29:01 GMT</pubDate>
    </item>
    <item>
      <title>如何聚合 DNA 序列数据 [关闭]</title>
      <link>https://stackoverflow.com/questions/78542443/how-to-aggregate-dna-sequence-data</link>
      <description><![CDATA[数据文件：Task1-serious data.zip，包含10,000,000个样本，每个样本为一行，第一个样本号，剩余64个样本特征。数据来自DNA序列

最大聚类数为1000000，如果超过1000000，则多余的聚类将合并到第1000000个聚类中

如enter image description here
什么样的聚类方案最好，我们应该怎么做呢]]></description>
      <guid>https://stackoverflow.com/questions/78542443/how-to-aggregate-dna-sequence-data</guid>
      <pubDate>Tue, 28 May 2024 07:13:48 GMT</pubDate>
    </item>
    <item>
      <title>微调模型时内存不足</title>
      <link>https://stackoverflow.com/questions/78542429/running-out-of-ram-when-finetuning-model</link>
      <description><![CDATA[我目前正在尝试从以下位置微调 Wav2Vec2 模型：https://huggingface。 co/dima806/bird_sounds_classification。但我的 RAM 使用率超过了 Google Colab 上的免费套餐。
以下是我的代码：
从 Transformers 导入 TrainingArguments、Trainer

# 使用ignore_mismatched_sizes=True加载模型
模型 = Wav2Vec2ForSequenceClassification.from_pretrained(
    “dima806/bird_sounds_classification”，
    num_labels=len(label2id),
    ignore_mismatched_sizes=True
）

# 设置梯度累积训练
batch_size = 1 # 减少批量大小以管理内存
accumulation_steps = 4 # 累积 4 个步骤的梯度

训练参数 = 训练参数（
    输出目录=“./结果”,
    evaluation_strategy=“纪元”，
    学习率=2e-5,
    per_device_train_batch_size=batch_size，
    per_device_eval_batch_size=batch_size，
    gradient_accumulation_steps=accumulation_steps, # 梯度累积
    num_train_epochs=3,
    权重衰减=0.01，
    fp16=True, # 启用混合精度训练
）

教练=教练（
    型号=型号，
    参数=训练参数，
    训练数据集=训练数据集，
    eval_dataset = val_dataset，
    分词器=特征提取器，
）

# 训练模型
训练师.train()

RAM 超过 12.7GB 的原因可能是什么？我的数据集仅包含 20 个项目。我该如何解决这个问题？]]></description>
      <guid>https://stackoverflow.com/questions/78542429/running-out-of-ram-when-finetuning-model</guid>
      <pubDate>Tue, 28 May 2024 07:10:21 GMT</pubDate>
    </item>
    <item>
      <title>如何标记像 COCOA 这样的非模态数据集？</title>
      <link>https://stackoverflow.com/questions/78541723/how-to-label-an-amodal-dataset-like-cocoa</link>
      <description><![CDATA[我们经常通过 LabelMe 使用 COCO 数据集进行实例分割，最近，模态实例分割变得很流行，因为它可以感知遮挡的对象。有一些公共的非模态数据集，例如 COCOA 和 KINS，但如何标记它们对我来说并不清楚。我要标记我自己的amodal数据集，我应该怎么做？我尝试通过LabelMe来标记它们，但是LableMe没有参数，例如occlude_rate，visible_mask，invisible_mask，
这是 COCOA 数据集 json flie
在此处输入图片说明
这是 COCO 数据集 json flie
在此处输入图片描述
我有 COCO 格式的数据集，如何将它们标记为像 COCOA 和 KINS 这样的模态数据集？]]></description>
      <guid>https://stackoverflow.com/questions/78541723/how-to-label-an-amodal-dataset-like-cocoa</guid>
      <pubDate>Tue, 28 May 2024 02:44:02 GMT</pubDate>
    </item>
    <item>
      <title>有没有可以根据文档布局自动标记 PDF 表单字段的工具？ [关闭]</title>
      <link>https://stackoverflow.com/questions/78541705/any-tool-to-label-pdf-form-fields-automatically-based-on-document-layout</link>
      <description><![CDATA[我正在寻找一种工具，可以接受带有可填写表单字段的 PDF，然后根据文档的布局检测标签和部分。
例如，在下面的文档中，以红色突出显示的表单字段应具有标签：“投保人详细信息 -&gt;”全名”。蓝色表格字段应有标签：“驾驶或负责车辆的人员（需填写，即使停放）-&gt;”业务”。

否则，如果不存在这样的工具，我如何开始训练可以实现这一目标的模型？有没有我可以训练的基础模型？我对机器学习还很陌生。]]></description>
      <guid>https://stackoverflow.com/questions/78541705/any-tool-to-label-pdf-form-fields-automatically-based-on-document-layout</guid>
      <pubDate>Tue, 28 May 2024 02:32:31 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Kaggle 上以 .h5 格式保存深度学习模型</title>
      <link>https://stackoverflow.com/questions/78541201/unable-to-save-deep-learning-model-in-h5-format-on-kaggle</link>
      <description><![CDATA[我在尝试将深度学习模型以 .h5 格式保存在 Kaggle 上时遇到问题。尽管遵循了标准程序，但保存过程始终失败。在此处输入图片说明我已添加代码和面临的问题。
在此处输入图片说明
我将格式指定为 .keras，但模型无法保存。但是，代码在 Google Colab 上运行良好。不幸的是，Google Colab 的内存不足以有效运行我的代码。
任何解决此问题并确保在 Kaggle 平台上成功以 .h5 格式保存我的模型的见解或潜在解决方案都将对我非常有帮助。]]></description>
      <guid>https://stackoverflow.com/questions/78541201/unable-to-save-deep-learning-model-in-h5-format-on-kaggle</guid>
      <pubDate>Mon, 27 May 2024 21:49:55 GMT</pubDate>
    </item>
    <item>
      <title>即使经过 500 个 epoch，结果也没有改善 [关闭]</title>
      <link>https://stackoverflow.com/questions/78540839/results-not-improving-even-after-500-epochs</link>
      <description><![CDATA[]]></description>
      <guid>https://stackoverflow.com/questions/78540839/results-not-improving-even-after-500-epochs</guid>
      <pubDate>Mon, 27 May 2024 19:40:59 GMT</pubDate>
    </item>
    <item>
      <title>在与我之前训练的数据集不同的数据集上训练 yolov8 变得非常慢</title>
      <link>https://stackoverflow.com/questions/78539266/training-yolov8-on-a-different-data-set-than-i-had-previously-trained-it-on-beca</link>
      <description><![CDATA[我正在尝试在与我之前训练过的数据集不同的数据集上训练 yolov8。尽管这是一个较小的数据集，但即使 1 个 epoch 也需要极长的时间才能完成。还有其他人遇到这个问题吗？我可能哪里出错了？
我正在尝试在与我之前训练过的数据集不同的数据集上训练 yolov8。尽管这是一个较小的数据集，但即使 1 个 epoch 也需要非常长的时间才能完成。]]></description>
      <guid>https://stackoverflow.com/questions/78539266/training-yolov8-on-a-different-data-set-than-i-had-previously-trained-it-on-beca</guid>
      <pubDate>Mon, 27 May 2024 13:08:40 GMT</pubDate>
    </item>
    <item>
      <title>层“dense_4”的输入 0 与该层不兼容：预期输入形状的轴 -1 值为 1，但收到的输入形状为（无，6）</title>
      <link>https://stackoverflow.com/questions/78538382/input-0-of-layer-dense-4-is-incompatible-with-the-layer-expected-axis-1-of-i</link>
      <description><![CDATA[我正在尝试实现多元回归模型。
使用以下代码：
all_normalizer = keras.layers.Normalization(input_shape=(1, ), axis=-1)
all_normalizer.adapt(x_train_all)

nn_model = tf.keras.Sequential([
    all_正规化器，
    tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(32, 激活=&#39;relu&#39;),
    tf.keras.layers.Dense(1)
]）

nn_model.compile(keras.optimizers.Adam(learning_rate=0.001), loss=&#39;mean_squared_error&#39;)

历史记录 = nn_model.fit(
    x_train_Temp,y_train_Temp,
    验证数据=（x_val_Temp，y_val_Temp），
    详细 = 0，纪元 = 100
）

我收到以下错误：
 文件“C:\~ai.py”，第 330 行，在  中
    历史记录 = nn_model.fit(
              ^^^^^^^^^^^^^^
  文件“C:\~\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\utils\traceback_utils.py”，第 122 行，位于 error_handler 中
    从 None 引发 e.with_traceback(filtered_tb)
  文件“C:\~PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\input_spec.py”，第 227 行，位于assert_input_compatibility
    引发值错误（
ValueError：调用 Sequential.call() 时遇到异常。

层“dense_4”的输入0与图层不兼容：预期输入形状的轴 -1 值为 1，但收到的输入形状为（无，6）

Sequential.call() 收到的参数：
  输入=tf.Tensor（形状=（无，1），dtype=float32）
  • 训练=真
  • 掩码=无

我应该在代码中更改/实现什么来解决此错误？
PS：我是初学者，可能不懂东西，所以请不要嫌弃。]]></description>
      <guid>https://stackoverflow.com/questions/78538382/input-0-of-layer-dense-4-is-incompatible-with-the-layer-expected-axis-1-of-i</guid>
      <pubDate>Mon, 27 May 2024 09:54:27 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程置信度与可信区间</title>
      <link>https://stackoverflow.com/questions/60560152/gaussian-process-confidence-vs-credible-intervals</link>
      <description><![CDATA[由于高斯过程返回分布而不是点估计，为什么会这样示例（实际上在 GP 的每个示例中）谈论贝叶斯统计类似物的置信区间“可信区间” ？
更新
一个建议（不是来自我）是他们将它们称为置信区间，因为他们使用最大似然法而不是使用完整的贝叶斯方法 - 我对此不相信，因为经验贝叶斯方法也可以提供可信的区间]]></description>
      <guid>https://stackoverflow.com/questions/60560152/gaussian-process-confidence-vs-credible-intervals</guid>
      <pubDate>Fri, 06 Mar 2020 08:27:27 GMT</pubDate>
    </item>
    <item>
      <title>数据挖掘方面 R 与 Matlab 的比较 [关闭]</title>
      <link>https://stackoverflow.com/questions/4811995/comparing-r-to-matlab-for-data-mining</link>
      <description><![CDATA[我最近开始学习 R，而不是开始在 Matlab 中编码，主要是因为它是开源的。我目前从事数据挖掘和机器学习领域的工作。我发现许多用 R 实现的机器学习算法，并且我仍在探索用 R 实现的不同包。
我有一个简单的问题：您如何比较 R 和 Matlab 在数据挖掘应用中的受欢迎程度、优缺点、行业和学术接受度等？您会选择哪一个？为什么？
我根据各种指标对 Matlab 与 R 进行了各种比较，但我特别有兴趣获得其在数据挖掘和机器学习中的适用性的答案。 
由于这两种语言对我来说都很新，我只是想知道 R 是否是一个不错的选择。
我很感激任何建议。]]></description>
      <guid>https://stackoverflow.com/questions/4811995/comparing-r-to-matlab-for-data-mining</guid>
      <pubDate>Thu, 27 Jan 2011 01:04:05 GMT</pubDate>
    </item>
    </channel>
</rss>